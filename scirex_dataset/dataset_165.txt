document O
: O
Large O
Scale O
GAN Method
Training O
for O
High Task
Fidelity Task
Natural Task
Image Task
Synthesis Task
Despite O
recent O
progress O
in O
generative Task
image Task
modeling Task
, O
successfully O
generating O
high Task
- Task
resolution Task
, Task
diverse Task
samples Task
from O
complex O
datasets O
such O
as O
ImageNet Material
remains O
an O
elusive O
goal O
. O
To O
this O
end O
, O
we O
train O
Generative Method
Adversarial Method
Networks Method
at O
the O
largest O
scale O
yet O
attempted O
, O
and O
study O
the O
instabilities O
specific O
to O
such O
scale O
. O
We O
find O
that O
applying O
orthogonal Method
regularization Method
to O
the O
generator O
renders O
it O
amenable O
to O
a O
simple O
“ O
truncation O
trick O
, O
” O
allowing O
fine O
control O
over O
the O
trade O
- O
off O
between O
sample Metric
fidelity Metric
and O
variety Metric
by O
reducing O
the O
variance O
of O
the O
Generator O
’s O
input O
. O
Our O
modifications O
lead O
to O
models O
which O
set O
the O
new O
state O
of O
the O
art O
in O
class Task
- Task
conditional Task
image Task
synthesis Task
. O
When O
trained O
on O
ImageNet Material
at O
128 O
128 O
resolution O
, O
our O
models O
( O
BigGANs Method
) O
achieve O
an O
Inception Metric
Score Metric
( O
IS Metric
) O
of O
166.5 O
and O
Fréchet Metric
Inception Metric
Distance Metric
( O
FID Metric
) O
of O
7.4 O
, O
improving O
over O
the O
previous O
best O
IS Metric
of O
52.52 O
and O
FID Metric
of O
18.65 O
. O
mygreenrgb0.032 O
, O
0.6392 O
, O
0.2039 O
section O
: O
Introduction O
The O
state O
of O
generative Method
image Method
modeling Method
has O
advanced O
dramatically O
in O
recent O
years O
, O
with O
Generative Method
Adversarial Method
Networks Method
( O
GANs Method
, O
goodfellow2014gans O
) O
at O
the O
forefront O
of O
efforts O
to O
generate O
high O
- O
fidelity Metric
, O
diverse O
images O
with O
models O
learned O
directly O
from O
data O
. O
GAN Method
training O
is O
dynamic O
, O
and O
sensitive O
to O
nearly O
every O
aspect O
of O
its O
setup O
( O
from O
optimization O
parameters O
to O
model Method
architecture Method
) O
, O
but O
a O
torrent O
of O
research O
has O
yielded O
empirical O
and O
theoretical O
insights O
enabling O
stable O
training Task
in O
a O
variety O
of O
settings O
. O
Despite O
this O
progress O
, O
the O
current O
state O
of O
the O
art O
in O
conditional O
ImageNet Material
modeling O
zhang2018sagan O
achieves O
an O
Inception Metric
Score Metric
salimans2016improved O
of O
52.5 O
, O
compared O
to O
233 O
for O
real O
data O
. O
In O
this O
work O
, O
we O
set O
out O
to O
close O
the O
gap O
in O
fidelity Metric
and O
variety O
between O
images O
generated O
by O
GANs Method
and O
real O
- O
world O
images O
from O
the O
ImageNet Material
dataset O
. O
We O
make O
the O
following O
three O
contributions O
towards O
this O
goal O
: O
We O
demonstrate O
that O
GANs Method
benefit O
dramatically O
from O
scaling Method
, O
and O
train O
models O
with O
two O
to O
four O
times O
as O
many O
parameters O
and O
eight O
times O
the O
batch O
size O
compared O
to O
prior O
art O
. O
We O
introduce O
two O
simple O
, O
general O
architectural O
changes O
that O
improve O
scalability O
, O
and O
modify O
a O
regularization Method
scheme Method
to O
improve O
conditioning Task
, O
demonstrably O
boosting O
performance O
. O
As O
a O
side O
effect O
of O
our O
modifications O
, O
our O
models O
become O
amenable O
to O
the O
“ O
truncation Method
trick Method
, O
” O
a O
simple O
sampling Method
technique Method
that O
allows O
explicit O
, O
fine O
- O
grained O
control O
of O
the O
trade O
- O
off O
between O
sample Metric
variety Metric
and O
fidelity Metric
. O
We O
discover O
instabilities O
specific O
to O
large Method
scale Method
GANs Method
, O
and O
characterize O
them O
empirically O
. O
Leveraging O
insights O
from O
this O
analysis O
, O
we O
demonstrate O
that O
a O
combination O
of O
novel O
and O
existing O
techniques O
can O
reduce O
these O
instabilities O
, O
but O
complete O
training Metric
stability Metric
can O
only O
be O
achieved O
at O
a O
dramatic O
cost O
to O
performance O
. O
Our O
modifications O
substantially O
improve O
class Method
- Method
conditional Method
GANs O
. O
When O
trained O
on O
ImageNet Material
at O
128 O
128 O
resolution O
, O
our O
models O
( O
BigGANs Method
) O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
Inception Metric
Score Metric
( O
IS Metric
) O
and O
Fréchet Metric
Inception Metric
Distance Metric
( O
FID Metric
) O
from O
52.52 O
and O
18.65 O
to O
166.5 O
and O
7.4 O
respectively O
. O
We O
also O
successfully O
train O
BigGANs Method
on O
ImageNet Material
at O
256 O
256 O
and O
512 O
512 O
resolution O
, O
and O
achieve O
IS Metric
and O
FID Metric
of O
232.5 O
and O
8.1 O
at O
256 O
256 O
and O
IS Metric
and O
FID Metric
of O
241.5 O
and O
11.5 O
at O
512 O
512 O
. O
Finally O
, O
we O
train O
our O
models O
on O
an O
even O
larger O
dataset O
– O
JFT Method
- Method
300 Method
M O
– O
and O
demonstrate O
that O
our O
design O
choices O
transfer O
well O
from O
ImageNet Material
. O
Code O
and O
weights O
for O
our O
pretrained Method
generators Method
are O
publicly O
available O
. O
section O
: O
Background O
A O
Generative Method
Adversarial Method
Network Method
( O
GAN Method
) O
involves O
Generator O
( O
G Method
) O
and O
Discriminator Method
( Method
D Method
) Method
networks Method
whose O
purpose O
, O
respectively O
, O
is O
to O
map O
random O
noise O
to O
samples O
and O
discriminate O
real O
and O
generated O
samples O
. O
Formally O
, O
the O
GAN Method
objective Method
, O
in O
its O
original O
form O
goodfellow2014gans O
involves O
finding O
a O
Nash Method
equilibrium Method
to O
the O
following O
two O
player Task
min Task
- Task
max Task
problem Task
: O
where O
is O
a O
latent O
variable O
drawn O
from O
distribution O
such O
as O
or O
. O
When O
applied O
to O
images O
, O
G Method
and O
D O
are O
usually O
convolutional Method
neural Method
networks Method
radford2016dcgan O
. O
Without O
auxiliary Method
stabilization Method
techniques Method
, O
this O
training Method
procedure Method
is O
notoriously O
brittle O
, O
requiring O
finely O
- O
tuned O
hyperparameters O
and O
architectural O
choices O
to O
work O
at O
all O
. O
Much O
recent O
research O
has O
accordingly O
focused O
on O
modifications O
to O
the O
vanilla O
GAN Method
procedure O
to O
impart O
stability O
, O
drawing O
on O
a O
growing O
body O
of O
empirical O
and O
theoretical O
insights O
nowozin2016fgan O
, O
sonderby2017map O
, O
fedus2018many O
. O
One O
line O
of O
work O
is O
focused O
on O
changing O
the O
objective O
function O
arjovsky2017wgan O
, O
mao2016lsgan O
, O
lim2017geometric O
, O
bellemare2017cramergan O
, O
salimans2016otgan O
to O
encourage O
convergence Task
. O
Another O
line O
is O
focused O
on O
constraining O
D O
through O
gradient Method
penalties Method
gulrajani2017improved O
, O
kodali2014dragan O
, O
mescheder2018r1gp O
or O
normalization Method
miyato2018spectral O
, O
both O
to O
counteract O
the O
use O
of O
unbounded O
loss O
functions O
and O
ensure O
D O
provides O
gradients O
everywhere O
to O
G Method
. O
Of O
particular O
relevance O
to O
our O
work O
is O
Spectral Method
Normalization Method
miyato2018spectral O
, O
which O
enforces O
Lipschitz O
continuity O
on O
D O
by O
normalizing O
its O
parameters O
with O
running O
estimates O
of O
their O
first O
singular O
values O
, O
inducing O
backwards O
dynamics O
that O
adaptively O
regularize O
the O
top O
singular O
direction O
. O
Relatedly O
odena2018causal O
analyze O
the O
condition O
number O
of O
the O
Jacobian O
of O
G Method
and O
find O
that O
performance O
is O
dependent O
on O
G Method
’s O
conditioning O
. O
zhang2018sagan O
find O
that O
employing O
Spectral Method
Normalization Method
in O
G Method
improves O
stability O
, O
allowing O
for O
fewer O
D O
steps O
per O
iteration O
. O
We O
extend O
on O
these O
analyses O
to O
gain O
further O
insight O
into O
the O
pathology O
of O
GAN Method
training O
. O
Other O
works O
focus O
on O
the O
choice O
of O
architecture O
, O
such O
as O
SA Method
- Method
GAN Method
zhang2018sagan O
which O
adds O
the O
self O
- O
attention O
block O
from O
wang2018nonlocal O
to O
improve O
the O
ability O
of O
both O
G Method
and O
D O
to O
model O
global O
structure O
. O
ProGAN O
karras2018progan O
trains O
high Method
- Method
resolution Method
GANs Method
in O
the O
single Task
- Task
class Task
setting Task
by O
training O
a O
single O
model O
across O
a O
sequence O
of O
increasing O
resolutions O
. O
In O
conditional O
GANs O
mirza2014conditional O
class O
information O
can O
be O
fed O
into O
the O
model O
in O
various O
ways O
. O
In O
odena2017acgan O
it O
is O
provided O
to O
G Method
by O
concatenating O
a O
1 O
- O
hot O
class O
vector O
to O
the O
noise O
vector O
, O
and O
the O
objective O
is O
modified O
to O
encourage O
conditional O
samples O
to O
maximize O
the O
corresponding O
class O
probability O
predicted O
by O
an O
auxiliary Method
classifier Method
. O
devries2017modulating O
and O
dumoulin2017artistic O
modify O
the O
way O
class O
conditioning O
is O
passed O
to O
G Method
by O
supplying O
it O
with O
class Method
- Method
conditional Method
gains O
and O
biases O
in O
BatchNorm Method
ioffe2015batchnorm O
layers O
. O
In O
, O
D O
is O
conditioned O
by O
using O
the O
cosine O
similarity O
between O
its O
features O
and O
a O
set O
of O
learned O
class O
embeddings O
as O
additional O
evidence O
for O
distinguishing O
real O
and O
generated O
samples O
, O
effectively O
encouraging O
generation O
of O
samples O
whose O
features O
match O
a O
learned O
class O
prototype O
. O
Objectively O
evaluating O
implicit Method
generative Method
models Method
is O
difficult O
theis2015note O
. O
A O
variety O
of O
works O
have O
proposed O
heuristics O
for O
measuring O
the O
sample Metric
quality Metric
of Metric
models Metric
without O
tractable O
likelihoods O
salimans2016improved O
, O
heusel2017ttur O
, O
biÅkowski2018demystifying O
, O
wu2017ais O
. O
Of O
these O
, O
the O
Inception Metric
Score Metric
( O
IS Metric
, O
salimans2016improved O
) O
and O
Fréchet Metric
Inception Metric
Distance Metric
( O
FID Metric
, O
heusel2017ttur O
) O
have O
become O
popular O
despite O
their O
notable O
flaws O
barratt2018note O
. O
We O
employ O
them O
as O
approximate Metric
measures Metric
of O
sample Metric
quality Metric
, O
and O
to O
enable O
comparison O
against O
previous O
work O
. O
section O
: O
Scaling O
Up O
GANs Task
In O
this O
section O
, O
we O
explore O
methods O
for O
scaling O
up O
GAN Method
training O
to O
reap O
the O
performance O
benefits O
of O
larger O
models O
and O
larger O
batches O
. O
As O
a O
baseline O
, O
we O
employ O
the O
SA O
- O
GAN Method
architecture O
of O
zhang2018sagan O
, O
which O
uses O
the O
hinge Method
loss Method
lim2017geometric O
, O
tran2017hierarchical O
GAN Method
objective Method
. O
We O
provide O
class O
information O
to O
G Method
with O
class Method
- Method
conditional Method
BatchNorm Method
dumoulin2017artistic O
, O
devries2017modulating O
and O
to O
D O
with O
projection Method
miyato2018cgans O
. O
The O
optimization Method
settings Method
follow O
zhang2018sagan O
( O
notably O
employing O
Spectral O
Norm O
in O
G Method
) O
with O
the O
modification O
that O
we O
halve O
the O
learning Metric
rates Metric
and O
take O
two O
D O
steps O
per O
G Method
step O
. O
For O
evaluation O
, O
we O
employ O
moving O
averages O
of O
G Method
’s O
weights O
following O
karras2018progan O
, O
mescheder2018r1gp O
, O
yazici2018ema O
, O
with O
a O
decay O
of O
. O
We O
use O
Orthogonal Method
Initialization Method
saxe2014ortho O
, O
whereas O
previous O
works O
used O
radford2016dcgan O
or O
Xavier Method
initialization Method
glorot2010init O
. O
Each O
model O
is O
trained O
on O
128 O
to O
512 O
cores O
of O
a O
Google Method
TPUv3 Method
Pod Method
tpu Method
, O
and O
computes O
BatchNorm Method
statistics O
in O
G Method
across O
all O
devices O
, O
rather O
than O
per O
- O
device O
as O
is O
typical O
. O
We O
find O
progressive Method
growing Method
karras2018progan O
unnecessary O
even O
for O
our O
512 Method
512 Method
models Method
. O
Additional O
details O
are O
in O
Appendix O
[ O
reference O
] O
. O
We O
begin O
by O
increasing O
the O
batch O
size O
for O
the O
baseline O
model O
, O
and O
immediately O
find O
tremendous O
benefits O
in O
doing O
so O
. O
Rows O
1 O
- O
4 O
of O
Table O
[ O
reference O
] O
show O
that O
simply O
increasing O
the O
batch O
size O
by O
a O
factor O
of O
8 O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
IS Metric
by O
46 O
% O
. O
We O
conjecture O
that O
this O
is O
a O
result O
of O
each O
batch O
covering O
more O
modes O
, O
providing O
better O
gradients O
for O
both O
networks O
. O
One O
notable O
side O
effect O
of O
this O
scaling O
is O
that O
our O
models O
reach O
better O
final O
performance O
in O
fewer O
iterations O
, O
but O
become O
unstable O
and O
undergo O
complete O
training O
collapse O
. O
We O
discuss O
the O
causes O
and O
ramifications O
of O
this O
in O
Section O
[ O
reference O
] O
. O
For O
these O
experiments O
, O
we O
report O
scores O
from O
checkpoints O
saved O
just O
before O
collapse O
. O
We O
then O
increase O
the O
width O
( O
number O
of O
channels O
) O
in O
each O
layer O
by O
50 O
% O
, O
approximately O
doubling O
the O
number O
of O
parameters O
in O
both O
models O
. O
This O
leads O
to O
a O
further O
IS Metric
improvement O
of O
21 O
% O
, O
which O
we O
posit O
is O
due O
to O
the O
increased O
capacity O
of O
the O
model O
relative O
to O
the O
complexity O
of O
the O
dataset O
. O
Doubling O
the O
depth O
did O
not O
initially O
lead O
to O
improvement O
– O
we O
addressed O
this O
later O
in O
the O
BigGAN Method
- Method
deep Method
model Method
, O
which O
uses O
a O
different O
residual O
block O
structure O
. O
We O
note O
that O
class O
embeddings O
used O
for O
the O
conditional O
BatchNorm Method
layers O
in O
G Method
contain O
a O
large O
number O
of O
weights O
. O
Instead O
of O
having O
a O
separate O
layer O
for O
each O
embedding O
miyato2018spectral O
, O
zhang2018sagan O
, O
we O
opt O
to O
use O
a O
shared Method
embedding Method
, O
which O
is O
linearly O
projected O
to O
each O
layer O
’s O
gains O
and O
biases O
perez2018film O
. O
This O
reduces O
computation Metric
and Metric
memory Metric
costs Metric
, O
and O
improves O
training Metric
speed Metric
( O
in O
number O
of O
iterations O
required O
to O
reach O
a O
given O
performance O
) O
by O
37 O
% O
. O
Next O
, O
we O
add O
direct O
skip O
connections O
( O
skip O
- O
) O
from O
the O
noise O
vector O
to O
multiple O
layers O
of O
G Method
rather O
than O
just O
the O
initial O
layer O
. O
The O
intuition O
behind O
this O
design O
is O
to O
allow O
G Method
to O
use O
the O
latent O
space O
to O
directly O
influence O
features O
at O
different O
resolutions O
and O
levels O
of O
hierarchy O
. O
In O
BigGAN Method
, O
this O
is O
accomplished O
by O
splitting O
into O
one O
chunk O
per O
resolution O
, O
and O
concatenating O
each O
chunk O
to O
the O
conditional O
vector O
which O
gets O
projected O
to O
the O
BatchNorm Method
gains O
and O
biases O
. O
In O
BigGAN Method
- O
deep O
, O
we O
use O
an O
even O
simpler O
design O
, O
concatenating O
the O
entire O
with O
the O
conditional O
vector O
without O
splitting O
it O
into O
chunks O
. O
Previous O
works O
goodfellow2014gans O
, O
denton2015lapgan O
have O
considered O
variants O
of O
this O
concept O
; O
our O
implementation O
is O
a O
minor O
modification O
of O
this O
design O
. O
Skip Method
- Method
provides O
a O
modest O
performance O
improvement O
of O
around O
4 O
% O
, O
and O
improves O
training Metric
speed Metric
by O
a O
further O
18 O
% O
. O
subsection O
: O
Trading O
off O
variety Metric
and O
fidelity Metric
with O
the O
Truncation Method
Trick Method
Unlike O
models O
which O
need O
to O
backpropagate O
through O
their O
latents O
, O
GANs Method
can O
employ O
an O
arbitrary O
prior O
, O
yet O
the O
vast O
majority O
of O
previous O
works O
have O
chosen O
to O
draw O
from O
either O
or O
. O
We O
question O
the O
optimality O
of O
this O
choice O
and O
explore O
alternatives O
in O
Appendix O
[ O
reference O
] O
. O
Remarkably O
, O
our O
best O
results O
come O
from O
using O
a O
different O
latent O
distribution O
for O
sampling Task
than O
was O
used O
in O
training O
. O
Taking O
a O
model O
trained O
with O
and O
sampling O
from O
a O
truncated O
normal O
( O
where O
values O
which O
fall O
outside O
a O
range O
are O
resampled O
to O
fall O
inside O
that O
range O
) O
immediately O
provides O
a O
boost O
to O
IS Metric
and O
FID Metric
. O
We O
call O
this O
the O
Truncation Method
Trick Method
: O
truncating O
a O
vector O
by O
resampling O
the O
values O
with O
magnitude O
above O
a O
chosen O
threshold O
leads O
to O
improvement O
in O
individual Metric
sample Metric
quality Metric
at O
the O
cost O
of O
reduction O
in O
overall Metric
sample Metric
variety Metric
. O
Figure O
[ O
reference O
] O
( O
a O
) O
demonstrates O
this O
: O
as O
the O
threshold O
is O
reduced O
, O
and O
elements O
of O
are O
truncated O
towards O
zero O
( O
the O
mode O
of O
the O
latent O
distribution O
) O
, O
individual O
samples O
approach O
the O
mode O
of O
G Method
’s O
output O
distribution O
. O
Related O
observations O
about O
this O
trade O
- O
off O
were O
made O
in O
marchesi2017megapixel O
, O
pieters2018bachelors O
. O
This O
technique O
allows O
fine O
- O
grained O
, O
post O
- O
hoc O
selection O
of O
the O
trade O
- O
off O
between O
sample Metric
quality Metric
and O
variety O
for O
a O
given O
G Method
. O
Notably O
, O
we O
can O
compute O
FID Metric
and O
IS Metric
for O
a O
range O
of O
thresholds O
, O
obtaining O
the O
variety Metric
- Metric
fidelity Metric
curve Metric
reminiscent O
of O
the O
precision Metric
- Metric
recall Metric
curve Metric
( O
Figure O
[ O
reference O
] O
) O
. O
As O
IS Metric
does O
not O
penalize O
lack O
of O
variety O
in O
class Method
- Method
conditional Method
models O
, O
reducing O
the O
truncation O
threshold O
leads O
to O
a O
direct O
increase O
in O
IS Metric
( O
analogous O
to O
precision Metric
) O
. O
FID Metric
penalizes O
lack O
of O
variety O
( O
analogous O
to O
recall Metric
) O
but O
also O
rewards O
precision Metric
, O
so O
we O
initially O
see O
a O
moderate O
improvement O
in O
FID Metric
, O
but O
as O
truncation O
approaches O
zero O
and O
variety O
diminishes O
, O
the O
FID Metric
sharply O
drops O
. O
The O
distribution O
shift O
caused O
by O
sampling O
with O
different O
latents O
than O
those O
seen O
in O
training O
is O
problematic O
for O
many O
models O
. O
Some O
of O
our O
larger O
models O
are O
not O
amenable O
to O
truncation O
, O
producing O
saturation O
artifacts O
( O
Figure O
[ O
reference O
] O
( O
b O
) O
) O
when O
fed O
truncated O
noise O
. O
To O
counteract O
this O
, O
we O
seek O
to O
enforce O
amenability O
to O
truncation Task
by O
conditioning O
G Method
to O
be O
smooth O
, O
so O
that O
the O
full O
space O
of O
will O
map O
to O
good O
output O
samples O
. O
For O
this O
, O
we O
turn O
to O
Orthogonal O
Regularization O
brock2017photo O
, O
which O
directly O
enforces O
the O
orthogonality O
condition O
: O
where O
is O
a O
weight O
matrix O
and O
a O
hyperparameter O
. O
This O
regularization O
is O
known O
to O
often O
be O
too O
limiting O
miyato2018spectral O
, O
so O
we O
explore O
several O
variants O
designed O
to O
relax O
the O
constraint O
while O
still O
imparting O
the O
desired O
smoothness O
to O
our O
models O
. O
The O
version O
we O
find O
to O
work O
best O
removes O
the O
diagonal O
terms O
from O
the O
regularization Method
, O
and O
aims O
to O
minimize O
the O
pairwise Metric
cosine Metric
similarity Metric
between O
filters O
but O
does O
not O
constrain O
their O
norm O
: O
where O
denotes O
a O
matrix O
with O
all O
elements O
set O
to O
. O
We O
sweep O
values O
and O
select O
, O
finding O
this O
small O
added O
penalty O
sufficient O
to O
improve O
the O
likelihood O
that O
our O
models O
will O
be O
amenable O
to O
truncation Task
. O
Across O
runs O
in O
Table O
[ O
reference O
] O
, O
we O
observe O
that O
without O
Orthogonal Method
Regularization Method
, O
only O
16 O
% O
of O
models O
are O
amenable O
to O
truncation Task
, O
compared O
to O
60 O
% O
when O
trained O
with O
Orthogonal Method
Regularization Method
. O
subsection O
: O
Summary O
We O
find O
that O
current O
GAN Method
techniques O
are O
sufficient O
to O
enable O
scaling O
to O
large O
models O
and O
distributed Task
, Task
large Task
- Task
batch Task
training Task
. O
We O
find O
that O
we O
can O
dramatically O
improve O
the O
state O
of O
the O
art O
and O
train O
models O
up O
to O
512 O
512 O
resolution O
without O
need O
for O
explicit Method
multiscale Method
methods Method
like O
. O
Despite O
these O
improvements O
, O
our O
models O
undergo O
training Task
collapse Task
, O
necessitating O
early O
stopping O
in O
practice O
. O
In O
the O
next O
two O
sections O
we O
investigate O
why O
settings O
which O
were O
stable O
in O
previous O
works O
become O
unstable O
when O
applied O
at O
scale O
. O
section O
: O
Analysis O
subsection O
: O
Characterizing Task
Instability Task
: O
The O
Generator O
Much O
previous O
work O
has O
investigated O
GAN Method
stability O
from O
a O
variety O
of O
analytical O
angles O
and O
on O
toy Task
problems Task
, O
but O
the O
instabilities O
we O
observe O
occur O
for O
settings O
which O
are O
stable O
at O
small O
scale O
, O
necessitating O
direct O
analysis O
at O
large O
scale O
. O
We O
monitor O
a O
range O
of O
weight O
, O
gradient O
, O
and O
loss O
statistics O
during O
training Task
, O
in O
search O
of O
a O
metric O
which O
might O
presage O
the O
onset O
of O
training Task
collapse Task
, O
similar O
to O
odena2018causal O
. O
We O
found O
the O
top O
three O
singular O
values O
of O
each O
weight O
matrix O
to O
be O
the O
most O
informative O
. O
They O
can O
be O
efficiently O
computed O
using O
the O
Alrnoldi Method
iteration Method
method Method
golub2000eigenvalue O
, O
which O
extends O
the O
power Method
iteration Method
method Method
, O
used O
in O
miyato2018spectral O
, O
to O
estimation O
of O
additional O
singular O
vectors O
and O
values O
. O
A O
clear O
pattern O
emerges O
, O
as O
can O
be O
seen O
in O
Figure O
[ O
reference O
] O
( O
a O
) O
and O
Appendix O
[ O
reference O
] O
: O
most O
G Method
layers O
have O
well O
- O
behaved O
spectral O
norms O
, O
but O
some O
layers O
( O
typically O
the O
first O
layer O
in O
G Method
, O
which O
is O
over O
- O
complete O
and O
not O
convolutional Method
) O
are O
ill O
- O
behaved O
, O
with O
spectral O
norms O
that O
grow O
throughout O
training O
and O
explode O
at O
collapse O
. O
To O
ascertain O
if O
this O
pathology O
is O
a O
cause O
of O
collapse O
or O
merely O
a O
symptom O
, O
we O
study O
the O
effects O
of O
imposing O
additional O
conditioning O
on O
G Method
to O
explicitly O
counteract O
spectral O
explosion O
. O
First O
, O
we O
directly O
regularize O
the O
top O
singular O
values O
of O
each O
weight O
, O
either O
towards O
a O
fixed O
value O
or O
towards O
some O
ratio O
of O
the O
second O
singular O
value O
, O
( O
with O
the O
stop O
- O
gradient O
operation O
to O
prevent O
the O
regularization O
from O
increasing O
) O
. O
Alternatively O
, O
we O
employ O
a O
partial Method
singular Method
value Method
decomposition Method
to O
instead O
clamp O
. O
Given O
a O
weight O
, O
its O
first O
singular O
vectors O
and O
, O
and O
the O
value O
to O
which O
the O
will O
be O
clamped O
, O
our O
weights O
become O
: O
where O
is O
set O
to O
either O
or O
. O
We O
observe O
that O
both O
with O
and O
without O
Spectral Method
Normalization Method
these O
techniques O
have O
the O
effect O
of O
preventing O
the O
gradual O
increase O
and O
explosion O
of O
either O
or O
, O
but O
even O
though O
in O
some O
cases O
they O
mildly O
improve O
performance O
, O
no O
combination O
prevents O
training Task
collapse Task
. O
This O
evidence O
suggests O
that O
while O
conditioning O
G Method
might O
improve O
stability O
, O
it O
is O
insufficient O
to O
ensure O
stability O
. O
We O
accordingly O
turn O
our O
attention O
to O
D O
. O
subsection O
: O
Characterizing O
Instability O
: O
The O
Discriminator O
As O
with O
G Method
, O
we O
analyze O
the O
spectra O
of O
D O
’s O
weights O
to O
gain O
insight O
into O
its O
behavior O
, O
then O
seek O
to O
stabilize O
training Task
by O
imposing O
additional O
constraints O
. O
Figure O
[ O
reference O
] O
( O
b O
) O
displays O
a O
typical O
plot O
of O
for O
D O
( O
with O
further O
plots O
in O
Appendix O
[ O
reference O
] O
) O
. O
Unlike O
G Method
, O
we O
see O
that O
the O
spectra O
are O
noisy O
, O
is O
well O
- O
behaved O
, O
and O
the O
singular O
values O
grow O
throughout O
training O
but O
only O
jump O
at O
collapse O
, O
instead O
of O
exploding O
. O
The O
spikes O
in O
D O
’s O
spectra O
might O
suggest O
that O
it O
periodically O
receives O
very O
large O
gradients O
, O
but O
we O
observe O
that O
the O
Frobenius O
norms O
are O
smooth O
( O
Appendix O
[ O
reference O
] O
) O
, O
suggesting O
that O
this O
effect O
is O
primarily O
concentrated O
on O
the O
top O
few O
singular O
directions O
. O
We O
posit O
that O
this O
noise O
is O
a O
result O
of O
optimization Task
through O
the O
adversarial Method
training Method
process Method
, O
where O
G Method
periodically O
produces O
batches O
which O
strongly O
perturb O
D O
. O
If O
this O
spectral O
noise O
is O
causally O
related O
to O
instability O
, O
a O
natural O
counter O
is O
to O
employ O
gradient Method
penalties Method
, O
which O
explicitly O
regularize O
changes O
in O
D O
’s O
Jacobian O
. O
We O
explore O
the O
zero Method
- Method
centered Method
gradient Method
penalty Method
from O
mescheder2018r1gp Method
: O
With O
the O
default O
suggested O
strength O
of O
10 O
, O
training O
becomes O
stable O
and O
improves O
the O
smoothness Metric
and O
boundedness O
of O
spectra O
in O
both O
G Method
and O
D O
, O
but O
performance O
severely O
degrades O
, O
resulting O
in O
a O
45 O
% O
reduction O
in O
IS Metric
. O
Reducing O
the O
penalty O
partially O
alleviates O
this O
degradation O
, O
but O
results O
in O
increasingly O
ill O
- O
behaved O
spectra O
; O
even O
with O
the O
penalty O
strength O
reduced O
to O
( O
the O
lowest O
strength O
for O
which O
sudden O
collapse O
does O
not O
occur O
) O
the O
IS Metric
is O
reduced O
by O
20 O
% O
. O
Repeating O
this O
experiment O
with O
various O
strengths O
of O
Orthogonal Method
Regularization Method
, O
DropOut Method
srivastava2014dropout O
, O
and O
L2 Method
( O
See O
Appendix O
[ O
reference O
] O
for O
details O
) O
, O
reveals O
similar O
behaviors O
for O
these O
regularization Method
strategies Method
: O
with O
high O
enough O
penalties O
on O
D O
, O
training Metric
stability Metric
can O
be O
achieved O
, O
but O
at O
a O
substantial O
cost O
to O
performance O
. O
We O
also O
observe O
that O
D O
’s O
loss O
approaches O
zero O
during O
training O
, O
but O
undergoes O
a O
sharp O
upward O
jump O
at O
collapse O
( O
Appendix O
[ O
reference O
] O
) O
. O
One O
possible O
explanation O
for O
this O
behavior O
is O
that O
D O
is O
overfitting O
to O
the O
training O
set O
, O
memorizing O
training O
examples O
rather O
than O
learning O
some O
meaningful O
boundary O
between O
real O
and O
generated O
images O
. O
As O
a O
simple O
test O
for O
D Task
’s Task
memorization Task
( O
related O
to O
) O
, O
we O
evaluate O
uncollapsed Method
discriminators Method
on O
the O
ImageNet Material
training Material
and O
validation Material
sets Material
, O
and O
measure O
what O
percentage O
of O
samples O
are O
classified O
as O
real O
or O
generated O
. O
While O
the O
training Metric
accuracy Metric
is O
consistently O
above O
98 O
% O
, O
the O
validation Metric
accuracy Metric
falls O
in O
the O
range O
of O
50 O
- O
55 O
% O
, O
no O
better O
than O
random Method
guessing Method
( O
regardless O
of O
regularization Method
strategy Method
) O
. O
This O
confirms O
that O
D O
is O
indeed O
memorizing O
the O
training O
set O
; O
we O
deem O
this O
in O
line O
with O
D O
’s O
role O
, O
which O
is O
not O
explicitly O
to O
generalize O
, O
but O
to O
distill O
the O
training O
data O
and O
provide O
a O
useful O
learning O
signal O
for O
G Method
. O
Additional O
experiments O
and O
discussion O
are O
provided O
in O
Appendix O
[ O
reference O
] O
. O
subsection O
: O
Summary O
We O
find O
that O
stability O
does O
not O
come O
solely O
from O
G Method
or O
D O
, O
but O
from O
their O
interaction O
through O
the O
adversarial Method
training Method
process Method
. O
While O
the O
symptoms O
of O
their O
poor O
conditioning O
can O
be O
used O
to O
track O
and O
identify O
instability O
, O
ensuring O
reasonable O
conditioning O
proves O
necessary O
for O
training Task
but O
insufficient O
to O
prevent O
eventual O
training O
collapse O
. O
It O
is O
possible O
to O
enforce O
stability O
by O
strongly O
constraining O
D O
, O
but O
doing O
so O
incurs O
a O
dramatic O
cost O
in O
performance O
. O
With O
current O
techniques O
, O
better O
final O
performance O
can O
be O
achieved O
by O
relaxing O
this O
conditioning O
and O
allowing O
collapse O
to O
occur O
at O
the O
later O
stages O
of O
training O
, O
by O
which O
time O
a O
model O
is O
sufficiently O
trained O
to O
achieve O
good O
results O
. O
section O
: O
Experiments O
subsection O
: O
Evaluation O
on O
ImageNet Material
We O
evaluate O
our O
models O
on O
ImageNet Material
ILSVRC Material
2012 Material
ILSVRC2015 Material
at O
128 O
128 O
, O
256 O
256 O
, O
and O
512 O
512 O
resolutions O
, O
employing O
the O
settings O
from O
Table O
[ O
reference O
] O
, O
row O
8 O
. O
The O
samples O
generated O
by O
our O
models O
are O
presented O
in O
Figure O
[ O
reference O
] O
, O
with O
additional O
samples O
in O
Appendix O
[ O
reference O
] O
, O
and O
online O
. O
We O
report O
IS Metric
and O
FID Metric
in O
Table O
[ O
reference O
] O
. O
As O
our O
models O
are O
able O
to O
trade O
sample O
variety O
for O
quality Metric
, O
it O
is O
unclear O
how O
best O
to O
compare O
against O
prior O
art O
; O
we O
accordingly O
report O
values O
at O
three O
settings O
, O
with O
complete O
curves O
in O
Appendix O
[ O
reference O
] O
. O
First O
, O
we O
report O
the O
FID Metric
/ O
IS Metric
values O
at O
the O
truncation O
setting O
which O
attains O
the O
best O
FID Metric
. O
Second O
, O
we O
report O
the O
FID Metric
at O
the O
truncation O
setting O
for O
which O
our O
model O
’s O
IS Metric
is O
the O
same O
as O
that O
attained O
by O
the O
real O
validation O
data O
, O
reasoning O
that O
this O
is O
a O
passable O
measure O
of O
maximum Metric
sample Metric
variety Metric
achieved O
while O
still O
achieving O
a O
good O
level O
of O
“ O
objectness O
. O
” O
Third O
, O
we O
report O
FID Metric
at O
the O
maximum O
IS Metric
achieved O
by O
each O
model O
, O
to O
demonstrate O
how O
much O
variety O
must O
be O
traded O
off O
to O
maximize O
quality Metric
. O
In O
all O
three O
cases O
, O
our O
models O
outperform O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
IS Metric
and O
FID Metric
scores O
achieved O
by O
miyato2018spectral Method
and O
. O
In O
addition O
to O
the O
BigGAN Method
model O
introduced O
in O
the O
first O
version O
of O
the O
paper O
and O
used O
in O
the O
majority O
of O
experiments O
( O
unless O
otherwise O
stated O
) O
, O
we O
also O
present O
a O
4x Method
deeper Method
model Method
( O
BigGAN Method
- O
deep O
) O
which O
uses O
a O
different O
configuration O
of O
residual O
blocks O
. O
As O
can O
be O
seen O
from O
Table O
[ O
reference O
] O
, O
BigGAN Method
- O
deep O
substantially O
outperforms O
BigGAN Method
across O
all O
resolutions Metric
and O
metrics Metric
. O
This O
confirms O
that O
our O
findings O
extend O
to O
other O
architectures O
, O
and O
that O
increased O
depth O
leads O
to O
improvement O
in O
sample Metric
quality Metric
. O
Both O
BigGAN Method
and O
BigGAN Method
- Method
deep Method
architectures Method
are O
described O
in O
Appendix O
[ O
reference O
] O
. O
Our O
observation O
that O
D O
overfits O
to O
the O
training O
set O
, O
coupled O
with O
our O
model O
’s O
sample Metric
quality Metric
, O
raises O
the O
obvious O
question O
of O
whether O
or O
not O
G Method
simply O
memorizes O
training O
points O
. O
To O
test O
this O
, O
we O
perform O
class Method
- Method
wise Method
nearest Method
neighbors Method
analysis Method
in O
pixel O
space O
and O
the O
feature O
space O
of O
pre O
- O
trained O
classifier Method
networks Method
( O
Appendix O
[ O
reference O
] O
) O
. O
In O
addition O
, O
we O
present O
both O
interpolations O
between O
samples O
and O
class Method
- Method
wise Method
interpolations Method
( O
where O
is O
held O
constant O
) O
in O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Our O
model O
convincingly O
interpolates O
between O
disparate O
samples O
, O
and O
the O
nearest O
neighbors O
for O
its O
samples O
are O
visually O
distinct O
, O
suggesting O
that O
our O
model O
does O
not O
simply O
memorize O
training O
data O
. O
We O
note O
that O
some O
failure O
modes O
of O
our O
partially Method
- Method
trained Method
models Method
are O
distinct O
from O
those O
previously O
observed O
. O
Most O
previous O
failures O
involve O
local O
artifacts O
odena2016deconvolution O
, O
images O
consisting O
of O
texture O
blobs O
instead O
of O
objects O
salimans2016improved O
, O
or O
the O
canonical Method
mode Method
collapse Method
. O
We O
observe O
class O
leakage O
, O
where O
images O
from O
one O
class O
contain O
properties O
of O
another O
, O
as O
exemplified O
by O
Figure O
[ O
reference O
] O
( O
d O
) O
. O
We O
also O
find O
that O
many O
classes O
on O
ImageNet Material
are O
more O
difficult O
than O
others O
for O
our O
model O
; O
our O
model O
is O
more O
successful O
at O
generating O
dogs O
( O
which O
make O
up O
a O
large O
portion O
of O
the O
dataset O
, O
and O
are O
mostly O
distinguished O
by O
their O
texture O
) O
than O
crowds O
( O
which O
comprise O
a O
small O
portion O
of O
the O
dataset O
and O
have O
more O
large O
- O
scale O
structure O
) O
. O
Further O
discussion O
is O
available O
in O
Appendix O
[ O
reference O
] O
. O
subsection O
: O
Additional O
evaluation O
on O
JFT Task
- Task
300 Task
M Task
To O
confirm O
that O
our O
design O
choices O
are O
effective O
for O
even O
larger O
and O
more O
complex O
and O
diverse O
datasets O
, O
we O
also O
present O
results O
of O
our O
system O
on O
a O
subset O
of O
JFT O
- O
300 O
M O
sun17revisiting O
. O
The O
full O
JFT O
- O
300 O
M O
dataset O
contains O
300 O
M O
real O
- O
world O
images O
labeled O
with O
18 O
K O
categories O
. O
Since O
the O
category O
distribution O
is O
heavily O
long O
- O
tailed O
, O
we O
subsample O
the O
dataset O
to O
keep O
only O
images O
with O
the O
8.5 O
K O
most O
common O
labels O
. O
The O
resulting O
dataset O
contains O
292 O
M O
images O
– O
two O
orders O
of O
magnitude O
larger O
than O
ImageNet Material
. O
For O
images O
with O
multiple O
labels O
, O
we O
sample O
a O
single O
label O
randomly O
and O
independently O
whenever O
an O
image O
is O
sampled O
. O
To O
compute O
IS Metric
and O
FID Metric
for O
the O
GANs Method
trained O
on O
this O
dataset O
, O
we O
use O
an O
Inception Method
v2 Method
classifier Method
szegedy2015rethinking O
trained O
on O
this O
dataset O
. O
Quantitative O
results O
are O
presented O
in O
Table O
[ O
reference O
] O
. O
All O
models O
are O
trained O
with O
batch O
size O
2048 O
. O
We O
compare O
an O
ablated Method
version Method
of O
our O
model O
– O
comparable O
to O
SA O
- O
GAN Method
zhang2018sagan O
but O
with O
the O
larger O
batch O
size O
– O
against O
a O
“ O
full O
” O
BigGAN Method
model O
that O
makes O
uses O
of O
all O
of O
the O
techniques O
applied O
to O
obtain O
the O
best O
results O
on O
ImageNet Material
( O
shared Task
embedding Task
, O
skip Method
- Method
, O
and O
orthogonal Method
regularization Method
) O
. O
Our O
results O
show O
that O
these O
techniques O
substantially O
improve O
performance O
even O
in O
the O
setting O
of O
this O
much O
larger O
dataset O
at O
the O
same O
model O
capacity O
( O
64 O
base O
channels O
) O
. O
We O
further O
show O
that O
for O
a O
dataset O
of O
this O
scale O
, O
we O
see O
significant O
additional O
improvements O
from O
expanding O
the O
capacity O
of O
our O
models O
to O
128 O
base O
channels O
, O
while O
for O
ImageNet Material
GANs O
that O
additional O
capacity O
was O
not O
beneficial O
. O
In O
Figure O
[ O
reference O
] O
( O
Appendix O
[ O
reference O
] O
) O
, O
we O
present O
truncation O
plots O
for O
models O
trained O
on O
this O
dataset O
. O
Unlike O
for O
ImageNet Material
, O
where O
truncation O
limits O
of O
tend O
to O
produce O
the O
highest O
fidelity Metric
scores O
, O
IS Metric
is O
typically O
maximized O
for O
our O
JFT Method
- Method
300 Method
M Method
models Method
when O
the O
truncation O
value O
ranges O
from O
0.5 O
to O
1 O
. O
We O
suspect O
that O
this O
is O
at O
least O
partially O
due O
to O
the O
intra O
- O
class O
variability O
of O
JFT O
- O
300 O
M O
labels O
, O
as O
well O
as O
the O
relative O
complexity O
of O
the O
image O
distribution O
, O
which O
includes O
images O
with O
multiple O
objects O
at O
a O
variety O
of O
scales O
. O
Interestingly O
, O
unlike O
models O
trained O
on O
ImageNet Material
, O
where O
training Task
tends O
to O
collapse O
without O
heavy O
regularization O
( O
Section O
[ O
reference O
] O
) O
, O
the O
models O
trained O
on O
JFT Method
- Method
300 Method
M Method
remain O
stable O
over O
many O
hundreds O
of O
thousands O
of O
iterations O
. O
This O
suggests O
that O
moving O
beyond O
ImageNet Material
to O
larger O
datasets O
may O
partially O
alleviate O
GAN Method
stability O
issues O
. O
The O
improvement O
over O
the O
baseline O
GAN Method
model O
that O
we O
achieve O
on O
this O
dataset O
without O
changes O
to O
the O
underlying O
models O
or O
training Method
and Method
regularization Method
techniques Method
( O
beyond O
expanded O
capacity O
) O
demonstrates O
that O
our O
findings O
extend O
from O
ImageNet Material
to O
datasets O
with O
scale O
and O
complexity O
thus O
far O
unprecedented O
for O
generative Method
models Method
of Method
images Method
. O
section O
: O
Conclusion O
We O
have O
demonstrated O
that O
Generative Method
Adversarial Method
Networks Method
trained O
to O
model O
natural O
images O
of O
multiple O
categories O
highly O
benefit O
from O
scaling O
up O
, O
both O
in O
terms O
of O
fidelity Metric
and O
variety O
of O
the O
generated O
samples O
. O
As O
a O
result O
, O
our O
models O
set O
a O
new O
level O
of O
performance O
among O
ImageNet Material
GAN Method
models O
, O
improving O
on O
the O
state O
of O
the O
art O
by O
a O
large O
margin O
. O
We O
have O
also O
presented O
an O
analysis O
of O
the O
training Task
behavior Task
of O
large Method
scale Method
GANs Method
, O
characterized O
their O
stability Metric
in O
terms O
of O
the O
singular O
values O
of O
their O
weights O
, O
and O
discussed O
the O
interplay O
between O
stability Metric
and O
performance O
. O
subsubsection O
: O
Acknowledgments O
We O
would O
like O
to O
thank O
Kai O
Arulkumaran O
, O
Matthias O
Bauer O
, O
Peter O
Buchlovsky O
, O
Jeffrey O
Defauw O
, O
Sander O
Dieleman O
, O
Ian O
Goodfellow O
, O
Ariel O
Gordon O
, O
Karol O
Gregor O
, O
Dominik O
Grewe O
, O
Chris O
Jones O
, O
Jacob O
Menick O
, O
Augustus O
Odena O
, O
Suman O
Ravuri O
, O
Ali O
Razavi O
, O
Mihaela O
Rosca O
, O
and O
Jeff O
Stanway O
. O
bibliography O
: O
References O
section O
: O
Additional O
Samples O
, O
Interpolations O
, O
and O
Nearest Method
Neighbors Method
from O
ImageNet Material
models O
section O
: O
Architectural O
details O
In O
the O
BigGAN Method
model O
( O
Figure O
[ O
reference O
] O
) O
, O
we O
use O
the O
ResNet O
he2016resnets O
GAN Method
architecture O
of O
zhang2018sagan Method
, O
which O
is O
identical O
to O
that O
used O
by O
miyato2018spectral O
, O
but O
with O
the O
channel O
pattern O
in O
D O
modified O
so O
that O
the O
number O
of O
filters O
in O
the O
first O
convolutional Method
layer Method
of O
each O
block O
is O
equal O
to O
the O
number O
of O
output O
filters O
( O
rather O
than O
the O
number O
of O
input O
filters O
, O
as O
in O
miyato2018spectral O
, O
gulrajani2017improved O
) O
. O
We O
use O
a O
single O
shared Method
class Method
embedding Method
in O
G Method
, O
and O
skip O
connections O
for O
the O
latent O
vector O
( O
skip O
- O
) O
. O
In O
particular O
, O
we O
employ O
hierarchical O
latent O
spaces O
, O
so O
that O
the O
latent O
vector O
is O
split O
along O
its O
channel O
dimension O
into O
chunks O
of O
equal O
size O
( O
- O
D O
in O
our O
case O
) O
, O
and O
each O
chunk O
is O
concatenated O
to O
the O
shared Method
class Method
embedding Method
and O
passed O
to O
a O
corresponding O
residual O
block O
as O
a O
conditioning O
vector O
. O
The O
conditioning O
of O
each O
block O
is O
linearly O
projected O
to O
produce O
per O
- O
sample O
gains O
and O
biases O
for O
the O
BatchNorm Method
layers O
of O
the O
block O
. O
The O
bias O
projections O
are O
zero O
- O
centered O
, O
while O
the O
gain O
projections O
are O
centered O
at O
. O
Since O
the O
number O
of O
residual O
blocks O
depends O
on O
the O
image O
resolution O
, O
the O
full O
dimensionality O
of O
is O
120 O
for O
, O
140 O
for O
, O
and O
160 O
for O
images O
. O
The O
BigGAN Method
- Method
deep Method
model Method
( O
Figure O
[ O
reference O
] O
) O
differs O
from O
BigGAN Method
in O
several O
aspects O
. O
It O
uses O
a O
simpler O
variant O
of O
skip Method
- Method
conditioning Method
: O
instead O
of O
first O
splitting O
into O
chunks O
, O
we O
concatenate O
the O
entire O
with O
the O
class Method
embedding Method
, O
and O
pass O
the O
resulting O
vector O
to O
each O
residual O
block O
through O
skip O
connections O
. O
BigGAN Method
- O
deep O
is O
based O
on O
residual Method
blocks Method
with O
bottlenecks Method
he2016resnets O
, O
which O
incorporate O
two O
additional O
convolutions Method
: O
the O
first O
reduces O
the O
number O
of O
channels O
by O
a O
factor O
of O
before O
the O
more O
expensive O
convolutions Method
; O
the O
second O
produces O
the O
required O
number O
of O
output O
channels O
. O
While O
BigGAN Method
relies O
on O
convolutions Method
in O
the O
skip O
connections O
whenever O
the O
number O
of O
channels O
needs O
to O
change O
, O
in O
BigGAN Method
- O
deep O
we O
use O
a O
different O
strategy O
aimed O
at O
preserving O
identity O
throughout O
the O
skip O
connections O
. O
In O
G Method
, O
where O
the O
number O
of O
channels O
needs O
to O
be O
reduced O
, O
we O
simply O
retain O
the O
first O
group O
of O
channels O
and O
drop O
the O
rest O
to O
produce O
the O
required O
number O
of O
channels O
. O
In O
D O
, O
where O
the O
number O
of O
channels O
should O
be O
increased O
, O
we O
pass O
the O
input O
channels O
unperturbed O
, O
and O
concatenate O
them O
with O
the O
remaining O
channels O
produced O
by O
a O
convolution Method
. O
As O
far O
as O
the O
network Method
configuration Method
is O
concerned O
, O
the O
discriminator Method
is O
an O
exact O
reflection O
of O
the O
generator Method
. O
There O
are O
two O
blocks O
at O
each O
resolution O
( O
BigGAN Method
uses O
one O
) O
, O
and O
as O
a O
result O
BigGAN Method
- O
deep O
is O
four O
times O
deeper O
than O
BigGAN Method
. O
Despite O
their O
increased O
depth O
, O
the O
BigGAN Method
- O
deep O
models O
have O
significantly O
fewer O
parameters O
mainly O
due O
to O
the O
bottleneck O
structure O
of O
their O
residual O
blocks O
. O
For O
example O
, O
the O
BigGAN Method
- O
deep O
G Method
and O
D O
have O
50.4 O
M O
and O
34.6 O
M O
parameters O
respectively O
, O
while O
the O
corresponding O
original O
BigGAN Method
models O
have O
70.4 O
M O
and O
88.0 O
M O
parameters O
. O
All O
BigGAN Method
- O
deep O
models O
use O
attention O
at O
resolution O
, O
channel O
width O
multiplier O
, O
and O
. O
.4 O
z∈R120∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
20128 O
) O
⁢×4416chResBlock O
up O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
up O
→⁢8ch⁢4chResBlock O
up O
→⁢4ch⁢2chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
up O
→⁢2ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×1281283ResBlock O
down O
→⁢ch⁢2chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢2ch⁢4chResBlock O
down O
→⁢4ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global O
sum O
poolingEmbed Method
( Method
y Method
) Method
⋅h Method
+ O
( O
linear O
→ O
1 O
) O
.4 O
z∈R140∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
20128 O
) O
⁢×4416chResBlock O
up O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
up O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢4chResBlock O
up O
→⁢4ch⁢2chNon O
- O
Local O
Block O
( O
×128128 O
) O
ResBlock O
up O
→⁢2ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×2562563ResBlock O
down O
→⁢ch⁢2chResBlock O
down O
→⁢2ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢4ch⁢8chResBlock O
down O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global Method
sum Method
poolingEmbed Method
( Method
y Method
) Method
⋅h Method
+ O
( O
linear O
→ O
1 O
) O
.4 O
z∈R160∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
20128 O
) O
⁢×4416chResBlock O
up O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
up O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
up O
→⁢4ch⁢2chResBlock O
up O
→⁢2ch⁢chResBlock O
up O
→⁢ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×5125123ResBlock O
down O
→⁢ch⁢chResBlock O
down O
→⁢ch⁢2chResBlock O
down O
→⁢2ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢4ch⁢8chResBlock O
down O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global O
sum O
poolingEmbed O
( O
y O
) O
⋅h O
+ O
( O
linear O
→ O
1 O
) O
.4 O
z∈R128∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
128128 O
) O
⁢×4416chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢4chResBlock O
→⁢4ch⁢4chResBlock O
up O
→⁢4ch⁢2chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
→⁢2ch⁢2chResBlock O
up O
→⁢2ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×1281283×33 O
Conv O
→3⁢chResBlock O
down O
→⁢ch⁢2chResBlock O
→⁢2ch⁢2chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢2ch⁢4chResBlock O
→⁢4ch⁢4chResBlock O
down O
→⁢4ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global O
sum O
poolingEmbed O
( O
y O
) O
⋅h O
+ O
( O
linear O
→ O
1 O
) O
.4 O
z∈R128∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
128128 O
) O
⁢×4416chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
→⁢4ch⁢4chResBlock O
up O
→⁢4ch⁢2chResBlock O
→⁢2ch⁢2chResBlock O
up O
→⁢2ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×2562563×33 O
Conv O
→3⁢chResBlock O
down O
→⁢ch⁢2chResBlock O
→⁢2ch⁢2chResBlock O
down O
→⁢2ch⁢4chResBlock O
→⁢4ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢4ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global O
sum O
poolingEmbed O
( O
y O
) O
⋅h O
+ O
( O
linear O
→ O
1 O
) O
.4 O
z∈R128∼⁢N O
( O
0 O
, O
I O
) O
Embed O
( O
y O
) O
∈R128Linear O
→ O
(+ O
128128 O
) O
⁢×4416chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
up O
→⁢16ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
up O
→⁢8ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
→⁢4ch⁢4chResBlock O
up O
→⁢4ch⁢2chResBlock O
→⁢2ch⁢2chResBlock O
up O
→⁢2ch⁢chResBlock O
→⁢ch⁢chResBlock O
up O
→⁢ch⁢chBN O
, O
ReLU O
, O
×33 O
Conv O
→⁢ch3Tanh O
.4 O
RGB Material
image Material
∈xR×5125123×33 O
Conv O
→3⁢chResBlock O
down O
→⁢ch⁢chResBlock O
→⁢ch⁢chResBlock O
down O
→⁢ch⁢2chResBlock O
→⁢2ch⁢2chResBlock O
down O
→⁢2ch⁢4chResBlock O
→⁢4ch⁢4chNon O
- O
Local O
Block O
( O
×6464 O
) O
ResBlock O
down O
→⁢4ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢8chResBlock O
→⁢8ch⁢8chResBlock O
down O
→⁢8ch⁢16chResBlock O
→⁢16ch⁢16chResBlock O
down O
→⁢16ch⁢16chResBlock O
→⁢16ch⁢16chReLU O
, O
Global O
sum O
poolingEmbed O
( O
y O
) O
⋅h O
+ O
( O
linear O
→ O
1 O
) O
section O
: O
Experimental O
Details O
Our O
basic O
setup O
follows O
SA Method
- Method
GAN Method
zhang2018sagan O
, O
and O
is O
implemented O
in O
TensorFlow Method
Abadi2016tf O
. O
We O
employ O
the O
architectures O
detailed O
in O
Appendix O
[ O
reference O
] O
, O
with O
non O
- O
local O
blocks O
inserted O
at O
a O
single O
stage O
in O
each O
network O
. O
Both O
G Method
and O
D O
networks O
are O
initialized O
with O
Orthogonal Method
Initialization Method
saxe2014ortho O
. O
We O
use O
Adam Method
optimizer Method
kingma2014adam O
with O
and O
and O
a O
constant O
learning Metric
rate Metric
. O
For O
BigGAN Method
models O
at O
all O
resolutions O
, O
we O
use O
in O
D O
and O
in O
G Method
. O
For O
BigGAN Method
- O
deep O
, O
we O
use O
the O
learning Metric
rate Metric
of O
in O
D O
and O
in O
G Method
for O
models O
, O
and O
in O
both O
D O
and O
G Method
for O
and Method
models Method
. O
We O
experimented O
with O
the O
number O
of O
D O
steps O
per O
G Method
step O
( O
varying O
it O
from O
to O
) O
and O
found O
that O
two O
D O
steps O
per O
G Method
step O
gave O
the O
best O
results O
. O
We O
use O
an O
exponential Method
moving Method
average Method
of O
the O
weights O
of O
G Method
at O
sampling O
time O
, O
with O
a O
decay O
rate O
set O
to O
0.9999 O
. O
We O
employ O
cross Method
- Method
replica Method
BatchNorm Method
ioffe2015batchnorm O
in O
G Method
, O
where O
batch O
statistics O
are O
aggregated O
across O
all O
devices O
, O
rather O
than O
a O
single O
device O
as O
in O
standard O
implementations O
. O
Spectral Method
Normalization Method
miyato2018spectral O
is O
used O
in O
both O
G Method
and O
D O
, O
following O
SA Method
- Method
GAN Method
zhang2018sagan O
. O
We O
train O
on O
a O
Google Material
TPU Material
v3 Material
Pod Material
, O
with O
the O
number O
of O
cores O
proportional O
to O
the O
resolution O
: O
128 O
for O
128 O
128 O
, O
256 O
for O
256 O
256 O
, O
and O
512 O
for O
512 O
512 O
. O
Training Task
takes O
between O
24 O
and O
48 O
hours O
for O
most O
models O
. O
We O
increase O
from O
the O
default O
to O
in O
BatchNorm Method
and O
Spectral Method
Norm Method
to O
mollify Task
low Task
- Task
precision Task
numerical Task
issues Task
. O
We O
preprocess O
data O
by O
cropping O
along O
the O
long O
edge O
and O
rescaling O
to O
a O
given O
resolution O
with O
area O
resampling O
. O
subsection O
: O
BatchNorm Method
Statistics O
and O
Sampling Task
The O
default O
behavior O
with O
batch Method
normalized Method
classifier Method
networks Method
is O
to O
use O
a O
running O
average O
of O
the O
activation O
moments O
at O
test O
time O
. O
Previous O
works O
radford2016dcgan O
have O
instead O
used O
batch O
statistics O
when O
sampling O
images O
. O
While O
this O
is O
not O
technically O
an O
invalid O
way O
to O
sample O
, O
it O
means O
that O
results O
are O
dependent O
on O
the O
test O
batch O
size O
( O
and O
how O
many O
devices O
it O
is O
split O
across O
) O
, O
and O
further O
complicates O
reproducibility O
. O
We O
find O
that O
this O
detail O
is O
extremely O
important O
, O
with O
changes O
in O
test O
batch O
size O
producing O
drastic O
changes O
in O
performance O
. O
This O
is O
further O
exacerbated O
when O
one O
uses O
exponential O
moving O
averages O
of O
G Method
’s O
weights O
for O
sampling Task
, O
as O
the O
BatchNorm Method
running O
averages O
are O
computed O
with O
non O
- O
averaged O
weights O
and O
are O
poor O
estimates O
of O
the O
activation O
statistics O
for O
the O
averaged O
weights O
. O
To O
counteract O
both O
these O
issues O
, O
we O
employ O
“ O
standing O
statistics O
, O
” O
where O
we O
compute O
activation O
statistics O
at O
sampling O
time O
by O
running O
the O
G Method
through O
multiple O
forward O
passes O
( O
typically O
100 O
) O
each O
with O
different O
batches O
of O
random O
noise O
, O
and O
storing O
means O
and O
variances O
aggregated O
across O
all O
forward O
passes O
. O
Analogous O
to O
using O
running O
statistics O
, O
this O
results O
in O
G Method
’s O
outputs O
becoming O
invariant O
to O
batch O
size O
and O
the O
number O
of O
devices O
, O
even O
when O
producing O
a O
single O
sample O
. O
subsection O
: O
CIFAR Task
- Task
10 Task
We O
run O
our O
networks O
on O
CIFAR Task
- Task
10 Task
krizhevsky2009cifar Task
using O
the O
settings O
from O
Table O
[ O
reference O
] O
, O
row O
8 O
, O
and O
achieve O
an O
IS Metric
of O
9.22 O
and O
an O
FID Metric
of O
14.73 O
without O
truncation O
. O
subsection O
: O
Inception Metric
Scores Metric
of O
ImageNet Material
Images O
We O
compute O
the O
IS Metric
for O
both O
the O
training O
and O
validation Material
sets Material
of O
ImageNet Material
. O
At O
128 O
128 O
the O
training O
data O
has O
an O
IS Metric
of O
233 O
, O
and O
the O
validation O
data O
has O
an O
IS Metric
of O
166 O
. O
At O
256 O
256 O
the O
training O
data O
has O
an O
IS Metric
of O
377 O
, O
and O
the O
validation O
data O
has O
an O
IS Metric
of O
234 O
. O
At O
512 O
512 O
the O
training O
data O
has O
an O
IS Metric
of O
348 O
, O
and O
the O
validation O
data O
has O
an O
IS Metric
of O
241 O
. O
The O
discrepancy O
between O
training Metric
and Metric
validation Metric
scores Metric
is O
due O
to O
the O
Inception Method
classifier Method
having O
been O
trained O
on O
the O
training O
data O
, O
resulting O
in O
high O
- O
confidence O
outputs O
that O
are O
preferred O
by O
the O
Inception Metric
Score Metric
. O
section O
: O
Additional O
Plots O
section O
: O
Choosing O
Latent O
Spaces O
While O
most O
previous O
work O
has O
employed O
or O
as O
the O
prior O
for O
( O
the O
noise O
input O
to O
G Method
) O
, O
we O
are O
free O
to O
choose O
any O
latent O
distribution O
from O
which O
we O
can O
sample O
. O
We O
explore O
the O
choice O
of O
latents O
by O
considering O
an O
array O
of O
possible O
designs O
, O
described O
below O
. O
For O
each O
latent O
, O
we O
provide O
the O
intuition O
behind O
its O
design O
and O
briefly O
describe O
how O
it O
performs O
when O
used O
as O
a O
drop Task
- Task
in Task
replacement Task
for O
in O
an O
SA O
- O
GAN Method
baseline O
. O
As O
the O
Truncation Method
Trick Method
proved O
more O
beneficial O
than O
switching O
to O
any O
of O
these O
latents O
, O
we O
do O
not O
perform O
a O
full O
ablation Task
study Task
, O
and O
employ O
for O
our O
main O
results O
to O
take O
full O
advantage O
of O
truncation O
. O
The O
two O
latents O
which O
we O
find O
to O
work O
best O
without O
truncation O
are O
Bernoulli Method
and O
Censored O
Normal O
, O
both O
of O
which O
improve O
speed O
of O
training Task
and O
lightly O
improve O
final O
performance O
, O
but O
are O
less O
amenable O
to O
truncation O
. O
We O
also O
ablate O
the O
choice O
of O
latent O
space O
dimensonality O
( O
which O
by O
default O
is O
) O
, O
finding O
that O
we O
are O
able O
to O
successfully O
train O
with O
latent O
dimensions O
as O
low O
as O
, O
and O
that O
with O
we O
see O
a O
minimal O
drop O
in O
performance O
. O
While O
this O
is O
substantially O
smaller O
than O
many O
previous O
works O
, O
direct O
comparison O
to O
single Method
- Method
class Method
networks Method
( O
such O
as O
those O
in O
karras2018progan O
, O
which O
employ O
a O
latent O
space O
on O
a O
highly O
constrained O
dataset O
with O
30 O
, O
000 O
images O
) O
is O
improper O
, O
as O
our O
networks O
have O
additional O
class O
information O
provided O
as O
input O
. O
subsection O
: O
Latents O
. O
A O
standard O
choice O
of O
the O
latent O
space O
which O
we O
use O
in O
the O
main O
experiments O
. O
. O
Another O
standard O
choice O
; O
we O
find O
that O
it O
performs O
similarly O
to O
. O
Bernoulli Method
. O
A O
discrete O
latent O
might O
reflect O
our O
prior O
that O
underlying O
factors O
of O
variation O
in O
natural O
images O
are O
not O
continuous O
, O
but O
discrete O
( O
one O
feature O
is O
present O
, O
another O
is O
not O
) O
. O
This O
latent O
outperforms O
( O
in O
terms O
of O
IS Metric
) O
by O
8 O
% O
and O
requires O
60 O
% O
fewer O
iterations O
. O
, O
also O
called O
Censored Method
Normal Method
. O
This O
latent O
is O
designed O
to O
introduce O
sparsity O
in O
the O
latent O
space O
( O
reflecting O
our O
prior O
that O
certain O
latent O
features O
are O
sometimes O
present O
and O
sometimes O
not O
) O
, O
but O
also O
allow O
those O
latents O
to O
vary O
continuously O
, O
expressing O
different O
degrees O
of O
intensity O
for O
latents O
which O
are O
active O
. O
This O
latent O
outperforms O
( O
in O
terms O
of O
IS Metric
) O
by O
15 O
- O
20 O
% O
and O
tends O
to O
require O
fewer O
iterations O
. O
Bernoulli Method
. O
This O
latent O
is O
designed O
to O
be O
discrete O
, O
but O
not O
sparse O
( O
as O
the O
network O
can O
learn O
to O
activate O
in O
response O
to O
negative O
inputs O
) O
. O
This O
latent O
performs O
near O
- O
identically O
to O
. O
Independent O
Categorical O
in O
, O
with O
equal O
probability O
. O
This O
distribution O
is O
chosen O
to O
be O
discrete O
and O
have O
sparsity O
, O
but O
also O
to O
allow O
latents O
to O
take O
on O
both O
positive O
and O
negative O
values O
. O
This O
latent O
performs O
near O
- O
identically O
to O
. O
multiplied O
by O
Bernoulli Method
. O
This O
distribution O
is O
chosen O
to O
have O
continuous O
latent O
factors O
which O
are O
also O
sparse O
( O
with O
a O
peak O
at O
zero O
) O
, O
similar O
to O
Censored Method
Normal Method
but O
not O
constrained O
to O
be O
positive O
. O
This O
latent O
performs O
near O
- O
identically O
to O
. O
Concatenating Method
and O
Bernoulli Method
, O
each O
taking O
half O
of O
the O
latent O
dimensions O
. O
This O
is O
inspired O
by O
chen2016infogan O
, O
and O
is O
chosen O
to O
allow O
some O
factors O
of O
variation O
to O
be O
discrete O
, O
while O
others O
are O
continuous O
. O
This O
latent O
outperforms O
by O
around O
5 O
% O
. O
Variance Method
annealing Method
: O
we O
sample O
from O
, O
where O
is O
allowed O
to O
vary O
over O
training O
. O
We O
compared O
a O
variety O
of O
piecewise Method
schedules Method
and O
found O
that O
starting O
with O
and O
annealing O
towards O
over O
the O
course O
of O
training O
mildly O
improved O
performance O
. O
The O
space O
of O
possible O
variance O
schedules O
is O
large O
, O
and O
we O
did O
not O
explore O
it O
in O
depth O
– O
we O
suspect O
that O
a O
more O
principled O
or O
better O
- O
tuned O
schedule O
could O
more O
strongly O
impact O
performance O
. O
Per O
- O
sample O
variable O
variance O
: O
, O
where O
independently O
for O
each O
sample O
in O
a O
batch O
, O
and O
are O
hyperparameters O
. O
This O
distribution O
was O
chosen O
to O
try O
and O
improve O
amenability O
to O
the O
Truncation O
Trick O
by O
feeding O
the O
network O
noise O
samples O
with O
non O
- O
constant O
variance O
. O
This O
did O
not O
appear O
to O
affect O
performance O
, O
but O
we O
did O
not O
explore O
it O
in O
depth O
. O
One O
might O
also O
consider O
scheduling Task
, O
similar O
to O
variance Method
annealing Method
. O
section O
: O
Monitored O
Training O
Statistics O
section O
: O
Additional O
Discussion O
: O
Stability Task
and O
Collapse Task
In O
this O
section O
, O
we O
present O
and O
discuss O
additional O
investigations O
into O
the O
stability O
of O
our O
models O
, O
expanding O
upon O
the O
discussion O
in O
Section O
[ O
reference O
] O
. O
subsection O
: O
Intervening O
Before O
Collapse O
The O
symptoms O
of O
collapse O
are O
sharp O
and O
sudden O
, O
with O
sample Metric
quality Metric
dropping O
from O
its O
peak O
to O
its O
lowest O
value O
over O
the O
course O
of O
a O
few O
hundred O
iterations O
. O
We O
can O
detect O
this O
collapse O
when O
the O
singular O
values O
in O
G Method
explode O
, O
but O
while O
the O
( O
unnormalized O
) O
singular O
values O
grow O
throughout O
training O
, O
there O
is O
no O
consistent O
threshold O
at O
which O
collapse O
occurs O
. O
This O
raises O
the O
question O
of O
whether O
it O
is O
possible O
to O
prevent O
or O
delay O
collapse O
by O
taking O
a O
model O
checkpoint O
several O
thousand O
iterations O
before O
collapse O
, O
and O
continuing O
training O
with O
some O
hyperparameters O
modified O
( O
e.g. O
, O
the O
learning Metric
rate Metric
) O
. O
We O
conducted O
a O
range O
of O
intervention O
experiments O
wherein O
we O
took O
checkpoints O
of O
a O
collapsed O
model O
ten O
or O
twenty O
thousand O
iterations O
before O
collapse O
, O
changed O
some O
aspect O
of O
the O
training O
setup O
, O
then O
observed O
whether O
collapse O
occurred O
, O
when O
it O
occurred O
relative O
to O
the O
original O
collapse O
, O
and O
the O
final O
performance O
attained O
at O
collapse O
. O
We O
found O
that O
increasing O
the O
learning Metric
rates Metric
( O
relative O
to O
their O
initial O
values O
) O
in O
either O
G Method
or O
D O
, O
or O
both O
G Method
and O
D O
, O
led O
to O
immediate O
collapse O
. O
This O
occurred O
even O
when O
doubling O
the O
learning Metric
rates Metric
from O
in O
D O
and O
in O
G Method
, O
to O
in O
D O
and O
in O
G Method
, O
a O
setting O
which O
is O
not O
normally O
unstable O
when O
used O
as O
the O
initial O
learning O
rates O
. O
We O
also O
tried O
changing O
the O
momentum O
terms O
( O
Adam O
’s O
and O
) O
, O
or O
resetting O
the O
momentum O
vectors O
to O
zero O
, O
but O
this O
tended O
to O
either O
make O
no O
difference O
or O
, O
when O
increasing O
the O
momentum O
, O
cause O
immediate O
collapse O
. O
We O
found O
that O
decreasing O
the O
learning Metric
rate Metric
in O
G Method
, O
but O
keeping O
the O
learning Metric
rate Metric
in O
D O
unchanged O
could O
delay O
collapse O
( O
in O
some O
cases O
by O
over O
one O
hundred O
thousand O
iterations O
) O
, O
but O
also O
crippled O
training O
— O
once O
the O
learning Metric
rate Metric
in O
G Method
was O
decayed O
, O
performance O
either O
stayed O
constant O
or O
slowly O
decayed O
. O
Conversely O
, O
reducing O
the O
learning Metric
rate Metric
in O
D O
while O
keeping O
G Method
’s O
learning Metric
rate Metric
led O
to O
immediate O
collapse O
. O
We O
hypothesize O
that O
this O
is O
because O
of O
the O
need O
for O
D O
to O
remain O
optimal O
throughout O
training Task
— O
if O
its O
learning Metric
rate Metric
is O
reduced O
, O
it O
can O
no O
longer O
“ O
keep O
up O
” O
with O
G Method
, O
and O
training O
collapses O
. O
With O
this O
in O
mind O
, O
we O
also O
tried O
increasing O
the O
number O
of O
D O
steps O
per O
G Method
step O
, O
but O
this O
either O
had O
no O
effect O
, O
or O
delayed O
collapse O
at O
the O
cost O
of O
crippling O
training Task
( O
similar O
to O
decaying O
G Method
’s O
learning Metric
rate Metric
) O
. O
To O
further O
illuminate O
these O
dynamics O
, O
we O
construct O
two O
additional O
intervention O
experiments O
, O
one O
where O
we O
freeze O
G Method
before O
collapse O
( O
by O
ceasing O
all O
parameter O
updates O
) O
and O
observe O
whether O
D O
remains O
stable O
, O
and O
the O
reverse O
, O
where O
we O
freeze O
D O
before O
collapse O
and O
observe O
whether O
G Method
remains O
stable O
. O
We O
find O
that O
when O
G Method
is O
frozen O
, O
D O
remains O
stable O
, O
and O
slowly O
reduces O
both O
components O
of O
its O
loss O
towards O
zero O
. O
However O
, O
when O
D O
is O
frozen O
, O
G Method
immediately O
and O
dramatically O
collapses O
, O
maxing O
out O
D O
’s O
loss O
to O
values O
upwards O
of O
300 O
, O
compared O
to O
the O
normal O
range O
of O
0 O
to O
3 O
. O
This O
leads O
to O
two O
conclusions O
: O
first O
, O
as O
has O
been O
noted O
in O
previous O
works O
miyato2018spectral O
, O
gulrajani2017improved O
, O
zhang2018sagan O
, O
D O
must O
remain O
optimal O
with O
respect O
to O
G Method
both O
for O
stability Task
and O
to O
provide O
useful O
gradient O
information O
. O
The O
consequence O
of O
G Method
being O
allowed O
to O
win O
the O
game O
is O
a O
complete O
breakdown O
of O
the O
training Method
process Method
, O
regardless O
of O
G Method
’s O
conditioning O
or O
optimization Method
settings Method
. O
Second O
, O
favoring O
D O
over O
G Method
( O
either O
by O
training O
it O
with O
a O
larger O
learning Metric
rate Metric
, O
or O
for O
more O
steps O
) O
is O
insufficient O
to O
ensure O
stability O
even O
if O
D O
is O
well O
- O
conditioned O
. O
This O
suggests O
either O
that O
in O
practice O
, O
an O
optimal O
D O
is O
necessary O
but O
insufficient O
for O
training O
stability O
, O
or O
that O
some O
aspect O
of O
the O
system O
results O
in O
D O
not O
being O
trained O
towards O
optimality O
. O
With O
the O
latter O
possibility O
in O
mind O
, O
we O
take O
a O
closer O
look O
at O
the O
noise O
in O
D O
’s O
spectra O
in O
the O
following O
section O
. O
subsection O
: O
Spikes O
in O
the O
Discriminator O
’s O
Spectra O
If O
some O
element O
of O
D Method
’s Method
training Method
process Method
results O
in O
undesirable O
dynamics O
, O
it O
follows O
that O
the O
behavior O
of O
D O
’s O
spectra O
may O
hold O
clues O
as O
to O
what O
that O
element O
is O
. O
The O
top O
three O
singular O
values O
of O
D O
differ O
from O
G Method
’s O
in O
that O
they O
have O
a O
large O
noise O
component O
, O
tend O
to O
grow O
throughout O
training O
but O
only O
show O
a O
small O
response O
to O
collapse O
, O
and O
the O
ratio O
of O
the O
first O
two O
singular O
values O
tends O
to O
be O
centered O
around O
one O
, O
suggesting O
that O
the O
spectra O
of O
D O
have O
a O
slow O
decay O
. O
When O
viewed O
up O
close O
( O
Figure O
[ O
reference O
] O
) O
, O
the O
noise O
spikes O
resemble O
an O
impulse Method
response Method
: O
at O
each O
spike O
, O
the O
spectra O
jump O
upwards O
, O
then O
slowly O
decrease O
, O
with O
some O
oscillation O
. O
One O
possible O
explanation O
is O
that O
this O
behavior O
is O
a O
consequence O
of O
D O
memorizing O
the O
training O
data O
, O
as O
suggested O
by O
experiments O
in O
Section O
[ O
reference O
] O
. O
As O
it O
approaches O
perfect O
memorization Method
, O
it O
receives O
less O
and O
less O
signal O
from O
real O
data O
, O
as O
both O
the O
original O
GAN Method
loss O
and O
the O
hinge Method
loss Method
provide O
zero O
gradients O
when O
D O
outputs O
a O
confident O
and O
correct O
prediction O
for O
a O
given O
example O
. O
If O
the O
gradient O
signal O
from O
real O
data O
attenuates O
to O
zero O
, O
this O
can O
result O
in O
D O
eventually O
becoming O
biased O
due O
to O
exclusively O
received O
gradients O
that O
encourage O
its O
outputs O
to O
be O
negative O
. O
If O
this O
bias O
passes O
a O
certain O
threshold O
, O
D O
will O
eventually O
misclassify O
a O
large O
number O
of O
real O
examples O
and O
receive O
a O
large O
gradient O
encouraging O
positive O
outputs O
, O
resulting O
in O
the O
observed O
impulse O
responses O
. O
This O
argument O
suggests O
several O
fixes O
. O
First O
, O
one O
might O
consider O
an O
unbounded O
loss O
( O
such O
as O
the O
Wasserstein Metric
loss Metric
arjovsky2017wgan O
) O
which O
would O
not O
suffer O
this O
gradient Method
attentuation Method
. O
We O
found O
that O
even O
with O
gradient O
penalties O
and O
brief O
re Method
- Method
tuning Method
of O
optimizer Method
hyperparameters Method
, O
our O
models O
did O
not O
stably O
train O
for O
more O
than O
a O
few O
thousand O
iterations O
with O
this O
loss O
. O
We O
instead O
explored O
changing O
the O
margin O
of O
the O
hinge Method
loss Method
as O
a O
partial O
compromise O
: O
for O
a O
given O
model O
and O
minibatch O
of O
data O
, O
increasing O
the O
margin O
will O
result O
in O
more O
examples O
falling O
within O
the O
margin O
, O
and O
thus O
contributing O
to O
the O
loss O
. O
. O
Training Method
with O
a O
smaller O
margin O
( O
by O
a O
factor O
of O
2 O
) O
measurably O
reduces O
performance O
, O
but O
training O
with O
a O
larger O
margin O
( O
by O
up O
to O
a O
factor O
of O
3 O
) O
does O
not O
prevent O
collapse O
or O
reduce O
the O
noise O
in O
D O
’s O
spectra O
. O
Increasing O
the O
margin O
beyond O
3 O
results O
in O
unstable Task
training Task
similar O
to O
using O
the O
Wasserstein Metric
loss Metric
. O
Finally O
, O
the O
memorization Method
argument Method
might O
suggest O
that O
using O
a O
smaller O
D O
or O
using O
dropout O
in O
D O
would O
improve O
training Task
by O
reducing O
its O
capacity O
to O
memorize O
, O
but O
in O
practice O
this O
degrades O
training Task
. O
section O
: O
Negative O
Results O
We O
explored O
a O
range O
of O
novel O
and O
existing O
techniques O
which O
ended O
up O
degrading O
or O
otherwise O
not O
affecting O
performance O
in O
our O
setting O
. O
We O
report O
them O
here O
; O
our O
evaluations O
for O
this O
section O
are O
not O
as O
thorough O
as O
those O
for O
the O
main O
architectural O
choices O
. O
Our O
intention O
in O
reporting O
these O
results O
is O
to O
save O
time O
for O
future O
work O
, O
and O
to O
give O
a O
more O
complete O
picture O
of O
our O
attempts O
to O
improve O
performance O
or O
stability Metric
. O
We O
note O
, O
however O
, O
that O
these O
results O
must O
be O
understood O
to O
be O
specific O
to O
the O
particular O
setup O
we O
used O
. O
A O
pitfall O
of O
reporting O
negative O
results O
is O
that O
one O
might O
report O
that O
a O
particular O
technique O
does O
n’t O
work O
, O
when O
the O
reality O
is O
that O
this O
technique O
did O
not O
have O
the O
desired O
effect O
when O
applied O
in O
a O
particular O
way O
to O
a O
particular O
problem O
. O
Drawing O
overly O
general O
conclusions O
might O
close O
off O
potentially O
fruitful O
avenues O
of O
research O
. O
We O
found O
that O
doubling O
the O
depth O
( O
by O
inserting O
an O
additional O
Residual O
block O
after O
every O
up O
- O
or O
down O
- O
sampling O
block O
) O
hampered O
performance O
. O
We O
experimented O
with O
sharing O
class O
embeddings O
between O
both O
G Method
and O
D O
( O
as O
opposed O
to O
just O
within O
G Method
) O
. O
This O
is O
accomplished O
by O
replacing O
D Method
’s Method
class Method
embedding Method
with O
a O
projection Method
from O
G Method
’s O
embeddings O
, O
as O
is O
done O
in O
G Method
’s O
BatchNorm Method
layers O
. O
In O
our O
initial O
experiments O
this O
seemed O
to O
help O
and O
accelerate O
training Task
, O
but O
we O
found O
this O
trick O
scaled O
poorly O
and O
was O
sensitive O
to O
optimization O
hyperparameters O
, O
particularly O
the O
choice O
of O
number O
of O
D O
steps O
per O
G Method
step O
. O
We O
tried O
replacing O
BatchNorm Method
in O
G Method
with O
WeightNorm Method
salimans2016weightnorm O
, O
but O
this O
crippled O
training O
. O
We O
also O
tried O
removing O
BatchNorm Method
and O
only O
having O
Spectral Method
Normalization Method
, O
but O
this O
also O
crippled O
training O
. O
We O
tried O
adding O
BatchNorm Method
to O
D O
( O
both O
class Method
- Method
conditional Method
and O
unconditional O
) O
in O
addition O
to O
Spectral Method
Normalization Method
, O
but O
this O
crippled O
training O
. O
We O
tried O
varying O
the O
choice O
of O
location O
of O
the O
attention O
block O
in O
G Method
and O
D O
( O
and O
inserting O
multiple O
attention O
blocks O
at O
different O
resolutions O
) O
but O
found O
that O
at O
128 O
128 O
there O
was O
no O
noticeable O
benefit O
to O
doing O
so O
, O
and O
compute Metric
and Metric
memory Metric
costs Metric
increased O
substantially O
. O
We O
found O
a O
benefit O
to O
moving O
the O
attention O
block O
up O
one O
stage O
when O
moving O
to O
256 O
256 O
, O
which O
is O
in O
line O
with O
our O
expectations O
given O
the O
increased O
resolution O
. O
We O
tried O
using O
filter O
sizes O
of O
5 O
or O
7 O
instead O
of O
3 O
in O
either O
G Method
or O
D O
or O
both O
. O
We O
found O
that O
having O
a O
filter O
size O
of O
5 O
in O
G Method
only O
provided O
a O
small O
improvement O
over O
the O
baseline O
but O
came O
at O
an O
unjustifiable O
compute Metric
cost Metric
. O
All O
other O
settings O
degraded O
performance O
. O
We O
tried O
varying O
the O
dilation O
for O
convolutional Method
filters Method
in O
both O
G Method
and O
D O
at O
128 O
128 O
, O
but O
found O
that O
even O
a O
small O
amount O
of O
dilation O
in O
either O
network O
degraded O
performance O
. O
We O
tried O
bilinear Method
upsampling Method
in O
G Method
in O
place O
of O
nearest Method
- Method
neighbors Method
upsampling Method
, O
but O
this O
degraded O
performance O
. O
In O
some O
of O
our O
models O
, O
we O
observed O
class Method
- Method
conditional Method
mode O
collapse O
, O
where O
the O
model O
would O
only O
output O
one O
or O
two O
samples O
for O
a O
subset O
of O
classes O
but O
was O
still O
able O
to O
generate O
samples O
for O
all O
other O
classes O
. O
We O
noticed O
that O
the O
collapsed O
classes O
had O
embedings O
which O
had O
become O
very O
large O
relative O
to O
the O
other O
embeddings O
, O
and O
attempted O
to O
ameliorate O
this O
issue O
by O
applying O
weight Method
decay Method
to O
the O
shared Method
embedding Method
only O
. O
We O
found O
that O
small O
amounts O
of O
weight O
decay O
( O
) O
instead O
degraded O
performance O
, O
and O
that O
only O
even O
smaller O
values O
( O
) O
did O
not O
degrade O
performance O
, O
but O
these O
values O
were O
also O
too O
small O
to O
prevent O
the O
class O
vectors O
from O
exploding O
. O
Higher Method
- Method
resolution Method
models Method
appear O
to O
be O
more O
resilient O
to O
this O
problem O
, O
and O
none O
of O
our O
final O
models O
appear O
to O
suffer O
from O
this O
type O
of O
collapse O
. O
We O
experimented O
with O
using O
MLPs Method
instead O
of O
linear Method
projections Method
from O
G Method
’s O
class O
embeddings O
to O
its O
BatchNorm Method
gains O
and O
biases O
, O
but O
did O
not O
find O
any O
benefit O
to O
doing O
so O
. O
We O
also O
experimented O
with O
Spectrally Method
Normalizing Method
these O
MLPs Method
, O
and O
with O
providing O
these O
( O
and O
the O
linear Method
projections Method
) O
with O
a O
bias O
at O
their O
output O
, O
but O
did O
not O
notice O
any O
benefit O
. O
We O
tried O
gradient Method
norm Method
clipping Method
( O
both O
the O
global Method
variant Method
typically O
used O
in O
recurrent Method
networks Method
, O
and O
a O
local Method
version Method
where O
the O
clipping O
value O
is O
determined O
on O
a O
per O
- O
parameter O
basis O
) O
but O
found O
this O
did O
not O
alleviate O
instability O
. O
section O
: O
Hyperparameters O
We O
performed O
various O
hyperparameter O
sweeps O
in O
this O
work O
: O
We O
swept O
the O
Cartesian O
product O
of O
the O
learning O
rates O
for O
each O
network O
through O
[ O
, O
, O
, O
, O
, O
, O
] O
, O
and O
initially O
found O
that O
the O
SA O
- O
GAN Method
settings O
( O
G Method
’s O
learning O
rate O
, O
D O
’s O
learning Metric
rate Metric
) O
were O
optimal O
at O
lower O
batch O
sizes O
; O
we O
did O
not O
repeat O
this O
sweep O
at O
higher O
batch O
sizes O
but O
did O
try O
halving O
and O
doubling O
the O
learning Metric
rate Metric
, O
arriving O
at O
the O
halved O
settings O
used O
for O
our O
experiments O
. O
We O
swept O
the O
R1 O
gradient O
penalty O
strength O
through O
[ O
, O
, O
, O
, O
, O
, O
, O
, O
] O
. O
We O
find O
that O
the O
strength O
of O
the O
penalty O
correlates O
negatively O
with O
performance O
, O
but O
that O
settings O
above O
impart O
training O
stability O
. O
We O
swept O
the O
keep O
probabilities O
for O
DropOut Method
in O
the O
final O
layer O
of O
D O
through O
[ O
, O
, O
, O
, O
, O
] O
. O
We O
find O
that O
DropOut Method
has O
a O
similar O
stabilizing O
effect O
to O
R1 O
but O
also O
degrades O
performance O
. O
We O
swept O
D O
’s O
Adam O
parameter O
through O
[ O
, O
, O
, O
, O
] O
and O
found O
it O
to O
have O
a O
light O
regularization O
effect O
similar O
to O
DropOut Method
, O
but O
not O
to O
significantly O
improve O
results O
. O
Higher O
terms O
in O
either O
network O
crippled O
training O
. O
We O
swept O
the O
strength O
of O
the O
modified O
Orthogonal O
Regularization O
penalty O
in O
G Method
through O
[ O
, O
, O
, O
, O
, O
] O
, O
and O
selected O
. O
