document O
: O
SyncSpecCNN Method
: O
Synchronized Method
Spectral Method
CNN Method
for O
3D Task
Shape Task
Segmentation Task
In O
this O
paper O
, O
we O
study O
the O
problem O
of O
semantic Task
annotation Task
on O
3D Task
models Task
that O
are O
represented O
as O
shape O
graphs O
. O
A O
functional Method
view Method
is O
taken O
to O
represent O
localized O
information O
on O
graphs O
, O
so O
that O
annotations O
such O
as O
part O
segment O
or O
keypoint O
are O
nothing O
but O
0 O
- O
1 O
indicator O
vertex O
functions O
. O
Compared O
with O
images O
that O
are O
2D O
grids O
, O
shape Method
graphs Method
are O
irregular O
and O
nonisomorphic O
data O
structures O
. O
To O
enable O
the O
prediction Task
of Task
vertex Task
functions Task
on O
them O
by O
convolutional Method
neural Method
networks Method
, O
we O
resort O
to O
spectral Method
CNN Method
method Method
that O
enables O
weight Method
sharing Method
by O
parameterizing Method
kernels Method
in O
the O
spectral O
domain O
spanned O
by O
graph Method
laplacian Method
eigenbases Method
. O
Under O
this O
setting O
, O
our O
network O
, O
named O
SyncSpecCNN Method
, O
strive O
to O
overcome O
two O
key O
challenges O
: O
how O
to O
share O
coefficients O
and O
conduct O
multi Task
- Task
scale Task
analysis Task
in O
different O
parts O
of O
the O
graph O
for O
a O
single O
shape O
, O
and O
how O
to O
share O
information O
across O
related O
but O
different O
shapes O
that O
may O
be O
represented O
by O
very O
different O
graphs O
. O
Towards O
these O
goals O
, O
we O
introduce O
a O
spectral Method
parameterization Method
of Method
dilated Method
convolutional Method
kernels Method
and O
a O
spectral Method
transformer Method
network Method
. O
Experimentally O
we O
tested O
our O
SyncSpecCNN Method
on O
various O
tasks O
, O
including O
3D Task
shape Task
part Task
segmentation Task
and O
3D Task
keypoint Task
prediction Task
. O
State O
- O
of O
- O
the O
- O
art O
performance O
has O
been O
achieved O
on O
all O
benchmark O
datasets O
. O
section O
: O
Introduction O
As O
has O
already O
happened O
in O
the O
image O
domain O
, O
the O
wide O
availability O
of O
3D Method
models Method
brings O
with O
it O
the O
need O
to O
associate O
semantic O
information O
with O
the O
3D O
data O
. O
In O
this O
work O
we O
focus O
on O
the O
problem O
of O
annotating Task
3D Task
models Task
represented O
by O
2D O
meshes O
with O
part O
information O
. O
Understanding O
of O
the O
parts O
of O
an O
object O
( O
e.g. O
, O
the O
back O
, O
seat O
and O
legs O
of O
a O
chair O
) O
is O
essential O
to O
its O
geometric O
structure O
, O
to O
its O
style O
, O
and O
to O
its O
function O
. O
There O
has O
been O
significant O
recent O
progress O
in O
the O
large Task
scale Task
part Task
annotation Task
of Task
3D Task
models Task
( O
e.g. O
, O
for O
a O
subset O
of O
the O
ShapeNet Method
models Method
) O
– O
our O
aim O
here O
is O
to O
leverage O
this O
rich O
data O
set O
so O
as O
to O
infer O
parts O
of O
new O
3D Method
object Method
models Method
. O
Our O
techniques O
can O
also O
be O
used O
to O
infer O
keypoints O
and O
other O
substructures O
within O
3D Method
models Method
. O
It O
is O
not O
straightforward O
to O
apply O
traditional O
deep Method
learning Method
approaches Method
to O
3D Task
models Task
because O
a O
mesh Method
representation Method
can O
be O
combinatorially O
irregular O
and O
does O
not O
permit O
the O
optimizations O
exploited O
by O
convolutional Method
approaches Method
, O
such O
as O
weight Method
sharing Method
, O
which O
depend O
on O
regular O
grid O
structures O
. O
In O
this O
paper O
we O
take O
a O
functional Method
approach Method
to O
represent O
information O
about O
shapes O
, O
starting O
with O
the O
observation O
that O
a O
shape O
part O
is O
itself O
nothing O
but O
a O
0 O
- O
1 O
indicator O
function O
defined O
on O
the O
shape O
. O
Our O
basic O
problem O
is O
to O
learn O
functions O
on O
shapes O
. O
We O
start O
with O
example O
functions O
provided O
on O
a O
given O
shape O
collection O
in O
the O
training O
set O
and O
build O
a O
neural Method
net Method
that O
can O
infer O
the O
same O
function O
when O
given O
a O
new O
3D Method
model Method
. O
This O
suggests O
the O
use O
of O
a O
spectral Method
formulation Method
, O
based O
on O
a O
dual Method
graph Method
representing O
the O
shape O
, O
yielding O
bases O
for O
a O
function O
space O
built O
over O
the O
mesh O
. O
With O
this O
graph Method
representation Method
we O
face O
multiple O
challenges O
in O
building O
a O
convolutional Method
neural Method
architecture Method
. O
One O
is O
how O
to O
share O
coefficients O
and O
conduct O
multiscale Method
analysis Method
in O
different O
parts O
of O
the O
graph O
for O
a O
single O
shape O
. O
Another O
is O
how O
to O
share O
information O
across O
related O
but O
different O
shapes O
that O
may O
be O
represented O
by O
very O
different O
graphs O
. O
We O
introduce O
a O
novel O
architecture O
, O
the O
Synchronized Method
Spectral Method
CNN Method
( O
SyncSpecCNN Method
) O
to O
address O
these O
issues O
. O
The O
basic O
architecture O
of O
our O
neural Method
network Method
is O
similar O
to O
the O
fully Method
convolutional Method
segmentation Method
network Method
of O
, O
namely O
, O
we O
repeat O
the O
operation O
of O
convolving O
a O
vertex O
function O
by O
kernels O
and O
applying O
a O
non Method
- Method
linear Method
transformation Method
. O
However O
, O
our O
network O
combines O
processing Task
in O
the O
primal Task
and Task
dual Task
( Task
spectral Task
) Task
domains Task
. O
We O
deal O
with O
the O
problem O
of O
weight Task
sharing Task
among O
convolution O
kernels O
at O
different O
scales O
in O
the O
primal O
domain O
by O
performing O
the O
convolutions Method
in O
the O
spectral O
domain O
, O
where O
they O
become O
just O
pointwise Method
multiplications Method
by O
the O
kernel Method
duals Method
. O
Our O
key O
building O
block O
consists O
of O
passing O
to O
the O
dual O
, O
performing O
a O
pointwise Method
multiplication Method
and O
then O
returning O
to O
the O
primal Method
representation Method
in O
order O
to O
perform O
an O
appropriate O
non O
- O
linear O
step O
( O
such O
operations O
are O
not O
easily O
dualized O
) O
. O
The O
issue O
of O
information Task
sharing Task
across O
shapes O
is O
more O
challenging O
. O
Since O
different O
shapes O
give O
rise O
to O
different O
nearest O
neighbor O
graphs O
on O
their O
point O
clouds O
, O
the O
eigenbases O
we O
get O
for O
the O
graph Method
laplacians Method
are O
not O
directly O
comparable O
. O
We O
synchronize O
all O
these O
laplacians O
by O
applying O
a O
functional Method
map Method
in O
the O
spectral O
domain O
to O
align O
them O
to O
a O
common O
canonical O
space O
. O
The O
aligning Method
functional Method
maps Method
succeed O
in O
encoding O
all O
the O
dual O
information O
on O
a O
common O
set O
of O
basis O
functions O
where O
global Task
learning Task
takes O
place O
. O
An O
initial O
version O
of O
the O
aligning O
maps O
is O
computed O
directly O
from O
the O
geometry O
and O
then O
is O
further O
refined O
during O
training Task
, O
in O
the O
style O
of O
a O
data Method
- Method
dependent Method
spatial Method
transformer Method
network Method
. O
We O
have O
tested O
our O
SyncSpecCNN Method
on O
various O
tasks O
including O
3D Task
shape Task
part Task
segmentation Task
and O
3D Task
keypoint Task
prediction Task
. O
We O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
all O
these O
tasks O
. O
Key O
contributions O
of O
our O
approach O
are O
as O
follows O
: O
We O
are O
the O
first O
to O
target O
at O
non O
- O
isometric O
shapes O
in O
the O
family O
of O
spectral Method
CNNs Method
. O
To O
allow O
weight O
sharing O
across O
different O
non O
- O
isometric O
shapes O
, O
we O
learn O
a O
Spectral Method
Transformer Method
Network Method
. O
We O
introduce O
an O
effective O
spectral Method
multiscale Method
kernel Method
construction Method
scheme Method
. O
section O
: O
Background O
paragraph O
: O
3D Task
Shape Task
Segmentation Task
An O
important O
application O
of O
our O
framework O
is O
to O
obtain O
semantic Task
part Task
segmentation Task
of Task
3D Task
shapes Task
in O
a O
supervised Task
fashion Task
. O
Along O
this O
track O
, O
most O
previous O
methods O
employ O
traditional O
machine Method
learning Method
techniques Method
and O
construct O
classifiers Method
based O
on O
geometric O
features O
. O
In O
the O
domain O
of O
unsupervised Task
shape Task
segmentation Task
, O
there O
is O
one O
family O
of O
methods O
emphasizes O
the O
effectiveness O
of O
spectral Method
analysis Method
for O
3D Task
shape Task
segmentation Task
. O
Inspired O
by O
this O
, O
our O
framework O
aims O
to O
marry O
the O
powerfulness O
of O
deep Method
neural Method
network Method
and O
spectral Method
analysis Method
for O
3D Task
shapes Task
segmentation Task
. O
paragraph O
: O
Spectral Method
analysis Method
on O
graphs O
We O
model O
a O
3D O
shape O
as O
a O
graph O
, O
whose O
vertices O
are O
points O
in O
and O
edges O
connect O
nearby O
points O
. O
At O
each O
vertex O
of O
the O
graph O
, O
we O
can O
assign O
a O
vector O
. O
In O
this O
way O
, O
we O
define O
a O
vector O
- O
valued O
vertex O
function O
on O
. O
For O
example O
, O
a O
segment O
on O
a O
shape O
can O
be O
represented O
as O
an O
indicator O
vertex O
function O
. O
this O
is O
the O
functional Task
view Task
of Task
segmentation Task
, O
as O
introduced O
in O
. O
The O
space O
of O
functions O
defined O
on O
can O
be O
represented O
under O
different O
bases O
, O
i.e. O
, O
for O
. O
One O
way O
to O
construct O
bases O
of O
is O
through O
spectral Method
analysis Method
– O
for O
each O
shape O
graph O
, O
the O
eigen O
vectors O
of O
its O
graph O
laplacian O
form O
an O
orthogonal O
bases O
. O
One O
type O
of O
graph O
laplacian O
can O
be O
constructed O
as O
, O
where O
is O
identity O
matrix O
, O
is O
the O
degree O
matrix O
and O
is O
the O
adjacency O
weight O
matrix O
of O
. O
Under O
this O
construction O
, O
the O
eigenvalues O
corresponding O
to O
satisfy O
. O
As O
is O
in O
Fourier Method
analysis Method
, O
the O
spectral Method
decomposition Method
also O
introduces O
the O
concept O
of O
frequency O
. O
For O
each O
basis O
, O
the O
eigenvalue O
in O
the O
decomposition O
defines O
its O
frequency O
, O
depicting O
its O
smoothness O
. O
By O
projecting O
on O
each O
basis O
, O
the O
coefficient O
can O
be O
obtained O
. O
is O
the O
spectral Method
representation Method
of O
, O
in O
analogy O
to O
the O
Fourier Method
transform Method
. O
The O
convolution Method
theorem Method
of Method
Fourier Method
analysis Method
can O
be O
extended O
to O
the O
laplacian O
spectrum O
: O
the O
convolution O
between O
a O
kernel O
and O
a O
function O
on O
the O
shape O
graph O
is O
equivalent O
to O
the O
point Method
wise Method
multiplication Method
of O
their O
spectral Method
representations Method
. O
paragraph O
: O
Functional Method
map Method
Different O
shapes O
define O
shape O
graphs O
with O
varied O
bases O
and O
spectral O
domains O
, O
which O
results O
in O
incomparable O
graph O
vertex O
function O
. O
Inspired O
by O
the O
recent O
work O
on O
synchronization Task
, O
we O
propose O
to O
align O
these O
different O
spectral O
domains O
using O
functional Method
map Method
. O
Functional Method
map Method
is O
initially O
introduced O
for O
this O
purpose O
on O
shapes O
. O
Specifically O
, O
given O
a O
pair O
of O
shape O
graph O
and O
, O
a O
functional Method
map Method
from O
to O
is O
given O
by O
a O
matrix O
, O
which O
maps O
a O
function O
with O
coefficient O
vector O
to O
the O
function O
with O
coefficient O
vector O
. O
and O
are O
computed O
according O
to O
a O
pair O
of O
bases O
. O
We O
refer O
the O
reader O
to O
for O
detailed O
introduction O
and O
intuition O
. O
paragraph O
: O
CNN Method
on O
Graphs O
We O
call O
such O
CNNs Method
as O
“ O
graph Method
CNNs Method
” O
. O
Graph Method
CNNs Method
takes O
a O
graph O
with O
vertex O
function O
as O
input O
. O
Conventional O
image Method
CNN Method
can O
be O
viewed O
as O
a O
graph Method
CNN Method
on O
2D O
regular O
grids O
of O
pixels O
, O
with O
RGB O
values O
as O
the O
vertex O
function O
. O
There O
have O
been O
some O
previous O
work O
studying O
graph Task
CNN Task
on O
more O
general O
graphs O
instead O
of O
2D O
regular O
grids O
, O
and O
have O
a O
special O
focus O
on O
near Task
- Task
isometric Task
3D Task
shape Task
graphs Task
like O
human O
bodies O
. O
To O
generalize O
image Task
CNN Task
, O
These O
work O
usually O
tries O
to O
tackle O
the O
following O
three O
challenges O
: O
defining O
translation O
structures O
on O
graphs O
to O
allow O
parameter O
sharing O
; O
designing O
compactly O
supported Method
filters Method
on O
graphs Method
; O
aggregating O
multi O
- O
scale O
information O
. O
Their O
constructions O
of O
deep Method
neural Method
network Method
usually O
fall O
into O
two O
types O
: O
spatial Method
construction Method
and O
spectral Method
construction Method
. O
The O
approach O
we O
propose O
belongs O
to O
the O
family O
of O
spectral Method
construction Method
but O
with O
two O
key O
differences O
: O
we O
explicitly O
design O
an O
effective O
multi Method
- Method
scale Method
information Method
aggregation Method
scheme Method
; O
we O
synchronize O
different O
spectral O
domains O
to O
allow O
parameter O
sharing O
among O
very O
different O
shape O
graphs O
thus O
increasing O
generalizability O
of O
our O
SyncSpecCNN Method
. O
section O
: O
Problem O
Given O
a O
3D O
shape O
represented O
as O
a O
shape O
graph O
, O
we O
seek O
for O
a O
per O
- O
vertex O
label O
, O
such O
as O
segmentation Task
or O
keypoints O
. O
These O
labels O
are O
represented O
as O
vertex O
functions O
on O
, O
i.e. O
, O
. O
We O
precompute O
a O
set O
of O
3D O
features O
for O
each O
vertex O
and O
use O
them O
as O
input O
vertex O
functions O
. O
These O
features O
capture O
location O
, O
curvature O
, O
and O
local O
context O
properties O
of O
each O
vertex O
and O
we O
use O
the O
publicly O
available O
implementation O
. O
To O
represent O
the O
functional O
space O
on O
shape O
graphs O
, O
we O
also O
construct O
the O
graph O
laplacian O
of O
each O
shape O
, O
compute O
the O
spectral O
frequency O
and O
corresponding O
bases O
through O
eigendecomposition Method
. O
We O
note O
that O
a O
basis O
is O
also O
a O
vertex O
function O
. O
Therefore O
, O
our O
neural Method
network Method
takes O
the O
laplacian O
of O
a O
graph O
and O
vertex O
functions O
of O
local O
geometric O
features O
as O
input O
, O
and O
predicts O
a O
vertex O
function O
such O
as O
segmentation O
or O
keypoint O
indicator O
function O
. O
section O
: O
Approach O
subsection O
: O
Overview O
The O
basic O
architecture O
of O
our O
SyncSpecCNN Method
is O
similar O
to O
the O
fully Method
convolutional Method
segmentation Method
network Method
as O
in O
, O
namely O
, O
we O
repeat O
the O
operation O
of O
convolving O
the O
vertex O
function O
by O
kernels O
and O
applying O
non Method
- Method
linear Method
transformation Method
. O
However O
, O
we O
have O
several O
key O
differences O
. O
First O
, O
we O
achieve O
convolution Method
by O
modulation Method
in O
the O
spectral O
domain O
. O
Second O
, O
we O
parametrize O
kernels O
in O
the O
spectral O
domain O
following O
a O
dilated O
fashion O
, O
so O
that O
kernel O
sizes O
could O
be O
effectively O
enlarged O
to O
capture O
large O
context O
information O
without O
increasing O
the O
number O
of O
parameters O
. O
Last O
, O
we O
design O
a O
Spectral Method
Transformer Method
Network Method
to O
synchronize O
the O
spectral O
domain O
of O
different O
shapes O
, O
allowing O
better O
parameter O
sharing O
. O
subsection O
: O
Network Method
Architecture Method
Similar O
to O
conventional O
CNN Method
, O
our O
SyncSpecCNN Method
contains O
layers O
including O
ReLU Method
, O
DropOut Method
, O
1 O
1 O
Convolution Method
, O
and O
BatchNormalization Method
, O
which O
all O
operate O
in O
the O
spatial O
domain O
on O
graph O
vertex O
functions O
. O
The O
difference O
comes O
from O
our O
graph Method
convolution Method
operation Method
, O
which O
introduces O
the O
following O
modules O
: O
Forward Method
Transform Method
, O
Backward Method
Transform Method
, O
Spectral Method
Multiplication Method
, O
and O
Spectral Method
Transformer Method
Network Method
, O
as O
is O
shown O
in O
Figure O
[ O
reference O
] O
and O
summarized O
in O
Table O
[ O
reference O
] O
. O
We O
provide O
more O
details O
about O
the O
newly O
introduces O
modules O
as O
below O
. O
In O
a O
basic O
convolution Method
block Method
, O
a O
vertex O
function O
defined O
on O
is O
first O
transformed O
into O
its O
spectral Method
representation Method
through O
Forward Method
Transform Method
. O
Then O
the O
functional O
map O
predicted O
by O
the O
Spectral Method
Transformer Method
Network Method
will O
be O
applied O
to O
and O
outputs O
for O
spectral Task
domain Task
synchronization Task
( O
Sec O
[ O
reference O
] O
) O
. O
A O
Spectral Method
Multiplication Method
layer Method
is O
followed O
, O
pointwisely O
multiplying O
by O
a O
set O
of O
multipliers O
and O
getting O
, O
where O
is O
a O
diagonal O
matrix O
with O
its O
diagonal O
being O
the O
set O
of O
multipliers O
, O
and O
is O
used O
to O
denote O
the O
multiplication O
result O
. O
This O
is O
how O
we O
conduct O
convolution Method
in O
the O
spectral O
domain O
, O
where O
spectral Method
dilated Method
kernels Method
are O
used O
to O
capture O
multiscale O
information O
( O
Sec O
[ O
reference O
] O
) O
. O
Then O
we O
apply O
the O
inverse Method
functional Method
map Method
to O
, O
so O
that O
we O
get O
the O
spectral Method
representation Method
in O
the O
original O
spectral O
domain O
before O
canonicalization O
. O
is O
then O
converted O
back O
to O
a O
graph O
vertex O
function O
through O
Backward Method
Transform Method
. O
This O
building O
block O
was O
repeated O
for O
several O
times O
and O
forms O
the O
backbone O
of O
our O
deep Method
architecture Method
. O
We O
also O
add O
skip O
links O
into O
our O
SyncSpecCNN Method
to O
better O
facilitate O
information O
flow O
across O
earlier O
and O
later O
layers O
. O
One O
interesting O
observation O
is O
worth O
mentioning O
: O
small O
convolution Method
kernels Method
correspond O
to O
smoothly O
transiting O
multipliers O
in O
the O
spectral O
domain O
, O
therefore O
not O
very O
sensitive O
to O
bases O
misalignment O
among O
shapes O
graphs O
in O
a O
certain O
range O
of O
spectrum O
and O
are O
more O
generalizable O
across O
graphs O
. O
As O
a O
result O
, O
we O
omit O
the O
spectral Method
transformer Method
network Method
when O
the O
convolution O
kernels O
are O
small O
. O
subsection O
: O
Spectral Method
Dilated Method
Kernel Method
Parameterization Method
Yu O
et O
al O
. O
has O
proved O
the O
effectiveness O
of O
multi Method
- Method
scale Method
kernels Method
for O
aggregating O
context O
information O
at O
different O
scales O
in O
the O
task O
of O
image Task
segmentation Task
. O
They O
propose O
to O
use O
dilated Method
kernels Method
to O
increase O
the O
kernel O
size O
without O
increasing O
the O
number O
of O
parameters O
. O
We O
parametrize O
our O
convolution Method
kernels Method
in O
a O
similar O
flavor O
but O
in O
the O
spectral O
domain O
, O
which O
turns O
out O
to O
be O
straightforward O
and O
effective O
. O
Essentially O
, O
we O
find O
that O
multi Task
- Task
resolution Task
analysis Task
on Task
graphs Task
could O
be O
achieved O
without O
complicated O
hierarchical Method
graph Method
clustering Method
. O
Before O
explaining O
what O
the O
exact O
parametrization O
is O
, O
we O
first O
discuss O
the O
intuition O
behind O
our O
design O
. O
The O
Spectral Method
Multiplication Method
layer Method
modulates O
the O
spectral Method
representation Method
by O
a O
set O
of O
multipliers O
from O
the O
kernel O
, O
where O
is O
the O
spectral O
coordinate O
of O
vertex O
function O
at O
basis O
. O
Note O
that O
can O
be O
interpreted O
as O
the O
frequency O
of O
its O
corresponding O
eigenbasis O
, O
and O
itself O
is O
a O
vertex O
function O
that O
captures O
the O
intrinsic O
geometry O
of O
the O
shape O
. O
We O
assume O
that O
’s O
are O
sorted O
ascendingly O
and O
arrange O
’s O
accordingly O
. O
The O
multiplers Method
are O
the O
spectral Method
representation Method
of Method
convolution Method
kernel Method
. O
Denote O
the O
set O
of O
multipliers O
as O
, O
each O
corresponds O
to O
one O
. O
Regard O
as O
a O
function O
of O
. O
Again O
, O
generalized O
from O
conventional O
Fourier Method
analysis Method
, O
if O
is O
concentrated O
in O
the O
low O
- O
end O
of O
the O
spectrum O
, O
the O
corresponding O
spatial O
kernel O
function O
is O
smooth O
; O
conversely O
, O
if O
the O
corresponding O
spatial O
functions O
is O
localized O
, O
is O
smooth O
. O
Therefore O
, O
to O
obtain O
a O
smoother O
kernel O
function O
as O
in O
, O
we O
constrain O
the O
bandwidth O
of O
, O
enabling O
us O
to O
learn O
a O
smaller O
number O
of O
parameters O
; O
in O
addition O
, O
varying O
the O
smoothness O
of O
would O
control O
the O
kernel O
size O
. O
To O
be O
specific O
, O
we O
associate O
each O
Spectral Method
Multiplication Method
layer Method
with O
a O
dilation O
parameter O
and O
parameterize O
as O
a O
combination O
of O
some O
modulated Method
exponential Method
window Method
functions Method
, O
namely O
Here O
is O
a O
set O
of O
learnable O
parameters O
, O
is O
a O
hyper O
- O
parameter O
controlling O
the O
number O
of O
learnable O
parameters O
. O
Large O
corresponds O
to O
rapidly O
changing O
multipliers O
with O
small O
bandwidth O
, O
thus O
a O
smooth Method
kernel Method
with O
large O
spatial O
support O
. O
On O
the O
other O
hand O
, O
small O
corresponds O
to O
slowly O
changing O
multipliers O
with O
large O
bandwidth O
, O
corresponding O
to O
kernels O
with O
small O
spatial O
support O
. O
Instead O
of O
using O
an O
exponential O
window O
only O
, O
we O
add O
modulation Method
to O
increase O
the O
expressive O
power O
of O
the O
kernel O
. O
Figure O
[ O
reference O
] O
shows O
a O
visualization O
of O
modulated O
exponential O
window O
function O
with O
different O
dilation O
parameter O
. O
Our O
parametrization O
has O
three O
main O
advantages O
: O
First O
, O
it O
allows O
aggregating O
multi O
- O
scale O
information O
since O
the O
size O
of O
convolution O
kernels O
vary O
in O
different O
layers O
; O
Second O
, O
large O
kernels O
could O
be O
easily O
acquired O
with O
a O
compact O
set O
of O
parameters O
, O
which O
effectively O
increases O
the O
receptive O
field O
while O
mitigates O
overfitting O
; O
Third O
, O
reduced O
parameters O
allow O
more O
efficient O
computation O
. O
subsection O
: O
Spectral Method
Transformer Method
Network Method
As O
is O
shown O
in O
Figure O
[ O
reference O
] O
, O
the O
same O
spectral Method
parametrization Method
of Method
kernels Method
could O
lead O
to O
very O
different O
vertex O
functions O
when O
the O
underlying O
spectral O
domains O
are O
different O
. O
This O
problem O
is O
especially O
prominent O
when O
the O
kernel Metric
size Metric
is O
large O
. O
Therefore O
, O
being O
able O
to O
synchronize O
different O
spectral O
domains O
is O
the O
key O
to O
allow O
large O
kernels O
sharing O
parameters O
across O
different O
shape O
graphs O
. O
subsubsection Method
: O
Basic O
idea O
According O
to O
and O
, O
one O
way O
to O
synchronize O
the O
spectral O
domains O
of O
a O
group O
of O
shapes O
is O
through O
a O
tool O
named O
functional Method
map Method
. O
In O
the O
functional Method
map Method
framework Method
, O
one O
can O
find O
a O
linear Method
map Method
to O
pull O
the O
spectral O
domain O
of O
each O
individual O
shape O
to O
a O
canonical O
space O
, O
so O
that O
representations O
in O
the O
individual O
spectral O
domains O
become O
comparable O
under O
a O
canonical O
set O
of O
bases O
. O
Indeed O
, O
given O
each O
shape O
, O
this O
linear Method
map Method
is O
as O
simple O
as O
a O
matrix O
, O
which O
linearly O
transforms O
the O
spectral Method
representation Method
on O
one O
shape O
to O
its O
counterpart O
in O
the O
canonical O
space O
. O
Note O
that O
, O
from O
the O
synchronization O
in O
the O
spectral O
domain O
, O
one O
induces O
a O
spatial O
correspondence O
on O
the O
graph O
, O
vice O
versa O
. O
Viewing O
the O
spectral O
domain O
as O
the O
dual O
space O
and O
spatial O
domain O
on O
graph O
as O
the O
primal O
space O
, O
this O
primal O
- O
dual O
relationship O
is O
the O
pivotal O
idea O
behind O
functional Method
map Method
. O
Inspired O
by O
this O
idea O
, O
we O
design O
a O
Spectral Method
Transformer Method
Network Method
( O
SpecTN Method
) O
for O
the O
spectral Task
domain Task
synchronization Task
task Task
. O
Our O
SpecTN Method
takes O
a O
shape O
as O
input O
and O
predicts O
a O
matrix O
for O
it O
( O
see O
Figure O
[ O
reference O
] O
) O
, O
so O
that O
. O
Thus O
, O
without O
SpecTN Method
, O
will O
be O
directly O
passed O
to O
subsequent O
modules O
of O
our O
network O
; O
with O
SpecTN Method
, O
will O
be O
passed O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
show O
an O
example O
of O
how O
different O
spectral O
domains O
are O
synchronized O
after O
applying O
the O
linear Method
map Method
predicted O
from O
our O
SpecTN Method
. O
Our O
SpecTN Method
draws O
inspiration O
from O
Spatial Method
Transformer Method
Network Method
( O
STN Method
) O
. O
From O
a O
high O
level O
, O
both O
SpecTN Method
and O
STN Method
are O
learned O
to O
align O
data O
to O
a O
canonical O
form O
. O
subsubsection Method
: O
Input O
to O
SpecTN Method
A O
proper O
representation O
for O
shape O
is O
needed O
as O
the O
input O
to O
our O
SpecTN Method
. O
To O
allow O
SpecTN Method
predicting O
a O
transform O
between O
different O
spectral O
domains O
, O
certain O
depiction O
about O
the O
underlying O
spectral O
domain O
is O
greatly O
helpful O
, O
i.e. O
graph O
laplacian O
eigenbases O
in O
our O
setting O
. O
In O
addition O
, O
since O
spectral Method
synchronization Method
couples O
with O
graph Task
alignment Task
, O
providing O
rough O
shape O
graph O
correspondences O
could O
facilitate O
good O
prediction Task
. O
Based O
on O
these O
, O
we O
use O
voxel O
functions O
that O
is O
computed O
from O
laplacian Method
eigenbases Method
as O
the O
input O
to O
SpecTN Method
: O
Specifically O
, O
is O
a O
volumetric Method
reparameterization Method
of O
the O
graph Method
laplacian Method
eigenbases Method
, O
defined O
voxel O
- O
wise O
in O
3D O
volumetric O
space O
. O
The O
volumetric Task
reparameterization Task
is O
conducted O
by O
converting O
graph O
vertex O
function O
into O
voxel O
function O
in O
a O
straightforward O
manner O
– O
we O
simply O
assign O
a O
vertex O
function O
value O
to O
the O
voxel O
where O
the O
vertex O
lies O
. O
Since O
all O
live O
in O
the O
same O
3D O
volumetric O
space O
, O
correspondences O
among O
them O
are O
associated O
accordingly O
. O
subsubsection Method
: O
Optimization O
of O
SpecTN Method
Ideally O
, O
SpecTN Method
should O
be O
learned O
automatically O
along O
with O
the O
minimization Task
of Task
the Task
prediction Task
loss Task
, O
as O
the O
case O
in O
STN Method
; O
however O
, O
in O
practice O
we O
find O
that O
such O
optimization Task
is O
extremely O
challenging O
. O
This O
is O
because O
the O
parameters O
of O
in O
SpecTN Method
is O
quadratic O
w.r.t O
the O
number O
of O
spectral O
bases O
, O
hundreds O
of O
times O
more O
than O
in O
the O
affine O
transformation O
matrix O
of O
STN Method
. O
We O
address O
this O
challenge O
from O
three O
aspects O
: O
limit O
our O
scope O
to O
a O
reduced O
set O
of O
prominent O
spectral O
bases O
to O
curtail O
the O
parameters O
of O
; O
add O
regularization Method
to O
constrain O
the O
optimization O
space O
; O
smartly O
initialize O
SpecTN Method
with O
a O
good O
starting O
point O
. O
paragraph O
: O
Reduced O
bases O
Synchronizing O
the O
whole O
spectrum O
could O
be O
a O
daunting O
task O
given O
its O
high O
dimensionality O
. O
In O
particular O
, O
free O
parameters O
in O
grows O
quadratically O
as O
the O
dimension O
of O
spectral O
domain O
increases O
. O
To O
favor O
optimization Task
, O
we O
adopt O
a O
natural O
strategy O
that O
only O
synchronizes O
the O
prominent O
part O
of O
the O
spectrum O
. O
In O
our O
case O
, O
the O
spectral Method
parametrization Method
of Method
large Method
kernels Method
are O
mainly O
determined O
by O
the O
low O
- O
frequency O
end O
of O
the O
spectrum O
, O
indicating O
that O
the O
synchronization O
in O
this O
part O
of O
spectrum O
is O
sufficient O
. O
In O
practice O
, O
we O
synchronize O
the O
top O
bases O
sorted O
by O
the O
frequency O
. O
This O
idea O
has O
been O
verified O
to O
be O
effective O
by O
. O
paragraph O
: O
Regularization O
Regularizations Method
are O
used O
during O
training Task
to O
force O
the O
output O
of O
SpecTN Method
to O
be O
close O
to O
an O
orthogonal O
map O
, O
namely O
, O
in O
the O
overall Metric
loss Metric
function Metric
we O
add O
a O
term O
. O
With O
this O
regularization Method
, O
can O
be O
used O
to O
approximate O
the O
inverse O
map O
. O
Such O
a O
maneuver O
is O
more O
friendly O
to O
differentiation O
and O
easier O
to O
train O
. O
paragraph O
: O
Initialization O
by O
precomputed Method
functional Method
map Method
Given O
the O
huge O
optimization O
space O
and O
the O
non O
- O
convex O
objective O
, O
a O
good O
starting O
point O
helps O
to O
avoid O
optimization Task
from O
getting O
stuck O
in O
bad O
local O
minima O
. O
As O
stated O
above O
, O
our O
linear Method
transformation Method
can O
be O
interpreted O
as O
a O
functional Method
map Method
; O
therefore O
, O
it O
is O
natural O
for O
us O
to O
initialize O
accordingly O
and O
then O
refine O
it O
to O
better O
serve O
the O
end Task
- Task
task Task
. O
To O
this O
end O
, O
we O
first O
precompute O
a O
set O
of O
function O
maps O
for O
each O
shape O
by O
an O
external O
routine O
, O
which O
roughly O
align O
each O
individual O
spectral O
domain O
of O
to O
a O
canonical O
domain O
. O
Then O
we O
pretrain O
the O
SpecTN Method
separately O
in O
a O
supervised Method
manner Method
: O
where O
indexes O
shapes O
. O
This O
pretrained O
SpecTN Method
is O
plugged O
into O
the O
SyncSpecCNN Method
pipeline Method
and O
fine O
- O
tuned O
while O
optimizing O
a O
specific O
task O
such O
as O
shape Task
segmentation Task
. O
Validated O
by O
our O
experiment O
, O
the O
pretraining Method
step Method
is O
crucial O
. O
Next O
we O
introduce O
how O
the O
external Method
routine Method
precomputes O
a O
functional Method
map Method
for O
some O
shape O
. O
This O
functional Method
map Method
aligns O
the O
spectral O
domain O
of O
to O
a O
canonical O
one O
of O
an O
“ O
average O
” O
shape O
. O
So O
we O
start O
from O
the O
construction O
of O
the O
“ O
average O
” O
shape O
and O
then O
proceed O
to O
the O
computation O
of O
the O
functional O
map O
. O
The O
geometry O
of O
is O
not O
generated O
explicitly O
. O
Instead O
, O
is O
represented O
by O
its O
volumetric O
adjacency O
matrix O
, O
which O
depicts O
the O
connectivity O
of O
voxels O
in O
the O
volumetric O
space O
that O
all O
shapes O
are O
voxelized O
. O
is O
obtained O
by O
averaging O
the O
volumetric Method
adjacency Method
matrices Method
of O
all O
shapes O
. O
The O
for O
each O
shape O
is O
the O
adjacency O
matrix O
of O
the O
corresponding O
volumetric O
graph O
, O
whose O
vertices O
are O
all O
the O
voxels O
and O
edges O
indicate O
the O
adjacency O
of O
occupied O
voxels O
in O
the O
volumetric O
space O
. O
The O
functional O
map O
from O
to O
could O
be O
induced O
from O
the O
spatial O
correspondences O
between O
and O
, O
by O
the O
primal Method
- Method
dual Method
relationship Method
. O
Since O
we O
already O
have O
the O
bases O
of O
and O
, O
as O
well O
as O
the O
rough O
spatial O
correspondences O
between O
them O
from O
the O
volumetric O
occupancy O
, O
this O
map O
can O
then O
be O
discovered O
by O
the O
approach O
proposed O
in O
. O
To O
be O
specific O
, O
we O
use O
to O
denote O
the O
volumetric O
reparametrization O
of O
graph O
laplacian O
eigenbases O
for O
each O
shape O
, O
and O
use O
to O
denote O
the O
grahp O
laplacian O
eigenbases O
of O
. O
and O
both O
lie O
in O
the O
volumetric O
space O
and O
their O
spatial O
correspondence O
is O
natural O
to O
acquire O
. O
The O
functional Task
map Task
aligning Task
with O
could O
be O
computed O
through O
simple O
matrix Method
multiplication Method
. O
The O
computed O
functional O
map O
will O
serve O
as O
supervision O
and O
SpecTN Method
is O
pretrained O
to O
minimize O
the O
loss O
function O
. O
It O
is O
worth O
mentioning O
that O
, O
if O
the O
shapes O
under O
consideration O
are O
diverse O
in O
topology O
and O
geometry O
, O
i.e. O
shapes O
from O
different O
categories O
, O
aligning O
every O
shape O
to O
a O
single O
“ O
average O
” O
shape O
might O
cause O
unwanted O
distortion O
. O
Therefore O
we O
leverage O
multiple O
“ O
average O
” O
shapes O
and O
use O
a O
combination O
of O
their O
spectral O
domains O
as O
the O
canonical O
domain O
. O
Specifically O
, O
we O
assign O
each O
shape O
to O
its O
closest O
“ O
average O
” O
shape O
under O
some O
global Metric
similarity Metric
measurement Metric
( O
i.e. O
lightfield O
descriptor O
) O
and O
use O
to O
represent O
such O
assignment O
, O
namely O
if O
is O
assigned O
to O
and O
otherwise O
. O
Also O
we O
use O
to O
denote O
the O
spectral O
bases O
of O
. O
Then O
the O
functional O
map O
for O
each O
shape O
could O
be O
computed O
through O
. O
The O
SpecTN Method
is O
pretrained O
to O
predict O
a O
functional Method
map Method
which O
only O
synchronizes O
spectral O
domain O
of O
each O
shape O
to O
its O
most O
similar O
“ O
average O
” O
shape O
. O
subsection O
: O
Implementation O
Details O
In O
most O
of O
our O
experiments O
, O
input O
shapes O
are O
represented O
as O
point O
cloud O
with O
around O
points O
. O
Given O
an O
input O
shape O
point O
cloud O
, O
we O
build O
a O
k Method
- Method
nearest Method
neighbor Method
graph Method
first O
. O
We O
use O
in O
all O
our O
experiments O
. O
Then O
a O
graph O
weight O
matrix O
could O
be O
constructed O
in O
which O
if O
point O
and O
are O
connected O
, O
otherwise O
. O
We O
then O
compute O
the O
symmetric O
normalized O
graph O
laplacian O
as O
, O
where O
is O
the O
degree O
matrix O
and O
denotes O
identity O
matrix O
. O
Since O
many O
natural O
functions O
we O
care O
about O
could O
be O
depicted O
by O
a O
small O
number O
of O
low O
- O
frequency O
laplacian O
eigenbases O
, O
we O
compute O
and O
use O
the O
smallest O
eigenvalues O
as O
well O
as O
the O
corresponding O
eigenbases O
for O
each O
in O
all O
our O
experiments O
. O
The O
choice O
of O
dilation O
parameters O
, O
number O
of O
output O
channels O
after O
each O
convolution Method
layer Method
, O
number O
of O
learnable O
parameters O
in O
each O
convolution Method
kernel Method
are O
shown O
in O
Table O
[ O
reference O
] O
. O
We O
choose O
in O
all O
of O
our O
experiments O
. O
As O
is O
mentioned O
, O
we O
only O
consider O
the O
problem O
of O
synchronizing O
the O
low O
- O
frequency O
end O
of O
different O
spectral O
domains O
, O
so O
we O
choose O
to O
predict O
a O
functional Method
map Method
in O
our O
experiments O
, O
which O
maps O
the O
first O
eigenbases O
of O
each O
individual O
spectral O
domain O
into O
a O
canonical O
domain O
of O
dimension O
. O
Notice O
the O
dimension O
of O
canonical O
domain O
is O
larger O
that O
each O
individual O
domain O
to O
allow O
very O
different O
shapes O
to O
be O
mapped O
into O
different O
subspaces O
. O
section O
: O
Experiment O
Our O
proposed O
SyncSpecCNN Method
takes O
one O
graph O
vertex O
function O
as O
input O
and O
predicts O
another O
as O
output O
. O
As O
a O
generic O
framework O
, O
the O
prediction Task
is O
not O
limited O
to O
a O
specific O
type O
of O
graph O
vertex O
function O
and O
can O
be O
tailored O
towards O
different O
goals O
. O
To O
evaluate O
the O
effectiveness O
of O
our O
framework O
, O
we O
divide O
our O
experiments O
into O
five O
parts O
. O
First O
, O
we O
evaluate O
on O
a O
benchmark O
of O
3D Task
shape Task
segmentation Task
. O
Second O
, O
we O
evaluate O
on O
keypoint Task
prediction Task
task Task
using O
a O
new O
large O
scale O
keypoint O
annotation O
dataset O
. O
Third O
, O
we O
leverage O
SyncSpecCNN Method
to O
learn O
vertex O
normal O
functions O
and O
visualize O
the O
prediction Task
results O
qualitatively O
. O
Fourth O
, O
we O
perform O
control O
experiments O
to O
compare O
different O
design O
choices O
of O
the O
framework O
and O
analyze O
the O
stability Metric
of O
our O
system O
under O
input O
sampling O
density O
variations O
. O
Last O
, O
we O
show O
qualitative O
results O
and O
analyze O
error O
patterns O
. O
subsection O
: O
Dataset O
For O
3D Task
shape Task
segmentation Task
task Task
, O
we O
use O
a O
large O
scale O
shape O
part O
annotation O
dataset O
introduced O
by O
, O
which O
augments O
a O
subset O
of O
ShapeNet Method
models Method
with O
semantic O
part O
annotations O
. O
The O
dataset O
contains O
16 O
categories O
of O
man O
- O
made O
shapes O
, O
with O
2 O
to O
6 O
parts O
per O
category O
. O
In O
total O
there O
are O
16 O
, O
881 O
models O
with O
expert O
verified O
part O
annotations O
. O
In O
addition O
, O
we O
use O
the O
official Metric
train Metric
/ Metric
test Metric
split Metric
provided O
along O
with O
ShapeNet Method
models Method
. O
For O
the O
keypoint Task
prediction Task
task Task
, O
we O
build O
a O
new O
large O
scale O
keypoint O
annotation O
dataset O
, O
containing O
1 O
, O
337 O
chair Method
models Method
with O
10 O
keypoints O
per O
shape O
, O
in O
contrast O
to O
traditional O
small O
scale O
dataset O
which O
has O
at O
most O
100 O
shapes O
annotated O
per O
category O
. O
These O
keypoints O
are O
all O
manually O
annotated O
by O
experts O
with O
consistency O
across O
different O
shapes O
. O
subsection O
: O
Shape Task
Part Task
Segmentation Task
paragraph O
: O
Per O
- O
category Method
shape Method
part Method
segmentation Method
We O
first O
conduct O
part Task
segmentation Task
assuming O
the O
category O
label O
of O
each O
shape O
is O
known O
, O
as O
the O
setting O
in O
. O
The O
task O
is O
to O
predict O
a O
part O
label O
for O
each O
sample O
point O
on O
shapes O
. O
We O
compare O
our O
framework O
with O
traditional O
learning Method
- Method
based Method
techniques Method
leveraging O
on O
local O
geometric O
features O
and O
shape O
alignment O
cues O
, O
as O
well O
as O
recent O
deep Method
learning Method
based Method
approaches Method
which O
also O
fall O
into O
the O
family O
of O
spectral Method
CNNs Method
. O
In O
addition O
we O
design O
an O
additional O
baseline O
using O
a O
3D Method
volumetric Method
CNN Method
architecture Method
, O
denoted O
as O
Voxel Method
CNN Method
, O
which O
generalizes O
VoxNet Method
for O
segmentation Task
tasks Task
. O
The O
network O
has O
10 O
convolutional Method
layers Method
without O
down Method
- Method
sampling Method
and O
keeps O
a O
receptive O
field O
of O
19 O
with O
spatial O
resolution O
of O
32 O
. O
We O
compute O
per O
- O
point O
features O
in O
the O
preprocessing O
step O
as O
is O
in O
and O
use O
the O
same O
set O
of O
input O
for O
all O
baselines O
except O
Voxel Method
CNN Method
. O
The O
set O
of O
input O
shapes O
are O
pre O
- O
aligned O
using O
a O
hierarchical Method
joint Method
alignment Method
algorithm Method
described O
in O
. O
Point Metric
intersection Metric
over Metric
union Metric
( O
IoU Metric
) O
is O
used O
as O
evaluation Metric
metric Metric
, O
averaged O
across O
all O
part O
classes O
. O
Cross Metric
- Metric
entropy Metric
loss Metric
is O
minimized O
during O
training O
. O
We O
evaluate O
our O
framework O
in O
two O
settings O
, O
with O
or O
without O
SpecTN Method
, O
and O
compare O
the O
results O
in O
Table O
[ O
reference O
] O
. O
Note O
that O
on O
most O
categories O
our O
approach O
achieves O
the O
best O
performance O
and O
on O
average O
outperforms O
state O
of O
the O
art O
by O
a O
large O
margin O
. O
In O
comparison O
to O
, O
the O
state O
of O
the O
art O
in O
the O
family O
of O
spectral Method
CNNs Method
, O
our O
approach O
introduces O
spectral Method
dilated Method
kernel Method
parametrization Method
, O
which O
increases O
the O
effectiveness O
of O
spectral Method
CNN Method
framework Method
. O
Moreover O
, O
the O
performance O
gain O
from O
SpecTN Method
shows O
that O
synchronizing O
spectral O
domains O
would O
greatly O
increase O
the O
generalizibility O
across O
shapes O
of O
different O
topology O
and O
geometry O
. O
paragraph O
: O
Cross Task
- Task
category Task
shape Task
part Task
segmentation Task
Next O
we O
evaluate O
our O
approach O
on O
the O
part Task
segmentation Task
task Task
in O
a O
cross Task
- Task
category Task
setting Task
. O
In O
this O
task O
, O
shape O
category O
label O
is O
not O
known O
during O
the O
test O
phase O
and O
for O
each O
point O
the O
network O
needs O
to O
select O
one O
of O
the O
part O
label O
from O
all O
possible O
part O
labels O
in O
all O
categories O
. O
Cross O
- O
category O
setting O
introduces O
larger O
geometric O
and O
topological O
variance O
among O
shapes O
, O
thus O
could O
help O
examining O
the O
spectral Method
CNN Method
’s O
ability O
of O
recognizing Task
objects Task
. O
At O
the O
same O
time O
the O
impact O
of O
spectral O
domain O
misalignment O
becomes O
stronger O
, O
providing O
a O
better O
testbed O
for O
validating O
the O
effectiveness O
of O
SpecTN Method
. O
Since O
this O
experiment O
is O
proposed O
to O
verify O
design O
choices O
of O
spectral Method
CNN Method
, O
we O
mainly O
compare O
with O
. O
We O
mix O
the O
16 O
categories O
of O
shapes O
in O
and O
train O
a O
single O
network O
for O
all O
categories O
. O
After O
predicting O
point O
segmentation O
labels O
, O
one O
can O
classify O
shapes O
through O
a O
point Method
- Method
wise Method
majority Method
voting Method
scheme Method
. O
Point O
IoU Metric
and O
classification Metric
accuracy Metric
( O
Acc Metric
) O
are O
chosen O
as O
the O
evalution Metric
metric Metric
for O
part Task
segmentation Task
and O
object Task
categorization Task
, O
respectively O
. O
The O
results O
are O
shown O
in O
the O
nd O
and O
rd O
column O
of O
Table O
[ O
reference O
] O
. O
Our O
approach O
outperforms O
the O
baseline O
ACNN Method
by O
a O
large O
margin Metric
on O
both O
segmentation Task
and O
classification Task
. O
Note O
that O
ACNN Method
does O
not O
explicitly O
conduct O
multi Method
- Method
scale Method
analysis Method
and O
is O
designed O
for O
near Task
- Task
isometric Task
3D Task
shapes Task
with O
similar O
spectral O
domains O
, O
thus O
generalizes O
less O
well O
across O
a O
diverse O
set O
of O
shapes O
. O
Our O
framework O
, O
in O
contrast O
, O
could O
effectively O
capture O
multi O
- O
scale O
context O
information O
, O
a O
feature O
that O
is O
highly O
important O
for O
both O
segmentation Task
and O
classification Task
. O
The O
spectral Method
domain Method
synchronization Method
ability Method
of O
SpecTN Method
further O
improves O
our O
generalizability O
, O
leading O
to O
an O
extra O
performance O
gain O
as O
is O
shown O
in O
Table O
[ O
reference O
] O
. O
paragraph O
: O
Partial Task
data Task
part Task
segmentation Task
To O
evaluate O
the O
robustness O
of O
our O
approach O
to O
incomplete O
data O
, O
we O
conduct O
part Task
segmentation Task
on O
simulated O
scans O
of O
3D O
shapes O
from O
a O
single O
viewpoint O
. O
To O
be O
specific O
, O
we O
generate O
simulated O
scans O
for O
each O
3D O
shape O
in O
the O
part O
annotation O
dataset O
from O
random O
viewpoints O
, O
and O
then O
use O
these O
partial O
point O
cloud O
with O
part O
annotations O
for O
train O
and O
test O
. O
All O
the O
partial O
point O
clouds O
are O
normalized O
to O
fit O
into O
a O
unit O
cube O
. O
Following O
the O
train O
/ O
test O
split O
provided O
by O
, O
we O
train O
our O
network O
to O
segment O
shape O
parts O
for O
each O
category O
. O
Again O
we O
compare O
our O
method O
with O
ACNN Method
. O
IoU Metric
is O
used O
as O
evaluation Metric
metric Metric
and O
the O
results O
are O
shown O
in O
the O
th O
and O
th O
column O
of O
Table O
[ O
reference O
] O
. O
Our O
approach O
outperforms O
the O
baseline O
on O
partial Task
data Task
part Task
segmentation Task
by O
a O
large O
margin O
. O
In O
particular O
, O
from O
complete O
shape O
to O
partial O
shape O
setting O
, O
the O
performance O
drop O
of O
our O
approach O
is O
less O
significantly O
than O
the O
baseline O
, O
reflected O
by O
the O
gap O
of O
mean Metric
IoU Metric
between O
the O
complete O
data O
setting O
and O
the O
partial O
setting O
. O
It O
verifies O
that O
our O
method O
is O
more O
robust O
to O
data O
incompleteness O
. O
We O
surmise O
that O
the O
performance O
of O
ACNN Method
is O
heavily O
influenced O
by O
noisy Method
and Method
sensitive Method
principal Method
curvature Method
estimation Method
on O
partial O
scans O
since O
this O
step O
plays O
a O
crucial O
rule O
in O
determining O
its O
local O
frames O
; O
whereas O
our O
approach O
makes O
less O
assumption O
about O
quality O
of O
the O
underlying O
shape O
. O
subsection O
: O
Keypoint Task
Prediction Task
Our O
framework O
is O
not O
limited O
to O
part Task
segmentation Task
but O
could O
learn O
more O
general O
functions O
on O
graphs O
. O
In O
this O
section O
, O
we O
evaluate O
our O
framework O
on O
the O
keypoint Task
prediction Task
task Task
. O
We O
associate O
each O
keypoint O
an O
individual O
label O
and O
assign O
all O
the O
non O
- O
keypoints O
a O
background O
class O
label O
. O
The O
keypoint Task
prediction Task
problem Task
could O
be O
treated O
as O
a O
multi Task
- Task
class Task
classification Task
problem Task
and O
the O
cross Metric
- Metric
entropy Metric
loss Metric
is O
optimized O
during O
training O
. O
We O
evaluate O
our O
approach O
against O
previous O
state O
- O
of O
- O
the O
- O
art O
method O
. O
first O
jointly O
aligns O
all O
the O
shapes O
in O
3D O
space O
via O
free Method
- Method
form Method
deformation Method
and O
then O
propagates O
keypoint O
labels O
to O
test O
shapes O
from O
its O
nearest O
training O
shapes O
. O
We O
manually O
tune O
and O
report O
the O
best O
performance O
of O
this O
method O
. O
Five O
- O
folds Method
cross Method
validation Method
is O
adopted O
during O
evaluation O
, O
and O
PCK Method
( O
percentage Metric
of Metric
correct Metric
keypoints Metric
) O
is O
used O
as O
evaluation Metric
metric Metric
. O
We O
show O
the O
PCK O
curve O
for O
the O
two O
approaches O
in O
Figure O
[ O
reference O
] O
. O
Each O
point O
on O
a O
curve O
indicates O
fraction O
of O
correctly O
predicted O
keypoints O
for O
a O
given O
Euclidean Metric
error Metric
threshold Metric
. O
Our O
approach O
outperforms O
, O
in O
particular O
, O
more O
precise O
predictions O
can O
be O
obtained O
by O
our O
method O
( O
see O
the O
region O
close O
to O
y O
- O
axis O
) O
. O
subsection O
: O
Normal Task
Prediction Task
To O
further O
validate O
the O
generality O
of O
our O
framework O
, O
we O
leverage O
our O
proposed O
SyncSpecCNN Method
to O
learn O
another O
type O
of O
graph O
vertex O
function O
, O
vertex O
normal O
function O
. O
Specifically O
, O
our O
SyncSpecCNN Method
takes O
the O
XYZ O
coordinate O
function O
of O
graph O
vertices O
as O
network O
input O
and O
predicts O
vertex O
normal O
as O
output O
. O
The O
network O
is O
trained O
to O
minimize O
the O
L2 Metric
loss Metric
between O
ground O
truth O
normals O
and O
predicted O
normals O
. O
We O
use O
the O
official O
train O
/ O
test O
split O
provided O
by O
and O
visualize O
some O
of O
the O
normal Task
prediction Task
results O
from O
test O
set O
in O
Figure O
[ O
reference O
] O
. O
It O
can O
be O
seen O
our O
predictions O
are O
very O
close O
to O
the O
ground O
truth O
at O
most O
of O
the O
time O
. O
Even O
on O
thin O
structures O
the O
normal O
predictions O
are O
still O
reasonable O
. O
One O
problem O
of O
our O
prediction Task
is O
that O
it O
tends O
to O
generate O
smoothly O
transiting O
normals O
along O
the O
boundary O
while O
the O
ground O
truth O
is O
sharper O
. O
This O
is O
due O
to O
the O
fact O
that O
we O
are O
using O
a O
small O
number O
of O
eigenbases O
in O
our O
experiments O
, O
which O
is O
not O
friendly O
to O
regression Task
tasks Task
with O
very O
high O
frequency O
signal O
as O
target O
. O
subsection O
: O
Diagnosis O
paragraph O
: O
Spectral Method
Dilated Method
Kernel Method
Parametrization Method
We O
evaluate O
our O
dilated Method
kernel Method
parametrization Method
from O
two O
aspects O
: O
the O
basis O
function O
choice O
and O
kernel Method
scale Method
choice Method
. O
Table O
[ O
reference O
] O
summarizes O
all O
the O
comparison O
results O
, O
as O
explained O
below O
. O
We O
explore O
the O
expressive O
power O
of O
different O
kernel Method
basis Method
. O
In O
the O
family O
of O
spectral Method
CNN Method
, O
convolution Method
kernels Method
are O
parametrized O
by O
a O
linear Method
combination Method
of Method
basis Method
functions Method
, O
i.e. O
modulated O
exponential O
window O
in O
our O
case O
. O
Previous O
methods O
have O
proposed O
to O
use O
different O
basis Method
functions Method
such O
as O
cubic Method
spline Method
basis Method
and O
exponential Method
window Method
basis Method
. O
Each O
row O
of O
Table O
[ O
reference O
] O
corresponds O
to O
a O
basis O
choice O
. O
We O
also O
evaluate O
the O
effectiveness O
of O
multi Task
- Task
scale Task
analysis Task
by O
changing O
the O
spatial O
sizes O
of O
convolution O
kernels O
. O
We O
compare O
with O
two O
baseline O
choices O
: O
set O
all O
kernel O
size O
to O
be O
the O
smallest O
kernel O
size O
in O
the O
current O
network O
; O
set O
to O
be O
the O
largest O
one O
. O
Each O
column O
of O
Table O
[ O
reference O
] O
corresponds O
to O
a O
kernel O
scale O
choice O
. O
All O
numbers O
are O
reported O
on O
the O
cross Task
- Task
category Task
part Task
segmentation Task
task Task
, O
by O
IoU. Metric
We O
only O
take O
the O
XYZ O
coordinate O
function O
of O
graph O
vertices O
as O
network O
input O
as O
opposed O
to O
handcrafted O
geometry O
features O
which O
may O
have O
already O
capture O
some O
multi O
- O
scale O
information O
. O
Also O
we O
remove O
the O
th O
and O
th O
layers O
from O
our O
network O
which O
involves O
SpecTN Method
and O
is O
designed O
for O
very O
large O
convolution O
kernels O
. O
It O
can O
be O
seen O
that O
modulated Method
exponential Method
window Method
basis Method
has O
a O
better O
expressive O
power O
compared O
with O
baselines O
for O
our O
segmentation Task
task Task
. O
Using O
multi Method
- Method
scale Method
kernels Method
also O
enables O
the O
aggregation O
of O
multi O
- O
scale O
information O
, O
thus O
producing O
better O
performance O
than O
small Method
or Method
large Method
kernels Method
alone O
. O
paragraph O
: O
Robustness Metric
to O
Sampling Metric
Density Metric
Variance Metric
In O
this O
experiment O
, O
we O
evaluate O
the O
robustness O
of O
our O
approach O
w.r.t O
point Task
cloud Task
density Task
variation Task
. O
To O
be O
specific O
, O
we O
train O
our O
SyncSpecCNN Method
for O
shape Task
segmentation Task
on O
the O
point O
cloud O
provided O
by O
first O
. O
Then O
we O
downsample O
the O
point O
cloud O
under O
different O
downsample O
ratio O
and O
evaluate O
our O
trained O
model O
to O
check O
how O
segmentation Task
performance O
would O
change O
. O
Again O
we O
evaluate O
our O
approach O
with O
/ O
without O
SpecTN Method
and O
the O
result O
is O
shown O
in O
Figure O
[ O
reference O
] O
. O
By O
introducing O
SpecTN Method
, O
our O
framework O
becomes O
more O
robust O
to O
sampling O
density O
variation O
. O
Our O
conjecture O
is O
that O
sampling O
density O
variation O
may O
result O
in O
large O
spectral O
space O
perturbation O
, O
therefore O
being O
able O
to O
synchronize O
different O
spectral O
domains O
becomes O
especially O
important O
. O
subsection O
: O
Qualitative O
Results O
and O
Error Metric
Analysis Metric
Figure O
[ O
reference O
] O
shows O
segmentation Task
results O
generated O
from O
our O
network O
on O
two O
categories O
, O
Chair O
and O
Lamp O
. O
Representative O
good O
results O
are O
shown O
in O
the O
first O
block O
and O
typical O
error O
patterns O
are O
summarized O
from O
the O
second O
to O
fourth O
blocks O
. O
Most O
of O
our O
segmentation Task
is O
very O
close O
to O
ground Metric
truth Metric
as O
is O
shown O
in O
the O
first O
block O
. O
We O
can O
accurately O
segment O
shapes O
with O
large O
geometric O
or O
topological O
variations O
like O
wide O
bench O
v.s O
. O
ordinary O
chair O
, O
pendant O
lamp O
v.s O
. O
table O
lamp O
. O
The O
lamp O
base O
on O
the O
first O
row O
and O
the O
lampshade O
on O
the O
second O
row O
are O
very O
similar O
regarding O
their O
local O
geometry O
; O
however O
, O
since O
our O
network O
is O
able O
to O
capture O
large O
scale O
context O
information O
, O
it O
could O
still O
differentiate O
the O
two O
and O
segment O
shapes O
correctly O
. O
We O
observe O
several O
typical O
error O
patterns O
in O
our O
results O
. O
Most O
segmentation Metric
error Metric
occurs O
along O
part O
boundaries O
. O
There O
are O
also O
cases O
where O
the O
semantic Task
definition Task
of Task
parts Task
has O
inherent O
ambiguities O
. O
We O
also O
observe O
a O
third O
type O
of O
error O
pattern O
, O
in O
which O
our O
prediction O
might O
miss O
a O
certain O
part O
completely O
, O
as O
is O
shown O
in O
the O
fourth O
block O
. O
section O
: O
Conclusion O
We O
introduce O
a O
novel O
neural Method
network Method
architecture Method
, O
the O
Synchronized Method
Spectral Method
CNN Method
( O
SyncSpecCNN Method
) O
, O
for O
semantic Task
annotation Task
on O
3D Task
shape Task
graphs Task
. O
To O
share O
coefficients O
and O
conduct O
multi Task
- Task
scale Task
analysis Task
in O
different O
parts O
of O
a O
single O
shape O
graph O
, O
we O
introduce O
a O
spectral Method
parametrization Method
of Method
dilated Method
convolutional Method
kernels Method
. O
To O
allow O
parameter O
sharing O
across O
related O
but O
different O
shapes O
that O
may O
be O
represented O
by O
very O
different O
graphs O
, O
we O
introduce O
a O
spectral Method
transformer Method
network Method
to O
synchronize O
different O
spectral O
domains O
. O
The O
effectiveness O
of O
different O
components O
in O
our O
network O
is O
validated O
through O
extensive O
experiments O
. O
Jointly O
these O
contributions O
lead O
to O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
various O
semantic Task
annotation Task
tasks Task
including O
3D Task
shape Task
part Task
segmentation Task
and O
3D Task
keypoint Task
prediction Task
. O
bibliography O
: O
References O
