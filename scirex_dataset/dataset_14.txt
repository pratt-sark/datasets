Learning	O
to	O
Compose	O
Task	O
-	O
Specific	O
Tree	O
Structures	O
section	O
:	O
Abstract	O
For	O
years	O
,	O
recursive	Method
neural	Method
networks	Method
(	O
RvNNs	Method
)	O
have	O
been	O
shown	O
to	O
be	O
suitable	O
for	O
representing	O
text	O
into	O
fixed	O
-	O
length	O
vectors	O
and	O
achieved	O
good	O
performance	O
on	O
several	O
natural	Task
language	Task
processing	Task
tasks	Task
.	O
However	O
,	O
the	O
main	O
drawback	O
of	O
RvNNs	Method
is	O
that	O
they	O
require	O
structured	O
input	O
,	O
which	O
makes	O
data	Task
preparation	Task
and	O
model	Task
implementation	Task
hard	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
,	O
a	O
novel	O
tree	Method
-	Method
structured	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
architecture	Method
that	O
learns	O
how	O
to	O
compose	O
task	O
-	O
specific	O
tree	O
structures	O
only	O
from	O
plain	O
text	O
data	O
efficiently	O
.	O
Our	O
model	O
uses	O
Straight	O
-	O
Through	O
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
to	O
decide	O
the	O
parent	O
node	O
among	O
candidates	O
dynamically	O
and	O
to	O
calculate	O
gradients	O
of	O
the	O
discrete	O
decision	O
.	O
We	O
evaluate	O
the	O
proposed	O
model	O
on	O
natural	Task
language	Task
inference	Task
and	O
sentiment	Task
analysis	Task
,	O
and	O
show	O
that	O
our	O
model	O
outperforms	O
or	O
is	O
at	O
least	O
comparable	O
to	O
previous	O
models	O
.	O
We	O
also	O
find	O
that	O
our	O
model	O
converges	O
significantly	O
faster	O
than	O
other	O
models	O
.	O
section	O
:	O
Introduction	O
Techniques	O
for	O
mapping	Task
natural	Task
language	Task
into	O
vector	Task
space	Task
have	O
received	O
a	O
lot	O
of	O
attention	O
,	O
due	O
to	O
their	O
capability	O
of	O
representing	O
ambiguous	O
semantics	O
of	O
natural	O
language	O
using	O
dense	O
vectors	O
.	O
Among	O
them	O
,	O
methods	O
of	O
learning	Task
representations	Task
of	Task
words	Task
,	O
e.g.	O
word2vec	O
[	O
reference	O
]	O
or	O
GloVe	Method
,	O
are	O
relatively	O
well	O
-	O
studied	O
empirically	O
and	O
theoretically	O
[	O
reference	O
][	O
reference	O
]	O
,	O
and	O
some	O
of	O
them	O
became	O
typical	O
choices	O
to	O
consider	O
when	O
initializing	O
word	Method
representations	Method
for	O
better	O
performance	O
at	O
downstream	Task
tasks	Task
.	O
Meanwhile	O
,	O
research	O
on	O
sentence	Task
representation	Task
is	O
still	O
in	O
active	O
progress	O
,	O
and	O
accordingly	O
various	O
architecturesdesigned	O
with	O
different	O
intuition	O
and	O
tailored	O
for	O
different	O
tasks	O
-	O
are	O
being	O
proposed	O
.	O
In	O
the	O
midst	O
of	O
them	O
,	O
three	O
architectures	O
are	O
most	O
frequently	O
used	O
in	O
obtaining	O
sentence	Task
representation	Task
from	O
words	O
.	O
Convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
[	O
reference	O
][	O
reference	O
]	O
utilize	O
local	O
distribution	O
of	O
words	O
to	O
encode	O
sentences	O
,	O
similar	O
to	O
n	Method
-	Method
gram	Method
models	Method
.	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
encode	O
sentences	O
by	O
reading	O
words	O
in	O
sequential	O
order	O
.	O
Recursive	Method
neural	Method
networks	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
,	O
on	O
which	O
this	O
paper	O
focuses	O
,	O
rely	O
on	O
structured	O
input	O
(	O
e.g.	O
parse	O
tree	O
)	O
to	O
encode	O
sentences	O
,	O
based	O
on	O
the	O
intuition	O
that	O
there	O
is	O
significant	O
semantics	O
in	O
the	O
hierarchical	O
structure	O
of	O
words	O
.	O
It	O
is	O
also	O
notable	O
that	O
RvNNs	Method
are	O
generalization	Method
of	Method
RNNs	Method
,	O
as	O
linear	O
chain	O
structures	O
on	O
which	O
RNNs	Method
operate	O
are	O
equivalent	O
to	O
left	O
-	O
or	O
right	O
-	O
skewed	O
trees	O
.	O
Although	O
there	O
is	O
significant	O
benefit	O
in	O
processing	O
a	O
sentence	O
in	O
a	O
tree	O
-	O
structured	O
recursive	O
manner	O
,	O
data	O
annotated	O
with	O
parse	O
trees	O
could	O
be	O
expensive	O
to	O
prepare	O
and	O
hard	O
to	O
be	O
computed	O
in	O
batches	O
[	O
reference	O
]	O
.	O
Furthermore	O
,	O
the	O
optimal	O
hierarchical	O
composition	O
of	O
words	O
might	O
differ	O
depending	O
on	O
the	O
properties	O
of	O
a	O
task	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
,	O
which	O
is	O
a	O
novel	O
RvNN	Method
architecture	Method
that	O
does	O
not	O
require	O
structured	O
data	O
and	O
learns	O
to	O
compose	O
task	O
-	O
specific	O
tree	O
structures	O
without	O
explicit	O
guidance	O
.	O
Our	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
model	O
is	O
based	O
on	O
tree	O
-	O
structured	O
long	O
short	O
-	O
term	O
memory	O
(	O
Tree	O
-	O
LSTM	Method
)	O
architecture	O
[	O
reference	O
][	O
reference	O
]	O
,	O
which	O
is	O
one	O
of	O
the	O
most	O
renowned	O
variants	O
of	O
RvNN	Method
.	O
To	O
learn	O
how	O
to	O
compose	O
task	O
-	O
specific	O
tree	O
structures	O
without	O
depending	O
on	O
structured	O
input	O
,	O
our	O
model	O
introduces	O
composition	O
query	O
vector	O
that	O
measures	O
validity	O
of	O
a	O
composition	O
.	O
Using	O
validity	O
scores	O
computed	O
by	O
the	O
composition	O
query	O
vector	O
,	O
our	O
model	O
recursively	O
selects	O
compositions	O
until	O
only	O
a	O
single	O
representation	O
remains	O
.	O
We	O
use	O
StraightThrough	Method
(	Method
ST	Method
)	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
[	O
reference	O
][	O
reference	O
]	O
to	O
sample	O
compositions	O
in	O
the	O
training	O
phase	O
.	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
relaxes	O
the	O
discrete	O
sampling	O
operation	O
to	O
be	O
continuous	O
in	O
the	O
backward	O
pass	O
,	O
thus	O
our	O
model	O
can	O
be	O
trained	O
via	O
the	O
standard	O
backpropagation	Method
.	O
Also	O
,	O
since	O
the	O
computation	O
is	O
performed	O
layer	O
-	O
wise	O
,	O
our	O
model	O
is	O
easy	O
to	O
implement	O
and	O
naturally	O
supports	O
batched	Task
computation	Task
.	O
From	O
experiments	O
on	O
natural	Task
language	Task
inference	Task
and	O
sentiment	Task
analysis	Task
tasks	Task
,	O
we	O
find	O
that	O
our	O
proposed	O
model	O
outperforms	O
or	O
is	O
at	O
least	O
comparable	O
to	O
previous	O
sentence	Method
encoder	Method
models	Method
and	O
converges	O
significantly	O
faster	O
than	O
them	O
.	O
The	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
•	O
We	O
designed	O
a	O
novel	O
sentence	Method
encoder	Method
architecture	Method
that	O
learns	O
to	O
compose	O
task	O
-	O
specific	O
trees	O
from	O
plain	O
text	O
data	O
.	O
•	O
We	O
showed	O
from	O
experiments	O
that	O
the	O
proposed	O
architecture	O
outperforms	O
or	O
is	O
competitive	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
We	O
also	O
observed	O
that	O
our	O
model	O
converges	O
faster	O
than	O
others	O
.	O
•	O
Specifically	O
,	O
we	O
saw	O
that	O
our	O
model	O
significantly	O
outperforms	O
previous	O
RvNN	Method
works	Method
trained	O
on	O
parse	O
trees	O
in	O
all	O
conducted	O
experiments	O
,	O
from	O
which	O
we	O
hypothesize	O
that	O
syntactic	O
parse	O
tree	O
may	O
not	O
be	O
the	O
best	O
structure	O
for	O
every	O
task	O
and	O
the	O
optimal	O
structure	O
could	O
differ	O
per	O
task	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
briefly	O
introduce	O
previous	O
works	O
which	O
have	O
similar	O
objectives	O
to	O
that	O
of	O
our	O
work	O
.	O
Then	O
we	O
describe	O
the	O
proposed	O
model	O
in	O
detail	O
and	O
present	O
findings	O
from	O
experiments	O
.	O
Lastly	O
we	O
summarize	O
the	O
overall	O
content	O
and	O
discuss	O
future	O
work	O
.	O
section	O
:	O
Related	O
Work	O
There	O
have	O
been	O
several	O
works	O
that	O
aim	O
to	O
learn	O
hierarchical	O
latent	O
structure	O
of	O
text	O
by	O
recursively	O
composing	O
words	O
into	O
sentence	Method
representation	Method
.	O
Some	O
of	O
them	O
carry	O
unsupervised	Task
learning	Task
on	O
structures	O
by	O
making	O
composition	O
operations	O
soft	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
gated	Method
recursive	Method
convolutional	Method
neural	Method
network	Method
(	O
grConv	Method
)	Method
[	O
reference	O
]	O
)	O
is	O
the	O
first	O
model	O
of	O
its	O
kind	O
and	O
used	O
as	O
an	O
encoder	Method
for	O
neural	Task
machine	Task
translation	Task
.	O
The	O
grConv	Method
architecture	Method
uses	O
gating	Method
mechanism	Method
to	O
control	O
the	O
information	O
flow	O
from	O
children	O
to	O
parent	O
.	O
grConv	Method
and	O
its	O
variants	O
are	O
also	O
applied	O
to	O
sentence	Task
classification	Task
tasks	Task
[	O
reference	O
][	O
reference	O
]	O
.	O
Neural	Method
tree	Method
indexer	Method
(	O
NTI	Method
)	O
[	O
reference	O
]	O
utilizes	O
soft	O
hierarchical	O
structures	O
by	O
using	O
Tree	O
-	O
LSTM	Method
instead	O
of	O
grConv	Method
.	O
Although	O
models	O
that	O
operate	O
with	O
soft	O
structures	O
are	O
naturally	O
capable	O
of	O
being	O
trained	O
via	O
backpropagation	Method
,	O
the	O
structures	O
predicted	O
by	O
them	O
are	O
ambiguous	O
and	O
thus	O
it	O
is	O
hard	O
to	O
interpret	O
them	O
.	O
CYK	O
Tree	O
-	O
LSTM	Method
[	O
reference	O
]	O
resolves	O
this	O
ambiguity	O
while	O
maintaining	O
the	O
soft	O
property	O
by	O
introducing	O
the	O
concept	O
of	O
CYK	Method
parsing	Method
algorithm	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
.	O
Though	O
their	O
model	O
reduces	O
the	O
ambiguity	O
by	O
explicitly	O
representing	O
a	O
node	O
as	O
a	O
weighted	O
sum	O
of	O
all	O
candidate	O
compositions	O
,	O
it	O
is	O
memory	O
intensive	O
since	O
the	O
number	O
of	O
candidates	O
linearly	O
increases	O
by	O
depth	O
.	O
On	O
the	O
other	O
hand	O
,	O
there	O
exist	O
some	O
previous	O
works	O
that	O
maintain	O
the	O
discreteness	O
of	O
tree	O
composition	O
processes	O
,	O
instead	O
of	O
relying	O
on	O
the	O
soft	O
hierarchical	O
structure	O
.	O
The	O
architecture	O
proposed	O
by	O
[	O
reference	O
]	O
greedily	O
selects	O
two	O
adjacent	O
nodes	O
whose	O
reconstruction	Metric
error	Metric
is	O
the	O
smallest	O
and	O
merges	O
them	O
into	O
the	O
parent	O
.	O
In	O
their	O
work	O
,	O
rather	O
than	O
directly	O
optimized	O
on	O
classification	O
loss	O
,	O
a	O
composition	Method
function	Method
is	O
optimized	O
to	O
minimize	O
reconstruction	Metric
error	Metric
.	O
[	O
reference	O
]	O
introduce	O
reinforcement	Method
learning	Method
to	O
achieve	O
the	O
desired	O
effect	O
of	O
discretization	Task
.	O
They	O
show	O
that	O
REINFORCE	O
[	O
reference	O
]	O
)	O
algorithm	O
can	O
be	O
used	O
in	O
estimating	Task
gradients	Task
to	O
learn	O
a	O
tree	O
composition	O
function	O
minimizing	O
classification	Metric
error	Metric
.	O
However	O
,	O
slow	O
convergence	Metric
due	O
to	O
the	O
reinforcement	Method
learning	Method
setting	Method
is	O
one	O
of	O
its	O
drawbacks	O
,	O
according	O
to	O
the	O
authors	O
.	O
In	O
the	O
research	O
area	O
outside	O
the	O
RvNN	Method
,	O
compositionality	Task
in	Task
vector	Task
space	Task
also	O
has	O
been	O
a	O
longstanding	O
subject	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
to	O
name	O
a	O
few	O
)	O
.	O
And	O
more	O
recently	O
,	O
there	O
exist	O
works	O
aiming	O
to	O
learn	O
hierarchical	O
latent	O
structure	O
from	O
unstructured	O
data	O
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O
section	O
:	O
Model	O
Description	O
Our	O
proposed	O
architecture	O
is	O
built	O
based	O
on	O
the	O
treestructured	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
network	Method
architecture	Method
.	O
We	O
introduce	O
several	O
additional	O
components	O
into	O
the	O
Tree	O
-	O
LSTM	Method
architecture	O
to	O
allow	O
the	O
model	O
to	O
dynamically	O
compose	O
tree	O
structure	O
in	O
a	O
bottom	O
-	O
up	O
manner	O
and	O
to	O
effectively	O
encode	O
a	O
sentence	O
into	O
a	O
vector	O
.	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
components	O
of	O
our	O
model	O
in	O
detail	O
.	O
section	O
:	O
Tree	O
-	O
LSTM	Method
Tree	Method
-	Method
structured	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
network	Method
(	O
Tree	O
-	O
LSTM	Method
)	O
[	O
reference	O
][	O
reference	O
]	O
is	O
an	O
elegant	O
variant	O
of	O
RvNN	Method
,	O
where	O
it	O
controls	O
information	O
flow	O
from	O
children	O
to	O
parent	O
using	O
similar	O
mechanism	O
to	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
[	O
reference	O
]	O
.	O
Tree	O
-	O
LSTM	Method
introduces	O
cell	O
state	O
in	O
computing	O
parent	Method
representation	Method
,	O
which	O
assists	O
each	O
cell	O
to	O
capture	O
distant	O
vertical	O
dependencies	O
.	O
The	O
following	O
are	O
formulae	O
that	O
our	O
model	O
uses	O
to	O
compute	O
parent	Method
representation	Method
from	O
its	O
children	O
:	O
where	O
,	O
and	O
is	O
the	O
element	O
-	O
wise	O
product	O
.	O
Note	O
that	O
our	O
formulation	O
is	O
akin	O
to	O
that	O
of	O
SPINN	Method
[	O
reference	O
]	O
)	O
,	O
but	O
our	O
version	O
does	O
not	O
include	O
the	O
tracking	O
LSTM	Method
.	O
Instead	O
,	O
our	O
model	O
can	O
apply	O
an	O
LSTM	Method
to	O
leaf	O
nodes	O
,	O
which	O
we	O
will	O
soon	O
describe	O
.	O
section	O
:	O
Gumbel	Method
-	Method
Softmax	Method
Gumbel	Method
-	Method
Softmax	Method
(	O
Jang	O
,	O
Gu	O
,	O
and	O
Poole	O
2017	O
)	O
(	O
or	O
Concrete	Method
distribution	Method
[	O
reference	O
]	O
)	O
is	O
a	O
method	O
of	O
utilizing	O
discrete	O
random	O
variables	O
in	O
a	O
network	O
.	O
Since	O
it	O
approximates	O
one	O
-	O
hot	O
vectors	O
sampled	O
from	O
a	O
categorical	O
distribution	O
by	O
making	O
them	O
continuous	O
,	O
gradients	O
of	O
model	O
parameters	O
can	O
be	O
calculated	O
using	O
the	O
reparameterization	Method
trick	Method
and	O
the	O
standard	O
backpropagation	Method
.	O
GumbelSoftmax	Method
is	O
known	O
to	O
have	O
an	O
advantage	O
over	O
score	Method
-	Method
functionbased	Method
gradient	Method
estimators	Method
such	O
as	O
REINFORCE	Method
[	O
reference	O
]	O
which	O
suffer	O
from	O
high	O
variance	Metric
and	O
slow	Metric
convergence	Metric
[	O
reference	O
]	O
.	O
Gumbel	Method
-	Method
Softmax	Method
distribution	Method
is	O
motivated	O
by	O
GumbelMax	Method
trick	Method
[	O
reference	O
]	O
,	O
an	O
algorithm	O
for	O
sampling	Task
from	O
a	O
categorical	O
distribution	O
.	O
Consider	O
Figure	O
1	O
:	O
Visualization	O
of	O
forward	O
and	O
backward	O
computation	O
path	O
of	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
.	O
In	O
the	O
forward	O
pass	O
,	O
a	O
model	O
can	O
maintain	O
sparseness	O
due	O
to	O
arg	Method
max	Method
operation	Method
.	O
In	O
the	O
backward	O
pass	O
,	O
since	O
there	O
is	O
no	O
discrete	O
operation	O
,	O
the	O
error	O
signal	O
can	O
backpropagate	O
.	O
a	O
k	O
-	O
dimensional	O
categorical	O
distribution	O
whose	O
class	O
probabilities	O
p	O
1	O
,	O
·	O
·	O
·	O
,	O
p	O
k	O
are	O
defined	O
in	O
terms	O
of	O
unnormalized	O
log	O
probabilities	O
π	O
1	O
,	O
·	O
·	O
·	O
,	O
π	O
k	O
:	O
Then	O
a	O
one	O
-	O
hot	O
sample	O
from	O
the	O
distribution	O
can	O
be	O
easily	O
drawn	O
by	O
the	O
following	O
equations	O
:	O
Here	O
,	O
g	O
i	O
,	O
namely	O
Gumbel	O
noise	O
,	O
perturbs	O
each	O
log	O
(	O
π	O
i	O
)	O
term	O
so	O
that	O
taking	O
arg	Task
max	Task
becomes	O
equivalent	O
to	O
drawing	O
a	O
sample	O
weighted	O
on	O
p	O
1	O
,	O
·	O
·	O
·	O
,	O
p	O
k	O
.	O
In	O
Gumbel	Method
-	Method
Softmax	Method
,	O
the	O
discontinuous	Method
arg	Method
max	Method
function	Method
of	O
Gumbel	Method
-	Method
Max	Method
trick	Method
is	O
replaced	O
by	O
the	O
differentiable	O
softmax	O
function	O
.	O
That	O
is	O
,	O
given	O
unnormalized	O
probabilities	O
π	O
1	O
,	O
·	O
·	O
·	O
,	O
π	O
k	O
,	O
a	O
sample	O
y	O
=	O
(	O
y	O
1	O
,	O
·	O
·	O
·	O
,	O
y	O
k	O
)	O
from	O
the	O
GumbelSoftmax	Method
distribution	Method
is	O
drawn	O
by	O
where	O
τ	O
is	O
a	O
temperature	O
parameter	O
;	O
as	O
τ	O
diminishes	O
to	O
zero	O
,	O
a	O
sample	O
from	O
the	O
Gumbel	Method
-	Method
Softmax	Method
distribution	Method
becomes	O
cold	O
and	O
resembles	O
the	O
one	O
-	O
hot	O
sample	O
.	O
Straight	Method
-	Method
Through	Method
(	Method
ST	Method
)	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
[	O
reference	O
]	O
,	O
whose	O
name	O
reminds	O
of	O
StraightThrough	Method
estimator	Method
(	O
STE	Method
)	O
[	O
reference	O
]	O
,	O
is	O
a	O
discrete	Method
version	Method
of	O
the	O
continuous	Method
GumbelSoftmax	Method
estimator	Method
.	O
Similar	O
to	O
the	O
STE	Method
,	O
it	O
maintains	O
sparsity	O
by	O
taking	O
different	O
paths	O
in	O
the	O
forward	Method
and	Method
backward	Method
propagation	Method
.	O
Obviously	O
ST	Method
estimators	Method
are	O
biased	O
,	O
however	O
they	O
perform	O
well	O
in	O
practice	O
,	O
according	O
to	O
several	O
previous	O
works	O
[	O
reference	O
][	O
reference	O
]	O
and	O
our	O
own	O
result	O
.	O
In	O
the	O
forward	O
pass	O
,	O
it	O
discretizes	O
a	O
continuous	O
probability	O
vector	O
y	O
sampled	O
from	O
the	O
Gumbel	Method
-	Method
Softmax	Method
distribution	Method
into	O
the	O
one	O
-	O
hot	O
vector	O
where	O
And	O
in	O
the	O
backward	O
pass	O
it	O
simply	O
uses	O
the	O
continuous	O
y	O
,	O
thus	O
the	O
error	O
signal	O
is	O
still	O
able	O
to	O
backpropagate	O
.	O
See	O
Figure	O
1	O
for	O
the	O
visualization	O
of	O
the	O
forward	O
and	O
backward	O
pass	O
.	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
is	O
useful	O
when	O
a	O
model	O
needs	O
to	O
utilize	O
discrete	O
values	O
directly	O
,	O
for	O
example	O
in	O
the	O
case	O
that	O
a	O
model	O
alters	O
its	O
computation	O
path	O
based	O
on	O
samples	O
drawn	O
from	O
a	O
categorical	O
distribution	O
.	O
section	O
:	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
In	O
our	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
model	O
,	O
an	O
input	O
sentence	O
composed	O
of	O
N	O
words	O
is	O
represented	O
as	O
a	O
sequence	O
of	O
word	O
vectors	O
(	O
x	O
1	O
,	O
·	O
·	O
·	O
,	O
x	O
N	O
)	O
,	O
where	O
x	O
i	O
∈	O
R	O
Dx	O
.	O
Our	O
basic	O
model	O
applies	O
an	O
affine	Method
transformation	Method
to	O
each	O
x	O
i	O
to	O
obtain	O
the	O
initial	O
hidden	O
and	O
cell	O
state	O
:	O
which	O
we	O
call	O
leaf	Method
transformation	Method
.	O
In	O
Eq	O
.	O
10	O
,	O
W	O
leaf	O
∈	O
R	O
2D	O
h	O
×Dx	O
and	O
b	O
leaf	O
∈	O
R	O
2D	O
h	O
.	O
Note	O
that	O
we	O
denote	O
the	O
representation	O
of	O
i	O
-	O
th	O
node	O
at	O
t	O
-	O
th	O
layer	O
as	O
r	O
.	O
Node	Method
representations	Method
which	O
are	O
not	O
selected	O
are	O
copied	O
to	O
the	O
corresponding	O
positions	O
at	O
layer	O
t	O
+	O
1	O
.	O
In	O
other	O
words	O
,	O
the	O
(	O
t	O
+	O
1	O
)-	O
th	O
layer	O
is	O
composed	O
of	O
This	O
procedure	O
is	O
repeated	O
until	O
the	O
model	O
reaches	O
N	O
-	O
th	O
layer	O
and	O
only	O
a	O
single	O
node	O
is	O
left	O
.	O
It	O
is	O
notable	O
that	O
the	O
property	O
of	O
selecting	O
the	O
best	O
node	O
pair	O
at	O
each	O
stage	O
resembles	O
that	O
of	O
easy	Method
-	Method
first	Method
parsing	Method
[	O
reference	O
]	O
Figure	O
2	O
:	O
An	O
example	O
of	O
the	O
parent	Method
selection	Method
.	O
At	O
layer	O
t	O
(	O
the	O
bottom	O
layer	O
)	O
,	O
the	O
model	O
computes	O
parent	O
candidates	O
(	O
the	O
middle	O
layer	O
)	O
.	O
Then	O
the	O
validity	Metric
score	Metric
of	O
each	O
candidate	O
is	O
computed	O
using	O
the	O
query	O
vector	O
q	O
(	O
denoted	O
as	O
v	O
1	O
,	O
v	O
2	O
,	O
v	O
3	O
)	O
.	O
In	O
the	O
training	O
time	O
,	O
the	O
model	O
samples	O
a	O
parent	O
node	O
among	O
candidates	O
weighted	O
on	O
v	O
1	O
,	O
v	O
2	O
,	O
v	O
3	O
,	O
using	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
,	O
and	O
in	O
the	O
testing	O
time	O
the	O
model	O
selects	O
the	O
candidate	O
with	O
the	O
highest	O
validity	O
.	O
At	O
layer	O
t	O
+	O
1	O
(	O
the	O
top	O
layer	O
)	O
,	O
the	O
representation	O
of	O
the	O
selected	O
candidate	O
(	O
'	O
the	O
cat	O
'	O
)	O
is	O
used	O
as	O
a	O
parent	O
,	O
and	O
the	O
rest	O
are	O
copied	O
from	O
those	O
of	O
layer	O
t	O
(	O
'	O
sat	O
'	O
,	O
'	O
on	O
'	O
)	O
.	O
Best	O
viewed	O
in	O
color	O
.	O
Parent	Method
selection	Method
.	O
Since	O
information	O
about	O
the	O
tree	O
structure	O
of	O
an	O
input	O
is	O
not	O
given	O
to	O
the	O
model	O
,	O
a	O
special	O
mechanism	O
is	O
needed	O
for	O
the	O
model	O
to	O
learn	O
to	O
compose	O
taskspecific	O
tree	O
structures	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
We	O
now	O
describe	O
the	O
mechanism	O
for	O
building	O
up	O
the	O
tree	O
structure	O
from	O
an	O
unstructured	O
sentence	O
.	O
First	O
,	O
our	O
model	O
introduces	O
the	O
trainable	O
composition	O
query	O
vector	O
q	O
∈	O
R	O
D	O
h	O
.	O
The	O
composition	O
query	O
vector	O
measures	O
how	O
valid	O
a	O
representation	O
is	O
.	O
Specifically	O
,	O
the	O
validity	Metric
score	Metric
of	O
a	O
representation	O
r	O
=	O
[	O
h	O
;	O
c	O
]	O
is	O
defined	O
by	O
q	O
·	O
h.	O
At	O
layer	O
t	O
,	O
the	O
model	O
computes	O
candidates	O
for	O
the	O
parent	O
representations	O
using	O
Eqs	O
.	O
1	O
-	O
3	O
:	O
(	O
r	O
In	O
the	O
training	O
phase	O
,	O
the	O
model	O
samples	O
a	O
parent	O
from	O
candidates	O
weighted	O
on	O
v	O
i	O
,	O
using	O
the	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
described	O
above	O
.	O
Since	O
the	O
continuous	O
GumbelSoftmax	O
function	O
is	O
used	O
in	O
the	O
backward	O
pass	O
,	O
the	O
error	O
backpropagation	O
signal	O
safely	O
passes	O
through	O
the	O
sampling	Method
operation	Method
,	O
hence	O
the	O
model	O
is	O
able	O
to	O
learn	O
to	O
construct	O
the	O
task	O
-	O
specific	O
tree	O
structures	O
that	O
minimize	O
the	O
loss	O
by	O
backpropagation	Method
.	O
In	O
the	O
validation	O
(	O
or	O
testing	O
)	O
phase	O
,	O
the	O
model	O
simply	O
selects	O
the	O
parent	O
which	O
maximizes	O
the	O
validity	Metric
score	Metric
.	O
An	O
example	O
of	O
the	O
parent	Task
selection	Task
is	O
depicted	O
in	O
Figure	O
2	O
.	O
LSTM	Method
-	O
based	O
leaf	O
transformation	O
.	O
The	O
basic	O
leaf	Method
transformation	Method
using	O
an	O
affine	Method
transformation	Method
(	O
Eq	O
.	O
10	O
)	O
does	O
not	O
consider	O
information	O
about	O
the	O
entire	O
sentence	O
of	O
an	O
input	O
and	O
thus	O
the	O
parent	Method
selection	Method
is	O
performed	O
based	O
only	O
on	O
local	O
information	O
.	O
SPINN	Method
[	O
reference	O
]	O
)	O
addresses	O
this	O
issue	O
by	O
using	O
the	O
tracking	O
LSTM	Method
which	O
sequentially	O
reads	O
input	O
words	O
.	O
The	O
tracking	O
LSTM	Method
makes	O
the	O
SPINN	Method
model	Method
hybrid	O
,	O
where	O
the	O
model	O
takes	O
advantage	O
of	O
both	O
tree	Method
-	Method
structured	Method
composition	Method
and	O
sequential	O
reading	O
.	O
However	O
,	O
the	O
tracking	O
LSTM	Method
is	O
not	O
applicable	O
to	O
our	O
model	O
,	O
since	O
our	O
model	O
does	O
not	O
use	O
shift	Method
-	Method
reduce	Method
parsing	Method
or	O
maintain	O
a	O
stack	O
.	O
In	O
the	O
tracking	O
LSTM	Method
's	O
stead	O
,	O
our	O
model	O
applies	O
an	O
LSTM	Method
on	O
input	Method
representations	Method
to	O
give	O
information	O
about	O
previous	O
words	O
to	O
each	O
leaf	O
node	O
:	O
where	O
h	O
1	O
0	O
=	O
c	O
1	O
0	O
=	O
0	O
.	O
From	O
the	O
experimental	O
results	O
,	O
we	O
validate	O
that	O
the	O
LSTM	Method
applied	O
to	O
leaf	O
nodes	O
has	O
a	O
substantial	O
gain	O
over	O
the	O
basic	O
leaf	Method
transformer	Method
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
performance	O
of	O
the	O
proposed	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
model	O
on	O
two	O
tasks	O
:	O
natural	Task
language	Task
inference	Task
and	O
sentiment	Task
analysis	Task
.	O
The	O
implementation	O
is	O
made	O
publicly	O
available	O
.	O
section	O
:	O
2	O
The	O
detailed	O
experimental	O
settings	O
are	O
described	O
in	O
the	O
supplementary	O
material	O
.	O
section	O
:	O
Natural	Task
Language	Task
Inference	Task
Natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
is	O
a	O
task	O
of	O
predicting	O
the	O
relationship	O
between	O
two	O
sentences	O
(	O
hypothesis	O
and	O
premise	O
)	O
.	O
In	O
the	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	Material
dataset	Material
[	O
reference	O
]	O
)	O
,	O
which	O
we	O
use	O
for	O
NLI	Task
experiments	Task
,	O
a	O
relationship	O
is	O
either	O
contradiction	O
,	O
entailment	O
,	O
or	O
neutral	O
.	O
For	O
a	O
model	O
to	O
correctly	O
predict	O
the	O
relationship	O
between	O
two	O
sentences	O
,	O
it	O
should	O
encode	O
semantics	O
of	O
sentences	O
accurately	O
,	O
thus	O
the	O
task	O
has	O
been	O
used	O
as	O
one	O
of	O
standard	O
tasks	O
for	O
evaluating	O
the	O
quality	O
of	O
sentence	Method
representations	Method
.	O
The	O
SNLI	Material
dataset	Material
is	O
composed	O
of	O
about	O
550	O
,	O
000	O
sentences	O
,	O
each	O
of	O
which	O
is	O
binary	O
-	O
parsed	O
.	O
However	O
,	O
since	O
our	O
model	O
operate	O
on	O
plain	O
text	O
,	O
we	O
do	O
not	O
use	O
the	O
parse	O
tree	O
information	O
in	O
both	O
training	O
and	O
testing	O
.	O
The	O
classifier	Method
architecture	Method
used	O
in	O
our	O
SNLI	Material
experiments	O
follows	O
[	O
reference	O
][	O
reference	O
]	O
.	O
Given	O
the	O
premise	O
sentence	O
vector	O
(	O
h	O
pre	O
)	O
and	O
the	O
hypothesis	O
sentence	O
vector	O
(	O
h	O
hyp	O
)	O
which	O
are	O
encoded	O
by	O
the	O
proposed	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
model	O
,	O
the	O
probability	O
of	O
relationship	O
r	O
∈	O
{	O
entailment	O
,	O
contradiction	O
,	O
neutral	O
}	O
is	O
computed	O
by	O
the	O
following	O
equations	O
:	O
[	O
reference	O
]	O
followed	O
by	O
dropout	O
[	O
reference	O
]	O
)	O
with	O
probability	O
0.1	O
to	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
MLP	Method
.	O
We	O
also	O
apply	O
dropout	Method
on	O
the	O
word	O
vectors	O
with	O
probability	O
0.1	O
.	O
Similar	O
to	O
100D	O
experiments	O
,	O
we	O
initialize	O
the	O
word	Method
embedding	Method
matrix	Method
with	O
GloVe	Method
300D	Method
pretrained	Method
vectors	Method
4	O
,	O
however	O
we	O
do	O
not	O
update	O
the	O
word	Method
representations	Method
during	O
training	O
.	O
Since	O
our	O
model	O
converges	O
relatively	O
fast	O
,	O
it	O
is	O
possible	O
to	O
train	O
a	O
model	O
of	O
larger	O
size	O
in	O
a	O
reasonable	O
time	O
.	O
In	O
the	O
600D	O
experiment	O
,	O
we	O
set	O
D	O
x	O
=	O
300	O
,	O
D	O
h	O
=	O
600	O
,	O
and	O
an	O
MLP	Method
with	O
three	O
hidden	Method
layers	Method
(	O
D	O
c	O
=	O
1024	O
)	O
is	O
used	O
.	O
The	O
dropout	O
probability	O
is	O
set	O
to	O
0.2	O
and	O
word	O
embeddings	O
are	O
not	O
updated	O
during	O
training	O
.	O
The	O
size	O
of	O
mini	O
-	O
batches	O
is	O
set	O
to	O
128	O
in	O
all	O
experiments	O
,	O
and	O
hyperparameters	O
are	O
tuned	O
using	O
the	O
validation	O
split	O
.	O
The	O
temperature	O
parameter	O
τ	O
of	O
Gumbel	Method
-	Method
Softmax	Method
is	O
set	O
to	O
1.0	O
,	O
and	O
we	O
did	O
not	O
find	O
that	O
temperature	Method
annealing	Method
improves	O
performance	O
.	O
For	O
training	Method
models	Method
,	O
Adam	Method
optimizer	Method
is	O
used	O
.	O
The	O
results	O
of	O
SNLI	Material
experiments	O
are	O
summarized	O
in	O
Table	O
1	O
.	O
First	O
,	O
we	O
can	O
see	O
that	O
LSTM	Method
-	O
based	O
leaf	O
transformation	O
has	O
a	O
clear	O
advantage	O
over	O
the	O
affine	Method
-	Method
transformation	Method
-	Method
based	Method
one	Method
.	O
It	O
improves	O
the	O
performance	O
substantially	O
and	O
also	O
leads	O
to	O
faster	O
convergence	Metric
.	O
Secondly	O
,	O
comparing	O
ours	O
with	O
other	O
models	O
,	O
we	O
find	O
that	O
our	O
100D	Method
and	Method
300D	Method
model	Method
outperform	O
all	O
other	O
models	O
of	O
similar	O
numbers	O
of	O
parameters	O
.	O
Our	O
600D	Method
model	Method
achieves	O
the	O
accuracy	Metric
of	O
86.0	O
%	O
,	O
which	O
is	O
comparable	O
to	O
that	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
[	O
reference	O
]	O
,	O
while	O
using	O
far	O
less	O
parameters	O
.	O
It	O
is	O
also	O
worth	O
noting	O
that	O
our	O
models	O
converge	O
much	O
faster	O
than	O
other	O
models	O
.	O
All	O
of	O
our	O
models	O
converged	O
within	O
a	O
few	O
hours	O
on	O
a	O
machine	O
with	O
NVIDIA	O
Titan	O
Xp	O
GPU	O
.	O
We	O
also	O
plot	O
validation	Metric
accuracies	Metric
of	O
various	O
models	O
during	O
first	O
5	O
training	O
epochs	O
in	O
Figure	O
3	O
,	O
and	O
validate	O
that	O
our	O
Model	O
SST	Method
-	Method
2	Method
(	O
%	O
)	O
SST	Method
-	Method
5	Method
(	O
%	O
)	O
DMN	Method
[	O
reference	O
]	O
88.6	O
52.1	O
NSE	Method
[	O
reference	O
]	O
89.7	O
52.8	O
byte	O
-	O
mLSTM	Method
[	O
reference	O
]	O
91.8	O
52.9	O
BCN	O
+	O
Char	O
+	O
CoVe	O
[	O
reference	O
]	O
90.3	O
53.7	O
RNTN	Method
[	O
reference	O
]	O
85.4	O
45.7	O
Constituency	O
Tree	O
-	O
LSTM	Method
[	O
reference	O
]	O
88.0	O
51.0	O
NTI	O
-	O
SLSTM	O
-	O
LSTM	Method
[	O
reference	O
]	O
89.3	O
53.1	O
Latent	O
Syntax	O
Tree	O
-	O
LSTM	Method
86.5	O
-	O
Constituency	O
Tree	O
-	O
LSTM	Method
+	O
Recurrent	O
Dropout	O
[	O
reference	O
]	O
89.4	O
52.3	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
(	O
Ours	O
)	O
90.7	O
53.7	O
Table	O
2	O
:	O
Results	O
of	O
SST	Method
experiments	Method
.	O
The	O
bottom	O
section	O
contains	O
results	O
of	O
RvNN	Method
-	Method
based	Method
models	Method
.	O
Underlined	O
score	O
indicates	O
the	O
best	O
among	O
RvNN	Method
-	Method
based	Method
models	Method
.	O
models	O
converge	O
significantly	O
faster	O
than	O
others	O
,	O
not	O
only	O
in	O
terms	O
of	O
total	Metric
training	Metric
time	Metric
but	O
also	O
in	O
the	O
number	O
of	O
iterations	O
.	O
section	O
:	O
5	O
section	O
:	O
Sentiment	Task
Analysis	Task
To	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
in	O
single	Task
-	Task
sentence	Task
classification	Task
,	O
we	O
conducted	O
experiments	O
on	O
Stanford	O
Sentiment	O
Treebank	O
(	O
SST	O
)	O
[	O
reference	O
]	O
dataset	O
.	O
In	O
the	O
SST	O
dataset	O
,	O
each	O
sentence	O
is	O
represented	O
as	O
a	O
binary	O
parse	O
tree	O
,	O
and	O
each	O
subtree	O
of	O
a	O
parse	O
tree	O
is	O
annotated	O
with	O
the	O
corresponding	O
sentiment	O
score	O
.	O
Following	O
the	O
experimental	O
setting	O
of	O
previous	O
works	O
,	O
we	O
use	O
all	O
subtrees	O
and	O
their	O
labels	O
for	O
training	O
,	O
and	O
only	O
the	O
root	O
labels	O
are	O
used	O
for	O
evaluation	Task
.	O
The	O
classifier	Method
has	O
a	O
similar	O
architecture	O
to	O
SNLI	Material
experiments	Material
.	O
Specifically	O
,	O
for	O
a	O
sentence	O
embedding	O
h	O
,	O
the	O
probability	O
for	O
the	O
sentence	O
to	O
be	O
predicted	O
as	O
label	O
s	O
∈	O
{	O
0	O
,	O
1	O
}	O
(	O
in	O
the	O
binary	O
setting	O
,	O
SST	O
-	O
2	O
)	O
or	O
s	O
∈	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
(	O
in	O
the	O
fine	O
-	O
grained	O
setting	O
,	O
SST	O
-	O
5	O
)	O
is	O
computed	O
as	O
follows	O
:	O
,	O
and	O
Φ	Method
is	O
a	O
single	Method
-	Method
hidden	Method
layer	Method
MLP	Method
with	O
the	O
ReLU	Method
activation	Method
function	Method
.	O
Note	O
that	O
subtrees	O
labeled	O
as	O
neutral	O
are	O
ignored	O
in	O
the	O
binary	O
setting	O
in	O
both	O
training	Task
and	O
evaluation	Task
.	O
We	O
trained	O
our	O
SST	Method
-	Method
2	Method
model	Method
with	O
hyperparameters	O
D	O
x	O
=	O
300	O
,	O
D	O
h	O
=	O
300	O
,	O
D	O
c	O
=	O
300	O
.	O
The	O
word	O
vectors	O
are	O
initialized	O
with	O
GloVe	O
300D	O
pretrained	O
vectors	O
and	O
fine	O
-	O
tuned	O
during	O
training	O
.	O
We	O
apply	O
dropout	Method
(	O
p	O
=	O
0.5	O
)	O
on	O
the	O
output	O
of	O
the	O
word	Method
embedding	Method
layer	Method
and	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
MLP	Method
layer	Method
.	O
The	O
size	O
of	O
mini	O
-	O
batches	O
is	O
set	O
to	O
32	O
and	O
Adadelta	Method
optimizer	Method
is	O
used	O
for	O
optimization	Task
.	O
For	O
our	O
SST	Method
-	Method
5	Method
model	Method
,	O
hyperparameters	O
are	O
set	O
to	O
D	O
x	O
=	O
300	O
,	O
D	O
h	O
=	O
300	O
,	O
D	O
c	O
=	O
1024	O
.	O
Similar	O
to	O
the	O
SST	Method
-	Method
2	Method
model	Method
,	O
we	O
optimize	O
the	O
model	O
using	O
Adadelta	Method
optimizer	Method
with	O
batch	O
size	O
64	O
and	O
apply	O
dropout	Method
with	O
p	O
=	O
0.5	O
.	O
Table	O
2	O
summarizes	O
the	O
results	O
of	O
SST	O
experiments	O
.	O
Our	O
SST	Method
-	Method
2	Method
model	Method
outperforms	O
all	O
other	O
models	O
substantially	O
[	O
reference	O
]	O
In	O
the	O
figure	O
,	O
our	O
models	O
and	O
300D	Method
NSE	Method
are	O
trained	O
with	O
batch	O
size	O
128	O
.	O
100D	O
CYK	Method
and	O
300D	Method
SPINN	Method
are	O
trained	O
with	O
batch	O
size	O
16	O
and	O
32	O
respectively	O
,	O
as	O
in	O
the	O
original	O
papers	O
.	O
We	O
observed	O
that	O
our	O
models	O
still	O
converge	O
faster	O
than	O
others	O
when	O
a	O
smaller	O
batch	O
size	O
(	O
16	O
or	O
32	O
)	O
is	O
used	O
.	O
except	O
byte	O
-	O
mLSTM	Method
(	O
Radford	O
,	O
Jozefowicz	O
,	O
and	O
Sutskever	O
2017	O
)	O
,	O
where	O
a	O
byte	Method
-	Method
level	Method
language	Method
model	Method
trained	O
on	O
the	O
large	O
product	O
review	O
dataset	O
is	O
used	O
to	O
obtain	O
sentence	Method
representations	Method
.	O
We	O
also	O
see	O
that	O
the	O
performance	O
of	O
our	O
SST	Method
-	Method
5	Method
model	Method
is	O
on	O
par	O
with	O
that	O
of	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
[	O
reference	O
]	O
,	O
which	O
is	O
pretrained	O
on	O
large	O
parallel	O
datasets	O
and	O
uses	O
character	O
n	O
-	O
gram	O
embeddings	O
alongside	O
word	O
embeddings	O
,	O
even	O
though	O
our	O
model	O
does	O
not	O
utilize	O
external	O
resources	O
other	O
than	O
GloVe	O
vectors	O
and	O
only	O
uses	O
wordlevel	Method
representations	Method
.	O
The	O
authors	O
of	O
[	O
reference	O
]	O
stated	O
that	O
utilizing	O
pretraining	Method
and	Method
character	Method
n	Method
-	Method
gram	Method
embeddings	Method
improves	O
validation	O
accuracy	Metric
by	O
2.8	O
%	O
(	O
SST	Method
-	Method
2	Method
)	O
or	O
1.7	O
%	O
(	O
SST	Method
-	Method
5	Method
)	O
.	O
In	O
addition	O
,	O
from	O
the	O
fact	O
that	O
our	O
models	O
substantially	O
outperform	O
all	O
other	O
RvNN	Method
-	Method
based	Method
models	Method
,	O
we	O
conjecture	O
that	O
task	O
-	O
specific	O
tree	O
structures	O
built	O
by	O
our	O
model	O
help	O
encode	O
sentences	O
into	O
vectors	O
more	O
efficiently	O
than	O
constituency	Method
-	Method
based	Method
or	Method
dependency	Method
-	Method
based	Method
parse	Method
trees	Method
do	O
.	O
section	O
:	O
Qualitative	Task
Analysis	Task
We	O
conduct	O
a	O
set	O
of	O
experiments	O
to	O
observe	O
various	O
properties	O
of	O
our	O
trained	O
models	O
.	O
First	O
,	O
to	O
see	O
how	O
well	O
the	O
model	O
encodes	O
sentences	O
with	O
similar	O
meaning	O
or	O
syntax	O
into	O
close	O
vectors	O
,	O
we	O
find	O
nearest	O
neighbors	O
of	O
a	O
query	O
sentence	O
.	O
Second	O
,	O
to	O
validate	O
that	O
the	O
trained	O
composition	Method
functions	Method
are	O
non	O
-	O
trivial	O
and	O
task	O
-	O
specific	O
,	O
we	O
visualize	O
trees	O
composed	O
by	O
SNLI	Material
and	O
SST	O
model	O
given	O
identical	O
sentence	O
.	O
Nearest	O
neighbors	O
We	O
encode	O
sentences	O
in	O
the	O
test	O
split	O
of	O
SNLI	Material
dataset	Material
using	O
the	O
trained	O
300D	Method
model	Method
and	O
find	O
nearest	O
neighbors	O
given	O
a	O
query	O
sentence	O
.	O
Table	O
3	O
presents	O
five	O
nearest	O
neighbors	O
for	O
each	O
selected	O
query	O
sentence	O
.	O
In	O
finding	Task
nearest	Task
neighbors	Task
,	O
cosine	Metric
distance	Metric
is	O
used	O
as	O
metric	O
.	O
The	O
result	O
shows	O
that	O
our	O
model	O
effectively	O
maps	O
similar	O
sentences	O
into	O
vectors	O
close	O
to	O
each	O
other	O
;	O
the	O
neighboring	O
sentences	O
are	O
similar	O
to	O
a	O
query	O
sentence	O
not	O
only	O
in	O
terms	O
of	O
word	O
overlap	O
,	O
but	O
also	O
in	O
semantics	O
.	O
For	O
example	O
in	O
the	O
second	O
column	O
,	O
the	O
nearest	O
sentence	O
is	O
'	O
the	O
woman	O
is	O
looking	O
at	O
a	O
dog	O
'	O
,	O
whose	O
meaning	O
is	O
almost	O
same	O
as	O
the	O
query	O
sentence	O
.	O
We	O
can	O
also	O
see	O
that	O
other	O
neighbors	O
partially	O
share	O
semantics	O
with	O
the	O
query	O
sentence	O
.	O
#	O
sunshine	O
is	O
on	O
a	O
man	O
's	O
face	O
.	O
a	O
girl	O
is	O
staring	O
at	O
a	O
dog	O
.	O
the	O
woman	O
is	O
wearing	O
boots	O
.	O
1	O
a	O
man	O
is	O
walking	O
on	O
sunshine	O
.	O
the	O
woman	O
is	O
looking	O
at	O
a	O
dog	O
.	O
the	O
girl	O
is	O
wearing	O
shoes	O
2	O
a	O
guy	O
is	O
in	O
a	O
hot	O
,	O
sunny	O
place	O
a	O
girl	O
takes	O
a	O
photo	O
of	O
a	O
dog	O
.	O
a	O
person	O
is	O
wearing	O
boots	O
.	O
3	O
a	O
man	O
is	O
working	O
in	O
the	O
sun	O
.	O
a	O
girl	O
is	O
petting	O
her	O
dog	O
.	O
the	O
woman	O
is	O
wearing	O
jeans	O
.	O
4	O
it	O
is	O
sunny	O
.	O
a	O
man	O
is	O
taking	O
a	O
picture	O
of	O
a	O
dog	O
,	O
while	O
a	O
woman	O
watches	O
.	O
a	O
woman	O
wearing	O
sunglasses	O
.	O
5	O
a	O
man	O
enjoys	O
the	O
sun	O
coming	O
through	O
the	O
window	O
.	O
a	O
woman	O
is	O
playing	O
with	O
her	O
dog	O
.	O
the	O
woman	O
is	O
wearing	O
a	O
vest	O
.	O
Tree	O
examples	O
Figure	O
4	O
show	O
that	O
two	O
models	O
(	O
300D	Method
SNLI	Method
and	O
SST	Method
-	Method
2	Method
)	O
generate	O
different	O
tree	O
structures	O
given	O
an	O
identical	O
sentence	O
.	O
In	O
Figure	O
4a	O
and	O
4b	O
,	O
the	O
SNLI	Material
model	Material
groups	O
the	O
phrase	O
'	O
i	O
love	O
this	O
'	O
first	O
,	O
while	O
the	O
SST	Method
model	Method
groups	O
'	O
this	O
very	O
much	O
'	O
first	O
.	O
Figure	O
4c	O
and	O
4d	O
present	O
how	O
differently	O
the	O
two	O
models	O
process	O
a	O
sentence	O
containing	O
relative	O
pronoun	O
'	O
which	O
'	O
.	O
It	O
is	O
intriguing	O
that	O
the	O
models	O
compose	O
visually	O
plausible	O
tree	O
structures	O
,	O
where	O
the	O
sentence	O
is	O
divided	O
into	O
two	O
phrases	O
by	O
relative	O
pronoun	O
,	O
even	O
though	O
they	O
are	O
trained	O
without	O
explicit	O
parse	O
trees	O
.	O
We	O
hypothesize	O
that	O
these	O
examples	O
demonstrate	O
that	O
each	O
model	O
generates	O
a	O
distinct	O
tree	O
structure	O
based	O
on	O
semantic	O
properties	O
of	O
the	O
task	O
and	O
learns	O
non	O
-	O
trivial	O
tree	Method
composition	Method
scheme	Method
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
,	O
a	O
novel	O
Tree	O
-	O
LSTM	Method
-	O
based	O
architecture	O
that	O
learns	O
to	O
compose	O
taskspecific	O
tree	O
structures	O
.	O
Our	O
model	O
introduces	O
the	O
composition	O
query	O
vector	O
to	O
compute	O
validity	O
of	O
the	O
candidate	O
parents	O
and	O
selects	O
the	O
appropriate	O
parent	O
according	O
to	O
validity	O
scores	O
.	O
In	O
training	O
time	O
,	O
the	O
model	O
samples	O
the	O
parent	O
from	O
candidates	O
using	O
ST	Method
Gumbel	Method
-	Method
Softmax	Method
estimator	Method
,	O
hence	O
it	O
is	O
able	O
to	O
be	O
trained	O
by	O
standard	O
backpropagation	Method
while	O
maintaining	O
its	O
property	O
of	O
discretely	O
determining	O
the	O
computation	O
path	O
in	O
forward	Method
propagation	Method
.	O
From	O
experiments	O
,	O
we	O
validate	O
that	O
our	O
model	O
outperforms	O
all	O
other	O
RvNN	Method
models	Method
and	O
is	O
competitive	O
to	O
state	O
-	O
ofthe	O
-	O
art	O
models	O
,	O
and	O
also	O
observed	O
that	O
our	O
model	O
converges	O
faster	O
than	O
other	O
complex	O
models	O
.	O
The	O
result	O
poses	O
an	O
important	O
question	O
:	O
what	O
is	O
the	O
optimal	O
input	O
structure	O
for	O
RvNN	Method
?	O
We	O
empirically	O
showed	O
that	O
the	O
optimal	O
structure	O
might	O
differ	O
per	O
task	O
,	O
and	O
investigating	O
task	O
-	O
specific	O
latent	O
tree	O
structures	O
could	O
be	O
an	O
interesting	O
future	O
research	O
direction	O
.	O
For	O
future	O
work	O
,	O
we	O
plan	O
to	O
apply	O
the	O
core	O
idea	O
beyond	O
sentence	Task
encoding	Task
.	O
The	O
performance	O
could	O
be	O
further	O
improved	O
by	O
applying	O
intra	Method
-	Method
sentence	Method
or	Method
inter	Method
-	Method
sentence	Method
attention	Method
mechanisms	Method
.	O
We	O
also	O
plan	O
to	O
design	O
an	O
architecture	O
that	O
generates	O
sentences	O
using	O
recursive	O
structures	O
.	O
section	O
:	O
Implementation	O
Details	O
Implementation	O
-	O
wise	O
,	O
we	O
used	O
multiple	O
mask	O
matrices	O
in	O
implementing	O
the	O
proposed	O
Gumbel	Method
Tree	Method
-	Method
LSTM	Method
model	O
.	O
Using	O
the	O
mask	O
matrices	O
,	O
Eq	O
.	O
11	O
can	O
be	O
rewritten	O
as	O
a	O
single	O
equation	O
:	O
,	O
and	O
r	O
is	O
a	O
matrix	O
whose	O
columns	O
are	O
r	O
The	O
mask	O
matrices	O
are	O
defined	O
by	O
the	O
following	O
equations	O
.	O
is	O
a	O
vector	O
which	O
will	O
be	O
defined	O
below	O
,	O
and	O
1	O
∈	O
R	O
Mt	O
+	O
1	O
is	O
a	O
vector	O
whose	O
values	O
are	O
all	O
ones	O
.	O
In	O
the	O
forward	O
pass	O
,	O
ȳ	O
1:Mt	O
+	O
1	O
is	O
defined	O
by	O
a	O
one	Method
-	Method
hot	Method
vector	Method
y	Method
ST	Method
1:Mt	Method
+	Method
1	Method
,	O
which	O
is	O
sampled	O
from	O
the	O
categorical	O
distribution	O
of	O
validity	O
scores	O
(	O
v	O
1	O
,	O
·	O
·	O
·	O
,	O
v	O
Mt	O
+	O
1	O
)	O
using	O
Gumbel	Method
-	Method
Max	Method
trick	Method
.	O
is	O
added	O
when	O
calculating	O
g	O
i	O
for	O
numerical	O
stability	O
.	O
In	O
the	O
backward	O
pass	O
,	O
instead	O
of	O
the	O
one	Method
-	Method
hot	Method
version	Method
,	O
the	O
continuous	O
vector	O
y	O
1:Mt	O
+	O
1	O
obtained	O
from	O
Gumbel	Method
-	Method
Softmax	Method
is	O
used	O
asȳ	O
1:Mt	O
+	O
1	O
.	O
Note	O
that	O
the	O
Gumbel	O
noise	O
samples	O
g	O
1	O
,	O
·	O
·	O
·	O
,	O
g	O
Mt	O
+	O
1	O
drawn	O
in	O
the	O
forward	O
pass	O
are	O
reused	O
in	O
the	O
backward	O
pass	O
(	O
i.e.	O
noise	O
values	O
are	O
not	O
resampled	O
in	O
the	O
backward	O
pass	O
)	O
.	O
In	O
typical	O
deep	Method
learning	Method
libraries	Method
supporting	O
automatic	Task
differentiation	Task
(	O
e.g.	O
PyTorch	O
,	O
TensorFlow	O
)	O
,	O
this	O
discrepancy	O
between	O
forward	O
and	O
backward	O
pass	O
can	O
be	O
implemented	O
as	O
y	Method
1:Mt	Method
+	Method
1	Method
=	O
detach	O
(	O
y	O
ST	O
1:Mt	O
+	O
1	O
−	O
y	O
1:Mt	O
+	O
1	O
)	O
+	O
y	O
1:Mt	O
+	O
1	O
,	O
(	O
S11	O
)	O
where	O
detach	O
(	O
·	O
)	O
is	O
a	O
function	O
that	O
prevents	O
error	O
from	O
backpropagating	O
through	O
its	O
input	O
.	O
section	O
:	O
Detailed	O
Experimental	O
Settings	O
All	O
experiments	O
are	O
conducted	O
using	O
the	O
publicized	O
codebase	O
.	O
section	O
:	O
1	O
section	O
:	O
SNLI	Material
The	O
composition	O
query	O
vector	O
is	O
initialized	O
by	O
sampling	O
from	O
Gaussian	Method
distribution	Method
N	Method
(	O
0	O
,	O
0.01	O
2	O
)	O
.	O
The	O
last	O
linear	Method
transformation	Method
that	O
outputs	O
the	O
unnormalized	O
log	O
probability	O
for	O
each	O
class	O
is	O
initialized	O
by	O
sampling	O
from	O
uniform	O
distribution	O
U	O
(	O
−0.005	O
,	O
0.005	O
)	O
.	O
All	O
other	O
parameters	O
are	O
initialized	O
following	O
the	O
scheme	O
proposed	O
by	O
[	O
reference	O
]	O
.	O
We	O
used	O
Adam	Method
optimizer	Method
with	O
default	O
hyperparameters	O
and	O
halved	O
learning	Metric
rate	Metric
if	O
there	O
is	O
no	O
improvement	O
in	O
accuracy	Metric
for	O
one	O
epoch	O
.	O
The	O
size	O
of	O
minibatch	O
is	O
set	O
to	O
128	O
in	O
all	O
experiments	O
.	O
In	O
100D	O
experiments	O
(	O
D	O
x	O
=	O
D	O
h	O
=	O
100	O
,	O
D	O
c	O
=	O
200	O
,	O
single	Method
-	Method
hidden	Method
layer	Method
MLP	Method
classifier	Method
)	O
,	O
GloVe	Method
(	O
6B	O
,	O
100D	O
)	O
pretrained	Method
word	Method
embeddings	Method
are	O
used	O
in	O
initializing	O
word	Method
representations	Method
.	O
We	O
fine	O
-	O
tuned	O
word	O
embedding	O
parameters	O
during	O
training	O
.	O
In	O
300D	O
(	O
D	O
x	O
=	O
D	O
h	O
=	O
300	O
,	O
D	O
c	O
=	O
1024	O
,	O
single	O
-	O
hidden	Method
layer	Method
MLP	Method
classifier	Method
)	O
and	O
600D	O
(	O
D	O
x	O
=	O
300	O
,	O
D	O
h	O
=	O
600	O
,	O
D	O
c	O
=	O
1024	O
,	O
MLP	Method
classifier	Method
with	O
three	O
hidden	O
layers	O
)	O
experiments	O
,	O
GloVe	Method
(	O
840B	O
,	O
300D	O
)	O
pretrained	Method
word	Method
embeddings	Method
are	O
used	O
as	O
word	Method
representations	Method
and	O
fixed	O
during	O
training	O
.	O
Batch	Method
normalization	Method
is	O
applied	O
before	O
the	O
input	O
and	O
after	O
the	O
output	O
of	O
the	O
MLP	Method
.	O
Dropout	Method
is	O
applied	O
to	O
word	O
embeddings	O
and	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
MLP	Method
with	O
dropout	O
probability	O
0.1	O
(	O
300D	O
)	O
or	O
0.2	O
(	O
600D	O
)	O
.	O
section	O
:	O
SST	O
The	O
composition	O
query	O
vector	O
is	O
initialized	O
by	O
sampling	O
from	O
Gaussian	Method
distribution	Method
N	Method
(	O
0	O
,	O
0.01	O
2	O
)	O
.	O
The	O
last	O
linear	Method
transformation	Method
that	O
outputs	O
the	O
unnormalized	O
log	O
probability	O
for	O
each	O
class	O
is	O
initialized	O
by	O
sampling	O
from	O
uniform	O
distribution	O
U	O
(	O
−0.002	O
,	O
0.002	O
)	O
.	O
All	O
other	O
parameters	O
are	O
initialized	O
following	O
the	O
scheme	O
proposed	O
by	O
[	O
reference	O
]	O
.	O
We	O
used	O
Adadelta	Method
optimizer	Method
)	O
with	O
default	O
hyperparameters	O
and	O
halved	O
learning	Metric
rate	Metric
if	O
there	O
is	O
no	O
improvement	O
in	O
accuracy	Metric
for	O
two	O
epochs	O
.	O
In	O
both	O
SST	Method
-	Method
2	Method
and	O
SST	Method
-	Method
5	Method
experiments	O
,	O
we	O
set	O
D	O
x	O
=	O
D	O
h	O
=	O
300	O
,	O
used	O
GloVe	O
(	O
840B	O
,	O
300D	O
)	O
pretrained	O
vectors	O
with	O
fine	Method
-	Method
tuning	Method
,	O
and	O
single	Method
-	Method
hidden	Method
layer	Method
MLP	Method
is	O
used	O
as	O
classifier	Method
.	O
Dropout	Method
is	O
applied	O
to	O
word	O
embeddings	O
and	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
MLP	Method
classifier	Method
with	O
probability	Method
0.5	Method
.	O
In	O
the	O
SST	Task
-	Task
2	Task
experiment	O
,	O
we	O
set	O
D	O
c	O
to	O
300	O
and	O
set	O
batch	O
size	O
to	O
32	O
.	O
In	O
the	O
SST	Method
-	Method
5	Method
experiment	O
,	O
D	O
c	O
is	O
increased	O
to	O
1024	O
,	O
and	O
mini	O
-	O
batches	O
of	O
64	O
sentences	O
are	O
fed	O
to	O
the	O
model	O
during	O
training	O
.	O
section	O
:	O
section	O
:	O
Acknowledgments	O
This	O
work	O
is	O
part	O
of	O
SNU	Task
-	Task
Samsung	Task
smart	Task
campus	Task
research	Task
program	Task
,	O
which	O
is	O
supported	O
by	O
Samsung	O
Electronics	O
.	O
The	O
authors	O
would	O
like	O
to	O
thank	O
anonymous	O
reviewers	O
for	O
valuable	O
comments	O
and	O
Volkan	O
Cirik	O
for	O
helpful	O
feedback	O
on	O
the	O
early	O
version	O
of	O
the	O
manuscript	O
.	O
section	O
:	O
section	O
:	O
Appendix	O
The	O
supplementary	O
material	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
jihunchoi	O
/	O
unsupervised	O
-	O
treelstm	O
/	O
blob	O
/	O
master	O
/	O
aaai18	O
/	O
supp.pdf	O
.	O
section	O
:	O
