document O
: O
A O
C Method
- Method
LSTM Method
Neural Method
Network Method
for O
Text Task
Classification Task
Neural Method
network Method
models Method
have O
been O
demonstrated O
to O
be O
capable O
of O
achieving O
remarkable O
performance O
in O
sentence Task
and Task
document Task
modeling Task
. O
Convolutional Method
neural Method
network Method
( O
CNN Method
) O
and O
recurrent Method
neural Method
network Method
( O
RNN Method
) O
are O
two O
mainstream O
architectures O
for O
such O
modeling Task
tasks Task
, O
which O
adopt O
totally O
different O
ways O
of O
understanding O
natural O
languages O
. O
In O
this O
work O
, O
we O
combine O
the O
strengths O
of O
both O
architectures O
and O
propose O
a O
novel O
and O
unified Method
model Method
called O
C O
- O
LSTM Method
for O
sentence Task
representation Task
and O
text Task
classification Task
. O
C O
- O
LSTM Method
utilizes O
CNN Method
to O
extract O
a O
sequence O
of O
higher Method
- Method
level Method
phrase Method
representations Method
, O
and O
are O
fed O
into O
a O
long O
short O
- O
term O
memory O
recurrent Method
neural Method
network Method
( O
LSTM Method
) O
to O
obtain O
the O
sentence Method
representation Method
. O
C O
- O
LSTM Method
is O
able O
to O
capture O
both O
local O
features O
of O
phrases O
as O
well O
as O
global O
and O
temporal O
sentence O
semantics O
. O
We O
evaluate O
the O
proposed O
architecture O
on O
sentiment Task
classification Task
and O
question Task
classification Task
tasks Task
. O
The O
experimental O
results O
show O
that O
the O
C O
- O
LSTM Method
outperforms O
both O
CNN Method
and O
LSTM Method
and O
can O
achieve O
excellent O
performance O
on O
these O
tasks O
. O
section O
: O
Introduction O
As O
one O
of O
the O
core O
steps O
in O
NLP Task
, O
sentence Task
modeling Task
aims O
at O
representing O
sentences O
as O
meaningful O
features O
for O
tasks O
such O
as O
sentiment Task
classification Task
. O
Traditional O
sentence Method
modeling Method
uses O
the O
bag Method
- Method
of Method
- Method
words Method
model Method
which O
often O
suffers O
from O
the O
curse O
of O
dimensionality O
; O
others O
use O
composition Method
based Method
methods Method
instead O
, O
e.g. O
, O
an O
algebraic Method
operation Method
over O
semantic O
word O
vectors O
to O
produce O
the O
semantic O
sentence O
vector O
. O
However O
, O
such O
methods O
may O
not O
perform O
well O
due O
to O
the O
loss O
of O
word O
order O
information O
. O
More O
recent O
models O
for O
distributed Task
sentence Task
representation Task
fall O
into O
two O
categories O
according O
to O
the O
form O
of O
input O
sentence O
: O
sequence Method
- Method
based Method
models Method
and O
tree Method
- Method
structured Method
models Method
. O
Sequence Method
- Method
based Method
models Method
construct O
sentence Method
representations Method
from O
word O
sequences O
by O
taking O
in O
account O
the O
relationship O
between O
successive O
words O
. O
Tree Method
- Method
structured Method
models Method
treat O
each O
word O
token O
as O
a O
node O
in O
a O
syntactic O
parse O
tree O
and O
learn O
sentence Method
representations Method
from O
leaves O
to O
the O
root O
in O
a O
recursive O
manner O
. O
Convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
and O
recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
have O
emerged O
as O
two O
widely O
used O
architectures O
and O
are O
often O
combined O
with O
sequence Method
- Method
based Method
or Method
tree Method
- Method
structured Method
models Method
. O
Owing O
to O
the O
capability O
of O
capturing O
local O
correlations O
of O
spatial O
or O
temporal O
structures O
, O
CNNs Method
have O
achieved O
top O
performance O
in O
computer Task
vision Task
, O
speech Task
recognition Task
and O
NLP Task
. O
For O
sentence Task
modeling Task
, O
CNNs Method
perform O
excellently O
in O
extracting O
n O
- O
gram O
features O
at O
different O
positions O
of O
a O
sentence O
through O
convolutional Method
filters Method
, O
and O
can O
learn O
short O
and O
long O
- O
range O
relations O
through O
pooling Method
operations Method
. O
CNNs Method
have O
been O
successfully O
combined O
with O
both O
sequence Method
- Method
based Method
model Method
and O
tree Method
- Method
structured Method
model Method
in O
sentence Task
modeling Task
. O
The O
other O
popular O
neural Method
network Method
architecture Method
– O
RNN Method
– O
is O
able O
to O
handle O
sequences O
of O
any O
length O
and O
capture O
long O
- O
term O
dependencies O
. O
To O
avoid O
the O
problem O
of O
gradient Task
exploding Task
or Task
vanishing Task
in O
the O
standard O
RNN Method
, O
Long O
Short O
- O
term O
Memory O
RNN Method
( O
LSTM Method
) O
and O
other O
variants O
were O
designed O
for O
better O
remembering Task
and Task
memory Task
accesses Task
. O
Along O
with O
the O
sequence Method
- Method
based Method
or O
the O
tree Method
- Method
structured Method
models Method
, O
RNNs Method
have O
achieved O
remarkable O
results O
in O
sentence Task
or Task
document Task
modeling Task
. O
To O
conclude O
, O
CNN Method
is O
able O
to O
learn O
local O
response O
from O
temporal O
or O
spatial O
data O
but O
lacks O
the O
ability O
of O
learning O
sequential O
correlations O
; O
on O
the O
other O
hand O
, O
RNN Method
is O
specilized O
for O
sequential Task
modelling Task
but O
unable O
to O
extract O
features O
in O
a O
parallel O
way O
. O
It O
has O
been O
shown O
that O
higher O
- O
level O
modeling O
of O
can O
help O
to O
disentangle O
underlying O
factors O
of O
variation O
within O
the O
input O
, O
which O
should O
then O
make O
it O
easier O
to O
learn O
temporal O
structure O
between O
successive O
time O
steps O
. O
For O
example O
, O
Sainath O
et O
al O
. O
have O
obtained O
respectable O
improvements O
in O
WER Metric
by O
learning O
a O
deep Method
LSTM Method
from O
multi O
- O
scale O
inputs O
. O
We O
explore O
training O
the O
LSTM Method
model Method
directly O
from O
sequences O
of O
higher Method
- Method
level Method
representaions Method
while O
preserving O
the O
sequence O
order O
of O
these O
representaions O
. O
In O
this O
paper O
, O
we O
introduce O
a O
new O
architecture O
short O
for O
C O
- O
LSTM Method
by O
combining O
CNN Method
and O
LSTM Method
to O
model O
sentences O
. O
To O
benefit O
from O
the O
advantages O
of O
both O
CNN Method
and O
RNN Method
, O
we O
design O
a O
simple O
end O
- O
to O
- O
end O
, O
unified Method
architecture Method
by O
feeding O
the O
output O
of O
a O
one O
- O
layer O
CNN Method
into O
LSTM Method
. O
The O
CNN Method
is O
constructed O
on O
top O
of O
the O
pre O
- O
trained O
word O
vectors O
from O
massive O
unlabeled O
text O
data O
to O
learn O
higher O
- O
level Method
representions Method
of Method
n Method
- Method
grams Method
. O
Then O
to O
learn O
sequential O
correlations O
from O
higher O
- O
level O
suqence O
representations O
, O
the O
feature O
maps O
of O
CNN Method
are O
organized O
as O
sequential O
window O
features O
to O
serve O
as O
the O
input O
of O
LSTM Method
. O
In O
this O
way O
, O
instead O
of O
constructing O
LSTM Method
directly O
from O
the O
input O
sentence O
, O
we O
first O
transform O
each O
sentence O
into O
successive O
window O
( O
n O
- O
gram O
) O
features O
to O
help O
disentangle O
factors O
of O
variations O
within O
sentences O
. O
We O
choose O
sequence O
- O
based O
input O
other O
than O
relying O
on O
the O
syntactic O
parse O
trees O
before O
feeding O
in O
the O
neural Method
network Method
, O
thus O
our O
model O
does O
n’t O
rely O
on O
any O
external O
language O
knowledge O
and O
complicated O
pre Method
- Method
processing Method
. O
In O
our O
experiments O
, O
we O
evaluate O
the O
semantic Method
sentence Method
representations Method
learned O
from O
C O
- O
LSTM Method
with O
two O
tasks O
: O
sentiment Task
classification Task
and O
6 Task
- Task
way Task
question Task
classification Task
. O
Our O
evaluations O
show O
that O
the O
C O
- O
LSTM Method
model O
can O
achieve O
excellent O
results O
with O
several O
benchmarks O
as O
compared O
with O
a O
wide O
range O
of O
baseline O
models O
. O
We O
also O
show O
that O
the O
combination O
of O
CNN Method
and O
LSTM Method
outperforms O
individual O
multi O
- O
layer O
CNN Method
models O
and O
RNN Method
models O
, O
which O
indicates O
that O
LSTM Method
can O
learn O
long O
- O
term O
dependencies O
from O
sequences O
of O
higher O
- O
level O
representations O
better O
than O
the O
other O
models O
. O
section O
: O
Related O
Work O
Deep Method
learning Method
based Method
neural Method
network Method
models Method
have O
achieved O
great O
success O
in O
many O
NLP Task
tasks Task
, O
including O
learning Task
distributed Task
word Task
, Task
sentence Task
and Task
document Task
representation Task
, O
parsing Task
, O
statistical Task
machine Task
translation Task
, O
sentiment Task
classification Task
, O
etc O
. O
Learning O
distributed Method
sentence Method
representation Method
through O
neural Method
network Method
models Method
requires O
little O
external O
domain O
knowledge O
and O
can O
reach O
satisfactory O
results O
in O
related O
tasks O
like O
sentiment Task
classification Task
, O
text Task
categorization Task
. O
In O
many O
recent O
sentence Task
representation Task
learning Task
works O
, O
neural Method
network Method
models Method
are O
constructed O
upon O
either O
the O
input O
word O
sequences O
or O
the O
transformed O
syntactic O
parse O
tree O
. O
Among O
them O
, O
convolutional Method
neural Method
network Method
( O
CNN Method
) O
and O
recurrent Method
neural Method
network Method
( O
RNN Method
) O
are O
two O
popular O
ones O
. O
The O
capability O
of O
capturing O
local O
correlations O
along O
with O
extracting O
higher O
- O
level O
correlations O
through O
pooling Method
empowers O
CNN Method
to O
model O
sentences O
naturally O
from O
consecutive O
context O
windows O
. O
In O
, O
Collobert O
et O
al O
. O
applied O
convolutional Method
filters Method
to O
successive O
windows O
for O
a O
given O
sequence O
to O
extract O
global O
features O
by O
max Method
- Method
pooling Method
. O
As O
a O
slight O
variant O
, O
Kim O
et O
al O
. O
kim O
proposed O
a O
CNN Method
architecture O
with O
multiple O
filters O
( O
with O
a O
varying O
window O
size O
) O
and O
two O
‘ O
channels O
’ O
of O
word O
vectors O
. O
To O
capture O
word O
relations O
of O
varying O
sizes O
, O
Kalchbrenner O
et O
al O
. O
dcnn Method
proposed O
a O
dynamic Method
k Method
- Method
max Method
pooling Method
mechanism Method
. O
In O
a O
more O
recent O
work O
, O
Tao O
et O
al O
. O
apply O
tensor Method
- Method
based Method
operations Method
between O
words O
to O
replace O
linear O
operations O
on O
concatenated O
word O
vectors O
in O
the O
standard O
convolutional Method
layer Method
and O
explore O
the O
non O
- O
linear O
interactions O
between O
nonconsective O
n O
- O
grams O
. O
Mou O
et O
al O
. O
mou Method
also O
explores O
convolutional Method
models Method
on O
tree O
- O
structured O
sentences O
. O
As O
a O
sequence Method
model Method
, O
RNN Method
is O
able O
to O
deal O
with O
variable O
- O
length O
input O
sequences O
and O
discover O
long O
- O
term O
dependencies O
. O
Various O
variants O
of O
RNN Method
have O
been O
proposed O
to O
better O
store O
and O
access O
memories O
. O
With O
the O
ability O
of O
explicitly O
modeling O
time O
- O
series O
data O
, O
RNNs Method
are O
being O
increasingly O
applied O
to O
sentence Task
modeling Task
. O
For O
example O
, O
Tai O
et O
al O
. O
tai2015 O
adjusted O
the O
standard O
LSTM Method
to O
tree O
- O
structured O
topologies O
and O
obtained O
superior O
results O
over O
a O
sequential O
LSTM Method
on O
related O
tasks O
. O
In O
this O
paper O
, O
we O
stack O
CNN Method
and O
LSTM Method
in O
a O
unified Method
architecture Method
for O
semantic Task
sentence Task
modeling Task
. O
The O
combination O
of O
CNN Method
and O
LSTM Method
can O
be O
seen O
in O
some O
computer Task
vision Task
tasks Task
like O
image Task
caption Task
and O
speech Task
recognition Task
. O
Most O
of O
these O
models O
use O
multi Method
- Method
layer Method
CNNs Method
and O
train O
CNNs Method
and O
RNNs Method
separately O
or O
throw O
the O
output O
of O
a O
fully O
connected O
layer O
of O
CNN Method
into O
RNN Method
as O
inputs O
. O
Our O
approach O
is O
different O
: O
we O
apply O
CNN Method
to O
text O
data O
and O
feed O
consecutive O
window O
features O
directly O
to O
LSTM Method
, O
and O
so O
our O
architecture O
enables O
LSTM Method
to O
learn O
long O
- O
range O
dependencies O
from O
higher O
- O
order O
sequential O
features O
. O
In O
, O
the O
authors O
suggest O
that O
sequence Method
- Method
based Method
models Method
are O
sufficient O
to O
capture O
the O
compositional O
semantics O
for O
many O
NLP Task
tasks Task
, O
thus O
in O
this O
work O
the O
CNN Method
is O
directly O
built O
upon O
word O
sequences O
other O
than O
the O
syntactic O
parse O
tree O
. O
Our O
experiments O
on O
sentiment Task
classification Task
and O
6 Task
- Task
way Task
question Task
classification Task
tasks Task
clearly O
demonstrate O
the O
superiority O
of O
our O
model O
over O
single Method
CNN Method
or O
LSTM Method
model Method
and O
other O
related O
sequence Method
- Method
based Method
models Method
. O
section O
: O
C O
- O
LSTM Method
Model O
The O
architecture O
of O
the O
C O
- O
LSTM Method
model O
is O
shown O
in O
Figure O
1 O
, O
which O
consists O
of O
two O
main O
components O
: O
convolutional Method
neural Method
network Method
( O
CNN Method
) O
and O
long Method
short Method
- Method
term Method
memory Method
network Method
( O
LSTM Method
) O
. O
The O
following O
two O
subsections O
describe O
how O
we O
apply O
CNN Method
to O
extract O
higher O
- O
level O
sequences O
of O
word O
features O
and O
LSTM Method
to O
capture O
long O
- O
term O
dependencies O
over O
window O
feature O
sequences O
respectively O
. O
subsection O
: O
N Task
- Task
gram Task
Feature Task
Extraction Task
through O
Convolution Method
The O
one Method
- Method
dimensional Method
convolution Method
involves O
a O
filter Method
vector Method
sliding O
over O
a O
sequence O
and O
detecting O
features O
at O
different O
positions O
. O
Let O
be O
the O
- O
dimensional O
word O
vectors O
for O
the O
- O
th O
word O
in O
a O
sentence O
. O
Let O
denote O
the O
input O
sentence O
where O
is O
the O
length O
of O
the O
sentence O
. O
Let O
be O
the O
length O
of O
the O
filter O
, O
and O
the O
vector O
is O
a O
filter Method
for O
the O
convolution Method
operation Method
. O
For O
each O
position O
in O
the O
sentence O
, O
we O
have O
a O
window O
vector O
with O
consecutive O
word O
vectors O
, O
denoted O
as O
: O
Here O
, O
the O
commas O
represent O
row Method
vector Method
concatenation Method
. O
A O
filter Method
convolves O
with O
the O
window O
vectors O
( O
- O
grams O
) O
at O
each O
position O
in O
a O
valid O
way O
to O
generate O
a O
feature O
map O
; O
each O
element O
of O
the O
feature O
map O
for O
window O
vector O
is O
produced O
as O
follows O
: O
where O
is O
element O
- O
wise O
multiplication O
, O
is O
a O
bias O
term O
and O
is O
a O
nonlinear O
transformation O
function O
that O
can O
be O
sigmoid O
, O
hyperbolic O
tangent O
, O
etc O
. O
In O
our O
case O
, O
we O
choose O
ReLU Method
as O
the O
nonlinear O
function O
. O
The O
C O
- O
LSTM Method
model O
uses O
multiple O
filters Method
to O
generate O
multiple O
feature O
maps O
. O
For O
filters O
with O
the O
same O
length O
, O
the O
generated O
feature O
maps O
can O
be O
rearranged O
as O
feature Method
representations Method
for O
each O
window O
, O
Here O
, O
semicolons O
represent O
column O
vector O
concatenation O
and O
is O
the O
feature O
map O
generated O
with O
the O
- Method
th Method
filter Method
. O
Each O
row O
of O
is O
the O
new O
feature Method
representation Method
generated O
from O
filters Method
for O
the O
window O
vector O
at O
position O
. O
The O
new O
successive O
higher Method
- Method
order Method
window Method
representations Method
then O
are O
fed O
into O
LSTM Method
which O
is O
described O
below O
. O
A O
max Method
- Method
over Method
- Method
pooling Method
or O
dynamic Method
k Method
- Method
max Method
pooling Method
is O
often O
applied O
to O
feature O
maps O
after O
the O
convolution Method
to O
select O
the O
most O
or O
the O
k O
- O
most O
important O
features O
. O
However O
, O
LSTM Method
is O
specified O
for O
sequence O
input O
, O
and O
pooling Task
will O
break O
such O
sequence Method
organization Method
due O
to O
the O
discontinuous O
selected O
features O
. O
Since O
we O
stack O
an O
LSTM Method
neural O
neural O
network O
on O
top O
of O
the O
CNN Method
, O
we O
will O
not O
apply O
pooling Method
after O
the O
convolution Method
operation Method
. O
subsection O
: O
Long Task
Short Task
- Task
Term Task
Memory Task
Networks Task
Recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
are O
able O
to O
propagate O
historical O
information O
via O
a O
chain Method
- Method
like Method
neural Method
network Method
architecture Method
. O
While O
processing O
sequential O
data O
, O
it O
looks O
at O
the O
current O
input O
as O
well O
as O
the O
previous O
output O
of O
hidden O
state O
at O
each O
time O
step O
. O
However O
, O
standard O
RNNs Method
becomes O
unable O
to O
learn O
long O
- O
term O
dependencies O
as O
the O
gap O
between O
two O
time O
steps O
becomes O
large O
. O
To O
address O
this O
issue O
, O
LSTM Method
was O
first O
introduced O
in O
and O
re O
- O
emerged O
as O
a O
successful O
architecture O
since O
Ilya O
et O
al O
. O
seq O
obtained O
remarkable O
performance O
in O
statistical Task
machine Task
translation Task
. O
Although O
many O
variants O
of O
LSTM Method
were O
proposed O
, O
we O
adopt O
the O
standard O
architecture O
in O
this O
work O
. O
The O
LSTM Method
architecture O
has O
a O
range O
of O
repeated Method
modules Method
for O
each O
time O
step O
as O
in O
a O
standard O
RNN Method
. O
At O
each O
time O
step O
, O
the O
output O
of O
the O
module O
is O
controlled O
by O
a O
set O
of O
gates O
in O
as O
a O
function O
of O
the O
old O
hidden O
state O
and O
the O
input O
at O
the O
current O
time O
step O
: O
the O
forget O
gate O
, O
the O
input O
gate O
, O
and O
the O
output O
gate O
. O
These O
gates O
collectively O
decide O
how O
to O
update O
the O
current O
memory O
cell O
and O
the O
current O
hidden O
state O
. O
We O
use O
to O
denote O
the O
memory O
dimension O
in O
the O
LSTM Method
and O
all O
vectors O
in O
this O
architecture O
share O
the O
same O
dimension O
. O
The O
LSTM Method
transition O
functions O
are O
defined O
as O
follows O
: O
Here O
, O
is O
the O
logistic Method
sigmoid Method
function Method
that O
has O
an O
output O
in O
, O
denotes O
the O
hyperbolic Method
tangent Method
function Method
that O
has O
an O
output O
in O
, O
and O
denotes O
the O
elementwise O
multiplication O
. O
To O
understand O
the O
mechanism O
behind O
the O
architecture O
, O
we O
can O
view O
as O
the O
function O
to O
control O
to O
what O
extent O
the O
information O
from O
the O
old O
memory O
cell O
is O
going O
to O
be O
thrown O
away O
, O
to O
control O
how O
much O
new O
information O
is O
going O
to O
be O
stored O
in O
the O
current O
memory O
cell O
, O
and O
to O
control O
what O
to O
output O
based O
on O
the O
memory O
cell O
. O
LSTM Method
is O
explicitly O
designed O
for O
time O
- O
series O
data O
for O
learning Task
long Task
- Task
term Task
dependencies Task
, O
and O
therefore O
we O
choose O
LSTM Method
upon O
the O
convolution Method
layer Method
to O
learn O
such O
dependencies O
in O
the O
sequence O
of O
higher O
- O
level O
features O
. O
section O
: O
Learning O
C O
- O
LSTM Method
for O
Text Task
Classification Task
For O
text Task
classification Task
, O
we O
regard O
the O
output O
of O
the O
hidden O
state O
at O
the O
last O
time O
step O
of O
LSTM Method
as O
the O
document Method
representation Method
and O
we O
add O
a O
softmax Method
layer Method
on O
top O
. O
We O
train O
the O
entire O
model O
by O
minimizing O
the O
cross Metric
- Metric
entropy Metric
error Metric
. O
Given O
a O
training O
sample O
and O
its O
true O
label O
where O
is O
the O
number O
of O
possible O
labels O
and O
the O
estimated O
probabilities O
for O
each O
label O
, O
the O
error Metric
is O
defined O
as O
: O
where O
is O
an O
indicator O
such O
that O
otherwise O
. O
We O
employ O
stochastic Method
gradient Method
descent Method
( O
SGD Method
) O
to O
learn O
the O
model O
parameters O
and O
adopt O
the O
optimizer Method
RMSprop Method
. O
subsection O
: O
Padding Task
and O
Word Task
Vector Task
Initialization Task
First O
, O
we O
use O
to O
denote O
the O
maximum O
length O
of O
the O
sentence O
in O
the O
training O
set O
. O
As O
the O
convolution Method
layer Method
in O
our O
model O
requires O
fixed O
- O
length O
input O
, O
we O
pad O
each O
sentence O
that O
has O
a O
length O
less O
than O
with O
special O
symbols O
at O
the O
end O
that O
indicate O
the O
unknown O
words O
. O
For O
a O
sentence O
in O
the O
test O
dataset O
, O
we O
pad O
sentences O
that O
are O
shorter O
than O
in O
the O
same O
way O
, O
but O
for O
sentences O
that O
have O
a O
length O
longer O
than O
, O
we O
simply O
cut O
extra O
words O
at O
the O
end O
of O
these O
sentences O
to O
reach O
. O
We O
initialize O
word O
vectors O
with O
the O
publicly O
available O
word2vec O
vectors O
that O
are O
pre O
- O
trained O
using O
about O
100B O
words O
from O
the O
Google O
News O
Dataset O
. O
The O
dimensionality O
of O
the O
word O
vectors O
is O
300 O
. O
We O
also O
initialize O
the O
word O
vector O
for O
the O
unknown O
words O
from O
the O
uniform O
distribution O
[ O
- O
0.25 O
, O
0.25 O
] O
. O
We O
then O
fine O
- O
tune O
the O
word O
vectors O
along O
with O
other O
model O
parameters O
during O
training O
. O
subsection O
: O
Regularization Task
For O
regularization Task
, O
we O
employ O
two O
commonly O
used O
techniques O
: O
dropout Method
and O
L2 Method
weight Method
regularization Method
. O
We O
apply O
dropout Method
to O
prevent O
co Task
- Task
adaptation Task
. O
In O
our O
model O
, O
we O
either O
apply O
dropout Method
to O
word O
vectors O
before O
feeding O
the O
sequence O
of O
words O
into O
the O
convolutional Method
layer Method
or O
to O
the O
output O
of O
LSTM Method
before O
the O
softmax Method
layer Method
. O
The O
L2 Method
regularization Method
is O
applied O
to O
the O
weight O
of O
the O
softmax O
layer O
. O
section O
: O
Experiments O
We O
evaluate O
the O
C O
- O
LSTM Method
model O
on O
two O
tasks O
: O
( O
1 O
) O
sentiment Task
classification Task
, O
and O
( O
2 O
) O
question Task
type Task
classification Task
. O
In O
this O
section O
, O
we O
introduce O
the O
datasets O
and O
the O
experimental O
settings O
. O
subsection O
: O
Datasets O
Sentiment Task
Classification Task
: O
Our O
task O
in O
this O
regard O
is O
to O
predict O
the O
sentiment Task
polarity Task
of Task
movie Task
reviews Task
. O
We O
use O
the O
Stanford Material
Sentiment Material
Treebank Material
( O
SST Material
) O
benchmark O
. O
This O
dataset O
consists O
of O
11855 O
movie O
reviews O
and O
are O
split O
into O
train O
( O
8544 O
) O
, O
dev O
( O
1101 O
) O
, O
and O
test O
( O
2210 O
) O
. O
Sentences O
in O
this O
corpus O
are O
parsed O
and O
all O
phrases O
along O
with O
the O
sentences O
are O
fully O
annotated O
with O
5 O
labels O
: O
very O
positive O
, O
positive O
, O
neural O
, O
negative O
, O
very O
negative O
. O
We O
consider O
two O
classification Task
tasks Task
on O
this O
dataset O
: O
fine Task
- Task
grained Task
classification Task
with O
5 O
labels O
and O
binary Task
classification Task
by O
removing O
neural O
labels O
. O
For O
the O
binary Task
classification Task
, O
the O
dataset O
has O
a O
split O
of O
train O
( O
6920 O
) O
/ O
dev O
( O
872 O
) O
/ O
test O
( O
1821 O
) O
. O
Since O
the O
data O
is O
provided O
in O
the O
format O
of O
sub O
- O
sentences O
, O
we O
train O
the O
model O
on O
both O
phrases O
and O
sentences O
but O
only O
test O
on O
the O
sentences O
as O
in O
several O
previous O
works O
. O
Question Task
type Task
classification Task
: O
Question Task
classification Task
is O
an O
important O
step O
in O
a O
question Task
answering Task
system Task
that O
classifies O
a O
question O
into O
a O
specific O
type O
, O
e.g. O
“ O
what O
is O
the O
highest O
waterfall O
in O
the O
United O
States O
? O
” O
is O
a O
question O
that O
belongs O
to O
“ O
location O
” O
. O
For O
this O
task O
, O
we O
use O
the O
benchmark O
TREC Material
. O
TREC Material
divides O
all O
questions O
into O
6 O
categories O
, O
including O
location O
, O
human O
, O
entity O
, O
abbreviation O
, O
description O
and O
numeric O
. O
The O
training O
dataset O
contains O
5452 O
labelled O
questions O
while O
the O
testing O
dataset O
contains O
500 O
questions O
. O
subsection O
: O
Experimental O
Settings O
We O
implement O
our O
model O
based O
on O
Theano O
– O
a O
python Method
library Method
, O
which O
supports O
efficient O
symbolic Task
differentiation Task
and O
transparent O
use O
of O
a O
GPU O
. O
To O
benefit O
from O
the O
efficiency O
of O
parallel O
computation O
of O
the O
tensors O
, O
we O
train O
the O
model O
on O
a O
GPU Method
. O
For O
text Task
preprocessing Task
, O
we O
only O
convert O
all O
characters O
in O
the O
dataset O
to O
lower O
case O
. O
For O
SST Material
, O
we O
conduct O
hyperparameter O
( O
number O
of O
filters O
, O
filter O
length O
in O
CNN Method
; O
memory O
dimension O
in O
LSTM Method
; O
dropout O
rate O
and O
which O
layer O
to O
apply O
, O
etc O
. O
) O
tuning O
on O
the O
validation O
data O
in O
the O
standard O
split O
. O
For O
TREC Material
, O
we O
hold O
out O
1000 O
samples O
from O
the O
training O
dataset O
for O
hyperparameter Task
search Task
and O
train O
the O
model O
using O
the O
remaining O
data O
. O
In O
our O
final O
settings O
, O
we O
only O
use O
one O
convolutional Method
layer Method
and O
one O
LSTM Method
layer Method
for O
both O
tasks O
. O
For O
the O
filter O
size O
, O
we O
investigated O
filter O
lengths O
of O
2 O
, O
3 O
and O
4 O
in O
two O
cases O
: O
a O
) O
single O
convolutional Method
layer Method
with O
the O
same O
filter O
length O
, O
and O
b O
) O
multiple O
convolutional Method
layers Method
with O
different O
lengths O
of O
filters O
in O
parallel O
. O
Here O
we O
denote O
the O
number O
of O
filters O
of O
length O
by O
for O
ease O
of O
clarification O
. O
For O
the O
first O
case O
, O
each O
n O
- O
gram O
window O
is O
transformed O
into O
convoluted O
features O
after O
convolution Method
and O
the O
sequence O
of O
window Method
representations Method
is O
fed O
into O
LSTM Method
. O
For O
the O
latter O
case O
, O
since O
the O
number O
of O
windows O
generated O
from O
each O
convolution Method
layer Method
varies O
when O
the O
filter O
length O
varies O
( O
see O
below O
equation O
( O
3 O
) O
) O
, O
we O
cut O
the O
window O
sequence O
at O
the O
end O
based O
on O
the O
maximum O
filter O
length O
that O
gives O
the O
shortest O
number O
of O
windows O
. O
Each O
window O
is O
represented O
as O
the O
concatenation O
of O
outputs O
from O
different O
convolutional Method
layers Method
. O
We O
also O
exploit O
different O
combinations O
of O
different O
filter O
lengths O
. O
We O
further O
present O
experimental O
analysis O
of O
the O
exploration O
on O
filter Metric
size Metric
later O
. O
According O
to O
the O
experiments O
, O
we O
choose O
a O
single O
convolutional Method
layer Method
with O
filter O
length O
3 O
. O
For O
SST Material
, O
the O
number O
of O
filters O
of O
length O
3 O
is O
set O
to O
be O
150 O
and O
the O
memory O
dimension O
of O
LSTM Method
is O
set O
to O
be O
150 O
, O
too O
. O
The O
word Method
vector Method
layer Method
and O
the O
LSTM Method
layer Method
are O
dropped O
out O
with O
a O
probability O
of O
0.5 O
. O
For O
TREC Material
, O
the O
number O
of O
filters O
is O
set O
to O
be O
300 O
and O
the O
memory O
dimension O
is O
set O
to O
be O
300 O
. O
The O
word Method
vector Method
layer Method
and O
the O
LSTM Method
layer Method
are O
dropped O
out O
with O
a O
probability O
of O
0.5 O
. O
We O
also O
add O
L2 Method
regularization Method
with O
a O
factor O
of O
0.001 O
to O
the O
weights O
in O
the O
softmax Method
layer Method
for O
both O
tasks O
. O
section O
: O
Results O
and O
Model O
Analysis O
In O
this O
section O
, O
we O
show O
our O
evaluation O
results O
on O
sentiment Task
classification Task
and O
question Task
type Task
classification Task
tasks Task
. O
Moreover O
, O
we O
give O
some O
model O
analysis O
on O
the O
filter Method
size Method
configuration Method
. O
subsection O
: O
Sentiment Task
Classification Task
The O
results O
are O
shown O
in O
Table O
1 O
. O
We O
compare O
our O
model O
with O
a O
large O
set O
of O
well O
- O
performed O
models O
on O
the O
Stanford Material
Sentiment Material
Treebank Material
. O
Generally O
, O
the O
baseline Method
models Method
consist O
of O
recursive Method
models Method
, O
convolutional Method
neural Method
network Method
models O
, O
LSTM Method
related O
models O
and O
others O
. O
The O
recursive Method
models Method
employ O
a O
syntactic O
parse O
tree O
as O
the O
sentence O
structure O
and O
the O
sentence Method
representation Method
is O
computed O
recursively O
in O
a O
bottom O
- O
up O
manner O
along O
the O
parse O
tree O
. O
Under O
this O
category O
, O
we O
choose O
recursive Method
autoencoder Method
( O
RAE Method
) O
, O
matrix Method
- Method
vector Method
( O
MV Method
- Method
RNN Method
) O
, O
tensor Method
based Method
composition Method
( O
RNTN Method
) O
and O
multi Method
- Method
layer Method
stacked Method
( Method
DRNN Method
) Method
recursive Method
neural Method
network Method
as O
baselines O
. O
Among O
CNNs Method
, O
we O
compare O
with O
Kim O
’s O
kim O
CNN Method
model O
with O
fine O
- O
tuned O
word O
vectors O
( O
CNN Method
- O
non O
- O
static O
) O
and O
multi O
- O
channels O
( O
CNN Method
- O
multichannel O
) O
, O
DCNN Method
with O
dynamic Method
k Method
- Method
max Method
pooling Method
, O
Tao O
’s O
CNN Method
( O
Molding O
- O
CNN Method
) O
with O
low Method
- Method
rank Method
tensor Method
based Method
non Method
- Method
linear Method
and Method
non Method
- Method
consecutive Method
convolutions Method
. O
Among O
LSTM Method
related O
models O
, O
we O
first O
compare O
with O
two O
tree Method
- Method
structured Method
LSTM Method
models Method
( O
Dependence Method
Tree Method
- Method
LSTM Method
and O
Constituency Method
Tree Method
- Method
LSTM Method
) O
that O
adjust O
LSTM Method
to O
tree O
- O
structured O
network O
topologies O
. O
Then O
we O
implement O
one Method
- Method
layer Method
LSTM Method
and O
Bi Method
- Method
LSTM Method
by O
ourselves O
. O
Since O
we O
could O
not O
tune O
the O
result O
of O
Bi Method
- Method
LSTM Method
to O
be O
as O
good O
as O
what O
has O
been O
reported O
in O
even O
if O
following O
their O
untied O
weight O
configuration O
, O
we O
report O
our O
own O
results O
. O
For O
other O
baseline O
methods O
, O
we O
compare O
against O
SVM Method
with O
unigram Method
and Method
bigram Method
features Method
, O
NBoW Method
with O
average Method
word Method
vector Method
features Method
and O
paragraph Method
vector Method
that O
infers O
the O
new O
paragraph O
vector O
for O
unseen O
documents O
. O
To O
the O
best O
of O
our O
knowledge O
, O
we O
achieve O
the O
fourth O
best O
published O
result O
for O
the O
5 Task
- Task
class Task
classification Task
task Task
on O
this O
dataset O
. O
For O
the O
binary Task
classification Task
task Task
, O
we O
achieve O
comparable O
results O
with O
respect O
to O
the O
state O
- O
of O
- O
the O
- O
art O
ones O
. O
From O
Table O
1 O
, O
we O
have O
the O
following O
observations O
: O
( O
1 O
) O
Although O
we O
did O
not O
beat O
the O
state O
- O
of O
- O
the O
- O
art O
ones O
, O
as O
an O
end O
- O
to O
- O
end Method
model Method
, O
the O
result O
is O
still O
promising O
and O
comparable O
with O
thoes Method
models Method
that O
heavily O
rely O
on O
linguistic O
annotations O
and O
knowledge O
, O
especially O
syntactic O
parse O
trees O
. O
This O
indicates O
C O
- O
LSTM Method
will O
be O
more O
feasible O
for O
various O
scenarios O
. O
( O
2 O
) O
Comparing O
our O
results O
against O
single Method
CNN Method
and O
LSTM Method
models Method
shows O
that O
LSTM Method
does O
learn O
long O
- O
term O
dependencies O
across O
sequences O
of O
higher O
- O
level Method
representations Method
better O
. O
We O
could O
explore O
in O
the O
future O
how O
to O
learn O
more O
compact O
higher O
- O
level O
representations O
by O
replacing O
standard O
convolution Method
with O
other O
non Method
- Method
linear Method
feature Method
mapping Method
functions Method
or O
appealing O
to O
tree O
- O
structured O
topologies O
before O
the O
convolutional Method
layer Method
. O
subsection O
: O
Question Method
Type Method
Classification Method
The O
prediction Metric
accuracy Metric
on O
TREC Material
question O
classification O
is O
reported O
in O
Table O
2 O
. O
We O
compare O
our O
model O
with O
a O
variety O
of O
models O
. O
The O
SVM Method
classifier Method
uses O
unigrams O
, O
bigrams O
, O
wh O
- O
word O
, O
head O
word O
, O
POS O
tags O
, O
parser O
, O
hypernyms O
, O
WordNet O
synsets O
as O
engineered O
features O
and O
60 O
hand O
- O
coded O
rules O
. O
Ada O
- O
CNN Method
is O
a O
self Method
- Method
adaptiive Method
hierarchical Method
sentence Method
model Method
with O
gating Method
networks Method
. O
Other O
baseline O
models O
have O
been O
introduced O
in O
the O
last O
task O
. O
From O
Table O
2 O
, O
we O
have O
the O
following O
observations O
: O
( O
1 O
) O
Our O
result O
consistently O
outperforms O
all O
published O
neural Method
baseline Method
models Method
, O
which O
means O
that O
C O
- O
LSTM Method
captures O
intentions O
of O
TREC Material
questions Material
well O
. O
( O
2 O
) O
Our O
result O
is O
close O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
SVM Method
that O
depends O
on O
highly O
engineered O
features O
. O
Such O
engineered O
features O
not O
only O
demands O
human O
laboring O
but O
also O
leads O
to O
the O
error Metric
propagation O
in O
the O
existing O
NLP Method
tools Method
, O
thus O
could O
n’t O
generalize O
well O
in O
other O
datasets O
and O
tasks O
. O
With O
the O
ability O
of O
automatically Task
learning Task
semantic Task
sentence Task
representations Task
, O
C O
- O
LSTM Method
does O
n’t O
require O
any O
human O
- O
designed O
features O
and O
has O
a O
better O
scalibility O
. O
subsection O
: O
Model O
Analysis O
Here O
we O
investigate O
the O
impact O
of O
different O
filter O
configurations O
in O
the O
convolutional Method
layer Method
on O
the O
model O
performance O
. O
In O
the O
convolutional Method
layer Method
of O
our O
model O
, O
filters Method
are O
used O
to O
capture O
local O
n O
- O
gram O
features O
. O
Intuitively O
, O
multiple O
convolutional Method
layers Method
in O
parallel O
with O
different O
filter O
sizes O
should O
perform O
better O
than O
single O
convolutional Method
layers Method
with O
the O
same O
length O
filters O
in O
that O
different O
filter O
sizes O
could O
exploit O
features O
of O
different O
n O
- O
grams O
. O
However O
, O
we O
found O
in O
our O
experiments O
that O
single O
convolutional Method
layer Method
with O
filter O
length O
3 O
always O
outperforms O
the O
other O
cases O
. O
We O
show O
in O
Figure O
2 O
the O
prediction Metric
accuracies Metric
on O
the O
6 Task
- Task
way Task
question Task
classification Task
task Task
using O
different O
filter Method
configurations Method
. O
Note O
that O
we O
also O
observe O
the O
similar O
phenomenon O
in O
the O
sentiment Task
classification Task
task Task
. O
For O
each O
filter O
configuration O
, O
we O
report O
in O
Figure O
2 O
the O
best O
result O
under O
extensive O
grid Method
- Method
search Method
on O
hyperparameters O
. O
It O
it O
shown O
that O
single O
convolutional Method
layer Method
with O
filter O
length O
3 O
performs O
best O
among O
all O
filter O
configurations O
. O
For O
the O
case O
of O
multiple O
convolutional O
layers O
in O
parallel O
, O
it O
is O
shown O
that O
filter Method
configurations Method
with O
filter O
length O
3 O
performs O
better O
that O
those O
without O
tri Method
- Method
gram Method
filters Method
, O
which O
further O
confirms O
that O
tri O
- O
gram O
features O
do O
play O
a O
significant O
role O
in O
capturing O
local O
features O
in O
our O
tasks O
. O
We O
conjecture O
that O
LSTM Method
could O
learn O
better O
semantic Method
sentence Method
representations Method
from O
sequences O
of O
tri O
- O
gram O
features O
. O
section O
: O
Conclusion O
and O
Future O
Work O
We O
have O
described O
a O
novel O
, O
unified Method
model Method
called O
C O
- O
LSTM Method
that O
combines O
convolutional Method
neural Method
network Method
with O
long Method
short Method
- Method
term Method
memory Method
network Method
( O
LSTM Method
) Method
. O
C O
- O
LSTM Method
is O
able O
to O
learn O
phrase O
- O
level O
features O
through O
a O
convolutional Method
layer Method
; O
sequences O
of O
such O
higher O
- O
level O
representations O
are O
then O
fed O
into O
the O
LSTM Method
to O
learn O
long O
- O
term O
dependencies O
. O
We O
evaluated O
the O
learned O
semantic Method
sentence Method
representations Method
on O
sentiment Task
classification Task
and O
question Task
type Task
classification Task
tasks Task
with O
very O
satisfactory O
results O
. O
We O
could O
explore O
in O
the O
future O
ways O
to O
replace O
the O
standard O
convolution Method
with O
tensor Method
- Method
based Method
operations Method
or O
tree Method
- Method
structured Method
convolutions Method
. O
We O
believe O
LSTM Method
will O
benefit O
from O
more O
structured O
higher Method
- Method
level Method
representations Method
. O
bibliography O
: O
References O
