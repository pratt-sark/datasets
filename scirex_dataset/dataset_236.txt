document	O
:	O
What	O
Do	O
Recurrent	Method
Neural	Method
Network	Method
Grammars	Method
Learn	O
About	O
Syntax	O
?	O
Recurrent	Method
neural	Method
network	Method
grammars	Method
(	O
RNNG	Method
)	O
are	O
a	O
recently	O
proposed	O
probabilistic	Method
generative	Method
modeling	Method
family	Method
for	O
natural	O
language	O
.	O
They	O
show	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
language	Method
modeling	Method
and	O
parsing	Task
performance	O
.	O
We	O
investigate	O
what	O
information	O
they	O
learn	O
,	O
from	O
a	O
linguistic	O
perspective	O
,	O
through	O
various	O
ablations	Method
to	O
the	O
model	O
and	O
the	O
data	O
,	O
and	O
by	O
augmenting	O
the	O
model	O
with	O
an	O
attention	Method
mechanism	Method
(	O
GA	Method
-	Method
RNNG	Method
)	O
to	O
enable	O
closer	O
inspection	O
.	O
We	O
find	O
that	O
explicit	O
modeling	O
of	O
composition	Task
is	O
crucial	O
for	O
achieving	O
the	O
best	O
performance	O
.	O
Through	O
the	O
attention	Method
mechanism	Method
,	O
we	O
find	O
that	O
headedness	O
plays	O
a	O
central	O
role	O
in	O
phrasal	Task
representation	Task
(	O
with	O
the	O
model	O
’s	O
latent	O
attention	O
largely	O
agreeing	O
with	O
predictions	O
made	O
by	O
hand	O
-	O
crafted	O
head	O
rules	O
,	O
albeit	O
with	O
some	O
important	O
differences	O
)	O
.	O
By	O
training	O
grammars	Method
without	O
nonterminal	O
labels	O
,	O
we	O
find	O
that	O
phrasal	O
representations	O
depend	O
minimally	O
on	O
nonterminals	O
,	O
providing	O
support	O
for	O
the	O
endocentricity	Method
hypothesis	Method
.	O
noitemsep	O
,	O
topsep=10pt	O
,	O
parsep=0pt	O
,	O
partopsep=0pt	O
noitemsep	O
,	O
topsep=10pt	O
,	O
parsep=0pt	O
,	O
partopsep=0pt	O
section	O
:	O
Introduction	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
a	O
recently	O
proposed	O
class	O
of	O
probability	Method
distributions	Method
,	O
recurrent	Method
neural	Method
network	Method
grammars	Method
(	O
RNNGs	Method
;	O
Dyer	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
designed	O
to	O
model	O
syntactic	Task
derivations	Task
of	Task
sentences	Task
.	O
We	O
focus	O
on	O
RNNGs	Method
as	O
generative	Method
probabilistic	Method
models	Method
over	O
trees	Method
,	O
as	O
summarized	O
in	O
§	O
[	O
reference	O
]	O
.	O
Fitting	O
a	O
probabilistic	Method
model	Method
to	O
data	O
has	O
often	O
been	O
understood	O
as	O
a	O
way	O
to	O
test	O
or	O
confirm	O
some	O
aspect	O
of	O
a	O
theory	O
.	O
We	O
talk	O
about	O
a	O
model	O
’s	O
assumptions	O
and	O
sometimes	O
explore	O
its	O
parameters	O
or	O
posteriors	O
over	O
its	O
latent	O
variables	O
in	O
order	O
to	O
gain	O
understanding	O
of	O
what	O
it	O
“	O
discovers	O
”	O
from	O
the	O
data	O
.	O
In	O
some	O
sense	O
,	O
such	O
models	O
can	O
be	O
thought	O
of	O
as	O
mini	O
-	O
scientists	O
.	O
Neural	Method
networks	Method
,	O
including	O
RNNGs	Method
,	O
are	O
capable	O
of	O
representing	O
larger	O
classes	O
of	O
hypotheses	O
than	O
traditional	O
probabilistic	Method
models	Method
,	O
giving	O
them	O
more	O
freedom	O
to	O
explore	O
.	O
Unfortunately	O
,	O
they	O
tend	O
to	O
be	O
bad	O
mini	O
-	O
scientists	O
,	O
because	O
their	O
parameters	O
are	O
difficult	O
for	O
human	O
scientists	O
to	O
interpret	O
.	O
RNNGs	Method
are	O
striking	O
because	O
they	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
parsing	Task
and	O
language	Task
modeling	Task
performance	O
.	O
Their	O
relative	O
lack	O
of	O
independence	O
assumptions	O
,	O
while	O
still	O
incorporating	O
a	O
degree	O
of	O
linguistically	O
-	O
motivated	O
prior	O
knowledge	O
,	O
affords	O
the	O
model	O
considerable	O
freedom	O
to	O
derive	O
its	O
own	O
insights	O
about	O
syntax	O
.	O
If	O
they	O
are	O
mini	O
-	O
scientists	O
,	O
the	O
discoveries	O
they	O
make	O
should	O
be	O
of	O
particular	O
interest	O
as	O
propositions	O
about	O
syntax	O
(	O
at	O
least	O
for	O
the	O
particular	O
genre	O
and	O
dialect	O
of	O
the	O
data	O
)	O
.	O
This	O
paper	O
manipulates	O
the	O
inductive	O
bias	O
of	O
RNNGs	Method
to	O
test	O
linguistic	Task
hypotheses	Task
.	O
We	O
begin	O
with	O
an	O
ablation	O
study	O
to	O
discover	O
the	O
importance	O
of	O
the	O
composition	O
function	O
in	O
§	O
[	O
reference	O
]	O
.	O
Based	O
on	O
the	O
findings	O
,	O
we	O
augment	O
the	O
RNNG	Method
composition	Method
function	Method
with	O
a	O
novel	O
gated	Method
attention	Method
mechanism	Method
(	O
leading	O
to	O
the	O
GA	Method
-	Method
RNNG	Method
)	O
to	O
incorporate	O
more	O
interpretability	O
into	O
the	O
model	O
in	O
§	O
[	O
reference	O
]	O
.	O
Using	O
the	O
GA	Method
-	Method
RNNG	Method
,	O
we	O
proceed	O
by	O
investigating	O
the	O
role	O
that	O
individual	O
heads	O
play	O
in	O
phrasal	Method
representation	Method
(	O
§	O
[	O
reference	O
]	O
)	O
and	O
the	O
role	O
that	O
nonterminal	O
category	O
labels	O
play	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O
Our	O
key	O
findings	O
are	O
that	O
lexical	O
heads	O
play	O
an	O
important	O
role	O
in	O
representing	O
most	O
phrase	O
types	O
(	O
although	O
compositions	O
of	O
multiple	O
salient	O
heads	O
are	O
not	O
infrequent	O
,	O
especially	O
for	O
conjunctions	O
)	O
and	O
that	O
nonterminal	O
labels	O
provide	O
little	O
additional	O
information	O
.	O
As	O
a	O
by	O
-	O
product	O
of	O
our	O
investigation	O
,	O
a	O
variant	O
of	O
the	O
RNNG	Method
without	O
ensembling	Method
achieved	O
the	O
best	O
reported	O
supervised	O
phrase	O
-	O
structure	O
parsing	Task
(	O
93.6	O
;	O
English	Material
PTB	Material
)	O
and	O
,	O
through	O
conversion	Method
,	O
dependency	O
parsing	Task
(	O
95.8	O
UAS	O
,	O
94.6	O
LAS	O
;	O
PTB	Material
SD	O
)	O
.	O
The	O
code	O
and	O
pretrained	Method
models	Method
to	O
replicate	O
our	O
results	O
are	O
publicly	O
available	O
.	O
section	O
:	O
Recurrent	Method
Neural	Method
Network	Method
Grammars	Method
An	O
RNNG	Method
defines	O
a	O
joint	O
probability	O
distribution	O
over	O
string	O
terminals	O
and	O
phrase	O
-	O
structure	O
nonterminals	O
.	O
Formally	O
,	O
the	O
RNNG	Method
is	O
defined	O
by	O
a	O
triple	O
,	O
where	O
denotes	O
the	O
set	O
of	O
nonterminal	O
symbols	O
(	O
NP	O
,	O
VP	O
,	O
etc	O
.	O
)	O
,	O
the	O
set	O
of	O
all	O
terminal	O
symbols	O
(	O
we	O
assume	O
that	O
)	O
,	O
and	O
the	O
set	O
of	O
all	O
model	O
parameters	O
.	O
Unlike	O
previous	O
works	O
that	O
rely	O
on	O
hand	O
-	O
crafted	O
rules	O
to	O
compose	O
more	O
fine	O
-	O
grained	O
phrase	Method
representations	Method
,	O
the	O
RNNG	Method
implicitly	O
parameterizes	O
the	O
information	O
passed	O
through	O
compositions	O
of	O
phrases	O
(	O
in	O
and	O
the	O
neural	Method
network	Method
architecture	Method
)	O
,	O
hence	O
weakening	O
the	O
strong	O
independence	O
assumptions	O
in	O
classical	O
probabilistic	Method
context	Method
-	Method
free	Method
grammars	Method
.	O
The	O
RNNG	Method
is	O
based	O
on	O
an	O
abstract	Method
state	Method
machine	Method
like	O
those	O
used	O
in	O
transition	O
-	O
based	O
parsing	Task
,	O
with	O
its	O
algorithmic	O
state	O
consisting	O
of	O
a	O
stack	O
of	O
partially	O
completed	O
constituents	O
,	O
a	O
buffer	O
of	O
already	O
-	O
generated	O
terminal	O
symbols	O
,	O
and	O
a	O
list	O
of	O
past	O
actions	O
.	O
To	O
generate	O
a	O
sentence	O
and	O
its	O
phrase	O
-	O
structure	O
tree	O
,	O
the	O
RNNG	Method
samples	O
a	O
sequence	O
of	O
actions	O
to	O
construct	O
top	O
-	O
down	O
.	O
Given	O
,	O
there	O
is	O
one	O
such	O
sequence	O
(	O
easily	O
identified	O
)	O
,	O
which	O
we	O
call	O
the	O
oracle	Method
,	O
used	O
during	O
supervised	Task
training	Task
.	O
The	O
RNNG	Method
uses	O
three	O
different	O
actions	O
:	O
,	O
where	O
,	O
introduces	O
an	O
open	O
nonterminal	O
symbol	O
onto	O
the	O
stack	O
,	O
e.g.	O
,	O
“	O
(	O
NP	O
”	O
;	O
,	O
where	O
,	O
generates	O
a	O
terminal	O
symbol	O
and	O
places	O
it	O
on	O
the	O
stack	O
and	O
buffer	O
;	O
and	O
reduce	O
indicates	O
a	O
constituent	O
is	O
now	O
complete	O
.	O
The	O
elements	O
of	O
the	O
stack	O
that	O
comprise	O
the	O
current	O
constituent	O
(	O
going	O
back	O
to	O
the	O
last	O
open	O
nonterminal	O
)	O
are	O
popped	O
,	O
a	O
composition	Method
function	Method
is	O
executed	O
,	O
yielding	O
a	O
composed	Method
representation	Method
that	O
is	O
pushed	O
onto	O
the	O
stack	O
.	O
At	O
each	O
timestep	O
,	O
the	O
model	O
encodes	O
the	O
stack	O
,	O
buffer	O
,	O
and	O
past	O
actions	O
,	O
with	O
a	O
separate	O
LSTM	Method
for	O
each	O
component	O
as	O
features	O
to	O
define	O
a	O
distribution	O
over	O
the	O
next	O
action	O
to	O
take	O
(	O
conditioned	O
on	O
the	O
full	O
algorithmic	O
state	O
)	O
.	O
The	O
overall	O
architecture	O
is	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
;	O
examples	O
of	O
full	O
action	O
sequences	O
can	O
be	O
found	O
in	O
rnng	Method
.	O
A	O
key	O
element	O
of	O
the	O
RNNG	Method
is	O
the	O
composition	Method
function	Method
,	O
which	O
reduces	O
a	O
completed	O
constituent	O
into	O
a	O
single	O
element	O
on	O
the	O
stack	O
.	O
This	O
function	O
computes	O
a	O
vector	Method
representation	Method
of	O
the	O
new	O
constituent	O
;	O
it	O
also	O
uses	O
an	O
LSTM	Method
(	O
here	O
a	O
bidirectional	Method
one	Method
)	O
.	O
This	O
composition	Method
function	Method
,	O
which	O
we	O
consider	O
in	O
greater	O
depth	O
in	O
§	O
[	O
reference	O
]	O
,	O
is	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
Since	O
the	O
RNNG	Method
is	O
a	O
generative	Method
model	Method
,	O
it	O
attempts	O
to	O
maximize	O
,	O
the	O
joint	O
distribution	O
of	O
strings	O
and	O
trees	O
,	O
defined	O
as	O
In	O
other	O
words	O
,	O
is	O
defined	O
as	O
a	O
product	O
of	O
local	O
probabilities	O
,	O
conditioned	O
on	O
all	O
past	O
actions	O
.	O
The	O
joint	Method
probability	Method
estimate	Method
can	O
be	O
used	O
for	O
both	O
phrase	O
-	O
structure	O
parsing	Task
(	O
finding	Task
)	O
and	O
language	Task
modeling	Task
(	O
finding	Task
by	O
marginalizing	O
over	O
the	O
set	O
of	O
possible	O
parses	O
for	O
)	O
.	O
Both	O
inference	Task
problems	Task
can	O
be	O
solved	O
using	O
an	O
importance	Method
sampling	Method
procedure	Method
.	O
We	O
report	O
all	O
RNNG	Method
performance	O
based	O
on	O
the	O
corrigendum	O
to	O
rnng	Method
.	O
section	O
:	O
Composition	Task
is	O
Key	O
Given	O
the	O
same	O
data	O
,	O
under	O
both	O
the	O
discriminative	Method
and	Method
generative	Method
settings	Method
RNNGs	Method
were	O
found	O
to	O
parse	O
with	O
significantly	O
higher	O
accuracy	Metric
than	O
(	O
respectively	O
)	O
the	O
models	O
of	O
vinyals:2015	O
and	O
choe:2016	O
that	O
represent	O
as	O
a	O
“	O
linearized	O
”	O
sequence	O
of	O
symbols	O
and	O
parentheses	O
without	O
explicitly	O
capturing	O
the	O
tree	O
structure	O
,	O
or	O
even	O
constraining	O
the	O
to	O
be	O
a	O
well	O
-	O
formed	O
tree	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O
vinyals:2015	O
directly	O
predict	O
the	O
sequence	O
of	O
nonterminals	O
,	O
“	O
shifts	O
”	O
(	O
which	O
consume	O
a	O
terminal	O
symbol	O
)	O
,	O
and	O
parentheses	O
from	O
left	O
to	O
right	O
,	O
conditional	O
on	O
the	O
input	O
terminal	O
sequence	O
,	O
while	O
choe:2016	O
used	O
a	O
sequential	Method
LSTM	Method
language	Method
model	Method
on	O
the	O
same	O
linearized	O
trees	O
to	O
create	O
a	O
generative	Method
variant	Method
of	O
the	O
vinyals:2015	O
model	O
.	O
The	O
generative	Method
model	Method
is	O
used	O
to	O
re	O
-	O
rank	O
parse	O
candidates	O
.	O
The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
suggest	O
that	O
the	O
RNNG	Method
’s	Method
explicit	Method
composition	Method
function	Method
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
,	O
which	O
vinyals:2015	O
and	O
choe:2016	O
must	O
learn	O
implicitly	O
,	O
plays	O
a	O
crucial	O
role	O
in	O
the	O
RNNG	Metric
’s	Metric
generalization	Metric
success	Metric
.	O
Beyond	O
this	O
,	O
Choe	O
and	O
Charniak	O
’s	O
generative	Method
variant	Method
of	O
Vinyals	O
et	O
al	O
.	O
(	O
2015	O
)	O
is	O
another	O
instance	O
where	O
generative	Method
models	Method
trained	O
on	O
the	O
PTB	Material
outperform	O
discriminative	Method
models	Method
.	O
subsection	O
:	O
Ablated	Method
RNNGs	Method
On	O
close	O
inspection	O
,	O
it	O
is	O
clear	O
that	O
the	O
RNNG	Method
’s	O
three	O
data	O
structures	O
—	O
stack	O
,	O
buffer	O
,	O
and	O
action	O
history	O
—	O
are	O
redundant	O
.	O
For	O
example	O
,	O
the	O
action	O
history	O
and	O
buffer	O
contents	O
completely	O
determine	O
the	O
structure	O
of	O
the	O
stack	O
at	O
every	O
timestep	O
.	O
Every	O
generated	O
word	O
goes	O
onto	O
the	O
stack	O
,	O
too	O
;	O
and	O
some	O
past	O
words	O
will	O
be	O
composed	O
into	O
larger	O
structures	O
,	O
but	O
through	O
the	O
composition	Method
function	Method
,	O
they	O
are	O
all	O
still	O
“	O
available	O
”	O
to	O
the	O
network	O
that	O
predicts	O
the	O
next	O
action	O
.	O
Similarly	O
,	O
the	O
past	O
actions	O
are	O
redundant	O
with	O
the	O
stack	O
.	O
Despite	O
this	O
redundancy	O
,	O
only	O
the	O
stack	O
incorporates	O
the	O
composition	O
function	O
.	O
Since	O
each	O
of	O
the	O
ablated	Method
models	Method
is	O
sufficient	O
to	O
encode	O
all	O
necessary	O
partial	O
tree	O
information	O
,	O
the	O
primary	O
difference	O
is	O
that	O
ablations	Method
with	O
the	O
stack	O
use	O
explicit	O
composition	O
,	O
to	O
which	O
we	O
can	O
therefore	O
attribute	O
most	O
of	O
the	O
performance	O
difference	O
.	O
We	O
conjecture	O
that	O
the	O
stack	O
—	O
the	O
component	O
that	O
makes	O
use	O
of	O
the	O
composition	O
function	O
—	O
is	O
critical	O
to	O
the	O
RNNG	Method
’s	O
performance	O
,	O
and	O
that	O
the	O
buffer	O
and	O
action	O
history	O
are	O
not	O
.	O
In	O
transition	Method
-	Method
based	Method
parsers	Method
built	O
on	O
expert	O
-	O
crafted	O
features	O
,	O
the	O
most	O
recent	O
words	O
and	O
actions	O
are	O
useful	O
if	O
they	O
are	O
salient	O
,	O
although	O
neural	Method
representation	Method
learners	Method
can	O
automatically	O
learn	O
what	O
information	O
should	O
be	O
salient	O
.	O
To	O
test	O
this	O
conjecture	O
,	O
we	O
train	O
ablated	Method
RNNGs	Method
that	O
lack	O
each	O
of	O
the	O
three	O
data	O
structures	O
(	O
action	O
history	O
,	O
buffer	O
,	O
stack	O
)	O
,	O
as	O
well	O
as	O
one	O
that	O
lacks	O
both	O
the	O
action	O
history	O
and	O
buffer	O
.	O
If	O
our	O
conjecture	O
is	O
correct	O
,	O
performance	O
should	O
degrade	O
most	O
without	O
the	O
stack	O
,	O
and	O
the	O
stack	Method
alone	O
should	O
perform	O
competitively	O
.	O
Experimental	O
settings	O
.	O
We	O
perform	O
our	O
experiments	O
on	O
the	O
English	Material
PTB	Material
corpus	Material
,	O
with	O
§	O
02–21	O
for	O
training	O
,	O
§	O
24	O
for	O
validation	Task
,	O
and	O
§	O
23	O
for	O
test	O
;	O
no	O
additional	O
data	O
were	O
used	O
for	O
training	O
.	O
We	O
follow	O
the	O
same	O
hyperparameters	O
as	O
the	O
generative	Method
model	Method
proposed	O
in	O
rnng	Method
.	O
The	O
generative	Method
model	Method
did	O
not	O
use	O
any	O
pretrained	O
word	O
embeddings	O
or	O
POS	O
tags	O
;	O
a	O
discriminative	Method
variant	Method
of	O
the	O
standard	O
RNNG	Method
was	O
used	O
to	O
obtain	O
tree	O
samples	O
for	O
the	O
generative	Method
model	Method
.	O
All	O
further	O
experiments	O
use	O
the	O
same	O
settings	O
and	O
hyperparameters	O
unless	O
otherwise	O
noted	O
.	O
Experimental	O
results	O
.	O
We	O
trained	O
each	O
ablation	O
from	O
scratch	O
,	O
and	O
compared	O
these	O
models	O
on	O
three	O
tasks	O
:	O
English	O
phrase	O
-	O
structure	O
parsing	Task
(	O
labeled	O
)	O
,	O
Table	O
[	O
reference	O
]	O
;	O
dependency	O
parsing	Task
,	O
Table	O
[	O
reference	O
]	O
,	O
by	O
converting	O
parse	O
output	O
to	O
Stanford	O
dependencies	O
using	O
the	O
tool	O
by	O
kong_14	O
;	O
and	O
language	Method
modeling	Method
,	O
Table	O
[	O
reference	O
]	O
.	O
The	O
last	O
row	O
of	O
each	O
table	O
reports	O
the	O
performance	O
of	O
a	O
novel	O
variant	O
of	O
the	O
(	Method
stack	Method
-	Method
only	Method
)	Method
RNNG	Method
with	O
attention	Method
,	O
to	O
be	O
presented	O
in	O
§	O
[	O
reference	O
]	O
.	O
Discussion	O
.	O
The	O
RNNG	Method
with	O
only	O
a	O
stack	O
is	O
the	O
strongest	O
of	O
the	O
ablations	Method
,	O
and	O
it	O
even	O
outperforms	O
the	O
“	O
full	Method
”	Method
RNNG	Method
with	O
all	O
three	O
data	O
structures	O
.	O
Ablating	O
the	O
stack	O
gives	O
the	O
worst	O
among	O
the	O
new	O
results	O
.	O
This	O
strongly	O
supports	O
the	O
importance	O
of	O
the	O
composition	Method
function	Method
:	O
a	O
proper	O
reduce	Method
operation	Method
that	O
transforms	O
a	O
constituent	O
’s	O
parts	O
and	O
nonterminal	O
label	O
into	O
a	O
single	O
explicit	O
(	O
vector	Method
)	Method
representation	Method
is	O
helpful	O
to	O
performance	O
.	O
It	O
is	O
noteworthy	O
that	O
the	O
stack	Method
alone	O
is	O
stronger	O
than	O
the	O
original	O
RNNG	Method
,	O
which	O
—	O
in	O
principle	O
—	O
can	O
learn	O
to	O
disregard	O
the	O
buffer	O
and	O
action	O
history	O
.	O
Since	O
the	O
stack	O
maintains	O
syntactically	O
“	O
recent	O
”	O
information	O
near	O
its	O
top	O
,	O
we	O
conjecture	O
that	O
the	O
learner	O
is	O
overfitting	O
to	O
spurious	O
predictors	O
in	O
the	O
buffer	O
and	O
action	O
history	O
that	O
explain	O
the	O
training	O
data	O
but	O
do	O
not	O
generalize	O
well	O
.	O
A	O
similar	O
performance	O
degradation	O
is	O
seen	O
in	O
language	Method
modeling	Method
(	O
Table	O
[	O
reference	O
]	O
)	O
:	O
the	O
stack	Method
-	Method
only	Method
RNNG	Method
achieves	O
the	O
best	O
performance	O
,	O
and	O
ablating	O
the	O
stack	O
is	O
most	O
harmful	O
.	O
Indeed	O
,	O
modeling	O
syntax	O
without	O
explicit	O
composition	O
(	O
the	O
stack	Method
-	Method
ablated	Method
RNNG	Method
)	O
provides	O
little	O
benefit	O
over	O
a	O
sequential	Method
LSTM	Method
language	Method
model	Method
.	O
We	O
remark	O
that	O
the	O
stack	O
-	O
only	O
results	O
are	O
the	O
best	O
published	O
PTB	Material
results	O
for	O
both	O
phrase	O
-	O
structure	O
and	O
dependency	O
parsing	Task
among	O
supervised	Method
models	Method
.	O
section	O
:	O
Gated	Method
Attention	Method
RNNG	Method
Having	O
established	O
that	O
the	O
composition	O
function	O
is	O
key	O
to	O
RNNG	Task
performance	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
we	O
now	O
seek	O
to	O
understand	O
the	O
nature	O
of	O
the	O
composed	O
phrasal	Method
representations	Method
that	O
are	O
learned	O
.	O
Like	O
most	O
neural	Method
networks	Method
,	O
interpreting	O
the	O
composition	O
function	O
’s	O
behavior	O
is	O
challenging	O
.	O
Fortunately	O
,	O
linguistic	Method
theories	Method
offer	O
a	O
number	O
of	O
hypotheses	O
about	O
the	O
nature	O
of	O
representations	O
of	O
phrases	O
that	O
can	O
provide	O
a	O
conceptual	O
scaffolding	O
to	O
understand	O
them	O
.	O
subsection	O
:	O
Linguistic	O
Hypotheses	O
We	O
consider	O
two	O
theories	O
about	O
phrasal	Task
representation	Task
.	O
The	O
first	O
is	O
that	O
phrasal	Method
representations	Method
are	O
strongly	O
determined	O
by	O
a	O
privileged	O
lexical	O
head	O
.	O
Augmenting	Method
grammars	Method
with	O
lexical	O
head	O
information	O
has	O
a	O
long	O
history	O
in	O
parsing	Task
,	O
starting	O
with	O
the	O
models	O
of	O
collins_97	Method
,	O
and	O
theories	O
of	O
syntax	O
such	O
as	O
the	O
“	O
bare	O
phrase	O
structure	O
”	O
hypothesis	O
of	O
the	O
Minimalist	Method
Program	Method
posit	O
that	O
phrases	O
are	O
represented	O
purely	O
by	O
single	O
lexical	O
heads	O
.	O
Proposals	O
for	O
multiple	O
headed	O
phrases	O
(	O
to	O
deal	O
with	O
tricky	O
cases	O
like	O
conjunction	O
)	O
likewise	O
exist	O
.	O
Do	O
the	O
phrasal	Method
representations	Method
learned	O
by	O
RNNGs	Method
depend	O
on	O
individual	O
lexical	O
heads	O
or	O
multiple	O
heads	O
?	O
Or	O
do	O
the	O
representations	O
combine	O
all	O
children	O
without	O
any	O
salient	O
head	O
?	O
Related	O
to	O
the	O
question	O
about	O
the	O
role	O
of	O
heads	O
in	O
phrasal	Task
representation	Task
is	O
the	O
question	O
of	O
whether	O
phrase	O
-	O
internal	O
material	O
wholly	O
determines	O
the	O
representation	O
of	O
a	O
phrase	O
(	O
an	O
endocentric	Method
representation	Method
)	O
or	O
whether	O
nonterminal	O
relabeling	O
of	O
a	O
constitutent	O
introduces	O
new	O
information	O
(	O
exocentric	O
representations	O
)	O
.	O
To	O
illustrate	O
the	O
contrast	O
,	O
an	O
endocentric	Method
representation	Method
is	O
representing	O
a	O
noun	O
phrase	O
with	O
a	O
noun	O
category	O
,	O
whereas	O
exocentrically	O
introduces	O
a	O
new	O
syntactic	O
category	O
that	O
is	O
neither	O
NP	O
nor	O
VP	O
.	O
subsection	O
:	O
Gated	Method
Attention	Method
Composition	Method
To	O
investigate	O
what	O
the	O
stack	Method
-	Method
only	Method
RNNG	Method
learns	O
about	O
headedness	O
(	O
and	O
later	O
endocentricity	O
)	O
,	O
we	O
propose	O
a	O
variant	O
of	O
the	O
composition	Method
function	Method
that	O
makes	O
use	O
of	O
an	O
explicit	O
attention	Method
mechanism	Method
and	O
a	O
sigmoid	Method
gate	Method
with	O
multiplicative	O
interactions	O
,	O
henceforth	O
called	O
GA	Method
-	Method
RNNG	Method
.	O
At	O
every	O
reduce	Method
operation	Method
,	O
the	O
GA	Method
-	Method
RNNG	Method
assigns	O
an	O
“	O
attention	O
weight	O
”	O
to	O
each	O
of	O
its	O
children	O
(	O
between	O
0	O
and	O
1	O
such	O
that	O
the	O
total	O
weight	O
off	O
all	O
children	O
sums	O
to	O
1	O
)	O
,	O
and	O
the	O
parent	O
phrase	O
is	O
represented	O
by	O
the	O
combination	O
of	O
a	O
sum	O
of	O
each	O
child	O
’s	O
representation	O
scaled	O
by	O
its	O
attention	O
weight	O
and	O
its	O
nonterminal	O
type	O
.	O
Our	O
weighted	Method
sum	Method
is	O
more	O
expressive	O
than	O
traditional	O
head	O
rules	O
,	O
however	O
,	O
because	O
it	O
allows	O
attention	O
to	O
be	O
divided	O
among	O
multiple	O
constituents	O
.	O
Head	O
rules	O
,	O
conversely	O
,	O
are	O
analogous	O
to	O
giving	O
all	O
attention	O
to	O
one	O
constituent	O
,	O
the	O
one	O
containing	O
the	O
lexical	O
head	O
.	O
We	O
now	O
formally	O
define	O
the	O
GA	Method
-	Method
RNNG	Method
’s	Method
composition	Method
function	Method
.	O
Recall	O
that	O
is	O
the	O
concatenation	O
of	O
the	O
vector	Method
representations	Method
of	Method
the	Method
RNNG	Method
’s	Method
data	Method
structures	Method
,	O
used	O
to	O
assign	O
probabilities	O
to	O
each	O
of	O
the	O
actions	O
available	O
at	O
timestep	O
(	O
see	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
the	O
layer	O
before	O
the	O
softmax	O
at	O
the	O
top	O
)	O
.	O
For	O
simplicity	O
,	O
we	O
drop	O
the	O
timestep	O
index	O
here	O
.	O
Let	O
denote	O
the	O
vector	O
embedding	O
(	O
learned	O
)	O
of	O
the	O
nonterminal	O
being	O
constructed	O
,	O
for	O
the	O
purpose	O
of	O
computing	O
attention	O
weights	O
.	O
Now	O
let	O
denote	O
the	O
sequence	O
of	O
vector	O
embeddings	O
for	O
the	O
constituents	O
of	O
the	O
new	O
phrase	O
.	O
The	O
length	O
of	O
these	O
vectors	O
is	O
defined	O
by	O
the	O
dimensionality	O
of	O
the	O
bidirectional	Method
LSTM	Method
used	O
in	O
the	O
original	O
composition	O
function	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
We	O
use	O
semicolon	O
(	O
;	O
)	O
to	O
denote	O
vector	O
concatenation	O
operations	O
.	O
The	O
attention	O
vector	O
is	O
given	O
by	O
:	O
Note	O
that	O
the	O
length	O
of	O
is	O
the	O
same	O
as	O
the	O
number	O
of	O
constituents	O
,	O
and	O
that	O
this	O
vector	O
sums	O
to	O
one	O
due	O
to	O
the	O
softmax	Method
.	O
It	O
divides	O
a	O
single	O
unit	O
of	O
attention	O
among	O
the	O
constituents	O
.	O
Next	O
,	O
note	O
that	O
the	O
constituent	O
source	O
vector	O
is	O
a	O
convex	Method
combination	Method
of	O
the	O
child	O
-	O
constituents	O
,	O
weighted	O
by	O
attention	O
.	O
We	O
will	O
combine	O
this	O
with	O
another	O
embedding	O
of	O
the	O
nonterminal	O
denoted	O
as	O
(	O
separate	O
from	O
)	O
using	O
a	O
sigmoid	Method
gating	Method
mechanism	Method
:	O
Note	O
that	O
the	O
value	O
of	O
the	O
gate	O
is	O
bounded	O
between	O
in	O
each	O
dimension	O
.	O
The	O
new	O
phrase	O
’s	O
final	O
representation	O
uses	O
element	O
-	O
wise	O
multiplication	O
(	O
)	O
with	O
respect	O
to	O
both	O
and	O
,	O
a	O
process	O
reminiscent	O
of	O
the	O
LSTM	Method
“	O
forget	Method
”	Method
gate	Method
:	O
The	O
intuition	O
is	O
that	O
the	O
composed	Method
representation	Method
should	O
incorporate	O
both	O
nonterminal	O
information	O
and	O
information	O
about	O
the	O
constituents	O
(	O
through	O
weighted	Method
sum	Method
and	Method
attention	Method
mechanism	Method
)	O
.	O
The	O
gate	O
modulates	O
the	O
interaction	O
between	O
them	O
to	O
account	O
for	O
varying	O
importance	O
between	O
the	O
two	O
in	O
different	O
contexts	O
.	O
Experimental	O
results	O
.	O
We	O
include	O
this	O
model	O
’s	O
performance	O
in	O
Tables	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
(	O
last	O
row	O
in	O
all	O
tables	O
)	O
.	O
It	O
is	O
clear	O
that	O
the	O
model	O
outperforms	O
the	O
baseline	Method
RNNG	Method
with	O
all	O
three	O
structures	O
present	O
and	O
achieves	O
competitive	O
performance	O
with	O
the	O
strongest	O
,	O
stack	Method
-	Method
only	Method
,	Method
RNNG	Method
variant	Method
.	O
section	O
:	O
Headedness	O
in	O
Phrases	O
We	O
now	O
exploit	O
the	O
attention	Method
mechanism	Method
to	O
probe	O
what	O
the	O
RNNG	Method
learns	O
about	O
headedness	O
on	O
the	O
WSJ	Material
§	O
23	O
test	O
set	O
(	O
unseen	O
before	O
by	O
the	O
model	O
)	O
.	O
subsection	O
:	O
The	O
Heads	O
that	O
GA	Method
-	Method
RNNG	Method
Learns	O
The	O
attention	O
weight	O
vectors	O
tell	O
us	O
which	O
constituents	O
are	O
most	O
important	O
to	O
a	O
phrase	O
’s	O
vector	Method
representation	Method
in	O
the	O
stack	O
.	O
Here	O
,	O
we	O
inspect	O
the	O
attention	O
vectors	O
to	O
investigate	O
whether	O
the	O
model	O
learns	O
to	O
center	O
its	O
attention	O
around	O
a	O
single	O
,	O
or	O
by	O
extension	O
a	O
few	O
,	O
salient	O
elements	O
,	O
which	O
would	O
confirm	O
the	O
presence	O
of	O
headedness	O
in	O
GA	Method
-	Method
RNNG	Method
.	O
First	O
,	O
we	O
consider	O
several	O
major	O
nonterminal	O
categories	O
,	O
and	O
estimate	O
the	O
average	Metric
perplexity	Metric
of	Metric
the	O
attention	O
vectors	O
.	O
The	O
average	O
perplexity	O
can	O
be	O
interpreted	O
as	O
the	O
average	O
number	O
of	O
“	O
choices	O
”	O
for	O
each	O
nonterminal	O
category	O
;	O
this	O
value	O
is	O
only	O
computed	O
for	O
the	O
cases	O
where	O
the	O
number	O
of	O
components	O
in	O
the	O
composed	O
phrase	O
is	O
at	O
least	O
two	O
(	O
otherwise	O
the	O
attention	O
weight	O
would	O
be	O
trivially	O
1	O
)	O
.	O
The	O
minimum	O
possible	O
value	O
for	O
the	O
perplexity	O
is	O
1	O
,	O
indicating	O
a	O
full	O
attention	O
weight	O
around	O
one	O
component	O
and	O
zero	O
everywhere	O
else	O
.	O
Figure	O
[	O
reference	O
]	O
(	O
in	O
blue	O
)	O
shows	O
much	O
less	O
than	O
2	O
average	O
“	O
choices	O
”	O
across	O
nonterminal	O
categories	O
,	O
which	O
also	O
holds	O
true	O
for	O
all	O
other	O
categories	O
not	O
shown	O
.	O
For	O
comparison	O
we	O
also	O
report	O
the	O
average	Metric
perplexity	Metric
of	O
the	O
uniform	Method
distribution	Method
for	O
the	O
same	O
nonterminal	O
categories	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
in	O
red	O
)	O
;	O
this	O
represents	O
the	O
highest	O
entropy	O
cases	O
where	O
there	O
is	O
no	O
headedness	O
at	O
all	O
by	O
assigning	O
the	O
same	O
attention	O
weight	O
to	O
each	O
constituent	O
(	O
e.g.	O
attention	O
weights	O
of	O
0.25	O
each	O
for	O
phrases	O
with	O
four	O
constituents	O
)	O
.	O
It	O
is	O
clear	O
that	O
the	O
learned	O
attention	O
weights	O
have	O
much	O
lower	O
perplexity	O
than	O
the	O
uniform	O
distribution	O
baseline	O
,	O
indicating	O
that	O
the	O
learned	O
attention	O
weights	O
are	O
quite	O
peaked	O
around	O
certain	O
components	O
.	O
This	O
implies	O
that	O
phrases	O
’	O
vectors	O
tend	O
to	O
resemble	O
the	O
vector	O
of	O
one	O
salient	O
constituent	O
,	O
but	O
not	O
exclusively	O
,	O
as	O
the	O
perplexity	O
for	O
most	O
categories	O
is	O
still	O
not	O
close	O
to	O
one	O
.	O
Next	O
,	O
we	O
consider	O
the	O
how	O
attention	O
is	O
distributed	O
for	O
the	O
major	O
nonterminal	O
categories	O
in	O
Table	O
[	O
reference	O
]	O
,	O
where	O
the	O
first	O
five	O
rows	O
of	O
each	O
category	O
represent	O
compositions	O
with	O
highest	O
entropy	O
,	O
and	O
the	O
next	O
five	O
rows	O
are	O
qualitatively	O
analyzed	O
.	O
The	O
high	O
-	O
entropy	O
cases	O
where	O
the	O
attention	O
is	O
most	O
divided	O
represent	O
more	O
complex	O
phrases	O
with	O
conjunctions	O
or	O
more	O
than	O
one	O
plausible	O
head	O
.	O
NPs	O
.	O
In	O
most	O
simple	O
noun	O
phrases	O
(	O
representative	O
samples	O
in	O
rows	O
6–7	O
of	O
Table	O
[	O
reference	O
]	O
)	O
,	O
the	O
model	O
pays	O
the	O
most	O
attention	O
to	O
the	O
rightmost	O
noun	O
and	O
assigns	O
near	O
-	O
zero	O
attention	O
on	O
determiners	O
and	O
possessive	O
determiners	O
,	O
while	O
also	O
paying	O
nontrivial	O
attention	O
weights	O
to	O
the	O
adjectives	O
.	O
This	O
finding	O
matches	O
existing	O
head	O
rules	O
and	O
our	O
intuition	O
that	O
nouns	O
head	O
noun	O
phrases	O
,	O
and	O
that	O
adjectives	O
are	O
more	O
important	O
than	O
determiners	O
.	O
We	O
analyze	O
the	O
case	O
where	O
the	O
noun	O
phrase	O
contains	O
a	O
conjunction	O
in	O
the	O
last	O
three	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
.	O
The	O
syntax	O
of	O
conjunction	O
is	O
a	O
long	O
-	O
standing	O
source	O
of	O
controversy	O
in	O
syntactic	Task
analysis	Task
.	O
Our	O
model	O
suggests	O
that	O
several	O
representational	Method
strategies	Method
are	O
used	O
,	O
when	O
coordinating	O
single	O
nouns	O
,	O
both	O
the	O
first	O
noun	O
(	O
8	O
)	O
and	O
the	O
last	O
noun	O
(	O
9	O
)	O
may	O
be	O
selected	O
.	O
However	O
,	O
in	O
the	O
case	O
of	O
conjunctions	O
of	O
multiple	O
noun	O
phrases	O
(	O
as	O
opposed	O
to	O
multiple	O
single	O
-	O
word	O
nouns	O
)	O
,	O
the	O
model	O
consistently	O
picks	O
the	O
conjunction	O
as	O
the	O
head	O
.	O
All	O
of	O
these	O
representational	Method
strategies	Method
have	O
been	O
argued	O
for	O
individually	O
on	O
linguistic	O
grounds	O
,	O
and	O
since	O
we	O
see	O
all	O
of	O
them	O
present	O
,	O
RNNGs	Method
face	O
the	O
same	O
confusion	O
that	O
linguists	O
do	O
.	O
VPs	O
.	O
The	O
attention	O
weights	O
on	O
simple	O
verb	O
phrases	O
(	O
e.g.	O
,	O
“	O
VP	O
V	O
NP	O
”	O
,	O
9	O
)	O
are	O
peaked	O
around	O
the	O
noun	O
phrase	O
instead	O
of	O
the	O
verb	O
.	O
This	O
implies	O
that	O
the	O
verb	O
phrase	O
would	O
look	O
most	O
similar	O
to	O
the	O
noun	O
under	O
it	O
and	O
contradicts	O
existing	O
head	O
rules	O
that	O
unanimously	O
put	O
the	O
verb	O
as	O
the	O
head	O
of	O
the	O
verb	O
phrase	O
.	O
Another	O
interesting	O
finding	O
is	O
that	O
the	O
model	O
pays	O
attention	O
to	O
polarity	O
information	O
,	O
where	O
negations	O
are	O
almost	O
always	O
assigned	O
non	O
-	O
trivial	O
attention	O
weights	O
.	O
Furthermore	O
,	O
we	O
find	O
that	O
the	O
model	O
attends	O
to	O
the	O
conjunction	O
terminal	O
in	O
conjunctions	O
of	O
verb	O
phrases	O
(	O
e.g.	O
,	O
“	O
VP	O
VP	O
and	O
VP	O
”	O
,	O
10	O
)	O
,	O
reinforcing	O
the	O
similar	O
finding	O
for	O
conjunction	Task
of	Task
noun	Task
phrases	Task
.	O
PPs	O
.	O
In	O
almost	O
all	O
cases	O
,	O
the	O
model	O
attends	O
to	O
the	O
preposition	O
terminal	O
instead	O
of	O
the	O
noun	O
phrases	O
or	O
complete	O
clauses	O
under	O
it	O
,	O
regardless	O
of	O
the	O
type	O
of	O
preposition	O
.	O
Even	O
when	O
the	O
prepositional	O
phrase	O
is	O
only	O
used	O
to	O
make	O
a	O
connection	O
between	O
two	O
noun	O
phrases	O
(	O
e.g.	O
,	O
“	O
PP	O
NP	O
after	O
NP	O
”	O
,	O
10	O
)	O
,	O
the	O
prepositional	O
connector	O
is	O
still	O
considered	O
the	O
most	O
salient	O
element	O
.	O
This	O
is	O
less	O
consistent	O
with	O
the	O
Collins	O
and	O
Stanford	O
head	O
rules	O
,	O
where	O
prepositions	O
are	O
assigned	O
a	O
lower	O
priority	O
when	O
composing	O
PPs	O
,	O
although	O
more	O
consistent	O
with	O
the	O
Johansson	Method
head	Method
rule	Method
.	O
ADJP	O
VP	O
NP	O
PP	O
QP	O
SBAR	O
subsection	O
:	O
Comparison	O
to	O
Existing	O
Head	O
Rules	O
To	O
better	O
measure	O
the	O
overlap	O
between	O
the	O
attention	O
vectors	O
and	O
existing	O
head	O
rules	O
,	O
we	O
converted	O
the	O
trees	O
in	O
PTB	Material
§	O
23	O
into	O
a	O
dependency	Method
representation	Method
using	O
the	O
attention	O
weights	O
.	O
In	O
this	O
case	O
,	O
the	O
attention	O
weight	O
functions	O
as	O
a	O
“	O
dynamic	O
”	O
head	O
rule	O
,	O
where	O
all	O
other	O
constituents	O
within	O
the	O
same	O
composed	O
phrase	O
are	O
considered	O
to	O
modify	O
the	O
constituent	O
with	O
the	O
highest	O
attention	O
weight	O
,	O
repeated	O
recursively	O
.	O
The	O
head	O
of	O
the	O
composed	O
representation	O
for	O
“	O
S	O
”	O
at	O
the	O
top	O
of	O
the	O
tree	O
is	O
attached	O
to	O
a	O
special	O
root	O
symbol	O
and	O
functions	O
as	O
the	O
head	O
of	O
the	O
sentence	O
.	O
We	O
measure	O
the	O
overlap	O
between	O
the	O
resulting	O
tree	O
and	O
conversion	O
results	O
of	O
the	O
same	O
trees	O
using	O
the	O
collins_97	O
and	O
Stanford	O
dependencies	O
head	O
rules	O
.	O
Results	O
are	O
evaluated	O
using	O
the	O
standard	O
evaluation	Metric
script	Metric
(	O
excluding	O
punctuation	O
)	O
in	O
terms	O
of	O
UAS	Task
,	O
since	O
the	O
attention	O
weights	O
do	O
not	O
provide	O
labels	O
.	O
Results	O
.	O
The	O
model	O
has	O
a	O
higher	O
overlap	O
with	O
the	O
conversion	O
using	O
Collins	O
head	O
rules	O
(	O
49.8	O
UAS	O
)	O
rather	O
than	O
the	O
Stanford	O
head	O
rules	O
(	O
40.4	O
UAS	O
)	O
.	O
We	O
attribute	O
this	O
large	O
gap	O
to	O
the	O
fact	O
that	O
the	O
Stanford	O
head	O
rules	O
incorporate	O
more	O
semantic	O
considerations	O
,	O
while	O
the	O
RNNG	Method
is	O
a	O
purely	O
syntactic	Method
model	Method
.	O
In	O
general	O
,	O
the	O
attention	Method
-	Method
based	Method
tree	Method
output	Method
has	O
a	O
high	O
error	Metric
rate	Metric
(	O
90	O
%	O
)	O
when	O
the	O
dependent	O
is	O
a	O
verb	O
,	O
since	O
the	O
constituent	O
with	O
the	O
highest	O
attention	O
weight	O
in	O
a	O
verb	O
phrase	O
is	O
often	O
the	O
noun	O
phrase	O
instead	O
of	O
the	O
verb	O
,	O
as	O
discussed	O
above	O
.	O
The	O
conversion	Metric
accuracy	Metric
is	O
better	O
for	O
nouns	O
(	O
50	O
%	O
error	Metric
)	O
,	O
and	O
much	O
better	O
for	O
determiners	Task
(	O
30	O
%	O
)	O
and	O
particles	O
(	O
6	O
%	O
)	O
with	O
respect	O
to	O
the	O
Collins	O
head	O
rules	O
.	O
Discussion	O
.	O
GA	Method
-	Method
RNNG	Method
has	O
the	O
power	O
to	O
infer	O
head	O
rules	O
,	O
and	O
to	O
a	O
large	O
extent	O
,	O
it	O
does	O
.	O
It	O
follows	O
some	O
conventions	O
that	O
are	O
established	O
in	O
one	O
or	O
more	O
previous	O
head	O
rule	O
sets	O
(	O
e.g.	O
,	O
prepositions	O
head	O
prepositional	O
phrases	O
,	O
nouns	O
head	O
noun	O
phrases	O
)	O
,	O
but	O
attends	O
more	O
to	O
a	O
verb	O
phrase	O
’s	O
nominal	O
constituents	O
than	O
the	O
verb	O
.	O
It	O
is	O
important	O
to	O
note	O
that	O
this	O
is	O
not	O
the	O
by	O
-	O
product	O
of	O
learning	O
a	O
specific	O
model	O
for	O
parsing	Task
;	O
the	O
training	Metric
objective	Metric
is	O
joint	O
likelihood	O
,	O
which	O
is	O
not	O
a	O
proxy	Metric
loss	Metric
for	O
parsing	Task
performance	O
.	O
These	O
decisions	O
were	O
selected	O
because	O
they	O
make	O
the	O
data	O
maximally	O
likely	O
(	O
though	O
admittedly	O
only	O
locally	O
maximally	O
likely	O
)	O
.	O
We	O
leave	O
deeper	O
consideration	O
of	O
this	O
noun	O
-	O
centered	O
verb	O
phrase	O
hypothesis	O
to	O
future	O
work	O
.	O
section	O
:	O
The	O
Role	O
of	O
Nonterminal	O
Labels	O
Emboldened	O
by	O
our	O
finding	O
that	O
GA	Method
-	Method
RNNGs	Method
learn	O
a	O
notion	O
of	O
headedness	O
,	O
we	O
next	O
explore	O
whether	O
heads	O
are	O
sufficient	O
to	O
create	O
representations	O
of	O
phrases	O
(	O
in	O
line	O
with	O
an	O
endocentric	Method
theory	Method
of	Method
phrasal	Method
representation	Method
)	O
or	O
whether	O
extra	O
nonterminal	O
information	O
is	O
necessary	O
.	O
If	O
the	O
endocentric	Method
hypothesis	Method
is	O
true	O
(	O
that	O
is	O
,	O
the	O
representation	O
of	O
a	O
phrase	O
is	O
built	O
from	O
within	O
depending	O
on	O
its	O
components	O
but	O
independent	O
of	O
explicit	O
category	O
labels	O
)	O
,	O
then	O
the	O
nonterminal	O
types	O
should	O
be	O
easily	O
inferred	O
given	O
the	O
endocentrically	Method
-	Method
composed	Method
representation	Method
,	O
and	O
that	O
ablating	O
the	O
nonterminal	O
information	O
would	O
not	O
make	O
much	O
difference	O
in	O
performance	O
.	O
Specifically	O
,	O
we	O
train	O
a	O
GA	Method
-	Method
RNNG	Method
on	O
unlabeled	O
trees	O
(	O
only	O
bracketings	O
without	O
nonterminal	O
types	O
)	O
,	O
denoted	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
.	O
This	O
idea	O
has	O
been	O
explored	O
in	O
research	O
on	O
methods	O
for	O
learning	Task
syntax	Task
with	Task
less	Task
complete	Task
annotation	Task
.	O
A	O
key	O
finding	O
from	O
klein_02	O
was	O
that	O
,	O
given	O
bracketing	O
structure	O
,	O
simple	O
dimensionality	Method
reduction	Method
techniques	Method
could	O
reveal	O
conventional	O
nonterminal	O
categories	O
with	O
high	O
accuracy	Metric
;	O
petrov_2006	O
also	O
showed	O
that	O
latent	O
variables	O
can	O
be	O
used	O
to	O
recover	O
fine	O
-	O
grained	O
nonterminal	O
categories	O
.	O
We	O
therefore	O
expect	O
that	O
the	O
vector	O
embeddings	O
of	O
the	O
constituents	O
that	O
the	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
correctly	O
recovers	O
(	O
on	O
test	O
data	O
)	O
will	O
capture	O
categories	O
similar	O
to	O
those	O
in	O
the	O
Penn	Material
Treebank	Material
.	O
Experiments	O
.	O
Using	O
the	O
same	O
hyperparameters	O
and	O
the	O
PTB	Material
dataset	O
,	O
we	O
first	O
consider	O
unlabeled	O
parsing	Task
accuracy	O
.	O
On	O
test	O
data	O
(	O
with	O
the	O
usual	O
split	O
)	O
,	O
the	O
GA	Method
-	Method
RNNG	Method
achieves	O
94.2	O
%	O
,	O
while	O
the	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
achieves	O
93.5	O
%	O
.	O
This	O
result	O
suggests	O
that	O
nonterminal	O
category	O
labels	O
add	O
a	O
relatively	O
small	O
amount	O
of	O
information	O
compared	O
to	O
purely	O
endocentric	Method
representations	Method
.	O
Visualization	Task
.	O
If	O
endocentricity	Method
is	O
largely	O
sufficient	O
to	O
account	O
for	O
the	O
behavior	O
of	O
phrases	O
,	O
where	O
do	O
our	O
robust	O
intuitions	O
for	O
syntactic	O
category	O
types	O
come	O
from	O
?	O
We	O
use	O
t	Method
-	Method
SNE	Method
to	O
visualize	O
composed	O
phrase	O
vectors	O
from	O
the	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
model	Method
applied	O
to	O
the	O
unseen	O
test	O
data	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
that	O
the	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
tends	O
to	O
recover	O
nonterminal	O
categories	O
as	O
encoded	O
in	O
the	O
PTB	Material
,	O
even	O
when	O
trained	O
without	O
them	O
.	O
These	O
results	O
suggest	O
nonterminal	O
types	O
can	O
be	O
inferred	O
from	O
the	O
purely	O
endocentric	Method
compositional	Method
process	Method
to	O
a	O
certain	O
extent	O
,	O
and	O
that	O
the	O
phrase	O
clusters	O
found	O
by	O
the	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
largely	O
overlap	O
with	O
nonterminal	O
categories	O
.	O
Analysis	Task
of	Task
PP	Task
and	O
SBAR	Method
.	O
Figure	O
[	O
reference	O
]	O
indicates	O
a	O
certain	O
degree	O
of	O
overlap	O
between	O
SBAR	O
(	O
red	O
)	O
and	O
PP	O
(	O
yellow	O
)	O
.	O
As	O
both	O
categories	O
are	O
interesting	O
from	O
the	O
linguistic	O
perspective	O
and	O
quite	O
similar	O
,	O
we	O
visualize	O
the	O
learned	O
phrase	O
vectors	O
of	O
40	O
randomly	O
selected	O
SBARs	O
and	O
PPs	O
from	O
the	O
test	O
set	O
(	O
using	O
U	Method
-	Method
GA	Method
-	Method
RNNG	Method
)	O
,	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
First	O
,	O
we	O
can	O
see	O
that	O
phrase	Method
representations	Method
for	O
PPs	O
and	O
SBARs	O
depend	O
less	O
on	O
the	O
nonterminal	O
categories	O
and	O
more	O
on	O
the	O
connector	O
.	O
For	O
instance	O
,	O
the	O
model	O
learns	O
to	O
cluster	O
phrases	O
that	O
start	O
with	O
words	O
that	O
can	O
be	O
either	O
prepositions	O
or	O
complementizers	O
(	O
e.g.	O
,	O
for	O
,	O
at	O
,	O
to	O
,	O
under	O
,	O
by	O
)	O
,	O
regardless	O
of	O
whether	O
the	O
true	O
nonterminal	O
labels	O
are	O
PPs	O
or	O
SBARs	O
.	O
This	O
suggests	O
that	O
SBARs	Method
that	O
start	O
with	O
“	O
prepositional	O
”	O
words	O
are	O
similar	O
to	O
PPs	O
from	O
the	O
model	O
’s	O
perspective	O
.	O
Second	O
,	O
the	O
model	O
learns	O
to	O
disregard	O
the	O
word	O
that	O
,	O
as	O
“	O
”	O
and	O
“	O
”	O
are	O
close	O
together	O
.	O
This	O
finding	O
is	O
intuitive	O
,	O
as	O
complementizer	O
that	O
is	O
often	O
optional	O
,	O
unlike	O
prepositional	O
words	O
that	O
might	O
describe	O
relative	O
time	O
and	O
location	O
.	O
Third	O
,	O
certain	O
categories	O
of	O
PPs	O
and	O
SBARs	O
form	O
their	O
own	O
separate	O
clusters	O
,	O
such	O
as	O
those	O
that	O
involve	O
the	O
words	O
because	O
and	O
of	O
.	O
We	O
attribute	O
these	O
distinctions	O
to	O
the	O
fact	O
that	O
these	O
words	O
convey	O
different	O
meanings	O
than	O
many	O
prepositional	O
words	O
;	O
the	O
word	O
of	O
indicates	O
possession	O
while	O
because	O
indicates	O
cause	O
-	O
and	O
-	O
effect	O
relationship	O
.	O
These	O
examples	O
show	O
that	O
,	O
to	O
a	O
certain	O
extent	O
,	O
the	O
GA	Method
-	Method
RNNG	Method
is	O
able	O
to	O
learn	O
non	O
-	O
trivial	O
semantic	O
information	O
,	O
even	O
when	O
trained	O
on	O
a	O
fairly	O
small	O
amount	O
of	O
syntactic	O
data	O
.	O
section	O
:	O
Related	O
Work	O
The	O
problem	O
of	O
understanding	O
neural	Method
network	Method
models	Method
in	O
NLP	Task
has	O
been	O
previously	O
studied	O
for	O
sequential	Method
RNNs	Method
.	O
shi_16	O
showed	O
that	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
neural	Method
translation	Method
models	Method
capture	O
a	O
certain	O
degree	O
of	O
syntactic	O
knowledge	O
of	O
the	O
source	O
language	O
,	O
such	O
as	O
voice	O
(	O
active	O
or	O
passive	O
)	O
and	O
tense	O
information	O
,	O
as	O
a	O
by	O
-	O
product	O
of	O
the	O
translation	Metric
objective	Metric
.	O
Our	O
experiment	O
on	O
the	O
importance	O
of	O
composition	O
function	O
was	O
motivated	O
by	O
vinyals:2015	O
and	O
wiseman_16	Method
,	O
who	O
achieved	O
competitive	O
parsing	Task
accuracy	O
without	O
explicit	O
composition	O
.	O
In	O
another	O
work	O
,	O
li:15	O
investigated	O
the	O
importance	O
of	O
recursive	O
tree	O
structures	O
(	O
as	O
opposed	O
to	O
linear	Method
recurrent	Method
models	Method
)	O
in	O
four	O
different	O
tasks	O
,	O
including	O
sentiment	Task
and	Task
semantic	Task
relation	Task
classification	Task
.	O
Their	O
findings	O
suggest	O
that	O
recursive	O
tree	O
structures	O
are	O
beneficial	O
for	O
tasks	O
that	O
require	O
identifying	Task
long	Task
-	Task
range	Task
relations	Task
,	O
such	O
as	O
semantic	Task
relationship	Task
classification	Task
,	O
with	O
no	O
conclusive	O
advantage	O
for	O
sentiment	Task
classification	Task
and	O
discourse	O
parsing	Task
.	O
Through	O
the	O
stack	Method
-	Method
only	Method
ablation	Method
we	O
demonstrate	O
that	O
the	O
RNNG	Method
composition	Method
function	Method
is	O
crucial	O
to	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
parsing	Task
performance	O
.	O
Extensive	O
prior	O
work	O
on	O
phrase	O
-	O
structure	O
parsing	Task
typically	O
employs	O
the	O
probabilistic	Method
context	Method
-	Method
free	Method
grammar	Method
formalism	Method
,	O
with	O
lexicalized	Method
and	Method
nonterminal	Method
augmentations	Method
.	O
The	O
conjecture	O
that	O
fine	O
-	O
grained	O
nonterminal	O
rules	O
and	O
labels	O
can	O
be	O
discovered	O
given	O
weaker	O
bracketing	O
structures	O
was	O
based	O
on	O
several	O
studies	O
.	O
In	O
a	O
similar	O
work	O
,	O
sangati:09	O
proposed	O
entropy	Method
minimization	Method
and	O
greedy	Method
familiarity	Method
maximization	Method
techniques	Method
to	O
obtain	O
lexical	O
heads	O
from	O
labeled	O
phrase	O
-	O
structure	O
trees	O
in	O
an	O
unsupervised	Method
manner	Method
.	O
In	O
contrast	O
,	O
we	O
used	O
neural	Method
attention	Method
to	O
obtain	O
the	O
“	O
head	O
rules	O
”	O
in	O
the	O
GA	Method
-	Method
RNNG	Method
;	O
the	O
whole	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
maximize	O
the	O
log	O
probability	O
of	O
the	O
correct	O
action	O
given	O
the	O
history	O
.	O
Unlike	O
prior	O
work	O
,	O
GA	Method
-	Method
RNNG	Method
allows	O
the	O
attention	O
weight	O
to	O
be	O
divided	O
among	O
phrase	O
constituents	O
,	O
essentially	O
propagating	O
(	O
weighted	O
)	O
headedness	O
information	O
from	O
multiple	O
components	O
.	O
section	O
:	O
Conclusion	O
We	O
probe	O
what	O
recurrent	Method
neural	Method
network	Method
grammars	Method
learn	O
about	O
syntax	O
,	O
through	O
ablation	Method
scenarios	Method
and	O
a	O
novel	O
variant	O
with	O
a	O
gated	Method
attention	Method
mechanism	Method
on	O
the	O
composition	O
function	O
.	O
The	O
composition	O
function	O
,	O
a	O
key	O
differentiator	O
between	O
the	O
RNNG	Method
and	O
other	O
neural	Method
models	Method
of	Method
syntax	Method
,	O
is	O
crucial	O
for	O
good	O
performance	O
.	O
Using	O
the	O
attention	O
vectors	O
we	O
discover	O
that	O
the	O
model	O
is	O
learning	O
something	O
similar	O
to	O
heads	O
,	O
although	O
the	O
attention	O
vectors	O
are	O
not	O
completely	O
peaked	O
around	O
a	O
single	O
component	O
.	O
We	O
show	O
some	O
cases	O
where	O
the	O
attention	O
vector	O
is	O
divided	O
and	O
measure	O
the	O
relationship	O
with	O
existing	O
head	O
rules	O
.	O
RNNGs	Method
without	O
access	O
to	O
nonterminal	O
information	O
during	O
training	O
are	O
used	O
to	O
support	O
the	O
hypothesis	O
that	O
phrasal	Method
representations	Method
are	O
largely	O
endocentric	O
,	O
and	O
a	O
visualization	Method
of	Method
representations	Method
shows	O
that	O
traditional	O
nonterminal	O
categories	O
fall	O
out	O
naturally	O
from	O
the	O
composed	O
phrase	O
vectors	O
.	O
This	O
confirms	O
previous	O
conjectures	O
that	O
bracketing	Method
annotation	Method
does	O
most	O
of	O
the	O
work	O
of	O
syntax	Task
,	O
with	O
nonterminal	O
categories	O
easily	O
discoverable	O
given	O
bracketing	Method
.	O
section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Information	O
Innovation	O
Office	O
(	O
I2O	O
)	O
under	O
the	O
Low	O
Resource	O
Languages	O
for	O
Emergent	O
Incidents	O
(	O
LORELEI	O
)	O
program	O
issued	O
by	O
DARPA	O
/	O
I2O	O
under	O
Contract	O
No	O
.	O
HR0011	O
-	O
15	O
-	O
C	O
-	O
0114	O
;	O
it	O
was	O
also	O
supported	O
in	O
part	O
by	O
Contract	O
No	O
.	O
W911NF	O
-	O
15	O
-	O
1	O
-	O
0543	O
with	O
DARPA	O
and	O
the	O
Army	O
Research	O
Office	O
(	O
ARO	O
)	O
.	O
Approved	O
for	O
public	O
release	O
,	O
distribution	O
unlimited	O
.	O
The	O
views	O
expressed	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
official	O
policy	O
or	O
position	O
of	O
the	O
Department	O
of	O
Defense	O
or	O
the	O
U.S.	O
Government	O
.	O
bibliography	O
:	O
References	O
