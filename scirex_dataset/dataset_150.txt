Morphosyntactic	Method
Tagging	Method
with	O
a	O
Meta	Method
-	Method
BiLSTM	Method
Model	Method
over	O
Context	Method
Sensitive	Method
Token	Method
Encodings	Method
section	O
:	O
Abstract	O
The	O
rise	O
of	O
neural	Method
networks	Method
,	O
and	O
particularly	O
recurrent	Method
neural	Method
networks	Method
,	O
has	O
produced	O
significant	O
advances	O
in	O
part	Metric
-	Metric
ofspeech	Metric
tagging	Metric
accuracy	Metric
[	O
reference	O
]	O
.	O
One	O
characteristic	O
common	O
among	O
these	O
models	O
is	O
the	O
presence	O
of	O
rich	O
initial	O
word	Method
encodings	Method
.	O
These	O
encodings	O
typically	O
are	O
composed	O
of	O
a	O
recurrent	Method
character	Method
-	Method
based	Method
representation	Method
with	O
learned	O
and	O
pre	Method
-	Method
trained	Method
word	Method
embeddings	Method
.	O
However	O
,	O
these	O
encodings	O
do	O
not	O
consider	O
a	O
context	O
wider	O
than	O
a	O
single	O
word	O
and	O
it	O
is	O
only	O
through	O
subsequent	O
recurrent	Method
layers	Method
that	O
word	O
or	O
sub	O
-	O
word	O
information	O
interacts	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
models	O
that	O
use	O
recurrent	Method
neural	Method
networks	Method
with	O
sentence	O
-	O
level	O
context	O
for	O
initial	O
character	Method
and	Method
word	Method
-	Method
based	Method
representations	Method
.	O
In	O
particular	O
we	O
show	O
that	O
optimal	O
results	O
are	O
obtained	O
by	O
integrating	O
these	O
context	Method
sensitive	Method
representations	Method
through	O
synchronized	Method
training	Method
with	O
a	O
meta	Method
-	Method
model	Method
that	O
learns	O
to	O
combine	O
their	O
states	O
.	O
We	O
present	O
results	O
on	O
part	Task
-	Task
of	Task
-	Task
speech	Task
and	O
morphological	Task
tagging	Task
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
number	O
of	O
languages	O
.	O
section	O
:	O
Introduction	O
Morphosyntactic	O
tagging	O
accuracy	Metric
has	O
seen	O
dramatic	O
improvements	O
through	O
the	O
adoption	O
of	O
recurrent	Method
neural	Method
networks	Method
-	O
specifically	O
BiLSTMs	Method
[	O
reference	O
][	O
reference	O
]	O
to	O
create	O
sentence	Method
-	Method
level	Method
context	Method
sensitive	Method
encodings	Method
of	O
words	O
.	O
A	O
successful	O
recipe	O
is	O
to	O
first	O
create	O
an	O
initial	O
context	Method
insensitive	Method
word	Method
representation	Method
,	O
which	O
usually	O
has	O
three	O
main	O
parts	O
:	O
1	O
)	O
A	O
dynamically	Method
trained	Method
word	Method
embedding	Method
;	O
2	O
)	O
a	O
fixed	O
pre	Method
-	Method
trained	Method
word	Method
-	Method
embedding	Method
,	O
induced	O
from	O
a	O
large	O
corpus	O
;	O
and	O
3	O
)	O
a	O
sub	Method
-	Method
word	Method
character	Method
model	Method
,	O
which	O
itself	O
is	O
usually	O
the	O
final	O
state	O
of	O
a	O
recurrent	Method
model	Method
that	O
ingests	O
one	O
character	O
at	O
a	O
time	O
.	O
Such	O
word	Method
/	Method
sub	Method
-	Method
word	Method
models	Method
originated	O
with	O
[	O
reference	O
]	O
.	O
Recently	O
,	O
[	O
reference	O
]	O
used	O
precisely	O
such	O
a	O
context	Method
insensitive	Method
word	Method
representation	Method
as	O
input	O
to	O
a	O
BiLSTM	Method
in	O
order	O
to	O
obtain	O
context	O
sensitive	O
word	Method
encodings	Method
used	O
to	O
predict	O
partof	O
-	O
speech	O
tags	O
.	O
The	O
Dozat	O
et	O
al	O
.	O
model	O
had	O
the	O
highest	O
accuracy	Metric
of	O
all	O
participating	O
systems	O
in	O
the	O
CoNLL	Task
2017	Task
shared	Task
task	Task
[	O
reference	O
]	O
.	O
In	O
such	O
a	O
model	O
,	O
sub	Method
-	Method
word	Method
character	Method
-	Method
based	Method
representations	Method
only	O
interact	O
indirectly	O
via	O
subsequent	O
recurrent	Method
layers	Method
.	O
For	O
example	O
,	O
consider	O
the	O
sentence	O
I	O
had	O
shingles	O
,	O
which	O
is	O
a	O
painful	O
disease	O
.	O
Context	Method
insensitive	Method
character	Method
and	Method
word	Method
representations	Method
may	O
have	O
learned	O
that	O
for	O
unknown	O
or	O
infrequent	O
words	O
like	O
'	O
shingles	O
'	O
,	O
's	O
'	O
and	O
more	O
so	O
'	O
es	O
'	O
is	O
a	O
common	O
way	O
to	O
end	O
a	O
plural	O
noun	O
.	O
It	O
is	O
up	O
to	O
the	O
subsequent	O
BiLSTM	Method
layer	O
to	O
override	O
this	O
once	O
it	O
sees	O
the	O
singular	O
verb	O
is	O
to	O
the	O
right	O
.	O
Note	O
that	O
this	O
differs	O
from	O
traditional	O
linear	Method
models	Method
where	O
word	Method
and	Method
sub	Method
-	Method
word	Method
representations	Method
are	O
directly	O
concatenated	O
with	O
similar	O
features	O
in	O
the	O
surrounding	O
context	O
[	O
reference	O
]	O
.	O
In	O
this	O
paper	O
we	O
aim	O
to	O
investigate	O
to	O
what	O
extent	O
having	O
initial	O
sub	O
-	O
word	O
and	O
word	O
context	O
insensitive	O
representations	O
affects	O
performance	O
.	O
We	O
propose	O
a	O
novel	O
model	O
where	O
we	O
learn	O
context	O
sensitive	O
initial	O
character	O
and	O
word	Method
representations	Method
through	O
two	O
separate	O
sentence	Method
-	Method
level	Method
recurrent	Method
models	Method
.	O
These	O
are	O
then	O
combined	O
via	O
a	O
metaBiLSTM	Method
model	Method
that	O
builds	O
a	O
unified	Method
representation	Method
of	O
each	O
word	O
that	O
is	O
then	O
used	O
for	O
syntactic	Task
tagging	Task
.	O
Critically	O
,	O
while	O
each	O
of	O
these	O
three	O
models	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
are	O
trained	O
synchronously	O
,	O
they	O
are	O
ultimately	O
separate	O
models	O
using	O
different	O
network	O
configurations	O
,	O
training	O
hyperparameters	O
and	O
loss	O
functions	O
.	O
Empirically	O
,	O
we	O
found	O
this	O
optimal	O
as	O
it	O
allowed	O
control	O
over	O
the	O
fact	O
that	O
each	O
representation	O
has	O
a	O
different	O
learning	O
capacity	O
.	O
We	O
tested	O
the	O
system	O
on	O
the	O
2017	O
CoNLL	Material
shared	Material
task	Material
data	Material
sets	Material
and	O
gain	O
improvements	O
compared	O
to	O
the	O
top	O
performing	O
systems	O
for	O
the	O
majority	O
of	O
languages	O
for	O
part	Task
-	Task
of	Task
-	Task
speech	Task
and	O
morphological	Task
tagging	Task
.	O
As	O
we	O
will	O
see	O
,	O
a	O
pattern	O
emerged	O
where	O
gains	O
were	O
largest	O
for	O
morphologically	O
rich	O
languages	O
,	O
especially	O
those	O
in	O
the	O
Slavic	O
family	O
group	O
.	O
We	O
also	O
applied	O
the	O
approach	O
to	O
the	O
benchmark	O
English	Material
PTB	Material
data	Material
,	O
where	O
our	O
model	O
achieved	O
97.9	O
using	O
the	O
standard	O
train	Metric
/	Metric
dev	Metric
/	Metric
test	Metric
split	Metric
,	O
which	O
constitutes	O
a	O
relative	O
reduction	O
in	O
error	Metric
of	O
12	O
%	O
over	O
the	O
previous	O
best	O
system	O
.	O
section	O
:	O
Related	O
Work	O
While	O
sub	Method
-	Method
word	Method
representations	Method
are	O
often	O
attributed	O
to	O
the	O
advent	O
of	O
deep	Method
learning	Method
in	O
NLP	Task
,	O
it	O
was	O
,	O
in	O
fact	O
,	O
commonplace	O
for	O
linear	Method
featurized	Method
machine	Method
learning	Method
methods	Method
to	O
incorporate	O
such	O
representations	O
.	O
While	O
the	O
literature	O
is	O
too	O
large	O
to	O
enumerate	O
,	O
[	O
reference	O
]	O
is	O
a	O
good	O
example	O
of	O
an	O
accurate	O
linear	Method
model	Method
that	O
uses	O
both	O
word	O
and	O
sub	O
-	O
word	O
features	O
.	O
Specifically	O
,	O
like	O
most	O
systems	O
of	O
the	O
time	O
,	O
they	O
use	O
ngram	O
affix	O
features	O
,	O
which	O
were	O
made	O
context	O
sensitive	O
via	O
manually	O
constructed	O
conjunctions	O
with	O
features	O
from	O
other	O
words	O
in	O
a	O
fixed	O
window	O
.	O
[	O
reference	O
]	O
was	O
perhaps	O
the	O
first	O
modern	O
neural	Method
network	Method
for	O
tagging	Task
.	O
While	O
this	O
first	O
study	O
used	O
only	O
word	Method
embeddings	Method
,	O
a	O
subsequent	O
model	O
extended	O
the	O
representation	O
to	O
include	O
suffix	O
embeddings	O
[	O
reference	O
]	O
.	O
The	O
seminal	O
dependency	Task
parsing	Task
paper	O
of	O
[	O
reference	O
]	O
led	O
to	O
a	O
number	O
of	O
tagging	O
papers	O
that	O
used	O
their	O
basic	O
architecture	O
of	O
highly	O
featurized	Method
(	Method
and	Method
embedded	Method
)	Method
feed	Method
-	Method
forward	Method
neural	Method
networks	Method
.	O
[	O
reference	O
]	O
,	O
for	O
example	O
,	O
studied	O
this	O
architecture	O
in	O
a	O
low	Task
resource	Task
setting	Task
using	O
word	O
,	O
sub	O
-	O
word	O
(	O
prefix	O
/	O
suffix	O
)	O
and	O
induced	O
cluster	O
features	O
to	O
obtain	O
competitive	O
accuracy	Metric
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
extended	O
the	O
work	O
of	O
Chen	O
et	O
al	O
.	O
to	O
a	O
structured	Task
prediction	Task
setting	Task
,	O
the	O
later	O
two	O
use	O
again	O
a	O
mix	O
of	O
word	O
and	O
sub	O
-	O
word	O
features	O
.	O
The	O
idea	O
of	O
using	O
a	O
recurrent	Method
layer	Method
over	O
characters	O
to	O
induce	O
a	O
complementary	O
view	O
of	O
a	O
word	O
has	O
occurred	O
in	O
numerous	O
papers	O
.	O
Perhaps	O
the	O
earliest	O
is	O
Santos	O
and	O
Zadrozny	O
(	O
2014	O
)	O
who	O
compare	O
character	O
-	O
based	O
LSTM	Method
encodings	O
to	O
traditional	O
word	Method
-	Method
based	Method
embeddings	Method
.	O
[	O
reference	O
]	O
take	O
this	O
a	O
step	O
further	O
and	O
combine	O
the	O
word	Method
embeddings	Method
with	O
a	O
recurrent	Method
character	Method
encoding	Method
of	O
the	O
word	O
-	O
instead	O
of	O
just	O
relying	O
on	O
one	O
or	O
the	O
other	O
.	O
[	O
reference	O
]	O
use	O
a	O
sentencelevel	O
character	O
LSTM	Method
encoding	O
for	O
parsing	Task
.	O
[	O
reference	O
]	O
show	O
that	O
contextual	Method
embeddings	Method
using	O
character	Method
convolutions	Method
improve	O
accuracy	Metric
for	O
number	O
of	O
NLP	Task
tasks	Task
.	O
[	O
reference	O
]	O
is	O
probably	O
the	O
jumping	O
-	O
off	O
point	O
for	O
most	O
current	O
architectures	O
for	O
tagging	Method
models	Method
with	O
recurrent	Method
neural	Method
networks	Method
.	O
Specifically	O
,	O
they	O
used	O
a	O
combined	O
word	Method
embedding	Method
and	O
recurrent	Method
character	Method
encoding	Method
as	O
the	O
initial	O
input	O
to	O
a	O
BiLSTM	Method
that	O
generated	O
context	O
sensitive	O
word	Method
encodings	Method
.	O
Though	O
,	O
like	O
most	O
previous	O
studies	O
,	O
these	O
initial	O
encodings	O
were	O
context	O
insensitive	O
and	O
relied	O
on	O
subsequent	O
layers	O
to	O
encode	O
sentence	O
-	O
level	O
interactions	O
.	O
Finally	O
,	O
[	O
reference	O
]	O
showed	O
that	O
subword	Method
/	Method
word	Method
combination	Method
representations	Method
lead	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
morphosyntactic	Task
tagging	Task
accuracy	Metric
across	O
a	O
number	O
of	O
languages	O
in	O
the	O
CoNLL	Material
2017	Material
shared	Material
task	Material
[	O
reference	O
]	O
.	O
Their	O
word	Method
representation	Method
consisted	O
of	O
three	O
parts	O
:	O
1	O
)	O
A	O
dynamically	Method
trained	Method
word	Method
embedding	Method
;	O
2	O
)	O
a	O
fixed	Method
pretrained	Method
word	Method
embedding	Method
;	O
3	O
)	O
a	O
character	O
LSTM	Method
encoding	O
that	O
summed	O
the	O
final	O
state	O
of	O
the	O
recurrent	Method
model	Method
with	O
vector	Method
constructed	O
using	O
an	O
attention	Method
mechanism	Method
over	O
all	O
character	O
states	O
.	O
Again	O
,	O
the	O
initial	O
representations	O
are	O
all	O
context	O
insensitive	O
.	O
As	O
this	O
model	O
is	O
currently	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
morphosyntactic	Task
tagging	Task
,	O
it	O
will	O
serve	O
as	O
a	O
baseline	O
during	O
our	O
discussion	O
and	O
experiments	O
.	O
section	O
:	O
Models	O
In	O
this	O
section	O
,	O
we	O
introduce	O
models	O
that	O
we	O
investigate	O
and	O
experiment	O
with	O
in	O
§	O
4	O
.	O
section	O
:	O
Sentence	Method
-	Method
based	Method
Character	Method
Model	Method
The	O
feature	O
that	O
distinguishes	O
our	O
model	O
most	O
from	O
previous	O
work	O
is	O
that	O
we	O
apply	O
a	O
bidirectional	Method
recurrent	Method
layer	Method
(	O
LSTM	Method
)	O
on	O
all	O
characters	O
of	O
a	O
sentence	O
to	O
induce	O
fully	O
context	O
sensitive	O
initial	O
word	Method
encodings	Method
.	O
That	O
is	O
,	O
we	O
do	O
not	O
restrict	O
the	O
context	O
of	O
this	O
layer	O
to	O
the	O
words	O
themselves	O
(	O
as	O
in	O
Figure	O
1b	O
)	O
.	O
Figure	O
1a	O
shows	O
the	O
sentence	Method
-	Method
based	Method
character	Method
model	Method
applied	O
to	O
an	O
example	O
token	O
in	O
context	O
.	O
The	O
character	Method
model	Method
uses	O
,	O
as	O
input	O
,	O
sentences	O
split	O
into	O
UTF8	O
characters	O
.	O
We	O
include	O
the	O
spaces	O
between	O
the	O
tokens	O
1	O
in	O
the	O
input	O
and	O
map	O
each	O
character	O
to	O
a	O
dynamically	O
learned	O
embedding	O
.	O
Next	O
,	O
a	O
forward	O
LSTM	Method
reads	O
the	O
characters	O
from	O
left	O
to	O
right	O
and	O
a	O
backward	O
LSTM	Method
reads	O
sentences	O
from	O
right	O
to	O
left	O
,	O
in	O
standard	O
BiLSTM	Method
fashion	Method
.	O
More	O
formally	O
,	O
for	O
an	O
n	O
-	O
character	O
sentence	O
,	O
we	O
apply	O
for	O
each	O
character	O
embedding	O
(	O
e	O
char	O
1	O
,	O
...	O
,	O
e	O
char	O
n	O
)	O
a	O
BiLSTM	Method
:	O
As	O
is	O
also	O
typical	O
,	O
we	O
can	O
have	O
multiple	O
such	O
layers	O
(	O
l	O
)	O
that	O
feed	O
into	O
each	O
other	O
through	O
the	O
concatenation	Method
of	Method
previous	Method
layer	Method
encodings	Method
.	O
The	O
last	O
layer	O
l	O
has	O
both	O
forward	O
(	O
f	O
l	O
c	O
,	O
1	O
,	O
...	O
,	O
f	O
l	O
c	O
,	O
n	O
)	O
and	O
backward	O
(	O
b	O
l	O
c	O
,	O
1	O
,	O
...	O
,	O
b	O
l	O
c	O
,	O
n	O
)	O
output	O
vectors	O
for	O
each	O
character	O
.	O
To	O
create	O
word	Method
encodings	Method
,	O
we	O
need	O
to	O
combine	O
a	O
relevant	O
subset	O
of	O
these	O
context	Method
sensitive	Method
character	Method
encodings	Method
.	O
These	O
word	Method
encodings	Method
can	O
then	O
be	O
used	O
in	O
a	O
model	O
that	O
assigns	O
morphosyntactic	O
tags	O
to	O
each	O
word	O
directly	O
or	O
via	O
subsequent	O
layers	O
.	O
To	O
accomplish	O
this	O
,	O
the	O
model	O
concatenates	O
up	O
to	O
four	O
character	O
output	O
vectors	O
:	O
the	O
{	O
forward	O
,	O
backward	O
}	O
output	O
of	O
the	O
{	O
first	O
,	O
last	O
}	O
character	O
in	O
the	O
token	O
(	O
F	O
1st	O
(	O
w	O
)	O
,	O
F	O
last	O
(	O
w	O
)	O
,	O
B	O
1st	O
(	O
w	O
)	O
,	O
B	O
last	O
(	O
w	O
)	O
)	O
.	O
In	O
Figure	O
1a	O
,	O
the	O
four	O
shaded	O
boxes	O
indicate	O
these	O
four	O
outputs	O
for	O
the	O
example	O
token	O
.	O
Thus	O
,	O
the	O
proposed	O
model	O
concatenates	O
all	O
four	O
of	O
these	O
and	O
passes	O
it	O
as	O
input	O
to	O
an	O
multilayer	Method
perceptron	Method
(	O
MLP	Method
)	O
:	O
A	O
tag	O
can	O
then	O
be	O
predicted	O
with	O
a	O
linear	Method
classifier	Method
that	O
takes	O
as	O
input	O
the	O
output	O
of	O
the	O
MLP	Method
enized	Method
/	O
segmented	Method
.	O
,	O
applies	O
a	O
softmax	Method
function	Method
and	O
chooses	O
for	O
each	O
word	O
the	O
tag	O
with	O
highest	O
probability	O
.	O
Table	O
8	O
investigates	O
the	O
empirical	O
impact	O
of	O
alternative	O
definitions	O
of	O
g	O
i	O
that	O
concatenate	O
only	O
subsets	O
of	O
{	O
F	O
1st	O
(	O
w	O
)	O
,	O
F	O
last	O
(	O
w	O
)	O
,	O
B	O
1st	O
(	O
w	O
)	O
,	O
B	O
last	O
(	O
w	O
)	O
}.	O
section	O
:	O
Word	Method
-	Method
based	Method
Character	Method
Model	Method
To	O
investigate	O
whether	O
a	O
sentence	Method
sensitive	Method
character	Method
model	Method
is	O
better	O
than	O
a	O
character	Method
model	Method
where	O
the	O
context	O
is	O
restricted	O
to	O
the	O
characters	O
of	O
a	O
word	O
,	O
we	O
reimplemented	O
the	O
word	Method
-	Method
based	Method
character	Method
model	Method
of	O
[	O
reference	O
]	O
as	O
shown	O
in	O
[	O
reference	O
]	O
.	O
This	O
model	O
uses	O
the	O
final	O
state	O
of	O
a	O
unidirectional	O
LSTM	Method
over	O
the	O
characters	O
of	O
the	O
word	O
,	O
combined	O
with	O
the	O
attention	Method
mechanism	Method
of	O
Cao	O
and	O
Rei	O
(	O
2016	O
)	O
over	O
all	O
characters	O
.	O
We	O
refer	O
the	O
reader	O
to	O
those	O
works	O
for	O
more	O
details	O
.	O
Critically	O
,	O
however	O
,	O
all	O
the	O
information	O
fed	O
to	O
this	O
representation	O
comes	O
from	O
the	O
word	O
itself	O
,	O
and	O
not	O
a	O
wider	O
sentence	O
-	O
level	O
context	O
.	O
section	O
:	O
Sentence	Method
-	Method
based	Method
Word	Method
Model	Method
We	O
used	O
a	O
similar	O
setup	O
for	O
our	O
context	O
sensitive	O
word	Method
encodings	Method
as	O
the	O
character	Method
encodings	Method
.	O
There	O
are	O
a	O
few	O
differences	O
.	O
Obviously	O
,	O
the	O
inputs	O
are	O
the	O
words	O
of	O
the	O
sentence	O
.	O
For	O
each	O
of	O
the	O
words	O
,	O
we	O
use	O
pretrained	O
word	O
embeddings	O
(	O
p	O
word	O
The	O
summed	O
embeddings	O
in	O
i	O
are	O
passed	O
as	O
input	O
to	O
one	O
or	O
more	O
BiLSTM	Method
layers	Method
whose	O
output	O
f	O
l	O
w	O
,	O
i	O
,	O
b	O
l	O
w	O
,	O
i	O
is	O
concatenated	O
and	O
used	O
as	O
the	O
final	O
encoding	O
,	O
which	O
is	O
then	O
passed	O
to	O
an	O
MLP	Method
It	O
should	O
be	O
noted	O
,	O
that	O
the	O
output	O
of	O
this	O
BiL	Method
-	Method
STM	Method
is	O
essentially	O
the	O
Dozat	O
et	O
al	O
.	O
model	O
before	O
tag	Task
prediction	Task
,	O
with	O
the	O
exception	O
that	O
the	O
wordbased	Method
character	Method
encodings	Method
are	O
excluded	O
.	O
section	O
:	O
Meta	Method
-	Method
BiLSTM	Method
:	O
Model	Method
Combination	Method
Given	O
initial	O
word	Method
encodings	Method
,	O
both	O
character	O
and	O
word	O
-	O
based	O
,	O
a	O
common	O
strategy	O
is	O
to	O
pass	O
these	O
through	O
a	O
sentence	O
-	O
level	O
BiLSTM	Method
to	O
create	O
context	O
sensitive	O
encodings	O
,	O
e.g.	O
,	O
this	O
is	O
precisely	O
what	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
do	O
.	O
However	O
,	O
we	O
found	O
that	O
if	O
we	O
trained	O
each	O
of	O
the	O
character	Method
-	Method
based	Method
and	Method
word	Method
-	Method
based	Method
encodings	Method
with	O
their	O
own	O
loss	O
,	O
and	O
combined	O
them	O
using	O
an	O
additional	O
meta	O
-	O
BiLSTM	Method
model	O
,	O
we	O
obtained	O
optimal	O
performance	O
.	O
In	O
the	O
meta	O
-	O
BiLSTM	Method
model	O
,	O
we	O
concatenate	O
the	O
output	O
,	O
for	O
each	O
word	O
,	O
of	O
its	O
context	O
sensitive	O
character	O
and	O
word	Method
-	Method
based	Method
encodings	Method
,	O
and	O
put	O
this	O
through	O
another	O
BiLSTM	Method
to	O
create	O
an	O
additional	O
combined	Method
context	Method
sensitive	Method
encoding	Method
.	O
This	O
is	O
followed	O
by	O
a	O
final	O
MLP	Method
whose	O
output	O
is	O
passed	O
to	O
a	O
linear	Method
layer	Method
for	O
tag	Task
prediction	Task
.	O
With	O
this	O
setup	O
,	O
each	O
of	O
the	O
models	O
can	O
be	O
optimized	O
independently	O
which	O
we	O
describe	O
in	O
more	O
detail	O
in	O
§	O
3.5	O
.	O
Figure	O
2b	O
depicts	O
the	O
architecture	O
of	O
the	O
combined	O
system	O
and	O
contrasts	O
it	O
with	O
that	O
of	O
the	O
[	O
reference	O
]	O
(	O
Figure	O
2a	O
)	O
.	O
section	O
:	O
Training	O
Schema	O
As	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
the	O
character	Method
and	Method
word	Method
-	Method
based	Method
encoding	Method
models	Method
have	O
their	O
own	O
tagging	O
loss	O
functions	O
,	O
which	O
are	O
trained	O
independently	O
and	O
joined	O
via	O
the	O
meta	O
-	O
BiLSTM	Method
.	O
I.e.	O
,	O
the	O
loss	O
of	O
each	O
model	O
is	O
minimized	O
independently	O
by	O
separate	O
optimizers	Method
with	O
their	O
own	O
hyperparameters	O
.	O
Thus	O
,	O
it	O
is	O
in	O
some	O
sense	O
a	O
multitask	Method
learning	Method
model	Method
and	O
we	O
must	O
define	O
a	O
schedule	O
in	O
which	O
individual	O
models	O
are	O
updated	O
.	O
We	O
opted	O
for	O
a	O
simple	O
synchronous	Method
schedule	Method
outline	O
in	O
Algorithm	O
1	O
.	O
Here	O
,	O
during	O
each	O
epoch	O
,	O
we	O
update	O
each	O
of	O
the	O
models	O
in	O
sequence	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
using	O
the	O
entire	O
training	O
data	O
.	O
Algorithm	O
1	O
:	O
Training	O
procedure	O
for	O
learning	O
initial	Task
character	Task
and	Task
word	Task
-	Task
based	Task
context	Task
sensitive	Task
encodings	Task
synchronously	O
with	O
meta	O
-	O
BiLSTM	Method
.	O
In	O
terms	O
of	O
model	Task
selection	Task
,	O
after	O
each	O
epoch	O
,	O
the	O
algorithm	O
evaluates	O
the	O
tagging	O
accuracy	Metric
of	O
the	O
development	O
set	O
and	O
keeps	O
the	O
parameters	O
of	O
the	O
best	O
model	O
.	O
Accuracy	Metric
is	O
measured	O
using	O
the	O
meta	O
-	O
BiLSTM	Method
tagging	O
layer	O
,	O
which	O
requires	O
a	O
forward	O
pass	O
through	O
all	O
three	O
models	O
.	O
Though	O
we	O
use	O
all	O
three	O
losses	O
to	O
update	O
the	O
models	O
,	O
only	O
the	O
meta	O
-	O
BiLSTM	Method
layer	O
is	O
used	O
for	O
model	Task
selection	Task
and	O
test	Task
-	Task
time	Task
prediction	Task
.	O
While	O
each	O
of	O
the	O
three	O
models	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
are	O
trained	O
with	O
their	O
own	O
loss	O
functions	O
,	O
it	O
should	O
be	O
emphasized	O
that	O
training	O
is	O
synchronous	O
in	O
the	O
sense	O
that	O
the	O
meta	O
-	O
BiLSTM	Method
model	O
is	O
trained	O
in	O
tandem	O
with	O
the	O
two	O
encoding	Method
models	Method
,	O
and	O
not	O
after	O
those	O
models	O
have	O
converged	O
.	O
Since	O
accuracy	Metric
from	O
the	O
meta	O
-	O
BiLSTM	Method
model	O
on	O
the	O
development	O
set	O
determines	O
the	O
best	O
parameters	O
,	O
training	O
is	O
not	O
completely	O
independent	O
.	O
We	O
found	O
this	O
to	O
improve	O
accuracy	Metric
overall	O
.	O
Crucially	O
,	O
when	O
we	O
allowed	O
the	O
meta	O
-	O
BiLSTM	Method
to	O
back	O
-	O
propagate	O
through	O
the	O
whole	O
network	O
,	O
per	O
-	O
formance	O
degraded	O
regardless	O
of	O
whether	O
one	O
or	O
multiple	O
loss	O
functions	O
were	O
used	O
.	O
Each	O
language	O
could	O
in	O
theory	O
use	O
separate	O
hyperparameters	O
,	O
optimized	O
for	O
highest	O
accuracy	Metric
.	O
However	O
,	O
for	O
our	O
main	O
experiments	O
we	O
use	O
identical	O
settings	O
for	O
each	O
language	O
which	O
worked	O
well	O
for	O
large	O
corpora	O
and	O
simplified	O
things	O
.	O
We	O
provide	O
an	O
overview	O
of	O
the	O
selected	O
hyperparameters	O
in	O
§	O
4.1	O
.	O
We	O
explored	O
more	O
settings	O
for	O
selected	O
individual	O
languages	O
with	O
a	O
grid	Method
search	Method
and	O
ablation	O
experiments	O
and	O
present	O
the	O
results	O
in	O
§	O
5	O
.	O
section	O
:	O
Experiments	O
and	O
Results	O
In	O
this	O
section	O
,	O
we	O
present	O
the	O
experimental	O
setup	O
and	O
the	O
selected	O
hyperparameter	O
for	O
the	O
main	O
experiments	O
where	O
we	O
use	O
the	O
CoNLL	Material
Shared	Material
Task	Material
2017	Material
treebanks	Material
and	O
compare	O
with	O
the	O
best	O
systems	O
of	O
the	O
shared	O
task	O
.	O
section	O
:	O
Experimental	O
Setup	O
For	O
our	O
main	O
results	O
,	O
we	O
selected	O
one	O
network	Method
configuration	Method
and	O
set	O
of	O
the	O
hyperparameters	O
.	O
These	O
settings	O
are	O
not	O
optimal	O
for	O
all	O
languages	O
.	O
However	O
,	O
since	O
hyperparameter	Method
exploration	Method
is	O
computationally	O
demanding	O
due	O
to	O
the	O
number	O
of	O
languages	O
we	O
optimized	O
these	O
hyperparameters	O
on	O
initial	O
development	O
data	O
experiments	O
over	O
a	O
few	O
languages	O
.	O
Table	O
1	O
:	O
Selected	O
hyperparameters	O
and	O
initialization	O
of	O
parameters	O
for	O
our	O
models	O
.	O
Chr	Method
,	O
Wrd	O
,	O
and	O
Mt	O
are	O
used	O
to	O
indicate	O
the	O
character	O
,	O
word	O
,	O
and	O
meta	Method
models	Method
respectively	O
.	O
The	O
Gaussian	Method
distribution	Method
is	O
used	O
with	O
a	O
mean	O
of	O
0	O
and	O
variance	O
of	O
1	O
to	O
generate	O
the	O
random	O
values	O
.	O
single	Method
dropout	Method
mask	Method
and	O
we	O
use	O
dropout	O
on	O
the	O
input	O
and	O
the	O
states	O
of	O
the	O
LSTM	Method
.	O
As	O
is	O
standard	O
,	O
model	Method
selection	Method
was	O
done	O
measuring	O
development	Metric
accuracy	Metric
/	O
F1	Metric
score	Metric
after	O
each	O
epoch	O
and	O
taking	O
the	O
model	O
with	O
maximum	O
value	O
on	O
the	O
development	O
set	O
.	O
section	O
:	O
Data	O
Sets	O
For	O
the	O
experiments	O
,	O
we	O
use	O
the	O
data	O
sets	O
as	O
provided	O
by	O
the	O
CoNLL	Material
Shared	Material
Task	Material
2017	Material
[	O
reference	O
]	O
.	O
For	O
training	O
,	O
we	O
use	O
the	O
training	O
sets	O
which	O
were	O
denoted	O
as	O
big	O
treebanks	O
2	O
.	O
We	O
followed	O
the	O
same	O
methodology	O
used	O
in	O
the	O
CoNLL	Task
Shared	Task
Task	Task
.	O
We	O
use	O
the	O
training	O
treebank	O
for	O
training	O
only	O
and	O
the	O
development	O
sets	O
for	O
hyperparameter	Method
tuning	Method
and	O
early	Task
stopping	Task
.	O
To	O
keep	O
our	O
results	O
comparable	O
with	O
the	O
Shared	Task
Task	Task
,	O
we	O
use	O
the	O
provided	O
precomputed	O
word	O
embeddings	O
.	O
We	O
excluded	O
Gothic	O
from	O
our	O
experiments	O
as	O
the	O
available	O
downloadable	O
content	O
did	O
not	O
include	O
embeddings	O
for	O
this	O
language	O
.	O
As	O
input	O
to	O
our	O
system	O
-	O
for	O
both	O
part	Task
-	Task
ofspeech	Task
tagging	Task
and	O
morphological	Task
tagging	Task
-	O
we	O
use	O
the	O
output	O
of	O
the	O
UDPipe	Method
-	O
base	O
baseline	O
system	O
[	O
reference	O
]	O
)	O
which	O
provides	O
segmentation	Task
.	O
The	O
segmentation	O
differs	O
from	O
the	O
gold	Method
segmentation	Method
and	O
impacts	O
accuracy	Metric
negatively	O
for	O
a	O
number	O
of	O
languages	O
.	O
Most	O
of	O
the	O
top	O
performing	O
systems	O
for	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	O
used	O
as	O
input	O
UDPipe	Method
to	O
obtain	O
the	O
segmentation	O
for	O
the	O
input	O
data	O
.	O
For	O
morphology	Task
,	O
the	O
top	O
system	O
for	O
most	O
languages	O
(	O
IMS	Method
)	O
used	O
its	O
own	O
segmentation	O
[	O
reference	O
]	O
.	O
For	O
the	O
evaluation	O
,	O
we	O
used	O
the	O
official	O
evaluation	O
script	O
[	O
reference	O
]	O
.	O
section	O
:	O
Part	Task
-	Task
of	Task
-	Task
Speech	Task
Tagging	Task
Results	O
In	O
this	O
section	O
,	O
we	O
present	O
the	O
results	O
of	O
the	O
application	O
of	O
our	O
model	O
to	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	O
.	O
In	O
our	O
first	O
experiment	O
,	O
we	O
used	O
our	O
model	O
in	O
the	O
setting	O
of	O
the	O
CoNLL	Material
2017	Material
Shared	Material
Task	Material
to	O
annotate	O
words	O
with	O
XPOS	O
3	O
tags	O
[	O
reference	O
]	O
.	O
We	O
compare	O
our	O
results	O
against	O
the	O
top	O
systems	O
of	O
the	O
CoNLL	Material
2017	Material
Shared	Material
Task	Material
.	O
Table	O
2	O
contains	O
the	O
results	O
of	O
this	O
task	O
for	O
the	O
large	O
treebanks	O
.	O
Because	O
[	O
reference	O
]	O
won	O
the	O
challenge	O
for	O
the	O
majority	O
of	O
the	O
languages	O
,	O
we	O
first	O
compare	O
our	O
results	O
with	O
the	O
performance	O
of	O
their	O
system	O
.	O
Our	O
model	O
outperforms	O
[	O
reference	O
]	O
in	O
32	O
of	O
the	O
54	O
treebanks	O
with	O
13	O
ties	O
.	O
These	O
ties	O
correspond	O
mostly	O
to	O
languages	O
where	O
XPOS	Method
tagging	Method
anyhow	O
obtains	O
accuracies	Metric
above	O
99	O
%	O
.	O
Our	O
model	O
tends	O
to	O
produce	O
better	O
results	O
,	O
especially	O
for	O
morphologically	O
rich	O
languages	O
(	O
e.g.	O
Slavic	Metric
System	Metric
Accuracy	Metric
Søgaard	O
(	O
2011	O
)	O
97.50	O
[	O
reference	O
]	O
97.64	O
[	O
reference	O
]	O
.	O
97.44	O
[	O
reference	O
]	O
97.41	O
ours	O
97.96	O
Table	O
3	O
:	O
Results	O
on	O
WSJ	Material
test	O
set	O
.	O
languages	O
)	O
,	O
whereas	O
[	O
reference	O
]	O
showed	O
higher	O
performance	O
in	O
10	O
languages	O
in	O
particular	O
English	Material
,	O
Greek	Material
,	O
Brazilian	Material
Portuguese	Material
and	O
Estonian	Material
.	O
section	O
:	O
Part	Task
-	Task
of	Task
-	Task
Speech	Task
Tagging	Task
on	O
WSJ	Material
We	O
also	O
performed	O
experiments	O
on	O
the	O
Penn	Material
Treebank	Material
with	O
the	O
usual	O
split	O
in	O
train	O
,	O
development	O
and	O
test	O
set	O
.	O
Table	O
3	O
shows	O
the	O
results	O
of	O
our	O
model	O
in	O
comparison	O
to	O
the	O
results	O
reported	O
in	O
state	O
-	O
ofthe	O
-	O
art	O
literature	O
.	O
Our	O
model	O
significantly	O
outperforms	O
these	O
systems	O
,	O
with	O
an	O
absolute	O
difference	O
of	O
0.32	O
%	O
in	O
accuracy	Metric
,	O
which	O
corresponds	O
to	O
a	O
RRIE	Metric
of	O
12	O
%	O
.	O
section	O
:	O
Morphological	Task
Tagging	Task
Results	O
In	O
addition	O
to	O
the	O
XPOS	Task
tagging	Task
experiments	O
,	O
we	O
performed	O
experiments	O
with	O
morphological	Task
tagging	Task
.	O
This	O
annotation	O
was	O
part	O
of	O
the	O
CONLL	Task
2017	Task
Shared	Task
Task	Task
and	O
the	O
objective	O
was	O
to	O
predict	O
a	O
bundle	O
of	O
morphological	O
features	O
for	O
each	O
token	O
in	O
the	O
text	O
.	O
Our	O
model	O
treats	O
the	O
morphological	O
bundle	O
as	O
one	O
tag	O
making	O
the	O
problem	O
equivalent	O
to	O
a	O
sequential	Task
tagging	Task
problem	Task
.	O
Table	O
4	O
shows	O
the	O
results	O
.	O
Our	O
models	O
tend	O
to	O
produce	O
significantly	O
better	O
results	O
than	O
the	O
winners	O
of	O
the	O
CoNLL	Material
2017	Material
Shared	Material
Task	Material
(	O
i.e.	O
,	O
1.8	O
%	O
absolute	O
improvement	O
on	O
average	O
,	O
corresponding	O
to	O
a	O
RRIE	Metric
of	O
21.20	O
%	O
)	O
.	O
The	O
only	O
cases	O
for	O
which	O
this	O
is	O
not	O
true	O
are	O
again	O
languages	O
that	O
require	O
significant	O
segmentation	O
efforts	O
(	O
i.e.	O
,	O
Hebrew	Material
,	O
Chinese	Material
,	O
Vietnamese	Material
and	O
Japanese	Material
)	O
or	O
when	O
the	O
task	O
was	O
trivial	O
.	O
Given	O
the	O
fact	O
that	O
[	O
reference	O
]	O
obtained	O
the	O
best	O
results	O
in	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	O
by	O
a	O
significant	O
margin	O
in	O
the	O
CoNLL	Task
2017	Task
Shared	Task
Task	Task
,	O
it	O
would	O
be	O
expected	O
that	O
their	O
model	O
would	O
also	O
perform	O
significantly	O
well	O
in	O
morphological	Task
tagging	Task
since	O
the	O
tasks	O
are	O
very	O
similar	O
.	O
Since	O
they	O
did	O
not	O
participate	O
in	O
this	O
particular	O
challenge	O
,	O
we	O
decided	O
to	O
reimplement	O
their	O
system	O
to	O
serve	O
[	O
reference	O
]	O
,	O
the	O
column	O
ours	O
shows	O
our	O
system	O
with	O
a	O
sentence	Method
-	Method
based	Method
character	Method
model	Method
;	O
RRIE	Method
gives	O
the	O
relative	O
reduction	O
in	O
error	Metric
between	O
the	O
Reimpl	Method
.	O
DQM	Method
and	O
sentencebased	Method
character	Method
system	Method
.	O
Our	O
system	O
outperforms	O
the	O
CoNLL	Method
Winner	Method
by	O
48	O
out	O
of	O
54	O
treebanks	Method
and	O
the	O
reimplementation	Method
of	Method
DQM	Method
,	O
by	O
43	O
of	O
54	O
treebanks	Method
,	O
with	O
6	O
ties	O
.	O
as	O
a	O
strong	O
baseline	O
.	O
As	O
expected	O
,	O
our	O
reimplementation	O
of	O
[	O
reference	O
]	O
tends	O
to	O
significantly	O
outperform	O
the	O
winners	O
of	O
the	O
CONLL	Material
2017	Material
Shared	Material
Task	Material
.	O
However	O
,	O
in	O
general	O
,	O
our	O
models	O
still	O
obtain	O
better	O
results	O
,	O
outperforming	O
Dozat	O
et	O
al	O
.	O
on	O
43	O
of	O
the	O
54	O
treebanks	O
,	O
with	O
an	O
absolute	O
difference	O
of	O
0.42	O
%	O
on	O
average	O
.	O
section	O
:	O
Ablation	Task
Study	Task
The	O
model	O
proposed	O
in	O
this	O
paper	O
of	O
a	O
MetaBiLSTM	Method
with	O
a	O
sentence	Method
-	Method
based	Method
character	Method
model	Method
differs	O
from	O
prior	O
work	O
in	O
multiple	O
aspects	O
.	O
In	O
this	O
section	O
,	O
we	O
perform	O
ablations	Task
to	O
determine	O
the	O
relative	O
impact	O
of	O
each	O
modeling	O
decision	O
.	O
For	O
the	O
experimental	O
setup	O
of	O
the	O
ablation	Task
experiments	O
,	O
we	O
report	O
accuracy	Metric
scores	O
for	O
the	O
development	O
sets	O
.	O
We	O
split	O
off	O
5	O
%	O
of	O
the	O
sentences	O
from	O
each	O
training	O
corpus	O
and	O
we	O
use	O
this	O
part	O
for	O
early	Task
stopping	Task
.	O
Ablation	Task
experiments	O
are	O
either	O
performed	O
on	O
a	O
few	O
selected	O
treebanks	O
to	O
show	O
individual	O
language	O
results	O
or	O
averaged	O
across	O
all	O
treebanks	O
for	O
which	O
tagging	Task
is	O
non	O
-	O
trivial	O
.	O
section	O
:	O
Impact	O
of	O
the	O
Training	O
Schema	O
We	O
first	O
compare	O
jointly	O
training	O
the	O
three	O
model	Method
components	Method
(	O
Meta	Method
-	Method
BiLSTM	Method
,	O
character	Method
model	Method
,	O
word	Method
model	Method
)	O
to	O
training	O
each	O
separately	O
.	O
Table	O
5	O
shows	O
that	O
separately	O
optimized	Method
models	Method
are	O
significantly	O
more	O
accurate	Metric
on	O
average	O
than	O
jointly	Method
optimized	Method
models	Method
.	O
Separate	O
optimization	Method
leads	O
to	O
better	O
accuracy	Metric
for	O
34	O
out	O
of	O
40	O
treebanks	O
for	O
the	O
morphological	Task
features	Task
task	Task
and	O
for	O
30	O
out	O
of	O
39	O
treebanks	O
for	O
xpos	Task
tagging	Task
.	O
Separate	Method
optimization	Method
outperformed	O
joint	Method
optimization	Method
by	O
up	O
to	O
2.1	O
percent	O
absolute	O
,	O
while	O
joint	O
never	O
out	O
-	O
performed	O
separate	O
by	O
more	O
than	O
0.5	O
%	O
absolute	O
.	O
We	O
hypothesize	O
that	O
separately	O
training	O
the	O
models	O
forces	O
each	O
submodel	O
(	O
word	O
and	O
character	O
)	O
to	O
be	O
strong	O
enough	O
to	O
make	O
high	O
accuracy	Metric
predictions	O
and	O
in	O
some	O
sense	O
serves	O
as	O
a	O
regularizer	O
in	O
the	O
same	O
way	O
that	O
dropout	Method
does	O
for	O
individual	O
neurons	O
.	O
optimization	Task
.	O
section	O
:	O
Impact	O
of	O
the	O
Sentence	Method
-	Method
based	Method
Character	Method
Model	Method
We	O
compared	O
the	O
setup	O
with	O
sentence	O
-	O
based	O
character	O
context	O
(	O
Figure	O
1a	O
)	O
to	O
word	O
-	O
based	O
character	O
context	O
(	O
Figure	O
1b	O
)	O
.	O
We	O
selected	O
for	O
these	O
experiments	O
a	O
number	O
of	O
morphological	O
rich	O
languages	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
.	O
The	O
accuracy	Metric
of	O
the	O
word	Method
-	Method
based	Method
character	Method
model	Method
joint	O
with	O
a	O
word	Method
-	Method
based	Method
model	Method
were	O
significantly	O
lower	O
than	O
a	O
sentence	Method
-	Method
based	Method
character	Method
model	Method
.	O
We	O
conclude	O
Table	O
6	O
:	O
F1	Metric
score	Metric
for	O
selected	O
languages	O
on	O
sentence	O
vs.	O
word	Method
level	Method
character	Method
models	Method
for	O
the	O
prediction	Task
of	Task
morphology	Task
using	O
late	Task
integration	Task
.	O
also	O
from	O
these	O
results	O
and	O
comparing	O
with	O
results	O
of	O
the	O
reimplementation	O
of	O
DQM	Method
that	O
early	O
integration	O
of	O
the	O
word	Method
-	Method
based	Method
character	Method
model	Method
performs	O
much	O
better	O
as	O
late	Method
integration	Method
via	O
MetaBiLSTM	Method
for	O
a	O
word	Method
-	Method
based	Method
character	Method
model	Method
.	O
Impact	O
of	O
the	O
Meta	O
-	O
BiLSTM	Method
Model	O
Combination	O
The	O
proposed	O
model	O
trains	O
word	Method
and	Method
character	Method
models	Method
independently	O
while	O
training	O
a	O
joint	Method
model	Method
on	O
top	O
.	O
Here	O
we	O
investigate	O
the	O
part	Task
-	Task
ofspeech	Task
tagging	Task
performance	O
of	O
the	O
joint	Method
model	Method
compared	O
with	O
the	O
word	Method
and	Method
character	Method
models	Method
on	O
their	O
own	O
(	O
using	O
hyperparameters	O
from	O
in	O
4.1	O
)	O
.	O
Table	O
7	O
shows	O
,	O
for	O
selected	O
languages	O
,	O
the	O
results	O
averaged	O
over	O
10	O
runs	O
in	O
order	O
to	O
measure	O
standard	O
deviation	O
.	O
The	O
examples	O
show	O
that	O
the	O
combined	O
model	O
has	O
significantly	O
higher	O
accuracy	Metric
compared	O
with	O
either	O
the	O
character	Method
and	Method
word	Method
models	Method
individually	O
.	O
section	O
:	O
Concatenation	Method
Strategies	Method
for	O
the	O
ContextSensitive	Method
Character	Method
Encodings	Method
The	O
proposed	O
model	O
bases	O
a	O
token	Method
encoding	Method
on	O
both	O
the	O
forward	O
and	O
the	O
backward	Method
character	Method
representations	Method
of	O
both	O
the	O
first	O
and	O
last	O
character	O
in	O
the	O
token	O
(	O
see	O
Equation	O
1	O
)	O
.	O
Table	O
8	O
reports	O
,	O
for	O
a	O
few	O
morphological	O
rich	O
languages	O
,	O
the	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	O
performance	O
of	O
different	O
strategies	O
to	O
gather	O
the	O
characters	O
when	O
creating	O
initial	O
word	Method
encodings	Method
.	O
The	O
strategies	O
were	O
defined	O
in	O
§	O
3.1	O
.	O
The	O
Table	O
also	O
reimplementation	O
of	O
[	O
reference	O
]	O
.	O
We	O
removed	O
,	O
for	O
all	O
systems	O
,	O
the	O
word	Method
model	Method
in	O
order	O
to	O
assess	O
each	O
strategy	O
in	O
isolation	O
.	O
The	O
performance	O
is	O
quite	O
different	O
per	O
language	O
.	O
E.g.	O
,	O
for	O
Latin	O
,	O
the	O
outputs	O
of	O
the	O
forward	Method
and	Method
backward	Method
LSTMs	Method
of	O
the	O
last	O
character	O
scored	O
highest	O
.	O
Sensitivity	Metric
to	O
Hyperparameter	Method
Search	Method
We	O
picked	O
Vietnamese	Material
for	O
a	O
more	O
in	O
-	O
depth	O
analysis	O
since	O
it	O
did	O
not	O
perform	O
well	O
and	O
investigated	O
the	O
influence	O
of	O
the	O
sizes	O
of	O
LSTMs	Method
for	O
the	O
word	Method
and	Method
character	Method
model	Method
on	O
the	O
accuracy	Metric
of	O
development	O
set	O
.	O
With	O
larger	O
network	O
sizes	O
,	O
the	O
capacity	O
of	O
the	O
network	O
increases	O
,	O
however	O
,	O
on	O
the	O
other	O
hand	O
it	O
is	O
prune	O
to	O
overfitting	O
.	O
We	O
fixed	O
all	O
the	O
hyperparameters	O
except	O
those	O
for	O
the	O
network	O
size	O
of	O
the	O
character	Method
model	Method
and	O
the	O
word	Method
model	Method
,	O
and	O
ran	O
a	O
grid	Method
search	Method
over	O
dimension	O
sizes	O
from	O
200	O
to	O
500	O
in	O
steps	O
of	O
50	O
.	O
The	O
surface	O
plot	O
in	O
3	O
shows	O
that	O
the	O
grid	O
peaks	O
with	O
more	O
moderate	O
settings	O
around	O
350	O
LSTM	Method
cells	O
which	O
might	O
lead	O
to	O
a	O
higher	O
accuracy	Metric
.	O
For	O
all	O
of	O
the	O
network	O
sizes	O
in	O
the	O
grid	Task
search	Task
,	O
we	O
still	O
observed	O
during	O
training	O
that	O
the	O
accuracy	Metric
reach	O
a	O
high	O
value	O
and	O
degrades	O
with	O
more	O
iterations	O
for	O
the	O
character	Method
and	Method
word	Method
model	Method
.	O
This	O
suggests	O
that	O
future	O
variants	O
of	O
this	O
model	O
might	O
benefit	O
from	O
higher	O
regularization	Method
.	O
Discussion	O
Generally	O
,	O
the	O
fact	O
that	O
different	O
techniques	O
for	O
creating	O
word	Method
encodings	Method
from	O
character	Method
encodings	Method
and	O
different	O
network	O
sizes	O
can	O
lead	O
to	O
different	O
accuracies	Metric
per	O
language	O
suggests	O
that	O
it	O
should	O
be	O
possible	O
to	O
increase	O
the	O
accuracy	Metric
of	O
our	O
model	O
on	O
a	O
per	O
language	O
basis	O
via	O
a	O
grid	Method
search	Method
over	O
all	O
possibilities	O
.	O
In	O
fact	O
,	O
there	O
are	O
many	O
variations	O
on	O
the	O
models	O
we	O
presented	O
in	O
this	O
work	O
(	O
e.g.	O
,	O
how	O
the	O
character	Method
and	Method
word	Method
models	Method
are	O
combined	O
with	O
the	O
meta	O
-	O
BiLSTM	Method
)	O
.	O
Since	O
we	O
are	O
using	O
separate	O
losses	O
,	O
we	O
could	O
also	O
change	O
our	O
training	O
schema	O
.	O
For	O
example	O
,	O
one	O
could	O
use	O
methods	O
like	O
stack	Method
-	Method
propagation	Method
[	O
reference	O
]	O
where	O
we	O
burn	O
-	O
in	O
the	O
character	Method
and	Method
word	Method
models	Method
and	O
then	O
train	O
the	O
meta	O
-	O
BiLSTM	Method
backpropagating	O
throughout	O
the	O
entire	O
network	O
.	O
section	O
:	O
Conclusions	O
We	O
presented	O
an	O
approach	O
to	O
morphosyntactic	Task
tagging	Task
that	O
combines	O
context	Method
-	Method
sensitive	Method
initial	Method
character	Method
and	O
word	Method
encodings	Method
with	O
a	O
meta	O
-	O
BiLSTM	Method
layer	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
art	O
accuracies	Metric
for	O
a	O
wide	O
variety	O
of	O
languages	O
.	O
section	O
:	O
section	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
as	O
well	O
as	O
Terry	O
Koo	O
,	O
Slav	O
Petrov	O
,	O
Vera	O
Axelrod	O
,	O
Kellie	O
Websterk	O
,	O
Jan	O
Botha	O
,	O
Kuzman	O
Ganchev	O
,	O
Zhuoran	O
Yu	O
,	O
Yuan	O
Zhang	O
,	O
Eva	O
Schlinger	O
,	O
Ji	O
Ma	O
,	O
and	O
John	O
Alex	O
for	O
their	O
helpful	O
suggestions	O
,	O
comments	O
and	O
discussions	O
.	O
section	O
:	O
