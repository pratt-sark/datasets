In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
new	O
architecture	O
for	O
dynamically	B-Task
incorporating	I-Task
external	I-Task
background	I-Task
knowledge	E-Task
in	O
NLU	S-Task
models	O
.	O

Rather	O
than	O
relying	O
only	O
on	O
static	O
knowledge	O
implicitly	O
present	O
in	O
the	O
training	O
data	O
,	O
supplementary	O
knowledge	O
is	O
retrieved	O
from	O
external	O
knowledge	O
sources	O
(	O
in	O
this	O
paper	O
,	O
ConceptNet	S-Material
and	O
Wikipedia	S-Material
)	O
to	O
assist	O
with	O
understanding	O
text	O
inputs	O
.	O

Since	O
NLU	S-Task
systems	O
must	O
already	O
read	O
and	O
understand	O
text	O
inputs	O
,	O
we	O
assume	O
that	O
background	O
knowledge	O
will	O
likewise	O
be	O
provided	O
in	O
text	O
form	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

The	O
retrieved	O
supplementary	O
texts	O
are	O
read	O
together	O
with	O
the	O
task	O
inputs	O
by	O
an	O
initial	O
reading	B-Method
module	E-Method
whose	O
outputs	O
are	O
contextually	O
refined	O
word	O
embeddings	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

These	O
refined	O
embeddings	O
are	O
then	O
used	O
as	O
input	O
to	O
a	O
task	O
-	O
specific	O
NLU	S-Task
architecture	O
(	O
any	O
architecture	O
that	O
reads	O
text	O
as	O
a	O
sequence	O
of	O
word	O
embeddings	O
can	O
be	O
used	O
here	O
)	O
.	O

The	O
initial	O
reading	B-Method
module	E-Method
and	O
the	O
task	B-Method
module	E-Method
are	O
learnt	O
jointly	O
,	O
end	O
-	O
to	O
-	O
end	O
.	O

We	O
experiment	O
with	O
several	O
different	O
datasets	O
on	O
the	O
tasks	O
of	O
document	B-Task
question	I-Task
answering	E-Task
(	O
DQA	S-Task
)	O
and	O
recognizing	B-Task
textual	I-Task
entailment	E-Task
(	O
RTE	S-Task
)	O
evaluating	O
the	O
impact	O
of	O
our	O
proposed	O
solution	O
with	O
both	O
basic	O
task	B-Method
architectures	E-Method
and	O
a	O
sophisticated	O
task	B-Method
architecture	E-Method
for	O
RTE	S-Task
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

We	O
find	O
that	O
our	O
embedding	B-Method
refinement	I-Method
strategy	E-Method
is	O
effective	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

On	O
four	O
competitive	O
benchmarks	O
,	O
we	O
show	O
that	O
refinement	S-Task
helps	O
.	O

First	O
,	O
simply	O
refining	O
the	O
embeddings	O
just	O
using	O
the	O
context	O
(	O
and	O
no	O
additional	O
background	O
information	O
)	O
can	O
improve	O
performance	O
significantly	O
,	O
but	O
adding	O
background	O
knowledge	O
helps	O
further	O
.	O

Our	O
results	O
are	O
competitive	O
with	O
the	O
best	O
systems	O
,	O
achieving	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
the	O
recent	O
TriviaQA	B-Material
benchmarks	E-Material
.	O

Our	O
success	O
on	O
this	O
task	O
is	O
especially	O
noteworthy	O
because	O
the	O
task	O
-	O
specific	O
architecture	O
is	O
a	O
simple	O
reading	B-Method
architecture	E-Method
,	O
in	O
particular	O
a	O
single	B-Method
layer	I-Method
BiLSTM	E-Method
with	O
a	O
feed	B-Method
-	I-Method
forward	I-Method
neural	I-Method
network	E-Method
for	O
span	B-Task
prediction	E-Task
.	O

Finally	O
,	O
we	O
provide	O
an	O
analysis	O
demonstrating	O
that	O
our	O
systems	O
are	O
able	O
to	O
exploit	O
background	O
knowledge	O
in	O
a	O
semantically	O
appropriate	O
manner	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

It	O
includes	O
,	O
for	O
instance	O
,	O
an	O
experiment	O
showing	O
that	O
our	O
system	O
is	O
capable	O
of	O
making	O
appropriate	O
counterfactual	O
inferences	O
when	O
provided	O
with	O
“	O
alternative	O
facts	O
”	O
.	O

section	O
:	O
External	O
Knowledge	O
as	O
Supplementary	O
Text	O
Inputs	O
Knowledge	O
resources	O
make	O
information	O
that	O
could	O
potentially	O
be	O
useful	O
for	O
improving	O
NLU	S-Task
available	O
in	O
a	O
variety	O
different	O
formats	O
,	O
such	O
as	O
natural	O
language	O
text	O
,	O
(	O
subject	O
,	O
predicate	O
,	O
object	O
)-	O
triples	O
,	O
relational	O
databases	O
,	O
and	O
other	O
structured	O
formats	O
.	O

Rather	O
than	O
tailoring	O
our	O
solution	O
to	O
a	O
particular	O
structured	B-Method
representation	E-Method
,	O
we	O
assume	O
that	O
all	O
supplementary	O
information	O
either	O
already	O
exists	O
in	O
natural	O
language	O
statements	O
(	O
e.g.	O
,	O
encyclopedias	O
)	O
or	O
can	O
easily	O
be	O
recoded	O
as	O
natural	O
language	O
.	O

Furthermore	O
,	O
while	O
mapping	O
from	O
unstructured	B-Task
to	I-Task
structured	I-Task
representations	E-Task
is	O
hard	O
,	O
the	O
inverse	B-Task
problem	E-Task
is	O
easy	O
.	O

For	O
example	O
,	O
given	O
a	O
triple	O
we	O
can	O
construct	O
the	O
free	O
-	O
text	O
assertion	O
“	O
Abdication	S-Method
is	O
a	O
resignation	O
.	O

”	O
using	O
simple	O
rules	O
.	O

Finally	O
,	O
the	O
free	O
-	O
text	O
format	O
means	O
that	O
knowledge	O
that	O
exists	O
only	O
in	O
unstructured	O
text	O
form	O
such	O
as	O
encyclopedic	O
knowledge	O
(	O
e.g.	O
,	O
Wikipedia	S-Material
)	O
is	O
usable	O
by	O
our	O
system	O
.	O

An	O
important	O
question	O
that	O
remains	O
to	O
be	O
answered	O
is	O
:	O
given	O
some	O
text	O
that	O
is	O
to	O
be	O
understood	O
,	O
what	O
supplementary	O
knowledge	O
should	O
be	O
incorporated	O
?	O
The	O
retrieval	B-Task
and	I-Task
preparation	I-Task
of	I-Task
contextually	I-Task
relevant	I-Task
information	E-Task
from	O
knowledge	O
sources	O
is	O
a	O
complex	O
research	O
topic	O
by	O
itself	O
,	O
and	O
there	O
are	O
several	O
statistical	S-Method
Manning:2008	O
and	O
more	O
recently	O
neural	B-Method
approaches	E-Method
mitra2017neural	O
as	O
well	O
as	O
approaches	O
based	O
on	O
reinforcement	B-Method
learning	E-Method
nogueira2017	O
.	O

Rather	O
than	O
learning	O
both	O
how	O
to	O
incorporate	O
relevant	O
information	O
and	O
which	O
information	O
is	O
relevant	O
,	O
we	O
use	O
a	O
heuristic	B-Method
retrieval	I-Method
mechanism	E-Method
(	O
§	O
[	O
reference	O
]	O
)	O
and	O
focus	O
on	O
the	O
integration	B-Method
model	E-Method
.	O

In	O
the	O
next	O
section	O
,	O
we	O
turn	O
to	O
the	O
question	O
of	O
how	O
to	O
leverage	O
the	O
retrieved	O
supplementary	O
knowledge	O
(	O
encoded	O
as	O
text	O
)	O
in	O
a	O
NLU	S-Task
system	O
.	O

section	O
:	O
Refining	O
Word	B-Task
Embeddings	E-Task
by	O
Reading	O
Virtually	O
every	O
NLU	S-Task
task	O
—	O
from	O
document	B-Task
classification	E-Task
to	O
translation	S-Task
to	O
question	B-Task
answering	E-Task
—	O
should	O
in	O
theory	O
be	O
able	O
to	O
benefit	O
from	O
supplementary	O
knowledge	O
.	O

While	O
one	O
could	O
develop	O
custom	O
architectures	O
for	O
each	O
task	O
so	O
as	O
to	O
read	O
supplementary	O
inputs	O
,	O
we	O
would	O
like	O
ours	O
to	O
augment	O
any	O
existing	O
NLU	S-Task
task	O
architectures	O
with	O
the	O
ability	O
to	O
read	O
relevant	O
information	O
with	O
minimal	O
effort	O
.	O

To	O
realize	O
this	O
goal	O
,	O
we	O
adopt	O
the	O
strategy	O
of	O
refining	B-Task
word	I-Task
embeddings	E-Task
;	O
that	O
is	O
,	O
we	O
replace	O
static	B-Method
word	I-Method
embeddings	E-Method
with	O
embeddings	O
that	O
are	O
functions	O
of	O
the	O
task	O
inputs	O
and	O
any	O
supplementary	O
inputs	O
.	O

Word	B-Method
embeddings	E-Method
can	O
be	O
considered	O
a	O
simple	O
form	O
of	O
key	B-Method
-	I-Method
value	I-Method
memory	I-Method
stores	E-Method
that	O
,	O
in	O
our	O
case	O
,	O
not	O
only	O
contain	O
general	O
-	O
purpose	O
knowledge	O
(	O
as	O
in	O
typical	O
neural	O
NLU	S-Task
systems	O
)	O
but	O
also	O
contextual	O
information	O
(	O
including	O
background	O
knowledge	O
)	O
.	O

The	O
use	O
of	O
word	B-Method
-	I-Method
embeddings	E-Method
as	O
memory	O
has	O
the	O
advantage	O
that	O
it	O
is	O
transparent	O
to	O
the	O
task	O
-	O
architecture	O
which	O
kinds	O
of	O
embeddings	O
(	O
refined	O
or	O
unrefined	O
)	O
are	O
used	O
.	O

Our	O
incremental	B-Method
refinement	I-Method
process	E-Method
encodes	O
input	O
texts	O
followed	O
by	O
updates	O
on	O
the	O
word	B-Method
embedding	I-Method
matrix	E-Method
in	O
multiple	O
reading	O
steps	O
.	O

Words	O
are	O
first	O
represented	O
non	O
-	O
contextually	O
(	O
i.e.	O
,	O
standard	O
word	O
embeddings	O
)	O
,	O
which	O
can	O
be	O
conceived	O
of	O
as	O
the	O
columns	O
in	O
an	O
embedding	B-Method
matrix	E-Method
.	O

At	O
each	O
progressive	O
reading	O
step	O
,	O
a	O
new	O
embedding	O
matrix	O
is	O
constructed	O
by	O
refining	O
the	O
embeddings	O
from	O
the	O
previous	O
step	O
using	O
(	O
user	O
-	O
specified	O
)	O
contextual	O
information	O
for	O
reading	O
step	O
,	O
which	O
is	O
a	O
set	O
of	O
natural	O
language	O
sequences	O
(	O
i.e.	O
,	O
texts	O
)	O
.	O

An	O
illustration	O
of	O
our	O
incremental	B-Method
refinement	I-Method
strategy	E-Method
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
the	O
following	O
,	O
we	O
define	O
this	O
procedure	O
formally	O
.	O

We	O
denote	O
the	O
hidden	O
dimensionality	O
of	O
our	O
model	O
by	O
and	O
a	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
by	O
,	O
.	O

subsection	O
:	O
Unrefined	O
Word	O
Embeddings	O
(	O
)	O
The	O
first	O
representation	B-Method
level	E-Method
consists	O
of	O
non	B-Method
-	I-Method
contextual	I-Method
word	I-Method
representations	E-Method
,	O
that	O
is	O
,	O
word	B-Method
representations	E-Method
that	O
do	O
not	O
depend	O
on	O
any	O
input	O
;	O
these	O
can	O
be	O
conceived	O
of	O
as	O
an	O
embedding	O
matrix	O
whose	O
columns	O
are	O
indexed	O
by	O
words	O
in	O
.	O

The	O
non	B-Method
-	I-Method
contextual	I-Method
word	I-Method
representation	E-Method
for	O
a	O
single	O
word	O
is	O
computed	O
by	O
using	O
a	O
gated	B-Method
combination	E-Method
of	O
fixed	O
,	O
pre	O
-	O
trained	O
word	O
vectors	O
with	O
learned	O
character	B-Method
-	I-Method
based	I-Method
embeddings	E-Method
.	O

We	O
compute	O
using	O
a	O
single	B-Method
-	I-Method
layer	I-Method
convolutional	I-Method
neural	I-Method
network	E-Method
with	O
convolutional	B-Method
filters	I-Method
of	I-Method
width	E-Method
followed	O
by	O
a	O
-	B-Method
pooling	I-Method
operation	E-Method
over	O
time	O
Seo2017	O
,	O
Weissenborn2017	O
.	O

The	O
formal	O
definition	O
of	O
this	O
combination	O
is	O
given	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Refined	B-Task
Word	I-Task
Embeddings	E-Task
(	O
)	O
In	O
order	O
to	O
compute	O
contextually	B-Task
refined	I-Task
word	I-Task
embeddings	E-Task
given	O
prior	B-Method
representations	E-Method
we	O
assume	O
a	O
given	O
set	O
of	O
texts	O
that	O
are	O
to	O
be	O
read	O
at	O
refinement	O
iteration	O
.	O

Each	O
text	O
is	O
a	O
sequence	O
of	O
word	O
tokens	O
.	O

We	O
embed	O
all	O
tokens	O
of	O
every	O
using	O
the	O
embedding	O
matrix	O
from	O
the	O
previous	O
layer	O
,	O
.	O

To	O
each	O
word	O
,	O
we	O
concatenate	O
a	O
one	O
-	O
hot	O
vector	O
of	O
length	O
with	O
position	O
set	O
to	O
,	O
indicating	O
which	O
layer	O
is	O
currently	O
being	O
processed	O
.	O

Stacking	O
the	O
vectors	O
into	O
a	O
matrix	O
,	O
we	O
obtain	O
a	O
.	O

This	O
matrix	O
is	O
processed	O
by	O
a	O
bidirectional	B-Method
recurrent	I-Method
neural	I-Method
network	E-Method
,	O
a	O
hochreiter1997long	O
in	O
this	O
work	O
.	O

The	O
resulting	O
output	O
is	O
further	O
projected	O
to	O
by	O
a	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
with	O
activation	S-Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

To	O
finally	O
update	O
the	O
previous	O
embedding	O
of	O
word	O
,	O
we	O
initially	O
all	O
representations	O
of	O
occurrences	O
matching	O
the	O
lemma	O
of	O
in	O
every	O
resulting	O
in	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
we	O
combine	O
the	O
previous	O
representation	O
with	O
to	O
form	O
an	O
updated	B-Method
representation	E-Method
via	O
a	O
gated	B-Method
addition	E-Method
.	O

This	O
lets	O
the	O
model	O
determine	O
how	O
much	O
to	O
revise	O
the	O
previous	O
embedding	O
with	O
the	O
newly	O
read	O
information	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Note	O
that	O
we	O
soften	O
the	O
matching	O
condition	O
for	O
using	O
lemmatization	O
,	O
,	O
during	O
the	O
pooling	O
operation	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
because	O
contextual	O
information	O
about	O
certain	O
words	O
is	O
usually	O
independent	O
of	O
the	O
current	O
word	O
form	O
they	O
appear	O
in	O
.	O

As	O
a	O
consequence	O
,	O
this	O
minor	O
linguistic	B-Method
pre	I-Method
-	I-Method
processing	I-Method
step	E-Method
allows	O
for	O
additional	O
interaction	O
between	O
tokens	O
of	O
the	O
same	O
lemma	O
.	O

Pooling	O
over	O
lemma	O
-	O
occurrences	O
effectively	O
connects	O
different	O
text	O
passages	O
(	O
even	O
across	O
texts	O
)	O
that	O
are	O
otherwise	O
disconnected	O
,	O
mitigating	O
the	O
problems	O
arising	O
from	O
long	O
-	O
distance	O
dependencies	O
.	O

This	O
is	O
reminiscent	O
of	O
the	O
(	O
soft	O
)	O
attention	B-Method
mechanism	E-Method
used	O
in	O
reading	B-Method
comprehension	I-Method
models	E-Method
(	O
e.g.	O
,	O
Cheng2016	O
,	O
wang2017gated	O
)	O
.	O

However	O
,	O
our	O
setup	O
is	O
more	O
general	O
as	O
it	O
allows	O
for	O
the	O
connection	O
of	O
multiple	O
passages	O
(	O
via	O
pooling	S-Method
)	O
at	O
once	O
and	O
is	O
able	O
to	O
deal	O
with	O
multiple	O
inputs	O
which	O
is	O
necessary	O
to	O
make	O
use	O
of	O
additional	O
input	O
texts	O
such	O
as	O
relevant	O
background	O
knowledge	O
.	O

section	O
:	O
Experimental	O
Setup	O
We	O
run	O
experiments	O
on	O
four	O
benchmarks	O
for	O
two	O
standard	O
NLU	S-Task
tasks	O
:	O
recognizing	B-Task
textual	I-Task
entailment	E-Task
(	O
RTE	S-Task
)	O
and	O
document	B-Task
question	I-Task
answering	E-Task
(	O
DQA	S-Task
)	O
.	O

In	O
the	O
following	O
we	O
describe	O
our	O
experimental	O
setup	O
.	O

paragraph	O
:	O
Task	B-Method
-	I-Method
specific	I-Method
Models	E-Method
Since	O
we	O
wish	O
to	O
assess	O
the	O
value	O
of	O
the	O
proposed	O
embedding	B-Method
refinement	I-Method
strategy	E-Method
,	O
we	O
focus	O
on	O
relatively	O
simple	O
task	B-Method
architectures	E-Method
.	O

We	O
use	O
single	B-Method
-	I-Method
layer	I-Method
bidirectional	I-Method
LSTMs	E-Method
(	O
BiLSTMs	S-Method
)	O
as	O
encoders	S-Method
of	O
the	O
inputs	O
represented	O
by	O
the	O
refined	O
or	O
unrefined	O
embeddings	O
with	O
a	O
task	O
-	O
specific	O
,	O
feed	B-Method
-	I-Method
forward	I-Method
network	E-Method
for	O
the	O
final	O
prediction	S-Task
.	O

Such	O
models	O
are	O
general	B-Method
reading	I-Method
architectures	E-Method
.	O

To	O
demonstrate	O
that	O
our	O
reading	B-Method
module	E-Method
can	O
be	O
integrated	O
into	O
arbitrary	O
task	B-Method
architectures	E-Method
,	O
we	O
also	O
add	O
our	O
refinement	B-Method
module	E-Method
to	O
a	O
reimplementation	O
of	O
a	O
state	O
of	O
the	O
art	O
architecture	O
for	O
RTE	S-Task
called	O
ESIM	S-Method
Chen2017_ESIM	O
.	O

We	O
refer	O
the	O
interested	O
reader	O
to	O
the	O
ESIM	S-Method
paper	O
for	O
details	O
of	O
the	O
model	O
.	O

All	O
models	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
jointly	O
with	O
the	O
refinement	B-Method
module	E-Method
using	O
a	O
dimensionality	O
of	O
for	O
all	O
but	O
the	O
TriviaQA	B-Task
experiments	E-Task
for	O
which	O
we	O
had	O
to	O
reduce	O
to	O
due	O
to	O
memory	O
constraints	O
.	O

All	O
baselines	O
operate	O
on	O
the	O
unrefined	O
word	O
embeddings	O
described	O
in	O
§	O
[	O
reference	O
]	O
.	O

For	O
the	O
DQA	S-Task
baseline	O
system	O
we	O
add	O
the	O
lemma	O
-	O
in	O
-	O
question	O
feature	O
(	O
liq	S-Method
)	O
suggested	O
in	O
Weissenborn2017	O
.	O

Implementation	O
details	O
for	O
the	O
BiLSTM	B-Method
task	I-Method
architectures	E-Method
,	O
as	O
well	O
as	O
training	O
details	O
,	O
are	O
available	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Question	B-Task
Answering	E-Task
We	O
use	O
2	O
recent	O
DQA	S-Task
benchmark	O
training	O
and	O
evaluation	O
datasets	O
,	O
SQuAD	S-Material
Rajpurkar2016	O
and	O
TriviaQA	S-Material
JoshiTriviaQA2017	O
.	O

The	O
task	O
is	O
to	O
predict	O
an	O
answer	O
span	O
within	O
a	O
provided	O
document	O
given	O
a	O
question	O
.	O

Both	O
datasets	O
are	O
large	O
-	O
scale	O
,	O
containing	O
on	O
the	O
order	O
of	O
100k	O
examples	O
,	O
however	O
,	O
TriviaQA	S-Material
is	O
more	O
complex	O
in	O
that	O
the	O
supporting	O
documents	O
are	O
much	O
larger	O
than	O
those	O
for	O
SQuAD	S-Material
.	O

Because	O
TriviaQA	S-Material
is	O
collected	O
via	O
distant	O
supervision	O
the	O
test	O
set	O
is	O
divided	O
into	O
a	O
large	O
but	O
noisy	O
distant	O
supervision	O
part	O
and	O
a	O
much	O
smaller	O
(	O
on	O
the	O
order	O
of	O
hundreds	O
)	O
human	O
verified	O
part	O
.	O

We	O
report	O
results	O
on	O
both	O
.	O

See	O
Appendix	O
[	O
reference	O
]	O
for	O
implementation	O
details	O
of	O
the	O
DQA	S-Task
system	O
.	O

paragraph	O
:	O
Recognizing	B-Task
Textual	I-Task
Entailment	E-Task
We	O
test	O
on	O
both	O
the	O
SNLI	B-Material
dataset	E-Material
Bowman2015	O
,	O
a	O
collection	O
of	O
sentence	O
pairs	O
,	O
and	O
the	O
more	O
recent	O
MultiNLI	S-Material
dataset	O
(	O
sentence	B-Material
pairs	E-Material
)	O
williams2017broad	O
.	O

Given	O
two	O
sentences	O
,	O
a	O
premise	O
and	O
a	O
hypothesis	O
,	O
the	O
task	O
is	O
to	O
determine	O
whether	O
either	O
entails	O
,	O
contradicts	O
or	O
is	O
neutral	O
to	O
.	O

See	O
Appendix	O
[	O
reference	O
]	O
for	O
implementation	O
details	O
of	O
the	O
RTE	S-Task
system	O
.	O

paragraph	O
:	O
Supplementary	O
Knowledge	O
Sources	O
We	O
use	O
ConceptNethttp:	O
//	O
conceptnet.io	O
/	O
Speer2012	O
,	O
a	O
freely	O
-	O
available	O
,	O
multi	B-Method
-	I-Method
lingual	I-Method
semantic	I-Method
network	E-Method
that	O
originated	O
from	O
the	O
Open	B-Material
Mind	I-Material
Common	I-Material
Sense	I-Material
project	E-Material
and	O
incorporates	O
selected	O
knowledge	O
from	O
various	O
other	O
knowledge	O
sources	O
,	O
such	O
as	O
Wiktionary	S-Material
,	O
Open	B-Material
Multilingual	I-Material
WordNet	E-Material
,	O
OpenCyc	S-Material
and	O
DBpedia	S-Material
.	O

It	O
presents	O
information	O
in	O
the	O
form	O
of	O
relational	O
triples	O
.	O

Additionally	O
,	O
we	O
exploit	O
Wikipedia	B-Material
abstracts	E-Material
in	O
our	O
DQA	S-Task
experiments	O
as	O
described	O
below	O
.	O

paragraph	O
:	O
ConceptNet	B-Method
Integration	E-Method
Here	O
we	O
describe	O
the	O
heuristic	O
we	O
use	O
to	O
obtain	O
plausibly	O
relevant	O
supplementary	O
knowledge	O
for	O
understanding	O
a	O
text	O
pair	O
from	O
ConceptNet	S-Material
.	O

Our	O
hypothesis	O
is	O
that	O
relations	O
that	O
link	O
words	O
and	O
phrases	O
across	O
and	O
are	O
likely	O
to	O
be	O
most	O
valuable	O
.	O

Because	O
assertions	O
in	O
ConceptNet	O
come	O
in	O
form	O
of	O
(	O
subject	O
,	O
predicate	O
,	O
object	O
)-	O
triples	O
,	O
we	O
retrieve	O
all	O
assertions	O
for	O
which	O
appears	O
in	O
and	O
appears	O
in	O
,	O
or	O
vice	O
versa	O
.	O

Because	O
still	O
too	O
many	O
such	O
assertions	O
might	O
be	O
retrieved	O
for	O
an	O
instance	O
,	O
we	O
rank	O
all	O
retrievals	O
based	O
on	O
their	O
respective	O
subject	O
and	O
object	O
.	O

The	O
ranking	B-Metric
score	E-Metric
we	O
use	O
is	O
the	O
inverse	O
product	O
of	O
appearances	O
of	O
the	O
subject	O
and	O
the	O
object	O
in	O
the	O
KB	O
,	O
that	O
is	O
,	O
where	O
denotes	O
the	O
indicator	O
function	O
.	O

During	O
training	O
and	O
evaluation	S-Task
we	O
retain	O
the	O
top	O
-	O
assertions	O
,	O
using	O
for	O
DQA	S-Task
and	O
for	O
RTE	S-Task
.	O

Note	O
that	O
fewer	O
or	O
even	O
no	O
assertions	O
might	O
be	O
retrieved	O
for	O
a	O
particular	O
instance	O
during	O
training	O
and	O
testing	O
.	O

paragraph	O
:	O
Wikipedia	B-Task
Integration	E-Task
Here	O
we	O
describe	O
the	O
heuristic	O
we	O
use	O
to	O
obtain	O
plausibly	O
relevant	O
supplementary	O
knowledge	O
from	O
Wikipedia	S-Material
.	O

We	O
wish	O
to	O
use	O
Wikipedia	B-Material
abstracts	E-Material
as	O
an	O
additional	O
knowledge	O
source	O
to	O
gather	O
more	O
information	O
about	O
the	O
top	O
answer	O
predictions	O
of	O
our	O
DQA	S-Task
model	O
.	O

To	O
this	O
end	O
,	O
we	O
let	O
the	O
system	O
first	O
predict	O
the	O
top	O
-	O
16	O
answer	O
spans	O
without	O
any	O
information	O
from	O
Wikipedia	S-Material
.	O

For	O
each	O
answer	O
candidate	O
string	O
,	O
we	O
collect	O
abstracts	O
for	O
their	O
3	O
most	O
frequently	O
linked	O
Wikipedia	O
entries	O
.	O

Using	O
more	O
than	O
only	O
the	O
most	O
frequently	O
linked	O
Wikipedia	O
entry	O
for	O
a	O
given	O
answer	O
string	O
,	O
lets	O
us	O
mitigate	O
problems	O
arising	O
from	O
polysemous	O
entity	O
names	O
,	O
although	O
it	O
does	O
mean	O
the	O
refinement	B-Method
model	E-Method
needs	O
to	O
be	O
selective	O
in	O
extracting	O
relevant	O
information	O
.	O

The	O
refinement	B-Method
module	E-Method
additionally	O
reads	O
the	O
initial	O
50	O
tokens	O
of	O
each	O
retrieved	O
Wikipedia	O
abstract	O
and	O
computes	O
the	O
final	O
predictions	O
.	O

paragraph	O
:	O
Refinement	O
Order	O
When	O
employing	O
our	O
embedding	B-Method
-	I-Method
refinement	I-Method
strategy	E-Method
,	O
we	O
first	O
read	O
the	O
document	O
(	O
)	O
followed	O
by	O
the	O
question	O
(	O
)	O
in	O
case	O
of	O
DQA	S-Task
,	O
and	O
the	O
premise	O
(	O
)	O
followed	O
by	O
the	O
hypothesis	O
(	O
)	O
for	O
RTE	S-Task
,	O
that	O
is	O
,	O
and	O
.	O

Additional	O
knowledge	O
in	O
the	O
form	O
of	O
a	O
set	O
of	O
assertions	O
is	O
integrated	O
after	O
reading	O
the	O
task	O
-	O
specific	O
input	O
for	O
both	O
DQA	S-Task
and	O
RTE	S-Task
,	O
that	O
is	O
,	O
.	O

Finally	O
,	O
for	O
DQA	S-Task
we	O
additionally	O
add	O
Wikipedia	B-Material
abstracts	E-Material
as	O
background	O
knowledge	O
as	O
described	O
previously	O
,	O
that	O
is	O
,	O
.	O

In	O
preliminary	O
experiments	O
we	O
found	O
that	O
the	O
final	O
performance	O
is	O
not	O
significantly	O
sensitive	O
to	O
the	O
order	O
of	O
presentation	O
so	O
we	O
decided	O
to	O
fix	O
our	O
order	O
as	O
defined	O
above	O
.	O

section	O
:	O
Results	O
This	O
section	O
presents	O
results	O
.	O

We	O
provide	O
ablations	O
for	O
a	O
total	O
of	O
7	O
task	O
-	O
dataset	O
-	O
model	O
combinations	O
and	O
compare	O
our	O
final	O
results	O
to	O
other	O
works	O
on	O
the	O
most	O
recent	O
benchmark	O
datasets	O
for	O
each	O
task	O
(	O
TriviaQA	S-Material
and	O
MultiNLI	S-Material
)	O
,	O
demonstrating	O
that	O
our	O
results	O
are	O
competitive	O
,	O
and	O
in	O
some	O
cases	O
,	O
state	O
of	O
the	O
art	O
,	O
even	O
without	O
sophisticated	O
task	B-Method
architectures	E-Method
.	O

subsection	O
:	O
Question	B-Task
Answering	E-Task
Table	O
[	O
reference	O
]	O
presents	O
our	O
results	O
on	O
two	O
question	B-Task
answering	I-Task
benchmarks	E-Task
.	O

The	O
results	O
demonstrate	O
that	O
the	O
introduction	O
of	O
the	O
refinement	B-Method
module	E-Method
helps	O
consistently	O
,	O
and	O
further	O
improvements	O
come	O
from	O
using	O
common	O
sense	O
knowledge	O
from	O
ConceptNet	O
(	O
)	O
.	O

Wikipedia	S-Method
(	O
)	O
yields	O
further	O
,	O
significant	O
improvements	O
on	O
TriviaQA	S-Material
,	O
slightly	O
outperforming	O
the	O
current	O
state	O
of	O
the	O
art	O
model	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

This	O
is	O
especially	O
noteworthy	O
given	O
the	O
simplicity	O
of	O
our	O
QA	B-Method
architecture	E-Method
(	O
i.e.	O
,	O
a	O
single	B-Method
layer	I-Method
BiLSTM	E-Method
)	O
compared	O
to	O
the	O
previous	O
SotA	S-Method
attained	O
by	O
clark2017simple	O
.	O

The	O
development	O
results	O
on	O
SQuAD	S-Material
show	O
the	O
same	O
pattern	O
of	O
improvement	O
,	O
but	O
here	O
the	O
results	O
are	O
slightly	O
worse	O
than	O
the	O
model	O
of	O
clark2017simple	O
,	O
and	O
they	O
are	O
way	O
off	O
from	O
the	O
current	O
best	O
-	O
known	O
results	O
(	O
currently	O
at	O
87	O
%	O
F1	S-Metric
)	O
;	O
however	O
,	O
our	O
intention	O
with	O
these	O
experiments	O
is	O
to	O
show	O
of	O
the	O
value	O
that	O
external	O
knowledge	O
and	O
our	O
refinement	B-Method
process	E-Method
can	O
bring	O
,	O
not	O
to	O
compete	O
with	O
highly	O
tuned	O
task	B-Method
architectures	E-Method
on	O
a	O
single	O
dataset	O
.	O

paragraph	O
:	O
Controlling	O
for	O
computation	S-Task
.	O

One	O
potential	O
explanation	O
for	O
the	O
improvement	O
obtained	O
using	O
the	O
refinement	B-Method
module	E-Method
is	O
that	O
we	O
are	O
enabling	O
more	O
computation	O
over	O
the	O
information	O
present	O
in	O
the	O
inputs	O
,	O
that	O
is	O
,	O
we	O
are	O
effectively	O
using	O
a	O
deeper	B-Method
architecture	E-Method
.	O

To	O
test	O
whether	O
this	O
might	O
be	O
the	O
case	O
,	O
we	O
also	O
ran	O
an	O
experiment	O
with	O
a	O
2	B-Method
-	I-Method
layer	I-Method
BiLSTM	I-Method
(	I-Method
+	I-Method
liq	E-Method
)	O
.	O

This	O
setup	O
exhibits	O
similar	O
computational	B-Metric
complexity	E-Metric
and	O
number	O
of	O
parameters	O
to	O
BiLSTM	B-Method
+	I-Method
+	E-Method
.	O

We	O
found	O
that	O
the	O
second	O
layer	O
did	O
not	O
improve	O
performance	O
,	O
suggesting	O
that	O
pooling	O
over	O
word	O
/	O
lemma	O
occurrences	O
in	O
a	O
given	O
context	O
between	O
layers	O
,	O
is	O
a	O
powerful	O
,	O
yet	O
simple	O
technique	O
.	O

subsection	O
:	O
Recognizing	B-Task
Textual	I-Task
Entailment	E-Task
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
our	O
RTE	S-Task
experiments	O
.	O

In	O
general	O
,	O
the	O
introduction	O
of	O
our	O
refinement	B-Method
strategy	E-Method
almost	O
always	O
helps	O
,	O
both	O
with	O
and	O
without	O
external	O
knowledge	O
.	O

When	O
providing	O
additional	O
background	O
knowledge	O
from	O
ConceptNet	O
,	O
our	O
BiLSTM	B-Method
based	I-Method
models	E-Method
improve	O
substantially	O
,	O
while	O
the	O
ESIM	S-Method
-	O
based	O
models	O
improve	O
only	O
on	O
the	O
more	O
difficult	O
MultiNLI	S-Material
dataset	O
.	O

Compared	O
to	O
previously	O
published	O
state	O
of	O
the	O
art	O
systems	O
,	O
our	O
models	O
acquit	O
themselves	O
quite	O
well	O
on	O
the	O
MultiNLI	S-Material
benchmark	O
,	O
and	O
competitively	O
on	O
the	O
SNLI	B-Material
benchmark	E-Material
.	O

In	O
parallel	O
to	O
this	O
work	O
,	O
gong2017natural	O
developed	O
a	O
novel	O
task	B-Method
-	I-Method
specific	I-Method
architecture	E-Method
for	O
RTE	S-Task
that	O
achieves	O
slightly	O
better	O
performance	O
on	O
MultiNLI	S-Material
than	O
our	O
ESIM	S-Method
+	O
+	O
+	O
based	O
models	O
.	O

It	O
draws	O
attention	O
to	O
the	O
fact	O
that	O
when	O
using	O
our	O
knowledge	B-Method
-	I-Method
enhanced	I-Method
embedding	I-Method
module	E-Method
,	O
on	O
the	O
MultiNLI	S-Material
,	O
the	O
basic	O
BiLSTM	B-Method
task	I-Method
model	E-Method
outperforms	O
the	O
task	O
-	O
specific	O
ESIM	S-Method
model	O
,	O
which	O
is	O
architecturally	O
much	O
more	O
complex	O
and	O
designed	O
specifically	O
for	O
the	O
RTE	S-Task
task	O
.	O

We	O
do	O
find	O
that	O
there	O
is	O
little	O
impact	O
of	O
using	O
external	O
knowledge	O
on	O
the	O
RTE	S-Task
task	O
with	O
ESIM	S-Method
,	O
although	O
the	O
refinement	B-Method
strategy	E-Method
helps	O
using	O
just	O
+	O
.	O

A	O
more	O
detailed	O
set	O
of	O
experiments	O
reported	O
in	O
Appendix	O
[	O
reference	O
]	O
shows	O
that	O
by	O
impoverishing	O
the	O
amount	O
of	O
training	O
data	O
and	O
information	O
present	O
in	O
the	O
GloVe	O
embeddings	O
,	O
the	O
positive	O
impact	O
of	O
supplemental	O
information	O
becomes	O
much	O
more	O
pronounced	O
.	O

These	O
results	O
suggest	O
that	O
ESIM	S-Method
is	O
able	O
to	O
learn	O
important	O
background	O
information	O
from	O
the	O
large	O
-	O
scale	O
datasets	O
and	O
from	O
pretrained	B-Method
embeddings	E-Method
,	O
but	O
this	O
can	O
be	O
supplemented	O
when	O
necessary	O
.	O

Nevertheless	O
,	O
both	O
ESIM	S-Method
and	O
our	O
BiLSTM	B-Method
models	E-Method
when	O
trained	O
with	O
knowledge	O
from	O
ConceptNet	S-Method
are	O
sensitive	O
to	O
the	O
semantics	O
of	O
the	O
provided	O
assertions	O
as	O
demonstrated	O
in	O
our	O
analysis	O
in	O
§	O
[	O
reference	O
]	O
.	O

We	O
argue	O
that	O
this	O
is	O
a	O
desirable	O
side	O
effect	O
because	O
it	O
makes	O
the	O
predictions	O
of	O
our	O
model	O
more	O
interpretable	O
than	O
those	O
not	O
trained	O
with	O
knowledge	O
.	O

Furthermore	O
,	O
increasing	O
the	O
coverage	O
of	O
assertions	O
in	O
ConceptNet	O
would	O
most	O
likely	O
yield	O
improved	O
performance	O
even	O
without	O
retraining	O
our	O
models	O
.	O

Finally	O
,	O
we	O
remark	O
that	O
despite	O
careful	O
tuning	O
,	O
our	O
re	B-Method
-	I-Method
implementation	E-Method
of	O
ESIM	S-Method
fails	O
to	O
match	O
the	O
88	O
%	O
reported	O
in	O
Chen2017_ESIM	O
by	O
0.8	O
%	O
;	O
however	O
,	O
with	O
MultiNLI	S-Material
,	O
we	O
find	O
that	O
our	O
implementation	O
of	O
ESIM	S-Method
performs	O
considerably	O
better	O
(	O
by	O
approximately	O
5	O
%	O
)	O
.	O

The	O
instability	O
of	O
the	O
results	O
suggests	O
,	O
as	O
well	O
as	O
the	O
failure	O
of	O
a	O
custom	O
RTE	S-Task
-	O
architecture	O
to	O
consistently	O
perform	O
well	O
suggests	O
that	O
current	O
SotA	O
RTE	S-Task
models	O
may	O
be	O
overfit	O
to	O
the	O
SNLI	B-Material
dataset	E-Material
.	O

subsection	O
:	O
Qualitative	B-Method
Analysis	E-Method
Although	O
our	O
empirical	O
results	O
show	O
our	O
knowledge	B-Method
-	I-Method
incorporation	I-Method
approach	E-Method
improves	O
performance	O
,	O
in	O
this	O
section	O
we	O
attempt	O
to	O
assess	O
whether	O
we	O
are	O
learning	O
to	O
use	O
the	O
provided	O
knowledge	O
in	O
a	O
semantically	O
appropriate	O
way	O
.	O

paragraph	O
:	O
RTE	S-Task
To	O
test	O
our	O
models	O
sensitivity	O
towards	O
the	O
semantics	O
of	O
the	O
assertions	O
for	O
recognizing	B-Task
textual	I-Task
entailment	E-Task
,	O
we	O
run	O
an	O
experiment	O
in	O
which	O
we	O
swap	O
the	O
synonym	O
with	O
the	O
antonym	O
predicate	O
in	O
the	O
provided	O
assertions	O
during	O
test	O
time	O
.	O

We	O
hypothesize	O
that	O
in	O
many	O
cases	O
these	O
two	O
predicates	O
are	O
very	O
important	O
for	O
predicting	O
either	O
contradiction	S-Task
or	O
entailment	S-Task
.	O

Indeed	O
,	O
there	O
is	O
a	O
strong	O
performance	O
drop	O
of	O
about	O
10	O
%	O
on	O
MultiNLI	S-Material
examples	O
for	O
both	O
the	O
BiLSTM	S-Method
and	O
the	O
ESIM	S-Method
model	O
for	O
which	O
either	O
a	O
synonym	O
or	O
an	O
antonym	O
-	O
assertion	O
is	O
present	O
.	O

This	O
very	O
large	O
drop	O
clearly	O
shows	O
that	O
our	O
models	O
are	O
sensitive	O
to	O
the	O
semantics	O
of	O
the	O
provided	O
knowledge	O
.	O

Examples	O
of	O
prediction	O
changes	O
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

They	O
demonstrate	O
that	O
the	O
system	O
has	O
learned	O
to	O
trust	O
the	O
presented	O
assertions	O
to	O
the	O
point	O
that	O
it	O
will	O
make	O
appropriate	O
counterfactual	O
inferences	O
—	O
that	O
is	O
,	O
the	O
change	O
in	O
knowledge	O
has	O
caused	O
the	O
change	O
in	O
prediction	S-Task
.	O

For	O
the	O
interested	O
reader	O
we	O
provide	O
additional	O
RTE	S-Task
analysis	O
results	O
in	O
Appendix	O
[	O
reference	O
]	O
paragraph	O
:	O
DQA	S-Task
The	O
following	O
is	O
an	O
example	O
question	O
from	O
the	O
TriviaQA	S-Material
dataset	O
:	O
[	O
roundcorner=2pt	O
]	O
Prince	O
Philip	O
[	O
…	O
]	O
was	O
born	O
on	O
which	O
island	O
?	O
Answer	O
candidates	O
with	O
corresponding	O
abstracts	O
:	O
Denmark	O
is	O
a	O
Scandinavian	O
country	O
with	O
territory	O
in	O
Europe	O
and	O
North	O
America	O
[	O
…	O
]	O
Corfu	O
is	O
a	O
Greek	O
island	O
in	O
the	O
Ionian	O
Sea	O
[	O
…	O
]	O
Greece	O
,	O
officially	O
the	O
Hellenic	O
Republic	O
,	O
[	O
…	O
]	O
is	O
a	O
transcontinental	O
country	O
[	O
…	O
]	O
Vanuatu	O
is	O
a	O
Pacific	O
island	O
nation	O
located	O
in	O
the	O
South	O
Pacific	O
Ocean	O
[	O
…	O
]	O
Answer	O
candidates	O
(	O
i.e.	O
,	O
Denmark	O
,	O
Corfu	O
,	O
Greece	O
,	O
Vanuata	O
)	O
were	O
obtained	O
from	O
the	O
top	O
predicted	O
answer	O
spans	O
computed	O
by	O
our	O
model	O
excluding	O
Wikipedia	S-Material
(	O
i.e.	O
,	O
BiLSTM	B-Method
+	E-Method
+	O
+	O
)	O
.	O

Their	O
corresponding	O
abstracts	O
were	O
retrieved	O
from	O
Wikipedia	O
and	O
then	O
given	O
to	O
our	O
model	O
in	O
a	O
second	O
pass	O
(	O
i.e.	O
,	O
BiLSTM	S-Method
+	O
+	O
+	O
+	O
)	O
.	O

In	O
this	O
example	O
,	O
the	O
final	O
best	O
prediction	O
of	O
the	O
model	O
changes	O
from	O
Denmark	O
to	O
Corfu	O
after	O
integrating	O
the	O
abstracts	O
(	O
here	O
,	O
the	O
abstract	O
clearly	O
states	O
that	O
Corfu	O
is	O
an	O
island	O
)	O
.	O

We	O
studied	O
a	O
total	O
of	O
25	O
similar	O
answer	O
changes	O
,	O
14	O
of	O
which	O
went	O
from	O
incorrect	O
to	O
correct	O
,	O
and	O
11	O
of	O
which	O
went	O
from	O
correct	O
to	O
incorrect	O
.	O

In	O
11	O
of	O
the	O
14	O
corrections	O
,	O
obvious	O
information	O
is	O
present	O
in	O
the	O
Wikipedia	B-Material
abstracts	E-Material
that	O
reinforced	O
the	O
correct	O
answer	O
.	O

Where	O
the	O
system	O
was	O
confused	O
by	O
the	O
answers	O
(	O
i.e.	O
,	O
when	O
the	O
abstracts	O
switched	O
the	O
production	O
from	O
correct	O
to	O
incorrect	O
)	O
,	O
no	O
obvious	O
information	O
was	O
present	O
in	O
8	O
of	O
the	O
11	O
cases	O
,	O
suggesting	O
that	O
the	O
model	O
had	O
difficulty	O
coping	O
with	O
unrelated	O
background	O
information	O
.	O

In	O
3	O
of	O
the	O
11	O
,	O
plausibly	O
relevant	O
information	O
was	O
present	O
in	O
the	O
abstract	O
of	O
the	O
correct	O
answer	O
,	O
yet	O
the	O
model	O
still	O
made	O
the	O
incorrect	O
answer	O
change	O
.	O

The	O
existence	O
of	O
counterfactual	O
inferences	O
in	O
RTE	S-Task
and	O
the	O
tendency	O
to	O
use	O
reinforcing	O
information	O
about	O
candidate	O
answers	O
in	O
DQA	S-Task
suggest	O
that	O
our	O
knowledge	B-Method
incorporating	I-Method
strategy	E-Method
is	O
exploiting	O
heterogeneous	O
knowledge	O
sources	O
in	O
semantically	O
sensible	O
ways	O
.	O

section	O
:	O
Related	O
Work	O
The	O
role	O
of	O
background	O
knowledge	O
in	O
natural	B-Task
language	I-Task
understanding	E-Task
has	O
long	O
been	O
remarked	O
on	O
,	O
especially	O
in	O
the	O
context	O
of	O
classical	O
models	O
of	O
AI	S-Method
schank:1977	O
,	O
minsky2000commonsense	O
;	O
however	O
,	O
it	O
has	O
only	O
recently	O
begun	O
to	O
play	O
a	O
role	O
in	O
neural	B-Method
network	I-Method
models	E-Method
of	O
NLU	S-Task
ahn2016neural	O
,	O
Xu2016	O
,	O
long2017world	O
,	O
Dhingra2017	O
.	O

Previous	O
efforts	O
have	O
focused	O
on	O
specific	O
tasks	O
or	O
certain	O
kinds	O
of	O
knowledge	O
,	O
whereas	O
we	O
take	O
a	O
step	O
towards	O
a	O
more	O
general	O
-	O
purpose	O
solution	O
for	O
the	O
integration	O
of	O
heterogeneous	B-Task
knowledge	E-Task
for	O
NLU	S-Task
systems	O
by	O
providing	O
a	O
simple	O
,	O
general	B-Method
-	I-Method
purpose	I-Method
reading	I-Method
architecture	E-Method
that	O
can	O
read	O
background	O
knowledge	O
encoded	O
in	O
simple	O
natural	O
language	O
statements	O
,	O
e.g.	O
,	O
“	O
abdication	S-Method
is	O
a	O
type	O
of	O
resignation	O
”	O
.	O

In	O
the	O
area	O
of	O
visual	B-Task
question	I-Task
answering	E-Task
Wu2016	O
utilize	O
external	O
knowledge	O
in	O
form	O
of	O
DBpedia	B-Material
comments	E-Material
(	O
short	O
abstracts	O
/	O
definitions	O
)	O
to	O
improve	O
the	O
answering	S-Task
ability	O
of	O
a	O
model	O
.	O

marino2016more	O
explicitly	O
incorporate	O
knowledge	O
graphs	O
into	O
an	O
image	B-Method
classification	I-Method
model	E-Method
.	O

Xu2016	O
created	O
a	O
recall	B-Method
mechanism	E-Method
into	O
a	O
standard	O
LSTM	B-Method
cell	E-Method
that	O
retrieves	O
pieces	O
of	O
external	O
knowledge	O
encoded	O
by	O
a	O
single	O
representation	O
for	O
a	O
conversation	B-Method
model	E-Method
.	O

Concurrently	O
,	O
Dhingra2017	O
exploit	O
linguistic	O
knowledge	O
using	O
MAGE	B-Method
-	I-Method
GRUs	E-Method
,	O
an	O
adapation	O
of	O
GRUs	S-Method
to	O
handle	O
graphs	O
,	O
however	O
,	O
external	O
knowledge	O
has	O
to	O
be	O
present	O
in	O
form	O
of	O
triples	O
.	O

ahn2016neural	O
exploit	O
knowledge	O
base	O
facts	O
about	O
mentioned	O
entities	O
for	O
neural	B-Method
language	I-Method
models	E-Method
.	O

bahdanau2017learning	O
and	O
long2017world	O
create	O
word	O
embeddings	O
on	O
-	O
the	O
-	O
fly	O
by	O
reading	O
word	O
definitions	O
prior	O
to	O
processing	O
the	O
task	O
at	O
hand	O
.	O

pilehvar2017towards	O
incorporate	O
information	O
about	O
word	O
senses	O
into	O
their	O
representations	O
before	O
solving	O
the	O
downstream	O
NLU	S-Task
task	O
,	O
which	O
is	O
similar	O
.	O

We	O
go	O
one	O
step	O
further	O
by	O
seamlessly	O
integrating	O
all	O
kinds	O
of	O
fine	O
-	O
grained	O
assertions	O
about	O
concepts	O
that	O
might	O
be	O
relevant	O
for	O
the	O
task	O
at	O
hand	O
.	O

Another	O
important	O
aspect	O
of	O
our	O
approach	O
is	O
the	O
notion	O
of	O
dynamically	B-Task
updating	I-Task
word	I-Task
-	I-Task
representations	E-Task
with	O
contextual	O
information	O
.	O

Tracking	B-Task
and	I-Task
updating	I-Task
concepts	E-Task
,	O
entities	O
or	O
sentences	O
with	O
dynamic	O
memories	O
is	O
a	O
very	O
active	O
research	O
direction	O
kumar2016ask	O
,	O
henaff2017tracking	O
,	O
ji2017dynamic	O
,	O
kobayashi2017neural	O
.	O

However	O
,	O
those	O
works	O
typically	O
focus	O
on	O
particular	O
tasks	O
whereas	O
our	O
approach	O
is	O
task	O
-	O
agnostic	O
and	O
most	O
importantly	O
allows	O
for	O
the	O
easy	O
integration	O
of	O
external	O
background	O
knowledge	O
.	O

Important	O
progress	O
has	O
also	O
been	O
made	O
in	O
creating	O
pre	O
-	O
trained	O
,	O
contextualized	B-Method
token	I-Method
representations	E-Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
novel	O
reading	B-Method
architecture	E-Method
that	O
allows	O
for	O
the	O
dynamic	B-Task
integration	I-Task
of	I-Task
background	I-Task
knowledge	E-Task
into	O
neural	O
NLU	S-Task
models	O
.	O

Our	O
solution	O
,	O
which	O
is	O
based	O
on	O
the	O
incremental	B-Method
refinement	I-Method
of	I-Method
word	I-Method
representations	E-Method
by	O
reading	O
supplementary	O
inputs	O
,	O
is	O
flexible	O
and	O
can	O
be	O
used	O
with	O
virtually	O
any	O
existing	O
NLU	S-Task
architecture	O
that	O
rely	O
on	O
word	O
embeddings	O
as	O
input	O
.	O

Our	O
results	O
show	O
that	O
embedding	B-Task
refinement	E-Task
using	O
both	O
the	O
system	O
’s	O
text	O
inputs	O
,	O
as	O
well	O
as	O
supplementary	O
text	O
from	O
external	O
background	O
knowledge	O
can	O
yield	O
large	O
improvements	O
.	O

In	O
particular	O
,	O
we	O
have	O
shown	O
that	O
relatively	O
simple	O
task	B-Method
architectures	E-Method
(	O
e.g.	O
,	O
based	O
on	O
simple	O
BiLSTM	B-Method
readers	E-Method
)	O
can	O
become	O
competitive	O
with	O
state	O
of	O
the	O
art	O
,	O
task	B-Method
-	I-Method
specific	I-Method
architectures	E-Method
when	O
augmented	O
with	O
our	O
reading	B-Method
architecture	E-Method
.	O

Our	O
analysis	O
demonstrates	O
that	O
our	O
model	O
learns	O
to	O
exploit	O
provided	O
background	O
knowledge	O
in	O
a	O
semantically	O
appropriate	O
way	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Implementation	O
Details	O
All	O
our	O
models	O
were	O
trained	O
with	O
3	O
different	O
random	O
seeds	O
and	O
the	O
top	O
performance	O
is	O
reported	O
.	O

An	O
overview	O
of	O
hyper	O
-	O
parameters	O
used	O
in	O
our	O
experiments	O
can	O
be	O
found	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
the	O
following	O
we	O
explain	O
the	O
detailed	O
implementation	O
of	O
our	O
two	O
task	O
-	O
specific	O
,	O
baseline	B-Method
models	E-Method
.	O

We	O
assume	O
to	O
have	O
computed	O
the	O
contextually	B-Method
(	I-Method
un	I-Method
-)	I-Method
refined	I-Method
word	I-Method
representations	E-Method
depending	O
on	O
the	O
setup	O
and	O
embedded	O
our	O
input	O
sequences	O
and	O
to	O
and	O
,	O
respectively	O
.	O

The	O
word	B-Method
representation	I-Method
update	I-Method
gate	E-Method
in	O
Eq	O
.	O

[	O
reference	O
]	O
is	O
initialized	O
with	O
a	O
bias	O
of	O
to	O
refine	O
representations	O
only	O
slightly	O
in	O
the	O
beginning	O
of	O
training	O
.	O

In	O
the	O
following	O
as	O
before	O
,	O
we	O
denote	O
the	O
hidden	O
dimensionality	O
of	O
our	O
model	O
by	O
and	O
a	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
by	O
,	O
.	O

subsection	O
:	O
Question	B-Task
Answering	E-Task
paragraph	O
:	O
Encoding	S-Task
In	O
the	O
DQA	B-Task
task	E-Task
refers	O
to	O
the	O
question	O
and	O
to	O
the	O
supporting	O
text	O
.	O

For	O
our	O
baseline	O
(	O
i.e.	O
,	O
BiLSTM	B-Method
+	I-Method
liq	E-Method
)	O
we	O
additionally	O
concatenate	O
a	O
binary	O
feature	O
to	O
and	O
indicating	O
whether	O
the	O
corresponding	O
token	O
lemma	O
appeared	O
in	O
the	O
question	O
.	O

However	O
,	O
it	O
is	O
omitted	O
in	O
the	O
following	O
for	O
the	O
sake	O
of	O
brevity	O
.	O

At	O
first	O
we	O
process	O
both	O
sequences	O
by	O
identical	O
s	O
in	O
parallel	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
followed	O
by	O
a	O
linear	B-Method
projection	E-Method
and	O
a	O
non	B-Method
-	I-Method
linearity	E-Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

is	O
initialized	O
by	O
where	O
is	O
the	O
identity	O
matrix	O
.	O

paragraph	O
:	O
Prediction	S-Task
Our	O
prediction–	B-Method
or	I-Method
answer	I-Method
layer	E-Method
is	O
similar	O
to	O
Weissenborn2017	O
.	O

We	O
first	O
compute	O
a	O
weighted	B-Method
,	I-Method
-	I-Method
dimensional	I-Method
representation	E-Method
of	O
the	O
processed	O
question	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
probability	O
distributions	O
/	O
for	O
the	O
start	O
/	O
end	O
location	O
of	O
the	O
answer	O
is	O
computed	O
by	O
a	O
2	B-Method
-	I-Method
layer	I-Method
MLP	E-Method
with	O
a	O
ReLU	B-Method
activated	E-Method
,	O
hidden	B-Method
layer	E-Method
as	O
follows	O
:	O
The	O
model	O
is	O
trained	O
to	O
maximize	O
the	O
log	O
-	O
likelihood	O
of	O
the	O
correct	O
answer	O
spans	O
by	O
computing	O
the	O
sum	O
of	O
the	O
correct	O
span	O
probabilities	O
for	O
span	O
under	O
our	O
model	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

During	O
evaluation	O
we	O
extract	O
the	O
span	O
with	O
the	O
best	O
score	O
and	O
maximum	O
token	O
length	O
for	O
SQuAD	S-Material
and	O
for	O
TriviaQA	S-Material
.	O

paragraph	O
:	O
TriviaQA	S-Material
Properly	O
training	O
a	O
QA	B-Method
system	E-Method
on	O
TriviaQA	S-Material
is	O
much	O
more	O
challenging	O
than	O
SQuAD	S-Material
because	O
of	O
the	O
large	O
document	O
sizes	O
and	O
the	O
use	O
of	O
multiple	O
paragraphs	O
.	O

Therefore	O
,	O
we	O
adopt	O
the	O
approach	O
of	O
clark2017simple	O
who	O
were	O
the	O
first	O
to	O
properly	O
train	O
neural	B-Method
QA	I-Method
models	E-Method
on	O
TriviaQA	S-Material
.	O

It	O
relies	O
on	O
splitting	O
documents	O
and	O
merging	O
paragraphs	O
up	O
to	O
a	O
certain	O
maximum	O
token	O
length	O
(	O
per	O
paragraph	O
in	O
our	O
experiments	O
)	O
,	O
and	O
only	O
retaining	O
the	O
top	O
-	O
paragraphs	O
(	O
in	O
our	O
case	O
)	O
for	O
prediction	S-Task
.	O

Paragraphs	O
are	O
ranked	O
using	O
the	O
tf	B-Metric
-	I-Metric
idf	I-Metric
cosine	I-Metric
similarity	E-Metric
between	O
question	O
and	O
paragraph	O
.	O

To	O
speed	O
up	O
training	S-Task
only	O
paragraphs	O
out	O
of	O
the	O
top	O
/	O
for	O
the	O
/	O
datasets	O
were	O
sampled	O
.	O

The	O
only	O
architectural	O
difference	O
for	O
this	O
multi	B-Task
-	I-Task
paragraph	I-Task
setup	E-Task
is	O
that	O
we	O
encode	O
multiple	O
for	O
each	O
question	O
and	O
the	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
is	O
taken	O
over	O
all	O
tokens	O
of	O
all	O
paragraphs	O
instead	O
of	O
only	O
a	O
single	O
paragraph	O
.	O

For	O
further	O
details	O
,	O
we	O
refer	O
the	O
interested	O
reader	O
to	O
clark2017simple	O
who	O
explain	O
this	O
process	O
in	O
more	O
detail	O
.	O

subsection	O
:	O
Recognizing	B-Task
Textual	I-Task
Entailment	E-Task
paragraph	O
:	O
Encoding	S-Task
Analogous	O
to	O
DQA	S-Task
we	O
encode	O
our	O
input	O
sequences	O
by	O
BiLSTMs	S-Method
,	O
however	O
,	O
for	O
RTE	S-Task
we	O
use	O
conditional	B-Method
encoding	E-Method
Rocktschel2015	O
instead	O
.	O

Therefore	O
,	O
we	O
initially	O
process	O
the	O
embedded	O
hypothesis	O
by	O
a	O
BiLSTM	S-Method
and	O
use	O
the	O
respective	O
end	O
states	O
of	O
the	O
forward	B-Method
and	I-Method
backward	I-Method
LSTM	E-Method
as	O
initial	O
states	O
for	O
the	O
forward	B-Method
and	I-Method
backward	I-Method
LSTM	E-Method
that	O
processes	O
the	O
embedded	O
premise	O
.	O

paragraph	O
:	O
Prediction	S-Task
We	O
concatenate	O
the	O
outputs	O
of	O
the	O
forward	B-Method
and	I-Method
backward	I-Method
LSTMs	E-Method
processing	O
the	O
premise	O
,	O
i.e.	O
,	O
and	O
run	O
each	O
of	O
the	O
resulting	O
outputs	O
through	O
a	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
with	O
activation	O
(	O
)	O
followed	O
by	O
a	O
-	O
pooling	O
operation	O
over	O
time	O
resulting	O
in	O
a	O
hidden	O
state	O
.	O

Finally	O
,	O
is	O
used	O
to	O
predict	O
the	O
RTE	S-Task
label	O
as	O
follows	O
:	O
The	O
probability	O
of	O
choosing	O
category	O
{	O
entailment	O
,	O
contradiction	O
,	O
neutral	O
}	O
is	O
defined	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

Finally	O
,	O
the	O
model	O
is	O
trained	O
to	O
maximize	O
the	O
log	O
-	O
likelihood	O
of	O
the	O
correct	O
category	O
label	O
given	O
probability	O
distribution	O
.	O

appendix	O
:	O
Reducing	O
Training	O
Data	O
&	O
Dimensionality	S-Metric
of	O
Pre	O
-	O
trained	O
Word	B-Method
Embeddings	E-Method
We	O
find	O
that	O
there	O
is	O
only	O
little	O
impact	O
when	O
using	O
external	O
knowledge	O
on	O
the	O
RTE	S-Task
task	O
when	O
using	O
a	O
more	O
sophisticated	O
task	B-Method
model	E-Method
such	O
as	O
ESIM	S-Method
.	O

We	O
hypothesize	O
that	O
the	O
attention	B-Method
mechanisms	E-Method
within	O
ESIM	S-Method
together	O
with	O
powerful	O
,	O
pre	O
-	O
trained	O
word	B-Method
representations	E-Method
allow	O
for	O
the	O
recovery	O
of	O
some	O
important	O
lexical	O
relations	O
when	O
trained	O
on	O
a	O
large	O
dataset	O
.	O

It	O
follows	O
that	O
by	O
reducing	O
the	O
number	O
of	O
training	O
data	O
and	O
impoverishing	O
pre	O
-	O
trained	O
word	B-Method
representations	E-Method
the	O
impact	O
of	O
using	O
external	O
knowledge	O
should	O
become	O
larger	O
.	O

To	O
test	O
this	O
hypothesis	O
,	O
we	O
gradually	O
impoverish	O
pre	O
-	O
trained	O
word	O
embeddings	O
by	O
reducing	O
their	O
dimensionality	O
with	O
PCA	S-Method
while	O
reducing	O
the	O
number	O
of	O
training	O
instances	O
at	O
the	O
same	O
time	O
.	O

Our	O
joint	O
data	O
and	O
dimensionality	B-Task
reduction	E-Task
results	O
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

They	O
show	O
that	O
there	O
is	O
indeed	O
a	O
slightly	O
larger	O
benefit	O
when	O
employing	O
background	O
knowledge	O
from	O
ConcepNet	B-Method
(	I-Method
)	E-Method
in	O
the	O
more	O
impoverished	O
settings	O
with	O
largest	O
improvements	O
when	O
using	O
around	O
10k	O
examples	O
and	O
reduced	O
dimensionality	O
to	O
10	O
.	O

However	O
,	O
we	O
observe	O
that	O
the	O
biggest	O
overall	O
impact	O
over	O
the	O
baseline	O
ESIM	S-Method
model	O
stems	O
from	O
our	O
contextual	B-Method
refinement	I-Method
strategy	E-Method
(	O
i.e.	O
,	O
reading	O
only	O
the	O
premise	O
and	O
hypothesis	O
)	O
which	O
is	O
especially	O
pronounced	O
for	O
the	O
1k	O
and	O
3k	O
experiments	O
.	O

This	O
highlights	O
once	O
more	O
the	O
usefulness	O
of	O
our	O
refinement	B-Method
strategy	E-Method
even	O
without	O
the	O
use	O
of	O
additional	O
knowledge	O
.	O

appendix	O
:	O
Further	O
Analysis	O
of	O
Knowledge	B-Task
Utilization	E-Task
in	O
RTE	S-Task
0.32	O
0.3	O
0.3	O
paragraph	O
:	O
Is	O
additional	O
knowledge	O
used	O
?	O
To	O
verify	O
whether	O
and	O
how	O
our	O
models	O
make	O
use	O
of	O
additional	O
knowledge	O
,	O
we	O
conducted	O
several	O
experiments	O
.	O

First	O
,	O
we	O
evaluated	O
models	O
trained	O
with	O
knowledge	O
on	O
our	O
tasks	O
while	O
not	O
providing	O
any	O
knowledge	O
at	O
test	O
time	O
.	O

This	O
ablation	O
drops	O
performance	O
by	O
3.7–3.9	O
%	O
accuracy	S-Metric
on	O
MultiNLI	S-Material
,	O
and	O
by	O
4	O
%	O
F1	S-Metric
on	O
SQuAD	S-Material
.	O

This	O
indicates	O
the	O
model	O
is	O
refining	O
the	O
representations	O
using	O
the	O
provided	O
assertions	O
in	O
a	O
useful	O
way	O
.	O

paragraph	O
:	O
What	O
knowledge	O
is	O
used	O
?	O
After	O
establishing	O
that	O
our	O
models	O
are	O
somehow	O
sensitive	O
to	O
semantics	O
we	O
wanted	O
to	O
find	O
out	O
which	O
type	O
of	O
knowledge	O
is	O
important	O
for	O
which	O
task	O
.	O

For	O
this	O
analysis	O
we	O
exclude	O
assertions	O
including	O
the	O
most	O
prominent	O
predicates	O
in	O
our	O
knowledge	O
base	O
individually	O
when	O
evaluating	O
our	O
models	O
.	O

The	O
results	O
are	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

They	O
demonstrate	O
that	O
the	O
biggest	O
performance	O
drop	O
in	O
total	O
(	O
blue	O
bars	O
)	O
stems	O
from	O
related	O
to	O
assertions	O
.	O

This	O
very	O
prominent	O
predicate	O
appears	O
much	O
more	O
frequently	O
than	O
other	O
assertions	O
and	O
helps	O
connecting	O
related	O
parts	O
of	O
the	O
2	O
input	O
sequences	O
with	O
each	O
other	O
.	O

We	O
believe	O
that	O
related	O
to	O
assertions	O
offer	O
benefits	O
mainly	O
from	O
a	O
modeling	O
perspective	O
by	O
strongly	O
connecting	O
the	O
input	O
sequences	O
with	O
each	O
other	O
and	O
thus	O
bridging	O
long	O
-	O
range	O
dependencies	O
similar	O
to	O
attention	O
.	O

Looking	O
at	O
the	O
relative	O
drops	O
obtained	O
by	O
normalizing	O
the	O
performance	O
differences	O
on	O
the	O
actually	O
affected	O
examples	O
(	O
green	O
)	O
we	O
find	O
that	O
our	O
models	O
depend	O
highly	O
on	O
the	O
presence	O
of	O
antonym	O
and	O
synonym	O
assertions	O
for	O
all	O
tasks	O
as	O
well	O
as	O
partially	O
on	O
is	O
a	O
and	O
derived	O
from	O
assertions	O
.	O

This	O
is	O
an	O
interesting	O
finding	O
which	O
shows	O
that	O
the	O
sensitivity	O
of	O
our	O
models	O
is	O
selective	O
wrt	O
.	O

the	O
type	O
of	O
knowledge	O
and	O
task	O
.	O

The	O
fact	O
that	O
the	O
largest	O
relative	O
impact	O
stems	O
from	O
antonyms	O
is	O
very	O
interesting	O
because	O
it	O
is	O
known	O
that	O
such	O
information	O
is	O
hard	O
to	O
capture	O
with	O
distributional	O
semantics	O
contained	O
in	O
pre	O
-	O
trained	O
word	B-Method
embeddings	E-Method
.	O

document	O
:	O
Neural	B-Method
Architectures	E-Method
for	O
Named	B-Task
Entity	I-Task
Recognition	E-Task
State	O
-	O
of	O
-	O
the	O
-	O
art	O
named	B-Method
entity	I-Method
recognition	I-Method
systems	E-Method
rely	O
heavily	O
on	O
hand	O
-	O
crafted	O
features	O
and	O
domain	O
-	O
specific	O
knowledge	O
in	O
order	O
to	O
learn	O
effectively	O
from	O
the	O
small	O
,	O
supervised	O
training	O
corpora	O
that	O
are	O
available	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
two	O
new	O
neural	B-Method
architectures	E-Method
—	O
one	O
based	O
on	O
bidirectional	O
LSTMs	S-Method
and	O
conditional	B-Method
random	I-Method
fields	E-Method
,	O
and	O
the	O
other	O
that	O
constructs	O
and	O
labels	O
segments	O
using	O
a	O
transition	B-Method
-	I-Method
based	I-Method
approach	E-Method
inspired	O
by	O
shift	B-Method
-	I-Method
reduce	I-Method
parsers	E-Method
.	O

Our	O
models	O
rely	O
on	O
two	O
sources	O
of	O
information	O
about	O
words	O
:	O
character	B-Method
-	I-Method
based	I-Method
word	I-Method
representations	E-Method
learned	O
from	O
the	O
supervised	O
corpus	O
and	O
unsupervised	B-Method
word	I-Method
representations	E-Method
learned	O
from	O
unannotated	O
corpora	O
.	O

Our	O
models	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
NER	S-Task
in	O
four	O
languages	O
without	O
resorting	O
to	O
any	O
language	O
-	O
specific	O
knowledge	O
or	O
resources	O
such	O
as	O
gazetteers	O
.	O

section	O
:	O
Introduction	O
Named	B-Task
entity	I-Task
recognition	E-Task
(	O
NER	S-Task
)	O
is	O
a	O
challenging	O
learning	B-Task
problem	E-Task
.	O

One	O
the	O
one	O
hand	O
,	O
in	O
most	O
languages	O
and	O
domains	O
,	O
there	O
is	O
only	O
a	O
very	O
small	O
amount	O
of	O
supervised	O
training	O
data	O
available	O
.	O

On	O
the	O
other	O
,	O
there	O
are	O
few	O
constraints	O
on	O
the	O
kinds	O
of	O
words	O
that	O
can	O
be	O
names	O
,	O
so	O
generalizing	O
from	O
this	O
small	O
sample	O
of	O
data	O
is	O
difficult	O
.	O

As	O
a	O
result	O
,	O
carefully	O
constructed	O
orthographic	O
features	O
and	O
language	O
-	O
specific	O
knowledge	O
resources	O
,	O
such	O
as	O
gazetteers	S-Method
,	O
are	O
widely	O
used	O
for	O
solving	O
this	O
task	O
.	O

Unfortunately	O
,	O
language	O
-	O
specific	O
resources	O
and	O
features	O
are	O
costly	O
to	O
develop	O
in	O
new	O
languages	O
and	O
new	O
domains	O
,	O
making	O
NER	S-Task
a	O
challenge	O
to	O
adapt	O
.	O

Unsupervised	B-Method
learning	E-Method
from	O
unannotated	O
corpora	O
offers	O
an	O
alternative	O
strategy	O
for	O
obtaining	O
better	O
generalization	S-Task
from	O
small	O
amounts	O
of	O
supervision	O
.	O

However	O
,	O
even	O
systems	O
that	O
have	O
relied	O
extensively	O
on	O
unsupervised	O
features	O
have	O
used	O
these	O
to	O
augment	O
,	O
rather	O
than	O
replace	O
,	O
hand	O
-	O
engineered	O
features	O
(	O
e.g.	O
,	O
knowledge	O
about	O
capitalization	O
patterns	O
and	O
character	O
classes	O
in	O
a	O
particular	O
language	O
)	O
and	O
specialized	O
knowledge	O
resources	O
(	O
e.g.	O
,	O
gazetteers	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
neural	B-Method
architectures	E-Method
for	O
NER	S-Task
that	O
use	O
no	O
language	O
-	O
specific	O
resources	O
or	O
features	O
beyond	O
a	O
small	O
amount	O
of	O
supervised	O
training	O
data	O
and	O
unlabeled	O
corpora	O
.	O

Our	O
models	O
are	O
designed	O
to	O
capture	O
two	O
intuitions	O
.	O

First	O
,	O
since	O
names	O
often	O
consist	O
of	O
multiple	O
tokens	O
,	O
reasoning	O
jointly	O
over	O
tagging	O
decisions	O
for	O
each	O
token	O
is	O
important	O
.	O

We	O
compare	O
two	O
models	O
here	O
,	O
(	O
i	O
)	O
a	O
bidirectional	O
LSTM	S-Method
with	O
a	O
sequential	B-Method
conditional	I-Method
random	I-Method
layer	E-Method
above	O
it	O
(	O
LSTM	B-Method
-	I-Method
CRF	E-Method
;	O
§	O
[	O
reference	O
]	O
)	O
,	O
and	O
(	O
ii	O
)	O
a	O
new	O
model	O
that	O
constructs	O
and	O
labels	O
chunks	O
of	O
input	O
sentences	O
using	O
an	O
algorithm	O
inspired	O
by	O
transition	B-Method
-	I-Method
based	I-Method
parsing	E-Method
with	O
states	O
represented	O
by	O
stack	O
LSTMs	S-Method
(	O
S	B-Method
-	I-Method
LSTM	E-Method
;	O
§	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
token	O
-	O
level	O
evidence	O
for	O
“	O
being	O
a	O
name	O
”	O
includes	O
both	O
orthographic	O
evidence	O
(	O
what	O
does	O
the	O
word	O
being	O
tagged	O
as	O
a	O
name	O
look	O
like	O
?	O
)	O
and	O
distributional	O
evidence	O
(	O
where	O
does	O
the	O
word	O
being	O
tagged	O
tend	O
to	O
occur	O
in	O
a	O
corpus	O
?	O
)	O
.	O

To	O
capture	O
orthographic	O
sensitivity	O
,	O
we	O
use	O
character	B-Method
-	I-Method
based	I-Method
word	I-Method
representation	I-Method
model	E-Method
to	O
capture	O
distributional	O
sensitivity	O
,	O
we	O
combine	O
these	O
representations	O
with	O
distributional	B-Method
representations	E-Method
.	O

Our	O
word	B-Method
representations	E-Method
combine	O
both	O
of	O
these	O
,	O
and	O
dropout	B-Method
training	E-Method
is	O
used	O
to	O
encourage	O
the	O
model	O
to	O
learn	O
to	O
trust	O
both	O
sources	O
of	O
evidence	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

Experiments	O
in	O
English	S-Material
,	O
Dutch	O
,	O
German	O
,	O
and	O
Spanish	O
show	O
that	O
we	O
are	O
able	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NER	B-Metric
performance	E-Metric
with	O
the	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
in	O
Dutch	O
,	O
German	O
,	O
and	O
Spanish	O
,	O
and	O
very	O
near	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
English	S-Material
without	O
any	O
hand	O
-	O
engineered	O
features	O
or	O
gazetteers	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

The	O
transition	B-Method
-	I-Method
based	I-Method
algorithm	E-Method
likewise	O
surpasses	O
the	O
best	O
previously	O
published	O
results	O
in	O
several	O
languages	O
,	O
although	O
it	O
performs	O
less	O
well	O
than	O
the	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
.	O

section	O
:	O
LSTM	S-Method
-	O
CRF	S-Method
Model	O
We	O
provide	O
a	O
brief	O
description	O
of	O
LSTMs	S-Method
and	O
CRFs	S-Method
,	O
and	O
present	O
a	O
hybrid	B-Method
tagging	I-Method
architecture	E-Method
.	O

This	O
architecture	O
is	O
similar	O
to	O
the	O
ones	O
presented	O
by	O
collobert2011natural	O
and	O
huang:2015	O
.	O

subsection	O
:	O
LSTM	B-Method
Recurrent	I-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
are	O
a	O
family	O
of	O
neural	B-Method
networks	E-Method
that	O
operate	O
on	O
sequential	O
data	O
.	O

They	O
take	O
as	O
input	O
a	O
sequence	O
of	O
vectors	O
and	O
return	O
another	O
sequence	O
that	O
represents	O
some	O
information	O
about	O
the	O
sequence	O
at	O
every	O
step	O
in	O
the	O
input	O
.	O

Although	O
RNNs	S-Method
can	O
,	O
in	O
theory	O
,	O
learn	O
long	O
dependencies	O
,	O
in	O
practice	O
they	O
fail	O
to	O
do	O
so	O
and	O
tend	O
to	O
be	O
biased	O
towards	O
their	O
most	O
recent	O
inputs	O
in	O
the	O
sequence	O
.	O

Long	B-Method
Short	I-Method
-	I-Method
term	I-Method
Memory	I-Method
Networks	E-Method
(	O
LSTMs	S-Method
)	O
have	O
been	O
designed	O
to	O
combat	O
this	O
issue	O
by	O
incorporating	O
a	O
memory	O
-	O
cell	O
and	O
have	O
been	O
shown	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
.	O

They	O
do	O
so	O
using	O
several	O
gates	O
that	O
control	O
the	O
proportion	O
of	O
the	O
input	O
to	O
give	O
to	O
the	O
memory	O
cell	O
,	O
and	O
the	O
proportion	O
from	O
the	O
previous	O
state	O
to	O
forget	O
.	O

We	O
use	O
the	O
following	O
implementation	O
:	O
where	O
is	O
the	O
element	B-Method
-	I-Method
wise	I-Method
sigmoid	I-Method
function	E-Method
,	O
and	O
is	O
the	O
element	O
-	O
wise	O
product	O
.	O

For	O
a	O
given	O
sentence	O
containing	O
words	O
,	O
each	O
represented	O
as	O
a	O
-	O
dimensional	O
vector	O
,	O
an	O
LSTM	S-Method
computes	O
a	O
representation	O
of	O
the	O
left	O
context	O
of	O
the	O
sentence	O
at	O
every	O
word	O
.	O

Naturally	O
,	O
generating	O
a	O
representation	O
of	O
the	O
right	O
context	O
as	O
well	O
should	O
add	O
useful	O
information	O
.	O

This	O
can	O
be	O
achieved	O
using	O
a	O
second	O
LSTM	S-Method
that	O
reads	O
the	O
same	O
sequence	O
in	O
reverse	O
.	O

We	O
will	O
refer	O
to	O
the	O
former	O
as	O
the	O
forward	O
LSTM	S-Method
and	O
the	O
latter	O
as	O
the	O
backward	O
LSTM	S-Method
.	O

These	O
are	O
two	O
distinct	O
networks	O
with	O
different	O
parameters	O
.	O

This	O
forward	O
and	O
backward	O
LSTM	S-Method
pair	O
is	O
referred	O
to	O
as	O
a	O
bidirectional	O
LSTM	S-Method
.	O

The	O
representation	O
of	O
a	O
word	O
using	O
this	O
model	O
is	O
obtained	O
by	O
concatenating	O
its	O
left	B-Method
and	I-Method
right	I-Method
context	I-Method
representations	E-Method
,	O
.	O

These	O
representations	O
effectively	O
include	O
a	O
representation	O
of	O
a	O
word	O
in	O
context	O
,	O
which	O
is	O
useful	O
for	O
numerous	O
tagging	B-Task
applications	E-Task
.	O

subsection	O
:	O
CRF	S-Method
Tagging	O
Models	O
A	O
very	O
simple	O
—	O
but	O
surprisingly	O
effective	O
—	O
tagging	B-Method
model	E-Method
is	O
to	O
use	O
the	O
’s	O
as	O
features	O
to	O
make	O
independent	O
tagging	O
decisions	O
for	O
each	O
output	O
.	O

Despite	O
this	O
model	O
’s	O
success	O
in	O
simple	O
problems	O
like	O
POS	B-Task
tagging	E-Task
,	O
its	O
independent	O
classification	B-Task
decisions	E-Task
are	O
limiting	O
when	O
there	O
are	O
strong	O
dependencies	O
across	O
output	O
labels	O
.	O

NER	S-Task
is	O
one	O
such	O
task	O
,	O
since	O
the	O
“	O
grammar	S-Method
”	O
that	O
characterizes	O
interpretable	O
sequences	O
of	O
tags	O
imposes	O
several	O
hard	O
constraints	O
(	O
e.g.	O
,	O
I	O
-	O
PER	O
can	O
not	O
follow	O
B	O
-	O
LOC	O
;	O
see	O
§	O
[	O
reference	O
]	O
for	O
details	O
)	O
that	O
would	O
be	O
impossible	O
to	O
model	O
with	O
independence	O
assumptions	O
.	O

Therefore	O
,	O
instead	O
of	O
modeling	O
tagging	B-Task
decisions	E-Task
independently	O
,	O
we	O
model	O
them	O
jointly	O
using	O
a	O
conditional	B-Method
random	I-Method
field	E-Method
.	O

For	O
an	O
input	O
sentence	O
we	O
consider	O
to	O
be	O
the	O
matrix	O
of	O
scores	O
output	O
by	O
the	O
bidirectional	O
LSTM	S-Method
network	O
.	O

is	O
of	O
size	O
,	O
where	O
is	O
the	O
number	O
of	O
distinct	O
tags	O
,	O
and	O
corresponds	O
to	O
the	O
score	O
of	O
the	O
tag	O
of	O
the	O
word	O
in	O
a	O
sentence	O
.	O

For	O
a	O
sequence	O
of	O
predictions	O
we	O
define	O
its	O
score	O
to	O
be	O
where	O
is	O
a	O
matrix	O
of	O
transition	O
scores	O
such	O
that	O
represents	O
the	O
score	O
of	O
a	O
transition	O
from	O
the	O
tag	O
to	O
tag	O
.	O

and	O
are	O
the	O
start	O
and	O
end	O
tags	O
of	O
a	O
sentence	O
,	O
that	O
we	O
add	O
to	O
the	O
set	O
of	O
possible	O
tags	O
.	O

is	O
therefore	O
a	O
square	O
matrix	O
of	O
size	O
.	O

A	O
softmax	O
over	O
all	O
possible	O
tag	O
sequences	O
yields	O
a	O
probability	O
for	O
the	O
sequence	O
:	O
During	O
training	S-Task
,	O
we	O
maximize	O
the	O
log	O
-	O
probability	O
of	O
the	O
correct	O
tag	O
sequence	O
:	O
where	O
represents	O
all	O
possible	O
tag	O
sequences	O
(	O
even	O
those	O
that	O
do	O
not	O
verify	O
the	O
IOB	O
format	O
)	O
for	O
a	O
sentence	O
.	O

From	O
the	O
formulation	O
above	O
,	O
it	O
is	O
evident	O
that	O
we	O
encourage	O
our	O
network	O
to	O
produce	O
a	O
valid	O
sequence	O
of	O
output	O
labels	O
.	O

While	O
decoding	S-Task
,	O
we	O
predict	O
the	O
output	O
sequence	O
that	O
obtains	O
the	O
maximum	O
score	O
given	O
by	O
:	O
Since	O
we	O
are	O
only	O
modeling	O
bigram	O
interactions	O
between	O
outputs	O
,	O
both	O
the	O
summation	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
and	O
the	O
maximum	O
a	O
posteriori	O
sequence	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
can	O
be	O
computed	O
using	O
dynamic	B-Method
programming	E-Method
.	O

subsection	O
:	O
Parameterization	O
and	O
Training	O
The	O
scores	O
associated	O
with	O
each	O
tagging	O
decision	O
for	O
each	O
token	O
(	O
i.e.	O
,	O
the	O
’s	O
)	O
are	O
defined	O
to	O
be	O
the	O
dot	O
product	O
between	O
the	O
embedding	O
of	O
a	O
word	O
-	O
in	O
-	O
context	O
computed	O
with	O
a	O
bidirectional	O
LSTM	S-Method
—	O
exactly	O
the	O
same	O
as	O
the	O
POS	B-Method
tagging	I-Method
model	E-Method
of	O
ling:2015	S-Method
and	O
these	O
are	O
combined	O
with	O
bigram	O
compatibility	O
scores	O
(	O
i.e.	O
,	O
the	O
’	O
s	O
)	O
.	O

This	O
architecture	O
is	O
shown	O
in	O
figure	O
[	O
reference	O
]	O
.	O

Circles	O
represent	O
observed	O
variables	O
,	O
diamonds	O
are	O
deterministic	O
functions	O
of	O
their	O
parents	O
,	O
and	O
double	O
circles	O
are	O
random	O
variables	O
.	O

The	O
parameters	O
of	O
this	O
model	O
are	O
thus	O
the	O
matrix	O
of	O
bigram	O
compatibility	O
scores	O
,	O
and	O
the	O
parameters	O
that	O
give	O
rise	O
to	O
the	O
matrix	O
,	O
namely	O
the	O
parameters	O
of	O
the	O
bidirectional	O
LSTM	S-Method
,	O
the	O
linear	O
feature	O
weights	O
,	O
and	O
the	O
word	O
embeddings	O
.	O

As	O
in	O
part	O
[	O
reference	O
]	O
,	O
let	O
denote	O
the	O
sequence	O
of	O
word	O
embeddings	O
for	O
every	O
word	O
in	O
a	O
sentence	O
,	O
and	O
be	O
their	O
associated	O
tags	O
.	O

We	O
return	O
to	O
a	O
discussion	O
of	O
how	O
the	O
embeddings	O
are	O
modeled	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
sequence	O
of	O
word	O
embeddings	O
is	O
given	O
as	O
input	O
to	O
a	O
bidirectional	O
LSTM	S-Method
,	O
which	O
returns	O
a	O
representation	O
of	O
the	O
left	O
and	O
right	O
context	O
for	O
each	O
word	O
as	O
explained	O
in	O
[	O
reference	O
]	O
.	O

These	O
representations	O
are	O
concatenated	O
(	O
)	O
and	O
linearly	O
projected	O
onto	O
a	O
layer	O
whose	O
size	O
is	O
equal	O
to	O
the	O
number	O
of	O
distinct	O
tags	O
.	O

Instead	O
of	O
using	O
the	O
softmax	O
output	O
from	O
this	O
layer	O
,	O
we	O
use	O
a	O
CRF	S-Method
as	O
previously	O
described	O
to	O
take	O
into	O
account	O
neighboring	O
tags	O
,	O
yielding	O
the	O
final	O
predictions	O
for	O
every	O
word	O
.	O

Additionally	O
,	O
we	O
observed	O
that	O
adding	O
a	O
hidden	O
layer	O
between	O
and	O
the	O
CRF	S-Method
layer	O
marginally	O
improved	O
our	O
results	O
.	O

All	O
results	O
reported	O
with	O
this	O
model	O
incorporate	O
this	O
extra	O
-	O
layer	O
.	O

The	O
parameters	O
are	O
trained	O
to	O
maximize	O
Eq	O
.	O

[	O
reference	O
]	O
of	O
observed	O
sequences	O
of	O
NER	S-Task
tags	O
in	O
an	O
annotated	O
corpus	O
,	O
given	O
the	O
observed	O
words	O
.	O

subsection	O
:	O
Tagging	B-Method
Schemes	E-Method
The	O
task	O
of	O
named	B-Task
entity	I-Task
recognition	E-Task
is	O
to	O
assign	O
a	O
named	O
entity	O
label	O
to	O
every	O
word	O
in	O
a	O
sentence	O
.	O

A	O
single	O
named	O
entity	O
could	O
span	O
several	O
tokens	O
within	O
a	O
sentence	O
.	O

Sentences	O
are	O
usually	O
represented	O
in	O
the	O
IOB	O
format	O
(	O
Inside	O
,	O
Outside	O
,	O
Beginning	O
)	O
where	O
every	O
token	O
is	O
labeled	O
as	O
B	O
-	O
label	O
if	O
the	O
token	O
is	O
the	O
beginning	O
of	O
a	O
named	O
entity	O
,	O
I	O
-	O
label	O
if	O
it	O
is	O
inside	O
a	O
named	O
entity	O
but	O
not	O
the	O
first	O
token	O
within	O
the	O
named	O
entity	O
,	O
or	O
O	O
otherwise	O
.	O

However	O
,	O
we	O
decided	O
to	O
use	O
the	O
IOBES	B-Method
tagging	I-Method
scheme	E-Method
,	O
a	O
variant	O
of	O
IOB	S-Method
commonly	O
used	O
for	O
named	B-Task
entity	I-Task
recognition	E-Task
,	O
which	O
encodes	O
information	O
about	O
singleton	O
entities	O
(	O
S	O
)	O
and	O
explicitly	O
marks	O
the	O
end	O
of	O
named	O
entities	O
(	O
E	O
)	O
.	O

Using	O
this	O
scheme	O
,	O
tagging	O
a	O
word	O
as	O
I	O
-	O
label	O
with	O
high	O
-	O
confidence	O
narrows	O
down	O
the	O
choices	O
for	O
the	O
subsequent	O
word	O
to	O
I	O
-	O
label	O
or	O
E	O
-	O
label	O
,	O
however	O
,	O
the	O
IOB	B-Method
scheme	E-Method
is	O
only	O
capable	O
of	O
determining	O
that	O
the	O
subsequent	O
word	O
can	O
not	O
be	O
the	O
interior	O
of	O
another	O
label	O
.	O

ratinov2009design	O
and	O
dai2015enhancing	O
showed	O
that	O
using	O
a	O
more	O
expressive	O
tagging	B-Method
scheme	E-Method
like	O
IOBES	S-Method
improves	O
model	O
performance	O
marginally	O
.	O

However	O
,	O
we	O
did	O
not	O
observe	O
a	O
significant	O
improvement	O
over	O
the	O
IOB	B-Method
tagging	I-Method
scheme	E-Method
.	O

section	O
:	O
Transition	B-Method
-	I-Method
Based	I-Method
Chunking	I-Method
Model	E-Method
As	O
an	O
alternative	O
to	O
the	O
LSTM	B-Method
-	I-Method
CRF	E-Method
discussed	O
in	O
the	O
previous	O
section	O
,	O
we	O
explore	O
a	O
new	O
architecture	O
that	O
chunks	O
and	O
labels	O
a	O
sequence	O
of	O
inputs	O
using	O
an	O
algorithm	O
similar	O
to	O
transition	B-Method
-	I-Method
based	I-Method
dependency	I-Method
parsing	E-Method
.	O

This	O
model	O
directly	O
constructs	O
representations	O
of	O
the	O
multi	O
-	O
token	O
names	O
(	O
e.g.	O
,	O
the	O
name	O
Mark	O
Watney	O
is	O
composed	O
into	O
a	O
single	O
representation	O
)	O
.	O

This	O
model	O
relies	O
on	O
a	O
stack	B-Method
data	I-Method
structure	E-Method
to	O
incrementally	O
construct	O
chunks	O
of	O
the	O
input	O
.	O

To	O
obtain	O
representations	O
of	O
this	O
stack	O
used	O
for	O
predicting	B-Task
subsequent	I-Task
actions	E-Task
,	O
we	O
use	O
the	O
Stack	B-Method
-	I-Method
LSTM	E-Method
presented	O
by	O
dyer:2015	O
,	O
in	O
which	O
the	O
LSTM	S-Method
is	O
augmented	O
with	O
a	O
“	O
stack	O
pointer	O
.	O

”	O
While	O
sequential	O
LSTMs	S-Method
model	O
sequences	O
from	O
left	O
to	O
right	O
,	O
stack	O
LSTMs	S-Method
permit	O
embedding	O
of	O
a	O
stack	O
of	O
objects	O
that	O
are	O
both	O
added	O
to	O
(	O
using	O
a	O
push	O
operation	O
)	O
and	O
removed	O
from	O
(	O
using	O
a	O
pop	O
operation	O
)	O
.	O

This	O
allows	O
the	O
Stack	B-Method
-	I-Method
LSTM	E-Method
to	O
work	O
like	O
a	O
stack	S-Method
that	O
maintains	O
a	O
“	O
summary	O
embedding	O
”	O
of	O
its	O
contents	O
.	O

We	O
refer	O
to	O
this	O
model	O
as	O
Stack	B-Method
-	I-Method
LSTM	E-Method
or	O
S	B-Method
-	I-Method
LSTM	E-Method
model	O
for	O
simplicity	O
.	O

Finally	O
,	O
we	O
refer	O
interested	O
readers	O
to	O
the	O
original	O
paper	O
for	O
details	O
about	O
the	O
Stack	O
-	O
LSTM	S-Method
model	O
since	O
in	O
this	O
paper	O
we	O
merely	O
use	O
the	O
same	O
architecture	O
through	O
a	O
new	O
transition	B-Method
-	I-Method
based	I-Method
algorithm	E-Method
presented	O
in	O
the	O
following	O
Section	O
.	O

subsection	O
:	O
Chunking	B-Method
Algorithm	E-Method
We	O
designed	O
a	O
transition	B-Method
inventory	E-Method
which	O
is	O
given	O
in	O
Figure	O
[	O
reference	O
]	O
that	O
is	O
inspired	O
by	O
transition	B-Method
-	I-Method
based	I-Method
parsers	E-Method
,	O
in	O
particular	O
the	O
arc	B-Method
-	I-Method
standard	I-Method
parser	E-Method
of	O
nivre2004	O
.	O

In	O
this	O
algorithm	O
,	O
we	O
make	O
use	O
of	O
two	O
stacks	O
(	O
designated	O
output	O
and	O
stack	O
representing	O
,	O
respectively	O
,	O
completed	O
chunks	O
and	O
scratch	O
space	O
)	O
and	O
a	O
buffer	O
that	O
contains	O
the	O
words	O
that	O
have	O
yet	O
to	O
be	O
processed	O
.	O

The	O
transition	O
inventory	O
contains	O
the	O
following	O
transitions	O
:	O
The	O
shift	B-Method
transition	E-Method
moves	O
a	O
word	O
from	O
the	O
buffer	O
to	O
the	O
stack	O
,	O
the	O
out	O
transition	O
moves	O
a	O
word	O
from	O
the	O
buffer	O
directly	O
into	O
the	O
output	O
stack	O
while	O
the	O
reduce	O
(	O
y	O
)	O
transition	O
pops	O
all	O
items	O
from	O
the	O
top	O
of	O
the	O
stack	O
creating	O
a	O
“	O
chunk	O
,	O
”	O
labels	O
this	O
with	O
label	O
,	O
and	O
pushes	O
a	O
representation	O
of	O
this	O
chunk	O
onto	O
the	O
output	O
stack	O
.	O

The	O
algorithm	O
completes	O
when	O
the	O
stack	O
and	O
buffer	O
are	O
both	O
empty	O
.	O

The	O
algorithm	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
which	O
shows	O
the	O
sequence	O
of	O
operations	O
required	O
to	O
process	O
the	O
sentence	O
Mark	O
Watney	O
visited	O
Mars	O
.	O

The	O
model	O
is	O
parameterized	O
by	O
defining	O
a	O
probability	O
distribution	O
over	O
actions	O
at	O
each	O
time	O
step	O
,	O
given	O
the	O
current	O
contents	O
of	O
the	O
stack	O
,	O
buffer	O
,	O
and	O
output	O
,	O
as	O
well	O
as	O
the	O
history	O
of	O
actions	O
taken	O
.	O

Following	O
dyer:2015	O
,	O
we	O
use	O
stack	O
LSTMs	S-Method
to	O
compute	O
a	O
fixed	O
dimensional	O
embedding	O
of	O
each	O
of	O
these	O
,	O
and	O
take	O
a	O
concatenation	O
of	O
these	O
to	O
obtain	O
the	O
full	O
algorithm	O
state	O
.	O

This	O
representation	O
is	O
used	O
to	O
define	O
a	O
distribution	O
over	O
the	O
possible	O
actions	O
that	O
can	O
be	O
taken	O
at	O
each	O
time	O
step	O
.	O

The	O
model	O
is	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
sequences	O
of	O
reference	O
actions	O
(	O
extracted	O
from	O
a	O
labeled	O
training	O
corpus	O
)	O
given	O
the	O
input	O
sentences	O
.	O

To	O
label	O
a	O
new	O
input	O
sequence	O
at	O
test	O
time	O
,	O
the	O
maximum	O
probability	O
action	O
is	O
chosen	O
greedily	O
until	O
the	O
algorithm	O
reaches	O
a	O
termination	O
state	O
.	O

Although	O
this	O
is	O
not	O
guaranteed	O
to	O
find	O
a	O
global	O
optimum	O
,	O
it	O
is	O
effective	O
in	O
practice	O
.	O

Since	O
each	O
token	O
is	O
either	O
moved	O
directly	O
to	O
the	O
output	O
(	O
1	O
action	O
)	O
or	O
first	O
to	O
the	O
stack	O
and	O
then	O
the	O
output	O
(	O
2	O
actions	O
)	O
,	O
the	O
total	O
number	O
of	O
actions	O
for	O
a	O
sequence	O
of	O
length	O
is	O
maximally	O
.	O

It	O
is	O
worth	O
noting	O
that	O
the	O
nature	O
of	O
this	O
algorithm	B-Method
model	E-Method
makes	O
it	O
agnostic	O
to	O
the	O
tagging	B-Method
scheme	E-Method
used	O
since	O
it	O
directly	O
predicts	O
labeled	O
chunks	O
.	O

subsection	O
:	O
Representing	B-Task
Labeled	I-Task
Chunks	E-Task
When	O
the	O
operation	O
is	O
executed	O
,	O
the	O
algorithm	O
shifts	O
a	O
sequence	O
of	O
tokens	O
(	O
together	O
with	O
their	O
vector	O
embeddings	O
)	O
from	O
the	O
stack	O
to	O
the	O
output	O
buffer	O
as	O
a	O
single	O
completed	O
chunk	O
.	O

To	O
compute	O
an	O
embedding	O
of	O
this	O
sequence	O
,	O
we	O
run	O
a	O
bidirectional	O
LSTM	S-Method
over	O
the	O
embeddings	O
of	O
its	O
constituent	O
tokens	O
together	O
with	O
a	O
token	O
representing	O
the	O
type	O
of	O
the	O
chunk	O
being	O
identified	O
(	O
i.e.	O
,	O
)	O
.	O

This	O
function	O
is	O
given	O
as	O
,	O
where	O
is	O
a	O
learned	O
embedding	O
of	O
a	O
label	O
type	O
.	O

Thus	O
,	O
the	O
output	O
buffer	O
contains	O
a	O
single	O
vector	B-Method
representation	E-Method
for	O
each	O
labeled	O
chunk	O
that	O
is	O
generated	O
,	O
regardless	O
of	O
its	O
length	O
.	O

section	O
:	O
Input	O
Word	B-Method
Embeddings	E-Method
The	O
input	O
layers	O
to	O
both	O
of	O
our	O
models	O
are	O
vector	B-Method
representations	I-Method
of	I-Method
individual	I-Method
words	E-Method
.	O

Learning	O
independent	B-Method
representations	E-Method
for	O
word	O
types	O
from	O
the	O
limited	O
NER	S-Task
training	O
data	O
is	O
a	O
difficult	O
problem	O
:	O
there	O
are	O
simply	O
too	O
many	O
parameters	O
to	O
reliably	O
estimate	O
.	O

Since	O
many	O
languages	O
have	O
orthographic	O
or	O
morphological	O
evidence	O
that	O
something	O
is	O
a	O
name	O
(	O
or	O
not	O
a	O
name	O
)	O
,	O
we	O
want	O
representations	O
that	O
are	O
sensitive	O
to	O
the	O
spelling	O
of	O
words	O
.	O

We	O
therefore	O
use	O
a	O
model	O
that	O
constructs	O
representations	O
of	O
words	O
from	O
representations	O
of	O
the	O
characters	O
they	O
are	O
composed	O
of	O
(	O
[	O
reference	O
]	O
)	O
.	O

Our	O
second	O
intuition	O
is	O
that	O
names	O
,	O
which	O
may	O
individually	O
be	O
quite	O
varied	O
,	O
appear	O
in	O
regular	O
contexts	O
in	O
large	O
corpora	O
.	O

Therefore	O
we	O
use	O
embeddings	O
learned	O
from	O
a	O
large	O
corpus	O
that	O
are	O
sensitive	O
to	O
word	O
order	O
(	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
to	O
prevent	O
the	O
models	O
from	O
depending	O
on	O
one	O
representation	O
or	O
the	O
other	O
too	O
strongly	O
,	O
we	O
use	O
dropout	B-Method
training	E-Method
and	O
find	O
this	O
is	O
crucial	O
for	O
good	O
generalization	S-Task
performance	O
(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Character	B-Method
-	I-Method
based	I-Method
models	I-Method
of	I-Method
words	E-Method
An	O
important	O
distinction	O
of	O
our	O
work	O
from	O
most	O
previous	O
approaches	O
is	O
that	O
we	O
learn	O
character	O
-	O
level	O
features	O
while	O
training	O
instead	O
of	O
hand	O
-	O
engineering	O
prefix	O
and	O
suffix	O
information	O
about	O
words	O
.	O

Learning	B-Task
character	I-Task
-	I-Task
level	I-Task
embeddings	E-Task
has	O
the	O
advantage	O
of	O
learning	O
representations	O
specific	O
to	O
the	O
task	O
and	O
domain	O
at	O
hand	O
.	O

They	O
have	O
been	O
found	O
useful	O
for	O
morphologically	O
rich	O
languages	O
and	O
to	O
handle	O
the	O
out	B-Task
-	I-Task
of	I-Task
-	I-Task
vocabulary	I-Task
problem	E-Task
for	O
tasks	O
like	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	I-Task
tagging	E-Task
and	O
language	B-Task
modeling	E-Task
or	O
dependency	B-Task
parsing	E-Task
.	O

Figure	O
[	O
reference	O
]	O
describes	O
our	O
architecture	O
to	O
generate	O
a	O
word	B-Method
embedding	E-Method
for	O
a	O
word	O
from	O
its	O
characters	O
.	O

A	O
character	O
lookup	O
table	O
initialized	O
at	O
random	O
contains	O
an	O
embedding	O
for	O
every	O
character	O
.	O

The	O
character	O
embeddings	O
corresponding	O
to	O
every	O
character	O
in	O
a	O
word	O
are	O
given	O
in	O
direct	O
and	O
reverse	O
order	O
to	O
a	O
forward	O
and	O
a	O
backward	O
LSTM	S-Method
.	O

The	O
embedding	O
for	O
a	O
word	O
derived	O
from	O
its	O
characters	O
is	O
the	O
concatenation	O
of	O
its	O
forward	B-Method
and	I-Method
backward	I-Method
representations	E-Method
from	O
the	O
bidirectional	O
LSTM	S-Method
.	O

This	O
character	B-Method
-	I-Method
level	I-Method
representation	E-Method
is	O
then	O
concatenated	O
with	O
a	O
word	B-Method
-	I-Method
level	I-Method
representation	E-Method
from	O
a	O
word	B-Method
lookup	I-Method
-	I-Method
table	E-Method
.	O

During	O
testing	O
,	O
words	O
that	O
do	O
not	O
have	O
an	O
embedding	O
in	O
the	O
lookup	O
table	O
are	O
mapped	O
to	O
a	O
UNK	B-Method
embedding	E-Method
.	O

To	O
train	O
the	O
UNK	B-Method
embedding	E-Method
,	O
we	O
replace	O
singletons	O
with	O
the	O
UNK	B-Method
embedding	E-Method
with	O
a	O
probability	O
.	O

In	O
all	O
our	O
experiments	O
,	O
the	O
hidden	O
dimension	O
of	O
the	O
forward	O
and	O
backward	O
character	O
LSTMs	S-Method
are	O
each	O
,	O
which	O
results	O
in	O
our	O
character	B-Method
-	I-Method
based	I-Method
representation	I-Method
of	I-Method
words	E-Method
being	O
of	O
dimension	O
.	O

Recurrent	B-Method
models	E-Method
like	O
RNNs	S-Method
and	O
LSTMs	S-Method
are	O
capable	O
of	O
encoding	O
very	O
long	O
sequences	O
,	O
however	O
,	O
they	O
have	O
a	O
representation	O
biased	O
towards	O
their	O
most	O
recent	O
inputs	O
.	O

As	O
a	O
result	O
,	O
we	O
expect	O
the	O
final	O
representation	O
of	O
the	O
forward	O
LSTM	S-Method
to	O
be	O
an	O
accurate	O
representation	O
of	O
the	O
suffix	O
of	O
the	O
word	O
,	O
and	O
the	O
final	O
state	O
of	O
the	O
backward	O
LSTM	S-Method
to	O
be	O
a	O
better	O
representation	O
of	O
its	O
prefix	O
.	O

Alternative	O
approaches	O
—	O
most	O
notably	O
like	O
convolutional	B-Method
networks	E-Method
—	O
have	O
been	O
proposed	O
to	O
learn	O
representations	B-Task
of	I-Task
words	E-Task
from	O
their	O
characters	O
.	O

However	O
,	O
convnets	S-Method
are	O
designed	O
to	O
discover	O
position	O
-	O
invariant	O
features	O
of	O
their	O
inputs	O
.	O

While	O
this	O
is	O
appropriate	O
for	O
many	O
problems	O
,	O
e.g.	O
,	O
image	B-Task
recognition	E-Task
(	O
a	O
cat	O
can	O
appear	O
anywhere	O
in	O
a	O
picture	O
)	O
,	O
we	O
argue	O
that	O
important	O
information	O
is	O
position	O
dependent	O
(	O
e.g.	O
,	O
prefixes	O
and	O
suffixes	O
encode	O
different	O
information	O
than	O
stems	O
)	O
,	O
making	O
LSTMs	S-Method
an	O
a	O
priori	O
better	O
function	O
class	O
for	O
modeling	O
the	O
relationship	O
between	O
words	O
and	O
their	O
characters	O
.	O

subsection	O
:	O
Pretrained	B-Method
embeddings	E-Method
As	O
in	O
collobert2011natural	O
,	O
we	O
use	O
pretrained	O
word	O
embeddings	O
to	O
initialize	O
our	O
lookup	O
table	O
.	O

We	O
observe	O
significant	O
improvements	O
using	O
pretrained	B-Method
word	I-Method
embeddings	E-Method
over	O
randomly	O
initialized	O
ones	O
.	O

Embeddings	S-Method
are	O
pretrained	O
using	O
skip	B-Method
-	I-Method
n	I-Method
-	I-Method
gram	E-Method
,	O
a	O
variation	O
of	O
word2vec	S-Method
that	O
accounts	O
for	O
word	O
order	O
.	O

These	O
embeddings	O
are	O
fine	O
-	O
tuned	O
during	O
training	O
.	O

Word	O
embeddings	O
for	O
Spanish	O
,	O
Dutch	O
,	O
German	O
and	O
English	S-Material
are	O
trained	O
using	O
the	O
Spanish	O
Gigaword	O
version	O
3	O
,	O
the	O
Leipzig	O
corpora	O
collection	O
,	O
the	O
German	O
monolingual	O
training	O
data	O
from	O
the	O
2010	O
Machine	B-Task
Translation	I-Task
Workshop	E-Task
and	O
the	O
English	O
Gigaword	O
version	O
4	O
(	O
with	O
the	O
LA	O
Times	O
and	O
NY	O
Times	O
portions	O
removed	O
)	O
respectively	O
.	O

We	O
use	O
an	O
embedding	O
dimension	O
of	O
for	O
English	S-Material
,	O
for	O
other	O
languages	O
,	O
a	O
minimum	O
word	O
frequency	O
cutoff	O
of	O
,	O
and	O
a	O
window	O
size	O
of	O
.	O

subsection	O
:	O
Dropout	B-Method
training	E-Method
Initial	O
experiments	O
showed	O
that	O
character	O
-	O
level	O
embeddings	O
did	O
not	O
improve	O
our	O
overall	O
performance	O
when	O
used	O
in	O
conjunction	O
with	O
pretrained	B-Method
word	I-Method
representations	E-Method
.	O

To	O
encourage	O
the	O
model	O
to	O
depend	O
on	O
both	O
representations	O
,	O
we	O
use	O
dropout	B-Method
training	E-Method
,	O
applying	O
a	O
dropout	O
mask	O
to	O
the	O
final	O
embedding	B-Method
layer	E-Method
just	O
before	O
the	O
input	O
to	O
the	O
bidirectional	O
LSTM	S-Method
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
observe	O
a	O
significant	O
improvement	O
in	O
our	O
model	O
’s	O
performance	O
after	O
using	O
dropout	S-Method
(	O
see	O
table	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Experiments	O
This	O
section	O
presents	O
the	O
methods	O
we	O
use	O
to	O
train	O
our	O
models	O
,	O
the	O
results	O
we	O
obtained	O
on	O
various	O
tasks	O
and	O
the	O
impact	O
of	O
our	O
networks	O
’	O
configuration	O
on	O
model	O
performance	O
.	O

subsection	O
:	O
Training	O
For	O
both	O
models	O
presented	O
,	O
we	O
train	O
our	O
networks	O
using	O
the	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
updating	O
our	O
parameters	O
on	O
every	O
training	O
example	O
,	O
one	O
at	O
a	O
time	O
,	O
using	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
with	O
a	O
learning	O
rate	O
of	O
and	O
a	O
gradient	B-Method
clipping	E-Method
of	O
.	O

Several	O
methods	O
have	O
been	O
proposed	O
to	O
enhance	O
the	O
performance	O
of	O
SGD	S-Method
,	O
such	O
as	O
Adadelta	S-Method
or	O
Adam	S-Method
.	O

Although	O
we	O
observe	O
faster	O
convergence	S-Metric
using	O
these	O
methods	O
,	O
none	O
of	O
them	O
perform	O
as	O
well	O
as	O
SGD	S-Method
with	O
gradient	B-Method
clipping	E-Method
.	O

Our	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
uses	O
a	O
single	B-Method
layer	E-Method
for	O
the	O
forward	O
and	O
backward	O
LSTMs	S-Method
whose	O
dimensions	O
are	O
set	O
to	O
.	O

Tuning	O
this	O
dimension	O
did	O
not	O
significantly	O
impact	O
model	O
performance	O
.	O

We	O
set	O
the	O
dropout	B-Metric
rate	E-Metric
to	O
.	O

Using	O
higher	O
rates	O
negatively	O
impacted	O
our	O
results	O
,	O
while	O
smaller	O
rates	O
led	O
to	O
longer	O
training	B-Metric
time	E-Metric
.	O

The	O
stack	O
-	O
LSTM	S-Method
model	O
uses	O
two	O
layers	O
each	O
of	O
dimension	O
for	O
each	O
stack	O
.	O

The	O
embeddings	O
of	O
the	O
actions	O
used	O
in	O
the	O
composition	B-Method
functions	E-Method
have	O
dimensions	O
each	O
,	O
and	O
the	O
output	O
embedding	O
is	O
of	O
dimension	O
.	O

We	O
experimented	O
with	O
different	O
dropout	B-Metric
rates	E-Metric
and	O
reported	O
the	O
scores	O
using	O
the	O
best	O
dropout	B-Metric
rate	E-Metric
for	O
each	O
language	O
.	O

It	O
is	O
a	O
greedy	B-Method
model	E-Method
that	O
apply	O
locally	O
optimal	O
actions	O
until	O
the	O
entire	O
sentence	O
is	O
processed	O
,	O
further	O
improvements	O
might	O
be	O
obtained	O
with	O
beam	B-Method
search	E-Method
or	O
training	O
with	O
exploration	S-Task
.	O

subsection	O
:	O
Data	O
Sets	O
We	O
test	O
our	O
model	O
on	O
different	O
datasets	O
for	O
named	B-Task
entity	I-Task
recognition	E-Task
.	O

To	O
demonstrate	O
our	O
model	O
’s	O
ability	O
to	O
generalize	O
to	O
different	O
languages	O
,	O
we	O
present	O
results	O
on	O
the	O
CoNLL	O
-	O
2002	O
and	O
CoNLL	B-Material
-	I-Material
2003	I-Material
datasets	E-Material
that	O
contain	O
independent	O
named	O
entity	O
labels	O
for	O
English	S-Material
,	O
Spanish	O
,	O
German	O
and	O
Dutch	O
.	O

All	O
datasets	O
contain	O
four	O
different	O
types	O
of	O
named	O
entities	O
:	O
locations	O
,	O
persons	O
,	O
organizations	O
,	O
and	O
miscellaneous	O
entities	O
that	O
do	O
not	O
belong	O
in	O
any	O
of	O
the	O
three	O
previous	O
categories	O
.	O

Although	O
POS	O
tags	O
were	O
made	O
available	O
for	O
all	O
datasets	O
,	O
we	O
did	O
not	O
include	O
them	O
in	O
our	O
models	O
.	O

We	O
did	O
not	O
perform	O
any	O
dataset	B-Task
preprocessing	E-Task
,	O
apart	O
from	O
replacing	O
every	O
digit	O
with	O
a	O
zero	O
in	O
the	O
English	O
NER	O
dataset	O
.	O

subsection	O
:	O
Results	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
comparisons	O
with	O
other	O
models	O
for	O
named	B-Task
entity	I-Task
recognition	E-Task
in	O
English	S-Material
.	O

To	O
make	O
the	O
comparison	O
between	O
our	O
model	O
and	O
others	O
fair	O
,	O
we	O
report	O
the	O
scores	O
of	O
other	O
models	O
with	O
and	O
without	O
the	O
use	O
of	O
external	O
labeled	O
data	O
such	O
as	O
gazetteers	O
and	O
knowledge	O
bases	O
.	O

Our	O
models	O
do	O
not	O
use	O
gazetteers	O
or	O
any	O
external	O
labeled	O
resources	O
.	O

The	O
best	O
score	O
reported	O
on	O
this	O
task	O
is	O
by	O
luojoint	O
.	O

They	O
obtained	O
a	O
F	S-Metric
of	O
91.2	O
by	O
jointly	O
modeling	O
the	O
NER	S-Task
and	O
entity	B-Task
linking	I-Task
tasks	E-Task
.	O

Their	O
model	O
uses	O
a	O
lot	O
of	O
hand	O
-	O
engineered	O
features	O
including	O
spelling	O
features	O
,	O
WordNet	O
clusters	O
,	O
Brown	O
clusters	O
,	O
POS	O
tags	O
,	O
chunks	O
tags	O
,	O
as	O
well	O
as	O
stemming	O
and	O
external	O
knowledge	O
bases	O
like	O
Freebase	O
and	O
Wikipedia	O
.	O

Our	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
outperforms	O
all	O
other	O
systems	O
,	O
including	O
the	O
ones	O
using	O
external	O
labeled	O
data	O
like	O
gazetteers	O
.	O

Our	O
Stack	O
-	O
LSTM	S-Method
model	O
also	O
outperforms	O
all	O
previous	O
models	O
that	O
do	O
not	O
incorporate	O
external	O
features	O
,	O
apart	O
from	O
the	O
one	O
presented	O
by	O
chiu2015named	O
.	O

Tables	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
present	O
our	O
results	O
on	O
NER	S-Task
for	O
German	O
,	O
Dutch	O
and	O
Spanish	O
respectively	O
in	O
comparison	O
to	O
other	O
models	O
.	O

On	O
these	O
three	O
languages	O
,	O
the	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
significantly	O
outperforms	O
all	O
previous	O
methods	O
,	O
including	O
the	O
ones	O
using	O
external	O
labeled	O
data	O
.	O

The	O
only	O
exception	O
is	O
Dutch	O
,	O
where	O
the	O
model	O
of	O
gillick2015multilingual	O
can	O
perform	O
better	O
by	O
leveraging	O
the	O
information	O
from	O
other	O
NER	S-Task
datasets	O
.	O

The	O
Stack	B-Method
-	I-Method
LSTM	E-Method
also	O
consistently	O
presents	O
state	O
-	O
the	O
-	O
art	O
(	O
or	O
close	O
to	O
)	O
results	O
compared	O
to	O
systems	O
that	O
do	O
not	O
use	O
external	O
data	O
.	O

As	O
we	O
can	O
see	O
in	O
the	O
tables	O
,	O
the	O
Stack	O
-	O
LSTM	S-Method
model	O
is	O
more	O
dependent	O
on	O
character	B-Method
-	I-Method
based	I-Method
representations	E-Method
to	O
achieve	O
competitive	O
performance	O
;	O
we	O
hypothesize	O
that	O
the	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
requires	O
less	O
orthographic	O
information	O
since	O
it	O
gets	O
more	O
contextual	O
information	O
out	O
of	O
the	O
bidirectional	O
LSTMs	S-Method
;	O
however	O
,	O
the	O
Stack	O
-	O
LSTM	S-Method
model	O
consumes	O
the	O
words	O
one	O
by	O
one	O
and	O
it	O
just	O
relies	O
on	O
the	O
word	B-Method
representations	E-Method
when	O
it	O
chunks	O
words	O
.	O

subsection	O
:	O
Network	B-Method
architectures	E-Method
Our	O
models	O
had	O
several	O
components	O
that	O
we	O
could	O
tweak	O
to	O
understand	O
their	O
impact	O
on	O
the	O
overall	O
performance	O
.	O

We	O
explored	O
the	O
impact	O
that	O
the	O
CRF	S-Method
,	O
the	O
character	B-Method
-	I-Method
level	I-Method
representations	E-Method
,	O
pretraining	O
of	O
our	O
word	B-Method
embeddings	E-Method
and	O
dropout	S-Method
had	O
on	O
our	O
LSTM	S-Method
-	O
CRF	S-Method
model	O
.	O

We	O
observed	O
that	O
pretraining	O
our	O
word	B-Method
embeddings	E-Method
gave	O
us	O
the	O
biggest	O
improvement	O
in	O
overall	O
performance	O
of	O
in	O
F	S-Metric
.	O

The	O
CRF	S-Method
layer	O
gave	O
us	O
an	O
increase	O
of	O
,	O
while	O
using	O
dropout	O
resulted	O
in	O
a	O
difference	O
of	O
and	O
finally	O
learning	O
character	O
-	O
level	O
word	O
embeddings	O
resulted	O
in	O
an	O
increase	O
of	O
about	O
.	O

For	O
the	O
Stack	B-Method
-	I-Method
LSTM	E-Method
we	O
performed	O
a	O
similar	O
set	O
of	O
experiments	O
.	O

Results	O
with	O
different	O
architectures	O
are	O
given	O
in	O
table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
CoNLL	B-Task
-	I-Task
2002	I-Task
shared	I-Task
task	E-Task
,	O
carreras2002named	O
obtained	O
among	O
the	O
best	O
results	O
on	O
both	O
Dutch	O
and	O
Spanish	O
by	O
combining	O
several	O
small	O
fixed	B-Method
-	I-Method
depth	I-Method
decision	I-Method
trees	E-Method
.	O

Next	O
year	O
,	O
in	O
the	O
CoNLL	B-Task
-	I-Task
2003	I-Task
Shared	I-Task
Task	E-Task
,	O
florian2003named	O
obtained	O
the	O
best	O
score	O
on	O
German	O
by	O
combining	O
the	O
output	O
of	O
four	O
diverse	B-Method
classifiers	E-Method
.	O

qi2009combining	O
later	O
improved	O
on	O
this	O
with	O
a	O
neural	B-Method
network	E-Method
by	O
doing	O
unsupervised	B-Method
learning	E-Method
on	O
a	O
massive	O
unlabeled	O
corpus	O
.	O

Several	O
other	O
neural	B-Method
architectures	E-Method
have	O
previously	O
been	O
proposed	O
for	O
NER	S-Task
.	O

For	O
instance	O
,	O
collobert2011natural	O
uses	O
a	O
CNN	S-Method
over	O
a	O
sequence	O
of	O
word	B-Method
embeddings	E-Method
with	O
a	O
CRF	S-Method
layer	O
on	O
top	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
our	O
first	O
model	O
without	O
character	O
-	O
level	O
embeddings	O
and	O
with	O
the	O
bidirectional	O
LSTM	S-Method
being	O
replaced	O
by	O
a	O
CNN	S-Method
.	O

More	O
recently	O
,	O
huang:2015	O
presented	O
a	O
model	O
similar	O
to	O
our	O
LSTM	B-Method
-	I-Method
CRF	E-Method
,	O
but	O
using	O
hand	O
-	O
crafted	O
spelling	O
features	O
.	O

zhou2015end	O
also	O
used	O
a	O
similar	O
model	O
and	O
adapted	O
it	O
to	O
the	O
semantic	B-Task
role	I-Task
labeling	I-Task
task	E-Task
.	O

lin2009phrase	O
used	O
a	O
linear	O
chain	O
CRF	S-Method
with	O
regularization	S-Method
,	O
they	O
added	O
phrase	O
cluster	O
features	O
extracted	O
from	O
the	O
web	O
data	O
and	O
spelling	O
features	O
.	O

passos2014lexicon	O
also	O
used	O
a	O
linear	O
chain	O
CRF	S-Method
with	O
spelling	O
features	O
and	O
gazetteers	O
.	O

Language	B-Method
independent	I-Method
NER	I-Method
models	E-Method
like	O
ours	O
have	O
also	O
been	O
proposed	O
in	O
the	O
past	O
.	O

Cucerzan	O
and	O
Yarowsky	O
cucerzan1999language	O
,	O
cucerzan2002language	O
present	O
semi	B-Method
-	I-Method
supervised	I-Method
bootstrapping	I-Method
algorithms	E-Method
for	O
named	B-Task
entity	I-Task
recognition	E-Task
by	O
co	O
-	O
training	O
character	O
-	O
level	O
(	O
word	O
-	O
internal	O
)	O
and	O
token	O
-	O
level	O
(	O
context	O
)	O
features	O
.	O

eisenstein2011structured	O
use	O
Bayesian	B-Method
nonparametrics	E-Method
to	O
construct	O
a	O
database	O
of	O
named	O
entities	O
in	O
an	O
almost	O
unsupervised	B-Task
setting	E-Task
.	O

ratinov2009design	O
quantitatively	O
compare	O
several	O
approaches	O
for	O
NER	S-Task
and	O
build	O
their	O
own	O
supervised	B-Method
model	E-Method
using	O
a	O
regularized	B-Method
average	I-Method
perceptron	E-Method
and	O
aggregating	B-Method
context	I-Method
information	E-Method
.	O

Finally	O
,	O
there	O
is	O
currently	O
a	O
lot	O
of	O
interest	O
in	O
models	O
for	O
NER	S-Task
that	O
use	O
letter	B-Method
-	I-Method
based	I-Method
representations	E-Method
.	O

gillick2015multilingual	O
model	O
the	O
task	O
of	O
sequence	B-Task
-	I-Task
labeling	E-Task
as	O
a	O
sequence	B-Task
to	I-Task
sequence	I-Task
learning	I-Task
problem	E-Task
and	O
incorporate	O
character	B-Method
-	I-Method
based	I-Method
representations	E-Method
into	O
their	O
encoder	B-Method
model	E-Method
.	O

chiu2015named	O
employ	O
an	O
architecture	O
similar	O
to	O
ours	O
,	O
but	O
instead	O
use	O
CNNs	S-Method
to	O
learn	O
character	O
-	O
level	O
features	O
,	O
in	O
a	O
way	O
similar	O
to	O
the	O
work	O
by	O
santos2015boosting	O
.	O

section	O
:	O
Conclusion	O
This	O
paper	O
presents	O
two	O
neural	B-Method
architectures	E-Method
for	O
sequence	B-Task
labeling	E-Task
that	O
provide	O
the	O
best	O
NER	S-Task
results	O
ever	O
reported	O
in	O
standard	O
evaluation	O
settings	O
,	O
even	O
compared	O
with	O
models	O
that	O
use	O
external	O
resources	O
,	O
such	O
as	O
gazetteers	O
.	O

A	O
key	O
aspect	O
of	O
our	O
models	O
are	O
that	O
they	O
model	O
output	O
label	O
dependencies	O
,	O
either	O
via	O
a	O
simple	O
CRF	S-Method
architecture	O
,	O
or	O
using	O
a	O
transition	B-Method
-	I-Method
based	I-Method
algorithm	E-Method
to	O
explicitly	O
construct	O
and	O
label	O
chunks	O
of	O
the	O
input	O
.	O

Word	B-Method
representations	E-Method
are	O
also	O
crucially	O
important	O
for	O
success	O
:	O
we	O
use	O
both	O
pre	O
-	O
trained	O
word	B-Method
representations	E-Method
and	O
“	O
character	B-Method
-	I-Method
based	I-Method
”	I-Method
representations	E-Method
that	O
capture	O
morphological	O
and	O
orthographic	O
information	O
.	O

To	O
prevent	O
the	O
learner	O
from	O
depending	O
too	O
heavily	O
on	O
one	O
representation	O
class	O
,	O
dropout	S-Method
is	O
used	O
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Information	O
Innovation	O
Office	O
(	O
I2O	O
)	O
under	O
the	O
Low	O
Resource	O
Languages	O
for	O
Emergent	O
Incidents	O
(	O
LORELEI	O
)	O
program	O
issued	O
by	O
DARPA	O
/	O
I2O	O
under	O
Contract	O
No	O
.	O

HR0011	O
-	O
15	O
-	O
C	O
-	O
0114	O
.	O

Miguel	O
Ballesteros	O
is	O
supported	O
by	O
the	O
European	O
Commission	O
under	O
the	O
contract	O
numbers	O
FP7	O
-	O
ICT	O
-	O
610411	O
(	O
project	O
MULTISENSOR	O
)	O
and	O
H2020	O
-	O
RIA	O
-	O
645012	O
(	O
project	O
KRISTINA	O
)	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Bag	B-Method
of	I-Method
Tricks	E-Method
for	O
Efficient	O
Text	B-Task
Classification	E-Task
This	O
paper	O
explores	O
a	O
simple	O
and	O
efficient	O
baseline	O
for	O
text	B-Task
classification	E-Task
.	O

Our	O
experiments	O
show	O
that	O
our	O
fast	O
text	O
classifier	O
fastText	S-Method
is	O
often	O
on	O
par	O
with	O
deep	B-Method
learning	I-Method
classifiers	E-Method
in	O
terms	O
of	O
accuracy	S-Metric
,	O
and	O
many	O
orders	O
of	O
magnitude	O
faster	O
for	O
training	S-Task
and	O
evaluation	S-Task
.	O

We	O
can	O
train	O
fastText	S-Method
on	O
more	O
than	O
one	O
billion	O
words	O
in	O
less	O
than	O
ten	O
minutes	O
using	O
a	O
standard	O
multicore	B-Method
CPU	E-Method
,	O
and	O
classify	O
half	O
a	O
million	O
sentences	O
among	O
312	O
K	O
classes	O
in	O
less	O
than	O
a	O
minute	O
.	O

section	O
:	O
Introduction	O
Text	B-Task
classification	E-Task
is	O
an	O
important	O
task	O
in	O
Natural	B-Task
Language	I-Task
Processing	E-Task
with	O
many	O
applications	O
,	O
such	O
as	O
web	B-Task
search	E-Task
,	O
information	B-Task
retrieval	E-Task
,	O
ranking	S-Task
and	O
document	B-Task
classification	E-Task
.	O

Recently	O
,	O
models	O
based	O
on	O
neural	B-Method
networks	E-Method
have	O
become	O
increasingly	O
popular	O
.	O

While	O
these	O
models	O
achieve	O
very	O
good	O
performance	O
in	O
practice	O
,	O
they	O
tend	O
to	O
be	O
relatively	O
slow	O
both	O
at	O
train	B-Metric
and	I-Metric
test	I-Metric
time	E-Metric
,	O
limiting	O
their	O
use	O
on	O
very	O
large	O
datasets	O
.	O

Meanwhile	O
,	O
linear	B-Method
classifiers	E-Method
are	O
often	O
considered	O
as	O
strong	O
baselines	O
for	O
text	B-Task
classification	I-Task
problems	E-Task
.	O

Despite	O
their	O
simplicity	O
,	O
they	O
often	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
if	O
the	O
right	O
features	O
are	O
used	O
.	O

They	O
also	O
have	O
the	O
potential	O
to	O
scale	O
to	O
very	O
large	O
corpus	O
.	O

In	O
this	O
work	O
,	O
we	O
explore	O
ways	O
to	O
scale	O
these	O
baselines	O
to	O
very	O
large	O
corpus	O
with	O
a	O
large	O
output	O
space	O
,	O
in	O
the	O
context	O
of	O
text	B-Task
classification	E-Task
.	O

Inspired	O
by	O
the	O
recent	O
work	O
in	O
efficient	O
word	B-Task
representation	I-Task
learning	E-Task
,	O
we	O
show	O
that	O
linear	B-Method
models	E-Method
with	O
a	O
rank	O
constraint	O
and	O
a	O
fast	B-Method
loss	I-Method
approximation	E-Method
can	O
train	O
on	O
a	O
billion	O
words	O
within	O
ten	O
minutes	O
,	O
while	O
achieving	O
performance	O
on	O
par	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

We	O
evaluate	O
the	O
quality	O
of	O
our	O
approach	O
fastTexthttps:	O
//	O
github.com	O
/	O
facebookresearch	O
/	O
fastText	S-Method
on	O
two	O
different	O
tasks	O
,	O
namely	O
tag	B-Task
prediction	E-Task
and	O
sentiment	B-Task
analysis	E-Task
.	O

section	O
:	O
Model	O
architecture	O
A	O
simple	O
and	O
efficient	O
baseline	O
for	O
sentence	B-Task
classification	E-Task
is	O
to	O
represent	O
sentences	O
as	O
bag	O
of	O
words	O
(	O
BoW	S-Method
)	O
and	O
train	O
a	O
linear	B-Method
classifier	E-Method
,	O
e.g.	O
,	O
a	O
logistic	B-Method
regression	E-Method
or	O
an	O
SVM	S-Method
.	O

However	O
,	O
linear	B-Method
classifiers	E-Method
do	O
not	O
share	O
parameters	O
among	O
features	O
and	O
classes	O
.	O

This	O
possibly	O
limits	O
their	O
generalization	O
in	O
the	O
context	O
of	O
large	O
output	O
space	O
where	O
some	O
classes	O
have	O
very	O
few	O
examples	O
.	O

Common	O
solutions	O
to	O
this	O
problem	O
are	O
to	O
factorize	O
the	O
linear	B-Method
classifier	E-Method
into	O
low	O
rank	O
matrices	O
or	O
to	O
use	O
multilayer	B-Method
neural	I-Method
networks	E-Method
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
simple	O
linear	B-Method
model	E-Method
with	O
rank	O
constraint	O
.	O

The	O
first	O
weight	B-Method
matrix	E-Method
is	O
a	O
look	O
-	O
up	O
table	O
over	O
the	O
words	O
.	O

The	O
word	B-Method
representations	E-Method
are	O
then	O
averaged	O
into	O
a	O
text	B-Method
representation	E-Method
,	O
which	O
is	O
in	O
turn	O
fed	O
to	O
a	O
linear	B-Method
classifier	E-Method
.	O

The	O
text	B-Method
representation	E-Method
is	O
an	O
hidden	O
variable	O
which	O
can	O
be	O
potentially	O
be	O
reused	O
.	O

This	O
architecture	O
is	O
similar	O
to	O
the	O
cbow	B-Method
model	E-Method
of	O
mikolov2013efficient	O
,	O
where	O
the	O
middle	O
word	O
is	O
replaced	O
by	O
a	O
label	O
.	O

We	O
use	O
the	O
softmax	B-Method
function	E-Method
to	O
compute	O
the	O
probability	O
distribution	O
over	O
the	O
predefined	O
classes	O
.	O

For	O
a	O
set	O
of	O
documents	O
,	O
this	O
leads	O
to	O
minimizing	O
the	O
negative	O
log	O
-	O
likelihood	O
over	O
the	O
classes	O
:	O
where	O
is	O
the	O
normalized	O
bag	O
of	O
features	O
of	O
the	O
-	O
th	O
document	O
,	O
the	O
label	O
,	O
and	O
the	O
weight	O
matrices	O
.	O

This	O
model	O
is	O
trained	O
asynchronously	O
on	O
multiple	O
CPUs	S-Method
using	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
and	O
a	O
linearly	B-Method
decaying	I-Method
learning	I-Method
rate	E-Method
.	O

subsection	O
:	O
Hierarchical	B-Method
softmax	E-Method
hidden	O
output	O
When	O
the	O
number	O
of	O
classes	O
is	O
large	O
,	O
computing	O
the	O
linear	B-Method
classifier	E-Method
is	O
computationally	O
expensive	O
.	O

More	O
precisely	O
,	O
the	O
computational	B-Metric
complexity	E-Metric
is	O
where	O
is	O
the	O
number	O
of	O
classes	O
and	O
the	O
dimension	O
of	O
the	O
text	B-Method
representation	E-Method
.	O

In	O
order	O
to	O
improve	O
our	O
running	O
time	O
,	O
we	O
use	O
a	O
hierarchical	B-Method
softmax	E-Method
based	O
on	O
the	O
Huffman	B-Method
coding	I-Method
tree	E-Method
.	O

During	O
training	S-Task
,	O
the	O
computational	B-Metric
complexity	E-Metric
drops	O
to	O
.	O

The	O
hierarchical	B-Method
softmax	E-Method
is	O
also	O
advantageous	O
at	O
test	O
time	O
when	O
searching	O
for	O
the	O
most	O
likely	O
class	O
.	O

Each	O
node	O
is	O
associated	O
with	O
a	O
probability	O
that	O
is	O
the	O
probability	O
of	O
the	O
path	O
from	O
the	O
root	O
to	O
that	O
node	O
.	O

If	O
the	O
node	O
is	O
at	O
depth	O
with	O
parents	O
,	O
its	O
probability	O
is	O
This	O
means	O
that	O
the	O
probability	O
of	O
a	O
node	O
is	O
always	O
lower	O
than	O
the	O
one	O
of	O
its	O
parent	O
.	O

Exploring	O
the	O
tree	O
with	O
a	O
depth	B-Method
first	I-Method
search	E-Method
and	O
tracking	O
the	O
maximum	O
probability	O
among	O
the	O
leaves	O
allows	O
us	O
to	O
discard	O
any	O
branch	O
associated	O
with	O
a	O
small	O
probability	O
.	O

In	O
practice	O
,	O
we	O
observe	O
a	O
reduction	O
of	O
the	O
complexity	S-Metric
to	O
at	O
test	O
time	O
.	O

This	O
approach	O
is	O
further	O
extended	O
to	O
compute	O
the	O
-	O
top	O
targets	O
at	O
the	O
cost	O
of	O
,	O
using	O
a	O
binary	O
heap	O
.	O

subsection	O
:	O
N	B-Method
-	I-Method
gram	I-Method
features	E-Method
Bag	O
of	O
words	O
is	O
invariant	O
to	O
word	O
order	O
but	O
taking	O
explicitly	O
this	O
order	O
into	O
account	O
is	O
often	O
computationally	O
very	O
expensive	O
.	O

Instead	O
,	O
we	O
use	O
a	O
bag	B-Method
of	I-Method
n	I-Method
-	I-Method
grams	E-Method
as	O
additional	O
features	O
to	O
capture	O
some	O
partial	O
information	O
about	O
the	O
local	O
word	O
order	O
.	O

This	O
is	O
very	O
efficient	O
in	O
practice	O
while	O
achieving	O
comparable	O
results	O
to	O
methods	O
that	O
explicitly	O
use	O
the	O
order	O
.	O

We	O
maintain	O
a	O
fast	O
and	O
memory	O
efficient	O
mapping	O
of	O
the	O
n	O
-	O
grams	O
by	O
using	O
the	O
hashing	B-Method
trick	E-Method
with	O
the	O
same	O
hashing	O
function	O
as	O
in	O
mikolov2011strategies	O
and	O
10	O
M	O
bins	O
if	O
we	O
only	O
used	O
bigrams	O
,	O
and	O
100	O
M	O
otherwise	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
fastText	S-Method
on	O
two	O
different	O
tasks	O
.	O

First	O
,	O
we	O
compare	O
it	O
to	O
existing	O
text	B-Method
classifers	E-Method
on	O
the	O
problem	O
of	O
sentiment	B-Task
analysis	E-Task
.	O

Then	O
,	O
we	O
evaluate	O
its	O
capacity	O
to	O
scale	O
to	O
large	O
output	O
space	O
on	O
a	O
tag	B-Task
prediction	I-Task
dataset	E-Task
.	O

Note	O
that	O
our	O
model	O
could	O
be	O
implemented	O
with	O
the	O
Vowpal	B-Method
Wabbit	I-Method
library	E-Method
,	O
but	O
we	O
observe	O
in	O
practice	O
,	O
that	O
our	O
tailored	O
implementation	O
is	O
at	O
least	O
2	O
-	O
5	O
faster	O
.	O

subsection	O
:	O
Sentiment	B-Task
analysis	E-Task
paragraph	O
:	O
Datasets	O
and	O
baselines	O
.	O

We	O
employ	O
the	O
same	O
8	O
datasets	O
and	O
evaluation	O
protocol	O
of	O
zhang2015character	O
.	O

We	O
report	O
the	O
n	O
-	O
grams	O
and	O
TFIDF	B-Method
baselines	E-Method
from	O
zhang2015character	O
,	O
as	O
well	O
as	O
the	O
character	B-Method
level	I-Method
convolutional	I-Method
model	E-Method
(	O
char	B-Method
-	I-Method
CNN	E-Method
)	O
of	O
zhang2015text	O
,	O
the	O
character	B-Method
based	I-Method
convolution	I-Method
recurrent	I-Method
network	E-Method
(	O
char	B-Method
-	I-Method
CRNN	E-Method
)	O
of	O
and	O
the	O
very	B-Method
deep	I-Method
convolutional	I-Method
network	E-Method
(	O
VDCNN	S-Method
)	O
of	O
conneau2016	O
.	O

We	O
also	O
compare	O
to	O
tang2015document	O
following	O
their	O
evaluation	O
protocol	O
.	O

We	O
report	O
their	O
main	O
baselines	O
as	O
well	O
as	O
their	O
two	O
approaches	O
based	O
on	O
recurrent	B-Method
networks	E-Method
(	O
Conv	B-Method
-	I-Method
GRNN	I-Method
and	I-Method
LSTM	I-Method
-	I-Method
GRNN	E-Method
)	O
.	O

paragraph	O
:	O
Results	O
.	O

We	O
present	O
the	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
use	O
10	O
hidden	O
units	O
and	O
run	O
fastText	S-Method
for	O
5	O
epochs	O
with	O
a	O
learning	B-Metric
rate	E-Metric
selected	O
on	O
a	O
validation	O
set	O
from	O
0.05	O
,	O
0.1	O
,	O
0.25	O
,	O
0.5	O
.	O

On	O
this	O
task	O
,	O
adding	O
bigram	O
information	O
improves	O
the	O
performance	O
by	O
1	O
-	O
4	O
.	O

Overall	O
our	O
accuracy	S-Metric
is	O
slightly	O
better	O
than	O
char	B-Method
-	I-Method
CNN	E-Method
and	O
char	B-Method
-	I-Method
CRNN	E-Method
and	O
,	O
a	O
bit	O
worse	O
than	O
VDCNN	S-Method
.	O

Note	O
that	O
we	O
can	O
increase	O
the	O
accuracy	S-Metric
slightly	O
by	O
using	O
more	O
n	O
-	O
grams	O
,	O
for	O
example	O
with	O
trigrams	O
,	O
the	O
performance	O
on	O
Sogou	S-Method
goes	O
up	O
to	O
97.1	O
.	O

Finally	O
,	O
Figure	O
[	O
reference	O
]	O
shows	O
that	O
our	O
method	O
is	O
competitive	O
with	O
the	O
methods	O
presented	O
in	O
tang2015document	S-Method
.	O

We	O
tune	O
the	O
hyper	O
-	O
parameters	O
on	O
the	O
validation	O
set	O
and	O
observe	O
that	O
using	O
n	O
-	O
grams	O
up	O
to	O
5	O
leads	O
to	O
the	O
best	O
performance	O
.	O

Unlike	O
tang2015document	O
,	O
fastText	S-Method
does	O
not	O
use	O
pre	O
-	O
trained	O
word	O
embeddings	O
,	O
which	O
can	O
be	O
explained	O
the	O
1	O
difference	O
in	O
accuracy	S-Metric
.	O

paragraph	O
:	O
Training	B-Metric
time	E-Metric
.	O

Both	O
char	B-Method
-	I-Method
CNN	E-Method
and	O
VDCNN	S-Method
are	O
trained	O
on	O
a	O
NVIDIA	B-Method
Tesla	I-Method
K40	I-Method
GPU	E-Method
,	O
while	O
our	O
models	O
are	O
trained	O
on	O
a	O
CPU	S-Method
using	O
20	O
threads	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
methods	O
using	O
convolutions	S-Method
are	O
several	O
orders	O
of	O
magnitude	O
slower	O
than	O
fastText	S-Method
.	O

While	O
it	O
is	O
possible	O
to	O
have	O
a	O
10	O
speed	B-Metric
up	E-Metric
for	O
char	B-Method
-	I-Method
CNN	E-Method
by	O
using	O
more	O
recent	O
CUDA	B-Method
implementations	E-Method
of	O
convolutions	S-Method
,	O
fastText	S-Method
takes	O
less	O
than	O
a	O
minute	O
to	O
train	O
on	O
these	O
datasets	O
.	O

The	O
GRNNs	B-Method
method	E-Method
of	O
tang2015document	S-Method
takes	O
around	O
12	O
hours	O
per	O
epoch	O
on	O
CPU	O
with	O
a	O
single	O
thread	O
.	O

Our	O
speed	O
-	O
up	O
compared	O
to	O
neural	B-Method
network	I-Method
based	I-Method
methods	E-Method
increases	O
with	O
the	O
size	O
of	O
the	O
dataset	O
,	O
going	O
up	O
to	O
at	O
least	O
a	O
15	O
,	O
000	O
speed	O
-	O
up	O
.	O

subsection	O
:	O
Tag	B-Task
prediction	E-Task
paragraph	O
:	O
Dataset	O
and	O
baselines	O
.	O

To	O
test	O
scalability	O
of	O
our	O
approach	O
,	O
further	O
evaluation	O
is	O
carried	O
on	O
the	O
YFCC100	O
M	O
dataset	O
which	O
consists	O
of	O
almost	O
100	O
M	O
images	O
with	O
captions	O
,	O
titles	O
and	O
tags	O
.	O

We	O
focus	O
on	O
predicting	O
the	O
tags	O
according	O
to	O
the	O
title	O
and	O
caption	O
(	O
we	O
do	O
not	O
use	O
the	O
images	O
)	O
.	O

We	O
remove	O
the	O
words	O
and	O
tags	O
occurring	O
less	O
than	O
100	O
times	O
and	O
split	O
the	O
data	O
into	O
a	O
train	O
,	O
validation	O
and	O
test	O
set	O
.	O

The	O
train	O
set	O
contains	O
91	O
,	O
188	O
,	O
648	O
examples	O
(	O
1.5B	O
tokens	O
)	O
.	O

The	O
validation	O
has	O
930	O
,	O
497	O
examples	O
and	O
the	O
test	O
set	O
543	O
,	O
424	O
.	O

The	O
vocabulary	B-Metric
size	E-Metric
is	O
297	O
,	O
141	O
and	O
there	O
are	O
312	O
,	O
116	O
unique	O
tags	O
.	O

We	O
will	O
release	O
a	O
script	O
that	O
recreates	O
this	O
dataset	O
so	O
that	O
our	O
numbers	O
could	O
be	O
reproduced	O
.	O

We	O
report	O
precision	S-Metric
at	O
1	O
.	O

We	O
consider	O
a	O
frequency	B-Method
-	I-Method
based	I-Method
baseline	E-Method
which	O
predicts	O
the	O
most	O
frequent	O
tag	O
.	O

We	O
also	O
compare	O
with	O
Tagspace	S-Method
,	O
which	O
is	O
a	O
tag	B-Method
prediction	I-Method
model	E-Method
similar	O
to	O
ours	O
,	O
but	O
based	O
on	O
the	O
Wsabie	B-Method
model	E-Method
of	O
weston2011wsabie	O
.	O

While	O
the	O
Tagspace	B-Method
model	E-Method
is	O
described	O
using	O
convolutions	S-Method
,	O
we	O
consider	O
the	O
linear	B-Method
version	E-Method
,	O
which	O
achieves	O
comparable	O
performance	O
but	O
is	O
much	O
faster	O
.	O

paragraph	O
:	O
Results	O
and	O
training	O
time	O
.	O

Table	O
[	O
reference	O
]	O
presents	O
a	O
comparison	O
of	O
fastText	S-Method
and	O
the	O
baselines	O
.	O

We	O
run	O
fastText	S-Method
for	O
5	O
epochs	O
and	O
compare	O
it	O
to	O
Tagspace	O
for	O
two	O
sizes	O
of	O
the	O
hidden	O
layer	O
,	O
i.e.	O
,	O
50	O
and	O
200	O
.	O

Both	O
models	O
achieve	O
a	O
similar	O
performance	O
with	O
a	O
small	O
hidden	O
layer	O
,	O
but	O
adding	O
bigrams	S-Method
gives	O
us	O
a	O
significant	O
boost	O
in	O
accuracy	S-Metric
.	O

At	O
test	O
time	O
,	O
Tagspace	O
needs	O
to	O
compute	O
the	O
scores	O
for	O
all	O
the	O
classes	O
which	O
makes	O
it	O
relatively	O
slow	O
,	O
while	O
our	O
fast	B-Method
inference	E-Method
gives	O
a	O
significant	O
speed	O
-	O
up	O
when	O
the	O
number	O
of	O
classes	O
is	O
large	O
(	O
more	O
than	O
300	O
K	O
here	O
)	O
.	O

Overall	O
,	O
we	O
are	O
more	O
than	O
an	O
order	O
of	O
magnitude	O
faster	O
to	O
obtain	O
model	O
with	O
a	O
better	O
quality	O
.	O

The	O
speedup	O
of	O
the	O
test	B-Method
phase	E-Method
is	O
even	O
more	O
significant	O
(	O
a	O
600	O
speedup	O
)	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
some	O
qualitative	O
examples	O
.	O

section	O
:	O
Discussion	O
and	O
conclusion	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
baseline	O
method	O
for	O
text	B-Task
classification	E-Task
.	O

Unlike	O
unsupervisedly	B-Method
trained	I-Method
word	I-Method
vectors	E-Method
from	O
word2vec	S-Method
,	O
our	O
word	O
features	O
can	O
be	O
averaged	O
together	O
to	O
form	O
good	O
sentence	B-Method
representations	E-Method
.	O

In	O
several	O
tasks	O
,	O
fastText	S-Method
obtains	O
performance	O
on	O
par	O
with	O
recently	O
proposed	O
methods	O
inspired	O
by	O
deep	B-Method
learning	E-Method
,	O
while	O
being	O
much	O
faster	O
.	O

Although	O
deep	B-Method
neural	I-Method
networks	E-Method
have	O
in	O
theory	O
much	O
higher	O
representational	B-Method
power	E-Method
than	O
shallow	B-Method
models	E-Method
,	O
it	O
is	O
not	O
clear	O
if	O
simple	O
text	B-Task
classification	I-Task
problems	E-Task
such	O
as	O
sentiment	B-Task
analysis	E-Task
are	O
the	O
right	O
ones	O
to	O
evaluate	O
them	O
.	O

We	O
will	O
publish	O
our	O
code	O
so	O
that	O
the	O
research	O
community	O
can	O
easily	O
build	O
on	O
top	O
of	O
our	O
work	O
.	O

paragraph	O
:	O
Acknowledgement	O
.	O

We	O
thank	O
Gabriel	O
Synnaeve	O
,	O
Hervé	O
Gégou	O
,	O
Jason	O
Weston	O
and	O
Léon	O
Bottou	O
for	O
their	O
help	O
and	O
comments	O
.	O

We	O
also	O
thank	O
Alexis	O
Conneau	O
,	O
Duyu	O
Tang	O
and	O
Zichao	O
Zhang	O
for	O
providing	O
us	O
with	O
information	O
about	O
their	O
methods	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Networks	E-Method
Graph	O
-	O
structured	O
data	O
appears	O
frequently	O
in	O
domains	O
including	O
chemistry	O
,	O
natural	O
language	O
semantics	O
,	O
social	O
networks	O
,	O
and	O
knowledge	O
bases	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
feature	B-Method
learning	I-Method
techniques	E-Method
for	O
graph	O
-	O
structured	O
inputs	O
.	O

Our	O
starting	O
point	O
is	O
previous	O
work	O
on	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
scarselli2009graph	O
,	O
which	O
we	O
modify	O
to	O
use	O
gated	B-Method
recurrent	I-Method
units	E-Method
and	O
modern	O
optimization	B-Method
techniques	E-Method
and	O
then	O
extend	O
to	O
output	O
sequences	O
.	O

The	O
result	O
is	O
a	O
flexible	O
and	O
broadly	O
useful	O
class	O
of	O
neural	B-Method
network	I-Method
models	E-Method
that	O
has	O
favorable	O
inductive	B-Metric
biases	E-Metric
relative	O
to	O
purely	O
sequence	B-Method
-	I-Method
based	I-Method
models	E-Method
(	O
e.g.	O
,	O
LSTMs	S-Method
)	O
when	O
the	O
problem	O
is	O
graph	O
-	O
structured	O
.	O

We	O
demonstrate	O
the	O
capabilities	O
on	O
some	O
simple	O
AI	S-Task
(	O
bAbI	O
)	O
and	O
graph	O
algorithm	O
learning	O
tasks	O
.	O

We	O
then	O
show	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
problem	O
from	O
program	B-Task
verification	E-Task
,	O
in	O
which	O
subgraphs	O
need	O
to	O
be	O
described	O
as	O
abstract	O
data	O
structures	O
.	O

Much	O
data	O
in	O
the	O
real	O
world	O
is	O
naturally	O
represented	O
as	O
a	O
graph	O
.	O

For	O
example	O
,	O
entities	O
can	O
be	O
represented	O
as	O
nodes	O
,	O
with	O
relations	O
between	O
them	O
represented	O
as	O
edges	O
.	O

Graph	O
-	O
structured	O
data	O
comes	O
up	O
often	O
in	O
chemistry	O
,	O
social	O
networks	O
,	O
knowledge	O
bases	O
,	O
and	O
web	O
-	O
based	O
data	O
,	O
and	O
in	O
other	O
domains	O
like	O
program	B-Task
verification	E-Task
which	O
requires	O
reasoning	O
about	O
abstract	O
data	O
structures	O
represented	O
as	O
graphs	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
feature	B-Method
learning	I-Method
techniques	E-Method
for	O
learning	S-Task
from	O
graph	O
-	O
structured	O
inputs	O
.	O

Our	O
starting	O
point	O
is	O
previous	O
work	O
on	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
(	O
GNNs	S-Method
)	O
scarselli2009graph	O
.	O

We	O
begin	O
by	O
adapting	O
the	O
model	O
and	O
learning	B-Method
algorithm	E-Method
to	O
make	O
use	O
of	O
recent	O
advances	O
in	O
recurrent	B-Method
neural	I-Method
networks	E-Method
,	O
incorporating	O
gated	B-Method
recurrent	I-Method
units	E-Method
and	O
modern	O
optimization	B-Method
routines	E-Method
.	O

Our	O
main	O
contribution	O
is	O
then	O
to	O
extend	O
GNNs	S-Method
to	O
output	O
sequences	O
.	O

We	O
use	O
bAbI	B-Task
tasks	E-Task
to	O
illustrate	O
the	O
components	O
of	O
the	O
model	O
and	O
explain	O
what	O
is	O
being	O
learned	O
by	O
the	O
GNN	B-Method
models	E-Method
,	O
and	O
then	O
we	O
develop	O
a	O
full	O
model	O
that	O
learns	O
to	O
predict	O
loop	O
invariants	O
for	O
a	O
program	B-Task
verification	I-Task
task	E-Task
,	O
where	O
we	O
map	O
from	O
graphs	O
representing	O
the	O
state	O
of	O
memory	O
during	O
a	O
program	O
’s	O
execution	O
to	O
a	O
logical	O
description	O
of	O
the	O
data	O
structures	O
that	O
have	O
been	O
instantiated	O
.	O

We	O
show	O
that	O
we	O
are	O
able	O
to	O
match	O
the	O
performance	O
of	O
a	O
system	O
that	O
has	O
been	O
heavily	O
hand	O
-	O
engineered	O
,	O
indicating	O
the	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Networks	E-Method
are	O
promising	O
models	O
for	O
feature	B-Task
learning	I-Task
on	I-Task
graphs	E-Task
.	O

arrows	O
,	O
decorations.pathmorphing	O
,	O
backgrounds	O
,	O
fit	O
,	O
automata	O
,	O
shapes	O
,	O
calc	O
,	O
positioning	O
,	O
shadows	O
heapgraph	O
/	O
.style=	O
everynode	O
/	O
.style	O
=	O
rectangle	O
,	O
font=	O
,	O
tree	O
/	O
.appendstyle	O
=	O
draw	O
=	O
black	O
,	O
thick	O
,	O
minimumwidth=.4	O
cm	O
,	O
fill	O
=	O
blue!20	O
,	O
list	O
/	O
.appendstyle	O
=	O
draw	O
=	O
black	O
,	O
thick	O
,	O
minimumwidth=.4	O
cm	O
,	O
fill	O
=	O
green!20	O
,	O
prefixaftercommand=	O
everylabel	O
/	O
.style	O
=	O
font=	O
,	O
innersep=2pt	O
,	O
active	O
/	O
.appendstyle	O
=	O
double	O
,	O
explained	O
/	O
.appendstyle	O
=	O
fill	O
=	O
white	O
,	O
colLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
stepLabel	O
/	O
.appendstyle	O
=	O
textwidth=2	O
cm	O
,	O
align	O
=	O
left	O
,	O
font=	O
,	O
outLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
heapEdgeLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
innersep=2pt	O
,	O
,	O
section	O
:	O
Introduction	O
Many	O
practical	O
applications	O
build	O
on	O
graph	O
-	O
structured	O
data	O
,	O
and	O
thus	O
we	O
often	O
want	O
to	O
perform	O
machine	B-Task
learning	I-Task
tasks	E-Task
that	O
take	O
graphs	O
as	O
inputs	O
.	O

Standard	O
approaches	O
to	O
the	O
problem	O
include	O
engineering	O
custom	O
features	O
of	O
an	O
input	O
graph	O
,	O
graph	B-Method
kernels	E-Method
kashima2003marginalized	O
,	O
shervashidze2011weisfeiler	O
,	O
and	O
methods	O
that	O
define	O
graph	O
features	O
in	O
terms	O
of	O
random	B-Method
walks	I-Method
on	I-Method
graphs	E-Method
perozzi2014deepwalk	O
.	O

More	O
closely	O
related	O
to	O
our	O
goal	O
in	O
this	O
work	O
are	O
methods	O
that	O
learn	O
features	O
on	O
graphs	O
,	O
including	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
gori2005new	O
,	O
scarselli2009graph	O
,	O
spectral	B-Method
networks	E-Method
bruna2013spectral	O
and	O
recent	O
work	O
on	O
learning	O
graph	B-Task
fingerprints	E-Task
for	O
classification	B-Task
tasks	E-Task
on	O
graph	B-Method
representations	E-Method
of	O
chemical	O
molecules	O
duvenaud2015convolutional	O
.	O

Our	O
main	O
contribution	O
is	O
an	O
extension	O
of	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
that	O
outputs	O
sequences	O
.	O

Previous	O
work	O
on	O
feature	B-Task
learning	E-Task
for	O
graph	B-Task
-	I-Task
structured	I-Task
inputs	E-Task
has	O
focused	O
on	O
models	O
that	O
produce	O
single	O
outputs	O
such	O
as	O
graph	O
-	O
level	O
classifications	O
,	O
but	O
many	O
problems	O
with	O
graph	O
inputs	O
require	O
outputting	O
sequences	O
.	O

Examples	O
include	O
paths	O
on	O
a	O
graph	O
,	O
enumerations	O
of	O
graph	O
nodes	O
with	O
desirable	O
properties	O
,	O
or	O
sequences	O
of	O
global	O
classifications	O
mixed	O
with	O
,	O
for	O
example	O
,	O
a	O
start	O
and	O
end	O
node	O
.	O

We	O
are	O
not	O
aware	O
of	O
existing	O
graph	B-Method
feature	I-Method
learning	E-Method
work	O
suitable	O
for	O
this	O
problem	O
.	O

Our	O
motivating	O
application	O
comes	O
from	O
program	B-Task
verification	E-Task
and	O
requires	O
outputting	O
logical	O
formulas	O
,	O
which	O
we	O
formulate	O
as	O
a	O
sequential	B-Task
output	I-Task
problem	E-Task
.	O

A	O
secondary	O
contribution	O
is	O
highlighting	O
that	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
(	O
and	O
further	O
extensions	O
we	O
develop	O
here	O
)	O
are	O
a	O
broadly	O
useful	O
class	O
of	O
neural	B-Method
network	I-Method
model	E-Method
that	O
is	O
applicable	O
to	O
many	O
problems	O
currently	O
facing	O
the	O
field	O
.	O

There	O
are	O
two	O
settings	O
for	O
feature	B-Task
learning	I-Task
on	I-Task
graphs	E-Task
:	O
(	O
1	O
)	O
learning	O
a	O
representation	B-Method
of	I-Method
the	I-Method
input	I-Method
graph	E-Method
,	O
and	O
(	O
2	O
)	O
learning	O
representations	O
of	O
the	O
internal	O
state	O
during	O
the	O
process	O
of	O
producing	O
a	O
sequence	O
of	O
outputs	O
.	O

Here	O
,	O
(	O
1	O
)	O
is	O
mostly	O
achieved	O
by	O
previous	O
work	O
on	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
scarselli2009graph	O
;	O
we	O
make	O
several	O
minor	O
adaptations	O
of	O
this	O
framework	O
,	O
including	O
changing	O
it	O
to	O
use	O
modern	O
practices	O
around	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
.	O

(	O
2	O
)	O
is	O
important	O
because	O
we	O
desire	O
outputs	O
from	O
graph	B-Task
-	I-Task
structured	I-Task
problems	E-Task
that	O
are	O
not	O
solely	O
individual	O
classifications	O
.	O

In	O
these	O
cases	O
,	O
the	O
challenge	O
is	O
how	O
to	O
learn	O
features	O
on	O
the	O
graph	O
that	O
encode	O
the	O
partial	O
output	O
sequence	O
that	O
has	O
already	O
been	O
produced	O
(	O
e.g.	O
,	O
the	O
path	O
so	O
far	O
if	O
outputting	O
a	O
path	O
)	O
and	O
that	O
still	O
needs	O
to	O
be	O
produced	O
(	O
e.g.	O
,	O
the	O
remaining	O
path	O
)	O
.	O

We	O
will	O
show	O
how	O
the	O
GNN	B-Method
framework	E-Method
can	O
be	O
adapted	O
to	O
these	O
settings	O
,	O
leading	O
to	O
a	O
novel	O
graph	B-Method
-	I-Method
based	I-Method
neural	I-Method
network	I-Method
model	E-Method
that	O
we	O
call	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Networks	E-Method
(	O
GGS	B-Method
-	I-Method
NNs	E-Method
)	O
.	O

We	O
illustrate	O
aspects	O
of	O
this	O
general	O
model	O
in	O
experiments	O
on	O
bAbI	B-Task
tasks	E-Task
weston2015towards	O
and	O
graph	B-Task
algorithm	I-Task
learning	I-Task
tasks	E-Task
that	O
illustrate	O
the	O
capabilities	O
of	O
the	O
model	O
.	O

We	O
then	O
present	O
an	O
application	O
to	O
the	O
verification	B-Task
of	I-Task
computer	I-Task
programs	E-Task
.	O

When	O
attempting	O
to	O
prove	O
properties	O
such	O
as	O
memory	B-Task
safety	E-Task
(	O
i.e.	O
,	O
that	O
there	O
are	O
no	O
null	O
pointer	O
dereferences	O
in	O
a	O
program	O
)	O
,	O
a	O
core	O
problem	O
is	O
to	O
find	O
mathematical	O
descriptions	O
of	O
the	O
data	O
structures	O
used	O
in	O
a	O
program	O
.	O

Following	O
,	O
we	O
have	O
phrased	O
this	O
as	O
a	O
machine	B-Task
learning	I-Task
problem	E-Task
where	O
we	O
will	O
learn	O
to	O
map	O
from	O
a	O
set	O
of	O
input	O
graphs	O
,	O
representing	O
the	O
state	O
of	O
memory	O
,	O
to	O
a	O
logical	O
description	O
of	O
the	O
data	O
structures	O
that	O
have	O
been	O
instantiated	O
.	O

Whereas	O
relied	O
on	O
a	O
large	O
amount	O
of	O
hand	O
-	O
engineering	O
of	O
features	O
,	O
we	O
show	O
that	O
the	O
system	O
can	O
be	O
replaced	O
with	O
a	O
GGS	B-Method
-	I-Method
NN	E-Method
at	O
no	O
cost	O
in	O
accuracy	S-Metric
.	O

section	O
:	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
In	O
this	O
section	O
,	O
we	O
review	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
(	O
GNNs	S-Method
)	O
gori2005new	O
,	O
scarselli2009graph	O
and	O
introduce	O
notation	O
and	O
concepts	O
that	O
will	O
be	O
used	O
throughout	O
.	O

GNNs	S-Method
are	O
a	O
general	B-Method
neural	I-Method
network	I-Method
architecture	E-Method
defined	O
according	O
to	O
a	O
graph	O
structure	O
.	O

Nodes	O
take	O
unique	O
values	O
from	O
,	O
and	O
edges	O
are	O
pairs	O
.	O

We	O
will	O
focus	O
in	O
this	O
work	O
on	O
directed	O
graphs	O
,	O
so	O
represents	O
a	O
directed	O
edge	O
,	O
but	O
we	O
note	O
that	O
the	O
framework	O
can	O
easily	O
be	O
adapted	O
to	O
undirected	O
graphs	O
;	O
see	O
.	O

The	O
node	O
vector	O
(	O
or	O
node	B-Method
representation	E-Method
or	O
node	B-Method
embedding	E-Method
)	O
for	O
node	O
is	O
denoted	O
by	O
.	O

Graphs	O
may	O
also	O
contain	O
node	O
labels	O
for	O
each	O
node	O
and	O
edge	O
labels	O
or	O
edge	O
types	O
for	O
each	O
edge	O
.	O

We	O
will	O
overload	O
notation	O
and	O
let	O
when	O
is	O
a	O
set	O
of	O
nodes	O
,	O
and	O
when	O
is	O
a	O
set	O
of	O
edges	O
.	O

The	O
function	O
returns	O
the	O
set	O
of	O
predecessor	O
nodes	O
with	O
.	O

Analogously	O
,	O
is	O
the	O
set	O
of	O
successor	O
nodes	O
with	O
edges	O
.	O

The	O
set	O
of	O
all	O
nodes	O
neighboring	O
is	O
,	O
and	O
the	O
set	O
of	O
all	O
edges	O
incoming	O
to	O
or	O
outgoing	O
from	O
is	O
.	O

GNNs	S-Method
map	O
graphs	O
to	O
outputs	O
via	O
two	O
steps	O
.	O

First	O
,	O
there	O
is	O
a	O
propagation	B-Method
step	E-Method
that	O
computes	O
node	B-Method
representations	E-Method
for	O
each	O
node	O
;	O
second	O
,	O
an	O
output	B-Method
model	E-Method
maps	O
from	O
node	O
representations	O
and	O
corresponding	O
labels	O
to	O
an	O
output	O
for	O
each	O
.	O

In	O
the	O
notation	O
for	O
,	O
we	O
leave	O
the	O
dependence	O
on	O
parameters	O
implicit	O
,	O
and	O
we	O
will	O
continue	O
to	O
do	O
this	O
throughout	O
.	O

The	O
system	O
is	O
differentiable	O
from	O
end	O
-	O
to	O
-	O
end	O
,	O
so	O
all	O
parameters	O
are	O
learned	O
jointly	O
using	O
gradient	B-Method
-	I-Method
based	I-Method
optimization	E-Method
.	O

subsection	O
:	O
Propagation	B-Method
Model	E-Method
Here	O
,	O
an	O
iterative	B-Method
procedure	E-Method
propagates	O
node	B-Method
representations	E-Method
.	O

Initial	O
node	B-Method
representations	E-Method
are	O
set	O
to	O
arbitrary	O
values	O
,	O
then	O
each	O
node	B-Method
representation	E-Method
is	O
updated	O
following	O
the	O
recurrence	O
below	O
until	O
convergence	O
,	O
where	O
denotes	O
the	O
timestep	O
:	O
Several	O
variants	O
are	O
discussed	O
in	O
including	O
positional	O
graph	O
forms	O
,	O
node	O
-	O
specific	O
updates	O
,	O
and	O
alternative	O
representations	B-Method
of	I-Method
neighborhoods	E-Method
.	O

Concretely	O
,	O
suggest	O
decomposing	O
to	O
be	O
a	O
sum	B-Method
of	I-Method
per	I-Method
-	I-Method
edge	I-Method
terms	E-Method
:	O
where	O
is	O
either	O
a	O
linear	O
function	O
of	O
or	O
a	O
neural	B-Method
network	E-Method
.	O

The	O
parameters	O
of	O
depends	O
on	O
the	O
configuration	O
of	O
labels	O
,	O
e.g.	O
in	O
the	O
following	O
linear	O
case	O
,	O
and	O
are	O
learnable	O
parameters	O
,	O
subsection	O
:	O
Output	B-Method
Model	E-Method
and	O
Learning	S-Task
The	O
output	B-Method
model	E-Method
is	O
defined	O
per	O
node	O
and	O
is	O
a	O
differentiable	O
function	O
that	O
maps	O
to	O
an	O
output	O
.	O

This	O
is	O
generally	O
a	O
linear	B-Method
or	I-Method
neural	I-Method
network	I-Method
mapping	E-Method
.	O

focus	O
on	O
outputs	O
that	O
are	O
independent	O
per	O
node	O
,	O
which	O
are	O
implemented	O
by	O
mapping	O
the	O
final	O
node	O
representations	O
,	O
to	O
an	O
output	O
for	O
each	O
node	O
.	O

To	O
handle	O
graph	B-Task
-	I-Task
level	I-Task
classifications	E-Task
,	O
they	O
suggest	O
to	O
create	O
a	O
dummy	O
“	O
super	O
node	O
”	O
that	O
is	O
connected	O
to	O
all	O
other	O
nodes	O
by	O
a	O
special	O
type	O
of	O
edge	O
.	O

Thus	O
,	O
graph	B-Task
-	I-Task
level	I-Task
regression	E-Task
or	O
classification	S-Task
can	O
be	O
handled	O
in	O
the	O
same	O
manner	O
as	O
node	B-Method
-	I-Method
level	I-Method
regression	E-Method
or	O
classification	S-Task
.	O

Learning	S-Method
is	O
done	O
via	O
the	O
Almeida	B-Method
-	I-Method
Pineda	I-Method
algorithm	I-Method
almeida1990learning	E-Method
,	O
pineda1987generalization	S-Method
,	O
which	O
works	O
by	O
running	O
the	O
propagation	S-Method
to	O
convergence	O
,	O
and	O
then	O
computing	O
gradients	O
based	O
upon	O
the	O
converged	O
solution	O
.	O

This	O
has	O
the	O
advantage	O
of	O
not	O
needing	O
to	O
store	O
intermediate	O
states	O
in	O
order	O
to	O
compute	O
gradients	O
.	O

The	O
disadvantage	O
is	O
that	O
parameters	O
must	O
be	O
constrained	O
so	O
that	O
the	O
propagation	O
step	O
is	O
a	O
contraction	B-Method
map	E-Method
.	O

This	O
is	O
needed	O
to	O
ensure	O
convergence	O
,	O
but	O
it	O
may	O
limit	O
the	O
expressivity	O
of	O
the	O
model	O
.	O

When	O
is	O
a	O
neural	B-Method
network	E-Method
,	O
this	O
is	O
encouraged	O
using	O
a	O
penalty	O
term	O
on	O
the	O
1	B-Method
-	I-Method
norm	I-Method
of	I-Method
the	I-Method
network	E-Method
’s	O
Jacobian	O
.	O

See	O
Appendix	O
[	O
reference	O
]	O
for	O
an	O
example	O
that	O
gives	O
the	O
intuition	O
that	O
contraction	O
maps	O
have	O
trouble	O
propagating	O
information	O
across	O
a	O
long	O
range	O
in	O
a	O
graph	O
.	O

section	O
:	O
Gated	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
We	O
now	O
describe	O
Gated	O
Graph	B-Method
Neural	I-Method
Networks	E-Method
(	O
GG	B-Method
-	I-Method
NNs	E-Method
)	O
,	O
our	O
adaptation	B-Method
of	I-Method
GNNs	E-Method
that	O
is	O
suitable	O
for	O
non	O
-	O
sequential	O
outputs	O
.	O

We	O
will	O
describe	O
sequential	O
outputs	O
in	O
the	O
next	O
section	O
.	O

The	O
biggest	O
modification	O
of	O
GNNs	S-Method
is	O
that	O
we	O
use	O
Gated	B-Method
Recurrent	I-Method
Units	E-Method
cho2014learning	O
and	O
unroll	O
the	O
recurrence	O
for	O
a	O
fixed	O
number	O
of	O
steps	O
and	O
use	O
backpropagation	O
through	O
time	O
in	O
order	O
to	O
compute	O
gradients	O
.	O

This	O
requires	O
more	O
memory	O
than	O
the	O
Almeida	B-Method
-	I-Method
Pineda	I-Method
algorithm	E-Method
,	O
but	O
it	O
removes	O
the	O
need	O
to	O
constrain	O
parameters	O
to	O
ensure	O
convergence	O
.	O

We	O
also	O
extend	O
the	O
underlying	O
representations	S-Method
and	O
output	B-Method
model	E-Method
.	O

subsection	O
:	O
Node	O
Annotations	O
In	O
GNNs	S-Method
,	O
there	O
is	O
no	O
point	O
in	O
initializing	O
node	O
representations	O
because	O
the	O
contraction	O
map	O
constraint	O
ensures	O
that	O
the	O
fixed	O
point	O
is	O
independent	O
of	O
the	O
initializations	O
.	O

This	O
is	O
no	O
longer	O
the	O
case	O
with	O
GG	B-Method
-	I-Method
NNs	E-Method
,	O
which	O
lets	O
us	O
incorporate	O
node	O
labels	O
as	O
additional	O
inputs	O
.	O

To	O
distinguish	O
these	O
node	O
labels	O
used	O
as	O
inputs	O
from	O
the	O
ones	O
introduced	O
before	O
,	O
we	O
call	O
them	O
node	O
annotations	O
,	O
and	O
use	O
vector	O
to	O
denote	O
these	O
annotations	O
.	O

To	O
illustrate	O
how	O
the	O
node	O
annotations	O
are	O
used	O
,	O
consider	O
an	O
example	O
task	O
of	O
training	O
a	O
graph	B-Method
neural	I-Method
network	E-Method
to	O
predict	O
whether	O
node	O
can	O
be	O
reached	O
from	O
node	O
on	O
a	O
given	O
graph	O
.	O

For	O
this	O
task	O
,	O
there	O
are	O
two	O
problem	O
-	O
related	O
special	O
nodes	O
,	O
and	O
.	O

To	O
mark	O
these	O
nodes	O
as	O
special	O
,	O
we	O
give	O
them	O
an	O
initial	O
annotation	O
.	O

The	O
first	O
node	O
gets	O
the	O
annotation	O
,	O
and	O
the	O
second	O
node	O
gets	O
the	O
annotation	O
.	O

All	O
other	O
nodes	O
have	O
their	O
initial	O
annotation	O
set	O
to	O
.	O

Intuitively	O
,	O
this	O
marks	O
as	O
the	O
first	O
input	O
argument	O
and	O
as	O
the	O
second	O
input	O
argument	O
.	O

We	O
then	O
initialize	O
the	O
node	O
state	O
vectors	O
using	O
these	O
label	O
vectors	O
by	O
copying	O
into	O
the	O
first	O
dimensions	O
and	O
padding	O
with	O
extra	O
0	O
’s	O
to	O
allow	O
hidden	O
states	O
that	O
are	O
larger	O
than	O
the	O
annotation	O
size	O
.	O

In	O
the	O
reachability	O
example	O
,	O
it	O
is	O
easy	O
for	O
the	O
propagation	B-Method
model	E-Method
to	O
learn	O
to	O
propagate	O
the	O
node	O
annotation	O
for	O
to	O
all	O
nodes	O
reachable	O
from	O
,	O
for	O
example	O
by	O
setting	O
the	O
propagation	O
matrix	O
associated	O
with	O
forward	O
edges	O
to	O
have	O
a	O
1	O
in	O
position	O
(	O
0	O
,	O
0	O
)	O
.	O

This	O
will	O
cause	O
the	O
first	O
dimension	O
of	O
node	B-Method
representation	E-Method
to	O
be	O
copied	O
along	O
forward	O
edges	O
.	O

With	O
this	O
setting	O
of	O
parameters	O
,	O
the	O
propagation	B-Method
step	E-Method
will	O
cause	O
all	O
nodes	O
reachable	O
from	O
to	O
have	O
their	O
first	O
bit	O
of	O
node	O
representation	O
set	O
to	O
1	O
.	O

The	O
output	O
step	O
classifier	S-Method
can	O
then	O
easily	O
tell	O
whether	O
node	O
is	O
reachable	O
from	O
by	O
looking	O
whether	O
some	O
node	O
has	O
nonzero	O
entries	O
in	O
the	O
first	O
two	O
dimensions	O
of	O
its	O
representation	O
vector	O
.	O

subsection	O
:	O
Propagation	B-Method
Model	E-Method
The	O
basic	O
recurrence	O
of	O
the	O
propagation	B-Method
model	E-Method
is	O
The	O
matrix	O
determines	O
how	O
nodes	O
in	O
the	O
graph	O
communicate	O
with	O
each	O
other	O
.	O

The	O
sparsity	O
structure	O
and	O
parameter	O
tying	O
in	O
is	O
illustrated	O
in	O
fig	O
:	O
graphs	O
-	O
and	O
-	O
sparsity	O
.	O

The	O
sparsity	O
structure	O
corresponds	O
to	O
the	O
edges	O
of	O
the	O
graph	O
,	O
and	O
the	O
parameters	O
in	O
each	O
submatrix	O
are	O
determined	O
by	O
the	O
edge	O
type	O
and	O
direction	O
.	O

are	O
the	O
two	O
columns	O
of	O
blocks	O
in	O
and	O
corresponding	O
to	O
node	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
the	O
initialization	B-Method
step	E-Method
,	O
which	O
copies	O
node	O
annotations	O
into	O
the	O
first	O
components	O
of	O
the	O
hidden	O
state	O
and	O
pads	O
the	O
rest	O
with	O
zeros	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
the	O
step	O
that	O
passes	O
information	O
between	O
different	O
nodes	O
of	O
the	O
graph	O
via	O
incoming	O
and	O
outgoing	O
edges	O
with	O
parameters	O
dependent	O
on	O
the	O
edge	O
type	O
and	O
direction	O
.	O

contains	O
activations	O
from	O
edges	O
in	O
both	O
directions	O
.	O

The	O
remaining	O
are	O
GRU	B-Method
-	I-Method
like	I-Method
updates	E-Method
that	O
incorporate	O
information	O
from	O
the	O
other	O
nodes	O
and	O
from	O
the	O
previous	O
timestep	O
to	O
update	O
each	O
node	O
’s	O
hidden	O
state	O
.	O

and	O
are	O
the	O
update	O
and	O
reset	O
gates	O
,	O
is	O
the	O
logistic	B-Method
sigmoid	I-Method
function	E-Method
,	O
and	O
is	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
.	O

We	O
initially	O
experimented	O
with	O
a	O
vanilla	B-Method
recurrent	I-Method
neural	I-Method
network	I-Method
-	I-Method
style	I-Method
update	E-Method
,	O
but	O
in	O
preliminary	O
experiments	O
we	O
found	O
this	O
GRU	B-Method
-	I-Method
like	I-Method
propagation	I-Method
step	E-Method
to	O
be	O
more	O
effective	O
.	O

subsection	O
:	O
Output	B-Method
Models	E-Method
There	O
are	O
several	O
types	O
of	O
one	O
-	O
step	O
outputs	O
that	O
we	O
would	O
like	O
to	O
produce	O
in	O
different	O
situations	O
.	O

First	O
,	O
GG	B-Method
-	I-Method
NNs	E-Method
support	O
node	B-Task
selection	I-Task
tasks	E-Task
by	O
making	O
for	O
each	O
node	O
output	O
node	O
scores	O
and	O
applying	O
a	O
softmax	O
over	O
node	O
scores	O
.	O

Second	O
,	O
for	O
graph	O
-	O
level	O
outputs	O
,	O
we	O
define	O
a	O
graph	B-Method
level	I-Method
representation	I-Method
vector	E-Method
as	O
where	O
acts	O
as	O
a	O
soft	B-Method
attention	I-Method
mechanism	E-Method
that	O
decides	O
which	O
nodes	O
are	O
relevant	O
to	O
the	O
current	O
graph	B-Task
-	I-Task
level	I-Task
task	E-Task
.	O

and	O
are	O
neural	B-Method
networks	E-Method
that	O
take	O
the	O
concatenation	O
of	O
and	O
as	O
input	O
and	O
outputs	O
real	O
-	O
valued	O
vectors	O
.	O

The	O
functions	O
can	O
also	O
be	O
replaced	O
with	O
the	O
identity	O
.	O

section	O
:	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Networks	E-Method
Here	O
we	O
describe	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Networks	E-Method
(	O
GGS	B-Method
-	I-Method
NNs	E-Method
)	O
,	O
in	O
which	O
several	O
GG	B-Method
-	I-Method
NNs	E-Method
operate	O
in	O
sequence	O
to	O
produce	O
an	O
output	O
sequence	O
.	O

For	O
the	O
output	O
step	O
,	O
we	O
denote	O
the	O
matrix	O
of	O
node	O
annotations	O
as	O
.	O

We	O
use	O
two	O
GG	B-Method
-	I-Method
NNs	E-Method
k	O
and	O
k	O
:	O
k	O
for	O
predicting	B-Task
from	E-Task
,	O
and	O
k	O
for	O
predicting	O
from	O
.	O

can	O
be	O
seen	O
as	O
the	O
states	O
carried	O
over	O
from	O
step	O
to	O
.	O

Both	O
k	O
and	O
k	O
contain	O
a	O
propagation	B-Method
model	E-Method
and	O
an	O
output	B-Method
model	E-Method
.	O

In	O
the	O
propagation	B-Method
models	E-Method
,	O
we	O
denote	O
the	O
matrix	O
of	O
node	O
vectors	O
at	O
the	O
propagation	O
step	O
of	O
the	O
output	O
step	O
as	O
.	O

As	O
before	O
,	O
in	O
step	O
,	O
we	O
set	O
by	O
-	O
extending	O
per	O
node	O
.	O

An	O
overview	O
of	O
the	O
model	O
is	O
shown	O
in	O
fig	O
:	O
seq	B-Method
-	I-Method
architecture2	E-Method
.	O

Alternatively	O
,	O
and	O
can	O
share	O
a	O
single	O
propagation	B-Method
model	E-Method
,	O
and	O
just	O
have	O
separate	O
output	B-Method
models	E-Method
.	O

This	O
simpler	O
variant	O
is	O
faster	O
to	O
train	O
and	O
evaluate	O
,	O
and	O
in	O
many	O
cases	O
can	O
achieve	O
similar	O
performance	O
level	O
as	O
the	O
full	B-Method
model	E-Method
.	O

But	O
in	O
cases	O
where	O
the	O
desired	O
propagation	O
behavior	O
for	O
and	O
are	O
different	O
,	O
this	O
variant	O
may	O
not	O
work	O
as	O
well	O
.	O

[	O
hstate	O
/	O
.style	O
=	O
draw	O
,	O
rectangle	O
,	O
rounded	O
corners	O
,	O
inner	O
sep=2pt	O
,	O
font=	O
,	O
updateLabel	O
/	O
.style	O
=	O
sloped	O
,	O
font=	O
]	O
[	O
hstate	O
]	O
(	O
L1	O
)	O
at	O
(	O
0	O
,	O
0	O
)	O
;	O
[	O
hstate	O
]	O
(	O
H11	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
O1	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
L2	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
H21	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
O2	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
L3	O
)	O
at	O
(	O
)	O
;	O
[	O
]	O
(	O
cnt	O
)	O
at	O
(	O
)	O
;	O
above	O
,	O
updateLabel	O
]	O
Init	O
(	O
H11	O
)	O
(	O
H11	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
1	O
(	O
O1	O
)	O
(	O
H11	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
1	O
(	O
L2	O
)	O
(	O
L2	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
Init	O
(	O
H21	O
)	O
(	O
H21	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
2	O
(	O
O2	O
)	O
(	O
H21	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
2	O
(	O
L3	O
)	O
(	O
L3	O
)	O
edge	O
(	O
cnt	O
)	O
;	O
We	O
introduce	O
a	O
node	B-Method
annotation	I-Method
output	I-Method
model	E-Method
for	O
predicting	S-Task
from	O
.	O

The	O
prediction	S-Task
is	O
done	O
for	O
each	O
node	O
independently	O
using	O
a	O
neural	B-Method
network	E-Method
that	O
takes	O
the	O
concatenation	O
of	O
and	O
as	O
input	O
and	O
outputs	O
a	O
vector	O
of	O
real	O
-	O
valued	O
scores	O
:	O
There	O
are	O
two	O
settings	O
for	O
training	O
GGS	B-Method
-	I-Method
NNs	E-Method
:	O
specifying	O
all	O
intermediate	O
annotations	O
,	O
or	O
training	O
the	O
full	B-Method
model	E-Method
end	O
-	O
to	O
-	O
end	O
given	O
only	O
,	O
graphs	O
and	O
target	O
sequences	O
.	O

The	O
former	O
can	O
improve	O
performance	O
when	O
we	O
have	O
domain	O
knowledge	O
about	O
specific	O
intermediate	O
information	O
that	O
should	O
be	O
represented	O
in	O
the	O
internal	O
state	O
of	O
nodes	O
,	O
while	O
the	O
latter	O
is	O
more	O
general	O
.	O

We	O
describe	O
both	O
.	O

paragraph	O
:	O
Sequence	O
outputs	O
with	O
observed	O
annotations	O
Consider	O
the	O
task	O
of	O
making	O
a	O
sequence	B-Task
of	I-Task
predictions	E-Task
for	O
a	O
graph	O
,	O
where	O
each	O
prediction	O
is	O
only	O
about	O
a	O
part	O
of	O
the	O
graph	O
.	O

In	O
order	O
to	O
ensure	O
we	O
predict	O
an	O
output	O
for	O
each	O
part	O
of	O
the	O
graph	O
exactly	O
once	O
,	O
it	O
suffices	O
to	O
have	O
one	O
bit	O
per	O
node	O
,	O
indicating	O
whether	O
the	O
node	O
has	O
been	O
“	O
explained	O
”	O
so	O
far	O
.	O

In	O
some	O
settings	O
,	O
a	O
small	O
number	O
of	O
annotations	O
are	O
sufficient	O
to	O
capture	O
the	O
state	O
of	O
the	O
output	O
procedure	O
.	O

When	O
this	O
is	O
the	O
case	O
,	O
we	O
may	O
want	O
to	O
directly	O
input	O
this	O
information	O
into	O
the	O
model	O
via	O
labels	O
indicating	O
target	O
intermediate	O
annotations	O
.	O

In	O
some	O
cases	O
,	O
these	O
annotations	O
may	O
be	O
sufficient	O
,	O
in	O
that	O
we	O
can	O
define	O
a	O
model	O
where	O
the	O
GG	B-Method
-	I-Method
NNs	E-Method
are	O
rendered	O
conditionally	O
independent	O
given	O
the	O
annotations	O
.	O

In	O
this	O
case	O
,	O
at	O
training	O
time	O
,	O
given	O
the	O
annotations	O
the	O
sequence	B-Task
prediction	I-Task
task	E-Task
decomposes	O
into	O
single	B-Task
step	I-Task
prediction	I-Task
tasks	E-Task
and	O
can	O
be	O
trained	O
as	O
separate	O
GG	B-Method
-	I-Method
NNs	E-Method
.	O

At	O
test	O
time	O
,	O
predicted	O
annotations	O
from	O
one	O
step	O
will	O
be	O
used	O
as	O
input	O
to	O
the	O
next	O
step	O
.	O

This	O
is	O
analogous	O
to	O
training	O
directed	B-Method
graphical	I-Method
models	E-Method
when	O
data	O
is	O
fully	O
observed	O
.	O

paragraph	O
:	O
Sequence	O
outputs	O
with	O
latent	O
annotations	O
More	O
generally	O
,	O
when	O
intermediate	O
node	O
annotations	O
are	O
not	O
available	O
during	O
training	O
,	O
we	O
treat	O
them	O
as	O
hidden	O
units	O
in	O
the	O
network	O
,	O
and	O
train	O
the	O
whole	O
model	O
jointly	O
by	O
backpropagating	O
through	O
the	O
whole	O
sequence	O
.	O

section	O
:	O
Explanatory	B-Task
Applications	E-Task
In	O
this	O
section	O
we	O
present	O
example	O
applications	O
that	O
concretely	O
illustrate	O
the	O
use	O
of	O
GGS	B-Method
-	I-Method
NNs	E-Method
.	O

We	O
focus	O
on	O
a	O
selection	O
of	O
bAbI	B-Task
artificial	I-Task
intelligence	E-Task
(	O
AI	S-Task
)	O
tasks	O
weston2015towards	O
and	O
two	O
graph	B-Method
algorithm	I-Method
learning	I-Method
tasks	E-Method
.	O

subsection	O
:	O
bAbI	B-Task
Tasks	E-Task
The	O
bAbI	B-Task
tasks	E-Task
are	O
meant	O
to	O
test	O
reasoning	B-Task
capabilities	E-Task
that	O
AI	S-Task
systems	O
should	O
be	O
capable	O
of	O
.	O

In	O
the	O
bAbI	O
suite	O
,	O
there	O
are	O
20	O
tasks	O
that	O
test	O
basic	O
forms	O
of	O
reasoning	S-Task
like	O
deduction	S-Task
,	O
induction	S-Task
,	O
counting	S-Task
,	O
and	O
path	B-Task
-	I-Task
finding	E-Task
.	O

We	O
have	O
defined	O
a	O
basic	O
transformation	B-Method
procedure	E-Method
that	O
maps	O
bAbI	B-Task
tasks	E-Task
to	O
GG	B-Method
-	I-Method
NNs	E-Method
or	O
GGS	B-Method
-	I-Method
NNs	E-Method
.	O

We	O
use	O
the	O
--	O
symbolic	O
option	O
from	O
the	O
released	O
bAbI	O
code	O
to	O
get	O
stories	O
that	O
just	O
involve	O
sequences	O
of	O
relations	O
between	O
entities	O
,	O
which	O
are	O
then	O
converted	O
into	O
a	O
graph	O
.	O

Each	O
entity	O
is	O
mapped	O
to	O
a	O
node	O
,	O
and	O
each	O
relation	O
is	O
mapped	O
to	O
an	O
edge	O
with	O
edge	O
label	O
given	O
by	O
the	O
relation	O
.	O

The	O
full	O
story	O
is	O
consumed	O
and	O
mapped	O
to	O
a	O
single	O
graph	O
.	O

Questions	O
are	O
marked	O
by	O
eval	O
in	O
the	O
data	O
and	O
are	O
comprised	O
of	O
a	O
question	O
type	O
(	O
e.g.	O
,	O
has_fear	O
)	O
,	O
and	O
some	O
argument	O
(	O
e.g.	O
,	O
one	O
or	O
more	O
nodes	O
)	O
.	O

The	O
arguments	O
are	O
converted	O
into	O
initial	O
node	O
annotations	O
,	O
with	O
the	O
-	O
th	O
bit	O
of	O
the	O
-	O
th	O
argument	O
node	O
’s	O
annotation	O
vector	O
set	O
to	O
1	O
.	O

For	O
example	O
,	O
if	O
the	O
eval	O
line	O
is	O
eval	O
E	O
>	O
A	O
true	O
,	O
then	O
E	O
gets	O
initial	O
annotation	O
,	O
A	O
gets	O
,	O
and	O
for	O
all	O
other	O
nodes	O
,	O
.	O

Question	O
type	O
is	O
1	O
(	O
for	O
‘	O
>	O
’	O
)	O
and	O
output	O
is	O
class	O
1	O
(	O
for	O
‘	O
true	O
’	O
)	O
.	O

Some	O
tasks	O
have	O
multiple	O
question	O
types	O
,	O
for	O
example	O
Task	O
4	O
which	O
has	O
4	O
question	O
types	O
:	O
e	O
,	O
s	O
,	O
w	O
,	O
n	O
.	O

For	O
such	O
tasks	O
we	O
simply	O
train	O
a	O
separate	O
GG	B-Method
-	I-Method
NN	E-Method
for	O
each	O
task	O
.	O

We	O
do	O
not	O
use	O
the	O
strong	O
supervision	O
labels	O
or	O
give	O
the	O
GGS	B-Method
-	I-Method
NNs	E-Method
any	O
intermediate	O
annotations	O
in	O
any	O
experiments	O
.	O

While	O
simple	O
,	O
this	O
transformation	O
does	O
not	O
preserve	O
all	O
information	O
about	O
the	O
story	O
(	O
e.g.	O
,	O
it	O
discards	O
temporal	O
order	O
of	O
the	O
inputs	O
)	O
,	O
and	O
it	O
does	O
not	O
easily	O
handle	O
ternary	O
and	O
higher	O
order	O
relations	O
(	O
e.g.	O
,	O
Yesterday	O
John	O
went	O
to	O
the	O
garden	O
is	O
not	O
easily	O
mapped	O
to	O
a	O
simple	O
edge	O
)	O
.	O

We	O
also	O
emphasize	O
that	O
it	O
is	O
a	O
non	O
-	O
trivial	O
task	O
to	O
map	O
general	O
natural	O
language	O
to	O
symbolic	O
form	O
,	O
so	O
we	O
could	O
not	O
directly	O
apply	O
this	O
approach	O
to	O
arbitrary	O
natural	O
language	O
.	O

Relaxing	O
these	O
restrictions	O
is	O
left	O
for	O
future	O
work	O
.	O

However	O
,	O
even	O
with	O
this	O
simple	O
transformation	O
,	O
there	O
are	O
a	O
variety	O
of	O
bAbI	B-Task
tasks	E-Task
that	O
can	O
be	O
formulated	O
,	O
including	O
Task	O
19	O
(	O
Path	B-Task
Finding	E-Task
)	O
,	O
which	O
is	O
arguably	O
the	O
hardest	O
task	O
.	O

We	O
provide	O
baselines	O
to	O
show	O
that	O
the	O
symbolic	B-Method
representation	E-Method
does	O
not	O
help	O
RNNs	S-Method
or	O
LSTMs	S-Method
significantly	O
,	O
and	O
show	O
that	O
GGS	B-Method
-	I-Method
NNs	E-Method
solve	O
the	O
problem	O
with	O
a	O
small	O
number	O
of	O
training	O
instances	O
.	O

We	O
also	O
develop	O
two	O
new	O
bAbI	B-Task
-	I-Task
like	I-Task
tasks	E-Task
that	O
involve	O
outputting	O
sequences	O
on	O
graphs	O
:	O
shortest	O
paths	O
,	O
and	O
a	O
simple	O
form	O
of	O
Eulerian	B-Method
circuits	E-Method
(	O
on	O
random	O
connected	O
2	O
-	O
regular	O
graphs	O
)	O
.	O

The	O
point	O
of	O
these	O
experiments	O
is	O
to	O
illustrate	O
the	O
capabilities	O
of	O
GGS	B-Method
-	I-Method
NNs	E-Method
across	O
a	O
variety	O
of	O
problems	O
.	O

paragraph	O
:	O
Example	O
1	O
.	O

As	O
an	O
example	O
,	O
below	O
is	O
an	O
instance	O
from	O
the	O
symbolic	O
dataset	O
for	O
bAbI	B-Task
task	E-Task
15	O
,	O
Basic	B-Task
Deduction	E-Task
.	O

Here	O
the	O
first	O
8	O
lines	O
describe	O
the	O
facts	O
,	O
the	O
GG	B-Method
-	I-Method
NN	E-Method
will	O
use	O
these	O
facts	O
to	O
build	O
a	O
graph	O
.	O

Capital	O
letters	O
are	O
nodes	O
,	O
is	O
and	O
has_fear	O
are	O
interpreted	O
as	O
edge	O
labels	O
or	O
edge	O
types	O
.	O

The	O
last	O
4	O
lines	O
are	O
4	O
questions	O
asked	O
for	O
this	O
input	O
data	O
.	O

has_fear	O
in	O
these	O
lines	O
are	O
interpreted	O
as	O
a	O
question	O
type	O
.	O

For	O
this	O
task	O
,	O
in	O
each	O
question	O
only	O
one	O
node	O
is	O
special	O
,	O
e.g.	O
the	O
B	O
in	O
eval	O
B	O
has_fear	O
,	O
and	O
we	O
assign	O
a	O
single	O
value	O
1	O
to	O
the	O
annotation	O
vector	O
for	O
this	O
special	O
node	O
and	O
0	O
to	O
all	O
the	O
other	O
nodes	O
.	O

For	O
RNN	S-Method
and	O
LSTM	S-Method
the	O
data	O
is	O
converted	O
into	O
token	O
sequences	O
like	O
below	O
:	O
n6	O
e1	O
n1	O
eol	O
n6	O
e1	O
n5	O
eol	O
n1	O
e1	O
n2	O
eol	O
n4	O
e1	O
n5	O
eol	O
n3	O
e1	O
n4	O
eol	O
n3	O
e1	O
n5	O
eol	O
n6	O
e1	O
n4	O
eol	O
q1	O
n6	O
n2	O
ans	O
1	O
where	O
n	O
<	O
i	O
d	O
>	O
are	O
nodes	O
,	O
e	O
<	O
i	O
d	O
>	O
are	O
edges	O
,	O
q	O
<	O
i	O
d	O
>	O
are	O
question	O
types	O
,	O
extra	O
tokens	O
eol	O
(	O
end	O
-	O
of	O
-	O
line	O
)	O
and	O
ans	O
(	O
answer	O
)	O
are	O
added	O
to	O
give	O
the	O
RNN	S-Method
&	O
LSTM	S-Method
access	O
to	O
the	O
complete	O
information	O
available	O
in	O
the	O
dataset	O
.	O

The	O
final	O
number	O
is	O
the	O
class	O
label	O
.	O

paragraph	O
:	O
Example	O
2	O
.	O

As	O
a	O
second	O
example	O
,	O
below	O
is	O
an	O
instance	O
from	O
the	O
symbolic	O
dataset	O
for	O
bAbI	B-Task
task	E-Task
19	O
,	O
Path	B-Task
Finding	E-Task
.	O

Here	O
the	O
first	O
4	O
lines	O
describe	O
edges	O
,	O
s	O
,	O
n	O
,	O
w	O
,	O
e	O
(	O
e	O
does	O
not	O
appear	O
in	O
this	O
example	O
)	O
are	O
all	O
different	O
edge	O
types	O
.	O

The	O
last	O
line	O
is	O
a	O
path	O
question	O
,	O
the	O
answer	O
is	O
a	O
sequence	O
of	O
directions	O
w	O
,	O
s	O
,	O
as	O
the	O
path	O
going	O
from	O
B	O
to	O
A	O
is	O
to	O
first	O
go	O
west	O
to	O
E	O
then	O
go	O
south	O
to	O
A	O
.	O

The	O
s	O
,	O
n	O
,	O
w	O
,	O
e	O
in	O
the	O
question	O
lines	O
are	O
treated	O
as	O
output	O
classes	O
.	O

paragraph	O
:	O
More	O
Training	O
Details	O
.	O

For	O
all	O
tasks	O
in	O
this	O
section	O
,	O
we	O
generate	O
1000	O
training	O
examples	O
and	O
1000	O
test	O
examples	O
,	O
50	O
of	O
the	O
training	O
examples	O
are	O
used	O
for	O
validation	S-Task
.	O

When	O
evaluating	O
model	S-Task
performance	O
,	O
for	O
all	O
bAbI	B-Task
tasks	E-Task
that	O
contain	O
more	O
than	O
one	O
questions	O
in	O
one	O
example	O
,	O
the	O
predictions	O
for	O
different	O
questions	O
were	O
evaluated	O
independently	O
.	O

As	O
there	O
is	O
randomness	O
in	O
the	O
dataset	B-Task
generation	I-Task
process	E-Task
,	O
we	O
generated	O
10	O
such	O
datasets	O
for	O
each	O
task	O
,	O
and	O
report	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
evaluation	S-Metric
performance	O
across	O
the	O
10	O
datasets	O
.	O

For	O
all	O
explanatory	B-Task
tasks	E-Task
,	O
we	O
start	O
by	O
training	O
different	O
models	O
on	O
only	O
50	O
training	O
examples	O
,	O
and	O
gradually	O
increase	O
the	O
number	O
of	O
training	O
examples	O
to	O
100	O
,	O
250	O
,	O
500	O
,	O
and	O
950	O
(	O
50	O
of	O
the	O
training	O
examples	O
are	O
reserved	O
for	O
validation	S-Task
)	O
until	O
the	O
model	O
’s	O
test	B-Metric
accuracy	E-Metric
reaches	O
95	O
%	O
or	O
above	O
,	O
a	O
success	O
by	O
bAbI	B-Metric
standard	E-Metric
.	O

For	O
each	O
method	O
,	O
we	O
report	O
the	O
minimum	O
number	O
of	O
training	O
examples	O
it	O
needs	O
to	O
reach	O
95	O
%	O
accuracy	S-Metric
along	O
with	O
the	O
accuracy	S-Metric
it	O
reaches	O
with	O
that	O
amount	O
of	O
training	O
examples	O
.	O

In	O
all	O
these	O
cases	O
,	O
we	O
unrolled	O
the	O
propagation	B-Method
process	E-Method
for	O
5	O
steps	O
.	O

For	O
bAbI	B-Task
task	E-Task
4	O
,	O
15	O
,	O
16	O
,	O
18	O
,	O
19	O
,	O
we	O
used	O
GG	B-Method
-	I-Method
NN	E-Method
with	O
the	O
size	O
of	O
node	O
vectors	O
set	O
to	O
,	O
,	O
,	O
and	O
respectively	O
.	O

For	O
all	O
the	O
GGS	B-Method
-	I-Method
NNs	E-Method
in	O
this	O
section	O
we	O
used	O
the	O
simpler	O
variant	O
in	O
which	O
and	O
share	O
a	O
single	O
propagation	B-Method
model	E-Method
.	O

For	O
shortest	B-Task
path	E-Task
and	O
Eulerian	B-Task
circuit	I-Task
tasks	E-Task
,	O
we	O
used	O
.	O

All	O
models	O
are	O
trained	O
long	O
enough	O
with	O
Adam	S-Method
kingma2014adam	O
,	O
and	O
the	O
validation	O
set	O
is	O
used	O
to	O
choose	O
the	O
best	O
model	O
to	O
evaluate	O
and	O
avoid	O
models	O
that	O
are	O
overfitting	O
.	O

subsubsection	S-Method
:	O
Single	O
Step	O
Outputs	O
We	O
choose	O
four	O
bAbI	B-Method
tasks	E-Method
that	O
are	O
suited	O
to	O
the	O
restrictions	O
described	O
above	O
and	O
require	O
single	O
step	O
outputs	O
:	O
4	O
(	O
Two	O
Argument	O
Relations	O
)	O
,	O
15	O
(	O
Basic	B-Task
Deduction	E-Task
)	O
,	O
16	O
(	O
Basic	B-Task
Induction	E-Task
)	O
,	O
and	O
18	O
(	O
Size	B-Task
Reasoning	E-Task
)	O
.	O

For	O
Task	O
4	O
,	O
15	O
and	O
16	O
,	O
a	O
node	O
selection	O
GG	B-Method
-	I-Method
NN	E-Method
is	O
used	O
.	O

For	O
Task	O
18	O
we	O
used	O
a	O
graph	B-Method
-	I-Method
level	I-Method
classification	I-Method
version	E-Method
.	O

All	O
the	O
GGNN	B-Method
networks	E-Method
contain	O
less	O
than	O
600	O
parameters	O
.	O

As	O
baselines	O
,	O
we	O
train	O
RNN	S-Method
and	O
LSTM	B-Method
models	E-Method
on	O
the	O
symbolic	O
data	O
in	O
raw	O
sequence	O
form	O
.	O

The	O
RNNs	S-Method
and	O
LSTMs	S-Method
use	O
50	O
dimensional	B-Method
embeddings	E-Method
and	O
50	O
dimensional	B-Method
hidden	I-Method
layers	E-Method
;	O
they	O
predict	O
a	O
single	O
output	O
at	O
the	O
end	O
of	O
the	O
sequences	O
and	O
the	O
output	O
is	O
treated	O
as	O
a	O
classification	B-Task
problem	E-Task
,	O
the	O
loss	S-Metric
is	O
cross	B-Metric
entropy	E-Metric
.	O

The	O
RNNs	S-Method
and	O
LSTMs	S-Method
contain	O
around	O
5k	O
and	O
30k	O
parameters	O
,	O
respectively	O
.	O

Test	O
results	O
appear	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
all	O
tasks	O
GG	B-Method
-	I-Method
NN	E-Method
achieves	O
perfect	O
test	B-Metric
accuracy	E-Metric
using	O
only	O
50	O
training	O
examples	O
,	O
while	O
the	O
RNN	S-Method
/	O
LSTM	S-Method
baselines	O
either	O
use	O
more	O
training	O
examples	O
(	O
Task	O
4	O
)	O
or	O
fail	O
to	O
solve	O
the	O
tasks	O
(	O
Task	O
15	O
,	O
16	O
and	O
18	O
)	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
further	O
break	O
down	O
performance	O
of	O
the	O
baselines	O
for	O
task	O
4	O
as	O
the	O
amount	O
of	O
training	O
data	O
varies	O
.	O

While	O
both	O
the	O
RNN	S-Method
and	O
LSTM	S-Method
are	O
able	O
to	O
solve	O
the	O
task	O
almost	O
perfectly	O
,	O
the	O
GG	B-Method
-	I-Method
NN	E-Method
reaches	O
100	O
%	O
accuracy	S-Metric
with	O
much	O
less	O
data	O
.	O

subsubsection	S-Method
:	O
Sequential	O
Outputs	O
The	O
bAbI	B-Task
Task	E-Task
19	O
(	O
Path	B-Task
Finding	E-Task
)	O
is	O
arguably	O
the	O
hardest	O
task	O
among	O
all	O
bAbI	B-Task
tasks	E-Task
(	O
see	O
e.g.	O
,	O
sukhbaatar2015end	O
,	O
which	O
reports	O
an	O
accuracy	S-Metric
of	O
less	O
than	O
20	O
%	O
for	O
all	O
methods	O
that	O
do	O
not	O
use	O
the	O
strong	O
supervision	O
)	O
.	O

We	O
apply	O
a	O
GGS	B-Method
-	I-Method
NN	E-Method
to	O
this	O
problem	O
,	O
again	O
on	O
the	O
symbolic	O
form	O
of	O
the	O
data	O
(	O
so	O
results	O
are	O
not	O
comparable	O
to	O
those	O
in	O
sukhbaatar2015end	O
)	O
.	O

An	O
extra	O
‘	O
end	O
’	O
class	O
is	O
added	O
to	O
the	O
end	O
of	O
each	O
output	O
sequence	O
;	O
at	O
test	O
time	O
the	O
network	O
will	O
keep	O
making	O
predictions	O
until	O
it	O
predicts	O
the	O
‘	O
end	O
’	O
class	O
.	O

The	O
results	O
for	O
this	O
task	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Both	O
RNN	S-Method
and	O
LSTM	S-Method
fail	O
on	O
this	O
task	O
.	O

However	O
,	O
with	O
only	O
50	O
training	O
examples	O
,	O
our	O
GGS	B-Method
-	I-Method
NNs	E-Method
achieve	O
much	O
better	O
test	B-Metric
accuracy	E-Metric
than	O
RNN	S-Method
and	O
LSTM	S-Method
.	O

subsection	O
:	O
Learning	B-Method
Graph	I-Method
Algorithms	E-Method
We	O
further	O
developed	O
two	O
new	O
bAbI	B-Task
-	I-Task
like	I-Task
tasks	E-Task
based	O
on	O
algorithmic	B-Task
problems	E-Task
on	O
graphs	O
:	O
Shortest	O
Paths	O
,	O
and	O
Eulerian	O
Circuits	O
.	O

For	O
the	O
first	O
,	O
we	O
generate	O
random	O
graphs	O
and	O
produce	O
a	O
story	O
that	O
lists	O
all	O
edges	O
in	O
the	O
graphs	O
.	O

Questions	O
come	O
from	O
choosing	O
two	O
random	O
nodes	O
and	O
and	O
asking	O
for	O
the	O
shortest	B-Task
path	E-Task
(	O
expressed	O
as	O
a	O
sequence	O
of	O
nodes	O
)	O
that	O
connects	O
the	O
two	O
chosen	O
nodes	O
.	O

We	O
constrain	O
the	O
data	B-Task
generation	E-Task
to	O
only	O
produce	O
questions	O
where	O
there	O
is	O
a	O
unique	O
shortest	B-Task
path	E-Task
from	O
to	O
of	O
length	O
at	O
least	O
2	O
.	O

For	O
Eulerian	O
circuits	O
,	O
we	O
generate	O
a	O
random	B-Method
two	I-Method
-	I-Method
regular	I-Method
connected	I-Method
graph	E-Method
and	O
a	O
separate	O
random	B-Method
distractor	I-Method
graph	E-Method
.	O

The	O
question	O
gives	O
two	O
nodes	O
and	O
to	O
start	O
the	O
circuit	O
,	O
then	O
the	O
question	O
is	O
to	O
return	O
the	O
Eulerian	O
circuit	O
(	O
again	O
expressed	O
as	O
a	O
sequence	O
of	O
nodes	O
)	O
on	O
the	O
given	O
subgraph	O
that	O
starts	O
by	O
going	O
from	O
to	O
.	O

Results	O
are	O
shown	O
in	O
the	O
Table	O
[	O
reference	O
]	O
.	O

RNN	S-Method
and	O
LSTM	S-Method
fail	O
on	O
both	O
tasks	O
,	O
but	O
GGS	B-Method
-	I-Method
NNs	E-Method
learns	O
to	O
make	O
perfect	O
predictions	O
using	O
only	O
50	O
training	O
examples	O
.	O

section	O
:	O
Program	B-Task
Verification	E-Task
with	O
GGS	B-Method
-	I-Method
NNs	E-Method
Our	O
work	O
on	O
GGS	B-Method
-	I-Method
NNs	E-Method
is	O
motivated	O
by	O
a	O
practical	O
application	O
in	O
program	B-Task
verification	E-Task
.	O

A	O
crucial	O
step	O
in	O
automatic	B-Task
program	I-Task
verification	E-Task
is	O
the	O
inference	B-Task
of	I-Task
program	I-Task
invariants	E-Task
,	O
which	O
approximate	O
the	O
set	O
of	O
program	O
states	O
reachable	O
in	O
an	O
execution	O
.	O

Finding	B-Task
invariants	I-Task
about	I-Task
data	I-Task
structures	E-Task
is	O
an	O
open	O
problem	O
.	O

As	O
an	O
example	O
,	O
consider	O
the	O
simple	O
C	O
function	O
on	O
the	O
right	O
.	O

To	O
prove	O
that	O
this	O
program	O
indeed	O
concatenates	O
the	O
two	O
lists	O
a	O
and	O
b	O
and	O
that	O
all	O
pointer	O
dereferences	O
are	O
valid	O
,	O
we	O
need	O
to	O
(	O
mathematically	O
)	O
characterize	O
the	O
program	O
’s	O
heap	O
in	O
each	O
iteration	O
of	O
the	O
loop	O
.	O

For	O
this	O
,	O
we	O
use	O
separation	B-Method
logic	E-Method
OHearn01	O
,	O
Reynolds02	O
,	O
which	O
uses	O
inductive	O
predicates	O
to	O
describe	O
abstract	O
data	O
structures	O
.	O

For	O
example	O
,	O
a	O
l	B-Task
ist	I-Task
s	I-Task
egment	E-Task
is	O
defined	O
as	O
,	O
where	O
means	O
that	O
points	O
to	O
a	O
memory	O
region	O
that	O
contains	O
a	O
structure	O
with	O
val	O
and	O
next	O
fields	O
whose	O
values	O
are	O
in	O
turn	O
and	O
.	O

The	O
connective	O
is	O
a	O
conjunction	O
as	O
in	O
Boolean	O
logic	O
,	O
but	O
additionally	O
requires	O
that	O
its	O
operators	O
refer	O
to	O
“	O
separate	O
”	O
parts	O
of	O
the	O
heap	O
.	O

Thus	O
,	O
implies	O
that	O
cur	O
is	O
either	O
NULL	O
,	O
or	O
that	O
it	O
points	O
to	O
two	O
values	O
on	O
the	O
heap	O
,	O
where	O
is	O
described	O
by	O
again	O
.	O

The	O
formula	O
is	O
an	O
invariant	O
of	O
the	O
loop	O
(	O
i.e.	O
,	O
it	O
holds	O
when	O
entering	O
the	O
loop	O
,	O
and	O
after	O
every	O
iteration	O
)	O
.	O

Using	O
it	O
,	O
we	O
can	O
prove	O
that	O
no	O
program	O
run	O
will	O
fail	O
due	O
to	O
dereferencing	O
an	O
unallocated	O
memory	O
address	O
(	O
this	O
property	O
is	O
called	O
memory	O
safety	O
)	O
and	O
that	O
the	O
function	O
indeed	O
concatenates	O
two	O
lists	O
using	O
a	O
Hoare	B-Method
-	I-Method
style	I-Method
verification	I-Method
scheme	E-Method
Hoare69	O
.	O

The	O
hardest	O
part	O
of	O
this	O
process	O
is	O
coming	O
up	O
with	O
formulas	O
that	O
describe	O
data	O
structures	O
,	O
and	O
this	O
is	O
where	O
we	O
propose	O
to	O
use	O
machine	B-Method
learning	E-Method
.	O

Given	O
a	O
program	O
,	O
we	O
run	O
it	O
a	O
few	O
times	O
and	O
extract	O
the	O
state	O
of	O
memory	O
(	O
represented	O
as	O
a	O
graph	O
;	O
see	O
below	O
)	O
at	O
relevant	O
program	O
locations	O
,	O
and	O
then	O
predict	O
a	O
separation	B-Method
logic	E-Method
formula	O
.	O

Static	B-Method
program	I-Method
analysis	I-Method
tools	E-Method
(	O
e.g.	O
,	O
Piskac14	S-Method
)	O
can	O
check	O
whether	O
a	O
candidate	O
formula	O
is	O
sufficient	O
to	O
prove	O
the	O
desired	O
properties	O
(	O
e.g.	O
,	O
memory	O
safety	O
)	O
.	O

subsection	O
:	O
Formalization	O
paragraph	O
:	O
Representing	O
Heap	O
State	O
as	O
a	O
Graph	O
As	O
inputs	O
we	O
consider	O
directed	B-Method
,	I-Method
possibly	I-Method
cyclic	I-Method
graphs	E-Method
representing	O
the	O
heap	O
of	O
a	O
program	O
.	O

These	O
graphs	O
can	O
be	O
automatically	O
constructed	O
from	O
a	O
program	O
’s	O
memory	O
state	O
.	O

Each	O
graph	O
node	O
corresponds	O
to	O
an	O
address	O
in	O
memory	O
at	O
which	O
a	O
sequence	O
of	O
pointers	O
is	O
stored	O
(	O
we	O
ignore	O
non	O
-	O
pointer	O
values	O
in	O
this	O
work	O
)	O
.	O

Graph	O
edges	O
reflect	O
these	O
pointer	O
values	O
,	O
i.e.	O
,	O
has	O
edges	O
labeled	O
with	O
that	O
point	O
to	O
nodes	O
,	O
respectively	O
.	O

A	O
subset	O
of	O
nodes	O
are	O
labeled	O
as	O
corresponding	O
to	O
program	O
variables	O
.	O

An	O
example	O
input	O
graph	O
is	O
displayed	O
as	O
“	O
Input	O
”	O
in	O
fig	O
:	O
annotations2	O
.	O

In	O
it	O
,	O
the	O
node	O
i	O
d	O
(	O
i.e.	O
,	O
memory	O
address	O
)	O
is	O
displayed	O
in	O
the	O
node	O
.	O

Edge	O
labels	O
correspond	O
to	O
specific	O
fields	O
in	O
the	O
program	O
,	O
e.g.	O
,	O
in	O
our	O
example	O
corresponds	O
to	O
the	O
next	O
pointer	O
in	O
our	O
example	O
function	O
from	O
the	O
previous	O
section	O
.	O

For	O
binary	O
trees	O
there	O
are	O
two	O
more	O
types	O
of	O
pointers	O
left	O
and	O
right	O
pointing	O
to	O
the	O
left	O
and	O
right	O
children	O
of	O
a	O
tree	O
node	O
.	O

paragraph	O
:	O
Output	O
Representation	O
Our	O
aim	O
is	O
to	O
mathematically	O
describe	O
the	O
shape	O
of	O
the	O
heap	O
.	O

In	O
our	O
model	O
,	O
we	O
restrict	O
ourselves	O
to	O
a	O
syntactically	O
restricted	O
version	O
of	O
separation	B-Method
logic	E-Method
,	O
in	O
which	O
formulas	O
are	O
of	O
the	O
form	O
,	O
where	O
each	O
atomic	O
formula	O
is	O
either	O
(	O
a	O
list	O
from	O
to	O
)	O
,	O
(	O
a	O
binary	O
tree	O
starting	O
in	O
)	O
,	O
or	O
(	O
no	O
data	O
structure	O
at	O
)	O
.	O

Existential	O
quantifiers	O
are	O
used	O
to	O
give	O
names	O
to	O
heap	O
nodes	O
which	O
are	O
needed	O
to	O
describe	O
a	O
shape	O
,	O
but	O
not	O
labeled	O
by	O
a	O
program	O
variable	O
.	O

For	O
example	O
,	O
to	O
describe	O
a	O
“	O
panhandle	O
list	O
”	O
(	O
a	O
list	O
that	O
ends	O
in	O
a	O
cycle	O
)	O
,	O
the	O
first	O
list	O
element	O
on	O
the	O
cycle	O
needs	O
to	O
be	O
named	O
.	O

In	O
separation	B-Method
logic	E-Method
,	O
this	O
can	O
be	O
expressed	O
as	O
.	O

paragraph	O
:	O
Data	O
We	O
can	O
generate	O
synthetic	O
(	O
labeled	O
)	O
datasets	O
for	O
this	O
problem	O
.	O

For	O
this	O
,	O
we	O
fix	O
a	O
set	O
of	O
predicates	O
such	O
as	O
and	O
(	O
extensions	O
could	O
consider	O
doubly	O
-	O
linked	O
list	O
segments	O
,	O
multi	O
-	O
trees	O
,	O
)	O
together	O
with	O
their	O
inductive	B-Method
definitions	E-Method
.	O

Then	O
we	O
enumerate	O
separation	B-Method
logic	E-Method
formulas	O
instantiating	O
our	O
predicates	O
using	O
a	O
given	O
set	O
of	O
program	O
variables	O
.	O

Finally	O
,	O
for	O
each	O
formula	O
,	O
we	O
enumerate	O
heap	O
graphs	O
satisfying	O
that	O
formula	O
.	O

The	O
result	O
is	O
a	O
dataset	O
consisting	O
of	O
pairs	O
of	O
heap	O
graphs	O
and	O
associated	O
formulas	O
that	O
are	O
used	O
by	O
our	O
learning	B-Method
procedures	E-Method
.	O

subsection	O
:	O
Formulation	O
as	O
GGS	B-Method
-	I-Method
NNs	E-Method
It	O
is	O
easy	O
to	O
obtain	O
the	O
node	O
annotations	O
for	O
the	O
intermediate	O
prediction	B-Task
steps	E-Task
from	O
the	O
data	B-Task
generation	I-Task
process	E-Task
.	O

So	O
we	O
train	O
a	O
variant	O
of	O
GGS	B-Method
-	I-Method
NN	E-Method
with	O
observed	O
annotations	O
(	O
observed	O
at	O
training	O
time	O
;	O
not	O
test	O
time	O
)	O
to	O
infer	O
formulas	O
from	O
heap	O
graphs	O
.	O

Note	O
that	O
it	O
is	O
also	O
possible	O
to	O
use	O
an	O
unobserved	B-Method
GGS	I-Method
-	I-Method
NN	I-Method
variant	E-Method
and	O
do	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
learning	E-Task
.	O

The	O
procedure	O
breaks	O
down	O
the	O
production	O
of	O
a	O
separation	B-Method
logic	E-Method
formula	O
into	O
a	O
sequence	O
of	O
steps	O
.	O

We	O
first	O
decide	O
whether	O
to	O
declare	O
existential	O
variables	O
,	O
and	O
if	O
so	O
,	O
choose	O
which	O
node	O
corresponds	O
to	O
the	O
variable	O
.	O

Once	O
we	O
have	O
declared	O
existentials	O
,	O
we	O
iterate	O
over	O
all	O
variable	O
names	O
and	O
produce	O
a	O
separation	B-Method
logic	E-Method
formula	O
describing	O
the	O
data	O
structure	O
rooted	O
at	O
the	O
node	O
corresponding	O
to	O
the	O
current	O
variable	O
.	O

The	O
full	O
algorithm	O
for	O
predicting	O
separation	B-Method
logic	E-Method
formula	O
appears	O
below	O
,	O
as	O
alg	O
:	O
seplogic	B-Method
-	I-Method
prediction	E-Method
.	O

We	O
use	O
three	O
explicit	O
node	O
annotations	O
,	O
namely	O
is	O
-	O
named	O
(	O
heap	O
node	O
labeled	O
by	O
program	O
variable	O
or	O
declared	O
existentially	O
quantified	O
variable	O
)	O
,	O
active	O
(	O
cf	O
.	O

algorithm	O
)	O
and	O
is	O
-	O
explained	O
(	O
heap	O
node	O
is	O
part	O
of	O
data	O
structure	O
already	O
predicted	O
)	O
.	O

Initial	O
node	O
labels	O
can	O
be	O
directly	O
computed	O
from	O
the	O
input	O
graph	O
:	O
“	O
is	O
-	O
named	O
”	O
is	O
on	O
for	O
nodes	O
labeled	O
by	O
program	O
variables	O
,	O
“	O
active	O
”	O
and	O
“	O
is	O
-	O
explained	O
”	O
are	O
always	O
off	O
(	O
done	O
in	O
line	O
2	O
)	O
.	O

The	O
commented	O
lines	O
in	O
the	O
algorithm	O
are	O
implemented	O
using	O
a	O
GG	B-Method
-	I-Method
NN	E-Method
,	O
i.e.	O
,	O
alg	O
:	O
seplogic	B-Method
-	I-Method
prediction	E-Method
is	O
an	O
instance	O
of	O
our	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
.	O

An	O
illustration	O
of	O
the	O
beginning	O
of	O
a	O
run	O
of	O
the	O
algorithm	O
is	O
shown	O
in	O
fig	O
:	O
annotations2	O
,	O
where	O
each	O
step	O
is	O
related	O
to	O
one	O
line	O
of	O
the	O
algorithm	O
.	O

[	O
heapgraph	O
,	O
scale=.9	O
]	O
[	O
colLabel	O
]	O
(	O
labelStep	O
)	O
at	O
(	O
0	O
,	O
0	O
)	O
Step	O
;	O
[	O
colLabel	O
]	O
(	O
labelGraph	O
)	O
at	O
(	O
)	O
Labeled	O
Graph	O
;	O
[	O
colLabel	O
]	O
(	O
labelOut	O
)	O
at	O
(	O
)	O
Out	O
p	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
0labelStep	O
)	O
at	O
(	O
)	O
Input	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
0b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
0labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0bn	O
)	O
(	O
0bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0	O
t	O
)	O
(	O
0	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0tn	O
)	O
(	O
0tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
0	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
1labelStep	O
)	O
at	O
(	O
)	O
Line	O
3	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
1b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
1bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
1	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
1tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
1labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1bn	O
)	O
(	O
1bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1	O
t	O
)	O
(	O
1	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1tn	O
)	O
(	O
1tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
1	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
2labelStep	O
)	O
at	O
(	O
)	O
Line	O
4	O
-	O
7	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
2b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
2bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
2	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
2tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
2labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2bn	O
)	O
(	O
2bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2	O
t	O
)	O
(	O
2	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2tn	O
)	O
(	O
2tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
2	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
3labelStep	O
)	O
at	O
(	O
)	O
Line	O
10	O
(	O
for	O
b	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
3b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
3bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
3	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
3tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
3labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3bn	O
)	O
(	O
3bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3	O
t	O
)	O
(	O
3	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3tn	O
)	O
(	O
3tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
3	O
t	O
)	O
;	O
[	O
-	O
]	O
(	O
)	O
–	O
(	O
)	O
;	O
[	O
colLabel	O
]	O
(	O
labelStep	O
’	O
)	O
at	O
(	O
)	O
Step	O
;	O
[	O
colLabel	O
]	O
(	O
labelGraph	O
’	O
)	O
at	O
(	O
)	O
Labeled	O
Graph	O
;	O
[	O
colLabel	O
]	O
(	O
labelOut	O
’	O
)	O
at	O
(	O
)	O
Out	O
p	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
4labelStep	O
)	O
at	O
(	O
)	O
Line	O
11	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
4b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
4bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
4	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
4tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
4labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4bn	O
)	O
(	O
4bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4	O
t	O
)	O
(	O
4	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4tn	O
)	O
(	O
4tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
4	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
5labelStep	O
)	O
at	O
(	O
)	O
Line	O
13	O
,	O
14	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
5b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
5bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
5	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
5tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
5labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5bn	O
)	O
(	O
5bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5	O
t	O
)	O
(	O
5	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5tn	O
)	O
(	O
5tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
5	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
6labelStep	O
)	O
at	O
(	O
)	O
Line	O
18	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
explained	O
,	O
label=90	O
:	O
b	O
]	O
(	O
6b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
explained	O
]	O
(	O
6bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
6	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
6tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
6labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6bn	O
)	O
(	O
6bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6	O
t	O
)	O
(	O
6	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6tn	O
)	O
(	O
6tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
6	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
7labelStep	O
)	O
at	O
(	O
)	O
Line	O
10	O
(	O
for	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
explained	O
,	O
label=90	O
:	O
b	O
]	O
(	O
7b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
explained	O
]	O
(	O
7bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
active	O
,	O
label=90	O
:	O
]	O
(	O
7	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
7tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
7labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7bn	O
)	O
(	O
7bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7	O
t	O
)	O
(	O
7	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7tn	O
)	O
(	O
7tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
7	O
t	O
)	O
;	O
[	O
1	O
]	O
Heap	O
graph	O
with	O
named	O
program	O
variables	O
compute	O
initial	O
labels	O
from	O
initialize	O
node	O
vectors	O
by	O
-	O
extending	O
quantifier	O
needed	O
Graph	O
-	O
level	O
Classification	O
(	O
†	O
)	O
fresh	O
variable	O
name	O
pick	O
node	O
Node	B-Method
Selection	E-Method
(	O
‡	O
)	O
turn	O
on	O
“	O
is	O
-	O
named	O
”	O
for	O
in	O
print	O
“	O
”	O
node	O
with	O
label	O
“	O
is	O
-	O
named	O
”	O
in	O
initialize	O
node	O
vectors	O
,	O
turn	O
on	O
“	O
active	O
”	O
label	O
for	O
in	O
pick	O
data	O
structure	O
predicate	O
Graph	O
-	O
level	O
Classification	S-Task
(	O
⋆	O
)	O
pick	O
list	O
end	O
node	O
Node	B-Task
Selection	E-Task
(	O
♡	O
)	O
print	O
“	O
”	O
print	O
“	O
”	O
update	O
node	O
annotations	O
in	O
Node	O
Annotation	O
(	O
♠	O
)	O
Separation	B-Method
logic	I-Method
formula	I-Method
prediction	I-Method
procedure	E-Method
subsection	O
:	O
Model	O
Setup	O
Details	O
We	O
use	O
the	O
full	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
where	O
and	O
have	O
separate	O
propagation	B-Method
models	E-Method
.	O

For	O
all	O
the	O
GG	B-Method
-	I-Method
NN	E-Method
components	O
in	O
the	O
GGS	B-Method
-	I-Method
NN	I-Method
pipeline	E-Method
,	O
we	O
unrolled	O
the	O
propagation	B-Method
process	E-Method
for	O
10	O
time	O
steps	O
.	O

The	O
GGS	B-Method
-	I-Method
NNs	E-Method
associated	O
with	O
step	O
(	O
)	O
(	O
deciding	O
wheter	O
more	O
existentially	O
quantified	O
variable	O
need	O
to	O
be	O
declared	O
)	O
and	O
(	O
)	O
(	O
identify	O
which	O
node	O
need	O
to	O
be	O
declared	O
as	O
existentially	O
quantified	O
)	O
uses	O
dimensional	B-Method
node	I-Method
representations	E-Method
.	O

For	O
all	O
other	O
GGS	B-Method
-	I-Method
NN	I-Method
components	E-Method
,	O
is	O
used	O
.	O

Adam	S-Method
kingma2014adam	O
is	O
used	O
for	O
optimization	S-Task
,	O
the	O
models	O
are	O
trained	O
on	O
minibatches	O
of	O
20	O
graphs	O
,	O
and	O
optimized	O
until	O
training	B-Metric
error	E-Metric
is	O
very	O
low	O
.	O

For	O
the	O
graph	B-Task
-	I-Task
level	I-Task
classification	I-Task
tasks	E-Task
,	O
we	O
also	O
artificially	O
balanced	O
classes	O
to	O
have	O
even	O
number	O
of	O
examples	O
from	O
each	O
class	O
in	O
each	O
minibatch	O
.	O

All	O
the	O
GGS	B-Method
-	I-Method
NN	I-Method
components	E-Method
contain	O
less	O
than	O
5k	O
parameters	O
and	O
no	O
overfitting	O
is	O
observed	O
during	O
training	O
.	O

subsection	O
:	O
Batch	B-Task
Prediction	E-Task
Details	O
In	O
practice	O
,	O
a	O
set	O
of	O
heap	O
graphs	O
will	O
be	O
given	O
as	O
input	O
and	O
a	O
single	O
output	O
formula	O
is	O
expected	O
to	O
describe	O
and	O
be	O
consistent	O
with	O
all	O
the	O
input	O
graphs	O
.	O

The	O
different	O
heap	O
graphs	O
can	O
be	O
snapshots	O
of	O
the	O
heap	O
state	O
at	O
different	O
points	O
in	O
the	O
program	O
execution	O
process	O
,	O
or	O
different	O
runs	O
of	O
the	O
same	O
program	O
with	O
different	O
inputs	O
.	O

We	O
call	O
this	O
the	O
“	O
batch	B-Task
prediction	I-Task
”	I-Task
setup	E-Task
contrasting	O
with	O
the	O
single	B-Method
graph	I-Method
prediction	E-Method
described	O
in	O
the	O
main	O
paper	O
.	O

To	O
make	O
batch	B-Task
predictions	E-Task
,	O
we	O
run	O
one	O
GGS	B-Method
-	I-Method
NN	E-Method
for	O
each	O
graph	O
simultaneously	O
.	O

For	O
each	O
prediction	O
step	O
,	O
the	O
outputs	O
of	O
all	O
the	O
GGS	B-Method
-	I-Method
NNs	E-Method
at	O
that	O
step	O
across	O
the	O
batch	O
of	O
graphs	O
are	O
aggregated	O
.	O

For	O
node	B-Task
selection	I-Task
outputs	E-Task
,	O
the	O
common	O
named	O
variables	O
link	O
nodes	O
on	O
different	O
graphs	O
togeter	O
,	O
which	O
is	O
the	O
key	O
for	O
aggregating	B-Task
predictions	E-Task
in	O
a	O
batch	O
.	O

We	O
compute	O
the	O
score	O
for	O
a	O
particular	O
named	O
variable	O
as	O
,	O
where	O
maps	O
variable	O
name	O
to	O
a	O
node	O
in	O
graph	O
,	O
and	O
is	O
the	O
output	O
score	O
for	O
named	O
variable	O
in	O
graph	O
.	O

When	O
applying	O
a	O
softmax	O
over	O
all	O
names	O
using	O
as	O
scores	O
,	O
this	O
is	O
equivalent	O
to	O
a	O
model	O
that	O
computes	O
.	O

For	O
graph	B-Task
-	I-Task
level	I-Task
classification	I-Task
outputs	E-Task
,	O
we	O
add	O
up	O
scores	O
of	O
a	O
particular	O
class	O
across	O
the	O
batch	O
of	O
graphs	O
,	O
or	O
equivalently	O
compute	O
.	O

Node	O
annotation	O
outputs	O
are	O
updated	O
for	O
each	O
graph	O
independently	O
as	O
different	O
graphs	O
have	O
completely	O
different	O
set	O
of	O
nodes	O
.	O

However	O
,	O
when	O
the	O
algorithm	O
tries	O
to	O
update	O
the	O
annotation	O
for	O
one	O
named	O
variable	O
,	O
the	O
nodes	O
associated	O
with	O
that	O
variable	O
in	O
all	O
graphs	O
are	O
updated	O
.	O

During	O
training	O
,	O
all	O
labels	O
for	O
intermediate	O
steps	O
are	O
available	O
to	O
us	O
from	O
the	O
data	B-Task
generation	I-Task
process	E-Task
,	O
so	O
the	O
training	B-Task
process	E-Task
again	O
can	O
be	O
decomposed	O
to	O
single	B-Task
output	I-Task
single	I-Task
graph	I-Task
training	E-Task
.	O

A	O
more	O
complex	O
scenario	O
allowing	O
for	O
nested	O
data	O
structures	O
(	O
e.g.	O
,	O
list	O
of	O
lists	O
)	O
was	O
discussed	O
in	O
.	O

We	O
have	O
also	O
successfully	O
extended	O
the	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
to	O
this	O
case	O
.	O

More	O
details	O
on	O
this	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Experiments	O
.	O

For	O
this	O
paper	O
,	O
we	O
produced	O
a	O
dataset	O
of	O
327	O
formulas	O
that	O
involves	O
three	O
program	O
variables	O
,	O
with	O
498	O
graphs	O
per	O
formula	O
,	O
yielding	O
around	O
160	O
,	O
000	O
formula	O
/	O
heap	O
graph	O
combinations	O
.	O

To	O
evaluate	O
,	O
we	O
split	O
the	O
data	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
using	O
a	O
6:2:2	O
split	O
on	O
the	O
formulas	O
(	O
i.e.	O
,	O
the	O
formulas	O
in	O
the	O
test	O
set	O
were	O
not	O
in	O
the	O
training	O
set	O
)	O
.	O

We	O
measure	O
correctness	S-Metric
by	O
whether	O
the	O
formula	O
predicted	O
at	O
test	O
time	O
is	O
logically	O
equivalent	O
to	O
the	O
ground	O
truth	O
;	O
equivalence	O
is	O
approximated	O
by	O
canonicalizing	O
names	O
and	O
order	O
of	O
the	O
formulas	O
and	O
then	O
comparing	O
for	O
exact	O
equality	O
.	O

We	O
compared	O
our	O
GGS	B-Method
-	I-Method
NN	I-Method
-	I-Method
based	I-Method
model	E-Method
with	O
a	O
method	O
we	O
developed	O
earlier	O
brockschmidt2015learning	O
.	O

The	O
earlier	O
approach	O
treats	O
each	O
prediction	O
step	O
as	O
standard	O
classification	S-Task
,	O
and	O
requires	O
complex	O
,	O
manual	B-Task
,	I-Task
problem	I-Task
-	I-Task
specific	I-Task
feature	I-Task
engineering	E-Task
,	O
to	O
achieve	O
an	O
accuracy	S-Metric
of	O
89.11	O
%	O
.	O

In	O
contrast	O
,	O
our	O
new	O
model	O
was	O
trained	O
with	O
no	O
feature	B-Method
engineering	E-Method
and	O
very	O
little	O
domain	O
knowledge	O
and	O
achieved	O
an	O
accuracy	S-Metric
of	O
89.96	O
%	O
.	O

An	O
example	O
heap	O
graph	O
and	O
the	O
corresponding	O
separation	B-Method
logic	E-Method
formula	O
found	O
by	O
our	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
is	O
shown	O
in	O
fig	O
:	O
appendix	O
-	O
heap	O
-	O
graph	O
-	O
example	O
.	O

This	O
example	O
also	O
involves	O
nested	O
data	O
structures	O
and	O
the	O
batching	B-Method
extension	E-Method
developed	O
in	O
the	O
previous	O
section	O
.	O

We	O
have	O
also	O
successfully	O
used	O
our	O
new	O
model	O
in	O
a	O
program	B-Method
verification	I-Method
framework	E-Method
,	O
supplying	O
needed	O
program	O
invariants	O
to	O
a	O
theorem	B-Method
prover	E-Method
to	O
prove	O
correctness	O
of	O
a	O
collection	O
of	O
list	B-Method
-	I-Method
manipulating	I-Method
algorithms	E-Method
such	O
as	O
insertion	B-Method
sort	E-Method
.	O

The	O
following	O
Table	O
[	O
reference	O
]	O
lists	O
a	O
set	O
of	O
benchmark	O
list	O
manipulation	O
programs	O
and	O
the	O
separation	B-Method
logic	E-Method
formula	O
invariants	O
found	O
by	O
the	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
,	O
which	O
were	O
successfully	O
used	O
in	O
a	O
verification	B-Method
framework	E-Method
to	O
prove	O
the	O
correctness	O
of	O
corresponding	O
programs	O
.	O

A	O
further	O
extension	O
of	O
the	O
current	O
pipeline	O
has	O
been	O
shown	O
to	O
be	O
able	O
to	O
successfully	O
prove	O
more	O
sophisticated	O
programs	O
like	O
sorting	B-Method
programs	E-Method
and	O
various	O
other	O
list	B-Task
-	I-Task
manipulating	I-Task
programs	E-Task
.	O

section	O
:	O
Related	O
Work	O
The	O
most	O
closely	O
related	O
work	O
is	O
GNNs	S-Method
,	O
which	O
we	O
have	O
discussed	O
at	O
length	O
above	O
.	O

proposed	O
another	O
closely	O
related	O
model	O
that	O
differs	O
from	O
GNNs	S-Method
mainly	O
in	O
the	O
output	B-Method
model	E-Method
.	O

GNNs	S-Method
have	O
been	O
applied	O
in	O
several	O
domains	O
gori2005new	O
,	O
di2006comparison	O
,	O
scarselli2009graph	O
,	O
uwents2011neural	O
,	O
but	O
they	O
do	O
not	O
appear	O
to	O
be	O
in	O
widespread	O
use	O
in	O
the	O
ICLR	B-Task
community	E-Task
.	O

Part	O
of	O
our	O
aim	O
here	O
is	O
to	O
publicize	O
GNNs	S-Method
as	O
a	O
useful	O
and	O
interesting	O
neural	B-Method
network	I-Method
variant	E-Method
.	O

An	O
analogy	O
can	O
be	O
drawn	O
between	O
our	O
adaptation	O
from	O
GNNs	S-Method
to	O
GG	B-Method
-	I-Method
NNs	E-Method
,	O
to	O
the	O
work	O
of	O
and	O
in	O
the	O
structured	B-Task
prediction	I-Task
setting	E-Task
.	O

There	O
belief	B-Method
propagation	E-Method
(	O
which	O
must	O
be	O
run	O
to	O
near	O
convergence	O
to	O
get	O
good	O
gradients	O
)	O
is	O
replaced	O
with	O
truncated	B-Method
belief	I-Method
propagation	I-Method
updates	E-Method
,	O
and	O
then	O
the	O
model	O
is	O
trained	O
so	O
that	O
the	O
truncated	O
iteration	O
produce	O
good	O
results	O
after	O
a	O
fixed	O
number	O
of	O
iterations	O
.	O

Similarly	O
,	O
Recursive	B-Method
Neural	I-Method
Networks	E-Method
goller1996learning	O
,	O
socher2011parsing	O
being	O
extended	O
to	O
Tree	B-Method
LSTMs	E-Method
tai2015improved	O
is	O
analogous	O
to	O
our	O
using	O
of	O
GRU	B-Method
updates	E-Method
in	O
GG	B-Method
-	I-Method
NNs	E-Method
instead	O
of	O
the	O
standard	O
GNN	B-Method
recurrence	E-Method
with	O
the	O
aim	O
of	O
improving	O
the	O
long	B-Task
-	I-Task
term	I-Task
propagation	I-Task
of	I-Task
information	E-Task
across	O
a	O
graph	O
structure	O
.	O

The	O
general	O
idea	O
expressed	O
in	O
this	O
paper	O
of	O
assembling	O
problem	B-Method
-	I-Method
specific	I-Method
neural	I-Method
networks	E-Method
as	O
a	O
composition	O
of	O
learned	B-Method
components	E-Method
has	O
a	O
long	O
history	O
,	O
dating	O
back	O
at	O
least	O
to	O
the	O
work	O
of	O
on	O
assembling	O
neural	B-Method
networks	E-Method
according	O
to	O
a	O
family	O
tree	O
structure	O
in	O
order	O
to	O
predict	O
relations	O
between	O
people	O
.	O

Similar	O
ideas	O
appear	O
in	O
and	O
.	O

Graph	B-Method
kernels	E-Method
shervashidze2011weisfeiler	O
,	O
kashima2003marginalized	O
can	O
be	O
used	O
for	O
a	O
variety	O
of	O
kernel	B-Task
-	I-Task
based	I-Task
learning	I-Task
tasks	E-Task
with	O
graph	O
-	O
structured	O
inputs	O
,	O
but	O
we	O
are	O
not	O
aware	O
of	O
work	O
that	O
learns	O
the	O
kernels	O
and	O
outputs	O
sequences	O
.	O

convert	O
graphs	O
into	O
sequences	O
by	O
following	O
random	O
walks	O
on	O
the	O
graph	O
then	O
learns	O
node	B-Method
embeddings	E-Method
using	O
sequence	B-Method
-	I-Method
based	I-Method
methods	E-Method
.	O

map	O
graphs	O
to	O
graph	O
vectors	O
then	O
classify	O
using	O
an	O
output	B-Method
neural	I-Method
network	E-Method
.	O

There	O
are	O
several	O
models	O
that	O
make	O
use	O
of	O
similar	B-Method
propagation	I-Method
of	I-Method
node	I-Method
representations	E-Method
on	O
a	O
graph	O
structure	O
.	O

generalize	O
convolutions	S-Method
to	O
graph	O
structures	O
.	O

The	O
difference	O
between	O
their	O
work	O
and	O
GNNs	S-Method
is	O
analogous	O
to	O
the	O
difference	O
between	O
convolutional	B-Method
and	I-Method
recurrent	I-Method
networks	E-Method
.	O

also	O
consider	O
convolutional	B-Method
like	I-Method
operations	E-Method
on	O
graphs	O
,	O
building	O
a	O
learnable	O
,	O
differentiable	B-Method
variant	E-Method
of	O
a	O
successful	O
graph	O
feature	O
.	O

converts	O
an	O
arbitrary	O
undirected	O
graph	O
to	O
a	O
number	O
of	O
different	O
DAGs	O
with	O
different	O
orientations	O
and	O
then	O
propagates	O
node	O
representations	O
inwards	O
towards	O
each	O
root	O
,	O
training	O
an	O
ensemble	O
of	O
models	O
.	O

In	O
all	O
of	O
the	O
above	O
,	O
the	O
focus	O
is	O
on	O
one	B-Task
-	I-Task
step	I-Task
problems	E-Task
.	O

GNNs	S-Method
and	O
our	O
extensions	O
have	O
many	O
of	O
the	O
same	O
desirable	O
properties	O
of	O
pointer	B-Method
networks	E-Method
vinyals2015pointer	O
;	O
when	O
using	O
node	B-Method
selection	I-Method
output	I-Method
layers	E-Method
,	O
nodes	O
from	O
the	O
input	O
can	O
be	O
chosen	O
as	O
outputs	O
.	O

There	O
are	O
two	O
main	O
differences	O
:	O
first	O
,	O
in	O
GNNs	S-Method
the	O
graph	O
structure	O
is	O
explicit	O
,	O
which	O
makes	O
the	O
models	O
less	O
general	O
but	O
may	O
provide	O
stronger	O
generalization	B-Metric
ability	E-Metric
;	O
second	O
,	O
pointer	B-Method
networks	E-Method
require	O
that	O
each	O
node	O
has	O
properties	O
(	O
e.g.	O
,	O
a	O
location	O
in	O
space	O
)	O
,	O
while	O
GNNs	S-Method
can	O
represent	O
nodes	O
that	O
are	O
defined	O
only	O
by	O
their	O
position	O
in	O
the	O
graph	O
,	O
which	O
makes	O
them	O
more	O
general	O
along	O
a	O
different	O
dimension	O
.	O

GGS	B-Method
-	I-Method
NNs	E-Method
are	O
related	O
to	O
soft	B-Method
alignment	I-Method
and	I-Method
attentional	I-Method
models	E-Method
(	O
e.g.	O
,	O
)	O
in	O
two	O
respects	O
:	O
first	O
,	O
the	O
graph	B-Method
representation	E-Method
in	O
(	O
[	O
reference	O
]	O
)	O
uses	O
context	O
to	O
focus	O
attention	O
on	O
which	O
nodes	O
are	O
important	O
to	O
the	O
current	O
decision	O
;	O
second	O
,	O
node	O
annotations	O
in	O
the	O
program	O
verification	O
example	O
keep	O
track	O
of	O
which	O
nodes	O
have	O
been	O
explained	O
so	O
far	O
,	O
which	O
gives	O
an	O
explicit	O
mechanism	O
for	O
making	O
sure	O
that	O
each	O
node	O
in	O
the	O
input	O
has	O
been	O
used	O
over	O
the	O
sequence	O
of	O
producing	O
an	O
output	O
.	O

section	O
:	O
Discussion	O
paragraph	O
:	O
What	O
is	O
being	O
learned	O
?	O
It	O
is	O
instructive	O
to	O
consider	O
what	O
is	O
being	O
learned	O
by	O
the	O
GG	B-Method
-	I-Method
NNs	E-Method
.	O

To	O
do	O
so	O
,	O
we	O
can	O
draw	O
analogy	O
between	O
how	O
the	O
bAbI	B-Task
task	E-Task
15	O
would	O
be	O
solved	O
via	O
a	O
logical	B-Method
formulation	E-Method
.	O

As	O
an	O
example	O
,	O
consider	O
the	O
subset	O
of	O
lines	O
needed	O
to	O
answer	O
one	O
example	O
on	O
the	O
right	O
.	O

To	O
do	O
logical	B-Task
reasoning	E-Task
,	O
we	O
would	O
need	O
not	O
only	O
a	O
logical	B-Method
encoding	E-Method
of	O
the	O
facts	O
present	O
in	O
the	O
story	O
but	O
also	O
the	O
background	O
world	O
knowledge	O
encoded	O
as	O
inference	O
rules	O
such	O
as	O
Our	O
encoding	O
of	O
the	O
tasks	O
simplifies	O
the	O
parsing	O
of	O
the	O
story	O
into	O
graph	O
form	O
,	O
but	O
it	O
does	O
not	O
provide	O
any	O
of	O
the	O
background	O
knowledge	O
.	O

The	O
GG	B-Method
-	I-Method
NN	E-Method
model	O
can	O
be	O
seen	O
as	O
learning	O
this	O
,	O
with	O
results	O
stored	O
in	O
the	O
neural	O
network	O
weights	O
.	O

To	O
do	O
logical	B-Task
reasoning	E-Task
,	O
we	O
would	O
need	O
not	O
only	O
a	O
logical	B-Method
encoding	E-Method
of	O
the	O
facts	O
present	O
in	O
the	O
story	O
but	O
also	O
the	O
background	O
world	O
knowledge	O
encoded	O
as	O
inference	O
rules	O
such	O
as	O
Our	O
encoding	O
of	O
the	O
tasks	O
simplifies	O
the	O
parsing	B-Task
of	I-Task
the	I-Task
story	E-Task
into	O
graph	O
form	O
,	O
but	O
it	O
does	O
not	O
provide	O
any	O
of	O
the	O
background	O
world	O
knowledge	O
.	O

Thus	O
,	O
the	O
GG	B-Method
-	I-Method
NN	E-Method
model	O
can	O
be	O
seen	O
as	O
learning	O
the	O
background	O
world	O
knowledge	O
,	O
with	O
results	O
stored	O
in	O
the	O
neural	B-Method
network	I-Method
weights	E-Method
.	O

paragraph	O
:	O
Discussion	O
The	O
results	O
in	O
the	O
paper	O
show	O
that	O
GGS	B-Method
-	I-Method
NNs	E-Method
have	O
desirable	O
inductive	O
biases	O
across	O
a	O
range	O
of	O
problems	O
that	O
have	O
some	O
intrinsic	O
graph	O
structure	O
to	O
them	O
,	O
and	O
we	O
believe	O
there	O
to	O
be	O
many	O
more	O
cases	O
where	O
GGS	B-Method
-	I-Method
NNs	E-Method
will	O
be	O
useful	O
.	O

There	O
are	O
,	O
however	O
,	O
some	O
limitations	O
that	O
need	O
to	O
be	O
overcome	O
to	O
make	O
them	O
apply	O
even	O
more	O
broadly	O
.	O

Two	O
limitations	O
that	O
we	O
mentioned	O
previously	O
are	O
that	O
the	O
bAbI	B-Task
task	E-Task
translation	O
does	O
not	O
incorporate	O
temporal	O
order	O
of	O
inputs	O
or	O
ternary	O
and	O
higher	O
order	O
relations	O
.	O

We	O
can	O
imagine	O
several	O
possibilities	O
for	O
lifting	O
these	O
restrictions	O
,	O
such	O
as	O
concatenating	O
a	O
series	O
of	O
GG	B-Method
-	I-Method
NNs	E-Method
,	O
where	O
there	O
is	O
one	O
GG	B-Method
-	I-Method
NNs	E-Method
for	O
each	O
edge	O
,	O
and	O
representing	O
higher	O
order	O
relations	O
as	O
factor	O
graphs	O
.	O

A	O
more	O
significant	O
challenge	O
is	O
how	O
to	O
handle	O
less	O
structured	O
input	O
representations	O
.	O

For	O
example	O
,	O
in	O
the	O
bAbI	B-Task
tasks	E-Task
it	O
would	O
be	O
desirable	O
not	O
to	O
use	O
the	O
symbolic	O
form	O
of	O
the	O
inputs	O
.	O

One	O
possible	O
approach	O
is	O
to	O
incorporate	O
less	O
structured	O
inputs	O
,	O
and	O
latent	O
vectors	O
,	O
in	O
our	O
GGS	B-Method
-	I-Method
NNs	E-Method
.	O

However	O
,	O
experimentation	O
is	O
needed	O
to	O
find	O
the	O
best	O
way	O
of	O
addressing	O
these	O
issues	O
.	O

The	O
current	O
GGS	B-Method
-	I-Method
NNs	E-Method
formulation	O
specifies	O
a	O
question	O
only	O
after	O
all	O
the	O
facts	O
have	O
been	O
consumed	O
.	O

This	O
implies	O
that	O
the	O
network	O
must	O
try	O
to	O
derive	O
all	O
consequences	O
of	O
the	O
seen	O
facts	O
and	O
store	O
all	O
pertinent	O
information	O
to	O
a	O
node	O
within	O
its	O
node	B-Method
representation	E-Method
.	O

This	O
is	O
likely	O
not	O
ideal	O
;	O
it	O
would	O
be	O
preferable	O
to	O
develop	O
methods	O
that	O
take	O
the	O
question	O
as	O
an	O
initial	O
input	O
,	O
and	O
then	O
dynamically	O
derive	O
the	O
facts	O
needed	O
to	O
answer	O
the	O
question	O
.	O

We	O
are	O
optimistic	O
about	O
the	O
further	O
applications	O
of	O
GGS	B-Method
-	I-Method
NNs	E-Method
.	O

We	O
are	O
particularly	O
interested	O
in	O
continuing	O
to	O
develop	O
end	O
-	O
to	O
-	O
end	O
learnable	B-Method
systems	E-Method
that	O
can	O
learn	O
about	O
semantic	O
properties	O
of	O
programs	O
,	O
that	O
can	O
learn	O
more	O
complicated	O
graph	B-Method
algorithms	E-Method
,	O
and	O
in	O
applying	O
these	O
ideas	O
to	O
problems	O
that	O
require	O
reasoning	O
over	O
knowledge	O
bases	O
and	O
databases	O
.	O

More	O
generally	O
,	O
we	O
consider	O
these	O
graph	B-Method
neural	I-Method
networks	E-Method
as	O
representing	O
a	O
step	O
towards	O
a	O
model	O
that	O
can	O
combine	O
structured	B-Method
representations	E-Method
with	O
the	O
powerful	O
algorithms	O
of	O
deep	B-Method
learning	E-Method
,	O
with	O
the	O
aim	O
of	O
taking	O
advantage	O
of	O
known	O
structure	O
while	O
learning	O
and	O
inferring	O
how	O
to	O
reason	O
with	O
and	O
extend	O
these	O
representations	O
.	O

section	O
:	O
Acknowledgements	O
We	O
thank	O
Siddharth	O
Krishna	O
,	O
Alex	O
Gaunt	O
,	O
Emine	O
Yilmaz	O
,	O
Milad	O
Shokouhi	O
,	O
and	O
Pushmeet	O
Kohli	O
for	O
useful	O
conversations	O
and	O
Douwe	O
Kiela	O
for	O
comments	O
on	O
an	O
earlier	O
draft	O
of	O
the	O
paper	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Contraction	B-Method
Map	E-Method
Example	O
Consider	O
a	O
linear	B-Method
1	I-Method
-	I-Method
hidden	I-Method
unit	I-Method
cycle	I-Method
-	I-Method
structured	I-Method
GNN	I-Method
with	I-Method
nodes	E-Method
.	O

For	O
simplicity	O
we	O
ignored	O
all	O
edge	O
labels	O
and	O
node	O
labels	O
,	O
equivalently	O
this	O
is	O
a	O
simple	O
example	O
with	O
and	O
.	O

At	O
each	O
timestep	O
we	O
update	O
hidden	O
states	O
as	O
for	O
each	O
,	O
where	O
and	O
are	O
parameters	O
of	O
the	O
propagation	B-Method
model	E-Method
.	O

We	O
use	O
the	O
convention	O
that	O
cycles	O
around	O
and	O
refers	O
to	O
when	O
.	O

Let	O
,	O
and	O
.	O

We	O
can	O
write	O
the	O
joint	B-Method
update	E-Method
for	O
all	O
as	O
Restrict	O
the	O
update	O
to	O
define	O
a	O
contraction	O
mapping	O
in	O
the	O
Euclidean	O
metric	O
.	O

This	O
means	O
that	O
there	O
is	O
some	O
such	O
that	O
for	O
any	O
,	O
or	O
in	O
other	O
words	O
,	O
We	O
can	O
immediately	O
see	O
that	O
this	O
implies	O
that	O
for	O
each	O
by	O
letting	O
be	O
the	O
elementary	O
vector	O
that	O
is	O
all	O
zero	O
except	O
for	O
a	O
1	O
in	O
position	O
and	O
letting	O
be	O
the	O
all	O
zeros	O
vector	O
.	O

Expanding	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
we	O
get	O
In	O
the	O
GNN	B-Method
model	E-Method
,	O
node	O
label	O
controls	O
which	O
values	O
of	O
and	O
are	O
used	O
during	O
the	O
propagation	S-Task
.	O

Looking	O
at	O
this	O
expansion	O
and	O
noting	O
that	O
for	O
all	O
,	O
we	O
see	O
that	O
information	O
about	O
labels	O
of	O
nodes	O
away	O
will	O
decay	O
at	O
a	O
rate	O
of	O
.	O

Thus	O
,	O
at	O
least	O
in	O
this	O
simple	O
case	O
,	O
the	O
restriction	O
that	O
be	O
a	O
contraction	O
means	O
that	O
it	O
is	O
not	O
able	O
to	O
maintain	O
long	O
-	O
range	O
dependencies	O
.	O

subsection	O
:	O
Nonlinear	B-Task
Case	E-Task
The	O
same	O
analysis	O
can	O
be	O
applied	O
to	O
a	O
nonlinear	B-Task
update	E-Task
,	O
i.e.	O
where	O
is	O
any	O
nonlinear	O
function	O
.	O

Then	O
.	O

Let	O
,	O
where	O
.	O

The	O
contraction	O
map	O
definition	O
Eq	O
.	O

[	O
reference	O
]	O
implies	O
that	O
each	O
entry	O
of	O
the	O
Jacobian	O
matrix	O
of	O
is	O
bounded	O
by	O
,	O
i.e.	O
To	O
see	O
this	O
,	O
consider	O
two	O
vectors	O
and	O
,	O
where	O
and	O
.	O

The	O
definition	O
in	O
(	O
[	O
reference	O
]	O
)	O
implies	O
that	O
for	O
all	O
,	O
Therefore	O
where	O
the	O
left	O
hand	O
side	O
is	O
by	O
definition	O
as	O
.	O

When	O
,	O
Also	O
,	O
because	O
of	O
the	O
special	O
cycle	O
graph	O
structure	O
,	O
for	O
all	O
other	O
s	O
we	O
have	O
.	O

Applying	O
this	O
to	O
the	O
update	O
at	O
timestep	O
,	O
we	O
get	O
Now	O
let	O
’s	O
see	O
how	O
a	O
change	O
in	O
could	O
affect	O
.	O

Using	O
the	O
chain	O
rule	O
and	O
the	O
special	O
graph	O
structure	O
,	O
we	O
have	O
As	O
,	O
this	O
derivative	O
will	O
approach	O
0	O
exponentially	O
fast	O
as	O
grows	O
.	O

Intuitively	O
,	O
this	O
means	O
that	O
the	O
impact	O
one	O
node	O
has	O
on	O
another	O
node	O
far	O
away	O
will	O
decay	O
exponetially	O
,	O
therefore	O
making	O
it	O
difficult	O
to	O
model	O
long	O
range	O
dependencies	O
.	O

appendix	O
:	O
Why	O
are	O
RNN	S-Method
and	O
LSTM	S-Method
so	O
Bad	O
on	O
the	O
Sequence	B-Task
Prediction	I-Task
Tasks	E-Task
?	O
RNN	S-Method
and	O
LSTM	S-Method
performance	O
on	O
the	O
sequence	B-Task
prediction	I-Task
tasks	E-Task
,	O
i.e.	O
bAbI	B-Task
task	E-Task
19	O
,	O
shortest	B-Task
path	E-Task
and	O
Eulerian	O
circuit	O
,	O
are	O
very	O
poor	O
compared	O
to	O
single	B-Task
output	I-Task
tasks	E-Task
.	O

The	O
Eulerian	B-Task
circuit	I-Task
task	E-Task
is	O
the	O
one	O
that	O
RNN	S-Method
and	O
LSTM	S-Method
fail	O
most	O
dramatically	O
.	O

A	O
typical	O
training	O
example	O
for	O
this	O
task	O
looks	O
like	O
the	O
following	O
,	O
This	O
describes	O
a	O
graph	O
with	O
two	O
cycles	O
3	O
-	O
7	O
-	O
5	O
-	O
8	O
-	O
6	O
and	O
1	O
-	O
2	O
-	O
4	O
-	O
0	O
,	O
where	O
3	O
-	O
7	O
-	O
5	O
-	O
8	O
-	O
6	O
is	O
the	O
target	O
cycle	O
and	O
1	O
-	O
2	O
-	O
4	O
-	O
0	O
is	O
a	O
smaller	O
distractor	O
graph	O
.	O

All	O
edges	O
are	O
presented	O
twice	O
in	O
both	O
directions	O
for	O
symmetry	O
.	O

The	O
task	O
is	O
to	O
find	O
the	O
cycle	O
that	O
starts	O
with	O
the	O
given	O
two	O
nodes	O
and	O
in	O
the	O
direction	O
from	O
the	O
first	O
to	O
the	O
second	O
.	O

The	O
distractor	O
graph	O
is	O
added	O
to	O
increase	O
the	O
difficulty	O
of	O
this	O
task	O
,	O
this	O
also	O
makes	O
the	O
output	O
cycle	O
not	O
strictly	O
“	O
Eulerian	O
”	O
.	O

For	O
RNN	S-Method
and	O
LSTM	S-Method
the	O
above	O
training	O
example	O
is	O
further	O
transformed	O
into	O
a	O
sequence	O
of	O
tokens	O
,	O
Note	O
the	O
node	O
IDs	O
here	O
are	O
different	O
from	O
the	O
ones	O
in	O
the	O
original	O
symbolic	O
data	O
.	O

The	O
RNN	S-Method
and	O
LSTM	S-Method
read	O
through	O
the	O
whole	O
sequence	O
,	O
and	O
start	O
to	O
predict	O
the	O
first	O
output	O
when	O
reading	O
the	O
ans	O
token	O
.	O

Then	O
for	O
each	O
prediction	O
step	O
,	O
the	O
ans	O
token	O
is	O
fed	O
as	O
the	O
input	O
and	O
the	O
target	O
node	O
ID	O
(	O
treated	O
as	O
a	O
class	O
label	O
)	O
is	O
expected	O
as	O
the	O
output	O
.	O

In	O
this	O
current	O
setup	O
,	O
the	O
output	O
of	O
each	O
prediction	O
step	O
is	O
not	O
fed	O
as	O
the	O
input	O
for	O
the	O
next	O
.	O

Our	O
GGS	B-Method
-	I-Method
NN	I-Method
model	E-Method
uses	O
the	O
same	O
setup	O
,	O
where	O
the	O
output	O
of	O
one	O
step	O
is	O
not	O
used	O
as	O
input	O
to	O
the	O
next	O
,	O
only	O
the	O
predicted	O
node	O
annotations	O
carry	O
over	O
from	O
one	O
step	O
to	O
the	O
next	O
,	O
so	O
the	O
comparison	O
is	O
still	O
fair	O
for	O
RNN	S-Method
and	O
LSTM	S-Method
.	O

Changing	O
both	O
our	O
method	O
and	O
the	O
baselines	O
to	O
make	O
use	O
of	O
previous	O
predictions	O
is	O
left	O
as	O
future	O
work	O
.	O

From	O
this	O
example	O
we	O
can	O
see	O
that	O
the	O
sequences	O
the	O
RNN	S-Method
and	O
LSTM	S-Method
have	O
to	O
handle	O
is	O
quite	O
long	O
,	O
close	O
to	O
80	O
tokens	O
before	O
the	O
predictions	O
are	O
made	O
.	O

Some	O
predictions	O
really	O
depend	O
on	O
long	O
range	O
memory	O
,	O
for	O
example	O
the	O
first	O
edge	O
(	O
3	O
-	O
7	O
)	O
and	O
first	O
a	O
few	O
tokens	O
(	O
n4	O
e1	O
n8	O
)	O
in	O
the	O
sequence	O
are	O
needed	O
to	O
make	O
prediction	O
in	O
the	O
third	O
prediction	O
step	O
(	O
3	O
in	O
the	O
original	O
symbolic	O
data	O
,	O
and	O
4	O
in	O
the	O
tokenized	O
RNN	S-Method
data	O
)	O
.	O

Keeping	O
long	O
range	O
memory	O
in	O
RNNs	S-Method
is	O
challenging	O
,	O
LSTMs	S-Method
do	O
better	O
than	O
RNNs	S-Method
but	O
still	O
ca	O
n’t	O
completely	O
solve	O
the	O
problem	O
.	O

Another	O
challenge	O
about	O
this	O
task	O
is	O
the	O
output	O
sequence	O
does	O
not	O
appear	O
in	O
the	O
same	O
order	O
as	O
in	O
the	O
input	O
sequence	O
.	O

In	O
fact	O
,	O
the	O
data	O
has	O
no	O
sequential	O
nature	O
at	O
all	O
,	O
even	O
when	O
the	O
edges	O
are	O
randomly	O
permutated	O
,	O
the	O
target	O
output	O
sequence	O
should	O
not	O
change	O
.	O

The	O
same	O
applies	O
for	O
bAbI	B-Task
task	E-Task
19	O
and	O
the	O
shortest	B-Task
path	E-Task
task	O
.	O

GGS	B-Method
-	I-Method
NNs	E-Method
are	O
good	O
at	O
handling	O
this	O
type	O
of	O
“	O
static	O
”	O
data	O
,	O
while	O
RNN	S-Method
and	O
LSTM	S-Method
are	O
not	O
.	O

However	O
future	O
work	O
is	O
needed	O
to	O
determine	O
how	O
best	O
to	O
apply	O
GGS	B-Method
-	I-Method
NNs	E-Method
to	O
temporal	O
sequential	O
data	O
which	O
RNN	S-Method
and	O
LSTM	S-Method
are	O
good	O
at	O
.	O

This	O
is	O
one	O
limitation	O
of	O
the	O
GGS	B-Method
-	I-Method
NNs	E-Method
model	O
which	O
we	O
discussed	O
in	O
sec	O
:	O
discussion	O
.	O

appendix	O
:	O
Nested	B-Task
Prediction	E-Task
Details	O
Data	O
structures	O
like	O
list	O
of	O
lists	O
are	O
nested	O
data	O
structures	O
,	O
in	O
which	O
the	O
val	O
pointer	O
of	O
each	O
node	O
in	O
a	O
data	O
structure	O
points	O
to	O
another	O
data	O
structure	O
.	O

Such	O
data	O
structures	O
can	O
be	O
represented	O
in	O
separation	B-Method
logic	E-Method
by	O
allowing	O
predicates	O
to	O
be	O
nested	O
.	O

For	O
example	O
,	O
a	O
list	O
of	O
lists	O
can	O
be	O
represented	O
as	O
,	O
where	O
is	O
a	O
lambda	O
expression	O
and	O
says	O
that	O
for	O
each	O
node	O
in	O
the	O
list	O
from	O
to	O
,	O
its	O
val	O
pointer	O
satisfies	O
.	O

So	O
there	O
is	O
a	O
list	O
from	O
to	O
,	O
where	O
each	O
node	O
in	O
that	O
list	O
points	O
to	O
another	O
list	O
.	O

A	O
simple	O
list	O
without	O
nested	O
structures	O
can	O
be	O
represented	O
as	O
where	O
represents	O
an	O
empty	O
predicate	O
.	O

Note	O
that	O
unlike	O
the	O
non	O
-	O
nested	O
case	O
where	O
the	O
val	O
pointer	O
always	O
points	O
to	O
NULL	O
,	O
we	O
have	O
to	O
consider	O
the	O
val	O
pointers	O
here	O
in	O
order	O
to	O
describe	O
and	O
handle	O
nested	O
data	O
structures	O
.	O

To	O
make	O
our	O
GGS	B-Method
-	I-Method
NNs	E-Method
able	O
to	O
predict	O
nested	O
formulas	O
,	O
we	O
adapt	O
alg	S-Method
:	O
seplogic	B-Method
-	I-Method
prediction	E-Method
to	O
alg	O
:	O
seplogic	O
-	O
nesting	O
.	O

Where	O
an	O
outer	O
loop	O
goes	O
through	O
each	O
named	O
variable	O
once	O
and	O
generate	O
a	O
nested	O
predicate	O
with	O
the	O
node	O
associated	O
with	O
that	O
variable	O
as	O
the	O
active	O
node	O
.	O

The	O
nested	B-Method
prediction	I-Method
procedure	E-Method
handles	O
prediction	S-Task
similarly	O
as	O
in	O
alg	O
:	O
seplogic	B-Method
-	I-Method
prediction	E-Method
.	O

Before	O
calling	O
the	O
nested	B-Method
prediction	I-Method
procedure	E-Method
recursively	O
,	O
the	O
node	B-Method
annotation	I-Method
update	E-Method
in	O
line	O
[	O
reference	O
]	O
not	O
only	O
annotates	O
nodes	O
in	O
the	O
current	O
structure	O
as	O
“	O
is	O
-	O
explained	O
”	O
,	O
but	O
also	O
annotates	O
nodes	O
linked	O
to	O
via	O
the	O
“	O
val	O
”	O
pointer	O
from	O
all	O
nodes	O
in	O
the	O
current	O
structure	O
as	O
“	O
active	O
”	O
.	O

For	O
the	O
list	O
of	O
lists	O
example	O
,	O
after	O
predicting	O
“	O
”	O
,	O
the	O
annotation	O
step	O
annotates	O
all	O
nodes	O
in	O
the	O
list	O
from	O
to	O
as	O
“	O
is	O
-	O
explained	O
”	O
and	O
all	O
nodes	O
the	O
val	O
pointer	O
points	O
to	O
from	O
the	O
list	O
as	O
“	O
active	O
”	O
.	O

This	O
knowledge	O
is	O
not	O
hard	O
coded	O
into	O
the	O
algorithm	O
,	O
the	O
annotation	B-Method
model	E-Method
can	O
learn	O
this	O
behavior	O
from	O
data	O
.	O

[	O
1	O
]	O
OuterLoop	O
Graph	O
with	O
named	O
program	O
variables	O
compute	O
initial	O
labels	O
from	O
each	O
variable	O
name	O
node	O
associated	O
with	O
turn	O
on	O
“	O
active	O
”	O
bit	O
for	O
in	O
PredictNestedFormula	O
,	O
,	O
PredictNestedFormula	O
,	O
,	O
initialize	O
node	O
vectors	O
by	O
-	O
extending	O
quantifier	O
needed	O
Graph	O
-	O
level	B-Method
Classification	E-Method
(	O
†	O
)	O
fresh	O
existentially	O
quantified	O
variable	O
name	O
pick	O
node	O
Node	B-Method
Selection	E-Method
(	O
‡	O
)	O
turn	O
on	O
“	O
is	O
-	O
named	O
”	O
for	O
in	O
print	O
“	O
”	O
is	O
a	O
lambda	O
variable	O
name	O
print	O
“	O
”	O
pick	O
data	O
structure	O
predicate	O
Graph	O
-	O
level	O
Classification	S-Task
(	O
⋆	O
)	O
pick	O
list	O
end	O
node	O
Node	O
Selection	O
(	O
♡	O
)	O
get	O
variable	O
name	O
associated	O
with	O
print	O
“	O
”	O
print	O
“	O
”	O
print	O
“	O
”	O
update	O
node	O
annotations	O
in	O
Node	O
Annotation	O
(	O
♠	O
)	O
fresh	O
lambda	O
variable	O
name	O
PredictNestedFormula	O
,	O
,	O
Recursively	O
predict	O
all	O
nested	O
formulas	O
.	O

print	O
“	O
”	O
Nested	O
separation	B-Method
logic	E-Method
formula	O
prediction	O
procedure	O
document	O
:	O
Character	B-Method
-	I-Method
level	I-Method
Convolutional	I-Method
Networks	E-Method
for	O
Text	B-Task
Classification	E-Task
This	O
article	O
offers	O
an	O
empirical	O
exploration	O
on	O
the	O
use	O
of	O
character	B-Method
-	I-Method
level	I-Method
convolutional	I-Method
networks	E-Method
(	O
ConvNets	S-Method
)	O
for	O
text	B-Task
classification	E-Task
.	O

We	O
constructed	O
several	O
large	O
-	O
scale	O
datasets	O
to	O
show	O
that	O
character	B-Method
-	I-Method
level	I-Method
convolutional	I-Method
networks	E-Method
could	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
or	O
competitive	O
results	O
.	O

Comparisons	O
are	O
offered	O
against	O
traditional	O
models	O
such	O
as	O
bag	B-Method
of	I-Method
words	E-Method
,	O
n	B-Method
-	I-Method
grams	E-Method
and	O
their	O
TFIDF	B-Method
variants	E-Method
,	O
and	O
deep	B-Method
learning	I-Method
models	E-Method
such	O
as	O
word	O
-	O
based	O
ConvNets	S-Method
and	O
recurrent	B-Method
neural	I-Method
networks	E-Method
.	O

section	O
:	O
Introduction	O
Text	B-Task
classification	E-Task
is	O
a	O
classic	O
topic	O
for	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
in	O
which	O
one	O
needs	O
to	O
assign	O
predefined	O
categories	O
to	O
free	O
-	O
text	O
documents	O
.	O

The	O
range	O
of	O
text	B-Task
classification	E-Task
research	O
goes	O
from	O
designing	O
the	O
best	O
features	O
to	O
choosing	O
the	O
best	O
possible	O
machine	B-Method
learning	I-Method
classifiers	E-Method
.	O

To	O
date	O
,	O
almost	O
all	O
techniques	O
of	O
text	B-Task
classification	E-Task
are	O
based	O
on	O
words	O
,	O
in	O
which	O
simple	O
statistics	O
of	O
some	O
ordered	O
word	O
combinations	O
(	O
such	O
as	O
n	O
-	O
grams	O
)	O
usually	O
perform	O
the	O
best	O
.	O

On	O
the	O
other	O
hand	O
,	O
many	O
researchers	O
have	O
found	O
convolutional	B-Method
networks	E-Method
(	O
ConvNets	S-Method
)	O
are	O
useful	O
in	O
extracting	B-Task
information	E-Task
from	O
raw	O
signals	O
,	O
ranging	O
from	O
computer	B-Task
vision	I-Task
applications	E-Task
to	O
speech	B-Task
recognition	E-Task
and	O
others	O
.	O

In	O
particular	O
,	O
time	B-Method
-	I-Method
delay	I-Method
networks	E-Method
used	O
in	O
the	O
early	O
days	O
of	O
deep	B-Task
learning	I-Task
research	E-Task
are	O
essentially	O
convolutional	B-Method
networks	E-Method
that	O
model	O
sequential	O
data	O
.	O

In	O
this	O
article	O
we	O
explore	O
treating	O
text	O
as	O
a	O
kind	O
of	O
raw	O
signal	O
at	O
character	O
level	O
,	O
and	O
applying	O
temporal	O
(	O
one	O
-	O
dimensional	O
)	O
ConvNets	S-Method
to	O
it	O
.	O

For	O
this	O
article	O
we	O
only	O
used	O
a	O
classification	B-Task
task	E-Task
as	O
a	O
way	O
to	O
exemplify	O
ConvNets	S-Method
’	O
ability	O
to	O
understand	O
texts	O
.	O

Historically	O
we	O
know	O
that	O
ConvNets	S-Method
usually	O
require	O
large	O
-	O
scale	O
datasets	O
to	O
work	O
,	O
therefore	O
we	O
also	O
build	O
several	O
of	O
them	O
.	O

An	O
extensive	O
set	O
of	O
comparisons	O
is	O
offered	O
with	O
traditional	O
models	O
and	O
other	O
deep	B-Method
learning	I-Method
models	E-Method
.	O

Applying	O
convolutional	B-Method
networks	E-Method
to	O
text	B-Task
classification	E-Task
or	O
natural	B-Task
language	I-Task
processing	E-Task
at	O
large	O
was	O
explored	O
in	O
literature	O
.	O

It	O
has	O
been	O
shown	O
that	O
ConvNets	S-Method
can	O
be	O
directly	O
applied	O
to	O
distributed	B-Task
or	I-Task
discrete	I-Task
embedding	I-Task
of	I-Task
words	E-Task
,	O
without	O
any	O
knowledge	O
on	O
the	O
syntactic	O
or	O
semantic	O
structures	O
of	O
a	O
language	O
.	O

These	O
approaches	O
have	O
been	O
proven	O
to	O
be	O
competitive	O
to	O
traditional	O
models	O
.	O

There	O
are	O
also	O
related	O
works	O
that	O
use	O
character	O
-	O
level	O
features	O
for	O
language	B-Task
processing	E-Task
.	O

These	O
include	O
using	O
character	B-Method
-	I-Method
level	I-Method
n	I-Method
-	I-Method
grams	E-Method
with	O
linear	B-Method
classifiers	E-Method
,	O
and	O
incorporating	O
character	O
-	O
level	O
features	O
to	O
ConvNets	S-Method
.	O

In	O
particular	O
,	O
these	O
ConvNet	B-Method
approaches	E-Method
use	O
words	O
as	O
a	O
basis	O
,	O
in	O
which	O
character	O
-	O
level	O
features	O
extracted	O
at	O
word	O
or	O
word	O
n	O
-	O
gram	O
level	O
form	O
a	O
distributed	B-Method
representation	E-Method
.	O

Improvements	O
for	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	I-Task
tagging	E-Task
and	O
information	B-Task
retrieval	E-Task
were	O
observed	O
.	O

This	O
article	O
is	O
the	O
first	O
to	O
apply	O
ConvNets	S-Method
only	O
on	O
characters	O
.	O

We	O
show	O
that	O
when	O
trained	O
on	O
large	O
-	O
scale	O
datasets	O
,	O
deep	O
ConvNets	S-Method
do	O
not	O
require	O
the	O
knowledge	O
of	O
words	O
,	O
in	O
addition	O
to	O
the	O
conclusion	O
from	O
previous	O
research	O
that	O
ConvNets	S-Method
do	O
not	O
require	O
the	O
knowledge	O
about	O
the	O
syntactic	O
or	O
semantic	O
structure	O
of	O
a	O
language	O
.	O

This	O
simplification	O
of	O
engineering	S-Task
could	O
be	O
crucial	O
for	O
a	O
single	O
system	O
that	O
can	O
work	O
for	O
different	O
languages	O
,	O
since	O
characters	O
always	O
constitute	O
a	O
necessary	O
construct	O
regardless	O
of	O
whether	O
segmentation	O
into	O
words	O
is	O
possible	O
.	O

Working	O
on	O
only	O
characters	O
also	O
has	O
the	O
advantage	O
that	O
abnormal	O
character	O
combinations	O
such	O
as	O
misspellings	O
and	O
emoticons	O
may	O
be	O
naturally	O
learnt	O
.	O

section	O
:	O
Character	B-Method
-	I-Method
level	I-Method
Convolutional	I-Method
Networks	E-Method
In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
design	O
of	O
character	O
-	O
level	O
ConvNets	S-Method
for	O
text	B-Task
classification	E-Task
.	O

The	O
design	O
is	O
modular	O
,	O
where	O
the	O
gradients	O
are	O
obtained	O
by	O
back	B-Method
-	I-Method
propagation	E-Method
to	O
perform	O
optimization	S-Task
.	O

subsection	O
:	O
Key	O
Modules	O
The	O
main	O
component	O
is	O
the	O
temporal	B-Method
convolutional	I-Method
module	E-Method
,	O
which	O
simply	O
computes	O
a	O
1	B-Method
-	I-Method
D	I-Method
convolution	E-Method
.	O

Suppose	O
we	O
have	O
a	O
discrete	O
input	O
function	O
and	O
a	O
discrete	B-Method
kernel	I-Method
function	E-Method
.	O

The	O
convolution	O
between	O
and	O
with	O
stride	O
is	O
defined	O
as	O
where	O
is	O
an	O
offset	O
constant	O
.	O

Just	O
as	O
in	O
traditional	O
convolutional	B-Method
networks	E-Method
in	O
vision	S-Task
,	O
the	O
module	O
is	O
parameterized	O
by	O
a	O
set	O
of	O
such	O
kernel	B-Method
functions	E-Method
which	O
we	O
call	O
weights	O
,	O
on	O
a	O
set	O
of	O
inputs	O
and	O
outputs	O
.	O

We	O
call	O
each	O
(	O
or	O
)	O
input	O
(	O
or	O
output	O
)	O
features	O
,	O
and	O
(	O
or	O
)	O
input	O
(	O
or	O
output	O
)	O
feature	O
size	O
.	O

The	O
outputs	O
is	O
obtained	O
by	O
a	O
sum	O
over	O
of	O
the	O
convolutions	O
between	O
and	O
.	O

One	O
key	O
module	O
that	O
helped	O
us	O
to	O
train	O
deeper	B-Method
models	E-Method
is	O
temporal	B-Method
max	I-Method
-	I-Method
pooling	E-Method
.	O

It	O
is	O
the	O
1	O
-	O
D	O
version	O
of	O
the	O
max	B-Method
-	I-Method
pooling	I-Method
module	E-Method
used	O
in	O
computer	B-Task
vision	E-Task
.	O

Given	O
a	O
discrete	O
input	O
function	O
,	O
the	O
max	B-Method
-	I-Method
pooling	I-Method
function	E-Method
of	O
is	O
defined	O
as	O
where	O
is	O
an	O
offset	O
constant	O
.	O

This	O
very	O
pooling	B-Method
module	E-Method
enabled	O
us	O
to	O
train	O
ConvNets	S-Method
deeper	O
than	O
6	O
layers	O
,	O
where	O
all	O
others	O
fail	O
.	O

The	O
analysis	O
by	O
might	O
shed	O
some	O
light	O
on	O
this	O
.	O

The	O
non	O
-	O
linearity	O
used	O
in	O
our	O
model	O
is	O
the	O
rectifier	B-Method
or	I-Method
thresholding	I-Method
function	E-Method
,	O
which	O
makes	O
our	O
convolutional	B-Method
layers	E-Method
similar	O
to	O
rectified	B-Method
linear	I-Method
units	E-Method
(	O
ReLUs	S-Method
)	O
.	O

The	O
algorithm	O
used	O
is	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
with	O
a	O
minibatch	O
of	O
size	O
128	O
,	O
using	O
momentum	O
and	O
initial	O
step	O
size	O
which	O
is	O
halved	O
every	O
3	O
epoches	O
for	O
10	O
times	O
.	O

Each	O
epoch	O
takes	O
a	O
fixed	O
number	O
of	O
random	O
training	O
samples	O
uniformly	O
sampled	O
across	O
classes	O
.	O

This	O
number	O
will	O
later	O
be	O
detailed	O
for	O
each	O
dataset	O
sparately	O
.	O

The	O
implementation	O
is	O
done	O
using	O
Torch	B-Method
7	E-Method
.	O

subsection	O
:	O
Character	B-Task
quantization	E-Task
Our	O
models	O
accept	O
a	O
sequence	O
of	O
encoded	O
characters	O
as	O
input	O
.	O

The	O
encoding	O
is	O
done	O
by	O
prescribing	O
an	O
alphabet	O
of	O
size	O
for	O
the	O
input	O
language	O
,	O
and	O
then	O
quantize	O
each	O
character	O
using	O
1	B-Method
-	I-Method
of	I-Method
-	I-Method
encoding	E-Method
(	O
or	O
“	O
one	B-Method
-	I-Method
hot	I-Method
”	I-Method
encoding	E-Method
)	O
.	O

Then	O
,	O
the	O
sequence	O
of	O
characters	O
is	O
transformed	O
to	O
a	O
sequence	O
of	O
such	O
sized	O
vectors	O
with	O
fixed	O
length	O
.	O

Any	O
character	O
exceeding	O
length	O
is	O
ignored	O
,	O
and	O
any	O
characters	O
that	O
are	O
not	O
in	O
the	O
alphabet	O
including	O
blank	O
characters	O
are	O
quantized	O
as	O
all	O
-	O
zero	O
vectors	O
.	O

The	O
character	O
quantization	O
order	O
is	O
backward	O
so	O
that	O
the	O
latest	O
reading	O
on	O
characters	O
is	O
always	O
placed	O
near	O
the	O
begin	O
of	O
the	O
output	O
,	O
making	O
it	O
easy	O
for	O
fully	O
connected	O
layers	O
to	O
associate	O
weights	O
with	O
the	O
latest	O
reading	O
.	O

The	O
alphabet	O
used	O
in	O
all	O
of	O
our	O
models	O
consists	O
of	O
70	O
characters	O
,	O
including	O
26	O
english	O
letters	O
,	O
10	O
digits	O
,	O
33	O
other	O
characters	O
and	O
the	O
new	O
line	O
character	O
.	O

The	O
non	O
-	O
space	O
characters	O
are	O
:	O
Later	O
we	O
also	O
compare	O
with	O
models	O
that	O
use	O
a	O
different	O
alphabet	O
in	O
which	O
we	O
distinguish	O
between	O
upper	O
-	O
case	O
and	O
lower	O
-	O
case	O
letters	O
.	O

subsection	O
:	O
Model	O
Design	O
We	O
designed	O
2	O
ConvNets	S-Method
–	O
one	O
large	O
and	O
one	O
small	O
.	O

They	O
are	O
both	O
9	O
layers	O
deep	O
with	O
6	O
convolutional	B-Method
layers	E-Method
and	O
3	O
fully	B-Method
-	I-Method
connected	I-Method
layers	E-Method
.	O

Figure	O
[	O
reference	O
]	O
gives	O
an	O
illustration	O
.	O

The	O
input	O
have	O
number	O
of	O
features	O
equal	O
to	O
70	O
due	O
to	O
our	O
character	B-Method
quantization	I-Method
method	E-Method
,	O
and	O
the	O
input	O
feature	O
length	O
is	O
1014	O
.	O

It	O
seems	O
that	O
1014	O
characters	O
could	O
already	O
capture	O
most	O
of	O
the	O
texts	O
of	O
interest	O
.	O

We	O
also	O
insert	O
2	O
dropout	B-Method
modules	E-Method
in	O
between	O
the	O
3	O
fully	B-Method
-	I-Method
connected	I-Method
layers	E-Method
to	O
regularize	O
.	O

They	O
have	O
dropout	B-Metric
probability	E-Metric
of	O
0.5	O
.	O

Table	O
[	O
reference	O
]	O
lists	O
the	O
configurations	O
for	O
convolutional	B-Method
layers	E-Method
,	O
and	O
table	O
[	O
reference	O
]	O
lists	O
the	O
configurations	O
for	O
fully	B-Method
-	I-Method
connected	I-Method
(	I-Method
linear	I-Method
)	I-Method
layers	E-Method
.	O

We	O
initialize	O
the	O
weights	O
using	O
a	O
Gaussian	B-Method
distribution	E-Method
.	O

The	O
mean	O
and	O
standard	O
deviation	O
used	O
for	O
initializing	O
the	O
large	B-Method
model	E-Method
is	O
and	O
small	B-Method
model	E-Method
.	O

For	O
different	O
problems	O
the	O
input	O
lengths	O
may	O
be	O
different	O
(	O
for	O
example	O
in	O
our	O
case	O
)	O
,	O
and	O
so	O
are	O
the	O
frame	O
lengths	O
.	O

From	O
our	O
model	O
design	O
,	O
it	O
is	O
easy	O
to	O
know	O
that	O
given	O
input	O
length	O
,	O
the	O
output	O
frame	O
length	O
after	O
the	O
last	O
convolutional	B-Method
layer	E-Method
(	O
but	O
before	O
any	O
of	O
the	O
fully	O
-	O
connected	O
layers	O
)	O
is	O
.	O

This	O
number	O
multiplied	O
with	O
the	O
frame	O
size	O
at	O
layer	O
6	O
will	O
give	O
the	O
input	O
dimension	O
the	O
first	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
accepts	O
.	O

subsection	O
:	O
Data	B-Task
Augmentation	E-Task
using	O
Thesaurus	O
Many	O
researchers	O
have	O
found	O
that	O
appropriate	O
data	B-Method
augmentation	I-Method
techniques	E-Method
are	O
useful	O
for	O
controlling	O
generalization	B-Task
error	E-Task
for	O
deep	B-Method
learning	I-Method
models	E-Method
.	O

These	O
techniques	O
usually	O
work	O
well	O
when	O
we	O
could	O
find	O
appropriate	O
invariance	O
properties	O
that	O
the	O
model	O
should	O
possess	O
.	O

In	O
terms	O
of	O
texts	O
,	O
it	O
is	O
not	O
reasonable	O
to	O
augment	O
the	O
data	O
using	O
signal	B-Method
transformations	E-Method
as	O
done	O
in	O
image	B-Task
or	I-Task
speech	I-Task
recognition	E-Task
,	O
because	O
the	O
exact	O
order	O
of	O
characters	O
may	O
form	O
rigorous	O
syntactic	O
and	O
semantic	O
meaning	O
.	O

Therefore	O
,	O
the	O
best	O
way	O
to	O
do	O
data	B-Task
augmentation	E-Task
would	O
have	O
been	O
using	O
human	O
rephrases	O
of	O
sentences	O
,	O
but	O
this	O
is	O
unrealistic	O
and	O
expensive	O
due	O
the	O
large	O
volume	O
of	O
samples	O
in	O
our	O
datasets	O
.	O

As	O
a	O
result	O
,	O
the	O
most	O
natural	O
choice	O
in	O
data	B-Task
augmentation	E-Task
for	O
us	O
is	O
to	O
replace	O
words	O
or	O
phrases	O
with	O
their	O
synonyms	O
.	O

We	O
experimented	O
data	B-Task
augmentation	E-Task
by	O
using	O
an	O
English	O
thesaurus	O
,	O
which	O
is	O
obtained	O
from	O
the	O
mytheas	B-Method
component	E-Method
used	O
in	O
LibreOffice	O
project	O
.	O

That	O
thesaurus	O
in	O
turn	O
was	O
obtained	O
from	O
WordNet	S-Material
,	O
where	O
every	O
synonym	O
to	O
a	O
word	O
or	O
phrase	O
is	O
ranked	O
by	O
the	O
semantic	O
closeness	O
to	O
the	O
most	O
frequently	O
seen	O
meaning	O
.	O

To	O
decide	O
on	O
how	O
many	O
words	O
to	O
replace	O
,	O
we	O
extract	O
all	O
replaceable	O
words	O
from	O
the	O
given	O
text	O
and	O
randomly	O
choose	O
of	O
them	O
to	O
be	O
replaced	O
.	O

The	O
probability	O
of	O
number	O
is	O
determined	O
by	O
a	O
geometric	B-Method
distribution	E-Method
with	O
parameter	O
in	O
which	O
.	O

The	O
index	O
of	O
the	O
synonym	O
chosen	O
given	O
a	O
word	O
is	O
also	O
determined	O
by	O
a	O
another	O
geometric	B-Method
distribution	E-Method
in	O
which	O
.	O

This	O
way	O
,	O
the	O
probability	O
of	O
a	O
synonym	O
chosen	O
becomes	O
smaller	O
when	O
it	O
moves	O
distant	O
from	O
the	O
most	O
frequently	O
seen	O
meaning	O
.	O

We	O
will	O
report	O
the	O
results	O
using	O
this	O
new	O
data	B-Method
augmentation	I-Method
technique	E-Method
with	O
and	O
.	O

section	O
:	O
Comparison	O
Models	O
To	O
offer	O
fair	O
comparisons	O
to	O
competitive	O
models	O
,	O
we	O
conducted	O
a	O
series	O
of	O
experiments	O
with	O
both	O
traditional	B-Method
and	I-Method
deep	I-Method
learning	I-Method
methods	E-Method
.	O

We	O
tried	O
our	O
best	O
to	O
choose	O
models	O
that	O
can	O
provide	O
comparable	O
and	O
competitive	O
results	O
,	O
and	O
the	O
results	O
are	O
reported	O
faithfully	O
without	O
any	O
model	B-Method
selection	E-Method
.	O

subsection	O
:	O
Traditional	O
Methods	O
We	O
refer	O
to	O
traditional	O
methods	O
as	O
those	O
that	O
using	O
a	O
hand	B-Method
-	I-Method
crafted	I-Method
feature	I-Method
extractor	E-Method
and	O
a	O
linear	B-Method
classifier	E-Method
.	O

The	O
classifier	S-Method
used	O
is	O
a	O
multinomial	B-Method
logistic	I-Method
regression	E-Method
in	O
all	O
these	O
models	O
.	O

Bag	O
-	O
of	O
-	O
words	O
and	O
its	O
TFIDF	S-Method
.	O

For	O
each	O
dataset	O
,	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
model	E-Method
is	O
constructed	O
by	O
selecting	O
50	O
,	O
000	O
most	O
frequent	O
words	O
from	O
the	O
training	O
subset	O
.	O

For	O
the	O
normal	B-Task
bag	I-Task
-	I-Task
of	I-Task
-	I-Task
words	E-Task
,	O
we	O
use	O
the	O
counts	O
of	O
each	O
word	O
as	O
the	O
features	O
.	O

For	O
the	O
TFIDF	B-Method
(	I-Method
term	I-Method
-	I-Method
frequency	I-Method
inverse	I-Method
-	I-Method
document	I-Method
-	I-Method
frequency	I-Method
)	I-Method
version	E-Method
,	O
we	O
use	O
the	O
counts	O
as	O
the	O
term	O
-	O
frequency	O
.	O

The	O
inverse	O
document	O
frequency	O
is	O
the	O
logarithm	O
of	O
the	O
division	O
between	O
total	O
number	O
of	O
samples	O
and	O
number	O
of	O
samples	O
with	O
the	O
word	O
in	O
the	O
training	O
subset	O
.	O

The	O
features	O
are	O
normalized	O
by	O
dividing	O
the	O
largest	O
feature	O
value	O
.	O

Bag	B-Method
-	I-Method
of	I-Method
-	I-Method
ngrams	E-Method
and	O
its	O
TFIDF	S-Method
.	O

The	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
ngrams	I-Method
models	E-Method
are	O
constructed	O
by	O
selecting	O
the	O
500	O
,	O
000	O
most	O
frequent	O
n	O
-	O
grams	O
(	O
up	O
to	O
5	O
-	O
grams	O
)	O
from	O
the	O
training	O
subset	O
for	O
each	O
dataset	O
.	O

The	O
feature	O
values	O
are	O
computed	O
the	O
same	O
way	O
as	O
in	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
model	E-Method
.	O

Bag	B-Method
-	I-Method
of	I-Method
-	I-Method
means	E-Method
on	O
word	B-Method
embedding	E-Method
.	O

We	O
also	O
have	O
an	O
experimental	O
model	O
that	O
uses	O
k	B-Method
-	I-Method
means	E-Method
on	O
word2vec	S-Method
learnt	O
from	O
the	O
training	O
subset	O
of	O
each	O
dataset	O
,	O
and	O
then	O
use	O
these	O
learnt	O
means	O
as	O
representatives	O
of	O
the	O
clustered	O
words	O
.	O

We	O
take	O
into	O
consideration	O
all	O
the	O
words	O
that	O
appeared	O
more	O
than	O
5	O
times	O
in	O
the	O
training	O
subset	O
.	O

The	O
dimension	O
of	O
the	O
embedding	O
is	O
300	O
.	O

The	O
bag	O
-	O
of	O
-	O
means	O
features	O
are	O
computed	O
the	O
same	O
way	O
as	O
in	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
model	E-Method
.	O

The	O
number	O
of	O
means	O
is	O
5000	O
.	O

subsection	O
:	O
Deep	B-Method
Learning	I-Method
Methods	E-Method
Recently	O
deep	B-Method
learning	I-Method
methods	E-Method
have	O
started	O
to	O
be	O
applied	O
to	O
text	B-Task
classification	E-Task
.	O

We	O
choose	O
two	O
simple	O
and	O
representative	O
models	O
for	O
comparison	O
,	O
in	O
which	O
one	O
is	O
word	B-Method
-	I-Method
based	I-Method
ConvNet	E-Method
and	O
the	O
other	O
a	O
simple	O
long	B-Method
-	I-Method
short	I-Method
term	I-Method
memory	E-Method
(	O
LSTM	S-Method
)	O
recurrent	O
neural	O
network	O
model	O
.	O

Word	O
-	O
based	O
ConvNets	S-Method
.	O

Among	O
the	O
large	O
number	O
of	O
recent	O
works	O
on	O
word	O
-	O
based	O
ConvNets	S-Method
for	O
text	B-Task
classification	E-Task
,	O
one	O
of	O
the	O
differences	O
is	O
the	O
choice	O
of	O
using	O
pretrained	O
or	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
learned	I-Method
word	I-Method
representations	E-Method
.	O

We	O
offer	O
comparisons	O
with	O
both	O
using	O
the	O
pretrained	B-Method
word2vec	I-Method
embedding	E-Method
and	O
using	O
lookup	B-Method
tables	E-Method
.	O

The	O
embedding	B-Metric
size	E-Metric
is	O
300	O
in	O
both	O
cases	O
,	O
in	O
the	O
same	O
way	O
as	O
our	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
means	I-Method
model	E-Method
.	O

To	O
ensure	O
fair	O
comparison	O
,	O
the	O
models	O
for	O
each	O
case	O
are	O
of	O
the	O
same	O
size	O
as	O
our	O
character	O
-	O
level	O
ConvNets	S-Method
,	O
in	O
terms	O
of	O
both	O
the	O
number	O
of	O
layers	O
and	O
each	O
layer	O
’s	O
output	O
size	O
.	O

Experiments	O
using	O
a	O
thesaurus	S-Method
for	O
data	B-Task
augmentation	E-Task
are	O
also	O
conducted	O
.	O

Long	B-Task
-	I-Task
short	I-Task
term	I-Task
memory	E-Task
.	O

We	O
also	O
offer	O
a	O
comparison	O
with	O
a	O
recurrent	B-Method
neural	I-Method
network	I-Method
model	E-Method
,	O
namely	O
long	B-Method
-	I-Method
short	I-Method
term	I-Method
memory	E-Method
(	O
LSTM	S-Method
)	O
.	O

The	O
LSTM	S-Method
model	O
used	O
in	O
our	O
case	O
is	O
word	B-Task
-	I-Task
based	E-Task
,	O
using	O
pretrained	B-Method
word2vec	I-Method
embedding	E-Method
of	O
size	O
300	O
as	O
in	O
previous	O
models	O
.	O

The	O
model	O
is	O
formed	O
by	O
taking	O
mean	O
of	O
the	O
outputs	O
of	O
all	O
LSTM	S-Method
cells	O
to	O
form	O
a	O
feature	O
vector	O
,	O
and	O
then	O
using	O
multinomial	B-Method
logistic	I-Method
regression	E-Method
on	O
this	O
feature	O
vector	O
.	O

The	O
output	O
dimension	O
is	O
512	O
.	O

The	O
variant	O
of	O
LSTM	S-Method
we	O
used	O
is	O
the	O
common	O
“	B-Method
vanilla	I-Method
”	I-Method
architecture	E-Method
.	O

We	O
also	O
used	O
gradient	B-Method
clipping	E-Method
in	O
which	O
the	O
gradient	O
norm	O
is	O
limited	O
to	O
5	O
.	O

Figure	O
[	O
reference	O
]	O
gives	O
an	O
illustration	O
.	O

subsection	O
:	O
Choice	O
of	O
Alphabet	O
For	O
the	O
alphabet	O
of	O
English	O
,	O
one	O
apparent	O
choice	O
is	O
whether	O
to	O
distinguish	O
between	O
upper	O
-	O
case	O
and	O
lower	O
-	O
case	O
letters	O
.	O

We	O
report	O
experiments	O
on	O
this	O
choice	O
and	O
observed	O
that	O
it	O
usually	O
(	O
but	O
not	O
always	O
)	O
gives	O
worse	O
results	O
when	O
such	O
distinction	O
is	O
made	O
.	O

One	O
possible	O
explanation	O
might	O
be	O
that	O
semantics	O
do	O
not	O
change	O
with	O
different	O
letter	O
cases	O
,	O
therefore	O
there	O
is	O
a	O
benefit	O
of	O
regularization	O
.	O

section	O
:	O
Large	O
-	O
scale	O
Datasets	O
and	O
Results	O
Previous	O
research	O
on	O
ConvNets	S-Method
in	O
different	O
areas	O
has	O
shown	O
that	O
they	O
usually	O
work	O
well	O
with	O
large	O
-	O
scale	O
datasets	O
,	O
especially	O
when	O
the	O
model	O
takes	O
in	O
low	O
-	O
level	O
raw	O
features	O
like	O
characters	O
in	O
our	O
case	O
.	O

However	O
,	O
most	O
open	O
datasets	O
for	O
text	B-Task
classification	E-Task
are	O
quite	O
small	O
,	O
and	O
large	O
-	O
scale	O
datasets	O
are	O
splitted	O
with	O
a	O
significantly	O
smaller	O
training	O
set	O
than	O
testing	O
.	O

Therefore	O
,	O
instead	O
of	O
confusing	O
our	O
community	O
more	O
by	O
using	O
them	O
,	O
we	O
built	O
several	O
large	O
-	O
scale	O
datasets	O
for	O
our	O
experiments	O
,	O
ranging	O
from	O
hundreds	O
of	O
thousands	O
to	O
several	O
millions	O
of	O
samples	O
.	O

Table	O
[	O
reference	O
]	O
is	O
a	O
summary	O
.	O

AG	B-Material
’s	I-Material
news	I-Material
corpus	E-Material
.	O

We	O
obtained	O
the	O
AG	B-Material
’s	I-Material
corpus	I-Material
of	I-Material
news	I-Material
article	E-Material
on	O
the	O
web	O
.	O

It	O
contains	O
496	O
,	O
835	O
categorized	O
news	B-Material
articles	E-Material
from	O
more	O
than	O
2000	O
news	O
sources	O
.	O

We	O
choose	O
the	O
4	O
largest	O
classes	O
from	O
this	O
corpus	O
to	O
construct	O
our	O
dataset	O
,	O
using	O
only	O
the	O
title	O
and	O
description	O
fields	O
.	O

The	O
number	O
of	O
training	O
samples	O
for	O
each	O
class	O
is	O
30	O
,	O
000	O
and	O
testing	O
1900	O
.	O

Sogou	B-Material
news	I-Material
corpus	E-Material
.	O

This	O
dataset	O
is	O
a	O
combination	O
of	O
the	O
SogouCA	S-Material
and	O
SogouCS	B-Material
news	I-Material
corpora	E-Material
,	O
containing	O
in	O
total	O
2	O
,	O
909	O
,	O
551	O
news	O
articles	O
in	O
various	O
topic	O
channels	O
.	O

We	O
then	O
labeled	O
each	O
piece	O
of	O
news	O
using	O
its	O
URL	O
,	O
by	O
manually	O
classifying	O
the	O
their	O
domain	O
names	O
.	O

This	O
gives	O
us	O
a	O
large	O
corpus	O
of	O
news	O
articles	O
labeled	O
with	O
their	O
categories	O
.	O

There	O
are	O
a	O
large	O
number	O
categories	O
but	O
most	O
of	O
them	O
contain	O
only	O
few	O
articles	O
.	O

We	O
choose	O
5	O
categories	O
–	O
“	O
sports	O
”	O
,	O
“	O
finance	S-Task
”	O
,	O
“	O
entertainment	O
”	O
,	O
“	O
automobile	O
”	O
and	O
“	O
technology	O
”	O
.	O

The	O
number	O
of	O
training	O
samples	O
selected	O
for	O
each	O
class	O
is	O
90	O
,	O
000	O
and	O
testing	O
12	O
,	O
000	O
.	O

Although	O
this	O
is	O
a	O
dataset	O
in	O
Chinese	O
,	O
we	O
used	O
pypinyin	B-Method
package	E-Method
combined	O
with	O
jieba	B-Method
Chinese	I-Method
segmentation	I-Method
system	E-Method
to	O
produce	O
Pinyin	S-Method
–	O
a	O
phonetic	O
romanization	O
of	O
Chinese	O
.	O

The	O
models	O
for	O
English	O
can	O
then	O
be	O
applied	O
to	O
this	O
dataset	O
without	O
change	O
.	O

The	O
fields	O
used	O
are	O
title	O
and	O
content	O
.	O

DBPedia	B-Material
ontology	I-Material
dataset	E-Material
.	O

DBpedia	S-Material
is	O
a	O
crowd	O
-	O
sourced	O
community	O
effort	O
to	O
extract	O
structured	O
information	O
from	O
Wikipedia	S-Material
.	O

The	O
DBpedia	S-Material
ontology	O
dataset	O
is	O
constructed	O
by	O
picking	O
14	O
non	O
-	O
overlapping	O
classes	O
from	O
DBpedia	S-Material
2014	O
.	O

From	O
each	O
of	O
these	O
14	O
ontology	O
classes	O
,	O
we	O
randomly	O
choose	O
40	O
,	O
000	O
training	O
samples	O
and	O
5	O
,	O
000	O
testing	O
samples	O
.	O

The	O
fields	O
we	O
used	O
for	O
this	O
dataset	O
contain	O
title	O
and	O
abstract	O
of	O
each	O
Wikipedia	O
article	O
.	O

Yelp	B-Material
reviews	E-Material
.	O

The	O
Yelp	S-Material
reviews	O
dataset	O
is	O
obtained	O
from	O
the	O
Yelp	S-Material
Dataset	O
Challenge	O
in	O
2015	O
.	O

This	O
dataset	O
contains	O
1	O
,	O
569	O
,	O
264	O
samples	O
that	O
have	O
review	O
texts	O
.	O

Two	O
classification	B-Task
tasks	E-Task
are	O
constructed	O
from	O
this	O
dataset	O
–	O
one	O
predicting	O
full	O
number	O
of	O
stars	O
the	O
user	O
has	O
given	O
,	O
and	O
the	O
other	O
predicting	O
a	O
polarity	O
label	O
by	O
considering	O
stars	O
1	O
and	O
2	O
negative	O
,	O
and	O
3	O
and	O
4	O
positive	O
.	O

The	O
full	O
dataset	O
has	O
130	O
,	O
000	O
training	O
samples	O
and	O
10	O
,	O
000	O
testing	O
samples	O
in	O
each	O
star	O
,	O
and	O
the	O
polarity	O
dataset	O
has	O
280	O
,	O
000	O
training	O
samples	O
and	O
19	O
,	O
000	O
test	O
samples	O
in	O
each	O
polarity	O
.	O

Yahoo	O
!	O
Answers	O
dataset	O
.	O

We	O
obtained	O
Yahoo	O
!	O
Answers	O
Comprehensive	O
Questions	O
and	O
Answers	O
version	O
1.0	O
dataset	O
through	O
the	O
Yahoo	O
!	O
Webscope	B-Method
program	E-Method
.	O

The	O
corpus	O
contains	O
4	O
,	O
483	O
,	O
032	O
questions	O
and	O
their	O
answers	O
.	O

We	O
constructed	O
a	O
topic	B-Method
classification	I-Method
dataset	E-Method
from	O
this	O
corpus	O
using	O
10	O
largest	O
main	O
categories	O
.	O

Each	O
class	O
contains	O
140	O
,	O
000	O
training	O
samples	O
and	O
5	O
,	O
000	O
testing	O
samples	O
.	O

The	O
fields	O
we	O
used	O
include	O
question	O
title	O
,	O
question	O
content	O
and	O
best	O
answer	O
.	O

Amazon	O
reviews	O
.	O

We	O
obtained	O
an	O
Amazon	B-Material
review	I-Material
dataset	E-Material
from	O
the	O
Stanford	B-Material
Network	I-Material
Analysis	I-Material
Project	E-Material
(	O
SNAP	S-Material
)	O
,	O
which	O
spans	O
18	O
years	O
with	O
34	O
,	O
686	O
,	O
770	O
reviews	O
from	O
6	O
,	O
643	O
,	O
669	O
users	O
on	O
2	O
,	O
441	O
,	O
053	O
products	O
.	O

Similarly	O
to	O
the	O
Yelp	S-Material
review	O
dataset	O
,	O
we	O
also	O
constructed	O
2	O
datasets	O
–	O
one	O
full	B-Method
score	I-Method
prediction	E-Method
and	O
another	O
polarity	B-Task
prediction	E-Task
.	O

The	O
full	O
dataset	O
contains	O
600	O
,	O
000	O
training	O
samples	O
and	O
130	O
,	O
000	O
testing	O
samples	O
in	O
each	O
class	O
,	O
whereas	O
the	O
polarity	O
dataset	O
contains	O
1	O
,	O
800	O
,	O
000	O
training	O
samples	O
and	O
200	O
,	O
000	O
testing	O
samples	O
in	O
each	O
polarity	O
sentiment	O
.	O

The	O
fields	O
used	O
are	O
review	O
title	O
and	O
review	O
content	O
.	O

Table	O
[	O
reference	O
]	O
lists	O
all	O
the	O
testing	O
errors	O
we	O
obtained	O
from	O
these	O
datasets	O
for	O
all	O
the	O
applicable	O
models	O
.	O

Note	O
that	O
since	O
we	O
do	O
not	O
have	O
a	O
Chinese	O
thesaurus	O
,	O
the	O
Sogou	B-Material
News	I-Material
dataset	E-Material
does	O
not	O
have	O
any	O
results	O
using	O
thesaurus	B-Method
augmentation	E-Method
.	O

We	O
labeled	O
the	O
best	O
result	O
in	O
blue	O
and	O
worse	O
result	O
in	O
red	O
.	O

section	O
:	O
Discussion	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.7	O
To	O
understand	O
the	O
results	O
in	O
table	O
[	O
reference	O
]	O
further	O
,	O
we	O
offer	O
some	O
empirical	O
analysis	O
in	O
this	O
section	O
.	O

To	O
facilitate	O
our	O
analysis	O
,	O
we	O
present	O
the	O
relative	B-Metric
errors	E-Metric
in	O
figure	O
[	O
reference	O
]	O
with	O
respect	O
to	O
comparison	O
models	O
.	O

Each	O
of	O
these	O
plots	O
is	O
computed	O
by	O
taking	O
the	O
difference	O
between	O
errors	O
on	O
comparison	B-Method
model	E-Method
and	O
our	O
character	B-Method
-	I-Method
level	I-Method
ConvNet	I-Method
model	E-Method
,	O
then	O
divided	O
by	O
the	O
comparison	B-Metric
model	I-Metric
error	E-Metric
.	O

All	O
ConvNets	S-Method
in	O
the	O
figure	O
are	O
the	O
large	B-Method
models	E-Method
with	O
thesaurus	B-Method
augmentation	E-Method
respectively	O
.	O

Character	B-Method
-	I-Method
level	I-Method
ConvNet	E-Method
is	O
an	O
effective	O
method	O
.	O

The	O
most	O
important	O
conclusion	O
from	O
our	O
experiments	O
is	O
that	O
character	O
-	O
level	O
ConvNets	S-Method
could	O
work	O
for	O
text	B-Task
classification	E-Task
without	O
the	O
need	O
for	O
words	O
.	O

This	O
is	O
a	O
strong	O
indication	O
that	O
language	O
could	O
also	O
be	O
thought	O
of	O
as	O
a	O
signal	O
no	O
different	O
from	O
any	O
other	O
kind	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
12	O
random	O
first	O
-	O
layer	O
patches	O
learnt	O
by	O
one	O
of	O
our	O
character	O
-	O
level	O
ConvNets	S-Method
for	O
DBPedia	B-Material
dataset	E-Material
.	O

Dataset	B-Metric
size	E-Metric
forms	O
a	O
dichotomy	O
between	O
traditional	O
and	O
ConvNets	S-Method
models	O
.	O

The	O
most	O
obvious	O
trend	O
coming	O
from	O
all	O
the	O
plots	O
in	O
figure	O
[	O
reference	O
]	O
is	O
that	O
the	O
larger	O
datasets	O
tend	O
to	O
perform	O
better	O
.	O

Traditional	O
methods	O
like	O
n	B-Method
-	I-Method
grams	I-Method
TFIDF	E-Method
remain	O
strong	O
candidates	O
for	O
dataset	O
of	O
size	O
up	O
to	O
several	O
hundreds	O
of	O
thousands	O
,	O
and	O
only	O
until	O
the	O
dataset	O
goes	O
to	O
the	O
scale	O
of	O
several	O
millions	O
do	O
we	O
observe	O
that	O
character	O
-	O
level	O
ConvNets	S-Method
start	O
to	O
do	O
better	O
.	O

ConvNets	S-Method
may	O
work	O
well	O
for	O
user	O
-	O
generated	O
data	O
.	O

User	O
-	O
generated	O
data	O
vary	O
in	O
the	O
degree	O
of	O
how	O
well	O
the	O
texts	O
are	O
curated	O
.	O

For	O
example	O
,	O
in	O
our	O
million	O
scale	O
datasets	O
,	O
Amazon	O
reviews	O
tend	O
to	O
be	O
raw	O
user	O
-	O
inputs	O
,	O
whereas	O
users	O
might	O
be	O
extra	O
careful	O
in	O
their	O
writings	O
on	O
Yahoo	O
!	O
Answers	O
.	O

Plots	O
comparing	O
word	B-Method
-	I-Method
based	I-Method
deep	I-Method
models	E-Method
(	O
figures	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
show	O
that	O
character	O
-	O
level	O
ConvNets	S-Method
work	O
better	O
for	O
less	O
curated	O
user	O
-	O
generated	O
texts	O
.	O

This	O
property	O
suggests	O
that	O
ConvNets	S-Method
may	O
have	O
better	O
applicability	O
to	O
real	B-Task
-	I-Task
world	I-Task
scenarios	E-Task
.	O

However	O
,	O
further	O
analysis	O
is	O
needed	O
to	O
validate	O
the	O
hypothesis	O
that	O
ConvNets	S-Method
are	O
truly	O
good	O
at	O
identifying	O
exotic	O
character	O
combinations	O
such	O
as	O
misspellings	O
and	O
emoticons	O
,	O
as	O
our	O
experiments	O
alone	O
do	O
not	O
show	O
any	O
explicit	O
evidence	O
.	O

Choice	O
of	O
alphabet	O
makes	O
a	O
difference	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
that	O
changing	O
the	O
alphabet	O
by	O
distinguishing	O
between	O
uppercase	O
and	O
lowercase	O
letters	O
could	O
make	O
a	O
difference	O
.	O

For	O
million	O
-	O
scale	O
datasets	O
,	O
it	O
seems	O
that	O
not	O
making	O
such	O
distinction	O
usually	O
works	O
better	O
.	O

One	O
possible	O
explanation	O
is	O
that	O
there	O
is	O
a	O
regularization	O
effect	O
,	O
but	O
this	O
is	O
to	O
be	O
validated	O
.	O

Semantics	B-Task
of	I-Task
tasks	E-Task
may	O
not	O
matter	O
.	O

Our	O
datasets	O
consist	O
of	O
two	O
kinds	O
of	O
tasks	O
:	O
sentiment	B-Task
analysis	E-Task
(	O
Yelp	S-Material
and	O
Amazon	B-Material
reviews	E-Material
)	O
and	O
topic	B-Task
classification	E-Task
(	O
all	O
others	O
)	O
.	O

This	O
dichotomy	O
in	O
task	O
semantics	O
does	O
not	O
seem	O
to	O
play	O
a	O
role	O
in	O
deciding	O
which	O
method	O
is	O
better	O
.	O

Bag	B-Method
-	I-Method
of	I-Method
-	I-Method
means	E-Method
is	O
a	O
misuse	O
of	O
word2vec	S-Method
[	O
]	O
.	O

One	O
of	O
the	O
most	O
obvious	O
facts	O
one	O
could	O
observe	O
from	O
table	O
[	O
reference	O
]	O
and	O
figure	O
[	O
reference	O
]	O
is	O
that	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
means	I-Method
model	E-Method
performs	O
worse	O
in	O
every	O
case	O
.	O

Comparing	O
with	O
traditional	O
models	O
,	O
this	O
suggests	O
such	O
a	O
simple	O
use	O
of	O
a	O
distributed	B-Method
word	I-Method
representation	E-Method
may	O
not	O
give	O
us	O
an	O
advantage	O
to	O
text	B-Task
classification	E-Task
.	O

However	O
,	O
our	O
experiments	O
does	O
not	O
speak	O
for	O
any	O
other	O
language	B-Task
processing	I-Task
tasks	E-Task
or	O
use	O
of	O
word2vec	O
in	O
any	O
other	O
way	O
.	O

There	O
is	O
no	O
free	O
lunch	O
.	O

Our	O
experiments	O
once	O
again	O
verifies	O
that	O
there	O
is	O
not	O
a	O
single	O
machine	B-Method
learning	I-Method
model	E-Method
that	O
can	O
work	O
for	O
all	O
kinds	O
of	O
datasets	O
.	O

The	O
factors	O
discussed	O
in	O
this	O
section	O
could	O
all	O
play	O
a	O
role	O
in	O
deciding	O
which	O
method	O
is	O
the	O
best	O
for	O
some	O
specific	O
application	O
.	O

section	O
:	O
Conclusion	O
and	O
Outlook	O
This	O
article	O
offers	O
an	O
empirical	O
study	O
on	O
character	B-Method
-	I-Method
level	I-Method
convolutional	I-Method
networks	E-Method
for	O
text	B-Task
classification	E-Task
.	O

We	O
compared	O
with	O
a	O
large	O
number	O
of	O
traditional	O
and	B-Method
deep	I-Method
learning	I-Method
models	E-Method
using	O
several	O
large	O
-	O
scale	O
datasets	O
.	O

On	O
one	O
hand	O
,	O
analysis	O
shows	O
that	O
character	B-Method
-	I-Method
level	I-Method
ConvNet	E-Method
is	O
an	O
effective	O
method	O
.	O

On	O
the	O
other	O
hand	O
,	O
how	O
well	O
our	O
model	O
performs	O
in	O
comparisons	O
depends	O
on	O
many	O
factors	O
,	O
such	O
as	O
dataset	O
size	O
,	O
whether	O
the	O
texts	O
are	O
curated	O
and	O
choice	O
of	O
alphabet	O
.	O

In	O
the	O
future	O
,	O
we	O
hope	O
to	O
apply	O
character	O
-	O
level	O
ConvNets	S-Method
for	O
a	O
broader	O
range	O
of	O
language	B-Task
processing	I-Task
tasks	E-Task
especially	O
when	O
structured	O
outputs	O
are	O
needed	O
.	O

section	O
:	O
Acknowledgement	O
We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
2	O
Tesla	O
K40	O
GPUs	O
used	O
for	O
this	O
research	O
.	O

We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
Amazon.com	O
Inc	O
for	O
an	O
AWS	O
in	O
Education	O
Research	O
grant	O
used	O
for	O
this	O
research	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Learning	O
a	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
for	O
Semantic	B-Task
Segmentation	E-Task
Most	O
existing	O
methods	O
of	O
semantic	B-Task
segmentation	E-Task
still	O
suffer	O
from	O
two	O
aspects	O
of	O
challenges	O
:	O
intra	B-Metric
-	I-Metric
class	I-Metric
inconsistency	E-Metric
and	O
inter	O
-	O
class	O
indistinction	O
.	O

To	O
tackle	O
these	O
two	O
problems	O
,	O
we	O
propose	O
a	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
(	O
DFN	S-Method
)	O
,	O
which	O
contains	O
two	O
sub	B-Method
-	I-Method
networks	E-Method
:	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
.	O

Specifically	O
,	O
to	O
handle	O
the	O
intra	B-Task
-	I-Task
class	I-Task
inconsistency	I-Task
problem	E-Task
,	O
we	O
specially	O
design	O
a	O
Smooth	O
Network	O
with	O
Channel	B-Method
Attention	I-Method
Block	E-Method
and	O
global	B-Method
average	I-Method
pooling	E-Method
to	O
select	O
the	O
more	O
discriminative	O
features	O
.	O

Furthermore	O
,	O
we	O
propose	O
a	O
Border	B-Method
Network	E-Method
to	O
make	O
the	O
bilateral	O
features	O
of	O
boundary	O
distinguishable	O
with	O
deep	B-Method
semantic	I-Method
boundary	I-Method
supervision	E-Method
.	O

Based	O
on	O
our	O
proposed	O
DFN	S-Method
,	O
we	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
86.2	O
%	O
mean	B-Metric
IOU	E-Metric
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
80.3	O
%	O
mean	B-Metric
IOU	E-Metric
on	O
Cityscapes	B-Material
dataset	E-Material
.	O

figures	O
/	O
section	O
:	O
Introduction	O
Semantic	B-Task
segmentation	E-Task
is	O
a	O
fundamental	O
technique	O
for	O
numerous	O
computer	B-Task
vision	I-Task
applications	E-Task
like	O
scene	B-Task
understanding	E-Task
,	O
human	B-Task
parsing	E-Task
and	O
autonomous	B-Task
driving	E-Task
.	O

With	O
the	O
recent	O
development	O
of	O
the	O
convolutional	B-Method
neural	I-Method
network	E-Method
,	O
especially	O
the	O
Fully	B-Method
Convolutional	I-Method
Network	E-Method
(	O
FCN	S-Method
)	O
,	O
a	O
lot	O
of	O
great	O
work	O
such	O
as	O
have	O
obtained	O
promising	O
results	O
on	O
the	O
benchmarks	O
.	O

However	O
,	O
the	O
features	O
learned	O
by	O
these	O
methods	O
are	O
usually	O
not	O
discriminative	O
to	O
differentiate	O
1	O
)	O
the	O
patches	O
which	O
share	O
the	O
same	O
semantic	O
label	O
but	O
different	O
appearances	O
,	O
named	O
intra	O
-	O
class	O
inconsistency	O
as	O
shown	O
in	O
the	O
first	O
row	O
of	O
Figure	O
[	O
reference	O
]	O
;	O
2	O
)	O
the	O
two	O
adjacent	O
patches	O
which	O
have	O
different	O
semantic	O
labels	O
but	O
with	O
similar	O
appearances	O
,	O
named	O
inter	O
-	O
class	O
indistinction	O
as	O
shown	O
in	O
the	O
second	O
row	O
of	O
Figure	O
[	O
reference	O
]	O
.	O

To	O
address	O
these	O
two	O
challenges	O
,	O
we	O
rethink	O
the	O
semantic	B-Task
segmentation	I-Task
task	E-Task
from	O
a	O
more	O
macroscopic	O
point	O
of	O
view	O
.	O

In	O
this	O
way	O
,	O
we	O
regard	O
the	O
semantic	B-Task
segmentation	E-Task
as	O
a	O
task	O
to	O
assign	O
a	O
consistent	O
semantic	O
label	O
to	O
a	O
category	O
of	O
things	O
,	O
rather	O
than	O
to	O
each	O
single	O
pixel	O
.	O

From	O
a	O
macroscopic	O
perspective	O
,	O
regarding	O
each	O
category	O
of	O
pixels	O
as	O
a	O
whole	O
,	O
inherently	O
considers	O
both	O
intra	O
-	O
class	O
consistency	O
and	O
inter	O
-	O
class	O
variation	O
.	O

It	O
means	O
that	O
the	O
task	O
demands	O
discriminative	O
features	O
.	O

To	O
this	O
end	O
,	O
we	O
present	O
a	O
novel	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
(	O
DFN	S-Method
)	O
to	O
learn	O
the	O
feature	B-Method
representation	E-Method
which	O
considers	O
both	O
the	O
“	O
intra	B-Metric
-	I-Metric
class	I-Metric
consistency	E-Metric
”	O
and	O
the	O
“	O
inter	O
-	O
class	O
distinction	O
”	O
.	O

Our	O
DFN	S-Method
involves	O
two	O
components	O
:	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
,	O
as	O
Figure	O
[	O
reference	O
]	O
illustrates	O
.	O

The	O
Smooth	B-Method
Network	E-Method
is	O
designed	O
to	O
address	O
the	O
intra	B-Task
-	I-Task
class	I-Task
inconsistency	I-Task
issue	E-Task
.	O

To	O
learn	O
a	O
robust	B-Method
feature	I-Method
representation	E-Method
for	O
intra	B-Task
-	I-Task
class	I-Task
consistency	E-Task
,	O
we	O
usually	O
consider	O
two	O
crucial	O
factors	O
.	O

On	O
the	O
one	O
hand	O
,	O
we	O
need	O
multi	O
-	O
scale	O
and	O
global	O
context	O
features	O
to	O
encode	O
the	O
local	O
and	O
global	O
information	O
.	O

For	O
example	O
,	O
the	O
small	O
white	O
patch	O
only	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
usually	O
can	O
not	O
predict	O
the	O
correct	O
category	O
due	O
to	O
the	O
lack	O
of	O
sufficient	O
context	O
information	O
.	O

On	O
the	O
other	O
hand	O
,	O
as	O
multi	O
-	O
scale	O
context	O
is	O
introduced	O
,	O
for	O
a	O
certain	O
scale	O
of	O
thing	O
,	O
the	O
features	O
have	O
different	O
extent	O
of	O
discrimination	O
,	O
some	O
of	O
which	O
may	O
predict	O
a	O
false	O
label	O
.	O

Therefore	O
,	O
it	O
is	O
necessary	O
to	O
select	O
the	O
discriminative	O
and	O
effective	O
features	O
.	O

Motivated	O
by	O
these	O
two	O
aspects	O
,	O
our	O
Smooth	B-Method
Network	E-Method
is	O
presented	O
based	O
on	O
the	O
U	O
-	O
shape	O
structure	O
to	O
capture	O
the	O
multi	O
-	O
scale	O
context	O
information	O
,	O
with	O
the	O
global	B-Method
average	I-Method
pooling	E-Method
to	O
capture	O
the	O
global	O
context	O
.	O

Also	O
,	O
we	O
propose	O
a	O
Channel	B-Method
Attention	I-Method
Block	E-Method
(	O
CAB	S-Method
)	O
,	O
which	O
utilizes	O
the	O
high	O
-	O
level	O
features	O
to	O
guide	O
the	O
selection	O
of	O
low	O
-	O
level	O
features	O
stage	O
-	O
by	O
-	O
stage	O
.	O

Border	B-Method
Network	E-Method
,	O
on	O
the	O
other	O
hand	O
,	O
tries	O
to	O
differentiate	O
the	O
adjacent	O
patches	O
with	O
similar	O
appearances	O
but	O
different	O
semantic	O
labels	O
.	O

Most	O
of	O
the	O
existing	O
approaches	O
consider	O
the	O
semantic	B-Task
segmentation	I-Task
task	E-Task
as	O
a	O
dense	B-Task
recognition	I-Task
problem	E-Task
,	O
which	O
usually	O
ignores	O
explicitly	O
modeling	O
the	O
inter	O
-	O
class	O
relationship	O
.	O

Consider	O
the	O
example	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)	O
,	O
if	O
more	O
and	O
more	O
global	O
context	O
is	O
integrated	O
into	O
the	O
classificiation	B-Method
process	E-Method
,	O
the	O
computer	O
case	O
next	O
to	O
the	O
monitor	O
can	O
be	O
easily	O
misclassified	O
as	O
a	O
monitor	O
due	O
to	O
the	O
similar	O
appearance	O
.	O

Thus	O
,	O
it	O
is	O
significant	O
to	O
explicitly	O
involve	O
the	O
semantic	O
boundary	O
to	O
guide	O
the	O
learning	B-Task
of	I-Task
the	I-Task
features	E-Task
.	O

It	O
can	O
amplify	O
the	O
variation	O
of	O
features	O
on	O
both	O
sides	O
.	O

In	O
our	O
Border	B-Method
Network	E-Method
,	O
we	O
integrate	O
semantic	O
boundary	O
loss	O
during	O
the	O
training	O
process	O
to	O
learn	O
the	O
discriminative	O
features	O
to	O
enlarge	O
the	O
“	O
inter	O
-	O
class	O
distinction	O
”	O
.	O

In	O
summary	O
,	O
there	O
are	O
four	O
contributions	O
in	O
our	O
paper	O
:	O
We	O
rethink	O
the	O
semantic	B-Task
segmentation	I-Task
task	E-Task
from	O
a	O
new	O
macroscopic	O
point	O
of	O
view	O
.	O

We	O
regard	O
the	O
semantic	B-Task
segmentation	E-Task
as	O
a	O
task	O
to	O
assign	O
a	O
consistent	O
semantic	O
label	O
to	O
one	O
category	O
of	O
things	O
,	O
not	O
just	O
at	O
the	O
pixel	O
level	O
.	O

We	O
propose	O
a	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
to	O
simultaneously	O
address	O
the	O
“	O
intra	B-Metric
-	I-Metric
class	I-Metric
consistency	I-Metric
”	E-Metric
and	O
“	O
inter	O
-	O
class	O
variation	O
”	O
issues	O
.	O

Experiments	O
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
Cityscapes	S-Material
datasets	O
validate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
algorithm	O
.	O

We	O
present	O
a	O
Smooth	B-Method
Network	E-Method
to	O
enhance	O
the	O
intra	O
-	O
class	O
consistency	O
with	O
the	O
global	O
context	O
and	O
the	O
Channel	B-Method
Attention	I-Method
Block	E-Method
.	O

We	O
design	O
a	O
bottom	B-Method
-	I-Method
up	I-Method
Border	I-Method
Network	E-Method
with	O
deep	O
supervision	O
to	O
enlarge	O
the	O
variation	O
of	O
features	O
on	O
both	O
sides	O
of	O
the	O
semantic	O
boundary	O
.	O

This	O
can	O
also	O
refine	O
the	O
semantic	O
boundary	O
of	O
prediction	S-Task
.	O

section	O
:	O
Related	O
Work	O
Recently	O
,	O
lots	O
of	O
approaches	O
based	O
on	O
FCN	S-Method
have	O
achieved	O
high	O
performance	O
on	O
different	O
benchmarks	O
.	O

Most	O
of	O
them	O
are	O
still	O
constrained	O
by	O
intra	O
-	O
class	O
inconsistency	O
and	O
inter	O
-	O
class	O
indistinction	O
issues	O
.	O

paragraph	O
:	O
Encoder	B-Method
-	I-Method
Decoder	E-Method
:	O
The	O
FCN	B-Method
model	E-Method
has	O
inherently	O
encoded	O
different	O
levels	O
of	O
feature	O
.	O

Naturally	O
,	O
some	O
methods	O
integrate	O
them	O
to	O
refine	O
the	O
final	O
prediction	S-Task
.	O

This	O
branch	O
of	O
methods	O
mainly	O
consider	O
how	O
to	O
recover	O
the	O
reduced	O
spatial	O
information	O
caused	O
by	O
consecutive	O
pooling	O
operator	O
or	O
convolution	O
with	O
stride	O
.	O

For	O
example	O
,	O
SegNet	S-Method
utilizes	O
the	O
saved	O
pool	O
indices	O
to	O
recover	O
the	O
reduced	O
spatial	O
information	O
.	O

U	B-Method
-	I-Method
net	E-Method
uses	O
the	O
skip	O
connection	O
,	O
while	O
the	O
Global	B-Method
Convolutional	I-Method
Network	E-Method
adapts	O
the	O
large	O
kernel	O
size	O
.	O

Besides	O
,	O
LRR	S-Method
adds	O
the	O
Laplacian	B-Method
Pyramid	I-Method
Reconstruction	I-Method
network	E-Method
,	O
while	O
RefineNet	S-Method
utilizes	O
multi	B-Method
-	I-Method
path	I-Method
refinement	I-Method
network	E-Method
.	O

However	O
,	O
this	O
type	O
of	O
architecture	O
ignores	O
the	O
global	O
context	O
.	O

In	O
addition	O
,	O
most	O
methods	O
of	O
this	O
type	O
are	O
just	O
summed	O
up	O
the	O
features	O
of	O
adjacent	O
stages	O
without	O
consideration	O
of	O
their	O
diverse	O
representation	O
.	O

This	O
leads	O
to	O
some	O
inconsistent	O
results	O
.	O

paragraph	O
:	O
Global	O
Context	O
:	O
Some	O
modern	O
methods	O
have	O
proven	O
the	O
effectiveness	O
of	O
global	B-Method
average	I-Method
pooling	E-Method
.	O

ParseNet	S-Method
firstly	O
applies	O
global	B-Method
average	I-Method
pooling	E-Method
in	O
the	O
semantic	B-Task
segmentation	I-Task
task	E-Task
.	O

Then	O
PSPNet	S-Method
and	O
Deeplab	B-Method
v3	E-Method
respectively	O
extend	O
it	O
to	O
the	O
Spatial	B-Method
Pyramid	I-Method
Pooling	E-Method
and	O
Atrous	B-Method
Spatial	I-Method
Pyramid	I-Method
Pooling	E-Method
,	O
resulting	O
in	O
great	O
performance	O
in	O
different	O
benchmarks	O
.	O

However	O
,	O
to	O
take	O
advantage	O
of	O
the	O
pyramid	B-Method
pooling	I-Method
module	E-Method
sufficiently	O
,	O
these	O
two	O
methods	O
adopt	O
the	O
base	B-Method
feature	I-Method
network	E-Method
to	O
8	O
times	O
down	O
-	O
sample	O
with	O
atrous	B-Method
convolution	E-Method
,	O
which	O
is	O
time	O
-	O
consuming	O
and	O
memory	O
intensive	O
.	O

paragraph	O
:	O
Attention	B-Method
Module	E-Method
:	O
Attention	O
is	O
helpful	O
to	O
focus	O
on	O
what	O
we	O
want	O
.	O

Recently	O
,	O
the	O
attention	B-Method
module	E-Method
becomes	O
increasingly	O
a	O
powerful	O
tool	O
for	O
deep	B-Method
neural	I-Method
networks	E-Method
.	O

The	O
method	O
in	O
pays	B-Task
attention	E-Task
to	O
different	O
scale	O
information	O
.	O

In	O
this	O
work	O
,	O
we	O
utilize	O
channel	O
attention	O
to	O
select	O
the	O
features	O
similar	O
to	O
SENet	O
.	O

paragraph	O
:	O
Semantic	B-Task
Boundary	I-Task
Detection	E-Task
:	O
Boundary	B-Task
detection	E-Task
is	O
a	O
fundamental	O
challenge	O
in	O
computer	B-Task
vision	E-Task
.	O

There	O
are	O
lots	O
of	O
specific	O
methods	O
proposed	O
for	O
the	O
task	O
of	O
boundary	B-Task
detection	E-Task
.	O

Most	O
of	O
these	O
methods	O
straightly	O
concatenate	O
the	O
different	O
level	O
of	O
features	O
to	O
extract	O
the	O
boundary	O
.	O

However	O
,	O
in	O
this	O
work	O
,	O
our	O
goal	O
is	O
to	O
obtain	O
the	O
features	O
with	O
inter	O
-	O
class	O
distinction	O
as	O
much	O
as	O
possible	O
with	O
accurate	O
boundary	O
supervision	O
.	O

Therefore	O
,	O
we	O
design	O
a	O
bottom	B-Method
-	I-Method
up	I-Method
structure	E-Method
to	O
optimize	O
the	O
features	O
on	O
each	O
stage	O
.	O

section	O
:	O
Method	O
In	O
this	O
section	O
,	O
we	O
first	O
detailedly	O
introduce	O
our	O
proposed	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
containing	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
.	O

Then	O
,	O
we	O
elaborate	O
how	O
these	O
two	O
networks	O
specifically	O
handle	O
the	O
intra	B-Task
-	I-Task
class	I-Task
consistency	I-Task
issue	E-Task
and	O
the	O
inter	B-Task
-	I-Task
class	I-Task
distinction	I-Task
issue	E-Task
.	O

Finally	O
,	O
we	O
describe	O
the	O
complete	O
encoder	B-Method
-	I-Method
decoder	I-Method
network	I-Method
architecture	E-Method
,	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
.	O

subsection	O
:	O
Smooth	B-Method
network	E-Method
In	O
the	O
task	O
of	O
semantic	B-Task
segmentation	E-Task
,	O
most	O
of	O
modern	O
methods	O
consider	O
it	O
as	O
a	O
dense	B-Task
prediction	I-Task
issue	E-Task
.	O

However	O
,	O
the	O
prediction	S-Task
sometimes	O
has	O
incorrect	O
results	O
in	O
some	O
parts	O
,	O
especially	O
the	O
parts	O
of	O
large	O
regions	O
and	O
complex	O
scenes	O
,	O
which	O
is	O
named	O
intra	B-Task
-	I-Task
class	I-Task
inconsistency	I-Task
issue	E-Task
.	O

The	O
intra	B-Task
-	I-Task
class	I-Task
inconsistency	I-Task
problem	E-Task
is	O
mainly	O
due	O
to	O
the	O
lack	O
of	O
context	O
.	O

Therefore	O
,	O
we	O
introduce	O
the	O
global	O
context	O
with	O
global	B-Method
average	I-Method
pooling	E-Method
.	O

However	O
,	O
global	O
context	O
just	O
has	O
the	O
high	O
semantic	O
information	O
,	O
which	O
is	O
not	O
helpful	O
for	O
recovering	O
the	O
spatial	O
information	O
.	O

Consequently	O
,	O
we	O
further	O
need	O
the	O
multi	O
-	O
scale	O
receptive	O
view	O
and	O
context	O
to	O
refine	O
the	O
spatial	O
information	O
,	O
as	O
most	O
modern	O
approaches	O
do	O
.	O

Nevertheless	O
,	O
there	O
exists	O
a	O
problem	O
that	O
the	O
different	O
scales	O
of	O
receptive	O
views	O
produce	O
the	O
features	O
with	O
different	O
extents	O
of	O
discrimination	O
,	O
leading	O
to	O
inconsistent	O
results	O
.	O

Therefore	O
,	O
we	O
need	O
to	O
select	O
more	O
discriminative	O
features	O
to	O
predict	O
the	O
unified	O
semantic	O
label	O
of	O
one	O
certain	O
category	O
.	O

In	O
our	O
proposed	O
network	O
,	O
we	O
use	O
ResNet	S-Method
as	O
a	O
base	B-Method
recognition	I-Method
model	E-Method
.	O

This	O
model	O
can	O
be	O
divided	O
into	O
five	O
stages	O
according	O
to	O
the	O
size	O
of	O
the	O
feature	O
maps	O
.	O

According	O
to	O
our	O
observation	O
,	O
the	O
different	O
stages	O
have	O
different	O
recognition	O
abilities	O
resulting	O
in	O
diverse	O
consistency	O
manifestation	O
.	O

In	O
the	O
lower	O
stage	O
,	O
the	O
network	O
encodes	O
finer	O
spatial	O
information	O
,	O
however	O
,	O
it	O
has	O
poor	O
semantic	B-Metric
consistency	E-Metric
because	O
of	O
its	O
small	O
receptive	O
view	O
and	O
without	O
the	O
guidance	O
of	O
spatial	O
context	O
.	O

While	O
in	O
the	O
high	O
stage	O
,	O
it	O
has	O
strong	O
semantic	O
consistency	O
due	O
to	O
large	O
receptive	O
view	O
,	O
however	O
,	O
the	O
prediction	S-Task
is	O
spatially	O
coarse	O
.	O

Overall	O
,	O
the	O
lower	O
stage	O
makes	O
more	O
accurate	O
spatial	O
predictions	O
,	O
while	O
the	O
higher	O
stage	O
gives	O
more	O
accurate	O
semantic	O
predictions	O
.	O

Based	O
on	O
this	O
observation	O
,	O
to	O
combine	O
their	O
advantages	O
,	O
we	O
propose	O
a	O
Smooth	B-Method
Network	E-Method
to	O
utilize	O
the	O
high	O
stage	O
’s	O
consistency	O
to	O
guide	O
the	O
low	O
stage	O
for	O
the	O
optimal	B-Task
prediction	E-Task
.	O

We	O
observe	O
that	O
in	O
the	O
current	O
prevalent	O
semantic	B-Task
segmentation	E-Task
architecture	O
,	O
there	O
are	O
mainly	O
two	O
styles	O
.	O

The	O
first	O
one	O
is	O
“	O
Backbone	B-Method
-	I-Method
Style	I-Method
”	E-Method
,	O
such	O
as	O
PSPNet	S-Method
,	O
Deeplab	O
v3	O
.	O

It	O
embeds	O
different	O
scale	O
context	O
information	O
to	O
improve	O
the	O
consistency	O
of	O
network	O
with	O
the	O
Pyramid	B-Method
Spatial	I-Method
Pooling	I-Method
module	E-Method
or	O
Atrous	B-Method
Spatial	I-Method
Pyramid	I-Method
Pooling	I-Method
module	E-Method
.	O

The	O
other	O
one	O
is	O
“	O
Encoder	B-Method
-	I-Method
Decoder	I-Method
-	I-Method
Style	I-Method
”	E-Method
,	O
like	O
RefineNet	S-Method
,	O
Global	B-Method
Convolutional	I-Method
Network	E-Method
.	O

This	O
style	O
of	O
network	O
utilizes	O
the	O
inherent	O
multi	O
-	O
scale	O
context	O
of	O
different	O
stage	O
,	O
but	O
it	O
lacks	O
the	O
global	O
context	O
which	O
has	O
the	O
strongest	O
consistency	O
.	O

In	O
addition	O
,	O
when	O
the	O
network	O
combines	O
the	O
features	O
of	O
adjacent	O
stages	O
,	O
it	O
just	O
sums	O
up	O
these	O
features	O
by	O
channel	O
.	O

This	O
operation	O
ignores	O
the	O
diverse	O
consistency	O
in	O
different	O
stages	O
.	O

To	O
remedy	O
the	O
defect	O
,	O
we	O
first	O
embed	O
a	O
global	B-Method
average	I-Method
pooling	I-Method
layer	E-Method
to	O
extend	O
the	O
U	B-Method
-	I-Method
shape	I-Method
architecture	E-Method
to	O
a	O
V	B-Method
-	I-Method
shape	I-Method
architecture	E-Method
.	O

With	O
the	O
global	B-Method
average	I-Method
pooling	I-Method
layer	E-Method
,	O
we	O
introduce	O
the	O
strongest	O
consistency	O
constraint	O
into	O
the	O
network	O
as	O
a	O
guidance	O
.	O

Furthermore	O
,	O
to	O
enhance	O
consistency	O
,	O
we	O
design	O
a	O
Channel	B-Method
Attention	I-Method
Block	E-Method
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
.	O

This	O
design	O
combines	O
the	O
features	O
of	O
adjacent	O
stages	O
to	O
compute	O
a	O
channel	O
attention	O
vector	O
[	O
reference	O
]	O
.	O

The	O
features	O
of	O
high	O
stage	O
provide	O
a	O
strong	O
consistency	O
guidance	O
,	O
while	O
the	O
features	O
of	O
low	O
stage	O
give	O
the	O
different	O
discrimination	O
information	O
of	O
features	O
.	O

In	O
this	O
way	O
,	O
the	O
channel	O
attention	O
vector	O
can	O
select	O
the	O
discriminative	O
features	O
.	O

paragraph	O
:	O
Channel	O
attention	O
block	O
:	O
Our	O
Channel	B-Method
Attention	I-Method
Block	E-Method
(	O
CAB	S-Method
)	O
is	O
designed	O
to	O
change	O
the	O
weights	O
of	O
the	O
features	O
on	O
each	O
stage	O
to	O
enhance	O
the	O
consistency	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
the	O
FCN	B-Method
architecture	E-Method
,	O
the	O
convolution	B-Method
operator	E-Method
outputs	O
a	O
score	O
map	O
,	O
which	O
gives	O
the	O
probability	O
of	O
each	O
class	O
at	O
each	O
pixel	O
.	O

In	O
Equation	O
[	O
reference	O
]	O
,	O
the	O
final	O
score	O
at	O
score	B-Metric
map	E-Metric
is	O
just	O
summed	O
over	O
all	O
channels	O
of	O
feature	O
maps	O
.	O

where	O
is	O
the	O
output	O
feature	O
of	O
network	O
.	O

represents	O
the	O
convolution	B-Method
kernel	E-Method
.	O

And	O
.	O

is	O
the	O
number	O
of	O
channels	O
.	O

is	O
the	O
set	O
of	O
pixel	O
positions	O
.	O

where	O
is	O
the	O
prediction	O
probability	O
.	O

is	O
the	O
output	O
of	O
network	O
.	O

As	O
shown	O
in	O
Equation	O
[	O
reference	O
]	O
and	O
Equation	O
[	O
reference	O
]	O
,	O
the	O
final	O
predicted	O
label	O
is	O
the	O
category	O
with	O
highest	O
probability	O
.	O

Therefore	O
,	O
we	O
assume	O
that	O
the	O
prediction	O
result	O
is	O
of	O
a	O
certain	O
patch	O
,	O
while	O
its	O
true	O
label	O
is	O
.	O

Consequently	O
,	O
we	O
can	O
introduce	O
a	O
parameter	O
to	O
change	O
the	O
highest	O
probability	O
value	O
from	O
to	O
,	O
as	O
Equation	O
[	O
reference	O
]	O
shows	O
.	O

where	O
is	O
the	O
new	O
prediction	B-Task
of	I-Task
network	E-Task
and	O
Based	O
on	O
the	O
above	O
formulation	O
of	O
the	O
Channel	B-Method
Attention	I-Method
Block	E-Method
(	O
CAB	S-Method
)	O
,	O
we	O
can	O
explore	O
its	O
practical	O
significance	O
.	O

In	O
Equation	O
[	O
reference	O
]	O
,	O
it	O
implicitly	O
indicates	O
that	O
the	O
weights	O
of	O
different	O
channels	O
are	O
equal	O
.	O

However	O
,	O
as	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
features	O
in	O
different	O
stages	O
have	O
different	O
degrees	O
of	O
discrimination	O
,	O
which	O
results	O
in	O
different	O
consistency	O
of	O
prediction	S-Task
.	O

In	O
order	O
to	O
obtain	O
the	O
intra	B-Task
-	I-Task
class	I-Task
consistent	I-Task
prediction	E-Task
,	O
we	O
should	O
extract	O
the	O
discriminative	O
features	O
and	O
inhibit	O
the	O
indiscriminative	O
features	O
.	O

Therefore	O
,	O
in	O
Equation	O
[	O
reference	O
]	O
,	O
the	O
value	O
applies	O
on	O
the	O
feature	O
maps	O
,	O
which	O
represents	O
the	O
feature	B-Method
selection	E-Method
with	O
CAB	S-Method
.	O

With	O
this	O
design	O
,	O
we	O
can	O
make	O
the	O
network	O
to	O
obtain	O
discriminative	O
features	O
stage	O
-	O
wise	O
to	O
make	O
the	O
prediction	B-Task
intra	I-Task
-	I-Task
class	I-Task
consistent	E-Task
.	O

paragraph	O
:	O
Refinement	O
residual	O
block	O
:	O
The	O
feature	O
maps	O
of	O
each	O
stage	O
in	O
feature	B-Method
network	E-Method
all	O
go	O
through	O
the	O
Refinement	O
Residual	O
Block	O
,	O
schematically	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
.	O

The	O
first	O
component	O
of	O
the	O
block	O
is	O
a	O
convolution	B-Method
layer	E-Method
.	O

We	O
use	O
it	O
to	O
unify	O
the	O
number	O
of	O
channels	O
to	O
512	O
.	O

Meanwhile	O
,	O
it	O
can	O
combine	O
the	O
information	O
across	O
all	O
channels	O
.	O

Then	O
the	O
following	O
is	O
a	O
basic	O
residual	B-Method
block	E-Method
,	O
which	O
can	O
refine	O
the	O
feature	O
map	O
.	O

Furthermore	O
,	O
this	O
block	O
can	O
strengthen	O
the	O
recognition	B-Metric
ability	E-Metric
of	O
each	O
stage	O
,	O
inspired	O
from	O
the	O
architecture	O
of	O
ResNet	S-Method
.	O

subsection	O
:	O
Border	B-Method
network	E-Method
In	O
the	O
semantic	B-Task
segmentation	I-Task
task	E-Task
,	O
the	O
prediction	S-Task
is	O
confused	O
with	O
the	O
different	O
categories	O
with	O
similar	O
appearances	O
,	O
especially	O
when	O
they	O
are	O
adjacent	O
spatially	O
.	O

Therefore	O
,	O
we	O
need	O
to	O
amplify	O
the	O
distinction	O
of	O
features	O
.	O

With	O
this	O
motivation	O
,	O
we	O
adopt	O
a	O
semantic	O
boundary	O
to	O
guide	O
the	O
feature	B-Task
learning	E-Task
.	O

To	O
extract	O
the	O
accurate	O
semantic	O
boundary	O
,	O
we	O
apply	O
the	O
explicit	O
supervision	O
of	O
semantic	O
boundary	O
,	O
which	O
makes	O
the	O
network	O
learn	O
a	O
feature	O
with	O
strong	O
inter	O
-	O
class	O
distinctive	O
ability	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
Border	B-Method
Network	E-Method
to	O
enlarge	O
the	O
inter	O
-	O
class	O
distinction	O
of	O
features	O
.	O

It	O
directly	O
learns	O
a	O
semantic	O
boundary	O
with	O
an	O
explicit	O
semantic	O
boundary	O
supervision	O
,	O
similar	O
to	O
a	O
semantic	B-Task
boundary	I-Task
detection	I-Task
task	E-Task
.	O

This	O
makes	O
the	O
features	O
on	O
both	O
sides	O
of	O
semantic	O
boundary	O
distinguishable	O
.	O

As	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
feature	B-Method
network	E-Method
has	O
different	O
stages	O
.	O

The	O
low	O
stage	O
features	O
have	O
more	O
detailed	O
information	O
,	O
while	O
the	O
high	O
stage	O
features	O
have	O
higher	O
semantic	O
information	O
.	O

In	O
our	O
work	O
,	O
we	O
need	O
semantic	O
boundary	O
with	O
more	O
semantic	O
meanings	O
.	O

Therefore	O
,	O
we	O
design	O
a	O
bottom	B-Method
-	I-Method
up	I-Method
Border	I-Method
Network	E-Method
.	O

This	O
network	O
can	O
simultaneously	O
get	O
accurate	O
edge	O
information	O
from	O
low	O
stage	O
and	O
obtain	O
semantic	O
information	O
from	O
high	O
stage	O
,	O
which	O
eliminates	O
some	O
original	O
edges	O
lack	O
of	O
semantic	O
information	O
.	O

In	O
this	O
way	O
,	O
the	O
semantic	O
information	O
of	O
high	O
stage	O
can	O
refine	O
the	O
detailed	O
edge	O
information	O
from	O
low	O
stage	O
stage	O
-	O
wise	O
.	O

The	O
supervisory	O
signal	O
of	O
the	O
network	O
is	O
obtained	O
from	O
the	O
semantic	B-Task
segmentation	E-Task
’s	O
groundtruth	O
with	O
a	O
traditional	O
image	B-Method
processing	I-Method
method	E-Method
,	O
such	O
as	O
Canny	S-Method
.	O

To	O
remedy	O
the	O
imbalance	O
of	O
the	O
positive	O
and	O
negative	O
samples	O
,	O
we	O
use	O
focal	B-Method
loss	E-Method
to	O
supervise	O
the	O
output	O
of	O
the	O
Border	B-Method
Network	E-Method
,	O
as	O
shown	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

We	O
adjust	O
the	O
parameters	O
and	O
of	O
focal	O
loss	O
for	O
better	O
performance	O
.	O

where	O
is	O
the	O
estimated	O
probability	O
for	O
class	O
,	O
.	O

And	O
is	O
the	O
maximum	O
value	O
of	O
class	O
label	O
.	O

The	O
Border	B-Method
Network	E-Method
mainly	O
focuses	O
on	O
the	O
semantic	O
boundary	O
which	O
separates	O
the	O
classes	O
on	O
two	O
sides	O
of	O
the	O
boundary	O
.	O

For	O
extracting	B-Task
accurate	I-Task
semantic	I-Task
boundary	E-Task
,	O
the	O
features	O
on	O
both	O
sides	O
will	O
become	O
more	O
distinguishable	O
.	O

This	O
exactly	O
reaches	O
our	O
goal	O
to	O
make	O
the	O
features	O
with	O
inter	O
-	O
class	O
distinction	O
as	O
much	O
as	O
possible	O
.	O

subsection	O
:	O
Network	B-Method
Architecture	E-Method
With	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
,	O
we	O
propose	O
our	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
for	O
semantic	B-Task
segmentation	E-Task
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
.	O

We	O
use	O
pre	O
-	O
trained	O
ResNet	S-Method
as	O
a	O
base	B-Method
network	E-Method
.	O

In	O
the	O
Smooth	B-Method
Network	E-Method
,	O
we	O
add	O
the	O
global	B-Method
average	I-Method
pooling	I-Method
layer	E-Method
on	O
the	O
top	O
of	O
the	O
network	O
to	O
get	O
the	O
strongest	O
consistency	O
.	O

Then	O
we	O
utilize	O
the	O
channel	O
attention	O
block	O
to	O
change	O
the	O
weights	O
of	O
channels	O
to	O
further	O
enhance	O
the	O
consistency	O
.	O

Meanwhile	O
,	O
in	O
the	O
Border	B-Method
Network	E-Method
,	O
with	O
the	O
explicit	O
semantic	O
boundary	O
supervision	O
,	O
the	O
network	O
obtains	O
accurate	O
semantic	O
boundary	O
and	O
makes	O
the	O
bilateral	O
features	O
more	O
distinct	O
.	O

With	O
the	O
support	O
of	O
both	O
sub	O
-	O
networks	O
,	O
the	O
intra	O
-	O
class	O
features	O
become	O
more	O
consistent	O
,	O
while	O
the	O
inter	O
-	O
class	O
ones	O
grow	O
more	O
distinct	O
.	O

For	O
explicit	B-Task
feature	I-Task
refinement	E-Task
,	O
we	O
use	O
deep	O
supervision	O
to	O
get	O
better	O
performance	O
and	O
make	O
the	O
network	O
easier	O
to	O
optimize	O
.	O

In	O
the	O
Smooth	B-Method
Network	E-Method
,	O
we	O
use	O
the	O
softmax	O
loss	O
to	O
supervise	O
the	O
each	O
stage	O
’s	O
upsampled	O
output	O
excluding	O
the	O
global	B-Method
average	I-Method
pooling	I-Method
layer	E-Method
,	O
while	O
we	O
use	O
the	O
focal	O
loss	O
to	O
supervise	O
the	O
outputs	O
of	O
Border	B-Method
Network	E-Method
.	O

Finally	O
,	O
we	O
use	O
a	O
parameter	O
to	O
balance	O
the	O
segmentation	O
loss	O
and	O
the	O
boundary	O
loss	O
,	O
as	O
Equation	O
[	O
reference	O
]	O
shows	O
.	O

section	O
:	O
Experimental	O
Results	O
We	O
evaluate	O
our	O
approach	O
on	O
two	O
public	O
datasets	O
:	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
Cityscapes	S-Material
.	O

We	O
first	O
introduce	O
the	O
datasets	O
and	O
report	O
the	O
implementation	O
details	O
.	O

Then	O
we	O
evaluate	O
each	O
component	O
of	O
the	O
proposed	O
method	O
,	O
and	O
analyze	O
the	O
results	O
in	O
detail	O
.	O

Finally	O
,	O
we	O
present	O
the	O
comparison	O
results	O
with	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

paragraph	O
:	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
:	O
The	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
is	O
a	O
well	O
-	O
known	O
semantic	B-Task
segmentation	E-Task
benchmark	O
which	O
contains	O
20	O
object	O
classes	O
and	O
one	O
background	O
,	O
involving	O
1	O
,	O
464	O
images	O
for	O
training	O
,	O
14	O
,	O
449	O
images	O
for	O
validation	S-Task
and	O
1	O
,	O
456	O
images	O
for	O
testing	O
.	O

The	O
original	O
dataset	O
is	O
augmented	O
by	O
the	O
Semantic	O
Boundaries	O
Dataset	O
,	O
resulting	O
in	O
10	O
,	O
582	O
images	O
for	O
training	O
.	O

paragraph	O
:	O
Cityscapes	S-Material
:	O
The	O
Cityscapes	S-Material
is	O
a	O
large	O
semantic	B-Task
segmentation	E-Task
dataset	O
of	O
urban	O
street	O
scene	O
in	O
car	O
perspective	O
.	O

The	O
dataset	O
contains	O
30	O
classes	O
,	O
of	O
which	O
19	O
classes	O
are	O
considered	O
for	O
training	O
and	O
evaluation	S-Task
.	O

There	O
are	O
2	O
,	O
979	O
images	O
for	O
training	O
,	O
500	O
images	O
for	O
validation	S-Task
and	O
1	O
,	O
525	O
images	O
for	O
testing	O
,	O
which	O
are	O
all	O
fine	O
annotated	O
.	O

And	O
there	O
are	O
another	O
19	O
,	O
998	O
images	O
with	O
coarse	O
annotation	O
.	O

The	O
images	O
all	O
have	O
a	O
high	O
resolution	O
of	O
2	O
,	O
048	O
1	O
,	O
024	O
.	O

subsection	O
:	O
Implementation	O
details	O
Our	O
proposed	O
network	O
is	O
based	O
on	O
the	O
ResNet	B-Method
-	I-Method
101	I-Method
pre	E-Method
-	O
trained	O
on	O
ImageNet	O
.	O

And	O
we	O
use	O
the	O
FCN4	S-Method
as	O
our	O
base	O
segmentation	B-Method
framework	E-Method
.	O

paragraph	O
:	O
Training	O
:	O
We	O
train	O
the	O
network	O
using	O
mini	B-Method
-	I-Method
batch	I-Method
stochastic	I-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
with	O
batch	O
size	O
,	O
momentum	O
and	O
weight	B-Method
decay	E-Method
.	O

Inspired	O
by	O
,	O
we	O
use	O
the	O
“	O
poly	B-Method
”	I-Method
learning	I-Method
rate	I-Method
policy	E-Method
where	O
the	O
learning	B-Metric
rate	E-Metric
is	O
multiplied	O
by	O
with	O
power	O
and	O
initial	O
learning	B-Metric
rate	E-Metric
.	O

As	O
for	O
the	O
,	O
we	O
finally	O
use	O
the	O
value	O
of	O
after	O
a	O
series	O
of	O
comparison	O
experiments	O
.	O

For	O
measuring	O
the	O
performance	O
of	O
our	O
proposed	O
network	O
,	O
we	O
use	O
the	O
mean	B-Metric
pixel	I-Metric
intersection	I-Metric
-	I-Metric
over	I-Metric
-	I-Metric
union	E-Metric
(	O
mean	B-Metric
IOU	E-Metric
)	O
as	O
the	O
metric	O
.	O

paragraph	O
:	O
Data	B-Task
augmentation	E-Task
:	O
We	O
use	O
mean	B-Method
subtraction	E-Method
and	O
random	B-Method
horizontal	I-Method
flip	E-Method
in	O
training	O
for	O
both	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
Cityscapes	S-Material
.	O

In	O
addition	O
,	O
we	O
find	O
it	O
is	O
crucial	O
to	O
randomly	O
scale	O
the	O
input	O
images	O
,	O
which	O
improves	O
the	O
performance	O
obviously	O
.	O

We	O
use	O
5	O
scales	O
on	O
both	O
datasets	O
.	O

subsection	O
:	O
Ablation	B-Task
study	E-Task
In	O
this	O
subsection	O
,	O
we	O
will	O
step	O
-	O
wise	O
decompose	O
our	O
approach	O
to	O
reveal	O
the	O
effect	O
of	O
each	O
component	O
.	O

In	O
the	O
following	O
experiments	O
,	O
we	O
evaluate	O
all	O
comparisons	O
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
dataset	O
.	O

And	O
we	O
report	O
the	O
comparison	O
results	O
in	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
dataset	O
and	O
Cityscapes	B-Material
dataset	E-Material
.	O

subsubsection	O
:	O
Smooth	B-Method
network	E-Method
We	O
use	O
the	O
ResNet	B-Method
-	I-Method
101	E-Method
as	O
our	O
base	O
feature	B-Method
network	E-Method
,	O
and	O
directly	O
upsample	O
the	O
ouput	O
.	O

First	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
the	O
base	O
ResNet	B-Method
-	I-Method
101	E-Method
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Then	O
we	O
extend	O
the	O
base	B-Method
network	E-Method
to	O
FCN4	B-Method
structure	E-Method
with	O
our	O
proposed	O
Refinement	B-Method
Residual	I-Method
Block	E-Method
(	O
RRB	S-Method
)	O
,	O
which	O
improves	O
the	O
performance	O
from	O
to	O
,	O
as	O
Table	O
[	O
reference	O
]	O
shows	O
.	O

We	O
visualize	O
the	O
effect	O
of	O
the	O
Smooth	B-Method
Network	E-Method
.	O

Figure	O
[	O
reference	O
]	O
presents	O
some	O
examples	O
of	O
semantic	B-Task
segmentation	E-Task
results	O
.	O

Obviously	O
,	O
our	O
Smooth	B-Method
Network	E-Method
can	O
effectively	O
make	O
the	O
prediction	O
more	O
consistent	O
.	O

paragraph	O
:	O
Ablation	S-Task
for	O
global	B-Task
pooling	E-Task
:	O
We	O
need	O
the	O
features	O
with	O
strong	O
consistency	O
.	O

Thus	O
based	O
our	O
observation	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
add	O
the	O
global	B-Method
average	I-Method
pooling	E-Method
on	O
the	O
top	O
of	O
the	O
network	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
global	B-Method
average	I-Method
pooling	E-Method
introduces	O
the	O
strongest	O
consistency	O
to	O
guide	O
other	O
stages	O
.	O

This	O
improves	O
the	O
performance	O
from	O
to	O
,	O
which	O
is	O
an	O
obvious	O
improvement	O
.	O

paragraph	O
:	O
Ablation	O
for	O
deep	B-Task
supervision	E-Task
:	O
To	O
refine	O
the	O
hierarchical	O
features	O
,	O
we	O
use	O
deep	B-Method
supervision	E-Method
.	O

We	O
add	O
the	O
softmax	O
loss	O
on	O
each	O
stage	O
excluding	O
the	O
global	B-Method
average	I-Method
pooling	I-Method
layer	E-Method
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
this	O
further	O
improves	O
the	O
performance	O
by	O
almost	O
.	O

paragraph	O
:	O
Ablation	S-Task
for	O
channel	B-Task
attention	I-Task
block	E-Task
:	O
Based	O
on	O
the	O
aforementioned	O
architecture	O
,	O
we	O
add	O
the	O
Channel	B-Method
Attention	I-Method
Block	E-Method
(	O
CAB	S-Method
)	O
.	O

It	O
utilizes	O
the	O
high	O
stage	O
to	O
guide	O
the	O
low	O
stage	O
with	O
a	O
channel	O
attention	O
vector	O
to	O
enhance	O
consistency	O
,	O
which	O
improves	O
the	O
performance	O
from	O
to	O
over	O
evaluation	O
,	O
as	O
Table	O
[	O
reference	O
]	O
shows	O
.	O

subsubsection	O
:	O
Border	B-Method
network	E-Method
While	O
the	O
Smooth	B-Method
Network	E-Method
pays	O
attention	O
to	O
the	O
intra	O
-	O
class	O
consistency	O
,	O
the	O
Border	B-Method
Network	E-Method
focuses	O
on	O
the	O
inter	O
-	O
class	O
indistinction	O
.	O

Due	O
to	O
the	O
accurate	O
boundary	O
supervisory	O
signal	O
,	O
the	O
network	O
amplifies	O
the	O
distinction	O
of	O
bilateral	O
feature	O
to	O
extract	O
the	O
semantic	O
boundary	O
.	O

Then	O
we	O
integrate	O
the	O
Border	B-Method
Network	E-Method
into	O
the	O
Smooth	B-Method
Network	E-Method
.	O

This	O
improves	O
the	O
performance	O
from	O
to	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
Border	B-Method
Network	E-Method
optimizes	O
the	O
semantic	O
boundary	O
,	O
which	O
is	O
a	O
comparably	O
small	O
part	O
of	O
the	O
whole	O
image	O
,	O
so	O
this	O
design	O
makes	O
a	O
minor	O
improvement	O
.	O

We	O
visualize	O
the	O
effect	O
of	O
Border	O
Network	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
addition	O
,	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
predicted	O
semantic	O
boundary	O
of	O
Border	O
Network	O
.	O

We	O
can	O
obviously	O
observe	O
that	O
the	O
Border	B-Method
Network	E-Method
can	O
focus	O
on	O
the	O
semantic	O
boundary	O
preferably	O
.	O

subsubsection	O
:	O
Discriminative	B-Method
Feature	I-Method
network	E-Method
With	O
the	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
(	O
DFN	S-Method
)	O
,	O
we	O
conduct	O
experiments	O
about	O
the	O
balance	O
parameter	O
of	O
the	O
combined	B-Metric
loss	E-Metric
.	O

Then	O
we	O
present	O
the	O
final	O
results	O
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
Cityscapes	B-Material
datasets	E-Material
.	O

paragraph	O
:	O
Balance	O
of	O
both	O
losses	O
:	O
The	O
balance	O
weight	O
between	O
the	O
losses	O
of	O
two	O
networks	O
is	O
crucial	O
.	O

To	O
further	O
analyze	O
the	O
effect	O
of	O
these	O
two	O
networks	O
,	O
we	O
conduct	O
experiments	O
for	O
different	O
balance	O
value	O
.	O

We	O
test	O
five	O
values	O
of	O
.	O

As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
with	O
the	O
same	O
setting	O
,	O
our	O
method	O
achieves	O
the	O
highest	O
performance	O
with	O
the	O
value	O
of	O
.	O

paragraph	O
:	O
Stage	O
-	O
wise	O
refinement	O
:	O
It	O
is	O
worth	O
noting	O
that	O
both	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
use	O
the	O
stage	B-Method
-	I-Method
wise	I-Method
mechanism	E-Method
.	O

The	O
Smooth	B-Method
Network	E-Method
utilizes	O
a	O
top	O
-	O
down	O
stage	O
-	O
wise	O
manner	O
to	O
transmit	O
the	O
context	O
information	O
from	O
high	O
stage	O
to	O
low	O
stage	O
,	O
to	O
ensure	O
the	O
inter	O
-	O
class	O
consistency	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
Border	B-Method
Network	E-Method
uses	O
a	O
bottom	O
-	O
up	O
stage	O
-	O
wise	O
manner	O
to	O
refine	O
the	O
semantic	O
boundary	O
with	O
the	O
edge	O
information	O
in	O
the	O
lower	O
stage	O
.	O

With	O
the	O
bidirectional	B-Method
stage	I-Method
-	I-Method
wise	I-Method
mechanism	E-Method
,	O
the	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
respectively	O
refine	O
the	O
segmentation	B-Task
and	I-Task
boundary	I-Task
prediction	E-Task
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
gradually	O
accurate	O
predictions	O
validate	O
the	O
effectiveness	O
of	O
the	O
stage	B-Method
-	I-Method
wise	I-Method
mechanism	E-Method
.	O

paragraph	O
:	O
Performance	O
evaluation	O
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
:	O
In	O
evaluation	O
,	O
we	O
apply	O
the	O
multi	O
-	O
scale	O
inputs	O
(	O
with	O
scales	O
)	O
and	O
also	O
horizontally	O
flip	O
the	O
inputs	O
to	O
further	O
improve	O
the	O
performance	O
.	O

In	O
addition	O
,	O
since	O
the	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
dataset	O
provides	O
higher	O
quality	O
of	O
annotation	O
than	O
the	O
augmented	O
datasets	O
,	O
we	O
further	O
fine	O
-	O
tune	O
our	O
model	O
on	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
train	O
set	O
for	O
evaluation	O
on	O
validation	B-Metric
set	E-Metric
.	O

More	O
performance	O
details	O
are	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
.	O

And	O
then	O
for	O
evaluation	O
on	O
test	O
set	O
,	O
we	O
use	O
the	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
trainval	O
set	O
to	O
further	O
fine	O
-	O
tune	O
our	O
proposed	O
method	O
.	O

In	O
the	O
end	O
,	O
our	O
proposed	O
approach	O
respectively	O
achieves	O
performance	O
of	O
and	O
with	O
and	O
without	O
MS	B-Method
-	I-Method
COCO	I-Method
fine	I-Method
-	I-Method
tuning	E-Method
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Note	O
that	O
,	O
we	O
do	O
not	O
use	O
Dense	B-Method
-	I-Method
CRF	I-Method
post	I-Method
-	I-Method
processing	E-Method
for	O
our	O
method	O
.	O

paragraph	O
:	O
Performance	O
evaluation	O
on	O
Cityscapes	S-Material
:	O
We	O
also	O
evaluate	O
our	O
approach	O
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
.	O

In	O
training	O
,	O
our	O
crop	O
size	O
of	O
image	O
is	O
.	O

We	O
observe	O
that	O
for	O
the	O
high	O
resolution	O
of	O
image	O
the	O
large	O
crop	O
size	O
is	O
useful	O
.	O

The	O
test	O
performance	O
results	O
are	O
specifically	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
visualize	O
the	O
results	O
of	O
our	O
approach	O
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

section	O
:	O
Conclusion	O
We	O
redefine	O
the	O
semantic	B-Task
segmentation	E-Task
from	O
a	O
macroscopic	O
view	O
of	O
point	O
,	O
regarding	O
it	O
as	O
a	O
task	O
to	O
assign	O
a	O
consistent	O
semantic	O
label	O
to	O
one	O
category	O
of	O
objects	O
,	O
rather	O
than	O
to	O
each	O
single	O
pixel	O
.	O

Inherently	O
,	O
this	O
task	O
requires	O
the	O
intra	O
-	O
class	O
consistency	O
and	O
inter	O
-	O
class	O
distinction	O
.	O

Aiming	O
to	O
consider	O
both	O
sides	O
,	O
we	O
propose	O
a	O
Discriminative	B-Method
Feature	I-Method
Network	E-Method
,	O
which	O
contains	O
two	O
sub	B-Method
-	I-Method
networks	E-Method
:	O
Smooth	B-Method
Network	E-Method
and	O
Border	B-Method
Network	E-Method
.	O

With	O
the	O
bidirectional	B-Method
stage	I-Method
-	I-Method
wise	I-Method
mechanism	E-Method
,	O
our	O
approach	O
can	O
capture	O
the	O
discriminative	O
features	O
for	O
semantic	B-Task
segmentation	E-Task
.	O

Our	O
experimental	O
results	O
show	O
that	O
the	O
proposed	O
approach	O
can	O
significantly	O
improve	O
the	O
performance	O
on	O
the	O
PASCAL	B-Material
VOC	I-Material
2012	E-Material
and	O
Cityscapes	B-Material
benchmarks	E-Material
.	O

section	O
:	O
Acknowledgment	O
This	O
work	O
has	O
been	O
supported	O
by	O
the	O
Project	O
of	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
No.61433007	O
and	O
No.61401170	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Teaching	B-Task
Machines	E-Task
to	O
Read	O
and	O
Comprehend	O
Teaching	B-Task
machines	E-Task
to	O
read	O
natural	O
language	O
documents	O
remains	O
an	O
elusive	O
challenge	O
.	O

Machine	B-Task
reading	I-Task
systems	E-Task
can	O
be	O
tested	O
on	O
their	O
ability	O
to	O
answer	O
questions	O
posed	O
on	O
the	O
contents	O
of	O
documents	O
that	O
they	O
have	O
seen	O
,	O
but	O
until	O
now	O
large	O
scale	O
training	O
and	O
test	O
datasets	O
have	O
been	O
missing	O
for	O
this	O
type	O
of	O
evaluation	S-Task
.	O

In	O
this	O
work	O
we	O
define	O
a	O
new	O
methodology	O
that	O
resolves	O
this	O
bottleneck	O
and	O
provides	O
large	B-Task
scale	I-Task
supervised	I-Task
reading	I-Task
comprehension	I-Task
data	E-Task
.	O

This	O
allows	O
us	O
to	O
develop	O
a	O
class	O
of	O
attention	B-Method
based	I-Method
deep	I-Method
neural	I-Method
networks	E-Method
that	O
learn	O
to	O
read	O
real	O
documents	O
and	O
answer	O
complex	O
questions	O
with	O
minimal	O
prior	O
knowledge	O
of	O
language	O
structure	O
.	O

figuret	O
section	O
:	O
Introduction	O
Progress	O
on	O
the	O
path	O
from	O
shallow	B-Method
bag	I-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
information	I-Method
retrieval	I-Method
algorithms	E-Method
to	O
machines	O
capable	O
of	O
reading	O
and	O
understanding	B-Task
documents	E-Task
has	O
been	O
slow	O
.	O

Traditional	O
approaches	O
to	O
machine	B-Task
reading	E-Task
and	O
comprehension	O
have	O
been	O
based	O
on	O
either	O
hand	B-Method
engineered	I-Method
grammars	E-Method
,	O
or	O
information	B-Method
extraction	I-Method
methods	E-Method
of	O
detecting	B-Task
predicate	I-Task
argument	I-Task
triples	E-Task
that	O
can	O
later	O
be	O
queried	O
as	O
a	O
relational	O
database	O
.	O

Supervised	B-Method
machine	I-Method
learning	I-Method
approaches	E-Method
have	O
largely	O
been	O
absent	O
from	O
this	O
space	O
due	O
to	O
both	O
the	O
lack	O
of	O
large	O
scale	O
training	O
datasets	O
,	O
and	O
the	O
difficulty	O
in	O
structuring	O
statistical	B-Method
models	E-Method
flexible	O
enough	O
to	O
learn	O
to	O
exploit	O
document	O
structure	O
.	O

While	O
obtaining	O
supervised	O
natural	O
language	O
reading	O
comprehension	O
data	O
has	O
proved	O
difficult	O
,	O
some	O
researchers	O
have	O
explored	O
generating	O
synthetic	O
narratives	O
and	O
queries	O
.	O

Such	O
approaches	O
allow	O
the	O
generation	O
of	O
almost	O
unlimited	O
amounts	O
of	O
supervised	O
data	O
and	O
enable	O
researchers	O
to	O
isolate	O
the	O
performance	O
of	O
their	O
algorithms	O
on	O
individual	O
simulated	O
phenomena	O
.	O

Work	O
on	O
such	O
data	O
has	O
shown	O
that	O
neural	B-Method
network	I-Method
based	I-Method
models	E-Method
hold	O
promise	O
for	O
modelling	O
reading	B-Task
comprehension	E-Task
,	O
something	O
that	O
we	O
will	O
build	O
upon	O
here	O
.	O

Historically	O
,	O
however	O
,	O
many	O
similar	O
approaches	O
in	O
Computational	B-Task
Linguistics	E-Task
have	O
failed	O
to	O
manage	O
the	O
transition	O
from	O
synthetic	O
data	O
to	O
real	O
environments	O
,	O
as	O
such	O
closed	O
worlds	O
inevitably	O
fail	O
to	O
capture	O
the	O
complexity	O
,	O
richness	O
,	O
and	O
noise	O
of	O
natural	O
language	O
.	O

In	O
this	O
work	O
we	O
seek	O
to	O
directly	O
address	O
the	O
lack	O
of	O
real	O
natural	O
language	O
training	O
data	O
by	O
introducing	O
a	O
novel	O
approach	O
to	O
building	O
a	O
supervised	B-Task
reading	I-Task
comprehension	I-Task
data	I-Task
set	E-Task
.	O

We	O
observe	O
that	O
summary	O
and	O
paraphrase	O
sentences	O
,	O
with	O
their	O
associated	O
documents	O
,	O
can	O
be	O
readily	O
converted	O
to	O
context	O
–	O
query	O
–	O
answer	O
triples	O
using	O
simple	O
entity	B-Method
detection	E-Method
and	O
anonymisation	B-Method
algorithms	E-Method
.	O

Using	O
this	O
approach	O
we	O
have	O
collected	O
two	O
new	O
corpora	O
of	O
roughly	O
a	O
million	O
news	O
stories	O
with	O
associated	O
queries	O
from	O
the	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
websites	E-Material
.	O

We	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
new	O
corpora	O
by	O
building	O
novel	O
deep	B-Method
learning	I-Method
models	E-Method
for	O
reading	B-Task
comprehension	E-Task
.	O

These	O
models	O
draw	O
on	O
recent	O
developments	O
for	O
incorporating	O
attention	B-Method
mechanisms	E-Method
into	O
recurrent	B-Method
neural	I-Method
network	I-Method
architectures	E-Method
.	O

This	O
allows	O
a	O
model	O
to	O
focus	O
on	O
the	O
aspects	O
of	O
a	O
document	O
that	O
it	O
believes	O
will	O
help	O
it	O
answer	O
a	O
question	O
,	O
and	O
also	O
allows	O
us	O
to	O
visualises	O
its	O
inference	B-Method
process	E-Method
.	O

We	O
compare	O
these	O
neural	B-Method
models	E-Method
to	O
a	O
range	O
of	O
baselines	O
and	O
heuristic	B-Metric
benchmarks	E-Metric
based	O
upon	O
a	O
traditional	O
frame	B-Method
semantic	I-Method
analysis	E-Method
provided	O
by	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
natural	B-Method
language	I-Method
processing	E-Method
(	O
NLP	S-Method
)	O
pipeline	O
.	O

Our	O
results	O
indicate	O
that	O
the	O
neural	B-Method
models	E-Method
achieve	O
a	O
higher	O
accuracy	S-Metric
,	O
and	O
do	O
so	O
without	O
any	O
specific	O
encoding	O
of	O
the	O
document	O
or	O
query	O
structure	O
.	O

section	O
:	O
Supervised	O
training	O
data	O
for	O
reading	B-Task
comprehension	E-Task
The	O
reading	B-Task
comprehension	I-Task
task	E-Task
naturally	O
lends	O
itself	O
to	O
a	O
formulation	O
as	O
a	O
supervised	B-Task
learning	I-Task
problem	E-Task
.	O

Specifically	O
we	O
seek	O
to	O
estimate	O
the	O
conditional	O
probability	O
,	O
where	O
is	O
a	O
context	O
document	O
,	O
a	O
query	O
relating	O
to	O
that	O
document	O
,	O
and	O
the	O
answer	O
to	O
that	O
query	O
.	O

For	O
a	O
focused	O
evaluation	O
we	O
wish	O
to	O
be	O
able	O
to	O
exclude	O
additional	O
information	O
,	O
such	O
as	O
world	O
knowledge	O
gained	O
from	O
co	O
-	O
occurrence	O
statistics	O
,	O
in	O
order	O
to	O
test	O
a	O
model	O
’s	O
core	O
capability	O
to	O
detect	O
and	O
understand	O
the	O
linguistic	O
relationships	O
between	O
entities	O
in	O
the	O
context	O
document	O
.	O

Such	O
an	O
approach	O
requires	O
a	O
large	O
training	O
corpus	O
of	O
document	O
–	O
query	B-Task
–	I-Task
answer	I-Task
triples	E-Task
and	O
until	O
now	O
such	O
corpora	O
have	O
been	O
limited	O
to	O
hundreds	O
of	O
examples	O
and	O
thus	O
mostly	O
of	O
use	O
only	O
for	O
testing	O
.	O

This	O
limitation	O
has	O
meant	O
that	O
most	O
work	O
in	O
this	O
area	O
has	O
taken	O
the	O
form	O
of	O
unsupervised	B-Method
approaches	E-Method
which	O
use	O
templates	O
or	O
syntactic	B-Method
/	I-Method
semantic	I-Method
analysers	E-Method
to	O
extract	O
relation	O
tuples	O
from	O
the	O
document	O
to	O
form	O
a	O
knowledge	B-Method
graph	E-Method
that	O
can	O
be	O
queried	O
.	O

Here	O
we	O
propose	O
a	O
methodology	O
for	O
creating	O
real	B-Task
-	I-Task
world	I-Task
,	I-Task
large	I-Task
scale	I-Task
supervised	I-Task
training	I-Task
data	E-Task
for	O
learning	B-Task
reading	I-Task
comprehension	I-Task
models	E-Task
.	O

Inspired	O
by	O
work	O
in	O
summarisation	S-Task
,	O
we	O
create	O
two	O
machine	B-Task
reading	E-Task
corpora	O
by	O
exploiting	O
online	O
newspaper	O
articles	O
and	O
their	O
matching	O
summaries	O
.	O

We	O
have	O
collected	O
93k	O
articles	O
from	O
the	O
CNN	S-Material
and	O
220k	O
articles	O
from	O
the	O
Daily	B-Material
Mail	I-Material
websites	E-Material
.	O

Both	O
news	O
providers	O
supplement	O
their	O
articles	O
with	O
a	O
number	O
of	O
bullet	O
points	O
,	O
summarising	O
aspects	O
of	O
the	O
information	O
contained	O
in	O
the	O
article	O
.	O

Of	O
key	O
importance	O
is	O
that	O
these	O
summary	O
points	O
are	O
abstractive	O
and	O
do	O
not	O
simply	O
copy	O
sentences	O
from	O
the	O
documents	O
.	O

We	O
construct	O
a	O
corpus	O
of	O
document	O
–	O
query	O
–	O
answer	O
triples	O
by	O
turning	O
these	O
bullet	O
points	O
into	O
Cloze	O
style	O
questions	O
by	O
replacing	O
one	O
entity	O
at	O
a	O
time	O
with	O
a	O
placeholder	O
.	O

This	O
results	O
in	O
a	O
combined	O
corpus	O
of	O
roughly	O
1	O
M	O
data	O
points	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Code	O
to	O
replicate	O
our	O
datasets	O
—	O
and	O
to	O
apply	O
this	O
method	O
to	O
other	O
sources	O
—	O
is	O
available	O
online	O
.	O

Corpus	O
statistics	O
.	O

Articles	O
were	O
collected	O
starting	O
in	O
April	O
2007	O
for	O
CNN	S-Material
and	O
June	O
2010	O
for	O
the	O
Daily	B-Material
Mail	E-Material
,	O
both	O
until	O
the	O
end	O
of	O
April	O
2015	O
.	O

Validation	O
data	O
is	O
from	O
March	O
,	O
test	O
data	O
from	O
April	O
2015	O
.	O

Articles	O
of	O
over	O
2000	O
tokens	O
and	O
queries	O
whose	O
answer	O
entity	O
did	O
not	O
appear	O
in	O
the	O
context	O
were	O
filtered	O
out	O
.	O

Percentage	O
of	O
time	O
that	O
the	O
correct	O
answer	O
is	O
contained	O
in	O
the	O
top	O
most	O
frequent	O
entities	O
in	O
a	O
given	O
document	O
.	O

subsection	O
:	O
Entity	B-Task
replacement	E-Task
and	O
permutation	S-Task
Note	O
that	O
the	O
focus	O
of	O
this	O
paper	O
is	O
to	O
provide	O
a	O
corpus	O
for	O
evaluating	O
a	O
model	O
’s	O
ability	O
to	O
read	O
and	O
comprehend	O
a	O
single	O
document	O
,	O
not	O
world	O
knowledge	O
or	O
co	O
-	O
occurrence	O
.	O

To	O
understand	O
that	O
distinction	O
consider	O
for	O
instance	O
the	O
following	O
Cloze	O
form	O
queries	O
(	O
created	O
from	O
headlines	O
in	O
the	O
Daily	B-Material
Mail	I-Material
validation	I-Material
set	E-Material
)	O
:	O
An	O
ngram	B-Method
language	I-Method
model	E-Method
trained	O
on	O
the	O
Daily	B-Material
Mail	E-Material
would	O
easily	O
correctly	O
predict	O
that	O
(	O
X	O
=	O
cancer	O
)	O
,	O
regardless	O
of	O
the	O
contents	O
of	O
the	O
context	O
document	O
,	O
simply	O
because	O
this	O
is	O
a	O
very	O
frequently	O
cured	O
entity	O
in	O
the	O
Daily	B-Material
Mail	I-Material
corpus	E-Material
.	O

To	O
prevent	O
such	O
degenerate	O
solutions	O
and	O
create	O
a	O
focused	B-Task
task	E-Task
we	O
anonymise	O
and	O
randomise	O
our	O
corpora	O
with	O
the	O
following	O
procedure	O
,	O
Compare	O
the	O
original	O
and	O
anonymised	O
version	O
of	O
the	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Clearly	O
a	O
human	O
reader	O
can	O
answer	O
both	O
queries	O
correctly	O
.	O

However	O
in	O
the	O
anonymised	O
setup	O
the	O
context	O
document	O
is	O
required	O
for	O
answering	O
the	O
query	O
,	O
whereas	O
the	O
original	O
version	O
could	O
also	O
be	O
answered	O
by	O
someone	O
with	O
the	O
requisite	O
background	O
knowledge	O
.	O

Therefore	O
,	O
following	O
this	O
procedure	O
,	O
the	O
only	O
remaining	O
strategy	O
for	O
answering	B-Task
questions	E-Task
is	O
to	O
do	O
so	O
by	O
exploiting	O
the	O
context	O
presented	O
with	O
each	O
question	O
.	O

Thus	O
performance	O
on	O
our	O
two	O
corpora	O
truly	O
measures	O
reading	B-Metric
comprehension	I-Metric
capability	E-Metric
.	O

Naturally	O
a	O
production	B-Task
system	E-Task
would	O
benefit	O
from	O
using	O
all	O
available	O
information	O
sources	O
,	O
such	O
as	O
clues	O
through	O
language	O
and	O
co	O
-	O
occurrence	O
statistics	O
.	O

Table	O
[	O
reference	O
]	O
gives	O
an	O
indication	O
of	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
showing	O
how	O
frequent	O
the	O
correct	O
answer	O
is	O
contained	O
in	O
the	O
top	O
entity	O
markers	O
in	O
a	O
given	O
document	O
.	O

Note	O
that	O
our	O
models	O
do	O
n’t	O
distinguish	O
between	O
entity	O
markers	O
and	O
regular	O
words	O
.	O

This	O
makes	O
the	O
task	O
harder	O
and	O
the	O
models	O
more	O
general	O
.	O

section	O
:	O
Models	O
So	O
far	O
we	O
have	O
motivated	O
the	O
need	O
for	O
better	O
datasets	O
and	O
tasks	O
to	O
evaluate	O
the	O
capabilities	O
of	O
machine	B-Task
reading	E-Task
models	O
.	O

We	O
proceed	O
by	O
describing	O
a	O
number	O
of	O
baselines	O
,	O
benchmarks	O
and	O
new	O
models	O
to	O
evaluate	O
against	O
this	O
paradigm	O
.	O

We	O
define	O
two	O
simple	O
baselines	O
,	O
the	O
majority	B-Method
baseline	E-Method
(	O
maximum	O
frequency	O
)	O
picks	O
the	O
entity	O
most	O
frequently	O
observed	O
in	O
the	O
context	O
document	O
,	O
whereas	O
the	O
exclusive	O
majority	O
(	O
exclusive	O
frequency	O
)	O
chooses	O
the	O
entity	O
most	O
frequently	O
observed	O
in	O
the	O
context	O
but	O
not	O
observed	O
in	O
the	O
query	O
.	O

The	O
idea	O
behind	O
this	O
exclusion	O
is	O
that	O
the	O
placeholder	O
is	O
unlikely	O
to	O
be	O
mentioned	O
twice	O
in	O
a	O
single	O
Cloze	O
form	O
query	O
.	O

subsection	O
:	O
Symbolic	B-Method
Matching	I-Method
Models	E-Method
Traditionally	O
,	O
a	O
pipeline	O
of	O
NLP	S-Method
models	O
has	O
been	O
used	O
for	O
attempting	O
question	B-Task
answering	E-Task
,	O
that	O
is	O
models	O
that	O
make	O
heavy	O
use	O
of	O
linguistic	O
annotation	O
,	O
structured	O
world	O
knowledge	O
and	O
semantic	B-Method
parsing	E-Method
and	O
similar	O
NLP	S-Method
pipeline	O
outputs	O
.	O

Building	O
on	O
these	O
approaches	O
,	O
we	O
define	O
a	O
number	O
of	O
NLP	S-Method
-	O
centric	O
models	O
for	O
our	O
machine	B-Task
reading	E-Task
task	O
.	O

paragraph	O
:	O
Frame	B-Method
-	I-Method
Semantic	I-Method
Parsing	I-Method
Frame	I-Method
-	I-Method
semantic	I-Method
parsing	E-Method
attempts	O
to	O
identify	O
predicates	O
and	O
their	O
arguments	O
,	O
allowing	O
models	O
access	O
to	O
information	O
about	O
“	O
who	O
did	O
what	O
to	O
whom	O
”	O
.	O

Naturally	O
this	O
kind	O
of	O
annotation	O
lends	O
itself	O
to	O
being	O
exploited	O
for	O
question	B-Task
answering	E-Task
.	O

We	O
develop	O
a	O
benchmark	O
that	O
makes	O
use	O
of	O
frame	O
-	O
semantic	O
annotations	O
which	O
we	O
obtained	O
by	O
parsing	O
our	O
model	O
with	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frame	B-Method
-	I-Method
semantic	I-Method
parser	E-Method
.	O

As	O
the	O
parser	S-Method
makes	O
extensive	O
use	O
of	O
linguistic	O
information	O
we	O
run	O
these	O
benchmarks	O
on	O
the	O
unanonymised	O
version	O
of	O
our	O
corpora	O
.	O

There	O
is	O
no	O
significant	O
advantage	O
in	O
this	O
as	O
the	O
frame	B-Method
-	I-Method
semantic	I-Method
approach	E-Method
used	O
here	O
does	O
not	O
possess	O
the	O
capability	O
to	O
generalise	O
through	O
a	O
language	B-Method
model	E-Method
beyond	O
exploiting	O
one	O
during	O
the	O
parsing	B-Task
phase	E-Task
.	O

Thus	O
,	O
the	O
key	O
objective	O
of	O
evaluating	O
machine	B-Task
comprehension	I-Task
abilities	E-Task
is	O
maintained	O
.	O

Extracting	O
entity	O
-	O
predicate	O
triples	O
—	O
denoted	O
as	O
—from	O
both	O
the	O
query	O
and	O
context	O
document	O
,	O
we	O
attempt	O
to	O
resolve	O
queries	O
using	O
a	O
number	O
of	O
rules	O
with	O
an	O
increasing	O
recall	B-Metric
/	I-Metric
precision	E-Metric
trade	O
-	O
off	O
as	O
follows	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

For	O
reasons	O
of	O
clarity	O
,	O
we	O
pretend	O
that	O
all	O
PropBank	O
triples	O
are	O
of	O
the	O
form	O
.	O

In	O
practice	O
,	O
we	O
take	O
the	O
argument	O
numberings	O
of	O
the	O
parser	S-Method
into	O
account	O
and	O
only	O
compare	O
like	O
with	O
like	O
,	O
except	O
in	O
cases	O
such	O
as	O
the	O
permuted	O
frame	O
rule	O
,	O
where	O
ordering	O
is	O
relaxed	O
.	O

In	O
the	O
case	O
of	O
multiple	O
possible	O
answers	O
from	O
a	O
single	O
rule	O
,	O
we	O
randomly	O
choose	O
one	O
.	O

paragraph	O
:	O
Word	B-Metric
Distance	I-Metric
Benchmark	E-Metric
We	O
consider	O
another	O
baseline	O
that	O
relies	O
on	O
word	B-Method
distance	I-Method
measurements	E-Method
.	O

Here	O
,	O
we	O
align	O
the	O
placeholder	O
of	O
the	O
Cloze	O
form	O
question	O
with	O
each	O
possible	O
entity	O
in	O
the	O
context	O
document	O
and	O
calculate	O
a	O
distance	B-Metric
measure	E-Metric
between	O
the	O
question	O
and	O
the	O
context	O
around	O
the	O
aligned	O
entity	O
.	O

This	O
score	O
is	O
calculated	O
by	O
summing	O
the	O
distances	O
of	O
every	O
word	O
in	O
to	O
their	O
nearest	O
aligned	O
word	O
in	O
,	O
where	O
alignment	S-Task
is	O
defined	O
by	O
matching	O
words	O
either	O
directly	O
or	O
as	O
aligned	O
by	O
the	O
coreference	B-Method
system	E-Method
.	O

We	O
tune	O
the	O
maximum	O
penalty	O
per	O
word	O
(	O
)	O
on	O
the	O
validation	O
data	O
.	O

subsection	O
:	O
Neural	B-Method
Network	I-Method
Models	I-Method
Neural	I-Method
networks	E-Method
have	O
successfully	O
been	O
applied	O
to	O
a	O
range	O
of	O
tasks	O
in	O
NLP	S-Method
.	O

This	O
includes	O
classification	B-Task
tasks	E-Task
such	O
as	O
sentiment	B-Task
analysis	E-Task
or	O
POS	B-Task
tagging	E-Task
,	O
as	O
well	O
as	O
generative	B-Task
problems	E-Task
such	O
as	O
language	B-Method
modelling	E-Method
or	O
machine	B-Task
translation	E-Task
.	O

We	O
propose	O
three	O
neural	B-Method
models	E-Method
for	O
estimating	O
the	O
probability	B-Task
of	I-Task
word	I-Task
type	E-Task
from	O
document	B-Task
answering	I-Task
query	E-Task
:	O
where	O
is	O
the	O
vocabulary	O
,	O
and	O
indexes	O
row	O
of	O
weight	O
matrix	O
and	O
through	O
a	O
slight	O
abuse	O
of	O
notation	O
word	O
types	O
double	O
as	O
indexes	O
.	O

Note	O
that	O
we	O
do	O
not	O
privilege	O
entities	O
or	O
variables	O
,	O
the	O
model	O
must	O
learn	O
to	O
differentiate	O
these	O
in	O
the	O
input	O
sequence	O
.	O

The	O
function	O
returns	O
a	O
vector	B-Method
embedding	E-Method
of	O
a	O
document	O
and	O
query	O
pair	O
.	O

paragraph	O
:	O
The	O
Deep	O
LSTM	S-Method
Reader	O
Long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	E-Method
(	O
LSTM	S-Method
,	O
)	O
networks	O
have	O
recently	O
seen	O
considerable	O
success	O
in	O
tasks	O
such	O
as	O
machine	B-Task
translation	E-Task
and	O
language	B-Task
modelling	E-Task
.	O

When	O
used	O
for	O
translation	S-Task
,	O
Deep	B-Method
LSTMs	E-Method
have	O
shown	O
a	O
remarkable	O
ability	O
to	O
embed	O
long	O
sequences	O
into	O
a	O
vector	B-Method
representation	E-Method
which	O
contains	O
enough	O
information	O
to	O
generate	O
a	O
full	O
translation	O
in	O
another	O
language	O
.	O

Our	O
first	O
neural	B-Method
model	E-Method
for	O
reading	B-Task
comprehension	E-Task
tests	O
the	O
ability	O
of	O
Deep	O
LSTM	S-Method
encoders	O
to	O
handle	O
significantly	O
longer	O
sequences	O
.	O

We	O
feed	O
our	O
documents	O
one	O
word	O
at	O
a	O
time	O
into	O
a	O
Deep	O
LSTM	S-Method
encoder	O
,	O
after	O
a	O
delimiter	O
we	O
then	O
also	O
feed	O
the	O
query	O
into	O
the	O
encoder	O
.	O

Alternatively	O
we	O
also	O
experiment	O
with	O
processing	O
the	O
query	O
then	O
the	O
document	O
.	O

The	O
result	O
is	O
that	O
this	O
model	O
processes	O
each	O
document	O
query	O
pair	O
as	O
a	O
single	O
long	O
sequence	O
.	O

Given	O
the	O
embedded	O
document	O
and	O
query	O
the	O
network	O
predicts	O
which	O
token	O
in	O
the	O
document	O
answers	O
the	O
query	O
.	O

We	O
employ	O
a	O
Deep	O
LSTM	S-Method
cell	O
with	O
skip	O
connections	O
from	O
each	O
input	O
to	O
every	O
hidden	O
layer	O
,	O
and	O
from	O
every	O
hidden	O
layer	O
to	O
the	O
output	O
:	O
where	O
indicates	O
vector	O
concatenation	O
is	O
the	O
hidden	O
state	O
for	O
layer	O
at	O
time	O
,	O
and	O
,	O
,	O
are	O
the	O
input	O
,	O
forget	O
,	O
and	O
output	O
gates	O
respectively	O
.	O

Thus	O
our	O
Deep	O
LSTM	S-Method
Reader	O
is	O
defined	O
by	O
with	O
input	O
the	O
concatenation	O
of	O
and	O
separated	O
by	O
the	O
delimiter	O
.	O

paragraph	O
:	O
The	O
Attentive	B-Method
Reader	E-Method
[	O
b	O
]	O
0.49	O
[	O
b	O
]	O
0.49	O
[	O
b	O
]	O
1.0	O
The	O
Deep	O
LSTM	S-Method
Reader	O
must	O
propagate	O
dependencies	O
over	O
long	O
distances	O
in	O
order	O
to	O
connect	O
queries	O
to	O
their	O
answers	O
.	O

The	O
fixed	O
width	O
hidden	O
vector	O
forms	O
a	O
bottleneck	O
for	O
this	O
information	B-Task
flow	E-Task
that	O
we	O
propose	O
to	O
circumvent	O
using	O
an	O
attention	B-Method
mechanism	E-Method
inspired	O
by	O
recent	O
results	O
in	O
translation	B-Task
and	I-Task
image	I-Task
recognition	E-Task
.	O

This	O
attention	B-Method
model	E-Method
first	O
encodes	O
the	O
document	O
and	O
the	O
query	O
using	O
separate	O
bidirectional	B-Method
single	I-Method
layer	I-Method
LSTMs	E-Method
.	O

We	O
denote	O
the	O
outputs	O
of	O
the	O
forward	B-Method
and	I-Method
backward	I-Method
LSTMs	E-Method
as	O
and	O
respectively	O
.	O

The	O
encoding	O
of	O
a	O
query	O
of	O
length	O
is	O
formed	O
by	O
the	O
concatenation	O
of	O
the	O
final	O
forward	O
and	O
backward	O
outputs	O
,	O
For	O
the	O
document	O
the	O
composite	O
output	O
for	O
each	O
token	O
at	O
position	O
is	O
,	O
The	O
representation	O
of	O
the	O
document	O
is	O
formed	O
by	O
a	O
weighted	O
sum	O
of	O
these	O
output	O
vectors	O
.	O

These	O
weights	O
are	O
interpreted	O
as	O
the	O
degree	O
to	O
which	O
the	O
network	O
attends	O
to	O
a	O
particular	O
token	O
in	O
the	O
document	O
when	O
answering	O
the	O
query	O
:	O
where	O
we	O
are	O
interpreting	O
as	O
a	O
matrix	O
with	O
each	O
column	O
being	O
the	O
composite	O
representation	O
of	O
document	O
token	O
.	O

The	O
variable	O
is	O
the	O
normalised	O
attention	O
at	O
token	O
.	O

Given	O
this	O
attention	O
score	O
the	O
embedding	O
of	O
the	O
document	O
is	O
computed	O
as	O
the	O
weighted	O
sum	O
of	O
the	O
token	B-Method
embeddings	E-Method
.	O

The	O
model	O
is	O
completed	O
with	O
the	O
definition	O
of	O
the	O
joint	B-Task
document	I-Task
and	I-Task
query	I-Task
embedding	E-Task
via	O
a	O
non	B-Method
-	I-Method
linear	I-Method
combination	E-Method
:	O
The	O
Attentive	B-Method
Reader	E-Method
can	O
be	O
viewed	O
as	O
a	O
generalisation	O
of	O
the	O
application	O
of	O
Memory	B-Method
Networks	E-Method
to	O
question	B-Task
answering	E-Task
.	O

That	O
model	O
employs	O
an	O
attention	B-Method
mechanism	E-Method
at	O
the	O
sentence	O
level	O
where	O
each	O
sentence	O
is	O
represented	O
by	O
a	O
bag	B-Method
of	I-Method
embeddings	E-Method
.	O

The	O
Attentive	B-Method
Reader	E-Method
employs	O
a	O
finer	O
grained	B-Method
token	I-Method
level	I-Method
attention	I-Method
mechanism	E-Method
where	O
the	O
tokens	O
are	O
embedded	O
given	O
their	O
entire	O
future	O
and	O
past	O
context	O
in	O
the	O
input	O
document	O
.	O

paragraph	O
:	O
The	O
Impatient	B-Method
Reader	E-Method
The	O
Attentive	B-Method
Reader	E-Method
is	O
able	O
to	O
focus	O
on	O
the	O
passages	O
of	O
a	O
context	O
document	O
that	O
are	O
most	O
likely	O
to	O
inform	O
the	O
answer	O
to	O
the	O
query	O
.	O

We	O
can	O
go	O
further	O
by	O
equipping	O
the	O
model	O
with	O
the	O
ability	O
to	O
reread	O
from	O
the	O
document	O
as	O
each	O
query	O
token	O
is	O
read	O
.	O

At	O
each	O
token	O
of	O
the	O
query	O
the	O
model	O
computes	O
a	O
document	B-Method
representation	I-Method
vector	E-Method
using	O
the	O
bidirectional	B-Method
embedding	E-Method
:	O
The	O
result	O
is	O
an	O
attention	B-Method
mechanism	E-Method
that	O
allows	O
the	O
model	O
to	O
recurrently	O
accumulate	O
information	O
from	O
the	O
document	O
as	O
it	O
sees	O
each	O
query	O
token	O
,	O
ultimately	O
outputting	O
a	O
final	O
joint	B-Method
document	I-Method
query	I-Method
representation	E-Method
for	O
the	O
answer	B-Task
prediction	E-Task
,	O
section	O
:	O
Empirical	O
Evaluation	O
Having	O
described	O
a	O
number	O
of	O
models	O
in	O
the	O
previous	O
section	O
,	O
we	O
next	O
evaluate	O
these	O
models	O
on	O
our	O
reading	O
comprehension	O
corpora	O
.	O

Our	O
hypothesis	O
is	O
that	O
neural	B-Method
models	E-Method
should	O
in	O
principle	O
be	O
well	O
suited	O
for	O
this	O
task	O
.	O

However	O
,	O
we	O
argued	O
that	O
simple	O
recurrent	B-Method
models	E-Method
such	O
as	O
the	O
LSTM	S-Method
probably	O
have	O
insufficient	O
expressive	O
power	O
for	O
solving	O
tasks	O
that	O
require	O
complex	O
inference	S-Task
.	O

We	O
expect	O
that	O
the	O
attention	B-Method
-	I-Method
based	I-Method
models	E-Method
would	O
therefore	O
outperform	O
the	O
pure	O
LSTM	S-Method
-	O
based	O
approaches	O
.	O

Considering	O
the	O
second	O
dimension	O
of	O
our	O
investigation	O
,	O
the	O
comparison	O
of	O
traditional	O
versus	O
neural	B-Method
approaches	E-Method
to	O
NLP	S-Method
,	O
we	O
do	O
not	O
have	O
a	O
strong	O
prior	O
favouring	O
one	O
approach	O
over	O
the	O
other	O
.	O

While	O
numerous	O
publications	O
in	O
the	O
past	O
few	O
years	O
have	O
demonstrated	O
neural	B-Method
models	E-Method
outperforming	O
classical	O
methods	O
,	O
it	O
remains	O
unclear	O
how	O
much	O
of	O
that	O
is	O
a	O
side	O
-	O
effect	O
of	O
the	O
language	B-Method
modelling	I-Method
capabilities	E-Method
intrinsic	O
to	O
any	O
neural	B-Method
model	E-Method
for	O
NLP	S-Method
.	O

The	O
entity	B-Task
anonymisation	I-Task
and	I-Task
permutation	I-Task
aspect	E-Task
of	O
the	O
task	O
presented	O
here	O
may	O
end	O
up	O
levelling	O
the	O
playing	O
field	O
in	O
that	O
regard	O
,	O
favouring	O
models	O
capable	O
of	O
dealing	O
with	O
syntax	O
rather	O
than	O
just	O
semantics	O
.	O

With	O
these	O
considerations	O
in	O
mind	O
,	O
the	O
experimental	O
part	O
of	O
this	O
paper	O
is	O
designed	O
with	O
a	O
three	O
-	O
fold	O
aim	O
.	O

First	O
,	O
we	O
want	O
to	O
establish	O
the	O
difficulty	O
of	O
our	O
machine	B-Task
reading	E-Task
task	O
by	O
applying	O
a	O
wide	O
range	O
of	O
models	O
to	O
it	O
.	O

Second	O
,	O
we	O
compare	O
the	O
performance	O
of	O
parse	B-Method
-	I-Method
based	I-Method
methods	E-Method
versus	O
that	O
of	O
neural	B-Method
models	E-Method
.	O

Third	O
,	O
within	O
the	O
group	O
of	O
neural	B-Method
models	E-Method
examined	O
,	O
we	O
want	O
to	O
determine	O
what	O
each	O
component	O
contributes	O
to	O
the	O
end	O
performance	O
;	O
that	O
is	O
,	O
we	O
want	O
to	O
analyse	O
the	O
extent	O
to	O
which	O
an	O
LSTM	S-Method
can	O
solve	O
this	O
task	O
,	O
and	O
to	O
what	O
extent	O
various	O
attention	B-Method
mechanisms	E-Method
impact	O
performance	O
.	O

All	O
model	O
hyperparameters	O
were	O
tuned	O
on	O
the	O
respective	O
validation	O
sets	O
of	O
the	O
two	O
corpora	O
.	O

Our	O
experimental	O
results	O
are	O
in	O
Table	O
[	O
reference	O
]	O
,	O
with	O
the	O
Attentive	O
and	O
Impatient	O
Readers	O
performing	O
best	O
across	O
both	O
datasets	O
.	O

Accuracy	S-Metric
of	O
all	O
the	O
models	O
and	O
benchmarks	O
on	O
the	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
.	O

The	O
Uniform	O
Reader	O
baseline	O
sets	O
all	O
of	O
the	O
⁢m	O
(	O
t	O
)	O
parameters	O
to	O
be	O
equal	O
.	O

[	O
-	O
0.3em	O
]	O
Precision@Recall	S-Metric
for	O
the	O
attention	B-Method
models	E-Method
on	O
the	O
CNN	B-Material
validation	I-Material
data	E-Material
.	O

paragraph	O
:	O
Frame	B-Metric
-	I-Metric
semantic	I-Metric
benchmark	E-Metric
While	O
the	O
one	B-Method
frame	I-Method
-	I-Method
semantic	I-Method
model	E-Method
proposed	O
in	O
this	O
paper	O
is	O
clearly	O
a	O
simplification	O
of	O
what	O
could	O
be	O
achieved	O
with	O
annotations	O
from	O
an	O
NLP	S-Method
pipeline	O
,	O
it	O
does	O
highlight	O
the	O
difficulty	O
of	O
the	O
task	O
when	O
approached	O
from	O
a	O
symbolic	O
NLP	S-Method
perspective	O
.	O

Two	O
issues	O
stand	O
out	O
when	O
analysing	O
the	O
results	O
in	O
detail	O
.	O

First	O
,	O
the	O
frame	B-Method
-	I-Method
semantic	I-Method
pipeline	E-Method
has	O
a	O
poor	O
degree	O
of	O
coverage	O
with	O
many	O
relations	O
not	O
being	O
picked	O
up	O
by	O
our	O
PropBank	B-Method
parser	E-Method
as	O
they	O
do	O
not	O
adhere	O
to	O
the	O
default	O
predicate	O
-	O
argument	O
structure	O
.	O

This	O
effect	O
is	O
exacerbated	O
by	O
the	O
type	O
of	O
language	O
used	O
in	O
the	O
highlights	O
that	O
form	O
the	O
basis	O
of	O
our	O
datasets	O
.	O

The	O
second	O
issue	O
is	O
that	O
the	O
frame	B-Method
-	I-Method
semantic	I-Method
approach	E-Method
does	O
not	O
trivially	O
scale	O
to	O
situations	O
where	O
several	O
sentences	O
,	O
and	O
thus	O
frames	O
,	O
are	O
required	O
to	O
answer	O
a	O
query	O
.	O

This	O
was	O
true	O
for	O
the	O
majority	O
of	O
queries	O
in	O
the	O
dataset	O
.	O

paragraph	O
:	O
Word	B-Metric
distance	I-Metric
benchmark	E-Metric
More	O
surprising	O
perhaps	O
is	O
the	O
relatively	O
strong	O
performance	O
of	O
the	O
word	B-Metric
distance	I-Metric
benchmark	E-Metric
,	O
particularly	O
relative	O
to	O
the	O
frame	B-Task
-	I-Task
semantic	I-Task
benchmark	E-Task
,	O
which	O
we	O
had	O
expected	O
to	O
perform	O
better	O
.	O

Here	O
,	O
again	O
,	O
the	O
nature	O
of	O
the	O
datasets	O
used	O
can	O
explain	O
aspects	O
of	O
this	O
result	O
.	O

Where	O
the	O
frame	B-Method
-	I-Method
semantic	I-Method
model	E-Method
suffered	O
due	O
to	O
the	O
language	O
used	O
in	O
the	O
highlights	O
,	O
the	O
word	B-Method
distance	I-Method
model	E-Method
benefited	O
.	O

Particularly	O
in	O
the	O
case	O
of	O
the	O
Daily	B-Material
Mail	I-Material
dataset	E-Material
,	O
highlights	O
frequently	O
have	O
significant	O
lexical	O
overlap	O
with	O
passages	O
in	O
the	O
accompanying	O
article	O
,	O
which	O
makes	O
it	O
easy	O
for	O
the	O
word	B-Metric
distance	I-Metric
benchmark	E-Metric
.	O

For	O
instance	O
the	O
query	O
“	O
Tom	O
Hanks	O
is	O
friends	O
with	O
X	O
’s	O
manager	O
,	O
Scooter	O
Brown	O
”	O
has	O
the	O
phrase	O
“	O
…	O
turns	O
out	O
he	O
is	O
good	O
friends	O
with	O
Scooter	O
Brown	O
,	O
manager	O
for	O
Carly	O
Rae	O
Jepson	O
”	O
in	O
the	O
context	O
.	O

The	O
word	B-Method
distance	I-Method
benchmark	E-Method
correctly	O
aligns	O
these	O
two	O
while	O
the	O
frame	B-Method
-	I-Method
semantic	I-Method
approach	E-Method
fails	O
to	O
pickup	O
the	O
friendship	O
or	O
management	O
relations	O
when	O
parsing	O
the	O
query	O
.	O

We	O
expect	O
that	O
on	O
other	O
types	O
of	O
machine	B-Task
reading	E-Task
data	O
where	O
questions	O
rather	O
than	O
Cloze	O
queries	O
are	O
used	O
this	O
particular	O
model	O
would	O
perform	O
significantly	O
worse	O
.	O

paragraph	O
:	O
Neural	B-Method
models	E-Method
Within	O
the	O
group	O
of	O
neural	B-Method
models	E-Method
explored	O
here	O
,	O
the	O
results	O
paint	O
a	O
clear	O
picture	O
with	O
the	O
Impatient	O
and	O
the	O
Attentive	O
Readers	O
outperforming	O
all	O
other	O
models	O
.	O

This	O
is	O
consistent	O
with	O
our	O
hypothesis	O
that	O
attention	O
is	O
a	O
key	O
ingredient	O
for	O
machine	B-Task
reading	E-Task
and	O
question	B-Task
answering	E-Task
due	O
to	O
the	O
need	O
to	O
propagate	O
information	O
over	O
long	O
distances	O
.	O

The	O
Deep	O
LSTM	S-Method
Reader	O
performs	O
surprisingly	O
well	O
,	O
once	O
again	O
demonstrating	O
that	O
this	O
simple	O
sequential	B-Method
architecture	E-Method
can	O
do	O
a	O
reasonable	O
job	O
of	O
learning	O
to	O
abstract	O
long	O
sequences	O
,	O
even	O
when	O
they	O
are	O
up	O
to	O
two	O
thousand	O
tokens	O
in	O
length	O
.	O

However	O
this	O
model	O
does	O
fail	O
to	O
match	O
the	O
performance	O
of	O
the	O
attention	B-Method
based	I-Method
models	E-Method
,	O
even	O
though	O
these	O
only	O
use	O
single	B-Method
layer	I-Method
LSTMs	E-Method
.	O

The	O
poor	O
results	O
of	O
the	O
Uniform	O
Reader	O
support	O
our	O
hypothesis	O
of	O
the	O
significance	O
of	O
the	O
attention	B-Method
mechanism	E-Method
in	O
the	O
Attentive	B-Method
model	E-Method
’s	O
performance	O
as	O
the	O
only	O
difference	O
between	O
these	O
models	O
is	O
that	O
the	O
attention	O
variables	O
are	O
ignored	O
in	O
the	O
Uniform	B-Method
Reader	E-Method
.	O

The	O
precision@recall	B-Metric
statistics	E-Metric
in	O
Figure	O
[	O
reference	O
]	O
again	O
highlight	O
the	O
strength	O
of	O
the	O
attentive	B-Method
approach	E-Method
.	O

We	O
can	O
visualise	O
the	O
attention	B-Method
mechanism	E-Method
as	O
a	O
heatmap	S-Method
over	O
a	O
context	O
document	O
to	O
gain	O
further	O
insight	O
into	O
the	O
models	O
’	O
performance	O
.	O

The	O
highlighted	O
words	O
show	O
which	O
tokens	O
in	O
the	O
document	O
were	O
attended	O
to	O
by	O
the	O
model	O
.	O

In	O
addition	O
we	O
must	O
also	O
take	O
into	O
account	O
that	O
the	O
vectors	O
at	O
each	O
token	O
integrate	O
long	O
range	O
contextual	O
information	O
via	O
the	O
bidirectional	O
LSTM	S-Method
encoders	O
.	O

Figure	O
[	O
reference	O
]	O
depicts	O
heat	O
maps	O
for	O
two	O
queries	O
that	O
were	O
correctly	O
answered	O
by	O
the	O
Attentive	B-Method
Reader	E-Method
.	O

In	O
both	O
cases	O
confidently	O
arriving	O
at	O
the	O
correct	O
answer	O
requires	O
the	O
model	O
to	O
perform	O
both	O
significant	O
lexical	B-Method
generalsiation	E-Method
,	O
e.g.	O
‘	O
killed	O
’	O
‘	O
deceased	O
’	O
,	O
and	O
co	B-Task
-	I-Task
reference	E-Task
or	O
anaphora	B-Task
resolution	E-Task
,	O
e.g.	O
‘	O
ent119	O
was	O
killed	O
’	O
‘	O
he	O
was	O
identified	O
.	O

’	O
However	O
it	O
is	O
also	O
clear	O
that	O
the	O
model	O
is	O
able	O
to	O
integrate	O
these	O
signals	O
with	O
rough	O
heuristic	O
indicators	O
such	O
as	O
the	O
proximity	O
of	O
query	O
words	O
to	O
the	O
candidate	O
answer	O
.	O

section	O
:	O
Conclusion	O
The	O
supervised	B-Method
paradigm	E-Method
for	O
training	O
machine	B-Task
reading	E-Task
and	O
comprehension	O
models	O
provides	O
a	O
promising	O
avenue	O
for	O
making	O
progress	O
on	O
the	O
path	O
to	O
building	O
full	B-Task
natural	I-Task
language	I-Task
understanding	I-Task
systems	E-Task
.	O

We	O
have	O
demonstrated	O
a	O
methodology	O
for	O
obtaining	O
a	O
large	O
number	O
of	O
document	B-Task
-	I-Task
query	I-Task
-	I-Task
answer	I-Task
triples	E-Task
and	O
shown	O
that	O
recurrent	B-Method
and	I-Method
attention	I-Method
based	I-Method
neural	I-Method
networks	E-Method
provide	O
an	O
effective	O
modelling	B-Method
framework	E-Method
for	O
this	O
task	O
.	O

Our	O
analysis	O
indicates	O
that	O
the	O
Attentive	O
and	O
Impatient	O
Readers	O
are	O
able	O
to	O
propagate	O
and	O
integrate	O
semantic	O
information	O
over	O
long	O
distances	O
.	O

In	O
particular	O
we	O
believe	O
that	O
the	O
incorporation	O
of	O
an	O
attention	B-Method
mechanism	E-Method
is	O
the	O
key	O
contributor	O
to	O
these	O
results	O
.	O

The	O
attention	B-Method
mechanism	E-Method
that	O
we	O
have	O
employed	O
is	O
just	O
one	O
instantiation	O
of	O
a	O
very	O
general	O
idea	O
which	O
can	O
be	O
further	O
exploited	O
.	O

However	O
,	O
the	O
incorporation	O
of	O
world	O
knowledge	O
and	O
multi	B-Task
-	I-Task
document	I-Task
queries	E-Task
will	O
also	O
require	O
the	O
development	O
of	O
attention	B-Method
and	I-Method
embedding	I-Method
mechanisms	E-Method
whose	O
complexity	S-Metric
to	O
query	O
does	O
not	O
scale	O
linearly	O
with	O
the	O
data	O
set	O
size	O
.	O

There	O
are	O
still	O
many	O
queries	O
requiring	O
complex	O
inference	S-Task
and	O
long	B-Task
range	I-Task
reference	I-Task
resolution	E-Task
that	O
our	O
models	O
are	O
not	O
yet	O
able	O
to	O
answer	O
.	O

As	O
such	O
our	O
data	O
provides	O
a	O
scalable	O
challenge	O
that	O
should	O
support	O
NLP	S-Method
research	O
into	O
the	O
future	O
.	O

Further	O
,	O
significantly	O
bigger	O
training	O
data	O
sets	O
can	O
be	O
acquired	O
using	O
the	O
techniques	O
we	O
have	O
described	O
,	O
undoubtedly	O
allowing	O
us	O
to	O
train	O
more	O
expressive	O
and	O
accurate	O
models	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Model	B-Method
hyperparameters	E-Method
The	O
precise	O
hyperparameters	O
used	O
for	O
the	O
various	O
attentive	B-Method
models	E-Method
are	O
as	O
in	O
Table	O
[	O
reference	O
]	O
.	O

All	O
models	O
were	O
trained	O
using	O
asynchronous	B-Method
RmsProp	E-Method
with	O
a	O
momentum	O
of	O
and	O
a	O
decay	O
of	O
.	O

appendix	O
:	O
Performance	O
across	O
document	B-Metric
length	E-Metric
To	O
understand	O
how	O
the	O
model	O
performance	O
depends	O
on	O
the	O
size	O
of	O
the	O
context	O
,	O
we	O
plot	O
performance	O
versus	O
document	O
lengths	O
in	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
first	O
figure	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
plots	O
a	O
sliding	O
window	O
of	O
performance	O
across	O
document	O
length	O
,	O
showing	O
that	O
performance	O
of	O
the	O
attentive	B-Method
models	E-Method
degrades	O
slightly	O
as	O
documents	O
increase	O
in	O
length	O
.	O

The	O
second	O
figure	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
shows	O
the	O
cumulative	O
performance	O
with	O
documents	O
up	O
to	O
length	O
,	O
showing	O
that	O
while	O
the	O
length	O
does	O
impact	O
the	O
models	O
’	O
performance	O
,	O
that	O
effect	O
becomes	O
negligible	O
after	O
reaching	O
a	O
length	O
of	O
~500	O
tokens	O
.	O

Precision@Document	B-Metric
Length	E-Metric
for	O
the	O
attention	B-Method
models	E-Method
on	O
the	O
CNN	B-Material
validation	I-Material
data	E-Material
.	O

The	O
chart	O
shows	O
the	O
precision	S-Metric
for	O
each	O
decile	O
in	O
document	O
lengths	O
across	O
the	O
corpus	O
as	O
well	O
as	O
the	O
precision	S-Metric
for	O
the	O
5	O
%	O
longest	O
articles	O
.	O

Aggregated	B-Metric
precision	E-Metric
for	O
documents	O
up	O
to	O
a	O
certain	O
lengths	O
.	O

The	O
points	O
mark	O
the	O
decile	O
in	O
document	O
lengths	O
across	O
the	O
corpus	O
.	O

appendix	O
:	O
Additional	O
Heatmap	B-Method
Analysis	E-Method
We	O
expand	O
on	O
the	O
analysis	O
of	O
the	O
attention	B-Method
mechanism	E-Method
presented	O
in	O
the	O
paper	O
by	O
including	O
visualisations	S-Method
for	O
additional	O
queries	O
from	O
the	O
CNN	B-Material
validation	I-Material
dataset	E-Material
below	O
.	O

We	O
consider	O
examples	O
from	O
the	O
Attentive	B-Method
Reader	E-Method
as	O
well	O
as	O
the	O
Impatient	B-Method
Reader	E-Method
in	O
this	O
appendix	O
.	O

subsection	O
:	O
Attentive	B-Method
Reader	E-Method
paragraph	O
:	O
Positive	O
Instances	O
Figure	O
[	O
reference	O
]	O
shows	O
two	O
positive	O
examples	O
from	O
the	O
CNN	B-Material
validation	I-Material
set	E-Material
that	O
require	O
reasonable	O
levels	O
of	O
lexical	O
generalisation	O
and	O
co	O
-	O
reference	O
in	O
order	O
to	O
be	O
answered	O
.	O

The	O
first	O
query	O
in	O
Figure	O
[	O
reference	O
]	O
contains	O
strong	O
lexical	O
cues	O
through	O
the	O
quote	O
,	O
but	O
requires	O
identifying	O
the	O
entity	O
quoted	O
,	O
which	O
is	O
non	O
-	O
trivial	O
in	O
the	O
context	O
document	O
.	O

The	O
final	O
positive	O
example	O
(	O
also	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
demonstrates	O
the	O
fearlessness	O
of	O
our	O
model	O
.	O

paragraph	O
:	O
Negative	O
Instances	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
examples	O
of	O
queries	O
where	O
the	O
Attentive	B-Method
Reader	E-Method
fails	O
to	O
select	O
the	O
correct	O
answer	O
.	O

The	O
two	O
examples	O
in	O
Figure	O
[	O
reference	O
]	O
highlight	O
a	O
fairly	O
common	O
phenomenon	O
in	O
the	O
data	O
,	O
namely	O
ambiguous	O
queries	O
,	O
where	O
—	O
at	O
least	O
following	O
the	O
anonymisation	B-Method
process	E-Method
—	O
multiple	O
entities	O
are	O
plausible	O
answers	O
even	O
when	O
evaluated	O
manually	O
.	O

Note	O
that	O
in	O
both	O
cases	O
the	O
query	O
searches	O
for	O
an	O
entity	O
marker	O
that	O
describes	O
a	O
geographic	O
location	O
,	O
preceded	O
by	O
the	O
word	O
“	O
in	O
”	O
.	O

Here	O
it	O
is	O
unclear	O
whether	O
the	O
placeholder	O
refers	O
to	O
a	O
part	O
of	O
town	O
,	O
town	O
,	O
region	O
or	O
country	O
.	O

Figure	O
[	O
reference	O
]	O
contains	O
two	O
additional	O
negative	O
cases	O
.	O

The	O
first	O
failure	O
is	O
caused	O
by	O
the	O
co	B-Method
-	I-Method
reference	I-Method
entity	I-Method
selection	I-Method
process	E-Method
.	O

The	O
correct	O
entity	O
,	O
ent15	O
,	O
and	O
the	O
predicted	O
one	O
,	O
ent81	O
,	O
both	O
refer	O
to	O
the	O
same	O
person	O
,	O
but	O
not	O
being	O
clustered	O
together	O
.	O

Arguably	O
this	O
is	O
a	O
difficult	O
clustering	O
as	O
one	O
entity	O
refers	O
to	O
“	O
Kate	O
Middleton	O
”	O
and	O
the	O
other	O
to	O
“	O
The	O
Duchess	O
of	O
Cambridge	O
”	O
.	O

The	O
right	O
example	O
shows	O
a	O
situation	O
in	O
which	O
the	O
model	O
fails	O
as	O
it	O
perhaps	O
gets	O
too	O
little	O
information	O
from	O
the	O
short	O
query	O
and	O
then	O
selects	O
the	O
wrong	O
cue	O
with	O
the	O
term	O
“	O
claims	O
”	O
near	O
the	O
wrongly	O
identified	O
entity	O
ent1	O
(	O
correct	O
:	O
ent74	O
)	O
.	O

subsection	O
:	O
Impatient	B-Method
Reader	E-Method
To	O
give	O
a	O
better	O
intuition	O
for	O
the	O
behaviour	O
of	O
the	O
Impatient	B-Method
Reader	E-Method
,	O
we	O
use	O
a	O
similar	O
visualisation	B-Method
technique	E-Method
as	O
before	O
.	O

However	O
,	O
this	O
time	O
around	O
we	O
highlight	O
the	O
attention	O
at	O
every	O
time	O
step	O
as	O
the	O
model	O
updates	O
its	O
focus	O
while	O
moving	O
through	O
a	O
given	O
query	O
.	O

Figures	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
shows	O
how	O
the	O
attention	O
of	O
the	O
Impatient	B-Method
Reader	E-Method
changes	O
and	O
becomes	O
increasingly	O
more	O
accurate	O
as	O
the	O
model	O
considers	O
larger	O
parts	O
of	O
the	O
query	O
.	O

Note	O
how	O
the	O
attention	O
is	O
distributed	O
fairly	O
arbitraty	O
at	O
first	O
,	O
slowly	O
focussing	O
on	O
the	O
correct	O
entity	O
ent5	O
only	O
once	O
the	O
question	O
has	O
sufficiently	O
been	O
parsed	O
.	O

document	O
:	O
What	O
Do	O
Recurrent	B-Method
Neural	I-Method
Network	I-Method
Grammars	E-Method
Learn	O
About	O
Syntax	O
?	O
Recurrent	B-Method
neural	I-Method
network	I-Method
grammars	E-Method
(	O
RNNG	S-Method
)	O
are	O
a	O
recently	O
proposed	O
probabilistic	B-Method
generative	I-Method
modeling	I-Method
family	E-Method
for	O
natural	O
language	O
.	O

They	O
show	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
language	B-Method
modeling	E-Method
and	O
parsing	S-Task
performance	O
.	O

We	O
investigate	O
what	O
information	O
they	O
learn	O
,	O
from	O
a	O
linguistic	O
perspective	O
,	O
through	O
various	O
ablations	S-Method
to	O
the	O
model	O
and	O
the	O
data	O
,	O
and	O
by	O
augmenting	O
the	O
model	O
with	O
an	O
attention	B-Method
mechanism	E-Method
(	O
GA	B-Method
-	I-Method
RNNG	E-Method
)	O
to	O
enable	O
closer	O
inspection	O
.	O

We	O
find	O
that	O
explicit	O
modeling	O
of	O
composition	S-Task
is	O
crucial	O
for	O
achieving	O
the	O
best	O
performance	O
.	O

Through	O
the	O
attention	B-Method
mechanism	E-Method
,	O
we	O
find	O
that	O
headedness	O
plays	O
a	O
central	O
role	O
in	O
phrasal	B-Task
representation	E-Task
(	O
with	O
the	O
model	O
’s	O
latent	O
attention	O
largely	O
agreeing	O
with	O
predictions	O
made	O
by	O
hand	O
-	O
crafted	O
head	O
rules	O
,	O
albeit	O
with	O
some	O
important	O
differences	O
)	O
.	O

By	O
training	O
grammars	S-Method
without	O
nonterminal	O
labels	O
,	O
we	O
find	O
that	O
phrasal	O
representations	O
depend	O
minimally	O
on	O
nonterminals	O
,	O
providing	O
support	O
for	O
the	O
endocentricity	B-Method
hypothesis	E-Method
.	O

noitemsep	O
,	O
topsep=10pt	O
,	O
parsep=0pt	O
,	O
partopsep=0pt	O
noitemsep	O
,	O
topsep=10pt	O
,	O
parsep=0pt	O
,	O
partopsep=0pt	O
section	O
:	O
Introduction	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
a	O
recently	O
proposed	O
class	O
of	O
probability	B-Method
distributions	E-Method
,	O
recurrent	B-Method
neural	I-Method
network	I-Method
grammars	E-Method
(	O
RNNGs	S-Method
;	O
Dyer	O
et	O
al	O
.	O

,	O
2016	O
)	O
,	O
designed	O
to	O
model	O
syntactic	B-Task
derivations	I-Task
of	I-Task
sentences	E-Task
.	O

We	O
focus	O
on	O
RNNGs	S-Method
as	O
generative	B-Method
probabilistic	I-Method
models	E-Method
over	O
trees	S-Method
,	O
as	O
summarized	O
in	O
§	O
[	O
reference	O
]	O
.	O

Fitting	O
a	O
probabilistic	B-Method
model	E-Method
to	O
data	O
has	O
often	O
been	O
understood	O
as	O
a	O
way	O
to	O
test	O
or	O
confirm	O
some	O
aspect	O
of	O
a	O
theory	O
.	O

We	O
talk	O
about	O
a	O
model	O
’s	O
assumptions	O
and	O
sometimes	O
explore	O
its	O
parameters	O
or	O
posteriors	O
over	O
its	O
latent	O
variables	O
in	O
order	O
to	O
gain	O
understanding	O
of	O
what	O
it	O
“	O
discovers	O
”	O
from	O
the	O
data	O
.	O

In	O
some	O
sense	O
,	O
such	O
models	O
can	O
be	O
thought	O
of	O
as	O
mini	O
-	O
scientists	O
.	O

Neural	B-Method
networks	E-Method
,	O
including	O
RNNGs	S-Method
,	O
are	O
capable	O
of	O
representing	O
larger	O
classes	O
of	O
hypotheses	O
than	O
traditional	O
probabilistic	B-Method
models	E-Method
,	O
giving	O
them	O
more	O
freedom	O
to	O
explore	O
.	O

Unfortunately	O
,	O
they	O
tend	O
to	O
be	O
bad	O
mini	O
-	O
scientists	O
,	O
because	O
their	O
parameters	O
are	O
difficult	O
for	O
human	O
scientists	O
to	O
interpret	O
.	O

RNNGs	S-Method
are	O
striking	O
because	O
they	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
parsing	S-Task
and	O
language	B-Task
modeling	E-Task
performance	O
.	O

Their	O
relative	O
lack	O
of	O
independence	O
assumptions	O
,	O
while	O
still	O
incorporating	O
a	O
degree	O
of	O
linguistically	O
-	O
motivated	O
prior	O
knowledge	O
,	O
affords	O
the	O
model	O
considerable	O
freedom	O
to	O
derive	O
its	O
own	O
insights	O
about	O
syntax	O
.	O

If	O
they	O
are	O
mini	O
-	O
scientists	O
,	O
the	O
discoveries	O
they	O
make	O
should	O
be	O
of	O
particular	O
interest	O
as	O
propositions	O
about	O
syntax	O
(	O
at	O
least	O
for	O
the	O
particular	O
genre	O
and	O
dialect	O
of	O
the	O
data	O
)	O
.	O

This	O
paper	O
manipulates	O
the	O
inductive	O
bias	O
of	O
RNNGs	S-Method
to	O
test	O
linguistic	B-Task
hypotheses	E-Task
.	O

We	O
begin	O
with	O
an	O
ablation	O
study	O
to	O
discover	O
the	O
importance	O
of	O
the	O
composition	O
function	O
in	O
§	O
[	O
reference	O
]	O
.	O

Based	O
on	O
the	O
findings	O
,	O
we	O
augment	O
the	O
RNNG	B-Method
composition	I-Method
function	E-Method
with	O
a	O
novel	O
gated	B-Method
attention	I-Method
mechanism	E-Method
(	O
leading	O
to	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
)	O
to	O
incorporate	O
more	O
interpretability	O
into	O
the	O
model	O
in	O
§	O
[	O
reference	O
]	O
.	O

Using	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
,	O
we	O
proceed	O
by	O
investigating	O
the	O
role	O
that	O
individual	O
heads	O
play	O
in	O
phrasal	B-Method
representation	E-Method
(	O
§	O
[	O
reference	O
]	O
)	O
and	O
the	O
role	O
that	O
nonterminal	O
category	O
labels	O
play	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

Our	O
key	O
findings	O
are	O
that	O
lexical	O
heads	O
play	O
an	O
important	O
role	O
in	O
representing	O
most	O
phrase	O
types	O
(	O
although	O
compositions	O
of	O
multiple	O
salient	O
heads	O
are	O
not	O
infrequent	O
,	O
especially	O
for	O
conjunctions	O
)	O
and	O
that	O
nonterminal	O
labels	O
provide	O
little	O
additional	O
information	O
.	O

As	O
a	O
by	O
-	O
product	O
of	O
our	O
investigation	O
,	O
a	O
variant	O
of	O
the	O
RNNG	S-Method
without	O
ensembling	S-Method
achieved	O
the	O
best	O
reported	O
supervised	O
phrase	O
-	O
structure	O
parsing	S-Task
(	O
93.6	O
;	O
English	B-Material
PTB	E-Material
)	O
and	O
,	O
through	O
conversion	S-Method
,	O
dependency	O
parsing	S-Task
(	O
95.8	O
UAS	O
,	O
94.6	O
LAS	O
;	O
PTB	S-Material
SD	O
)	O
.	O

The	O
code	O
and	O
pretrained	B-Method
models	E-Method
to	O
replicate	O
our	O
results	O
are	O
publicly	O
available	O
.	O

section	O
:	O
Recurrent	B-Method
Neural	I-Method
Network	I-Method
Grammars	E-Method
An	O
RNNG	S-Method
defines	O
a	O
joint	O
probability	O
distribution	O
over	O
string	O
terminals	O
and	O
phrase	O
-	O
structure	O
nonterminals	O
.	O

Formally	O
,	O
the	O
RNNG	S-Method
is	O
defined	O
by	O
a	O
triple	O
,	O
where	O
denotes	O
the	O
set	O
of	O
nonterminal	O
symbols	O
(	O
NP	O
,	O
VP	O
,	O
etc	O
.	O

)	O
,	O
the	O
set	O
of	O
all	O
terminal	O
symbols	O
(	O
we	O
assume	O
that	O
)	O
,	O
and	O
the	O
set	O
of	O
all	O
model	O
parameters	O
.	O

Unlike	O
previous	O
works	O
that	O
rely	O
on	O
hand	O
-	O
crafted	O
rules	O
to	O
compose	O
more	O
fine	O
-	O
grained	O
phrase	B-Method
representations	E-Method
,	O
the	O
RNNG	S-Method
implicitly	O
parameterizes	O
the	O
information	O
passed	O
through	O
compositions	O
of	O
phrases	O
(	O
in	O
and	O
the	O
neural	B-Method
network	I-Method
architecture	E-Method
)	O
,	O
hence	O
weakening	O
the	O
strong	O
independence	O
assumptions	O
in	O
classical	O
probabilistic	B-Method
context	I-Method
-	I-Method
free	I-Method
grammars	E-Method
.	O

The	O
RNNG	S-Method
is	O
based	O
on	O
an	O
abstract	B-Method
state	I-Method
machine	E-Method
like	O
those	O
used	O
in	O
transition	O
-	O
based	O
parsing	S-Task
,	O
with	O
its	O
algorithmic	O
state	O
consisting	O
of	O
a	O
stack	O
of	O
partially	O
completed	O
constituents	O
,	O
a	O
buffer	O
of	O
already	O
-	O
generated	O
terminal	O
symbols	O
,	O
and	O
a	O
list	O
of	O
past	O
actions	O
.	O

To	O
generate	O
a	O
sentence	O
and	O
its	O
phrase	O
-	O
structure	O
tree	O
,	O
the	O
RNNG	S-Method
samples	O
a	O
sequence	O
of	O
actions	O
to	O
construct	O
top	O
-	O
down	O
.	O

Given	O
,	O
there	O
is	O
one	O
such	O
sequence	O
(	O
easily	O
identified	O
)	O
,	O
which	O
we	O
call	O
the	O
oracle	S-Method
,	O
used	O
during	O
supervised	B-Task
training	E-Task
.	O

The	O
RNNG	S-Method
uses	O
three	O
different	O
actions	O
:	O
,	O
where	O
,	O
introduces	O
an	O
open	O
nonterminal	O
symbol	O
onto	O
the	O
stack	O
,	O
e.g.	O
,	O
“	O
(	O
NP	O
”	O
;	O
,	O
where	O
,	O
generates	O
a	O
terminal	O
symbol	O
and	O
places	O
it	O
on	O
the	O
stack	O
and	O
buffer	O
;	O
and	O
reduce	O
indicates	O
a	O
constituent	O
is	O
now	O
complete	O
.	O

The	O
elements	O
of	O
the	O
stack	O
that	O
comprise	O
the	O
current	O
constituent	O
(	O
going	O
back	O
to	O
the	O
last	O
open	O
nonterminal	O
)	O
are	O
popped	O
,	O
a	O
composition	B-Method
function	E-Method
is	O
executed	O
,	O
yielding	O
a	O
composed	B-Method
representation	E-Method
that	O
is	O
pushed	O
onto	O
the	O
stack	O
.	O

At	O
each	O
timestep	O
,	O
the	O
model	O
encodes	O
the	O
stack	O
,	O
buffer	O
,	O
and	O
past	O
actions	O
,	O
with	O
a	O
separate	O
LSTM	S-Method
for	O
each	O
component	O
as	O
features	O
to	O
define	O
a	O
distribution	O
over	O
the	O
next	O
action	O
to	O
take	O
(	O
conditioned	O
on	O
the	O
full	O
algorithmic	O
state	O
)	O
.	O

The	O
overall	O
architecture	O
is	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
;	O
examples	O
of	O
full	O
action	O
sequences	O
can	O
be	O
found	O
in	O
rnng	S-Method
.	O

A	O
key	O
element	O
of	O
the	O
RNNG	S-Method
is	O
the	O
composition	B-Method
function	E-Method
,	O
which	O
reduces	O
a	O
completed	O
constituent	O
into	O
a	O
single	O
element	O
on	O
the	O
stack	O
.	O

This	O
function	O
computes	O
a	O
vector	B-Method
representation	E-Method
of	O
the	O
new	O
constituent	O
;	O
it	O
also	O
uses	O
an	O
LSTM	S-Method
(	O
here	O
a	O
bidirectional	B-Method
one	E-Method
)	O
.	O

This	O
composition	B-Method
function	E-Method
,	O
which	O
we	O
consider	O
in	O
greater	O
depth	O
in	O
§	O
[	O
reference	O
]	O
,	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Since	O
the	O
RNNG	S-Method
is	O
a	O
generative	B-Method
model	E-Method
,	O
it	O
attempts	O
to	O
maximize	O
,	O
the	O
joint	O
distribution	O
of	O
strings	O
and	O
trees	O
,	O
defined	O
as	O
In	O
other	O
words	O
,	O
is	O
defined	O
as	O
a	O
product	O
of	O
local	O
probabilities	O
,	O
conditioned	O
on	O
all	O
past	O
actions	O
.	O

The	O
joint	B-Method
probability	I-Method
estimate	E-Method
can	O
be	O
used	O
for	O
both	O
phrase	O
-	O
structure	O
parsing	S-Task
(	O
finding	S-Task
)	O
and	O
language	B-Task
modeling	E-Task
(	O
finding	S-Task
by	O
marginalizing	O
over	O
the	O
set	O
of	O
possible	O
parses	O
for	O
)	O
.	O

Both	O
inference	B-Task
problems	E-Task
can	O
be	O
solved	O
using	O
an	O
importance	B-Method
sampling	I-Method
procedure	E-Method
.	O

We	O
report	O
all	O
RNNG	S-Method
performance	O
based	O
on	O
the	O
corrigendum	O
to	O
rnng	S-Method
.	O

section	O
:	O
Composition	S-Task
is	O
Key	O
Given	O
the	O
same	O
data	O
,	O
under	O
both	O
the	O
discriminative	B-Method
and	I-Method
generative	I-Method
settings	I-Method
RNNGs	E-Method
were	O
found	O
to	O
parse	O
with	O
significantly	O
higher	O
accuracy	S-Metric
than	O
(	O
respectively	O
)	O
the	O
models	O
of	O
vinyals:2015	O
and	O
choe:2016	O
that	O
represent	O
as	O
a	O
“	O
linearized	O
”	O
sequence	O
of	O
symbols	O
and	O
parentheses	O
without	O
explicitly	O
capturing	O
the	O
tree	O
structure	O
,	O
or	O
even	O
constraining	O
the	O
to	O
be	O
a	O
well	O
-	O
formed	O
tree	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

vinyals:2015	O
directly	O
predict	O
the	O
sequence	O
of	O
nonterminals	O
,	O
“	O
shifts	O
”	O
(	O
which	O
consume	O
a	O
terminal	O
symbol	O
)	O
,	O
and	O
parentheses	O
from	O
left	O
to	O
right	O
,	O
conditional	O
on	O
the	O
input	O
terminal	O
sequence	O
,	O
while	O
choe:2016	O
used	O
a	O
sequential	B-Method
LSTM	I-Method
language	I-Method
model	E-Method
on	O
the	O
same	O
linearized	O
trees	O
to	O
create	O
a	O
generative	B-Method
variant	E-Method
of	O
the	O
vinyals:2015	O
model	O
.	O

The	O
generative	B-Method
model	E-Method
is	O
used	O
to	O
re	O
-	O
rank	O
parse	O
candidates	O
.	O

The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
suggest	O
that	O
the	O
RNNG	B-Method
’s	I-Method
explicit	I-Method
composition	I-Method
function	E-Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
which	O
vinyals:2015	O
and	O
choe:2016	O
must	O
learn	O
implicitly	O
,	O
plays	O
a	O
crucial	O
role	O
in	O
the	O
RNNG	B-Metric
’s	I-Metric
generalization	I-Metric
success	E-Metric
.	O

Beyond	O
this	O
,	O
Choe	O
and	O
Charniak	O
’s	O
generative	B-Method
variant	E-Method
of	O
Vinyals	O
et	O
al	O
.	O

(	O
2015	O
)	O
is	O
another	O
instance	O
where	O
generative	B-Method
models	E-Method
trained	O
on	O
the	O
PTB	S-Material
outperform	O
discriminative	B-Method
models	E-Method
.	O

subsection	O
:	O
Ablated	B-Method
RNNGs	E-Method
On	O
close	O
inspection	O
,	O
it	O
is	O
clear	O
that	O
the	O
RNNG	S-Method
’s	O
three	O
data	O
structures	O
—	O
stack	O
,	O
buffer	O
,	O
and	O
action	O
history	O
—	O
are	O
redundant	O
.	O

For	O
example	O
,	O
the	O
action	O
history	O
and	O
buffer	O
contents	O
completely	O
determine	O
the	O
structure	O
of	O
the	O
stack	O
at	O
every	O
timestep	O
.	O

Every	O
generated	O
word	O
goes	O
onto	O
the	O
stack	O
,	O
too	O
;	O
and	O
some	O
past	O
words	O
will	O
be	O
composed	O
into	O
larger	O
structures	O
,	O
but	O
through	O
the	O
composition	B-Method
function	E-Method
,	O
they	O
are	O
all	O
still	O
“	O
available	O
”	O
to	O
the	O
network	O
that	O
predicts	O
the	O
next	O
action	O
.	O

Similarly	O
,	O
the	O
past	O
actions	O
are	O
redundant	O
with	O
the	O
stack	O
.	O

Despite	O
this	O
redundancy	O
,	O
only	O
the	O
stack	O
incorporates	O
the	O
composition	O
function	O
.	O

Since	O
each	O
of	O
the	O
ablated	B-Method
models	E-Method
is	O
sufficient	O
to	O
encode	O
all	O
necessary	O
partial	O
tree	O
information	O
,	O
the	O
primary	O
difference	O
is	O
that	O
ablations	S-Method
with	O
the	O
stack	O
use	O
explicit	O
composition	O
,	O
to	O
which	O
we	O
can	O
therefore	O
attribute	O
most	O
of	O
the	O
performance	O
difference	O
.	O

We	O
conjecture	O
that	O
the	O
stack	O
—	O
the	O
component	O
that	O
makes	O
use	O
of	O
the	O
composition	O
function	O
—	O
is	O
critical	O
to	O
the	O
RNNG	S-Method
’s	O
performance	O
,	O
and	O
that	O
the	O
buffer	O
and	O
action	O
history	O
are	O
not	O
.	O

In	O
transition	B-Method
-	I-Method
based	I-Method
parsers	E-Method
built	O
on	O
expert	O
-	O
crafted	O
features	O
,	O
the	O
most	O
recent	O
words	O
and	O
actions	O
are	O
useful	O
if	O
they	O
are	O
salient	O
,	O
although	O
neural	B-Method
representation	I-Method
learners	E-Method
can	O
automatically	O
learn	O
what	O
information	O
should	O
be	O
salient	O
.	O

To	O
test	O
this	O
conjecture	O
,	O
we	O
train	O
ablated	B-Method
RNNGs	E-Method
that	O
lack	O
each	O
of	O
the	O
three	O
data	O
structures	O
(	O
action	O
history	O
,	O
buffer	O
,	O
stack	O
)	O
,	O
as	O
well	O
as	O
one	O
that	O
lacks	O
both	O
the	O
action	O
history	O
and	O
buffer	O
.	O

If	O
our	O
conjecture	O
is	O
correct	O
,	O
performance	O
should	O
degrade	O
most	O
without	O
the	O
stack	O
,	O
and	O
the	O
stack	S-Method
alone	O
should	O
perform	O
competitively	O
.	O

Experimental	O
settings	O
.	O

We	O
perform	O
our	O
experiments	O
on	O
the	O
English	B-Material
PTB	I-Material
corpus	E-Material
,	O
with	O
§	O
02–21	O
for	O
training	O
,	O
§	O
24	O
for	O
validation	S-Task
,	O
and	O
§	O
23	O
for	O
test	O
;	O
no	O
additional	O
data	O
were	O
used	O
for	O
training	O
.	O

We	O
follow	O
the	O
same	O
hyperparameters	O
as	O
the	O
generative	B-Method
model	E-Method
proposed	O
in	O
rnng	S-Method
.	O

The	O
generative	B-Method
model	E-Method
did	O
not	O
use	O
any	O
pretrained	O
word	O
embeddings	O
or	O
POS	O
tags	O
;	O
a	O
discriminative	B-Method
variant	E-Method
of	O
the	O
standard	O
RNNG	S-Method
was	O
used	O
to	O
obtain	O
tree	O
samples	O
for	O
the	O
generative	B-Method
model	E-Method
.	O

All	O
further	O
experiments	O
use	O
the	O
same	O
settings	O
and	O
hyperparameters	O
unless	O
otherwise	O
noted	O
.	O

Experimental	O
results	O
.	O

We	O
trained	O
each	O
ablation	O
from	O
scratch	O
,	O
and	O
compared	O
these	O
models	O
on	O
three	O
tasks	O
:	O
English	O
phrase	O
-	O
structure	O
parsing	S-Task
(	O
labeled	O
)	O
,	O
Table	O
[	O
reference	O
]	O
;	O
dependency	O
parsing	S-Task
,	O
Table	O
[	O
reference	O
]	O
,	O
by	O
converting	O
parse	O
output	O
to	O
Stanford	O
dependencies	O
using	O
the	O
tool	O
by	O
kong_14	O
;	O
and	O
language	B-Method
modeling	E-Method
,	O
Table	O
[	O
reference	O
]	O
.	O

The	O
last	O
row	O
of	O
each	O
table	O
reports	O
the	O
performance	O
of	O
a	O
novel	O
variant	O
of	O
the	O
(	B-Method
stack	I-Method
-	I-Method
only	I-Method
)	I-Method
RNNG	E-Method
with	O
attention	S-Method
,	O
to	O
be	O
presented	O
in	O
§	O
[	O
reference	O
]	O
.	O

Discussion	O
.	O

The	O
RNNG	S-Method
with	O
only	O
a	O
stack	O
is	O
the	O
strongest	O
of	O
the	O
ablations	S-Method
,	O
and	O
it	O
even	O
outperforms	O
the	O
“	O
full	B-Method
”	I-Method
RNNG	E-Method
with	O
all	O
three	O
data	O
structures	O
.	O

Ablating	O
the	O
stack	O
gives	O
the	O
worst	O
among	O
the	O
new	O
results	O
.	O

This	O
strongly	O
supports	O
the	O
importance	O
of	O
the	O
composition	B-Method
function	E-Method
:	O
a	O
proper	O
reduce	B-Method
operation	E-Method
that	O
transforms	O
a	O
constituent	O
’s	O
parts	O
and	O
nonterminal	O
label	O
into	O
a	O
single	O
explicit	O
(	O
vector	B-Method
)	I-Method
representation	E-Method
is	O
helpful	O
to	O
performance	O
.	O

It	O
is	O
noteworthy	O
that	O
the	O
stack	S-Method
alone	O
is	O
stronger	O
than	O
the	O
original	O
RNNG	S-Method
,	O
which	O
—	O
in	O
principle	O
—	O
can	O
learn	O
to	O
disregard	O
the	O
buffer	O
and	O
action	O
history	O
.	O

Since	O
the	O
stack	O
maintains	O
syntactically	O
“	O
recent	O
”	O
information	O
near	O
its	O
top	O
,	O
we	O
conjecture	O
that	O
the	O
learner	O
is	O
overfitting	O
to	O
spurious	O
predictors	O
in	O
the	O
buffer	O
and	O
action	O
history	O
that	O
explain	O
the	O
training	O
data	O
but	O
do	O
not	O
generalize	O
well	O
.	O

A	O
similar	O
performance	O
degradation	O
is	O
seen	O
in	O
language	B-Method
modeling	E-Method
(	O
Table	O
[	O
reference	O
]	O
)	O
:	O
the	O
stack	B-Method
-	I-Method
only	I-Method
RNNG	E-Method
achieves	O
the	O
best	O
performance	O
,	O
and	O
ablating	O
the	O
stack	O
is	O
most	O
harmful	O
.	O

Indeed	O
,	O
modeling	O
syntax	O
without	O
explicit	O
composition	O
(	O
the	O
stack	B-Method
-	I-Method
ablated	I-Method
RNNG	E-Method
)	O
provides	O
little	O
benefit	O
over	O
a	O
sequential	B-Method
LSTM	I-Method
language	I-Method
model	E-Method
.	O

We	O
remark	O
that	O
the	O
stack	O
-	O
only	O
results	O
are	O
the	O
best	O
published	O
PTB	S-Material
results	O
for	O
both	O
phrase	O
-	O
structure	O
and	O
dependency	O
parsing	S-Task
among	O
supervised	B-Method
models	E-Method
.	O

section	O
:	O
Gated	B-Method
Attention	I-Method
RNNG	E-Method
Having	O
established	O
that	O
the	O
composition	O
function	O
is	O
key	O
to	O
RNNG	S-Task
performance	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
we	O
now	O
seek	O
to	O
understand	O
the	O
nature	O
of	O
the	O
composed	O
phrasal	B-Method
representations	E-Method
that	O
are	O
learned	O
.	O

Like	O
most	O
neural	B-Method
networks	E-Method
,	O
interpreting	O
the	O
composition	O
function	O
’s	O
behavior	O
is	O
challenging	O
.	O

Fortunately	O
,	O
linguistic	B-Method
theories	E-Method
offer	O
a	O
number	O
of	O
hypotheses	O
about	O
the	O
nature	O
of	O
representations	O
of	O
phrases	O
that	O
can	O
provide	O
a	O
conceptual	O
scaffolding	O
to	O
understand	O
them	O
.	O

subsection	O
:	O
Linguistic	O
Hypotheses	O
We	O
consider	O
two	O
theories	O
about	O
phrasal	B-Task
representation	E-Task
.	O

The	O
first	O
is	O
that	O
phrasal	B-Method
representations	E-Method
are	O
strongly	O
determined	O
by	O
a	O
privileged	O
lexical	O
head	O
.	O

Augmenting	B-Method
grammars	E-Method
with	O
lexical	O
head	O
information	O
has	O
a	O
long	O
history	O
in	O
parsing	S-Task
,	O
starting	O
with	O
the	O
models	O
of	O
collins_97	S-Method
,	O
and	O
theories	O
of	O
syntax	O
such	O
as	O
the	O
“	O
bare	O
phrase	O
structure	O
”	O
hypothesis	O
of	O
the	O
Minimalist	B-Method
Program	E-Method
posit	O
that	O
phrases	O
are	O
represented	O
purely	O
by	O
single	O
lexical	O
heads	O
.	O

Proposals	O
for	O
multiple	O
headed	O
phrases	O
(	O
to	O
deal	O
with	O
tricky	O
cases	O
like	O
conjunction	O
)	O
likewise	O
exist	O
.	O

Do	O
the	O
phrasal	B-Method
representations	E-Method
learned	O
by	O
RNNGs	S-Method
depend	O
on	O
individual	O
lexical	O
heads	O
or	O
multiple	O
heads	O
?	O
Or	O
do	O
the	O
representations	O
combine	O
all	O
children	O
without	O
any	O
salient	O
head	O
?	O
Related	O
to	O
the	O
question	O
about	O
the	O
role	O
of	O
heads	O
in	O
phrasal	B-Task
representation	E-Task
is	O
the	O
question	O
of	O
whether	O
phrase	O
-	O
internal	O
material	O
wholly	O
determines	O
the	O
representation	O
of	O
a	O
phrase	O
(	O
an	O
endocentric	B-Method
representation	E-Method
)	O
or	O
whether	O
nonterminal	O
relabeling	O
of	O
a	O
constitutent	O
introduces	O
new	O
information	O
(	O
exocentric	O
representations	O
)	O
.	O

To	O
illustrate	O
the	O
contrast	O
,	O
an	O
endocentric	B-Method
representation	E-Method
is	O
representing	O
a	O
noun	O
phrase	O
with	O
a	O
noun	O
category	O
,	O
whereas	O
exocentrically	O
introduces	O
a	O
new	O
syntactic	O
category	O
that	O
is	O
neither	O
NP	O
nor	O
VP	O
.	O

subsection	O
:	O
Gated	B-Method
Attention	I-Method
Composition	E-Method
To	O
investigate	O
what	O
the	O
stack	B-Method
-	I-Method
only	I-Method
RNNG	E-Method
learns	O
about	O
headedness	O
(	O
and	O
later	O
endocentricity	O
)	O
,	O
we	O
propose	O
a	O
variant	O
of	O
the	O
composition	B-Method
function	E-Method
that	O
makes	O
use	O
of	O
an	O
explicit	O
attention	B-Method
mechanism	E-Method
and	O
a	O
sigmoid	B-Method
gate	E-Method
with	O
multiplicative	O
interactions	O
,	O
henceforth	O
called	O
GA	B-Method
-	I-Method
RNNG	E-Method
.	O

At	O
every	O
reduce	B-Method
operation	E-Method
,	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
assigns	O
an	O
“	O
attention	O
weight	O
”	O
to	O
each	O
of	O
its	O
children	O
(	O
between	O
0	O
and	O
1	O
such	O
that	O
the	O
total	O
weight	O
off	O
all	O
children	O
sums	O
to	O
1	O
)	O
,	O
and	O
the	O
parent	O
phrase	O
is	O
represented	O
by	O
the	O
combination	O
of	O
a	O
sum	O
of	O
each	O
child	O
’s	O
representation	O
scaled	O
by	O
its	O
attention	O
weight	O
and	O
its	O
nonterminal	O
type	O
.	O

Our	O
weighted	B-Method
sum	E-Method
is	O
more	O
expressive	O
than	O
traditional	O
head	O
rules	O
,	O
however	O
,	O
because	O
it	O
allows	O
attention	O
to	O
be	O
divided	O
among	O
multiple	O
constituents	O
.	O

Head	O
rules	O
,	O
conversely	O
,	O
are	O
analogous	O
to	O
giving	O
all	O
attention	O
to	O
one	O
constituent	O
,	O
the	O
one	O
containing	O
the	O
lexical	O
head	O
.	O

We	O
now	O
formally	O
define	O
the	O
GA	B-Method
-	I-Method
RNNG	I-Method
’s	I-Method
composition	I-Method
function	E-Method
.	O

Recall	O
that	O
is	O
the	O
concatenation	O
of	O
the	O
vector	B-Method
representations	I-Method
of	I-Method
the	I-Method
RNNG	I-Method
’s	I-Method
data	I-Method
structures	E-Method
,	O
used	O
to	O
assign	O
probabilities	O
to	O
each	O
of	O
the	O
actions	O
available	O
at	O
timestep	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
the	O
layer	O
before	O
the	O
softmax	O
at	O
the	O
top	O
)	O
.	O

For	O
simplicity	O
,	O
we	O
drop	O
the	O
timestep	O
index	O
here	O
.	O

Let	O
denote	O
the	O
vector	O
embedding	O
(	O
learned	O
)	O
of	O
the	O
nonterminal	O
being	O
constructed	O
,	O
for	O
the	O
purpose	O
of	O
computing	O
attention	O
weights	O
.	O

Now	O
let	O
denote	O
the	O
sequence	O
of	O
vector	O
embeddings	O
for	O
the	O
constituents	O
of	O
the	O
new	O
phrase	O
.	O

The	O
length	O
of	O
these	O
vectors	O
is	O
defined	O
by	O
the	O
dimensionality	O
of	O
the	O
bidirectional	B-Method
LSTM	E-Method
used	O
in	O
the	O
original	O
composition	O
function	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
use	O
semicolon	O
(	O
;	O
)	O
to	O
denote	O
vector	O
concatenation	O
operations	O
.	O

The	O
attention	O
vector	O
is	O
given	O
by	O
:	O
Note	O
that	O
the	O
length	O
of	O
is	O
the	O
same	O
as	O
the	O
number	O
of	O
constituents	O
,	O
and	O
that	O
this	O
vector	O
sums	O
to	O
one	O
due	O
to	O
the	O
softmax	S-Method
.	O

It	O
divides	O
a	O
single	O
unit	O
of	O
attention	O
among	O
the	O
constituents	O
.	O

Next	O
,	O
note	O
that	O
the	O
constituent	O
source	O
vector	O
is	O
a	O
convex	B-Method
combination	E-Method
of	O
the	O
child	O
-	O
constituents	O
,	O
weighted	O
by	O
attention	O
.	O

We	O
will	O
combine	O
this	O
with	O
another	O
embedding	O
of	O
the	O
nonterminal	O
denoted	O
as	O
(	O
separate	O
from	O
)	O
using	O
a	O
sigmoid	B-Method
gating	I-Method
mechanism	E-Method
:	O
Note	O
that	O
the	O
value	O
of	O
the	O
gate	O
is	O
bounded	O
between	O
in	O
each	O
dimension	O
.	O

The	O
new	O
phrase	O
’s	O
final	O
representation	O
uses	O
element	O
-	O
wise	O
multiplication	O
(	O
)	O
with	O
respect	O
to	O
both	O
and	O
,	O
a	O
process	O
reminiscent	O
of	O
the	O
LSTM	S-Method
“	O
forget	B-Method
”	I-Method
gate	E-Method
:	O
The	O
intuition	O
is	O
that	O
the	O
composed	B-Method
representation	E-Method
should	O
incorporate	O
both	O
nonterminal	O
information	O
and	O
information	O
about	O
the	O
constituents	O
(	O
through	O
weighted	B-Method
sum	I-Method
and	I-Method
attention	I-Method
mechanism	E-Method
)	O
.	O

The	O
gate	O
modulates	O
the	O
interaction	O
between	O
them	O
to	O
account	O
for	O
varying	O
importance	O
between	O
the	O
two	O
in	O
different	O
contexts	O
.	O

Experimental	O
results	O
.	O

We	O
include	O
this	O
model	O
’s	O
performance	O
in	O
Tables	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
(	O
last	O
row	O
in	O
all	O
tables	O
)	O
.	O

It	O
is	O
clear	O
that	O
the	O
model	O
outperforms	O
the	O
baseline	B-Method
RNNG	E-Method
with	O
all	O
three	O
structures	O
present	O
and	O
achieves	O
competitive	O
performance	O
with	O
the	O
strongest	O
,	O
stack	B-Method
-	I-Method
only	I-Method
,	I-Method
RNNG	I-Method
variant	E-Method
.	O

section	O
:	O
Headedness	O
in	O
Phrases	O
We	O
now	O
exploit	O
the	O
attention	B-Method
mechanism	E-Method
to	O
probe	O
what	O
the	O
RNNG	S-Method
learns	O
about	O
headedness	O
on	O
the	O
WSJ	S-Material
§	O
23	O
test	O
set	O
(	O
unseen	O
before	O
by	O
the	O
model	O
)	O
.	O

subsection	O
:	O
The	O
Heads	O
that	O
GA	B-Method
-	I-Method
RNNG	E-Method
Learns	O
The	O
attention	O
weight	O
vectors	O
tell	O
us	O
which	O
constituents	O
are	O
most	O
important	O
to	O
a	O
phrase	O
’s	O
vector	B-Method
representation	E-Method
in	O
the	O
stack	O
.	O

Here	O
,	O
we	O
inspect	O
the	O
attention	O
vectors	O
to	O
investigate	O
whether	O
the	O
model	O
learns	O
to	O
center	O
its	O
attention	O
around	O
a	O
single	O
,	O
or	O
by	O
extension	O
a	O
few	O
,	O
salient	O
elements	O
,	O
which	O
would	O
confirm	O
the	O
presence	O
of	O
headedness	O
in	O
GA	B-Method
-	I-Method
RNNG	E-Method
.	O

First	O
,	O
we	O
consider	O
several	O
major	O
nonterminal	O
categories	O
,	O
and	O
estimate	O
the	O
average	B-Metric
perplexity	I-Metric
of	E-Metric
the	O
attention	O
vectors	O
.	O

The	O
average	O
perplexity	O
can	O
be	O
interpreted	O
as	O
the	O
average	O
number	O
of	O
“	O
choices	O
”	O
for	O
each	O
nonterminal	O
category	O
;	O
this	O
value	O
is	O
only	O
computed	O
for	O
the	O
cases	O
where	O
the	O
number	O
of	O
components	O
in	O
the	O
composed	O
phrase	O
is	O
at	O
least	O
two	O
(	O
otherwise	O
the	O
attention	O
weight	O
would	O
be	O
trivially	O
1	O
)	O
.	O

The	O
minimum	O
possible	O
value	O
for	O
the	O
perplexity	O
is	O
1	O
,	O
indicating	O
a	O
full	O
attention	O
weight	O
around	O
one	O
component	O
and	O
zero	O
everywhere	O
else	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
in	O
blue	O
)	O
shows	O
much	O
less	O
than	O
2	O
average	O
“	O
choices	O
”	O
across	O
nonterminal	O
categories	O
,	O
which	O
also	O
holds	O
true	O
for	O
all	O
other	O
categories	O
not	O
shown	O
.	O

For	O
comparison	O
we	O
also	O
report	O
the	O
average	B-Metric
perplexity	E-Metric
of	O
the	O
uniform	B-Method
distribution	E-Method
for	O
the	O
same	O
nonterminal	O
categories	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
in	O
red	O
)	O
;	O
this	O
represents	O
the	O
highest	O
entropy	O
cases	O
where	O
there	O
is	O
no	O
headedness	O
at	O
all	O
by	O
assigning	O
the	O
same	O
attention	O
weight	O
to	O
each	O
constituent	O
(	O
e.g.	O
attention	O
weights	O
of	O
0.25	O
each	O
for	O
phrases	O
with	O
four	O
constituents	O
)	O
.	O

It	O
is	O
clear	O
that	O
the	O
learned	O
attention	O
weights	O
have	O
much	O
lower	O
perplexity	O
than	O
the	O
uniform	O
distribution	O
baseline	O
,	O
indicating	O
that	O
the	O
learned	O
attention	O
weights	O
are	O
quite	O
peaked	O
around	O
certain	O
components	O
.	O

This	O
implies	O
that	O
phrases	O
’	O
vectors	O
tend	O
to	O
resemble	O
the	O
vector	O
of	O
one	O
salient	O
constituent	O
,	O
but	O
not	O
exclusively	O
,	O
as	O
the	O
perplexity	O
for	O
most	O
categories	O
is	O
still	O
not	O
close	O
to	O
one	O
.	O

Next	O
,	O
we	O
consider	O
the	O
how	O
attention	O
is	O
distributed	O
for	O
the	O
major	O
nonterminal	O
categories	O
in	O
Table	O
[	O
reference	O
]	O
,	O
where	O
the	O
first	O
five	O
rows	O
of	O
each	O
category	O
represent	O
compositions	O
with	O
highest	O
entropy	O
,	O
and	O
the	O
next	O
five	O
rows	O
are	O
qualitatively	O
analyzed	O
.	O

The	O
high	O
-	O
entropy	O
cases	O
where	O
the	O
attention	O
is	O
most	O
divided	O
represent	O
more	O
complex	O
phrases	O
with	O
conjunctions	O
or	O
more	O
than	O
one	O
plausible	O
head	O
.	O

NPs	O
.	O

In	O
most	O
simple	O
noun	O
phrases	O
(	O
representative	O
samples	O
in	O
rows	O
6–7	O
of	O
Table	O
[	O
reference	O
]	O
)	O
,	O
the	O
model	O
pays	O
the	O
most	O
attention	O
to	O
the	O
rightmost	O
noun	O
and	O
assigns	O
near	O
-	O
zero	O
attention	O
on	O
determiners	O
and	O
possessive	O
determiners	O
,	O
while	O
also	O
paying	O
nontrivial	O
attention	O
weights	O
to	O
the	O
adjectives	O
.	O

This	O
finding	O
matches	O
existing	O
head	O
rules	O
and	O
our	O
intuition	O
that	O
nouns	O
head	O
noun	O
phrases	O
,	O
and	O
that	O
adjectives	O
are	O
more	O
important	O
than	O
determiners	O
.	O

We	O
analyze	O
the	O
case	O
where	O
the	O
noun	O
phrase	O
contains	O
a	O
conjunction	O
in	O
the	O
last	O
three	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
.	O

The	O
syntax	O
of	O
conjunction	O
is	O
a	O
long	O
-	O
standing	O
source	O
of	O
controversy	O
in	O
syntactic	B-Task
analysis	E-Task
.	O

Our	O
model	O
suggests	O
that	O
several	O
representational	B-Method
strategies	E-Method
are	O
used	O
,	O
when	O
coordinating	O
single	O
nouns	O
,	O
both	O
the	O
first	O
noun	O
(	O
8	O
)	O
and	O
the	O
last	O
noun	O
(	O
9	O
)	O
may	O
be	O
selected	O
.	O

However	O
,	O
in	O
the	O
case	O
of	O
conjunctions	O
of	O
multiple	O
noun	O
phrases	O
(	O
as	O
opposed	O
to	O
multiple	O
single	O
-	O
word	O
nouns	O
)	O
,	O
the	O
model	O
consistently	O
picks	O
the	O
conjunction	O
as	O
the	O
head	O
.	O

All	O
of	O
these	O
representational	B-Method
strategies	E-Method
have	O
been	O
argued	O
for	O
individually	O
on	O
linguistic	O
grounds	O
,	O
and	O
since	O
we	O
see	O
all	O
of	O
them	O
present	O
,	O
RNNGs	S-Method
face	O
the	O
same	O
confusion	O
that	O
linguists	O
do	O
.	O

VPs	O
.	O

The	O
attention	O
weights	O
on	O
simple	O
verb	O
phrases	O
(	O
e.g.	O
,	O
“	O
VP	O
V	O
NP	O
”	O
,	O
9	O
)	O
are	O
peaked	O
around	O
the	O
noun	O
phrase	O
instead	O
of	O
the	O
verb	O
.	O

This	O
implies	O
that	O
the	O
verb	O
phrase	O
would	O
look	O
most	O
similar	O
to	O
the	O
noun	O
under	O
it	O
and	O
contradicts	O
existing	O
head	O
rules	O
that	O
unanimously	O
put	O
the	O
verb	O
as	O
the	O
head	O
of	O
the	O
verb	O
phrase	O
.	O

Another	O
interesting	O
finding	O
is	O
that	O
the	O
model	O
pays	O
attention	O
to	O
polarity	O
information	O
,	O
where	O
negations	O
are	O
almost	O
always	O
assigned	O
non	O
-	O
trivial	O
attention	O
weights	O
.	O

Furthermore	O
,	O
we	O
find	O
that	O
the	O
model	O
attends	O
to	O
the	O
conjunction	O
terminal	O
in	O
conjunctions	O
of	O
verb	O
phrases	O
(	O
e.g.	O
,	O
“	O
VP	O
VP	O
and	O
VP	O
”	O
,	O
10	O
)	O
,	O
reinforcing	O
the	O
similar	O
finding	O
for	O
conjunction	B-Task
of	I-Task
noun	I-Task
phrases	E-Task
.	O

PPs	O
.	O

In	O
almost	O
all	O
cases	O
,	O
the	O
model	O
attends	O
to	O
the	O
preposition	O
terminal	O
instead	O
of	O
the	O
noun	O
phrases	O
or	O
complete	O
clauses	O
under	O
it	O
,	O
regardless	O
of	O
the	O
type	O
of	O
preposition	O
.	O

Even	O
when	O
the	O
prepositional	O
phrase	O
is	O
only	O
used	O
to	O
make	O
a	O
connection	O
between	O
two	O
noun	O
phrases	O
(	O
e.g.	O
,	O
“	O
PP	O
NP	O
after	O
NP	O
”	O
,	O
10	O
)	O
,	O
the	O
prepositional	O
connector	O
is	O
still	O
considered	O
the	O
most	O
salient	O
element	O
.	O

This	O
is	O
less	O
consistent	O
with	O
the	O
Collins	O
and	O
Stanford	O
head	O
rules	O
,	O
where	O
prepositions	O
are	O
assigned	O
a	O
lower	O
priority	O
when	O
composing	O
PPs	O
,	O
although	O
more	O
consistent	O
with	O
the	O
Johansson	B-Method
head	I-Method
rule	E-Method
.	O

ADJP	O
VP	O
NP	O
PP	O
QP	O
SBAR	O
subsection	O
:	O
Comparison	O
to	O
Existing	O
Head	O
Rules	O
To	O
better	O
measure	O
the	O
overlap	O
between	O
the	O
attention	O
vectors	O
and	O
existing	O
head	O
rules	O
,	O
we	O
converted	O
the	O
trees	O
in	O
PTB	S-Material
§	O
23	O
into	O
a	O
dependency	B-Method
representation	E-Method
using	O
the	O
attention	O
weights	O
.	O

In	O
this	O
case	O
,	O
the	O
attention	O
weight	O
functions	O
as	O
a	O
“	O
dynamic	O
”	O
head	O
rule	O
,	O
where	O
all	O
other	O
constituents	O
within	O
the	O
same	O
composed	O
phrase	O
are	O
considered	O
to	O
modify	O
the	O
constituent	O
with	O
the	O
highest	O
attention	O
weight	O
,	O
repeated	O
recursively	O
.	O

The	O
head	O
of	O
the	O
composed	O
representation	O
for	O
“	O
S	O
”	O
at	O
the	O
top	O
of	O
the	O
tree	O
is	O
attached	O
to	O
a	O
special	O
root	O
symbol	O
and	O
functions	O
as	O
the	O
head	O
of	O
the	O
sentence	O
.	O

We	O
measure	O
the	O
overlap	O
between	O
the	O
resulting	O
tree	O
and	O
conversion	O
results	O
of	O
the	O
same	O
trees	O
using	O
the	O
collins_97	O
and	O
Stanford	O
dependencies	O
head	O
rules	O
.	O

Results	O
are	O
evaluated	O
using	O
the	O
standard	O
evaluation	B-Metric
script	E-Metric
(	O
excluding	O
punctuation	O
)	O
in	O
terms	O
of	O
UAS	S-Task
,	O
since	O
the	O
attention	O
weights	O
do	O
not	O
provide	O
labels	O
.	O

Results	O
.	O

The	O
model	O
has	O
a	O
higher	O
overlap	O
with	O
the	O
conversion	O
using	O
Collins	O
head	O
rules	O
(	O
49.8	O
UAS	O
)	O
rather	O
than	O
the	O
Stanford	O
head	O
rules	O
(	O
40.4	O
UAS	O
)	O
.	O

We	O
attribute	O
this	O
large	O
gap	O
to	O
the	O
fact	O
that	O
the	O
Stanford	O
head	O
rules	O
incorporate	O
more	O
semantic	O
considerations	O
,	O
while	O
the	O
RNNG	S-Method
is	O
a	O
purely	O
syntactic	B-Method
model	E-Method
.	O

In	O
general	O
,	O
the	O
attention	B-Method
-	I-Method
based	I-Method
tree	I-Method
output	E-Method
has	O
a	O
high	O
error	B-Metric
rate	E-Metric
(	O
90	O
%	O
)	O
when	O
the	O
dependent	O
is	O
a	O
verb	O
,	O
since	O
the	O
constituent	O
with	O
the	O
highest	O
attention	O
weight	O
in	O
a	O
verb	O
phrase	O
is	O
often	O
the	O
noun	O
phrase	O
instead	O
of	O
the	O
verb	O
,	O
as	O
discussed	O
above	O
.	O

The	O
conversion	B-Metric
accuracy	E-Metric
is	O
better	O
for	O
nouns	O
(	O
50	O
%	O
error	S-Metric
)	O
,	O
and	O
much	O
better	O
for	O
determiners	S-Task
(	O
30	O
%	O
)	O
and	O
particles	O
(	O
6	O
%	O
)	O
with	O
respect	O
to	O
the	O
Collins	O
head	O
rules	O
.	O

Discussion	O
.	O

GA	B-Method
-	I-Method
RNNG	E-Method
has	O
the	O
power	O
to	O
infer	O
head	O
rules	O
,	O
and	O
to	O
a	O
large	O
extent	O
,	O
it	O
does	O
.	O

It	O
follows	O
some	O
conventions	O
that	O
are	O
established	O
in	O
one	O
or	O
more	O
previous	O
head	O
rule	O
sets	O
(	O
e.g.	O
,	O
prepositions	O
head	O
prepositional	O
phrases	O
,	O
nouns	O
head	O
noun	O
phrases	O
)	O
,	O
but	O
attends	O
more	O
to	O
a	O
verb	O
phrase	O
’s	O
nominal	O
constituents	O
than	O
the	O
verb	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
this	O
is	O
not	O
the	O
by	O
-	O
product	O
of	O
learning	O
a	O
specific	O
model	O
for	O
parsing	S-Task
;	O
the	O
training	B-Metric
objective	E-Metric
is	O
joint	O
likelihood	O
,	O
which	O
is	O
not	O
a	O
proxy	B-Metric
loss	E-Metric
for	O
parsing	S-Task
performance	O
.	O

These	O
decisions	O
were	O
selected	O
because	O
they	O
make	O
the	O
data	O
maximally	O
likely	O
(	O
though	O
admittedly	O
only	O
locally	O
maximally	O
likely	O
)	O
.	O

We	O
leave	O
deeper	O
consideration	O
of	O
this	O
noun	O
-	O
centered	O
verb	O
phrase	O
hypothesis	O
to	O
future	O
work	O
.	O

section	O
:	O
The	O
Role	O
of	O
Nonterminal	O
Labels	O
Emboldened	O
by	O
our	O
finding	O
that	O
GA	B-Method
-	I-Method
RNNGs	E-Method
learn	O
a	O
notion	O
of	O
headedness	O
,	O
we	O
next	O
explore	O
whether	O
heads	O
are	O
sufficient	O
to	O
create	O
representations	O
of	O
phrases	O
(	O
in	O
line	O
with	O
an	O
endocentric	B-Method
theory	I-Method
of	I-Method
phrasal	I-Method
representation	E-Method
)	O
or	O
whether	O
extra	O
nonterminal	O
information	O
is	O
necessary	O
.	O

If	O
the	O
endocentric	B-Method
hypothesis	E-Method
is	O
true	O
(	O
that	O
is	O
,	O
the	O
representation	O
of	O
a	O
phrase	O
is	O
built	O
from	O
within	O
depending	O
on	O
its	O
components	O
but	O
independent	O
of	O
explicit	O
category	O
labels	O
)	O
,	O
then	O
the	O
nonterminal	O
types	O
should	O
be	O
easily	O
inferred	O
given	O
the	O
endocentrically	B-Method
-	I-Method
composed	I-Method
representation	E-Method
,	O
and	O
that	O
ablating	O
the	O
nonterminal	O
information	O
would	O
not	O
make	O
much	O
difference	O
in	O
performance	O
.	O

Specifically	O
,	O
we	O
train	O
a	O
GA	B-Method
-	I-Method
RNNG	E-Method
on	O
unlabeled	O
trees	O
(	O
only	O
bracketings	O
without	O
nonterminal	O
types	O
)	O
,	O
denoted	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
.	O

This	O
idea	O
has	O
been	O
explored	O
in	O
research	O
on	O
methods	O
for	O
learning	B-Task
syntax	I-Task
with	I-Task
less	I-Task
complete	I-Task
annotation	E-Task
.	O

A	O
key	O
finding	O
from	O
klein_02	O
was	O
that	O
,	O
given	O
bracketing	O
structure	O
,	O
simple	O
dimensionality	B-Method
reduction	I-Method
techniques	E-Method
could	O
reveal	O
conventional	O
nonterminal	O
categories	O
with	O
high	O
accuracy	S-Metric
;	O
petrov_2006	O
also	O
showed	O
that	O
latent	O
variables	O
can	O
be	O
used	O
to	O
recover	O
fine	O
-	O
grained	O
nonterminal	O
categories	O
.	O

We	O
therefore	O
expect	O
that	O
the	O
vector	O
embeddings	O
of	O
the	O
constituents	O
that	O
the	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
correctly	O
recovers	O
(	O
on	O
test	O
data	O
)	O
will	O
capture	O
categories	O
similar	O
to	O
those	O
in	O
the	O
Penn	B-Material
Treebank	E-Material
.	O

Experiments	O
.	O

Using	O
the	O
same	O
hyperparameters	O
and	O
the	O
PTB	S-Material
dataset	O
,	O
we	O
first	O
consider	O
unlabeled	O
parsing	S-Task
accuracy	O
.	O

On	O
test	O
data	O
(	O
with	O
the	O
usual	O
split	O
)	O
,	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
achieves	O
94.2	O
%	O
,	O
while	O
the	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
achieves	O
93.5	O
%	O
.	O

This	O
result	O
suggests	O
that	O
nonterminal	O
category	O
labels	O
add	O
a	O
relatively	O
small	O
amount	O
of	O
information	O
compared	O
to	O
purely	O
endocentric	B-Method
representations	E-Method
.	O

Visualization	S-Task
.	O

If	O
endocentricity	S-Method
is	O
largely	O
sufficient	O
to	O
account	O
for	O
the	O
behavior	O
of	O
phrases	O
,	O
where	O
do	O
our	O
robust	O
intuitions	O
for	O
syntactic	O
category	O
types	O
come	O
from	O
?	O
We	O
use	O
t	B-Method
-	I-Method
SNE	E-Method
to	O
visualize	O
composed	O
phrase	O
vectors	O
from	O
the	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	I-Method
model	E-Method
applied	O
to	O
the	O
unseen	O
test	O
data	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
the	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
tends	O
to	O
recover	O
nonterminal	O
categories	O
as	O
encoded	O
in	O
the	O
PTB	S-Material
,	O
even	O
when	O
trained	O
without	O
them	O
.	O

These	O
results	O
suggest	O
nonterminal	O
types	O
can	O
be	O
inferred	O
from	O
the	O
purely	O
endocentric	B-Method
compositional	I-Method
process	E-Method
to	O
a	O
certain	O
extent	O
,	O
and	O
that	O
the	O
phrase	O
clusters	O
found	O
by	O
the	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
largely	O
overlap	O
with	O
nonterminal	O
categories	O
.	O

Analysis	B-Task
of	I-Task
PP	E-Task
and	O
SBAR	S-Method
.	O

Figure	O
[	O
reference	O
]	O
indicates	O
a	O
certain	O
degree	O
of	O
overlap	O
between	O
SBAR	O
(	O
red	O
)	O
and	O
PP	O
(	O
yellow	O
)	O
.	O

As	O
both	O
categories	O
are	O
interesting	O
from	O
the	O
linguistic	O
perspective	O
and	O
quite	O
similar	O
,	O
we	O
visualize	O
the	O
learned	O
phrase	O
vectors	O
of	O
40	O
randomly	O
selected	O
SBARs	O
and	O
PPs	O
from	O
the	O
test	O
set	O
(	O
using	O
U	B-Method
-	I-Method
GA	I-Method
-	I-Method
RNNG	E-Method
)	O
,	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

First	O
,	O
we	O
can	O
see	O
that	O
phrase	B-Method
representations	E-Method
for	O
PPs	O
and	O
SBARs	O
depend	O
less	O
on	O
the	O
nonterminal	O
categories	O
and	O
more	O
on	O
the	O
connector	O
.	O

For	O
instance	O
,	O
the	O
model	O
learns	O
to	O
cluster	O
phrases	O
that	O
start	O
with	O
words	O
that	O
can	O
be	O
either	O
prepositions	O
or	O
complementizers	O
(	O
e.g.	O
,	O
for	O
,	O
at	O
,	O
to	O
,	O
under	O
,	O
by	O
)	O
,	O
regardless	O
of	O
whether	O
the	O
true	O
nonterminal	O
labels	O
are	O
PPs	O
or	O
SBARs	O
.	O

This	O
suggests	O
that	O
SBARs	S-Method
that	O
start	O
with	O
“	O
prepositional	O
”	O
words	O
are	O
similar	O
to	O
PPs	O
from	O
the	O
model	O
’s	O
perspective	O
.	O

Second	O
,	O
the	O
model	O
learns	O
to	O
disregard	O
the	O
word	O
that	O
,	O
as	O
“	O
”	O
and	O
“	O
”	O
are	O
close	O
together	O
.	O

This	O
finding	O
is	O
intuitive	O
,	O
as	O
complementizer	O
that	O
is	O
often	O
optional	O
,	O
unlike	O
prepositional	O
words	O
that	O
might	O
describe	O
relative	O
time	O
and	O
location	O
.	O

Third	O
,	O
certain	O
categories	O
of	O
PPs	O
and	O
SBARs	O
form	O
their	O
own	O
separate	O
clusters	O
,	O
such	O
as	O
those	O
that	O
involve	O
the	O
words	O
because	O
and	O
of	O
.	O

We	O
attribute	O
these	O
distinctions	O
to	O
the	O
fact	O
that	O
these	O
words	O
convey	O
different	O
meanings	O
than	O
many	O
prepositional	O
words	O
;	O
the	O
word	O
of	O
indicates	O
possession	O
while	O
because	O
indicates	O
cause	O
-	O
and	O
-	O
effect	O
relationship	O
.	O

These	O
examples	O
show	O
that	O
,	O
to	O
a	O
certain	O
extent	O
,	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
is	O
able	O
to	O
learn	O
non	O
-	O
trivial	O
semantic	O
information	O
,	O
even	O
when	O
trained	O
on	O
a	O
fairly	O
small	O
amount	O
of	O
syntactic	O
data	O
.	O

section	O
:	O
Related	O
Work	O
The	O
problem	O
of	O
understanding	O
neural	B-Method
network	I-Method
models	E-Method
in	O
NLP	S-Task
has	O
been	O
previously	O
studied	O
for	O
sequential	B-Method
RNNs	E-Method
.	O

shi_16	O
showed	O
that	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
neural	I-Method
translation	I-Method
models	E-Method
capture	O
a	O
certain	O
degree	O
of	O
syntactic	O
knowledge	O
of	O
the	O
source	O
language	O
,	O
such	O
as	O
voice	O
(	O
active	O
or	O
passive	O
)	O
and	O
tense	O
information	O
,	O
as	O
a	O
by	O
-	O
product	O
of	O
the	O
translation	B-Metric
objective	E-Metric
.	O

Our	O
experiment	O
on	O
the	O
importance	O
of	O
composition	O
function	O
was	O
motivated	O
by	O
vinyals:2015	O
and	O
wiseman_16	S-Method
,	O
who	O
achieved	O
competitive	O
parsing	S-Task
accuracy	O
without	O
explicit	O
composition	O
.	O

In	O
another	O
work	O
,	O
li:15	O
investigated	O
the	O
importance	O
of	O
recursive	O
tree	O
structures	O
(	O
as	O
opposed	O
to	O
linear	B-Method
recurrent	I-Method
models	E-Method
)	O
in	O
four	O
different	O
tasks	O
,	O
including	O
sentiment	B-Task
and	I-Task
semantic	I-Task
relation	I-Task
classification	E-Task
.	O

Their	O
findings	O
suggest	O
that	O
recursive	O
tree	O
structures	O
are	O
beneficial	O
for	O
tasks	O
that	O
require	O
identifying	B-Task
long	I-Task
-	I-Task
range	I-Task
relations	E-Task
,	O
such	O
as	O
semantic	B-Task
relationship	I-Task
classification	E-Task
,	O
with	O
no	O
conclusive	O
advantage	O
for	O
sentiment	B-Task
classification	E-Task
and	O
discourse	O
parsing	S-Task
.	O

Through	O
the	O
stack	B-Method
-	I-Method
only	I-Method
ablation	E-Method
we	O
demonstrate	O
that	O
the	O
RNNG	B-Method
composition	I-Method
function	E-Method
is	O
crucial	O
to	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
parsing	S-Task
performance	O
.	O

Extensive	O
prior	O
work	O
on	O
phrase	O
-	O
structure	O
parsing	S-Task
typically	O
employs	O
the	O
probabilistic	B-Method
context	I-Method
-	I-Method
free	I-Method
grammar	I-Method
formalism	E-Method
,	O
with	O
lexicalized	B-Method
and	I-Method
nonterminal	I-Method
augmentations	E-Method
.	O

The	O
conjecture	O
that	O
fine	O
-	O
grained	O
nonterminal	O
rules	O
and	O
labels	O
can	O
be	O
discovered	O
given	O
weaker	O
bracketing	O
structures	O
was	O
based	O
on	O
several	O
studies	O
.	O

In	O
a	O
similar	O
work	O
,	O
sangati:09	O
proposed	O
entropy	B-Method
minimization	E-Method
and	O
greedy	B-Method
familiarity	I-Method
maximization	I-Method
techniques	E-Method
to	O
obtain	O
lexical	O
heads	O
from	O
labeled	O
phrase	O
-	O
structure	O
trees	O
in	O
an	O
unsupervised	B-Method
manner	E-Method
.	O

In	O
contrast	O
,	O
we	O
used	O
neural	B-Method
attention	E-Method
to	O
obtain	O
the	O
“	O
head	O
rules	O
”	O
in	O
the	O
GA	B-Method
-	I-Method
RNNG	E-Method
;	O
the	O
whole	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
maximize	O
the	O
log	O
probability	O
of	O
the	O
correct	O
action	O
given	O
the	O
history	O
.	O

Unlike	O
prior	O
work	O
,	O
GA	B-Method
-	I-Method
RNNG	E-Method
allows	O
the	O
attention	O
weight	O
to	O
be	O
divided	O
among	O
phrase	O
constituents	O
,	O
essentially	O
propagating	O
(	O
weighted	O
)	O
headedness	O
information	O
from	O
multiple	O
components	O
.	O

section	O
:	O
Conclusion	O
We	O
probe	O
what	O
recurrent	B-Method
neural	I-Method
network	I-Method
grammars	E-Method
learn	O
about	O
syntax	O
,	O
through	O
ablation	B-Method
scenarios	E-Method
and	O
a	O
novel	O
variant	O
with	O
a	O
gated	B-Method
attention	I-Method
mechanism	E-Method
on	O
the	O
composition	O
function	O
.	O

The	O
composition	O
function	O
,	O
a	O
key	O
differentiator	O
between	O
the	O
RNNG	S-Method
and	O
other	O
neural	B-Method
models	I-Method
of	I-Method
syntax	E-Method
,	O
is	O
crucial	O
for	O
good	O
performance	O
.	O

Using	O
the	O
attention	O
vectors	O
we	O
discover	O
that	O
the	O
model	O
is	O
learning	O
something	O
similar	O
to	O
heads	O
,	O
although	O
the	O
attention	O
vectors	O
are	O
not	O
completely	O
peaked	O
around	O
a	O
single	O
component	O
.	O

We	O
show	O
some	O
cases	O
where	O
the	O
attention	O
vector	O
is	O
divided	O
and	O
measure	O
the	O
relationship	O
with	O
existing	O
head	O
rules	O
.	O

RNNGs	S-Method
without	O
access	O
to	O
nonterminal	O
information	O
during	O
training	O
are	O
used	O
to	O
support	O
the	O
hypothesis	O
that	O
phrasal	B-Method
representations	E-Method
are	O
largely	O
endocentric	O
,	O
and	O
a	O
visualization	B-Method
of	I-Method
representations	E-Method
shows	O
that	O
traditional	O
nonterminal	O
categories	O
fall	O
out	O
naturally	O
from	O
the	O
composed	O
phrase	O
vectors	O
.	O

This	O
confirms	O
previous	O
conjectures	O
that	O
bracketing	B-Method
annotation	E-Method
does	O
most	O
of	O
the	O
work	O
of	O
syntax	S-Task
,	O
with	O
nonterminal	O
categories	O
easily	O
discoverable	O
given	O
bracketing	S-Method
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Information	O
Innovation	O
Office	O
(	O
I2O	O
)	O
under	O
the	O
Low	O
Resource	O
Languages	O
for	O
Emergent	O
Incidents	O
(	O
LORELEI	O
)	O
program	O
issued	O
by	O
DARPA	O
/	O
I2O	O
under	O
Contract	O
No	O
.	O

HR0011	O
-	O
15	O
-	O
C	O
-	O
0114	O
;	O
it	O
was	O
also	O
supported	O
in	O
part	O
by	O
Contract	O
No	O
.	O

W911NF	O
-	O
15	O
-	O
1	O
-	O
0543	O
with	O
DARPA	O
and	O
the	O
Army	O
Research	O
Office	O
(	O
ARO	O
)	O
.	O

Approved	O
for	O
public	O
release	O
,	O
distribution	O
unlimited	O
.	O

The	O
views	O
expressed	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
official	O
policy	O
or	O
position	O
of	O
the	O
Department	O
of	O
Defense	O
or	O
the	O
U.S.	O
Government	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Evaluation	O
of	O
Output	B-Method
Embeddings	E-Method
for	O
Fine	B-Task
-	I-Task
Grained	I-Task
Image	I-Task
Classification	I-Task
Image	I-Task
classification	E-Task
has	O
advanced	O
significantly	O
in	O
recent	O
years	O
with	O
the	O
availability	O
of	O
large	O
-	O
scale	O
image	O
sets	O
.	O

However	O
,	O
fine	B-Task
-	I-Task
grained	I-Task
classification	E-Task
remains	O
a	O
major	O
challenge	O
due	O
to	O
the	O
annotation	B-Metric
cost	E-Metric
of	O
large	O
numbers	O
of	O
fine	O
-	O
grained	O
categories	O
.	O

This	O
project	O
shows	O
that	O
compelling	O
classification	S-Task
performance	O
can	O
be	O
achieved	O
on	O
such	O
categories	O
even	O
without	O
labeled	O
training	O
data	O
.	O

Given	O
image	O
and	O
class	O
embeddings	O
,	O
we	O
learn	O
a	O
compatibility	O
function	O
such	O
that	O
matching	O
embeddings	O
are	O
assigned	O
a	O
higher	O
score	O
than	O
mismatching	O
ones	O
;	O
zero	B-Task
-	I-Task
shot	I-Task
classification	E-Task
of	O
an	O
image	O
proceeds	O
by	O
finding	O
the	O
label	O
yielding	O
the	O
highest	O
joint	B-Metric
compatibility	I-Metric
score	E-Metric
.	O

We	O
use	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
image	O
features	O
and	O
focus	O
on	O
different	O
supervised	O
attributes	O
and	O
unsupervised	O
output	O
embeddings	O
either	O
derived	O
from	O
hierarchies	S-Method
or	O
learned	O
from	O
unlabeled	O
text	O
corpora	O
.	O

We	O
establish	O
a	O
substantially	O
improved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
Animals	O
with	O
Attributes	O
and	O
Caltech	O
-	O
UCSD	O
Birds	O
datasets	O
.	O

Most	O
encouragingly	O
,	O
we	O
demonstrate	O
that	O
purely	O
unsupervised	O
output	O
embeddings	O
(	O
learned	O
from	O
Wikipedia	O
and	O
improved	O
with	O
fine	O
-	O
grained	O
text	O
)	O
achieve	O
compelling	O
results	O
,	O
even	O
outperforming	O
the	O
previous	O
supervised	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

By	O
combining	O
different	O
output	O
embeddings	O
,	O
we	O
further	O
improve	O
results	O
.	O

section	O
:	O
Introduction	O
The	O
image	B-Task
classification	I-Task
problem	E-Task
has	O
been	O
redefined	O
by	O
the	O
emergence	O
of	O
large	O
scale	O
datasets	O
such	O
as	O
ImageNet	O
.	O

Since	O
deep	B-Method
learning	I-Method
methods	E-Method
dominated	O
recent	O
Large	B-Task
-	I-Task
Scale	I-Task
Visual	I-Task
Recognition	I-Task
Challenges	E-Task
(	O
ILSVRC12	S-Task
-	O
14	O
)	O
,	O
the	O
attention	O
of	O
the	O
computer	B-Task
vision	I-Task
community	E-Task
has	O
been	O
drawn	O
to	O
Convolutional	B-Method
Neural	I-Method
Networks	E-Method
(	O
CNN	S-Method
)	O
.	O

Training	O
CNNs	S-Method
requires	O
massive	O
amounts	O
of	O
labeled	O
data	O
;	O
but	O
,	O
in	O
fine	O
-	O
grained	O
image	O
collections	O
,	O
where	O
the	O
categories	O
are	O
visually	O
very	O
similar	O
,	O
the	O
data	O
population	O
decreases	O
significantly	O
.	O

We	O
are	O
interested	O
in	O
the	O
most	O
extreme	O
case	O
of	O
learning	S-Task
with	O
a	O
limited	O
amount	O
of	O
labeled	O
data	O
,	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
,	O
in	O
which	O
no	O
labeled	O
data	O
is	O
available	O
for	O
some	O
classes	O
.	O

Without	O
labels	O
,	O
we	O
need	O
alternative	O
sources	O
of	O
information	O
that	O
relate	O
object	O
classes	O
.	O

Attributes	O
,	O
which	O
describe	O
well	O
-	O
known	O
common	O
characteristics	O
of	O
objects	O
,	O
are	O
an	O
appealing	O
source	O
of	O
information	O
,	O
and	O
they	O
can	O
be	O
easily	O
obtained	O
through	O
crowd	B-Method
-	I-Method
sourcing	I-Method
techniques	E-Method
.	O

However	O
,	O
fine	O
-	O
grained	O
concepts	O
present	O
a	O
special	O
challenge	O
:	O
due	O
to	O
the	O
high	O
degree	O
of	O
similarity	O
among	O
categories	O
,	O
a	O
large	O
number	O
of	O
attributes	O
are	O
required	O
to	O
effectively	O
model	O
these	O
subtle	O
differences	O
.	O

This	O
increases	O
the	O
cost	O
of	O
attribute	B-Task
annotation	E-Task
.	O

One	O
aim	O
of	O
this	O
work	O
is	O
to	O
move	O
towards	O
eliminating	O
the	O
human	B-Method
labeling	I-Method
component	E-Method
from	O
zero	B-Method
-	I-Method
shot	I-Method
learning	E-Method
,	O
e.g.	O
by	O
using	O
alternative	O
sources	O
of	O
information	O
.	O

On	O
the	O
other	O
hand	O
,	O
large	B-Method
-	I-Method
margin	I-Method
support	I-Method
vector	I-Method
machines	E-Method
(	O
SVM	B-Method
)	E-Method
operate	O
with	O
labeled	O
training	O
images	O
,	O
so	O
a	O
lack	O
of	O
labels	O
limits	O
their	O
use	O
for	O
this	O
task	O
.	O

Inspired	O
by	O
previous	O
work	O
on	O
label	B-Task
embedding	E-Task
and	O
structured	B-Task
SVMs	E-Task
,	O
we	O
propose	O
to	O
use	O
a	O
Structured	B-Method
Joint	I-Method
Embedding	E-Method
(	O
SJE	S-Method
)	O
framework	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
that	O
relates	O
input	O
embeddings	O
(	O
i.e.	O
image	O
features	O
)	O
and	O
output	O
embeddings	O
(	O
i.e.	O
side	O
information	O
)	O
through	O
a	O
compatibility	O
function	O
,	O
therefore	O
taking	O
advantage	O
of	O
a	O
structure	O
in	O
the	O
output	O
space	O
.	O

The	O
SJE	S-Method
framework	O
separates	O
the	O
subspace	B-Task
learning	I-Task
problem	E-Task
from	O
the	O
specific	O
input	O
and	O
output	O
features	O
used	O
in	O
a	O
given	O
application	O
.	O

As	O
a	O
general	O
framework	O
,	O
it	O
can	O
be	O
applied	O
to	O
any	O
learning	B-Task
problem	E-Task
where	O
more	O
than	O
one	O
modality	O
is	O
provided	O
for	O
an	O
object	O
.	O

Our	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
demonstrate	O
that	O
unsupervised	B-Method
class	I-Method
embeddings	E-Method
trained	O
from	O
large	O
unlabeled	O
text	O
corpora	O
are	O
competitive	O
to	O
previously	O
published	O
results	O
that	O
use	O
human	O
supervision	O
.	O

(	O
2	O
)	O
Using	O
the	O
most	O
recent	O
deep	B-Method
architectures	E-Method
as	O
input	O
embeddings	O
,	O
we	O
significantly	O
improve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SoA	O
)	O
.	O

(	O
3	O
)	O
We	O
extensively	O
evaluate	O
several	O
unsupervised	B-Method
output	I-Method
embeddings	E-Method
for	O
fine	B-Task
-	I-Task
grained	I-Task
classification	E-Task
in	O
a	O
zero	B-Method
-	I-Method
shot	I-Method
setting	E-Method
on	O
three	O
challenging	O
datasets	O
.	O

(	O
4	O
)	O
By	O
combining	O
different	O
output	O
embeddings	O
we	O
obtain	O
best	O
results	O
,	O
surpassing	O
the	O
SoA	O
by	O
a	O
large	O
margin	O
.	O

(	O
5	O
)	O
We	O
propose	O
a	O
novel	O
weakly	B-Method
-	I-Method
supervised	I-Method
Word2Vec	I-Method
variant	E-Method
that	O
improves	O
the	O
accuracy	S-Metric
when	O
combined	O
with	O
other	O
output	O
embeddings	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
provides	O
a	O
review	O
of	O
the	O
relevant	O
literature	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
details	O
the	O
SJE	S-Method
method	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
explains	O
the	O
output	O
embeddings	O
that	O
we	O
analyze	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
our	O
experimental	O
evaluation	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
the	O
discussion	O
and	O
our	O
conclusions	O
.	O

section	O
:	O
Related	O
Work	O
Learning	S-Task
to	O
classify	S-Task
in	O
the	O
absence	O
of	O
labeled	O
data	O
(	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
)	O
is	O
a	O
challenging	O
problem	O
,	O
and	O
achieving	O
better	O
-	O
than	O
-	O
chance	O
performance	O
requires	O
structure	O
in	O
the	O
output	O
space	O
.	O

Attributes	O
provide	O
one	O
such	O
space	O
;	O
they	O
relate	O
different	O
classes	O
through	O
well	O
-	O
known	O
and	O
shared	O
characteristics	O
of	O
objects	O
.	O

Attributes	O
,	O
which	O
are	O
often	O
collected	O
manually	O
,	O
have	O
shown	O
promising	O
results	O
in	O
various	O
applications	O
,	O
i.e.	O
caption	B-Task
generation	E-Task
,	O
face	B-Task
recognition	E-Task
,	O
image	B-Task
retrieval	E-Task
,	O
action	B-Task
recognition	E-Task
and	O
image	B-Task
classification	E-Task
.	O

The	O
main	O
challenge	O
of	O
attribute	B-Method
-	I-Method
based	I-Method
zero	I-Method
-	I-Method
shot	I-Method
learning	E-Method
arises	O
on	O
more	O
challenging	O
fine	O
-	O
grained	O
data	O
collections	O
,	O
in	O
which	O
categories	O
may	O
visually	O
differ	O
only	O
subtly	O
.	O

Therefore	O
,	O
generic	O
attributes	O
fail	O
at	O
modeling	O
small	O
intra	O
-	O
class	O
variance	O
between	O
objects	O
.	O

Improved	O
performance	O
requires	O
a	O
large	O
number	O
of	O
specific	O
attributes	O
which	O
increases	O
the	O
cost	O
of	O
data	B-Task
gathering	E-Task
.	O

As	O
an	O
alternative	O
to	O
manual	B-Method
annotation	E-Method
,	O
side	O
information	O
can	O
be	O
collected	O
automatically	O
from	O
text	O
corpora	O
.	O

Bag	B-Task
-	I-Task
of	I-Task
-	I-Task
words	E-Task
is	O
an	O
example	O
where	O
class	O
embeddings	O
correspond	O
to	O
histograms	O
of	O
vocabulary	O
words	O
extracted	O
automatically	O
from	O
unlabeled	O
text	O
.	O

Another	O
example	O
is	O
using	O
taxonomical	O
order	O
of	O
classes	O
as	O
structured	O
output	O
embeddings	O
.	O

Such	O
a	O
taxonomy	O
can	O
be	O
built	O
automatically	O
from	O
a	O
pre	O
-	O
defined	O
ontology	S-Method
such	O
as	O
WordNet	O
.	O

In	O
this	O
case	O
,	O
the	O
distance	O
between	O
nodes	O
is	O
measured	O
using	O
semantic	B-Metric
similarity	I-Metric
metrics	E-Metric
.	O

Finally	O
,	O
distributed	B-Method
text	I-Method
representations	E-Method
learned	O
from	O
large	O
unsupervised	O
text	O
corpora	O
can	O
be	O
employed	O
as	O
structured	O
embeddings	O
.	O

We	O
compare	O
several	O
representatives	O
of	O
these	O
methods	O
(	O
and	O
their	O
combinations	O
)	O
in	O
our	O
evaluation	O
.	O

Embedding	O
labels	O
in	O
an	O
Euclidean	O
space	O
is	O
an	O
effective	O
tool	O
to	O
model	O
latent	O
relationships	O
between	O
classes	O
.	O

These	O
relationships	O
can	O
be	O
collected	O
separately	O
from	O
the	O
data	O
,	O
learned	O
from	O
the	O
data	O
or	O
derived	O
from	O
side	O
information	O
.	O

In	O
order	O
to	O
collect	O
relationships	O
independently	O
of	O
data	O
,	O
compressed	B-Method
sensing	E-Method
uses	O
random	B-Method
projections	E-Method
whereas	O
Error	B-Method
Correcting	I-Method
Output	I-Method
Codes	E-Method
builds	O
embeddings	O
inspired	O
from	O
information	B-Method
theory	E-Method
.	O

WSABIE	S-Method
uses	O
images	O
with	O
their	O
corresponding	O
labels	O
to	O
learn	O
an	O
embedding	O
of	O
the	O
labels	O
,	O
and	O
CCA	S-Method
maximizes	O
the	O
correlation	S-Metric
between	O
two	O
different	O
data	O
modalities	O
.	O

DeViSE	O
employs	O
a	O
ranking	B-Method
formulation	E-Method
for	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
using	O
images	O
and	O
distributed	B-Method
text	I-Method
representations	E-Method
.	O

The	O
ALE	B-Method
method	E-Method
employs	O
an	O
approximate	B-Method
ranking	I-Method
formulation	E-Method
for	O
the	O
same	O
using	O
images	O
and	O
attributes	O
.	O

ConSe	S-Method
uses	O
the	O
probabilities	O
of	O
a	O
softmax	B-Method
-	I-Method
output	I-Method
layer	E-Method
to	O
weigh	O
the	O
semantic	O
vectors	O
of	O
all	O
the	O
classes	O
.	O

In	O
this	O
work	O
,	O
we	O
use	O
the	O
multiclass	O
objective	O
to	O
learn	O
structured	O
output	O
embeddings	O
obtained	O
from	O
various	O
sources	O
.	O

Among	O
the	O
closest	O
related	O
work	O
,	O
ALE	S-Method
uses	O
Fisher	O
Vectors	O
(	O
FV	S-Method
)	O
as	O
input	O
and	O
binary	O
attributes	O
/	O
hierarchies	O
as	O
output	O
embeddings	O
.	O

Similarly	O
,	O
DeviSe	O
uses	O
CNN	O
features	O
as	O
input	O
and	O
Word2Vec	B-Method
representations	E-Method
as	O
output	O
embeddings	O
.	O

In	O
this	O
work	O
,	O
we	O
benefit	O
from	O
both	O
ideas	O
:	O
(	O
1	O
)	O
We	O
use	O
SoA	O
image	O
features	O
,	O
i.e.	O
FV	S-Method
and	O
CNN	S-Method
,	O
(	O
2	O
)	O
among	O
others	O
,	O
we	O
also	O
use	O
attributes	O
and	O
Word2Vec	O
as	O
output	O
embeddings	O
.	O

Our	O
work	O
differs	O
from	O
w.r.t	S-Method
.	O

two	O
aspects	O
:	O
(	O
1	O
)	O
We	O
propose	O
and	O
evaluate	O
several	O
output	B-Method
embedding	I-Method
methods	E-Method
specifically	O
built	O
for	O
fine	B-Task
-	I-Task
grained	I-Task
classification	E-Task
.	O

(	O
2	O
)	O
We	O
show	O
how	O
some	O
of	O
these	O
output	O
embeddings	O
complement	O
each	O
other	O
for	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
on	O
general	O
and	O
fine	O
-	O
grained	O
datasets	O
.	O

The	O
reader	O
should	O
be	O
aware	O
of	O
.	O

section	O
:	O
Structured	B-Method
Joint	I-Method
Embedding	E-Method
s	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
leverage	O
input	O
and	O
output	O
embeddings	O
in	O
a	O
joint	B-Method
framework	E-Method
by	O
learning	O
a	O
compatibility	O
between	O
these	O
embeddings	O
.	O

We	O
are	O
interested	O
in	O
the	O
problem	O
of	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
for	O
image	B-Task
classification	E-Task
where	O
training	O
and	O
test	O
images	O
belong	O
to	O
two	O
disjoint	O
sets	O
of	O
classes	O
.	O

Following	O
,	O
given	O
input	O
/	O
output	O
and	O
from	O
,	O
Structured	B-Method
Joint	I-Method
Embedding	E-Method
(	O
SJE	S-Method
)	O
learns	O
by	O
minimizing	O
the	O
empirical	B-Metric
risk	E-Metric
where	O
defines	O
the	O
cost	O
of	O
predicting	O
when	O
the	O
true	O
label	O
is	O
.	O

Here	O
,	O
we	O
use	O
the	O
loss	O
.	O

subsection	O
:	O
Model	O
We	O
define	O
a	O
compatibility	O
function	O
between	O
an	O
input	O
space	O
and	O
a	O
structured	O
output	O
space	O
.	O

Given	O
a	O
specific	O
input	O
embedding	O
,	O
we	O
derive	O
a	O
prediction	S-Task
by	O
maximizing	O
the	O
compatibility	O
over	O
SJE	S-Method
as	O
follows	O
:	O
The	O
parameter	O
vector	O
can	O
be	O
written	O
as	O
a	O
matrix	O
with	O
being	O
the	O
input	O
embedding	O
dimension	O
and	O
being	O
the	O
output	O
embedding	O
dimension	O
.	O

This	O
leads	O
to	O
the	O
bi	O
-	O
linear	O
form	O
of	O
the	O
compatibility	O
function	O
:	O
Here	O
,	O
the	O
input	O
embedding	O
is	O
denoted	O
by	O
and	O
the	O
output	O
embedding	O
by	O
.	O

The	O
matrix	O
is	O
learned	O
by	O
enforcing	O
the	O
correct	O
label	O
to	O
be	O
ranked	O
higher	O
than	O
any	O
of	O
the	O
other	O
labels	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
i.e.	O
multiclass	O
objective	O
.	O

This	O
formulation	O
is	O
closely	O
related	O
to	O
.	O

Within	O
the	O
label	B-Method
embedding	I-Method
framework	E-Method
,	O
ALE	S-Method
and	O
DeViSe	O
use	O
pairwise	B-Metric
ranking	I-Metric
objective	E-Metric
,	O
WSABIE	S-Method
learns	O
both	O
and	O
through	O
ranking	S-Task
,	O
whereas	O
we	O
use	O
multiclass	O
objective	O
.	O

Similarly	O
,	O
use	O
the	O
regression	O
objective	O
and	O
CCA	S-Method
maximizes	O
the	O
correlation	O
of	O
input	O
and	O
output	O
embeddings	O
.	O

subsection	O
:	O
Parameter	B-Method
Learning	E-Method
According	O
to	O
the	O
unregularized	B-Method
structured	I-Method
SVM	I-Method
formulation	E-Method
,	O
the	O
objective	O
is	O
:	O
where	O
the	O
misclassification	B-Metric
loss	E-Metric
takes	O
the	O
form	O
:	O
For	O
the	O
zero	B-Task
-	I-Task
shot	I-Task
learning	I-Task
scenario	E-Task
,	O
the	O
training	O
and	O
test	O
classes	O
are	O
disjoint	O
.	O

Therefore	O
,	O
we	O
fix	O
to	O
the	O
output	O
embeddings	O
of	O
training	O
classes	O
and	O
learn	O
.	O

For	O
prediction	S-Task
,	O
we	O
project	O
a	O
test	O
image	O
onto	O
the	O
and	O
search	O
for	O
the	O
nearest	O
output	O
embedding	O
vector	O
(	O
using	O
the	O
dot	O
product	O
similarity	O
)	O
that	O
corresponds	O
to	O
one	O
of	O
the	O
test	O
classes	O
.	O

We	O
use	O
Stochastic	B-Method
Gradient	I-Method
Descent	E-Method
(	O
SGD	B-Method
)	E-Method
for	O
optimization	S-Task
which	O
consists	O
in	O
sampling	O
at	O
each	O
step	O
and	O
searching	O
for	O
the	O
highest	O
ranked	O
class	O
.	O

If	O
,	O
we	O
update	O
as	O
follows	O
:	O
where	O
is	O
the	O
learning	O
step	O
-	O
size	O
used	O
at	O
iteration	O
.	O

We	O
use	O
a	O
constant	O
step	O
size	O
chosen	O
by	O
cross	B-Method
-	I-Method
validation	E-Method
and	O
we	O
perform	O
regularization	S-Method
through	O
early	B-Method
stopping	E-Method
.	O

subsection	O
:	O
Learning	O
Combined	B-Task
Embeddings	E-Task
For	O
some	O
classification	B-Task
tasks	E-Task
,	O
there	O
may	O
be	O
multiple	O
output	O
embeddings	O
available	O
,	O
each	O
capturing	O
a	O
different	O
aspect	O
of	O
the	O
structure	O
of	O
the	O
output	O
space	O
.	O

Each	O
may	O
also	O
have	O
a	O
different	O
signal	B-Metric
-	I-Metric
to	I-Metric
-	I-Metric
noise	I-Metric
ratio	E-Metric
.	O

Since	O
each	O
output	B-Method
embedding	E-Method
possibly	O
offers	O
non	O
-	O
redundant	O
information	O
about	O
the	O
output	O
space	O
,	O
as	O
also	O
shown	O
in	O
,	O
we	O
can	O
learn	O
a	O
better	O
joint	B-Method
embedding	E-Method
by	O
combining	O
them	O
together	O
.	O

We	O
model	O
the	O
resulting	O
compatibility	B-Metric
score	E-Metric
as	O
where	O
are	O
the	O
joint	O
embedding	O
weight	O
matrices	O
corresponding	O
to	O
the	O
output	O
embeddings	O
(	O
)	O
.	O

In	O
our	O
experiments	O
,	O
we	O
first	O
train	O
each	O
independently	O
,	O
then	O
perform	O
a	O
grid	B-Method
search	E-Method
over	O
on	O
a	O
validation	O
set	O
.	O

Interestingly	O
,	O
we	O
found	O
that	O
the	O
optimal	O
for	O
previously	O
-	O
seen	O
classes	O
is	O
often	O
different	O
from	O
the	O
one	O
for	O
unseen	O
classes	O
.	O

Therefore	O
,	O
it	O
is	O
critical	O
to	O
cross	O
-	O
validate	O
on	O
the	O
zero	B-Method
-	I-Method
shot	I-Method
setting	E-Method
.	O

Note	O
that	O
if	O
we	O
take	O
,	O
Equation	O
[	O
reference	O
]	O
is	O
equivalent	O
to	O
simply	O
concatenating	O
the	O
.	O

This	O
corresponds	O
to	O
stacking	O
the	O
into	O
a	O
single	O
matrix	O
and	O
computing	O
the	O
standard	O
compatibility	O
as	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

However	O
,	O
such	O
a	O
stacking	S-Method
learns	O
a	O
large	O
where	O
a	O
high	O
dimensional	O
biases	O
the	O
final	O
prediction	O
.	O

In	O
contrast	O
,	O
eliminates	O
the	O
bias	O
,	O
leading	O
to	O
better	O
predictions	O
.	O

Thus	O
,	O
can	O
be	O
thought	O
of	O
as	O
the	O
confidence	O
associated	O
with	O
whose	O
contribution	O
we	O
can	O
control	O
.	O

We	O
show	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
that	O
finding	O
an	O
appropriate	O
can	O
yield	O
improved	O
accuracy	S-Metric
compared	O
to	O
any	O
single	O
.	O

section	O
:	O
Output	B-Method
Embeddings	E-Method
In	O
this	O
section	O
,	O
we	O
describe	O
three	O
types	O
of	O
output	O
embeddings	O
:	O
human	O
-	O
annotated	O
attributes	O
,	O
unsupervised	O
word	O
embeddings	O
learned	O
from	O
large	O
text	O
corpora	O
,	O
and	O
hierarchical	B-Method
embeddings	E-Method
derived	O
from	O
WordNet	O
.	O

subsection	O
:	O
Embedding	S-Task
by	O
Human	B-Task
Annotation	E-Task
:	O
Attributes	O
Annotating	B-Task
images	E-Task
with	O
class	O
labels	O
is	O
a	O
laborious	O
process	O
when	O
the	O
objects	O
represent	O
fine	O
-	O
grained	O
concepts	O
that	O
are	O
not	O
common	O
in	O
our	O
daily	O
lives	O
.	O

Attributes	O
provide	O
a	O
means	O
to	O
describe	O
such	O
fine	O
-	O
grained	O
concepts	O
.	O

They	O
model	O
shared	O
characteristics	O
of	O
objects	O
such	O
as	O
color	O
and	O
texture	O
which	O
are	O
easily	O
annotated	O
by	O
humans	O
and	O
converted	O
to	O
machine	O
-	O
readable	O
vector	O
format	O
.	O

The	O
set	O
of	O
descriptive	O
attributes	O
may	O
be	O
determined	O
by	O
language	O
experts	O
or	O
by	O
fine	O
-	O
grained	O
object	O
experts	O
.	O

The	O
association	O
between	O
an	O
attribute	O
and	O
a	O
category	O
can	O
be	O
a	O
binary	O
value	O
depicting	O
the	O
presence	O
/	O
absence	O
of	O
an	O
attribute	O
(	O
)	O
or	O
a	O
continuous	O
value	O
that	O
defines	O
the	O
confidence	O
level	O
of	O
an	O
attribute	O
(	O
)	O
for	O
each	O
class	O
.	O

We	O
write	O
per	O
-	O
class	O
attributes	O
as	O
:	O
where	O
can	O
be	O
or	O
a	O
real	O
number	O
that	O
associates	O
a	O
class	O
with	O
an	O
attribute	O
,	O
denotes	O
the	O
associated	O
class	O
and	O
is	O
the	O
number	O
of	O
attributes	O
.	O

Potentially	O
,	O
encodes	O
more	O
information	O
than	O
.	O

For	O
instance	O
,	O
for	O
classes	O
rat	O
,	O
monkey	O
,	O
whale	O
and	O
the	O
attribute	O
big	O
,	O
implies	O
that	O
in	O
terms	O
of	O
size	O
rat	O
monkey	O
whale	O
,	O
whereas	O
can	O
be	O
interpreted	O
as	O
rat	O
monkey	O
whale	O
which	O
is	O
more	O
accurate	O
.	O

We	O
empirically	O
show	O
the	O
benefit	O
of	O
over	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

In	O
practice	O
,	O
our	O
output	B-Method
embeddings	E-Method
use	O
a	O
per	O
-	O
class	O
vector	O
form	O
,	O
but	O
they	O
can	O
vary	O
in	O
dimensionality	O
(	O
)	O
.	O

For	O
the	O
rest	O
of	O
the	O
section	O
we	O
denote	O
the	O
output	O
embeddings	O
as	O
for	O
brevity	O
.	O

subsection	O
:	O
Learning	B-Task
Label	I-Task
Embeddings	E-Task
from	O
Text	O
In	O
this	O
section	O
,	O
we	O
describe	O
unsupervised	B-Task
and	I-Task
weakly	I-Task
-	I-Task
supervised	I-Task
label	I-Task
embeddings	E-Task
mined	O
from	O
text	O
.	O

With	O
these	O
label	B-Method
embeddings	E-Method
,	O
we	O
can	O
(	O
1	O
)	O
avoid	O
dependence	O
on	O
costly	O
manual	O
annotation	O
of	O
attributes	O
and	O
(	O
2	O
)	O
combine	O
the	O
embeddings	O
with	O
attributes	O
,	O
where	O
available	O
,	O
to	O
achieve	O
better	O
performance	O
.	O

Word2Vec	S-Method
(	O
φW	O
)	O
.	O

In	O
Word2Vec	S-Method
,	O
a	O
two	B-Method
-	I-Method
layer	I-Method
neural	I-Method
network	E-Method
is	O
trained	O
to	O
predict	O
a	O
set	O
of	O
target	O
words	O
from	O
a	O
set	O
of	O
context	O
words	O
.	O

Words	O
in	O
the	O
vocabulary	O
are	O
assigned	O
with	O
one	B-Method
-	I-Method
shot	I-Method
encoding	E-Method
so	O
that	O
the	O
first	O
layer	O
acts	O
as	O
a	O
look	O
-	O
up	O
table	O
to	O
retrieve	O
the	O
embedding	O
for	O
any	O
word	O
in	O
the	O
vocabulary	O
.	O

The	O
second	O
layer	O
predicts	O
the	O
target	O
word	O
(	O
s	O
)	O
via	O
hierarchical	B-Method
soft	I-Method
-	I-Method
max	E-Method
.	O

Word2Vec	S-Method
has	O
two	O
main	O
formulations	O
for	O
the	O
target	B-Task
prediction	E-Task
:	O
skip	B-Method
-	I-Method
gram	I-Method
(	I-Method
SG	E-Method
)	O
and	O
continuous	B-Method
bag	I-Method
-	I-Method
of	I-Method
-	I-Method
words	E-Method
(	O
CBOW	S-Method
)	O
.	O

In	O
SG	S-Method
,	O
words	O
within	O
a	O
local	O
context	O
window	O
are	O
predicted	O
from	O
the	O
centering	O
word	O
.	O

In	O
CBOW	S-Method
,	O
the	O
center	O
word	O
of	O
a	O
context	O
window	O
is	O
predicted	O
from	O
the	O
surrounding	O
words	O
.	O

Embeddings	S-Method
are	O
obtained	O
by	O
back	O
-	O
propagating	O
the	O
prediction	O
error	O
gradient	O
over	O
a	O
training	O
set	O
of	O
context	O
windows	O
sampled	O
from	O
the	O
text	O
corpus	O
.	O

GloVe	O
(	O
φG	O
)	O
.	O

GloVe	S-Method
incorporates	O
co	O
-	O
occurrence	O
statistics	O
of	O
words	O
that	O
frequently	O
appear	O
together	O
within	O
the	O
document	O
.	O

Intuitively	O
,	O
the	O
co	O
-	O
occurrence	O
statistics	O
encode	O
meaning	O
since	O
semantically	O
similar	O
words	O
such	O
as	O
“	O
ice	O
”	O
and	O
“	O
water	O
”	O
occur	O
together	O
more	O
frequently	O
than	O
semantically	O
dissimilar	O
words	O
such	O
as	O
“	O
ice	O
”	O
and	O
“	O
fashion	O
.	O

”	O
The	O
training	O
objective	O
is	O
to	O
learn	O
word	O
vectors	O
such	O
that	O
their	O
dot	O
product	O
equals	O
the	O
co	O
-	O
occurrence	O
probability	O
of	O
these	O
two	O
words	O
.	O

This	O
approach	O
has	O
recently	O
been	O
shown	O
to	O
outperform	O
Word2Vec	S-Method
on	O
the	O
word	B-Task
analogy	I-Task
prediction	I-Task
task	E-Task
.	O

Weakly	B-Method
-	I-Method
supervised	I-Method
Word2Vec	I-Method
(	I-Method
φW⁢ws	E-Method
)	O
.	O

The	O
standard	O
Word2Vec	S-Method
scans	O
the	O
entire	O
document	O
using	O
each	O
word	O
within	O
a	O
sample	O
window	O
as	O
the	O
target	O
for	O
prediction	S-Task
.	O

However	O
,	O
if	O
we	O
know	O
the	O
global	O
context	O
,	O
i.e.	O
the	O
topic	O
of	O
the	O
document	O
,	O
we	O
can	O
use	O
that	O
topic	O
as	O
our	O
target	O
.	O

For	O
instance	O
,	O
in	O
Wikipedia	O
,	O
the	O
entire	O
article	O
is	O
related	O
to	O
the	O
same	O
topic	O
.	O

Therefore	O
,	O
we	O
can	O
sample	O
our	O
context	O
windows	O
from	O
any	O
location	O
within	O
the	O
article	O
rather	O
than	O
searching	O
for	O
context	O
windows	O
where	O
the	O
topic	O
explicitly	O
appears	O
in	O
the	O
text	O
.	O

We	O
consider	O
this	O
method	O
as	O
a	O
weak	B-Method
form	I-Method
of	I-Method
supervision	E-Method
.	O

We	O
achieve	O
the	O
best	O
results	O
in	O
our	O
experiments	O
using	O
our	O
novel	O
variant	O
of	O
the	O
CBOW	B-Method
formulation	E-Method
.	O

Here	O
,	O
we	O
pre	O
-	O
train	O
the	O
first	O
layer	O
weights	O
using	O
standard	O
Word2Vec	S-Method
on	O
Wikipedia	O
,	O
and	O
fine	O
-	O
tune	O
the	O
second	B-Method
layer	I-Method
weights	E-Method
using	O
a	O
negative	B-Method
-	I-Method
sampling	I-Method
objective	E-Method
only	O
on	O
the	O
fine	O
-	O
grained	O
text	O
corpus	O
.	O

These	O
weights	O
correspond	O
to	O
the	O
final	O
output	O
embedding	O
.	O

The	O
negative	B-Task
sampling	I-Task
objective	E-Task
is	O
formulated	O
as	O
follows	O
:	O
where	O
and	O
are	O
the	O
label	O
embeddings	O
we	O
seek	O
to	O
learn	O
,	O
and	O
is	O
the	O
average	O
of	O
word	O
embeddings	O
within	O
a	O
context	O
window	O
around	O
word	O
.	O

consists	O
of	O
context	O
and	O
matching	O
targets	O
,	O
and	O
consists	O
of	O
the	O
same	O
and	O
mismatching	O
.	O

To	O
find	O
the	O
(	O
which	O
are	O
the	O
columns	O
of	O
the	O
first	O
-	O
layer	O
network	O
weights	O
)	O
,	O
we	O
take	O
them	O
from	O
a	O
standard	O
unsupervised	B-Method
Word2Vec	I-Method
model	E-Method
trained	O
on	O
Wikipedia	O
.	O

During	O
SGD	S-Method
,	O
the	O
are	O
fixed	O
and	O
we	O
update	O
each	O
sampled	O
and	O
at	O
each	O
iteration	O
.	O

Intuitively	O
,	O
we	O
seek	O
to	O
maximize	O
the	O
similarity	O
between	O
context	O
and	O
target	O
vectors	O
for	O
matching	O
pairs	O
,	O
and	O
minimize	O
it	O
for	O
mismatching	O
pairs	O
.	O

Bag	B-Method
-	I-Method
of	I-Method
-	I-Method
Words	E-Method
(	O
φB	O
)	O
.	O

BoW	S-Method
builds	O
a	O
“	O
bag	O
”	O
of	O
word	O
frequencies	O
by	O
counting	O
the	O
occurrence	O
of	O
each	O
vocabulary	O
word	O
that	O
appears	O
within	O
a	O
document	O
.	O

It	O
does	O
not	O
preserve	O
the	O
order	O
in	O
which	O
words	O
appear	O
in	O
a	O
document	O
,	O
so	O
it	O
disregards	O
the	O
grammar	O
.	O

We	O
collect	O
Wikipedia	O
articles	O
that	O
correspond	O
to	O
each	O
object	O
class	O
and	O
build	O
a	O
vocabulary	O
of	O
most	O
frequently	O
occurring	O
words	O
.	O

We	O
then	O
build	O
histograms	O
of	O
these	O
words	O
to	O
vectorize	O
our	O
classes	O
.	O

subsection	O
:	O
Hierarchical	B-Method
Embeddings	E-Method
Semantic	O
similarity	B-Method
measures	E-Method
how	O
closely	O
related	O
two	O
word	O
senses	O
are	O
according	O
to	O
their	O
meaning	O
.	O

Such	O
a	O
similarity	O
can	O
be	O
estimated	O
by	O
measuring	O
the	O
distance	O
between	O
terms	O
in	O
an	O
ontology	O
.	O

WordNet	S-Method
,	O
a	O
large	O
-	O
scale	O
hierarchical	O
database	O
of	O
over	O
100	O
,	O
000	O
words	O
for	O
English	O
,	O
provides	O
us	O
a	O
means	O
of	O
building	O
our	O
class	O
hierarchy	O
.	O

To	O
measure	O
similarity	S-Metric
,	O
we	O
use	O
Jiang	O
-	O
Conrath	O
(	O
)	O
,	O
Lin	O
(	O
)	O
and	O
path	O
(	O
)	O
similarities	O
formulated	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
denote	O
our	O
whole	O
family	O
of	O
hierarchical	O
embeddings	O
as	O
.	O

For	O
a	O
more	O
detailed	O
survey	O
,	O
the	O
reader	O
may	O
refer	O
to	O
.	O

section	O
:	O
Experiments	O
While	O
our	O
main	O
contribution	O
is	O
a	O
detailed	O
analysis	O
of	O
output	B-Task
embeddings	E-Task
,	O
good	O
image	B-Method
representations	E-Method
are	O
crucial	O
to	O
obtain	O
good	O
classification	S-Task
performance	O
.	O

In	O
Sec	O
.	O

[	O
reference	O
]	O
we	O
detail	O
datasets	O
,	O
input	O
and	O
output	O
embeddings	O
used	O
in	O
our	O
experiments	O
and	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
we	O
present	O
our	O
results	O
.	O

subsection	O
:	O
Experimental	O
Setting	O
We	O
evaluate	O
SJE	S-Method
on	O
three	O
datasets	O
:	O
Caltech	B-Material
UCSD	I-Material
Birds	E-Material
(	O
CUB	S-Material
)	O
and	O
Stanford	O
Dogs	O
(	O
Dogs	O
)	O
are	O
fine	O
-	O
grained	O
,	O
and	O
Animals	O
With	O
Attributes	O
(	O
AWA	O
)	O
is	O
a	O
standard	O
attribute	O
dataset	O
for	O
zero	B-Task
-	I-Task
shot	I-Task
classification	E-Task
.	O

CUB	S-Material
contains	O
11	O
,	O
788	O
images	O
of	O
200	O
bird	O
species	O
,	O
Dogs	O
contains	O
19	O
,	O
501	O
images	O
of	O
113	O
dog	O
breeds	O
and	O
AWA	O
contains	O
30	O
,	O
475	O
images	O
of	O
50	O
different	O
animals	O
.	O

We	O
use	O
a	O
truly	O
zero	B-Method
-	I-Method
shot	I-Method
setting	E-Method
where	O
the	O
train	O
,	O
val	O
,	O
and	O
test	O
sets	O
belong	O
to	O
mutually	O
exclusive	O
classes	O
.	O

We	O
employ	O
train	O
and	O
val	O
,	O
i.e.	O
disjoint	O
subsets	O
of	O
training	O
set	O
,	O
for	O
cross	B-Metric
-	I-Metric
validation	E-Metric
.	O

We	O
report	O
average	O
per	O
-	O
class	O
top	O
-	O
1	O
accuracy	S-Metric
on	O
the	O
test	O
set	O
.	O

For	O
CUB	S-Material
,	O
we	O
use	O
the	O
same	O
zero	O
-	O
shot	O
split	O
as	O
with	O
150	O
classes	O
for	O
the	O
train	O
+	O
val	O
set	O
and	O
50	O
disjoint	O
classes	O
for	O
the	O
test	O
set	O
.	O

AWA	O
has	O
a	O
predefined	O
split	O
for	O
40	O
train	O
+	O
val	O
and	O
10	O
test	O
classes	O
.	O

For	O
Dogs	O
,	O
we	O
use	O
approximately	O
the	O
same	O
ratio	O
of	O
classes	O
for	O
train	O
+	O
val	O
/	O
test	O
as	O
CUB	S-Material
,	O
i.e.	O
85	O
classes	O
for	O
train	O
+	O
val	O
and	O
28	O
classes	O
for	O
test	O
.	O

This	O
is	O
the	O
first	O
attempt	O
to	O
perform	O
zero	B-Method
-	I-Method
shot	I-Method
learning	E-Method
on	O
the	O
Dogs	O
dataset	O
.	O

Input	O
Embeddings	O
.	O

We	O
use	O
Fisher	O
Vectors	O
(	O
FV	S-Method
)	O
and	O
Deep	B-Method
CNN	I-Method
Features	E-Method
(	O
CNN	S-Method
)	O
.	O

FV	S-Method
aggregates	O
per	O
image	O
statistics	O
computed	O
from	O
local	O
image	O
patches	O
into	O
a	O
fixed	B-Method
-	I-Method
length	I-Method
local	I-Method
image	I-Method
descriptor	E-Method
.	O

We	O
extract	O
128	O
-	O
dim	O
SIFT	O
from	O
regular	O
grids	O
at	O
multiple	O
scales	O
,	O
reduce	O
them	O
to	O
64	O
-	O
dim	O
using	O
PCA	S-Method
,	O
build	O
a	O
visual	O
vocabulary	O
with	O
256	O
Gaussians	S-Method
and	O
finally	O
reduce	O
the	O
FVs	S-Method
to	O
4	O
,	O
096	O
.	O

As	O
an	O
alternative	O
,	O
we	O
extract	O
features	O
from	O
a	O
deep	B-Method
convolutional	I-Method
network	E-Method
.	O

Features	O
that	O
are	O
typically	O
obtained	O
from	O
the	O
activations	O
of	O
the	O
fully	B-Method
connected	I-Method
layers	E-Method
have	O
been	O
shown	O
to	O
induce	O
semantic	O
similarities	O
.	O

We	O
resize	O
each	O
image	O
to	O
224	O
224	O
and	O
feed	O
into	O
the	O
network	O
which	O
was	O
pre	O
-	O
trained	O
following	O
the	O
model	B-Method
architecture	E-Method
of	O
either	O
AlexNet	S-Method
or	O
GoogLeNet	S-Method
.	O

For	O
AlexNet	O
(	O
denoted	O
as	O
CNN	S-Method
)	O
we	O
use	O
the	O
4	O
,	O
096	O
-	O
dim	O
top	O
-	O
layer	O
hidden	O
unit	O
activations	O
(	O
âfc7â	O
)	O
as	O
features	O
,	O
and	O
for	O
GoogLeNet	O
(	O
denoted	O
as	O
GOOG	O
)	O
we	O
use	O
the	O
1	B-Method
,	I-Method
024	I-Method
-	I-Method
dim	I-Method
top	I-Method
-	I-Method
layer	I-Method
pooling	I-Method
units	E-Method
.	O

For	O
both	O
networks	O
,	O
we	O
used	O
the	O
publicly	O
-	O
available	O
BVLC	B-Method
implementations	E-Method
.	O

We	O
do	O
not	O
perform	O
any	O
task	B-Method
-	I-Method
specific	I-Method
pre	I-Method
-	I-Method
processing	E-Method
,	O
such	O
as	O
cropping	O
foreground	O
objects	O
or	O
detecting	O
parts	O
.	O

Output	B-Method
Embeddings	E-Method
.	O

AWA	O
classes	O
have	O
85	O
binary	O
and	O
continuous	O
attributes	O
.	O

CUB	S-Material
classes	O
have	O
312	O
continuous	O
attributes	O
and	O
the	O
continuous	O
values	O
are	O
thresholded	O
around	O
the	O
mean	O
to	O
obtain	O
binary	O
attributes	O
.	O

The	O
Dogs	O
dataset	O
does	O
not	O
have	O
human	O
-	O
annotated	O
attributes	O
available	O
.	O

We	O
train	O
Word2Vec	B-Method
(	I-Method
)	E-Method
and	O
GloVe	B-Method
(	E-Method
)	O
on	O
the	O
English	O
-	O
language	O
Wikipedia	O
from	O
13.02.2014	O
.	O

We	O
first	O
pre	O
-	O
process	O
it	O
by	O
replacing	O
the	O
class	O
-	O
names	O
,	O
i.e.	O
black	O
-	O
footed	O
albatross	O
,	O
with	O
alternative	O
unique	O
names	O
,	O
i.e.	O
scientific	O
name	O
,	O
phoebastrianigripes	O
.	O

We	O
cross	O
-	O
validate	O
the	O
skip	O
-	O
window	O
size	O
and	O
embedding	O
dimensions	O
.	O

For	O
our	O
proposed	O
weakly	B-Method
-	I-Method
supervised	I-Method
Word2Vec	I-Method
(	I-Method
)	E-Method
,	O
we	O
use	O
the	O
same	O
embedding	O
dimensions	O
as	O
the	O
plain	O
Word2Vec	O
(	O
)	O
.	O

For	O
BoW	S-Method
,	O
we	O
download	O
the	O
Wikipedia	O
articles	O
that	O
correspond	O
to	O
each	O
class	O
and	O
build	O
the	O
vocabulary	O
by	O
omitting	O
least	O
-	O
and	O
most	O
-	O
frequently	O
occurring	O
words	O
.	O

We	O
cross	O
-	O
validate	O
the	O
vocabulary	O
size	O
.	O

is	O
a	O
histogram	O
of	O
the	O
vocabulary	O
words	O
as	O
they	O
appear	O
in	O
the	O
respective	O
document	O
.	O

For	O
hierarchical	B-Task
embeddings	E-Task
(	O
)	O
,	O
we	O
use	O
the	O
WordNet	O
hierarchy	O
spanning	O
our	O
classes	O
and	O
their	O
ancestors	O
up	O
to	O
the	O
root	O
of	O
the	O
tree	O
.	O

We	O
employ	O
the	O
widely	O
used	O
NLTK	B-Method
library	E-Method
for	O
building	O
the	O
hierarchy	O
and	O
measuring	O
the	O
similarity	O
between	O
nodes	O
.	O

Therefore	O
,	O
each	O
vector	O
is	O
populated	O
with	O
similarity	B-Method
measures	E-Method
of	O
the	O
class	O
to	O
all	O
other	O
classes	O
.	O

Combination	B-Method
of	I-Method
output	I-Method
embeddings	E-Method
.	O

We	O
explore	O
combinations	O
of	O
five	O
types	O
of	O
output	O
embeddings	O
:	O
supervised	O
attributes	O
,	O
unsupervised	O
Word2Vec	O
,	O
GloVe	S-Method
,	O
BoW	S-Method
and	O
WordNet	B-Method
-	I-Method
derived	I-Method
similarity	I-Method
embeddings	E-Method
.	O

We	O
either	O
concatenate	O
(	O
cnc	S-Method
)	O
or	O
combine	O
(	O
cmb	S-Method
)	O
different	O
embeddings	S-Method
.	O

In	O
cnc	S-Method
,	O
for	O
instance	O
in	O
AWA	O
,	O
85	O
-	O
dim	O
and	O
400	O
-	O
dim	O
would	O
be	O
merged	O
to	O
485	O
-	O
dim	O
output	O
embeddings	O
.	O

In	O
this	O
case	O
,	O
if	O
we	O
use	O
1	O
,	O
024	O
-	O
dim	O
GOOG	O
as	O
input	O
embeddings	O
,	O
we	O
learn	O
a	O
single	O
1	O
,	O
024	O
485	O
-	O
dim	O
.	O

In	O
cmb	S-Method
,	O
we	O
first	O
learn	O
1	O
,	O
024	O
85	O
-	O
dim	O
and	O
1	O
,	O
024	O
400	O
-	O
dim	O
and	O
then	O
cross	O
-	O
validate	O
the	O
coefficients	O
to	O
determine	O
the	O
amount	O
each	O
embedding	O
contributes	O
to	O
the	O
final	O
score	O
.	O

subsection	O
:	O
Experimental	O
Results	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
several	O
output	B-Method
embeddings	E-Method
on	O
the	O
CUB	S-Material
,	O
AWA	O
and	O
Dogs	O
datasets	O
.	O

Discrete	O
vs	O
Continuous	O
Attributes	O
.	O

Attribute	B-Method
representations	E-Method
are	O
defined	O
as	O
a	O
vector	O
per	O
class	O
,	O
or	O
a	O
column	O
of	O
the	O
(	O
class	O
attribute	O
)	O
matrix	O
.	O

These	O
vectors	O
(	O
85	O
-	O
dim	O
for	O
AWA	O
,	O
312	O
-	O
dim	O
for	O
CUB	S-Material
)	O
can	O
either	O
model	O
the	O
presence	O
/	O
absence	O
(	O
)	O
or	O
the	O
confidence	O
level	O
(	O
)	O
of	O
each	O
attribute	O
.	O

We	O
show	O
that	O
continuous	O
attributes	O
indeed	O
encode	O
more	O
semantics	O
than	O
binary	O
attributes	O
by	O
observing	O
a	O
substantial	O
improvement	O
with	O
over	O
with	O
deep	O
features	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Overall	O
,	O
CNN	S-Method
outperforms	O
FV	S-Method
,	O
while	O
GOOG	S-Method
gives	O
the	O
best	O
performing	O
results	O
;	O
therefore	O
in	O
the	O
following	O
,	O
we	O
comment	O
only	O
on	O
our	O
results	O
obtained	O
using	O
GOOG	S-Method
.	O

On	O
CUB	S-Material
,	O
i.e.	O
a	O
fine	O
-	O
grained	O
dataset	O
,	O
obtains	O
37.8	O
%	O
accuracy	S-Metric
,	O
which	O
is	O
significantly	O
above	O
the	O
SoA	O
(	O
26.9	O
%	O
)	O
.	O

Moreover	O
,	O
achieves	O
an	O
impressive	O
50.1	O
%	O
accuracy	S-Metric
;	O
outperforming	O
the	O
SoA	O
by	O
a	O
large	O
margin	O
.	O

We	O
observe	O
the	O
same	O
trend	O
for	O
AWA	O
,	O
which	O
is	O
a	O
benchmark	O
dataset	O
for	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
.	O

On	O
AWA	O
,	O
obtains	O
52.0	O
%	O
accuracy	S-Metric
and	O
improves	O
the	O
accuracy	S-Metric
substantially	O
to	O
66.7	O
%	O
,	O
significantly	O
outperforming	O
the	O
SoA	O
(	O
48.5	O
%	O
)	O
.	O

To	O
summarize	O
,	O
we	O
have	O
shown	O
that	O
improves	O
the	O
performance	O
of	O
using	O
deep	O
features	O
,	O
which	O
indicates	O
that	O
with	O
,	O
the	O
SJE	S-Method
method	O
learns	O
a	O
matrix	O
that	O
better	O
approximates	O
the	O
compatibility	O
of	O
images	O
and	O
side	O
information	O
than	O
.	O

Learned	O
Embeddings	O
from	O
Text	O
.	O

As	O
the	O
visual	O
similarity	O
between	O
objects	O
in	O
different	O
classes	O
increases	O
,	O
e.g.	O
in	O
fine	O
-	O
grained	O
datasets	O
,	O
the	O
cost	O
of	O
collecting	O
attributes	O
also	O
increases	O
.	O

Therefore	O
,	O
we	O
aim	O
to	O
extract	O
class	O
similarities	O
automatically	O
from	O
unlabeled	O
online	O
textual	O
resources	O
.	O

We	O
evaluate	O
three	O
methods	O
,	O
Word2Vec	B-Method
(	E-Method
)	O
,	O
GloVe	B-Method
(	E-Method
)	O
and	O
the	O
historically	O
most	O
commonly	O
-	O
used	O
method	O
BoW	S-Method
(	O
)	O
.	O

We	O
build	O
and	O
on	O
the	O
entire	O
English	O
Wikipedia	O
dump	O
.	O

Note	O
that	O
the	O
plain	O
Word2Vec	O
was	O
used	O
in	O
;	O
however	O
,	O
rather	O
than	O
using	O
Word2Vec	S-Method
in	O
an	O
averaging	B-Method
mechanism	E-Method
,	O
we	O
pre	O
-	O
process	O
the	O
Wikipedia	O
as	O
described	O
in	O
Sec	O
[	O
reference	O
]	O
so	O
that	O
our	O
class	O
names	O
are	O
directly	O
present	O
in	O
the	O
Word2Vec	O
vocabulary	O
.	O

This	O
leads	O
to	O
a	O
significant	O
accuracy	S-Metric
improvement	O
.	O

For	O
we	O
use	O
a	O
subset	O
of	O
Wikipedia	O
populated	O
only	O
with	O
articles	O
that	O
correspond	O
to	O
our	O
classes	O
.	O

On	O
CUB	S-Material
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	S-Metric
is	O
observed	O
with	O
(	O
28.4	O
%	O
)	O
improving	O
the	O
supervised	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
is	O
promising	O
and	O
impressive	O
since	O
does	O
not	O
use	O
any	O
human	O
supervision	O
.	O

On	O
AWA	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	S-Metric
is	O
observed	O
with	O
(	O
58.8	O
%	O
)	O
followed	O
by	O
(	O
51.2	O
%	O
)	O
,	O
improving	O
the	O
supervised	O
SoA	O
(	O
48.5	O
%	O
)	O
significantly	O
.	O

On	O
Dogs	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	S-Metric
is	O
obtained	O
with	O
(	O
33.0	O
%	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
using	O
(	O
19.6	O
%	O
)	O
and	O
(	O
17.8	O
%	O
)	O
leads	O
to	O
significantly	O
lower	O
accuracies	S-Metric
.	O

Unlike	O
birds	O
,	O
different	O
dog	O
breeds	O
belong	O
to	O
the	O
same	O
species	O
and	O
thus	O
they	O
share	O
a	O
common	O
scientific	O
name	O
.	O

As	O
a	O
result	O
,	O
our	O
method	O
of	O
cleanly	B-Task
pre	I-Task
-	I-Task
processing	I-Task
Wikipedia	E-Task
by	O
replacing	O
the	O
occurrences	O
of	O
bird	O
names	O
with	O
a	O
unique	O
scientific	O
name	O
was	O
not	O
possible	O
for	O
Dogs	O
.	O

This	O
may	O
lead	O
to	O
vectors	O
obtained	O
from	O
Wikipedia	O
for	O
dogs	O
that	O
are	O
vulnerable	O
to	O
variation	O
in	O
nomenclature	O
.	O

In	O
summary	O
,	O
our	O
results	O
indicate	O
no	O
winner	O
among	O
,	O
and	O
.	O

These	O
embeddings	O
may	O
be	O
task	O
specific	O
and	O
complement	O
each	O
other	O
.	O

We	O
investigate	O
the	O
complementarity	O
of	O
embeddings	O
in	O
the	O
following	O
sections	O
.	O

Effect	O
of	O
Text	O
Corpus	O
.	O

For	O
and	O
,	O
we	O
analyze	O
the	O
effects	O
of	O
three	O
text	O
corpora	O
(	O
B	O
,	O
W	O
,	O
B	O
+	O
W	O
)	O
with	O
varying	O
size	O
and	O
specificity	O
.	O

We	O
build	O
our	O
specialized	O
bird	O
corpus	O
(	O
B	O
)	O
by	O
collecting	O
bird	O
-	O
related	O
information	O
from	O
various	O
online	O
resources	O
,	O
i.e.	O
audubon.org	O
,	O
birdweb.org	O
,	O
allaboutbirds.org	O
and	O
BNA	O
.	O

In	O
combination	O
,	O
this	O
corresponds	O
to	O
50	O
MB	O
of	O
bird	O
-	O
related	O
text	O
.	O

We	O
use	O
the	O
English	O
-	O
language	O
Wikipedia	O
from	O
13.02.2014	O
as	O
our	O
large	O
and	O
general	O
corpus	O
(	O
W	O
)	O
which	O
is	O
40	O
GB	O
of	O
text	O
.	O

Finally	O
,	O
we	O
combine	O
B	O
and	O
W	O
to	O
build	O
a	O
large	O
-	O
scale	O
text	O
corpus	O
enriched	O
with	O
bird	O
specific	O
text	O
(	O
B	O
+	O
W	O
)	O
.	O

On	O
W	O
and	O
B	O
+	O
W	O
,	O
a	O
small	O
window	O
size	O
(	O
10	O
for	O
and	O
20	O
for	O
)	O
;	O
on	O
B	O
,	O
a	O
large	O
window	O
size	O
(	O
35	O
for	O
and	O
50	O
for	O
)	O
is	O
required	O
.	O

We	O
choose	O
parameters	O
after	O
a	O
grid	B-Method
search	E-Method
.	O

Increased	O
specificity	O
of	O
the	O
text	O
corpus	O
implies	O
semantic	O
consistency	O
throughout	O
the	O
text	O
.	O

Therefore	O
,	O
large	O
context	O
windows	O
capture	O
semantics	O
well	O
in	O
our	O
bird	O
specific	O
(	O
B	O
)	O
corpus	O
.	O

On	O
the	O
other	O
hand	O
,	O
W	O
is	O
organized	O
alphabetically	O
w.r.t	O
.	O

the	O
document	O
title	O
;	O
hence	O
,	O
a	O
large	O
sampling	O
window	O
can	O
include	O
content	O
from	O
another	O
article	O
that	O
is	O
adjacent	O
to	O
the	O
target	O
word	O
alphabetically	O
.	O

Here	O
,	O
small	O
windows	O
capture	O
semantics	O
better	O
by	O
looking	O
at	O
the	O
text	O
locally	O
.	O

We	O
report	O
our	O
results	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

Using	O
,	O
B	O
+	O
W	O
(	O
26.1	O
%	O
)	O
gives	O
the	O
highest	O
accuracy	S-Metric
,	O
followed	O
by	O
W	O
(	O
24.2	O
%	O
)	O
.	O

One	O
possible	O
reason	O
is	O
that	O
when	O
the	O
semantic	O
similarity	O
is	O
modeled	O
with	O
cooccurrence	O
statistics	O
,	O
output	O
embeddings	O
become	O
more	O
informative	O
with	O
the	O
increasing	O
corpus	O
size	O
,	O
since	O
the	O
probability	O
of	O
cooccurrence	O
of	O
similar	O
concepts	O
increases	O
.	O

Using	O
,	O
the	O
accuracy	S-Metric
obtained	O
with	O
B	O
(	O
22.5	O
%	O
)	O
is	O
already	O
higher	O
than	O
the	O
-	O
based	O
SoA	O
(	O
22.3	O
%	O
)	O
,	O
illustrating	O
the	O
benefit	O
of	O
using	O
fine	O
-	O
grained	O
text	O
for	O
fine	B-Task
-	I-Task
grained	I-Task
tasks	E-Task
.	O

Another	O
advantage	O
of	O
using	O
B	O
is	O
that	O
,	O
since	O
it	O
is	O
short	O
,	O
building	O
is	O
efficient	O
.	O

Moreover	O
,	O
building	O
with	O
B	O
does	O
not	O
require	O
any	O
annotation	O
effort	O
.	O

Building	O
using	O
W	S-Method
(	O
28.4	O
%	O
)	O
gives	O
the	O
highest	O
accuracy	S-Metric
,	O
followed	O
by	O
W	B-Method
+	I-Method
B	E-Method
(	O
27.5	O
%	O
)	O
which	O
improves	O
the	O
supervised	O
SoA	O
(	O
26.9	O
%	O
)	O
.	O

We	O
speculate	O
that	O
since	O
Word2Vec	S-Method
is	O
a	O
variant	O
of	O
the	O
Feedforward	B-Method
Neural	I-Method
Network	I-Method
Language	I-Method
Model	I-Method
(	I-Method
FNNLM	I-Method
)	E-Method
,	O
a	O
deep	B-Method
architecture	E-Method
,	O
it	O
may	O
learn	O
more	O
from	O
negative	O
data	O
than	O
positives	O
.	O

This	O
was	O
also	O
observed	O
for	O
CNN	O
features	O
learned	O
with	O
a	O
large	O
number	O
of	O
unlabeled	O
surrogate	O
classes	O
.	O

Additionally	O
,	O
we	O
propose	O
a	O
weakly	B-Method
-	I-Method
supervised	I-Method
alternative	E-Method
to	O
Word2Vec	B-Method
framework	E-Method
(	O
,	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
weak	O
-	O
supervision	O
comes	O
from	O
using	O
the	O
specialized	O
B	O
corpus	O
to	O
fine	O
-	O
tune	O
the	O
weights	O
of	O
the	O
network	O
and	O
model	O
the	O
bird	O
-	O
related	O
information	O
.	O

With	O
alone	O
,	O
we	O
obtain	O
21.0	O
%	O
accuracy	S-Metric
.	O

However	O
,	O
when	O
it	O
is	O
combined	O
with	O
(	O
28.4	O
%	O
)	O
,	O
the	O
accuracy	S-Metric
improves	O
to	O
29.7	O
%	O
.	O

Compared	O
to	O
the	O
results	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
29.7	O
%	O
is	O
the	O
highest	O
accuracy	S-Metric
obtained	O
using	O
unsupervised	B-Method
embeddings	E-Method
.	O

We	O
regard	O
these	O
results	O
as	O
a	O
very	O
encouraging	O
evidence	O
that	O
Word2Vec	B-Method
representations	E-Method
can	O
indeed	O
be	O
made	O
more	O
discriminative	O
for	O
fine	B-Task
-	I-Task
grained	I-Task
zero	I-Task
-	I-Task
shot	I-Task
learning	E-Task
by	O
integrating	O
a	O
fine	O
-	O
grained	O
text	O
corpus	O
directly	O
to	O
the	O
output	B-Task
embedding	I-Task
learning	I-Task
problem	E-Task
.	O

Hierarchical	B-Method
Embeddings	E-Method
.	O

The	O
hierarchical	O
organization	O
of	O
concepts	O
typically	O
embodies	O
a	O
fair	O
amount	O
of	O
hidden	O
information	O
about	O
language	O
,	O
such	O
as	O
synonymy	O
,	O
semantic	O
relations	O
,	O
etc	O
.	O

Therefore	O
,	O
semantic	O
relatedness	O
defined	O
by	O
hierarchical	O
distance	O
between	O
classes	O
can	O
form	O
numerical	O
vectors	O
to	O
be	O
used	O
as	O
output	B-Task
embeddings	E-Task
for	O
zero	B-Task
-	I-Task
shot	I-Task
learning	E-Task
.	O

We	O
build	O
ontological	O
relationships	O
between	O
our	O
classes	O
using	O
the	O
WordNet	B-Method
taxonomy	E-Method
.	O

Due	O
to	O
its	O
large	O
size	O
,	O
WordNet	O
encapsulates	O
all	O
of	O
our	O
AWA	O
and	O
Dog	O
classes	O
.	O

For	O
CUB	S-Material
,	O
the	O
high	O
level	O
bird	O
species	O
,	O
i.e.	O
albatross	O
,	O
appear	O
as	O
synsets	O
in	O
WordNet	O
,	O
but	O
the	O
specific	O
bird	O
names	O
,	O
i.e.	O
black	O
-	O
footed	O
albatross	O
,	O
are	O
not	O
always	O
present	O
.	O

Therefore	O
we	O
take	O
the	O
hierarchy	O
up	O
to	O
high	O
level	O
bird	O
species	O
as	O
-	O
is	O
and	O
we	O
assume	O
the	O
specific	O
bird	O
classes	O
are	O
all	O
at	O
the	O
bottom	O
of	O
the	O
hierarchy	O
located	O
with	O
the	O
same	O
distance	O
to	O
their	O
immediate	O
ancestors	O
.	O

The	O
WordNet	O
hierarchy	O
contains	O
319	O
nodes	O
for	O
CUB	S-Material
(	O
200	O
classes	O
)	O
,	O
104	O
nodes	O
for	O
AWA	O
(	O
50	O
classes	O
)	O
and	O
163	O
nodes	O
for	O
Dogs	O
(	O
113	O
classes	O
)	O
.	O

We	O
measure	O
the	O
distance	O
between	O
classes	O
using	O
the	O
similarity	B-Method
measures	E-Method
from	O
Sec	O
[	O
reference	O
]	O
.	O

While	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
different	O
hierarchical	O
similarity	B-Method
measures	E-Method
have	O
very	O
different	O
behaviors	O
on	O
each	O
dataset	O
.	O

The	O
best	O
performing	O
obtains	O
51.2	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
accuracy	S-Metric
on	O
AWA	O
which	O
reaches	O
our	O
(	O
52.0	O
%	O
)	O
and	O
improves	O
(	O
44.9	O
%	O
)	O
significantly	O
.	O

On	O
CUB	S-Material
,	O
obtains	O
20.6	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
remain	O
below	O
our	O
(	O
37.8	O
%	O
)	O
and	O
approaches	O
(	O
22.1	O
%	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
on	O
Dogs	O
obtains	O
24.3	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
is	O
significantly	O
higher	O
than	O
the	O
unsupervised	B-Method
text	I-Method
embeddings	E-Method
(	O
19.6	O
%	O
)	O
and	O
(	O
17.8	O
%	O
)	O
.	O

Combining	O
Output	B-Method
Embeddings	E-Method
.	O

In	O
this	O
section	O
,	O
we	O
combine	O
output	O
embeddings	O
obtained	O
through	O
human	O
annotation	O
(	O
)	O
,	O
from	O
text	O
(	O
)	O
and	O
from	O
hierarchies	O
(	O
)	O
.	O

As	O
a	O
reference	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
the	O
results	O
obtained	O
using	O
one	O
output	B-Method
embedding	E-Method
at	O
a	O
time	O
.	O

Our	O
intuition	O
is	O
that	O
because	O
the	O
different	O
embeddings	O
attempt	O
to	O
encapsulate	O
different	O
information	O
,	O
accuracy	S-Metric
should	O
improve	O
when	O
multiple	O
embeddings	O
are	O
combined	O
.	O

We	O
can	O
observe	O
this	O
complementarity	O
either	O
by	O
simple	O
concatenation	S-Method
(	O
cnc	S-Method
)	O
or	O
systematically	O
combining	O
(	O
cmb	B-Method
)	I-Method
output	I-Method
embeddings	E-Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
also	O
known	O
as	O
early	O
/	O
late	O
fusion	O
.	O

For	O
cnc	S-Method
,	O
we	O
perform	O
full	O
SJE	S-Method
training	O
and	O
cross	B-Method
-	I-Method
validation	E-Method
on	O
the	O
concatenated	O
output	O
embeddings	O
.	O

For	O
cmb	S-Method
,	O
we	O
learn	O
joint	O
embeddings	O
for	O
each	O
output	O
separately	O
(	O
which	O
is	O
trivially	O
parallelized	O
)	O
,	O
and	O
find	O
ensemble	O
weights	O
via	O
cross	B-Method
-	I-Method
validation	E-Method
.	O

In	O
contrast	O
to	O
the	O
cnc	B-Method
method	E-Method
,	O
no	O
additional	O
joint	B-Method
training	E-Method
is	O
used	O
,	O
although	O
it	O
can	O
improve	O
performance	O
in	O
practice	O
.	O

We	O
observe	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
in	O
almost	O
all	O
cases	O
cmb	S-Method
outperforms	O
cnc	S-Method
.	O

We	O
analyze	O
the	O
combination	O
of	O
unsupervised	B-Method
embeddings	E-Method
(	O
)	O
.	O

On	O
AWA	O
,	O
(	O
58.8	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
combined	O
with	O
(	O
51.2	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
achieve	O
60.1	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
improves	O
the	O
SoA	O
(	O
48.5	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
by	O
a	O
large	O
margin	O
.	O

On	O
CUB	S-Material
,	O
combining	O
(	O
24.2	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
(	O
20.6	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
get	O
29.9	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
improve	O
the	O
supervised	O
-	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Supporting	O
our	O
initial	O
claim	O
,	O
unsupervised	O
output	O
embeddings	O
obtained	O
from	O
different	O
sources	O
,	O
i.e.	O
text	O
vs	O
hierarchy	O
,	O
seem	O
to	O
be	O
complementary	O
to	O
each	O
other	O
.	O

In	O
some	O
cases	O
,	O
cmb	S-Method
performs	O
worse	O
than	O
cnc	S-Method
;	O
e.g.	O
28.2	O
%	O
versus	O
35.1	O
%	O
when	O
using	O
with	O
on	O
Dogs	O
.	O

In	O
most	O
other	O
cases	O
cmb	S-Method
performs	O
equivalent	O
or	O
better	O
.	O

Combining	O
supervised	B-Method
(	E-Method
)	O
and	O
unsupervised	B-Method
embeddings	E-Method
(	O
)	O
shows	O
a	O
similar	O
trend	O
.	O

On	O
AWA	O
,	O
combining	O
(	O
66.7	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
and	O
leads	O
to	O
73.9	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
significantly	O
exceeds	O
the	O
SoA	O
(	O
48.5	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

On	O
CUB	S-Material
,	O
combining	O
with	O
and	O
leads	O
to	O
51.7	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
improving	O
both	O
the	O
results	O
we	O
obtained	O
with	O
(	O
50.1	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
supervised	O
-	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
have	O
shown	O
with	O
these	O
experiments	O
that	O
output	O
embeddings	O
obtained	O
through	O
human	O
annotation	O
can	O
also	O
be	O
complemented	O
with	O
unsupervised	B-Method
output	I-Method
embeddings	E-Method
using	O
the	O
SJE	S-Method
framework	O
.	O

Qualitative	O
Results	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
top	O
-	O
5	O
highest	O
ranked	O
images	O
for	O
classes	O
chimpanzee	O
,	O
leopard	O
and	O
seal	O
that	O
are	O
selected	O
from	O
10	O
test	O
classes	O
of	O
AWA	O
.	O

We	O
use	O
GOOG	O
as	O
input	O
embeddings	O
and	O
as	O
output	O
embeddings	O
we	O
use	O
supervised	S-Method
,	O
the	O
best	O
performing	O
unsupervised	B-Method
embedding	E-Method
on	O
AWA	O
(	O
)	O
,	O
and	O
the	O
combination	O
of	O
the	O
two	O
(	O
)	O
.	O

For	O
the	O
class	O
chimpanzee	O
,	O
emphasizes	O
that	O
chimpanzees	O
live	O
on	O
trees	O
,	O
which	O
is	O
among	O
the	O
list	O
of	O
attributes	O
.	O

On	O
the	O
other	O
hand	O
,	O
models	O
the	O
social	O
nature	O
of	O
the	O
animal	O
,	O
ranking	O
a	O
group	O
of	O
chimpanzees	O
interacting	O
with	O
each	O
other	O
at	O
the	O
highest	O
.	O

Indeed	O
this	O
information	O
can	O
easily	O
be	O
retrieved	O
from	O
Wikipedia	O
.	O

synthesizes	O
both	O
aspects	O
.	O

Similarly	O
,	O
for	O
leopard	O
puts	O
an	O
emphasis	O
on	O
the	O
head	O
where	O
we	O
can	O
observe	O
several	O
of	O
the	O
attributes	O
,	O
i.e.	O
color	O
,	O
spotted	O
,	O
whereas	O
seems	O
to	O
place	O
the	O
animal	O
in	O
the	O
wild	O
.	O

combines	O
both	O
aspects	O
.	O

In	O
case	O
of	O
class	B-Task
seal	E-Task
,	O
retrieves	O
images	O
related	O
to	O
water	O
and	O
ranks	O
whales	O
and	O
seals	O
highest	O
,	O
whereas	O
adds	O
more	O
context	O
by	O
placing	O
seals	O
in	O
the	O
icy	O
natural	O
environment	O
and	O
within	O
groups	O
.	O

Finally	O
,	O
ranks	O
seal	O
-	O
shaped	O
animals	O
on	O
ice	O
,	O
close	O
to	O
water	O
and	O
within	O
groups	O
the	O
highest	O
.	O

We	O
find	O
these	O
qualitative	O
results	O
interesting	O
as	O
they	O
depict	O
how	O
(	O
1	O
)	O
unsupervised	O
embeddings	O
capture	O
nameable	O
semantics	O
about	O
objects	O
and	O
(	O
2	O
)	O
different	O
output	O
embeddings	O
are	O
semantically	O
complementary	O
for	O
zero	B-Method
-	I-Method
shot	I-Method
learning	E-Method
.	O

section	O
:	O
Conclusion	O
We	O
evaluated	O
the	O
Structured	B-Method
Joint	I-Method
Embedding	E-Method
(	O
SJE	S-Method
)	O
framework	O
on	O
supervised	O
attributes	O
and	O
unsupervised	O
output	O
embeddings	O
obtained	O
from	O
hierarchies	O
and	O
unlabeled	O
text	O
corpora	O
.	O

We	O
proposed	O
a	O
novel	O
weakly	B-Method
-	I-Method
supervised	I-Method
label	I-Method
embedding	I-Method
technique	E-Method
.	O

By	O
combining	O
multiple	O
output	O
embeddings	O
(	O
cmb	S-Method
)	O
,	O
we	O
established	O
a	O
new	O
SoA	O
on	O
AWA	O
(	O
73.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
CUB	S-Material
(	O
51.7	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Moreover	O
,	O
we	O
showed	O
that	O
unsupervised	B-Task
zero	I-Task
-	I-Task
shot	I-Task
learning	E-Task
with	O
SJE	S-Method
improves	O
the	O
SoA	O
,	O
to	O
60.1	O
%	O
on	O
AWA	O
and	O
29.9	O
%	O
on	O
CUB	S-Material
,	O
and	O
obtains	O
35.1	O
%	O
on	O
Dogs	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
emphasize	O
the	O
following	O
take	O
-	O
home	O
points	O
:	O
(	O
1	O
)	O
Unsupervised	B-Method
label	I-Method
embeddings	E-Method
learned	O
from	O
text	O
corpora	O
yield	O
compelling	O
zero	O
-	O
shot	O
results	O
,	O
outperforming	O
previous	O
supervised	O
SoA	O
on	O
AWA	O
and	O
CUB	S-Material
(	O
Tab	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

(	O
2	O
)	O
Integrating	O
specialized	O
text	O
corpora	O
helps	O
due	O
to	O
incorporating	O
more	O
fine	O
-	O
grained	O
information	O
to	O
output	O
embeddings	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

(	O
3	O
)	O
Combining	O
unsupervised	B-Method
output	I-Method
embeddings	E-Method
improve	O
the	O
zero	B-Metric
-	I-Metric
shot	I-Metric
performance	E-Metric
,	O
suggesting	O
that	O
they	O
provide	O
complementary	O
information	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

(	O
4	O
)	O
There	O
is	O
still	O
a	O
large	O
gap	O
between	O
the	O
performance	O
of	O
unsupervised	O
output	O
embeddings	O
and	O
human	O
-	O
annotated	O
attributes	O
on	O
AWA	O
and	O
CUB	S-Material
,	O
suggesting	O
that	O
better	O
methods	O
are	O
needed	O
for	O
learning	O
discriminative	B-Task
output	I-Task
embeddings	E-Task
from	O
text	O
.	O

(	O
5	O
)	O
Finally	O
,	O
supporting	O
,	O
encoding	O
continuous	O
nature	O
of	O
attributes	O
significantly	O
improve	O
upon	O
binary	O
attributes	O
for	O
zero	B-Task
-	I-Task
shot	I-Task
classification	E-Task
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

As	O
future	O
work	O
,	O
we	O
plan	O
to	O
investigate	O
other	O
methods	O
to	O
combine	O
multiple	O
output	O
embeddings	O
and	O
to	O
improve	O
the	O
discriminative	B-Metric
power	E-Metric
of	O
unsupervised	B-Method
and	I-Method
weakly	I-Method
-	I-Method
supervised	I-Method
label	I-Method
embeddings	E-Method
for	O
fine	B-Task
-	I-Task
grained	I-Task
classification	E-Task
.	O

subsection	O
:	O
Acknowledgments	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
ONR	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0762	O
,	O
NSF	O
CMMI	O
-	O
1266184	O
,	O
Google	O
Faculty	O
Research	O
Award	O
,	O
and	O
NSF	O
Graduate	O
Fellowship	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Partially	B-Method
Shuffling	E-Method
the	O
Training	O
Data	O
to	O
Improve	O
Language	B-Method
Models	E-Method
Although	O
SGD	S-Method
requires	O
shuffling	O
the	O
training	O
data	O
between	O
epochs	O
,	O
currently	O
none	O
of	O
the	O
word	B-Task
-	I-Task
level	I-Task
language	I-Task
modeling	E-Task
systems	O
do	O
this	O
.	O

Naively	O
shuffling	O
all	O
sentences	O
in	O
the	O
training	O
data	O
would	O
not	O
permit	O
the	O
model	O
to	O
learn	O
inter	O
-	O
sentence	O
dependencies	O
.	O

Here	O
we	O
present	O
a	O
method	O
that	O
partially	O
shuffles	O
the	O
training	O
data	O
between	O
epochs	O
.	O

This	O
method	O
makes	O
each	O
batch	O
random	O
,	O
while	O
keeping	O
most	O
sentence	O
ordering	O
intact	O
.	O

It	O
achieves	O
new	O
state	O
of	O
the	O
art	O
results	O
on	O
word	B-Task
-	I-Task
level	I-Task
language	I-Task
modeling	E-Task
on	O
both	O
the	O
Penn	B-Material
Treebank	E-Material
and	O
WikiText	B-Material
-	I-Material
2	E-Material
datasets	O
.	O

section	O
:	O
Background	O
A	O
language	B-Method
model	E-Method
is	O
trained	O
to	O
predict	O
word	O
given	O
all	O
previous	O
words	O
.	O

A	O
recurrent	B-Method
language	I-Method
model	E-Method
receives	O
at	O
timestep	O
the	O
th	O
word	O
and	O
the	O
previous	O
hidden	O
state	O
and	O
outputs	O
a	O
prediction	O
of	O
the	O
next	O
word	O
and	O
the	O
next	O
hidden	O
state	O
.	O

The	O
training	O
data	O
for	O
word	B-Task
-	I-Task
level	I-Task
language	I-Task
modeling	E-Task
consists	O
of	O
a	O
series	O
of	O
concatenated	O
documents	O
.	O

The	O
sentences	O
from	O
these	O
documents	O
are	O
unshuffled	O
.	O

This	O
lets	O
the	O
model	O
learn	O
long	O
term	O
,	O
multi	O
-	O
sentence	O
dependencies	O
between	O
words	O
.	O

The	O
concatenation	B-Method
operation	E-Method
results	O
in	O
a	O
single	O
long	O
sequence	O
of	O
words	O
.	O

The	O
naive	O
way	O
to	O
train	O
a	O
language	B-Method
model	E-Method
would	O
be	O
to	O
,	O
at	O
every	O
epoch	O
,	O
use	O
the	O
entire	O
training	O
sequence	O
as	O
the	O
input	O
,	O
and	O
use	O
the	O
same	O
sequence	O
shifted	O
one	O
word	O
to	O
the	O
left	O
as	O
target	O
output	O
.	O

Since	O
the	O
training	O
sequence	O
is	O
too	O
long	O
,	O
this	O
solution	O
is	O
infeasible	O
.	O

To	O
solve	O
this	O
,	O
we	O
set	O
a	O
back	O
propagation	O
through	O
-	O
time	O
length	O
(	O
)	O
,	O
and	O
split	O
the	O
training	O
sequence	O
into	O
sub	O
-	O
sequences	O
of	O
length	O
.	O

In	O
this	O
case	O
,	O
in	O
each	O
epoch	O
the	O
model	O
is	O
first	O
trained	O
on	O
the	O
first	O
sub	O
-	O
sequence	O
,	O
and	O
then	O
on	O
the	O
second	O
one	O
,	O
and	O
so	O
on	O
.	O

While	O
gradients	O
are	O
not	O
passed	O
between	O
different	O
sub	O
-	O
sequences	O
,	O
the	O
last	O
hidden	O
state	O
from	O
sub	O
-	O
sequence	O
becomes	O
the	O
initial	O
hidden	O
state	O
while	O
training	O
the	O
model	O
with	O
sub	O
-	O
sequence	O
.	O

For	O
example	O
,	O
if	O
the	O
training	O
sequence	O
of	O
words	O
is	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
for	O
,	O
the	O
resulting	O
four	O
sub	O
-	O
sequences	O
are	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
]	O
[	O
J	O
K	O
L	O
]	O
Note	O
that	O
we	O
only	O
present	O
the	O
input	O
sub	O
-	O
sequences	O
,	O
as	O
the	O
target	O
output	O
sub	O
-	O
sequences	O
are	O
simply	O
the	O
input	O
sub	O
-	O
sequences	O
shifted	O
one	O
word	O
to	O
the	O
left	O
.	O

This	O
method	O
works	O
,	O
but	O
it	O
does	O
not	O
utilize	O
current	O
GPUs	S-Method
to	O
their	O
full	O
potential	O
.	O

In	O
order	O
to	O
speed	O
up	O
training	S-Task
,	O
we	O
batch	O
our	O
training	O
data	O
.	O

We	O
set	O
a	O
batch	O
size	O
,	O
and	O
at	O
every	O
training	O
step	O
we	O
train	O
the	O
model	O
on	O
sub	O
-	O
sequences	O
in	O
parallel	O
.	O

To	O
do	O
this	O
,	O
we	O
first	O
split	O
the	O
training	O
sequence	O
into	O
parts	O
.	O

Continuing	O
the	O
example	O
from	O
above	O
,	O
for	O
,	O
this	O
results	O
in	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
Then	O
,	O
as	O
before	O
,	O
we	O
split	O
each	O
part	O
into	O
sub	O
-	O
sequences	O
of	O
length	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
]	O
[	O
J	O
K	O
L	O
]	O
Then	O
,	O
during	O
the	O
first	O
training	O
step	O
in	O
each	O
epoch	O
we	O
train	O
on	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
G	O
H	O
I	O
]	O
and	O
during	O
the	O
second	O
training	O
step	O
in	O
each	O
epoch	O
we	O
train	O
on	O
:	O
[	O
D	O
E	O
F	O
]	O
[	O
J	O
K	O
L	O
]	O
Note	O
that	O
at	O
every	O
step	O
,	O
all	O
sub	O
-	O
sequences	O
in	O
the	O
batch	O
are	O
processed	O
in	O
parallel	O
.	O

Before	O
we	O
introduced	O
batching	O
,	O
in	O
each	O
epoch	O
the	O
output	O
for	O
each	O
word	O
in	O
the	O
training	O
sequence	O
was	O
dependant	O
on	O
all	O
previous	O
words	O
.	O

With	O
batching	S-Method
,	O
the	O
output	O
of	O
the	O
model	O
for	O
each	O
word	O
is	O
only	O
dependant	O
on	O
the	O
previous	O
words	O
in	O
that	O
batch	O
element	O
(	O
or	O
equivalently	O
,	O
row	O
in	O
our	O
example	O
)	O
,	O
and	O
the	O
other	O
words	O
are	O
ignored	O
.	O

In	O
our	O
example	O
,	O
the	O
hidden	O
state	O
that	O
is	O
given	O
when	O
inputting	O
G	O
is	O
the	O
default	O
initial	O
hidden	O
state	O
,	O
and	O
not	O
the	O
one	O
that	O
resulted	O
after	O
the	O
input	O
of	O
F	O
.	O

This	O
is	O
not	O
optimal	O
,	O
but	O
since	O
batching	S-Method
reduces	O
the	O
training	B-Metric
time	E-Metric
by	O
a	O
significant	O
amount	O
,	O
all	O
current	O
models	O
use	O
this	O
method	O
.	O

section	O
:	O
The	O
Partial	B-Method
Shuffle	I-Method
Method	E-Method
While	O
SGD	S-Method
calls	O
for	O
random	O
batches	O
in	O
each	O
epoch	O
,	O
in	O
existing	O
language	B-Method
models	E-Method
,	O
the	O
data	O
is	O
not	O
shuffled	O
between	O
epochs	O
during	O
training	O
.	O

This	O
means	O
that	O
batch	O
in	O
every	O
epoch	O
is	O
made	O
up	O
of	O
the	O
same	O
sub	O
-	O
sequences	O
.	O

The	O
straightforward	O
way	O
to	O
shuffle	O
the	O
data	O
would	O
be	O
to	O
shuffle	O
all	O
sentences	O
in	O
the	O
training	O
sequence	O
between	O
each	O
epoch	O
.	O

This	O
hurts	O
the	O
language	B-Method
model	E-Method
’s	O
performance	O
,	O
since	O
it	O
does	O
not	O
learn	O
inter	O
-	O
sentence	O
dependencies	O
.	O

Here	O
we	O
present	O
the	O
Partial	B-Method
Shuffle	I-Method
method	E-Method
,	O
which	O
improves	O
the	O
performance	O
of	O
the	O
model	O
.	O

Like	O
before	O
,	O
we	O
first	O
separate	O
the	O
sequence	O
of	O
words	O
into	O
rows	O
.	O

Using	O
the	O
example	O
sequence	O
from	O
above	O
,	O
this	O
would	O
result	O
in	O
(	O
for	O
)	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
Then	O
,	O
for	O
each	O
row	O
,	O
we	O
pick	O
a	O
random	O
index	O
between	O
zero	O
and	O
the	O
length	O
of	O
the	O
row	O
and	O
we	O
take	O
the	O
words	O
that	O
are	O
located	O
before	O
this	O
index	O
and	O
move	O
them	O
to	O
the	O
end	O
of	O
the	O
row	O
.	O

So	O
in	O
our	O
example	O
,	O
if	O
the	O
random	O
index	O
for	O
row	O
one	O
was	O
and	O
for	O
row	O
two	O
was	O
this	O
would	O
result	O
in	O
(	O
red	O
marks	O
the	O
words	O
which	O
were	O
moved	O
)	O
:	O
[	O
C	O
D	O
E	O
F	O
A	O
B	O
]	O
[	O
L	O
H	O
J	O
K	O
]	O
Finally	O
,	O
as	O
before	O
,	O
each	O
row	O
(	O
or	O
equivalently	O
,	O
batch	O
element	O
)	O
is	O
divided	O
into	O
back	B-Method
-	I-Method
propagation	E-Method
through	O
time	O
segments	O
.	O

For	O
,	O
this	O
will	O
result	O
in	O
:	O
[	O
C	O
D	O
E	O
]	O
[	O
F	O
A	O
B	O
]	O
[	O
L	O
G	O
H	O
]	O
[	O
I	O
J	O
K	O
]	O
This	O
method	O
randomizes	O
the	O
batches	O
while	O
still	O
keeping	O
most	O
of	O
the	O
word	O
ordering	O
intact	O
.	O

section	O
:	O
Results	O
We	O
evaluate	O
our	O
method	O
on	O
the	O
current	O
state	O
of	O
the	O
art	B-Method
model	E-Method
,	O
DOC	S-Method
,	O
and	O
the	O
previous	O
state	O
of	O
the	O
art	O
model	O
,	O
MoS	S-Method
,	O
on	O
the	O
Penn	B-Material
Treebank	E-Material
and	O
WikiText	B-Material
-	I-Material
2	E-Material
language	O
modeling	O
datasets	O
.	O

For	O
each	O
model	O
,	O
the	O
hyper	O
-	O
parameters	O
(	O
including	O
and	O
)	O
are	O
not	O
modified	O
from	O
their	O
original	O
values	O
.	O

In	O
addition	O
,	O
we	O
present	O
results	O
for	O
finetuned	B-Method
models	E-Method
,	O
with	O
and	O
without	O
the	O
Partial	B-Method
Shuffle	E-Method
.	O

Our	O
shuffling	B-Method
method	E-Method
improves	O
the	O
performance	O
of	O
all	O
models	O
,	O
and	O
achieves	O
new	O
state	O
of	O
the	O
art	O
results	O
on	O
both	O
datasets	O
.	O

Our	O
method	O
does	O
not	O
require	O
any	O
additional	O
parameters	O
or	O
hyper	O
-	O
parameters	O
,	O
and	O
runs	O
in	O
less	O
than	O
th	O
of	O
a	O
second	O
per	O
epoch	O
on	O
the	O
Penn	B-Material
Treebank	E-Material
dataset	O
.	O

section	O
:	O
Acknowledgements	O
This	O
note	O
benefited	O
from	O
feedback	O
from	O
Judit	O
Acs	O
,	O
Shimi	O
Salant	O
and	O
Noah	O
A.	O
Smith	O
,	O
which	O
is	O
acknowledged	O
with	O
gratitude	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Regularizing	B-Method
Face	I-Method
Verification	I-Method
Nets	E-Method
For	O
Pain	B-Task
Intensity	I-Task
Regression	E-Task
Limited	O
labeled	O
data	O
are	O
available	O
for	O
the	O
research	O
of	O
estimating	B-Task
facial	I-Task
expression	I-Task
intensities	E-Task
.	O

For	O
instance	O
,	O
the	O
ability	O
to	O
train	O
deep	B-Method
networks	E-Method
for	O
automated	B-Task
pain	I-Task
assessment	E-Task
is	O
limited	O
by	O
small	O
datasets	O
with	O
labels	O
of	O
patient	O
-	O
reported	O
pain	O
intensities	O
.	O

Fortunately	O
,	O
fine	B-Task
-	I-Task
tuning	E-Task
from	O
a	O
data	O
-	O
extensive	O
pre	O
-	O
trained	O
domain	O
,	O
such	O
as	O
face	B-Task
verification	E-Task
,	O
can	O
alleviate	O
this	O
problem	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
network	O
that	O
fine	O
-	O
tunes	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
face	B-Method
verification	I-Method
network	E-Method
using	O
a	O
regularized	B-Method
regression	I-Method
loss	E-Method
and	O
additional	O
data	O
with	O
expression	O
labels	O
.	O

In	O
this	O
way	O
,	O
the	O
expression	O
intensity	O
regression	S-Method
task	O
can	O
benefit	O
from	O
the	O
rich	O
feature	B-Method
representations	E-Method
trained	O
on	O
a	O
huge	O
amount	O
of	O
data	O
for	O
face	B-Task
verification	E-Task
.	O

The	O
proposed	O
regularized	B-Method
deep	I-Method
regressor	E-Method
is	O
applied	O
to	O
estimate	O
the	O
pain	O
expression	O
intensity	O
and	O
verified	O
on	O
the	O
widely	O
-	O
used	O
UNBC	B-Material
-	I-Material
McMaster	I-Material
Shoulder	I-Material
-	I-Material
Pain	I-Material
dataset	E-Material
,	O
achieving	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

A	O
weighted	B-Metric
evaluation	I-Metric
metric	E-Metric
is	O
also	O
proposed	O
to	O
address	O
the	O
imbalance	B-Task
issue	E-Task
of	O
different	O
pain	O
intensities	O
.	O

Feng	O
Wang1	O
,	O
4	O
,	O
Xiang	O
Xiang1⁣	O
*	O
*	O
Corresponding	O
author	O
(	O
e	O
-	O
mail	O
:	O
xxiang@cs.jhu.edu	O
)	O
,	O
Chang	O
Liu1	O
,	O
5	O
,	O
Trac	O
D.	O
Tran2	O
,	O
Austin	O
Reiter1	O
,	O
Gregory	O
D.	O
Hager1	O
,	O
Harry	O
Quon3	O
,	O
Jian	O
Cheng4	O
,	O
Alan	O
L.	O
Yuille1	O
Dept	O
.	O

of	O
{	O
Computer	B-Task
Science	E-Task
,	O
Electrical	B-Task
&	I-Task
Computer	I-Task
Engineering	E-Task
,	O
Radiation	B-Task
Oncology	E-Task
}	O
Johns	O
Hopkins	O
University	O
,	O
3400	O
N.	O
Charles	O
St	O
,	O
Baltimore	O
,	O
MD	O
21218	O
,	O
USA	O
Dept	O
.	O

of	O
EE	O
,	O
UESTC	O
,	O
2006	O
Xiyuan	O
Ave	O
,	O
Chengdu	O
,	O
Sichuan	O
611731	O
,	O
China	O
Dept	O
.	O

of	O
CS	O
,	O
Tsinghua	O
University	O
,	O
Beijing	O
100084	O
,	O
China	B-Task
fine	I-Task
-	I-Task
tuning	E-Task
,	O
CNN	S-Method
,	O
regularizer	S-Method
,	O
regression	S-Method
section	O
:	O
Introduction	O
Obtaining	O
accurate	O
patient	O
-	O
reported	O
pain	O
intensities	O
is	O
important	O
to	O
effectively	O
manage	O
pain	O
and	O
thus	O
reduce	O
anesthetic	O
doses	O
and	O
in	B-Task
-	I-Task
hospital	I-Task
deterioration	E-Task
.	O

Traditionally	O
,	O
caregivers	O
work	O
with	O
patients	O
to	O
manually	O
input	O
the	O
patients	O
’	O
pain	O
intensity	O
,	O
ranging	O
among	O
a	O
few	O
levels	O
such	O
as	O
mild	O
,	O
moderate	O
,	O
severe	O
and	O
excruciating	O
.	O

Recently	O
,	O
a	O
couple	O
of	O
concepts	O
have	O
been	O
proposed	O
such	O
as	O
active	B-Task
,	I-Task
automated	I-Task
and	I-Task
objective	I-Task
pain	I-Task
monitoring	E-Task
over	O
the	O
patient	O
’s	O
stay	O
in	O
hospital	O
,	O
with	O
roughly	O
the	O
same	O
motivation	O
:	O
first	O
to	O
simplify	O
the	O
pain	B-Task
reporting	I-Task
process	E-Task
and	O
reduce	O
the	O
strain	O
on	O
manual	O
efforts	O
;	O
second	O
to	O
standardize	O
the	O
feedback	B-Method
mechanism	E-Method
by	O
ensuring	O
a	O
single	O
metric	O
that	O
performs	O
all	O
assessments	O
and	O
thus	O
reduces	O
bias	O
.	O

There	O
indeed	O
exist	O
efforts	O
to	O
assess	O
pain	O
from	O
the	O
observational	O
or	O
behavioral	O
effect	O
caused	O
by	O
pain	O
such	O
as	O
physiological	O
data	O
.	O

©	O
Medasense	S-Method
has	O
developed	O
medical	B-Method
devices	E-Method
for	O
objective	B-Task
pain	I-Task
monitoring	E-Task
.	O

Their	O
basic	O
premise	O
is	O
that	O
pain	O
may	O
cause	O
vital	O
signs	O
such	O
as	O
blood	O
pressure	O
,	O
pulse	O
rate	O
,	O
respiration	B-Metric
rate	E-Metric
,	O
SpO2	O
from	O
EMG	O
,	O
ECG	O
or	O
EEG	O
,	O
alone	O
or	O
in	O
combination	O
to	O
change	O
and	O
often	O
to	O
increase	O
.	O

Nevertheless	O
,	O
it	O
takes	O
much	O
more	O
effort	O
to	O
obtain	O
physiological	O
data	O
than	O
videos	O
of	O
faces	O
.	O

Computer	B-Task
vision	E-Task
and	O
supervised	B-Task
learning	E-Task
have	O
come	O
a	O
long	O
way	O
in	O
recent	O
years	O
,	O
redefining	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
using	O
deep	B-Method
Convolutional	I-Method
Neural	I-Method
Networks	E-Method
(	O
CNNs	S-Method
)	O
.	O

However	O
,	O
the	O
ability	O
to	O
train	O
deep	B-Method
CNNs	E-Method
for	O
pain	B-Task
assessment	E-Task
is	O
limited	O
by	O
small	O
datasets	O
with	O
labels	O
of	O
patient	O
-	O
reported	O
pain	O
intensities	O
,	O
i.e.	O
,	O
annotated	O
datasets	O
such	O
as	O
EmoPain	S-Material
,	O
Shoulder	B-Material
-	I-Material
Pain	E-Material
,	O
BioVid	B-Material
Heat	I-Material
Pain	E-Material
.	O

Particularly	O
,	O
Shoulder	B-Material
-	I-Material
Pain	E-Material
is	O
the	O
only	O
dataset	O
available	O
for	O
visual	B-Task
analysis	E-Task
with	O
per	O
-	O
frame	O
labels	O
.	O

It	O
contains	O
only	O
200	O
videos	O
of	O
25	O
patients	O
who	O
suffer	O
from	O
shoulder	O
pain	O
and	O
repeatedly	O
raise	O
their	O
arms	O
and	O
then	O
put	O
them	O
down	O
(	O
onset	O
-	O
apex	O
-	O
offset	O
)	O
.	O

While	O
all	O
frames	O
are	O
labeled	O
with	O
discrete	O
-	O
valued	O
pain	O
intensities	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
dataset	O
is	O
small	O
,	O
the	O
label	O
is	O
discrete	O
and	O
most	O
labels	O
are	O
0	O
.	O

Although	O
the	O
small	O
dataset	O
problem	O
prevents	O
us	O
from	O
directly	O
training	O
a	O
deep	B-Method
pain	I-Method
intensity	I-Method
regressor	E-Method
,	O
we	O
show	O
that	O
fine	B-Task
-	I-Task
tuning	E-Task
from	O
a	O
data	O
-	O
extensive	O
pre	O
-	O
trained	O
domain	O
such	O
as	O
face	B-Task
verification	E-Task
can	O
alleviate	O
this	O
problem	O
.	O

Our	O
solutions	O
are	O
fine	O
-	O
tuning	O
a	O
well	O
-	O
trained	O
face	B-Method
verification	I-Method
net	E-Method
on	O
additional	O
data	O
with	O
a	O
regularized	B-Method
regression	I-Method
loss	E-Method
and	O
a	O
hidden	B-Method
full	I-Method
-	I-Method
connected	I-Method
layer	I-Method
regularized	E-Method
using	O
dropout	S-Method
,	O
regularizing	S-Method
the	O
regression	S-Method
loss	O
using	O
a	O
center	B-Method
loss	E-Method
,	O
and	O
re	O
-	O
sampling	O
the	O
training	O
data	O
by	O
the	O
population	O
proportion	O
of	O
a	O
certain	O
pain	O
intensity	O
w.r.t	O
.	O

the	O
total	O
population	O
.	O

While	O
our	O
work	O
is	O
not	O
the	O
first	O
attempt	O
of	O
this	O
regularization	B-Method
idea	E-Method
,	O
to	O
our	O
knowledge	O
we	O
are	O
the	O
first	O
to	O
apply	O
it	O
to	O
the	O
pain	B-Task
expression	I-Task
intensity	I-Task
estimation	E-Task
.	O

Correspondingly	O
,	O
we	O
propose	O
three	O
solutions	O
to	O
address	O
the	O
four	O
issues	O
mentioned	O
above	O
.	O

In	O
summary	O
,	O
the	O
contributions	O
of	O
this	O
work	O
include	O
addressing	O
limited	O
data	O
with	O
expression	O
intensity	O
labels	O
by	O
relating	O
two	O
mappings	O
from	O
the	O
same	O
input	O
face	O
space	O
to	O
different	O
output	O
label	O
space	O
where	O
the	O
identity	O
labels	O
are	O
rich	O
,	O
pushing	O
the	O
pain	B-Metric
assessment	E-Metric
performance	O
by	O
a	O
large	O
margin	O
,	O
proposing	O
to	O
add	O
center	B-Method
loss	I-Method
regularizer	E-Method
to	O
make	O
the	O
regressed	O
values	O
closer	O
to	O
discrete	O
values	O
,	O
and	O
proposing	O
a	O
more	O
sensible	O
evaluation	B-Metric
metric	E-Metric
to	O
address	O
the	O
imbalance	B-Task
issue	E-Task
caused	O
by	O
a	O
natural	O
phenomena	O
where	O
most	O
of	O
the	O
time	O
a	O
patient	O
does	O
not	O
express	O
pain	O
.	O

section	O
:	O
Related	O
Works	O
Two	O
pieces	O
of	O
recent	O
work	O
make	O
progress	O
in	O
estimating	B-Task
pain	I-Task
intensity	I-Task
visually	E-Task
using	O
the	O
Shoulder	B-Material
-	I-Material
Pain	I-Material
dataset	E-Material
only	O
:	O
Ordinal	B-Method
Support	I-Method
Vector	I-Method
Regression	E-Method
(	O
OSVR	S-Method
)	O
and	O
Recurrent	B-Method
Convolutional	I-Method
Regression	E-Method
(	O
RCR	S-Method
)	O
.	O

Notably	O
,	O
RCR	S-Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
yet	O
achieving	O
sub	O
-	O
optimal	O
performance	O
.	O

Please	O
see	O
reference	O
therein	O
for	O
other	O
existing	O
works	O
.	O

For	O
facial	B-Task
expression	I-Task
recognition	E-Task
in	O
general	O
,	O
there	O
is	O
a	O
trade	O
-	O
off	O
between	O
method	B-Metric
simplicity	E-Metric
and	O
performance	O
,	O
i.e.	O
,	O
image	B-Method
-	I-Method
based	I-Method
vs.	I-Method
video	I-Method
-	I-Method
based	I-Method
methods	E-Method
.	O

As	O
videos	O
are	O
sequential	O
signals	O
,	O
appearance	B-Method
-	I-Method
based	I-Method
methods	E-Method
including	O
ours	O
can	O
not	O
model	O
the	O
dynamics	O
given	O
by	O
a	O
temporal	B-Method
model	E-Method
or	O
spatio	B-Method
-	I-Method
temporal	I-Method
models	E-Method
.	O

As	O
regards	O
regularizing	S-Method
deep	O
networks	O
,	O
there	O
exists	O
recent	O
work	O
that	O
regularize	O
deep	B-Method
face	I-Method
recognition	I-Method
nets	E-Method
for	O
expression	B-Task
classification	E-Task
-	O
FaceNet2ExpNet	S-Task
.	O

During	O
pre	B-Task
-	I-Task
training	E-Task
,	O
they	O
train	O
convolutional	B-Method
layers	E-Method
of	O
the	O
expression	B-Method
net	E-Method
,	O
regularized	O
by	O
the	O
deep	B-Method
face	I-Method
recognition	I-Method
net	E-Method
.	O

In	O
the	O
refining	B-Task
stage	E-Task
,	O
they	O
append	O
fully	B-Method
-	I-Method
connected	E-Method
(	O
FC	S-Method
)	O
layers	O
to	O
the	O
pre	O
-	O
trained	O
convolutional	B-Method
layers	E-Method
and	O
train	O
the	O
whole	O
network	O
jointly	O
.	O

section	O
:	O
Regularized	O
deep	O
regressor	S-Method
Our	O
network	O
is	O
based	O
on	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
face	B-Method
verification	I-Method
network	E-Method
trained	O
using	O
the	O
CASIA	B-Material
-	I-Material
WebFace	I-Material
dataset	E-Material
contaning	O
million	O
face	O
images	O
with	O
identity	O
labels	O
.	O

As	O
a	O
classification	B-Method
network	E-Method
,	O
it	O
employs	O
the	O
Softmax	B-Method
loss	I-Method
regularized	E-Method
with	O
its	O
proposed	O
center	B-Method
loss	E-Method
.	O

But	O
it	O
is	O
difficult	O
to	O
directly	O
fine	O
-	O
tune	O
the	O
network	O
for	O
pain	B-Task
intensity	I-Task
classification	E-Task
due	O
to	O
limited	O
face	O
images	O
with	O
pain	O
labels	O
.	O

However	O
,	O
it	O
is	O
feasible	O
to	O
fit	O
the	O
data	O
points	O
as	O
a	O
regression	S-Method
problem	O
.	O

Our	O
fine	B-Method
-	I-Method
tuning	I-Method
network	E-Method
employs	O
a	O
regression	B-Method
loss	I-Method
regularized	E-Method
with	O
the	O
center	O
loss	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

First	O
,	O
we	O
modify	O
the	O
face	B-Task
verification	I-Task
net	E-Task
’s	O
softmax	B-Method
loss	E-Method
to	O
be	O
a	O
Mean	B-Metric
Square	I-Metric
Error	E-Metric
(	O
MSE	S-Metric
)	O
loss	O
for	O
regression	S-Method
.	O

The	O
last	O
layer	O
of	O
such	O
a	O
network	O
is	O
a	O
distance	B-Method
layer	E-Method
,	O
which	O
easily	O
causes	O
gradient	O
exploding	O
due	O
to	O
large	O
magnitudes	O
of	O
the	O
gradients	O
at	O
initial	O
iterations	O
.	O

Thus	O
,	O
we	O
replace	O
the	O
MSE	S-Metric
loss	O
using	O
a	O
smooth	B-Method
loss	E-Method
with	O
a	O
Huber	O
loss	O
flavor	O
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

Secondly	O
,	O
as	O
labels	O
are	O
discrete	O
,	O
it	O
is	O
sensible	O
to	O
regularize	O
the	O
loss	O
to	O
make	O
the	O
regressed	O
values	O
to	O
be	O
more	O
âdiscreteâ.	O
We	O
introduce	O
the	O
center	O
loss	O
as	O
a	O
regularizer	S-Method
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

Thirdly	O
,	O
we	O
propose	O
two	O
weighted	B-Metric
evaluation	I-Metric
metrics	E-Metric
in	O
Sec	O
.	O

[	O
reference	O
]	O
to	O
address	O
label	B-Task
imbalance	E-Task
which	O
may	O
induce	O
trivial	O
method	O
.	O

In	O
the	O
following	O
,	O
we	O
elaborate	O
on	O
the	O
three	O
solutions	O
.	O

subsection	O
:	O
Regression	B-Method
Loss	E-Method
Similar	O
to	O
conventional	O
regression	S-Method
models	O
,	O
a	O
regression	S-Method
net	O
minimizes	O
the	O
Mean	B-Metric
Square	I-Metric
Error	E-Metric
(	O
MSE	S-Metric
)	O
loss	O
defined	O
as	O
where	O
is	O
the	O
output	O
vector	O
of	O
the	O
hidden	O
FC	S-Method
layer	O
,	O
is	O
a	O
vector	O
of	O
real	O
-	O
valued	O
weights	O
,	O
is	O
the	O
ground	O
-	O
truth	O
label	O
,	O
and	O
is	O
a	O
sigmoid	B-Method
activation	I-Method
function	E-Method
.	O

We	O
use	O
to	O
truncate	O
the	O
output	O
of	O
the	O
second	O
FC	S-Method
layer	O
to	O
be	O
in	O
the	O
range	O
of	O
pain	O
intensity	O
.	O

Here	O
we	O
omitted	O
the	O
bias	O
term	O
for	O
elegance	O
.	O

The	O
gradient	B-Task
exploding	I-Task
problem	E-Task
often	O
happens	O
due	O
to	O
the	O
relatively	O
large	O
gradient	O
magnitude	O
during	O
initial	O
iterations	O
.	O

This	O
phenomenon	O
is	O
also	O
described	O
in	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
follow	O
to	O
apply	O
the	O
smooth	B-Method
loss	E-Method
which	O
makes	O
the	O
gradient	O
smaller	O
than	O
the	O
case	O
with	O
the	O
MSE	S-Metric
loss	O
when	O
the	O
absolute	B-Metric
error	E-Metric
is	O
large	O
.	O

Different	O
from	O
,	O
our	O
regressor	S-Method
outputs	O
a	O
scalar	O
instead	O
of	O
a	O
vector	O
.	O

It	O
is	O
a	O
compromise	O
between	O
squared	B-Metric
and	I-Metric
absolute	I-Metric
error	I-Metric
losses	E-Metric
:	O
where	O
is	O
the	O
turning	O
point	O
of	O
the	O
absolute	B-Metric
error	E-Metric
between	O
the	O
squared	O
error	O
function	O
and	O
the	O
absolute	O
error	O
function	O
.	O

It	O
has	O
a	O
flavor	O
with	O
the	O
Huber	O
loss	O
.	O

When	O
,	O
it	O
works	O
similar	O
with	O
MSE	S-Metric
loss	O
since	O
the	O
error	S-Metric
is	O
usually	O
below	O
1	O
.	O

When	O
,	O
it	O
is	O
equivalent	O
with	O
the	O
Mean	B-Metric
Abosolute	I-Metric
Error	E-Metric
(	O
MAE	S-Metric
)	O
loss	O
.	O

subsection	O
:	O
Regularization	S-Task
Using	O
Center	B-Method
Loss	E-Method
Since	O
the	O
pain	O
intensity	O
is	O
labeled	O
as	O
discrete	O
values	O
in	O
the	O
Shoulder	B-Material
-	I-Material
Pain	I-Material
dataset	E-Material
,	O
it	O
is	O
natural	O
to	O
regularize	O
the	O
network	O
to	O
make	O
the	O
regressed	O
values	O
to	O
be	O
‘	O
discrete	O
’	O
-	O
during	O
training	O
,	O
to	O
make	O
same	O
-	O
intensity	O
’s	O
regressed	O
values	O
as	O
compact	O
as	O
possible	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
use	O
the	O
center	B-Method
loss	E-Method
which	O
minimizes	O
the	O
within	O
-	O
class	O
distance	O
and	O
thus	O
is	O
defined	O
as	O
where	O
represents	O
the	O
center	O
for	O
class	O
and	O
is	O
essentially	O
the	O
mean	O
of	O
features	O
per	O
class	O
.	O

denotes	O
the	O
norm	O
and	O
is	O
typically	O
or	O
.	O

We	O
observe	O
from	O
expriments	O
that	O
the	O
center	O
loss	O
shrinks	O
the	O
distances	O
of	O
features	O
that	O
have	O
the	O
same	O
label	O
,	O
which	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

To	O
relate	O
it	O
with	O
the	O
literature	O
,	O
it	O
is	O
a	O
similar	O
idea	O
to	O
the	O
Linear	B-Method
Discriminant	I-Method
Analysis	E-Method
yet	O
without	O
minimizing	O
between	O
-	O
class	O
distances	O
.	O

It	O
also	O
has	O
a	O
flavor	O
of	O
the	O
k	B-Method
-	I-Method
means	I-Method
clustering	E-Method
yet	O
in	O
a	O
supervised	O
way	O
.	O

Now	O
,	O
the	O
center	O
loss	O
is	O
added	O
to	O
the	O
regression	S-Method
loss	O
after	O
the	O
hidden	O
FC	S-Method
layer	O
to	O
induce	O
the	O
loss	O
where	O
is	O
a	O
coefficient	O
.	O

Thus	O
,	O
the	O
supervision	O
of	O
the	O
regularizer	S-Method
is	O
applied	O
to	O
the	O
features	O
.	O

Different	O
from	O
,	O
we	O
jointly	O
learn	O
the	O
centers	O
and	O
minimize	O
within	O
-	O
class	O
distances	O
by	O
gradient	B-Method
descent	E-Method
,	O
while	O
’s	O
centers	O
are	O
learned	O
by	O
moving	B-Method
average	E-Method
.	O

subsection	O
:	O
Weighted	B-Metric
Evaluation	I-Metric
Metrics	E-Metric
Labels	O
in	O
the	O
Shoulder	B-Material
-	I-Material
Pain	I-Material
dataset	E-Material
are	O
highly	O
imbalanced	O
,	O
as	O
91.35	O
%	O
of	O
the	O
frames	O
are	O
labeled	O
as	O
pain	O
intensity	O
0	O
.	O

Thus	O
,	O
it	O
is	O
relatively	O
safe	O
to	O
predict	O
the	O
pain	O
intensity	O
to	O
be	O
zero	O
.	O

To	O
fairly	O
evaluate	O
the	O
performance	O
,	O
we	O
propose	O
the	O
weighted	B-Metric
version	I-Metric
of	I-Metric
evaluation	I-Metric
metrics	E-Metric
,	O
i.e.	O
,	O
weighted	B-Metric
MAE	E-Metric
(	O
wMAE	S-Metric
)	O
and	O
weighted	O
MSE	S-Metric
(	O
wMSE	S-Method
)	O
to	O
address	O
the	O
dataset	B-Task
imbalance	I-Task
issue	E-Task
.	O

For	O
example	O
,	O
the	O
wMAE	S-Metric
is	O
simply	O
the	O
mean	O
of	O
MAE	S-Metric
on	O
each	O
pain	O
intensity	O
.	O

In	O
this	O
way	O
,	O
the	O
MAE	S-Metric
is	O
weighted	O
by	O
the	O
population	O
of	O
each	O
pain	O
intensity	O
.	O

We	O
apply	O
two	O
techniques	O
to	O
sample	O
the	O
training	O
data	O
to	O
make	O
our	O
training	O
set	O
more	O
consistent	O
with	O
the	O
new	O
metrics	O
.	O

First	O
,	O
we	O
eliminate	O
the	O
redundant	O
frames	O
on	O
the	O
sequences	O
following	O
.	O

If	O
the	O
intensity	O
remains	O
the	O
same	O
for	O
more	O
than	O
5	O
consecutive	O
frames	O
,	O
we	O
choose	O
the	O
first	O
one	O
as	O
the	O
representative	O
frame	O
.	O

Second	O
,	O
during	O
training	O
,	O
we	O
uniformly	O
sample	O
images	O
from	O
the	O
6	O
classes	O
to	O
feed	O
into	O
the	O
network	O
.	O

In	O
this	O
way	O
,	O
what	O
the	O
neural	B-Method
network	E-Method
‘	O
see	O
’	O
is	O
a	O
totally	O
balanced	O
dataset	O
.	O

section	O
:	O
Experiments	O
In	O
this	O
section	O
,	O
we	O
present	O
implementations	O
and	O
experiments	O
.	O

The	O
project	O
page	O
has	O
been	O
set	O
up	O
with	O
programs	O
and	O
data	O
.	O

subsection	O
:	O
Dataset	O
and	O
Training	O
Details	O
We	O
test	O
our	O
network	O
on	O
the	O
Shoulder	B-Material
-	I-Material
Pain	I-Material
dataset	E-Material
that	O
contains	O
200	O
videos	O
of	O
25	O
subjects	O
and	O
is	O
widely	O
used	O
for	O
benchmarking	O
the	O
pain	B-Task
intensity	I-Task
estimation	E-Task
.	O

The	O
dataset	O
comes	O
with	O
four	O
types	O
of	O
labels	O
.	O

The	O
three	O
annotated	O
online	O
during	O
the	O
video	O
collection	O
are	O
the	O
sensory	O
scale	O
,	O
affective	O
scale	O
and	O
visual	O
analog	O
scale	O
ranging	O
from	O
(	O
i.e.	O
,	O
no	O
pain	O
)	O
to	O
(	O
i.e.	O
,	O
severe	O
pain	O
)	O
.	O

In	O
addition	O
,	O
observers	O
rated	O
pain	O
intensity	O
(	O
OPI	O
)	O
offline	O
from	O
recorded	O
videos	O
ranging	O
from	O
(	O
no	O
pain	O
)	O
to	O
(	O
severe	O
pain	O
)	O
.	O

In	O
the	O
same	O
way	O
as	O
previous	O
works	O
,	O
we	O
take	O
the	O
same	O
online	O
label	O
and	O
quantify	O
the	O
original	O
pain	O
intensity	O
in	O
the	O
range	O
of	O
to	O
be	O
in	O
range	O
.	O

The	O
face	B-Method
verification	I-Method
network	E-Method
is	O
trained	O
on	O
CASIA	B-Material
-	I-Material
WebFace	I-Material
dataset	E-Material
,	O
which	O
contains	O
494	O
,	O
414	O
training	O
images	O
from	O
10	O
,	O
575	O
identities	O
.	O

To	O
be	O
consistent	O
with	O
face	B-Task
verification	E-Task
,	O
we	O
perform	O
the	O
same	O
pre	B-Method
-	I-Method
processing	E-Method
on	O
the	O
images	B-Material
of	I-Material
Shoulder	I-Material
-	I-Material
Pain	I-Material
dataset	E-Material
.	O

To	O
be	O
specific	O
,	O
we	O
leverage	O
MTCNN	B-Method
model	E-Method
to	O
detect	O
faces	O
and	O
facial	O
landmarks	O
.	O

Then	O
the	O
faces	O
are	O
aligned	O
according	O
to	O
the	O
detected	O
landmarks	O
.	O

The	O
learning	B-Metric
rate	E-Metric
is	O
set	O
to	O
to	O
avoid	O
huge	O
modification	O
on	O
the	O
convolution	O
layers	O
.	O

The	O
network	O
is	O
trained	O
over	O
5	O
,	O
000	O
iterations	O
,	O
which	O
is	O
reasonable	O
for	O
the	O
networks	O
to	O
converge	O
observed	O
in	O
a	O
few	O
cross	B-Metric
validation	I-Metric
folds	E-Metric
.	O

We	O
set	O
the	O
weight	O
of	O
the	O
regression	S-Method
loss	O
to	O
be	O
1	O
and	O
the	O
weights	O
of	O
softmax	O
loss	O
and	O
center	O
loss	O
to	O
be	O
1	O
and	O
0.01	O
respectively	O
.	O

subsection	O
:	O
Evaluation	S-Metric
Using	O
Unweighted	B-Metric
Metrics	E-Metric
Cross	B-Method
validation	E-Method
is	O
a	O
conventional	O
way	O
to	O
address	O
over	B-Task
-	I-Task
fitting	I-Task
small	I-Task
dataset	E-Task
.	O

In	O
our	O
case	O
,	O
we	O
run	O
25	B-Method
-	I-Method
fold	I-Method
cross	I-Method
validation	E-Method
25	O
times	O
on	O
the	O
Shoulder	B-Material
-	I-Material
Pain	I-Material
dataset	E-Material
which	O
contains	O
25	O
subjects	O
.	O

This	O
setting	O
is	O
exactly	O
the	O
leave	O
-	O
one	O
-	O
subject	O
-	O
out	O
setting	O
in	O
OSVR	S-Method
except	O
that	O
OSVR	O
’s	O
experiments	O
exclude	O
one	O
subject	O
whose	O
expressions	O
do	O
not	O
have	O
noticeable	O
pain	O
(	O
namely	O
24	O
-	O
fold	O
)	O
.	O

Each	O
time	O
,	O
the	O
videos	O
of	O
one	O
subject	O
are	O
reserved	O
for	O
testing	O
.	O

All	O
the	O
other	O
videos	O
are	O
used	O
to	O
train	O
the	O
deep	O
regression	S-Method
network	O
.	O

The	O
performance	O
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
concluded	O
that	O
our	O
algorithm	O
performs	O
best	O
or	O
equally	O
best	O
on	O
various	O
evaluation	B-Metric
metrics	E-Metric
,	O
especially	O
the	O
combination	O
of	O
smooth	B-Metric
loss	E-Metric
and	O
center	B-Metric
loss	E-Metric
.	O

Note	O
that	O
OSVR	S-Method
uses	O
hand	O
-	O
crafted	O
features	O
concatenated	O
from	O
landmark	O
points	O
,	O
Gabor	O
wavelet	O
coefficients	O
and	O
LBP	S-Method
+	O
PCA	S-Method
.	O

subsection	O
:	O
Evaluation	S-Metric
Using	O
Weighted	B-Metric
Metrics	E-Metric
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
provide	O
the	O
performance	O
of	O
predicting	O
all	O
zeros	O
as	O
a	O
baseline	O
.	O

Interestingly	O
,	O
on	O
the	O
metrics	O
MAE	S-Metric
and	O
MSE	S-Metric
,	O
zero	B-Method
prediction	E-Method
performs	O
much	O
better	O
than	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
algorithms	O
.	O

Now	O
,	O
using	O
the	O
new	O
proposed	O
metrics	O
,	O
the	O
performance	O
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
performance	O
of	O
previous	O
work	O
OSVR	S-Method
is	O
no	O
longer	O
below	O
that	O
of	O
predicting	O
all	O
zeros	O
.	O

We	O
can	O
also	O
see	O
from	O
Table	O
[	O
reference	O
]	O
that	O
the	O
uniform	B-Method
class	I-Method
sampling	I-Method
strategy	E-Method
does	O
help	O
a	O
lot	O
on	O
the	O
new	O
evaluation	B-Metric
metrics	E-Metric
.	O

Moreover	O
,	O
we	O
have	O
provided	O
the	O
evaluation	O
program	O
in	O
our	O
project	O
page	O
and	O
encourage	O
future	O
works	O
to	O
report	O
their	O
performance	O
with	O
the	O
new	O
evaluation	B-Metric
metrics	E-Metric
.	O

section	O
:	O
Summary	O
Given	O
the	O
restriction	O
of	O
labeled	O
data	O
which	O
prevents	O
us	O
from	O
directly	O
training	O
a	O
deep	B-Method
pain	I-Method
intensity	I-Method
regressor	E-Method
,	O
fine	B-Task
-	I-Task
tuning	E-Task
from	O
a	O
data	O
-	O
extensive	O
pre	O
-	O
trained	O
domain	O
such	O
as	O
face	B-Task
verification	E-Task
can	O
alleviate	O
the	O
problem	O
.	O

In	O
this	O
paper	O
,	O
we	O
regularize	O
a	O
face	B-Method
verification	I-Method
network	E-Method
for	O
pain	B-Task
intensity	I-Task
regression	E-Task
.	O

In	O
particular	O
,	O
we	O
introduce	O
the	O
Smooth	B-Method
Loss	E-Method
to	O
(	O
continuous	O
-	O
valued	O
)	O
pain	B-Task
intensity	I-Task
regression	E-Task
as	O
well	O
as	O
introduce	O
the	O
center	B-Method
loss	E-Method
as	O
a	O
regularizer	S-Method
to	O
induce	O
concentration	O
on	O
discrete	O
values	O
.	O

The	O
fine	B-Method
-	I-Method
tuned	I-Method
regularizered	I-Method
network	E-Method
with	O
a	O
regression	S-Method
layer	O
is	O
tested	O
on	O
the	O
UNBC	B-Material
-	I-Material
McMaster	I-Material
Shoulder	I-Material
-	I-Material
Pain	I-Material
dataset	E-Material
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
pain	B-Task
intensity	I-Task
estimation	E-Task
.	O

The	O
main	O
problem	O
that	O
motivates	O
this	O
work	O
is	O
that	O
expertise	O
is	O
needed	O
to	O
label	O
the	O
pain	O
.	O

The	O
take	O
-	O
home	O
message	O
is	O
that	O
fine	B-Task
-	I-Task
tuning	E-Task
from	O
a	O
data	O
-	O
extensive	O
pre	O
-	O
trained	O
domain	O
can	O
alleviate	O
small	O
training	O
set	O
problems	O
.	O

On	O
the	O
other	O
hand	O
,	O
unsupervised	B-Method
learning	E-Method
does	O
not	O
rely	O
on	O
training	O
data	O
.	O

Indeed	O
,	O
discrete	O
-	O
valued	O
regression	S-Method
is	O
a	O
good	O
test	O
bed	O
for	O
center	B-Task
-	I-Task
based	I-Task
clustering	E-Task
.	O

Although	O
regularizing	S-Method
a	O
supervised	B-Method
deep	I-Method
network	E-Method
is	O
intuitive	O
,	O
its	O
performance	O
is	O
rather	O
empirical	O
.	O

In	O
the	O
future	O
,	O
we	O
need	O
insights	O
about	O
when	O
and	O
why	O
it	O
may	O
function	O
as	O
transfer	B-Method
learning	E-Method
.	O

Note	O
that	O
no	O
temporal	O
information	O
is	O
modeled	O
in	O
this	O
paper	O
.	O

As	O
pain	O
is	O
temporal	O
and	O
subjective	O
,	O
prior	O
knowledge	O
about	O
the	O
stimulus	O
needs	O
to	O
be	O
incorporated	O
to	O
help	O
quantify	O
individual	O
differences	O
.	O

section	O
:	O
ACKNOWLEDGMENT	O
When	O
performing	O
this	O
work	O
,	O
Xiang	O
Xiang	O
is	O
funded	O
by	O
JHU	O
CS	O
Dept	O
’s	O
teaching	O
assistantship	O
,	O
Feng	O
Wang	O
&	O
Alan	O
Yuille	O
are	O
supported	O
by	O
the	O
Office	O
of	O
Naval	O
Research	O
(	O
ONR	O
N00014	O
-	O
15	O
-	O
1	O
-	O
2356	O
)	O
,	O
Feng	O
&	O
Jian	O
Chen	O
are	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61671125	O
,	O
61201271	O
)	O
,	O
and	O
Feng	O
is	O
also	O
funded	O
by	O
China	O
Scholarship	O
Council	O
(	O
CSC	O
)	O
.	O

Xiang	O
is	O
grateful	O
for	O
a	O
fellowship	O
from	O
CSC	O
in	O
previous	O
years	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Light	B-Method
Gated	I-Method
Recurrent	I-Method
Units	E-Method
for	O
Speech	B-Task
Recognition	E-Task
A	O
field	O
that	O
has	O
directly	O
benefited	O
from	O
the	O
recent	O
advances	O
in	O
deep	B-Method
learning	E-Method
is	O
Automatic	O
Speech	B-Task
Recognition	E-Task
(	O
ASR	S-Task
)	O
.	O

Despite	O
the	O
great	O
achievements	O
of	O
the	O
past	O
decades	O
,	O
however	O
,	O
a	O
natural	O
and	O
robust	O
human	B-Task
-	I-Task
machine	I-Task
speech	I-Task
interaction	E-Task
still	O
appears	O
to	O
be	O
out	O
of	O
reach	O
,	O
especially	O
in	O
challenging	O
environments	O
characterized	O
by	O
significant	O
noise	O
and	O
reverberation	O
.	O

To	O
improve	O
robustness	S-Metric
,	O
modern	O
speech	B-Method
recognizers	E-Method
often	O
employ	O
acoustic	B-Method
models	E-Method
based	O
on	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
(	O
RNNs	S-Method
)	O
,	O
that	O
are	O
naturally	O
able	O
to	O
exploit	O
large	O
time	O
contexts	O
and	O
long	O
-	O
term	O
speech	O
modulations	O
.	O

It	O
is	O
thus	O
of	O
great	O
interest	O
to	O
continue	O
the	O
study	O
of	O
proper	O
techniques	O
for	O
improving	O
the	O
effectiveness	O
of	O
RNNs	S-Method
in	O
processing	O
speech	O
signals	O
.	O

In	O
this	O
paper	O
,	O
we	O
revise	O
one	O
of	O
the	O
most	O
popular	O
RNN	S-Method
models	O
,	O
namely	O
Gated	B-Method
Recurrent	I-Method
Units	E-Method
(	O
GRUs	S-Method
)	O
,	O
and	O
propose	O
a	O
simplified	O
architecture	O
that	O
turned	O
out	O
to	O
be	O
very	O
effective	O
for	O
ASR	S-Task
.	O

The	O
contribution	O
of	O
this	O
work	O
is	O
two	O
-	O
fold	O
:	O
First	O
,	O
we	O
analyze	O
the	O
role	O
played	O
by	O
the	O
reset	O
gate	O
,	O
showing	O
that	O
a	O
significant	O
redundancy	O
with	O
the	O
update	O
gate	O
occurs	O
.	O

As	O
a	O
result	O
,	O
we	O
propose	O
to	O
remove	O
the	O
former	O
from	O
the	O
GRU	S-Method
design	O
,	O
leading	O
to	O
a	O
more	O
efficient	O
and	O
compact	O
single	B-Method
-	I-Method
gate	I-Method
model	E-Method
.	O

Second	O
,	O
we	O
propose	O
to	O
replace	O
hyperbolic	B-Method
tangent	E-Method
with	O
ReLU	S-Method
activations	O
.	O

This	O
variation	O
couples	O
well	O
with	O
batch	B-Method
normalization	E-Method
and	O
could	O
help	O
the	O
model	O
learn	O
long	O
-	O
term	O
dependencies	O
without	O
numerical	O
issues	O
.	O

Results	O
show	O
that	O
the	O
proposed	O
architecture	O
,	O
called	O
Light	B-Method
GRU	E-Method
(	O
Li	O
-	O
GRU	S-Method
)	O
,	O
not	O
only	O
reduces	O
the	O
per	B-Metric
-	I-Metric
epoch	I-Metric
training	I-Metric
time	E-Metric
by	O
more	O
than	O
30	O
%	O
over	O
a	O
standard	O
GRU	S-Method
,	O
but	O
also	O
consistently	O
improves	O
the	O
recognition	B-Metric
accuracy	E-Metric
across	O
different	O
tasks	O
,	O
input	O
features	O
,	O
noisy	O
conditions	O
,	O
as	O
well	O
as	O
across	O
different	O
ASR	S-Task
paradigms	O
,	O
ranging	O
from	O
standard	O
DNN	S-Method
-	O
HMM	O
speech	O
recognizers	O
to	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
CTC	I-Method
models	E-Method
.	O

speech	B-Task
recognition	E-Task
,	O
deep	B-Task
learning	E-Task
,	O
recurrent	B-Method
neural	I-Method
networks	E-Method
,	O
LSTM	S-Method
,	O
GRU	S-Method
section	O
:	O
Introduction	O
Deep	B-Method
learning	E-Method
is	O
an	O
emerging	O
technology	O
that	O
is	O
considered	O
one	O
of	O
the	O
most	O
promising	O
directions	O
for	O
reaching	O
higher	O
levels	O
of	O
artificial	B-Task
intelligence	E-Task
.	O

This	O
paradigm	O
is	O
rapidly	O
evolving	O
and	O
some	O
noteworthy	O
achievements	O
of	O
the	O
last	O
years	O
include	O
,	O
among	O
the	O
others	O
,	O
the	O
development	O
of	O
effective	O
regularization	B-Method
methods	E-Method
,	O
improved	O
optimization	B-Method
algorithms	E-Method
,	O
and	O
better	O
architectures	O
.	O

The	O
exploration	O
of	O
generative	B-Method
models	E-Method
,	O
deep	B-Method
reinforcement	I-Method
learning	E-Method
as	O
well	O
as	O
the	O
evolution	O
of	O
sequence	B-Task
to	I-Task
sequence	I-Task
paradigms	E-Task
also	O
represent	O
important	O
milestones	O
in	O
the	O
field	O
.	O

Deep	B-Method
learning	E-Method
is	O
now	O
being	O
deployed	O
in	O
a	O
wide	O
range	O
of	O
domains	O
,	O
including	O
bio	B-Task
-	I-Task
informatics	E-Task
,	O
computer	B-Task
vision	E-Task
,	O
machine	B-Task
translation	E-Task
,	O
dialogue	B-Task
systems	E-Task
and	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
just	O
to	O
name	O
a	O
few	O
.	O

Another	O
field	O
that	O
has	O
been	O
transformed	O
by	O
this	O
technology	O
is	O
Automatic	O
Speech	B-Task
Recognition	E-Task
(	O
ASR	S-Task
)	O
.	O

Thanks	O
to	O
modern	O
Deep	B-Method
Neural	I-Method
Networks	E-Method
(	O
DNNs	S-Method
)	O
,	O
current	O
speech	B-Method
recognizers	E-Method
are	O
now	O
able	O
to	O
significantly	O
outperform	O
previous	O
GMM	B-Method
-	I-Method
HMM	I-Method
systems	E-Method
,	O
allowing	O
ASR	S-Task
to	O
be	O
applied	O
in	O
several	O
contexts	O
,	O
such	O
as	O
web	B-Task
-	I-Task
search	E-Task
,	O
intelligent	B-Task
personal	I-Task
assistants	E-Task
,	O
car	B-Task
control	E-Task
and	O
radiological	B-Task
reporting	E-Task
.	O

Despite	O
the	O
progress	O
of	O
the	O
last	O
decade	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
speech	B-Method
recognizers	E-Method
are	O
still	O
far	O
away	O
from	O
reaching	O
satisfactory	O
robustness	S-Metric
and	O
flexibility	S-Metric
.	O

This	O
lack	O
of	O
robustness	S-Metric
typically	O
happens	O
when	O
facing	O
challenging	O
acoustic	O
conditions	O
,	O
characterized	O
by	O
considerable	O
levels	O
of	O
non	O
-	O
stationary	O
noise	O
and	O
acoustic	O
reverberation	O
.	O

The	O
development	O
of	O
robust	O
ASR	S-Task
has	O
been	O
recently	O
fostered	O
by	O
the	O
great	O
success	O
of	O
some	O
international	O
challenges	O
such	O
as	O
CHiME	S-Material
,	O
REVERB	S-Method
and	O
ASpIRE	S-Method
,	O
which	O
were	O
also	O
extremely	O
useful	O
to	O
establish	O
common	O
evaluation	B-Method
frameworks	E-Method
among	O
researchers	O
.	O

Currently	O
,	O
the	O
dominant	O
approach	O
to	O
automatic	B-Task
speech	I-Task
recognition	E-Task
relies	O
on	O
a	O
combination	O
of	O
a	O
discriminative	O
DNN	S-Method
and	O
a	O
generative	B-Method
Hidden	I-Method
Markov	I-Method
Model	E-Method
(	O
HMM	S-Method
)	O
.	O

The	O
DNN	S-Method
is	O
normally	O
employed	O
for	O
acoustic	B-Task
modeling	I-Task
purposes	E-Task
to	O
predict	O
context	B-Task
-	I-Task
dependent	I-Task
phone	I-Task
targets	E-Task
.	O

The	O
acoustic	O
-	O
level	O
predictions	O
are	O
later	O
embedded	O
in	O
an	O
HMM	B-Method
-	I-Method
based	I-Method
framework	E-Method
,	O
that	O
also	O
integrates	O
phone	O
-	O
transitions	O
,	O
lexicon	O
,	O
and	O
language	O
model	O
information	O
to	O
retrieve	O
the	O
final	O
sequence	O
of	O
words	O
.	O

An	O
emerging	O
alternative	O
is	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
speech	I-Task
recognition	E-Task
,	O
that	O
aims	O
to	O
drastically	O
simplify	O
the	O
current	O
ASR	S-Task
pipeline	O
by	O
using	O
fully	B-Method
discriminative	I-Method
systems	E-Method
that	O
learn	O
everything	O
from	O
data	O
without	O
(	O
ideally	O
)	O
any	O
additional	O
human	O
effort	O
.	O

Popular	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
techniques	E-Method
are	O
attention	B-Method
models	E-Method
and	O
Connectionist	B-Method
Temporal	I-Method
Classification	E-Method
(	O
CTC	S-Method
)	O
.	O

Attention	B-Method
models	E-Method
are	O
based	O
on	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
architecture	E-Method
coupled	O
with	O
an	O
attention	B-Method
mechanism	E-Method
that	O
decides	O
which	O
input	O
information	O
to	O
analyze	O
at	O
each	O
decoding	O
step	O
.	O

CTC	S-Method
is	O
based	O
on	O
a	O
DNN	S-Method
predicting	O
symbols	O
from	O
a	O
predefined	O
alphabet	O
(	O
characters	O
,	O
phones	O
,	O
words	O
)	O
to	O
which	O
an	O
extra	O
unit	O
(	O
blank	O
)	O
that	O
emits	O
no	O
labels	O
is	O
added	O
.	O

Similarly	O
to	O
HMMs	S-Method
,	O
the	O
likelihood	O
(	O
and	O
its	O
gradient	O
with	O
respect	O
to	O
the	O
DNN	S-Method
parameters	O
)	O
are	O
computed	O
with	O
dynamic	B-Method
programming	E-Method
by	O
summing	O
over	O
all	O
the	O
paths	O
that	O
are	O
possible	O
realizations	O
of	O
the	O
ground	O
-	O
truth	O
label	O
sequence	O
.	O

This	O
way	O
,	O
CTC	S-Method
allows	O
one	O
to	O
optimize	O
the	O
likelihood	O
of	O
the	O
desired	O
output	O
sequence	O
directly	O
,	O
without	O
the	O
need	O
for	O
an	O
explicit	O
label	O
alignment	O
.	O

For	O
both	O
the	O
aforementioned	O
frameworks	O
,	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
(	O
RNNs	S-Method
)	O
represent	O
a	O
valid	O
alternative	O
to	O
standard	O
feed	O
-	O
forward	O
DNNs	S-Method
.	O

RNNs	S-Method
,	O
in	O
fact	O
,	O
are	O
more	O
and	O
more	O
often	O
employed	O
in	O
speech	B-Task
recognition	E-Task
,	O
due	O
to	O
their	O
capabilities	O
to	O
properly	O
manage	O
time	O
contexts	O
and	O
capture	O
long	O
-	O
term	O
speech	O
modulations	O
.	O

In	O
the	O
machine	B-Task
learning	I-Task
community	E-Task
,	O
the	O
research	O
of	O
novel	O
and	O
powerful	O
RNN	S-Method
models	O
is	O
a	O
very	O
active	O
research	O
topic	O
.	O

General	O
-	O
purpose	O
RNNs	S-Method
such	O
as	O
Long	B-Method
Short	I-Method
Term	I-Method
Memories	E-Method
(	O
LSTMs	S-Method
)	O
have	O
been	O
the	O
subject	O
of	O
several	O
studies	O
and	O
modifications	O
over	O
the	O
past	O
years	O
.	O

This	O
evolution	O
has	O
recently	O
led	O
to	O
a	O
novel	O
architecture	O
called	O
Gated	B-Method
Recurrent	I-Method
Unit	E-Method
(	O
GRU	S-Method
)	O
,	O
that	O
simplifies	O
the	O
complex	O
LSTM	B-Method
cell	I-Method
design	E-Method
.	O

Our	O
work	O
continues	O
these	O
efforts	O
by	O
further	O
revising	O
GRUs	S-Method
.	O

Differently	O
from	O
previous	O
efforts	O
,	O
our	O
primary	O
goal	O
is	O
not	O
to	O
derive	O
a	O
general	O
-	O
purpose	O
RNN	S-Method
,	O
but	O
to	O
modify	O
the	O
standard	O
GRU	S-Method
design	O
in	O
order	O
to	O
better	O
address	O
speech	B-Task
recognition	E-Task
.	O

In	O
particular	O
,	O
the	O
major	O
contribution	O
of	O
this	O
paper	O
is	O
twofold	O
:	O
First	O
,	O
we	O
propose	O
to	O
remove	O
the	O
reset	O
gate	O
from	O
the	O
network	B-Method
design	E-Method
.	O

Similarly	O
to	O
,	O
we	O
found	O
that	O
removing	O
it	O
does	O
not	O
significantly	O
affect	O
the	O
system	O
performance	O
,	O
also	O
due	O
to	O
a	O
certain	O
redundancy	O
observed	O
between	O
update	O
and	O
reset	O
gates	O
.	O

Second	O
,	O
we	O
propose	O
to	O
replace	O
hyperbolic	B-Method
tangent	E-Method
(	O
tanh	S-Method
)	O
with	O
Rectified	B-Method
Linear	I-Method
Unit	E-Method
(	O
ReLU	S-Method
)	O
activations	O
in	O
the	O
state	B-Method
update	I-Method
equation	E-Method
.	O

ReLU	S-Method
units	O
have	O
been	O
shown	O
to	O
be	O
more	O
effective	O
than	O
sigmoid	B-Method
non	I-Method
-	I-Method
linearities	E-Method
for	O
feed	O
-	O
forward	O
DNNs	S-Method
.	O

Despite	O
its	O
recent	O
success	O
,	O
this	O
non	O
-	O
linearity	O
has	O
largely	O
been	O
avoided	O
for	O
RNNs	S-Method
,	O
due	O
to	O
the	O
numerical	O
instabilities	O
caused	O
by	O
the	O
unboundedness	O
of	O
ReLU	S-Method
activations	O
:	O
composing	O
many	O
times	O
GRU	S-Method
layers	O
(	O
i.e.	O
,	O
GRU	B-Method
units	E-Method
following	O
an	O
affine	B-Method
transformation	E-Method
)	O
with	O
sufficiently	O
large	O
weights	O
can	O
lead	O
to	O
arbitrarily	O
large	O
state	O
values	O
.	O

However	O
,	O
when	O
coupling	O
our	O
ReLU	S-Method
-	O
based	O
GRU	S-Method
with	O
batch	B-Method
normalization	E-Method
,	O
we	O
did	O
not	O
experience	O
such	O
numerical	O
issues	O
.	O

This	O
allows	O
us	O
to	O
take	O
advantage	O
of	O
both	O
techniques	O
,	O
that	O
have	O
been	O
proven	O
effective	O
to	O
mitigate	O
the	O
vanishing	B-Task
gradient	I-Task
problem	E-Task
as	O
well	O
as	O
to	O
speed	O
up	O
network	B-Task
training	E-Task
.	O

We	O
evaluated	O
our	O
proposed	O
architecture	O
on	O
different	O
tasks	O
,	O
datasets	O
,	O
input	O
features	O
,	O
noisy	O
conditions	O
as	O
well	O
as	O
on	O
different	O
ASR	S-Task
frameworks	O
(	O
i.e.	O
,	O
DNN	S-Method
-	O
HMM	O
and	O
CTC	S-Method
)	O
.	O

Results	O
show	O
that	O
the	O
revised	O
architecture	O
reduces	O
the	O
per	B-Metric
-	I-Metric
epoch	I-Metric
training	I-Metric
wall	I-Metric
-	I-Metric
clock	I-Metric
time	E-Metric
by	O
more	O
than	O
30	O
%	O
,	O
while	O
improving	O
the	O
recognition	B-Metric
accuracy	E-Metric
.	O

Moreover	O
,	O
the	O
proposed	O
solution	O
leads	O
to	O
a	O
compact	B-Method
model	E-Method
,	O
that	O
is	O
arguably	O
easier	O
to	O
interpret	O
,	O
understand	O
and	O
implement	O
,	O
due	O
to	O
a	O
simplified	O
design	O
based	O
on	O
a	O
single	O
gate	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Sec	O
.	O

[	O
reference	O
]	O
recalls	O
the	O
standard	O
GRU	S-Method
architecture	O
,	O
while	O
Sec	O
.	O

[	O
reference	O
]	O
illustrates	O
in	O
detail	O
the	O
proposed	O
model	O
and	O
the	O
related	O
work	O
.	O

In	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
a	O
description	O
of	O
the	O
adopted	O
corpora	O
and	O
experimental	O
setup	O
is	O
provided	O
.	O

The	O
results	O
are	O
then	O
reported	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

Finally	O
,	O
our	O
conclusions	O
are	O
drawn	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Gated	B-Method
Recurrent	I-Method
Units	E-Method
The	O
most	O
suitable	O
architecture	O
able	O
to	O
learn	O
short	O
and	O
long	O
-	O
term	O
speech	O
dependencies	O
is	O
represented	O
by	O
RNNs	S-Method
.	O

RNNs	S-Method
,	O
indeed	O
,	O
can	O
potentially	O
capture	O
temporal	O
information	O
in	O
a	O
very	O
dynamic	O
fashion	O
,	O
allowing	O
the	O
network	O
to	O
freely	O
decide	O
the	O
amount	O
of	O
contextual	O
information	O
to	O
use	O
for	O
each	O
time	O
step	O
.	O

Several	O
works	O
have	O
already	O
highlighted	O
the	O
effectiveness	O
of	O
RNNs	S-Method
in	O
various	O
speech	B-Task
processing	I-Task
tasks	E-Task
,	O
such	O
as	O
speech	B-Task
recognition	E-Task
,	O
speech	B-Task
enhancement	E-Task
,	O
speech	B-Task
separation	E-Task
as	O
well	O
as	O
speech	B-Task
activity	I-Task
detection	E-Task
.	O

Training	O
RNNs	S-Method
,	O
however	O
,	O
can	O
be	O
complicated	O
by	O
vanishing	O
and	O
exploding	O
gradients	O
,	O
that	O
might	O
impair	O
learning	O
long	O
-	O
term	O
dependencies	O
.	O

Although	O
exploding	O
gradients	O
can	O
be	O
tackled	O
with	O
simple	O
clipping	B-Method
strategies	E-Method
,	O
the	O
vanishing	B-Task
gradient	I-Task
problem	E-Task
requires	O
special	O
architectures	O
to	O
be	O
properly	O
addressed	O
.	O

A	O
common	O
approach	O
relies	O
on	O
the	O
so	O
-	O
called	O
gated	B-Method
RNNs	E-Method
,	O
whose	O
core	O
idea	O
is	O
to	O
introduce	O
a	O
gating	B-Method
mechanism	E-Method
for	O
better	O
controlling	O
the	O
flow	O
of	O
the	O
information	O
through	O
the	O
various	O
time	O
-	O
steps	O
.	O

Within	O
this	O
family	O
of	O
architectures	O
,	O
vanishing	O
gradient	O
issues	O
are	O
mitigated	O
by	O
creating	O
effective	O
“	O
shortcuts	O
”	O
,	O
in	O
which	O
the	O
gradients	O
can	O
bypass	O
multiple	O
temporal	O
steps	O
.	O

The	O
most	O
popular	O
gated	B-Method
RNNs	E-Method
are	O
LSTMs	S-Method
,	O
that	O
often	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
several	O
machine	B-Task
learning	I-Task
tasks	E-Task
,	O
including	O
speech	B-Task
recognition	E-Task
.	O

LSTMs	S-Method
rely	O
on	O
memory	B-Method
cells	E-Method
that	O
are	O
controlled	O
by	O
forget	O
,	O
input	O
,	O
and	O
output	O
gates	O
.	O

Despite	O
their	O
effectiveness	O
,	O
such	O
a	O
sophisticated	O
gating	B-Method
mechanism	E-Method
might	O
result	O
in	O
an	O
overly	O
complex	O
model	O
.	O

On	O
the	O
other	O
hand	O
,	O
computational	B-Metric
efficiency	E-Metric
is	O
a	O
crucial	O
issue	O
for	O
RNNs	S-Method
and	O
considerable	O
research	O
efforts	O
have	O
recently	O
been	O
devoted	O
to	O
the	O
development	O
of	O
alternative	O
architectures	O
.	O

A	O
noteworthy	O
attempt	O
to	O
simplify	O
LSTMs	S-Method
has	O
recently	O
led	O
to	O
a	O
novel	O
model	O
called	O
Gated	B-Method
Recurrent	I-Method
Unit	E-Method
(	O
GRU	S-Method
)	O
,	O
that	O
is	O
based	O
on	O
just	O
two	O
multiplicative	O
gates	O
.	O

In	O
particular	O
,	O
the	O
standard	O
GRU	S-Method
architecture	O
is	O
defined	O
by	O
the	O
following	O
equations	O
:	O
where	O
and	O
are	O
vectors	O
corresponding	O
to	O
the	O
update	O
and	O
reset	O
gates	O
,	O
respectively	O
,	O
while	O
represents	O
the	O
state	O
vector	O
for	O
the	O
current	O
time	O
frame	O
.	O

Element	O
-	O
wise	O
multiplications	O
are	O
denoted	O
with	O
.	O

The	O
activations	O
of	O
both	O
gates	O
are	O
logistic	B-Method
sigmoid	I-Method
functions	E-Method
,	O
that	O
constrain	O
and	O
to	O
take	O
values	O
ranging	O
from	O
0	O
and	O
1	O
.	O

The	O
candidate	O
state	O
is	O
processed	O
with	O
a	O
hyperbolic	B-Method
tangent	E-Method
.	O

The	O
network	O
is	O
fed	O
by	O
the	O
current	O
input	O
vector	O
(	O
e.g.	O
,	O
a	O
vector	O
of	O
speech	O
features	O
)	O
,	O
while	O
the	O
parameters	O
of	O
the	O
model	O
are	O
the	O
matrices	O
,	O
,	O
(	O
the	O
feed	O
-	O
forward	O
connections	O
)	O
and	O
,	O
,	O
(	O
the	O
recurrent	O
weights	O
)	O
.	O

The	O
architecture	O
finally	O
includes	O
trainable	O
bias	O
vectors	O
,	O
and	O
,	O
that	O
are	O
added	O
before	O
the	O
non	O
-	O
linearities	O
are	O
applied	O
.	O

As	O
shown	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
the	O
current	O
state	O
vector	O
is	O
a	O
linear	B-Method
interpolation	E-Method
between	O
the	O
previous	O
activation	O
and	O
the	O
current	O
candidate	O
state	O
.	O

The	O
weighting	O
factors	O
are	O
set	O
by	O
the	O
update	B-Method
gate	E-Method
,	O
that	O
decides	O
how	O
much	O
the	O
units	O
will	O
update	O
their	O
activations	O
.	O

This	O
linear	B-Method
interpolation	E-Method
is	O
the	O
key	O
component	O
for	O
learning	B-Task
long	I-Task
-	I-Task
term	I-Task
dependencies	E-Task
.	O

If	O
is	O
close	O
to	O
one	O
,	O
in	O
fact	O
,	O
the	O
previous	O
state	O
is	O
kept	O
unaltered	O
and	O
can	O
remain	O
unchanged	O
for	O
an	O
arbitrary	O
number	O
of	O
time	O
steps	O
.	O

On	O
the	O
other	O
hand	O
,	O
if	O
is	O
close	O
to	O
zero	O
,	O
the	O
network	O
tends	O
to	O
favor	O
the	O
candidate	O
state	O
,	O
that	O
depends	O
more	O
heavily	O
on	O
the	O
current	O
input	O
and	O
on	O
the	O
closer	O
hidden	O
states	O
.	O

The	O
candidate	O
state	O
also	O
depends	O
on	O
the	O
reset	O
gate	O
,	O
that	O
allows	O
the	O
model	O
to	O
possibly	O
delete	O
the	O
past	O
memory	O
by	O
forgetting	O
the	O
previously	O
computed	O
states	O
.	O

section	O
:	O
A	O
novel	O
GRU	S-Method
framework	O
The	O
main	O
changes	O
to	O
the	O
standard	O
GRU	S-Method
model	O
concern	O
the	O
reset	O
gate	O
,	O
ReLU	S-Method
activations	O
,	O
and	O
batch	B-Method
normalization	E-Method
,	O
as	O
outlined	O
in	O
the	O
next	O
sub	O
-	O
sections	O
.	O

subsection	O
:	O
Removing	O
the	O
reset	O
gate	O
From	O
the	O
previous	O
introduction	O
to	O
GRUs	S-Method
,	O
it	O
follows	O
that	O
the	O
reset	O
gate	O
can	O
be	O
useful	O
when	O
significant	O
discontinuities	O
occur	O
in	O
the	O
sequence	O
.	O

For	O
language	B-Task
modeling	E-Task
,	O
this	O
may	O
happen	O
when	O
moving	O
from	O
one	O
text	O
to	O
another	O
that	O
is	O
not	O
semantically	O
related	O
.	O

In	O
such	O
situation	O
,	O
it	O
is	O
convenient	O
to	O
reset	O
the	O
stored	O
memory	O
to	O
avoid	O
taking	O
a	O
decision	O
biased	O
by	O
an	O
unrelated	O
history	O
.	O

Nevertheless	O
,	O
we	O
believe	O
that	O
for	O
some	O
specific	O
tasks	O
like	O
speech	B-Task
recognition	E-Task
this	O
functionality	O
might	O
not	O
be	O
useful	O
.	O

In	O
fact	O
,	O
a	O
speech	O
signal	O
is	O
a	O
sequence	O
that	O
evolves	O
rather	O
slowly	O
(	O
the	O
features	O
are	O
typically	O
computed	O
every	O
10	O
ms	O
)	O
,	O
in	O
which	O
the	O
past	O
history	O
can	O
virtually	O
always	O
be	O
helpful	O
.	O

Even	O
in	O
the	O
presence	O
of	O
strong	O
discontinuities	O
,	O
for	O
instance	O
observable	O
at	O
the	O
boundary	O
between	O
a	O
vowel	O
and	O
a	O
fricative	O
,	O
completely	O
resetting	O
the	O
past	O
memory	O
can	O
be	O
harmful	O
.	O

On	O
the	O
other	O
hand	O
,	O
it	O
is	O
helpful	O
to	O
memorize	O
phonotactic	O
features	O
,	O
since	O
some	O
phone	O
transitions	O
are	O
more	O
likely	O
than	O
others	O
.	O

We	O
also	O
argue	O
that	O
a	O
certain	O
redundancy	O
in	O
the	O
activations	O
of	O
reset	O
and	O
update	O
gates	O
might	O
occur	O
when	O
processing	O
speech	O
sequences	O
.	O

For	O
instance	O
,	O
when	O
it	O
is	O
necessary	O
to	O
give	O
more	O
importance	O
to	O
the	O
current	O
information	O
,	O
the	O
GRU	S-Method
model	O
can	O
set	O
small	O
values	O
of	O
.	O

A	O
similar	O
effect	O
can	O
be	O
achieved	O
with	O
the	O
update	O
gate	O
only	O
,	O
if	O
small	O
values	O
are	O
assigned	O
to	O
.	O

The	O
latter	O
solution	O
tends	O
to	O
weight	O
more	O
the	O
candidate	O
state	O
,	O
that	O
depends	O
heavily	O
on	O
the	O
current	O
input	O
.	O

Similarly	O
,	O
a	O
high	O
value	O
can	O
be	O
assigned	O
either	O
to	O
or	O
to	O
,	O
in	O
order	O
to	O
place	O
more	O
importance	O
on	O
past	O
states	O
.	O

This	O
redundancy	O
is	O
also	O
highlighted	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
where	O
a	O
temporal	O
correlation	O
in	O
the	O
average	O
activations	O
of	O
update	O
and	O
reset	O
gates	O
can	O
be	O
readily	O
appreciated	O
for	O
a	O
GRU	S-Method
trained	O
on	O
TIMIT	S-Material
.	O

This	O
degree	O
of	O
redundancy	O
will	O
be	O
analyzed	O
in	O
a	O
quantitative	O
way	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
using	O
the	O
cross	B-Metric
-	I-Metric
correlation	I-Metric
metric	E-Metric
:	O
where	O
and	O
are	O
the	O
average	O
activations	O
(	O
over	O
the	O
neurons	O
)	O
of	O
update	O
and	O
reset	O
gates	O
,	O
respectively	O
,	O
and	O
is	O
the	O
cross	O
-	O
correlation	O
operator	O
.	O

Based	O
on	O
these	O
reasons	O
,	O
the	O
first	O
variation	O
to	O
standard	O
GRUs	S-Method
thus	O
concerns	O
the	O
removal	O
of	O
the	O
reset	O
gate	O
.	O

This	O
change	O
leads	O
to	O
the	O
following	O
modification	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
:	O
The	O
main	O
benefits	O
of	O
this	O
intervention	O
are	O
related	O
to	O
the	O
improved	O
computational	B-Metric
efficiency	E-Metric
,	O
that	O
is	O
achieved	O
thanks	O
to	O
a	O
more	O
compact	O
single	B-Method
-	I-Method
gate	I-Method
model	E-Method
.	O

subsection	O
:	O
ReLU	S-Method
activations	O
The	O
second	O
modification	O
consists	O
in	O
replacing	O
the	O
standard	O
hyperbolic	B-Method
tangent	E-Method
with	O
ReLU	S-Method
activation	O
.	O

In	O
particular	O
,	O
we	O
modify	O
the	O
computation	O
of	O
candidate	O
state	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
,	O
as	O
follows	O
:	O
Standard	O
tanh	S-Method
activations	O
are	O
less	O
used	O
in	O
feedforward	B-Method
networks	E-Method
because	O
they	O
do	O
not	O
work	O
as	O
well	O
as	O
piecewise	B-Method
-	I-Method
linear	I-Method
activations	E-Method
when	O
training	O
deeper	B-Method
networks	E-Method
.	O

The	O
adoption	O
of	O
ReLU	S-Method
-	O
based	O
neurons	O
,	O
that	O
have	O
shown	O
to	O
be	O
effective	O
in	O
improving	O
such	O
limitations	O
,	O
was	O
not	O
so	O
common	O
in	O
the	O
past	O
for	O
RNNs	S-Method
.	O

This	O
was	O
due	O
to	O
numerical	O
instabilities	O
originating	O
from	O
the	O
unbounded	O
ReLU	S-Method
functions	O
applied	O
over	O
long	O
time	O
series	O
.	O

However	O
,	O
coupling	O
this	O
activation	B-Method
function	E-Method
with	O
batch	B-Method
normalization	E-Method
turned	O
out	O
to	O
be	O
helpful	O
for	O
taking	O
advantage	O
of	O
ReLU	S-Method
neurons	O
without	O
numerical	O
issues	O
,	O
as	O
will	O
be	O
discussed	O
in	O
the	O
next	O
sub	O
-	O
section	O
.	O

subsection	O
:	O
Batch	B-Method
Normalization	I-Method
Batch	I-Method
normalization	E-Method
has	O
been	O
recently	O
proposed	O
in	O
the	O
machine	B-Task
learning	I-Task
community	E-Task
and	O
addresses	O
the	O
so	O
-	O
called	O
internal	B-Task
covariate	I-Task
shift	I-Task
problem	E-Task
by	O
normalizing	O
the	O
mean	O
and	O
the	O
variance	O
of	O
each	O
layer	O
’s	O
pre	O
-	O
activations	O
for	O
each	O
training	O
mini	O
-	O
batch	O
.	O

Several	O
works	O
have	O
already	O
shown	O
that	O
this	O
technique	O
is	O
effective	O
both	O
to	O
improve	O
the	O
system	O
performance	O
and	O
to	O
speed	O
-	O
up	O
the	O
training	B-Task
procedure	E-Task
.	O

Batch	B-Method
normalization	E-Method
can	O
be	O
applied	O
to	O
RNNs	S-Method
in	O
different	O
ways	O
.	O

In	O
,	O
the	O
authors	O
suggest	O
to	O
apply	O
it	O
to	O
feed	O
-	O
forward	O
connections	O
only	O
,	O
while	O
in	O
the	O
normalization	B-Method
step	E-Method
is	O
extended	O
to	O
recurrent	O
connections	O
,	O
using	O
separate	O
statistics	O
for	O
each	O
time	O
-	O
step	O
.	O

In	O
our	O
work	O
,	O
we	O
tried	O
both	O
approaches	O
,	O
but	O
we	O
did	O
not	O
observe	O
substantial	O
benefits	O
when	O
extending	O
batch	B-Method
normalization	E-Method
to	O
recurrent	O
parameters	O
(	O
i.e.	O
,	O
and	O
)	O
.	O

For	O
this	O
reason	O
,	O
we	O
applied	O
this	O
technique	O
to	O
feed	O
-	O
forward	O
connections	O
only	O
(	O
i.e.	O
,	O
and	O
)	O
,	O
obtaining	O
a	O
more	O
compact	O
model	O
that	O
is	O
almost	O
equally	O
performing	O
but	O
significantly	O
less	O
computationally	O
expensive	O
.	O

When	O
batch	B-Method
normalization	E-Method
is	O
limited	O
to	O
feed	O
-	O
forward	O
connections	O
,	O
indeed	O
,	O
all	O
the	O
related	O
computations	O
become	O
independent	O
at	O
each	O
time	O
step	O
and	O
they	O
can	O
be	O
performed	O
in	O
parallel	O
.	O

This	O
offers	O
the	O
possibility	O
to	O
apply	O
it	O
with	O
reduced	O
computational	B-Metric
efforts	E-Metric
.	O

As	O
outlined	O
in	O
the	O
previous	O
sub	O
-	O
section	O
,	O
coupling	O
the	O
proposed	O
model	O
with	O
batch	B-Method
-	I-Method
normalization	E-Method
could	O
also	O
help	O
in	O
limiting	O
the	O
numerical	O
issues	O
of	O
ReLU	B-Method
RNNs	E-Method
.	O

Batch	B-Method
normalization	E-Method
,	O
in	O
fact	O
,	O
rescales	O
the	O
neuron	O
pre	O
-	O
activations	O
,	O
inherently	O
bounding	O
the	O
values	O
of	O
the	O
ReLU	S-Method
neurons	O
.	O

In	O
this	O
way	O
,	O
our	O
model	O
concurrently	O
takes	O
advantage	O
of	O
the	O
well	O
-	O
known	O
benefits	O
of	O
both	O
ReLU	S-Method
activation	O
and	O
batch	B-Method
normalization	E-Method
.	O

In	O
our	O
experiments	O
,	O
we	O
found	O
that	O
the	O
latter	O
technique	O
helps	O
against	O
numerical	O
issues	O
also	O
when	O
it	O
is	O
limited	O
to	O
feed	O
-	O
forward	O
connections	O
only	O
.	O

Formally	O
,	O
removing	O
the	O
reset	O
gate	O
,	O
replacing	O
the	O
hyperbolic	B-Method
tangent	E-Method
function	O
with	O
the	O
ReLU	S-Method
activation	O
,	O
and	O
applying	O
batch	B-Method
normalization	E-Method
,	O
now	O
leads	O
to	O
the	O
following	O
model	O
:	O
The	O
batch	B-Task
normalization	E-Task
works	O
as	O
described	O
in	O
,	O
and	O
is	O
defined	O
as	O
follows	O
:	O
where	O
and	O
are	O
the	O
minibatch	O
mean	O
and	O
variance	O
,	O
respectively	O
.	O

A	O
small	O
constant	O
is	O
added	O
for	O
numerical	O
stability	O
.	O

The	O
variables	O
and	O
are	O
trainable	O
scaling	O
and	O
shifting	O
parameters	O
,	O
introduced	O
to	O
restore	O
the	O
network	O
capacity	O
.	O

Note	O
that	O
the	O
presence	O
of	O
makes	O
the	O
biases	O
and	O
redundant	O
.	O

Therefore	O
,	O
they	O
are	O
omitted	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

We	O
called	O
this	O
architecture	O
Light	B-Method
GRU	E-Method
(	O
Li	B-Method
-	I-Method
GRU	E-Method
)	O
,	O
to	O
emphasize	O
the	O
simplification	B-Task
process	E-Task
conducted	O
on	O
a	O
standard	O
GRU	S-Method
.	O

subsection	O
:	O
Related	O
work	O
A	O
first	O
attempt	O
to	O
remove	O
from	O
GRUs	S-Method
has	O
recently	O
led	O
to	O
a	O
single	B-Method
-	I-Method
gate	I-Method
architecture	E-Method
called	O
Minimal	B-Method
Gated	I-Method
Recurrent	I-Method
Unit	E-Method
(	O
M	B-Method
-	I-Method
GRU	E-Method
)	O
,	O
that	O
achieves	O
a	O
performance	O
comparable	O
to	O
that	O
obtained	O
by	O
standard	O
GRUs	S-Method
in	O
handwritten	B-Task
digit	I-Task
recognition	E-Task
as	O
well	O
as	O
in	O
a	O
sentiment	B-Task
classification	I-Task
task	E-Task
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
contribution	O
is	O
the	O
first	O
attempt	O
that	O
explores	O
this	O
architectural	B-Method
variation	E-Method
in	O
speech	B-Task
recognition	E-Task
.	O

Recently	O
,	O
some	O
attempts	O
have	O
also	O
been	O
done	O
for	O
embedding	O
ReLU	S-Method
units	O
in	O
the	O
RNN	S-Method
framework	O
.	O

For	O
instance	O
,	O
in	O
authors	O
replaced	O
tanh	S-Method
activations	O
with	O
ReLU	S-Method
neurons	O
in	O
a	O
vanilla	O
RNN	S-Method
,	O
showing	O
the	O
capability	O
of	O
this	O
model	O
to	O
learn	O
long	B-Task
-	I-Task
term	I-Task
dependencies	E-Task
when	O
a	O
proper	O
orthogonal	B-Method
initialization	E-Method
is	O
adopted	O
.	O

In	O
this	O
work	O
,	O
we	O
extend	O
the	O
use	O
of	O
ReLU	S-Method
to	O
a	O
GRU	S-Method
architecture	O
.	O

In	O
summary	O
,	O
the	O
novelty	O
of	O
our	O
approach	O
consists	O
in	O
the	O
integration	O
of	O
three	O
key	O
design	O
aspects	O
(	O
i.e	O
,	O
the	O
removal	O
of	O
the	O
reset	O
gate	O
,	O
ReLU	S-Method
activations	O
and	O
batch	B-Method
normalization	E-Method
)	O
in	O
a	O
single	O
model	O
,	O
that	O
turned	O
out	O
to	O
be	O
particularly	O
suitable	O
for	O
speech	B-Task
recognition	E-Task
.	O

The	O
potential	O
benefits	O
Li	B-Method
-	I-Method
GRUs	E-Method
have	O
been	O
preliminarily	O
observed	O
as	O
part	O
of	O
a	O
work	O
on	O
speech	B-Task
recognition	E-Task
described	O
in	O
.	O

This	O
study	O
extends	O
our	O
previous	O
effort	O
in	O
several	O
ways	O
.	O

First	O
of	O
all	O
,	O
we	O
better	O
analyze	O
the	O
correlation	O
arising	O
between	O
reset	O
and	O
update	O
gates	O
.	O

We	O
then	O
analyze	O
some	O
gradient	O
statistics	O
,	O
and	O
we	O
better	O
study	O
the	O
impact	O
of	O
batch	B-Method
normalization	E-Method
.	O

Moreover	O
,	O
we	O
assess	O
our	O
approach	O
on	O
a	O
larger	O
variety	O
of	O
speech	B-Task
recognition	I-Task
tasks	E-Task
,	O
considering	O
several	O
different	O
datasets	O
as	O
well	O
as	O
noisy	O
and	O
reverberant	O
conditions	O
.	O

Finally	O
,	O
we	O
extend	O
our	O
experimental	O
validation	O
to	O
a	O
end	O
-	O
to	O
-	O
end	O
CTC	B-Method
model	E-Method
.	O

section	O
:	O
Experimental	O
setup	O
In	O
the	O
following	O
sub	O
-	O
sections	O
,	O
the	O
considered	O
corpora	O
,	O
the	O
RNN	S-Method
setting	O
as	O
well	O
as	O
the	O
HMM	O
-	O
DNN	S-Method
and	O
CTC	O
setups	O
are	O
described	O
.	O

subsection	O
:	O
Corpora	O
and	O
tasks	O
The	O
main	O
features	O
of	O
the	O
corpora	O
considered	O
in	O
this	O
work	O
are	O
summarized	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

A	O
first	O
set	O
of	O
experiments	O
with	O
the	O
TIMIT	S-Material
corpus	O
was	O
performed	O
to	O
test	O
the	O
proposed	O
model	O
in	O
a	O
close	B-Task
-	I-Task
talking	I-Task
scenario	E-Task
.	O

These	O
experiments	O
are	O
based	O
on	O
the	O
standard	O
phoneme	B-Task
recognition	I-Task
task	E-Task
,	O
which	O
is	O
aligned	O
with	O
that	O
proposed	O
in	O
the	O
Kaldi	B-Method
s5	I-Method
recipe	E-Method
.	O

To	O
validate	O
our	O
model	O
in	O
a	O
more	O
realistic	O
scenario	O
,	O
a	O
set	O
of	O
experiments	O
was	O
also	O
conducted	O
in	O
distant	O
-	O
talking	O
conditions	O
with	O
the	O
DIRHA	B-Material
-	I-Material
English	E-Material
corpus	O
.	O

The	O
reference	O
context	O
was	O
a	O
domestic	O
environment	O
characterized	O
by	O
the	O
presence	O
of	O
non	O
-	O
stationary	O
noise	O
(	O
with	O
an	O
average	O
SNR	S-Metric
of	O
about	O
10dB	O
)	O
and	O
acoustic	O
reverberation	O
(	O
with	O
an	O
average	O
reverberation	O
time	O
of	O
about	O
0.7	O
seconds	O
)	O
.	O

Training	S-Method
was	O
based	O
on	O
the	O
original	O
WSJ	B-Material
-	I-Material
5k	I-Material
corpus	E-Material
(	O
consisting	O
of	O
7138	O
sentences	O
uttered	O
by	O
83	O
speakers	O
)	O
that	O
was	O
contaminated	O
with	O
a	O
set	O
of	O
impulse	O
responses	O
measured	O
in	O
a	O
real	O
apartment	O
.	O

The	O
test	O
phase	O
was	O
carried	O
out	O
with	O
both	O
real	O
and	O
simulated	O
datasets	O
,	O
each	O
consisting	O
of	O
409	O
WSJ	B-Material
sentences	E-Material
uttered	O
by	O
six	O
native	O
American	O
speakers	O
.	O

A	O
development	O
set	O
of	O
310	O
WSJ	B-Material
sentences	E-Material
uttered	O
by	O
six	O
different	O
speakers	O
was	O
also	O
used	O
for	O
hyperparameter	B-Method
tuning	E-Method
.	O

To	O
test	O
our	O
approach	O
in	O
different	O
reverberation	O
conditions	O
,	O
other	O
contaminated	O
versions	O
of	O
the	O
latter	O
training	O
and	O
test	O
data	O
are	O
generated	O
with	O
different	O
impulse	O
responses	O
.	O

These	O
simulations	O
are	O
based	O
on	O
the	O
image	B-Method
method	E-Method
and	O
correspond	O
to	O
four	O
different	O
reverberation	O
times	O
(	O
ranging	O
from	O
250	O
to	O
1000	O
ms	O
)	O
.	O

More	O
details	O
on	O
the	O
realistic	O
impulse	O
responses	O
adopted	O
in	O
this	O
corpus	O
can	O
be	O
found	O
in	O
.	O

Additional	O
experiments	O
were	O
conducted	O
with	O
the	O
CHiME	S-Material
4	O
dataset	O
,	O
that	O
is	O
based	O
on	O
both	O
real	O
and	O
simulated	O
data	O
recorded	O
in	O
four	O
noisy	O
environments	O
(	O
on	O
a	O
bus	O
,	O
cafe	O
,	O
pedestrian	O
area	O
,	O
and	O
street	O
junction	O
)	O
.	O

The	O
training	O
set	O
is	O
composed	O
of	O
43690	O
noisy	B-Material
WSJ	I-Material
sentences	E-Material
recored	O
by	O
five	O
microphones	O
(	O
arranged	O
on	O
a	O
tablet	O
)	O
and	O
uttered	O
by	O
a	O
total	O
of	O
87	O
speakers	O
.	O

The	O
development	O
set	O
(	O
DT	S-Material
)	O
is	O
based	O
on	O
3280	O
WSJ	B-Material
sentences	E-Material
uttered	O
by	O
four	O
speakers	O
(	O
1640	O
are	O
real	O
utterances	O
referred	O
to	O
as	O
DT	S-Material
-	O
real	O
,	O
and	O
1640	O
are	O
simulated	O
denoted	O
as	O
DT	S-Material
-	O
sim	O
)	O
.	O

The	O
test	O
set	O
(	O
ET	S-Material
)	O
is	O
based	O
on	O
1320	O
real	O
utterances	O
(	O
ET	B-Material
-	I-Material
real	E-Material
)	O
and	O
1320	O
simulated	O
sentences	O
(	O
DT	S-Material
-	O
real	O
)	O
from	O
other	O
four	O
speakers	O
.	O

The	O
experiments	O
reported	O
in	O
this	O
paper	O
are	O
based	O
on	O
the	O
single	B-Task
channel	I-Task
setting	E-Task
,	O
in	O
which	O
the	O
test	O
phase	O
is	O
carried	O
out	O
with	O
a	O
single	O
microphone	O
(	O
randomly	O
selected	O
from	O
the	O
considered	O
microphone	O
setup	O
)	O
.	O

More	O
information	O
on	O
CHiME	S-Material
data	O
can	O
be	O
found	O
in	O
.	O

To	O
evaluate	O
the	O
proposed	O
model	O
on	O
a	O
larger	O
scale	O
ASR	S-Task
task	O
,	O
some	O
additional	O
experiments	O
were	O
performed	O
with	O
the	O
TED	B-Material
-	I-Material
talk	I-Material
dataset	E-Material
,	O
that	O
was	O
released	O
in	O
the	O
context	O
of	O
the	O
IWSLT	B-Material
evaluation	I-Material
campaigns	E-Material
.	O

The	O
training	O
set	O
is	O
composed	O
of	O
820	O
talks	O
with	O
a	O
total	O
of	O
about	O
166	O
hours	O
of	O
speech	O
.	O

The	O
development	O
test	O
is	O
composed	O
of	O
81	O
talks	O
(	O
16	O
hours	O
)	O
,	O
while	O
the	O
test	O
sets	O
(	O
TST	B-Material
2011	E-Material
and	O
TST	B-Material
2012	E-Material
)	O
are	O
based	O
on	O
8	O
talks	O
(	O
1.5	O
hours	O
)	O
and	O
32	O
talks	O
(	O
6.5	O
hours	O
)	O
,	O
respectively	O
.	O

subsection	O
:	O
RNN	S-Method
setting	O
The	O
architecture	O
adopted	O
for	O
the	O
experiments	O
consisted	O
of	O
multiple	O
recurrent	B-Method
layers	E-Method
,	O
that	O
were	O
stacked	O
together	O
prior	O
to	O
the	O
final	O
softmax	B-Method
classifier	E-Method
.	O

These	O
recurrent	B-Method
layers	E-Method
were	O
bidirectional	O
RNNs	S-Method
,	O
which	O
were	O
obtained	O
by	O
concatenating	O
the	O
forward	O
hidden	O
states	O
(	O
collected	O
by	O
processing	O
the	O
sequence	O
from	O
the	O
beginning	O
to	O
the	O
end	O
)	O
with	O
backward	O
hidden	O
states	O
(	O
gathered	O
by	O
scanning	O
the	O
speech	O
in	O
the	O
reverse	O
time	O
order	O
)	O
.	O

Recurrent	B-Method
dropout	E-Method
was	O
used	O
as	O
regularization	B-Method
technique	E-Method
.	O

Since	O
extending	O
standard	O
dropout	S-Method
to	O
recurrent	O
connections	O
hinders	O
learning	B-Task
long	I-Task
-	I-Task
term	I-Task
dependencies	E-Task
,	O
we	O
followed	O
the	O
approach	O
introduced	O
in	O
,	O
that	O
tackles	O
this	O
issue	O
by	O
sharing	O
the	O
same	O
dropout	O
mask	O
across	O
all	O
the	O
time	O
steps	O
.	O

Moreover	O
,	O
batch	B-Method
normalization	E-Method
was	O
adopted	O
exploiting	O
the	O
method	O
suggested	O
in	O
,	O
as	O
discussed	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

The	O
feed	O
-	O
forward	O
connections	O
of	O
the	O
architecture	O
were	O
initialized	O
according	O
to	O
the	O
Glorot	B-Method
’s	I-Method
scheme	E-Method
,	O
while	O
recurrent	O
weights	O
were	O
initialized	O
with	O
orthogonal	O
matrices	O
.	O

Similarly	O
to	O
,	O
the	O
gain	O
factor	O
of	O
batch	B-Method
normalization	E-Method
was	O
initialized	O
to	O
0.1	O
and	O
the	O
shift	O
parameter	O
was	O
initialized	O
to	O
0	O
.	O

Before	O
training	O
,	O
the	O
sentences	O
were	O
sorted	O
in	O
ascending	O
order	O
according	O
to	O
their	O
lengths	O
and	O
,	O
starting	O
from	O
the	O
shortest	O
utterance	O
,	O
minibatches	O
of	O
8	O
sentences	O
were	O
progressively	O
processed	O
by	O
the	O
training	B-Method
algorithm	E-Method
.	O

This	O
sorting	B-Method
approach	E-Method
minimizes	O
the	O
need	O
of	O
zero	O
-	O
paddings	O
when	O
forming	O
mini	O
-	O
batches	O
,	O
resulting	O
helpful	O
to	O
avoid	O
possible	O
biases	O
on	O
batch	O
normalization	O
statistics	O
.	O

Moreover	O
,	O
the	O
sorting	B-Method
approach	E-Method
exploits	O
a	O
curriculum	B-Method
learning	I-Method
strategy	E-Method
that	O
has	O
been	O
shown	O
to	O
slightly	O
improve	O
the	O
performance	O
and	O
to	O
ensure	O
numerical	O
stability	O
of	O
gradients	O
.	O

The	O
optimization	S-Task
was	O
done	O
using	O
the	O
Adaptive	B-Method
Moment	I-Method
Estimation	E-Method
(	O
Adam	S-Method
)	O
algorithm	O
running	O
for	O
22	O
epochs	O
(	O
35	O
for	O
the	O
TED	B-Material
-	I-Material
talk	I-Material
corpus	E-Material
)	O
with	O
,	O
,	O
.	O

The	O
performance	O
on	O
the	O
development	O
set	O
was	O
monitored	O
after	O
each	O
epoch	O
,	O
while	O
the	O
learning	B-Metric
rate	E-Metric
was	O
halved	O
when	O
the	O
performance	O
improvement	O
went	O
below	O
a	O
certain	O
threshold	O
(	O
)	O
.	O

Gradient	B-Method
truncation	E-Method
was	O
not	O
applied	O
,	O
allowing	O
the	O
system	O
to	O
learn	O
arbitrarily	O
long	O
time	O
dependencies	O
.	O

The	O
main	O
hyperparameters	O
of	O
the	O
model	O
(	O
i.e.	O
,	O
learning	B-Metric
rate	E-Metric
,	O
number	O
of	O
hidden	O
layers	O
,	O
hidden	O
neurons	O
per	O
layer	O
,	O
dropout	O
factor	O
)	O
were	O
optimized	O
on	O
the	O
development	O
data	O
.	O

In	O
particular	O
,	O
we	O
guessed	O
some	O
initial	O
values	O
according	O
to	O
our	O
experience	O
,	O
and	O
starting	O
from	O
them	O
we	O
performed	O
a	O
grid	B-Method
search	E-Method
to	O
progressively	O
explore	O
better	O
configurations	O
.	O

A	O
total	O
of	O
20	O
-	O
25	O
experiments	O
were	O
conducted	O
for	O
all	O
the	O
various	O
RNN	S-Method
models	O
.	O

subsection	O
:	O
DNN	S-Method
-	O
HMM	O
setup	O
In	O
the	O
DNN	S-Method
-	O
HMM	O
experiments	O
,	O
the	O
DNN	S-Method
is	O
trained	O
to	O
predict	O
context	O
-	O
dependent	O
phone	O
targets	O
.	O

The	O
feature	B-Method
extraction	E-Method
is	O
based	O
on	O
blocking	O
the	O
signal	O
into	O
frames	O
of	O
25	O
ms	O
with	O
an	O
overlap	O
of	O
10	O
ms	O
.	O

The	O
experimental	O
activity	O
is	O
conducted	O
considering	O
different	O
acoustic	O
features	O
,	O
i.e.	O
,	O
39	O
MFCCs	S-Method
(	O
13	O
static	O
+	O
+	O
)	O
,	O
40	O
log	B-Method
-	I-Method
mel	I-Method
filter	I-Method
-	I-Method
bank	I-Method
features	E-Method
(	O
FBANKS	S-Method
)	O
,	O
as	O
well	O
as	O
40	O
fMLLR	B-Method
features	E-Method
(	O
extracted	O
as	O
reported	O
in	O
the	O
s5	O
recipe	O
of	O
Kaldi	S-Material
)	O
.	O

The	O
labels	O
were	O
derived	O
by	O
performing	O
a	O
forced	B-Method
alignment	I-Method
procedure	E-Method
on	O
the	O
original	O
training	O
datasets	O
.	O

See	O
the	O
standard	O
s5	B-Method
recipe	E-Method
of	O
Kaldi	S-Method
for	O
more	O
details	O
.	O

During	O
test	O
,	O
the	O
posterior	O
probabilities	O
generated	O
for	O
each	O
frame	O
by	O
the	O
RNN	S-Method
are	O
normalized	O
by	O
their	O
prior	O
probabilities	O
.	O

The	O
obtained	O
likelihoods	O
are	O
processed	O
by	O
an	O
HMM	B-Method
-	I-Method
based	I-Method
decoder	E-Method
,	O
that	O
,	O
after	O
integrating	O
the	O
acoustic	O
,	O
lexicon	O
and	O
language	O
model	O
information	O
in	O
a	O
single	O
search	B-Method
graph	E-Method
,	O
finally	O
estimates	O
the	O
sequence	O
of	O
words	O
uttered	O
by	O
the	O
speaker	O
.	O

The	O
RNN	S-Method
part	O
of	O
the	O
ASR	S-Task
system	O
was	O
implemented	O
with	O
Theano	O
,	O
that	O
was	O
coupled	O
with	O
the	O
Kaldi	B-Method
decoder	E-Method
to	O
form	O
a	O
context	O
-	O
dependent	O
RNN	S-Method
-	O
HMM	O
speech	O
recognizer	O
.	O

subsection	O
:	O
CTC	B-Method
setup	E-Method
The	O
models	O
used	O
for	O
the	O
CTC	B-Task
experiments	E-Task
consisted	O
of	O
5	O
layers	O
of	O
bidirectional	O
RNNs	S-Method
of	O
either	O
250	O
or	O
465	O
units	O
.	O

Unlike	O
in	O
the	O
other	O
experiments	O
,	O
weight	O
noise	O
was	O
used	O
for	O
regularization	S-Task
.	O

The	O
application	O
of	O
weight	O
noise	O
is	O
a	O
simplification	O
of	O
adaptive	B-Method
weight	I-Method
noise	E-Method
and	O
has	O
been	O
successfully	O
used	O
before	O
to	O
regularize	B-Method
CTC	I-Method
-	I-Method
LSTM	I-Method
models	E-Method
.	O

The	O
weight	O
noise	O
was	O
applied	O
to	O
all	O
the	O
weight	O
matrices	O
and	O
sampled	O
from	O
a	O
zero	B-Method
mean	I-Method
normal	I-Method
distribution	E-Method
with	O
a	O
standard	O
deviation	O
of	O
.	O

Batch	B-Method
normalization	E-Method
was	O
used	O
with	O
the	O
same	O
initialization	O
settings	O
as	O
in	O
the	O
other	O
experiments	O
.	O

Glorot	B-Method
’s	I-Method
scheme	E-Method
was	O
used	O
to	O
initialize	O
all	O
the	O
weights	O
(	O
also	O
the	O
recurrent	O
ones	O
)	O
.	O

The	O
input	O
features	O
for	O
these	O
experiments	O
were	O
123	O
dimensional	B-Method
FBANK	I-Method
features	E-Method
(	O
40	O
+	O
energy	O
+	O
+	O
)	O
.	O

These	O
features	O
were	O
also	O
used	O
in	O
the	O
original	O
work	O
on	O
CTC	B-Method
-	I-Method
LSTM	I-Method
models	E-Method
for	O
speech	B-Task
recognition	E-Task
.	O

The	O
CTC	B-Method
layer	E-Method
itself	O
was	O
trained	O
on	O
the	O
61	O
label	O
set	O
.	O

Decoding	S-Task
was	O
done	O
using	O
the	O
best	B-Method
-	I-Method
path	I-Method
method	E-Method
,	O
without	O
adding	O
any	O
external	B-Method
phone	I-Method
-	I-Method
based	I-Method
language	I-Method
model	E-Method
.	O

After	O
decoding	O
,	O
the	O
labels	O
were	O
mapped	O
to	O
the	O
final	O
39	O
label	O
set	O
.	O

The	O
models	O
were	O
trained	O
for	O
50	O
epochs	O
using	O
batches	O
of	O
8	O
utterances	O
.	O

section	O
:	O
Results	O
In	O
the	O
following	O
sub	O
-	O
sections	O
,	O
we	O
describe	O
the	O
experimental	O
activity	O
conducted	O
to	O
assess	O
the	O
proposed	O
model	O
.	O

Most	O
of	O
the	O
experiments	O
reported	O
in	O
the	O
following	O
are	O
based	O
on	O
hybrid	O
DNN	S-Method
-	O
HMM	O
speech	O
recognizers	O
,	O
since	O
the	O
latter	O
ASR	S-Task
paradigm	O
typically	O
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

However	O
,	O
for	O
the	O
sake	O
of	O
comparison	O
,	O
we	O
also	O
extended	O
the	O
experimental	O
validation	O
to	O
an	O
end	O
-	O
to	O
-	O
end	O
CTC	B-Method
model	E-Method
.	O

More	O
precisely	O
,	O
in	O
sub	O
-	O
section	O
[	O
reference	O
]	O
,	O
we	O
first	O
quantitatively	O
analyze	O
the	O
correlations	O
between	O
the	O
update	O
and	O
reset	O
gates	O
in	O
a	O
standard	O
GRU	S-Method
.	O

In	O
sub	O
-	O
section	O
[	O
reference	O
]	O
,	O
we	O
extend	O
our	O
study	O
with	O
some	O
analysis	O
of	O
gradient	B-Method
statistics	E-Method
.	O

The	O
role	O
of	O
batch	B-Method
normalization	E-Method
and	O
the	O
CTC	B-Method
experiments	E-Method
are	O
described	O
in	O
sub	O
-	O
sections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
respectively	O
.	O

The	O
speech	B-Task
recognition	E-Task
performance	O
will	O
then	O
be	O
reported	O
for	O
TIMIT	S-Material
,	O
DIRHA	B-Material
-	I-Material
English	E-Material
,	O
CHiME	S-Material
as	O
well	O
as	O
for	O
the	O
TED	B-Material
-	I-Material
talk	I-Material
corpus	E-Material
in	O
sub	O
-	O
sections	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
,	O
respectively	O
.	O

The	O
computational	S-Metric
benefits	O
of	O
Li	B-Method
-	I-Method
GRU	E-Method
are	O
finally	O
discussed	O
in	O
sub	O
-	O
section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Correlation	B-Method
analysis	E-Method
The	O
correlation	O
between	O
update	O
and	O
reset	O
gates	O
have	O
been	O
preliminarily	O
discussed	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

In	O
this	O
sub	O
-	O
section	O
,	O
we	O
take	O
a	O
step	O
further	O
by	O
analyzing	O
it	O
in	O
a	O
quantitative	O
way	O
using	O
the	O
cross	O
-	O
correlation	O
function	O
defined	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

In	O
particular	O
,	O
the	O
cross	B-Metric
-	I-Metric
correlation	E-Metric
between	O
the	O
average	O
activations	O
of	O
update	O
and	O
reset	O
gates	O
is	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

The	O
gate	O
activations	O
are	O
computed	O
for	O
all	O
the	O
input	O
frames	O
and	O
,	O
at	O
each	O
time	O
step	O
,	O
an	O
average	O
over	O
the	O
hidden	O
neurons	O
is	O
considered	O
.	O

The	O
cross	B-Metric
-	I-Metric
correlation	E-Metric
is	O
displayed	O
along	O
with	O
the	O
auto	B-Metric
-	I-Metric
correlation	E-Metric
,	O
that	O
represents	O
the	O
upper	O
-	O
bound	O
limit	O
of	O
the	O
former	O
function	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
clearly	O
shows	O
a	O
high	O
peak	O
of	O
,	O
revealing	O
that	O
update	O
and	O
reset	O
gates	O
end	O
up	O
being	O
redundant	O
.	O

This	O
peak	O
is	O
about	O
66	O
%	O
of	O
the	O
maximum	O
of	O
and	O
it	O
is	O
centered	O
at	O
,	O
indicating	O
that	O
almost	O
no	O
-	O
delay	O
occurs	O
between	O
gate	O
activations	O
.	O

This	O
result	O
is	O
obtained	O
with	O
a	O
single	O
-	O
layer	O
GRU	S-Method
of	O
200	O
bidirectional	O
neurons	O
fed	O
with	O
MFCC	O
features	O
and	O
trained	O
with	O
TIMIT	S-Material
.	O

After	O
the	O
training	O
-	O
step	O
,	O
the	O
cross	B-Metric
-	I-Metric
correlation	E-Metric
is	O
averaged	O
over	O
all	O
the	O
development	O
sentences	O
.	O

It	O
would	O
be	O
of	O
interest	O
to	O
examine	O
the	O
evolution	O
of	O
this	O
correlation	O
over	O
the	O
epochs	O
.	O

With	O
this	O
regard	O
,	O
Fig	O
.	O

[	O
reference	O
]	O
reports	O
the	O
peak	O
of	O
for	O
some	O
training	O
epochs	O
,	O
showing	O
that	O
the	O
GRU	S-Method
attributes	O
rather	O
quickly	O
a	O
similar	O
role	O
to	O
update	O
and	O
reset	O
gates	O
.	O

In	O
fact	O
,	O
after	O
3	O
-	O
4	O
epochs	O
,	O
the	O
correlation	O
peak	O
reaches	O
its	O
maximum	O
value	O
,	O
that	O
is	O
almost	O
maintained	O
for	O
all	O
the	O
subsequent	O
training	O
iterations	O
.	O

subsection	O
:	O
Gradient	B-Method
analysis	E-Method
The	O
analysis	O
of	O
the	O
main	O
gradient	B-Metric
statistics	E-Metric
can	O
give	O
some	O
preliminary	O
indications	O
on	O
the	O
role	O
played	O
by	O
the	O
various	O
parameters	O
.	O

With	O
this	O
goal	O
,	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
L2	B-Metric
norm	E-Metric
of	O
the	O
gradient	O
for	O
the	O
main	O
parameters	O
of	O
the	O
considered	O
GRU	S-Method
models	O
.	O

Results	O
reveal	O
that	O
the	O
reset	O
gate	O
weight	O
matrices	O
(	O
i.e.	O
,	O
and	O
)	O
have	O
a	O
smaller	O
gradient	B-Metric
norm	E-Metric
when	O
compared	O
to	O
the	O
other	O
parameters	O
.	O

This	O
result	O
is	O
somewhat	O
expected	O
,	O
since	O
the	O
reset	O
gate	O
parameters	O
are	O
processed	O
by	O
two	O
different	O
non	O
-	O
linearities	O
(	O
i.e.	O
,	O
the	O
sigmoid	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
and	O
the	O
tanh	S-Method
of	O
[	O
reference	O
]	O
)	O
,	O
that	O
can	O
attenuate	O
their	O
gradients	O
.	O

This	O
would	O
anyway	O
indicate	O
that	O
,	O
on	O
average	O
,	O
the	O
reset	O
gate	O
has	O
less	O
impact	O
on	O
the	O
final	O
cost	B-Metric
function	E-Metric
,	O
further	O
supporting	O
its	O
removal	O
from	O
the	O
GRU	S-Method
design	O
.	O

When	O
avoiding	O
it	O
,	O
the	O
norm	O
of	O
the	O
gradient	O
tends	O
to	O
increase	O
(	O
see	O
for	O
instance	O
the	O
recurrent	B-Method
weights	I-Method
of	I-Method
M	I-Method
-	I-Method
GRU	I-Method
model	E-Method
)	O
.	O

This	O
suggests	O
that	O
the	O
functionalities	O
of	O
the	O
reset	O
gate	O
,	O
can	O
be	O
performed	O
by	O
other	O
model	B-Method
parameters	E-Method
.	O

The	O
norm	O
further	O
increases	O
in	O
the	O
case	O
of	O
Li	O
-	O
GRUs	S-Method
,	O
due	O
to	O
the	O
adoption	O
of	O
ReLu	O
units	O
.	O

This	O
non	O
-	O
linearity	O
,	O
indeed	O
,	O
improves	O
the	O
back	B-Method
-	I-Method
propagation	E-Method
of	O
the	O
gradient	O
over	O
both	O
time	O
-	O
steps	O
and	O
hidden	O
layers	O
,	O
making	O
long	O
-	O
term	O
dependencies	O
easier	O
to	O
learn	O
.	O

Results	O
are	O
obtained	O
with	O
the	O
same	O
GRU	S-Method
used	O
in	O
subsection	O
[	O
reference	O
]	O
,	O
considering	O
M	B-Method
-	I-Method
GRU	E-Method
and	O
Li	B-Method
-	I-Method
GRU	I-Method
models	E-Method
with	O
the	O
same	O
number	O
of	O
hidden	O
units	O
.	O

subsection	O
:	O
Role	O
of	O
batch	B-Method
normalization	E-Method
After	O
the	O
preliminary	O
analysis	O
on	O
correlation	S-Metric
and	O
gradients	O
done	O
in	O
previous	O
sub	O
-	O
sections	O
,	O
we	O
now	O
compare	O
RNN	S-Method
models	O
in	O
terms	O
of	O
their	O
final	O
speech	B-Task
recognition	E-Task
performance	O
.	O

To	O
highlight	O
the	O
importance	O
of	O
batch	B-Method
normalization	E-Method
,	O
Table	O
[	O
reference	O
]	O
compares	O
the	O
Phone	B-Metric
Error	I-Metric
Rate	E-Metric
(	O
PER%	S-Metric
)	O
achieved	O
with	O
and	O
without	O
this	O
technique	O
.	O

Results	O
show	O
that	O
batch	B-Method
normalization	E-Method
is	O
helpful	O
to	O
improve	O
the	O
ASR	S-Task
performance	O
,	O
leading	O
to	O
a	O
relative	O
improvement	O
of	O
about	O
7	O
%	O
for	O
GRU	S-Method
and	O
M	O
-	O
GRU	S-Method
and	O
18	O
%	O
for	O
the	O
proposed	O
Li	B-Method
-	I-Method
GRU	E-Method
.	O

The	O
latter	O
improvement	O
confirms	O
that	O
our	O
model	O
couples	O
particularly	O
well	O
with	O
this	O
technique	O
,	O
due	O
to	O
the	O
adopted	O
ReLU	S-Method
activations	O
.	O

Without	O
batch	B-Method
normalization	E-Method
,	O
the	O
ReLU	S-Method
activations	O
of	O
the	O
Li	B-Method
-	I-Method
GRU	E-Method
are	O
unbounded	O
and	O
tend	O
to	O
cause	O
numerical	O
instabilities	O
.	O

According	O
to	O
our	O
experience	O
,	O
the	O
convergence	O
of	O
Li	B-Method
-	I-Method
GRU	E-Method
without	O
batch	B-Method
normalization	E-Method
,	O
can	O
be	O
achieved	O
only	O
by	O
setting	O
rather	O
small	O
learning	O
rate	O
values	O
.	O

The	O
latter	O
setting	O
,	O
however	O
,	O
can	O
lead	O
to	O
a	O
poor	O
performance	O
and	O
,	O
as	O
clearly	O
emerged	O
from	O
this	O
experiment	O
,	O
coupling	O
Li	B-Method
-	I-Method
GRU	E-Method
with	O
this	O
technique	O
is	O
strongly	O
recommended	O
.	O

subsection	O
:	O
CTC	O
results	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
results	O
of	O
CTC	S-Method
on	O
the	O
TIMIT	S-Material
data	O
set	O
.	O

In	O
these	O
experiments	O
,	O
the	O
Li	B-Method
-	I-Method
GRU	E-Method
clearly	O
outperforms	O
the	O
standard	O
GRU	S-Method
,	O
showing	O
the	O
effectiveness	O
of	O
the	O
proposed	O
model	O
even	O
in	O
a	O
end	O
-	O
to	O
-	O
end	O
ASR	S-Task
framework	O
.	O

The	O
improvement	O
is	O
obtained	O
both	O
with	O
and	O
without	O
batch	B-Method
normalization	E-Method
and	O
,	O
similarly	O
to	O
what	O
observed	O
for	O
hybrid	B-Method
systems	E-Method
,	O
the	O
latter	O
technique	O
leads	O
to	O
better	O
performance	O
when	O
coupled	O
with	O
Li	B-Method
-	I-Method
GRU	E-Method
.	O

However	O
,	O
a	O
smaller	O
performance	O
gain	O
is	O
observed	O
when	O
batch	B-Method
normalization	E-Method
is	O
applied	O
to	O
the	O
CTC	S-Method
.	O

This	O
result	O
could	O
also	O
be	O
related	O
to	O
the	O
different	O
choice	O
of	O
the	O
regularizer	S-Method
,	O
as	O
weight	O
noise	O
was	O
used	O
instead	O
of	O
recurrent	B-Method
dropout	E-Method
.	O

In	O
general	O
,	O
PERs	O
are	O
higher	O
than	O
those	O
of	O
the	O
hybrid	B-Method
systems	E-Method
.	O

End	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
methods	E-Method
,	O
in	O
fact	O
,	O
are	O
relatively	O
young	O
models	O
,	O
that	O
are	O
currently	O
still	O
less	O
competitive	O
than	O
more	O
complex	O
(	O
and	O
mature	O
)	O
DNN	S-Method
-	O
HMM	O
approaches	O
.	O

We	O
believe	O
that	O
the	O
gap	O
between	O
CTC	B-Method
and	I-Method
hybrid	I-Method
speech	I-Method
recognizers	E-Method
could	O
be	O
partially	O
reduced	O
in	O
our	O
experiments	O
with	O
a	O
more	O
careful	O
setting	O
of	O
the	O
hyperparameters	O
and	O
with	O
the	O
introduction	O
of	O
an	O
external	O
phone	B-Method
-	I-Method
based	I-Method
language	I-Method
model	E-Method
.	O

The	O
main	O
focus	O
of	O
the	O
paper	O
,	O
however	O
,	O
is	O
to	O
show	O
the	O
effectiveness	O
of	O
the	O
proposed	O
Li	O
-	O
GRU	S-Method
model	O
,	O
and	O
a	O
fair	O
comparison	O
between	O
CTC	S-Method
and	O
hybrid	B-Method
systems	E-Method
is	O
out	O
of	O
the	O
scope	O
of	O
this	O
work	O
.	O

subsection	O
:	O
Other	O
results	O
on	O
TIMIT	S-Material
The	O
results	O
of	O
Table	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
highlighted	O
that	O
the	O
proposed	O
Li	O
-	O
GRU	S-Method
model	O
outperforms	O
other	O
GRU	S-Method
architectures	O
.	O

In	O
this	O
sub	O
-	O
section	O
,	O
we	O
extend	O
this	O
study	O
by	O
performing	O
a	O
more	O
detailed	O
comparison	O
with	O
the	O
most	O
popular	O
RNN	S-Method
architectures	O
.	O

To	O
provide	O
a	O
fair	O
comparison	O
,	O
batch	B-Method
normalization	E-Method
is	O
hereinafter	O
applied	O
to	O
all	O
the	O
considered	O
RNN	S-Method
models	O
.	O

Moreover	O
,	O
at	O
least	O
five	O
experiments	O
varying	O
the	O
initialization	O
seeds	O
were	O
conducted	O
for	O
each	O
RNN	S-Method
architecture	O
.	O

The	O
results	O
are	O
thus	O
reported	O
as	O
the	O
average	O
PER	S-Metric
with	O
their	O
corresponding	O
standard	O
deviation	O
.	O

Table	O
[	O
reference	O
]	O
presents	O
the	O
ASR	S-Task
performance	O
obtained	O
with	O
the	O
TIMIT	S-Material
dataset	O
.	O

The	O
first	O
row	O
reports	O
the	O
results	O
achieved	O
with	O
a	O
simple	O
RNN	S-Method
with	O
ReLU	S-Method
activations	O
(	O
no	O
gating	B-Method
mechanisms	E-Method
are	O
used	O
here	O
)	O
.	O

Although	O
this	O
architecture	O
has	O
recently	O
shown	O
promising	O
results	O
in	O
some	O
machine	B-Task
learning	I-Task
tasks	E-Task
,	O
our	O
results	O
confirm	O
that	O
gated	B-Method
recurrent	I-Method
networks	E-Method
(	O
rows	O
2	O
-	O
5	O
)	O
outperform	O
traditional	O
RNNs	S-Method
.	O

We	O
also	O
observe	O
that	O
GRUs	S-Method
tend	O
to	O
slightly	O
outperform	O
the	O
LSTM	B-Method
model	E-Method
.	O

As	O
expected	O
,	O
M	B-Method
-	I-Method
GRU	E-Method
(	O
i.e.	O
,	O
the	O
architecture	O
without	O
reset	O
gate	O
)	O
achieves	O
a	O
performance	O
very	O
similar	O
to	O
that	O
obtained	O
with	O
standard	O
GRUs	S-Method
,	O
further	O
supporting	O
our	O
speculation	O
on	O
the	O
redundant	O
role	O
played	O
by	O
the	O
reset	O
gate	O
in	O
a	O
speech	B-Task
recognition	I-Task
application	E-Task
.	O

The	O
last	O
row	O
reports	O
the	O
performance	O
achieved	O
with	O
the	O
proposed	O
model	O
,	O
in	O
which	O
,	O
besides	O
removing	O
the	O
reset	O
gate	O
,	O
ReLU	S-Method
activations	O
are	O
used	O
.	O

The	O
Li	B-Method
-	I-Method
GRU	E-Method
performance	O
indicates	O
that	O
our	O
architecture	O
consistently	O
outperforms	O
the	O
other	O
RNNs	S-Method
over	O
all	O
the	O
considered	O
input	O
features	O
.	O

A	O
remarkable	O
achievement	O
is	O
the	O
average	O
PER	S-Metric
(	O
%	O
)	O
of	O
%	O
obtained	O
with	O
fMLLR	B-Method
features	E-Method
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
result	O
yields	O
the	O
best	O
published	O
performance	O
on	O
the	O
TIMIT	S-Material
test	O
-	O
set	O
.	O

In	O
Table	O
[	O
reference	O
]	O
the	O
PER	S-Metric
(	O
%	O
)	O
performance	O
is	O
split	O
into	O
five	O
different	O
phonetic	O
categories	O
(	O
vowels	O
,	O
liquids	O
,	O
nasals	O
,	O
fricatives	O
and	O
stops	O
)	O
,	O
showing	O
that	O
Li	B-Method
-	I-Method
GRU	E-Method
exhibits	O
the	O
best	O
results	O
for	O
all	O
the	O
considered	O
classes	O
.	O

Previous	O
results	O
are	O
obtained	O
after	O
optimizing	O
the	O
main	O
hyperparameters	S-Method
of	O
the	O
model	O
on	O
the	O
development	O
set	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
outcome	O
of	O
this	O
optimization	B-Method
process	E-Method
,	O
with	O
the	O
corresponding	O
best	O
architectures	O
obtained	O
for	O
each	O
RNN	S-Method
architecture	O
.	O

For	O
GRU	S-Method
models	O
,	O
the	O
best	O
performance	O
is	O
achieved	O
with	O
5	O
hidden	O
layers	O
of	O
465	O
neurons	O
.	O

It	O
is	O
also	O
worth	O
noting	O
that	O
M	B-Method
-	I-Method
GRU	E-Method
and	O
Li	B-Method
-	I-Method
GRU	E-Method
have	O
about	O
30	O
%	O
fewer	O
parameters	O
compared	O
to	O
the	O
standard	O
GRU	S-Method
.	O

subsection	O
:	O
Recognition	S-Task
performance	O
on	O
DIRHA	B-Material
English	I-Material
WSJ	E-Material
After	O
a	O
first	O
set	O
of	O
experiments	O
on	O
TIMIT	S-Material
,	O
in	O
this	O
sub	O
-	O
section	O
we	O
assess	O
our	O
model	O
on	O
a	O
more	O
challenging	O
and	O
realistic	O
distant	B-Task
-	I-Task
talking	I-Task
task	E-Task
,	O
using	O
the	O
DIRHA	B-Material
English	I-Material
WSJ	I-Material
corpus	E-Material
.	O

A	O
challenging	O
aspect	O
of	O
this	O
dataset	O
is	O
the	O
acoustic	O
mismatch	O
between	O
training	O
and	O
testing	O
conditions	O
.	O

Training	S-Task
,	O
in	O
fact	O
,	O
is	O
performed	O
with	O
a	O
reverberated	B-Material
version	I-Material
of	I-Material
WSJ	E-Material
,	O
while	O
test	O
is	O
characterized	O
by	O
both	O
non	O
-	O
stationary	O
noises	O
and	O
reverberation	O
.	O

Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
summarize	O
the	O
results	O
obtained	O
with	O
the	O
simulated	O
and	O
real	O
parts	O
of	O
this	O
dataset	O
.	O

These	O
results	O
exhibit	O
a	O
trend	O
comparable	O
to	O
that	O
observed	O
for	O
TIMIT	S-Material
,	O
confirming	O
that	O
Li	B-Method
-	I-Method
GRU	E-Method
still	O
outperform	O
GRU	S-Method
even	O
in	O
a	O
more	O
challenging	O
scenario	O
.	O

The	O
results	O
are	O
consistent	O
over	O
both	O
real	O
and	O
simulated	O
data	O
as	O
well	O
as	O
across	O
the	O
different	O
features	O
considered	O
in	O
this	O
study	O
.	O

The	O
reset	B-Method
gate	I-Method
removal	E-Method
seems	O
to	O
play	O
a	O
more	O
crucial	O
role	O
in	O
the	O
addressed	O
distant	B-Task
-	I-Task
talking	I-Task
scenario	E-Task
.	O

If	O
the	O
close	O
-	O
talking	O
performance	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
highlights	O
comparable	O
error	B-Metric
rates	E-Metric
between	O
standard	O
GRU	S-Method
and	O
M	B-Method
-	I-Method
GRU	E-Method
,	O
in	O
the	O
distant	B-Task
-	I-Task
talking	I-Task
case	E-Task
we	O
even	O
observe	O
a	O
small	O
performance	O
gain	O
when	O
removing	O
the	O
reset	O
gate	O
.	O

We	O
suppose	O
that	O
this	O
behaviour	O
is	O
due	O
to	O
reverberation	O
,	O
that	O
implicitly	O
introduces	O
redundancy	O
in	O
the	O
signal	O
,	O
due	O
to	O
the	O
multiple	O
delayed	O
replicas	O
of	O
each	O
sample	O
.	O

This	O
results	O
in	O
a	O
forward	O
memory	O
effect	O
,	O
that	O
can	O
make	O
reset	O
gate	O
ineffective	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
extend	O
our	O
previous	O
experiments	O
by	O
generating	O
simulated	O
data	O
with	O
different	O
reverberation	O
times	O
ranging	O
from	O
250	O
to	O
1000	O
ms	O
,	O
as	O
outlined	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

In	O
order	O
to	O
simulate	O
a	O
more	O
realistic	O
situation	O
,	O
different	O
impulse	O
responses	O
have	O
been	O
used	O
for	O
training	O
and	O
testing	O
purposes	O
.	O

No	O
additive	O
noise	O
is	O
considered	O
for	O
these	O
experiments	O
.	O

As	O
expected	O
,	O
the	O
performance	O
degrades	O
as	O
the	O
reverberation	O
time	O
increases	O
.	O

Similarly	O
to	O
previous	O
achievements	O
,	O
we	O
still	O
observe	O
that	O
Li	B-Method
-	I-Method
GRU	E-Method
outperform	O
GRU	S-Method
under	O
all	O
the	O
considered	O
reverberation	O
conditions	O
.	O

subsection	O
:	O
Recognition	S-Task
performance	O
on	O
CHiME	S-Material
In	O
this	O
sub	O
-	O
section	O
we	O
extend	O
the	O
results	O
to	O
the	O
CHiME	S-Material
corpus	O
,	O
that	O
is	O
an	O
important	O
benchmark	O
in	O
the	O
ASR	B-Task
field	E-Task
,	O
thanks	O
to	O
the	O
success	O
of	O
CHiME	S-Material
challenges	O
.	O

In	O
Tab	O
.	O

[	O
reference	O
]	O
a	O
comparison	O
across	O
the	O
various	O
GRU	S-Method
architectures	O
is	O
presented	O
.	O

For	O
the	O
sake	O
of	O
comparison	O
,	O
the	O
results	O
obtained	O
with	O
the	O
official	O
CHiME	S-Material
4	O
are	O
also	O
reported	O
in	O
the	O
first	O
two	O
rows	O
.	O

Results	O
confirm	O
the	O
trend	O
previously	O
observed	O
,	O
highlighting	O
a	O
significant	O
relative	O
improvement	O
of	O
about	O
14	O
%	O
achieved	O
when	O
passing	O
from	O
GRU	S-Method
to	O
the	O
proposed	O
Li	B-Method
-	I-Method
GRU	E-Method
.	O

Similarly	O
to	O
our	O
findings	O
of	O
the	O
previous	O
section	O
,	O
some	O
small	O
benefits	O
can	O
be	O
observed	O
when	O
removing	O
the	O
reset	O
gate	O
.	O

The	O
largest	O
performance	O
gap	O
,	O
however	O
,	O
is	O
reached	O
when	O
adopting	O
ReLU	S-Method
units	O
(	O
see	O
M	O
-	O
GRU	S-Method
and	O
Li	B-Method
-	I-Method
GRU	E-Method
columns	O
)	O
,	O
confirming	O
the	O
effectiveness	O
of	O
this	O
architectural	B-Method
variation	E-Method
.	O

Note	O
also	O
that	O
the	O
GRU	S-Method
systems	O
significantly	O
outperform	O
the	O
DNN	S-Method
baseline	O
,	O
even	O
when	O
the	O
latter	O
is	O
based	O
on	O
sequence	B-Method
discriminative	I-Method
training	E-Method
(	O
DNN	S-Method
+	O
sMBR	O
)	O
.	O

Tab	O
.	O

[	O
reference	O
]	O
splits	O
the	O
ASR	S-Task
performance	O
of	O
the	O
real	O
test	O
set	O
into	O
the	O
four	O
noisy	O
categories	O
.	O

Li	B-Method
-	I-Method
GRU	E-Method
outperforms	O
GRU	S-Method
in	O
all	O
the	O
considered	O
environments	O
,	O
with	O
a	O
performance	O
gain	O
that	O
is	O
higher	O
when	O
more	O
challenging	O
acoustic	O
conditions	O
are	O
met	O
.	O

For	O
instance	O
,	O
we	O
obtain	O
a	O
relative	O
improvement	O
of	O
16	O
%	O
in	O
the	O
bus	O
(	O
BUS	O
)	O
environment	O
(	O
the	O
noisiest	O
)	O
,	O
against	O
the	O
relative	O
improvement	O
of	O
9.5	O
%	O
observed	O
in	O
the	O
street	O
(	O
STR	O
)	O
recordings	O
.	O

subsection	O
:	O
Recognition	S-Task
performance	O
on	O
TED	B-Material
-	I-Material
talks	E-Material
Tab	O
.	O

[	O
reference	O
]	O
reports	O
a	O
comparison	O
between	O
GRU	S-Method
and	O
Li	B-Method
-	I-Method
GRU	E-Method
on	O
the	O
TED	B-Material
-	I-Material
talks	I-Material
corpus	E-Material
.	O

The	O
experiments	O
are	O
performed	O
with	O
standard	O
MFCC	O
features	O
,	O
and	O
a	O
four	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
is	O
considered	O
in	O
the	O
decoding	B-Task
step	E-Task
(	O
see	O
for	O
more	O
details	O
)	O
.	O

Results	O
on	O
both	O
test	O
sets	O
consistently	O
shows	O
the	O
performance	O
gain	O
achieved	O
with	O
the	O
proposed	O
architecture	O
.	O

This	O
further	O
confirms	O
the	O
effectiveness	O
of	O
Li	B-Method
-	I-Method
GRU	E-Method
,	O
even	O
for	O
a	O
larger	O
scale	O
ASR	S-Task
task	O
.	O

In	O
particular	O
,	O
a	O
relative	O
improvement	O
of	O
about	O
14	O
-	O
17	O
%	O
is	O
achieved	O
.	O

This	O
improvement	O
is	O
statistically	O
significant	O
according	O
to	O
the	O
Matched	B-Method
Pairs	I-Method
Sentence	I-Method
Segment	I-Method
Word	I-Method
Error	I-Method
Test	E-Method
(	O
MPSSW	S-Method
)	O
,	O
that	O
is	O
conducted	O
with	O
NIST	B-Method
sclite	I-Method
t	I-Method
tool	E-Method
with	O
a	O
p	B-Metric
-	I-Metric
value	E-Metric
of	O
.	O

subsection	O
:	O
Training	B-Metric
time	I-Metric
comparison	E-Metric
In	O
the	O
previous	O
subsections	O
,	O
we	O
reported	O
several	O
speech	B-Task
recognition	E-Task
results	O
,	O
showing	O
that	O
Li	B-Method
-	I-Method
GRU	E-Method
outperforms	O
other	O
RNNs	S-Method
.	O

In	O
this	O
sub	O
-	O
section	O
,	O
we	O
finally	O
focus	O
on	O
another	O
key	O
aspect	O
of	O
the	O
proposed	O
architecture	O
,	O
namely	O
its	O
improved	O
computational	B-Metric
efficiency	E-Metric
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
compare	O
the	O
per	B-Metric
-	I-Metric
epoch	I-Metric
wall	I-Metric
-	I-Metric
clock	I-Metric
training	I-Metric
time	E-Metric
of	O
GRU	S-Method
and	O
Li	B-Method
-	I-Method
GRU	I-Method
models	E-Method
.	O

The	O
training	B-Metric
time	I-Metric
reduction	E-Metric
achieved	O
with	O
the	O
proposed	O
architecture	O
is	O
about	O
30	O
%	O
for	O
all	O
the	O
datasets	O
.	O

This	O
reduction	O
reflects	O
the	O
amount	O
of	O
parameters	O
saved	O
by	O
Li	B-Method
-	I-Method
GRU	E-Method
,	O
that	O
is	O
also	O
around	O
30	O
%	O
.	O

The	O
reduction	O
of	O
the	O
computational	B-Metric
complexity	E-Metric
,	O
originated	O
by	O
a	O
more	O
compact	O
model	O
,	O
also	O
arises	O
for	O
testing	O
purposes	O
,	O
making	O
our	O
model	O
potentially	O
suitable	O
for	O
small	O
-	O
footprint	O
ASR	S-Task
,	O
,	O
which	O
studies	O
DNNs	S-Method
designed	O
for	O
portable	B-Task
devices	E-Task
with	O
small	O
computational	O
capabilities	O
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
revised	O
standard	O
GRUs	S-Method
for	O
speech	B-Task
recognition	I-Task
purposes	E-Task
.	O

The	O
proposed	O
Li	O
-	O
GRU	S-Method
architecture	O
is	O
a	O
simplified	O
version	O
of	O
a	O
standard	O
GRU	S-Method
,	O
in	O
which	O
the	O
reset	O
gate	O
is	O
removed	O
and	O
ReLU	S-Method
activations	O
are	O
considered	O
.	O

Batch	B-Method
normalization	E-Method
is	O
also	O
used	O
to	O
further	O
improve	O
the	O
system	O
performance	O
as	O
well	O
as	O
to	O
limit	O
the	O
numerical	O
instabilities	O
originated	O
from	O
ReLU	S-Method
non	O
-	O
linearities	O
.	O

The	O
experiments	O
,	O
conducted	O
on	O
different	O
ASR	S-Task
paradigms	O
,	O
tasks	O
,	O
features	O
and	O
environmental	O
conditions	O
,	O
have	O
confirmed	O
the	O
effectiveness	O
of	O
the	O
proposed	O
model	O
.	O

The	O
Li	B-Method
-	I-Method
GRU	E-Method
,	O
in	O
fact	O
,	O
not	O
only	O
yields	O
a	O
better	O
recognition	S-Task
performance	O
,	O
but	O
also	O
reduces	O
the	O
computational	B-Metric
complexity	E-Metric
,	O
with	O
a	O
reduction	O
of	O
more	O
than	O
30	O
%	O
of	O
the	O
training	B-Metric
time	E-Metric
over	O
a	O
standard	O
GRU	S-Method
.	O

Future	O
efforts	O
will	O
be	O
focused	O
on	O
extending	O
this	O
work	O
to	O
other	O
speech	B-Task
-	I-Task
based	I-Task
tasks	E-Task
,	O
such	O
as	O
speech	B-Task
enhancement	E-Task
and	O
speech	B-Task
separation	E-Task
as	O
well	O
as	O
to	O
explore	O
the	O
use	O
of	O
Li	B-Method
-	I-Method
GRU	E-Method
in	O
other	O
possible	O
fields	O
.	O

section	O
:	O
Acknowledgments	O
The	O
authors	O
would	O
like	O
to	O
thank	O
Piergiorgio	O
Svaizer	O
for	O
his	O
insightful	O
suggestions	O
on	O
an	O
earlier	O
version	O
of	O
this	O
paper	O
.	O

We	O
would	O
also	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
careful	O
reading	O
of	O
our	O
manuscript	O
and	O
their	O
helpful	O
comments	O
.	O

We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
a	O
Tesla	O
K40	O
GPU	O
used	O
for	O
this	O
research	O
.	O

Computations	O
were	O
also	O
made	O
on	O
the	O
Helios	O
supercomputer	O
from	O
the	O
University	O
of	O
Montreal	O
,	O
managed	O
by	O
Calcul	O
QuÃ	O
©	O
bec	O
and	O
Compute	O
Canada	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Text	B-Task
Understanding	E-Task
with	O
the	O
Attention	B-Method
Sum	I-Method
Reader	I-Method
Network	E-Method
black	O
Several	O
large	O
cloze	O
-	O
style	O
context	O
-	O
question	O
-	O
answer	O
datasets	O
have	O
been	O
introduced	O
recently	O
:	O
the	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
news	I-Material
data	E-Material
and	O
the	O
Children	B-Material
’s	I-Material
Book	I-Material
Test	E-Material
.	O

Thanks	O
to	O
the	O
size	O
of	O
these	O
datasets	O
,	O
the	O
associated	O
text	B-Task
comprehension	I-Task
task	E-Task
is	O
well	O
suited	O
for	O
deep	B-Method
-	I-Method
learning	I-Method
techniques	E-Method
that	O
currently	O
seem	O
to	O
outperform	O
all	O
alternative	O
approaches	O
.	O

We	O
present	O
a	O
new	O
,	O
simple	O
model	O
that	O
uses	O
attention	O
to	O
directly	O
pick	O
the	O
answer	O
from	O
the	O
context	O
as	O
opposed	O
to	O
computing	O
the	O
answer	O
using	O
a	O
blended	B-Method
representation	E-Method
of	O
words	O
in	O
the	O
document	O
as	O
is	O
usual	O
in	O
similar	O
models	O
.	O

This	O
makes	O
the	O
model	O
particularly	O
suitable	O
for	O
question	B-Task
-	I-Task
answering	I-Task
problems	E-Task
where	O
the	O
answer	O
is	O
a	O
single	O
word	O
from	O
the	O
document	O
.	O

blackEnsemble	O
of	O
our	O
models	O
sets	O
new	O
state	O
of	O
the	O
art	O
on	O
all	O
evaluated	O
datasets	O
.	O

MemNNMemNNMemoryNetwork	O
ptrnetPtr	O
-	O
NetPointerNetwork	O
psrASReaderAttentionSumReader	O
MenNNname	O
=	O
MemNN	O
,	O
description	O
=	O
isaprogrammablemachinethatreceivesinput	O
,	O
storesandmanipulatesdata	O
,	O
andprovidesoutputinausefulformat	O
section	O
:	O
Introduction	O
Most	O
of	O
the	O
information	O
humanity	O
has	O
gathered	O
up	O
to	O
this	O
point	O
is	O
stored	O
in	O
the	O
form	O
of	O
plain	O
text	O
.	O

Hence	O
the	O
task	O
of	O
teaching	B-Task
machines	E-Task
how	O
to	O
understand	O
this	O
data	O
is	O
of	O
utmost	O
importance	O
in	O
the	O
field	O
of	O
Artificial	B-Task
Intelligence	E-Task
.	O

One	O
way	O
of	O
testing	O
the	O
level	O
of	O
text	B-Task
understanding	E-Task
is	O
simply	O
to	O
ask	O
the	O
system	O
questions	O
for	O
which	O
the	O
answer	O
can	O
be	O
inferred	O
from	O
the	O
text	O
.	O

A	O
well	O
-	O
known	O
example	O
of	O
a	O
system	O
that	O
could	O
make	O
use	O
of	O
a	O
huge	O
collection	O
of	O
unstructured	O
documents	O
to	O
answer	O
questions	O
is	O
for	O
instance	O
IBM	O
’s	O
Watson	B-Method
system	E-Method
used	O
for	O
the	O
Jeopardy	O
challenge	O
.	O

Cloze	O
-	O
style	O
questions	O
,	O
i.e.	O
questions	O
formed	O
by	O
removing	O
a	O
phrase	O
from	O
a	O
sentence	O
,	O
are	O
an	O
appealing	O
form	O
of	O
such	O
questions	O
(	O
for	O
example	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

While	O
the	O
task	O
is	O
easy	O
to	O
evaluate	O
,	O
one	O
can	O
vary	O
the	O
context	O
,	O
the	O
question	O
sentence	O
or	O
the	O
specific	O
phrase	O
missing	O
in	O
the	O
question	O
to	O
dramatically	O
change	O
the	O
task	O
structure	O
and	O
difficulty	O
.	O

One	O
way	O
of	O
altering	O
the	O
task	B-Metric
difficulty	E-Metric
is	O
to	O
vary	O
the	O
word	O
type	O
being	O
replaced	O
,	O
as	O
in	O
.	O

The	O
complexity	O
of	O
such	O
variation	O
comes	O
from	O
the	O
fact	O
that	O
the	O
level	O
of	O
context	O
understanding	O
needed	O
in	O
order	O
to	O
correctly	O
predict	O
different	O
types	O
of	O
words	O
varies	O
greatly	O
.	O

While	O
predicting	B-Task
prepositions	E-Task
can	O
easily	O
be	O
done	O
using	O
relatively	O
simple	O
models	O
with	O
very	O
little	O
context	O
knowledge	O
,	O
predicting	O
named	O
entities	O
requires	O
a	O
deeper	O
understanding	O
of	O
the	O
context	O
.	O

Also	O
,	O
as	O
opposed	O
to	O
selecting	O
a	O
random	O
sentence	O
from	O
a	O
text	O
as	O
in	O
)	O
,	O
the	O
question	O
can	O
be	O
formed	O
from	O
a	O
specific	O
part	O
of	O
the	O
document	O
,	O
such	O
as	O
a	O
short	O
summary	O
or	O
a	O
list	O
of	O
tags	O
.	O

Since	O
such	O
sentences	O
often	O
paraphrase	O
in	O
a	O
condensed	O
form	O
what	O
was	O
said	O
in	O
the	O
text	O
,	O
they	O
are	O
particularly	O
suitable	O
for	O
testing	O
text	B-Task
comprehension	E-Task
.	O

An	O
important	O
property	O
of	O
cloze	O
-	O
style	O
questions	O
is	O
that	O
a	O
large	O
amount	O
of	O
such	O
questions	O
can	O
be	O
automatically	O
generated	O
from	O
real	O
world	O
documents	O
.	O

This	O
opens	O
the	O
task	O
to	O
data	B-Method
-	I-Method
hungry	I-Method
techniques	E-Method
such	O
as	O
deep	B-Method
learning	E-Method
.	O

This	O
is	O
an	O
advantage	O
compared	O
to	O
smaller	O
machine	O
understanding	O
datasets	O
like	O
MCTest	S-Method
that	O
have	O
only	O
hundreds	O
of	O
training	O
examples	O
and	O
therefore	O
the	O
best	O
performing	O
systems	O
usually	O
rely	O
on	O
hand	O
-	O
crafted	O
features	O
.	O

In	O
the	O
first	O
part	O
of	O
this	O
article	O
we	O
introduce	O
the	O
task	O
at	O
hand	O
and	O
the	O
main	O
aspects	O
of	O
the	O
relevant	O
datasets	O
.	O

Then	O
we	O
present	O
our	O
own	O
model	O
to	O
tackle	O
the	O
problem	O
.	O

Subsequently	O
we	O
compare	O
the	O
model	O
to	O
previously	O
proposed	O
architectures	O
and	O
finally	O
describe	O
the	O
experimental	O
results	O
on	O
the	O
performance	O
of	O
our	O
model	O
.	O

section	O
:	O
Task	O
and	O
datasets	O
In	O
this	O
section	O
we	O
introduce	O
the	O
task	O
that	O
we	O
are	O
seeking	O
to	O
solve	O
and	O
relevant	O
large	O
-	O
scale	O
datasets	O
that	O
have	O
recently	O
been	O
introduced	O
for	O
this	O
task	O
.	O

subsection	O
:	O
Formal	O
Task	O
Description	O
The	O
task	O
consists	O
of	O
answering	O
a	O
cloze	B-Task
-	I-Task
style	I-Task
question	E-Task
,	O
the	O
answer	O
to	O
which	O
depends	O
on	O
the	O
understanding	O
of	O
a	O
context	O
document	O
provided	O
with	O
the	O
question	O
.	O

The	O
model	O
is	O
also	O
provided	O
with	O
a	O
set	O
of	O
possible	O
answers	O
from	O
which	O
the	O
correct	O
one	O
is	O
to	O
be	O
selected	O
.	O

This	O
can	O
be	O
formalized	O
as	O
follows	O
:	O
The	O
training	O
data	O
consist	O
of	O
tuples	O
,	O
where	O
is	O
a	O
question	O
,	O
is	O
a	O
document	O
that	O
contains	O
the	O
answer	O
to	O
question	O
,	O
is	O
a	O
set	O
of	O
possible	O
answers	O
and	O
is	O
the	O
ground	O
truth	O
answer	O
.	O

Both	O
and	O
are	O
sequences	O
of	O
words	O
from	O
vocabulary	O
.	O

We	O
also	O
assume	O
that	O
all	O
possible	O
answers	O
are	O
words	O
from	O
the	O
vocabulary	O
,	O
that	O
is	O
,	O
and	O
that	O
the	O
ground	O
truth	O
answer	O
appears	O
in	O
the	O
document	O
,	O
that	O
is	O
.	O

subsection	O
:	O
Datasets	O
We	O
will	O
now	O
briefly	O
summarize	O
important	O
features	O
of	O
the	O
datasets	O
.	O

subsubsection	O
:	O
News	O
Articles	O
—	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	E-Material
The	O
first	O
two	O
datasets	O
were	O
constructed	O
from	O
a	O
large	O
number	O
of	O
news	O
articles	O
from	O
the	O
CNN	S-Material
and	O
Daily	O
Mail	O
websites	O
.	O

The	O
main	O
body	O
of	O
each	O
article	O
forms	O
a	O
context	O
,	O
while	O
the	O
cloze	O
-	O
style	O
question	O
is	O
formed	O
from	O
one	O
of	O
short	O
highlight	O
sentences	O
,	O
appearing	O
at	O
the	O
top	O
of	O
each	O
article	O
page	O
.	O

Specifically	O
,	O
the	O
question	O
is	O
created	O
by	O
replacing	O
a	O
named	O
entity	O
from	O
the	O
summary	O
sentence	O
(	O
e.g.	O
“	O
Producer	O
X	O
will	O
not	O
press	O
charges	O
against	O
Jeremy	O
Clarkson	O
,	O
his	O
lawyer	O
says	O
.	O

”	O
)	O
.	O

Furthermore	O
the	O
named	O
entities	O
in	O
the	O
whole	O
dataset	O
were	O
replaced	O
by	O
anonymous	O
tokens	O
which	O
were	O
further	O
shuffled	O
for	O
each	O
example	O
so	O
that	O
the	O
model	O
can	O
not	O
build	O
up	O
any	O
world	O
knowledge	O
about	O
the	O
entities	O
and	O
hence	O
has	O
to	O
genuinely	O
rely	O
on	O
the	O
context	O
document	O
to	O
search	O
for	O
an	O
answer	O
to	O
the	O
question	O
.	O

black	O
Qualitative	O
analysis	O
of	O
reasoning	O
patterns	O
needed	O
to	O
answer	O
questions	O
in	O
the	O
CNN	B-Material
dataset	E-Material
together	O
with	O
human	O
performance	O
on	O
this	O
task	O
are	O
provided	O
in	O
.	O

subsubsection	O
:	O
Children	B-Material
’s	I-Material
Book	I-Material
Test	E-Material
The	O
third	O
dataset	O
,	O
the	O
Children	B-Material
’s	I-Material
Book	I-Material
Test	E-Material
(	O
CBT	S-Material
)	O
,	O
is	O
built	O
from	O
books	O
that	O
are	O
freely	O
available	O
thanks	O
to	O
Project	O
Gutenberg	O
.	O

Each	O
context	O
document	O
is	O
formed	O
by	O
consecutive	O
sentences	O
taken	O
from	O
a	O
children	O
’s	O
book	O
story	O
.	O

Due	O
to	O
the	O
lack	O
of	O
summary	O
,	O
the	O
cloze	O
-	O
style	O
question	O
is	O
then	O
constructed	O
from	O
the	O
subsequent	O
(	O
st	O
)	O
sentence	O
.	O

One	O
can	O
also	O
see	O
how	O
the	O
task	B-Metric
complexity	E-Metric
varies	O
with	O
the	O
type	O
of	O
the	O
omitted	O
word	O
(	O
named	O
entity	O
,	O
common	O
noun	O
,	O
verb	O
,	O
preposition	O
)	O
.	O

have	O
shown	O
that	O
while	O
standard	O
LSTM	B-Method
language	I-Method
models	E-Method
have	O
human	O
level	O
performance	O
on	O
predicting	O
verbs	O
and	O
prepositions	O
,	O
they	O
lack	O
behind	O
on	O
named	O
entities	O
and	O
common	O
nouns	O
.	O

In	O
this	O
article	O
we	O
therefore	O
focus	O
only	O
on	O
predicting	O
the	O
first	O
two	O
word	O
types	O
.	O

Basic	O
statistics	O
about	O
the	O
CNN	B-Material
,	I-Material
Daily	I-Material
Mail	E-Material
and	O
CBT	S-Material
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Our	O
Model	O
—	O
Attention	O
Sum	O
Reader	O
black	O
Our	O
model	O
called	O
the	O
is	O
tailor	O
-	O
made	O
to	O
leverage	O
the	O
fact	O
that	O
the	O
answer	O
is	O
a	O
word	O
from	O
the	O
context	O
document	O
.	O

This	O
is	O
a	O
double	O
-	O
edged	O
sword	O
.	O

While	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
of	O
the	O
mentioned	O
datasets	O
(	O
where	O
this	O
assumption	O
holds	O
true	O
)	O
,	O
it	O
can	O
not	O
produce	O
an	O
answer	O
which	O
is	O
not	O
contained	O
in	O
the	O
document	O
.	O

Intuitively	O
,	O
our	O
model	O
is	O
structured	O
as	O
follows	O
:	O
We	O
compute	O
a	O
vector	B-Method
embedding	I-Method
of	I-Method
the	I-Method
query	E-Method
.	O

We	O
compute	O
a	O
vector	B-Method
embedding	E-Method
of	O
each	O
individual	O
word	O
in	O
the	O
context	O
of	O
the	O
whole	O
document	O
(	O
contextual	B-Method
embedding	E-Method
)	O
.	O

Using	O
a	O
dot	O
product	O
between	O
the	O
question	B-Method
embedding	E-Method
and	O
the	O
contextual	B-Method
embedding	E-Method
of	O
each	O
occurrence	O
of	O
a	O
candidate	O
answer	O
in	O
the	O
document	O
,	O
we	O
select	O
the	O
most	O
likely	O
answer	O
.	O

subsection	O
:	O
Formal	O
Description	O
Our	O
model	O
uses	O
one	O
word	B-Method
embedding	I-Method
function	E-Method
and	O
two	O
encoder	B-Method
functions	E-Method
.	O

The	O
word	B-Method
embedding	I-Method
function	E-Method
translates	O
words	O
into	O
vector	B-Method
representations	E-Method
.	O

The	O
first	O
encoder	B-Method
function	E-Method
is	O
a	O
document	B-Method
encoder	E-Method
that	O
encodes	O
every	O
word	O
from	O
the	O
document	O
in	O
the	O
context	O
of	O
the	O
whole	O
document	O
.	O

We	O
call	O
this	O
the	O
contextual	B-Method
embedding	E-Method
.	O

For	O
convenience	O
we	O
will	O
denote	O
the	O
contextual	O
embedding	O
of	O
the	O
-	O
th	O
word	O
in	O
as	O
.	O

The	O
second	O
encoder	O
is	O
used	O
to	O
translate	O
the	O
query	O
into	O
a	O
fixed	B-Method
length	I-Method
representation	E-Method
of	O
the	O
same	O
dimensionality	O
as	O
each	O
black	O
.	O

Both	O
encoders	O
use	O
word	O
embeddings	O
computed	O
by	O
as	O
their	O
input	O
.	O

Then	O
we	O
compute	O
a	O
weight	O
for	O
every	O
word	O
in	O
the	O
document	O
as	O
the	O
dot	O
product	O
of	O
its	O
contextual	B-Method
embedding	E-Method
and	O
the	O
query	B-Method
embedding	E-Method
.	O

This	O
weight	O
might	O
be	O
viewed	O
as	O
an	O
attention	O
over	O
the	O
document	O
.	O

To	O
form	O
a	O
proper	O
probability	O
distribution	O
over	O
the	O
words	O
in	O
the	O
document	O
,	O
we	O
normalize	O
the	O
weights	O
using	O
the	O
softmax	B-Method
function	E-Method
.	O

This	O
way	O
we	O
model	O
probability	O
that	O
the	O
answer	O
to	O
query	O
appears	O
at	O
position	O
in	O
the	O
document	O
.	O

In	O
a	O
functional	O
form	O
this	O
is	O
:	O
Finally	O
we	O
compute	O
the	O
probability	O
that	O
word	O
is	O
a	O
correct	O
answer	O
as	O
:	O
where	O
is	O
a	O
set	O
of	O
positions	O
where	O
appears	O
in	O
the	O
document	O
.	O

We	O
call	O
this	O
mechanism	O
pointer	O
sum	O
attention	O
since	O
we	O
use	O
attention	O
as	O
a	O
pointer	O
over	O
discrete	O
tokens	O
in	O
the	O
context	O
document	O
and	O
then	O
we	O
directly	O
sum	O
the	O
word	O
’s	O
attention	O
across	O
all	O
the	O
occurrences	O
.	O

This	O
differs	O
from	O
the	O
usual	O
use	O
of	O
attention	S-Method
in	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
models	E-Method
where	O
attention	S-Method
is	O
used	O
to	O
blend	O
representations	O
of	O
words	O
into	O
a	O
new	O
embedding	O
vector	O
.	O

Our	O
use	O
of	O
attention	O
was	O
inspired	O
by	O
ptrnet	S-Method
.	O

A	O
high	O
level	O
structure	O
of	O
our	O
model	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Model	O
instance	O
details	O
In	O
our	O
model	O
the	O
document	B-Method
encoder	E-Method
is	O
implemented	O
as	O
a	O
bidirectional	B-Method
Gated	I-Method
Recurrent	I-Method
Unit	I-Method
(	I-Method
GRU	I-Method
)	I-Method
network	E-Method
whose	O
hidden	O
states	O
form	O
the	O
contextual	O
word	O
embeddings	O
,	O
that	O
is	O
,	O
where	O
denotes	O
vector	O
concatenation	O
and	O
and	O
denote	O
forward	O
and	O
backward	O
contextual	O
embeddings	O
from	O
the	O
respective	O
recurrent	B-Method
networks	E-Method
.	O

The	O
query	B-Method
encoder	E-Method
is	O
implemented	O
by	O
another	O
bidirectional	B-Method
GRU	I-Method
network	E-Method
.	O

This	O
time	O
the	O
last	O
hidden	O
state	O
of	O
the	O
forward	B-Method
network	E-Method
is	O
concatenated	O
with	O
the	O
last	O
hidden	O
state	O
of	O
the	O
backward	B-Method
network	E-Method
to	O
form	O
the	O
query	B-Task
embedding	E-Task
,	O
that	O
is	O
.	O

The	O
word	B-Method
embedding	I-Method
function	E-Method
is	O
implemented	O
in	O
a	O
usual	O
way	O
as	O
a	O
look	O
-	O
up	O
table	O
.	O

is	O
a	O
matrix	O
whose	O
rows	O
can	O
be	O
indexed	O
by	O
words	O
from	O
the	O
vocabulary	O
,	O
that	O
is	O
.	O

Therefore	O
,	O
each	O
row	O
of	O
contains	O
embedding	O
of	O
one	O
word	O
from	O
the	O
vocabulary	O
.	O

During	O
training	O
we	O
jointly	O
optimize	O
parameters	O
of	O
,	O
and	O
.	O

section	O
:	O
Related	O
Work	O
black	O
Several	O
recent	O
deep	B-Method
neural	I-Method
network	I-Method
architectures	E-Method
were	O
applied	O
to	O
the	O
task	O
of	O
text	B-Task
comprehension	E-Task
.	O

The	O
last	O
two	O
architectures	O
were	O
developed	O
independently	O
at	O
the	O
same	O
time	O
as	O
our	O
work	O
.	O

All	O
of	O
these	O
architectures	O
use	O
an	O
attention	B-Method
mechanism	E-Method
that	O
allows	O
them	O
to	O
highlight	O
places	O
in	O
the	O
document	O
that	O
might	O
be	O
relevant	O
to	O
answering	O
the	O
question	O
.	O

We	O
will	O
now	O
briefly	O
describe	O
these	O
architectures	O
and	O
compare	O
them	O
to	O
our	O
approach	O
.	O

subsection	O
:	O
Attentive	O
and	O
Impatient	O
Readers	O
Attentive	O
and	O
Impatient	O
Readers	O
were	O
proposed	O
in	O
.	O

The	O
simpler	O
Attentive	B-Method
Reader	E-Method
is	O
very	O
similar	O
to	O
our	O
architecture	O
.	O

It	O
also	O
uses	O
bidirectional	B-Method
document	E-Method
and	O
query	B-Method
encoders	E-Method
to	O
compute	O
an	O
attention	O
in	O
a	O
similar	O
way	O
we	O
do	O
.	O

The	O
more	O
complex	O
Impatient	O
Reader	O
computes	O
attention	O
over	O
the	O
document	O
after	O
reading	O
every	O
word	O
of	O
the	O
query	O
.	O

However	O
,	O
empirical	O
evaluation	O
has	O
shown	O
that	O
both	O
models	O
perform	O
almost	O
identically	O
on	O
the	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
.	O

The	O
key	O
difference	O
between	O
the	O
Attentive	B-Method
Reader	E-Method
and	O
our	O
model	O
is	O
that	O
the	O
Attentive	B-Method
Reader	E-Method
uses	O
attention	O
to	O
compute	O
a	O
fixed	B-Method
length	I-Method
representation	E-Method
of	O
the	O
document	O
that	O
is	O
equal	O
to	O
a	O
weighted	O
sum	O
of	O
contextual	O
embeddings	O
of	O
words	O
in	O
,	O
that	O
is	O
.	O

A	O
joint	B-Task
query	I-Task
and	I-Task
document	I-Task
embedding	E-Task
is	O
then	O
a	O
non	O
-	O
linear	O
function	O
of	O
and	O
the	O
query	B-Method
embedding	E-Method
.	O

This	O
joint	B-Method
embedding	E-Method
is	O
in	O
the	O
end	O
compared	O
against	O
all	O
candidate	O
answers	O
using	O
the	O
dot	O
product	O
,	O
in	O
the	O
end	O
the	O
scores	O
are	O
normalized	O
by	O
softmax	S-Method
.	O

That	O
is	O
:	O
.	O

black	O
In	O
contrast	O
to	O
the	O
Attentive	O
Reader	O
,	O
we	O
select	O
the	O
answer	O
from	O
the	O
context	O
directly	O
using	O
the	O
computed	O
attention	O
rather	O
than	O
using	O
such	O
attention	O
for	O
a	O
weighted	O
sum	O
of	O
the	O
individual	O
representations	O
(	O
see	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
motivation	O
for	O
such	O
simplification	O
is	O
the	O
following	O
.	O

Consider	O
a	O
context	O
“	O
A	O
UFO	O
was	O
observed	O
above	O
our	O
city	O
in	O
January	O
and	O
again	O
in	O
March	O
.	O

”	O
and	O
question	O
“	O
An	O
observer	O
has	O
spotted	O
a	O
UFO	O
in	O
_	O
_	O
_	O
.	O

”	O
Since	O
both	O
January	O
and	O
March	O
are	O
equally	O
good	O
candidates	O
,	O
the	O
attention	B-Method
mechanism	E-Method
might	O
put	O
the	O
same	O
attention	O
on	O
both	O
these	O
candidates	O
in	O
the	O
context	O
.	O

The	O
blending	B-Method
mechanism	E-Method
described	O
above	O
would	O
compute	O
a	O
vector	O
between	O
the	O
representations	O
of	O
these	O
two	O
words	O
and	O
propose	O
the	O
closest	O
word	O
as	O
the	O
answer	O
-	O
this	O
may	O
well	O
happen	O
to	O
be	O
February	O
(	O
it	O
is	O
indeed	O
the	O
case	O
for	O
Word2Vec	S-Method
trained	O
on	O
Google	O
News	O
)	O
.	O

By	O
contrast	O
,	O
our	O
model	O
would	O
correctly	O
propose	O
January	O
or	O
March	O
.	O

black	O
subsection	O
:	O
Chen	O
et	O
al	O
.	O

2016	O
A	O
model	O
presented	O
in	O
is	O
inspired	O
by	O
the	O
Attentive	O
Reader	O
.	O

One	O
difference	O
is	O
that	O
the	O
attention	O
weights	O
are	O
computed	O
with	O
a	O
bilinear	O
term	O
instead	O
of	O
simple	O
dot	O
-	O
product	O
,	O
that	O
is	O
.	O

The	O
document	B-Task
embedding	E-Task
is	O
computed	O
using	O
a	O
weighted	B-Method
sum	E-Method
as	O
in	O
the	O
Attentive	O
Reader	O
,	O
.	O

In	O
the	O
end	O
,	O
where	O
is	O
a	O
new	O
embedding	O
function	O
.	O

Even	O
though	O
it	O
is	O
a	O
simplification	O
of	O
the	O
Attentive	B-Method
Reader	E-Method
this	O
model	O
performs	O
significantly	O
better	O
than	O
the	O
original	O
.	O

black	O
subsection	O
:	O
Memory	B-Method
Networks	I-Method
MenNN	E-Method
were	O
applied	O
to	O
the	O
task	O
of	O
text	B-Task
comprehension	E-Task
in	O
.	O

The	O
best	O
performing	O
memory	B-Method
networks	I-Method
model	I-Method
setup	I-Method
-	I-Method
window	I-Method
memory	E-Method
-	O
uses	O
windows	O
of	O
fixed	O
length	O
(	O
8	O
)	O
centered	O
around	O
the	O
candidate	O
words	O
as	O
memory	O
cells	O
.	O

Due	O
to	O
this	O
limited	O
context	O
window	O
,	O
the	O
model	O
is	O
unable	O
to	O
capture	O
dependencies	O
out	O
of	O
scope	O
of	O
this	O
window	O
.	O

Furthermore	O
,	O
the	O
representation	O
within	O
such	O
window	O
is	O
computed	O
simply	O
as	O
the	O
sum	O
of	O
embeddings	O
of	O
words	O
in	O
that	O
window	O
.	O

By	O
contrast	O
,	O
in	O
our	O
model	O
the	O
representation	O
of	O
each	O
individual	O
word	O
is	O
computed	O
using	O
a	O
recurrent	B-Method
network	E-Method
,	O
which	O
not	O
only	O
allows	O
it	O
to	O
capture	O
context	O
from	O
the	O
entire	O
document	O
but	O
also	O
the	O
embedding	B-Task
computation	E-Task
is	O
much	O
more	O
flexible	O
than	O
a	O
simple	O
sum	O
.	O

To	O
improve	O
on	O
the	O
initial	O
accuracy	S-Metric
,	O
a	O
heuristic	B-Method
approach	E-Method
called	O
self	B-Method
supervision	E-Method
is	O
used	O
in	O
to	O
help	O
the	O
network	O
to	O
select	O
the	O
right	O
supporting	O
“	O
memories	O
”	O
using	O
an	O
attention	B-Method
mechanism	E-Method
showing	O
similarities	O
to	O
the	O
ours	O
.	O

Plain	O
MenNN	S-Method
without	O
this	O
heuristic	O
are	O
not	O
competitive	O
on	O
these	O
machine	B-Task
reading	I-Task
tasks	E-Task
.	O

Our	O
model	O
does	O
not	O
need	O
any	O
similar	O
heuristics	O
.	O

black	O
subsection	O
:	O
Dynamic	B-Method
Entity	I-Method
Representation	E-Method
The	O
Dynamic	B-Method
Entity	I-Method
Representation	I-Method
model	E-Method
has	O
a	O
complex	O
architecture	O
also	O
based	O
on	O
the	O
weighted	B-Method
attention	I-Method
mechanism	E-Method
and	O
max	B-Method
-	I-Method
pooling	E-Method
over	O
contextual	O
embeddings	O
of	O
vectors	O
for	O
each	O
named	O
entity	O
.	O

subsection	O
:	O
Pointer	B-Method
Networks	E-Method
Our	O
model	O
architecture	O
was	O
inspired	O
by	O
ptrnet	S-Method
in	O
using	O
an	O
attention	B-Method
mechanism	E-Method
to	O
select	O
the	O
answer	O
in	O
the	O
context	O
rather	O
than	O
to	O
blend	O
words	O
from	O
the	O
context	O
into	O
an	O
answer	B-Method
representation	E-Method
.	O

While	O
a	O
ptrnet	S-Method
consists	O
of	O
an	O
encoder	S-Method
as	O
well	O
as	O
a	O
decoder	S-Method
,	O
which	O
uses	O
the	O
attention	O
to	O
select	O
the	O
output	O
at	O
each	O
step	O
,	O
our	O
model	O
outputs	O
the	O
answer	O
in	O
a	O
single	O
step	O
.	O

Furthermore	O
,	O
the	O
pointer	B-Method
networks	E-Method
assume	O
that	O
no	O
input	O
in	O
the	O
sequence	O
appears	O
more	O
than	O
once	O
,	O
which	O
is	O
not	O
the	O
case	O
in	O
our	O
settings	O
.	O

subsection	O
:	O
Summary	O
Our	O
model	O
combines	O
the	O
best	O
features	O
of	O
the	O
architectures	O
mentioned	O
above	O
.	O

We	O
use	O
recurrent	B-Method
networks	E-Method
to	O
“	O
read	O
”	O
the	O
document	O
and	O
the	O
query	O
as	O
black	O
done	O
in	O
and	O
we	O
use	O
attention	O
in	O
a	O
way	O
similar	O
to	O
ptrnet	S-Method
.	O

We	O
also	O
use	O
summation	O
of	O
attention	O
weights	O
in	O
a	O
way	O
similar	O
to	O
MenNN	S-Method
.	O

black	O
From	O
a	O
high	O
level	O
perspective	O
we	O
simplify	O
all	O
the	O
discussed	O
text	B-Method
comprehension	I-Method
models	E-Method
by	O
removing	O
all	O
transformations	O
past	O
the	O
attention	O
step	O
.	O

Instead	O
we	O
use	O
the	O
attention	O
directly	O
to	O
compute	O
the	O
answer	O
probability	O
.	O

section	O
:	O
Evaluation	O
In	O
this	O
section	O
we	O
evaluate	O
our	O
model	O
on	O
the	O
CNN	B-Material
,	I-Material
Daily	I-Material
Mail	E-Material
and	O
CBT	B-Material
datasets	E-Material
.	O

We	O
show	O
that	O
despite	O
the	O
model	O
’s	O
simplicity	O
its	O
ensembles	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
each	O
of	O
these	O
datasets	O
.	O

subsection	O
:	O
Training	O
Details	O
black	O
To	O
train	O
the	O
model	O
we	O
used	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
with	O
the	O
ADAM	B-Method
update	I-Method
rule	E-Method
and	O
learning	B-Method
rate	I-Method
of	I-Method
or	E-Method
.	O

During	O
training	O
we	O
minimized	O
the	O
following	O
negative	O
log	O
-	O
likelihood	O
with	O
respect	O
to	O
:	O
where	O
is	O
the	O
correct	O
answer	O
for	O
query	O
and	O
document	O
,	O
and	O
represents	O
parameters	O
of	O
the	O
encoder	B-Method
functions	E-Method
and	O
and	O
of	O
the	O
word	B-Method
embedding	I-Method
function	E-Method
.	O

The	O
optimized	O
probability	O
distribution	O
is	O
defined	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

The	O
initial	O
weights	O
in	O
the	O
word	O
embedding	O
matrix	O
were	O
drawn	O
randomly	O
uniformly	O
from	O
the	O
interval	O
.	O

Weights	O
in	O
the	O
GRU	B-Method
networks	E-Method
were	O
initialized	O
by	O
random	O
orthogonal	O
matrices	O
and	O
biases	O
were	O
initialized	O
to	O
zero	O
.	O

We	O
also	O
used	O
a	O
gradient	O
clipping	O
threshold	O
of	O
10	O
and	O
batches	O
of	O
size	O
32	O
.	O

During	O
training	O
we	O
randomly	O
shuffled	O
all	O
examples	O
in	O
each	O
epoch	O
.	O

To	O
speedup	O
training	S-Task
,	O
we	O
always	O
pre	O
-	O
fetched	O
batches	O
worth	O
of	O
examples	O
and	O
sorted	O
them	O
according	O
to	O
document	O
length	O
.	O

Hence	O
each	O
batch	O
contained	O
documents	O
of	O
roughly	O
the	O
same	O
length	O
.	O

For	O
each	O
batch	O
of	O
the	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
we	O
randomly	O
reshuffled	O
the	O
assignment	O
of	O
named	O
entities	O
to	O
the	O
corresponding	O
word	O
embedding	O
vectors	O
to	O
match	O
the	O
procedure	O
proposed	O
in	O
.	O

This	O
guaranteed	O
that	O
word	O
embeddings	O
of	O
named	O
entities	O
were	O
used	O
only	O
as	O
semantically	O
meaningless	O
labels	O
not	O
encoding	O
any	O
intrinsic	O
features	O
of	O
the	O
represented	O
entities	O
.	O

This	O
forced	O
the	O
model	O
to	O
truly	O
deduce	O
the	O
answer	O
from	O
the	O
single	O
context	O
document	O
associated	O
with	O
the	O
question	O
.	O

black	O
We	O
also	O
do	O
not	O
use	O
pre	O
-	O
trained	O
word	O
embeddings	O
to	O
make	O
our	O
training	O
procedure	O
comparable	O
to	O
.	O

We	O
did	O
not	O
perform	O
any	O
text	B-Task
pre	I-Task
-	I-Task
processing	E-Task
since	O
the	O
original	O
datasets	O
were	O
already	O
tokenized	O
.	O

black	O
We	O
do	O
not	O
use	O
any	O
regularization	S-Method
since	O
in	O
our	O
experience	O
it	O
leads	O
to	O
longer	O
training	B-Metric
times	E-Metric
of	O
single	B-Method
models	E-Method
,	O
however	O
,	O
performance	O
of	O
a	O
model	B-Method
ensemble	E-Method
is	O
usually	O
the	O
same	O
.	O

This	O
way	O
we	O
can	O
train	O
the	O
whole	O
ensemble	O
faster	O
when	O
using	O
multiple	O
GPUs	S-Method
for	O
parallel	B-Task
training	E-Task
.	O

black	O
For	O
Additional	O
details	O
about	O
the	O
training	O
procedure	O
see	O
Appendix	O
[	O
reference	O
]	O
.	O

black	O
subsection	O
:	O
Evaluation	O
Method	O
We	O
evaluated	O
the	O
proposed	O
model	O
both	O
as	O
a	O
single	O
model	O
and	O
using	O
ensemble	B-Method
averaging	E-Method
.	O

blackAlthough	O
the	O
model	O
computes	O
attention	O
for	O
every	O
word	O
in	O
the	O
document	O
we	O
restrict	O
the	O
model	O
to	O
select	O
an	O
answer	O
from	O
a	O
list	O
of	O
candidate	O
answers	O
associated	O
with	O
each	O
question	O
-	O
document	O
pair	O
.	O

For	O
single	B-Method
models	E-Method
we	O
are	O
reporting	O
results	O
for	O
the	O
best	O
model	O
as	O
well	O
as	O
the	O
average	B-Metric
of	I-Metric
accuracies	E-Metric
for	O
the	O
best	O
20	O
%	O
of	O
models	O
with	O
best	O
performance	O
on	O
validation	O
data	O
since	O
single	B-Method
models	E-Method
display	O
considerable	O
variation	O
of	O
results	O
due	O
to	O
random	B-Method
weight	I-Method
initialization	E-Method
even	O
for	O
identical	O
hyperparameter	O
values	O
.	O

Single	B-Method
model	E-Method
performance	O
may	O
consequently	O
prove	O
difficult	O
to	O
reproduce	O
.	O

What	O
concerns	O
ensembles	O
,	O
we	O
used	O
simple	O
averaging	O
of	O
the	O
answer	O
probabilities	O
predicted	O
by	O
ensemble	B-Method
members	E-Method
.	O

For	O
ensembling	S-Task
we	O
used	O
14	O
,	O
16	O
,	O
84	O
and	O
53	O
models	O
for	O
CNN	S-Material
,	O
Daily	B-Material
Mail	E-Material
and	O
CBT	S-Material
CN	O
and	O
NE	O
respectively	O
.	O

The	O
ensemble	B-Method
models	E-Method
were	O
chosen	O
either	O
as	O
the	O
top	O
70	O
%	O
of	O
all	O
trained	O
models	O
,	O
we	O
call	O
this	O
avg	B-Method
ensemble	E-Method
.	O

Alternatively	O
we	O
use	O
the	O
following	O
algorithm	O
:	O
We	O
started	O
with	O
the	O
best	O
performing	O
model	O
according	O
to	O
validation	B-Metric
performance	E-Metric
.	O

Then	O
in	O
each	O
step	O
we	O
tried	O
adding	O
the	O
best	O
performing	O
model	O
that	O
had	O
not	O
been	O
previously	O
tried	O
.	O

We	O
kept	O
it	O
in	O
the	O
ensemble	O
if	O
it	O
did	O
improve	O
its	O
validation	O
performance	O
and	O
discarded	O
it	O
otherwise	O
.	O

This	O
way	O
we	O
gradually	O
tried	O
each	O
model	O
once	O
.	O

We	O
call	O
the	O
resulting	O
model	O
a	O
greedy	B-Method
ensemble	E-Method
.	O

[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
subsection	O
:	O
Results	O
black	O
Performance	O
of	O
our	O
models	O
on	O
the	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
shows	O
results	O
on	O
the	O
CBT	B-Material
dataset	E-Material
.	O

The	O
tables	O
also	O
list	O
performance	O
of	O
other	O
published	O
models	O
that	O
were	O
evaluated	O
on	O
these	O
datasets	O
.	O

Ensembles	O
of	O
our	O
models	O
set	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
evaluated	O
datasets	O
.	O

Table	O
[	O
reference	O
]	O
then	O
measures	O
accuracy	S-Metric
as	O
the	O
proportion	O
of	O
test	O
cases	O
where	O
the	O
ground	O
truth	O
was	O
among	O
the	O
top	O
answers	O
proposed	O
by	O
the	O
greedy	B-Method
ensemble	E-Method
model	O
for	O
.	O

CNN	S-Material
and	O
Daily	O
Mail	O
.	O

blackThe	O
CNN	B-Material
dataset	E-Material
is	O
the	O
most	O
widely	O
used	O
dataset	O
for	O
evaluation	B-Task
of	I-Task
text	I-Task
comprehension	I-Task
systems	E-Task
published	O
so	O
far	O
.	O

Performance	O
of	O
our	O
single	O
model	O
is	O
a	O
little	O
bit	O
worse	O
than	O
performance	O
of	O
simultaneously	O
published	O
models	O
.	O

Compared	O
to	O
our	O
work	O
these	O
models	O
were	O
trained	O
with	O
Dropout	B-Method
regularization	E-Method
which	O
might	O
improve	O
single	O
model	O
performance	O
.	O

However	O
,	O
ensemble	O
of	O
our	O
models	O
outperforms	O
these	O
models	O
even	O
though	O
they	O
use	O
pre	O
-	O
trained	O
word	B-Method
embeddings	E-Method
.	O

On	O
the	O
CNN	B-Material
dataset	E-Material
our	O
single	O
model	O
with	O
best	O
validation	B-Metric
accuracy	E-Metric
achieves	O
a	O
test	B-Metric
accuracy	E-Metric
of	O
69.5	O
%	O
.	O

The	O
average	O
performance	O
of	O
the	O
top	O
20	O
%	O
models	O
according	O
to	O
validation	B-Metric
accuracy	E-Metric
is	O
69.9	O
%	O
which	O
is	O
even	O
0.5	O
%	O
better	O
than	O
the	O
single	O
best	O
-	O
validation	B-Method
model	E-Method
.	O

This	O
shows	O
that	O
there	O
were	O
many	O
models	O
that	O
performed	O
better	O
on	O
test	O
set	O
than	O
the	O
best	O
-	O
validation	B-Method
model	E-Method
.	O

Fusing	O
multiple	B-Method
models	E-Method
then	O
gives	O
a	O
significant	O
further	O
increase	O
in	O
accuracy	S-Metric
on	O
both	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
..	O
CBT	S-Material
.	O

In	O
named	B-Task
entity	I-Task
prediction	E-Task
our	O
best	O
single	O
model	O
with	O
accuracy	S-Metric
of	O
68.6	O
%	O
performs	O
2	O
%	O
absolute	O
better	O
than	O
the	O
MenNN	S-Method
with	O
self	B-Method
supervision	E-Method
,	O
the	O
averaging	B-Method
ensemble	E-Method
performs	O
4	O
%	O
absolute	O
better	O
than	O
the	O
best	O
previous	O
result	O
.	O

In	O
common	B-Task
noun	I-Task
prediction	E-Task
our	O
single	B-Method
models	E-Method
is	O
0.4	O
%	O
absolute	O
better	O
than	O
MenNN	S-Method
however	O
the	O
ensemble	O
improves	O
the	O
performance	O
to	O
69	O
%	O
which	O
is	O
6	O
%	O
absolute	O
better	O
than	O
MenNN	S-Method
.	O

black	O
section	O
:	O
Analysis	O
To	O
further	O
analyze	O
the	O
properties	O
of	O
our	O
model	O
,	O
we	O
examined	O
the	O
dependence	O
of	O
accuracy	S-Metric
on	O
the	O
length	O
of	O
the	O
context	O
document	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
the	O
number	O
of	O
candidate	O
answers	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
and	O
the	O
frequency	O
of	O
the	O
correct	O
answer	O
in	O
the	O
context	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

On	O
the	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
,	O
the	O
accuracy	S-Metric
decreases	O
with	O
increasing	O
document	O
length	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

We	O
hypothesize	O
this	O
may	O
be	O
due	O
to	O
multiple	O
factors	O
.	O

Firstly	O
long	O
documents	O
may	O
make	O
the	O
task	O
more	O
complex	O
.	O

Secondly	O
such	O
cases	O
are	O
quite	O
rare	O
in	O
the	O
training	O
data	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
which	O
motivates	O
the	O
model	O
to	O
specialize	O
on	O
shorter	O
contexts	O
.	O

Finally	O
the	O
context	O
length	O
is	O
correlated	O
with	O
the	O
number	O
of	O
named	O
entities	O
,	O
i.e.	O
the	O
number	O
of	O
possible	O
answers	O
which	O
is	O
itself	O
negatively	O
correlated	O
with	O
accuracy	S-Metric
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

On	O
the	O
CBT	B-Material
dataset	E-Material
this	O
negative	O
trend	O
seems	O
to	O
disappear	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
supports	O
the	O
later	O
two	O
explanations	O
since	O
the	O
distribution	O
of	O
document	O
lengths	O
is	O
somewhat	O
more	O
uniform	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
and	O
the	O
number	O
of	O
candidate	O
answers	O
is	O
constant	O
(	O
)	O
for	O
all	O
examples	O
in	O
this	O
dataset	O
.	O

The	O
effect	O
of	O
increasing	O
number	O
of	O
candidate	O
answers	O
on	O
the	O
model	O
’s	O
accuracy	S-Metric
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
can	O
clearly	O
see	O
that	O
as	O
the	O
number	O
of	O
candidate	O
answers	O
increases	O
,	O
the	O
accuracy	S-Metric
drops	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
amount	O
of	O
examples	O
with	O
large	O
number	O
of	O
candidate	O
answers	O
is	O
quite	O
small	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
since	O
the	O
summation	O
of	O
attention	O
in	O
our	O
model	O
inherently	O
favours	O
frequently	O
occurring	O
tokens	O
,	O
we	O
also	O
visualize	O
how	O
the	O
accuracy	S-Metric
depends	O
on	O
the	O
frequency	O
of	O
the	O
correct	O
answer	O
in	O
the	O
document	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
that	O
the	O
accuracy	S-Metric
significantly	O
drops	O
as	O
the	O
correct	O
answer	O
gets	O
less	O
and	O
less	O
frequent	O
in	O
the	O
document	O
compared	O
to	O
other	O
candidate	O
answers	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
correct	O
answer	O
is	O
likely	O
to	O
occur	O
frequently	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

black	O
section	O
:	O
Conclusion	O
In	O
this	O
article	O
we	O
presented	O
a	O
new	O
neural	B-Method
network	I-Method
architecture	E-Method
for	O
natural	B-Task
language	I-Task
text	I-Task
comprehension	E-Task
.	O

While	O
our	O
model	O
is	O
simpler	O
than	O
previously	O
published	O
models	O
,	O
it	O
gives	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	S-Metric
on	O
all	O
evaluated	O
datasets	O
.	O

An	O
analysis	O
by	O
suggests	O
that	O
on	O
CNN	S-Material
and	O
Daily	O
Mail	O
datasets	O
a	O
significant	O
proportion	O
of	O
questions	O
is	O
ambiguous	O
or	O
too	O
difficult	O
to	O
answer	O
even	O
for	O
humans	O
(	O
partly	O
due	O
to	O
entity	O
anonymization	O
)	O
so	O
the	O
ensemble	O
of	O
our	O
models	O
may	O
be	O
very	O
near	O
to	O
the	O
maximal	O
accuracy	S-Metric
achievable	O
on	O
these	O
datasets	O
.	O

section	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
Tim	O
Klinger	O
for	O
providing	O
us	O
with	O
masked	O
softmax	O
code	O
that	O
we	O
used	O
in	O
our	O
implementation	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Training	O
Details	O
During	O
training	O
we	O
evaluated	O
the	O
model	O
performance	O
after	O
each	O
epoch	O
and	O
stopped	O
the	O
training	O
when	O
the	O
error	O
on	O
the	O
validation	O
set	O
started	O
increasing	O
.	O

The	O
models	O
usually	O
converged	O
after	O
two	O
epochs	O
of	O
training	O
.	O

Time	O
needed	O
to	O
complete	O
a	O
single	O
epoch	O
of	O
training	O
on	O
each	O
dataset	O
on	O
an	O
Nvidia	O
K40	O
GPU	O
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

black	O
The	O
hyperparameters	O
,	O
namely	O
the	O
recurrent	O
hidden	O
layer	O
dimension	O
and	O
the	O
source	O
embedding	O
dimension	O
,	O
were	O
chosen	O
by	O
grid	B-Method
search	E-Method
.	O

We	O
started	O
with	O
a	O
range	O
of	O
128	O
to	O
384	O
for	O
both	O
parameters	O
and	O
subsequently	O
kept	O
increasing	O
the	O
upper	O
bound	O
by	O
128	O
until	O
we	O
started	O
observing	O
a	O
consistent	O
decrease	O
in	O
validation	B-Metric
accuracy	E-Metric
.	O

The	O
region	O
of	O
the	O
parameter	O
space	O
that	O
we	O
explored	O
together	O
with	O
the	O
parameters	O
of	O
the	O
model	O
with	O
best	O
validation	B-Metric
accuracy	E-Metric
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

black	O
Our	O
model	O
was	O
implemented	O
using	O
Theano	O
and	O
Blocks	O
.	O

section	O
:	O
Dependence	O
of	O
accuracy	S-Metric
on	O
the	O
frequency	O
of	O
the	O
correct	O
answer	O
In	O
Section	O
[	O
reference	O
]	O
we	O
analysed	O
how	O
the	O
test	B-Metric
accuracy	E-Metric
depends	O
on	O
how	O
frequent	O
the	O
correct	O
answer	O
is	O
compared	O
to	O
other	O
answer	O
candidates	O
for	O
the	O
news	O
datasets	O
.	O

The	O
plots	O
for	O
the	O
Children	B-Material
’s	I-Material
Book	I-Material
Test	E-Material
looks	O
very	O
similar	O
,	O
however	O
we	O
are	O
adding	O
it	O
here	O
for	O
completeness	O
.	O

[	O
b	O
]	O
0.475	O
[	O
b	O
]	O
0.475	O
document	O
:	O
Learning	O
Phrase	B-Method
Representations	E-Method
using	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
for	O
Statistical	B-Task
Machine	I-Task
Translation	E-Task
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
neural	B-Method
network	I-Method
model	E-Method
called	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
that	O
consists	O
of	O
two	O
recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNN	S-Method
)	O
.	O

One	O
RNN	S-Method
encodes	O
a	O
sequence	O
of	O
symbols	O
into	O
a	O
fixed	B-Method
-	I-Method
length	I-Method
vector	I-Method
representation	E-Method
,	O
and	O
the	O
other	O
decodes	O
the	O
representation	O
into	O
another	O
sequence	O
of	O
symbols	O
.	O

The	O
encoder	S-Method
and	O
decoder	S-Method
of	O
the	O
proposed	O
model	O
are	O
jointly	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
a	O
target	O
sequence	O
given	O
a	O
source	O
sequence	O
.	O

The	O
performance	O
of	O
a	O
statistical	B-Task
machine	I-Task
translation	I-Task
system	E-Task
is	O
empirically	O
found	O
to	O
improve	O
by	O
using	O
the	O
conditional	O
probabilities	O
of	O
phrase	O
pairs	O
computed	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
as	O
an	O
additional	O
feature	O
in	O
the	O
existing	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
.	O

Qualitatively	O
,	O
we	O
show	O
that	O
the	O
proposed	O
model	O
learns	O
a	O
semantically	S-Method
and	O
syntactically	B-Method
meaningful	I-Method
representation	E-Method
of	O
linguistic	O
phrases	O
.	O

arxiv	O
arxiv.	O
/	O
figures	O
/	O
section	O
:	O
Introduction	O
Deep	B-Method
neural	I-Method
networks	E-Method
have	O
shown	O
great	O
success	O
in	O
various	O
applications	O
such	O
as	O
objection	B-Task
recognition	E-Task
(	O
see	O
,	O
e.g.	O
,	O
)	O
and	O
speech	B-Task
recognition	E-Task
(	O
see	O
,	O
e.g.	O
,	O
)	O
.	O

Furthermore	O
,	O
many	O
recent	O
works	O
showed	O
that	O
neural	B-Method
networks	E-Method
can	O
be	O
successfully	O
used	O
in	O
a	O
number	O
of	O
tasks	O
in	O
natural	B-Task
language	I-Task
processing	E-Task
(	O
NLP	B-Task
)	E-Task
.	O

These	O
include	O
,	O
but	O
are	O
not	O
limited	O
to	O
,	O
language	B-Task
modeling	E-Task
,	O
paraphrase	B-Task
detection	E-Task
and	O
word	B-Task
embedding	I-Task
extraction	E-Task
.	O

In	O
the	O
field	O
of	O
statistical	B-Task
machine	I-Task
translation	E-Task
(	O
SMT	B-Task
)	E-Task
,	O
deep	B-Method
neural	I-Method
networks	E-Method
have	O
begun	O
to	O
show	O
promising	O
results	O
.	O

summarizes	O
a	O
successful	O
usage	O
of	O
feedforward	B-Method
neural	I-Method
networks	E-Method
in	O
the	O
framework	O
of	O
phrase	B-Method
-	I-Method
based	I-Method
SMT	I-Method
system	E-Method
.	O

Along	O
this	O
line	O
of	O
research	O
on	O
using	O
neural	B-Method
networks	E-Method
for	O
SMT	S-Task
,	O
this	O
paper	O
focuses	O
on	O
a	O
novel	O
neural	B-Method
network	I-Method
architecture	E-Method
that	O
can	O
be	O
used	O
as	O
a	O
part	O
of	O
the	O
conventional	O
phrase	B-Method
-	I-Method
based	I-Method
SMT	I-Method
system	E-Method
.	O

The	O
proposed	O
neural	B-Method
network	I-Method
architecture	E-Method
,	O
which	O
we	O
will	O
refer	O
to	O
as	O
an	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
consists	O
of	O
two	O
recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNN	S-Method
)	O
that	O
act	O
as	O
an	O
encoder	S-Method
and	O
a	O
decoder	O
pair	O
.	O

The	O
encoder	S-Method
maps	O
a	O
variable	O
-	O
length	O
source	O
sequence	O
to	O
a	O
fixed	O
-	O
length	O
vector	O
,	O
and	O
the	O
decoder	S-Method
maps	O
the	O
vector	B-Method
representation	E-Method
back	O
to	O
a	O
variable	O
-	O
length	O
target	O
sequence	O
.	O

The	O
two	O
networks	O
are	O
trained	O
jointly	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
the	O
target	O
sequence	O
given	O
a	O
source	O
sequence	O
.	O

Additionally	O
,	O
we	O
propose	O
to	O
use	O
a	O
rather	O
sophisticated	O
hidden	O
unit	O
in	O
order	O
to	O
improve	O
both	O
the	O
memory	B-Metric
capacity	E-Metric
and	O
the	O
ease	O
of	O
training	S-Task
.	O

The	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
with	O
a	O
novel	O
hidden	B-Method
unit	E-Method
is	O
empirically	O
evaluated	O
on	O
the	O
task	O
of	O
translating	S-Task
from	O
English	B-Material
to	I-Material
French	E-Material
.	O

We	O
train	O
the	O
model	O
to	O
learn	O
the	O
translation	O
probability	O
of	O
an	O
English	O
phrase	O
to	O
a	O
corresponding	O
French	O
phrase	O
.	O

The	O
model	O
is	O
then	O
used	O
as	O
a	O
part	O
of	O
a	O
standard	O
phrase	B-Method
-	I-Method
based	I-Method
SMT	I-Method
system	E-Method
by	O
scoring	O
each	O
phrase	O
pair	O
in	O
the	O
phrase	O
table	O
.	O

The	O
empirical	O
evaluation	O
reveals	O
that	O
this	O
approach	O
of	O
scoring	B-Task
phrase	I-Task
pairs	E-Task
with	O
an	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
improves	O
the	O
translation	S-Task
performance	O
.	O

We	O
qualitatively	O
analyze	O
the	O
trained	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
by	O
comparing	O
its	O
phrase	O
scores	O
with	O
those	O
given	O
by	O
the	O
existing	O
translation	B-Method
model	E-Method
.	O

The	O
qualitative	O
analysis	O
shows	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
better	O
at	O
capturing	O
the	O
linguistic	O
regularities	O
in	O
the	O
phrase	O
table	O
,	O
indirectly	O
explaining	O
the	O
quantitative	O
improvements	O
in	O
the	O
overall	O
translation	B-Metric
performance	E-Metric
.	O

The	O
further	O
analysis	O
of	O
the	O
model	O
reveals	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
learns	O
a	O
continuous	B-Method
space	I-Method
representation	E-Method
of	O
a	O
phrase	O
that	O
preserves	O
both	O
the	O
semantic	O
and	O
syntactic	O
structure	O
of	O
the	O
phrase	O
.	O

section	O
:	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
subsection	O
:	O
Preliminary	O
:	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
A	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
is	O
a	O
neural	B-Method
network	E-Method
that	O
consists	O
of	O
a	O
hidden	O
state	O
and	O
an	O
optional	O
output	O
which	O
operates	O
on	O
a	O
variable	O
-	O
length	O
sequence	O
.	O

At	O
each	O
time	O
step	O
,	O
the	O
hidden	O
state	O
of	O
the	O
RNN	S-Method
is	O
updated	O
by	O
where	O
is	O
a	O
non	B-Method
-	I-Method
linear	I-Method
activation	I-Method
function	E-Method
.	O

may	O
be	O
as	O
simple	O
as	O
an	O
element	B-Method
-	I-Method
wise	I-Method
logistic	I-Method
sigmoid	I-Method
function	E-Method
and	O
as	O
complex	O
as	O
a	O
long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	I-Method
(	I-Method
LSTM	I-Method
)	I-Method
unit	E-Method
.	O

An	O
RNN	S-Method
can	O
learn	O
a	O
probability	O
distribution	O
over	O
a	O
sequence	O
by	O
being	O
trained	O
to	O
predict	O
the	O
next	O
symbol	O
in	O
a	O
sequence	O
.	O

In	O
that	O
case	O
,	O
the	O
output	O
at	O
each	O
timestep	O
is	O
the	O
conditional	O
distribution	O
.	O

For	O
example	O
,	O
a	O
multinomial	B-Method
distribution	I-Method
(	I-Method
-	I-Method
of	I-Method
-	I-Method
coding	E-Method
)	O
can	O
be	O
output	O
using	O
a	O
softmax	B-Method
activation	I-Method
function	E-Method
for	O
all	O
possible	O
symbols	O
,	O
where	O
are	O
the	O
rows	O
of	O
a	O
weight	O
matrix	O
.	O

By	O
combining	O
these	O
probabilities	O
,	O
we	O
can	O
compute	O
the	O
probability	O
of	O
the	O
sequence	O
using	O
From	O
this	O
learned	B-Method
distribution	E-Method
,	O
it	O
is	O
straightforward	O
to	O
sample	O
a	O
new	O
sequence	O
by	O
iteratively	O
sampling	O
a	O
symbol	O
at	O
each	O
time	O
step	O
.	O

subsection	O
:	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
neural	B-Method
network	I-Method
architecture	E-Method
that	O
learns	O
to	O
encode	O
a	O
variable	O
-	O
length	O
sequence	O
into	O
a	O
fixed	B-Method
-	I-Method
length	I-Method
vector	I-Method
representation	E-Method
and	O
to	O
decode	O
a	O
given	O
fixed	B-Method
-	I-Method
length	I-Method
vector	I-Method
representation	E-Method
back	O
into	O
a	O
variable	O
-	O
length	O
sequence	O
.	O

From	O
a	O
probabilistic	O
perspective	O
,	O
this	O
new	O
model	O
is	O
a	O
general	O
method	O
to	O
learn	O
the	O
conditional	O
distribution	O
over	O
a	O
variable	O
-	O
length	O
sequence	O
conditioned	O
on	O
yet	O
another	O
variable	O
-	O
length	O
sequence	O
,	O
e.g.	O
,	O
where	O
one	O
should	O
note	O
that	O
the	O
input	O
and	O
output	O
sequence	O
lengths	O
and	O
may	O
differ	O
.	O

The	O
encoder	S-Method
is	O
an	O
RNN	S-Method
that	O
reads	O
each	O
symbol	O
of	O
an	O
input	O
sequence	O
sequentially	O
.	O

As	O
it	O
reads	O
each	O
symbol	O
,	O
the	O
hidden	O
state	O
of	O
the	O
RNN	S-Method
changes	O
according	O
to	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

After	O
reading	O
the	O
end	O
of	O
the	O
sequence	O
(	O
marked	O
by	O
an	O
end	O
-	O
of	O
-	O
sequence	O
symbol	O
)	O
,	O
the	O
hidden	O
state	O
of	O
the	O
RNN	S-Method
is	O
a	O
summary	O
of	O
the	O
whole	O
input	O
sequence	O
.	O

The	O
decoder	O
of	O
the	O
proposed	O
model	O
is	O
another	O
RNN	S-Method
which	O
is	O
trained	O
to	O
generate	O
the	O
output	O
sequence	O
by	O
predicting	O
the	O
next	O
symbol	O
given	O
the	O
hidden	O
state	O
.	O

However	O
,	O
unlike	O
the	O
RNN	S-Method
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
both	O
and	O
are	O
also	O
conditioned	O
on	O
and	O
on	O
the	O
summary	O
of	O
the	O
input	O
sequence	O
.	O

Hence	O
,	O
the	O
hidden	O
state	O
of	O
the	O
decoder	O
at	O
time	O
is	O
computed	O
by	O
,	O
and	O
similarly	O
,	O
the	O
conditional	O
distribution	O
of	O
the	O
next	O
symbol	O
is	O
for	O
given	O
activation	O
functions	O
and	O
(	O
the	O
latter	O
must	O
produce	O
valid	O
probabilities	O
,	O
e.g.	O
with	O
a	O
softmax	S-Method
)	O
.	O

See	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
a	O
graphical	O
depiction	O
of	O
the	O
proposed	O
model	B-Method
architecture	E-Method
.	O

The	O
two	O
components	O
of	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
are	O
jointly	O
trained	O
to	O
maximize	O
the	O
conditional	O
log	O
-	O
likelihood	O
where	O
is	O
the	O
set	O
of	O
the	O
model	O
parameters	O
and	O
each	O
is	O
an	O
(	O
input	O
sequence	O
,	O
output	O
sequence	O
)	O
pair	O
from	O
the	O
training	O
set	O
.	O

In	O
our	O
case	O
,	O
as	O
the	O
output	O
of	O
the	O
decoder	S-Method
,	O
starting	O
from	O
the	O
input	O
,	O
is	O
differentiable	O
,	O
we	O
can	O
use	O
a	O
gradient	B-Method
-	I-Method
based	I-Method
algorithm	E-Method
to	O
estimate	O
the	O
model	O
parameters	O
.	O

Once	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
trained	O
,	O
the	O
model	O
can	O
be	O
used	O
in	O
two	O
ways	O
.	O

One	O
way	O
is	O
to	O
use	O
the	O
model	O
to	O
generate	O
a	O
target	O
sequence	O
given	O
an	O
input	O
sequence	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
model	O
can	O
be	O
used	O
to	O
score	O
a	O
given	O
pair	O
of	O
input	O
and	O
output	O
sequences	O
,	O
where	O
the	O
score	O
is	O
simply	O
a	O
probability	O
from	O
Eqs	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Hidden	B-Method
Unit	E-Method
that	O
Adaptively	O
Remembers	O
and	O
Forgets	O
In	O
addition	O
to	O
a	O
novel	O
model	B-Method
architecture	E-Method
,	O
we	O
also	O
propose	O
a	O
new	O
type	O
of	O
hidden	O
unit	O
(	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
that	O
has	O
been	O
motivated	O
by	O
the	O
LSTM	B-Method
unit	E-Method
but	O
is	O
much	O
simpler	O
to	O
compute	O
and	O
implement	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
graphical	O
depiction	O
of	O
the	O
proposed	O
hidden	B-Method
unit	E-Method
.	O

Let	O
us	O
describe	O
how	O
the	O
activation	O
of	O
the	O
-	O
th	O
hidden	B-Method
unit	E-Method
is	O
computed	O
.	O

First	O
,	O
the	O
reset	O
gate	O
is	O
computed	O
by	O
where	O
is	O
the	O
logistic	B-Method
sigmoid	I-Method
function	E-Method
,	O
and	O
denotes	O
the	O
-	O
th	O
element	O
of	O
a	O
vector	O
.	O

and	O
are	O
the	O
input	O
and	O
the	O
previous	O
hidden	O
state	O
,	O
respectively	O
.	O

and	O
are	O
weight	O
matrices	O
which	O
are	O
learned	O
.	O

Similarly	O
,	O
the	O
update	O
gate	O
is	O
computed	O
by	O
The	O
actual	O
activation	O
of	O
the	O
proposed	O
unit	O
is	O
then	O
computed	O
by	O
where	O
In	O
this	O
formulation	O
,	O
when	O
the	O
reset	O
gate	O
is	O
close	O
to	O
0	O
,	O
the	O
hidden	O
state	O
is	O
forced	O
to	O
ignore	O
the	O
previous	O
hidden	O
state	O
and	O
reset	O
with	O
the	O
current	O
input	O
only	O
.	O

This	O
effectively	O
allows	O
the	O
hidden	O
state	O
to	O
drop	O
any	O
information	O
that	O
is	O
found	O
to	O
be	O
irrelevant	O
later	O
in	O
the	O
future	O
,	O
thus	O
,	O
allowing	O
a	O
more	O
compact	O
representation	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
update	O
gate	O
controls	O
how	O
much	O
information	O
from	O
the	O
previous	O
hidden	O
state	O
will	O
carry	O
over	O
to	O
the	O
current	O
hidden	O
state	O
.	O

This	O
acts	O
similarly	O
to	O
the	O
memory	O
cell	O
in	O
the	O
LSTM	B-Method
network	E-Method
and	O
helps	O
the	O
RNN	S-Method
to	O
remember	O
long	O
-	O
term	O
information	O
.	O

Furthermore	O
,	O
this	O
may	O
be	O
considered	O
an	O
adaptive	B-Method
variant	E-Method
of	O
a	O
leaky	B-Method
-	I-Method
integration	I-Method
unit	E-Method
.	O

As	O
each	O
hidden	B-Method
unit	E-Method
has	O
separate	O
reset	O
and	O
update	O
gates	O
,	O
each	O
hidden	B-Method
unit	E-Method
will	O
learn	O
to	O
capture	O
dependencies	O
over	O
different	O
time	O
scales	O
.	O

Those	O
units	O
that	O
learn	O
to	O
capture	O
short	O
-	O
term	O
dependencies	O
will	O
tend	O
to	O
have	O
reset	O
gates	O
that	O
are	O
frequently	O
active	O
,	O
but	O
those	O
that	O
capture	O
longer	O
-	O
term	O
dependencies	O
will	O
have	O
update	O
gates	O
that	O
are	O
mostly	O
active	O
.	O

In	O
our	O
preliminary	O
experiments	O
,	O
we	O
found	O
that	O
it	O
is	O
crucial	O
to	O
use	O
this	O
new	O
unit	O
with	O
gating	B-Method
units	E-Method
.	O

We	O
were	O
not	O
able	O
to	O
get	O
meaningful	O
result	O
with	O
an	O
oft	O
-	O
used	O
unit	O
without	O
any	O
gating	O
.	O

section	O
:	O
Statistical	B-Task
Machine	I-Task
Translation	E-Task
In	O
a	O
commonly	O
used	O
statistical	B-Task
machine	I-Task
translation	I-Task
system	E-Task
(	O
SMT	B-Task
)	E-Task
,	O
the	O
goal	O
of	O
the	O
system	O
(	O
decoder	S-Method
,	O
specifically	O
)	O
is	O
to	O
find	O
a	O
translation	O
given	O
a	O
source	O
sentence	O
,	O
which	O
maximizes	O
where	O
the	O
first	O
term	O
at	O
the	O
right	O
hand	O
side	O
is	O
called	O
translation	B-Method
model	E-Method
and	O
the	O
latter	O
language	B-Method
model	E-Method
(	O
see	O
,	O
e.g.	O
,	O
)	O
.	O

In	O
practice	O
,	O
however	O
,	O
most	O
SMT	S-Task
systems	O
model	O
as	O
a	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
with	O
additional	O
features	O
and	O
corresponding	O
weights	O
:	O
where	O
and	O
are	O
the	O
-	O
th	O
feature	O
and	O
weight	O
,	O
respectively	O
.	O

is	O
a	O
normalization	O
constant	O
that	O
does	O
not	O
depend	O
on	O
the	O
weights	O
.	O

The	O
weights	O
are	O
often	O
optimized	O
to	O
maximize	O
the	O
BLEU	B-Metric
score	E-Metric
on	O
a	O
development	O
set	O
.	O

In	O
the	O
phrase	O
-	O
based	O
SMT	S-Task
framework	O
introduced	O
in	O
and	O
,	O
the	O
translation	B-Method
model	E-Method
is	O
factorized	O
into	O
the	O
translation	O
probabilities	O
of	O
matching	O
phrases	O
in	O
the	O
source	O
and	O
target	O
sentences	O
.	O

These	O
probabilities	O
are	O
once	O
again	O
considered	O
additional	O
features	O
in	O
the	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
(	O
see	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
and	O
are	O
weighted	O
accordingly	O
to	O
maximize	O
the	O
BLEU	B-Metric
score	E-Metric
.	O

Since	O
the	O
neural	B-Method
net	I-Method
language	I-Method
model	E-Method
was	O
proposed	O
in	O
,	O
neural	B-Method
networks	E-Method
have	O
been	O
used	O
widely	O
in	O
SMT	B-Method
systems	E-Method
.	O

In	O
many	O
cases	O
,	O
neural	B-Method
networks	E-Method
have	O
been	O
used	O
to	O
rescore	B-Task
translation	I-Task
hypotheses	E-Task
(	O
-	O
best	O
lists	O
)	O
(	O
see	O
,	O
e.g.	O
,	O
)	O
.	O

Recently	O
,	O
however	O
,	O
there	O
has	O
been	O
interest	O
in	O
training	O
neural	B-Method
networks	E-Method
to	O
score	O
the	O
translated	O
sentence	O
(	O
or	O
phrase	O
pairs	O
)	O
using	O
a	O
representation	O
of	O
the	O
source	O
sentence	O
as	O
an	O
additional	O
input	O
.	O

See	O
,	O
e.g.	O
,	O
,	O
and	O
.	O

subsection	O
:	O
Scoring	O
Phrase	O
Pairs	O
with	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
Here	O
we	O
propose	O
to	O
train	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
on	O
a	O
table	O
of	O
phrase	O
pairs	O
and	O
use	O
its	O
scores	O
as	O
additional	O
features	O
in	O
the	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
when	O
tuning	O
the	O
SMT	S-Task
decoder	O
.	O

When	O
we	O
train	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
we	O
ignore	O
the	O
(	O
normalized	O
)	O
frequencies	O
of	O
each	O
phrase	O
pair	O
in	O
the	O
original	O
corpora	O
.	O

This	O
measure	O
was	O
taken	O
in	O
order	O
(	O
1	O
)	O
to	O
reduce	O
the	O
computational	B-Metric
expense	E-Metric
of	O
randomly	O
selecting	O
phrase	O
pairs	O
from	O
a	O
large	O
phrase	O
table	O
according	O
to	O
the	O
normalized	O
frequencies	O
and	O
(	O
2	O
)	O
to	O
ensure	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
does	O
not	O
simply	O
learn	O
to	O
rank	O
the	O
phrase	O
pairs	O
according	O
to	O
their	O
numbers	O
of	O
occurrences	O
.	O

One	O
underlying	O
reason	O
for	O
this	O
choice	O
was	O
that	O
the	O
existing	O
translation	O
probability	O
in	O
the	O
phrase	O
table	O
already	O
reflects	O
the	O
frequencies	O
of	O
the	O
phrase	O
pairs	O
in	O
the	O
original	O
corpus	O
.	O

With	O
a	O
fixed	O
capacity	O
of	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
we	O
try	O
to	O
ensure	O
that	O
most	O
of	O
the	O
capacity	O
of	O
the	O
model	O
is	O
focused	O
toward	O
learning	O
linguistic	O
regularities	O
,	O
i.e.	O
,	O
distinguishing	O
between	O
plausible	O
and	O
implausible	O
translations	O
,	O
or	O
learning	O
the	O
“	O
manifold	O
”	O
(	O
region	O
of	O
probability	O
concentration	O
)	O
of	O
plausible	O
translations	O
.	O

Once	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
trained	O
,	O
we	O
add	O
a	O
new	O
score	O
for	O
each	O
phrase	O
pair	O
to	O
the	O
existing	O
phrase	O
table	O
.	O

This	O
allows	O
the	O
new	O
scores	O
to	O
enter	O
into	O
the	O
existing	O
tuning	B-Method
algorithm	E-Method
with	O
minimal	O
additional	O
overhead	O
in	O
computation	S-Metric
.	O

As	O
Schwenk	O
pointed	O
out	O
in	O
,	O
it	O
is	O
possible	O
to	O
completely	O
replace	O
the	O
existing	O
phrase	O
table	O
with	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

In	O
that	O
case	O
,	O
for	O
a	O
given	O
source	O
phrase	O
,	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
will	O
need	O
to	O
generate	O
a	O
list	O
of	O
(	O
good	O
)	O
target	O
phrases	O
.	O

This	O
requires	O
,	O
however	O
,	O
an	O
expensive	O
sampling	B-Method
procedure	E-Method
to	O
be	O
performed	O
repeatedly	O
.	O

In	O
this	O
paper	O
,	O
thus	O
,	O
we	O
only	O
consider	O
rescoring	O
the	O
phrase	O
pairs	O
in	O
the	O
phrase	O
table	O
.	O

subsection	O
:	O
Related	O
Approaches	O
:	O
Neural	B-Method
Networks	E-Method
in	O
Machine	B-Task
Translation	E-Task
Before	O
presenting	O
the	O
empirical	O
results	O
,	O
we	O
discuss	O
a	O
number	O
of	O
recent	O
works	O
that	O
have	O
proposed	O
to	O
use	O
neural	B-Method
networks	E-Method
in	O
the	O
context	O
of	O
SMT	S-Task
.	O

Schwenk	O
in	O
proposed	O
a	O
similar	O
approach	O
of	O
scoring	B-Task
phrase	I-Task
pairs	E-Task
.	O

Instead	O
of	O
the	O
RNN	S-Method
-	O
based	O
neural	O
network	O
,	O
he	O
used	O
a	O
feedforward	B-Method
neural	I-Method
network	E-Method
that	O
has	O
fixed	O
-	O
size	O
inputs	O
(	O
7	O
words	O
in	O
his	O
case	O
,	O
with	O
zero	O
-	O
padding	O
for	O
shorter	O
phrases	O
)	O
and	O
fixed	O
-	O
size	O
outputs	O
(	O
7	O
words	O
in	O
the	O
target	O
language	O
)	O
.	O

When	O
it	O
is	O
used	O
specifically	O
for	O
scoring	B-Task
phrases	E-Task
for	O
the	O
SMT	B-Method
system	E-Method
,	O
the	O
maximum	O
phrase	O
length	O
is	O
often	O
chosen	O
to	O
be	O
small	O
.	O

However	O
,	O
as	O
the	O
length	O
of	O
phrases	O
increases	O
or	O
as	O
we	O
apply	O
neural	B-Method
networks	E-Method
to	O
other	O
variable	O
-	O
length	O
sequence	O
data	O
,	O
it	O
is	O
important	O
that	O
the	O
neural	B-Method
network	E-Method
can	O
handle	O
variable	O
-	O
length	O
input	O
and	O
output	O
.	O

The	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
well	O
-	O
suited	O
for	O
these	O
applications	O
.	O

Similar	O
to	O
,	O
Devlin	O
et	O
al	O
.	O

proposed	O
to	O
use	O
a	O
feedforward	B-Method
neural	I-Method
network	E-Method
to	O
model	O
a	O
translation	B-Method
model	E-Method
,	O
however	O
,	O
by	O
predicting	O
one	O
word	O
in	O
a	O
target	O
phrase	O
at	O
a	O
time	O
.	O

They	O
reported	O
an	O
impressive	O
improvement	O
,	O
but	O
their	O
approach	O
still	O
requires	O
the	O
maximum	O
length	O
of	O
the	O
input	O
phrase	O
(	O
or	O
context	O
words	O
)	O
to	O
be	O
fixed	O
a	O
priori	O
.	O

Although	O
it	O
is	O
not	O
exactly	O
a	O
neural	B-Method
network	E-Method
they	O
train	O
,	O
the	O
authors	O
of	O
proposed	O
to	O
learn	O
a	O
bilingual	O
embedding	O
of	O
words	O
/	O
phrases	O
.	O

They	O
use	O
the	O
learned	O
embedding	S-Method
to	O
compute	O
the	O
distance	O
between	O
a	O
pair	O
of	O
phrases	O
which	O
is	O
used	O
as	O
an	O
additional	O
score	O
of	O
the	O
phrase	O
pair	O
in	O
an	O
SMT	B-Method
system	E-Method
.	O

In	O
,	O
a	O
feedforward	B-Method
neural	I-Method
network	E-Method
was	O
trained	O
to	O
learn	O
a	O
mapping	O
from	O
a	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
representation	E-Method
of	O
an	O
input	O
phrase	O
to	O
an	O
output	O
phrase	O
.	O

This	O
is	O
closely	O
related	O
to	O
both	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
and	O
the	O
model	O
proposed	O
in	O
,	O
except	O
that	O
their	O
input	O
representation	O
of	O
a	O
phrase	O
is	O
a	O
bag	O
-	O
of	O
-	O
words	O
.	O

A	O
similar	O
approach	O
of	O
using	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
representations	E-Method
was	O
proposed	O
in	O
as	O
well	O
.	O

Earlier	O
,	O
a	O
similar	O
encoder	B-Method
–	I-Method
decoder	I-Method
model	E-Method
using	O
two	O
recursive	B-Method
neural	I-Method
networks	E-Method
was	O
proposed	O
in	O
,	O
but	O
their	O
model	O
was	O
restricted	O
to	O
a	O
monolingual	B-Task
setting	E-Task
,	O
i.e.	O
the	O
model	O
reconstructs	O
an	O
input	O
sentence	O
.	O

More	O
recently	O
,	O
another	O
encoder	B-Method
–	I-Method
decoder	I-Method
model	E-Method
using	O
an	O
RNN	S-Method
was	O
proposed	O
in	O
,	O
where	O
the	O
decoder	S-Method
is	O
conditioned	O
on	O
a	O
representation	O
of	O
either	O
a	O
source	O
sentence	O
or	O
a	O
source	O
context	O
.	O

One	O
important	O
difference	O
between	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
and	O
the	O
approaches	O
in	O
and	O
is	O
that	O
the	O
order	O
of	O
the	O
words	O
in	O
source	O
and	O
target	O
phrases	O
is	O
taken	O
into	O
account	O
.	O

The	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
naturally	O
distinguishes	O
between	O
sequences	O
that	O
have	O
the	O
same	O
words	O
but	O
in	O
a	O
different	O
order	O
,	O
whereas	O
the	O
aforementioned	O
approaches	O
effectively	O
ignore	O
order	O
information	O
.	O

The	O
closest	O
approach	O
related	O
to	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
the	O
Recurrent	B-Method
Continuous	I-Method
Translation	I-Method
Model	E-Method
(	O
Model	O
2	O
)	O
proposed	O
in	O
.	O

In	O
their	O
paper	O
,	O
they	O
proposed	O
a	O
similar	O
model	O
that	O
consists	O
of	O
an	O
encoder	B-Method
and	I-Method
decoder	E-Method
.	O

The	O
difference	O
with	O
our	O
model	O
is	O
that	O
they	O
used	O
a	O
convolutional	B-Method
-	I-Method
gram	I-Method
model	E-Method
(	O
CGM	B-Method
)	E-Method
for	O
the	O
encoder	S-Method
and	O
the	O
hybrid	O
of	O
an	O
inverse	B-Method
CGM	E-Method
and	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
for	O
the	O
decoder	S-Method
.	O

They	O
,	O
however	O
,	O
evaluated	O
their	O
model	O
on	O
rescoring	B-Task
the	I-Task
-	I-Task
best	I-Task
list	E-Task
proposed	O
by	O
the	O
conventional	O
SMT	B-Method
system	E-Method
and	O
computing	O
the	O
perplexity	S-Metric
of	O
the	O
gold	O
standard	O
translations	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
our	O
approach	O
on	O
the	O
English	B-Task
/	I-Task
French	I-Task
translation	I-Task
task	E-Task
of	O
the	O
WMT’14	B-Material
workshop	E-Material
.	O

subsection	O
:	O
Data	O
and	O
Baseline	S-Method
System	O
Large	O
amounts	O
of	O
resources	O
are	O
available	O
to	O
build	O
an	O
English	O
/	O
French	O
SMT	S-Task
system	O
in	O
the	O
framework	O
of	O
the	O
WMT’14	B-Task
translation	I-Task
task	E-Task
.	O

The	O
bilingual	O
corpora	O
include	O
Europarl	O
(	O
61	O
M	O
words	O
)	O
,	O
news	O
commentary	O
(	O
5.5	O
M	O
)	O
,	O
UN	O
(	O
421	O
M	O
)	O
,	O
and	O
two	O
crawled	O
corpora	O
of	O
90	O
M	O
and	O
780	O
M	O
words	O
respectively	O
.	O

The	O
last	O
two	O
corpora	O
are	O
quite	O
noisy	O
.	O

To	O
train	O
the	O
French	B-Method
language	I-Method
model	E-Method
,	O
about	O
712	O
M	O
words	O
of	O
crawled	O
newspaper	O
material	O
is	O
available	O
in	O
addition	O
to	O
the	O
target	O
side	O
of	O
the	O
bitexts	O
.	O

All	O
the	O
word	O
counts	O
refer	O
to	O
French	B-Material
words	E-Material
after	O
tokenization	O
.	O

It	O
is	O
commonly	O
acknowledged	O
that	O
training	O
statistical	B-Method
models	E-Method
on	O
the	O
concatenation	O
of	O
all	O
this	O
data	O
does	O
not	O
necessarily	O
lead	O
to	O
optimal	O
performance	O
,	O
and	O
results	O
in	O
extremely	O
large	O
models	O
which	O
are	O
difficult	O
to	O
handle	O
.	O

Instead	O
,	O
one	O
should	O
focus	O
on	O
the	O
most	O
relevant	O
subset	O
of	O
the	O
data	O
for	O
a	O
given	O
task	O
.	O

We	O
have	O
done	O
so	O
by	O
applying	O
the	O
data	B-Method
selection	I-Method
method	E-Method
proposed	O
in	O
,	O
and	O
its	O
extension	O
to	O
bitexts	O
.	O

By	O
these	O
means	O
we	O
selected	O
a	O
subset	O
of	O
418	O
M	O
words	O
out	O
of	O
more	O
than	O
2	O
G	O
words	O
for	O
language	B-Task
modeling	E-Task
and	O
a	O
subset	O
of	O
348	O
M	O
out	O
of	O
850	O
M	O
words	O
for	O
training	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

We	O
used	O
the	O
test	O
set	O
newstest2012	O
and	O
2013	O
for	O
data	B-Task
selection	E-Task
and	O
weight	B-Task
tuning	E-Task
with	O
MERT	S-Method
,	O
and	O
newstest2014	O
as	O
our	O
test	O
set	O
.	O

Each	O
set	O
has	O
more	O
than	O
70	O
thousand	O
words	O
and	O
a	O
single	O
reference	O
translation	O
.	O

For	O
training	O
the	O
neural	B-Method
networks	E-Method
,	O
including	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
we	O
limited	O
the	O
source	O
and	O
target	O
vocabulary	O
to	O
the	O
most	O
frequent	O
15	O
,	O
000	O
words	O
for	O
both	O
English	B-Material
and	I-Material
French	E-Material
.	O

This	O
covers	O
approximately	O
93	O
%	O
of	O
the	O
dataset	O
.	O

All	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
were	O
mapped	O
to	O
a	O
special	O
token	O
(	O
)	O
.	O

The	O
baseline	O
phrase	B-Method
-	I-Method
based	I-Method
SMT	I-Method
system	E-Method
was	O
built	O
using	O
Moses	S-Method
with	O
default	O
settings	O
.	O

This	O
system	O
achieves	O
a	O
BLEU	B-Metric
score	E-Metric
of	O
30.64	O
and	O
33.3	O
on	O
the	O
development	O
and	O
test	O
sets	O
,	O
respectively	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

subsubsection	O
:	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
The	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
used	O
in	O
the	O
experiment	O
had	O
1000	O
hidden	O
units	O
with	O
the	O
proposed	O
gates	O
at	O
the	O
encoder	S-Method
and	O
at	O
the	O
decoder	S-Method
.	O

The	O
input	O
matrix	O
between	O
each	O
input	O
symbol	O
and	O
the	O
hidden	O
unit	O
is	O
approximated	O
with	O
two	O
lower	O
-	O
rank	O
matrices	O
,	O
and	O
the	O
output	O
matrix	O
is	O
approximated	O
similarly	O
.	O

We	O
used	O
rank	O
-	O
100	O
matrices	O
,	O
equivalent	O
to	O
learning	O
an	O
embedding	O
of	O
dimension	O
100	O
for	O
each	O
word	O
.	O

The	O
activation	O
function	O
used	O
for	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
a	O
hyperbolic	O
tangent	O
function	O
.	O

The	O
computation	O
from	O
the	O
hidden	O
state	O
in	O
the	O
decoder	O
to	O
the	O
output	O
is	O
implemented	O
as	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
with	O
a	O
single	O
intermediate	B-Method
layer	E-Method
having	O
500	O
maxout	B-Method
units	E-Method
each	O
pooling	O
2	O
inputs	O
.	O

All	O
the	O
weight	O
parameters	O
in	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
were	O
initialized	O
by	O
sampling	O
from	O
an	O
isotropic	B-Method
zero	I-Method
-	I-Method
mean	I-Method
(	I-Method
white	I-Method
)	I-Method
Gaussian	I-Method
distribution	E-Method
with	O
its	O
standard	O
deviation	O
fixed	O
to	O
,	O
except	O
for	O
the	O
recurrent	O
weight	O
parameters	O
.	O

For	O
the	O
recurrent	O
weight	O
matrices	O
,	O
we	O
first	O
sampled	O
from	O
a	O
white	B-Method
Gaussian	I-Method
distribution	E-Method
and	O
used	O
its	O
left	O
singular	O
vectors	O
matrix	O
,	O
following	O
.	O

We	O
used	O
Adadelta	S-Method
and	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
to	O
train	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
with	O
hyperparameters	S-Method
and	O
.	O

At	O
each	O
update	O
,	O
we	O
used	O
64	O
randomly	O
selected	O
phrase	O
pairs	O
from	O
a	O
phrase	O
table	O
(	O
which	O
was	O
created	O
from	O
348	O
M	O
words	O
)	O
.	O

The	O
model	O
was	O
trained	O
for	O
approximately	O
three	O
days	O
.	O

Details	O
of	O
the	O
architecture	O
used	O
in	O
the	O
experiments	O
are	O
explained	O
in	O
more	O
depth	O
in	O
the	O
supplementary	O
material	O
.	O

subsubsection	O
:	O
Neural	B-Method
Language	I-Method
Model	E-Method
In	O
order	O
to	O
assess	O
the	O
effectiveness	O
of	O
scoring	O
phrase	O
pairs	O
with	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
we	O
also	O
tried	O
a	O
more	O
traditional	O
approach	O
of	O
using	O
a	O
neural	B-Method
network	E-Method
for	O
learning	O
a	O
target	B-Method
language	I-Method
model	E-Method
(	O
CSLM	S-Method
)	O
.	O

Especially	O
,	O
the	O
comparison	O
between	O
the	O
SMT	B-Method
system	E-Method
using	O
CSLM	S-Method
and	O
that	O
using	O
the	O
proposed	O
approach	O
of	O
phrase	B-Method
scoring	E-Method
by	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
will	O
clarify	O
whether	O
the	O
contributions	O
from	O
multiple	O
neural	B-Method
networks	E-Method
in	O
different	O
parts	O
of	O
the	O
SMT	B-Method
system	E-Method
add	O
up	O
or	O
are	O
redundant	O
.	O

We	O
trained	O
the	O
CSLM	S-Method
model	O
on	O
7	O
-	O
grams	O
from	O
the	O
target	O
corpus	O
.	O

Each	O
input	O
word	O
was	O
projected	O
into	O
the	O
embedding	O
space	O
,	O
and	O
they	O
were	O
concatenated	O
to	O
form	O
a	O
3072	O
-	O
dimensional	O
vector	O
.	O

The	O
concatenated	O
vector	O
was	O
fed	O
through	O
two	O
rectified	B-Method
layers	E-Method
(	O
of	O
size	O
1536	O
and	O
1024	O
)	O
.	O

The	O
output	O
layer	O
was	O
a	O
simple	O
softmax	B-Method
layer	E-Method
(	O
see	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
.	O

All	O
the	O
weight	O
parameters	O
were	O
initialized	O
uniformly	O
between	O
and	O
,	O
and	O
the	O
model	O
was	O
trained	O
until	O
the	O
validation	B-Metric
perplexity	E-Metric
did	O
not	O
improve	O
for	O
10	O
epochs	O
.	O

After	O
training	O
,	O
the	O
language	B-Method
model	E-Method
achieved	O
a	O
perplexity	S-Metric
of	O
45.80	O
.	O

The	O
validation	O
set	O
was	O
a	O
random	O
selection	O
of	O
0.1	O
%	O
of	O
the	O
corpus	O
.	O

The	O
model	O
was	O
used	O
to	O
score	O
partial	O
translations	O
during	O
the	O
decoding	B-Task
process	E-Task
,	O
which	O
generally	O
leads	O
to	O
higher	O
gains	O
in	O
BLEU	B-Metric
score	E-Metric
than	O
n	B-Method
-	I-Method
best	I-Method
list	I-Method
rescoring	E-Method
.	O

To	O
address	O
the	O
computational	B-Metric
complexity	E-Metric
of	O
using	O
a	O
CSLM	S-Method
in	O
the	O
decoder	O
a	O
buffer	O
was	O
used	O
to	O
aggregate	O
n	O
-	O
grams	O
during	O
the	O
stack	B-Task
-	I-Task
search	E-Task
performed	O
by	O
the	O
decoder	O
.	O

Only	O
when	O
the	O
buffer	O
is	O
full	O
,	O
or	O
a	O
stack	O
is	O
about	O
to	O
be	O
pruned	O
,	O
the	O
n	O
-	O
grams	O
are	O
scored	O
by	O
the	O
CSLM	S-Method
.	O

This	O
allows	O
us	O
to	O
perform	O
fast	O
matrix	B-Task
-	I-Task
matrix	I-Task
multiplication	E-Task
on	O
GPU	O
using	O
Theano	O
.	O

subsection	O
:	O
Quantitative	B-Task
Analysis	E-Task
We	O
tried	O
the	O
following	O
combinations	O
:	O
Baseline	S-Method
configuration	O
Baseline	S-Method
+	O
RNN	B-Method
Baseline	E-Method
+	O
CSLM	S-Method
+	O
RNN	B-Method
Baseline	E-Method
+	O
CSLM	S-Method
+	O
RNN	S-Method
+	O
Word	B-Method
penalty	E-Method
The	O
results	O
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
expected	O
,	O
adding	O
features	O
computed	O
by	O
neural	B-Method
networks	E-Method
consistently	O
improves	O
the	O
performance	O
over	O
the	O
baseline	O
performance	O
.	O

The	O
best	O
performance	O
was	O
achieved	O
when	O
we	O
used	O
both	O
CSLM	S-Method
and	O
the	O
phrase	O
scores	O
from	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

This	O
suggests	O
that	O
the	O
contributions	O
of	O
the	O
CSLM	S-Method
and	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
are	O
not	O
too	O
correlated	O
and	O
that	O
one	O
can	O
expect	O
better	O
results	O
by	O
improving	O
each	O
method	O
independently	O
.	O

Furthermore	O
,	O
we	O
tried	O
penalizing	O
the	O
number	O
of	O
words	O
that	O
are	O
unknown	O
to	O
the	O
neural	B-Method
networks	E-Method
(	O
i.e.	O
words	O
which	O
are	O
not	O
in	O
the	O
shortlist	O
)	O
.	O

We	O
do	O
so	O
by	O
simply	O
adding	O
the	O
number	O
of	O
unknown	O
words	O
as	O
an	O
additional	O
feature	O
the	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

However	O
,	O
in	O
this	O
case	O
we	O
were	O
not	O
able	O
to	O
achieve	O
better	O
performance	O
on	O
the	O
test	O
set	O
,	O
but	O
only	O
on	O
the	O
development	O
set	O
.	O

subsection	O
:	O
Qualitative	B-Method
Analysis	E-Method
In	O
order	O
to	O
understand	O
where	O
the	O
performance	O
improvement	O
comes	O
from	O
,	O
we	O
analyze	O
the	O
phrase	O
pair	O
scores	O
computed	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
against	O
the	O
corresponding	O
from	O
the	O
translation	B-Method
model	E-Method
.	O

Since	O
the	O
existing	O
translation	B-Method
model	E-Method
relies	O
solely	O
on	O
the	O
statistics	O
of	O
the	O
phrase	O
pairs	O
in	O
the	O
corpus	O
,	O
we	O
expect	O
its	O
scores	O
to	O
be	O
better	O
estimated	O
for	O
the	O
frequent	O
phrases	O
but	O
badly	O
estimated	O
for	O
rare	O
phrases	O
.	O

Also	O
,	O
as	O
we	O
mentioned	O
earlier	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
we	O
further	O
expect	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
which	O
was	O
trained	O
without	O
any	O
frequency	O
information	O
to	O
score	O
the	O
phrase	O
pairs	O
based	O
rather	O
on	O
the	O
linguistic	O
regularities	O
than	O
on	O
the	O
statistics	O
of	O
their	O
occurrences	O
in	O
the	O
corpus	O
.	O

We	O
focus	O
on	O
those	O
pairs	O
whose	O
source	O
phrase	O
is	O
long	O
(	O
more	O
than	O
3	O
words	O
per	O
source	O
phrase	O
)	O
and	O
frequent	O
.	O

For	O
each	O
such	O
source	O
phrase	O
,	O
we	O
look	O
at	O
the	O
target	O
phrases	O
that	O
have	O
been	O
scored	O
high	O
either	O
by	O
the	O
translation	O
probability	O
or	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

Similarly	O
,	O
we	O
perform	O
the	O
same	O
procedure	O
with	O
those	O
pairs	O
whose	O
source	O
phrase	O
is	O
long	O
but	O
rare	O
in	O
the	O
corpus	O
.	O

Table	O
[	O
reference	O
]	O
lists	O
the	O
top	O
-	O
target	O
phrases	O
per	O
source	O
phrase	O
favored	O
either	O
by	O
the	O
translation	B-Method
model	E-Method
or	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

The	O
source	O
phrases	O
were	O
randomly	O
chosen	O
among	O
long	O
ones	O
having	O
more	O
than	O
4	O
or	O
5	O
words	O
.	O

In	O
most	O
cases	O
,	O
the	O
choices	O
of	O
the	O
target	O
phrases	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
are	O
closer	O
to	O
actual	O
or	O
literal	O
translations	O
.	O

We	O
can	O
observe	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
prefers	O
shorter	O
phrases	O
in	O
general	O
.	O

Interestingly	O
,	O
many	O
phrase	O
pairs	O
were	O
scored	O
similarly	O
by	O
both	O
the	O
translation	B-Method
model	E-Method
and	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
,	O
but	O
there	O
were	O
as	O
many	O
other	O
phrase	O
pairs	O
that	O
were	O
scored	O
radically	O
different	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
could	O
arise	O
from	O
the	O
proposed	O
approach	O
of	O
training	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
on	O
a	O
set	O
of	O
unique	O
phrase	O
pairs	O
,	O
discouraging	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
from	O
learning	O
simply	O
the	O
frequencies	O
of	O
the	O
phrase	O
pairs	O
from	O
the	O
corpus	O
,	O
as	O
explained	O
earlier	O
.	O

Furthermore	O
,	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
for	O
each	O
of	O
the	O
source	O
phrases	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
generated	O
samples	O
from	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

For	O
each	O
source	O
phrase	O
,	O
we	O
generated	O
50	O
samples	O
and	O
show	O
the	O
top	O
-	O
five	O
phrases	O
accordingly	O
to	O
their	O
scores	O
.	O

We	O
can	O
see	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
able	O
to	O
propose	O
well	O
-	O
formed	O
target	O
phrases	O
without	O
looking	O
at	O
the	O
actual	O
phrase	O
table	O
.	O

Importantly	O
,	O
the	O
generated	O
phrases	O
do	O
not	O
overlap	O
completely	O
with	O
the	O
target	O
phrases	O
from	O
the	O
phrase	O
table	O
.	O

This	O
encourages	O
us	O
to	O
further	O
investigate	O
the	O
possibility	O
of	O
replacing	O
the	O
whole	O
or	O
a	O
part	O
of	O
the	O
phrase	O
table	O
with	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
in	O
the	O
future	O
.	O

subsection	O
:	O
Word	B-Method
and	I-Method
Phrase	I-Method
Representations	E-Method
Since	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
not	O
specifically	O
designed	O
only	O
for	O
the	O
task	O
of	O
machine	B-Task
translation	E-Task
,	O
here	O
we	O
briefly	O
look	O
at	O
the	O
properties	O
of	O
the	O
trained	O
model	O
.	O

It	O
has	O
been	O
known	O
for	O
some	O
time	O
that	O
continuous	B-Method
space	I-Method
language	I-Method
models	E-Method
using	O
neural	B-Method
networks	E-Method
are	O
able	O
to	O
learn	O
semantically	S-Method
meaningful	O
embeddings	O
(	O
See	O
,	O
e.g.	O
,	O
)	O
.	O

Since	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
also	O
projects	O
to	O
and	O
maps	O
back	O
from	O
a	O
sequence	O
of	O
words	O
into	O
a	O
continuous	O
space	O
vector	O
,	O
we	O
expect	O
to	O
see	O
a	O
similar	O
property	O
with	O
the	O
proposed	O
model	O
as	O
well	O
.	O

The	O
left	O
plot	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
2–D	O
embedding	O
of	O
the	O
words	O
using	O
the	O
word	B-Method
embedding	I-Method
matrix	E-Method
learned	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

The	O
projection	O
was	O
done	O
by	O
the	O
recently	O
proposed	O
Barnes	B-Method
-	I-Method
Hut	I-Method
-	I-Method
SNE	E-Method
.	O

We	O
can	O
clearly	O
see	O
that	O
semantically	S-Method
similar	O
words	O
are	O
clustered	O
with	O
each	O
other	O
(	O
see	O
the	O
zoomed	O
-	O
in	O
plots	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
naturally	O
generates	O
a	O
continuous	B-Method
-	I-Method
space	I-Method
representation	I-Method
of	I-Method
a	I-Method
phrase	E-Method
.	O

The	O
representation	O
(	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
in	O
this	O
case	O
is	O
a	O
1000	O
-	O
dimensional	O
vector	O
.	O

Similarly	O
to	O
the	O
word	B-Method
representations	E-Method
,	O
we	O
visualize	O
the	O
representations	O
of	O
the	O
phrases	O
that	O
consists	O
of	O
four	O
or	O
more	O
words	O
using	O
the	O
Barnes	B-Method
-	I-Method
Hut	I-Method
-	I-Method
SNE	E-Method
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

From	O
the	O
visualization	O
,	O
it	O
is	O
clear	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
captures	O
both	O
semantic	O
and	O
syntactic	O
structures	O
of	O
the	O
phrases	O
.	O

For	O
instance	O
,	O
in	O
the	O
bottom	O
-	O
left	O
plot	O
,	O
most	O
of	O
the	O
phrases	O
are	O
about	O
the	O
duration	O
of	O
time	O
,	O
while	O
those	O
phrases	O
that	O
are	O
syntactically	O
similar	O
are	O
clustered	O
together	O
.	O

The	O
bottom	O
-	O
right	O
plot	O
shows	O
the	O
cluster	O
of	O
phrases	O
that	O
are	O
semantically	S-Method
similar	O
(	O
countries	O
or	O
regions	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
top	O
-	O
right	O
plot	O
shows	O
the	O
phrases	O
that	O
are	O
syntactically	O
similar	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
new	O
neural	B-Method
network	I-Method
architecture	E-Method
,	O
called	O
an	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
that	O
is	O
able	O
to	O
learn	O
the	O
mapping	O
from	O
a	O
sequence	O
of	O
an	O
arbitrary	O
length	O
to	O
another	O
sequence	O
,	O
possibly	O
from	O
a	O
different	O
set	O
,	O
of	O
an	O
arbitrary	O
length	O
.	O

The	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
able	O
to	O
either	O
score	O
a	O
pair	O
of	O
sequences	O
(	O
in	O
terms	O
of	O
a	O
conditional	O
probability	O
)	O
or	O
generate	O
a	O
target	O
sequence	O
given	O
a	O
source	O
sequence	O
.	O

Along	O
with	O
the	O
new	O
architecture	O
,	O
we	O
proposed	O
a	O
novel	O
hidden	B-Method
unit	E-Method
that	O
includes	O
a	O
reset	O
gate	O
and	O
an	O
update	O
gate	O
that	O
adaptively	O
control	O
how	O
much	O
each	O
hidden	B-Method
unit	E-Method
remembers	O
or	O
forgets	O
while	O
reading	O
/	O
generating	O
a	O
sequence	O
.	O

We	O
evaluated	O
the	O
proposed	O
model	O
with	O
the	O
task	O
of	O
statistical	B-Task
machine	I-Task
translation	E-Task
,	O
where	O
we	O
used	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
to	O
score	O
each	O
phrase	O
pair	O
in	O
the	O
phrase	O
table	O
.	O

Qualitatively	O
,	O
we	O
were	O
able	O
to	O
show	O
that	O
the	O
new	O
model	O
is	O
able	O
to	O
capture	O
linguistic	O
regularities	O
in	O
the	O
phrase	O
pairs	O
well	O
and	O
also	O
that	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
able	O
to	O
propose	O
well	O
-	O
formed	O
target	O
phrases	O
.	O

The	O
scores	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
were	O
found	O
to	O
improve	O
the	O
overall	O
translation	B-Metric
performance	E-Metric
in	O
terms	O
of	O
BLEU	B-Metric
scores	E-Metric
.	O

Also	O
,	O
we	O
found	O
that	O
the	O
contribution	O
by	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
is	O
rather	O
orthogonal	O
to	O
the	O
existing	O
approach	O
of	O
using	O
neural	B-Method
networks	E-Method
in	O
the	O
SMT	B-Method
system	E-Method
,	O
so	O
that	O
we	O
can	O
improve	O
further	O
the	O
performance	O
by	O
using	O
,	O
for	O
instance	O
,	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
and	O
the	O
neural	B-Method
net	I-Method
language	I-Method
model	E-Method
together	O
.	O

Our	O
qualitative	O
analysis	O
of	O
the	O
trained	O
model	O
shows	O
that	O
it	O
indeed	O
captures	O
the	O
linguistic	O
regularities	O
in	O
multiple	O
levels	O
i.e.	O
at	O
the	O
word	O
level	O
as	O
well	O
as	O
phrase	O
level	O
.	O

This	O
suggests	O
that	O
there	O
may	O
be	O
more	O
natural	B-Task
language	I-Task
related	I-Task
applications	E-Task
that	O
may	O
benefit	O
from	O
the	O
proposed	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
.	O

The	O
proposed	O
architecture	O
has	O
large	O
potential	O
for	O
further	O
improvement	O
and	O
analysis	O
.	O

One	O
approach	O
that	O
was	O
not	O
investigated	O
here	O
is	O
to	O
replace	O
the	O
whole	O
,	O
or	O
a	O
part	O
of	O
the	O
phrase	O
table	O
by	O
letting	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
propose	O
target	O
phrases	O
.	O

Also	O
,	O
noting	O
that	O
the	O
proposed	O
model	O
is	O
not	O
limited	O
to	O
being	O
used	O
with	O
written	O
language	O
,	O
it	O
will	O
be	O
an	O
important	O
future	O
research	O
to	O
apply	O
the	O
proposed	O
architecture	O
to	O
other	O
applications	O
such	O
as	O
speech	B-Task
transcription	E-Task
.	O

section	O
:	O
Acknowledgments	O
KC	O
,	O
BM	O
,	O
CG	O
,	O
DB	O
and	O
YB	O
would	O
like	O
to	O
thank	O
NSERC	O
,	O
Calcul	O
Québec	O
,	O
Compute	O
Canada	O
,	O
the	O
Canada	O
Research	O
Chairs	O
and	O
CIFAR	O
.	O

FB	O
and	O
HS	O
were	O
partially	O
funded	O
by	O
the	O
European	O
Commission	O
under	O
the	O
project	O
MateCat	O
,	O
and	O
by	O
DARPA	O
under	O
the	O
BOLT	O
project	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
In	O
this	O
document	O
,	O
we	O
describe	O
in	O
detail	O
the	O
architecture	O
of	O
the	O
RNN	S-Method
Encoder	O
–	O
Decoder	O
used	O
in	O
the	O
experiments	O
.	O

Let	O
us	O
denote	O
an	O
source	O
phrase	O
by	O
and	O
a	O
target	O
phrase	O
by	O
.	O

Each	O
phrase	O
is	O
a	O
sequence	O
of	O
-	O
dimensional	O
one	O
-	O
hot	O
vectors	O
,	O
such	O
that	O
only	O
one	O
element	O
of	O
the	O
vector	O
is	O
and	O
all	O
the	O
others	O
are	O
.	O

The	O
index	O
of	O
the	O
active	O
(	O
)	O
element	O
indicates	O
the	O
word	O
represented	O
by	O
the	O
vector	O
.	O

subsection	O
:	O
Encoder	O
Each	O
word	O
of	O
the	O
source	O
phrase	O
is	O
embedded	O
in	O
a	O
-	O
dimensional	O
vector	O
space	O
:	O
.	O

is	O
used	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
to	O
visualize	O
the	O
words	O
.	O

The	O
hidden	O
state	O
of	O
an	O
encoder	S-Method
consists	O
of	O
hidden	O
units	O
,	O
and	O
each	O
one	O
of	O
them	O
at	O
time	O
is	O
computed	O
by	O
where	O
and	O
are	O
a	O
logistic	B-Method
sigmoid	I-Method
function	E-Method
and	O
an	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
,	O
respectively	O
.	O

To	O
make	O
the	O
equations	O
uncluttered	O
,	O
we	O
omit	O
biases	O
.	O

The	O
initial	O
hidden	O
state	O
is	O
fixed	O
to	O
.	O

Once	O
the	O
hidden	O
state	O
at	O
the	O
step	O
(	O
the	O
end	O
of	O
the	O
source	O
phrase	O
)	O
is	O
computed	O
,	O
the	O
representation	O
of	O
the	O
source	O
phrase	O
is	O
subsubsection	O
:	O
Decoder	S-Method
The	O
decoder	O
starts	O
by	O
initializing	O
the	O
hidden	O
state	O
with	O
where	O
we	O
will	O
use	O
to	O
distinguish	O
parameters	O
of	O
the	O
decoder	O
from	O
those	O
of	O
the	O
encoder	S-Method
.	O

The	O
hidden	O
state	O
at	O
time	O
of	O
the	O
decoder	O
is	O
computed	O
by	O
where	O
and	O
is	O
an	O
all	O
-	O
zero	O
vector	O
.	O

Similarly	O
to	O
the	O
case	O
of	O
the	O
encoder	S-Method
,	O
is	O
an	O
embedding	O
of	O
a	O
target	O
word	O
.	O

Unlike	O
the	O
encoder	S-Method
which	O
simply	O
encodes	O
the	O
source	O
phrase	O
,	O
the	O
decoder	S-Method
is	O
learned	O
to	O
generate	O
a	O
target	O
phrase	O
.	O

At	O
each	O
time	O
,	O
the	O
decoder	S-Method
computes	O
the	O
probability	O
of	O
generating	O
-	O
th	O
word	O
by	O
where	O
the	O
-	O
element	O
of	O
is	O
and	O
In	O
short	O
,	O
the	O
is	O
a	O
so	O
-	O
called	O
maxout	O
unit	O
.	O

For	O
the	O
computational	B-Metric
efficiency	E-Metric
,	O
instead	O
of	O
a	O
single	O
-	O
matrix	O
output	O
weight	O
,	O
we	O
use	O
a	O
product	O
of	O
two	O
matrices	O
such	O
that	O
where	O
and	O
.	O

appendix	O
:	O
Word	B-Method
and	I-Method
Phrase	I-Method
Representations	E-Method
Here	O
,	O
we	O
show	O
enlarged	O
plots	O
of	O
the	O
word	B-Method
and	I-Method
phrase	I-Method
representations	E-Method
in	O
Figs	O
.	O

[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
.	O

arxiv	O
arxiv	O
document	O
:	O
Image	B-Task
-	I-Task
to	I-Task
-	I-Task
Image	I-Task
Translation	E-Task
with	O
Conditional	B-Method
Adversarial	I-Method
Networks	E-Method
We	O
investigate	O
conditional	B-Method
adversarial	I-Method
networks	E-Method
as	O
a	O
general	O
-	O
purpose	O
solution	O
to	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
problems	E-Task
.	O

These	O
networks	O
not	O
only	O
learn	O
the	O
mapping	O
from	O
input	O
image	O
to	O
output	O
image	O
,	O
but	O
also	O
learn	O
a	O
loss	O
function	O
to	O
train	O
this	O
mapping	O
.	O

This	O
makes	O
it	O
possible	O
to	O
apply	O
the	O
same	O
generic	O
approach	O
to	O
problems	O
that	O
traditionally	O
would	O
require	O
very	O
different	O
loss	B-Method
formulations	E-Method
.	O

We	O
demonstrate	O
that	O
this	O
approach	O
is	O
effective	O
at	O
synthesizing	B-Task
photos	E-Task
from	O
label	B-Method
maps	E-Method
,	O
reconstructing	O
objects	O
from	O
edge	O
maps	O
,	O
and	O
colorizing	O
images	O
,	O
among	O
other	O
tasks	O
.	O

Indeed	O
,	O
since	O
the	O
release	O
of	O
the	O
pix2pix	S-Method
software	O
associated	O
with	O
this	O
paper	O
,	O
a	O
large	O
number	O
of	O
internet	O
users	O
(	O
many	O
of	O
them	O
artists	O
)	O
have	O
posted	O
their	O
own	O
experiments	O
with	O
our	O
system	O
,	O
further	O
demonstrating	O
its	O
wide	O
applicability	O
and	O
ease	O
of	O
adoption	O
without	O
the	O
need	O
for	O
parameter	B-Method
tweaking	E-Method
.	O

As	O
a	O
community	O
,	O
we	O
no	O
longer	O
hand	O
-	O
engineer	O
our	O
mapping	B-Method
functions	E-Method
,	O
and	O
this	O
work	O
suggests	O
we	O
can	O
achieve	O
reasonable	O
results	O
without	O
hand	O
-	O
engineering	O
our	O
loss	B-Method
functions	E-Method
either	O
.	O

section	O
:	O
Introduction	O
Many	O
problems	O
in	O
image	B-Task
processing	E-Task
,	O
computer	B-Task
graphics	E-Task
,	O
and	O
computer	B-Task
vision	E-Task
can	O
be	O
posed	O
as	O
“	O
translating	O
”	O
an	O
input	O
image	O
into	O
a	O
corresponding	O
output	O
image	O
.	O

Just	O
as	O
a	O
concept	O
may	O
be	O
expressed	O
in	O
either	O
English	O
or	O
French	O
,	O
a	O
scene	O
may	O
be	O
rendered	O
as	O
an	O
RGB	O
image	O
,	O
a	O
gradient	O
field	O
,	O
an	O
edge	O
map	O
,	O
a	O
semantic	O
label	O
map	O
,	O
etc	O
.	O

In	O
analogy	O
to	O
automatic	B-Task
language	I-Task
translation	E-Task
,	O
we	O
define	O
automatic	B-Task
image	I-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	E-Task
as	O
the	O
task	O
of	O
translating	O
one	O
possible	O
representation	B-Task
of	I-Task
a	I-Task
scene	E-Task
into	O
another	O
,	O
given	O
sufficient	O
training	O
data	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Traditionally	O
,	O
each	O
of	O
these	O
tasks	O
has	O
been	O
tackled	O
with	O
separate	O
,	O
special	O
-	O
purpose	O
machinery	O
(	O
e.g.	O
,	O
)	O
,	O
despite	O
the	O
fact	O
that	O
the	O
setting	O
is	O
always	O
the	O
same	O
:	O
predict	O
pixels	O
from	O
pixels	O
.	O

Our	O
goal	O
in	O
this	O
paper	O
is	O
to	O
develop	O
a	O
common	O
framework	O
for	O
all	O
these	O
problems	O
.	O

The	O
community	O
has	O
already	O
taken	O
significant	O
steps	O
in	O
this	O
direction	O
,	O
with	O
convolutional	B-Method
neural	I-Method
nets	E-Method
(	O
CNNs	S-Method
)	O
becoming	O
the	O
common	O
workhorse	O
behind	O
a	O
wide	O
variety	O
of	O
image	B-Task
prediction	I-Task
problems	E-Task
.	O

CNNs	S-Method
learn	O
to	O
minimize	O
a	O
loss	O
function	O
–	O
an	O
objective	O
that	O
scores	O
the	O
quality	O
of	O
results	O
–	O
and	O
although	O
the	O
learning	B-Method
process	E-Method
is	O
automatic	O
,	O
a	O
lot	O
of	O
manual	O
effort	O
still	O
goes	O
into	O
designing	O
effective	O
losses	O
.	O

In	O
other	O
words	O
,	O
we	O
still	O
have	O
to	O
tell	O
the	O
CNN	O
what	O
we	O
wish	O
it	O
to	O
minimize	O
.	O

But	O
,	O
just	O
like	O
King	O
Midas	O
,	O
we	O
must	O
be	O
careful	O
what	O
we	O
wish	O
for	O
!	O
If	O
we	O
take	O
a	O
naive	O
approach	O
and	O
ask	O
the	O
CNN	S-Method
to	O
minimize	O
the	O
Euclidean	O
distance	O
between	O
predicted	O
and	O
ground	O
truth	O
pixels	O
,	O
it	O
will	O
tend	O
to	O
produce	O
blurry	O
results	O
.	O

This	O
is	O
because	O
Euclidean	O
distance	O
is	O
minimized	O
by	O
averaging	O
all	O
plausible	O
outputs	O
,	O
which	O
causes	O
blurring	O
.	O

Coming	O
up	O
with	O
loss	B-Method
functions	E-Method
that	O
force	O
the	O
CNN	S-Method
to	O
do	O
what	O
we	O
really	O
want	O
–	O
e.g.	O
,	O
output	O
sharp	O
,	O
realistic	O
images	O
–	O
is	O
an	O
open	O
problem	O
and	O
generally	O
requires	O
expert	O
knowledge	O
.	O

It	O
would	O
be	O
highly	O
desirable	O
if	O
we	O
could	O
instead	O
specify	O
only	O
a	O
high	O
-	O
level	O
goal	O
,	O
like	O
“	O
make	O
the	O
output	O
indistinguishable	O
from	O
reality	O
”	O
,	O
and	O
then	O
automatically	O
learn	O
a	O
loss	B-Method
function	E-Method
appropriate	O
for	O
satisfying	O
this	O
goal	O
.	O

Fortunately	O
,	O
this	O
is	O
exactly	O
what	O
is	O
done	O
by	O
the	O
recently	O
proposed	O
Generative	B-Method
Adversarial	I-Method
Networks	E-Method
(	O
GANs	S-Method
)	O
.	O

GANs	S-Method
learn	O
a	O
loss	O
that	O
tries	O
to	O
classify	O
if	O
the	O
output	O
image	O
is	O
real	O
or	O
fake	O
,	O
while	O
simultaneously	O
training	O
a	O
generative	B-Method
model	E-Method
to	O
minimize	O
this	O
loss	O
.	O

Blurry	O
images	O
will	O
not	O
be	O
tolerated	O
since	O
they	O
look	O
obviously	O
fake	O
.	O

Because	O
GANs	S-Method
learn	O
a	O
loss	S-Method
that	O
adapts	O
to	O
the	O
data	O
,	O
they	O
can	O
be	O
applied	O
to	O
a	O
multitude	O
of	O
tasks	O
that	O
traditionally	O
would	O
require	O
very	O
different	O
kinds	O
of	O
loss	O
functions	O
.	O

In	O
this	O
paper	O
,	O
we	O
explore	O
GANs	S-Method
in	O
the	O
conditional	B-Task
setting	E-Task
.	O

Just	O
as	O
GANs	S-Method
learn	O
a	O
generative	B-Method
model	I-Method
of	I-Method
data	E-Method
,	O
conditional	B-Method
GANs	E-Method
(	O
cGANs	S-Method
)	O
learn	O
a	O
conditional	B-Method
generative	I-Method
model	E-Method
.	O

This	O
makes	O
cGANs	S-Method
suitable	O
for	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
tasks	E-Task
,	O
where	O
we	O
condition	O
on	O
an	O
input	O
image	O
and	O
generate	O
a	O
corresponding	O
output	O
image	O
.	O

GANs	S-Method
have	O
been	O
vigorously	O
studied	O
in	O
the	O
last	O
two	O
years	O
and	O
many	O
of	O
the	O
techniques	O
we	O
explore	O
in	O
this	O
paper	O
have	O
been	O
previously	O
proposed	O
.	O

Nonetheless	O
,	O
earlier	O
papers	O
have	O
focused	O
on	O
specific	O
applications	O
,	O
and	O
it	O
has	O
remained	O
unclear	O
how	O
effective	O
image	O
-	O
conditional	O
GANs	S-Method
can	O
be	O
as	O
a	O
general	O
-	O
purpose	O
solution	O
for	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	E-Task
.	O

Our	O
primary	O
contribution	O
is	O
to	O
demonstrate	O
that	O
on	O
a	O
wide	O
variety	O
of	O
problems	O
,	O
conditional	B-Method
GANs	E-Method
produce	O
reasonable	O
results	O
.	O

Our	O
second	O
contribution	O
is	O
to	O
present	O
a	O
simple	O
framework	O
sufficient	O
to	O
achieve	O
good	O
results	O
,	O
and	O
to	O
analyze	O
the	O
effects	O
of	O
several	O
important	O
architectural	O
choices	O
.	O

Code	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
phillipi	O
/	O
pix2pix	S-Method
.	O

section	O
:	O
Related	O
work	O
Structured	B-Method
losses	E-Method
for	O
image	B-Task
modeling	I-Task
Image	I-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
problems	E-Task
are	O
often	O
formulated	O
as	O
per	B-Task
-	I-Task
pixel	I-Task
classification	I-Task
or	I-Task
regression	E-Task
(	O
e.g.	O
,	O
)	O
.	O

These	O
formulations	O
treat	O
the	O
output	O
space	O
as	O
“	O
unstructured	O
”	O
in	O
the	O
sense	O
that	O
each	O
output	O
pixel	O
is	O
considered	O
conditionally	O
independent	O
from	O
all	O
others	O
given	O
the	O
input	O
image	O
.	O

Conditional	O
GANs	S-Method
instead	O
learn	O
a	O
structured	O
loss	O
.	O

Structured	O
losses	O
penalize	O
the	O
joint	O
configuration	O
of	O
the	O
output	O
.	O

A	O
large	O
body	O
of	O
literature	O
has	O
considered	O
losses	O
of	O
this	O
kind	O
,	O
with	O
methods	O
including	O
conditional	B-Method
random	I-Method
fields	E-Method
,	O
the	O
SSIM	B-Method
metric	E-Method
,	O
feature	B-Method
matching	E-Method
,	O
nonparametric	B-Method
losses	E-Method
,	O
the	O
convolutional	B-Method
pseudo	I-Method
-	I-Method
prior	E-Method
,	O
and	O
losses	S-Method
based	O
on	O
matching	B-Method
covariance	I-Method
statistics	E-Method
.	O

The	O
conditional	O
GAN	S-Method
is	O
different	O
in	O
that	O
the	O
loss	O
is	O
learned	O
,	O
and	O
can	O
,	O
in	O
theory	O
,	O
penalize	O
any	O
possible	O
structure	O
that	O
differs	O
between	O
output	O
and	O
target	O
.	O

Conditional	O
GANs	S-Method
We	O
are	O
not	O
the	O
first	O
to	O
apply	O
GANs	S-Method
in	O
the	O
conditional	O
setting	O
.	O

Prior	O
and	O
concurrent	O
works	O
have	O
conditioned	O
GANs	S-Method
on	O
discrete	O
labels	O
,	O
text	O
,	O
and	O
,	O
indeed	O
,	O
images	O
.	O

The	O
image	B-Method
-	I-Method
conditional	I-Method
models	E-Method
have	O
tackled	O
image	B-Task
prediction	E-Task
from	O
a	O
normal	B-Task
map	E-Task
,	O
future	B-Task
frame	I-Task
prediction	E-Task
,	O
product	B-Task
photo	I-Task
generation	E-Task
,	O
and	O
image	B-Task
generation	E-Task
from	O
sparse	O
annotations	O
(	O
c.f	O
.	O

for	O
an	O
autoregressive	B-Method
approach	E-Method
to	O
the	O
same	O
problem	O
)	O
.	O

Several	O
other	O
papers	O
have	O
also	O
used	O
GANs	S-Method
for	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
mappings	E-Task
,	O
but	O
only	O
applied	O
the	O
GAN	S-Method
unconditionally	O
,	O
relying	O
on	O
other	O
terms	O
(	O
such	O
as	O
L2	B-Method
regression	E-Method
)	O
to	O
force	O
the	O
output	O
to	O
be	O
conditioned	O
on	O
the	O
input	O
.	O

These	O
papers	O
have	O
achieved	O
impressive	O
results	O
on	O
inpainting	S-Task
,	O
future	B-Task
state	I-Task
prediction	E-Task
,	O
image	B-Task
manipulation	E-Task
guided	O
by	O
user	O
constraints	O
,	O
style	B-Task
transfer	E-Task
,	O
and	O
superresolution	S-Task
.	O

Each	O
of	O
the	O
methods	O
was	O
tailored	O
for	O
a	O
specific	O
application	O
.	O

Our	O
framework	O
differs	O
in	O
that	O
nothing	O
is	O
application	O
-	O
specific	O
.	O

This	O
makes	O
our	O
setup	O
considerably	O
simpler	O
than	O
most	O
others	O
.	O

Our	O
method	O
also	O
differs	O
from	O
the	O
prior	O
works	O
in	O
several	O
architectural	O
choices	O
for	O
the	O
generator	B-Method
and	I-Method
discriminator	E-Method
.	O

Unlike	O
past	O
work	O
,	O
for	O
our	O
generator	O
we	O
use	O
a	O
“	B-Method
U	I-Method
-	I-Method
Net”	I-Method
-	I-Method
based	I-Method
architecture	E-Method
,	O
and	O
for	O
our	O
discriminator	S-Method
we	O
use	O
a	O
convolutional	B-Method
“	I-Method
PatchGAN	I-Method
”	I-Method
classifier	E-Method
,	O
which	O
only	O
penalizes	O
structure	O
at	O
the	O
scale	O
of	O
image	O
patches	O
.	O

A	O
similar	O
PatchGAN	B-Method
architecture	E-Method
was	O
previously	O
proposed	O
in	O
to	O
capture	O
local	O
style	O
statistics	O
.	O

Here	O
we	O
show	O
that	O
this	O
approach	O
is	O
effective	O
on	O
a	O
wider	O
range	O
of	O
problems	O
,	O
and	O
we	O
investigate	O
the	O
effect	O
of	O
changing	O
the	O
patch	O
size	O
.	O

section	O
:	O
Method	O
GANs	S-Method
are	O
generative	B-Method
models	E-Method
that	O
learn	O
a	O
mapping	O
from	O
random	O
noise	O
vector	O
to	O
output	O
image	O
,	O
.	O

In	O
contrast	O
,	O
conditional	B-Method
GANs	E-Method
learn	O
a	O
mapping	O
from	O
observed	O
image	O
and	O
random	O
noise	O
vector	O
,	O
to	O
,	O
.	O

The	O
generator	O
is	O
trained	O
to	O
produce	O
outputs	O
that	O
can	O
not	O
be	O
distinguished	O
from	O
“	O
real	O
”	O
images	O
by	O
an	O
adversarially	O
trained	O
discriminator	S-Method
,	O
,	O
which	O
is	O
trained	O
to	O
do	O
as	O
well	O
as	O
possible	O
at	O
detecting	O
the	O
generator	O
’s	O
“	O
fakes	O
”	O
.	O

This	O
training	O
procedure	O
is	O
diagrammed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Objective	O
The	O
objective	O
of	O
a	O
conditional	O
GAN	S-Method

can	O
be	O
expressed	O
as	O
where	O
tries	O
to	O
minimize	O
this	O
objective	O
against	O
an	O
adversarial	O
that	O
tries	O
to	O
maximize	O
it	O
,	O
i.e.	O
.	O


To	O
test	O
the	O
importance	O
of	O
conditioning	O
the	O
discriminator	O
,	O
we	O
also	O
compare	O
to	O
an	O
unconditional	B-Method
variant	E-Method
in	O
which	O
the	O
discriminator	O
does	O
not	O
observe	O
:	O
Previous	O
approaches	O
have	O
found	O
it	O
beneficial	O
to	O
mix	O
the	O
GAN	S-Method
objective	O
with	O
a	O
more	O
traditional	O
loss	S-Metric
,	O
such	O
as	O
L2	B-Method
distance	E-Method
.	O

The	O
discriminator	O
’s	O
job	O
remains	O
unchanged	O
,	O
but	O
the	O
generator	O
is	O
tasked	O
to	O
not	O
only	O
fool	O
the	O
discriminator	S-Method
but	O
also	O
to	O
be	O
near	O
the	O
ground	O
truth	O
output	O
in	O
an	O
L2	O
sense	O
.	O

We	O
also	O
explore	O
this	O
option	O
,	O
using	O
L1	O
distance	O
rather	O
than	O
L2	O
as	O
L1	O
encourages	O
less	O
blurring	O
:	O
Our	O
final	O
objective	O
is	O
Without	O
,	O
the	O
net	O
could	O
still	O
learn	O
a	O
mapping	O
from	O
to	O
,	O
but	O
would	O
produce	O
deterministic	O
outputs	O
,	O
and	O
therefore	O
fail	O
to	O
match	O
any	O
distribution	O
other	O
than	O
a	O
delta	O
function	O
.	O

Past	O
conditional	B-Method
GANs	E-Method
have	O
acknowledged	O
this	O
and	O
provided	O
Gaussian	O
noise	O
as	O
an	O
input	O
to	O
the	O
generator	S-Method
,	O
in	O
addition	O
to	O
(	O
e.g.	O
,	O
)	O
.	O

In	O
initial	O
experiments	O
,	O
we	O
did	O
not	O
find	O
this	O
strategy	O
effective	O
–	O
the	O
generator	S-Method

simply	O
learned	O
to	O
ignore	O
the	O
noise	O
–	O
which	O
is	O
consistent	O
with	O
Mathieu	O
et	O
al	O
.	O

.	O


Instead	O
,	O
for	O
our	O
final	O
models	O
,	O
we	O
provide	O
noise	O
only	O
in	O
the	O
form	O
of	O
dropout	S-Method
,	O
applied	O
on	O
several	O
layers	O
of	O
our	O
generator	O
at	O
both	O
training	O
and	O
test	O
time	O
.	O

Despite	O
the	O
dropout	O
noise	O
,	O
we	O
observe	O
only	O
minor	O
stochasticity	O
in	O
the	O
output	O
of	O
our	O
nets	O
.	O

Designing	O
conditional	B-Method
GANs	E-Method
that	O
produce	O
highly	O
stochastic	O
output	O
,	O
and	O
thereby	O
capture	O
the	O
full	O
entropy	O
of	O
the	O
conditional	B-Method
distributions	E-Method
they	O
model	O
,	O
is	O
an	O
important	O
question	O
left	O
open	O
by	O
the	O
present	O
work	O
.	O

subsection	O
:	O
Network	B-Method
architectures	E-Method
We	O
adapt	O
our	O
generator	B-Method
and	I-Method
discriminator	I-Method
architectures	E-Method
from	O
those	O
in	O
.	O

Both	O
generator	S-Method
and	O
discriminator	S-Method
use	O
modules	O
of	O
the	O
form	O
convolution	B-Method
-	I-Method
BatchNorm	I-Method
-	I-Method
ReLu	E-Method
.	O

Details	O
of	O
the	O
architecture	O
are	O
provided	O
in	O
the	O
supplemental	O
materials	O
online	O
,	O
with	O
key	O
features	O
discussed	O
below	O
.	O

subsubsection	O
:	O
Generator	S-Method
with	O
skips	O
A	O
defining	O
feature	O
of	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
problems	E-Task
is	O
that	O
they	O
map	O
a	O
high	O
resolution	O
input	O
grid	O
to	O
a	O
high	O
resolution	O
output	O
grid	O
.	O

In	O
addition	O
,	O
for	O
the	O
problems	O
we	O
consider	O
,	O
the	O
input	O
and	O
output	O
differ	O
in	O
surface	O
appearance	O
,	O
but	O
both	O
are	O
renderings	O
of	O
the	O
same	O
underlying	O
structure	O
.	O

Therefore	O
,	O
structure	O
in	O
the	O
input	O
is	O
roughly	O
aligned	O
with	O
structure	O
in	O
the	O
output	O
.	O

We	O
design	O
the	O
generator	B-Method
architecture	E-Method
around	O
these	O
considerations	O
.	O

Many	O
previous	O
solutions	O
to	O
problems	O
in	O
this	O
area	O
have	O
used	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
network	E-Method
.	O

In	O
such	O
a	O
network	O
,	O
the	O
input	O
is	O
passed	O
through	O
a	O
series	O
of	O
layers	O
that	O
progressively	O
downsample	O
,	O
until	O
a	O
bottleneck	O
layer	O
,	O
at	O
which	O
point	O
the	O
process	O
is	O
reversed	O
.	O

Such	O
a	O
network	O
requires	O
that	O
all	O
information	O
flow	O
pass	O
through	O
all	O
the	O
layers	O
,	O
including	O
the	O
bottleneck	O
.	O

For	O
many	O
image	B-Task
translation	I-Task
problems	E-Task
,	O
there	O
is	O
a	O
great	O
deal	O
of	O
low	O
-	O
level	O
information	O
shared	O
between	O
the	O
input	O
and	O
output	O
,	O
and	O
it	O
would	O
be	O
desirable	O
to	O
shuttle	O
this	O
information	O
directly	O
across	O
the	O
net	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
image	B-Task
colorization	E-Task
,	O
the	O
input	O
and	O
output	O
share	O
the	O
location	O
of	O
prominent	O
edges	O
.	O

To	O
give	O
the	O
generator	O
a	O
means	O
to	O
circumvent	O
the	O
bottleneck	O
for	O
information	O
like	O
this	O
,	O
we	O
add	O
skip	O
connections	O
,	O
following	O
the	O
general	O
shape	O
of	O
a	O
“	O
U	B-Method
-	I-Method
Net	E-Method
”	O
.	O

Specifically	O
,	O
we	O
add	O
skip	O
connections	O
between	O
each	O
layer	O
and	O
layer	O
,	O
where	O
is	O
the	O
total	O
number	O
of	O
layers	O
.	O

Each	O
skip	B-Method
connection	E-Method
simply	O
concatenates	O
all	O
channels	O
at	O
layer	O
with	O
those	O
at	O
layer	O
.	O

subsubsection	O
:	O
Markovian	B-Method
discriminator	E-Method
(	O
PatchGAN	S-Method
)	O
It	O
is	O
well	O
known	O
that	O
the	O
L2	O
loss	O
–	O
and	O
L1	O
,	O
see	O
Figure	O
[	O
reference	O
]	O
–	O
produces	O
blurry	O
results	O
on	O
image	B-Task
generation	I-Task
problems	E-Task
.	O

Although	O
these	O
losses	O
fail	O
to	O
encourage	O
high	O
-	O
frequency	O
crispness	O
,	O
in	O
many	O
cases	O
they	O
nonetheless	O
accurately	O
capture	O
the	O
low	O
frequencies	O
.	O

For	O
problems	O
where	O
this	O
is	O
the	O
case	O
,	O
we	O
do	O
not	O
need	O
an	O
entirely	O
new	O
framework	O
to	O
enforce	O
correctness	O
at	O
the	O
low	O
frequencies	O
.	O

L1	O
will	O
already	O
do	O
.	O

This	O
motivates	O
restricting	O
the	O
GAN	S-Method
discriminator	O
to	O
only	O
model	O
high	O
-	O
frequency	O
structure	O
,	O
relying	O
on	O
an	O
L1	B-Method
term	E-Method
to	O
force	O
low	O
-	O
frequency	O
correctness	O
(	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
.	O

In	O
order	O
to	O
model	O
high	O
-	O
frequencies	O
,	O
it	O
is	O
sufficient	O
to	O
restrict	O
our	O
attention	O
to	O
the	O
structure	O
in	O
local	O
image	O
patches	O
.	O

Therefore	O
,	O
we	O
design	O
a	O
discriminator	B-Method
architecture	E-Method
–	O
which	O
we	O
term	O
a	O
Patch	B-Method
GAN	E-Method
–	O
that	O
only	O
penalizes	O
structure	O
at	O
the	O
scale	O
of	O
patches	O
.	O

This	O
discriminator	O
tries	O
to	O
classify	O
if	O
each	O
patch	O
in	O
an	O
image	O
is	O
real	O
or	O
fake	O
.	O

We	O
run	O
this	O
discriminator	O
convolutionally	O
across	O
the	O
image	O
,	O
averaging	O
all	O
responses	O
to	O
provide	O
the	O
ultimate	O
output	O
of	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
demonstrate	O
that	O
can	O
be	O
much	O
smaller	O
than	O
the	O
full	O
size	O
of	O
the	O
image	O
and	O
still	O
produce	O
high	O
quality	O
results	O
.	O

This	O
is	O
advantageous	O
because	O
a	O
smaller	O
PatchGAN	O
has	O
fewer	O
parameters	O
,	O
runs	O
faster	O
,	O
and	O
can	O
be	O
applied	O
to	O
arbitrarily	O
large	O
images	O
.	O

Such	O
a	O
discriminator	S-Method
effectively	O
models	O
the	O
image	O
as	O
a	O
Markov	B-Method
random	I-Method
field	E-Method
,	O
assuming	O
independence	O
between	O
pixels	O
separated	O
by	O
more	O
than	O
a	O
patch	O
diameter	O
.	O

This	O
connection	O
was	O
previously	O
explored	O
in	O
,	O
and	O
is	O
also	O
the	O
common	O
assumption	O
in	O
models	O
of	O
texture	O
and	O
style	O
.	O

Therefore	O
,	O
our	O
PatchGAN	S-Method
can	O
be	O
understood	O
as	O
a	O
form	O
of	O
texture	B-Task
/	I-Task
style	I-Task
loss	E-Task
.	O

subsection	O
:	O
Optimization	S-Task
and	O
inference	S-Task
To	O
optimize	O
our	O
networks	O
,	O
we	O
follow	O
the	O
standard	O
approach	O
from	O
:	O
we	O
alternate	O
between	O
one	O
gradient	B-Method
descent	I-Method
step	E-Method
on	O
,	O
then	O
one	O
step	O
on	O
.	O

As	O
suggested	O
in	O
the	O
original	O
GAN	S-Method
paper	O
,	O
rather	O
than	O
training	O
to	O
minimize	O
,	O
we	O
instead	O
train	O
to	O
maximize	O
.	O

In	O
addition	O
,	O
we	O
divide	O
the	O
objective	O
by	O
while	O
optimizing	S-Task
,	O
which	O
slows	O
down	O
the	O
rate	O
at	O
which	O
learns	O
relative	O
to	O
.	O

We	O
use	O
minibatch	B-Method
SGD	E-Method
and	O
apply	O
the	O
Adam	B-Method
solver	E-Method
,	O
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
,	O
and	O
momentum	O
parameters	O
,	O
.	O

At	O
inference	O
time	O
,	O
we	O
run	O
the	O
generator	B-Method
net	E-Method
in	O
exactly	O
the	O
same	O
manner	O
as	O
during	O
the	O
training	O
phase	O
.	O

This	O
differs	O
from	O
the	O
usual	O
protocol	O
in	O
that	O
we	O
apply	O
dropout	O
at	O
test	O
time	O
,	O
and	O
we	O
apply	O
batch	B-Method
normalization	E-Method
using	O
the	O
statistics	O
of	O
the	O
test	O
batch	O
,	O
rather	O
than	O
aggregated	O
statistics	O
of	O
the	O
training	O
batch	O
.	O

This	O
approach	O
to	O
batch	B-Task
normalization	E-Task
,	O
when	O
the	O
batch	O
size	O
is	O
set	O
to	O
1	O
,	O
has	O
been	O
termed	O
“	O
instance	B-Method
normalization	E-Method
”	O
and	O
has	O
been	O
demonstrated	O
to	O
be	O
effective	O
at	O
image	B-Task
generation	I-Task
tasks	E-Task
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
batch	O
sizes	O
between	O
1	O
and	O
10	O
depending	O
on	O
the	O
experiment	O
.	O

section	O
:	O
Experiments	O
To	O
explore	O
the	O
generality	O
of	O
conditional	B-Method
GANs	E-Method
,	O
we	O
test	O
the	O
method	O
on	O
a	O
variety	O
of	O
tasks	O
and	O
datasets	O
,	O
including	O
both	O
graphics	B-Task
tasks	E-Task
,	O
like	O
photo	B-Task
generation	E-Task
,	O
and	O
vision	B-Task
tasks	E-Task
,	O
like	O
semantic	B-Task
segmentation	E-Task
:	O
Semantic	O
labels↔photo	O
,	O
trained	O
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
.	O

Architectural	O
labels→photo	O
,	O
trained	O
on	O
CMP	B-Method
Facades	E-Method
.	O

Map↔aerial	B-Material
photo	E-Material
,	O
trained	O
on	O
data	O
scraped	O
from	O
Google	O
Maps	O
.	O

BW→color	O
photos	O
,	O
trained	O
on	O
.	O

Edges→photo	O
,	O
trained	O
on	O
data	O
from	O
and	O
;	O
binary	O
edges	O
generated	O
using	O
the	O
HED	B-Method
edge	I-Method
detector	E-Method
plus	O
postprocessing	S-Method
.	O

Sketch→photo	S-Method
:	O
tests	O
edges	B-Method
photo	I-Method
models	E-Method
on	O
human	O
-	O
drawn	O
sketches	O
from	O
.	O

Day→night	O
,	O
trained	O
on	O
.	O

Thermal→color	O
photos	O
,	O
trained	O
on	O
data	O
from	O
.	O

Photo	O
with	O
missing	O
pixels→inpainted	O
photo	O
,	O
trained	O
on	O
Paris	O
StreetView	O
from	O
.	O

Details	O
of	O
training	O
on	O
each	O
of	O
these	O
datasets	O
are	O
provided	O
in	O
the	O
supplemental	O
materials	O
online	O
.	O

In	O
all	O
cases	O
,	O
the	O
input	O
and	O
output	O
are	O
simply	O
1	O
-	O
3	O
channel	O
images	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figures	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
.	O

Several	O
failure	O
cases	O
are	O
highlighted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

More	O
comprehensive	O
results	O
are	O
available	O
at	O
https:	O
//	O
phillipi.github.io	O
/	O
pix2pix	S-Method
/	O
.	O

Data	O
requirements	O
and	O
speed	O
We	O
note	O
that	O
decent	O
results	O
can	O
often	O
be	O
obtained	O
even	O
on	O
small	O
datasets	O
.	O

Our	O
facade	O
training	O
set	O
consists	O
of	O
just	O
400	O
images	O
(	O
see	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
day	O
to	O
night	O
training	O
set	O
consists	O
of	O
only	O
91	O
unique	O
webcams	O
(	O
see	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

On	O
datasets	O
of	O
this	O
size	O
,	O
training	S-Task
can	O
be	O
very	O
fast	O
:	O
for	O
example	O
,	O
the	O
results	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
took	O
less	O
than	O
two	O
hours	O
of	O
training	O
on	O
a	O
single	O
Pascal	O
Titan	O
X	O
GPU	O
.	O

At	O
test	O
time	O
,	O
all	O
models	O
run	O
in	O
well	O
under	O
a	O
second	O
on	O
this	O
GPU	O
.	O

subsection	O
:	O
Evaluation	B-Metric
metrics	E-Metric
Evaluating	O
the	O
quality	S-Metric
of	O
synthesized	O
images	O
is	O
an	O
open	O
and	O
difficult	O
problem	O
.	O

Traditional	O
metrics	O
such	O
as	O
per	B-Metric
-	I-Metric
pixel	I-Metric
mean	I-Metric
-	I-Metric
squared	I-Metric
error	E-Metric
do	O
not	O
assess	O
joint	O
statistics	O
of	O
the	O
result	O
,	O
and	O
therefore	O
do	O
not	O
measure	O
the	O
very	O
structure	O
that	O
structured	O
losses	O
aim	O
to	O
capture	O
.	O

To	O
more	O
holistically	O
evaluate	O
the	O
visual	B-Metric
quality	E-Metric
of	O
our	O
results	O
,	O
we	O
employ	O
two	O
tactics	O
.	O

First	O
,	O
we	O
run	O
“	O
real	O
vs.	O
fake	O
”	O
perceptual	O
studies	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
.	O

For	O
graphics	B-Task
problems	E-Task
like	O
colorization	S-Task
and	O
photo	B-Task
generation	E-Task
,	O
plausibility	O
to	O
a	O
human	O
observer	O
is	O
often	O
the	O
ultimate	O
goal	O
.	O

Therefore	O
,	O
we	O
test	O
our	O
map	B-Task
generation	E-Task
,	O
aerial	B-Task
photo	I-Task
generation	E-Task
,	O
and	O
image	B-Task
colorization	E-Task
using	O
this	O
approach	O
.	O

Second	O
,	O
we	O
measure	O
whether	O
or	O
not	O
our	O
synthesized	O
cityscapes	O
are	O
realistic	O
enough	O
that	O
off	O
-	O
the	O
-	O
shelf	O
recognition	B-Method
system	E-Method
can	O
recognize	O
the	O
objects	O
in	O
them	O
.	O

This	O
metric	O
is	O
similar	O
to	O
the	O
“	O
inception	B-Metric
score	I-Metric
”	E-Metric
from	O
,	O
the	O
object	B-Metric
detection	I-Metric
evaluation	E-Metric
in	O
,	O
and	O
the	O
“	O
semantic	B-Metric
interpretability	I-Metric
”	I-Metric
measures	E-Metric
in	O
and	O
.	O

AMT	B-Task
perceptual	I-Task
studies	E-Task
For	O
our	O
AMT	O
experiments	O
,	O
we	O
followed	O
the	O
protocol	O
from	O
:	O
Turkers	O
were	O
presented	O
with	O
a	O
series	O
of	O
trials	O
that	O
pitted	O
a	O
“	O
real	O
”	O
image	O
against	O
a	O
“	O
fake	O
”	O
image	O
generated	O
by	O
our	O
algorithm	O
.	O

On	O
each	O
trial	O
,	O
each	O
image	O
appeared	O
for	O
1	O
second	O
,	O
after	O
which	O
the	O
images	O
disappeared	O
and	O
Turkers	O
were	O
given	O
unlimited	O
time	O
to	O
respond	O
as	O
to	O
which	O
was	O
fake	O
.	O

The	O
first	O
10	O
images	O
of	O
each	O
session	O
were	O
practice	O
and	O
Turkers	O
were	O
given	O
feedback	O
.	O

No	O
feedback	O
was	O
provided	O
on	O
the	O
40	O
trials	O
of	O
the	O
main	O
experiment	O
.	O

Each	O
session	O
tested	O
just	O
one	O
algorithm	O
at	O
a	O
time	O
,	O
and	O
Turkers	O
were	O
not	O
allowed	O
to	O
complete	O
more	O
than	O
one	O
session	O
.	O

Turkers	O
evaluated	O
each	O
algorithm	O
.	O

Unlike	O
,	O
we	O
did	O
not	O
include	O
vigilance	O
trials	O
.	O

For	O
our	O
colorization	B-Task
experiments	E-Task
,	O
the	O
real	O
and	O
fake	O
images	O
were	O
generated	O
from	O
the	O
same	O
grayscale	O
input	O
.	O

For	O
map	B-Material
aerial	I-Material
photo	E-Material
,	O
the	O
real	O
and	O
fake	O
images	O
were	O
not	O
generated	O
from	O
the	O
same	O
input	O
,	O
in	O
order	O
to	O
make	O
the	O
task	O
more	O
difficult	O
and	O
avoid	O
floor	O
-	O
level	O
results	O
.	O

For	O
map	B-Material
aerial	I-Material
photo	E-Material
,	O
we	O
trained	O
on	O
resolution	O
images	O
,	O
but	O
exploited	O
fully	B-Method
-	I-Method
convolutional	I-Method
translation	E-Method
(	O
described	O
above	O
)	O
to	O
test	O
on	O
images	O
,	O
which	O
were	O
then	O
downsampled	O
and	O
presented	O
to	O
Turkers	O
at	O
resolution	O
.	O

For	O
colorization	S-Task
,	O
we	O
trained	O
and	O
tested	O
on	O
resolution	O
images	O
and	O
presented	O
the	O
results	O
to	O
Turkers	O
at	O
this	O
same	O
resolution	O
.	O

“	O
FCN	B-Metric
-	I-Metric
score	E-Metric
”	O
While	O
quantitative	B-Task
evaluation	E-Task
of	O
generative	B-Method
models	E-Method
is	O
known	O
to	O
be	O
challenging	O
,	O
recent	O
works	O
have	O
tried	O
using	O
pre	O
-	O
trained	O
semantic	B-Method
classifiers	E-Method
to	O
measure	O
the	O
discriminability	O
of	O
the	O
generated	O
stimuli	O
as	O
a	O
pseudo	B-Metric
-	I-Metric
metric	E-Metric
.	O

The	O
intuition	O
is	O
that	O
if	O
the	O
generated	O
images	O
are	O
realistic	O
,	O
classifiers	S-Method
trained	O
on	O
real	O
images	O
will	O
be	O
able	O
to	O
classify	O
the	O
synthesized	O
image	O
correctly	O
as	O
well	O
.	O

To	O
this	O
end	O
,	O
we	O
adopt	O
the	O
popular	O
FCN	B-Method
-	I-Method
8s	I-Method
architecture	E-Method
for	O
semantic	B-Task
segmentation	E-Task
,	O
and	O
train	O
it	O
on	O
the	O
cityscapes	B-Material
dataset	E-Material
.	O

We	O
then	O
score	O
synthesized	O
photos	O
by	O
the	O
classification	B-Metric
accuracy	E-Metric
against	O
the	O
labels	O
these	O
photos	O
were	O
synthesized	O
from	O
.	O

subsection	O
:	O
Analysis	O
of	O
the	O
objective	B-Metric
function	E-Metric
Which	O
components	O
of	O
the	O
objective	O
in	O
Eqn	O
.	O

[	O
reference	O
]	O
are	O
important	O
?	O
We	O
run	O
ablation	B-Task
studies	E-Task
to	O
isolate	O
the	O
effect	O
of	O
the	O
L1	O
term	O
,	O
the	O
GAN	S-Method
term	O
,	O
and	O
to	O
compare	O
using	O
a	O
discriminator	S-Method
conditioned	O
on	O
the	O
input	O
(	O
cGAN	S-Method
,	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
against	O
using	O
an	O
unconditional	B-Method
discriminator	E-Method
(	O
GAN	S-Method
,	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
qualitative	O
effects	O
of	O
these	O
variations	O
on	O
two	O
labels	B-Task
photo	I-Task
problems	E-Task
.	O

L1	S-Method
alone	O
leads	O
to	O
reasonable	O
but	O
blurry	O
results	O
.	O

The	O
cGAN	S-Method
alone	O
(	O
setting	O
in	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
gives	O
much	O
sharper	O
results	O
but	O
introduces	O
visual	O
artifacts	O
on	O
certain	O
applications	O
.	O

Adding	O
both	O
terms	O
together	O
(	O
with	O
)	O
reduces	O
these	O
artifacts	O
.	O

We	O
quantify	O
these	O
observations	O
using	O
the	O
FCN	B-Metric
-	I-Metric
score	E-Metric
on	O
the	O
cityscapes	B-Task
labels	I-Task
photo	I-Task
task	E-Task
(	O
Table	O
[	O
reference	O
]	O
)	O
:	O
the	O
GAN	S-Method
-	O
based	O
objectives	O
achieve	O
higher	O
scores	O
,	O
indicating	O
that	O
the	O
synthesized	O
images	O
include	O
more	O
recognizable	O
structure	O
.	O

We	O
also	O
test	O
the	O
effect	O
of	O
removing	O
conditioning	O
from	O
the	O
discriminator	S-Method
(	O
labeled	O
as	O
GAN	S-Method
)	O
.	O

In	O
this	O
case	O
,	O
the	O
loss	O
does	O
not	O
penalize	O
mismatch	O
between	O
the	O
input	O
and	O
output	O
;	O
it	O
only	O
cares	O
that	O
the	O
output	O
look	O
realistic	O
.	O

This	O
variant	O
results	O
in	O
poor	O
performance	O
;	O
examining	O
the	O
results	O
reveals	O
that	O
the	O
generator	O
collapsed	O
into	O
producing	O
nearly	O
the	O
exact	O
same	O
output	O
regardless	O
of	O
input	O
photograph	O
.	O

Clearly	O
,	O
it	O
is	O
important	O
,	O
in	O
this	O
case	O
,	O
that	O
the	O
loss	S-Metric
measure	O
the	O
quality	O
of	O
the	O
match	O
between	O
input	O
and	O
output	O
,	O
and	O
indeed	O
cGAN	S-Method
performs	O
much	O
better	O
than	O
GAN	S-Method
.	O

Note	O
,	O
however	O
,	O
that	O
adding	O
an	O
L1	O
term	O
also	O
encourages	O
that	O
the	O
output	O
respect	O
the	O
input	O
,	O
since	O
the	O
L1	O
loss	O
penalizes	O
the	O
distance	O
between	O
ground	O
truth	O
outputs	O
,	O
which	O
correctly	O
match	O
the	O
input	O
,	O
and	O
synthesized	O
outputs	O
,	O
which	O
may	O
not	O
.	O

Correspondingly	O
,	O
L1	B-Method
+	I-Method
GAN	E-Method
is	O
also	O
effective	O
at	O
creating	O
realistic	B-Task
renderings	E-Task
that	O
respect	O
the	O
input	O
label	O
maps	O
.	O

Combining	O
all	O
terms	O
,	O
L1	O
+	O
cGAN	S-Method
,	O
performs	O
similarly	O
well	O
.	O

Colorfulness	O
A	O
striking	O
effect	O
of	O
conditional	B-Method
GANs	E-Method
is	O
that	O
they	O
produce	O
sharp	O
images	O
,	O
hallucinating	O
spatial	O
structure	O
even	O
where	O
it	O
does	O
not	O
exist	O
in	O
the	O
input	O
label	O
map	O
.	O

One	O
might	O
imagine	O
cGANs	S-Method
have	O
a	O
similar	O
effect	O
on	O
“	O
sharpening	O
”	O
in	O
the	O
spectral	O
dimension	O
–	O
i.e.	O
making	O
images	O
more	O
colorful	O
.	O

Just	O
as	O
L1	S-Method
will	O
incentivize	O
a	O
blur	O
when	O
it	O
is	O
uncertain	O
where	O
exactly	O
to	O
locate	O
an	O
edge	O
,	O
it	O
will	O
also	O
incentivize	O
an	O
average	O
,	O
grayish	O
color	O
when	O
it	O
is	O
uncertain	O
which	O
of	O
several	O
plausible	O
color	O
values	O
a	O
pixel	O
should	O
take	O
on	O
.	O

Specially	O
,	O
L1	O
will	O
be	O
minimized	O
by	O
choosing	O
the	O
median	O
of	O
the	O
conditional	O
probability	O
density	O
function	O
over	O
possible	O
colors	O
.	O

An	O
adversarial	B-Method
loss	E-Method
,	O
on	O
the	O
other	O
hand	O
,	O
can	O
in	O
principle	O
become	O
aware	O
that	O
grayish	O
outputs	O
are	O
unrealistic	O
,	O
and	O
encourage	O
matching	O
the	O
true	O
color	O
distribution	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
investigate	O
whether	O
our	O
cGANs	S-Method
actually	O
achieve	O
this	O
effect	O
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
.	O

The	O
plots	O
show	O
the	O
marginal	O
distributions	O
over	O
output	O
color	O
values	O
in	O
Lab	O
color	O
space	O
.	O

The	O
ground	B-Metric
truth	I-Metric
distributions	E-Metric
are	O
shown	O
with	O
a	O
dotted	O
line	O
.	O

It	O
is	O
apparent	O
that	O
L1	O
leads	O
to	O
a	O
narrower	O
distribution	O
than	O
the	O
ground	O
truth	O
,	O
confirming	O
the	O
hypothesis	O
that	O
L1	O
encourages	O
average	O
,	O
grayish	O
colors	O
.	O

Using	O
a	O
cGAN	S-Method
,	O
on	O
the	O
other	O
hand	O
,	O
pushes	O
the	O
output	O
distribution	O
closer	O
to	O
the	O
ground	O
truth	O
.	O

subsection	O
:	O
Analysis	O
of	O
the	O
generator	B-Method
architecture	E-Method
A	O
U	B-Method
-	I-Method
Net	I-Method
architecture	E-Method
allows	O
low	O
-	O
level	O
information	O
to	O
shortcut	O
across	O
the	O
network	O
.	O

Does	O
this	O
lead	O
to	O
better	O
results	O
?	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
compare	O
the	O
U	B-Method
-	I-Method
Net	E-Method
against	O
an	O
encoder	B-Method
-	I-Method
decoder	E-Method
on	O
cityscape	B-Task
generation	E-Task
.	O

The	O
encoder	B-Method
-	I-Method
decoder	E-Method
is	O
created	O
simply	O
by	O
severing	O
the	O
skip	O
connections	O
in	O
the	O
U	O
-	O
Net	O
.	O

The	O
encoder	B-Method
-	I-Method
decoder	E-Method
is	O
unable	O
to	O
learn	O
to	O
generate	O
realistic	O
images	O
in	O
our	O
experiments	O
.	O

The	O
advantages	O
of	O
the	O
U	B-Method
-	I-Method
Net	E-Method
appear	O
not	O
to	O
be	O
specific	O
to	O
conditional	B-Method
GANs	E-Method
:	O
when	O
both	O
U	B-Method
-	I-Method
Net	E-Method
and	O
encoder	B-Method
-	I-Method
decoder	E-Method
are	O
trained	O
with	O
an	O
L1	O
loss	O
,	O
the	O
U	B-Method
-	I-Method
Net	E-Method
again	O
achieves	O
the	O
superior	O
results	O
.	O

subsection	O
:	O
From	O
PixelGANs	S-Method
to	O
PatchGANs	S-Method
to	O
ImageGANs	S-Method
We	O
test	O
the	O
effect	O
of	O
varying	O
the	O
patch	O
size	O
of	O
our	O
discriminator	O
receptive	O
fields	O
,	O
from	O
a	O
“	O
PixelGAN	S-Method
”	O
to	O
a	O
full	O
“	O
ImageGAN	O
”	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
qualitative	O
results	O
of	O
this	O
analysis	O
and	O
Table	O
[	O
reference	O
]	O
quantifies	O
the	O
effects	O
using	O
the	O
FCN	B-Metric
-	I-Metric
score	E-Metric
.	O

Note	O
that	O
elsewhere	O
in	O
this	O
paper	O
,	O
unless	O
specified	O
,	O
all	O
experiments	O
use	O
PatchGANs	S-Method
,	O
and	O
for	O
this	O
section	O
all	O
experiments	O
use	O
an	O
L1	O
+	O
cGAN	S-Method
loss	O
.	O

The	O
PixelGAN	S-Method
has	O
no	O
effect	O
on	O
spatial	O
sharpness	O
but	O
does	O
increase	O
the	O
colorfulness	O
of	O
the	O
results	O
(	O
quantified	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

For	O
example	O
,	O
the	O
bus	O
in	O
Figure	O
[	O
reference	O
]	O
is	O
painted	O
gray	O
when	O
the	O
net	O
is	O
trained	O
with	O
an	O
L1	B-Method
loss	E-Method
,	O
but	O
becomes	O
red	O
with	O
the	O
PixelGAN	B-Method
loss	E-Method
.	O

Color	B-Method
histogram	I-Method
matching	E-Method
is	O
a	O
common	O
problem	O
in	O
image	B-Task
processing	E-Task
,	O
and	O
PixelGANs	S-Method
may	O
be	O
a	O
promising	O
lightweight	O
solution	O
.	O

Using	O
a	O
PatchGAN	S-Method
is	O
sufficient	O
to	O
promote	O
sharp	O
outputs	O
,	O
and	O
achieves	O
good	O
FCN	B-Metric
-	I-Metric
scores	E-Metric
,	O
but	O
also	O
leads	O
to	O
tiling	O
artifacts	O
.	O

The	O
PatchGAN	S-Method
alleviates	O
these	O
artifacts	O
and	O
achieves	O
slightly	O
better	O
scores	O
.	O

Scaling	O
beyond	O
this	O
,	O
to	O
the	O
full	O
ImageGAN	S-Method
,	O
does	O
not	O
appear	O
to	O
improve	O
the	O
visual	B-Metric
quality	E-Metric
of	O
the	O
results	O
,	O
and	O
in	O
fact	O
gets	O
a	O
considerably	O
lower	O
FCN	B-Metric
-	I-Metric
score	E-Metric
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

This	O
may	O
be	O
because	O
the	O
ImageGAN	O
has	O
many	O
more	O
parameters	O
and	O
greater	O
depth	O
than	O
the	O
PatchGAN	S-Method
,	O
and	O
may	O
be	O
harder	O
to	O
train	O
.	O

Fully	B-Method
-	I-Method
convolutional	I-Method
translation	E-Method
An	O
advantage	O
of	O
the	O
PatchGAN	S-Method
is	O
that	O
a	O
fixed	B-Method
-	I-Method
size	I-Method
patch	I-Method
discriminator	E-Method
can	O
be	O
applied	O
to	O
arbitrarily	O
large	O
images	O
.	O

We	O
may	O
also	O
apply	O
the	O
generator	B-Method
convolutionally	E-Method
,	O
on	O
larger	O
images	O
than	O
those	O
on	O
which	O
it	O
was	O
trained	O
.	O

We	O
test	O
this	O
on	O
the	O
map	B-Material
aerial	I-Material
photo	E-Material
task	O
.	O

After	O
training	O
a	O
generator	O
on	O
images	O
,	O
we	O
test	O
it	O
on	O
images	O
.	O

The	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
demonstrate	O
the	O
effectiveness	O
of	O
this	O
approach	O
.	O

subsection	O
:	O
Perceptual	B-Metric
validation	E-Metric
We	O
validate	O
the	O
perceptual	B-Metric
realism	E-Metric
of	O
our	O
results	O
on	O
the	O
tasks	O
of	O
map	B-Material
aerial	I-Material
photograph	E-Material
and	O
grayscale	O
color	O
.	O

Results	O
of	O
our	O
AMT	B-Method
experiment	E-Method
for	O
map	B-Task
photo	E-Task
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
aerial	B-Material
photos	E-Material
generated	O
by	O
our	O
method	O
fooled	O
participants	O
on	O
of	O
trials	O
,	O
significantly	O
above	O
the	O
L1	O
baseline	O
,	O
which	O
produces	O
blurry	O
results	O
and	O
nearly	O
never	O
fooled	O
participants	O
.	O

In	O
contrast	O
,	O
in	O
the	O
photo	O
map	O
direction	O
our	O
method	O
only	O
fooled	O
participants	O
on	O
%	O
of	O
trials	O
,	O
and	O
this	O
was	O
not	O
significantly	O
different	O
than	O
the	O
performance	O
of	O
the	O
L1	B-Method
baseline	E-Method
(	O
based	O
on	O
bootstrap	B-Method
test	E-Method
)	O
.	O

This	O
may	O
be	O
because	O
minor	O
structural	O
errors	O
are	O
more	O
visible	O
in	O
maps	O
,	O
which	O
have	O
rigid	O
geometry	O
,	O
than	O
in	O
aerial	B-Material
photographs	E-Material
,	O
which	O
are	O
more	O
chaotic	O
.	O

We	O
trained	O
colorization	S-Method
on	O
ImageNet	O
,	O
and	O
tested	O
on	O
the	O
test	O
split	O
introduced	O
by	O
.	O

Our	O
method	O
,	O
with	O
L1	O
+	O
cGAN	S-Method
loss	O
,	O
fooled	O
participants	O
on	O
of	O
trials	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

We	O
also	O
tested	O
the	O
results	O
of	O
and	O
a	O
variant	O
of	O
their	O
method	O
that	O
used	O
an	O
L2	O
loss	O
(	O
see	O
for	O
details	O
)	O
.	O

The	O
conditional	O
GAN	S-Method
scored	O
similarly	O
to	O
the	O
L2	B-Method
variant	E-Method
of	O
(	O
difference	O
insignificant	O
by	O
bootstrap	O
test	O
)	O
,	O
but	O
fell	O
short	O
of	O
’s	O
full	O
method	O
,	O
which	O
fooled	O
participants	O
on	O
of	O
trials	O
in	O
our	O
experiment	O
.	O

We	O
note	O
that	O
their	O
method	O
was	O
specifically	O
engineered	O
to	O
do	O
well	O
on	O
colorization	S-Task
.	O

subsection	O
:	O
Semantic	B-Task
segmentation	E-Task
Conditional	O
GANs	S-Method
appear	O
to	O
be	O
effective	O
on	O
problems	O
where	O
the	O
output	O
is	O
highly	O
detailed	O
or	O
photographic	O
,	O
as	O
is	O
common	O
in	O
image	B-Task
processing	I-Task
and	I-Task
graphics	I-Task
tasks	E-Task
.	O

What	O
about	O
vision	B-Task
problems	E-Task
,	O
like	O
semantic	B-Task
segmentation	E-Task
,	O
where	O
the	O
output	O
is	O
instead	O
less	O
complex	O
than	O
the	O
input	O
?	O
To	O
begin	O
to	O
test	O
this	O
,	O
we	O
train	O
a	O
cGAN	S-Method
(	O
with	O
/	O
without	O
L1	B-Method
loss	E-Method
)	O
on	O
cityscape	B-Material
photo	I-Material
labels	E-Material
.	O

Figure	O
[	O
reference	O
]	O
shows	O
qualitative	O
results	O
,	O
and	O
quantitative	B-Metric
classification	I-Metric
accuracies	E-Metric
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Interestingly	O
,	O
cGANs	S-Method
,	O
trained	O
without	O
the	O
L1	B-Method
loss	E-Method
,	O
are	O
able	O
to	O
solve	O
this	O
problem	O
at	O
a	O
reasonable	O
degree	O
of	O
accuracy	S-Metric
.	O

To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
demonstration	O
of	O
GANs	S-Method
successfully	O
generating	O
“	O
labels	O
”	O
,	O
which	O
are	O
nearly	O
discrete	O
,	O
rather	O
than	O
“	O
images	O
”	O
,	O
with	O
their	O
continuous	O
-	O
valued	O
variation	O
.	O

Although	O
cGANs	S-Method
achieve	O
some	O
success	O
,	O
they	O
are	O
far	O
from	O
the	O
best	O
available	O
method	O
for	O
solving	O
this	O
problem	O
:	O
simply	O
using	O
L1	B-Method
regression	E-Method
gets	O
better	O
scores	O
than	O
using	O
a	O
cGAN	S-Method
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
argue	O
that	O
for	O
vision	B-Task
problems	E-Task
,	O
the	O
goal	O
(	O
i.e.	O
predicting	O
output	O
close	O
to	O
the	O
ground	O
truth	O
)	O
may	O
be	O
less	O
ambiguous	O
than	O
graphics	B-Task
tasks	E-Task
,	O
and	O
reconstruction	O
losses	O
like	O
L1	S-Method
are	O
mostly	O
sufficient	O
.	O

subsection	O
:	O
Community	O
-	O
driven	O
Research	O
Since	O
the	O
initial	O
release	O
of	O
the	O
paper	O
and	O
our	O
pix2pix	S-Method
codebase	O
,	O
the	O
Twitter	O
community	O
,	O
including	O
computer	B-Task
vision	E-Task
and	O
graphics	B-Task
practitioners	E-Task
as	O
well	O
as	O
visual	B-Task
artists	E-Task
,	O
have	O
successfully	O
applied	O
our	O
framework	O
to	O
a	O
variety	O
of	O
novel	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
tasks	E-Task
,	O
far	O
beyond	O
the	O
scope	O
of	O
the	O
original	O
paper	O
.	O

Figure	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
show	O
just	O
a	O
few	O
examples	O
from	O
the	O
#	O
pix2pix	S-Method
hashtag	O
,	O
including	O
Background	B-Task
removal	E-Task
,	O
Palette	B-Task
generation	E-Task
,	O
Sketch	O
→	O
Portrait	O
,	O
Sketch→Pokemon	O
,	O
”	O
Do	O
as	O
I	O
Do	O
”	O
pose	B-Task
transfer	E-Task
,	O
Learning	O
to	O
see	O
:	O
Gloomy	O
Sunday	O
,	O
as	O
well	O
as	O
the	O
bizarrely	O
popular	O
#	O
edges2cats	O
and	O
#	O
fotogenerator	S-Method
.	O

Note	O
that	O
these	O
applications	O
are	O
creative	O
projects	O
,	O
were	O
not	O
obtained	O
in	O
controlled	O
,	O
scientific	O
conditions	O
,	O
and	O
may	O
rely	O
on	O
some	O
modifications	O
to	O
the	O
pix2pix	S-Method
code	O
we	O
released	O
.	O

Nonetheless	O
,	O
they	O
demonstrate	O
the	O
promise	O
of	O
our	O
approach	O
as	O
a	O
generic	O
commodity	B-Method
tool	E-Method
for	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
problems	E-Task
.	O

section	O
:	O
Conclusion	O
The	O
results	O
in	O
this	O
paper	O
suggest	O
that	O
conditional	B-Method
adversarial	I-Method
networks	E-Method
are	O
a	O
promising	O
approach	O
for	O
many	O
image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	I-Task
tasks	E-Task
,	O
especially	O
those	O
involving	O
highly	O
structured	O
graphical	O
outputs	O
.	O

These	O
networks	O
learn	O
a	O
loss	S-Metric
adapted	O
to	O
the	O
task	O
and	O
data	O
at	O
hand	O
,	O
which	O
makes	O
them	O
applicable	O
in	O
a	O
wide	O
variety	O
of	O
settings	O
.	O

paragraph	O
:	O
Acknowledgments	O
:	O
We	O
thank	O
Richard	O
Zhang	O
,	O
Deepak	O
Pathak	O
,	O
and	O
Shubham	O
Tulsiani	O
for	O
helpful	O
discussions	O
,	O
Saining	O
Xie	O
for	O
help	O
with	O
the	O
HED	B-Method
edge	I-Method
detector	E-Method

,	O
and	O
the	O
online	O
community	O
for	O
exploring	O
many	O
applications	O
and	O
suggesting	O
improvements	O
.	O

Thanks	O
to	O
Christopher	O
Hesse	O
,	O
Memo	O
Akten	O
,	O
Kaihu	O
Chen	O
,	O
Jack	O
Qiao	O
,	O
Mario	O
Klingemann	O
,	O
Brannon	O
Dorsey	O
,	O
Gerda	O
Bosman	O
,	O
Ivy	O
Tsai	O
,	O
and	O
Yann	O
LeCun	O
for	O
allowing	O
the	O
use	O
of	O
their	O
creations	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
NSF	O
SMA	O
-	O
1514512	O
,	O
NGA	O
NURI	O
,	O
IARPA	O
via	O
Air	O
Force	O
Research	O
Laboratory	O
,	O
Intel	O
Corp	O
,	O
Berkeley	O
Deep	O
Drive	O
,	O
and	O
hardware	O
donations	O
by	O
Nvidia	O
.	O

J.	O

-	O
Y.Z.	O
is	O
supported	O
by	O
the	O
Facebook	O
Graduate	O
Fellowship	O
.	O

Disclaimer	O
:	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
or	O
endorsements	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
IARPA	O
,	O
AFRL	O
or	O
the	O
U.S.	O
Government	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
Network	B-Method
architectures	E-Method
We	O
adapt	O
our	O
network	B-Method
architectures	E-Method
from	O
those	O
in	O
.	O

Code	O
for	O
the	O
models	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
phillipi	O
/	O
pix2pix	S-Method
.	O

Let	O
Ck	S-Method
denote	O
a	O
Convolution	B-Method
-	I-Method
BatchNorm	I-Method
-	I-Method
ReLU	I-Method
layer	E-Method
with	O
k	B-Method
filters	E-Method
.	O

CDk	S-Method
denotes	O
a	O
Convolution	B-Method
-	I-Method
BatchNorm	I-Method
-	I-Method
Dropout	I-Method
-	I-Method
ReLU	I-Method
layer	E-Method
with	O
a	O
dropout	O
rate	O
of	O
.	O

All	O
convolutions	S-Method
are	O
spatial	B-Method
filters	E-Method
applied	O
with	O
stride	O
2	O
.	O

Convolutions	S-Method
in	O
the	O
encoder	S-Method
,	O
and	O
in	O
the	O
discriminator	S-Method
,	O
downsample	O
by	O
a	O
factor	O
of	O
2	O
,	O
whereas	O
in	O
the	O
decoder	O
they	O
upsample	O
by	O
a	O
factor	O
of	O
2	O
.	O

subsubsection	O
:	O
Generator	B-Method
architectures	E-Method
The	O
encoder	B-Method
-	I-Method
decoder	I-Method
architecture	E-Method
consists	O
of	O
:	O
encoder	S-Method
:	O
C64	B-Method
-	I-Method
C128	I-Method
-	I-Method
C256	I-Method
-	I-Method
C512	I-Method
-	I-Method
C512	I-Method
-	I-Method
C512	I-Method
-	I-Method
C512	I-Method
-	I-Method
C512	I-Method
decoder	E-Method
:	O
CD512	B-Method
-	I-Method
CD512	I-Method
-	I-Method
CD512	I-Method
-	I-Method
C512	I-Method
-	I-Method
C256	I-Method
-	I-Method
C128	I-Method
-	I-Method
C64	E-Method
After	O
the	O
last	O
layer	O
in	O
the	O
decoder	S-Method
,	O
a	O
convolution	S-Method
is	O
applied	O
to	O
map	O
to	O
the	O
number	O
of	O
output	O
channels	O
(	O
3	O
in	O
general	O
,	O
except	O
in	O
colorization	O
,	O
where	O
it	O
is	O
2	O
)	O
,	O
followed	O
by	O
a	O
Tanh	B-Method
function	E-Method
.	O

As	O
an	O
exception	O
to	O
the	O
above	O
notation	O
,	O
BatchNorm	S-Method
is	O
not	O
applied	O
to	O
the	O
first	O
C64	O
layer	O
in	O
the	O
encoder	S-Method
.	O

All	O
ReLUs	O
in	O
the	O
encoder	O
are	O
leaky	O
,	O
with	O
slope	O
0.2	O
,	O
while	O
ReLUs	O
in	O
the	O
decoder	O
are	O
not	O
leaky	O
.	O

The	O
U	B-Method
-	I-Method
Net	I-Method
architecture	E-Method
is	O
identical	O
except	O
with	O
skip	O
connections	O
between	O
each	O
layer	O
in	O
the	O
encoder	O
and	O
layer	O
in	O
the	O
decoder	O
,	O
where	O
is	O
the	O
total	O
number	O
of	O
layers	O
.	O

The	O
skip	O
connections	O
concatenate	O
activations	O
from	O
layer	O
to	O
layer	O
.	O

This	O
changes	O
the	O
number	O
of	O
channels	O
in	O
the	O
decoder	O
:	O
U	B-Method
-	I-Method
Net	I-Method
decoder	E-Method
:	O
CD512	B-Method
-	I-Method
CD1024	I-Method
-	I-Method
CD1024	I-Method
-	I-Method
C1024	I-Method
-	I-Method
C1024	I-Method
-	I-Method
C512	I-Method
-	I-Method
C256	I-Method
-	I-Method
C128	E-Method
subsubsection	O
:	O
Discriminator	B-Method
architectures	E-Method
The	O
discriminator	B-Method
architecture	E-Method
is	O
:	O
C64	B-Method
-	I-Method
C128	I-Method
-	I-Method
C256	I-Method
-	I-Method
C512	E-Method
After	O
the	O
last	O
layer	O
,	O
a	O
convolution	S-Method
is	O
applied	O
to	O
map	O
to	O
a	O
1	O
-	O
dimensional	O
output	O
,	O
followed	O
by	O
a	O
Sigmoid	O
function	O
.	O

As	O
an	O
exception	O
to	O
the	O
above	O
notation	O
,	O
BatchNorm	S-Method
is	O
not	O
applied	O
to	O
the	O
first	O
C64	B-Method
layer	E-Method
.	O

All	O
ReLUs	S-Method
are	O
leaky	O
,	O
with	O
slope	O
0.2	O
.	O

All	O
other	O
discriminators	O
follow	O
the	O
same	O
basic	O
architecture	O
,	O
with	O
depth	O
varied	O
to	O
modify	O
the	O
receptive	O
field	O
size	O
:	O
discriminator	S-Method
:	O
C64	O
-	O
C128	O
(	O
note	O
,	O
in	O
this	O
special	O
case	O
,	O
all	O
convolutions	S-Method
are	O
spatial	B-Method
filters	E-Method
)	O
discriminator	S-Method
:	O
C64	B-Method
-	I-Method
C128×286286	I-Method
discriminator	E-Method
:	O
C64	O
-	O
C128	O
-	O
C256	O
-	O
C512	O
-	O
C512	O
-	O
C512	O
subsection	O
:	O
Training	O
details	O
Random	O
jitter	O
was	O
applied	O
by	O
resizing	O
the	O
input	O
images	O
to	O
and	O
then	O
randomly	O
cropping	O
back	O
to	O
size	O
.	O

All	O
networks	O
were	O
trained	O
from	O
scratch	O
.	O

Weights	O
were	O
initialized	O
from	O
a	O
Gaussian	B-Method
distribution	E-Method
with	O
mean	O
0	O
and	O
standard	O
deviation	O
0.02	O
.	O

Cityscapes	B-Material
labels→photo	E-Material
2975	O
training	O
images	O
from	O
the	O
Cityscapes	B-Material
training	I-Material
set	E-Material
,	O
trained	O
for	O
200	O
epochs	O
,	O
with	O
random	O
jitter	O
and	O
mirroring	O
.	O

We	O
used	O
the	O
Cityscapes	B-Material
validation	I-Material
set	E-Material
for	O
testing	O
.	O

To	O
compare	O
the	O
U	B-Method
-	I-Method
net	E-Method
against	O
an	O
encoder	B-Method
-	I-Method
decoder	E-Method
,	O
we	O
used	O
a	O
batch	O
size	O
of	O
10	O
,	O
whereas	O
for	O
the	O
objective	O
function	O
experiments	O
we	O
used	O
batch	O
size	O
1	O
.	O

We	O
find	O
that	O
batch	O
size	O
1	O
produces	O
better	O
results	O
for	O
the	O
U	B-Task
-	I-Task
net	E-Task
,	O
but	O
is	O
inappropriate	O
for	O
the	O
encoder	B-Method
-	I-Method
decoder	E-Method
.	O

This	O
is	O
because	O
we	O
apply	O
batchnorm	S-Method
on	O
all	O
layers	O
of	O
our	O
network	O
,	O
and	O
for	O
batch	O
size	O
1	O
this	O
operation	O
zeros	O
the	O
activations	O
on	O
the	O
bottleneck	O
layer	O
.	O

The	O
U	B-Method
-	I-Method
net	E-Method
can	O
skip	O
over	O
the	O
bottleneck	O
,	O
but	O
the	O
encoder	B-Method
-	I-Method
decoder	E-Method
can	O
not	O
,	O
and	O
so	O
the	O
encoder	B-Method
-	I-Method
decoder	E-Method
requires	O
a	O
batch	O
size	O
greater	O
than	O
1	O
.	O

Note	O
,	O
an	O
alternative	O
strategy	O
is	O
to	O
remove	O
batchnorm	O
from	O
the	O
bottleneck	B-Method
layer	E-Method
.	O

See	O
errata	O
for	O
more	O
details	O
.	O

Architectural	O
labels→photo	O
400	O
training	O
images	O
from	O
,	O
trained	O
for	O
200	O
epochs	O
,	O
batch	O
size	O
1	O
,	O
with	O
random	O
jitter	O
and	O
mirroring	O
.	O

Data	O
were	O
split	O
into	O
train	O
and	O
test	O
randomly	O
.	O

Maps↔aerial	B-Material
photograph	E-Material
1096	O
training	O
images	O
scraped	O
from	O
Google	O
Maps	O
,	O
trained	O
for	O
200	O
epochs	O
,	O
batch	O
size	O
1	O
,	O
with	O
random	O
jitter	O
and	O
mirroring	O
.	O

Images	O
were	O
sampled	O
from	O
in	O
and	O
around	O
New	O
York	O
City	O
.	O

Data	O
were	O
then	O
split	O
into	O
train	O
and	O
test	O
about	O
the	O
median	O
latitude	O
of	O
the	O
sampling	O
region	O
(	O
with	O
a	O
buffer	O
region	O
added	O
to	O
ensure	O
that	O
no	O
training	O
pixel	O
appeared	O
in	O
the	O
test	O
set	O
)	O
.	O

BW→color	S-Method
1.2	O
million	O
training	O
images	O
(	O
Imagenet	O
training	O
set	O
)	O
,	O
trained	O
for	O
epochs	O
,	O
batch	O
size	O
4	O
,	O
with	O
only	O
mirroring	O
,	O
no	O
random	O
jitter	O
.	O

Tested	O
on	O
subset	O
of	O
Imagenet	O
val	O
set	O
,	O
following	O
protocol	O
of	O
and	O
.	O

Edges→shoes	O
50k	O
training	O
images	O
from	O
UT	O
Zappos50	O
K	O
dataset	O
trained	O
for	O
15	O
epochs	O
,	O
batch	O
size	O
4	O
.	O

Data	O
were	O
split	O
into	O
train	O
and	O
test	O
randomly	O
.	O

Edges→Handbag	O
137	O
K	O
Amazon	O
Handbag	O
images	O
from	O
,	O
trained	O
for	O
15	O
epochs	O
,	O
batch	O
size	O
4	O
.	O

Data	O
were	O
split	O
into	O
train	O
and	O
test	O
randomly	O
.	O

Day→night	O
17823	O
training	O
images	O
extracted	O
from	O
91	O
webcams	O
,	O
from	O
trained	O
for	O
17	O
epochs	O
,	O
batch	O
size	O
4	O
,	O
with	O
random	O
jitter	O
and	O
mirroring	O
.	O

We	O
use	O
91	O
webcams	O
as	O
training	O
,	O
and	O
10	O
webcams	O
for	O
test	O
.	O

Thermal→color	O
photos	O
36609	O
training	O
images	O
from	O
set	O
00–05	O
of	O
,	O
trained	O
for	O
10	O
epochs	O
,	O
batch	O
size	O
4	O
.	O

Images	O
from	O
set	O
06	O
-	O
11	O
are	O
used	O
for	O
testing	O
.	O

Photo	O
with	O
missing	O
pixels→inpainted	O
photo	O
14900	O
training	O
images	O
from	O
,	O
trained	O
for	O
25	O
epochs	O
,	O
batch	O
size	O
4	O
,	O
and	O
tested	O
on	O
100	O
held	O
out	O
images	O
following	O
the	O
split	O
of	O
.	O

subsection	O
:	O
Errata	O
For	O
all	O
experiments	O
reported	O
in	O
this	O
paper	O
with	O
batch	O
size	O
1	O
,	O
the	O
activations	O
of	O
the	O
bottleneck	O
layer	O
are	O
zeroed	O
by	O
the	O
batchnorm	B-Method
operation	E-Method
,	O
effectively	O
making	O
the	O
innermost	B-Method
layer	E-Method
skipped	O
.	O

This	O
issue	O
can	O
be	O
fixed	O
by	O
removing	O
batchnorm	S-Method
from	O
this	O
layer	O
,	O
as	O
has	O
been	O
done	O
in	O
the	O
public	O
code	O
.	O

We	O
observe	O
little	O
difference	O
with	O
this	O
change	O
and	O
therefore	O
leave	O
the	O
experiments	O
as	O
is	O
in	O
the	O
paper	O
.	O

subsection	O
:	O
Change	O
log	O
arXiv	O
v2	O
Reran	B-Method
generator	I-Method
architecture	E-Method
comparisons	O
(	O
Section	O
[	O
reference	O
]	O
)	O
with	O
batch	O
size	O
equal	O
to	O
10	O
rather	O
than	O
1	O
,	O
so	O
that	O
bottleneck	O
layer	O
is	O
not	O
zeroed	O
(	O
see	O
Errata	O
)	O
.	O

Reran	O
FCN	O
-	O
scores	O
with	O
minor	O
details	O
cleaned	O
up	O
(	O
results	O
saved	O
losslessly	O
as	O
pngs	O
,	O
removed	O
unecessary	O
downsampling	O
)	O
.	O

FCN	B-Metric
-	I-Metric
scores	E-Metric
computed	O
using	O
scripts	O
at	O
https:	O
//	O
github.com	O
/	O
phillipi	O
/	O
pix2pix	S-Method
/	O
tree	O
/	O
master	O
/	O
scripts	O
/	O
eval_cityscapes	O
,	O
commit	O
d7e7b8b	O
.	O

Updated	O
several	O
figures	O
and	O
text	O
.	O

Added	O
additional	O
results	O
on	O
thermal	O
color	O
photos	O
and	O
inpainting	S-Task
,	O
as	O
well	O
as	O
community	O
contributions	O
.	O

arXiv	O
v3	O
Added	O
additional	O
results	O
on	O
community	O
contributions	O
.	O

Fixed	O
minor	O
typos	O
.	O

	O
section	O
:	O
Abstract	O
section	O
:	O
Introduction	O
Texts	O
in	O
natural	O
scenes	O
(	O
e.g.	O
,	O
street	O
nameplates	O
,	O
store	O
names	O
,	O
good	O
names	O
)	O
play	O
an	O
important	O
role	O
in	O
our	O
daily	O
life	O
.	O

They	O
carry	O
essential	O
information	O
about	O
the	O
environment	O
.	O

After	O
understanding	O
scene	O
texts	O
,	O
they	O
can	O
be	O
used	O
in	O
many	O
areas	O
,	O
such	O
as	O
text	B-Task
-	I-Task
based	I-Task
retrieval	E-Task
,	O
translation	S-Task
,	O
etc	O
.	O

There	O
are	O
usually	O
two	O
key	O
steps	O
to	O
understand	O
scene	O
texts	O
:	O
text	B-Task
detection	E-Task
and	O
text	B-Task
recognition	E-Task
.	O

This	O
paper	O
focuses	O
on	O
scene	B-Task
text	I-Task
detection	E-Task
.	O

Scene	B-Task
text	I-Task
detection	E-Task
is	O
challenging	O
because	O
scene	O
texts	O
have	O
different	O
sizes	O
,	O
width	O
-	O
height	O
aspect	O
ratios	O
,	O
font	O
styles	O
,	O
lighting	O
,	O
perspective	O
distortion	O
,	O
orientation	O
,	O
etc	O
.	O

As	O
the	O
orientation	O
information	O
is	O
useful	O
for	O
scene	B-Task
text	I-Task
recognition	E-Task
and	O
other	O
tasks	O
,	O
scene	B-Task
text	I-Task
detection	E-Task
is	O
different	O
from	O
common	O
object	B-Task
detection	I-Task
tasks	E-Task
that	O
the	O
text	O
orientation	O
should	O
be	O
also	O
be	O
predicted	O
in	O
addition	O
to	O
the	O
axis	O
-	O
aligned	O
bounding	O
box	O
information	O
.	O

While	O
most	O
previous	O
text	B-Method
detection	I-Method
methods	E-Method
are	O
designed	O
for	O
detecting	B-Task
horizontal	I-Task
or	I-Task
near	I-Task
-	I-Task
horizontal	I-Task
texts	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
some	O
methods	O
try	O
to	O
address	O
the	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
text	I-Task
detection	E-Task
problem	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Recently	O
,	O
arbitraryoriented	B-Task
scene	I-Task
text	I-Task
detection	E-Task
is	O
a	O
hot	O
research	O
area	O
,	O
which	O
can	O
be	O
seen	O
from	O
the	O
frequent	O
result	O
updates	O
in	O
ICDAR2015	S-Material
Robust	O
Reading	O
competition	O
in	O
incidental	B-Task
scene	I-Task
text	I-Task
detection	E-Task
[	O
reference	O
]	O
.	O

While	O
traditional	O
text	B-Method
detection	I-Method
methods	E-Method
are	O
based	O
on	O
sliding	O
-	O
window	O
or	O
Connected	B-Method
Components	E-Method
(	O
CCs	S-Method
)	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
deep	B-Method
learning	I-Method
based	I-Method
methods	E-Method
have	O
been	O
widely	O
studied	O
recently	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

This	O
paper	O
presents	O
a	O
Rotational	B-Method
Region	I-Method
CNN	E-Method
(	O
R	B-Method
2	I-Method
CNN	E-Method
)	O
for	O
detecting	B-Task
arbitrary	I-Task
-	I-Task
oriented	I-Task
scene	I-Task
texts	E-Task
.	O

It	O
is	O
based	O
on	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	I-Method
architecture	E-Method
[	O
reference	O
]	O
.	O

Figure	O
1	O
shows	O
the	O
procedure	O
of	O
the	O
proposed	O
method	O
.	O

Figure	O
1	O
(	O
a	O
)	O
is	O
the	O
original	O
input	O
image	O
.	O

We	O
first	O
use	O
the	O
RPN	S-Method
to	O
propose	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
that	O
enclose	O
the	O
texts	O
(	O
Figure	O
1	O
(	O
b	O
)	O
)	O
.	O

Then	O
we	O
classify	O
the	O
proposals	O
,	O
refine	O
the	O
axis	O
-	O
aligned	O
boxes	O
and	O
predict	O
the	O
inclined	O
minimum	O
area	O
boxes	O
with	O
pooled	O
features	O
of	O
different	O
pooled	O
sizes	O
(	O
Figure	O
1	O
(	O
c	O
)	O
)	O
.	O

At	O
last	O
,	O
inclined	O
non	B-Method
-	I-Method
maximum	I-Method
suppression	E-Method
is	O
used	O
to	O
post	O
-	O
process	O
the	O
detection	O
candidates	O
to	O
get	O
the	O
final	O
detection	S-Task
results	O
(	O
Figure	O
1	O
(	O
d	O
)	O
)	O
.	O

Our	O
method	O
yields	O
an	O
F	B-Metric
-	I-Metric
measure	E-Metric
of	O
82.54	O
%	O
on	O
ICDAR	B-Material
2015	I-Material
incidental	I-Material
text	I-Material
detection	I-Material
benchmark	E-Material
and	O
87.73	O
%	O
on	O
ICDAR	O
2013	O
focused	O
text	O
detection	O
benchmark	O
.	O

The	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
-	O
We	O
introduce	O
a	O
novel	O
framework	O
for	O
detecting	B-Task
scene	I-Task
texts	I-Task
of	I-Task
arbitrary	I-Task
orientations	E-Task
(	O
Figure	O
2	O
)	O
.	O

It	O
is	O
based	O
on	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
.	O

The	O
RPN	S-Method
is	O
used	O
for	O
proposing	O
text	B-Task
regions	E-Task
and	O
the	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	I-Method
model	E-Method
[	O
reference	O
]	O
is	O
modified	O
to	O
do	O
text	B-Task
region	I-Task
classification	E-Task
,	O
refinement	S-Task
and	O
inclined	B-Task
box	I-Task
prediction	E-Task
.	O

-	O
The	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
text	I-Task
detection	I-Task
problem	E-Task
is	O
formulated	O
as	O
a	O
multi	B-Task
-	I-Task
task	I-Task
problem	E-Task
.	O

The	O
core	O
of	O
the	O
approach	O
is	O
predicting	O
text	O
scores	O
,	O
axis	O
-	O
aligned	O
boxes	O
and	O
inclined	O
minimum	O
area	O
boxes	O
for	O
each	O
proposal	O
generated	O
by	O
the	O
RPN	S-Method
.	O

-	O
To	O
make	O
the	O
most	O
of	O
text	O
characteristics	O
,	O
we	O
do	O
several	O
ROIPoolings	S-Method
with	O
different	O
pooled	O
sizes	O
(	O
7	O
×	O
7	O
,	O
11	O
×	O
3	O
,	O
3	O
×	O
11	O
)	O
for	O
each	O
RPN	B-Method
proposal	E-Method
.	O

The	O
concatenated	O
features	O
are	O
then	O
used	O
for	O
further	O
detection	S-Task
.	O

-	O
Our	O
modification	O
of	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
also	O
include	O
adding	O
a	O
smaller	O
anchor	O
for	O
detecting	B-Task
small	I-Task
scene	I-Task
texts	E-Task
and	O
using	O
inclined	O
non	O
-	O
maximum	O
suppression	O
to	O
post	O
-	O
process	O
the	O
detection	O
candidates	O
to	O
get	O
the	O
final	O
result	O
.	O

section	O
:	O
Related	O
Work	O
The	O
traditional	O
scene	B-Method
text	I-Method
detection	I-Method
methods	E-Method
consist	O
of	O
sliding	B-Method
-	I-Method
window	I-Method
based	I-Method
methods	E-Method
and	O
Connected	B-Method
Components	E-Method
(	O
CCs	S-Method
)	O
based	O
methods	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

The	O
sliding	B-Method
-	I-Method
window	I-Method
based	I-Method
methods	E-Method
move	O
a	O
multi	O
-	O
scale	O
window	O
through	O
an	O
image	O
densely	O
and	O
then	O
classify	O
the	O
candidates	O
as	O
character	O
or	O
non	O
-	O
character	O
to	O
detect	O
character	O
candidates	O
.	O

The	O
CCs	S-Method
based	O
approaches	O
generate	O
character	O
candidates	O
based	O
on	O
CCs	S-Method
.	O

In	O
particular	O
,	O
the	O
Maximally	B-Method
Stable	I-Method
Extremal	I-Method
Regions	E-Method
(	O
MSER	S-Method
)	O
based	O
methods	O
used	O
to	O
achieve	O
good	O
performances	O
in	O
ICDAR	B-Task
2015	E-Task
[	O
reference	O
]	O
and	O
ICDAR	O
2013	O
[	O
reference	O
]	O
competitions	O
.	O

These	O
traditional	O
methods	O
adopt	O
a	O
bottom	B-Method
-	I-Method
up	I-Method
strategy	E-Method
and	O
often	O
needs	O
several	O
steps	O
to	O
detect	O
texts	O
(	O
e.g.	O
,	O
character	B-Task
detection	E-Task
,	O
text	B-Task
line	I-Task
construction	E-Task
and	O
text	B-Task
line	I-Task
classification	E-Task
)	O
.	O

The	O
general	B-Task
object	I-Task
detection	E-Task
is	O
a	O
hot	O
research	O
area	O
recently	O
.	O

Deep	B-Method
learning	I-Method
based	I-Method
techniques	E-Method
have	O
advanced	O
object	B-Task
detection	E-Task
substantially	O
.	O

One	O
kind	O
of	O
object	B-Method
detection	I-Method
methods	E-Method
rely	O
on	O
region	B-Method
proposal	E-Method
,	O
such	O
as	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
SPPnet	S-Method
[	O
reference	O
]	O
,	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
and	O
R	B-Method
-	I-Method
FCN	E-Method
[	O
reference	O
]	O
.	O

Another	O
family	O
of	O
object	B-Method
detectors	E-Method
do	O
not	O
rely	O
on	O
region	B-Method
proposal	E-Method
and	O
directly	O
estimate	O
object	O
candidates	O
,	O
such	O
as	O
SSD	S-Method
[	O
reference	O
]	O
and	O
YOLO	O
[	O
reference	O
]	O
.	O

Our	O
method	O
is	O
based	O
on	O
the	O
Faster	O
R	B-Method
-	I-Method
CNN	I-Method
architecture	E-Method
.	O

In	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
,	O
a	O
Region	B-Method
Proposal	I-Method
Network	E-Method
(	O
RPN	S-Method
)	O
is	O
proposed	O
to	O
generate	O
high	O
-	O
quality	O
object	O
proposals	O
directly	O
from	O
the	O
convolutional	O
feature	O
maps	O
.	O

The	O
proposals	O
generated	O
by	O
RPN	S-Method
is	O
then	O
refined	O
and	O
classified	O
with	O
the	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	I-Method
model	E-Method
[	O
reference	O
]	O
.	O

As	O
scene	O
texts	O
have	O
orientations	O
and	O
are	O
different	O
from	O
general	O
objects	O
,	O
the	O
general	B-Method
object	I-Method
detection	I-Method
methods	E-Method
can	O
not	O
be	O
used	O
directly	O
in	O
scene	B-Task
text	I-Task
detection	E-Task
.	O

Deep	B-Method
learning	I-Method
based	I-Method
scene	I-Method
text	I-Method
detection	I-Method
methods	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
have	O
been	O
studied	O
and	O
achieve	O
better	O
performance	O
than	O
traditional	O
methods	O
.	O

TextBoxes	S-Method
is	O
an	O
end	O
-	O
to	O
-	O
end	O
fast	B-Task
scene	I-Task
text	I-Task
detector	E-Task
with	O
a	O
single	O
deep	B-Method
neural	I-Method
network	E-Method
[	O
reference	O
]	O
.	O

DeepText	S-Method
generates	O
word	B-Method
region	I-Method
proposals	E-Method
via	O
Inception	B-Method
-	I-Method
RPN	E-Method
and	O
then	O
scores	O
and	O
refines	O
each	O
word	B-Method
proposal	E-Method
using	O
the	O
text	B-Method
detection	I-Method
network	E-Method
[	O
reference	O
]	O
.	O

Fully	B-Method
-	I-Method
Convolutional	I-Method
Regression	I-Method
Network	E-Method
(	O
FCRN	S-Method
)	O
utilizes	O
synthetic	O
images	O
to	O
train	O
the	O
model	O
for	O
scene	B-Task
text	I-Task
detection	E-Task
[	O
reference	O
]	O
.	O

However	O
,	O
these	O
methods	O
are	O
designed	O
to	O
generate	O
axis	O
-	O
aligned	O
detection	O
boxes	O
and	O
do	O
not	O
address	O
the	O
text	B-Task
orientation	I-Task
problem	E-Task
.	O

Connectionist	B-Method
Text	I-Method
Proposal	I-Method
Network	E-Method
(	O
CTPN	S-Method
)	O
detects	O
vertical	O
boxes	O
with	O
fixed	O
width	O
,	O
uses	O
BLSTM	S-Method
to	O
catch	O
the	O
sequential	O
information	O
and	O
then	O
links	O
the	O
vertical	O
boxes	O
to	O
get	O
final	O
detection	O
boxes	O
[	O
reference	O
]	O
.	O

It	O
is	O
good	O
at	O
detecting	O
horizontal	O
texts	O
but	O
not	O
fit	O
for	O
high	O
inclined	O
texts	O
.	O

The	O
method	O
based	O
on	O
Fully	B-Method
Convolutional	I-Method
Network	I-Method
(	I-Method
FCN	I-Method
)	E-Method
is	O
designed	O
to	O
detect	O
multi	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
texts	E-Task
[	O
reference	O
]	O
[	O
reference	O
]	O
is	O
proposed	O
for	O
multi	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
text	I-Task
detection	E-Task
.	O

Our	O
goal	O
is	O
to	O
detect	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
texts	E-Task
.	O

Similar	O
to	O
RRPN	S-Method
[	O
reference	O
]	O
,	O
our	O
network	O
is	O
also	O
based	O
on	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
but	O
we	O
utilize	O
a	O
different	O
strategy	O
other	O
than	O
generating	O
inclined	O
proposals	O
.	O

We	O
think	O
the	O
RPN	S-Method
is	O
qualified	O
to	O
generate	O
text	O
candidates	O
and	O
we	O
predict	O
the	O
orientation	O
information	O
based	O
on	O
the	O
text	O
candidates	O
proposed	O
by	O
RPN	S-Method
.	O

section	O
:	O
Proposed	O
Approach	O
In	O
this	O
section	O
,	O
we	O
introduce	O
our	O
approach	O
to	O
detect	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
texts	E-Task
.	O

Figure	O
2	O
shows	O
the	O
architecture	O
of	O
the	O
proposed	O
Rotational	B-Method
Region	I-Method
CNN	E-Method
(	O
R	B-Method
2	I-Method
CNN	E-Method
)	O
.	O

We	O
first	O
present	O
how	O
we	O
formalize	O
the	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
text	I-Task
detection	I-Task
problem	E-Task
and	O
then	O
introduce	O
the	O
details	O
of	O
R	B-Method
2	I-Method
CNN	E-Method
.	O

After	O
that	O
,	O
we	O
describe	O
our	O
training	O
objectives	O
.	O

section	O
:	O
Problem	O
definition	O
In	O
ICDAR	B-Task
2015	I-Task
competition	E-Task
[	O
reference	O
]	O
,	O
the	O
ground	O
truth	O
of	O
incidental	B-Task
scene	I-Task
text	I-Task
detection	E-Task
is	O
represented	O
by	O
four	O
points	O
in	O
clockwise	O
(	O
x1	O
,	O
y1	O
,	O
x2	O
,	O
y2	O
,	O
x3	O
,	O
y3	O
,	O
x4	O
,	O
y4	O
)	O
as	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
.	O

The	O
label	O
is	O
at	O
word	O
level	O
.	O

The	O
four	O
points	O
form	O
a	O
quadrangle	O
,	O
which	O
is	O
probably	O
not	O
a	O
rectangle	O
.	O

Although	O
scene	O
texts	O
can	O
be	O
more	O
closely	O
enclosed	O
by	O
irregular	O
quadrangles	O
due	O
to	O
perspective	O
distortion	O
,	O
they	O
can	O
be	O
roughly	O
enclosed	O
by	O
inclined	O
rectangles	O
with	O
orientation	O
(	O
Figure	O
3	O
(	O
b	O
)	O
)	O
.	O

As	O
we	O
think	O
an	O
inclined	O
rectangle	O
is	O
able	O
to	O
cover	O
most	O
of	O
the	O
text	O
area	O
,	O
we	O
approximate	O
the	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
text	I-Task
detection	I-Task
task	E-Task
as	O
detecting	O
an	O
inclined	O
minimum	O
area	O
rectangle	O
in	O
our	O
approach	O
.	O

In	O
the	O
rest	O
of	O
the	O
paper	O
,	O
when	O
we	O
mention	O
the	O
bounding	O
box	O
,	O
it	O
refers	O
to	O
a	O
rectangular	O
box	O
.	O

Although	O
the	O
straightforward	O
method	O
to	O
represent	O
a	O
inclined	O
rectangle	O
is	O
using	O
an	O
angle	O
to	O
represent	O
its	O
orientation	O
,	O
we	O
do	O
n't	O
adopt	O
this	O
strategy	O
because	O
the	O
angle	O
target	O
is	O
not	O
stable	O
in	O
some	O
special	O
points	O
.	O

For	O
example	O
,	O
a	O
rectangle	O
with	O
rotation	O
angle	O
90	O
0	O
is	O
very	O
similar	O
to	O
the	O
same	O
rectangle	O
with	O
rotation	O
angle	O
-	O
90	O
0	O
,	O
but	O
their	O
angles	O
are	O
quite	O
different	O
.	O

This	O
makes	O
the	O
network	O
hard	O
to	O
learn	O
to	O
detect	O
vertical	O
texts	O
.	O

Instead	O
of	O
using	O
an	O
angle	O
to	O
represent	O
the	O
orientation	O
information	O
,	O
we	O
use	O
the	O
coordinates	O
of	O
the	O
first	O
two	O
points	O
in	O
clockwise	O
and	O
the	O
height	O
of	O
the	O
bounding	O
box	O
to	O
represent	O
an	O
inclined	O
rectangle	O
(	O
x1	O
,	O
y1	O
,	O
x2	O
,	O
y2	O
,	O
h	O
)	O
.	O

We	O
suppose	O
the	O
first	O
point	O
always	O
means	O
the	O
point	O
at	O
the	O
left	O
-	O
top	O
corner	O
of	O
the	O
scene	O
text	O
.	O

Figure	O
3	O
Overview	O
.	O

We	O
adopt	O
the	O
popular	O
two	B-Method
-	I-Method
stage	I-Method
object	I-Method
detection	I-Method
strategy	E-Method
that	O
consists	O
of	O
region	B-Method
proposal	E-Method
and	O
region	B-Method
classification	E-Method
.	O

Rotational	B-Method
Region	I-Method
CNN	E-Method
(	O
R	B-Method
2	I-Method
CNN	E-Method
)	O
is	O
based	O
on	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
.	O

Figure	O
2	O
shows	O
the	O
architecture	O
of	O
R	B-Method
2	I-Method
CNN	E-Method
.	O

The	O
RPN	S-Method
is	O
first	O
used	O
to	O
generate	O
text	B-Task
region	I-Task
proposals	E-Task
,	O
which	O
are	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
that	O
enclose	O
the	O
arbitrary	O
-	O
oriented	O
texts	O
(	O
Figure	O
1	O
(	O
b	O
)	O
)	O
.	O

And	O
then	O
for	O
each	O
proposal	O
,	O
several	O
ROIPoolings	O
with	O
different	O
pooled	O
sizes	O
(	O
7	O
×	O
7	O
,	O
11	O
×	O
3	O
,	O
3	O
×	O
11	O
)	O
are	O
performed	O
on	O
the	O
convolutional	O
feature	O
maps	O
and	O
the	O
pooled	O
features	O
are	O
concatenated	O
for	O
further	O
classification	S-Task
and	O
regression	S-Task
.	O

With	O
concatenated	O
features	O
and	O
fully	B-Method
connected	I-Method
layers	E-Method
,	O
we	O
predict	O
text	O
/	O
non	O
-	O
text	O
scores	O
,	O
axis	O
-	O
aligned	O
boxes	O
and	O
inclined	O
minimum	O
area	O
boxes	O
(	O
Figure	O
1	O
(	O
c	O
)	O
)	O
.	O

After	O
that	O
,	O
the	O
inclined	O
boxes	O
are	O
post	O
-	O
processed	O
by	O
inclined	B-Method
non	I-Method
-	I-Method
maximum	I-Method
suppression	E-Method
to	O
get	O
the	O
detection	S-Task
results	O
(	O
Figure	O
1	O
(	O
d	O
)	O
)	O
.	O

section	O
:	O
RPN	S-Method
for	O
proposing	O
axis	B-Task
-	I-Task
aligned	I-Task
boxes	E-Task
.	O

We	O
use	O
the	O
RPN	S-Method
to	O
generate	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
that	O
enclose	O
the	O
arbitrary	O
-	O
oriented	O
texts	O
.	O

This	O
is	O
reasonable	O
because	O
the	O
text	O
in	O
the	O
axis	O
-	O
aligned	O
box	O
belongs	O
to	O
one	O
of	O
the	O
following	O
situations	O
:	O
a	O
)	O
the	O
text	O
is	O
in	O
the	O
horizontal	O
direction	O
;	O
b	O
)	O
the	O
text	O
is	O
in	O
the	O
vertical	O
direction	O
;	O
c	O
)	O
the	O
text	O
is	O
in	O
the	O
diagonal	O
direction	O
of	O
the	O
axis	O
-	O
aligned	O
box	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
,	O
the	O
RPN	S-Method
is	O
competent	O
for	O
generating	O
text	O
regions	O
in	O
the	O
form	O
of	O
axis	O
-	O
aligned	O
boxes	O
for	O
arbitrary	O
-	O
oriented	O
texts	O
.	O

Compared	O
to	O
general	O
objects	O
,	O
there	O
are	O
more	O
small	O
scene	O
texts	O
.	O

We	O
support	O
this	O
by	O
utilizing	O
a	O
smaller	O
anchor	O
scale	O
in	O
RPN	S-Method
.	O

While	O
the	O
original	O
anchor	O
scales	O
are	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
in	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
we	O
investigate	O
two	O
strategies	O
:	O
a	O
)	O
changing	O
an	O
anchor	O
scale	O
to	O
a	O
smaller	O
size	O
and	O
using	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
;	O
b	O
)	O
adding	O
a	O
new	O
anchor	O
scale	O
and	O
utilizing	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Our	O
experiments	O
confirm	O
that	O
the	O
adoption	O
of	O
the	O
smaller	O
anchor	O
is	O
helpful	O
for	O
scene	B-Task
text	I-Task
detection	E-Task
.	O

We	O
keep	O
other	O
settings	O
of	O
RPN	S-Method
the	O
same	O
as	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
including	O
the	O
anchor	O
aspect	O
ratios	O
,	O
the	O
definition	O
of	O
positive	O
samples	O
and	O
negative	O
samples	O
,	O
and	O
etc	O
.	O

section	O
:	O
ROIPoolings	O
of	O
different	O
pooled	O
sizes	O
.	O

Regression	S-Method
for	O
text	O
/	O
non	O
-	O
text	O
scores	O
,	O
axis	O
-	O
aligned	O
boxes	O
,	O
and	O
inclined	O
minimum	O
area	O
boxes	O
.	O

In	O
our	O
approach	O
,	O
after	O
RPN	S-Method
,	O
we	O
classify	O
the	O
proposal	O
generated	O
by	O
RPN	S-Method
as	O
text	O
or	O
non	O
-	O
text	O
,	O
refine	O
the	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
that	O
contain	O
the	O
arbitrary	O
-	O
oriented	O
texts	O
and	O
predict	O
inclined	O
bounding	O
boxes	O
.	O

Each	O
inclined	O
box	O
is	O
associated	O
with	O
an	O
axis	O
-	O
aligned	O
box	O
(	O
Figure	O
1	O
(	O
c	O
)	O
and	O
Figure	O
4	O
(	O
a	O
)	O
)	O
.	O

Although	O
our	O
detection	B-Task
targets	E-Task
are	O
the	O
inclined	O
bounding	O
boxes	O
,	O
we	O
think	O
adding	O
additional	O
constraints	O
(	O
axis	O
-	O
aligned	O
bounding	O
box	O
)	O
could	O
improve	O
the	O
performance	O
.	O

And	O
our	O
evaluation	O
also	O
confirm	O
the	O
effectiveness	O
of	O
this	O
idea	O
.	O

Inclined	B-Task
non	I-Task
-	I-Task
maximum	I-Task
suppression	E-Task
.	O

Non	B-Method
-	I-Method
Maximum	I-Method
Suppression	E-Method
(	O
NMS	S-Method
)	O
is	O
extensively	O
used	O
to	O
post	O
-	O
process	O
detection	B-Task
candidates	E-Task
by	O
current	O
object	B-Method
detection	I-Method
methods	E-Method
.	O

As	O
we	O
estimate	O
both	O
the	O
axis	O
-	O
aligned	O
bounding	O
box	O
and	O
the	O
inclined	O
bounding	O
box	O
,	O
we	O
can	O
either	O
do	O
normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
,	O
or	O
do	O
inclined	O
NMS	S-Method
on	O
inclined	O
bounding	O
boxes	O
.	O

In	O
the	O
inclined	O
NMS	S-Method
,	O
the	O
calculation	O
of	O
the	O
traditional	O
Intersection	B-Metric
-	I-Metric
over	I-Metric
-	I-Metric
Union	E-Metric
(	O
IoU	S-Metric
)	O
is	O
modified	O
to	O
be	O
the	O
IoU	S-Metric
between	O
two	O
inclined	O
bounding	O
boxes	O
.	O

The	O
IoU	S-Metric
calculation	O
method	O
used	O
in	O
[	O
reference	O
]	O
is	O
utilized	O
.	O

Figure	O
4	O
illustrates	O
the	O
detection	S-Task
results	O
after	O
two	O
kinds	O
of	O
NMS	S-Method
are	O
performed	O
.	O

Figure	O
4	O
(	O
a	O
)	O
shows	O
predicted	O
candidate	O
boxes	O
that	O
each	O
axis	O
-	O
aligned	O
bounding	O
box	O
is	O
associated	O
with	O
an	O
inclined	O
bounding	O
box	O
.	O

Figure	O
4	O
(	O
b	O
)	O
shows	O
the	O
effects	O
of	O
the	O
normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
boxes	O
and	O
Figure	O
4	O
(	O
c	O
)	O
demonstrates	O
the	O
effects	O
of	O
the	O
inclined	O
NMS	S-Method
on	O
inclined	O
boxes	O
.	O

As	O
show	O
in	O
Figure	O
4	O
(	O
b	O
)	O
,	O
the	O
text	O
in	O
red	O
dashed	O
box	O
is	O
not	O
detected	O
under	O
normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
boxes	O
.	O

Figure	O
4	O
(	O
d	O
)	O
and	O
Figure	O
4	O
(	O
e	O
)	O
shows	O
the	O
reason	O
why	O
the	O
inclined	O
NMS	S-Method
is	O
better	O
for	O
inclined	B-Task
scene	I-Task
text	I-Task
detection	E-Task
.	O

We	O
can	O
see	O
that	O
for	O
closely	O
adjacent	O
inclined	O
texts	O
,	O
normal	O
NMS	S-Method
may	O
miss	O
some	O
text	O
as	O
the	O
IoU	S-Metric
between	O
axis	O
-	O
aligned	O
boxes	O
can	O
be	O
high	O
(	O
Figure	O
4	O
(	O
d	O
)	O
)	O
,	O
but	O
inclined	O
NMS	S-Method
will	O
not	O
miss	O
the	O
text	O
because	O
the	O
inclined	O
IoU	S-Metric
value	O
is	O
low	O
(	O
Figure	O
4	O
(	O
e	O
)	O
)	O
.	O

Fig.4	O
.	O

Inclined	B-Task
non	I-Task
-	I-Task
maximum	I-Task
suppression	E-Task
.	O

(	O
a	O
)	O
The	O
candidate	O
axis	O
-	O
aligned	O
boxes	O
and	O
inclined	O
boxes	O
;	O
(	O
b	O
)	O
the	O
detection	S-Task
results	O
based	O
on	O
normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
boxes	O
(	O
the	O
green	O
boxes	O
are	O
the	O
correct	O
detections	O
,	O
and	O
the	O
red	O
dashed	O
box	O
is	O
the	O
box	O
that	O
is	O
not	O
detected	O
)	O
;	O
(	O
c	O
)	O
the	O
detection	S-Task
results	O
based	O
on	O
inclined	O
NMS	S-Method
on	O
inclined	O
boxes	O
;	O
(	O
d	O
)	O
an	O
example	O
of	O
two	O
axis	O
-	O
aligned	O
boxes	O
;	O
(	O
e	O
)	O
an	O
example	O
of	O
two	O
inclined	O
boxes	O
.	O

section	O
:	O
Training	B-Metric
objective	E-Metric
(	O
Multi	B-Task
-	I-Task
task	I-Task
loss	E-Task
)	O
The	O
training	B-Metric
loss	E-Metric
on	O
RPN	S-Task
is	O
the	O
same	O
as	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
.	O

In	O
this	O
section	O
,	O
we	O
only	O
introduce	O
the	O
loss	O
function	O
of	O
R	B-Method
2	I-Method
CNN	E-Method
on	O
each	O
axis	O
-	O
aligned	O
box	O
proposal	O
generated	O
by	O
RPN	S-Method
.	O

Our	O
loss	B-Method
function	E-Method
defined	O
on	O
each	O
proposal	O
is	O
the	O
summation	O
of	O
the	O
text	B-Method
/	I-Method
non	I-Method
-	I-Method
text	I-Method
classification	I-Method
loss	E-Method
and	O
the	O
box	B-Method
regression	I-Method
loss	E-Method
.	O

The	O
box	B-Method
regression	I-Method
loss	E-Method
consists	O
of	O
two	O
parts	O
:	O
the	O
loss	O
of	O
axis	O
-	O
aligned	O
boxes	O
that	O
enclose	O
the	O
arbitrary	O
-	O
oriented	O
texts	O
and	O
the	O
loss	O
of	O
inclined	O
minimum	O
area	O
boxes	O
.	O

The	O
multi	B-Metric
-	I-Metric
task	I-Metric
loss	I-Metric
function	E-Metric
on	O
each	O
proposal	O
is	O
defined	O
as	O
:	O
,	O
,	O
,	O
*	O
,	O
,	O
*	O
=	O
cls	O
,	O
+	O
1	O
,	O
*	O
∈	O
,	O
,	O
,	O
ℎ	O
+	O
2	O
,	O
*	O
∈	O
1	O
,	O
1	O
,	O
2	O
,	O
2	O
,	O
ℎ	O
(	O
1	O
)	O
1	O
and	O
2	O
are	O
the	O
balancing	O
parameters	O
that	O
control	O
the	O
trade	O
-	O
off	O
between	O
three	O
terms	O
.	O

The	O
box	B-Method
regression	E-Method
only	O
conducts	O
on	O
text	O
.	O

t	O
is	O
the	O
indicator	O
of	O
the	O
class	O
label	O
.	O

Text	O
is	O
labeled	O
as	O
1	O
(	O
t	O
=	O
1	O
)	O
,	O
and	O
background	O
is	O
labeled	O
as	O
0	O
(	O
t	O
=	O
0	O
)	O
.	O

The	O
parameter	O
p	O
=	O
(	O
p	O
0	O
,	O
p	O
1	O
)	O
is	O
the	O
probability	O
over	O
text	O
and	O
background	O
classes	O
computed	O
by	O
the	O
softmax	B-Method
function	E-Method
.	O

cls	S-Method
,	O
=	O
−log	O
is	O
the	O
log	O
loss	O
for	O
true	O
class	O
t.	O
=	O
(	O
,	O
,	O
,	O
ℎ	O
)	O
is	O
a	O
tuple	O
of	O
true	O
axis	O
-	O
aligned	O
bounding	O
box	O
regression	O
targets	O
including	O
coordinates	O
of	O
the	O
center	O
point	O
and	O
its	O
width	O
and	O
height	O
,	O
and	O
*	O
=	O
*	O
,	O
*	O
,	O
*	O
,	O
ℎ	O
*	O
is	O
the	O
predicted	O
tuple	O
for	O
the	O
text	O
label	O
.	O

=	O
(	O
1	O
,	O
1	O
,	O
2	O
,	O
2	O
,	O
ℎ	O
)	O
is	O
a	O
tuple	O
of	O
true	O
inclined	O
bounding	O
box	O
regression	O
targets	O
including	O
coordinates	O
of	O
first	O
two	O
points	O
of	O
the	O
inclined	O
box	O
and	O
its	O
height	O
,	O
and	O
*	O
=	O
Although	O
our	O
simple	O
experiments	O
show	O
that	O
the	O
additional	O
collected	O
images	O
do	O
not	O
increase	O
the	O
performance	O
on	O
ICDAR2015	S-Material
,	O
we	O
still	O
include	O
them	O
in	O
the	O
training	O
to	O
make	O
our	O
model	O
more	O
robust	O
to	O
different	O
kinds	O
of	O
scene	O
texts	O
.	O

As	O
ICDAR	B-Task
2015	E-Task
training	O
dataset	O
contains	O
difficult	O
texts	O
that	O
is	O
hard	O
to	O
detect	O
which	O
are	O
labeled	O
as	O
"	O
#	O
#	O
#	O
"	O
,	O
we	O
only	O
use	O
those	O
readable	O
text	O
for	O
training	O
.	O

Moreover	O
,	O
we	O
only	O
use	O
those	O
scene	O
texts	O
composed	O
of	O
more	O
than	O
one	O
character	O
for	O
training	O
.	O

To	O
support	O
arbitrary	B-Task
-	I-Task
oriented	I-Task
scene	I-Task
text	I-Task
detection	E-Task
,	O
we	O
augment	O
ICDAR	B-Task
2015	E-Task
training	O
dataset	O
and	O
our	O
own	O
data	O
by	O
rotating	O
images	O
.	O

We	O
rotate	O
our	O
image	O
at	O
the	O
following	O
angles	O
(	O
-	O
90	O
,	O
-	O
75	O
,	O
-	O
60	O
,	O
-	O
45	O
,	O
-	O
30	O
,	O
-	O
15	O
,	O
0	O
,	O
15	O
,	O
30	O
,	O
45	O
,	O
60	O
,	O
75	O
,	O
90	O
)	O
.	O

Thus	O
,	O
after	O
data	B-Task
augmentation	E-Task
,	O
our	O
training	O
data	O
consists	O
of	O
39000	O
images	O
.	O

The	O
texts	O
in	O
ICDAR	B-Task
2015	E-Task
are	O
labeled	O
at	O
word	O
level	O
with	O
four	O
clockwise	O
point	O
coordinates	O
of	O
quadrangle	O
.	O

As	O
we	O
simplify	O
the	O
problem	O
of	O
incidental	B-Task
text	I-Task
detection	E-Task
as	O
detecting	B-Task
inclined	I-Task
rectangles	E-Task
as	O
introduced	O
in	O
Section	O
3.1	O
,	O
we	O
generate	O
the	O
ground	O
truth	O
inclined	O
bounding	O
box	O
(	O
rectangular	O
data	O
)	O
from	O
the	O
quadrangle	O
by	O
computing	O
the	O
minimum	O
area	O
rectangle	O
that	O
encloses	O
the	O
quadrangle	O
.	O

We	O
then	O
compute	O
the	O
minimum	O
axis	O
-	O
aligned	O
bounding	O
box	O
that	O
encloses	O
the	O
text	O
as	O
the	O
ground	O
truth	O
axis	O
-	O
aligned	O
box	O
.	O

Similar	O
processing	O
is	O
done	O
to	O
generate	O
ground	O
truth	O
data	O
for	O
images	O
we	O
collected	O
.	O

Training	O
.	O

Our	O
network	O
is	O
initialized	O
by	O
the	O
pre	B-Method
-	I-Method
trained	I-Method
VGG16	I-Method
model	E-Method
for	O
ImageNet	B-Task
classification	E-Task
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
training	I-Method
strategy	E-Method
.	O

All	O
the	O
models	O
are	O
trained	O
20	O
×	O
10	O
4	O
iterations	O
in	O
all	O
.	O

Learning	B-Metric
rates	E-Metric
start	O
from	O
10	O
−3	O
,	O
and	O
are	O
multiplied	O
by	O
section	O
:	O
Performance	O
We	O
evaluate	O
our	O
method	O
on	O
ICDAR	B-Task
2015	E-Task
[	O
reference	O
]	O
and	O
ICDAR	O
2013	O
[	O
reference	O
]	O
.	O

The	O
evaluation	O
follows	O
the	O
ICDAR	B-Metric
Robust	I-Metric
Reading	I-Metric
Competition	I-Metric
metrics	E-Metric
in	O
the	O
form	O
of	O
Precision	S-Metric
,	O
Recall	S-Metric
and	O
F	B-Metric
-	I-Metric
measure	E-Metric
.	O

The	O
results	O
are	O
obtained	O
by	O
submitting	O
the	O
detection	O
results	O
to	O
the	O
competition	O
website	O
and	O
get	O
the	O
evaluation	O
results	O
online	O
.	O

section	O
:	O
A.	O
ICDAR	B-Task
2015	E-Task
This	O
section	O
introduces	O
our	O
performances	O
on	O
ICDAR	B-Task
2015	E-Task
[	O
reference	O
]	O
.	O

ICDAR	B-Material
2015	I-Material
competition	I-Material
test	I-Material
dataset	E-Material
consists	O
of	O
500	O
images	O
containing	O
incidental	O
scene	O
texts	O
with	O
arbitrary	O
orientations	O
.	O

Our	O
method	O
could	O
achieve	O
competitive	O
results	O
of	O
Recall	S-Metric
79.68	O
%	O
,	O
Precision	S-Metric
85.62	O
%	O
,	O
and	O
F	B-Metric
-	I-Metric
measure	E-Metric
82.54	O
%	O
.	O

We	O
conduct	O
several	O
experiments	O
to	O
confirm	O
the	O
effectiveness	O
of	O
our	O
design	O
.	O

Table	O
1	O
summarizes	O
the	O
results	O
of	O
our	O
models	O
under	O
different	O
settings	O
.	O

We	O
will	O
compare	O
the	O
following	O
models	O
:	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
R	O
2	O
CNN	B-Method
-	I-Method
1	E-Method
,	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
2	E-Method
,	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
3	E-Method
,	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
4	E-Method
,	O
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
5	E-Method
.	O

We	O
mainly	O
focus	O
on	O
evaluating	O
the	O
influence	O
of	O
the	O
axis	B-Method
-	I-Method
aligned	I-Method
box	I-Method
regression	E-Method
(	O
1	O
)	O
and	O
the	O
inclined	B-Method
box	I-Method
regression	E-Method
(	O
2	O
)	O
,	O
the	O
influence	O
of	O
anchor	O
scales	O
and	O
NMS	S-Method
strategy	O
,	O
and	O
the	O
impact	O
of	O
different	O
pooled	O
sizes	O
of	O
ROIPoolings	O
.	O

All	O
these	O
models	O
are	O
trained	O
on	O
the	O
same	O
dataset	O
introduced	O
in	O
the	O
last	O
section	O
.	O

We	O
first	O
perform	O
single	O
-	O
scale	B-Task
testing	E-Task
with	O
all	O
models	O
on	O
ICDAR	B-Task
2015	E-Task
.	O

The	O
test	O
images	O
keep	O
the	O
original	O
size	O
(	O
width	O
:	O
1280	O
,	O
height	O
:	O
720	O
)	O
when	O
performing	O
the	O
testing	O
.	O

We	O
then	O
do	O
multi	B-Task
-	I-Task
scale	I-Task
testing	E-Task
following	O
[	O
reference	O
]	O
on	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
3	E-Method
,	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
4	E-Method
,	O
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
5	E-Method
.	O

With	O
a	O
trained	O
model	O
,	O
we	O
compute	O
convolutional	B-Method
feature	I-Method
maps	E-Method
on	O
an	O
image	O
pyramid	O
,	O
where	O
the	O
image	O
's	O
short	O
sides	O
are	O
s	O
∈	O
720	O
,	O
1200	O
.	O

The	O
results	O
of	O
all	O
our	O
designs	O
are	O
better	O
than	O
the	O
baseline	B-Method
Faster	I-Method
R	I-Method
-	I-Method
CNN	E-Method
.	O

Axis	O
-	O
aligned	O
box	O
and	O
inclined	O
box	O
.	O

While	O
Faster	O
RCNN	S-Method
only	O
regresses	O
axis	O
-	O
aligned	O
bounding	O
boxes	O
which	O
is	O
implemented	O
by	O
setting	O
1	O
=	O
1	O
and	O
2	O
=	O
0	O
in	O
Equation	O
(	O
1	O
)	O
,	O
the	O
detection	O
outputs	O
are	O
axis	O
-	O
aligned	O
boxes	O
.	O

Different	O
from	O
Faster	O
RCNN	S-Method
,	O
R	O
2	O
CNN	B-Method
-	I-Method
1	E-Method
only	O
regresses	O
inclined	O
boxes	O
(	O
1	O
=	O
0	O
and	O
2	O
=	O
1	O
in	O
Equation	O
(	O
1	O
)	O
)	O
and	O
this	O
leads	O
to	O
about	O
6	O
%	O
performance	O
improvement	O
over	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
(	O
F	B-Metric
-	I-Metric
measure	E-Metric
:	O
62.40	O
%	O
vs.	O
56.63	O
%	O
)	O
.	O

The	O
reason	O
is	O
that	O
the	O
outputs	O
of	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
are	O
axis	O
-	O
aligned	O
boxes	O
and	O
the	O
orientation	O
information	O
is	O
ignored	O
.	O

R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
2	E-Method
regresses	O
both	O
the	O
axis	O
-	O
aligned	O
boxes	O
that	O
enclose	O
the	O
texts	O
and	O
the	O
inclined	O
boxes	O
(	O
1	O
=	O
1	O
and	O
2	O
=	O
1	O
in	O
Equation	O
(	O
1	O
)	O
)	O
and	O
leads	O
to	O
another	O
6	O
%	O
performance	O
improvement	O
over	O
R	O
2	O
CNN	B-Method
-	I-Method
1	E-Method
(	O
F	B-Metric
-	I-Metric
measure	E-Metric
:	O
68.49	O
%	O
vs.	O
62.40	O
%	O
)	O
.	O

This	O
means	O
that	O
learning	O
the	O
additional	O
axis	O
-	O
aligned	O
box	O
could	O
help	O
the	O
detection	O
of	O
the	O
inclined	O
box	O
.	O

Anchor	B-Method
scales	E-Method
.	O

R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
3	E-Method
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
4	E-Method
are	O
designed	O
to	O
evaluate	O
the	O
influence	O
of	O
anchor	O
scales	O
on	O
scene	B-Task
text	I-Task
detection	E-Task
.	O

They	O
should	O
regress	O
both	O
the	O
axis	O
-	O
aligned	O
boxes	O
and	O
the	O
inclined	O
boxes	O
(	O
1	O
=	O
1	O
and	O
2	O
=	O
1	O
in	O
Equation	O
(	O
1	O
)	O
)	O
.	O

R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
3	E-Method
utilizes	O
smaller	O
anchor	O
scales	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
compared	O
to	O
the	O
original	O
scales	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
Normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
boxes	O
vs.	O
inclined	O
NMS	S-Method
on	O
inclined	O
boxes	O
.	O

Because	O
we	O
regress	O
both	O
the	O
axis	O
-	O
aligned	O
box	O
and	O
the	O
inclined	O
minimum	O
area	O
box	O
and	O
each	O
axis	O
-	O
aligned	O
box	O
is	O
associated	O
with	O
an	O
inclined	O
box	O
,	O
we	O
compare	O
the	O
performance	O
of	O
normal	O
NMS	S-Method
on	O
axis	O
-	O
aligned	O
boxes	O
and	O
the	O
performance	O
of	O
inclined	O
NMS	S-Method
on	O
inclined	O
boxes	O
.	O

We	O
can	O
see	O
that	O
inclined	O
NMS	S-Method
with	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
3	E-Method
,	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
4	E-Method
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
5	E-Method
under	O
both	O
single	O
test	O
and	O
multi	B-Metric
-	I-Metric
scale	I-Metric
test	E-Metric
consistently	O
perform	O
than	O
their	O
counterparts	O
.	O

Table	O
1	O
are	O
obtained	O
when	O
doing	O
test	O
on	O
a	O
Tesla	O
K80	O
GPU	O
.	O

Under	O
single	O
-	O
scale	O
test	O
,	O
our	O
method	O
only	O
increase	O
little	O
detection	B-Metric
time	E-Metric
compared	O
to	O
the	O
Faster	O
R	B-Method
-	I-Method
CNN	I-Method
baseline	E-Method
.	O

section	O
:	O
Test	B-Metric
time	E-Metric
.	O

The	O
test	O
times	O
in	O
Comparisons	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Table	O
2	O
shows	O
the	O
comparison	O
of	O
R	B-Method
2	I-Method
CNN	E-Method
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ICDAR	B-Task
2015	E-Task
[	O
reference	O
]	O
.	O

Here	O
,	O
R	B-Method
2	I-Method
CNN	E-Method
refers	O
to	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
5	E-Method
with	O
inclined	O
NMS	S-Method
.	O

We	O
can	O
see	O
that	O
our	O
method	O
can	O
get	O
competitive	O
results	O
of	O
Recall	S-Metric
79.68	O
%	O
,	O
Precision	S-Metric
85.62	O
%	O
and	O
F	B-Metric
-	I-Metric
measure	E-Metric
82.54	O
%	O
.	O

As	O
our	O
approach	O
can	O
be	O
considered	O
as	O
learning	O
the	O
inclined	O
box	O
based	O
on	O
the	O
axis	O
-	O
aligned	O
box	O
,	O
it	O
can	O
be	O
easily	O
adapted	O
to	O
other	O
architectures	O
,	O
such	O
as	O
SSD	S-Method
[	O
reference	O
]	O
and	O
YOLO	O
[	O
reference	O
]	O
.	O

[	O
reference	O
]	O
80.00	O
%	O
82.00	O
%	O
81.00	O
%	O
EAST	O
[	O
reference	O
]	O
78.33	O
%	O
83.27	O
%	O
80.72	O
%	O
RRPN	S-Method
[	O
reference	O
]	O
82.17	O
%	O
73.23	O
%	O
77.44	O
%	O
SegLink	S-Metric
[	O
reference	O
]	O
76.80	O
%	O
73.10	O
%	O
75.00	O
%	O
DMPNet	S-Method
[	O
reference	O
]	O
68.22	O
%	O
73.23	O
%	O
70.64	O
%	O
CTPN	S-Method
[	O
reference	O
]	O
51.56	O
%	O
74.22	O
%	O
60.85	O
%	O
MCLAB_FCN	S-Metric
[	O
reference	O
]	O
43.09	O
%	O
70.81	O
%	O
53.58	O
%	O
Figure	O
5	O
demonstrates	O
some	O
detection	S-Task
results	O
of	O
our	O
R	B-Method
2	I-Method
CNN	E-Method
on	O
ICDAR	B-Task
2015	E-Task
.	O

We	O
can	O
see	O
that	O
our	O
method	O
can	O
detect	O
scene	O
texts	O
that	O
have	O
different	O
orientations	O
.	O

section	O
:	O
B.	O
ICDAR	O
2013	O
To	O
evaluate	O
our	O
method	O
's	O
adaptability	O
,	O
we	O
conduct	O
experiments	O
on	O
ICDAR	O
2013	O
[	O
reference	O
]	O
.	O

ICDAR	O
2013	O
test	O
dataset	O
consists	O
of	O
233	O
focused	O
scene	O
text	O
images	O
.	O

The	O
texts	O
in	O
the	O
images	O
are	O
horizontal	O
.	O

As	O
we	O
can	O
estimate	O
both	O
the	O
axis	O
-	O
aligned	O
box	O
and	O
the	O
inclined	O
box	O
,	O
we	O
use	O
the	O
axis	O
-	O
aligned	O
box	O
as	O
the	O
output	O
for	O
ICDAR	B-Task
2013	E-Task
.	O

We	O
conduct	O
experiments	O
on	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	I-Method
model	E-Method
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
5	E-Method
model	O
trained	O
in	O
last	O
section	O
for	O
ICDAR	B-Task
2015	E-Task
.	O

Table	O
3	O
shows	O
our	O
results	O
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

Our	O
approach	O
could	O
reach	O
the	O
result	O
of	O
F	B-Metric
-	I-Metric
measure	E-Metric
87.73	O
%	O
.	O

As	O
the	O
training	O
data	O
we	O
used	O
does	O
not	O
include	O
single	O
characters	O
but	O
single	O
characters	O
should	O
be	O
detected	O
in	O
ICDAR	B-Material
2013	E-Material
,	O
we	O
think	O
our	O
method	O
could	O
achieve	O
even	O
better	O
results	O
when	O
single	O
characters	O
are	O
used	O
for	O
training	O
our	O
model	O
.	O

To	O
compare	O
our	O
method	O
with	O
the	O
Faster	O
R	B-Method
-	I-Method
CNN	I-Method
baseline	E-Method
,	O
we	O
also	O
do	O
a	O
single	O
-	O
scale	O
test	O
in	O
which	O
the	O
short	O
side	O
of	O
the	O
image	O
is	O
set	O
to	O
720	O
pixels	O
.	O

In	O
Table	O
3	O
,	O
both	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
and	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
720	E-Method
adopt	O
this	O
testing	O
scale	O
.	O

The	O
result	O
is	O
that	O
R	B-Method
2	I-Method
CNN	I-Method
-	I-Method
720	E-Method
is	O
much	O
better	O
than	O
the	O
Faster	O
R	B-Method
-	I-Method
CNN	I-Method
baseline	E-Method
(	O
F	B-Metric
-	I-Metric
measure	E-Metric
:	O
83.16	O
%	O
vs.	O
78.45	O
%	O
)	O
.	O

This	O
means	O
our	O
design	O
is	O
also	O
useful	O
for	O
horizontal	B-Task
text	I-Task
detection	E-Task
.	O

Figure	O
6	O
shows	O
some	O
detection	S-Task
results	O
on	O
ICDAR	B-Task
2013	E-Task
.	O

We	O
can	O
see	O
R	O
2	O
CNN	S-Method
could	O
detect	O
horizontal	O
focused	O
scene	O
texts	O
well	O
.	O

The	O
missed	O
text	O
in	O
the	O
figure	O
is	O
a	O
single	O
character	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
Rotational	B-Method
Region	I-Method
CNN	E-Method
(	O
R	B-Method
2	I-Method
CNN	E-Method
)	O
for	O
detecting	B-Task
scene	I-Task
texts	E-Task
in	O
arbitrary	O
orientations	O
.	O

The	O
framework	O
is	O
based	O
on	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	I-Method
architecture	E-Method
[	O
reference	O
]	O
.	O

The	O
RPN	S-Method
is	O
used	O
to	O
generate	O
axis	O
-	O
aligned	O
region	O
proposals	O
.	O

And	O
then	O
several	O
ROIPoolings	S-Method
with	O
different	O
pooled	O
sizes	O
(	O
7	O
×	O
7	O
,	O
11	O
×	O
3	O
,	O
3	O
×	O
11	O
)	O
are	O
performed	O
on	O
the	O
proposal	O
and	O
the	O
concatenated	O
pooled	O
features	O
are	O
used	O
for	O
classifying	O
the	O
proposal	O
,	O
estimating	O
both	O
the	O
axis	O
-	O
aligned	O
box	O
and	O
the	O
inclined	O
minimum	O
area	O
box	O
.	O

After	O
that	O
,	O
inclined	O
NMS	S-Method
is	O
performed	O
on	O
the	O
inclined	O
boxes	O
.	O

Evaluation	O
shows	O
that	O
our	O
approach	O
can	O
achieve	O
competitive	O
results	O
on	O
ICDAR2015	S-Material
and	O
ICDAR2013	O
.	O

The	O
method	O
can	O
be	O
considered	O
as	O
learning	O
the	O
inclined	O
box	O
based	O
on	O
the	O
axis	O
-	O
aligned	O
box	O
and	O
it	O
can	O
be	O
easily	O
adapted	O
to	O
other	O
general	O
object	B-Method
detection	I-Method
frameworks	E-Method
such	O
as	O
SSD	S-Method
[	O
reference	O
]	O
and	O
YOLO	O
[	O
reference	O
]	O
to	O
detect	O
object	O
with	O
orientations	O
.	O

section	O
:	O
document	O
:	O
Simple	O
Baseline	O
for	O
Visual	B-Task
Question	I-Task
Answering	E-Task
We	O
describe	O
a	O
very	O
simple	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
baseline	E-Method
for	O
visual	B-Task
question	I-Task
answering	E-Task
.	O

This	O
baseline	O
concatenates	O
the	O
word	O
features	O
from	O
the	O
question	O
and	O
CNN	O
features	O
from	O
the	O
image	O
to	O
predict	O
the	O
answer	O
.	O

When	O
evaluated	O
on	O
the	O
challenging	O
VQA	B-Material
dataset	E-Material
,	O
it	O
shows	O
comparable	O
performance	O
to	O
many	O
recent	O
approaches	O
using	O
recurrent	B-Method
neural	I-Method
networks	E-Method
.	O

To	O
explore	O
the	O
strength	O
and	O
weakness	O
of	O
the	O
trained	O
model	O
,	O
we	O
also	O
provide	O
an	O
interactive	O
web	O
demo	O
,	O
and	O
open	O
-	O
source	O
code	O
.	O

1	O
]	O
BoleiZhou2	O
]	O
YuandongTian2	O
]	O
SainbayarSukhbaatar2	O
]	O
ArthurSzlam2	O
]	O
RobFergus	O
[	O
1	O
]	O
MassachusettsInstituteofTechnology	O
[	O
2	O
]	O
FacebookAIResearch	O
section	O
:	O
Introduction	O
Combining	O
Natural	B-Task
Language	I-Task
Processing	E-Task
with	O
Computer	B-Task
Vision	E-Task
for	O
high	B-Task
-	I-Task
level	I-Task
scene	I-Task
interpretation	E-Task
is	O
a	O
recent	O
trend	O
,	O
e.g.	O
,	O
image	B-Task
captioning	E-Task
.	O

These	O
works	O
have	O
benefited	O
from	O
the	O
rapid	O
development	O
of	O
deep	B-Method
learning	E-Method
for	O
visual	B-Task
recognition	E-Task
(	O
object	B-Task
recognition	I-Task
and	I-Task
scene	I-Task
recognition	E-Task
)	O
,	O
and	O
have	O
been	O
made	O
possible	O
by	O
the	O
emergence	O
of	O
large	O
image	O
datasets	O
and	O
text	O
corpus	O
(	O
e.g.	O
,	O
)	O
.	O

Beyond	O
image	B-Task
captioning	E-Task
,	O
a	O
natural	O
next	O
step	O
is	O
visual	B-Task
question	I-Task
answering	E-Task
(	O
QA	S-Task
)	O
.	O

Compared	O
with	O
the	O
image	B-Task
captioning	I-Task
task	E-Task
,	O
in	O
which	O
an	O
algorithm	O
is	O
required	O
to	O
generate	O
free	O
-	O
form	O
text	O
description	O
for	O
a	O
given	O
image	O
,	O
visual	B-Task
QA	E-Task
can	O
involve	O
a	O
wider	O
range	O
of	O
knowledge	O
and	O
reasoning	O
skills	O
.	O

A	O
captioning	B-Method
algorithm	E-Method
has	O
the	O
liberty	O
to	O
pick	O
the	O
easiest	O
relevant	O
descriptions	O
of	O
the	O
image	O
,	O
whereas	O
as	O
responding	O
to	O
a	O
question	O
needs	O
to	O
find	O
the	O
correct	O
answer	O
for	O
*	O
that	O
*	O
question	O
.	O

Furthermore	O
,	O
the	O
algorithms	O
for	O
visual	B-Task
QA	E-Task
are	O
required	O
to	O
answer	O
all	O
kinds	O
of	O
questions	O
people	O
might	O
ask	O
about	O
the	O
image	O
,	O
some	O
of	O
which	O
might	O
be	O
relevant	O
to	O
the	O
image	O
contents	O
,	O
such	O
as	O
“	O
what	O
books	O
are	O
under	O
the	O
television	O
”	O
and	O
“	O
what	O
is	O
the	O
color	O
of	O
the	O
boat	O
”	O
,	O
while	O
others	O
might	O
require	O
knowledge	O
or	O
reasoning	O
beyond	O
the	O
image	O
content	O
,	O
such	O
as	O
“	O
why	O
is	O
the	O
baby	O
crying	O
?	O
”	O
and	O
“	O
which	O
chair	O
is	O
the	O
most	O
expensive	O
?	O
”	O
.	O

Building	O
robust	B-Method
algorithms	E-Method
for	O
visual	B-Task
QA	E-Task
that	O
perform	O
at	O
near	O
human	O
levels	O
would	O
be	O
an	O
important	O
step	O
towards	O
solving	O
AI	S-Task
.	O

Recently	O
,	O
several	O
papers	O
have	O
appeared	O
on	O
arXiv	O
(	O
after	O
CVPR’16	O
submission	O
deadline	O
)	O
proposing	O
neural	B-Method
network	I-Method
architectures	E-Method
for	O
visual	B-Task
question	I-Task
answering	E-Task
,	O
such	O
as	O
.	O

Some	O
of	O
them	O
are	O
derived	O
from	O
the	O
image	B-Method
captioning	I-Method
framework	E-Method
,	O
in	O
which	O
the	O
output	O
of	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
e.g.	O
,	O
LSTM	S-Method
)	O
applied	O
to	O
the	O
question	O
sentence	O
is	O
concatenated	O
with	O
visual	O
features	O
from	O
VGG	S-Method
or	O
other	O
CNNs	S-Method
to	O
feed	O
a	O
classifier	S-Method
to	O
predict	O
the	O
answer	O
.	O

Other	O
models	O
integrate	O
visual	B-Method
attention	I-Method
mechanisms	E-Method
and	O
visualize	O
how	O
the	O
network	O
learns	O
to	O
attend	O
the	O
local	O
image	O
regions	O
relevant	O
to	O
the	O
content	O
of	O
the	O
question	O
.	O

Interestingly	O
,	O
we	O
notice	O
that	O
in	O
one	O
of	O
the	O
earliest	O
VQA	O
papers	O
,	O
the	O
simple	O
baseline	B-Method
Bag	I-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
+	I-Method
image	I-Method
feature	E-Method
(	O
referred	O
to	O
as	O
BOWIMG	B-Method
baseline	E-Method
)	O
outperforms	O
the	O
LSTM	B-Method
-	I-Method
based	I-Method
models	E-Method
on	O
a	O
synthesized	O
visual	O
QA	O
dataset	O
built	O
up	O
on	O
top	O
of	O
the	O
image	O
captions	O
of	O
COCO	B-Material
dataset	E-Material
.	O

For	O
the	O
recent	O
much	O
larger	O
COCO	B-Material
VQA	I-Material
dataset	E-Material
,	O
the	O
BOWIMG	B-Method
baseline	E-Method
performs	O
worse	O
than	O
the	O
LSTM	B-Method
-	I-Method
based	I-Method
models	E-Method
.	O

In	O
this	O
work	O
,	O
we	O
carefully	O
implement	O
the	O
BOWIMG	B-Method
baseline	I-Method
model	E-Method
.	O

We	O
call	O
it	O
iBOWIMG	S-Method
to	O
avoid	O
confusion	O
with	O
the	O
implementation	O
in	O
.	O

With	O
proper	O
setup	O
and	O
training	O
,	O
this	O
simple	O
baseline	O
model	O
shows	O
comparable	O
performance	O
to	O
many	O
recent	O
recurrent	B-Method
network	I-Method
-	I-Method
based	I-Method
approaches	E-Method
for	O
visual	B-Task
QA	E-Task
.	O

Further	O
analysis	O
shows	O
that	O
the	O
baseline	O
learns	O
to	O
correlate	O
the	O
informative	O
words	O
in	O
the	O
question	O
sentence	O
and	O
visual	O
concepts	O
in	O
the	O
image	O
with	O
the	O
answer	O
.	O

Furthermore	O
,	O
such	O
correlations	O
can	O
be	O
used	O
to	O
compute	O
reasonable	O
spatial	O
attention	O
map	O
with	O
the	O
help	O
of	O
the	O
CAM	B-Method
technique	E-Method
proposed	O
in	O
.	O

The	O
source	O
code	O
and	O
the	O
visual	O
QA	O
demo	O
based	O
on	O
the	O
trained	O
model	O
are	O
publicly	O
available	O
.	O

In	O
the	O
demo	O
,	O
iBOWIMG	S-Method
baseline	O
gives	O
answers	O
to	O
any	O
question	O
relevant	O
to	O
the	O
given	O
images	O
.	O

Playing	O
with	O
the	O
visual	B-Method
QA	I-Method
models	E-Method
interactively	O
could	O
reveal	O
the	O
strengths	O
and	O
weakness	O
of	O
the	O
trained	O
model	O
.	O

section	O
:	O
iBOWIMG	S-Method
for	O
Visual	B-Task
Question	I-Task
Answering	E-Task
In	O
most	O
of	O
the	O
recent	O
proposed	O
models	O
,	O
visual	B-Task
QA	E-Task
is	O
simplified	O
to	O
a	O
classification	B-Task
task	E-Task
:	O
the	O
number	O
of	O
the	O
different	O
answers	O
in	O
the	O
training	O
set	O
is	O
the	O
number	O
of	O
the	O
final	O
classes	O
the	O
models	O
need	O
to	O
learn	O
to	O
predict	O
.	O

The	O
general	O
pipeline	O
of	O
those	O
models	O
is	O
that	O
the	O
word	O
feature	O
extracted	O
from	O
the	O
question	O
sentence	O
is	O
concatenated	O
with	O
the	O
visual	O
feature	O
extracted	O
from	O
the	O
image	O
,	O
then	O
they	O
are	O
fed	O
into	O
a	O
softmax	B-Method
layer	E-Method
to	O
predict	O
the	O
answer	O
class	O
.	O

The	O
visual	O
feature	O
is	O
usually	O
taken	O
from	O
the	O
top	O
of	O
the	O
VGG	B-Method
network	E-Method
or	O
GoogLeNet	S-Method
,	O
while	O
the	O
word	O
features	O
of	O
the	O
question	O
sentence	O
are	O
usually	O
the	O
popular	O
LSTM	B-Method
-	I-Method
based	I-Method
features	E-Method
.	O

In	O
our	O
iBOWIMG	S-Method
model	O
,	O
we	O
simply	O
use	O
naive	O
bag	O
-	O
of	O
-	O
words	O
as	O
the	O
text	O
feature	O
,	O
and	O
use	O
the	O
deep	O
features	O
from	O
GoogLeNet	S-Method
as	O
the	O
visual	O
features	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
framework	O
of	O
the	O
iBOWIMG	S-Method
model	O
,	O
which	O
can	O
be	O
implemented	O
in	O
Torch	S-Method
with	O
no	O
more	O
than	O
10	O
lines	O
of	O
code	O
.	O

The	O
input	O
question	O
is	O
first	O
converted	O
to	O
a	O
one	O
-	O
hot	O
vector	O
,	O
which	O
is	O
transformed	O
to	O
word	O
feature	O
via	O
a	O
word	B-Method
embedding	I-Method
layer	E-Method
and	O
then	O
is	O
concatenated	O
with	O
the	O
image	O
feature	O
from	O
CNN	S-Method
.	O

The	O
combined	O
feature	O
is	O
sent	O
to	O
the	O
softmax	B-Method
layer	E-Method
to	O
predict	O
the	O
answer	O
class	O
,	O
which	O
essentially	O
is	O
a	O
multi	B-Method
-	I-Method
class	I-Method
logistic	I-Method
regression	I-Method
model	E-Method
.	O

section	O
:	O
Experiments	O
Here	O
we	O
train	O
and	O
evaluate	O
the	O
iBOWIMG	S-Method
model	O
on	O
the	O
Full	O
release	O
of	O
COCO	B-Material
VQA	I-Material
dataset	E-Material
,	O
the	O
largest	O
VQA	B-Material
dataset	E-Material
so	O
far	O
.	O

In	O
the	O
COCO	B-Material
VQA	I-Material
dataset	E-Material
,	O
there	O
are	O
3	O
questions	O
annotated	O
by	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
workers	O
for	O
each	O
image	O
in	O
the	O
COCO	B-Material
dataset	E-Material
.	O

For	O
each	O
question	O
,	O
10	O
answers	O
are	O
annotated	O
by	O
another	O
batch	O
of	O
AMT	B-Method
workers	E-Method
.	O

To	O
pre	O
-	O
process	O
the	O
annotation	O
for	O
training	S-Task
,	O
we	O
perform	O
majority	B-Method
voting	E-Method
on	O
the	O
10	O
ground	O
-	O
truth	O
answers	O
to	O
get	O
the	O
most	O
certain	O
answer	O
for	O
each	O
question	O
.	O

Here	O
the	O
answer	O
could	O
be	O
in	O
single	O
word	O
or	O
multiple	O
words	O
.	O

Then	O
we	O
have	O
the	O
3	O
question	O
-	O
answer	O
pairs	O
from	O
each	O
image	O
for	O
training	O
.	O

There	O
are	O
in	O
total	O
248	O
,	O
349	O
pairs	O
in	O
train2014	S-Method
and	O
121	O
,	O
512	O
pairs	O
in	O
val2014	O
,	O
for	O
123	O
,	O
287	O
images	O
overall	O
in	O
the	O
training	O
set	O
.	O

Here	O
train2014	S-Method
and	O
val2014	S-Method
are	O
the	O
standard	O
splits	O
of	O
the	O
image	O
set	O
in	O
the	O
COCO	B-Material
dataset	E-Material
.	O

To	O
generate	O
the	O
training	O
set	O
and	O
validation	O
set	O
for	O
our	O
model	O
,	O
we	O
first	O
randomly	O
split	O
the	O
images	O
of	O
COCO	B-Material
val2014	E-Material
into	O
70	O
%	O
subset	O
A	O
and	O
30	O
%	O
subset	O
B.	O
To	O
avoid	O
potential	O
overfitting	O
,	O
questions	O
sharing	O
the	O
same	O
image	O
will	O
be	O
placed	O
into	O
the	O
same	O
split	O
.	O

The	O
question	O
-	O
answer	O
pairs	O
from	O
the	O
images	O
of	O
COCO	B-Material
train2014	E-Material
+	O
val2014	O
subset	O
A	O
are	O
combined	O
and	O
used	O
for	O
training	O
,	O
while	O
the	O
val2014	O
subset	O
B	O
is	O
used	O
as	O
validation	O
set	O
for	O
parameter	B-Task
tuning	E-Task
.	O

After	O
we	O
find	O
the	O
best	O
model	O
parameters	O
,	O
we	O
combine	O
the	O
whole	O
train2014	S-Method
and	O
val2014	S-Method
to	O
train	O
the	O
final	O
model	O
.	O

We	O
submit	O
the	O
prediction	O
result	O
given	O
by	O
the	O
final	O
model	O
on	O
the	O
testing	O
set	O
(	O
COCO	B-Material
test2015	E-Material
)	O
to	O
the	O
evaluation	O
server	O
,	O
to	O
get	O
the	O
final	O
accuracy	S-Metric
on	O
the	O
test	O
-	O
dev	O
and	O
test	O
-	O
standard	O
set	O
.	O

For	O
Open	B-Material
-	I-Material
Ended	I-Material
Question	I-Material
track	E-Material
,	O
we	O
take	O
the	O
top	O
-	O
1	O
predicted	O
answer	O
from	O
the	O
softmax	O
output	O
.	O

For	O
the	O
Multiple	B-Material
-	I-Material
Choice	I-Material
Question	I-Material
track	E-Material
,	O
we	O
first	O
get	O
the	O
softmax	O
probability	O
for	O
each	O
of	O
the	O
given	O
choices	O
then	O
select	O
the	O
most	O
confident	O
one	O
.	O

The	O
code	O
is	O
implemented	O
in	O
Torch	S-Method
.	O

The	O
training	O
takes	O
about	O
10	O
hours	O
on	O
a	O
single	O
GPU	O
NVIDIA	O
Titan	O
Black	O
.	O

subsection	O
:	O
Benchmark	O
Performance	O
According	O
to	O
the	O
evaluation	O
standard	O
of	O
the	O
VQA	B-Material
dataset	E-Material
,	O
the	O
result	O
of	O
the	O
any	O
proposed	O
VQA	B-Method
models	E-Method
should	O
report	O
accuracy	S-Metric
on	O
the	O
test	O
-	O
standard	O
set	O
for	O
fair	O
comparison	O
.	O

We	O
report	O
our	O
baseline	O
on	O
the	O
test	O
-	O
dev	O
set	O
in	O
Table	O
[	O
reference	O
]	O
and	O
the	O
test	O
-	O
standard	O
set	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
test	O
-	O
dev	O
set	O
is	O
used	O
for	O
debugging	B-Task
and	I-Task
validation	I-Task
experiments	E-Task
and	O
allows	O
for	O
unlimited	O
submission	O
to	O
the	O
evaluation	O
server	O
,	O
while	O
test	O
-	O
standard	O
is	O
used	O
for	O
model	B-Task
comparison	E-Task
with	O
limited	B-Metric
submission	I-Metric
times	E-Metric
.	O

Since	O
this	O
VQA	B-Material
dataset	E-Material
is	O
rather	O
new	O
,	O
the	O
publicly	O
available	O
models	O
evaluated	O
on	O
the	O
dataset	O
are	O
all	O
from	O
non	O
-	O
peer	O
reviewed	O
arXiv	O
papers	O
.	O

We	O
include	O
the	O
performance	O
of	O
the	O
models	O
available	O
at	O
the	O
time	O
of	O
writing	O
(	O
Dec.5	O
,	O
2015	O
)	O
.	O

Note	O
that	O
some	O
models	O
are	O
evaluated	O
on	O
either	O
test	B-Metric
-	I-Metric
dev	E-Metric
or	O
test	B-Metric
-	I-Metric
standard	E-Metric
for	O
either	O
Open	B-Task
-	I-Task
Ended	I-Task
or	I-Task
Multiple	I-Task
-	I-Task
Choice	I-Task
track	E-Task
.	O

The	O
full	O
set	O
of	O
the	O
VQA	B-Material
dataset	E-Material
was	O
released	O
on	O
Oct.6	O
2015	O
;	O
previously	O
the	O
v0.1	O
version	O
and	O
v0.9	O
version	O
had	O
been	O
released	O
.	O

We	O
notice	O
that	O
some	O
models	O
are	O
evaluated	O
using	O
non	O
-	O
standard	O
setups	O
,	O
rendering	O
performance	O
comparisons	O
difficult	O
.	O

(	O
arXiv	O
dated	O
at	O
Nov.17	O
2015	O
)	O
used	O
v0.9	O
version	O
of	O
VQA	S-Method
with	O
their	O
own	O
split	O
of	O
training	O
and	O
testing	O
;	O
(	O
arXiv	O
dated	O
at	O
Nov.7	O
2015	O
)	O
used	O
their	O
own	O
split	O
of	O
training	O
and	O
testing	O
for	O
the	O
val2014	O
;	O
(	O
arXiv	O
dated	O
at	O
Nov.18	O
2015	O
)	O
used	O
v0.9	O
version	O
of	O
VQA	B-Material
dataset	E-Material
.	O

So	O
these	O
are	O
not	O
included	O
in	O
the	O
comparison	O
.	O

Except	O
for	O
these	O
IMG	S-Method
,	O
BOW	S-Method
,	O
BOWIMG	B-Method
baselines	E-Method
provided	O
in	O
the	O
,	O
all	O
the	O
compared	O
methods	O
use	O
either	O
deep	B-Method
or	I-Method
recursive	I-Method
neural	I-Method
networks	E-Method
.	O

However	O
,	O
our	O
iBOWIMG	S-Method
baseline	O
shows	O
comparable	O
performances	O
against	O
these	O
much	O
more	O
complex	O
models	O
,	O
except	O
for	O
DPPnet	S-Method
that	O
is	O
about	O
1.5	O
%	O
better	O
.	O

subsection	O
:	O
Training	O
Details	O
Learning	B-Metric
rate	E-Metric
and	O
weight	B-Metric
clip	E-Metric
.	O

We	O
find	O
that	O
setting	O
up	O
a	O
different	O
learning	B-Metric
rate	E-Metric
and	O
weight	O
clipping	O
for	O
the	O
word	B-Method
embedding	I-Method
layer	E-Method
and	O
softmax	B-Method
layer	E-Method
leads	O
to	O
better	O
performance	O
.	O

The	O
learning	B-Metric
rate	E-Metric
for	O
the	O
word	B-Method
embedding	I-Method
layer	E-Method
should	O
be	O
much	O
higher	O
than	O
the	O
learning	B-Metric
rate	E-Metric
of	O
softmax	B-Method
layer	E-Method
to	O
learn	O
a	O
good	O
word	B-Method
embedding	E-Method
.	O

From	O
the	O
performance	O
of	O
BOW	S-Method
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
a	O
good	O
word	B-Method
model	E-Method
is	O
crucial	O
to	O
the	O
accuracy	S-Metric
,	O
as	O
BOW	S-Method
model	O
alone	O
could	O
achieve	O
closely	O
to	O
48	O
%	O
,	O
even	O
without	O
looking	O
at	O
the	O
image	O
content	O
.	O

Model	O
parameters	O
to	O
tune	O
.	O

Though	O
our	O
model	O
could	O
be	O
considered	O
as	O
the	O
simplest	O
baseline	O
so	O
far	O
for	O
visual	B-Task
QA	E-Task
,	O
there	O
are	O
several	O
model	O
parameters	O
to	O
tune	O
:	O
1	O
)	O
the	O
number	O
of	O
epochs	O
to	O
train	O
.	O

2	O
)	O
the	O
learning	B-Metric
rate	E-Metric
and	O
weight	O
clip	O
.	O

3	O
)	O
the	O
threshold	O
for	O
removing	O
less	O
frequent	O
question	O
word	O
and	O
answer	O
classes	O
.	O

We	O
iterate	O
to	O
search	O
the	O
best	O
value	O
of	O
each	O
model	O
parameter	O
separately	O
on	O
the	O
val2014	O
subset	O
B.	O
In	O
our	O
best	O
model	O
,	O
there	O
are	O
5	O
,	O
746	O
words	O
in	O
the	O
dictionary	O
of	O
question	O
sentence	O
,	O
5	O
,	O
216	O
classes	O
of	O
answers	O
.	O

The	O
specific	O
model	O
parameters	O
can	O
be	O
found	O
in	O
the	O
source	O
code	O
.	O

subsection	O
:	O
Understanding	O
the	O
Visual	B-Task
QA	E-Task
model	O
From	O
the	O
comparisons	O
above	O
,	O
we	O
can	O
see	O
that	O
our	O
baseline	O
model	O
performs	O
as	O
well	O
as	O
the	O
recurrent	B-Method
neural	I-Method
network	I-Method
models	E-Method
on	O
the	O
VQA	B-Material
dataset	E-Material
.	O

Furthermore	O
,	O
due	O
to	O
its	O
simplicity	O
,	O
the	O
behavior	O
of	O
the	O
model	O
could	O
be	O
easily	O
interpreted	O
,	O
demonstrating	O
what	O
it	O
learned	O
for	O
visual	B-Task
QA	E-Task
.	O

Essentially	O
,	O
the	O
BOWIMG	B-Method
baseline	I-Method
model	E-Method
learns	O
to	O
memorize	O
the	O
correlation	O
between	O
the	O
answer	O
class	O
and	O
the	O
informative	O
words	O
in	O
the	O
question	O
sentence	O
along	O
with	O
the	O
visual	O
feature	O
.	O

We	O
split	O
the	O
learned	O
weights	O
of	O
softmax	O
into	O
two	O
parts	O
,	O
one	O
part	O
for	O
the	O
word	O
feature	O
and	O
the	O
other	O
part	O
for	O
the	O
visual	O
feature	O
.	O

Therefore	O
,	O
Here	O
the	O
softmax	O
matrix	O
M	O
is	O
decomposed	O
into	O
the	O
weights	O
for	O
word	O
feature	O
and	O
the	O
weights	O
for	O
the	O
visual	O
feature	O
whereas	O
.	O

is	O
the	O
response	O
of	O
the	O
answer	O
class	O
before	O
softmax	B-Method
normalization	E-Method
.	O

Denote	O
the	O
response	O
as	O
the	O
contribution	O
from	O
question	O
words	O
and	O
as	O
the	O
contribution	O
from	O
the	O
image	O
contents	O
.	O

Thus	O
for	O
each	O
predicted	O
answer	O
,	O
we	O
know	O
exactly	O
the	O
proportions	O
of	O
contribution	O
from	O
word	O
and	O
image	O
content	O
respectively	O
.	O

We	O
also	O
could	O
rank	O
and	O
to	O
know	O
what	O
the	O
predicted	O
answer	O
could	O
be	O
if	O
the	O
model	O
only	O
relies	O
on	O
one	O
side	O
of	O
information	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
some	O
examples	O
of	O
the	O
predictions	O
,	O
revealing	O
that	O
the	O
question	O
words	O
usually	O
have	O
dominant	O
influence	O
on	O
predicting	O
the	O
answer	O
.	O

For	O
example	O
,	O
the	O
correctly	O
predicted	O
answers	O
for	O
the	O
two	O
questions	O
given	O
for	O
the	O
first	O
image	O
‘	O
what	O
is	O
the	O
color	O
of	O
sofa	O
’	O
and	O
‘	O
which	O
brand	O
is	O
the	O
laptop	O
’	O
come	O
mostly	O
from	O
the	O
question	O
words	O
,	O
without	O
the	O
need	O
for	O
image	O
.	O

This	O
demonstrates	O
the	O
bias	O
in	O
the	O
frequency	O
of	O
object	O
and	O
actions	O
appearing	O
in	O
the	O
images	O
of	O
COCO	B-Material
dataset	E-Material
.	O

For	O
the	O
second	O
image	O
,	O
we	O
ask	O
‘	O
what	O
are	O
they	O
doing	O
’	O
:	O
the	O
words	B-Method
-	I-Method
only	I-Method
prediction	E-Method
gives	O
‘	O
playing	O
wii	O
(	O
10.62	O
)	O
,	O
eating	O
(	O
9.97	O
)	O
,	O
playing	O
frisbee	O
(	O
9.24	O
)	O
’	O
,	O
while	O
full	B-Method
prediction	E-Method
gives	O
the	O
correct	O
prediction	O
‘	O
playing	O
baseball	O
(	O
10.67	O
=	O
2.01	O
[	O
image	O
]	O
+	O
8.66	O
[	O
word	O
]	O
)	O
’	O
.	O

To	O
further	O
understand	O
the	O
answers	O
predicted	O
by	O
the	O
model	O
given	O
the	O
visual	O
feature	O
and	O
question	O
sentence	O
,	O
we	O
first	O
decompose	O
the	O
word	O
contribution	O
of	O
the	O
answer	O
into	O
single	O
words	O
of	O
the	O
question	O
sentence	O
,	O
then	O
we	O
visualize	O
the	O
informative	O
image	O
regions	O
relevant	O
to	O
the	O
answer	O
through	O
the	O
technique	O
proposed	O
in	O
.	O

Since	O
there	O
are	O
just	O
two	O
linear	B-Method
transformations	E-Method
(	O
one	O
is	O
word	B-Method
embedding	E-Method
and	O
the	O
other	O
is	O
softmax	B-Method
matrix	I-Method
multiplication	E-Method
)	O
from	O
the	O
one	O
hot	O
vector	O
to	O
the	O
answer	O
response	O
,	O
we	O
could	O
easily	O
know	O
the	O
importance	O
of	O
each	O
single	O
word	O
in	O
the	O
question	O
to	O
the	O
predicted	O
answer	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
plot	O
the	O
ranked	O
word	O
importance	O
for	O
each	O
word	O
in	O
the	O
question	O
sentence	O
.	O

In	O
the	O
first	O
image	O
question	O
word	O
‘	O
doing	O
’	O
is	O
informative	O
to	O
the	O
answer	O
‘	O
texting	O
’	O
while	O
in	O
the	O
second	O
image	O
question	O
word	O
‘	O
eating	O
’	O
is	O
informative	O
to	O
the	O
answer	O
‘	O
hot	O
dog	O
’	O
.	O

To	O
highlight	O
the	O
informative	O
image	O
regions	O
relevant	O
to	O
the	O
predicted	O
answer	O
we	O
apply	O
a	O
technique	O
called	O
Class	B-Method
Activation	I-Method
Mapping	E-Method
(	O
CAM	S-Method
)	O
proposed	O
in	O
.	O

The	O
CAM	B-Method
technique	E-Method
leverages	O
the	O
linear	O
relation	O
between	O
the	O
softmax	B-Method
prediction	E-Method
and	O
the	O
final	O
convolutional	O
feature	O
map	O
,	O
which	O
allows	O
us	O
to	O
identify	O
the	O
most	O
discriminative	O
image	O
regions	O
relevant	O
to	O
the	O
predicted	O
result	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
plot	O
the	O
heatmaps	O
generated	O
by	O
the	O
CAM	S-Method
associated	O
with	O
the	O
predicted	O
answer	O
,	O
which	O
highlight	O
the	O
informative	O
image	O
regions	O
such	O
as	O
the	O
cellphone	O
in	O
the	O
first	O
image	O
to	O
the	O
answer	O
‘	O
texting	O
’	O
and	O
the	O
hot	O
dog	O
in	O
the	O
first	O
image	O
to	O
the	O
answer	O
‘	O
hot	O
dog	O
’	O
.	O

The	O
example	O
in	O
lower	O
part	O
of	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
heatmaps	O
generated	O
by	O
two	O
different	O
questions	O
and	O
answers	O
.	O

Visual	O
features	O
from	O
CNN	S-Method
already	O
have	O
implicit	O
attention	O
and	O
selectivity	O
over	O
the	O
image	O
region	O
,	O
thus	O
the	O
resulting	O
class	O
activation	O
maps	O
are	O
similar	O
to	O
the	O
maps	O
generated	O
by	O
the	O
attention	B-Method
mechanisms	E-Method
of	O
the	O
VQA	B-Method
models	E-Method
in	O
.	O

section	O
:	O
Interactive	O
Visual	B-Task
QA	E-Task
Demo	O
Question	B-Task
answering	E-Task
is	O
essentially	O
an	O
interactive	B-Task
activity	E-Task
,	O
thus	O
it	O
would	O
be	O
good	O
to	O
make	O
the	O
trained	O
models	O
able	O
to	O
interact	O
with	O
people	O
in	O
real	O
time	O
.	O

Aided	O
by	O
the	O
simplicity	O
of	O
the	O
baseline	O
model	O
,	O
we	O
built	O
a	O
web	O
demo	O
that	O
people	O
could	O
type	O
question	O
about	O
a	O
given	O
image	O
and	O
our	O
AI	S-Task
system	O
powered	O
by	O
iBOWIMG	S-Method
will	O
reply	O
the	O
most	O
possible	O
answers	O
.	O

Here	O
the	O
deep	O
feature	O
of	O
the	O
images	O
are	O
extracted	O
beforehand	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
snapshot	O
of	O
the	O
demo	O
.	O

People	O
could	O
play	O
with	O
the	O
demo	O
to	O
see	O
the	O
strength	O
and	O
weakness	O
of	O
VQA	B-Method
model	E-Method
.	O

section	O
:	O
Concluding	O
Remarks	O
For	O
visual	B-Task
question	I-Task
answering	E-Task
on	O
COCO	B-Material
dataset	E-Material
,	O
our	O
implementation	O
of	O
a	O
simple	O
baseline	O
achieves	O
comparable	O
performance	O
to	O
several	O
recently	O
proposed	O
recurrent	B-Method
neural	I-Method
network	I-Method
-	I-Method
based	I-Method
approaches	E-Method
.	O

To	O
reach	O
the	O
correct	O
prediction	O
,	O
the	O
baseline	O
captures	O
the	O
correlation	O
between	O
the	O
informative	O
words	O
in	O
the	O
question	O
and	O
the	O
answer	O
,	O
and	O
that	O
between	O
image	O
contents	O
and	O
the	O
answer	O
.	O

How	O
to	O
move	O
beyond	O
this	O
,	O
from	O
memorizing	O
the	O
correlations	O
to	O
actual	O
reasoning	O
and	O
understanding	O
of	O
the	O
question	O
and	O
image	O
,	O
is	O
a	O
goal	O
for	O
future	O
research	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Adversarial	B-Method
Autoencoders	E-Method
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
‘	O
‘	O
adversarial	B-Method
autoencoder	E-Method
’	O
’	O
(	O
AAE	S-Method
)	O
,	O
which	O
is	O
a	O
probabilistic	B-Method
autoencoder	E-Method
that	O
uses	O
the	O
recently	O
proposed	O
generative	B-Method
adversarial	I-Method
networks	E-Method
(	O
GAN	S-Method
)	O
to	O
perform	O
variational	B-Task
inference	E-Task
by	O
matching	O
the	O
aggregated	O
posterior	O
of	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	S-Method
with	O
an	O
arbitrary	O
prior	O
distribution	O
.	O

Matching	O
the	O
aggregated	O
posterior	O
to	O
the	O
prior	O
ensures	O
that	O
generating	O
from	O
any	O
part	O
of	O
prior	O
space	O
results	O
in	O
meaningful	O
samples	O
.	O

As	O
a	O
result	O
,	O
the	O
decoder	S-Method
of	O
the	O
adversarial	B-Method
autoencoder	E-Method
learns	O
a	O
deep	B-Method
generative	I-Method
model	E-Method
that	O
maps	O
the	O
imposed	O
prior	O
to	O
the	O
data	O
distribution	O
.	O

We	O
show	O
how	O
the	O
adversarial	B-Method
autoencoder	E-Method
can	O
be	O
used	O
in	O
applications	O
such	O
as	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
,	O
disentangling	B-Task
style	I-Task
and	I-Task
content	I-Task
of	I-Task
images	E-Task
,	O
unsupervised	B-Task
clustering	E-Task
,	O
dimensionality	B-Task
reduction	E-Task
and	O
data	B-Task
visualization	E-Task
.	O

We	O
performed	O
experiments	O
on	O
MNIST	S-Material
,	O
Street	B-Material
View	I-Material
House	I-Material
Numbers	E-Material
and	O
Toronto	B-Material
Face	I-Material
datasets	E-Material
and	O
show	O
that	O
adversarial	B-Method
autoencoders	E-Method
achieve	O
competitive	O
results	O
in	O
generative	B-Task
modeling	E-Task
and	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	I-Task
tasks	E-Task
.	O

section	O
:	O
Introduction	O
Building	O
scalable	B-Method
generative	I-Method
models	E-Method
to	O
capture	O
rich	O
distributions	O
such	O
as	O
audio	O
,	O
images	O
or	O
video	O
is	O
one	O
of	O
the	O
central	O
challenges	O
of	O
machine	B-Task
learning	E-Task
.	O

Until	O
recently	O
,	O
deep	B-Method
generative	I-Method
models	E-Method
,	O
such	O
as	O
Restricted	B-Method
Boltzmann	I-Method
Machines	E-Method
(	O
RBM	S-Method
)	O
,	O
Deep	B-Method
Belief	I-Method
Networks	E-Method
(	O
DBNs	S-Method
)	O
and	O
Deep	B-Method
Boltzmann	I-Method
Machines	E-Method
(	O
DBMs	S-Method
)	O
were	O
trained	O
primarily	O
by	O
MCMC	B-Method
-	I-Method
based	I-Method
algorithms	E-Method
geoff	O
,	O
russ	S-Method
.	O

In	O
these	O
approaches	O
the	O
MCMC	B-Method
methods	E-Method
compute	O
the	O
gradient	B-Method
of	I-Method
log	I-Method
-	I-Method
likelihood	E-Method
which	O
becomes	O
more	O
imprecise	O
as	O
training	O
progresses	O
.	O

This	O
is	O
because	O
samples	O
from	O
the	O
Markov	B-Method
Chains	E-Method
are	O
unable	O
to	O
mix	O
between	O
modes	O
fast	O
enough	O
.	O

In	O
recent	O
years	O
,	O
generative	B-Method
models	E-Method
have	O
been	O
developed	O
that	O
may	O
be	O
trained	O
via	O
direct	B-Method
back	I-Method
-	I-Method
propagation	E-Method
and	O
avoid	O
the	O
difficulties	O
that	O
come	O
with	O
MCMC	B-Method
training	E-Method
.	O

For	O
example	O
,	O
variational	B-Method
autoencoders	E-Method
(	O
VAE	S-Method
)	O
vae	S-Method
,	O
rezende	O
or	O
importance	O
weighted	B-Method
autoencoders	I-Method
yuri	E-Method
use	O
a	O
recognition	B-Method
network	E-Method
to	O
predict	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
,	O
generative	B-Method
adversarial	I-Method
networks	E-Method
(	O
GAN	S-Method
)	O
gan	S-Method
use	O
an	O
adversarial	B-Method
training	I-Method
procedure	E-Method
to	O
directly	O
shape	O
the	O
output	O
distribution	O
of	O
the	O
network	O
via	O
back	B-Method
-	I-Method
propagation	E-Method
and	O
generative	B-Method
moment	I-Method
matching	I-Method
networks	E-Method
(	O
GMMN	S-Method
)	O
gmmn	S-Method
use	O
a	O
moment	B-Method
matching	I-Method
cost	I-Method
function	E-Method
to	O
learn	O
the	O
data	O
distribution	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
general	O
approach	O
,	O
called	O
an	O
adversarial	B-Method
autoencoder	E-Method
(	O
AAE	S-Method
)	O
that	O
can	O
turn	O
an	O
autoencoder	S-Method
into	O
a	O
generative	B-Method
model	E-Method
.	O

In	O
our	O
model	O
,	O
an	O
autoencoder	S-Method
is	O
trained	O
with	O
dual	O
objectives	O
–	O
a	O
traditional	O
reconstruction	B-Metric
error	I-Metric
criterion	E-Metric
,	O
and	O
an	O
adversarial	B-Method
training	I-Method
criterion	I-Method
gan	E-Method
that	O
matches	O
the	O
aggregated	O
posterior	O
distribution	O
of	O
the	O
latent	B-Method
representation	E-Method
of	O
the	O
autoencoder	S-Method
to	O
an	O
arbitrary	O
prior	O
distribution	O
.	O

We	O
show	O
that	O
this	O
training	O
criterion	O
has	O
a	O
strong	O
connection	O
to	O
VAE	S-Method
training	O
.	O

The	O
result	O
of	O
the	O
training	O
is	O
that	O
the	O
encoder	S-Method
learns	O
to	O
convert	O
the	O
data	O
distribution	O
to	O
the	O
prior	O
distribution	O
,	O
while	O
the	O
decoder	S-Method
learns	O
a	O
deep	B-Method
generative	I-Method
model	E-Method
that	O
maps	O
the	O
imposed	O
prior	O
to	O
the	O
data	O
distribution	O
.	O

subsection	O
:	O
Generative	B-Method
Adversarial	I-Method
Networks	E-Method
The	O
Generative	B-Method
Adversarial	I-Method
Networks	E-Method
(	O
GAN	S-Method
)	O
gan	S-Method
framework	O
establishes	O
a	O
min	B-Method
-	I-Method
max	I-Method
adversarial	I-Method
game	E-Method
between	O
two	O
neural	B-Method
networks	E-Method
–	O
a	O
generative	B-Method
model	E-Method
,	O
,	O
and	O
a	O
discriminative	B-Method
model	E-Method
,	O
.	O

The	O
discriminator	B-Method
model	E-Method
,	O
,	O
is	O
a	O
neural	B-Method
network	E-Method
that	O
computes	O
the	O
probability	O
that	O
a	O
point	O
in	O
data	O
space	O
is	O
a	O
sample	O
from	O
the	O
data	O
distribution	O
(	O
positive	O
samples	O
)	O
that	O
we	O
are	O
trying	O
to	O
model	O
,	O
rather	O
than	O
a	O
sample	O
from	O
our	O
generative	B-Method
model	E-Method
(	O
negative	O
samples	O
)	O
.	O

Concurrently	O
,	O
the	O
generator	O
uses	O
a	O
function	O
that	O
maps	O
samples	O
from	O
the	O
prior	O
to	O
the	O
data	O
space	O
.	O

is	O
trained	O
to	O
maximally	O
confuse	O
the	O
discriminator	O
into	O
believing	O
that	O
samples	O
it	O
generates	O
come	O
from	O
the	O
data	O
distribution	O
.	O

The	O
generator	O
is	O
trained	O
by	O
leveraging	O
the	O
gradient	B-Method
of	I-Method
w.r.t	E-Method
.	O

,	O
and	O
using	O
that	O
to	O
modify	O
its	O
parameters	O
.	O

The	O
solution	O
to	O
this	O
game	O
can	O
be	O
expressed	O
as	O
following	O
gan	S-Method
:	O
The	O
generator	S-Method
and	O
the	O
discriminator	S-Method
can	O
be	O
found	O
using	O
alternating	B-Method
SGD	E-Method
in	O
two	O
stages	O
:	O
(	O
a	O
)	O
Train	O
the	O
discriminator	S-Method
to	O
distinguish	O
the	O
true	O
samples	O
from	O
the	O
fake	O
samples	O
generated	O
by	O
the	O
generator	O
.	O

(	O
b	O
)	O
Train	O
the	O
generator	S-Method
so	O
as	O
to	O
fool	O
the	O
discriminator	S-Method
with	O
its	O
generated	O
samples	O
.	O

section	O
:	O
Adversarial	B-Method
Autoencoders	E-Method
Let	O
be	O
the	O
input	O
and	O
be	O
the	O
latent	O
code	O
vector	O
(	O
hidden	O
units	O
)	O
of	O
an	O
autoencoder	S-Method
with	O
a	O
deep	B-Method
encoder	I-Method
and	I-Method
decoder	E-Method
.	O

Let	O
be	O
the	O
prior	O
distribution	O
we	O
want	O
to	O
impose	O
on	O
the	O
codes	O
,	O
be	O
an	O
encoding	B-Method
distribution	E-Method
and	O
be	O
the	O
decoding	B-Method
distribution	E-Method
.	O

Also	O
let	O
be	O
the	O
data	O
distribution	O
,	O
and	O
be	O
the	O
model	B-Method
distribution	E-Method
.	O

The	O
encoding	B-Method
function	E-Method
of	O
the	O
autoencoder	S-Method
defines	O
an	O
aggregated	O
posterior	O
distribution	O
of	O
on	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	S-Method
as	O
follows	O
:	O
The	O
adversarial	B-Method
autoencoder	E-Method
is	O
an	O
autoencoder	S-Method
that	O
is	O
regularized	O
by	O
matching	O
the	O
aggregated	O
posterior	O
,	O
,	O
to	O
an	O
arbitrary	O
prior	O
,	O
.	O

In	O
order	O
to	O
do	O
so	O
,	O
an	O
adversarial	B-Method
network	E-Method
is	O
attached	O
on	O
top	O
of	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	S-Method
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

It	O
is	O
the	O
adversarial	B-Method
network	E-Method
that	O
guides	O
to	O
match	O
.	O

The	O
autoencoder	S-Method
,	O
meanwhile	O
,	O
attempts	O
to	O
minimize	O
the	O
reconstruction	B-Metric
error	E-Metric
.	O

The	O
generator	O
of	O
the	O
adversarial	B-Method
network	E-Method
is	O
also	O
the	O
encoder	O
of	O
the	O
autoencoder	S-Method
.	O

The	O
encoder	S-Method
ensures	O
the	O
aggregated	O
posterior	O
distribution	O
can	O
fool	O
the	O
discriminative	B-Method
adversarial	I-Method
network	E-Method
into	O
thinking	O
that	O
the	O
hidden	O
code	O
comes	O
from	O
the	O
true	O
prior	O
distribution	O
.	O

Both	O
,	O
the	O
adversarial	B-Method
network	E-Method
and	O
the	O
autoencoder	S-Method
are	O
trained	O
jointly	O
with	O
SGD	S-Method
in	O
two	O
phases	O
–	O
the	O
reconstruction	B-Method
phase	E-Method
and	O
the	O
regularization	B-Method
phase	E-Method
–	O
executed	O
on	O
each	O
mini	O
-	O
batch	O
.	O

In	O
the	O
reconstruction	B-Task
phase	E-Task
,	O
the	O
autoencoder	S-Method
updates	O
the	O
encoder	S-Method
and	O
the	O
decoder	S-Method
to	O
minimize	O
the	O
reconstruction	B-Metric
error	E-Metric
of	O
the	O
inputs	O
.	O

In	O
the	O
regularization	B-Task
phase	E-Task
,	O
the	O
adversarial	B-Method
network	E-Method
first	O
updates	O
its	O
discriminative	B-Method
network	E-Method
to	O
tell	O
apart	O
the	O
true	O
samples	O
(	O
generated	O
using	O
the	O
prior	O
)	O
from	O
the	O
generated	O
samples	O
(	O
the	O
hidden	O
codes	O
computed	O
by	O
the	O
autoencoder	S-Method
)	O
.	O

The	O
adversarial	B-Method
network	E-Method
then	O
updates	O
its	O
generator	S-Method
(	O
which	O
is	O
also	O
the	O
encoder	O
of	O
the	O
autoencoder	S-Method
)	O
to	O
confuse	O
the	O
discriminative	B-Method
network	E-Method
.	O

Once	O
the	O
training	O
procedure	O
is	O
done	O
,	O
the	O
decoder	S-Method
of	O
the	O
autoencoder	S-Method
will	O
define	O
a	O
generative	B-Method
model	E-Method
that	O
maps	O
the	O
imposed	O
prior	O
of	O
to	O
the	O
data	O
distribution	O
.	O

There	O
are	O
several	O
possible	O
choices	O
for	O
the	O
encoder	S-Method
,	O
,	O
of	O
adversarial	B-Method
autoencoders	E-Method
:	O
Deterministic	O
:	O
Here	O
we	O
assume	O
that	O
is	O
a	O
deterministic	O
function	O
of	O
.	O

In	O
this	O
case	O
,	O
the	O
encoder	O
is	O
similar	O
to	O
the	O
encoder	S-Method
of	O
a	O
standard	O
autoencoder	S-Method
and	O
the	O
only	O
source	O
of	O
stochasticity	O
in	O
is	O
the	O
data	O
distribution	O
,	O
.	O

Gaussian	B-Method
posterior	E-Method
:	O
Here	O
we	O
assume	O
that	O
is	O
a	O
Gaussian	O
distribution	O
whose	O
mean	O
and	O
variance	O
is	O
predicted	O
by	O
the	O
encoder	B-Method
network	E-Method
:	O
.	O

In	O
this	O
case	O
,	O
the	O
stochasticity	O
in	O
comes	O
from	O
both	O
the	O
data	O
-	O
distribution	O
and	O
the	O
randomness	O
of	O
the	O
Gaussian	B-Method
distribution	E-Method
at	O
the	O
output	O
of	O
the	O
encoder	S-Method
.	O

We	O
can	O
use	O
the	O
same	O
re	B-Method
-	I-Method
parametrization	I-Method
trick	E-Method
of	O
vae	S-Method
for	O
back	B-Task
-	I-Task
propagation	E-Task
through	O
the	O
encoder	B-Method
network	E-Method
.	O

Universal	B-Method
approximator	I-Method
posterior	E-Method
:	O
Adversarial	B-Method
autoencoders	E-Method
can	O
be	O
used	O
to	O
train	O
the	O
as	O
the	O
universal	B-Method
approximator	I-Method
of	I-Method
the	I-Method
posterior	E-Method
.	O

Suppose	O
the	O
encoder	B-Method
network	E-Method
of	O
the	O
adversarial	B-Method
autoencoder	E-Method
is	O
the	O
function	O
that	O
takes	O
the	O
input	O
and	O
a	O
random	O
noise	O
with	O
a	O
fixed	O
distribution	O
(	O
e.g.	O
,	O
Gaussian	O
)	O
.	O

We	O
can	O
sample	O
from	O
arbitrary	O
posterior	O
distribution	O
,	O
by	O
evaluating	O
at	O
different	O
samples	O
of	O
.	O

In	O
other	O
words	O
,	O
we	O
can	O
assume	O
and	O
the	O
posterior	O
and	O
the	O
aggregated	O
posterior	O
are	O
defined	O
as	O
follows	O
:	O
In	O
this	O
case	O
,	O
the	O
stochasticity	O
in	O
comes	O
from	O
both	O
the	O
data	O
-	O
distribution	O
and	O
the	O
random	O
noise	O
at	O
the	O
input	O
of	O
the	O
encoder	S-Method
.	O

Note	O
that	O
in	O
this	O
case	O
the	O
posterior	O
is	O
no	O
longer	O
constrained	O
to	O
be	O
Gaussian	O
and	O
the	O
encoder	S-Method
can	O
learn	O
any	O
arbitrary	O
posterior	O
distribution	O
for	O
a	O
given	O
input	O
.	O

Since	O
there	O
is	O
an	O
efficient	O
method	O
of	O
sampling	O
from	O
the	O
aggregated	O
posterior	O
,	O
the	O
adversarial	B-Method
training	I-Method
procedure	E-Method
can	O
match	O
to	O
by	O
direct	O
back	B-Method
-	I-Method
propagation	E-Method
through	O
the	O
encoder	B-Method
network	E-Method
.	O

Choosing	O
different	O
types	O
of	O
will	O
result	O
in	O
different	O
kinds	O
of	O
models	O
with	O
different	O
training	O
dynamics	O
.	O

For	O
example	O
,	O
in	O
the	O
deterministic	O
case	O
of	O
,	O
the	O
network	O
has	O
to	O
match	O
to	O
by	O
only	O
exploiting	O
the	O
stochasticity	O
of	O
the	O
data	O
distribution	O
,	O
but	O
since	O
the	O
empirical	O
distribution	O
of	O
the	O
data	O
is	O
fixed	O
by	O
the	O
training	O
set	O
,	O
and	O
the	O
mapping	S-Method
is	O
deterministic	O
,	O
this	O
might	O
produce	O
a	O
that	O
is	O
not	O
very	O
smooth	O
.	O

However	O
,	O
in	O
the	O
Gaussian	B-Task
or	I-Task
universal	I-Task
approximator	I-Task
case	E-Task
,	O
the	O
network	O
has	O
access	O
to	O
additional	O
sources	O
of	O
stochasticity	O
that	O
could	O
help	O
it	O
in	O
the	O
adversarial	B-Method
regularization	E-Method
stage	O
by	O
smoothing	O
out	O
.	O

Nevertheless	O
,	O
after	O
extensive	O
hyper	O
-	O
parameter	O
search	O
,	O
we	O
obtained	O
similar	O
test	O
-	O
likelihood	O
with	O
each	O
type	O
of	O
.	O

So	O
in	O
the	O
rest	O
of	O
the	O
paper	O
,	O
we	O
only	O
report	O
results	O
with	O
the	O
deterministic	O
version	O
of	O
.	O

subsection	O
:	O
Relationship	O
to	O
Variational	B-Method
Autoencoders	E-Method
Our	O
work	O
is	O
similar	O
in	O
spirit	O
to	O
variational	O
autoencoders	O
vae	S-Method
;	O
however	O
,	O
while	O
they	O
use	O
a	O
KL	O
divergence	O
penalty	O
to	O
impose	O
a	O
prior	O
distribution	O
on	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	S-Method
,	O
we	O
use	O
an	O
adversarial	B-Method
training	I-Method
procedure	E-Method
to	O
do	O
so	O
by	O
matching	O
the	O
aggregated	O
posterior	O
of	O
the	O
hidden	O
code	O
vector	O
with	O
the	O
prior	O
distribution	O
.	O

VAE	B-Method
vae	E-Method
minimizes	O
the	O
following	O
upper	O
-	O
bound	O
on	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
:	O
where	O
the	O
aggregated	O
posterior	O
is	O
defined	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
we	O
have	O
assumed	O
is	O
Gaussian	O
and	O
is	O
an	O
arbitrary	O
distribution	O
.	O

The	O
variational	B-Method
bound	E-Method
contains	O
three	O
terms	O
.	O

The	O
first	O
term	O
can	O
be	O
viewed	O
as	O
the	O
reconstruction	O
term	O
of	O
an	O
autoencoder	S-Method
and	O
the	O
second	O
and	O
third	O
terms	O
can	O
be	O
viewed	O
as	O
regularization	O
terms	O
.	O

Without	O
the	O
regularization	O
terms	O
,	O
the	O
model	O
is	O
simply	O
a	O
standard	O
autoencoder	S-Method
that	O
reconstructs	O
the	O
input	O
.	O

However	O
,	O
in	O
the	O
presence	O
of	O
the	O
regularization	O
terms	O
,	O
the	O
VAE	S-Method
learns	O
a	O
latent	B-Method
representation	E-Method
that	O
is	O
compatible	O
with	O
.	O

The	O
second	O
term	O
of	O
the	O
cost	O
function	O
encourages	O
large	O
variances	O
for	O
the	O
posterior	O
distribution	O
while	O
the	O
third	O
term	O
minimizes	O
the	O
cross	B-Metric
-	I-Metric
entropy	E-Metric
between	O
the	O
aggregated	O
posterior	O
and	O
the	O
prior	O
.	O

KL	O
divergence	O
or	O
the	O
cross	O
-	O
entropy	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
encourages	O
to	O
pick	O
the	O
modes	O
of	O
.	O

In	O
adversarial	B-Method
autoencoders	E-Method
,	O
we	O
replace	O
the	O
second	O
two	O
terms	O
with	O
an	O
adversarial	B-Method
training	I-Method
procedure	E-Method
that	O
encourages	O
to	O
match	O
to	O
the	O
whole	O
distribution	O
of	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
the	O
ability	O
of	O
the	O
adversarial	B-Method
autoencoder	E-Method
to	O
the	O
VAE	S-Method
to	O
impose	O
a	O
specified	O
prior	O
distribution	O
on	O
the	O
coding	O
distribution	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
shows	O
the	O
coding	O
space	O
of	O
the	O
test	O
data	O
resulting	O
from	O
an	O
adversarial	B-Method
autoencoder	E-Method
trained	O
on	O
MNIST	S-Material
digits	O
in	O
which	O
a	O
spherical	O
2	O
-	O
D	O
Gaussian	O
prior	O
distribution	O
is	O
imposed	O
on	O
the	O
hidden	O
codes	O
.	O

The	O
learned	O
manifold	O
in	O
Figure	O
[	O
reference	O
]	O
a	O
exhibits	O
sharp	O
transitions	O
indicating	O
that	O
the	O
coding	O
space	O
is	O
filled	O
and	O
exhibits	O
no	O
‘	O
‘	O
holes	O
’	O
’	O
.	O

In	O
practice	O
,	O
sharp	O
transitions	O
in	O
the	O
coding	O
space	O
indicate	O
that	O
images	O
generated	O
by	O
interpolating	O
within	O
lie	O
on	O
the	O
data	O
manifold	O
(	O
Figure	O
[	O
reference	O
]	O
e	O
)	O
.	O

By	O
contrast	O
,	O
Figure	O
[	O
reference	O
]	O
c	O
shows	O
the	O
coding	O
space	O
of	O
a	O
VAE	S-Method
with	O
the	O
same	O
architecture	O
used	O
in	O
the	O
adversarial	B-Method
autoencoder	E-Method
experiments	O
.	O

We	O
can	O
see	O
that	O
in	O
this	O
case	O
the	O
VAE	S-Method
roughly	O
matches	O
the	O
shape	O
of	O
a	O
2	B-Method
-	I-Method
D	I-Method
Gaussian	I-Method
distribution	E-Method
.	O

However	O
,	O
no	O
data	O
points	O
map	O
to	O
several	O
local	O
regions	O
of	O
the	O
coding	O
space	O
indicating	O
that	O
the	O
VAE	S-Method
may	O
not	O
have	O
captured	O
the	O
data	O
manifold	O
as	O
well	O
as	O
the	O
adversarial	B-Method
autoencoder	E-Method
.	O

Figures	O
[	O
reference	O
]	O
b	O
and	O
[	O
reference	O
]	O
d	O
show	O
the	O
code	O
space	O
of	O
an	O
adversarial	B-Method
autoencoder	E-Method
and	O
of	O
a	O
VAE	S-Method
where	O
the	O
imposed	O
distribution	O
is	O
a	O
mixture	B-Method
of	I-Method
10	I-Method
2	I-Method
-	I-Method
D	I-Method
Gaussians	E-Method
.	O

The	O
adversarial	B-Method
autoencoder	E-Method
successfully	O
matched	O
the	O
aggregated	O
posterior	O
with	O
the	O
prior	O
distribution	O
(	O
Figure	O
[	O
reference	O
]	O
b	O
)	O
.	O

In	O
contrast	O
,	O
the	O
VAE	S-Method
exhibit	O
systematic	O
differences	O
from	O
the	O
mixture	B-Method
10	I-Method
Gaussians	E-Method
indicating	O
that	O
the	O
VAE	S-Method
emphasizes	O
matching	O
the	O
modes	O
of	O
the	O
distribution	O
as	O
discussed	O
above	O
(	O
Figure	O
[	O
reference	O
]	O
d	O
)	O
.	O

An	O
important	O
difference	O
between	O
VAEs	S-Method
and	O
adversarial	B-Method
autoencoders	E-Method
is	O
that	O
in	O
VAEs	S-Method
,	O
in	O
order	O
to	O
back	O
-	O
propagate	O
through	O
the	O
KL	O
divergence	O
by	O
Monte	B-Method
-	I-Method
Carlo	I-Method
sampling	E-Method
,	O
we	O
need	O
to	O
have	O
access	O
to	O
the	O
exact	O
functional	O
form	O
of	O
the	O
prior	O
distribution	O
.	O

However	O
,	O
in	O
AAEs	S-Method
,	O
we	O
only	O
need	O
to	O
be	O
able	O
to	O
sample	O
from	O
the	O
prior	O
distribution	O
in	O
order	O
to	O
induce	O
to	O
match	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
demonstrate	O
that	O
the	O
adversarial	B-Method
autoencoder	E-Method
can	O
impose	O
complicated	O
distributions	O
(	O
e.g.	O
,	O
swiss	O
roll	O
distribution	O
)	O
without	O
having	O
access	O
to	O
the	O
explicit	O
functional	O
form	O
of	O
the	O
distribution	O
.	O

subsection	O
:	O
Relationship	O
to	O
GANs	S-Method
and	O
GMMNs	S-Method
In	O
the	O
original	O
generative	B-Method
adversarial	I-Method
networks	E-Method
(	O
GAN	S-Method
)	O
paper	O
gan	S-Method
,	O
GANs	S-Method
were	O
used	O
to	O
impose	O
the	O
data	O
distribution	O
at	O
the	O
pixel	O
level	O
on	O
the	O
output	O
layer	O
of	O
a	O
neural	B-Method
network	E-Method
.	O

Adversarial	B-Method
autoencoders	E-Method
,	O
however	O
,	O
rely	O
on	O
the	O
autoencoder	B-Method
training	E-Method
to	O
capture	O
the	O
data	O
distribution	O
.	O

In	O
adversarial	B-Method
training	I-Method
procedure	E-Method
of	O
our	O
method	O
,	O
a	O
much	O
simpler	O
distribution	O
(	O
e.g.	O
,	O
Gaussian	O
as	O
opposed	O
to	O
the	O
data	O
distribution	O
)	O
is	O
imposed	O
in	O
a	O
much	O
lower	O
dimensional	O
space	O
(	O
e.g.	O
,	O
as	O
opposed	O
to	O
)	O
which	O
results	O
in	O
a	O
better	O
test	B-Metric
-	I-Metric
likelihood	E-Metric
as	O
is	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Generative	B-Method
moment	I-Method
matching	I-Method
networks	E-Method
(	O
GMMN	S-Method
)	O
gmmn	S-Method
use	O
the	O
maximum	B-Method
mean	I-Method
discrepancy	E-Method
(	O
MMD	S-Method
)	O
objective	O
to	O
shape	O
the	O
distribution	O
of	O
the	O
output	O
layer	O
of	O
a	O
neural	B-Method
network	E-Method
.	O

The	O
MMD	S-Method
objective	O
can	O
be	O
interpreted	O
as	O
minimizing	O
the	O
distance	O
between	O
all	O
moments	O
of	O
the	O
model	B-Method
distribution	E-Method
and	O
the	O
data	O
distribution	O
.	O

It	O
has	O
been	O
shown	O
that	O
GMMNs	S-Method
can	O
be	O
combined	O
with	O
pre	O
-	O
trained	O
dropout	B-Method
autoencoders	E-Method
to	O
achieve	O
better	O
likelihood	O
results	O
(	O
GMMN	B-Method
+	I-Method
AE	E-Method
)	O
.	O

Our	O
adversarial	B-Method
autoencoder	E-Method
also	O
relies	O
on	O
the	O
autoencoder	S-Method
to	O
capture	O
the	O
data	O
distribution	O
.	O

However	O
,	O
the	O
main	O
difference	O
of	O
our	O
work	O
with	O
GMMN	B-Method
+	I-Method
AE	E-Method
is	O
that	O
the	O
adversarial	B-Method
training	I-Method
procedure	E-Method
of	O
our	O
method	O
acts	O
as	O
a	O
regularizer	S-Method
that	O
shapes	O
the	O
code	O
distribution	O
while	O
training	O
the	O
autoencoder	S-Method
from	O
scratch	O
;	O
whereas	O
,	O
the	O
GMMN	S-Method
+	O
AE	O
model	O
first	O
trains	O
a	O
standard	O
dropout	B-Method
autoencoder	E-Method
and	O
then	O
fits	O
a	O
distribution	O
in	O
the	O
code	O
space	O
of	O
the	O
pre	O
-	O
trained	O
network	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
show	O
that	O
the	O
test	B-Metric
-	I-Metric
likelihood	E-Metric
achieved	O
by	O
the	O
joint	B-Method
training	I-Method
scheme	E-Method
of	O
adversarial	B-Method
autoencoders	E-Method
outperforms	O
the	O
test	B-Metric
-	I-Metric
likelihood	E-Metric
of	O
GMMN	S-Method
and	O
GMMN	B-Method
+	I-Method
AE	E-Method
on	O
MNIST	S-Material
and	O
Toronto	B-Material
Face	I-Material
datasets	E-Material
.	O

subsection	O
:	O
Incorporating	O
Label	O
Information	O
in	O
the	O
Adversarial	B-Task
Regularization	E-Task
In	O
the	O
scenarios	O
where	O
data	O
is	O
labeled	O
,	O
we	O
can	O
incorporate	O
the	O
label	O
information	O
in	O
the	O
adversarial	B-Method
training	I-Method
stage	E-Method
to	O
better	O
shape	O
the	O
distribution	O
of	O
the	O
hidden	O
code	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
how	O
to	O
leverage	O
partial	O
or	O
complete	O
label	O
information	O
to	O
regularize	O
the	O
latent	B-Method
representation	E-Method
of	O
the	O
autoencoder	S-Method
more	O
heavily	O
.	O

To	O
demonstrate	O
this	O
architecture	O
we	O
return	O
to	O
Figure	O
[	O
reference	O
]	O
b	O
in	O
which	O
the	O
adversarial	B-Method
autoencoder	E-Method
is	O
fit	O
to	O
a	O
mixture	B-Method
of	I-Method
10	I-Method
2	I-Method
-	I-Method
D	I-Method
Gaussians	E-Method
.	O

We	O
now	O
aim	O
to	O
force	O
each	O
mode	O
of	O
the	O
mixture	B-Method
of	I-Method
Gaussian	I-Method
distribution	E-Method
to	O
represent	O
a	O
single	O
label	O
of	O
MNIST	S-Material
.	O

Figure	O
[	O
reference	O
]	O
demonstrates	O
the	O
training	O
procedure	O
for	O
this	O
semi	B-Method
-	I-Method
supervised	I-Method
approach	E-Method
.	O

We	O
add	O
a	O
one	O
-	O
hot	O
vector	O
to	O
the	O
input	O
of	O
the	O
discriminative	B-Method
network	E-Method
to	O
associate	O
the	O
label	O
with	O
a	O
mode	O
of	O
the	O
distribution	O
.	O

The	O
one	O
-	O
hot	O
vector	O
acts	O
as	O
switch	O
that	O
selects	O
the	O
corresponding	O
decision	O
boundary	O
of	O
the	O
discriminative	B-Method
network	E-Method
given	O
the	O
class	O
label	O
.	O

This	O
one	O
-	O
hot	O
vector	O
has	O
an	O
extra	O
class	O
for	O
unlabeled	O
examples	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
imposing	O
a	O
mixture	O
of	O
10	O
2	B-Method
-	I-Method
D	I-Method
Gaussians	E-Method
(	O
Figure	O
[	O
reference	O
]	O
b	O
and	O
[	O
reference	O
]	O
a	O
)	O
,	O
the	O
one	O
hot	O
vector	O
contains	O
11	O
classes	O
.	O

Each	O
of	O
the	O
first	O
10	O
class	O
selects	O
a	O
decision	O
boundary	O
for	O
the	O
corresponding	O
individual	O
mixture	B-Method
component	E-Method
.	O

The	O
extra	O
class	O
in	O
the	O
one	O
-	O
hot	O
vector	O
corresponds	O
to	O
unlabeled	O
training	O
points	O
.	O

When	O
an	O
unlabeled	O
point	O
is	O
presented	O
to	O
the	O
model	O
,	O
the	O
extra	O
class	O
is	O
turned	O
on	O
,	O
to	O
select	O
the	O
decision	O
boundary	O
for	O
the	O
full	B-Method
mixture	I-Method
of	I-Method
Gaussian	I-Method
distribution	E-Method
.	O

During	O
the	O
positive	B-Task
phase	E-Task
of	O
adversarial	B-Task
training	E-Task
,	O
we	O
provide	O
the	O
label	O
of	O
the	O
mixture	B-Method
component	E-Method
(	O
that	O
the	O
positive	O
sample	O
is	O
drawn	O
from	O
)	O
to	O
the	O
discriminator	O
through	O
the	O
one	B-Method
-	I-Method
hot	I-Method
vector	E-Method
.	O

The	O
positive	O
samples	O
fed	O
for	O
unlabeled	O
examples	O
come	O
from	O
the	O
full	O
mixture	B-Method
of	I-Method
Gaussian	E-Method
,	O
rather	O
than	O
from	O
a	O
particular	O
class	O
.	O

During	O
the	O
negative	B-Task
phase	E-Task
,	O
we	O
provide	O
the	O
label	O
of	O
the	O
training	O
point	O
image	O
to	O
the	O
discriminator	O
through	O
the	O
one	B-Method
-	I-Method
hot	I-Method
vector	E-Method
.	O

Figure	O
[	O
reference	O
]	O
a	O
shows	O
the	O
latent	B-Method
representation	E-Method
of	O
an	O
adversarial	B-Method
autoencoder	E-Method
trained	O
with	O
a	O
prior	O
that	O
is	O
a	O
mixture	O
of	O
10	O
2	B-Method
-	I-Method
D	I-Method
Gaussians	E-Method
trained	O
on	O
10	O
K	O
labeled	O
MNIST	S-Material
examples	O
and	O
40	O
K	O
unlabeled	O
MNIST	S-Material
examples	O
.	O

In	O
this	O
case	O
,	O
the	O
-	O
th	O
mixture	B-Method
component	E-Method
of	O
the	O
prior	O
has	O
been	O
assigned	O
to	O
the	O
-	O
th	O
class	O
in	O
a	O
semi	B-Method
-	I-Method
supervised	I-Method
fashion	E-Method
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
the	O
manifold	O
of	O
the	O
first	O
three	O
mixture	B-Method
components	E-Method
.	O

Note	O
that	O
the	O
style	B-Method
representation	E-Method
is	O
consistently	O
represented	O
within	O
each	O
mixture	B-Method
component	E-Method
,	O
independent	O
of	O
its	O
class	O
.	O

For	O
example	O
,	O
the	O
upper	O
-	O
left	O
region	O
of	O
all	O
panels	O
in	O
Figure	O
[	O
reference	O
]	O
b	O
correspond	O
to	O
the	O
upright	O
writing	O
style	O
and	O
lower	O
-	O
right	O
region	O
of	O
these	O
panels	O
correspond	O
to	O
the	O
tilted	O
writing	O
style	O
of	O
digits	O
.	O

This	O
method	O
may	O
be	O
extended	O
to	O
arbitrary	O
distributions	O
with	O
no	O
parametric	O
forms	O
–	O
as	O
demonstrated	O
by	O
mapping	O
the	O
MNIST	S-Material
data	O
set	O
onto	O
a	O
‘	O
‘	O
swiss	O
roll	O
’	O
’	O
(	O
a	O
conditional	B-Method
Gaussian	I-Method
distribution	E-Method
whose	O
mean	O
is	O
uniformly	O
distributed	O
along	O
the	O
length	O
of	O
a	O
swiss	O
roll	O
axis	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
depicts	O
the	O
coding	O
space	O
and	O
Figure	O
[	O
reference	O
]	O
d	O
highlights	O
the	O
images	O
generated	O
by	O
walking	O
along	O
the	O
swiss	O
roll	O
axis	O
in	O
the	O
latent	O
space	O
.	O

section	O
:	O
Likelihood	B-Method
Analysis	E-Method
of	O
Adversarial	B-Method
Autoencoders	E-Method
The	O
experiments	O
presented	O
in	O
the	O
previous	O
sections	O
have	O
only	O
demonstrated	O
qualitative	O
results	O
.	O

In	O
this	O
section	O
we	O
measure	O
the	O
ability	O
of	O
the	O
AAE	S-Method
as	O
a	O
generative	B-Method
model	E-Method
to	O
capture	O
the	O
data	O
distribution	O
by	O
comparing	O
the	O
likelihood	O
of	O
this	O
model	O
to	O
generate	O
hold	O
-	O
out	O
images	O
on	O
the	O
MNIST	S-Material
and	O
Toronto	B-Material
face	I-Material
dataset	E-Material
(	O
TFD	S-Material
)	O
using	O
the	O
evaluation	O
procedure	O
described	O
in	O
gan	S-Method
.	O

We	O
trained	O
an	O
adversarial	B-Method
autoencoder	E-Method
on	O
MNIST	S-Material
and	O
TFD	S-Material
in	O
which	O
the	O
model	O
imposed	O
a	O
high	O
-	O
dimensional	O
Gaussian	O
distribution	O
on	O
the	O
underlying	O
hidden	O
code	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
samples	O
drawn	O
from	O
the	O
adversarial	B-Method
autoencoder	E-Method
trained	O
on	O
these	O
datasets	O
.	O

A	O
video	O
showing	O
the	O
learnt	O
TFD	S-Material
manifold	O
can	O
be	O
found	O
at	O
.	O

To	O
determine	O
whether	O
the	O
model	O
is	O
over	O
-	O
fitting	O
by	O
copying	O
the	O
training	O
data	O
points	O
,	O
we	O
used	O
the	O
last	O
column	O
of	O
these	O
figures	O
to	O
show	O
the	O
nearest	O
neighbors	O
,	O
in	O
Euclidean	O
distance	O
,	O
to	O
the	O
generative	B-Method
model	E-Method
samples	O
in	O
the	O
second	O
-	O
to	O
-	O
last	O
column	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
the	O
adversarial	B-Method
autoencoder	E-Method
by	O
computing	O
its	O
log	B-Method
-	I-Method
likelihood	E-Method
on	O
the	O
hold	O
out	O
test	O
set	O
.	O

Evaluation	O
of	O
the	O
model	O
using	O
likelihood	O
is	O
not	O
straightforward	O
because	O
we	O
can	O
not	O
directly	O
compute	O
the	O
probability	O
of	O
an	O
image	O
.	O

Thus	O
,	O
we	O
calculate	O
a	O
lower	B-Metric
bound	E-Metric
of	O
the	O
true	B-Metric
log	I-Metric
-	I-Metric
likelihood	E-Metric
using	O
the	O
methods	O
described	O
in	O
prior	O
work	O
stacked_cae	O
,	O
gsn	S-Method
,	O
gan	S-Method
.	O

We	O
fit	O
a	O
Gaussian	B-Method
Parzen	I-Method
window	E-Method
(	O
kernel	B-Method
density	I-Method
estimator	E-Method
)	O
to	O
samples	O
generated	O
from	O
the	O
model	O
and	O
compute	O
the	O
likelihood	O
of	O
the	O
test	O
data	O
under	O
this	O
distribution	O
.	O

The	O
free	O
-	O
parameter	O
of	O
the	O
Parzen	B-Method
window	E-Method
is	O
selected	O
via	O
cross	B-Method
-	I-Method
validation	E-Method
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
log	B-Metric
-	I-Metric
likelihood	E-Metric
of	O
the	O
adversarial	B-Method
autoencoder	E-Method
for	O
real	B-Material
-	I-Material
valued	I-Material
MNIST	E-Material
and	O
TFD	S-Material
to	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
including	O
DBN	B-Method
geoff	E-Method
,	O
Stacked	B-Method
CAE	I-Method
stacked_cae	E-Method
,	O
Deep	B-Method
GSN	I-Method
gsn	E-Method
,	O
Generative	B-Method
Adversarial	I-Method
Networks	I-Method
gan	E-Method
and	O
GMMN	S-Method
+	O
AE	O
gmmn	S-Method
.	O

Note	O
that	O
the	O
Parzen	B-Method
window	I-Method
estimate	E-Method
is	O
a	O
lower	O
bound	O
on	O
the	O
true	B-Metric
log	I-Metric
-	I-Metric
likelihood	E-Metric
and	O
the	O
tightness	O
of	O
this	O
bound	O
depends	O
on	O
the	O
number	O
of	O
samples	O
drawn	O
.	O

To	O
obtain	O
a	O
comparison	O
with	O
a	O
tighter	O
lower	O
bound	O
,	O
we	O
additionally	O
report	O
Parzen	B-Method
window	I-Method
estimates	E-Method
evaluated	O
with	O
10	O
million	O
samples	O
for	O
both	O
the	O
adversarial	B-Method
autoencoders	E-Method
and	O
the	O
generative	B-Method
adversarial	I-Method
network	I-Method
gan	E-Method
.	O

In	O
all	O
comparisons	O
we	O
find	O
that	O
the	O
adversarial	B-Method
autoencoder	E-Method
achieves	O
superior	O
log	B-Metric
-	I-Metric
likelihoods	E-Metric
to	O
competing	O
methods	O
.	O

However	O
,	O
the	O
reader	O
must	O
be	O
aware	O
that	O
the	O
metrics	O
currently	O
available	O
for	O
evaluating	O
the	O
likelihood	O
of	O
generative	B-Method
models	E-Method
such	O
as	O
GANs	S-Method
are	O
deeply	O
flawed	O
.	O

Theis	O
et	O
al	O
.	O

theis	O
detail	O
the	O
problems	O
with	O
such	O
metrics	O
,	O
including	O
the	O
10	B-Method
K	I-Method
and	I-Method
10	I-Method
M	I-Method
sample	I-Method
Parzen	I-Method
window	I-Method
estimate	E-Method
.	O

section	O
:	O
Supervised	O
Adversarial	B-Method
Autoencoders	E-Method
Semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
is	O
a	O
long	O
-	O
standing	O
conceptual	B-Task
problem	E-Task
in	O
machine	B-Task
learning	E-Task
.	O

Recently	O
,	O
generative	B-Method
models	E-Method
have	O
become	O
one	O
of	O
the	O
most	O
popular	O
approaches	O
for	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
as	O
they	O
can	O
disentangle	O
the	O
class	O
label	O
information	O
from	O
many	O
other	O
latent	O
factors	O
of	O
variation	O
in	O
a	O
principled	O
way	O
semi	O
-	O
vae	S-Method
,	O
adgm	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
focus	O
on	O
the	O
fully	B-Task
supervised	I-Task
scenarios	E-Task
and	O
discuss	O
an	O
architecture	O
of	O
adversarial	B-Method
autoencoders	E-Method
that	O
can	O
separate	O
the	O
class	O
label	O
information	O
from	O
the	O
image	O
style	O
information	O
.	O

We	O
then	O
extend	O
this	O
architecture	O
to	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
settings	E-Task
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
order	O
to	O
incorporate	O
the	O
label	O
information	O
,	O
we	O
alter	O
the	O
network	B-Method
architecture	E-Method
of	O
Figure	O
[	O
reference	O
]	O
to	O
provide	O
a	O
one	B-Method
-	I-Method
hot	I-Method
vector	I-Method
encoding	E-Method
of	O
the	O
label	O
to	O
the	O
decoder	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
decoder	S-Method
utilizes	O
both	O
the	O
one	O
-	O
hot	O
vector	O
identifying	O
the	O
label	O
and	O
the	O
hidden	O
code	O
to	O
reconstruct	O
the	O
image	O
.	O

This	O
architecture	O
forces	O
the	O
network	O
to	O
retain	O
all	O
information	O
independent	O
of	O
the	O
label	O
in	O
the	O
hidden	O
code	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
demonstrates	O
the	O
results	O
of	O
such	O
a	O
network	O
trained	O
on	O
MNIST	S-Material
digits	O
in	O
which	O
the	O
hidden	O
code	O
is	O
forced	O
into	O
a	O
15	B-Method
-	I-Method
D	I-Method
Gaussian	E-Method
.	O

Each	O
row	O
of	O
Figure	O
[	O
reference	O
]	O
a	O
presents	O
reconstructed	O
images	O
in	O
which	O
the	O
hidden	O
code	O
is	O
fixed	O
to	O
a	O
particular	O
value	O
but	O
the	O
label	O
is	O
systematically	O
explored	O
.	O

Note	O
that	O
the	O
style	O
of	O
the	O
reconstructed	O
images	O
is	O
consistent	O
across	O
a	O
given	O
row	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
demonstrates	O
the	O
same	O
experiment	O
applied	O
to	O
Street	B-Material
View	I-Material
House	I-Material
Numbers	I-Material
dataset	I-Material
svhn	E-Material
.	O

A	O
video	O
showing	O
the	O
learnt	O
SVHN	S-Material
style	O
manifold	O
can	O
be	O
found	O
at	O
.	O

In	O
this	O
experiment	O
,	O
the	O
one	O
-	O
hot	O
vector	O
represents	O
the	O
label	O
associated	O
with	O
the	O
central	O
digit	O
in	O
the	O
image	O
.	O

Note	O
that	O
the	O
style	O
information	O
in	O
each	O
row	O
contains	O
information	O
about	O
the	O
labels	O
of	O
the	O
left	O
-	O
most	O
and	O
right	O
-	O
most	O
digits	O
because	O
the	O
left	O
-	O
most	O
and	O
right	O
-	O
most	O
digits	O
are	O
not	O
provided	O
as	O
label	O
information	O
in	O
the	O
one	B-Method
-	I-Method
hot	I-Method
encoding	E-Method
.	O

section	O
:	O
Semi	O
-	O
Supervised	O
Adversarial	B-Method
Autoencoders	E-Method
Building	O
on	O
the	O
foundations	O
from	O
Section	O
[	O
reference	O
]	O
,	O
we	O
now	O
use	O
the	O
adversarial	B-Method
autoencoder	E-Method
to	O
develop	O
models	O
for	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
that	O
exploit	O
the	O
generative	B-Method
description	E-Method
of	O
the	O
unlabeled	O
data	O
to	O
improve	O
the	O
classification	S-Task
performance	O
that	O
would	O
be	O
obtained	O
by	O
using	O
only	O
the	O
labeled	O
data	O
.	O

Specifically	O
,	O
we	O
assume	O
the	O
data	O
is	O
generated	O
by	O
a	O
latent	O
class	O
variable	O
that	O
comes	O
from	O
a	O
Categorical	O
distribution	O
as	O
well	O
as	O
a	O
continuous	O
latent	O
variable	O
that	O
comes	O
from	O
a	O
Gaussian	B-Method
distribution	E-Method
:	O
We	O
alter	O
the	O
network	B-Method
architecture	E-Method
of	O
Figure	O
[	O
reference	O
]	O
so	O
that	O
the	O
inference	B-Method
network	E-Method
of	O
the	O
AAE	S-Method
predicts	O
both	O
the	O
discrete	O
class	O
variable	O
and	O
the	O
continuous	O
latent	O
variable	O
using	O
the	O
encoder	S-Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
decoder	O
then	O
utilizes	O
both	O
the	O
class	O
label	O
as	O
a	O
one	O
-	O
hot	O
vector	O
and	O
the	O
continuous	B-Method
hidden	I-Method
code	E-Method
to	O
reconstruct	O
the	O
image	O
.	O

There	O
are	O
two	O
separate	O
adversarial	B-Method
networks	E-Method
that	O
regularize	O
the	O
hidden	B-Method
representation	E-Method
of	O
the	O
autoencoder	S-Method
.	O

The	O
first	O
adversarial	B-Method
network	E-Method
imposes	O
a	O
Categorical	O
distribution	O
on	O
the	O
label	B-Method
representation	E-Method
.	O

This	O
adversarial	B-Method
network	E-Method
ensures	O
that	O
the	O
latent	O
class	O
variable	O
does	O
not	O
carry	O
any	O
style	O
information	O
and	O
that	O
the	O
aggregated	O
posterior	O
distribution	O
of	O
matches	O
the	O
Categorical	O
distribution	O
.	O

The	O
second	O
adversarial	B-Method
network	E-Method
imposes	O
a	O
Gaussian	B-Method
distribution	E-Method
on	O
the	O
style	B-Method
representation	E-Method
which	O
ensures	O
the	O
latent	O
variable	O
is	O
a	O
continuous	O
Gaussian	O
variable	O
.	O

Both	O
of	O
the	O
adversarial	B-Method
networks	E-Method
as	O
well	O
as	O
the	O
autoencoder	S-Method
are	O
trained	O
jointly	O
with	O
SGD	S-Method
in	O
three	O
phases	O
–	O
the	O
reconstruction	B-Method
phase	E-Method
,	O
regularization	B-Method
phase	E-Method
and	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
phase	O
.	O

In	O
the	O
reconstruction	B-Task
phase	E-Task
,	O
the	O
autoencoder	S-Method
updates	O
the	O
encoder	S-Method
and	O
the	O
decoder	S-Method
to	O
minimize	O
the	O
reconstruction	B-Metric
error	E-Metric
of	O
the	O
inputs	O
on	O
an	O
unlabeled	O
mini	O
-	O
batch	O
.	O

In	O
the	O
regularization	B-Method
phase	E-Method
,	O
each	O
of	O
the	O
adversarial	B-Method
networks	E-Method
first	O
updates	O
their	O
discriminative	B-Method
network	E-Method
to	O
tell	O
apart	O
the	O
true	O
samples	O
(	O
generated	O
using	O
the	O
Categorical	O
and	O
Gaussian	O
priors	O
)	O
from	O
the	O
generated	O
samples	O
(	O
the	O
hidden	O
codes	O
computed	O
by	O
the	O
autoencoder	S-Method
)	O
.	O

The	O
adversarial	B-Method
networks	E-Method
then	O
update	O
their	O
generator	S-Method
to	O
confuse	O
their	O
discriminative	B-Method
networks	E-Method
.	O

In	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
phase	O
,	O
the	O
autoencoder	B-Method
updates	E-Method
to	O
minimize	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
cost	E-Metric
on	O
a	O
labeled	O
mini	O
-	O
batch	O
.	O

The	O
results	O
of	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
experiments	O
on	O
MNIST	S-Material
and	O
SVHN	S-Material
datasets	O
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

On	O
the	O
MNIST	S-Material
dataset	O
with	O
100	O
and	O
1000	O
labels	O
,	O
the	O
performance	O
of	O
AAEs	S-Method
is	O
significantly	O
better	O
than	O
VAEs	S-Method
,	O
on	O
par	O
with	O
VAT	B-Method
vat	E-Method
and	O
CatGAN	B-Method
catgan	E-Method
,	O
but	O
is	O
outperformed	O
by	O
the	O
Ladder	B-Method
networks	I-Method
ladder	E-Method
and	O
the	O
ADGM	B-Method
adgm	E-Method
.	O

We	O
also	O
trained	O
a	O
supervised	B-Method
AAE	I-Method
model	E-Method
on	O
all	O
the	O
available	O
labels	O
,	O
and	O
obtained	O
the	O
error	B-Metric
rate	E-Metric
of	O
.	O

In	O
comparison	O
,	O
a	O
dropout	B-Method
supervised	I-Method
neural	I-Method
network	E-Method
with	O
the	O
same	O
architecture	O
achieves	O
the	O
error	B-Metric
rate	E-Metric
of	O
on	O
the	O
full	O
MNIST	S-Material
dataset	O
,	O
which	O
highlights	O
the	O
regularization	O
effect	O
of	O
the	O
adversarial	B-Method
training	E-Method
.	O

On	O
the	O
SVHN	S-Material
dataset	O
with	O
1000	O
labels	O
,	O
the	O
AAE	S-Method
almost	O
matches	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
classification	B-Metric
performance	E-Metric
achieved	O
by	O
the	O
ADGM	S-Method
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
all	O
the	O
AAE	B-Method
models	E-Method
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
whereas	O
the	O
semi	O
-	O
supervised	O
VAE	S-Method
models	O
have	O
to	O
be	O
trained	O
one	O
layer	O
at	O
a	O
time	O
semi	O
-	O
vae	S-Method
.	O

section	O
:	O
Unsupervised	B-Task
Clustering	E-Task
with	O
Adversarial	B-Method
Autoencoders	E-Method
In	O
the	O
previous	O
section	O
,	O
we	O
showed	O
that	O
with	O
a	O
limited	O
label	O
information	O
,	O
the	O
adversarial	B-Method
autoencoder	E-Method
is	O
able	O
to	O
learn	O
powerful	O
semi	B-Method
-	I-Method
supervised	I-Method
representations	E-Method
.	O

However	O
,	O
the	O
question	O
that	O
has	O
remained	O
unanswered	O
is	O
whether	O
it	O
is	O
possible	O
to	O
learn	O
as	O
‘	O
‘	O
powerful	O
’	O
’	O
representations	O
from	O
unlabeled	O
data	O
without	O
any	O
supervision	O
.	O

In	O
this	O
section	O
,	O
we	O
show	O
that	O
the	O
adversarial	B-Method
autoencoder	E-Method
can	O
disentangle	O
discrete	O
class	O
variables	O
from	O
the	O
continuous	O
latent	O
style	O
variables	O
in	O
a	O
purely	O
unsupervised	B-Method
fashion	E-Method
.	O

The	O
architecture	O
that	O
we	O
use	O
is	O
similar	O
to	O
Figure	O
[	O
reference	O
]	O
,	O
with	O
the	O
difference	O
that	O
we	O
remove	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
stage	O
and	O
thus	O
no	O
longer	O
train	O
the	O
network	O
on	O
any	O
labeled	O
mini	O
-	O
batch	O
.	O

Another	O
difference	O
is	O
that	O
the	O
inference	B-Method
network	E-Method
predicts	O
a	O
one	O
-	O
hot	O
vector	O
whose	O
dimension	O
is	O
the	O
number	O
of	O
categories	O
that	O
we	O
wish	O
the	O
data	O
to	O
be	O
clustered	O
into	O
.	O

Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
unsupervised	B-Task
clustering	E-Task
performance	O
of	O
the	O
AAE	S-Method
on	O
MNIST	S-Material
when	O
the	O
number	O
of	O
clusters	O
is	O
16	O
.	O

Each	O
row	O
corresponds	O
to	O
one	O
cluster	O
.	O

The	O
first	O
image	O
in	O
each	O
row	O
shows	O
the	O
cluster	O
heads	O
,	O
which	O
are	O
digits	O
generated	O
by	O
fixing	O
the	O
style	O
variable	O
to	O
zero	O
and	O
setting	O
the	O
label	O
variable	O
to	O
one	O
of	O
the	O
16	O
one	O
-	O
hot	O
vectors	O
.	O

The	O
rest	O
of	O
the	O
images	O
in	O
each	O
row	O
are	O
random	O
test	O
images	O
that	O
have	O
been	O
categorized	O
into	O
the	O
corresponding	O
category	O
based	O
on	O
.	O

We	O
can	O
see	O
that	O
the	O
AAE	S-Method
has	O
picked	O
up	O
some	O
discrete	O
styles	O
as	O
the	O
class	O
labels	O
.	O

For	O
example	O
,	O
the	O
digit	O
s	O
and	O
s	O
that	O
are	O
tilted	O
(	O
cluster	O
16	O
and	O
11	O
)	O
are	O
put	O
in	O
a	O
separate	O
cluster	O
than	O
the	O
straight	O
s	O
and	O
s	O
(	O
cluster	O
15	O
and	O
10	O
)	O
,	O
or	O
the	O
network	O
has	O
separated	O
digit	O
s	O
into	O
two	O
clusters	O
(	O
cluster	O
4	O
,	O
6	O
)	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
written	O
with	O
a	O
loop	O
.	O

We	O
performed	O
an	O
experiment	O
to	O
evaluate	O
the	O
unsupervised	B-Task
clustering	E-Task
performance	O
of	O
AAEs	S-Method
.	O

We	O
used	O
the	O
following	O
evaluation	O
protocol	O
:	O
Once	O
the	O
training	O
is	O
done	O
,	O
for	O
each	O
cluster	O
,	O
we	O
found	O
the	O
validation	O
example	O
that	O
maximizes	O
,	O
and	O
assigned	O
the	O
label	O
of	O
to	O
all	O
the	O
points	O
in	O
the	O
cluster	O
.	O

We	O
then	O
computed	O
the	O
test	B-Metric
error	E-Metric
based	O
on	O
the	O
assigned	O
class	O
labels	O
to	O
each	O
cluster	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
AAE	S-Method
achieves	O
the	O
classification	B-Metric
error	I-Metric
rate	E-Metric
of	O
9.55	O
%	O
and	O
4.10	O
%	O
with	O
16	O
and	O
30	O
total	O
labels	O
respectively	O
.	O

We	O
observed	O
that	O
as	O
the	O
number	O
of	O
clusters	O
grows	O
,	O
the	O
classification	B-Metric
rate	E-Metric
improves	O
.	O

section	O
:	O
Dimensionality	B-Task
Reduction	E-Task
with	O
Adversarial	B-Method
Autoencoders	E-Method
Visualization	B-Task
of	I-Task
high	I-Task
dimensional	I-Task
data	E-Task
is	O
a	O
very	O
important	O
problem	O
in	O
many	O
applications	O
as	O
it	O
facilitates	O
the	O
understanding	O
of	O
the	O
generative	B-Task
process	E-Task
of	O
the	O
data	O
and	O
allows	O
us	O
to	O
extract	O
useful	O
information	O
about	O
the	O
data	O
.	O

A	O
popular	O
approach	O
of	O
data	B-Task
visualization	E-Task
is	O
learning	O
a	O
low	B-Method
dimensional	I-Method
embedding	E-Method
in	O
which	O
nearby	O
points	O
correspond	O
to	O
similar	O
objects	O
.	O

Over	O
the	O
last	O
decade	O
,	O
a	O
large	O
number	O
of	O
new	O
non	B-Method
-	I-Method
parametric	I-Method
dimensionality	I-Method
reduction	I-Method
techniques	E-Method
such	O
as	O
t	B-Method
-	I-Method
SNE	I-Method
tsne	E-Method
have	O
been	O
proposed	O
.	O

The	O
main	O
drawback	O
of	O
these	O
methods	O
is	O
that	O
they	O
do	O
not	O
have	O
a	O
parametric	B-Method
encoder	E-Method
that	O
can	O
be	O
used	O
to	O
find	O
the	O
embedding	O
of	O
the	O
new	O
data	O
points	O
.	O

Different	O
methods	O
such	O
as	O
parametric	B-Method
t	I-Method
-	I-Method
SNE	I-Method
param_tsne	E-Method
have	O
been	O
proposed	O
to	O
address	O
this	O
issue	O
.	O

Autoencoders	S-Method
are	O
interesting	O
alternatives	O
as	O
they	O
provide	O
the	O
non	O
-	O
linear	O
mapping	O
required	O
for	O
such	O
embeddings	O
;	O
but	O
it	O
is	O
widely	O
known	O
that	O
non	B-Method
-	I-Method
regularized	I-Method
autoencoders	E-Method
‘	O
‘	O
fracture	O
’	O
’	O
the	O
manifold	O
into	O
many	O
different	O
domains	O
which	O
result	O
in	O
very	O
different	O
codes	O
for	O
similar	O
images	O
geoff_dim_reduce	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
an	O
adversarial	B-Method
autoencoder	E-Method
architecture	O
for	O
dimensionality	B-Task
reduction	E-Task
and	O
data	B-Task
visualization	I-Task
purposes	E-Task
.	O

We	O
will	O
show	O
that	O
in	O
these	O
autoencoders	S-Method
,	O
the	O
adversarial	B-Method
regularization	E-Method
attaches	O
the	O
hidden	O
code	O
of	O
similar	O
images	O
to	O
each	O
other	O
and	O
thus	O
prevents	O
the	O
manifold	B-Task
fracturing	I-Task
problem	E-Task
that	O
is	O
typically	O
encountered	O
in	O
the	O
embeddings	O
learnt	O
by	O
the	O
autoencoders	S-Method
.	O

Suppose	O
we	O
have	O
a	O
dataset	O
with	O
class	O
labels	O
and	O
we	O
would	O
like	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
dataset	O
to	O
,	O
where	O
is	O
typically	O
2	O
or	O
3	O
for	O
the	O
visualization	B-Task
purposes	E-Task
.	O

We	O
alter	O
the	O
architecture	O
of	O
Figure	O
[	O
reference	O
]	O
to	O
Figure	O
[	O
reference	O
]	O
in	O
which	O
the	O
final	O
representation	O
is	O
achieved	O
by	O
adding	O
the	O
dimensional	B-Method
distributed	I-Method
representation	E-Method
of	O
the	O
cluster	B-Method
head	E-Method
with	O
the	O
dimensional	B-Method
style	I-Method
representation	E-Method
.	O

The	O
cluster	B-Method
head	I-Method
representation	E-Method
is	O
obtained	O
by	O
multiplying	O
the	O
dimensional	O
one	O
-	O
hot	O
class	O
label	O
vector	O
by	O
an	O
matrix	O
,	O
where	O
the	O
rows	O
of	O
represent	O
the	O
cluster	B-Method
head	I-Method
representations	E-Method
that	O
are	O
learned	O
with	O
SGD	S-Method
.	O

We	O
introduce	O
an	O
additional	O
cost	O
function	O
that	O
penalizes	O
the	O
Euclidean	O
distance	O
between	O
every	O
two	O
cluster	O
heads	O
.	O

Specifically	O
,	O
if	O
the	O
Euclidean	O
distance	O
is	O
larger	O
than	O
a	O
threshold	O
,	O
the	O
cost	O
function	O
is	O
zero	O
,	O
and	O
if	O
it	O
is	O
smaller	O
than	O
,	O
the	O
cost	B-Method
function	E-Method
linearly	O
penalizes	O
the	O
distance	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
a	O
,	O
b	O
)	O
show	O
the	O
results	O
of	O
the	O
semi	B-Method
-	I-Method
supervised	I-Method
dimensionality	I-Method
reduction	I-Method
in	I-Method
dimensions	E-Method
on	O
the	O
MNIST	S-Material
dataset	O
(	O
)	O
with	O
1000	O
and	O
100	O
labels	O
.	O

We	O
can	O
see	O
that	O
the	O
network	O
can	O
achieve	O
a	O
clean	O
separation	O
of	O
the	O
digit	O
clusters	O
and	O
obtain	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
error	O
of	O
4.20	O
%	O
and	O
6.08	O
%	O
respectively	O
.	O

Note	O
that	O
because	O
of	O
the	O
2D	O
constraint	O
,	O
the	O
classification	B-Metric
error	E-Metric
is	O
not	O
as	O
good	O
as	O
the	O
high	O
-	O
dimensional	O
cases	O
;	O
and	O
that	O
the	O
style	O
distribution	O
of	O
each	O
cluster	O
is	O
not	O
quite	O
Gaussian	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
shows	O
the	O
result	O
of	O
unsupervised	B-Task
dimensionality	I-Task
reduction	E-Task
in	O
dimensions	O
where	O
the	O
number	O
of	O
clusters	O
have	O
chosen	O
to	O
be	O
.	O

We	O
can	O
see	O
that	O
the	O
network	O
can	O
achieve	O
a	O
rather	O
clean	O
separation	O
of	O
the	O
digit	O
clusters	O
and	O
sub	O
-	O
clusters	O
.	O

For	O
example	O
,	O
the	O
network	O
has	O
assigned	O
two	O
different	O
clusters	O
to	O
digit	O
1	O
(	O
green	O
clusters	O
)	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
straight	O
or	O
tilted	O
.	O

The	O
network	O
is	O
also	O
clustering	O
digit	O
6	O
into	O
three	O
clusters	O
(	O
black	O
clusters	O
)	O
depending	O
on	O
how	O
much	O
tilted	O
the	O
digit	O
is	O
.	O

Also	O
the	O
network	O
has	O
assigned	O
two	O
separate	O
clusters	O
for	O
digit	O
2	O
(	O
red	O
clusters	O
)	O
,	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
written	O
with	O
a	O
loop	O
.	O

This	O
AAE	S-Method
architecture	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
can	O
also	O
be	O
used	O
to	O
embed	O
images	O
into	O
larger	O
dimensionalities	O
(	O
)	O
.	O

For	O
example	O
,	O
Figure	O
[	O
reference	O
]	O
d	O
shows	O
the	O
result	O
of	O
semi	B-Task
-	I-Task
supervised	I-Task
dimensionality	I-Task
reduction	E-Task
in	O
dimensions	O
with	O
100	O
labels	O
.	O

In	O
this	O
case	O
,	O
we	O
fixed	O
matrix	O
to	O
and	O
thus	O
the	O
cluster	O
heads	O
are	O
the	O
corners	O
of	O
a	O
dimensional	O
simplex	O
.	O

The	O
style	B-Method
representation	E-Method
is	O
learnt	O
to	O
be	O
a	O
10D	B-Method
Gaussian	I-Method
distribution	E-Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
and	O
is	O
directly	O
added	O
to	O
the	O
cluster	O
head	O
to	O
construct	O
the	O
final	O
representation	O
.	O

Once	O
the	O
network	O
is	O
trained	O
,	O
in	O
order	O
to	O
visualize	O
the	O
10D	B-Method
learnt	I-Method
representation	E-Method
,	O
we	O
use	O
a	O
linear	B-Method
transformation	E-Method
to	O
map	O
the	O
10D	B-Method
representation	E-Method
to	O
a	O
2D	O
space	O
such	O
that	O
the	O
cluster	O
heads	O
are	O
mapped	O
to	O
the	O
points	O
that	O
are	O
uniformly	O
placed	O
on	O
a	O
2D	O
circle	O
.	O

We	O
can	O
verify	O
from	O
this	O
figure	O
that	O
in	O
this	O
high	O
-	O
dimensional	O
case	O
,	O
the	O
style	B-Method
representation	E-Method
has	O
indeed	O
learnt	O
to	O
have	O
a	O
Gaussian	B-Method
distribution	E-Method
.	O

With	O
100	O
total	O
labels	O
,	O
this	O
model	O
achieves	O
the	O
classification	B-Metric
error	I-Metric
-	I-Metric
rate	E-Metric
of	O
3.90	O
%	O
which	O
is	O
worse	O
than	O
the	O
classification	B-Metric
error	I-Metric
-	I-Metric
rate	E-Metric
of	O
1.90	O
%	O
that	O
is	O
achieved	O
by	O
the	O
AAE	S-Method
architecture	O
with	O
the	O
concatenated	B-Method
style	I-Method
and	I-Method
label	I-Method
representation	E-Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
proposed	O
to	O
use	O
the	O
GAN	S-Method
framework	O
as	O
a	O
variational	B-Method
inference	I-Method
algorithm	E-Method
for	O
both	O
discrete	O
and	O
continuous	O
latent	O
variables	O
in	O
probabilistic	B-Method
autoencoders	E-Method
.	O

Our	O
method	O
called	O
the	O
adversarial	B-Method
autoencoder	E-Method
(	O
AAE	S-Method
)	O
,	O
is	O
a	O
generative	B-Method
autoencoder	E-Method
that	O
achieves	O
competitive	B-Metric
test	I-Metric
likelihoods	E-Metric
on	O
real	B-Material
-	I-Material
valued	I-Material
MNIST	E-Material
and	O
Toronto	B-Material
Face	I-Material
datasets	E-Material
.	O

We	O
discussed	O
how	O
this	O
method	O
can	O
be	O
extended	O
to	O
semi	B-Task
-	I-Task
supervised	I-Task
scenarios	E-Task
and	O
showed	O
that	O
it	O
achieves	O
competitive	O
semi	B-Task
-	I-Task
supervised	I-Task
classification	E-Task
performance	O
on	O
MNIST	S-Material
and	O
SVHN	S-Material
datasets	O
.	O

Finally	O
,	O
we	O
demonstrated	O
the	O
applications	O
of	O
adversarial	B-Method
autoencoders	E-Method
in	O
disentangling	O
the	O
style	O
and	O
content	O
of	O
images	O
,	O
unsupervised	B-Task
clustering	E-Task
,	O
dimensionality	B-Task
reduction	E-Task
and	O
data	B-Task
visualization	E-Task
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
Ilya	O
Sutskever	O
,	O
Oriol	O
Vinyals	O
,	O
Jon	O
Gauthier	O
,	O
Sam	O
Bowman	O
and	O
other	O
members	O
of	O
the	O
Google	O
Brain	O
team	O
for	O
helpful	O
discussions	O
.	O

We	O
thank	O
the	O
developers	O
of	O
TensorFlow	S-Method
tensorflow2015	O
-	O
whitepaper	O
,	O
which	O
we	O
used	O
for	O
all	O
of	O
our	O
experiments	O
.	O

We	O
also	O
thank	O
NVIDIA	S-Material
for	O
GPU	O
donations	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Experiment	O
Details	O
subsection	O
:	O
Likelihood	O
Experiments	O
The	O
encoder	S-Method
,	O
decoder	S-Method
and	O
discriminator	S-Method
each	O
have	O
two	O
layers	O
of	O
1000	O
hidden	O
units	O
with	O
ReLU	O
activation	O
function	O
.	O

The	O
activation	O
of	O
the	O
last	O
layer	O
of	O
is	O
linear	O
.	O

The	O
weights	O
are	O
initialized	O
with	O
a	O
Gaussian	B-Method
distribution	E-Method
with	O
the	O
standard	O
deviation	O
of	O
0.01	O
.	O

The	O
mini	O
-	O
batch	O
size	O
is	O
100	O
.	O

The	O
autoencoder	S-Method
is	O
trained	O
with	O
a	O
Euclidean	B-Method
cost	I-Method
function	E-Method
for	O
reconstruction	S-Task
.	O

On	O
the	O
MNIST	S-Material
dataset	O
we	O
use	O
the	O
sigmoid	O
activation	O
function	O
in	O
the	O
last	O
layer	O
of	O
the	O
autoencoder	S-Method
and	O
on	O
the	O
TFD	S-Material
dataset	O
we	O
use	O
the	O
linear	B-Method
activation	I-Method
function	E-Method
.	O

The	O
dimensionality	O
of	O
the	O
hidden	O
code	O
is	O
and	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
Gaussian	O
prior	O
is	O
and	O
for	O
MNIST	S-Material
and	O
TFD	S-Material
,	O
respectively	O
.	O

On	O
the	O
Toronto	B-Material
Face	I-Material
dataset	E-Material
,	O
data	O
points	O
are	O
subtracted	O
by	O
the	O
mean	O
and	O
divided	O
by	O
the	O
standard	O
deviation	O
along	O
each	O
input	O
dimension	O
across	O
the	O
whole	O
training	O
set	O
to	O
normalize	O
the	O
contrast	O
.	O

However	O
,	O
after	O
obtaining	O
the	O
samples	O
,	O
we	O
rescaled	O
the	O
images	O
(	O
by	O
inverting	O
the	O
pre	B-Method
-	I-Method
processing	I-Method
stage	E-Method
)	O
to	O
have	O
pixel	O
intensities	O
between	O
0	O
and	O
1	O
so	O
that	O
we	O
can	O
have	O
a	O
fair	O
likelihood	O
comparison	O
with	O
other	O
methods	O
.	O

In	O
the	O
deterministic	O
case	O
of	O
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
code	O
should	O
be	O
consistent	O
with	O
the	O
intrinsic	O
dimensionality	O
of	O
the	O
data	O
,	O
since	O
the	O
only	O
source	O
of	O
stochasticity	O
in	O
is	O
the	O
data	O
distribution	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
MNIST	S-Material
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
code	O
can	O
be	O
between	O
5	O
to	O
8	O
,	O
and	O
for	O
TFD	S-Material
and	O
SVHN	S-Material
,	O
it	O
can	O
be	O
between	O
10	O
to	O
20	O
.	O

For	O
training	O
AAEs	S-Method
with	O
higher	O
dimensionalities	O
in	O
the	O
code	O
space	O
(	O
e.g.	O
,	O
1000	O
)	O
,	O
the	O
probabilistic	S-Method
along	O
with	O
the	O
re	B-Method
-	I-Method
parametrization	I-Method
trick	E-Method
can	O
be	O
used	O
.	O

subsection	O
:	O
Semi	B-Task
-	I-Task
Supervised	I-Task
Experiments	E-Task
subsubsection	O
:	O
MNIST	S-Material
The	O
encoder	S-Method
,	O
decoder	S-Method
and	O
discriminator	S-Method
each	O
have	O
two	O
layers	O
of	O
1000	O
hidden	O
units	O
with	O
ReLU	O
activation	O
function	O
.	O

The	O
last	O
layer	O
of	O
the	O
autoencoder	S-Method
can	O
have	O
a	O
linear	O
or	O
sigmoid	O
activation	O
(	O
sigmoid	O
is	O
better	O
for	O
sample	B-Task
visualization	E-Task
)	O
.	O

The	O
cost	B-Method
function	E-Method
is	O
half	O
the	O
Euclidean	B-Metric
error	E-Metric
.	O

The	O
last	O
layer	O
of	O
and	O
has	O
the	O
softmax	O
and	O
linear	O
activation	O
function	O
,	O
respectively	O
.	O

The	O
and	O
share	O
the	O
first	O
two	O
1000	O
-	O
unit	O
layers	O
of	O
the	O
encoder	S-Method
.	O

The	O
dimensionality	S-Metric
of	O
both	O
the	O
style	B-Method
and	I-Method
label	I-Method
representation	E-Method
is	O
10	O
.	O

On	O
the	O
style	B-Method
representation	E-Method
,	O
we	O
impose	O
a	O
Gaussian	B-Method
distribution	E-Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
.	O

On	O
the	O
label	B-Task
representation	E-Task
,	O
we	O
impose	O
a	O
Categorical	O
distribution	O
.	O

The	O
semi	B-Task
-	I-Task
supervised	I-Task
cost	E-Task
is	O
a	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
cost	I-Metric
function	E-Metric
at	O
the	O
output	O
of	O
.	O

We	O
use	O
gradient	B-Method
descent	E-Method
with	O
momentum	S-Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	B-Metric
reconstruction	I-Metric
cost	E-Metric
and	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
cost	E-Task
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	B-Method
and	I-Method
discriminator	E-Method
of	O
both	O
of	O
the	O
adversarial	B-Method
networks	E-Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	B-Metric
cost	E-Metric
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.01	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.001	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.0001	O
.	O

For	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
cost	E-Task
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.1	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

For	O
both	O
the	O
discriminative	B-Metric
and	I-Metric
generative	I-Metric
costs	E-Metric
of	O
the	O
adversarial	B-Method
networks	E-Method
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.1	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

We	O
train	O
the	O
network	O
for	O
5000	O
epochs	O
.	O

We	O
add	O
a	O
Gaussian	O
noise	O
with	O
standard	O
deviation	O
of	O
0.3	O
only	O
to	O
the	O
input	O
layer	O
and	O
only	O
at	O
the	O
training	O
time	O
.	O

No	O
dropout	S-Method
,	O
weight	B-Method
decay	E-Method
or	O
other	O
Gaussian	B-Method
noise	I-Method
regularization	E-Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

The	O
labeled	O
examples	O
were	O
chosen	O
at	O
random	O
,	O
but	O
we	O
made	O
sure	O
they	O
are	O
distributed	O
evenly	O
across	O
the	O
classes	O
.	O

In	O
the	O
case	O
of	O
MNIST	S-Material
with	O
100	O
labels	O
,	O
the	O
test	B-Metric
error	E-Metric
after	O
the	O
first	O
epochs	O
is	O
16.50	O
%	O
,	O
after	O
50	O
epochs	O
is	O
3.40	O
%	O
,	O
after	O
500	O
epochs	O
is	O
2.21	O
%	O
and	O
after	O
5000	O
epochs	O
is	O
1.80	O
%	O
.	O

Batch	B-Method
-	I-Method
normalization	I-Method
batch	E-Method
did	O
not	O
help	O
in	O
the	O
case	O
of	O
the	O
MNIST	S-Material
dataset	O
.	O

subsubsection	O
:	O
SVHN	S-Material
The	O
SVHN	S-Material
dataset	O
has	O
about	O
530	O
K	O
training	O
points	O
and	O
26	O
K	O
test	O
points	O
.	O

Data	O
points	O
are	O
subtracted	O
by	O
the	O
mean	O
and	O
divided	O
by	O
the	O
standard	O
deviation	O
along	O
each	O
input	O
dimension	O
across	O
the	O
whole	O
training	O
set	O
to	O
normalize	O
the	O
contrast	O
.	O

The	O
dimensionality	S-Metric
of	O
the	O
label	B-Method
representation	E-Method
is	O
10	O
and	O
for	O
the	O
style	B-Method
representation	E-Method
we	O
use	O
20	O
dimensions	O
.	O

We	O
use	O
gradient	B-Method
descent	E-Method
with	O
momentum	S-Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	B-Metric
reconstruction	I-Metric
cost	E-Metric
and	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
cost	E-Task
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	B-Method
and	I-Method
discriminator	E-Method
of	O
both	O
of	O
the	O
adversarial	B-Method
networks	E-Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	B-Metric
cost	E-Metric
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.0001	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.00001	O
.	O

For	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
cost	E-Task
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.1	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.01	O
.	O

For	O
both	O
the	O
discriminative	B-Metric
and	I-Metric
generative	I-Metric
costs	E-Metric
of	O
the	O
adversarial	B-Method
networks	E-Method
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.01	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

We	O
train	O
the	O
network	O
for	O
1000	O
epochs	O
.	O

We	O
use	O
dropout	S-Method
at	O
the	O
input	B-Method
layer	E-Method
with	O
the	O
dropout	B-Metric
rate	E-Metric
of	O
20	O
%	O
.	O

No	O
other	O
dropout	S-Method
,	O
weight	B-Method
decay	E-Method
or	O
Gaussian	B-Method
noise	I-Method
regularization	E-Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

The	O
labeled	O
examples	O
were	O
chosen	O
at	O
random	O
,	O
but	O
we	O
made	O
sure	O
they	O
are	O
distributed	O
evenly	O
across	O
the	O
classes	O
.	O

In	O
the	O
case	O
of	O
SVHN	S-Material
with	O
1000	O
labels	O
,	O
the	O
test	B-Metric
error	E-Metric
after	O
the	O
first	O
epochs	O
is	O
49.34	O
%	O
,	O
after	O
50	O
epochs	O
is	O
25.86	O
%	O
,	O
after	O
500	O
epochs	O
is	O
18.15	O
%	O
and	O
after	O
1000	O
epochs	O
is	O
17.66	O
%	O
.	O

Batch	B-Method
-	I-Method
normalization	E-Method
were	O
used	O
in	O
all	O
the	O
autoencoder	B-Method
layers	E-Method
including	O
the	O
softmax	O
layer	O
of	O
,	O
the	O
linear	B-Method
layer	E-Method
of	O
as	O
well	O
as	O
the	O
linear	O
output	O
layer	O
of	O
the	O
autoencoder	S-Method
.	O

We	O
found	O
batch	O
-	O
normalization	O
batch	O
to	O
be	O
crucial	O
in	O
training	O
the	O
AAE	S-Method
network	O
on	O
the	O
SVHN	S-Material
dataset	O
.	O

subsection	O
:	O
Unsupervised	B-Task
Clustering	E-Task
Experiments	O
The	O
encoder	S-Method
,	O
decoder	S-Method
and	O
discriminator	S-Method
each	O
have	O
two	O
layers	O
of	O
3000	O
hidden	O
units	O
with	O
ReLU	B-Method
activation	I-Method
function	E-Method
.	O

The	O
last	O
layer	O
of	O
the	O
autoencoder	S-Method
has	O
a	O
sigmoid	O
activation	O
function	O
.	O

The	O
cost	B-Method
function	E-Method
is	O
half	O
the	O
Euclidean	B-Metric
error	E-Metric
.	O

The	O
dimensionality	S-Metric
of	O
the	O
style	B-Method
and	I-Method
label	I-Method
representation	E-Method
is	O
5	O
and	O
30	O
(	O
number	O
of	O
clusters	O
)	O
,	O
respectively	O
.	O

On	O
the	O
style	B-Method
representation	E-Method
,	O
we	O
impose	O
a	O
Gaussian	B-Method
distribution	E-Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
.	O

On	O
the	O
label	B-Task
representation	E-Task
,	O
we	O
impose	O
a	O
Categorical	O
distribution	O
.	O

We	O
use	O
gradient	B-Method
descent	E-Method
with	O
momentum	S-Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	B-Metric
reconstruction	I-Metric
cost	E-Metric
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	B-Method
and	I-Method
discriminator	E-Method
of	O
both	O
of	O
the	O
adversarial	B-Method
networks	E-Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	B-Metric
cost	E-Metric
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.01	O
and	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

For	O
both	O
the	O
discriminative	B-Metric
and	I-Metric
generative	I-Metric
costs	E-Metric
of	O
the	O
adversarial	B-Method
networks	E-Method
,	O
we	O
use	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.1	O
and	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
.	O

We	O
train	O
the	O
network	O
for	O
1500	O
epochs	O
.	O

We	O
use	O
dropout	S-Method
at	O
the	O
input	B-Method
layer	E-Method
with	O
the	O
dropout	B-Metric
rate	E-Metric
of	O
20	O
%	O
.	O

No	O
other	O
dropout	S-Method
,	O
weight	B-Method
decay	E-Method
or	O
Gaussian	B-Method
noise	I-Method
regularization	E-Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

Batch	B-Method
-	I-Method
normalization	E-Method
was	O
used	O
only	O
in	O
the	O
encoder	O
layers	O
of	O
the	O
autoencoder	S-Method
including	O
the	O
last	O
layer	O
of	O
and	O
.	O

We	O
found	O
batch	O
-	O
normalization	O
batch	O
to	O
be	O
crucial	O
in	O
training	O
the	O
AAE	S-Method
networks	O
for	O
unsupervised	B-Task
clustering	E-Task
.	O

GENERATIVE	B-Method
MULTI	I-Method
-	I-Method
ADVERSARIAL	I-Method
NETWORKS	E-Method
section	O
:	O
ABSTRACT	O
Generative	B-Method
adversarial	I-Method
networks	E-Method
(	O
GANs	S-Method
)	O
are	O
a	O
framework	O
for	O
producing	O
a	O
generative	B-Method
model	E-Method
by	O
way	O
of	O
a	O
two	B-Method
-	I-Method
player	I-Method
minimax	I-Method
game	E-Method
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
Generative	B-Method
Multi	I-Method
-	I-Method
Adversarial	I-Method
Network	E-Method
(	O
GMAN	S-Method
)	O
,	O
a	O
framework	O
that	O
extends	O
GANs	S-Method
to	O
multiple	O
discriminators	O
.	O

In	O
previous	O
work	O
,	O
the	O
successful	O
training	S-Task
of	O
GANs	S-Method
requires	O
modifying	O
the	O
minimax	O
objective	O
to	O
accelerate	O
training	S-Task
early	O
on	O
.	O

In	O
contrast	O
,	O
GMAN	S-Method
can	O
be	O
reliably	O
trained	O
with	O
the	O
original	O
,	O
untampered	O
objective	O
.	O

We	O
explore	O
a	O
number	O
of	O
design	O
perspectives	O
with	O
the	O
discriminator	O
role	O
ranging	O
from	O
formidable	O
adversary	O
to	O
forgiving	O
teacher	O
.	O

Image	B-Task
generation	I-Task
tasks	E-Task
comparing	O
the	O
proposed	O
framework	O
to	O
standard	O
GANs	S-Method
demonstrate	O
GMAN	S-Method
produces	O
higher	O
quality	O
samples	O
in	O
a	O
fraction	O
of	O
the	O
iterations	O
when	O
measured	O
by	O
a	O
pairwise	B-Metric
GAM	I-Metric
-	I-Metric
type	I-Metric
metric	E-Metric
.	O

section	O
:	O
INTRODUCTION	O
Generative	B-Method
adversarial	I-Method
networks	E-Method
[	O
reference	O
]	O
)	O
(	O
GANs	S-Method
)	O
are	O
a	O
framework	O
for	O
producing	O
a	O
generative	B-Method
model	E-Method
by	O
way	O
of	O
a	O
two	B-Method
-	I-Method
player	I-Method
minimax	I-Method
game	E-Method
.	O

One	O
player	O
,	O
the	O
generator	S-Method
,	O
attempts	O
to	O
generate	O
realistic	O
data	O
samples	O
by	O
transforming	O
noisy	O
samples	O
,	O
z	O
,	O
drawn	O
from	O
a	O
simple	O
distribution	O
(	O
e.g.	O
,	O
z	O
∼	O
N	O
(	O
0	O
,	O
1	O
)	O
)	O
using	O
a	O
transformation	B-Method
function	E-Method
G	O
θ	O
(	O
z	O
)	O
with	O
learned	O
weights	O
,	O
θ	O
.	O

The	O
generator	O
receives	O
feedback	O
as	O
to	O
how	O
realistic	O
its	O
synthetic	O
sample	O
is	O
from	O
another	O
player	O
,	O
the	O
discriminator	S-Method
,	O
which	O
attempts	O
to	O
discern	O
between	O
synthetic	O
data	O
samples	O
produced	O
by	O
the	O
generator	O
and	O
samples	O
drawn	O
from	O
an	O
actual	O
dataset	O
using	O
a	O
function	O
D	O
ω	O
(	O
x	O
)	O
with	O
learned	O
weights	O
,	O
ω	O
.	O

The	O
GAN	S-Method
framework	O
is	O
one	O
of	O
the	O
more	O
recent	O
successes	O
in	O
a	O
line	O
of	O
research	O
on	O
adversarial	B-Task
training	E-Task
in	O
machine	B-Task
learning	E-Task
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
)	O
where	O
games	O
between	O
learners	O
are	O
carefully	O
crafted	O
so	O
that	O
Nash	O
equilibria	O
coincide	O
with	O
some	O
set	O
of	O
desired	O
optimality	B-Metric
criteria	E-Metric
.	O

Preliminary	O
work	O
on	O
GANs	S-Method
focused	O
on	O
generating	B-Task
images	E-Task
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
,	O
CIFAR	S-Material
[	O
reference	O
]	O
)	O
)	O
,	O
however	O
,	O
GANs	S-Method
have	O
proven	O
useful	O
in	O
a	O
variety	O
of	O
application	O
domains	O
including	O
learning	B-Task
censored	I-Task
representations	E-Task
[	O
reference	O
]	O
)	O
,	O
imitating	O
expert	O
policies	O
[	O
reference	O
]	O
)	O
,	O
and	O
domain	B-Task
transfer	E-Task
[	O
reference	O
]	O
)	O
.	O

Work	O
extending	O
GANs	S-Method
to	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
;	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
;	O
Springenberg	O
(	O
2015	O
)	O
)	O
,	O
inference	S-Task
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
)	O
,	O
feature	B-Method
learning	E-Method
[	O
reference	O
]	O
)	O
,	O
and	O
improved	O
image	B-Task
generation	E-Task
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
)	O
have	O
shown	O
promise	O
as	O
well	O
.	O

Despite	O
these	O
successes	O
,	O
GANs	S-Method
are	O
reputably	O
difficult	O
to	O
train	O
.	O

While	O
research	O
is	O
still	O
underway	O
to	O
improve	O
training	B-Method
techniques	E-Method
and	O
heuristics	O
[	O
reference	O
]	O
)	O
,	O
most	O
approaches	O
have	O
focused	O
on	O
understanding	O
and	O
generalizing	O
GANs	S-Method
theoretically	O
with	O
the	O
aim	O
of	O
exploring	O
more	O
tractable	O
formulations	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
theoretically	O
and	O
empirically	O
justify	O
generalizing	O
the	O
GAN	S-Method
framework	O
to	O
multiple	O
discriminators	S-Method
.	O

We	O
review	O
GANs	S-Method
and	O
summarize	O
our	O
extension	O
in	O
Section	O
2	O
.	O

In	O
Sections	O
3	O
and	O
4	O
,	O
we	O
present	O
our	O
N	B-Method
-	I-Method
discriminator	I-Method
extension	E-Method
to	O
the	O
GAN	S-Method
framework	O
(	O
Generative	B-Method
Multi	I-Method
-	I-Method
Adversarial	I-Method
Networks	E-Method
)	O
with	O
several	O
variants	O
which	O
range	O
the	O
role	O
of	O
the	O
discriminator	S-Method
from	O
formidable	O
adversary	O
to	O
forgiving	O
teacher	O
.	O

Section	O
4.2	O
explains	O
how	O
this	O
extension	O
makes	O
training	S-Task
with	O
the	O
untampered	B-Task
minimax	I-Task
objective	E-Task
tractable	O
.	O

In	O
Section	O
5	O
,	O
we	O
define	O
an	O
intuitive	B-Metric
metric	E-Metric
(	O
GMAM	S-Method
)	O
to	O
quantify	O
GMAN	S-Method
performance	O
and	O
evaluate	O
our	O
framework	O
on	O
a	O
variety	O
of	O
image	B-Task
generation	E-Task
tasks	O
.	O

Section	O
6	O
concludes	O
with	O
a	O
summary	O
of	O
our	O
contributions	O
and	O
directions	O
for	O
future	O
research	O
.	O

Contributions	O
-	O
To	O
summarize	O
,	O
our	O
main	O
contributions	O
are	O
:	O
i	O
)	O
a	O
multi	O
-	O
discriminator	O
GAN	S-Method
framework	O
,	O
GMAN	S-Method
,	O
that	O
allows	O
training	O
with	O
the	O
original	O
,	O
untampered	O
minimax	O
objective	O
;	O
ii	O
)	O
a	O
generative	B-Metric
multi	I-Metric
-	I-Metric
adversarial	I-Metric
metric	E-Metric
(	O
GMAM	S-Method
)	O
to	O
perform	O
pairwise	B-Task
evaluation	E-Task
of	O
separately	B-Method
trained	I-Method
frameworks	E-Method
;	O
iii	O
)	O
a	O
particular	O
instance	O
of	O
GMAN	S-Method
,	O
GMAN	S-Method
*	O
,	O
that	O
allows	O
the	O
generator	S-Method
to	O
automatically	O
regulate	O
training	O
and	O
reach	O
higher	O
performance	O
(	O
as	O
measured	O
by	O
GMAM	S-Method
)	O
in	O
a	O
fraction	O
of	O
the	O
training	B-Metric
time	E-Metric
required	O
for	O
the	O
standard	O
GAN	S-Method
model	O
.	O

section	O
:	O
GENERATIVE	B-Method
ADVERSARIAL	I-Method
NETWORKS	E-Method
TO	O
GMAN	S-Method
The	O
original	O
formulation	O
of	O
a	O
GAN	S-Method
is	O
a	O
minimax	B-Method
game	E-Method
between	O
a	O
generator	S-Method
,	O
G	O
θ	O
(	O
z	O
)	O
:	O
z	O
→	O
x	O
,	O
and	O
a	O
discriminator	S-Method
,	O
where	O
p	O
data	O
(	O
x	O
)	O
is	O
the	O
true	O
data	O
distribution	O
and	O
p	O
z	O
(	O
z	O
)	O
is	O
a	O
simple	O
(	O
usually	O
fixed	O
)	O
distribution	O
that	O
is	O
easy	O
to	O
draw	O
samples	O
from	O
(	O
e.g.	O
,	O
N	O
(	O
0	O
,	O
1	O
)	O
)	O
.	O

We	O
differentiate	O
between	O
the	O
function	O
space	O
of	O
discriminators	O
,	O
D	O
,	O
and	O
elements	O
of	O
this	O
space	O
,	O
D.	O
Let	O
p	O
G	O
(	O
x	O
)	O
be	O
the	O
distribution	O
induced	O
by	O
the	O
generator	S-Method
,	O
G	O
θ	O
(	O
z	O
)	O
.	O

We	O
assume	O
D	O
,	O
G	O
to	O
be	O
deep	B-Method
neural	I-Method
networks	E-Method
as	O
is	O
typically	O
the	O
case	O
.	O

In	O
their	O
original	O
work	O
,	O
[	O
reference	O
]	O
proved	O
that	O
given	O
sufficient	O
network	O
capacities	O
and	O
an	O
oracle	O
providing	O
the	O
optimal	O
discriminator	S-Method
,	O
will	O
recover	O
the	O
desired	O
globally	O
optimal	O
solution	O
,	O
p	O
G	O
(	O
x	O
)	O
=	O
p	O
data	O
(	O
x	O
)	O
,	O
so	O
that	O
the	O
generator	B-Method
distribution	E-Method
exactly	O
matches	O
the	O
data	O
distribution	O
.	O

In	O
practice	O
,	O
they	O
replaced	O
the	O
second	O
term	O
,	O
log	O
(	O
1−	O
D	O
(	O
G	O
(	O
z	O
)	O
)	O
)	O
,	O
with	O
−	O
log	O
(	O
D	O
(	O
G	O
(	O
z	O
)	O
)	O
)	O
to	O
enhance	O
gradient	O
signals	O
at	O
the	O
start	O
of	O
the	O
game	O
;	O
note	O
this	O
is	O
no	O
longer	O
a	O
zero	B-Task
-	I-Task
sum	I-Task
game	E-Task
.	O

Part	O
of	O
their	O
convergence	O
and	O
optimality	O
proof	O
involves	O
using	O
the	O
oracle	O
,	O
D	B-Method
*	E-Method
,	O
to	O
reduce	O
the	O
minimax	B-Method
game	E-Method
to	O
a	O
minimization	B-Task
over	I-Task
G	E-Task
only	O
:	O
where	O
JSD	S-Method
denotes	O
Jensen	B-Method
-	I-Method
Shannon	I-Method
divergence	E-Method
.	O

Minimizing	B-Task
C	I-Task
(	I-Task
G	E-Task
)	O
necessarily	O
minimizes	O
JSD	S-Method
,	O
however	O
,	O
we	O
rarely	O
know	O
D	O
*	O
and	O
so	O
we	O
instead	O
minimize	O
V	O
(	O
D	O
,	O
G	O
)	O
,	O
which	O
is	O
only	O
a	O
lower	O
bound	O
.	O

This	O
perspective	O
of	O
minimizing	O
the	O
distance	O
between	O
the	O
distributions	O
,	O
p	O
data	O
and	O
p	O
G	O
,	O
motivated	O
[	O
reference	O
]	O
to	O
develop	O
a	O
generative	B-Method
model	E-Method
that	O
matches	O
all	O
moments	O
of	O
p	O
G	O
(	O
x	O
)	O
with	O
p	O
data	O
(	O
x	O
)	O
(	O
at	O
optimality	O
)	O
by	O
minimizing	O
maximum	B-Method
mean	I-Method
discrepancy	E-Method
(	O
MMD	S-Method
)	O
.	O

Another	O
approach	O
,	O
EBGAN	S-Method
,	O
[	O
reference	O
]	O
)	O
explores	O
a	O
larger	O
class	O
of	O
games	O
(	O
non	B-Task
-	I-Task
zero	I-Task
-	I-Task
sum	I-Task
games	E-Task
)	O
which	O
generalize	O
the	O
generator	B-Method
and	I-Method
discriminator	I-Method
objectives	E-Method
to	O
take	O
real	O
-	O
valued	O
"	O
energies	O
"	O
as	O
input	O
instead	O
of	O
probabilities	O
.	O

[	O
reference	O
]	O
and	O
then	O
[	O
reference	O
]	O
extended	O
the	O
JSD	B-Method
perspective	E-Method
on	O
GANs	S-Method
to	O
more	O
general	O
divergences	O
,	O
specifically	O
f	B-Method
-	I-Method
divergences	E-Method
and	O
then	O
Bregman	B-Method
-	I-Method
divergences	E-Method
respectively	O
.	O

In	O
general	O
,	O
these	O
approaches	O
focus	O
on	O
exploring	O
fundamental	O
reformulations	O
of	O
V	B-Task
(	I-Task
D	I-Task
,	I-Task
G	E-Task
)	O
.	O

Similarly	O
,	O
our	O
work	O
focuses	O
on	O
a	O
fundamental	O
reformulation	O
,	O
however	O
,	O
our	O
aim	O
is	O
to	O
provide	O
a	O
framework	O
that	O
accelerates	O
training	O
of	O
the	O
generator	O
to	O
a	O
more	O
robust	O
state	O
irrespective	O
of	O
the	O
choice	O
of	O
V	O
.	O

section	O
:	O
GMAN	S-Method
:	O
A	O
MULTI	B-Method
-	I-Method
ADVERSARIAL	I-Method
EXTENSION	E-Method
We	O
propose	O
introducing	O
multiple	B-Method
discriminators	E-Method


,	O
which	O
brings	O
with	O
it	O
a	O
number	O
of	O
design	O
possibilities	O
.	O

We	O
explore	O
approaches	O
ranging	O
between	O
two	O
extremes	O
:	O
1	O
)	O
a	O
more	O
discriminating	O
D	O
(	O
better	O
approximating	O
max	O
D	O
V	O
(	O
D	O
,	O
G	O
)	O
)	O
and	O
2	O
)	O
a	O
D	O
better	O
matched	O
to	O
the	O
generator	O
's	O
capabilities	O
.	O

Mathematically	O
,	O
we	O
reformulate	O
G	O
's	O
objective	O
as	O
Figure	O
1	O
)	O
.	O

Each	O
D	O
i	O
is	O
still	O
expected	O
to	O
independently	O
maximize	O
its	O
own	O
V	O
(	O
D	O
i	O
,	O
G	O
)	O
(	O
i.e.	O
no	O
cooperation	O
)	O
.	O

We	O
sometimes	O
abbreviate	O
V	O
(	O
D	O
i	O
,	O
G	O
)	O
with	O
V	O
i	O
and	O
F	O
(	O
V	O
1	O
,	O
.	O

.	O


.	O


,	O
V	O
N	O
)	O
with	O
F	O
G	O
(	O
V	O
i	O
)	O
.	O

section	O
:	O
A	O
FORMIDABLE	B-Method
ADVERSARY	E-Method
Here	O
,	O
we	O
consider	O
multi	B-Method
-	I-Method
discriminator	I-Method
variants	E-Method
that	O
attempt	O
to	O
better	O
approximate	O
max	O
D	O
V	O
(	O
D	O
,	O
G	O
)	O
,	O
providing	O
a	O
harsher	O
critic	O
to	O
the	O
generator	O
.	O

Figure	O
1	O
:	O
(	O
GMAN	S-Method
)	O
The	O
generator	S-Method
trains	O
using	O
feedback	O
aggregated	O
over	O
multiple	O
discriminators	S-Method
.	O

If	O
F	O
:	O
=	O
max	O
,	O
G	O
trains	O
against	O
the	O
best	O
discriminator	S-Method
.	O

If	O
F	O
:	O
=	O
mean	O
,	O
G	O
trains	O
against	O
an	O
ensemble	O
.	O

We	O
explore	O
other	O
alternatives	O
to	O
F	O
in	O
Sections	O
4.1	O
&	O
4.4	O
that	O
improve	O
on	O
both	O
these	O
options	O
.	O

section	O
:	O
MAXIMIZING	O
V	O
(	O
D	O
,	O
G	O
)	O
For	O
a	O
fixed	O
G	O
,	O
maximizing	O
F	O
G	O
(	O
V	O
i	O
)	O
with	O
F	O
:	O
=	O
max	O
and	O
N	O
randomly	O
instantiated	O
copies	O
of	O
our	O
discriminator	S-Method
is	O
functionally	O
equivalent	O
to	O
optimizing	O
V	O
(	O
e.g.	O
,	O
stochastic	B-Method
gradient	I-Method
ascent	E-Method


)	O
with	O
random	O
restarts	O
in	O
parallel	O
and	O
then	O
presenting	O
max	O
i∈{1	O
,	O
...	O
,	O
N	O
}	O
V	O
(	O
D	O
i	O
,	O
G	O
)	O
as	O
the	O
loss	O
to	O
the	O
generator	O
-	O
a	O
very	O
pragmatic	O
approach	O
to	O
the	O
difficulties	O
presented	O
by	O
the	O
non	O
-	O
convexity	O
of	O
V	O
caused	O
by	O
the	O
deep	O
net	O
.	O

Requiring	O
the	O
generator	O
to	O
minimize	O
the	O
max	O
forces	O
G	O
to	O
generate	O
high	O
fidelity	O
samples	O
that	O
must	O
hold	O
up	O
under	O
the	O
scrutiny	O
of	O
all	O
N	O
discriminators	O
,	O
each	O
potentially	O
representing	O
a	O
distinct	O
max	O
.	O

In	O
practice	O
,	O
max	O
Di∈D	O
V	O
(	O
D	O
i	O
,	O
G	O
)	O
is	O
not	O
performed	O
to	O
convergence	O
(	O
or	O
global	O
optimality	O
)	O
,	O
so	O
the	O
above	O
problem	O
is	O
oversimplified	O
.	O

Furthermore	O
,	O
introducing	O
N	O
discriminators	O
affects	O
the	O
dynamics	O
of	O
the	O
game	O
which	O
affects	O
the	O
trajectories	O
of	O
the	O
discriminators	O
.	O

This	O
prevents	O
us	O
from	O
claiming	O
max{V	O
1	O
(	O
t	O
)	O
,	O
.	O

.	O


.	O


,	O
V	O
N	O
(	O
t	O
)	O
}	O
>	O
max{V	O
1	O
(	O
t	O
)	O
}	O
∀t	O
even	O
if	O
we	O
initalize	O
D	O
1	O
(	O
0	O
)	O
=	O
D	O
1	O
(	O
0	O
)	O
as	O
it	O
is	O
unlikely	O
that	O
D	O
1	O
(	O
t	O
)	O
=	O
D	O
1	O
(	O
t	O
)	O
at	O
some	O
time	O
t	O
after	O
the	O
start	O
of	O
the	O
game	O
.	O

section	O
:	O
BOOSTING	S-Task
We	O
can	O
also	O
consider	O
taking	O
the	O
max	B-Method
over	I-Method
N	I-Method
discriminators	E-Method
as	O
a	O
form	O
of	O
boosting	S-Method
for	O
the	O
discriminator	B-Task
's	I-Task
online	I-Task
classification	I-Task
problem	E-Task
(	O
online	O
because	O
G	O
can	O
produce	O
an	O
infinite	O
data	O
stream	O
)	O
.	O

The	O
boosted	B-Method
discriminator	E-Method
is	O
given	O
a	O
sample	O
x	O
t	O
and	O
must	O
predict	O
whether	O
it	O
came	O
from	O
the	O
generator	O
or	O
the	O
dataset	O
.	O

The	O
booster	O
then	O
makes	O
its	O
prediction	O
using	O
the	O
predictions	O
of	O
the	O
N	O
weaker	O
D	O
i	O
.	O

There	O
are	O
a	O
few	O
differences	O
between	O
taking	O
the	O
max	O
(	O
case	O
1	O
)	O
and	O
online	B-Method
boosting	E-Method
(	O
case	O
2	O
)	O
.	O

In	O
case	O
1	O
,	O
our	O
booster	S-Method
is	O
limited	O
to	O
selecting	O
a	O
single	O
weak	B-Method
discriminator	E-Method
(	O
i.e.	O
a	O
pure	O
strategy	O
)	O
,	O
while	O
in	O
case	O
2	O
,	O
many	O
boosting	B-Method
algorithms	E-Method
more	O
generally	O
use	O
linear	B-Method
combinations	E-Method
of	O
the	O
discriminators	O
.	O

Moreover	O
,	O
in	O
case	O
2	O
,	O
a	O
booster	S-Method
must	O
make	O
a	O
prediction	O
before	O
receiving	O
a	O
loss	O
function	O
.	O

In	O
case	O
1	O
,	O
we	O
assume	O
access	O
to	O
the	O
loss	O
function	O
at	O
prediction	O
time	O
,	O
which	O
allows	O
us	O
to	O
compute	O
the	O
max	O
.	O

It	O
is	O
possible	O
to	O
train	O
the	O
weak	B-Method
discriminators	E-Method
using	O
boosting	S-Method
and	O
then	O
ignore	O
the	O
booster	B-Method
's	I-Method
prediction	E-Method
by	O
instead	O
presenting	O
max{V	O
i	O
}	O
.	O

We	O
explore	O
both	O
variants	O
in	O
our	O
experiments	O
,	O
using	O
the	O
adaptive	B-Method
algorithm	E-Method
proposed	O
in	O
[	O
reference	O
]	O
.	O

Unfortunately	O
,	O
boosting	S-Method
failed	O
to	O
produce	O
promising	O
results	O
on	O
the	O
image	B-Task
generation	E-Task
tasks	O
.	O

It	O
is	O
possible	O
that	O
boosting	S-Method
produces	O
too	O
strong	O
an	O
adversary	O
for	O
learning	S-Task
which	O
motivates	O
the	O
next	O
section	O
.	O

Boosting	S-Method
results	O
appear	O
in	O
Appendix	O
A.7	O
.	O

section	O
:	O
A	O
FORGIVING	O
TEACHER	O
The	O
previous	O
perspectives	O
focus	O
on	O
improving	O
the	O
discriminator	S-Method
with	O
the	O
goal	O
of	O
presenting	O
a	O
better	O
approximation	O
of	O
max	O
D	O
V	O
(	O
D	O
,	O
G	O
)	O
to	O
the	O
generator	S-Method
.	O

Our	O
next	O
perspective	O
asks	O
the	O
question	O
,	O
"	O
Is	O
max	O
D	O
V	O
(	O
D	O
,	O
G	O
)	O
too	O
harsh	O
a	O
critic	O
?	O
"	O
section	O
:	O
Soft	B-Method
-	I-Method
DISCRIMINATOR	E-Method
In	O
practice	O
,	O
training	O
against	O
a	O
far	O
superior	O
discriminator	S-Method
can	O
impede	O
the	O
generator	B-Method
's	I-Method
learning	E-Method
.	O

This	O
is	O
because	O
the	O
generator	O
is	O
unlikely	O
to	O
generate	O
any	O
samples	O
considered	O
"	O
realistic	O
"	O
by	O
the	O
discriminator	O
's	O
standards	O
,	O
and	O
so	O
the	O
generator	O
will	O
receive	O
uniformly	O
negative	O
feedback	O
.	O

This	O
is	O
problem	O
-	O
atic	O
because	O
the	O
information	O
contained	O
in	O
the	O
gradient	O
derived	O
from	O
negative	O
feedback	O
only	O
dictates	O
where	O
to	O
drive	O
down	O
p	O
G	O
(	O
x	O
)	O
,	O
not	O
specifically	O
where	O
to	O
increase	O
p	O
G	O
(	O
x	O
)	O
.	O

Furthermore	O
,	O
driving	O
down	O
p	O
G	O
(	O
x	O
)	O
necessarily	O
increases	O
p	O
G	O
(	O
x	O
)	O
in	O
other	O
regions	O
of	O
X	O
(	O
to	O
maintain	O
X	O
p	O
G	O
(	O
x	O
)	O
=	O
1	O
)	O
which	O
may	O
or	O
may	O
not	O
contain	O
samples	O
from	O
the	O
true	O
dataset	O
(	O
whack	O
-	O
a	O
-	O
mole	O
dilemma	O
)	O
.	O

In	O
contrast	O
,	O
a	O
generator	S-Method
is	O
more	O
likely	O
to	O
see	O
positive	O
feedback	O
against	O
a	O
more	O
lenient	O
discriminator	O
,	O
which	O
may	O
better	O
guide	O
a	O
generator	O
towards	O
amassing	O
p	O
G	O
(	O
x	O
)	O
in	O
approximately	O
correct	O
regions	O
of	O
X	O
.	O

For	O
this	O
reason	O
,	O
we	O
explore	O
a	O
variety	O
of	O
functions	O
that	O
allow	O
us	O
to	O
soften	O
the	O
max	O
operator	O
.	O

We	O
choose	O
to	O
focus	O
on	O
soft	O
versions	O
of	O
the	O
three	O
classical	O
Pythagorean	B-Method
means	E-Method
parameterized	O
by	O
λ	O
where	O
λ	O
=	O
0	O
corresponds	O
to	O
the	O
mean	O
and	O
the	O
max	O
is	O
recovered	O
as	O
λ	O
→	O
∞	O
:	O
where	O
Using	O
a	O
softmax	S-Method
also	O
has	O
the	O
well	O
known	O
advantage	O
of	O
being	O
differentiable	O
(	O
as	O
opposed	O
to	O
subdifferentiable	O
for	O
max	O
)	O
.	O

Note	O
that	O
we	O
only	O
require	O
continuity	O
to	O
guarantee	O
that	O
computing	O
the	O
softmax	O
is	O
actually	O
equivalent	O
to	O
computing	O
V	O
(	O
D	O
,	O
G	O
)	O
whereD	O
is	O
some	O
convex	O
combination	O
of	O
D	O
i	O
(	O
see	O
Appendix	O
A.5	O
)	O
.	O

section	O
:	O
USING	O
THE	O
ORIGINAL	O
MINIMAX	B-Method
OBJECTIVE	E-Method
To	O
illustrate	O
the	O
effect	O
the	O
softmax	S-Method
has	O
on	O
training	S-Task
,	O
observe	O
that	O
the	O
component	O
of	O
AM	O
sof	O
t	O
(	O
V	O
,	O
0	O
)	O
relevant	O
to	O
generator	B-Task
training	E-Task
can	O
be	O
rewritten	O
as	O
where	O
.	O

From	O
this	O
form	O
,	O
it	O
is	O
clear	O
that	O
z	O
=	O
1	O
if	O
and	O
only	O
if	O
D	O
i	O
=	O
0	O
∀i	O
,	O
so	O
G	O
only	O
receives	O
a	O
vanishing	O
gradient	O
if	O
all	O
D	O
i	O
agree	O
that	O
the	O
sample	O
is	O
fake	O
;	O
this	O
is	O
especially	O
unlikely	O
for	O
large	O
N	O
.	O

In	O
other	O
words	O
,	O
G	O
only	O
needs	O
to	O
fool	O
a	O
single	O
D	O
i	O
to	O
receive	O
constructive	O
feedback	O
.	O

This	O
result	O
allows	O
the	O
generator	O
to	O
successfully	O
minimize	O
the	O
original	O
generator	B-Metric
objective	E-Metric
,	O
log	O
(	O
1	O
−	O
D	O
)	O
.	O

This	O
is	O
in	O
contrast	O
to	O
the	O
more	O
popular	O
−	B-Method
log	I-Method
(	I-Method
D	I-Method
)	E-Method
introduced	O
to	O
artificially	O
enhance	O
gradients	O
at	O
the	O
start	O
of	O
training	O
.	O

At	O
the	O
beginning	O
of	O
training	O
,	O
when	O
max	O
Di	O
V	O
(	O
D	O
i	O
,	O
G	O
)	O
is	O
likely	O
too	O
harsh	O
a	O
critic	O
for	O
the	O
generator	O
,	O
we	O
can	O
set	O
λ	O
closer	O
to	O
zero	O
to	O
use	O
the	O
mean	O
,	O
increasing	O
the	O
odds	O
of	O
providing	O
constructive	O
feedback	O
to	O
the	O
generator	O
.	O

In	O
addition	O
,	O
the	O
discriminators	S-Method
have	O
the	O
added	O
benefit	O
of	O
functioning	O
as	O
an	O
ensemble	S-Method
,	O
reducing	O
the	O
variance	O
of	O
the	O
feedback	O
presented	O
to	O
the	O
generator	S-Method
,	O
which	O
is	O
especially	O
important	O
when	O
the	O
discriminators	O
are	O
far	O
from	O
optimal	O
and	O
are	O
still	O
learning	O
a	O
reasonable	O
decision	O
boundary	O
.	O

As	O
training	O
progresses	O
and	O
the	O
discriminators	O
improve	O
,	O
we	O
can	O
increase	O
λ	O
to	O
become	O
more	O
critical	O
of	O
the	O
generator	O
for	O
more	O
refined	O
training	S-Task
.	O

section	O
:	O
MAINTAINING	O
MULTIPLE	O
HYPOTHESES	O
We	O
argue	O
for	O
this	O
ensemble	B-Method
approach	E-Method
on	O
a	O
more	O
fundamental	O
level	O
as	O
well	O
.	O

Here	O
,	O
we	O
draw	O
on	O
the	O
density	B-Method
ratio	I-Method
estimation	I-Method
perspective	E-Method
of	O
GANs	S-Method
[	O
reference	O
]	O
)	O
.	O

The	O
original	O
GAN	S-Method
proof	O
assumes	O
we	O
have	O
access	O
to	O
p	O
data	O
(	O
x	O
)	O
,	O
if	O
only	O
implicitly	O
.	O

In	O
most	O
cases	O
of	O
interest	O
,	O
the	O
discriminator	S-Method
only	O
has	O
access	O
to	O
a	O
finite	O
dataset	O
sampled	O
from	O
p	O
data	O
(	O
x	O
)	O
;	O
therefore	O
,	O
when	O
computing	O
expectations	O
of	O
V	O
(	O
D	O
,	O
G	O
)	O
,	O
we	O
only	O
draw	O
samples	O
from	O
our	O
finite	O
dataset	O
.	O

This	O
is	O
equivalent	O
to	O
training	O
a	O
GAN	S-Method
with	O
p	O
data	O
(	O
x	O
)	O
=	O
p	O
data	O
which	O
is	O
a	O
distribution	O
consisting	O
of	O
point	O
masses	O
on	O
all	O
the	O
data	O
points	O
in	O
the	O
dataset	O
.	O

For	O
the	O
sake	O
of	O
argument	O
,	O
let	O
's	O
assume	O
we	O
are	O
training	O
a	O
discriminator	B-Method
and	I-Method
generator	E-Method
,	O
each	O
with	O
infinite	O
capacity	O
.	O

In	O
this	O
case	O
,	O
the	O
global	O
optimum	O
(	O
p	O
G	O
(	O
x	O
)	O
=	O
p	O
data	O
(	O
x	O
)	O
)	O
fails	O
to	O
capture	O
any	O
of	O
the	O
interesting	O
structure	O
from	O
p	O
data	O
(	O
x	O
)	O
,	O
the	O
true	O
distribution	O
we	O
are	O
trying	O
to	O
learn	O
.	O

Therefore	O
,	O
it	O
is	O
actually	O
critical	O
that	O
we	O
avoid	O
this	O
global	O
optimum	O
.	O

Figure	O
2	O
:	O
Consider	O
a	O
dataset	O
consisting	O
of	O
the	O
nine	O
1	O
-	O
dimensional	O
samples	O
in	O
black	O
.	O

Their	O
corresponding	O
probability	O
mass	O
function	O
is	O
given	O
in	O
light	O
gray	O
.	O

After	O
training	O
GMAN	S-Method
,	O
three	O
discriminators	S-Method
converge	O
to	O
distinct	O
local	O
optima	O
which	O
implicitly	O
define	O
distributions	O
over	O
the	O
data	O
(	O
red	O
,	O
blue	O
,	O
yellow	O
)	O
.	O

Each	O
discriminator	O
may	O
specialize	O
in	O
discriminating	O
a	O
region	O
of	O
the	O
data	O
space	O
(	O
placing	O
more	O
diffuse	O
mass	O
in	O
other	O
regions	O
)	O
.	O

Averaging	O
over	O
the	O
three	O
discriminators	O
results	O
in	O
the	O
distribution	O
in	O
black	O
,	O
which	O
we	O
expect	O
has	O
higher	O
likelihood	O
under	O
reasonable	O
assumptions	O
on	O
the	O
structure	O
of	O
the	O
true	O
distribution	O
.	O

In	O
practice	O
,	O
this	O
degenerate	O
result	O
is	O
avoided	O
by	O
employing	O
learners	B-Method
with	I-Method
limited	I-Method
capacity	E-Method
and	O
corrupting	O
data	O
samples	O
with	O
noise	O
(	O
i.e.	O
,	O
dropout	S-Method
)	O
,	O
but	O
we	O
might	O
better	O
accomplish	O
this	O
by	O
simultaneously	O
training	O
a	O
variety	O
of	O
limited	B-Method
capacity	I-Method
discriminators	E-Method
.	O

With	O
this	O
approach	O
,	O
we	O
might	O
obtain	O
a	O
diverse	O
set	O
of	O
seemingly	O
tenable	O
hypotheses	O
for	O
the	O
true	O
p	O
data	O
(	O
x	O
)	O
.	O

Averaging	O
over	O
these	O
multiple	O
locally	B-Method
optimal	I-Method
discriminators	E-Method
increases	O
the	O
entropy	O
ofp	O
data	O
(	O
x	O
)	O
by	O
diffusing	O
the	O
probability	O
mass	O
over	O
the	O
data	O
space	O
(	O
see	O
Figure	O
2	O
for	O
an	O
example	O
)	O
.	O

section	O
:	O
AUTOMATING	B-Task
REGULATION	E-Task
The	O
problem	O
of	O
keeping	O
the	O
discriminator	O
and	O
generator	O
in	O
balance	O
has	O
been	O
widely	O
recognized	O
in	O
previous	O
work	O
with	O
GANs	S-Method
.	O

Issues	O
with	O
unstable	O
dynamics	O
,	O
oscillatory	O
behavior	O
,	O
and	O
generator	B-Task
collapse	E-Task
are	O
not	O
uncommon	O
.	O

In	O
addition	O
,	O
the	O
discriminator	S-Method
is	O
often	O
times	O
able	O
to	O
achieve	O
a	O
high	O
degree	O
of	O
classification	B-Metric
accuracy	E-Metric
(	O
producing	O
a	O
single	O
scalar	O
)	O
before	O
the	O
generator	O
has	O
made	O
sufficient	O
progress	O
on	O
the	O
arguably	O
more	O
difficult	O
generative	B-Task
task	E-Task
(	O
producing	O
a	O
high	O
dimensional	O
sample	O
)	O
.	O

[	O
reference	O
]	O
suggested	O
label	B-Method
smoothing	E-Method
to	O
reduce	O
the	O
vulnerability	O
of	O
the	O
generator	O
to	O
a	O
relatively	O
superior	O
discriminator	O
.	O

Here	O
,	O
we	O
explore	O
an	O
approach	O
that	O
enables	O
the	O
generator	O
to	O
automatically	O
temper	O
the	O
performance	O
of	O
the	O
discriminator	O
when	O
necessary	O
,	O
but	O
still	O
encourages	O
the	O
generator	O
to	O
challenge	O
itself	O
against	O
more	O
accurate	O
adversaries	O
.	O

Specifically	O
,	O
we	O
augment	O
the	O
generator	B-Metric
objective	E-Metric
:	O
min	O
G	O
,	O
λ>0	O
where	O
f	O
(	O
λ	O
)	O
is	O
monotonically	O
increasing	O
in	O
λ	O
which	O
appears	O
in	O
the	O
softmax	O
equations	O
,	O
(	O
3	O
)-(	O
5	O
)	O
.	O

In	O
experiments	O
,	O
we	O
simply	O
set	O
f	O
(	O
λ	O
)	O
=	O
cλ	O
with	O
c	O
a	O
constant	O
(	O
e.g.	O
,	O
0.001	O
)	O
.	O

The	O
generator	O
is	O
incentivized	O
to	O
increase	O
λ	O
to	O
reduce	O
its	O
objective	O
at	O
the	O
expense	O
of	O
competing	O
against	O
the	O
best	O
available	O
adversary	O
D	O
*	O
(	O
see	O
Appendix	O
A.6	O
)	O
.	O

section	O
:	O
EVALUATION	O
Evaluating	B-Task
GANs	E-Task
is	O
still	O
an	O
open	O
problem	O
.	O

In	O
their	O
original	O
work	O
,	O
[	O
reference	O
]	O
report	O
log	B-Method
likelihood	I-Method
estimates	E-Method
from	O
Gaussian	B-Method
Parzen	I-Method
windows	E-Method
,	O
which	O
they	O
admit	O
,	O
has	O
high	O
variance	O
and	O
is	O
known	O
not	O
to	O
perform	O
well	O
in	O
high	O
dimensions	O
.	O

[	O
reference	O
]	O
recommend	O
avoiding	O
Parzen	B-Method
windows	E-Method
and	O
argue	O
that	O
generative	B-Method
models	E-Method
should	O
be	O
evaluated	O
with	O
respect	O
to	O
their	O
intended	O
application	O
.	O

[	O
reference	O
]	O
suggest	O
an	O
Inception	B-Metric
score	E-Metric
,	O
however	O
,	O
it	O
assumes	O
labels	O
exist	O
for	O
the	O
dataset	O
.	O

Recently	O
,	O
[	O
reference	O
]	O
introduced	O
the	O
Generative	B-Method
Adversarial	I-Method
Metric	I-Method
(	I-Method
GAM	I-Method
)	E-Method
for	O
making	O
pairwise	B-Task
comparisons	E-Task
between	O
independently	O
trained	O
GAN	S-Method
models	O
.	O

The	O
core	O
idea	O
behind	O
their	O
approach	O
is	O
given	O
two	O
generator	B-Method
,	I-Method
discriminator	I-Method
pairs	E-Method
(	O
G	O
1	O
,	O
D	O
1	O
)	O
and	O
(	O
G	O
2	O
,	O
D	O
2	O
)	O
,	O
we	O
should	O
be	O
able	O
to	O
learn	O
their	O
relative	O
performance	O
by	O
judging	O
each	O
generator	O
under	O
the	O
opponent	O
's	O
discriminator	O
.	O

section	O
:	O
METRIC	O
In	O
GMAN	S-Method
,	O
the	O
opponent	O
may	O
have	O
multiple	O
discriminators	O
,	O
which	O
makes	O
it	O
unclear	O
how	O
to	O
perform	O
the	O
swaps	O
needed	O
for	O
GAM	O
.	O

We	O
introduce	O
a	O
variant	O
of	O
GAM	S-Method
,	O
the	O
generative	B-Metric
multi	I-Metric
-	I-Metric
adversarial	I-Metric
metric	E-Metric
(	O
GMAM	S-Method
)	O
,	O
that	O
is	O
amenable	O
to	O
training	O
with	O
multiple	B-Method
discriminators	E-Method
,	O
where	O
a	O
and	O
b	O
refer	O
to	O
the	O
two	O
GMAN	S-Method
variants	O
(	O
see	O
Section	O
3	O
for	O
notation	O
F	O
G	O
(	O
V	O
i	O
)	O
)	O
.	O

The	O
idea	O
here	O
is	O
similar	O
.	O

If	O
G	O
2	O
performs	O
better	O
than	O
G	O
1	O
with	O
respect	O
to	O
both	O
D	O
1	O
and	O
D	O
2	O
,	O
then	O
GMAM>0	S-Method
(	O
remember	O
V	O
≤0	O
always	O
)	O
.	O

If	O
G	O
1	O
performs	O
better	O
in	O
both	O
cases	O
,	O
GMAM<0	S-Method
,	O
otherwise	O
,	O
the	O
result	O
is	O
indeterminate	O
.	O

section	O
:	O
EXPERIMENTS	O
We	O
evaluate	O
the	O
aforementioned	O
variations	O
of	O
GMAN	S-Method
on	O
a	O
variety	O
of	O
image	B-Task
generation	E-Task
tasks	O
:	O
MNIST	S-Material
(	O
LeCun	O
et	O
al	O
.	O

(	O
1998	O
)	O
)	O
,	O
CIFAR	B-Material
-	I-Material
10	E-Material
(	O
Krizhevsky	O
(	O
2009	O
)	O
)	O
and	O
CelebA	S-Material
[	O
reference	O
]	O
)	O
.	O

We	O
focus	O
on	O
rates	O
of	O
convergence	S-Metric
to	O
steady	O
state	O
along	O
with	O
quality	O
of	O
the	O
steady	B-Method
state	I-Method
generator	E-Method
according	O
to	O
the	O
GMAM	S-Method
metric	O
.	O

To	O
summarize	O
,	O
loosely	O
in	O
order	O
of	O
increasing	O
discriminator	O
leniency	O
,	O
we	O
compare	O
•	O
F	B-Method
-	I-Method
boost	E-Method
:	O
A	O
single	O
AdaBoost	S-Method
.	O

OL	B-Method
-	I-Method
boosted	I-Method
discriminator	E-Method
(	O
see	O
Appendix	O
A.7	O
)	O
.	O

•	O
P	B-Method
-	I-Method
boost	E-Method
:	O
D	O
i	O
is	O
trained	O
according	O
to	O
AdaBoost	S-Method
.	O

OL	O
.	O

A	O
max	S-Method
over	O
the	O
weak	O
learner	O
losses	O
is	O
presented	O
to	O
the	O
generator	O
instead	O
of	O
the	O
boosted	B-Method
prediction	E-Method
(	O
see	O
Appendix	O
A.7	O
)	O
.	O

•	O
GMAN	S-Method
-	O
max	O
:	O
max{V	O
i	O
}	O
is	O
presented	O
to	O
the	O
generator	O
.	O

•	O
GAN	S-Method
:	O
Standard	O
GAN	S-Method
with	O
a	O
single	O
discriminator	S-Method
(	O
see	O
Appendix	O
A.2	O
)	O
.	O

•	O
mod	O
-	O
GAN	S-Method
:	O
GAN	S-Method
with	O
modified	O
objective	O
(	O
generator	O
minimizes	O
−	O
log	O
(	O
D	O
(	O
G	O
(	O
z	O
)	O
)	O
)	O
.	O

•	O
GMAN	S-Method
-	O
λ	O
:	O
GMAN	S-Method
with	O
F	O
:	O
=	O
arithmetic	B-Method
softmax	E-Method
with	O
parameter	O
λ	O
.	O

•	O
GMAN	S-Method
*	O
:	O
The	O
arithmetic	O
softmax	O
is	O
controlled	O
by	O
the	O
generator	S-Method
through	O
λ	S-Method
.	O

All	O
generator	B-Method
and	I-Method
discriminator	I-Method
models	E-Method
are	O
deep	O
(	O
de	B-Method
)	I-Method
convolutional	I-Method
networks	E-Method
[	O
reference	O
]	O
)	O
,	O
and	O
aside	O
from	O
the	O
boosted	B-Method
variants	E-Method
,	O
all	O
are	O
trained	O
with	O
Adam	S-Method
[	O
reference	O
]	O
)	O
and	O
batch	B-Method
normalization	E-Method
[	O
reference	O
]	O
)	O
.	O

Discriminators	S-Method
convert	O
the	O
real	O
-	O
valued	O
outputs	O
of	O
their	O
networks	O
to	O
probabilities	O
with	O
squashed	B-Method
-	I-Method
sigmoids	E-Method
to	O
prevent	O
saturating	O
logarithms	O
in	O
the	O
minimax	O
objective	O
(	O
+	O
1−2	O
1	O
+	O
e	O
−z	O
)	O
.	O

See	O
Appendix	O
A.8	O
for	O
further	O
details	O
.	O

We	O
test	O
GMAN	S-Method
systems	O
with	O
N	O
=	O
{	O
2	O
,	O
5	O
}	O
discriminators	O
.	O

We	O
maintain	O
discriminator	O
diversity	O
by	O
varying	O
dropout	O
and	O
network	O
depth	O
.	O

Figure	O
3	O
reveals	O
that	O
increasing	O
the	O
number	O
of	O
discriminators	S-Method
reduces	O
the	O
number	O
of	O
iterations	O
to	O
steady	O
-	O
state	O
by	O
2x	O
on	O
MNIST	S-Material
;	O
increasing	O
N	O
(	O
the	O
size	O
of	O
the	O
discriminator	B-Method
ensemble	E-Method
)	O
also	O
has	O
the	O
added	O
benefit	O
of	O
reducing	O
the	O
variance	S-Metric
the	O
minimax	B-Metric
objective	E-Metric
over	O
runs	O
.	O

Figure	O
4	O
displays	O
the	O
variance	O
of	O
the	O
same	O
objective	O
over	O
a	O
sliding	O
time	O
window	O
,	O
reaffirming	O
GMAN	S-Method
's	O
acceleration	O
to	O
steadystate	O
.	O

Figure	O
5	O
corroborates	O
this	O
conclusion	O
with	O
recognizable	O
digits	O
appearing	O
approximately	O
an	O
epoch	O
before	O
the	O
single	O
discriminator	O
run	O
;	O
digits	O
at	O
steady	O
-	O
state	O
appear	O
slightly	O
sharper	O
as	O
well	O
.	O

section	O
:	O
MNIST	S-Material
Our	O
GMAM	S-Method
metric	O
(	O
see	O
Table	O
1	O
)	O
agrees	O
with	O
the	O
relative	O
quality	O
of	O
images	O
in	O
Figure	O
5	O
with	O
GMAN	S-Method
*	O
achieving	O
the	O
best	O
overall	O
performance	O
.	O

section	O
:	O
CELEBA	S-Material
&	O
CIFAR	B-Material
-	I-Material
10	E-Material
We	O
see	O
similar	O
accelerated	O
convergence	O
behavior	O
for	O
the	O
CelebA	S-Material
dataset	O
in	O
Figure	O
8	O
.	O

We	O
also	O
found	O
that	O
GMAN	S-Method
is	O
robust	O
to	O
mode	B-Task
collapse	E-Task
.	O

We	O
believe	O
this	O
is	O
because	O
the	O
generator	O
must	O
appease	O
a	O
diverse	O
set	O
of	O
discriminators	O
in	O
each	O
minibatch	O
.	O

Emitting	O
a	O
single	O
sample	O
will	O
score	O
well	O
for	O
one	O
discriminator	O
at	O
the	O
expense	O
of	O
the	O
rest	O
of	O
the	O
discriminators	O
.	O

Current	O
solutions	O
(	O
e.g.	O
,	O
minibatch	O
discrimination	O
)	O
are	O
quadratic	O
in	O
batch	O
size	O
.	O

GMAN	S-Method
,	O
however	O
,	O
is	O
linear	O
in	O
batch	O
size	O
.	O

section	O
:	O
CONCLUSION	O
We	O
introduced	O
multiple	O
discriminators	S-Method
into	O
the	O
GAN	S-Method
framework	O
and	O
explored	O
discriminator	O
roles	O
ranging	O
from	O
a	O
formidable	B-Method
adversary	E-Method
to	O
a	O
forgiving	O
teacher	O
.	O

Allowing	O
the	O
generator	O
to	O
automatically	O
tune	O
its	O
learning	B-Method
schedule	E-Method
(	O
GMAN	S-Method
*	O
)	O
outperformed	O
GANs	S-Method
with	O
a	O
single	O
discriminator	S-Method
on	O
MNIST	S-Material
.	O

In	O
general	O
,	O
GMAN	S-Method
variants	O
achieved	O
faster	O
convergence	S-Metric
to	O
a	O
higher	O
quality	O
steady	O
state	O
on	O
a	O
variety	O
of	O
tasks	O
as	O
measured	O
by	O
a	O
GAM	B-Metric
-	I-Metric
type	I-Metric
metric	E-Metric
(	O
GMAM	S-Method
)	O
.	O

In	O
addition	O
,	O
GMAN	S-Method
makes	O
using	O
the	O
original	O
GAN	S-Method
objective	O
possible	O
by	O
increasing	O
the	O
odds	O
of	O
the	O
generator	O
receiving	O
constructive	O
feedback	O
.	O

In	O
future	O
work	O
,	O
we	O
will	O
look	O
at	O
more	O
sophisticated	O
mechanisms	O
for	O
letting	O
the	O
generator	O
control	O
the	O
game	O
as	O
well	O
as	O
other	O
ways	O
to	O
ensure	O
diversity	O
among	O
the	O
discriminators	O
.	O

Introducing	O
multiple	O
generators	O
is	O
conceptually	O
an	O
obvious	O
next	O
step	O
,	O
however	O
,	O
we	O
expect	O
difficulties	O
to	O
arise	O
from	O
more	O
complex	O
game	O
dynamics	O
.	O

For	O
this	O
reason	O
,	O
game	B-Method
theory	E-Method
and	O
game	B-Task
design	E-Task
will	O
likely	O
be	O
important	O
.	O

See	O
Tables	O
2	O
,	O
3	O
[	O
reference	O
]	O
.	O

Increasing	O
the	O
number	O
of	O
discriminators	O
from	O
2	O
to	O
5	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
significantly	O
improves	O
scores	O
over	O
the	O
standard	O
GAN	S-Method
both	O
in	O
terms	O
of	O
the	O
GMAM	S-Method
metric	O
and	O
Inception	B-Metric
scores	E-Metric
.	O

Table	O
6	O
:	O
Inception	B-Metric
score	E-Metric
means	O
with	O
standard	O
deviations	O
for	O
select	O
models	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
.	O

Higher	O
scores	O
are	O
better	O
.	O

GMAN	S-Method
variants	O
were	O
trained	O
with	O
five	O
discriminators	S-Method
.	O

section	O
:	O
A.3	O
GENERATED	O
IMAGES	O
section	O
:	O
A.4	O
SOMEWHAT	O
RELATED	O
WORK	O
A	O
GAN	S-Method
framework	O
with	O
two	O
discriminators	S-Method
appeared	O
in	O
[	O
reference	O
]	O
,	O
however	O
,	O
it	O
is	O
applicable	O
only	O
in	O
a	O
semi	B-Task
-	I-Task
supervised	I-Task
case	E-Task


where	O
a	O
label	O
can	O
be	O
assigned	O
to	O
subsets	O
of	O
the	O
dataset	O
(	O
e.g.	O
,	O
X	O
=	O
{	O
X	O
1	O
=	O
Domain	O
1	O
,	O
X	O
2	O
=	O
Domain	O
2	O
,	O
.	O

.	O


.	O


}	O
)	O
.	O

In	O
contrast	O
,	O
our	O
framework	O
applies	O
to	O
an	O
unsupervised	B-Task
scenario	E-Task
where	O
an	O
obvious	O
partition	O
of	O
the	O
dataset	O
is	O
unknown	O
.	O

Furthermore	O
,	O
extending	O
GMAN	S-Method
to	O
the	O
semi	B-Task
-	I-Task
supervised	I-Task
domain	I-Task
-	I-Task
adaptation	I-Task
scenario	E-Task
would	O
suggest	O
multiple	O
discriminators	O
per	O
domain	O
,	O
therefore	O
our	O
line	O
of	O
research	O
is	O
strictly	O
orthogonal	O
to	O
that	O
of	O
their	O
multi	B-Method
-	I-Method
domain	I-Method
discriminator	I-Method
approach	E-Method
.	O

Also	O
,	O
note	O
that	O
assigning	O
a	O
discriminator	O
to	O
each	O
domain	O
is	O
akin	O
to	O
prescribing	O
a	O
new	O
discriminator	O
to	O
each	O
value	O
of	O
a	O
conditional	O
variable	O
in	O
conditional	B-Method
GANs	E-Method
[	O
reference	O
]	O
)	O
.	O

In	O
this	O
case	O
,	O
we	O
interpret	O
GMAN	S-Method
as	O
introducing	O
multiple	O
conditional	B-Method
discriminators	E-Method
and	O
not	O
a	O
discriminator	O
for	O
each	O
of	O
the	O
possibly	O
exponentially	O
many	O
conditional	O
labels	O
.	O

In	O
Section	O
4.4	O
,	O
we	O
describe	O
an	O
approach	O
to	O
customize	O
adversarial	B-Method
training	E-Method
to	O
better	O
suit	O
the	O
development	O
of	O
the	O
generator	O
.	O

An	O
approach	O
with	O
similar	O
conceptual	O
underpinnings	O
was	O
described	O
in	O
[	O
reference	O
]	O
,	O
however	O
,	O
similar	O
to	O
the	O
above	O
,	O
it	O
is	O
only	O
admissible	O
in	O
a	O
semi	B-Task
-	I-Task
supervised	I-Task
scenario	E-Task
whereas	O
our	O
applies	O
to	O
the	O
unsupervised	B-Task
case	E-Task
.	O

section	O
:	O
A.6	O
UNCONSTRAINED	B-Task
OPTIMIZATION	E-Task
To	O
convert	O
GMAN	S-Method
*	O
minimax	O
formulation	O
to	O
an	O
unconstrained	B-Task
minimax	I-Task
formulation	E-Task
,	O
we	O
introduce	O
an	O
auxiliary	O
variable	O
,	O
Λ	O
,	O
define	O
λ	O
(	O
Λ	O
)	O
=	O
log	O
(	O
1	O
+	O
e	O
Λ	O
)	O
,	O
and	O
let	O
the	O
generator	O
minimize	O
over	O
Λ	O
∈	O
R.	O
A.7	O
BOOSTING	S-Method
WITH	O
AdaBoost	S-Method
.	O

OL	O
AdaBoost	S-Method
.	O

OL	O
[	O
reference	O
]	O
)	O
does	O
not	O
require	O
knowledge	O
of	O
the	O
weak	B-Method
learner	E-Method
's	O
slight	O
edge	O
over	O
random	B-Method
guessing	E-Method
(	O
P	O
(	O
correct	O
label	O
)	O
=	O
0.5	O
+	O
γ	O
∈	O
(	O
0	O
,	O
0.5	O
]	O
)	O
,	O
and	O
in	O
fact	O
,	O
allows	O
γ	O
<	O
0	O
.	O

This	O
is	O
crucial	O
because	O
our	O
weak	B-Method
learners	E-Method
are	O
deep	B-Method
nets	E-Method
with	O
unknown	O
,	O
possibly	O
negative	O
,	O
γ	O
's	O
.	O

Figure	O
16	O
:	O
Sample	O
of	O
pictures	O
generated	O
across	O
4	O
independent	O
runs	O
on	O
MNIST	S-Material
with	O
F	B-Method
-	I-Method
boost	E-Method
(	O
similar	O
results	O
with	O
P	B-Method
-	I-Method
boost	E-Method
)	O
.	O

section	O
:	O
A.8	O
EXPERIMENTAL	O
SETUP	O
All	O
experiments	O
were	O
conducted	O
using	O
an	O
architecture	O
similar	O
to	O
DCGAN	S-Method
[	O
reference	O
]	O
)	O
.	O

We	O
use	O
convolutional	B-Method
transpose	I-Method
layers	E-Method
[	O
reference	O
]	O
)	O
for	O
G	O
and	O
strided	B-Method
convolutions	E-Method
for	O
D	O
except	O
for	O
the	O
input	O
of	O
G	O
and	O
the	O
last	O
layer	O
of	O
D.	O
We	O
use	O
the	O
single	B-Method
step	I-Method
gradient	I-Method
method	E-Method
as	O
in	O
[	O
reference	O
]	O
)	O
,	O
and	O
batch	B-Method
normalization	E-Method
[	O
reference	O
]	O
)	O
was	O
used	O
in	O
each	O
of	O
the	O
generator	B-Method
layers	E-Method
.	O

The	O
different	O
discriminators	O
were	O
trained	O
with	O
varying	O
dropout	B-Metric
rates	E-Metric
from	O
[	O
reference	O
]	O
.	O

Variations	O
in	O
the	O
discriminators	O
were	O
effected	O
in	O
two	O
ways	O
.	O

We	O
varied	O
the	O
architecture	O
by	O
varying	O
the	O
number	O
of	O
filters	O
in	O
the	O
discriminator	O
layers	O
(	O
reduced	O
by	O
factors	O
of	O
2	O
,	O
4	O
and	O
so	O
on	O
)	O
,	O
as	O
well	O
as	O
varying	O
dropout	B-Metric
rates	E-Metric
.	O

Secondly	O
we	O
also	O
decorrelated	O
the	O
samples	O
that	O
the	O
disriminators	O
were	O
training	O
on	O
by	O
splitting	O
the	O
minibatch	O
across	O
the	O
discriminators	O
.	O

The	O
code	O
was	O
written	O
in	O
Tensorflow	S-Method
[	O
reference	O
]	O
)	O
and	O
run	O
on	O
Nvidia	B-Method
GTX	I-Method
980	I-Method
GPUs	E-Method
.	O

Code	O
to	O
reproduce	O
experiments	O
and	O
plots	O
is	O
at	O
https:	O
//	O
github.com	O
/	O
iDurugkar	O
/	O
GMAN	S-Method
.	O

Specifics	O
for	O
the	O
MNIST	S-Material
architecture	O
and	O
training	S-Task
are	O
:	O
•	O
Generator	B-Method
latent	I-Method
variables	E-Method
z	O
∼	O
U	O
(	O
−1	O
,	O
1	O
)	O
section	O
:	O
100	O
•	O
Generator	B-Method
convolution	I-Method
transpose	I-Method
layers	E-Method
:	O
(	O
4	O
,	O
4	O
,	O
128	O
)	O
,	O
[	O
reference	O
]	O
,	O
(	O
32	O
,	O
32	O
,	O
1	O
)	O
•	O
Base	B-Method
Discriminator	I-Method
architecture	E-Method
:	O
(	O
32	O
,	O
32	O
,	O
1	O
)	O
,	O
[	O
reference	O
]	O
,	O
(	O
4	O
,	O
4	O
,	O
128	O
)	O
.	O

•	O
Variants	O
have	O
either	O
convolution	O
3	O
(	O
4	O
,	O
4	O
,	O
128	O
)	O
removed	O
or	O
all	O
the	O
filter	O
sizes	O
are	O
divided	O
by	O
2	O
or	O
4	O
.	O

That	O
is	O
,	O
(	O
32	O
,	O
32	O
,	O
1	O
)	O
,	O
[	O
reference	O
]	O
,	O
(	O
4	O
,	O
4	O
,	O
64	O
)	O
or	O
(	O
32	O
,	O
32	O
,	O
1	O
)	O
,	O
[	O
reference	O
]	O
.	O

•	O
ReLu	O
activations	O
for	O
all	O
the	O
hidden	O
units	O
.	O

Tanh	O
activation	O
at	O
the	O
output	O
units	O
of	O
the	O
generator	S-Method
.	O

Sigmoid	S-Method
at	O
the	O
output	O
of	O
the	O
Discriminator	S-Method
.	O

•	O
Training	O
was	O
performed	O
with	O
Adam	S-Method
[	O
reference	O
]	O
)	O
(	O
lr	O
=	O
2	O
×	O
10	O
−4	O
,	O
β	O
1	O
=	O
0.5	O
)	O
.	O

•	O
MNIST	S-Material
was	O
trained	O
for	O
20	O
epochs	O
with	O
a	O
minibatch	O
of	O
size	O
100	O
.	O

•	O
CelebA	S-Material
and	O
CIFAR	S-Material
were	O
trained	O
over	O
24000	O
iterations	O
with	O
a	O
minibatch	O
of	O
size	O
100	O
.	O

section	O
:	O
section	O
:	O
ACKNOWLEDGMENTS	O
We	O
acknowledge	O
helpful	O
conversations	O
with	O
Stefan	O
Dernbach	O
,	O
Archan	O
Ray	O
,	O
Luke	O
Vilnis	O
,	O
Ben	O
Turtel	O
,	O
Stephen	O
Giguere	O
,	O
Rajarshi	O
Das	O
,	O
and	O
Subhransu	O
Maji	O
.	O

We	O
also	O
thank	O
NVIDIA	O
for	O
donating	O
a	O
K40	O
GPU	O
.	O

This	O
material	O
is	O
based	O
upon	O
work	O
supported	O
by	O
the	O
National	O
Science	O
Foundation	O
under	O
Grant	O
Nos	O
.	O

IIS	O
-	O
1564032	O
.	O

Any	O
opinions	O
,	O
findings	O
,	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
views	O
of	O
the	O
NSF	O
.	O

section	O
:	O
document	O
:	O
Multimodal	B-Method
Residual	I-Method
Learning	E-Method
for	O
Visual	B-Task
QA	E-Task
Deep	B-Method
neural	I-Method
networks	E-Method
continue	O
to	O
advance	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
of	O
image	B-Task
recognition	I-Task
tasks	E-Task
with	O
various	O
methods	O
.	O

However	O
,	O
applications	O
of	O
these	O
methods	O
to	O
multimodality	S-Task
remain	O
limited	O
.	O

We	O
present	O
Multimodal	B-Method
Residual	I-Method
Networks	E-Method
(	O
MRN	B-Method
)	E-Method
for	O
the	O
multimodal	O
residual	O
learning	O
of	O
visual	O
question	O
-	O
answering	S-Task
,	O
which	O
extends	O
the	O
idea	O
of	O
the	O
deep	B-Method
residual	I-Method
learning	E-Method
.	O

Unlike	O
the	O
deep	B-Method
residual	I-Method
learning	E-Method
,	O
MRN	S-Method
effectively	O
learns	O
the	O
joint	B-Method
representation	E-Method
from	O
vision	O
and	O
language	O
information	O
.	O

The	O
main	O
idea	O
is	O
to	O
use	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
for	O
the	O
joint	O
residual	O
mappings	O
exploiting	O
the	O
residual	B-Method
learning	E-Method
of	O
the	O
attentional	B-Method
models	E-Method
in	O
recent	O
studies	O
.	O

Various	O
alternative	O
models	O
introduced	O
by	O
multimodality	O
are	O
explored	O
based	O
on	O
our	O
study	O
.	O

We	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
Visual	B-Material
QA	I-Material
dataset	E-Material
for	O
both	O
Open	B-Task
-	I-Task
Ended	E-Task
and	O
Multiple	B-Task
-	I-Task
Choice	I-Task
tasks	E-Task
.	O

Moreover	O
,	O
we	O
introduce	O
a	O
novel	O
method	O
to	O
visualize	O
the	O
attention	O
effect	O
of	O
the	O
joint	B-Method
representations	E-Method
for	O
each	O
learning	B-Method
block	E-Method
using	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
,	O
even	O
though	O
the	O
visual	O
features	O
are	O
collapsed	O
without	O
spatial	O
information	O
.	O

section	O
:	O
Introduction	O
Visual	O
question	O
-	O
answering	S-Task
tasks	O
provide	O
a	O
testbed	O
to	O
cultivate	O
the	O
synergistic	O
proposals	O
which	O
handle	O
multidisciplinary	B-Task
problems	I-Task
of	I-Task
vision	E-Task
,	O
language	S-Task
and	O
integrated	B-Task
reasoning	E-Task
.	O

So	O
,	O
the	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
let	O
the	O
studies	O
in	O
artificial	B-Task
intelligence	E-Task
go	O
beyond	O
narrow	B-Task
tasks	E-Task
.	O

Furthermore	O
,	O
it	O
may	O
help	O
to	O
solve	O
the	O
real	B-Task
world	I-Task
problems	E-Task
which	O
need	O
the	O
integrated	B-Task
reasoning	I-Task
of	I-Task
vision	E-Task
and	O
language	O
.	O

Deep	B-Method
residual	I-Method
learning	E-Method
not	O
only	O
advances	O
the	O
studies	O
in	O
object	B-Task
recognition	I-Task
problems	E-Task
,	O
but	O
also	O
gives	O
a	O
general	O
framework	O
for	O
deep	B-Method
neural	I-Method
networks	E-Method
.	O

The	O
existing	O
non	B-Method
-	I-Method
linear	I-Method
layers	I-Method
of	I-Method
neural	I-Method
networks	E-Method
serve	O
to	O
fit	O
another	O
mapping	O
of	O
,	O
which	O
is	O
the	O
residual	O
of	O
identity	O
mapping	O
.	O

So	O
,	O
with	O
the	O
shortcut	O
connection	O
of	O
identity	B-Method
mapping	E-Method
,	O
the	O
whole	O
module	O
of	O
layers	O
fit	O
for	O
the	O
desired	O
underlying	O
mapping	O
.	O

In	O
other	O
words	O
,	O
the	O
only	O
residual	O
mapping	O
,	O
defined	O
by	O
,	O
is	O
learned	O
with	O
non	O
-	O
linear	O
layers	O
.	O

In	O
this	O
way	O
,	O
very	O
deep	B-Method
neural	I-Method
networks	E-Method
effectively	O
learn	O
representations	O
in	O
an	O
efficient	O
manner	O
.	O

Many	O
attentional	B-Method
models	E-Method
utilize	O
the	O
residual	B-Method
learning	E-Method
to	O
deal	O
with	O
various	O
tasks	O
,	O
including	O
textual	B-Task
reasoning	E-Task
and	O
visual	O
question	O
-	O
answering	S-Task
.	O

They	O
use	O
an	O
attentional	B-Method
mechanism	E-Method
to	O
handle	O
two	O
different	O
information	O
sources	O
,	O
a	O
query	O
and	O
the	O
context	O
of	O
the	O
query	O
(	O
e.g	O
.	O

contextual	O
sentences	O
or	O
an	O
image	O
)	O
.	O

The	O
query	O
is	O
added	O
to	O
the	O
output	O
of	O
the	O
attentional	B-Method
module	E-Method
,	O
that	O
makes	O
the	O
attentional	B-Method
module	E-Method
learn	O
the	O
residual	B-Method
of	I-Method
query	I-Method
mapping	E-Method
as	O
in	O
deep	B-Method
residual	I-Method
learning	E-Method
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
Multimodal	B-Method
Residual	I-Method
Networks	E-Method
(	O
MRN	B-Method
)	E-Method
to	O
learn	O
multimodality	O
of	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
exploiting	O
the	O
excellence	O
of	O
deep	B-Method
residual	I-Method
learning	E-Method
.	O

MRN	S-Method
inherently	O
uses	O
shortcuts	S-Method
and	O
residual	B-Method
mappings	E-Method
for	O
multimodality	S-Task
.	O

We	O
explore	O
various	O
models	O
upon	O
the	O
choice	O
of	O
the	O
shortcuts	O
for	O
each	O
modality	O
,	O
and	O
the	O
joint	O
residual	O
mappings	O
based	O
on	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
,	O
which	O
effectively	O
learn	O
the	O
multimodal	B-Method
representations	E-Method
not	O
using	O
explicit	O
attention	O
parameters	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
inference	O
flow	O
of	O
the	O
proposed	O
MRN	S-Method
.	O

Additionally	O
,	O
we	O
propose	O
a	O
novel	O
method	O
to	O
visualize	O
the	O
attention	O
effects	O
of	O
each	O
joint	B-Method
residual	I-Method
mapping	E-Method
.	O

The	O
visualization	B-Method
method	E-Method
uses	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
for	O
the	O
difference	O
between	O
the	O
visual	O
input	O
and	O
the	O
output	O
of	O
the	O
joint	B-Method
residual	I-Method
mapping	E-Method
.	O

The	O
difference	O
is	O
back	O
-	O
propagated	O
up	O
to	O
an	O
input	O
image	O
.	O

Since	O
we	O
use	O
the	O
pretrained	O
visual	O
features	O
,	O
the	O
pretrained	B-Method
CNN	E-Method
is	O
augmented	O
for	O
visualization	S-Task
.	O

Based	O
on	O
this	O
,	O
we	O
argue	O
that	O
MRN	S-Method
is	O
an	O
implicit	B-Method
attention	I-Method
model	E-Method
without	O
explicit	O
attention	O
parameters	O
.	O

Our	O
contribution	O
is	O
three	O
-	O
fold	O
:	O
1	O
)	O
extending	O
the	O
deep	B-Method
residual	I-Method
learning	E-Method
for	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
.	O

This	O
method	O
utilizes	O
multimodal	O
inputs	O
,	O
and	O
allows	O
a	O
deeper	O
network	O
structure	O
,	O
2	O
)	O
achieving	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
Visual	B-Material
QA	I-Material
dataset	E-Material
for	O
both	O
Open	B-Task
-	I-Task
Ended	E-Task
and	O
Multiple	B-Task
-	I-Task
Choice	I-Task
tasks	E-Task
,	O
and	O
finally	O
,	O
3	O
)	O
introducing	O
a	O
novel	O
method	O
to	O
visualize	O
spatial	O
attention	O
effect	O
of	O
joint	O
residual	O
mappings	O
from	O
the	O
collapsed	O
visual	O
feature	O
using	O
back	B-Method
-	I-Method
propagation	E-Method
.	O

section	O
:	O
Related	O
Works	O
subsection	O
:	O
Deep	B-Method
Residual	I-Method
Learning	I-Method
Deep	I-Method
residual	I-Method
learning	E-Method
allows	O
neural	B-Method
networks	E-Method
to	O
have	O
a	O
deeper	O
structure	O
of	O
over	O
-	O
100	O
layers	O
.	O

The	O
very	O
deep	B-Method
neural	I-Method
networks	E-Method
are	O
usually	O
hard	O
to	O
be	O
optimized	O
even	O
though	O
the	O
well	O
-	O
known	O
activation	B-Method
functions	E-Method
and	O
regularization	B-Method
techniques	E-Method
are	O
applied	O
.	O

This	O
method	O
consistently	O
shows	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
multiple	O
visual	B-Task
tasks	E-Task
including	O
image	B-Task
classification	E-Task
,	O
object	B-Task
detection	E-Task
,	O
localization	S-Task
and	O
segmentation	S-Task
.	O

This	O
idea	O
assumes	O
that	O
a	O
block	B-Method
of	I-Method
deep	I-Method
neural	I-Method
networks	E-Method
forming	O
a	O
non	B-Method
-	I-Method
linear	I-Method
mapping	E-Method
may	O
paradoxically	O
fail	O
to	O
fit	O
into	O
an	O
identity	B-Method
mapping	E-Method
.	O

To	O
resolve	O
this	O
,	O
the	O
deep	B-Method
residual	I-Method
learning	E-Method
adds	O
to	O
as	O
a	O
shortcut	O
connection	O
.	O

With	O
this	O
idea	O
,	O
the	O
non	B-Method
-	I-Method
linear	I-Method
mapping	E-Method
can	O
focus	O
on	O
the	O
residual	O
of	O
the	O
shortcut	B-Method
mapping	E-Method
.	O

Therefore	O
,	O
a	O
learning	B-Method
block	E-Method
is	O
defined	O
as	O
:	O
where	O
and	O
are	O
the	O
input	O
and	O
output	O
of	O
the	O
learning	B-Method
block	E-Method
,	O
respectively	O
.	O

subsection	O
:	O
Stacked	B-Method
Attention	I-Method
Networks	I-Method
Stacked	I-Method
Attention	I-Method
Networks	E-Method
(	O
SAN	S-Task
)	O
explicitly	O
learns	O
the	O
weights	O
of	O
visual	O
feature	O
vectors	O
to	O
select	O
a	O
small	O
portion	O
of	O
visual	O
information	O
for	O
a	O
given	O
question	O
vector	O
.	O

Furthermore	O
,	O
this	O
model	O
stacks	O
the	O
attention	B-Method
networks	E-Method
for	O
multi	B-Task
-	I-Task
step	I-Task
reasoning	E-Task
narrowing	O
down	O
the	O
selection	B-Task
of	I-Task
visual	I-Task
information	E-Task
.	O

For	O
example	O
,	O
if	O
the	O
attention	B-Method
networks	E-Method
are	O
asked	O
to	O
find	O
a	O
pink	O
handbag	O
in	O
a	O
scene	O
,	O
they	O
try	O
to	O
find	O
pink	O
objects	O
first	O
,	O
and	O
then	O
,	O
narrow	O
down	O
to	O
the	O
pink	O
handbag	O
.	O

For	O
the	O
attention	B-Method
networks	E-Method
,	O
the	O
weights	O
are	O
learned	O
by	O
a	O
question	O
vector	O
and	O
the	O
corresponding	O
visual	O
feature	O
vectors	O
.	O

These	O
weights	O
are	O
used	O
for	O
the	O
linear	B-Task
combination	I-Task
of	I-Task
multiple	I-Task
visual	I-Task
feature	I-Task
vectors	E-Task
indexing	O
spatial	O
information	O
.	O

Through	O
this	O
,	O
SAN	S-Task
successfully	O
selects	O
a	O
portion	O
of	O
visual	O
information	O
.	O

Finally	O
,	O
an	O
addition	O
of	O
the	O
combined	O
visual	O
feature	O
vector	O
and	O
the	O
previous	O
question	O
vector	O
is	O
transferred	O
as	O
a	O
new	O
input	O
question	O
vector	O
to	O
next	O
learning	O
block	O
.	O

Here	O
,	O
is	O
a	O
question	O
vector	O
for	O
-	O
th	O
learning	O
block	O
and	O
is	O
a	O
visual	O
feature	O
matrix	O
,	O
whose	O
columns	O
indicate	O
the	O
specific	O
spatial	O
indexes	O
.	O

is	O
the	O
attention	O
networks	O
of	O
SAN	S-Task
.	O

section	O
:	O
Multimodal	B-Method
Residual	I-Method
Networks	I-Method
Deep	I-Method
residual	I-Method
learning	E-Method
emphasizes	O
the	O
importance	O
of	O
identity	O
(	O
or	O
linear	O
)	O
shortcuts	O
to	O
have	O
the	O
non	O
-	O
linear	O
mappings	O
efficiently	O
learn	O
only	O
residuals	O
.	O

In	O
multimodal	B-Task
learning	E-Task
,	O
this	O
idea	O
may	O
not	O
be	O
readily	O
applied	O
.	O

Since	O
the	O
modalities	O
may	O
have	O
correlations	O
,	O
we	O
need	O
to	O
carefully	O
define	O
joint	O
residual	O
functions	O
as	O
the	O
non	O
-	O
linear	O
mappings	O
.	O

Moreover	O
,	O
the	O
shortcuts	O
are	O
undetermined	O
due	O
to	O
its	O
multimodality	O
.	O

Therefore	O
,	O
the	O
characteristics	O
of	O
a	O
given	O
task	O
ought	O
to	O
be	O
considered	O
to	O
determine	O
the	O
model	O
structure	O
.	O

subsection	O
:	O
Background	O
We	O
infer	O
a	O
residual	B-Method
learning	E-Method
in	O
the	O
attention	O
networks	O
of	O
SAN	S-Task
.	O

Since	O
Equation	O
18	O
in	O
shows	O
a	O
question	O
vector	O
transferred	O
directly	O
through	O
successive	O
layers	O
of	O
the	O
attention	B-Method
networks	E-Method
.	O

In	O
the	O
case	O
of	O
SAN	S-Task
,	O
the	O
shortcut	B-Method
mapping	E-Method
is	O
for	O
the	O
question	O
vector	O
,	O
and	O
the	O
non	B-Method
-	I-Method
linear	I-Method
mapping	E-Method
is	O
the	O
attention	B-Method
networks	E-Method
.	O

In	O
the	O
attention	B-Method
networks	E-Method
,	O
Yang2015	O
assume	O
that	O
an	O
appropriate	O
choice	O
of	O
weights	O
on	O
visual	O
feature	O
vectors	O
for	O
a	O
given	O
question	O
vector	O
sufficiently	O
captures	O
the	O
joint	B-Method
representation	E-Method
for	O
answering	S-Task
.	O

However	O
,	O
question	O
information	O
weakly	O
contributes	O
to	O
the	O
joint	B-Method
representation	E-Method
only	O
through	O
coefficients	O
,	O
which	O
may	O
cause	O
a	O
bottleneck	O
to	O
learn	O
the	O
joint	B-Method
representation	E-Method
.	O

The	O
coefficients	O
are	O
the	O
output	O
of	O
a	O
nonlinear	B-Method
function	E-Method
of	O
a	O
question	O
vector	O
and	O
a	O
visual	O
feature	O
matrix	O
(	O
see	O
Equation	O
15	O
-	O
16	O
in	O
Yang2015	O
)	O
.	O

The	O
is	O
a	O
visual	O
feature	O
vector	O
of	O
spatial	O
index	O
in	O
grids	O
.	O

Lu2015	S-Method
propose	O
an	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
of	O
a	O
question	O
vector	O
and	O
a	O
visual	O
feature	O
vector	O
after	O
appropriate	O
embeddings	O
for	O
a	O
joint	B-Method
model	E-Method
.	O

This	O
makes	O
a	O
strong	O
baseline	O
outperforming	O
some	O
of	O
the	O
recent	O
works	O
.	O

We	O
firstly	O
take	O
this	O
approach	O
as	O
a	O
candidate	O
for	O
the	O
joint	O
residual	O
function	O
,	O
since	O
it	O
is	O
simple	O
yet	O
successful	O
for	O
visual	O
question	O
-	O
answering	S-Task
.	O

In	O
this	O
context	O
,	O
we	O
take	O
the	O
global	B-Method
visual	I-Method
feature	I-Method
approach	E-Method
for	O
the	O
element	B-Task
-	I-Task
wise	I-Task
multiplication	E-Task
,	O
instead	O
of	O
the	O
multiple	B-Method
(	I-Method
spatial	I-Method
)	I-Method
visual	I-Method
features	I-Method
approach	E-Method
for	O
the	O
explicit	O
attention	O
mechanism	O
of	O
SAN	S-Task
.	O

(	O
We	O
present	O
a	O
visualization	B-Method
technique	E-Method
exploiting	O
the	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
in	O
Section	O
[	O
reference	O
]	O
.	O

)	O
Based	O
on	O
these	O
observations	O
,	O
we	O
follow	O
the	O
shortcut	B-Method
mapping	E-Method
and	O
the	O
stacking	B-Method
architecture	E-Method
of	O
SAN	S-Task
;	O
however	O
,	O
the	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
is	O
used	O
for	O
the	O
joint	O
residual	O
function	O
.	O

These	O
updates	O
effectively	O
learn	O
the	O
joint	B-Task
representation	I-Task
of	I-Task
given	I-Task
vision	I-Task
and	I-Task
language	I-Task
information	E-Task
addressing	O
the	O
bottleneck	O
issue	O
of	O
the	O
attention	O
networks	O
of	O
SAN	S-Task
.	O

subsection	O
:	O
Multimodal	B-Method
Residual	I-Method
Networks	I-Method
MRN	E-Method
consists	O
of	O
multiple	O
learning	B-Method
blocks	E-Method
,	O
which	O
are	O
stacked	O
for	O
deep	B-Task
residual	I-Task
learning	E-Task
.	O

Denoting	O
an	O
optimal	O
mapping	O
by	O
,	O
we	O
approximate	O
it	O
using	O
The	O
first	O
(	O
linear	O
)	O
approximation	B-Method
term	E-Method
is	O
and	O
the	O
first	O
joint	O
residual	O
function	O
is	O
given	O
by	O
.	O

The	O
linear	B-Method
mapping	E-Method
is	O
used	O
for	O
matching	O
a	O
feature	O
dimension	O
.	O

We	O
define	O
the	O
joint	O
residual	O
function	O
as	O
where	O
is	O
,	O
and	O
is	O
element	O
-	O
wise	O
multiplication	O
.	O

The	O
question	O
vector	O
and	O
the	O
visual	O
feature	O
vector	O
directly	O
contribute	O
to	O
the	O
joint	B-Method
representation	E-Method
.	O

We	O
justify	O
this	O
choice	O
in	O
Sections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

For	O
a	O
deeper	B-Task
residual	I-Task
learning	E-Task
,	O
we	O
replace	O
with	O
in	O
the	O
next	O
layer	O
.	O

In	O
more	O
general	O
terms	O
,	O
Equations	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
can	O
be	O
rewritten	O
as	O
where	O
is	O
the	O
number	O
of	O
learning	O
blocks	O
,	O
,	O
,	O
and	O
.	O

The	O
cascading	O
in	O
Equation	O
[	O
reference	O
]	O
can	O
intuitively	O
be	O
represented	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Notice	O
that	O
the	O
shortcuts	O
for	O
a	O
visual	O
part	O
are	O
identity	O
mappings	O
to	O
transfer	O
the	O
input	O
visual	O
feature	O
vector	O
to	O
each	O
layer	O
(	O
dashed	O
line	O
)	O
.	O

At	O
the	O
end	O
of	O
each	O
block	O
,	O
we	O
denote	O
as	O
the	O
output	O
of	O
the	O
-	O
th	O
learning	O
block	O
,	O
and	O
is	O
element	O
-	O
wise	O
addition	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Visual	B-Material
QA	I-Material
Dataset	E-Material
We	O
choose	O
the	O
Visual	B-Task
QA	E-Task
(	O
VQA	S-Material
)	O
dataset	O
for	O
the	O
evaluation	O
of	O
our	O
models	O
.	O

Other	O
datasets	O
may	O
not	O
be	O
ideal	O
,	O
since	O
they	O
have	O
limited	O
number	O
of	O
examples	O
to	O
train	O
and	O
test	O
,	O
or	O
have	O
synthesized	O
questions	O
from	O
the	O
image	O
captions	O
.	O

The	O
questions	O
and	O
answers	O
of	O
the	O
VQA	S-Material
dataset	O
are	O
collected	O
via	O
Amazon	B-Material
Mechanical	I-Material
Turk	E-Material
from	O
human	O
subjects	O
,	O
who	O
satisfy	O
the	O
experimental	O
requirement	O
.	O

The	O
dataset	O
includes	O
614	O
,	O
163	O
questions	O
and	O
7	O
,	O
984	O
,	O
119	O
answers	O
,	O
since	O
ten	O
answers	O
are	O
gathered	O
for	O
each	O
question	O
from	O
unique	O
human	O
subjects	O
.	O

Therefore	O
,	O
Antol2015	O
proposed	O
a	O
new	O
accuracy	B-Metric
metric	E-Metric
as	O
follows	O
:	O
The	O
questions	O
are	O
answered	O
in	O
two	O
ways	O
:	O
Open	B-Task
-	I-Task
Ended	E-Task
and	O
Multiple	O
-	O
Choice	O
.	O

Unlike	O
Open	B-Task
-	I-Task
Ended	E-Task
,	O
Multiple	O
-	O
Choice	O
allows	O
additional	O
information	O
of	O
eighteen	O
candidate	O
answers	O
for	O
each	O
question	O
.	O

There	O
are	O
three	O
types	O
of	O
answers	O
:	O
yes	O
/	O
no	O
(	O
Y	O
/	O
N	O
)	O
,	O
numbers	O
(	O
Num	O
.	O

)	O
and	O
others	O
(	O
Other	O
)	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
Other	O
type	O
has	O
the	O
most	O
benefit	O
from	O
Multiple	O
-	O
Choice	O
.	O

The	O
images	O
come	O
from	O
the	O
MS	B-Material
-	I-Material
COCO	I-Material
dataset	E-Material
,	O
123	O
,	O
287	O
of	O
them	O
for	O
training	O
and	O
validation	S-Metric
,	O
and	O
81	O
,	O
434	O
for	O
test	O
.	O

The	O
images	O
are	O
carefully	O
collected	O
to	O
contain	O
multiple	O
objects	O
and	O
natural	O
situations	O
,	O
which	O
is	O
also	O
valid	O
for	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
.	O

subsection	O
:	O
Implementation	O
Torch	B-Method
framework	E-Method
and	O
rnn	B-Method
package	E-Method
are	O
used	O
to	O
build	O
our	O
models	O
.	O

For	O
efficient	O
computation	B-Task
of	I-Task
variable	I-Task
-	I-Task
length	I-Task
questions	E-Task
,	O
TrimZero	S-Method
is	O
used	O
to	O
trim	O
out	O
zero	O
vectors	O
.	O

TrimZero	S-Method
eliminates	O
zero	O
computations	O
at	O
every	O
time	O
-	O
step	O
in	O
mini	B-Method
-	I-Method
batch	I-Method
learning	E-Method
.	O

Its	O
efficiency	O
is	O
affected	O
by	O
a	O
batch	B-Metric
size	E-Metric
,	O
RNN	B-Method
model	I-Method
size	E-Method
,	O
and	O
the	O
number	O
of	O
zeros	O
in	O
inputs	O
.	O

We	O
found	O
out	O
that	O
TrimZero	S-Method
was	O
suitable	O
for	O
VQA	S-Material
tasks	O
.	O

Approximately	O
,	O
37.5	O
%	O
of	O
training	B-Metric
time	E-Metric
is	O
reduced	O
in	O
our	O
experiments	O
using	O
this	O
technique	O
.	O

paragraph	O
:	O
Preprocessing	O
We	O
follow	O
the	O
same	O
preprocessing	B-Method
procedure	E-Method
of	O
DeeperLSTM	S-Method
+	O
NormalizedCNN	S-Method
(	O
Deep	B-Method
Q	I-Method
+	I-Method
I	E-Method
)	O
by	O
default	O
.	O

The	O
number	O
of	O
answers	O
is	O
1k	O
,	O
2k	O
,	O
or	O
3k	O
using	O
the	O
most	O
frequent	O
answers	O
,	O
which	O
covers	O
86.52	O
%	O
,	O
90.45	O
%	O
and	O
92.42	O
%	O
of	O
questions	O
,	O
respectively	O
.	O

The	O
questions	O
are	O
tokenized	O
using	O
Python	B-Method
Natural	I-Method
Language	I-Method
Toolkit	E-Method
(	O
nltk	S-Method
)	O
.	O

Subsequently	O
,	O
the	O
vocabulary	O
sizes	O
are	O
14	O
,	O
770	O
,	O
15	O
,	O
031	O
and	O
15	O
,	O
169	O
,	O
respectively	O
.	O

paragraph	O
:	O
Pretrained	B-Method
Models	E-Method
A	O
question	O
vector	O
is	O
the	O
last	O
output	O
vector	O
of	O
GRU	S-Method
,	O
initialized	O
with	O
the	O
parameters	O
of	O
Skip	O
-	O
Thought	O
Vectors	O
.	O

Based	O
on	O
the	O
study	O
of	O
Noh2015	O
,	O
this	O
method	O
shows	O
effectiveness	O
of	O
question	B-Task
embedding	E-Task
in	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
.	O

A	O
visual	O
feature	O
vector	O
is	O
an	O
output	O
of	O
the	O
first	O
fully	B-Method
-	I-Method
connected	I-Method
layer	I-Method
of	I-Method
VGG	I-Method
-	I-Method
19	I-Method
networks	E-Method
,	O
whose	O
dimension	O
is	O
4	O
,	O
096	O
.	O

Alternatively	O
,	O
ResNet	B-Method
-	I-Method
152	E-Method
is	O
used	O
,	O
whose	O
dimension	O
is	O
of	O
2	O
,	O
048	O
.	O

The	O
error	O
is	O
back	O
-	O
propagated	O
to	O
the	O
input	O
question	O
for	O
fine	B-Task
-	I-Task
tuning	E-Task
,	O
yet	O
,	O
not	O
for	O
the	O
visual	B-Task
part	E-Task
due	O
to	O
the	O
heavy	O
computational	B-Metric
cost	E-Metric
of	O
training	S-Task
.	O

paragraph	O
:	O
Postprocessing	O
Image	B-Method
captioning	I-Method
model	E-Method
is	O
used	O
to	O
improve	O
the	O
accuracy	S-Metric
of	O
Other	O
type	O
.	O

Let	O
the	O
intermediate	B-Method
representation	E-Method
which	O
is	O
right	O
before	O
applying	O
softmax	S-Method
.	O

is	O
the	O
vocabulary	O
size	O
of	O
answers	O
,	O
and	O
is	O
corresponding	O
to	O
answer	O
.	O

If	O
is	O
not	O
a	O
number	O
or	O
yes	O
or	O
no	O
,	O
and	O
appeared	O
at	O
least	O
once	O
in	O
the	O
generated	O
caption	O
,	O
then	O
update	O
.	O

Notice	O
that	O
the	O
pretrained	B-Method
image	I-Method
captioning	I-Method
model	E-Method
is	O
not	O
part	O
of	O
training	O
.	O

This	O
simple	O
procedure	O
improves	O
around	O
of	O
the	O
test	O
-	O
dev	B-Metric
overall	I-Metric
accuracy	E-Metric
(	O
for	O
Other	O
type	O
)	O
.	O

We	O
attribute	O
this	O
improvement	O
to	O
‘	O
‘	O
tie	O
break	O
’	O
’	O
in	O
Other	O
type	O
.	O

For	O
the	O
Multiple	B-Task
-	I-Task
Choice	I-Task
task	E-Task
,	O
we	O
mask	O
the	O
output	O
of	O
softmax	B-Method
layer	E-Method
with	O
the	O
given	O
candidate	O
answers	O
.	O

paragraph	O
:	O
Hyperparameters	O
By	O
default	O
,	O
we	O
follow	O
Deep	B-Method
Q	I-Method
+	I-Method
I	E-Method
.	O

The	O
common	O
embedding	B-Metric
size	E-Metric
of	O
the	O
joint	B-Method
representation	E-Method
is	O
1	O
,	O
200	O
.	O

The	O
learnable	O
parameters	O
are	O
initialized	O
using	O
a	O
uniform	O
distribution	O
from	O
to	O
except	O
for	O
the	O
pretrained	B-Method
models	E-Method
.	O

The	O
batch	O
size	O
is	O
200	O
,	O
and	O
the	O
number	O
of	O
iterations	O
is	O
fixed	O
to	O
250k	O
.	O

The	O
RMSProp	S-Method
is	O
used	O
for	O
optimization	S-Task
,	O
and	O
dropouts	S-Method
are	O
used	O
for	O
regularization	S-Task
.	O

The	O
hyperparameters	O
are	O
fixed	O
using	O
test	O
-	O
dev	O
results	O
.	O

We	O
compare	O
our	O
method	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
using	O
test	O
-	O
standard	O
results	O
.	O

subsection	O
:	O
Exploring	O
Alternative	O
Models	O
Figure	O
[	O
reference	O
]	O
shows	O
alternative	O
models	O
we	O
explored	O
,	O
based	O
on	O
the	O
observations	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
carefully	O
select	O
alternative	O
models	O
(	O
a	O
)-(	O
c	O
)	O
for	O
the	O
importance	O
of	O
embeddings	S-Task
in	O
multimodal	B-Task
learning	E-Task
,	O
(	O
d	O
)	O
for	O
the	O
effectiveness	O
of	O
identity	B-Task
mapping	E-Task
as	O
reported	O
by	O
,	O
and	O
(	O
e	O
)	O
for	O
the	O
confirmation	O
of	O
using	O
question	O
-	O
only	O
shortcuts	O
in	O
the	O
multiple	O
blocks	O
as	O
in	O
.	O

For	O
comparison	O
,	O
all	O
models	O
have	O
three	O
-	O
block	O
layers	O
(	O
selected	O
after	O
a	O
pilot	O
test	O
)	O
,	O
using	O
VGG	O
-	O
19	O
features	O
and	O
1k	O
answers	O
,	O
then	O
,	O
the	O
number	O
of	O
learning	O
blocks	O
is	O
explored	O
to	O
confirm	O
the	O
pilot	O
test	O
.	O

The	O
effect	O
of	O
the	O
pretrained	B-Method
visual	I-Method
feature	I-Method
models	E-Method
and	O
the	O
number	O
of	O
answers	O
are	O
also	O
explored	O
.	O

All	O
validation	O
is	O
performed	O
on	O
the	O
test	O
-	O
dev	O
split	O
.	O

section	O
:	O
Results	O
subsection	O
:	O
Quantitative	B-Task
Analysis	E-Task
The	O
VQA	S-Material
Challenge	O
,	O
which	O
released	O
the	O
VQA	S-Material
dataset	O
,	O
provides	O
evaluation	B-Metric
servers	E-Metric
for	O
test	O
-	O
dev	O
and	O
test	B-Metric
-	I-Metric
standard	I-Metric
test	I-Metric
splits	E-Metric
.	O

For	O
the	O
test	O
-	O
dev	O
,	O
the	O
evaluation	O
server	O
permits	O
unlimited	O
submissions	O
for	O
validation	S-Task
,	O
while	O
the	O
test	O
-	O
standard	O
permits	O
limited	O
submissions	O
for	O
the	O
competition	O
.	O

We	O
report	O
accuracies	S-Metric
in	O
percentage	O
.	O

paragraph	O
:	O
Alternative	O
Models	O
The	O
test	O
-	O
dev	O
results	O
of	O
the	O
alternative	O
models	O
for	O
the	O
Open	B-Task
-	I-Task
Ended	E-Task
task	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

(	O
a	O
)	O
shows	O
a	O
significant	O
improvement	O
over	O
SAN	S-Task
.	O

However	O
,	O
(	O
b	O
)	O
is	O
marginally	O
better	O
than	O
(	O
a	O
)	O
.	O

As	O
compared	O
to	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
deteriorates	O
the	O
performance	O
.	O

An	O
extra	O
embedding	O
for	O
a	O
question	O
vector	O
may	O
easily	O
cause	O
overfitting	O
leading	O
to	O
the	O
overall	O
degradation	O
.	O

And	O
,	O
the	O
identity	O
shortcuts	O
in	O
(	O
d	O
)	O
cause	O
the	O
degradation	B-Task
problem	E-Task
,	O
too	O
.	O

Extra	O
parameters	O
of	O
the	O
linear	B-Method
mappings	E-Method
may	O
effectively	O
support	O
to	O
do	O
the	O
task	O
.	O

(	O
e	O
)	O
shows	O
a	O
reasonable	O
performance	O
,	O
however	O
,	O
the	O
extra	O
shortcut	O
is	O
not	O
essential	O
.	O

The	O
empirical	O
results	O
seem	O
to	O
support	O
this	O
idea	O
.	O

Since	O
the	O
question	B-Method
-	I-Method
only	I-Method
model	E-Method
(	O
50.39	O
%	O
)	O
achieves	O
a	O
competitive	O
result	O
to	O
the	O
joint	B-Method
model	E-Method
(	O
57.75	O
%	O
)	O
,	O
while	O
the	O
image	B-Method
-	I-Method
only	I-Method
model	E-Method
gets	O
a	O
poor	O
accuracy	S-Metric
(	O
28.13	O
%	O
)	O
(	O
see	O
Table	O
2	O
in	O
)	O
.	O

Eventually	O
,	O
we	O
chose	O
model	O
(	O
b	O
)	O
as	O
the	O
best	O
performance	O
and	O
relative	O
simplicity	O
.	O

The	O
effects	O
of	O
other	O
various	O
options	O
,	O
Skip	O
-	O
Thought	O
Vectors	O
for	O
parameter	B-Task
initialization	E-Task
,	O
Bayesian	B-Method
Dropout	E-Method
for	O
regularization	S-Method
,	O
image	B-Method
captioning	I-Method
model	E-Method
for	O
postprocessing	S-Task
,	O
and	O
the	O
usage	O
of	O
shortcut	O
connections	O
,	O
are	O
explored	O
in	O
Appendix	O
A.1	O
.	O

paragraph	O
:	O
Number	O
of	O
Learning	O
Blocks	O
To	O
confirm	O
the	O
effectiveness	O
of	O
the	O
number	O
of	O
learning	O
blocks	O
selected	O
via	O
a	O
pilot	O
test	O
(	O
)	O
,	O
we	O
explore	O
this	O
on	O
the	O
chosen	O
model	O
(	O
b	O
)	O
,	O
again	O
.	O

As	O
the	O
depth	O
increases	O
,	O
the	O
overall	O
accuracies	S-Metric
are	O
58.85	O
%	O
(	O
)	O
,	O
59.44	O
%	O
(	O
)	O
,	O
60.53	O
%	O
(	O
)	O
and	O
60.42	O
%	O
(	O
)	O
.	O

paragraph	O
:	O
Visual	O
Features	O
The	O
ResNet	O
-	O
152	O
visual	O
features	O
are	O
significantly	O
better	O
than	O
VGG	O
-	O
19	O
features	O
for	O
Other	O
type	O
in	O
Table	O
[	O
reference	O
]	O
,	O
even	O
if	O
the	O
dimension	O
of	O
the	O
ResNet	O
features	O
(	O
2	O
,	O
048	O
)	O
is	O
a	O
half	O
of	O
VGG	O
features	O
’	O
(	O
4	O
,	O
096	O
)	O
.	O

The	O
ResNet	O
visual	O
features	O
are	O
also	O
used	O
in	O
the	O
previous	O
work	O
;	O
however	O
,	O
our	O
model	O
achieves	O
a	O
remarkably	O
better	O
performance	O
with	O
a	O
large	O
margin	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

paragraph	O
:	O
Number	O
of	O
Target	O
Answers	O
The	O
number	O
of	O
target	O
answers	O
slightly	O
affects	O
the	O
overall	O
accuracies	S-Metric
with	O
the	O
trade	O
-	O
off	O
among	O
answer	O
types	O
.	O

So	O
,	O
the	O
decision	O
on	O
the	O
number	O
of	O
target	O
answers	O
is	O
difficult	O
to	O
be	O
made	O
.	O

We	O
chose	O
Res	O
,	O
2k	O
in	O
Table	O
[	O
reference	O
]	O
based	O
on	O
the	O
overall	O
accuracy	S-Metric
(	O
for	O
Multiple	B-Task
-	I-Task
Choice	I-Task
task	E-Task
,	O
see	O
Appendix	O
A.1	O
)	O
.	O

paragraph	O
:	O
Comparisons	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
arts	O
Our	O
chosen	O
model	O
significantly	O
outperforms	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
both	O
Open	B-Task
-	I-Task
Ended	E-Task
and	O
Multiple	B-Task
-	I-Task
Choice	I-Task
tasks	E-Task
in	O
Table	O
[	O
reference	O
]	O
.	O

However	O
,	O
the	O
performance	O
of	O
Number	O
and	O
Other	O
types	O
are	O
still	O
not	O
satisfactory	O
compared	O
to	O
Human	O
performance	O
,	O
though	O
the	O
advances	O
in	O
the	O
recent	O
works	O
were	O
mainly	O
for	O
Other	O
-	O
type	O
answers	O
.	O

This	O
fact	O
motivates	O
to	O
study	O
on	O
a	O
counting	B-Method
mechanism	E-Method
in	O
future	O
work	O
.	O

The	O
model	O
comparison	O
is	O
performed	O
on	O
the	O
test	O
-	O
standard	O
results	O
.	O

subsection	O
:	O
Qualitative	B-Method
Analysis	E-Method
In	O
Equation	O
[	O
reference	O
]	O
,	O
the	O
left	O
term	O
can	O
be	O
seen	O
as	O
a	O
masking	O
(	O
attention	O
)	O
vector	O
to	O
select	O
a	O
part	O
of	O
visual	O
information	O
.	O

We	O
assume	O
that	O
the	O
difference	O
between	O
the	O
right	O
term	O
and	O
the	O
masked	O
vector	O
indicates	O
an	O
attention	O
effect	O
caused	O
by	O
the	O
masking	O
vector	O
.	O

Then	O
,	O
the	O
attention	O
effect	O
is	O
visualized	O
on	O
the	O
image	O
by	O
calculating	O
the	O
gradient	O
of	O
with	O
respect	O
to	O
a	O
given	O
image	O
,	O
while	O
treating	O
as	O
a	O
constant	O
.	O

This	O
technique	O
can	O
be	O
applied	O
to	O
each	O
learning	O
block	O
in	O
a	O
similar	O
way	O
.	O

Since	O
we	O
use	O
the	O
preprocessed	O
visual	O
features	O
,	O
the	O
pretrained	B-Method
CNN	E-Method
is	O
augmented	O
only	O
for	O
this	O
visualization	S-Task
.	O

Note	O
that	O
model	O
(	O
b	O
)	O
in	O
Table	O
[	O
reference	O
]	O
is	O
used	O
for	O
this	O
visualization	S-Task
,	O
and	O
the	O
pretrained	B-Method
VGG	I-Method
-	I-Method
19	E-Method
is	O
used	O
for	O
preprocessing	B-Task
and	I-Task
augmentation	E-Task
.	O

The	O
model	O
is	O
trained	O
using	O
the	O
training	O
set	O
of	O
the	O
VQA	S-Material
dataset	O
,	O
and	O
visualized	O
using	O
the	O
validation	O
set	O
.	O

Examples	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
more	O
examples	O
in	O
Appendix	O
A.2	O
-	O
4	O
)	O
.	O

Unlike	O
the	O
other	O
works	O
that	O
use	O
explicit	O
attention	O
parameters	O
,	O
MRN	S-Method
does	O
not	O
use	O
any	O
explicit	B-Method
attentional	I-Method
mechanism	E-Method
.	O

However	O
,	O
we	O
observe	O
the	O
interpretability	O
of	O
element	B-Method
-	I-Method
wise	I-Method
multiplication	E-Method
as	O
an	O
information	B-Task
masking	E-Task
,	O
which	O
yields	O
a	O
novel	O
method	O
for	O
visualizing	O
the	O
attention	O
effect	O
from	O
this	O
operation	O
.	O

Since	O
MRN	S-Method
does	O
not	O
depend	O
on	O
a	O
few	O
attention	O
parameters	O
(	O
e.g	O
.	O

)	O
,	O
our	O
visualization	B-Method
method	E-Method
shows	O
a	O
higher	O
resolution	O
than	O
others	O
.	O

Based	O
on	O
this	O
,	O
we	O
argue	O
that	O
MRN	S-Method
is	O
an	O
implicit	B-Method
attention	I-Method
model	E-Method
without	O
explicit	B-Method
attention	I-Method
mechanism	E-Method
.	O

section	O
:	O
Conclusions	O
The	O
idea	O
of	O
deep	B-Method
residual	I-Method
learning	E-Method
is	O
applied	O
to	O
visual	O
question	O
-	O
answering	S-Task
tasks	O
.	O

Based	O
on	O
the	O
two	O
observations	O
of	O
the	O
previous	O
works	O
,	O
various	O
alternative	O
models	O
are	O
suggested	O
and	O
validated	O
to	O
propose	O
the	O
three	B-Method
-	I-Method
block	I-Method
layered	I-Method
MRN	E-Method
.	O

Our	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
VQA	S-Material
dataset	O
for	O
both	O
Open	B-Task
-	I-Task
Ended	E-Task
and	O
Multiple	B-Task
-	I-Task
Choice	I-Task
tasks	E-Task
.	O

Moreover	O
,	O
we	O
have	O
introduced	O
a	O
novel	O
method	O
to	O
visualize	O
the	O
spatial	O
attention	O
from	O
the	O
collapsed	O
visual	O
features	O
using	O
back	B-Method
-	I-Method
propagation	E-Method
.	O

We	O
believe	O
our	O
visualization	B-Method
method	E-Method
brings	O
implicit	B-Method
attention	I-Method
mechanism	E-Method
to	O
research	O
of	O
attentional	B-Method
models	E-Method
.	O

Using	O
back	B-Method
-	I-Method
propagation	I-Method
of	I-Method
attention	I-Method
effect	E-Method
,	O
extensive	O
research	O
in	O
object	B-Task
detection	E-Task
,	O
segmentation	B-Task
and	I-Task
tracking	E-Task
are	O
worth	O
further	O
investigations	O
.	O

subsubsection	O
:	O
Acknowledgments	O
The	O
authors	O
would	O
like	O
to	O
thank	O
Patrick	O
Emaase	O
for	O
helpful	O
comments	O
and	O
editing	O
.	O

This	O
work	O
was	O
supported	O
by	O
Naver	O
Corp.	O
and	O
partly	O
by	O
the	O
Korea	O
government	O
(	O
IITP	O
-	O
R0126	O
-	O
16	O
-	O
1072	O
-	O
SW.StarLab	O
,	O
KEIT	O
-	O
10044009	O
-	O
HRI.MESSI	O
,	O
KEIT	O
-	O
10060086	O
-	O
RISF	O
,	O
ADD	O
-	O
UD130070ID	O
-	O
BMRR	O
)	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
VQA	S-Material
test	O
-	O
dev	O
Results	O
subsection	O
:	O
More	O
Examples	O
subsection	O
:	O
Comparative	O
Analysis	O
subsection	O
:	O
Failure	O
Examples	O
document	O
:	O
Nonlinear	B-Method
3D	I-Method
Face	I-Method
Morphable	I-Method
Model	E-Method
As	O
a	O
classic	O
statistical	B-Method
model	I-Method
of	I-Method
3D	I-Method
facial	I-Method
shape	I-Method
and	I-Method
texture	E-Method
,	O
3D	B-Method
Morphable	I-Method
Model	E-Method
(	O
3DMM	B-Method
)	E-Method
is	O
widely	O
used	O
in	O
facial	B-Task
analysis	E-Task
,	O
e.g.	O
,	O
model	B-Task
fitting	E-Task
,	O
image	B-Task
synthesis	E-Task
.	O

Conventional	O
3DMM	S-Method
is	O
learned	O
from	O
a	O
set	O
of	O
well	O
-	O
controlled	O
2D	O
face	O
images	O
with	O
associated	O
3D	O
face	O
scans	O
,	O
and	O
represented	O
by	O
two	O
sets	O
of	O
PCA	B-Method
basis	I-Method
functions	E-Method
.	O

Due	O
to	O
the	O
type	O
and	O
amount	O
of	O
training	O
data	O
,	O
as	O
well	O
as	O
the	O
linear	O
bases	O
,	O
the	O
representation	O
power	O
of	O
3DMM	S-Method
can	O
be	O
limited	O
.	O

To	O
address	O
these	O
problems	O
,	O
this	O
paper	O
proposes	O
an	O
innovative	O
framework	O
to	O
learn	O
a	O
nonlinear	B-Method
3DMM	E-Method
model	O
from	O
a	O
large	O
set	O
of	O
unconstrained	O
face	O
images	O
,	O
without	O
collecting	O
3D	O
face	O
scans	O
.	O

Specifically	O
,	O
given	O
a	O
face	O
image	O
as	O
input	O
,	O
a	O
network	B-Method
encoder	E-Method
estimates	O
the	O
projection	O
,	O
shape	O
and	O
texture	O
parameters	O
.	O

Two	O
decoders	S-Method
serve	O
as	O
the	O
nonlinear	B-Method
3DMM	E-Method
to	O
map	O
from	O
the	O
shape	O
and	O
texture	O
parameters	O
to	O
the	O
3D	O
shape	O
and	O
texture	O
,	O
respectively	O
.	O

With	O
the	O
projection	O
parameter	O
,	O
3D	O
shape	O
,	O
and	O
texture	O
,	O
a	O
novel	O
analytically	B-Method
-	I-Method
differentiable	I-Method
rendering	I-Method
layer	E-Method
is	O
designed	O
to	O
reconstruct	O
the	O
original	O
input	O
face	O
.	O

The	O
entire	O
network	O
is	O
end	O
-	O
to	O
-	O
end	O
trainable	O
with	O
only	O
weak	O
supervision	O
.	O

We	O
demonstrate	O
the	O
superior	O
representation	B-Metric
power	E-Metric
of	O
our	O
nonlinear	B-Method
3DMM	E-Method
over	O
its	O
linear	B-Method
counterpart	E-Method
,	O
and	O
its	O
contribution	O
to	O
face	B-Task
alignment	E-Task
and	O
3D	B-Task
reconstruction	E-Task
.	O

⌊⌋	O
⌈⌉	O
section	O
:	O
Introduction	O
3D	B-Method
Morphable	I-Method
Model	E-Method
(	O
3DMM	S-Method
)	O
is	O
a	O
statistical	B-Method
model	E-Method
of	O
3D	O
facial	O
shape	O
and	O
texture	O
in	O
a	O
space	O
where	O
there	O
are	O
explicit	O
correspondences	O
.	O

The	O
morphable	B-Method
model	I-Method
framework	E-Method
provides	O
two	O
key	O
benefits	O
:	O
first	O
,	O
a	O
point	O
-	O
to	O
-	O
point	O
correspondence	O
between	O
the	O
reconstruction	S-Method
and	O
all	O
other	O
models	O
,	O
enabling	O
âmorphingâ	O
,	O
and	O
second	O
,	O
modeling	O
underlying	O
transformations	O
between	O
types	O
of	O
faces	O
(	O
male	O
to	O
female	O
,	O
neutral	O
to	O
smile	O
,	O
etc	O
.	O

)	O
.	O

3DMM	S-Method
has	O
been	O
widely	O
applied	O
in	O
numerous	O
areas	O
,	O
such	O
as	O
computer	B-Task
vision	E-Task
,	O
graphics	S-Task
,	O
human	B-Task
behavioral	I-Task
analysis	E-Task
and	O
craniofacial	B-Task
surgery	E-Task
.	O

3DMM	S-Method
is	O
learnt	O
through	O
supervision	S-Method
by	O
performing	O
dimension	B-Method
reduction	E-Method
,	O
normally	O
Principal	B-Method
Component	I-Method
Analysis	E-Method
(	O
PCA	S-Method
)	O
,	O
on	O
a	O
training	O
set	O
of	O
face	O
images	O
/	O
scans	O
.	O

To	O
model	O
highly	O
variable	O
3D	O
face	O
shapes	O
,	O
a	O
large	O
amount	O
of	O
high	O
-	O
quality	O
3D	O
face	O
scans	O
is	O
required	O
.	O

However	O
,	O
this	O
requirement	O
is	O
expensive	O
to	O
fulfill	O
.	O

The	O
first	O
3DMM	S-Method
was	O
built	O
from	O
scans	O
of	O
subjects	O
with	O
a	O
similar	O
ethnicity	O
/	O
age	O
group	O
.	O

They	O
were	O
also	O
captured	O
in	O
well	O
-	O
controlled	O
conditions	O
,	O
with	O
only	O
neutral	O
expressions	O
.	O

Hence	O
,	O
it	O
is	O
fragile	O
to	O
large	O
variances	O
in	O
the	O
face	O
identity	O
.	O

The	O
widely	O
used	O
Basel	B-Method
Face	I-Method
Model	E-Method
(	O
BFM	B-Method
)	E-Method
is	O
also	O
built	O
with	O
only	O
subjects	O
in	O
neutral	O
expressions	O
.	O

Lack	O
of	O
expression	O
can	O
be	O
compensated	O
using	O
expression	O
bases	O
from	O
FaceWarehouse	S-Method
or	O
BD	B-Method
-	I-Method
3FE	E-Method
.	O

After	O
more	O
than	O
a	O
decade	O
,	O
almost	O
all	O
models	O
use	O
less	O
than	O
training	O
scans	O
.	O

Such	O
a	O
small	O
training	O
set	O
is	O
far	O
from	O
adequate	O
to	O
describe	O
the	O
full	O
variability	O
of	O
human	O
faces	O
.	O

Only	O
recently	O
,	O
Booth	O
et	O
al	O
.	O

spent	O
a	O
significant	O
effort	O
to	O
build	O
3DMM	S-Method
from	O
scans	O
of	O
subjects	O
.	O

Second	O
,	O
the	O
texture	B-Method
model	E-Method
of	O
3DMM	S-Method
is	O
normally	O
built	O
with	O
a	O
small	O
number	O
of	O
2D	O
face	O
images	O
co	O
-	O
captured	O
with	O
3D	O
scans	O
,	O
under	O
well	O
-	O
controlled	O
conditions	O
.	O

Therefore	O
,	O
such	O
a	O
model	O
is	O
only	O
learnt	O
to	O
represent	O
the	O
facial	O
texture	O
in	O
similar	O
conditions	O
,	O
rather	O
than	O
in	O
-	O
the	O
-	O
wild	O
environments	O
.	O

This	O
substantially	O
limits	O
the	O
application	O
scenarios	O
of	O
3DMM	S-Method
.	O

Finally	O
,	O
the	O
representation	O
power	O
of	O
3DMM	S-Method
is	O
limited	O
by	O
not	O
only	O
the	O
size	O
of	O
training	O
set	O
but	O
also	O
its	O
formulation	O
.	O

The	O
facial	O
variations	O
are	O
nonlinear	S-Method

in	O
nature	O
.	O

E.g.	O

,	O
the	O
variations	O
in	O
different	O
facial	O
expressions	O
or	O
poses	O
are	O
nonlinear	S-Method
,	O
which	O
violates	O
the	O
linear	B-Method
assumption	I-Method
of	I-Method
PCA	I-Method
-	I-Method
based	I-Method
models	E-Method
.	O

Thus	O
,	O
a	O
PCA	B-Method
model	E-Method
is	O
unable	O
to	O
interpret	O
facial	O
variations	O
well	O
.	O

Given	O
the	O
barrier	O
of	O
3DMM	S-Method
in	O
its	O
data	O
,	O
supervision	B-Task
and	I-Task
linear	I-Task
bases	E-Task
,	O
this	O
paper	O
aims	O
to	O
revolutionize	O
the	O
paradigm	O
of	O
learning	O
3DMM	S-Method
by	O
answering	O
a	O
fundamental	O
question	O
:	O
Whether	O
and	O
how	O
can	O
we	O
learn	O
a	O
nonlinear	B-Method
3D	I-Method
Morphable	I-Method
Model	E-Method
of	O
face	O
shape	O
and	O
texture	O
from	O
a	O
set	O
of	O
unconstrained	O
2D	O
face	O
images	O
,	O
without	O
collecting	O
3D	O
face	O
scans	O
?	O
If	O
the	O
answer	O
were	O
yes	O
,	O
this	O
would	O
be	O
in	O
sharp	O
contrast	O
to	O
the	O
conventional	O
3DMM	S-Method
approach	O
,	O
and	O
remedy	O
all	O
aforementioned	O
limitations	O
.	O

Fortunately	O
,	O
we	O
have	O
developed	O
approaches	O
that	O
offer	O
positive	O
answers	O
to	O
this	O
question	O
.	O

Therefore	O
,	O
the	O
core	O
of	O
this	O
paper	O
is	O
regarding	O
how	O
to	O
learn	O
this	O
new	O
3DMM	S-Method
,	O
what	O
is	O
the	O
representation	O
power	O
of	O
the	O
model	O
,	O
and	O
what	O
is	O
the	O
benefit	O
of	O
the	O
model	O
to	O
facial	B-Task
analysis	E-Task
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
starting	O
with	O
an	O
observation	O
that	O
the	O
linear	O
3DMM	S-Method
formulation	O
is	O
equivalent	O
to	O
a	O
single	B-Method
layer	I-Method
network	E-Method
,	O
using	O
a	O
deep	B-Method
network	I-Method
architecture	E-Method
naturally	O
increases	O
the	O
model	O
capacity	O
.	O

Hence	O
,	O
we	O
utilize	O
two	O
network	B-Method
decoders	E-Method
,	O
instead	O
of	O
two	O
PCA	O
spaces	O
,	O
as	O
the	O
shape	B-Method
and	I-Method
texture	I-Method
model	I-Method
components	E-Method
,	O
respectively	O
.	O

With	O
careful	O
consideration	O
of	O
each	O
component	O
,	O
we	O
design	O
different	O
networks	O
for	O
shape	O
and	O
texture	O
:	O
the	O
multi	B-Method
-	I-Method
layer	I-Method
perceptron	I-Method
(	I-Method
MLP	I-Method
)	E-Method
for	O
shape	S-Method
and	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
for	O
texture	O
.	O

Each	O
decoder	O
will	O
take	O
a	O
shape	O
or	O
texture	O
representation	O
as	O
input	O
and	O
output	O
the	O
dense	O
3D	O
face	O
or	O
a	O
face	O
texture	O
.	O

These	O
two	O
decoders	O
are	O
essentially	O
the	O
nonlinear	B-Method
3DMM	E-Method
.	O

Further	O
,	O
we	O
learn	O
the	O
fitting	B-Method
algorithm	E-Method
to	O
our	O
nonlinear	B-Method
3DMM	E-Method
,	O
which	O
is	O
formulated	O
as	O
a	O
CNN	B-Method
encoder	E-Method
.	O

The	O
encoder	O
takes	O
a	O
2D	O
face	O
image	O
as	O
input	O
and	O
generates	O
the	O
shape	O
and	O
texture	O
parameters	O
,	O
from	O
which	O
two	O
decoders	S-Method
estimate	O
the	O
3D	O
face	O
and	O
texture	O
.	O

The	O
3D	O
face	O
and	O
texture	O
would	O
perfectly	O
reconstruct	O
the	O
input	O
face	O
,	O
if	O
the	O
fitting	B-Method
algorithm	E-Method
and	O
3DMM	S-Method
are	O
well	O
learnt	O
.	O

Therefore	O
,	O
we	O
design	O
a	O
differentiable	B-Method
rendering	I-Method
layer	E-Method
to	O
generate	O
a	O
reconstructed	O
face	O
by	O
fusing	O
the	O
3D	O
face	O
,	O
texture	O
,	O
and	O
the	O
camera	O
projection	O
parameters	O
estimated	O
by	O
the	O
encoder	S-Method
.	O

Finally	O
,	O
the	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
learning	I-Method
scheme	E-Method
is	O
constructed	O
where	O
the	O
encoder	S-Method
and	O
two	O
decoders	S-Method
are	O
learnt	O
jointly	O
to	O
minimize	O
the	O
difference	O
between	O
the	O
reconstructed	O
face	O
and	O
the	O
input	O
face	O
.	O

Jointly	O
learning	O
the	O
3DMM	S-Method
and	O
the	O
model	B-Method
fitting	I-Method
encoder	E-Method
allows	O
us	O
to	O
leverage	O
the	O
large	O
collection	O
of	O
unconstrained	O
2D	O
images	O
without	O
relying	O
on	O
3D	O
scans	O
.	O

We	O
show	O
significantly	O
improved	O
shape	O
and	O
texture	O
representation	O
power	O
over	O
the	O
linear	B-Method
3DMM	E-Method
.	O

Consequently	O
,	O
this	O
also	O
benefits	O
other	O
tasks	O
such	O
as	O
2D	O
face	B-Task
alignment	E-Task
and	O
3D	B-Task
reconstruction	E-Task
.	O

In	O
this	O
paper	O
,	O
we	O
make	O
the	O
following	O
contributions	O
:	O
1	O
)	O
We	O
learn	O
a	O
nonlinear	B-Method
3DMM	E-Method
model	O
that	O
has	O
greater	O
representation	O
power	O
than	O
its	O
traditional	O
linear	B-Method
counterpart	E-Method
.	O

2	O
)	O
We	O
jointly	O
learn	O
the	O
model	O
and	O
the	O
model	B-Method
fitting	I-Method
algorithm	E-Method
via	O
weak	B-Method
supervision	E-Method
,	O
by	O
leveraging	O
a	O
large	O
collection	O
of	O
2D	O
images	O
without	O
3D	O
scans	O
.	O

The	O
novel	O
rendering	B-Method
layer	E-Method
enables	O
the	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
training	E-Task
.	O

3	O
)	O
The	O
new	O
3DMM	S-Method
further	O
improves	O
performance	O
in	O
related	O
tasks	O
:	O
face	B-Task
alignment	E-Task
and	O
face	B-Task
reconstruction	E-Task
.	O

section	O
:	O
Prior	O
Work	O
Linear	B-Method
3DMM	E-Method
.	O

Since	O
the	O
original	O
work	O
by	O
Blanz	O
and	O
Vetter	O
,	O
there	O
has	O
been	O
a	O
large	O
amount	O
of	O
effort	O
trying	O
to	O
improve	O
3DMM	S-Method
modeling	O
mechanism	O
.	O

Paysan	O
et	O
al	O
.	O

use	O
a	O
Nonrigid	B-Method
Iterative	I-Method
Closest	I-Method
Point	E-Method
to	O
directly	O
align	O
3D	O
scans	O
as	O
an	O
alternative	O
to	O
the	O
UV	B-Method
space	I-Method
alignment	I-Method
method	E-Method
in	O
.	O

Vlasic	O
et	O
al	O
.	O

use	O
a	O
multilinear	B-Method
model	E-Method
to	O
model	O
the	O
combined	O
effect	O
of	O
identity	O
and	O
expression	O
variation	O
on	O
the	O
facial	O
shape	O
.	O

Later	O
,	O
Bolkart	O
and	O
Wuhrer	O
show	O
how	O
such	O
a	O
multilinear	B-Method
model	E-Method
can	O
be	O
estimated	O
directly	O
from	O
the	O
3D	O
scans	O
using	O
a	O
joint	B-Method
optimization	E-Method
over	O
the	O
model	O
parameters	O
and	O
groupwise	B-Method
registration	E-Method
of	O
3D	O
scans	O
.	O

Improving	O
Linear	B-Method
3DMM	E-Method
.	O

With	O
PCA	B-Method
bases	E-Method
,	O
the	O
statistical	B-Method
distribution	E-Method
underlying	O
3DMM	S-Method
is	O
Gaussian	S-Method
.	O

Koppen	O
et	O
al	O
.	O

argue	O
that	O
single	B-Method
-	I-Method
mode	I-Method
Gaussian	I-Method
ca	I-Method
n’t	E-Method
represent	O
real	O
-	O
world	O
distribution	O
.	O

They	O
introduce	O
the	O
Gaussian	O
Mixture	O
3DMM	S-Method
that	O
models	O
the	O
global	O
population	O
as	O
a	O
mixture	B-Method
of	I-Method
Gaussian	I-Method
subpopulations	E-Method
,	O
each	O
with	O
its	O
own	O
mean	O
,	O
but	O
shared	O
covariance	O
.	O

Booth	O
el	O
al	O
.	O

aim	O
to	O
improve	O
texture	O
of	O
3DMM	S-Method
to	O
go	O
beyond	O
controlled	O
settings	O
by	O
learning	O
âin	B-Method
-	I-Method
the	I-Method
-	I-Method
wildâ	I-Method
feature	I-Method
-	I-Method
based	I-Method
texture	I-Method
model	E-Method
.	O

However	O
,	O
both	O
works	O
are	O
still	O
based	O
on	O
statistical	B-Method
PCA	I-Method
bases	E-Method
.	O

Duong	O
et	O
al	O
.	O

address	O
the	O
problem	O
of	O
linearity	B-Task
in	I-Task
face	I-Task
modeling	E-Task
by	O
using	O
Deep	B-Method
Boltzmann	I-Method
Machines	E-Method
.	O

However	O
,	O
they	O
only	O
work	O
with	O
2D	O
face	O
and	O
sparse	O
landmarks	O
;	O
and	O
hence	O
can	O
not	O
handle	O
faces	O
with	O
large	O
-	O
pose	O
variations	O
or	O
occlusion	O
well	O
.	O

2D	B-Task
Face	I-Task
Alignment	E-Task
.	O

2D	B-Task
Face	I-Task
Alignment	E-Task
can	O
be	O
cast	O
as	O
a	O
regression	B-Task
problem	E-Task
where	O
2D	O
landmark	O
locations	O
are	O
regressed	O
directly	O
.	O

For	O
large	B-Task
-	I-Task
pose	I-Task
or	I-Task
occluded	I-Task
faces	E-Task
,	O
strong	O
priors	O
of	O
3DMM	S-Method
face	O
shape	O
have	O
been	O
shown	O
to	O
be	O
beneficial	O
.	O

Hence	O
,	O
there	O
is	O
increasing	O
attention	O
in	O
conducting	O
face	B-Task
alignment	E-Task
by	O
fitting	O
a	O
3D	B-Method
face	I-Method
model	E-Method
to	O
a	O
single	O
2D	O
image	O
.	O

Among	O
the	O
prior	O
works	O
,	O
iterative	B-Method
approaches	E-Method
with	O
cascades	O
of	O
regressors	O
tend	O
to	O
be	O
preferred	O
.	O

At	O
each	O
cascade	O
,	O
it	O
can	O
be	O
a	O
single	O
or	O
even	O
two	O
regressors	O
.	O

In	O
contrast	O
to	O
aforementioned	O
works	O
that	O
use	O
a	O
fixed	O
3DMM	S-Method
model	O
,	O
our	O
model	O
and	O
model	B-Method
fitting	E-Method
are	O
learned	O
jointly	O
.	O

This	O
results	O
in	O
a	O
more	O
powerful	O
model	O
:	O
a	O
single	B-Method
-	I-Method
pass	I-Method
encoder	E-Method
,	O
which	O
is	O
learnt	O
jointly	O
with	O
the	O
model	O
,	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
face	B-Task
alignment	I-Task
performance	E-Task
on	O
AFLW2000	B-Material
benchmark	I-Material
dataset	E-Material
.	O

3D	B-Task
Face	I-Task
Reconstruction	E-Task
.	O

3DMM	S-Method
also	O
demonstrates	O
its	O
strength	O
in	O
face	B-Task
reconstruction	E-Task
.	O

Since	O
with	O
a	O
single	O
image	O
,	O
present	O
information	O
about	O
the	O
surface	O
is	O
limited	O
;	O
3D	B-Task
face	I-Task
reconstruction	E-Task
must	O
rely	O
on	O
prior	O
knowledge	O
like	O
3DMM	S-Method
.	O

Besides	O
3DMM	S-Method
fitting	O
methods	O
,	O
recently	O
,	O
Richardson	O
et	O
al	O
.	O

design	O
a	O
refinement	B-Method
network	E-Method
that	O
adds	O
facial	O
details	O
on	O
top	O
of	O
the	O
3DMM	S-Method
-	O
based	O
geometry	O
.	O

However	O
,	O
this	O
approach	O
can	O
only	O
learn	O
2.5D	O
depth	O
map	O
,	O
which	O
loses	O
the	O
correspondence	O
property	O
of	O
3DMM	S-Method
.	O

The	O
recent	O
work	O
of	O
Tewari	O
et	O
al	O
.	O

reconstruct	O
a	O
3D	O
face	O
by	O
an	O
elegant	O
encoder	B-Method
-	I-Method
decoder	I-Method
network	E-Method
.	O

While	O
their	O
ability	O
to	O
decompose	O
lighting	O
with	O
reflectance	O
is	O
satisfactory	O
,	O
our	O
work	O
has	O
a	O
different	O
objective	O
of	O
learning	O
a	O
nonlinear	B-Method
3DMM	E-Method
.	O

section	O
:	O
Proposed	O
Method	O
subsection	O
:	O
Conventional	O
Linear	B-Method
3DMM	E-Method
The	O
3D	B-Method
Morphable	I-Method
Model	E-Method
(	O
3DMM	S-Method
)	O
and	O
its	O
2D	B-Method
counterpart	E-Method
,	O
Active	B-Method
Appearance	I-Method
Model	E-Method
,	O
provide	O
parametric	B-Method
models	E-Method
for	O
synthesizing	B-Task
faces	E-Task
,	O
where	O
faces	O
are	O
modeled	O
using	O
two	O
components	O
:	O
shape	O
and	O
texture	O
.	O

In	O
,	O
Blanz	O
et	O
al	O
.	O

propose	O
to	O
describe	O
the	O
3D	O
face	O
space	O
with	O
PCA	S-Method
:	O
where	O
is	O
a	O
3D	O
face	O
with	O
vertices	O
,	O
is	O
the	O
mean	O
shape	O
,	O
is	O
the	O
shape	O
parameter	O
corresponding	O
to	O
a	O
3D	O
shape	O
bases	O
.	O

The	O
shape	O
bases	O
can	O
be	O
further	O
split	O
into	O
,	O
where	O
is	O
trained	O
from	O
3D	O
scans	O
with	O
neutral	O
expression	O
,	O
and	O
is	O
from	O
the	O
offsets	O
between	O
expression	O
and	O
neutral	O
scans	O
.	O

The	O
texture	O
of	O
the	O
face	O
is	O
defined	O
within	O
the	O
mean	O
shape	O
,	O
which	O
describes	O
the	O
R	O
,	O
G	O
,	O
B	O
colors	O
of	O
corresponding	O
vertices	O
.	O

is	O
also	O
formulated	O
as	O
a	O
linear	B-Method
combination	I-Method
of	I-Method
texture	I-Method
basis	I-Method
functions	E-Method
:	O
where	O
is	O
the	O
mean	O
texture	O
,	O
is	O
the	O
texture	O
bases	O
,	O
and	O
is	O
the	O
texture	O
parameter	O
.	O

The	O
3DMM	S-Method
can	O
be	O
used	O
to	O
synthesize	O
novel	O
views	O
of	O
the	O
face	O
.	O

Firstly	O
,	O
a	O
3D	O
face	O
is	O
projected	O
onto	O
the	O
image	O
plane	O
with	O
the	O
weak	B-Method
perspective	I-Method
projection	I-Method
model	E-Method
:	O
where	O
is	O
the	O
model	B-Method
construction	E-Method
and	O
projection	O
function	O
leading	O
to	O
the	O
2D	O
positions	O
of	O
3D	O
vertices	O
,	O
is	O
the	O
scale	O
factor	O
,	O
is	O
the	O
orthographic	O
projection	O
matrix	O
,	O
is	O
the	O
rotation	O
matrix	O
constructed	O
from	O
three	O
rotation	O
angles	O
pitch	O
,	O
yaw	O
,	O
roll	O
,	O
and	O
is	O
the	O
translation	O
vector	O
.	O

While	O
the	O
projection	O
matrix	O
has	O
dimensions	O
,	O
it	O
has	O
six	O
degrees	O
of	O
freedom	O
,	O
which	O
is	O
parameterized	O
by	O
a	O
-	O
dim	O
vector	O
.	O

Then	O
,	O
the	O
2D	O
image	O
is	O
rendered	O
using	O
texture	S-Method
and	O
an	O
illumination	B-Method
model	E-Method
as	O
described	O
in	O
.	O

subsection	O
:	O
Nonlinear	O
3DMM	S-Method
As	O
mentioned	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
the	O
linear	B-Method
3DMM	E-Method
has	O
the	O
problems	O
such	O
as	O
requiring	O
3D	O
face	O
scans	O
for	O
supervised	B-Task
learning	E-Task
,	O
unable	O
to	O
leverage	O
massive	O
unconstrained	O
face	O
images	O
for	O
learning	S-Task
,	O
and	O
the	O
limited	O
representation	O
power	O
due	O
to	O
the	O
linear	B-Method
bases	E-Method
.	O

We	O
propose	O
to	O
learn	O
a	O
nonlinear	B-Method
3DMM	E-Method
model	O
using	O
only	O
large	O
-	O
scale	O
in	O
-	O
the	O
-	O
wild	O
2D	O
face	O
images	O
.	O

subsubsection	O
:	O
Problem	B-Task
Formulation	E-Task
In	O
linear	B-Method
3DMM	E-Method
,	O
the	O
factorization	O
of	O
each	O
components	O
(	O
texture	O
,	O
shape	O
)	O
can	O
be	O
seen	O
as	O
a	O
matrix	B-Method
multiplication	E-Method
between	O
coefficients	O
and	O
bases	O
.	O

From	O
a	O
neural	B-Method
network	E-Method
’s	O
perspective	O
,	O
this	O
can	O
be	O
viewed	O
as	O
a	O
shallow	B-Method
network	E-Method
with	O
only	O
one	O
fully	O
connected	O
layer	O
and	O
no	O
activation	O
function	O
.	O

Naturally	O
,	O
to	O
increase	O
the	O
model	O
’s	O
representative	O
power	O
,	O
the	O
shallow	B-Method
network	E-Method
can	O
be	O
extended	O
to	O
a	O
deep	B-Method
architecture	E-Method
.	O

In	O
this	O
work	O
,	O
we	O
design	O
a	O
novel	O
learning	B-Method
scheme	E-Method
to	O
learn	O
a	O
deep	O
3DMM	S-Method
and	O
its	O
inference	B-Method
(	I-Method
or	I-Method
fitting	I-Method
)	I-Method
algorithm	E-Method
.	O

Specifically	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
use	O
two	O
deep	B-Method
networks	E-Method
to	O
decode	O
the	O
shape	O
,	O
texture	O
parameters	O
into	O
the	O
3D	O
facial	O
shape	O
and	O
texture	O
respectively	O
.	O

To	O
make	O
the	O
framework	O
end	O
-	O
to	O
-	O
end	O
trainable	O
,	O
these	O
parameters	O
are	O
estimated	O
by	O
an	O
encoder	B-Method
network	E-Method
,	O
which	O
is	O
essentially	O
the	O
fitting	B-Method
algorithm	E-Method
of	O
our	O
3DMM	S-Method
.	O

Three	O
deep	B-Method
networks	E-Method
join	O
forces	O
for	O
the	O
ultimate	O
goal	O
of	O
reconstructing	O
the	O
input	O
face	O
image	O
,	O
with	O
the	O
assistance	O
of	O
a	O
geometry	B-Method
-	I-Method
based	I-Method
rendering	I-Method
layer	E-Method
.	O

Formally	O
,	O
given	O
a	O
set	O
of	O
2D	O
face	O
images	O
,	O
we	O
aim	O
to	O
learn	O
an	O
encoder	S-Method
:	O
that	O
estimates	O
the	O
projection	O
parameter	O
,	O
and	O
shape	O
and	O
texture	O
parameters	O
,	O
a	O
3D	B-Method
shape	I-Method
decoder	E-Method
:	O
that	O
decodes	O
the	O
shape	O
parameter	O
to	O
a	O
3D	O
shape	O
,	O
and	O
a	O
texture	B-Method
decoder	E-Method
:	O
that	O
decodes	O
the	O
texture	O
parameter	O
to	O
a	O
realistic	O
texture	O
,	O
with	O
the	O
objective	O
that	O
the	O
rendered	O
image	O
with	O
,	O
,	O
and	O
can	O
approximate	O
the	O
original	O
image	O
well	O
.	O

Mathematically	O
,	O
the	O
objective	B-Metric
function	E-Metric
is	O
:	O
where	O
is	O
the	O
rendering	O
layer	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

subsubsection	O
:	O
Shape	B-Method
&	I-Method
Texture	I-Method
Representation	E-Method
Our	O
shape	B-Method
representation	E-Method
is	O
the	O
same	O
as	O
that	O
of	O
the	O
linear	O
3DMM	S-Method
,	O
i.e.	O
,	O
is	O
a	O
set	O
of	O
vertices	O
on	O
the	O
face	O
surface	O
.	O

The	O
shape	B-Method
decoder	E-Method
is	O
a	O
MLP	S-Method
whose	O
input	O
is	O
the	O
shape	O
parameter	O
from	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
illustrates	O
three	O
possible	O
texture	B-Method
representations	E-Method
.	O

Texture	O
is	O
defined	O
per	O
vertex	O
in	O
the	O
linear	B-Method
3DMM	E-Method
and	O
recent	O
work	O
such	O
as	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
)	O
.	O

There	O
is	O
a	O
texture	O
intensity	O
value	O
corresponding	O
to	O
each	O
vertex	O
in	O
the	O
face	O
mesh	O
.	O

Since	O
3D	O
vertices	O
are	O
not	O
defined	O
on	O
a	O
2D	O
grid	O
,	O
this	O
representation	O
will	O
be	O
parameterized	O
as	O
a	O
vector	O
,	O
which	O
not	O
only	O
loses	O
the	O
spatial	O
relation	O
of	O
vertices	O
,	O
but	O
also	O
prevents	O
it	O
from	O
leveraging	O
the	O
convenience	O
of	O
deploying	O
CNN	S-Method
on	O
2D	O
imagery	O
.	O

In	O
contrast	O
,	O
given	O
the	O
rapid	O
progress	O
in	O
image	B-Task
synthesis	E-Task
,	O
it	O
is	O
desirable	O
to	O
choose	O
a	O
2D	O
image	O
,	O
e.g.	O
,	O
a	O
frontal	O
-	O
view	O
face	O
image	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
,	O
as	O
a	O
texture	B-Method
representation	E-Method
.	O

However	O
,	O
frontal	O
faces	O
contain	O
little	O
information	O
of	O
two	O
sides	O
,	O
which	O
would	O
lose	O
much	O
texture	O
information	O
for	O
side	O
-	O
view	O
faces	O
.	O

In	O
light	O
of	O
these	O
considerations	O
,	O
we	O
use	O
an	O
unwrapped	O
2D	O
texture	O
as	O
our	O
texture	B-Method
representation	E-Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
)	O
.	O

Specifically	O
,	O
each	O
3D	O
vertex	O
is	O
projected	O
onto	O
the	O
UV	O
space	O
using	O
cylindrical	O
unwarp	O
.	O

Assuming	O
that	O
the	O
face	O
mesh	O
has	O
the	O
top	O
pointing	O
up	O
the	O
axis	O
,	O
the	O
projection	O
of	O
onto	O
the	O
UV	O
space	O
is	O
computed	O
as	O
:	O
where	O
are	O
constant	O
scale	O
and	O
translation	O
scalars	O
to	O
place	O
the	O
unwrapped	O
face	O
into	O
the	O
image	O
boundaries	O
.	O

Also	O
,	O
the	O
texture	B-Method
decoder	E-Method
is	O
a	O
CNN	S-Method
constructed	O
by	O
fractionally	B-Method
-	I-Method
strided	I-Method
convolution	I-Method
layers	E-Method
.	O

subsubsection	O
:	O
In	B-Task
-	I-Task
Network	I-Task
Face	I-Task
Rendering	E-Task
To	O
reconstruct	O
a	O
face	O
image	O
from	O
the	O
texture	O
,	O
shape	O
,	O
and	O
projection	O
parameter	O
,	O
we	O
define	O
a	O
rendering	B-Method
layer	E-Method
.	O

This	O
is	O
accomplished	O
in	O
three	O
steps	O
.	O

Firstly	O
,	O
the	O
texture	O
value	O
of	O
each	O
vertex	O
in	O
is	O
determined	O
by	O
its	O
predefined	O
location	O
in	O
the	O
2D	O
texture	O
.	O

Usually	O
,	O
it	O
involves	O
sub	B-Method
-	I-Method
pixel	I-Method
sampling	E-Method
via	O
a	O
bilinear	B-Method
sampling	I-Method
kernel	E-Method
:	O
where	O
is	O
the	O
UV	O
space	O
projection	O
of	O
via	O
Eqn	O
.	O

[	O
reference	O
]	O
.	O

Secondly	O
,	O
the	O
3D	O
shape	O
/	O
mesh	O
is	O
projected	O
to	O
the	O
image	O
plane	O
via	O
Eqn	O
.	O

[	O
reference	O
]	O
.	O

Finally	O
,	O
the	O
3D	O
mesh	O
is	O
then	O
rendered	O
using	O
a	O
Z	B-Method
-	I-Method
buffer	I-Method
renderer	E-Method
,	O
where	O
each	O
pixel	O
is	O
associated	O
with	O
a	O
single	O
triangle	O
of	O
the	O
mesh	O
,	O
where	O
is	O
an	O
operation	O
returning	O
three	O
vertices	O
of	O
the	O
triangle	O
that	O
encloses	O
the	O
pixel	O
after	O
projection	O
.	O

In	O
order	O
to	O
handle	O
occlusions	O
,	O
when	O
a	O
single	O
pixel	O
resides	O
in	O
more	O
than	O
one	O
triangle	O
,	O
the	O
triangle	O
that	O
is	O
closest	O
to	O
the	O
image	O
plane	O
is	O
selected	O
.	O

The	O
value	O
of	O
each	O
pixel	O
is	O
determined	O
by	O
interpolating	O
the	O
intensity	O
of	O
the	O
mesh	O
vertices	O
via	O
barycentric	O
coordinates	O
.	O

There	O
are	O
alternative	O
designs	O
to	O
our	O
rendering	B-Method
layer	E-Method
.	O

If	O
the	O
texture	B-Method
representation	E-Method
is	O
defined	O
per	O
vertex	O
,	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
,	O
one	O
may	O
warp	O
the	O
input	O
image	O
onto	O
the	O
vertex	O
space	O
of	O
the	O
3D	O
shape	O
,	O
whose	O
distance	O
to	O
the	O
per	B-Method
-	I-Method
vertex	I-Method
texture	I-Method
representation	E-Method
can	O
form	O
a	O
reconstruction	O
loss	O
.	O

This	O
design	O
is	O
adopted	O
by	O
the	O
recent	O
work	O
of	O
.	O

In	O
comparison	O
,	O
our	O
rendered	O
image	O
is	O
defined	O
on	O
a	O
2D	O
grid	O
while	O
the	O
alternative	O
is	O
on	O
top	O
of	O
the	O
3D	O
mesh	O
.	O

As	O
a	O
result	O
,	O
our	O
rendered	O
image	O
can	O
enjoy	O
the	O
convenience	O
of	O
applying	O
the	O
adversarial	B-Method
loss	E-Method
,	O
which	O
is	O
shown	O
to	O
be	O
critical	O
in	O
improving	O
the	O
quality	B-Metric
of	I-Metric
synthetic	I-Metric
texture	E-Metric
.	O

Another	O
design	O
for	O
rendering	B-Task
layer	E-Task
is	O
image	B-Task
warping	E-Task
based	O
on	O
the	O
spline	B-Method
interpolation	E-Method
,	O
as	O
in	O
.	O

However	O
,	O
this	O
warping	O
is	O
continuous	O
:	O
every	O
pixel	O
in	O
the	O
input	O
will	O
map	O
to	O
the	O
output	O
.	O

Hence	O
this	O
warping	B-Method
operation	E-Method
fails	O
in	O
the	O
occlusion	O
part	O
.	O

As	O
a	O
result	O
,	O
Cole	O
et	O
al	O
.	O

limit	O
their	O
scope	O
to	O
only	O
synthesizing	O
frontal	O
faces	O
by	O
warping	O
from	O
normalized	O
faces	O
.	O

subsubsection	O
:	O
Network	B-Method
Architecture	E-Method
We	O
design	O
our	O
network	B-Method
architecture	E-Method
as	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

Also	O
,	O
includes	O
two	O
fully	B-Method
connected	I-Method
layers	E-Method
with	O
-	B-Method
dim	I-Method
intermediate	I-Method
representation	E-Method
with	O
eLU	B-Method
activation	E-Method
.	O

The	O
entire	O
network	O
is	O
end	O
-	O
to	O
-	O
end	O
trained	O
to	O
reconstruct	O
the	O
input	O
images	O
,	O
with	O
the	O
loss	O
function	O
:	O
where	O
the	O
reconstruction	O
loss	O
enforces	O
the	O
rendered	O
image	O
to	O
be	O
similar	O
to	O
the	O
input	O
,	O
the	O
adversarial	B-Method
loss	E-Method
favors	O
realistic	B-Task
rendering	E-Task
,	O
and	O
the	O
landmark	O
loss	O
enforces	O
geometry	O
constraint	O
.	O

Adversarial	O
Loss	O
.	O

Based	O
on	O
the	O
principal	O
of	O
Generative	B-Method
Adversarial	I-Method
Network	E-Method
(	O
GAN	S-Method
)	O
,	O
the	O
adversarial	B-Method
loss	E-Method
is	O
widely	O
used	O
to	O
synthesize	O
photo	O
-	O
realistic	O
images	O
,	O
where	O
the	O
generator	B-Method
and	I-Method
discriminator	E-Method
are	O
trained	O
alternatively	O
.	O

In	O
our	O
case	O
,	O
networks	O
that	O
generate	O
the	O
rendered	O
image	O
is	O
the	O
generator	O
.	O

The	O
discriminator	O
includes	O
a	O
dedicated	B-Method
network	E-Method
,	O
which	O
aims	O
to	O
distinguish	O
between	O
the	O
real	O
face	O
image	O
and	O
rendered	O
image	O
.	O

During	O
the	O
training	O
of	O
the	O
generator	S-Method
,	O
the	O
texture	B-Method
model	E-Method
will	O
be	O
updated	O
with	O
the	O
objective	O
that	O
is	O
being	O
classified	O
as	O
real	O
faces	O
by	O
.	O

Since	O
our	O
face	B-Method
rendering	E-Method
already	O
creates	O
correct	O
global	O
structure	O
of	O
the	O
face	O
image	O
,	O
the	O
global	B-Method
image	I-Method
-	I-Method
based	I-Method
adversarial	I-Method
loss	E-Method
may	O
not	O
be	O
effective	O
in	O
producing	O
high	O
-	O
quality	O
textures	O
on	O
local	O
facial	O
regions	O
.	O

Therefore	O
,	O
we	O
employ	O
patchGAN	S-Method
in	O
our	O
discriminator	S-Method
.	O

Here	O
,	O
is	O
a	O
CNN	S-Method
consisting	O
of	O
four	O
conv	B-Method
layers	E-Method
with	O
stride	O
of	O
,	O
and	O
number	O
of	O
filters	O
are	O
,	O
,	O
and	O
,	O
respectively	O
.	O

Finally	O
,	O
one	O
of	O
key	O
reasons	O
we	O
are	O
able	O
to	O
employ	O
adversarial	B-Method
loss	E-Method
is	O
that	O
we	O
are	O
rendering	O
in	O
the	O
2D	O
image	O
space	O
,	O
rather	O
than	O
the	O
3D	O
vertices	O
space	O
or	O
unwrapped	O
texture	O
space	O
.	O

This	O
shows	O
the	O
necessity	O
and	O
importance	O
of	O
our	O
rendering	B-Method
layer	E-Method
.	O

Semi	B-Task
-	I-Task
Supervised	I-Task
Pre	I-Task
-	I-Task
Training	E-Task
.	O

Fully	O
unsupervised	B-Method
training	E-Method
using	O
only	O
the	O
mentioned	O
reconstruction	B-Method
and	I-Method
adversarial	I-Method
loss	E-Method
on	O
the	O
rendered	O
image	O
could	O
lead	O
to	O
a	O
degenerate	O
solution	O
,	O
since	O
the	O
initial	B-Method
estimation	E-Method
is	O
far	O
from	O
ideal	O
to	O
render	O
meaningful	O
images	O
.	O

Hence	O
,	O
we	O
introduce	O
pre	B-Method
-	I-Method
training	I-Method
loss	I-Method
functions	E-Method
to	O
guide	O
the	O
training	S-Task
in	O
the	O
early	O
iterations	O
.	O

With	O
face	B-Method
profiling	I-Method
technique	E-Method
,	O
Zhu	O
et	O
al	O
.	O

expands	O
the	O
300W	O
dataset	O
into	O
images	O
with	O
the	O
fitted	O
3DMM	S-Method
shape	O
and	O
projection	O
parameters	O
.	O

Given	O
and	O
,	O
we	O
create	O
the	O
pseudo	O
groundtruth	O
texture	O
by	O
referring	O
every	O
pixel	O
in	O
the	O
UV	O
space	O
back	O
to	O
the	O
input	O
image	O
,	O
i.e.	O
,	O
backward	O
of	O
our	O
rendering	B-Method
layer	E-Method
.	O

With	O
,	O
,	O
,	O
we	O
define	O
our	O
pre	B-Metric
-	I-Metric
training	I-Metric
loss	E-Metric
by	O
:	O
where	O
Due	O
to	O
the	O
pseudo	O
groundtruth	O
,	O
using	O
may	O
run	O
into	O
the	O
risk	O
that	O
our	O
solution	O
learns	O
to	O
mimic	O
the	O
linear	B-Method
model	E-Method
.	O

Thus	O
,	O
we	O
switch	O
to	O
the	O
loss	O
of	O
Eqn	O
.	O

[	O
reference	O
]	O
after	O
converges	O
.	O

Sparse	B-Task
Landmark	I-Task
Alignment	E-Task
.	O

To	O
help	O
to	O
better	O
learn	O
the	O
facial	O
shape	O
,	O
the	O
landmark	B-Task
loss	E-Task
can	O
be	O
an	O
auxiliary	B-Task
task	E-Task
.	O

where	O
is	O
the	O
manually	O
labeled	O
2D	O
landmark	O
locations	O
,	O
is	O
a	O
constant	O
-	O
dim	O
vector	O
storing	O
the	O
indexes	O
of	O
3D	O
vertices	O
corresponding	O
to	O
the	O
labeled	O
2D	O
landmarks	O
.	O

Unlike	O
the	O
three	O
losses	O
above	O
,	O
these	O
landmark	O
annotations	O
are	O
“	O
golden	O
”	O
groundtruth	O
,	O
and	O
hence	O
can	O
be	O
used	O
during	O
the	O
entire	O
training	B-Task
process	E-Task
.	O

Different	O
from	O
traditional	O
face	B-Task
alignment	E-Task
work	O
where	O
the	O
shape	O
bases	O
are	O
fixed	O
,	O
our	O
work	O
jointly	O
learns	O
the	O
bases	O
functions	O
(	O
i.e.	O
,	O
the	O
shape	B-Method
decoder	E-Method
)	O
as	O
well	O
.	O

Minimizing	B-Task
the	I-Task
landmark	I-Task
loss	E-Task
when	O
updating	O
only	O
moves	O
a	O
tiny	O
subset	O
of	O
vertices	O
,	O
since	O
our	O
is	O
a	O
MLP	S-Method
consisting	O
of	O
fully	B-Method
connected	I-Method
layers	E-Method
.	O

This	O
could	O
lead	O
to	O
unrealistic	O
shapes	O
.	O

Hence	O
,	O
when	O
optimizing	O
the	O
landmark	B-Task
loss	E-Task
,	O
we	O
fix	O
the	O
decoder	S-Method
and	O
only	O
update	O
the	O
encoder	S-Method
.	O

Note	O
that	O
the	O
estimated	O
groundtruth	O
in	O
and	O
the	O
landmarks	O
are	O
the	O
only	O
supervision	O
used	O
in	O
our	O
training	O
,	O
due	O
to	O
this	O
our	O
learning	O
is	O
considered	O
as	O
weakly	B-Task
supervised	E-Task
.	O

section	O
:	O
Experimental	O
Results	O
The	O
experiments	O
study	O
three	O
aspects	O
of	O
the	O
proposed	O
nonlinear	B-Method
3DMM	E-Method
,	O
in	O
terms	O
of	O
its	O
expressiveness	S-Metric
,	O
representation	B-Metric
power	E-Metric
,	O
and	O
applications	O
to	O
facial	B-Task
analysis	E-Task
.	O

Using	O
facial	O
mesh	O
triangle	O
definition	O
by	O
Basel	B-Method
Face	I-Method
Model	E-Method
(	O
BFM	S-Method
)	O
,	O
we	O
train	O
our	O
3DMM	S-Method
using	O
300W	O
-	O
LP	O
dataset	O
.	O

The	O
model	O
is	O
optimized	O
using	O
Adam	B-Method
optimizer	E-Method
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
when	O
minimizing	O
,	O
and	O
when	O
minimizing	O
.	O

We	O
set	O
the	O
following	O
parameters	O
:	O
,	O
,	O
.	O

values	O
are	O
set	O
to	O
make	O
losses	O
to	O
have	O
similar	O
magnitudes	O
.	O

subsection	O
:	O
Expressiveness	O
Exploring	B-Task
feature	I-Task
space	E-Task
.	O

We	O
use	O
the	O
entire	O
CelebA	O
dataset	O
with	O
k	O
images	O
to	O
feed	O
to	O
our	O
network	O
to	O
obtain	O
the	O
empirical	O
distribution	O
of	O
our	O
shape	O
and	O
texture	O
parameters	O
.	O

By	O
varying	O
the	O
mean	O
parameter	O
along	O
each	O
dimension	O
proportional	O
to	O
their	O
standard	O
deviations	O
,	O
we	O
can	O
get	O
a	O
sense	O
how	O
each	O
element	O
contributes	O
to	O
the	O
final	O
shape	O
and	O
texture	O
.	O

We	O
sort	O
elements	O
in	O
the	O
shape	O
parameter	O
based	O
on	O
their	O
differences	O
to	O
the	O
mean	O
3D	O
shape	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
four	O
examples	O
of	O
shape	O
changes	O
,	O
whose	O
differences	O
rank	O
No	O
.	O

,	O
,	O
,	O
and	O
among	O
elements	O
.	O

Most	O
of	O
top	O
changes	O
are	O
expression	O
related	O
.	O

Similarly	O
,	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
visualize	O
different	O
texture	O
changes	O
by	O
adjusting	O
only	O
one	O
element	O
of	O
off	O
the	O
mean	O
parameter	O
.	O

The	O
elements	O
with	O
the	O
same	O
ranks	O
as	O
the	O
shape	O
counterpart	O
are	O
selected	O
.	O

Attribute	B-Method
Embedding	E-Method
.	O

To	O
better	O
understand	O
different	O
shape	O
and	O
texture	O
instances	O
embedded	O
in	O
our	O
two	O
decoders	O
,	O
we	O
dig	O
into	O
their	O
attribute	O
meaning	O
.	O

For	O
a	O
given	O
attribute	O
,	O
e.g.	O
,	O
male	O
,	O
we	O
feed	O
images	O
with	O
that	O
attribute	O
into	O
our	O
encoder	O
to	O
obtain	O
two	O
sets	O
of	O
parameters	O
and	O
.	O

These	O
sets	O
represent	O
corresponding	O
empirical	O
distributions	O
of	O
the	O
data	O
in	O
the	O
low	O
dimensional	O
spaces	O
.	O

By	O
computing	O
the	O
mean	O
parameters	O
,	O
and	O
feed	O
into	O
their	O
respective	O
decoders	S-Method
,	O
we	O
can	O
reconstruct	O
the	O
mean	O
shape	O
and	O
texture	O
with	O
that	O
attribute	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
visualizes	O
the	O
reconstructed	O
shape	O
and	O
texture	O
related	O
to	O
some	O
attributes	O
.	O

Differences	O
among	O
attributes	O
present	O
in	O
both	O
shape	O
and	O
texture	O
.	O

subsection	O
:	O
Representation	B-Method
Power	E-Method
Texture	O
.	O

Given	O
a	O
face	O
image	O
,	O
assuming	O
we	O
know	O
the	O
groundtruth	O
shape	O
and	O
projection	O
parameters	O
,	O
we	O
can	O
unwarp	O
the	O
texture	O
into	O
the	O
UV	O
space	O
,	O
as	O
we	O
generate	O
“	O
pseudo	O
groundtruth	O
”	O
texture	O
in	O
the	O
weakly	B-Task
supervised	I-Task
step	E-Task
.	O

With	O
the	O
groundtruth	O
texture	O
,	O
by	O
using	O
gradient	B-Method
descent	E-Method
,	O
we	O
can	O
estimate	O
a	O
texture	O
parameter	O
whose	O
decoded	O
texture	O
matches	O
with	O
the	O
groundtruth	O
.	O

Alternatively	O
,	O
we	O
can	O
minimize	O
the	O
reconstruction	B-Metric
error	E-Metric
in	O
the	O
image	O
space	O
,	O
through	O
the	O
rendering	B-Method
layer	E-Method
with	O
the	O
groundtruth	O
and	O
.	O

Empirically	O
,	O
the	O
two	O
methods	O
give	O
similar	O
performances	O
but	O
we	O
choose	O
the	O
first	O
option	O
as	O
it	O
involves	O
only	O
one	O
warping	O
step	O
,	O
instead	O
of	O
rendering	O
in	O
every	O
optimization	B-Task
iteration	E-Task
.	O

For	O
the	O
linear	B-Method
model	E-Method
,	O
we	O
use	O
the	O
fitting	O
results	O
of	O
Basel	B-Method
texture	E-Method
and	O
Phong	B-Method
illumination	I-Method
model	E-Method
given	O
by	O
.	O

As	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
nonlinear	B-Method
texture	E-Method
is	O
closer	O
to	O
the	O
groundtruth	O
than	O
the	O
linear	B-Method
model	E-Method
,	O
especially	O
for	O
in	O
-	O
the	O
-	O
wild	O
images	O
(	O
the	O
first	O
two	O
rows	O
)	O
.	O

This	O
is	O
expected	O
since	O
the	O
linear	B-Method
model	E-Method
is	O
trained	O
with	O
controlled	O
images	O
.	O

Quantitatively	O
,	O
our	O
nonlinear	S-Method
model	O
has	O
significantly	O
lower	O
reconstruction	B-Metric
error	E-Metric
than	O
the	O
linear	B-Method
model	E-Method
(	O
vs.	O
,	O
as	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

3D	O
Shape	O
.	O

We	O
also	O
compare	O
the	O
power	O
of	O
nonlinear	S-Method
and	O
linear	B-Method
3DMM	E-Method
in	O
representing	O
real	B-Task
-	I-Task
world	I-Task
3D	I-Task
scans	E-Task
.	O

We	O
compare	O
with	O
BFM	S-Method
,	O
the	O
most	O
commonly	O
used	O
3DMM	S-Method
at	O
present	O
.	O

We	O
use	O
ten	O
3D	O
face	O
scans	O
provided	O
by	O
,	O
which	O
are	O
not	O
included	O
in	O
the	O
training	O
set	O
of	O
BFM	S-Method
.	O

As	O
these	O
face	O
meshes	O
are	O
already	O
registered	O
using	O
the	O
same	O
triangle	O
definition	O
with	O
BFM	S-Method
,	O
no	O
registration	S-Task
is	O
necessary	O
.	O

Given	O
the	O
groundtruth	O
shape	O
,	O
by	O
using	O
gradient	B-Method
descent	E-Method
,	O
we	O
can	O
estimate	O
a	O
shape	O
parameter	O
whose	O
decoded	O
shape	O
matches	O
the	O
groundtruth	O
.	O

We	O
define	O
matching	B-Metric
criteria	E-Metric
on	O
both	O
vertex	O
distances	O
and	O
surface	O
normal	O
direction	O
.	O

This	O
empirically	O
improves	O
fidelity	O
of	O
final	O
results	O
compared	O
to	O
only	O
optimizing	O
vertex	O
distances	O
.	O

Also	O
,	O
to	O
emphasize	O
the	O
compactness	O
of	O
nonlinear	S-Method
models	O
,	O
we	O
train	O
different	O
models	O
with	O
different	O
latent	O
space	O
sizes	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
visual	B-Metric
quality	E-Metric
of	O
two	O
models	O
’	O
reconstructions	O
.	O

As	O
we	O
can	O
see	O
,	O
our	O
reconstructions	O
closely	O
match	O
the	O
face	O
shapes	O
.	O

Meanwhile	O
the	O
linear	B-Method
model	E-Method
struggles	O
with	O
face	O
shapes	O
outside	O
its	O
PCA	O
span	O
.	O

To	O
quantify	O
the	O
difference	O
,	O
we	O
use	O
NME	S-Method
,	O
averaged	B-Metric
per	I-Metric
-	I-Metric
vertex	I-Metric
errors	E-Metric
between	O
the	O
recovered	O
and	O
groundtruth	O
shapes	O
,	O
normalized	O
by	O
inter	O
-	O
ocular	O
distances	O
.	O

Our	O
nonlinear	S-Method
model	O
has	O
a	O
significantly	O
smaller	O
reconstruction	B-Metric
error	E-Metric
than	O
the	O
linear	B-Method
model	E-Method
,	O
vs.	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Also	O
,	O
the	O
non	O
-	O
linear	B-Method
models	E-Method
are	O
more	O
compact	O
.	O

They	O
can	O
achieve	O
similar	O
performances	O
as	O
linear	B-Method
models	E-Method
whose	O
latent	O
spaceâs	O
sizes	O
doubled	O
.	O

subsection	O
:	O
Applications	O
Having	O
shown	O
the	O
capability	O
of	O
our	O
nonlinear	B-Method
3DMM	E-Method
(	O
i.e.	O
,	O
two	O
decoders	S-Method
)	O
,	O
now	O
we	O
demonstrate	O
the	O
applications	O
of	O
our	O
entire	O
network	O
,	O
which	O
has	O
the	O
additional	O
encoder	O
.	O

Many	O
applications	O
of	O
3DMM	S-Method
are	O
centered	O
on	O
its	O
ability	O
to	O
fit	O
to	O
2D	O
face	O
images	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
visualizes	O
our	O
3DMM	S-Method
fitting	O
results	O
on	O
CelebA	O
dataset	O
.	O

Our	O
encoder	S-Method
estimates	O
the	O
shape	O
,	O
texture	O
as	O
well	O
as	O
projection	O
parameter	O
.	O

We	O
can	O
recover	O
personal	O
facial	O
characteristic	O
in	O
both	O
shape	O
and	O
texture	O
.	O

Our	O
texture	O
can	O
have	O
variety	O
skin	O
color	O
or	O
facial	O
hair	O
,	O
which	O
is	O
normally	O
hard	O
to	O
be	O
recovered	O
by	O
linear	B-Method
3DMM	E-Method
.	O

2D	B-Task
Face	I-Task
Alignment	E-Task
.	O

Face	B-Task
alignment	E-Task
is	O
a	O
critical	O
step	O
for	O
any	O
facial	B-Task
analysis	I-Task
task	E-Task
such	O
as	O
face	B-Task
recognition	E-Task
.	O

With	O
enhancement	O
in	O
the	O
modeling	S-Task
,	O
we	O
hope	O
to	O
improve	O
this	O
task	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
compare	O
face	B-Task
alignment	E-Task
performance	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
SDM	S-Method
and	O
3DDFA	S-Method
,	O
on	O
the	O
AFLW2000	B-Material
dataset	E-Material
.	O

The	O
alignment	B-Metric
accuracy	E-Metric
is	O
evaluated	O
by	O
the	O
Normalized	B-Metric
Mean	I-Metric
Error	E-Metric
(	O
NME	S-Method
)	O
,	O
the	O
average	B-Metric
of	I-Metric
visible	I-Metric
landmark	I-Metric
error	E-Metric
normalized	O
by	O
the	O
bounding	O
box	O
size	O
.	O

Here	O
,	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
3DDFA	S-Method
is	O
a	O
cascade	B-Method
of	I-Method
CNNs	E-Method
that	O
iteratively	O
refines	O
its	O
estimation	O
in	O
multiple	O
steps	O
,	O
meanwhile	O
ours	O
is	O
a	O
single	O
-	O
pass	O
of	O
and	O
.	O

However	O
,	O
by	O
jointly	O
learning	O
model	B-Method
fitting	E-Method
with	O
3DMM	S-Method
,	O
our	O
network	O
can	O
surpass	O
’s	O
performance	O
,	O
as	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

Another	O
perspective	O
is	O
that	O
in	O
conventional	O
3DMM	S-Method
fitting	O
,	O
the	O
texture	O
is	O
used	O
as	O
the	O
input	O
to	O
regress	O
the	O
shape	O
parameter	O
,	O
while	O
ours	O
adopts	O
an	O
analysis	B-Method
-	I-Method
by	I-Method
-	I-Method
synthesis	I-Method
scheme	E-Method
and	O
texture	O
is	O
the	O
output	O
of	O
the	O
synthesis	O
.	O

Further	O
,	O
for	O
a	O
more	O
fair	O
comparison	O
of	O
nonlinear	S-Method
vs.	O
linear	B-Method
models	E-Method
,	O
we	O
train	O
an	O
encoder	S-Method
with	O
the	O
same	O
architecture	O
as	O
our	O
,	O
whose	O
output	O
parameter	O
will	O
multiple	O
with	O
the	O
linear	O
shape	O
bases	O
,	O
and	O
train	O
with	O
the	O
landmark	O
loss	O
function	O
(	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
.	O

Again	O
we	O
observe	O
the	O
higher	O
error	O
from	O
the	O
linear	B-Method
model	E-Method
-	O
based	O
fitting	O
.	O

3D	B-Task
Face	I-Task
Reconstruction	E-Task
.	O

We	O
compare	O
our	O
approach	O
to	O
recent	O
works	O
:	O
the	O
CNN	B-Method
-	I-Method
based	I-Method
iterative	I-Method
supervised	I-Method
regressor	E-Method
of	O
Richardson	O
et	O
al	O
.	O

and	O
unsupervised	B-Method
regressor	I-Method
method	E-Method

of	O
Tewari	O
et	O
al	O
.	O

.	O


The	O
work	O
by	O
Tewari	O
et	O
al	O
.	O

is	O
relevant	O
to	O
us	O
as	O
they	O
also	O
learn	O
to	O
fit	O
3DMM	S-Method
in	O
an	O
unsupervised	B-Method
fashion	E-Method
.	O

However	O
,	O
they	O
are	O
limited	O
to	O
linear	O
3DMM	S-Method
bases	O
,	O
which	O
of	O
course	O
are	O
not	O
jointly	O
trained	O
with	O
the	O
model	O
.	O

Also	O
,	O
we	O
only	O
compare	O
with	O
the	O
coarse	B-Method
network	E-Method
in	O
as	O
their	O
refinement	B-Method
network	E-Method
use	O
SfS	S-Method
,	O
which	O
leads	O
to	O
a	O
2.5D	B-Method
representation	E-Method
and	O
loses	O
correspondence	O
between	O
different	O
3D	O
shapes	O
.	O

This	O
is	O
orthogonal	O
to	O
our	O
approach	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
visual	O
comparison	O
.	O

Following	O
the	O
same	O
setting	O
in	O
,	O
we	O
also	O
quantitatively	O
compare	O
our	O
method	O
with	O
prior	O
works	O
on	O
subjects	O
of	O
FaceWarehouse	O
database	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
achieve	O
on	O
-	O
par	O
results	O
with	O
Garrido	O
et	O
al	O
.	O

,	O
an	O
offline	B-Method
optimization	I-Method
method	E-Method
,	O
while	O
surpassing	O
all	O
other	O
regression	B-Method
methods	E-Method
.	O

subsection	O
:	O
Ablation	O
on	O
Texture	B-Task
Learning	E-Task
With	O
great	O
representation	O
power	O
,	O
we	O
would	O
like	O
to	O
learn	O
a	O
realistic	O
texture	B-Method
model	E-Method
from	O
in	O
-	O
the	O
-	O
wild	O
images	O
.	O

The	O
rendering	B-Method
layer	E-Method
opens	O
a	O
possibility	O
to	O
apply	O
adversarial	O
loss	O
in	O
addition	O
to	O
global	O
loss	O
.	O

Using	O
a	O
global	B-Method
image	I-Method
-	I-Method
based	I-Method
discriminator	E-Method
is	O
redundant	O
as	O
the	O
global	O
structure	O
is	O
guaranteed	O
by	O
the	O
rendering	B-Method
layer	E-Method
.	O

Also	O
,	O
we	O
empirically	O
find	O
that	O
using	O
global	B-Method
image	I-Method
-	I-Method
based	I-Method
discriminator	E-Method
can	O
cause	O
severe	O
artifacts	O
in	O
the	O
resultant	O
texture	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
visualizes	O
outputs	O
of	O
our	O
network	O
with	O
different	O
options	O
of	O
adversarial	O
loss	O
.	O

Clearly	O
,	O
patchGAN	S-Method
offers	O
higher	O
realism	O
and	O
fewer	O
artifacts	O
.	O

section	O
:	O
Conclusions	O
Since	O
its	O
debut	O
in	O
1999	O
,	O
3DMM	S-Method
has	O
became	O
a	O
cornerstone	O
of	O
facial	B-Task
analysis	I-Task
research	E-Task
with	O
applications	O
to	O
many	O
problems	O
.	O

Despite	O
its	O
impact	O
,	O
it	O
has	O
drawbacks	O
in	O
requiring	O
training	O
data	O
of	O
3D	O
scans	O
,	O
learning	O
from	O
controlled	O
2D	O
images	O
,	O
and	O
limited	O
representation	O
power	O
due	O
to	O
linear	B-Method
bases	E-Method
.	O

These	O
drawbacks	O
could	O
be	O
formidable	O
when	O
fitting	O
3DMM	S-Method
to	O
unconstrained	B-Task
faces	E-Task
,	O
or	O
learning	O
3DMM	S-Method
for	O
generic	B-Task
objects	E-Task
such	O
as	O
shoes	O
.	O

This	O
paper	O
demonstrates	O
that	O
there	O
exists	O
an	O
alternative	O
approach	O
to	O
3DMM	S-Method
learning	O
,	O
where	O
a	O
nonlinear	B-Method
3DMM	E-Method
can	O
be	O
learned	O
from	O
a	O
large	O
set	O
of	O
unconstrained	O
face	O
images	O
without	O
collecting	O
3D	O
face	O
scans	O
.	O

Further	O
,	O
the	O
model	B-Method
fitting	I-Method
algorithm	E-Method
can	O
be	O
learnt	O
jointly	O
with	O
3DMM	S-Method
,	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
.	O

Our	O
experiments	O
cover	O
a	O
diverse	O
aspects	O
of	O
our	O
learnt	B-Method
model	E-Method
,	O
some	O
of	O
which	O
might	O
need	O
the	O
subjective	O
judgment	O
of	O
the	O
readers	O
.	O

We	O
hope	O
that	O
both	O
the	O
judgment	O
and	O
quantitative	O
results	O
could	O
be	O
viewed	O
under	O
the	O
context	O
that	O
,	O
unlike	O
linear	B-Method
3DMM	E-Method
,	O
no	O
genuine	O
3D	O
scans	O
are	O
used	O
in	O
our	O
learning	O
.	O

Finally	O
,	O
we	O
believe	O
that	O
unsupervisedly	B-Method
learning	I-Method
3D	I-Method
models	E-Method
from	O
large	B-Task
-	I-Task
scale	I-Task
in	I-Task
-	I-Task
the	I-Task
-	I-Task
wild	I-Task
2D	I-Task
images	E-Task
is	O
one	O
promising	O
research	O
direction	O
.	O

This	O
work	O
is	O
one	O
step	O
along	O
this	O
direction	O
.	O

bibliography	O
:	O
References	O
Neural	B-Task
Machine	I-Task
Translation	I-Task
of	I-Task
Rare	I-Task
Words	E-Task
with	O
Subword	B-Method
Units	E-Method
section	O
:	O
Abstract	O
Neural	B-Task
machine	I-Task
translation	E-Task
(	O
NMT	S-Task
)	O
models	O
typically	O
operate	O
with	O
a	O
fixed	O
vocabulary	O
,	O
but	O
translation	S-Task
is	O
an	O
open	B-Task
-	I-Task
vocabulary	I-Task
problem	E-Task
.	O

Previous	O
work	O
addresses	O
the	O
translation	S-Task
of	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
by	O
backing	O
off	O
to	O
a	O
dictionary	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
simpler	O
and	O
more	O
effective	O
approach	O
,	O
making	O
the	O
NMT	S-Task
model	O
capable	O
of	O
open	O
-	O
vocabulary	O
translation	S-Task
by	O
encoding	O
rare	O
and	O
unknown	O
words	O
as	O
sequences	O
of	O
subword	O
units	O
.	O

This	O
is	O
based	O
on	O
the	O
intuition	O
that	O
various	O
word	O
classes	O
are	O
translatable	O
via	O
smaller	O
units	O
than	O
words	O
,	O
for	O
instance	O
names	O
(	O
via	O
character	O
copying	O
or	O
transliteration	O
)	O
,	O
compounds	O
(	O
via	O
compositional	O
translation	S-Task
)	O
,	O
and	O
cognates	O
and	O
loanwords	O
(	O
via	O
phonological	O
and	O
morphological	O
transformations	O
)	O
.	O

We	O
discuss	O
the	O
suitability	O
of	O
different	O
word	B-Method
segmentation	I-Method
techniques	E-Method
,	O
including	O
simple	O
character	B-Method
ngram	I-Method
models	E-Method
and	O
a	O
segmentation	S-Method
based	O
on	O
the	O
byte	B-Method
pair	I-Method
encoding	I-Method
compression	I-Method
algorithm	E-Method
,	O
and	O
empirically	O
show	O
that	O
subword	B-Method
models	E-Method
improve	O
over	O
a	O
back	B-Method
-	I-Method
off	I-Method
dictionary	I-Method
baseline	E-Method
for	O
the	O
WMT	B-Material
15	I-Material
translation	I-Material
tasks	I-Material
English→German	E-Material
and	O
English→Russian	S-Material
by	O
up	O
to	O
1.1	O
and	O
1.3	O
BLEU	S-Metric
,	O
respectively	O
.	O

section	O
:	O
Introduction	O
Neural	B-Task
machine	I-Task
translation	E-Task
has	O
recently	O
shown	O
impressive	O
results	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
the	O
translation	S-Task
of	O
rare	O
words	O
is	O
an	O
open	O
problem	O
.	O

The	O
vocabulary	O
of	O
neural	B-Method
models	E-Method
is	O
typically	O
limited	O
to	O
30	O
000	O
-	O
50	O
000	O
words	O
,	O
but	O
translation	S-Task
is	O
an	O
open	B-Task
-	I-Task
vocabulary	I-Task
prob	I-Task
-	E-Task
The	O
research	O
presented	O
in	O
this	O
publication	O
was	O
conducted	O
in	O
cooperation	O
with	O
Samsung	O
Electronics	O
Polska	O
sp	O
.	O

z	O
o.o	O
.	O

-	O
Samsung	O
R	O
&	O
D	O
Institute	O
Poland	O
.	O

lem	S-Method
,	O
and	O
especially	O
for	O
languages	O
with	O
productive	B-Task
word	I-Task
formation	I-Task
processes	E-Task
such	O
as	O
agglutination	S-Task
and	O
compounding	S-Method
,	O
translation	S-Task
models	O
require	O
mechanisms	O
that	O
go	O
below	O
the	O
word	O
level	O
.	O

As	O
an	O
example	O
,	O
consider	O
compounds	O
such	O
as	O
the	O
German	O
Abwasser|behandlungs|anlange	O
'	O
sewage	O
water	O
treatment	O
plant	O
'	O
,	O
for	O
which	O
a	O
segmented	B-Method
,	I-Method
variable	I-Method
-	I-Method
length	I-Method
representation	E-Method
is	O
intuitively	O
more	O
appealing	O
than	O
encoding	O
the	O
word	O
as	O
a	O
fixed	O
-	O
length	O
vector	O
.	O

For	O
word	O
-	O
level	O
NMT	S-Task
models	O
,	O
the	O
translation	S-Task
of	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
has	O
been	O
addressed	O
through	O
a	O
back	O
-	O
off	O
to	O
a	O
dictionary	B-Method
look	I-Method
-	I-Method
up	E-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

We	O
note	O
that	O
such	O
techniques	O
make	O
assumptions	O
that	O
often	O
do	O
not	O
hold	O
true	O
in	O
practice	O
.	O

For	O
instance	O
,	O
there	O
is	O
not	O
always	O
a	O
1	O
-	O
to	O
-	O
1	O
correspondence	O
between	O
source	O
and	O
target	O
words	O
because	O
of	O
variance	O
in	O
the	O
degree	O
of	O
morphological	O
synthesis	O
between	O
languages	O
,	O
like	O
in	O
our	O
introductory	O
compounding	O
example	O
.	O

Also	O
,	O
word	B-Method
-	I-Method
level	I-Method
models	E-Method
are	O
unable	O
to	O
translate	O
or	O
generate	O
unseen	O
words	O
.	O

Copying	O
unknown	O
words	O
into	O
the	O
target	O
text	O
,	O
as	O
done	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
is	O
a	O
reasonable	O
strategy	O
for	O
names	S-Task
,	O
but	O
morphological	O
changes	O
and	O
transliteration	S-Task
is	O
often	O
required	O
,	O
especially	O
if	O
alphabets	O
differ	O
.	O

We	O
investigate	O
NMT	S-Task
models	O
that	O
operate	O
on	O
the	O
level	O
of	O
subword	O
units	O
.	O

Our	O
main	O
goal	O
is	O
to	O
model	O
open	O
-	O
vocabulary	O
translation	S-Task
in	O
the	O
NMT	S-Task
network	O
itself	O
,	O
without	O
requiring	O
a	O
back	B-Method
-	I-Method
off	I-Method
model	E-Method
for	O
rare	O
words	O
.	O

In	O
addition	O
to	O
making	O
the	O
translation	B-Task
process	E-Task
simpler	O
,	O
we	O
also	O
find	O
that	O
the	O
subword	B-Method
models	E-Method
achieve	O
better	O
accuracy	S-Metric
for	O
the	O
translation	S-Task
of	O
rare	O
words	O
than	O
large	B-Method
-	I-Method
vocabulary	I-Method
models	E-Method
and	O
back	B-Method
-	I-Method
off	I-Method
dictionaries	E-Method
,	O
and	O
are	O
able	O
to	O
productively	O
generate	O
new	O
words	O
that	O
were	O
not	O
seen	O
at	O
training	O
time	O
.	O

Our	O
analysis	O
shows	O
that	O
the	O
neural	B-Method
networks	E-Method
are	O
able	O
to	O
learn	O
compounding	S-Task
and	O
transliteration	S-Task
from	O
subword	B-Method
representations	E-Method
.	O

This	O
paper	O
has	O
two	O
main	O
contributions	O
:	O
•	O
We	O
show	O
that	O
open	O
-	O
vocabulary	O
neural	O
machine	O
translation	S-Task
is	O
possible	O
by	O
encoding	O
(	O
rare	O
)	O
words	O
via	O
subword	O
units	O
.	O

We	O
find	O
our	O
architecture	O
simpler	O
and	O
more	O
effective	O
than	O
using	O
large	O
vocabularies	O
and	O
back	O
-	O
off	O
dictionaries	O
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

•	O
We	O
adapt	O
byte	B-Method
pair	I-Method
encoding	E-Method
(	O
BPE	S-Method
)	O
[	O
reference	O
]	O
,	O
a	O
compression	B-Method
algorithm	E-Method
,	O
to	O
the	O
task	O
of	O
word	B-Task
segmentation	E-Task
.	O

BPE	S-Method
allows	O
for	O
the	O
representation	O
of	O
an	O
open	O
vocabulary	O
through	O
a	O
fixed	O
-	O
size	O
vocabulary	O
of	O
variable	O
-	O
length	O
character	O
sequences	O
,	O
making	O
it	O
a	O
very	O
suitable	O
word	B-Method
segmentation	I-Method
strategy	E-Method
for	O
neural	B-Method
network	I-Method
models	E-Method
.	O

section	O
:	O
Neural	B-Task
Machine	I-Task
Translation	E-Task
We	O
follow	O
the	O
neural	O
machine	O
translation	S-Task
architecture	O
by	O
[	O
reference	O
]	O
,	O
which	O
we	O
will	O
briefly	O
summarize	O
here	O
.	O

However	O
,	O
we	O
note	O
that	O
our	O
approach	O
is	O
not	O
specific	O
to	O
this	O
architecture	O
.	O

The	O
neural	O
machine	O
translation	S-Task
system	O
is	O
implemented	O
as	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
network	E-Method
with	O
recurrent	B-Method
neural	I-Method
networks	E-Method
.	O

The	O
encoder	S-Method
is	O
a	O
bidirectional	B-Method
neural	I-Method
network	E-Method
with	O
gated	B-Method
recurrent	I-Method
units	E-Method
[	O
reference	O
]	O
that	O
reads	O
an	O
input	O
sequence	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
m	O
)	O
and	O
calculates	O
a	O
forward	O
sequence	O
of	O
hidden	O
states	O
(	O
−	O
→	O
h	O
1	O
,	O
...	O
,	O
−	O
→	O
h	O
m	O
)	O
,	O
and	O
a	O
backward	O
sequence	O
The	O
hidden	O
states	O
−	O
→	O
h	O
j	O
and	O
←	O
−	O
h	O
j	O
are	O
concatenated	O
to	O
obtain	O
the	O
annotation	O
vector	O
h	O
j	O
.	O

The	O
decoder	S-Method
is	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
that	O
predicts	O
a	O
target	O
sequence	O
y	O
=	O
(	O
y	O
1	O
,	O
...	O
,	O
y	O
n	O
)	O
.	O

Each	O
word	O
y	O
i	O
is	O
predicted	O
based	O
on	O
a	O
recurrent	O
hidden	O
state	O
s	O
i	O
,	O
the	O
previously	O
predicted	O
word	O
y	O
i−1	O
,	O
and	O
a	O
context	O
vector	O
c	O
i	O
.	O

c	O
i	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
annotations	O
h	O
j	O
.	O

The	O
weight	O
of	O
each	O
annotation	O
h	O
j	O
is	O
computed	O
through	O
an	O
alignment	B-Method
model	I-Method
α	I-Method
ij	E-Method
,	O
which	O
models	O
the	O
probability	O
that	O
y	O
i	O
is	O
aligned	O
to	O
x	O
j	O
.	O

The	O
alignment	B-Method
model	E-Method
is	O
a	O
singlelayer	B-Method
feedforward	I-Method
neural	I-Method
network	E-Method
that	O
is	O
learned	O
jointly	O
with	O
the	O
rest	O
of	O
the	O
network	O
through	O
backpropagation	S-Method
.	O

A	O
detailed	O
description	O
can	O
be	O
found	O
in	O
[	O
reference	O
]	O
.	O

Training	O
is	O
performed	O
on	O
a	O
parallel	O
corpus	O
with	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
.	O

For	O
translation	S-Task
,	O
a	O
beam	B-Method
search	E-Method
with	O
small	O
beam	O
size	O
is	O
employed	O
.	O

section	O
:	O
Subword	B-Task
Translation	E-Task
The	O
main	O
motivation	O
behind	O
this	O
paper	O
is	O
that	O
the	O
translation	S-Task
of	O
some	O
words	O
is	O
transparent	O
in	O
that	O
they	O
are	O
translatable	O
by	O
a	O
competent	O
translator	O
even	O
if	O
they	O
are	O
novel	O
to	O
him	O
or	O
her	O
,	O
based	O
on	O
a	O
translation	S-Task
of	O
known	O
subword	O
units	O
such	O
as	O
morphemes	O
or	O
phonemes	O
.	O

Word	O
categories	O
whose	O
translation	S-Task
is	O
potentially	O
transparent	O
include	O
:	O
•	O
named	O
entities	O
.	O

Between	O
languages	O
that	O
share	O
an	O
alphabet	O
,	O
names	O
can	O
often	O
be	O
copied	O
from	O
source	O
to	O
target	O
text	O
.	O

Transcription	S-Task
or	O
transliteration	S-Task
may	O
be	O
required	O
,	O
especially	O
if	O
the	O
alphabets	O
or	O
syllabaries	O
differ	O
.	O

Example	O
:	O
Barack	O
Obama	O
(	O
English	O
;	O
German	O
)	O
Барак	O
Обама	O
(	O
Russian	O
)	O
バラク・オバマ	O
(	O
ba	O
-	O
ra	O
-	O
ku	O
o	O
-	O
ba	O
-	O
ma	O
)	O
(	O
Japanese	O
)	O
•	O
cognates	O
and	O
loanwords	O
.	O

Cognates	O
and	O
loanwords	O
with	O
a	O
common	O
origin	O
can	O
differ	O
in	O
regular	O
ways	O
between	O
languages	O
,	O
so	O
that	O
character	O
-	O
level	O
translation	S-Task
rules	O
are	O
sufficient	O
[	O
reference	O
]	O
.	O

Example	O
:	O
•	O
morphologically	O
complex	O
words	O
.	O

Words	O
containing	O
multiple	O
morphemes	O
,	O
for	O
instance	O
formed	O
via	O
compounding	O
,	O
affixation	O
,	O
or	O
inflection	O
,	O
may	O
be	O
translatable	O
by	O
translating	O
the	O
morphemes	O
separately	O
.	O

Example	O
:	O
solar	B-Task
system	E-Task
(	O
English	O
)	O
Sonnensystem	O
(	O
Sonne	B-Method
+	I-Method
System	E-Method
)	O
(	O
German	O
)	O
Naprendszer	O
(	O
Nap	O
+	O
Rendszer	O
)	O
(	O
Hungarian	O
)	O
In	O
an	O
analysis	O
of	O
100	O
rare	O
tokens	O
(	O
not	O
among	O
the	O
50	O
000	O
most	O
frequent	O
types	O
)	O
in	O
our	O
German	O
training	O
data	O
1	O
,	O
the	O
majority	O
of	O
tokens	O
are	O
potentially	O
translatable	O
from	O
English	O
through	O
smaller	O
units	O
.	O

We	O
find	O
56	O
compounds	O
,	O
21	O
names	O
,	O
6	O
loanwords	O
with	O
a	O
common	O
origin	O
(	O
emancipate→emanzipieren	O
)	O
,	O
5	O
cases	O
of	O
transparent	O
affixation	O
(	O
sweetish	O
'	O
sweet	O
'	O
+	O
'	O
-	O
ish	O
'	O
→	O
süßlich	O
'	O
süß	O
'	O
+	O
'	O
-	O
lich	O
'	O
)	O
,	O
1	O
number	O
and	O
1	O
computer	O
language	O
identifier	O
.	O

Our	O
hypothesis	O
is	O
that	O
a	O
segmentation	O
of	O
rare	O
words	O
into	O
appropriate	O
subword	O
units	O
is	O
sufficient	O
to	O
allow	O
for	O
the	O
neural	O
translation	S-Task
network	O
to	O
learn	O
transparent	B-Task
translations	E-Task
,	O
and	O
to	O
generalize	O
this	O
knowledge	O
to	O
translate	O
and	O
produce	O
unseen	O
words	O
.	O

[	O
reference	O
]	O
We	O
provide	O
empirical	O
support	O
for	O
this	O
hy	O
-	O
pothesis	O
in	O
Sections	O
4	O
and	O
5	O
.	O

First	O
,	O
we	O
discuss	O
different	O
subword	B-Method
representations	E-Method
.	O

section	O
:	O
Related	O
Work	O
For	O
Statistical	B-Task
Machine	I-Task
Translation	E-Task
(	O
SMT	S-Method
)	O
,	O
the	O
translation	S-Task
of	O
unknown	O
words	O
has	O
been	O
the	O
subject	O
of	O
intensive	O
research	O
.	O

A	O
large	O
proportion	O
of	O
unknown	O
words	O
are	O
names	O
,	O
which	O
can	O
just	O
be	O
copied	O
into	O
the	O
target	O
text	O
if	O
both	O
languages	O
share	O
an	O
alphabet	O
.	O

If	O
alphabets	O
differ	O
,	O
transliteration	O
is	O
required	O
[	O
reference	O
]	O
.	O

Character	O
-	O
based	O
translation	S-Task
has	O
also	O
been	O
investigated	O
with	O
phrase	B-Method
-	I-Method
based	I-Method
models	E-Method
,	O
which	O
proved	O
especially	O
successful	O
for	O
closely	O
related	O
languages	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

The	O
segmentation	B-Task
of	I-Task
morphologically	I-Task
complex	I-Task
words	E-Task
such	O
as	O
compounds	O
is	O
widely	O
used	O
for	O
SMT	S-Method
,	O
and	O
various	O
algorithms	O
for	O
morpheme	B-Task
segmentation	E-Task
have	O
been	O
investigated	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Segmentation	B-Method
algorithms	E-Method
commonly	O
used	O
for	O
phrase	O
-	O
based	O
SMT	S-Method
tend	O
to	O
be	O
conservative	O
in	O
their	O
splitting	O
decisions	O
,	O
whereas	O
we	O
aim	O
for	O
an	O
aggressive	B-Method
segmentation	E-Method
that	O
allows	O
for	O
open	O
-	O
vocabulary	O
translation	S-Task
with	O
a	O
compact	O
network	O
vocabulary	O
,	O
and	O
without	O
having	O
to	O
resort	O
to	O
back	O
-	O
off	O
dictionaries	O
.	O

The	O
best	O
choice	O
of	O
subword	O
units	O
may	O
be	O
taskspecific	O
.	O

For	O
speech	B-Task
recognition	E-Task
,	O
phone	B-Method
-	I-Method
level	I-Method
language	I-Method
models	E-Method
have	O
been	O
used	O
[	O
reference	O
]	O
.	O

[	O
reference	O
]	O
investigate	O
subword	B-Method
language	I-Method
models	E-Method
,	O
and	O
propose	O
to	O
use	O
syllables	O
.	O

For	O
multilingual	B-Task
segmentation	I-Task
tasks	E-Task
,	O
multilingual	B-Method
algorithms	E-Method
have	O
been	O
proposed	O
[	O
reference	O
]	O
.	O

We	O
find	O
these	O
intriguing	O
,	O
but	O
inapplicable	O
at	O
test	O
time	O
.	O

Various	O
techniques	O
have	O
been	O
proposed	O
to	O
produce	O
fixed	O
-	O
length	O
continuous	O
word	O
vectors	O
based	O
on	O
characters	O
or	O
morphemes	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

An	O
effort	O
to	O
apply	O
such	O
techniques	O
to	O
NMT	S-Task
,	O
parallel	O
to	O
ours	O
,	O
has	O
found	O
no	O
significant	O
improvement	O
over	O
word	B-Method
-	I-Method
based	I-Method
approaches	E-Method
[	O
reference	O
]	O
.	O

One	O
technical	O
difference	O
from	O
our	O
work	O
is	O
that	O
the	O
attention	B-Method
mechanism	E-Method
still	O
operates	O
on	O
the	O
level	O
of	O
words	O
in	O
the	O
model	O
by	O
[	O
reference	O
]	O
,	O
and	O
that	O
the	O
representation	O
of	O
each	O
word	O
is	O
fixed	O
-	O
length	O
.	O

We	O
expect	O
that	O
the	O
attention	B-Method
mechanism	E-Method
benefits	O
from	O
our	O
variable	B-Method
-	I-Method
length	I-Method
representation	E-Method
:	O
the	O
network	O
can	O
learn	O
to	O
place	O
attention	O
on	O
different	O
subword	O
units	O
at	O
each	O
step	O
.	O

Recall	O
our	O
introductory	O
example	O
Abwasserbehandlungsanlange	O
,	O
for	O
which	O
a	O
subword	B-Method
segmentation	E-Method
avoids	O
the	O
information	O
bottleneck	O
of	O
a	O
fixed	B-Method
-	I-Method
length	I-Method
representation	E-Method
.	O

Neural	B-Task
machine	I-Task
translation	E-Task
differs	O
from	O
phrasebased	B-Method
methods	E-Method
in	O
that	O
there	O
are	O
strong	O
incentives	O
to	O
minimize	O
the	O
vocabulary	O
size	O
of	O
neural	B-Method
models	E-Method
to	O
increase	O
time	B-Metric
and	I-Metric
space	I-Metric
efficiency	E-Metric
,	O
and	O
to	O
allow	O
for	O
translation	S-Task
without	O
back	B-Method
-	I-Method
off	I-Method
models	E-Method
.	O

At	O
the	O
same	O
time	O
,	O
we	O
also	O
want	O
a	O
compact	O
representation	O
of	O
the	O
text	O
itself	O
,	O
since	O
an	O
increase	O
in	O
text	O
length	O
reduces	O
efficiency	O
and	O
increases	O
the	O
distances	O
over	O
which	O
neural	B-Method
models	E-Method
need	O
to	O
pass	O
information	O
.	O

A	O
simple	O
method	O
to	O
manipulate	O
the	O
trade	O
-	O
off	O
between	O
vocabulary	B-Metric
size	E-Metric
and	O
text	O
size	O
is	O
to	O
use	O
shortlists	O
of	O
unsegmented	O
words	O
,	O
using	O
subword	O
units	O
only	O
for	O
rare	O
words	O
.	O

As	O
an	O
alternative	O
,	O
we	O
propose	O
a	O
segmentation	B-Method
algorithm	E-Method
based	O
on	O
byte	B-Method
pair	I-Method
encoding	E-Method
(	O
BPE	S-Method
)	O
,	O
which	O
lets	O
us	O
learn	O
a	O
vocabulary	O
that	O
provides	O
a	O
good	O
compression	B-Metric
rate	E-Metric
of	O
the	O
text	O
.	O

section	O
:	O
Byte	B-Method
Pair	I-Method
Encoding	E-Method
(	O
BPE	S-Method
)	O
Byte	B-Method
Pair	I-Method
Encoding	E-Method
(	O
BPE	S-Method
)	O
[	O
reference	O
]	O
)	O
is	O
a	O
simple	O
data	B-Method
compression	I-Method
technique	E-Method
that	O
iteratively	O
replaces	O
the	O
most	O
frequent	O
pair	O
of	O
bytes	O
in	O
a	O
sequence	O
with	O
a	O
single	O
,	O
unused	O
byte	O
.	O

We	O
adapt	O
this	O
algorithm	O
for	O
word	B-Task
segmentation	E-Task
.	O

Instead	O
of	O
merging	O
frequent	O
pairs	O
of	O
bytes	O
,	O
we	O
merge	O
characters	O
or	O
character	O
sequences	O
.	O

Firstly	O
,	O
we	O
initialize	O
the	O
symbol	O
vocabulary	O
with	O
the	O
character	O
vocabulary	O
,	O
and	O
represent	O
each	O
word	O
as	O
a	O
sequence	O
of	O
characters	O
,	O
plus	O
a	O
special	O
end	O
-	O
ofword	O
symbol	O
'	O
·	O
'	O
,	O
which	O
allows	O
us	O
to	O
restore	O
the	O
original	O
tokenization	O
after	O
translation	S-Task
.	O

We	O
iteratively	O
count	O
all	O
symbol	O
pairs	O
and	O
replace	O
each	O
occurrence	O
of	O
the	O
most	O
frequent	O
pair	O
(	O
'	O
A	O
'	O
,	O
'	O
B	O
'	O
)	O
with	O
a	O
new	O
symbol	O
'	O
AB	O
'	O
.	O

Each	O
merge	B-Method
operation	E-Method
produces	O
a	O
new	O
symbol	O
which	O
represents	O
a	O
character	O
n	O
-	O
gram	O
.	O

Frequent	O
character	O
n	O
-	O
grams	O
(	O
or	O
whole	O
words	O
)	O
are	O
eventually	O
merged	O
into	O
a	O
single	O
symbol	O
,	O
thus	O
BPE	S-Method
requires	O
no	O
shortlist	O
.	O

The	O
final	O
symbol	B-Metric
vocabulary	I-Metric
size	E-Metric
is	O
equal	O
to	O
the	O
size	O
of	O
the	O
initial	O
vocabulary	O
,	O
plus	O
the	O
number	O
of	O
merge	O
operations	O
-	O
the	O
latter	O
is	O
the	O
only	O
hyperparameter	O
of	O
the	O
algorithm	O
.	O

For	O
efficiency	O
,	O
we	O
do	O
not	O
consider	O
pairs	O
that	O
cross	O
word	O
boundaries	O
.	O

The	O
algorithm	O
can	O
thus	O
be	O
run	O
on	O
the	O
dictionary	O
extracted	O
from	O
a	O
text	O
,	O
with	O
each	O
word	O
being	O
weighted	O
by	O
its	O
frequency	O
.	O

A	O
minimal	O
Python	B-Method
implementation	E-Method
is	O
shown	O
in	O
Al	O
-	O
'	O
n	O
e	O
w	O
e	O
s	O
t	O
<	O
/	O
w>':6	O
,	O
'	O
w	O
i	O
d	O
e	O
s	O
t	O
<	O
/	O
w>':3	O
}	O
num_merges	S-Method
=	O
10	O
for	O
i	O
in	O
range	O
(	O
num_merges	O
)	O
:	O
pairs	O
=	O
get_stats	O
(	O
vocab	O
)	O
best	O
=	O
max	B-Method
(	I-Method
pairs	E-Method
,	O
key	O
=	O
pairs.get	O
)	O
vocab	O
=	O
merge_vocab	O
(	O
best	O
,	O
vocab	O
)	O
Figure	O
1	O
:	O
BPE	S-Method
merge	O
operations	O
learned	O
from	O
dictionary	O
{	O
'	O
low	O
'	O
,	O
'	O
lowest	O
'	O
,	O
'	O
newer	O
'	O
,	O
'	O
wider'}.	O
gorithm	O
1	O
.	O

In	O
practice	O
,	O
we	O
increase	O
efficiency	O
by	O
indexing	O
all	O
pairs	O
,	O
and	O
updating	O
data	B-Method
structures	E-Method
incrementally	O
.	O

The	O
main	O
difference	O
to	O
other	O
compression	B-Method
algorithms	E-Method
,	O
such	O
as	O
Huffman	B-Method
encoding	E-Method
,	O
which	O
have	O
been	O
proposed	O
to	O
produce	O
a	O
variable	B-Method
-	I-Method
length	I-Method
encoding	I-Method
of	I-Method
words	E-Method
for	O
NMT	S-Task
[	O
reference	O
]	O
,	O
is	O
that	O
our	O
symbol	O
sequences	O
are	O
still	O
interpretable	O
as	O
subword	O
units	O
,	O
and	O
that	O
the	O
network	O
can	O
generalize	O
to	O
translate	O
and	O
produce	O
new	O
words	O
(	O
unseen	O
at	O
training	O
time	O
)	O
on	O
the	O
basis	O
of	O
these	O
subword	O
units	O
.	O

Figure	O
1	O
shows	O
a	O
toy	O
example	O
of	O
learned	O
BPE	S-Method
operations	O
.	O

At	O
test	O
time	O
,	O
we	O
first	O
split	O
words	O
into	O
sequences	O
of	O
characters	O
,	O
then	O
apply	O
the	O
learned	O
operations	O
to	O
merge	O
the	O
characters	O
into	O
larger	O
,	O
known	O
symbols	O
.	O

This	O
is	O
applicable	O
to	O
any	O
word	O
,	O
and	O
allows	O
for	O
open	B-Method
-	I-Method
vocabulary	I-Method
networks	E-Method
with	O
fixed	O
symbol	O
vocabularies	O
.	O

[	O
reference	O
]	O
In	O
our	O
example	O
,	O
the	O
OOV	O
'	O
lower	O
'	O
would	O
be	O
segmented	O
into	O
'	O
low	O
er	O
·	O
'	O
.	O

We	O
evaluate	O
two	O
methods	O
of	O
applying	O
BPE	S-Method
:	O
learning	O
two	O
independent	B-Method
encodings	E-Method
,	O
one	O
for	O
the	O
source	O
,	O
one	O
for	O
the	O
target	O
vocabulary	O
,	O
or	O
learning	O
the	O
encoding	O
on	O
the	O
union	O
of	O
the	O
two	O
vocabularies	O
(	O
which	O
we	O
call	O
joint	O
BPE	S-Method
)	O
.	O

[	O
reference	O
]	O
The	O
former	O
has	O
the	O
advantage	O
of	O
being	O
more	O
compact	O
in	O
terms	O
of	O
text	O
and	O
vocabulary	B-Metric
size	E-Metric
,	O
and	O
having	O
stronger	O
guarantees	O
that	O
each	O
subword	O
unit	O
has	O
been	O
seen	O
in	O
the	O
training	O
text	O
of	O
the	O
respective	O
language	O
,	O
whereas	O
the	O
latter	O
improves	O
consistency	O
between	O
the	O
source	O
and	O
the	O
target	O
segmentation	O
.	O

If	O
we	O
apply	O
BPE	S-Method
independently	O
,	O
the	O
same	O
name	O
may	O
be	O
segmented	O
differently	O
in	O
the	O
two	O
languages	O
,	O
which	O
makes	O
it	O
harder	O
for	O
the	O
neural	B-Method
models	E-Method
to	O
learn	O
a	O
mapping	O
between	O
the	O
subword	O
units	O
.	O

To	O
increase	O
the	O
consistency	O
between	O
English	O
and	O
Russian	O
segmentation	O
despite	O
the	O
differing	O
alphabets	O
,	O
we	O
transliterate	O
the	O
Russian	O
vocabulary	O
into	O
Latin	O
characters	O
with	O
ISO	O
-	O
9	O
to	O
learn	O
the	O
joint	O
BPE	S-Method
encoding	O
,	O
then	O
transliterate	O
the	O
BPE	S-Method
merge	O
operations	O
back	O
into	O
Cyrillic	O
to	O
apply	O
them	O
to	O
the	O
Russian	B-Material
training	I-Material
text	E-Material
.	O

5	O
section	O
:	O
Evaluation	O
We	O
aim	O
to	O
answer	O
the	O
following	O
empirical	O
questions	O
:	O
•	O
Can	O
we	O
improve	O
the	O
translation	S-Task
of	O
rare	O
and	O
unseen	O
words	O
in	O
neural	O
machine	O
translation	S-Task
by	O
representing	O
them	O
via	O
subword	B-Method
units	E-Method
?	O
•	O
Which	O
segmentation	O
into	O
subword	O
units	O
performs	O
best	O
in	O
terms	O
of	O
vocabulary	B-Metric
size	E-Metric
,	O
text	B-Metric
size	E-Metric
,	O
and	O
translation	S-Task
quality	O
?	O
We	O
perform	O
experiments	O
on	O
data	O
from	O
the	O
shared	O
translation	S-Task
task	O
of	O
WMT	B-Material
2015	E-Material
.	O

For	O
English→German	S-Material
,	O
our	O
training	O
set	O
consists	O
of	O
4.2	O
million	O
sentence	O
pairs	O
,	O
or	O
approximately	O
100	O
million	O
tokens	O
.	O

For	O
English→Russian	S-Material
,	O
the	O
training	O
set	O
consists	O
of	O
2.6	O
million	O
sentence	O
pairs	O
,	O
or	O
approximately	O
50	O
million	O
tokens	O
.	O

We	O
tokenize	O
and	O
truecase	O
the	O
data	O
with	O
the	O
scripts	O
provided	O
in	O
Moses	O
[	O
reference	O
]	O
.	O

We	O
use	O
newstest2013	O
as	O
development	O
set	O
,	O
and	O
report	O
results	O
on	O
newstest2014	O
and	O
newstest2015	O
.	O

We	O
report	O
results	O
with	O
BLEU	S-Metric
(	O
mteval	O
-	O
v13a.pl	O
)	O
,	O
and	O
CHRF3	S-Method
[	O
reference	O
]	O
,	O
a	O
character	B-Metric
n	I-Metric
-	I-Metric
gram	I-Metric
F	I-Metric
3	I-Metric
score	E-Metric
which	O
was	O
found	O
to	O
correlate	O
well	O
with	O
human	O
judgments	O
,	O
especially	O
for	O
translations	O
out	O
of	O
English	O
[	O
reference	O
]	O
.	O

Since	O
our	O
main	O
claim	O
is	O
concerned	O
with	O
the	O
translation	S-Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
we	O
report	O
separate	O
statistics	O
for	O
these	O
.	O

We	O
measure	O
these	O
through	O
unigram	B-Metric
F	I-Metric
1	E-Metric
,	O
which	O
we	O
calculate	O
as	O
the	O
harmonic	B-Metric
mean	I-Metric
of	I-Metric
clipped	I-Metric
unigram	I-Metric
precision	I-Metric
and	I-Metric
recall	E-Metric
.	O

[	O
reference	O
]	O
We	O
perform	O
all	O
experiments	O
with	O
Groundhog	O
7	O
[	O
reference	O
]	O
.	O

We	O
generally	O
follow	O
settings	O
by	O
previous	O
work	O
[	O
reference	O
][	O
reference	O
]	O
.	O

All	O
networks	O
have	O
a	O
hidden	O
layer	O
size	O
of	O
1000	O
,	O
and	O
an	O
embedding	B-Metric
layer	I-Metric
size	E-Metric
of	O
620	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
only	O
keep	O
a	O
shortlist	O
of	O
τ	O
=	O
30000	O
words	O
in	O
memory	O
.	O

During	O
training	O
,	O
we	O
use	O
Adadelta	S-Method
(	O
Zeiler	O
,	O
2012	O
)	O
,	O
a	O
minibatch	O
size	O
of	O
80	O
,	O
and	O
reshuffle	O
the	O
training	O
set	O
between	O
epochs	O
.	O

We	O
train	O
a	O
network	O
for	O
approximately	O
7	O
days	O
,	O
then	O
take	O
the	O
last	O
4	O
saved	O
models	O
(	O
models	O
being	O
saved	O
every	O
12	O
hours	O
)	O
,	O
and	O
continue	O
training	O
each	O
with	O
a	O
fixed	O
embedding	B-Method
layer	E-Method
(	O
as	O
suggested	O
by	O
[	O
reference	O
]	O
)	O
for	O
12	O
hours	O
.	O

We	O
perform	O
two	O
independent	O
training	O
runs	O
for	O
each	O
models	O
,	O
once	O
with	O
cut	O
-	O
off	O
for	O
gradient	O
clipping	O
[	O
reference	O
]	O
of	O
5.0	O
,	O
once	O
with	O
a	O
cut	O
-	O
off	O
of	O
1.0	O
-	O
the	O
latter	O
produced	O
better	O
single	O
models	O
for	O
most	O
settings	O
.	O

We	O
report	O
results	O
of	O
the	O
system	O
that	O
performed	O
best	O
on	O
our	O
development	O
set	O
(	O
newstest2013	O
)	O
,	O
and	O
of	O
an	O
ensemble	O
of	O
all	O
8	O
models	O
.	O

We	O
use	O
a	O
beam	O
size	O
of	O
12	O
for	O
beam	B-Task
search	E-Task
,	O
with	O
probabilities	O
normalized	O
by	O
sentence	O
length	O
.	O

We	O
use	O
a	O
bilingual	O
dictionary	O
based	O
on	O
fast	B-Method
-	I-Method
align	E-Method
[	O
reference	O
]	O
.	O

For	O
our	O
baseline	O
,	O
this	O
serves	O
as	O
back	O
-	O
off	O
dictionary	O
for	O
rare	O
words	O
.	O

We	O
also	O
use	O
the	O
dictionary	O
to	O
speed	O
up	O
translation	S-Task
for	O
all	O
experiments	O
,	O
only	O
performing	O
the	O
softmax	S-Method
over	O
a	O
filtered	O
list	O
of	O
candidate	O
translations	O
(	O
like	O
[	O
reference	O
]	O
,	O
we	O
use	O
K	O
=	O
30000	O
;	O
K	O
′	O
=	O
10	O
)	O
.	O

section	O
:	O
Subword	B-Metric
statistics	E-Metric
Apart	O
from	O
translation	S-Task
quality	O
,	O
which	O
we	O
will	O
verify	O
empirically	O
,	O
our	O
main	O
objective	O
is	O
to	O
represent	O
an	O
open	O
vocabulary	O
through	O
a	O
compact	O
fixed	O
-	O
size	O
subword	O
vocabulary	O
,	O
and	O
allow	O
for	O
efficient	O
training	B-Task
and	I-Task
decoding	E-Task
.	O

[	O
reference	O
]	O
Statistics	O
for	O
different	O
segmentations	O
of	O
the	O
Ger	O
-	O
man	O
side	O
of	O
the	O
parallel	O
data	O
are	O
shown	O
in	O
Table	O
1	O
.	O

A	O
simple	O
baseline	O
is	O
the	O
segmentation	B-Task
of	I-Task
words	E-Task
into	O
character	B-Method
n	I-Method
-	I-Method
grams	E-Method
.	O

9	O
Character	B-Method
n	I-Method
-	I-Method
grams	E-Method
allow	O
for	O
different	O
trade	O
-	O
offs	O
between	O
sequence	O
length	O
(	O
#	O
tokens	O
)	O
and	O
vocabulary	O
size	O
(	O
#	O
types	O
)	O
,	O
depending	O
on	O
the	O
choice	O
of	O
n.	O
The	O
increase	O
in	O
sequence	O
length	O
is	O
substantial	O
;	O
one	O
way	O
to	O
reduce	O
sequence	O
length	O
is	O
to	O
leave	O
a	O
shortlist	O
of	O
the	O
k	O
most	O
frequent	O
word	O
types	O
unsegmented	O
.	O

Only	O
the	O
unigram	B-Method
representation	E-Method
is	O
truly	O
open	O
-	O
vocabulary	O
.	O

However	O
,	O
the	O
unigram	B-Method
representation	E-Method
performed	O
poorly	O
in	O
preliminary	O
experiments	O
,	O
and	O
we	O
report	O
translation	S-Task
results	O
with	O
a	O
bigram	B-Method
representation	E-Method
,	O
which	O
is	O
empirically	O
better	O
,	O
but	O
unable	O
to	O
produce	O
some	O
tokens	O
in	O
the	O
test	O
set	O
with	O
the	O
training	O
set	O
vocabulary	O
.	O

We	O
report	O
statistics	O
for	O
several	O
word	B-Method
segmentation	I-Method
techniques	E-Method
that	O
have	O
proven	O
useful	O
in	O
previous	O
SMT	S-Method
research	O
,	O
including	O
frequency	B-Method
-	I-Method
based	I-Method
compound	I-Method
splitting	E-Method
[	O
reference	O
]	O
,	O
rulebased	B-Method
hyphenation	E-Method
[	O
reference	O
]	O
,	O
and	O
Morfessor	S-Method
[	O
reference	O
]	O
.	O

We	O
find	O
that	O
they	O
only	O
moderately	O
reduce	O
vocabulary	B-Metric
size	E-Metric
,	O
and	O
do	O
not	O
solve	O
the	O
unknown	B-Task
word	I-Task
problem	E-Task
,	O
and	O
we	O
thus	O
find	O
them	O
unsuitable	O
for	O
our	O
goal	O
of	O
open	O
-	O
vocabulary	O
translation	S-Task
without	O
back	B-Task
-	I-Task
off	I-Task
dictionary	E-Task
.	O

BPE	S-Method
meets	O
our	O
goal	O
of	O
being	O
open	O
-	O
vocabulary	O
,	O
and	O
the	O
learned	O
merge	B-Method
operations	E-Method
can	O
be	O
applied	O
to	O
the	O
test	O
set	O
to	O
obtain	O
a	O
segmentation	S-Task
with	O
no	O
unknown	O
symbols	O
.	O

[	O
reference	O
]	O
Its	O
main	O
difference	O
from	O
the	O
character	B-Method
-	I-Method
level	I-Method
model	E-Method
is	O
that	O
the	O
more	O
compact	B-Method
representation	E-Method
of	O
BPE	S-Method
allows	O
for	O
shorter	O
sequences	O
,	O
and	O
that	O
the	O
attention	B-Method
model	E-Method
operates	O
on	O
variable	O
-	O
length	O
units	O
.	O

11	O
Table	O
1	O
shows	O
BPE	S-Method
with	O
59	O
500	O
merge	B-Method
operations	E-Method
,	O
and	O
joint	O
BPE	S-Method
with	O
89	O
500	O
operations	O
.	O

In	O
practice	O
,	O
we	O
did	O
not	O
include	O
infrequent	O
subword	O
units	O
in	O
the	O
NMT	S-Task
network	O
vocabulary	O
,	O
since	O
there	O
is	O
noise	O
in	O
the	O
subword	O
symbol	O
sets	O
,	O
e.g.	O
because	O
of	O
characters	O
from	O
foreign	O
alphabets	O
.	O

Hence	O
,	O
our	O
network	O
vocabularies	O
in	O
Table	O
2	O
are	O
typically	O
slightly	O
smaller	O
than	O
the	O
number	O
of	O
types	O
in	O
Table	O
1	O
.	O

9	O
Our	O
character	B-Method
n	I-Method
-	I-Method
grams	E-Method
do	O
not	O
cross	O
word	O
boundaries	O
.	O

We	O
mark	O
whether	O
a	O
subword	O
is	O
word	O
-	O
final	O
or	O
not	O
with	O
a	O
special	O
character	O
,	O
which	O
allows	O
us	O
to	O
restore	O
the	O
original	O
tokenization	O
.	O

10	O
Joint	O
BPE	S-Method
can	O
produce	O
segments	O
that	O
are	O
unknown	O
because	O
they	O
only	O
occur	O
in	O
the	O
English	O
training	O
text	O
,	O
but	O
these	O
are	O
rare	O
(	O
0.05	O
%	O
of	O
test	O
tokens	O
)	O
.	O

[	O
reference	O
]	O
We	O
highlighted	O
the	O
limitations	O
of	O
word	O
-	O
level	O
attention	O
in	O
section	O
3.1	O
.	O

At	O
the	O
other	O
end	O
of	O
the	O
spectrum	O
,	O
the	O
character	O
level	O
is	O
suboptimal	O
for	O
alignment	S-Task
[	O
reference	O
]	O
[	O
reference	O
]	O
;	O
⋄	O
:	O
[	O
reference	O
]	O
.	O

section	O
:	O
Translation	B-Task
experiments	E-Task
English→German	S-Material
translation	S-Task
results	O
are	O
shown	O
in	O
Table	O
2	O
;	O
English→Russian	S-Material
results	O
in	O
Table	O
3	O
.	O

Our	O
baseline	O
WDict	S-Method
is	O
a	O
word	B-Method
-	I-Method
level	I-Method
model	E-Method
with	O
a	O
back	B-Method
-	I-Method
off	I-Method
dictionary	E-Method
.	O

It	O
differs	O
from	O
WUnk	S-Method
in	O
that	O
the	O
latter	O
uses	O
no	O
back	O
-	O
off	O
dictionary	O
,	O
and	O
just	O
represents	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
as	O
UNK	O
12	O
.	O

The	O
back	B-Method
-	I-Method
off	I-Method
dictionary	E-Method
improves	O
unigram	B-Metric
F	I-Metric
1	E-Metric
for	O
rare	O
and	O
unseen	O
words	O
,	O
although	O
the	O
improvement	O
is	O
smaller	O
for	O
English→Russian	S-Material
,	O
since	O
the	O
back	O
-	O
off	O
dictionary	O
is	O
incapable	O
of	O
transliterating	O
names	O
.	O

All	O
subword	B-Method
systems	E-Method
operate	O
without	O
a	O
back	O
-	O
off	O
dictionary	O
.	O

We	O
first	O
focus	O
on	O
unigram	B-Task
F	I-Task
1	E-Task
,	O
where	O
all	O
systems	O
improve	O
over	O
the	O
baseline	O
,	O
especially	O
for	O
rare	O
words	O
(	O
36.8%→41.8	O
%	O
for	O
EN→DE	S-Material
;	O
26.5%→29.7	O
%	O
for	O
EN→RU	S-Material
)	O
.	O

For	O
OOVs	S-Task
,	O
the	O
baseline	O
strategy	O
of	O
copying	B-Task
unknown	I-Task
words	E-Task
works	O
well	O
for	O
English→German	S-Material
.	O

However	O
,	O
when	O
alphabets	O
differ	O
,	O
like	O
in	O
English→Russian	S-Material
,	O
the	O
subword	B-Method
models	E-Method
do	O
much	O
better	O
.	O

Unigram	B-Metric
F	I-Metric
1	I-Metric
scores	E-Metric
indicate	O
that	O
learning	O
the	O
BPE	S-Method
symbols	O
on	O
the	O
vocabulary	O
union	O
(	O
BPEJ90k	S-Method
)	O
is	O
more	O
effective	O
than	O
learning	O
them	O
separately	O
(	O
BPE	S-Method
-	O
60k	O
)	O
,	O
and	O
more	O
effective	O
than	O
using	O
character	B-Method
bigrams	E-Method
with	O
a	O
shortlist	O
of	O
50	O
000	O
unsegmented	O
words	O
(	O
C2	O
-	O
50k	O
)	O
,	O
but	O
all	O
reported	O
subword	B-Method
segmentations	E-Method
are	O
viable	O
choices	O
and	O
outperform	O
the	O
back	B-Method
-	I-Method
off	I-Method
dictionary	I-Method
baseline	E-Method
.	O

Our	O
subword	B-Method
representations	E-Method
cause	O
big	O
improvements	O
in	O
the	O
translation	S-Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
but	O
these	O
only	O
constitute	O
9	O
-	O
11	O
%	O
of	O
the	O
test	O
sets	O
.	O

Since	O
rare	O
words	O
tend	O
to	O
carry	O
central	O
information	O
in	O
a	O
sentence	O
,	O
we	O
suspect	O
that	O
BLEU	S-Metric
and	O
CHRF3	S-Method
underestimate	O
their	O
effect	O
on	O
translation	S-Task
quality	O
.	O

Still	O
,	O
we	O
also	O
see	O
improvements	O
over	O
the	O
baseline	O
in	O
total	B-Metric
unigram	I-Metric
F	I-Metric
1	E-Metric
,	O
as	O
well	O
as	O
BLEU	S-Metric
and	O
CHRF3	S-Method
,	O
and	O
the	O
subword	B-Method
ensembles	E-Method
outperform	O
the	O
WDict	B-Method
baseline	E-Method
by	O
0.3	O
-	O
1.3	O
BLEU	S-Metric
and	O
0.6	O
-	O
2	O
CHRF3	O
.	O

There	O
is	O
some	O
inconsistency	O
between	O
BLEU	S-Metric
and	O
CHRF3	S-Method
,	O
which	O
we	O
attribute	O
to	O
the	O
fact	O
that	O
BLEU	S-Metric
has	O
a	O
precision	B-Metric
bias	E-Metric
,	O
and	O
CHRF3	S-Method
a	O
recall	B-Metric
bias	E-Metric
.	O

For	O
English→German	S-Material
,	O
we	O
observe	O
the	O
best	O
BLEU	S-Metric
score	O
of	O
25.3	O
with	O
C2	S-Method
-	O
50k	O
,	O
but	O
the	O
best	O
CHRF3	B-Metric
score	E-Metric
of	O
54.1	O
with	O
BPE	B-Method
-	I-Method
J90k	E-Method
.	O

For	O
comparison	O
to	O
the	O
(	O
to	O
our	O
knowledge	O
)	O
best	O
non	B-Method
-	I-Method
neural	I-Method
MT	I-Method
system	E-Method
on	O
this	O
data	O
set	O
,	O
we	O
report	O
syntaxbased	O
SMT	S-Method
results	O
[	O
reference	O
]	O
.	O

We	O
observe	O
that	O
our	O
best	O
systems	O
outperform	O
the	O
syntax	B-Method
-	I-Method
based	I-Method
system	E-Method
in	O
terms	O
of	O
BLEU	S-Metric
,	O
but	O
not	O
in	O
terms	O
of	O
CHRF3	S-Method
.	O

Regarding	O
other	O
neural	B-Method
systems	E-Method
,	O
[	O
reference	O
]	O
report	O
a	O
BLEU	S-Metric
score	O
of	O
25.9	O
on	O
newstest2015	O
,	O
but	O
we	O
note	O
that	O
they	O
use	O
an	O
ensemble	O
of	O
8	O
independently	O
trained	O
models	O
,	O
and	O
also	O
report	O
strong	O
improvements	O
from	O
applying	O
dropout	S-Method
,	O
which	O
we	O
did	O
not	O
use	O
.	O

We	O
are	O
confident	O
that	O
our	O
improvements	O
to	O
the	O
translation	S-Task
of	O
rare	O
words	O
are	O
orthogonal	O
to	O
improvements	O
achievable	O
through	O
other	O
improvements	O
in	O
the	O
network	B-Method
architecture	E-Method
,	O
training	B-Method
algorithm	E-Method
,	O
or	O
better	O
ensembles	S-Method
.	O

For	O
English→Russian	S-Material
,	O
the	O
state	O
of	O
the	O
art	O
is	O
the	O
phrase	B-Method
-	I-Method
based	I-Method
system	E-Method
by	O
.	O

It	O
outperforms	O
our	O
WDict	B-Method
baseline	E-Method
by	O
1.5	O
BLEU	S-Metric
.	O

The	O
subword	B-Method
models	E-Method
are	O
a	O
step	O
towards	O
closing	O
this	O
gap	O
,	O
and	O
BPE	B-Method
-	I-Method
J90k	E-Method
yields	O
an	O
improvement	O
of	O
1.3	O
BLEU	S-Metric
,	O
and	O
2.0	O
CHRF3	S-Method
,	O
over	O
WDict	S-Method
.	O

As	O
a	O
further	O
comment	O
on	O
our	O
translation	S-Task
results	O
,	O
we	O
want	O
to	O
emphasize	O
that	O
performance	O
variability	O
is	O
still	O
an	O
open	O
problem	O
with	O
NMT	S-Task
.	O

On	O
our	O
development	O
set	O
,	O
we	O
observe	O
differences	O
of	O
up	O
to	O
1	O
BLEU	S-Metric
between	O
different	O
models	O
.	O

For	O
single	O
systems	O
,	O
we	O
report	O
the	O
results	O
of	O
the	O
model	O
that	O
performs	O
best	O
on	O
dev	O
(	O
out	O
of	O
8	O
)	O
,	O
which	O
has	O
a	O
stabilizing	O
effect	O
,	O
but	O
how	O
to	O
control	O
for	O
randomness	O
deserves	O
further	O
attention	O
in	O
future	O
research	O
.	O

section	O
:	O
Analysis	O
section	O
:	O
Unigram	B-Metric
accuracy	E-Metric
Our	O
main	O
claims	O
are	O
that	O
the	O
translation	S-Task
of	O
rare	O
and	O
unknown	O
words	O
is	O
poor	O
in	O
word	O
-	O
level	O
NMT	S-Task
models	O
,	O
and	O
that	O
subword	B-Method
models	E-Method
improve	O
the	O
translation	S-Task
of	O
these	O
word	O
types	O
.	O

To	O
further	O
illustrate	O
the	O
effect	O
of	O
different	O
subword	B-Method
segmentations	E-Method
on	O
the	O
translation	S-Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
we	O
plot	O
target	O
-	O
side	O
words	O
sorted	O
by	O
their	O
frequency	O
in	O
the	O
training	O
set	O
.	O

[	O
reference	O
]	O
To	O
analyze	O
the	O
effect	O
of	O
vocabulary	B-Metric
size	E-Metric
,	O
we	O
also	O
include	O
the	O
system	B-Method
C2	I-Method
-	I-Method
3	I-Method
/	I-Method
500k	E-Method
,	O
which	O
is	O
a	O
system	O
with	O
the	O
same	O
vocabulary	O
size	O
as	O
the	O
WDict	B-Method
baseline	E-Method
,	O
and	O
character	B-Method
bigrams	E-Method
to	O
represent	O
unseen	O
words	O
.	O

Figure	O
2	O
shows	O
results	O
for	O
the	O
English	B-Method
-	I-Method
German	I-Method
ensemble	I-Method
systems	E-Method
on	O
newstest2015	O
.	O

Unigram	B-Metric
F	I-Metric
1	E-Metric
of	O
all	O
systems	O
tends	O
to	O
decrease	O
for	O
lowerfrequency	O
words	O
.	O

The	O
baseline	O
system	O
has	O
a	O
spike	O
in	O
F	B-Metric
1	E-Metric
for	O
OOVs	O
,	O
i.e.	O
words	O
that	O
do	O
not	O
occur	O
in	O
the	O
training	O
text	O
.	O

This	O
is	O
because	O
a	O
high	O
proportion	O
of	O
OOVs	O
are	O
names	O
,	O
for	O
which	O
a	O
copy	O
from	O
the	O
source	O
to	O
the	O
target	O
text	O
is	O
a	O
good	O
strategy	O
for	O
English→German	S-Material
.	O

The	O
systems	O
with	O
a	O
target	O
vocabulary	O
of	O
500	O
000	O
words	O
mostly	O
differ	O
in	O
how	O
well	O
they	O
translate	O
words	O
with	O
rank	O
>	O
500	O
000	O
.	O

A	O
back	O
-	O
off	O
dictionary	O
is	O
an	O
obvious	O
improvement	O
over	O
producing	O
UNK	S-Method
,	O
but	O
the	O
subword	B-Method
system	I-Method
C2	I-Method
-	I-Method
3	I-Method
/	I-Method
500k	E-Method
achieves	O
better	O
performance	O
.	O

Note	O
that	O
all	O
OOVs	O
that	O
the	O
backoff	O
dictionary	O
produces	O
are	O
words	O
that	O
are	O
copied	O
from	O
the	O
source	O
,	O
usually	O
names	O
,	O
while	O
the	O
subword	O
[	O
reference	O
]	O
We	O
perform	O
binning	O
of	O
words	O
with	O
the	O
same	O
training	O
set	O
frequency	O
,	O
and	O
apply	O
bezier	B-Method
smoothing	E-Method
to	O
the	O
graph	O
.	O

systems	O
can	O
productively	O
form	O
new	O
words	O
such	O
as	O
compounds	O
.	O

For	O
the	O
50	O
000	O
most	O
frequent	O
words	O
,	O
the	O
representation	O
is	O
the	O
same	O
for	O
all	O
neural	B-Method
networks	E-Method
,	O
and	O
all	O
neural	B-Method
networks	E-Method
achieve	O
comparable	O
unigram	B-Metric
F	I-Metric
1	E-Metric
for	O
this	O
category	O
.	O

For	O
the	O
interval	O
between	O
frequency	O
rank	O
50	O
000	O
and	O
500	O
000	O
,	O
the	O
comparison	O
between	O
C2	O
-	O
3	O
/	O
500k	O
and	O
C2	O
-	O
50k	O
unveils	O
an	O
interesting	O
difference	O
.	O

The	O
two	O
systems	O
only	O
differ	O
in	O
the	O
size	O
of	O
the	O
shortlist	O
,	O
with	O
C2	O
-	O
3	O
/	O
500k	O
representing	O
words	O
in	O
this	O
interval	O
as	O
single	O
units	O
,	O
and	O
C2	O
-	O
50k	O
via	O
subword	B-Method
units	E-Method
.	O

We	O
find	O
that	O
the	O
performance	O
of	O
C2	B-Method
-	I-Method
3	I-Method
/	I-Method
500k	E-Method
degrades	O
heavily	O
up	O
to	O
frequency	O
rank	O
500	O
000	O
,	O
at	O
which	O
point	O
the	O
model	O
switches	O
to	O
a	O
subword	B-Method
representation	E-Method
and	O
performance	O
recovers	O
.	O

The	O
performance	O
of	O
C2	O
-	O
50k	O
remains	O
more	O
stable	O
.	O

We	O
attribute	O
this	O
to	O
the	O
fact	O
that	O
subword	O
units	O
are	O
less	O
sparse	O
than	O
words	O
.	O

In	O
our	O
training	O
set	O
,	O
the	O
frequency	O
rank	O
50	O
000	O
corresponds	O
to	O
a	O
frequency	O
of	O
60	O
in	O
the	O
training	O
data	O
;	O
the	O
frequency	O
rank	O
500	O
000	O
to	O
a	O
frequency	O
of	O
2	O
.	O

Because	O
subword	B-Method
representations	E-Method
are	O
less	O
sparse	O
,	O
reducing	O
the	O
size	O
of	O
the	O
network	O
vocabulary	O
,	O
and	O
representing	O
more	O
words	O
via	O
subword	O
units	O
,	O
can	O
lead	O
to	O
better	O
performance	O
.	O

The	O
F	B-Metric
1	I-Metric
numbers	E-Metric
hide	O
some	O
qualitative	O
differences	O
between	O
systems	O
.	O

For	O
English→German	S-Material
,	O
WDict	O
produces	O
few	O
OOVs	O
(	O
26.5	O
%	O
recall	S-Metric
)	O
,	O
but	O
with	O
high	O
precision	S-Metric
(	O
60.6	O
%	O
)	O
,	O
whereas	O
the	O
subword	B-Method
systems	E-Method
achieve	O
higher	O
recall	S-Metric
,	O
but	O
lower	O
precision	S-Metric
.	O

We	O
note	O
that	O
the	O
character	B-Method
bigram	I-Method
model	I-Method
C2	I-Method
-	I-Method
50k	E-Method
produces	O
the	O
most	O
OOV	O
words	O
,	O
and	O
achieves	O
relatively	O
low	O
precision	S-Metric
of	O
29.1	O
%	O
for	O
this	O
category	O
.	O

However	O
,	O
it	O
outperforms	O
the	O
back	B-Method
-	I-Method
off	I-Method
dictionary	E-Method
in	O
recall	S-Metric
(	O
33.0	O
%	O
)	O
.	O

BPE	S-Method
-	O
60k	O
,	O
which	O
suffers	O
from	O
transliteration	O
(	O
or	O
copy	O
)	O
errors	O
due	O
to	O
segmentation	O
inconsistencies	O
,	O
obtains	O
a	O
slightly	O
better	O
precision	S-Metric
(	O
32.4	O
%	O
)	O
,	O
but	O
a	O
worse	O
recall	S-Metric
(	O
26.6	O
%	O
)	O
.	O

In	O
contrast	O
to	O
BPE	S-Method
-	O
60k	O
,	O
the	O
joint	O
BPE	S-Method
encoding	O
of	O
BPEJ90k	S-Method
improves	O
both	O
precision	S-Metric
(	O
38.6	O
%	O
)	O
and	O
recall	S-Metric
(	O
29.8	O
%	O
)	O
.	O

For	O
English→Russian	S-Material
,	O
unknown	O
names	O
can	O
only	O
rarely	O
be	O
copied	O
,	O
and	O
usually	O
require	O
transliteration	O
.	O

Consequently	O
,	O
the	O
WDict	B-Method
baseline	E-Method
performs	O
more	O
poorly	O
for	O
OOVs	S-Task
(	O
9.2	O
%	O
precision	S-Metric
;	O
5.2	O
%	O
recall	S-Metric
)	O
,	O
and	O
the	O
subword	B-Method
models	E-Method
improve	O
both	O
precision	S-Metric
and	O
recall	S-Metric
(	O
21.9	O
%	O
precision	S-Metric
and	O
15.6	O
%	O
recall	S-Metric
for	O
BPE	B-Method
-	I-Method
J90k	E-Method
)	O
.	O

The	O
full	O
unigram	B-Metric
F	I-Metric
1	I-Metric
plot	E-Metric
is	O
shown	O
in	O
Figure	O
3	O
.	O

24	O
.	O

Table	O
3	O
:	O
English→Russian	S-Material
translation	S-Task
performance	O
(	O
BLEU	S-Metric
,	O
CHRF3	S-Metric
and	O
unigram	B-Metric
F	I-Metric
1	E-Metric
)	O
on	O
newstest2015	O
.	O

Ens	O
-	O
8	O
:	O
ensemble	O
of	O
8	O
models	O
.	O

Best	O
NMT	S-Task
system	O
in	O
bold	O
.	O

Unigram	B-Metric
F	I-Metric
1	E-Metric
(	O
with	O
ensembles	S-Method
)	O
is	O
computed	O
for	O
all	O
words	O
(	O
n	O
=	O
55654	O
)	O
,	O
rare	O
words	O
(	O
not	O
among	O
top	O
50	O
000	O
in	O
training	O
set	O
;	O
n	O
=	O
5442	O
)	O
,	O
and	O
OOVs	O
(	O
not	O
in	O
training	O
set	O
;	O
n	O
=	O
851	O
)	O
.	O

The	O
English→Russian	S-Material
examples	O
show	O
that	O
the	O
subword	B-Method
systems	E-Method
are	O
capable	O
of	O
transliteration	S-Task
.	O

However	O
,	O
transliteration	B-Metric
errors	E-Metric
do	O
occur	O
,	O
either	O
due	O
to	O
ambiguous	O
transliterations	O
,	O
or	O
because	O
of	O
non	O
-	O
consistent	O
segmentations	O
between	O
source	O
and	O
target	O
text	O
which	O
make	O
it	O
hard	O
for	O
the	O
system	O
to	O
learn	O
a	O
transliteration	B-Task
mapping	E-Task
.	O

Note	O
that	O
the	O
BPE	S-Method
-	O
60k	O
system	O
encodes	O
Mirzayeva	O
inconsistently	O
for	O
the	O
two	O
language	O
pairs	O
(	O
Mirz|ayeva→Мир|за|ева	O
Mir|za|eva	O
)	O
.	O

This	O
example	O
is	O
still	O
translated	O
correctly	O
,	O
but	O
we	O
observe	O
spurious	O
insertions	O
and	O
deletions	O
of	O
characters	O
in	O
the	O
BPE	S-Method
-	O
60k	O
system	O
.	O

An	O
example	O
is	O
the	O
transliteration	O
of	O
rakfisk	S-Method
,	O
where	O
a	O
п	O
is	O
inserted	O
and	O
a	O
к	O
is	O
deleted	O
.	O

We	O
trace	O
this	O
error	O
back	O
to	O
translation	S-Task
pairs	O
in	O
the	O
training	O
data	O
with	O
inconsistent	O
segmentations	O
,	O
such	O
as	O
(	O
p|rak|ri|ti→пра|крит|и	O
system	O
sentence	O
source	O
health	O
research	O
institutes	O
reference	O
Gesundheitsforschungsinstitute	O
WDict	O
Forschungsinstitute	O
C2	O
-	O
50k	O
Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n	O
BPE	S-Method
-	O
60k	O
Gesundheits|forsch|ungsinstitu|ten	O
BPE	S-Method
-	O
J90k	O
Gesundheits|forsch|ungsin|stitute	O
source	O
asinine	O
situation	O
reference	O
dumme	O
Situation	O
WDict	O
asinine	O
situation	O
→	O
UNK	O
→	O
asinine	O
C2	O
-	O
50k	O
as|in|in|e	O
situation	O
→	O
As|in|en|si|tu|at|io|n	O
BPE	S-Method
-	O
60k	O
as|in|ine	O
situation	O
→	O
A|in|line	O
-	O
|Situation	O
BPE	S-Method
-	O
J90	O
K	O
as|in|ine	O
situation	O
→	O
As|in|in	O
-	O
|Situation	O
(	O
pra|krit|i	O
)	O
)	O
,	O
from	O
which	O
the	O
translation	S-Task
(	O
rak→пра	O
)	O
is	O
erroneously	O
learned	O
.	O

The	O
segmentation	S-Task
of	O
the	O
joint	O
BPE	S-Method
system	O
(	O
BPE	B-Method
-	I-Method
J90k	E-Method
)	O
is	O
more	O
consistent	O
(	O
pra|krit|i→пра|крит|и	O
(	O
pra|krit|i	O
)	O
)	O
.	O

section	O
:	O
Manual	B-Task
Analysis	E-Task
section	O
:	O
Conclusion	O
The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
that	O
we	O
show	O
that	O
neural	O
machine	O
translation	S-Task
systems	O
are	O
capable	O
of	O
open	O
-	O
vocabulary	O
translation	S-Task
by	O
representing	O
rare	O
and	O
unseen	O
words	O
as	O
a	O
sequence	O
of	O
subword	O
units	O
.	O

14	O
This	O
is	O
both	O
simpler	O
and	O
more	O
effective	O
than	O
using	O
a	O
back	O
-	O
off	O
translation	S-Task
model	O
.	O

We	O
introduce	O
a	O
variant	O
of	O
byte	B-Method
pair	I-Method
encoding	E-Method
for	O
word	B-Task
segmentation	E-Task
,	O
which	O
is	O
capable	O
of	O
encoding	O
open	O
vocabularies	O
with	O
a	O
compact	O
symbol	O
vocabulary	O
of	O
variable	O
-	O
length	O
subword	O
units	O
.	O

We	O
show	O
performance	O
gains	O
over	O
the	O
baseline	O
with	O
both	O
BPE	B-Method
segmentation	E-Method
,	O
and	O
a	O
simple	O
character	B-Method
bigram	I-Method
segmentation	E-Method
.	O

Our	O
analysis	O
shows	O
that	O
not	O
only	O
out	O
-	O
ofvocabulary	O
words	O
,	O
but	O
also	O
rare	O
in	O
-	O
vocabulary	O
words	O
are	O
translated	O
poorly	O
by	O
our	O
baseline	O
NMT	S-Task
system	O
,	O
and	O
that	O
reducing	O
the	O
vocabulary	B-Metric
size	E-Metric
of	O
subword	B-Method
models	E-Method
can	O
actually	O
improve	O
performance	O
.	O

In	O
this	O
work	O
,	O
our	O
choice	O
of	O
vocabulary	O
size	O
is	O
somewhat	O
arbitrary	O
,	O
and	O
mainly	O
motivated	O
by	O
comparison	O
to	O
prior	O
work	O
.	O

One	O
avenue	O
of	O
future	O
research	O
is	O
to	O
learn	O
the	O
optimal	O
vocabulary	O
size	O
for	O
a	O
translation	S-Task
task	O
,	O
which	O
we	O
expect	O
to	O
depend	O
on	O
the	O
language	O
pair	O
and	O
amount	O
of	O
training	O
data	O
,	O
automatically	O
.	O

We	O
also	O
believe	O
there	O
is	O
further	O
potential	O
in	O
bilingually	B-Method
informed	I-Method
segmentation	I-Method
algorithms	E-Method
to	O
create	O
more	O
alignable	O
subword	O
units	O
,	O
although	O
the	O
segmentation	B-Method
algorithm	E-Method
can	O
not	O
rely	O
on	O
the	O
target	O
text	O
at	O
runtime	O
.	O

While	O
the	O
relative	O
effectiveness	O
will	O
depend	O
on	O
language	O
-	O
specific	O
factors	O
such	O
as	O
vocabulary	O
size	O
,	O
we	O
believe	O
that	O
subword	B-Method
segmentations	E-Method
are	O
suitable	O
for	O
most	O
language	O
pairs	O
,	O
eliminating	O
the	O
need	O
for	O
large	O
NMT	S-Task
vocabularies	O
or	O
back	B-Method
-	I-Method
off	I-Method
models	E-Method
.	O

section	O
:	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
Maja	O
Popović	O
for	O
her	O
implementation	O
of	O
CHRF	S-Method
,	O
with	O
which	O
we	O
verified	O
our	O
reimplementation	O
.	O

The	O
research	O
presented	O
in	O
this	O
publication	O
was	O
conducted	O
in	O
cooperation	O
with	O
Samsung	O
Electronics	O
Polska	O
sp	O
.	O

z	O
o.o	O
.	O

-	O
Samsung	O
R	O
&	O
D	O
Institute	O
Poland	O
.	O

This	O
project	O
received	O
funding	O
from	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
under	O
grant	O
agreement	O
645452	O
(	O
QT21	O
)	O
.	O

section	O
:	O
document	O
:	O
SNIPER	S-Method
:	O
Efficient	O
Multi	B-Task
-	I-Task
Scale	I-Task
Training	E-Task
We	O
present	O
SNIPER	S-Method
,	O
an	O
algorithm	O
for	O
performing	O
efficient	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
in	O
instance	B-Task
level	I-Task
visual	I-Task
recognition	I-Task
tasks	E-Task
.	O

Instead	O
of	O
processing	O
every	O
pixel	O
in	O
an	O
image	O
pyramid	O
,	O
SNIPER	S-Method
processes	O
context	O
regions	O
around	O
ground	O
-	O
truth	O
instances	O
(	O
referred	O
to	O
as	O
chips	O
)	O
at	O
the	O
appropriate	O
scale	O
.	O

For	O
background	B-Task
sampling	E-Task
,	O
these	O
context	O
-	O
regions	O
are	O
generated	O
using	O
proposals	O
extracted	O
from	O
a	O
region	B-Method
proposal	I-Method
network	E-Method
trained	O
with	O
a	O
short	B-Method
learning	I-Method
schedule	E-Method
.	O

Hence	O
,	O
the	O
number	O
of	O
chips	O
generated	O
per	O
image	O
during	O
training	O
adaptively	O
changes	O
based	O
on	O
the	O
scene	O
complexity	O
.	O

SNIPER	S-Method
only	O
processes	O
30	O
%	O
more	O
pixels	O
compared	O
to	O
the	O
commonly	O
used	O
single	B-Method
scale	I-Method
training	E-Method
at	O
800x1333	O
pixels	O
on	O
the	O
COCO	B-Material
dataset	E-Material
.	O

But	O
,	O
it	O
also	O
observes	O
samples	O
from	O
extreme	O
resolutions	O
of	O
the	O
image	O
pyramid	O
,	O
like	O
1400x2000	O
pixels	O
.	O

As	O
SNIPER	S-Method
operates	O
on	O
resampled	O
low	O
resolution	O
chips	O
(	O
512x512	O
pixels	O
)	O
,	O
it	O
can	O
have	O
a	O
batch	O
size	O
as	O
large	O
as	O
on	O
a	O
single	O
GPU	S-Method
even	O
with	O
a	O
ResNet	B-Method
-	I-Method
101	I-Method
backbone	E-Method
.	O

Therefore	O
it	O
can	O
benefit	O
from	O
batch	B-Method
-	I-Method
normalization	E-Method
during	O
training	S-Task
without	O
the	O
need	O
for	O
synchronizing	O
batch	O
-	O
normalization	O
statistics	O
across	O
GPUs	O
.	O

SNIPER	S-Method
brings	O
training	O
of	O
instance	B-Task
level	I-Task
recognition	I-Task
tasks	E-Task
like	O
object	B-Task
detection	E-Task
closer	O
to	O
the	O
protocol	O
for	O
image	B-Task
classification	E-Task
and	O
suggests	O
that	O
the	O
commonly	O
accepted	O
guideline	O
that	O
it	O
is	O
important	O
to	O
train	O
on	O
high	O
resolution	O
images	O
for	O
instance	B-Task
level	I-Task
visual	I-Task
recognition	I-Task
tasks	E-Task
might	O
not	O
be	O
correct	O
.	O

Our	O
implementation	O
based	O
on	O
Faster	B-Method
-	I-Method
RCNN	E-Method
with	O
a	O
ResNet	B-Method
-	I-Method
101	I-Method
backbone	E-Method
obtains	O
an	O
mAP	S-Metric
of	O
%	O
on	O
the	O
COCO	B-Material
dataset	E-Material
for	O
bounding	B-Task
box	I-Task
detection	E-Task
and	O
can	O
process	O
images	O
per	O
second	O
during	O
inference	S-Task
with	O
a	O
single	O
GPU	O
.	O

Code	O
is	O
available	O
at	O
.	O

section	O
:	O
Introduction	O
Humans	O
have	O
a	O
foveal	B-Method
visual	I-Method
system	E-Method
which	O
attends	O
to	O
objects	O
at	O
a	O
fixed	O
distance	O
and	O
size	O
.	O

For	O
example	O
,	O
when	O
we	O
focus	O
on	O
nearby	O
objects	O
,	O
far	O
away	O
objects	O
get	O
blurred	O
.	O

Naturally	O
,	O
it	O
is	O
difficult	O
for	O
us	O
to	O
focus	O
on	O
objects	O
of	O
different	O
scales	O
simultaneously	O
.	O

We	O
only	O
process	O
a	O
small	O
field	O
of	O
view	O
at	O
any	O
given	O
point	O
of	O
time	O
and	O
adaptively	O
ignore	O
the	O
remaining	O
visual	O
content	O
in	O
the	O
image	O
.	O

However	O
,	O
computer	B-Method
algorithms	E-Method
which	O
are	O
designed	O
for	O
instance	B-Task
level	I-Task
visual	I-Task
recognition	I-Task
tasks	E-Task
like	O
object	B-Task
detection	E-Task
depart	O
from	O
this	O
natural	O
way	O
of	O
processing	B-Task
visual	I-Task
information	E-Task
.	O

For	O
obtaining	O
a	O
representation	S-Task
robust	O
to	O
scale	O
,	O
popular	O
detection	S-Task
algorithms	O
like	O
Faster	B-Method
-	I-Method
RCNN	E-Method
/	O
Mask	B-Method
-	I-Method
RCNN	E-Method
are	O
trained	O
on	O
a	O
multi	O
-	O
scale	O
image	O
pyramid	O
.	O

Since	O
every	O
pixel	O
is	O
processed	O
at	O
each	O
scale	O
,	O
this	O
approach	O
to	O
processing	B-Task
visual	I-Task
information	E-Task
increases	O
the	O
training	B-Metric
time	E-Metric
significantly	O
.	O

For	O
example	O
,	O
constructing	O
a	O
scale	O
image	O
pyramid	O
(	O
e.g.	O
scales=	O
x	O
,	O
x	O
,	O
x	O
)	O
requires	O
processing	O
times	O
the	O
number	O
of	O
pixels	O
present	O
in	O
the	O
original	O
image	O
.	O

For	O
this	O
reason	O
,	O
it	O
is	O
impractical	O
to	O
use	O
multi	B-Method
-	I-Method
scale	I-Method
training	E-Method
in	O
many	O
scenarios	O
.	O

Recently	O
,	O
it	O
is	O
shown	O
that	O
ignoring	O
gradients	O
of	O
objects	O
which	O
are	O
of	O
extreme	O
resolutions	O
is	O
beneficial	O
while	O
using	O
multiple	O
scales	O
during	O
training	O
.	O

For	O
example	O
,	O
when	O
constructing	O
an	O
image	O
pyramid	O
of	O
scales	O
,	O
the	O
gradients	O
of	O
large	O
and	O
small	O
objects	O
should	O
be	O
ignored	O
at	O
large	O
and	O
small	O
resolutions	O
respectively	O
.	O

If	O
this	O
is	O
the	O
case	O
,	O
an	O
intuitive	O
question	O
which	O
arises	O
is	O
,	O
do	O
we	O
need	O
to	O
process	O
the	O
entire	O
image	O
at	O
a	O
x	O
resolution	O
?	O
Would	O
n’t	O
it	O
suffice	O
to	O
sample	O
a	O
much	O
smaller	O
RoI	O
(	O
chip	O
)	O
around	O
small	O
objects	O
at	O
this	O
resolution	O
?	O
On	O
the	O
other	O
hand	O
,	O
if	O
the	O
image	O
is	O
already	O
high	O
resolution	O
,	O
and	O
objects	O
in	O
it	O
are	O
also	O
large	O
in	O
size	O
,	O
is	O
there	O
any	O
benefit	O
in	O
upsampling	O
that	O
image	O
?	O
While	O
ignoring	O
significant	O
portions	O
of	O
the	O
image	O
would	O
save	O
computation	O
,	O
a	O
smaller	O
chip	O
would	O
also	O
lack	O
context	O
required	O
for	O
recognition	S-Task
.	O

A	O
significant	O
portion	O
of	O
background	O
would	O
also	O
be	O
ignored	O
at	O
a	O
higher	O
resolution	O
.	O

So	O
,	O
there	O
is	O
a	O
trade	O
-	O
off	O
between	O
computation	S-Metric
,	O
context	O
and	O
negative	B-Task
mining	E-Task
while	O
accelerating	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
.	O

To	O
this	O
end	O
,	O
we	O
present	O
a	O
novel	O
training	B-Method
algorithm	E-Method
called	O
Scale	B-Method
Normalization	E-Method
for	O
Image	B-Task
Pyramids	E-Task
with	O
Efficient	B-Task
Resampling	E-Task
(	O
SNIPER	S-Method
)	O
,	O
which	O
adaptively	O
samples	O
chips	O
from	O
multiple	O
scales	O
of	O
an	O
image	O
pyramid	O
,	O
conditioned	O
on	O
the	O
image	O
content	O
.	O

We	O
sample	O
positive	O
chips	O
conditioned	O
on	O
the	O
ground	O
-	O
truth	O
instances	O
and	O
negative	O
chips	O
based	O
on	O
proposals	O
generated	O
by	O
a	O
region	B-Method
proposal	I-Method
network	E-Method
.	O

Under	O
the	O
same	O
conditions	O
(	O
fixed	O
batch	O
normalization	O
)	O
,	O
we	O
show	O
that	O
SNIPER	S-Method
performs	O
as	O
well	O
as	O
the	O
multi	B-Method
-	I-Method
scale	I-Method
strategy	E-Method
proposed	O
in	O
SNIP	S-Method
while	O
reducing	O
the	O
number	O
of	O
pixels	O
processed	O
by	O
a	O
factor	O
of	O
during	O
training	S-Task
on	O
the	O
COCO	B-Material
dataset	E-Material
.	O

Since	O
SNIPER	S-Method
is	O
trained	O
on	O
x	O
size	O
chips	O
,	O
it	O
can	O
reap	O
the	O
benefits	O
of	O
a	O
large	O
batch	O
size	O
and	O
training	O
with	O
batch	B-Method
-	I-Method
normalization	E-Method
on	O
a	O
single	O
GPU	O
node	O
.	O

In	O
particular	O
,	O
we	O
can	O
use	O
a	O
batch	O
size	O
of	O
per	O
GPU	O
(	O
leading	O
to	O
a	O
total	O
batch	O
size	O
of	O
160	O
)	O
,	O
even	O
with	O
a	O
ResNet	O
-	O
101	O
based	O
Faster	B-Method
-	I-Method
RCNN	E-Method
detector	O
.	O

While	O
being	O
efficient	O
,	O
SNIPER	S-Method
obtains	O
competitive	O
performance	O
on	O
the	O
COCO	S-Material
detection	S-Task
dataset	O
even	O
with	O
simple	O
detection	S-Task
architectures	O
like	O
Faster	B-Method
-	I-Method
RCNN	E-Method
.	O

section	O
:	O
Background	O
Deep	O
learning	O
based	O
object	B-Task
detection	E-Task
algorithms	O
have	O
primarily	O
evolved	O
from	O
the	O
R	B-Method
-	I-Method
CNN	I-Method
detector	E-Method
,	O
which	O
started	O
with	O
object	O
proposals	O
generated	O
with	O
an	O
unsupervised	B-Method
algorithm	E-Method
,	O
resized	O
these	O
proposals	O
to	O
a	O
canonical	O
x	O
size	O
image	O
and	O
classified	O
them	O
using	O
a	O
convolutional	B-Method
neural	I-Method
network	E-Method
.	O

This	O
model	O
is	O
scale	O
invariant	O
,	O
but	O
the	O
computational	B-Metric
cost	E-Metric
for	O
training	S-Task
and	O
inference	S-Task
for	O
R	B-Method
-	I-Method
CNN	E-Method
scales	O
linearly	O
with	O
the	O
number	O
of	O
proposals	O
.	O

To	O
alleviate	O
this	O
computational	O
bottleneck	O
,	O
Fast	B-Method
-	I-Method
RCNN	E-Method
proposed	O
to	O
project	O
region	O
proposals	O
to	O
a	O
high	O
level	O
convolutional	O
feature	O
map	O
and	O
use	O
the	O
pooled	O
features	O
as	O
a	O
semantic	B-Method
representation	E-Method
for	O
region	B-Task
proposals	E-Task
.	O

In	O
this	O
process	O
,	O
the	O
computation	O
is	O
shared	O
for	O
the	O
convolutional	B-Method
layers	E-Method
and	O
only	O
lightweight	B-Method
fully	I-Method
connected	I-Method
layers	E-Method
are	O
applied	O
on	O
each	O
proposal	O
.	O

However	O
,	O
convolution	S-Method
for	O
objects	O
of	O
different	O
sizes	O
is	O
performed	O
at	O
a	O
single	O
scale	O
,	O
which	O
destroys	O
the	O
scale	O
invariance	O
properties	O
of	O
R	B-Method
-	I-Method
CNN	E-Method
.	O

Hence	O
,	O
inference	S-Task
at	O
multiple	O
scales	O
is	O
performed	O
and	O
detections	O
from	O
multiple	O
scales	O
are	O
combined	O
by	O
selecting	O
features	O
from	O
a	O
pair	O
of	O
adjacent	O
scales	O
closer	O
to	O
the	O
resolution	O
of	O
the	O
pre	O
-	O
trained	B-Method
network	E-Method
.	O

The	O
Fast	B-Method
-	I-Method
RCNN	I-Method
model	E-Method
has	O
since	O
become	O
the	O
de	O
-	O
facto	O
approach	O
for	O
classifying	B-Task
region	I-Task
proposals	E-Task
as	O
it	O
is	O
fast	O
and	O
also	O
captures	O
more	O
context	O
in	O
its	O
features	O
,	O
which	O
is	O
lacking	O
in	O
RCNN	S-Method
.	O

It	O
is	O
worth	O
noting	O
that	O
in	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
,	O
Fast	B-Method
-	I-Method
RCNN	I-Method
upsamples	E-Method
and	O
downsamples	O
every	O
proposal	O
(	O
whether	O
small	O
or	O
big	O
)	O
in	O
the	O
image	O
.	O

This	O
is	O
unlike	O
R	B-Method
-	I-Method
CNN	E-Method
,	O
where	O
each	O
proposal	O
is	O
resized	O
to	O
a	O
canonical	O
size	O
of	O
x	O
pixels	O
.	O

Large	O
objects	O
are	O
not	O
upsampled	O
and	O
small	O
objects	O
are	O
not	O
downsampled	O
in	O
R	B-Method
-	I-Method
CNN	E-Method
.	O

In	O
this	O
regard	O
,	O
R	B-Method
-	I-Method
CNN	E-Method
more	O
appropriately	O
does	O
not	O
up	O
/	O
downsample	O
every	O
pixel	O
in	O
the	O
image	O
but	O
only	O
in	O
those	O
regions	O
which	O
are	O
likely	O
to	O
contain	O
objects	O
to	O
an	O
appropriate	O
resolution	O
.	O

However	O
,	O
R	B-Method
-	I-Method
CNN	E-Method
does	O
not	O
share	O
the	O
convolutional	O
features	O
for	O
nearby	O
proposals	O
like	O
Fast	B-Method
-	I-Method
RCNN	E-Method
,	O
which	O
makes	O
it	O
slow	O
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
SNIPER	S-Method
,	O
which	O
retains	O
the	O
benefits	O
of	O
both	O
these	O
approaches	O
by	O
generating	O
scale	O
specific	O
context	O
-	O
regions	O
(	O
chips	O
)	O
that	O
cover	O
maximum	O
proposals	O
at	O
a	O
particular	O
scale	O
.	O

SNIPER	S-Method
classifies	O
all	O
the	O
proposals	O
inside	O
a	O
chip	O
like	O
Fast	B-Method
-	I-Method
RCNN	E-Method
which	O
enables	O
us	O
to	O
perform	O
efficient	O
classification	S-Task
of	O
multiple	O
proposals	O
within	O
a	O
chip	O
.	O

As	O
SNIPER	S-Method
does	O
not	O
upsample	O
the	O
image	O
where	O
there	O
are	O
large	O
objects	O
and	O
also	O
does	O
not	O
process	O
easy	O
background	O
regions	O
,	O
it	O
is	O
significantly	O
faster	O
compared	O
to	O
a	O
Fast	B-Method
-	I-Method
RCNN	I-Method
detector	E-Method
trained	O
on	O
an	O
image	O
pyramid	O
.	O

SNIP	S-Method
is	O
also	O
trained	O
on	O
almost	O
all	O
the	O
pixels	O
of	O
the	O
image	O
pyramid	O
(	O
like	O
Fast	B-Method
-	I-Method
RCNN	E-Method
)	O
,	O
although	O
gradients	O
arising	O
from	O
objects	O
of	O
extreme	O
resolutions	O
are	O
ignored	O
.	O

In	O
particular	O
,	O
resolutions	O
of	O
the	O
image	O
pyramid	O
(	O
and	O
pixels	O
)	O
always	O
engage	O
in	O
training	O
and	O
multiple	O
pixel	O
crops	O
are	O
sampled	O
out	O
of	O
the	O
pixel	O
resolution	O
of	O
the	O
image	O
in	O
the	O
finest	O
scale	O
.	O

SNIPER	S-Method
takes	O
this	O
cropping	B-Method
procedure	E-Method
to	O
an	O
extreme	O
level	O
by	O
sampling	O
pixels	O
crops	O
from	O
scales	O
of	O
an	O
image	O
pyramid	O
.	O

At	O
extreme	O
scales	O
(	O
like	O
x	O
)	O
,	O
SNIPER	S-Method
observes	O
less	O
than	O
one	O
tenth	O
of	O
the	O
original	O
content	O
present	O
in	O
the	O
image	O
!	O
Unfortunately	O
,	O
as	O
SNIPER	S-Method
chips	O
generated	O
only	O
using	O
ground	O
-	O
truth	O
instances	O
are	O
very	O
small	O
compared	O
to	O
the	O
resolution	O
of	O
the	O
original	O
image	O
,	O
a	O
significant	O
portion	O
of	O
the	O
background	O
does	O
not	O
participate	O
in	O
training	O
.	O

This	O
causes	O
the	O
false	B-Metric
positive	I-Metric
rate	E-Metric
to	O
increase	O
.	O

Therefore	O
,	O
it	O
is	O
important	O
to	O
generate	O
chips	O
for	O
background	O
regions	O
as	O
well	O
.	O

In	O
SNIPER	S-Method
,	O
this	O
is	O
achieved	O
by	O
randomly	O
sampling	O
a	O
fixed	O
number	O
of	O
chips	O
(	O
maximum	O
of	O
in	O
this	O
paper	O
)	O
from	O
regions	O
which	O
are	O
likely	O
to	O
cover	O
false	O
positives	O
.	O

To	O
find	O
such	O
regions	O
,	O
we	O
train	O
a	O
lightweight	B-Method
RPN	I-Method
network	E-Method
with	O
a	O
short	O
schedule	O
.	O

The	O
proposals	O
of	O
this	O
network	O
are	O
used	O
to	O
generate	O
chips	O
for	O
regions	O
which	O
are	O
likely	O
to	O
contain	O
false	O
positives	O
(	O
this	O
could	O
potentially	O
be	O
replaced	O
with	O
unsupervised	B-Method
proposals	E-Method
like	O
EdgeBoxes	S-Method
as	O
well	O
)	O
.	O

After	O
adding	O
negative	B-Method
chip	I-Method
sampling	E-Method
,	O
the	O
performance	O
of	O
SNIPER	S-Method
matches	O
SNIP	S-Method
,	O
but	O
it	O
is	O
times	O
faster	O
!	O
Since	O
we	O
are	O
able	O
to	O
obtain	O
similar	O
performance	O
by	O
observing	O
less	O
than	O
one	O
tenth	O
of	O
the	O
image	O
,	O
it	O
implies	O
that	O
very	O
large	O
context	O
during	O
training	O
is	O
not	O
important	O
for	O
training	O
high	O
-	O
performance	O
detectors	S-Task
but	O
sampling	O
regions	O
containing	O
hard	O
negatives	O
is	O
.	O

section	O
:	O
SNIPER	S-Method
We	O
describe	O
the	O
major	O
components	O
of	O
SNIPER	S-Method
in	O
this	O
section	O
.	O

One	O
is	O
positive	B-Task
/	I-Task
negative	I-Task
chip	I-Task
mining	E-Task
and	O
the	O
other	O
is	O
label	B-Task
assignment	E-Task
after	O
chips	O
are	O
generated	O
.	O

Finally	O
,	O
we	O
will	O
discuss	O
the	O
benefits	O
of	O
training	O
with	O
SNIPER	S-Method
.	O

subsection	O
:	O
Chip	B-Task
Generation	E-Task
SNIPER	S-Method
generates	O
chips	O
at	O
multiple	O
scales	O
in	O
the	O
image	O
.	O

For	O
each	O
scale	O
,	O
the	O
image	O
is	O
first	O
re	O
-	O
sized	O
to	O
width	O
(	O
)	O
and	O
height	O
(	O
)	O
.	O

On	O
this	O
canvas	O
,	O
pixel	O
chips	O
are	O
placed	O
at	O
equal	O
intervals	O
of	O
pixels	O
(	O
we	O
set	O
to	O
in	O
this	O
paper	O
)	O
.	O

This	O
leads	O
to	O
a	O
two	O
-	O
dimensional	O
array	O
of	O
chips	O
at	O
each	O
scale	O
.	O

subsection	O
:	O
Positive	B-Method
Chip	I-Method
Selection	E-Method
For	O
each	O
scale	O
,	O
there	O
is	O
a	O
desired	O
area	O
range	O
,	O
which	O
determines	O
which	O
ground	O
-	O
truth	O
boxes	O
/	O
proposals	O
participate	O
in	O
training	O
for	O
each	O
scale	O
.	O

The	O
valid	O
list	O
of	O
ground	O
-	O
truth	O
bounding	O
boxes	O
which	O
lie	O
in	O
are	O
referred	O
to	O
as	O
.	O

Then	O
,	O
chips	O
are	O
greedily	O
selected	O
so	O
that	O
maximum	O
number	O
of	O
valid	O
ground	O
-	O
truth	O
boxes	O
(	O
)	O
are	O
covered	O
.	O

A	O
ground	O
-	O
truth	O
box	O
is	O
said	O
to	O
be	O
covered	O
if	O
it	O
is	O
completely	O
enclosed	O
inside	O
a	O
chip	O
.	O

All	O
the	O
positive	O
chips	O
from	O
a	O
scale	O
are	O
combined	O
per	O
image	O
and	O
are	O
referred	O
to	O
as	O
.	O

For	O
each	O
ground	O
-	O
truth	O
bounding	O
box	O
,	O
there	O
always	O
exists	O
a	O
chip	O
which	O
covers	O
it	O
.	O

Since	O
consecutive	O
contain	O
overlapping	O
intervals	O
,	O
a	O
ground	O
-	O
truth	O
bounding	O
box	O
may	O
be	O
assigned	O
to	O
multiple	O
chips	O
at	O
different	O
scales	O
.	O

It	O
is	O
also	O
possible	O
that	O
the	O
same	O
ground	O
-	O
truth	O
bounding	O
box	O
may	O
be	O
in	O
multiple	O
chips	O
from	O
the	O
same	O
scale	O
.	O

Ground	O
-	O
truth	O
instances	O
which	O
have	O
a	O
partial	O
overlap	O
(	O
IoU	O
>	O
0	O
)	O
with	O
a	O
chip	O
are	O
cropped	O
.	O

All	O
the	O
cropped	O
ground	O
-	O
truth	O
boxes	O
(	O
valid	O
or	O
invalid	O
)	O
are	O
retained	O
in	O
the	O
chip	O
and	O
are	O
used	O
in	O
label	B-Task
assignment	E-Task
.	O

In	O
this	O
way	O
,	O
every	O
ground	O
-	O
truth	O
box	O
is	O
covered	O
at	O
the	O
appropriate	O
scale	O
.	O

Since	O
the	O
crop	O
-	O
size	O
is	O
much	O
smaller	O
than	O
the	O
resolution	O
of	O
the	O
image	O
(	O
i.e.	O
more	O
than	O
x	O
smaller	O
for	O
high	O
-	O
resolution	O
images	O
)	O
,	O
SNIPER	S-Method
does	O
not	O
process	O
most	O
of	O
the	O
background	O
at	O
high	O
-	O
resolutions	O
.	O

This	O
leads	O
to	O
significant	O
savings	O
in	O
computation	B-Metric
and	I-Metric
memory	I-Metric
requirement	E-Metric
while	O
processing	O
high	B-Task
-	I-Task
resolution	I-Task
images	E-Task
.	O

We	O
illustrate	O
this	O
with	O
an	O
example	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
left	O
side	O
of	O
the	O
figure	O
shows	O
the	O
image	O
with	O
the	O
ground	O
-	O
truth	O
boxes	O
represented	O
by	O
green	O
bounding	O
boxes	O
.	O

Other	O
colored	O
rectangles	O
on	O
the	O
left	O
side	O
of	O
the	O
figure	O
show	O
the	O
chips	O
generated	O
by	O
SNIPER	S-Method
in	O
the	O
original	O
image	O
resolution	O
which	O
cover	O
all	O
objects	O
.	O

These	O
chips	O
are	O
illustrated	O
on	O
the	O
right	O
side	O
of	O
the	O
figure	O
with	O
the	O
same	O
border	O
color	O
.	O

Green	O
and	O
red	O
bounding	O
boxes	O
represent	O
the	O
valid	O
and	O
invalid	O
ground	O
-	O
truth	O
objects	O
corresponding	O
to	O
the	O
scale	O
of	O
the	O
chip	O
.	O

As	O
can	O
be	O
seen	O
,	O
in	O
this	O
example	O
,	O
SNIPER	S-Method
efficiently	O
processes	O
all	O
ground	O
-	O
truth	O
objects	O
in	O
an	O
appropriate	O
scale	O
by	O
forming	O
low	B-Method
-	I-Method
resolution	I-Method
chips	E-Method
.	O

subsection	O
:	O
Negative	B-Method
Chip	I-Method
Selection	E-Method
Although	O
positive	O
chips	O
cover	O
all	O
the	O
positive	O
instances	O
,	O
a	O
significant	O
portion	O
of	O
the	O
background	O
is	O
not	O
covered	O
by	O
them	O
.	O

Incorrectly	O
classifying	B-Metric
background	E-Metric
increases	O
the	O
false	B-Metric
positive	I-Metric
rate	E-Metric
.	O

In	O
current	O
object	B-Task
detection	E-Task
algorithms	O
,	O
when	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
is	O
performed	O
,	O
every	O
pixel	O
in	O
the	O
image	O
is	O
processed	O
at	O
all	O
scales	O
.	O

Although	O
training	O
on	O
all	O
scales	O
reduces	O
the	O
false	B-Metric
positive	I-Metric
rate	E-Metric
,	O
it	O
also	O
increases	O
computation	S-Metric
.	O

We	O
posit	O
that	O
a	O
significant	O
amount	O
of	O
the	O
background	O
is	O
easy	O
to	O
classify	O
and	O
hence	O
,	O
we	O
can	O
avoid	O
performing	O
any	O
computation	O
in	O
those	O
regions	O
.	O

So	O
,	O
how	O
do	O
we	O
eliminate	O
regions	O
which	O
are	O
easy	O
to	O
classify	O
?	O
A	O
simple	O
approach	O
is	O
to	O
employ	O
object	B-Method
proposals	E-Method
to	O
identify	O
regions	O
where	O
objects	O
are	O
likely	O
to	O
be	O
present	O
.	O

After	O
all	O
,	O
our	O
classifier	S-Method
operates	O
on	O
region	O
proposals	O
and	O
if	O
there	O
are	O
no	O
region	O
proposals	O
in	O
a	O
part	O
of	O
the	O
image	O
,	O
it	O
implies	O
that	O
it	O
is	O
very	O
easy	O
to	O
classify	O
as	O
background	O
.	O

Hence	O
,	O
we	O
can	O
ignore	O
those	O
parts	O
of	O
the	O
image	O
during	O
training	O
.	O

To	O
this	O
end	O
,	O
for	O
negative	B-Task
chip	I-Task
mining	E-Task
,	O
we	O
first	O
train	O
RPN	S-Method
for	O
a	O
couple	O
of	O
epochs	O
.	O

No	O
negative	O
chips	O
are	O
used	O
for	O
training	O
this	O
network	O
.	O

The	O
task	O
of	O
this	O
network	O
is	O
to	O
roughly	O
guide	O
us	O
in	O
selecting	O
regions	O
which	O
are	O
likely	O
to	O
contain	O
false	O
positives	O
,	O
so	O
it	O
is	O
not	O
necessary	O
for	O
it	O
to	O
be	O
very	O
accurate	O
.	O

This	O
RPN	S-Method
is	O
used	O
to	O
generate	O
proposals	O
over	O
the	O
entire	O
training	O
set	O
.	O

We	O
assume	O
that	O
if	O
no	O
proposals	O
are	O
generated	O
in	O
a	O
major	O
portion	O
of	O
the	O
image	O
by	O
RPN	S-Method
,	O
then	O
it	O
is	O
unlikely	O
to	O
contain	O
an	O
object	O
instance	O
.	O

For	O
negative	B-Task
chip	I-Task
selection	E-Task
,	O
for	O
each	O
scale	O
,	O
we	O
first	O
remove	O
all	O
the	O
proposals	O
which	O
have	O
been	O
covered	O
in	O
.	O

Then	O
,	O
for	O
each	O
scale	O
,	O
we	O
greedily	O
select	O
all	O
the	O
chips	O
which	O
cover	O
at	O
least	O
proposals	O
in	O
.	O

This	O
generates	O
a	O
set	O
of	O
negative	O
chips	O
for	O
each	O
scale	O
per	O
image	O
,	O
.	O

During	O
training	S-Task

,	O
we	O
randomly	O
sample	O
a	O
fixed	O
number	O
of	O
negative	O
chips	O
per	O
epoch	O
(	O
per	O
image	O
)	O
from	O
this	O
pool	O
of	O
negative	O
chips	O
which	O
are	O
generated	O
from	O
all	O
scales	O
,	O
i.e.	O
.	O


Figure	O
[	O
reference	O
]	O
shows	O
examples	O
of	O
the	O
generated	O
negative	O
chips	O
by	O
SNIPER	S-Method
.	O

The	O
first	O
row	O
shows	O
the	O
image	O
and	O
the	O
ground	O
-	O
truth	O
boxes	O
.	O

In	O
the	O
bottom	O
row	O
,	O
we	O
show	O
the	O
proposals	O
not	O
covered	O
by	O
and	O
the	O
corresponding	O
negative	O
chips	O
generated	O
(	O
the	O
orange	O
boxes	O
)	O
.	O

However	O
,	O
for	O
clarity	O
,	O
we	O
represent	O
each	O
proposal	O
by	O
a	O
red	O
circle	O
in	O
its	O
center	O
.	O

As	O
illustrated	O
,	O
SNIPER	S-Method
only	O
processes	O
regions	O
which	O
likely	O
contain	O
false	O
positives	O
,	O
leading	O
to	O
faster	O
processing	B-Metric
time	E-Metric
.	O

subsection	O
:	O
Label	B-Task
Assignment	E-Task
Our	O
network	O
is	O
trained	O
end	O
to	O
end	O
on	O
these	O
chips	O
like	O
Faster	B-Method
-	I-Method
RCNN	E-Method
,	O
i.e.	O
it	O
learns	O
to	O
generate	O
proposals	O
as	O
well	O
as	O
classify	O
them	O
with	O
a	O
single	O
network	O
.	O

While	O
training	O
,	O
proposals	O
generated	O
by	O
RPN	S-Method
are	O
assigned	O
labels	O
and	O
bounding	O
box	O
targets	O
(	O
for	O
regression	S-Task
)	O
based	O
on	O
all	O
the	O
ground	O
-	O
truth	O
boxes	O
which	O
are	O
present	O
inside	O
the	O
chip	O
.	O

We	O
do	O
not	O
filter	O
ground	O
-	O
truth	O
boxes	O
based	O
on	O
.	O

Instead	O
,	O
proposals	O
which	O
do	O
not	O
fall	O
in	O
are	O
ignored	O
during	O
training	O
.	O

So	O
,	O
a	O
large	O
ground	O
-	O
truth	O
box	O
which	O
is	O
cropped	O
,	O
could	O
generate	O
a	O
valid	O
proposal	O
which	O
is	O
small	O
.	O

Like	O
Fast	B-Method
-	I-Method
RCNN	E-Method
,	O
we	O
mark	O
any	O
proposal	O
which	O
has	O
an	O
overlap	O
greater	O
than	O
0.5	O
with	O
a	O
ground	O
-	O
truth	O
box	O
as	O
positive	O
and	O
assign	O
bounding	O
-	O
box	O
targets	O
for	O
the	O
proposal	O
.	O

Our	O
network	O
is	O
trained	O
end	O
to	O
end	O
and	O
we	O
generate	O
300	O
proposals	O
per	O
chip	O
.	O

We	O
do	O
not	O
apply	O
any	O
constraint	O
that	O
a	O
fraction	O
of	O
these	O
proposals	O
should	O
be	O
re	O
-	O
sampled	O
as	O
positives	O
,	O
as	O
in	O
Fast	B-Method
-	I-Method
RCNN	E-Method
.	O

We	O
did	O
not	O
use	O
OHEM	S-Method
for	O
classification	S-Task
and	O
use	O
a	O
simple	O
softmax	B-Method
cross	I-Method
-	I-Method
entropy	I-Method
loss	E-Method
for	O
classification	S-Task
.	O

For	O
assigning	B-Task
RPN	I-Task
labels	E-Task
,	O
we	O
use	O
valid	O
ground	O
-	O
truth	O
boxes	O
to	O
assign	O
labels	O
and	O
invalid	O
ground	O
-	O
truth	O
boxes	O
to	O
invalidate	O
anchors	O
,	O
as	O
done	O
in	O
SNIP	S-Method
.	O

subsection	O
:	O
Benefits	O
For	O
training	O
,	O
we	O
randomly	O
sample	O
chips	O
from	O
the	O
whole	O
dataset	O
for	O
generating	O
a	O
batch	O
.	O

On	O
average	O
,	O
we	O
generate	O
chips	O
of	O
size	O
x	O
per	O
image	O
on	O
the	O
COCO	B-Material
dataset	E-Material
(	O
including	O
negative	O
chips	O
)	O
when	O
training	O
on	O
three	O
scales	O
(	O
/	O
ms	O
,	O
,	O
)	O
.	O

This	O
is	O
only	O
%	O
more	O
than	O
the	O
number	O
of	O
pixels	O
processed	O
per	O
image	O
when	O
single	B-Method
scale	I-Method
training	E-Method
is	O
performed	O
with	O
an	O
image	O
resolution	O
of	O
x	O
.	O

Since	O
all	O
our	O
images	O
are	O
of	O
the	O
same	O
size	O
,	O
data	O
is	O
much	O
better	O
packed	O
leading	O
to	O
better	O
GPU	O
utilization	O
which	O
easily	O
overcomes	O
the	O
extra	O
%	O
overhead	O
.	O

But	O
more	O
importantly	O
,	O
we	O
reap	O
the	O
benefits	O
of	O
multi	B-Method
-	I-Method
scale	I-Method
training	E-Method
on	O
3	O
scales	O
,	O
large	O
batch	O
size	O
and	O
training	S-Task
with	O
batch	B-Method
-	I-Method
normalization	E-Method
without	O
any	O
slowdown	O
in	O
performance	O
on	O
a	O
single	O
8	O
GPU	O
node	O
!	O
.	O

It	O
is	O
commonly	O
believed	O
that	O
high	O
resolution	O
images	O
(	O
e.g.	O
x	O
)	O
are	O
necessary	O
for	O
instance	B-Task
level	I-Task
recognition	I-Task
tasks	E-Task
.	O

Therefore	O
,	O
for	O
instance	B-Task
level	I-Task
recognition	I-Task
tasks	E-Task
,	O
it	O
was	O
not	O
possible	O
to	O
train	O
with	O
batch	O
-	O
normalization	O
statistics	O
computed	O
on	O
a	O
single	O
GPU	S-Method
.	O

Methods	O
like	O
synchronized	B-Method
batch	I-Method
-	I-Method
normalization	E-Method
or	O
training	S-Method
on	O
GPUs	S-Method
have	O
been	O
proposed	O
to	O
alleviate	O
this	O
problem	O
.	O

Synchronized	B-Method
batch	I-Method
-	I-Method
normalization	E-Method
slows	O
down	O
training	S-Task
significantly	O
and	O
training	O
on	O
GPUs	S-Method
is	O
also	O
impractical	O
for	O
most	O
people	O
.	O

Therefore	O
,	O
group	B-Method
normalization	E-Method
has	O
been	O
recently	O
proposed	O
so	O
that	O
instance	B-Task
level	I-Task
recognition	I-Task
tasks	E-Task
can	O
benefit	O
from	O
another	O
form	O
of	O
normalization	S-Task
in	O
a	O
low	B-Task
batch	I-Task
setting	E-Task
during	O
training	O
.	O

With	O
SNIPER	S-Method
,	O
we	O
show	O
that	O
the	O
image	B-Task
resolution	I-Task
bottleneck	E-Task
can	O
be	O
alleviated	O
for	O
instance	B-Task
level	I-Task
recognition	I-Task
tasks	E-Task
.	O

As	O
long	O
as	O
we	O
can	O
cover	O
negatives	O
and	O
use	O
appropriate	O
scale	B-Method
normalization	I-Method
methods	E-Method
,	O
we	O
can	O
train	O
with	O
a	O
large	O
batch	O
size	O
of	O
resampled	O
low	O
resolution	O
chips	O
,	O
even	O
on	O
challenging	O
datasets	O
like	O
COCO	S-Material
.	O

Our	O
results	O
suggest	O
that	O
context	O
beyond	O
a	O
certain	O
field	O
of	O
view	O
may	O
not	O
be	O
beneficial	O
during	O
training	O
.	O

It	O
is	O
also	O
possible	O
that	O
the	O
effective	O
receptive	O
field	O
of	O
deep	B-Method
neural	I-Method
networks	E-Method
is	O
not	O
large	O
enough	O
to	O
leverage	O
far	O
away	O
pixels	O
in	O
the	O
image	O
,	O
as	O
suggested	O
in	O
.	O

In	O
very	O
large	O
datasets	O
like	O
OpenImagesV4	B-Material
containing	I-Material
million	I-Material
images	E-Material
,	O
most	O
objects	O
are	O
large	O
and	O
images	O
provided	O
are	O
high	O
resolution	O
(	O
x	O
)	O
,	O
so	O
it	O
is	O
less	O
important	O
to	O
upsample	O
images	O
by	O
.	O

In	O
this	O
case	O
,	O
with	O
SNIPER	S-Method
,	O
we	O
generate	O
million	O
chips	O
of	O
size	O
x	O
using	O
scales	O
of	O
(	O
/	O
ms	O
,	O
1	O
)	O
.	O

Note	O
that	O
SNIPER	S-Method
also	O
performs	O
adaptive	B-Method
downsampling	E-Method
.	O

Since	O
the	O
scales	O
are	O
smaller	O
,	O
chips	O
would	O
cover	O
more	O
background	O
,	O
due	O
to	O
which	O
the	O
impact	O
of	O
negative	B-Method
sampling	E-Method
is	O
diminished	O
.	O

In	O
this	O
case	O
(	O
of	O
positive	B-Task
chip	I-Task
selection	E-Task
)	O
,	O
SNIPER	S-Method
processes	O
only	O
half	O
the	O
number	O
of	O
pixels	O
compared	O
to	O
naïve	O
multi	B-Method
-	I-Method
scale	I-Method
training	E-Method
on	O
the	O
above	O
mentioned	O
scales	O
in	O
OpenImagesV4	S-Material
.	O

Due	O
to	O
this	O
,	O
we	O
were	O
able	O
to	O
train	O
Faster	B-Method
-	I-Method
RCNN	E-Method
with	O
a	O
ResNet	B-Method
-	I-Method
101	I-Method
backbone	E-Method
on	O
million	O
images	O
in	O
just	O
days	O
on	O
a	O
single	O
GPU	O
node	O
!	O
section	O
:	O
Experimental	O
Details	O
We	O
evaluate	O
SNIPER	S-Method
on	O
the	O
COCO	B-Material
dataset	E-Material
for	O
object	B-Task
detection	E-Task
.	O

COCO	S-Material
contains	O
123	O
,	O
000	O
images	O
in	O
the	O
training	O
and	O
validation	O
set	O
and	O
20	O
,	O
288	O
images	O
in	O
the	O
test	O
-	O
dev	O
set	O
.	O

We	O
train	O
on	O
the	O
combined	O
training	O
and	O
validation	O
set	O
and	O
report	O
results	O
on	O
the	O
test	O
-	O
dev	O
set	O
.	O

Since	O
recall	S-Metric
for	O
proposals	S-Task
is	O
not	O
provided	O
by	O
the	O
evaluation	O
server	O
,	O
we	O
train	O
on	O
118	O
,	O
000	O
images	O
and	O
report	O
recall	S-Metric
on	O
the	O
remaining	O
5	O
,	O
000	O
images	O
(	O
commonly	O
referred	O
to	O
as	O
the	O
minival	O
set	O
)	O
.	O

On	O
COCO	S-Material
,	O
we	O
train	O
SNIPER	S-Method
with	O
a	O
batch	B-Metric
-	I-Metric
size	E-Metric
of	O
and	O
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
.	O

We	O
use	O
a	O
chip	O
size	O
of	O
pixels	O
.	O

Training	O
scales	O
are	O
set	O
to	O
(	O
/	O
ms	O
,	O
,	O
)	O
where	O
is	O
the	O
maximum	O
value	O
width	O
and	O
height	O
of	O
the	O
image	O
.	O

The	O
desired	O
area	O
ranges	O
(	O
i.e.	O
)	O
are	O
set	O
to	O
(	O
,	O
)	O
,	O
(	O
,	O
)	O
,	O
and	O
(	O
,	O
)	O
for	O
each	O
of	O
the	O
scales	O
respectively	O
.	O

Training	S-Method
is	O
performed	O
for	O
a	O
total	O
of	O
epochs	O
with	O
step	O
-	O
down	O
at	O
the	O
end	O
of	O
epoch	O
.	O

Image	B-Task
flipping	E-Task
is	O
used	O
as	O
a	O
data	B-Method
-	I-Method
augmentation	I-Method
technique	E-Method
.	O

Every	O
epoch	O
requires	O
11	O
,	O
000	O
iterations	O
.	O

For	O
training	O
RPN	B-Task
without	I-Task
negatives	E-Task
,	O
each	O
epoch	O
requires	O
7000	O
iterations	O
.	O

We	O
use	O
RPN	S-Method
for	O
generating	O
negative	O
chips	O
and	O
train	O
it	O
for	O
epochs	O
with	O
a	O
fixed	O
learning	B-Metric
rate	E-Metric
of	O
without	O
any	O
step	O
-	O
down	O
.	O

Therefore	O
,	O
training	O
RPN	S-Method
for	O
epochs	O
requires	O
less	O
than	O
%	O
of	O
the	O
total	O
training	B-Metric
time	E-Metric
.	O

RPN	B-Method
proposals	E-Method
are	O
extracted	O
from	O
all	O
scales	O
.	O

Note	O
that	O
inference	S-Task
takes	O
the	O
time	O
for	O
a	O
full	O
forward	O
-	O
backward	O
pass	O
and	O
we	O
do	O
not	O
perform	O
any	O
flipping	O
for	O
extracting	B-Task
proposals	E-Task
.	O

Hence	O
,	O
this	O
process	O
is	O
also	O
efficient	O
.	O

We	O
use	O
mixed	B-Method
precision	I-Method
training	E-Method
as	O
described	O
in	O
.	O

To	O
this	O
end	O
,	O
we	O
re	O
-	O
scale	O
weight	O
-	O
decay	O
by	O
,	O
drop	O
the	O
learning	B-Metric
rate	E-Metric
by	O
and	O
rescale	O
gradients	O
by	O
.	O

This	O
ensures	O
that	O
we	O
can	O
train	O
with	O
activations	O
of	O
half	O
precision	O
(	O
and	O
hence	O
x	O
larger	O
batch	O
size	O
)	O
without	O
any	O
loss	O
in	O
accuracy	S-Metric
.	O

We	O
use	O
fp32	O
weights	O
for	O
the	O
first	O
convolution	B-Method
layer	E-Method
,	O
last	B-Method
convolution	I-Method
layer	E-Method
in	O
RPN	S-Method
(	O
classification	B-Task
and	I-Task
regression	E-Task
)	O
and	O
the	O
fully	B-Method
connected	I-Method
layers	E-Method
in	O
Faster	B-Method
-	I-Method
RCNN	E-Method
.	O

We	O
evaluate	O
SNIPER	S-Method
using	O
a	O
popular	O
detector	S-Method
,	O
Faster	B-Method
-	I-Method
RCNN	E-Method
with	O
ResNets	S-Method
and	O
MobileNetV2	S-Material
.	O

Proposals	O
are	O
generated	O
using	O
RPN	S-Method
on	O
top	O
of	O
conv4	O
features	O
and	O
classification	S-Task
is	O
performed	O
after	O
concatenating	O
conv4	O
and	O
conv5	O
features	O
.	O

In	O
the	O
conv5	O
branch	O
,	O
we	O
use	O
deformable	B-Method
convolutions	E-Method
and	O
a	O
stride	O
of	O
.	O

We	O
use	O
a	O
dimensional	B-Method
feature	I-Method
map	E-Method
in	O
RPN	S-Method
.	O

For	O
the	O
classification	B-Task
branch	E-Task
,	O
we	O
first	O
project	O
the	O
concatenated	O
feature	O
map	O
to	O
dimensions	O
and	O
then	O
add	O
fully	B-Method
connected	I-Method
layers	E-Method
with	O
hidden	O
units	O
.	O

For	O
lightweight	B-Task
networks	E-Task
like	O
MobileNetv2	S-Method
,	O
to	O
preserve	O
the	O
computational	B-Metric
processing	I-Metric
power	E-Metric
of	O
the	O
network	O
,	O
we	O
did	O
not	O
make	O
any	O
architectural	O
changes	O
to	O
the	O
network	O
like	O
changing	O
the	O
stride	O
of	O
the	O
network	O
or	O
added	O
deformable	B-Method
convolutions	E-Method
.	O

We	O
reduced	O
the	O
RPN	O
dimension	O
to	O
and	O
size	O
of	O
fc	O
layers	O
to	O
from	O
.	O

RPN	S-Method
and	O
classification	B-Method
branch	E-Method
are	O
both	O
applied	O
on	O
the	O
layer	O
with	O
stride	O
for	O
MobileNetv2	S-Method
.	O

SNIPER	S-Method
generates	O
million	O
chips	O
for	O
the	O
COCO	B-Material
dataset	E-Material
after	O
the	O
images	O
are	O
flipped	O
.	O

This	O
results	O
in	O
around	O
chips	O
per	O
image	O
.	O

In	O
some	O
images	O
which	O
contain	O
many	O
object	O
instances	O
,	O
SNIPER	S-Method
can	O
generate	O
as	O
many	O
as	O
chips	O
and	O
others	O
where	O
there	O
is	O
a	O
single	O
large	O
salient	O
object	O
,	O
it	O
would	O
only	O
generate	O
a	O
single	O
chip	O
.	O

In	O
a	O
sense	O
,	O
it	O
reduces	O
the	O
imbalance	O
in	O
gradients	O
propagated	O
to	O
an	O
instance	O
level	O
which	O
is	O
present	O
in	O
detectors	S-Method
which	O
are	O
trained	O
on	O
full	O
resolution	O
images	O
.	O

At	O
least	O
in	O
theory	O
,	O
training	O
on	O
full	O
resolution	O
images	O
is	O
biased	O
towards	O
large	O
object	O
instances	O
.	O

subsection	O
:	O
Recall	O
Analysis	O
We	O
observe	O
that	O
recall	S-Metric
(	O
averaged	O
over	O
multiple	O
overlap	O
thresholds	O
0.5:0.05:0.95	O
)	O
for	O
RPN	S-Method
does	O
not	O
decrease	O
if	O
we	O
do	O
not	O
perform	O
negative	B-Method
sampling	E-Method
.	O

This	O
is	O
because	O
recall	S-Metric
does	O
not	O
account	O
for	O
false	O
positives	O
.	O

As	O
shown	O
in	O
Section	O
[	O
reference	O
]	O
,	O
this	O
is	O
in	O
contrast	O
to	O
mAP	S-Metric
for	O
detection	S-Task
in	O
which	O
negative	B-Task
sampling	E-Task
plays	O
an	O
important	O
role	O
.	O

Moreover	O
,	O
in	O
positive	B-Task
chip	I-Task
sampling	E-Task
,	O
we	O
do	O
cover	O
every	O
ground	O
truth	O
sample	O
.	O

Therefore	O
,	O
for	O
generating	B-Task
proposals	E-Task
,	O
it	O
is	O
sufficient	O
to	O
train	O
on	O
just	O
positive	O
samples	O
.	O

This	O
result	O
further	O
bolsters	O
SNIPER	S-Method
’s	O
strategy	O
of	O
finding	O
negatives	O
based	O
on	O
an	O
RPN	S-Method
in	O
which	O
the	O
training	O
is	O
performed	O
just	O
on	O
positive	O
samples	O
.	O

subsection	O
:	O
Negative	B-Task
Chip	I-Task
Mining	E-Task
and	O
Scale	S-Task
SNIPER	S-Method
uses	O
negative	B-Method
chip	I-Method
mining	E-Method
to	O
reduce	O
the	O
false	B-Metric
positive	I-Metric
rate	E-Metric
while	O
speeding	O
up	O
the	O
training	S-Task
by	O
skipping	O
the	O
easy	O
regions	O
inside	O
the	O
image	O
.	O

As	O
proposed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
use	O
a	O
region	B-Method
proposal	I-Method
network	E-Method
trained	O
with	O
a	O
short	B-Method
learning	I-Method
schedule	E-Method
to	O
find	O
such	O
regions	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
negative	B-Method
mining	I-Method
approach	E-Method
,	O
we	O
compare	O
SNIPER	B-Metric
’s	I-Metric
mean	I-Metric
average	I-Metric
precision	E-Metric
with	O
a	O
slight	O
variant	O
which	O
only	O
uses	O
positive	O
chips	O
during	O
training	O
(	O
denoted	O
as	O
SNIPER	S-Method
w	O
/	O
o	O
neg	O
)	O
.	O

All	O
other	O
parameters	O
remain	O
the	O
same	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
performance	O
of	O
these	O
models	O
.	O

The	O
proposed	O
negative	B-Method
chip	I-Method
mining	I-Method
approach	E-Method
noticeably	O
improves	O
AP	S-Metric
for	O
all	O
localization	B-Metric
thresholds	E-Metric
and	O
object	O
sizes	O
.	O

Noticeably	O
,	O
negative	B-Method
chip	I-Method
mining	E-Method
improves	O
the	O
average	B-Metric
precision	E-Metric
from	O
to	O
.	O

This	O
is	O
in	O
contrast	O
to	O
the	O
last	O
section	O
where	O
we	O
were	O
evaluating	O
proposals	O
.	O

This	O
is	O
because	O
mAP	S-Metric
is	O
affected	O
by	O
false	O
positives	O
.	O

If	O
we	O
do	O
not	O
include	O
regions	O
in	O
the	O
image	O
containing	O
negatives	O
which	O
are	O
similar	O
in	O
appearance	O
to	O
positive	O
instances	O
,	O
it	O
would	O
increase	O
our	O
false	B-Metric
positive	I-Metric
rate	E-Metric
and	O
adversely	O
affect	O
detection	S-Task
performance	O
.	O

SNIPER	S-Method
is	O
an	O
efficient	O
multi	B-Method
-	I-Method
scale	I-Method
training	I-Method
algorithm	E-Method
.	O

In	O
all	O
experiments	O
in	O
this	O
paper	O
we	O
use	O
the	O
aforementioned	O
three	O
scales	O
(	O
See	O
Section	O
[	O
reference	O
]	O
for	O
the	O
details	O
)	O
.	O

To	O
show	O
that	O
SNIPER	S-Method
effectively	O
benefits	O
from	O
multi	B-Method
-	I-Method
scale	I-Method
training	E-Method
,	O
we	O
reduce	O
the	O
number	O
of	O
scales	O
from	O
to	O
by	O
dropping	O
the	O
high	O
resolution	O
scale	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
mean	O
average	B-Metric
precision	E-Metric
for	O
SNIPER	S-Method
under	O
these	O
two	O
settings	O
.	O

As	O
can	O
be	O
seen	O
,	O
by	O
reducing	O
the	O
number	O
of	O
scales	O
,	O
the	O
performance	O
consistently	O
drops	O
by	O
a	O
large	O
margin	O
on	O
all	O
evaluation	B-Metric
metrics	E-Metric
.	O

subsection	O
:	O
Timing	O
It	O
takes	O
14	O
hours	O
to	O
train	O
SNIPER	S-Method
end	O
to	O
end	O
on	O
a	O
8	O
GPU	B-Method
V100	I-Method
node	E-Method
with	O
a	O
Faster	B-Method
-	I-Method
RCNN	E-Method
detector	O
which	O
has	O
a	O
ResNet	O
-	O
101	O
backbone	O
.	O

It	O
is	O
worth	O
noting	O
that	O
we	O
train	O
on	O
3	O
scales	O
of	O
an	O
image	O
pyramid	O
(	O
max	O
size	O
of	O
512	O
,	O
1.667	O
and	O
3	O
)	O
.	O

Training	O
RPN	S-Method
is	O
much	O
more	O
efficient	O
and	O
it	O
only	O
takes	O
2	O
hours	O
for	O
pre	B-Task
-	I-Task
training	E-Task
.	O

Not	O
only	O
is	O
SNIPER	S-Method
efficient	O
in	O
training	S-Task
,	O
it	O
can	O
also	O
process	O
around	O
5	O
images	O
per	O
second	O
on	O
a	O
single	O
V100	O
GPU	O
.	O

For	O
better	O
utilization	O
of	O
resources	O
,	O
we	O
run	O
multiple	O
processes	O
in	O
parallel	O
during	O
inference	S-Task
and	O
compute	O
the	O
average	B-Metric
time	E-Metric
it	O
takes	O
to	O
process	O
a	O
batch	O
of	O
100	O
images	O
.	O

subsection	O
:	O
Inference	S-Task
We	O
perform	O
inference	S-Task
on	O
an	O
image	O
pyramid	O
and	O
scale	O
the	O
original	O
image	O
to	O
the	O
following	O
resolutions	O
(	O
,	O
)	O
,	O
(	O
,	O
)	O
and	O
(	O
,	O
)	O
.	O

The	O
first	O
element	O
is	O
the	O
minimum	O
size	O
with	O
the	O
condition	O
that	O
the	O
maximum	O
size	O
does	O
not	O
exceed	O
the	O
second	O
element	O
.	O

The	O
valid	O
ranges	O
for	O
training	O
and	O
inference	S-Task
are	O
similar	O
to	O
SNIP	S-Method
.	O

For	O
combining	O
the	O
detections	O
,	O
we	O
use	O
Soft	B-Method
-	I-Method
NMS	E-Method
.	O

We	O
do	O
not	O
perform	O
flipping	O
,	O
iterative	B-Method
bounding	I-Method
box	I-Method
regression	E-Method
or	O
mask	B-Method
tightening	E-Method
.	O

subsection	O
:	O
Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
It	O
is	O
difficult	O
to	O
fairly	O
compare	O
different	O
detectors	O
as	O
they	O
differ	O
in	O
backbone	B-Method
architectures	E-Method
(	O
like	O
ResNet	S-Method
,	O
ResNext	O
,	O
Xception	S-Method
)	O
,	O
pre	O
-	O
training	O
data	O
(	O
e.g.	O
ImageNet	B-Material
-	I-Material
5k	E-Material
,	O
JFT	S-Material
,	O
OpenImages	S-Material
)	O
,	O
different	O
structures	O
in	O
the	O
underlying	O
network	O
(	O
e.g	O
multi	O
-	O
scale	O
features	O
,	O
deformable	O
convolutions	O
,	O
heavier	O
heads	O
,	O
anchor	O
sizes	O
,	O
path	B-Method
aggregation	E-Method
)	O
,	O
test	O
time	O
augmentations	O
like	O
flipping	O
,	O
mask	O
tightening	O
,	O
iterative	B-Method
bounding	I-Method
box	I-Method
regression	E-Method
etc	O
.	O

Therefore	O
,	O
we	O
compare	O
our	O
results	O
with	O
SNIP	S-Method
,	O
which	O
is	O
a	O
recent	O
method	O
for	O
training	O
object	B-Task
detectors	E-Task
on	O
an	O
image	B-Task
pyramid	E-Task
.	O

The	O
results	O
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Without	O
using	O
batch	B-Method
normalization	E-Method
,	O
SNIPER	S-Method
achieves	O
comparable	O
results	O
.	O

While	O
SNIP	S-Method
processes	O
almost	O
all	O
the	O
image	O
pyramid	O
,	O
SNIPER	S-Method
on	O
the	O
other	O
hand	O
,	O
reduces	O
the	O
computational	B-Metric
cost	E-Metric
by	O
skipping	O
easy	O
regions	O
.	O

Moreover	O
,	O
since	O
SNIPER	S-Method
operates	O
on	O
a	O
lower	O
resolution	O
input	O
,	O
it	O
reduces	O
the	O
memory	O
footprint	O
.	O

This	O
allows	O
us	O
to	O
increase	O
the	O
batch	O
size	O
and	O
unlike	O
SNIP	S-Method
,	O
we	O
can	O
benefit	O
from	O
batch	B-Method
normalization	E-Method
during	O
training	S-Task
.	O

With	O
batch	B-Method
normalization	E-Method
,	O
SNIPER	S-Method
significantly	O
outperforms	O
SNIP	S-Method
in	O
all	O
metrics	O
.	O

It	O
should	O
be	O
noted	O
that	O
not	O
only	O
the	O
proposed	O
method	O
is	O
more	O
accurate	O
,	O
it	O
is	O
also	O
faster	O
during	O
training	S-Task
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
for	O
a	O
Faster	B-Method
-	I-Method
RCNN	E-Method
architecture	O
with	O
a	O
ResNet	B-Method
-	I-Method
101	I-Method
backbone	E-Method
(	O
with	O
deformable	B-Method
convolutions	E-Method
)	O
,	O
our	O
reported	O
result	O
of	O
46.1	O
%	O
is	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

This	O
result	O
improves	O
to	O
46.8	O
%	O
if	O
we	O
pre	O
-	O
train	O
the	O
detector	S-Method
on	O
the	O
OpenImagesV4	B-Material
dataset	E-Material
.	O

Adding	O
an	O
instance	B-Method
segmentation	I-Method
head	E-Method
and	O
training	O
the	O
detection	S-Task
network	O
along	O
with	O
it	O
improves	O
the	O
performance	O
to	O
47.6	O
%	O
.	O

With	O
our	O
efficient	O
batch	B-Method
inference	I-Method
pipeline	E-Method
,	O
we	O
can	O
process	O
5	O
images	O
per	O
second	O
on	O
a	O
single	O
V100	O
GPU	O
and	O
still	O
obtain	O
an	O
mAP	S-Metric
of	O
47.6	O
%	O
.	O

This	O
implies	O
that	O
on	O
modern	O
GPUs	S-Method
,	O
it	O
is	O
practical	O
to	O
perform	O
inference	S-Task
on	O
an	O
image	B-Task
pyramid	E-Task
which	O
includes	O
high	O
resolutions	O
like	O
1400x2000	O
.	O

We	O
also	O
show	O
results	O
for	O
Faster	B-Method
-	I-Method
RCNN	E-Method
trained	O
with	O
MobileNetV2	S-Material
.	O

It	O
obtains	O
an	O
mAP	S-Metric
of	O
34.1	O
%	O
compared	O
to	O
the	O
SSDLite	B-Method
version	E-Method
which	O
obtained	O
22.1	O
%	O
.	O

This	O
again	O
highlights	O
the	O
importance	O
of	O
image	O
pyramids	O
(	O
and	O
SNIPER	S-Method
training	O
)	O
as	O
we	O
can	O
improve	O
the	O
performance	O
of	O
the	O
detector	O
by	O
12	O
%	O
.	O

We	O
also	O
show	O
results	O
for	O
instance	B-Task
segmentation	E-Task
.	O

The	O
network	B-Method
architecture	E-Method
is	O
same	O
as	O
Mask	B-Method
-	I-Method
RCNN	E-Method
,	O
just	O
that	O
we	O
do	O
not	O
use	O
FPN	S-Method
and	O
use	O
the	O
same	O
detection	S-Task
architecture	O
which	O
was	O
described	O
for	O
object	B-Task
detection	E-Task
.	O

For	O
multi	B-Task
-	I-Task
tasking	E-Task
,	O
we	O
tried	O
two	O
variants	O
of	O
loss	B-Method
functions	E-Method
for	O
training	O
the	O
mask	O
branch	O
.	O

One	O
was	O
a	O
foreground	B-Method
-	I-Method
background	I-Method
softmax	I-Method
function	E-Method
for	O
N	O
classes	O
and	O
another	O
was	O
a	O
N	B-Method
+	I-Method
1	I-Method
way	I-Method
softmax	I-Method
function	E-Method
.	O

For	O
instance	B-Task
segmentation	E-Task
,	O
the	O
network	O
which	O
is	O
trained	O
with	O
2	O
-	O
way	O
Softmax	O
loss	O
for	O
each	O
class	O
clearly	O
performs	O
better	O
.	O

But	O
,	O
for	O
object	B-Task
detection	E-Task
,	O
the	O
N	B-Method
+	I-Method
1	I-Method
way	I-Method
Softmax	I-Method
loss	E-Method
leads	O
to	O
slightly	O
better	O
results	O
.	O

We	O
only	O
use	O
3	O
scales	O
during	O
inference	S-Task
and	O
do	O
not	O
perform	O
flipping	O
,	O
mask	O
tightening	O
,	O
iterative	B-Method
bounding	I-Method
-	I-Method
box	I-Method
regression	E-Method
,	O
padding	O
masks	O
before	O
resizing	O
etc	O
.	O

Our	O
instance	B-Task
segmentation	E-Task
results	O
are	O
preliminary	O
and	O
we	O
have	O
only	O
trained	O
2	O
models	O
so	O
far	O
.	O

section	O
:	O
Related	O
Work	O
SNIPER	S-Method
benefits	O
from	O
multiple	O
techniques	O
which	O
were	O
developed	O
over	O
the	O
last	O
year	O
.	O

Notably	O
,	O
it	O
was	O
shown	O
that	O
it	O
is	O
important	O
to	O
train	O
with	O
batch	O
normalization	O
statistics	O
for	O
tasks	O
like	O
object	B-Task
detection	E-Task
and	O
semantic	B-Task
segmentation	E-Task
.	O

This	O
is	O
one	O
important	O
reason	O
for	O
SNIPER	S-Method
’s	O
better	O
performance	O
.	O

SNIPER	S-Method
also	O
benefits	O
from	O
a	O
large	O
batch	O
size	O
which	O
was	O
shown	O
to	O
be	O
effective	O
for	O
object	B-Task
detection	E-Task
.	O

Like	O
SNIP	S-Method
,	O
SNIPER	S-Method
ignores	O
gradients	O
of	O
objects	O
at	O
extreme	O
scales	O
in	O
the	O
image	O
pyramid	O
to	O
improve	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
.	O

In	O
the	O
past	O
,	O
many	O
different	O
methods	O
have	O
been	O
proposed	O
to	O
understand	O
the	O
role	O
of	O
context	O
,	O
scale	O
and	O
sampling	O
.	O

Considerable	O
importance	O
has	O
been	O
given	O
to	O
leveraging	O
features	O
of	O
different	O
layers	O
of	O
the	O
network	O
and	O
designing	O
architectures	O
for	O
explicitly	O
encoding	O
context	O
/	O
multi	O
-	O
scale	O
information	O
for	O
classification	S-Task
.	O

Our	O
results	O
highlight	O
that	O
context	O
may	O
not	O
be	O
very	O
important	O
for	O
training	O
high	O
performance	O
object	B-Task
detectors	E-Task
.	O

section	O
:	O
Conclusion	O
and	O
Future	O
Work	O
We	O
presented	O
an	O
algorithm	O
for	O
efficient	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
which	O
sampled	O
low	O
resolution	O
chips	O
from	O
a	O
multi	O
-	O
scale	O
image	O
pyramid	O
to	O
accelerate	O
multi	B-Task
-	I-Task
scale	I-Task
training	E-Task
by	O
a	O
factor	O
of	O
3	O
times	O
.	O

In	O
doing	O
so	O
,	O
SNIPER	S-Method
did	O
not	O
compromise	O
on	O
the	O
performance	O
of	O
the	O
detector	O
due	O
to	O
effective	O
sampling	B-Method
techniques	E-Method
for	O
positive	O
and	O
negative	O
chips	O
.	O

As	O
SNIPER	S-Method
operates	O
on	O
re	O
-	O
sampled	O
low	O
resolution	O
chips	O
,	O
it	O
can	O
be	O
trained	O
with	O
a	O
large	O
batch	O
size	O
on	O
a	O
single	O
GPU	S-Method
which	O
brings	O
it	O
closer	O
to	O
the	O
protocol	O
for	O
training	O
image	B-Task
classification	E-Task
.	O

This	O
is	O
in	O
contrast	O
with	O
the	O
common	O
practice	O
of	O
training	O
on	O
high	O
resolution	O
images	O
for	O
instance	B-Task
-	I-Task
level	I-Task
recognition	I-Task
tasks	E-Task
.	O

In	O
future	O
,	O
we	O
would	O
like	O
to	O
accelerate	O
multi	B-Task
-	I-Task
scale	I-Task
inference	E-Task
,	O
because	O
a	O
significant	O
portion	O
of	O
the	O
background	O
can	O
be	O
eliminated	O
without	O
performing	O
expensive	O
computation	O
.	O

It	O
would	O
also	O
be	O
interesting	O
to	O
evaluate	O
at	O
what	O
chip	O
resolution	O
does	O
context	O
start	O
to	O
hurt	O
the	O
performance	O
of	O
object	B-Method
detectors	E-Method
.	O

section	O
:	O
Acknowledgement	O
The	O
authors	O
would	O
like	O
to	O
thank	O
an	O
Amazon	O
Machine	B-Method
Learning	E-Method
gift	O
for	O
the	O
AWS	O
credits	O
used	O
for	O
this	O
research	O
.	O

The	O
research	O
is	O
based	O
upon	O
work	O
supported	O
by	O
the	O
Office	O
of	O
the	O
Director	O
of	O
National	O
Intelligence	O
(	O
ODNI	O
)	O
,	O
Intelligence	O
Advanced	O
Research	O
Projects	O
Activity	O
(	O
IARPA	O
)	O
,	O
via	O
DOI	O
/	O
IBC	O
Contract	O
Numbers	O
D17PC00287	O
and	O
D17PC00345	O
.	O

The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
Governmental	O
purposes	O
not	O
withstanding	O
any	O
copyright	O
annotation	O
thereon	O
.	O

Disclaimer	O
:	O
The	O
views	O
and	O
conclusions	O
contained	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
the	O
official	O
policies	O
or	O
endorsements	O
,	O
either	O
expressed	O
or	O
implied	O
of	O
IARPA	O
,	O
DOI	O
/	O
IBC	O
or	O
the	O
U.S.	O
Government	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Multilingual	B-Task
Part	I-Task
-	I-Task
of	I-Task
-	I-Task
Speech	I-Task
Tagging	E-Task
with	O
Bidirectional	B-Method
Long	I-Method
Short	I-Method
-	I-Method
Term	I-Method
Memory	I-Method
Models	E-Method
and	O
Auxiliary	B-Method
Loss	I-Method
Bidirectional	I-Method
long	I-Method
short	I-Method
-	I-Method
term	I-Method
memory	E-Method
(	O
bi	B-Method
-	I-Method
LSTM	E-Method
)	O
networks	O
have	O
recently	O
proven	O
successful	O
for	O
various	O
NLP	B-Task
sequence	I-Task
modeling	I-Task
tasks	E-Task
,	O
but	O
little	O
is	O
known	O
about	O
their	O
reliance	O
to	O
input	O
representations	O
,	O
target	O
languages	O
,	O
data	O
set	O
size	O
,	O
and	O
label	O
noise	O
.	O

We	O
address	O
these	O
issues	O
and	O
evaluate	O
bi	O
-	O
LSTMs	S-Method
with	O
word	B-Method
,	I-Method
character	I-Method
,	I-Method
and	I-Method
unicode	I-Method
byte	I-Method
embeddings	E-Method
for	O
POS	B-Task
tagging	E-Task
.	O

We	O
compare	O
bi	O
-	O
LSTMs	S-Method
to	O
traditional	O
POS	B-Method
taggers	E-Method
across	O
languages	O
and	O
data	O
sizes	O
.	O

We	O
also	O
present	O
a	O
novel	O
bi	B-Method
-	I-Method
LSTM	E-Method
model	O
,	O
which	O
combines	O
the	O
POS	B-Task
tagging	E-Task
loss	O
function	O
with	O
an	O
auxiliary	B-Method
loss	E-Method
function	O
that	O
accounts	O
for	O
rare	O
words	O
.	O

The	O
model	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
22	O
languages	O
,	O
and	O
works	O
especially	O
well	O
for	O
morphologically	O
complex	O
languages	O
.	O

Our	O
analysis	O
suggests	O
that	O
bi	O
-	O
LSTMs	S-Method
are	O
less	O
sensitive	O
to	O
training	O
data	O
size	O
and	O
label	O
corruptions	O
(	O
at	O
small	O
noise	O
levels	O
)	O
than	O
previously	O
assumed	O
.	O

section	O
:	O
Introduction	O
Recently	O
,	O
bidirectional	B-Method
long	I-Method
short	I-Method
-	I-Method
term	I-Method
memory	I-Method
networks	E-Method
(	O
bi	B-Method
-	I-Method
LSTM	E-Method
)	O
have	O
been	O
used	O
for	O
language	B-Task
modelling	E-Task
,	O
POS	B-Task
tagging	E-Task
,	O
transition	B-Task
-	I-Task
based	I-Task
dependency	I-Task
parsing	E-Task
,	O
fine	B-Task
-	I-Task
grained	I-Task
sentiment	I-Task
analysis	E-Task
,	O
syntactic	B-Task
chunking	E-Task
,	O
and	O
semantic	B-Task
role	I-Task
labeling	E-Task
.	O

LSTMs	S-Method
are	O
recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
in	O
which	O
layers	O
are	O
designed	O
to	O
prevent	O
vanishing	O
gradients	O
.	O

Bidirectional	O
LSTMs	S-Method
make	O
a	O
backward	O
and	O
forward	O
pass	O
through	O
the	O
sequence	O
before	O
passing	O
on	O
to	O
the	O
next	O
layer	O
.	O

For	O
further	O
details	O
,	O
see	O
.	O

We	O
consider	O
using	O
bi	O
-	O
LSTMs	S-Method
for	O
POS	B-Task
tagging	E-Task
.	O

Previous	O
work	O
on	O
using	O
deep	B-Method
learning	I-Method
-	I-Method
based	I-Method
methods	E-Method
for	O
POS	B-Task
tagging	E-Task
has	O
focused	O
either	O
on	O
a	O
single	O
language	O
or	O
a	O
small	O
set	O
of	O
languages	O
.	O

Instead	O
we	O
evaluate	O
our	O
models	O
across	O
22	O
languages	O
.	O

In	O
addition	O
,	O
we	O
compare	O
performance	O
with	O
representations	O
at	O
different	O
levels	O
of	O
granularity	O
(	O
words	O
,	O
characters	O
,	O
and	O
bytes	O
)	O
.	O

These	O
levels	O
of	O
representation	O
were	O
previously	O
introduced	O
in	O
different	O
efforts	O
,	O
but	O
a	O
comparative	O
evaluation	O
was	O
missing	O
.	O

Moreover	O
,	O
deep	B-Method
networks	E-Method
are	O
often	O
said	O
to	O
require	O
large	O
volumes	O
of	O
training	O
data	O
.	O

We	O
investigate	O
to	O
what	O
extent	O
bi	O
-	O
LSTMs	S-Method
are	O
more	O
sensitive	O
to	O
the	O
amount	O
of	O
training	O
data	O
and	O
label	O
noise	O
than	O
standard	O
POS	B-Method
taggers	E-Method
.	O

Finally	O
,	O
we	O
introduce	O
a	O
novel	O
model	O
,	O
a	O
bi	B-Method
-	I-Method
LSTM	E-Method
trained	O
with	O
auxiliary	B-Method
loss	E-Method
.	O

The	O
model	O
jointly	O
predicts	O
the	O
POS	O
and	O
the	O
log	O
frequency	O
of	O
the	O
word	O
.	O

The	O
intuition	O
behind	O
this	O
model	O
is	O
that	O
the	O
auxiliary	B-Method
loss	E-Method
,	O
being	O
predictive	O
of	O
word	O
frequency	O
,	O
helps	O
to	O
differentiate	O
the	O
representations	O
of	O
rare	O
and	O
common	O
words	O
.	O

We	O
indeed	O
observe	O
performance	O
gains	O
on	O
rare	O
and	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O

These	O
performance	O
gains	O
transfer	O
into	O
general	O
improvements	O
for	O
morphologically	O
rich	O
languages	O
.	O

paragraph	O
:	O
Contributions	O
In	O
this	O
paper	O
,	O
we	O
a	O
)	O
evaluate	O
the	O
effectiveness	O
of	O
different	O
representations	O
in	O
bi	O
-	O
LSTMs	S-Method
,	O
b	O
)	O
compare	O
these	O
models	O
across	O
a	O
large	O
set	O
of	O
languages	O
and	O
under	O
varying	O
conditions	O
(	O
data	O
size	O
,	O
label	O
noise	O
)	O
and	O
c	O
)	O
propose	O
a	O
novel	O
bi	B-Method
-	I-Method
LSTM	E-Method
model	O
with	O
auxiliary	B-Method
loss	E-Method
(	O
Logfreq	B-Method
)	E-Method
.	O

section	O
:	O
Tagging	S-Task
with	O
bi	O
-	O
LSTMs	B-Method
Recurrent	I-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
allow	O
the	O
computation	O
of	O
fixed	B-Task
-	I-Task
size	I-Task
vector	I-Task
representations	E-Task
for	O
word	O
sequences	O
of	O
arbitrary	O
length	O
.	O

An	O
RNN	S-Method
is	O
a	O
function	O
that	O
reads	O
in	O
vectors	O
and	O
produces	O
an	O
output	O
vector	O
,	O
that	O
depends	O
on	O
the	O
entire	O
sequence	O
.	O

The	O
vector	O
is	O
then	O
fed	O
as	O
an	O
input	O
to	O
some	O
classifier	S-Method
,	O
or	O
higher	B-Method
-	I-Method
level	I-Method
RNNs	E-Method
in	O
stacked	B-Method
/	I-Method
hierarchical	I-Method
models	E-Method
.	O

The	O
entire	O
network	O
is	O
trained	O
jointly	O
such	O
that	O
the	O
hidden	B-Method
representation	E-Method
captures	O
the	O
important	O
information	O
from	O
the	O
sequence	O
for	O
the	O
prediction	B-Task
task	E-Task
.	O

A	O
bidirectional	B-Method
recurrent	I-Method
neural	I-Method
network	E-Method
(	O
bi	B-Method
-	I-Method
RNN	E-Method
)	O
is	O
an	O
extension	O
of	O
an	O
RNN	S-Method
that	O
reads	O
the	O
input	O
sequence	O
twice	O
,	O
from	O
left	O
to	O
right	O
and	O
right	O
to	O
left	O
,	O
and	O
the	O
encodings	O
are	O
concatenated	O
.	O

The	O
literature	O
uses	O
the	O
term	O
bi	B-Method
-	I-Method
RNN	E-Method
to	O
refer	O
to	O
two	O
related	O
architectures	O
,	O
which	O
we	O
refer	O
to	O
here	O
as	O
“	O
context	O
bi	B-Method
-	I-Method
RNN	E-Method
”	O
and	O
“	O
sequence	B-Method
bi	I-Method
-	I-Method
RNN	E-Method
”	O
.	O

In	O
a	O
sequence	B-Method
bi	I-Method
-	I-Method
RNN	E-Method
(	O
bi	B-Method
-	I-Method
RNN	E-Method
)	O
,	O
the	O
input	O
is	O
a	O
sequence	O
of	O
vectors	O
and	O
the	O
output	O
is	O
a	O
concatenation	O
(	O
)	O
of	O
a	O
forward	O
(	O
)	O
and	O
reverse	O
(	O
)	O
RNN	S-Method
each	O
reading	O
the	O
sequence	O
in	O
a	O
different	O
directions	O
:	O
In	O
a	O
context	O
bi	B-Method
-	I-Method
RNN	E-Method
(	O
bi	B-Method
-	I-Method
RNN	E-Method
)	O
,	O
we	O
get	O
an	O
additional	O
input	O
indicating	O
a	O
sequence	O
position	O
,	O
and	O
the	O
resulting	O
vectors	O
result	O
from	O
concatenating	O
the	O
RNN	B-Method
encodings	E-Method
up	O
to	O
:	O
Thus	O
,	O
the	O
state	O
vector	O
in	O
this	O
bi	B-Method
-	I-Method
RNN	E-Method
encodes	O
information	O
at	O
position	O
and	O
its	O
entire	O
sequential	O
context	O
.	O

Another	O
view	O
of	O
the	O
context	O
bi	B-Method
-	I-Method
RNN	E-Method
is	O
of	O
taking	O
a	O
sequence	O
and	O
returning	O
the	O
corresponding	O
sequence	O
of	O
state	O
vectors	O
.	O

LSTMs	S-Method
are	O
a	O
variant	O
of	O
RNNs	S-Method
that	O
replace	O
the	O
cells	O
of	O
RNNs	S-Method
with	O
LSTM	B-Method
cells	E-Method
that	O
were	O
designed	O
to	O
prevent	O
vanishing	O
gradients	O
.	O

Bidirectional	O
LSTMs	S-Method
are	O
the	O
bi	B-Method
-	I-Method
RNN	E-Method
counterpart	O
based	O
on	O
LSTMs	S-Method
.	O

Our	O
basic	O
bi	B-Method
-	I-Method
LSTM	E-Method
tagging	O
model	O
is	O
a	O
context	O
bi	B-Method
-	I-Method
LSTM	E-Method
taking	O
as	O
input	O
word	O
embeddings	O
.	O

We	O
incorporate	O
subtoken	O
information	O
using	O
an	O
hierarchical	O
bi	B-Method
-	I-Method
LSTM	E-Method
architecture	O
.	O

We	O
compute	O
subtoken	O
-	O
level	O
(	O
either	O
characters	O
or	O
unicode	O
byte	O
)	O
embeddings	O
of	O
words	O
using	O
a	O
sequence	O
bi	B-Method
-	I-Method
LSTM	E-Method
at	O
the	O
lower	O
level	O
.	O

This	O
representation	O
is	O
then	O
concatenated	O
with	O
the	O
(	O
learned	O
)	O
word	O
embeddings	O
vector	O
which	O
forms	O
the	O
input	O
to	O
the	O
context	O
bi	B-Method
-	I-Method
LSTM	E-Method
at	O
the	O
next	O
layer	O
.	O

This	O
model	O
,	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
lower	O
part	O
in	O
left	O
figure	O
)	O
,	O
is	O
inspired	O
by	O
ballesteros	O
:	O
ea:2015	O
.	O

We	O
also	O
test	O
models	O
in	O
which	O
we	O
only	O
keep	O
sub	O
-	O
token	O
information	O
,	O
e.g.	O
,	O
either	O
both	O
byte	O
and	O
character	O
embeddings	O
(	O
Figure	O
[	O
reference	O
]	O
,	O
right	O
)	O
or	O
a	O
single	O
(	O
sub	B-Method
-)	I-Method
token	I-Method
representation	E-Method
alone	O
.	O

In	O
our	O
novel	O
model	O
,	O
cf	O
.	O

Figure	O
[	O
reference	O
]	O
left	O
,	O
we	O
train	O
the	O
bi	B-Method
-	I-Method
LSTM	E-Method
tagger	O
to	O
predict	O
both	O
the	O
tags	O
of	O
the	O
sequence	O
,	O
as	O
well	O
as	O
a	O
label	O
that	O
represents	O
the	O
log	O
frequency	O
of	O
the	O
token	O
as	O
estimated	O
from	O
the	O
training	O
data	O
.	O

Our	O
combined	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
is	O
now	O
:	O
,	O
where	O
stands	O
for	O
a	O
POS	O
tag	O
and	O
is	O
the	O
log	O
frequency	O
label	O
,	O
i.e.	O
,	O
.	O

Combining	O
this	O
log	B-Method
frequency	I-Method
objective	E-Method
with	O
the	O
tagging	B-Task
task	E-Task
can	O
be	O
seen	O
as	O
an	O
instance	O
of	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
in	O
which	O
the	O
labels	O
are	O
predicted	O
jointly	O
.	O

The	O
idea	O
behind	O
this	O
model	O
is	O
to	O
make	O
the	O
representation	O
predictive	O
for	O
frequency	O
,	O
which	O
encourages	O
the	O
model	O
to	O
not	O
share	O
representations	O
between	O
common	O
and	O
rare	O
words	O
,	O
thus	O
benefiting	O
the	O
handling	O
of	O
rare	O
tokens	O
.	O

section	O
:	O
Experiments	O
All	O
bi	B-Method
-	I-Method
LSTM	E-Method
models	O
were	O
implemented	O
in	O
CNN	S-Method
/	O
pycnn	S-Method
,	O
a	O
flexible	B-Method
neural	I-Method
network	I-Method
library	E-Method
.	O

For	O
all	O
models	O
we	O
use	O
the	O
same	O
hyperparameters	O
,	O
which	O
were	O
set	O
on	O
English	B-Material
dev	E-Material
,	O
i.e.	O
,	O
SGD	B-Method
training	E-Method
with	O
cross	B-Method
-	I-Method
entropy	I-Method
loss	E-Method
,	O
no	O
mini	O
-	O
batches	O
,	O
20	O
epochs	O
,	O
default	O
learning	B-Metric
rate	E-Metric
(	O
0.1	O
)	O
,	O
128	O
dimensions	O
for	O
word	O
embeddings	O
,	O
100	O
for	O
character	O
and	O
byte	O
embeddings	O
,	O
100	O
hidden	O
states	O
and	O
Gaussian	O
noise	O
with	O
=	O
0.2	O
.	O

As	O
training	S-Task
is	O
stochastic	O
in	O
nature	O
,	O
we	O
use	O
a	O
fixed	O
seed	O
throughout	O
.	O

Embeddings	S-Method
are	O
not	O
initialized	O
with	O
pre	O
-	O
trained	O
embeddings	O
,	O
except	O
when	O
reported	O
otherwise	O
.	O

In	O
that	O
case	O
we	O
use	O
off	O
-	O
the	O
-	O
shelf	O
polyglot	B-Method
embeddings	E-Method
.	O

No	O
further	O
unlabeled	O
data	O
is	O
considered	O
in	O
this	O
paper	O
.	O

The	O
code	O
is	O
released	O
at	O
:	O
paragraph	O
:	O
Taggers	O
We	O
want	O
to	O
compare	O
POS	O
taggers	O
under	O
varying	O
conditions	O
.	O

We	O
hence	O
use	O
three	O
different	O
types	O
of	O
taggers	S-Method
:	O
our	O
implementation	O
of	O
a	O
bi	B-Method
-	I-Method
LSTM	E-Method
;	O
Tnt	S-Method
—a	O
second	B-Method
order	I-Method
HMM	E-Method
with	O
suffix	B-Method
trie	I-Method
handling	E-Method
for	O
OOVs	S-Method
.	O

We	O
use	O
Tnt	S-Method
as	O
it	O
was	O
among	O
the	O
best	O
performing	O
taggers	S-Method
evaluated	O
in	O
horsmann	O
:	O
ea:2015	O
.	O

We	O
complement	O
the	O
NN	B-Method
-	I-Method
based	E-Method
and	O
HMM	B-Method
-	I-Method
based	I-Method
tagger	E-Method
with	O
a	O
CRF	B-Method
tagger	E-Method
,	O
using	O
a	O
freely	O
available	O
implementation	O
based	O
on	O
crfsuite	S-Method
.	O

subsection	O
:	O
Datasets	O
For	O
the	O
multilingual	B-Task
experiments	E-Task
,	O
we	O
use	O
the	O
data	O
from	O
the	O
Universal	O
Dependencies	O
project	O
v1.2	O
(	O
17	O
POS	O
)	O
with	O
the	O
canonical	O
data	O
splits	O
.	O

For	O
languages	O
with	O
token	B-Task
segmentation	I-Task
ambiguity	E-Task
we	O
use	O
the	O
provided	O
gold	B-Method
segmentation	E-Method
.	O

If	O
there	O
is	O
more	O
than	O
one	O
treebank	O
per	O
language	O
,	O
we	O
use	O
the	O
treebank	O
that	O
has	O
the	O
canonical	O
language	O
name	O
(	O
e.g.	O
,	O
Finnish	S-Material
instead	O
of	O
Finnish	S-Material
-	O
FTB	S-Material
)	O
.	O

We	O
consider	O
all	O
languages	O
that	O
have	O
at	O
least	O
60k	O
tokens	O
and	O
are	O
distributed	O
with	O
word	O
forms	O
,	O
resulting	O
in	O
22	O
languages	O
.	O

We	O
also	O
report	O
accuracies	S-Metric
on	O
WSJ	S-Material
(	O
45	O
POS	S-Material
)	O
using	O
the	O
standard	O
splits	O
.	O

The	O
overview	O
of	O
languages	O
is	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Results	O
Our	O
results	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

First	O
of	O
all	O
,	O
notice	O
that	O
TnT	S-Method
performs	O
remarkably	O
well	O
across	O
the	O
22	O
languages	O
,	O
closely	O
followed	O
by	O
CRF	S-Method
.	O

The	O
bi	B-Method
-	I-Method
LSTM	E-Method
tagger	O
(	O
)	O
without	O
lower	O
-	O
level	O
bi	B-Method
-	I-Method
LSTM	E-Method
for	O
subtokens	O
falls	O
short	O
,	O
outperforms	O
the	O
traditional	O
taggers	S-Method
only	O
on	O
3	O
languages	O
.	O

The	O
bi	B-Method
-	I-Method
LSTM	E-Method
model	O
clearly	O
benefits	O
from	O
character	B-Method
representations	E-Method
.	O

The	O
model	O
using	O
characters	O
alone	O
(	O
)	O
works	O
remarkably	O
well	O
,	O
it	O
improves	O
over	O
TnT	S-Method
on	O
9	O
languages	O
(	O
incl	O
.	O

Slavic	S-Material
and	O
Nordic	B-Material
languages	E-Material
)	O
.	O

The	O
combined	O
word	B-Method
+	I-Method
character	I-Method
representation	I-Method
model	E-Method
is	O
the	O
best	O
representation	O
,	O
outperforming	O
the	O
baseline	O
on	O
all	O
except	O
one	O
language	O
(	O
Indonesian	S-Material
)	O
,	O
providing	O
strong	O
results	O
already	O
without	O
pre	O
-	O
trained	O
embeddings	O
.	O

This	O
model	O
(	O
)	O
reaches	O
the	O
biggest	O
improvement	O
(	O
more	O
than	O
+	O
2	O
%	O
accuracy	S-Metric
)	O
on	O
Hebrew	S-Material
and	O
Slovene	S-Material
.	O

Initializing	O
the	O
word	B-Method
embeddings	E-Method
(	O
+	O
Polyglot	S-Method
)	O
with	O
off	O
-	O
the	O
-	O
shelf	O
language	B-Method
-	I-Method
specific	I-Method
embeddings	E-Method
further	O
improves	O
accuracy	S-Metric
.	O

The	O
only	O
system	O
we	O
are	O
aware	O
of	O
that	O
evaluates	O
on	O
UD	S-Material
is	O
gillick	O
:	O
ea:2016	O
(	O
last	O
column	O
)	O
.	O

However	O
,	O
note	O
that	O
these	O
results	O
are	O
not	O
strictly	O
comparable	O
as	O
they	O
use	O
the	O
earlier	O
UD	B-Material
v1.1	I-Material
version	E-Material
.	O

The	O
overall	O
best	O
system	O
is	O
the	O
multi	O
-	O
task	O
bi	B-Method
-	I-Method
LSTM	E-Method
freqbin	O
(	O
it	O
uses	O
and	O
Polyglot	O
initialization	O
for	O
)	O
.	O

While	O
on	O
macro	O
average	O
it	O
is	O
on	O
par	O
with	O
bi	B-Method
-	I-Method
LSTM	E-Method
,	O
it	O
obtains	O
the	O
best	O
results	O
on	O
12	O
/	O
22	O
languages	O
,	O
and	O
it	O
is	O
successful	O
in	O
predicting	B-Task
POS	E-Task
for	O
OOV	B-Task
tokens	E-Task
(	O
cf	O
.	O

Table	O
[	O
reference	O
]	O
OOV	O
Acc	O
columns	O
)	O
,	O
especially	O
for	O
languages	O
like	O
Arabic	S-Material
,	O
Farsi	S-Material
,	O
Hebrew	O
,	O
Finnish	S-Material
.	O

We	O
examined	O
simple	O
RNNs	S-Method
and	O
confirm	O
the	O
finding	O
of	O
ling	O
:	O
ea:2015	S-Method
that	O
they	O
performed	O
worse	O
than	O
their	O
LSTM	B-Method
counterparts	E-Method
.	O

Finally	O
,	O
the	O
bi	B-Method
-	I-Method
LSTM	E-Method
tagger	O
is	O
competitive	O
on	O
WSJ	S-Material
,	O
cf	O
.	O

Table	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Rare	O
words	O
In	O
order	O
to	O
evaluate	O
the	O
effect	O
of	O
modeling	O
sub	O
-	O
token	O
information	O
,	O
we	O
examine	O
accuracy	S-Metric
rates	O
at	O
different	O
frequency	B-Metric
rates	E-Metric
.	O

Figure	O
[	O
reference	O
]	O
shows	O
absolute	O
improvements	O
in	O
accuracy	S-Metric
of	O
bi	B-Method
-	I-Method
LSTM	E-Method
over	O
mean	B-Method
log	I-Method
frequency	E-Method
,	O
for	O
different	O
language	O
families	O
.	O

We	O
see	O
that	O
especially	O
for	O
Slavic	S-Material
and	O
non	B-Material
-	I-Material
Indoeuropean	I-Material
languages	E-Material
,	O
having	O
high	O
morphologic	B-Metric
complexity	E-Metric
,	O
most	O
of	O
the	O
improvement	O
is	O
obtained	O
in	O
the	O
Zipfian	O
tail	O
.	O

Rare	O
tokens	O
benefit	O
from	O
the	O
sub	B-Method
-	I-Method
token	I-Method
representations	E-Method
.	O

paragraph	O
:	O
Data	O
set	O
size	O
Prior	O
work	O
mostly	O
used	O
large	O
data	O
sets	O
when	O
applying	O
neural	B-Method
network	I-Method
based	I-Method
approaches	E-Method
.	O

We	O
evaluate	O
how	O
brittle	O
such	O
models	O
are	O
with	O
respect	O
to	O
their	O
more	O
traditional	O
counterparts	O
by	O
training	O
bi	B-Method
-	I-Method
LSTM	E-Method
(	O
without	O
Polyglot	B-Method
embeddings	E-Method
)	O
for	O
increasing	O
amounts	O
of	O
training	O
instances	O
(	O
number	O
of	O
sentences	O
)	O
.	O

The	O
learning	O
curves	O
in	O
Figure	O
[	O
reference	O
]	O
show	O
similar	O
trends	O
across	O
language	O
families	O
.	O

TnT	S-Method
is	O
better	O
with	O
little	O
data	O
,	O
bi	B-Method
-	I-Method
LSTM	E-Method
is	O
better	O
with	O
more	O
data	O
,	O
and	O
bi	B-Method
-	I-Method
LSTM	E-Method
always	O
wins	O
over	O
CRF	S-Method
.	O

The	O
bi	B-Method
-	I-Method
LSTM	E-Method
model	O
performs	O
already	O
surprisingly	O
well	O
after	O
only	O
500	O
training	O
sentences	O
.	O

For	O
non	B-Material
-	I-Material
Indoeuropean	I-Material
languages	E-Material
it	O
is	O
on	O
par	O
and	O
above	O
the	O
other	O
taggers	S-Method
with	O
even	O
less	O
data	O
(	O
100	O
sentences	O
)	O
.	O

This	O
shows	O
that	O
the	O
bi	O
-	O
LSTMs	S-Method
often	O
needs	O
more	O
data	O
than	O
the	O
generative	B-Method
markovian	I-Method
model	E-Method
,	O
but	O
this	O
is	O
definitely	O
less	O
than	O
what	O
we	O
expected	O
.	O

paragraph	O
:	O
Label	O
Noise	O
We	O
investigated	O
the	O
susceptibility	O
of	O
the	O
models	O
to	O
noise	O
,	O
by	O
artificially	O
corrupting	O
training	O
labels	O
.	O

Our	O
initial	O
results	O
show	O
that	O
at	O
low	B-Metric
noise	I-Metric
rates	E-Metric
,	O
bi	O
-	O
LSTMs	S-Method
and	O
TnT	S-Method
are	O
affected	O
similarly	O
,	O
their	O
accuracies	S-Metric
drop	O
to	O
a	O
similar	O
degree	O
.	O

Only	O
at	O
higher	O
noise	O
levels	O
(	O
more	O
than	O
30	O
%	O
corrupted	O
labels	O
)	O
,	O
bi	O
-	O
LSTMs	S-Method
are	O
less	O
robust	O
,	O
showing	O
higher	O
drops	O
in	O
accuracy	S-Metric
compared	O
to	O
TnT	S-Method
.	O

This	O
is	O
the	O
case	O
for	O
all	O
investigated	O
language	O
families	O
.	O

section	O
:	O
Related	O
Work	O
Character	B-Method
embeddings	E-Method
were	O
first	O
introduced	O
by	O
sutskever	O
:	O
ea:2011	O
for	O
language	B-Task
modeling	E-Task
.	O

Early	O
applications	O
include	O
text	B-Task
classification	E-Task
.	O

Recently	O
,	O
these	O
representations	O
were	O
successfully	O
applied	O
to	O
a	O
range	O
of	O
structured	B-Task
prediction	I-Task
tasks	E-Task
.	O

For	O
POS	B-Task
tagging	E-Task
,	O
santos	O
:	O
zadrozny:2014	O
were	O
the	O
first	O
to	O
propose	O
character	B-Method
-	I-Method
based	I-Method
models	E-Method
.	O

They	O
use	O
a	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
;	O
or	O
convnet	S-Method
)	O
and	O
evaluated	O
their	O
model	O
on	O
English	S-Material
(	O
PTB	S-Material
)	O
and	O
Portuguese	S-Material
,	O
showing	O
that	O
the	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
close	O
to	O
taggers	S-Method
using	O
carefully	O
designed	O
feature	O
templates	O
.	O

ling	O
:	O
ea:2015	O
extend	O
this	O
line	O
and	O
compare	O
a	O
novel	O
bi	B-Method
-	I-Method
LSTM	E-Method
model	O
,	O
learning	O
word	B-Method
representations	E-Method
through	O
character	B-Method
embeddings	E-Method
.	O

They	O
evaluate	O
their	O
model	O
on	O
a	O
language	B-Task
modeling	E-Task
and	O
POS	B-Task
tagging	I-Task
setup	E-Task
,	O
and	O
show	O
that	O
bi	O
-	O
LSTMs	S-Method
outperform	O
the	O
CNN	S-Method
approach	O
of	O
santos	O
:	O
zadrozny:2014	O
.	O

Similarly	O
,	O
labeau	O
:	O
ea:2015	O
evaluate	O
character	B-Task
embeddings	E-Task
for	O
German	S-Material
.	O

Bi	O
-	O
LSTMs	S-Method
for	O
POS	B-Task
tagging	E-Task
are	O
also	O
reported	O
in	O
wang	O
:	O
ea:2015:arxiv	O
,	O
however	O
,	O
they	O
only	O
explore	O
word	O
embeddings	O
,	O
orthographic	O
information	O
and	O
evaluate	O
on	O
WSJ	S-Material
only	O
.	O

A	O
related	O
study	O
is	O
cheng	O
:	O
fang	O
:	O
ostendorf:2015	O
who	O
propose	O
a	O
multi	B-Method
-	I-Method
task	I-Method
RNN	E-Method
for	O
named	B-Task
entity	I-Task
recognition	E-Task
by	O
jointly	O
predicting	O
the	O
next	O
token	O
and	O
current	O
token	O
’s	O
name	O
label	O
.	O

Our	O
model	O
is	O
simpler	O
,	O
it	O
uses	O
a	O
very	O
coarse	O
set	O
of	O
labels	O
rather	O
then	O
integrating	O
an	O
entire	O
language	B-Task
modeling	E-Task
task	O
which	O
is	O
computationally	O
more	O
expensive	O
.	O

An	O
interesting	O
recent	O
study	O
is	O
gillick	O
:	O
ea:2016	O
,	O
they	O
build	O
a	O
single	O
byte	B-Method
-	I-Method
to	I-Method
-	I-Method
span	I-Method
model	E-Method
for	O
multiple	O
languages	O
based	O
on	O
a	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
RNN	E-Method
achieving	O
impressive	O
results	O
.	O

We	O
would	O
like	O
to	O
extend	O
this	O
work	O
in	O
their	O
direction	O
.	O

section	O
:	O
Conclusions	O
We	O
evaluated	O
token	B-Method
and	I-Method
subtoken	I-Method
-	I-Method
level	I-Method
representations	E-Method
for	O
neural	B-Task
network	I-Task
-	I-Task
based	I-Task
part	I-Task
-	I-Task
of	I-Task
-	I-Task
speech	I-Task
tagging	E-Task
across	O
22	O
languages	O
and	O
proposed	O
a	O
novel	O
multi	B-Method
-	I-Method
task	I-Method
bi	I-Method
-	I-Method
LSTM	E-Method
with	O
auxiliary	B-Method
loss	E-Method
.	O

The	O
auxiliary	B-Method
loss	E-Method
is	O
effective	O
at	O
improving	O
the	O
accuracy	S-Metric
of	O
rare	O
words	O
.	O

Subtoken	B-Method
representations	E-Method
are	O
necessary	O
to	O
obtain	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
POS	B-Method
tagger	E-Method
,	O
and	O
character	B-Method
embeddings	E-Method
are	O
particularly	O
helpful	O
for	O
non	B-Material
-	I-Material
Indoeuropean	E-Material
and	O
Slavic	B-Material
languages	E-Material
.	O

Combining	O
them	O
with	O
word	B-Method
embeddings	E-Method
in	O
a	O
hierarchical	B-Method
network	E-Method
provides	O
the	O
best	O
representation	O
.	O

The	O
bi	B-Method
-	I-Method
LSTM	E-Method
tagger	O
is	O
as	O
effective	O
as	O
the	O
CRF	B-Method
and	I-Method
HMM	I-Method
taggers	E-Method
with	O
already	O
as	O
little	O
as	O
500	O
training	O
sentences	O
,	O
but	O
is	O
less	O
robust	O
to	O
label	O
noise	O
(	O
at	O
higher	O
noise	B-Metric
rates	E-Metric
)	O
.	O

section	O
:	O
Acknowledgments	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
feedback	O
.	O

AS	O
is	O
funded	O
by	O
the	O
ERC	O
Starting	O
Grant	O
LOWLANDS	O
No	O
.	O

313695	O
.	O

YG	O
is	O
supported	O
by	O
The	O
Israeli	O
Science	O
Foundation	O
(	O
grant	O
number	O
1555	O
/	O
15	O
)	O
and	O
a	O
Google	O
Research	O
Award	O
.	O

bibliography	O
:	O
References	O
Machine	B-Task
reading	I-Task
comprehension	E-Task
(	O
MRC	S-Task
)	O
on	O
real	O
web	O
data	O
usually	O
requires	O
the	O
machine	O
to	O
answer	B-Task
a	I-Task
question	E-Task
by	O
analyzing	O
multiple	O
passages	O
retrieved	O
by	O
search	B-Method
engine	E-Method
.	O

Compared	O
with	O
MRC	S-Task
on	O
a	O
single	O
passage	O
,	O
multi	B-Task
-	I-Task
passage	I-Task
MRC	E-Task
is	O
more	O
challenging	O
,	O
since	O
we	O
are	O
likely	O
to	O
get	O
multiple	O
confusing	O
answer	O
candidates	O
from	O
different	O
passages	O
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
neural	I-Method
model	E-Method
that	O
enables	O
those	O
answer	O
candidates	O
from	O
different	O
passages	O
to	O
verify	O
each	O
other	O
based	O
on	O
their	O
content	S-Method
representations	O
.	O

Specifically	O
,	O
we	O
jointly	O
train	O
three	O
modules	O
that	O
can	O
predict	O
the	O
final	O
answer	O
based	O
on	O
three	O
factors	O
:	O
the	O
answer	O
boundary	O
,	O
the	O
answer	O
content	S-Method
and	O
the	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	E-Task
.	O

The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
outperforms	O
the	O
baseline	O
by	O
a	O
large	O
margin	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
English	B-Material
MS	I-Material
-	I-Material
MARCO	I-Material
dataset	E-Material
and	O
the	O
Chinese	O
DuReader	O
dataset	O
,	O
both	O
of	O
which	O
are	O
designed	O
for	O
MRC	S-Task
in	O
real	B-Task
-	I-Task
world	I-Task
settings	E-Task
.	O

Multi	O
-	O
PassageMachineReadingComprehension	O
withCross	O
-	O
PassageAnswerVerification	O
[	O
1*	O
]	O
YizhongWang	O
*	O
ThisworkwasdonewhilethefirstauthorwasdoinginternshipatBaiduInc.	O
[	O
2	O
]	O
KaiLiu	O
[	O
2	O
]	O
JingLiu	O
[	O
2	O
]	O
WeiHe	O
[	O
2	O
]	O
YajuanLyu	O
[	O
2	O
]	O
HuaWu	O
[	O
1	O
]	O
SujianLi	O
[	O
2	O
]	O
HaifengWang	O
[	O
1	O
]	O
KeyLaboratoryofComputationalLinguistics	O
,	O
PekingUniversity	O
,	O
MOE	O
,	O
China	O
[	O
2	O
]	O
BaiduInc.	O
,	O
Beijing	O
,	O
China	O
[	O
]	O
{	O
yizhong	O
,	O
lisujian	O
}	O
@pku.edu.cn	O
,	O
{	O
liukai20	O
,	O
liujing46	O
,	O
[	O
]	O
hewei06	O
,	O
lvyajuan	O
,	O
wu_hua	O
,	O
wanghaifeng	O
}	O
@baidu.com	O
section	O
:	O
Introduction	O
Machine	B-Task
reading	I-Task
comprehension	E-Task
(	O
MRC	S-Task
)	O
,	O
empowering	O
computers	O
with	O
the	O
ability	O
to	O
acquire	O
knowledge	O
and	O
answer	O
questions	O
from	O
textual	O
data	O
,	O
is	O
believed	O
to	O
be	O
a	O
crucial	O
step	O
in	O
building	O
a	O
general	O
intelligent	B-Method
agent	E-Method
.	O

Recent	O
years	O
have	O
seen	O
rapid	O
growth	O
in	O
the	O
MRC	B-Task
community	E-Task
.	O

With	O
the	O
release	O
of	O
various	O
datasets	O
,	O
the	O
MRC	B-Task
task	E-Task
has	O
evolved	O
from	O
the	O
early	O
cloze	O
-	O
style	O
test	O
to	O
answer	B-Task
extraction	E-Task
from	O
a	O
single	O
passage	O
and	O
to	O
the	O
latest	O
more	O
complex	O
question	B-Task
answering	E-Task
on	O
web	O
data	O
.	O

Great	O
efforts	O
have	O
also	O
been	O
made	O
to	O
develop	O
models	O
for	O
these	O
MRC	B-Task
tasks	E-Task
,	O
especially	O
for	O
the	O
answer	B-Task
extraction	E-Task
on	O
single	O
passage	O
.	O

A	O
significant	O
milestone	O
is	O
that	O
several	O
MRC	B-Method
models	E-Method
have	O
exceeded	O
the	O
performance	O
of	O
human	O
annotators	O
on	O
the	O
SQuAD	O
dataset	O
.	O

However	O
,	O
this	O
success	O
on	O
single	O
Wikipedia	O
passage	O
is	O
still	O
not	O
adequate	O
,	O
considering	O
the	O
ultimate	O
goal	O
of	O
reading	O
the	O
whole	O
web	O
.	O

Therefore	O
,	O
several	O
latest	O
datasets	O
attempt	O
to	O
design	O
the	O
MRC	B-Task
tasks	E-Task
in	O
more	O
realistic	O
settings	O
by	O
involving	O
search	B-Method
engines	E-Method
.	O

For	O
each	O
question	O
,	O
they	O
use	O
the	O
search	B-Method
engine	E-Method
to	O
retrieve	O
multiple	O
passages	O
and	O
the	O
MRC	B-Method
models	E-Method
are	O
required	O
to	O
read	O
these	O
passages	O
in	O
order	O
to	O
give	O
the	O
final	O
answer	O
.	O

One	O
of	O
the	O
intrinsic	O
challenges	O
for	O
such	O
multi	B-Task
-	I-Task
passage	I-Task
MRC	E-Task
is	O
that	O
since	O
all	O
the	O
passages	O
are	O
question	O
-	O
related	O
but	O
usually	O
independently	O
written	O
,	O
it	O
’s	O
probable	O
that	O
multiple	O
confusing	O
answer	O
candidates	O
(	O
correct	O
or	O
incorrect	O
)	O
exist	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
an	O
example	O
from	O
MS	B-Material
-	I-Material
MARCO	E-Material
.	O

We	O
can	O
see	O
that	O
all	O
the	O
answer	O
candidates	O
have	O
semantic	O
matching	O
with	O
the	O
question	O
while	O
they	O
are	O
literally	O
different	O
and	O
some	O
of	O
them	O
are	O
even	O
incorrect	O
.	O

As	O
is	O
shown	O
by	O
adversarial	O
-	O
examples	O
,	O
these	O
confusing	O
answer	O
candidates	O
could	O
be	O
quite	O
difficult	O
for	O
MRC	B-Method
models	E-Method
to	O
distinguish	O
.	O

Therefore	O
,	O
special	O
consideration	O
is	O
required	O
for	O
such	O
multi	B-Task
-	I-Task
passage	I-Task
MRC	I-Task
problem	E-Task
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
leverage	O
the	O
answer	O
candidates	O
from	O
different	O
passages	O
to	O
verify	O
the	O
final	O
correct	O
answer	O
and	O
rule	O
out	O
the	O
noisy	O
incorrect	O
answers	O
.	O

Our	O
hypothesis	O
is	O
that	O
the	O
correct	O
answers	O
could	O
occur	O
more	O
frequently	O
in	O
those	O
passages	O
and	O
usually	O
share	O
some	O
commonalities	O
,	O
while	O
incorrect	O
answers	O
are	O
usually	O
different	O
from	O
one	O
another	O
.	O

The	O
example	O
in	O
Table	O
[	O
reference	O
]	O
demonstrates	O
this	O
phenomenon	O
.	O

We	O
can	O
see	O
that	O
the	O
answer	O
candidates	O
extracted	O
from	O
the	O
last	O
four	O
passages	O
are	O
all	O
valid	O
answers	O
to	O
the	O
question	O
and	O
they	O
are	O
semantically	O
similar	O
to	O
each	O
other	O
,	O
while	O
the	O
answer	O
candidates	O
from	O
the	O
other	O
two	O
passages	O
are	O
incorrect	O
and	O
there	O
is	O
no	O
supportive	O
information	O
from	O
other	O
passages	O
.	O

As	O
human	O
beings	O
usually	O
compare	O
the	O
answer	O
candidates	O
from	O
different	O
sources	O
to	O
deduce	O
the	O
final	O
answer	O
,	O
we	O
hope	O
that	O
MRC	B-Method
model	E-Method
can	O
also	O
benefit	O
from	O
the	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	I-Task
process	E-Task
.	O

The	O
overall	O
framework	O
of	O
our	O
model	O
is	O
demonstrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
which	O
consists	O
of	O
three	O
modules	O
.	O

First	O
,	O
we	O
follow	O
the	O
boundary	B-Method
-	I-Method
based	I-Method
MRC	I-Method
models	E-Method
to	O
find	O
an	O
answer	O
candidate	O
for	O
each	O
passage	O
by	O
identifying	O
the	O
start	O
and	O
end	O
position	O
of	O
the	O
answer	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
we	O
model	O
the	O
meanings	O
of	O
the	O
answer	O
candidates	O
extracted	O
from	O
those	O
passages	O
and	O
use	O
the	O
content	S-Method
scores	O
to	O
measure	O
the	O
quality	O
of	O
the	O
candidates	O
from	O
a	O
second	O
perspective	O
.	O

Third	O
,	O
we	O
conduct	O
the	O
answer	B-Task
verification	E-Task
by	O
enabling	O
each	O
answer	O
candidate	O
to	O
attend	O
to	O
the	O
other	O
candidates	O
based	O
on	O
their	O
representations	O
.	O

We	O
hope	O
that	O
the	O
answer	O
candidates	O
can	O
collect	O
supportive	O
information	O
from	O
each	O
other	O
according	O
to	O
their	O
semantic	O
similarities	O
and	O
further	O
decide	O
whether	O
each	O
candidate	O
is	O
correct	O
or	O
not	O
.	O

Therefore	O
,	O
the	O
final	O
answer	O
is	O
determined	O
by	O
three	O
factors	O
:	O
the	O
boundary	O
,	O
the	O
content	S-Method
and	O
the	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	E-Task
.	O

The	O
three	O
steps	O
are	O
modeled	O
using	O
different	O
modules	O
,	O
which	O
can	O
be	O
jointly	O
trained	O
in	O
our	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
framework	E-Method
.	O

We	O
conduct	O
extensive	O
experiments	O
on	O
the	O
MS	B-Material
-	I-Material
MARCO	E-Material
and	O
DuReader	O
datasets	O
.	O

The	O
results	O
show	O
that	O
our	O
answer	B-Method
verification	I-Method
MRC	I-Method
model	E-Method
outperforms	O
the	O
baseline	O
models	O
by	O
a	O
large	O
margin	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
both	O
datasets	O
.	O

section	O
:	O
Our	O
Approach	O
Figure	O
[	O
reference	O
]	O
gives	O
an	O
overview	O
of	O
our	O
multi	B-Method
-	I-Method
passage	I-Method
MRC	I-Method
model	E-Method
which	O
is	O
mainly	O
composed	O
of	O
three	O
modules	O
including	O
answer	B-Task
boundary	I-Task
prediction	E-Task
,	O
answer	O
content	S-Method
modeling	O
and	O
answer	B-Task
verification	E-Task
.	O

First	O
of	O
all	O
,	O
we	O
need	O
to	O
model	O
the	O
question	O
and	O
passages	O
.	O

Following	O
bidaf	S-Method
,	O
we	O
compute	O
the	O
question	B-Method
-	I-Method
aware	I-Method
representation	E-Method
for	O
each	O
passage	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Based	O
on	O
this	O
representation	O
,	O
we	O
employ	O
a	O
Pointer	B-Method
Network	E-Method
to	O
predict	O
the	O
start	O
and	O
end	O
position	O
of	O
the	O
answer	O
in	O
the	O
module	O
of	O
answer	B-Task
boundary	I-Task
prediction	E-Task
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

At	O
the	O
same	O
time	O
,	O
with	O
the	O
answer	O
content	S-Method
model	O
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
we	O
estimate	O
whether	O
each	O
word	O
should	O
be	O
included	O
in	O
the	O
answer	O
and	O
thus	O
obtain	O
the	O
answer	B-Method
representations	E-Method
.	O

Next	O
,	O
in	O
the	O
answer	B-Method
verification	I-Method
module	E-Method
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
each	O
answer	O
candidate	O
can	O
attend	O
to	O
the	O
other	O
answer	O
candidates	O
to	O
collect	O
supportive	O
information	O
and	O
we	O
compute	O
one	O
score	O
for	O
each	O
candidate	O
to	O
indicate	O
whether	O
it	O
is	O
correct	O
or	O
not	O
according	O
to	O
the	O
verification	S-Task
.	O

The	O
final	O
answer	O
is	O
determined	O
by	O
not	O
only	O
the	O
boundary	O
but	O
also	O
the	O
answer	O
content	S-Method
and	O
its	O
verification	B-Metric
score	E-Metric
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Question	S-Method
and	O
Passage	B-Method
Modeling	E-Method
Given	O
a	O
question	O
and	O
a	O
set	O
of	O
passages	O
retrieved	O
by	O
search	B-Method
engines	E-Method
,	O
our	O
task	O
is	O
to	O
find	O
the	O
best	O
concise	O
answer	O
to	O
the	O
question	O
.	O

First	O
,	O
we	O
formally	O
present	O
the	O
details	O
of	O
modeling	O
the	O
question	S-Method
and	O
passages	S-Method
.	O

paragraph	O
:	O
Encoding	S-Task
We	O
first	O
map	O
each	O
word	O
into	O
the	O
vector	O
space	O
by	O
concatenating	O
its	O
word	B-Method
embedding	E-Method
and	O
sum	O
of	O
its	O
character	B-Method
embeddings	E-Method
.	O

Then	O
we	O
employ	O
bi	B-Method
-	I-Method
directional	I-Method
LSTMs	E-Method
(	O
BiLSTM	S-Method
)	O
to	O
encode	O
the	O
question	O
and	O
passages	O
as	O
follows	O
:	O
where	O
,	O
,	O
,	O
are	O
the	O
word	O
-	O
level	O
and	O
character	O
-	O
level	O
embeddings	O
of	O
the	O
word	O
.	O

and	O
are	O
the	O
encoding	O
vectors	O
of	O
the	O
words	O
in	O
and	O
respectively	O
.	O

Unlike	O
previous	O
work	O
that	O
simply	O
concatenates	O
all	O
the	O
passages	O
,	O
we	O
process	O
the	O
passages	O
independently	O
at	O
the	O
encoding	S-Method
and	O
matching	B-Method
steps	E-Method
.	O

paragraph	O
:	O
Q	B-Method
-	I-Method
P	I-Method
Matching	E-Method
One	O
essential	O
step	O
in	O
MRC	S-Task
is	O
to	O
match	O
the	O
question	O
with	O
passages	O
so	O
that	O
important	O
information	O
can	O
be	O
highlighted	O
.	O

We	O
use	O
the	O
Attention	B-Method
Flow	I-Method
Layer	E-Method
to	O
conduct	O
the	O
Q	B-Task
-	I-Task
P	I-Task
matching	E-Task
in	O
two	O
directions	O
.	O

The	O
similarity	O
matrix	O
between	O
the	O
question	O
and	O
passage	O
is	O
changed	O
to	O
a	O
simpler	O
version	O
,	O
where	O
the	O
similarity	O
between	O
the	O
word	O
in	O
the	O
question	O
and	O
the	O
word	O
in	O
passage	O
is	O
computed	O
as	O
:	O
Then	O
the	O
context	O
-	O
to	O
-	O
question	O
attention	O
and	O
question	O
-	O
to	O
-	O
context	O
attention	O
is	O
applied	O
strictly	O
following	O
bidaf	O
to	O
obtain	O
the	O
question	B-Method
-	I-Method
aware	I-Method
passage	I-Method
representation	E-Method
.	O

We	O
do	O
not	O
give	O
the	O
details	O
here	O
due	O
to	O
space	O
limitation	O
.	O

Next	O
,	O
another	O
BiLSTM	S-Method
is	O
applied	O
in	O
order	O
to	O
fuse	O
the	O
contextual	O
information	O
and	O
get	O
the	O
new	O
representation	O
for	O
each	O
word	O
in	O
the	O
passage	O
,	O
which	O
is	O
regarded	O
as	O
the	O
match	O
output	O
:	O
Based	O
on	O
the	O
passage	B-Method
representations	E-Method
,	O
we	O
introduce	O
the	O
three	O
main	O
modules	O
of	O
our	O
model	O
.	O

subsection	O
:	O
Answer	B-Task
Boundary	I-Task
Prediction	E-Task
To	O
extract	O
the	O
answer	O
span	O
from	O
passages	O
,	O
mainstream	O
studies	O
try	O
to	O
locate	O
the	O
boundary	O
of	O
the	O
answer	O
,	O
which	O
is	O
called	O
boundary	B-Method
model	E-Method
.	O

Following	O
,	O
we	O
employ	O
Pointer	B-Method
Network	E-Method
to	O
compute	O
the	O
probability	O
of	O
each	O
word	O
to	O
be	O
the	O
start	O
or	O
end	O
position	O
of	O
the	O
span	O
:	O
By	O
utilizing	O
the	O
attention	O
weights	O
,	O
the	O
probability	O
of	O
the	O
word	O
in	O
the	O
passage	O
to	O
be	O
the	O
start	O
and	O
end	O
position	O
of	O
the	O
answer	O
is	O
obtained	O
as	O
and	O
.	O

It	O
should	O
be	O
noted	O
that	O
the	O
pointer	B-Method
network	E-Method
is	O
applied	O
to	O
the	O
concatenation	O
of	O
all	O
passages	O
,	O
which	O
is	O
denoted	O
as	O
P	O
so	O
that	O
the	O
probabilities	O
are	O
comparable	O
across	O
passages	O
.	O

This	O
boundary	B-Method
model	E-Method
can	O
be	O
trained	O
by	O
minimizing	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
start	O
and	O
end	O
indices	O
:	O
where	O
is	O
the	O
number	O
of	O
samples	O
in	O
the	O
dataset	O
and	O
,	O
are	O
the	O
gold	O
start	O
and	O
end	O
positions	O
.	O

subsection	O
:	O
Answer	B-Method
Content	I-Method
Modeling	E-Method
Previous	O
work	O
employs	O
the	O
boundary	B-Method
model	E-Method
to	O
find	O
the	O
text	O
span	O
with	O
the	O
maximum	O
boundary	O
score	O
as	O
the	O
final	O
answer	O
.	O

However	O
,	O
in	O
our	O
context	O
,	O
besides	O
locating	O
the	O
answer	O
candidates	O
,	O
we	O
also	O
need	O
to	O
model	O
their	O
meanings	O
in	O
order	O
to	O
conduct	O
the	O
verification	S-Task
.	O

An	O
intuitive	O
method	O
is	O
to	O
compute	O
the	O
representation	O
of	O
the	O
answer	O
candidates	O
separately	O
after	O
extracting	O
them	O
,	O
but	O
it	O
could	O
be	O
hard	O
to	O
train	O
such	O
model	O
end	O
-	O
to	O
-	O
end	O
.	O

Here	O
,	O
we	O
propose	O
a	O
novel	O
method	O
that	O
can	O
obtain	O
the	O
representation	O
of	O
the	O
answer	O
candidates	O
based	O
on	O
probabilities	O
.	O

Specifically	O
,	O
we	O
change	O
the	O
output	O
layer	O
of	O
the	O
classic	O
MRC	B-Method
model	E-Method
.	O

Besides	O
predicting	O
the	O
boundary	O
probabilities	O
for	O
the	O
words	O
in	O
the	O
passages	O
,	O
we	O
also	O
predict	O
whether	O
each	O
word	O
should	O
be	O
included	O
in	O
the	O
content	S-Method
of	O
the	O
answer	O
.	O

The	O
content	S-Method
probability	O
of	O
the	O
word	O
is	O
computed	O
as	O
:	O
Training	O
this	O
content	S-Method
model	O
is	O
also	O
quite	O
intuitive	O
.	O

We	O
transform	O
the	O
boundary	O
labels	O
into	O
a	O
continuous	O
segment	O
,	O
which	O
means	O
the	O
words	O
within	O
the	O
answer	O
span	O
will	O
be	O
labeled	O
as	O
1	O
and	O
other	O
words	O
will	O
be	O
labeled	O
as	O
0	O
.	O

In	O
this	O
way	O
,	O
we	O
define	O
the	O
loss	O
function	O
as	O
the	O
averaged	O
cross	O
entropy	O
:	O
The	O
content	S-Method
probabilities	O
provide	O
another	O
view	O
to	O
measure	O
the	O
quality	O
of	O
the	O
answer	O
in	O
addition	O
to	O
the	O
boundary	O
.	O

Moreover	O
,	O
with	O
these	O
probabilities	O
,	O
we	O
can	O
represent	O
the	O
answer	O
from	O
passage	O
as	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
word	O
embeddings	O
in	O
this	O
passage	O
:	O
subsection	O
:	O
Cross	B-Task
-	I-Task
Passage	I-Task
Answer	I-Task
Verification	E-Task
The	O
boundary	B-Method
model	E-Method
and	O
the	O
content	S-Method
model	O
focus	O
on	O
extracting	O
and	O
modeling	O
the	O
answer	O
within	O
a	O
single	O
passage	O
respectively	O
,	O
with	O
little	O
consideration	O
of	O
the	O
cross	O
-	O
passage	O
information	O
.	O

However	O
,	O
as	O
is	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
there	O
could	O
be	O
multiple	O
answer	O
candidates	O
from	O
different	O
passages	O
and	O
some	O
of	O
them	O
may	O
mislead	O
the	O
MRC	B-Method
model	E-Method
to	O
make	O
an	O
incorrect	O
prediction	O
.	O

It	O
’s	O
necessary	O
to	O
aggregate	O
the	O
information	O
from	O
different	O
passages	O
and	O
choose	O
the	O
best	O
one	O
from	O
those	O
candidates	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
method	O
to	O
enable	O
the	O
answer	O
candidates	O
to	O
exchange	O
information	O
and	O
verify	O
each	O
other	O
through	O
the	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	I-Task
process	E-Task
.	O

Given	O
the	O
representation	O
of	O
the	O
answer	O
candidates	O
from	O
all	O
passages	O
,	O
each	O
answer	O
candidate	O
then	O
attends	O
to	O
other	O
candidates	O
to	O
collect	O
supportive	O
information	O
via	O
attention	B-Method
mechanism	E-Method
:	O
Here	O
is	O
the	O
collected	O
verification	O
information	O
from	O
other	O
passages	O
based	O
on	O
the	O
attention	O
weights	O
.	O

Then	O
we	O
pass	O
it	O
together	O
with	O
the	O
original	O
representation	O
to	O
a	O
fully	B-Method
connected	I-Method
layer	E-Method
:	O
We	O
further	O
normalize	O
these	O
scores	O
over	O
all	O
passages	O
to	O
get	O
the	O
verification	B-Metric
score	E-Metric
for	O
answer	O
candidate	O
:	O
In	O
order	O
to	O
train	O
this	O
verification	B-Method
model	E-Method
,	O
we	O
take	O
the	O
answer	O
from	O
the	O
gold	O
passage	O
as	O
the	O
gold	O
answer	O
.	O

And	O
the	O
loss	O
function	O
can	O
be	O
formulated	O
as	O
the	O
negative	O
log	O
probability	O
of	O
the	O
correct	O
answer	O
:	O
where	O
is	O
the	O
index	O
of	O
the	O
correct	O
answer	O
in	O
all	O
the	O
answer	O
candidates	O
of	O
the	O
instance	O
.	O

subsection	O
:	O
Joint	B-Task
Training	E-Task
and	O
Prediction	S-Task
As	O
is	O
described	O
above	O
,	O
we	O
define	O
three	O
objectives	O
for	O
the	O
reading	B-Method
comprehension	I-Method
model	E-Method
over	O
multiple	O
passages	O
:	O
1	O
.	O

finding	O
the	O
boundary	O
of	O
the	O
answer	O
;	O
2	O
.	O

predicting	O
whether	O
each	O
word	O
should	O
be	O
included	O
in	O
the	O
content	S-Method
;	O
3	O
.	O

selecting	O
the	O
best	O
answer	O
via	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	E-Task
.	O

According	O
to	O
our	O
design	O
,	O
these	O
three	O
tasks	O
can	O
share	O
the	O
same	O
embedding	O
,	O
encoding	O
and	O
matching	O
layers	O
.	O

Therefore	O
,	O
we	O
propose	O
to	O
train	O
them	O
together	O
as	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

The	O
joint	B-Metric
objective	I-Metric
function	E-Metric
is	O
formulated	O
as	O
follows	O
:	O
where	O
and	O
are	O
two	O
hyper	O
-	O
parameters	O
that	O
control	O
the	O
weights	O
of	O
those	O
tasks	O
.	O

When	O
predicting	O
the	O
final	O
answer	O
,	O
we	O
take	O
the	O
boundary	B-Metric
score	E-Metric
,	O
content	S-Method
score	O
and	O
verification	B-Metric
score	E-Metric
into	O
consideration	O
.	O

We	O
first	O
extract	O
the	O
answer	O
candidate	O
that	O
has	O
the	O
maximum	O
boundary	O
score	O
from	O
each	O
passage	O
.	O

This	O
boundary	B-Metric
score	E-Metric
is	O
computed	O
as	O
the	O
product	O
of	O
the	O
start	O
and	O
end	O
probability	O
of	O
the	O
answer	O
span	O
.	O

Then	O
for	O
each	O
answer	O
candidate	O
,	O
we	O
average	O
the	O
content	S-Method
probabilities	O
of	O
all	O
its	O
words	O
as	O
the	O
content	S-Method
score	O
of	O
.	O

And	O
we	O
can	O
also	O
predict	O
the	O
verification	B-Metric
score	E-Metric
for	O
using	O
the	O
verification	B-Method
model	E-Method
.	O

Therefore	O
,	O
the	O
final	O
answer	O
can	O
be	O
selected	O
from	O
all	O
the	O
answer	O
candidates	O
according	O
to	O
the	O
product	O
of	O
these	O
three	O
scores	O
.	O

section	O
:	O
Experiments	O
To	O
verify	O
the	O
effectiveness	O
of	O
our	O
model	O
on	O
multi	B-Task
-	I-Task
passage	I-Task
machine	I-Task
reading	I-Task
comprehension	E-Task
,	O
we	O
conduct	O
experiments	O
on	O
the	O
MS	B-Material
-	I-Material
MARCO	E-Material
and	O
DuReader	O
datasets	O
.	O

Our	O
method	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
both	O
datasets	O
.	O

subsection	O
:	O
Datasets	O
We	O
choose	O
the	O
MS	B-Material
-	I-Material
MARCO	E-Material
and	O
DuReader	O
datasets	O
to	O
test	O
our	O
method	O
,	O
since	O
both	O
of	O
them	O
are	O
designed	O
from	O
real	B-Method
-	I-Method
world	I-Method
search	I-Method
engines	E-Method
and	O
involve	O
a	O
large	O
number	O
of	O
passages	O
retrieved	O
from	O
the	O
web	O
.	O

One	O
difference	O
of	O
these	O
two	O
datasets	O
is	O
that	O
MS	B-Material
-	I-Material
MARCO	E-Material
mainly	O
focuses	O
on	O
the	O
English	O
web	O
data	O
,	O
while	O
DuReader	O
is	O
designed	O
for	O
Chinese	O
MRC	O
.	O

This	O
diversity	O
is	O
expected	O
to	O
reflect	O
the	O
generality	O
of	O
our	O
method	O
.	O

In	O
terms	O
of	O
the	O
data	O
size	O
,	O
MS	B-Material
-	I-Material
MARCO	E-Material
contains	O
102023	O
questions	O
,	O
each	O
of	O
which	O
is	O
paired	O
up	O
with	O
approximately	O
10	O
passages	O
for	O
reading	B-Task
comprehension	E-Task
.	O

As	O
for	O
DuReader	O
,	O
it	O
keeps	O
the	O
top	O
-	O
5	O
search	O
results	O
for	O
each	O
question	O
and	O
there	O
are	O
totally	O
201574	O
questions	O
.	O

One	O
prerequisite	O
for	O
answer	B-Task
verification	E-Task
is	O
that	O
there	O
should	O
be	O
multiple	O
correct	O
answers	O
so	O
that	O
they	O
can	O
verify	O
each	O
other	O
.	O

Both	O
the	O
MS	B-Material
-	I-Material
MARCO	E-Material
and	O
DuReader	O
datasets	O
require	O
the	O
human	O
annotators	O
to	O
generate	O
multiple	O
answers	O
if	O
possible	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
proportion	O
of	O
questions	O
that	O
have	O
multiple	O
answers	O
.	O

However	O
,	O
the	O
same	O
answer	O
that	O
occurs	O
many	O
times	O
is	O
treated	O
as	O
one	O
single	O
answer	O
here	O
.	O

Therefore	O
,	O
we	O
also	O
report	O
the	O
proportion	O
of	O
questions	O
that	O
have	O
multiple	O
answer	O
spans	O
to	O
match	O
with	O
the	O
human	O
-	O
generated	O
answers	O
.	O

A	O
span	O
is	O
taken	O
as	O
valid	O
if	O
it	O
can	O
achieve	O
F1	B-Metric
score	E-Metric
larger	O
than	O
0.7	O
compared	O
with	O
any	O
reference	O
answer	O
.	O

From	O
these	O
statistics	O
,	O
we	O
can	O
see	O
that	O
the	O
phenomenon	O
of	O
multiple	O
answers	O
is	O
quite	O
common	O
for	O
both	O
MS	B-Material
-	I-Material
MARCO	E-Material
and	O
DuReader	O
.	O

These	O
answers	O
will	O
provide	O
strong	O
signals	O
for	O
answer	B-Task
verification	E-Task
if	O
we	O
can	O
leverage	O
them	O
properly	O
.	O

subsection	O
:	O
Implementation	O
Details	O
For	O
MS	B-Material
-	I-Material
MARCO	E-Material
,	O
we	O
preprocess	O
the	O
corpus	O
with	O
the	O
reversible	B-Method
tokenizer	E-Method
from	O
Stanford	O
CoreNLP	O
and	O
we	O
choose	O
the	O
span	O
that	O
achieves	O
the	O
highest	O
ROUGE	B-Metric
-	I-Metric
L	I-Metric
score	E-Metric
with	O
the	O
reference	O
answers	O
as	O
the	O
gold	O
span	O
for	O
training	O
.	O

We	O
employ	O
the	O
300	B-Method
-	I-Method
D	I-Method
pre	I-Method
-	I-Method
trained	I-Method
Glove	I-Method
embeddings	E-Method
and	O
keep	O
it	O
fixed	O
during	O
training	O
.	O

The	O
character	O
embeddings	O
are	O
randomly	O
initialized	O
with	O
its	O
dimension	O
as	O
30	O
.	O

For	O
DuReader	O
,	O
we	O
follow	O
the	O
preprocessing	O
described	O
in	O
dureader	S-Method
.	O

We	O
tune	O
the	O
hyper	O
-	O
parameters	O
according	O
to	O
the	O
validation	O
performance	O
on	O
the	O
MS	B-Material
-	I-Material
MARCO	I-Material
development	I-Material
set	E-Material
.	O

The	O
hidden	O
size	O
is	O
set	O
to	O
be	O
150	O
and	O
we	O
apply	O
regularization	S-Method
with	O
its	O
weight	O
as	O
0.0003	O
.	O

The	O
task	O
weights	O
,	O
are	O
both	O
set	O
to	O
be	O
0.5	O
.	O

To	O
train	O
our	O
model	O
,	O
we	O
employ	O
the	O
Adam	B-Method
algorithm	E-Method
with	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
as	O
0.0004	O
and	O
the	O
mini	O
-	O
batch	O
size	O
as	O
32	O
.	O

Exponential	B-Method
moving	I-Method
average	E-Method
is	O
applied	O
on	O
all	O
trainable	O
variables	O
with	O
a	O
decay	B-Metric
rate	E-Metric
0.9999	O
.	O

Two	O
simple	O
yet	O
effective	O
technologies	O
are	O
employed	O
to	O
improve	O
the	O
final	O
performance	O
on	O
these	O
two	O
datasets	O
respectively	O
.	O

For	O
MS	B-Material
-	I-Material
MARCO	E-Material
,	O
approximately	O
8	O
%	O
questions	O
have	O
the	O
answers	O
as	O
Yes	O
or	O
No	O
,	O
which	O
usually	O
can	O
not	O
be	O
solved	O
by	O
extractive	B-Method
approach	E-Method
.	O

We	O
address	O
this	O
problem	O
by	O
training	O
a	O
simple	O
Yes	B-Method
/	I-Method
No	I-Method
classifier	E-Method
for	O
those	O
questions	O
with	O
certain	O
patterns	O
(	O
e.g.	O
,	O
starting	O
with	O
“	O
is	O
”	O
)	O
.	O

Concretely	O
,	O
we	O
simply	O
change	O
the	O
output	O
layer	O
of	O
the	O
basic	B-Method
boundary	I-Method
model	E-Method
so	O
that	O
it	O
can	O
predict	O
whether	O
the	O
answer	O
is	O
“	O
Yes	O
”	O
or	O
“	O
No	O
”	O
.	O

For	O
DuReader	O
,	O
the	O
retrieved	O
document	O
usually	O
contains	O
a	O
large	O
number	O
of	O
paragraphs	O
that	O
can	O
not	O
be	O
fed	O
into	O
MRC	B-Method
models	E-Method
directly	O
.	O

The	O
original	O
paper	O
employs	O
a	O
simple	O
a	O
simple	O
heuristic	B-Method
strategy	E-Method
to	O
select	O
a	O
representative	O
paragraph	O
for	O
each	O
document	O
,	O
while	O
we	O
train	O
a	O
paragraph	B-Method
ranking	I-Method
model	E-Method
for	O
this	O
.	O

We	O
will	O
demonstrate	O
the	O
effects	O
of	O
these	O
two	O
technologies	O
later	O
.	O

subsection	O
:	O
Results	O
on	O
MS	B-Material
-	I-Material
MARCO	E-Material
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
our	O
system	O
and	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
MS	B-Material
-	I-Material
MARCO	I-Material
test	I-Material
set	E-Material
.	O

We	O
adopt	O
the	O
official	B-Metric
evaluation	I-Metric
metrics	E-Metric
,	O
including	O
ROUGE	B-Metric
-	I-Metric
L	E-Metric
and	O
BLEU	B-Metric
-	I-Metric
1	E-Metric
.	O

As	O
we	O
can	O
see	O
,	O
for	O
both	O
metrics	O
,	O
our	O
single	O
model	O
outperforms	O
all	O
the	O
other	O
competing	O
models	O
with	O
an	O
evident	O
margin	O
,	O
which	O
is	O
extremely	O
hard	O
considering	O
the	O
near	O
-	O
human	O
performance	O
.	O

If	O
we	O
ensemble	O
the	O
models	O
trained	O
with	O
different	O
random	O
seeds	O
and	O
hyper	O
-	O
parameters	O
,	O
the	O
results	O
can	O
be	O
further	O
improved	O
and	O
outperform	O
the	O
ensemble	B-Method
model	E-Method
in	O
snet	S-Method
,	O
especially	O
in	O
terms	O
of	O
the	O
BLEU	B-Metric
-	I-Metric
1	E-Metric
.	O

subsection	O
:	O
Results	O
on	O
DuReader	O
The	O
results	O
of	O
our	O
model	O
and	O
several	O
baseline	O
systems	O
on	O
the	O
test	O
set	O
of	O
DuReader	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
BiDAF	S-Method
and	O
Match	B-Method
-	I-Method
LSTM	I-Method
models	E-Method
are	O
provided	O
as	O
two	O
baseline	O
systems	O
.	O

Based	O
on	O
BiDAF	S-Method
,	O
as	O
is	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
tried	O
a	O
new	O
paragraph	B-Method
selection	I-Method
strategy	E-Method
by	O
employing	O
a	O
paragraph	B-Method
ranking	E-Method
(	O
PR	S-Method
)	O
model	O
.	O

We	O
can	O
see	O
that	O
this	O
paragraph	B-Task
ranking	E-Task
can	O
boost	O
the	O
BiDAF	B-Metric
baseline	E-Metric
significantly	O
.	O

Finally	O
,	O
we	O
implement	O
our	O
system	O
based	O
on	O
this	O
new	O
strategy	O
,	O
and	O
our	O
system	O
(	O
single	O
model	O
)	O
achieves	O
further	O
improvement	O
by	O
a	O
large	O
margin	O
.	O

section	O
:	O
Analysis	O
and	O
Discussion	O
subsection	O
:	O
Ablation	B-Task
Study	E-Task
To	O
get	O
better	O
insight	O
into	O
our	O
system	O
,	O
we	O
conduct	O
in	O
-	O
depth	O
ablation	B-Task
study	E-Task
on	O
the	O
development	O
set	O
of	O
MS	B-Material
-	I-Material
MARCO	E-Material
,	O
which	O
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Following	O
snet	S-Method
,	O
we	O
mainly	O
focus	O
on	O
the	O
ROUGE	B-Metric
-	I-Metric
L	I-Metric
score	E-Metric
that	O
is	O
averaged	O
case	O
by	O
case	O
.	O

We	O
first	O
evaluate	O
the	O
answer	B-Task
verification	E-Task
by	O
ablating	O
the	O
cross	B-Method
-	I-Method
passage	I-Method
verification	I-Method
model	E-Method
so	O
that	O
the	O
verification	B-Metric
loss	E-Metric
and	O
verification	B-Metric
score	E-Metric
will	O
not	O
be	O
used	O
during	O
training	O
and	O
testing	O
.	O

Then	O
we	O
remove	O
the	O
content	S-Method
model	O
in	O
order	O
to	O
test	O
the	O
necessity	O
of	O
modeling	O
the	O
content	S-Method
of	O
the	O
answer	O
.	O

Since	O
we	O
do	O
n’t	O
have	O
the	O
content	S-Method
scores	O
,	O
we	O
use	O
the	O
boundary	O
probabilities	O
instead	O
to	O
compute	O
the	O
answer	B-Method
representation	E-Method
for	O
verification	S-Task
.	O

Next	O
,	O
to	O
show	O
the	O
benefits	O
of	O
joint	B-Task
training	E-Task
,	O
we	O
train	O
the	O
boundary	B-Method
model	E-Method
separately	O
from	O
the	O
other	O
two	O
models	O
.	O

Finally	O
,	O
we	O
remove	O
the	O
yes	B-Task
/	I-Task
no	I-Task
classification	E-Task
in	O
order	O
to	O
show	O
the	O
real	O
improvement	O
of	O
our	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
model	E-Method
compared	O
with	O
the	O
baseline	O
method	O
that	O
predicts	O
the	O
answer	O
with	O
only	O
the	O
boundary	B-Method
model	E-Method
.	O

From	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
the	O
answer	B-Task
verification	E-Task
makes	O
a	O
great	O
contribution	O
to	O
the	O
overall	O
improvement	O
,	O
which	O
confirms	O
our	O
hypothesis	O
that	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	E-Task
is	O
useful	O
for	O
the	O
multi	B-Task
-	I-Task
passage	I-Task
MRC	E-Task
.	O

For	O
the	O
ablation	O
of	O
the	O
content	S-Method
model	O
,	O
we	O
analyze	O
that	O
it	O
will	O
not	O
only	O
affect	O
the	O
content	S-Method
score	O
itself	O
,	O
but	O
also	O
violate	O
the	O
verification	B-Method
model	E-Method
since	O
the	O
content	S-Method
probabilities	O
are	O
necessary	O
for	O
the	O
answer	B-Task
representation	E-Task
,	O
which	O
will	O
be	O
further	O
analyzed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Another	O
discovery	O
is	O
that	O
jointly	O
training	O
the	O
three	O
models	O
can	O
provide	O
great	O
benefits	O
,	O
which	O
shows	O
that	O
the	O
three	O
tasks	O
are	O
actually	O
closely	O
related	O
and	O
can	O
boost	O
each	O
other	O
with	O
shared	O
representations	O
at	O
bottom	O
layers	O
.	O

At	O
last	O
,	O
comparing	O
our	O
method	O
with	O
the	O
baseline	O
,	O
we	O
achieve	O
an	O
improvement	O
of	O
nearly	O
3	O
points	O
without	O
the	O
yes	B-Task
/	I-Task
no	I-Task
classification	E-Task
.	O

This	O
significant	O
improvement	O
proves	O
the	O
effectiveness	O
of	O
our	O
approach	O
.	O

subsection	O
:	O
Case	O
Study	O
To	O
demonstrate	O
how	O
each	O
module	O
of	O
our	O
model	O
takes	O
effect	O
when	O
predicting	O
the	O
final	O
answer	O
,	O
we	O
conduct	O
a	O
case	O
study	O
in	O
Table	O
[	O
reference	O
]	O
with	O
the	O
same	O
example	O
that	O
we	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

For	O
each	O
answer	O
candidate	O
,	O
we	O
list	O
three	O
scores	O
predicted	O
by	O
the	O
boundary	B-Method
model	E-Method
,	O
content	S-Method
model	O
and	O
verification	B-Method
model	E-Method
respectively	O
.	O

On	O
the	O
one	O
hand	O
,	O
we	O
can	O
see	O
that	O
these	O
three	O
scores	O
generally	O
have	O
some	O
relevance	O
.	O

For	O
example	O
,	O
the	O
second	O
candidate	O
is	O
given	O
lowest	O
scores	O
by	O
all	O
the	O
three	O
models	O
.	O

We	O
analyze	O
that	O
this	O
is	O
because	O
the	O
models	O
share	O
the	O
same	O
encoding	O
and	O
matching	O
layers	O
at	O
bottom	O
level	O
and	O
this	O
relevance	O
guarantees	O
that	O
the	O
content	S-Method
and	O
verification	B-Method
models	E-Method
will	O
not	O
violate	O
the	O
boundary	B-Method
model	E-Method
too	O
much	O
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
also	O
see	O
that	O
the	O
verification	B-Metric
score	E-Metric
can	O
really	O
make	O
a	O
difference	O
here	O
when	O
the	O
boundary	B-Method
model	E-Method
makes	O
an	O
incorrect	O
decision	O
among	O
the	O
confusing	O
answer	O
candidates	O
(	O
[	O
1	O
]	O
,	O
[	O
3	O
]	O
,	O
[	O
4	O
]	O
,	O
[	O
6	O
]	O
)	O
.	O

Besides	O
,	O
as	O
we	O
expected	O
,	O
the	O
verification	B-Method
model	E-Method
tends	O
to	O
give	O
higher	O
scores	O
for	O
those	O
answers	O
that	O
have	O
semantic	O
commonality	O
with	O
each	O
other	O
(	O
[	O
3	O
]	O
,	O
[	O
4	O
]	O
,	O
[	O
6	O
]	O
)	O
,	O
which	O
are	O
all	O
valid	O
answers	O
in	O
this	O
case	O
.	O

By	O
multiplying	O
the	O
three	O
scores	O
,	O
our	O
model	O
finally	O
predicts	O
the	O
answer	O
correctly	O
.	O

subsection	O
:	O
Necessity	O
of	O
the	O
Content	B-Method
Model	E-Method
In	O
our	O
model	O
,	O
we	O
compute	O
the	O
answer	B-Method
representation	E-Method
based	O
on	O
the	O
content	S-Method
probabilities	O
predicted	O
by	O
a	O
separate	O
content	S-Method
model	O
instead	O
of	O
directly	O
using	O
the	O
boundary	O
probabilities	O
.	O

We	O
argue	O
that	O
this	O
content	S-Method
model	O
is	O
necessary	O
for	O
our	O
answer	B-Task
verification	I-Task
process	E-Task
.	O

Figure	O
[	O
reference	O
]	O
plots	O
the	O
predicted	O
content	S-Method
probabilities	O
as	O
well	O
as	O
the	O
boundary	O
probabilities	O
for	O
a	O
passage	O
.	O

We	O
can	O
see	O
that	O
the	O
boundary	O
and	O
content	S-Method
probabilities	O
capture	O
different	O
aspects	O
of	O
the	O
answer	O
.	O

Since	O
answer	O
candidates	O
usually	O
have	O
similar	O
boundary	O
words	O
,	O
if	O
we	O
compute	O
the	O
answer	B-Method
representation	E-Method
based	O
on	O
the	O
boundary	O
probabilities	O
,	O
it	O
’s	O
difficult	O
to	O
model	O
the	O
real	O
difference	O
among	O
different	O
answer	O
candidates	O
.	O

On	O
the	O
contrary	O
,	O
with	O
the	O
content	S-Method
probabilities	O
,	O
we	O
pay	O
more	O
attention	O
to	O
the	O
content	S-Method
part	O
of	O
the	O
answer	O
,	O
which	O
can	O
provide	O
more	O
distinguishable	O
information	O
for	O
verifying	O
the	O
correct	O
answer	O
.	O

Furthermore	O
,	O
the	O
content	S-Method
probabilities	O
can	O
also	O
adjust	O
the	O
weights	O
of	O
the	O
words	O
within	O
the	O
answer	O
span	O
so	O
that	O
unimportant	O
words	O
(	O
e.g.	O
“	O
and	O
”	O
and	O
“	O
.	O

”	O
)	O
get	O
lower	O
weights	O
in	O
the	O
final	O
answer	B-Method
representation	E-Method
.	O

We	O
believe	O
that	O
this	O
refined	O
representation	O
is	O
also	O
good	O
for	O
the	O
answer	B-Task
verification	I-Task
process	E-Task
.	O

section	O
:	O
Related	O
Work	O
Machine	B-Task
reading	I-Task
comprehension	E-Task
made	O
rapid	O
progress	O
in	O
recent	O
years	O
,	O
especially	O
for	O
single	B-Task
-	I-Task
passage	I-Task
MRC	I-Task
task	E-Task
,	O
such	O
as	O
SQuAD	O
.	O

Mainstream	O
studies	O
treat	O
reading	B-Task
comprehension	E-Task
as	O
extracting	O
answer	O
span	O
from	O
the	O
given	O
passage	O
,	O
which	O
is	O
usually	O
achieved	O
by	O
predicting	O
the	O
start	O
and	O
end	O
position	O
of	O
the	O
answer	O
.	O

We	O
implement	O
our	O
boundary	B-Method
model	E-Method
similarly	O
by	O
employing	O
the	O
boundary	B-Method
-	I-Method
based	I-Method
pointer	I-Method
network	E-Method
.	O

Another	O
inspiring	O
work	O
is	O
from	O
rnet	S-Method
,	O
where	O
the	O
authors	O
propose	O
to	O
match	O
the	O
passage	O
against	O
itself	O
so	O
that	O
the	O
representation	O
can	O
aggregate	O
evidence	O
from	O
the	O
whole	O
passage	O
.	O

Our	O
verification	B-Method
model	E-Method
adopts	O
a	O
similar	O
idea	O
.	O

However	O
,	O
we	O
collect	O
information	O
across	O
passages	O
and	O
our	O
attention	S-Method
is	O
based	O
on	O
the	O
answer	B-Method
representation	E-Method
,	O
which	O
is	O
much	O
more	O
efficient	O
than	O
attention	S-Method
over	O
all	O
passages	O
.	O

For	O
the	O
model	B-Task
training	E-Task
,	O
dcn	B-Method
+	E-Method
argues	O
that	O
the	O
boundary	O
loss	O
encourages	O
exact	O
answers	O
at	O
the	O
cost	O
of	O
penalizing	O
overlapping	O
answers	O
.	O

Therefore	O
they	O
propose	O
a	O
mixed	B-Method
objective	E-Method
that	O
incorporates	O
rewards	O
derived	O
from	O
word	O
overlap	O
.	O

Our	O
joint	B-Method
training	I-Method
approach	E-Method
has	O
a	O
similar	O
function	O
.	O

By	O
taking	O
the	O
content	S-Method
and	O
verification	O
loss	O
into	O
consideration	O
,	O
our	O
model	O
will	O
give	O
less	O
loss	O
for	O
overlapping	O
answers	O
than	O
those	O
unmatched	O
answers	O
,	O
and	O
our	O
loss	O
function	O
is	O
totally	O
differentiable	O
.	O

Recently	O
,	O
we	O
also	O
see	O
emerging	O
interests	O
in	O
multi	O
-	O
passage	B-Task
MRC	E-Task
from	O
both	O
the	O
academic	O
and	O
industrial	O
community	O
.	O

Early	O
studies	O
usually	O
concat	O
those	O
passages	O
and	O
employ	O
the	O
same	O
models	O
designed	O
for	O
single	B-Task
-	I-Task
passage	I-Task
MRC	E-Task
.	O

However	O
,	O
more	O
and	O
more	O
latest	O
studies	O
start	O
to	O
design	O
specific	O
methods	O
that	O
can	O
read	O
multiple	O
passages	O
more	O
effectively	O
.	O

In	O
the	O
aspect	O
of	O
passage	B-Task
selection	E-Task
,	O
r3	S-Method
introduced	O
a	O
pipelined	B-Method
approach	E-Method
that	O
rank	O
the	O
passages	O
first	O
and	O
then	O
read	O
the	O
selected	O
passages	O
for	O
answering	O
questions	O
.	O

snet	S-Method
treats	O
the	O
passage	B-Task
ranking	E-Task
as	O
an	O
auxiliary	B-Task
task	E-Task
that	O
can	O
be	O
trained	O
jointly	O
with	O
the	O
reading	B-Method
comprehension	I-Method
model	E-Method
.	O

Actually	O
,	O
the	O
target	O
of	O
our	O
answer	B-Task
verification	E-Task
is	O
very	O
similar	O
to	O
that	O
of	O
the	O
passage	B-Task
selection	E-Task
,	O
while	O
we	O
pay	O
more	O
attention	O
to	O
the	O
answer	O
content	S-Method
and	O
the	O
answer	B-Task
verification	I-Task
process	E-Task
.	O

Speaking	O
of	O
the	O
answer	B-Task
verification	E-Task
,	O
evidence_aggregation	S-Task
has	O
a	O
similar	O
motivation	O
to	O
ours	O
.	O

They	O
attempt	O
to	O
aggregate	O
the	O
evidence	O
from	O
different	O
passages	O
and	O
choose	O
the	O
final	O
answer	O
from	O
n	O
-	O
best	O
candidates	O
.	O

However	O
,	O
they	O
implement	O
their	O
idea	O
as	O
a	O
separate	O
reranking	B-Task
step	E-Task
after	O
reading	B-Task
comprehension	E-Task
,	O
while	O
our	O
answer	B-Task
verification	E-Task
is	O
a	O
component	O
of	O
the	O
whole	O
model	O
that	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
framework	E-Method
to	O
tackle	O
the	O
multi	B-Task
-	I-Task
passage	I-Task
MRC	I-Task
task	E-Task
.	O

We	O
creatively	O
design	O
three	O
different	O
modules	O
in	O
our	O
model	O
,	O
which	O
can	O
find	O
the	O
answer	O
boundary	O
,	O
model	O
the	O
answer	O
content	S-Method
and	O
conduct	O
cross	B-Task
-	I-Task
passage	I-Task
answer	I-Task
verification	E-Task
respectively	O
.	O

All	O
these	O
three	O
modules	O
can	O
be	O
trained	O
with	O
different	O
forms	O
of	O
the	O
answer	O
labels	O
and	O
training	O
them	O
jointly	O
can	O
provide	O
further	O
improvement	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
our	O
model	O
outperforms	O
the	O
baseline	O
models	O
by	O
a	O
large	O
margin	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
two	O
challenging	O
datasets	O
,	O
both	O
of	O
which	O
are	O
designed	O
for	O
MRC	S-Task
on	O
real	O
web	O
data	O
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
is	O
supported	O
by	O
the	O
National	O
Basic	O
Research	O
Program	O
of	O
China	O
(	O
973	O
program	O
,	O
No	O
.	O

2014CB340505	O
)	O
and	O
Baidu	O
-	O
Peking	O
University	O
Joint	O
Project	O
.	O

We	O
thank	O
the	O
Microsoft	O
MSMARCO	O
team	O
for	O
evaluating	O
our	O
results	O
on	O
the	O
anonymous	O
test	O
set	O
.	O

We	O
also	O
thank	O
Ying	O
Chen	O
,	O
Xuan	O
Liu	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
criticism	O
of	O
the	O
manuscript	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Practical	O
Text	B-Task
Classification	E-Task
With	O
Large	O
Pre	O
-	O
Trained	O
Language	B-Method
Models	E-Method
Multi	O
-	O
emotion	O
sentiment	S-Task
classification	O
is	O
a	O
natural	B-Task
language	I-Task
processing	E-Task
(	O
NLP	S-Task
)	O
problem	O
with	O
valuable	O
use	O
cases	O
on	O
real	O
-	O
world	O
data	O
.	O

We	O
demonstrate	O
that	O
large	O
-	O
scale	O
unsupervised	B-Method
language	I-Method
modeling	E-Method
combined	O
with	O
finetuning	S-Method
offers	O
a	O
practical	O
solution	O
to	O
this	O
task	O
on	O
difficult	O
datasets	O
,	O
including	O
those	O
with	O
label	O
class	O
imbalance	O
and	O
domain	O
-	O
specific	O
context	O
.	O

By	O
training	O
an	O
attention	O
-	O
based	O
Transformer	S-Method
network	O
on	O
40	O
GB	O
of	O
text	O
(	O
Amazon	O
reviews	O
)	O
and	O
fine	O
-	O
tuning	O
on	O
the	O
training	O
set	O
,	O
our	O
model	O
achieves	O
a	O
0.69	O
F1	B-Metric
score	E-Metric
on	O
the	O
SemEval	S-Material
Task	O
1:E	O
-	O
c	O
multidimensional	B-Task
emotion	I-Task
classification	E-Task
problem	O
,	O
based	O
on	O
the	O
Plutchik	O
wheel	O
of	O
emotions	O
.	O

These	O
results	O
are	O
competitive	O
with	O
state	O
of	O
the	O
art	O
models	O
,	O
including	O
strong	O
F1	B-Metric
scores	E-Metric
on	O
difficult	O
(	O
emotion	O
)	O
categories	O
such	O
as	O
Fear	O
(	O
0.73	O
)	O
,	O
Disgust	O
(	O
0.77	O
)	O
and	O
Anger	O
(	O
0.78	O
)	O
,	O
as	O
well	O
as	O
competitive	O
results	O
on	O
rare	O
categories	O
such	O
as	O
Anticipation	O
(	O
0.42	O
)	O
and	O
Surprise	O
(	O
0.37	O
)	O
.	O

Furthermore	O
,	O
we	O
demonstrate	O
our	O
application	O
on	O
a	O
real	B-Task
world	I-Task
text	I-Task
classification	I-Task
task	E-Task
.	O

We	O
create	O
a	O
narrowly	O
collected	O
text	O
dataset	O
of	O
real	O
tweets	O
on	O
several	O
topics	O
,	O
and	O
show	O
that	O
our	O
finetuned	B-Method
model	E-Method
outperforms	O
general	O
purpose	O
commercially	O
available	O
APIs	O
for	O
sentiment	S-Task
and	O
multidimensional	B-Task
emotion	I-Task
classification	E-Task
on	O
this	O
dataset	O
by	O
a	O
significant	O
margin	O
.	O

We	O
also	O
perform	O
a	O
variety	O
of	O
additional	O
studies	O
,	O
investigating	O
properties	O
of	O
deep	B-Method
learning	I-Method
architectures	E-Method
,	O
datasets	O
and	O
algorithms	O
for	O
achieving	O
practical	O
multidimensional	O
sentiment	S-Task
classification	O
.	O

Overall	O
,	O
we	O
find	O
that	O
unsupervised	B-Method
language	I-Method
modeling	E-Method
and	O
finetuning	S-Method
is	O
a	O
simple	O
framework	O
for	O
achieving	O
high	O
quality	O
results	O
on	O
real	O
-	O
world	O
sentiment	S-Task
classification	O
.	O

section	O
:	O
Introduction	O
Recent	O
work	O
has	O
shown	O
that	O
language	B-Method
models	E-Method
–	O
both	O
RNN	S-Method
variants	O
like	O
the	O
multiplicative	B-Method
LSTM	E-Method
(	O
mLSTM	S-Method
)	O
,	O
as	O
well	O
as	O
the	O
attention	O
-	O
based	O
Transformer	S-Method
network	O
–	O
can	O
be	O
trained	O
efficiently	O
over	O
very	O
large	O
datasets	O
,	O
and	O
that	O
the	O
resulting	O
models	O
can	O
be	O
transferred	O
to	O
downstream	B-Task
language	I-Task
understanding	I-Task
problems	E-Task
,	O
often	O
matching	O
or	O
exceeding	O
the	O
previous	O
state	O
of	O
the	O
art	O
approaches	O
on	O
academic	O
datasets	O
.	O

However	O
,	O
how	O
well	O
do	O
these	O
models	O
perform	O
on	O
practical	O
text	B-Task
classification	I-Task
problems	E-Task
,	O
with	O
real	O
world	O
data	O
?	O
In	O
this	O
work	O
,	O
we	O
train	O
both	O
mLSTM	S-Method
and	O
Transformer	S-Method
language	O
models	O
on	O
a	O
large	O
40	O
GB	O
text	O
dataset	O
,	O
then	O
transfer	O
those	O
models	O
to	O
two	O
text	B-Task
classification	I-Task
problems	E-Task
:	O
binary	O
sentiment	S-Task
(	O
including	O
Neutral	O
labels	O
)	O
,	O
and	O
multidimensional	B-Task
emotion	I-Task
classification	E-Task
based	O
on	O
the	O
Plutchik	O
wheel	O
of	O
emotions	O
.	O

We	O
examine	O
our	O
performance	O
on	O
these	O
tasks	O
,	O
both	O
against	O
large	O
academic	O
datasets	O
,	O
and	O
on	O
an	O
original	O
text	O
dataset	O
that	O
we	O
compiled	O
from	O
social	O
media	O
messages	O
about	O
several	O
specific	O
topics	O
,	O
such	O
as	O
video	O
games	O
.	O

We	O
demonstrate	O
that	O
our	O
approach	O
matches	O
the	O
state	O
of	O
the	O
art	O
on	O
the	O
academic	O
datasets	O
without	O
domain	O
-	O
specific	O
training	O
and	O
without	O
excessive	O
hyper	B-Method
-	I-Method
parameter	I-Method
tuning	E-Method
.	O

Meanwhile	O
on	O
the	O
social	O
media	O
dataset	O
,	O
our	O
approach	O
outperforms	O
commercially	O
available	O
APIs	O
by	O
significant	O
margins	O
,	O
even	O
when	O
those	O
models	O
are	O
re	O
-	O
calibrated	O
to	O
the	O
test	O
set	O
.	O

Furthermore	O
,	O
we	O
notice	O
that	O
1	O
)	O
the	O
Transformer	S-Method
model	O
generally	O
out	O
-	O
performs	O
the	O
mLSTM	S-Method
model	O
,	O
especially	O
when	O
fine	B-Task
-	I-Task
tuning	E-Task
on	O
multidimensional	B-Task
emotion	I-Task
classification	E-Task
,	O
and	O
2	O
)	O
fine	O
-	O
tuning	O
the	O
model	O
significantly	O
improves	O
performance	O
on	O
the	O
emotion	B-Task
tasks	E-Task
,	O
both	O
for	O
the	O
mLSTM	S-Method
and	O
the	O
Transformer	S-Method
model	O
.	O

We	O
suggest	O
that	O
our	O
approach	O
creates	O
models	O
with	O
good	O
generalization	S-Task
to	O
increasingly	O
difficult	O
text	B-Task
classification	I-Task
problems	E-Task
,	O
and	O
we	O
offer	O
ablation	O
studies	O
to	O
demonstrate	O
that	O
effect	O
.	O

It	O
is	O
difficult	O
to	O
fit	O
a	O
single	O
model	O
for	O
text	B-Task
classification	E-Task
across	O
domains	O
,	O
due	O
to	O
unknown	O
words	O
,	O
specialized	O
context	O
,	O
colloquial	O
language	O
,	O
and	O
other	O
differences	O
between	O
domains	O
.	O

For	O
example	O
,	O
words	O
such	O
as	O
war	O
and	O
sick	O
are	O
not	O
necessarily	O
negative	O
in	O
the	O
context	O
of	O
video	O
games	O
,	O
which	O
are	O
significantly	O
represented	O
in	O
our	O
dataset	O
.	O

By	O
training	O
a	O
language	B-Method
model	E-Method
across	O
a	O
large	O
text	O
dataset	O
,	O
we	O
expose	O
our	O
model	O
to	O
many	O
contexts	O
.	O

Perhaps	O
a	O
small	O
amount	O
of	O
downstream	B-Task
transfer	E-Task
is	O
enough	O
to	O
choose	O
the	O
right	O
context	O
features	O
for	O
emotion	B-Task
classification	E-Task
in	O
the	O
appropriate	O
setting	O
.	O

Our	O
work	O
shows	O
that	O
unsupervised	B-Method
language	I-Method
modeling	E-Method
combined	O
with	O
finetuning	S-Method
offers	O
a	O
practical	O
solution	O
to	O
specialized	O
text	B-Task
classification	I-Task
problems	E-Task
,	O
including	O
those	O
with	O
large	O
category	O
class	O
imbalance	O
,	O
and	O
significant	O
human	O
label	O
disagreement	O
.	O

section	O
:	O
Background	O
Supervised	B-Method
learning	E-Method
is	O
difficult	O
to	O
apply	O
to	O
NLP	S-Task
problems	O
because	O
labels	O
are	O
expensive	O
.	O

Following	O
,	O
and	O
,	O
we	O
train	O
unsupervised	B-Method
text	I-Method
models	E-Method
on	O
large	O
amounts	O
of	O
unlabelled	O
text	O
data	O
,	O
and	O
transfer	O
the	O
model	O
features	O
to	O
small	O
supervised	B-Task
text	I-Task
problems	E-Task
.	O

The	O
supervised	B-Task
text	I-Task
classification	I-Task
problem	E-Task
used	O
for	O
transfer	S-Task
is	O
binary	O
sentiment	S-Task
on	O
the	O
Stanford	B-Material
Sentiment	I-Material
Treebank	E-Material
(	O
SST	S-Material
)	O
.	O

Some	O
of	O
these	O
binary	O
text	O
examples	O
are	O
subtle	O
.	O

Prior	O
works	O
show	O
that	O
unsupervised	B-Method
language	I-Method
models	E-Method
can	O
learn	O
nuanced	O
features	O
of	O
text	O
,	O
such	O
as	O
word	O
ordering	O
and	O
double	O
negation	O
,	O
just	O
from	O
the	O
underlying	O
task	O
of	O
next	B-Task
-	I-Task
word	I-Task
prediction	E-Task
.	O

However	O
,	O
while	O
this	O
includes	O
difficult	O
examples	O
,	O
it	O
does	O
not	O
necessarily	O
represent	O
sentiment	S-Task
on	O
practical	O
text	B-Task
problems	E-Task
.	O

The	O
source	O
material	O
(	O
professionally	O
written	O
movie	O
reviews	O
)	O
does	O
not	O
include	O
colloquial	O
language	O
.	O

The	O
dataset	O
excludes	O
Neutral	O
sentiment	S-Task
texts	O
and	O
those	O
with	O
weak	O
directional	O
sentiment	S-Task
.	O

The	O
dataset	O
does	O
not	O
include	O
dimensions	O
of	O
sentiment	S-Task
apart	O
from	O
Positive	O
and	O
Negative	O
.	O

TweetWatsonSadJoyFearGCLOursBinaryBinaryBinaryEncouraging	O
collaboration	O
among	O
players	O
in	O
Sea	O
of	O
Thieves	O
<	O
url>	O
-	O
0.3020.2290.1940.150	O
-	O
0.80Posgot	O
my	O
first	O
kill	O
on	O
Fortnite	O
all	O
by	O
myself	O
I	O
’	O
m	O
geeked	O
<	O
emoji	O
>	O
perioddddd.	O
-	O
0.8470.0030.6660.225	O
+	O
0.60NeuFar	O
Cry	O
5	O
”	O
Lost	O
On	O
Mars	O
”	O
Gameplay	O
Walkthrough	O
-	O
DLC2	O
:	O
<	O
url	O
>	O
via	O
@YouTube	O
-	O
0.9090.0470.0150.873	O
+	O
0.00NeuNEW	O
SUBMACHINE	B-Task
GUN	E-Task
IS	O
INSANE	O
!	O
—	O
Fortnite	O
Best	O
Moments	O
39	O
(	O
Fortnite	O
Funny	O
Fails	O
&	O
WTF	O
Moments	O
)	O
<	O
url>	O
-	O
0.9360.8210.1780.056	O
-	O
0.10Pos	O
paragraph	O
:	O
Plutchik	O
’s	O
Wheel	O
of	O
Emotions	O
We	O
focus	O
our	O
multi	B-Task
-	I-Task
dimension	I-Task
emotion	I-Task
classification	E-Task
on	O
Plutchik	B-Task
’s	I-Task
wheel	I-Task
of	I-Task
emotions	E-Task
.	O

This	O
taxonomy	O
,	O
in	O
use	O
since	O
1979	O
,	O
aims	O
to	O
classify	O
human	O
emotions	O
as	O
a	O
combination	O
of	O
four	O
dualities	O
:	O
Joy	O
-	O
Sadness	O
,	O
Anger	O
-	O
Fear	O
,	O
Trust	O
-	O
Disgust	O
,	O
and	O
Surprise	O
-	O
Anticipation	O
.	O

According	O
to	O
the	O
basic	O
emotion	B-Method
model	E-Method
,	O
while	O
humans	O
experience	O
hundreds	O
of	O
emotions	O
,	O
some	O
emotions	O
are	O
more	O
fundamental	O
than	O
others	O
.	O

The	O
commercial	O
general	B-Task
purpose	I-Task
emotion	I-Task
classification	I-Task
API	E-Task
that	O
we	O
compare	O
against	O
,	O
IBM	O
’s	O
Watson	S-Method
,	O
offers	O
classification	B-Metric
scores	E-Metric
for	O
the	O
Joy	O
,	O
Sadness	O
,	O
Fear	O
,	O
Disgust	O
and	O
Anger	O
emotions	O
–	O
all	O
present	O
in	O
Plutchik	B-Method
’s	I-Method
wheel	E-Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

paragraph	O
:	O
SemEval	B-Material
Multidimension	I-Material
Emotion	I-Material
Dataset	E-Material
The	O
SemEval	S-Material
Task	O
1:E	O
-	O
c	O
problem	O
offers	O
a	O
training	O
set	O
of	O
6	O
,	O
857	O
tweets	O
,	O
with	O
binary	O
labels	O
for	O
the	O
eight	O
Plutchik	O
categories	O
,	O
plus	O
Optimism	O
,	O
Pessimism	O
,	O
and	O
Love	O
.	O

This	O
dataset	O
was	O
created	O
through	O
a	O
process	O
of	O
text	B-Task
selection	E-Task
and	O
human	B-Task
labeling	E-Task
.	O

We	O
show	O
our	O
results	O
on	O
this	O
dataset	O
and	O
compare	O
it	O
to	O
the	O
current	O
state	O
of	O
the	O
art	O
performance	O
.	O

While	O
it	O
is	O
not	O
possible	O
to	O
report	O
rater	O
agreement	O
on	O
these	O
categories	O
for	O
the	O
compilation	O
of	O
the	O
dataset	O
,	O
the	O
authors	O
note	O
that	O
2	O
out	O
of	O
7	O
raters	O
had	O
to	O
agree	O
for	O
a	O
positive	O
label	O
to	O
be	O
applied	O
,	O
as	O
requiring	O
larger	O
agreement	O
caused	O
a	O
scarcity	O
of	O
labels	O
for	O
some	O
categories	O
.	O

This	O
indicates	O
that	O
some	O
of	O
the	O
categories	O
had	O
significant	O
rater	O
disagreement	O
between	O
the	O
human	O
raters	O
.	O

The	O
dataset	O
also	O
included	O
a	O
substantial	O
degree	O
of	O
label	O
class	O
imbalance	O
,	O
with	O
some	O
categories	O
like	O
Anger	O
(	O
37	O
%	O
)	O
,	O
Disgust	O
(	O
38	O
%	O
)	O
,	O
Joy	O
(	O
36	O
%	O
)	O
and	O
Sadness	O
(	O
29	O
%	O
)	O
represented	O
often	O
in	O
the	O
dataset	O
,	O
while	O
others	O
like	O
Trust	O
(	O
5	O
%	O
)	O
and	O
Surprise	O
(	O
5	O
%	O
)	O
present	O
much	O
less	O
frequently	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
class	B-Metric
imbalance	E-Metric
and	O
human	B-Metric
rater	I-Metric
disagreement	E-Metric
is	O
not	O
uncommon	O
for	O
real	B-Task
world	I-Task
text	I-Task
classification	I-Task
problems	E-Task
.	O

SizeAngerAnticipationDisgustFearJoySadSurpriseTrustAve	O
/	O
NoneSemEval6	O
,	O
85837.214.338.018.236.229.45.35.223.0	O
/	O
2.9	O
(	O
Random	O
)	O
4	O
,	O
0217.814.75.21.721.93.44.36.68.2	O
/	O
52.1	O
(	O
Active	O
)	O
5	O
,	O
02422.010.212.35.619.76.37.16.511.2	O
/	O
35.6	O
(	O
All	O
)	O
13	O
,	O
32611.712.96.82.920.64.25.07.68.9	O
/	O
47.0	O
paragraph	O
:	O
Company	O
Tweet	O
Dataset	O
In	O
addition	O
to	O
the	O
SemEval	S-Material
tweet	O
dataset	O
,	O
we	O
wanted	O
to	O
see	O
how	O
our	O
model	O
would	O
perform	O
on	O
a	O
similar	O
but	O
domain	B-Task
-	I-Task
specific	I-Task
task	E-Task
:	O
Plutchik	B-Task
emotion	I-Task
classification	E-Task
on	O
tweets	O
relevant	O
to	O
a	O
particular	O
company	O
.	O

We	O
collected	O
tweets	O
on	O
a	O
variety	O
of	O
topics	O
,	O
including	O
:	O
Video	O
game	O
tweets	O
Tweets	O
about	O
the	O
company	O
stock	O
We	O
submitted	O
the	O
first	O
batch	O
of	O
4	O
,	O
000	O
tweets	O
to	O
human	O
raters	O
on	O
the	O
FigureEight	B-Method
platform	E-Method
,	O
with	O
rules	O
similar	O
to	O
those	O
used	O
by	O
SemEval	S-Material
,	O
which	O
also	O
used	O
the	O
FigureEight	B-Method
platform	E-Method
for	O
human	B-Task
labeling	E-Task
.	O

Specifically	O
,	O
we	O
verified	O
that	O
raters	O
passed	O
our	O
golden	O
set	O
(	O
answering	O
70	O
%	O
of	O
test	O
questions	O
correctly	O
)	O
.	O

We	O
applied	O
positive	O
labels	O
for	O
each	O
category	O
where	O
2	O
out	O
of	O
5	O
raters	O
agreed	O
.	O

This	O
is	O
slightly	O
less	O
permissive	O
than	O
the	O
2	O
out	O
of	O
7	O
raters	O
used	O
by	O
SemEval	S-Material
,	O
because	O
we	O
did	O
not	O
have	O
a	O
budget	O
for	O
7	O
raters	O
per	O
tweet	O
.	O

After	O
the	O
first	O
pass	O
,	O
we	O
noticed	O
that	O
random	B-Method
sampling	E-Method
led	O
to	O
some	O
categories	O
being	O
severely	O
under	O
-	O
sampled	O
,	O
below	O
5	O
%	O
of	O
tweets	O
.	O

Thus	O
we	O
employed	O
a	O
bootstrapping	B-Method
technique	E-Method
to	O
pre	O
-	O
classify	O
tweets	O
by	O
category	O
using	O
our	O
current	O
model	O
,	O
and	O
choose	O
tweets	O
with	O
more	O
likely	O
emotion	O
tweets	O
for	O
classification	S-Task
.	O

See	O
Active	B-Method
Learning	E-Method
section	O
for	O
details	O
.	O

We	O
also	O
sampled	O
5	O
,	O
000	O
tweets	O
balanced	O
by	O
source	O
category	O
,	O
since	O
video	O
game	O
tweets	O
have	O
much	O
more	O
emotion	O
,	O
thus	O
dominated	O
the	O
bootstrapped	O
selections	O
.	O

Henceforth	O
,	O
we	O
refer	O
to	O
the	O
combined	O
company	O
tweets	O
dataset	O
consisting	O
of	O
:	O
4	O
,	O
021	O
random	O
tweets	O
5	O
,	O
024	O
tweets	O
selected	O
for	O
higher	O
emotion	O
content	O
4	O
,	O
281	O
tweets	O
selected	O
for	O
source	O
category	O
balance	O
DatasetJudgmentsBinaryPlutchik	O
(	O
3	O
choices	O
)(	O
8	O
choices	O
)	O
SemEval20	O
,	O
51477.3%61.1%Company	O
(	O
random	O
)	O
20	O
,	O
00580.7%67.3%Company	O
(	O
active	O
)	O
25	O
,	O
01779.0%52.3%Company	O
(	O
balanced	O
)	O
23	O
,	O
81280.0%71.0	O
%	O
paragraph	O
:	O
Finetuning	O
Recent	O
work	O
has	O
shown	O
promising	O
results	O
using	O
unsupervised	B-Task
language	I-Task
modeling	E-Task
,	O
followed	O
by	O
transfer	B-Method
learning	E-Method
to	O
natural	B-Task
language	I-Task
tasks	E-Task
,	O
.	O

Furthermore	O
,	O
these	O
models	O
benefit	O
when	O
the	O
entire	O
model	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
transfer	B-Task
task	E-Task
,	O
as	O
demonstrated	O
in	O
.	O

Specifically	O
,	O
these	O
methods	O
have	O
beaten	O
the	O
state	O
of	O
the	O
art	O
on	O
binary	O
sentiment	S-Task
classification	O
.	O

These	O
models	O
have	O
also	O
attained	O
the	O
best	O
overall	O
score	O
on	O
the	O
GLUE	O
Benchmark	O
,	O
comprised	O
of	O
a	O
variety	O
of	O
text	B-Task
understanding	I-Task
tasks	E-Task
,	O
including	O
entailment	B-Task
and	I-Task
question	I-Task
answering	E-Task
.	O

section	O
:	O
Methodology	O
We	O
use	O
a	O
larger	O
batch	O
size	O
with	O
shorter	O
sequence	O
length	O
,	O
specifically	O
a	O
global	O
batch	O
of	O
512	O
and	O
sequence	O
length	O
64	O
tokens	O
(	O
tokenized	O
with	O
a	O
32	O
,	O
000	O
BPE	O
vocabulary	O
,	O
as	O
detailed	O
in	O
Characters	O
and	O
Subword	O
Units	O
.	O

The	O
shorter	O
sequence	O
length	O
works	O
well	O
because	O
the	O
transfer	O
target	O
are	O
tweets	O
,	O
which	O
are	O
short	O
pieces	O
of	O
text	O
.	O

We	O
trained	O
our	O
language	B-Method
model	E-Method
on	O
the	O
Amazon	O
Reviews	O
dataset	O
rather	O
than	O
other	O
large	O
datasets	O
like	O
BooksCorpus	O
,	O
because	O
reviews	O
are	O
rich	O
in	O
emotional	O
context	O
.	O

We	O
also	O
train	O
an	O
mLSTM	S-Method
network	O
on	O
the	O
same	O
dataset	O
,	O
based	O
on	O
the	O
model	O
from	O
.	O

We	O
chose	O
to	O
compare	O
these	O
particular	O
models	O
because	O
they	O
work	O
in	O
fundamentally	O
different	O
ways	O
and	O
because	O
they	O
collectively	O
hold	O
state	O
of	O
the	O
art	O
results	O
on	O
many	O
significant	O
academic	O
NLP	S-Task
benchmarks	O
.	O

We	O
wanted	O
to	O
test	O
these	O
models	O
on	O
difficult	O
classification	B-Task
problems	E-Task
with	O
real	O
-	O
world	O
data	O
.	O

paragraph	O
:	O
Unsupervised	B-Task
Pretraining	E-Task
.	O

The	O
language	B-Task
modeling	I-Task
objective	E-Task
can	O
be	O
summarized	O
as	O
a	O
maximum	B-Task
likelihood	I-Task
estimation	I-Task
problem	E-Task
for	O
a	O
sequence	O
of	O
tokens	O
.	O

We	O
treat	O
our	O
model	O
as	O
a	O
function	O
with	O
two	O
parts	O
:	O
an	O
encoder	B-Method
and	I-Method
decoder	E-Method
.	O

The	O
encoder	S-Method
forms	O
the	O
bulk	O
of	O
the	O
model	O
,	O
including	O
the	O
token	B-Method
embedding	I-Method
dictionary	E-Method
as	O
the	O
first	O
module	O
.	O

The	O
decoder	S-Method
is	O
simply	O
a	O
softmax	B-Method
linear	I-Method
layer	E-Method
that	O
projects	O
the	O
encoder	O
output	O
into	O
the	O
dimension	O
equal	O
to	O
the	O
vocabulary	O
size	O
.	O

The	O
objective	O
to	O
maximize	O
is	O
as	O
follows	O
.	O

where	O
is	O
a	O
hidden	O
layer	O
activation	O
in	O
the	O
final	O
layer	O
of	O
,	O
indexed	O
for	O
timestep	O
.	O

The	O
model	O
is	O
tasked	O
with	O
predicting	O
the	O
next	O
token	O
given	O
all	O
of	O
the	O
ones	O
prior	O
by	O
outputting	O
a	O
probability	O
distribution	O
over	O
the	O
vocabulary	O
of	O
tokens	O
.	O

Doing	O
this	O
for	O
each	O
timestep	O
produces	O
each	O
term	O
in	O
the	O
sum	O
of	O
the	O
log	B-Method
-	I-Method
likelihood	I-Method
formulation	E-Method
,	O
and	O
so	O
maximizing	O
the	O
correct	O
probabilities	O
is	O
a	O
way	O
to	O
understand	O
the	O
joint	O
probability	O
distribution	O
of	O
sequences	O
in	O
this	O
corpus	O
of	O
text	O
.	O

paragraph	O
:	O
Characters	O
and	O
Subword	O
Units	O
.	O

While	O
,	O
and	O
have	O
shown	O
state	O
of	O
the	O
art	O
results	O
for	O
language	B-Task
modeling	E-Task
and	O
task	B-Task
transfer	E-Task
with	O
character	O
-	O
level	O
mLSTM	S-Method
models	O
,	O
we	O
found	O
that	O
our	O
Transformer	S-Method
model	O
benefits	O
from	O
modeling	O
language	O
through	O
subword	O
units	O
.	O

Using	O
a	O
byte	B-Method
-	I-Method
pair	I-Method
-	I-Method
encoding	E-Method
(	O
BPE	S-Method
)	O
of	O
various	O
sized	O
we	O
notice	O
that	O
a	O
32	O
,	O
000	O
word	O
-	O
piece	O
vocabulary	O
achieves	O
a	O
better	O
bits	B-Metric
per	I-Metric
character	E-Metric
(	O
BPC	S-Metric
)	O
loss	O
over	O
one	O
epoch	O
of	O
the	O
Amazon	O
Reviews	O
dataset	O
than	O
a	O
small	O
vocabulary	O
.	O

We	O
compute	O
the	O
BPC	B-Method
equivalent	E-Method
over	O
word	O
pieces	O
,	O
following	O
.	O

For	O
the	O
remainder	O
of	O
this	O
work	O
,	O
our	O
Transformer	S-Method
models	O
use	O
32	O
,	O
000	O
word	O
pieces	O
.	O

Recent	O
work	O
has	O
shown	O
that	O
it	O
is	O
possible	O
to	O
train	O
a	O
character	O
level	O
Transformer	S-Method
that	O
is	O
up	O
to	O
64	O
layers	O
deep	O
and	O
which	O
beat	O
state	O
of	O
the	O
art	O
BPC	S-Method
over	O
large	O
text	O
datasets	O
.	O

However	O
this	O
requires	O
intermediate	O
layer	O
losses	O
,	O
and	O
other	O
auxiliary	O
losses	O
for	O
optimal	B-Task
convergence	E-Task
.	O

By	O
comparison	O
,	O
uses	O
a	O
bytepair	B-Method
encoding	I-Method
vocabulary	E-Method
with	O
40	O
,	O
000	O
word	O
pieces	O
for	O
their	O
state	O
of	O
the	O
art	O
results	O
on	O
language	B-Task
transfer	I-Task
tasks	E-Task
with	O
a	O
Transformer	S-Method
model	O
.	O

Our	O
work	O
closely	O
follows	O
their	O
model	O
.	O

paragraph	O
:	O
Supervised	B-Task
Finetuning	E-Task
.	O

After	O
the	O
pretraining	O
,	O
we	O
initialize	O
a	O
new	O
decoder	S-Method
to	O
be	O
exclusively	O
trained	O
on	O
the	O
supervised	B-Task
problem	E-Task
.	O

Depending	O
on	O
the	O
task	O
,	O
this	O
decoder	O
may	O
be	O
a	O
single	O
linear	B-Method
layer	E-Method
with	O
activation	S-Method
or	O
an	O
MLP	S-Method
.	O

We	O
also	O
retain	O
the	O
original	O
decoder	O
and	O
continue	O
to	O
train	O
it	O
by	O
using	O
language	B-Method
modeling	E-Method
as	O
an	O
auxiliary	O
loss	O
when	O
finetuning	S-Method
on	O
the	O
new	O
corpus	O
.	O

Error	O
signals	O
from	O
both	O
decoders	O
are	O
backpropagated	O
into	O
the	O
language	B-Method
model	E-Method
.	O

The	O
differences	O
between	O
the	O
hyperparameters	S-Method
for	O
finetuning	S-Method
and	O
language	B-Task
modeling	E-Task
are	O
described	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Language	O
ModelingFinetuningglobal	O
batch	O
size512	O
(	O
size	O
64	O
on	O
8	O
GPUs	O
)	O
32sequence	O
length64	O
-	O
kept	O
short	O
because	O
targeting	O
tweet	O
applicationmax	O
(	O
batch	O
)	O
optimizerADAM⁢lr	O
(	O
schedule	O
)	O
×210	O
-	O
4	O
(	O
cosine	O
decay	O
after	O
linear	O
warmup	O
on	O
2000	O
iterations	O
)	O
×110	O
-	O
5	O
(	O
constant	O
after	O
1	O
/	O
2	O
epoch	O
linear	O
warmup	O
)	O
Decoder	O
moduleR×dh32000Binary	O
:	O
MLP	O
(	O
1024	O
→	O
nc	O
)	O
with	O
PReLU	O
and	O
0.3	O
dropoutMulticlass	O
:	O
MLP	O
(	O
4096	O
→	O
2048	O
→	O
1024	O
→	O
nc	O
)	O
with	O
PReLU	S-Method
and	O
0.3	O
dropout	O
#	O
Epochs15LossL⁢LM	O
=	O
Softmax	O
Cross	O
EntropySigmoid	O
Binary	O
Cross	O
Entropy	O
+	O
⋅0.02L⁢LM	O
paragraph	O
:	O
ELMo	O
Baseline	O
We	O
also	O
compare	O
our	O
language	B-Method
models	E-Method
to	O
ELMo	S-Method
,	O
a	O
contextualized	B-Method
word	I-Method
representation	E-Method
based	O
on	O
a	O
deep	B-Method
bidirectional	I-Method
language	I-Method
model	E-Method
,	O
trained	O
on	O
large	O
text	O
corpus	O
.	O

We	O
use	O
a	O
publicly	O
available	O
pretrained	B-Method
ELMo	I-Method
model	E-Method
from	O
the	O
authors	O
.	O

During	O
finetuning	S-Method
,	O
text	O
is	O
embedded	O
with	O
ELMo	S-Method
before	O
being	O
passed	O
into	O
a	O
decoder	S-Method
.	O

Error	O
signals	O
are	O
backpropagated	O
into	O
the	O
ELMo	B-Method
language	I-Method
model	E-Method
.	O

Unlike	O
our	O
other	O
models	O
,	O
we	O
do	O
not	O
use	O
an	O
auxiliary	O
language	O
modeling	O
loss	O
during	O
finetuning	S-Method
,	O
as	O
the	O
ELMo	B-Method
language	I-Method
model	E-Method
is	O
bidirectional	O
.	O

Finetuning	O
the	O
ELMo	B-Method
model	E-Method
substantially	O
improves	O
accuracy	S-Metric
on	O
our	O
tasks	O
,	O
thus	O
we	O
include	O
only	O
finetuned	O
ELMo	O
results	O
.	O

paragraph	O
:	O
Multihead	O
vs.	O
Single	B-Method
Head	I-Method
Finetuning	I-Method
Decoders	E-Method
The	O
tweet	O
datasets	O
are	O
an	O
example	O
of	O
a	O
multilabel	B-Task
classification	I-Task
problem	E-Task
.	O

We	O
can	O
formulate	O
the	O
problem	O
for	O
the	O
finetuning	S-Method
decoder	O
,	O
as	O
either	O
a	O
collection	O
of	O
single	O
binary	B-Task
problems	E-Task
or	O
multiple	O
problems	O
put	O
together	O
.	O

The	O
single	O
binary	B-Method
problem	I-Method
formulation	E-Method
allows	O
for	O
a	O
focus	O
on	O
one	O
class	O
and	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
optimization	E-Task
will	O
only	O
have	O
one	O
error	O
signal	O
.	O

However	O
,	O
because	O
the	O
label	O
classes	O
are	O
imbalanced	O
in	O
all	O
categories	O
,	O
this	O
may	O
lead	O
to	O
a	O
sparse	O
gradient	O
signal	O
for	O
the	O
positive	O
label	O
,	O
which	O
may	O
impact	O
recall	S-Metric
and	O
precision	S-Metric
.	O

Increasing	O
the	O
size	O
of	O
to	O
more	O
than	O
one	O
linear	B-Method
layer	E-Method
leads	O
to	O
rapid	O
overfitting	O
and	O
lower	O
validation	B-Metric
performance	E-Metric
.	O

The	O
combined	O
binary	B-Method
problems	I-Method
formulation	E-Method
(	O
henceforth	O
described	O
as	O
multihead	O
)	O
allows	O
for	O
a	O
richer	O
error	O
signal	O
that	O
propagates	O
more	O
information	O
through	O
the	O
encoder	S-Method
and	O
sentiment	S-Task
representation	O
in	O
.	O

In	O
this	O
setup	O
,	O
constructing	O
a	O
Multilayer	B-Method
network	E-Method
is	O
far	O
more	O
useful	O
,	O
and	O
can	O
be	O
thought	O
of	O
as	O
specifically	O
creating	O
sentiment	S-Task
features	O
to	O
be	O
used	O
at	O
the	O
final	O
layer	O
to	O
predict	O
the	O
presence	O
of	O
the	O
individual	O
emotions	O
.	O

We	O
find	O
that	O
the	O
inclusion	O
of	O
easier	O
,	O
more	O
balanced	O
label	O
categories	O
improves	O
performance	O
on	O
harder	O
ones	O
in	O
Table	O
[	O
reference	O
]	O
.	O

However	O
,	O
the	O
easier	O
categories	O
have	O
slightly	O
lower	O
performance	O
because	O
the	O
network	O
is	O
not	O
being	O
optimized	O
for	O
only	O
those	O
categories	O
.	O

paragraph	O
:	O
Thresholding	O
Supervised	O
Results	O
For	O
both	O
the	O
multihead	B-Method
MLP	E-Method
and	O
the	O
single	B-Method
linear	I-Method
layer	I-Method
instantiating	I-Method
of	E-Method
,	O
we	O
found	O
that	O
thresholding	O
predictions	O
produced	O
noticeably	O
better	O
results	O
than	O
using	O
a	O
fixed	O
threshold	O
value	O
such	O
as	O
.	O

This	O
makes	O
sense	O
since	O
the	O
label	O
classes	O
for	O
most	O
categories	O
are	O
very	O
imbalanced	O
.	O

For	O
thresholding	S-Task
,	O
we	O
take	O
a	O
dataset	O
of	O
tweets	O
and	O
split	O
it	O
into	O
training	O
(	O
70	O
%	O
)	O
,	O
thresholding	S-Method
(	O
10	O
%	O
)	O
and	O
validation	S-Metric
(	O
20	O
%	O
)	O
sets	O
.	O

At	O
each	O
epoch	O
of	O
finetuning	S-Method
on	O
the	O
training	O
set	O
,	O
we	O
calculate	O
validation	B-Metric
accuracy	E-Metric
and	O
save	O
predictions	O
on	O
the	O
threshold	O
set	O
on	O
the	O
epoch	O
for	O
which	O
this	O
is	O
maximized	O
.	O

To	O
threshold	O
,	O
we	O
search	O
the	O
discretized	O
version	O
of	O
[	O
0	O
,	O
1	O
]	O
:	O
the	O
linear	O
space	O
for	O
the	O
positive	O
label	O
threshold	O
for	O
each	O
category	O
.	O

We	O
denoted	O
the	O
threshold	O
which	O
gave	O
the	O
best	O
score	O
on	O
the	O
threshold	O
set	O
as	O
.	O

IBM	O
Watson	S-Method
and	O
Google	O
NLP	S-Task
both	O
offer	O
commercial	O
APIs	O
for	O
binary	O
sentiment	S-Task
analysis	O
,	O
producing	O
scalar	O
values	O
that	O
correspond	O
to	O
a	O
continuous	O
[	O
-	O
1	O
,+	O
1	O
]	O
sentiment	S-Task
score	O
.	O

We	O
applied	O
our	O
thresholding	B-Method
procedure	E-Method
to	O
these	O
scores	O
.	O

In	O
the	O
case	O
of	O
classification	S-Task
with	O
neutrals	O
we	O
create	O
two	O
thresholds	O
which	O
we	O
individually	O
optimized	O
jointly	O
over	O
as	O
well	O
.	O

With	O
the	O
finetuning	S-Method
procedure	O
,	O
we	O
found	O
success	O
with	O
a	O
decoder	S-Method
,	O
whose	O
two	O
output	O
units	O
are	O
probability	O
estimates	O
of	O
the	O
positive	O
and	O
negative	O
labels	O
.	O

These	O
units	O
both	O
have	O
sigmoid	O
activations	O
,	O
since	O
we	O
denote	O
a	O
neutral	O
as	O
.	O

To	O
threshold	O
these	O
predictions	O
,	O
we	O
searched	O
the	O
cartesian	O
product	O
to	O
determine	O
.	O

paragraph	O
:	O
Active	B-Method
Learning	E-Method
We	O
hypothesized	O
that	O
we	O
could	O
achieve	O
greater	O
precision	S-Metric
and	O
recall	S-Metric
on	O
our	O
datasets	O
if	O
our	O
class	O
label	O
were	O
more	O
equally	O
balanced	O
.	O

To	O
this	O
end	O
,	O
we	O
employed	O
an	O
active	B-Method
learning	I-Method
procedure	E-Method
to	O
select	O
unlabeled	O
tweets	O
to	O
be	O
labeled	O
.	O

The	O
algorithm	O
consisted	O
of	O
first	O
finetuning	S-Method
a	O
language	B-Method
model	E-Method
on	O
labeled	O
tweets	O
for	O
5	O
epochs	O
.	O

At	O
peak	O
validation	B-Metric
accuracy	E-Metric
,	O
we	O
obtain	O
predictions	O
,	O
for	O
Plutchik	O
sentiment	S-Task
on	O
the	O
unlabeled	O
tweets	O
.	O

From	O
the	O
labeled	O
dataset	O
,	O
we	O
calculate	O
the	O
negative	O
class	O
percentage	O
for	O
each	O
category	O
.	O

Then	O
we	O
obtain	O
category	O
a	O
weighting	O
parameter	O
so	O
that	O
for	O
Then	O
,	O
we	O
get	O
scores	O
for	O
each	O
unlabeled	O
point	O
as	O
weighted	O
features	O
:	O
.	O

This	O
way	O
,	O
positive	O
predictions	O
for	O
sentiment	S-Task
categories	O
are	O
weighted	O
by	O
how	O
much	O
they	O
would	O
contribute	O
towards	O
balancing	O
all	O
of	O
the	O
class	O
distributions	O
.	O

The	O
scores	O
are	O
used	O
as	O
weights	O
in	O
a	O
weighted	B-Method
uniform	I-Method
random	I-Method
sampler	E-Method
,	O
and	O
from	O
this	O
,	O
we	O
sampled	O
5	O
,	O
000	O
tweets	O
to	O
be	O
labeled	O
.	O

We	O
found	O
that	O
overall	O
,	O
the	O
method	O
produced	O
tweets	O
with	O
more	O
emotion	O
.	O

Not	O
only	O
was	O
the	O
positive	O
class	O
balance	O
averaged	O
across	O
label	O
categories	O
higher	O
(	O
11.2	O
%	O
compared	O
to	O
8.2	O
%	O
for	O
random	B-Method
sampling	E-Method
)	O
,	O
but	O
the	O
percentage	O
of	O
tweets	O
which	O
had	O
no	O
emotion	O
was	O
dramatically	O
lower	O
:	O
35.6	O
%	O
compared	O
to	O
52.1	O
%	O
for	O
random	B-Method
sampling	E-Method
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

We	O
hence	O
achieved	O
better	O
class	O
balance	O
than	O
the	O
dataset	O
prior	O
to	O
the	O
augmentation	S-Task
.	O

SST	S-Material
(	O
acc	O
)	O
Company	O
-/	O
=	O
/+	O
Transformer	S-Method
(	O
finetune	O
)	O
90.9%81.2%88.2	O
/	O
73.5	O
/	O
81.9mLSTM	O
(	O
finetune	O
)	O
90.4%78.2%87.0	O
/	O
69.3	O
/	O
78.38k	O
mLSTM	S-Method
[]	O
93.8%77.3%86.0	O
/	O
67.4	O
/	O
78.6	O
[]	O
93.1%	O
--	O
ELMo	O
(	O
finetuned	O
)	O
79.9%71.4%81.7	O
/	O
60.1	O
/	O
72.4ELMo	O
+	O
BiLSTM	O
+	O
Attn	O
[	O
]	O
91.6%	O
--	O
Watson	S-Method
API84.4%56.7%42.9	O
/	O
54.0	O
/	O
73.3Google	B-Method
Sentiment	E-Method
(	O
GCL	S-Method
)	O
API81.3%62.5%69.6	O
/	O
54.0	O
/	O
63.8Class	O
Balance50.0	O
/	O
50.0	O
-	O
22.4	O
/	O
46.0	O
/	O
31.6	O
AccuracyMicro	O
F1Macro	O
F1	O
(	O
Jaccard	O
)	O
Transformer	S-Method
(	O
ours	O
)	O
0.5770.6900.561	O
[]	O
0.5950.7090.542	O
[]	O
0.5820.6940.534	O
AngerAnticipationDisgustFearJoySadnessSurpriseTrustAverageCompanyTransformer	O
(	O
MH	O
)	O
.684.486.441.400.634.333.269.300.443Transformer	O
(	O
SH	O
)	O
.679.491.371.400.675.286.210.279.424mLSTM	O
(	O
SH	O
)	O
.636.426.319.232.609.260.201.284.371ELMo	O
(	O
MH	O
)	O
.515.306.325.086.489.182.161.182.281Watson.358	O
-	O
.179.086.520.096	O
-	O
--	O
SemevalTransformer	O
(	O
MH	O
)	O
.779.413.769.723.850.712.360.240.606Transformer	O
(	O
SH	O
)	O
.774.425.765.735.832.699.373.247.606mLSTM	O
(	O
SH	O
)	O
.668.189.691.535.763.557.103.000.438ELMo	O
(	O
MH	O
)	O
.506.215.351.172.540.348.164.239.317Watson.498	O
-	O
.331.149.684.359	O
-	O
--	O
section	O
:	O
Results	O
subsection	O
:	O
Binary	B-Material
Sentiment	I-Material
Tweets	E-Material
For	O
binary	O
sentiment	S-Task
,	O
we	O
compare	O
our	O
model	O
on	O
two	O
tasks	O
:	O
the	O
academic	B-Material
SST	I-Material
dataset	E-Material
,	O
which	O
consists	O
of	O
a	O
balanced	O
set	O
of	O
Positive	O
and	O
Negative	O
labels	O
,	O
and	O
the	O
company	O
tweets	O
dataset	O
,	O
which	O
consists	O
of	O
a	O
balance	O
between	O
Positive	O
,	O
Neutral	O
and	O
Negative	O
labels	O
.	O

See	O
Table	O
[	O
reference	O
]	O
.	O

While	O
the	O
Transformer	S-Method
gets	O
close	O
but	O
does	O
not	O
exceed	O
the	O
state	O
of	O
the	O
art	O
on	O
the	O
SST	B-Material
dataset	E-Material
,	O
it	O
exceeds	O
both	O
the	O
mLSTM	S-Method
and	O
ELMo	B-Method
baseline	E-Method
as	O
well	O
as	O
both	O
Watson	S-Method
and	O
Google	B-Method
Sentiment	I-Method
APIs	E-Method
on	O
the	O
company	O
tweets	O
.	O

This	O
is	O
despite	O
optimally	O
calibrating	O
the	O
API	O
results	O
on	O
the	O
test	O
set	O
.	O

subsection	O
:	O
Multi	O
-	O
Label	O
Emotion	O
Tweets	O
The	O
IBM	O
Watson	S-Method
API	O
offers	O
multi	O
-	O
label	B-Task
emotion	I-Task
predictions	E-Task
for	O
five	O
categories	O
:	O
Anger	O
,	O
Disgust	O
,	O
Fear	O
,	O
Joy	O
and	O
Sadness	O
.	O

We	O
compare	O
our	O
models	O
to	O
Watson	S-Method
on	O
these	O
categories	O
for	O
both	O
the	O
SemEval	S-Material
dataset	O
and	O
the	O
company	O
tweets	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
find	O
that	O
our	O
models	O
outperform	O
Watson	S-Method
on	O
every	O
emotion	O
category	O
.	O

paragraph	O
:	O
SemEval	S-Material
Tweets	O
We	O
submitted	O
our	O
finetuned	O
Transformer	S-Method
model	O
to	O
the	O
SemEval	S-Material
Task1:E	O
-	O
C	O
challenge	O
,	O
as	O
seen	O
in	O
Table	O
[	O
reference	O
]	O
.	O

These	O
results	O
were	O
computed	O
by	O
the	O
organizers	O
on	O
a	O
golden	O
test	O
set	O
,	O
for	O
which	O
we	O
do	O
not	O
have	O
access	O
to	O
the	O
truth	O
labels	O
.	O

Our	O
model	O
achieved	O
the	O
top	O
macro	S-Metric
-	O
averaged	O
F1	O
score	O
among	O
all	O
submission	O
,	O
with	O
competitive	O
but	O
lower	O
scores	O
for	O
the	O
micro	S-Metric
-	O
average	O
F1	O
an	O
the	O
Jaccard	B-Metric
Index	I-Metric
accuracy	E-Metric
.	O

This	O
suggests	O
that	O
our	O
model	O
out	O
-	O
performs	O
the	O
other	O
top	O
submission	O
on	O
rare	O
and	O
difficult	O
categories	O
,	O
since	O
macro	B-Metric
-	I-Metric
average	E-Metric
weighs	O
performance	O
on	O
all	O
classes	O
equally	O
,	O
and	O
the	O
most	O
common	O
categories	O
of	O
Joy	O
,	O
Anger	O
,	O
Disgust	O
and	O
Optimism	O
get	O
relatively	O
higher	O
F1	B-Metric
scores	E-Metric
across	O
all	O
models	O
.	O

We	O
also	O
compare	O
the	O
deep	B-Method
learning	I-Method
architectures	E-Method
of	O
the	O
Transformer	S-Method
and	O
mLSTM	S-Method
on	O
this	O
dataset	O
in	O
Table	O
[	O
reference	O
]	O
and	O
find	O
that	O
the	O
Transformer	S-Method
outperforms	O
the	O
mLSTM	S-Method
across	O
Plutchik	O
categories	O
.	O

The	O
winner	O
of	O
the	O
Task1:E	O
-	O
c	O
challenge	O
trained	O
a	O
bidirectional	B-Method
LSTM	E-Method
with	O
an	O
800	O
,	O
000	O
word	O
embedding	O
vocabulary	O
derived	O
from	O
training	O
word	O
vectors	O
on	O
a	O
dataset	O
of	O
550	O
million	O
tweets	O
.	O

Similarly	O
,	O
the	O
second	O
place	O
winner	O
of	O
the	O
SemEval	B-Material
leaderboard	E-Material
trained	O
a	O
word	B-Method
-	I-Method
level	I-Method
bidirectional	I-Method
LSTM	E-Method
with	O
attention	S-Method
,	O
as	O
well	O
as	O
including	O
non	O
-	O
deep	O
learning	O
features	O
into	O
their	O
ensemble	O
.	O

Both	O
submissions	O
used	O
training	O
data	O
across	O
SemEval	S-Material
tasks	O
,	O
as	O
well	O
as	O
additional	O
training	O
data	O
outside	O
of	O
the	O
training	O
set	O
.	O

In	O
comparison	O
,	O
we	O
demonstrate	O
that	O
finetuning	S-Method
can	O
be	O
as	O
effective	O
on	O
this	O
task	O
,	O
despite	O
training	O
only	O
on	O
7	O
,	O
000	O
tweets	O
.	O

Furthermore	O
,	O
out	B-Method
language	I-Method
modeling	E-Method
took	O
place	O
on	O
the	O
Amazon	O
Reviews	O
dataset	O
,	O
which	O
does	O
not	O
contain	O
emoji	O
,	O
hashtags	O
or	O
usernames	O
.	O

We	O
would	O
expect	O
to	O
see	O
improvements	O
if	O
our	O
unsupervised	O
dataset	O
contained	O
emoji	O
,	O
for	O
example	O
.	O

paragraph	O
:	O
Plutchik	O
on	O
Company	O
Tweets	O
Our	O
models	O
gets	O
lower	O
F1	B-Metric
scores	E-Metric
on	O
the	O
company	O
tweets	O
dataset	O
than	O
on	O
equivalent	O
SemEval	S-Material
categories	O
.	O

As	O
with	O
the	O
SemEval	S-Material
challenge	O
tweets	O
,	O
the	O
Transformer	S-Method
outperformed	O
the	O
mLSTM	S-Method
.	O

These	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Both	O
models	O
performed	O
significantly	O
better	O
than	O
the	O
Watson	S-Method
API	O
on	O
all	O
categories	O
for	O
which	O
Watson	S-Method
supplies	O
predictions	O
.	O

We	O
could	O
not	O
conclusively	O
determine	O
whether	O
the	O
singlehead	O
or	O
the	O
multihead	O
Transformer	S-Method
will	O
perform	O
better	O
on	O
a	O
given	O
task	O
.	O

Thus	O
we	O
recommend	O
trying	O
both	O
methods	O
on	O
a	O
new	O
dataset	O
.	O

section	O
:	O
Analysis	O
paragraph	O
:	O
Classification	S-Task
Performance	O
by	O
Dataset	O
Size	O
We	O
would	O
have	O
liked	O
to	O
label	O
more	O
data	O
for	O
the	O
company	O
tweets	O
dataset	O
,	O
and	O
thus	O
looked	O
into	O
how	O
much	O
extra	O
labeling	O
contributes	O
to	O
finetuned	O
model	B-Metric
performance	I-Metric
accuracy	E-Metric
.	O

First	O
,	O
let	O
us	O
explain	O
the	O
difference	O
between	O
micro	S-Metric
and	O
macro	B-Metric
averaging	I-Metric
of	I-Metric
the	I-Metric
F1	I-Metric
scores	E-Metric
.	O

We	O
can	O
summarize	O
the	O
F1	B-Metric
scores	E-Metric
of	O
categories	O
(	O
or	O
any	O
other	O
metric	O
)	O
through	O
macro	B-Metric
and	I-Metric
micro	I-Metric
averaging	E-Metric
to	O
obtain	O
.	O

The	O
macro	S-Metric
method	O
weights	O
each	O
class	O
equally	O
by	O
averaging	O
the	O
metric	O
calculated	O
on	O
each	O
individual	O
class	O
.	O

The	O
micro	S-Metric
method	O
accounts	O
for	O
the	O
class	O
imbalances	O
in	O
each	O
category	O
by	O
aggregating	O
all	O
of	O
the	O
true	O
/	O
false	O
positives	O
/	O
negatives	O
first	O
,	O
and	O
then	O
calculating	O
an	O
overall	B-Metric
metric	E-Metric
.	O

In	O
one	O
experiment	O
,	O
we	O
decreased	O
the	O
size	O
of	O
the	O
training	O
dataset	O
and	O
observed	O
the	O
resulting	O
macro	S-Metric
and	O
micro	B-Metric
averaged	I-Metric
F1	I-Metric
scores	E-Metric
across	O
all	O
categories	O
on	O
company	O
tweets	O
.	O

The	O
results	O
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

We	O
observe	O
that	O
the	O
macro	S-Metric
average	O
is	O
more	O
sensitive	O
to	O
dataset	O
size	O
and	O
falls	O
more	O
quickly	O
than	O
the	O
micro	S-Metric
average	O
.	O

The	O
interpretation	O
of	O
this	O
is	O
that	O
categories	O
with	O
worse	O
class	O
imbalance	O
(	O
which	O
consequently	O
influence	O
macro	S-Metric
more	O
than	O
micro	S-Metric
average	O
)	O
benefit	O
more	O
from	O
having	O
a	O
larger	O
training	O
dataset	O
size	O
.	O

This	O
suggests	O
that	O
we	O
may	O
obtain	O
substantially	O
better	O
results	O
with	O
more	O
data	O
in	O
the	O
harder	O
categories	O
.	O

We	O
conducted	O
a	O
related	O
experiment	O
that	O
focused	O
on	O
the	O
difference	O
in	O
category	B-Metric
performance	E-Metric
when	O
using	O
a	O
single	O
head	O
versus	O
a	O
multihead	B-Method
decoder	E-Method
.	O

We	O
apply	O
the	O
two	O
architectures	O
at	O
different	O
training	O
dataset	O
sizes	O
for	O
three	O
different	O
label	O
categories	O
:	O
Anger	O
,	O
Anticipation	O
and	O
Trust	O
,	O
which	O
we	O
categorize	O
as	O
low	O
,	O
medium	O
and	O
high	O
difficulty	O
,	O
respectively	O
.	O

As	O
seen	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
it	O
appears	O
that	O
the	O
difference	O
between	O
the	O
single	O
and	O
multihead	O
becomes	O
more	O
pronounced	O
for	O
more	O
difficult	O
categories	O
,	O
as	O
well	O
as	O
for	O
smaller	O
dataset	O
sizes	O
.	O

We	O
do	O
not	O
have	O
enough	O
data	O
to	O
make	O
a	O
firm	O
conclusion	O
,	O
but	O
this	O
study	O
suggests	O
that	O
we	O
could	O
get	O
more	O
out	O
of	O
the	O
labeled	O
data	O
that	O
we	O
have	O
,	O
by	O
studying	O
which	O
categories	O
benefit	O
from	O
single	B-Method
head	I-Method
and	I-Method
multihead	I-Method
decoders	E-Method
.	O

All	O
categories	O
benefit	O
from	O
more	O
training	O
data	O
,	O
but	O
some	O
categories	O
benefit	O
from	O
from	O
marginal	O
labeled	O
data	O
than	O
others	O
.	O

This	O
suggests	O
further	O
and	O
more	O
rigorous	O
study	O
of	O
the	O
boostrapping	B-Method
methods	E-Method
we	O
used	O
to	O
select	O
tweets	O
for	O
our	O
human	B-Task
labeling	I-Task
budget	E-Task
,	O
as	O
described	O
in	O
the	O
Active	B-Task
Learning	E-Task
section	O
.	O

[	O
t	O
]	O
0.263	O
[	O
t	O
]	O
0.717	O
paragraph	O
:	O
Dataset	B-Metric
Quality	E-Metric
and	O
Human	B-Metric
Rater	I-Metric
Agreement	E-Metric
The	O
SemEval	S-Material
dataset	O
applies	O
a	O
positive	O
label	O
for	O
every	O
category	O
where	O
2	O
out	O
of	O
7	O
vetted	O
raters	O
agree	O
.	O

The	O
reason	O
is	O
for	O
the	O
dataset	O
to	O
contain	O
difficult	O
and	O
subtle	O
examples	O
of	O
sentiments	O
,	O
not	O
just	O
those	O
examples	O
where	O
everyone	O
agrees	O
.	O

The	O
raters	O
also	O
have	O
a	O
tendency	O
to	O
under	O
-	O
label	O
categories	O
,	O
especially	O
when	O
presented	O
multiple	O
options	O
.	O

Following	O
a	O
similar	O
process	O
,	O
we	O
required	O
2	O
out	O
of	O
5	O
raters	O
for	O
a	O
positive	O
label	O
,	O
and	O
in	O
the	O
case	O
of	O
binary	O
sentiment	S-Task
labels	O
(	O
Positive	O
,	O
Neutral	O
,	O
Negative	O
)	O
,	O
we	O
rounded	O
toward	O
polarized	O
sentiment	S-Task
and	O
away	O
from	O
Neutral	O
labels	O
in	O
the	O
case	O
of	O
a	O
2	O
/	O
3	O
split	O
.	O

Applying	O
the	O
SemEval	S-Material
-	O
trained	O
Transformer	S-Method
directly	O
to	O
our	O
company	O
tweets	O
dataset	O
gets	O
reasonably	O
good	O
results	O
(	O
0.338	O
macro	S-Metric
average	O
)	O
,	O
also	O
validating	O
that	O
our	O
labeling	B-Method
technique	E-Method
is	O
similar	O
to	O
that	O
of	O
SemEval	S-Material
.	O

Looking	O
at	O
rater	B-Metric
agreement	E-Metric
by	O
dataset	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
see	O
that	O
Plutchik	O
category	O
labels	O
contain	O
large	O
rater	O
disagreement	O
,	O
even	O
among	O
vetted	O
raters	O
who	O
passed	O
the	O
golden	O
set	O
test	O
.	O

Furthermore	O
,	O
datasets	O
with	O
more	O
emotions	O
(	O
the	O
SemEval	S-Material
dataset	O
and	O
our	O
active	O
learning	O
sampled	O
company	O
tweets	O
)	O
contain	O
higher	O
Plutchik	O
disagreement	O
than	O
random	O
company	O
tweets	O
.	O

This	O
is	O
likely	O
because	O
raters	O
tend	O
to	O
apply	O
the	O
”	O
No	O
Emotion	O
”	O
label	O
when	O
they	O
are	O
not	O
sure	O
about	O
a	O
category	O
.	O

As	O
Table	O
[	O
reference	O
]	O
shows	O
,	O
the	O
SemEval	S-Material
and	O
active	O
company	O
tweets	O
datasets	O
contain	O
fewer	O
no	O
-	O
emotion	O
tweets	O
than	O
other	O
datsets	O
.	O

It	O
would	O
be	O
interesting	O
to	O
analyze	O
rater	O
disagreement	O
by	O
category	O
,	O
how	O
much	O
this	O
effects	O
classifier	B-Task
convergence	E-Task
,	O
whether	O
getting	O
7	O
+	O
ratings	O
per	O
tweet	O
helps	O
classifier	B-Task
convergence	E-Task
,	O
and	O
also	O
whether	O
this	O
work	O
could	O
benefit	O
from	O
estimating	O
rater	B-Metric
quality	E-Metric
via	O
agreement	O
with	O
the	O
crowd	O
,	O
as	O
proposed	O
in	O
.	O

However	O
this	O
analysis	O
is	O
not	O
straightforward	O
,	O
as	O
the	O
truth	O
data	O
is	O
itself	O
collected	O
through	O
human	B-Task
labeling	E-Task
.	O

Alongside	O
classifier	B-Metric
convergence	E-Metric
by	O
dataset	O
size	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
think	O
that	O
this	O
could	O
be	O
an	O
interesting	O
area	O
a	O
future	O
research	O
.	O

paragraph	O
:	O
Difficult	O
tweets	O
and	O
challenging	O
contexts	O
.	O

There	O
is	O
not	O
sufficient	O
space	O
for	O
a	O
thorough	O
analysis	O
,	O
but	O
we	O
wanted	O
to	O
suggest	O
why	O
general	B-Method
purpose	I-Method
APIs	E-Method
may	O
not	O
work	O
well	O
on	O
our	O
company	O
tweets	O
dataset	O
.	O

Table	O
[	O
reference	O
]	O
samples	O
the	O
largest	O
binary	O
sentiment	S-Task
disagreements	O
between	O
human	O
raters	O
and	O
the	O
Watson	S-Method
API	O
.	O

For	O
simplicity	O
,	O
we	O
restrict	O
examples	O
to	O
video	O
game	O
tweets	O
,	O
which	O
comprise	O
19.1	O
%	O
of	O
our	O
test	O
set	O
.	O

As	O
we	O
can	O
see	O
,	O
all	O
of	O
these	O
examples	O
appear	O
to	O
ascribe	O
negative	O
emotion	O
to	O
generally	O
negative	O
terms	O
which	O
,	O
in	O
a	O
video	O
game	O
context	O
,	O
do	O
not	O
indicate	O
negative	O
sentiment	S-Task
.	O

Our	O
purpose	O
is	O
not	O
to	O
castigate	O
the	O
Watson	S-Method
or	O
the	O
GCL	O
APIs	O
.	O

Rather	O
,	O
we	O
propose	O
that	O
it	O
may	O
not	O
be	O
possible	O
to	O
provide	O
context	O
-	O
independent	O
emotion	B-Method
classification	I-Method
scores	E-Method
that	O
work	O
well	O
across	O
text	O
contexts	O
.	O

It	O
may	O
work	O
better	O
in	O
practice	O
,	O
on	O
some	O
tasks	O
,	O
to	O
train	O
a	O
large	O
unsupervised	B-Method
model	E-Method
and	O
to	O
use	O
a	O
small	O
amount	O
of	O
labeled	O
data	O
to	O
finetune	O
on	O
the	O
context	O
present	O
in	O
the	O
specific	O
dataset	O
.	O

We	O
would	O
like	O
to	O
quantify	O
this	O
further	O
in	O
future	O
work	O
.	O

Recent	O
work	O
shows	O
that	O
training	O
an	O
RNN	S-Method
with	O
multiple	O
softmax	O
outputs	O
leads	O
to	O
a	O
much	O
improved	O
BPC	S-Method
on	O
language	B-Task
modeling	E-Task
,	O
especially	O
for	O
diverse	O
datasets	O
and	O
models	O
with	O
large	O
vocabularies	O
.	O

This	O
is	O
because	O
the	O
multiple	O
softmaxes	S-Method
are	O
able	O
to	O
capture	O
a	O
larger	O
number	O
of	O
distinct	O
contexts	O
in	O
the	O
text	O
than	O
a	O
single	O
output	O
.	O

Perhaps	O
our	O
Transformer	S-Method
also	O
captures	O
the	O
features	O
relevant	O
to	O
a	O
large	O
number	O
of	O
distinct	O
contexts	O
,	O
and	O
the	O
finetuning	S-Method
is	O
able	O
to	O
select	O
the	O
most	O
significant	O
of	O
these	O
features	O
,	O
while	O
ignoring	O
those	O
features	O
that	O
–	O
while	O
adding	O
value	O
in	O
general	O
–	O
are	O
not	O
appropriate	O
in	O
a	O
video	B-Task
game	I-Task
setting	E-Task
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
demonstrate	O
that	O
unsupervised	B-Task
pretraining	E-Task
and	O
finetuning	S-Method
provides	O
a	O
flexible	O
framework	O
that	O
is	O
effective	O
for	O
difficult	O
text	B-Task
classification	I-Task
tasks	E-Task
.	O

We	O
noticed	O
that	O
the	O
finetuning	S-Method
was	O
especially	O
effective	O
with	O
the	O
Transformer	S-Method
network	O
,	O
when	O
transferring	O
to	O
downstream	B-Task
tasks	E-Task
with	O
noisy	O
labels	O
and	O
specialized	O
context	O
.	O

We	O
think	O
that	O
this	O
framework	O
makes	O
it	O
easy	O
to	O
customize	O
a	O
text	B-Method
classification	I-Method
model	E-Method
on	O
niche	B-Task
tasks	E-Task
.	O

Unsupervised	B-Method
language	I-Method
modeling	E-Method
can	O
be	O
done	O
on	O
general	O
text	O
datasets	O
,	O
and	O
requires	O
no	O
labels	O
.	O

Meanwhile	O
downstream	B-Task
task	I-Task
transfer	E-Task
works	O
well	O
enough	O
,	O
even	O
on	O
small	O
amounts	O
of	O
domain	O
-	O
specific	O
labelled	O
data	O
,	O
to	O
be	O
accessible	O
to	O
most	O
academics	O
and	O
small	O
organization	O
.	O

It	O
would	O
be	O
great	O
to	O
see	O
this	O
approach	O
applied	O
to	O
a	O
variety	O
of	O
practical	O
text	B-Task
classification	I-Task
problems	E-Task
,	O
much	O
as	O
and	O
have	O
applied	O
language	B-Method
modeling	E-Method
and	O
transfer	S-Task
to	O
a	O
variety	O
of	O
academic	B-Task
text	I-Task
understanding	I-Task
problems	E-Task
on	O
the	O
GLUE	O
Benchmark	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
A	O
C	B-Method
-	I-Method
LSTM	I-Method
Neural	I-Method
Network	E-Method
for	O
Text	B-Task
Classification	E-Task
Neural	B-Method
network	I-Method
models	E-Method
have	O
been	O
demonstrated	O
to	O
be	O
capable	O
of	O
achieving	O
remarkable	O
performance	O
in	O
sentence	B-Task
and	I-Task
document	I-Task
modeling	E-Task
.	O

Convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
and	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
are	O
two	O
mainstream	O
architectures	O
for	O
such	O
modeling	B-Task
tasks	E-Task
,	O
which	O
adopt	O
totally	O
different	O
ways	O
of	O
understanding	O
natural	O
languages	O
.	O

In	O
this	O
work	O
,	O
we	O
combine	O
the	O
strengths	O
of	O
both	O
architectures	O
and	O
propose	O
a	O
novel	O
and	O
unified	B-Method
model	E-Method
called	O
C	O
-	O
LSTM	S-Method
for	O
sentence	B-Task
representation	E-Task
and	O
text	B-Task
classification	E-Task
.	O

C	O
-	O
LSTM	S-Method
utilizes	O
CNN	S-Method
to	O
extract	O
a	O
sequence	O
of	O
higher	B-Method
-	I-Method
level	I-Method
phrase	I-Method
representations	E-Method
,	O
and	O
are	O
fed	O
into	O
a	O
long	O
short	O
-	O
term	O
memory	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
LSTM	S-Method
)	O
to	O
obtain	O
the	O
sentence	B-Method
representation	E-Method
.	O

C	O
-	O
LSTM	S-Method
is	O
able	O
to	O
capture	O
both	O
local	O
features	O
of	O
phrases	O
as	O
well	O
as	O
global	O
and	O
temporal	O
sentence	O
semantics	O
.	O

We	O
evaluate	O
the	O
proposed	O
architecture	O
on	O
sentiment	B-Task
classification	E-Task
and	O
question	B-Task
classification	I-Task
tasks	E-Task
.	O

The	O
experimental	O
results	O
show	O
that	O
the	O
C	O
-	O
LSTM	S-Method
outperforms	O
both	O
CNN	S-Method
and	O
LSTM	S-Method
and	O
can	O
achieve	O
excellent	O
performance	O
on	O
these	O
tasks	O
.	O

section	O
:	O
Introduction	O
As	O
one	O
of	O
the	O
core	O
steps	O
in	O
NLP	S-Task
,	O
sentence	B-Task
modeling	E-Task
aims	O
at	O
representing	O
sentences	O
as	O
meaningful	O
features	O
for	O
tasks	O
such	O
as	O
sentiment	B-Task
classification	E-Task
.	O

Traditional	O
sentence	B-Method
modeling	E-Method
uses	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
words	I-Method
model	E-Method
which	O
often	O
suffers	O
from	O
the	O
curse	O
of	O
dimensionality	O
;	O
others	O
use	O
composition	B-Method
based	I-Method
methods	E-Method
instead	O
,	O
e.g.	O
,	O
an	O
algebraic	B-Method
operation	E-Method
over	O
semantic	O
word	O
vectors	O
to	O
produce	O
the	O
semantic	O
sentence	O
vector	O
.	O

However	O
,	O
such	O
methods	O
may	O
not	O
perform	O
well	O
due	O
to	O
the	O
loss	O
of	O
word	O
order	O
information	O
.	O

More	O
recent	O
models	O
for	O
distributed	B-Task
sentence	I-Task
representation	E-Task
fall	O
into	O
two	O
categories	O
according	O
to	O
the	O
form	O
of	O
input	O
sentence	O
:	O
sequence	B-Method
-	I-Method
based	I-Method
models	E-Method
and	O
tree	B-Method
-	I-Method
structured	I-Method
models	E-Method
.	O

Sequence	B-Method
-	I-Method
based	I-Method
models	E-Method
construct	O
sentence	B-Method
representations	E-Method
from	O
word	O
sequences	O
by	O
taking	O
in	O
account	O
the	O
relationship	O
between	O
successive	O
words	O
.	O

Tree	B-Method
-	I-Method
structured	I-Method
models	E-Method
treat	O
each	O
word	O
token	O
as	O
a	O
node	O
in	O
a	O
syntactic	O
parse	O
tree	O
and	O
learn	O
sentence	B-Method
representations	E-Method
from	O
leaves	O
to	O
the	O
root	O
in	O
a	O
recursive	O
manner	O
.	O

Convolutional	B-Method
neural	I-Method
networks	E-Method
(	O
CNNs	S-Method
)	O
and	O
recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
have	O
emerged	O
as	O
two	O
widely	O
used	O
architectures	O
and	O
are	O
often	O
combined	O
with	O
sequence	B-Method
-	I-Method
based	I-Method
or	I-Method
tree	I-Method
-	I-Method
structured	I-Method
models	E-Method
.	O

Owing	O
to	O
the	O
capability	O
of	O
capturing	O
local	O
correlations	O
of	O
spatial	O
or	O
temporal	O
structures	O
,	O
CNNs	S-Method
have	O
achieved	O
top	O
performance	O
in	O
computer	B-Task
vision	E-Task
,	O
speech	B-Task
recognition	E-Task
and	O
NLP	S-Task
.	O

For	O
sentence	B-Task
modeling	E-Task
,	O
CNNs	S-Method
perform	O
excellently	O
in	O
extracting	O
n	O
-	O
gram	O
features	O
at	O
different	O
positions	O
of	O
a	O
sentence	O
through	O
convolutional	B-Method
filters	E-Method
,	O
and	O
can	O
learn	O
short	O
and	O
long	O
-	O
range	O
relations	O
through	O
pooling	B-Method
operations	E-Method
.	O

CNNs	S-Method
have	O
been	O
successfully	O
combined	O
with	O
both	O
sequence	B-Method
-	I-Method
based	I-Method
model	E-Method
and	O
tree	B-Method
-	I-Method
structured	I-Method
model	E-Method
in	O
sentence	B-Task
modeling	E-Task
.	O

The	O
other	O
popular	O
neural	B-Method
network	I-Method
architecture	E-Method
–	O
RNN	S-Method
–	O
is	O
able	O
to	O
handle	O
sequences	O
of	O
any	O
length	O
and	O
capture	O
long	O
-	O
term	O
dependencies	O
.	O

To	O
avoid	O
the	O
problem	O
of	O
gradient	B-Task
exploding	I-Task
or	I-Task
vanishing	E-Task
in	O
the	O
standard	O
RNN	S-Method
,	O
Long	O
Short	O
-	O
term	O
Memory	O
RNN	S-Method
(	O
LSTM	S-Method
)	O
and	O
other	O
variants	O
were	O
designed	O
for	O
better	O
remembering	B-Task
and	I-Task
memory	I-Task
accesses	E-Task
.	O

Along	O
with	O
the	O
sequence	B-Method
-	I-Method
based	E-Method
or	O
the	O
tree	B-Method
-	I-Method
structured	I-Method
models	E-Method
,	O
RNNs	S-Method
have	O
achieved	O
remarkable	O
results	O
in	O
sentence	B-Task
or	I-Task
document	I-Task
modeling	E-Task
.	O

To	O
conclude	O
,	O
CNN	S-Method
is	O
able	O
to	O
learn	O
local	O
response	O
from	O
temporal	O
or	O
spatial	O
data	O
but	O
lacks	O
the	O
ability	O
of	O
learning	O
sequential	O
correlations	O
;	O
on	O
the	O
other	O
hand	O
,	O
RNN	S-Method
is	O
specilized	O
for	O
sequential	B-Task
modelling	E-Task
but	O
unable	O
to	O
extract	O
features	O
in	O
a	O
parallel	O
way	O
.	O

It	O
has	O
been	O
shown	O
that	O
higher	O
-	O
level	O
modeling	O
of	O
can	O
help	O
to	O
disentangle	O
underlying	O
factors	O
of	O
variation	O
within	O
the	O
input	O
,	O
which	O
should	O
then	O
make	O
it	O
easier	O
to	O
learn	O
temporal	O
structure	O
between	O
successive	O
time	O
steps	O
.	O

For	O
example	O
,	O
Sainath	O
et	O
al	O
.	O

have	O
obtained	O
respectable	O
improvements	O
in	O
WER	S-Metric
by	O
learning	O
a	O
deep	B-Method
LSTM	E-Method
from	O
multi	O
-	O
scale	O
inputs	O
.	O

We	O
explore	O
training	O
the	O
LSTM	B-Method
model	E-Method
directly	O
from	O
sequences	O
of	O
higher	B-Method
-	I-Method
level	I-Method
representaions	E-Method
while	O
preserving	O
the	O
sequence	O
order	O
of	O
these	O
representaions	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
architecture	O
short	O
for	O
C	O
-	O
LSTM	S-Method
by	O
combining	O
CNN	S-Method
and	O
LSTM	S-Method
to	O
model	O
sentences	O
.	O

To	O
benefit	O
from	O
the	O
advantages	O
of	O
both	O
CNN	S-Method
and	O
RNN	S-Method
,	O
we	O
design	O
a	O
simple	O
end	O
-	O
to	O
-	O
end	O
,	O
unified	B-Method
architecture	E-Method
by	O
feeding	O
the	O
output	O
of	O
a	O
one	O
-	O
layer	O
CNN	S-Method
into	O
LSTM	S-Method
.	O

The	O
CNN	S-Method
is	O
constructed	O
on	O
top	O
of	O
the	O
pre	O
-	O
trained	O
word	O
vectors	O
from	O
massive	O
unlabeled	O
text	O
data	O
to	O
learn	O
higher	O
-	O
level	B-Method
representions	I-Method
of	I-Method
n	I-Method
-	I-Method
grams	E-Method
.	O

Then	O
to	O
learn	O
sequential	O
correlations	O
from	O
higher	O
-	O
level	O
suqence	O
representations	O
,	O
the	O
feature	O
maps	O
of	O
CNN	S-Method
are	O
organized	O
as	O
sequential	O
window	O
features	O
to	O
serve	O
as	O
the	O
input	O
of	O
LSTM	S-Method
.	O

In	O
this	O
way	O
,	O
instead	O
of	O
constructing	O
LSTM	S-Method
directly	O
from	O
the	O
input	O
sentence	O
,	O
we	O
first	O
transform	O
each	O
sentence	O
into	O
successive	O
window	O
(	O
n	O
-	O
gram	O
)	O
features	O
to	O
help	O
disentangle	O
factors	O
of	O
variations	O
within	O
sentences	O
.	O

We	O
choose	O
sequence	O
-	O
based	O
input	O
other	O
than	O
relying	O
on	O
the	O
syntactic	O
parse	O
trees	O
before	O
feeding	O
in	O
the	O
neural	B-Method
network	E-Method
,	O
thus	O
our	O
model	O
does	O
n’t	O
rely	O
on	O
any	O
external	O
language	O
knowledge	O
and	O
complicated	O
pre	B-Method
-	I-Method
processing	E-Method
.	O

In	O
our	O
experiments	O
,	O
we	O
evaluate	O
the	O
semantic	B-Method
sentence	I-Method
representations	E-Method
learned	O
from	O
C	O
-	O
LSTM	S-Method
with	O
two	O
tasks	O
:	O
sentiment	B-Task
classification	E-Task
and	O
6	B-Task
-	I-Task
way	I-Task
question	I-Task
classification	E-Task
.	O

Our	O
evaluations	O
show	O
that	O
the	O
C	O
-	O
LSTM	S-Method
model	O
can	O
achieve	O
excellent	O
results	O
with	O
several	O
benchmarks	O
as	O
compared	O
with	O
a	O
wide	O
range	O
of	O
baseline	O
models	O
.	O

We	O
also	O
show	O
that	O
the	O
combination	O
of	O
CNN	S-Method
and	O
LSTM	S-Method
outperforms	O
individual	O
multi	O
-	O
layer	O
CNN	S-Method
models	O
and	O
RNN	S-Method
models	O
,	O
which	O
indicates	O
that	O
LSTM	S-Method
can	O
learn	O
long	O
-	O
term	O
dependencies	O
from	O
sequences	O
of	O
higher	O
-	O
level	O
representations	O
better	O
than	O
the	O
other	O
models	O
.	O

section	O
:	O
Related	O
Work	O
Deep	B-Method
learning	I-Method
based	I-Method
neural	I-Method
network	I-Method
models	E-Method
have	O
achieved	O
great	O
success	O
in	O
many	O
NLP	B-Task
tasks	E-Task
,	O
including	O
learning	B-Task
distributed	I-Task
word	I-Task
,	I-Task
sentence	I-Task
and	I-Task
document	I-Task
representation	E-Task
,	O
parsing	S-Task
,	O
statistical	B-Task
machine	I-Task
translation	E-Task
,	O
sentiment	B-Task
classification	E-Task
,	O
etc	O
.	O

Learning	O
distributed	B-Method
sentence	I-Method
representation	E-Method
through	O
neural	B-Method
network	I-Method
models	E-Method
requires	O
little	O
external	O
domain	O
knowledge	O
and	O
can	O
reach	O
satisfactory	O
results	O
in	O
related	O
tasks	O
like	O
sentiment	B-Task
classification	E-Task
,	O
text	B-Task
categorization	E-Task
.	O

In	O
many	O
recent	O
sentence	B-Task
representation	I-Task
learning	E-Task
works	O
,	O
neural	B-Method
network	I-Method
models	E-Method
are	O
constructed	O
upon	O
either	O
the	O
input	O
word	O
sequences	O
or	O
the	O
transformed	O
syntactic	O
parse	O
tree	O
.	O

Among	O
them	O
,	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
and	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
are	O
two	O
popular	O
ones	O
.	O

The	O
capability	O
of	O
capturing	O
local	O
correlations	O
along	O
with	O
extracting	O
higher	O
-	O
level	O
correlations	O
through	O
pooling	S-Method
empowers	O
CNN	S-Method
to	O
model	O
sentences	O
naturally	O
from	O
consecutive	O
context	O
windows	O
.	O

In	O
,	O
Collobert	O
et	O
al	O
.	O

applied	O
convolutional	B-Method
filters	E-Method
to	O
successive	O
windows	O
for	O
a	O
given	O
sequence	O
to	O
extract	O
global	O
features	O
by	O
max	B-Method
-	I-Method
pooling	E-Method
.	O

As	O
a	O
slight	O
variant	O
,	O
Kim	O
et	O
al	O
.	O

kim	O
proposed	O
a	O
CNN	S-Method
architecture	O
with	O
multiple	O
filters	O
(	O
with	O
a	O
varying	O
window	O
size	O
)	O
and	O
two	O
‘	O
channels	O
’	O
of	O
word	O
vectors	O
.	O

To	O
capture	O
word	O
relations	O
of	O
varying	O
sizes	O
,	O
Kalchbrenner	O
et	O
al	O
.	O

dcnn	S-Method
proposed	O
a	O
dynamic	B-Method
k	I-Method
-	I-Method
max	I-Method
pooling	I-Method
mechanism	E-Method
.	O

In	O
a	O
more	O
recent	O
work	O
,	O
Tao	O
et	O
al	O
.	O

apply	O
tensor	B-Method
-	I-Method
based	I-Method
operations	E-Method
between	O
words	O
to	O
replace	O
linear	O
operations	O
on	O
concatenated	O
word	O
vectors	O
in	O
the	O
standard	O
convolutional	B-Method
layer	E-Method
and	O
explore	O
the	O
non	O
-	O
linear	O
interactions	O
between	O
nonconsective	O
n	O
-	O
grams	O
.	O

Mou	O
et	O
al	O
.	O

mou	S-Method
also	O
explores	O
convolutional	B-Method
models	E-Method
on	O
tree	O
-	O
structured	O
sentences	O
.	O

As	O
a	O
sequence	B-Method
model	E-Method
,	O
RNN	S-Method
is	O
able	O
to	O
deal	O
with	O
variable	O
-	O
length	O
input	O
sequences	O
and	O
discover	O
long	O
-	O
term	O
dependencies	O
.	O

Various	O
variants	O
of	O
RNN	S-Method
have	O
been	O
proposed	O
to	O
better	O
store	O
and	O
access	O
memories	O
.	O

With	O
the	O
ability	O
of	O
explicitly	O
modeling	O
time	O
-	O
series	O
data	O
,	O
RNNs	S-Method
are	O
being	O
increasingly	O
applied	O
to	O
sentence	B-Task
modeling	E-Task
.	O

For	O
example	O
,	O
Tai	O
et	O
al	O
.	O

tai2015	O
adjusted	O
the	O
standard	O
LSTM	S-Method
to	O
tree	O
-	O
structured	O
topologies	O
and	O
obtained	O
superior	O
results	O
over	O
a	O
sequential	O
LSTM	S-Method
on	O
related	O
tasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
stack	O
CNN	S-Method
and	O
LSTM	S-Method
in	O
a	O
unified	B-Method
architecture	E-Method
for	O
semantic	B-Task
sentence	I-Task
modeling	E-Task
.	O

The	O
combination	O
of	O
CNN	S-Method
and	O
LSTM	S-Method
can	O
be	O
seen	O
in	O
some	O
computer	B-Task
vision	I-Task
tasks	E-Task
like	O
image	B-Task
caption	E-Task
and	O
speech	B-Task
recognition	E-Task
.	O

Most	O
of	O
these	O
models	O
use	O
multi	B-Method
-	I-Method
layer	I-Method
CNNs	E-Method
and	O
train	O
CNNs	S-Method
and	O
RNNs	S-Method
separately	O
or	O
throw	O
the	O
output	O
of	O
a	O
fully	O
connected	O
layer	O
of	O
CNN	S-Method
into	O
RNN	S-Method
as	O
inputs	O
.	O

Our	O
approach	O
is	O
different	O
:	O
we	O
apply	O
CNN	S-Method
to	O
text	O
data	O
and	O
feed	O
consecutive	O
window	O
features	O
directly	O
to	O
LSTM	S-Method
,	O
and	O
so	O
our	O
architecture	O
enables	O
LSTM	S-Method
to	O
learn	O
long	O
-	O
range	O
dependencies	O
from	O
higher	O
-	O
order	O
sequential	O
features	O
.	O

In	O
,	O
the	O
authors	O
suggest	O
that	O
sequence	B-Method
-	I-Method
based	I-Method
models	E-Method
are	O
sufficient	O
to	O
capture	O
the	O
compositional	O
semantics	O
for	O
many	O
NLP	B-Task
tasks	E-Task
,	O
thus	O
in	O
this	O
work	O
the	O
CNN	S-Method
is	O
directly	O
built	O
upon	O
word	O
sequences	O
other	O
than	O
the	O
syntactic	O
parse	O
tree	O
.	O

Our	O
experiments	O
on	O
sentiment	B-Task
classification	E-Task
and	O
6	B-Task
-	I-Task
way	I-Task
question	I-Task
classification	I-Task
tasks	E-Task
clearly	O
demonstrate	O
the	O
superiority	O
of	O
our	O
model	O
over	O
single	B-Method
CNN	E-Method
or	O
LSTM	B-Method
model	E-Method
and	O
other	O
related	O
sequence	B-Method
-	I-Method
based	I-Method
models	E-Method
.	O

section	O
:	O
C	O
-	O
LSTM	S-Method
Model	O
The	O
architecture	O
of	O
the	O
C	O
-	O
LSTM	S-Method
model	O
is	O
shown	O
in	O
Figure	O
1	O
,	O
which	O
consists	O
of	O
two	O
main	O
components	O
:	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
and	O
long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	I-Method
network	E-Method
(	O
LSTM	S-Method
)	O
.	O

The	O
following	O
two	O
subsections	O
describe	O
how	O
we	O
apply	O
CNN	S-Method
to	O
extract	O
higher	O
-	O
level	O
sequences	O
of	O
word	O
features	O
and	O
LSTM	S-Method
to	O
capture	O
long	O
-	O
term	O
dependencies	O
over	O
window	O
feature	O
sequences	O
respectively	O
.	O

subsection	O
:	O
N	B-Task
-	I-Task
gram	I-Task
Feature	I-Task
Extraction	E-Task
through	O
Convolution	S-Method
The	O
one	B-Method
-	I-Method
dimensional	I-Method
convolution	E-Method
involves	O
a	O
filter	B-Method
vector	E-Method
sliding	O
over	O
a	O
sequence	O
and	O
detecting	O
features	O
at	O
different	O
positions	O
.	O

Let	O
be	O
the	O
-	O
dimensional	O
word	O
vectors	O
for	O
the	O
-	O
th	O
word	O
in	O
a	O
sentence	O
.	O

Let	O
denote	O
the	O
input	O
sentence	O
where	O
is	O
the	O
length	O
of	O
the	O
sentence	O
.	O

Let	O
be	O
the	O
length	O
of	O
the	O
filter	O
,	O
and	O
the	O
vector	O
is	O
a	O
filter	S-Method
for	O
the	O
convolution	B-Method
operation	E-Method
.	O

For	O
each	O
position	O
in	O
the	O
sentence	O
,	O
we	O
have	O
a	O
window	O
vector	O
with	O
consecutive	O
word	O
vectors	O
,	O
denoted	O
as	O
:	O
Here	O
,	O
the	O
commas	O
represent	O
row	B-Method
vector	I-Method
concatenation	E-Method
.	O

A	O
filter	S-Method
convolves	O
with	O
the	O
window	O
vectors	O
(	O
-	O
grams	O
)	O
at	O
each	O
position	O
in	O
a	O
valid	O
way	O
to	O
generate	O
a	O
feature	O
map	O
;	O
each	O
element	O
of	O
the	O
feature	O
map	O
for	O
window	O
vector	O
is	O
produced	O
as	O
follows	O
:	O
where	O
is	O
element	O
-	O
wise	O
multiplication	O
,	O
is	O
a	O
bias	O
term	O
and	O
is	O
a	O
nonlinear	O
transformation	O
function	O
that	O
can	O
be	O
sigmoid	O
,	O
hyperbolic	O
tangent	O
,	O
etc	O
.	O

In	O
our	O
case	O
,	O
we	O
choose	O
ReLU	S-Method
as	O
the	O
nonlinear	O
function	O
.	O

The	O
C	O
-	O
LSTM	S-Method
model	O
uses	O
multiple	O
filters	S-Method
to	O
generate	O
multiple	O
feature	O
maps	O
.	O

For	O
filters	O
with	O
the	O
same	O
length	O
,	O
the	O
generated	O
feature	O
maps	O
can	O
be	O
rearranged	O
as	O
feature	B-Method
representations	E-Method
for	O
each	O
window	O
,	O
Here	O
,	O
semicolons	O
represent	O
column	O
vector	O
concatenation	O
and	O
is	O
the	O
feature	O
map	O
generated	O
with	O
the	O
-	B-Method
th	I-Method
filter	E-Method
.	O

Each	O
row	O
of	O
is	O
the	O
new	O
feature	B-Method
representation	E-Method
generated	O
from	O
filters	S-Method
for	O
the	O
window	O
vector	O
at	O
position	O
.	O

The	O
new	O
successive	O
higher	B-Method
-	I-Method
order	I-Method
window	I-Method
representations	E-Method
then	O
are	O
fed	O
into	O
LSTM	S-Method
which	O
is	O
described	O
below	O
.	O

A	O
max	B-Method
-	I-Method
over	I-Method
-	I-Method
pooling	E-Method
or	O
dynamic	B-Method
k	I-Method
-	I-Method
max	I-Method
pooling	E-Method
is	O
often	O
applied	O
to	O
feature	O
maps	O
after	O
the	O
convolution	S-Method
to	O
select	O
the	O
most	O
or	O
the	O
k	O
-	O
most	O
important	O
features	O
.	O

However	O
,	O
LSTM	S-Method
is	O
specified	O
for	O
sequence	O
input	O
,	O
and	O
pooling	S-Task
will	O
break	O
such	O
sequence	B-Method
organization	E-Method
due	O
to	O
the	O
discontinuous	O
selected	O
features	O
.	O

Since	O
we	O
stack	O
an	O
LSTM	S-Method
neural	O
neural	O
network	O
on	O
top	O
of	O
the	O
CNN	S-Method
,	O
we	O
will	O
not	O
apply	O
pooling	S-Method
after	O
the	O
convolution	B-Method
operation	E-Method
.	O

subsection	O
:	O
Long	B-Task
Short	I-Task
-	I-Task
Term	I-Task
Memory	I-Task
Networks	E-Task
Recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
are	O
able	O
to	O
propagate	O
historical	O
information	O
via	O
a	O
chain	B-Method
-	I-Method
like	I-Method
neural	I-Method
network	I-Method
architecture	E-Method
.	O

While	O
processing	O
sequential	O
data	O
,	O
it	O
looks	O
at	O
the	O
current	O
input	O
as	O
well	O
as	O
the	O
previous	O
output	O
of	O
hidden	O
state	O
at	O
each	O
time	O
step	O
.	O

However	O
,	O
standard	O
RNNs	S-Method
becomes	O
unable	O
to	O
learn	O
long	O
-	O
term	O
dependencies	O
as	O
the	O
gap	O
between	O
two	O
time	O
steps	O
becomes	O
large	O
.	O

To	O
address	O
this	O
issue	O
,	O
LSTM	S-Method
was	O
first	O
introduced	O
in	O
and	O
re	O
-	O
emerged	O
as	O
a	O
successful	O
architecture	O
since	O
Ilya	O
et	O
al	O
.	O

seq	O
obtained	O
remarkable	O
performance	O
in	O
statistical	B-Task
machine	I-Task
translation	E-Task
.	O

Although	O
many	O
variants	O
of	O
LSTM	S-Method
were	O
proposed	O
,	O
we	O
adopt	O
the	O
standard	O
architecture	O
in	O
this	O
work	O
.	O

The	O
LSTM	S-Method
architecture	O
has	O
a	O
range	O
of	O
repeated	B-Method
modules	E-Method
for	O
each	O
time	O
step	O
as	O
in	O
a	O
standard	O
RNN	S-Method
.	O

At	O
each	O
time	O
step	O
,	O
the	O
output	O
of	O
the	O
module	O
is	O
controlled	O
by	O
a	O
set	O
of	O
gates	O
in	O
as	O
a	O
function	O
of	O
the	O
old	O
hidden	O
state	O
and	O
the	O
input	O
at	O
the	O
current	O
time	O
step	O
:	O
the	O
forget	O
gate	O
,	O
the	O
input	O
gate	O
,	O
and	O
the	O
output	O
gate	O
.	O

These	O
gates	O
collectively	O
decide	O
how	O
to	O
update	O
the	O
current	O
memory	O
cell	O
and	O
the	O
current	O
hidden	O
state	O
.	O

We	O
use	O
to	O
denote	O
the	O
memory	O
dimension	O
in	O
the	O
LSTM	S-Method
and	O
all	O
vectors	O
in	O
this	O
architecture	O
share	O
the	O
same	O
dimension	O
.	O

The	O
LSTM	S-Method
transition	O
functions	O
are	O
defined	O
as	O
follows	O
:	O
Here	O
,	O
is	O
the	O
logistic	B-Method
sigmoid	I-Method
function	E-Method
that	O
has	O
an	O
output	O
in	O
,	O
denotes	O
the	O
hyperbolic	B-Method
tangent	I-Method
function	E-Method
that	O
has	O
an	O
output	O
in	O
,	O
and	O
denotes	O
the	O
elementwise	O
multiplication	O
.	O

To	O
understand	O
the	O
mechanism	O
behind	O
the	O
architecture	O
,	O
we	O
can	O
view	O
as	O
the	O
function	O
to	O
control	O
to	O
what	O
extent	O
the	O
information	O
from	O
the	O
old	O
memory	O
cell	O
is	O
going	O
to	O
be	O
thrown	O
away	O
,	O
to	O
control	O
how	O
much	O
new	O
information	O
is	O
going	O
to	O
be	O
stored	O
in	O
the	O
current	O
memory	O
cell	O
,	O
and	O
to	O
control	O
what	O
to	O
output	O
based	O
on	O
the	O
memory	O
cell	O
.	O

LSTM	S-Method
is	O
explicitly	O
designed	O
for	O
time	O
-	O
series	O
data	O
for	O
learning	B-Task
long	I-Task
-	I-Task
term	I-Task
dependencies	E-Task
,	O
and	O
therefore	O
we	O
choose	O
LSTM	S-Method
upon	O
the	O
convolution	B-Method
layer	E-Method
to	O
learn	O
such	O
dependencies	O
in	O
the	O
sequence	O
of	O
higher	O
-	O
level	O
features	O
.	O

section	O
:	O
Learning	O
C	O
-	O
LSTM	S-Method
for	O
Text	B-Task
Classification	E-Task
For	O
text	B-Task
classification	E-Task
,	O
we	O
regard	O
the	O
output	O
of	O
the	O
hidden	O
state	O
at	O
the	O
last	O
time	O
step	O
of	O
LSTM	S-Method
as	O
the	O
document	B-Method
representation	E-Method
and	O
we	O
add	O
a	O
softmax	B-Method
layer	E-Method
on	O
top	O
.	O

We	O
train	O
the	O
entire	O
model	O
by	O
minimizing	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
error	E-Metric
.	O

Given	O
a	O
training	O
sample	O
and	O
its	O
true	O
label	O
where	O
is	O
the	O
number	O
of	O
possible	O
labels	O
and	O
the	O
estimated	O
probabilities	O
for	O
each	O
label	O
,	O
the	O
error	S-Metric
is	O
defined	O
as	O
:	O
where	O
is	O
an	O
indicator	O
such	O
that	O
otherwise	O
.	O

We	O
employ	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
to	O
learn	O
the	O
model	O
parameters	O
and	O
adopt	O
the	O
optimizer	B-Method
RMSprop	E-Method
.	O

subsection	O
:	O
Padding	S-Task
and	O
Word	B-Task
Vector	I-Task
Initialization	E-Task
First	O
,	O
we	O
use	O
to	O
denote	O
the	O
maximum	O
length	O
of	O
the	O
sentence	O
in	O
the	O
training	O
set	O
.	O

As	O
the	O
convolution	B-Method
layer	E-Method
in	O
our	O
model	O
requires	O
fixed	O
-	O
length	O
input	O
,	O
we	O
pad	O
each	O
sentence	O
that	O
has	O
a	O
length	O
less	O
than	O
with	O
special	O
symbols	O
at	O
the	O
end	O
that	O
indicate	O
the	O
unknown	O
words	O
.	O

For	O
a	O
sentence	O
in	O
the	O
test	O
dataset	O
,	O
we	O
pad	O
sentences	O
that	O
are	O
shorter	O
than	O
in	O
the	O
same	O
way	O
,	O
but	O
for	O
sentences	O
that	O
have	O
a	O
length	O
longer	O
than	O
,	O
we	O
simply	O
cut	O
extra	O
words	O
at	O
the	O
end	O
of	O
these	O
sentences	O
to	O
reach	O
.	O

We	O
initialize	O
word	O
vectors	O
with	O
the	O
publicly	O
available	O
word2vec	O
vectors	O
that	O
are	O
pre	O
-	O
trained	O
using	O
about	O
100B	O
words	O
from	O
the	O
Google	O
News	O
Dataset	O
.	O

The	O
dimensionality	O
of	O
the	O
word	O
vectors	O
is	O
300	O
.	O

We	O
also	O
initialize	O
the	O
word	O
vector	O
for	O
the	O
unknown	O
words	O
from	O
the	O
uniform	O
distribution	O
[	O
-	O
0.25	O
,	O
0.25	O
]	O
.	O

We	O
then	O
fine	O
-	O
tune	O
the	O
word	O
vectors	O
along	O
with	O
other	O
model	O
parameters	O
during	O
training	O
.	O

subsection	O
:	O
Regularization	S-Task
For	O
regularization	S-Task
,	O
we	O
employ	O
two	O
commonly	O
used	O
techniques	O
:	O
dropout	S-Method
and	O
L2	B-Method
weight	I-Method
regularization	E-Method
.	O

We	O
apply	O
dropout	S-Method
to	O
prevent	O
co	B-Task
-	I-Task
adaptation	E-Task
.	O

In	O
our	O
model	O
,	O
we	O
either	O
apply	O
dropout	S-Method
to	O
word	O
vectors	O
before	O
feeding	O
the	O
sequence	O
of	O
words	O
into	O
the	O
convolutional	B-Method
layer	E-Method
or	O
to	O
the	O
output	O
of	O
LSTM	S-Method
before	O
the	O
softmax	B-Method
layer	E-Method
.	O

The	O
L2	B-Method
regularization	E-Method
is	O
applied	O
to	O
the	O
weight	O
of	O
the	O
softmax	O
layer	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
C	O
-	O
LSTM	S-Method
model	O
on	O
two	O
tasks	O
:	O
(	O
1	O
)	O
sentiment	B-Task
classification	E-Task
,	O
and	O
(	O
2	O
)	O
question	B-Task
type	I-Task
classification	E-Task
.	O

In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
datasets	O
and	O
the	O
experimental	O
settings	O
.	O

subsection	O
:	O
Datasets	O
Sentiment	B-Task
Classification	E-Task
:	O
Our	O
task	O
in	O
this	O
regard	O
is	O
to	O
predict	O
the	O
sentiment	B-Task
polarity	I-Task
of	I-Task
movie	I-Task
reviews	E-Task
.	O

We	O
use	O
the	O
Stanford	B-Material
Sentiment	I-Material
Treebank	E-Material
(	O
SST	S-Material
)	O
benchmark	O
.	O

This	O
dataset	O
consists	O
of	O
11855	O
movie	O
reviews	O
and	O
are	O
split	O
into	O
train	O
(	O
8544	O
)	O
,	O
dev	O
(	O
1101	O
)	O
,	O
and	O
test	O
(	O
2210	O
)	O
.	O

Sentences	O
in	O
this	O
corpus	O
are	O
parsed	O
and	O
all	O
phrases	O
along	O
with	O
the	O
sentences	O
are	O
fully	O
annotated	O
with	O
5	O
labels	O
:	O
very	O
positive	O
,	O
positive	O
,	O
neural	O
,	O
negative	O
,	O
very	O
negative	O
.	O

We	O
consider	O
two	O
classification	B-Task
tasks	E-Task
on	O
this	O
dataset	O
:	O
fine	B-Task
-	I-Task
grained	I-Task
classification	E-Task
with	O
5	O
labels	O
and	O
binary	B-Task
classification	E-Task
by	O
removing	O
neural	O
labels	O
.	O

For	O
the	O
binary	B-Task
classification	E-Task
,	O
the	O
dataset	O
has	O
a	O
split	O
of	O
train	O
(	O
6920	O
)	O
/	O
dev	O
(	O
872	O
)	O
/	O
test	O
(	O
1821	O
)	O
.	O

Since	O
the	O
data	O
is	O
provided	O
in	O
the	O
format	O
of	O
sub	O
-	O
sentences	O
,	O
we	O
train	O
the	O
model	O
on	O
both	O
phrases	O
and	O
sentences	O
but	O
only	O
test	O
on	O
the	O
sentences	O
as	O
in	O
several	O
previous	O
works	O
.	O

Question	B-Task
type	I-Task
classification	E-Task
:	O
Question	B-Task
classification	E-Task
is	O
an	O
important	O
step	O
in	O
a	O
question	B-Task
answering	I-Task
system	E-Task
that	O
classifies	O
a	O
question	O
into	O
a	O
specific	O
type	O
,	O
e.g.	O
“	O
what	O
is	O
the	O
highest	O
waterfall	O
in	O
the	O
United	O
States	O
?	O
”	O
is	O
a	O
question	O
that	O
belongs	O
to	O
“	O
location	O
”	O
.	O

For	O
this	O
task	O
,	O
we	O
use	O
the	O
benchmark	O
TREC	S-Material
.	O

TREC	S-Material
divides	O
all	O
questions	O
into	O
6	O
categories	O
,	O
including	O
location	O
,	O
human	O
,	O
entity	O
,	O
abbreviation	O
,	O
description	O
and	O
numeric	O
.	O

The	O
training	O
dataset	O
contains	O
5452	O
labelled	O
questions	O
while	O
the	O
testing	O
dataset	O
contains	O
500	O
questions	O
.	O

subsection	O
:	O
Experimental	O
Settings	O
We	O
implement	O
our	O
model	O
based	O
on	O
Theano	O
–	O
a	O
python	B-Method
library	E-Method
,	O
which	O
supports	O
efficient	O
symbolic	B-Task
differentiation	E-Task
and	O
transparent	O
use	O
of	O
a	O
GPU	O
.	O

To	O
benefit	O
from	O
the	O
efficiency	O
of	O
parallel	O
computation	O
of	O
the	O
tensors	O
,	O
we	O
train	O
the	O
model	O
on	O
a	O
GPU	S-Method
.	O

For	O
text	B-Task
preprocessing	E-Task
,	O
we	O
only	O
convert	O
all	O
characters	O
in	O
the	O
dataset	O
to	O
lower	O
case	O
.	O

For	O
SST	S-Material
,	O
we	O
conduct	O
hyperparameter	O
(	O
number	O
of	O
filters	O
,	O
filter	O
length	O
in	O
CNN	S-Method
;	O
memory	O
dimension	O
in	O
LSTM	S-Method
;	O
dropout	O
rate	O
and	O
which	O
layer	O
to	O
apply	O
,	O
etc	O
.	O

)	O
tuning	O
on	O
the	O
validation	O
data	O
in	O
the	O
standard	O
split	O
.	O

For	O
TREC	S-Material
,	O
we	O
hold	O
out	O
1000	O
samples	O
from	O
the	O
training	O
dataset	O
for	O
hyperparameter	B-Task
search	E-Task
and	O
train	O
the	O
model	O
using	O
the	O
remaining	O
data	O
.	O

In	O
our	O
final	O
settings	O
,	O
we	O
only	O
use	O
one	O
convolutional	B-Method
layer	E-Method
and	O
one	O
LSTM	B-Method
layer	E-Method
for	O
both	O
tasks	O
.	O

For	O
the	O
filter	O
size	O
,	O
we	O
investigated	O
filter	O
lengths	O
of	O
2	O
,	O
3	O
and	O
4	O
in	O
two	O
cases	O
:	O
a	O
)	O
single	O
convolutional	B-Method
layer	E-Method
with	O
the	O
same	O
filter	O
length	O
,	O
and	O
b	O
)	O
multiple	O
convolutional	B-Method
layers	E-Method
with	O
different	O
lengths	O
of	O
filters	O
in	O
parallel	O
.	O

Here	O
we	O
denote	O
the	O
number	O
of	O
filters	O
of	O
length	O
by	O
for	O
ease	O
of	O
clarification	O
.	O

For	O
the	O
first	O
case	O
,	O
each	O
n	O
-	O
gram	O
window	O
is	O
transformed	O
into	O
convoluted	O
features	O
after	O
convolution	S-Method
and	O
the	O
sequence	O
of	O
window	B-Method
representations	E-Method
is	O
fed	O
into	O
LSTM	S-Method
.	O

For	O
the	O
latter	O
case	O
,	O
since	O
the	O
number	O
of	O
windows	O
generated	O
from	O
each	O
convolution	B-Method
layer	E-Method
varies	O
when	O
the	O
filter	O
length	O
varies	O
(	O
see	O
below	O
equation	O
(	O
3	O
)	O
)	O
,	O
we	O
cut	O
the	O
window	O
sequence	O
at	O
the	O
end	O
based	O
on	O
the	O
maximum	O
filter	O
length	O
that	O
gives	O
the	O
shortest	O
number	O
of	O
windows	O
.	O

Each	O
window	O
is	O
represented	O
as	O
the	O
concatenation	O
of	O
outputs	O
from	O
different	O
convolutional	B-Method
layers	E-Method
.	O

We	O
also	O
exploit	O
different	O
combinations	O
of	O
different	O
filter	O
lengths	O
.	O

We	O
further	O
present	O
experimental	O
analysis	O
of	O
the	O
exploration	O
on	O
filter	B-Metric
size	E-Metric
later	O
.	O

According	O
to	O
the	O
experiments	O
,	O
we	O
choose	O
a	O
single	O
convolutional	B-Method
layer	E-Method
with	O
filter	O
length	O
3	O
.	O

For	O
SST	S-Material
,	O
the	O
number	O
of	O
filters	O
of	O
length	O
3	O
is	O
set	O
to	O
be	O
150	O
and	O
the	O
memory	O
dimension	O
of	O
LSTM	S-Method
is	O
set	O
to	O
be	O
150	O
,	O
too	O
.	O

The	O
word	B-Method
vector	I-Method
layer	E-Method
and	O
the	O
LSTM	B-Method
layer	E-Method
are	O
dropped	O
out	O
with	O
a	O
probability	O
of	O
0.5	O
.	O

For	O
TREC	S-Material
,	O
the	O
number	O
of	O
filters	O
is	O
set	O
to	O
be	O
300	O
and	O
the	O
memory	O
dimension	O
is	O
set	O
to	O
be	O
300	O
.	O

The	O
word	B-Method
vector	I-Method
layer	E-Method
and	O
the	O
LSTM	B-Method
layer	E-Method
are	O
dropped	O
out	O
with	O
a	O
probability	O
of	O
0.5	O
.	O

We	O
also	O
add	O
L2	B-Method
regularization	E-Method
with	O
a	O
factor	O
of	O
0.001	O
to	O
the	O
weights	O
in	O
the	O
softmax	B-Method
layer	E-Method
for	O
both	O
tasks	O
.	O

section	O
:	O
Results	O
and	O
Model	O
Analysis	O
In	O
this	O
section	O
,	O
we	O
show	O
our	O
evaluation	O
results	O
on	O
sentiment	B-Task
classification	E-Task
and	O
question	B-Task
type	I-Task
classification	I-Task
tasks	E-Task
.	O

Moreover	O
,	O
we	O
give	O
some	O
model	O
analysis	O
on	O
the	O
filter	B-Method
size	I-Method
configuration	E-Method
.	O

subsection	O
:	O
Sentiment	B-Task
Classification	E-Task
The	O
results	O
are	O
shown	O
in	O
Table	O
1	O
.	O

We	O
compare	O
our	O
model	O
with	O
a	O
large	O
set	O
of	O
well	O
-	O
performed	O
models	O
on	O
the	O
Stanford	B-Material
Sentiment	I-Material
Treebank	E-Material
.	O

Generally	O
,	O
the	O
baseline	B-Method
models	E-Method
consist	O
of	O
recursive	B-Method
models	E-Method
,	O
convolutional	B-Method
neural	I-Method
network	E-Method
models	O
,	O
LSTM	S-Method
related	O
models	O
and	O
others	O
.	O

The	O
recursive	B-Method
models	E-Method
employ	O
a	O
syntactic	O
parse	O
tree	O
as	O
the	O
sentence	O
structure	O
and	O
the	O
sentence	B-Method
representation	E-Method
is	O
computed	O
recursively	O
in	O
a	O
bottom	O
-	O
up	O
manner	O
along	O
the	O
parse	O
tree	O
.	O

Under	O
this	O
category	O
,	O
we	O
choose	O
recursive	B-Method
autoencoder	E-Method
(	O
RAE	S-Method
)	O
,	O
matrix	B-Method
-	I-Method
vector	E-Method
(	O
MV	B-Method
-	I-Method
RNN	E-Method
)	O
,	O
tensor	B-Method
based	I-Method
composition	E-Method
(	O
RNTN	S-Method
)	O
and	O
multi	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
(	I-Method
DRNN	I-Method
)	I-Method
recursive	I-Method
neural	I-Method
network	E-Method
as	O
baselines	O
.	O

Among	O
CNNs	S-Method
,	O
we	O
compare	O
with	O
Kim	O
’s	O
kim	O
CNN	S-Method
model	O
with	O
fine	O
-	O
tuned	O
word	O
vectors	O
(	O
CNN	S-Method
-	O
non	O
-	O
static	O
)	O
and	O
multi	O
-	O
channels	O
(	O
CNN	S-Method
-	O
multichannel	O
)	O
,	O
DCNN	S-Method
with	O
dynamic	B-Method
k	I-Method
-	I-Method
max	I-Method
pooling	E-Method
,	O
Tao	O
’s	O
CNN	S-Method
(	O
Molding	O
-	O
CNN	S-Method
)	O
with	O
low	B-Method
-	I-Method
rank	I-Method
tensor	I-Method
based	I-Method
non	I-Method
-	I-Method
linear	I-Method
and	I-Method
non	I-Method
-	I-Method
consecutive	I-Method
convolutions	E-Method
.	O

Among	O
LSTM	S-Method
related	O
models	O
,	O
we	O
first	O
compare	O
with	O
two	O
tree	B-Method
-	I-Method
structured	I-Method
LSTM	I-Method
models	E-Method
(	O
Dependence	B-Method
Tree	I-Method
-	I-Method
LSTM	E-Method
and	O
Constituency	B-Method
Tree	I-Method
-	I-Method
LSTM	E-Method
)	O
that	O
adjust	O
LSTM	S-Method
to	O
tree	O
-	O
structured	O
network	O
topologies	O
.	O

Then	O
we	O
implement	O
one	B-Method
-	I-Method
layer	I-Method
LSTM	E-Method
and	O
Bi	B-Method
-	I-Method
LSTM	E-Method
by	O
ourselves	O
.	O

Since	O
we	O
could	O
not	O
tune	O
the	O
result	O
of	O
Bi	B-Method
-	I-Method
LSTM	E-Method
to	O
be	O
as	O
good	O
as	O
what	O
has	O
been	O
reported	O
in	O
even	O
if	O
following	O
their	O
untied	O
weight	O
configuration	O
,	O
we	O
report	O
our	O
own	O
results	O
.	O

For	O
other	O
baseline	O
methods	O
,	O
we	O
compare	O
against	O
SVM	S-Method
with	O
unigram	B-Method
and	I-Method
bigram	I-Method
features	E-Method
,	O
NBoW	S-Method
with	O
average	B-Method
word	I-Method
vector	I-Method
features	E-Method
and	O
paragraph	B-Method
vector	E-Method
that	O
infers	O
the	O
new	O
paragraph	O
vector	O
for	O
unseen	O
documents	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
achieve	O
the	O
fourth	O
best	O
published	O
result	O
for	O
the	O
5	B-Task
-	I-Task
class	I-Task
classification	I-Task
task	E-Task
on	O
this	O
dataset	O
.	O

For	O
the	O
binary	B-Task
classification	I-Task
task	E-Task
,	O
we	O
achieve	O
comparable	O
results	O
with	O
respect	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
ones	O
.	O

From	O
Table	O
1	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
Although	O
we	O
did	O
not	O
beat	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
ones	O
,	O
as	O
an	O
end	O
-	O
to	O
-	O
end	B-Method
model	E-Method
,	O
the	O
result	O
is	O
still	O
promising	O
and	O
comparable	O
with	O
thoes	B-Method
models	E-Method
that	O
heavily	O
rely	O
on	O
linguistic	O
annotations	O
and	O
knowledge	O
,	O
especially	O
syntactic	O
parse	O
trees	O
.	O

This	O
indicates	O
C	O
-	O
LSTM	S-Method
will	O
be	O
more	O
feasible	O
for	O
various	O
scenarios	O
.	O

(	O
2	O
)	O
Comparing	O
our	O
results	O
against	O
single	B-Method
CNN	E-Method
and	O
LSTM	B-Method
models	E-Method
shows	O
that	O
LSTM	S-Method
does	O
learn	O
long	O
-	O
term	O
dependencies	O
across	O
sequences	O
of	O
higher	O
-	O
level	B-Method
representations	E-Method
better	O
.	O

We	O
could	O
explore	O
in	O
the	O
future	O
how	O
to	O
learn	O
more	O
compact	O
higher	O
-	O
level	O
representations	O
by	O
replacing	O
standard	O
convolution	S-Method
with	O
other	O
non	B-Method
-	I-Method
linear	I-Method
feature	I-Method
mapping	I-Method
functions	E-Method
or	O
appealing	O
to	O
tree	O
-	O
structured	O
topologies	O
before	O
the	O
convolutional	B-Method
layer	E-Method
.	O

subsection	O
:	O
Question	B-Method
Type	I-Method
Classification	E-Method
The	O
prediction	B-Metric
accuracy	E-Metric
on	O
TREC	S-Material
question	O
classification	O
is	O
reported	O
in	O
Table	O
2	O
.	O

We	O
compare	O
our	O
model	O
with	O
a	O
variety	O
of	O
models	O
.	O

The	O
SVM	B-Method
classifier	E-Method
uses	O
unigrams	O
,	O
bigrams	O
,	O
wh	O
-	O
word	O
,	O
head	O
word	O
,	O
POS	O
tags	O
,	O
parser	O
,	O
hypernyms	O
,	O
WordNet	O
synsets	O
as	O
engineered	O
features	O
and	O
60	O
hand	O
-	O
coded	O
rules	O
.	O

Ada	O
-	O
CNN	S-Method
is	O
a	O
self	B-Method
-	I-Method
adaptiive	I-Method
hierarchical	I-Method
sentence	I-Method
model	E-Method
with	O
gating	B-Method
networks	E-Method
.	O

Other	O
baseline	O
models	O
have	O
been	O
introduced	O
in	O
the	O
last	O
task	O
.	O

From	O
Table	O
2	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
Our	O
result	O
consistently	O
outperforms	O
all	O
published	O
neural	B-Method
baseline	I-Method
models	E-Method
,	O
which	O
means	O
that	O
C	O
-	O
LSTM	S-Method
captures	O
intentions	O
of	O
TREC	B-Material
questions	E-Material
well	O
.	O

(	O
2	O
)	O
Our	O
result	O
is	O
close	O
to	O
that	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
SVM	S-Method
that	O
depends	O
on	O
highly	O
engineered	O
features	O
.	O

Such	O
engineered	O
features	O
not	O
only	O
demands	O
human	O
laboring	O
but	O
also	O
leads	O
to	O
the	O
error	S-Metric
propagation	O
in	O
the	O
existing	O
NLP	B-Method
tools	E-Method
,	O
thus	O
could	O
n’t	O
generalize	O
well	O
in	O
other	O
datasets	O
and	O
tasks	O
.	O

With	O
the	O
ability	O
of	O
automatically	B-Task
learning	I-Task
semantic	I-Task
sentence	I-Task
representations	E-Task
,	O
C	O
-	O
LSTM	S-Method
does	O
n’t	O
require	O
any	O
human	O
-	O
designed	O
features	O
and	O
has	O
a	O
better	O
scalibility	O
.	O

subsection	O
:	O
Model	O
Analysis	O
Here	O
we	O
investigate	O
the	O
impact	O
of	O
different	O
filter	O
configurations	O
in	O
the	O
convolutional	B-Method
layer	E-Method
on	O
the	O
model	O
performance	O
.	O

In	O
the	O
convolutional	B-Method
layer	E-Method
of	O
our	O
model	O
,	O
filters	S-Method
are	O
used	O
to	O
capture	O
local	O
n	O
-	O
gram	O
features	O
.	O

Intuitively	O
,	O
multiple	O
convolutional	B-Method
layers	E-Method
in	O
parallel	O
with	O
different	O
filter	O
sizes	O
should	O
perform	O
better	O
than	O
single	O
convolutional	B-Method
layers	E-Method
with	O
the	O
same	O
length	O
filters	O
in	O
that	O
different	O
filter	O
sizes	O
could	O
exploit	O
features	O
of	O
different	O
n	O
-	O
grams	O
.	O

However	O
,	O
we	O
found	O
in	O
our	O
experiments	O
that	O
single	O
convolutional	B-Method
layer	E-Method
with	O
filter	O
length	O
3	O
always	O
outperforms	O
the	O
other	O
cases	O
.	O

We	O
show	O
in	O
Figure	O
2	O
the	O
prediction	B-Metric
accuracies	E-Metric
on	O
the	O
6	B-Task
-	I-Task
way	I-Task
question	I-Task
classification	I-Task
task	E-Task
using	O
different	O
filter	B-Method
configurations	E-Method
.	O

Note	O
that	O
we	O
also	O
observe	O
the	O
similar	O
phenomenon	O
in	O
the	O
sentiment	B-Task
classification	I-Task
task	E-Task
.	O

For	O
each	O
filter	O
configuration	O
,	O
we	O
report	O
in	O
Figure	O
2	O
the	O
best	O
result	O
under	O
extensive	O
grid	B-Method
-	I-Method
search	E-Method
on	O
hyperparameters	O
.	O

It	O
it	O
shown	O
that	O
single	O
convolutional	B-Method
layer	E-Method
with	O
filter	O
length	O
3	O
performs	O
best	O
among	O
all	O
filter	O
configurations	O
.	O

For	O
the	O
case	O
of	O
multiple	O
convolutional	O
layers	O
in	O
parallel	O
,	O
it	O
is	O
shown	O
that	O
filter	B-Method
configurations	E-Method
with	O
filter	O
length	O
3	O
performs	O
better	O
that	O
those	O
without	O
tri	B-Method
-	I-Method
gram	I-Method
filters	E-Method
,	O
which	O
further	O
confirms	O
that	O
tri	O
-	O
gram	O
features	O
do	O
play	O
a	O
significant	O
role	O
in	O
capturing	O
local	O
features	O
in	O
our	O
tasks	O
.	O

We	O
conjecture	O
that	O
LSTM	S-Method
could	O
learn	O
better	O
semantic	B-Method
sentence	I-Method
representations	E-Method
from	O
sequences	O
of	O
tri	O
-	O
gram	O
features	O
.	O

section	O
:	O
Conclusion	O
and	O
Future	O
Work	O
We	O
have	O
described	O
a	O
novel	O
,	O
unified	B-Method
model	E-Method
called	O
C	O
-	O
LSTM	S-Method
that	O
combines	O
convolutional	B-Method
neural	I-Method
network	E-Method
with	O
long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	I-Method
network	E-Method
(	O
LSTM	B-Method
)	E-Method
.	O

C	O
-	O
LSTM	S-Method
is	O
able	O
to	O
learn	O
phrase	O
-	O
level	O
features	O
through	O
a	O
convolutional	B-Method
layer	E-Method
;	O
sequences	O
of	O
such	O
higher	O
-	O
level	O
representations	O
are	O
then	O
fed	O
into	O
the	O
LSTM	S-Method
to	O
learn	O
long	O
-	O
term	O
dependencies	O
.	O

We	O
evaluated	O
the	O
learned	O
semantic	B-Method
sentence	I-Method
representations	E-Method
on	O
sentiment	B-Task
classification	E-Task
and	O
question	B-Task
type	I-Task
classification	I-Task
tasks	E-Task
with	O
very	O
satisfactory	O
results	O
.	O

We	O
could	O
explore	O
in	O
the	O
future	O
ways	O
to	O
replace	O
the	O
standard	O
convolution	S-Method
with	O
tensor	B-Method
-	I-Method
based	I-Method
operations	E-Method
or	O
tree	B-Method
-	I-Method
structured	I-Method
convolutions	E-Method
.	O

We	O
believe	O
LSTM	S-Method
will	O
benefit	O
from	O
more	O
structured	O
higher	B-Method
-	I-Method
level	I-Method
representations	E-Method
.	O

bibliography	O
:	O
References	O
document	O
:	O
Multiple	B-Method
Instance	I-Method
Detection	I-Method
Network	E-Method
with	O
Online	B-Method
Instance	I-Method
Classifier	I-Method
Refinement	E-Method
Of	O
late	O
,	O
weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
is	O
with	O
great	O
importance	O
in	O
object	B-Task
recognition	E-Task
.	O

Based	O
on	O
deep	B-Method
learning	E-Method
,	O
weakly	B-Method
supervised	I-Method
detectors	E-Method
have	O
achieved	O
many	O
promising	O
results	O
.	O

However	O
,	O
compared	O
with	O
fully	B-Task
supervised	I-Task
detection	E-Task
,	O
it	O
is	O
more	O
challenging	O
to	O
train	O
deep	B-Method
network	I-Method
based	I-Method
detectors	E-Method
in	O
a	O
weakly	B-Method
supervised	I-Method
manner	E-Method
.	O

Here	O
we	O
formulate	O
weakly	B-Task
supervised	I-Task
detection	E-Task
as	O
a	O
Multiple	B-Task
Instance	I-Task
Learning	I-Task
(	I-Task
MIL	I-Task
)	I-Task
problem	E-Task
,	O
where	O
instance	B-Method
classifiers	E-Method
(	O
object	B-Method
detectors	E-Method
)	O
are	O
put	O
into	O
the	O
network	O
as	O
hidden	O
nodes	O
.	O

We	O
propose	O
a	O
novel	O
online	B-Method
instance	I-Method
classifier	I-Method
refinement	I-Method
algorithm	E-Method
to	O
integrate	O
MIL	S-Method
and	O
the	O
instance	B-Method
classifier	I-Method
refinement	I-Method
procedure	E-Method
into	O
a	O
single	O
deep	B-Method
network	E-Method
,	O
and	O
train	O
the	O
network	O
end	O
-	O
to	O
-	O
end	O
with	O
only	O
image	O
-	O
level	O
supervision	O
,	O
,	O
without	O
object	O
location	O
information	O
.	O

More	O
precisely	O
,	O
instance	O
labels	O
inferred	O
from	O
weak	O
supervision	O
are	O
propagated	O
to	O
their	O
spatially	O
overlapped	O
instances	O
to	O
refine	O
instance	B-Method
classifier	E-Method
online	O
.	O

The	O
iterative	B-Method
instance	I-Method
classifier	I-Method
refinement	I-Method
procedure	E-Method
is	O
implemented	O
using	O
multiple	O
streams	O
in	O
deep	B-Method
network	E-Method
,	O
where	O
each	O
stream	O
supervises	O
its	O
latter	O
stream	O
.	O

Weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
experiments	O
are	O
carried	O
out	O
on	O
the	O
challenging	O
PASCAL	B-Material
VOC	I-Material
2007	E-Material
and	O
2012	S-Material
benchmarks	O
.	O

We	O
obtain	O
mAP	S-Metric
on	O
VOC	B-Material
2007	E-Material
that	O
significantly	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

section	O
:	O
Introduction	O
With	O
the	O
development	O
of	O
Convolutional	O
Neural	O
Network	O
(	O
CNN	S-Method
)	O
,	O
great	O
improvements	O
have	O
been	O
achieved	O
on	O
object	B-Task
detection	E-Task
,	O
due	O
to	O
the	O
availability	O
of	O
large	O
scale	O
datasets	O
with	O
accurate	O
boundingbox	O
-	O
level	O
annotations	O
.	O

However	O
,	O
collecting	O
such	O
accurate	O
annotations	O
can	O
be	O
very	O
labor	O
-	O
intensive	O
and	O
time	O
-	O
consuming	O
,	O
whereas	O
achieving	O
only	O
image	O
-	O
level	O
annotations	O
(	O
,	O
image	O
tags	O
)	O
is	O
much	O
easier	O
,	O
as	O
these	O
annotations	O
are	O
often	O
available	O
at	O
the	O
Internet	O
(	O
,	O
image	O
search	O
queries	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
aim	O
at	O
the	O
Weakly	B-Task
Supervised	I-Task
Object	I-Task
Detection	E-Task
(	O
WSOD	S-Task
)	O
problem	O
,	O
,	O
only	O
image	O
tags	O
are	O
available	O
during	O
training	O
to	O
indicate	O
whether	O
an	O
object	O
exists	O
in	O
an	O
image	O
.	O

Most	O
of	O
previous	O
methods	O
follow	O
the	O
Multiple	B-Method
Instance	I-Method
Learning	I-Method
(	I-Method
MIL	I-Method
)	I-Method
pipeline	E-Method
for	O
WSOD	S-Task
.	O

They	O
treat	O
images	O
as	O
bags	O
and	O
image	O
regions	O
generated	O
by	O
object	B-Method
proposal	I-Method
methods	E-Method
as	O
instances	O
to	O
train	O
instance	B-Method
classifiers	E-Method
(	O
object	B-Method
detectors	E-Method
)	O
under	O
the	O
MIL	O
constraints	O
.	O

Meanwhile	O
,	O
recent	O
efforts	O
tend	O
to	O
combine	O
MIL	S-Method
and	O
CNN	S-Method
by	O
either	O
using	O
CNN	S-Method
as	O
an	O
off	B-Method
-	I-Method
the	I-Method
-	I-Method
shelf	I-Method
feature	I-Method
extractor	E-Method
or	O
training	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
MIL	I-Method
network	E-Method
.	O

Here	O
we	O
are	O
also	O
along	O
the	O
MIL	O
line	O
for	O
WSOD	S-Task
,	O
and	O
train	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
network	E-Method
.	O

Though	O
many	O
promising	O
results	O
have	O
been	O
achieved	O
in	O
WSOD	S-Task
,	O
they	O
are	O
still	O
far	O
from	O
comparable	O
to	O
fully	O
supervised	O
ones	O
.	O

Weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
only	O
requires	O
supervision	O
at	O
image	O
category	O
level	O
.	O

Bilen	O
and	O
Vedaldi	O
presents	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
deep	I-Method
network	E-Method
for	O
WSOD	S-Task
,	O
in	O
which	O
final	O
image	B-Metric
classification	I-Metric
score	E-Metric
is	O
the	O
weighted	O
sum	O
of	O
proposal	O
scores	O
,	O
that	O
is	O
,	O
each	O
proposal	O
contributes	O
a	O
percentage	O
to	O
the	O
final	O
image	B-Task
classification	E-Task
.	O

The	O
deep	B-Method
network	E-Method
can	O
correctly	O
classify	O
image	O
even	O
only	O
“	O
see	O
”	O
a	O
part	O
of	O
object	O
,	O
and	O
as	O
a	O
result	O
,	O
the	O
top	O
ranking	O
proposal	O
may	O
fail	O
to	O
meet	O
the	O
standard	O
object	B-Metric
detection	I-Metric
requirement	E-Metric
(	O
IoU	O
0.5	O
between	O
ground	O
truths	O
and	O
predicted	O
boxes	O
)	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
left	O
)	O
,	O
the	O
top	O
-	O
ranking	O
proposal	O
A	O
is	O
too	O
small	O
.	O

Meanwhile	O
,	O
proposals	O
B	O
,	O
C	O
,	O
and	O
D	O
have	O
similar	O
detection	B-Metric
scores	E-Metric
.	O

This	O
shows	O
that	O
the	O
WSOD	B-Task
network	E-Task
is	O
not	O
discriminative	O
enough	O
to	O
correctly	O
localize	O
object	O
.	O

This	O
is	O
a	O
core	O
problem	O
of	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
deep	I-Task
network	I-Task
based	I-Task
WSOD	E-Task
.	O

To	O
address	O
this	O
problem	O
,	O
we	O
put	O
forward	O
two	O
improvements	O
in	O
this	O
paper	O
:	O
1	O
)	O
Instead	O
of	O
estimating	O
instance	O
weights	O
through	O
weighted	B-Method
sum	I-Method
pooling	E-Method
,	O
we	O
propose	O
to	O
add	O
some	O
blocks	O
in	O
the	O
network	O
for	O
learning	O
more	O
discriminative	O
instance	B-Method
classifiers	E-Method
by	O
explicitly	O
assigning	O
binary	O
instance	O
labels	O
;	O
2	O
)	O
We	O
propose	O
to	O
refine	O
instance	B-Method
classifier	E-Method
online	O
using	O
spatial	O
relation	O
.	O

Our	O
motivation	O
is	O
that	O
,	O
though	O
some	O
detectors	O
only	O
capture	O
objects	O
partially	O
,	O
proposals	O
having	O
high	O
spatial	O
overlaps	O
with	O
detected	O
parts	O
may	O
cover	O
the	O
whole	O
object	O
,	O
or	O
at	O
least	O
contain	O
larger	O
portion	O
of	O
the	O
object	O
.	O

In	O
,	O
Bilen	O
and	O
Vedaldi	O
propose	O
a	O
spatial	B-Method
regulariser	E-Method
via	O
forcing	O
features	O
of	O
highest	O
scoring	O
region	O
and	O
its	O
adjacent	O
regions	O
to	O
be	O
the	O
same	O
,	O
which	O
significantly	O
improves	O
WSOD	S-Task
performance	O
.	O

Nevertheless	O
,	O
forcing	O
spatially	O
overlapped	O
proposals	O
to	O
have	O
the	O
same	O
features	O
seems	O
too	O
rigorous	O
.	O

Rather	O
than	O
taking	O
the	O
rigorous	O
constraint	O
,	O
we	O
think	O
the	O
features	O
of	O
spatially	O
overlapped	O
proposals	O
are	O
in	O
the	O
same	O
manifold	O
.	O

Then	O
these	O
overlapped	O
proposals	O
could	O
share	O
similar	O
label	O
information	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
right	O
)	O
,	O
we	O
except	O
the	O
label	O
information	O
of	O
A	O
can	O
propagate	O
to	O
B	O
and	O
C	O
which	O
has	O
large	O
overlap	O
with	O
A	O
,	O
and	O
then	O
the	O
label	O
information	O
of	O
B	O
and	O
C	O
can	O
propagate	O
to	O
D	O
to	O
correctly	O
localize	O
object	O
.	O

To	O
implement	O
this	O
idea	O
,	O
we	O
design	O
some	O
instance	B-Method
classifiers	E-Method
in	O
the	O
network	O
of	O
.	O

The	O
labels	O
of	O
instance	O
could	O
be	O
refined	O
by	O
their	O
spatially	O
overlapped	O
instances	O
.	O

We	O
name	O
this	O
new	O
network	B-Method
structure	I-Method
Multiple	I-Method
Instance	I-Method
Detection	I-Method
Network	E-Method
(	O
MIDN	S-Method
)	O
with	O
instance	B-Method
classifier	E-Method
.	O

In	O
practice	O
,	O
there	O
are	O
two	O
important	O
issues	O
.	O

1	O
)	O
How	O
to	O
initialize	O
instance	O
labels	O
,	O
since	O
there	O
is	O
no	O
instance	O
-	O
level	O
supervision	O
in	O
this	O
task	O
.	O

2	O
)	O
How	O
to	O
train	O
the	O
network	O
with	O
instance	B-Method
classifier	E-Method
efficiently	O
.	O

A	O
natural	O
way	O
for	O
classifier	B-Task
refinement	E-Task
is	O
the	O
alternative	O
strategy	O
,	O
that	O
is	O
,	O
alternatively	O
relabelling	B-Method
instance	I-Method
and	I-Method
training	I-Method
instance	I-Method
classifier	E-Method
,	O
while	O
this	O
procedure	O
is	O
very	O
time	O
-	O
consuming	O
,	O
especially	O
considering	O
training	O
deep	B-Method
networks	E-Method
with	O
a	O
huge	O
number	O
of	O
Stochastic	B-Method
Gradient	I-Method
Descent	E-Method
(	O
SGD	B-Method
)	I-Method
iterations	E-Method
.	O

To	O
overcome	O
these	O
difficulties	O
,	O
we	O
propose	O
a	O
novel	O
Online	B-Method
Instance	I-Method
Classifier	I-Method
Refinement	E-Method
(	O
OICR	B-Method
)	I-Method
algorithm	E-Method
to	O
train	O
the	O
network	O
online	O
.	O

Our	O
method	O
has	O
multiple	O
output	O
streams	O
for	O
different	O
stages	O
:	O
the	O
first	O
is	O
the	O
MIDN	S-Method
to	O
train	O
a	O
basic	O
instance	B-Method
classifier	E-Method
and	O
others	O
refine	O
the	O
classifier	O
.	O

To	O
refine	O
instance	B-Method
classifier	E-Method
online	O
,	O
after	O
the	O
forward	O
process	O
of	O
SGD	S-Method
,	O
we	O
can	O
obtain	O
a	O
set	O
of	O
proposal	O
scores	O
.	O

According	O
to	O
these	O
scores	O
,	O
for	O
each	O
stage	O
,	O
we	O
can	O
label	O
the	O
top	O
-	O
scoring	O
proposal	O
along	O
with	O
its	O
spatially	O
overlapped	O
proposals	O
to	O
the	O
image	O
label	O
.	O

Then	O
these	O
proposal	O
labels	O
can	O
be	O
used	O
as	O
the	O
supervision	O
to	O
train	O
instance	B-Method
classifier	E-Method
in	O
the	O
next	O
stage	O
.	O

Though	O
the	O
top	O
-	O
scoring	O
proposal	O
may	O
only	O
contain	O
a	O
part	O
of	O
an	O
object	O
,	O
its	O
adjacent	O
proposals	O
will	O
cover	O
larger	O
portion	O
of	O
the	O
object	O
.	O

Thus	O
the	O
instance	B-Method
classifier	E-Method
can	O
be	O
refined	O
.	O

After	O
implementing	O
the	O
refinement	B-Method
procedure	E-Method
multiple	O
times	O
,	O
the	O
detector	S-Method
can	O
discover	O
the	O
whole	O
object	O
instead	O
of	O
parts	O
gradually	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

But	O
in	O
the	O
beginning	O
of	O
training	O
,	O
all	O
classifiers	S-Method
are	O
almost	O
non	O
-	O
trained	O
,	O
which	O
will	O
result	O
in	O
very	O
noisy	O
supervision	O
of	O
refined	O
classifiers	O
,	O
and	O
then	O
the	O
training	O
will	O
deviate	O
from	O
correct	O
solutions	O
a	O
lot	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
design	O
a	O
weighted	O
loss	O
further	O
by	O
assigning	O
different	O
weights	O
to	O
different	O
proposals	O
in	O
different	O
training	O
iterations	O
.	O

Using	O
this	O
strategy	O
,	O
all	O
classifier	B-Method
refinement	I-Method
procedures	E-Method
can	O
thus	O
be	O
integrated	O
into	O
a	O
single	O
network	O
and	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

It	O
can	O
improve	O
the	O
performance	O
benefiting	O
from	O
the	O
classifier	B-Method
refinement	I-Method
procedure	E-Method
.	O

Meanwhile	O
,	O
the	O
multi	B-Method
-	I-Method
stage	I-Method
strategy	E-Method
and	O
online	B-Method
refinement	I-Method
algorithm	E-Method
is	O
very	O
computational	O
efficient	O
in	O
both	O
training	S-Task
and	O
testing	S-Task
.	O

Moreover	O
,	O
performance	O
can	O
be	O
improved	O
by	O
sharing	O
representations	O
among	O
different	O
training	O
stages	O
.	O

We	O
elaborately	O
conduct	O
many	O
experiments	O
on	O
the	O
challenging	B-Material
PASCAL	I-Material
VOC	I-Material
dataset	E-Material
to	O
confirm	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O

Our	O
method	O
achieves	O
mAP	S-Metric
and	O
CorLoc	S-Task
on	O
VOC	B-Material
2007	E-Material
that	O
outperforms	O
previous	O
best	O
performed	O
methods	O
by	O
a	O
large	O
margin	O
.	O

In	O
summary	O
,	O
the	O
main	O
contributions	O
of	O
our	O
work	O
are	O
listed	O
as	O
follows	O
.	O

We	O
propose	O
a	O
framework	O
for	O
weakly	B-Task
supervised	I-Task
learning	E-Task
that	O
combines	O
MIDN	S-Method
with	O
multi	O
-	O
stage	O
instance	B-Method
classifiers	E-Method
.	O

With	O
only	O
supervision	O
of	O
the	O
outputs	O
from	O
its	O
preceding	O
stage	O
,	O
the	O
discriminatory	O
power	O
of	O
the	O
instance	B-Method
classifier	E-Method
can	O
be	O
enhanced	O
iteratively	O
.	O

We	O
further	O
design	O
a	O
novel	O
OICR	B-Method
algorithm	E-Method
that	O
integrates	O
the	O
basic	B-Method
detection	I-Method
network	E-Method
and	O
the	O
multi	B-Method
-	I-Method
stage	I-Method
instance	I-Method
-	I-Method
level	I-Method
classifier	E-Method
into	O
a	O
single	O
network	O
.	O

The	O
proposed	O
network	O
is	O
end	O
-	O
to	O
-	O
end	O
trainable	O
.	O

Compared	O
with	O
the	O
alternatively	B-Method
training	I-Method
strategy	E-Method
,	O
we	O
demonstrate	O
that	O
our	O
method	O
can	O
not	O
only	O
reduce	O
the	O
training	B-Metric
time	E-Metric
,	O
but	O
also	O
boost	O
the	O
performance	O
.	O

Our	O
method	O
achieves	O
significantly	O
better	O
results	O
over	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
the	O
challenging	O
PASCAL	B-Material
VOC	I-Material
2007	E-Material
and	O
2012	S-Material
benchmarks	O
for	O
weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
.	O

section	O
:	O
Related	O
work	O
MIL	S-Method
is	O
a	O
classical	O
weakly	B-Task
supervised	I-Task
learning	I-Task
problem	E-Task
and	O
was	O
first	O
proposed	O
in	O
for	O
drug	B-Task
activity	I-Task
prediction	E-Task
.	O

After	O
that	O
,	O
many	O
solutions	O
have	O
been	O
proposed	O
for	O
MIL	S-Method
.	O

In	O
MIL	S-Method
,	O
a	O
set	O
of	O
bags	O
are	O
given	O
,	O
and	O
each	O
bag	O
is	O
associated	O
with	O
a	O
collection	O
of	O
instances	O
.	O

MIL	S-Method
has	O
two	O
constraints	O
:	O
1	O
)	O
If	O
a	O
bag	O
is	O
positive	O
,	O
at	O
least	O
one	O
instance	O
in	O
the	O
bag	O
is	O
positive	O
;	O
2	O
)	O
If	O
a	O
bag	O
is	O
negative	O
,	O
all	O
instances	O
in	O
the	O
bag	O
are	O
negative	O
.	O

It	O
is	O
natural	O
to	O
treat	O
WSOD	S-Task
as	O
a	O
MIL	B-Task
problem	E-Task
.	O

Then	O
the	O
problem	O
turns	O
into	O
finding	O
an	O
instance	B-Method
classifier	E-Method
only	O
given	O
bag	O
labels	O
.	O

Our	O
method	O
also	O
follows	O
the	O
MIL	O
line	O
,	O
and	O
the	O
classifier	B-Method
refinement	E-Method
is	O
inspired	O
by	O
the	O
classifier	B-Method
updating	I-Method
procedure	E-Method
in	O
mi	B-Method
-	I-Method
SVM	E-Method
to	O
some	O
extent	O
.	O

The	O
differences	O
are	O
that	O
,	O
in	O
mi	B-Method
-	I-Method
SVM	E-Method
,	O
it	O
uses	O
an	O
alternative	O
strategy	O
to	O
relabel	O
instances	O
and	O
retrain	O
a	O
classifier	S-Method
,	O
while	O
we	O
adopt	O
an	O
online	B-Method
refinement	I-Method
algorithm	E-Method
;	O
the	O
mi	B-Method
-	I-Method
SVM	E-Method
relabel	O
instances	O
according	O
to	O
the	O
instance	O
score	O
predicted	O
by	O
the	O
classifier	S-Method
,	O
while	O
we	O
select	O
instances	O
according	O
to	O
the	O
spatial	O
relation	O
.	O

Most	O
of	O
the	O
existing	O
methods	O
solve	O
the	O
WSOD	B-Task
problem	E-Task
based	O
on	O
MIL	S-Method
.	O

For	O
example	O
,	O
Wang	O
relaxed	O
the	O
MIL	O
restraints	O
into	O
a	O
differentiable	O
loss	O
function	O
and	O
optimized	O
it	O
by	O
SGD	S-Method
to	O
speed	O
up	O
training	S-Task
and	O
improve	O
results	O
.	O

Cibis	S-Method
trained	O
a	O
multi	B-Method
-	I-Method
fold	I-Method
MIL	I-Method
detector	E-Method
by	O
alternatively	O
relabelling	O
instances	O
and	O
retraining	B-Method
classifier	E-Method
.	O

Recently	O
,	O
some	O
researchers	O
combined	O
CNN	S-Method
and	O
MIL	S-Method
to	O
train	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
network	E-Method
for	O
WSOD	S-Task
.	O

Oquab	S-Method
trained	O
a	O
CNN	S-Method
network	O
using	O
the	O
max	B-Method
-	I-Method
pooing	I-Method
MIL	I-Method
strategy	E-Method
to	O
localize	B-Task
objects	E-Task
.	O

But	O
their	O
methods	O
can	O
only	O
coarsely	O
localize	O
objects	O
regardless	O
of	O
their	O
sizes	O
and	O
aspect	O
ratios	O
,	O
our	O
method	O
can	O
detect	O
objects	O
more	O
accurately	O
.	O

Bilen	O
and	O
Vedaldi	O
proposed	O
a	O
Weakly	B-Method
Supervised	I-Method
Deep	I-Method
Detection	I-Method
Network	E-Method
(	O
WSDDN	B-Method
)	E-Method
,	O
which	O
presents	O
a	O
novel	O
weighted	B-Method
MIL	I-Method
pooling	I-Method
strategy	E-Method
and	O
combines	O
with	O
the	O
proposal	B-Method
objectness	I-Method
and	I-Method
spatial	I-Method
regulariser	E-Method
for	O
better	O
performance	O
.	O

Based	O
on	O
the	O
WSDDN	S-Method
,	O
Kantorov	S-Method
used	O
a	O
contrastive	B-Method
model	E-Method
to	O
consider	O
the	O
context	O
information	O
for	O
improvement	O
.	O

We	O
also	O
choose	O
the	O
WSDDN	S-Method
as	O
our	O
basic	O
network	O
,	O
but	O
we	O
combine	O
it	O
with	O
multi	B-Method
-	I-Method
stage	I-Method
classifier	I-Method
refinement	E-Method
,	O
and	O
propose	O
a	O
novel	O
OICR	B-Method
algorithm	E-Method
to	O
train	O
our	O
network	O
effectively	O
and	O
efficiently	O
,	O
which	O
can	O
boost	O
the	O
performance	O
significantly	O
.	O

Different	O
from	O
the	O
spatial	B-Method
regulariser	E-Method
in	O
WSDDN	S-Method
that	O
forces	O
features	O
of	O
highest	O
scoring	O
proposal	O
and	O
its	O
spatially	O
overlapped	O
proposals	O
to	O
be	O
the	O
same	O
,	O
our	O
OICR	S-Method
assumes	O
features	O
of	O
spatially	O
overlapped	O
proposals	O
are	O
in	O
the	O
same	O
manifold	O
,	O
which	O
is	O
more	O
reasonable	O
.	O

Experiments	O
on	O
Section	O
[	O
reference	O
]	O
demonstrate	O
that	O
our	O
strategy	O
can	O
obtain	O
more	O
superior	O
results	O
.	O

The	O
proposal	B-Method
labelling	I-Method
procedure	E-Method
is	O
also	O
related	O
to	O
the	O
semi	B-Method
-	I-Method
supervised	I-Method
label	I-Method
propagation	I-Method
method	E-Method
.	O

But	O
in	O
label	B-Task
propagation	E-Task
,	O
it	O
labels	O
data	O
according	O
to	O
the	O
similarity	O
among	O
labelled	O
and	O
unlabelled	O
data	O
,	O
while	O
we	O
use	O
spatial	O
overlap	O
as	O
the	O
metric	O
;	O
and	O
there	O
are	O
no	O
available	O
labelled	O
instances	O
for	O
propagation	S-Task
,	O
which	O
is	O
quite	O
different	O
from	O
semi	B-Method
-	I-Method
supervised	I-Method
methods	E-Method
.	O

Meanwhile	O
,	O
the	O
sharing	B-Method
representation	I-Method
strategy	E-Method
in	O
our	O
network	O
is	O
similar	O
to	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

Unlike	O
the	O
multi	B-Method
-	I-Method
task	I-Method
learning	E-Method
that	O
each	O
output	O
stream	O
has	O
their	O
own	O
relatively	O
independent	O
external	O
supervision	O
,	O
in	O
our	O
method	O
,	O
supervision	O
of	O
latter	O
streams	O
only	O
depends	O
on	O
the	O
outputs	O
from	O
their	O
preceding	O
streams	O
.	O

section	O
:	O
Method	O
The	O
overall	O
architecture	O
of	O
our	O
method	O
is	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Given	O
an	O
image	O
,	O
we	O
first	O
generate	O
about	O
object	O
proposals	O
by	O
Selective	B-Method
Search	E-Method
.	O

The	O
image	O
and	O
these	O
proposals	O
are	O
fed	O
into	O
some	O
convolutional	B-Method
(	I-Method
conv	I-Method
)	I-Method
layers	E-Method
with	O
Spatial	B-Method
Pyramid	I-Method
Pooling	I-Method
(	I-Method
SPP	I-Method
)	I-Method
layer	E-Method
to	O
produce	O
a	O
fixed	O
-	O
size	O
conv	O
feature	O
map	O
per	O
-	O
proposal	O
,	O
and	O
then	O
they	O
are	O
fed	O
into	O
two	O
fully	B-Method
connected	I-Method
(	I-Method
fc	I-Method
)	I-Method
layers	E-Method
to	O
generate	O
a	O
collection	O
of	O
proposal	O
feature	O
vectors	O
.	O

These	O
features	O
are	O
branched	O
into	O
different	O
streams	O
,	O
,	O
different	O
stages	O
:	O
the	O
first	O
one	O
is	O
the	O
MIDN	S-Method
to	O
train	O
a	O
basic	O
instance	B-Method
classifier	E-Method
and	O
others	O
refine	O
classifier	S-Method
.	O

Specially	O
,	O
supervision	O
for	O
classifier	B-Task
refinement	E-Task
is	O
decided	O
by	O
outputs	O
from	O
their	O
preceding	O
stages	O
,	O
,	O
supervision	O
of	O
the	O
first	O
refined	B-Method
classifier	E-Method
depends	O
on	O
the	O
output	O
from	O
the	O
basic	B-Method
classifier	E-Method
,	O
and	O
supervision	B-Method
of	I-Method
refined	I-Method
classifier	E-Method
depends	O
on	O
outputs	O
from	O
refined	B-Method
classifier	E-Method
.	O

In	O
this	O
section	O
,	O
we	O
will	O
introduce	O
the	O
chosen	O
basic	O
MIDN	S-Method
,	O
and	O
explain	O
our	O
OICR	B-Method
algorithm	E-Method
in	O
detail	O
.	O

subsection	O
:	O
Multiple	B-Method
instance	I-Method
detection	I-Method
network	E-Method
It	O
is	O
necessary	O
to	O
achieve	O
instance	O
-	O
level	O
supervision	O
to	O
train	O
refined	O
classifier	S-Method
,	O
yet	O
such	O
supervision	S-Method
is	O
unavailable	O
.	O

As	O
we	O
have	O
stated	O
before	O
,	O
the	O
top	O
-	O
scoring	O
proposal	O
by	O
instance	B-Method
classifiers	E-Method
and	O
its	O
adjacent	O
proposals	O
can	O
be	O
labelled	O
to	O
its	O
image	O
label	O
as	O
supervision	O
.	O

So	O
we	O
first	O
introduce	O
our	O
MIDN	S-Method
to	O
generate	O
the	O
basic	O
instance	B-Method
classifier	E-Method
.	O

There	O
are	O
many	O
possible	O
choices	O
to	O
achieve	O
this	O
.	O

Here	O
we	O
choose	O
the	O
method	O
by	O
Bilen	O
and	O
Vedaldi	O
which	O
proposes	O
a	O
weighted	B-Method
pooling	I-Method
strategy	E-Method
to	O
obtain	O
the	O
instance	B-Method
classifier	E-Method
,	O
for	O
its	O
effectiveness	O
and	O
implementation	O
convenience	O
.	O

Notice	O
that	O
our	O
network	O
is	O
independent	O
of	O
special	O
MIL	B-Method
methods	E-Method
,	O
so	O
any	O
method	O
that	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
could	O
be	O
embedded	O
into	O
our	O
network	O
.	O

As	O
shown	O
in	O
the	O
“	O
Multiple	B-Method
instance	I-Method
detection	I-Method
network	E-Method
”	O
block	O
of	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
proposal	O
features	O
are	O
branched	O
into	O
two	O
streams	O
to	O
produce	O
two	O
matrices	O
of	O
image	O
by	O
two	O
fc	B-Method
layers	E-Method
,	O
where	O
denotes	O
the	O
number	O
of	O
image	O
classes	O
and	O
denotes	O
the	O
number	O
of	O
proposals	O
.	O

Then	O
the	O
two	O
matrices	O
are	O
passing	O
through	O
two	O
softmax	B-Method
layer	E-Method
along	O
different	O
directions	O
:	O
and	O
.	O

The	O
proposal	O
scores	O
are	O
generated	O
by	O
element	B-Method
-	I-Method
wise	I-Method
product	E-Method
.	O

At	O
last	O
,	O
image	O
score	O
of	O
class	O
can	O
be	O
obtained	O
by	O
the	O
sum	O
over	O
all	O
proposals	O
:	O
.	O

The	O
interpretation	O
of	O
the	O
two	O
streams	B-Method
framework	E-Method
is	O
as	O
follows	O
.	O

The	O
is	O
the	O
probability	O
of	O
proposal	O
belonging	O
to	O
class	O
.	O

The	O
is	O
the	O
normalized	O
weight	O
that	O
indicates	O
the	O
contribution	O
of	O
proposal	O
to	O
image	O
being	O
classified	O
to	O
class	O
.	O

So	O
is	O
achieved	O
by	O
weighted	B-Method
sum	I-Method
pooling	E-Method
and	O
falls	O
in	O
the	O
range	O
of	O
.	O

Given	O
image	O
label	O
,	O
where	O
or	O
indicates	O
the	O
image	O
with	O
or	O
without	O
object	O
.	O

We	O
can	O
train	O
the	O
basic	O
instance	B-Method
classifier	E-Method
by	O
standard	O
multi	B-Method
-	I-Method
class	I-Method
cross	I-Method
entropy	I-Method
loss	E-Method
,	O
as	O
shown	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
then	O
the	O
instance	B-Method
classifier	E-Method
can	O
be	O
obtained	O
according	O
to	O
the	O
proposal	O
score	O
.	O

More	O
details	O
can	O
be	O
found	O
in	O
.	O

subsection	O
:	O
Online	B-Task
instance	I-Task
classifier	I-Task
refinement	E-Task
In	O
the	O
last	O
subsection	O
,	O
we	O
have	O
obtained	O
the	O
basic	O
instance	B-Method
classifier	E-Method
.	O

Here	O
we	O
will	O
expound	O
how	O
to	O
refine	O
instance	B-Method
classifiers	E-Method
online	O
.	O

A	O
natural	O
way	O
to	O
refine	O
classifier	S-Method
is	O
an	O
alternative	O
strategy	O
,	O
that	O
is	O
,	O
fixing	O
the	O
classifier	O
and	O
labelling	O
proposals	O
,	O
fixing	O
proposal	O
labels	O
and	O
training	O
the	O
classifier	S-Method
.	O

But	O
it	O
has	O
some	O
limitations	O
:	O
1	O
)	O
It	O
is	O
very	O
time	O
-	O
consuming	O
as	O
it	O
requires	O
training	O
the	O
classifier	S-Method
multiple	O
times	O
;	O
2	O
)	O
Training	O
different	O
classifiers	S-Method
in	O
different	O
refinement	O
steps	O
separately	O
may	O
harm	O
the	O
performance	O
because	O
it	O
hinders	O
the	O
process	O
to	O
benefit	O
from	O
the	O
shared	B-Method
representations	E-Method
.	O

Hence	O
,	O
we	O
integrate	O
the	O
basic	O
MIDN	S-Method
and	O
different	O
classifier	B-Method
refinement	I-Method
stages	E-Method
into	O
a	O
single	O
network	O
and	O
train	O
it	O
end	O
-	O
to	O
-	O
end	O
.	O

The	O
difficulty	O
is	O
how	O
to	O
obtain	O
instance	O
labels	O
for	O
refinement	S-Task
when	O
there	O
are	O
no	O
available	O
labelled	O
instances	O
.	O

To	O
deal	O
with	O
this	O
problem	O
,	O
we	O
propose	O
an	O
online	B-Method
labelling	I-Method
and	I-Method
refinement	I-Method
strategy	E-Method
.	O

Different	O
from	O
the	O
basic	O
instance	B-Method
classifier	E-Method
,	O
the	O
output	O
score	O
vector	O
of	O
proposal	O
for	O
refined	B-Method
classifier	E-Method
is	O
a	O
-	O
dimensional	O
vector	O
,	O
,	O
,	O
where	O
the	O
is	O
for	O
time	B-Task
refinement	E-Task
,	O
is	O
the	O
total	O
refinement	O
times	O
,	O
and	O
the	O
dimension	O
is	O
for	O
background	O
(	O
here	O
we	O
represent	O
the	O
proposal	O
score	O
vector	O
from	O
the	O
basic	B-Method
classifier	E-Method
as	O
)	O
.	O

The	O
is	O
obtained	O
by	O
passing	O
the	O
proposal	O
feature	O
vector	O
through	O
a	O
single	O
fc	B-Method
layer	E-Method
and	O
a	O
softmax	B-Method
over	I-Method
classes	I-Method
layer	E-Method
,	O
as	O
shown	O
in	O
the	O
“	O
Instance	B-Method
classifier	I-Method
refinement	E-Method
”	O
block	O
of	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Suppose	O
the	O
label	O
vector	O
for	O
proposal	O
is	O
.	O

In	O
each	O
training	O
iteration	O
,	O
after	O
the	O
forward	B-Method
process	E-Method
of	O
SGD	S-Method
,	O
we	O
can	O
get	O
a	O
set	O
of	O
proposal	O
scores	O
.	O

Then	O
we	O
can	O
obtain	O
the	O
supervision	O
of	O
refinement	O
time	O
according	O
to	O
.	O

There	O
are	O
many	O
possible	O
methods	O
to	O
obtain	O
instance	O
labels	O
using	O
,	O
,	O
labeling	O
an	O
instance	O
as	O
positive	O
if	O
its	O
score	O
exceeds	O
a	O
threshold	O
,	O
otherwise	O
as	O
negative	O
,	O
as	O
the	O
mi	B-Method
-	I-Method
SVM	E-Method
.	O

But	O
in	O
our	O
case	O
,	O
the	O
score	O
for	O
each	O
instance	O
is	O
changed	O
during	O
each	O
training	O
iteration	O
,	O
and	O
for	O
different	O
classes	O
,	O
using	O
the	O
same	O
threshold	O
may	O
not	O
be	O
suitable	O
,	O
thus	O
it	O
is	O
hard	O
to	O
settle	O
a	O
threshold	O
.	O

Here	O
we	O
choose	O
a	O
different	O
strategy	O
,	O
inspired	O
by	O
the	O
fact	O
that	O
highly	O
spatially	O
overlapped	O
instances	O
should	O
have	O
the	O
same	O
label	O
.	O

Suppose	O
an	O
image	O
has	O
class	O
label	O
,	O
we	O
first	O
select	O
proposal	O
with	O
highest	O
score	O
for	O
time	O
as	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
label	O
it	O
to	O
class	O
,	O
,	O
and	O
.	O

As	O
different	O
proposals	O
always	O
have	O
overlaps	O
,	O
and	O
proposals	O
with	O
high	O
overlap	O
should	O
belong	O
to	O
the	O
same	O
class	O
,	O
we	O
can	O
label	O
proposal	O
and	O
its	O
adjacent	O
proposals	O
to	O
class	O
for	O
refinement	S-Task
,	O
,	O
if	O
proposal	O
have	O
a	O
high	O
overlap	O
with	O
proposal	O
,	O
we	O
label	O
proposal	O
to	O
class	O
(	O
)	O
,	O
otherwise	O
we	O
label	O
proposal	O
as	O
background	O
(	O
)	O
.	O

Here	O
we	O
label	O
proposal	O
to	O
class	O
if	O
the	O
IoU	O
between	O
proposal	O
and	O
greater	O
than	O
a	O
threshold	O
which	O
is	O
determined	O
by	O
experiments	O
.	O

Meanwhile	O
,	O
if	O
there	O
is	O
no	O
object	O
in	O
the	O
image	O
,	O
we	O
set	O
all	O
.	O

Using	O
this	O
supervision	O
,	O
we	O
can	O
train	O
the	O
refined	B-Method
classifier	E-Method
based	O
on	O
the	O
loss	O
function	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Through	O
multiple	O
times	O
of	O
refinement	O
,	O
our	O
detector	O
can	O
detect	O
larger	O
parts	O
of	O
objects	O
gradually	O
.	O

[	O
t	O
]	O
Online	B-Method
instance	I-Method
classifier	I-Method
refinement	E-Method
[	O
1	O
]	O
Image	O
and	O
its	O
proposals	O
;	O
image	O
label	O
vector	O
;	O
refinement	O
times	O
.	O

Loss	O
weights	O
;	O
proposal	O
label	O
vectors	O
.	O

Where	O
and	O
.	O

Feed	O
and	O
its	O
proposals	O
into	O
the	O
network	O
to	O
produce	O
proposal	O
score	O
matrices	O
,	O
.	O

Set	O
all	O
elements	O
in	O
to	O
.	O

Set	O
all	O
and	O
.	O

Choose	O
the	O
top	O
-	O
scoring	O
proposal	O
by	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Compute	O
IoU	O
between	O
proposal	O
and	O
.	O

Set	O
and	O
.	O

Set	O
and	O
.	O

Actually	O
the	O
acquired	O
supervision	O
for	O
refining	B-Task
classifier	E-Task
is	O
very	O
noisy	O
,	O
especially	O
in	O
the	O
beginning	O
of	O
training	O
,	O
which	O
will	O
result	O
in	O
unstable	O
solutions	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
change	O
the	O
loss	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
to	O
a	O
weighted	B-Method
version	E-Method
,	O
as	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

where	O
is	O
the	O
loss	O
weight	O
and	O
can	O
be	O
acquired	O
by	O
the	O
line	B-Method
of	I-Method
Algorithm	E-Method
[	O
reference	O
]	O
.	O

The	O
explanation	O
of	O
such	O
choice	O
is	O
as	O
follows	O
.	O

In	O
the	O
beginning	O
of	O
training	O
,	O
the	O
is	O
small	O
,	O
hence	O
,	O
the	O
loss	S-Metric
is	O
also	O
small	O
.	O

As	O
a	O
consequence	O
,	O
the	O
performance	O
of	O
the	O
network	O
will	O
not	O
decrease	O
a	O
lot	O
though	O
good	O
positive	O
instances	O
can	O
not	O
be	O
found	O
.	O

Meanwhile	O
,	O
during	O
the	O
training	O
procedure	O
,	O
the	O
network	O
can	O
achieve	O
positive	O
instances	O
with	O
high	O
scores	O
easily	O
for	O
easy	O
bags	O
,	O
and	O
these	O
positive	O
instances	O
are	O
always	O
with	O
high	O
scores	O
,	O
,	O
is	O
large	O
.	O

On	O
the	O
contrary	O
,	O
it	O
is	O
difficult	O
to	O
get	O
positive	O
instances	O
for	O
difficult	O
bags	O
,	O
as	O
a	O
result	O
,	O
these	O
positive	O
instances	O
are	O
always	O
very	O
noisy	O
.	O

Nevertheless	O
,	O
the	O
refined	O
classifier	S-Method
will	O
not	O
deviate	O
from	O
the	O
correct	O
solution	O
a	O
lot	O
,	O
because	O
the	O
scores	O
of	O
these	O
noisy	O
positive	O
instances	O
are	O
relatively	O
low	O
,	O
,	O
is	O
small	O
.	O

To	O
make	O
the	O
OICR	B-Method
algorithm	E-Method
more	O
clear	O
,	O
we	O
summarize	O
the	O
process	O
to	O
obtain	O
supervision	O
in	O
Algorithm	O
[	O
reference	O
]	O
,	O
where	O
indicates	O
the	O
maximum	O
IoU	O
between	O
proposal	O
and	O
the	O
top	O
-	O
scoring	O
proposal	O
.	O

After	O
obtaining	O
supervision	O
and	O
loss	O
for	O
training	O
refined	B-Method
classifiers	E-Method
,	O
we	O
can	O
get	O
the	O
loss	O
of	O
our	O
overall	B-Method
network	E-Method
by	O
combining	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
as	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Through	O
optimizing	O
this	O
loss	O
function	O
,	O
we	O
can	O
integrate	O
the	O
basic	B-Method
network	E-Method
and	O
different	O
classifier	B-Method
refinement	I-Method
stages	E-Method
into	O
a	O
single	O
network	O
,	O
and	O
share	O
representations	O
among	O
different	O
stages	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Experimental	O
setup	O
In	O
this	O
section	O
we	O
will	O
perform	O
thorough	O
experiments	O
to	O
analyse	O
our	O
OICR	S-Method
and	O
its	O
components	O
for	O
weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
.	O

paragraph	O
:	O
Datasets	O
and	O
evaluation	B-Metric
measures	E-Metric
We	O
evaluate	O
our	O
method	O
on	O
the	O
challenging	O
PASCAL	B-Material
VOC	I-Material
2007	E-Material
and	O
2012	S-Material
datasets	O
which	O
have	O
and	O
images	O
respectively	O
for	O
object	O
classes	O
.	O

These	O
two	O
datasets	O
are	O
divided	O
into	O
train	O
,	O
val	O
,	O
and	O
test	O
sets	O
.	O

Here	O
we	O
choose	O
the	O
trainval	O
set	O
(	O
images	O
for	O
2007	O
and	O
for	O
2012	S-Material
)	O
to	O
train	O
our	O
network	O
.	O

As	O
we	O
focus	O
on	O
weakly	B-Task
supervised	I-Task
detection	E-Task
,	O
only	O
image	O
-	O
level	O
labels	O
are	O
utilized	O
during	O
training	O
.	O

For	O
testing	O
,	O
there	O
are	O
two	O
metrics	O
for	O
evaluation	S-Task
:	O
mAP	S-Metric
and	O
CorLoc	S-Metric
.	O

Average	B-Metric
Precision	E-Metric
(	O
AP	S-Metric
)	O
and	O
the	O
mean	B-Metric
of	I-Metric
AP	E-Metric
(	O
mAP	S-Metric
)	O
is	O
the	O
evaluation	B-Metric
metric	E-Metric
to	O
test	O
our	O
model	O
on	O
the	O
testing	O
set	O
,	O
which	O
follows	O
the	O
standard	O
PASCAL	B-Material
VOC	I-Material
protocol	E-Material
.	O

Correct	B-Metric
localization	E-Metric
(	O
CorLoc	S-Metric
)	O
is	O
to	O
test	O
our	O
model	O
on	O
the	O
training	O
set	O
measuring	O
the	O
localization	B-Metric
accuracy	E-Metric
.	O

All	O
these	O
two	O
metrics	O
are	O
based	O
on	O
the	O
PASCAL	B-Metric
criteria	E-Metric
,	O
,	O
IoU	O
0.5	O
between	O
ground	O
truths	O
and	O
predicted	O
boxes	O
.	O

paragraph	O
:	O
Implementation	O
details	O
Our	O
method	O
is	O
built	O
on	O
two	O
pre	B-Method
-	I-Method
trained	I-Method
ImageNet	I-Method
networks	E-Method
:	O
VGG	B-Method
M	E-Method
and	O
VGG16	S-Method
,	O
each	O
of	O
which	O
has	O
some	O
conv	B-Method
layers	E-Method
with	O
max	B-Method
-	I-Method
pooling	I-Method
layer	E-Method
and	O
three	O
fc	B-Method
layers	E-Method
.	O

We	O
replace	O
the	O
last	O
max	B-Method
-	I-Method
pooling	I-Method
layer	E-Method
of	O
the	O
two	O
models	O
by	O
SPP	B-Method
layer	E-Method
,	O
and	O
the	O
last	O
fc	B-Method
layer	E-Method
and	O
softmax	B-Method
loss	I-Method
layer	E-Method
by	O
the	O
layers	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

To	O
increase	O
the	O
feature	O
map	O
size	O
from	O
the	O
last	O
conv	O
layer	O
,	O
we	O
replace	O
the	O
penultimate	B-Method
max	I-Method
-	I-Method
pooling	I-Method
layer	E-Method
and	O
its	O
subsequent	O
conv	B-Method
layers	E-Method
by	O
the	O
dilated	B-Method
conv	I-Method
layers	E-Method
.	O

The	O
new	O
added	O
layers	O
are	O
initialized	O
using	O
Gaussian	B-Method
distributions	E-Method
with	O
-	O
mean	O
and	O
standard	O
deviations	O
.	O

Biases	O
are	O
initialized	O
to	O
.	O

During	O
training	S-Task
,	O
the	O
mini	O
-	O
batch	O
size	O
for	O
SGD	S-Method
is	O
set	O
to	O
,	O
and	O
the	O
learning	B-Metric
rate	E-Metric
is	O
set	O
to	O
for	O
the	O
first	O
K	O
iterations	O
and	O
then	O
decrease	O
to	O
in	O
the	O
following	O
K	O
iterations	O
.	O

The	O
momentum	O
and	O
weight	O
decay	O
are	O
set	O
to	O
and	O
respectively	O
.	O

As	O
we	O
have	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
Selective	B-Method
Search	E-Method
(	O
SS	B-Method
)	E-Method
is	O
adopted	O
to	O
generate	O
about	O
proposals	O
per	O
-	O
image	O
.	O

For	O
data	B-Task
augmentation	E-Task
,	O
we	O
use	O
five	O
image	O
scales	O
(	O
resize	O
the	O
shortest	O
side	O
to	O
one	O
of	O
these	O
scales	O
)	O
and	O
cap	O
the	O
longest	O
image	O
side	O
to	O
less	O
than	O
with	O
horizontal	O
flips	O
for	O
both	O
training	O
and	O
testing	S-Task
.	O

We	O
refine	O
instance	B-Method
classifier	E-Method
three	O
times	O
,	O
,	O
in	O
Section	O
[	O
reference	O
]	O
,	O
so	O
there	O
are	O
four	O
stages	O
in	O
total	O
.	O

The	O
IoU	O
threshold	O
in	O
the	O
line	O
of	O
Algorithm	O
[	O
reference	O
]	O
is	O
set	O
to	O
.	O

During	O
testing	O
,	O
the	O
mean	O
output	O
of	O
these	O
three	O
refined	B-Method
classifiers	E-Method
is	O
chosen	O
.	O

We	O
also	O
follow	O
the	O
to	O
train	O
a	O
supervised	B-Method
object	I-Method
detector	E-Method
by	O
choosing	O
top	O
-	O
scoring	O
proposals	O
given	O
by	O
our	O
method	O
as	O
pseudo	O
ground	O
truths	O
to	O
further	O
improve	O
our	O
results	O
.	O

Here	O
we	O
train	O
a	O
Fast	B-Method
RCNN	E-Method
(	O
FRCNN	S-Method
)	O
detector	O
using	O
the	O
VGG16	B-Method
model	E-Method
and	O
the	O
same	O
five	O
image	O
scales	O
(	O
horizontal	O
flips	O
only	O
in	O
training	O
)	O
.	O

SS	S-Method
is	O
also	O
chosen	O
for	O
proposal	B-Task
generation	E-Task
to	O
train	O
the	O
FRCNN	S-Method
.	O

Non	B-Method
-	I-Method
maxima	I-Method
suppression	E-Method
(	O
with	O
IoU	O
threshold	O
)	O
is	O
applied	O
to	O
compute	O
AP	S-Metric
and	O
CorLoc	O
.	O

Our	O
experiments	O
are	O
implemented	O
based	O
on	O
the	O
Caffe	B-Method
deep	I-Method
learning	I-Method
framework	E-Method
.	O

All	O
of	O
our	O
experiments	O
are	O
running	O
on	O
a	O
NVIDIA	B-Method
GTX	I-Method
TitanX	I-Method
GPU	E-Method
.	O

Codes	O
for	O
reproducing	O
the	O
results	O
are	O
available	O
at	O
.	O

subsection	O
:	O
Ablation	S-Task
experiments	O
We	O
first	O
conduct	O
some	O
ablation	O
experiments	O
to	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
training	B-Method
strategy	E-Method
,	O
including	O
the	O
influence	O
of	O
classifier	B-Method
refinement	E-Method
,	O
OICR	S-Method
,	O
weighted	B-Method
loss	E-Method
,	O
and	O
the	O
IoU	O
threshold	O
.	O

Without	O
loss	O
generality	O
,	O
we	O
only	O
perform	O
experiments	O
on	O
VOC	B-Material
2007	E-Material
and	O
use	O
the	O
VGG	B-Method
M	I-Method
model	E-Method
.	O

subsubsection	O
:	O
The	O
influence	O
of	O
instance	B-Method
classifier	I-Method
refinement	E-Method
As	O
in	O
the	O
blue	O
line	O
of	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
can	O
observe	O
that	O
compared	O
with	O
the	O
basic	O
network	O
,	O
even	O
just	O
refining	O
instance	B-Method
classifier	E-Method
one	O
time	O
can	O
boost	O
the	O
performance	O
a	O
lot	O
(	O
mAP	S-Metric
from	O
to	O
and	O
CorLoc	O
from	O
to	O
)	O
,	O
which	O
confirms	O
the	O
necessity	O
of	O
refinement	O
.	O

If	O
we	O
refine	O
the	O
classifier	O
multiple	O
times	O
,	O
the	O
results	O
can	O
be	O
improved	O
further	O
.	O

But	O
when	O
refinement	S-Method
is	O
implemented	O
too	O
many	O
times	O
,	O
the	O
performance	O
tends	O
to	O
be	O
saturated	O
(	O
the	O
improvement	O
from	O
2	O
times	O
to	O
3	O
times	O
is	O
small	O
)	O
.	O

Maybe	O
this	O
is	O
because	O
the	O
network	O
tends	O
to	O
converge	O
so	O
that	O
the	O
supervision	O
of	O
time	O
is	O
similar	O
to	O
time	O
.	O

In	O
the	O
rest	O
of	O
this	O
paper	O
we	O
only	O
refine	O
the	O
classifier	O
times	O
.	O

Notice	O
that	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
the	O
“	O
0	O
time	O
”	O
is	O
similar	O
to	O
the	O
WSDDN	S-Method
using	O
SS	S-Method
as	O
proposals	O
.	O

Our	O
result	O
is	O
a	O
little	O
worse	O
than	O
theirs	O
(	O
mAP	S-Metric
in	O
their	O
paper	O
)	O
,	O
due	O
to	O
the	O
different	O
implementing	O
platform	O
and	O
details	O
.	O

subsubsection	O
:	O
The	O
influence	O
of	O
OICR	S-Method
Fig	O
.	O

[	O
reference	O
]	O
compares	O
the	O
results	O
of	O
different	O
refinement	O
times	O
and	O
different	O
training	B-Method
strategies	E-Method
for	O
classifier	B-Task
refinement	E-Task
.	O

As	O
we	O
can	O
see	O
,	O
whether	O
for	O
our	O
OICR	B-Method
algorithm	E-Method
or	O
the	O
alternative	O
strategy	O
,	O
results	O
can	O
be	O
improved	O
by	O
refinement	O
.	O

More	O
importantly	O
,	O
compared	O
with	O
the	O
alternatively	B-Method
refinement	I-Method
strategy	E-Method
,	O
our	O
OICR	S-Method
can	O
boost	O
the	O
performance	O
consistently	O
and	O
significantly	O
,	O
which	O
confirms	O
the	O
necessity	O
of	O
sharing	O
representations	O
.	O

Meanwhile	O
,	O
our	O
method	O
can	O
also	O
reduce	O
the	O
training	B-Metric
time	E-Metric
a	O
lot	O
,	O
as	O
it	O
only	O
requires	O
to	O
train	O
a	O
single	O
model	O
instead	O
of	O
training	O
models	O
for	O
times	O
refinement	O
in	O
the	O
alternative	O
strategy	O
.	O

subsubsection	O
:	O
The	O
influence	O
of	O
weighted	B-Method
loss	E-Method
We	O
also	O
study	O
the	O
influence	O
of	O
our	O
weighted	B-Method
loss	E-Method
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

So	O
here	O
we	O
train	O
a	O
network	O
based	O
on	O
the	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

From	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
using	O
the	O
unweighted	O
loss	O
,	O
the	O
improvement	O
from	O
refinement	S-Task
is	O
very	O
scant	O
,	O
and	O
the	O
performance	O
is	O
even	O
worse	O
than	O
the	O
alternative	O
strategy	O
.	O

Using	O
the	O
weighted	B-Method
loss	E-Method
can	O
achieve	O
much	O
better	O
performance	O
,	O
which	O
confirms	O
our	O
theory	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
The	O
influence	O
of	O
IoU	O
threshold	O
In	O
previous	O
experiments	O
,	O
we	O
set	O
the	O
IoU	O
threshold	O
in	O
the	O
line	O
of	O
Algorithm	O
[	O
reference	O
]	O
to	O
.	O

Here	O
we	O
conduct	O
experiments	O
to	O
analyse	O
the	O
influence	O
of	O
.	O

As	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
outperforms	O
other	O
choices	O
,	O
and	O
the	O
results	O
are	O
not	O
very	O
sensitive	O
to	O
the	O
:	O
when	O
changing	O
from	O
to	O
,	O
the	O
performance	O
only	O
drops	O
a	O
little	O
(	O
mAP	S-Metric
from	O
to	O
,	O
CorLoc	O
maintains	O
)	O
.	O

Here	O
we	O
set	O
to	O
in	O
other	O
experiments	O
.	O

subsection	O
:	O
Comparison	O
with	O
other	O
methods	O
We	O
report	O
our	O
results	O
for	O
each	O
class	O
on	O
VOC	B-Material
2007	E-Material
and	O
2012	S-Material
in	O
Table	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
,	O
and	O
Table	O
[	O
reference	O
]	O
.	O

Compared	O
with	O
other	O
methods	O
,	O
our	O
method	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
using	O
single	O
model	O
,	O
and	O
even	O
outperforms	O
the	O
results	O
by	O
combining	O
multiple	O
different	O
models	O
.	O

Specially	O
,	O
our	O
methods	O
achieves	O
much	O
better	O
performance	O
than	O
the	O
method	O
by	O
Bilen	S-Method
and	O
Vedaldi	S-Method
using	O
the	O
same	O
CNN	S-Method
model	O
.	O

Notice	O
that	O
not	O
only	O
uses	O
the	O
weighted	B-Method
pooling	E-Method
as	O
we	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
but	O
also	O
combines	O
the	O
objectness	B-Method
measure	I-Method
of	I-Method
EdgeBoxes	E-Method
and	O
the	O
spatial	B-Method
regulariser	E-Method
,	O
which	O
is	O
much	O
complicated	O
than	O
our	O
basic	O
MIDN	S-Method
.	O

We	O
believe	O
that	O
our	O
performance	O
can	O
be	O
improved	O
by	O
choosing	O
better	O
basic	B-Method
detection	I-Method
network	E-Method
,	O
like	O
the	O
complete	B-Method
network	E-Method
in	O
and	O
using	O
the	O
context	O
information	O
.	O

As	O
reimplementing	O
their	O
method	O
completely	O
is	O
trivial	O
,	O
here	O
we	O
only	O
choose	O
the	O
simplest	O
architecture	O
in	O
.	O

Even	O
in	O
this	O
simplified	O
case	O
,	O
our	O
method	O
can	O
achieve	O
very	O
promising	O
results	O
.	O

We	O
also	O
show	O
some	O
visualization	O
comparisons	O
among	O
the	O
WSDDN	S-Method
,	O
the	O
WSDDN	B-Method
+	I-Method
context	E-Method
,	O
and	O
our	O
method	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
of	O
the	O
Supplementary	O
Material	O
.	O

Our	O
results	O
can	O
also	O
be	O
improved	O
by	O
combing	O
multiple	B-Method
models	E-Method
.	O

As	O
shown	O
in	O
the	O
tables	O
,	O
if	O
we	O
simply	O
sum	O
up	O
the	O
scores	O
produced	O
by	O
the	O
VGG	B-Method
M	I-Method
model	E-Method
and	O
VGG16	B-Method
model	E-Method
(	O
OICR	B-Method
-	I-Method
Ens	E-Method
.	O

in	O
tables	O
)	O
,	O
there	O
is	O
little	O
improvement	O
.	O

Also	O
,	O
as	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
train	O
a	O
FRCNN	S-Method
detector	O
using	O
top	O
-	O
scoring	O
proposals	O
produced	O
by	O
OICR	B-Method
-	I-Method
Ens	E-Method
.	O

as	O
ground	O
truths	O
(	O
OICR	B-Method
-	I-Method
Ens.	E-Method
+	O
FRCNN	S-Method
in	O
tables	O
)	O
.	O

As	O
we	O
can	O
see	O
,	O
the	O
performance	O
can	O
be	O
improved	O
further	O
.	O

Though	O
our	O
method	O
significantly	O
outperforms	O
other	O
methods	O
for	O
some	O
class	O
,	O
like	O
“	O
bicyle	O
”	O
,	O
“	O
bus	O
”	O
,	O
“	O
motorbike	O
”	O
,	O
etc	O
,	O
the	O
performance	O
is	O
poor	O
for	O
classes	O
like	O
“	O
cat	O
”	O
,	O
“	O
dog	O
”	O
,	O
and	O
“	O
person	O
”	O
.	O

For	O
analysis	O
,	O
we	O
visualize	O
some	O
success	B-Metric
and	I-Metric
failure	I-Metric
detection	E-Metric
results	O
on	O
VOC	B-Material
2007	E-Material
trainval	O
by	O
OICR	B-Method
-	I-Method
Ens	E-Method
.	O

,	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

We	O
can	O
observe	O
that	O
,	O
our	O
method	O
is	O
robust	O
to	O
the	O
size	O
and	O
aspect	O
of	O
objects	O
,	O
especially	O
for	O
rigid	O
objects	O
.	O

The	O
main	O
failures	O
for	O
these	O
rigid	O
objects	O
are	O
always	O
due	O
to	O
overlarge	O
boxes	O
that	O
not	O
only	O
contain	O
objects	O
,	O
but	O
also	O
include	O
their	O
adjacent	O
similar	O
objects	O
.	O

For	O
non	O
-	O
rigid	O
objects	O
like	O
“	O
cat	O
”	O
,	O
“	O
dog	O
”	O
,	O
and	O
“	O
person	O
”	O
,	O
they	O
are	O
always	O
with	O
great	O
deformation	O
,	O
while	O
there	O
is	O
less	O
deformation	O
of	O
their	O
most	O
representative	O
parts	O
(	O
like	O
head	O
)	O
,	O
so	O
our	O
detector	O
is	O
still	O
inclined	O
to	O
find	O
these	O
parts	O
.	O

An	O
ideal	O
solution	O
is	O
yet	O
wanted	O
because	O
there	O
is	O
still	O
room	O
for	O
improvement	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
algorithm	B-Method
framework	E-Method
for	O
weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
.	O

Different	O
from	O
traditional	O
approaches	O
in	O
this	O
field	O
,	O
our	O
method	O
integrates	O
a	O
basic	O
multiple	B-Method
instance	I-Method
detection	I-Method
network	E-Method
and	O
multi	O
-	O
stage	O
instance	B-Method
classifiers	E-Method
into	O
a	O
single	O
network	O
.	O

Moreover	O
,	O
we	O
propose	O
an	O
online	B-Method
instance	I-Method
classifier	I-Method
refinement	I-Method
algorithm	E-Method
to	O
train	O
the	O
proposed	O
network	O
end	O
-	O
to	O
-	O
end	O
.	O

Experiments	O
show	O
substantial	O
and	O
consistent	O
improvements	O
by	O
our	O
method	O
.	O

Our	O
learning	B-Method
algorithm	E-Method
is	O
potential	O
to	O
be	O
applied	O
in	O
many	O
other	O
weakly	B-Task
supervised	I-Task
visual	I-Task
learning	I-Task
tasks	E-Task
.	O

In	O
the	O
future	O
,	O
we	O
will	O
explore	O
other	O
cues	O
such	O
as	O
instance	O
visual	O
similarity	O
for	O
performing	O
instance	B-Task
classifier	I-Task
refinement	E-Task
better	O
.	O

paragraph	O
:	O
Acknowledgements	O
This	O
work	O
was	O
partly	O
supported	O
by	O
NSFC	O
(	O
No	O
.	O

61503145	O
,	O
No	O
.	O

61572207	O
,	O
No	O
.	O

61573160	O
)	O
and	O
the	O
CAST	O
Young	O
Talent	O
Supporting	O
Program	O
.	O

bibliography	O
:	O
References	O
Supplementary	O
Material	O
Here	O
are	O
the	O
supplementary	O
materials	O
for	O
“	O
Multiple	B-Method
Instance	I-Method
Detection	I-Method
Network	E-Method
with	O
Online	B-Method
Instance	I-Method
Classifier	I-Method
Refinement	E-Method
”	O
.	O

We	O
provide	O
detailed	O
per	O
-	O
class	O
results	O
on	O
VOC	O
2012	S-Material
,	O
and	O
some	O
visualization	O
comparisons	O
among	O
the	O
WSDDN	S-Method
,	O
the	O
WSDDN	B-Method
+	I-Method
context	E-Method
,	O
and	O
our	O
method	O
(	O
OICR	S-Method
)	O
.	O

section	O
:	O
Per	O
-	O
class	O
results	O
on	O
VOC	O
2012	S-Material
The	O
detailed	O
per	O
-	O
class	O
results	O
on	O
VOC	O
2012	S-Material
can	O
be	O
viewed	O
in	O
Supplementary	O
Table	O
[	O
reference	O
]	O
and	O
Supplementary	O
Table	O
[	O
reference	O
]	O
.	O

Obviously	O
our	O
method	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
a	O
large	O
margin	O
.	O

Similar	O
to	O
results	O
on	O
VOC	B-Material
2007	E-Material
,	O
our	O
results	O
are	O
better	O
than	O
for	O
rigid	O
objects	O
such	O
as	O
“	O
bicycle	O
”	O
,	O
“	O
car	O
”	O
,	O
and	O
“	O
motorbike	O
”	O
,	O
but	O
worse	O
than	O
for	O
non	O
-	O
rigid	O
objects	O
“	O
cat	O
”	O
,	O
“	O
dog	O
”	O
,	O
and	O
“	O
person	O
”	O
.	O

This	O
is	O
because	O
non	O
-	O
rigid	O
objects	O
are	O
always	O
with	O
great	O
deformation	O
.	O

Our	O
method	O
tends	O
to	O
detect	O
the	O
most	O
discriminative	O
parts	O
of	O
these	O
objects	O
(	O
like	O
head	O
)	O
.	O

The	O
considers	O
more	O
context	O
information	O
thus	O
can	O
deal	O
with	O
these	O
classes	O
better	O
.	O

section	O
:	O
Visualization	S-Task
comparisons	O
We	O
show	O
some	O
visualization	O
comparisons	O
among	O
the	O
WSDDN	S-Method
,	O
the	O
WSDDN	B-Method
+	I-Method
context	E-Method
,	O
and	O
our	O
method	O
in	O
Supplementary	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

From	O
this	O
visualization	O
and	O
results	O
from	O
tables	O
,	O
we	O
can	O
observe	O
that	O
for	O
classes	O
such	O
as	O
aeroplane	O
,	O
bike	O
,	O
car	O
,	O
,	O
our	O
method	O
tends	O
to	O
provide	O
more	O
accurate	O
detections	O
,	O
whereas	O
other	O
two	O
methods	O
sometimes	O
fails	O
to	O
produce	O
boxes	O
that	O
are	O
overlarge	O
or	O
only	O
contain	O
parts	O
of	O
objects	O
(	O
the	O
first	O
four	O
rows	O
in	O
Supplementary	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

But	O
for	O
some	O
classes	O
such	O
as	O
person	O
,	O
our	O
method	O
always	O
fails	O
to	O
detect	O
only	O
parts	O
of	O
objects	O
(	O
the	O
fifth	O
row	O
in	O
Supplementary	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Because	O
considering	O
context	O
information	O
sometimes	O
help	O
the	O
detection	S-Task
(	O
as	O
in	O
WSDDN	B-Task
+	I-Task
context	E-Task
)	O
,	O
we	O
believe	O
our	O
method	O
can	O
be	O
further	O
improved	O
by	O
incorporating	O
context	O
information	O
into	O
our	O
framework	O
.	O

All	O
these	O
three	O
methods	O
(	O
actually	O
almost	O
all	O
weakly	B-Task
supervised	I-Task
object	I-Task
detection	E-Task
methods	O
)	O
suffers	O
from	O
two	O
problems	O
:	O
producing	O
boxes	O
that	O
not	O
only	O
contain	O
the	O
target	O
object	O
but	O
also	O
include	O
their	O
adjacent	O
similar	O
objects	O
,	O
or	O
only	O
detecting	O
parts	O
of	O
object	O
for	O
objects	O
with	O
deformation	O
(	O
the	O
last	O
row	O
in	O
Supplementary	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

An	O
ideal	O
solution	O
for	O
these	O
problems	O
is	O
yet	O
wanted	O
.	O

document	O
:	O
FPNN	S-Method
:	O
Field	B-Method
Probing	I-Method
Neural	I-Method
Networks	E-Method
for	O
3D	B-Task
Data	E-Task
Building	O
discriminative	B-Method
representations	E-Method
for	O
3D	O
data	O
has	O
been	O
an	O
important	O
task	O
in	O
computer	B-Task
graphics	E-Task
and	O
computer	B-Task
vision	I-Task
research	E-Task
.	O

Convolutional	B-Method
Neural	I-Method
Networks	E-Method
(	O
CNNs	S-Method
)	O
have	O
shown	O
to	O
operate	O
on	O
2D	O
images	O
with	O
great	O
success	O
for	O
a	O
variety	O
of	O
tasks	O
.	O

Lifting	B-Method
convolution	I-Method
operators	E-Method
to	O
3D	O
(	O
3DCNNs	S-Method
)	O
seems	O
like	O
a	O
plausible	O
and	O
promising	O
next	O
step	O
.	O

Unfortunately	O
,	O
the	O
computational	B-Metric
complexity	E-Metric
of	O
3D	B-Method
CNNs	E-Method
grows	O
cubically	O
with	O
respect	O
to	O
voxel	O
resolution	O
.	O

Moreover	O
,	O
since	O
most	O
3D	B-Method
geometry	I-Method
representations	E-Method
are	O
boundary	B-Method
based	E-Method
,	O
occupied	O
regions	O
do	O
not	O
increase	O
proportionately	O
with	O
the	O
size	O
of	O
the	O
discretization	O
,	O
resulting	O
in	O
wasted	O
computation	O
.	O

In	O
this	O
work	O
,	O
we	O
represent	O
3D	O
spaces	O
as	O
volumetric	O
fields	O
,	O
and	O
propose	O
a	O
novel	O
design	O
that	O
employs	O
field	B-Method
probing	I-Method
filters	E-Method
to	O
efficiently	O
extract	O
features	O
from	O
them	O
.	O

Each	O
field	B-Method
probing	I-Method
filter	E-Method
is	O
a	O
set	O
of	O
probing	O
points	O
—	O
sensors	O
that	O
perceive	O
the	O
space	O
.	O

Our	O
learning	B-Method
algorithm	E-Method
optimizes	O
not	O
only	O
the	O
weights	O
associated	O
with	O
the	O
probing	O
points	O
,	O
but	O
also	O
their	O
locations	O
,	O
which	O
deforms	O
the	O
shape	O
of	O
the	O
probing	B-Method
filters	E-Method
and	O
adaptively	O
distributes	O
them	O
in	O
3D	O
space	O
.	O

The	O
optimized	O
probing	O
points	O
sense	O
the	O
3D	O
space	O
‘	O
‘	O
intelligently	O
’	O
’	O
,	O
rather	O
than	O
operating	O
blindly	O
over	O
the	O
entire	O
domain	O
.	O

We	O
show	O
that	O
field	B-Method
probing	E-Method
is	O
significantly	O
more	O
efficient	O
than	O
3DCNNs	S-Method
,	O
while	O
providing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
on	O
classification	B-Task
tasks	E-Task
for	O
3D	B-Task
object	I-Task
recognition	E-Task
benchmark	O
datasets	O
.	O

section	O
:	O
Introduction	O
Rapid	O
advances	O
in	O
3D	B-Method
sensing	I-Method
technology	E-Method
have	O
made	O
3D	O
data	O
ubiquitous	O
and	O
easily	O
accessible	O
,	O
rendering	O
them	O
an	O
important	O
data	O
source	O
for	O
high	B-Task
level	I-Task
semantic	I-Task
understanding	E-Task
in	O
a	O
variety	O
of	O
environments	O
.	O

The	O
semantic	B-Task
understanding	I-Task
problem	E-Task
,	O
however	O
,	O
remains	O
very	O
challenging	O
for	O
3D	O
data	O
as	O
it	O
is	O
hard	O
to	O
find	O
an	O
effective	O
scheme	O
for	O
converting	O
input	O
data	O
into	O
informative	O
features	O
for	O
further	O
processing	O
by	O
machine	B-Method
learning	I-Method
algorithms	E-Method
.	O

For	O
semantic	B-Task
understanding	I-Task
problems	E-Task
in	O
2D	O
images	O
,	O
deep	B-Method
CNNs	E-Method
have	O
been	O
widely	O
used	O
and	O
have	O
achieved	O
great	O
success	O
,	O
where	O
the	O
convolutional	B-Method
layers	E-Method
play	O
an	O
essential	O
role	O
.	O

They	O
provide	O
a	O
set	O
of	O
2D	B-Method
filters	E-Method
,	O
which	O
when	O
convolved	O
with	O
input	O
data	O
,	O
transform	O
the	O
data	O
to	O
informative	O
features	O
for	O
higher	O
level	B-Task
inference	E-Task
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
problem	O
of	O
learning	O
a	O
3D	B-Task
shape	I-Task
representation	E-Task
by	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
.	O

We	O
keep	O
two	O
goals	O
in	O
mind	O
when	O
designing	O
the	O
network	O
:	O
the	O
shape	O
features	O
should	O
be	O
discriminative	O
for	O
shape	B-Task
recognition	E-Task
and	O
efficient	O
for	O
extraction	S-Task
at	O
runtime	O
.	O

However	O
,	O
existing	O
3D	B-Task
CNN	E-Task
pipelines	O
that	O
simply	O
replace	O
the	O
conventional	O
2D	B-Method
filters	E-Method
by	O
3D	B-Method
ones	E-Method
,	O
have	O
difficulty	O
in	O
capturing	O
geometric	O
structures	O
with	O
sufficient	O
efficiency	O
.	O

The	O
input	O
to	O
these	O
3D	B-Method
CNNs	E-Method
are	O
voxelized	O
shapes	O
represented	O
by	O
occupancy	B-Method
grids	E-Method
,	O
in	O
direct	O
analogy	O
to	O
pixel	B-Method
array	I-Method
representation	E-Method
for	O
images	O
.	O

We	O
observe	O
that	O
the	O
computational	B-Metric
cost	E-Metric
of	O
3D	B-Method
convolution	E-Method
is	O
quite	O
high	O
,	O
since	O
convolving	O
3D	O
voxels	O
has	O
cubical	O
complexity	O
with	O
respect	O
to	O
spatial	O
resolution	O
,	O
one	O
order	O
higher	O
than	O
the	O
2D	O
case	O
.	O

Due	O
to	O
this	O
high	O
computational	B-Metric
cost	E-Metric
,	O
researchers	O
typically	O
choose	O
resolution	O
to	O
voxelize	O
shapes	O
,	O
which	O
is	O
significantly	O
lower	O
than	O
the	O
widely	O
adopted	O
resolution	O
for	O
processing	B-Task
images	E-Task
.	O

We	O
suspect	O
that	O
the	O
strong	O
artifacts	O
introduced	O
at	O
this	O
level	O
of	O
quantization	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
hinder	O
the	O
process	O
of	O
learning	O
effective	O
3D	B-Method
convolutional	I-Method
filters	E-Method
.	O

Two	O
significant	O
differences	O
between	O
2D	O
images	O
and	O
3D	O
shapes	O
interfere	O
with	O
the	O
success	O
of	O
directly	O
applying	O
2D	B-Method
CNNs	E-Method
on	O
3D	O
data	O
.	O

First	O
,	O
as	O
the	O
voxel	O
resolution	O
grows	O
,	O
the	O
grids	O
occupied	O
by	O
shape	O
surfaces	O
get	O
sparser	O
and	O
sparser	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
convolutional	B-Method
layers	E-Method
that	O
are	O
designed	O
for	O
2D	O
images	O
thereby	O
waste	O
much	O
computation	O
resource	O
in	O
such	O
a	O
setting	O
,	O
since	O
they	O
convolve	O
with	O
3D	O
blocks	O
that	O
are	O
largely	O
empty	O
and	O
a	O
large	O
portion	O
of	O
multiplications	O
are	O
with	O
zeros	O
.	O

Moreover	O
,	O
as	O
the	O
voxel	O
resolution	O
grows	O
,	O
the	O
local	O
3D	O
blocks	O
become	O
less	O
and	O
less	O
discriminative	O
.	O

To	O
capture	O
informative	O
features	O
,	O
long	O
range	O
connections	O
have	O
to	O
be	O
established	O
for	O
taking	O
distant	O
voxels	O
into	O
consideration	O
.	O

This	O
long	O
range	O
effect	O
demands	O
larger	O
3D	B-Method
filters	E-Method
,	O
which	O
yields	O
an	O
even	O
higher	O
computation	B-Metric
overhead	E-Metric
.	O

To	O
address	O
these	O
issues	O
,	O
we	O
represent	O
3D	O
data	O
as	O
3D	O
fields	O
,	O
and	O
propose	O
a	O
field	B-Method
probing	I-Method
scheme	E-Method
,	O
which	O
samples	O
the	O
input	O
field	O
by	O
a	O
set	O
of	O
probing	B-Method
filters	E-Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Each	O
probing	B-Method
filter	E-Method
is	O
composed	O
of	O
a	O
set	O
of	O
probing	O
points	O
which	O
determine	O
the	O
shape	O
and	O
location	O
of	O
the	O
filter	O
,	O
and	O
filter	O
weights	O
associated	O
with	O
probing	O
points	O
.	O

In	O
typical	O
CNNs	S-Method
,	O
only	O
the	O
filter	O
weights	O
are	O
trained	O
,	O
while	O
the	O
filter	O
shape	O
themselves	O
are	O
fixed	O
.	O

In	O
our	O
framework	O
,	O
due	O
to	O
the	O
usage	O
of	O
3D	B-Method
field	I-Method
representation	E-Method
,	O
both	O
the	O
weights	O
and	O
probing	O
point	O
locations	O
are	O
trainable	O
,	O
making	O
the	O
filters	O
highly	O
flexible	O
in	O
coupling	O
long	O
range	O
effects	O
and	O
adapting	O
to	O
the	O
sparsity	O
of	O
3D	O
data	O
when	O
it	O
comes	O
to	O
feature	B-Task
extraction	E-Task
.	O

The	O
computation	B-Metric
amount	E-Metric
of	O
our	O
field	B-Method
probing	I-Method
scheme	E-Method
is	O
determined	O
by	O
how	O
many	O
probing	B-Method
filters	E-Method
we	O
place	O
in	O
the	O
3D	O
space	O
,	O
and	O
how	O
many	O
probing	O
points	O
are	O
sampled	O
per	O
filter	O
.	O

Thus	O
,	O
the	O
computational	B-Metric
complexity	E-Metric
does	O
not	O
grow	O
as	O
a	O
function	O
of	O
the	O
input	O
resolution	O
.	O

We	O
found	O
that	O
a	O
small	O
set	O
of	O
field	B-Method
probing	I-Method
filters	E-Method
is	O
enough	O
for	O
sampling	O
sufficient	O
information	O
,	O
probably	O
due	O
to	O
the	O
sparsity	O
characteristic	O
of	O
3D	O
data	O
.	O

Intuitively	O
,	O
we	O
can	O
think	O
our	O
field	B-Method
probing	I-Method
scheme	E-Method
as	O
a	O
set	O
of	O
sensors	O
placed	O
in	O
the	O
space	O
to	O
collect	O
informative	O
signals	O
for	O
high	B-Task
level	I-Task
semantic	I-Task
tasks	E-Task
.	O

With	O
the	O
long	O
range	O
connections	O
between	O
the	O
sensors	O
,	O
global	O
overview	O
of	O
the	O
underlying	O
object	O
can	O
be	O
easily	O
established	O
for	O
effective	O
inference	S-Task
.	O

Moreover	O
,	O
the	O
sensors	O
are	O
‘	O
‘	O
smart	O
’	O
’	O
in	O
the	O
sense	O
that	O
they	O
learn	O
how	O
to	O
sense	O
the	O
space	O
(	O
by	O
optimizing	O
the	O
filter	O
weights	O
)	O
,	O
as	O
well	O
as	O
where	O
to	O
sense	O
(	O
by	O
optimizing	O
the	O
probing	O
point	O
locations	O
)	O
.	O

Note	O
that	O
the	O
intelligence	O
of	O
the	O
sensors	O
is	O
not	O
hand	O
-	O
crafted	O
,	O
but	O
solely	O
derived	O
from	O
data	O
.	O

We	O
evaluate	O
our	O
field	B-Method
probing	I-Method
based	I-Method
neural	I-Method
networks	E-Method
(	O
FPNN	S-Method
)	O
on	O
a	O
classification	B-Task
task	E-Task
on	O
ModelNet	B-Material
dataset	E-Material
,	O
and	O
show	O
that	O
they	O
match	O
the	O
performance	O
of	O
3DCNNs	S-Method
while	O
requiring	O
much	O
less	O
computation	O
,	O
as	O
they	O
are	O
designed	O
and	O
trained	O
to	O
respect	O
the	O
sparsity	O
of	O
3D	O
data	O
.	O

section	O
:	O
Related	O
Work	O
paragraph	O
:	O
3D	B-Method
Shape	I-Method
Descriptors	E-Method
.	O

3D	B-Method
shape	I-Method
descriptors	E-Method
lie	O
at	O
the	O
core	O
of	O
shape	B-Task
analysis	E-Task
and	O
a	O
large	O
variety	O
of	O
shape	B-Method
descriptors	E-Method
have	O
been	O
designed	O
in	O
the	O
past	O
few	O
decades	O
.	O

3D	O
shapes	O
can	O
be	O
converted	O
into	O
2D	O
images	O
and	O
represented	O
by	O
descriptors	S-Method
of	O
the	O
converted	O
images	O
.	O

3D	O
shapes	O
can	O
also	O
be	O
represented	O
by	O
their	O
inherent	O
statistical	O
properties	O
,	O
such	O
as	O
distance	B-Method
distribution	E-Method
and	O
spherical	B-Method
harmonic	I-Method
decomposition	E-Method
.	O

Heat	B-Method
kernel	I-Method
signatures	E-Method
extract	O
shape	O
descriptions	O
by	O
simulating	O
an	O
heat	B-Method
diffusion	I-Method
process	E-Method
on	O
3D	O
shapes	O
.	O

In	O
contrast	O
,	O
we	O
propose	O
an	O
approach	O
for	O
learning	O
the	O
shape	B-Method
descriptor	I-Method
extraction	I-Method
scheme	E-Method
,	O
rather	O
than	O
hand	O
-	O
crafting	O
it	O
.	O

paragraph	O
:	O
Convolutional	B-Method
Neural	I-Method
Networks	E-Method
.	O

The	O
architecture	O
of	O
CNN	S-Method
is	O
designed	O
to	O
take	O
advantage	O
of	O
the	O
2D	O
structure	O
of	O
an	O
input	O
image	O
(	O
or	O
other	O
2D	O
input	O
such	O
as	O
a	O
speech	O
signal	O
)	O
,	O
and	O
CNNs	S-Method
have	O
advanced	O
the	O
performance	O
records	O
in	O
most	O
image	B-Task
understanding	I-Task
tasks	E-Task
in	O
computer	B-Task
vision	E-Task
.	O

An	O
important	O
reason	O
for	O
this	O
success	O
is	O
that	O
by	O
leveraging	O
large	O
image	O
datasets	O
(	O
e.g.	O
,	O
ImageNet	O
)	O
,	O
general	B-Method
purpose	I-Method
image	I-Method
descriptors	E-Method
can	O
be	O
directly	O
learned	O
from	O
data	O
,	O
which	O
adapt	O
to	O
the	O
data	O
better	O
and	O
outperform	O
hand	O
-	O
crafted	O
features	O
.	O

Our	O
approach	O
follows	O
this	O
paradigm	O
of	O
feature	B-Method
learning	E-Method
,	O
but	O
is	O
specifically	O
designed	O
for	O
3D	O
data	O
coming	O
from	O
object	B-Method
surface	I-Method
representations	E-Method
.	O

paragraph	O
:	O
CNNs	S-Method
on	O
Depth	O
and	O
3D	O
Data	O
.	O

With	O
rapid	O
advances	O
in	O
3D	B-Task
sensing	I-Task
technology	E-Task
,	O
depth	O
has	O
became	O
available	O
as	O
an	O
additional	O
information	O
channel	O
beyond	O
color	O
.	O

Such	O
2.5D	O
data	O
can	O
be	O
represented	O
as	O
multiple	O
channel	O
images	O
,	O
and	O
processed	O
by	O
2D	B-Method
CNNs	E-Method
.	O

Wu	O
et	O
al	O
.	O

in	O
a	O
pioneering	O
paper	O
proposed	O
to	O
extend	O
2D	B-Method
CNNs	E-Method
to	O
process	O
3D	O
data	O
directly	O
(	O
3D	O
ShapeNets	O
)	O
.	O

A	O
similar	O
approach	O
(	O
VoxNet	S-Method
)	O
was	O
proposed	O
in	O
.	O

However	O
,	O
such	O
approaches	O
can	O
not	O
work	O
on	O
high	O
resolution	O
3D	O
data	O
,	O
as	O
the	O
computational	B-Metric
complexity	E-Metric
is	O
a	O
cubic	O
function	O
of	O
the	O
voxel	O
grid	O
resolution	O
.	O

Since	O
CNNs	S-Method
for	O
images	O
have	O
been	O
extensively	O
studied	O
,	O
3D	O
shapes	O
can	O
be	O
rendered	O
into	O
2D	O
images	O
,	O
and	O
be	O
represented	O
by	O
the	O
CNN	O
features	O
of	O
the	O
images	O
,	O
which	O
,	O
surprisingly	O
,	O
outperforms	O
any	O
3D	B-Task
CNN	E-Task
approaches	O
,	O
in	O
a	O
3D	B-Task
shape	I-Task
classification	I-Task
task	E-Task
.	O

Recently	O
,	O
Qi	O
et	O
al	O
.	O

presented	O
an	O
extensive	O
study	O
of	O
these	O
volumetric	B-Method
and	I-Method
multi	I-Method
-	I-Method
view	I-Method
CNNs	E-Method
and	O
refreshed	O
the	O
performance	O
records	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
feature	B-Method
learning	I-Method
approach	E-Method
that	O
is	O
specifically	O
designed	O
to	O
take	O
advantage	O
of	O
the	O
sparsity	O
of	O
3D	O
data	O
,	O
and	O
compare	O
against	O
results	O
reported	O
in	O
.	O

Note	O
that	O
our	O
method	O
was	O
designed	O
without	O
explicit	O
consideration	O
of	O
deformable	O
objects	O
,	O
which	O
is	O
a	O
purely	O
extrinsic	B-Method
construction	E-Method
.	O

While	O
3D	O
data	O
is	O
represented	O
as	O
meshes	O
,	O
neural	B-Method
networks	E-Method
can	O
benefit	O
from	O
intrinsic	O
constructions	O
to	O
learn	O
object	O
invariance	O
to	O
isometries	O
,	O
thus	O
require	O
less	O
training	O
data	O
for	O
handling	O
deformable	O
objects	O
.	O

Our	O
method	O
can	O
be	O
viewed	O
as	O
an	O
efficient	O
scheme	B-Method
of	I-Method
sparse	I-Method
coding	E-Method
.	O

The	O
learned	O
weights	O
of	O
each	O
probing	O
curve	O
can	O
be	O
interpreted	O
as	O
the	O
entries	O
of	O
the	O
coding	O
matrix	O
in	O
the	O
sparse	B-Method
coding	I-Method
framework	E-Method
.	O

Compared	O
with	O
conventional	O
sparse	B-Method
coding	E-Method
,	O
our	O
framework	O
is	O
not	O
only	O
computationally	O
more	O
tractable	O
,	O
but	O
also	O
enables	O
an	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
learning	I-Task
system	E-Task
.	O

section	O
:	O
Field	B-Method
Probing	I-Method
Neural	I-Method
Network	E-Method
subsection	O
:	O
Input	O
3D	O
Fields	O
We	O
study	O
the	O
3D	B-Task
shape	I-Task
classification	I-Task
problem	E-Task
by	O
employing	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
.	O

The	O
input	O
of	O
our	O
network	O
is	O
a	O
3D	O
vector	O
field	O
built	O
from	O
the	O
input	O
shape	O
and	O
the	O
output	O
is	O
an	O
object	O
category	O
label	O
.	O

3D	O
shapes	O
represented	O
as	O
meshes	O
or	O
point	O
clouds	O
can	O
be	O
converted	O
into	O
3D	O
distance	O
fields	O
.	O

Given	O
a	O
mesh	O
(	O
or	O
point	O
cloud	O
)	O
,	O
we	O
first	O
convert	O
it	O
into	O
a	O
binary	B-Method
occupancy	I-Method
grid	I-Method
representation	E-Method
,	O
where	O
the	O
binary	O
occupancy	O
value	O
in	O
each	O
grid	O
is	O
determined	O
by	O
whether	O
it	O
intersects	O
with	O
any	O
mesh	O
surface	O
(	O
or	O
contains	O
any	O
sample	O
point	O
)	O
.	O

Then	O
we	O
treat	O
the	O
occupied	O
cells	O
as	O
the	O
zero	O
level	O
set	O
of	O
a	O
surface	O
,	O
and	O
apply	O
a	O
distance	B-Method
transform	E-Method
to	O
build	O
a	O
3D	O
distance	O
field	O
,	O
which	O
is	O
stored	O
in	O
a	O
3D	O
array	O
indexed	O
by	O
,	O
where	O
,	O
and	O
is	O
the	O
resolution	O
of	O
the	O
distance	O
field	O
.	O

We	O
denote	O
the	O
distance	O
value	O
at	O
by	O
.	O

Note	O
that	O
represents	O
distance	O
values	O
at	O
discrete	O
grid	O
locations	O
.	O

The	O
distance	O
value	O
at	O
an	O
arbitrary	O
location	O
can	O
be	O
computed	O
by	O
standard	O
trilinear	B-Method
interpolation	E-Method
over	O
.	O

See	O
Figure	O
[	O
reference	O
]	O
for	O
an	O
illustration	O
of	O
the	O
3D	B-Method
data	I-Method
representations	E-Method
.	O

Similar	O
to	O
3D	O
distance	O
fields	O
,	O
other	O
3D	O
fields	O
,	O
such	O
as	O
normal	O
fields	O
,	O
,	O
and	O
,	O
can	O
also	O
be	O
used	O
for	O
representing	B-Task
shapes	E-Task
.	O

Note	O
that	O
the	O
normal	O
fields	O
can	O
be	O
derived	O
from	O
the	O
gradient	O
of	O
the	O
distance	O
field	O
:	O
where	O
.	O

Our	O
framework	O
can	O
employ	O
any	O
set	O
of	O
fields	O
as	O
input	O
,	O
as	O
long	O
as	O
the	O
gradients	O
can	O
be	O
computed	O
.	O

subsection	O
:	O
Field	B-Task
Probing	I-Task
Layers	E-Task
The	O
basic	O
modules	O
of	O
deep	B-Method
neural	I-Method
networks	E-Method
are	O
layers	O
,	O
which	O
gradually	O
convert	O
input	O
to	O
output	O
in	O
a	O
forward	O
pass	O
,	O
and	O
get	O
updated	O
during	O
a	O
backward	O
pass	O
through	O
the	O
Back	B-Method
-	I-Method
propagation	I-Method
mechanism	E-Method
.	O

The	O
key	O
contribution	O
of	O
our	O
approach	O
is	O
that	O
we	O
replace	O
the	O
convolutional	B-Method
layers	E-Method
in	O
CNNs	S-Method
by	O
field	B-Method
probing	I-Method
layers	E-Method
,	O
a	O
novel	O
component	O
that	O
uses	O
field	B-Method
probing	I-Method
filters	E-Method
to	O
efficiently	O
extract	O
features	O
from	O
the	O
3D	O
vector	O
field	O
.	O

They	O
are	O
composed	O
of	O
three	O
layers	O
:	O
Sensor	B-Method
layer	E-Method
,	O
DotProduct	B-Method
layer	E-Method
and	O
Gaussian	B-Method
layer	E-Method
.	O

The	O
Sensor	B-Method
layer	E-Method
is	O
responsible	O
for	O
collecting	O
the	O
signals	O
(	O
the	O
values	O
in	O
the	O
input	O
fields	O
)	O
at	O
the	O
probing	O
points	O
in	O
the	O
forward	O
pass	O
,	O
and	O
updating	O
the	O
probing	O
point	O
locations	O
in	O
the	O
backward	O
pass	O
.	O

The	O
DotProduct	B-Method
layer	E-Method
computes	O
the	O
dot	O
product	O
between	O
the	O
probing	B-Method
filter	E-Method
weights	O
and	O
the	O
signals	O
from	O
the	O
Sensor	B-Method
layer	E-Method
.	O

The	O
Gaussian	B-Method
layer	E-Method
is	O
an	O
utility	B-Method
layer	E-Method
that	O
transforms	O
distance	O
field	O
into	O
a	O
representation	O
that	O
is	O
more	O
friendly	O
for	O
numerical	B-Task
computation	E-Task
.	O

We	O
introduce	O
them	O
in	O
the	O
following	O
paragraphs	O
,	O
and	O
show	O
that	O
they	O
fit	O
well	O
for	O
training	O
a	O
deep	B-Method
network	E-Method
.	O

paragraph	O
:	O
Sensor	B-Method
Layer	E-Method
.	O

The	O
input	O
to	O
this	O
layer	O
is	O
a	O
3D	O
field	O
,	O
where	O
yields	O
a	O
channel	O
(	O
for	O
distance	O
field	O
and	O
for	O
normal	O
fields	O
)	O
vector	O
at	O
location	O
.	O

This	O
layer	O
contains	O
probing	B-Method
filters	E-Method
scattered	O
in	O
space	O
,	O
each	O
with	O
probing	O
points	O
.	O

The	O
parameters	O
of	O
this	O
layer	O
are	O
the	O
locations	O
of	O
all	O
probing	O
points	O
,	O
where	O
indexes	O
the	O
filter	O
and	O
indexes	O
the	O
probing	O
point	O
within	O
each	O
filter	O
.	O

This	O
layer	O
simply	O
outputs	O
the	O
vector	O
at	O
the	O
probing	O
points	O
.	O

The	O
output	O
of	O
this	O
layer	O
forms	O
a	O
data	O
chunk	O
of	O
size	O
.	O

The	O
gradient	O
of	O
this	O
function	O
can	O
be	O
evaluated	O
by	O
numerical	B-Task
computation	E-Task
,	O
which	O
will	O
be	O
used	O
for	O
updating	O
the	O
locations	O
of	O
probing	O
points	O
in	O
the	O
back	B-Task
-	I-Task
propagation	I-Task
process	E-Task
.	O

This	O
formal	O
definition	O
emphasizes	O
why	O
we	O
need	O
the	O
input	O
being	O
represented	O
as	O
3D	O
fields	O
:	O
the	O
gradients	O
computed	O
from	O
the	O
input	O
fields	O
are	O
the	O
forces	O
to	O
push	O
the	O
probing	O
points	O
towards	O
more	O
informative	O
locations	O
until	O
they	O
converge	O
to	O
a	O
local	O
optimum	O
.	O

paragraph	O
:	O
DotProduct	B-Method
Layer	E-Method
.	O

The	O
input	O
to	O
this	O
layer	O
is	O
the	O
output	O
of	O
the	O
Sensor	B-Method
layer	E-Method
—	O
a	O
data	O
chunk	O
of	O
size	O
,	O
denoted	O
as	O
.	O

The	O
parameters	O
of	O
DotProduct	B-Method
layer	E-Method
are	O
the	O
filter	O
weights	O
associated	O
with	O
probing	O
points	O
,	O
i.e.	O
,	O
there	O
are	O
filters	O
,	O
each	O
of	O
length	O
,	O
in	O
channels	O
.	O

We	O
denote	O
the	O
set	O
of	O
parameters	O
as	O
.	O

The	O
function	O
at	O
this	O
layer	O
computes	O
a	O
dot	O
product	O
between	O
and	O
,	O
and	O
outputs	O
—	O
a	O
-	O
dimensional	O
vector	O
,	O
and	O
the	O
gradient	O
for	O
the	O
backward	O
pass	O
is	O
:	O
Typical	O
convolution	S-Method
encourages	O
weight	O
sharing	O
within	O
an	O
image	O
patch	O
by	O
‘	O
‘	O
zipping	O
’	O
’	O
the	O
patch	O
into	O
a	O
single	O
value	O
for	O
upper	O
layers	O
by	O
a	O
dot	B-Method
production	E-Method
between	O
the	O
patch	O
and	O
a	O
2D	B-Method
filter	E-Method
.	O

Our	O
DotProduct	B-Method
layer	E-Method
shares	O
the	O
same	O
‘	O
‘	O
zipping	O
’	O
’	O
idea	O
,	O
which	O
facilitates	O
to	O
fully	O
connect	O
it	O
:	O
probing	O
points	O
are	O
grouped	O
into	O
probing	B-Method
filters	E-Method
to	O
generate	O
output	O
with	O
lower	O
dimensionality	O
.	O

Another	O
option	O
in	O
designing	O
convolutional	B-Method
layers	E-Method
is	O
to	O
decide	O
whether	O
their	O
weights	O
should	O
be	O
shared	O
across	O
different	O
spatial	O
locations	O
.	O

In	O
2D	B-Method
CNNs	E-Method
,	O
these	O
parameters	O
are	O
usually	O
shared	O
when	O
processing	O
general	O
images	O
.	O

In	O
our	O
case	O
,	O
we	O
opt	O
not	O
to	O
share	O
the	O
weights	O
,	O
as	O
information	O
is	O
not	O
evenly	O
distributed	O
in	O
3D	O
space	O
,	O
and	O
we	O
encourage	O
our	O
probing	B-Method
filters	E-Method
to	O
individually	O
deviate	O
for	O
adapting	O
to	O
the	O
data	O
.	O

paragraph	O
:	O
Gaussian	B-Method
Layer	E-Method
.	O

Samples	O
in	O
locations	O
distant	O
to	O
the	O
object	O
surface	O
are	O
associated	O
with	O
large	O
distance	O
values	O
from	O
the	O
distance	O
field	O
.	O

Directly	O
feeding	O
them	O
into	O
the	O
DotProduct	B-Method
layer	E-Method
does	O
not	O
converge	O
and	O
thus	O
does	O
not	O
yield	O
reasonable	O
performance	O
.	O

To	O
emphasize	O
the	O
importance	O
of	O
samples	O
in	O
the	O
vicinity	O
of	O
the	O
object	O
surface	O
,	O
we	O
apply	O
a	O
Gaussian	B-Method
transform	E-Method
(	O
inverse	B-Method
exponential	E-Method

)	O
on	O
the	O
distances	O
so	O
that	O
regions	O
approaching	O
the	O
zero	O
surface	O
have	O
larger	O
weights	O
while	O
distant	O
regions	O
matter	O
less	O
.	O

.	O


We	O
implement	O
this	O
transform	O
with	O
a	O
Gaussian	B-Method
layer	E-Method
.	O

The	O
input	O
is	O
the	O
output	O
values	O
of	O
the	O
Sensor	B-Method
layer	E-Method
.	O

Let	O
us	O
assume	O
the	O
values	O
are	O
,	O
then	O
this	O
layer	O
applies	O
an	O
element	B-Method
-	I-Method
wise	I-Method
Gaussian	I-Method
transform	E-Method
,	O
and	O
the	O
gradient	O
is	O
for	O
the	O
backward	O
pass	O
.	O

paragraph	O
:	O
Complexity	S-Metric
of	O
Field	O
Probing	O
Layers	O
.	O

The	O
complexity	S-Metric
of	O
field	B-Method
probing	I-Method
layers	E-Method
is	O
,	O
where	O
is	O
the	O
number	O
of	O
probing	B-Method
filters	E-Method
,	O
is	O
the	O
number	O
of	O
probing	O
points	O
on	O
each	O
filter	O
,	O
and	O
is	O
the	O
number	O
of	O
input	O
fields	O
.	O

The	O
complexity	S-Metric
of	O
the	O
convolutional	B-Method
layer	E-Method
is	O
,	O
where	O
is	O
the	O
3D	O
kernel	O
size	O
,	O
is	O
the	O
output	O
channel	O
number	O
,	O
and	O
is	O
the	O
number	O
of	O
the	O
sliding	O
locations	O
for	O
each	O
dimension	O
.	O

In	O
field	B-Task
probing	I-Task
layers	E-Task
,	O
we	O
typically	O
use	O
,	O
,	O
and	O
(	O
distance	O
and	O
normal	O
fields	O
)	O
,	O
while	O
in	O
3D	B-Task
CNN	E-Task
,	O
and	O
.	O

Compared	O
with	O
convolutional	B-Method
layers	E-Method
,	O
field	B-Method
probing	I-Method
layers	E-Method
save	O
a	O
majority	O
of	O
computation	O
(	O
)	O
,	O
as	O
the	O
probing	B-Method
filters	E-Method
in	O
field	B-Method
probing	I-Method
layers	E-Method
are	O
capable	O
of	O
learning	O
where	O
to	O
‘	O
‘	O
sense	O
’	O
’	O
,	O
whereas	O
convolutional	B-Method
layers	E-Method
exhaustively	O
examine	O
everywhere	O
by	O
sliding	O
the	O
3D	O
kernels	O
.	O

paragraph	O
:	O
Initialization	B-Task
of	I-Task
Field	I-Task
Probing	I-Task
Layers	E-Task
.	O

There	O
are	O
two	O
sets	O
of	O
parameters	O
:	O
the	O
probing	O
point	O
locations	O
and	O
the	O
weights	O
associated	O
with	O
them	O
.	O

To	O
encourage	O
the	O
probing	O
points	O
to	O
explore	O
as	O
many	O
potential	O
locations	O
as	O
possible	O
,	O
we	O
initialize	O
them	O
to	O
be	O
widely	O
distributed	O
in	O
the	O
input	O
fields	O
.	O

We	O
first	O
divide	O
the	O
space	O
into	O
grids	O
and	O
then	O
generate	O
filters	S-Method
in	O
each	O
grid	O
.	O

Each	O
filter	O
is	O
initialized	O
as	O
a	O
line	O
segment	O
with	O
a	O
random	O
orientation	O
,	O
a	O
random	O
length	O
in	O
(	O
we	O
use	O
by	O
default	O
)	O
,	O
and	O
a	O
random	O
center	O
point	O
within	O
the	O
grid	O
it	O
belongs	O
to	O
(	O
Figure	O
[	O
reference	O
]	O
left	O
)	O
.	O

Note	O
that	O
a	O
probing	B-Method
filter	E-Method
spans	O
distantly	O
in	O
the	O
3D	O
space	O
,	O
so	O
they	O
capture	O
long	O
range	O
effects	O
well	O
.	O

This	O
is	O
a	O
property	O
that	O
distinguishes	O
our	O
design	O
from	O
those	O
convolutional	B-Method
layers	E-Method
,	O
as	O
they	O
have	O
to	O
increase	O
the	O
kernel	O
size	O
to	O
capture	O
long	O
range	O
effects	O
,	O
at	O
the	O
cost	O
of	O
increased	O
complexity	S-Metric
.	O

The	O
weights	O
of	O
field	B-Method
probing	I-Method
filters	E-Method
are	O
initialized	O
by	O
the	O
Xavier	B-Method
scheme	E-Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
right	O
,	O
weights	O
for	O
distance	O
field	O
are	O
visualized	O
by	O
probing	O
point	O
colors	O
and	O
weights	O
for	O
normal	O
fields	O
by	O
arrows	O
attached	O
to	O
each	O
probing	O
point	O
.	O

paragraph	O
:	O
FPNN	S-Method
Architecture	O
and	O
Usage	O
.	O

Field	B-Method
probing	I-Method
layers	E-Method
transform	O
input	O
3D	O
fields	O
into	O
an	O
intermediate	B-Method
representation	E-Method
,	O
which	O
can	O
further	O
be	O
processed	O
and	O
eventually	O
linked	O
to	O
task	O
specific	O
loss	O
layers	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

To	O
further	O
encourage	O
long	O
range	O
connections	O
,	O
we	O
feed	O
the	O
output	O
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
into	O
fully	B-Method
connected	I-Method
layers	E-Method
.	O

The	O
advantage	O
of	O
long	O
range	O
connections	O
makes	O
it	O
possible	O
to	O
stick	O
with	O
a	O
small	O
number	O
of	O
probing	B-Method
filters	E-Method
,	O
while	O
the	O
small	O
number	O
of	O
probing	B-Method
filters	E-Method
makes	O
it	O
possible	O
to	O
directly	O
use	O
fully	O
connected	O
layers	O
.	O

Object	B-Task
classification	E-Task
is	O
widely	O
used	O
in	O
computer	B-Task
vision	E-Task
as	O
a	O
testbed	O
for	O
evaluating	O
neural	B-Method
network	I-Method
designs	E-Method
,	O
and	O
the	O
neural	B-Method
network	I-Method
parameters	E-Method
learned	O
from	O
this	O
task	O
may	O
be	O
transferred	O
to	O
other	O
high	B-Task
-	I-Task
level	I-Task
understanding	I-Task
tasks	E-Task
such	O
as	O
object	B-Task
retrieval	E-Task
and	O
scene	B-Task
parsing	E-Task
.	O

Thus	O
we	O
choose	O
3D	B-Task
object	I-Task
classification	E-Task
as	O
the	O
task	O
for	O
evaluating	O
our	O
FPNN	S-Method
.	O

section	O
:	O
Results	O
and	O
Discussions	O
subsection	O
:	O
Timing	O
We	O
implemented	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
in	O
Caffe	S-Method
.	O

The	O
Sensor	B-Method
layer	E-Method
is	O
parallelized	O
by	O
assigning	O
computation	O
on	O
each	O
probing	O
point	O
to	O
one	O
GPU	O
thread	O
,	O
and	O
DotProduct	B-Method
layer	E-Method
by	O
assigning	O
computation	O
on	O
each	O
probing	B-Method
filter	E-Method
to	O
one	O
GPU	O
thread	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
run	B-Metric
time	I-Metric
comparison	E-Metric
between	O
convonlutional	B-Method
layers	E-Method
and	O
field	B-Method
probing	I-Method
layers	E-Method
on	O
different	O
input	O
resolutions	O
.	O

The	O
computation	B-Metric
cost	E-Metric
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
is	O
agnostic	O
to	O
input	O
resolutions	O
,	O
the	O
slight	O
increase	O
of	O
the	O
run	B-Metric
time	E-Metric
on	O
higher	O
resolution	O
is	O
due	O
to	O
GPU	O
memory	O
latency	O
introduced	O
by	O
the	O
larger	O
3D	O
fields	O
.	O

Note	O
that	O
the	O
convolutional	B-Method
layers	E-Method
in	O
are	O
based	O
on	O
highly	O
optimized	O
cuBlas	B-Method
library	E-Method
from	O
NVIDIA	S-Method
,	O
while	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
are	O
implemented	O
with	O
our	O
naive	O
parallelism	O
,	O
which	O
is	O
likely	O
to	O
be	O
further	O
improved	O
.	O

subsection	O
:	O
Datasets	O
and	O
Evaluation	B-Metric
Protocols	E-Metric
We	O
use	O
ModelNet40	S-Method
(	O
12	O
,	O
311	O
models	O
from	O
40	O
categories	O
,	O
training	O
/	O
testing	O
split	O
with	O
9	O
,	O
843	O
/	O
2	O
,	O
468	O
models	O
)	O
—	O
the	O
standard	O
benchmark	O
for	O
3D	B-Task
object	I-Task
classification	I-Task
task	E-Task
,	O
in	O
our	O
experiments	O
.	O

Models	O
in	O
this	O
dataset	O
are	O
already	O
aligned	O
with	O
a	O
canonical	O
orientation	O
.	O

For	O
3D	B-Task
object	I-Task
recognition	I-Task
scenarios	E-Task
in	O
real	O
world	O
,	O
the	O
gravity	O
direction	O
can	O
often	O
be	O
captured	O
by	O
the	O
sensor	O
,	O
but	O
the	O
horizontal	O
‘	O
‘	O
facing	O
’	O
’	O
direction	O
of	O
the	O
objects	O
are	O
unknown	O
.	O

We	O
augment	O
ModelNet40	B-Material
data	E-Material
by	O
randomly	O
rotating	O
the	O
shapes	O
horizontally	O
.	O

Note	O
that	O
this	O
is	O
done	O
for	O
both	O
training	O
and	O
testing	O
samples	O
,	O
thus	O
in	O
the	O
testing	O
phase	O
,	O
the	O
orientation	O
of	O
the	O
inputs	O
are	O
unknown	O
.	O

This	O
allows	O
us	O
to	O
assess	O
how	O
well	O
the	O
trained	O
network	O
perform	O
on	O
real	O
world	O
data	O
.	O

subsection	O
:	O
Performance	O
of	O
Field	B-Method
Probing	I-Method
Layers	E-Method
We	O
train	O
our	O
FPNN	S-Method
iterations	O
on	O
distance	O
field	O
with	O
batch	O
size	O
.	O

,	O
with	O
SGD	B-Method
solver	E-Method
,	O
learning	B-Metric
rate	E-Metric
,	O
momentum	O
,	O
and	O
weight	B-Method
decay	E-Method
.	O

Trying	O
to	O
study	O
the	O
performance	O
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
separately	O
,	O
we	O
build	O
up	O
an	O
FPNN	S-Method
with	O
only	O
one	O
fully	B-Method
connected	I-Method
layer	E-Method
that	O
converts	O
the	O
output	O
of	O
field	B-Method
probing	I-Method
layers	E-Method
into	O
the	O
representation	O
for	O
softmax	B-Task
classification	I-Task
loss	E-Task
(	O
1	O
-	O
FC	O
setting	O
)	O
.	O

Batch	B-Method
normalization	E-Method
and	O
rectified	B-Method
-	I-Method
linear	I-Method
unit	E-Method
are	O
used	O
in	O
-	O
between	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
and	O
the	O
fully	B-Method
connected	I-Method
layer	E-Method
for	O
reducing	O
internal	O
covariate	O
shift	O
and	O
introducing	O
non	O
-	O
linearity	O
.	O

We	O
train	O
the	O
network	O
without	O
/	O
with	O
updating	O
the	O
field	O
probing	O
layer	O
parameters	O
.	O

We	O
show	O
their	O
top	O
-	O
1	O
accuracy	S-Metric
on	O
3D	B-Task
object	I-Task
classification	I-Task
task	E-Task
on	O
dataset	O
with	O
single	O
testing	O
view	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
is	O
clear	O
that	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
learned	O
to	O
sense	O
the	O
input	O
field	O
more	O
intelligently	O
,	O
with	O
a	O
performance	O
gain	O
from	O
to	O
.	O

Note	O
that	O
,	O
what	O
achieved	O
by	O
this	O
simple	O
network	O
,	O
,	O
is	O
already	O
better	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
3DCNN	S-Method
before	O
(	O
in	O
and	O
in	O
)	O
.	O

We	O
also	O
evaluate	O
the	O
performance	O
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
in	O
the	O
context	O
of	O
a	O
deeper	O
FPNN	S-Method
,	O
where	O
four	O
fully	B-Method
connected	I-Method
layers	E-Method
,	O
with	O
in	O
-	O
between	O
batch	O
normalization	O
,	O
rectified	O
-	O
linear	O
unit	O
and	O
Dropout	O
layers	O
,	O
are	O
used	O
(	O
4	B-Method
-	I-Method
FCs	E-Method
setting	O
)	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
deeper	O
FPNN	S-Method
performs	O
better	O
,	O
while	O
the	O
gap	O
between	O
with	O
and	O
without	O
field	O
probing	O
layers	O
,	O
,	O
is	O
smaller	O
than	O
that	O
in	O
one	O
fully	O
connected	O
FPNN	S-Method
setting	O
.	O

This	O
is	O
not	O
surprising	O
,	O
as	O
the	O
additional	O
fully	O
connected	O
layers	O
,	O
with	O
many	O
parameters	O
introduced	O
,	O
have	O
strong	O
learning	O
capability	O
.	O

The	O
performance	O
gap	O
introduced	O
by	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
is	O
a	O
precious	O
extra	O
over	O
a	O
strong	O
baseline	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
in	O
both	O
settings	O
(	O
1	O
-	O
FC	O
and	O
4	B-Method
-	I-Method
FCs	E-Method
)	O
,	O
our	O
FPNNs	S-Method
provides	O
reasonable	O
performance	O
even	O
without	O
optimizing	O
the	O
field	O
probing	O
layers	O
.	O

This	O
confirms	O
that	O
long	O
range	O
connections	O
among	O
the	O
sensors	O
are	O
beneficial	O
.	O

Furthermore	O
,	O
we	O
evaluate	O
our	O
FPNNs	S-Method
with	O
multiple	O
input	O
fields	O
(	O
+	O
NF	S-Method
setting	O
)	O
.	O

We	O
did	O
not	O
only	O
employ	O
distance	O
fields	O
,	O
but	O
also	O
normal	O
fields	O
for	O
our	O
probing	O
layers	O
and	O
found	O
a	O
consistent	O
performance	O
gain	O
for	O
both	O
of	O
the	O
aforementioned	O
FPNNs	S-Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Since	O
normal	O
fields	O
are	O
derived	O
from	O
distance	O
fields	O
,	O
the	O
same	O
group	O
of	O
probing	B-Method
filters	E-Method
are	O
used	O
for	O
both	O
fields	O
.	O

Employing	O
multiple	O
fields	O
in	O
the	O
field	O
probing	O
layers	O
with	O
different	O
groups	O
of	O
filters	O
potentially	O
enables	O
even	O
higher	O
performance	O
.	O

paragraph	O
:	O
Robustness	S-Metric
Against	O
Spatial	O
Perturbations	O
.	O

We	O
evaluate	O
our	O
FPNNs	S-Method
on	O
different	O
levels	O
of	O
spatial	O
perturbations	O
,	O
and	O
summarize	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
where	O
indicates	O
random	O
horizontal	O
rotation	O
,	O
indicates	O
plus	O
a	O
small	O
random	O
rotation	O
(	O
,	O
)	O
in	O
the	O
other	O
two	O
directions	O
,	O
indicates	O
random	O
translations	O
within	O
range	O
(	O
,	O
)	O
of	O
the	O
object	O
size	O
in	O
all	O
directions	O
,	O
indicates	O
random	O
scaling	O
within	O
range	O
(	O
,	O
)	O
in	O
all	O
directions	O
.	O

and	O
shares	O
the	O
same	O
notations	O
,	O
but	O
with	O
even	O
stronger	O
rotation	O
and	O
translation	O
,	O
and	O
are	O
used	O
in	O
for	O
evaluating	O
the	O
performance	O
of	O
.	O

Note	O
that	O
such	O
perturbations	O
are	O
done	O
on	O
both	O
training	O
and	O
testing	O
samples	O
.	O

It	O
is	O
clear	O
that	O
our	O
FPNNs	S-Method
are	O
robust	O
against	O
spatial	O
perturbations	O
.	O

paragraph	O
:	O
Advantage	O
of	O
Long	O
Range	O
Connections	O
.	O

We	O
evaluate	O
our	O
FPNNs	S-Method
with	O
different	O
range	O
parameters	O
used	O
in	O
initializing	O
the	O
probing	B-Method
filters	E-Method
,	O
and	O
summarize	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Note	O
that	O
since	O
the	O
output	O
dimensionality	O
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
is	O
low	O
enough	O
to	O
be	O
directly	O
feed	O
into	O
fully	O
connected	O
layers	O
,	O
distant	O
sensor	O
information	O
is	O
directly	O
coupled	O
by	O
them	O
.	O

This	O
is	O
a	O
desirable	O
property	O
,	O
however	O
,	O
it	O
poses	O
the	O
difficulty	O
to	O
study	O
the	O
advantage	O
of	O
field	O
probing	O
layers	O
in	O
coupling	B-Task
long	I-Task
range	I-Task
information	E-Task
separately	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
even	O
if	O
the	O
following	O
fully	B-Method
connected	I-Method
layer	E-Method
has	O
the	O
capability	O
to	O
couple	O
distance	O
information	O
,	O
the	O
long	O
range	O
connections	O
introduced	O
in	O
our	O
field	O
probing	O
layers	O
are	O
beneficial	O
.	O

paragraph	O
:	O
Performance	O
on	O
Different	O
Field	O
Resolutions	O
.	O

We	O
evaluate	O
our	O
FPNNs	S-Method
on	O
different	O
input	O
field	O
resolutions	O
,	O
and	O
summarize	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Higher	O
resolution	O
input	O
fields	O
can	O
represent	O
input	O
data	O
more	O
accurately	O
,	O
and	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
our	O
FPNN	S-Method
can	O
take	O
advantage	O
of	O
the	O
more	O
accurate	O
representations	O
.	O

Since	O
the	O
computation	B-Metric
cost	E-Metric
of	O
our	O
field	B-Method
probing	I-Method
layers	E-Method
is	O
agnostic	O
to	O
the	O
resolution	O
of	O
the	O
data	B-Method
representation	E-Method
,	O
higher	O
resolution	O
input	O
fields	O
are	O
preferred	O
for	O
better	O
performance	O
,	O
while	O
coupling	O
with	O
efficient	O
data	B-Method
structures	E-Method
reduces	O
the	O
I	B-Metric
/	I-Metric
O	I-Metric
footprint	E-Metric
.	O

paragraph	O
:	O
‘	O
‘	O
Sharpness	O
’	O
’	O
of	O
Gaussian	O
Layer	O
.	O

The	O
hyper	O
-	O
parameter	O
in	O
Gaussian	B-Method
layer	E-Method
controls	O
how	O
‘	O
‘	O
sharp	O
’	O
’	O
is	O
the	O
transform	O
.	O

We	O
select	O
its	O
value	O
empirically	O
in	O
our	O
experiments	O
,	O
and	O
the	O
best	O
performance	O
is	O
given	O
when	O
we	O
use	O
of	O
the	O
object	O
size	O
.	O

Smaller	O
slightly	O
hurts	O
the	O
performance	O
(	O
)	O
,	O
but	O
has	O
the	O
potential	O
of	O
reducing	O
I	B-Metric
/	I-Metric
O	I-Metric
footprint	E-Metric
.	O

paragraph	O
:	O
FPNN	S-Method
Features	O
and	O
Visual	O
Similarity	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
visualization	O
of	O
the	O
features	O
extracted	O
by	O
the	O
FPNN	S-Method
trained	O
for	O
a	O
classification	B-Task
task	E-Task
.	O

Our	O
FPNN	S-Method
is	O
capable	O
of	O
capturing	O
3D	O
geometric	O
structures	O
such	O
that	O
it	O
allows	O
to	O
map	O
3D	B-Method
models	E-Method
that	O
belong	O
to	O
the	O
same	O
categories	O
(	O
indicated	O
by	O
colors	O
)	O
to	O
similar	O
regions	O
in	O
the	O
feature	O
space	O
.	O

More	O
specifically	O
,	O
our	O
FPNN	S-Method
maps	O
3D	B-Method
models	E-Method
into	O
points	O
in	O
a	O
high	O
dimensional	O
feature	O
space	O
,	O
where	O
the	O
distances	O
between	O
the	O
points	O
measure	O
the	O
similarity	O
between	O
their	O
corresponding	O
3D	B-Method
models	E-Method
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
[	O
reference	O
]	O
(	O
better	O
viewed	O
in	O
zoomin	O
mode	O
)	O
,	O
the	O
FPNN	S-Method
feature	O
distances	O
between	O
3D	B-Method
models	E-Method
represent	O
their	O
shape	O
similarities	O
,	O
thus	O
FPNN	S-Method
features	O
can	O
support	O
shape	B-Task
exploration	E-Task
and	O
retrieval	B-Task
tasks	E-Task
.	O

subsection	O
:	O
Generalizability	O
of	O
FPNN	S-Method
Features	O
One	O
superior	O
characteristic	O
of	O
CNN	O
features	O
is	O
that	O
features	O
from	O
one	O
task	O
or	O
dataset	O
can	O
be	O
transferred	O
to	O
another	O
task	O
or	O
dataset	O
.	O

We	O
evaluate	O
the	O
generalizability	O
of	O
FPNN	S-Method
features	O
by	O
cross	B-Method
validation	E-Method
—	O
we	O
train	O
on	O
one	O
dataset	O
and	O
test	O
on	O
another	O
.	O

We	O
first	O
split	O
(	O
lexicographically	O
by	O
the	O
category	O
names	O
)	O
into	O
two	O
parts	O
and	O
,	O
where	O
each	O
of	O
them	O
contains	O
non	O
-	O
overlapping	O
categories	O
.	O

Then	O
we	O
train	O
two	O
FPNNs	S-Method
in	O
a	O
1	B-Method
-	I-Method
FC	I-Method
setting	E-Method
(	O
updating	O
both	O
field	B-Method
probing	I-Method
layers	E-Method
and	O
the	O
only	O
one	O
fully	B-Method
connected	I-Method
layer	E-Method
)	O
on	O
these	O
two	O
datasets	O
,	O
achieving	O
and	O
accuracy	S-Metric
,	O
respectively	O
(	O
the	O
second	O
column	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
we	O
fine	O
tune	O
only	O
the	O
fully	B-Method
connected	I-Method
layer	E-Method
of	O
these	O
two	O
FPNNs	S-Method
on	O
the	O
dataset	O
that	O
they	O
were	O
not	O
trained	O
from	O
,	O
and	O
achieved	O
and	O
on	O
and	O
,	O
respectively	O
(	O
the	O
fourth	O
column	O
in	O
Table	O
[	O
reference	O
]	O
)	O
,	O
which	O
is	O
comparable	O
to	O
that	O
directly	O
trained	O
from	O
the	O
testing	O
categories	O
.	O

We	O
also	O
trained	O
two	O
FPNNs	S-Method
in	O
1	B-Method
-	I-Method
FC	I-Method
setting	E-Method
with	O
updating	O
only	O
the	O
fully	O
connected	O
layer	O
,	O
which	O
achieves	O
and	O
accuracy	S-Metric
on	O
and	O
,	O
respectively	O
(	O
the	O
third	O
column	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

These	O
two	O
FPNNs	S-Method
do	O
not	O
perform	O
as	O
well	O
as	O
the	O
fine	B-Method
-	I-Method
tuned	I-Method
FPNNs	E-Method
(	O
on	O
and	O
on	O
)	O
,	O
although	O
all	O
of	O
them	O
only	O
update	O
the	O
fully	B-Method
connected	I-Method
layer	E-Method
.	O

These	O
experiments	O
show	O
that	O
the	O
field	B-Method
probing	I-Method
filters	E-Method
learned	O
from	O
one	O
dataset	O
can	O
be	O
applied	O
to	O
another	O
one	O
.	O

subsection	O
:	O
Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
We	O
compare	O
the	O
performance	O
of	O
our	O
FPNNs	S-Method
against	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
—	O
SubvolSup	S-Method
+	O
BN	S-Method
and	O
MVCNN	S-Method
-	O
MultiRes	S-Method
,	O
both	O
from	O
,	O
in	O
Table	O
[	O
reference	O
]	O
.	O

SubvolSup	S-Method
+	O
BN	S-Method
is	O
a	O
subvolume	O
supervised	O
volumetric	O
3D	B-Task
CNN	E-Task
,	O
with	O
batch	B-Method
normalization	E-Method
applied	O
during	O
the	O
training	O
,	O
and	O
MVCNN	S-Method
-	O
MultiRes	S-Method
is	O
a	O
multi	B-Method
-	I-Method
view	I-Method
multi	I-Method
-	I-Method
resolution	I-Method
image	I-Method
based	I-Method
2D	I-Method
CNN	E-Method
.	O

Note	O
that	O
our	O
FPNN	S-Method
achieves	O
comparable	O
performance	O
to	O
SubvolSup	S-Method
+	O
BN	S-Method
with	O
less	O
computational	B-Metric
complexity	E-Metric
.	O

However	O
,	O
both	O
our	O
FPNN	S-Method
and	O
SubvolSup	S-Method
+	O
BN	S-Method
do	O
not	O
perform	O
as	O
well	O
as	O
MVCNN	S-Method
-	O
MultiRes	S-Method
.	O

It	O
is	O
intriguing	O
to	O
answer	O
the	O
question	O
why	O
methods	O
directly	O
operating	O
on	O
3D	O
data	O
can	O
not	O
match	O
or	O
outperform	O
multi	B-Method
-	I-Method
view	I-Method
2D	I-Method
CNNs	E-Method
.	O

The	O
research	O
on	O
closing	O
the	O
gap	O
between	O
these	O
modalities	O
can	O
lead	O
to	O
a	O
deeper	O
understanding	O
of	O
both	O
2D	O
images	O
and	O
3D	O
shapes	O
or	O
even	O
higher	O
dimensional	O
data	O
.	O

subsection	O
:	O
Limitations	O
and	O
Future	O
Work	O
paragraph	O
:	O
FPNN	S-Method
on	O
Generic	O
Fields	O
.	O

Our	O
framework	O
provides	O
a	O
general	O
means	O
for	O
optimizing	B-Task
probing	I-Task
locations	I-Task
in	I-Task
3D	I-Task
fields	E-Task
where	O
the	O
gradients	O
can	O
be	O
computed	O
.	O

We	O
suspect	O
this	O
capability	O
might	O
be	O
particularly	O
important	O
for	O
analyzing	B-Task
3D	I-Task
data	E-Task
with	O
invisible	O
internal	O
structures	O
.	O

Moreover	O
,	O
our	O
approach	O
can	O
easily	O
be	O
extended	O
into	O
higher	O
dimensional	O
fields	O
,	O
where	O
a	O
careful	O
storage	O
design	O
of	O
the	O
input	O
fields	O
is	O
important	O
for	O
making	O
the	O
I	B-Metric
/	I-Metric
O	I-Metric
footprint	E-Metric
tractable	O
though	O
.	O

paragraph	O
:	O
From	O
Probing	B-Method
Filters	E-Method
to	O
Probing	B-Task
Network	E-Task
.	O

In	O
our	O
current	O
framework	O
,	O
the	O
probing	B-Method
filters	E-Method
are	O
independent	O
to	O
each	O
other	O
,	O
which	O
means	O
,	O
they	O
do	O
not	O
share	O
locations	O
and	O
weights	O
,	O
which	O
may	O
result	O
in	O
too	O
many	O
parameters	O
for	O
small	O
training	O
sets	O
.	O

On	O
the	O
other	O
hand	O
,	O
fully	O
shared	O
weights	O
greatly	O
limit	O
the	O
representation	O
power	O
of	O
the	O
probing	B-Method
filters	E-Method
.	O

A	O
trade	O
-	O
off	O
might	O
be	O
learning	O
a	O
probing	B-Method
network	E-Method
,	O
where	O
each	O
probing	O
point	O
belongs	O
to	O
multiple	O
‘	O
‘	O
pathes	O
’	O
’	O
in	O
the	O
network	O
for	O
partially	O
sharing	O
parameters	O
.	O

paragraph	O
:	O
FPNN	S-Method
for	O
Finer	B-Task
Shape	I-Task
Understanding	E-Task
.	O

Our	O
current	O
approach	O
is	O
superior	O
for	O
extracting	O
robust	B-Task
global	I-Task
descriptions	I-Task
of	I-Task
the	I-Task
input	I-Task
data	E-Task
,	O
but	O
lacks	O
the	O
capability	O
of	O
understanding	O
finer	O
structures	O
inside	O
the	O
input	O
data	O
.	O

This	O
capability	O
might	O
be	O
realized	O
by	O
strategically	O
initializing	O
the	O
probing	B-Method
filters	I-Method
hierarchically	E-Method
,	O
and	O
jointly	O
optimizing	B-Method
filters	E-Method
at	O
different	O
hierarchies	O
.	O

section	O
:	O
Conclusions	O
We	O
proposed	O
a	O
novel	O
design	O
for	O
feature	B-Task
extraction	E-Task
from	O
3D	O
data	O
,	O
whose	O
computation	B-Metric
cost	E-Metric
is	O
agnostic	O
to	O
the	O
resolution	B-Method
of	I-Method
data	I-Method
representation	E-Method
.	O

A	O
significant	O
advantage	O
of	O
our	O
design	O
is	O
that	O
long	O
range	O
interaction	O
can	O
be	O
easily	O
coupled	O
.	O

As	O
3D	O
data	O
is	O
becoming	O
more	O
accessible	O
,	O
we	O
believe	O
that	O
our	O
method	O
will	O
stimulate	O
more	O
work	O
on	O
feature	B-Task
learning	E-Task
from	O
3D	O
data	O
.	O

We	O
open	O
-	O
source	O
our	O
code	O
at	O
for	O
encouraging	O
future	O
developments	O
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
would	O
first	O
like	O
to	O
thank	O
all	O
the	O
reviewers	O
for	O
their	O
valuable	O
comments	O
and	O
suggestions	O
.	O

Yangyan	O
thanks	O
Daniel	O
Cohen	O
-	O
Or	O
and	O
Zhenhua	O
Wang	O
for	O
their	O
insightful	O
proofreading	O
.	O

The	O
work	O
was	O
supported	O
in	O
part	O
by	O
NSF	O
grants	O
DMS	O
-	O
1546206	O
and	O
IIS	O
-	O
1528025	O
,	O
UCB	O
MURI	O
grant	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0341	O
,	O
Chinese	O
National	O
973	O
Program	O
(	O
2015CB352501	O
)	O
,	O
the	O
Stanford	O
AI	O
Lab	O
-	O
Toyota	O
Center	O
for	O
Artificial	O
Intelligence	O
Research	O
,	O
the	O
Max	O
Planck	O
Center	O
for	O
Visual	O
Computing	O
and	O
Communication	O
,	O
and	O
a	O
Google	O
Focused	O
Research	O
award	O
.	O

bibliography	O
:	O
References	O
Morphosyntactic	B-Method
Tagging	E-Method
with	O
a	O
Meta	B-Method
-	I-Method
BiLSTM	I-Method
Model	E-Method
over	O
Context	B-Method
Sensitive	I-Method
Token	I-Method
Encodings	E-Method
section	O
:	O
Abstract	O
The	O
rise	O
of	O
neural	B-Method
networks	E-Method
,	O
and	O
particularly	O
recurrent	B-Method
neural	I-Method
networks	E-Method
,	O
has	O
produced	O
significant	O
advances	O
in	O
part	B-Metric
-	I-Metric
ofspeech	I-Metric
tagging	I-Metric
accuracy	E-Metric
[	O
reference	O
]	O
.	O

One	O
characteristic	O
common	O
among	O
these	O
models	O
is	O
the	O
presence	O
of	O
rich	O
initial	O
word	B-Method
encodings	E-Method
.	O

These	O
encodings	O
typically	O
are	O
composed	O
of	O
a	O
recurrent	B-Method
character	I-Method
-	I-Method
based	I-Method
representation	E-Method
with	O
learned	O
and	O
pre	B-Method
-	I-Method
trained	I-Method
word	I-Method
embeddings	E-Method
.	O

However	O
,	O
these	O
encodings	O
do	O
not	O
consider	O
a	O
context	O
wider	O
than	O
a	O
single	O
word	O
and	O
it	O
is	O
only	O
through	O
subsequent	O
recurrent	B-Method
layers	E-Method
that	O
word	O
or	O
sub	O
-	O
word	O
information	O
interacts	O
.	O

In	O
this	O
paper	O
,	O
we	O
investigate	O
models	O
that	O
use	O
recurrent	B-Method
neural	I-Method
networks	E-Method
with	O
sentence	O
-	O
level	O
context	O
for	O
initial	O
character	B-Method
and	I-Method
word	I-Method
-	I-Method
based	I-Method
representations	E-Method
.	O

In	O
particular	O
we	O
show	O
that	O
optimal	O
results	O
are	O
obtained	O
by	O
integrating	O
these	O
context	B-Method
sensitive	I-Method
representations	E-Method
through	O
synchronized	B-Method
training	E-Method
with	O
a	O
meta	B-Method
-	I-Method
model	E-Method
that	O
learns	O
to	O
combine	O
their	O
states	O
.	O

We	O
present	O
results	O
on	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
and	O
morphological	B-Task
tagging	E-Task
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
number	O
of	O
languages	O
.	O

section	O
:	O
Introduction	O
Morphosyntactic	O
tagging	O
accuracy	S-Metric
has	O
seen	O
dramatic	O
improvements	O
through	O
the	O
adoption	O
of	O
recurrent	B-Method
neural	I-Method
networks	E-Method
-	O
specifically	O
BiLSTMs	S-Method
[	O
reference	O
][	O
reference	O
]	O
to	O
create	O
sentence	B-Method
-	I-Method
level	I-Method
context	I-Method
sensitive	I-Method
encodings	E-Method
of	O
words	O
.	O

A	O
successful	O
recipe	O
is	O
to	O
first	O
create	O
an	O
initial	O
context	B-Method
insensitive	I-Method
word	I-Method
representation	E-Method
,	O
which	O
usually	O
has	O
three	O
main	O
parts	O
:	O
1	O
)	O
A	O
dynamically	B-Method
trained	I-Method
word	I-Method
embedding	E-Method
;	O
2	O
)	O
a	O
fixed	O
pre	B-Method
-	I-Method
trained	I-Method
word	I-Method
-	I-Method
embedding	E-Method
,	O
induced	O
from	O
a	O
large	O
corpus	O
;	O
and	O
3	O
)	O
a	O
sub	B-Method
-	I-Method
word	I-Method
character	I-Method
model	E-Method
,	O
which	O
itself	O
is	O
usually	O
the	O
final	O
state	O
of	O
a	O
recurrent	B-Method
model	E-Method
that	O
ingests	O
one	O
character	O
at	O
a	O
time	O
.	O

Such	O
word	B-Method
/	I-Method
sub	I-Method
-	I-Method
word	I-Method
models	E-Method
originated	O
with	O
[	O
reference	O
]	O
.	O

Recently	O
,	O
[	O
reference	O
]	O
used	O
precisely	O
such	O
a	O
context	B-Method
insensitive	I-Method
word	I-Method
representation	E-Method
as	O
input	O
to	O
a	O
BiLSTM	S-Method
in	O
order	O
to	O
obtain	O
context	O
sensitive	O
word	B-Method
encodings	E-Method
used	O
to	O
predict	O
partof	O
-	O
speech	O
tags	O
.	O

The	O
Dozat	O
et	O
al	O
.	O

model	O
had	O
the	O
highest	O
accuracy	S-Metric
of	O
all	O
participating	O
systems	O
in	O
the	O
CoNLL	B-Task
2017	I-Task
shared	I-Task
task	E-Task
[	O
reference	O
]	O
.	O

In	O
such	O
a	O
model	O
,	O
sub	B-Method
-	I-Method
word	I-Method
character	I-Method
-	I-Method
based	I-Method
representations	E-Method
only	O
interact	O
indirectly	O
via	O
subsequent	O
recurrent	B-Method
layers	E-Method
.	O

For	O
example	O
,	O
consider	O
the	O
sentence	O
I	O
had	O
shingles	O
,	O
which	O
is	O
a	O
painful	O
disease	O
.	O

Context	B-Method
insensitive	I-Method
character	I-Method
and	I-Method
word	I-Method
representations	E-Method
may	O
have	O
learned	O
that	O
for	O
unknown	O
or	O
infrequent	O
words	O
like	O
'	O
shingles	O
'	O
,	O
's	O
'	O
and	O
more	O
so	O
'	O
es	O
'	O
is	O
a	O
common	O
way	O
to	O
end	O
a	O
plural	O
noun	O
.	O

It	O
is	O
up	O
to	O
the	O
subsequent	O
BiLSTM	S-Method
layer	O
to	O
override	O
this	O
once	O
it	O
sees	O
the	O
singular	O
verb	O
is	O
to	O
the	O
right	O
.	O

Note	O
that	O
this	O
differs	O
from	O
traditional	O
linear	B-Method
models	E-Method
where	O
word	B-Method
and	I-Method
sub	I-Method
-	I-Method
word	I-Method
representations	E-Method
are	O
directly	O
concatenated	O
with	O
similar	O
features	O
in	O
the	O
surrounding	O
context	O
[	O
reference	O
]	O
.	O

In	O
this	O
paper	O
we	O
aim	O
to	O
investigate	O
to	O
what	O
extent	O
having	O
initial	O
sub	O
-	O
word	O
and	O
word	O
context	O
insensitive	O
representations	O
affects	O
performance	O
.	O

We	O
propose	O
a	O
novel	O
model	O
where	O
we	O
learn	O
context	O
sensitive	O
initial	O
character	O
and	O
word	B-Method
representations	E-Method
through	O
two	O
separate	O
sentence	B-Method
-	I-Method
level	I-Method
recurrent	I-Method
models	E-Method
.	O

These	O
are	O
then	O
combined	O
via	O
a	O
metaBiLSTM	B-Method
model	E-Method
that	O
builds	O
a	O
unified	B-Method
representation	E-Method
of	O
each	O
word	O
that	O
is	O
then	O
used	O
for	O
syntactic	B-Task
tagging	E-Task
.	O

Critically	O
,	O
while	O
each	O
of	O
these	O
three	O
models	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
are	O
trained	O
synchronously	O
,	O
they	O
are	O
ultimately	O
separate	O
models	O
using	O
different	O
network	O
configurations	O
,	O
training	O
hyperparameters	O
and	O
loss	O
functions	O
.	O

Empirically	O
,	O
we	O
found	O
this	O
optimal	O
as	O
it	O
allowed	O
control	O
over	O
the	O
fact	O
that	O
each	O
representation	O
has	O
a	O
different	O
learning	O
capacity	O
.	O

We	O
tested	O
the	O
system	O
on	O
the	O
2017	O
CoNLL	B-Material
shared	I-Material
task	I-Material
data	I-Material
sets	E-Material
and	O
gain	O
improvements	O
compared	O
to	O
the	O
top	O
performing	O
systems	O
for	O
the	O
majority	O
of	O
languages	O
for	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
and	O
morphological	B-Task
tagging	E-Task
.	O

As	O
we	O
will	O
see	O
,	O
a	O
pattern	O
emerged	O
where	O
gains	O
were	O
largest	O
for	O
morphologically	O
rich	O
languages	O
,	O
especially	O
those	O
in	O
the	O
Slavic	O
family	O
group	O
.	O

We	O
also	O
applied	O
the	O
approach	O
to	O
the	O
benchmark	O
English	B-Material
PTB	I-Material
data	E-Material
,	O
where	O
our	O
model	O
achieved	O
97.9	O
using	O
the	O
standard	O
train	B-Metric
/	I-Metric
dev	I-Metric
/	I-Metric
test	I-Metric
split	E-Metric
,	O
which	O
constitutes	O
a	O
relative	O
reduction	O
in	O
error	S-Metric
of	O
12	O
%	O
over	O
the	O
previous	O
best	O
system	O
.	O

section	O
:	O
Related	O
Work	O
While	O
sub	B-Method
-	I-Method
word	I-Method
representations	E-Method
are	O
often	O
attributed	O
to	O
the	O
advent	O
of	O
deep	B-Method
learning	E-Method
in	O
NLP	S-Task
,	O
it	O
was	O
,	O
in	O
fact	O
,	O
commonplace	O
for	O
linear	B-Method
featurized	I-Method
machine	I-Method
learning	I-Method
methods	E-Method
to	O
incorporate	O
such	O
representations	O
.	O

While	O
the	O
literature	O
is	O
too	O
large	O
to	O
enumerate	O
,	O
[	O
reference	O
]	O
is	O
a	O
good	O
example	O
of	O
an	O
accurate	O
linear	B-Method
model	E-Method
that	O
uses	O
both	O
word	O
and	O
sub	O
-	O
word	O
features	O
.	O

Specifically	O
,	O
like	O
most	O
systems	O
of	O
the	O
time	O
,	O
they	O
use	O
ngram	O
affix	O
features	O
,	O
which	O
were	O
made	O
context	O
sensitive	O
via	O
manually	O
constructed	O
conjunctions	O
with	O
features	O
from	O
other	O
words	O
in	O
a	O
fixed	O
window	O
.	O

[	O
reference	O
]	O
was	O
perhaps	O
the	O
first	O
modern	O
neural	B-Method
network	E-Method
for	O
tagging	S-Task
.	O

While	O
this	O
first	O
study	O
used	O
only	O
word	B-Method
embeddings	E-Method
,	O
a	O
subsequent	O
model	O
extended	O
the	O
representation	O
to	O
include	O
suffix	O
embeddings	O
[	O
reference	O
]	O
.	O

The	O
seminal	O
dependency	B-Task
parsing	E-Task
paper	O
of	O
[	O
reference	O
]	O
led	O
to	O
a	O
number	O
of	O
tagging	O
papers	O
that	O
used	O
their	O
basic	O
architecture	O
of	O
highly	O
featurized	B-Method
(	I-Method
and	I-Method
embedded	I-Method
)	I-Method
feed	I-Method
-	I-Method
forward	I-Method
neural	I-Method
networks	E-Method
.	O

[	O
reference	O
]	O
,	O
for	O
example	O
,	O
studied	O
this	O
architecture	O
in	O
a	O
low	B-Task
resource	I-Task
setting	E-Task
using	O
word	O
,	O
sub	O
-	O
word	O
(	O
prefix	O
/	O
suffix	O
)	O
and	O
induced	O
cluster	O
features	O
to	O
obtain	O
competitive	O
accuracy	S-Metric
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
extended	O
the	O
work	O
of	O
Chen	O
et	O
al	O
.	O

to	O
a	O
structured	B-Task
prediction	I-Task
setting	E-Task
,	O
the	O
later	O
two	O
use	O
again	O
a	O
mix	O
of	O
word	O
and	O
sub	O
-	O
word	O
features	O
.	O

The	O
idea	O
of	O
using	O
a	O
recurrent	B-Method
layer	E-Method
over	O
characters	O
to	O
induce	O
a	O
complementary	O
view	O
of	O
a	O
word	O
has	O
occurred	O
in	O
numerous	O
papers	O
.	O

Perhaps	O
the	O
earliest	O
is	O
Santos	O
and	O
Zadrozny	O
(	O
2014	O
)	O
who	O
compare	O
character	O
-	O
based	O
LSTM	S-Method
encodings	O
to	O
traditional	O
word	B-Method
-	I-Method
based	I-Method
embeddings	E-Method
.	O

[	O
reference	O
]	O
take	O
this	O
a	O
step	O
further	O
and	O
combine	O
the	O
word	B-Method
embeddings	E-Method
with	O
a	O
recurrent	B-Method
character	I-Method
encoding	E-Method
of	O
the	O
word	O
-	O
instead	O
of	O
just	O
relying	O
on	O
one	O
or	O
the	O
other	O
.	O

[	O
reference	O
]	O
use	O
a	O
sentencelevel	O
character	O
LSTM	S-Method
encoding	O
for	O
parsing	S-Task
.	O

[	O
reference	O
]	O
show	O
that	O
contextual	B-Method
embeddings	E-Method
using	O
character	B-Method
convolutions	E-Method
improve	O
accuracy	S-Metric
for	O
number	O
of	O
NLP	B-Task
tasks	E-Task
.	O

[	O
reference	O
]	O
is	O
probably	O
the	O
jumping	O
-	O
off	O
point	O
for	O
most	O
current	O
architectures	O
for	O
tagging	B-Method
models	E-Method
with	O
recurrent	B-Method
neural	I-Method
networks	E-Method
.	O

Specifically	O
,	O
they	O
used	O
a	O
combined	O
word	B-Method
embedding	E-Method
and	O
recurrent	B-Method
character	I-Method
encoding	E-Method
as	O
the	O
initial	O
input	O
to	O
a	O
BiLSTM	S-Method
that	O
generated	O
context	O
sensitive	O
word	B-Method
encodings	E-Method
.	O

Though	O
,	O
like	O
most	O
previous	O
studies	O
,	O
these	O
initial	O
encodings	O
were	O
context	O
insensitive	O
and	O
relied	O
on	O
subsequent	O
layers	O
to	O
encode	O
sentence	O
-	O
level	O
interactions	O
.	O

Finally	O
,	O
[	O
reference	O
]	O
showed	O
that	O
subword	B-Method
/	I-Method
word	I-Method
combination	I-Method
representations	E-Method
lead	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
morphosyntactic	B-Task
tagging	E-Task
accuracy	S-Metric
across	O
a	O
number	O
of	O
languages	O
in	O
the	O
CoNLL	B-Material
2017	I-Material
shared	I-Material
task	E-Material
[	O
reference	O
]	O
.	O

Their	O
word	B-Method
representation	E-Method
consisted	O
of	O
three	O
parts	O
:	O
1	O
)	O
A	O
dynamically	B-Method
trained	I-Method
word	I-Method
embedding	E-Method
;	O
2	O
)	O
a	O
fixed	B-Method
pretrained	I-Method
word	I-Method
embedding	E-Method
;	O
3	O
)	O
a	O
character	O
LSTM	S-Method
encoding	O
that	O
summed	O
the	O
final	O
state	O
of	O
the	O
recurrent	B-Method
model	E-Method
with	O
vector	S-Method
constructed	O
using	O
an	O
attention	B-Method
mechanism	E-Method
over	O
all	O
character	O
states	O
.	O

Again	O
,	O
the	O
initial	O
representations	O
are	O
all	O
context	O
insensitive	O
.	O

As	O
this	O
model	O
is	O
currently	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
morphosyntactic	B-Task
tagging	E-Task
,	O
it	O
will	O
serve	O
as	O
a	O
baseline	O
during	O
our	O
discussion	O
and	O
experiments	O
.	O

section	O
:	O
Models	O
In	O
this	O
section	O
,	O
we	O
introduce	O
models	O
that	O
we	O
investigate	O
and	O
experiment	O
with	O
in	O
§	O
4	O
.	O

section	O
:	O
Sentence	B-Method
-	I-Method
based	I-Method
Character	I-Method
Model	E-Method
The	O
feature	O
that	O
distinguishes	O
our	O
model	O
most	O
from	O
previous	O
work	O
is	O
that	O
we	O
apply	O
a	O
bidirectional	B-Method
recurrent	I-Method
layer	E-Method
(	O
LSTM	S-Method
)	O
on	O
all	O
characters	O
of	O
a	O
sentence	O
to	O
induce	O
fully	O
context	O
sensitive	O
initial	O
word	B-Method
encodings	E-Method
.	O

That	O
is	O
,	O
we	O
do	O
not	O
restrict	O
the	O
context	O
of	O
this	O
layer	O
to	O
the	O
words	O
themselves	O
(	O
as	O
in	O
Figure	O
1b	O
)	O
.	O

Figure	O
1a	O
shows	O
the	O
sentence	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
applied	O
to	O
an	O
example	O
token	O
in	O
context	O
.	O

The	O
character	B-Method
model	E-Method
uses	O
,	O
as	O
input	O
,	O
sentences	O
split	O
into	O
UTF8	O
characters	O
.	O

We	O
include	O
the	O
spaces	O
between	O
the	O
tokens	O
1	O
in	O
the	O
input	O
and	O
map	O
each	O
character	O
to	O
a	O
dynamically	O
learned	O
embedding	O
.	O

Next	O
,	O
a	O
forward	O
LSTM	S-Method
reads	O
the	O
characters	O
from	O
left	O
to	O
right	O
and	O
a	O
backward	O
LSTM	S-Method
reads	O
sentences	O
from	O
right	O
to	O
left	O
,	O
in	O
standard	O
BiLSTM	B-Method
fashion	E-Method
.	O

More	O
formally	O
,	O
for	O
an	O
n	O
-	O
character	O
sentence	O
,	O
we	O
apply	O
for	O
each	O
character	O
embedding	O
(	O
e	O
char	O
1	O
,	O
...	O
,	O
e	O
char	O
n	O
)	O
a	O
BiLSTM	S-Method
:	O
As	O
is	O
also	O
typical	O
,	O
we	O
can	O
have	O
multiple	O
such	O
layers	O
(	O
l	O
)	O
that	O
feed	O
into	O
each	O
other	O
through	O
the	O
concatenation	B-Method
of	I-Method
previous	I-Method
layer	I-Method
encodings	E-Method
.	O

The	O
last	O
layer	O
l	O
has	O
both	O
forward	O
(	O
f	O
l	O
c	O
,	O
1	O
,	O
...	O
,	O
f	O
l	O
c	O
,	O
n	O
)	O
and	O
backward	O
(	O
b	O
l	O
c	O
,	O
1	O
,	O
...	O
,	O
b	O
l	O
c	O
,	O
n	O
)	O
output	O
vectors	O
for	O
each	O
character	O
.	O

To	O
create	O
word	B-Method
encodings	E-Method
,	O
we	O
need	O
to	O
combine	O
a	O
relevant	O
subset	O
of	O
these	O
context	B-Method
sensitive	I-Method
character	I-Method
encodings	E-Method
.	O

These	O
word	B-Method
encodings	E-Method
can	O
then	O
be	O
used	O
in	O
a	O
model	O
that	O
assigns	O
morphosyntactic	O
tags	O
to	O
each	O
word	O
directly	O
or	O
via	O
subsequent	O
layers	O
.	O

To	O
accomplish	O
this	O
,	O
the	O
model	O
concatenates	O
up	O
to	O
four	O
character	O
output	O
vectors	O
:	O
the	O
{	O
forward	O
,	O
backward	O
}	O
output	O
of	O
the	O
{	O
first	O
,	O
last	O
}	O
character	O
in	O
the	O
token	O
(	O
F	O
1st	O
(	O
w	O
)	O
,	O
F	O
last	O
(	O
w	O
)	O
,	O
B	O
1st	O
(	O
w	O
)	O
,	O
B	O
last	O
(	O
w	O
)	O
)	O
.	O

In	O
Figure	O
1a	O
,	O
the	O
four	O
shaded	O
boxes	O
indicate	O
these	O
four	O
outputs	O
for	O
the	O
example	O
token	O
.	O

Thus	O
,	O
the	O
proposed	O
model	O
concatenates	O
all	O
four	O
of	O
these	O
and	O
passes	O
it	O
as	O
input	O
to	O
an	O
multilayer	B-Method
perceptron	E-Method
(	O
MLP	S-Method
)	O
:	O
A	O
tag	O
can	O
then	O
be	O
predicted	O
with	O
a	O
linear	B-Method
classifier	E-Method
that	O
takes	O
as	O
input	O
the	O
output	O
of	O
the	O
MLP	B-Method
enized	E-Method
/	O
segmented	S-Method
.	O

,	O
applies	O
a	O
softmax	B-Method
function	E-Method
and	O
chooses	O
for	O
each	O
word	O
the	O
tag	O
with	O
highest	O
probability	O
.	O

Table	O
8	O
investigates	O
the	O
empirical	O
impact	O
of	O
alternative	O
definitions	O
of	O
g	O
i	O
that	O
concatenate	O
only	O
subsets	O
of	O
{	O
F	O
1st	O
(	O
w	O
)	O
,	O
F	O
last	O
(	O
w	O
)	O
,	O
B	O
1st	O
(	O
w	O
)	O
,	O
B	O
last	O
(	O
w	O
)	O
}.	O
section	O
:	O
Word	B-Method
-	I-Method
based	I-Method
Character	I-Method
Model	E-Method
To	O
investigate	O
whether	O
a	O
sentence	B-Method
sensitive	I-Method
character	I-Method
model	E-Method
is	O
better	O
than	O
a	O
character	B-Method
model	E-Method
where	O
the	O
context	O
is	O
restricted	O
to	O
the	O
characters	O
of	O
a	O
word	O
,	O
we	O
reimplemented	O
the	O
word	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
of	O
[	O
reference	O
]	O
as	O
shown	O
in	O
[	O
reference	O
]	O
.	O

This	O
model	O
uses	O
the	O
final	O
state	O
of	O
a	O
unidirectional	O
LSTM	S-Method
over	O
the	O
characters	O
of	O
the	O
word	O
,	O
combined	O
with	O
the	O
attention	B-Method
mechanism	E-Method
of	O
Cao	O
and	O
Rei	O
(	O
2016	O
)	O
over	O
all	O
characters	O
.	O

We	O
refer	O
the	O
reader	O
to	O
those	O
works	O
for	O
more	O
details	O
.	O

Critically	O
,	O
however	O
,	O
all	O
the	O
information	O
fed	O
to	O
this	O
representation	O
comes	O
from	O
the	O
word	O
itself	O
,	O
and	O
not	O
a	O
wider	O
sentence	O
-	O
level	O
context	O
.	O

section	O
:	O
Sentence	B-Method
-	I-Method
based	I-Method
Word	I-Method
Model	E-Method
We	O
used	O
a	O
similar	O
setup	O
for	O
our	O
context	O
sensitive	O
word	B-Method
encodings	E-Method
as	O
the	O
character	B-Method
encodings	E-Method
.	O

There	O
are	O
a	O
few	O
differences	O
.	O

Obviously	O
,	O
the	O
inputs	O
are	O
the	O
words	O
of	O
the	O
sentence	O
.	O

For	O
each	O
of	O
the	O
words	O
,	O
we	O
use	O
pretrained	O
word	O
embeddings	O
(	O
p	O
word	O
The	O
summed	O
embeddings	O
in	O
i	O
are	O
passed	O
as	O
input	O
to	O
one	O
or	O
more	O
BiLSTM	B-Method
layers	E-Method
whose	O
output	O
f	O
l	O
w	O
,	O
i	O
,	O
b	O
l	O
w	O
,	O
i	O
is	O
concatenated	O
and	O
used	O
as	O
the	O
final	O
encoding	O
,	O
which	O
is	O
then	O
passed	O
to	O
an	O
MLP	S-Method
It	O
should	O
be	O
noted	O
,	O
that	O
the	O
output	O
of	O
this	O
BiL	B-Method
-	I-Method
STM	E-Method
is	O
essentially	O
the	O
Dozat	O
et	O
al	O
.	O

model	O
before	O
tag	B-Task
prediction	E-Task
,	O
with	O
the	O
exception	O
that	O
the	O
wordbased	B-Method
character	I-Method
encodings	E-Method
are	O
excluded	O
.	O

section	O
:	O
Meta	B-Method
-	I-Method
BiLSTM	E-Method
:	O
Model	B-Method
Combination	E-Method
Given	O
initial	O
word	B-Method
encodings	E-Method
,	O
both	O
character	O
and	O
word	O
-	O
based	O
,	O
a	O
common	O
strategy	O
is	O
to	O
pass	O
these	O
through	O
a	O
sentence	O
-	O
level	O
BiLSTM	S-Method
to	O
create	O
context	O
sensitive	O
encodings	O
,	O
e.g.	O
,	O
this	O
is	O
precisely	O
what	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
do	O
.	O

However	O
,	O
we	O
found	O
that	O
if	O
we	O
trained	O
each	O
of	O
the	O
character	B-Method
-	I-Method
based	I-Method
and	I-Method
word	I-Method
-	I-Method
based	I-Method
encodings	E-Method
with	O
their	O
own	O
loss	O
,	O
and	O
combined	O
them	O
using	O
an	O
additional	O
meta	O
-	O
BiLSTM	S-Method
model	O
,	O
we	O
obtained	O
optimal	O
performance	O
.	O

In	O
the	O
meta	O
-	O
BiLSTM	S-Method
model	O
,	O
we	O
concatenate	O
the	O
output	O
,	O
for	O
each	O
word	O
,	O
of	O
its	O
context	O
sensitive	O
character	O
and	O
word	B-Method
-	I-Method
based	I-Method
encodings	E-Method
,	O
and	O
put	O
this	O
through	O
another	O
BiLSTM	S-Method
to	O
create	O
an	O
additional	O
combined	B-Method
context	I-Method
sensitive	I-Method
encoding	E-Method
.	O

This	O
is	O
followed	O
by	O
a	O
final	O
MLP	S-Method
whose	O
output	O
is	O
passed	O
to	O
a	O
linear	B-Method
layer	E-Method
for	O
tag	B-Task
prediction	E-Task
.	O

With	O
this	O
setup	O
,	O
each	O
of	O
the	O
models	O
can	O
be	O
optimized	O
independently	O
which	O
we	O
describe	O
in	O
more	O
detail	O
in	O
§	O
3.5	O
.	O

Figure	O
2b	O
depicts	O
the	O
architecture	O
of	O
the	O
combined	O
system	O
and	O
contrasts	O
it	O
with	O
that	O
of	O
the	O
[	O
reference	O
]	O
(	O
Figure	O
2a	O
)	O
.	O

section	O
:	O
Training	O
Schema	O
As	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
the	O
character	B-Method
and	I-Method
word	I-Method
-	I-Method
based	I-Method
encoding	I-Method
models	E-Method
have	O
their	O
own	O
tagging	O
loss	O
functions	O
,	O
which	O
are	O
trained	O
independently	O
and	O
joined	O
via	O
the	O
meta	O
-	O
BiLSTM	S-Method

.	O

I.e.	O

,	O
the	O
loss	O
of	O
each	O
model	O
is	O
minimized	O
independently	O
by	O
separate	O
optimizers	S-Method
with	O
their	O
own	O
hyperparameters	O
.	O

Thus	O
,	O
it	O
is	O
in	O
some	O
sense	O
a	O
multitask	B-Method
learning	I-Method
model	E-Method
and	O
we	O
must	O
define	O
a	O
schedule	O
in	O
which	O
individual	O
models	O
are	O
updated	O
.	O

We	O
opted	O
for	O
a	O
simple	O
synchronous	B-Method
schedule	E-Method
outline	O
in	O
Algorithm	O
1	O
.	O

Here	O
,	O
during	O
each	O
epoch	O
,	O
we	O
update	O
each	O
of	O
the	O
models	O
in	O
sequence	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
using	O
the	O
entire	O
training	O
data	O
.	O

Algorithm	O
1	O
:	O
Training	O
procedure	O
for	O
learning	O
initial	B-Task
character	I-Task
and	I-Task
word	I-Task
-	I-Task
based	I-Task
context	I-Task
sensitive	I-Task
encodings	E-Task
synchronously	O
with	O
meta	O
-	O
BiLSTM	S-Method
.	O

In	O
terms	O
of	O
model	B-Task
selection	E-Task
,	O
after	O
each	O
epoch	O
,	O
the	O
algorithm	O
evaluates	O
the	O
tagging	O
accuracy	S-Metric
of	O
the	O
development	O
set	O
and	O
keeps	O
the	O
parameters	O
of	O
the	O
best	O
model	O
.	O

Accuracy	S-Metric
is	O
measured	O
using	O
the	O
meta	O
-	O
BiLSTM	S-Method
tagging	O
layer	O
,	O
which	O
requires	O
a	O
forward	O
pass	O
through	O
all	O
three	O
models	O
.	O

Though	O
we	O
use	O
all	O
three	O
losses	O
to	O
update	O
the	O
models	O
,	O
only	O
the	O
meta	O
-	O
BiLSTM	S-Method
layer	O
is	O
used	O
for	O
model	B-Task
selection	E-Task
and	O
test	B-Task
-	I-Task
time	I-Task
prediction	E-Task
.	O

While	O
each	O
of	O
the	O
three	O
models	O
-	O
character	O
,	O
word	O
and	O
meta	O
-	O
are	O
trained	O
with	O
their	O
own	O
loss	O
functions	O
,	O
it	O
should	O
be	O
emphasized	O
that	O
training	O
is	O
synchronous	O
in	O
the	O
sense	O
that	O
the	O
meta	O
-	O
BiLSTM	S-Method
model	O
is	O
trained	O
in	O
tandem	O
with	O
the	O
two	O
encoding	B-Method
models	E-Method
,	O
and	O
not	O
after	O
those	O
models	O
have	O
converged	O
.	O

Since	O
accuracy	S-Metric
from	O
the	O
meta	O
-	O
BiLSTM	S-Method
model	O
on	O
the	O
development	O
set	O
determines	O
the	O
best	O
parameters	O
,	O
training	O
is	O
not	O
completely	O
independent	O
.	O

We	O
found	O
this	O
to	O
improve	O
accuracy	S-Metric
overall	O
.	O

Crucially	O
,	O
when	O
we	O
allowed	O
the	O
meta	O
-	O
BiLSTM	S-Method
to	O
back	O
-	O
propagate	O
through	O
the	O
whole	O
network	O
,	O
per	O
-	O
formance	O
degraded	O
regardless	O
of	O
whether	O
one	O
or	O
multiple	O
loss	O
functions	O
were	O
used	O
.	O

Each	O
language	O
could	O
in	O
theory	O
use	O
separate	O
hyperparameters	O
,	O
optimized	O
for	O
highest	O
accuracy	S-Metric
.	O

However	O
,	O
for	O
our	O
main	O
experiments	O
we	O
use	O
identical	O
settings	O
for	O
each	O
language	O
which	O
worked	O
well	O
for	O
large	O
corpora	O
and	O
simplified	O
things	O
.	O

We	O
provide	O
an	O
overview	O
of	O
the	O
selected	O
hyperparameters	O
in	O
§	O
4.1	O
.	O

We	O
explored	O
more	O
settings	O
for	O
selected	O
individual	O
languages	O
with	O
a	O
grid	B-Method
search	E-Method
and	O
ablation	O
experiments	O
and	O
present	O
the	O
results	O
in	O
§	O
5	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
In	O
this	O
section	O
,	O
we	O
present	O
the	O
experimental	O
setup	O
and	O
the	O
selected	O
hyperparameter	O
for	O
the	O
main	O
experiments	O
where	O
we	O
use	O
the	O
CoNLL	B-Material
Shared	I-Material
Task	I-Material
2017	I-Material
treebanks	E-Material
and	O
compare	O
with	O
the	O
best	O
systems	O
of	O
the	O
shared	O
task	O
.	O

section	O
:	O
Experimental	O
Setup	O
For	O
our	O
main	O
results	O
,	O
we	O
selected	O
one	O
network	B-Method
configuration	E-Method
and	O
set	O
of	O
the	O
hyperparameters	O
.	O

These	O
settings	O
are	O
not	O
optimal	O
for	O
all	O
languages	O
.	O

However	O
,	O
since	O
hyperparameter	B-Method
exploration	E-Method
is	O
computationally	O
demanding	O
due	O
to	O
the	O
number	O
of	O
languages	O
we	O
optimized	O
these	O
hyperparameters	O
on	O
initial	O
development	O
data	O
experiments	O
over	O
a	O
few	O
languages	O
.	O

Table	O
1	O
:	O
Selected	O
hyperparameters	O
and	O
initialization	O
of	O
parameters	O
for	O
our	O
models	O
.	O

Chr	S-Method
,	O
Wrd	O
,	O
and	O
Mt	O
are	O
used	O
to	O
indicate	O
the	O
character	O
,	O
word	O
,	O
and	O
meta	B-Method
models	E-Method
respectively	O
.	O

The	O
Gaussian	B-Method
distribution	E-Method
is	O
used	O
with	O
a	O
mean	O
of	O
0	O
and	O
variance	O
of	O
1	O
to	O
generate	O
the	O
random	O
values	O
.	O

single	B-Method
dropout	I-Method
mask	E-Method
and	O
we	O
use	O
dropout	O
on	O
the	O
input	O
and	O
the	O
states	O
of	O
the	O
LSTM	S-Method
.	O

As	O
is	O
standard	O
,	O
model	B-Method
selection	E-Method
was	O
done	O
measuring	O
development	B-Metric
accuracy	E-Metric
/	O
F1	B-Metric
score	E-Metric
after	O
each	O
epoch	O
and	O
taking	O
the	O
model	O
with	O
maximum	O
value	O
on	O
the	O
development	O
set	O
.	O

section	O
:	O
Data	O
Sets	O
For	O
the	O
experiments	O
,	O
we	O
use	O
the	O
data	O
sets	O
as	O
provided	O
by	O
the	O
CoNLL	B-Material
Shared	I-Material
Task	I-Material
2017	E-Material
[	O
reference	O
]	O
.	O

For	O
training	O
,	O
we	O
use	O
the	O
training	O
sets	O
which	O
were	O
denoted	O
as	O
big	O
treebanks	O
2	O
.	O

We	O
followed	O
the	O
same	O
methodology	O
used	O
in	O
the	O
CoNLL	B-Task
Shared	I-Task
Task	E-Task
.	O

We	O
use	O
the	O
training	O
treebank	O
for	O
training	O
only	O
and	O
the	O
development	O
sets	O
for	O
hyperparameter	B-Method
tuning	E-Method
and	O
early	B-Task
stopping	E-Task
.	O

To	O
keep	O
our	O
results	O
comparable	O
with	O
the	O
Shared	B-Task
Task	E-Task
,	O
we	O
use	O
the	O
provided	O
precomputed	O
word	O
embeddings	O
.	O

We	O
excluded	O
Gothic	O
from	O
our	O
experiments	O
as	O
the	O
available	O
downloadable	O
content	O
did	O
not	O
include	O
embeddings	O
for	O
this	O
language	O
.	O

As	O
input	O
to	O
our	O
system	O
-	O
for	O
both	O
part	B-Task
-	I-Task
ofspeech	I-Task
tagging	E-Task
and	O
morphological	B-Task
tagging	E-Task
-	O
we	O
use	O
the	O
output	O
of	O
the	O
UDPipe	S-Method
-	O
base	O
baseline	O
system	O
[	O
reference	O
]	O
)	O
which	O
provides	O
segmentation	S-Task
.	O

The	O
segmentation	O
differs	O
from	O
the	O
gold	B-Method
segmentation	E-Method
and	O
impacts	O
accuracy	S-Metric
negatively	O
for	O
a	O
number	O
of	O
languages	O
.	O

Most	O
of	O
the	O
top	O
performing	O
systems	O
for	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
tagging	O
used	O
as	O
input	O
UDPipe	S-Method
to	O
obtain	O
the	O
segmentation	O
for	O
the	O
input	O
data	O
.	O

For	O
morphology	S-Task
,	O
the	O
top	O
system	O
for	O
most	O
languages	O
(	O
IMS	S-Method
)	O
used	O
its	O
own	O
segmentation	O
[	O
reference	O
]	O
.	O

For	O
the	O
evaluation	O
,	O
we	O
used	O
the	O
official	O
evaluation	O
script	O
[	O
reference	O
]	O
.	O

section	O
:	O
Part	B-Task
-	I-Task
of	I-Task
-	I-Task
Speech	I-Task
Tagging	E-Task
Results	O
In	O
this	O
section	O
,	O
we	O
present	O
the	O
results	O
of	O
the	O
application	O
of	O
our	O
model	O
to	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
tagging	O
.	O

In	O
our	O
first	O
experiment	O
,	O
we	O
used	O
our	O
model	O
in	O
the	O
setting	O
of	O
the	O
CoNLL	B-Material
2017	I-Material
Shared	I-Material
Task	E-Material
to	O
annotate	O
words	O
with	O
XPOS	O
3	O
tags	O
[	O
reference	O
]	O
.	O

We	O
compare	O
our	O
results	O
against	O
the	O
top	O
systems	O
of	O
the	O
CoNLL	B-Material
2017	I-Material
Shared	I-Material
Task	E-Material
.	O

Table	O
2	O
contains	O
the	O
results	O
of	O
this	O
task	O
for	O
the	O
large	O
treebanks	O
.	O

Because	O
[	O
reference	O
]	O
won	O
the	O
challenge	O
for	O
the	O
majority	O
of	O
the	O
languages	O
,	O
we	O
first	O
compare	O
our	O
results	O
with	O
the	O
performance	O
of	O
their	O
system	O
.	O

Our	O
model	O
outperforms	O
[	O
reference	O
]	O
in	O
32	O
of	O
the	O
54	O
treebanks	O
with	O
13	O
ties	O
.	O

These	O
ties	O
correspond	O
mostly	O
to	O
languages	O
where	O
XPOS	B-Method
tagging	E-Method
anyhow	O
obtains	O
accuracies	S-Metric
above	O
99	O
%	O
.	O

Our	O
model	O
tends	O
to	O
produce	O
better	O
results	O
,	O
especially	O
for	O
morphologically	O
rich	O
languages	O
(	O
e.g.	O
Slavic	B-Metric
System	I-Metric
Accuracy	E-Metric
Søgaard	O
(	O
2011	O
)	O
97.50	O
[	O
reference	O
]	O
97.64	O
[	O
reference	O
]	O
.	O

97.44	O
[	O
reference	O
]	O
97.41	O
ours	O
97.96	O
Table	O
3	O
:	O
Results	O
on	O
WSJ	S-Material
test	O
set	O
.	O

languages	O
)	O
,	O
whereas	O
[	O
reference	O
]	O
showed	O
higher	O
performance	O
in	O
10	O
languages	O
in	O
particular	O
English	S-Material
,	O
Greek	S-Material
,	O
Brazilian	B-Material
Portuguese	E-Material
and	O
Estonian	S-Material
.	O

section	O
:	O
Part	B-Task
-	I-Task
of	I-Task
-	I-Task
Speech	I-Task
Tagging	E-Task
on	O
WSJ	S-Material
We	O
also	O
performed	O
experiments	O
on	O
the	O
Penn	B-Material
Treebank	E-Material
with	O
the	O
usual	O
split	O
in	O
train	O
,	O
development	O
and	O
test	O
set	O
.	O

Table	O
3	O
shows	O
the	O
results	O
of	O
our	O
model	O
in	O
comparison	O
to	O
the	O
results	O
reported	O
in	O
state	O
-	O
ofthe	O
-	O
art	O
literature	O
.	O

Our	O
model	O
significantly	O
outperforms	O
these	O
systems	O
,	O
with	O
an	O
absolute	O
difference	O
of	O
0.32	O
%	O
in	O
accuracy	S-Metric
,	O
which	O
corresponds	O
to	O
a	O
RRIE	S-Metric
of	O
12	O
%	O
.	O

section	O
:	O
Morphological	B-Task
Tagging	E-Task
Results	O
In	O
addition	O
to	O
the	O
XPOS	B-Task
tagging	E-Task
experiments	O
,	O
we	O
performed	O
experiments	O
with	O
morphological	B-Task
tagging	E-Task
.	O

This	O
annotation	O
was	O
part	O
of	O
the	O
CONLL	B-Task
2017	I-Task
Shared	I-Task
Task	E-Task
and	O
the	O
objective	O
was	O
to	O
predict	O
a	O
bundle	O
of	O
morphological	O
features	O
for	O
each	O
token	O
in	O
the	O
text	O
.	O

Our	O
model	O
treats	O
the	O
morphological	O
bundle	O
as	O
one	O
tag	O
making	O
the	O
problem	O
equivalent	O
to	O
a	O
sequential	B-Task
tagging	I-Task
problem	E-Task
.	O

Table	O
4	O
shows	O
the	O
results	O
.	O

Our	O
models	O
tend	O
to	O
produce	O
significantly	O
better	O
results	O
than	O
the	O
winners	O
of	O
the	O
CoNLL	B-Material
2017	I-Material
Shared	I-Material
Task	E-Material
(	O
i.e.	O
,	O
1.8	O
%	O
absolute	O
improvement	O
on	O
average	O
,	O
corresponding	O
to	O
a	O
RRIE	S-Metric
of	O
21.20	O
%	O
)	O
.	O

The	O
only	O
cases	O
for	O
which	O
this	O
is	O
not	O
true	O
are	O
again	O
languages	O
that	O
require	O
significant	O
segmentation	O
efforts	O
(	O
i.e.	O
,	O
Hebrew	S-Material
,	O
Chinese	S-Material
,	O
Vietnamese	S-Material
and	O
Japanese	S-Material
)	O
or	O
when	O
the	O
task	O
was	O
trivial	O
.	O

Given	O
the	O
fact	O
that	O
[	O
reference	O
]	O
obtained	O
the	O
best	O
results	O
in	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
tagging	O
by	O
a	O
significant	O
margin	O
in	O
the	O
CoNLL	B-Task
2017	I-Task
Shared	I-Task
Task	E-Task
,	O
it	O
would	O
be	O
expected	O
that	O
their	O
model	O
would	O
also	O
perform	O
significantly	O
well	O
in	O
morphological	B-Task
tagging	E-Task
since	O
the	O
tasks	O
are	O
very	O
similar	O
.	O

Since	O
they	O
did	O
not	O
participate	O
in	O
this	O
particular	O
challenge	O
,	O
we	O
decided	O
to	O
reimplement	O
their	O
system	O
to	O
serve	O
[	O
reference	O
]	O
,	O
the	O
column	O
ours	O
shows	O
our	O
system	O
with	O
a	O
sentence	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
;	O
RRIE	S-Method
gives	O
the	O
relative	O
reduction	O
in	O
error	S-Metric
between	O
the	O
Reimpl	S-Method
.	O

DQM	S-Method
and	O
sentencebased	B-Method
character	I-Method
system	E-Method
.	O

Our	O
system	O
outperforms	O
the	O
CoNLL	B-Method
Winner	E-Method
by	O
48	O
out	O
of	O
54	O
treebanks	S-Method
and	O
the	O
reimplementation	B-Method
of	I-Method
DQM	E-Method
,	O
by	O
43	O
of	O
54	O
treebanks	S-Method
,	O
with	O
6	O
ties	O
.	O

as	O
a	O
strong	O
baseline	O
.	O

As	O
expected	O
,	O
our	O
reimplementation	O
of	O
[	O
reference	O
]	O
tends	O
to	O
significantly	O
outperform	O
the	O
winners	O
of	O
the	O
CONLL	B-Material
2017	I-Material
Shared	I-Material
Task	E-Material
.	O

However	O
,	O
in	O
general	O
,	O
our	O
models	O
still	O
obtain	O
better	O
results	O
,	O
outperforming	O
Dozat	O
et	O
al	O
.	O

on	O
43	O
of	O
the	O
54	O
treebanks	O
,	O
with	O
an	O
absolute	O
difference	O
of	O
0.42	O
%	O
on	O
average	O
.	O

section	O
:	O
Ablation	B-Task
Study	E-Task
The	O
model	O
proposed	O
in	O
this	O
paper	O
of	O
a	O
MetaBiLSTM	S-Method
with	O
a	O
sentence	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
differs	O
from	O
prior	O
work	O
in	O
multiple	O
aspects	O
.	O

In	O
this	O
section	O
,	O
we	O
perform	O
ablations	S-Task
to	O
determine	O
the	O
relative	O
impact	O
of	O
each	O
modeling	O
decision	O
.	O

For	O
the	O
experimental	O
setup	O
of	O
the	O
ablation	S-Task
experiments	O
,	O
we	O
report	O
accuracy	S-Metric
scores	O
for	O
the	O
development	O
sets	O
.	O

We	O
split	O
off	O
5	O
%	O
of	O
the	O
sentences	O
from	O
each	O
training	O
corpus	O
and	O
we	O
use	O
this	O
part	O
for	O
early	B-Task
stopping	E-Task
.	O

Ablation	S-Task
experiments	O
are	O
either	O
performed	O
on	O
a	O
few	O
selected	O
treebanks	O
to	O
show	O
individual	O
language	O
results	O
or	O
averaged	O
across	O
all	O
treebanks	O
for	O
which	O
tagging	S-Task
is	O
non	O
-	O
trivial	O
.	O

section	O
:	O
Impact	O
of	O
the	O
Training	O
Schema	O
We	O
first	O
compare	O
jointly	O
training	O
the	O
three	O
model	B-Method
components	E-Method
(	O
Meta	B-Method
-	I-Method
BiLSTM	E-Method
,	O
character	B-Method
model	E-Method
,	O
word	B-Method
model	E-Method
)	O
to	O
training	O
each	O
separately	O
.	O

Table	O
5	O
shows	O
that	O
separately	O
optimized	B-Method
models	E-Method
are	O
significantly	O
more	O
accurate	S-Metric
on	O
average	O
than	O
jointly	B-Method
optimized	I-Method
models	E-Method
.	O

Separate	O
optimization	S-Method
leads	O
to	O
better	O
accuracy	S-Metric
for	O
34	O
out	O
of	O
40	O
treebanks	O
for	O
the	O
morphological	B-Task
features	I-Task
task	E-Task
and	O
for	O
30	O
out	O
of	O
39	O
treebanks	O
for	O
xpos	B-Task
tagging	E-Task
.	O

Separate	B-Method
optimization	E-Method
outperformed	O
joint	B-Method
optimization	E-Method
by	O
up	O
to	O
2.1	O
percent	O
absolute	O
,	O
while	O
joint	O
never	O
out	O
-	O
performed	O
separate	O
by	O
more	O
than	O
0.5	O
%	O
absolute	O
.	O

We	O
hypothesize	O
that	O
separately	O
training	O
the	O
models	O
forces	O
each	O
submodel	O
(	O
word	O
and	O
character	O
)	O
to	O
be	O
strong	O
enough	O
to	O
make	O
high	O
accuracy	S-Metric
predictions	O
and	O
in	O
some	O
sense	O
serves	O
as	O
a	O
regularizer	O
in	O
the	O
same	O
way	O
that	O
dropout	S-Method
does	O
for	O
individual	O
neurons	O
.	O

optimization	S-Task
.	O

section	O
:	O
Impact	O
of	O
the	O
Sentence	B-Method
-	I-Method
based	I-Method
Character	I-Method
Model	E-Method
We	O
compared	O
the	O
setup	O
with	O
sentence	O
-	O
based	O
character	O
context	O
(	O
Figure	O
1a	O
)	O
to	O
word	O
-	O
based	O
character	O
context	O
(	O
Figure	O
1b	O
)	O
.	O

We	O
selected	O
for	O
these	O
experiments	O
a	O
number	O
of	O
morphological	O
rich	O
languages	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
.	O

The	O
accuracy	S-Metric
of	O
the	O
word	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
joint	O
with	O
a	O
word	B-Method
-	I-Method
based	I-Method
model	E-Method
were	O
significantly	O
lower	O
than	O
a	O
sentence	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
.	O

We	O
conclude	O
Table	O
6	O
:	O
F1	B-Metric
score	E-Metric
for	O
selected	O
languages	O
on	O
sentence	O
vs.	O
word	B-Method
level	I-Method
character	I-Method
models	E-Method
for	O
the	O
prediction	B-Task
of	I-Task
morphology	E-Task
using	O
late	B-Task
integration	E-Task
.	O

also	O
from	O
these	O
results	O
and	O
comparing	O
with	O
results	O
of	O
the	O
reimplementation	O
of	O
DQM	S-Method
that	O
early	O
integration	O
of	O
the	O
word	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
performs	O
much	O
better	O
as	O
late	B-Method
integration	E-Method
via	O
MetaBiLSTM	S-Method
for	O
a	O
word	B-Method
-	I-Method
based	I-Method
character	I-Method
model	E-Method
.	O

Impact	O
of	O
the	O
Meta	O
-	O
BiLSTM	S-Method
Model	O
Combination	O
The	O
proposed	O
model	O
trains	O
word	B-Method
and	I-Method
character	I-Method
models	E-Method
independently	O
while	O
training	O
a	O
joint	B-Method
model	E-Method
on	O
top	O
.	O

Here	O
we	O
investigate	O
the	O
part	B-Task
-	I-Task
ofspeech	I-Task
tagging	E-Task
performance	O
of	O
the	O
joint	B-Method
model	E-Method
compared	O
with	O
the	O
word	B-Method
and	I-Method
character	I-Method
models	E-Method
on	O
their	O
own	O
(	O
using	O
hyperparameters	O
from	O
in	O
4.1	O
)	O
.	O

Table	O
7	O
shows	O
,	O
for	O
selected	O
languages	O
,	O
the	O
results	O
averaged	O
over	O
10	O
runs	O
in	O
order	O
to	O
measure	O
standard	O
deviation	O
.	O

The	O
examples	O
show	O
that	O
the	O
combined	O
model	O
has	O
significantly	O
higher	O
accuracy	S-Metric
compared	O
with	O
either	O
the	O
character	B-Method
and	I-Method
word	I-Method
models	E-Method
individually	O
.	O

section	O
:	O
Concatenation	B-Method
Strategies	E-Method
for	O
the	O
ContextSensitive	B-Method
Character	I-Method
Encodings	E-Method
The	O
proposed	O
model	O
bases	O
a	O
token	B-Method
encoding	E-Method
on	O
both	O
the	O
forward	O
and	O
the	O
backward	B-Method
character	I-Method
representations	E-Method
of	O
both	O
the	O
first	O
and	O
last	O
character	O
in	O
the	O
token	O
(	O
see	O
Equation	O
1	O
)	O
.	O

Table	O
8	O
reports	O
,	O
for	O
a	O
few	O
morphological	O
rich	O
languages	O
,	O
the	O
part	B-Task
-	I-Task
of	I-Task
-	I-Task
speech	E-Task
tagging	O
performance	O
of	O
different	O
strategies	O
to	O
gather	O
the	O
characters	O
when	O
creating	O
initial	O
word	B-Method
encodings	E-Method
.	O

The	O
strategies	O
were	O
defined	O
in	O
§	O
3.1	O
.	O

The	O
Table	O
also	O
reimplementation	O
of	O
[	O
reference	O
]	O
.	O

We	O
removed	O
,	O
for	O
all	O
systems	O
,	O
the	O
word	B-Method
model	E-Method

in	O
order	O
to	O
assess	O
each	O
strategy	O
in	O
isolation	O
.	O

The	O
performance	O
is	O
quite	O
different	O
per	O
language	O
.	O

E.g.	O

,	O
for	O
Latin	O
,	O
the	O
outputs	O
of	O
the	O
forward	B-Method
and	I-Method
backward	I-Method
LSTMs	E-Method
of	O
the	O
last	O
character	O
scored	O
highest	O
.	O

Sensitivity	S-Metric
to	O
Hyperparameter	B-Method
Search	E-Method
We	O
picked	O
Vietnamese	S-Material
for	O
a	O
more	O
in	O
-	O
depth	O
analysis	O
since	O
it	O
did	O
not	O
perform	O
well	O
and	O
investigated	O
the	O
influence	O
of	O
the	O
sizes	O
of	O
LSTMs	S-Method
for	O
the	O
word	B-Method
and	I-Method
character	I-Method
model	E-Method
on	O
the	O
accuracy	S-Metric
of	O
development	O
set	O
.	O

With	O
larger	O
network	O
sizes	O
,	O
the	O
capacity	O
of	O
the	O
network	O
increases	O
,	O
however	O
,	O
on	O
the	O
other	O
hand	O
it	O
is	O
prune	O
to	O
overfitting	O
.	O

We	O
fixed	O
all	O
the	O
hyperparameters	O
except	O
those	O
for	O
the	O
network	O
size	O
of	O
the	O
character	B-Method
model	E-Method
and	O
the	O
word	B-Method
model	E-Method
,	O
and	O
ran	O
a	O
grid	B-Method
search	E-Method
over	O
dimension	O
sizes	O
from	O
200	O
to	O
500	O
in	O
steps	O
of	O
50	O
.	O

The	O
surface	O
plot	O
in	O
3	O
shows	O
that	O
the	O
grid	O
peaks	O
with	O
more	O
moderate	O
settings	O
around	O
350	O
LSTM	S-Method
cells	O
which	O
might	O
lead	O
to	O
a	O
higher	O
accuracy	S-Metric
.	O

For	O
all	O
of	O
the	O
network	O
sizes	O
in	O
the	O
grid	B-Task
search	E-Task
,	O
we	O
still	O
observed	O
during	O
training	O
that	O
the	O
accuracy	S-Metric
reach	O
a	O
high	O
value	O
and	O
degrades	O
with	O
more	O
iterations	O
for	O
the	O
character	B-Method
and	I-Method
word	I-Method
model	E-Method
.	O

This	O
suggests	O
that	O
future	O
variants	O
of	O
this	O
model	O
might	O
benefit	O
from	O
higher	O
regularization	S-Method
.	O

Discussion	O
Generally	O
,	O
the	O
fact	O
that	O
different	O
techniques	O
for	O
creating	O
word	B-Method
encodings	E-Method
from	O
character	B-Method
encodings	E-Method
and	O
different	O
network	O
sizes	O
can	O
lead	O
to	O
different	O
accuracies	S-Metric
per	O
language	O
suggests	O
that	O
it	O
should	O
be	O
possible	O
to	O
increase	O
the	O
accuracy	S-Metric
of	O
our	O
model	O
on	O
a	O
per	O
language	O
basis	O
via	O
a	O
grid	B-Method
search	E-Method
over	O
all	O
possibilities	O
.	O

In	O
fact	O
,	O
there	O
are	O
many	O
variations	O
on	O
the	O
models	O
we	O
presented	O
in	O
this	O
work	O
(	O
e.g.	O
,	O
how	O
the	O
character	B-Method
and	I-Method
word	I-Method
models	E-Method
are	O
combined	O
with	O
the	O
meta	O
-	O
BiLSTM	S-Method
)	O
.	O

Since	O
we	O
are	O
using	O
separate	O
losses	O
,	O
we	O
could	O
also	O
change	O
our	O
training	O
schema	O
.	O

For	O
example	O
,	O
one	O
could	O
use	O
methods	O
like	O
stack	B-Method
-	I-Method
propagation	E-Method
[	O
reference	O
]	O
where	O
we	O
burn	O
-	O
in	O
the	O
character	B-Method
and	I-Method
word	I-Method
models	E-Method
and	O
then	O
train	O
the	O
meta	O
-	O
BiLSTM	S-Method
backpropagating	O
throughout	O
the	O
entire	O
network	O
.	O

section	O
:	O
Conclusions	O
We	O
presented	O
an	O
approach	O
to	O
morphosyntactic	B-Task
tagging	E-Task
that	O
combines	O
context	B-Method
-	I-Method
sensitive	I-Method
initial	I-Method
character	E-Method
and	O
word	B-Method
encodings	E-Method
with	O
a	O
meta	O
-	O
BiLSTM	S-Method
layer	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
art	O
accuracies	S-Metric
for	O
a	O
wide	O
variety	O
of	O
languages	O
.	O

section	O
:	O
section	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
as	O
well	O
as	O
Terry	O
Koo	O
,	O
Slav	O
Petrov	O
,	O
Vera	O
Axelrod	O
,	O
Kellie	O
Websterk	O
,	O
Jan	O
Botha	O
,	O
Kuzman	O
Ganchev	O
,	O
Zhuoran	O
Yu	O
,	O
Yuan	O
Zhang	O
,	O
Eva	O
Schlinger	O
,	O
Ji	O
Ma	O
,	O
and	O
John	O
Alex	O
for	O
their	O
helpful	O
suggestions	O
,	O
comments	O
and	O
discussions	O
.	O

section	O
:	O
Model	B-Method
-	I-Method
Agnostic	I-Method
Meta	I-Method
-	I-Method
Learning	E-Method
for	O
Fast	B-Task
Adaptation	I-Task
of	I-Task
Deep	I-Task
Networks	E-Task
section	O
:	O
Abstract	O
We	O
propose	O
an	O
algorithm	O
for	O
meta	B-Task
-	I-Task
learning	E-Task
that	O
is	O
model	B-Task
-	I-Task
agnostic	E-Task
,	O
in	O
the	O
sense	O
that	O
it	O
is	O
compatible	O
with	O
any	O
model	O
trained	O
with	O
gradient	B-Method
descent	E-Method
and	O
applicable	O
to	O
a	O
variety	O
of	O
different	O
learning	B-Task
problems	E-Task
,	O
including	O
classification	S-Task
,	O
regression	S-Task
,	O
and	O
reinforcement	B-Task
learning	E-Task
.	O

The	O
goal	O
of	O
meta	B-Method
-	I-Method
learning	E-Method
is	O
to	O
train	O
a	O
model	O
on	O
a	O
variety	O
of	O
learning	B-Task
tasks	E-Task
,	O
such	O
that	O
it	O
can	O
solve	O
new	O
learning	B-Task
tasks	E-Task
using	O
only	O
a	O
small	O
number	O
of	O
training	O
samples	O
.	O

In	O
our	O
approach	O
,	O
the	O
parameters	O
of	O
the	O
model	O
are	O
explicitly	O
trained	O
such	O
that	O
a	O
small	O
number	O
of	O
gradient	O
steps	O
with	O
a	O
small	O
amount	O
of	O
training	O
data	O
from	O
a	O
new	O
task	O
will	O
produce	O
good	O
generalization	S-Task
performance	O
on	O
that	O
task	O
.	O

In	O
effect	O
,	O
our	O
method	O
trains	O
the	O
model	O
to	O
be	O
easy	O
to	O
fine	O
-	O
tune	O
.	O

We	O
demonstrate	O
that	O
this	O
approach	O
leads	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
two	O
fewshot	B-Task
image	I-Task
classification	I-Task
benchmarks	E-Task
,	O
produces	O
good	O
results	O
on	O
few	B-Task
-	I-Task
shot	I-Task
regression	E-Task
,	O
and	O
accelerates	O
fine	B-Method
-	I-Method
tuning	E-Method
for	O
policy	B-Method
gradient	I-Method
reinforcement	I-Method
learning	E-Method
with	O
neural	B-Method
network	I-Method
policies	E-Method
.	O

section	O
:	O
Introduction	O
Learning	B-Task
quickly	E-Task
is	O
a	O
hallmark	O
of	O
human	B-Task
intelligence	E-Task
,	O
whether	O
it	O
involves	O
recognizing	O
objects	O
from	O
a	O
few	O
examples	O
or	O
quickly	O
learning	O
new	O
skills	O
after	O
just	O
minutes	O
of	O
experience	O
.	O

Our	O
artificial	B-Method
agents	E-Method
should	O
be	O
able	O
to	O
do	O
the	O
same	O
,	O
learning	O
and	O
adapting	O
quickly	O
from	O
only	O
a	O
few	O
examples	O
,	O
and	O
continuing	O
to	O
adapt	O
as	O
more	O
data	O
becomes	O
available	O
.	O

This	O
kind	O
of	O
fast	B-Task
and	I-Task
flexible	I-Task
learning	E-Task
is	O
challenging	O
,	O
since	O
the	O
agent	O
must	O
integrate	O
its	O
prior	O
experience	O
with	O
a	O
small	O
amount	O
of	O
new	O
information	O
,	O
while	O
avoiding	O
overfitting	O
to	O
the	O
new	O
data	O
.	O

Furthermore	O
,	O
the	O
form	O
of	O
prior	O
experience	O
and	O
new	O
data	O
will	O
depend	O
on	O
the	O
task	O
.	O

As	O
such	O
,	O
for	O
the	O
greatest	O
applicability	O
,	O
the	O
mechanism	O
for	O
learning	S-Task
to	O
learn	O
(	O
or	O
meta	B-Task
-	I-Task
learning	E-Task
)	O
should	O
be	O
general	O
to	O
the	O
task	O
and	O
1	O
University	O
of	O
California	O
,	O
Berkeley	O
2	O
OpenAI	O
.	O

Correspondence	O
to	O
:	O
Chelsea	O
Finn	O
<	O
cbfinn@eecs.berkeley.edu>.	O
section	O
:	O
Proceedings	O
of	O
the	O
34	O
th	O
International	O
Conference	O
on	O
Machine	B-Task
Learning	E-Task
,	O
Sydney	O
,	O
[	O
reference	O
][	O
reference	O
]	O
by	O
the	O
author	O
(	O
s	O
)	O
.	O

the	O
form	O
of	O
computation	O
required	O
to	O
complete	O
the	O
task	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
meta	B-Method
-	I-Method
learning	I-Method
algorithm	E-Method
that	O
is	O
general	O
and	O
model	O
-	O
agnostic	O
,	O
in	O
the	O
sense	O
that	O
it	O
can	O
be	O
directly	O
applied	O
to	O
any	O
learning	B-Task
problem	E-Task
and	O
model	O
that	O
is	O
trained	O
with	O
a	O
gradient	B-Method
descent	I-Method
procedure	E-Method
.	O

Our	O
focus	O
is	O
on	O
deep	B-Method
neural	I-Method
network	I-Method
models	E-Method
,	O
but	O
we	O
illustrate	O
how	O
our	O
approach	O
can	O
easily	O
handle	O
different	O
architectures	O
and	O
different	O
problem	O
settings	O
,	O
including	O
classification	S-Task
,	O
regression	S-Task
,	O
and	O
policy	B-Method
gradient	I-Method
reinforcement	I-Method
learning	E-Method
,	O
with	O
minimal	O
modification	O
.	O

In	O
meta	B-Method
-	I-Method
learning	E-Method
,	O
the	O
goal	O
of	O
the	O
trained	O
model	O
is	O
to	O
quickly	O
learn	O
a	O
new	O
task	O
from	O
a	O
small	O
amount	O
of	O
new	O
data	O
,	O
and	O
the	O
model	O
is	O
trained	O
by	O
the	O
meta	B-Method
-	I-Method
learner	E-Method
to	O
be	O
able	O
to	O
learn	O
on	O
a	O
large	O
number	O
of	O
different	O
tasks	O
.	O

The	O
key	O
idea	O
underlying	O
our	O
method	O
is	O
to	O
train	O
the	O
model	O
's	O
initial	O
parameters	O
such	O
that	O
the	O
model	O
has	O
maximal	O
performance	O
on	O
a	O
new	O
task	O
after	O
the	O
parameters	O
have	O
been	O
updated	O
through	O
one	O
or	O
more	O
gradient	O
steps	O
computed	O
with	O
a	O
small	O
amount	O
of	O
data	O
from	O
that	O
new	O
task	O
.	O

Unlike	O
prior	O
meta	B-Method
-	I-Method
learning	I-Method
methods	E-Method
that	O
learn	O
an	O
update	O
function	O
or	O
learning	B-Method
rule	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
our	O
algorithm	O
does	O
not	O
expand	O
the	O
number	O
of	O
learned	O
parameters	O
nor	O
place	O
constraints	O
on	O
the	O
model	O
architecture	O
(	O
e.g.	O
by	O
requiring	O
a	O
recurrent	B-Method
model	E-Method
[	O
reference	O
]	O
or	O
a	O
Siamese	B-Method
network	E-Method
[	O
reference	O
]	O
)	O
,	O
and	O
it	O
can	O
be	O
readily	O
combined	O
with	O
fully	B-Method
connected	I-Method
,	I-Method
convolutional	I-Method
,	I-Method
or	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
.	O

It	O
can	O
also	O
be	O
used	O
with	O
a	O
variety	O
of	O
loss	O
functions	O
,	O
including	O
differentiable	O
supervised	O
losses	O
and	O
nondifferentiable	O
reinforcement	O
learning	O
objectives	O
.	O

The	O
process	O
of	O
training	O
a	O
model	O
's	O
parameters	O
such	O
that	O
a	O
few	O
gradient	O
steps	O
,	O
or	O
even	O
a	O
single	O
gradient	O
step	O
,	O
can	O
produce	O
good	O
results	O
on	O
a	O
new	O
task	O
can	O
be	O
viewed	O
from	O
a	O
feature	B-Method
learning	I-Method
standpoint	E-Method
as	O
building	O
an	O
internal	B-Method
representation	E-Method
that	O
is	O
broadly	O
suitable	O
for	O
many	O
tasks	O
.	O

If	O
the	O
internal	B-Method
representation	E-Method
is	O
suitable	O
to	O
many	O
tasks	O
,	O
simply	O
fine	O
-	O
tuning	O
the	O
parameters	O
slightly	O
(	O
e.g.	O
by	O
primarily	O
modifying	O
the	O
top	O
layer	O
weights	O
in	O
a	O
feedforward	B-Method
model	E-Method
)	O
can	O
produce	O
good	O
results	O
.	O

In	O
effect	O
,	O
our	O
procedure	O
optimizes	O
for	O
models	O
that	O
are	O
easy	O
and	O
fast	O
to	O
fine	O
-	O
tune	O
,	O
allowing	O
the	O
adaptation	O
to	O
happen	O
in	O
the	O
right	O
space	O
for	O
fast	B-Task
learning	E-Task
.	O

From	O
a	O
dynamical	B-Method
systems	I-Method
standpoint	E-Method
,	O
our	O
learning	B-Method
process	E-Method
can	O
be	O
viewed	O
as	O
maximizing	O
the	O
sensitivity	O
of	O
the	O
loss	O
functions	O
of	O
new	O
tasks	O
with	O
respect	O
to	O
the	O
parameters	O
:	O
when	O
the	O
sensitivity	O
is	O
high	O
,	O
small	O
local	O
changes	O
to	O
the	O
parameters	O
can	O
lead	O
to	O
large	O
improvements	O
in	O
the	O
task	B-Metric
loss	E-Metric
.	O

The	O
primary	O
contribution	O
of	O
this	O
work	O
is	O
a	O
simple	O
modeland	B-Method
task	I-Method
-	I-Method
agnostic	I-Method
algorithm	E-Method
for	O
meta	B-Task
-	I-Task
learning	E-Task
that	O
trains	O
a	O
model	O
's	O
parameters	O
such	O
that	O
a	O
small	O
number	O
of	O
gradient	O
updates	O
will	O
lead	O
to	O
fast	B-Task
learning	E-Task
on	O
a	O
new	O
task	O
.	O

We	O
demonstrate	O
the	O
algorithm	O
on	O
different	O
model	O
types	O
,	O
including	O
fully	B-Method
connected	I-Method
and	I-Method
convolutional	I-Method
networks	E-Method
,	O
and	O
in	O
several	O
distinct	O
domains	O
,	O
including	O
few	B-Task
-	I-Task
shot	I-Task
regression	E-Task
,	O
image	B-Task
classification	E-Task
,	O
and	O
reinforcement	B-Task
learning	E-Task
.	O

Our	O
evaluation	O
shows	O
that	O
our	O
meta	B-Method
-	I-Method
learning	I-Method
algorithm	E-Method
compares	O
favorably	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
one	B-Method
-	I-Method
shot	I-Method
learning	I-Method
methods	E-Method
designed	O
specifically	O
for	O
supervised	B-Task
classification	E-Task
,	O
while	O
using	O
fewer	O
parameters	O
,	O
but	O
that	O
it	O
can	O
also	O
be	O
readily	O
applied	O
to	O
regression	S-Task
and	O
can	O
accelerate	O
reinforcement	B-Task
learning	E-Task
in	O
the	O
presence	O
of	O
task	O
variability	O
,	O
substantially	O
outperforming	O
direct	B-Method
pretraining	E-Method
as	O
initialization	S-Task
.	O

section	O
:	O
Model	B-Method
-	I-Method
Agnostic	I-Method
Meta	I-Method
-	I-Method
Learning	E-Method
We	O
aim	O
to	O
train	O
models	O
that	O
can	O
achieve	O
rapid	B-Task
adaptation	E-Task
,	O
a	O
problem	O
setting	O
that	O
is	O
often	O
formalized	O
as	O
few	B-Method
-	I-Method
shot	I-Method
learning	E-Method
.	O

In	O
this	O
section	O
,	O
we	O
will	O
define	O
the	O
problem	O
setup	O
and	O
present	O
the	O
general	O
form	O
of	O
our	O
algorithm	O
.	O

section	O
:	O
Meta	B-Task
-	I-Task
Learning	I-Task
Problem	E-Task
Set	O
-	O
Up	O
The	O
goal	O
of	O
few	B-Method
-	I-Method
shot	I-Method
meta	I-Method
-	I-Method
learning	E-Method
is	O
to	O
train	O
a	O
model	O
that	O
can	O
quickly	O
adapt	O
to	O
a	O
new	O
task	O
using	O
only	O
a	O
few	O
datapoints	O
and	O
training	O
iterations	O
.	O

To	O
accomplish	O
this	O
,	O
the	O
model	O
or	O
learner	S-Method
is	O
trained	O
during	O
a	O
meta	B-Method
-	I-Method
learning	I-Method
phase	E-Method
on	O
a	O
set	O
of	O
tasks	O
,	O
such	O
that	O
the	O
trained	O
model	O
can	O
quickly	O
adapt	O
to	O
new	O
tasks	O
using	O
only	O
a	O
small	O
number	O
of	O
examples	O
or	O
trials	O
.	O

In	O
effect	O
,	O
the	O
meta	B-Task
-	I-Task
learning	I-Task
problem	E-Task
treats	O
entire	O
tasks	O
as	O
training	O
examples	O
.	O

In	O
this	O
section	O
,	O
we	O
formalize	O
this	O
metalearning	B-Task
problem	I-Task
setting	E-Task
in	O
a	O
general	O
manner	O
,	O
including	O
brief	O
examples	O
of	O
different	O
learning	B-Task
domains	E-Task
.	O

We	O
will	O
discuss	O
two	O
different	O
learning	B-Task
domains	E-Task
in	O
detail	O
in	O
Section	O
3	O
.	O

We	O
consider	O
a	O
model	O
,	O
denoted	O
f	S-Method
,	O
that	O
maps	O
observations	O
x	O
to	O
outputs	O
a.	O
During	O
meta	B-Method
-	I-Method
learning	E-Method
,	O
the	O
model	O
is	O
trained	O
to	O
be	O
able	O
to	O
adapt	O
to	O
a	O
large	O
or	O
infinite	O
number	O
of	O
tasks	O
.	O

Since	O
we	O
would	O
like	O
to	O
apply	O
our	O
framework	O
to	O
a	O
variety	O
of	O
learning	B-Task
problems	E-Task
,	O
from	O
classification	S-Task
to	O
reinforcement	B-Task
learning	E-Task
,	O
we	O
introduce	O
a	O
generic	O
notion	O
of	O
a	O
learning	B-Task
task	E-Task


below	O
.	O

Formally	O
,	O
each	O
task	O
T	O
=	O
{	O
L	O
(	O
x	O
1	O
,	O
a	O
1	O
,	O
.	O

.	O


.	O


,	O
x	O
H	O
,	O
a	O
H	O
)	O
,	O
q	O
(	O
x	O
1	O
)	O
,	O
q	O
(	O
x	O
t	O
+	O
1	O
|x	O
t	O
,	O
a	O
t	O
)	O
,	O
H	O
}	O
consists	O
of	O
a	O
loss	B-Method
function	I-Method
L	E-Method
,	O
a	O
distribution	O
over	O
initial	O
observations	O
q	O
(	O
x	O
1	O
)	O
,	O
a	O
transition	B-Method
distribution	E-Method
q	O
(	O
x	O
t	O
+	O
1	O
|x	O
t	O
,	O
a	O
t	O
)	O
,	O
and	O
an	O
episode	O
length	O
H.	O
In	O
i.i.d	O
.	O

supervised	B-Task
learning	I-Task
problems	E-Task


,	O
the	O
length	O
H	O
=	O
1	O
.	O

The	O
model	O
may	O
generate	O
samples	O
of	O
length	O
H	O
by	O
choosing	O
an	O
output	O
a	O
t	O
at	O
each	O
time	O
t.	O
The	O
loss	O
L	O
(	O
x	O
1	O
,	O
a	O
1	O
,	O
.	O

.	O


.	O


,	O
x	O
H	O
,	O
a	O
H	O
)	O
→	O
R	S-Method
,	O
provides	O
task	O
-	O
specific	O
feedback	O
,	O
which	O
might	O
be	O
in	O
the	O
form	O
of	O
a	O
misclassification	O
loss	O
or	O
a	O
cost	O
function	O
in	O
a	O
Markov	B-Method
decision	I-Method
process	E-Method
.	O

Figure	O
1	O
.	O

Diagram	O
of	O
our	O
model	B-Method
-	I-Method
agnostic	I-Method
meta	I-Method
-	I-Method
learning	I-Method
algorithm	E-Method
(	O
MAML	S-Method
)	O
,	O
which	O
optimizes	O
for	O
a	O
representation	B-Method
θ	E-Method
that	O
can	O
quickly	O
adapt	O
to	O
new	O
tasks	O
.	O

In	O
our	O
meta	B-Task
-	I-Task
learning	I-Task
scenario	E-Task
,	O
we	O
consider	O
a	O
distribution	O
over	O
tasks	O
p	O
(	O
T	O
)	O
that	O
we	O
want	O
our	O
model	O
to	O
be	O
able	O
to	O
adapt	O
to	O
.	O

In	O
the	O
K	B-Task
-	I-Task
shot	I-Task
learning	I-Task
setting	E-Task
,	O
the	O
model	O
is	O
trained	O
to	O
learn	O
a	O
new	O
task	O
T	O
i	O
drawn	O
from	O
p	O
(	O
T	O
)	O
from	O
only	O
K	O
samples	O
drawn	O
from	O
q	O
i	O
and	O
feedback	O
L	O
Ti	O
generated	O
by	O
T	O
i	O
.	O

During	O
meta	B-Task
-	I-Task
training	E-Task
,	O
a	O
task	O
T	O
i	O
is	O
sampled	O
from	O
p	O
(	O
T	O
)	O
,	O
the	O
model	O
is	O
trained	O
with	O
K	O
samples	O
and	O
feedback	O
from	O
the	O
corresponding	O
loss	O
L	O
Ti	O
from	O
T	O
i	O
,	O
and	O
then	O
tested	O
on	O
new	O
samples	O
from	O
T	O
i	O
.	O

The	O
model	O
f	O
is	O
then	O
improved	O
by	O
considering	O
how	O
the	O
test	O
error	O
on	O
new	O
data	O
from	O
q	O
i	O
changes	O
with	O
respect	O
to	O
the	O
parameters	O
.	O

In	O
effect	O
,	O
the	O
test	B-Metric
error	E-Metric
on	O
sampled	B-Task
tasks	E-Task
T	O
i	O
serves	O
as	O
the	O
training	B-Metric
error	E-Metric
of	O
the	O
meta	B-Method
-	I-Method
learning	I-Method
process	E-Method
.	O

At	O
the	O
end	O
of	O
meta	B-Task
-	I-Task
training	E-Task
,	O
new	O
tasks	O
are	O
sampled	O
from	O
p	O
(	O
T	O
)	O
,	O
and	O
meta	B-Metric
-	I-Metric
performance	E-Metric
is	O
measured	O
by	O
the	O
model	O
's	O
performance	O
after	O
learning	O
from	O
K	O
samples	O
.	O

Generally	O
,	O
tasks	O
used	O
for	O
meta	B-Task
-	I-Task
testing	E-Task
are	O
held	O
out	O
during	O
meta	B-Task
-	I-Task
training	E-Task
.	O

section	O
:	O
A	O
Model	B-Method
-	I-Method
Agnostic	I-Method
Meta	I-Method
-	I-Method
Learning	E-Method
Algorithm	O
In	O
contrast	O
to	O
prior	O
work	O
,	O
which	O
has	O
sought	O
to	O
train	O
recurrent	B-Method
neural	I-Method
networks	E-Method
that	O
ingest	O
entire	O
datasets	O
[	O
reference	O
][	O
reference	O
]	O
or	O
feature	O
embeddings	O
that	O
can	O
be	O
combined	O
with	O
nonparametric	B-Method
methods	E-Method
at	O
test	O
time	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
propose	O
a	O
method	O
that	O
can	O
learn	O
the	O
parameters	O
of	O
any	O
standard	O
model	O
via	O
meta	B-Method
-	I-Method
learning	E-Method
in	O
such	O
a	O
way	O
as	O
to	O
prepare	O
that	O
model	O
for	O
fast	B-Task
adaptation	E-Task
.	O

The	O
intuition	O
behind	O
this	O
approach	O
is	O
that	O
some	O
internal	B-Method
representations	E-Method
are	O
more	O
transferrable	O
than	O
others	O
.	O

For	O
example	O
,	O
a	O
neural	B-Method
network	E-Method
might	O
learn	O
internal	O
features	O
that	O
are	O
broadly	O
applicable	O
to	O
all	O
tasks	O
in	O
p	O
(	O
T	O
)	O
,	O
rather	O
than	O
a	O
single	O
individual	O
task	O
.	O

How	O
can	O
we	O
encourage	O
the	O
emergence	O
of	O
such	O
general	B-Method
-	I-Method
purpose	I-Method
representations	E-Method
?	O
We	O
take	O
an	O
explicit	O
approach	O
to	O
this	O
problem	O
:	O
since	O
the	O
model	O
will	O
be	O
fine	O
-	O
tuned	O
using	O
a	O
gradient	B-Method
-	I-Method
based	I-Method
learning	I-Method
rule	E-Method
on	O
a	O
new	O
task	O
,	O
we	O
will	O
aim	O
to	O
learn	O
a	O
model	O
in	O
such	O
a	O
way	O
that	O
this	O
gradient	B-Method
-	I-Method
based	I-Method
learning	I-Method
rule	E-Method
can	O
make	O
rapid	O
progress	O
on	O
new	O
tasks	O
drawn	O
from	O
p	O
(	O
T	O
)	O
,	O
without	O
overfitting	O
.	O

In	O
effect	O
,	O
we	O
will	O
aim	O
to	O
find	O
model	O
parameters	O
that	O
are	O
sensitive	O
to	O
changes	O
in	O
the	O
task	O
,	O
such	O
that	O
small	O
changes	O
in	O
the	O
parameters	O
will	O
produce	O
large	O
improvements	O
on	O
the	O
loss	B-Metric
function	E-Metric
of	O
any	O
task	O
drawn	O
from	O
p	O
(	O
T	O
)	O
,	O
when	O
altered	O
in	O
the	O
direction	O
of	O
the	O
gradient	O
of	O
that	O
loss	O
(	O
see	O
Figure	O
1	O
)	O
.	O

We	O
Compute	O
adapted	O
parameters	O
with	O
gradient	B-Method
descent	E-Method
:	O
end	O
for	O
8	O
:	O
end	O
while	O
make	O
no	O
assumption	O
on	O
the	O
form	O
of	O
the	O
model	O
,	O
other	O
than	O
to	O
assume	O
that	O
it	O
is	O
parametrized	O
by	O
some	O
parameter	O
vector	O
θ	O
,	O
and	O
that	O
the	O
loss	O
function	O
is	O
smooth	O
enough	O
in	O
θ	O
that	O
we	O
can	O
use	O
gradient	B-Method
-	I-Method
based	I-Method
learning	I-Method
techniques	E-Method
.	O

Formally	O
,	O
we	O
consider	O
a	O
model	O
represented	O
by	O
a	O
parametrized	B-Method
function	I-Method
f	I-Method
θ	E-Method
with	O
parameters	O
θ	O
.	O

When	O
adapting	O
to	O
a	O
new	O
task	O
T	O
i	O
,	O
the	O
model	O
's	O
parameters	O
θ	O
become	O
θ	O
i	O
.	O

In	O
our	O
method	O
,	O
the	O
updated	O
parameter	O
vector	O
θ	O
i	O
is	O
computed	O
using	O
one	O
or	O
more	O
gradient	B-Method
descent	I-Method
updates	E-Method
on	O
task	O
T	O
i	O
.	O

For	O
example	O
,	O
when	O
using	O
one	O
gradient	B-Method
update	E-Method
,	O
.	O

The	O
step	O
size	O
α	O
may	O
be	O
fixed	O
as	O
a	O
hyperparameter	O
or	O
metalearned	O
.	O

For	O
simplicity	O
of	O
notation	O
,	O
we	O
will	O
consider	O
one	O
gradient	B-Method
update	E-Method
for	O
the	O
rest	O
of	O
this	O
section	O
,	O
but	O
using	O
multiple	B-Method
gradient	I-Method
updates	E-Method
is	O
a	O
straightforward	O
extension	O
.	O

The	O
model	O
parameters	O
are	O
trained	O
by	O
optimizing	O
for	O
the	O
performance	O
of	O
f	O
θ	O
i	O
with	O
respect	O
to	O
θ	O
across	O
tasks	O
sampled	O
from	O
p	O
(	O
T	O
)	O
.	O

More	O
concretely	O
,	O
the	O
meta	B-Metric
-	I-Metric
objective	E-Metric
is	O
as	O
follows	O
:	O
Note	O
that	O
the	O
meta	B-Method
-	I-Method
optimization	E-Method
is	O
performed	O
over	O
the	O
model	O
parameters	O
θ	O
,	O
whereas	O
the	O
objective	O
is	O
computed	O
using	O
the	O
updated	O
model	O
parameters	O
θ	O
.	O

In	O
effect	O
,	O
our	O
proposed	O
method	O
aims	O
to	O
optimize	O
the	O
model	O
parameters	O
such	O
that	O
one	O
or	O
a	O
small	O
number	O
of	O
gradient	O
steps	O
on	O
a	O
new	O
task	O
will	O
produce	O
maximally	O
effective	O
behavior	O
on	O
that	O
task	O
.	O

The	O
meta	B-Task
-	I-Task
optimization	E-Task
across	O
tasks	O
is	O
performed	O
via	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	B-Method
)	E-Method
,	O
such	O
that	O
the	O
model	O
parameters	O
θ	O
are	O
updated	O
as	O
follows	O
:	O
where	O
β	O
is	O
the	O
meta	O
step	O
size	O
.	O

The	O
full	O
algorithm	O
,	O
in	O
the	O
general	O
case	O
,	O
is	O
outlined	O
in	O
Algorithm	O
1	O
.	O

The	O
MAML	S-Method
meta	O
-	O
gradient	O
update	O
involves	O
a	O
gradient	O
through	O
a	O
gradient	O
.	O

Computationally	O
,	O
this	O
requires	O
an	O
additional	O
backward	O
pass	O
through	O
f	O
to	O
compute	O
Hessian	O
-	O
vector	O
products	O
,	O
which	O
is	O
supported	O
by	O
standard	O
deep	B-Method
learning	I-Method
libraries	E-Method
such	O
as	O
TensorFlow	S-Method
[	O
reference	O
]	O
.	O

In	O
our	O
experiments	O
,	O
we	O
also	O
include	O
a	O
comparison	O
to	O
dropping	O
this	O
backward	B-Method
pass	E-Method
and	O
using	O
a	O
first	B-Method
-	I-Method
order	I-Method
approximation	E-Method
,	O
which	O
we	O
discuss	O
in	O
Section	O
5.2	O
.	O

section	O
:	O
Species	O
of	O
MAML	S-Method
In	O
this	O
section	O
,	O
we	O
discuss	O
specific	O
instantiations	O
of	O
our	O
meta	B-Method
-	I-Method
learning	I-Method
algorithm	E-Method
for	O
supervised	B-Task
learning	E-Task
and	O
reinforcement	B-Task
learning	E-Task
.	O

The	O
domains	O
differ	O
in	O
the	O
form	O
of	O
loss	O
function	O
and	O
in	O
how	O
data	O
is	O
generated	O
by	O
the	O
task	O
and	O
presented	O
to	O
the	O
model	O
,	O
but	O
the	O
same	O
basic	O
adaptation	B-Method
mechanism	E-Method
can	O
be	O
applied	O
in	O
both	O
cases	O
.	O

section	O
:	O
Supervised	B-Task
Regression	E-Task
and	O
Classification	S-Task
Few	B-Method
-	I-Method
shot	I-Method
learning	E-Method
is	O
well	O
-	O
studied	O
in	O
the	O
domain	O
of	O
supervised	B-Task
tasks	E-Task
,	O
where	O
the	O
goal	O
is	O
to	O
learn	O
a	O
new	O
function	O
from	O
only	O
a	O
few	O
input	O
/	O
output	O
pairs	O
for	O
that	O
task	O
,	O
using	O
prior	O
data	O
from	O
similar	O
tasks	O
for	O
meta	B-Task
-	I-Task
learning	E-Task
.	O

For	O
example	O
,	O
the	O
goal	O
might	O
be	O
to	O
classify	O
images	O
of	O
a	O
Segway	O
after	O
seeing	O
only	O
one	O
or	O
a	O
few	O
examples	O
of	O
a	O
Segway	O
,	O
with	O
a	O
model	O
that	O
has	O
previously	O
seen	O
many	O
other	O
types	O
of	O
objects	O
.	O

Likewise	O
,	O
in	O
few	B-Task
-	I-Task
shot	I-Task
regression	E-Task
,	O
the	O
goal	O
is	O
to	O
predict	O
the	O
outputs	O
of	O
a	O
continuous	O
-	O
valued	O
function	O
from	O
only	O
a	O
few	O
datapoints	O
sampled	O
from	O
that	O
function	O
,	O
after	O
training	O
on	O
many	O
functions	O
with	O
similar	O
statistical	O
properties	O
.	O

To	O
formalize	O
the	O
supervised	B-Task
regression	I-Task
and	I-Task
classification	I-Task
problems	E-Task
in	O
the	O
context	O
of	O
the	O
meta	B-Task
-	I-Task
learning	I-Task
definitions	E-Task
in	O
Section	O
2.1	O
,	O
we	O
can	O
define	O
the	O
horizon	O
H	O
=	O
1	O
and	O
drop	O
the	O
timestep	O
subscript	O
on	O
x	O
t	O
,	O
since	O
the	O
model	O
accepts	O
a	O
single	O
input	O
and	O
produces	O
a	O
single	O
output	O
,	O
rather	O
than	O
a	O
sequence	O
of	O
inputs	O
and	O
outputs	O
.	O

The	O
task	O
T	O
i	O
generates	O
K	O
i.i.d	O
.	O

observations	O
x	O
from	O
q	O
i	O
,	O
and	O
the	O
task	B-Metric
loss	E-Metric
is	O
represented	O
by	O
the	O
error	O
between	O
the	O
model	O
's	O
output	O
for	O
x	O
and	O
the	O
corresponding	O
target	O
values	O
y	O
for	O
that	O
observation	O
and	O
task	O
.	O

Two	O
common	O
loss	B-Method
functions	E-Method
used	O
for	O
supervised	B-Task
classification	I-Task
and	I-Task
regression	E-Task
are	O
cross	B-Metric
-	I-Metric
entropy	E-Metric
and	O
mean	B-Metric
-	I-Metric
squared	I-Metric
error	E-Metric
(	O
MSE	S-Metric
)	O
,	O
which	O
we	O
will	O
describe	O
below	O
;	O
though	O
,	O
other	O
supervised	B-Method
loss	I-Method
functions	E-Method
may	O
be	O
used	O
as	O
well	O
.	O

For	O
regression	B-Task
tasks	E-Task
using	O
mean	B-Metric
-	I-Metric
squared	I-Metric
error	E-Metric
,	O
the	O
loss	O
takes	O
the	O
form	O
:	O
where	O
x	O
(	O
j	O
)	O
,	O
y	O
(	O
j	O
)	O
are	O
an	O
input	O
/	O
output	O
pair	O
sampled	O
from	O
task	O
T	O
i	O
.	O

In	O
K	B-Task
-	I-Task
shot	I-Task
regression	I-Task
tasks	E-Task
,	O
K	O
input	O
/	O
output	O
pairs	O
are	O
provided	O
for	O
learning	O
for	O
each	O
task	O
.	O

Similarly	O
,	O
for	O
discrete	B-Task
classification	I-Task
tasks	E-Task
with	O
a	O
crossentropy	O
loss	O
,	O
the	O
loss	O
takes	O
the	O
form	O
:	O
section	O
:	O
Algorithm	O
2	O
MAML	S-Method
for	O
Few	B-Task
-	I-Task
Shot	I-Task
Supervised	I-Task
Learning	E-Task
Require	O
:	O
p	O
(	O
T	O
)	O
:	O
distribution	O
over	O
tasks	O
Require	O
:	O
α	O
,	O
β	O
:	O
step	O
size	O
hyperparameters	O
1	O
:	O
randomly	O
initialize	O
θ	O
2	O
:	O
while	O
not	O
done	O
do	O
3	O
:	O
Sample	O
batch	O
of	O
tasks	O
Ti	O
∼	O
p	O
(	O
T	O
)	O
4	O
:	O
for	O
all	O
Ti	O
do	O
5	O
:	O
Evaluate	O
∇	O
θ	O
LT	O
i	O
(	O
f	O
θ	O
)	O
using	O
D	O
and	O
LT	O
i	O
in	O
Equation	O
(	O
2	O
)	O
or	O
(	O
3	O
)	O
7	O
:	O
Compute	O
adapted	O
parameters	O
with	O
gradient	B-Method
descent	E-Method
:	O
Sample	O
datapoints	O
D	O
i	O
=	O
{	O
x	O
(	O
j	O
)	O
,	O
y	O
(	O
j	O
)	O
}	O
from	O
Ti	O
for	O
the	O
meta	B-Task
-	I-Task
update	E-Task
9	O
:	O
end	O
for	O
10	O
:	O
Update	O
and	O
LT	O
i	O
in	O
Equation	O
2	O
or	O
3	O
11	O
:	O
end	O
while	O
According	O
to	O
the	O
conventional	O
terminology	O
,	O
K	B-Task
-	I-Task
shot	I-Task
classification	I-Task
tasks	E-Task
use	O
K	O
input	O
/	O
output	O
pairs	O
from	O
each	O
class	O
,	O
for	O
a	O
total	O
of	O
N	O
K	O
data	O
points	O
for	O
N	B-Task
-	I-Task
way	I-Task
classification	E-Task
.	O

Given	O
a	O
distribution	O
over	O
tasks	O
p	O
(	O
T	O
i	O
)	O
,	O
these	O
loss	B-Method
functions	E-Method
can	O
be	O
directly	O
inserted	O
into	O
the	O
equations	O
in	O
Section	O
2.2	O
to	O
perform	O
meta	B-Task
-	I-Task
learning	E-Task
,	O
as	O
detailed	O
in	O
Algorithm	O
2	O
.	O

section	O
:	O
Reinforcement	B-Method
Learning	E-Method
In	O
reinforcement	B-Task
learning	E-Task
(	O
RL	S-Task
)	O
,	O
the	O
goal	O
of	O
few	B-Task
-	I-Task
shot	I-Task
metalearning	E-Task
is	O
to	O
enable	O
an	O
agent	O
to	O
quickly	O
acquire	O
a	O
policy	O
for	O
a	O
new	O
test	O
task	O
using	O
only	O
a	O
small	O
amount	O
of	O
experience	O
in	O
the	O
test	O
setting	O
.	O

A	O
new	O
task	O
might	O
involve	O
achieving	O
a	O
new	O
goal	O
or	O
succeeding	O
on	O
a	O
previously	O
trained	O
goal	O
in	O
a	O
new	O
environment	O
.	O

For	O
example	O
,	O
an	O
agent	O
might	O
learn	O
to	O
quickly	O
figure	O
out	O
how	O
to	O
navigate	O
mazes	O
so	O
that	O
,	O
when	O
faced	O
with	O
a	O
new	O
maze	O
,	O
it	O
can	O
determine	O
how	O
to	O
reliably	O
reach	O
the	O
exit	O
with	O
only	O
a	O
few	O
samples	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
discuss	O
how	O
MAML	S-Method
can	O
be	O
applied	O
to	O
meta	B-Method
-	I-Method
learning	E-Method
for	O
RL	S-Task
.	O

Each	O
RL	B-Task
task	E-Task
T	O
i	O
contains	O
an	O
initial	O
state	O
distribution	O
q	O
i	O
(	O
x	O
1	O
)	O
and	O
a	O
transition	B-Method
distribution	E-Method
q	O
i	O
(	O
x	O
t	O
+	O
1	O
|x	O
t	O
,	O
a	O
t	O
)	O
,	O
and	O
the	O
loss	O
L	O
Ti	O
corresponds	O
to	O
the	O
(	O
negative	O
)	O
reward	O
function	O
R.	O
The	O
entire	O
task	O
is	O
therefore	O
a	O
Markov	B-Method
decision	I-Method
process	I-Method
(	I-Method
MDP	I-Method
)	E-Method
with	O
horizon	B-Method
H	E-Method
,	O
where	O
the	O
learner	O
is	O
allowed	O
to	O
query	O
a	O
limited	O
number	O
of	O
sample	O
trajectories	O
for	O
few	B-Method
-	I-Method
shot	I-Method
learning	E-Method
.	O

Any	O
aspect	O
of	O
the	O
MDP	S-Method
may	O
change	O
across	O
tasks	O
in	O
p	O
(	O
T	O
)	O
.	O

The	O
model	O
being	O
learned	O
,	O
f	B-Method
θ	E-Method
,	O
is	O
a	O
policy	S-Method
that	O
maps	O
from	O
states	O
x	O
t	O
to	O
a	O
distribution	O
over	O
actions	O
a	O
t	O
at	O
each	O
timestep	O
t	O
∈	O
{	O
1	O
,	O
...	O
,	O
H}.	O
The	O
loss	O
for	O
task	O
T	O
i	O
and	O
model	B-Method
f	I-Method
φ	E-Method
takes	O
the	O
form	O
In	O
K	B-Task
-	I-Task
shot	I-Task
reinforcement	I-Task
learning	E-Task
,	O
K	O
rollouts	O
from	O
f	O
θ	O
and	O
task	O
T	O
i	O
,	O
(	O
x	O
1	O
,	O
a	O
1	O
,	O
...	O
x	O
H	O
)	O
,	O
and	O
the	O
corresponding	O
rewards	O
R	O
(	O
x	O
t	O
,	O
a	O
t	O
)	O
,	O
may	O
be	O
used	O
for	O
adaptation	S-Task
on	O
a	O
new	O
task	O
T	O
i	O
.	O

section	O
:	O
Algorithm	O
3	O
MAML	S-Method
for	O
Reinforcement	B-Task
Learning	E-Task
Require	O
:	O
p	O
(	O
T	O
)	O
:	O
distribution	O
over	O
tasks	O
Require	O
:	O
α	O
,	O
β	O
:	O
step	O
size	O
hyperparameters	O
1	O
:	O
randomly	O
initialize	O
θ	O
2	O
:	O
while	O
not	O
done	O
do	O
3	O
:	O
Sample	O
batch	O
of	O
tasks	O
Ti	O
∼	O
p	O
(	O
T	O
)	O
4	O
:	O
for	O
all	O
Ti	O
do	O
5	O
:	O
Compute	O
adapted	O
parameters	O
with	O
gradient	B-Method
descent	E-Method
:	O
Sample	O
trajectories	O
end	O
for	O
10	O
:	O
Update	O
Since	O
the	O
expected	O
reward	O
is	O
generally	O
not	O
differentiable	O
due	O
to	O
unknown	O
dynamics	O
,	O
we	O
use	O
policy	B-Method
gradient	I-Method
methods	E-Method
to	O
estimate	O
the	O
gradient	O
both	O
for	O
the	O
model	B-Method
gradient	I-Method
update	I-Method
(	I-Method
s	E-Method
)	O
and	O
the	O
meta	B-Task
-	I-Task
optimization	E-Task
.	O

Since	O
policy	B-Method
gradients	E-Method
are	O
an	O
on	B-Method
-	I-Method
policy	I-Method
algorithm	E-Method
,	O
each	O
additional	O
gradient	B-Method
step	E-Method
during	O
the	O
adaptation	B-Task
of	I-Task
f	I-Task
θ	E-Task
requires	O
new	O
samples	O
from	O
the	O
current	O
policy	O
f	O
θ	O
i	O
.	O

We	O
detail	O
the	O
algorithm	O
in	O
Algorithm	O
3	O
.	O

This	O
algorithm	O
has	O
the	O
same	O
structure	O
as	O
Algorithm	O
2	O
,	O
with	O
the	O
principal	O
difference	O
being	O
that	O
steps	O
5	O
and	O
8	O
require	O
sampling	O
trajectories	O
from	O
the	O
environment	O
corresponding	O
to	O
task	O
T	O
i	O
.	O

Practical	O
implementations	O
of	O
this	O
method	O
may	O
also	O
use	O
a	O
variety	O
of	O
improvements	O
recently	O
proposed	O
for	O
policy	B-Method
gradient	I-Method
algorithms	E-Method
,	O
including	O
state	B-Method
or	I-Method
action	I-Method
-	I-Method
dependent	I-Method
baselines	E-Method
and	O
trust	O
regions	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
The	O
method	O
that	O
we	O
propose	O
in	O
this	O
paper	O
addresses	O
the	O
general	O
problem	O
of	O
meta	B-Task
-	I-Task
learning	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
which	O
includes	O
few	B-Method
-	I-Method
shot	I-Method
learning	E-Method
.	O

A	O
popular	O
approach	O
for	O
metalearning	S-Task
is	O
to	O
train	O
a	O
meta	B-Method
-	I-Method
learner	E-Method
that	O
learns	O
how	O
to	O
update	O
the	O
parameters	O
of	O
the	O
learner	B-Method
's	I-Method
model	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

This	O
approach	O
has	O
been	O
applied	O
to	O
learning	S-Task
to	O
optimize	O
deep	B-Method
networks	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
as	O
well	O
as	O
for	O
learning	B-Task
dynamically	I-Task
changing	I-Task
recurrent	I-Task
networks	E-Task
[	O
reference	O
]	O
.	O

One	O
recent	O
approach	O
learns	O
both	O
the	O
weight	B-Method
initialization	E-Method
and	O
the	O
optimizer	S-Method
,	O
for	O
few	B-Task
-	I-Task
shot	I-Task
image	I-Task
recognition	E-Task
[	O
reference	O
]	O
.	O

Unlike	O
these	O
methods	O
,	O
the	O
MAML	S-Method
learner	O
's	O
weights	O
are	O
updated	O
using	O
the	O
gradient	O
,	O
rather	O
than	O
a	O
learned	B-Method
update	E-Method
;	O
our	O
method	O
does	O
not	O
introduce	O
additional	O
parameters	O
for	O
meta	B-Method
-	I-Method
learning	E-Method
nor	O
require	O
a	O
particular	O
learner	B-Method
architecture	E-Method
.	O

Few	B-Method
-	I-Method
shot	I-Method
learning	I-Method
methods	E-Method
have	O
also	O
been	O
developed	O
for	O
specific	O
tasks	O
such	O
as	O
generative	B-Method
modeling	E-Method
[	O
reference	O
][	O
reference	O
]	O
and	O
image	B-Task
recognition	E-Task
[	O
reference	O
]	O
.	O

One	O
successful	O
approach	O
for	O
few	B-Task
-	I-Task
shot	I-Task
classification	E-Task
is	O
to	O
learn	O
to	O
compare	O
new	O
examples	O
in	O
a	O
learned	O
metric	O
space	O
using	O
e.g.	O
Siamese	B-Method
networks	E-Method
[	O
reference	O
]	O
or	O
recurrence	S-Method
with	O
attention	B-Method
mechanisms	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

These	O
approaches	O
have	O
generated	O
some	O
of	O
the	O
most	O
successful	O
results	O
,	O
but	O
are	O
difficult	O
to	O
directly	O
extend	O
to	O
other	O
problems	O
,	O
such	O
as	O
reinforcement	B-Method
learning	E-Method
.	O

Our	O
method	O
,	O
in	O
contrast	O
,	O
is	O
agnostic	O
to	O
the	O
form	O
of	O
the	O
model	O
and	O
to	O
the	O
particular	O
learning	B-Task
task	E-Task
.	O

Another	O
approach	O
to	O
meta	B-Task
-	I-Task
learning	E-Task
is	O
to	O
train	O
memoryaugmented	B-Method
models	E-Method
on	O
many	O
tasks	O
,	O
where	O
the	O
recurrent	B-Method
learner	E-Method
is	O
trained	O
to	O
adapt	O
to	O
new	O
tasks	O
as	O
it	O
is	O
rolled	O
out	O
.	O

Such	O
networks	O
have	O
been	O
applied	O
to	O
few	B-Task
-	I-Task
shot	I-Task
image	I-Task
recognition	E-Task
[	O
reference	O
][	O
reference	O
]	O
and	O
learning	O
"	O
fast	O
"	O
reinforcement	B-Method
learning	I-Method
agents	E-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

Our	O
experiments	O
show	O
that	O
our	O
method	O
outperforms	O
the	O
recurrent	B-Method
approach	E-Method
on	O
fewshot	B-Task
classification	E-Task
.	O

Furthermore	O
,	O
unlike	O
these	O
methods	O
,	O
our	O
approach	O
simply	O
provides	O
a	O
good	O
weight	B-Method
initialization	E-Method
and	O
uses	O
the	O
same	O
gradient	B-Method
descent	I-Method
update	E-Method
for	O
both	O
the	O
learner	S-Method
and	O
meta	B-Task
-	I-Task
update	E-Task
.	O

As	O
a	O
result	O
,	O
it	O
is	O
straightforward	O
to	O
finetune	O
the	O
learner	S-Method
for	O
additional	O
gradient	O
steps	O
.	O

Our	O
approach	O
is	O
also	O
related	O
to	O
methods	O
for	O
initialization	B-Task
of	I-Task
deep	I-Task
networks	E-Task
.	O

In	O
computer	B-Task
vision	E-Task
,	O
models	O
pretrained	O
on	O
large	B-Task
-	I-Task
scale	I-Task
image	I-Task
classification	E-Task
have	O
been	O
shown	O
to	O
learn	O
effective	O
features	O
for	O
a	O
range	O
of	O
problems	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
,	O
our	O
method	O
explicitly	O
optimizes	O
the	O
model	O
for	O
fast	O
adaptability	S-Task
,	O
allowing	O
it	O
to	O
adapt	O
to	O
new	O
tasks	O
with	O
only	O
a	O
few	O
examples	O
.	O

Our	O
method	O
can	O
also	O
be	O
viewed	O
as	O
explicitly	O
maximizing	O
sensitivity	O
of	O
new	O
task	O
losses	O
to	O
the	O
model	O
parameters	O
.	O

A	O
number	O
of	O
prior	O
works	O
have	O
explored	O
sensitivity	B-Task
in	I-Task
deep	I-Task
networks	E-Task
,	O
often	O
in	O
the	O
context	O
of	O
initialization	S-Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Most	O
of	O
these	O
works	O
have	O
considered	O
good	O
random	B-Method
initializations	E-Method
,	O
though	O
a	O
number	O
of	O
papers	O
have	O
addressed	O
datadependent	B-Method
initializers	E-Method
[	O
reference	O
][	O
reference	O
]	O
,	O
including	O
learned	B-Method
initializations	E-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

In	O
contrast	O
,	O
our	O
method	O
explicitly	O
trains	O
the	O
parameters	O
for	O
sensitivity	O
on	O
a	O
given	O
task	O
distribution	O
,	O
allowing	O
for	O
extremely	O
efficient	O
adaptation	S-Task
for	O
problems	O
such	O
as	O
K	B-Task
-	I-Task
shot	I-Task
learning	E-Task
and	O
rapid	B-Task
reinforcement	I-Task
learning	E-Task
in	O
only	O
one	O
or	O
a	O
few	O
gradient	O
steps	O
.	O

section	O
:	O
Experimental	O
Evaluation	O
The	O
goal	O
of	O
our	O
experimental	O
evaluation	O
is	O
to	O
answer	O
the	O
following	O
questions	O
:	O
(	O
1	O
)	O
Can	O
MAML	S-Method
enable	O
fast	B-Task
learning	I-Task
of	I-Task
new	I-Task
tasks	E-Task
?	O
(	O
2	O
)	O
Can	O
MAML	S-Method
be	O
used	O
for	O
meta	B-Task
-	I-Task
learning	E-Task
in	O
multiple	O
different	O
domains	O
,	O
including	O
supervised	B-Task
regression	E-Task
,	O
classification	S-Task
,	O
and	O
reinforcement	B-Task
learning	E-Task
?	O
(	O
3	O
)	O
Can	O
a	O
model	O
learned	O
with	O
MAML	S-Method
continue	O
to	O
improve	O
with	O
additional	O
gradient	O
updates	O
and	O
/	O
or	O
examples	O
?	O
All	O
of	O
the	O
meta	B-Task
-	I-Task
learning	I-Task
problems	E-Task
that	O
we	O
consider	O
require	O
some	O
amount	O
of	O
adaptation	O
to	O
new	O
tasks	O
at	O
test	O
-	O
time	O
.	O

When	O
possible	O
,	O
we	O
compare	O
our	O
results	O
to	O
an	O
oracle	O
that	O
receives	O
the	O
identity	O
of	O
the	O
task	O
(	O
which	O
is	O
a	O
problem	O
-	O
dependent	O
representation	O
)	O
as	O
an	O
additional	O
input	O
,	O
as	O
an	O
upper	O
bound	O
on	O
the	O
performance	O
of	O
the	O
model	O
.	O

All	O
of	O
the	O
experiments	O
were	O
performed	O
using	O
TensorFlow	S-Method
[	O
reference	O
]	O
,	O
which	O
allows	O
for	O
automatic	B-Task
differentiation	E-Task
through	O
the	O
gradient	O
update	O
(	O
s	O
)	O
during	O
meta	B-Method
-	I-Method
learning	E-Method
.	O

The	O
code	O
is	O
available	O
online	O
1	O
.	O

section	O
:	O
Regression	S-Task
We	O
start	O
with	O
a	O
simple	O
regression	B-Task
problem	E-Task
that	O
illustrates	O
the	O
basic	O
principles	O
of	O
MAML	S-Method
.	O

[	O
reference	O
]	O
.	O

The	O
baselines	O
are	O
likewise	O
trained	O
with	O
Adam	S-Method
.	O

To	O
evaluate	O
performance	O
,	O
we	O
finetune	O
a	O
single	O
meta	B-Method
-	I-Method
learned	I-Method
model	E-Method
on	O
varying	O
numbers	O
of	O
K	O
examples	O
,	O
and	O
compare	O
performance	O
to	O
two	O
baselines	O
:	O
(	O
a	O
)	O
pretraining	O
on	O
all	O
of	O
the	O
tasks	O
,	O
which	O
entails	O
training	O
a	O
network	O
to	O
regress	O
to	O
random	O
sinusoid	O
functions	O
and	O
then	O
,	O
at	O
test	O
-	O
time	O
,	O
fine	B-Method
-	I-Method
tuning	E-Method
with	O
gradient	B-Method
descent	E-Method
on	O
the	O
K	O
provided	O
points	O
,	O
using	O
an	O
automatically	O
tuned	O
step	O
size	O
,	O
and	O
(	O
b	O
)	O
an	O
oracle	O
which	O
receives	O
the	O
true	O
amplitude	O
and	O
phase	O
as	O
input	O
.	O

In	O
Appendix	O
C	O
,	O
we	O
show	O
comparisons	O
to	O
additional	O
multi	B-Method
-	I-Method
task	I-Method
and	I-Method
adaptation	I-Method
methods	E-Method
.	O

We	O
evaluate	O
performance	O
by	O
fine	O
-	O
tuning	O
the	O
model	O
learned	O
by	O
MAML	S-Method
and	O
the	O
pretrained	B-Method
model	E-Method
on	O
K	O
=	O
{	O
5	O
,	O
10	O
,	O
20	O
}	O
datapoints	O
.	O

During	O
fine	B-Task
-	I-Task
tuning	E-Task
,	O
each	O
gradient	O
step	O
is	O
computed	O
using	O
the	O
same	O
K	O
datapoints	O
.	O

The	O
qualitative	O
results	O
,	O
shown	O
in	O
Figure	O
2	O
and	O
further	O
expanded	O
on	O
in	O
Appendix	O
B	O
show	O
that	O
the	O
learned	O
model	O
is	O
able	O
to	O
quickly	O
adapt	O
with	O
only	O
5	O
datapoints	O
,	O
shown	O
as	O
purple	O
triangles	O
,	O
whereas	O
the	O
model	O
that	O
is	O
pretrained	O
using	O
standard	O
supervised	B-Method
learning	E-Method
on	O
all	O
tasks	O
is	O
unable	O
to	O
adequately	O
adapt	O
with	O
so	O
few	O
datapoints	O
without	O
catastrophic	O
overfitting	O
.	O

Crucially	O
,	O
when	O
the	O
K	O
datapoints	O
are	O
all	O
in	O
one	O
half	O
of	O
the	O
input	O
range	O
,	O
the	O
model	O
trained	O
with	O
MAML	S-Method
can	O
still	O
infer	O
the	O
amplitude	O
and	O
phase	O
in	O
the	O
other	O
half	O
of	O
the	O
range	O
,	O
demonstrating	O
that	O
the	O
MAML	S-Method
trained	O
model	O
f	O
has	O
learned	O
to	O
model	O
the	O
periodic	O
nature	O
of	O
the	O
sine	O
wave	O
.	O

Furthermore	O
,	O
we	O
observe	O
both	O
in	O
the	O
qualitative	O
and	O
quantitative	O
results	O
(	O
Figure	O
3	O
and	O
Appendix	O
B	O
)	O
that	O
the	O
model	O
learned	O
with	O
MAML	S-Method
continues	O
to	O
improve	O
with	O
additional	O
gradient	O
steps	O
,	O
despite	O
being	O
trained	O
for	O
maximal	O
performance	O
after	O
one	O
gradient	O
step	O
.	O

This	O
improvement	O
suggests	O
that	O
MAML	S-Method
optimizes	O
the	O
parameters	O
such	O
that	O
they	O
lie	O
in	O
a	O
region	O
that	O
is	O
amenable	O
to	O
fast	O
adaptation	S-Task
and	O
is	O
sensitive	O
to	O
loss	O
functions	O
from	O
p	O
(	O
T	O
)	O
,	O
as	O
discussed	O
in	O
Section	O
2.2	O
,	O
rather	O
than	O
overfitting	O
to	O
parameters	O
θ	O
that	O
only	O
improve	O
after	O
one	O
step	O
.	O

section	O
:	O
Classification	S-Task
To	O
evaluate	O
MAML	S-Method
in	O
comparison	O
to	O
prior	O
meta	B-Method
-	I-Method
learning	I-Method
and	I-Method
few	I-Method
-	I-Method
shot	I-Method
learning	I-Method
algorithms	E-Method
,	O
we	O
applied	O
our	O
method	O
to	O
few	B-Task
-	I-Task
shot	I-Task
image	I-Task
recognition	E-Task
on	O
the	O
Omniglot	S-Material
[	O
reference	O
]	O
and	O
MiniImagenet	S-Material
datasets	O
.	O

The	O
Omniglot	B-Material
dataset	E-Material
consists	O
of	O
20	O
instances	O
of	O
1623	O
characters	O
from	O
50	O
different	O
alphabets	O
.	O

Each	O
instance	O
was	O
drawn	O
by	O
a	O
different	O
person	O
.	O

The	O
MiniImagenet	S-Material
dataset	O
was	O
proposed	O
by	O
[	O
reference	O
]	O
,	O
and	O
involves	O
64	O
training	O
classes	O
,	O
12	O
validation	O
classes	O
,	O
and	O
24	O
test	O
classes	O
.	O

The	O
Omniglot	S-Material
and	O
MiniImagenet	B-Task
image	I-Task
recognition	I-Task
tasks	E-Task
are	O
the	O
most	O
common	O
recently	O
used	O
few	O
-	O
shot	O
learning	O
benchmarks	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
follow	O
the	O
experimental	O
protocol	O
proposed	O
by	O
[	O
reference	O
]	O
,	O
which	O
involves	O
fast	O
learning	O
of	O
N	B-Task
-	I-Task
way	I-Task
classification	E-Task
with	O
1	O
or	O
5	O
shots	O
.	O

The	O
problem	O
of	O
N	B-Task
-	I-Task
way	I-Task
classification	E-Task
is	O
set	O
up	O
as	O
follows	O
:	O
select	O
N	O
unseen	O
classes	O
,	O
provide	O
the	O
model	O
with	O
K	O
different	O
instances	O
of	O
each	O
of	O
the	O
N	O
classes	O
,	O
and	O
evaluate	O
the	O
model	O
's	O
ability	O
to	O
classify	O
new	O
instances	O
within	O
the	O
N	O
classes	O
.	O

For	O
Omniglot	S-Material
,	O
we	O
randomly	O
select	O
1200	O
characters	O
for	O
training	O
,	O
irrespective	O
of	O
alphabet	O
,	O
and	O
use	O
the	O
remaining	O
for	O
testing	O
.	O

The	O
Omniglot	B-Material
dataset	E-Material
is	O
augmented	O
with	O
rotations	O
by	O
multiples	O
of	O
90	O
degrees	O
,	O
as	O
proposed	O
by	O
[	O
reference	O
]	O
.	O

Our	O
model	O
follows	O
the	O
same	O
architecture	O
as	O
the	O
embedding	O
function	O
used	O
by	O
[	O
reference	O
]	O
,	O
which	O
has	O
4	O
modules	O
with	O
a	O
3	O
×	O
3	O
convolutions	O
and	O
64	O
filters	O
,	O
followed	O
by	O
batch	B-Method
normalization	E-Method
[	O
reference	O
]	O
,	O
a	O
ReLU	B-Method
nonlinearity	E-Method
,	O
and	O
2	B-Method
×	I-Method
2	I-Method
max	I-Method
-	I-Method
pooling	E-Method
.	O

The	O
Omniglot	S-Material
images	O
are	O
downsampled	O
to	O
28	O
×	O
28	O
,	O
so	O
the	O
dimensionality	O
of	O
the	O
last	O
hidden	O
layer	O
is	O
64	O
.	O

As	O
in	O
the	O
baseline	B-Method
classifier	E-Method
used	O
by	O
[	O
reference	O
]	O
,	O
the	O
last	O
layer	O
is	O
fed	O
into	O
a	O
softmax	S-Method
.	O

For	O
Omniglot	S-Material
,	O
we	O
used	O
strided	B-Method
convolutions	E-Method
instead	O
of	O
max	B-Method
-	I-Method
pooling	E-Method
.	O

For	O
MiniImagenet	S-Material
,	O
we	O
used	O
32	O
filters	O
per	O
layer	O
to	O
reduce	O
overfitting	O
,	O
as	O
done	O
by	O
[	O
reference	O
]	O
.	O

In	O
order	O
to	O
also	O
provide	O
a	O
fair	O
comparison	O
against	O
memory	B-Method
-	I-Method
augmented	I-Method
neural	I-Method
networks	E-Method
[	O
reference	O
]	O
and	O
to	O
test	O
the	O
flexibility	O
of	O
MAML	S-Method
,	O
we	O
also	O
provide	O
results	O
for	O
a	O
non	B-Method
-	I-Method
convolutional	I-Method
network	E-Method
.	O

For	O
this	O
,	O
we	O
use	O
a	O
network	O
with	O
4	O
hidden	O
layers	O
with	O
sizes	O
256	O
,	O
128	O
,	O
64	O
,	O
64	O
,	O
each	O
including	O
batch	B-Method
normalization	E-Method
and	O
ReLU	O
nonlinearities	O
,	O
followed	O
by	O
a	O
linear	B-Method
layer	E-Method
and	O
softmax	S-Method
.	O

For	O
all	O
models	O
,	O
the	O
loss	B-Metric
function	E-Metric
is	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
error	E-Metric
between	O
the	O
predicted	O
and	O
true	O
class	O
.	O

Additional	O
hyperparameter	O
details	O
are	O
included	O
in	O
Appendix	O
A.1	O
.	O

We	O
present	O
the	O
results	O
in	O
Table	O
1	O
.	O

The	O
convolutional	B-Method
model	E-Method
learned	O
by	O
MAML	S-Method
compares	O
well	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
this	O
task	O
,	O
narrowly	O
outperforming	O
the	O
prior	O
methods	O
.	O

Some	O
of	O
these	O
existing	O
methods	O
,	O
such	O
as	O
matching	B-Method
networks	E-Method
,	O
Siamese	B-Method
networks	E-Method
,	O
and	O
memory	B-Method
models	E-Method
are	O
designed	O
with	O
few	B-Task
-	I-Task
shot	I-Task
classification	E-Task
in	O
mind	O
,	O
and	O
are	O
not	O
readily	O
applicable	O
to	O
domains	O
such	O
as	O
reinforcement	B-Task
learning	E-Task
.	O

Additionally	O
,	O
the	O
model	O
learned	O
with	O
MAML	S-Method
uses	O
[	O
reference	O
]	O
.	O

5	B-Metric
-	I-Metric
way	I-Metric
Accuracy	E-Metric
20	O
-	O
way	O
Accuracy	O
Omniglot	S-Material
[	O
reference	O
]	O
1	B-Method
-	I-Method
shot	I-Method
5	I-Method
-	I-Method
shot	I-Method
1	I-Method
-	I-Method
shot	I-Method
5	I-Method
-	I-Method
shot	I-Method
MANN	E-Method
,	O
no	O
conv	O
[	O
reference	O
]	O
82.8	O
%	O
94.9	O
%	O
--	O
MAML	S-Method
,	O
no	O
conv	O
(	O
ours	O
)	O
89.7	O
±	O
1.1	O
%	O
97.5	O
±	O
0.6	O
%	O
--	B-Method
Siamese	I-Method
nets	E-Method
[	O
reference	O
]	O
97.3	O
%	O
98.4	O
%	O
88.2	O
%	O
97.0	O
%	O
matching	B-Method
nets	E-Method
[	O
reference	O
]	O
98.1	O
%	O
98.9	O
%	O
93.8	O
%	O
98.5	O
%	O
neural	B-Metric
statistician	E-Metric
[	O
reference	O
]	O
98.1	O
%	O
99.5	O
%	O
93.2	O
%	O
98.1	O
%	O
memory	B-Metric
mod	E-Metric
.	O

[	O
reference	O
]	O
98.4	O
%	O
99.6	O
%	O
95.0	O
%	O
98.6	O
%	O
MAML	S-Method
(	O
ours	O
)	O
98.7	O
±	O
0.4	O
%	O
99.9	O
±	O
0.1	O
%	O
95.8	O
±	O
0.3	O
%	O
98.9	O
±	O
0.2	O
%	O
5	B-Metric
-	I-Metric
way	I-Metric
Accuracy	E-Metric
MiniImagenet	S-Material
[	O
reference	O
]	O
1	O
-	O
shot	O
5	O
-	O
shot	O
fine	O
-	O
tuning	O
baseline	O
28.86	O
±	O
0.54	O
%	O
49.79	O
±	O
0.79	O
%	O
nearest	B-Method
neighbor	I-Method
baseline	E-Method
41.08	O
±	O
0.70	O
%	O
51.04	O
±	O
0.65	O
%	O
matching	B-Method
nets	E-Method
[	O
reference	O
]	O
43.56	O
±	O
0.84	O
%	O
55.31	O
±	O
0.73	O
%	O
meta	B-Method
-	I-Method
learner	I-Method
LSTM	E-Method
[	O
reference	O
]	O
43.44	O
±	O
0.77	O
%	O
60.60	O
±	O
0.71	O
%	O
MAML	S-Method
,	O
first	B-Method
order	I-Method
approx	E-Method
.	O

(	O
ours	O
)	O
48.07	O
±	O
1.75	O
%	O
63.15	O
±	O
0.91	O
%	O
MAML	S-Method
(	O
ours	O
)	O
48.70	O
±	O
1.84	O
%	O
63.11	O
±	O
0.92	O
%	O
fewer	O
overall	O
parameters	O
compared	O
to	O
matching	B-Method
networks	E-Method
and	O
the	O
meta	B-Method
-	I-Method
learner	I-Method
LSTM	E-Method
,	O
since	O
the	O
algorithm	O
does	O
not	O
introduce	O
any	O
additional	O
parameters	O
beyond	O
the	O
weights	O
of	O
the	O
classifier	S-Method
itself	O
.	O

Compared	O
to	O
these	O
prior	O
methods	O
,	O
memory	B-Method
-	I-Method
augmented	I-Method
neural	I-Method
networks	E-Method
[	O
reference	O
]	O
specifically	O
,	O
and	O
recurrent	B-Method
meta	I-Method
-	I-Method
learning	I-Method
models	E-Method
in	O
general	O
,	O
represent	O
a	O
more	O
broadly	O
applicable	O
class	O
of	O
methods	O
that	O
,	O
like	O
MAML	S-Method
,	O
can	O
be	O
used	O
for	O
other	O
tasks	O
such	O
as	O
reinforcement	B-Task
learning	E-Task
[	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
as	O
shown	O
in	O
the	O
comparison	O
,	O
MAML	S-Method
significantly	O
outperforms	O
memory	B-Method
-	I-Method
augmented	I-Method
networks	E-Method
and	O
the	O
meta	B-Method
-	I-Method
learner	I-Method
LSTM	E-Method
on	O
5	O
-	O
way	O
Omniglot	S-Material
and	O
MiniImagenet	S-Material
classification	O
,	O
both	O
in	O
the	O
1	B-Task
-	I-Task
shot	I-Task
and	I-Task
5	I-Task
-	I-Task
shot	I-Task
case	E-Task
.	O

A	O
significant	O
computational	B-Metric
expense	E-Metric
in	O
MAML	S-Method
comes	O
from	O
the	O
use	O
of	O
second	O
derivatives	O
when	O
backpropagating	O
the	O
meta	O
-	O
gradient	O
through	O
the	O
gradient	B-Method
operator	E-Method
in	O
the	O
meta	O
-	O
objective	O
(	O
see	O
Equation	O
(	O
1	O
)	O
)	O
.	O

On	O
MiniImagenet	S-Material
,	O
we	O
show	O
a	O
comparison	O
to	O
a	O
first	O
-	O
order	O
approximation	O
of	O
MAML	S-Method
,	O
where	O
these	O
second	O
derivatives	O
are	O
omitted	O
.	O

Note	O
that	O
the	O
resulting	O
method	O
still	O
computes	O
the	O
meta	O
-	O
gradient	O
at	O
the	O
post	O
-	O
update	O
parameter	O
values	O
θ	O
i	O
,	O
which	O
provides	O
for	O
effective	O
meta	B-Task
-	I-Task
learning	E-Task
.	O

Surprisingly	O
however	O
,	O
the	O
performance	O
of	O
this	O
method	O
is	O
nearly	O
the	O
same	O
as	O
that	O
obtained	O
with	O
full	O
second	O
derivatives	O
,	O
suggesting	O
that	O
most	O
of	O
the	O
improvement	O
in	O
MAML	S-Method
comes	O
from	O
the	O
gradients	O
of	O
the	O
objective	O
at	O
the	O
post	O
-	O
update	O
parameter	O
values	O
,	O
rather	O
than	O
the	O
second	B-Method
order	I-Method
updates	E-Method
from	O
differentiating	O
through	O
the	O
gradient	B-Method
update	E-Method
.	O

Past	O
work	O
has	O
observed	O
that	O
ReLU	B-Method
neural	I-Method
networks	E-Method
are	O
locally	O
almost	O
linear	O
[	O
reference	O
]	O
,	O
which	O
suggests	O
that	O
second	O
derivatives	O
may	O
be	O
close	O
to	O
zero	O
in	O
most	O
cases	O
,	O
partially	O
explaining	O
the	O
good	O
perfor	O
-	O
mance	O
of	O
the	O
first	B-Method
-	I-Method
order	I-Method
approximation	E-Method
.	O

This	O
approximation	O
removes	O
the	O
need	O
for	O
computing	O
Hessian	O
-	O
vector	O
products	O
in	O
an	O
additional	O
backward	O
pass	O
,	O
which	O
we	O
found	O
led	O
to	O
roughly	O
33	O
%	O
speed	O
-	O
up	O
in	O
network	B-Task
computation	E-Task
.	O

section	O
:	O
Reinforcement	B-Method
Learning	E-Method
To	O
evaluate	O
MAML	S-Method
on	O
reinforcement	B-Task
learning	I-Task
problems	E-Task
,	O
we	O
constructed	O
several	O
sets	O
of	O
tasks	O
based	O
off	O
of	O
the	O
simulated	O
continuous	O
control	O
environments	O
in	O
the	O
rllab	O
benchmark	O
suite	O
[	O
reference	O
]	O
.	O

We	O
discuss	O
the	O
individual	O
domains	O
below	O
.	O

In	O
all	O
of	O
the	O
domains	O
,	O
the	O
model	O
trained	O
by	O
MAML	S-Method
is	O
a	O
neural	B-Method
network	I-Method
policy	E-Method
with	O
two	O
hidden	O
layers	O
of	O
size	O
100	O
,	O
with	O
ReLU	O
nonlinearities	O
.	O

The	O
gradient	B-Method
updates	E-Method
are	O
computed	O
using	O
vanilla	B-Method
policy	I-Method
gradient	E-Method
(	O
RE	B-Method
-	I-Method
INFORCE	E-Method
)	O
[	O
reference	O
]	O
,	O
and	O
we	O
use	O
trust	B-Method
-	I-Method
region	I-Method
policy	I-Method
optimization	E-Method
(	O
TRPO	S-Method
)	O
as	O
the	O
meta	B-Method
-	I-Method
optimizer	E-Method
[	O
reference	O
]	O
.	O

In	O
order	O
to	O
avoid	O
computing	O
third	O
derivatives	O
,	O
Figure	O
5	O
.	O

Reinforcement	B-Method
learning	E-Method
results	O
for	O
the	O
half	B-Task
-	I-Task
cheetah	I-Task
and	I-Task
ant	I-Task
locomotion	I-Task
tasks	E-Task
,	O
with	O
the	O
tasks	O
shown	O
on	O
the	O
far	O
right	O
.	O

Each	O
gradient	B-Method
step	E-Method
requires	O
additional	O
samples	O
from	O
the	O
environment	O
,	O
unlike	O
the	O
supervised	B-Task
learning	I-Task
tasks	E-Task
.	O

The	O
results	O
show	O
that	O
MAML	S-Method
can	O
adapt	O
to	O
new	O
goal	O
velocities	O
and	O
directions	O
substantially	O
faster	O
than	O
conventional	O
pretraining	S-Method
or	O
random	B-Method
initialization	E-Method
,	O
achieving	O
good	O
performs	O
in	O
just	O
two	O
or	O
three	O
gradient	O
steps	O
.	O

We	O
exclude	O
the	O
goal	O
velocity	O
,	O
random	O
baseline	O
curves	O
,	O
since	O
the	O
returns	O
are	O
much	O
worse	O
(	O
<	O
−200	O
for	O
cheetah	O
and	O
<	O
−25	O
for	O
ant	O
)	O
.	O

we	O
use	O
finite	B-Method
differences	E-Method
to	O
compute	O
the	O
Hessian	B-Method
-	I-Method
vector	I-Method
products	E-Method
for	O
TRPO	S-Method
.	O

For	O
both	O
learning	B-Task
and	I-Task
meta	I-Task
-	I-Task
learning	I-Task
updates	E-Task
,	O
we	O
use	O
the	O
standard	O
linear	B-Method
feature	I-Method
baseline	E-Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
which	O
is	O
fitted	O
separately	O
at	O
each	O
iteration	O
for	O
each	O
sampled	O
task	O
in	O
the	O
batch	O
.	O

We	O
compare	O
to	O
three	O
baseline	O
models	O
:	O
(	O
a	O
)	O
pretraining	B-Method
one	I-Method
policy	E-Method
on	O
all	O
of	O
the	O
tasks	O
and	O
then	O
fine	B-Task
-	I-Task
tuning	E-Task
,	O
(	O
b	O
)	O
training	O
a	O
policy	O
from	O
randomly	O
initialized	O
weights	O
,	O
and	O
(	O
c	O
)	O
an	O
oracle	B-Method
policy	E-Method
which	O
receives	O
the	O
parameters	O
of	O
the	O
task	O
as	O
input	O
,	O
which	O
for	O
the	O
tasks	O
below	O
corresponds	O
to	O
a	O
goal	O
position	O
,	O
goal	O
direction	O
,	O
or	O
goal	O
velocity	O
for	O
the	O
agent	O
.	O

The	O
baseline	O
models	O
of	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
are	O
fine	O
-	O
tuned	O
with	O
gradient	B-Method
descent	E-Method
with	O
a	O
manually	O
tuned	O
step	O
size	O
.	O

Videos	O
of	O
the	O
learned	O
policies	O
can	O
be	O
viewed	O
at	O
sites.google.com	O
/	O
view	O
/	O
maml	O
2D	O
Navigation	O
.	O

In	O
our	O
first	O
meta	B-Task
-	I-Task
RL	E-Task
experiment	O
,	O
we	O
study	O
a	O
set	O
of	O
tasks	O
where	O
a	O
point	O
agent	O
must	O
move	O
to	O
different	O
goal	O
positions	O
in	O
2D	O
,	O
randomly	O
chosen	O
for	O
each	O
task	O
within	O
a	O
unit	O
square	O
.	O

The	O
observation	O
is	O
the	O
current	O
2D	O
position	O
,	O
and	O
actions	O
correspond	O
to	O
velocity	O
commands	O
clipped	O
to	O
be	O
in	O
the	O
range	O
[	O
−0.1	O
,	O
0.1	O
]	O
.	O

The	O
reward	O
is	O
the	O
negative	O
squared	O
distance	O
to	O
the	O
goal	O
,	O
and	O
episodes	O
terminate	O
when	O
the	O
agent	O
is	O
within	O
0.01	O
of	O
the	O
goal	O
or	O
at	O
the	O
horizon	O
of	O
H	O
=	O
100	O
.	O

The	O
policy	O
was	O
trained	O
with	O
MAML	S-Method
to	O
maximize	O
performance	O
after	O
1	O
policy	B-Method
gradient	I-Method
update	E-Method
using	O
20	O
trajectories	O
.	O

Additional	O
hyperparameter	O
settings	O
for	O
this	O
problem	O
and	O
the	O
following	O
RL	B-Task
problems	E-Task
are	O
in	O
Appendix	O
A.2	O
.	O

In	O
our	O
evaluation	O
,	O
we	O
compare	O
adaptation	S-Task
to	O
a	O
new	O
task	O
with	O
up	O
to	O
4	O
gradient	O
updates	O
,	O
each	O
with	O
40	O
samples	O
.	O

The	O
results	O
in	O
Figure	O
4	O
show	O
the	O
adaptation	S-Task
performance	O
of	O
models	O
that	O
are	O
initialized	O
with	O
MAML	S-Method
,	O
conventional	O
pretraining	S-Method
on	O
the	O
same	O
set	O
of	O
tasks	O
,	O
random	B-Method
initialization	E-Method
,	O
and	O
an	O
oracle	B-Method
policy	E-Method
that	O
receives	O
the	O
goal	O
position	O
as	O
input	O
.	O

The	O
results	O
show	O
that	O
MAML	S-Method
can	O
learn	O
a	O
model	O
that	O
adapts	O
much	O
more	O
quickly	O
in	O
a	O
single	O
gradient	O
update	O
,	O
and	O
furthermore	O
continues	O
to	O
improve	O
with	O
additional	O
updates	O
.	O

Locomotion	S-Task
.	O

To	O
study	O
how	O
well	O
MAML	S-Method
can	O
scale	O
to	O
more	O
complex	O
deep	B-Task
RL	I-Task
problems	E-Task
,	O
we	O
also	O
study	O
adaptation	S-Task
on	O
high	B-Task
-	I-Task
dimensional	I-Task
locomotion	I-Task
tasks	E-Task
with	O
the	O
MuJoCo	B-Method
simulator	E-Method
[	O
reference	O
]	O
.	O

The	O
tasks	O
require	O
two	O
simulated	O
robots	O
-	O
a	O
planar	O
cheetah	O
and	O
a	O
3D	O
quadruped	O
(	O
the	O
"	O
ant	O
"	O
)	O
-	O
to	O
run	O
in	O
a	O
particular	O
direction	O
or	O
at	O
a	O
particular	O
velocity	O
.	O

In	O
the	O
goal	O
velocity	O
experiments	O
,	O
the	O
reward	O
is	O
the	O
negative	O
absolute	O
value	O
between	O
the	O
current	O
velocity	O
of	O
the	O
agent	O
and	O
a	O
goal	O
,	O
which	O
is	O
chosen	O
uniformly	O
at	O
random	O
between	O
0.0	O
and	O
2.0	O
for	O
the	O
cheetah	O
and	O
between	O
0.0	O
and	O
3.0	O
for	O
the	O
ant	O
.	O

In	O
the	O
goal	O
direction	O
experiments	O
,	O
the	O
reward	O
is	O
the	O
magnitude	O
of	O
the	O
velocity	O
in	O
either	O
the	O
forward	O
or	O
backward	O
direction	O
,	O
chosen	O
at	O
random	O
for	O
each	O
task	O
in	O
p	O
(	O
T	O
)	O
.	O

The	O
horizon	O
is	O
H	O
=	O
200	O
,	O
with	O
20	O
rollouts	O
per	O
gradient	O
step	O
for	O
all	O
problems	O
except	O
the	O
ant	B-Task
forward	I-Task
/	I-Task
backward	I-Task
task	E-Task
,	O
which	O
used	O
40	O
rollouts	O
per	O
step	O
.	O

The	O
results	O
in	O
[	O
reference	O
]	O
show	O
that	O
MAML	S-Method
learns	O
a	O
model	O
that	O
can	O
quickly	O
adapt	O
its	O
velocity	O
and	O
direction	O
with	O
even	O
just	O
a	O
single	O
gradient	O
update	O
,	O
and	O
continues	O
to	O
improve	O
with	O
more	O
gradient	O
steps	O
.	O

The	O
results	O
also	O
show	O
that	O
,	O
on	O
these	O
challenging	O
tasks	O
,	O
the	O
MAML	S-Method
initialization	O
substantially	O
outperforms	O
random	B-Method
initialization	E-Method
and	O
pretraining	S-Method
.	O

In	O
fact	O
,	O
pretraining	S-Method
is	O
in	O
some	O
cases	O
worse	O
than	O
random	B-Method
initialization	E-Method
,	O
a	O
fact	O
observed	O
in	O
prior	O
RL	O
work	O
[	O
reference	O
]	O
.	O

section	O
:	O
Discussion	O
and	O
Future	O
Work	O
We	O
introduced	O
a	O
meta	B-Method
-	I-Method
learning	I-Method
method	E-Method
based	O
on	O
learning	O
easily	O
adaptable	O
model	O
parameters	O
through	O
gradient	B-Method
descent	E-Method
.	O

Our	O
approach	O
has	O
a	O
number	O
of	O
benefits	O
.	O

It	O
is	O
simple	O
and	O
does	O
not	O
introduce	O
any	O
learned	O
parameters	O
for	O
metalearning	S-Task
.	O

It	O
can	O
be	O
combined	O
with	O
any	O
model	B-Method
representation	E-Method
that	O
is	O
amenable	O
to	O
gradient	B-Method
-	I-Method
based	I-Method
training	E-Method
,	O
and	O
any	O
differentiable	B-Task
objective	E-Task
,	O
including	O
classification	S-Task
,	O
regression	S-Task
,	O
and	O
reinforcement	B-Task
learning	E-Task
.	O

Lastly	O
,	O
since	O
our	O
method	O
merely	O
produces	O
a	O
weight	B-Method
initialization	E-Method
,	O
adaptation	S-Task
can	O
be	O
performed	O
with	O
any	O
amount	O
of	O
data	O
and	O
any	O
number	O
of	O
gradient	O
steps	O
,	O
though	O
we	O
demonstrate	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
classification	S-Task
with	O
only	O
one	O
or	O
five	O
examples	O
per	O
class	O
.	O

We	O
also	O
show	O
that	O
our	O
method	O
can	O
adapt	O
an	O
RL	B-Method
agent	E-Method
using	O
policy	O
gradients	O
and	O
a	O
very	O
modest	O
amount	O
of	O
experience	O
.	O

Reusing	O
knowledge	O
from	O
past	O
tasks	O
may	O
be	O
a	O
crucial	O
ingredient	O
in	O
making	O
high	O
-	O
capacity	B-Method
scalable	I-Method
models	E-Method
,	O
such	O
as	O
deep	B-Method
neural	I-Method
networks	E-Method
,	O
amenable	O
to	O
fast	B-Task
training	E-Task
with	O
small	O
datasets	O
.	O

We	O
believe	O
that	O
this	O
work	O
is	O
one	O
step	O
toward	O
a	O
simple	O
and	O
general	O
-	O
purpose	O
meta	B-Method
-	I-Method
learning	I-Method
technique	E-Method
that	O
can	O
be	O
applied	O
to	O
any	O
problem	O
and	O
any	O
model	O
.	O

Further	O
research	O
in	O
this	O
area	O
can	O
make	O
multitask	B-Task
initialization	E-Task
a	O
standard	O
ingredient	O
in	O
deep	B-Task
learning	E-Task
and	O
reinforcement	B-Task
learning	E-Task
.	O

section	O
:	O
A.	O
Additional	O
Experiment	O
Details	O
In	O
this	O
section	O
,	O
we	O
provide	O
additional	O
details	O
of	O
the	O
experimental	O
set	O
-	O
up	O
and	O
hyperparameters	O
.	O

section	O
:	O
A.1	O
.	O

Classification	S-Task
For	O
N	B-Task
-	I-Task
way	I-Task
,	I-Task
K	I-Task
-	I-Task
shot	I-Task
classification	E-Task
,	O
each	O
gradient	O
is	O
computed	O
using	O
a	O
batch	O
size	O
of	O
N	O
K	O
examples	O
.	O

For	O
Omniglot	S-Material
,	O
the	O
5	O
-	O
way	O
convolutional	O
and	O
non	O
-	O
convolutional	O
MAML	S-Method
models	O
were	O
each	O
trained	O
with	O
1	O
gradient	O
step	O
with	O
step	O
size	O
α	O
=	O
0.4	O
and	O
a	O
meta	O
batch	O
-	O
size	O
of	O
32	O
tasks	O
.	O

The	O
network	O
was	O
evaluated	O
using	O
3	O
gradient	O
steps	O
with	O
the	O
same	O
step	O
size	O
α	O
=	O
0.4	O
.	O

The	O
20	O
-	O
way	O
convolutional	O
MAML	B-Method
model	E-Method
was	O
trained	O
and	O
evaluated	O
with	O
5	O
gradient	O
steps	O
with	O
step	O
size	O
α	O
=	O
0.1	O
.	O

During	O
training	O
,	O
the	O
meta	B-Metric
batch	I-Metric
-	I-Metric
size	E-Metric
was	O
set	O
to	O
16	O
tasks	O
.	O

For	O
MiniImagenet	S-Material
,	O
both	O
models	O
were	O
trained	O
using	O
5	O
gradient	O
steps	O
of	O
size	O
α	O
=	O
0.01	O
,	O
and	O
evaluated	O
using	O
10	O
gradient	O
steps	O
at	O
test	O
time	O
.	O

Following	O
[	O
reference	O
]	O
,	O
15	O
examples	O
per	O
class	O
were	O
used	O
for	O
evaluating	O
the	O
post	O
-	O
update	O
meta	O
-	O
gradient	O
.	O

We	O
used	O
a	O
meta	O
batch	O
-	O
size	O
of	O
4	O
and	O
2	O
tasks	O
for	O
1	B-Task
-	I-Task
shot	I-Task
and	I-Task
5	I-Task
-	I-Task
shot	I-Task
training	E-Task
respectively	O
.	O

All	O
models	O
were	O
trained	O
for	O
60000	O
iterations	O
on	O
a	O
single	O
NVIDIA	O
Pascal	O
Titan	O
X	O
GPU	O
.	O

section	O
:	O
A.2	O
.	O

Reinforcement	B-Method
Learning	E-Method
In	O
all	O
reinforcement	B-Task
learning	E-Task
experiments	O
,	O
the	O
MAML	S-Method
policy	O
was	O
trained	O
using	O
a	O
single	O
gradient	B-Method
step	E-Method
with	O
α	O
=	O
0.1	O
.	O

During	O
evaluation	O
,	O
we	O
found	O
that	O
halving	O
the	O
learning	B-Metric
rate	E-Metric
after	O
the	O
first	O
gradient	O
step	O
produced	O
superior	O
performance	O
.	O

Thus	O
,	O
the	O
step	O
size	O
during	O
adaptation	S-Task
was	O
set	O
to	O
α	O
=	O
0.1	O
for	O
the	O
first	O
step	O
,	O
and	O
α	O
=	O
0.05	O
for	O
all	O
future	O
steps	O
.	O

The	O
step	O
sizes	O
for	O
the	O
baseline	O
methods	O
were	O
manually	O
tuned	O
for	O
each	O
domain	O
.	O

In	O
the	O
2D	B-Task
navigation	E-Task
,	O
we	O
used	O
a	O
meta	O
batch	O
size	O
of	O
20	O
;	O
in	O
the	O
locomotion	B-Task
problems	E-Task
,	O
we	O
used	O
a	O
meta	O
batch	O
size	O
of	O
40	O
tasks	O
.	O

The	O
MAML	S-Method
models	O
were	O
trained	O
for	O
up	O
to	O
500	O
meta	O
-	O
iterations	O
,	O
and	O
the	O
model	O
with	O
the	O
best	O
average	O
return	O
during	O
training	O
was	O
used	O
for	O
evaluation	O
.	O

For	O
the	O
ant	B-Task
goal	I-Task
velocity	I-Task
task	E-Task
,	O
we	O
added	O
a	O
positive	O
reward	O
bonus	O
at	O
each	O
timestep	O
to	O
prevent	O
the	O
ant	O
from	O
ending	O
the	O
episode	O
.	O

section	O
:	O
B.	O
Additional	O
Sinusoid	O
Results	O
In	O
Figure	O
6	O
,	O
we	O
show	O
the	O
full	O
quantitative	O
results	O
of	O
the	O
MAML	B-Method
model	E-Method
trained	O
on	O
10	B-Material
-	I-Material
shot	I-Material
learning	E-Material
and	O
evaluated	O
on	O
5	B-Material
-	I-Material
shot	E-Material
,	O
10	B-Material
-	I-Material
shot	E-Material
,	O
and	O
20	B-Material
-	I-Material
shot	E-Material
.	O

In	O
Figure	O
7	O
,	O
we	O
show	O
the	O
qualitative	O
performance	O
of	O
MAML	S-Method
and	O
the	O
pretrained	B-Method
baseline	E-Method
on	O
randomly	O
sampled	O
sinusoids	O
.	O

section	O
:	O
C.	O
Additional	O
Comparisons	O
In	O
this	O
section	O
,	O
we	O
include	O
more	O
thorough	O
evaluations	O
of	O
our	O
approach	O
,	O
including	O
additional	O
multi	B-Task
-	I-Task
task	I-Task
baselines	E-Task
and	O
a	O
comparison	O
representative	O
of	O
the	O
approach	O
of	O
[	O
reference	O
]	O
.	O

section	O
:	O
C.1	O
.	O

Multi	B-Task
-	I-Task
task	I-Task
baselines	E-Task
The	O
pretraining	B-Method
baseline	E-Method
in	O
the	O
main	O
text	O
trained	O
a	O
single	O
network	O
on	O
all	O
tasks	O
,	O
which	O
we	O
referred	O
to	O
as	O
"	O
pretraining	O
on	O
all	O
tasks	O
"	O
.	O

To	O
evaluate	O
the	O
model	O
,	O
as	O
with	O
MAML	S-Method
,	O
we	O
fine	O
-	O
tuned	O
this	O
model	O
on	O
each	O
test	O
task	O
using	O
K	O
examples	O
.	O

In	O
the	O
domains	O
that	O
we	O
study	O
,	O
different	O
tasks	O
involve	O
different	O
output	O
values	O
for	O
the	O
same	O
input	O
.	O

As	O
a	O
result	O
,	O
by	O
pre	O
-	O
training	O
on	O
all	O
tasks	O
,	O
the	O
model	O
would	O
learn	O
to	O
output	O
the	O
average	O
output	O
for	O
a	O
particular	O
input	O
value	O
.	O

In	O
some	O
instances	O
,	O
this	O
model	O
may	O
learn	O
very	O
little	O
about	O
the	O
actual	O
domain	O
,	O
and	O
instead	O
learn	O
about	O
the	O
range	O
of	O
the	O
output	O
space	O
.	O

We	O
experimented	O
with	O
a	O
multi	B-Method
-	I-Method
task	I-Method
method	E-Method
to	O
provide	O
a	O
point	O
of	O
comparison	O
,	O
where	O
instead	O
of	O
averaging	O
in	O
the	O
output	O
space	O
,	O
we	O
averaged	O
in	O
the	O
parameter	O
space	O
.	O

To	O
achieve	O
averaging	O
in	O
parameter	O
space	O
,	O
we	O
sequentially	O
trained	O
500	O
separate	O
models	O
on	O
500	O
tasks	O
drawn	O
from	O
p	O
(	O
T	O
)	O
.	O

Each	O
model	O
was	O
initialized	O
randomly	O
and	O
trained	O
on	O
a	O
large	O
amount	O
of	O
data	O
from	O
its	O
assigned	O
task	O
.	O

We	O
then	O
took	O
the	O
average	O
parameter	O
vector	O
across	O
models	O
and	O
fine	O
-	O
tuned	O
on	O
5	O
datapoints	O
with	O
a	O
tuned	O
step	O
size	O
.	O

All	O
of	O
our	O
experiments	O
for	O
this	O
method	O
were	O
on	O
the	O
sinusoid	B-Task
task	E-Task
because	O
of	O
computational	O
requirements	O
.	O

The	O
error	S-Metric
of	O
the	O
individual	O
regressors	O
was	O
low	O
:	O
less	O
than	O
0.02	O
on	O
their	O
respective	O
sine	O
waves	O
.	O

We	O
tried	O
three	O
variants	O
of	O
this	O
set	O
-	O
up	O
.	O

During	O
training	O
of	O
the	O
individual	O
regressors	O
,	O
we	O
tried	O
using	O
one	O
of	O
the	O
following	O
:	O
no	B-Method
regularization	E-Method
,	O
standard	O
2	B-Method
weight	I-Method
decay	E-Method
,	O
and	O
2	O
weight	B-Method
regularization	E-Method
to	O
the	O
mean	O
parameter	O
vector	O
thus	O
far	O
of	O
the	O
trained	O
regressors	O
.	O

The	O
latter	O
two	O
variants	O
encourage	O
the	O
individual	O
models	O
to	O
find	O
parsimonious	O
solutions	O
.	O

When	O
using	O
regularization	S-Method
,	O
we	O
set	O
the	O
magnitude	O
of	O
the	O
regularization	O
to	O
be	O
as	O
high	O
as	O
possible	O
without	O
significantly	O
deterring	O
performance	O
.	O

In	O
our	O
results	O
,	O
we	O
refer	O
to	O
this	O
approach	O
as	O
"	O
multi	B-Task
-	I-Task
task	E-Task
"	O
.	O

As	O
seen	O
in	O
the	O
results	O
in	O
Table	O
2	O
,	O
we	O
find	O
averaging	S-Method
in	O
the	O
parameter	O
space	O
(	O
multi	B-Task
-	I-Task
task	E-Task
)	O
performed	O
worse	O
than	O
averaging	O
in	O
the	O
output	O
space	O
(	O
pretraining	O
on	O
all	O
tasks	O
)	O
.	O

This	O
suggests	O
that	O
it	O
is	O
difficult	O
to	O
find	O
parsimonious	O
solutions	O
to	O
multiple	O
tasks	O
when	O
training	O
on	O
tasks	O
separately	O
,	O
and	O
that	O
MAML	S-Method
is	O
learning	O
a	O
solution	O
that	O
is	O
more	O
sophisticated	O
than	O
the	O
mean	O
optimal	O
parameter	O
vector	O
.	O

section	O
:	O
C.2	O
.	O

Context	B-Method
vector	I-Method
adaptation	E-Method
Rei	O
(	O
2015	O
)	O
developed	O
a	O
method	O
which	O
learns	O
a	O
context	O
vector	O
that	O
can	O
be	O
adapted	O
online	O
,	O
with	O
an	O
application	O
to	O
recurrent	B-Method
language	I-Method
models	E-Method
.	O

The	O
parameters	O
in	O
this	O
context	O
vector	O
are	O
learned	O
and	O
adapted	O
in	O
the	O
same	O
way	O
as	O
the	O
parameters	O
in	O
the	O
MAML	B-Method
model	E-Method
.	O

To	O
provide	O
a	O
comparison	O
to	O
using	O
such	O
a	O
context	O
vector	O
for	O
meta	B-Task
-	I-Task
learning	I-Task
problems	E-Task
,	O
we	O
concatenated	O
a	O
set	O
of	O
free	O
parameters	O
z	O
to	O
the	O
input	O
x	O
,	O
and	O
only	O
allowed	O
the	O
gradient	O
steps	O
to	O
modify	O
z	O
,	O
rather	O
than	O
modifying	O
the	O
model	O
parameters	O
θ	O
,	O
as	O
in	O
MAML	S-Method
.	O

For	O
im	O
-	O
Figure	O
6	O
.	O

Quantitative	B-Method
sinusoid	I-Method
regression	E-Method
results	O
showing	O
test	O
-	O
time	O
learning	O
curves	O
with	O
varying	O
numbers	O
of	O
K	O
test	O
-	O
time	O
samples	O
.	O

Each	O
gradient	O
step	O
is	O
computed	O
using	O
the	O
same	O
K	O
examples	O
.	O

Note	O
that	O
MAML	S-Method
continues	O
to	O
improve	O
with	O
additional	O
gradient	O
steps	O
without	O
overfitting	O
to	O
the	O
extremely	O
small	O
dataset	O
during	O
meta	B-Task
-	I-Task
testing	E-Task
,	O
and	O
achieves	O
a	O
loss	O
that	O
is	O
substantially	O
lower	O
than	O
the	O
baseline	B-Method
fine	I-Method
-	I-Method
tuning	I-Method
approach	E-Method
.	O

Table	O
3	O
.	O

5	O
-	O
way	O
Omniglot	B-Material
Classification	E-Material
1	O
-	O
shot	O
5	O
-	O
shot	O
context	O
vector	O
94.9	O
±	O
0.9	O
%	O
97.7	O
±	O
0.3	O
%	O
MAML	S-Method
98.7	O
±	O
0.4	O
%	O
99.9	O
±	O
0.1	O
%	O
age	O
inputs	O
,	O
z	O
was	O
concatenated	O
channel	O
-	O
wise	O
with	O
the	O
input	O
image	O
.	O

We	O
ran	O
this	O
method	O
on	O
Omniglot	S-Material
and	O
two	O
RL	O
domains	O
following	O
the	O
same	O
experimental	O
protocol	O
.	O

We	O
report	O
the	O
results	O
in	O
Tables	O
3	O
,	O
4	O
,	O
and	O
5	O
.	O

Learning	O
an	O
adaptable	O
context	O
vector	O
performed	O
well	O
on	O
the	O
toy	B-Task
pointmass	I-Task
problem	E-Task
,	O
but	O
sub	O
-	O
par	O
on	O
more	O
difficult	O
problems	O
,	O
likely	O
due	O
to	O
a	O
less	O
flexible	O
meta	B-Method
-	I-Method
optimization	E-Method
.	O

section	O
:	O
section	O
:	O
Acknowledgements	O
The	O
authors	O
would	O
like	O
to	O
thank	O
Xi	O
Chen	O
and	O
Trevor	O
Darrell	O
for	O
helpful	O
discussions	O
,	O
Yan	O
Duan	O
and	O
Alex	O
Lee	O
for	O
technical	O
advice	O
,	O
Nikhil	O
Mishra	O
,	O
Haoran	O
Tang	O
,	O
and	O
Greg	O
Kahn	O
for	O
feedback	O
on	O
an	O
early	O
draft	O
of	O
the	O
paper	O
,	O
and	O
the	O
anonymous	O
reviewers	O
for	O
their	O
comments	O
.	O

This	O
work	O
was	O
supported	O
in	O
part	O
by	O
an	O
ONR	O
PECASE	O
award	O
and	O
an	O
NSF	O
GRFP	O
award	O
.	O

section	O
:	O
document	O
:	O
Learning	O
to	O
Make	O
Predictions	B-Task
on	I-Task
Graphs	E-Task
with	O
Autoencoders	S-Method
We	O
examine	O
two	O
fundamental	O
tasks	O
associated	O
with	O
graph	B-Task
representation	I-Task
learning	E-Task
:	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

We	O
present	O
a	O
novel	O
autoencoder	B-Method
architecture	E-Method
capable	O
of	O
learning	O
a	O
joint	B-Method
representation	E-Method
of	O
both	O
local	O
graph	O
structure	O
and	O
available	O
node	O
features	O
for	O
the	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
of	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
.	O

Our	O
autoencoder	B-Method
architecture	E-Method
is	O
efficiently	O
trained	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
single	O
learning	B-Method
stage	E-Method
to	O
simultaneously	O
perform	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
,	O
whereas	O
previous	O
related	O
methods	O
require	O
multiple	O
training	O
steps	O
that	O
are	O
difficult	O
to	O
optimize	O
.	O

We	O
provide	O
a	O
comprehensive	O
empirical	O
evaluation	O
of	O
our	O
models	O
on	O
nine	O
benchmark	O
graph	O
-	O
structured	O
datasets	O
and	O
demonstrate	O
significant	O
improvement	O
over	O
related	O
methods	O
for	O
graph	B-Task
representation	I-Task
learning	E-Task
.	O

Reference	O
code	O
and	O
data	O
are	O
available	O
at	O
.	O

network	B-Task
embedding	E-Task
,	O
link	B-Task
prediction	E-Task
,	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
,	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
section	O
:	O
Introduction	O
A	O
s	O
the	O
world	O
is	O
becoming	O
increasingly	O
interconnected	O
,	O
graph	O
-	O
structured	O
data	O
are	O
also	O
growing	O
in	O
ubiquity	O
.	O

In	O
this	O
work	O
,	O
we	O
examine	O
the	O
task	O
of	O
learning	S-Task
to	O
make	O
predictions	B-Task
on	I-Task
graphs	E-Task
for	O
a	O
broad	O
range	O
of	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
.	O

Specifically	O
,	O
we	O
study	O
two	O
canonical	B-Task
subtasks	E-Task
associated	O
with	O
graph	O
-	O
structured	O
datasets	O
:	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
(	O
LPNC	S-Task
)	O
.	O

A	O
graph	O
is	O
a	O
partially	O
observed	O
set	O
of	O
edges	O
and	O
nodes	O
(	O
or	O
vertices	O
)	O
,	O
and	O
the	O
learning	B-Task
task	E-Task
is	O
to	O
predict	O
the	O
labels	O
for	O
edges	O
and	O
nodes	O
.	O

In	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
,	O
the	O
input	O
graph	O
is	O
a	O
network	O
with	O
nodes	O
representing	O
unique	O
entities	O
,	O
and	O
edges	O
representing	O
relationships	O
(	O
or	O
links	O
)	O
between	O
entities	O
.	O

Further	O
,	O
the	O
labels	O
of	O
nodes	O
and	O
edges	O
in	O
a	O
graph	O
are	O
often	O
correlated	O
,	O
exhibiting	O
complex	O
relational	O
structures	O
that	O
violate	O
the	O
general	O
assumption	O
of	O
independent	B-Method
and	I-Method
identical	I-Method
distribution	E-Method
fundamental	O
in	O
traditional	O
machine	B-Method
learning	E-Method
.	O

Therefore	O
,	O
models	O
capable	O
of	O
exploiting	O
topological	O
structures	O
of	O
graphs	O
have	O
been	O
shown	O
to	O
achieve	O
superior	O
predictive	S-Metric
performances	O
on	O
many	O
LPNC	S-Task
tasks	O
.	O

We	O
present	O
a	O
novel	O
densely	B-Method
connected	I-Method
autoencoder	I-Method
architecture	E-Method
capable	O
of	O
learning	O
a	O
shared	B-Method
representation	I-Method
of	I-Method
latent	I-Method
node	I-Method
embeddings	E-Method
from	O
both	O
local	O
graph	O
topology	O
and	O
available	O
explicit	O
node	O
features	O
for	O
LPNC	S-Task
.	O

The	O
resulting	O
autoencoder	B-Method
models	E-Method
are	O
useful	O
for	O
many	O
applications	O
across	O
multiple	O
domains	O
,	O
including	O
analysis	B-Task
of	I-Task
metabolic	I-Task
networks	E-Task
for	O
drug	B-Task
-	I-Task
target	I-Task
interaction	E-Task
,	O
bibliographic	B-Task
networks	E-Task
,	O
social	B-Task
networks	E-Task
such	O
as	O
Facebook	O
(	O
“	O
People	O
You	O
May	O
Know	O
”	O
)	O
,	O
terrorist	B-Task
networks	E-Task
,	O
communication	B-Task
networks	E-Task
,	O
cybersecurity	S-Task
,	O
recommender	B-Task
systems	E-Task
,	O
and	O
knowledge	O
bases	O
such	O
as	O
DBpedia	S-Material
and	O
Wikidata	S-Material
.	O

There	O
are	O
a	O
number	O
of	O
technical	O
challenges	O
associated	O
with	O
learning	S-Task
to	O
make	O
meaningful	B-Task
predictions	E-Task
on	O
complex	B-Task
graphs	E-Task
:	O
Extreme	B-Task
class	I-Task
imbalance	E-Task
:	O
in	O
link	B-Task
prediction	E-Task
,	O
the	O
number	O
of	O
known	O
present	O
(	O
positive	O
)	O
edges	O
is	O
often	O
significantly	O
less	O
than	O
the	O
number	O
of	O
known	O
absent	O
(	O
negative	O
)	O
edges	O
,	O
making	O
it	O
difficult	O
to	O
reliably	O
learn	O
from	O
rare	O
examples	O
;	O
Learn	O
from	O
complex	O
graph	O
structures	O
:	O
edges	O
may	O
be	O
directed	O
or	O
undirected	O
,	O
weighted	O
or	O
unweighted	O
,	O
highly	O
sparse	O
in	O
occurrence	O
,	O
and	O
/	O
or	O
consisting	O
of	O
multiple	O
types	O
.	O

A	O
useful	O
model	O
should	O
be	O
versatile	O
to	O
address	O
a	O
variety	O
of	O
graph	B-Task
types	E-Task
,	O
including	O
bipartite	O
graphs	O
;	O
Incorporate	O
side	O
information	O
:	O
nodes	O
(	O
and	O
maybe	O
edges	O
)	O
are	O
sometimes	O
described	O
by	O
a	O
set	O
of	O
features	O
,	O
called	O
side	O
information	O
,	O
that	O
could	O
encode	O
information	O
complementary	O
to	O
topological	O
features	O
of	O
the	O
input	O
graph	O
.	O

Such	O
explicit	O
data	O
on	O
nodes	O
and	O
edges	O
are	O
not	O
always	O
readily	O
available	O
and	O
are	O
considered	O
optional	O
.	O

A	O
useful	O
model	O
should	O
be	O
able	O
to	O
incorporate	O
optional	O
side	O
information	O
about	O
nodes	O
and	O
/	O
or	O
edges	O
,	O
whenever	O
available	O
,	O
to	O
potentially	O
improve	O
predictive	S-Task
performance	O
;	O
Efficiency	S-Metric
and	O
scalability	S-Metric
:	O
real	O
-	O
world	O
graph	O
datasets	O
contain	O
large	O
numbers	O
of	O
nodes	O
and	O
/	O
or	O
edges	O
.	O

It	O
is	O
essential	O
for	O
a	O
model	O
to	O
be	O
memory	O
and	O
computationally	O
efficient	O
to	O
achieve	O
practical	O
utility	O
on	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
.	O

Our	O
contribution	O
in	O
this	O
work	O
is	O
a	O
simple	O
,	O
yet	O
versatile	O
autoencoder	B-Method
architecture	E-Method
that	O
addresses	O
all	O
of	O
the	O
above	O
technical	O
challenges	O
.	O

We	O
demonstrate	O
that	O
our	O
autoencoder	B-Method
models	E-Method
:	O
1	O
)	O
can	O
handle	O
extreme	O
class	O
imbalance	O
common	O
in	O
link	B-Task
prediction	E-Task
problems	O
;	O
2	O
)	O
can	O
learn	O
expressive	O
latent	O
features	O
for	O
nodes	O
from	O
topological	O
structures	O
of	O
sparse	O
,	O
bipartite	O
graphs	O
that	O
may	O
have	O
directed	O
and	O
/	O
or	O
weighted	O
edges	O
;	O
3	O
)	O
is	O
flexible	O
to	O
incorporate	O
explicit	O
side	O
features	O
about	O
nodes	O
as	O
an	O
optional	O
component	O
to	O
improve	O
predictive	S-Task
performance	O
;	O
and	O
4	O
)	O
utilize	O
extensive	O
parameter	O
sharing	O
to	O
reduce	O
memory	B-Metric
footprint	E-Metric
and	O
computational	B-Metric
complexity	E-Metric
,	O
while	O
leveraging	O
available	O
GPU	B-Method
-	I-Method
based	I-Method
implementations	E-Method
for	O
increased	O
scalability	O
.	O

Further	O
,	O
the	O
autoencoder	B-Method
architecture	E-Method
has	O
the	O
novelty	O
of	O
being	O
efficiently	O
trained	O
end	O
-	O
to	O
-	O
end	O
for	O
the	O
joint	O
,	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
(	O
MTL	S-Task
)	O
of	O
both	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	I-Task
tasks	E-Task
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
architecture	O
capable	O
of	O
performing	O
simultaneous	B-Task
link	I-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
in	O
a	O
single	O
learning	B-Method
stage	E-Method
,	O
whereas	O
previous	O
related	O
methods	O
require	O
multiple	O
training	O
stages	O
that	O
are	O
difficult	O
to	O
optimize	O
.	O

Lastly	O
,	O
we	O
conduct	O
a	O
comprehensive	O
evaluation	O
of	O
the	O
proposed	O
autoencoder	B-Method
architecture	E-Method
on	O
nine	O
challenging	O
benchmark	O
graph	O
-	O
structured	O
datasets	O
comprising	O
a	O
wide	O
range	O
of	O
LPNC	S-Task
applications	O
.	O

Numerical	O
experiments	O
validate	O
the	O
efficacy	O
of	O
our	O
models	O
by	O
showing	O
significant	O
improvement	O
on	O
multiple	O
evaluation	B-Metric
measures	E-Metric
over	O
related	O
methods	O
designed	O
for	O
link	B-Task
prediction	E-Task
and	O
/	O
or	O
node	B-Task
classification	E-Task
.	O

section	O
:	O
Autoencoder	B-Method
Architecture	E-Method
for	O
Link	B-Task
Prediction	E-Task
and	O
Node	B-Task
Classification	E-Task
We	O
now	O
characterize	O
our	O
proposed	O
autoencoder	B-Method
architecture	E-Method
,	O
schematically	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
for	O
LPNC	S-Task
and	O
formalize	O
the	O
notation	O
used	O
in	O
this	O
paper	O
.	O

The	O
input	O
to	O
the	O
autoencoder	S-Method
is	O
a	O
graph	O
of	O
nodes	O
.	O

Graph	S-Method
is	O
represented	O
by	O
its	O
adjacency	O
matrix	O
.	O

For	O
a	O
partially	O
observed	O
graph	O
,	O
,	O
where	O
denotes	O
a	O
known	O
present	O
positive	O
edge	O
,	O
denotes	O
a	O
known	O
absent	O
negative	O
edge	O
,	O
and	O
unk	O
denotes	O
an	O
unknown	O
status	O
(	O
missing	O
or	O
unobserved	O
)	O
edge	O
.	O

In	O
general	O
,	O
the	O
input	O
to	O
the	O
autoencoder	S-Method
can	O
be	O
directed	O
or	O
undirected	O
,	O
weighted	O
or	O
unweighted	O
,	O
and	O
/	O
or	O
bipartite	O
graphs	O
.	O

However	O
,	O
for	O
the	O
remainder	O
of	O
this	O
paper	O
and	O
throughout	O
the	O
numerical	O
experiments	O
,	O
we	O
assume	O
undirected	O
and	O
symmetric	O
graphs	O
with	O
binary	O
edges	O
to	O
maintain	O
parity	O
with	O
previous	O
related	O
work	O
.	O

Optionally	O
,	O
we	O
are	O
given	O
a	O
matrix	O
of	O
available	O
explicit	O
node	O
features	O
,	O
i.e.	O
side	O
information	O
.	O

The	O
aim	O
of	O
the	O
autoencoder	B-Method
model	E-Method
is	O
to	O
learn	O
a	O
set	O
of	O
low	O
-	O
dimensional	O
latent	O
variables	O
for	O
the	O
nodes	O
that	O
can	O
produce	O
an	O
approximate	O
reconstruction	O
output	O
such	O
that	O
the	O
error	O
between	O
and	O
is	O
minimized	O
,	O
thereby	O
preserving	O
the	O
global	O
graph	O
structure	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
capital	O
variables	O
(	O
e.g.	O
,	O
)	O
to	O
denote	O
matrices	O
and	O
lower	O
-	O
case	O
variables	O
(	O
e.g.	O
,	O
)	O
to	O
denote	O
row	O
vectors	O
.	O

For	O
example	O
,	O
we	O
use	O
to	O
mean	O
the	O
th	O
row	O
of	O
the	O
matrix	O
.	O

subsection	O
:	O
Link	B-Task
Prediction	E-Task
Research	O
on	O
link	B-Task
prediction	E-Task
attempts	O
to	O
answer	O
the	O
principal	O
question	O
:	O
given	O
two	O
entities	O
,	O
should	O
there	O
be	O
a	O
connection	O
between	O
them	O
?	O
We	O
focus	O
on	O
the	O
structural	O
link	B-Task
prediction	E-Task
problem	O
,	O
where	O
the	O
task	O
is	O
to	O
compute	O
the	O
likelihood	O
that	O
an	O
unobserved	O
or	O
missing	O
edge	O
exists	O
between	O
two	O
nodes	O
in	O
a	O
partially	O
observed	O
graph	O
.	O

For	O
a	O
comprehensive	O
survey	O
on	O
link	B-Task
prediction	E-Task
,	O
to	O
include	O
structural	O
and	O
temporal	O
link	B-Task
prediction	E-Task
using	O
unsupervised	B-Method
and	I-Method
supervised	I-Method
models	E-Method
,	O
see	O
.	O

Link	B-Task
Prediction	E-Task
from	O
Graph	O
Topology	O
Let	O
be	O
an	O
adjacency	O
vector	O
of	O
that	O
contains	O
the	O
local	O
neighborhood	O
of	O
the	O
th	O
node	O
.	O

Our	O
proposed	O
autoencoder	B-Method
architecture	E-Method
comprises	O
a	O
set	O
of	O
non	B-Method
-	I-Method
linear	I-Method
transformations	E-Method
on	O
summarized	O
in	O
two	O
component	O
parts	O
:	O
encoder	S-Method
,	O
and	O
decoder	S-Method
.	O

We	O
stack	O
two	O
layers	O
of	O
the	O
encoder	B-Method
part	E-Method
to	O
derive	O
-	O
dimensional	O
latent	B-Method
feature	I-Method
representation	E-Method
of	O
the	O
th	O
node	O
,	O
and	O
then	O
stack	O
two	O
layers	O
of	O
the	O
decoder	B-Method
part	E-Method
to	O
obtain	O
an	O
approximate	O
reconstruction	O
output	O
,	O
resulting	O
in	O
a	O
four	B-Method
-	I-Method
layer	I-Method
autoencoder	I-Method
architecture	E-Method
.	O

Note	O
that	O
is	O
highly	O
sparse	O
,	O
with	O
up	O
to	O
90	O
percent	O
of	O
the	O
edges	O
missing	O
at	O
random	O
in	O
some	O
of	O
our	O
experiments	O
,	O
and	O
the	O
dense	O
reconstructed	O
output	O
contains	O
the	O
predictions	O
for	O
the	O
missing	O
edges	O
.	O

The	O
hidden	B-Method
representations	E-Method
for	O
the	O
encoder	O
and	O
decoder	O
parts	O
are	O
computed	O
as	O
follows	O
:	O
The	O
choice	O
of	O
non	B-Method
-	I-Method
linear	I-Method
,	I-Method
element	I-Method
-	I-Method
wise	I-Method
activation	I-Method
function	E-Method
is	O
the	O
rectified	B-Method
linear	I-Method
unit	E-Method
.	O

The	O
last	O
decoder	B-Method
layer	E-Method
computes	O
a	O
linear	B-Method
transformation	E-Method
to	O
score	O
the	O
missing	O
links	O
as	O
part	O
of	O
the	O
reconstruction	S-Task
.	O

We	O
constrain	O
the	O
autoencoder	S-Method
to	O
be	O
symmetrical	O
with	O
shared	O
parameters	O
for	O
between	O
the	O
encoder	O
and	O
decoder	O
parts	O
,	O
resulting	O
in	O
almost	O
fewer	O
parameters	O
than	O
an	O
unconstrained	B-Method
architecture	E-Method
.	O

Parameter	B-Method
sharing	E-Method
is	O
a	O
powerful	O
form	O
of	O
regularization	S-Method
that	O
helps	O
improve	O
learning	S-Task
and	O
generalization	S-Task
,	O
and	O
is	O
also	O
the	O
main	O
motivation	O
for	O
MTL	S-Task
,	O
first	O
explored	O
in	O
and	O
most	O
recently	O
in	O
.	O

Notice	O
the	O
bias	O
units	O
do	O
not	O
share	O
parameters	O
,	O
and	O
,	O
are	O
transposed	O
copies	O
of	O
,	O
.	O

For	O
brevity	O
of	O
notation	O
,	O
we	O
summarize	O
the	O
parameters	O
to	O
be	O
learned	O
in	O
.	O

Since	O
our	O
autoencoder	S-Method
learns	O
node	O
embeddings	O
from	O
local	O
neighborhood	O
structures	O
of	O
the	O
graph	O
,	O
we	O
refer	O
to	O
it	O
as	O
LoNGAE	S-Method
for	O
Local	B-Method
Neighborhood	I-Method
Graph	I-Method
Autoencoder	E-Method
.	O

Link	B-Task
Prediction	E-Task
with	O
Node	O
Features	O
Optionally	O
,	O
if	O
a	O
matrix	O
of	O
explicit	O
node	O
features	O
is	O
available	O
,	O
then	O
we	O
concatenate	O
to	O
obtain	O
an	O
augmented	O
adjacency	O
matrix	O
and	O
perform	O
the	O
above	O
encoder	B-Method
-	I-Method
decoder	I-Method
transformations	E-Method
on	O
for	O
link	B-Task
prediction	E-Task
.	O

We	O
refer	O
to	O
this	O
variant	O
as	O
LoNGAE	S-Method
.	O

Notice	O
the	O
augmented	O
adjacency	O
matrix	O
is	O
no	O
longer	O
square	O
and	O
symmetric	O
.	O

The	O
intuition	O
behind	O
the	O
concatenation	O
of	O
node	O
features	O
is	O
to	O
enable	O
a	O
shared	O
representation	O
of	O
both	O
graph	O
and	O
node	O
features	O
throughout	O
the	O
autoencoding	O
transformations	O
by	O
way	O
of	O
the	O
tied	O
parameters	O
.	O

This	O
idea	O
draws	O
inspiration	O
from	O
recent	O
work	O
by	O
Vukotić	O
et	O
al	O
.	O

,	O
where	O
they	O
successfully	O
applied	O
symmetrical	B-Method
autoencoders	E-Method
with	O
parameter	B-Method
sharing	E-Method
for	O
multi	B-Task
-	I-Task
modal	I-Task
and	I-Task
cross	I-Task
-	I-Task
modal	I-Task
representation	I-Task
learning	I-Task
of	I-Task
textual	I-Task
and	I-Task
visual	I-Task
features	E-Task
.	O

The	O
training	B-Metric
complexity	E-Metric
of	O
LoNGAE	S-Method
is	O
,	O
where	O
is	O
the	O
number	O
of	O
nodes	O
,	O
is	O
the	O
dimensionality	O
of	O
node	O
features	O
,	O
is	O
the	O
size	O
of	O
the	O
hidden	O
layer	O
,	O
and	O
is	O
the	O
number	O
of	O
iterations	O
.	O

In	O
practice	O
,	O
,	O
,	O
and	O
are	O
independent	O
of	O
.	O

Thus	O
,	O
the	O
overall	O
complexity	S-Metric
of	O
the	O
autoencoder	S-Method
is	O
,	O
linear	O
in	O
the	O
number	O
of	O
nodes	O
.	O

Inference	S-Task
and	O
Learning	S-Task
During	O
the	O
forward	B-Task
pass	E-Task
,	O
or	O
inference	S-Task
,	O
the	O
model	O
takes	O
as	O
input	O
an	O
adjacency	O
vector	O
and	O
computes	O
its	O
reconstructed	O
output	O
for	O
link	B-Task
prediction	E-Task
.	O

The	O
parameters	O
are	O
learned	O
via	O
backpropagation	S-Method
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
estimate	O
by	O
minimizing	O
the	O
Masked	B-Metric
Balanced	I-Metric
Cross	I-Metric
-	I-Metric
Entropy	I-Metric
(	I-Metric
MBCE	I-Metric
)	I-Metric
loss	E-Metric
,	O
which	O
only	O
allows	O
for	O
the	O
contributions	O
of	O
those	O
parameters	O
associated	O
with	O
observed	O
edges	O
,	O
as	O
in	O
.	O

Moreover	O
,	O
can	O
exhibit	O
extreme	O
class	O
imbalance	O
between	O
known	O
present	O
and	O
absent	O
links	O
,	O
as	O
is	O
common	O
in	O
link	B-Task
prediction	E-Task
problems	O
.	O

We	O
handle	O
class	O
imbalance	O
by	O
defining	O
a	O
weighting	O
factor	O
to	O
be	O
used	O
as	O
a	O
multiplier	O
for	O
the	O
positive	O
class	O
in	O
the	O
cross	B-Method
-	I-Method
entropy	I-Method
loss	I-Method
formulation	E-Method
.	O

This	O
approach	O
is	O
referred	O
to	O
as	O
balanced	B-Method
cross	I-Method
-	I-Method
entropy	E-Method
.	O

Other	O
approaches	O
to	O
class	B-Task
imbalance	E-Task
include	O
optimizing	O
for	O
a	O
ranking	B-Task
loss	E-Task
and	O
the	O
recent	O
work	O
on	O
focal	B-Task
loss	E-Task

by	O
Lin	O
et	O
al	O
.	O

.	O


For	O
a	O
single	O
example	O
and	O
its	O
reconstructed	O
output	O
,	O
we	O
compute	O
the	O
MBCE	B-Metric
loss	E-Metric
as	O
follows	O
:	O
Here	O
,	O
is	O
the	O
balanced	O
cross	O
-	O
entropy	O
loss	O
with	O
weighting	O
factor	O
,	O
is	O
the	O
sigmoid	B-Method
function	E-Method
,	O
is	O
the	O
Hadamard	O
(	O
element	O
-	O
wise	O
)	O
product	O
,	O
and	O
is	O
the	O
boolean	O
function	O
:	O
if	O
,	O
else	O
.	O

The	O
same	O
autoencoder	B-Method
architecture	E-Method
can	O
be	O
applied	O
to	O
a	O
row	O
vector	O
in	O
the	O
augmented	O
adjacency	O
matrix	O
.	O

However	O
,	O
at	O
the	O
final	O
decoder	B-Method
layer	E-Method
,	O
we	O
slice	O
the	O
reconstruction	O
into	O
two	O
outputs	O
:	O
corresponding	O
to	O
the	O
reconstructed	O
example	O
in	O
the	O
original	O
adjacency	O
matrix	O
,	O
and	O
corresponding	O
to	O
the	O
reconstructed	O
example	O
in	O
the	O
matrix	O
of	O
node	O
features	O
.	O

During	O
learning	S-Task
,	O
we	O
optimize	O
on	O
the	O
concatenation	O
of	O
graph	O
topology	O
and	O
side	O
node	O
features	O
,	O
but	O
compute	O
the	O
losses	O
for	O
the	O
reconstructed	O
outputs	O
separately	O
with	O
different	O
loss	O
functions	O
.	O

The	O
motivation	O
behind	O
this	O
design	O
is	O
to	O
maintain	O
flexibility	O
to	O
handle	O
different	O
input	O
formats	O
;	O
the	O
input	O
is	O
usually	O
binary	O
,	O
but	O
the	O
input	O
can	O
be	O
binary	O
,	O
real	O
-	O
valued	O
,	O
or	O
both	O
.	O

In	O
this	O
work	O
,	O
we	O
enforce	O
both	O
inputs	O
to	O
be	O
in	O
the	O
range	O
for	O
simplicity	O
and	O
improved	O
performance	O
,	O
and	O
compute	O
the	O
augmented	B-Method
MBCE	I-Method
loss	E-Method
as	O
follows	O
:	O
where	O
is	O
the	O
standard	O
cross	B-Method
-	I-Method
entropy	I-Method
loss	E-Method
with	O
sigmoid	B-Method
function	E-Method
.	O

At	O
inference	O
time	O
,	O
we	O
use	O
the	O
reconstructed	O
output	O
for	O
link	B-Task
prediction	E-Task
and	O
disregard	O
the	O
output	O
.	O

subsection	O
:	O
Semi	O
-	O
Supervised	O
Node	B-Task
Classification	E-Task
The	O
LoNGAE	S-Method
model	O
can	O
also	O
be	O
used	O
to	O
perform	O
efficient	O
information	B-Task
propagation	I-Task
on	I-Task
graphs	E-Task
for	O
the	O
task	O
of	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

Node	B-Task
classification	E-Task
is	O
the	O
task	O
of	O
predicting	B-Task
the	I-Task
labels	I-Task
or	I-Task
types	I-Task
of	I-Task
entities	I-Task
in	I-Task
a	I-Task
graph	E-Task
,	O
such	O
as	O
the	O
types	O
of	O
molecules	O
in	O
a	O
metabolic	O
network	O
or	O
document	O
categories	O
in	O
a	O
citation	B-Task
network	E-Task
.	O

For	O
a	O
given	O
augmented	O
adjacency	O
vector	O
,	O
the	O
autoencoder	S-Method
learns	O
the	O
corresponding	O
node	O
embeddings	O
to	O
obtain	O
an	O
optimal	B-Task
reconstruction	E-Task
.	O

Intuitively	O
,	O
encodes	O
a	O
vector	O
of	O
latent	O
features	O
derived	O
from	O
the	O
concatenation	O
of	O
both	O
graph	O
and	O
node	O
features	O
,	O
and	O
can	O
be	O
used	O
to	O
predict	O
the	O
label	O
of	O
the	O
th	O
node	O
.	O

For	O
multi	B-Task
-	I-Task
class	I-Task
classification	E-Task
,	O
we	O
can	O
decode	O
using	O
the	O
softmax	B-Method
activation	I-Method
function	E-Method
to	O
learn	O
a	O
probability	O
distribution	O
over	O
node	O
labels	O
.	O

More	O
precisely	O
,	O
we	O
predict	O
node	O
labels	O
via	O
the	O
following	O
transformation	O
:	O
,	O
where	O
and	O
.	O

In	O
many	O
applications	O
,	O
only	O
a	O
small	O
fraction	O
of	O
the	O
nodes	O
are	O
labeled	O
.	O

For	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
,	O
it	O
is	O
advantageous	O
to	O
utilize	O
unlabeled	O
examples	O
in	O
conjunction	O
with	O
labeled	O
instances	O
to	O
better	O
capture	O
the	O
underlying	O
data	O
patterns	O
for	O
improved	O
learning	S-Task
and	O
generalization	S-Task
.	O

We	O
achieve	O
this	O
by	O
jointly	O
training	O
the	O
autoencoder	S-Method
with	O
a	O
masked	B-Method
softmax	I-Method
classifier	E-Method
to	O
collectively	O
learn	O
node	O
labels	O
from	O
minimizing	O
their	O
combined	O
losses	O
:	O
where	O
is	O
the	O
set	O
of	O
node	O
labels	O
,	O
if	O
node	O
belongs	O
to	O
class	O
,	O
is	O
the	O
softmax	O
probability	O
that	O
node	O
belongs	O
to	O
class	O
,	O
is	O
the	O
loss	O
defined	O
for	O
the	O
autoencoder	S-Method
,	O
and	O
the	O
boolean	O
function	O
if	O
node	O
has	O
a	O
label	O
,	O
otherwise	O
.	O

Notice	O
in	O
this	O
configuration	O
,	O
we	O
can	O
perform	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
for	O
both	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
,	O
simultaneously	O
.	O

section	O
:	O
Related	O
Work	O
The	O
field	O
of	O
graph	B-Task
representation	I-Task
learning	E-Task
is	O
seeing	O
a	O
resurgence	O
of	O
research	O
interest	O
in	O
recent	O
years	O
,	O
driven	O
in	O
part	O
by	O
the	O
latest	O
advances	O
in	O
deep	B-Method
learning	E-Method
.	O

The	O
aim	O
is	O
to	O
learn	O
a	O
mapping	S-Method
that	O
encodes	O
the	O
input	O
graph	O
into	O
low	O
-	O
dimensional	O
feature	O
embeddings	O
while	O
preserving	O
its	O
original	O
global	O
structure	O
.	O

Hamilton	O
et	O
al	O
.	O

succinctly	O
articulate	O
the	O
diverse	O
set	O
of	O
previously	O
proposed	O
approaches	O
for	O
graph	B-Task
representation	I-Task
learning	E-Task
,	O
or	O
graph	B-Task
embedding	E-Task
,	O
as	O
belonging	O
within	O
a	O
unified	O
encoder	B-Method
-	I-Method
decoder	I-Method
framework	E-Method
.	O

In	O
this	O
section	O
,	O
we	O
summarize	O
three	O
classes	O
of	O
encoder	B-Method
-	I-Method
decoder	I-Method
models	E-Method
most	O
related	O
to	O
our	O
work	O
:	O
matrix	B-Method
factorization	E-Method
(	O
MF	S-Method
)	O
,	O
autoencoders	S-Method
,	O
and	O
graph	B-Method
convolutional	I-Method
networks	E-Method
(	O
GCNs	S-Method
)	O
.	O

MF	S-Method
has	O
its	O
roots	O
in	O
dimensionality	B-Task
reduction	E-Task
and	O
gained	O
popularity	O
with	O
extensive	O
applications	O
in	O
collaborative	B-Task
filtering	E-Task
(	O
CF	B-Task
)	E-Task
and	O
recommender	B-Task
systems	E-Task
.	O

MF	S-Method
models	O
take	O
an	O
input	O
matrix	O
,	O
learn	O
a	O
shared	B-Method
linear	I-Method
latent	I-Method
representation	E-Method
for	O
rows	O
(	O
)	O
and	O
columns	O
(	O
)	O
during	O
an	O
encoder	O
step	O
,	O
and	O
then	O
use	O
a	O
bilinear	B-Method
(	I-Method
pairwise	I-Method
)	I-Method
decoder	E-Method
based	O
on	O
the	O
inner	B-Method
product	E-Method
to	O
produce	O
a	O
reconstructed	O
matrix	O
.	O

CF	S-Method
is	O
mathematically	O
similar	O
to	O
link	B-Task
prediction	E-Task
,	O
where	O
the	O
goal	O
is	O
essentially	O
matrix	B-Task
completion	E-Task
.	O

Menon	O
and	O
Elkan	O
proposed	O
an	O
MF	S-Method
model	O
capable	O
of	O
incorporating	O
side	O
information	O
about	O
nodes	O
and	O
/	O
or	O
edges	O
to	O
demonstrate	O
strong	O
link	B-Task
prediction	E-Task
results	O
on	O
several	O
challenging	O
network	O
datasets	O
.	O

Other	O
recent	O
approaches	O
similar	O
to	O
MF	S-Method
that	O
learn	O
node	O
embeddings	O
via	O
some	O
encoder	B-Method
transformation	E-Method
and	O
then	O
use	O
a	O
bilinear	B-Method
decoder	E-Method
for	O
the	O
reconstruction	S-Task
include	O
DeepWalk	S-Method
and	O
its	O
variants	O
LINE	S-Method
and	O
node2vec	S-Method
.	O

DeepWalk	S-Method
,	O
LINE	S-Method
,	O
and	O
node2vec	S-Method
do	O
not	O
support	O
external	O
node	O
/	O
edge	O
features	O
.	O

Our	O
work	O
is	O
inspired	O
by	O
recent	O
successful	O
applications	O
of	O
autoencoder	B-Method
architectures	E-Method
for	O
collaborative	B-Task
filtering	E-Task
that	O
outperform	O
popular	O
matrix	B-Method
factorization	E-Method
methods	O
,	O
and	O
is	O
related	O
to	O
Structural	B-Method
Deep	I-Method
Network	I-Method
Embedding	E-Method
(	O
SDNE	S-Method
)	O
for	O
link	B-Task
prediction	E-Task
.	O

Similar	O
to	O
SDNE	S-Method
,	O
our	O
models	O
rely	O
on	O
the	O
autoencoder	S-Method
to	O
learn	O
non	O
-	O
linear	O
node	O
embeddings	O
from	O
local	O
graph	O
neighborhoods	O
.	O

However	O
,	O
our	O
models	O
have	O
several	O
important	O
distinctions	O
:	O
1	O
)	O
we	O
leverage	O
extensive	O
parameter	O
sharing	O
between	O
the	O
encoder	B-Method
and	I-Method
decoder	I-Method
parts	E-Method
to	O
enhance	O
representation	B-Task
learning	E-Task
;	O
2	O
)	O
our	O
LoNGAE	S-Method
model	O
can	O
optionally	O
concatenate	O
side	O
node	O
features	O
to	O
the	O
adjacency	O
matrix	O
for	O
improved	O
link	B-Task
prediction	E-Task
performance	O
;	O
and	O
3	O
)	O
the	O
LoNGAE	S-Method
model	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
single	O
stage	O
for	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
of	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

On	O
the	O
other	O
hand	O
,	O
training	O
SDNE	S-Method
requires	O
multiple	O
steps	O
that	O
are	O
difficult	O
to	O
jointly	O
optimize	O
:	O
i	O
)	O
pre	O
-	O
training	O
via	O
a	O
deep	B-Method
belief	I-Method
network	E-Method
;	O
and	O
ii	O
)	O
utilizing	O
a	O
separate	O
downstream	B-Method
classifier	E-Method
on	O
top	O
of	O
node	B-Method
embeddings	E-Method
for	O
LPNC	S-Task
.	O

Lastly	O
,	O
GCNs	S-Method
are	O
a	O
recent	O
class	O
of	O
algorithms	O
based	O
on	O
convolutional	B-Method
encoders	E-Method
for	O
learning	B-Task
node	I-Task
embeddings	E-Task
.	O

The	O
GCN	B-Method
model	E-Method
is	O
motivated	O
by	O
a	O
localized	B-Method
first	I-Method
-	I-Method
order	I-Method
approximation	I-Method
of	I-Method
spectral	I-Method
convolutions	E-Method
for	O
layer	B-Task
-	I-Task
wise	I-Task
information	I-Task
propagation	I-Task
on	I-Task
graphs	E-Task
.	O

Similar	O
to	O
our	O
LoNGAE	S-Method
model	O
,	O
the	O
GCN	B-Method
model	E-Method
can	O
learn	O
hidden	B-Method
layer	I-Method
representations	E-Method
that	O
encode	O
both	O
local	O
graph	O
structure	O
and	O
features	O
of	O
nodes	O
.	O

The	O
choice	O
of	O
the	O
decoder	O
depends	O
on	O
the	O
task	O
.	O

For	O
link	B-Task
prediction	E-Task
,	O
the	O
bilinear	B-Method
inner	I-Method
product	E-Method
is	O
used	O
in	O
the	O
context	O
of	O
the	O
variational	B-Method
graph	I-Method
autoencoder	E-Method
(	O
VGAE	S-Method
)	O
.	O

For	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
,	O
the	O
softmax	O
activation	O
function	O
is	O
employed	O
.	O

The	O
GCN	B-Method
model	E-Method
provides	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
learning	I-Method
framework	E-Method
that	O
scales	O
linearly	O
in	O
the	O
number	O
of	O
graph	O
edges	O
and	O
has	O
been	O
shown	O
to	O
achieve	O
strong	O
LPNC	S-Task
results	O
on	O
a	O
number	O
of	O
graph	O
-	O
structured	O
datasets	O
.	O

However	O
,	O
the	O
GCN	B-Method
model	E-Method
has	O
a	O
drawback	O
of	O
being	O
memory	O
intensive	O
because	O
it	O
is	O
trained	O
on	O
the	O
full	O
dataset	O
using	O
batch	B-Method
gradient	I-Method
descent	E-Method
for	O
every	O
training	O
iteration	O
.	O

We	O
show	O
that	O
our	O
models	O
outperform	O
GCN	B-Method
-	I-Method
based	I-Method
models	E-Method
for	O
LPNC	S-Task
while	O
consuming	O
a	O
constant	O
memory	O
budget	O
by	O
way	O
of	O
mini	B-Method
-	I-Method
batch	I-Method
training	E-Method
.	O

section	O
:	O
Experimental	O
Design	O
In	O
this	O
section	O
,	O
we	O
expound	O
our	O
protocol	O
for	O
the	O
empirical	O
evaluation	O
of	O
our	O
models	O
’	O
capability	O
for	O
learning	B-Task
and	I-Task
generalization	E-Task
on	O
the	O
tasks	O
of	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

Secondarily	O
,	O
we	O
also	O
present	O
results	O
of	O
the	O
models	O
’	O
representation	B-Method
capacity	E-Method
on	O
the	O
task	O
of	O
network	B-Task
reconstruction	E-Task
.	O

subsection	O
:	O
Datasets	O
and	O
Baselines	O
We	O
evaluate	O
our	O
proposed	O
autoencoder	B-Method
models	E-Method
on	O
nine	O
graph	O
-	O
structured	O
datasets	O
,	O
spanning	O
multiple	O
application	O
domains	O
,	O
from	O
which	O
previous	O
graph	B-Method
embedding	I-Method
methods	E-Method
have	O
achieved	O
strong	O
results	O
for	O
LPNC	S-Task
.	O

The	O
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
and	O
include	O
networks	O
for	O
Protein	S-Material
interactions	O
,	O
Metabolic	S-Material
pathways	O
,	O
military	O
Conflict	S-Material
between	O
countries	O
,	O
the	O
U.S.	O
PowerGrid	O
,	O
collaboration	O
between	O
users	O
on	O
the	O
BlogCatalog	S-Material
social	O
website	O
,	O
and	O
publication	O
citations	O
from	O
the	O
Cora	S-Material
,	O
Citeseer	S-Material
,	O
Pubmed	S-Material
,	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
databases	O
.	O

{	O
Protein	S-Material
,	O
Metabolic	S-Material
,	O
Conflict	S-Material
,	O
PowerGrid	O
}	O
are	O
reported	O
in	O
.	O

{	O
Cora	S-Material
,	O
Citeseer	S-Material
,	O
Pubmed	S-Material
}	O
are	O
from	O
and	O
reported	O
in	O
.	O

And	O
{	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
,	O
BlogCatalog	S-Material
}	O
are	O
reported	O
in	O
.	O

width=0.5	O
width=0.5	O
We	O
empirically	O
compare	O
our	O
autoencoder	B-Method
models	E-Method
against	O
four	O
strong	O
baselines	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
,	O
which	O
were	O
designed	O
specifically	O
for	O
link	B-Task
prediction	E-Task
and	O
/	O
or	O
node	B-Task
classification	E-Task
.	O

We	O
begin	O
our	O
empirical	O
evaluation	O
with	O
the	O
SDNE	S-Method
baseline	O
,	O
where	O
we	O
compare	O
the	O
representation	B-Metric
capacity	E-Metric
of	O
our	O
models	O
on	O
the	O
network	B-Task
reconstruction	I-Task
task	E-Task
using	O
the	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
and	O
BlogCatalog	B-Material
datasets	E-Material
.	O

For	O
the	O
MF	S-Method
baseline	O
,	O
we	O
closely	O
follow	O
the	O
experimental	O
protocol	O
in	O
,	O
where	O
we	O
randomly	O
sample	O
10	O
percent	O
of	O
the	O
observed	O
links	O
for	O
training	O
and	O
evaluate	O
link	B-Task
prediction	E-Task
performance	O
on	O
the	O
other	O
disjoint	O
90	O
percent	O
for	O
the	O
{	O
Protein	S-Material
,	O
Metabolic	S-Material
,	O
Conflict	S-Material
}	O
datasets	O
.	O

For	O
PowerGrid	S-Material
,	O
we	O
use	O
90	O
percent	O
of	O
observed	O
links	O
for	O
training	O
and	O
evaluate	O
on	O
the	O
remaining	O
10	O
percent	O
.	O

And	O
for	O
the	O
VGAE	S-Method
and	O
GCN	O
baselines	O
,	O
we	O
use	O
the	O
same	O
train	B-Metric
/	I-Metric
validation	I-Metric
/	I-Metric
test	I-Metric
segments	E-Metric
described	O
in	O
and	O
for	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
,	O
respectively	O
,	O
on	O
the	O
{	O
Cora	S-Material
,	O
Citeseer	S-Material
,	O
Pubmed	S-Material
}	O
citation	O
networks	O
.	O

subsection	O
:	O
Implementation	O
Details	O
We	O
implement	O
the	O
autoencoder	B-Method
architecture	E-Method
using	O
Keras	S-Method
on	O
top	O
of	O
the	O
GPU	B-Method
-	I-Method
enabled	I-Method
TensorFlow	I-Method
backend	E-Method
,	O
along	O
with	O
several	O
additional	O
details	O
.	O

The	O
diagonal	O
elements	O
of	O
the	O
adjacency	O
matrix	O
are	O
set	O
to	O
with	O
the	O
interpretation	O
that	O
every	O
node	O
is	O
connected	O
to	O
itself	O
.	O

We	O
impute	O
missing	O
or	O
unk	O
elements	O
in	O
the	O
adjacency	O
matrix	O
with	O
.	O

Note	O
that	O
imputed	O
edges	O
are	O
not	O
observed	O
elements	O
in	O
the	O
adjacency	O
matrix	O
and	O
hence	O
do	O
not	O
contribute	O
to	O
the	O
masked	B-Task
loss	I-Task
computations	E-Task
during	O
training	S-Task
.	O

We	O
are	O
free	O
to	O
impute	O
any	O
values	O
for	O
the	O
missing	O
edges	O
,	O
but	O
through	O
cross	B-Metric
-	I-Metric
validation	E-Metric
we	O
found	O
that	O
the	O
uniform	O
value	O
of	O
produces	O
the	O
best	O
results	O
.	O

Hyper	B-Method
-	I-Method
parameter	I-Method
tuning	E-Method
is	O
performed	O
via	O
cross	B-Method
-	I-Method
validation	E-Method
or	O
on	O
the	O
available	O
validation	O
set	O
.	O

Key	O
hyper	O
-	O
parameters	O
include	O
mini	O
-	O
batch	O
size	O
,	O
dimensionality	O
of	O
the	O
hidden	O
layers	O
,	O
and	O
the	O
percentage	O
of	O
dropout	B-Method
regularization	E-Method
.	O

In	O
general	O
,	O
we	O
strive	O
to	O
keep	O
a	O
similar	O
set	O
of	O
hyper	O
-	O
parameters	O
across	O
datasets	O
to	O
highlight	O
the	O
consistency	O
of	O
our	O
models	O
.	O

In	O
all	O
experiments	O
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
layers	O
in	O
the	O
autoencoder	B-Method
architecture	E-Method
is	O
fixed	O
at	O
-	O
256	O
-	O
128	O
-	O
256	O
-	O
.	O

For	O
reconstruction	O
and	O
link	B-Task
prediction	E-Task
,	O
we	O
train	O
for	O
50	O
epochs	O
using	O
mini	O
-	O
batch	O
size	O
of	O
8	O
samples	O
.	O

For	O
node	B-Task
classification	E-Task
,	O
we	O
train	O
for	O
100	O
epochs	O
using	O
mini	O
-	O
batch	O
size	O
of	O
64	O
samples	O
.	O

We	O
utilize	O
early	O
stopping	O
as	O
a	O
form	O
of	O
regularization	O
in	O
time	O
when	O
the	O
model	O
shows	O
signs	O
of	O
overfitting	O
on	O
the	O
validation	O
set	O
.	O

We	O
apply	O
mean	B-Method
-	I-Method
variance	I-Method
normalization	E-Method
(	O
MVN	S-Method
)	O
after	O
each	O
ReLU	B-Method
activation	I-Method
layer	E-Method
to	O
help	O
improve	O
link	B-Task
prediction	E-Task
performance	O
,	O
where	O
it	O
compensates	O
for	O
noise	O
between	O
train	O
and	O
test	O
instances	O
by	O
normalizing	O
the	O
activations	O
to	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O

MVN	S-Method
enables	O
efficient	O
learning	S-Method
and	O
has	O
been	O
shown	O
effective	O
in	O
cardiac	B-Task
semantic	I-Task
segmentation	E-Task
and	O
speech	B-Task
recognition	E-Task
.	O

During	O
training	S-Task
,	O
we	O
apply	O
dropout	B-Method
regularization	E-Method
throughout	O
the	O
architecture	O
to	O
mitigate	O
overfitting	O
,	O
depending	O
on	O
the	O
sparsity	O
of	O
the	O
input	O
graph	O
.	O

For	O
link	B-Task
prediction	E-Task
,	O
dropout	S-Method
is	O
also	O
applied	O
at	O
the	O
input	O
layer	O
to	O
produce	O
an	O
effect	O
similar	O
to	O
using	O
a	O
denoising	B-Method
autoencoder	E-Method
.	O

This	O
denoising	B-Method
technique	E-Method
was	O
previously	O
employed	O
for	O
link	B-Task
prediction	E-Task
in	O
.	O

We	O
initialize	O
weights	O
according	O
to	O
the	O
Xavier	B-Method
scheme	E-Method
described	O
in	O
.	O

We	O
do	O
not	O
apply	O
weight	B-Method
decay	I-Method
regularization	E-Method
.	O

We	O
employ	O
the	O
Adam	B-Method
algorithm	E-Method
for	O
gradient	B-Task
descent	I-Task
optimization	E-Task
with	O
a	O
fixed	O
learning	B-Metric
rate	E-Metric
of	O
.	O

As	O
part	O
of	O
our	O
experimental	O
design	O
,	O
we	O
also	O
performed	O
experiments	O
without	O
parameter	O
sharing	O
between	O
the	O
encoder	B-Method
and	I-Method
decoder	I-Method
parts	E-Method
of	O
the	O
architecture	O
and	O
found	O
severely	O
degraded	O
predictive	B-Metric
performance	E-Metric
.	O

This	O
observation	O
is	O
consistent	O
with	O
prior	O
findings	O
that	O
parameter	B-Method
sharing	E-Method
helps	O
improve	O
generalization	S-Task
by	O
providing	O
additional	O
regularization	O
to	O
mitigate	O
the	O
adverse	O
effects	O
of	O
overfitting	O
and	O
enhance	O
representation	B-Method
learning	E-Method
.	O

subsection	O
:	O
Results	O
and	O
Analysis	O
Reconstruction	S-Task
Results	O
of	O
the	O
reconstruction	B-Task
task	E-Task
for	O
the	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
and	O
BlogCatalog	S-Material
network	O
datasets	O
are	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
this	O
experiment	O
,	O
we	O
compare	O
the	O
results	O
obtained	O
by	O
our	O
LoNGAE	S-Method
model	O
to	O
those	O
obtained	O
by	O
the	O
related	O
autoencoder	O
-	O
based	O
SDNE	S-Method
model	O
.	O

The	O
evaluation	B-Metric
metric	E-Metric
is	O
precision@	S-Metric
,	O
which	O
is	O
a	O
rank	B-Metric
-	I-Metric
based	I-Metric
measure	E-Metric
used	O
in	O
information	B-Task
retrieval	E-Task
and	O
is	O
defined	O
as	O
the	O
proportion	O
of	O
retrieved	O
edges	O
/	O
links	O
in	O
the	O
top	O
-	O
set	O
that	O
are	O
relevant	O
.	O

We	O
use	O
precision@	S-Metric
to	O
evaluate	O
the	O
model	O
’s	O
ability	O
to	O
retrieve	O
edges	O
known	O
to	O
be	O
present	O
(	O
positive	O
edges	O
)	O
as	O
part	O
of	O
the	O
reconstruction	S-Task
.	O

width=0.8	O
In	O
comparison	O
to	O
SDNE	S-Method
,	O
we	O
show	O
that	O
our	O
LoNGAE	S-Method
model	O
achieves	O
better	O
precision@	S-Metric
performance	O
for	O
all	O
values	O
,	O
up	O
to	O
for	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
and	O
for	O
BlogCatalog	S-Material
,	O
when	O
trained	O
on	O
the	O
complete	O
datasets	O
.	O

We	O
also	O
systematically	O
test	O
the	O
capacity	O
of	O
the	O
LoNGAE	S-Method
model	O
to	O
reconstruct	O
the	O
original	O
networks	O
when	O
up	O
to	O
80	O
percent	O
of	O
the	O
edges	O
are	O
randomly	O
removed	O
,	O
akin	O
to	O
the	O
link	B-Task
prediction	E-Task
task	O
.	O

We	O
show	O
that	O
the	O
LoNGAE	S-Method
model	O
only	O
gets	O
worse	O
precision@	S-Metric
performance	O
than	O
SDNE	S-Method
on	O
the	O
Arxiv	B-Material
-	I-Material
GRQC	E-Material
dataset	O
when	O
more	O
than	O
40	O
percent	O
of	O
the	O
edges	O
are	O
missing	O
at	O
random	O
.	O

On	O
the	O
BlogCatalog	S-Material
dataset	O
,	O
the	O
LoNGAE	S-Method
model	O
achieves	O
better	O
precision@	S-Metric
performance	O
than	O
SDNE	S-Method
for	O
large	O
values	O
even	O
when	O
80	O
percent	O
of	O
the	O
edges	O
are	O
missing	O
at	O
random	O
.	O

This	O
experiment	O
demonstrates	O
the	O
superior	O
representation	B-Metric
capacity	E-Metric
of	O
our	O
LoNGAE	S-Method
model	O
compared	O
to	O
SDNE	S-Method
.	O

Link	B-Task
Prediction	E-Task
Table	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
between	O
our	O
autoencoder	B-Method
models	E-Method
and	O
the	O
matrix	B-Method
factorization	E-Method
(	O
MF	S-Method
)	O
model	O
proposed	O
in	O
for	O
link	B-Task
prediction	E-Task
with	O
and	O
without	O
node	O
features	O
.	O

Recall	O
that	O
our	O
goal	O
is	O
to	O
recover	O
the	O
statuses	O
of	O
the	O
missing	O
or	O
unknown	O
links	O
in	O
the	O
input	O
graph	O
.	O

As	O
part	O
of	O
the	O
experimental	O
design	O
,	O
we	O
pretend	O
that	O
a	O
randomly	O
selected	O
set	O
of	O
elements	O
in	O
the	O
adjacency	O
matrix	O
are	O
missing	O
and	O
collect	O
their	O
indices	O
to	O
be	O
used	O
as	O
a	O
validation	O
set	O
.	O

Our	O
task	O
is	O
to	O
train	O
the	O
autoencoder	S-Method
to	O
produce	O
a	O
set	O
of	O
predictions	O
,	O
a	O
list	O
of	O
ones	O
and	O
zeros	O
,	O
on	O
those	O
missing	O
indices	O
and	O
see	O
how	O
well	O
the	O
model	O
performs	O
when	O
compared	O
to	O
the	O
ground	O
-	O
truth	O
.	O

The	O
evaluation	B-Metric
metric	E-Metric
is	O
the	O
area	O
under	O
the	O
ROC	B-Metric
curve	E-Metric
(	O
AUC	S-Metric
)	O
.	O

Results	O
are	O
reported	O
as	O
mean	O
AUC	S-Metric
and	O
standard	O
deviation	O
over	O
10	B-Metric
-	I-Metric
fold	I-Metric
cross	I-Metric
-	I-Metric
validation	E-Metric
.	O

The	O
datasets	O
under	O
consideration	O
for	O
link	B-Task
prediction	E-Task
exhibit	O
varying	O
degrees	O
of	O
class	O
imbalance	O
.	O

For	O
featureless	O
link	B-Task
prediction	E-Task
,	O
our	O
LoNGAE	S-Method
model	O
marginally	O
outperforms	O
MF	S-Method
on	O
{	O
Protein	S-Material
,	O
Metabolic	S-Material
,	O
Conflict	S-Material
}	O
and	O
is	O
significantly	O
better	O
than	O
MF	S-Method
on	O
PowerGrid	S-Method
.	O

Consistent	O
with	O
MF	S-Method
results	O
,	O
we	O
observe	O
that	O
incorporating	O
external	O
node	O
features	O
provides	O
a	O
boost	O
in	O
link	B-Task
prediction	E-Task
accuracy	O
,	O
especially	O
for	O
the	O
Protein	S-Material
dataset	O
where	O
we	O
achieve	O
a	O
6	O
percent	O
increase	O
in	O
performance	O
.	O

Metabolic	S-Material
and	O
Conflict	S-Material
also	O
come	O
with	O
external	O
edge	O
features	O
,	O
which	O
were	O
exploited	O
by	O
the	O
MF	S-Method
model	O
for	O
further	O
performance	O
gains	O
.	O

We	O
leave	O
the	O
task	O
of	O
combining	O
edge	O
features	O
for	O
future	O
work	O
.	O

Each	O
node	O
in	O
Conflict	S-Material
only	O
has	O
three	O
features	O
,	O
which	O
are	O
unable	O
to	O
significantly	O
boost	O
link	B-Task
prediction	E-Task
accuracy	O
.	O

PowerGrid	S-Method
does	O
not	O
have	O
node	O
features	O
so	O
there	O
are	O
no	O
results	O
for	O
the	O
respective	O
rows	O
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
the	O
performances	O
between	O
our	O
autoencoder	B-Method
models	E-Method
and	O
related	O
graph	B-Method
embedding	I-Method
methods	E-Method
for	O
link	B-Task
prediction	E-Task
with	O
and	O
without	O
node	O
features	O
.	O

Following	O
the	O
protocol	O
described	O
in	O
,	O
we	O
report	O
AUC	S-Metric
and	O
average	B-Metric
precision	E-Metric
(	O
AP	S-Metric
)	O
scores	O
for	O
each	O
model	O
on	O
the	O
held	O
-	O
out	O
test	O
set	O
containing	O
10	O
percent	O
of	O
randomly	O
sampled	O
positive	O
links	O
and	O
the	O
same	O
number	O
of	O
negative	O
links	O
.	O

We	O
show	O
mean	O
AUC	S-Metric
and	O
AP	S-Metric
with	O
standard	O
error	O
over	O
10	O
runs	O
with	O
random	B-Method
weight	I-Method
initializations	E-Method
on	O
fixed	O
data	O
splits	O
.	O

Results	O
for	O
the	O
baseline	O
methods	O
are	O
taken	O
from	O
Kipf	S-Method
and	O
Welling	O
,	O
where	O
we	O
pick	O
the	O
best	O
performing	O
models	O
for	O
comparison	O
.	O

Similar	O
to	O
the	O
MF	S-Method
model	O
,	O
the	O
graph	B-Method
embedding	I-Method
methods	E-Method
that	O
can	O
combine	O
side	O
node	O
features	O
always	O
produce	O
a	O
boost	O
in	O
link	B-Task
prediction	E-Task
accuracy	O
.	O

In	O
this	O
comparison	O
,	O
we	O
significantly	O
outperform	O
the	O
best	O
graph	B-Method
embedding	I-Method
methods	E-Method
by	O
as	O
much	O
as	O
10	O
percent	O
,	O
with	O
and	O
without	O
node	O
features	O
.	O

Our	O
LoNGAE	S-Method
model	O
achieves	O
competitive	O
link	B-Task
prediction	E-Task
performance	O
when	O
compared	O
against	O
the	O
best	O
model	O
presented	O
in	O
on	O
the	O
Pubmed	S-Material
dataset	O
.	O

width=0.9	O
Node	B-Task
Classification	E-Task
Results	O
of	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
for	O
the	O
{	O
Cora	S-Material
,	O
Citeseer	S-Material
,	O
Pubmed	S-Material
}	O
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
this	O
context	O
of	O
citation	B-Task
networks	E-Task
,	O
node	B-Task
classification	E-Task
is	O
equivalent	O
to	O
the	O
task	O
of	O
document	B-Task
classification	E-Task
.	O

We	O
closely	O
follow	O
the	O
experimental	O
setup	O
of	O
Kipf	S-Method
and	O
Welling	O
,	O
where	O
we	O
use	O
their	O
provided	O
train	O
/	O
validation	O
/	O
test	O
splits	O
for	O
evaluation	O
.	O

Accuracy	S-Metric
performance	O
is	O
measured	O
on	O
the	O
held	O
-	O
out	O
test	O
set	O
of	O
1	O
,	O
000	O
examples	O
.	O

We	O
tune	O
hyper	O
-	O
parameters	O
on	O
the	O
validation	O
set	O
of	O
500	O
examples	O
.	O

The	O
train	O
set	O
only	O
contains	O
20	O
examples	O
per	O
class	O
.	O

All	O
methods	O
use	O
the	O
complete	O
adjacency	O
matrix	O
,	O
and	O
available	O
node	O
features	O
,	O
to	O
learn	O
latent	O
embeddings	O
for	O
node	B-Task
classification	E-Task
.	O

For	O
comparison	O
,	O
we	O
train	O
and	O
test	O
our	O
LoNGAE	S-Method
model	O
on	O
the	O
same	O
data	O
splits	O
over	O
10	O
runs	O
with	O
random	B-Method
weight	I-Method
initializations	E-Method
and	O
report	O
mean	B-Metric
accuracy	E-Metric
.	O

Kipf	O
and	O
Welling	O
report	O
their	O
mean	O
GCN	B-Method
and	I-Method
ICA	E-Method
results	O
on	O
the	O
same	O
data	O
splits	O
over	O
100	O
runs	O
with	O
random	B-Method
weight	I-Method
initializations	E-Method

.	O

The	O
other	O
baseline	O
methods	O
are	O
taken	O
from	O
Yang	O
et	O
al	O
.	O

.	O


In	O
this	O
comparison	O
,	O
our	O
LoNGAE	S-Method
model	O
achieves	O
competitive	O
performance	O
when	O
compared	O
against	O
the	O
GCN	B-Method
model	E-Method
on	O
the	O
Cora	S-Material
dataset	O
,	O
but	O
outperforms	O
GCN	S-Method
and	O
all	O
other	O
baseline	O
methods	O
on	O
the	O
Citeseer	S-Material
and	O
Pubmed	B-Material
datasets	E-Material
.	O

width=0.5	O
Multi	B-Task
-	I-Task
task	I-Task
Learning	E-Task
Lastly	O
,	O
we	O
report	O
LPNC	S-Task
results	O
obtained	O
by	O
our	O
LoNGAE	S-Method
model	O
in	O
the	O
MTL	S-Task
setting	O
over	O
10	O
runs	O
with	O
random	B-Method
weight	I-Method
initializations	E-Method
.	O

In	O
the	O
MTL	S-Task
scenario	O
,	O
the	O
LoNGAE	S-Method
model	O
takes	O
as	O
input	O
an	O
incomplete	O
graph	O
with	O
10	O
percent	O
of	O
the	O
positive	O
edges	O
,	O
and	O
the	O
same	O
number	O
of	O
negative	O
edges	O
,	O
missing	O
at	O
random	O
and	O
all	O
available	O
node	O
features	O
to	O
simultaneously	O
produce	O
predictions	O
for	O
the	O
missing	O
edges	O
and	O
labels	O
for	O
the	O
nodes	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
efficacy	O
of	O
the	O
LoNGAE	S-Method
model	O
for	O
MTL	S-Task
when	O
compared	O
against	O
the	O
best	O
performing	O
task	O
-	O
specific	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
models	O
,	O
which	O
require	O
the	O
complete	O
adjacency	O
matrix	O
as	O
input	O
.	O

For	O
link	B-Task
prediction	E-Task
,	O
multi	O
-	O
task	O
LoNGAE	S-Method
achieves	O
competitive	O
performance	O
against	O
task	O
-	O
specific	O
LoNGAE	S-Method
,	O
and	O
significantly	O
outperforms	O
the	O
best	O
VGAE	S-Method
model	O
from	O
Kipf	S-Method
and	O
Welling	S-Method
on	O
Cora	S-Material
and	O
Citeseer	B-Material
datasets	E-Material
.	O

For	O
node	B-Task
classification	E-Task
,	O
multi	O
-	O
task	O
LoNGAE	S-Method
is	O
the	O
best	O
performing	O
model	O
across	O
the	O
board	O
,	O
only	O
trailing	O
behind	O
the	O
GCN	B-Method
model	E-Method
on	O
the	O
Cora	S-Material
dataset	O
.	O

width=0.5	O
section	O
:	O
Discussion	O
In	O
our	O
experiments	O
,	O
we	O
show	O
that	O
a	O
simple	O
autoencoder	B-Method
architecture	E-Method
with	O
parameter	B-Method
sharing	E-Method
consistently	O
outperforms	O
previous	O
related	O
methods	O
on	O
a	O
range	O
of	O
challenging	O
graph	O
-	O
structured	O
benchmarks	O
for	O
three	O
separate	O
tasks	O
:	O
reconstruction	S-Task
,	O
link	B-Task
prediction	E-Task
,	O
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

For	O
the	O
reconstruction	B-Task
task	E-Task
,	O
our	O
LoNGAE	S-Method
model	O
achieves	O
superior	O
precision@	S-Metric
performance	O
when	O
compared	O
to	O
the	O
related	O
SDNE	S-Method
model	O
.	O

Although	O
both	O
models	O
leverage	O
a	O
deep	B-Method
autoencoder	I-Method
architecture	E-Method
for	O
graph	B-Method
representation	I-Method
learning	E-Method
,	O
the	O
SDNE	S-Method
model	O
lacks	O
several	O
key	O
implementations	O
necessary	O
for	O
enhanced	B-Task
representation	I-Task
capacity	E-Task
,	O
namely	O
parameter	B-Method
sharing	E-Method
between	O
the	O
encoder	B-Method
-	I-Method
decoder	I-Method
parts	E-Method
and	O
end	O
-	O
to	O
-	O
end	O
training	O
of	O
deep	B-Method
architectures	E-Method
.	O

For	O
link	B-Task
prediction	E-Task
,	O
we	O
observe	O
that	O
combining	O
available	O
node	O
features	O
always	O
produces	O
a	O
significant	O
boost	O
in	O
predictive	B-Metric
performance	E-Metric
.	O

This	O
observation	O
was	O
previously	O
reported	O
in	O
,	O
among	O
others	O
.	O

Intuitively	O
,	O
we	O
expect	O
topological	O
graph	O
features	O
provide	O
complementary	O
information	O
not	O
present	O
in	O
the	O
node	O
features	O
,	O
and	O
the	O
combination	O
of	O
both	O
feature	O
sets	O
should	O
improve	O
predictive	B-Metric
power	E-Metric
.	O

Although	O
explicit	O
node	O
features	O
may	O
not	O
always	O
be	O
readily	O
available	O
,	O
a	O
link	B-Task
prediction	E-Task
model	O
capable	O
of	O
incorporating	O
optional	O
side	O
information	O
has	O
broader	O
applicability	O
.	O

Our	O
LoNGAE	S-Method
model	O
also	O
performs	O
favorably	O
well	O
on	O
the	O
task	O
of	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
.	O

The	O
model	O
is	O
capable	O
of	O
encoding	O
non	O
-	O
linear	O
node	O
embeddings	O
from	O
both	O
local	O
graph	O
structure	O
and	O
explicit	O
node	O
features	O
,	O
which	O
can	O
be	O
decoded	O
by	O
a	O
softmax	B-Method
activation	I-Method
function	E-Method
to	O
yield	O
accurate	O
node	O
labels	O
.	O

The	O
efficacy	O
of	O
the	O
proposed	O
LoNGAE	S-Method
model	O
is	O
evident	O
especially	O
on	O
the	O
Pubmed	S-Material
dataset	O
,	O
where	O
the	O
label	B-Metric
rate	E-Metric
is	O
only	O
0.003	O
.	O

This	O
efficacy	O
is	O
attributed	O
to	O
parameter	B-Method
sharing	E-Method
being	O
used	O
in	O
the	O
autoencoder	B-Method
architecture	E-Method
,	O
which	O
provides	O
regularization	O
to	O
help	O
improve	O
representation	B-Task
learning	E-Task
and	O
generalization	S-Task
.	O

Our	O
autoencoder	B-Method
architecture	E-Method
naturally	O
supports	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
,	O
where	O
a	O
joint	B-Method
representation	E-Method
for	O
both	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
is	O
enabled	O
via	O
parameter	O
sharing	O
.	O

MTL	S-Task
aims	O
to	O
exploit	O
commonalities	O
and	O
differences	O
across	O
multiple	O
tasks	O
to	O
find	O
a	O
shared	O
representation	O
that	O
can	O
result	O
in	O
improved	O
performance	O
for	O
each	O
task	O
-	O
specific	O
metric	O
.	O

In	O
this	O
work	O
,	O
we	O
show	O
that	O
our	O
multi	O
-	O
task	O
LoNGAE	S-Method
model	O
improves	O
node	B-Task
classification	E-Task
accuracy	O
by	O
learning	O
to	O
predict	O
missing	O
edges	O
at	O
the	O
same	O
time	O
.	O

Our	O
multi	B-Method
-	I-Method
task	I-Method
model	E-Method
has	O
broad	O
practical	O
utility	O
to	O
address	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
where	O
the	O
input	O
graphs	O
may	O
have	O
both	O
missing	O
edges	O
and	O
node	O
labels	O
.	O

Finally	O
,	O
we	O
address	O
one	O
major	O
limitation	O
associated	O
with	O
our	O
autoencoder	B-Method
models	E-Method
having	O
complexity	S-Metric
scale	O
linearly	O
in	O
the	O
number	O
of	O
nodes	O
.	O

Hamilton	O
et	O
al	O
.	O

express	O
that	O
the	O
complexity	O
in	O
nodes	O
may	O
limit	O
the	O
utility	O
of	O
the	O
models	O
on	O
massive	O
graphs	O
with	O
hundreds	O
of	O
millions	O
of	O
nodes	O
.	O

In	O
practice	O
,	O
we	O
would	O
implement	O
our	O
models	O
to	O
leverage	O
data	O
parallelism	O
across	O
commodity	O
CPU	O
and	O
/	O
or	O
GPU	O
resources	O
for	O
effective	O
distributed	B-Task
learning	E-Task
on	O
massive	B-Task
graphs	E-Task
.	O

Data	B-Task
parallelism	E-Task
is	O
possible	O
because	O
our	O
models	O
learn	O
node	O
embeddings	O
from	O
each	O
row	O
vector	O
of	O
the	O
adjacency	O
matrix	O
independently	O
.	O

Nevertheless	O
,	O
the	O
area	O
of	O
improvement	O
in	O
future	O
work	O
is	O
to	O
take	O
advantage	O
of	O
the	O
sparsity	O
of	O
edges	O
in	O
the	O
graphs	O
to	O
scale	O
our	O
models	O
linearly	O
in	O
the	O
number	O
of	O
observed	O
edges	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
presented	O
a	O
new	O
autoencoder	B-Method
architecture	E-Method
for	O
link	B-Task
prediction	E-Task
and	O
semi	O
-	O
supervised	O
node	B-Task
classification	E-Task
,	O
and	O
showed	O
that	O
the	O
resulting	O
models	O
outperform	O
related	O
methods	O
in	O
accuracy	S-Metric
performance	O
on	O
a	O
range	O
of	O
real	O
-	O
world	O
graph	O
-	O
structured	O
datasets	O
.	O

The	O
success	O
of	O
our	O
models	O
is	O
primarily	O
attributed	O
to	O
extensive	O
parameter	O
sharing	O
between	O
the	O
encoder	B-Method
and	I-Method
decoder	I-Method
parts	E-Method
of	O
the	O
architecture	O
,	O
coupled	O
with	O
the	O
capability	O
to	O
learn	O
expressive	O
non	B-Method
-	I-Method
linear	I-Method
latent	I-Method
node	I-Method
representations	E-Method
from	O
both	O
local	O
graph	O
neighborhoods	O
and	O
explicit	O
node	O
features	O
.	O

Further	O
,	O
our	O
novel	O
architecture	O
is	O
capable	O
of	O
simultaneous	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
of	O
both	O
link	B-Task
prediction	E-Task
and	O
node	B-Task
classification	E-Task
in	O
one	O
efficient	O
end	O
-	O
to	O
-	O
end	B-Task
training	I-Task
stage	E-Task
.	O

Our	O
work	O
provides	O
a	O
useful	O
framework	O
to	O
make	O
accurate	O
and	O
meaningful	O
predictions	O
on	O
a	O
diverse	O
set	O
of	O
complex	O
graph	O
structures	O
for	O
a	O
wide	O
range	O
of	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
.	O

section	O
:	O
Acknowledgment	O
The	O
author	O
thanks	O
Edward	O
Raff	O
and	O
Jared	O
Sylvester	O
for	O
insightful	O
discussions	O
,	O
and	O
gracious	O
reviewers	O
for	O
constructive	O
feedback	O
on	O
the	O
paper	O
.	O

bibliography	O
:	O
References	O
Multi	B-Task
-	I-Task
Task	I-Task
Learning	E-Task
as	O
Multi	B-Task
-	I-Task
Objective	I-Task
Optimization	E-Task
section	O
:	O
Abstract	O
In	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
,	O
multiple	O
tasks	O
are	O
solved	O
jointly	O
,	O
sharing	O
inductive	O
bias	O
between	O
them	O
.	O

Multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
is	O
inherently	O
a	O
multi	B-Task
-	I-Task
objective	I-Task
problem	E-Task
because	O
different	O
tasks	O
may	O
conflict	O
,	O
necessitating	O
a	O
trade	O
-	O
off	O
.	O

A	O
common	O
compromise	O
is	O
to	O
optimize	O
a	O
proxy	O
objective	O
that	O
minimizes	O
a	O
weighted	O
linear	O
combination	O
of	O
pertask	O
losses	O
.	O

However	O
,	O
this	O
workaround	O
is	O
only	O
valid	O
when	O
the	O
tasks	O
do	O
not	O
compete	O
,	O
which	O
is	O
rarely	O
the	O
case	O
.	O

In	O
this	O
paper	O
,	O
we	O
explicitly	O
cast	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
as	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
,	O
with	O
the	O
overall	O
objective	O
of	O
finding	O
a	O
Pareto	O
optimal	O
solution	O
.	O

To	O
this	O
end	O
,	O
we	O
use	O
algorithms	O
developed	O
in	O
the	O
gradient	B-Method
-	I-Method
based	I-Method
multiobjective	I-Method
optimization	I-Method
literature	E-Method
.	O

These	O
algorithms	O
are	O
not	O
directly	O
applicable	O
to	O
large	B-Task
-	I-Task
scale	I-Task
learning	I-Task
problems	E-Task
since	O
they	O
scale	O
poorly	O
with	O
the	O
dimensionality	O
of	O
the	O
gradients	O
and	O
the	O
number	O
of	O
tasks	O
.	O

We	O
therefore	O
propose	O
an	O
upper	B-Method
bound	E-Method
for	O
the	O
multi	B-Metric
-	I-Metric
objective	I-Metric
loss	E-Metric
and	O
show	O
that	O
it	O
can	O
be	O
optimized	O
efficiently	O
.	O

We	O
further	O
prove	O
that	O
optimizing	O
this	O
upper	B-Method
bound	E-Method
yields	O
a	O
Pareto	B-Method
optimal	I-Method
solution	E-Method
under	O
realistic	O
assumptions	O
.	O

We	O
apply	O
our	O
method	O
to	O
a	O
variety	O
of	O
multi	B-Task
-	I-Task
task	I-Task
deep	I-Task
learning	I-Task
problems	E-Task
including	O
digit	B-Task
classification	E-Task
,	O
scene	B-Task
understanding	E-Task
(	O
joint	B-Task
semantic	I-Task
segmentation	E-Task
,	O
instance	B-Task
segmentation	E-Task
,	O
and	O
depth	B-Task
estimation	E-Task
)	O
,	O
and	O
multilabel	B-Task
classification	E-Task
.	O

Our	O
method	O
produces	O
higher	O
-	O
performing	O
models	O
than	O
recent	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
formulations	O
or	O
per	B-Task
-	I-Task
task	I-Task
training	E-Task
.	O

section	O
:	O
The	O
problem	O
of	O
finding	B-Task
Pareto	I-Task
optimal	I-Task
solutions	E-Task
given	O
multiple	O
criteria	O
is	O
called	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
.	O

A	O
variety	O
of	O
algorithms	O
for	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
exist	O
.	O

One	O
such	O
approach	O
is	O
the	O
multiple	B-Method
-	I-Method
gradient	I-Method
descent	I-Method
algorithm	E-Method
(	O
MGDA	B-Method
)	E-Method
,	O
which	O
uses	O
gradient	B-Method
-	I-Method
based	I-Method
optimization	E-Method
and	O
provably	O
converges	O
to	O
a	O
point	O
on	O
the	O
Pareto	O
set	O
[	O
reference	O
]	O
.	O

MGDA	S-Method
is	O
well	O
-	O
suited	O
for	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
with	O
deep	B-Method
networks	E-Method
.	O

It	O
can	O
use	O
the	O
gradients	O
of	O
each	O
task	O
and	O
solve	O
an	O
optimization	B-Task
problem	E-Task
to	O
decide	O
on	O
an	O
update	O
over	O
the	O
shared	O
parameters	O
.	O

However	O
,	O
there	O
are	O
two	O
technical	O
problems	O
that	O
hinder	O
the	O
applicability	O
of	O
MGDA	S-Method
on	O
a	O
large	O
scale	O
.	O

(	O
i	O
)	O
The	O
underlying	O
optimization	B-Task
problem	E-Task
does	O
not	O
scale	O
gracefully	O
to	O
high	O
-	O
dimensional	O
gradients	O
,	O
which	O
arise	O
naturally	O
in	O
deep	B-Task
networks	E-Task
.	O

(	O
ii	O
)	O
The	O
algorithm	O
requires	O
explicit	O
computation	O
of	O
gradients	O
per	O
task	O
,	O
which	O
results	O
in	O
linear	O
scaling	O
of	O
the	O
number	O
of	O
backward	O
passes	O
and	O
roughly	O
multiplies	O
the	O
training	B-Metric
time	E-Metric
by	O
the	O
number	O
of	O
tasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
Frank	B-Method
-	I-Method
Wolfe	I-Method
-	I-Method
based	I-Method
optimizer	E-Method
that	O
scales	O
to	O
high	B-Task
-	I-Task
dimensional	I-Task
problems	E-Task
.	O

Furthermore	O
,	O
we	O
provide	O
an	O
upper	B-Metric
bound	E-Metric
for	O
the	O
MGDA	S-Method
optimization	O
objective	O
and	O
show	O
that	O
it	O
can	O
be	O
computed	O
via	O
a	O
single	O
backward	B-Method
pass	E-Method
without	O
explicit	O
task	O
-	O
specific	O
gradients	O
,	O
thus	O
making	O
the	O
computational	B-Metric
overhead	E-Metric
of	O
the	O
method	O
negligible	O
.	O

We	O
prove	O
that	O
using	O
our	O
upper	B-Method
bound	E-Method
yields	O
a	O
Pareto	B-Method
optimal	I-Method
solution	E-Method
under	O
realistic	O
assumptions	O
.	O

The	O
result	O
is	O
an	O
exact	O
algorithm	O
for	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
of	O
deep	O
networks	O
with	O
negligible	O
computational	B-Metric
overhead	E-Metric
.	O

We	O
empirically	O
evaluate	O
the	O
presented	O
method	O
on	O
three	O
different	O
problems	O
.	O

First	O
,	O
we	O
perform	O
an	O
extensive	O
evaluation	O
on	O
multi	B-Task
-	I-Task
digit	I-Task
classification	E-Task
with	O
MultiMNIST	S-Material
[	O
reference	O
]	O
.	O

Second	O
,	O
we	O
cast	O
multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
as	O
MTL	S-Task
and	O
conduct	O
experiments	O
with	O
the	O
CelebA	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

Lastly	O
,	O
we	O
apply	O
the	O
presented	O
method	O
to	O
scene	B-Task
understanding	E-Task
;	O
specifically	O
,	O
we	O
perform	O
joint	B-Task
semantic	I-Task
segmentation	E-Task
,	O
instance	B-Task
segmentation	E-Task
,	O
and	O
depth	B-Task
estimation	E-Task
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

The	O
number	O
of	O
tasks	O
in	O
our	O
evaluation	O
varies	O
from	O
2	O
to	O
40	O
.	O

Our	O
method	O
clearly	O
outperforms	O
all	O
baselines	O
.	O

section	O
:	O
Related	O
Work	O
Multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

We	O
summarize	O
the	O
work	O
most	O
closely	O
related	O
to	O
ours	O
and	O
refer	O
the	O
interested	O
reader	O
to	O
reviews	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
additional	O
background	O
.	O

Multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
(	O
MTL	S-Task
)	O
is	O
typically	O
conducted	O
via	O
hard	B-Method
or	I-Method
soft	I-Method
parameter	I-Method
sharing	E-Method
.	O

In	O
hard	B-Task
parameter	I-Task
sharing	E-Task
,	O
a	O
subset	O
of	O
the	O
parameters	O
is	O
shared	O
between	O
tasks	O
while	O
other	O
parameters	O
are	O
task	O
-	O
specific	O
.	O

In	O
soft	B-Task
parameter	I-Task
sharing	E-Task
,	O
all	O
parameters	O
are	O
task	O
-	O
specific	O
but	O
they	O
are	O
jointly	O
constrained	O
via	O
Bayesian	O
priors	O
[	O
reference	O
][	O
reference	O
]	O
or	O
a	O
joint	O
dictionary	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
focus	O
on	O
hard	B-Method
parameter	I-Method
sharing	E-Method
with	O
gradient	B-Method
-	I-Method
based	I-Method
optimization	E-Method
,	O
following	O
the	O
success	O
of	O
deep	O
MTL	S-Task
in	O
computer	B-Task
vision	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
natural	B-Task
language	I-Task
processing	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
speech	B-Task
processing	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
even	O
seemingly	O
unrelated	O
domains	O
over	O
multiple	O
modalities	O
[	O
reference	O
]	O
.	O

[	O
reference	O
]	O
theoretically	O
analyze	O
the	O
MTL	B-Task
problem	E-Task
as	O
interaction	O
between	O
individual	O
learners	O
and	O
a	O
meta	B-Method
-	I-Method
algorithm	E-Method
.	O

Each	O
learner	O
is	O
responsible	O
for	O
one	O
task	O
and	O
a	O
meta	B-Method
-	I-Method
algorithm	E-Method
decides	O
how	O
the	O
shared	O
parameters	O
are	O
updated	O
.	O

All	O
aforementioned	O
MTL	S-Task
algorithms	O
use	O
weighted	B-Method
summation	E-Method
as	O
the	O
meta	B-Method
-	I-Method
algorithm	E-Method
.	O

Meta	B-Method
-	I-Method
algorithms	E-Method
that	O
go	O
beyond	O
weighted	B-Method
summation	E-Method
have	O
also	O
been	O
explored	O
.	O

[	O
reference	O
]	O
consider	O
the	O
case	O
where	O
each	O
individual	O
learner	S-Method
is	O
based	O
on	O
kernel	B-Method
learning	E-Method
and	O
utilize	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
.	O

[	O
reference	O
]	O
consider	O
the	O
case	O
where	O
each	O
learner	S-Method
is	O
a	O
linear	B-Method
model	E-Method
and	O
use	O
a	O
task	B-Method
affinity	I-Method
matrix	E-Method
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
use	O
the	O
assumption	O
that	O
tasks	O
share	O
a	O
dictionary	O
and	O
develop	O
an	O
expectation	B-Method
-	I-Method
maximization	I-Method
-	I-Method
like	I-Method
metaalgorithm	E-Method
.	O

de	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
use	O
swarm	B-Method
optimization	E-Method
.	O

None	O
of	O
these	O
methods	O
apply	O
to	O
gradient	B-Method
-	I-Method
based	I-Method
learning	I-Method
of	I-Method
high	I-Method
-	I-Method
capacity	I-Method
models	E-Method
such	O
as	O
modern	O
deep	B-Method
networks	E-Method
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
propose	O
heuristics	S-Method
based	O
on	O
uncertainty	O
and	O
gradient	O
magnitudes	O
,	O
respectively	O
,	O
and	O
apply	O
their	O
methods	O
to	O
convolutional	B-Method
neural	I-Method
networks	E-Method
.	O

Another	O
recent	O
work	O
uses	O
multi	B-Method
-	I-Method
agent	I-Method
reinforcement	I-Method
learning	E-Method
[	O
reference	O
]	O
.	O

Multi	B-Task
-	I-Task
objective	I-Task
optimization	E-Task
.	O

Multi	B-Task
-	I-Task
objective	I-Task
optimization	E-Task
addresses	O
the	O
problem	O
of	O
optimizing	O
a	O
set	O
of	O
possibly	O
contrasting	O
objectives	O
.	O

We	O
recommend	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
surveys	O
of	O
this	O
field	O
.	O

Of	O
particular	O
relevance	O
to	O
our	O
work	O
is	O
gradient	B-Method
-	I-Method
based	I-Method
multi	I-Method
-	I-Method
objective	I-Method
optimization	E-Method
,	O
as	O
developed	O
by	O
[	O
reference	O
]	O
,	O
[	O
reference	O
][	O
reference	O
]	O
.	O

These	O
methods	O
use	O
multi	O
-	O
objective	O
Karush	B-Method
-	I-Method
Kuhn	I-Method
-	I-Method
Tucker	E-Method
(	O
KKT	S-Method
)	O
conditions	O
[	O
reference	O
]	O
and	O
find	O
a	O
descent	O
direction	O
that	O
decreases	O
all	O
objectives	O
.	O

This	O
approach	O
was	O
extended	O
to	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

In	O
machine	B-Task
learning	E-Task
,	O
these	O
methods	O
have	O
been	O
applied	O
to	O
multi	B-Task
-	I-Task
agent	I-Task
learning	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
kernel	B-Method
learning	E-Method
[	O
reference	O
]	O
,	O
sequential	B-Task
decision	I-Task
making	E-Task
[	O
reference	O
]	O
,	O
and	O
Bayesian	B-Task
optimization	E-Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Our	O
work	O
applies	O
gradient	B-Method
-	I-Method
based	I-Method
multi	I-Method
-	I-Method
objective	I-Method
optimization	E-Method
to	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

3	O
Multi	B-Task
-	I-Task
Task	I-Task
Learning	E-Task
as	O
Multi	B-Task
-	I-Task
Objective	I-Task
Optimization	E-Task
Consider	O
a	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
(	O
MTL	S-Task
)	O
problem	O
over	O
an	O
input	O
space	O
X	O
and	O
a	O
collection	O
of	O
task	O
spaces	O
is	O
given	O
where	O
T	O
is	O
the	O
number	O
of	O
tasks	O
,	O
N	O
is	O
the	O
number	O
of	O
data	O
points	O
,	O
and	O
y	O
t	O
i	O
is	O
the	O
label	O
of	O
the	O
t	O
th	O
task	O
for	O
the	O
i	O
th	O
data	O
point	O
.	O

[	O
reference	O
]	O
We	O
further	O
consider	O
a	O
parametric	O
hypothesis	O
class	O
per	O
task	O
as	O
f	O
t	O
(	O
x	O
;	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
:	O
X	O
→	O
Y	O
t	O
,	O
such	O
that	O
some	O
parameters	O
(	O
θ	O
sh	O
)	O
are	O
shared	O
between	O
tasks	O
and	O
some	O
(	O
θ	O
t	O
)	O
are	O
task	O
-	O
specific	O
.	O

We	O
also	O
consider	O
task	O
-	O
specific	O
loss	O
functions	O
Although	O
many	O
hypothesis	O
classes	O
and	O
loss	B-Method
functions	E-Method
have	O
been	O
proposed	O
in	O
the	O
MTL	S-Task
literature	O
,	O
they	O
generally	O
yield	O
the	O
following	O
empirical	B-Method
risk	I-Method
minimization	I-Method
formulation	E-Method
:	O
for	O
some	O
static	O
or	O
dynamically	O
computed	O
weights	O
c	O
t	O
per	O
task	O
,	O
whereL	O
t	O
(	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
is	O
the	O
empirical	O
loss	O
of	O
the	O
task	O
t	O
,	O
defined	O
asL	O
.	O

Although	O
the	O
weighted	B-Method
summation	I-Method
formulation	E-Method
(	O
1	O
)	O
is	O
intuitively	O
appealing	O
,	O
it	O
typically	O
either	O
requires	O
an	O
expensive	O
grid	B-Method
search	E-Method
over	O
various	O
scalings	O
or	O
the	O
use	O
of	O
a	O
heuristic	O
[	O
reference	O
][	O
reference	O
]	O
.	O

A	O
basic	O
justification	O
for	O
scaling	O
is	O
that	O
it	O
is	O
not	O
possible	O
to	O
define	O
global	O
optimality	O
in	O
the	O
MTL	S-Task
setting	O
.	O

Consider	O
two	O
sets	O
of	O
solutions	O
θ	O
andθ	O
such	O
thatL	O
,	O
for	O
some	O
tasks	O
t	O
1	O
and	O
t	O
2	O
.	O

In	O
other	O
words	O
,	O
solution	O
θ	O
is	O
better	O
for	O
task	O
t	O
1	O
whereasθ	O
is	O
better	O
for	O
t	O
2	O
.	O

It	O
is	O
not	O
possible	O
to	O
compare	O
these	O
two	O
solutions	O
without	O
a	O
pairwise	O
importance	O
of	O
tasks	O
,	O
which	O
is	O
typically	O
not	O
available	O
.	O

Alternatively	O
,	O
MTL	S-Task
can	O
be	O
formulated	O
as	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
:	O
optimizing	O
a	O
collection	O
of	O
possibly	O
conflicting	O
objectives	O
.	O

This	O
is	O
the	O
approach	O
we	O
take	O
.	O

We	O
specify	O
the	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
formulation	O
of	O
MTL	S-Task
using	O
a	O
vector	B-Method
-	I-Method
valued	I-Method
loss	I-Method
L	E-Method
:	O
The	O
goal	O
of	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
is	O
achieving	O
Pareto	O
optimality	O
.	O

Definition	O
1	O
(	O
Pareto	O
optimality	O
for	O
MTL	S-Task
)	O
(	O
b	O
)	O
A	O
solution	O
θ	O
is	O
called	O
Pareto	O
optimal	O
if	O
there	O
exists	O
no	O
solution	O
θ	O
that	O
dominates	O
θ	O
.	O

The	O
set	O
of	O
Pareto	O
optimal	O
solutions	O
is	O
called	O
the	O
Pareto	O
set	O
(	O
P	O
θ	O
)	O
and	O
its	O
image	O
is	O
called	O
the	O
Pareto	O
front	O
(	O
P	O
L	O
=	O
{	O
L	O
(	O
θ	O
)	O
}	O
θ∈P	O
θ	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
gradient	B-Method
-	I-Method
based	I-Method
multi	I-Method
-	I-Method
objective	I-Method
optimization	E-Method
due	O
to	O
its	O
direct	O
relevance	O
to	O
gradient	O
-	O
based	O
MTL	S-Task
.	O

In	O
the	O
rest	O
of	O
this	O
section	O
,	O
we	O
first	O
summarize	O
in	O
Section	O
3.1	O
how	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
can	O
be	O
performed	O
with	O
gradient	B-Method
descent	E-Method
.	O

Then	O
,	O
we	O
suggest	O
in	O
Section	O
3.2	O
a	O
practical	O
algorithm	O
for	O
performing	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
over	O
very	O
large	O
parameter	O
spaces	O
.	O

Finally	O
,	O
in	O
Section	O
3.3	O
we	O
propose	O
an	O
efficient	O
solution	O
for	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
designed	O
directly	O
for	O
high	B-Task
-	I-Task
capacity	I-Task
deep	I-Task
networks	E-Task
.	O

Our	O
method	O
scales	O
to	O
very	O
large	O
models	O
and	O
a	O
high	O
number	O
of	O
tasks	O
with	O
negligible	O
overhead	O
.	O

section	O
:	O
Multiple	B-Method
Gradient	I-Method
Descent	I-Method
Algorithm	E-Method
As	O
in	O
the	O
single	B-Task
-	I-Task
objective	I-Task
case	E-Task
,	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
can	O
be	O
solved	O
to	O
local	O
optimality	O
via	O
gradient	B-Method
descent	E-Method
.	O

In	O
this	O
section	O
,	O
we	O
summarize	O
one	O
such	O
approach	O
,	O
called	O
the	O
multiple	B-Method
gradient	I-Method
descent	I-Method
algorithm	E-Method
(	O
MGDA	S-Method
)	O
[	O
reference	O
]	O
.	O

MGDA	S-Method
leverages	O
the	O
Karush	B-Method
-	I-Method
Kuhn	I-Method
-	I-Method
Tucker	E-Method
(	O
KKT	S-Method
)	O
conditions	O
,	O
which	O
are	O
necessary	O
for	O
optimality	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
now	O
state	O
the	O
KKT	S-Method


conditions	O
for	O
both	O
task	O
-	O
specific	O
and	O
shared	O
parameters	O
:	O
•	O
There	O
exist	O
α	O
1	O
,	O
.	O

.	O


.	O


,	O
α	O
T	O
≥	O
0	O
such	O
that	O
Any	O
solution	O
that	O
satisfies	O
these	O
conditions	O
is	O
called	O
a	O
Pareto	O
stationary	O
point	O
.	O

Although	O
every	O
Pareto	O
optimal	O
point	O
is	O
Pareto	O
stationary	O
,	O
the	O
reverse	O
may	O
not	O
be	O
true	O
.	O

Consider	O
the	O
optimization	B-Task
problem	E-Task
Désidéri	O
(	O
2012	O
)	O
showed	O
that	O
either	O
the	O
solution	O
to	O
this	O
optimization	B-Task
problem	E-Task
is	O
0	O
and	O
the	O
resulting	O
point	O
satisfies	O
the	O
KKT	S-Method
conditions	O
,	O
or	O
the	O
solution	O
gives	O
a	O
descent	O
direction	O
that	O
improves	O
all	O
tasks	O
.	O

Hence	O
,	O
the	O
resulting	O
MTL	S-Task
algorithm	O
would	O
be	O
gradient	B-Method
descent	E-Method
on	O
the	O
task	O
-	O
specific	O
parameters	O
followed	O
by	O
solving	O
(	O
3	O
)	O
and	O
applying	O
the	O
solution	O
(	O
as	O
a	O
gradient	O
update	O
to	O
shared	O
parameters	O
.	O

We	O
discuss	O
how	O
to	O
solve	O
(	O
3	O
)	O
for	O
an	O
arbitrary	O
model	O
in	O
Section	O
3.2	O
and	O
present	O
an	O
efficient	O
solution	O
when	O
the	O
underlying	O
model	O
is	O
an	O
encoder	B-Method
-	I-Method
decoder	E-Method
in	O
Section	O
3.3	O
.	O

section	O
:	O
Solving	O
the	O
Optimization	B-Task
Problem	E-Task
The	O
optimization	B-Task
problem	E-Task
defined	O
in	O
(	O
3	O
)	O
is	O
equivalent	O
to	O
finding	O
a	O
minimum	O
-	O
norm	O
point	O
in	O
the	O
convex	O
hull	O
of	O
the	O
set	O
of	O
input	O
points	O
.	O

This	O
problem	O
arises	O
naturally	O
in	O
computational	B-Task
geometry	E-Task
:	O
it	O
is	O
equivalent	O
to	O
finding	O
the	O
closest	O
point	O
within	O
a	O
convex	O
hull	O
to	O
a	O
given	O
query	O
point	O
.	O

It	O
has	O
been	O
studied	O
extensively	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Although	O
many	O
algorithms	O
have	O
been	O
proposed	O
,	O
they	O
do	O
not	O
apply	O
in	O
our	O
setting	O
because	O
the	O
assumptions	O
they	O
make	O
do	O
not	O
hold	O
.	O

Algorithms	O
proposed	O
in	O
the	O
computational	B-Task
geometry	E-Task
literature	O
address	O
the	O
problem	O
of	O
finding	B-Task
minimum	I-Task
-	I-Task
norm	I-Task
points	E-Task
in	O
the	O
convex	O
hull	O
of	O
a	O
large	O
number	O
of	O
points	O
in	O
a	O
low	O
-	O
dimensional	O
space	O
(	O
typically	O
of	O
dimensionality	O
2	O
or	O
3	O
)	O
.	O

In	O
our	O
setting	O
,	O
the	O
number	O
of	O
points	O
is	O
the	O
number	O
of	O
tasks	O
and	O
is	O
typically	O
low	O
;	O
in	O
contrast	O
,	O
the	O
dimensionality	S-Metric
is	O
the	O
number	O
of	O
shared	O
parameters	O
and	O
can	O
be	O
in	O
the	O
millions	O
.	O

We	O
therefore	O
use	O
a	O
different	O
approach	O
based	O
on	O
convex	B-Task
optimization	E-Task
,	O
since	O
(	O
3	O
)	O
is	O
a	O
convex	B-Task
quadratic	I-Task
problem	E-Task
with	O
linear	O
constraints	O
.	O

Before	O
we	O
tackle	O
the	O
general	O
case	O
,	O
let	O
's	O
consider	O
the	O
case	O
of	O
two	O
tasks	O
.	O

The	O
optimization	B-Task
problem	E-Task
can	O
be	O
defined	O
as	O
,	O
which	O
is	O
a	O
onedimensional	B-Method
quadratic	I-Method
function	I-Method
of	I-Method
α	E-Method
with	O
an	O
analytical	B-Method
solution	E-Method
:	O
where	O
.	O

We	O
further	O
visualize	O
this	O
solution	O
in	O
Figure	O
1	O
.	O

Although	O
this	O
is	O
only	O
applicable	O
when	O
T	O
=	O
2	O
,	O
this	O
enables	O
efficient	O
application	O
of	O
the	O
Frank	B-Method
-	I-Method
Wolfe	I-Method
algorithm	E-Method
[	O
reference	O
]	O
since	O
the	O
line	B-Method
search	E-Method
can	O
be	O
solved	O
analytically	O
.	O

Hence	O
,	O
we	O
use	O
Frank	B-Method
-	I-Method
Wolfe	E-Method
to	O
solve	O
the	O
constrained	B-Task
optimization	I-Task
problem	E-Task
,	O
using	O
(	O
4	O
)	O
as	O
a	O
subroutine	O
for	O
the	O
line	B-Method
search	E-Method
.	O

We	O
give	O
all	O
the	O
update	B-Method
equations	E-Method
for	O
the	O
Frank	B-Method
-	I-Method
Wolfe	I-Method
solver	E-Method
in	O
Algorithm	O
2	O
.	O

Gradient	B-Method
descent	E-Method
on	O
task	O
-	O
specific	O
parameters	O
3	O
:	O
end	O
for	O
Gradient	B-Method
descent	E-Method
on	O
shared	O
parameters	O
6	O
:	O
procedure	O
FRANKWOLFESOLVER	B-Method
(	I-Method
θ	E-Method


)	O
7	O
:	O
repeat	O
10:t	O
=	O
arg	O
min	O
r	O
t	O
α	O
t	O
M	O
rt	O
untilγ	O
∼	O
0	O
or	O
Number	O
of	O
Iterations	O
Limit	O
14	O
:	O
return	O
α	O
1	O
,	O
.	O

.	O


.	O


,	O
α	O
T	O
15	O
:	O
end	O
procedure	O
section	O
:	O
Efficient	O
Optimization	S-Task
for	O
Encoder	B-Task
-	I-Task
Decoder	I-Task
Architectures	E-Task
The	O
MTL	S-Task
update	O
described	O
in	O
Algorithm	O
2	O
is	O
applicable	O
to	O
any	O
problem	O
that	O
uses	O
optimization	S-Task
based	O
on	O
gradient	B-Method
descent	E-Method
.	O

Our	O
experiments	O
also	O
suggest	O
that	O
the	O
Frank	B-Method
-	I-Method
Wolfe	I-Method
solver	E-Method
is	O
efficient	O
and	O
accurate	O
as	O
it	O
typically	O
converges	O
in	O
a	O
modest	O
number	O
of	O
iterations	O
with	O
negligible	O
effect	O
on	O
training	B-Metric
time	E-Metric
.	O

However	O
,	O
the	O
algorithm	O
we	O
described	O
needs	O
to	O
compute	O
∇	O
θ	O
shL	O
t	O
(	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
for	O
each	O
task	O
t	O
,	O
which	O
requires	O
a	O
backward	O
pass	O
over	O
the	O
shared	O
parameters	O
for	O
each	O
task	O
.	O

Hence	O
,	O
the	O
resulting	O
gradient	B-Task
computation	E-Task
would	O
be	O
the	O
forward	O
pass	O
followed	O
by	O
T	O
backward	O
passes	O
.	O

Considering	O
the	O
fact	O
that	O
computation	O
of	O
the	O
backward	B-Method
pass	E-Method
is	O
typically	O
more	O
expensive	O
than	O
the	O
forward	B-Method
pass	E-Method
,	O
this	O
results	O
in	O
linear	B-Metric
scaling	I-Metric
of	I-Metric
the	I-Metric
training	I-Metric
time	E-Metric
and	O
can	O
be	O
prohibitive	O
for	O
problems	O
with	O
more	O
than	O
a	O
few	O
tasks	O
.	O

We	O
now	O
propose	O
an	O
efficient	O
method	O
that	O
optimizes	O
an	O
upper	O
bound	O
of	O
the	O
objective	O
and	O
requires	O
only	O
a	O
single	O
backward	O
pass	O
.	O

We	O
further	O
show	O
that	O
optimizing	O
this	O
upper	B-Method
bound	E-Method
yields	O
a	O
Pareto	B-Method
optimal	I-Method
solution	E-Method
under	O
realistic	O
assumptions	O
.	O

The	O
architectures	O
we	O
address	O
conjoin	O
a	O
shared	B-Method
representation	I-Method
function	E-Method
with	O
task	O
-	O
specific	O
decision	O
functions	O
.	O

This	O
class	O
of	O
architectures	O
covers	O
most	O
of	O
the	O
existing	O
deep	O
MTL	S-Task


models	O
and	O
can	O
be	O
formally	O
defined	O
by	O
constraining	O
the	O
hypothesis	O
class	O
as	O
where	O
g	O
is	O
the	O
representation	O
function	O
shared	O
by	O
all	O
tasks	O
and	O
f	O
t	O
are	O
the	O
task	O
-	O
specific	O
functions	O
that	O
take	O
this	O
representation	O
as	O
input	O
.	O

If	O
we	O
denote	O
the	O
representations	O
as	O
Z	O
=	O
z	O
1	O
,	O
.	O

.	O


.	O


,	O
z	O
N	O
,	O
where	O
z	O
i	O
=	O
g	O
(	O
x	O
i	O
;	O
θ	O
sh	O
)	O
,	O
we	O
can	O
state	O
the	O
following	O
upper	O
bound	O
as	O
a	O
direct	O
consequence	O
of	O
the	O
chain	B-Method
rule	E-Method
:	O
term	O
since	O
it	O
does	O
not	O
affect	O
the	O
optimization	S-Task
.	O

The	O
resulting	O
optimization	B-Task
problem	E-Task
is	O
We	O
refer	O
to	O
this	O
problem	O
as	O
MGDA	B-Method
-	I-Method
UB	E-Method
(	O
Multiple	B-Method
Gradient	I-Method
Descent	I-Method
Algorithm	I-Method
-	I-Method
Upper	I-Method
Bound	E-Method
)	O
.	O

In	O
practice	O
,	O
MGDA	B-Method
-	I-Method
UB	E-Method
corresponds	O
to	O
using	O
the	O
gradients	O
of	O
the	O
task	O
losses	O
with	O
respect	O
to	O
the	O
representations	O
instead	O
of	O
the	O
shared	O
parameters	O
.	O

We	O
use	O
Algorithm	O
2	O
with	O
only	O
this	O
change	O
as	O
the	O
final	O
method	O
.	O

Although	O
MGDA	B-Method
-	I-Method
UB	E-Method
is	O
an	O
approximation	O
of	O
the	O
original	O
optimization	B-Task
problem	E-Task
,	O
we	O
now	O
state	O
a	O
theorem	O
that	O
shows	O
that	O
our	O
method	O
produces	O
a	O
Pareto	B-Task
optimal	I-Task
solution	E-Task
under	O
mild	O
assumptions	O
.	O

The	O
proof	O
is	O
given	O
in	O
the	O
supplement	O
.	O

section	O
:	O
Theorem	O
1	O
Assume	O
∂Z	B-Method
∂θ	I-Method
sh	E-Method
is	O
full	O
-	O
rank	O
.	O

If	O
α	O
1	O
,	O
...	O
,	O
T	O
is	O
the	O
solution	O
of	O
MGDA	B-Method
-	I-Method
UB	E-Method
,	O
one	O
of	O
the	O
following	O
is	O
true	O
:	O
=	O
0	O
and	O
the	O
current	O
parameters	O
are	O
Pareto	O
stationary	O
.	O

This	O
result	O
follows	O
from	O
the	O
fact	O
that	O
as	O
long	O
as	O
∂Z	O
∂θ	O
sh	O
is	O
full	O
rank	O
,	O
optimizing	O
the	O
upper	O
bound	O
corresponds	O
to	O
minimizing	O
the	O
norm	O
of	O
the	O
convex	O
combination	O
of	O
the	O
gradients	O
using	O
the	O
Mahalonobis	O
norm	O
defined	O
by	O
∂Z	O
∂θ	O
sh	O
∂Z	O
∂θ	O
sh	O
.	O

The	O
non	O
-	O
singularity	O
assumption	O
is	O
reasonable	O
as	O
singularity	O
implies	O
that	O
tasks	O
are	O
linearly	O
related	O
and	O
a	O
trade	O
-	O
off	O
is	O
not	O
necessary	O
.	O

In	O
summary	O
,	O
our	O
method	O
provably	O
finds	O
a	O
Pareto	O
stationary	O
point	O
with	O
negligible	O
computational	B-Metric
overhead	E-Metric
and	O
can	O
be	O
applied	O
to	O
any	O
deep	B-Task
multi	I-Task
-	I-Task
objective	I-Task
problem	E-Task
with	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
model	E-Method
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
presented	O
MTL	S-Task
method	O
on	O
a	O
number	O
of	O
problems	O
.	O

First	O
,	O
we	O
use	O
MultiMNIST	S-Method
[	O
reference	O
]	O
,	O
an	O
MTL	S-Task
adaptation	O
of	O
MNIST	S-Method
[	O
reference	O
]	O
.	O

Next	O
,	O
we	O
tackle	O
multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
on	O
the	O
CelebA	B-Material
dataset	E-Material
[	O
reference	O
]	O
by	O
considering	O
each	O
label	O
as	O
a	O
distinct	O
binary	B-Task
classification	I-Task
task	E-Task
.	O

These	O
problems	O
include	O
both	O
classification	B-Task
and	I-Task
regression	E-Task
,	O
with	O
the	O
number	O
of	O
tasks	O
ranging	O
from	O
2	O
to	O
40	O
.	O

Finally	O
,	O
we	O
experiment	O
with	O
scene	B-Task
understanding	E-Task
,	O
jointly	O
tackling	O
the	O
tasks	O
of	O
semantic	B-Task
segmentation	E-Task
,	O
instance	B-Task
segmentation	E-Task
,	O
and	O
depth	B-Task
estimation	E-Task
on	O
the	O
Cityscapes	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

We	O
discuss	O
each	O
experiment	O
separately	O
in	O
the	O
following	O
subsections	O
.	O

The	O
baselines	O
we	O
consider	O
are	O
(	O
i	O
)	O
uniform	B-Method
scaling	E-Method
:	O
minimizing	O
a	O
uniformly	B-Method
weighted	I-Method
sum	I-Method
of	I-Method
loss	I-Method
functions	E-Method
1	O
T	O
t	O
L	O
t	O
,	O
(	O
ii	O
)	O
single	O
task	O
:	O
solving	O
tasks	O
independently	O
,	O
(	O
iii	O
)	O
grid	B-Method
search	E-Method
:	O
exhaustively	O
trying	O
various	O
values	O
from	O
{	O
c	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
|	O
t	O
c	O
t	O
=	O
1	O
}	O
and	O
optimizing	O
for	O
[	O
reference	O
]	O
:	O
using	O
the	O
uncertainty	B-Method
weighting	E-Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
and	O
(	O
v	O
)	O
GradNorm	S-Method
:	O
using	O
the	O
normalization	S-Method
proposed	O
by	O
[	O
reference	O
]	O
.	O

section	O
:	O
MultiMNIST	O
Our	O
initial	O
experiments	O
are	O
on	O
MultiMNIST	S-Material
,	O
an	O
MTL	S-Task
version	O
of	O
the	O
MNIST	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

In	O
order	O
to	O
convert	O
digit	B-Task
classification	E-Task
into	O
a	O
multi	B-Task
-	I-Task
task	I-Task
problem	E-Task
,	O
[	O
reference	O
]	O
overlaid	O
multiple	O
images	O
together	O
.	O

We	O
use	O
a	O
similar	O
construction	O
.	O

For	O
each	O
image	O
,	O
a	O
different	O
one	O
is	O
chosen	O
uniformly	O
in	O
random	O
.	O

Then	O
one	O
of	O
these	O
images	O
is	O
put	O
at	O
the	O
top	O
-	O
left	O
and	O
the	O
other	O
one	O
is	O
at	O
the	O
bottom	O
-	O
right	O
.	O

The	O
resulting	O
tasks	O
are	O
:	O
classifying	O
the	O
digit	O
on	O
the	O
top	O
-	O
left	O
(	O
task	O
-	O
L	O
)	O
and	O
classifying	O
the	O
digit	O
on	O
the	O
bottom	O
-	O
right	O
(	O
task	O
-	O
R	O
)	O
.	O

We	O
use	O
60	O
K	O
examples	O
and	O
directly	O
apply	O
existing	O
single	B-Method
-	I-Method
task	I-Method
MNIST	I-Method
models	E-Method
.	O

The	O
MultiMNIST	B-Material
dataset	E-Material
is	O
illustrated	O
in	O
the	O
supplement	O
.	O

We	O
use	O
the	O
LeNet	B-Method
architecture	E-Method
[	O
reference	O
]	O
.	O

We	O
treat	O
all	O
layers	O
except	O
the	O
last	O
as	O
the	O
representation	O
function	O
g	O
and	O
put	O
two	O
fully	O
-	O
connected	O
layers	O
as	O
task	O
-	O
specific	O
functions	O
(	O
see	O
the	O
[	O
reference	O
]	O
.	O

Lower	O
is	O
better	O
.	O

We	O
divide	O
attributes	O
into	O
two	O
sets	O
for	O
legibility	O
:	O
easy	O
on	O
the	O
left	O
,	O
hard	O
on	O
the	O
right	O
.	O

Zoom	O
in	O
for	O
details	O
.	O

supplement	O
for	O
details	O
)	O
.	O

We	O
visualize	O
the	O
performance	B-Metric
profile	E-Metric
as	O
a	O
scatter	B-Metric
plot	I-Metric
of	I-Metric
accuracies	E-Metric
on	O
task	O
-	O
L	O
and	O
task	O
-	O
R	O
in	O
Figure	O
3	O
,	O
and	O
list	O
the	O
results	O
in	O
Table	O
3	O
.	O

In	O
this	O
setup	O
,	O
any	O
static	B-Method
scaling	E-Method
results	O
in	O
lower	O
accuracy	S-Metric
than	O
solving	O
each	O
task	O
separately	O
(	O
the	O
singletask	B-Method
baseline	E-Method
)	O
.	O

The	O
two	O
tasks	O
appear	O
to	O
compete	O
for	O
model	O
capacity	O
,	O
since	O
increase	O
in	O
the	O
accuracy	S-Metric
of	O
one	O
task	O
results	O
in	O
decrease	O
in	O
the	O
accuracy	S-Metric
of	O
the	O
other	O
.	O

Uncertainty	B-Task
weighting	E-Task
[	O
reference	O
]	O
and	O
GradNorm	S-Method
[	O
reference	O
]	O
find	O
solutions	O
that	O
are	O
slightly	O
better	O
than	O
grid	B-Method
search	E-Method
but	O
distinctly	O
worse	O
than	O
the	O
single	O
-	O
task	O
baseline	O
.	O

In	O
contrast	O
,	O
our	O
method	O
finds	O
a	O
solution	O
that	O
efficiently	O
utilizes	O
the	O
model	O
capacity	O
and	O
yields	O
accuracies	S-Metric
that	O
are	O
as	O
good	O
as	O
the	O
single	B-Method
-	I-Method
task	I-Method
solutions	E-Method
.	O

This	O
experiment	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
method	O
as	O
well	O
as	O
the	O
necessity	O
of	O
treating	O
MTL	S-Task
as	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
.	O

Even	O
after	O
a	O
large	O
hyper	O
-	O
parameter	O
search	O
,	O
any	O
scaling	O
of	O
tasks	O
does	O
not	O
approach	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O

Next	O
,	O
we	O
tackle	O
multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
.	O

Given	O
a	O
set	O
of	O
attributes	O
,	O
multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
calls	O
for	O
deciding	O
whether	O
each	O
attribute	O
holds	O
for	O
the	O
input	O
.	O

We	O
use	O
the	O
CelebA	B-Material
dataset	E-Material
[	O
reference	O
]	O
,	O
which	O
includes	O
200	O
K	O
face	O
images	O
annotated	O
with	O
40	O
attributes	O
.	O

Each	O
attribute	O
gives	O
rise	O
to	O
a	O
binary	B-Task
classification	I-Task
task	E-Task
and	O
we	O
cast	O
this	O
as	O
a	O
40	O
-	O
way	O
MTL	S-Task
problem	O
.	O

We	O
use	O
ResNet	B-Method
-	I-Method
18	E-Method
[	O
reference	O
]	O
without	O
the	O
final	O
layer	O
as	O
a	O
shared	O
representation	O
function	O
,	O
and	O
attach	O
a	O
linear	B-Method
layer	E-Method
for	O
each	O
attribute	O
(	O
see	O
the	O
supplement	O
for	O
further	O
details	O
)	O
.	O

section	O
:	O
Multi	B-Task
-	I-Task
Label	I-Task
Classification	E-Task
We	O
plot	O
the	O
resulting	O
error	S-Metric
for	O
each	O
binary	B-Task
classification	I-Task
task	E-Task
as	O
a	O
radar	O
chart	O
in	O
Figure	O
2	O
.	O

The	O
average	O
over	O
them	O
is	O
listed	O
in	O
Table	O
1	O
.	O

We	O
skip	O
grid	B-Method
search	E-Method
since	O
it	O
is	O
not	O
feasible	O
over	O
40	O
tasks	O
.	O

Although	O
uniform	B-Method
scaling	E-Method
is	O
the	O
norm	O
in	O
the	O
multi	B-Task
-	I-Task
label	I-Task
classification	I-Task
literature	E-Task
,	O
single	B-Task
-	I-Task
task	E-Task
performance	O
is	O
significantly	O
better	O
.	O

Our	O
method	O
outperforms	O
baselines	O
for	O
significant	O
majority	O
of	O
tasks	O
and	O
achieves	O
comparable	O
performance	O
in	O
rest	O
.	O

This	O
experiment	O
also	O
shows	O
that	O
our	O
method	O
remains	O
effective	O
when	O
the	O
number	O
of	O
tasks	O
is	O
high	O
.	O

section	O
:	O
Scene	B-Task
Understanding	E-Task
To	O
evaluate	O
our	O
method	O
in	O
a	O
more	O
realistic	O
setting	O
,	O
we	O
use	O
scene	B-Task
understanding	E-Task
.	O

Given	O
an	O
RGB	O
image	O
,	O
we	O
solve	O
three	O
tasks	O
:	O
semantic	B-Task
segmentation	E-Task
(	O
assigning	O
pixel	O
-	O
level	O
class	O
labels	O
)	O
,	O
instance	B-Method
segmentation	E-Method
(	O
assigning	O
pixel	O
-	O
level	O
instance	O
labels	O
)	O
,	O
and	O
monocular	B-Task
depth	I-Task
estimation	E-Task
(	O
estimating	O
continuous	O
disparity	O
per	O
pixel	O
)	O
.	O

We	O
follow	O
the	O
experimental	O
procedure	O
of	O
[	O
reference	O
]	O
and	O
use	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
architecture	E-Method
.	O

The	O
encoder	S-Method
is	O
based	O
on	O
ResNet	B-Method
-	I-Method
50	E-Method
[	O
reference	O
]	O
and	O
is	O
shared	O
by	O
all	O
three	O
tasks	O
.	O

The	O
decoders	O
are	O
task	O
-	O
specific	O
and	O
are	O
based	O
on	O
the	O
pyramid	B-Method
pooling	I-Method
module	E-Method
[	O
reference	O
]	O
)	O
(	O
see	O
the	O
supplement	O
for	O
further	O
implementation	O
details	O
)	O
.	O

Since	O
the	O
output	O
space	O
of	O
instance	B-Task
segmentation	E-Task
is	O
unconstrained	O
(	O
the	O
number	O
of	O
instances	O
is	O
not	O
known	O
in	O
advance	O
)	O
,	O
we	O
use	O
a	O
proxy	B-Method
problem	E-Method
as	O
in	O
[	O
reference	O
]	O
.	O

For	O
each	O
pixel	O
,	O
we	O
estimate	O
the	O
location	O
of	O
the	O
center	O
of	O
mass	O
of	O
the	O
instance	O
that	O
encompasses	O
the	O
pixel	O
.	O

These	O
center	O
votes	O
can	O
then	O
be	O
clustered	O
to	O
extract	O
the	O
instances	O
.	O

In	O
our	O
experiments	O
,	O
we	O
directly	O
report	O
the	O
MSE	S-Metric
in	O
the	O
proxy	B-Task
task	E-Task
.	O

Figure	O
4	O
shows	O
the	O
performance	O
profile	O
for	O
each	O
pair	O
of	O
tasks	O
,	O
although	O
we	O
perform	O
all	O
experiments	O
on	O
all	O
three	O
tasks	O
jointly	O
.	O

The	O
pairwise	B-Metric
performance	I-Metric
profiles	E-Metric
shown	O
in	O
Figure	O
4	O
are	O
simply	O
2D	O
projections	O
of	O
the	O
three	O
-	O
dimensional	O
profile	O
,	O
presented	O
this	O
way	O
for	O
legibility	O
.	O

The	O
results	O
are	O
also	O
listed	O
in	O
Table	O
4	O
.	O

MTL	S-Task
outperforms	O
single	O
-	O
task	B-Metric
accuracy	E-Metric
,	O
indicating	O
that	O
the	O
tasks	O
cooperate	O
and	O
help	O
each	O
other	O
.	O

Our	O
method	O
outperforms	O
all	O
baselines	O
on	O
all	O
tasks	O
.	O

section	O
:	O
Role	O
of	O
the	O
Approximation	O
In	O
order	O
to	O
understand	O
the	O
role	O
of	O
the	O
approximation	O
proposed	O
in	O
Section	O
3.3	O
,	O
we	O
compare	O
the	O
final	O
performance	O
and	O
training	B-Metric
time	E-Metric
of	O
our	O
algorithm	O
with	O
and	O
without	O
the	O
presented	O
approximation	O
in	O
Table	O
2	O
(	O
runtime	S-Metric
measured	O
on	O
a	O
single	O
Titan	O
Xp	O
GPU	O
)	O
.	O

For	O
a	O
small	O
number	O
of	O
tasks	O
(	O
3	O
for	O
scene	B-Task
understanding	E-Task
)	O
,	O
training	B-Metric
time	E-Metric
is	O
reduced	O
by	O
40	O
%	O
.	O

For	O
the	O
multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
experiment	O
(	O
40	O
tasks	O
)	O
,	O
the	O
presented	O
approximation	O
accelerates	O
learning	S-Task
by	O
a	O
factor	O
of	O
25	O
.	O

On	O
the	O
accuracy	B-Metric
side	E-Metric
,	O
we	O
expect	O
both	O
methods	O
to	O
perform	O
similarly	O
as	O
long	O
as	O
the	O
full	O
-	O
rank	O
assumption	O
is	O
satisfied	O
.	O

As	O
expected	O
,	O
the	O
accuracy	S-Metric
of	O
both	O
methods	O
is	O
very	O
similar	O
.	O

Somewhat	O
surprisingly	O
,	O
our	O
approximation	O
results	O
in	O
slightly	O
improved	O
accuracy	S-Metric
in	O
all	O
experiments	O
.	O

While	O
counter	O
-	O
intuitive	O
at	O
first	O
,	O
we	O
hypothesize	O
that	O
this	O
is	O
related	O
to	O
the	O
use	O
of	O
SGD	S-Method
in	O
the	O
learning	B-Method
algorithm	E-Method
.	O

Stability	B-Task
analysis	E-Task
in	O
convex	B-Task
optimization	E-Task
suggests	O
that	O
if	O
gradients	O
are	O
computed	O
with	O
an	O
error∇	S-Metric
θ	O
L	O
t	O
=	O
∇	O
θ	O
L	O
t	O
+	O
e	O
t	O
(	O
θ	O
corresponds	O
to	O
θ	O
sh	O
in	O
(	O
3	O
)	O
)	O
,	O
as	O
opposed	O
to	O
Z	O
in	O
the	O
approximate	B-Task
problem	E-Task
in	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method
)	O
,	O
the	O
error	S-Metric
in	O
the	O
solution	O
is	O
bounded	O
as	O
α	O
−	O
α	O
2	O
≤	O
O	O
(	O
max	O
t	O
e	O
t	O
2	O
)	O
.	O

Considering	O
the	O
fact	O
that	O
the	O
gradients	O
are	O
computed	O
over	O
the	O
full	O
parameter	O
set	O
(	O
millions	O
of	O
dimensions	O
)	O
for	O
the	O
original	O
problem	O
and	O
over	O
a	O
smaller	O
space	O
for	O
the	O
approximation	O
(	O
batch	B-Method
size	I-Method
times	I-Method
representation	E-Method
which	O
is	O
in	O
the	O
thousands	O
)	O
,	O
the	O
dimension	O
of	O
the	O
error	S-Metric
vector	O
is	O
significantly	O
higher	O
in	O
the	O
original	O
problem	O
.	O

We	O
expect	O
the	O
l	O
2	O
norm	O
of	O
such	O
a	O
random	O
vector	O
to	O
depend	O
on	O
the	O
dimension	O
.	O

In	O
summary	O
,	O
our	O
quantitative	O
analysis	O
of	O
the	O
approximation	O
suggests	O
that	O
(	O
i	O
)	O
the	O
approximation	O
does	O
not	O
cause	O
an	O
accuracy	O
drop	O
and	O
(	O
ii	O
)	O
by	O
solving	O
an	O
equivalent	B-Task
problem	E-Task
in	O
a	O
lower	O
-	O
dimensional	O
space	O
,	O
our	O
method	O
achieves	O
both	O
better	O
computational	B-Metric
efficiency	E-Metric
and	O
higher	O
stability	S-Metric
.	O

section	O
:	O
Conclusion	O
We	O
described	O
an	O
approach	O
to	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

Our	O
approach	O
is	O
based	O
on	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
.	O

In	O
order	O
to	O
apply	O
multi	B-Method
-	I-Method
objective	I-Method
optimization	E-Method
to	O
MTL	S-Task
,	O
we	O
described	O
an	O
efficient	O
algorithm	O
as	O
well	O
as	O
specific	O
approximations	O
that	O
yielded	O
a	O
deep	O
MTL	S-Task
algorithm	O
with	O
almost	O
no	O
computational	B-Metric
overhead	E-Metric
.	O

Our	O
experiments	O
indicate	O
that	O
the	O
resulting	O
algorithm	O
is	O
effective	O
for	O
a	O
wide	O
range	O
of	O
multi	B-Task
-	I-Task
task	I-Task
scenarios	E-Task
.	O

Figure	O
3	O
:	O
MultiMNIST	B-Metric
accuracy	I-Metric
profile	E-Metric
.	O

We	O
plot	O
the	O
obtained	O
accuracy	S-Metric
in	O
detecting	O
the	O
left	O
and	O
right	O
digits	O
for	O
all	O
baselines	O
.	O

The	O
grid	B-Task
-	I-Task
search	E-Task
results	O
suggest	O
that	O
the	O
tasks	O
compete	O
for	O
model	B-Task
capacity	E-Task
.	O

Our	O
method	O
is	O
the	O
only	O
one	O
that	O
finds	O
a	O
solution	O
that	O
is	O
as	O
good	O
as	O
training	O
a	O
dedicated	O
model	O
for	O
each	O
task	O
.	O

Top	O
-	O
right	O
is	O
better	O
.	O

Figure	O
4	O
:	O
Cityscapes	B-Metric
performance	I-Metric
profile	E-Metric
.	O

We	O
plot	O
the	O
performance	O
of	O
all	O
baselines	O
for	O
the	O
tasks	O
of	O
semantic	B-Task
segmentation	E-Task
,	O
instance	B-Task
segmentation	E-Task
,	O
and	O
depth	B-Task
estimation	E-Task
.	O

We	O
use	O
mIoU	S-Metric
for	O
semantic	B-Task
segmentation	E-Task
,	O
error	S-Metric
of	O
per	B-Method
-	I-Method
pixel	I-Method
regression	E-Method
(	O
normalized	O
to	O
image	O
size	O
)	O
for	O
instance	B-Task
segmentation	E-Task
,	O
and	O
disparity	O
error	S-Metric
for	O
depth	B-Task
estimation	E-Task
.	O

To	O
convert	O
errors	O
to	O
performance	B-Metric
measures	E-Metric
,	O
we	O
use	O
1	O
−	O
instance	O
error	S-Metric
and	O
1	O
/	O
disparity	O
error	S-Metric
.	O

We	O
plot	O
2D	O
projections	O
of	O
the	O
performance	O
profile	O
for	O
each	O
pair	O
of	O
tasks	O
.	O

Although	O
we	O
plot	O
pairwise	O
projections	O
for	O
visualization	S-Task
,	O
each	O
point	O
in	O
the	O
plots	O
solves	O
all	O
tasks	O
.	O

Top	O
-	O
right	O
is	O
better	O
.	O

The	O
KKT	S-Method
condition	O
for	O
this	O
Lagrangian	O
yields	O
the	O
desired	O
result	O
as	O
section	O
:	O
B	O
Additional	O
Results	O
on	O
Multi	B-Task
-	I-Task
label	I-Task
Classification	E-Task
In	O
this	O
section	O
,	O
we	O
present	O
the	O
experimental	O
results	O
we	O
did	O
not	O
include	O
in	O
the	O
main	O
text	O
.	O

In	O
the	O
main	O
text	O
,	O
we	O
plotted	O
a	O
radar	B-Method
chart	E-Method
of	O
the	O
binary	B-Metric
attribute	I-Metric
classification	I-Metric
errors	E-Metric
.	O

However	O
,	O
we	O
did	O
not	O
include	O
the	O
tabulated	O
results	O
due	O
to	O
the	O
space	O
limitations	O
.	O

Here	O
we	O
list	O
the	O
binary	O
classification	O
error	S-Metric
of	O
each	O
attribute	O
for	O
each	O
algorithm	O
in	O
Table	O
5	O
.	O

C	O
Implementation	O
Details	O
section	O
:	O
C.1	O
MultiMNIST	O
We	O
use	O
the	O
MultiMNIST	B-Material
dataset	E-Material
,	O
which	O
overlays	O
multiple	O
images	O
together	O
[	O
reference	O
]	O
.	O

For	O
each	O
image	O
,	O
a	O
different	O
one	O
is	O
chosen	O
uniformly	O
in	O
random	O
.	O

One	O
of	O
these	O
images	O
is	O
placed	O
at	O
the	O
top	O
-	O
left	O
and	O
the	O
other	O
at	O
the	O
bottom	O
-	O
right	O
.	O

We	O
show	O
sample	O
MultiMNIST	B-Material
images	E-Material
in	O
Figure	O
6	O
.	O

For	O
the	O
MultiMNIST	O
experiments	O
,	O
we	O
use	O
an	O
architecture	O
based	O
on	O
[	O
reference	O
]	O
.	O

We	O
use	O
all	O
layers	O
except	O
the	O
final	O
one	O
as	O
a	O
shared	B-Method
encoder	E-Method
.	O

We	O
use	O
the	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
as	O
a	O
task	O
-	O
specific	O
function	O
for	O
the	O
left	B-Task
and	I-Task
right	I-Task
tasks	E-Task
by	O
simply	O
adding	O
two	O
independent	O
fully	B-Method
-	I-Method
connected	I-Method
layers	E-Method
,	O
each	O
taking	O
the	O
output	O
of	O
the	O
shared	B-Method
encoder	E-Method
as	O
input	O
.	O

As	O
a	O
task	O
-	O
specific	O
loss	O
function	O
,	O
we	O
use	O
the	O
cross	B-Method
-	I-Method
entropy	I-Method
loss	E-Method
with	O
a	O
softmax	S-Method
for	O
both	O
tasks	O
.	O

The	O
architecture	O
is	O
visualized	O
in	O
Figure	O
5	O
.	O

The	O
implementation	O
uses	O
PyTorch	S-Method
[	O
reference	O
]	O
.	O

For	O
all	O
baselines	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	B-Metric
rates	E-Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
validation	B-Metric
accuracy	E-Metric
.	O

We	O
used	O
SGD	S-Method
with	O
momentum	S-Method
,	O
halving	O
the	O
learning	B-Metric
rate	E-Metric
every	O
30	O
epochs	O
.	O

We	O
use	O
batch	O
size	O
256	O
and	O
train	O
for	O
100	O
epochs	O
.	O

We	O
report	O
test	B-Metric
accuracy	E-Metric
.	O

section	O
:	O
C.2	O
Multi	B-Task
-	I-Task
label	I-Task
classification	E-Task
For	O
multi	B-Task
-	I-Task
label	I-Task
classification	I-Task
experiments	E-Task
,	O
we	O
use	O
ResNet	B-Method
-	I-Method
18	E-Method
[	O
reference	O
]	O
without	O
the	O
final	O
layer	O
as	O
a	O
shared	O
representation	O
function	O
.	O

Since	O
there	O
are	O
40	O
attributes	O
,	O
we	O
add	O
40	O
separate	O
2048	O
×	O
2	O
dimensional	O
fully	O
-	O
connected	O
layers	O
as	O
task	O
-	O
specific	O
functions	O
.	O

The	O
final	O
two	O
-	O
dimensional	O
output	O
is	O
passed	O
through	O
a	O
2	B-Method
-	I-Method
class	I-Method
softmax	E-Method
to	O
get	O
binary	O
attribute	O
classification	O
probabilities	O
.	O

We	O
use	O
cross	O
-	O
entropy	O
as	O
a	O
task	B-Task
-	I-Task
specific	I-Task
loss	E-Task
.	O

The	O
architecture	O
is	O
visualized	O
in	O
Figure	O
7	O
.	O

The	O
implementation	O
uses	O
PyTorch	S-Method
[	O
reference	O
]	O
.	O

We	O
resize	O
each	O
CelebA	B-Material
image	E-Material
[	O
reference	O
]	O
to	O
64	O
×	O
64	O
×	O
3	O
.	O

For	O
all	O
experiments	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	B-Metric
rates	E-Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
validation	B-Metric
accuracy	E-Metric
.	O

We	O
used	O
SGD	S-Method
with	O
momentum	S-Method
,	O
halving	O
the	O
learning	B-Metric
rate	E-Metric
every	O
30	O
epochs	O
.	O

We	O
use	O
batch	O
size	O
256	O
and	O
train	O
for	O
100	O
epochs	O
.	O

We	O
report	O
attribute	B-Metric
-	I-Metric
wise	I-Metric
binary	I-Metric
accuracies	E-Metric
on	O
the	O
test	O
set	O
as	O
well	O
as	O
the	O
average	B-Metric
accuracy	E-Metric
.	O

section	O
:	O
C.3	O
Scene	B-Task
understanding	E-Task
For	O
scene	B-Task
understanding	I-Task
experiments	E-Task
,	O
we	O
use	O
the	O
Cityscapes	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

We	O
resize	O
all	O
images	O
to	O
resolution	O
256	O
×	O
512	O
for	O
computational	B-Metric
efficiency	E-Metric
.	O

As	O
a	O
shared	B-Method
representation	I-Method
function	E-Method
(	O
encoder	S-Method
)	O
,	O
we	O
use	O
the	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
[	O
reference	O
]	O
in	O
fully	B-Method
-	I-Method
convolutional	I-Method
fashion	E-Method
.	O

We	O
take	O
the	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
and	O
only	O
use	O
layers	O
prior	O
to	O
average	B-Method
pooling	E-Method
that	O
are	O
fully	B-Method
convolutional	E-Method
.	O

As	O
a	O
decoder	O
,	O
we	O
use	O
the	O
pyramid	B-Method
pooling	I-Method
module	E-Method
[	O
reference	O
]	O
and	O
set	O
the	O
output	O
sizes	O
to	O
256	O
×	O
512	O
×	O
19	O
for	O
semantic	B-Task
segmentation	E-Task
(	O
19	O
classes	O
)	O
,	O
256	O
×	O
512	O
×	O
2	O
for	O
instance	B-Task
segmentation	E-Task
(	O
one	O
output	O
channel	O
for	O
the	O
x	O
-	O
offset	O
of	O
the	O
center	O
location	O
and	O
another	O
channel	O
for	O
the	O
y	O
-	O
offset	O
)	O
,	O
and	O
256	O
×	O
512	O
×	O
1	O
for	O
monocular	B-Task
depth	I-Task
estimation	E-Task
.	O

For	O
instance	B-Task
segmentation	E-Task
,	O
we	O
use	O
the	O
proxy	B-Task
task	E-Task
of	O
estimating	O
the	O
offset	O
for	O
the	O
center	O
location	O
of	O
the	O
instance	O
that	O
encompasses	O
the	O
pixel	O
.	O

We	O
directly	O
estimate	O
disparity	O
instead	O
of	O
depth	O
and	O
later	O
convert	O
it	O
to	O
depth	O
using	O
the	O
provided	O
camera	O
intrinsics	O
.	O

As	O
a	O
loss	O
function	O
,	O
we	O
use	O
cross	B-Method
-	I-Method
entropy	E-Method
with	O
a	O
softmax	S-Method
for	O
semantic	B-Task
segmentation	E-Task
,	O
and	O
MSE	S-Metric
for	O
depth	B-Task
and	I-Task
instance	I-Task
segmentation	E-Task
.	O

We	O
visualize	O
the	O
architecture	O
in	O
Figure	O
8	O
.	O

We	O
initialize	O
the	O
encoder	O
with	O
a	O
model	O
pretrained	O
on	O
ImageNet	S-Material
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
implementation	O
of	O
the	O
pyramidal	B-Method
pooling	I-Method
network	E-Method
with	O
bilinear	B-Method
interpolation	E-Method
shared	O
by	O
[	O
reference	O
]	O
.	O

Ground	O
-	O
truth	O
results	O
for	O
the	O
Cityscapes	B-Material
test	I-Material
set	E-Material
are	O
not	O
publicly	O
available	O
.	O

Therefore	O
,	O
we	O
report	O
numbers	O
on	O
the	O
validation	O
set	O
.	O

As	O
a	O
validation	O
set	O
for	O
hyperparameter	B-Task
search	E-Task
,	O
we	O
randomly	O
choose	O
275	O
images	O
from	O
the	O
training	O
set	O
.	O

After	O
the	O
best	O
hyperparameters	O
are	O
chosen	O
,	O
we	O
retrain	O
with	O
the	O
full	O
training	O
set	O
and	O
report	O
the	O
metrics	O
on	O
the	O
Cityscapes	B-Material
validation	I-Material
set	E-Material
,	O
which	O
our	O
algorithm	O
never	O
sees	O
during	O
training	O
or	O
hyperparameter	B-Method
search	E-Method
.	O

As	O
metrics	O
,	O
we	O
use	O
mean	B-Metric
intersection	I-Metric
over	I-Metric
union	E-Metric
(	O
mIoU	S-Metric
)	O
for	O
semantic	B-Task
segmentation	E-Task
,	O
MSE	S-Metric
for	O
instance	B-Task
segmentation	E-Task
,	O
and	O
MSE	S-Metric
for	O
disparities	S-Task
(	O
depth	B-Task
estimation	E-Task
)	O
.	O

We	O
directly	O
report	O
the	O
metric	O
in	O
the	O
proxy	B-Task
task	E-Task
for	O
instance	B-Task
segmentation	E-Task
instead	O
of	O
performing	O
a	O
further	O
clustering	B-Method
operation	E-Method
.	O

For	O
all	O
experiments	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	B-Metric
rates	E-Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
Figure	O
6	O
:	O
Sample	O
MultiMNIST	B-Material
images	E-Material
.	O

In	O
each	O
image	O
,	O
one	O
task	O
(	O
task	O
-	O
L	O
)	O
is	O
classifying	O
the	O
digit	O
on	O
the	O
top	O
-	O
left	O
and	O
the	O
second	O
task	O
(	O
task	O
-	O
R	O
)	O
is	O
classifying	O
the	O
digit	O
on	O
the	O
bottom	O
-	O
right	O
.	O

validation	B-Metric
accuracy	E-Metric
.	O

We	O
used	O
SGD	S-Method
with	O
momentum	S-Method
,	O
halving	O
the	O
learning	B-Metric
rate	E-Metric
every	O
30	O
epochs	O
.	O

We	O
use	O
batch	O
size	O
8	O
and	O
train	O
for	O
250	O
epochs	O
.	O

section	O
:	O
Convolutional	B-Method
ResNet	E-Method
50	O
:	O
ResNet	S-Method
50	O
without	O
final	O
average	B-Method
poling	E-Method
and	O
fully	B-Method
connected	I-Method
layer	E-Method
.	O

section	O
:	O
Pyramid	B-Method
Pooling	E-Method
section	O
:	O
section	O
:	O
A	O
Proof	O
of	O
Theorem	O
1	O
Proof	O
.	O

We	O
begin	O
by	O
showing	O
that	O
if	O
the	O
optimum	O
value	O
of	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method
)	O
is	O
0	O
,	O
so	O
is	O
the	O
optimum	O
value	O
of	O
(	O
3	O
)	O
.	O

This	O
shows	O
the	O
first	O
case	O
of	O
the	O
theorem	O
.	O

Then	O
,	O
we	O
will	O
show	O
the	O
second	O
part	O
.	O

If	O
the	O
optimum	O
value	O
of	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method




)	O
is	O
0	O
,	O
Hence	O
α	O
1	O
,	O
.	O

.	O


.	O


,	O
α	O
T	O
is	O
the	O
solution	O
of	O
(	O
3	O
)	O
and	O
the	O
optimal	O
value	O
of	O
(	O
3	O
)	O
is	O
0	O
.	O

This	O
proves	O
the	O
first	O
case	O
of	O
the	O
theorem	O
.	O

Before	O
we	O
move	O
to	O
the	O
second	O
case	O
,	O
we	O
state	O
a	O
straightforward	O
corollary	O
.	O

Since	O
∂Z	O
∂θ	O
sh	O
is	O
full	O
rank	O
,	O
this	O
equivalence	O
is	O
bi	O
-	O
directional	O
.	O

In	O
other	O
words	O
,	O
if	O
α	O
1	O
,	O
.	O

.	O


.	O


,	O
α	O
T	O
is	O
the	O
solution	O
of	O
(	O
3	O
)	O
,	O
it	O
is	O
the	O
solution	O
of	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method
)	O
as	O
well	O
.	O

Hence	O
,	O
both	O
formulations	O
completely	O
agree	O
on	O
Pareto	O
stationarity	O
.	O

In	O
order	O
to	O
prove	O
the	O
second	O
case	O
,	O
we	O
need	O
to	O
show	O
that	O
the	O
resulting	O
descent	O
direction	O
computed	O
by	O
solving	O
(	O
MGDA	S-Method
-	O
UB	O
)	O
does	O
not	O
increase	O
any	O
of	O
the	O
loss	O
functions	O
.	O

Formally	O
,	O
we	O
need	O
to	O
show	O
that	O
This	O
condition	O
is	O
equivalent	O
to	O
where	O
M	O
=	O
∂Z	O
∂θ	O
sh	O
∂Z	O
∂θ	O
sh	O
.	O

Since	O
M	O
is	O
positive	O
definite	O
(	O
following	O
the	O
assumption	O
)	O
,	O
this	O
is	O
further	O
equivalent	O
to	O
We	O
show	O
that	O
this	O
follows	O
from	O
the	O
optimality	O
conditions	O
for	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method
)	O
.	O

The	O
Lagrangian	O
of	O
(	O
MGDA	B-Method
-	I-Method
UB	E-Method
)	O
is	O
section	O
:	O
