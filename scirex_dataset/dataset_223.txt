document O
: O
The O
Variational Method
Fair Method
Autoencoder Method
We O
investigate O
the O
problem O
of O
learning Task
representations Task
that O
are O
invariant O
to O
certain O
nuisance O
or O
sensitive O
factors O
of O
variation O
in O
the O
data O
while O
retaining O
as O
much O
of O
the O
remaining O
information O
as O
possible O
. O
Our O
model O
is O
based O
on O
a O
variational Method
autoencoding Method
architecture Method
kingma2013auto O
, O
rezende2014stochastic O
with O
priors O
that O
encourage O
independence O
between O
sensitive O
and O
latent O
factors O
of O
variation O
. O
Any O
subsequent O
processing O
, O
such O
as O
classification Task
, O
can O
then O
be O
performed O
on O
this O
purged O
latent Method
representation Method
. O
To O
remove O
any O
remaining O
dependencies O
we O
incorporate O
an O
additional O
penalty O
term O
based O
on O
the O
“ O
Maximum Method
Mean Method
Discrepancy Method
” Method
( O
MMD Method
) O
gretton2006kernel O
measure O
. O
We O
discuss O
how O
these O
architectures O
can O
be O
efficiently O
trained O
on O
data O
and O
show O
in O
experiments O
that O
this O
method O
is O
more O
effective O
than O
previous O
work O
in O
removing O
unwanted O
sources O
of O
variation O
while O
maintaining O
informative Method
latent Method
representations Method
. O
section O
: O
Introduction O
In O
“ O
Representation Method
Learning Method
” O
one O
tries O
to O
find O
representations O
of O
the O
data O
that O
are O
informative O
for O
a O
particular O
task O
while O
removing O
the O
factors O
of O
variation O
that O
are O
uninformative O
and O
are O
typically O
detrimental O
for O
the O
task O
under O
consideration O
. O
Uninformative O
dimensions O
are O
often O
called O
“ O
noise O
” O
or O
“ O
nuisance O
variables O
” O
while O
informative O
dimensions O
are O
usually O
called O
latent O
or O
hidden O
factors O
of O
variation O
. O
Many O
machine Method
learning Method
algorithms Method
can O
be O
understood O
in O
this O
way O
: O
principal Method
component Method
analysis Method
, O
nonlinear Method
dimensional Method
reduction Method
and O
latent Method
Dirichlet Method
allocation Method
are O
all O
models O
that O
extract O
informative O
factors O
( O
dimensions O
, O
causes O
, O
topics O
) O
of O
the O
data O
which O
can O
often O
be O
used O
to O
visualize O
the O
data O
. O
On O
the O
other O
hand O
, O
linear Method
discriminant Method
analysis Method
and O
deep Method
( Method
convolutional Method
) Method
neural Method
nets Method
learn O
representations O
that O
are O
good O
for O
classification Task
. O
In O
this O
paper O
we O
consider O
the O
case O
where O
we O
wish O
to O
learn O
latent Method
representations Method
where O
( O
almost O
) O
all O
of O
the O
information O
about O
certain O
known O
factors O
of O
variation O
are O
purged O
from O
the O
representation O
while O
still O
retaining O
as O
much O
information O
about O
the O
data O
as O
possible O
. O
In O
other O
words O
, O
we O
want O
a O
latent Method
representation Method
that O
is O
maximally O
informative O
about O
an O
observed O
random O
variable O
( O
e.g. O
, O
class O
label O
) O
while O
minimally O
informative O
about O
a O
sensitive O
or O
nuisance O
variable O
. O
By O
treating O
as O
a O
sensitive O
variable O
, O
i.e. O
is O
correlated O
with O
our O
objective O
, O
we O
are O
dealing O
with O
“ O
fair Method
representations Method
” O
, O
a O
problem O
previously O
considered O
by O
. O
If O
we O
instead O
treat O
as O
a O
nuisance O
variable O
we O
are O
dealing O
with O
“ O
domain Task
adaptation Task
” O
, O
in O
other O
words O
by O
removing O
the O
domain O
from O
our O
representations O
we O
will O
obtain O
improved O
performance O
. O
In O
this O
paper O
we O
introduce O
a O
novel O
model O
based O
on O
deep O
variational Method
autoencoders Method
( O
VAE Method
) O
kingma2013auto O
, O
rezende2014stochastic O
. O
These O
models O
can O
naturally O
encourage O
separation O
between O
latent O
variables O
and O
sensitive O
variables O
by O
using O
factorized O
priors O
. O
However O
, O
some O
dependencies O
may O
still O
remain O
when O
mapping O
data O
- O
cases O
to O
their O
hidden Method
representation Method
using O
the O
variational O
posterior O
, O
which O
we O
stamp O
out O
using O
a O
“ O
Maximum Method
Mean Method
Discrepancy Method
” O
gretton2006kernel O
term O
that O
penalizes O
differences O
between O
all O
order O
moments O
of O
the O
marginal O
posterior O
distributions O
and O
( O
for O
a O
discrete O
RV O
) O
. O
In O
experiments O
we O
show O
that O
this O
combined O
approach O
is O
highly O
successful O
in O
learning O
representations Task
that O
are O
devoid O
of O
unwanted O
information O
while O
retaining O
as O
much O
information O
as O
possible O
from O
what O
remains O
. O
section O
: O
Learning Task
Invariant Task
Representations Task
[ O
scale=0.75 O
] O
unsupervised.pdf O
[ O
scale=0.75 O
] O
semisupervised.pdf O
subsection O
: O
Unsupervised Method
model Method
Factoring O
out O
undesired O
variations O
from O
the O
data O
can O
be O
easily O
formulated O
as O
a O
general O
probabilistic Method
model Method
which O
admits O
two O
distinct O
( O
independent O
) O
“ O
sources O
” O
; O
an O
observed O
variable O
, O
which O
denotes O
the O
variations O
that O
we O
want O
to O
remove O
, O
and O
a O
continuous O
latent O
variable O
which O
models O
all O
the O
remaining O
information O
. O
This O
generative Method
process Method
can O
be O
formally O
defined O
as O
: O
where O
is O
an O
appropriate O
probability O
distribution O
for O
the O
data O
we O
are O
modelling O
. O
With O
this O
formulation O
we O
explicitly O
encode O
a O
notion O
of O
‘ O
invariance O
’ O
in O
our O
model O
, O
since O
the O
latent Method
representation Method
is O
marginally O
independent O
of O
the O
factors O
of O
variation O
. O
Therefore O
the O
problem O
of O
finding O
an O
invariant Method
representation Method
for O
a O
data O
point O
and O
variation O
can O
be O
cast O
as O
performing O
inference Task
on O
this O
graphical Method
model Method
and O
obtaining O
the O
posterior O
distribution O
of O
, O
. O
For O
our O
model O
we O
will O
employ O
a O
variational Method
autoencoder Method
architecture Method
kingma2013auto O
, O
rezende2014stochastic O
; O
namely O
we O
will O
parametrize O
the O
generative Method
model Method
( O
decoder Method
) O
and O
the O
variational Method
posterior Method
( O
encoder Method
) O
as O
( O
deep O
) O
neural Method
networks Method
which O
accept O
as O
inputs O
and O
respectively O
and O
produce O
the O
parameters O
of O
each O
distribution O
after O
a O
series O
of O
non O
- O
linear O
transformations O
. O
Both O
the O
model O
( O
) O
and O
variational Method
( Method
) O
parameters O
will O
be O
jointly O
optimized O
with O
the O
SGVB Method
kingma2013auto O
algorithm O
according O
to O
a O
lower O
bound O
on O
the O
log Metric
- Metric
likelihood Metric
. O
This O
parametrization O
will O
allow O
us O
to O
capture O
most O
of O
the O
salient O
information O
of O
in O
our O
embedding O
. O
Furthermore O
the O
distributed Method
representation Method
of O
a O
neural Method
network Method
would O
allow O
us O
to O
better O
resolve O
the O
dependencies O
between O
and O
thus O
yielding O
a O
better O
disentangling O
between O
the O
independent O
factors O
and O
. O
By O
choosing O
a O
Gaussian Method
posterior Method
and O
standard O
isotropic Method
Gaussian Method
prior Method
we O
can O
obtain O
the O
following O
lower O
bound O
: O
with O
and O
with O
being O
an O
appropriate O
probability O
distribution O
for O
the O
data O
we O
are O
modelling O
. O
subsection O
: O
Semi Method
- Method
Supervised Method
model Method
Factoring O
out O
variations O
in O
an O
unsupervised O
way O
can O
however O
be O
harmful O
in O
cases O
where O
we O
want O
to O
use O
this O
invariant Method
representation Method
for O
a O
subsequent O
prediction Task
task Task
. O
In O
particular O
if O
we O
have O
a O
situation O
where O
the O
nuisance O
variable O
and O
the O
actual O
label O
are O
correlated O
, O
then O
training O
an O
unsupervised Method
model Method
could O
yield O
random O
or O
degenerate O
representations O
with O
respect O
to O
. O
Therefore O
it O
is O
more O
appropriate O
to O
try O
to O
“ O
inject O
” O
the O
information O
about O
the O
label O
during O
the O
feature Method
extraction Method
phase Method
. O
This O
can O
be O
quite O
simply O
achieved O
by O
introducing O
a O
second O
“ O
layer O
” O
of O
latent O
variables O
to O
our O
generative Method
model Method
where O
we O
try O
to O
correlate O
with O
the O
prediction Task
task Task
. O
Assuming O
that O
the O
invariant O
features O
are O
now O
called O
we O
enrich O
the O
generative O
story O
by O
similarly O
providing O
two O
distinct O
( O
independent O
) O
sources O
for O
; O
a O
discrete O
( O
in O
case O
of O
classification Task
) Task
variable Task
which O
denotes O
the O
label O
of O
the O
data O
point O
and O
a O
continuous O
latent O
variable O
which O
encodes O
the O
variation O
on O
that O
is O
not O
explained O
by O
( O
dependent O
noise O
) O
. O
The O
process O
now O
can O
be O
formally O
defined O
as O
: O
Similarly O
to O
the O
unsupervised Task
case Task
we O
use O
a O
variational Method
auto Method
- Method
encoder Method
and O
jointly O
optimize O
the O
variational O
and O
model O
parameters O
. O
The O
lower O
bound O
now O
becomes O
: O
where O
we O
assume O
that O
the O
posterior O
is O
factorized O
as O
, O
and O
where O
: O
with O
again O
being O
an O
appropriate O
probability O
distribution O
for O
the O
data O
we O
are O
modelling O
. O
The O
model O
proposed O
here O
can O
be O
seen O
as O
an O
extension O
to O
the O
‘ O
stacked Method
M1 Method
+ Method
M2 Method
’ Method
model Method
originally O
proposed O
from O
, O
where O
we O
have O
additionally O
introduced O
the O
nuisance O
variable O
during O
the O
feature Method
extraction Method
. O
Thus O
following O
we O
can O
also O
handle O
the O
‘ O
semi Task
- Task
supervised Task
’ Task
case Task
, O
i.e. O
, O
missing O
labels O
. O
In O
situations O
where O
the O
label O
is O
observed O
the O
lower O
bound O
takes O
the O
following O
form O
( O
exploiting O
the O
fact O
that O
we O
can O
compute O
some O
Kullback O
- O
Leibler O
divergences O
explicitly O
in O
our O
case O
) O
: O
and O
in O
the O
case O
that O
it O
is O
not O
observed O
we O
use O
to O
‘ O
impute O
’ O
our O
data O
: O
therefore O
the O
final O
objective Metric
function Metric
is O
: O
where O
the O
last O
term O
is O
introduced O
so O
as O
to O
ensure O
that O
the O
predictive O
posterior O
learns O
from O
both O
labeled O
and O
unlabeled O
data O
. O
This O
semi Method
- Method
supervised Method
model Method
will O
be O
called O
“ O
VAE Method
” O
in O
our O
experiments O
. O
However O
, O
there O
is O
a O
subtle O
difference O
between O
the O
approach O
of O
and O
our O
model O
. O
Instead O
of O
training O
separately O
each O
layer O
of O
stochastic O
variables O
we O
optimize O
the O
model O
jointly O
. O
The O
potential O
advantages O
of O
this O
approach O
are O
two O
fold O
: O
as O
we O
previously O
mentioned O
if O
the O
label O
and O
the O
nuisance O
information O
are O
correlated O
then O
training O
a O
( O
conditional Method
) Method
feature Method
extractor Method
separately O
poses O
the O
danger O
of O
creating O
a O
degenerate Method
representation Method
with O
respect O
to O
the O
label O
. O
Furthermore O
the O
label O
information O
will O
also O
better O
guide O
the O
feature Method
extraction Method
towards O
the O
more O
salient O
parts O
of O
the O
data O
, O
thus O
maintaining O
most O
of O
the O
( O
predictive O
) O
information O
. O
subsection O
: O
Further O
invariance O
via O
Maximum Method
Mean Method
Discrepancy Method
Despite O
the O
fact O
that O
we O
have O
a O
model O
that O
encourages O
statistical O
independence O
between O
and O
a O
- O
priori O
we O
might O
still O
have O
some O
dependence O
in O
the O
( O
approximate O
) O
marginal O
posterior O
. O
In O
particular O
, O
this O
can O
happen O
if O
the O
label O
is O
correlated O
with O
the O
sensitive O
variable O
, O
which O
can O
allow O
information O
about O
to O
“ O
leak O
” O
into O
the O
posterior O
. O
Thus O
instead O
we O
could O
maximize O
a O
“ O
penalized Metric
” Metric
lower Metric
bound Metric
where O
we O
impose O
some O
sort O
of O
regularization O
on O
the O
marginal O
. O
In O
the O
following O
we O
will O
describe O
one O
way O
to O
achieve O
this O
regularization O
through O
the O
Maximum Method
Mean Method
Discrepancy Method
( O
MMD Method
) O
gretton2006kernel O
measure O
. O
subsubsection Method
: O
Maximum Method
Mean Method
Discrepancy Method
Consider O
the O
problem O
of O
determining O
whether O
two O
datasets O
and O
are O
drawn O
from O
the O
same O
distribution O
, O
i.e. O
, O
. O
A O
simple O
test O
is O
to O
consider O
the O
distance O
between O
empirical O
statistics O
of O
the O
two O
datasets O
: O
Expanding O
the O
square O
yields O
an O
estimator O
composed O
only O
of O
inner O
products O
on O
which O
the O
kernel Method
trick Method
can O
be O
applied O
. O
The O
resulting O
estimator O
is O
known O
as O
Maximum Method
Mean Method
Discrepancy Method
( O
MMD Method
) O
gretton2006kernel O
: O
Asymptotically O
, O
for O
a O
universal Method
kernel Method
such O
as O
the O
Gaussian Method
kernel Method
, O
is O
if O
and O
only O
if O
. O
Equivalently O
, O
minimizing O
MMD Method
can O
be O
viewed O
as O
matching O
all O
of O
the O
moments O
of O
and O
. O
Therefore O
, O
we O
can O
use O
it O
as O
an O
extra O
“ O
regularizer O
” O
and O
force O
the O
model O
to O
try O
to O
match O
the O
moments O
between O
the O
marginal O
posterior O
distributions O
of O
our O
latent O
variables O
, O
i.e. O
, O
and O
( O
in O
the O
case O
of O
binary O
nuisance O
information O
) O
. O
By O
adding O
the O
MMD Method
penalty O
into O
the O
lower O
bound O
of O
our O
aforementioned O
VAE Method
architecture O
we O
obtain O
our O
proposed O
model O
, O
the O
“ O
Variational Method
Fair Method
Autoencoder Method
” O
( O
VFAE Method
) O
: O
where O
: O
subsection O
: O
Fast O
MMD Method
via O
Random Method
Fourier Method
Features Method
A O
naive O
implementation O
of O
MMD Method
in O
minibatch Method
stochastic Method
gradient Method
descent Method
would O
require O
computing O
the O
Gram O
matrix O
for O
each O
minibatch O
during O
training O
, O
where O
is O
the O
minibatch O
size O
. O
Instead O
, O
we O
can O
use O
random Method
kitchen Method
sinks Method
rahimi2009weighted O
to O
compute O
a O
feature Method
expansion Method
such O
that O
computing O
the O
estimator O
approximates O
the O
full O
MMD Method
( O
[ O
reference O
] O
) O
. O
To O
compute O
this O
, O
we O
draw O
a O
random O
matrix O
, O
where O
is O
the O
dimensionality O
of O
, O
is O
the O
number O
of O
random O
features O
and O
each O
entry O
of O
is O
drawn O
from O
a O
standard O
isotropic Method
Gaussian Method
. O
The O
feature Method
expansion Method
is O
then O
given O
as O
: O
where O
is O
a O
- O
dimensional O
uniform O
random O
vector O
with O
entries O
in O
. O
have O
successfully O
applied O
the O
idea O
of O
using O
random Method
kitchen Method
sinks Method
to O
approximate O
MMD Method
. O
This O
estimator O
is O
fairly O
accurate O
, O
and O
is O
typically O
much O
faster O
than O
the O
full O
MMD Method
penalty O
. O
We O
use O
in O
our O
experiments O
. O
section O
: O
Experiments O
We O
performed O
experiments O
on O
the O
three O
datasets O
that O
correspond O
to O
a O
“ O
fair O
” O
classification Task
scenario Task
and O
were O
previously O
used O
by O
. O
In O
these O
datasets O
the O
“ O
nuisance O
” O
or O
sensitive O
variable O
is O
significantly O
correlated O
with O
the O
label O
thus O
making O
the O
proper O
removal Task
of Task
challenging Task
. O
Furthermore O
, O
we O
also O
experimented O
with O
the O
Amazon Material
reviews Material
dataset Material
to O
make O
a O
connection O
with O
the O
“ O
domain Material
- Material
adaptation Material
” Material
literature Material
. O
Finally O
, O
we O
also O
experimented O
with O
a O
more O
general O
task O
on O
the O
extended Material
Yale Material
B Material
dataset Material
; O
that O
of O
learning Task
invariant Task
representations Task
. O
subsection O
: O
Datasets O
For O
the O
fairness Task
task Task
we O
experimented O
with O
three O
datasets O
that O
were O
previously O
used O
by O
zemel2013learning O
. O
The O
German Material
dataset Material
is O
the O
smallest O
one O
with O
data O
points O
and O
the O
objective O
is O
to O
predict O
whether O
a O
person O
has O
a O
good O
or O
bad O
credit O
rating O
. O
The O
sensitive O
variable O
is O
the O
gender O
of O
the O
individual O
. O
The O
Adult Material
income Material
dataset Material
contains O
entries O
and O
describes O
whether O
an O
account O
holder O
has O
over O
dollars O
in O
their O
account O
. O
The O
sensitive O
variable O
is O
age O
. O
Both O
of O
these O
are O
obtained O
from O
the O
UCI Material
machine Material
learning Material
repository Material
UCI Material
. O
The O
health Material
dataset Material
is O
derived O
from O
the O
Heritage O
Health Material
Prize O
. O
It O
is O
the O
largest O
of O
the O
three O
datasets O
with O
entries O
. O
The O
task O
is O
to O
predict O
whether O
a O
patient O
will O
spend O
any O
days O
in O
the O
hospital O
in O
the O
next O
year O
and O
the O
sensitive O
variable O
is O
the O
age O
of O
the O
individual O
. O
We O
use O
the O
same O
train O
/ O
test O
/ O
validation O
splits O
as O
zemel2013learning O
for O
our O
experiments O
. O
Finally O
we O
also O
binarized O
the O
data O
and O
used O
a O
multivariate Method
Bernoulli Method
distribution Method
for O
, O
where O
is O
the O
sigmoid O
function O
. O
For O
the O
domain Task
adaptation Task
task Task
we O
used O
the O
Amazon Material
reviews Material
dataset Material
( O
with O
similar O
preprocessing O
) O
that O
was O
also O
employed O
by O
chen2012marginalized O
and O
2015arXiv150507818G. O
It O
is O
composed O
from O
text Material
reviews Material
about O
particular O
products O
, O
where O
each O
product O
belongs O
to O
one O
out O
of O
four O
different O
domains O
: O
“ O
books O
” O
, O
“ O
dvd O
” O
, O
“ O
electronics O
” O
and O
“ O
kitchen O
” O
. O
As O
a O
result O
we O
performed O
twelve O
domain Task
adaptation Task
tasks Task
. O
The O
labels O
correspond O
to O
the O
sentiment O
of O
each O
review O
, O
i.e. O
either O
positive O
or O
negative O
. O
Since O
each O
feature O
vector O
is O
composed O
from O
counts O
of O
unigrams O
and O
bigrams Method
we O
used O
a O
Poisson Method
distribution Method
for O
. O
It O
is O
also O
worthwhile O
to O
mention O
that O
we O
can O
fully O
exploit O
the O
semi O
- O
supervised O
nature O
of O
our O
model O
in O
this O
dataset O
, O
and O
thus O
for O
training O
we O
only O
use O
the O
source O
domain O
labels O
and O
consider O
the O
labels O
of O
the O
target O
domain O
as O
“ O
missing O
” O
. O
For O
the O
general O
task O
of O
learning Task
invariant Task
representations Task
we O
used O
the O
Extended Material
Yale Material
B Material
dataset Material
, O
which O
was O
also O
employed O
in O
a O
similar O
fashion O
by O
li2014learning O
. O
It O
is O
composed O
from O
face O
images O
of O
38 O
people O
under O
different O
lighting O
conditions O
( O
directions O
of O
the O
light O
source O
) O
. O
Similarly O
to O
li2014learning O
, O
we O
created O
5 O
states O
for O
the O
nuisance O
variable O
: O
light O
source O
in O
upper O
right O
, O
lower O
right O
, O
lower O
left O
, O
upper O
left O
and O
the O
front O
. O
The O
labels O
correspond O
to O
the O
identity O
of O
the O
person O
. O
Following O
li2014learning O
, O
we O
used O
the O
same O
training O
, O
test O
set O
and O
no O
validation O
set O
. O
For O
the O
distribution O
we O
used O
a O
Gaussian Method
with O
means O
constrained O
in O
the O
0 O
- O
1 O
range O
( O
since O
we O
have O
intensity O
images O
) O
by O
a O
sigmoid Method
, O
i.e. O
. O
subsection O
: O
Experimental O
Setup O
For O
the O
Adult Material
dataset Material
both O
encoders Method
, O
for O
and O
, O
and O
both O
decoders Method
, O
for O
and O
, O
had O
one O
hidden Method
layer Method
of O
100 O
units O
. O
For O
the O
Health Material
dataset Material
we O
had O
one O
hidden O
layer O
of O
300 O
units O
for O
the O
encoder Method
and Method
decoder Method
and O
one O
hidden Method
layer Method
of O
150 O
units O
for O
the O
encoder Method
and Method
decoder Method
. O
For O
the O
much O
smaller O
German Material
dataset Material
we O
used O
60 O
hidden O
units O
for O
both O
encoders Method
and O
decoders Method
. O
Finally O
, O
for O
the O
Amazon Material
reviews Material
and O
Extended Material
Yale Material
B Material
datasets Material
we O
had O
one O
hidden Method
layer Method
with O
500 O
, O
400 O
units O
respectively O
for O
the O
encoder Method
, O
decoder Method
, O
and O
300 O
, O
100 O
units O
respectively O
for O
the O
encoder Method
and Method
decoder Method
. O
On O
all O
of O
the O
datasets O
we O
used O
50 O
latent O
dimensions O
for O
and O
, O
except O
for O
the O
small Material
German Material
dataset Material
, O
where O
we O
used O
30 O
latent O
dimensions O
for O
both O
variables O
. O
For O
the O
predictive Task
posterior Task
we O
used O
a O
simple O
Logistic Method
regression Method
classifier Method
. O
Optimization Task
of O
the O
objective Metric
function Metric
was O
done O
with O
Adam O
DBLP Method
: O
journals O
/ O
corr O
/ O
KingmaB14 O
using O
the O
default O
values O
for O
the O
hyperparameters O
, O
minibatches O
of O
100 O
data O
points O
and O
temporal Method
averaging Method
. O
The O
MMD Method
penalty O
was O
simply O
multiplied O
by O
the O
minibatch O
size O
so O
as O
to O
keep O
the O
scale O
of O
the O
penalty O
similar O
to O
the O
lower O
bound O
. O
Furthermore O
, O
the O
extra O
strength O
of O
the O
MMD Method
, O
, O
was O
tuned O
according O
to O
a O
validation O
set O
. O
The O
scaling O
of O
the O
supervised Metric
cost Metric
was O
low O
( O
) O
for O
the O
Adult Material
, O
Health Material
and O
German Material
datasets Material
due O
to O
the O
correlation O
of O
with O
. O
On O
the O
Amazon Material
reviews Material
and O
Extended Material
Yale Material
B Material
datasets Material
however O
the O
scaling O
of O
the O
supervised Metric
cost Metric
was O
higher O
: O
for O
the O
Amazon Material
reviews Material
dataset Material
( O
empirically O
determined O
after O
observing O
the O
classification Metric
loss Metric
on O
the O
first O
few O
iterations O
on O
the O
first O
source O
- O
target O
pair O
) O
and O
for O
the O
Extended Material
Yale Material
B Material
dataset Material
. O
Similarly O
, O
the O
scaling O
of O
the O
MMD Method
penalty O
was O
for O
the O
Amazon Material
reviews Material
dataset Material
and O
for O
the O
Extended Material
Yale Material
B. Material
Our O
evaluation O
is O
geared O
towards O
two O
fronts O
; O
removing O
information O
about O
and O
classification Metric
accuracy Metric
for O
. O
To O
measure O
the O
information O
about O
in O
our O
new O
representation O
we O
simply O
train O
a O
classifier Method
to O
predict O
from O
. O
We O
utilize O
both O
Logistic Method
Regression Method
( O
LR Method
) O
which O
is O
a O
simple O
linear Method
classifier Method
, O
and O
Random Method
Forest Method
( O
RF Method
) O
which O
is O
a O
powerful O
non Method
- Method
linear Method
classifier Method
. O
Since O
on O
the O
datasets O
that O
we O
experimented O
with O
the O
nuisance O
variable O
is O
binary O
we O
can O
easily O
find O
the O
random Metric
chance Metric
accuracy Metric
for O
and O
measure O
the O
discriminatory Metric
information Metric
of Metric
in Metric
. O
Furthermore O
, O
we O
also O
used O
the O
discrimination Method
metric Method
from O
as O
well O
a O
more O
“ O
informed O
” O
version O
of O
the O
discrimination Method
metric Method
that O
instead O
of O
the O
predictions O
, O
takes O
into O
account O
the O
probabilities O
of O
the O
correct O
class O
. O
They O
are O
provided O
in O
the O
appendix O
A. O
Finally O
, O
for O
the O
classification Task
performance O
on O
we O
used O
the O
predictive O
posterior O
for O
the O
VAE Method
/ O
VFAE Method
and O
a O
simple O
Logistic Method
Regression Method
for O
the O
original O
representations O
. O
It O
should O
be O
noted O
that O
for O
the O
VFAE Method
and O
VAE Method
models O
we O
use O
a O
sample O
from O
to O
make O
predictions O
, O
instead O
of O
using O
the O
mean O
. O
We O
found O
that O
the O
extra O
noise O
helps O
with O
invariance O
. O
We O
implemented O
the O
Learning Method
Fair Method
Representations Method
zemel2013learning O
method O
( O
LFR Method
) O
as O
a O
baseline O
using O
dimensions O
for O
the O
latent O
space O
. O
To O
measure O
the O
accuracy Metric
on O
in O
the O
results O
below O
we O
similarly O
used O
the O
LFR Method
model Method
predictions Method
. O
subsection O
: O
Results O
subsubsection O
: O
Fair Task
classification Task
The O
results O
for O
all O
three O
datasets O
can O
be O
seen O
in O
Figure O
[ O
reference O
] O
. O
Since O
we O
are O
dealing O
with O
the O
“ O
fair Task
” Task
classification Task
scenario Task
here O
, O
low Metric
accuracy Metric
and O
discrimination Metric
against Metric
is O
more O
important O
than O
the O
accuracy Metric
on O
( O
as O
long O
as O
we O
do O
not O
produce O
degenerate O
representations O
) O
. O
.329 O
[ O
width=1.1 O
] O
adult_s.pdf O
.329 O
[ O
width=1.1 O
] O
adult_discr.pdf O
.329 O
[ O
width=1.1 O
] O
adult_y.pdf O
.329 O
[ O
width=1.1 O
] O
german_s.pdf O
.329 O
[ O
width=1.1 O
] O
german_discr.pdf O
.329 O
[ O
width=1.1 O
] O
german_y.pdf O
.329 O
[ O
width=1.1 O
] O
health_s.pdf O
.329 O
[ O
width=1.1 O
] O
health_discr.pdf O
.329 O
[ O
width=1.1 O
] O
health_y.pdf O
On O
the O
Adult Material
dataset Material
, O
the O
highest O
accuracy Metric
on O
the O
label O
and O
the O
lowest O
discrimination Metric
against O
is O
obtained O
by O
our O
LFR Method
baseline Method
. O
Despite O
the O
fact O
that O
LFR Method
appears O
to O
give O
the O
best O
tradeoff O
between O
accuracy Metric
and O
discrimination Task
, O
it O
appears O
to O
retain O
information O
about O
in O
its O
representation O
, O
which O
is O
discovered O
from O
the O
random Method
forest Method
classifier Method
. O
In O
that O
sense O
, O
the O
VFAE Method
method O
appears O
to O
do O
the O
best O
job O
in O
actually O
removing O
the O
sensitive O
information O
and O
maintaining O
most O
of O
the O
predictive O
information O
. O
Furthermore O
, O
the O
introduction O
of O
the O
MMD Method
penalty O
in O
the O
VFAE Method
model O
seems O
to O
provide O
a O
significant O
benefit O
with O
respect O
to O
our O
discrimination Metric
metrics Metric
, O
as O
both O
were O
reduced O
considerably O
compared O
to O
the O
regular O
VAE Method
. O
On O
the O
German Material
dataset Material
, O
all O
methods O
appear O
to O
be O
invariant O
with O
respect O
to O
the O
sensitive O
information O
. O
However O
this O
is O
not O
the O
case O
for O
the O
discrimination Metric
metric Metric
, O
since O
LFR Method
does O
appear O
to O
retain O
information O
compared O
to O
the O
VAE Method
and O
VFAE Method
. O
The O
MMD Method
penalty O
in O
VFAE Method
did O
seem O
improve O
the O
discrimination Metric
scores Metric
over O
the O
original O
VAE Method
, O
while O
the O
accuracy Metric
on O
the O
labels O
remained O
similar O
. O
As O
for O
the O
Health Material
dataset Material
; O
this O
dataset O
is O
extremely O
imbalanced O
, O
with O
only O
15 O
% O
of O
the O
patients O
being O
admitted O
to O
a O
hospital O
. O
Therefore O
, O
each O
of O
the O
classifiers Method
seems O
to O
predict O
the O
majority O
class O
as O
the O
label O
for O
every O
point O
. O
For O
the O
invariance O
against O
however O
, O
the O
results O
were O
more O
interesting O
. O
On O
the O
one O
hand O
, O
the O
VAE Method
model O
on O
this O
dataset O
did O
maintain O
some O
sensitive O
information O
, O
which O
could O
be O
identified O
both O
linearly O
and O
non O
- O
linearly O
. O
On O
the O
other O
hand O
, O
VFAE Method
and O
the O
LFR Method
methods Method
were O
able O
to O
retain O
less O
information O
in O
their O
latent Method
representation Method
, O
since O
only O
Random Method
Forest Method
was O
able O
to O
achieve O
higher O
than O
random Metric
chance Metric
accuracy Metric
. O
This O
further O
justifies O
our O
choice O
for O
including O
the O
MMD Method
penalty O
in O
the O
lower Metric
bound Metric
of O
the O
VAE Method
. O
. O
In O
order O
to O
further O
assess O
the O
nature O
of O
our O
new O
representations O
, O
we O
visualized O
two O
dimensional O
Barnes O
- O
Hut O
SNE O
2013arXiv1301.3342V O
embeddings O
of O
the O
representations O
, O
obtained O
from O
the O
model O
trained O
on O
the O
Adult Material
dataset Material
, O
in O
Figure O
[ O
reference O
] O
. O
As O
we O
can O
see O
, O
the O
nuisance O
/ O
sensitive O
variables O
can O
be O
identified O
both O
on O
the O
original O
representation O
and O
on O
a O
latent Method
representation Method
that O
does O
not O
have O
the O
MMD Method
penalty O
and O
the O
independence O
properties O
between O
and O
in O
the O
prior O
. O
By O
introducing O
these O
independence O
properties O
as O
well O
as O
the O
MMD Method
penalty O
the O
nuisance O
variable O
groups O
become O
practically O
indistinguishable O
. O
.25 O
[ O
width=1. O
] O
tsne_adult_x.pdf O
.25 O
[ O
width=1. O
] O
tsne_adult.pdf O
.25 O
[ O
width=1. O
] O
tsne_adult_s.pdf O
.25 O
[ O
width=1. O
] O
tsne_adult_mmd_s.pdf O
subsubsection Method
: O
Domain Method
adaptation Method
As O
for O
the O
domain Task
adaptation Task
scenario Task
and O
the O
Amazon Material
reviews Material
dataset Material
, O
the O
results O
of O
our O
VFAE Method
model O
can O
be O
seen O
in O
Table O
[ O
reference O
] O
. O
Our O
model O
was O
successful O
in O
factoring O
out O
the O
domain O
information O
, O
since O
the O
accuracy Metric
, O
measured O
both O
linearly O
( O
LR Method
) O
and O
non O
- O
linearly O
( O
RF Method
) O
, O
was O
towards O
random O
chance O
( O
which O
for O
this O
dataset O
is O
0.5 O
) O
. O
We O
should O
also O
mention O
that O
, O
on O
this O
dataset O
at O
least O
, O
completely O
removing O
information O
about O
the O
domain O
does O
not O
guarantee O
a O
better O
performance O
on O
. O
The O
same O
effect O
was O
also O
observed O
by O
and O
. O
As O
far O
as O
the O
accuracy Metric
on O
is O
concerned O
, O
we O
compared O
against O
a O
recent O
neural Method
network Method
based O
state O
of O
the O
art O
method O
for O
domain Task
adaptation Task
, O
Domain Method
Adversarial Method
Neural Method
Network Method
( O
DANN Method
) O
2015arXiv150507818G. O
As O
we O
can O
observe O
in O
table O
[ O
reference O
] O
, O
our O
accuracy Metric
on O
the O
labels O
is O
higher O
on O
9 O
out O
of O
the O
12 O
domain Task
adaptation Task
tasks Task
whereas O
on O
the O
remaining O
3 O
it O
is O
quite O
similar O
to O
the O
DANN Method
architecture Method
. O
subsection O
: O
Learning Task
Invariant Task
Representations Task
Regarding O
the O
more O
general O
task O
of O
learning Task
invariant Task
representations Task
; O
our O
results O
on O
the O
Extended Material
Yale Material
B Material
dataset Material
also O
demonstrate O
our O
model O
’s O
ability O
to O
learn O
such O
representations O
. O
As O
expected O
, O
on O
the O
original O
representation O
the O
lighting O
conditions O
, O
, O
are O
well O
identifiable O
with O
almost O
perfect O
accuracy Metric
from O
both O
RF Method
and O
LR Method
. O
This O
can O
also O
be O
seen O
in O
the O
two O
dimensional O
embeddings O
of O
the O
original O
space O
in O
Figure O
[ O
reference O
] O
: O
the O
images O
are O
mostly O
clustered O
according O
to O
the O
lighting O
conditions O
. O
As O
soon O
as O
we O
utilize O
our O
VFAE Method
model O
we O
simultaneously O
decrease O
the O
accuracy Metric
on O
, O
from O
96 O
% O
to O
about O
50 O
% O
, O
and O
increase O
our O
accuracy Metric
on O
, O
from O
78 O
% O
to O
about O
85 O
% O
. O
This O
effect O
can O
also O
be O
seen O
in O
Figure O
[ O
reference O
] O
: O
the O
images O
are O
now O
mostly O
clustered O
according O
to O
the O
person O
ID O
( O
the O
label O
) O
. O
It O
is O
clear O
that O
in O
this O
scenario O
the O
information O
about O
is O
purely O
“ O
nuisance O
” O
with O
respect O
to O
the O
labels O
. O
Therefore O
, O
by O
using O
our O
VFAE Method
model O
we O
are O
able O
to O
obtain O
improved O
generalization Metric
and O
classification Task
performance O
by O
effectively O
removing O
from O
our O
representations O
. O
.49 O
[ O
width= O
] O
yaleb_x.pdf O
.49 O
[ O
width= O
] O
yaleb_z.pdf O
section O
: O
Related O
Work O
Most O
related O
to O
our O
“ O
fair O
” O
representations O
view O
is O
the O
work O
from O
. O
They O
proposed O
a O
neural Method
network Method
based Method
semi Method
- Method
supervised Method
clustering Method
model Method
for O
learning Task
fair Task
representations Task
. O
The O
idea O
is O
to O
learn O
a O
localised Method
representation Method
that O
maps O
each O
datapoint O
to O
a O
cluster O
in O
such O
a O
way O
that O
each O
cluster O
gets O
assigned O
roughly O
equal O
proportions O
of O
data O
from O
each O
group O
in O
. O
Although O
their O
approach O
was O
successfully O
applied O
on O
several O
datasets O
, O
the O
restriction O
to O
clustering Task
means O
that O
it O
can O
not O
leverage O
the O
representational O
power O
of O
a O
distributed Method
representation Method
. O
Furthermore O
, O
this O
penalty O
does O
not O
account O
for O
higher O
order O
moments O
in O
the O
latent O
distribution O
. O
For O
example O
, O
if O
always O
returns O
or O
, O
while O
returns O
values O
between O
values O
and O
, O
then O
the O
penalty O
could O
still O
be O
satisfied O
, O
but O
information O
could O
still O
leak O
through O
. O
We O
addressed O
both O
of O
these O
issues O
in O
this O
paper O
. O
Domain Task
adaptation Task
can O
also O
be O
cast O
as O
learning Method
representations Method
that O
are O
“ O
invariant O
” O
with O
respect O
to O
a O
discrete O
variable O
, O
the O
domain O
. O
Most O
similar O
to O
our O
work O
are O
neural Method
network Method
approaches Method
which O
try O
to O
match O
the O
feature O
distributions O
between O
the O
domains O
. O
This O
was O
performed O
in O
an O
unsupervised O
way O
with O
mSDA Method
chen2012marginalized O
by O
training O
denoising Method
autoencoders Method
jointly O
on O
all O
domains O
, O
thus O
implicitly O
obtaining O
a O
representation O
general O
enough O
to O
explain O
both O
the O
domain O
and O
the O
data O
. O
This O
is O
in O
contrast O
to O
our O
approach O
where O
we O
instead O
try O
to O
learn O
representations O
that O
explicitly O
remove O
domain O
information O
during O
the O
learning Method
process Method
. O
For O
the O
latter O
we O
find O
more O
similarities O
with O
“ O
domain Method
- Method
regularized Method
” Method
supervised Method
approaches Method
that O
simultaneously O
try O
to O
predict O
the O
label O
for O
a O
data O
point O
and O
remove O
domain O
specific O
information O
. O
This O
is O
done O
with O
either O
MMD Method
long2015learning O
, O
DBLP Method
: O
journals O
/ O
corr O
/ O
TzengHZSD14 O
or O
adversarial O
2015arXiv150507818 O
G O
penalties O
at O
the O
hidden O
layers O
of O
the O
network O
. O
In O
our O
model O
however O
the O
main O
“ O
domain O
- O
regularizer O
” O
stems O
from O
the O
independence O
properties O
of O
the O
prior O
over O
the O
domain O
and O
latent Method
representations Method
. O
We O
also O
employ O
MMD Method
on O
our O
model O
but O
from O
a O
different O
perspective O
since O
we O
consider O
a O
slightly O
more O
difficult O
case O
where O
the O
domain O
and O
label O
are O
correlated O
; O
we O
need O
to O
ensure O
that O
we O
remain O
as O
“ O
invariant O
” O
as O
possible O
since O
might O
‘ O
leak O
’ O
information O
about O
. O
section O
: O
Conclusion O
We O
introduce O
the O
Variational Method
Fair Method
Autoencoder Method
( O
VFAE Method
) O
, O
an O
extension O
of O
the O
semi Method
- Method
supervised Method
variational Method
autoencoder Method
in O
order O
to O
learn O
representations O
that O
are O
explicitly O
invariant O
with O
respect O
to O
some O
known O
aspect O
of O
a O
dataset O
while O
retaining O
as O
much O
remaining O
information O
as O
possible O
. O
We O
further O
use O
a O
Maximum Method
Mean Method
Discrepancy Method
regularizer O
in O
order O
to O
further O
promote O
invariance O
in O
the O
posterior O
distribution O
over O
latent O
variables O
. O
We O
apply O
this O
model O
to O
tasks O
involving O
developing O
fair Method
classifiers Method
that O
are O
invariant O
to O
sensitive O
demographic O
information O
and O
show O
that O
it O
produces O
a O
better O
tradeoff O
with O
respect O
to O
accuracy Metric
and O
invariance Metric
. O
As O
a O
second O
application O
, O
we O
consider O
the O
task O
of O
domain Task
adaptation Task
, O
where O
the O
goal O
is O
to O
improve O
classification Task
by O
training O
a O
classifier Method
that O
is O
invariant O
to O
the O
domain O
. O
We O
find O
that O
our O
model O
is O
competitive O
with O
recently O
proposed O
adversarial Method
approaches Method
. O
Finally O
, O
we O
also O
consider O
the O
more O
general O
task O
of O
learning Task
invariant Task
representations Task
. O
We O
can O
observe O
that O
our O
model O
provides O
a O
clear O
improvement O
against O
a O
neural Method
network Method
that O
incorporates O
a O
Maximum Method
Mean Method
Discrepancy Method
penalty O
. O
bibliography O
: O
References O
appendix O
: O
Discrimination Metric
metrics Metric
The O
Discrimination Metric
metric Metric
zemel2013learning O
and O
the O
Discrimination Metric
metric Metric
that O
takes O
into O
account O
the O
probabilities O
of O
the O
correct O
class O
are O
mathematically O
formalized O
as O
: O
where O
for O
the O
predictions O
that O
were O
done O
on O
the O
datapoints O
with O
nuisance O
variable O
, O
denotes O
the O
total O
amount O
of O
datapoints O
that O
had O
nuisance O
variable O
and O
denotes O
the O
probability O
of O
the O
prediction O
for O
the O
datapoints O
with O
. O
For O
the O
predictions O
and O
their O
respective O
probabilities O
we O
used O
a O
Logistic Method
Regression Method
classifier Method
. O
appendix O
: O
Proxy Method
A Method
- Method
Distance Method
( O
PAD Method
) O
for O
Amazon Material
Reviews Material
dataset Material
Similarly O
to O
, O
we O
also O
calculated O
the O
Proxy O
A O
- O
distance O
( O
PAD Method
) O
ben2007analysis O
, O
ben2010theory O
scores O
for O
the O
raw O
data O
and O
for O
the O
representations O
of O
VFAE Method
. O
Briefly O
, O
Proxy Method
A Method
- Method
distance Method
is O
an O
approximation O
to O
the O
- Metric
divergence Metric
measure Metric
of Metric
domain Metric
distinguishability Metric
proposed O
in O
and O
. O
To O
compute O
it O
we O
first O
need O
to O
train O
a O
learning Method
algorithm Method
on O
the O
task O
of O
discriminating O
examples O
from O
the O
source O
and O
target O
domain O
. O
Afterwards O
we O
can O
use O
the O
test O
error O
of O
that O
algorithm O
in O
the O
following O
formula O
: O
It O
is O
clear O
that O
low O
PAD Metric
scores Metric
correspond O
to O
low O
discrimination O
of O
the O
source O
and O
target O
domain O
examples O
from O
the O
classifier Method
. O
To O
obtain O
for O
our O
model O
we O
used O
Logistic Method
Regression Method
. O
The O
resulting O
plot O
can O
be O
seen O
in O
Figure O
[ O
reference O
] O
, O
where O
we O
have O
also O
added O
the O
plot O
from O
DANN O
2015arXiv150507818 O
G O
, O
where O
they O
used O
a O
linear Method
Support Method
Vector Method
Machine Method
for O
the O
classifier Method
, O
as O
a O
reference O
. O
It O
can O
be O
seen O
that O
our O
VFAE Method
model O
can O
factor O
out O
the O
information O
about O
better O
, O
since O
the O
PAD Metric
scores Metric
on O
our O
new O
representation O
are O
, O
overall O
, O
lower O
than O
the O
ones O
obtained O
from O
the O
DANN Method
architecture Method
. O
.49 O
[ O
width=1. O
] O
pad_vfae_x_reviews.pdf O
.49 O
[ O
width=.9 O
] O
PAD_DANN.pdf O
