Canonical	Method
Polyadic	Method
(	O
CP	Method
)	O
decomposition	O
fails	O
on	O
both	O
relations	O
as	O
it	O
has	O
to	O
push	O
symmetric	O
and	O
antisymmetric	O
patterns	O
through	O
the	O
entity	O
embeddings	O
.	O

Surprisingly	O
,	O
only	O
our	O
model	O
succeeds	O
on	O
such	O
simple	O
data	O
.	O

subsection	O
:	O
Datasets	O
:	O
FB15	O
K	O
and	O
WN18	Material
We	O
next	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
FB15	O
K	O
and	O
WN18	Material
datasets	Material
.	O

FB15	O
K	O
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
curated	O
KB	O
of	O
general	O
facts	O
,	O
whereas	O
WN18	Material
is	O
a	O
subset	O
of	O
Wordnet	Material
,	O
a	O
database	O
featuring	O
lexical	O
relations	O
between	O
words	O
.	O

We	O
use	O
original	O
training	O
,	O
validation	O
and	O
test	O
set	O
splits	O
as	O
provided	O
by	O
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
the	O
metadata	O
of	O
the	O
two	O
datasets	O
.	O

Both	O
datasets	O
contain	O
only	O
positive	O
triples	O
.	O

As	O
in	O
,	O
we	O
generated	O
negatives	O
using	O
the	O
local	O
closed	O
world	O
assumption	O
.	O

That	O
is	O
,	O
for	O
a	O
triple	O
,	O
we	O
randomly	O
change	O
either	O
the	O
subject	O
or	O
the	O
object	O
at	O
random	O
,	O
to	O
form	O
a	O
negative	O
example	O
.	O

This	O
negative	Method
sampling	Method
is	O
performed	O
at	O
runtime	O
for	O
each	O
batch	O
of	O
training	O
positive	O
examples	O
.	O

For	O
evaluation	O
,	O
we	O
measure	O
the	O
quality	O
of	O
the	O
ranking	Metric
of	O
each	O
test	O
triple	O
among	O
all	O
possible	O
subject	O
and	O
object	O
substitutions	O
:	O
and	O
,	O
.	O

Mean	Metric
Reciprocal	Metric
Rank	Metric
(	O
MRR	Metric
)	O
and	O
Hits	Metric
at	O
are	O
the	O
standard	O
evaluation	Metric
measures	Metric
for	O
these	O
datasets	O
and	O
come	O
in	O
two	O
flavours	O
:	O
raw	O
and	O
filtered	O
.	O

The	O
filtered	Metric
metrics	Metric
are	O
computed	O
after	O
removing	O
all	O
the	O
other	O
positive	O
observed	O
triples	O
that	O
appear	O
in	O
either	O
training	O
,	O
validation	O
or	O
test	O
set	O
from	O
the	O
ranking	Task
,	O
whereas	O
the	O
raw	O
metrics	O
do	O
not	O
remove	O
these	O
.	O

Since	O
ranking	Metric
measures	Metric
are	O
used	O
,	O
previous	O
studies	O
generally	O
preferred	O
a	O
pairwise	Metric
ranking	Metric
loss	Metric
for	O
the	O
task	O
.	O

We	O
chose	O
to	O
use	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
the	O
logistic	Method
model	Method
,	O
as	O
it	O
is	O
a	O
continuous	O
surrogate	O
of	O
the	O
sign	O
-	O
rank	O
,	O
and	O
has	O
been	O
shown	O
to	O
learn	O
compact	Method
representations	Method
for	O
several	O
important	O
relations	O
,	O
especially	O
for	O
transitive	O
relations	O
.	O

In	O
preliminary	O
work	O
,	O
we	O
tried	O
both	O
losses	O
,	O
and	O
indeed	O
the	O
log	Method
-	Method
likelihood	Method
yielded	O
better	O
results	O
than	O
the	O
ranking	Metric
loss	Metric
(	O
except	O
with	O
TransE	Method
)	O
,	O
especially	O
on	O
FB15K.	O
We	O
report	O
both	O
filtered	Metric
and	O
raw	Metric
MRR	Metric
,	O
and	O
filtered	O
Hits	Metric
at	Metric
1	Metric
,	O
3	Metric
and	O
10	Metric
in	O
Table	O
[	O
reference	O
]	O
for	O
the	O
evaluated	O
models	O
.	O

Furthermore	O
,	O
we	O
chose	O
TransE	O
,	O
DistMult	Method
and	O
HolE	Method
as	O
baselines	O
since	O
they	O
are	O
the	O
best	O
performing	O
models	O
on	O
those	O
datasets	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

We	O
also	O
compare	O
with	O
the	O
CP	Method
model	O
to	O
emphasize	O
empirically	O
the	O
importance	O
of	O
learning	O
unique	O
embeddings	O
for	O
entities	O
.	O

For	O
experimental	O
fairness	O
,	O
we	O
reimplemented	O
these	O
methods	O
within	O
the	O
same	O
framework	O
as	O
the	O
ComplEx	Method
model	Method
,	O
using	O
theano	O
.	O

However	O
,	O
due	O
to	O
time	O
constraints	O
and	O
the	O
complexity	O
of	O
an	O
efficient	O
implementation	O
of	O
HolE	Method
,	O
we	O
record	O
the	O
original	O
results	O
for	O
HolE	Method
as	O
reported	O
in	O
.	O

subsection	O
:	O
Results	O
WN18	Material
describes	O
lexical	O
and	O
semantic	O
hierarchies	O
between	O
concepts	O
and	O
contains	O
many	O
antisymmetric	O
relations	O
such	O
as	O
hypernymy	O
,	O
hyponymy	O
,	O
or	O
being	O
”	O
part	O
of	O
”	O
.	O

Indeed	O
,	O
the	O
DistMult	Method
and	O
TransE	Method
models	Method
are	O
outperformed	O
here	O
by	O
ComplEx	Method
and	O
HolE	Method
,	O
which	O
are	O
on	O
par	O
with	O
respective	O
filtered	Metric
MRR	Metric
scores	Metric
of	O
0.941	O
and	O
0.938	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
filtered	O
test	O
set	O
MRR	Metric
for	O
the	O
models	O
considered	O
and	O
each	O
relation	O
of	O
WN18	Material
,	O
confirming	O
the	O
advantage	O
of	O
our	O
model	O
on	O
antisymmetric	O
relations	O
while	O
losing	O
nothing	O
on	O
the	O
others	O
.	O

2D	O
projections	O
of	O
the	O
relation	O
embeddings	O
provided	O
in	O
Appendix	O
[	O
reference	O
]	O
visually	O
corroborate	O
the	O
results	O
.	O

On	O
FB15	O
K	O
,	O
the	O
gap	O
is	O
much	O
more	O
pronounced	O
and	O
the	O
ComplEx	Method
model	Method
largely	O
outperforms	O
HolE	Method
,	O
with	O
a	O
filtered	Metric
MRR	Metric
of	O
0.692	O
and	O
59.9	O
%	O
of	O
Hits	Metric
at	Metric
1	Metric
,	O
compared	O
to	O
0.524	O
and	O
40.2	O
%	O
for	O
HolE.	Method
We	O
attribute	O
this	O
to	O
the	O
simplicity	O
of	O
our	O
model	O
and	O
the	O
different	O
loss	O
function	O
.	O

This	O
is	O
supported	O
by	O
the	O
relatively	O
small	O
gap	O
in	O
MRR	Metric
compared	O
to	O
DistMult	Method
(	O
0.654	O
)	O
;	O
our	O
model	O
can	O
in	O
fact	O
be	O
interpreted	O
as	O
a	O
complex	O
number	O
version	O
of	O
DistMult	Method
.	O

On	O
both	O
datasets	O
,	O
TransE	Method
and	O
CP	Method
are	O
largely	O
left	O
behind	O
.	O

This	O
illustrates	O
the	O
power	O
of	O
the	O
simple	O
dot	Method
product	Method
in	O
the	O
first	O
case	O
,	O
and	O
the	O
importance	O
of	O
learning	O
unique	O
entity	O
embeddings	O
in	O
the	O
second	O
.	O

CP	Method
performs	O
poorly	O
on	O
WN18	Material
due	O
to	O
the	O
small	O
number	O
of	O
relations	O
,	O
which	O
magnifies	O
this	O
subject	O
/	O
object	O
difference	O
.	O

Reported	O
results	O
are	O
given	O
for	O
the	O
best	O
set	O
of	O
hyper	O
-	O
parameters	O
evaluated	O
on	O
the	O
validation	O
set	O
for	O
each	O
model	O
,	O
after	O
grid	Method
search	Method
on	O
the	O
following	O
values	O
:	O
,	O
,	O
,	O
with	O
the	O
regularization	O
parameter	O
,	O
the	O
initial	O
learning	Metric
rate	Metric
(	O
then	O
tuned	O
at	O
runtime	O
with	O
AdaGrad	Method
)	O
,	O
and	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
triple	O
.	O

We	O
also	O
tried	O
varying	O
the	O
batch	O
size	O
but	O
this	O
had	O
no	O
impact	O
and	O
we	O
settled	O
with	O
100	O
batches	O
per	O
epoch	O
.	O

Best	O
ranks	O
were	O
generally	O
150	O
or	O
200	O
,	O
in	O
both	O
cases	O
scores	O
were	O
always	O
very	O
close	O
for	O
all	O
models	O
.	O

The	O
number	O
of	O
negative	O
samples	O
per	O
positive	O
sample	O
also	O
had	O
a	O
large	O
influence	O
on	O
the	O
filtered	O
MRR	Metric
on	O
FB15	O
K	O
(	O
up	O
to	O
+	O
0.08	O
improvement	O
from	O
1	O
to	O
10	Metric
negatives	O
)	O
,	O
but	O
not	O
much	O
on	O
WN18	Material
.	O

On	O
both	O
datasets	O
regularization	Task
was	O
important	O
(	O
up	O
to	O
+	O
0.05	O
on	O
filtered	Metric
MRR	Metric
between	O
and	O
optimal	O
one	O
)	O
.	O

We	O
found	O
the	O
initial	O
learning	Metric
rate	Metric
to	O
be	O
very	O
important	O
on	O
FB15	O
K	O
,	O
while	O
not	O
so	O
much	O
on	O
WN18	Material
.	O

We	O
think	O
this	O
may	O
also	O
explain	O
the	O
large	O
gap	O
of	O
improvement	O
our	O
model	O
provides	O
on	O
this	O
dataset	O
compared	O
to	O
previously	O
published	O
results	O
–	O
as	O
DistMult	Method
results	O
are	O
also	O
better	O
than	O
those	O
previously	O
reported	O
–	O
along	O
with	O
the	O
use	O
of	O
the	O
log	Method
-	Method
likelihood	Method
objective	Method
.	O

It	O
seems	O
that	O
in	O
general	O
AdaGrad	Method
is	O
relatively	O
insensitive	O
to	O
the	O
initial	O
learning	Metric
rate	Metric
,	O
perhaps	O
causing	O
some	O
overconfidence	O
in	O
its	O
ability	O
to	O
tune	O
the	O
step	O
size	O
online	O
and	O
consequently	O
leading	O
to	O
less	O
efforts	O
when	O
selecting	O
the	O
initial	O
step	O
size	O
.	O

Training	Task
was	O
stopped	O
using	O
early	O
stopping	O
on	O
the	O
validation	O
set	O
filtered	O
MRR	Metric
,	O
computed	O
every	O
50	O
epochs	O
with	O
a	O
maximum	O
of	O
1000	O
epochs	O
.	O

subsection	O
:	O
Influence	O
of	O
Negative	O
Samples	O
We	O
further	O
investigated	O
the	O
influence	O
of	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
sample	O
.	O

In	O
the	O
previous	O
experiment	O
,	O
due	O
to	O
computational	O
limitations	O
,	O
the	O
number	O
of	O
negatives	O
per	O
training	O
sample	O
,	O
,	O
was	O
validated	O
among	O
the	O
possible	O
numbers	O
.	O

We	O
want	O
to	O
explore	O
here	O
whether	O
increasing	O
these	O
numbers	O
could	O
lead	O
to	O
better	O
results	O
.	O

To	O
do	O
so	O
,	O
we	O
focused	O
on	O
FB15	O
K	O
,	O
with	O
the	O
best	O
validated	O
,	O
obtained	O
from	O
the	O
previous	O
experiment	O
.	O

We	O
then	O
let	O
vary	O
in	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
influence	O
of	O
the	O
number	O
of	O
generated	O
negatives	O
per	O
positive	O
training	O
triple	O
on	O
the	O
performance	O
of	O
our	O
model	O
on	O
FB15K.	O
Generating	O
more	O
negatives	O
clearly	O
improves	O
the	O
results	O
,	O
with	O
a	O
filtered	Metric
MRR	Metric
of	O
0.737	O
with	O
100	O
negative	O
triples	O
(	O
and	O
64.8	O
%	O
of	O
Hits@1	Metric
)	O
,	O
before	O
decreasing	O
again	O
with	O
200	O
negatives	O
.	O

The	O
model	O
also	O
converges	O
with	O
fewer	O
epochs	O
,	O
which	O
compensates	O
partially	O
for	O
the	O
additional	O
training	O
time	O
per	O
epoch	O
,	O
up	O
to	O
50	O
negatives	O
.	O

It	O
then	O
grows	O
linearly	O
as	O
the	O
number	O
of	O
negatives	O
increases	O
,	O
making	O
50	O
a	O
good	O
trade	O
-	O
off	O
between	O
accuracy	Metric
and	O
training	Metric
time	Metric
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
early	O
age	O
of	O
spectral	Method
theory	Method
in	O
linear	Task
algebra	Task
,	O
complex	O
numbers	O
were	O
not	O
used	O
for	O
matrix	Task
factorization	Task
and	O
mathematicians	O
mostly	O
focused	O
on	O
bi	O
-	O
linear	O
forms	O
.	O

The	O
eigen	Method
-	Method
decomposition	Method
in	O
the	O
complex	O
domain	O
as	O
taught	O
today	O
in	O
linear	Task
algebra	Task
courses	Task
came	O
40	O
years	O
later	O
.	O

Similarly	O
,	O
most	O
of	O
the	O
existing	O
approaches	O
for	O
tensor	Task
factorization	Task
were	O
based	O
on	O
decompositions	O
in	O
the	O
real	O
domain	O
,	O
such	O
as	O
the	O
Canonical	Method
Polyadic	Method
(	O
CP	Method
)	O
decomposition	O
.	O

These	O
methods	O
are	O
very	O
effective	O
in	O
many	O
applications	O
that	O
use	O
different	O
modes	O
of	O
the	O
tensor	O
for	O
different	O
types	O
of	O
entities	O
.	O

But	O
in	O
the	O
link	Task
prediction	Task
problem	Task
,	O
antisymmetry	Task
of	Task
relations	Task
was	O
quickly	O
seen	O
as	O
a	O
problem	O
and	O
asymmetric	O
extensions	O
of	O
tensors	O
were	O
studied	O
,	O
mostly	O
by	O
either	O
considering	O
independent	Method
embeddings	Method
or	O
considering	O
relations	O
as	O
matrices	O
instead	O
of	O
vectors	O
in	O
the	O
RESCAL	Method
model	Method
.	O

Direct	O
extensions	O
were	O
based	O
on	O
uni	O
-,	O
bi	O
-	O
and	O
trigram	O
latent	O
factors	O
for	O
triple	O
data	O
,	O
as	O
well	O
as	O
a	O
low	O
-	O
rank	O
relation	O
matrix	O
.	O

Pairwise	Method
interaction	Method
models	Method
were	O
also	O
considered	O
to	O
improve	O
prediction	Task
performances	O
.	O

For	O
example	O
,	O
the	O
Universal	Method
Schema	Method
approach	Method
factorizes	O
a	O
2D	Method
unfolding	Method
of	Method
the	Method
tensor	Method
(	O
a	O
matrix	O
of	O
entity	O
pairs	O
vs.	O
relations	O
)	O
while	O
extend	O
this	O
also	O
to	O
other	O
pairs	O
.	O

In	O
the	O
Neural	Method
Tensor	Method
Network	Method
(	O
NTN	Method
)	O
model	O
,	O
combine	O
linear	O
transformations	O
and	O
multiple	O
bilinear	O
forms	O
of	O
subject	O
and	O
object	O
embeddings	O
to	O
jointly	O
feed	O
them	O
into	O
a	O
nonlinear	Method
neural	Method
layer	Method
.	O

Its	O
non	O
-	O
linearity	O
and	O
multiple	O
ways	O
of	O
including	O
interactions	O
between	O
embeddings	O
gives	O
it	O
an	O
advantage	O
in	O
expressiveness	O
over	O
models	O
with	O
simpler	O
scoring	O
function	O
like	O
DistMult	Method
or	O
RESCAL	Method
.	O

As	O
a	O
downside	O
,	O
its	O
very	O
large	O
number	O
of	O
parameters	O
can	O
make	O
the	O
NTN	Method
model	O
harder	O
to	O
train	O
and	O
overfit	O
more	O
easily	O
.	O

The	O
original	O
multi	O
-	O
linear	O
DistMult	Method
model	O
is	O
symmetric	O
in	O
subject	O
and	O
object	O
for	O
every	O
relation	O
and	O
achieves	O
good	O
performance	O
,	O
presumably	O
due	O
to	O
its	O
simplicity	O
.	O

The	O
TransE	Method
model	Method
from	O
also	O
embeds	O
entities	O
and	O
relations	O
in	O
the	O
same	O
space	O
and	O
imposes	O
a	O
geometrical	O
structural	O
bias	O
into	O
the	O
model	O
:	O
the	O
subject	O
entity	O
vector	O
should	O
be	O
close	O
to	O
the	O
object	O
entity	O
vector	O
once	O
translated	O
by	O
the	O
relation	O
vector	O
.	O

A	O
recent	O
novel	O
way	O
to	O
handle	O
antisymmetry	O
is	O
via	O
the	O
Holographic	Method
Embeddings	Method
(	O
HolE	Method
)	O
model	O
by	O
.	O

In	O
HolE	Method
the	O
circular	O
correlation	O
is	O
used	O
for	O
combining	O
entity	O
embeddings	O
,	O
measuring	O
the	O
covariance	O
between	O
embeddings	O
at	O
different	O
dimension	O
shifts	O
.	O

This	O
generally	O
suggests	O
that	O
other	O
composition	O
functions	O
than	O
the	O
classical	O
tensor	Method
product	Method
can	O
be	O
helpful	O
as	O
they	O
allow	O
for	O
a	O
richer	O
interaction	O
of	O
embeddings	O
.	O

However	O
,	O
the	O
asymmetry	O
in	O
the	O
composition	O
function	O
in	O
HolE	Method
stems	O
from	O
the	O
asymmetry	O
of	O
circular	O
correlation	O
,	O
an	O
operation	O
,	O
whereas	O
ours	O
is	O
inherited	O
from	O
the	O
complex	O
inner	O
product	O
,	O
in	O
.	O

section	O
:	O
Conclusion	O
We	O
described	O
a	O
simple	O
approach	O
to	O
matrix	Task
and	Task
tensor	Task
factorization	Task
for	O
link	Task
prediction	Task
data	Task
that	O
uses	O
vectors	O
with	O
complex	O
values	O
and	O
retains	O
the	O
mathematical	O
definition	O
of	O
the	O
dot	O
product	O
.	O

The	O
class	Method
of	Method
normal	Method
matrices	Method
is	O
a	O
natural	O
fit	O
for	O
binary	O
relations	O
,	O
and	O
using	O
the	O
real	O
part	O
allows	O
for	O
efficient	O
approximation	O
of	O
any	O
learnable	O
relation	O
.	O

Results	O
on	O
standard	O
benchmarks	O
show	O
that	O
no	O
more	O
modifications	O
are	O
needed	O
to	O
improve	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

There	O
are	O
several	O
directions	O
in	O
which	O
this	O
work	O
can	O
be	O
extended	O
.	O

An	O
obvious	O
one	O
is	O
to	O
merge	O
our	O
approach	O
with	O
known	O
extensions	O
to	O
tensor	Method
factorization	Method
in	O
order	O
to	O
further	O
improve	O
predictive	Task
performance	O
.	O

For	O
example	O
,	O
the	O
use	O
of	O
pairwise	O
embeddings	O
together	O
with	O
complex	O
numbers	O
might	O
lead	O
to	O
improved	O
results	O
in	O
many	O
situations	O
that	O
involve	O
non	O
-	O
compositionality	O
.	O

Another	O
direction	O
would	O
be	O
to	O
develop	O
a	O
more	O
intelligent	O
negative	Method
sampling	Method
procedure	Method
,	O
to	O
generate	O
more	O
informative	O
negatives	O
with	O
respect	O
to	O
the	O
positive	O
sample	O
from	O
which	O
they	O
have	O
been	O
sampled	O
.	O

It	O
would	O
reduce	O
the	O
number	O
of	O
negatives	O
required	O
to	O
reach	O
good	O
performance	O
,	O
thus	O
accelerating	O
training	Metric
time	Metric
.	O

Also	O
,	O
if	O
we	O
were	O
to	O
use	O
complex	O
embeddings	O
every	O
time	O
a	O
model	O
includes	O
a	O
dot	O
product	O
,	O
e.g.	O
in	O
deep	Method
neural	Method
networks	Method
,	O
would	O
it	O
lead	O
to	O
a	O
similar	O
systematic	O
improvement	O
?	O
section	O
:	O
Acknowledgements	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
Paul	O
Allen	O
Foundation	O
through	O
an	O
Allen	O
Distinguished	O
Investigator	O
grant	O
and	O
in	O
part	O
by	O
a	O
Google	O
Focused	O
Research	O
Award	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
SGD	Method
algorithm	Method
We	O
describe	O
the	O
algorithm	O
to	O
learn	O
the	O
ComplEx	Method
model	Method
with	O
Stochastic	Method
Gradient	Method
Descent	Method
using	O
only	O
real	O
-	O
valued	O
vectors	O
.	O

Let	O
us	O
rewrite	O
equation	O
[	O
reference	O
]	O
,	O
by	O
denoting	O
the	O
real	O
part	O
of	O
embeddings	O
with	O
primes	O
and	O
the	O
imaginary	O
part	O
with	O
double	O
primes	O
:	O
,	O
,	O
,	O
.	O

The	O
set	O
of	O
parameters	O
is	O
,	O
and	O
the	O
scoring	Method
function	Method
involves	O
only	O
real	O
vectors	O
:	O
where	O
each	O
entity	O
and	O
each	O
relation	O
has	O
two	O
real	O
embeddings	O
.	O

Gradients	O
are	O
now	O
easy	O
to	O
write	O
:	O
where	O
is	O
the	O
element	O
-	O
wise	O
(	O
Hadamard	O
)	O
product	O
.	O

As	O
stated	O
in	O
equation	O
[	O
reference	O
]	O
we	O
use	O
the	O
sigmoid	O
link	O
function	O
,	O
and	O
minimize	O
the	O
-	O
regularized	O
negative	O
log	O
-	O
likelihood	O
:	O
To	O
handle	O
regularization	Task
,	O
note	O
that	O
the	O
squared	O
-	O
norm	O
of	O
a	O
complex	O
vector	O
is	O
the	O
sum	O
of	O
the	O
squared	O
modulus	O
of	O
each	O
entry	O
:	O
which	O
is	O
actually	O
the	O
sum	O
of	O
the	O
-	O
norms	O
of	O
the	O
vectors	O
of	O
the	O
real	O
and	O
imaginary	O
parts	O
.	O

We	O
can	O
finally	O
write	O
the	O
gradient	O
of	O
with	O
respect	O
to	O
a	O
real	O
embedding	O
for	O
one	O
triple	O
:	O
where	O
is	O
the	O
sigmoid	O
function	O
.	O

Algorithm	O
[	O
reference	O
]	O
describes	O
SGD	Method
for	O
this	O
formulation	O
of	O
the	O
scoring	O
function	O
.	O

When	O
contains	O
only	O
positive	O
triples	O
,	O
we	O
generate	O
negatives	O
per	O
positive	O
train	O
triple	O
,	O
by	O
corrupting	O
either	O
the	O
subject	O
or	O
the	O
object	O
of	O
the	O
positive	O
triple	O
,	O
as	O
described	O
in	O
.	O

[	O
t	O
]	O
SGD	Method
for	O
the	O
ComplEx	Method
model	Method
Training	O
set	O
,	O
Validation	O
set	O
,	O
learning	Metric
rate	Metric
,	O
embedding	O
dim	O
.	O

,	O
regularization	O
factor	O
,	O
negative	O
ratio	O
,	O
batch	O
size	O
,	O
max	O
iter	O
,	O
early	O
stopping	O
.	O

,	O
for	O
each	O
,	O
for	O
each	O
sample	O
Update	O
embeddings	O
w.r.t	O
.	O

:	O
Update	O
learning	Metric
rate	Metric
using	O
Adagrad	Method
break	O
if	O
filteredMRR	O
or	O
AP	O
on	O
decreased	O
appendix	O
:	O
WN18	Task
embeddings	Task
visualization	Task
We	O
used	O
principal	Method
component	Method
analysis	Method
(	O
PCA	Method
)	O
to	O
visualize	O
embeddings	O
of	O
the	O
relations	O
of	O
the	O
wordnet	Material
dataset	Material
(	O
WN18	Material
)	O
.	O

We	O
plotted	O
the	O
four	O
first	O
components	O
of	O
the	O
best	O
DistMult	Method
and	O
ComplEx	Method
model	Method
’s	O
embeddings	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

For	O
the	O
ComplEx	Method
model	Method
,	O
we	O
simply	O
concatenated	O
the	O
real	O
and	O
imaginary	O
parts	O
of	O
each	O
embedding	O
.	O

Most	O
of	O
WN18	Material
relations	O
describe	O
hierarchies	O
,	O
and	O
are	O
thus	O
antisymmetric	O
.	O

Each	O
of	O
these	O
hierarchic	O
relations	O
has	O
its	O
inverse	O
relation	O
in	O
the	O
dataset	O
.	O

For	O
example	O
:	O
hypernym	O
/	O
hyponym	O
,	O
part_of	O
/	O
has_part	O
,	O
synset_domain_topic_of	O
/	O
member_of_domain_topic	O
.	O

Since	O
DistMult	Method
is	O
unable	O
to	O
model	O
antisymmetry	O
,	O
it	O
will	O
correctly	O
represent	O
the	O
nature	O
of	O
each	O
pair	O
of	O
opposite	O
relations	O
,	O
but	O
not	O
the	O
direction	O
of	O
the	O
relations	O
.	O

Loosely	O
speaking	O
,	O
in	O
the	O
hypernym	O
/	O
hyponym	O
pair	O
the	O
nature	O
is	O
sharing	O
semantics	O
,	O
and	O
the	O
direction	O
is	O
that	O
one	O
entity	O
generalizes	O
the	O
semantics	O
of	O
the	O
other	O
.	O

This	O
makes	O
DistMult	Method
reprensenting	O
the	O
opposite	O
relations	O
with	O
very	O
close	O
embeddings	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
.	O

It	O
is	O
especially	O
striking	O
for	O
the	O
third	O
and	O
fourth	O
principal	O
component	O
(	O
bottom	O
-	O
left	O
)	O
.	O

Conversely	O
,	O
ComplEx	Method
manages	O
to	O
oppose	O
spatially	O
the	O
opposite	O
relations	O
.	O

DeepWalk	Method
:	O
Online	Task
Learning	Task
of	Task
Social	Task
Representations	Task
section	O
:	O
ABSTRACT	O
We	O
present	O
DeepWalk	Method
,	O
a	O
novel	O
approach	O
for	O
learning	Task
latent	Task
representations	Task
of	Task
vertices	Task
in	Task
a	Task
network	Task
.	O

These	O
latent	Method
representations	Method
encode	O
social	O
relations	O
in	O
a	O
continuous	O
vector	O
space	O
,	O
which	O
is	O
easily	O
exploited	O
by	O
statistical	Method
models	Method
.	O

DeepWalk	Method
generalizes	O
recent	O
advancements	O
in	O
language	Task
modeling	Task
and	O
unsupervised	Task
feature	Task
learning	Task
(	O
or	O
deep	Task
learning	Task
)	O
from	O
sequences	O
of	O
words	O
to	O
graphs	O
.	O

DeepWalk	Method
uses	O
local	O
information	O
obtained	O
from	O
truncated	O
random	O
walks	O
to	O
learn	O
latent	Method
representations	Method
by	O
treating	O
walks	O
as	O
the	O
equivalent	O
of	O
sentences	O
.	O

We	O
demonstrate	O
DeepWalk	Method
's	O
latent	O
representations	O
on	O
several	O
multi	Task
-	Task
label	Task
network	Task
classification	Task
tasks	Task
for	O
social	O
networks	O
such	O
as	O
BlogCatalog	Material
,	O
Flickr	Material
,	O
and	O
YouTube	Material
.	O

Our	O
results	O
show	O
that	O
DeepWalk	Method
outperforms	O
challenging	O
baselines	O
which	O
are	O
allowed	O
a	O
global	O
view	O
of	O
the	O
network	O
,	O
especially	O
in	O
the	O
presence	O
of	O
missing	O
information	O
.	O

DeepWalk	Method
's	O
representations	O
can	O
provide	O
F1	Metric
scores	Metric
up	O
to	O
10	O
%	O
higher	O
than	O
competing	O
methods	O
when	O
labeled	O
data	O
is	O
sparse	O
.	O

In	O
some	O
experiments	O
,	O
DeepWalk	Method
's	O
representations	O
are	O
able	O
to	O
outperform	O
all	O
baseline	O
methods	O
while	O
using	O
60	O
%	O
less	O
training	O
data	O
.	O

DeepWalk	Method
is	O
also	O
scalable	O
.	O

It	O
is	O
an	O
online	Method
learning	Method
algorithm	Method
which	O
builds	O
useful	O
incremental	O
results	O
,	O
and	O
is	O
trivially	O
parallelizable	O
.	O

These	O
qualities	O
make	O
it	O
suitable	O
for	O
a	O
broad	O
class	O
of	O
real	Task
world	Task
applications	Task
such	O
as	O
network	Task
classification	Task
,	O
and	O
anomaly	Task
detection	Task
.	O

section	O
:	O
INTRODUCTION	O
The	O
sparsity	O
of	O
a	O
network	Method
representation	Method
is	O
both	O
a	O
strength	O
and	O
a	O
weakness	O
.	O

Sparsity	Method
enables	O
the	O
design	O
of	O
efficient	O
discrete	Method
algorithms	Method
,	O
but	O
can	O
make	O
it	O
harder	O
to	O
generalize	O
in	O
statistical	Task
learning	Task
.	O

Machine	Task
learning	Task
applications	Task
in	O
networks	Task
(	O
such	O
as	O
network	Task
classification	Task
[	O
reference	O
][	O
reference	O
]	O
,	O
content	O
rec	O
-	O
The	O
learned	O
representation	O
encodes	O
community	O
structure	O
so	O
it	O
can	O
be	O
easily	O
exploited	O
by	O
standard	O
classification	Task
methods	O
.	O

Here	O
,	O
our	O
method	O
is	O
used	O
on	O
Zachary	Method
's	Method
Karate	Method
network	Method
[	O
reference	O
]	O
to	O
generate	O
a	O
latent	Method
representation	Method
in	O
R	O
2	O
.	O

Note	O
the	O
correspondence	O
between	O
community	O
structure	O
in	O
the	O
input	O
graph	O
and	O
the	O
embedding	O
.	O

Vertex	O
colors	O
represent	O
a	O
modularity	Method
-	Method
based	Method
clustering	Method
of	O
the	O
input	O
graph	O
.	O

ommendation	Method
[	O
reference	O
]	O
,	O
anomaly	Task
detection	Task
[	O
reference	O
]	O
,	O
and	O
missing	Task
link	Task
prediction	Task
[	O
reference	O
]	O
)	O
must	O
be	O
able	O
to	O
deal	O
with	O
this	O
sparsity	O
in	O
order	O
to	O
survive	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
deep	Method
learning	Method
(	O
unsupervised	Method
feature	Method
learning	Method
)	O
[	O
reference	O
]	O
techniques	O
,	O
which	O
have	O
proven	O
successful	O
in	O
natural	Task
language	Task
processing	Task
,	O
into	O
network	Task
analysis	Task
for	O
the	O
first	O
time	O
.	O

We	O
develop	O
an	O
algorithm	O
(	O
DeepWalk	Method
)	O
that	O
learns	O
social	Method
representations	Method
of	O
a	O
graph	O
's	O
vertices	O
,	O
by	O
modeling	O
a	O
stream	Method
of	Method
short	Method
random	Method
walks	Method
.	O

Social	Method
representations	Method
are	O
latent	O
features	O
of	O
the	O
vertices	O
that	O
capture	O
neighborhood	O
similarity	O
and	O
community	O
membership	O
.	O

These	O
latent	Method
representations	Method
encode	O
social	O
relations	O
in	O
a	O
continuous	O
vector	O
space	O
with	O
a	O
relatively	O
small	O
number	O
of	O
dimensions	O
.	O

DeepWalk	Method
generalizes	O
neural	Method
language	Method
models	Method
to	O
process	O
a	O
special	O
language	O
composed	O
of	O
a	O
set	O
of	O
randomly	O
-	O
generated	O
walks	O
.	O

These	O
neural	Method
language	Method
models	Method
have	O
been	O
used	O
to	O
capture	O
the	O
semantic	O
and	O
syntactic	O
structure	O
of	O
human	O
language	O
[	O
reference	O
]	O
,	O
and	O
even	O
logical	O
analogies	O
[	O
reference	O
]	O
.	O

DeepWalk	Method
takes	O
a	O
graph	O
as	O
input	O
and	O
produces	O
a	O
latent	Method
representation	Method
as	O
an	O
output	O
.	O

The	O
result	O
of	O
applying	O
our	O
method	O
to	O
the	O
well	O
-	O
studied	O
Karate	Task
network	Task
is	O
shown	O
in	O
Figure	O
1	O
.	O

The	O
graph	O
,	O
as	O
typically	O
presented	O
by	O
force	Method
-	Method
directed	Method
layouts	Method
,	O
is	O
shown	O
in	O
Figure	O
1a	O
.	O

Figure	O
1b	O
shows	O
the	O
output	O
of	O
our	O
method	O
with	O
2	O
latent	O
dimensions	O
.	O

Beyond	O
the	O
striking	O
similarity	O
,	O
we	O
note	O
that	O
linearly	O
separable	O
portions	O
of	O
(	O
1b	O
)	O
correspond	O
to	O
clusters	O
found	O
through	O
modularity	Method
maximization	Method
in	O
the	O
input	O
graph	O
(	O
1a	O
)	O
(	O
shown	O
as	O
vertex	O
colors	O
)	O
.	O

To	O
demonstrate	O
DeepWalk	Method
's	O
potential	O
in	O
real	Task
world	Task
sce	Task
-	Task
narios	Task
,	O
we	O
evaluate	O
its	O
performance	O
on	O
challenging	O
multilabel	O
network	Task
classification	Task
problems	O
in	O
large	O
heterogeneous	O
graphs	O
.	O

In	O
the	O
relational	Task
classification	Task
problem	Task
,	O
the	O
links	O
between	O
feature	O
vectors	O
violate	O
the	O
traditional	O
i.i.d	O
.	O

assumption	O
.	O

Techniques	O
to	O
address	O
this	O
problem	O
typically	O
use	O
approximate	Method
inference	Method
techniques	Method
[	O
reference	O
][	O
reference	O
]	O
to	O
leverage	O
the	O
dependency	O
information	O
to	O
improve	O
classification	Task
results	O
.	O

We	O
distance	O
ourselves	O
from	O
these	O
approaches	O
by	O
learning	O
labelindependent	Method
representations	Method
of	Method
the	Method
graph	Method
.	O

Our	O
representation	Metric
quality	Metric
is	O
not	O
influenced	O
by	O
the	O
choice	O
of	O
labeled	O
vertices	O
,	O
so	O
they	O
can	O
be	O
shared	O
among	O
tasks	O
.	O

DeepWalk	Method
outperforms	O
other	O
latent	Method
representation	Method
methods	Method
for	O
creating	Task
social	Task
dimensions	Task
[	O
reference	O
][	O
reference	O
]	O
,	O
especially	O
when	O
labeled	O
nodes	O
are	O
scarce	O
.	O

Strong	O
performance	O
with	O
our	O
representations	O
is	O
possible	O
with	O
very	O
simple	O
linear	Method
classifiers	Method
(	O
e.g.	O
logistic	Method
regression	Method
)	O
.	O

Our	O
representations	O
are	O
general	O
,	O
and	O
can	O
be	O
combined	O
with	O
any	O
classification	Task
method	O
(	O
including	O
iterative	Method
inference	Method
methods	Method
)	O
.	O

DeepWalk	Method
achieves	O
all	O
of	O
that	O
while	O
being	O
an	O
online	Method
algorithm	Method
that	O
is	O
trivially	O
parallelizable	O
.	O

Our	O
contributions	O
are	O
as	O
follows	O
:	O
•	O
We	O
introduce	O
deep	Method
learning	Method
as	O
a	O
tool	O
to	O
analyze	O
graphs	Task
,	O
to	O
build	O
robust	Method
representations	Method
that	O
are	O
suitable	O
for	O
statistical	Task
modeling	Task
.	O

DeepWalk	Method
learns	O
structural	O
regularities	O
present	O
within	O
short	O
random	O
walks	O
.	O

•	O
We	O
extensively	O
evaluate	O
our	O
representations	O
on	O
multilabel	Task
classification	Task
tasks	Task
on	O
several	O
social	O
networks	O
.	O

We	O
show	O
significantly	O
increased	O
classification	Task
performance	O
in	O
the	O
presence	O
of	O
label	O
sparsity	O
,	O
getting	O
improvements	O
5%	O
-	O
10	O
%	O
of	O
Micro	Metric
F1	Metric
,	O
on	O
the	O
sparsest	Task
problems	Task
we	O
consider	O
.	O

In	O
some	O
cases	O
,	O
DeepWalk	Method
's	O
representations	O
can	O
outperform	O
its	O
competitors	O
even	O
when	O
given	O
60	O
%	O
less	O
training	O
data	O
.	O

•	O
We	O
demonstrate	O
the	O
scalability	O
of	O
our	O
algorithm	O
by	O
building	O
representations	Method
of	Method
web	Method
-	Method
scale	Method
graphs	Method
,	O
(	O
such	O
as	O
YouTube	Material
)	O
using	O
a	O
parallel	Method
implementation	Method
.	O

Moreover	O
,	O
we	O
describe	O
the	O
minimal	O
changes	O
necessary	O
to	O
build	O
a	O
streaming	Method
version	Method
of	O
our	O
approach	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
arranged	O
as	O
follows	O
.	O

In	O
Sections	O
2	O
and	O
3	O
,	O
we	O
discuss	O
the	O
problem	Task
formulation	Task
of	O
classification	Task
in	O
data	Task
networks	Task
,	O
and	O
how	O
it	O
relates	O
to	O
our	O
work	O
.	O

In	O
Section	O
4	O
we	O
present	O
DeepWalk	Method
,	O
our	O
approach	O
for	O
Social	Task
Representation	Task
Learning	Task
.	O

We	O
outline	O
ours	O
experiments	O
in	O
Section	O
5	O
,	O
and	O
present	O
their	O
results	O
in	O
Section	O
6	O
.	O

We	O
close	O
with	O
a	O
discussion	O
of	O
related	O
work	O
in	O
Section	O
7	O
,	O
and	O
our	O
conclusions	O
.	O

section	O
:	O
PROBLEM	O
DEFINITION	O
We	O
consider	O
the	O
problem	O
of	O
classifying	O
members	O
of	O
a	O
social	Task
network	Task
into	O
one	O
or	O
more	O
categories	O
.	O

More	O
formally	O
,	O
let	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
are	O
the	O
members	O
of	O
the	O
network	O
,	O
and	O
E	O
be	O
its	O
edges	O
,	O
E	O
⊆	O
(	O
V	O
×	O
V	O
)	O
.	O

Given	O
a	O
partially	O
labeled	O
social	O
network	O
GL	O
=	O
(	O
V	O
,	O
E	O
,	O
X	O
,	O
Y	O
)	O
,	O
with	O
attributes	O
X	O
∈	O
R	O
section	O
:	O
|V	O
|×S	O
where	O
S	O
is	O
the	O
size	O
of	O
the	O
feature	O
space	O
for	O
each	O
attribute	O
vector	O
,	O
and	O
Y	O
∈	O
R	O
|V	O
|×|Y|	O
,	O
Y	O
is	O
the	O
set	O
of	O
labels	O
.	O

In	O
a	O
traditional	O
machine	Task
learning	Task
classification	Task
setting	Task
,	O
we	O
aim	O
to	O
learn	O
a	O
hypothesis	O
H	O
that	O
maps	O
elements	O
of	O
X	O
to	O
the	O
labels	O
set	O
Y.	O
In	O
our	O
case	O
,	O
we	O
can	O
utilize	O
the	O
significant	O
information	O
about	O
the	O
dependence	O
of	O
the	O
examples	O
embedded	O
in	O
the	O
structure	O
of	O
G	O
to	O
achieve	O
superior	O
performance	O
.	O

In	O
the	O
literature	O
,	O
this	O
is	O
known	O
as	O
the	O
relational	Task
classification	Task
(	O
or	O
the	O
collective	Task
classification	Task
problem	Task
[	O
reference	O
]	O
)	O
.	O

Traditional	O
approaches	O
to	O
relational	Task
classification	Task
pose	O
the	O
problem	O
as	O
an	O
inference	Task
in	O
an	O
undirected	Method
Markov	Method
network	Method
,	O
and	O
then	O
use	O
iterative	Method
approximate	Method
inference	Method
algorithms	Method
(	O
such	O
as	O
the	O
iterative	O
classification	Task
algorithm	O
[	O
reference	O
]	O
,	O
Gibbs	Method
Sampling	Method
[	O
reference	O
]	O
,	O
or	O
label	Method
relaxation	Method
[	O
reference	O
]	O
)	O
to	O
compute	O
the	O
posterior	O
distribution	O
of	O
labels	O
given	O
the	O
network	O
structure	O
.	O

We	O
propose	O
a	O
different	O
approach	O
to	O
capture	O
the	O
network	O
topology	O
information	O
.	O

Instead	O
of	O
mixing	O
the	O
label	O
space	O
as	O
part	O
of	O
the	O
feature	O
space	O
,	O
we	O
propose	O
an	O
unsupervised	Method
method	Method
which	O
learns	O
features	O
that	O
capture	O
the	O
graph	O
structure	O
independent	O
of	O
the	O
labels	O
'	O
distribution	O
.	O

This	O
separation	O
between	O
the	O
structural	Method
representation	Method
and	O
the	O
labeling	Task
task	Task
avoids	O
cascading	O
errors	O
,	O
which	O
can	O
occur	O
in	O
iterative	Method
methods	Method
[	O
reference	O
]	O
.	O

Moreover	O
,	O
the	O
same	O
representation	O
can	O
be	O
used	O
for	O
multiple	O
classification	Task
problems	Task
concerning	O
that	O
network	O
.	O

Our	O
goal	O
is	O
to	O
learn	O
XE	O
∈	O
R	O
|V	O
|×d	O
,	O
where	O
d	O
is	O
small	O
number	O
of	O
latent	O
dimensions	O
.	O

These	O
low	Method
-	Method
dimensional	Method
representations	Method
are	O
distributed	O
;	O
meaning	O
each	O
social	O
phenomena	O
is	O
expressed	O
by	O
a	O
subset	O
of	O
the	O
dimensions	O
and	O
each	O
dimension	O
contributes	O
to	O
a	O
subset	O
of	O
the	O
social	O
concepts	O
expressed	O
by	O
the	O
space	O
.	O

Using	O
these	O
structural	O
features	O
,	O
we	O
will	O
augment	O
the	O
attributes	O
space	O
to	O
help	O
the	O
classification	Task
decision	Task
.	O

These	O
features	O
are	O
general	O
,	O
and	O
can	O
be	O
used	O
with	O
any	O
classification	Task
algorithm	O
(	O
including	O
iterative	Method
methods	Method
)	O
.	O

However	O
,	O
we	O
believe	O
that	O
the	O
greatest	O
utility	O
of	O
these	O
features	O
is	O
their	O
easy	O
integration	O
with	O
simple	O
machine	Method
learning	Method
algorithms	Method
.	O

They	O
scale	O
appropriately	O
in	O
real	Task
-	Task
world	Task
networks	Task
,	O
as	O
we	O
will	O
show	O
in	O
Section	O
6	O
.	O

section	O
:	O
LEARNING	Task
SOCIAL	Task
REPRESENTATIONS	Task
We	O
seek	O
learning	Task
social	Task
representations	Task
with	O
the	O
following	O
characteristics	O
:	O
•	O
Adaptability	Task
-	Task
Real	Task
social	Task
networks	Task
are	O
constantly	O
evolving	O
;	O
new	O
social	O
relations	O
should	O
not	O
require	O
repeating	O
the	O
learning	Method
process	Method
all	O
over	O
again	O
.	O

•	O
Community	O
aware	O
-	O
The	O
distance	O
between	O
latent	O
dimensions	O
should	O
represent	O
a	O
metric	O
for	O
evaluating	Task
social	Task
similarity	Task
between	O
the	O
corresponding	O
members	O
of	O
the	O
network	O
.	O

This	O
allows	O
generalization	Task
in	O
networks	Task
with	O
homophily	O
.	O

•	O
Low	Material
dimensional	Material
-	Material
When	Material
labeled	Material
data	Material
is	O
scarce	O
,	O
lowdimensional	Method
models	Method
generalize	O
better	O
,	O
and	O
speed	O
up	O
convergence	Task
and	O
inference	Task
.	O

•	O
Continuous	O
-	O
We	O
require	O
latent	Method
representations	Method
to	O
model	O
partial	O
community	O
membership	O
in	O
continuous	O
space	O
.	O

In	O
addition	O
to	O
providing	O
a	O
nuanced	O
view	O
of	O
community	O
membership	O
,	O
a	O
continuous	Method
representation	Method
has	O
smooth	O
decision	O
boundaries	O
between	O
communities	O
which	O
allows	O
more	O
robust	O
classification	Task
.	O

Our	O
method	O
for	O
satisfying	O
these	O
requirements	O
learns	O
representation	O
for	O
vertices	O
from	O
a	O
stream	O
of	O
short	Method
random	Method
walks	Method
,	O
using	O
optimization	Method
techniques	Method
originally	O
designed	O
for	O
language	Task
modeling	Task
.	O

Here	O
,	O
we	O
review	O
the	O
basics	O
of	O
both	O
random	Method
walks	Method
and	O
language	Method
modeling	Method
,	O
and	O
describe	O
how	O
their	O
combination	O
satisfies	O
our	O
requirements	O
.	O

section	O
:	O
Random	O
Walks	O
We	O
denote	O
a	O
random	O
walk	O
rooted	O
at	O
vertex	O
vi	O
as	O
Wv	O
i	O
.	O

It	O
is	O
a	O
stochastic	Method
process	Method
with	O
random	O
variables	O
W	O
is	O
a	O
vertex	O
chosen	O
at	O
random	O
from	O
the	O
neighbors	O
of	O
vertex	O
v	O
k	O
.	O

Random	Method
walks	Method
have	O
been	O
used	O
as	O
a	O
similarity	Metric
measure	Metric
for	O
a	O
variety	O
of	O
problems	O
in	O
content	Task
recommendation	Task
[	O
reference	O
]	O
and	O
community	Task
detection	Task
[	O
reference	O
]	O
.	O

They	O
are	O
also	O
the	O
foundation	O
of	O
a	O
class	O
of	O
output	Method
sensitive	Method
algorithms	Method
which	O
use	O
them	O
to	O
compute	O
local	O
community	O
structure	O
information	O
in	O
time	O
sublinear	O
to	O
the	O
size	O
of	O
the	O
input	O
graph	O
[	O
reference	O
]	O
.	O

It	O
is	O
this	O
connection	O
to	O
local	O
structure	O
that	O
motivates	O
us	O
to	O
use	O
a	O
stream	Method
of	Method
short	Method
random	Method
walks	Method
as	O
our	O
basic	O
tool	O
for	O
extracting	Task
information	Task
from	O
a	O
network	Task
.	O

In	O
addition	O
to	O
capturing	O
community	O
information	O
,	O
using	O
random	O
walks	O
as	O
the	O
basis	O
for	O
our	O
algorithm	O
gives	O
us	O
two	O
other	O
desirable	O
properties	O
.	O

First	O
,	O
local	Task
exploration	Task
is	O
easy	O
to	O
parallelize	O
.	O

Several	O
random	Method
walkers	Method
(	O
in	O
different	O
threads	O
,	O
processes	O
,	O
or	O
machines	O
)	O
can	O
simultaneously	O
explore	O
different	O
parts	O
of	O
the	O
same	O
graph	O
.	O

Secondly	O
,	O
relying	O
on	O
information	O
obtained	O
from	O
short	O
random	O
walks	O
make	O
it	O
possible	O
to	O
accommodate	O
small	O
changes	O
in	O
the	O
graph	O
structure	O
without	O
the	O
need	O
for	O
global	O
recomputation	O
.	O

We	O
can	O
iteratively	O
update	O
the	O
learned	O
model	O
with	O
new	O
random	O
walks	O
from	O
the	O
changed	O
region	O
in	O
time	O
sub	O
-	O
linear	O
to	O
the	O
entire	O
graph	O
.	O

section	O
:	O
Connection	O
:	O
Power	O
laws	O
Having	O
chosen	O
online	O
random	O
walks	O
as	O
our	O
primitive	O
for	O
capturing	Task
graph	Task
structure	Task
,	O
we	O
now	O
need	O
a	O
suitable	O
method	O
to	O
capture	O
this	O
information	O
.	O

If	O
the	O
degree	O
distribution	O
of	O
a	O
connected	O
graph	O
follows	O
a	O
power	O
law	O
(	O
is	O
scale	O
-	O
free	O
)	O
,	O
we	O
observe	O
that	O
the	O
frequency	O
which	O
vertices	O
appear	O
in	O
the	O
short	O
random	O
walks	O
will	O
also	O
follow	O
a	O
power	Method
-	Method
law	Method
distribution	Method
.	O

Word	O
frequency	O
in	O
natural	O
language	O
follows	O
a	O
similar	O
distribution	O
,	O
and	O
techniques	O
from	O
language	Method
modeling	Method
account	O
for	O
this	O
distributional	O
behavior	O
.	O

To	O
emphasize	O
this	O
similarity	O
we	O
show	O
two	O
different	O
power	O
-	O
law	O
distributions	O
in	O
Figure	O
2	O
.	O

The	O
first	O
comes	O
from	O
a	O
series	O
of	O
short	Method
random	Method
walks	Method
on	O
a	O
scale	Method
-	Method
free	Method
graph	Method
,	O
and	O
the	O
second	O
comes	O
from	O
the	O
text	O
of	O
100	O
,	O
000	O
articles	O
from	O
the	O
English	Material
Wikipedia	Material
.	O

A	O
core	O
contribution	O
of	O
our	O
work	O
is	O
the	O
idea	O
that	O
techniques	O
which	O
have	O
been	O
used	O
to	O
model	O
natural	O
language	O
(	O
where	O
the	O
symbol	O
frequency	O
follows	O
a	O
power	O
law	O
distribution	O
(	O
or	O
Zipf	Method
's	Method
law	Method
)	O
)	O
can	O
be	O
re	O
-	O
purposed	O
to	O
model	O
community	Task
structure	Task
in	Task
networks	Task
.	O

We	O
spend	O
the	O
rest	O
of	O
this	O
section	O
reviewing	O
the	O
growing	O
work	O
in	O
language	Task
modeling	Task
,	O
and	O
transforming	O
it	O
to	O
learn	O
representations	O
of	O
vertices	O
which	O
satisfy	O
our	O
criteria	O
.	O

section	O
:	O
Language	Method
Modeling	Method
The	O
goal	O
of	O
language	Task
modeling	Task
is	O
estimate	O
the	O
likelihood	O
of	O
a	O
specific	O
sequence	O
of	O
words	O
appearing	O
in	O
a	O
corpus	O
.	O

More	O
formally	O
,	O
given	O
a	O
sequence	O
of	O
words	O
where	O
wi	O
∈	O
V	O
(	O
V	O
is	O
the	O
vocabulary	O
)	O
,	O
we	O
would	O
like	O
to	O
maximize	O
the	O
Pr	O
(	O
wn|w0	O
,	O
w1	O
,	O
·	O
·	O
·	O
,	O
wn−1	O
)	O
over	O
all	O
the	O
training	O
corpus	O
.	O

Recent	O
work	O
in	O
representation	Task
learning	Task
has	O
focused	O
on	O
using	O
probabilistic	Method
neural	Method
networks	Method
to	O
build	O
general	Method
representations	Method
of	O
words	O
which	O
extend	O
the	O
scope	O
of	O
language	Method
modeling	Method
beyond	O
its	O
original	O
goals	O
.	O

In	O
this	O
work	O
,	O
we	O
present	O
a	O
generalization	Method
of	Method
language	Method
modeling	Method
to	O
explore	O
the	O
graph	O
through	O
a	O
stream	Method
of	Method
short	Method
random	Method
walks	Method
.	O

These	O
walks	O
can	O
be	O
thought	O
of	O
short	O
sentences	O
and	O
phrases	O
in	O
a	O
special	O
language	O
.	O

The	O
direct	O
analog	O
is	O
to	O
estimate	O
the	O
likelihood	O
of	O
observing	O
vertex	O
vi	O
given	O
all	O
the	O
previous	O
vertices	O
visited	O
so	O
far	O
in	O
the	O
random	O
walk	O
.	O

Our	O
goal	O
is	O
to	O
learn	O
a	O
latent	Method
representation	Method
,	O
not	O
only	O
a	O
probability	O
distribution	O
of	O
node	O
co	O
-	O
occurrences	O
,	O
and	O
so	O
we	O
introduce	O
a	O
mapping	O
function	O
Φ	O
:	O
v	O
∈	O
V	O
→	O
R	O
|V	O
|×d	O
.	O

This	O
mapping	Method
Φ	Method
represents	O
the	O
latent	Method
social	Method
representation	Method
associated	O
with	O
each	O
vertex	O
v	O
in	O
the	O
graph	O
.	O

(	O
In	O
practice	O
,	O
we	O
represent	O
Φ	O
by	O
a	O
|V	O
|	O
×	O
d	O
matrix	O
of	O
free	O
parameters	O
,	O
which	O
will	O
serve	O
later	O
on	O
as	O
our	O
XE	O
.	O

)	O
The	O
problem	O
then	O
,	O
is	O
to	O
estimate	O
the	O
likelihood	O
:	O
However	O
as	O
the	O
walk	O
length	O
grows	O
,	O
computing	O
this	O
objective	O
function	O
becomes	O
unfeasible	O
.	O

A	O
recent	O
relaxation	O
in	O
language	Task
modeling	Task
[	O
reference	O
][	O
reference	O
]	O
turns	O
the	O
prediction	Task
problem	Task
on	O
its	O
head	O
.	O

First	O
,	O
instead	O
of	O
using	O
the	O
context	O
to	O
predict	O
a	O
missing	O
word	O
,	O
it	O
uses	O
one	O
word	O
to	O
predict	O
the	O
context	O
.	O

Secondly	O
,	O
the	O
context	O
is	O
composed	O
of	O
the	O
words	O
appearing	O
to	O
right	O
side	O
of	O
the	O
given	O
word	O
as	O
well	O
as	O
the	O
left	O
side	O
.	O

Finally	O
,	O
it	O
removes	O
the	O
ordering	O
constraint	O
on	O
the	O
problem	O
.	O

Instead	O
,	O
the	O
model	O
is	O
required	O
to	O
maximize	O
the	O
probability	O
of	O
any	O
word	O
appearing	O
in	O
the	O
context	O
without	O
the	O
knowledge	O
of	O
its	O
offset	O
from	O
the	O
given	O
word	O
.	O

In	O
terms	O
of	O
vertex	Method
representation	Method
modeling	Method
,	O
this	O
yields	O
the	O
optimization	Task
problem	Task
:	O
We	O
find	O
these	O
relaxations	O
are	O
particularly	O
desirable	O
for	O
social	Task
representation	Task
learning	Task
.	O

First	O
,	O
the	O
order	O
independence	O
assumption	O
better	O
captures	O
a	O
sense	O
of	O
'	O
nearness	O
'	O
that	O
is	O
provided	O
by	O
random	O
walks	O
.	O

Moreover	O
,	O
this	O
relaxation	O
is	O
quite	O
useful	O
for	O
speeding	O
up	O
the	O
training	Metric
time	Metric
by	O
building	O
small	Method
models	Method
as	O
one	O
vertex	O
is	O
given	O
at	O
a	O
time	O
.	O

Solving	O
the	O
optimization	Task
problem	Task
from	O
Eq	O
.	O

2	O
builds	O
representations	O
that	O
capture	O
the	O
shared	O
similarities	O
in	O
local	O
graph	O
structure	O
between	O
vertices	O
.	O

Vertices	O
which	O
have	O
similar	O
neighborhoods	O
will	O
acquire	O
similar	O
representations	O
(	O
encoding	O
cocitation	O
similarity	O
)	O
,	O
and	O
allowing	O
generalization	Task
on	O
machine	Task
learning	Task
tasks	Task
.	O

By	O
combining	O
both	O
truncated	Method
random	Method
walks	Method
and	O
neural	Method
language	Method
models	Method
we	O
formulate	O
a	O
method	O
which	O
satisfies	O
all	O
window	O
size	O
w	O
embedding	O
size	O
d	O
walks	O
per	O
vertex	O
γ	O
walk	O
length	O
t	O
Output	O
:	O
matrix	Method
of	Method
vertex	Method
representations	Method
Φ	O
∈	O
R	O
|V	O
|×d	O
1	O
:	O
Initialization	O
:	O
Sample	O
Φ	O
from	O
U	O
|V	O
|×d	O
2	O
:	O
Build	O
a	O
binary	O
Tree	O
T	O
from	O
V	O
3	O
:	O
for	O
i	O
=	O
0	O
to	O
γ	O
do	O
4	O
:	O
for	O
each	O
vi	O
∈	O
O	O
do	O
6	O
:	O
end	O
for	O
9	O
:	O
end	O
for	O
of	O
our	O
desired	O
properties	O
.	O

This	O
method	O
generates	O
representations	Task
of	Task
social	Task
networks	Task
that	O
are	O
low	O
-	O
dimensional	O
,	O
and	O
exist	O
in	O
a	O
continuous	O
vector	O
space	O
.	O

Its	O
representations	O
encode	O
latent	O
forms	O
of	O
community	O
membership	O
,	O
and	O
because	O
the	O
method	O
outputs	O
useful	O
intermediate	Method
representations	Method
,	O
it	O
can	O
adapt	O
to	O
changing	O
network	O
topology	O
.	O

section	O
:	O
METHOD	O
In	O
this	O
section	O
we	O
discuss	O
the	O
main	O
components	O
of	O
our	O
algorithm	O
.	O

We	O
also	O
present	O
several	O
variants	O
of	O
our	O
approach	O
and	O
discuss	O
their	O
merits	O
.	O

section	O
:	O
Overview	O
As	O
in	O
any	O
language	Method
modeling	Method
algorithm	Method
,	O
the	O
only	O
required	O
input	O
is	O
a	O
corpus	O
and	O
a	O
vocabulary	O
V.	O
DeepWalk	Method
considers	O
a	O
set	O
of	O
short	O
truncated	O
random	O
walks	O
its	O
own	O
corpus	O
,	O
and	O
the	O
graph	O
vertices	O
as	O
its	O
own	O
vocabulary	O
(	O
V	O
=	O
V	O
)	O
.	O

While	O
it	O
is	O
beneficial	O
to	O
know	O
the	O
V	O
and	O
the	O
frequency	O
distribution	O
of	O
vertices	O
in	O
the	O
random	O
walks	O
ahead	O
of	O
the	O
training	O
,	O
it	O
is	O
not	O
necessary	O
for	O
the	O
algorithm	O
to	O
work	O
as	O
we	O
will	O
show	O
in	O
4.2.2	O
.	O

section	O
:	O
Algorithm	O
:	O
DeepWalk	Method
The	O
algorithm	O
consists	O
of	O
two	O
main	O
components	O
;	O
first	O
a	O
random	Method
walk	Method
generator	Method
and	O
second	O
an	O
update	Method
procedure	Method
.	O

The	O
random	Method
walk	Method
generator	Method
takes	O
a	O
graph	O
G	O
and	O
samples	O
uniformly	O
a	O
random	O
vertex	O
vi	O
as	O
the	O
root	O
of	O
the	O
random	Method
walk	Method
Wv	O
i	O
.	O

A	O
walk	O
samples	O
uniformly	O
from	O
the	O
neighbors	O
of	O
the	O
last	O
vertex	O
visited	O
until	O
the	O
maximum	O
length	O
(	O
t	O
)	O
is	O
reached	O
.	O

While	O
we	O
set	O
the	O
length	O
of	O
our	O
random	O
walks	O
in	O
the	O
experiments	O
to	O
be	O
fixed	O
,	O
there	O
is	O
no	O
restriction	O
for	O
the	O
random	O
walks	O
to	O
be	O
of	O
the	O
same	O
length	O
.	O

These	O
walks	O
could	O
have	O
restarts	O
(	O
i.e.	O
a	O
teleport	O
probability	O
of	O
returning	O
back	O
to	O
their	O
root	O
)	O
,	O
but	O
our	O
preliminary	O
results	O
did	O
not	O
show	O
any	O
advantage	O
of	O
using	O
restarts	Method
.	O

In	O
practice	O
,	O
our	O
implementation	O
specifies	O
a	O
number	O
of	O
random	O
walks	O
γ	O
of	O
length	O
t	O
to	O
start	O
at	O
each	O
vertex	O
.	O

Lines	O
3	O
-	O
9	O
in	O
Algorithm	O
1	O
shows	O
the	O
core	O
of	O
our	O
approach	O
.	O

The	O
outer	O
loop	O
specifies	O
the	O
number	O
of	O
times	O
,	O
γ	O
,	O
which	O
we	O
should	O
start	O
random	O
walks	O
at	O
each	O
vertex	O
.	O

We	O
think	O
of	O
each	O
iteration	O
as	O
making	O
a	O
'	O
pass	O
'	O
over	O
the	O
data	O
and	O
sample	O
one	O
walk	O
per	O
node	O
during	O
this	O
pass	O
.	O

At	O
the	O
start	O
of	O
each	O
pass	O
we	O
generate	O
a	O
random	O
ordering	O
to	O
traverse	O
the	O
vertices	O
.	O

This	O
is	O
not	O
strictly	O
required	O
,	O
but	O
is	O
well	O
-	O
known	O
to	O
speed	O
up	O
the	O
convergence	O
of	O
stochastic	Method
gradient	Method
descent	Method
.	O

section	O
:	O
Algorithm	O
2	O
5	O
:	O
end	O
for	O
6	O
:	O
end	O
for	O
In	O
the	O
inner	O
loop	O
,	O
we	O
iterate	O
over	O
all	O
the	O
vertices	O
of	O
the	O
graph	O
.	O

For	O
each	O
vertex	O
vi	O
we	O
generate	O
a	O
random	Method
walk	Method
|Wv	O
i	O
|	O
=	O
t	O
,	O
and	O
then	O
use	O
it	O
to	O
update	O
our	O
representations	O
(	O
Line	O
7	O
)	O
.	O

We	O
use	O
the	O
SkipGram	Method
algorithm	Method
[	O
reference	O
]	O
to	O
update	O
these	O
representations	O
in	O
accordance	O
with	O
our	O
objective	Metric
function	Metric
in	O
Eq	O
.	O

2	O
.	O

section	O
:	O
SkipGram	Method
SkipGram	Method
is	O
a	O
language	Method
model	Method
that	O
maximizes	O
the	O
cooccurrence	O
probability	O
among	O
the	O
words	O
that	O
appear	O
within	O
a	O
window	O
,	O
w	O
,	O
in	O
a	O
sentence	O
[	O
reference	O
]	O
.	O

Algorithm	O
2	O
iterates	O
over	O
all	O
possible	O
collocations	O
in	O
random	O
walk	O
that	O
appear	O
within	O
the	O
window	O
w	O
(	O
lines	O
1	O
-	O
2	O
)	O
.	O

For	O
each	O
,	O
we	O
map	O
each	O
vertex	O
vj	O
to	O
its	O
current	O
representation	O
vector	O
Φ	O
(	O
vj	O
)	O
∈	O
R	O
d	O
(	O
See	O
Figure	O
3b	O
)	O
.	O

Given	O
the	O
representation	O
of	O
vj	O
,	O
we	O
would	O
like	O
to	O
maximize	O
the	O
probability	O
of	O
its	O
neighbors	O
in	O
the	O
walk	O
(	O
line	O
3	O
)	O
.	O

We	O
can	O
learn	O
such	O
posterior	O
distribution	O
using	O
several	O
choices	O
of	O
classifiers	Method
.	O

For	O
example	O
,	O
modeling	O
the	O
previous	Task
problem	Task
using	O
logistic	Method
regression	Method
would	O
result	O
in	O
a	O
huge	O
number	O
of	O
labels	O
that	O
is	O
equal	O
to	O
|V	O
|	O
which	O
could	O
be	O
in	O
millions	O
or	O
billions	O
.	O

Such	O
models	O
require	O
large	O
amount	O
of	O
computational	O
resources	O
that	O
could	O
span	O
a	O
whole	O
cluster	O
of	O
computers	O
[	O
reference	O
]	O
.	O

To	O
speed	O
the	O
training	Metric
time	Metric
,	O
Hierarchical	O
Softmax	O
[	O
reference	O
][	O
reference	O
]	O
can	O
be	O
used	O
to	O
approximate	O
the	O
probability	O
distribution	O
.	O

section	O
:	O
Hierarchical	Method
Softmax	Method
Given	O
that	O
u	O
k	O
∈	O
V	O
,	O
calculating	O
Pr	O
(	O
u	O
k	O
|	O
Φ	O
(	O
vj	O
)	O
)	O
in	O
line	O
3	O
is	O
not	O
feasible	O
.	O

Computing	O
the	O
partition	O
function	O
(	O
normalization	O
factor	O
)	O
is	O
expensive	O
.	O

If	O
we	O
assign	O
the	O
vertices	O
to	O
the	O
leaves	O
of	O
a	O
binary	O
tree	O
,	O
the	O
prediction	Task
problem	Task
turns	O
into	O
maximizing	O
the	O
probability	O
of	O
a	O
specific	O
path	O
in	O
the	O
tree	O
(	O
See	O
Figure	O
3c	O
)	O
.	O

If	O
the	O
path	O
to	O
vertex	O
u	O
k	O
is	O
identified	O
by	O
a	O
sequence	O
of	O
tree	O
nodes	O
(	O
b0	O
,	O
b1	O
,	O
.	O

.	O

.	O

,	O
b	O
log	O
|V	O
|	O
)	O
,	O
(	O
b0	O
=	O
root	O
,	O
Now	O
,	O
Pr	O
(	O
b	O
l	O
|	O
Φ	O
(	O
vj	O
)	O
)	O
could	O
be	O
modeled	O
by	O
a	O
binary	Method
classifier	Method
that	O
is	O
assigned	O
to	O
the	O
parent	O
of	O
the	O
node	O
b	O
l	O
.	O

This	O
reduces	O
the	O
computational	Metric
complexity	Metric
of	O
calculating	O
Pr	Metric
(	O
We	O
can	O
speed	O
up	O
the	O
training	O
process	O
further	O
,	O
by	O
assigning	O
shorter	O
paths	O
to	O
the	O
frequent	O
vertices	O
in	O
the	O
random	O
walks	O
.	O

Huffman	Method
coding	Method
is	O
used	O
to	O
reduce	O
the	O
access	O
time	O
of	O
frequent	O
elements	O
in	O
the	O
tree	O
.	O

section	O
:	O
Optimization	Task
The	O
model	O
parameter	O
set	O
is	O
{	O
Φ	O
,	O
T	O
}	O
where	O
the	O
size	O
of	O
each	O
is	O
O	O
(	O
d|V	O
|	O
)	O
.	O

Stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
[	O
reference	O
]	O
is	O
used	O
to	O
optimize	O
these	O
parameters	O
(	O
Line	O
4	O
,	O
Algorithm	O
2	O
)	O
.	O

The	O
derivatives	O
are	O
estimated	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

The	O
learning	Metric
rate	Metric
α	Metric
for	O
SGD	Method
is	O
initially	O
set	O
to	O
2.5	O
%	O
at	O
the	O
beginning	O
of	O
the	O
training	O
and	O
then	O
decreased	O
linearly	O
(	O
v1	O
)	O
)	O
and	O
Pr	O
(	O
v5	O
|	O
Φ	O
(	O
v1	O
)	O
)	O
over	O
sequences	O
of	O
probability	O
distributions	O
corresponding	O
to	O
the	O
paths	O
starting	O
at	O
the	O
root	O
and	O
ending	O
at	O
v3	O
and	O
v5	O
.	O

The	O
representation	Method
Φ	Method
is	O
updated	O
to	O
maximize	O
the	O
probability	O
of	O
v1	O
co	O
-	O
occurring	O
with	O
its	O
context	O
{	O
v3	O
,	O
v5}.	O
with	O
the	O
number	O
of	O
vertices	O
that	O
are	O
seen	O
so	O
far	O
.	O

section	O
:	O
Parallelizability	O
As	O
shown	O
in	O
Figure	O
2	O
the	O
frequency	O
distribution	O
of	O
vertices	O
in	O
random	O
walks	O
of	O
social	O
network	O
and	O
words	O
in	O
a	O
language	O
both	O
follow	O
a	O
power	Method
law	Method
.	O

This	O
results	O
in	O
a	O
long	O
tail	O
of	O
infrequent	O
vertices	O
,	O
therefore	O
,	O
the	O
updates	O
that	O
affect	O
Φ	O
will	O
be	O
sparse	O
in	O
nature	O
.	O

This	O
allows	O
us	O
to	O
use	O
asynchronous	Method
version	Method
of	Method
stochastic	Method
gradient	Method
descent	Method
(	O
ASGD	Method
)	Method
,	O
in	O
the	O
multi	Task
-	Task
worker	Task
case	Task
.	O

Given	O
that	O
our	O
updates	O
are	O
sparse	O
and	O
we	O
do	O
not	O
acquire	O
a	O
lock	O
to	O
access	O
the	O
model	O
shared	O
parameters	O
,	O
ASGD	O
will	O
achieve	O
an	O
optimal	O
rate	O
of	O
convergence	Metric
[	O
reference	O
]	O
.	O

While	O
we	O
run	O
experiments	O
on	O
one	O
machine	O
using	O
multiple	O
threads	O
,	O
it	O
has	O
been	O
demonstrated	O
that	O
this	O
technique	O
is	O
highly	O
scalable	O
,	O
and	O
can	O
be	O
used	O
in	O
very	O
large	Task
scale	Task
machine	Task
learning	Task
[	O
reference	O
]	O
.	O

Figure	O
4	O
presents	O
the	O
effects	O
of	O
parallelizing	O
DeepWalk	Method
.	O

It	O
shows	O
the	O
speed	O
up	O
in	O
processing	O
BlogCatalog	Material
and	O
Flickr	O
networks	O
is	O
consistent	O
as	O
we	O
increase	O
the	O
number	O
of	O
workers	O
to	O
8	O
(	O
Figure	O
4a	O
)	O
.	O

It	O
also	O
shows	O
that	O
there	O
is	O
no	O
loss	O
of	O
predictive	Metric
performance	Metric
relative	O
to	O
the	O
running	O
DeepWalk	Method
serially	O
(	O
Figure	O
4b	O
)	O
.	O

section	O
:	O
Algorithm	O
Variants	O
Here	O
we	O
discuss	O
some	O
variants	O
of	O
our	O
proposed	O
method	O
,	O
which	O
we	O
believe	O
may	O
be	O
of	O
interest	O
.	O

section	O
:	O
Streaming	Task
One	O
interesting	O
variant	O
of	O
this	O
method	O
is	O
a	O
streaming	Method
approach	Method
,	O
which	O
could	O
be	O
implemented	O
without	O
knowledge	O
of	O
the	O
entire	O
graph	O
.	O

In	O
this	O
variant	O
small	O
walks	O
from	O
the	O
graph	O
are	O
passed	O
directly	O
to	O
the	O
representation	Method
learning	Method
code	Method
,	O
and	O
the	O
model	O
is	O
updated	O
directly	O
.	O

Some	O
modifications	O
to	O
the	O
learning	Method
process	Method
will	O
also	O
be	O
necessary	O
.	O

First	O
,	O
using	O
a	O
decaying	O
learning	O
rate	O
will	O
no	O
longer	O
be	O
possible	O
.	O

Instead	O
,	O
we	O
can	O
initialize	O
the	O
learning	Metric
rate	Metric
α	Metric
to	O
a	O
small	O
constant	O
value	O
.	O

This	O
will	O
take	O
longer	O
to	O
learn	O
,	O
but	O
may	O
be	O
worth	O
it	O
in	O
some	O
applications	O
.	O

Second	O
,	O
we	O
can	O
not	O
necessarily	O
build	O
a	O
tree	O
of	O
parameters	O
any	O
more	O
.	O

If	O
the	O
cardinality	O
of	O
V	O
is	O
known	O
(	O
or	O
can	O
be	O
bounded	O
)	O
,	O
we	O
can	O
build	O
the	O
Hierarchical	O
Softmax	O
tree	O
for	O
that	O
maximum	O
value	O
.	O

Vertices	O
can	O
be	O
assigned	O
to	O
one	O
of	O
the	O
remaining	O
leaves	O
when	O
they	O
are	O
first	O
seen	O
.	O

If	O
we	O
have	O
the	O
ability	O
to	O
estimate	O
the	O
vertex	O
frequency	O
a	O
priori	O
,	O
we	O
can	O
section	O
:	O
Non	Task
-	Task
random	Task
walks	Task
Some	O
graphs	O
are	O
created	O
as	O
a	O
by	O
-	O
product	O
of	O
agents	O
interacting	O
with	O
a	O
sequence	O
of	O
elements	O
(	O
e.g.	O
users	O
'	O
navigation	O
of	O
pages	O
on	O
a	O
website	O
)	O
.	O

When	O
a	O
graph	O
is	O
created	O
by	O
such	O
a	O
stream	O
of	O
non	Method
-	Method
random	Method
walks	Method
,	O
we	O
can	O
use	O
this	O
process	O
to	O
feed	O
the	O
modeling	Task
phase	Task
directly	O
.	O

Graphs	O
sampled	O
in	O
this	O
way	O
will	O
not	O
only	O
capture	O
information	O
related	O
to	O
network	O
structure	O
,	O
but	O
also	O
to	O
the	O
frequency	O
at	O
which	O
paths	O
are	O
traversed	O
.	O

In	O
our	O
view	O
,	O
this	O
variant	O
also	O
encompasses	O
language	Method
modeling	Method
.	O

Sentences	O
can	O
be	O
viewed	O
as	O
purposed	O
walks	O
through	O
an	O
appropriately	O
designed	O
language	Method
network	Method
,	O
and	O
language	Method
models	Method
like	O
SkipGram	Method
are	O
designed	O
to	O
capture	O
this	O
behavior	O
.	O

This	O
approach	O
can	O
be	O
combined	O
with	O
the	O
streaming	Method
variant	Method
(	O
Section	O
4.4.1	O
)	O
to	O
train	O
features	O
on	O
a	O
continually	Method
evolving	Method
network	Method
without	O
ever	O
explicitly	O
constructing	O
the	O
entire	O
graph	O
.	O

Maintaining	Method
representations	Method
with	O
this	O
technique	O
could	O
enable	O
web	Task
-	Task
scale	Task
classification	Task
without	O
the	O
hassles	O
of	O
dealing	O
with	O
a	O
web	O
-	O
scale	O
graph	O
.	O

section	O
:	O
EXPERIMENTAL	O
DESIGN	O
In	O
this	O
section	O
we	O
provide	O
an	O
overview	O
of	O
the	O
datasets	O
and	O
methods	O
which	O
we	O
will	O
use	O
in	O
our	O
experiments	O
.	O

Code	O
and	O
data	O
to	O
reproduce	O
our	O
results	O
will	O
be	O
available	O
at	O
the	O
first	O
author	O
's	O
website	O
.	O

An	O
overview	O
of	O
the	O
graphs	O
we	O
consider	O
in	O
our	O
experiments	O
is	O
given	O
in	O
Figure	O
1	O
.	O

section	O
:	O
Datasets	O
•	O
BlogCatalog	Material
[	O
reference	O
]	O
is	O
a	O
network	O
of	O
social	O
relationships	O
provided	O
by	O
blogger	O
authors	O
.	O

The	O
labels	O
represent	O
the	O
topic	O
categories	O
provided	O
by	O
the	O
authors	O
.	O

•	O
Flickr	Material
[	O
reference	O
]	O
is	O
a	O
network	O
of	O
the	O
contacts	O
between	O
users	O
of	O
the	O
photo	O
sharing	O
website	O
.	O

The	O
labels	O
represent	O
the	O
interest	O
groups	O
of	O
the	O
users	O
such	O
as	O
'	O
black	O
and	O
white	O
photos	O
'	O
.	O

•	O
YouTube	Material
[	O
reference	O
]	O
is	O
a	O
social	O
network	O
between	O
users	O
of	O
the	O
popular	O
video	O
sharing	O
website	O
.	O

The	O
labels	O
here	O
represent	O
groups	O
of	O
viewers	O
that	O
enjoy	O
common	O
video	O
genres	O
(	O
e.g.	O
anime	O
and	O
wrestling	O
)	O
.	O

section	O
:	O
Baseline	O
Methods	O
To	O
validate	O
the	O
performance	O
of	O
our	O
approach	O
we	O
compare	O
it	O
against	O
a	O
number	O
of	O
baselines	O
:	O
•	O
SpectralClustering	Method
[	O
reference	O
]	O
:	O
This	O
method	O
generates	O
a	O
representation	O
in	O
R	O
d	O
from	O
the	O
d	O
-	O
smallest	O
eigenvectors	O
of	O
L	O
,	O
the	O
normalized	O
graph	O
Laplacian	O
of	O
G.	O
Utilizing	O
the	O
eigenvectors	O
of	O
L	O
implicitly	O
assumes	O
that	O
graph	O
cuts	O
will	O
be	O
useful	O
for	O
classification	Task
.	O

•	O
Modularity	O
[	O
reference	O
]	O
:	O
This	O
method	O
generates	O
a	O
representation	O
in	O
R	O
d	O
from	O
the	O
top	O
-	O
d	O
eigenvectors	O
of	O
B	O
,	O
the	O
Modularity	O
matrix	O
of	O
G.	O
The	O
eigenvectors	O
of	O
B	O
encode	O
information	O
about	O
modular	O
graph	O
partitions	O
of	O
G	O
[	O
reference	O
]	O
.	O

Using	O
them	O
as	O
features	O
assumes	O
that	O
modular	O
graph	O
partitions	O
will	O
be	O
useful	O
for	O
classification	Task
.	O

•	O
EdgeCluster	Method
[	O
reference	O
]	O
:	O
This	O
method	O
uses	O
k	Method
-	Method
means	Method
clustering	Method
to	O
cluster	O
the	O
adjacency	O
matrix	O
of	O
G.	O
Its	O
has	O
been	O
shown	O
to	O
perform	O
comparably	O
to	O
the	O
Modularity	Method
method	Method
,	O
with	O
the	O
added	O
advantage	O
of	O
scaling	O
to	O
graphs	O
which	O
are	O
too	O
large	O
for	O
spectral	Method
decomposition	Method
.	O

•	O
wvRN	Method
[	O
reference	O
]	O
:	O
The	O
weighted	Method
-	Method
vote	Method
Relational	Method
Neighbor	Method
is	O
a	O
relational	Method
classifier	Method
.	O

Given	O
the	O
neighborhood	O
Ni	O
of	O
vertex	O
vi	O
,	O
wvRN	O
estimates	O
Pr	O
(	O
yi|Ni	O
)	O
with	O
the	O
(	O
appropriately	O
normalized	O
)	O
weighted	O
mean	O
of	O
its	O
neighbors	O
(	O
i.e	O
Pr	O
(	O
yi|Ni	O
)	O
=	O
wij	O
Pr	O
(	O
yj	O
|	O
Nj	O
)	O
)	O
.	O

It	O
has	O
shown	O
surprisingly	O
good	O
performance	O
in	O
real	Task
networks	Task
,	O
and	O
has	O
been	O
advocated	O
as	O
a	O
sensible	O
relational	O
classification	Task
baseline	O
[	O
reference	O
]	O
.	O

•	O
Majority	O
:	O
This	O
naïve	O
method	O
simply	O
chooses	O
the	O
most	O
frequent	O
labels	O
in	O
the	O
training	O
set	O
.	O

section	O
:	O
EXPERIMENTS	O
In	O
this	O
section	O
we	O
present	O
an	O
experimental	O
analysis	O
of	O
our	O
method	O
.	O

We	O
thoroughly	O
evaluate	O
it	O
on	O
a	O
number	O
of	O
multilabel	Task
classification	Task
tasks	Task
,	O
and	O
analyze	O
its	O
sensitivity	O
across	O
several	O
parameters	O
.	O

section	O
:	O
Multi	Task
-	Task
Label	Task
Classification	Task
To	O
facilitate	O
the	O
comparison	O
between	O
our	O
method	O
and	O
the	O
relevant	O
baselines	O
,	O
we	O
use	O
the	O
exact	O
same	O
datasets	O
and	O
experimental	O
procedure	O
as	O
in	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Specifically	O
,	O
we	O
randomly	O
sample	O
a	O
portion	O
(	O
TR	O
)	O
of	O
the	O
labeled	O
nodes	O
,	O
and	O
use	O
them	O
as	O
training	O
data	O
.	O

The	O
rest	O
of	O
the	O
nodes	O
are	O
used	O
as	O
test	O
.	O

We	O
repeat	O
this	O
process	O
10	O
times	O
,	O
and	O
report	O
the	O
average	O
performance	O
in	O
terms	O
of	O
both	O
Macro	Metric
-	Metric
F1	Metric
and	O
Micro	Metric
-	Metric
F1	Metric
.	O

When	O
possible	O
we	O
report	O
the	O
original	O
results	O
[	O
reference	O
][	O
reference	O
]	O
here	O
directly	O
.	O

For	O
all	O
models	O
we	O
use	O
a	O
one	Method
-	Method
vs	Method
-	Method
rest	Method
logistic	Method
regression	Method
implemented	O
by	O
LibLinear	Method
[	O
reference	O
]	O
section	O
:	O
BlogCatalog	Material
In	O
this	O
experiment	O
we	O
increase	O
the	O
training	Metric
ratio	Metric
(	O
TR	Metric
)	O
on	O
the	O
BlogCatalog	Material
network	O
from	O
10	O
%	O
to	O
90	O
%	O
.	O

Our	O
results	O
are	O
presented	O
in	O
Table	O
2	O
.	O

Numbers	O
in	O
bold	O
represent	O
the	O
highest	O
performance	O
in	O
each	O
column	O
.	O

DeepWalk	Method
performs	O
consistently	O
better	O
than	O
EdgeCluster	Method
,	O
Modularity	Method
,	O
and	O
wvRN	Method
.	O

In	O
fact	O
,	O
when	O
trained	O
with	O
only	O
20	O
%	O
of	O
the	O
nodes	O
labeled	O
,	O
DeepWalk	Method
performs	O
better	O
than	O
these	O
approaches	O
when	O
they	O
are	O
given	O
90	O
%	O
of	O
the	O
data	O
.	O

The	O
performance	O
of	O
SpectralClustering	Method
proves	O
much	O
more	O
competitive	O
,	O
but	O
DeepWalk	Method
still	O
outperforms	O
when	O
labeled	O
data	O
is	O
sparse	O
on	O
both	O
Macro	Metric
-	Metric
F1	Metric
(	O
TR	Metric
≤	O
20	O
%	O
)	O
and	O
Micro	Metric
-	Metric
F1	Metric
(	O
TR	Metric
≤	O
60	O
%	O
)	O
.	O

This	O
strong	O
performance	O
when	O
only	O
small	O
fractions	O
of	O
the	O
graph	O
are	O
labeled	O
is	O
a	O
core	O
strength	O
of	O
our	O
approach	O
.	O

In	O
the	O
following	O
experiments	O
,	O
we	O
investigate	O
the	O
performance	O
of	O
our	O
representations	O
on	O
even	O
more	O
sparsely	O
labeled	O
graphs	O
.	O

section	O
:	O
Flickr	Material
In	O
this	O
experiment	O
we	O
vary	O
the	O
training	Metric
ratio	Metric
(	O
TR	Metric
)	O
on	O
the	O
Flickr	Method
network	Method
from	O
1	O
%	O
to	O
10	O
%	O
.	O

This	O
corresponds	O
to	O
having	O
approximately	O
800	O
to	O
8	O
,	O
000	O
nodes	O
labeled	O
for	O
classification	Task
in	O
the	O
entire	O
network	O
.	O

Table	O
3	O
presents	O
our	O
results	O
,	O
which	O
are	O
consistent	O
with	O
the	O
previous	O
experiment	O
.	O

DeepWalk	Method
outperforms	O
all	O
baselines	O
by	O
at	O
least	O
3	O
%	O
with	O
respect	O
to	O
Micro	Metric
-	Metric
F1	Metric
.	O

Additionally	O
,	O
its	O
Micro	Metric
-	Metric
F1	Metric
performance	O
when	O
only	O
3	O
%	O
of	O
the	O
graph	O
is	O
labeled	O
beats	O
all	O
other	O
methods	O
even	O
when	O
they	O
have	O
been	O
given	O
10	O
%	O
of	O
the	O
data	O
.	O

In	O
other	O
words	O
,	O
DeepWalk	Method
can	O
outperform	O
the	O
baselines	O
with	O
60	O
%	O
less	O
training	O
data	O
.	O

It	O
also	O
performs	O
quite	O
well	O
in	O
Macro	Metric
-	Metric
F1	Metric
,	O
initially	O
performing	O
close	O
to	O
SpectralClustering	Method
,	O
but	O
distancing	O
itself	O
to	O
a	O
1	O
%	O
improvement	O
.	O

section	O
:	O
YouTube	Material
The	O
YouTube	Material
network	Material
is	O
considerably	O
larger	O
than	O
the	O
previous	O
ones	O
we	O
have	O
experimented	O
on	O
,	O
and	O
its	O
size	O
prevents	O
two	O
of	O
our	O
baseline	O
methods	O
(	O
SpectralClustering	Method
and	O
Modularity	Method
)	O
from	O
running	O
on	O
it	O
.	O

It	O
is	O
much	O
closer	O
to	O
a	O
real	O
world	O
graph	O
than	O
those	O
we	O
have	O
previously	O
considered	O
.	O

The	O
results	O
of	O
varying	O
the	O
training	Metric
ratio	Metric
(	O
TR	Metric
)	O
from	O
1	O
%	O
to	O
10	O
%	O
are	O
presented	O
in	O
Table	O
4	O
.	O

They	O
show	O
that	O
DeepWalk	Method
significantly	O
outperforms	O
the	O
scalable	O
baseline	O
for	O
creating	Task
graph	Task
representations	Task
,	O
EdgeCluster	O
.	O

When	O
1	O
%	O
of	O
the	O
labeled	O
nodes	O
are	O
used	O
for	O
test	O
,	O
the	O
Micro	Metric
-	Metric
F1	Metric
improves	O
by	O
14	O
%	O
.	O

The	O
Macro	Metric
-	Metric
F1	Metric
shows	O
a	O
corresponding	O
10	O
%	O
increase	O
.	O

This	O
lead	O
narrows	O
as	O
the	O
training	O
data	O
increases	O
,	O
but	O
DeepWalk	Method
ends	O
with	O
a	O
3	O
%	O
lead	O
in	O
Micro	Metric
-	Metric
F1	Metric
,	O
and	O
an	O
impressive	O
5	O
%	O
improvement	O
in	O
Macro	Metric
-	Metric
F1	Metric
.	O

This	O
experiment	O
showcases	O
the	O
performance	O
benefits	O
that	O
can	O
occur	O
from	O
using	O
social	Method
representation	Method
learning	Method
for	O
multilabel	Task
classification	Task
.	O

DeepWalk	Method
,	O
can	O
scale	O
to	O
large	O
graphs	O
,	O
and	O
performs	O
exceedingly	O
well	O
in	O
such	O
a	O
sparsely	O
labeled	O
environment	O
.	O

section	O
:	O
Parameter	Metric
Sensitivity	Metric
In	O
order	O
to	O
evaluate	O
how	O
changes	O
to	O
the	O
parameterization	O
of	O
DeepWalk	Method
effect	O
its	O
performance	O
on	O
classification	Task
tasks	Task
,	O
we	O
conducted	O
experiments	O
on	O
two	O
multi	Task
-	Task
label	Task
classifications	Task
tasks	Task
(	O
Flickr	Material
,	O
and	O
BlogCatalog	Material
)	O
.	O

For	O
this	O
test	O
,	O
we	O
have	O
fixed	O
the	O
window	O
size	O
and	O
the	O
walk	O
length	O
to	O
sensible	O
values	O
(	O
w	O
=	O
10	O
,	O
t	O
=	O
40	O
)	O
which	O
should	O
emphasize	O
local	O
structure	O
.	O

We	O
then	O
vary	O
the	O
number	O
of	O
latent	O
dimensions	O
(	O
d	O
)	O
,	O
the	O
number	O
of	O
walks	O
started	O
per	O
vertex	O
(	O
γ	O
)	O
,	O
and	O
the	O
amount	O
of	O
training	O
data	O
available	O
(	O
TR	O
)	O
to	O
determine	O
their	O
impact	O
on	O
the	O
network	Task
classification	Task
performance	O
.	O

Figure	O
5a	O
shows	O
the	O
effects	O
of	O
increasing	O
the	O
number	O
of	O
latent	O
dimensions	O
available	O
to	O
our	O
model	O
.	O

section	O
:	O
Effect	O
of	O
Dimensionality	Metric
Figures	O
5a1	O
and	O
5a3	O
examine	O
the	O
effects	O
of	O
varying	O
the	O
dimensionality	Metric
and	O
training	Metric
rate	Metric
.	O

The	O
performance	O
is	O
quite	O
consistent	O
between	O
both	O
Flickr	Material
and	O
BlogCatalog	Material
and	O
show	O
that	O
the	O
optimal	O
dimensionality	Metric
for	O
a	O
model	O
is	O
dependent	O
on	O
the	O
number	O
of	O
training	O
examples	O
.	O

(	O
Note	O
that	O
1	O
%	O
of	O
Flickr	Material
has	O
approximately	O
as	O
many	O
labeled	O
examples	O
as	O
10	O
%	O
of	O
BlogCatalog	Material
)	O
.	O

Figures	O
5a2	O
and	O
5a3	O
examine	O
the	O
effects	O
of	O
varying	O
the	O
dimensionality	O
and	O
number	O
of	O
walks	O
per	O
vertex	O
.	O

The	O
relative	O
performance	O
between	O
dimensions	O
is	O
relatively	O
stable	O
across	O
different	O
values	O
of	O
γ	O
.	O

These	O
charts	O
have	O
two	O
interesting	O
observations	O
.	O

The	O
first	O
is	O
that	O
there	O
is	O
most	O
of	O
the	O
benefit	O
is	O
accomplished	O
by	O
starting	O
γ	O
=	O
30	O
walks	O
per	O
node	O
in	O
both	O
graphs	O
.	O

The	O
second	O
is	O
that	O
the	O
relative	O
difference	O
between	O
different	O
values	O
of	O
γ	O
is	O
quite	O
consistent	O
between	O
the	O
two	O
graphs	O
.	O

Flickr	Method
has	O
an	O
order	O
of	O
magnitude	O
more	O
edges	O
than	O
BlogCatalog	Material
,	O
and	O
we	O
find	O
this	O
behavior	O
interesting	O
.	O

These	O
experiments	O
show	O
that	O
our	O
method	O
can	O
make	O
useful	O
models	O
of	O
various	O
sizes	O
.	O

They	O
also	O
show	O
that	O
the	O
performance	O
of	O
the	O
model	O
depends	O
on	O
the	O
number	O
of	O
random	O
walks	O
it	O
has	O
seen	O
,	O
and	O
the	O
appropriate	O
dimensionality	O
of	O
the	O
model	O
depends	O
on	O
the	O
training	O
examples	O
available	O
.	O

Figure	O
5a	O
shows	O
the	O
effects	O
of	O
increasing	O
γ	O
,	O
the	O
number	O
of	O
random	O
walks	O
that	O
we	O
start	O
from	O
each	O
vertex	O
.	O

section	O
:	O
Effect	O
of	O
sampling	O
frequency	O
The	O
results	O
are	O
very	O
consistent	O
for	O
different	O
dimensions	O
(	O
Fig	O
.	O

5b1	O
,	O
Fig	O
.	O

5b3	O
)	O
and	O
the	O
amount	O
of	O
training	O
data	O
(	O
Fig	O
.	O

5b2	O
,	O
Fig	O
.	O

5b4	O
)	O
.	O

Initially	O
,	O
increasing	O
γ	O
has	O
a	O
big	O
effect	O
in	O
the	O
results	O
,	O
but	O
this	O
effect	O
quickly	O
slows	O
(	O
γ	O
>	O
10	O
)	O
.	O

These	O
results	O
demonstrate	O
that	O
we	O
are	O
able	O
to	O
learn	O
meaningful	O
latent	O
representations	O
for	O
vertices	O
after	O
only	O
a	O
small	O
number	O
of	O
random	O
walks	O
.	O

section	O
:	O
RELATED	O
WORK	O
The	O
main	O
differences	O
between	O
our	O
proposed	O
method	O
and	O
previous	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
1	O
.	O

We	O
learn	O
our	O
latent	Method
social	Method
representations	Method
,	O
instead	O
of	O
computing	O
statistics	O
related	O
to	O
centrality	O
[	O
reference	O
]	O
or	O
partitioning	O
[	O
reference	O
]	O
.	O

2	O
.	O

We	O
do	O
not	O
attempt	O
to	O
extend	O
the	O
classification	Task
procedure	O
itself	O
(	O
through	O
collective	Method
inference	Method
[	O
reference	O
]	O
or	O
graph	Method
kernels	Method
[	O
reference	O
]	O
)	O
.	O

3	O
.	O

We	O
propose	O
a	O
scalable	Method
online	Method
method	Method
which	O
uses	O
only	O
local	O
information	O
.	O

Most	O
methods	O
require	O
global	O
information	O
and	O
are	O
offline	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

4	O
.	O

We	O
apply	O
unsupervised	Method
representation	Method
learning	Method
to	O
graphs	O
.	O

In	O
this	O
section	O
we	O
discuss	O
related	O
work	O
in	O
network	Task
classification	Task
and	O
unsupervised	Task
feature	Task
learning	Task
.	O

section	O
:	O
Relational	Task
Learning	Task
Relational	O
classification	Task
(	O
or	O
collective	O
classification	Task
)	O
methods	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
use	O
links	O
between	O
data	O
items	O
as	O
part	O
of	O
the	O
classification	Task
process	Task
.	O

Exact	Task
inference	Task
in	O
the	O
collective	Task
classification	Task
problem	Task
is	O
NP	Task
-	Task
hard	Task
,	O
and	O
solutions	O
have	O
focused	O
on	O
the	O
use	O
of	O
approximate	Method
inference	Method
algorithm	Method
which	O
may	O
not	O
be	O
guaranteed	O
to	O
converge	O
[	O
reference	O
]	O
.	O

The	O
most	O
relevant	O
relational	O
classification	Task
algorithms	O
to	O
our	O
work	O
incorporate	O
community	O
information	O
by	O
learning	O
clusters	O
[	O
reference	O
]	O
,	O
by	O
adding	O
edges	O
between	O
nearby	O
nodes	O
[	O
reference	O
]	O
,	O
by	O
using	O
PageRank	Method
[	O
reference	O
]	O
,	O
or	O
by	O
extending	O
relational	Task
classification	Task
to	O
take	O
additional	O
features	O
into	O
account	O
[	O
reference	O
]	O
.	O

Our	O
work	O
takes	O
a	O
substantially	O
different	O
approach	O
.	O

Instead	O
of	O
a	O
new	O
approximation	Method
inference	Method
algorithm	Method
,	O
we	O
propose	O
a	O
procedure	O
which	O
learns	O
representations	Method
of	Method
network	Method
structure	Method
which	O
can	O
then	O
be	O
used	O
by	O
existing	O
inference	Method
procedure	Method
(	O
including	O
iterative	Method
ones	Method
)	O
.	O

A	O
number	O
of	O
techniques	O
for	O
generating	Task
features	Task
from	O
graphs	O
have	O
also	O
been	O
proposed	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

In	O
contrast	O
to	O
these	O
methods	O
,	O
we	O
frame	O
the	O
feature	Task
creation	Task
procedure	Task
as	O
a	O
representation	Task
learning	Task
problem	Task
.	O

Graph	Method
Kernels	Method
[	O
reference	O
]	O
have	O
been	O
proposed	O
as	O
a	O
way	O
to	O
use	O
relational	O
data	O
as	O
part	O
of	O
the	O
classification	Task
process	Task
,	O
but	O
are	O
quite	O
slow	O
unless	O
approximated	O
[	O
reference	O
]	O
.	O

Our	O
approach	O
is	O
complementary	O
;	O
instead	O
of	O
encoding	O
the	O
structure	O
as	O
part	O
of	O
a	O
kernel	Method
function	Method
,	O
we	O
learn	O
a	O
representation	O
which	O
allows	O
them	O
to	O
be	O
used	O
directly	O
as	O
features	O
for	O
any	O
classification	Task
method	O
.	O

section	O
:	O
Unsupervised	Task
Feature	Task
Learning	Task
Distributed	Method
representations	Method
have	O
been	O
proposed	O
to	O
model	O
structural	O
relationship	O
between	O
concepts	O
[	O
reference	O
]	O
.	O

These	O
representations	O
are	O
trained	O
by	O
the	O
back	Method
-	Method
propagation	Method
and	O
gradient	Method
descent	Method
.	O

Computational	Metric
costs	Metric
and	O
numerical	Metric
instability	Metric
led	O
to	O
these	O
techniques	O
to	O
be	O
abandoned	O
for	O
almost	O
a	O
decade	O
.	O

Recently	O
,	O
distributed	Task
computing	Task
allowed	O
for	O
larger	O
models	O
to	O
be	O
trained	O
[	O
reference	O
]	O
,	O
and	O
the	O
growth	O
of	O
data	O
for	O
unsupervised	Method
learning	Method
algorithms	Method
to	O
emerge	O
[	O
reference	O
]	O
.	O

Distributed	Method
representations	Method
usually	O
are	O
trained	O
through	O
neural	Method
networks	Method
,	O
these	O
networks	O
have	O
made	O
advancements	O
in	O
diverse	O
fields	O
such	O
as	O
computer	Task
vision	Task
[	O
reference	O
]	O
,	O
speech	Task
recognition	Task
[	O
reference	O
]	O
,	O
and	O
natural	Task
language	Task
processing	Task
[	O
reference	O
]	O
.	O

section	O
:	O
CONCLUSIONS	O
We	O
propose	O
DeepWalk	Method
,	O
a	O
novel	O
approach	O
for	O
learning	O
latent	Task
social	Task
representations	Task
of	Task
vertices	Task
.	O

Using	O
local	O
information	O
from	O
truncated	O
random	O
walks	O
as	O
input	O
,	O
our	O
method	O
learns	O
a	O
representation	O
which	O
encodes	O
structural	O
regularities	O
.	O

Experiments	O
on	O
a	O
variety	O
of	O
different	O
graphs	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
on	O
challenging	O
multi	Task
-	Task
label	Task
classification	Task
tasks	Task
.	O

As	O
an	O
online	Method
algorithm	Method
,	O
DeepWalk	Method
is	O
also	O
scalable	O
.	O

Our	O
results	O
show	O
that	O
we	O
can	O
create	O
meaningful	Method
representations	Method
for	O
graphs	Task
too	O
large	O
to	O
run	O
spectral	Method
methods	Method
on	O
.	O

On	O
such	O
large	O
graphs	O
,	O
our	O
method	O
significantly	O
outperforms	O
other	O
methods	O
designed	O
to	O
operate	O
for	O
sparsity	Task
.	O

We	O
also	O
show	O
that	O
our	O
approach	O
is	O
parallelizable	O
,	O
allowing	O
workers	O
to	O
update	O
different	O
parts	O
of	O
the	O
model	O
concurrently	O
.	O

In	O
addition	O
to	O
being	O
effective	O
and	O
scalable	O
,	O
our	O
approach	O
is	O
also	O
an	O
appealing	O
generalization	Method
of	Method
language	Method
modeling	Method
.	O

This	O
connection	O
is	O
mutually	O
beneficial	O
.	O

Advances	O
in	O
language	Method
modeling	Method
may	O
continue	O
to	O
generate	O
improved	O
latent	Method
representations	Method
for	O
networks	Task
.	O

In	O
our	O
view	O
,	O
language	Task
modeling	Task
is	O
actually	O
sampling	O
from	O
an	O
unobservable	O
language	O
graph	O
.	O

We	O
believe	O
that	O
insights	O
obtained	O
from	O
modeling	O
observable	O
graphs	O
may	O
in	O
turn	O
yield	O
improvements	O
to	O
modeling	O
unobservable	O
ones	O
.	O

Our	O
future	O
work	O
in	O
the	O
area	O
will	O
focus	O
on	O
investigating	O
this	O
duality	O
further	O
,	O
using	O
our	O
results	O
to	O
improve	O
language	Task
modeling	Task
,	O
and	O
strengthening	O
the	O
theoretical	O
justifications	O
of	O
the	O
method	O
.	O

section	O
:	O
Feature	Method
Pyramid	Method
Networks	Method
for	O
Object	Task
Detection	Task
section	O
:	O
Abstract	O
Feature	Method
pyramids	Method
are	O
a	O
basic	O
component	O
in	O
recognition	Method
systems	Method
for	O
detecting	Task
objects	Task
at	O
different	O
scales	O
.	O

But	O
recent	O
deep	Method
learning	Method
object	Method
detectors	Method
have	O
avoided	O
pyramid	Method
representations	Method
,	O
in	O
part	O
because	O
they	O
are	O
compute	O
and	O
memory	O
intensive	O
.	O

In	O
this	O
paper	O
,	O
we	O
exploit	O
the	O
inherent	O
multi	O
-	O
scale	O
,	O
pyramidal	O
hierarchy	O
of	O
deep	Method
convolutional	Method
networks	Method
to	O
construct	O
feature	O
pyramids	O
with	O
marginal	O
extra	O
cost	O
.	O

A	O
topdown	Method
architecture	Method
with	O
lateral	O
connections	O
is	O
developed	O
for	O
building	O
high	Task
-	Task
level	Task
semantic	Task
feature	Task
maps	Task
at	O
all	O
scales	O
.	O

This	O
architecture	O
,	O
called	O
a	O
Feature	Method
Pyramid	Method
Network	Method
(	O
FPN	Method
)	O
,	O
shows	O
significant	O
improvement	O
as	O
a	O
generic	O
feature	Method
extractor	Method
in	O
several	O
applications	O
.	O

Using	O
FPN	Method
in	O
a	O
basic	O
Faster	Method
R	Method
-	Method
CNN	Method
system	Method
,	O
our	O
method	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
singlemodel	O
results	O
on	O
the	O
COCO	Material
detection	Material
benchmark	Material
without	O
bells	O
and	O
whistles	O
,	O
surpassing	O
all	O
existing	O
single	O
-	O
model	O
entries	O
including	O
those	O
from	O
the	O
COCO	Material
2016	O
challenge	O
winners	O
.	O

In	O
addition	O
,	O
our	O
method	O
can	O
run	O
at	O
6	O
FPS	O
on	O
a	O
GPU	O
and	O
thus	O
is	O
a	O
practical	O
and	O
accurate	O
solution	O
to	O
multi	Task
-	Task
scale	Task
object	Task
detection	Task
.	O

Code	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Introduction	O
Recognizing	Task
objects	Task
at	O
vastly	O
different	O
scales	O
is	O
a	O
fundamental	O
challenge	O
in	O
computer	Task
vision	Task
.	O

Feature	Method
pyramids	Method
built	O
upon	O
image	Method
pyramids	Method
(	O
for	O
short	O
we	O
call	O
these	O
featurized	Method
image	Method
pyramids	Method
)	O
form	O
the	O
basis	O
of	O
a	O
standard	O
solution	O
[	O
reference	O
]	O
(	O
Fig	O
.	O

1	O
(	O
a	O
)	O
)	O
.	O

These	O
pyramids	O
are	O
scale	O
-	O
invariant	O
in	O
the	O
sense	O
that	O
an	O
object	O
's	O
scale	O
change	O
is	O
offset	O
by	O
shifting	O
its	O
level	O
in	O
the	O
pyramid	O
.	O

Intuitively	O
,	O
this	O
property	O
enables	O
a	O
model	O
to	O
detect	O
objects	O
across	O
a	O
large	O
range	O
of	O
scales	O
by	O
scanning	O
the	O
model	O
over	O
both	O
positions	O
and	O
pyramid	O
levels	O
.	O

Featurized	Method
image	Method
pyramids	Method
were	O
heavily	O
used	O
in	O
the	O
era	O
of	O
hand	O
-	O
engineered	O
features	O
[	O
reference	O
][	O
reference	O
]	O
.	O

They	O
were	O
so	O
critical	O
that	O
object	Method
detectors	Method
like	O
DPM	Method
[	O
reference	O
]	O
required	O
dense	Method
scale	Method
sampling	Method
to	O
achieve	O
good	O
results	O
(	O
e.g.	O
,	O
10	O
scales	O
per	O
octave	O
)	O
.	O

For	O
recognition	Task
tasks	Task
,	O
engineered	O
features	O
have	O
largely	O
been	O
replaced	O
with	O
features	O
computed	O
by	O
deep	Method
convolutional	Method
networks	Method
(	O
ConvNets	Method
)	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Aside	O
from	O
being	O
capable	O
of	O
representing	O
higher	O
-	O
level	O
semantics	O
,	O
ConvNets	Method
are	O
also	O
more	O
robust	O
to	O
variance	O
in	O
scale	O
and	O
thus	O
facilitate	O
recognition	Task
from	O
features	O
computed	O
on	O
a	O
single	O
input	O
scale	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
.	O

But	O
even	O
with	O
this	O
robustness	O
,	O
pyramids	O
are	O
still	O
needed	O
to	O
get	O
the	O
most	O
accurate	O
results	O
.	O

All	O
recent	O
top	O
entries	O
in	O
the	O
ImageNet	Material
[	O
reference	O
]	O
and	O
COCO	Material
[	O
reference	O
]	O
detection	Task
challenges	Task
use	O
multi	Method
-	Method
scale	Method
testing	Method
on	O
featurized	Method
image	Method
pyramids	Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

The	O
principle	O
advantage	O
of	O
featurizing	O
each	O
level	O
of	O
an	O
image	Method
pyramid	Method
is	O
that	O
it	O
produces	O
a	O
multi	Method
-	Method
scale	Method
feature	Method
representation	Method
in	O
which	O
all	O
levels	O
are	O
semantically	O
strong	O
,	O
including	O
the	O
high	O
-	O
resolution	O
levels	O
.	O

Nevertheless	O
,	O
featurizing	O
each	O
level	O
of	O
an	O
image	Method
pyramid	Method
has	O
obvious	O
limitations	O
.	O

Inference	Metric
time	Metric
increases	O
considerably	O
(	O
e.g.	O
,	O
by	O
four	O
times	O
[	O
reference	O
]	O
)	O
,	O
making	O
this	O
approach	O
impractical	O
for	O
real	O
applications	O
.	O

Moreover	O
,	O
training	O
deep	Method
networks	Method
end	O
-	O
to	O
-	O
end	O
on	O
an	O
image	O
pyramid	O
is	O
infeasible	O
in	O
terms	O
of	O
memory	O
,	O
and	O
so	O
,	O
if	O
exploited	O
,	O
image	O
pyramids	O
are	O
used	O
only	O
at	O
test	O
time	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
which	O
creates	O
an	O
inconsistency	O
between	O
train	O
/	O
test	O
-	O
time	Method
inference	Method
.	O

For	O
these	O
reasons	O
,	O
Fast	Method
and	O
Faster	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
][	O
reference	O
]	O
opt	O
to	O
not	O
use	O
featurized	O
image	O
pyramids	O
under	O
default	O
settings	O
.	O

However	O
,	O
image	Method
pyramids	Method
are	O
not	O
the	O
only	O
way	O
to	O
compute	O
a	O
multi	Method
-	Method
scale	Method
feature	Method
representation	Method
.	O

A	O
deep	Method
ConvNet	Method
computes	O
a	O
feature	Method
hierarchy	Method
layer	Method
by	Method
layer	Method
,	O
and	O
with	O
subsampling	Method
layers	Method
the	O
feature	O
hierarchy	O
has	O
an	O
inherent	O
multiscale	O
,	O
pyramidal	O
shape	O
.	O

This	O
in	O
-	O
network	O
feature	O
hierarchy	O
produces	O
feature	O
maps	O
of	O
different	O
spatial	O
resolutions	O
,	O
but	O
introduces	O
large	O
semantic	O
gaps	O
caused	O
by	O
different	O
depths	O
.	O

The	O
high	O
-	O
resolution	O
maps	O
have	O
low	O
-	O
level	O
features	O
that	O
harm	O
their	O
representational	Method
capacity	Method
for	O
object	Task
recognition	Task
.	O

The	O
Single	Method
Shot	Method
Detector	Method
(	O
SSD	Method
)	O
[	O
reference	O
]	O
is	O
one	O
of	O
the	O
first	O
attempts	O
at	O
using	O
a	O
ConvNet	O
's	O
pyramidal	O
feature	O
hierarchy	O
as	O
if	O
it	O
were	O
a	O
featurized	O
image	O
pyramid	O
(	O
Fig	O
.	O

1	O
(	O
c	O
)	O
)	O
.	O

Ideally	O
,	O
the	O
SSD	Method
-	Method
style	Method
pyramid	Method
would	O
reuse	O
the	O
multi	O
-	O
scale	O
feature	O
maps	O
from	O
different	O
layers	O
computed	O
in	O
the	O
forward	O
pass	O
and	O
thus	O
come	O
free	O
of	O
cost	O
.	O

But	O
to	O
avoid	O
using	O
low	O
-	O
level	O
features	O
SSD	O
foregoes	O
reusing	O
already	O
computed	O
layers	O
and	O
instead	O
builds	O
the	O
pyramid	O
starting	O
from	O
high	O
up	O
in	O
the	O
network	O
(	O
e.g.	O
,	O
conv4	O
3	O
of	O
VGG	Method
nets	Method
[	O
reference	O
]	O
)	O
and	O
then	O
by	O
adding	O
several	O
new	O
layers	O
.	O

Thus	O
it	O
misses	O
the	O
opportunity	O
to	O
reuse	O
the	O
higher	O
-	O
resolution	O
maps	O
of	O
the	O
feature	O
hierarchy	O
.	O

We	O
show	O
that	O
these	O
are	O
important	O
for	O
detecting	Task
small	Task
objects	Task
.	O

The	O
goal	O
of	O
this	O
paper	O
is	O
to	O
naturally	O
leverage	O
the	O
pyramidal	O
shape	O
of	O
a	O
ConvNet	O
's	O
feature	O
hierarchy	O
while	O
creating	O
a	O
feature	Method
pyramid	Method
that	O
has	O
strong	O
semantics	O
at	O
all	O
scales	O
.	O

To	O
achieve	O
this	O
goal	O
,	O
we	O
rely	O
on	O
an	O
architecture	O
that	O
combines	O
low	O
-	O
resolution	O
,	O
semantically	O
strong	O
features	O
with	O
high	O
-	O
resolution	O
,	O
semantically	O
weak	O
features	O
via	O
a	O
top	Method
-	Method
down	Method
pathway	Method
and	O
lateral	Method
connections	Method
(	O
Fig	O
.	O

1	O
(	O
d	O
)	O
)	O
.	O

The	O
result	O
is	O
a	O
feature	Method
pyramid	Method
that	O
has	O
rich	O
semantics	O
at	O
all	O
levels	O
and	O
is	O
built	O
quickly	O
from	O
a	O
single	O
input	O
image	O
scale	O
.	O

In	O
other	O
words	O
,	O
we	O
show	O
how	O
to	O
create	O
in	Method
-	Method
network	Method
feature	Method
pyramids	Method
that	O
can	O
be	O
used	O
to	O
replace	O
featurized	O
image	O
pyramids	O
without	O
sacrificing	O
representational	O
power	O
,	O
speed	O
,	O
or	O
memory	O
.	O

Similar	O
architectures	O
adopting	O
top	Method
-	Method
down	Method
and	Method
skip	Method
connections	Method
are	O
popular	O
in	O
recent	O
research	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Their	O
goals	O
are	O
to	O
produce	O
a	O
single	O
high	O
-	O
level	O
feature	O
map	O
of	O
a	O
fine	O
resolution	O
on	O
which	O
the	O
predictions	O
are	O
to	O
be	O
made	O
(	O
Fig	O
.	O

2	O
top	O
)	O
.	O

On	O
the	O
contrary	O
,	O
our	O
method	O
leverages	O
the	O
architecture	O
as	O
a	O
feature	O
pyramid	O
where	O
predictions	O
(	O
e.g.	O
,	O
object	O
detections	O
)	O
are	O
independently	O
made	O
on	O
each	O
level	O
(	O
Fig	O
.	O

2	O
bottom	O
)	O
.	O

Our	O
model	O
echoes	O
a	O
featurized	Method
image	Method
pyramid	Method
,	O
which	O
has	O
not	O
been	O
explored	O
in	O
these	O
works	O
.	O

We	O
evaluate	O
our	O
method	O
,	O
called	O
a	O
Feature	Method
Pyramid	Method
Network	Method
(	O
FPN	Method
)	O
,	O
in	O
various	O
systems	O
for	O
detection	Task
and	Task
segmentation	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Without	O
bells	O
and	O
whistles	O
,	O
we	O
report	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
-	O
model	O
result	O
on	O
the	O
challenging	O
COCO	Material
detection	Material
benchmark	Material
[	O
reference	O
]	O
simply	O
based	O
on	O
FPN	Method
and	O
predict	O
predict	O
predict	O
predict	O
Figure	O
2	O
.	O

Top	O
:	O
a	O
top	Method
-	Method
down	Method
architecture	Method
with	O
skip	O
connections	O
,	O
where	O
predictions	O
are	O
made	O
on	O
the	O
finest	O
level	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
.	O

Bottom	O
:	O
our	O
model	O
that	O
has	O
a	O
similar	O
structure	O
but	O
leverages	O
it	O
as	O
a	O
feature	O
pyramid	O
,	O
with	O
predictions	O
made	O
independently	O
at	O
all	O
levels	O
.	O

a	O
basic	O
Faster	Method
R	Method
-	Method
CNN	Method
detector	O
[	O
reference	O
]	O
,	O
surpassing	O
all	O
existing	O
heavily	O
-	O
engineered	O
single	O
-	O
model	O
entries	O
of	O
competition	O
winners	O
.	O

In	O
ablation	O
experiments	O
,	O
we	O
find	O
that	O
for	O
bounding	Task
box	Task
proposals	Task
,	O
FPN	Method
significantly	O
increases	O
the	O
Average	Metric
Recall	Metric
(	O
AR	Metric
)	O
by	O
8.0	O
points	O
;	O
for	O
object	Task
detection	Task
,	O
it	O
improves	O
the	O
COCO	Material
-	O
style	O
Average	Metric
Precision	Metric
(	O
AP	Metric
)	O
by	O
2.3	O
points	O
and	O
PASCAL	O
-	O
style	O
AP	Metric
by	O
3.8	O
points	O
,	O
over	O
a	O
strong	O
single	O
-	O
scale	O
baseline	O
of	O
Faster	Method
R	Method
-	Method
CNN	Method
on	O
ResNets	Method
[	O
reference	O
]	O
.	O

Our	O
method	O
is	O
also	O
easily	O
extended	O
to	O
mask	Task
proposals	Task
and	O
improves	O
both	O
instance	O
segmentation	O
AR	Metric
and	O
speed	Metric
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
that	O
heavily	O
depend	O
on	O
image	O
pyramids	O
.	O

In	O
addition	O
,	O
our	O
pyramid	Method
structure	Method
can	O
be	O
trained	O
end	O
-	O
toend	O
with	O
all	O
scales	O
and	O
is	O
used	O
consistently	O
at	O
train	Metric
/	Metric
test	Metric
time	Metric
,	O
which	O
would	O
be	O
memory	O
-	O
infeasible	O
using	O
image	Method
pyramids	Method
.	O

As	O
a	O
result	O
,	O
FPNs	Method
are	O
able	O
to	O
achieve	O
higher	O
accuracy	Metric
than	O
all	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Moreover	O
,	O
this	O
improvement	O
is	O
achieved	O
without	O
increasing	O
testing	Metric
time	Metric
over	O
the	O
single	O
-	O
scale	O
baseline	O
.	O

We	O
believe	O
these	O
advances	O
will	O
facilitate	O
future	O
research	O
and	O
applications	O
.	O

Our	O
code	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Related	O
Work	O
Hand	O
-	O
engineered	O
features	O
and	O
early	Method
neural	Method
networks	Method
.	O

SIFT	O
features	O
[	O
reference	O
]	O
were	O
originally	O
extracted	O
at	O
scale	O
-	O
space	O
extrema	O
and	O
used	O
for	O
feature	Task
point	Task
matching	Task
.	O

HOG	O
features	O
[	O
reference	O
]	O
,	O
and	O
later	O
SIFT	O
features	O
as	O
well	O
,	O
were	O
computed	O
densely	O
over	O
entire	O
image	O
pyramids	O
.	O

These	O
HOG	Method
and	Method
SIFT	Method
pyramids	Method
have	O
been	O
used	O
in	O
numerous	O
works	O
for	O
image	O
classification	Method
,	O
object	Task
detection	Task
,	O
human	Task
pose	Task
estimation	Task
,	O
and	O
more	O
.	O

There	O
has	O
also	O
been	O
significant	O
interest	O
in	O
computing	O
featurized	Task
image	Task
pyramids	Task
quickly	O
.	O

Dollár	O
et	O
al	O
.	O

[	O
reference	O
]	O
demonstrated	O
fast	O
pyramid	Task
computation	Task
by	O
first	O
computing	O
a	O
sparsely	Method
sampled	Method
(	Method
in	Method
scale	Method
)	Method
pyramid	Method
and	O
then	O
interpolating	O
missing	O
levels	O
.	O

Before	O
HOG	Method
and	O
SIFT	Method
,	O
early	O
work	O
on	O
face	Task
detection	Task
with	O
ConvNets	Method
[	O
reference	O
][	O
reference	O
]	O
computed	O
shallow	Method
networks	Method
over	O
image	O
pyramids	O
to	O
detect	O
faces	O
across	O
scales	O
.	O

Deep	Method
ConvNet	Method
object	Method
detectors	Method
.	O

With	O
the	O
development	O
of	O
modern	O
deep	Method
ConvNets	Method
[	O
reference	O
]	O
,	O
object	Method
detectors	Method
like	O
OverFeat	Method
[	O
reference	O
]	O
and	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
showed	O
dramatic	O
improvements	O
in	O
accuracy	Metric
.	O

OverFeat	O
adopted	O
a	O
strategy	O
similar	O
to	O
early	O
neural	Method
network	Method
face	Method
detectors	Method
by	O
applying	O
a	O
ConvNet	Method
as	O
a	O
sliding	Method
window	Method
detector	Method
on	O
an	O
image	O
pyramid	O
.	O

R	Method
-	Method
CNN	Method
adopted	O
a	O
region	Method
proposal	Method
-	Method
based	Method
strategy	Method
[	O
reference	O
]	O
in	O
which	O
each	O
proposal	O
was	O
scale	O
-	O
normalized	O
before	O
classifying	O
with	O
a	O
ConvNet	Method
.	O

SPPnet	Method
[	O
reference	O
]	O
demonstrated	O
that	O
such	O
region	Method
-	Method
based	Method
detectors	Method
could	O
be	O
applied	O
much	O
more	O
efficiently	O
on	O
feature	O
maps	O
extracted	O
on	O
a	O
single	O
image	O
scale	O
.	O

Recent	O
and	O
more	O
accurate	O
detection	Method
methods	Method
like	O
Fast	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
and	O
Faster	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
advocate	O
using	O
features	O
computed	O
from	O
a	O
single	O
scale	O
,	O
because	O
it	O
offers	O
a	O
good	O
trade	O
-	O
off	O
between	O
accuracy	Metric
and	O
speed	Metric
.	O

Multi	Task
-	Task
scale	Task
detection	Task
,	O
however	O
,	O
still	O
performs	O
better	O
,	O
especially	O
for	O
small	O
objects	O
.	O

Methods	O
using	O
multiple	O
layers	O
.	O

A	O
number	O
of	O
recent	O
approaches	O
improve	O
detection	Task
and	Task
segmentation	Task
by	O
using	O
different	O
layers	O
in	O
a	O
ConvNet	Method
.	O

FCN	Method
[	O
reference	O
]	O
sums	O
partial	O
scores	O
for	O
each	O
category	O
over	O
multiple	O
scales	O
to	O
compute	O
semantic	Task
segmentations	Task
.	O

Hypercolumns	Method
[	O
reference	O
]	O
uses	O
a	O
similar	O
method	O
for	O
object	Task
instance	Task
segmentation	Task
.	O

Several	O
other	O
approaches	O
(	O
HyperNet	Method
[	O
reference	O
]	O
,	O
ParseNet	Method
[	O
reference	O
]	O
,	O
and	O
ION	Method
[	O
reference	O
]	O
)	O
concatenate	O
features	O
of	O
multiple	O
layers	O
before	O
computing	O
predictions	Task
,	O
which	O
is	O
equivalent	O
to	O
summing	O
transformed	O
features	O
.	O

SSD	Method
[	O
reference	O
]	O
and	O
MS	Method
-	Method
CNN	Method
[	O
reference	O
]	O
predict	O
objects	O
at	O
multiple	O
layers	O
of	O
the	O
feature	O
hierarchy	O
without	O
combining	O
features	O
or	O
scores	O
.	O

There	O
are	O
recent	O
methods	O
exploiting	O
lateral	O
/	O
skip	O
connections	O
that	O
associate	O
low	O
-	O
level	O
feature	O
maps	O
across	O
resolutions	O
and	O
semantic	O
levels	O
,	O
including	O
U	Method
-	Method
Net	Method
[	O
reference	O
]	O
and	O
SharpMask	Method
[	O
reference	O
]	O
for	O
segmentation	Task
,	O
Recombinator	Method
networks	Method
[	O
reference	O
]	O
for	O
face	Task
detection	Task
,	O
and	O
Stacked	Method
Hourglass	Method
networks	Method
[	O
reference	O
]	O
for	O
keypoint	Task
estimation	Task
.	O

Ghiasi	O
et	O
al	O
.	O

[	O
reference	O
]	O
present	O
a	O
Laplacian	Method
pyramid	Method
presentation	Method
for	O
FCNs	Method
to	O
progressively	O
refine	O
segmentation	Task
.	O

Although	O
these	O
methods	O
adopt	O
architectures	O
with	O
pyramidal	O
shapes	O
,	O
they	O
are	O
unlike	O
featurized	Method
image	Method
pyramids	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
where	O
predictions	O
are	O
made	O
independently	O
at	O
all	O
levels	O
,	O
see	O
Fig	O
.	O

2	O
.	O

In	O
fact	O
,	O
for	O
the	O
pyramidal	O
architecture	O
in	O
Fig	O
.	O

2	O
(	O
top	O
)	O
,	O
image	O
pyramids	O
are	O
still	O
needed	O
to	O
recognize	O
objects	O
across	O
multiple	O
scales	O
[	O
reference	O
]	O
.	O

section	O
:	O
Feature	Method
Pyramid	Method
Networks	Method
Our	O
goal	O
is	O
to	O
leverage	O
a	O
ConvNet	O
's	O
pyramidal	O
feature	O
hierarchy	O
,	O
which	O
has	O
semantics	O
from	O
low	O
to	O
high	O
levels	O
,	O
and	O
build	O
a	O
feature	O
pyramid	O
with	O
high	O
-	O
level	O
semantics	O
throughout	O
.	O

The	O
resulting	O
Feature	Method
Pyramid	Method
Network	Method
is	O
generalpurpose	O
and	O
in	O
this	O
paper	O
we	O
focus	O
on	O
sliding	Method
window	Method
proposers	Method
(	O
Region	Method
Proposal	Method
Network	Method
,	O
RPN	Method
for	O
short	O
)	O
[	O
reference	O
]	O
and	O
region	Method
-	Method
based	Method
detectors	Method
(	O
Fast	Method
R	Method
-	Method
CNN	Method
)	O
[	O
reference	O
]	O
.	O

We	O
also	O
generalize	O
FPNs	Method
to	O
instance	Task
segmentation	Task
proposals	Task
in	O
Sec	O
.	O

6	O
.	O

Our	O
method	O
takes	O
a	O
single	O
-	O
scale	O
image	O
of	O
an	O
arbitrary	O
size	O
as	O
input	O
,	O
and	O
outputs	O
proportionally	O
sized	O
feature	O
maps	O
at	O
multiple	O
levels	O
,	O
in	O
a	O
fully	Method
convolutional	Method
fashion	Method
.	O

This	O
process	O
is	O
independent	O
of	O
the	O
backbone	Method
convolutional	Method
architectures	Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
,	O
and	O
in	O
this	O
paper	O
we	O
present	O
results	O
using	O
ResNets	Method
[	O
reference	O
]	O
.	O

The	O
construction	O
of	O
our	O
pyramid	O
involves	O
a	O
bottom	O
-	O
up	O
pathway	O
,	O
a	O
top	O
-	O
down	O
pathway	O
,	O
and	O
lateral	O
connections	O
,	O
as	O
introduced	O
in	O
the	O
following	O
.	O

Bottom	O
-	O
up	O
pathway	O
.	O

The	O
bottom	O
-	O
up	O
pathway	O
is	O
the	O
feedforward	Method
computation	Method
of	O
the	O
backbone	Method
ConvNet	Method
,	O
which	O
computes	O
a	O
feature	O
hierarchy	O
consisting	O
of	O
feature	O
maps	O
at	O
several	O
scales	O
with	O
a	O
scaling	O
step	O
of	O
2	O
.	O

There	O
are	O
often	O
many	O
layers	O
producing	O
output	O
maps	O
of	O
the	O
same	O
size	O
and	O
we	O
say	O
these	O
layers	O
are	O
in	O
the	O
same	O
network	O
stage	O
.	O

For	O
our	O
feature	O
pyramid	O
,	O
we	O
define	O
one	O
pyramid	O
level	O
for	O
each	O
stage	O
.	O

We	O
choose	O
the	O
output	O
of	O
the	O
last	O
layer	O
of	O
each	O
stage	O
as	O
our	O
reference	O
set	O
of	O
feature	O
maps	O
,	O
which	O
we	O
will	O
enrich	O
to	O
create	O
our	O
pyramid	O
.	O

This	O
choice	O
is	O
natural	O
since	O
the	O
deepest	O
layer	O
of	O
each	O
stage	O
should	O
have	O
the	O
strongest	O
features	O
.	O

Specifically	O
,	O
for	O
ResNets	O
[	O
reference	O
]	O
we	O
use	O
the	O
feature	O
activations	O
output	O
by	O
each	O
stage	O
's	O
last	O
residual	O
block	O
.	O

We	O
denote	O
the	O
output	O
of	O
these	O
last	O
residual	O
blocks	O
as	O
{	O
C	O
2	O
,	O
C	O
3	O
,	O
C	O
4	O
,	O
C	O
5	O
}	O
for	O
conv2	Method
,	O
conv3	O
,	O
conv4	Method
,	O
and	O
conv5	O
outputs	O
,	O
and	O
note	O
that	O
they	O
have	O
strides	O
of	O
{	O
4	O
,	O
8	O
,	O
16	O
,	O
32	O
}	O
pixels	O
with	O
respect	O
to	O
the	O
input	O
image	O
.	O

We	O
do	O
not	O
include	O
conv1	O
into	O
the	O
pyramid	O
due	O
to	O
its	O
large	O
memory	O
footprint	O
.	O

Top	O
-	O
down	O
pathway	O
and	O
lateral	O
connections	O
.	O

The	O
topdown	Method
pathway	Method
hallucinates	O
higher	O
resolution	O
features	O
by	O
upsampling	O
spatially	O
coarser	O
,	O
but	O
semantically	O
stronger	O
,	O
feature	O
maps	O
from	O
higher	O
pyramid	O
levels	O
.	O

These	O
features	O
are	O
then	O
enhanced	O
with	O
features	O
from	O
the	O
bottom	Method
-	Method
up	Method
pathway	Method
via	O
lateral	Method
connections	Method
.	O

Each	O
lateral	O
connection	O
merges	O
feature	O
maps	O
of	O
the	O
same	O
spatial	O
size	O
from	O
the	O
bottom	O
-	O
up	O
pathway	O
and	O
the	O
top	O
-	O
down	O
pathway	O
.	O

The	O
bottom	Method
-	Method
up	Method
feature	Method
map	Method
is	O
of	O
lower	O
-	O
level	O
semantics	O
,	O
but	O
its	O
activations	O
are	O
more	O
accurately	O
localized	O
as	O
it	O
was	O
subsampled	O
fewer	O
times	O
.	O

Fig	O
.	O

3	O
shows	O
the	O
building	O
block	O
that	O
constructs	O
our	O
topdown	Method
feature	Method
maps	Method
.	O

With	O
a	O
coarser	O
-	O
resolution	O
feature	O
map	O
,	O
we	O
upsample	O
the	O
spatial	O
resolution	O
by	O
a	O
factor	O
of	O
2	O
(	O
using	O
nearest	Method
neighbor	Method
upsampling	Method
for	O
simplicity	O
)	O
.	O

The	O
upsam	Method
-	Method
pled	Method
map	Method
is	O
then	O
merged	O
with	O
the	O
corresponding	O
bottom	O
-	O
up	O
map	O
(	O
which	O
undergoes	O
a	O
1×1	Method
convolutional	Method
layer	Method
to	O
reduce	O
channel	O
dimensions	O
)	O
by	O
element	O
-	O
wise	O
addition	O
.	O

This	O
process	O
is	O
iterated	O
until	O
the	O
finest	O
resolution	O
map	O
is	O
generated	O
.	O

To	O
start	O
the	O
iteration	O
,	O
we	O
simply	O
attach	O
a	O
1×1	Method
convolutional	Method
layer	Method
on	O
C	O
5	O
to	O
produce	O
the	O
coarsest	O
resolution	O
map	O
.	O

Finally	O
,	O
we	O
append	O
a	O
3×3	Method
convolution	Method
on	O
each	O
merged	O
map	O
to	O
generate	O
the	O
final	O
feature	O
map	O
,	O
which	O
is	O
to	O
reduce	O
the	O
aliasing	O
effect	O
of	O
upsampling	O
.	O

This	O
final	O
set	O
of	O
feature	O
maps	O
is	O
called	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
}	O
,	O
corresponding	O
to	O
{	O
C	O
2	O
,	O
C	O
3	O
,	O
C	O
4	O
,	O
C	O
5	O
}	O
that	O
are	O
respectively	O
of	O
the	O
same	O
spatial	O
sizes	O
.	O

Because	O
all	O
levels	O
of	O
the	O
pyramid	O
use	O
shared	Method
classifiers	Method
/	Method
regressors	Method
as	O
in	O
a	O
traditional	O
featurized	Method
image	Method
pyramid	Method
,	O
we	O
fix	O
the	O
feature	O
dimension	O
(	O
numbers	O
of	O
channels	O
,	O
denoted	O
as	O
d	O
)	O
in	O
all	O
the	O
feature	O
maps	O
.	O

We	O
set	O
d	O
=	O
256	O
in	O
this	O
paper	O
and	O
thus	O
all	O
extra	O
convolutional	Method
layers	Method
have	O
256	O
-	O
channel	O
outputs	O
.	O

There	O
are	O
no	O
non	O
-	O
linearities	O
in	O
these	O
extra	O
layers	O
,	O
which	O
we	O
have	O
empirically	O
found	O
to	O
have	O
minor	O
impacts	O
.	O

Simplicity	O
is	O
central	O
to	O
our	O
design	O
and	O
we	O
have	O
found	O
that	O
our	O
model	O
is	O
robust	O
to	O
many	O
design	O
choices	O
.	O

We	O
have	O
experimented	O
with	O
more	O
sophisticated	O
blocks	O
(	O
e.g.	O
,	O
using	O
multilayer	O
residual	O
blocks	O
[	O
reference	O
]	O
as	O
the	O
connections	O
)	O
and	O
observed	O
marginally	O
better	O
results	O
.	O

Designing	O
better	O
connection	O
modules	O
is	O
not	O
the	O
focus	O
of	O
this	O
paper	O
,	O
so	O
we	O
opt	O
for	O
the	O
simple	O
design	O
described	O
above	O
.	O

section	O
:	O
Applications	O
Our	O
method	O
is	O
a	O
generic	O
solution	O
for	O
building	O
feature	Task
pyramids	Task
inside	O
deep	Task
ConvNets	Task
.	O

In	O
the	O
following	O
we	O
adopt	O
our	O
method	O
in	O
RPN	Method
[	O
reference	O
]	O
for	O
bounding	Task
box	Task
proposal	Task
generation	Task
and	O
in	O
Fast	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
for	O
object	Task
detection	Task
.	O

To	O
demonstrate	O
the	O
simplicity	O
and	O
effectiveness	O
of	O
our	O
method	O
,	O
we	O
make	O
minimal	O
modifications	O
to	O
the	O
original	O
systems	O
of	O
[	O
reference	O
][	O
reference	O
]	O
when	O
adapting	O
them	O
to	O
our	O
feature	Method
pyramid	Method
.	O

section	O
:	O
Feature	Method
Pyramid	Method
Networks	Method
for	O
RPN	Method
RPN	Method
[	O
reference	O
]	O
is	O
a	O
sliding	Method
-	Method
window	Method
class	Method
-	Method
agnostic	Method
object	Method
detector	Method
.	O

In	O
the	O
original	O
RPN	Method
design	O
,	O
a	O
small	O
subnetwork	O
is	O
evaluated	O
on	O
dense	O
3×3	O
sliding	O
windows	O
,	O
on	O
top	O
of	O
a	O
singlescale	Method
convolutional	Method
feature	Method
map	Method
,	O
performing	O
object	O
/	O
nonobject	O
binary	O
classification	Method
and	O
bounding	Method
box	Method
regression	Method
.	O

This	O
is	O
realized	O
by	O
a	O
3×3	Method
convolutional	Method
layer	Method
followed	O
by	O
two	O
sibling	Method
1×1	Method
convolutions	Method
for	O
classification	Method
and	O
regression	Task
,	O
which	O
we	O
refer	O
to	O
as	O
a	O
network	O
head	O
.	O

The	O
object	Task
/	Task
nonobject	Task
criterion	Task
and	O
bounding	O
box	O
regression	O
target	O
are	O
defined	O
with	O
respect	O
to	O
a	O
set	O
of	O
reference	O
boxes	O
called	O
anchors	O
[	O
reference	O
]	O
.	O

The	O
anchors	O
are	O
of	O
multiple	O
pre	O
-	O
defined	O
scales	O
and	O
aspect	O
ratios	O
in	O
order	O
to	O
cover	O
objects	O
of	O
different	O
shapes	O
.	O

We	O
adapt	O
RPN	Method
by	O
replacing	O
the	O
single	Method
-	Method
scale	Method
feature	Method
map	Method
with	O
our	O
FPN	Method
.	O

We	O
attach	O
a	O
head	O
of	O
the	O
same	O
design	O
(	O
3×3	O
conv	O
and	O
two	O
sibling	O
1×1	O
convs	O
)	O
to	O
each	O
level	O
on	O
our	O
feature	O
pyramid	O
.	O

Because	O
the	O
head	O
slides	O
densely	O
over	O
all	O
locations	O
in	O
all	O
pyramid	O
levels	O
,	O
it	O
is	O
not	O
necessary	O
to	O
have	O
multi	O
-	O
scale	O
anchors	O
on	O
a	O
specific	O
level	O
.	O

Instead	O
,	O
we	O
assign	O
anchors	O
of	O
a	O
single	O
scale	O
to	O
each	O
level	O
.	O

Formally	O
,	O
we	O
define	O
the	O
anchors	O
to	O
have	O
areas	O
of	O
{	O
32	O
2	O
,	O
64	O
2	O
,	O
128	O
2	O
,	O
256	O
2	O
,	O
512	O
2	O
}	O
pixels	O
on	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
respectively	O
.	O

[	O
reference	O
]	O
As	O
in	O
[	O
reference	O
]	O
we	O
also	O
use	O
anchors	O
of	O
multiple	O
aspect	O
ratios	O
{	O
1:2	O
,	O
1:1	O
,	O
2:1	O
}	O
at	O
each	O
level	O
.	O

So	O
in	O
total	O
there	O
are	O
15	O
anchors	O
over	O
the	O
pyramid	O
.	O

We	O
assign	O
training	O
labels	O
to	O
the	O
anchors	O
based	O
on	O
their	O
Intersection	Metric
-	Metric
over	Metric
-	Metric
Union	Metric
(	O
IoU	Metric
)	O
ratios	O
with	O
ground	O
-	O
truth	O
bounding	O
boxes	O
as	O
in	O
[	O
reference	O
]	O
.	O

Formally	O
,	O
an	O
anchor	O
is	O
assigned	O
a	O
positive	O
label	O
if	O
it	O
has	O
the	O
highest	O
IoU	Metric
for	O
a	O
given	O
groundtruth	O
box	O
or	O
an	O
IoU	Metric
over	O
0.7	O
with	O
any	O
ground	O
-	O
truth	O
box	O
,	O
and	O
a	O
negative	O
label	O
if	O
it	O
has	O
IoU	Metric
lower	O
than	O
0.3	O
for	O
all	O
ground	O
-	O
truth	O
boxes	O
.	O

Note	O
that	O
scales	O
of	O
ground	O
-	O
truth	O
boxes	O
are	O
not	O
explicitly	O
used	O
to	O
assign	O
them	O
to	O
the	O
levels	O
of	O
the	O
pyramid	O
;	O
instead	O
,	O
ground	O
-	O
truth	O
boxes	O
are	O
associated	O
with	O
anchors	O
,	O
which	O
have	O
been	O
assigned	O
to	O
pyramid	O
levels	O
.	O

As	O
such	O
,	O
we	O
introduce	O
no	O
extra	O
rules	O
in	O
addition	O
to	O
those	O
in	O
[	O
reference	O
]	O
.	O

We	O
note	O
that	O
the	O
parameters	O
of	O
the	O
heads	O
are	O
shared	O
across	O
all	O
feature	O
pyramid	O
levels	O
;	O
we	O
have	O
also	O
evaluated	O
the	O
alternative	O
without	O
sharing	O
parameters	O
and	O
observed	O
similar	O
accuracy	Metric
.	O

The	O
good	O
performance	O
of	O
sharing	O
parameters	O
indicates	O
that	O
all	O
levels	O
of	O
our	O
pyramid	O
share	O
similar	O
semantic	O
levels	O
.	O

This	O
advantage	O
is	O
analogous	O
to	O
that	O
of	O
using	O
a	O
featurized	Method
image	Method
pyramid	Method
,	O
where	O
a	O
common	Method
head	Method
classifier	Method
can	O
be	O
applied	O
to	O
features	O
computed	O
at	O
any	O
image	O
scale	O
.	O

With	O
the	O
above	O
adaptations	O
,	O
RPN	Method
can	O
be	O
naturally	O
trained	O
and	O
tested	O
with	O
our	O
FPN	Method
,	O
in	O
the	O
same	O
fashion	O
as	O
in	O
[	O
reference	O
]	O
.	O

We	O
elaborate	O
on	O
the	O
implementation	O
details	O
in	O
the	O
experiments	O
.	O

section	O
:	O
Feature	Method
Pyramid	Method
Networks	Method
for	O
Fast	Method
R	Method
-	Method
CNN	Method
Fast	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
is	O
a	O
region	Method
-	Method
based	Method
object	Method
detector	Method
in	O
which	O
Region	Method
-	Method
of	Method
-	Method
Interest	Method
(	O
RoI	Method
)	O
pooling	O
is	O
used	O
to	O
extract	O
features	O
.	O

Fast	Method
R	Method
-	Method
CNN	Method
is	O
most	O
commonly	O
performed	O
on	O
a	O
single	O
-	O
scale	O
feature	O
map	O
.	O

To	O
use	O
it	O
with	O
our	O
FPN	Method
,	O
we	O
need	O
to	O
assign	O
RoIs	O
of	O
different	O
scales	O
to	O
the	O
pyramid	O
levels	O
.	O

We	O
view	O
our	O
feature	Method
pyramid	Method
as	O
if	O
it	O
were	O
produced	O
from	O
an	O
image	Method
pyramid	Method
.	O

Thus	O
we	O
can	O
adapt	O
the	O
assignment	Method
strategy	Method
of	O
region	Method
-	Method
based	Method
detectors	Method
[	O
reference	O
][	O
reference	O
]	O
in	O
the	O
case	O
when	O
they	O
are	O
run	O
on	O
image	O
pyramids	O
.	O

Formally	O
,	O
we	O
assign	O
an	O
RoI	Method
of	O
width	O
w	O
and	O
height	O
h	O
(	O
on	O
the	O
input	O
image	O
to	O
the	O
network	O
)	O
to	O
the	O
level	O
P	O
k	O
of	O
our	O
feature	Method
pyramid	Method
by	O
:	O
Here	O
224	O
is	O
the	O
canonical	Material
ImageNet	Material
pre	Material
-	Material
training	Material
size	Material
,	O
and	O
k	O
0	O
is	O
the	O
target	O
level	O
on	O
which	O
an	O
RoI	Method
with	O
w	O
×	O
h	O
=	O
224	O
2	O
should	O
be	O
mapped	O
into	O
.	O

Analogous	O
to	O
the	O
ResNet	O
-	O
based	O
Faster	Method
R	Method
-	Method
CNN	Method
system	O
[	O
reference	O
]	O
that	O
uses	O
C	O
4	O
as	O
the	O
single	O
-	O
scale	O
feature	O
map	O
,	O
we	O
set	O
k	O
0	O
to	O
4	O
.	O

Intuitively	O
,	O
Eqn	O
.	O

(	O
1	O
)	O
means	O
that	O
if	O
the	O
RoI	Method
's	O
scale	O
becomes	O
smaller	O
(	O
say	O
,	O
1	O
/	O
2	O
of	O
224	O
)	O
,	O
it	O
should	O
be	O
mapped	O
into	O
a	O
finer	O
-	O
resolution	O
level	O
(	O
say	O
,	O
k	O
=	O
3	O
)	O
.	O

We	O
attach	O
predictor	O
heads	O
(	O
in	O
Fast	Method
R	Method
-	Method
CNN	Method
the	O
heads	O
are	O
class	Method
-	Method
specific	Method
classifiers	Method
and	O
bounding	Method
box	Method
regressors	Method
)	O
to	O
all	O
RoIs	O
of	O
all	O
levels	O
.	O

Again	O
,	O
the	O
heads	O
all	O
share	O
parameters	O
,	O
regardless	O
of	O
their	O
levels	O
.	O

In	O
[	O
reference	O
]	O
,	O
a	O
ResNet	O
's	O
conv5	O
layers	O
(	O
a	O
9	Method
-	Method
layer	Method
deep	Method
subnetwork	Method
)	O
are	O
adopted	O
as	O
the	O
head	O
on	O
top	O
of	O
the	O
conv4	O
features	O
,	O
but	O
our	O
method	O
has	O
already	O
harnessed	O
conv5	Method
to	O
construct	O
the	O
feature	O
pyramid	O
.	O

So	O
unlike	O
[	O
reference	O
]	O
,	O
we	O
simply	O
adopt	O
RoI	Method
pooling	O
to	O
extract	O
7×7	O
features	O
,	O
and	O
attach	O
two	O
hidden	O
1	O
,	O
024	O
-	O
d	O
fully	Method
-	Method
connected	Method
(	O
fc	Method
)	O
layers	O
(	O
each	O
followed	O
by	O
ReLU	Method
)	O
before	O
the	O
final	O
classification	Method
and	O
bounding	Method
box	Method
regression	Method
layers	Method
.	O

These	O
layers	O
are	O
randomly	O
initialized	O
,	O
as	O
there	O
are	O
no	O
pre	O
-	O
trained	O
fc	Method
layers	O
available	O
in	O
ResNets	Material
.	O

Note	O
that	O
compared	O
to	O
the	O
standard	O
conv5	Method
head	Method
,	O
our	O
2	O
-	O
fc	Method
MLP	O
head	O
is	O
lighter	O
weight	O
and	O
faster	O
.	O

Based	O
on	O
these	O
adaptations	O
,	O
we	O
can	O
train	O
and	O
test	O
Fast	Method
R	Method
-	Method
CNN	Method
on	O
top	O
of	O
the	O
feature	O
pyramid	O
.	O

Implementation	O
details	O
are	O
given	O
in	O
the	O
experimental	O
section	O
.	O

section	O
:	O
Experiments	O
on	O
Object	Task
Detection	Task
We	O
perform	O
experiments	O
on	O
the	O
80	O
category	O
COCO	Material
detection	O
dataset	O
[	O
reference	O
]	O
.	O

We	O
train	O
using	O
the	O
union	O
of	O
80k	O
train	O
images	O
and	O
a	O
35k	O
subset	O
of	O
val	O
images	O
(	O
trainval35k	O
[	O
reference	O
]	O
)	O
,	O
and	O
report	O
ablations	Method
on	O
a	O
5k	O
subset	O
of	O
val	O
images	O
(	O
minival	O
)	O
.	O

We	O
also	O
report	O
final	O
results	O
on	O
the	O
standard	O
test	O
set	O
(	O
test	O
-	O
std	O
)	O
[	O
reference	O
]	O
which	O
has	O
no	O
disclosed	O
labels	O
.	O

As	O
is	O
common	O
practice	O
[	O
reference	O
]	O
,	O
all	O
network	Method
backbones	Method
are	O
pre	O
-	O
trained	O
on	O
the	O
ImageNet1k	O
classification	Method
set	O
[	O
reference	O
]	O
and	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
detection	O
dataset	O
.	O

We	O
use	O
the	O
pre	O
-	O
trained	O
ResNet	Method
-	Method
50	Method
and	O
ResNet	Method
-	Method
101	Method
models	Method
that	O
are	O
publicly	O
available	O
.	O

[	O
reference	O
]	O
Our	O
code	O
is	O
a	O
reimplementation	O
of	O
py	Method
-	Method
faster	Method
-	Method
rcnn	Method
3	Method
using	O
Caffe2	Method
.	O

[	O
reference	O
]	O
section	O
:	O
Region	Method
Proposal	Method
with	O
RPN	Method
We	O
evaluate	O
the	O
COCO	Material
-	O
style	O
Average	Metric
Recall	Metric
(	O
AR	Metric
)	O
and	O
AR	Metric
on	O
small	O
,	O
medium	O
,	O
and	O
large	O
objects	O
(	O
AR	Metric
s	O
,	O
AR	Metric
m	O
,	O
and	O
AR	Metric
l	O
)	O
following	O
the	O
definitions	O
in	O
[	O
reference	O
]	O
.	O

We	O
report	O
results	O
for	O
100	O
and	O
1000	O
proposals	O
per	O
images	O
(	O
AR	Metric
100	O
and	O
AR	Metric
1k	O
)	O
.	O

Implementation	O
details	O
.	O

All	O
architectures	O
in	O
Table	O
1	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

The	O
input	O
image	O
is	O
resized	O
such	O
that	O
its	O
shorter	O
side	O
has	O
800	O
pixels	O
.	O

We	O
adopt	O
synchronized	Method
SGD	Method
training	Method
on	O
8	O
GPUs	Method
.	O

A	O
mini	O
-	O
batch	O
involves	O
2	O
images	O
per	O
GPU	O
and	O
256	O
anchors	O
per	O
image	O
.	O

We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

The	O
learning	Metric
rate	Metric
is	O
0.02	O
for	O
the	O
first	O
30k	O
mini	O
-	O
batches	O
and	O
0.002	O
for	O
the	O
next	O
10k	O
.	O

For	O
all	O
RPN	Method
experiments	O
(	O
including	O
baselines	O
)	O
,	O
we	O
include	O
the	O
anchor	O
boxes	O
that	O
are	O
outside	O
the	O
image	O
for	O
training	O
,	O
which	O
is	O
unlike	O
[	O
reference	O
]	O
where	O
these	O
anchor	O
boxes	O
are	O
ignored	O
.	O

Other	O
implementation	O
details	O
are	O
as	O
in	O
[	O
reference	O
]	O
.	O

Training	O
RPN	Method
with	O
FPN	Method
on	O
8	O
GPUs	Method
takes	O
about	O
8	O
hours	O
on	O
COCO	Material
.	O

1	O
(	O
a	O
)	O
)	O
.	O

In	O
addition	O
,	O
the	O
performance	O
on	O
small	O
objects	O
(	O
AR	Metric
1k	O
s	O
)	O
is	O
boosted	O
by	O
a	O
large	O
margin	O
of	O
12.9	O
points	O
.	O

Our	O
pyramid	Method
representation	Method
greatly	O
improves	O
RPN	Method
's	O
robustness	Metric
to	O
object	O
scale	O
variation	O
.	O

How	O
important	O
is	O
top	O
-	O
down	O
enrichment	O
?	O
Table	O
1	O
(	O
d	O
)	O
shows	O
the	O
results	O
of	O
our	O
feature	Method
pyramid	Method
without	O
the	O
topdown	Method
pathway	Method
.	O

With	O
this	O
modification	O
,	O
the	O
1×1	O
lateral	O
connections	O
followed	O
by	O
3×3	Method
convolutions	Method
are	O
attached	O
to	O
the	O
bottom	O
-	O
up	O
pyramid	O
.	O

This	O
architecture	O
simulates	O
the	O
effect	O
of	O
reusing	O
the	O
pyramidal	O
feature	O
hierarchy	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
.	O

The	O
results	O
in	O
Table	O
1	O
(	O
d	O
)	O
are	O
just	O
on	O
par	O
with	O
the	O
RPN	Method
baseline	O
and	O
lag	O
far	O
behind	O
ours	O
.	O

We	O
conjecture	O
that	O
this	O
is	O
because	O
there	O
are	O
large	O
semantic	O
gaps	O
between	O
different	O
levels	O
on	O
the	O
bottom	O
-	O
up	O
pyramid	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
,	O
especially	O
for	O
very	O
deep	Task
ResNets	Task
.	O

We	O
have	O
also	O
evaluated	O
a	O
variant	O
of	O
Table	Method
1	Method
(	Method
d	Method
)	O
without	O
sharing	O
the	O
parameters	O
of	O
the	O
heads	O
,	O
but	O
observed	O
similarly	O
degraded	O
performance	O
.	O

This	O
issue	O
can	O
not	O
be	O
simply	O
remedied	O
by	O
level	O
-	O
specific	O
heads	O
.	O

How	O
important	O
are	O
lateral	O
connections	O
?	O
Table	O
1	O
(	O
e	O
)	O
shows	O
the	O
ablation	O
results	O
of	O
a	O
top	Method
-	Method
down	Method
feature	Method
pyramid	Method
without	O
the	O
1×1	O
lateral	O
connections	O
.	O

This	O
top	Method
-	Method
down	Method
pyramid	Method
has	O
strong	O
semantic	O
features	O
and	O
fine	O
resolutions	O
.	O

But	O
we	O
argue	O
that	O
the	O
locations	O
of	O
these	O
features	O
are	O
not	O
precise	O
,	O
because	O
these	O
maps	O
have	O
been	O
downsampled	O
and	O
upsampled	O
several	O
times	O
.	O

More	O
precise	O
locations	O
of	O
features	O
can	O
be	O
directly	O
passed	O
from	O
the	O
finer	O
levels	O
of	O
the	O
bottom	O
-	O
up	O
maps	O
via	O
the	O
lateral	O
connections	O
to	O
the	O
top	O
-	O
down	O
maps	O
.	O

As	O
a	O
results	O
,	O
FPN	Method
has	O
an	O
AR	Metric
1k	O
score	O
10	O
points	O
higher	O
than	O
Table	O
1	O
section	O
:	O
(	O
e	O
)	O
.	O

How	O
important	O
are	O
pyramid	Method
representations	Method
?	O
Instead	O
of	O
resorting	O
to	O
pyramid	Method
representations	Method
,	O
one	O
can	O
attach	O
the	O
head	O
to	O
the	O
highest	O
-	O
resolution	O
,	O
strongly	O
semantic	O
feature	O
maps	O
of	O
P	O
2	O
(	O
i.e.	O
,	O
the	O
finest	O
level	O
in	O
our	O
pyramids	O
)	O
.	O

Similar	O
to	O
the	O
single	O
-	O
scale	Method
baselines	Method
,	O
we	O
assign	O
all	O
anchors	O
to	O
the	O
P	O
2	O
feature	O
map	O
.	O

This	O
variant	O
(	O
Table	O
1	O
(	O
f	O
)	O
)	O
is	O
better	O
than	O
the	O
baseline	O
but	O
inferior	O
to	O
our	O
approach	O
.	O

RPN	Method
is	O
a	O
sliding	Method
window	Method
detector	Method
with	O
a	O
fixed	O
window	O
size	O
,	O
so	O
scanning	O
over	O
pyramid	O
levels	O
can	O
increase	O
its	O
robustness	Metric
to	O
scale	O
variance	O
.	O

In	O
addition	O
,	O
we	O
note	O
that	O
using	O
P	Method
2	Method
alone	O
leads	O
to	O
more	O
anchors	O
(	O
750k	O
,	O
Table	O
1	O
.	O

Bounding	Method
box	Method
proposal	Method
results	O
using	O
RPN	Method
[	O
reference	O
]	O
,	O
evaluated	O
on	O
the	O
COCO	Material
minival	O
set	O
.	O

All	O
models	O
are	O
trained	O
on	O
trainval35k	O
.	O

The	O
columns	O
"	O
lateral	O
"	O
and	O
"	O
top	O
-	O
down	O
"	O
denote	O
the	O
presence	O
of	O
lateral	O
and	O
top	O
-	O
down	O
connections	O
,	O
respectively	O
.	O

The	O
column	O
"	O
feature	O
"	O
denotes	O
the	O
feature	O
maps	O
on	O
which	O
the	O
heads	O
are	O
attached	O
.	O

All	O
results	O
are	O
based	O
on	O
ResNet	Method
-	Method
50	Method
and	O
share	O
the	O
same	O
hyper	O
-	O
parameters	O
.	O

section	O
:	O
Object	Task
Detection	Task
with	O
Fast	Method
/	O
Faster	Method
R	Method
-	Method
CNN	Method
Next	O
we	O
investigate	O
FPN	Method
for	O
region	Method
-	Method
based	Method
(	Method
non	Method
-	Method
sliding	Method
window	Method
)	Method
detectors	Method
.	O

We	O
evaluate	O
object	Task
detection	Task
by	O
the	O
COCO	Metric
-	Metric
style	Metric
Average	Metric
Precision	Metric
(	O
AP	Metric
)	O
and	O
PASCAL	O
-	O
style	O
AP	Metric
(	O
at	O
a	O
single	O
IoU	Metric
threshold	O
of	O
0.5	O
)	O
.	O

We	O
also	O
report	O
COCO	Metric
AP	Metric
on	O
objects	O
of	O
small	O
,	O
medium	O
,	O
and	O
large	O
sizes	O
(	O
namely	O
,	O
AP	Metric
s	O
,	O
AP	Metric
m	O
,	O
and	O
AP	Metric
l	O
)	O
following	O
the	O
definitions	O
in	O
[	O
reference	O
]	O
.	O

Implementation	O
details	O
.	O

The	O
input	O
image	O
is	O
resized	O
such	O
that	O
its	O
shorter	O
side	O
has	O
800	O
pixels	O
.	O

Synchronized	Method
SGD	Method
is	O
used	O
to	O
train	O
the	O
model	O
on	O
8	O
GPUs	O
.	O

Each	O
mini	O
-	O
batch	O
involves	O
2	O
image	O
per	O
GPU	O
and	O
512	O
RoIs	O
per	O
image	O
.	O

We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

The	O
learning	Metric
rate	Metric
is	O
0.02	O
for	O
the	O
first	O
60k	O
mini	O
-	O
batches	O
and	O
0.002	O
for	O
the	O
next	O
20k	O
.	O

We	O
use	O
2000	O
RoIs	O
per	O
image	O
for	O
training	O
and	O
1000	O
for	O
testing	O
.	O

Training	O
Fast	Method
R	Method
-	Method
CNN	Method
with	O
FPN	Method
takes	O
about	O
10	O
hours	O
on	O
the	O
COCO	Material
dataset	O
.	O

section	O
:	O
Fast	Method
R	Method
-	Method
CNN	Method
(	O
on	O
fixed	O
proposals	O
)	O
To	O
better	O
investigate	O
FPN	Method
's	O
effects	O
on	O
the	O
region	Method
-	Method
based	Method
detector	Method
alone	O
,	O
we	O
conduct	O
ablations	O
of	O
Fast	Method
R	Method
-	Method
CNN	Method
on	O
a	O
fixed	O
set	O
of	O
proposals	O
.	O

We	O
choose	O
to	O
freeze	O
the	O
proposals	O
as	O
computed	O
by	O
RPN	Method
on	O
FPN	Method
(	O
Table	O
1	O
(	O
c	O
)	O
)	O
,	O
because	O
it	O
has	O
good	O
performance	O
on	O
small	O
objects	O
that	O
are	O
to	O
be	O
recognized	O
by	O
the	O
detector	O
.	O

For	O
simplicity	O
we	O
do	O
not	O
share	O
features	O
between	O
Fast	Method
R	Method
-	Method
CNN	Method
and	O
RPN	Method
,	O
except	O
when	O
specified	O
.	O

As	O
a	O
ResNet	O
-	O
based	O
Fast	Method
R	Method
-	Method
CNN	Method
baseline	O
,	O
following	O
[	O
reference	O
]	O
,	O
we	O
adopt	O
RoI	Method
pooling	O
with	O
an	O
output	O
size	O
of	O
14×14	O
and	O
attach	O
all	O
conv5	Method
layers	Method
as	O
the	O
hidden	O
layers	O
of	O
the	O
head	O
.	O

This	O
gives	O
an	O
AP	Metric
of	O
31.9	O
in	O
Table	O
2	O
(	O
a	O
)	O
.	O

Table	O
2	O
(	O
b	O
)	O
is	O
a	O
baseline	O
exploiting	O
an	O
MLP	Method
head	Method
with	O
2	O
hidden	O
fc	Method
layers	O
,	O
similar	O
to	O
the	O
head	O
in	O
our	O
architecture	O
.	O

It	O
gets	O
an	O
AP	Metric
of	O
28.8	O
,	O
indicating	O
that	O
the	O
2	O
-	O
fc	Method
head	O
does	O
not	O
give	O
us	O
any	O
orthogonal	O
advantage	O
over	O
the	O
baseline	O
in	O
Table	O
2	O
(	O
a	O
)	O
.	O

Table	O
2	O
(	O
c	O
)	O
shows	O
the	O
results	O
of	O
our	O
FPN	Method
in	O
Fast	Method
R	Method
-	Method
CNN	Method
.	O

Comparing	O
with	O
the	O
baseline	O
in	O
Table	O
2	O
(	O
a	O
)	O
,	O
our	O
method	O
improves	O
AP	Metric
by	O
2.0	O
points	O
and	O
small	O
object	O
AP	Metric
by	O
2.1	O
points	O
.	O

Comparing	O
with	O
the	O
baseline	O
that	O
also	O
adopts	O
a	O
2fc	O
head	O
(	O
Table	O
2	O
(	O
b	O
)	O
)	O
,	O
our	O
method	O
improves	O
AP	Metric
by	O
5.1	O
points	O
.	O

[	O
reference	O
]	O
These	O
comparisons	O
indicate	O
that	O
our	O
feature	Method
pyramid	Method
is	O
superior	O
to	O
single	Method
-	Method
scale	Method
features	Method
for	O
a	O
region	Method
-	Method
based	Method
object	Method
detector	Method
.	O

nections	O
or	O
removing	O
lateral	O
connections	O
leads	O
to	O
inferior	O
results	O
,	O
similar	O
to	O
what	O
we	O
have	O
observed	O
in	O
the	O
above	O
subsection	O
for	O
RPN	Method
.	O

It	O
is	O
noteworthy	O
that	O
removing	O
top	O
-	O
down	O
connections	O
(	O
Table	O
2	O
(	O
d	O
)	O
)	O
significantly	O
degrades	O
the	O
accuracy	Metric
,	O
suggesting	O
that	O
Fast	Method
R	Method
-	Method
CNN	Method
suffers	O
from	O
using	O
the	O
low	O
-	O
level	O
features	O
at	O
the	O
high	O
-	O
resolution	O
maps	O
.	O

In	O
Table	O
2	O
(	O
f	O
)	O
,	O
we	O
adopt	O
Fast	Method
R	Method
-	Method
CNN	Method
on	O
the	O
single	O
finest	O
scale	O
feature	O
map	O
of	O
P	O
2	O
.	O

Its	O
result	O
(	O
33.4	O
AP	Metric
)	O
is	O
marginally	O
worse	O
than	O
that	O
of	O
using	O
all	O
pyramid	O
levels	O
(	O
33.9	O
AP	Metric
,	O
Table	O
2	O
(	O
c	O
)	O
)	O
.	O

We	O
argue	O
that	O
this	O
is	O
because	O
RoI	Method
pooling	O
is	O
a	O
warping	Method
-	Method
like	Method
operation	Method
,	O
which	O
is	O
less	O
sensitive	O
to	O
the	O
region	O
's	O
scales	O
.	O

Despite	O
the	O
good	O
accuracy	Metric
of	O
this	O
variant	O
,	O
it	O
is	O
based	O
on	O
the	O
RPN	Method
proposals	O
of	O
{	O
P	O
k	O
}	O
and	O
has	O
thus	O
already	O
benefited	O
from	O
the	O
pyramid	Method
representation	Method
.	O

section	O
:	O
Faster	Method
R	Method
-	Method
CNN	Method
(	O
on	O
consistent	O
proposals	O
)	O
In	O
the	O
above	O
we	O
used	O
a	O
fixed	O
set	O
of	O
proposals	O
to	O
investigate	O
the	O
detectors	O
.	O

But	O
in	O
a	O
Faster	Method
R	Method
-	Method
CNN	Method
system	Method
[	O
reference	O
]	O
,	O
the	O
RPN	Method
and	O
Fast	Method
R	Method
-	Method
CNN	Method
must	O
use	O
the	O
same	O
network	O
backbone	O
in	O
order	O
to	O
make	O
feature	Task
sharing	Task
possible	O
.	O

Table	O
3	O
shows	O
the	O
comparisons	O
between	O
our	O
method	O
and	O
two	O
baselines	O
,	O
all	O
using	O
consistent	Method
backbone	Method
architectures	Method
for	O
RPN	Method
and	O
Fast	Method
R	Method
-	Method
CNN	Method
.	O

Table	O
3	O
(	O
a	O
)	O
shows	O
our	O
reproduction	O
of	O
the	O
baseline	O
Faster	Method
R	Method
-	Method
CNN	Method
system	O
as	O
described	O
in	O
[	O
reference	O
]	O
.	O

Under	O
controlled	O
settings	O
,	O
our	O
FPN	Method
(	O
Table	O
3	O
(	O
c	O
)	O
)	O
is	O
better	O
than	O
this	O
strong	O
baseline	O
by	O
2.3	O
points	O
AP	Metric
and	O
3.8	O
points	O
AP@0.5	O
.	O

Note	O
that	O
Table	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
are	O
baselines	O
that	O
are	O
much	O
stronger	O
than	O
the	O
baseline	O
provided	O
by	O
He	O
et	O
al	O
.	O

[	O
reference	O
]	O
in	O
Table	O
3	O
(	O
*	O
)	O
.	O

We	O
find	O
the	O
following	O
implementations	O
contribute	O
to	O
the	O
gap	O
:	O
(	O
i	O
)	O
We	O
use	O
an	O
image	O
scale	O
of	O
800	O
pixels	O
instead	O
of	O
600	O
in	O
[	O
reference	O
][	O
reference	O
]	O
;	O
(	O
ii	O
)	O
We	O
train	O
with	O
512	O
RoIs	O
per	O
image	O
which	O
accelerate	O
convergence	Task
,	O
in	O
contrast	O
to	O
64	O
RoIs	O
in	O
[	O
reference	O
][	O
reference	O
]	O
;	O
(	O
iii	O
)	O
We	O
use	O
5	O
scale	O
anchors	O
instead	O
of	O
4	O
in	O
[	O
reference	O
]	O
(	O
adding	O
32	O
2	O
)	O
;	O
(	O
iv	O
)	O
At	O
test	O
time	O
we	O
use	O
1000	O
proposals	O
per	O
image	O
instead	O
of	O
300	O
in	O
[	O
reference	O
]	O
.	O

So	O
comparing	O
with	O
He	O
et	O
al	O
.	O

's	O
ResNet	Method
-	Method
50	Method
Faster	Method
R	Method
-	Method
CNN	Method
baseline	Method
in	O
Table	O
3	O
(	O
*	O
)	O
,	O
our	O
method	O
improves	O
AP	Metric
by	O
7.6	O
points	O
and	O
AP@0.5	O
by	O
9.6	O
points	O
.	O

Sharing	O
features	O
.	O

In	O
the	O
above	O
,	O
for	O
simplicity	O
we	O
do	O
not	O
share	O
the	O
features	O
between	O
RPN	Method
and	O
Fast	Method
R	Method
-	Method
CNN	Method
.	O

In	O
Ta	O
-	O
Table	O
5	O
.	O

More	O
object	Task
detection	Task
results	O
using	O
Faster	Method
R	Method
-	Method
CNN	Method
and	O
our	O
FPNs	Method
,	O
evaluated	O
on	O
minival	Material
.	O

Sharing	O
features	O
increases	O
train	Metric
time	Metric
by	O
1.5×	O
(	O
using	O
4	Method
-	Method
step	Method
training	Method
[	O
reference	O
]	O
)	O
,	O
but	O
reduces	O
test	Metric
time	Metric
.	O

ble	O
5	O
,	O
we	O
evaluate	O
sharing	O
features	O
following	O
the	O
4	Method
-	Method
step	Method
training	Method
described	O
in	O
[	O
reference	O
]	O
.	O

Similar	O
to	O
[	O
reference	O
]	O
,	O
we	O
find	O
that	O
sharing	O
features	O
improves	O
accuracy	Metric
by	O
a	O
small	O
margin	O
.	O

Feature	Method
sharing	Method
also	O
reduces	O
the	O
testing	Metric
time	Metric
.	O

Running	Metric
time	Metric
.	O

With	O
feature	Method
sharing	Method
,	O
our	O
FPN	Method
-	O
based	O
Faster	Method
R	Method
-	Method
CNN	Method
system	O
has	O
inference	Metric
time	Metric
of	O
0.148	O
seconds	O
per	O
image	O
on	O
a	O
single	O
NVIDIA	O
M40	O
GPU	O
for	O
ResNet	Method
-	Method
50	Method
,	O
and	O
0.172	O
seconds	O
for	O
ResNet	Task
-	Task
101	Task
.	O

[	O
reference	O
]	O
As	O
a	O
comparison	O
,	O
the	O
single	O
-	O
scale	O
ResNet	Method
-	Method
50	Method
baseline	O
in	O
Table	O
3	O
(	O
a	O
)	O
runs	O
at	O
0.32	O
seconds	O
.	O

Our	O
method	O
introduces	O
small	O
extra	O
cost	O
by	O
the	O
extra	O
layers	O
in	O
the	O
FPN	Method
,	O
but	O
has	O
a	O
lighter	O
weight	O
head	O
.	O

Overall	O
our	O
system	O
is	O
faster	O
than	O
the	O
ResNet	O
-	O
based	O
Faster	Method
R	Method
-	Method
CNN	Method
counterpart	O
.	O

We	O
believe	O
the	O
efficiency	O
and	O
simplicity	O
of	O
our	O
method	O
will	O
benefit	O
future	O
research	O
and	O
applications	O
.	O

section	O
:	O
Comparing	O
with	O
COCO	Material
Competition	O
Winners	O
We	O
find	O
that	O
our	O
ResNet	Method
-	Method
101	Method
model	Method
in	O
Table	O
5	O
is	O
not	O
sufficiently	O
trained	O
with	O
the	O
default	Metric
learning	Metric
rate	Metric
schedule	Metric
.	O

So	O
we	O
increase	O
the	O
number	O
of	O
mini	O
-	O
batches	O
by	O
2×	O
at	O
each	O
learning	Metric
rate	Metric
when	O
training	O
the	O
Fast	Method
R	Method
-	Method
CNN	Method
step	O
.	O

This	O
increases	O
AP	Metric
on	O
minival	O
to	O
35.6	O
,	O
without	O
sharing	O
features	O
.	O

This	O
model	O
is	O
the	O
one	O
we	O
submitted	O
to	O
the	O
COCO	Material
detection	O
leaderboard	O
,	O
shown	O
in	O
Table	O
4	O
.	O

We	O
have	O
not	O
evaluated	O
its	O
feature	Method
-	Method
sharing	Method
version	Method
due	O
to	O
limited	O
time	O
,	O
which	O
should	O
be	O
slightly	O
better	O
as	O
implied	O
by	O
Table	O
5	O
.	O

Table	O
4	O
compares	O
our	O
method	O
with	O
the	O
single	O
-	O
model	O
results	O
of	O
the	O
COCO	Material
competition	O
winners	O
,	O
including	O
the	O
2016	O
winner	O
G	Method
-	Method
RMI	Method
and	O
the	O
2015	O
winner	O
Faster	O
R	Method
-	Method
CNN	Method
+++	Method
.	O

Without	O
adding	O
bells	O
and	O
whistles	O
,	O
our	O
single	O
-	O
model	O
entry	O
has	O
surpassed	O
these	O
strong	O
,	O
heavily	O
engineered	O
competitors	O
.	O

On	O
the	O
test	O
-	O
dev	O
set	O
,	O
our	O
method	O
increases	O
over	O
the	O
existing	O
best	O
results	O
by	O
0.5	O
points	O
of	O
AP	Metric
(	O
36.2	O
vs.	O
35.7	O
)	O
and	O
3.4	O
points	O
of	O
AP@0.5	O
(	O
59.1	O
vs.	O
55.7	O
)	O
.	O

It	O
is	O
worth	O
noting	O
that	O
our	O
method	O
does	O
not	O
rely	O
on	O
image	Method
pyramids	Method
and	O
only	O
uses	O
a	O
single	O
input	O
image	O
scale	O
,	O
but	O
still	O
has	O
outstanding	O
AP	Metric
on	O
small	O
-	O
scale	O
objects	O
.	O

This	O
could	O
only	O
be	O
achieved	O
by	O
highresolution	O
image	O
inputs	O
with	O
previous	O
methods	O
.	O

Moreover	O
,	O
our	O
method	O
does	O
not	O
exploit	O
many	O
popular	O
improvements	O
,	O
such	O
as	O
iterative	Method
regression	Method
[	O
reference	O
]	O
,	O
hard	Method
negative	Method
mining	Method
[	O
reference	O
]	O
,	O
context	Method
modeling	Method
[	O
reference	O
]	O
,	O
stronger	O
data	Method
augmentation	Method
[	O
reference	O
]	O
,	O
etc	O
.	O

These	O
improvements	O
are	O
complementary	O
to	O
FPNs	Method
and	O
should	O
boost	O
accuracy	Metric
further	O
.	O

Recently	O
,	O
FPN	Method
has	O
enabled	O
new	O
top	O
results	O
in	O
all	O
tracks	O
of	O
the	O
COCO	Material
competition	O
,	O
including	O
detection	Task
,	O
instance	Task
segmentation	Task
,	O
and	O
keypoint	Task
estimation	Task
.	O

See	O
[	O
reference	O
]	O
for	O
details	O
.	O

section	O
:	O
Extensions	O
:	O
Segmentation	Task
Proposals	Task
Our	O
method	O
is	O
a	O
generic	Method
pyramid	Method
representation	Method
and	O
can	O
be	O
used	O
in	O
applications	O
other	O
than	O
object	Task
detection	Task
.	O

In	O
this	O
section	O
we	O
use	O
FPNs	Method
to	O
generate	O
segmentation	Task
proposals	Task
,	O
following	O
the	O
DeepMask	O
/	O
SharpMask	Method
framework	O
[	O
reference	O
][	O
reference	O
]	O
.	O

DeepMask	O
/	O
SharpMask	Method
were	O
trained	O
on	O
image	O
crops	O
for	O
predicting	Task
instance	Task
segments	Task
and	O
object	Task
/	Task
non	Task
-	Task
object	Task
scores	Task
.	O

At	O
inference	O
time	O
,	O
these	O
models	O
are	O
run	O
convolutionally	Method
to	O
generate	O
dense	Task
proposals	Task
in	O
an	O
image	O
.	O

To	O
generate	O
segments	O
at	O
multiple	O
scales	O
,	O
image	O
pyramids	O
are	O
necessary	O
[	O
reference	O
][	O
reference	O
]	O
.	O

It	O
is	O
easy	O
to	O
adapt	O
FPN	Method
to	O
generate	O
mask	O
proposals	O
.	O

We	O
use	O
a	O
fully	Method
convolutional	Method
setup	Method
for	O
both	O
training	Task
and	O
inference	Task
.	O

We	O
construct	O
our	O
feature	Method
pyramid	Method
as	O
in	O
Sec	O
.	O

5.1	O
and	O
set	O
d	O
=	O
128	O
.	O

On	O
top	O
of	O
each	O
level	O
of	O
the	O
feature	O
pyramid	O
,	O
we	O
apply	O
a	O
small	O
5×5	Method
MLP	Method
to	O
predict	O
14×14	O
masks	O
and	O
object	O
scores	O
in	O
a	O
fully	Method
convolutional	Method
fashion	Method
,	O
see	O
Fig	O
.	O

4	O
.	O

Additionally	O
,	O
motivated	O
by	O
the	O
use	O
of	O
2	O
scales	O
per	O
octave	O
in	O
the	O
image	O
pyramid	O
of	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
use	O
a	O
second	O
MLP	Method
of	O
input	O
size	O
7×7	O
to	O
handle	O
half	O
octaves	O
.	O

The	O
two	O
MLPs	Method
play	O
a	O
similar	O
role	O
as	O
anchors	O
in	O
RPN	Method
.	O

The	O
architecture	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
;	O
full	O
implementation	O
details	O
are	O
given	O
in	O
the	O
appendix	O
.	O

section	O
:	O
Segmentation	O
Proposal	O
Results	O
Results	O
are	O
shown	O
in	O
Table	O
6	O
.	O

We	O
report	O
segment	O
AR	Metric
and	O
segment	O
AR	Metric
on	O
small	O
,	O
medium	O
,	O
and	O
large	O
objects	O
,	O
always	O
for	O
1000	O
proposals	O
.	O

Our	O
baseline	O
FPN	Method
model	O
with	O
a	O
single	O
5×5	Method
MLP	Method
achieves	O
an	O
AR	Metric
of	O
43.4	O
.	O

Switching	O
to	O
a	O
slightly	O
larger	O
7×7	O
MLP	Method
leaves	O
accuracy	Metric
largely	O
unchanged	O
.	O

Using	O
both	O
MLPs	Method
together	O
increases	O
accuracy	Metric
to	O
45.7	O
AR	Metric
.	O

Increasing	O
mask	O
output	O
size	O
from	O
14×14	O
to	O
28×28	O
increases	O
AR	Metric
another	O
point	O
(	O
larger	O
sizes	O
begin	O
to	O
degrade	O
accuracy	Metric
)	O
.	O

Finally	O
,	O
doubling	O
the	O
training	O
iterations	O
increases	O
AR	Metric
to	O
48.1	O
.	O

We	O
also	O
report	O
comparisons	O
to	O
DeepMask	Method
[	O
reference	O
]	O
,	O
SharpMask	Method
[	O
reference	O
]	O
,	O
and	O
InstanceFCN	Method
[	O
reference	O
]	O
,	O
the	O
previous	O
state	O
of	O
the	O
art	O
methods	O
in	O
mask	Task
proposal	Task
generation	Task
.	O

We	O
outperform	O
the	O
accuracy	Metric
of	O
these	O
approaches	O
by	O
over	O
8.3	O
points	O
AR	Metric
.	O

In	O
particular	O
,	O
we	O
nearly	O
double	O
the	O
accuracy	Metric
on	O
small	O
objects	O
.	O

Existing	O
mask	Method
proposal	Method
methods	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
are	O
based	O
on	O
densely	O
sampled	O
image	O
pyramids	O
(	O
e.g.	O
,	O
scaled	O
by	O
2	O
{	O
−2:0.5:1	O
}	O
in	O
[	O
reference	O
][	O
reference	O
]	O
)	O
,	O
making	O
them	O
computationally	O
expensive	O
.	O

Our	O
approach	O
,	O
based	O
on	O
FPNs	Method
,	O
is	O
substantially	O
faster	O
(	O
our	O
models	O
run	O
at	O
6	O
to	O
7	O
FPS	Metric
)	O
.	O

These	O
results	O
demonstrate	O
that	O
our	O
model	O
is	O
a	O
generic	Method
feature	Method
extractor	Method
and	O
can	O
replace	O
image	Method
pyramids	Method
for	O
other	O
multi	Task
-	Task
scale	Task
detection	Task
problems	Task
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
clean	O
and	O
simple	O
framework	O
for	O
building	O
feature	Task
pyramids	Task
inside	O
ConvNets	Method
.	O

Our	O
method	O
shows	O
significant	O
improvements	O
over	O
several	O
strong	O
baselines	O
and	O
competition	O
winners	O
.	O

Thus	O
,	O
it	O
provides	O
a	O
practical	O
solution	O
for	O
research	Task
and	Task
applications	Task
of	Task
feature	Task
pyramids	Task
,	O
without	O
the	O
need	O
of	O
computing	O
image	O
pyramids	O
.	O

Finally	O
,	O
our	O
study	O
suggests	O
that	O
despite	O
the	O
strong	O
representational	O
power	O
of	O
deep	Method
ConvNets	Method
and	O
their	O
implicit	O
robustness	O
to	O
scale	O
variation	O
,	O
it	O
is	O
still	O
critical	O
to	O
explicitly	O
address	O
multiscale	Task
problems	Task
using	O
pyramid	Method
representations	Method
.	O

section	O
:	O
A.	O
Implementation	O
of	O
Segmentation	Task
Proposals	Task
We	O
use	O
our	O
feature	Method
pyramid	Method
networks	Method
to	O
efficiently	O
generate	O
object	Task
segment	Task
proposals	Task
,	O
adopting	O
an	O
image	Method
-	Method
centric	Method
training	Method
strategy	Method
popular	O
for	O
object	Task
detection	Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Our	O
FPN	Method
mask	O
generation	O
model	O
inherits	O
many	O
of	O
the	O
ideas	O
and	O
motivations	O
from	O
DeepMask	O
/	O
SharpMask	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
in	O
contrast	O
to	O
these	O
models	O
,	O
which	O
were	O
trained	O
on	O
image	O
crops	O
and	O
used	O
a	O
densely	O
sampled	O
image	O
pyramid	O
for	O
inference	Task
,	O
we	O
perform	O
fully	Method
-	Method
convolutional	Method
training	Method
for	O
mask	Task
prediction	Task
on	O
a	O
feature	O
pyramid	O
.	O

While	O
this	O
requires	O
changing	O
many	O
of	O
the	O
specifics	O
,	O
our	O
implementation	O
remains	O
similar	O
in	O
spirit	O
to	O
DeepMask	Method
.	O

Specifically	O
,	O
to	O
define	O
the	O
label	O
of	O
a	O
mask	O
instance	O
at	O
each	O
sliding	O
window	O
,	O
we	O
think	O
of	O
this	O
window	O
as	O
being	O
a	O
crop	O
on	O
the	O
input	O
image	O
,	O
allowing	O
us	O
to	O
inherit	O
definitions	O
of	O
positives	O
/	O
negatives	O
from	O
DeepMask	Method
.	O

We	O
give	O
more	O
details	O
next	O
,	O
see	O
also	O
Fig	O
.	O

4	O
for	O
a	O
visualization	O
.	O

We	O
construct	O
the	O
feature	O
pyramid	O
with	O
P	O
2−6	O
using	O
the	O
same	O
architecture	O
as	O
described	O
in	O
Sec	O
.	O

5.1	O
.	O

We	O
set	O
d	O
=	O
128	O
.	O

Each	O
level	O
of	O
our	O
feature	Method
pyramid	Method
is	O
used	O
for	O
predicting	O
masks	O
at	O
a	O
different	O
scale	O
.	O

As	O
in	O
DeepMask	Method
,	O
we	O
define	O
the	O
scale	O
of	O
a	O
mask	O
as	O
the	O
max	O
of	O
its	O
width	O
and	O
height	O
.	O

Masks	O
with	O
scales	O
of	O
{	O
32	O
,	O
64	O
,	O
128	O
,	O
256	O
,	O
512	O
}	O
pixels	O
map	O
to	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
,	O
respectively	O
,	O
and	O
are	O
handled	O
by	O
a	O
5×5	O
MLP	Method
.	O

As	O
DeepMask	Method
uses	O
a	O
pyramid	O
with	O
half	O
octaves	O
,	O
we	O
use	O
a	O
second	O
slightly	O
larger	O
MLP	Method
of	O
size	O
7×7	O
(	O
7	O
≈	O
5	O
√	O
2	O
)	O
to	O
handle	O
half	O
-	O
octaves	O
in	O
our	O
model	O
(	O
e.g.	O
,	O
a	O
128	O
√	O
2	O
scale	O
mask	O
is	O
predicted	O
by	O
the	O
7×7	O
MLP	Method
on	O
P	O
4	O
)	O
.	O

Objects	O
at	O
intermediate	O
scales	O
are	O
mapped	O
to	O
the	O
nearest	O
scale	O
in	O
log	O
space	O
.	O

As	O
the	O
MLP	Method
must	O
predict	O
objects	O
at	O
a	O
range	O
of	O
scales	O
for	O
each	O
pyramid	O
level	O
(	O
specifically	O
a	O
half	O
octave	O
range	O
)	O
,	O
some	O
padding	O
must	O
be	O
given	O
around	O
the	O
canonical	O
object	O
size	O
.	O

We	O
use	O
25	O
%	O
padding	Method
.	O

This	O
means	O
that	O
the	O
mask	O
output	O
over	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
maps	O
to	O
{	O
40	O
,	O
80	O
,	O
160	O
,	O
320	O
,	O
640	O
}	O
sized	O
image	O
regions	O
for	O
the	O
5×5	O
MLP	Method
(	O
and	O
to	O
√	O
2	O
larger	O
corresponding	O
sizes	O
for	O
the	O
7×7	Method
MLP	Method
)	O
.	O

Each	O
spatial	O
position	O
in	O
the	O
feature	O
map	O
is	O
used	O
to	O
predict	O
a	O
mask	O
at	O
a	O
different	O
location	O
.	O

Specifically	O
,	O
at	O
scale	O
P	O
k	O
,	O
each	O
spatial	O
position	O
in	O
the	O
feature	O
map	O
is	O
used	O
to	O
predict	O
the	O
mask	O
whose	O
center	O
falls	O
within	O
2	O
k	O
pixels	O
of	O
that	O
location	O
(	O
corresponding	O
to	O
±1	O
cell	O
offset	O
in	O
the	O
feature	O
map	O
)	O
.	O

If	O
no	O
object	O
center	O
falls	O
within	O
this	O
range	O
,	O
the	O
location	O
is	O
considered	O
a	O
negative	O
,	O
and	O
,	O
as	O
in	O
DeepMask	Method
,	O
is	O
used	O
only	O
for	O
training	O
the	O
score	O
branch	O
and	O
not	O
the	O
mask	O
branch	O
.	O

The	O
MLP	Method
we	O
use	O
for	O
predicting	O
the	O
mask	Metric
and	Metric
score	Metric
is	O
fairly	O
simple	O
.	O

We	O
apply	O
a	O
5×5	Method
kernel	Method
with	O
512	O
outputs	O
,	O
followed	O
by	O
sibling	O
fully	Method
connected	Method
layers	Method
to	O
predict	O
a	O
14×14	O
mask	O
(	O
14	O
2	O
outputs	O
)	O
and	O
object	O
score	O
(	O
1	O
output	O
)	O
.	O

The	O
model	O
is	O
implemented	O
in	O
a	O
fully	Method
convolutional	Method
manner	Method
(	O
using	O
1×1	Method
convolutions	Method
in	O
place	O
of	O
fully	Method
connected	Method
layers	Method
)	O
.	O

The	O
7×7	Method
MLP	Method
for	O
handling	O
objects	O
at	O
half	O
octave	O
scales	O
is	O
identical	O
to	O
the	O
5×5	O
MLP	Method
except	O
for	O
its	O
larger	O
input	O
region	O
.	O

During	O
training	Task
,	O
we	O
randomly	O
sample	O
2048	O
examples	O
per	O
mini	O
-	O
batch	O
(	O
128	O
examples	O
per	O
image	O
from	O
16	O
images	O
)	O
with	O
a	O
positive	O
/	O
negative	O
sampling	O
ratio	O
of	O
1:3	O
.	O

The	O
mask	O
loss	O
is	O
given	O
10×	O
higher	O
weight	O
than	O
the	O
score	Metric
loss	Metric
.	O

This	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
on	O
8	O
GPUs	Method
using	O
synchronized	Method
SGD	Method
(	O
2	O
images	O
per	O
GPU	O
)	O
.	O

We	O
start	O
with	O
a	O
learning	Metric
rate	Metric
of	O
0.03	O
and	O
train	O
for	O
80k	O
mini	O
-	O
batches	O
,	O
dividing	O
the	O
learning	Metric
rate	Metric
by	O
10	O
after	O
60k	O
mini	O
-	O
batches	O
.	O

The	O
image	O
scale	O
is	O
set	O
to	O
800	O
pixels	O
during	O
training	O
and	O
testing	O
(	O
we	O
do	O
not	O
use	O
scale	O
jitter	O
)	O
.	O

During	O
inference	Task
our	O
fully	Method
-	Method
convolutional	Method
model	Method
predicts	O
scores	O
at	O
all	O
positions	O
and	O
scales	O
and	O
masks	O
at	O
the	O
1000	O
highest	O
scoring	O
locations	O
.	O

We	O
do	O
not	O
perform	O
any	O
non	O
-	O
maximum	Method
suppression	Method
or	O
post	Task
-	Task
processing	Task
.	O

section	O
:	O
document	O
:	O
Improved	O
Language	Task
Modeling	Task
by	O
Decoding	O
the	O
Past	O
Highly	O
regularized	O
LSTMs	Method
achieve	O
impressive	O
results	O
on	O
several	O
benchmark	O
datasets	O
in	O
language	Task
modeling	Task
.	O

We	O
propose	O
a	O
new	O
regularization	Method
method	Method
based	O
on	O
decoding	O
the	O
last	O
token	O
in	O
the	O
context	O
using	O
the	O
predicted	O
distribution	O
of	O
the	O
next	O
token	O
.	O

This	O
biases	O
the	O
model	O
towards	O
retaining	O
more	O
contextual	O
information	O
,	O
in	O
turn	O
improving	O
its	O
ability	O
to	O
predict	O
the	O
next	O
token	O
.	O

With	O
negligible	O
overhead	O
in	O
the	O
number	O
of	O
parameters	O
and	O
training	Material
time	O
,	O
our	O
Past	Method
Decode	Method
Regularization	Method
(	O
PDR	Method
)	O
method	O
achieves	O
a	O
word	Metric
level	Metric
perplexity	Metric
of	O
55.6	O
on	O
the	O
Penn	Material
Treebank	Material
and	O
63.5	O
on	O
the	O
WikiText	Material
-	Material
2	Material
datasets	Material
using	O
a	O
single	O
softmax	Method
.	O

We	O
also	O
show	O
gains	O
by	O
using	O
PDR	Method
in	O
combination	O
with	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
,	O
achieving	O
a	O
word	Metric
level	Metric
perplexity	Metric
of	O
53.8	O
and	O
60.5	O
on	O
these	O
datasets	O
.	O

In	O
addition	O
,	O
our	O
method	O
achieves	O
1.169	O
bits	Metric
-	Metric
per	Metric
-	Metric
character	Metric
on	O
the	O
Penn	Material
Treebank	Material
Character	Material
dataset	Material
for	O
character	Task
level	Task
language	Task
modeling	Task
.	O

These	O
results	O
constitute	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
their	O
respective	O
settings	O
.	O

tickpos	O
=	O
left	O
colorbrewer	O
section	O
:	O
Introduction	O
Language	Task
modeling	Task
is	O
a	O
fundamental	O
task	O
in	O
natural	Task
language	Task
processing	Task
.	O

Given	O
a	O
sequence	O
of	O
tokens	O
,	O
its	O
joint	O
probability	O
distribution	O
can	O
be	O
modeled	O
using	O
the	O
auto	Method
-	Method
regressive	Method
conditional	Method
factorization	Method
.	O

This	O
leads	O
to	O
a	O
convenient	O
formulation	O
where	O
a	O
language	Method
model	Method
has	O
to	O
predict	O
the	O
next	O
token	O
given	O
a	O
sequence	O
of	O
tokens	O
as	O
context	O
.	O

Recurrent	Method
neural	Method
networks	Method
are	O
an	O
effective	O
way	O
to	O
compute	O
distributed	Task
representations	Task
of	Task
the	Task
context	Task
by	O
sequentially	O
operating	O
on	O
the	O
embeddings	O
of	O
the	O
tokens	O
.	O

These	O
representations	O
can	O
then	O
be	O
used	O
to	O
predict	O
the	O
next	O
token	O
as	O
a	O
probability	O
distribution	O
over	O
a	O
fixed	O
vocabulary	O
using	O
a	O
linear	O
decoder	Method
followed	O
by	O
Softmax	Method
.	O

Starting	O
from	O
the	O
work	O
of	O
,	O
there	O
has	O
been	O
a	O
long	O
list	O
of	O
works	O
that	O
seek	O
to	O
improve	O
language	Task
modeling	Task
performance	O
using	O
more	O
sophisticated	O
recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
(	O
)	O
.	O

However	O
,	O
in	O
more	O
recent	O
work	O
vanilla	O
LSTMs	Method
(	O
)	O
with	O
relatively	O
large	O
number	O
of	O
parameters	O
have	O
been	O
shown	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
several	O
standard	O
benchmark	O
datasets	O
both	O
in	O
word	Metric
-	Metric
level	Metric
and	O
character	Task
-	Task
level	Task
perplexity	Task
(	O
)	O
.	O

A	O
key	O
component	O
in	O
these	O
models	O
is	O
the	O
use	O
of	O
several	O
forms	O
of	O
regularization	Method
e.g.	O
variational	Method
dropout	Method
on	O
the	O
token	O
embeddings	O
(	O
)	O
,	O
dropout	Method
on	O
the	O
hidden	O
-	O
to	O
-	O
hidden	O
weights	O
in	O
the	O
LSTM	Method
(	O
)	O
,	O
norm	Method
regularization	Method
on	O
the	O
outputs	O
of	O
the	O
LSTM	Method
and	O
classical	Method
dropout	Method
(	O
)	O
.	O

By	O
carefully	O
tuning	O
the	O
hyperparameters	O
associated	O
with	O
these	O
regularizers	Method
combined	O
with	O
optimization	Method
algorithms	Method
like	O
NT	Method
-	Method
ASGD	Method
(	O
a	O
variant	O
of	O
the	O
Averaged	O
SGD	Method
)	O
,	O
it	O
is	O
possible	O
to	O
achieve	O
very	O
good	O
performance	O
.	O

Each	O
of	O
these	O
regularizations	O
address	O
different	O
parts	O
of	O
the	O
LSTM	Method
model	Method
and	O
are	O
general	O
techniques	O
that	O
could	O
be	O
applied	O
to	O
any	O
other	O
sequence	Task
modeling	Task
problem	Task
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
regularization	Method
technique	Method
that	O
is	O
specific	O
to	O
language	Task
modeling	Task
.	O

One	O
unique	O
aspect	O
of	O
language	Method
modeling	Method
using	O
LSTMs	Method
(	O
or	O
any	O
RNN	Method
)	O
is	O
that	O
at	O
each	O
time	O
step	O
,	O
the	O
model	O
takes	O
as	O
input	O
a	O
particular	O
token	O
from	O
a	O
vocabulary	O
and	O
using	O
the	O
hidden	O
state	O
of	O
the	O
LSTM	Method
(	O
which	O
encodes	O
the	O
context	O
till	O
)	O
predicts	O
a	O
probability	O
distribution	O
on	O
the	O
next	O
token	O
over	O
the	O
same	O
vocabulary	O
as	O
output	O
.	O

Since	O
can	O
be	O
mapped	O
to	O
a	O
trivial	O
probability	O
distribution	O
over	O
,	O
this	O
operation	O
can	O
be	O
interpreted	O
as	O
transforming	O
distributions	O
over	O
(	O
)	O
.	O

Clearly	O
,	O
the	O
output	O
distribution	O
is	O
dependent	O
on	O
and	O
is	O
a	O
function	O
of	O
and	O
the	O
context	O
further	O
in	O
the	O
past	O
and	O
encodes	O
information	O
about	O
it	O
.	O

We	O
ask	O
the	O
following	O
question	O
–	O
How	O
much	O
information	O
is	O
it	O
possible	O
to	O
decode	O
about	O
the	O
input	O
distribution	O
(	O
and	O
hence	O
)	O
from	O
the	O
output	O
distribution	O
?	O
In	O
general	O
,	O
it	O
is	O
impossible	O
to	O
decode	O
unambiguously	O
.	O

Even	O
if	O
the	O
language	Method
model	Method
is	O
perfect	O
and	O
correctly	O
predicts	O
with	O
probability	O
1	O
,	O
there	O
could	O
be	O
many	O
tokens	O
preceding	O
it	O
.	O

However	O
,	O
in	O
this	O
case	O
the	O
number	O
of	O
possibilities	O
for	O
will	O
be	O
limited	O
,	O
as	O
dictated	O
by	O
the	O
bigram	O
statistics	O
of	O
the	O
corpus	O
and	O
the	O
language	O
in	O
general	O
.	O

We	O
argue	O
that	O
biasing	O
the	O
language	Method
model	Method
such	O
that	O
it	O
is	O
possible	O
to	O
decode	O
more	O
information	O
about	O
the	O
past	O
tokens	O
from	O
the	O
predicted	O
next	O
token	O
distribution	O
is	O
beneficial	O
.	O

We	O
incorporate	O
this	O
intuition	O
into	O
a	O
regularization	Method
term	Method
in	O
the	O
loss	O
function	O
of	O
the	O
language	Method
model	Method
.	O

The	O
symmetry	O
in	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
language	Method
model	Method
at	O
each	O
step	O
lends	O
itself	O
to	O
a	O
simple	O
decoding	Method
operation	Method
.	O

It	O
can	O
be	O
cast	O
as	O
a	O
(	O
pseudo	Task
)	Task
language	Task
modeling	Task
problem	Task
in	O
“	O
reverse	Task
”	Task
,	O
where	O
the	O
future	Task
prediction	Task
acts	O
as	O
the	O
input	O
and	O
the	O
last	O
token	O
acts	O
as	O
the	O
target	O
of	O
prediction	Task
.	O

The	O
token	O
embedding	O
matrix	O
and	O
weights	O
of	O
the	O
linear	O
decoder	Method
of	O
the	O
main	Method
language	Method
model	Method
can	O
be	O
reused	O
in	O
the	O
past	O
decoding	Task
operation	Task
.	O

We	O
only	O
need	O
a	O
few	O
extra	O
parameters	O
to	O
model	O
the	O
nonlinear	O
transformation	O
performed	O
by	O
the	O
LSTM	Method
,	O
which	O
we	O
do	O
by	O
using	O
a	O
simple	O
stateless	Method
layer	Method
.	O

We	O
compute	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
between	O
the	O
decoded	O
distribution	O
for	O
the	O
past	O
token	O
and	O
and	O
add	O
it	O
to	O
the	O
main	O
loss	O
function	O
after	O
suitable	O
weighting	O
.	O

The	O
extra	O
parameters	O
used	O
in	O
the	O
past	O
decoding	O
are	O
discarded	O
during	O
inference	O
time	O
.	O

We	O
call	O
our	O
method	O
Past	Method
Decode	Method
Regularization	Method
or	O
PDR	Method
for	O
short	O
.	O

We	O
conduct	O
extensive	O
experiments	O
on	O
four	O
benchmark	O
datasets	O
for	O
word	Task
level	Task
and	Task
character	Task
level	Task
language	Task
modeling	Task
by	O
combining	O
PDR	Method
with	O
existing	O
LSTM	Method
based	O
language	O
models	O
and	O
achieve	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
three	O
of	O
them	O
.	O

section	O
:	O
Past	Method
Decode	Method
Regularization	Method
(	O
PDR	Method
)	O
Let	O
be	O
a	O
sequence	O
of	O
tokens	O
.	O

In	O
this	O
paper	O
,	O
we	O
will	O
experiment	O
with	O
both	O
word	Method
level	Method
and	Method
character	Method
level	Method
language	Method
modeling	Method
.	O

Therefore	O
,	O
tokens	O
can	O
be	O
either	O
words	O
or	O
characters	O
.	O

The	O
joint	O
probability	O
factorizes	O
into	O
Let	O
denote	O
the	O
context	O
available	O
to	O
the	O
language	Method
model	Method
for	O
.	O

Let	O
denote	O
the	O
vocabulary	O
of	O
tokens	O
,	O
each	O
of	O
which	O
is	O
embedded	O
into	O
a	O
vector	O
of	O
dimension	O
.	O

Let	O
denote	O
the	O
token	O
embedding	O
matrix	O
of	O
dimension	O
and	O
denote	O
the	O
embedding	O
of	O
.	O

An	O
LSTM	Method
computes	O
a	O
distributed	Method
representation	Method
of	O
in	O
the	O
form	O
of	O
its	O
hidden	O
state	O
,	O
which	O
we	O
assume	O
has	O
dimension	O
as	O
well	O
.	O

The	O
probability	O
that	O
the	O
next	O
token	O
is	O
can	O
then	O
be	O
calculated	O
using	O
a	O
linear	O
decoder	Method
followed	O
by	O
a	O
Softmax	Method
layer	Method
as	O
where	O
is	O
the	O
entry	O
corresponding	O
to	O
in	O
a	O
bias	O
vector	O
of	O
dimension	O
and	O
represents	O
projection	O
onto	O
.	O

Here	O
we	O
assume	O
that	O
the	O
weights	O
of	O
the	O
decoder	Method
are	O
tied	O
with	O
the	O
token	Method
embedding	Method
matrix	Method
(	O
)	O
.	O

To	O
optimize	O
the	O
parameters	O
of	O
the	O
language	Method
model	Method
,	O
the	O
loss	O
function	O
to	O
be	O
minimized	O
during	O
training	Material
is	O
set	O
as	O
the	O
cross	O
-	O
entropy	O
between	O
the	O
predicted	O
distribution	O
and	O
the	O
actual	O
token	O
.	O

Note	O
that	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
when	O
applied	O
to	O
all	O
produces	O
a	O
vector	O
,	O
encapsulating	O
the	O
prediction	O
the	O
language	Method
model	Method
has	O
about	O
the	O
next	O
token	O
.	O

Since	O
this	O
is	O
dependent	O
on	O
and	O
conditioned	O
on	O
,	O
clearly	O
encodes	O
information	O
about	O
it	O
;	O
in	O
particular	O
about	O
the	O
last	O
token	O
in	O
.	O

In	O
turn	O
,	O
it	O
should	O
be	O
possible	O
to	O
infer	O
or	O
decode	O
some	O
limited	O
information	O
about	O
from	O
.	O

We	O
argue	O
that	O
by	O
biasing	O
the	O
model	O
to	O
be	O
more	O
accurate	O
in	O
recalling	O
information	O
about	O
past	O
tokens	O
,	O
we	O
can	O
help	O
it	O
in	O
predicting	O
the	O
next	O
token	O
better	O
.	O

To	O
this	O
end	O
,	O
we	O
define	O
the	O
following	O
decoding	Method
operation	Method
to	O
compute	O
a	O
probability	O
distribution	O
over	O
as	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

Here	O
is	O
a	O
non	Method
-	Method
linear	Method
function	Method
that	O
maps	O
vectors	O
in	O
to	O
vectors	O
in	O
and	O
is	O
a	O
bias	O
vector	O
of	O
dimension	O
,	O
together	O
with	O
parameters	O
.	O

In	O
effect	O
,	O
we	O
are	O
decoding	O
the	O
past	O
–	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

This	O
produces	O
a	O
vector	O
of	O
dimension	O
.	O

The	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
with	O
respect	O
to	O
the	O
actual	O
last	O
token	O
can	O
then	O
be	O
computed	O
as	O
Here	O
stands	O
for	O
Past	Method
Decode	Method
Regularization	Method
.	O

captures	O
the	O
extent	O
to	O
which	O
the	O
decoded	O
distribution	O
of	O
tokens	O
differs	O
from	O
the	O
actual	O
tokens	O
in	O
the	O
context	O
.	O

Note	O
the	O
symmetry	O
between	O
Eqs	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
“	O
input	O
”	O
in	O
the	O
latter	O
case	O
is	O
and	O
the	O
“	O
context	O
”	O
is	O
provided	O
by	O
a	O
nonlinear	Method
transformation	Method
of	O
.	O

Different	O
from	O
the	O
former	O
,	O
the	O
context	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
does	O
not	O
preserve	O
any	O
state	O
information	O
across	O
time	O
steps	O
as	O
we	O
want	O
to	O
decode	O
only	O
using	O
.	O

The	O
term	O
can	O
be	O
interpreted	O
as	O
a	O
“	O
soft	Method
”	Method
token	Method
embedding	Method
lookup	Method
,	O
where	O
the	O
token	O
vector	O
is	O
a	O
probability	O
distribution	O
instead	O
of	O
a	O
unit	O
vector	O
.	O

We	O
add	O
to	O
the	O
loss	O
function	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
as	O
a	O
regularization	O
term	O
,	O
where	O
is	O
a	O
positive	O
weighting	O
coefficient	O
,	O
to	O
construct	O
the	O
following	O
new	O
loss	Method
function	Method
for	O
the	O
language	Method
model	Method
.	O

Thus	O
equivalently	O
PDR	Method
can	O
also	O
be	O
viewed	O
as	O
a	O
method	O
of	O
defining	O
an	O
augmented	Method
loss	Method
function	Method
for	O
language	Task
modeling	Task
.	O

The	O
choice	O
of	O
dictates	O
the	O
degree	O
to	O
which	O
we	O
want	O
the	O
language	Method
model	Method
to	O
incorporate	O
our	O
inductive	O
bias	O
i.e.	O
decodability	O
of	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

If	O
it	O
is	O
too	O
large	O
,	O
the	O
model	O
will	O
fail	O
to	O
predict	O
the	O
next	O
token	O
,	O
which	O
is	O
its	O
primary	O
task	O
.	O

If	O
it	O
is	O
zero	O
or	O
too	O
small	O
,	O
the	O
model	O
will	O
retain	O
less	O
information	O
about	O
the	O
last	O
token	O
which	O
hampers	O
its	O
predictive	O
performance	O
.	O

In	O
practice	O
,	O
we	O
choose	O
by	O
a	O
search	O
based	O
on	O
validation	Metric
set	Metric
performance	Metric
.	O

Note	O
that	O
the	O
trainable	O
parameters	O
associated	O
with	O
PDR	Method
are	O
used	O
only	O
during	O
training	Material
to	O
bias	O
the	O
language	Method
model	Method
and	O
are	O
not	O
used	O
at	O
inference	Task
time	Task
.	O

This	O
also	O
means	O
that	O
it	O
is	O
important	O
to	O
control	O
the	O
complexity	O
of	O
the	O
nonlinear	O
function	O
so	O
as	O
not	O
to	O
overly	O
bias	O
the	O
training	Material
.	O

As	O
a	O
simple	O
choice	O
,	O
we	O
use	O
a	O
single	O
fully	Method
connected	Method
layer	Method
of	Method
size	Method
followed	O
by	O
a	O
Tanh	O
nonlinearity	O
as	O
.	O

This	O
introduces	O
few	O
extra	O
parameters	O
and	O
a	O
small	O
increase	O
in	O
training	Material
time	O
as	O
compared	O
to	O
a	O
model	O
not	O
using	O
PDR	Method
.	O

section	O
:	O
Experiments	O
We	O
present	O
extensive	O
experimental	O
results	O
to	O
show	O
the	O
efficacy	O
of	O
using	O
PDR	Method
for	O
language	Task
modeling	Task
on	O
four	O
standard	O
benchmark	O
datasets	O
–	O
two	O
each	O
for	O
word	Task
level	Task
and	Task
character	Task
level	Task
language	Task
modeling	Task
.	O

For	O
the	O
former	O
,	O
we	O
evaluate	O
our	O
method	O
on	O
the	O
Penn	Material
Treebank	Material
(	O
PTB	Material
)	O
(	O
)	O
and	O
the	O
WikiText	Material
-	Material
2	Material
(	O
WT2	Material
)	O
(	O
)	O
datasets	O
.	O

For	O
the	O
latter	O
,	O
we	O
use	O
the	O
Penn	Material
Treebank	Material
Character	Material
(	O
PTBC	Material
)	O
(	O
)	O
and	O
the	O
Hutter	Material
Prize	Material
Wikipedia	Material
Prize	Material
(	O
)	O
(	O
also	O
known	O
as	O
Enwik8	Material
)	O
datasets	O
.	O

Key	O
statistics	O
for	O
these	O
datasets	O
is	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
mentioned	O
in	O
the	O
introduction	O
,	O
some	O
of	O
the	O
best	O
existing	O
results	O
on	O
these	O
datasets	O
are	O
obtained	O
by	O
using	O
extensive	O
regularization	Method
techniques	Method
on	O
relatively	O
large	O
LSTMs	Method
(	O
)	O
.	O

We	O
apply	O
our	O
regularization	Method
technique	Method
to	O
these	O
models	O
,	O
the	O
so	O
called	O
AWD	Method
-	Method
LSTM	Method
.	O

We	O
consider	O
two	O
versions	O
of	O
the	O
model	O
–	O
one	O
with	O
a	O
single	O
softmax	Method
(	Method
AWD	Method
-	Method
LSTM	Method
)	O
and	O
one	O
with	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
(	O
AWD	Method
-	Method
LSTM	Method
-	Method
MoS	Method
)	O
.	O

The	O
PDR	Method
regularization	O
term	O
is	O
computed	O
according	O
to	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

We	O
call	O
our	O
model	Method
AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
when	O
using	O
a	O
single	O
softmax	Method
and	O
AWD	Method
-	Method
LSTM	Method
-	Method
MoS	Method
+	Method
PDR	Method
when	O
using	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
.	O

We	O
largely	O
follow	O
the	O
experimental	O
procedure	O
of	O
the	O
original	O
models	O
and	O
incorporate	O
their	O
dropouts	Method
and	O
regularizations	Method
in	O
our	O
experiments	O
.	O

The	O
relative	O
contribution	O
of	O
these	O
existing	O
regularizations	Method
and	O
PDR	Method
will	O
be	O
analyzed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

There	O
are	O
7	O
hyperparameters	O
associated	O
with	O
the	O
regularizations	Method
used	O
in	O
AWD	Method
-	Method
LSTM	Method
(	O
and	O
one	O
extra	O
with	O
MoS	Method
)	O
.	O

PDR	Method
also	O
has	O
an	O
associated	O
weighting	O
coefficient	O
.	O

For	O
our	O
experiments	O
,	O
we	O
set	O
which	O
was	O
determined	O
by	O
a	O
coarse	Method
search	Method
on	O
the	O
PTB	Material
and	O
WT2	Material
validation	O
sets	O
.	O

For	O
the	O
remaining	O
ones	O
,	O
we	O
perform	O
light	O
hyperparameter	Method
search	Method
in	O
the	O
vicinity	O
of	O
those	O
reported	O
for	O
AWD	Method
-	Method
LSTM	Method
in	Method
and	O
for	O
AWD	Method
-	Method
LSTM	Method
-	Method
MoS	Method
in	O
.	O

subsection	O
:	O
Model	O
and	O
training	Material
for	O
PTB	Material
and	O
WikiText	Material
-	Material
2	Material
For	O
the	O
single	Method
softmax	Method
model	Method
(	O
AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
)	Method
,	O
for	O
both	O
PTB	Material
and	O
WT2	Material
,	O
we	O
use	O
a	O
3	O
-	O
layered	O
LSTM	Method
with	O
1150	O
,	O
1150	O
and	O
400	O
hidden	O
dimensions	O
.	O

The	O
word	O
embedding	O
dimension	O
is	O
set	O
to	O
.	O

For	O
the	O
mixture	Method
-	Method
of	Method
-	Method
softmax	Method
model	Method
,	O
we	O
use	O
a	O
3	O
-	O
layer	O
LSTM	Method
with	O
dimensions	O
960	O
,	O
960	O
and	O
620	O
,	O
embedding	O
dimension	O
of	O
280	O
and	O
15	O
experts	O
for	O
PTB	Material
and	O
a	O
3	O
-	O
layer	O
LSTM	Method
with	O
dimensions	O
1150	O
,	O
1150	O
and	O
650	O
,	O
embedding	O
dimension	O
of	O
and	O
15	O
experts	O
for	O
WT2	Material
.	O

Weight	Method
tying	Method
is	O
used	O
in	O
all	O
the	O
models	O
.	O

For	O
training	Material
the	O
models	O
,	O
we	O
follow	O
the	O
same	O
procedure	O
as	O
AWD	Method
-	Method
LSTM	Method
i.e.	Method
a	O
combination	O
of	O
SGD	Method
and	O
NT	Method
-	Method
ASGD	Method
,	O
followed	O
by	O
finetuning	Method
.	O

We	O
adopt	O
the	O
learning	O
rate	O
schedules	O
and	O
batch	O
sizes	O
of	O
and	O
in	O
our	O
experiments	O
.	O

subsection	O
:	O
Model	O
and	O
training	Material
for	O
PTBC	Material
and	O
Enwik8	Material
For	O
PTBC	Material
,	O
we	O
use	O
a	O
3	O
-	O
layer	O
LSTM	Method
with	O
1000	O
,	O
1000	O
and	O
200	O
hidden	O
dimensions	O
and	O
a	O
character	O
embedding	O
dimension	O
of	O
.	O

For	O
Enwik8	Material
,	O
we	O
use	O
a	O
LSTM	Method
with	O
1850	O
,	O
1850	O
and	O
400	O
hidden	O
dimensions	O
and	O
the	O
characters	O
are	O
embedded	O
in	O
dimensions	O
.	O

For	O
training	Material
,	O
we	O
largely	O
follow	O
the	O
procedure	O
laid	O
out	O
in	O
.	O

For	O
each	O
of	O
the	O
datasets	O
,	O
AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
has	O
less	O
than	O
1	O
%	O
more	O
parameters	O
than	O
the	O
corresponding	O
AWD	Method
-	Method
LSTM	Method
model	Method
(	O
during	O
training	Material
only	O
)	O
.	O

The	O
maximum	O
observed	Metric
time	Metric
overhead	Metric
due	O
to	O
the	O
additional	O
computation	O
is	O
less	O
than	O
3	O
%	O
.	O

section	O
:	O
Results	O
on	O
Word	O
Level	O
Language	Task
Modeling	Task
The	O
results	O
for	O
PTB	Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

With	O
a	O
single	O
softmax	Method
,	O
our	O
method	O
(	O
AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
)	O
achieves	O
a	O
perplexity	Metric
of	O
55.6	O
on	O
the	O
PTB	Material
test	Material
set	Material
,	O
which	O
improves	O
on	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
a	O
single	O
softmax	O
by	O
an	O
absolute	O
1.7	O
points	O
.	O

The	O
advantages	O
of	O
better	O
information	Task
retention	Task
due	O
to	O
PDR	Method
are	O
maintained	O
when	O
combined	O
with	O
a	O
continuous	O
cache	O
pointer	O
(	O
)	O
,	O
where	O
our	O
method	O
yields	O
an	O
absolute	O
improvement	O
of	O
1.2	O
over	O
AWD	Method
-	Method
LSTM	Method
.	O

Notably	O
,	O
when	O
coupled	O
with	O
dynamic	Method
evaluation	Method
(	O
)	O
,	O
the	O
perplexity	Metric
is	O
decreased	O
further	O
to	O
49.3	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
ours	O
is	O
the	O
first	O
method	O
to	O
achieve	O
a	O
sub	Task
50	Task
perplexity	Task
on	O
the	O
PTB	Material
test	Material
set	Material
with	O
a	O
single	O
softmax	Method
.	O

Note	O
that	O
,	O
for	O
both	O
cache	Task
pointer	Task
and	Task
dynamic	Task
evaluation	Task
,	O
we	O
coarsely	O
tune	O
the	O
associated	O
hyperparameters	O
on	O
the	O
validation	O
set	O
.	O

Using	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
,	O
our	O
method	O
(	O
AWD	Method
-	Method
LSTM	Method
-	Method
MoS	Method
+	Method
PDR	Method
)	O
achieves	O
a	O
test	Metric
perplexity	Metric
of	O
53.8	O
,	O
an	O
improvement	O
of	O
0.6	O
points	O
over	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

The	O
use	O
of	O
dynamic	Method
evaluation	Method
pushes	O
the	O
perplexity	O
further	O
down	O
to	O
47.3	O
.	O

PTB	Material
is	O
a	O
restrictive	O
dataset	O
with	O
a	O
vocabulary	O
of	O
10	O
K	O
words	O
.	O

Achieving	O
good	O
perplexity	Task
requires	O
considerable	O
regularization	O
.	O

The	O
fact	O
that	O
PDR	Method
can	O
improve	O
upon	O
existing	O
heavily	Method
regularized	Method
models	Method
is	O
empirical	O
evidence	O
of	O
its	O
distinctive	O
nature	O
and	O
its	O
effectiveness	O
in	O
improving	O
language	Method
models	Method
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
perplexities	O
achieved	O
by	O
our	O
model	O
on	O
WT2	Material
.	O

This	O
dataset	O
is	O
considerably	O
more	O
complex	O
than	O
PTB	Material
with	O
a	O
vocabulary	O
of	O
more	O
than	O
33	O
K	O
words	O
.	O

AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
improves	O
over	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
a	O
single	O
softmax	Method
by	O
a	O
significant	O
2.3	O
points	O
,	O
achieving	O
a	O
perplexity	Metric
of	O
63.5	O
.	O

The	O
gains	O
are	O
maintained	O
with	O
the	O
use	O
of	O
cache	O
pointer	O
(	O
2.4	O
points	O
)	O
and	O
with	O
the	O
use	O
of	O
dynamic	O
evaluation	O
(	O
1.7	O
points	O
)	O
.	O

Using	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
,	O
AWD	Method
-	Method
LSTM	Method
-	Method
MoS	Method
+	Method
PDR	Method
achieves	O
perplexities	Method
of	O
60.5	O
and	O
40.3	O
(	O
with	O
dynamic	Metric
evaluation	Metric
)	O
on	O
the	O
WT2	Material
test	Material
set	Material
,	O
improving	O
upon	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
1.0	O
and	O
0.4	O
points	O
respectively	O
.	O

subsection	O
:	O
Performance	O
on	O
Larger	O
Datasets	O
We	O
consider	O
the	O
Gigaword	O
dataset	O
with	O
a	O
truncated	O
vocabulary	O
of	O
about	O
100	O
K	O
tokens	O
with	O
the	O
highest	O
frequency	O
and	O
apply	O
PDR	Method
to	O
a	O
baseline	O
2	O
-	O
layer	O
LSTM	Method
language	O
model	O
with	O
embedding	O
and	O
hidden	O
dimensions	O
set	O
to	O
1024	O
.	O

We	O
use	O
all	O
the	O
shards	O
from	O
the	O
training	Material
set	O
for	O
training	Material
and	O
a	O
few	O
shards	O
from	O
the	O
heldout	O
set	O
for	O
validation	Task
(	O
heldout	O
-	O
0	O
,	O
10	O
)	O
and	O
test	O
(	O
heldout	O
-	O
20	O
,	O
30	O
,	O
40	O
)	O
.	O

We	O
tuned	O
the	O
PDR	Method
coefficient	O
coarsely	O
in	O
the	O
vicinity	O
of	O
0.001	O
.	O

While	O
the	O
baseline	O
model	O
achieved	O
a	O
validation	O
(	O
test	O
)	O
perplexity	Metric
of	O
44.3	O
(	O
43.1	O
)	O
,	O
on	O
applying	O
PDR	Method
,	O
the	O
model	O
achieved	O
a	O
perplexity	Metric
of	O
44.0	O
(	O
42.5	O
)	O
.	O

Thus	O
,	O
PDR	Method
is	O
relatively	O
less	O
effective	O
on	O
larger	O
datasets	O
,	O
a	O
fact	O
also	O
observed	O
for	O
other	O
regularization	Method
techniques	Method
on	O
such	O
datasets	O
(	O
)	O
.	O

section	O
:	O
Results	O
on	O
Character	O
Level	O
Language	Task
Modeling	Task
The	O
results	O
on	O
PTBC	Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
method	O
achieves	O
a	O
bits	Metric
-	Metric
per	Metric
-	Metric
character	Metric
(	O
BPC	Metric
)	O
performance	O
of	O
1.169	O
on	O
the	O
PTBC	Material
test	Material
set	Material
,	O
improving	O
on	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
0.006	O
or	O
0.5	O
%	O
.	O

It	O
is	O
notable	O
that	O
even	O
with	O
this	O
highly	O
processed	O
dataset	O
and	O
a	O
small	O
vocabulary	O
of	O
only	O
51	O
tokens	O
,	O
our	O
method	O
improves	O
on	O
already	O
highly	O
regularized	Method
models	Method
.	O

Finally	O
,	O
we	O
present	O
results	O
on	O
Enwik8	Material
in	O
Table	O
[	O
reference	O
]	O
.	O

AWD	Method
-	Method
LSTM	Method
+	Method
PDR	Method
achieves	O
1.245	O
BPC	Metric
.	O

This	O
is	O
0.012	O
or	O
about	O
1	O
%	O
less	O
than	O
the	O
1.257	O
BPC	Metric
achieved	O
by	O
AWD	Method
-	Method
LSTM	Method
in	O
our	O
experiments	O
(	O
with	O
hyperparameters	O
from	O
)	O
.	O

section	O
:	O
Analysis	O
of	O
PDR	Method
In	O
this	O
section	O
,	O
we	O
analyze	O
PDR	Method
by	O
probing	O
its	O
performance	O
in	O
several	O
ways	O
and	O
comparing	O
it	O
with	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
that	O
do	O
not	O
use	O
PDR	Method
.	O

subsection	O
:	O
A	O
Valid	O
Regularization	Method
To	O
verify	O
that	O
indeed	O
PDR	Method
can	O
act	O
as	O
a	O
form	O
of	O
regularization	Method
,	O
we	O
perform	O
the	O
following	O
experiment	O
.	O

We	O
take	O
the	O
models	O
for	O
PTB	Material
and	O
WT2	Material
and	O
turn	O
off	O
all	O
dropouts	Method
and	O
regularization	Method
and	O
compare	O
its	O
performance	O
with	O
only	O
PDR	Method
turned	O
on	O
.	O

The	O
results	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
validate	O
the	O
premise	O
of	O
PDR	Method
.	O

The	O
model	O
with	O
only	O
PDR	Method
turned	O
on	O
achieves	O
2.4	O
and	O
5.1	O
better	O
validation	Metric
perplexity	Metric
on	O
PTB	Material
and	O
WT2	Material
as	O
compared	O
to	O
the	O
model	O
without	O
any	O
regularization	Method
.	O

Thus	O
,	O
biasing	O
the	O
LSTM	Method
by	O
decoding	O
the	O
distribution	O
of	O
past	O
tokens	O
from	O
the	O
predicted	O
next	O
-	O
token	O
distribution	O
can	O
indeed	O
act	O
as	O
a	O
regularizer	Method
leading	O
to	O
better	O
generalization	Task
performance	O
.	O

Next	O
,	O
we	O
plot	O
histograms	O
of	O
the	O
negative	O
log	O
-	O
likelihoods	O
of	O
the	O
correct	O
context	O
tokens	O
in	O
the	O
past	O
decoded	O
vector	O
computed	O
using	O
our	O
best	O
models	O
on	O
the	O
PTB	Material
and	O
WT2	Material
validation	O
sets	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
.	O

The	O
NLL	O
values	O
are	O
significantly	O
peaked	O
near	O
0	O
,	O
which	O
means	O
that	O
the	O
past	Method
decoding	Method
operation	Method
is	O
able	O
to	O
decode	O
significant	O
amount	O
of	O
information	O
about	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

To	O
investigate	O
the	O
effect	O
of	O
hyperparameters	O
on	O
PDR	Method
,	O
we	O
pick	O
60	O
sets	O
of	O
random	O
hyperparameters	O
in	O
the	O
vicinity	O
of	O
those	O
reported	O
by	O
and	O
compute	O
the	O
validation	Metric
set	Metric
perplexity	Metric
after	O
training	Material
(	O
without	O
finetuning	Method
)	O
on	O
PTB	Material
,	O
for	O
both	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
and	O
AWD	Method
-	Method
LSTM	Method
.	O

Their	O
histograms	O
are	O
plotted	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
.	O

The	O
perplexities	O
for	O
models	O
with	O
PDR	Method
are	O
distributed	O
slightly	O
to	O
the	O
left	O
of	O
those	O
without	O
PDR	Method
.	O

There	O
appears	O
to	O
be	O
more	O
instances	O
of	O
perplexities	O
in	O
the	O
higher	O
range	O
for	O
models	O
without	O
PDR	Method
.	O

Note	O
that	O
there	O
are	O
certainly	O
hyperparameter	O
settings	O
where	O
adding	O
PDR	Method
leads	O
to	O
lower	O
validation	Metric
complexity	Metric
,	O
as	O
is	O
generally	O
the	O
case	O
for	O
any	O
regularization	Method
method	Method
.	O

[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Negative	O
log	O
-	O
likelihood	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.05	O
,	O
0.5	O
)	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
legend	O
pos=	O
north	O
east	O
,	O
legend	O
entries	O
=	O
PTB	Material
-	O
Valid	O
,	O
WT2	Material
-	O
Valid	O
,	O
]	O
[	O
magenta	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.316464626105	O
)(	O
1.0	O
,	O
0.0851737740793	O
)(	O
1.67	O
,	O
0.0652768579573	O
)(	O
2.33	O
,	O
0.057819113709	O
)(	O
3.0	O
,	O
0.0520170462726	O
)(	O
3.67	O
,	O
0.0518154856172	O
)(	O
4.33	O
,	O
0.050044631288	O
)(	O
5.0	O
,	O
0.0517866912379	O
)(	O
5.67	O
,	O
0.0515851305825	O
)(	O
6.33	O
,	O
0.0497854818739	O
)(	O
7.0	O
,	O
0.0443865357482	O
)(	O
7.67	O
,	O
0.0388580149155	O
)(	O
8.33	O
,	O
0.0341789282732	O
)(	O
9.0	O
,	O
0.0278297676294	O
)(	O
9.67	O
,	O
0.022977914711	O
)	O
;	O
[	O
black	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.30860908651	O
)(	O
1.0	O
,	O
0.0787424358073	O
)(	O
1.67	O
,	O
0.0643498909958	O
)(	O
2.33	O
,	O
0.059959790672	O
)(	O
3.0	O
,	O
0.0559238916244	O
)(	O
3.67	O
,	O
0.0553451965817	O
)(	O
4.33	O
,	O
0.0522072726003	O
)(	O
5.0	O
,	O
0.0524417438676	O
)(	O
5.67	O
,	O
0.0500670986924	O
)(	O
6.33	O
,	O
0.0488797761049	O
)(	O
7.0	O
,	O
0.0446293607914	O
)(	O
7.67	O
,	O
0.0396455991739	O
)(	O
8.33	O
,	O
0.0349711400791	O
)(	O
9.0	O
,	O
0.0289646846361	O
)(	O
9.67	O
,	O
0.0252630318631	O
)	O
;	O
[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
ymax=0.3	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Perplexity	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.02	O
,	O
0.5	O
)	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
y	O
tick	O
label	O
style=	O
/	O
pgf	O
/	O
number	O
format	O
/	O
.cd	O
,	O
fixed	O
,	O
fixed	O
zerofill	O
,	O
precision=2	O
,	O
legend	O
pos=	O
north	O
west	O
,	O
legend	O
entries	O
=	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
,	O
AWD	Method
-	Method
LSTM	Method
,	O
]	O
[	O
red	O
]	O
coordinates	O
(	O
60.17	O
,	O
0.016393442623	O
)(	O
60.32	O
,	O
0.0	O
)(	O
60.47	O
,	O
0.016393442623	O
)(	O
60.61	O
,	O
0.0655737704918	O
)(	O
60.76	O
,	O
0.114754098361	O
)(	O
60.91	O
,	O
0.131147540984	O
)(	O
61.05	O
,	O
0.114754098361	O
)(	O
61.2	O
,	O
0.147540983607	O
)(	O
61.35	O
,	O
0.229508196721	O
)(	O
61.49	O
,	O
0.0655737704918	O
)(	O
61.64	O
,	O
0.0655737704918	O
)(	O
61.79	O
,	O
0.0	O
)(	O
61.93	O
,	O
0.016393442623	O
)(	O
62.08	O
,	O
0.0	O
)(	O
62.23	O
,	O
0.016393442623	O
)	O
;	O
[	O
black!60!green	O
]	O
coordinates	O
(	O
60.17	O
,	O
0.0	O
)(	O
60.32	O
,	O
0.0	O
)(	O
60.47	O
,	O
0.0	O
)(	O
60.61	O
,	O
0.0655737704918	O
)(	O
60.76	O
,	O
0.0819672131148	O
)(	O
60.91	O
,	O
0.16393442623	O
)(	O
61.05	O
,	O
0.180327868852	O
)(	O
61.2	O
,	O
0.131147540984	O
)(	O
61.35	O
,	O
0.0983606557377	O
)(	O
61.49	O
,	O
0.114754098361	O
)(	O
61.64	O
,	O
0.131147540984	O
)(	O
61.79	O
,	O
0.0327868852459	O
)(	O
61.93	O
,	O
0.0	O
)(	O
62.08	O
,	O
0.0	O
)(	O
62.23	O
,	O
0.0	O
)	O
;	O
subsection	O
:	O
Comparison	O
with	O
AWD	Method
-	Method
LSTM	Method
cycle	O
list	O
/	O
Set1	O
-	O
4	O
[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Predicted	O
token	O
entropy	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.02	O
,	O
0.5	O
)	O
,	O
y	O
tick	O
label	O
style=	O
/	O
pgf	O
/	O
number	O
format	O
/	O
.cd	O
,	O
fixed	O
,	O
fixed	O
zerofill	O
,	O
precision=2	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
legend	O
pos=	O
north	O
west	O
,	O
legend	O
entries	O
=	O
AWD	O
-	O
LSTM	Method
+	O
PDR	Method
,	O
AWD	O
-	O
LSTM	Method
,	O
]	O
[	O
red	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.0668799739693	O
)(	O
1.0	O
,	O
0.031575807698	O
)(	O
1.67	O
,	O
0.0282406214835	O
)(	O
2.33	O
,	O
0.0319960953917	O
)(	O
3.0	O
,	O
0.037310701067	O
)(	O
3.67	O
,	O
0.045865589284	O
)(	O
4.33	O
,	O
0.064331132472	O
)(	O
5.0	O
,	O
0.0876367629713	O
)(	O
5.67	O
,	O
0.108068167952	O
)(	O
6.33	O
,	O
0.133922639949	O
)(	O
7.0	O
,	O
0.145975406391	O
)(	O
7.67	O
,	O
0.117490746892	O
)(	O
8.33	O
,	O
0.0765737062596	O
)(	O
9.0	O
,	O
0.0230073618135	O
)(	O
9.67	O
,	O
0.00112528640573	O
)	O
;	O
[	O
black!60!green	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.0684582220475	O
)(	O
1.0	O
,	O
0.0332840738079	O
)(	O
1.67	O
,	O
0.031372442685	O
)(	O
2.33	O
,	O
0.0344093602137	O
)(	O
3.0	O
,	O
0.0409305982999	O
)(	O
3.67	O
,	O
0.0500684662211	O
)(	O
4.33	O
,	O
0.0660529562494	O
)(	O
5.0	O
,	O
0.084057538741	O
)(	O
5.67	O
,	O
0.103553464662	O
)(	O
6.33	O
,	O
0.123117178921	O
)(	O
7.0	O
,	O
0.133583698261	O
)(	O
7.67	O
,	O
0.11149825784	O
)(	O
8.33	O
,	O
0.083824279071	O
)(	O
9.0	O
,	O
0.0334982171667	O
)(	O
9.67	O
,	O
0.00229124581407	O
)	O
;	O
[	O
scale=0.80	O
]	O
[	O
xmax=1200	O
,	O
xmin=50	O
,	O
ymax=85	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
No	O
.	O

of	O
epochs	O
,	O
ylabel	O
=	O
Perplexity	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.05	O
,	O
0.5	O
)	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
legend	O
entries	O
=	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
(	O
Train	O
),	O
AWD	Method
-	Method
LSTM	Method
(	O
Train	O
),	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
(	O
Valid	O
),	O
AWD	Method
-	Method
LSTM	Method
(	O
Valid	O
)	O
]	O
[	O
red	O
,	O
thick	O
,	O
dashed	O
]	O
coordinates	O
(	O
10	O
,	O
155.13	O
)(	O
20	O
,	O
107.08	O
)(	O
30	O
,	O
88.67	O
)(	O
40	O
,	O
78.86	O
)(	O
50	O
,	O
72.14	O
)(	O
60	O
,	O
68.24	O
)(	O
70	O
,	O
66.26	O
)(	O
80	O
,	O
62.77	O
)(	O
90	O
,	O
61.43	O
)(	O
100	O
,	O
59.24	O
)(	O
110	O
,	O
57.73	O
)(	O
120	O
,	O
56.89	O
)(	O
130	O
,	O
55.47	O
)(	O
140	O
,	O
54.97	O
)(	O
150	O
,	O
53.56	O
)(	O
160	O
,	O
53.39	O
)(	O
170	O
,	O
51.40	O
)(	O
180	O
,	O
50.83	O
)(	O
190	O
,	O
51.02	O
)(	O
200	O
,	O
50.42	O
)(	O
210	O
,	O
49.81	O
)(	O
220	O
,	O
49.46	O
)(	O
230	O
,	O
49.05	O
)(	O
240	O
,	O
48.19	O
)(	O
250	O
,	O
48.29	O
)(	O
260	O
,	O
48.05	O
)(	O
270	O
,	O
47.82	O
)(	O
280	O
,	O
47.44	O
)(	O
290	O
,	O
47.52	O
)(	O
300	O
,	O
46.70	O
)(	O
310	O
,	O
46.44	O
)(	O
320	O
,	O
46.24	O
)(	O
330	O
,	O
46.49	O
)(	O
340	O
,	O
45.73	O
)(	O
350	O
,	O
45.62	O
)(	O
360	O
,	O
45.44	O
)(	O
370	O
,	O
44.94	O
)(	O
380	O
,	O
45.05	O
)(	O
390	O
,	O
44.54	O
)(	O
400	O
,	O
44.91	O
)(	O
410	O
,	O
44.52	O
)(	O
420	O
,	O
44.00	O
)(	O
430	O
,	O
43.94	O
)(	O
440	O
,	O
43.95	O
)(	O
450	O
,	O
44.13	O
)(	O
460	O
,	O
43.84	O
)(	O
470	O
,	O
43.88	O
)(	O
480	O
,	O
43.63	O
)(	O
490	O
,	O
43.73	O
)(	O
500	O
,	O
43.57	O
)(	O
510	O
,	O
42.99	O
)(	O
520	O
,	O
42.78	O
)(	O
530	O
,	O
42.89	O
)(	O
540	O
,	O
43.10	O
)(	O
550	O
,	O
43.06	O
)(	O
560	O
,	O
42.55	O
)(	O
570	O
,	O
42.55	O
)(	O
580	O
,	O
42.27	O
)(	O
590	O
,	O
41.90	O
)(	O
600	O
,	O
42.71	O
)(	O
610	O
,	O
42.08	O
)(	O
620	O
,	O
42.40	O
)(	O
630	O
,	O
42.15	O
)(	O
640	O
,	O
42.14	O
)(	O
650	O
,	O
42.57	O
)(	O
660	O
,	O
41.67	O
)(	O
670	O
,	O
41.60	O
)(	O
680	O
,	O
41.43	O
)(	O
690	O
,	O
41.55	O
)(	O
700	O
,	O
41.68	O
)(	O
710	O
,	O
41.68	O
)(	O
720	O
,	O
41.44	O
)(	O
730	O
,	O
41.21	O
)(	O
740	O
,	O
41.70	O
)(	O
750	O
,	O
41.69	O
)(	O
760	O
,	O
39.79	O
)(	O
770	O
,	O
41.44	O
)(	O
780	O
,	O
41.96	O
)(	O
790	O
,	O
42.13	O
)(	O
800	O
,	O
41.67	O
)(	O
810	O
,	O
41.85	O
)(	O
820	O
,	O
42.46	O
)(	O
830	O
,	O
41.82	O
)(	O
840	O
,	O
41.82	O
)(	O
850	O
,	O
41.76	O
)(	O
860	O
,	O
41.43	O
)(	O
870	O
,	O
41.65	O
)(	O
880	O
,	O
41.61	O
)(	O
890	O
,	O
41.94	O
)(	O
900	O
,	O
41.35	O
)(	O
910	O
,	O
41.76	O
)(	O
920	O
,	O
41.30	O
)(	O
930	O
,	O
41.34	O
)(	O
940	O
,	O
41.24	O
)(	O
950	O
,	O
41.09	O
)(	O
960	O
,	O
41.11	O
)(	O
970	O
,	O
40.72	O
)(	O
980	O
,	O
41.14	O
)(	O
990	O
,	O
40.53	O
)(	O
1000	O
,	O
41.03	O
)(	O
1010	O
,	O
41.11	O
)(	O
1020	O
,	O
41.06	O
)(	O
1030	O
,	O
40.65	O
)(	O
1040	O
,	O
41.14	O
)(	O
1050	O
,	O
40.73	O
)(	O
1060	O
,	O
40.62	O
)(	O
1070	O
,	O
40.48	O
)(	O
1080	O
,	O
40.83	O
)(	O
1090	O
,	O
41.12	O
)(	O
1100	O
,	O
40.41	O
)(	O
1110	O
,	O
40.65	O
)(	O
1120	O
,	O
40.17	O
)(	O
1130	O
,	O
40.44	O
)(	O
1140	O
,	O
40.28	O
)(	O
1150	O
,	O
40.05	O
)(	O
1160	O
,	O
40.44	O
)(	O
1170	O
,	O
39.93	O
)(	O
1180	O
,	O
39.90	O
)(	O
1190	O
,	O
39.59	O
)(	O
1200	O
,	O
40.16	O
)(	O
1210	O
,	O
40.28	O
)(	O
1220	O
,	O
39.82	O
)(	O
1230	O
,	O
40.00	O
)(	O
1240	O
,	O
40.08	O
)(	O
1250	O
,	O
39.93	O
)(	O
1260	O
,	O
39.62	O
)(	O
1270	O
,	O
39.45	O
)(	O
1280	O
,	O
39.85	O
)(	O
1290	O
,	O
39.82	O
)(	O
1300	O
,	O
40.09	O
)(	O
1310	O
,	O
39.63	O
)(	O
1320	O
,	O
39.64	O
)(	O
1330	O
,	O
39.34	O
)(	O
1340	O
,	O
39.05	O
)(	O
1350	O
,	O
39.43	O
)(	O
1360	O
,	O
39.12	O
)(	O
1370	O
,	O
39.32	O
)(	O
1380	O
,	O
39.37	O
)(	O
1390	O
,	O
39.63	O
)(	O
1400	O
,	O
39.71	O
)	O
;	O
[	O
black!60!green	O
,	O
thick	O
,	O
dashed	O
]	O
coordinates	O
(	O
10	O
,	O
140.66	O
)(	O
20	O
,	O
95.19	O
)(	O
30	O
,	O
79.04	O
)(	O
40	O
,	O
69.99	O
)(	O
50	O
,	O
64.20	O
)(	O
60	O
,	O
60.38	O
)(	O
70	O
,	O
58.07	O
)(	O
80	O
,	O
55.14	O
)(	O
90	O
,	O
53.71	O
)(	O
100	O
,	O
51.55	O
)(	O
110	O
,	O
50.41	O
)(	O
120	O
,	O
49.34	O
)(	O
130	O
,	O
48.23	O
)(	O
140	O
,	O
47.76	O
)(	O
150	O
,	O
46.76	O
)(	O
160	O
,	O
46.68	O
)(	O
170	O
,	O
44.84	O
)(	O
180	O
,	O
44.30	O
)(	O
190	O
,	O
44.08	O
)(	O
200	O
,	O
43.60	O
)(	O
210	O
,	O
43.48	O
)(	O
220	O
,	O
43.03	O
)(	O
230	O
,	O
42.56	O
)(	O
240	O
,	O
41.78	O
)(	O
250	O
,	O
41.93	O
)(	O
260	O
,	O
41.61	O
)(	O
270	O
,	O
41.81	O
)(	O
280	O
,	O
41.40	O
)(	O
290	O
,	O
41.20	O
)(	O
300	O
,	O
40.69	O
)(	O
310	O
,	O
40.43	O
)(	O
320	O
,	O
40.17	O
)(	O
330	O
,	O
40.40	O
)(	O
340	O
,	O
39.86	O
)(	O
350	O
,	O
39.66	O
)(	O
360	O
,	O
39.46	O
)(	O
370	O
,	O
39.03	O
)(	O
380	O
,	O
38.83	O
)(	O
390	O
,	O
38.53	O
)(	O
400	O
,	O
38.91	O
)(	O
410	O
,	O
38.74	O
)(	O
420	O
,	O
38.21	O
)(	O
430	O
,	O
37.93	O
)(	O
440	O
,	O
38.15	O
)(	O
450	O
,	O
38.09	O
)(	O
460	O
,	O
37.83	O
)(	O
470	O
,	O
37.85	O
)(	O
480	O
,	O
37.75	O
)(	O
490	O
,	O
37.72	O
)(	O
500	O
,	O
37.27	O
)(	O
510	O
,	O
37.09	O
)(	O
520	O
,	O
37.19	O
)(	O
530	O
,	O
36.76	O
)(	O
540	O
,	O
37.15	O
)(	O
550	O
,	O
37.02	O
)(	O
560	O
,	O
36.77	O
)(	O
570	O
,	O
36.77	O
)(	O
580	O
,	O
36.53	O
)(	O
590	O
,	O
36.16	O
)(	O
600	O
,	O
36.76	O
)(	O
610	O
,	O
36.43	O
)(	O
620	O
,	O
36.24	O
)(	O
630	O
,	O
36.26	O
)(	O
640	O
,	O
36.12	O
)(	O
650	O
,	O
36.44	O
)(	O
660	O
,	O
35.96	O
)(	O
670	O
,	O
35.79	O
)(	O
680	O
,	O
35.75	O
)(	O
690	O
,	O
35.62	O
)(	O
700	O
,	O
35.79	O
)(	O
710	O
,	O
35.62	O
)(	O
720	O
,	O
35.97	O
)(	O
730	O
,	O
35.30	O
)(	O
740	O
,	O
35.43	O
)(	O
750	O
,	O
35.72	O
)(	O
760	O
,	O
36.88	O
)(	O
770	O
,	O
36.87	O
)(	O
780	O
,	O
37.12	O
)(	O
790	O
,	O
36.91	O
)(	O
800	O
,	O
36.36	O
)(	O
810	O
,	O
36.07	O
)(	O
820	O
,	O
36.33	O
)(	O
830	O
,	O
35.65	O
)(	O
840	O
,	O
35.58	O
)(	O
850	O
,	O
35.48	O
)(	O
860	O
,	O
35.18	O
)(	O
870	O
,	O
35.29	O
)(	O
880	O
,	O
34.86	O
)(	O
890	O
,	O
35.3	O
)(	O
900	O
,	O
34.87	O
)(	O
910	O
,	O
35.37	O
)(	O
920	O
,	O
34.76	O
)(	O
930	O
,	O
34.68	O
)(	O
940	O
,	O
34.69	O
)(	O
950	O
,	O
34.31	O
)(	O
960	O
,	O
34.56	O
)(	O
970	O
,	O
34.15	O
)(	O
980	O
,	O
34.4	O
)(	O
990	O
,	O
33.95	O
)(	O
1000	O
,	O
34.02	O
)(	O
1010	O
,	O
34.12	O
)(	O
1020	O
,	O
34.24	O
)(	O
1030	O
,	O
33.9	O
)(	O
1040	O
,	O
34.11	O
)(	O
1050	O
,	O
33.79	O
)(	O
1060	O
,	O
33.91	O
)(	O
1070	O
,	O
33.84	O
)(	O
1080	O
,	O
34.06	O
)(	O
1090	O
,	O
34.3	O
)(	O
1100	O
,	O
33.75	O
)(	O
1110	O
,	O
33.54	O
)(	O
1120	O
,	O
33.57	O
)(	O
1130	O
,	O
33.55	O
)(	O
1140	O
,	O
33.42	O
)(	O
1150	O
,	O
33.17	O
)(	O
1160	O
,	O
33.78	O
)(	O
1170	O
,	O
33.1	O
)(	O
1180	O
,	O
33.01	O
)(	O
1190	O
,	O
32.9	O
)(	O
1200	O
,	O
33.38	O
)(	O
1210	O
,	O
33.12	O
)(	O
1220	O
,	O
32.83	O
)(	O
1230	O
,	O
32.94	O
)(	O
1240	O
,	O
32.95	O
)(	O
1250	O
,	O
33.04	O
)(	O
1260	O
,	O
32.57	O
)(	O
1270	O
,	O
32.57	O
)(	O
1280	O
,	O
32.63	O
)(	O
1290	O
,	O
32.93	O
)(	O
1300	O
,	O
32.83	O
)(	O
1310	O
,	O
32.68	O
)(	O
1320	O
,	O
32.54	O
)(	O
1330	O
,	O
32.72	O
)(	O
1340	O
,	O
32.24	O
)(	O
1350	O
,	O
32.65	O
)(	O
1360	O
,	O
32.42	O
)(	O
1370	O
,	O
32.24	O
)(	O
1380	O
,	O
32.53	O
)(	O
1390	O
,	O
32.42	O
)(	O
1400	O
,	O
32.45	O
)	O
;	O
[	O
red	O
,	O
thick	O
]	O
coordinates	O
(	O
10	O
,	O
121.40	O
)(	O
20	O
,	O
89.75	O
)(	O
30	O
,	O
81.34	O
)(	O
40	O
,	O
75.91	O
)(	O
50	O
,	O
73.53	O
)(	O
60	O
,	O
68.18	O
)(	O
70	O
,	O
66.82	O
)(	O
80	O
,	O
66.04	O
)(	O
90	O
,	O
65.49	O
)(	O
100	O
,	O
65.03	O
)(	O
110	O
,	O
64.64	O
)(	O
120	O
,	O
64.29	O
)(	O
130	O
,	O
63.97	O
)(	O
140	O
,	O
63.70	O
)(	O
150	O
,	O
63.46	O
)(	O
160	O
,	O
63.25	O
)(	O
170	O
,	O
63.04	O
)(	O
180	O
,	O
62.85	O
)(	O
190	O
,	O
62.69	O
)(	O
200	O
,	O
62.54	O
)(	O
210	O
,	O
62.40	O
)(	O
220	O
,	O
62.27	O
)(	O
230	O
,	O
62.15	O
)(	O
240	O
,	O
62.04	O
)(	O
250	O
,	O
61.93	O
)(	O
260	O
,	O
61.84	O
)(	O
270	O
,	O
61.76	O
)(	O
280	O
,	O
61.68	O
)(	O
290	O
,	O
61.60	O
)(	O
300	O
,	O
61.53	O
)(	O
310	O
,	O
61.46	O
)(	O
320	O
,	O
61.40	O
)(	O
330	O
,	O
61.35	O
)(	O
340	O
,	O
61.30	O
)(	O
350	O
,	O
61.25	O
)(	O
360	O
,	O
61.20	O
)(	O
370	O
,	O
61.15	O
)(	O
380	O
,	O
61.11	O
)(	O
390	O
,	O
61.06	O
)(	O
400	O
,	O
61.03	O
)(	O
410	O
,	O
60.99	O
)(	O
420	O
,	O
60.95	O
)(	O
430	O
,	O
60.92	O
)(	O
440	O
,	O
60.88	O
)(	O
450	O
,	O
60.85	O
)(	O
460	O
,	O
60.82	O
)(	O
470	O
,	O
60.80	O
)(	O
480	O
,	O
60.77	O
)(	O
490	O
,	O
60.75	O
)(	O
500	O
,	O
60.73	O
)(	O
510	O
,	O
60.71	O
)(	O
520	O
,	O
60.68	O
)(	O
530	O
,	O
60.66	O
)(	O
540	O
,	O
60.65	O
)(	O
550	O
,	O
60.63	O
)(	O
560	O
,	O
60.61	O
)(	O
570	O
,	O
60.59	O
)(	O
580	O
,	O
60.58	O
)(	O
590	O
,	O
60.56	O
)(	O
600	O
,	O
60.54	O
)(	O
610	O
,	O
60.53	O
)(	O
620	O
,	O
60.51	O
)(	O
630	O
,	O
60.50	O
)(	O
640	O
,	O
60.49	O
)(	O
650	O
,	O
60.48	O
)(	O
660	O
,	O
60.47	O
)(	O
670	O
,	O
60.46	O
)(	O
680	O
,	O
60.44	O
)(	O
690	O
,	O
60.43	O
)(	O
700	O
,	O
60.42	O
)(	O
710	O
,	O
60.41	O
)(	O
720	O
,	O
60.40	O
)(	O
730	O
,	O
60.40	O
)(	O
740	O
,	O
60.39	O
)(	O
750	O
,	O
60.39	O
)(	O
760	O
,	O
59.97	O
)(	O
770	O
,	O
59.78	O
)(	O
780	O
,	O
59.64	O
)(	O
790	O
,	O
59.51	O
)(	O
800	O
,	O
59.41	O
)(	O
810	O
,	O
59.32	O
)(	O
820	O
,	O
59.25	O
)(	O
830	O
,	O
59.18	O
)(	O
840	O
,	O
59.13	O
)(	O
850	O
,	O
59.08	O
)(	O
860	O
,	O
59.03	O
)(	O
870	O
,	O
59.00	O
)(	O
880	O
,	O
58.96	O
)(	O
890	O
,	O
58.93	O
)(	O
900	O
,	O
58.90	O
)(	O
910	O
,	O
58.88	O
)(	O
920	O
,	O
58.85	O
)(	O
930	O
,	O
58.83	O
)(	O
940	O
,	O
58.80	O
)(	O
950	O
,	O
58.78	O
)(	O
960	O
,	O
58.75	O
)(	O
970	O
,	O
58.73	O
)(	O
980	O
,	O
58.71	O
)(	O
990	O
,	O
58.69	O
)(	O
1000	O
,	O
58.67	O
)(	O
1010	O
,	O
58.65	O
)(	O
1020	O
,	O
58.63	O
)(	O
1030	O
,	O
58.61	O
)(	O
1040	O
,	O
58.59	O
)(	O
1050	O
,	O
58.57	O
)(	O
1060	O
,	O
58.56	O
)(	O
1070	O
,	O
58.54	O
)(	O
1080	O
,	O
58.52	O
)(	O
1090	O
,	O
58.50	O
)(	O
1100	O
,	O
58.48	O
)(	O
1110	O
,	O
58.47	O
)(	O
1120	O
,	O
58.45	O
)(	O
1130	O
,	O
58.44	O
)(	O
1140	O
,	O
58.42	O
)(	O
1150	O
,	O
58.41	O
)(	O
1160	O
,	O
58.39	O
)(	O
1170	O
,	O
58.38	O
)(	O
1180	O
,	O
58.37	O
)(	O
1190	O
,	O
58.35	O
)(	O
1200	O
,	O
58.34	O
)(	O
1210	O
,	O
58.33	O
)(	O
1220	O
,	O
58.31	O
)(	O
1230	O
,	O
58.30	O
)(	O
1240	O
,	O
58.29	O
)(	O
1250	O
,	O
58.28	O
)(	O
1260	O
,	O
58.27	O
)(	O
1270	O
,	O
58.26	O
)(	O
1280	O
,	O
58.25	O
)(	O
1290	O
,	O
58.24	O
)(	O
1300	O
,	O
58.23	O
)(	O
1310	O
,	O
58.22	O
)(	O
1320	O
,	O
58.21	O
)(	O
1330	O
,	O
58.21	O
)(	O
1340	O
,	O
58.20	O
)(	O
1350	O
,	O
58.19	O
)(	O
1360	O
,	O
58.18	O
)(	O
1370	O
,	O
58.18	O
)(	O
1380	O
,	O
58.17	O
)(	O
1390	O
,	O
58.16	O
)(	O
1400	O
,	O
58.15	O
)	O
;	O
[	O
black!60!green	O
,	O
thick	O
]	O
coordinates	O
(	O
10	O
,	O
115.36	O
)(	O
20	O
,	O
87.02	O
)(	O
30	O
,	O
79.04	O
)(	O
40	O
,	O
74.46	O
)(	O
50	O
,	O
72.82	O
)(	O
60	O
,	O
67.61	O
)(	O
70	O
,	O
66.66	O
)(	O
80	O
,	O
65.95	O
)(	O
90	O
,	O
65.42	O
)(	O
100	O
,	O
64.98	O
)(	O
110	O
,	O
64.60	O
)(	O
120	O
,	O
64.27	O
)(	O
130	O
,	O
63.96	O
)(	O
140	O
,	O
63.70	O
)(	O
150	O
,	O
63.48	O
)(	O
160	O
,	O
63.29	O
)(	O
170	O
,	O
63.12	O
)(	O
180	O
,	O
62.96	O
)(	O
190	O
,	O
62.81	O
)(	O
200	O
,	O
62.68	O
)(	O
210	O
,	O
62.56	O
)(	O
220	O
,	O
62.45	O
)(	O
230	O
,	O
62.35	O
)(	O
240	O
,	O
62.26	O
)(	O
250	O
,	O
62.17	O
)(	O
260	O
,	O
62.09	O
)(	O
270	O
,	O
62.02	O
)(	O
280	O
,	O
61.95	O
)(	O
290	O
,	O
61.89	O
)(	O
300	O
,	O
61.83	O
)(	O
310	O
,	O
61.77	O
)(	O
320	O
,	O
61.72	O
)(	O
330	O
,	O
61.66	O
)(	O
340	O
,	O
61.62	O
)(	O
350	O
,	O
61.57	O
)(	O
360	O
,	O
61.53	O
)(	O
370	O
,	O
61.48	O
)(	O
380	O
,	O
61.45	O
)(	O
390	O
,	O
61.41	O
)(	O
400	O
,	O
61.38	O
)(	O
410	O
,	O
61.35	O
)(	O
420	O
,	O
61.32	O
)(	O
430	O
,	O
61.29	O
)(	O
440	O
,	O
61.26	O
)(	O
450	O
,	O
61.23	O
)(	O
460	O
,	O
61.21	O
)(	O
470	O
,	O
61.18	O
)(	O
480	O
,	O
61.16	O
)(	O
490	O
,	O
61.14	O
)(	O
500	O
,	O
61.13	O
)(	O
510	O
,	O
61.11	O
)(	O
520	O
,	O
61.09	O
)(	O
530	O
,	O
61.07	O
)(	O
540	O
,	O
61.06	O
)(	O
550	O
,	O
61.04	O
)(	O
560	O
,	O
61.02	O
)(	O
570	O
,	O
61.01	O
)(	O
580	O
,	O
61.00	O
)(	O
590	O
,	O
60.98	O
)(	O
600	O
,	O
60.97	O
)(	O
610	O
,	O
60.96	O
)(	O
620	O
,	O
60.94	O
)(	O
630	O
,	O
60.93	O
)(	O
640	O
,	O
60.92	O
)(	O
650	O
,	O
60.91	O
)(	O
660	O
,	O
60.90	O
)(	O
670	O
,	O
60.89	O
)(	O
680	O
,	O
60.88	O
)(	O
690	O
,	O
60.87	O
)(	O
700	O
,	O
60.86	O
)(	O
710	O
,	O
60.85	O
)(	O
720	O
,	O
60.84	O
)(	O
730	O
,	O
60.83	O
)(	O
740	O
,	O
60.83	O
)(	O
750	O
,	O
60.82	O
)(	O
760	O
,	O
60.37	O
)(	O
770	O
,	O
60.30	O
)(	O
780	O
,	O
60.25	O
)(	O
790	O
,	O
60.18	O
)(	O
800	O
,	O
60.13	O
)(	O
810	O
,	O
60.07	O
)(	O
820	O
,	O
60.01	O
)(	O
830	O
,	O
59.97	O
)(	O
840	O
,	O
59.93	O
)(	O
850	O
,	O
59.91	O
)(	O
860	O
,	O
59.89	O
)(	O
870	O
,	O
59.86	O
)(	O
880	O
,	O
59.83	O
)(	O
890	O
,	O
59.81	O
)(	O
900	O
,	O
59.78	O
)(	O
910	O
,	O
59.76	O
)(	O
920	O
,	O
59.75	O
)(	O
930	O
,	O
59.73	O
)(	O
940	O
,	O
59.72	O
)(	O
950	O
,	O
59.70	O
)(	O
960	O
,	O
59.69	O
)(	O
970	O
,	O
59.68	O
)(	O
980	O
,	O
59.67	O
)(	O
990	O
,	O
59.66	O
)(	O
1000	O
,	O
59.65	O
)(	O
1010	O
,	O
59.63	O
)(	O
1020	O
,	O
59.62	O
)(	O
1030	O
,	O
59.61	O
)(	O
1040	O
,	O
59.60	O
)(	O
1050	O
,	O
59.58	O
)(	O
1060	O
,	O
59.57	O
)(	O
1070	O
,	O
59.56	O
)(	O
1080	O
,	O
59.55	O
)(	O
1090	O
,	O
59.54	O
)(	O
1100	O
,	O
59.52	O
)(	O
1110	O
,	O
59.51	O
)(	O
1120	O
,	O
59.49	O
)(	O
1130	O
,	O
59.48	O
)(	O
1140	O
,	O
59.47	O
)(	O
1150	O
,	O
59.46	O
)(	O
1160	O
,	O
59.45	O
)(	O
1170	O
,	O
59.44	O
)(	O
1180	O
,	O
59.43	O
)(	O
1190	O
,	O
59.43	O
)(	O
1200	O
,	O
59.42	O
)(	O
1210	O
,	O
59.41	O
)(	O
1220	O
,	O
59.40	O
)(	O
1230	O
,	O
59.39	O
)(	O
1240	O
,	O
59.39	O
)(	O
1250	O
,	O
59.38	O
)(	O
1260	O
,	O
59.37	O
)(	O
1270	O
,	O
59.37	O
)(	O
1280	O
,	O
59.36	O
)(	O
1290	O
,	O
59.36	O
)(	O
1300	O
,	O
59.35	O
)(	O
1310	O
,	O
59.35	O
)(	O
1320	O
,	O
59.34	O
)(	O
1330	O
,	O
59.34	O
)(	O
1340	O
,	O
59.34	O
)(	O
1350	O
,	O
59.33	O
)(	O
1360	O
,	O
59.33	O
)(	O
1370	O
,	O
59.33	O
)(	O
1380	O
,	O
59.32	O
)(	O
1390	O
,	O
59.32	O
)(	O
1400	O
,	O
59.31	O
)	O
;	O
To	O
show	O
the	O
qualitative	O
difference	O
between	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
and	O
AWD	Method
-	Method
LSTM	Method
,	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
,	O
we	O
plot	O
a	O
histogram	O
of	O
the	O
entropy	Metric
of	O
the	O
predicted	O
next	O
token	O
distribution	O
for	O
all	O
the	O
tokens	O
in	O
the	O
validation	O
set	O
of	O
PTB	Material
achieved	O
by	O
their	O
respective	O
best	O
models	O
.	O

The	O
distributions	O
for	O
the	O
two	O
models	O
is	O
slightly	O
different	O
,	O
with	O
some	O
identifiable	O
patterns	O
.	O

The	O
use	O
of	O
PDR	Method
has	O
the	O
effect	O
of	O
reducing	O
the	O
entropy	Metric
of	O
the	O
predicted	O
distribution	O
when	O
it	O
is	O
in	O
the	O
higher	O
range	O
of	O
8	O
and	O
above	O
,	O
pushing	O
it	O
into	O
the	O
range	O
of	O
5	O
-	O
8	O
.	O

This	O
shows	O
that	O
one	O
way	O
PDR	Method
biases	O
the	O
language	Method
model	Method
is	O
by	O
reducing	O
the	O
entropy	O
of	O
the	O
predicted	O
next	O
token	O
distribution	O
.	O

Indeed	O
,	O
one	O
way	O
to	O
reduce	O
the	O
cross	Metric
-	Metric
entropy	Metric
between	O
and	O
is	O
by	O
making	O
less	O
spread	O
out	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

This	O
tends	O
to	O
benefits	O
the	O
language	Method
model	Method
when	O
the	O
predictions	O
are	O
correct	O
.	O

We	O
also	O
compare	O
the	O
training	Material
curves	O
for	O
the	O
two	O
models	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
on	O
PTB	Material
.	O

Although	O
the	O
two	O
models	O
use	O
slightly	O
different	O
hyperparameters	O
,	O
the	O
regularization	O
effect	O
of	O
PDR	Method
is	O
apparent	O
with	O
a	O
lower	O
validation	Metric
perplexity	Metric
but	O
higher	O
training	Material
perplexity	O
.	O

The	O
corresponding	O
trends	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
,	O
b	O
)	O
for	O
WT2	Material
have	O
similar	O
characteristics	O
.	O

subsection	O
:	O
Ablation	Task
Studies	Task
We	O
perform	O
a	O
set	O
of	O
ablation	Task
experiments	O
on	O
the	O
best	O
AWD	Method
-	Method
LSTM	Method
+	O
PDR	Method
models	O
for	O
PTB	Material
and	O
WT2	Material
to	O
understand	O
the	O
relative	O
contribution	O
of	O
PDR	Method
and	O
the	O
other	O
regularizations	Method
used	O
in	O
the	O
model	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
both	O
cases	O
,	O
PDR	Method
has	O
a	O
significant	O
effect	O
in	O
decreasing	O
the	O
validation	Metric
set	Metric
performance	Metric
,	O
albeit	O
lesser	O
than	O
the	O
other	O
forms	O
of	O
regularization	Method
.	O

This	O
is	O
not	O
surprising	O
as	O
PDR	Method
does	O
not	O
influence	O
the	O
LSTM	Method
directly	O
.	O

section	O
:	O
Related	O
Work	O
Our	O
method	O
builds	O
on	O
the	O
work	O
of	O
using	O
sophisticated	O
regularization	Method
techniques	Method
to	O
train	O
LSTMs	Method
for	O
language	Task
modeling	Task
.	O

In	O
particular	O
,	O
the	O
AWD	Method
-	Method
LSTM	Method
model	Method
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
a	O
single	O
softmax	Method
on	O
the	O
four	O
datasets	O
considered	O
in	O
this	O
paper	O
(	O
)	O
.	O

also	O
achieve	O
similar	O
results	O
with	O
highly	O
regularized	O
LSTMs	Method
.	O

By	O
addressing	O
the	O
so	O
-	O
called	O
softmax	Method
bottleneck	Method
in	O
single	O
softmax	Method
models	Method
,	O
use	O
a	O
mixture	Method
-	Method
of	Method
-	Method
softmaxes	Method
to	O
achieve	O
significantly	O
lower	O
perplexities	Metric
.	O

PDR	Method
utilizes	O
the	O
symmetry	O
between	O
the	O
inputs	O
and	O
outputs	O
of	O
a	O
language	Method
model	Method
,	O
a	O
fact	O
that	O
is	O
also	O
exploited	O
in	O
weight	Method
tying	Method
(	O
)	O
.	O

Our	O
method	O
can	O
be	O
used	O
with	O
untied	O
weights	O
as	O
well	O
.	O

Although	O
motivated	O
by	O
language	Method
modeling	Method
,	O
PDR	Method
can	O
also	O
be	O
applied	O
to	O
seq2seq	Method
models	Method
with	O
shared	O
input	O
-	O
output	O
vocabularies	O
,	O
such	O
as	O
those	O
used	O
for	O
text	Task
summarization	Task
and	O
neural	Task
machine	Task
translation	Task
(	O
with	O
byte	O
pair	O
encoding	O
of	O
words	O
)	O
(	O
)	O
.	O

Regularizing	O
the	O
training	Material
of	O
an	O
LSTM	Method
by	O
combining	O
the	O
main	O
objective	O
function	O
with	O
auxiliary	Task
tasks	Task
has	O
been	O
successfully	O
applied	O
to	O
several	O
tasks	O
in	O
NLP	Task
(	Task
)	O
.	O

In	O
fact	O
,	O
a	O
popular	O
choice	O
for	O
the	O
auxiliary	Task
task	Task
is	O
language	Method
modeling	Method
itself	O
.	O

This	O
in	O
turn	O
is	O
related	O
to	O
multi	Task
-	Task
task	Task
learning	Task
(	O
)	O
.	O

Specialized	Method
architectures	Method
like	O
Recurrent	Method
Highway	Method
Networks	Method
(	O
)	O
and	O
NAS	Method
(	O
)	O
have	O
been	O
successfully	O
used	O
to	O
achieve	O
competitive	O
performance	O
in	O
language	Task
modeling	Task
.	O

The	O
former	O
one	O
makes	O
the	O
hidden	O
-	O
to	O
-	O
hidden	O
transition	O
function	O
more	O
complex	O
allowing	O
for	O
more	O
refined	O
information	O
flow	O
.	O

Such	O
architectures	O
are	O
especially	O
important	O
for	O
character	Task
level	Task
language	Task
modeling	Task
where	O
strong	O
results	O
have	O
been	O
shown	O
using	O
Fast	Method
-	Method
Slow	Method
RNNs	Method
(	O
)	O
,	O
a	O
two	Method
level	Method
architecture	Method
where	O
the	O
slowly	Method
changing	Method
recurrent	Method
network	Method
tries	O
to	O
capture	O
more	O
long	O
range	O
dependencies	O
.	O

The	O
use	O
of	O
historical	O
information	O
can	O
greatly	O
help	O
language	Method
models	Method
deal	O
with	O
long	O
range	O
dependencies	O
as	O
shown	O
by	O
.	O

Finally	O
,	O
in	O
a	O
recent	O
paper	O
,	O
achieve	O
improved	O
performance	O
for	O
language	Task
modeling	Task
by	O
using	O
frequency	Method
agnostic	Method
word	Method
embeddings	Method
,	O
a	O
technique	O
orthogonal	O
to	O
and	O
combinable	O
with	O
PDR	Method
.	O

bibliography	O
:	O
References	O
document	O
:	O
DialogueRNN	Method
:	O
An	O
Attentive	O
RNN	Method
for	O
Emotion	Task
Detection	Task
in	O
Conversations	Task
Emotion	Task
detection	Task
in	O
conversations	O
is	O
a	O
necessary	O
step	O
for	O
a	O
number	O
of	O
applications	O
,	O
including	O
opinion	Task
mining	Task
over	O
chat	O
history	O
,	O
social	O
media	O
threads	O
,	O
debates	O
,	O
argumentation	Task
mining	Task
,	O
understanding	Task
consumer	Task
feedback	Task
in	O
live	O
conversations	O
,	O
etc	O
.	O

Currently	O
systems	O
do	O
not	O
treat	O
the	O
parties	O
in	O
the	O
conversation	O
individually	O
by	O
adapting	O
to	O
the	O
speaker	O
of	O
each	O
utterance	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
new	O
method	O
based	O
on	O
recurrent	Method
neural	Method
networks	Method
that	O
keeps	O
track	O
of	O
the	O
individual	O
party	O
states	O
throughout	O
the	O
conversation	O
and	O
uses	O
this	O
information	O
for	O
emotion	Task
classification	Task
.	O

Our	O
model	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
a	O
significant	O
margin	O
on	O
two	O
different	O
datasets	O
.	O

section	O
:	O
Introduction	O
Emotion	Task
detection	Task
in	O
conversation	O
attracts	O
increasing	O
attention	O
of	O
the	O
community	O
due	O
to	O
its	O
applications	O
in	O
many	O
important	O
tasks	O
such	O
as	O
opinion	Task
mining	Task
over	O
chat	O
history	O
and	O
social	O
media	O
threads	O
in	O
YouTube	O
,	O
Facebook	O
,	O
Twitter	O
,	O
etc	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
method	O
based	O
on	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
that	O
can	O
cater	O
to	O
these	O
needs	O
by	O
processing	O
the	O
huge	O
amount	O
of	O
available	O
conversational	O
data	O
.	O

Current	O
systems	O
,	O
including	O
the	O
state	O
of	O
the	O
art	O
,	O
do	O
not	O
distinguish	O
different	O
parties	O
in	O
a	O
conversation	O
in	O
a	O
meaningful	O
way	O
.	O

They	O
are	O
not	O
aware	O
of	O
the	O
speaker	O
of	O
a	O
given	O
utterance	O
.	O

In	O
contrast	O
,	O
we	O
model	O
individual	O
party	O
with	O
party	O
states	O
,	O
as	O
the	O
conversation	O
flows	O
,	O
basing	O
on	O
the	O
utterance	O
,	O
the	O
context	O
,	O
and	O
current	O
party	O
state	O
.	O

Our	O
model	O
is	O
based	O
on	O
the	O
assumption	O
that	O
there	O
are	O
three	O
major	O
aspects	O
relevant	O
to	O
the	O
emotion	O
in	O
a	O
conversation	O
:	O
the	O
speaker	O
,	O
the	O
context	O
from	O
the	O
preceding	O
utterances	O
,	O
and	O
the	O
emotion	O
of	O
the	O
preceding	O
utterances	O
.	O

These	O
three	O
aspects	O
are	O
not	O
necessarily	O
independent	O
,	O
but	O
their	O
separate	O
modeling	O
significantly	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
(	O
tab	O
:	O
results	O
-	O
text	O
)	O
.	O

In	O
dyadic	O
conversations	O
,	O
the	O
parties	O
have	O
distinct	O
roles	O
.	O

Hence	O
,	O
to	O
extract	O
the	O
context	O
,	O
it	O
is	O
crucial	O
to	O
consider	O
the	O
preceding	O
turns	O
of	O
both	O
speaker	O
and	O
listener	O
at	O
a	O
given	O
moment	O
(	O
fig	O
:	O
example	O
)	O
.	O

Our	O
DialogueRNN	Method
employs	O
three	O
gated	Method
recurrent	Method
units	Method
(	O
GRU	Method
)	O
to	O
model	O
these	O
aspects	O
.	O

The	O
incoming	O
utterance	O
is	O
fed	O
into	O
two	O
GRUs	Method
called	O
global	Method
GRU	Method
and	O
party	Method
GRU	Method
to	O
update	O
the	O
context	O
and	O
party	O
state	O
,	O
respectively	O
.	O

The	O
global	Method
GRU	Method
encodes	O
corresponding	O
party	O
information	O
while	O
encoding	O
an	O
utterance	O
.	O

Attending	O
over	O
this	O
GRU	Method
gives	O
contextual	Method
representation	Method
that	O
has	O
information	O
of	O
all	O
preceding	O
utterances	O
by	O
different	O
parties	O
in	O
the	O
conversation	O
.	O

The	O
speaker	O
state	O
depends	O
on	O
this	O
context	O
through	O
attention	O
and	O
the	O
speaker	O
’s	O
previous	O
state	O
.	O

This	O
ensures	O
that	O
at	O
time	O
,	O
the	O
speaker	O
state	O
directly	O
gets	O
information	O
from	O
the	O
speaker	O
’s	O
previous	O
state	O
and	O
global	O
GRU	Method
which	O
has	O
information	O
on	O
the	O
preceding	O
parties	O
.	O

Finally	O
,	O
the	O
updated	O
speaker	O
state	O
is	O
fed	O
into	O
the	O
emotion	O
GRU	Method
to	O
decode	O
the	O
emotion	Method
representation	Method
of	O
the	O
given	O
utterance	O
,	O
which	O
is	O
used	O
for	O
emotion	Task
classification	Task
.	O

At	O
time	O
,	O
emotion	Method
GRU	Method
cell	Method
gets	O
the	O
emotion	O
representation	O
of	O
and	O
speaker	O
state	O
of	O
.	O

The	O
emotion	O
GRU	Method
,	O
along	O
with	O
the	O
global	Method
GRU	Method
,	O
plays	O
a	O
pivotal	O
role	O
in	O
inter	Task
-	Task
party	Task
relation	Task
modeling	Task
.	O

On	O
the	O
other	O
hand	O
,	O
party	Method
GRU	Method
models	O
relation	O
between	O
two	O
sequential	O
states	O
of	O
the	O
same	O
party	O
.	O

In	O
DialogueRNN	Method
,	O
all	O
these	O
three	O
different	O
types	O
of	O
GRUs	Method
are	O
connected	O
in	O
a	O
recurrent	O
manner	O
.	O

We	O
believe	O
that	O
DialogueRNN	Method
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
contextual	Method
emotion	Method
classifiers	Method
such	O
as	O
because	O
of	O
better	O
context	Method
representation	Method
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
sec	O
:	O
related	O
-	O
works	O
discusses	O
related	O
work	O
;	O
sec	O
:	O
method	O
provides	O
detailed	O
description	O
of	O
our	O
model	O
;	O
sec	O
:	O
experiments	O
,	O
sec	O
:	O
results	O
-	O
discussion	O
present	O
the	O
experimental	O
results	O
;	O
finally	O
,	O
sec	O
:	O
conclusion	O
concludes	O
the	O
paper	O
.	O

section	O
:	O
Related	O
Work	O
Emotion	Task
recognition	Task
has	O
attracted	O
attention	O
in	O
various	O
fields	O
such	O
as	O
natural	Task
language	Task
processing	Task
,	O
psychology	Task
,	O
cognitive	Task
science	Task
,	O
and	O
so	O
on	O
.	O

ekman1993facial	O
ekman1993facial	O
found	O
correlation	O
between	O
emotion	O
and	O
facial	O
cues	O
.	O

datcu2008semantic	O
datcu2008semantic	O
fused	O
acoustic	O
information	O
with	O
visual	O
cues	O
for	O
emotion	Task
recognition	Task
.	O

alm2005emotions	O
alm2005emotions	O
introduced	O
text	Task
-	Task
based	Task
emotion	Task
recognition	Task
,	O
developed	O
in	O
the	O
work	O
of	O
strapparava2010annotating	O
strapparava2010annotating	O
.	O

wollmer2010context	O
wollmer2010context	O
used	O
contextual	O
information	O
for	O
emotion	Task
recognition	Task
in	O
multimodal	Task
setting	Task
.	O

Recently	O
,	O
poria	O
-	O
EtAl:2017:Long	O
poria	O
-	O
EtAl:2017:Long	O
successfully	O
used	O
RNN	Method
-	O
based	O
deep	O
networks	O
for	O
multimodal	Task
emotion	Task
recognition	Task
,	O
which	O
was	O
followed	O
by	O
other	O
works	O
.	O

Reproducing	Task
human	Task
interaction	Task
requires	O
deep	Task
understanding	Task
of	Task
conversation	Task
.	O

ruusuvuori2013emotion	O
ruusuvuori2013emotion	O
states	O
that	O
emotion	O
plays	O
a	O
pivotal	O
role	O
in	O
conversations	O
.	O

It	O
has	O
been	O
argued	O
that	O
emotional	O
dynamics	O
in	O
a	O
conversation	O
is	O
an	O
inter	O
-	O
personal	O
phenomenon	O
.	O

Hence	O
,	O
our	O
model	O
incorporates	O
inter	O
-	O
personal	O
interactions	O
in	O
an	O
effective	O
way	O
.	O

Further	O
,	O
since	O
conversations	O
have	O
a	O
natural	O
temporal	O
nature	O
,	O
we	O
adopt	O
the	O
temporal	O
nature	O
through	O
recurrent	Method
network	Method
.	O

Memory	Method
networks	Method
has	O
been	O
successful	O
in	O
several	O
NLP	Task
areas	Task
,	O
including	O
question	Task
answering	Task
,	O
machine	Task
translation	Task
,	O
speech	Task
recognition	Task
,	O
and	O
so	O
on	O
.	O

Thus	O
,	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
used	O
memory	Method
networks	Method
for	O
emotion	Task
recognition	Task
in	O
dyadic	O
conversations	O
,	O
where	O
two	O
distinct	O
memory	Method
networks	Method
enabled	O
inter	Task
-	Task
speaker	Task
interaction	Task
,	O
yielding	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

section	O
:	O
Methodology	O
subsection	O
:	O
Problem	O
Definition	O
Let	O
there	O
be	O
parties	O
/	O
participants	O
(	O
for	O
the	O
datasets	O
we	O
used	O
)	O
in	O
a	O
conversation	O
.	O

The	O
task	O
is	O
to	O
predict	O
the	O
emotion	O
labels	O
(	O
happy	O
,	O
sad	O
,	O
neutral	O
,	O
angry	O
,	O
excited	O
,	O
and	O
frustrated	O
)	O
of	O
the	O
constituent	O
utterances	O
,	O
where	O
utterance	O
is	O
uttered	O
by	O
party	O
,	O
while	O
being	O
the	O
mapping	O
between	O
utterance	O
and	O
index	O
of	O
its	O
corresponding	O
party	O
.	O

Also	O
,	O
is	O
the	O
utterance	Method
representation	Method
,	O
obtained	O
using	O
feature	Method
extractors	Method
described	O
below	O
.	O

subsection	O
:	O
Unimodal	Task
Feature	Task
Extraction	Task
For	O
a	O
fair	O
comparison	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
,	O
conversational	Method
memory	Method
networks	Method
(	O
CMN	Method
)	O
,	O
we	O
follow	O
identical	O
feature	Method
extraction	Method
procedures	Method
.	O

subsubsection	O
:	O
Textual	Method
Feature	Method
Extraction	Method
We	O
employ	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
for	O
textual	Task
feature	O
extraction	O
.	O

Following	O
kim2014convolutional	O
kim2014convolutional	O
,	O
we	O
obtain	O
n	O
-	O
gram	O
features	O
from	O
each	O
utterance	O
using	O
three	O
distinct	O
convolution	Method
filters	Method
of	O
sizes	O
3	O
,	O
4	O
,	O
and	O
5	O
respectively	O
,	O
each	O
having	O
50	O
feature	O
-	O
maps	O
.	O

Outputs	O
are	O
then	O
subjected	O
to	O
max	Method
-	Method
pooling	Method
followed	O
by	O
rectified	Method
linear	Method
unit	Method
(	O
ReLU	Method
)	O
activation	O
.	O

These	O
activations	O
are	O
concatenated	O
and	O
fed	O
to	O
a	O
dimensional	Method
dense	Method
layer	Method
,	O
which	O
is	O
regarded	O
as	O
the	O
textual	Task
utterance	O
representation	O
.	O

This	O
network	O
is	O
trained	O
at	O
utterance	O
level	O
with	O
the	O
emotion	O
labels	O
.	O

subsubsection	O
:	O
Audio	Task
and	Task
Visual	Task
Feature	Task
Extraction	Task
Identical	O
to	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
we	O
use	O
3D	Method
-	Method
CNN	Method
and	O
openSMILE	Method
for	O
visual	Task
and	Task
acoustic	Task
feature	Task
extraction	Task
,	O
respectively	O
.	O

subsection	O
:	O
Our	O
Model	O
We	O
assume	O
that	O
the	O
emotion	O
of	O
an	O
utterance	O
in	O
a	O
conversation	O
depends	O
on	O
three	O
major	O
factors	O
:	O
the	O
speaker	O
.	O

the	O
context	O
given	O
by	O
the	O
preceding	O
utterances	O
.	O

the	O
emotion	O
behind	O
the	O
preceding	O
utterances	O
.	O

Our	O
model	O
DialogueRNN	Method
,	O
shown	O
in	O
fig	O
:	O
architecture	O
,	O
models	O
these	O
three	O
factors	O
as	O
follows	O
:	O
each	O
party	O
is	O
modeled	O
using	O
a	O
party	O
state	O
which	O
changes	O
as	O
and	O
when	O
that	O
party	O
utters	O
an	O
utterance	O
.	O

This	O
enables	O
the	O
model	O
to	O
track	O
the	O
parties	O
’	O
emotion	O
dynamics	O
through	O
the	O
conversations	O
,	O
which	O
is	O
related	O
to	O
the	O
emotion	O
behind	O
the	O
utterances	O
.	O

Furthermore	O
,	O
the	O
context	O
of	O
an	O
utterance	O
is	O
modeled	O
using	O
a	O
global	O
state	O
(	O
called	O
global	O
,	O
because	O
of	O
being	O
shared	O
among	O
the	O
parties	O
)	O
,	O
where	O
the	O
preceding	O
utterances	O
and	O
the	O
party	O
states	O
are	O
jointly	O
encoded	O
for	O
context	Method
representation	Method
,	O
necessary	O
for	O
accurate	O
party	Method
state	Method
representation	Method
.	O

Finally	O
,	O
the	O
model	O
infers	O
emotion	Method
representation	Method
from	O
the	O
party	O
state	O
of	O
the	O
speaker	O
along	O
with	O
the	O
preceding	O
speakers	O
’	O
states	O
as	O
context	O
.	O

This	O
emotion	Method
representation	Method
is	O
used	O
for	O
the	O
final	O
emotion	Task
classification	Task
.	O

We	O
use	O
GRU	O
cells	O
to	O
update	O
the	O
states	O
and	O
representations	O
.	O

Each	O
GRU	Method
cell	Method
computes	O
a	O
hidden	O
state	O
defined	O
as	O
,	O
where	O
is	O
the	O
current	O
input	O
and	O
is	O
the	O
previous	O
GRU	O
state	O
.	O

also	O
serves	O
as	O
the	O
current	O
GRU	O
output	O
.	O

We	O
provide	O
the	O
GRU	O
computation	O
details	O
in	O
the	O
supplementary	O
.	O

GRUs	Method
are	O
efficient	O
networks	O
with	O
trainable	O
parameters	O
:	O
and	O
.	O

We	O
model	O
the	O
emotion	Method
representation	Method
of	O
the	O
current	O
utterance	O
as	O
a	O
function	O
of	O
the	O
emotion	Method
representation	Method
of	O
the	O
previous	O
utterance	O
and	O
the	O
state	O
of	O
the	O
current	O
speaker	O
.	O

Finally	O
,	O
this	O
emotion	Method
representation	Method
is	O
sent	O
to	O
a	O
softmax	Method
layer	Method
for	O
emotion	Task
classification	Task
.	O

0.73	O
0.25	O
subsubsection	O
:	O
Global	O
State	O
(	O
Global	O
GRU	O
)	O
Global	O
state	O
aims	O
to	O
capture	O
the	O
context	O
of	O
a	O
given	O
utterance	O
by	O
jointly	O
encoding	O
utterance	O
and	O
speaker	O
state	O
.	O

Each	O
state	O
also	O
serves	O
as	O
speaker	Method
-	Method
specific	Method
utterance	Method
representation	Method
.	O

Attending	O
on	O
these	O
states	O
facilitates	O
the	O
inter	O
-	O
speaker	O
and	O
inter	O
-	O
utterance	O
dependencies	O
to	O
produce	O
improved	O
context	Method
representation	Method
.	O

The	O
current	O
utterance	O
changes	O
the	O
speaker	O
’s	O
state	O
from	O
to	O
.	O

We	O
capture	O
this	O
change	O
with	O
GRU	O
cell	O
with	O
output	O
size	O
,	O
using	O
and	O
:	O
where	O
is	O
the	O
size	O
of	O
global	O
state	O
vector	O
,	O
is	O
the	O
size	O
of	O
party	O
state	O
vector	O
,	O
,	O
,	O
,	O
,	O
,	O
is	O
party	O
state	O
size	O
,	O
and	O
represents	O
concatenation	O
.	O

subsubsection	O
:	O
Party	O
State	O
(	O
Party	O
GRU	O
)	O
DialogueRNN	Method
keeps	O
track	O
of	O
the	O
state	O
of	O
individual	O
speakers	O
using	O
fixed	O
size	O
vectors	O
through	O
out	O
the	O
conversation	O
.	O

These	O
states	O
are	O
representative	O
of	O
the	O
speakers	O
’	O
state	O
in	O
the	O
conversation	O
,	O
relevant	O
to	O
emotion	Task
classification	Task
.	O

We	O
update	O
these	O
states	O
based	O
on	O
the	O
current	O
(	O
at	O
time	O
)	O
role	O
of	O
a	O
participant	O
in	O
the	O
conversation	O
,	O
which	O
is	O
either	O
speaker	O
or	O
listener	O
,	O
and	O
the	O
incoming	O
utterance	O
.	O

These	O
state	O
vectors	O
are	O
initialized	O
with	O
null	O
vectors	O
for	O
all	O
the	O
participants	O
.	O

The	O
main	O
purpose	O
of	O
this	O
module	O
is	O
to	O
ensure	O
that	O
the	O
model	O
is	O
aware	O
of	O
the	O
speaker	O
of	O
each	O
utterance	O
and	O
handle	O
it	O
accordingly	O
.	O

subsubsection	O
:	O
Speaker	Method
Update	Method
(	O
Speaker	Method
GRU	Method
)	O
:	O
Speaker	O
usually	O
frames	O
the	O
response	O
based	O
on	O
the	O
context	O
,	O
which	O
is	O
the	O
preceding	O
utterances	O
in	O
the	O
conversation	O
.	O

Hence	O
,	O
we	O
capture	O
context	O
relevant	O
to	O
the	O
utterance	O
as	O
follows	O
:	O
where	O
are	O
preceding	O
global	O
states	O
(	O
)	O
,	O
,	O
,	O
and	O
.	O

In	O
eq:7	O
,	O
we	O
calculate	O
attention	O
scores	O
over	O
the	O
previous	O
global	O
states	O
representative	O
of	O
the	O
previous	O
utterances	O
.	O

This	O
assigns	O
higher	O
attention	O
scores	O
to	O
the	O
utterances	O
emotionally	O
relevant	O
to	O
.	O

Finally	O
,	O
in	O
eq:6	O
the	O
context	O
vector	O
is	O
calculated	O
by	O
pooling	O
the	O
previous	O
global	O
states	O
with	O
.	O

Now	O
,	O
we	O
employ	O
a	O
GRU	Method
cell	Method
to	O
update	O
the	O
current	O
speaker	O
state	O
to	O
the	O
new	O
state	O
based	O
on	O
incoming	O
utterance	O
and	O
context	O
using	O
GRU	O
cell	O
of	O
output	O
size	O
where	O
,	O
,	O
,	O
and	O
.	O

This	O
encodes	O
the	O
information	O
on	O
the	O
current	O
utterance	O
along	O
with	O
its	O
context	O
from	O
the	O
global	O
GRU	O
into	O
the	O
speaker	O
’s	O
state	O
,	O
which	O
helps	O
in	O
emotion	Task
classification	Task
down	O
the	O
line	O
.	O

subsubsection	O
:	O
Listener	O
Update	O
:	O
Listener	O
state	O
models	O
the	O
listeners	O
’	O
change	O
of	O
state	O
due	O
to	O
the	O
speaker	O
’s	O
utterance	O
.	O

We	O
tried	O
two	O
listener	Method
state	Method
update	Method
mechanisms	Method
:	O
Simply	O
keep	O
the	O
state	O
of	O
the	O
listener	O
unchanged	O
,	O
that	O
is	O
Employ	O
another	O
GRU	Method
cell	Method
to	O
update	O
the	O
listener	O
state	O
based	O
on	O
listener	O
visual	O
cues	O
(	O
facial	O
expression	O
)	O
and	O
its	O
context	O
,	O
as	O
where	O
,	O
,	O
,	O
and	O
.	O

Listener	O
visual	O
features	O
of	O
party	O
at	O
time	O
are	O
extracted	O
using	O
the	O
model	O
introduced	O
by	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1710	O
-	O
07557	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1710	O
-	O
07557	O
,	O
pretrained	O
on	O
FER2013	Material
dataset	Material
,	O
where	O
feature	Metric
size	Metric
.	O

The	O
simpler	O
first	O
approach	O
turns	O
out	O
to	O
be	O
sufficient	O
,	O
since	O
the	O
second	O
approach	O
yields	O
very	O
similar	O
result	O
while	O
increasing	O
number	O
of	O
parameters	O
.	O

This	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
listener	O
becomes	O
relevant	O
to	O
the	O
conversation	O
only	O
when	O
he	O
/	O
she	O
speaks	O
.	O

In	O
other	O
words	O
,	O
a	O
silent	O
party	O
has	O
no	O
influence	O
in	O
a	O
conversation	O
.	O

Now	O
,	O
when	O
a	O
party	O
speaks	O
,	O
we	O
update	O
his	O
/	O
her	O
state	O
with	O
context	O
which	O
contains	O
relevant	O
information	O
on	O
all	O
the	O
preceding	O
utterances	O
,	O
rendering	O
explicit	O
listener	O
state	O
update	O
unnecessary	O
.	O

This	O
is	O
shown	O
in	O
tab	O
:	O
results	O
-	O
text	O
.	O

subsubsection	O
:	O
Emotion	Method
Representation	Method
(	O
Emotion	Method
GRU	Method
)	O
We	O
infer	O
the	O
emotionally	Method
relevant	Method
representation	Method
of	O
utterance	O
from	O
the	O
speaker	O
’s	O
state	O
and	O
the	O
emotion	Method
representation	Method
of	O
the	O
previous	O
utterance	O
.	O

Since	O
context	O
is	O
important	O
to	O
the	O
emotion	O
of	O
the	O
incoming	O
utterance	O
,	O
feeds	O
fine	O
-	O
tuned	O
emotionally	O
relevant	O
contextual	O
information	O
from	O
other	O
the	O
party	O
states	O
into	O
the	O
emotion	Method
representation	Method
.	O

This	O
establishes	O
a	O
connection	O
between	O
the	O
speaker	O
state	O
and	O
the	O
other	O
party	O
states	O
.	O

Hence	O
,	O
we	O
model	O
with	O
a	O
GRU	Method
cell	Method
(	O
)	O
with	O
output	O
size	O
as	O
where	O
is	O
the	O
size	O
of	O
emotion	O
representation	O
vector	O
,	O
,	O
,	O
,	O
and	O
.	O

Since	O
speaker	O
state	O
gets	O
information	O
from	O
global	O
states	O
,	O
which	O
serve	O
as	O
speaker	Method
-	Method
specific	Method
utterance	Method
representation	Method
,	O
one	O
may	O
claim	O
that	O
this	O
way	O
the	O
model	O
already	O
has	O
access	O
to	O
the	O
information	O
on	O
other	O
parties	O
.	O

However	O
,	O
as	O
shown	O
in	O
the	O
ablation	O
study	O
(	O
sec	O
:	O
ablation	O
-	O
study	O
)	O
emotion	Method
GRU	Method
helps	O
to	O
improve	O
the	O
performance	O
by	O
directly	O
linking	O
states	O
of	O
preceding	O
parties	O
.	O

Further	O
,	O
we	O
believe	O
that	O
speaker	O
and	O
global	O
GRUs	O
(	O
,	O
)	O
jointly	O
act	O
similar	O
to	O
an	O
encoder	Method
,	O
whereas	O
emotion	Method
GRU	Method
serves	O
as	O
a	O
decoder	Method
.	O

subsubsection	O
:	O
Emotion	Task
Classification	Task
We	O
use	O
a	O
two	Method
-	Method
layer	Method
perceptron	Method
with	O
a	O
final	O
softmax	Method
layer	Method
to	O
calculate	O
emotion	O
-	O
class	O
probabilities	O
from	O
emotion	Method
representation	Method
of	O
utterance	O
and	O
then	O
we	O
pick	O
the	O
most	O
likely	O
emotion	O
class	O
:	O
where	O
,	O
,	O
,	O
,	O
,	O
and	O
is	O
the	O
predicted	O
label	O
for	O
utterance	O
.	O

subsubsection	O
:	O
Training	O
We	O
use	O
categorical	Metric
cross	Metric
-	Metric
entropy	Metric
along	O
with	O
L2	Method
-	Method
regularization	Method
as	O
the	O
measure	Metric
of	Metric
loss	Metric
(	Metric
)	O
during	O
training	O
:	O
where	O
is	O
the	O
number	O
of	O
samples	O
/	O
dialogues	O
,	O
is	O
the	O
number	O
of	O
utterances	O
in	O
sample	O
,	O
is	O
the	O
probability	O
distribution	O
of	O
emotion	O
labels	O
for	O
utterance	O
of	O
dialogue	O
,	O
is	O
the	O
expected	O
class	O
label	O
of	O
utterance	O
of	O
dialogue	O
,	O
is	O
the	O
L2	O
-	O
regularizer	O
weight	O
,	O
and	O
is	O
the	O
set	O
of	O
trainable	O
parameters	O
where	O
We	O
used	O
stochastic	Method
gradient	Method
descent	Method
based	Method
Adam	Method
optimizer	Method
to	O
train	O
our	O
network	O
.	O

Hyperparameters	Method
are	O
optimized	O
using	O
grid	Method
search	Method
(	O
values	O
are	O
added	O
to	O
the	O
supplementary	O
material	O
)	O
.	O

subsection	O
:	O
DialogueRNN	Method
Variants	Method
We	O
use	O
DialogueRNN	Method
(	O
sec	O
:	O
model	O
)	O
as	O
the	O
basis	O
for	O
the	O
following	O
models	O
:	O
subsubsection	O
:	O
DialogueRNN	Method
+	O
Listener	O
State	O
Update	O
(	O
DialogueRNN	Method
)	O
:	O
This	O
variant	O
updates	O
the	O
listener	O
state	O
based	O
on	O
the	O
the	O
resulting	O
speaker	O
state	O
,	O
as	O
described	O
in	O
eq:8	O
.	O

subsubsection	O
:	O
Bidirectional	Method
DialogueRNN	Method
(	O
BiDialogueRNN	Method
)	O
:	O
Bidirectional	Method
DialogueRNN	Method
is	O
analogous	O
to	O
bidirectional	Method
RNNs	Method
,	O
where	O
two	O
different	O
RNNs	Method
are	O
used	O
for	O
forward	O
and	O
backward	O
passes	O
of	O
the	O
input	O
sequence	O
.	O

Outputs	O
from	O
the	O
RNNs	Method
are	O
concatenated	O
in	O
sequence	O
level	O
.	O

Similarly	O
,	O
in	O
BiDialogueRNN	Method
,	O
the	O
final	O
emotion	Method
representation	Method
contains	O
information	O
from	O
both	O
past	O
and	O
future	O
utterances	O
in	O
the	O
dialogue	O
through	O
forward	O
and	O
backward	O
DialogueRNNs	O
respectively	O
,	O
which	O
provides	O
better	O
context	O
for	O
emotion	Task
classification	Task
.	O

subsubsection	O
:	O
DialogueRNN	Method
+	O
attention	O
(	O
DialogueRNN	Method
+	O
Att	O
)	O
:	O
For	O
each	O
emotion	Method
representation	Method
,	O
attention	O
is	O
applied	O
over	O
all	O
surrounding	O
emotion	O
representations	O
in	O
the	O
dialogue	O
by	O
matching	O
them	O
with	O
(	O
eq	O
:	O
beta	O
,	O
eq	O
:	O
beta	O
-	O
2	O
)	O
.	O

This	O
provides	O
context	O
from	O
the	O
relevant	O
(	O
based	O
on	O
attention	O
score	O
)	O
future	O
and	O
preceding	O
utterances	O
.	O

subsubsection	O
:	O
Bidirectional	O
DialogueRNN	Method
+	O
Emotional	O
attention	O
(	O
BiDialogueRNN	Method
+	O
Att	O
)	O
:	O
For	O
each	O
emotion	Method
representation	Method
of	O
BiDialogueRNN	Method
,	O
attention	O
is	O
applied	O
over	O
all	O
the	O
emotion	Method
representations	Method
in	O
the	O
dialogue	O
to	O
capture	O
context	O
from	O
the	O
other	O
utterances	O
in	O
dialogue	O
:	O
where	O
,	O
,	O
,	O
and	O
.	O

Further	O
,	O
are	O
fed	O
to	O
a	O
two	Method
-	Method
layer	Method
perceptron	Method
for	O
emotion	Task
classification	Task
,	O
as	O
in	O
eq:5	O
,	O
eq	O
:	O
c	O
-	O
6	O
,	O
eq	O
:	O
c	O
-	O
7	O
.	O

section	O
:	O
Experimental	O
Setting	O
subsection	O
:	O
Datasets	O
Used	O
We	O
use	O
two	O
emotion	O
detection	O
datasets	O
IEMOCAP	Material
and	O
AVEC	Material
to	O
evaluate	O
DialogueRNN	Method
.	O

We	O
partition	O
both	O
datasets	O
into	O
train	O
and	O
test	O
sets	O
with	O
roughly	O
ratio	O
such	O
that	O
the	O
partitions	O
do	O
not	O
share	O
any	O
speaker	O
.	O

table	O
:	O
dataset	O
shows	O
the	O
distribution	O
of	O
train	O
and	O
test	O
samples	O
for	O
both	O
dataset	O
.	O

subsubsection	O
:	O
IEMOCAP	Material
:	O
IEMOCAP	Material
dataset	O
contains	O
videos	O
of	O
two	O
-	O
way	O
conversations	O
of	O
ten	O
unique	O
speakers	O
,	O
where	O
only	O
the	O
first	O
eight	O
speakers	O
from	O
session	O
one	O
to	O
four	O
belong	O
to	O
the	O
train	O
-	O
set	O
.	O

Each	O
video	O
contains	O
a	O
single	O
dyadic	O
dialogue	O
,	O
segmented	O
into	O
utterances	O
.	O

The	O
utterances	O
are	O
annotated	O
with	O
one	O
of	O
six	O
emotion	O
labels	O
,	O
which	O
are	O
happy	O
,	O
sad	O
,	O
neutral	O
,	O
angry	O
,	O
excited	O
,	O
and	O
frustrated	O
.	O

subsubsection	O
:	O
AVEC	Material
:	O
AVEC	Material
dataset	Material
is	O
a	O
modification	O
of	O
SEMAINE	O
database	O
containing	O
interactions	O
between	O
humans	O
and	O
artificially	O
intelligent	O
agents	O
.	O

Each	O
utterance	O
of	O
a	O
dialogue	O
is	O
annotated	O
with	O
four	O
real	O
valued	O
affective	O
attributes	O
:	O
valence	O
(	O
)	O
,	O
arousal	O
(	O
)	O
,	O
expectancy	O
(	O
)	O
,	O
and	O
power	O
(	O
)	O
.	O

The	O
annotations	O
are	O
available	O
every	O
0.2	O
seconds	O
in	O
the	O
original	O
database	O
.	O

However	O
,	O
in	O
order	O
to	O
adapt	O
the	O
annotations	O
to	O
our	O
need	O
of	O
utterance	Task
-	Task
level	Task
annotation	Task
,	O
we	O
averaged	O
the	O
attributes	O
over	O
the	O
span	O
of	O
an	O
utterance	O
.	O

subsection	O
:	O
Baselines	O
and	O
State	O
of	O
the	O
Art	O
For	O
a	O
comprehensive	O
evaluation	O
of	O
DialogueRNN	Method
,	O
we	O
compare	O
our	O
model	O
with	O
the	O
following	O
baseline	O
methods	O
:	O
subsubsection	O
:	O
c	Method
-	Method
LSTM	Method
:	O
Biredectional	Method
LSTM	Method
is	O
used	O
to	O
capture	O
the	O
context	O
from	O
the	O
surrounding	O
utterances	O
to	O
generate	O
context	Method
-	Method
aware	Method
utterance	Method
representation	Method
.	O

However	O
,	O
this	O
model	O
does	O
not	O
differentiate	O
among	O
the	O
speakers	O
.	O

subsubsection	O
:	O
c	Method
-	Method
LSTM	Method
+	Method
Att	Method
:	O
In	O
this	O
variant	O
attention	O
is	O
applied	O
applied	O
to	O
the	O
c	Method
-	Method
LSTM	Method
output	Method
at	O
each	O
timestamp	O
by	O
following	O
eq	O
:	O
beta	O
,	O
eq	O
:	O
beta	O
-	O
2	O
.	O

This	O
provides	O
better	O
context	O
to	O
the	O
final	O
utterance	Method
representation	Method
.	O

subsubsection	O
:	O
TFN	Method
:	O
This	O
is	O
specific	O
to	O
multimodal	Task
scenario	Task
.	O

Tensor	Method
outer	Method
product	Method
is	O
used	O
to	O
capture	O
inter	O
-	O
modality	O
and	O
intra	O
-	O
modality	O
interactions	O
.	O

This	O
model	O
does	O
not	O
capture	O
context	O
from	O
surrounding	O
utterances	O
.	O

subsubsection	O
:	O
MFN	Method
:	O
Specific	O
to	O
multimodal	Task
scenario	Task
,	O
this	O
model	O
utilizes	O
multi	Method
-	Method
view	Method
learning	Method
by	O
modeling	O
view	O
-	O
specific	O
and	O
cross	O
-	O
view	O
interactions	O
.	O

Similar	O
to	O
TFN	Method
,	O
this	O
model	O
does	O
not	O
use	O
contextual	O
information	O
.	O

subsubsection	O
:	O
CNN	Method
:	O
This	O
is	O
identical	O
to	O
our	O
textual	Task
feature	O
extractor	O
network	O
(	O
sec	O
:	O
feature	Method
-	Method
extraction	Method
)	O
and	O
it	O
does	O
not	O
use	O
contextual	O
information	O
from	O
the	O
surrounding	O
utterances	O
.	O

subsubsection	O
:	O
Memnet	Method
:	O
As	O
described	O
in	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
the	O
current	O
utterance	O
is	O
fed	O
to	O
a	O
memory	Method
network	Method
,	O
where	O
the	O
memories	O
correspond	O
to	O
preceding	O
utterances	O
.	O

The	O
output	O
from	O
the	O
memory	Method
network	Method
is	O
used	O
as	O
the	O
final	O
utterance	Method
representation	Method
for	O
emotion	Task
classification	Task
.	O

subsubsection	O
:	O
CMN	Method
:	O
This	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
models	O
utterance	O
context	O
from	O
dialogue	O
history	O
using	O
two	O
distinct	O
GRUs	Method
for	O
two	O
speakers	O
.	O

Finally	O
,	O
utterance	Method
representation	Method
is	O
obtained	O
by	O
feeding	O
the	O
current	O
utterance	O
as	O
query	O
to	O
two	O
distinct	O
memory	Method
networks	Method
for	O
both	O
speakers	O
.	O

subsection	O
:	O
Modalities	O
We	O
evaluated	O
our	O
model	O
primarily	O
on	O
textual	Task
modality	O
.	O

However	O
,	O
to	O
substantiate	O
efficacy	O
of	O
our	O
model	O
in	O
multimodal	Task
scenario	Task
,	O
we	O
also	O
experimented	O
with	O
multimodal	O
features	O
.	O

section	O
:	O
Results	O
and	O
Discussion	O
We	O
compare	O
DialogueRNN	Method
and	O
its	O
variants	O
with	O
the	O
baselines	O
for	O
textual	Task
data	O
in	O
tab	O
:	O
results	O
-	O
text	O
.	O

As	O
expected	O
,	O
on	O
average	O
DialogueRNN	Method
outperforms	O
all	O
the	O
baseline	O
methods	O
,	O
including	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	Method
,	O
on	O
both	O
of	O
the	O
datasets	O
.	O

subsection	O
:	O
Comparison	O
with	O
the	O
State	O
of	O
the	O
Art	O
We	O
compare	O
the	O
performance	O
of	O
DialogueRNN	Method
against	O
the	O
performance	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	Method
on	O
IEMOCAP	Material
and	O
AVEC	Method
datasets	Method
for	O
textual	Task
modality	O
.	O

subsubsection	O
:	O
IEMOCAP	Material
As	O
evidenced	O
by	O
tab	O
:	O
results	O
-	O
text	O
,	O
for	O
IEMOCAP	Material
dataset	O
,	O
our	O
model	O
surpasses	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	Method
by	O
accuracy	Metric
and	O
f1	Metric
-	Metric
score	Metric
on	O
average	O
.	O

We	O
think	O
that	O
this	O
enhancement	O
is	O
caused	O
by	O
the	O
fundamental	O
differences	O
between	O
CMN	Method
and	O
DialogueRNN	Method
,	O
which	O
are	O
party	Method
state	Method
modeling	Method
with	O
in	O
eq:2	Method
,	O
speaker	O
specific	O
utterance	O
treatment	O
in	O
eq:2	O
,	O
eq:3	O
,	O
and	O
global	Method
state	Method
capturing	Method
with	O
in	O
eq:3	O
.	O

Since	O
we	O
deal	O
with	O
six	O
unbalanced	O
emotion	O
labels	O
,	O
we	O
also	O
explored	O
the	O
model	O
performance	O
for	O
individual	O
labels	O
.	O

DialogueRNN	Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	Method
in	O
five	O
out	O
of	O
six	O
emotion	O
classes	O
by	O
significant	O
margin	O
.	O

For	O
frustrated	O
class	O
,	O
DialogueRNN	Method
lags	O
behind	O
CMN	Method
by	O
f1	Metric
-	Metric
score	Metric
.	O

We	O
think	O
that	O
DialogueRNN	Method
may	O
surpass	O
CMN	Method
using	O
a	O
standalone	Method
classifier	Method
for	O
frustrated	O
class	O
.	O

However	O
,	O
it	O
can	O
be	O
observed	O
in	O
tab	O
:	O
results	O
-	O
text	O
that	O
some	O
of	O
the	O
other	O
variants	O
of	O
DialogueRNN	Method
,	O
like	O
BiDialogueRNN	Method
has	O
already	O
outperformed	O
CMN	Method
for	O
frustrated	O
class	O
.	O

subsubsection	O
:	O
AVEC	Material
DialogueRNN	Method
outperforms	O
CMN	Method
for	O
valence	O
,	O
arousal	O
,	O
expectancy	O
,	O
and	O
power	O
attributes	O
;	O
see	O
tab	O
:	O
results	O
-	O
text	O
.	O

It	O
yields	O
significantly	O
lower	O
mean	Metric
absolute	Metric
error	Metric
(	O
)	O
and	O
higher	O
Pearson	Metric
correlation	Metric
coefficient	Metric
(	O
)	O
for	O
all	O
four	O
attributes	O
.	O

We	O
believe	O
this	O
to	O
be	O
due	O
to	O
the	O
incorporation	O
of	O
party	O
state	O
and	O
emotion	O
GRU	O
,	O
which	O
are	O
missing	O
from	O
CMN	Method
.	O

subsection	O
:	O
DialogueRNN	Method
vs.	O
DialogueRNN	Method
Variants	Method
We	O
discuss	O
the	O
performance	O
of	O
different	O
DialogueRNN	Method
variants	O
on	O
IEMOCAP	Material
and	O
AVEC	Method
datasets	Method
for	O
textual	Task
modality	O
.	O

subsubsection	O
:	O
DialogueRNN	Method
:	O
Following	O
tab	O
:	O
results	O
-	O
text	O
,	O
using	O
explicit	Method
listener	Method
state	Method
update	Method
yields	O
slightly	O
worse	O
performance	O
than	O
regular	O
DialogueRNN	Method
.	O

This	O
is	O
true	O
for	O
both	O
IEMOCAP	Material
and	O
AVEC	Method
datasets	Method
in	O
general	O
.	O

However	O
,	O
the	O
only	O
exception	O
to	O
this	O
trend	O
is	O
for	O
happy	O
emotion	O
label	O
for	O
IEMOCAP	Material
,	O
where	O
DialogueRNN	Method
outperforms	O
DialogueRNN	Method
by	O
f1	Metric
-	Metric
score	Metric
.	O

We	O
surmise	O
that	O
,	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
listener	O
becomes	O
relevant	O
to	O
the	O
conversation	O
only	O
when	O
he	O
/	O
she	O
speaks	O
.	O

Now	O
,	O
in	O
DialogueRNN	Method
,	O
when	O
a	O
party	O
speaks	O
,	O
we	O
update	O
his	O
/	O
her	O
state	O
with	O
context	O
which	O
contains	O
relevant	O
information	O
on	O
all	O
the	O
preceding	O
utterances	O
,	O
rendering	O
explicit	O
listener	O
state	O
update	O
of	O
DialogueRNN	Method
unnecessary	O
.	O

subsubsection	O
:	O
BiDialogueRNN	Method
:	O
Since	O
BiDialogueRNN	Method
captures	O
context	O
from	O
the	O
future	O
utterances	O
,	O
we	O
expect	O
improved	O
performance	O
from	O
it	O
over	O
DialogueRNN	Method
.	O

This	O
is	O
confirmed	O
in	O
tab	O
:	O
results	O
-	O
text	O
,	O
where	O
BiDialogueRNN	Method
outperforms	O
DialogueRNN	Method
on	O
average	O
on	O
both	O
datasets	O
.	O

subsubsection	O
:	O
DialogueRNN	Method
+	Method
Attn	Method
:	O
DialogueRNN	Method
+	Method
Attn	Method
also	O
uses	O
information	O
from	O
the	O
future	O
utterances	O
.	O

However	O
,	O
here	O
we	O
take	O
information	O
from	O
both	O
past	O
and	O
future	O
utterances	O
by	O
matching	O
them	O
with	O
the	O
current	O
utterance	O
and	O
calculating	O
attention	O
score	O
over	O
them	O
.	O

This	O
provides	O
relevance	O
to	O
emotionally	O
important	O
context	O
utterances	O
,	O
yielding	O
better	O
performance	O
than	O
BiDialogueRNN	Method
.	O

The	O
improvement	O
over	O
BiDialogueRNN	Method
is	O
f1	Metric
-	Metric
score	Metric
for	O
IEMOCAP	Material
and	O
consistently	O
lower	O
and	O
higher	O
in	O
AVEC	Material
.	O

subsubsection	O
:	O
BiDialogueRNN	Method
+	Method
Attn	Method
:	O
Since	O
this	O
setting	O
generates	O
the	O
final	O
emotion	Method
representation	Method
by	O
attending	O
over	O
the	O
emotion	Method
representation	Method
from	O
BiDialogueRNN	Method
,	O
we	O
expect	O
better	O
performance	O
than	O
both	O
BiDialogueRNN	Method
and	O
DialogueRNN	Method
+	Method
Attn	Method
.	O

This	O
is	O
confirmed	O
in	O
tab	O
:	O
results	O
-	O
text	O
,	O
where	O
this	O
setting	O
performs	O
the	O
best	O
in	O
general	O
than	O
any	O
other	O
methods	O
discussed	O
,	O
on	O
both	O
datasets	O
.	O

This	O
setting	O
yields	O
higher	O
f1	Metric
-	Metric
score	Metric
on	O
average	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	Method
and	O
higher	O
f1	Metric
-	Metric
score	Metric
than	O
vanilla	O
DialogueRNN	Method
for	O
IEMOCAP	Material
dataset	O
.	O

For	O
AVEC	Material
dataset	Material
also	O
,	O
this	O
setting	O
gives	O
the	O
best	O
performance	O
across	O
all	O
the	O
four	O
attributes	O
.	O

subsection	O
:	O
Multimodal	Task
Setting	Task
As	O
both	O
IEMOCAP	Material
and	O
AVEC	Material
dataset	Material
contain	O
multimodal	O
information	O
,	O
we	O
have	O
evaluated	O
DialogueRNN	Method
on	O
multimodal	O
features	O
as	O
used	O
and	O
provided	O
by	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
.	O

We	O
use	O
concatenation	O
of	O
the	O
unimodal	O
features	O
as	O
a	O
fusion	Method
method	Method
by	O
following	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
since	O
fusion	Method
mechanism	Method
is	O
not	O
a	O
focus	O
of	O
this	O
paper	O
.	O

Now	O
,	O
as	O
we	O
can	O
see	O
in	O
tab	O
:	O
results	O
-	O
multimodal	O
,	O
DialogueRNN	Method
significantly	O
outperforms	O
the	O
strong	O
baselines	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	Method
.	O

subsection	O
:	O
Case	O
Studies	O
subsubsection	O
:	O
Dependency	O
on	O
preceding	O
utterances	O
(	O
DialogueRNN	Method
)	O
One	O
of	O
the	O
crucial	O
components	O
of	O
DialogueRNN	Method
is	O
its	O
attention	Method
module	Method
over	O
the	O
outputs	O
of	O
global	Method
GRU	Method
(	Method
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
attention	O
vector	O
(	O
eq:7	O
)	O
over	O
the	O
history	O
of	O
a	O
given	O
test	O
utterance	O
compared	O
with	O
the	O
attention	O
vector	O
from	O
the	O
CMN	Method
model	Method
.	O

The	O
attention	O
of	O
our	O
model	O
is	O
more	O
focused	O
compared	O
with	O
CMN	Method
:	O
the	O
latter	O
gives	O
diluted	O
attention	Metric
scores	Metric
leading	O
to	O
misclassifications	O
.	O

We	O
observe	O
this	O
trend	O
of	O
focused	O
-	O
attention	O
across	O
cases	O
and	O
posit	O
that	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
confidence	Metric
indicator	Metric
.	O

Further	O
in	O
this	O
example	O
,	O
the	O
test	O
utterance	O
by	O
(	O
turn	O
44	O
)	O
comprises	O
of	O
a	O
change	O
in	O
emotion	O
from	O
neutral	O
to	O
frustrated	O
.	O

DialogueRNN	Method
anticipates	O
this	O
correctly	O
by	O
attending	O
to	O
turn	O
41	O
and	O
42	O
that	O
are	O
spoken	O
by	O
and	O
,	O
respectively	O
.	O

These	O
two	O
utterances	O
provide	O
self	O
and	O
inter	O
-	O
party	O
influences	O
that	O
trigger	O
the	O
emotional	O
shift	O
.	O

CMN	Method
,	O
however	O
,	O
fails	O
to	O
capture	O
such	O
dependencies	O
and	O
wrongly	O
predicts	O
neutral	O
emotion	O
.	O

subsubsection	O
:	O
Dependency	O
on	O
future	O
utterances	O
(	O
BiDialogueRNN	Method
+	O
Att	O
)	O
fig	O
:	O
case	O
-	O
study	O
visualizes	O
the	O
(	O
eq	O
:	O
beta	O
)	O
attention	O
over	O
the	O
emotion	Method
representations	Method
for	O
a	O
segment	O
of	O
a	O
conversation	O
between	O
a	O
couple	O
.	O

In	O
the	O
discussion	O
,	O
the	O
woman	O
(	O
)	O
is	O
initially	O
at	O
a	O
neutral	O
state	O
,	O
whereas	O
the	O
man	O
(	O
)	O
is	O
angry	O
throughout	O
.	O

The	O
figure	O
reveals	O
that	O
the	O
emotional	O
attention	O
of	O
the	O
woman	O
is	O
localized	O
to	O
the	O
duration	O
of	O
her	O
neutral	O
state	O
(	O
turns	O
1	O
-	O
16	O
approximately	O
)	O
.	O

For	O
example	O
,	O
in	O
the	O
dialogue	O
,	O
turns	O
,	O
and	O
strongly	O
attend	O
to	O
turn	O
.	O

Interestingly	O
,	O
turn	O
attends	O
to	O
both	O
past	O
(	O
turn	O
)	O
and	O
future	O
(	O
turn	O
)	O
utterances	O
.	O

Similar	O
trend	O
across	O
other	O
utterances	O
establish	O
inter	O
-	O
dependence	O
between	O
emotional	O
states	O
of	O
future	O
and	O
past	O
utterances	O
.	O

0.49	O
0.49	O
0.49	O
0.49	O
The	O
beneficial	O
consideration	O
of	O
future	O
utterances	O
through	O
is	O
also	O
apparent	O
through	O
turns	O
.	O

These	O
utterances	O
focus	O
on	O
the	O
distant	O
future	O
(	O
turn	O
)	O
where	O
the	O
man	O
is	O
at	O
an	O
enraged	O
state	O
,	O
thus	O
capturing	O
emotional	O
correlations	O
across	O
time	O
.	O

Although	O
,	O
turn	O
is	O
misclassified	O
by	O
our	O
model	O
,	O
it	O
still	O
manages	O
to	O
infer	O
a	O
related	O
emotional	O
state	O
(	O
anger	O
)	O
against	O
the	O
correct	O
state	O
(	O
frustrated	O
)	O
.	O

We	O
analyze	O
more	O
of	O
this	O
trend	O
in	O
section	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Dependency	O
on	O
distant	O
context	O
For	O
all	O
correct	O
predictions	O
in	O
the	O
IEMOCAP	Material
test	O
set	O
in	O
fig	O
:	O
DeltaTTrend	O
we	O
summarize	O
the	O
distribution	O
over	O
the	O
relative	O
distance	O
between	O
test	O
utterance	O
and	O
(	O
)	O
highest	O
attended	O
utterance	O
–	O
either	O
in	O
the	O
history	O
or	O
future	O
–	O
in	O
the	O
conversation	O
.	O

This	O
reveals	O
a	O
decreasing	O
trend	O
with	O
the	O
highest	O
dependence	O
being	O
within	O
the	O
local	O
context	O
.	O

However	O
,	O
a	O
significant	O
portion	O
of	O
the	O
test	O
utterances	O
(	O
)	O
,	O
attend	O
to	O
utterances	O
that	O
are	O
to	O
turns	O
away	O
from	O
themselves	O
,	O
which	O
highlights	O
the	O
important	O
role	O
of	O
long	O
-	O
term	O
emotional	O
dependencies	O
.	O

Such	O
cases	O
primarily	O
occur	O
in	O
conversations	O
that	O
maintain	O
a	O
specific	O
affective	O
tone	O
and	O
do	O
not	O
incur	O
frequent	O
emotional	O
shifts	O
.	O

fig	O
:	O
DistantAttention	O
demonstrates	O
a	O
case	O
of	O
long	Task
-	Task
term	Task
context	Task
dependency	Task
.	O

The	O
presented	O
conversation	O
maintains	O
a	O
happy	O
mood	O
throughout	O
the	O
dialogue	O
.	O

Although	O
the	O
turn	O
comprising	O
the	O
sentence	O
Horrible	O
thing	O
.	O

I	O
hated	O
it	O
.	O

seems	O
to	O
be	O
a	O
negative	O
expression	O
,	O
when	O
seen	O
with	O
the	O
global	O
context	O
,	O
it	O
reveals	O
the	O
excitement	O
present	O
in	O
the	O
speaker	O
.	O

To	O
disambiguate	O
such	O
cases	O
,	O
our	O
model	O
attends	O
to	O
distant	O
utterances	O
in	O
the	O
past	O
(	O
turn	O
,	O
)	O
which	O
serve	O
as	O
prototypes	O
of	O
the	O
emotional	O
tonality	O
of	O
the	O
overall	O
conversation	O
.	O

subsection	O
:	O
Error	Method
Analysis	Method
A	O
noticeable	O
trend	O
in	O
the	O
predictions	O
is	O
the	O
high	O
level	O
of	O
cross	O
-	O
predictions	O
amongst	O
related	O
emotions	O
.	O

Most	O
of	O
the	O
misclassifications	O
by	O
the	O
model	O
for	O
happy	O
emotion	O
are	O
for	O
excited	O
class	O
.	O

Also	O
,	O
anger	O
and	O
frustrated	O
share	O
misclassifications	O
amongst	O
each	O
other	O
.	O

We	O
suspect	O
this	O
is	O
due	O
to	O
subtle	O
difference	O
between	O
those	O
emotion	O
pairs	O
,	O
resulting	O
in	O
harder	O
disambiguation	Task
.	O

Another	O
class	O
with	O
high	O
rate	O
of	O
false	Metric
-	Metric
positives	Metric
is	O
the	O
neutral	O
class	O
.	O

Primary	O
reason	O
for	O
this	O
could	O
be	O
its	O
majority	O
in	O
the	O
class	O
distribution	O
over	O
the	O
considered	O
emotions	O
.	O

At	O
the	O
dialogue	O
level	O
,	O
we	O
observe	O
that	O
a	O
significant	O
amount	O
of	O
errors	O
occur	O
at	O
turns	O
having	O
a	O
change	O
of	O
emotion	O
from	O
the	O
previous	O
turn	O
of	O
the	O
same	O
party	O
.	O

Across	O
all	O
the	O
occurrences	O
of	O
these	O
emotional	O
-	O
shifts	O
in	O
the	O
testing	O
set	O
,	O
our	O
model	O
correctly	O
predicts	O
instances	O
.	O

This	O
stands	O
less	O
as	O
compared	O
to	O
the	O
success	O
that	O
it	O
achieves	O
at	O
regions	O
of	O
no	O
emotional	O
-	O
shift	O
.	O

Changes	O
in	O
emotions	O
in	O
a	O
dialogue	O
is	O
a	O
complex	O
phenomenon	O
governed	O
by	O
latent	O
dynamics	O
.	O

Further	O
improvement	O
of	O
these	O
cases	O
remain	O
as	O
an	O
open	O
area	O
of	O
research	O
.	O

subsection	O
:	O
Ablation	Task
Study	Task
The	O
main	O
novelty	O
of	O
our	O
method	O
is	O
the	O
introduction	O
of	O
party	O
state	O
and	O
emotion	Method
GRU	Method
(	Method
)	O
.	O

To	O
comprehensively	O
study	O
the	O
impact	O
of	O
these	O
two	O
components	O
,	O
we	O
remove	O
them	O
one	O
at	O
a	O
time	O
and	O
evaluate	O
their	O
impact	O
on	O
IEMOCAP	Material
.	O

As	O
expected	O
,	O
following	O
tab	O
:	O
ablation	Task
,	O
party	O
state	O
stands	O
very	O
important	O
,	O
as	O
without	O
its	O
presence	O
the	O
performance	O
falls	O
by	O
.	O

We	O
suspect	O
that	O
party	O
state	O
helps	O
in	O
extracting	O
useful	O
contextual	O
information	O
relevant	O
to	O
parties	O
’	O
emotion	O
.	O

Emotion	Method
GRU	Method
is	O
also	O
impactful	O
,	O
but	O
less	O
than	O
party	O
state	O
,	O
as	O
its	O
absence	O
causes	O
performance	O
to	O
fall	O
by	O
only	O
.	O

We	O
believe	O
the	O
reason	O
to	O
be	O
the	O
lack	O
of	O
context	O
flow	O
from	O
the	O
other	O
parties	O
’	O
states	O
through	O
the	O
emotion	Method
representation	Method
of	O
the	O
preceding	O
utterances	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
an	O
RNN	Method
-	O
based	O
neural	O
architecture	O
for	O
emotion	Task
detection	Task
in	O
a	O
conversation	O
.	O

In	O
contrast	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
,	O
CMN	Method
,	O
our	O
method	O
treats	O
each	O
incoming	O
utterance	O
taking	O
into	O
account	O
characteristics	O
of	O
the	O
speaker	O
,	O
which	O
gives	O
finer	O
context	O
to	O
the	O
utterance	O
.	O

Our	O
model	O
outperforms	O
the	O
current	O
state	O
of	O
the	O
art	O
on	O
two	O
distinct	O
datasets	O
in	O
both	O
textual	Task
and	O
multimodal	Task
setting	Task
.	O

Our	O
method	O
is	O
designed	O
to	O
be	O
scalable	O
to	O
multi	Task
-	Task
party	Task
setting	Task
with	O
more	O
than	O
two	O
speakers	O
,	O
though	O
we	O
could	O
not	O
test	O
it	O
due	O
to	O
unavailability	O
of	O
a	O
multi	O
-	O
party	O
conversation	O
dataset	O
with	O
emotion	O
labels	O
.	O

This	O
is	O
left	O
to	O
our	O
future	O
work	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Supplementary	O
Material	O
subsection	O
:	O
GRU	Method
Details	O
We	O
use	O
GRU	O
cells	O
are	O
defined	O
as	O
,	O
where	O
:	O
is	O
the	O
current	O
input	O
,	O
is	O
the	O
previous	O
GRU	O
output	O
,	O
and	O
is	O
the	O
current	O
GRU	O
output	O
.	O

and	O
are	O
GRU	O
parameters	O
.	O

,	O
are	O
refresh	O
gate	O
and	O
update	O
gate	O
respectively	O
.	O

is	O
the	O
candidate	O
output	O
.	O

stands	O
for	O
hadamard	O
product	O
.	O

[	O
b	O
]	O
DialogueRNN	Method
algorithm	O
[	O
1	O
]	O
DialogueRNN	Method
,	O
=	O
utterances	O
in	O
the	O
conversation	O
,	O
S	O
=	O
speakers	O
Initialize	O
the	O
participant	O
states	O
with	O
null	O
vector	O
:	O
i:	O
[	O
1	O
,	O
M	O
]	O
Set	O
the	O
initial	O
global	O
and	O
emotional	O
state	O
as	O
null	O
vector	O
:	O
Pass	O
the	O
dialogue	O
through	O
RNN	Method
:	O
t:	O
[	O
1	O
,	O
N	O
]	O
return	O
DialogueCell	O
,	O
,	O
,	O
,	O
Update	O
global	O
state	O
:	O
Get	O
context	O
from	O
preceding	O
global	O
states	O
:	O
Update	O
participant	O
states	O
:	O
i:	O
[	O
1	O
,	O
M	O
]	O
Update	O
speaker	O
state	O
:	O
Update	O
listener	O
state	O
:	O
Update	O
emotion	Method
representation	Method
:	O
return	O
document	O
:	O
Deep	O
Multi	Method
-	Method
Center	Method
Learning	Method
for	O
Face	Task
Alignment	Task
Facial	O
landmarks	O
are	O
highly	O
correlated	O
with	O
each	O
other	O
since	O
a	O
certain	O
landmark	O
can	O
be	O
estimated	O
by	O
its	O
neighboring	O
landmarks	O
.	O

Most	O
of	O
the	O
existing	O
deep	Method
learning	Method
methods	Method
only	O
use	O
one	O
fully	Method
-	Method
connected	Method
layer	Method
called	O
shape	Method
prediction	Method
layer	Method
to	O
estimate	O
the	O
locations	O
of	O
facial	O
landmarks	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
deep	Method
learning	Method
framework	Method
named	O
Multi	Method
-	Method
Center	Method
Learning	Method
with	O
multiple	Method
shape	Method
prediction	Method
layers	Method
for	O
face	Task
alignment	Task
.	O

In	O
particular	O
,	O
each	O
shape	Method
prediction	Method
layer	Method
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
semantically	O
relevant	O
landmarks	O
respectively	O
.	O

Challenging	O
landmarks	O
are	O
focused	O
firstly	O
,	O
and	O
each	O
cluster	O
of	O
landmarks	O
is	O
further	O
optimized	O
respectively	O
.	O

Moreover	O
,	O
to	O
reduce	O
the	O
model	Metric
complexity	Metric
,	O
we	O
propose	O
a	O
model	Method
assembling	Method
method	Method
to	O
integrate	O
multiple	Method
shape	Method
prediction	Method
layers	Method
into	O
one	O
shape	Method
prediction	Method
layer	Method
.	O

Extensive	O
experiments	O
demonstrate	O
that	O
our	O
method	O
is	O
effective	O
for	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
with	O
real	O
-	O
time	O
performance	O
.	O

The	O
code	O
for	O
our	O
method	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
ZhiwenShao	O
/	O
MCNet	O
-	O
Extension	O
.	O

Multi	Method
-	Method
Center	Method
Learning	Method
,	O
Model	Task
Assembling	Task
,	O
Face	Task
Alignment	Task
section	O
:	O
Introduction	O
Face	Task
alignment	Task
refers	O
to	O
detecting	Task
facial	Task
landmarks	Task
such	O
as	O
eye	O
centers	O
,	O
nose	O
tip	O
,	O
and	O
mouth	O
corners	O
.	O

It	O
is	O
the	O
preprocessor	O
stage	O
of	O
many	O
face	Task
analysis	Task
tasks	Task
like	O
face	Task
animation	Task
,	O
face	Task
beautification	Task
,	O
and	O
face	Task
recognition	Task
.	O

A	O
robust	Task
and	Task
accurate	Task
face	Task
alignment	Task
is	O
still	O
challenging	O
in	O
unconstrained	Task
scenarios	Task
,	O
owing	O
to	O
severe	O
occlusions	O
and	O
large	O
appearance	O
variations	O
.	O

Most	O
conventional	O
methods	O
only	O
use	O
low	O
-	O
level	O
handcrafted	O
features	O
and	O
are	O
not	O
based	O
on	O
the	O
prevailing	O
deep	Method
neural	Method
networks	Method
,	O
which	O
limits	O
their	O
capacity	O
to	O
represent	O
highly	O
complex	O
faces	O
.	O

Recently	O
,	O
several	O
methods	O
use	O
deep	Method
networks	Method
to	O
estimate	O
shapes	O
from	O
input	O
faces	O
.	O

Sun	O
et	O
al	O
.	O

,	O
Zhou	O
et	O
al	O
.	O

,	O
and	O
Zhang	O
et	O
al	O
.	O

employed	O
cascaded	Method
deep	Method
networks	Method
to	O
refine	O
predicted	O
shapes	O
successively	O
.	O

Due	O
to	O
the	O
use	O
of	O
multiple	Method
networks	Method
,	O
these	O
methods	O
have	O
high	O
model	Metric
complexity	Metric
with	O
complicated	O
training	Method
processes	Method
.	O

Taking	O
this	O
into	O
account	O
,	O
Zhang	O
et	O
al	O
.	O

proposed	O
a	O
Tasks	Method
-	Method
Constrained	Method
Deep	Method
Convolutional	Method
Network	Method
(	O
TCDCN	Method
)	O
,	O
which	O
uses	O
only	O
one	O
deep	Method
network	Method
with	O
excellent	O
performance	O
.	O

However	O
,	O
it	O
needs	O
extra	O
labels	O
of	O
facial	O
attributes	O
for	O
training	O
samples	O
,	O
which	O
limits	O
its	O
universality	O
.	O

Each	O
facial	O
landmark	O
is	O
not	O
isolated	O
but	O
highly	O
correlated	O
with	O
adjacent	O
landmarks	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
facial	O
landmarks	O
along	O
the	O
chin	O
are	O
all	O
occluded	O
,	O
and	O
landmarks	O
around	O
the	O
mouth	O
are	O
partially	O
occluded	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
landmarks	O
on	O
the	O
right	O
side	O
of	O
face	O
are	O
almost	O
invisible	O
.	O

Therefore	O
,	O
landmarks	O
in	O
the	O
same	O
local	O
face	O
region	O
have	O
similar	O
properties	O
including	O
occlusion	O
and	O
visibility	O
.	O

It	O
is	O
observed	O
that	O
the	O
nose	O
can	O
be	O
localized	O
roughly	O
with	O
the	O
locations	O
of	O
eyes	O
and	O
mouth	O
.	O

There	O
are	O
also	O
structural	O
correlations	O
among	O
different	O
facial	O
parts	O
.	O

Motivated	O
by	O
this	O
fact	O
,	O
facial	O
landmarks	O
are	O
divided	O
into	O
several	O
clusters	O
based	O
on	O
their	O
semantic	O
relevance	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
deep	Method
learning	Method
framework	Method
named	O
Multi	Method
-	Method
Center	Method
Learning	Method
(	O
MCL	Method
)	O
to	O
exploit	O
the	O
strong	O
correlations	O
among	O
landmarks	O
.	O

In	O
particular	O
,	O
our	O
network	O
uses	O
multiple	Method
shape	Method
prediction	Method
layers	Method
to	O
predict	O
the	O
locations	O
of	O
landmarks	O
,	O
and	O
each	O
shape	Method
prediction	Method
layer	Method
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
landmarks	O
respectively	O
.	O

By	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
,	O
challenging	O
landmarks	O
are	O
focused	O
firstly	O
,	O
and	O
each	O
cluster	O
of	O
landmarks	O
is	O
further	O
optimized	O
respectively	O
.	O

Moreover	O
,	O
to	O
decrease	O
the	O
model	Metric
complexity	Metric
,	O
we	O
propose	O
a	O
model	Method
assembling	Method
method	Method
to	O
integrate	O
multiple	Method
shape	Method
prediction	Method
layers	Method
into	O
one	O
shape	Method
prediction	Method
layer	Method
.	O

The	O
entire	O
framework	O
reinforces	O
the	O
learning	Method
process	Method
of	O
each	O
landmark	O
with	O
a	O
low	O
model	Metric
complexity	Metric
.	O

The	O
main	O
contributions	O
of	O
this	O
study	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
multi	Method
-	Method
center	Method
learning	Method
framework	Method
for	O
exploiting	O
the	O
strong	O
correlations	O
among	O
landmarks	O
.	O

We	O
propose	O
a	O
model	Method
assembling	Method
method	Method
which	O
ensures	O
a	O
low	O
model	Metric
complexity	Metric
.	O

Extensive	O
experiments	O
demonstrate	O
that	O
our	O
method	O
is	O
effective	O
for	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
with	O
real	O
-	O
time	O
performance	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
below	O
.	O

We	O
discuss	O
related	O
works	O
in	O
the	O
next	O
section	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
illuminate	O
the	O
structure	O
of	O
our	O
network	O
and	O
the	O
learning	Method
algorithm	Method
.	O

Extensive	O
experiments	O
are	O
carried	O
out	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Section	O
[	O
reference	O
]	O
concludes	O
this	O
work	O
.	O

section	O
:	O
Related	O
Work	O
We	O
review	O
researches	O
from	O
three	O
aspects	O
related	O
to	O
our	O
method	O
:	O
conventional	O
face	Task
alignment	Task
,	O
unconstrained	Task
face	Task
alignment	Task
,	O
face	Task
alignment	Task
via	O
deep	Method
learning	Method
.	O

subsection	O
:	O
Conventional	O
Face	Task
Alignment	Task
Conventional	O
face	Method
alignment	Method
methods	Method
can	O
be	O
classified	O
as	O
two	O
categories	O
:	O
template	Method
fitting	Method
and	O
regression	Method
-	Method
based	Method
.	O

Template	Method
fitting	Method
methods	Method
match	O
faces	O
by	O
constructing	O
shape	Method
templates	Method
.	O

Cootes	O
et	O
al	O
.	O

proposed	O
a	O
typical	O
template	Method
fitting	Method
method	Method
named	O
Active	Method
Appearance	Method
Model	Method
(	O
AAM	Method
)	O
,	O
which	O
minimizes	O
the	O
texture	O
residual	O
to	O
estimate	O
the	O
shape	O
.	O

Asthana	O
et	O
al	O
.	O

used	O
regression	Method
techniques	Method
to	O
learn	O
functions	O
from	O
response	O
maps	O
to	O
shapes	O
,	O
in	O
which	O
the	O
response	O
map	O
has	O
stronger	O
robustness	Metric
and	O
generalization	Metric
ability	Metric
than	O
texture	O
based	O
features	O
of	O
AAM	Method
.	O

Pedersoli	O
et	O
al	O
.	O

developed	O
the	O
mixture	Method
of	Method
trees	Method
of	Method
parts	Method
method	Method
by	O
extending	O
the	O
mixtures	O
from	O
trees	O
to	O
graphs	O
,	O
and	O
learned	O
a	O
deformable	Method
detector	Method
to	O
align	O
its	O
parts	O
to	O
faces	O
.	O

However	O
,	O
these	O
templates	O
are	O
not	O
complete	O
enough	O
to	O
cover	O
complex	O
variations	O
,	O
which	O
are	O
difficult	O
to	O
be	O
generalized	O
to	O
unseen	O
faces	O
.	O

Regression	Method
-	Method
based	Method
methods	Method
predict	O
the	O
locations	O
of	O
facial	O
landmarks	O
by	O
learning	O
a	O
regression	Method
function	Method
from	O
face	O
features	O
to	O
shapes	O
.	O

Cao	O
et	O
al	O
.	O

proposed	O
an	O
Explicit	Method
Shape	Method
Regression	Method
(	O
ESR	Method
)	O
method	O
to	O
predict	O
the	O
shape	O
increment	O
with	O
pixel	O
-	O
difference	O
features	O
.	O

Xiong	O
et	O
al	O
.	O

proposed	O
a	O
Supervised	Method
Descent	Method
Method	Method
(	O
SDM	Method
)	O
to	O
detect	Task
landmarks	Task
by	O
solving	O
the	O
nonlinear	Task
least	Task
squares	Task
problem	Task
,	O
with	O
Scale	O
-	O
Invariant	O
Feature	O
Transform	O
(	O
SIFT	O
)	O
features	O
and	O
linear	Method
regressors	Method
being	O
applied	O
.	O

Ren	O
et	O
al	O
.	O

used	O
a	O
locality	Method
principle	Method
to	O
extract	O
a	O
set	O
of	O
Local	O
Binary	O
Features	O
(	O
LBF	Method
)	Method
,	O
in	O
which	O
a	O
linear	Method
regression	Method
is	O
utilized	O
for	O
localizing	Task
landmarks	Task
.	O

Lee	O
et	O
al	O
.	O

employs	O
Cascade	Method
Gaussian	Method
Process	Method
Regression	Method
Trees	Method
(	O
cGPRT	Method
)	O
with	O
shape	Method
-	Method
indexed	Method
difference	Method
of	Method
Gaussian	Method
features	Method
to	O
achieve	O
face	Task
alignment	Task
.	O

It	O
has	O
a	O
better	O
generalization	Metric
ability	Metric
than	O
cascade	Method
regression	Method
trees	Method
,	O
and	O
shows	O
strong	O
robustness	Metric
against	O
geometric	O
variations	O
of	O
faces	O
.	O

Most	O
of	O
these	O
methods	O
give	O
an	O
initial	O
shape	O
and	O
refine	O
the	O
shape	O
in	O
an	O
iterative	O
manner	O
,	O
where	O
the	O
final	O
solutions	O
are	O
prone	O
to	O
getting	O
trapped	O
in	O
a	O
local	O
optimum	O
with	O
a	O
poor	O
initialization	O
.	O

In	O
contrast	O
,	O
our	O
method	O
uses	O
a	O
deep	Method
neural	Method
network	Method
to	O
regress	O
from	O
raw	O
face	O
patches	O
to	O
the	O
locations	O
of	O
landmarks	O
.	O

subsection	O
:	O
Unconstrained	O
Face	Task
Alignment	Task
Large	O
pose	O
variations	O
and	O
severe	O
occlusions	O
are	O
major	O
challenges	O
in	O
unconstrained	O
environments	O
.	O

Unconstrained	Method
face	Method
alignment	Method
methods	Method
are	O
based	O
on	O
3D	Method
models	Method
or	O
deal	O
with	O
occlusions	O
explicitly	O
.	O

Many	O
methods	O
utilize	O
3D	Method
shape	Method
models	Method
to	O
solve	O
large	Task
-	Task
pose	Task
face	Task
alignment	Task
.	O

Nair	O
et	O
al	O
.	O

refined	O
the	O
fit	O
of	O
a	O
3D	Method
point	Method
distribution	Method
model	Method
to	O
perform	O
landmark	Task
detection	Task
.	O

Yu	O
et	O
al	O
.	O

used	O
a	O
cascaded	Method
deformable	Method
shape	Method
model	Method
to	O
detect	O
landmarks	Task
of	Task
large	Task
-	Task
pose	Task
faces	Task
.	O

Cao	O
et	O
al	O
.	O

employed	O
a	O
displaced	Method
dynamic	Method
expression	Method
regression	Method
to	O
estimate	O
the	O
3D	O
face	O
shape	O
and	O
2D	O
facial	O
landmarks	O
.	O

The	O
predicted	O
2D	O
landmarks	O
are	O
used	O
to	O
adjust	O
the	O
model	O
parameters	O
to	O
better	O
fit	O
the	O
current	O
user	O
.	O

Jeni	O
et	O
al	O
.	O

proposed	O
a	O
3D	Method
cascade	Method
regression	Method
method	Method
to	O
implement	O
3D	Task
face	Task
alignment	Task
,	O
which	O
can	O
maintain	O
the	O
pose	O
invariance	O
of	O
facial	O
landmarks	O
within	O
the	O
range	O
of	O
around	O
degrees	O
.	O

There	O
are	O
several	O
occlusion	Method
-	Method
free	Method
face	Method
alignment	Method
methods	Method
.	O

Burgos	O
-	O
Artizzu	O
et	O
al	O
.	O

developed	O
a	O
Robust	Method
Cascaded	Method
Pose	Method
Regression	Method
(	O
RCPR	Method
)	O
method	O
to	O
detect	O
occlusions	O
explicitly	O
,	O
and	O
uses	O
shape	O
-	O
indexed	O
features	O
to	O
regress	O
the	O
shape	O
increment	O
.	O

Yu	O
et	O
al	O
.	O

utilizes	O
a	O
Bayesian	Method
model	Method
to	O
merge	O
the	O
estimation	Task
results	O
from	O
multiple	O
regressors	O
,	O
in	O
which	O
each	O
regressor	O
is	O
trained	O
to	O
localize	O
facial	O
landmarks	O
with	O
a	O
specific	O
pre	O
-	O
defined	O
facial	O
part	O
being	O
occluded	O
.	O

Wu	O
et	O
al	O
.	O

proposed	O
a	O
Robust	Method
Facial	Method
Landmark	Method
Detection	Method
(	O
RFLD	Method
)	O
method	O
,	O
which	O
uses	O
a	O
robust	Method
cascaded	Method
regressor	Method
to	O
handle	O
complex	O
occlusions	O
and	O
large	O
head	O
poses	O
.	O

To	O
improve	O
the	O
performance	O
of	O
occlusion	Task
estimation	Task
,	O
landmark	O
visibility	O
probabilities	O
are	O
estimated	O
with	O
an	O
explicit	O
occlusion	O
constraint	O
.	O

Different	O
from	O
these	O
methods	O
,	O
our	O
method	O
is	O
not	O
based	O
on	O
3D	Method
models	Method
and	O
does	O
not	O
process	O
occlusions	O
explicitly	O
.	O

subsection	O
:	O
Face	Task
Alignment	Task
via	O
Deep	Method
Learning	Method
Deep	Method
learning	Method
methods	Method
can	O
be	O
divided	O
into	O
two	O
classes	O
:	O
single	Method
network	Method
based	Method
and	O
multiple	Method
networks	Method
based	O
.	O

Sun	O
et	O
al	O
.	O

estimated	O
the	O
locations	O
of	O
facial	O
landmarks	O
using	O
Cascaded	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
Cascaded	Method
CNN	Method
)	O
,	O
in	O
which	O
each	O
level	O
computes	O
averaged	O
estimated	O
shape	O
and	O
the	O
shape	O
is	O
refined	O
level	O
by	O
level	O
.	O

Zhou	O
et	O
al	O
.	O

used	O
multi	Method
-	Method
level	Method
deep	Method
networks	Method
to	O
detect	O
facial	O
landmarks	O
from	O
coarse	O
to	O
fine	O
.	O

Similarly	O
,	O
Zhang	O
et	O
al	O
.	O

proposed	O
Coarse	Method
-	Method
to	Method
-	Method
Fine	Method
Auto	Method
-	Method
encoder	Method
Networks	Method
(	O
CFAN	Method
)	Method
.	O

These	O
methods	O
all	O
use	O
multi	Method
-	Method
stage	Method
deep	Method
networks	Method
to	O
localize	O
landmarks	O
in	O
a	O
coarse	O
-	O
to	O
-	O
fine	O
manner	O
.	O

Instead	O
of	O
using	O
cascaded	Method
networks	Method
,	O
Honari	O
et	O
al	O
.	O

proposed	O
Recombinator	Method
Networks	Method
(	O
RecNet	Method
)	O
for	O
learning	O
coarse	Task
-	Task
to	Task
-	Task
fine	Task
feature	Task
aggregation	Task
with	O
multi	O
-	O
scale	O
input	O
maps	O
,	O
where	O
each	O
branch	O
extracts	O
features	O
based	O
on	O
current	O
maps	O
and	O
the	O
feature	O
maps	O
of	O
coarser	O
branches	O
.	O

A	O
few	O
methods	O
employ	O
a	O
single	O
network	O
to	O
solve	O
the	O
face	Task
alignment	Task
problem	Task
.	O

Shao	O
et	O
al	O
.	O

proposed	O
a	O
Coarse	Method
-	Method
to	Method
-	Method
Fine	Method
Training	Method
(	O
CFT	Method
)	O
method	O
to	O
learn	O
the	O
mapping	O
from	O
input	O
face	O
patches	O
to	O
estimated	O
shapes	O
,	O
which	O
searches	O
the	O
solutions	O
smoothly	O
by	O
adjusting	O
the	O
relative	O
weights	O
between	O
principal	O
landmarks	O
and	O
elaborate	O
landmarks	O
.	O

Zhang	O
et	O
al	O
.	O

used	O
the	O
TCDCN	Method
with	O
auxiliary	Method
facial	Method
attribute	Method
recognition	Method
to	O
predict	O
correlative	O
facial	O
properties	O
like	O
expression	O
and	O
pose	O
,	O
which	O
improves	O
the	O
performance	O
of	O
face	Task
alignment	Task
.	O

Xiao	O
et	O
al	O
.	O

proposed	O
a	O
Recurrent	Method
Attentive	Method
-	Method
Refinement	Method
(	Method
RAR	Method
)	Method
network	Method
for	O
face	Task
alignment	Task
under	O
unconstrained	O
conditions	O
,	O
where	O
shape	O
-	O
indexed	O
deep	O
features	O
and	O
temporal	O
information	O
are	O
taken	O
as	O
inputs	O
and	O
shape	O
predictions	O
are	O
recurrently	O
revised	O
.	O

Compared	O
to	O
these	O
methods	O
,	O
our	O
method	O
uses	O
only	O
one	O
network	O
and	O
is	O
independent	O
of	O
additional	O
facial	O
attributes	O
.	O

section	O
:	O
Multi	Method
-	Method
Center	Method
Learning	Method
for	O
Face	Task
Alignment	Task
subsection	O
:	O
Network	Method
Architecture	Method
The	O
architecture	O
of	O
our	O
network	O
MCL	Method
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

MCL	Method
contains	O
three	O
max	Method
-	Method
pooling	Method
layers	Method
,	O
each	O
of	O
which	O
follows	O
a	O
stack	O
of	O
two	O
convolutional	Method
layers	Method
proposed	O
by	O
VGGNet	Method
.	O

In	O
the	O
fourth	O
stack	O
of	O
convolutional	O
layers	O
,	O
we	O
use	O
a	O
convolutional	Method
layer	Method
with	O
feature	O
maps	O
above	O
two	O
convolutional	Method
layers	Method
.	O

We	O
perform	O
Batch	Method
Normalization	Method
(	O
BN	Method
)	O
and	O
Rectified	Method
Linear	Method
Unit	Method
(	O
ReLU	Method
)	O
after	O
each	O
convolution	Method
to	O
accelerate	O
the	O
convergence	O
of	O
our	O
network	O
.	O

Most	O
of	O
the	O
existing	O
deep	Method
learning	Method
methods	Method
such	O
as	O
TCDCN	Method
use	O
the	O
fully	Method
-	Method
connected	Method
layer	Method
to	O
extract	O
features	O
,	O
which	O
is	O
apt	O
to	O
overfit	O
and	O
hamper	O
the	O
generalization	Metric
ability	Metric
of	O
the	O
network	O
.	O

To	O
sidestep	O
these	O
problems	O
,	O
we	O
operate	O
Global	Method
Average	Method
Pooling	Method
on	O
the	O
last	O
convolutional	Method
layer	Method
to	O
extract	O
a	O
high	Method
-	Method
level	Method
feature	Method
representation	Method
,	O
which	O
computes	O
the	O
average	O
of	O
each	O
feature	O
map	O
.	O

With	O
this	O
improvement	O
,	O
our	O
MCL	Method
acquires	O
a	O
higher	O
representation	O
power	O
with	O
fewer	O
parameters	O
.	O

Face	Task
alignment	Task
can	O
be	O
regarded	O
as	O
a	O
nonlinear	Task
regression	Task
problem	Task
,	O
which	O
transforms	O
appearance	O
to	O
shape	O
.	O

A	O
transformation	O
is	O
used	O
for	O
modeling	O
this	O
highly	O
nonlinear	O
function	O
,	O
which	O
extracts	O
the	O
feature	O
from	O
the	O
input	O
face	O
image	O
,	O
formulated	O
as	O
where	O
,	O
corresponds	O
to	O
the	O
bias	O
,	O
and	O
is	O
a	O
composite	O
function	O
of	O
operations	O
including	O
convolution	Method
,	O
BN	Method
,	O
ReLU	Method
,	O
and	O
pooling	Method
.	O

Traditionally	O
,	O
only	O
one	O
shape	Method
prediction	Method
layer	Method
is	O
used	O
,	O
which	O
limits	O
the	O
performance	O
.	O

In	O
contrast	O
,	O
our	O
MCL	Method
uses	O
multiple	Method
shape	Method
prediction	Method
layers	Method
,	O
each	O
of	O
which	O
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
landmarks	O
.	O

The	O
first	O
several	O
layers	O
are	O
shared	O
by	O
multiple	Method
shape	Method
prediction	Method
layers	Method
,	O
which	O
are	O
called	O
shared	O
layers	O
forming	O
the	O
composite	O
function	O
.	O

For	O
the	O
-	O
th	O
shape	O
prediction	O
layer	O
,	O
,	O
a	O
weight	O
matrix	O
is	O
used	O
to	O
connect	O
the	O
feature	O
,	O
where	O
and	O
are	O
the	O
number	O
of	O
shape	O
prediction	O
layers	O
and	O
landmarks	O
,	O
respectively	O
.	O

The	O
reason	O
why	O
we	O
train	O
each	O
shape	Method
prediction	Method
layer	Method
to	O
predict	O
landmarks	O
instead	O
of	O
one	O
cluster	O
of	O
landmarks	O
is	O
that	O
different	O
facial	O
parts	O
have	O
correlations	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

To	O
decrease	O
the	O
model	Metric
complexity	Metric
,	O
we	O
use	O
a	O
model	Method
assembling	Method
function	Method
to	O
integrate	O
multiple	Method
shape	Method
prediction	Method
layers	Method
into	O
one	O
shape	Method
prediction	Method
layer	Method
,	O
which	O
is	O
formulated	O
as	O
where	O
is	O
the	O
assembled	O
weight	O
matrix	O
.	O

Specifically	O
,	O
,	O
,	O
,	O
,	O
where	O
is	O
the	O
-	O
th	O
cluster	O
of	O
indexes	O
of	O
landmarks	O
.	O

The	O
final	O
prediction	Task
is	O
defined	O
as	O
where	O
and	O
denote	O
the	O
predicted	O
x	O
-	O
coordinate	O
and	O
y	O
-	O
coordinate	O
of	O
the	O
-	O
th	O
landmark	O
respectively	O
.	O

Compared	O
to	O
other	O
typical	O
convolutional	Method
networks	Method
like	O
VGGNet	Method
,	O
GoogLe	Method
-	Method
Net	Method
,	O
and	O
ResNet	Method
,	O
our	O
network	O
MCL	Method
is	O
substantially	O
smaller	O
and	O
shallower	O
.	O

We	O
believe	O
that	O
such	O
a	O
concise	Method
structure	Method
is	O
efficient	O
for	O
estimating	Task
the	Task
locations	Task
of	Task
facial	Task
landmarks	Task
.	O

Firstly	O
,	O
face	Task
alignment	Task
aims	O
to	O
regress	O
coordinates	O
of	O
fewer	O
than	O
facial	O
landmarks	O
generally	O
,	O
which	O
demands	O
much	O
lower	O
model	Metric
complexity	Metric
than	O
visual	Task
recognition	Task
problems	Task
with	O
more	O
than	O
classes	O
.	O

Secondly	O
,	O
a	O
very	O
deep	Method
network	Method
may	O
fail	O
to	O
work	O
well	O
for	O
landmark	Task
detection	Task
owing	O
to	O
the	O
reduction	O
of	O
spatial	O
information	O
layer	O
by	O
layer	O
.	O

Other	O
visual	Task
localization	Task
tasks	Task
,	O
like	O
face	Task
detection	Task
,	O
usually	O
use	O
multiple	O
cascaded	Method
shallow	Method
networks	Method
rather	O
than	O
a	O
single	O
very	Method
deep	Method
network	Method
.	O

Finally	O
,	O
common	O
face	Task
alignment	Task
benchmarks	Task
only	O
contain	O
thousands	O
of	O
training	O
images	O
.	O

A	O
simple	O
network	O
is	O
not	O
easy	O
to	O
overfit	O
given	O
a	O
small	O
amount	O
of	O
raw	O
training	O
data	O
.	O

[	O
!	O

htb	O
]	O
Multi	Method
-	Method
Center	Method
Learning	Method
Algorithm	O
.	O

[	O
1	O
]	O
A	O
network	O
MCL	Method
,	O
,	O
,	O
initialized	O
.	O

.	O

Pre	O
-	O
train	O
shared	Method
layers	Method
and	O
one	O
shape	Method
prediction	Method
layer	Method
until	O
convergence	O
;	O
Fix	O
the	O
parameters	O
of	O
the	O
first	O
six	O
convolutional	Method
layers	Method
and	O
fine	O
-	O
tune	O
subsequent	O
layers	O
until	O
convergence	O
;	O
Fine	O
-	O
tune	O
all	O
the	O
layers	O
until	O
convergence	O
;	O
to	O
Fix	O
and	O
fine	O
-	O
tune	O
the	O
-	O
th	O
shape	Method
prediction	Method
layer	Method
until	O
convergence	O
;	O
;	O
Return	O
.	O

subsection	O
:	O
Learning	Method
Algorithm	Method
The	O
overview	O
of	O
our	O
learning	Method
algorithm	Method
is	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

and	O
are	O
the	O
training	O
set	O
and	O
the	O
validation	O
set	O
respectively	O
.	O

is	O
the	O
set	O
of	O
parameters	O
including	O
weights	O
and	O
biases	O
of	O
our	O
network	O
MCL	Method
,	O
which	O
is	O
updated	O
using	O
Mini	Method
-	Method
Batch	Method
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
at	O
each	O
iteration	O
.	O

The	O
face	Task
alignment	Task
loss	Task
is	O
defined	O
as	O
where	O
is	O
the	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
,	O
and	O
denote	O
the	O
ground	O
-	O
truth	O
x	O
-	O
coordinate	O
and	O
y	O
-	O
coordinate	O
of	O
the	O
-	O
th	O
landmark	O
respectively	O
,	O
and	O
is	O
the	O
ground	O
truth	O
inter	O
-	O
ocular	O
distance	O
between	O
the	O
eye	O
centers	O
.	O

Inter	Method
-	Method
ocular	Method
distance	Method
normalization	Method
provides	O
fair	O
comparisons	O
among	O
faces	O
with	O
different	O
size	O
,	O
and	O
reduces	O
the	O
magnitude	O
of	O
loss	Metric
to	O
speed	O
up	O
the	O
learning	Method
process	Method
.	O

During	O
training	Task
,	O
a	O
too	O
high	O
learning	Metric
rate	Metric
may	O
cause	O
the	O
missing	O
of	O
optimum	O
so	O
far	O
as	O
to	O
the	O
divergence	O
of	O
network	O
,	O
and	O
a	O
too	O
low	O
learning	Metric
rate	Metric
may	O
lead	O
to	O
falling	O
into	O
a	O
local	O
optimum	O
.	O

We	O
employ	O
a	O
low	O
initial	Metric
learning	Metric
rate	Metric
to	O
avoid	O
the	O
divergence	O
,	O
and	O
increase	O
the	O
learning	Metric
rate	Metric
when	O
the	O
loss	Metric
is	O
reduced	O
significantly	O
and	O
continue	O
the	O
training	O
procedure	O
.	O

subsubsection	Method
:	O
Pre	Method
-	Method
Training	Method
and	O
Weighting	Method
Fine	Method
-	Method
Tuning	Method
In	O
Step	O
[	O
reference	O
]	O
,	O
a	O
basic	Method
model	Method
(	O
BM	Method
)	O
with	O
one	O
shape	Method
prediction	Method
layer	Method
is	O
pre	O
-	O
trained	O
to	O
learn	O
a	O
good	O
initial	O
solution	O
.	O

In	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
for	O
all	O
.	O

The	O
average	Metric
alignment	Metric
error	Metric
of	O
each	O
landmark	O
of	O
BM	O
on	O
are	O
respectively	O
,	O
which	O
are	O
averaged	O
over	O
all	O
the	O
images	O
.	O

The	O
landmarks	O
with	O
larger	O
errors	O
than	O
remaining	O
landmarks	O
are	O
treated	O
as	O
challenging	O
landmarks	O
.	O

In	O
Steps	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
the	O
detection	Task
of	Task
challenging	Task
landmarks	Task
by	O
assigning	O
them	O
larger	O
weights	O
.	O

The	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
is	O
proportional	O
to	O
its	O
alignment	Metric
error	Metric
as	O
Instead	O
of	O
fine	O
-	O
tuning	O
all	O
the	O
layers	O
from	O
BM	Method
directly	O
,	O
we	O
use	O
two	O
steps	O
to	O
search	O
the	O
solution	O
smoothly	O
.	O

Step	O
[	O
reference	O
]	O
searches	O
the	O
solution	O
without	O
deviating	O
from	O
BM	O
overly	O
.	O

Step	O
[	O
reference	O
]	O
searches	O
the	O
solution	O
within	O
a	O
larger	O
range	O
on	O
the	O
basis	O
of	O
the	O
previous	O
step	O
.	O

This	O
stage	O
is	O
named	O
weighting	Task
fine	Task
-	Task
tuning	Task
,	O
which	O
learns	O
a	O
weighting	Method
model	Method
(	O
WM	Method
)	O
with	O
higher	O
localization	Metric
accuracy	Metric
of	O
challenging	O
landmarks	O
.	O

subsubsection	O
:	O
Multi	Method
-	Method
Center	Method
Fine	Method
-	Method
Tuning	Method
and	O
Model	Method
Assembling	Method
The	O
face	O
is	O
partitioned	O
into	O
seven	O
parts	O
according	O
to	O
its	O
semantic	O
structure	O
:	O
left	O
eye	O
,	O
right	O
eye	O
,	O
nose	O
,	O
mouth	O
,	O
left	O
contour	O
,	O
chin	O
,	O
and	O
right	O
contour	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
different	O
labeling	O
patterns	O
of	O
,	O
,	O
and	O
facial	O
landmarks	O
are	O
partitioned	O
into	O
,	O
,	O
and	O
clusters	O
respectively	O
.	O

For	O
the	O
-	Task
th	Task
shape	Task
prediction	Task
layer	Task
,	O
the	O
-	O
th	O
cluster	O
of	O
landmarks	O
are	O
treated	O
as	O
the	O
optimized	O
center	O
,	O
and	O
the	O
set	O
of	O
indexes	O
of	O
remaining	O
landmarks	O
is	O
denoted	O
as	O
.	O

From	O
Steps	O
[	O
reference	O
]	O
to	O
[	O
reference	O
]	O
,	O
the	O
parameters	O
of	O
shared	O
layers	O
are	O
fixed	O
,	O
and	O
each	O
shape	Method
prediction	Method
layer	Method
is	O
initialized	O
with	O
the	O
parameters	O
of	O
the	O
shape	Method
prediction	Method
layer	Method
of	O
WM	Method
.	O

When	O
fine	O
-	O
tuning	O
the	O
-	Method
th	Method
shape	Method
prediction	Method
layer	Method
,	O
the	O
weights	O
of	O
landmarks	O
in	O
and	O
are	O
defined	O
as	O
where	O
is	O
a	O
coefficient	O
to	O
make	O
the	O
-	Method
th	Method
shape	Method
prediction	Method
layer	Method
emphasize	O
on	O
the	O
detection	O
of	O
the	O
-	O
th	O
cluster	O
of	O
landmarks	O
.	O

The	O
constraint	O
between	O
and	O
is	O
formulated	O
as	O
where	O
refers	O
to	O
the	O
number	O
of	O
elements	O
in	O
a	O
cluster	O
.	O

With	O
Eqs	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
the	O
solved	O
weights	O
are	O
formulated	O
as	O
The	O
average	Metric
alignment	Metric
error	Metric
of	O
each	O
landmark	O
of	O
WM	Method
on	O
are	O
respectively	O
.	O

Similar	O
to	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
the	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
is	O
Although	O
the	O
landmarks	O
in	O
are	O
mainly	O
optimized	O
,	O
remaining	O
landmarks	O
are	O
still	O
considered	O
with	O
very	O
small	O
weights	O
rather	O
than	O
zero	O
.	O

This	O
is	O
beneficial	O
for	O
utilizing	O
implicit	O
structural	O
correlations	O
of	O
different	O
facial	O
parts	O
and	O
searching	O
the	O
solutions	O
smoothly	O
.	O

This	O
stage	O
is	O
called	O
multi	Method
-	Method
center	Method
fine	Method
-	Method
tuning	Method
which	O
learns	O
multiple	Method
shape	Method
prediction	Method
layers	Method
.	O

In	O
Step	O
[	O
reference	O
]	O
,	O
multiple	Method
shape	Method
prediction	Method
layers	Method
are	O
assembled	O
into	O
one	O
shape	Method
prediction	Method
layer	Method
by	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

With	O
this	O
model	Method
assembling	Method
stage	Method
,	O
our	O
method	O
learns	O
an	O
assembling	Method
model	Method
(	O
AM	Method
)	O
.	O

There	O
is	O
no	O
increase	O
of	O
model	Metric
complexity	Metric
in	O
the	O
assembling	Task
process	Task
,	O
so	O
AM	Method
has	O
a	O
low	O
computational	Metric
cost	Metric
.	O

It	O
improves	O
the	O
detection	Metric
precision	Metric
of	O
each	O
facial	O
landmark	O
by	O
integrating	O
the	O
advantage	O
of	O
each	O
shape	Method
prediction	Method
layer	Method
.	O

subsubsection	O
:	O
Analysis	O
of	O
Model	Method
Learning	Method
To	O
investigate	O
the	O
influence	O
from	O
the	O
weights	O
of	O
landmarks	O
on	O
learning	Task
procedure	Task
,	O
we	O
calculate	O
the	O
derivative	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
with	O
respect	O
to	O
:	O
where	O
,	O
.	O

During	O
the	O
learning	Task
process	Task
,	O
the	O
assembled	O
weight	O
matrix	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
is	O
updated	O
by	O
SGD	Method
.	O

Specifically	O
,	O
.	O

In	O
summary	O
,	O
is	O
updated	O
as	O
where	O
is	O
the	O
learning	Metric
rate	Metric
.	O

If	O
the	O
-	O
th	O
landmark	O
is	O
given	O
a	O
larger	O
weight	O
,	O
its	O
corresponding	O
parameters	O
will	O
be	O
updated	O
with	O
a	O
larger	O
step	O
towards	O
the	O
optimal	O
solution	O
.	O

Therefore	O
,	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
ensures	O
that	O
the	O
landmarks	O
with	O
larger	O
weights	O
are	O
mainly	O
optimized	O
.	O

Our	O
method	O
first	O
uses	O
the	O
weighting	Method
fine	Method
-	Method
tuning	Method
stage	Method
to	O
optimize	O
challenging	O
landmarks	O
,	O
and	O
further	O
uses	O
the	O
multi	Method
-	Method
center	Method
fine	Method
-	Method
tuning	Method
stage	Method
to	O
optimize	O
each	O
cluster	O
of	O
landmarks	O
respectively	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Datasets	O
and	O
Settings	O
subsubsection	O
:	O
Datasets	O
There	O
are	O
three	O
challenging	O
benchmarks	O
AFLW	Material
,	O
COFW	Method
,	O
and	O
IBUG	Method
,	O
which	O
are	O
used	O
for	O
evaluating	Task
face	Task
alignment	Task
with	O
severe	O
occlusion	O
and	O
large	O
variations	O
of	O
pose	O
,	O
expression	O
,	O
and	O
illumination	O
.	O

The	O
provided	O
face	O
bounding	O
boxes	O
are	O
employed	O
to	O
crop	O
face	O
patches	O
during	O
testing	O
.	O

AFLW	Material
[	O
]	O
contains	O
faces	O
under	O
real	O
-	O
world	O
conditions	O
gathered	O
from	O
Flickr	Material
.	O

Compared	O
with	O
other	O
datasets	O
like	O
MUCT	Material
and	O
LFPW	Material
,	O
AFLW	Material
exhibits	O
larger	O
pose	O
variations	O
and	O
extreme	O
partial	O
occlusions	O
.	O

Following	O
the	O
settings	O
of	O
,	O
images	O
are	O
used	O
for	O
testing	O
,	O
and	O
images	O
annotated	O
with	O
landmarks	O
are	O
used	O
for	O
training	Task
,	O
which	O
includes	O
LFW	Material
images	Material
and	O
web	O
images	O
.	O

COFW	O
[	O
]	O
is	O
an	O
occluded	O
face	O
dataset	O
in	O
the	O
wild	O
,	O
in	O
which	O
the	O
faces	O
are	O
designed	O
with	O
severe	O
occlusions	O
using	O
accessories	O
and	O
interactions	O
with	O
objects	O
.	O

It	O
contains	O
images	O
annotated	O
with	O
landmarks	O
.	O

The	O
training	O
set	O
includes	O
LFPW	Material
faces	O
and	O
COFW	Material
faces	Material
,	O
and	O
the	O
testing	O
set	O
includes	O
remaining	O
COFW	O
faces	O
.	O

IBUG	Method
[	O
]	O
contains	O
testing	O
images	O
which	O
present	O
large	O
variations	O
in	O
pose	O
,	O
expression	O
,	O
illumination	O
,	O
and	O
occlusion	O
.	O

The	O
training	O
set	O
consists	O
of	O
AFW	Method
,	O
the	O
training	O
set	O
of	O
LFPW	Material
,	O
and	O
the	O
training	O
set	O
of	O
Helen	Method
,	O
which	O
are	O
from	O
300	O
-	O
W	O
with	O
images	O
labeled	O
with	O
landmarks	O
.	O

subsubsection	O
:	O
Implementation	O
Details	O
We	O
enhance	O
the	O
diversity	O
of	O
raw	O
training	O
data	O
on	O
account	O
of	O
their	O
limited	O
variation	O
patterns	O
,	O
using	O
five	O
steps	O
:	O
rotation	O
,	O
uniform	O
scaling	O
,	O
translation	O
,	O
horizontal	O
flip	O
,	O
and	O
JPEG	Method
compression	Method
.	O

In	O
particular	O
,	O
for	O
each	O
training	O
face	O
,	O
we	O
firstly	O
perform	O
multiple	O
rotations	O
,	O
and	O
attain	O
a	O
tight	O
face	O
bounding	O
box	O
covering	O
the	O
ground	O
truth	O
locations	O
of	O
landmarks	O
of	O
each	O
rotated	O
result	O
respectively	O
.	O

Uniform	O
scaling	O
and	O
translation	O
with	O
different	O
extents	O
on	O
face	O
bounding	O
boxes	O
are	O
further	O
conducted	O
,	O
in	O
which	O
each	O
newly	O
generated	O
face	O
bounding	O
box	O
is	O
used	O
to	O
crop	O
the	O
face	O
.	O

Finally	O
training	O
samples	O
are	O
augmented	O
through	O
horizontal	Method
flip	Method
and	O
JPEG	Method
compression	Method
.	O

It	O
is	O
beneficial	O
for	O
avoiding	O
overfitting	O
and	O
improving	O
the	O
robustness	Metric
of	O
learned	Method
models	Method
by	O
covering	O
various	O
patterns	O
.	O

We	O
train	O
our	O
MCL	Method
using	O
an	O
open	Method
source	Method
deep	Method
learning	Method
framework	Method
Caffe	Method
.	O

The	O
input	O
face	O
patch	O
is	O
a	O
grayscale	O
image	O
,	O
and	O
each	O
pixel	O
value	O
is	O
normalized	O
to	O
by	O
subtracting	O
and	O
multiplying	O
.	O

A	O
more	O
complex	O
model	O
is	O
needed	O
for	O
a	O
labeling	O
pattern	O
with	O
more	O
facial	O
landmarks	O
,	O
so	O
is	O
set	O
to	O
be	O
for	O
facial	O
landmarks	O
.	O

The	O
type	O
of	O
solver	O
is	O
SGD	Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
,	O
a	O
momentum	O
of	O
,	O
and	O
a	O
weight	O
decay	O
of	O
.	O

The	O
maximum	O
learning	O
iterations	O
of	O
pre	O
-	O
training	O
and	O
each	O
fine	Method
-	Method
tuning	Method
step	Method
are	O
and	O
respectively	O
,	O
and	O
the	O
initial	O
learning	Metric
rates	Metric
of	O
pre	O
-	O
training	O
and	O
each	O
fine	Method
-	Method
tuning	Method
step	Method
are	O
and	O
respectively	O
.	O

Note	O
that	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
fine	Method
-	Method
tuning	Method
should	O
be	O
low	O
to	O
preserve	O
some	O
representational	O
structures	O
learned	O
in	O
the	O
pre	O
-	O
training	O
stage	O
and	O
avoid	O
missing	O
good	O
intermediate	O
solutions	O
.	O

The	O
learning	Metric
rate	Metric
is	O
multiplied	O
by	O
a	O
factor	O
of	O
at	O
every	O
iterations	O
,	O
and	O
the	O
remaining	O
parameter	O
is	O
set	O
to	O
be	O
.	O

subsubsection	O
:	O
Evaluation	Metric
Metric	Metric
Similar	O
to	O
previous	O
methods	O
,	O
we	O
report	O
the	O
inter	O
-	O
ocular	O
distance	O
normalized	O
mean	Metric
error	Metric
,	O
and	O
treat	O
the	O
mean	Metric
error	Metric
larger	O
than	O
as	O
a	O
failure	O
.	O

To	O
conduct	O
a	O
more	O
comprehensive	O
comparison	O
,	O
the	O
cumulative	Metric
errors	Metric
distribution	Metric
(	Metric
CED	Metric
)	Metric
curves	Metric
are	O
plotted	O
.	O

To	O
measure	O
the	O
time	Metric
efficiency	Metric
,	O
the	O
average	Metric
running	Metric
speed	Metric
(	O
Frame	Metric
per	Metric
Second	Metric
,	O
FPS	Metric
)	O
on	O
a	O
single	O
core	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
is	O
also	O
reported	O
.	O

A	O
single	O
image	O
is	O
fed	O
into	O
the	O
model	O
at	O
a	O
time	O
when	O
computing	O
the	O
speed	O
.	O

In	O
other	O
words	O
,	O
we	O
evaluate	O
methods	O
on	O
four	O
popular	O
metrics	Metric
:	O
mean	Metric
error	Metric
(	O
%	O
)	O
,	O
failure	Metric
rate	Metric
(	O
%	O
)	O
,	O
CED	Metric
curves	Metric
,	O
and	O
average	Metric
running	Metric
speed	Metric
.	O

In	O
the	O
next	O
sections	O
,	O
%	O
in	O
all	O
the	O
results	O
are	O
omitted	O
for	O
simplicity	O
.	O

subsection	O
:	O
Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Methods	O
We	O
compare	O
our	O
work	O
MCL	Method
against	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
including	O
ESR	Method
,	O
SDM	Method
,	O
Cascaded	Method
CNN	Method
,	O
RCPR	Method
,	O
CFAN	Method
,	O
LBF	Method
,	O
cGPRT	Method
,	O
CFSS	Method
,	O
TCDCN	Method
,	O
ALR	Method
,	O
CFT	Method
,	O
RFLD	Method
,	O
RecNet	Method
,	O
RAR	Method
,	O
and	O
FLD	Method
+	Method
PDE	Method
.	O

All	O
the	O
methods	O
are	O
evaluated	O
on	O
testing	O
images	O
using	O
the	O
face	O
bounding	O
boxes	O
provided	O
by	O
benchmarks	O
.	O

In	O
addition	O
to	O
given	O
training	O
images	O
,	O
TCDCN	Method
uses	O
outside	O
training	O
data	O
labeled	O
with	O
facial	O
attributes	O
.	O

RAR	Method
augments	O
training	O
images	O
with	O
occlusions	O
incurred	O
by	O
outside	O
natural	O
objects	O
like	O
sunglasses	O
,	O
phones	O
,	O
and	O
hands	O
.	O

FLD	Method
+	Method
PDE	Method
performs	O
facial	Task
landmark	Task
detection	Task
,	O
pose	Task
and	Task
deformation	Task
estimation	Task
simultaneously	O
,	O
in	O
which	O
the	O
training	O
data	O
of	O
pose	Task
and	Task
deformation	Task
estimation	Task
are	O
used	O
.	O

Other	O
methods	O
including	O
our	O
MCL	Method
only	O
utilize	O
given	O
training	O
images	O
from	O
the	O
benchmarks	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
our	O
method	O
and	O
previous	O
works	O
on	O
three	O
benchmarks	O
.	O

Our	O
method	O
MCL	Method
outperforms	O
most	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
especially	O
on	O
AFLW	Material
dataset	Material
where	O
a	O
relative	Metric
error	Metric
reduction	Metric
of	O
is	O
achieved	O
compared	O
to	O
RecNet	Method
.	O

Cascaded	Method
CNN	Method
estimates	O
the	O
location	O
of	O
each	O
landmark	O
separately	O
in	O
the	O
second	O
and	O
third	O
level	O
,	O
and	O
every	O
two	O
networks	O
are	O
used	O
to	O
detect	O
one	O
landmark	O
.	O

It	O
is	O
difficult	O
to	O
be	O
extended	O
to	O
dense	O
landmarks	O
owing	O
to	O
the	O
explosion	O
of	O
the	O
number	O
of	O
networks	O
.	O

TCDCN	Method
relies	O
on	O
outside	O
training	O
data	O
for	O
auxiliary	Task
facial	Task
attribute	Task
recognition	Task
,	O
which	O
limits	O
the	O
universality	O
.	O

It	O
can	O
be	O
seen	O
that	O
MCL	Method
outperforms	O
Cascaded	Method
CNN	Method
and	O
TCDCN	Method
on	O
all	O
the	O
benchmarks	O
.	O

Moreover	O
,	O
MCL	Method
is	O
robust	O
to	O
occlusions	O
with	O
the	O
performance	O
on	O
par	O
with	O
RFLD	Method
,	O
benefiting	O
from	O
utilizing	O
semantical	O
correlations	O
among	O
different	O
landmarks	O
.	O

RecNet	Method
and	O
RAR	Method
show	O
significant	O
results	O
,	O
but	O
their	O
models	O
are	O
very	O
complex	O
with	O
high	O
computational	Metric
costs	Metric
.	O

We	O
compare	O
with	O
other	O
methods	O
on	O
several	O
challenging	O
images	O
from	O
AFLW	Material
and	O
COFW	Material
respectively	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Our	O
method	O
MCL	Method
indicates	O
higher	O
accuracy	Metric
in	O
the	O
details	O
than	O
previous	O
works	O
.	O

More	O
examples	O
on	O
challenging	O
IBUG	Task
are	O
presented	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

MCL	Method
demonstrates	O
a	O
superior	O
capability	O
of	O
handling	O
severe	O
occlusions	O
and	O
complex	O
variations	O
of	O
pose	O
,	O
expression	O
,	O
illumination	O
.	O

The	O
CED	Metric
curves	Metric
of	O
MCL	Method
and	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

It	O
is	O
observed	O
that	O
MCL	Method
achieves	O
competitive	O
performance	O
on	O
all	O
three	O
benchmarks	O
.	O

The	O
average	Metric
running	Metric
speed	Metric
of	O
deep	Method
learning	Method
methods	Method
for	O
detecting	Task
facial	Task
landmarks	Task
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Except	O
for	O
the	O
methods	O
tested	O
on	O
the	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
,	O
other	O
methods	O
are	O
reported	O
with	O
the	O
results	O
in	O
the	O
original	O
papers	O
.	O

Since	O
CFAN	Method
utilizes	O
multiple	O
networks	O
,	O
it	O
costs	O
more	O
running	Metric
time	Metric
.	O

RAR	Method
achieves	O
only	O
FPS	O
on	O
a	O
Titan	Method
-	Method
Z	Method
GPU	Method
,	O
which	O
can	O
not	O
be	O
applied	O
to	O
practical	O
scenarios	O
.	O

Both	O
TCDCN	Method
and	O
our	O
method	O
MCL	Method
are	O
based	O
on	O
only	O
one	O
network	O
,	O
so	O
they	O
show	O
higher	O
speed	O
.	O

Our	O
method	O
only	O
takes	O
ms	O
per	O
face	O
on	O
a	O
single	O
core	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
.	O

This	O
profits	O
from	O
low	O
model	Metric
complexity	Metric
and	O
computational	Metric
costs	Metric
of	O
our	O
network	O
.	O

It	O
can	O
be	O
concluded	O
that	O
our	O
method	O
is	O
able	O
to	O
be	O
extended	O
to	O
real	Task
-	Task
time	Task
facial	Task
landmark	Task
tracking	Task
in	O
unconstrained	Task
environments	Task
.	O

subsection	O
:	O
Ablation	Task
Study	Task
subsubsection	O
:	O
Global	Method
Average	Method
Pooling	Method
vs.	O
Full	Method
Connection	Method
Based	O
on	O
the	O
previous	O
version	O
of	O
our	O
work	O
,	O
the	O
last	O
max	Method
-	Method
pooling	Method
layer	Method
and	O
the	O
-	Method
dimensional	Method
fully	Method
-	Method
connected	Method
layer	Method
are	O
replaced	O
with	O
a	O
convolutional	Method
layer	Method
and	O
a	O
Global	Method
Average	Method
Pooling	Method
layer	Method
.	O

The	O
results	O
of	O
the	O
mean	Metric
error	Metric
of	O
BM	Method
and	O
the	O
previous	O
version	O
(	O
pre	Method
-	Method
BM	Method
)	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
BM	Method
performs	O
better	O
on	O
IBUG	Method
and	O
COFW	Method
but	O
worse	O
on	O
AFLW	Material
than	O
pre	O
-	O
BM	Method
.	O

It	O
demonstrates	O
that	O
Global	Method
Average	Method
Pooling	Method
is	O
more	O
advantageous	O
for	O
more	O
complex	O
problems	O
with	O
more	O
facial	O
landmarks	O
.	O

There	O
are	O
higher	O
requirements	O
for	O
learned	O
features	O
when	O
localizing	O
more	O
facial	O
landmarks	O
.	O

For	O
simple	O
problems	O
especially	O
for	O
localizing	O
landmarks	O
of	O
AFLW	Material
,	O
a	O
plain	Method
network	Method
with	O
full	Method
connection	Method
is	O
more	O
prone	O
to	O
being	O
trained	O
.	O

The	O
difference	O
between	O
pre	Method
-	Method
BM	Method
and	O
BM	Method
is	O
the	O
structure	O
of	O
learning	O
the	O
feature	O
.	O

The	O
number	O
of	O
parameters	O
for	O
this	O
part	O
of	O
pre	Method
-	Method
BM	Method
and	O
BM	Method
are	O
and	O
respectively	O
,	O
where	O
the	O
three	O
terms	O
for	O
BM	Method
correspond	O
to	O
the	O
convolution	O
,	O
the	O
expectation	O
and	O
variance	O
of	O
BN	O
,	O
and	O
the	O
scaling	Method
and	Method
shifting	Method
of	Method
BN	Method
.	O

Therefore	O
,	O
BM	Method
has	O
a	O
stronger	O
feature	O
learning	O
ability	O
with	O
fewer	O
parameters	O
than	O
pre	O
-	O
BM	Method
.	O

subsubsection	O
:	O
Robustness	O
of	O
Weighting	O
To	O
verify	O
the	O
robustness	O
of	O
weighting	Task
,	O
random	O
perturbations	O
are	O
added	O
to	O
the	O
weights	O
of	O
landmarks	O
.	O

In	O
particular	O
,	O
we	O
plus	O
a	O
perturbation	O
to	O
the	O
weight	O
of	O
each	O
of	O
random	O
landmarks	O
and	O
minus	O
to	O
the	O
weight	O
of	O
each	O
of	O
remaining	O
landmarks	O
,	O
where	O
refers	O
to	O
rounding	O
down	O
to	O
the	O
nearest	O
integer	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
variations	O
of	O
mean	Metric
error	Metric
of	Metric
WM	Metric
with	O
the	O
increase	O
of	O
.	O

When	O
is	O
,	O
WM	Method
can	O
still	O
achieves	O
good	O
performance	O
.	O

Therefore	O
,	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
is	O
robust	O
to	O
random	O
perturbations	O
.	O

Even	O
if	O
different	O
weights	O
are	O
obtained	O
,	O
the	O
results	O
will	O
not	O
be	O
affected	O
as	O
long	O
as	O
the	O
relative	O
sizes	O
of	O
weights	O
are	O
identical	O
.	O

subsubsection	O
:	O
Analysis	Method
of	Method
Shape	Method
Prediction	Method
Layers	Method
Our	O
method	O
learns	O
each	O
shape	Method
prediction	Method
layer	Method
respectively	O
with	O
a	O
certain	O
cluster	O
of	O
landmarks	O
being	O
emphasized	O
.	O

The	O
results	O
of	O
WM	Method
and	O
two	O
shape	Method
prediction	Method
layers	Method
with	O
respect	O
to	O
the	O
left	O
eye	O
and	O
the	O
right	O
eye	O
on	O
IBUG	Material
benchmark	Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Compared	O
to	O
WM	Method
,	O
the	O
left	Method
eye	Method
model	Method
and	O
the	O
right	Method
eye	Method
model	Method
both	O
reduce	O
the	O
alignment	Metric
errors	Metric
of	O
their	O
corresponding	O
clusters	O
.	O

As	O
a	O
result	O
,	O
the	O
assembled	O
AM	Method
can	O
improve	O
the	O
detection	Metric
accuracy	Metric
of	O
landmarks	O
of	O
the	O
left	O
eye	O
and	O
the	O
right	O
eye	O
on	O
the	O
basis	O
of	O
WM	Method
.	O

Note	O
that	O
the	O
two	O
models	O
also	O
improve	O
the	O
localization	Metric
precision	Metric
of	O
other	O
clusters	O
.	O

Taking	O
the	O
left	Method
eye	Method
model	Method
as	O
an	O
example	O
,	O
it	O
additionally	O
reduces	O
the	O
errors	O
of	O
landmarks	O
of	O
right	O
eye	O
,	O
mouth	O
,	O
and	O
chin	O
,	O
which	O
is	O
due	O
to	O
the	O
correlations	O
among	O
different	O
facial	O
parts	O
.	O

Moreover	O
,	O
for	O
the	O
right	O
eye	O
cluster	O
,	O
the	O
right	Method
eye	Method
model	Method
improves	O
the	O
accuracy	Metric
more	O
significantly	O
than	O
the	O
left	Method
eye	Method
model	Method
.	O

It	O
can	O
be	O
concluded	O
that	O
each	O
shape	Method
prediction	Method
layer	Method
emphasizes	O
on	O
the	O
corresponding	O
cluster	O
respectively	O
.	O

subsubsection	O
:	O
Integration	O
of	O
Weighting	Method
Fine	Method
-	Method
Tuning	Method
and	O
Multi	Method
-	Method
Center	Method
Fine	Method
-	Method
Tuning	Method
Here	O
we	O
validate	O
the	O
effectiveness	O
of	O
weighting	Method
fine	Method
-	Method
tuning	Method
by	O
removing	O
the	O
weighting	Method
fine	Method
-	Method
tuning	Method
stage	Method
to	O
learn	O
a	O
Simplified	O
AM	Method
from	O
BM	Method
.	O

Table	O
[	O
reference	O
]	O
presents	O
the	O
results	O
of	O
mean	Metric
error	Metric
of	O
Simplified	O
AM	Method
and	O
AM	Method
respectively	O
on	O
COFW	Method
and	O
IBUG	Method
.	O

Note	O
that	O
Simplified	O
AM	Method
has	O
already	O
acquired	O
good	O
results	O
,	O
which	O
verifies	O
the	O
effectiveness	O
of	O
the	O
multi	Method
-	Method
center	Method
fine	Method
-	Method
tuning	Method
stage	Method
.	O

The	O
accuracy	Metric
of	O
AM	Method
is	O
superior	O
to	O
that	O
of	O
Simplified	O
AM	Method
especially	O
on	O
challenging	O
IBUG	Task
,	O
which	O
is	O
attributed	O
to	O
the	O
integration	O
of	O
two	O
stages	O
.	O

A	O
Weighting	O
Simplified	O
AM	Method
from	O
Simplified	O
AM	Method
using	O
the	O
weighting	Method
fine	Method
-	Method
tuning	Method
stage	Method
is	O
also	O
learned	O
,	O
whose	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
Weighting	O
Simplified	O
AM	Method
improves	O
slightly	O
on	O
COFW	Method
but	O
fails	O
to	O
search	O
a	O
better	O
solution	O
on	O
IBUG	Method
.	O

Therefore	O
,	O
we	O
choose	O
to	O
use	O
the	O
multi	Method
-	Method
center	Method
fine	Method
-	Method
tuning	Method
stage	Method
after	O
the	O
weighting	Method
fine	Method
-	Method
tuning	Method
stage	Method
.	O

subsubsection	O
:	O
Discussion	O
of	O
All	O
Stages	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
results	O
of	O
mean	Metric
error	Metric
and	O
failure	Metric
rate	Metric
of	O
BM	Method
,	O
WM	Method
,	O
and	O
AM	Method
.	O

It	O
can	O
be	O
observed	O
that	O
AM	Method
has	O
higher	O
accuracy	Metric
and	O
stronger	O
robustness	Metric
than	O
BM	Method
and	O
WM	Method
.	O

Fig	O
.	O

[	O
reference	O
]	O
depicts	O
the	O
enhancement	O
from	O
WM	Method
to	O
AM	Method
for	O
several	O
examples	O
of	O
COFW	Task
.	O

The	O
localization	Metric
accuracy	Metric
of	O
facial	O
landmarks	O
from	O
each	O
cluster	O
is	O
improved	O
in	O
the	O
details	O
.	O

It	O
is	O
because	O
each	O
shape	Method
prediction	Method
layer	Method
increases	O
the	O
detection	Metric
precision	Metric
of	O
corresponding	O
cluster	O
respectively	O
.	O

subsection	O
:	O
MCL	Method
for	O
Partially	Task
Occluded	Task
Faces	Task
The	O
correlations	O
among	O
different	O
facial	O
parts	O
are	O
very	O
useful	O
for	O
face	Task
alignment	Task
especially	O
for	O
partially	Task
occluded	Task
faces	Task
.	O

To	O
investigate	O
the	O
influence	O
of	O
occlusions	O
,	O
we	O
directly	O
use	O
trained	O
WM	Method
and	O
AM	Method
without	O
any	O
additional	O
processing	O
for	O
partially	Task
occluded	Task
faces	Task
.	O

Randomly	O
testing	O
faces	O
from	O
COFW	Material
are	O
processed	O
with	O
left	O
eyes	O
being	O
occluded	O
,	O
where	O
the	O
tight	O
bounding	O
box	O
covering	O
landmarks	O
of	O
left	O
eye	O
is	O
filled	O
with	O
gray	O
color	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
mean	Metric
error	Metric
results	O
for	O
the	O
left	O
eye	O
cluster	O
and	O
other	O
clusters	O
of	O
WM	Method
and	O
AM	Method
on	O
COFW	Material
benchmark	Material
,	O
where	O
“	O
with	O
(	O
w	O
/	O
)	O
occlusion	O
(	O
occlu	O
.	O

)	O
”	O
denotes	O
that	O
left	O
eyes	O
of	O
the	O
testing	O
faces	O
are	O
processed	O
with	O
handcrafted	O
occlusions	O
as	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
and	O
“	O
without	O
(	O
w	O
/	O
o	O
)	O
occlu	O
.	O

”	O
denotes	O
that	O
the	O
testing	O
faces	O
are	O
kept	O
unchanged	O
.	O

Note	O
that	O
our	O
method	O
does	O
not	O
process	O
occlusions	O
explicitly	O
,	O
in	O
which	O
the	O
training	O
data	O
is	O
not	O
performed	O
handcrafted	O
occlusions	O
.	O

After	O
processing	O
testing	O
faces	O
with	O
occlusions	O
,	O
the	O
mean	Metric
error	Metric
results	O
of	O
both	O
WM	Method
and	O
AM	Method
increase	O
.	O

Besides	O
the	O
results	O
of	O
landmarks	O
from	O
the	O
left	O
eye	O
cluster	O
,	O
the	O
results	O
of	O
remaining	O
landmarks	O
from	O
other	O
clusters	O
become	O
worse	O
slightly	O
.	O

This	O
is	O
because	O
different	O
facial	O
parts	O
have	O
correlations	O
and	O
the	O
occlusions	O
of	O
the	O
left	O
eye	O
influences	O
results	O
of	O
other	O
facial	O
parts	O
.	O

Note	O
that	O
WM	Method
and	O
AM	Method
still	O
perform	O
well	O
on	O
occluded	O
left	O
eyes	O
with	O
the	O
mean	Metric
error	Metric
of	O
and	O
respectively	O
,	O
due	O
to	O
the	O
following	O
reasons	O
.	O

First	O
,	O
WM	Method
weights	O
each	O
landmark	O
proportional	O
to	O
its	O
alignment	Metric
error	Metric
,	O
which	O
exploits	O
correlations	O
among	O
landmarks	O
.	O

Second	O
,	O
AM	Method
uses	O
an	O
independent	Method
shape	Method
prediction	Method
layer	Method
focusing	O
on	O
a	O
certain	O
cluster	O
of	O
landmarks	O
with	O
small	O
weights	O
,	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
for	O
remaining	O
landmarks	O
,	O
respectively	O
,	O
where	O
correlations	O
among	O
landmarks	O
are	O
further	O
exploited	O
.	O

subsection	O
:	O
Weighting	Task
Fine	Task
-	Task
Tuning	Task
for	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Frameworks	O
Most	O
recently	O
,	O
there	O
are	O
a	O
few	O
well	O
-	O
designed	O
and	O
well	O
-	O
trained	O
deep	Method
learning	Method
frameworks	Method
advancing	O
the	O
performance	O
of	O
face	Task
alignment	Task
,	O
in	O
which	O
DAN	Task
is	O
a	O
typical	O
work	O
.	O

DAN	Method
uses	O
cascaded	Method
deep	Method
neural	Method
networks	Method
to	O
refine	O
the	O
localization	Task
accuracy	Task
of	Task
landmarks	Task
iteratively	O
,	O
where	O
the	O
entire	O
face	O
image	O
and	O
the	O
landmark	O
heatmap	O
generated	O
from	O
the	O
previous	O
stage	O
are	O
used	O
in	O
each	O
stage	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
method	O
extended	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frameworks	O
,	O
we	O
conduct	O
experiments	O
with	O
our	O
proposed	O
weighting	Method
fine	Method
-	Method
tuning	Method
being	O
applied	O
to	O
DAN	Method
.	O

In	O
particular	O
,	O
each	O
stage	O
of	O
DAN	Method
is	O
first	O
pre	O
-	O
trained	O
and	O
further	O
weighting	O
fine	O
-	O
tuned	O
,	O
where	O
DAN	Method
with	O
weighting	Method
fine	Method
-	Method
tuning	Method
is	O
named	O
DAN	O
-	O
WM	Method
.	O

Note	O
that	O
the	O
results	O
of	O
retrained	Method
DAN	Method
(	O
re	Method
-	Method
DAN	Method
)	O
using	O
the	O
published	O
code	O
are	O
slightly	O
worse	O
than	O
reported	O
results	O
of	O
DAN	Method
.	O

For	O
a	O
fair	O
comparison	O
,	O
the	O
results	O
of	O
mean	Metric
error	Metric
of	O
DAN	Method
,	O
re	Method
-	Method
DAN	Method
,	O
and	O
DAN	O
-	O
WM	Method
on	O
IBUG	Task
benchmark	Task
are	O
all	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
the	O
mean	Metric
error	Metric
of	O
re	Metric
-	Metric
DAN	Metric
is	O
reduced	O
from	O
to	O
after	O
using	O
our	O
proposed	O
weighting	Method
fine	Method
-	Method
tuning	Method
.	O

Note	O
that	O
our	O
method	O
uses	O
only	O
a	O
single	O
neural	Method
network	Method
,	O
which	O
has	O
a	O
concise	O
structure	O
with	O
low	Metric
model	Metric
complexity	Metric
.	O

Our	O
network	O
can	O
be	O
replaced	O
with	O
a	O
more	O
powerful	O
one	O
such	O
as	O
cascaded	Method
deep	Method
neural	Method
networks	Method
,	O
which	O
could	O
further	O
improve	O
the	O
performance	O
of	O
face	Task
alignment	Task
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
have	O
developed	O
a	O
novel	O
multi	Method
-	Method
center	Method
learning	Method
framework	Method
with	O
multiple	Method
shape	Method
prediction	Method
layers	Method
for	O
face	Task
alignment	Task
.	O

The	O
structure	O
of	O
multiple	Method
shape	Method
prediction	Method
layers	Method
is	O
beneficial	O
for	O
reinforcing	O
the	O
learning	Task
process	Task
of	O
each	O
cluster	O
of	O
landmarks	O
.	O

In	O
addition	O
,	O
we	O
have	O
proposed	O
the	O
model	Method
assembling	Method
method	Method
to	O
integrate	O
multiple	Method
shape	Method
prediction	Method
layers	Method
into	O
one	O
shape	Method
prediction	Method
layer	Method
so	O
as	O
to	O
ensure	O
a	O
low	O
model	Metric
complexity	Metric
.	O

Extensive	O
experiments	O
have	O
demonstrated	O
the	O
effectiveness	O
of	O
our	O
method	O
including	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
.	O

First	O
,	O
each	O
component	O
of	O
our	O
framework	O
including	O
Global	Method
Average	Method
Pooling	Method
,	O
multiple	Method
shape	Method
prediction	Method
layers	Method
,	O
weighting	Method
fine	Method
-	Method
tuning	Method
,	O
and	O
multi	Method
-	Method
center	Method
fine	Method
-	Method
tuning	Method
contributes	O
to	O
face	Task
alignment	Task
.	O

Second	O
,	O
our	O
proposed	O
neural	Method
network	Method
and	O
model	Method
assembling	Method
method	Method
allow	O
real	O
-	O
time	O
performance	O
.	O

Third	O
,	O
we	O
have	O
extended	O
our	O
method	O
for	O
detecting	Task
partially	Task
occluded	Task
faces	Task
and	O
integrating	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	Method
frameworks	Method
,	O
and	O
have	O
shown	O
that	O
our	O
method	O
exploits	O
correlations	O
among	O
landmarks	O
and	O
can	O
further	O
improve	O
the	O
performance	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frameworks	O
.	O

The	O
proposed	O
framework	O
is	O
also	O
promising	O
to	O
be	O
applied	O
for	O
other	O
face	Task
analysis	Task
tasks	Task
and	O
multi	Task
-	Task
label	Task
problems	Task
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O

61472245	O
)	O
,	O
and	O
the	O
Science	O
and	O
Technology	O
Commission	O
of	O
Shanghai	O
Municipality	O
Program	O
(	O
No	O
.	O

16511101300	O
)	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Quasi	Method
-	Method
Recurrent	Method
Neural	Method
Networks	Method
Recurrent	Method
neural	Method
networks	Method
are	O
a	O
powerful	O
tool	O
for	O
modeling	O
sequential	O
data	O
,	O
but	O
the	O
dependence	O
of	O
each	O
timestep	O
’s	O
computation	O
on	O
the	O
previous	O
timestep	O
’s	O
output	O
limits	O
parallelism	O
and	O
makes	O
RNNs	Method
unwieldy	O
for	O
very	O
long	O
sequences	O
.	O

We	O
introduce	O
quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
(	O
QRNNs	Method
)	O
,	O
an	O
approach	O
to	O
neural	Task
sequence	Task
modeling	Task
that	O
alternates	O
convolutional	Method
layers	Method
,	O
which	O
apply	O
in	O
parallel	O
across	O
timesteps	O
,	O
and	O
a	O
minimalist	Method
recurrent	Method
pooling	Method
function	Method
that	O
applies	O
in	O
parallel	O
across	O
channels	O
.	O

Despite	O
lacking	O
trainable	O
recurrent	Method
layers	Method
,	O
stacked	O
QRNNs	Method
have	O
better	O
predictive	Metric
accuracy	Metric
than	O
stacked	Method
LSTMs	Method
of	O
the	O
same	O
hidden	O
size	O
.	O

Due	O
to	O
their	O
increased	O
parallelism	O
,	O
they	O
are	O
up	O
to	O
16	O
times	O
faster	O
at	O
train	Metric
and	Metric
test	Metric
time	Metric
.	O

Experiments	O
on	O
language	Task
modeling	Task
,	O
sentiment	Task
classification	Task
,	O
and	O
character	O
-	O
level	O
neural	O
machine	Task
translation	Task
demonstrate	O
these	O
advantages	O
and	O
underline	O
the	O
viability	O
of	O
QRNNs	Method
as	O
a	O
basic	O
building	O
block	O
for	O
a	O
variety	O
of	O
sequence	Task
tasks	Task
.	O

section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
,	O
including	O
gated	Method
variants	Method
such	O
as	O
the	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
Hochreiter1997	O
have	O
become	O
the	O
standard	O
model	Method
architecture	Method
for	O
deep	Method
learning	Method
approaches	Method
to	O
sequence	Task
modeling	Task
tasks	Task
.	O

RNNs	Method
repeatedly	O
apply	O
a	O
function	O
with	O
trainable	O
parameters	O
to	O
a	O
hidden	O
state	O
.	O

Recurrent	Method
layers	Method
can	O
also	O
be	O
stacked	O
,	O
increasing	O
network	O
depth	O
,	O
representational	O
power	O
and	O
often	O
accuracy	Metric
.	O

RNN	Method
applications	Method
in	O
the	O
natural	Task
language	Task
domain	Task
range	O
from	O
sentence	Task
classification	Task
Wang2015	O
to	O
word	Task
-	Task
and	Task
character	Task
-	Task
level	Task
language	Task
modeling	Task
Zaremba2014	O
.	O

RNNs	Method
are	O
also	O
commonly	O
the	O
basic	O
building	O
block	O
for	O
more	O
complex	O
models	O
for	O
tasks	O
such	O
as	O
machine	Task
translation	Task
Bahdanau2015	O
,	O
Luong2015	O
,	O
Bradbury2016	O
or	O
question	Task
answering	Task
Kumar2016	O
,	O
Xiong2016	O
.	O

Unfortunately	O
standard	O
RNNs	Method
,	O
including	O
LSTMs	Method
,	O
are	O
limited	O
in	O
their	O
capability	O
to	O
handle	O
tasks	O
involving	O
very	O
long	O
sequences	O
,	O
such	O
as	O
document	Task
classification	Task
or	O
character	O
-	O
level	O
machine	Task
translation	Task
,	O
as	O
the	O
computation	O
of	O
features	O
or	O
states	O
for	O
different	O
parts	O
of	O
the	O
document	O
can	O
not	O
occur	O
in	O
parallel	O
.	O

Convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
Krizhevsky2012	O
,	O
though	O
more	O
popular	O
on	O
tasks	O
involving	O
image	O
data	O
,	O
have	O
also	O
been	O
applied	O
to	O
sequence	Task
encoding	Task
tasks	Task
Zhang2015	O
.	O

Such	O
models	O
apply	O
time	Method
-	Method
invariant	Method
filter	Method
functions	Method
in	O
parallel	O
to	O
windows	O
along	O
the	O
input	O
sequence	O
.	O

CNNs	Method
possess	O
several	O
advantages	O
over	O
recurrent	Method
models	Method
,	O
including	O
increased	O
parallelism	O
and	O
better	O
scaling	O
to	O
long	O
sequences	O
such	O
as	O
those	O
often	O
seen	O
with	O
character	O
-	O
level	O
language	O
data	O
.	O

Convolutional	Method
models	Method
for	O
sequence	Task
processing	Task
have	O
been	O
more	O
successful	O
when	O
combined	O
with	O
RNN	Method
layers	Method
in	O
a	O
hybrid	Method
architecture	Method
Lee2016	O
,	O
because	O
traditional	O
max	Method
-	Method
and	Method
average	Method
-	Method
pooling	Method
approaches	Method
to	O
combining	O
convolutional	O
features	O
across	O
timesteps	O
assume	O
time	O
invariance	O
and	O
hence	O
can	O
not	O
make	O
full	O
use	O
of	O
large	O
-	O
scale	O
sequence	O
order	O
information	O
.	O

We	O
present	O
quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
for	O
neural	Task
sequence	Task
modeling	Task
.	O

QRNNs	Method
address	O
both	O
drawbacks	O
of	O
standard	O
models	O
:	O
like	O
CNNs	Method
,	O
QRNNs	Method
allow	O
for	O
parallel	Task
computation	Task
across	O
both	O
timestep	O
and	O
minibatch	O
dimensions	O
,	O
enabling	O
high	O
throughput	Metric
and	O
good	O
scaling	O
to	O
long	O
sequences	O
.	O

Like	O
RNNs	Method
,	O
QRNNs	Method
allow	O
the	O
output	O
to	O
depend	O
on	O
the	O
overall	O
order	O
of	O
elements	O
in	O
the	O
sequence	O
.	O

We	O
describe	O
QRNN	Method
variants	Method
tailored	O
to	O
several	O
natural	Task
language	Task
tasks	Task
,	O
including	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
,	O
language	Task
modeling	Task
,	O
and	O
character	O
-	O
level	O
machine	Task
translation	Task
.	O

These	O
models	O
outperform	O
strong	O
LSTM	Method
baselines	O
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
reducing	O
computation	Metric
time	Metric
.	O

section	O
:	O
Model	O
Each	O
layer	O
of	O
a	O
quasi	Method
-	Method
recurrent	Method
neural	Method
network	Method
consists	O
of	O
two	O
kinds	O
of	O
subcomponents	O
,	O
analogous	O
to	O
convolution	Method
and	Method
pooling	Method
layers	Method
in	O
CNNs	Method
.	O

The	O
convolutional	Method
component	Method
,	O
like	O
convolutional	Method
layers	Method
in	O
CNNs	Method
,	O
allows	O
fully	O
parallel	Task
computation	Task
across	O
both	O
minibatches	O
and	O
spatial	O
dimensions	O
,	O
in	O
this	O
case	O
the	O
sequence	O
dimension	O
.	O

The	O
pooling	Method
component	Method
,	O
like	O
pooling	Method
layers	Method
in	O
CNNs	Method
,	O
lacks	O
trainable	O
parameters	O
and	O
allows	O
fully	O
parallel	O
computation	O
across	O
minibatch	O
and	O
feature	O
dimensions	O
.	O

Given	O
an	O
input	O
sequence	O
of	O
-	O
dimensional	O
vectors	O
,	O
the	O
convolutional	Method
subcomponent	Method
of	O
a	O
QRNN	Method
performs	O
convolutions	Method
in	O
the	O
timestep	O
dimension	O
with	O
a	O
bank	Method
of	Method
filters	Method
,	O
producing	O
a	O
sequence	O
of	O
-	O
dimensional	O
candidate	O
vectors	O
.	O

In	O
order	O
to	O
be	O
useful	O
for	O
tasks	O
that	O
include	O
prediction	Task
of	Task
the	Task
next	Task
token	Task
,	O
the	O
filters	Method
must	O
not	O
allow	O
the	O
computation	O
for	O
any	O
given	O
timestep	O
to	O
access	O
information	O
from	O
future	O
timesteps	O
.	O

That	O
is	O
,	O
with	O
filters	O
of	O
width	O
,	O
each	O
depends	O
only	O
on	O
through	O
.	O

This	O
concept	O
,	O
known	O
as	O
a	O
masked	Method
convolution	Method
vandenOord2016	O
,	O
is	O
implemented	O
by	O
padding	O
the	O
input	O
to	O
the	O
left	O
by	O
the	O
convolution	O
’s	O
filter	O
size	O
minus	O
one	O
.	O

We	O
apply	O
additional	O
convolutions	Method
with	O
separate	O
filter	Method
banks	Method
to	O
obtain	O
sequences	O
of	O
vectors	O
for	O
the	O
elementwise	O
gates	O
that	O
are	O
needed	O
for	O
the	O
pooling	O
function	O
.	O

While	O
the	O
candidate	O
vectors	O
are	O
passed	O
through	O
a	O
nonlinearity	Method
,	O
the	O
gates	O
use	O
an	O
elementwise	Method
sigmoid	Method
.	O

If	O
the	O
pooling	Method
function	Method
requires	O
a	O
forget	O
gate	O
and	O
an	O
output	O
gate	O
at	O
each	O
timestep	O
,	O
the	O
full	O
set	O
of	O
computations	O
in	O
the	O
convolutional	Method
component	Method
is	O
then	O
:	O
where	O
,	O
,	O
and	O
,	O
each	O
in	O
,	O
are	O
the	O
convolutional	Method
filter	Method
banks	Method
and	O
denotes	O
a	O
masked	Method
convolution	Method
along	O
the	O
timestep	O
dimension	O
.	O

Note	O
that	O
if	O
the	O
filter	O
width	O
is	O
2	O
,	O
these	O
equations	O
reduce	O
to	O
the	O
LSTM	Method
-	O
like	O
Convolution	O
filters	O
of	O
larger	O
width	O
effectively	O
compute	O
higher	O
-	O
gram	O
features	O
at	O
each	O
timestep	O
;	O
thus	O
larger	O
widths	O
are	O
especially	O
important	O
for	O
character	Task
-	Task
level	Task
tasks	Task
.	O

Suitable	O
functions	O
for	O
the	O
pooling	O
subcomponent	O
can	O
be	O
constructed	O
from	O
the	O
familiar	O
elementwise	O
gates	O
of	O
the	O
traditional	O
LSTM	Method
cell	O
.	O

We	O
seek	O
a	O
function	O
controlled	O
by	O
gates	O
that	O
can	O
mix	O
states	O
across	O
timesteps	O
,	O
but	O
which	O
acts	O
independently	O
on	O
each	O
channel	O
of	O
the	O
state	O
vector	O
.	O

The	O
simplest	O
option	O
,	O
which	O
term	O
“	O
dynamic	Method
average	Method
pooling	Method
”	O
,	O
uses	O
only	O
a	O
forget	O
gate	O
:	O
where	O
denotes	O
elementwise	O
multiplication	O
.	O

The	O
function	O
may	O
also	O
include	O
an	O
output	O
gate	O
:	O
Or	O
the	O
recurrence	O
relation	O
may	O
include	O
an	O
independent	O
input	O
and	O
forget	O
gate	O
:	O
We	O
term	O
these	O
three	O
options	O
f	O
-	O
pooling	O
,	O
fo	Method
-	Method
pooling	Method
,	O
and	O
ifo	O
-	O
pooling	O
respectively	O
;	O
in	O
each	O
case	O
we	O
initialize	O
or	O
to	O
zero	O
.	O

Although	O
the	O
recurrent	O
parts	O
of	O
these	O
functions	O
must	O
be	O
calculated	O
for	O
each	O
timestep	O
in	O
sequence	O
,	O
their	O
simplicity	O
and	O
parallelism	O
along	O
feature	O
dimensions	O
means	O
that	O
,	O
in	O
practice	O
,	O
evaluating	O
them	O
over	O
even	O
long	O
sequences	O
requires	O
a	O
negligible	O
amount	O
of	O
computation	Metric
time	Metric
.	O

A	O
single	O
QRNN	Method
layer	Method
thus	O
performs	O
an	O
input	Method
-	Method
dependent	Method
pooling	Method
,	O
followed	O
by	O
a	O
gated	Method
linear	Method
combination	Method
of	Method
convolutional	Method
features	Method
.	O

As	O
with	O
convolutional	Method
neural	Method
networks	Method
,	O
two	O
or	O
more	O
QRNN	Method
layers	Method
should	O
be	O
stacked	O
to	O
create	O
a	O
model	O
with	O
the	O
capacity	O
to	O
approximate	O
more	O
complex	O
functions	O
.	O

subsection	O
:	O
Variants	O
Motivated	O
by	O
several	O
common	O
natural	Task
language	Task
tasks	Task
,	O
and	O
the	O
long	O
history	O
of	O
work	O
on	O
related	O
architectures	O
,	O
we	O
introduce	O
several	O
extensions	O
to	O
the	O
stacked	Method
QRNN	Method
described	O
above	O
.	O

Notably	O
,	O
many	O
extensions	O
to	O
both	O
recurrent	Method
and	Method
convolutional	Method
models	Method
can	O
be	O
applied	O
directly	O
to	O
the	O
QRNN	Method
as	O
it	O
combines	O
elements	O
of	O
both	O
model	O
types	O
.	O

Regularization	Task
An	O
important	O
extension	O
to	O
the	O
stacked	Method
QRNN	Method
is	O
a	O
robust	Method
regularization	Method
scheme	Method
inspired	O
by	O
recent	O
work	O
in	O
regularizing	Method
LSTMs	Method
.	O

The	O
need	O
for	O
an	O
effective	O
regularization	Method
method	Method
for	O
LSTMs	Method
,	O
and	O
dropout	Method
’s	O
relative	O
lack	O
of	O
efficacy	O
when	O
applied	O
to	O
recurrent	O
connections	O
,	O
led	O
to	O
the	O
development	O
of	O
recurrent	Method
dropout	Method
schemes	Method
,	O
including	O
variational	Method
inference	Method
–	O
based	O
dropout	O
Gal2015	Method
and	O
zoneout	Method
Krueger2016	O
.	O

These	O
schemes	O
extend	O
dropout	Method
to	O
the	O
recurrent	Task
setting	Task
by	O
taking	O
advantage	O
of	O
the	O
repeating	Method
structure	Method
of	Method
recurrent	Method
networks	Method
,	O
providing	O
more	O
powerful	O
and	O
less	O
destructive	Method
regularization	Method
.	O

Variational	Method
inference	Method
–	Method
based	Method
dropout	Method
locks	O
the	O
dropout	O
mask	O
used	O
for	O
the	O
recurrent	O
connections	O
across	O
timesteps	O
,	O
so	O
a	O
single	O
RNN	Method
pass	Method
uses	O
a	O
single	O
stochastic	O
subset	O
of	O
the	O
recurrent	O
weights	O
.	O

Zoneout	Method
stochastically	O
chooses	O
a	O
new	O
subset	O
of	O
channels	O
to	O
“	O
zone	O
out	O
”	O
at	O
each	O
timestep	O
;	O
for	O
these	O
channels	O
the	O
network	O
copies	O
states	O
from	O
one	O
timestep	O
to	O
the	O
next	O
without	O
modification	O
.	O

As	O
QRNNs	Method
lack	O
recurrent	O
weights	O
,	O
the	O
variational	Method
inference	Method
approach	Method
does	O
not	O
apply	O
.	O

Thus	O
we	O
extended	O
zoneout	Method
to	O
the	O
QRNN	Method
architecture	Method
by	O
modifying	O
the	O
pooling	Method
function	Method
to	O
keep	O
the	O
previous	O
pooling	O
state	O
for	O
a	O
stochastic	O
subset	O
of	O
channels	O
.	O

Conveniently	O
,	O
this	O
is	O
equivalent	O
to	O
stochastically	O
setting	O
a	O
subset	O
of	O
the	O
QRNN	O
’s	O
gate	O
channels	O
to	O
1	O
,	O
or	O
applying	O
dropout	Method
on	O
:	O
Thus	O
the	O
pooling	Method
function	Method
itself	O
need	O
not	O
be	O
modified	O
at	O
all	O
.	O

We	O
note	O
that	O
when	O
using	O
an	O
off	O
-	O
the	O
-	O
shelf	O
dropout	Method
layer	Method
in	O
this	O
context	O
,	O
it	O
is	O
important	O
to	O
remove	O
automatic	O
rescaling	O
functionality	O
from	O
the	O
implementation	O
if	O
it	O
is	O
present	O
.	O

In	O
many	O
experiments	O
,	O
we	O
also	O
apply	O
ordinary	Method
dropout	Method
between	Method
layers	Method
,	O
including	O
between	O
word	Method
embeddings	Method
and	O
the	O
first	Method
QRNN	Method
layer	Method
.	O

Densely	Method
-	Method
Connected	Method
Layers	Method
We	O
can	O
also	O
extend	O
the	O
QRNN	Method
architecture	Method
using	O
techniques	O
introduced	O
for	O
convolutional	Method
networks	Method
.	O

For	O
sequence	Task
classification	Task
tasks	Task
,	O
we	O
found	O
it	O
helpful	O
to	O
use	O
skip	O
-	O
connections	O
between	O
every	O
QRNN	Method
layer	Method
,	O
a	O
technique	O
termed	O
“	O
dense	Method
convolution	Method
”	O
by	O
.	O

Where	O
traditional	O
feed	Method
-	Method
forward	Method
or	Method
convolutional	Method
networks	Method
have	O
connections	O
only	O
between	O
subsequent	O
layers	O
,	O
a	O
“	O
DenseNet	Method
”	Method
with	O
layers	O
has	O
feed	O
-	O
forward	O
or	O
convolutional	O
connections	O
between	O
every	O
pair	O
of	O
layers	O
,	O
for	O
a	O
total	O
of	O
.	O

This	O
can	O
improve	O
gradient	Metric
flow	Metric
and	O
convergence	Metric
properties	Metric
,	O
especially	O
in	O
deeper	Task
networks	Task
,	O
although	O
it	O
requires	O
a	O
parameter	O
count	O
that	O
is	O
quadratic	O
in	O
the	O
number	O
of	O
layers	O
.	O

When	O
applying	O
this	O
technique	O
to	O
the	O
QRNN	Method
,	O
we	O
include	O
connections	O
between	O
the	O
input	O
embeddings	O
and	O
every	O
QRNN	Method
layer	Method
and	O
between	O
every	O
pair	O
of	O
QRNN	Method
layers	Method
.	O

This	O
is	O
equivalent	O
to	O
concatenating	O
each	O
QRNN	Method
layer	Method
’s	O
input	O
to	O
its	O
output	O
along	O
the	O
channel	O
dimension	O
before	O
feeding	O
the	O
state	O
into	O
the	O
next	O
layer	O
.	O

The	O
output	O
of	O
the	O
last	O
layer	O
alone	O
is	O
then	O
used	O
as	O
the	O
overall	O
encoding	O
result	O
.	O

Encoder	Method
–	Method
Decoder	Method
Models	Method
To	O
demonstrate	O
the	O
generality	O
of	O
QRNNs	Method
,	O
we	O
extend	O
the	O
model	Method
architecture	Method
to	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
tasks	Task
,	O
such	O
as	O
machine	Task
translation	Task
,	O
by	O
using	O
a	O
QRNN	Method
as	O
encoder	Method
and	O
a	O
modified	O
QRNN	Method
,	O
enhanced	O
with	O
attention	Method
,	O
as	O
decoder	Method
.	O

The	O
motivation	O
for	O
modifying	O
the	O
decoder	O
is	O
that	O
simply	O
feeding	O
the	O
last	O
encoder	O
hidden	O
state	O
(	O
the	O
output	O
of	O
the	O
encoder	Method
’s	Method
pooling	Method
layer	Method
)	O
into	O
the	O
decoder	Method
’s	Method
recurrent	Method
pooling	Method
layer	Method
,	O
analogously	O
to	O
conventional	O
recurrent	Method
encoder	Method
–	Method
decoder	Method
architectures	Method
,	O
would	O
not	O
allow	O
the	O
encoder	O
state	O
to	O
affect	O
the	O
gate	O
or	O
update	O
values	O
that	O
are	O
provided	O
to	O
the	O
decoder	Method
’s	Method
pooling	Method
layer	Method
.	O

This	O
would	O
substantially	O
limit	O
the	O
representational	O
power	O
of	O
the	O
decoder	Method
.	O

Instead	O
,	O
the	O
output	O
of	O
each	O
decoder	Method
QRNN	Method
layer	Method
’s	Method
convolution	Method
functions	Method
is	O
supplemented	O
at	O
every	O
timestep	O
with	O
the	O
final	O
encoder	O
hidden	O
state	O
.	O

This	O
is	O
accomplished	O
by	O
adding	O
the	O
result	O
of	O
the	O
convolution	O
for	O
layer	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
with	O
broadcasting	O
to	O
a	O
linearly	O
projected	O
copy	O
of	O
layer	O
’s	O
last	O
encoder	O
state	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
:	O
where	O
the	O
tilde	O
denotes	O
that	O
is	O
an	O
encoder	O
variable	O
.	O

Encoder	Method
–	Method
decoder	Method
models	Method
which	O
operate	O
on	O
long	O
sequences	O
are	O
made	O
significantly	O
more	O
powerful	O
with	O
the	O
addition	O
of	O
soft	O
attention	O
Bahdanau2015	O
,	O
which	O
removes	O
the	O
need	O
for	O
the	O
entire	O
input	O
representation	O
to	O
fit	O
into	O
a	O
fixed	O
-	O
length	O
encoding	O
vector	O
.	O

In	O
our	O
experiments	O
,	O
we	O
computed	O
an	O
attentional	O
sum	O
of	O
the	O
encoder	Method
’s	O
last	O
layer	O
’s	O
hidden	O
states	O
.	O

We	O
used	O
the	O
dot	O
products	O
of	O
these	O
encoder	O
hidden	O
states	O
with	O
the	O
decoder	O
’s	O
last	O
layer	O
’s	O
un	O
-	O
gated	O
hidden	O
states	O
,	O
applying	O
a	O
along	O
the	O
encoder	O
timesteps	O
,	O
to	O
weight	O
the	O
encoder	O
states	O
into	O
an	O
attentional	O
sum	O
for	O
each	O
decoder	O
timestep	O
.	O

This	O
context	O
,	O
and	O
the	O
decoder	O
state	O
,	O
are	O
then	O
fed	O
into	O
a	O
linear	Method
layer	Method
followed	O
by	O
the	O
output	O
gate	O
:	O
where	O
is	O
the	O
last	O
layer	O
.	O

While	O
the	O
first	O
step	O
of	O
this	O
attention	Method
procedure	Method
is	O
quadratic	O
in	O
the	O
sequence	O
length	O
,	O
in	O
practice	O
it	O
takes	O
significantly	O
less	O
computation	Metric
time	Metric
than	O
the	O
model	O
’s	O
linear	Method
and	Method
convolutional	Method
layers	Method
due	O
to	O
the	O
simple	O
and	O
highly	O
parallel	O
dot	O
-	O
product	O
scoring	O
function	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
performance	O
of	O
the	O
QRNN	Method
on	O
three	O
different	O
natural	Task
language	Task
tasks	Task
:	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
,	O
language	Task
modeling	Task
,	O
and	O
character	O
-	O
based	O
neural	O
machine	Task
translation	Task
.	O

Our	O
QRNN	Method
models	Method
outperform	O
LSTM	Method
-	O
based	O
models	O
of	O
equal	Method
hidden	Method
size	Method
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
improving	O
computation	Metric
speed	Metric
.	O

Experiments	O
were	O
implemented	O
in	O
Chainer	O
Tokui2015	O
.	O

subsection	O
:	O
Sentiment	Task
Classification	Task
We	O
evaluate	O
the	O
QRNN	Method
architecture	Method
on	O
a	O
popular	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
benchmark	Task
,	O
the	O
IMDb	Material
movie	Material
review	Material
dataset	Material
Maas2011	O
.	O

The	O
dataset	O
consists	O
of	O
a	O
balanced	O
sample	O
of	O
25	O
,	O
000	O
positive	O
and	O
25	O
,	O
000	O
negative	O
reviews	O
,	O
divided	O
into	O
equal	O
-	O
size	O
train	O
and	O
test	O
sets	O
,	O
with	O
an	O
average	O
document	O
length	O
of	O
231	O
words	O
Wang2012	O
.	O

We	O
compare	O
only	O
to	O
other	O
results	O
that	O
do	O
not	O
make	O
use	O
of	O
additional	O
unlabeled	O
data	O
(	O
thus	O
excluding	O
e.g.	O
,	O
Miyato2016	O
)	O
.	O

Our	O
best	O
performance	O
on	O
a	O
held	O
-	O
out	O
development	O
set	O
was	O
achieved	O
using	O
a	O
four	Method
-	Method
layer	Method
densely	Method
-	Method
connected	Method
QRNN	Method
with	O
256	O
units	O
per	O
layer	O
and	O
word	O
vectors	O
initialized	O
using	O
300	Method
-	Method
dimensional	Method
cased	Method
GloVe	Method
embeddings	Method
Pennington2014	O
.	O

Dropout	O
of	O
0.3	O
was	O
applied	O
between	O
layers	O
,	O
and	O
we	O
used	O
regularization	Method
of	Method
.	O

Optimization	Task
was	O
performed	O
on	O
minibatches	O
of	O
24	O
examples	O
using	O
RMSprop	Method
Tieleman2012	O
with	O
learning	Metric
rate	Metric
of	O
,	O
,	O
and	O
.	O

Small	O
batch	O
sizes	O
and	O
long	O
sequence	O
lengths	O
provide	O
an	O
ideal	O
situation	O
for	O
demonstrating	O
the	O
QRNN	Method
’s	Method
performance	O
advantages	O
over	O
traditional	O
recurrent	Method
architectures	Method
.	O

We	O
observed	O
a	O
speedup	O
of	O
3.2x	O
on	O
IMDb	Metric
train	Metric
time	Metric
per	O
epoch	O
compared	O
to	O
the	O
optimized	O
LSTM	Method
implementation	O
provided	O
in	O
NVIDIA	Method
’s	Method
cuDNN	Method
library	Method
.	O

For	O
specific	O
batch	O
sizes	O
and	O
sequence	O
lengths	O
,	O
a	O
16x	O
speed	O
gain	O
is	O
possible	O
.	O

Figure	O
[	O
reference	O
]	O
provides	O
extensive	O
speed	O
comparisons	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
visualize	O
the	O
hidden	O
state	O
vectors	O
of	O
the	O
final	O
QRNN	Method
layer	Method
on	O
part	O
of	O
an	O
example	O
from	O
the	O
IMDb	Material
dataset	Material
.	O

Even	O
without	O
any	O
post	Method
-	Method
processing	Method
,	O
changes	O
in	O
the	O
hidden	O
state	O
are	O
visible	O
and	O
interpretable	O
in	O
regards	O
to	O
the	O
input	O
.	O

This	O
is	O
a	O
consequence	O
of	O
the	O
elementwise	O
nature	O
of	O
the	O
recurrent	Method
pooling	Method
function	Method
,	O
which	O
delays	O
direct	O
interaction	O
between	O
different	O
channels	O
of	O
the	O
hidden	O
state	O
until	O
the	O
computation	O
of	O
the	O
next	O
QRNN	Method
layer	Method
.	O

subsection	O
:	O
Language	Method
Modeling	Method
We	O
replicate	O
the	O
language	Method
modeling	Method
experiment	O
of	O
Zaremba2014	O
and	O
Gal2015	O
to	O
benchmark	O
the	O
QRNN	Method
architecture	Method
for	O
natural	Task
language	Task
sequence	Task
prediction	Task
.	O

The	O
experiment	O
uses	O
a	O
standard	O
preprocessed	Method
version	Method
of	O
the	O
Penn	Material
Treebank	Material
(	O
PTB	Material
)	O
by	O
Mikolov2010	O
.	O

We	O
implemented	O
a	O
gated	Method
QRNN	Method
model	Method
with	O
medium	O
hidden	O
size	O
:	O
2	O
layers	O
with	O
640	O
units	O
in	O
each	O
layer	O
.	O

Both	O
QRNN	Method
layers	Method
use	O
a	O
convolutional	Method
filter	Method
width	Method
of	O
two	O
timesteps	O
.	O

While	O
the	O
“	O
medium	Method
”	Method
models	Method
used	O
in	O
other	O
work	O
Zaremba2014	O
,	O
Gal2015	O
consist	O
of	O
650	O
units	O
in	O
each	O
layer	O
,	O
it	O
was	O
more	O
computationally	O
convenient	O
to	O
use	O
a	O
multiple	O
of	O
32	O
.	O

As	O
the	O
Penn	Material
Treebank	Material
is	O
a	O
relatively	O
small	O
dataset	O
,	O
preventing	O
overfitting	Task
is	O
of	O
considerable	O
importance	O
and	O
a	O
major	O
focus	O
of	O
recent	O
research	O
.	O

It	O
is	O
not	O
obvious	O
in	O
advance	O
which	O
of	O
the	O
many	O
RNN	Method
regularization	Method
schemes	Method
would	O
perform	O
well	O
when	O
applied	O
to	O
the	O
QRNN	Method
.	O

Our	O
tests	O
showed	O
encouraging	O
results	O
from	O
zoneout	Method
applied	O
to	O
the	O
QRNN	Method
’s	Method
recurrent	Method
pooling	Method
layer	Method
,	O
implemented	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
experimental	O
settings	O
largely	O
followed	O
the	O
“	O
medium	O
”	O
setup	O
of	O
Zaremba2014	O
.	O

Optimization	Task
was	O
performed	O
by	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
without	O
momentum	Method
.	O

The	O
learning	Metric
rate	Metric
was	O
set	O
at	O
1	O
for	O
six	O
epochs	O
,	O
then	O
decayed	O
by	O
0.95	O
for	O
each	O
subsequent	O
epoch	O
,	O
for	O
a	O
total	O
of	O
72	O
epochs	O
.	O

We	O
additionally	O
used	O
regularization	O
of	O
and	O
rescaled	O
gradients	O
with	O
norm	O
above	O
10	O
.	O

Zoneout	Method
was	O
applied	O
by	O
performing	O
dropout	Method
with	O
ratio	O
0.1	O
on	O
the	O
forget	O
gates	O
of	O
the	O
QRNN	Method
,	O
without	O
rescaling	O
the	O
output	O
of	O
the	O
dropout	O
function	O
.	O

Batches	O
consist	O
of	O
20	O
examples	O
,	O
each	O
105	O
timesteps	O
.	O

Comparing	O
our	O
results	O
on	O
the	O
gated	Method
QRNN	Method
with	O
zoneout	Method
to	O
the	O
results	O
of	O
LSTMs	Method
with	O
both	O
ordinary	Method
and	Method
variational	Method
dropout	Method
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
the	O
QRNN	Method
is	O
highly	O
competitive	O
.	O

The	O
QRNN	O
without	O
zoneout	Method
strongly	O
outperforms	O
both	O
our	O
medium	O
LSTM	Method
and	O
the	O
medium	O
LSTM	Method
of	O
Zaremba2014	Method
which	O
do	O
not	O
use	O
recurrent	Method
dropout	Method
and	O
is	O
even	O
competitive	O
with	O
variational	Method
LSTMs	Method
.	O

This	O
may	O
be	O
due	O
to	O
the	O
limited	O
computational	O
capacity	O
that	O
the	O
QRNN	Method
’s	Method
pooling	Method
layer	Method
has	O
relative	O
to	O
the	O
LSTM	Method
’s	O
recurrent	O
weights	O
,	O
providing	O
structural	O
regularization	O
over	O
the	O
recurrence	O
.	O

Without	O
zoneout	Method
,	O
early	O
stopping	O
based	O
upon	O
validation	Metric
loss	Metric
was	O
required	O
as	O
the	O
QRNN	Method
would	O
begin	O
overfitting	O
.	O

By	O
applying	O
a	O
small	O
amount	O
of	O
zoneout	Method
(	O
)	O
,	O
no	O
early	O
stopping	O
is	O
required	O
and	O
the	O
QRNN	Method
achieves	O
competitive	O
levels	O
of	O
perplexity	Metric
to	O
the	O
variational	O
LSTM	Method
of	O
Gal2015	O
,	O
which	O
had	O
variational	Method
inference	Method
based	Method
dropout	Method
of	Method
0.2	Method
applied	O
recurrently	O
.	O

Their	O
best	O
performing	O
variation	O
also	O
used	O
Monte	Method
Carlo	Method
(	Method
MC	Method
)	Method
dropout	Method
averaging	Method
at	O
test	O
time	O
of	O
1000	O
different	O
masks	O
,	O
making	O
it	O
computationally	O
more	O
expensive	O
to	O
run	O
.	O

When	O
training	O
on	O
the	O
PTB	Material
dataset	O
with	O
an	O
NVIDIA	Method
K40	Method
GPU	Method
,	O
we	O
found	O
that	O
the	O
QRNN	Method
is	O
substantially	O
faster	O
than	O
a	O
standard	O
LSTM	Method
,	O
even	O
when	O
comparing	O
against	O
the	O
optimized	O
cuDNN	O
LSTM	Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
provide	O
a	O
breakdown	O
of	O
the	O
time	O
taken	O
for	O
Chainer	O
’s	O
default	O
LSTM	Method
,	O
the	O
cuDNN	O
LSTM	Method
,	O
and	O
QRNN	Method
to	O
perform	O
a	O
full	O
forward	Method
and	Method
backward	Method
pass	Method
on	O
a	O
single	O
batch	O
during	O
training	O
of	O
the	O
RNN	Method
LM	Method
on	O
PTB	Material
.	O

For	O
both	O
LSTM	Method
implementations	O
,	O
running	Metric
time	Metric
was	O
dominated	O
by	O
the	O
RNN	Method
computations	Method
,	O
even	O
with	O
the	O
highly	O
optimized	O
cuDNN	Method
implementation	Method
.	O

For	O
the	O
QRNN	Method
implementation	Method
,	O
however	O
,	O
the	O
“	O
RNN	Method
”	Method
layers	Method
are	O
no	O
longer	O
the	O
bottleneck	O
.	O

Indeed	O
,	O
there	O
are	O
diminishing	O
returns	O
from	O
further	O
optimization	Task
of	O
the	O
QRNN	Method
itself	Method
as	O
the	O
softmax	O
and	O
optimization	O
overhead	O
take	O
equal	O
or	O
greater	O
time	O
.	O

Note	O
that	O
the	O
softmax	Method
,	O
over	O
a	O
vocabulary	O
size	O
of	O
only	O
10	O
,	O
000	O
words	O
,	O
is	O
relatively	O
small	O
;	O
for	O
tasks	O
with	O
larger	O
vocabularies	O
,	O
the	O
softmax	Method
would	O
likely	O
dominate	O
computation	Metric
time	Metric
.	O

It	O
is	O
also	O
important	O
to	O
note	O
that	O
the	O
cuDNN	Method
library	Method
’s	Method
RNN	Method
primitives	Method
do	O
not	O
natively	O
support	O
any	O
form	O
of	O
recurrent	Method
dropout	Method
.	O

That	O
is	O
,	O
running	O
an	O
LSTM	Method
that	O
uses	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
regularization	Method
scheme	Method
at	O
cuDNN	Method
-	O
like	O
speeds	O
would	O
likely	O
require	O
an	O
entirely	O
custom	O
kernel	O
.	O

Batch	O
size	O
subsection	O
:	O
Character	Task
-	Task
level	Task
Neural	Task
Machine	Task
Translation	Task
We	O
evaluate	O
the	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
QRNN	Method
architecture	Method
described	O
in	O
[	O
reference	O
]	O
on	O
a	O
challenging	O
neural	O
machine	Task
translation	Task
task	O
,	O
IWSLT	Task
German	Task
–	Task
English	Task
spoken	Task
-	Task
domain	Task
translation	Task
,	O
applying	O
fully	Task
character	Task
-	Task
level	Task
segmentation	Task
.	O

This	O
dataset	O
consists	O
of	O
209	O
,	O
772	O
sentence	O
pairs	O
of	O
parallel	O
training	O
data	O
from	O
transcribed	O
TED	O
and	O
TEDx	O
presentations	O
,	O
with	O
a	O
mean	O
sentence	O
length	O
of	O
103	O
characters	O
for	O
German	Material
and	O
93	O
for	O
English	Material
.	O

We	O
remove	O
training	O
sentences	O
with	O
more	O
than	O
300	O
characters	O
in	O
English	O
or	O
German	O
,	O
and	O
use	O
a	O
unified	O
vocabulary	O
of	O
187	O
Unicode	O
code	O
points	O
.	O

Our	O
best	O
performance	O
on	O
a	O
development	O
set	O
(	O
TED.tst2013	Material
)	O
was	O
achieved	O
using	O
a	O
four	Method
-	Method
layer	Method
encoder	Method
–	Method
decoder	Method
QRNN	Method
with	O
320	O
units	O
per	O
layer	O
,	O
no	O
dropout	O
or	O
regularization	O
,	O
and	O
gradient	Method
rescaling	Method
to	O
a	O
maximum	O
magnitude	O
of	O
5	O
.	O

Inputs	O
were	O
supplied	O
to	O
the	O
encoder	O
reversed	O
,	O
while	O
the	O
encoder	Method
convolutions	Method
were	O
not	O
masked	O
.	O

The	O
first	O
encoder	Method
layer	Method
used	O
convolutional	O
filter	O
width	O
,	O
while	O
the	O
other	O
encoder	Method
layers	Method
used	O
.	O

Optimization	Task
was	O
performed	O
for	O
10	O
epochs	O
on	O
minibatches	O
of	O
16	O
examples	O
using	O
Adam	Method
kingma2014adam	Method
with	O
,	O
,	O
,	O
and	O
.	O

Decoding	Task
was	O
performed	O
using	O
beam	Method
search	Method
with	O
beam	O
width	O
8	O
and	O
length	Method
normalization	Method
.	O

The	O
modified	O
log	Metric
-	Metric
probability	Metric
ranking	Metric
criterion	Metric
is	O
provided	O
in	O
the	O
appendix	O
.	O

Results	O
using	O
this	O
architecture	O
were	O
compared	O
to	O
an	O
equal	O
-	O
sized	O
four	O
-	O
layer	O
encoder	O
–	O
decoder	O
LSTM	Method
with	O
attention	Method
,	O
applying	O
dropout	Method
of	Method
0.2	Method
.	O

We	O
again	O
optimized	O
using	O
Adam	Method
;	O
other	O
hyperparameters	O
were	O
equal	O
to	O
their	O
values	O
for	O
the	O
QRNN	Method
and	O
the	O
same	O
beam	Method
search	Method
procedure	Method
was	O
applied	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
QRNN	Method
outperformed	O
the	O
character	O
-	O
level	O
LSTM	Method
,	O
almost	O
matching	O
the	O
performance	O
of	O
a	O
word	Method
-	Method
level	Method
attentional	Method
baseline	Method
.	O

section	O
:	O
Related	O
Work	O
Exploring	O
alternatives	O
to	O
traditional	O
RNNs	Method
for	O
sequence	Task
tasks	Task
is	O
a	O
major	O
area	O
of	O
current	O
research	O
.	O

Quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
are	O
related	O
to	O
several	O
such	O
recently	O
described	O
models	O
,	O
especially	O
the	O
strongly	Method
-	Method
typed	Method
recurrent	Method
neural	Method
networks	Method
(	O
T	Method
-	Method
RNN	Method
)	O
introduced	O
by	O
.	O

While	O
the	O
motivation	O
and	O
constraints	O
described	O
in	O
that	O
work	O
are	O
different	O
,	O
’s	O
concepts	O
of	O
“	O
learnware	O
”	O
and	O
“	O
firmware	Method
”	O
parallel	O
our	O
discussion	O
of	O
convolution	Method
-	Method
like	Method
and	Method
pooling	Method
-	Method
like	Method
subcomponents	Method
.	O

As	O
the	O
use	O
of	O
a	O
fully	O
connected	O
layer	O
for	O
recurrent	O
connections	O
violates	O
the	O
constraint	O
of	O
“	O
strong	O
typing	O
”	O
,	O
all	O
strongly	Method
-	Method
typed	Method
RNN	Method
architectures	Method
(	O
including	O
the	O
T	Method
-	Method
RNN	Method
,	O
T	Method
-	Method
GRU	Method
,	O
and	O
T	O
-	O
LSTM	Method
)	O
are	O
also	O
quasi	Method
-	Method
recurrent	Method
.	O

However	O
,	O
some	O
QRNN	Method
models	Method
(	O
including	O
those	O
with	O
attention	O
or	O
skip	O
-	O
connections	O
)	O
are	O
not	O
“	O
strongly	O
typed	O
”	O
.	O

In	O
particular	O
,	O
a	O
T	Method
-	Method
RNN	Method
differs	O
from	O
a	O
QRNN	Method
as	O
described	O
in	O
this	O
paper	O
with	O
filter	O
size	O
1	O
and	O
f	Method
-	Method
pooling	Method
only	O
in	O
the	O
absence	O
of	O
an	O
activation	O
function	O
on	O
.	O

Similarly	O
,	O
T	Method
-	Method
GRUs	Method
and	O
T	Method
-	Method
LSTMs	Method
differ	O
from	O
QRNNs	Method
with	O
filter	O
size	O
2	O
and	O
fo	Method
-	Method
or	Method
ifo	Method
-	Method
pooling	Method
respectively	O
in	O
that	O
they	O
lack	O
on	O
and	O
use	O
rather	O
than	O
sigmoid	O
on	O
.	O

The	O
QRNN	Method
is	O
also	O
related	O
to	O
work	O
in	O
hybrid	Method
convolutional	Method
–	Method
recurrent	Method
models	Method
.	O

Zhou2015b	O
apply	O
CNNs	Method
at	O
the	O
word	O
level	O
to	O
generate	O
-	O
gram	O
features	O
used	O
by	O
an	O
LSTM	Method
for	O
text	Task
classification	Task
.	O

Xiao2016	O
also	O
tackle	O
text	Task
classification	Task
by	O
applying	O
convolutions	Method
at	O
the	O
character	O
level	O
,	O
with	O
a	O
stride	O
to	O
reduce	O
sequence	O
length	O
,	O
then	O
feeding	O
these	O
features	O
into	O
a	O
bidirectional	O
LSTM	Method
.	O

A	O
similar	O
approach	O
was	O
taken	O
by	O
Lee2016	O
for	O
character	O
-	O
level	O
machine	Task
translation	Task
.	O

Their	O
model	O
’s	O
encoder	Method
uses	O
a	O
convolutional	Method
layer	Method
followed	O
by	O
max	Method
-	Method
pooling	Method
to	O
reduce	O
sequence	O
length	O
,	O
a	O
four	Method
-	Method
layer	Method
highway	Method
network	Method
,	O
and	O
a	O
bidirectional	Method
GRU	Method
.	O

The	O
parallelism	O
of	O
the	O
convolutional	Method
,	Method
pooling	Method
,	Method
and	Method
highway	Method
layers	Method
allows	O
training	O
speed	O
comparable	O
to	O
subword	Method
-	Method
level	Method
models	Method
without	O
hard	Task
-	Task
coded	Task
text	Task
segmentation	Task
.	O

The	O
QRNN	Method
encoder	Method
–	Method
decoder	Method
model	Method
shares	O
the	O
favorable	O
parallelism	O
and	O
path	O
-	O
length	O
properties	O
exhibited	O
by	O
the	O
ByteNet	Method
Kalchbrenner2016	O
,	O
an	O
architecture	O
for	O
character	O
-	O
level	O
machine	Task
translation	Task
based	O
on	O
residual	Method
convolutions	Method
over	O
binary	O
trees	O
.	O

Their	O
model	O
was	O
constructed	O
to	O
achieve	O
three	O
desired	O
properties	O
:	O
parallelism	O
,	O
linear	Metric
-	Metric
time	Metric
computational	Metric
complexity	Metric
,	O
and	O
short	O
paths	O
between	O
any	O
pair	O
of	O
words	O
in	O
order	O
to	O
better	O
propagate	O
gradient	O
signals	O
.	O

section	O
:	O
Conclusion	O
Intuitively	O
,	O
many	O
aspects	O
of	O
the	O
semantics	O
of	O
long	O
sequences	O
are	O
context	O
-	O
invariant	O
and	O
can	O
be	O
computed	O
in	O
parallel	O
(	O
e.g.	O
,	O
convolutionally	O
)	O
,	O
but	O
some	O
aspects	O
require	O
long	O
-	O
distance	O
context	O
and	O
must	O
be	O
computed	O
recurrently	O
.	O

Many	O
existing	O
neural	Method
network	Method
architectures	Method
either	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
contextual	O
information	O
or	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
parallelism	O
.	O

QRNNs	Method
exploit	O
both	O
parallelism	O
and	O
context	O
,	O
exhibiting	O
advantages	O
from	O
both	O
convolutional	Method
and	Method
recurrent	Method
neural	Method
networks	Method
.	O

QRNNs	Method
have	O
better	O
predictive	Metric
accuracy	Metric
than	O
LSTM	Method
-	O
based	O
models	O
of	O
equal	Method
hidden	Method
size	Method
,	O
even	O
though	O
they	O
use	O
fewer	O
parameters	O
and	O
run	O
substantially	O
faster	O
.	O

Our	O
experiments	O
show	O
that	O
the	O
speed	Metric
and	Metric
accuracy	Metric
advantages	Metric
remain	O
consistent	O
across	O
tasks	O
and	O
at	O
both	O
word	O
and	O
character	O
levels	O
.	O

Extensions	O
to	O
both	O
CNNs	Method
and	O
RNNs	Method
are	O
often	O
directly	O
applicable	O
to	O
the	O
QRNN	Method
,	O
while	O
the	O
model	O
’s	O
hidden	O
states	O
are	O
more	O
interpretable	O
than	O
those	O
of	O
other	O
recurrent	Method
architectures	Method
as	O
its	O
channels	O
maintain	O
their	O
independence	O
across	O
timesteps	O
.	O

We	O
believe	O
that	O
QRNNs	Method
can	O
serve	O
as	O
a	O
building	O
block	O
for	O
long	Task
-	Task
sequence	Task
tasks	Task
that	O
were	O
previously	O
impractical	O
with	O
traditional	O
RNNs	Method
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
Beam	Metric
search	Metric
ranking	Metric
criterion	Metric
The	O
modified	O
log	Metric
-	Metric
probability	Metric
ranking	Metric
criterion	Metric
we	O
used	O
in	O
beam	Task
search	Task
for	O
translation	Task
experiments	Task
is	O
:	O
where	O
is	O
a	O
length	O
normalization	O
parameter	O
Wu2016	O
,	O
is	O
the	O
th	O
output	O
character	O
,	O
and	O
is	O
a	O
“	O
target	O
length	O
”	O
equal	O
to	O
the	O
source	O
sentence	O
length	O
plus	O
five	O
characters	O
.	O

This	O
reduces	O
at	O
to	O
ordinary	O
beam	Method
search	Method
with	O
probabilities	O
:	O
and	O
at	O
to	O
beam	Method
search	Method
with	O
probabilities	O
normalized	O
by	O
length	O
(	O
up	O
to	O
the	O
target	O
length	O
)	O
:	O
Conveniently	O
,	O
this	O
ranking	Metric
criterion	Metric
can	O
be	O
computed	O
at	O
intermediate	O
beam	O
-	O
search	O
timesteps	O
,	O
obviating	O
the	O
need	O
to	O
apply	O
a	O
separate	O
reranking	Method
on	O
complete	O
hypotheses	O
.	O

document	O
:	O
The	O
Arcade	Method
Learning	Method
Environment	Method
:	O
An	O
Evaluation	Method
Platform	Method
for	O
General	Task
Agents	Task
In	O
this	O
article	O
we	O
introduce	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
:	O
both	O
a	O
challenge	O
problem	O
and	O
a	O
platform	O
and	O
methodology	O
for	O
evaluating	O
the	O
development	O
of	O
general	Task
,	Task
domain	Task
-	Task
independent	Task
AI	Task
technology	Task
.	O

ALE	Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	Material
2600	Material
game	Material
environments	Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

ALE	Method
presents	O
significant	O
research	O
challenges	O
for	O
reinforcement	Task
learning	Task
,	O
model	Method
learning	Method
,	O
model	Method
-	Method
based	Method
planning	Method
,	O
imitation	Task
learning	Task
,	O
transfer	Task
learning	Task
,	O
and	O
intrinsic	Task
motivation	Task
.	O

Most	O
importantly	O
,	O
it	O
provides	O
a	O
rigorous	O
testbed	O
for	O
evaluating	O
and	O
comparing	O
approaches	O
to	O
these	O
problems	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	Method
by	O
developing	O
and	O
benchmarking	O
domain	Method
-	Method
independent	Method
agents	Method
designed	O
using	O
well	O
-	O
established	O
AI	Method
techniques	Method
for	O
both	O
reinforcement	Task
learning	Task
and	O
planning	Task
.	O

In	O
doing	O
so	O
,	O
we	O
also	O
propose	O
an	O
evaluation	O
methodology	O
made	O
possible	O
by	O
ALE	Method
,	O
reporting	O
empirical	O
results	O
on	O
over	O
55	O
different	O
games	O
.	O

All	O
of	O
the	O
software	O
,	O
including	O
the	O
benchmark	O
agents	O
,	O
is	O
publicly	O
available	O
.	O

472013253–27902	O
/	O
1306	O
/	O
13	O
TheArcadeLearningEnvironment	O
:	O
AnEvaluationPlatformforGeneralAgents	O
Bellemare	O
,	O
Naddaf	O
,	O
Veness	O
,&	O
Bowling	O
253	O
section	O
:	O
Introduction	O
A	O
longstanding	O
goal	O
of	O
artificial	Task
intelligence	Task
is	O
the	O
development	O
of	O
algorithms	O
capable	O
of	O
general	Task
competency	Task
in	O
a	O
variety	O
of	O
tasks	O
and	O
domains	O
without	O
the	O
need	O
for	O
domain	O
-	O
specific	O
tailoring	O
.	O

To	O
this	O
end	O
,	O
different	O
theoretical	O
frameworks	O
have	O
been	O
proposed	O
to	O
formalize	O
the	O
notion	O
of	O
“	O
big	O
”	O
artificial	O
intelligence	O
e.g.	O
,	O
¿Russell97rationalityand	O
,	O
Hutter:04uaibook	O
,	O
legg08machine	O
.	O

Similar	O
ideas	O
have	O
been	O
developed	O
around	O
the	O
theme	O
of	O
lifelong	Task
learning	Task
:	O
learning	O
a	O
reusable	Task
,	Task
high	Task
-	Task
level	Task
understanding	Task
of	Task
the	Task
world	Task
from	O
raw	Material
sensory	Material
data	Material
thrun95lifelong	O
,	O
pierce_kuipers_97	O
,	O
stober08pixels	O
,	O
sutton11horde	O
.	O

The	O
growing	O
interest	O
in	O
competitions	O
such	O
as	O
the	O
General	Task
Game	Task
Playing	Task
competition	Task
,	O
Reinforcement	Task
Learning	Task
competition	Task
,	O
and	O
the	O
International	Task
Planning	Task
competition	Task
coles_12	O
also	O
suggests	O
the	O
artificial	Task
intelligence	Task
community	O
’s	O
desire	O
for	O
the	O
emergence	O
of	O
algorithms	O
that	O
provide	O
general	O
competency	O
.	O

Designing	O
generally	O
competent	O
agents	O
raises	O
the	O
question	O
of	O
how	O
to	O
best	O
evaluate	O
them	O
.	O

Empirically	O
evaluating	O
general	Task
competency	Task
on	O
a	O
handful	O
of	O
parametrized	Material
benchmark	Material
problems	Material
is	O
,	O
by	O
definition	O
,	O
flawed	O
.	O

Such	O
an	O
evaluation	O
is	O
prone	O
to	O
method	O
overfitting	O
and	O
discounts	O
the	O
amount	O
of	O
expert	O
effort	O
necessary	O
to	O
transfer	O
the	O
algorithm	O
to	O
new	O
domains	O
.	O

Ideally	O
,	O
the	O
algorithm	O
should	O
be	O
compared	O
across	O
domains	O
that	O
are	O
(	O
i	O
)	O
varied	O
enough	O
to	O
claim	O
generality	O
,	O
(	O
ii	O
)	O
each	O
interesting	O
enough	O
to	O
be	O
representative	O
of	O
settings	O
that	O
might	O
be	O
faced	O
in	O
practice	O
,	O
and	O
(	O
iii	O
)	O
each	O
created	O
by	O
an	O
independent	O
party	O
to	O
be	O
free	O
of	O
experimenter	O
’s	O
bias	O
.	O

In	O
this	O
article	O
,	O
we	O
introduce	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
:	O
a	O
new	O
challenge	O
problem	O
,	O
platform	O
,	O
and	O
experimental	O
methodology	O
for	O
empirically	Task
assessing	Task
agents	Task
designed	O
for	O
general	Task
competency	Task
.	O

ALE	Method
is	O
a	O
software	Method
framework	Method
for	O
interfacing	O
with	O
emulated	O
Atari	Material
2600	Material
game	Material
environments	Material
.	O

The	O
Atari	Method
2600	Method
,	O
a	O
second	Method
generation	Method
game	Method
console	Method
,	O
was	O
originally	O
released	O
in	O
1977	O
and	O
remained	O
massively	O
popular	O
for	O
over	O
a	O
decade	O
.	O

Over	O
500	O
games	O
were	O
developed	O
for	O
the	O
Atari	Material
2600	Material
,	O
spanning	O
a	O
diverse	O
range	O
of	O
genres	O
such	O
as	O
shooters	Material
,	O
beat’em	Material
ups	Material
,	O
puzzle	Material
,	O
sports	Material
,	O
and	O
action	Task
-	Task
adventure	Task
games	Task
;	O
many	O
game	O
genres	O
were	O
pioneered	O
on	O
the	O
console	O
.	O

While	O
modern	O
game	O
consoles	O
involve	O
visuals	O
,	O
controls	O
,	O
and	O
a	O
general	O
complexity	O
that	O
rivals	O
the	O
real	O
world	O
,	O
Atari	Material
2600	Material
games	Material
are	O
far	O
simpler	O
.	O

In	O
spite	O
of	O
this	O
,	O
they	O
still	O
pose	O
a	O
variety	O
of	O
challenging	O
and	O
interesting	O
situations	O
for	O
human	O
players	O
.	O

ALE	Method
is	O
both	O
an	O
experimental	O
methodology	O
and	O
a	O
challenge	O
problem	O
for	O
general	Task
AI	Task
competency	Task
.	O

In	O
machine	Task
learning	Task
,	O
it	O
is	O
considered	O
poor	O
experimental	O
practice	O
to	O
both	O
train	O
and	O
evaluate	O
an	O
algorithm	O
on	O
the	O
same	O
data	O
set	O
,	O
as	O
it	O
can	O
grossly	O
over	O
-	O
estimate	O
the	O
algorithm	O
’s	O
performance	O
.	O

The	O
typical	O
practice	O
is	O
instead	O
to	O
train	O
on	O
a	O
training	O
set	O
then	O
evaluate	O
on	O
a	O
disjoint	O
test	O
set	O
.	O

With	O
the	O
large	O
number	O
of	O
available	O
games	O
in	O
ALE	Method
,	O
we	O
propose	O
that	O
a	O
similar	O
methodology	O
can	O
be	O
used	O
to	O
the	O
same	O
effect	O
:	O
an	O
approach	O
’s	O
domain	Method
representation	Method
and	O
parametrization	Method
should	O
be	O
first	O
tuned	O
on	O
a	O
small	O
number	O
of	O
training	O
games	O
,	O
before	O
testing	O
the	O
approach	O
on	O
unseen	Material
testing	Material
games	Material
.	O

Ideally	O
,	O
agents	O
designed	O
in	O
this	O
fashion	O
are	O
evaluated	O
on	O
the	O
testing	O
games	O
only	O
once	O
,	O
with	O
no	O
possibility	O
for	O
subsequent	O
modifications	O
to	O
the	O
algorithm	O
.	O

While	O
general	Method
competency	Method
remains	O
the	O
long	O
-	O
term	O
goal	O
for	O
artificial	Task
intelligence	Task
,	O
ALE	Method
proposes	O
an	O
achievable	O
stepping	O
stone	O
:	O
techniques	O
for	O
general	Task
competency	Task
across	O
the	O
gamut	O
of	O
Atari	Material
2600	Material
games	Material
.	O

We	O
believe	O
this	O
represents	O
a	O
goal	O
that	O
is	O
attainable	O
in	O
a	O
short	O
time	O
-	O
frame	O
yet	O
formidable	O
enough	O
to	O
require	O
new	O
technological	O
breakthroughs	O
.	O

section	O
:	O
Arcade	Method
Learning	Method
Environment	Method
We	O
begin	O
by	O
describing	O
our	O
main	O
contribution	O
,	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
.	O

ALE	Method
is	O
a	O
software	Method
framework	Method
designed	O
to	O
make	O
it	O
easy	O
to	O
develop	O
agents	O
that	O
play	O
arbitrary	O
Atari	Material
2600	Material
games	Material
.	O

subsection	O
:	O
The	O
Atari	Material
2600	Material
The	O
Atari	Method
2600	Method
is	O
a	O
home	Method
video	Method
game	Method
console	Method
developed	O
in	O
1977	O
and	O
sold	O
for	O
over	O
a	O
decade	O
.	O

It	O
popularized	O
the	O
use	O
of	O
general	Method
purpose	Method
CPUs	Method
in	O
game	O
console	O
hardware	O
,	O
with	O
game	O
code	O
distributed	O
through	O
cartridges	O
.	O

Over	O
500	O
original	O
games	O
were	O
released	O
for	O
the	O
console	O
;	O
“	O
homebrew	Material
”	Material
games	Material
continue	O
to	O
be	O
developed	O
today	O
,	O
over	O
thirty	O
years	O
later	O
.	O

The	O
console	O
’s	O
joystick	O
,	O
as	O
well	O
as	O
some	O
of	O
the	O
original	O
games	O
such	O
as	O
Adventure	O
and	O
Pitfall	Material
!	Material

,	O
are	O
iconic	O
symbols	O
of	O
early	O
video	Material
games	Material
.	O

Nearly	O
all	O
arcade	Method
games	Method
of	O
the	O
time	O
–	O
Pac	Method
-	Method
Man	Method
and	O
Space	Task
Invaders	Task
are	O
two	O
well	O
-	O
known	O
examples	O
–	O
were	O
ported	O
to	O
the	O
console	O
.	O

Despite	O
the	O
number	O
and	O
variety	O
of	O
games	O
developed	O
for	O
the	O
Atari	Material
2600	Material
,	O
the	O
hardware	O
is	O
relatively	O
simple	O
.	O

It	O
has	O
a	O
1.19Mhz	Method
CPU	Method
and	O
can	O
be	O
emulated	O
much	O
faster	O
than	O
real	O
-	O
time	O
on	O
modern	O
hardware	O
.	O

The	O
cartridge	O
ROM	O
(	O
typically	O
2–4kB	O
)	O
holds	O
the	O
game	O
code	O
,	O
while	O
the	O
console	O
RAM	O
itself	O
only	O
holds	O
128	O
bytes	O
(	O
1024	O
bits	O
)	O
.	O

A	O
single	O
game	O
screen	O
is	O
160	O
pixels	O
wide	O
and	O
210	O
pixels	O
high	O
,	O
with	O
a	O
128	O
-	O
colour	O
palette	O
;	O
18	O
“	O
actions	O
”	O
can	O
be	O
input	O
to	O
the	O
game	O
via	O
a	O
digital	O
joystick	O
:	O
three	O
positions	O
of	O
the	O
joystick	O
for	O
each	O
axis	O
,	O
plus	O
a	O
single	O
button	O
.	O

The	O
Atari	Method
2600	Method
hardware	Method
limits	O
the	O
possible	O
complexity	O
of	O
games	O
,	O
which	O
we	O
believe	O
strikes	O
the	O
perfect	O
balance	O
:	O
a	O
challenging	O
platform	O
offering	O
conceivable	O
near	O
-	O
term	O
advancements	O
in	O
learning	Task
,	O
modelling	Task
,	O
and	O
planning	Task
.	O

subsection	O
:	O
Interface	O
ALE	Method
is	O
built	O
on	O
top	O
of	O
Stella	Method
,	O
an	O
open	Method
-	Method
source	Method
Atari	Method
2600	Method
emulator	Method
.	O

It	O
allows	O
the	O
user	O
to	O
interface	O
with	O
the	O
Atari	Material
2600	Material
by	O
receiving	O
joystick	O
motions	O
,	O
sending	O
screen	O
and	O
/	O
or	O
RAM	O
information	O
,	O
and	O
emulating	O
the	O
platform	O
.	O

ALE	Method
also	O
provides	O
a	O
game	Method
-	Method
handling	Method
layer	Method
which	O
transforms	O
each	O
game	O
into	O
a	O
standard	O
reinforcement	Task
learning	Task
problem	Task
by	O
identifying	O
the	O
accumulated	O
score	O
and	O
whether	O
the	O
game	O
has	O
ended	O
.	O

By	O
default	O
,	O
each	O
observation	O
consists	O
of	O
a	O
single	O
game	O
screen	O
(	O
frame	O
)	O
:	O
a	O
2D	O
array	O
of	O
7	O
-	O
bit	O
pixels	O
,	O
160	O
pixels	O
wide	O
by	O
210	O
pixels	O
high	O
.	O

The	O
action	O
space	O
consists	O
of	O
the	O
18	O
discrete	O
actions	O
defined	O
by	O
the	O
joystick	Method
controller	Method
.	O

The	O
game	Method
-	Method
handling	Method
layer	Method
also	O
specifies	O
the	O
minimal	O
set	O
of	O
actions	O
needed	O
to	O
play	O
a	O
particular	O
game	O
,	O
although	O
none	O
of	O
the	O
results	O
in	O
this	O
paper	O
make	O
use	O
of	O
this	O
information	O
.	O

When	O
running	O
in	O
real	O
-	O
time	O
,	O
the	O
simulator	O
generates	O
60	O
frames	O
per	O
second	O
,	O
and	O
at	O
full	O
speed	O
emulates	O
up	O
to	O
6000	O
frames	O
per	O
second	O
.	O

The	O
reward	O
at	O
each	O
time	O
-	O
step	O
is	O
defined	O
on	O
a	O
game	O
by	O
game	O
basis	O
,	O
typically	O
by	O
taking	O
the	O
difference	O
in	O
score	O
or	O
points	O
between	O
frames	O
.	O

An	O
episode	O
begins	O
on	O
the	O
first	O
frame	O
after	O
a	O
reset	O
command	O
is	O
issued	O
,	O
and	O
terminates	O
when	O
the	O
game	O
ends	O
.	O

The	O
game	Method
-	Method
handling	Method
layer	Method
also	O
offers	O
the	O
ability	O
to	O
end	O
the	O
episode	O
after	O
a	O
predefined	O
number	O
of	O
frames	O
.	O

The	O
user	O
therefore	O
has	O
access	O
to	O
several	O
dozen	O
games	O
through	O
a	O
single	O
common	O
interface	O
,	O
and	O
adding	O
support	O
for	O
new	O
games	O
is	O
relatively	O
straightforward	O
.	O

ALE	Method
further	O
provides	O
the	O
functionality	O
to	O
save	O
and	O
restore	O
the	O
state	O
of	O
the	O
emulator	O
.	O

When	O
issued	O
a	O
save	O
-	O
state	O
command	O
,	O
ALE	Method
saves	O
all	O
the	O
relevant	O
data	O
about	O
the	O
current	O
game	O
,	O
including	O
the	O
contents	O
of	O
the	O
RAM	O
,	O
registers	O
,	O
and	O
address	O
counters	O
.	O

The	O
restore	O
-	O
state	O
command	O
similarly	O
resets	O
the	O
game	O
to	O
a	O
previously	O
saved	O
state	O
.	O

This	O
allows	O
the	O
use	O
of	O
ALE	Method
as	O
a	O
generative	Method
model	Method
to	O
study	O
topics	O
such	O
as	O
planning	Task
and	O
model	Task
-	Task
based	Task
reinforcement	Task
learning	Task
.	O

subsection	O
:	O
Source	O
Code	O
ALE	Method
is	O
released	O
as	O
free	O
,	O
open	O
-	O
source	O
software	O
under	O
the	O
terms	O
of	O
the	O
GNU	O
General	O
Public	O
License	O
.	O

The	O
latest	O
version	O
of	O
the	O
source	O
code	O
is	O
publicly	O
available	O
at	O
:	O
The	O
source	O
code	O
for	O
the	O
agents	O
used	O
in	O
the	O
benchmark	O
experiments	O
below	O
is	O
also	O
available	O
on	O
the	O
publication	O
page	O
for	O
this	O
article	O
on	O
the	O
same	O
website	O
.	O

While	O
ALE	Method
itself	O
is	O
written	O
in	O
C	Method
++	Method
,	O
a	O
variety	O
of	O
interfaces	O
are	O
available	O
that	O
allow	O
users	O
to	O
interact	O
with	O
ALE	Method
in	O
the	O
programming	O
language	O
of	O
their	O
choice	O
.	O

Support	O
for	O
new	O
games	O
is	O
easily	O
added	O
by	O
implementing	O
a	O
derived	Method
class	Method
representing	O
the	O
game	O
’s	O
particular	O
reward	O
and	O
termination	O
functions	O
.	O

section	O
:	O
Benchmark	O
Results	O
Planning	Task
and	O
reinforcement	Method
learning	Method
are	O
two	O
different	O
AI	Task
problem	Task
formulations	Task
that	O
can	O
naturally	O
be	O
investigated	O
within	O
the	O
ALE	Method
framework	O
.	O

Our	O
purpose	O
in	O
presenting	O
benchmark	O
results	O
for	O
both	O
of	O
these	O
formulations	O
is	O
two	O
-	O
fold	O
.	O

First	O
,	O
these	O
results	O
provide	O
a	O
baseline	O
performance	O
for	O
traditional	O
techniques	O
,	O
establishing	O
a	O
point	O
of	O
comparison	O
with	O
future	O
,	O
more	O
advanced	O
,	O
approaches	O
.	O

Second	O
,	O
in	O
describing	O
these	O
results	O
we	O
illustrate	O
our	O
proposed	O
methodology	O
for	O
doing	O
empirical	Task
validation	Task
with	O
ALE	Method
.	O

subsection	O
:	O
Reinforcement	Method
Learning	Method
We	O
begin	O
by	O
providing	O
benchmark	O
results	O
using	O
SARSA	Method
,	O
a	O
traditional	O
technique	O
for	O
model	Method
-	Method
free	Method
reinforcement	Method
learning	Method
.	O

Note	O
that	O
in	O
the	O
reinforcement	Task
learning	Task
setting	Task
,	O
the	O
agent	O
does	O
not	O
have	O
access	O
to	O
a	O
model	O
of	O
the	O
game	O
dynamics	O
.	O

At	O
each	O
time	O
step	O
,	O
the	O
agent	O
selects	O
an	O
action	O
and	O
receives	O
a	O
reward	O
and	O
an	O
observation	O
,	O
and	O
the	O
agent	O
’s	O
aim	O
is	O
to	O
maximize	O
its	O
accumulated	O
reward	O
.	O

In	O
these	O
experiments	O
,	O
we	O
augmented	O
the	O
SARSA	Method
(	Method
)	Method
algorithm	Method
with	O
linear	Method
function	Method
approximation	Method
,	O
replacing	Method
traces	Method
,	O
and	O
-	Method
greedy	Method
exploration	Method
.	O

A	O
detailed	O
explanation	O
of	O
SARSA	Method
(	Method
)	O
and	O
its	O
extensions	O
can	O
be	O
found	O
in	O
the	O
work	O
of	O
sutton_barto_98	O
.	O

subsubsection	Method
:	O
Feature	Method
Construction	Method
In	O
our	O
approach	O
to	O
the	O
reinforcement	Task
learning	Task
setting	Task
,	O
the	O
most	O
important	O
design	O
issue	O
is	O
the	O
choice	O
of	O
features	O
to	O
use	O
with	O
linear	Method
function	Method
approximation	Method
.	O

We	O
ran	O
experiments	O
using	O
five	O
different	O
sets	O
of	O
features	O
,	O
which	O
we	O
now	O
briefly	O
explain	O
;	O
a	O
complete	O
description	O
of	O
these	O
feature	O
sets	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Of	O
these	O
sets	O
of	O
features	O
,	O
BASS	O
,	O
DISCO	Method
and	O
RAM	Method
were	O
originally	O
introduced	O
by	O
naddaf2010	O
,	O
while	O
the	O
rest	O
are	O
novel	O
.	O

paragraph	O
:	O
Basic	O
.	O

The	O
Basic	O
method	O
,	O
derived	O
from	O
naddaf2010	Method
’s	Method
BASS	Method
naddaf2010	Method
,	O
encodes	O
the	O
presence	O
of	O
colours	O
on	O
the	O
Atari	Material
2600	Material
screen	Material
.	O

The	O
Basic	O
method	O
first	O
removes	O
the	O
image	O
background	O
by	O
storing	O
the	O
frequency	O
of	O
colours	O
at	O
each	O
pixel	O
location	O
within	O
a	O
histogram	O
.	O

Each	O
game	Material
background	Material
is	O
precomputed	O
offline	O
,	O
using	O
18	O
,	O
000	O
observations	O
collected	O
from	O
sample	O
trajectories	O
.	O

The	O
sample	O
trajectories	O
are	O
generated	O
by	O
following	O
a	O
human	O
-	O
provided	O
trajectory	O
for	O
a	O
random	O
number	O
of	O
steps	O
and	O
subsequently	O
selecting	O
actions	O
uniformly	O
at	O
random	O
.	O

The	O
screen	O
is	O
then	O
divided	O
into	O
tiles	O
.	O

Basic	O
generates	O
one	O
binary	O
feature	O
for	O
each	O
of	O
the	O
colours	O
and	O
each	O
of	O
the	O
tiles	O
,	O
giving	O
a	O
total	O
of	O
28	O
,	O
672	O
features	O
.	O

paragraph	O
:	O
BASS	O
.	O

The	O
BASS	Method
method	Method
behaves	O
identically	O
to	O
the	O
Basic	O
method	O
save	O
in	O
two	O
respects	O
.	O

First	O
,	O
BASS	Method
augments	O
the	O
Basic	O
feature	O
set	O
with	O
pairwise	O
combinations	O
of	O
its	O
features	O
.	O

Second	O
,	O
BASS	Method
uses	O
a	O
smaller	O
,	O
8	Method
-	Method
colour	Method
encoding	Method
to	O
ensure	O
that	O
the	O
number	O
of	O
pairwise	O
combinations	O
remains	O
tractable	O
.	O

paragraph	O
:	O
DISCO	Method
.	O

The	O
DISCO	Method
method	Method
aims	O
to	O
detect	O
objects	O
within	O
the	O
Atari	Material
2600	Material
screen	Material
.	O

To	O
do	O
so	O
,	O
it	O
first	O
preprocesses	O
36	O
,	O
000	O
observations	O
from	O
sample	O
trajectories	O
generated	O
as	O
in	O
the	O
Basic	O
method	O
.	O

DISCO	Method
also	O
performs	O
the	O
background	Method
subtraction	Method
steps	Method
as	O
in	O
Basic	O
and	O
BASS	O
.	O

Extracted	O
objects	O
are	O
then	O
labelled	O
into	O
classes	O
.	O

During	O
the	O
actual	O
training	O
,	O
DISCO	Method
infers	O
the	O
class	O
label	O
of	O
detected	O
objects	O
and	O
encodes	O
their	O
position	O
and	O
velocity	O
using	O
tile	Method
coding	Method
.	O

paragraph	O
:	O
LSH	O
.	O

The	O
LSH	Method
method	Method
maps	O
raw	O
Atari	Material
2600	Material
screens	Material
into	O
a	O
small	O
set	O
of	O
binary	O
features	O
using	O
Locally	Method
Sensitive	Method
Hashing	Method
.	O

The	O
screens	O
are	O
mapped	O
using	O
random	O
projections	O
,	O
such	O
that	O
visually	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
generate	O
the	O
same	O
features	O
.	O

paragraph	O
:	O
RAM	O
.	O

The	O
RAM	Method
method	Method
works	O
on	O
an	O
entirely	O
different	O
observation	O
space	O
than	O
the	O
other	O
four	O
methods	O
.	O

Rather	O
than	O
receiving	O
in	O
Atari	Material
2600	Material
screen	Material
as	O
an	O
observation	O
,	O
it	O
directly	O
observes	O
the	O
Atari	O
2600	O
’s	O
1024	O
bits	O
of	O
memory	O
.	O

Each	O
bit	O
of	O
RAM	O
is	O
provided	O
as	O
a	O
binary	O
feature	O
together	O
with	O
the	O
pairwise	O
logical	O
-	O
AND	O
of	O
every	O
pair	O
of	O
bits	O
.	O

subsubsection	O
:	O
Evaluation	O
Methodology	O
We	O
first	O
constructed	O
two	O
sets	O
of	O
games	O
,	O
one	O
for	O
training	O
and	O
the	O
other	O
for	O
testing	O
.	O

We	O
used	O
the	O
training	O
games	O
for	O
parameter	Task
tuning	Task
as	O
well	O
as	O
design	Task
refinements	Task
,	O
and	O
the	O
testing	O
games	O
for	O
the	O
final	O
evaluation	O
of	O
our	O
methods	O
.	O

Our	O
training	O
set	O
consisted	O
of	O
five	O
games	O
:	O
Asterix	O
,	O
Beam	Task
Rider	Task
,	O
Freeway	Material
,	O
Seaquest	Material
and	O
Space	O
Invaders	O
.	O

The	O
parameter	Task
search	Task
involved	O
finding	O
suitable	O
values	O
for	O
the	O
parameters	O
to	O
the	O
SARSA	Method
(	Method
)	Method
algorithm	Method
,	O
i.e.	O
the	O
learning	Metric
rate	Metric
,	O
exploration	Metric
rate	Metric
,	O
discount	O
factor	O
,	O
and	O
the	O
decay	Metric
rate	Metric
.	O

We	O
also	O
searched	O
the	O
space	O
of	O
feature	O
generation	O
parameters	O
,	O
for	O
example	O
the	O
abstraction	O
level	O
for	O
the	O
BASS	Method
agent	Method
.	O

The	O
results	O
of	O
our	O
parameter	Method
search	Method
are	O
summarized	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Our	O
testing	O
set	O
was	O
constructed	O
by	O
choosing	O
semi	O
-	O
randomly	O
from	O
the	O
381	O
games	O
listed	O
on	O
Wikipedia	Material
at	O
the	O
time	O
of	O
writing	O
.	O

Of	O
these	O
games	O
,	O
123	O
games	O
have	O
their	O
own	O
Wikipedia	Material
page	Material
,	O
have	O
a	O
single	O
player	O
mode	O
,	O
are	O
not	O
adult	O
-	O
themed	O
or	O
prototypes	O
,	O
and	O
can	O
be	O
emulated	O
in	O
ALE	Method
.	O

From	O
this	O
list	O
,	O
50	O
games	O
were	O
chosen	O
at	O
random	O
to	O
form	O
the	O
test	O
set	O
.	O

Evaluation	O
of	O
each	O
method	O
on	O
each	O
game	O
was	O
performed	O
as	O
follows	O
.	O

An	O
episode	O
starts	O
on	O
the	O
frame	O
that	O
follows	O
the	O
reset	O
command	O
,	O
and	O
terminates	O
when	O
the	O
end	O
-	O
of	O
-	O
game	O
condition	O
is	O
detected	O
or	O
after	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
whichever	O
comes	O
first	O
.	O

During	O
an	O
episode	O
,	O
the	O
agent	O
acts	O
every	O
5	O
frames	O
,	O
or	O
equivalently	O
12	O
times	O
per	O
second	O
of	O
gameplay	O
.	O

A	O
reinforcement	Method
learning	Method
trial	Method
consists	O
of	O
5	O
,	O
000	O
training	O
episodes	O
,	O
followed	O
by	O
500	O
evaluation	O
episodes	O
during	O
which	O
no	O
learning	O
takes	O
place	O
.	O

The	O
agent	O
’s	O
performance	O
is	O
measured	O
as	O
the	O
average	Metric
score	Metric
achieved	O
during	O
the	O
evaluation	O
episodes	O
.	O

For	O
each	O
game	O
,	O
we	O
report	O
our	O
methods	O
’	O
average	O
performance	O
across	O
30	O
trials	O
.	O

For	O
purposes	O
of	O
comparison	O
,	O
we	O
also	O
provide	O
performance	O
measures	O
for	O
three	O
simple	O
baseline	O
agents	O
–	O
Random	O
,	O
Const	O
and	O
Perturb	Method
–	O
as	O
well	O
as	O
the	O
performance	O
of	O
a	O
non	O
-	O
expert	Method
human	Method
player	Method
.	O

The	O
Random	Method
agent	Method
picks	O
a	O
random	O
action	O
on	O
every	O
frame	O
.	O

The	O
Const	Method
agent	Method
selects	O
a	O
single	O
fixed	O
action	O
throughout	O
an	O
episode	O
;	O
our	O
results	O
reflect	O
the	O
highest	O
score	O
achieved	O
by	O
any	O
single	O
action	O
within	O
each	O
game	O
.	O

The	O
Perturb	Method
agent	Method
selects	O
a	O
fixed	O
action	O
with	O
probability	O
0.95	O
and	O
otherwise	O
acts	O
uniformly	O
randomly	O
;	O
for	O
each	O
game	O
,	O
we	O
report	O
the	O
performance	O
of	O
the	O
best	O
policy	O
of	O
this	O
type	O
.	O

Additionally	O
,	O
we	O
provide	O
human	O
player	O
results	O
that	O
report	O
the	O
five	O
-	O
episode	Metric
average	Metric
score	Metric
obtained	O
by	O
a	O
beginner	O
(	O
who	O
had	O
never	O
previously	O
played	O
Atari	O
2600	O
games	O
)	O
playing	O
selected	O
games	O
.	O

Our	O
aim	O
is	O
not	O
to	O
provide	O
exhaustive	O
or	O
accurate	O
human	Metric
-	Metric
level	Metric
benchmarks	Metric
,	O
which	O
would	O
be	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
,	O
but	O
rather	O
to	O
offer	O
insight	O
into	O
the	O
performance	O
level	O
achieved	O
by	O
our	O
agents	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
reinforcement	Method
learning	Method
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
a	O
small	O
subset	O
of	O
results	O
from	O
two	O
training	O
games	O
and	O
three	O
test	O
games	O
.	O

In	O
40	O
games	O
out	O
of	O
55	O
,	O
learning	Method
agents	Method
perform	O
better	O
than	O
the	O
baseline	O
agents	O
.	O

In	O
some	O
games	O
,	O
e.g.	O
,	O
Double	Task
Dunk	Task
,	O
Journey	Task
Escape	Task
and	O
Tennis	Task
,	O
the	O
no	Method
-	Method
action	Method
baseline	Method
policy	Method
performs	O
the	O
best	O
by	O
essentially	O
refusing	O
to	O
play	O
and	O
thus	O
incurring	O
no	O
negative	O
reward	O
.	O

Within	O
the	O
40	O
games	O
for	O
which	O
learning	Task
occurs	O
,	O
the	O
BASS	Method
method	Method
generally	O
performs	O
best	O
.	O

DISCO	Method
performed	O
particularly	O
poorly	O
compared	O
to	O
the	O
other	O
learning	Method
methods	Method
.	O

The	O
RAM	Method
-	Method
based	Method
agent	Method
,	O
surprisingly	O
,	O
did	O
not	O
outperform	O
image	Method
-	Method
based	Method
methods	Method
,	O
despite	O
building	O
its	O
representation	O
from	O
raw	O
game	O
state	O
.	O

It	O
appears	O
the	O
screen	Material
image	Material
carries	O
structural	O
information	O
that	O
is	O
not	O
easily	O
extracted	O
from	O
the	O
RAM	O
bits	O
.	O

Our	O
reinforcement	Method
learning	Method
results	O
show	O
that	O
while	O
some	O
learning	Task
progress	O
is	O
already	O
possible	O
in	O
Atari	Material
2600	Material
games	Material
,	O
much	O
more	O
work	O
remains	O
to	O
be	O
done	O
.	O

Different	O
methods	O
perform	O
well	O
on	O
different	O
games	O
,	O
and	O
no	O
single	O
method	O
performs	O
well	O
on	O
all	O
games	O
.	O

Some	O
games	O
are	O
particularly	O
challenging	O
.	O

For	O
example	O
,	O
platformers	O
such	O
as	O
Montezuma	Material
’s	Material
Revenge	Material
seem	O
to	O
require	O
high	O
-	O
level	O
planning	O
far	O
beyond	O
what	O
our	O
current	O
,	O
domain	Method
-	Method
independent	Method
methods	Method
provide	O
.	O

Tennis	Method
requires	O
fairly	O
elaborate	O
behaviour	O
before	O
observing	O
any	O
positive	O
reward	O
,	O
but	O
simple	O
behaviour	O
can	O
avoid	O
negative	O
rewards	O
.	O

Our	O
results	O
also	O
highlight	O
the	O
value	O
of	O
ALE	Method
as	O
an	O
experimental	O
methodology	O
.	O

For	O
example	O
,	O
the	O
DISCO	Method
approach	Method
performs	O
reasonably	O
well	O
on	O
the	O
training	Material
set	Material
,	O
but	O
suffers	O
a	O
dramatic	O
reduction	O
in	O
performance	O
when	O
applied	O
to	O
unseen	Material
games	Material
.	O

This	O
suggests	O
the	O
method	O
is	O
less	O
robust	O
than	O
the	O
other	O
methods	O
we	O
studied	O
.	O

After	O
a	O
quick	O
glance	O
at	O
the	O
full	O
table	O
of	O
results	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
it	O
is	O
clear	O
that	O
summarizing	O
results	O
across	O
such	O
varied	O
domains	O
needs	O
further	O
attention	O
;	O
we	O
explore	O
this	O
issue	O
further	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Planning	Task
The	O
Arcade	Method
Learning	Method
Environment	Method
can	O
naturally	O
be	O
used	O
to	O
study	O
planning	Method
techniques	Method
by	O
using	O
the	O
emulator	Method
itself	Method
as	O
a	O
generative	Method
model	Method
.	O

Initially	O
it	O
may	O
seem	O
that	O
allowing	O
the	O
agent	O
to	O
plan	O
into	O
the	O
future	O
with	O
a	O
perfect	O
model	O
trivializes	O
the	O
problem	O
.	O

However	O
,	O
this	O
is	O
not	O
the	O
case	O
:	O
the	O
size	O
of	O
state	O
space	O
in	O
Atari	Material
2600	Material
games	Material
prohibits	O
exhaustive	Method
search	Method
.	O

Eighteen	O
different	O
actions	O
are	O
available	O
at	O
every	O
frame	O
;	O
at	O
60	O
frames	O
per	O
second	O
,	O
looking	O
ahead	O
one	O
second	O
requires	O
simulation	O
steps	O
.	O

Furthermore	O
,	O
rewards	O
are	O
often	O
sparsely	O
distributed	O
,	O
which	O
causes	O
significant	O
horizon	O
effects	O
in	O
many	O
search	Method
algorithms	Method
.	O

subsubsection	Method
:	O
Search	Method
Methods	Method
We	O
now	O
provide	O
benchmark	O
ALE	Method
results	O
for	O
two	O
traditional	O
search	Method
methods	Method
.	O

Each	O
method	O
was	O
applied	O
online	O
to	O
select	O
an	O
action	O
at	O
every	O
time	O
step	O
(	O
every	O
five	O
frames	O
)	O
until	O
the	O
game	O
was	O
over	O
.	O

paragraph	O
:	O
Breadth	Method
-	Method
first	Method
Search	Method
.	O

Our	O
first	O
approach	O
builds	O
a	O
search	O
tree	O
in	O
a	O
breadth	O
-	O
first	O
fashion	O
until	O
a	O
node	O
limit	O
is	O
reached	O
.	O

Once	O
the	O
tree	O
is	O
expanded	O
,	O
node	O
values	O
are	O
updated	O
recursively	O
from	O
the	O
bottom	O
of	O
the	O
tree	O
to	O
the	O
root	O
.	O

The	O
agent	O
then	O
selects	O
the	O
action	O
corresponding	O
to	O
the	O
branch	O
with	O
the	O
highest	O
discounted	O
sum	O
of	O
rewards	O
.	O

Expanding	O
the	O
full	O
search	O
tree	O
requires	O
a	O
large	O
number	O
of	O
simulation	O
steps	O
.	O

For	O
instance	O
,	O
selecting	O
an	O
action	O
every	O
5	O
frames	O
and	O
allowing	O
a	O
maximum	O
of	O
100	O
,	O
000	O
simulation	O
steps	O
per	O
frame	O
,	O
the	O
agent	O
can	O
only	O
look	O
ahead	O
about	O
a	O
third	O
of	O
a	O
second	O
.	O

In	O
many	O
games	O
,	O
this	O
allows	O
the	O
agent	O
to	O
collect	O
immediate	O
rewards	O
and	O
avoid	O
death	O
but	O
little	O
else	O
.	O

For	O
example	O
,	O
in	O
Seaquest	O
the	O
agent	O
must	O
collect	O
a	O
swimmer	O
and	O
return	O
to	O
the	O
surface	O
before	O
running	O
out	O
of	O
air	O
,	O
which	O
involves	O
planning	O
far	O
beyond	O
one	O
second	O
.	O

paragraph	O
:	O
UCT	Method
:	O
Upper	Method
Confidence	Method
Bounds	Method
Applied	O
to	O
Trees	O
.	O

A	O
preferable	O
alternative	O
to	O
exhaustively	O
expanding	O
the	O
tree	O
is	O
to	O
simulate	O
deeper	O
into	O
the	O
more	O
promising	O
branches	O
.	O

To	O
do	O
this	O
,	O
we	O
need	O
to	O
find	O
a	O
balance	O
between	O
expanding	O
the	O
higher	O
-	O
valued	O
branches	O
and	O
spending	O
simulation	O
steps	O
on	O
the	O
lower	O
-	O
valued	O
branches	O
to	O
get	O
a	O
better	O
estimate	O
of	O
their	O
values	O
.	O

The	O
UCT	Method
algorithm	Method
,	O
developed	O
by	O
kocsis_06	Method
,	O
deals	O
with	O
the	O
exploration	Task
-	Task
exploitation	Task
dilemma	Task
by	O
treating	O
each	O
node	O
of	O
a	O
search	O
tree	O
as	O
a	O
multi	Task
-	Task
armed	Task
bandit	Task
problem	Task
.	O

UCT	Method
uses	O
a	O
variation	O
of	O
UCB1	Method
,	O
a	O
bandit	Method
algorithm	Method
,	O
to	O
choose	O
which	O
child	O
node	O
to	O
visit	O
next	O
.	O

A	O
common	O
practice	O
is	O
to	O
apply	O
a	O
-	O
step	O
random	Method
simulation	Method
at	O
the	O
end	O
of	O
each	O
leaf	O
node	O
to	O
obtain	O
an	O
estimate	O
from	O
a	O
longer	O
trajectory	O
.	O

By	O
expanding	O
the	O
more	O
valuable	O
branches	O
of	O
the	O
tree	O
and	O
carrying	O
out	O
a	O
random	O
simulation	O
at	O
the	O
leaf	O
nodes	O
,	O
UCT	Method
is	O
known	O
to	O
perform	O
well	O
in	O
many	O
different	O
settings	O
mcts_survery2012	Method
.	O

Our	O
UCT	Method
implementation	Method
was	O
entirely	O
standard	O
,	O
except	O
for	O
one	O
optimization	Task
.	O

Few	O
Atari	Method
games	Method
actually	O
distinguish	O
between	O
all	O
18	O
actions	O
at	O
every	O
time	O
step	O
.	O

In	O
Beam	Task
Rider	Task
,	O
for	O
example	O
,	O
the	O
down	O
action	O
does	O
nothing	O
,	O
and	O
pressing	O
the	O
button	O
when	O
a	O
bullet	O
has	O
already	O
been	O
shot	O
has	O
no	O
effect	O
.	O

We	O
exploit	O
this	O
fact	O
as	O
follows	O
:	O
after	O
expanding	O
the	O
children	O
of	O
a	O
node	O
in	O
the	O
search	O
tree	O
,	O
we	O
compare	O
the	O
resulting	O
emulator	O
states	O
.	O

Actions	O
that	O
result	O
in	O
the	O
same	O
state	O
are	O
treated	O
as	O
duplicates	O
and	O
only	O
one	O
of	O
the	O
actions	O
is	O
considered	O
in	O
the	O
search	O
tree	O
.	O

This	O
reduces	O
the	O
branching	O
factor	O
,	O
thus	O
allowing	O
deeper	Task
search	Task
.	O

At	O
every	O
step	O
,	O
we	O
also	O
reuse	O
the	O
part	O
of	O
our	O
search	O
tree	O
corresponding	O
to	O
the	O
selected	O
action	O
.	O

Pseudocode	O
for	O
our	O
implementation	O
of	O
the	O
UCT	Method
algorithm	Method
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Experimental	O
Setup	O
We	O
designed	O
and	O
tuned	O
our	O
algorithms	O
based	O
on	O
the	O
same	O
five	O
training	O
games	O
used	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
subsequently	O
evaluated	O
the	O
methods	O
on	O
the	O
fifty	O
games	O
of	O
the	O
testing	O
set	O
.	O

The	O
training	Material
games	Material
were	O
used	O
to	O
determine	O
the	O
length	O
of	O
the	O
search	O
horizon	O
as	O
well	O
as	O
the	O
constant	O
controlling	O
the	O
amount	O
of	O
exploration	O
at	O
internal	O
nodes	O
of	O
the	O
tree	O
.	O

Each	O
episode	O
was	O
set	O
to	O
last	O
up	O
to	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
with	O
actions	O
selected	O
every	O
5	O
frames	O
,	O
matching	O
our	O
settings	O
in	O
Section	O
[	O
reference	O
]	O
.	O

On	O
average	O
,	O
each	O
action	Method
selection	Method
step	O
took	O
on	O
the	O
order	O
of	O
15	O
seconds	O
.	O

We	O
also	O
used	O
the	O
same	O
discount	O
factor	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
ran	O
our	O
algorithms	O
for	O
10	O
episodes	O
per	O
game	O
.	O

Details	O
of	O
the	O
algorithmic	O
parameters	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
search	O
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
results	O
on	O
a	O
selected	O
subset	O
of	O
games	O
.	O

For	O
reference	O
purposes	O
,	O
we	O
also	O
include	O
the	O
performance	O
of	O
the	O
best	O
learning	Method
agent	Method
and	O
the	O
best	O
baseline	O
policy	O
from	O
Table	O
[	O
reference	O
]	O
.	O

Together	O
,	O
our	O
two	O
search	Method
methods	Method
performed	O
better	O
than	O
both	O
learning	Method
agents	Method
and	O
the	O
baseline	Method
policies	Method
on	O
49	O
of	O
55	O
games	O
.	O

In	O
most	O
cases	O
,	O
UCT	Method
performs	O
significantly	O
better	O
than	O
breadth	Method
-	Method
first	Method
search	Method
.	O

Four	O
of	O
the	O
six	O
games	O
for	O
which	O
search	Method
methods	Method
do	O
not	O
perform	O
best	O
are	O
games	O
where	O
rewards	O
are	O
sparse	O
and	O
require	O
long	O
-	O
term	O
planning	O
.	O

These	O
are	O
Freeway	O
,	O
Private	O
Eye	O
,	O
Montezuma	O
’s	O
Revenge	O
and	O
Venture	O
.	O

section	O
:	O
Evaluation	Metric
Metrics	Metric
for	O
General	O
Atari	Task
2600	Task
Agents	Task
Applying	O
algorithms	O
to	O
a	O
large	O
set	O
of	O
games	O
as	O
we	O
did	O
in	O
Sections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
presents	O
difficulties	O
when	O
interpreting	O
the	O
results	O
.	O

While	O
the	O
agent	O
’s	O
goal	O
in	O
all	O
games	O
is	O
to	O
maximize	O
its	O
score	O
,	O
scores	O
for	O
two	O
different	O
games	O
can	O
not	O
be	O
easily	O
compared	O
.	O

Each	O
game	O
uses	O
its	O
own	O
scale	O
for	O
scores	O
,	O
and	O
different	O
game	O
mechanics	O
make	O
some	O
games	O
harder	O
to	O
learn	O
than	O
others	O
.	O

The	O
challenges	O
associated	O
with	O
comparing	O
general	Method
agents	Method
has	O
been	O
previously	O
highlighted	O
by	O
whiteson11	O
.	O

Although	O
we	O
can	O
always	O
report	O
full	O
performance	O
tables	O
,	O
as	O
we	O
did	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
some	O
more	O
compact	O
summary	Metric
statistics	Metric
are	O
also	O
desirable	O
.	O

We	O
now	O
introduce	O
some	O
simple	O
metrics	O
that	O
help	O
compare	O
agents	O
across	O
a	O
diverse	O
set	O
of	O
domains	O
,	O
such	O
as	O
our	O
test	O
set	O
of	O
Atari	Material
2600	Material
games	Material
.	O

subsection	O
:	O
Normalized	Metric
Scores	Metric
Consider	O
the	O
scores	O
and	O
achieved	O
by	O
two	O
algorithms	O
in	O
game	O
.	O

Our	O
goal	O
here	O
is	O
to	O
explore	O
methods	O
that	O
allow	O
us	O
to	O
compare	O
two	O
sets	O
of	O
scores	O
and	O
.	O

The	O
approach	O
we	O
take	O
is	O
to	O
transform	O
into	O
a	O
normalized	O
score	O
with	O
the	O
aim	O
of	O
comparing	O
normalized	O
scores	O
across	O
games	O
;	O
in	O
the	O
ideal	O
case	O
,	O
implies	O
that	O
algorithm	O
performs	O
as	O
well	O
on	O
game	O
as	O
on	O
game	O
.	O

In	O
order	O
to	O
compare	O
algorithms	O
over	O
a	O
set	O
of	O
games	O
,	O
we	O
aggregate	O
normalized	O
scores	O
for	O
each	O
game	O
and	O
each	O
algorithm	O
.	O

The	O
most	O
natural	O
way	O
to	O
compare	O
games	O
with	O
different	O
scoring	O
scales	O
is	O
to	O
normalize	O
scores	O
so	O
that	O
the	O
numerical	O
values	O
become	O
comparable	O
.	O

All	O
of	O
our	O
normalization	Method
methods	Method
are	O
defined	O
using	O
the	O
notion	O
of	O
a	O
score	O
range	O
computed	O
for	O
each	O
game	O
.	O

Given	O
such	O
a	O
score	O
range	O
,	O
score	O
is	O
normalized	O
by	O
computing	O
.	O

subsubsection	Method
:	O
Normalization	Method
to	O
a	O
Reference	O
Score	O
One	O
straightforward	O
method	O
is	O
to	O
normalize	O
to	O
a	O
score	O
range	O
defined	O
by	O
repeated	O
runs	O
of	O
a	O
random	O
agent	O
across	O
each	O
game	O
.	O

Here	O
,	O
is	O
the	O
absolute	O
value	O
of	O
the	O
average	Metric
score	Metric
achieved	O
by	O
the	O
random	Method
agent	Method
,	O
and	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
depicts	O
the	O
random	Metric
-	Metric
normalized	Metric
scores	Metric
achieved	O
by	O
BASS	Method
and	O
RAM	Method
on	O
three	O
games	O
.	O

Two	O
issues	O
arise	O
with	O
this	O
approach	O
:	O
the	O
scale	O
of	O
normalized	O
scores	O
may	O
be	O
excessively	O
large	O
and	O
normalized	O
scores	O
are	O
generally	O
not	O
translation	O
invariant	O
.	O

The	O
issue	O
of	O
scale	O
is	O
best	O
seen	O
in	O
a	O
game	O
such	O
as	O
Freeway	Material
,	O
for	O
which	O
the	O
random	Method
agent	Method
achieves	O
a	O
score	O
close	O
to	O
0	O
:	O
scores	O
achieved	O
by	O
learning	Method
agents	Method
,	O
in	O
the	O
10	O
-	O
20	O
range	O
,	O
are	O
normalized	O
into	O
thousands	O
.	O

By	O
contrast	O
,	O
no	O
learning	Method
agent	Method
achieves	O
a	O
random	Metric
-	Metric
normalized	Metric
score	Metric
greater	O
than	O
1	O
in	O
Asteroids	O
.	O

subsubsection	Method
:	O
Normalizing	O
to	O
a	O
Baseline	O
Set	O
Rather	O
than	O
normalizing	O
to	O
a	O
single	O
reference	O
we	O
may	O
normalize	O
to	O
the	O
score	O
range	O
implied	O
by	O
a	O
set	O
of	O
references	O
.	O

Let	O
be	O
a	O
set	O
of	O
reference	O
scores	O
.	O

A	O
method	O
’s	O
baseline	Metric
score	Metric
is	O
computed	O
using	O
the	O
score	O
range	O
.	O

Given	O
a	O
sufficiently	O
rich	O
set	O
of	O
reference	O
scores	O
,	O
baseline	Method
normalization	Method
allows	O
us	O
to	O
reduce	O
the	O
scores	O
for	O
most	O
games	O
to	O
comparable	O
quantities	O
,	O
and	O
lets	O
us	O
know	O
whether	O
meaningful	O
performance	O
was	O
obtained	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
example	O
baseline	O
scores	O
.	O

The	O
score	O
range	O
for	O
these	O
scores	O
corresponds	O
to	O
the	O
scores	O
achieved	O
by	O
37	O
baseline	O
agents	O
(	O
Section	O
[	O
reference	O
]	O
)	O
:	O
Random	O
,	O
Const	O
(	O
one	O
policy	O
per	O
action	O
)	O
,	O
and	O
Perturb	O
(	O
one	O
policy	O
per	O
action	O
)	O
.	O

A	O
natural	O
idea	O
is	O
to	O
also	O
include	O
scores	O
achieved	O
by	O
human	O
players	O
into	O
the	O
baseline	O
set	O
.	O

For	O
example	O
,	O
one	O
may	O
include	O
the	O
score	O
achieved	O
by	O
an	O
expert	O
as	O
well	O
as	O
the	O
score	O
achieved	O
by	O
a	O
beginner	O
.	O

However	O
,	O
using	O
human	O
scores	O
raises	O
its	O
own	O
set	O
of	O
issues	O
.	O

For	O
example	O
,	O
humans	O
often	O
play	O
games	O
without	O
seeking	O
to	O
maximize	O
score	O
;	O
humans	O
also	O
benefit	O
from	O
prior	O
knowledge	O
that	O
is	O
difficult	O
to	O
incorporate	O
into	O
domain	Method
-	Method
independent	Method
agents	Method
.	O

subsubsection	Method
:	O
Inter	Method
-	Method
Algorithm	Method
Normalization	Method
A	O
third	O
alternative	O
is	O
to	O
normalize	O
using	O
the	O
scores	O
achieved	O
by	O
the	O
algorithms	O
themselves	O
.	O

Given	O
algorithms	O
,	O
each	O
achieving	O
score	O
on	O
game	O
,	O
we	O
define	O
the	O
inter	Metric
-	Metric
algorithm	Metric
score	Metric
using	O
the	O
score	O
range	O
.	O

By	O
definition	O
,	O
.	O

A	O
special	O
case	O
of	O
this	O
is	O
when	O
n=2	O
,	O
where	O
indicates	O
which	O
algorithm	O
is	O
better	O
than	O
the	O
other	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
shows	O
example	O
inter	O
-	O
algorithm	O
scores	O
;	O
the	O
relevant	O
score	O
ranges	O
are	O
constructed	O
from	O
the	O
performance	O
of	O
all	O
five	O
learning	Method
agents	Method
.	O

Because	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
are	O
bounded	O
,	O
this	O
type	O
of	O
normalization	Method
is	O
an	O
appealing	O
solution	O
to	O
compare	O
the	O
relative	O
performance	O
of	O
different	O
methods	O
.	O

Its	O
main	O
drawback	O
is	O
that	O
it	O
gives	O
no	O
indication	O
of	O
the	O
objective	O
performance	O
of	O
the	O
best	O
algorithm	O
.	O

A	O
good	O
example	O
of	O
this	O
is	O
Venture	O
:	O
the	O
inter	O
-	O
algorithm	Metric
score	Metric
of	O
1.0	O
achieved	O
by	O
BASS	Method
does	O
not	O
reflect	O
the	O
fact	O
that	O
none	O
of	O
our	O
agents	O
achieved	O
a	O
score	O
remotely	O
comparable	O
to	O
a	O
human	O
’s	O
performance	O
.	O

The	O
lack	O
of	O
objective	O
reference	O
in	O
inter	O
-	O
algorithm	Method
normalization	Method
suggests	O
that	O
it	O
should	O
be	O
used	O
to	O
complement	O
other	O
scoring	Metric
metrics	Metric
.	O

subsection	O
:	O
Aggregating	O
Scores	O
Once	O
normalized	O
scores	O
are	O
obtained	O
for	O
each	O
game	O
,	O
the	O
next	O
step	O
is	O
to	O
produce	O
a	O
measure	O
that	O
reflects	O
how	O
well	O
each	O
agent	O
performs	O
across	O
the	O
set	O
of	O
games	O
.	O

As	O
illustrated	O
by	O
Table	O
[	O
reference	O
]	O
,	O
a	O
large	O
table	O
of	O
numbers	O
does	O
not	O
easily	O
permit	O
comparison	O
between	O
algorithms	O
.	O

We	O
now	O
describe	O
three	O
methods	O
to	O
aggregate	O
normalized	O
scores	O
.	O

subsubsection	Method
:	O
Average	Metric
Score	Metric
The	O
most	O
straightforward	O
method	O
of	O
aggregating	O
normalized	O
scores	O
is	O
to	O
compute	O
their	O
average	O
.	O

Without	O
perfect	Method
score	Method
normalization	Method
,	O
however	O
,	O
score	Metric
averages	Metric
tend	O
to	O
be	O
heavily	O
influenced	O
by	O
games	O
such	O
as	O
Zaxxon	O
for	O
which	O
baseline	O
scores	O
are	O
high	O
.	O

Averaging	Metric
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
obviates	O
this	O
issue	O
as	O
all	O
scores	O
are	O
bounded	O
between	O
0	O
and	O
1	O
.	O

Figure	O
[	O
reference	O
]	O
displays	O
average	O
baseline	O
and	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
for	O
our	O
learning	Method
agents	Method
.	O

subsubsection	O
:	O
Median	Method
Score	Method
Median	Metric
scores	Metric
are	O
generally	O
more	O
robust	O
to	O
outliers	O
than	O
average	O
scores	O
.	O

The	O
median	O
is	O
obtained	O
by	O
sorting	O
all	O
normalized	O
scores	O
and	O
selecting	O
the	O
middle	O
element	O
(	O
the	O
average	O
of	O
the	O
two	O
middle	O
elements	O
is	O
used	O
if	O
the	O
number	O
of	O
scores	O
is	O
even	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
median	O
baseline	O
and	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
for	O
our	O
learning	Method
agents	Method
.	O

Comparing	O
medians	Metric
and	O
averages	O
in	O
the	O
baseline	Metric
score	Metric
(	O
upper	O
two	O
graphs	O
)	O
illustrates	O
exactly	O
the	O
outlier	Metric
sensitivity	Metric
of	O
the	O
average	Metric
score	Metric
,	O
where	O
the	O
LSH	Method
method	Method
appears	O
dramatically	O
superior	O
due	O
entirely	O
to	O
its	O
performance	O
in	O
Zaxxon	Task
.	O

subsubsection	O
:	O
Score	Method
Distribution	Method
The	O
score	Method
distribution	Method
aggregate	Method
is	O
a	O
natural	O
generalization	O
of	O
the	O
median	Metric
score	Metric
:	O
it	O
shows	O
the	O
fraction	O
of	O
games	O
on	O
which	O
an	O
algorithm	O
achieves	O
a	O
certain	O
normalized	O
score	O
or	O
better	O
.	O

It	O
is	O
essentially	O
a	O
quantile	Method
plot	Method
or	O
inverse	Method
empirical	Method
CDF	Method
.	O

Unlike	O
the	O
average	Metric
and	Metric
median	Metric
scores	Metric
,	O
the	O
score	Metric
distribution	Metric
accurately	O
represents	O
the	O
performance	O
of	O
an	O
agent	O
irrespective	O
of	O
how	O
individual	O
scores	O
are	O
distributed	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
baseline	O
and	O
inter	O
-	O
algorithm	Metric
score	Metric
distributions	Metric
.	O

Score	O
distributions	O
allow	O
us	O
to	O
compare	O
different	O
algorithms	O
at	O
a	O
glance	O
–	O
if	O
one	O
curve	O
is	O
above	O
another	O
,	O
the	O
corresponding	O
method	O
generally	O
obtains	O
higher	O
scores	O
.	O

Using	O
the	O
baseline	Metric
score	Metric
distribution	Metric
,	O
we	O
can	O
easily	O
determine	O
the	O
proportion	O
of	O
games	O
for	O
which	O
methods	O
perform	O
better	O
than	O
the	O
baseline	O
policies	O
(	O
scores	O
above	O
1	O
)	O
.	O

The	O
inter	Metric
-	Metric
algorithm	Metric
score	Metric
distribution	Metric
,	O
on	O
the	O
other	O
hand	O
,	O
effectively	O
conveys	O
the	O
relative	O
performance	O
of	O
each	O
method	O
.	O

In	O
particular	O
,	O
it	O
allows	O
us	O
to	O
conclude	O
that	O
BASS	Method
performs	O
slightly	O
better	O
than	O
Basic	Method
and	O
RAM	Method
,	O
and	O
that	O
DISCO	Method
performs	O
significantly	O
worse	O
than	O
the	O
other	O
methods	O
.	O

subsection	O
:	O
Paired	O
Tests	O
An	O
alternate	O
evaluation	Metric
metric	Metric
,	O
especially	O
useful	O
when	O
comparing	O
only	O
a	O
few	O
algorithms	O
,	O
is	O
to	O
perform	O
paired	O
tests	O
over	O
the	O
raw	O
scores	O
.	O

For	O
each	O
game	O
,	O
we	O
performed	O
a	O
two	O
-	O
tailed	O
Welsh	O
’s	O
-	O
test	O
with	O
99	O
%	O
confidence	O
intervals	O
to	O
determine	O
whether	O
one	O
algorithm	O
’s	O
score	O
was	O
statistically	O
different	O
than	O
the	O
other	O
’s	O
.	O

Table	O
[	O
reference	O
]	O
provides	O
,	O
for	O
each	O
pair	O
of	O
algorithms	O
,	O
the	O
number	O
of	O
games	O
for	O
which	O
one	O
algorithm	O
performs	O
statistically	O
better	O
or	O
worse	O
than	O
the	O
other	O
.	O

Because	O
of	O
their	O
ternary	O
nature	O
,	O
paired	Method
tests	Method
tend	O
to	O
magnify	O
small	O
but	O
significant	O
differences	O
in	O
scores	O
.	O

section	O
:	O
Related	O
Work	O
We	O
now	O
briefly	O
survey	O
recent	O
research	O
related	O
to	O
Atari	Material
2600	Material
games	Material
and	O
some	O
prior	O
work	O
on	O
the	O
construction	O
of	O
empirical	Metric
benchmarks	Metric
for	O
measuring	Task
general	Task
competency	Task
.	O

subsection	O
:	O
Atari	Task
Games	Task
There	O
has	O
been	O
some	O
attention	O
devoted	O
to	O
Atari	Task
2600	Task
game	Task
playing	Task
within	O
the	O
reinforcement	Task
learning	Task
community	Task
.	O

For	O
the	O
most	O
part	O
,	O
prior	O
work	O
has	O
focused	O
on	O
the	O
challenge	O
of	O
finding	O
good	O
state	O
features	O
for	O
this	O
domain	O
.	O

diuk2008	O
applied	O
their	O
DOORMAX	Method
algorithm	Method
to	O
a	O
restricted	O
version	O
of	O
the	O
game	Task
of	Task
Pitfall	Task
!	Task

.	O

Their	O
method	O
extracts	O
objects	O
from	O
the	O
displayed	Material
image	Material
with	O
game	Task
-	Task
specific	Task
object	Task
detection	Task
.	O

These	O
objects	O
are	O
then	O
converted	O
into	O
a	O
first	Method
-	Method
order	Method
logic	Method
representation	Method
of	Method
the	Method
world	Method
,	O
the	O
Object	Method
-	Method
Oriented	Method
Markov	Method
Decision	Method
Process	Method
(	O
OO	Method
-	Method
MDP	Method
)	O
.	O

Their	O
results	O
show	O
that	O
DOORMAX	Method
can	O
discover	O
the	O
optimal	O
behaviour	O
for	O
this	O
OO	Task
-	Task
MDP	Task
within	O
one	O
episode	O
.	O

wintermute2010	O
proposed	O
a	O
method	O
that	O
also	O
extracts	O
objects	O
from	O
the	O
displayed	Material
image	Material
and	O
embeds	O
them	O
into	O
a	O
logic	Method
-	Method
based	Method
architecture	Method
,	O
SOAR	Method
.	O

Their	O
method	O
uses	O
a	O
forward	Method
model	Method
of	O
the	O
scene	O
to	O
improve	O
the	O
performance	O
of	O
the	O
Q	Method
-	Method
Learning	Method
algorithm	Method
.	O

They	O
showed	O
that	O
by	O
using	O
such	O
a	O
model	O
,	O
a	O
reinforcement	Method
learning	Method
agent	Method
could	O
learn	O
to	O
play	O
a	O
restricted	O
version	O
of	O
the	O
game	O
of	O
Frogger	Material
.	O

cobo2011	Method
investigated	O
automatic	Task
feature	Task
discovery	Task
in	O
the	O
games	O
of	O
Pong	Task
and	O
Frogger	Task
,	O
using	O
their	O
own	O
simulator	O
.	O

Their	O
proposed	O
method	O
takes	O
advantage	O
of	O
human	O
trajectories	O
to	O
identify	O
state	O
features	O
that	O
are	O
important	O
for	O
playing	Task
console	Task
games	Task
.	O

Recently	O
,	O
hausknecht_12	O
proposed	O
HyperNEAT	Method
-	Method
GGP	Method
,	O
an	O
evolutionary	Method
approach	Method
for	O
finding	Task
policies	Task
to	O
play	O
Atari	Task
2600	Task
games	Task
.	O

Although	O
HyperNEAT	Method
-	Method
GGP	Method
is	O
presented	O
as	O
a	O
general	O
game	Method
playing	Method
approach	Method
,	O
it	O
is	O
currently	O
difficult	O
to	O
assess	O
its	O
general	O
performance	O
as	O
the	O
reported	O
results	O
were	O
limited	O
to	O
only	O
two	O
games	O
.	O

Finally	O
,	O
some	O
of	O
the	O
authors	O
of	O
this	O
paper	O
recently	O
presented	O
a	O
domain	Method
-	Method
independent	Method
feature	Method
generation	Method
technique	Method
that	O
attempts	O
to	O
focus	O
its	O
effort	O
around	O
the	O
location	O
of	O
the	O
player	O
avatar	O
.	O

This	O
work	O
used	O
the	O
evaluation	O
methodology	O
advocated	O
here	O
and	O
is	O
the	O
only	O
one	O
to	O
demonstrate	O
the	O
technique	O
across	O
a	O
large	O
set	O
of	O
testing	O
games	O
.	O

subsection	O
:	O
Evaluation	Method
Frameworks	Method
for	O
General	Task
Agents	Task
Although	O
the	O
idea	O
of	O
using	O
games	Method
to	O
evaluate	O
the	O
performance	Task
of	Task
agents	Task
has	O
a	O
long	O
history	O
in	O
artificial	Task
intelligence	Task
,	O
it	O
is	O
only	O
more	O
recently	O
that	O
an	O
emphasis	O
on	O
generality	O
has	O
assumed	O
a	O
more	O
prominent	O
role	O
.	O

pell93strategy	Method
advocated	O
the	O
design	O
of	O
agents	O
that	O
,	O
given	O
an	O
abstract	O
description	O
of	O
a	O
game	O
,	O
could	O
automatically	O
play	O
them	O
.	O

His	O
work	O
strongly	O
influenced	O
the	O
design	O
of	O
the	O
now	O
annual	O
General	Task
Game	Task
Playing	Task
competition	Task
.	O

Our	O
framework	O
differs	O
in	O
that	O
we	O
do	O
not	O
assume	O
to	O
have	O
access	O
to	O
a	O
compact	O
logical	O
description	O
of	O
the	O
game	O
semantics	O
.	O

schaul11	O
also	O
recently	O
presented	O
an	O
interesting	O
proposal	O
for	O
using	O
games	Method
to	O
measure	O
the	O
general	O
capabilities	O
of	O
an	O
agent	O
.	O

whiteson11	O
discuss	O
a	O
number	O
of	O
challenges	O
in	O
designing	O
empirical	Task
tests	Task
to	O
measure	O
general	Task
reinforcement	Task
learning	Task
performance	O
;	O
this	O
work	O
can	O
be	O
seen	O
as	O
attempting	O
to	O
address	O
their	O
important	O
concerns	O
.	O

Starting	O
in	O
2004	O
as	O
a	O
conference	O
workshop	O
,	O
the	O
Reinforcement	Task
Learning	Task
competition	Task
was	O
held	O
until	O
2009	O
(	O
a	O
new	O
iteration	O
of	O
the	O
competition	O
has	O
been	O
announced	O
for	O
2013	O
)	O
.	O

Each	O
year	O
new	O
domains	O
are	O
proposed	O
,	O
including	O
standard	O
RL	Method
benchmarks	Method
,	O
Tetris	Method
,	O
and	O
Infinite	Material
Mario	Material
.	O

In	O
a	O
typical	O
competition	Material
domain	Material
,	O
the	O
agent	O
’s	O
state	O
information	O
is	O
summarized	O
through	O
a	O
series	O
of	O
high	O
-	O
level	O
state	O
variables	O
rather	O
than	O
direct	O
sensory	O
information	O
.	O

Infinite	Task
Mario	Task
,	O
for	O
example	O
,	O
provides	O
the	O
agent	O
with	O
an	O
object	O
-	O
oriented	O
observation	O
space	O
.	O

In	O
the	O
past	O
,	O
organizers	O
have	O
provided	O
a	O
special	O
‘	O
Polyathlon	O
’	O
track	O
in	O
which	O
agents	O
must	O
behave	O
in	O
a	O
medley	O
of	O
continuous	O
-	O
observation	O
,	O
discrete	O
-	O
action	O
domains	O
.	O

Another	O
longstanding	O
competition	O
,	O
the	O
International	Task
Planning	Task
Competition	Task
(	O
IPC	Task
)	O
,	O
has	O
been	O
organized	O
since	O
1998	O
,	O
and	O
aims	O
to	O
“	O
produce	O
new	O
benchmarks	O
,	O
and	O
to	O
gather	O
and	O
disseminate	O
data	O
about	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
”	O
.	O

The	O
IPC	Method
is	O
composed	O
of	O
different	O
tracks	O
corresponding	O
to	O
different	O
types	O
of	O
planning	Task
problems	Task
,	O
including	O
factory	Task
optimization	Task
,	O
elevator	Task
control	Task
and	O
agent	Task
coordination	Task
.	O

For	O
example	O
,	O
one	O
of	O
the	O
problems	O
in	O
the	O
2011	O
competition	O
consists	O
in	O
coordinating	O
a	O
set	O
of	O
robots	O
around	O
a	O
two	O
-	O
dimensional	O
gridworld	O
so	O
that	O
every	O
tile	O
is	O
painted	O
with	O
a	O
specific	O
colour	O
.	O

Domains	Material
are	O
described	O
using	O
either	O
relational	Method
reinforcement	Method
learning	Method
,	O
yielding	O
parametrized	Method
Markov	Method
Decision	Method
Processes	Method
(	O
MDPs	Method
)	O
and	O
Partially	Method
Observable	Method
MDPs	Method
,	O
or	O
using	O
logic	O
predicates	O
,	O
e.g.	O
in	O
STRIPS	O
notation	O
.	O

One	O
indication	O
of	O
how	O
much	O
these	O
competitions	O
value	O
domain	O
variety	O
can	O
be	O
seen	O
in	O
the	O
time	O
spent	O
on	O
finding	O
a	O
good	O
specification	Method
language	Method
.	O

The	O
2008	O
-	O
2009	O
RL	Material
competitions	Material
,	O
for	O
example	O
,	O
used	O
RL	Method
-	Method
Glue	Method
specifically	O
for	O
this	O
purpose	O
;	O
the	O
2011	O
planning	O
under	O
uncertainty	O
track	O
of	O
the	O
IPC	O
similar	O
employed	O
the	O
Relation	Method
Dynamic	Method
Influence	Method
Diagram	Method
Language	Method
.	O

While	O
competitions	O
seek	O
to	O
spur	O
new	O
research	O
and	O
evaluate	O
existing	O
algorithms	O
through	O
a	O
standardized	O
set	O
of	O
benchmarks	O
,	O
they	O
are	O
not	O
independently	O
developed	O
,	O
in	O
the	O
sense	O
that	O
the	O
vast	O
majority	O
of	O
domains	O
are	O
provided	O
by	O
the	O
research	O
community	O
.	O

Thus	O
a	O
typical	O
competition	O
domain	O
reflects	O
existing	O
research	O
directions	O
:	O
Mountain	Material
Car	Material
and	O
Acrobot	Material
remain	O
staples	O
of	O
the	O
RL	Task
competition	Task
.	O

These	O
competitions	O
also	O
focus	O
their	O
research	O
effort	O
on	O
domains	O
that	O
provide	O
high	O
-	O
level	O
state	O
variables	O
,	O
for	O
example	O
the	O
location	Task
of	Task
robots	Task
in	O
the	O
floor	Task
-	Task
painting	Task
domain	Task
described	O
above	O
.	O

By	O
contrast	O
,	O
the	O
Arcade	Method
Learning	Method
Environment	Method
and	O
the	O
domain	O
-	O
independent	O
setting	O
force	O
us	O
to	O
consider	O
the	O
question	O
of	O
perceptual	Task
grounding	Task
:	O
how	O
to	O
extract	O
meaningful	O
state	O
information	O
from	O
raw	O
game	O
screens	O
(	O
or	O
RAM	O
information	O
)	O
.	O

In	O
turn	O
,	O
this	O
emphasizes	O
the	O
design	O
of	O
algorithms	O
that	O
can	O
be	O
applied	O
to	O
sensor	Material
-	Material
rich	Material
domains	Material
without	O
significant	O
expert	O
knowledge	O
.	O

There	O
have	O
also	O
been	O
a	O
number	O
of	O
attempts	O
to	O
define	O
formal	O
agent	Metric
performance	Metric
metrics	Metric
based	O
on	O
algorithmic	Method
information	Method
theory	Method
.	O

The	O
first	O
such	O
attempts	O
were	O
due	O
to	O
Hernandez	O
-	O
orallo98aformal	O
and	O
to	O
DoweHajek98	O
.	O

More	O
recently	O
,	O
the	O
approaches	O
of	O
Hern10	Method
and	O
of	O
legg11	Method
appear	O
to	O
have	O
some	O
potential	O
.	O

Although	O
these	O
frameworks	O
are	O
general	O
and	O
conceptually	O
clean	O
,	O
the	O
key	O
challenge	O
remains	O
how	O
to	O
specify	O
sufficiently	O
interesting	O
classes	O
of	O
environments	O
.	O

In	O
our	O
opinion	O
,	O
much	O
more	O
work	O
is	O
required	O
before	O
these	O
approaches	O
can	O
claim	O
to	O
rival	O
the	O
practicality	O
of	O
using	O
a	O
large	O
set	O
of	O
existing	O
human	Material
-	Material
designed	Material
environments	Material
for	O
agent	Task
evaluation	Task
.	O

section	O
:	O
Final	O
Remarks	O
The	O
Atari	Material
2600	Material
games	Material
were	O
developed	O
for	O
humans	O
and	O
as	O
such	O
exhibit	O
many	O
idiosyncrasies	O
that	O
make	O
them	O
both	O
challenging	O
and	O
exciting	O
.	O

Consider	O
,	O
for	O
example	O
,	O
the	O
game	O
Pong	O
.	O

Pong	Method
has	O
been	O
studied	O
in	O
a	O
variety	O
of	O
contexts	O
as	O
an	O
interesting	O
reinforcement	Task
learning	Task
domain	Task
.	O

The	O
Atari	Method
2600	Method
Pong	Method
,	O
however	O
,	O
is	O
significantly	O
more	O
complex	O
than	O
Pong	O
domains	O
developed	O
for	O
research	O
.	O

Games	Material
can	O
easily	O
last	O
10	O
,	O
000	O
time	O
steps	O
(	O
compared	O
to	O
200–1000	O
in	O
other	O
domains	O
)	O
;	O
observations	O
are	O
composed	O
of	O
7	Material
-	Material
bit	Material
images	Material
(	O
compared	O
to	O
black	Material
and	Material
white	Material
images	Material
in	O
the	O
work	O
of	O
stober08pixels	O
,	O
or	O
5	O
-	O
6	O
input	O
features	O
elsewhere	O
)	O
;	O
observations	O
are	O
also	O
more	O
complex	O
,	O
containing	O
the	O
two	O
players	O
’	O
score	O
and	O
side	O
walls	O
.	O

In	O
sheer	O
size	O
,	O
the	O
Atari	Material
2600	Material
Pong	Material
is	O
thus	O
a	O
larger	O
domain	O
.	O

Its	O
dynamics	O
are	O
also	O
more	O
complicated	O
.	O

In	O
research	O
implementations	O
of	O
Pong	Task
object	Task
motion	Task
is	O
implemented	O
using	O
first	Method
-	Method
order	Method
mechanics	Method
.	O

However	O
,	O
in	O
Atari	Task
2600	Task
Pong	Task
paddle	Task
control	Task
is	O
nonlinear	O
:	O
simple	O
experimentation	O
shows	O
that	O
fully	O
predicting	O
the	O
player	O
’s	O
paddle	O
requires	O
knowledge	O
of	O
the	O
last	O
18	O
actions	O
.	O

As	O
with	O
many	O
other	O
Atari	Task
games	Task
,	O
the	O
player	O
paddle	O
also	O
moves	O
every	O
other	O
frame	O
,	O
adding	O
a	O
degree	O
of	O
temporal	O
aliasing	O
to	O
the	O
domain	O
.	O

While	O
Atari	O
2600	O
Pong	O
may	O
appear	O
unnecessarily	O
contrived	O
,	O
it	O
in	O
fact	O
reflects	O
the	O
unexpected	O
complexity	O
of	O
the	O
problems	O
with	O
which	O
humans	O
are	O
faced	O
.	O

Most	O
,	O
if	O
not	O
all	O
Atari	Material
2600	Material
games	Material
are	O
subject	O
to	O
similar	O
programming	O
artifacts	O
:	O
in	O
Space	O
Invaders	O
,	O
for	O
example	O
,	O
the	O
invaders	O
’	O
velocity	O
increases	O
nonlinearly	O
with	O
the	O
number	O
of	O
remaining	O
invaders	O
.	O

In	O
this	O
way	O
the	O
Atari	Method
2600	Method
platform	Method
provides	O
AI	O
researchers	O
with	O
something	O
unique	O
:	O
clean	O
,	O
easily	O
-	O
emulated	O
domains	O
which	O
nevertheless	O
provide	O
many	O
of	O
the	O
challenges	O
typically	O
associated	O
with	O
real	Task
-	Task
world	Task
applications	Task
.	O

Should	O
technology	O
advance	O
so	O
as	O
to	O
render	O
general	O
Atari	Task
2600	Task
game	Task
playing	Task
achievable	O
,	O
our	O
challenge	O
problem	O
can	O
always	O
be	O
extended	O
to	O
use	O
more	O
recent	O
video	Method
game	Method
platforms	Method
.	O

A	O
natural	O
progression	O
,	O
for	O
example	O
,	O
would	O
be	O
to	O
move	O
on	O
to	O
the	O
Commodore	O
64	O
,	O
then	O
to	O
the	O
Nintendo	O
,	O
and	O
so	O
forth	O
towards	O
current	O
generation	O
consoles	O
.	O

All	O
of	O
these	O
consoles	O
have	O
hundreds	O
of	O
released	Material
games	Material
,	O
and	O
older	O
platforms	O
have	O
readily	O
available	O
emulators	Method
.	O

With	O
the	O
ultra	O
-	O
realism	O
of	O
current	O
generation	O
consoles	O
,	O
each	O
console	O
represents	O
a	O
natural	O
stepping	O
stone	O
toward	O
general	O
real	Task
-	Task
world	Task
competency	Task
.	O

Our	O
hope	O
is	O
that	O
by	O
using	O
the	O
methodology	O
advocated	O
in	O
this	O
paper	O
,	O
we	O
can	O
work	O
in	O
a	O
bottom	O
-	O
up	O
fashion	O
towards	O
developing	O
more	O
sophisticated	O
AI	Method
technology	Method
while	O
still	O
maintaining	O
empirical	O
rigor	O
.	O

section	O
:	O
Conclusion	O
This	O
article	O
has	O
introduced	O
the	O
Arcade	Method
Learning	Method
Environment	Method
,	O
a	O
platform	O
for	O
evaluating	O
the	O
development	Task
of	Task
general	Task
,	Task
domain	Task
-	Task
independent	Task
agents	Task
.	O

ALE	Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	Material
2600	Material
game	Material
environments	Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	Method
as	O
a	O
challenge	O
problem	O
by	O
benchmarking	O
several	O
domain	Method
-	Method
independent	Method
agents	Method
that	O
use	O
well	O
-	O
established	O
reinforcement	Method
learning	Method
and	Method
planning	Method
techniques	Method
.	O

Our	O
results	O
suggest	O
that	O
general	Task
Atari	Task
game	Task
playing	Task
is	O
a	O
challenging	O
but	O
not	O
intractable	O
problem	O
domain	O
with	O
the	O
potential	O
to	O
aid	O
the	O
development	O
and	O
evaluation	Task
of	Task
general	Task
agents	Task
.	O

We	O
would	O
like	O
to	O
thank	O
Marc	O
Lanctot	O
,	O
Erik	O
Talvitie	O
,	O
and	O
Matthew	O
Hausknecht	O
for	O
providing	O
suggestions	O
on	O
helping	O
debug	O
and	O
improving	O
the	O
Arcade	Method
Learning	Method
Environment	Method
source	O
code	O
.	O

We	O
would	O
also	O
like	O
to	O
thank	O
our	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
enthusiasm	O
about	O
the	O
Atari	Material
2600	Material
as	O
a	O
research	O
platform	O
.	O

The	O
work	O
presented	O
here	O
was	O
supported	O
by	O
the	O
Alberta	Material
Innovates	Material
Technology	Material
Futures	Material
,	O
the	O
Alberta	O
Innovates	O
Centre	O
for	O
Machine	Task
Learning	Task
at	O
the	O
University	O
of	O
Alberta	O
,	O
and	O
the	O
Natural	Material
Science	Material
and	Material
Engineering	Material
Research	Material
Council	Material
of	O
Canada	O
.	O

Invaluable	Material
computational	Material
resources	Material
were	O
provided	O
by	O
Compute	O
/	O
Calcul	O
Canada	O
.	O

appendix	O
:	O
Feature	Method
Set	Method
Construction	Method
This	O
section	O
gives	O
a	O
detailed	O
description	O
of	O
the	O
five	O
feature	Method
generation	Method
techniques	Method
from	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Basic	O
Abstraction	O
of	O
the	O
ScreenShots	O
(	O
BASS	O
)	O
The	O
idea	O
behind	O
BASS	Method
is	O
to	O
directly	O
encode	O
colours	O
present	O
on	O
the	O
screen	O
.	O

This	O
method	O
is	O
motivated	O
by	O
three	O
observations	O
on	O
the	O
Atari	Material
2600	Material
hardware	Material
and	O
games	Material
:	O
While	O
the	O
Atari	Method
2600	Method
hardware	Method
supports	O
a	O
screen	O
resolution	O
of	O
,	O
game	O
objects	O
are	O
usually	O
larger	O
than	O
a	O
few	O
pixels	O
.	O

Overall	O
,	O
important	O
game	O
events	O
happen	O
at	O
a	O
much	O
lower	O
resolution	O
.	O

Many	O
Atari	Material
2600	Material
games	Material
have	O
a	O
static	O
background	O
,	O
with	O
a	O
few	O
important	O
objects	O
moving	O
on	O
the	O
screen	O
.	O

While	O
the	O
screen	O
matrix	O
is	O
densely	O
populated	O
,	O
the	O
actual	O
interesting	O
features	O
on	O
the	O
screen	O
are	O
often	O
sparse	O
.	O

While	O
the	O
hardware	O
can	O
show	O
up	O
to	O
128	O
colours	O
in	O
the	O
NTSC	O
mode	O
,	O
it	O
is	O
limited	O
to	O
only	O
8	O
colours	O
in	O
the	O
SECAM	Method
mode	Method
.	O

Consequently	O
,	O
most	O
games	O
use	O
a	O
few	O
number	O
of	O
colours	O
to	O
distinguish	O
important	O
objects	O
on	O
the	O
screen	O
.	O

The	O
game	Material
screen	Material
is	O
first	O
preprocessed	O
by	O
subtracting	O
its	O
background	O
,	O
detected	O
using	O
a	O
simple	O
histogram	Method
method	Method
.	O

BASS	Method
then	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
eight	O
SECAM	O
palette	O
colours	O
at	O
a	O
low	O
resolution	O
,	O
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Intuitively	O
,	O
BASS	Method
seeks	O
to	O
capture	O
the	O
presence	O
of	O
objects	O
of	O
certain	O
colours	O
at	O
different	O
screen	O
locations	O
.	O

BASS	Method
also	O
encodes	O
relations	O
between	O
objects	O
by	O
constructing	O
all	O
pairwise	O
combinations	O
of	O
its	O
encoded	O
colour	O
features	O
.	O

In	O
Asterix	Material
,	O
for	O
example	O
,	O
it	O
is	O
important	O
to	O
know	O
if	O
there	O
is	O
a	O
green	O
object	O
(	O
player	O
character	O
)	O
and	O
a	O
red	O
object	O
(	O
collectable	O
object	O
)	O
in	O
its	O
vicinity	O
.	O

Pairwise	O
features	O
allow	O
us	O
to	O
capture	O
such	O
object	O
relations	O
.	O

subsection	O
:	O
Basic	O
The	O
Basic	O
method	O
generates	O
the	O
same	O
set	O
of	O
features	O
as	O
BASS	O
,	O
but	O
omits	O
the	O
pairwise	O
combinations	O
.	O

This	O
allows	O
us	O
to	O
study	O
whether	O
the	O
additional	O
features	O
are	O
beneficial	O
or	O
harmful	O
to	O
learning	Task
.	O

Because	O
the	O
Basic	O
method	O
has	O
fewer	O
features	O
than	O
BASS	O
,	O
it	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
128	O
colours	O
.	O

In	O
comparison	O
to	O
BASS	O
,	O
Basic	O
therefore	O
represents	O
colour	O
more	O
accurately	O
,	O
but	O
can	O
not	O
represent	O
object	O
interactions	O
.	O

subsection	O
:	O
Detecting	Task
Instances	Task
of	Task
Classes	Task
of	Task
Objects	Task
(	O
DISCO	Method
)	O
This	O
feature	Method
generation	Method
method	Method
is	O
based	O
on	O
detecting	O
a	O
set	O
of	O
classes	O
representing	O
game	O
entities	O
and	O
locating	O
instances	O
of	O
these	O
classes	O
on	O
the	O
screen	O
.	O

DISCO	Method
is	O
motivated	O
by	O
the	O
following	O
additional	O
observations	O
on	O
Atari	Task
2600	Task
games	Task
:	O
The	O
game	O
entities	O
are	O
often	O
instances	O
of	O
a	O
few	O
classes	O
of	O
objects	O
.	O

For	O
instance	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
while	O
there	O
are	O
many	O
objects	O
in	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Freeway	O
,	O
all	O
of	O
these	O
objects	O
are	O
instances	O
of	O
only	O
two	O
classes	O
:	O
Chicken	O
and	O
Car	O
.	O

Similarly	O
,	O
all	O
the	O
objects	O
on	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Seaquest	O
are	O
instances	O
of	O
one	O
of	O
these	O
six	O
classes	O
:	O
Fish	O
,	O
Swimmer	O
,	O
Player	O
Submarine	O
,	O
Enemy	O
Submarine	O
,	O
Player	O
Bullet	O
,	O
and	O
Enemy	O
Bullet	O
.	O

The	O
interaction	O
between	O
two	O
objects	O
can	O
often	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
their	O
respective	O
classes	O
.	O

As	O
an	O
example	O
,	O
consider	O
Car	Task
-	Task
Chicken	Task
object	Task
interactions	Task
in	O
Freeway	Material
:	O
learning	O
that	O
there	O
is	O
lower	O
value	O
associated	O
with	O
one	O
Chicken	O
instance	O
hitting	O
a	O
Car	O
instance	O
can	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
those	O
two	O
classes	O
.	O

DISCO	Method
first	O
performs	O
a	O
series	O
of	O
preprocessing	Method
steps	Method
to	O
discover	O
classes	O
,	O
during	O
which	O
no	O
value	Method
function	Method
learning	Method
is	O
performed	O
.	O

When	O
the	O
agent	O
subsequently	O
learns	O
to	O
play	O
the	O
game	O
,	O
DISCO	O
generates	O
features	O
by	O
detecting	O
objects	O
on	O
the	O
screen	O
and	O
classifying	O
them	O
.	O

The	O
DISCO	Method
process	Method
is	O
summarized	O
by	O
the	O
following	O
steps	O
:	O
Locally	Method
Sensitive	Method
Hashing	Method
(	Method
LSH	Method
)	Method
Feature	Method
Generation	Method
(	O
hash	O
table	O
size	O
)	O
,	O
(	O
screen	O
bit	O
vector	O
size	O
)	O
(	O
number	O
of	O
random	O
bit	O
vectors	O
)	O
,	O
(	O
number	O
of	O
non	O
-	O
zero	O
entries	O
)	O
A	O
screen	O
matrix	O
with	O
elements	O
(	O
has	O
length	O
)	O
Initialize	O
hash	O
the	O
projection	O
of	O
onto	O
one	O
binary	O
feature	O
per	O
random	O
bit	O
vector	O
Initialize	O
,	O
(	O
)	O
Initialize	O
Select	O
distinct	O
coordinates	O
between	O
1	O
and	O
uniformly	O
at	O
random	O
;	O
;	O
…	O
;	O
Initialize	O
,	O
(	O
uniformly	O
random	O
coordinate	O
between	O
1	O
and	O
M	O
)	O
Preprocessing	Task
:	O
Background	Task
detection	Task
:	O
The	O
static	O
background	O
matrix	O
is	O
extracted	O
using	O
a	O
histogram	Method
method	Method
,	O
as	O
with	O
BASS	Method
.	O

Blob	Task
extraction	Task
:	O
A	O
list	O
of	O
moving	O
blob	O
(	O
foreground	O
)	O
objects	O
is	O
detected	O
in	O
each	O
game	O
screen	O
.	O

Class	Task
discovery	Task
:	O
A	O
set	O
of	O
classes	O
is	O
detected	O
from	O
the	O
extracted	O
blob	O
objects	O
.	O

Class	Method
filtering	Method
:	O
Classes	O
that	O
appear	O
infrequently	O
or	O
are	O
restricted	O
to	O
small	O
region	O
of	O
the	O
screen	O
are	O
removed	O
from	O
the	O
set	O
.	O

Class	Task
merging	Task
:	O
Classes	O
that	O
have	O
similar	O
shapes	O
are	O
merged	O
together	O
.	O

Feature	Task
generation	Task
:	O
Class	Method
instance	Method
detection	Method
:	O
At	O
each	O
time	O
step	O
,	O
class	O
instances	O
are	O
detected	O
from	O
the	O
current	O
screen	O
matrix	O
.	O

Feature	Method
vector	Method
generation	Method
:	O
A	O
feature	O
vector	O
is	O
generated	O
from	O
the	O
detected	O
instances	O
by	O
tile	O
-	O
coding	O
their	O
absolute	O
position	O
as	O
well	O
as	O
the	O
relative	O
position	O
and	O
velocity	O
of	O
every	O
pair	O
of	O
instances	O
from	O
different	O
classes	O
.	O

Multiple	O
instances	O
of	O
the	O
same	O
objects	O
are	O
combined	O
additively	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
discovered	O
objects	O
in	O
a	O
Seaquest	Material
frame	Material
.	O

This	O
image	O
illustrates	O
the	O
difficulties	O
in	O
detecting	Task
objects	Task
:	O
although	O
DISCO	Method
correctly	O
classifies	O
the	O
different	O
fish	O
as	O
part	O
of	O
the	O
same	O
class	O
,	O
it	O
also	O
detects	O
a	O
life	O
icon	O
and	O
the	O
oxygen	O
bar	O
as	O
part	O
of	O
that	O
class	O
.	O

subsection	O
:	O
Locality	Method
Sensitive	Method
Hashing	Method
(	O
LSH	Method
)	O
An	O
alternative	O
approach	O
to	O
BASS	Task
and	O
DISCO	Task
is	O
to	O
use	O
well	O
-	O
established	O
feature	Method
generation	Method
methods	Method
that	O
are	O
agnostic	O
about	O
the	O
type	O
of	O
input	O
they	O
receive	O
.	O

Such	O
methods	O
include	O
polynomial	Method
bases	Method
,	O
sparse	Method
distributed	Method
memories	Method
and	O
locality	Method
sensitive	Method
hashing	Method
(	Method
LSH	Method
)	Method
.	O

In	O
this	O
paper	O
we	O
consider	O
the	O
latter	O
as	O
a	O
simple	O
mean	O
of	O
reducing	O
the	O
large	Task
image	Task
space	Task
to	O
a	O
smaller	O
,	O
more	O
manageable	O
set	O
of	O
features	O
.	O

The	O
input	O
–	O
here	O
,	O
a	O
game	Material
screen	Material
–	O
is	O
first	O
mapped	O
to	O
a	O
bit	O
vector	O
of	O
size	O
.	O

The	O
resulting	O
vector	O
is	O
then	O
hashed	O
down	O
into	O
a	O
smaller	O
set	O
of	O
features	O
.	O

LSH	Method
performs	O
an	O
additional	O
random	Method
projection	Method
step	Method
to	O
ensure	O
that	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
be	O
binned	O
together	O
.	O

The	O
LSH	Method
generation	Method
method	Method
is	O
detailed	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
RAM	Method
-	Method
based	Method
Feature	Method
Generation	Method
Unlike	O
the	O
previous	O
three	O
methods	O
,	O
which	O
generate	O
feature	O
vectors	O
based	O
on	O
the	O
game	Material
screen	Material
,	O
the	O
RAM	Method
-	Method
based	Method
feature	Method
generation	Method
method	Method
relies	O
on	O
the	O
contents	O
of	O
the	O
console	O
memory	O
.	O

The	O
Atari	Method
2600	Method
has	O
only	O
bits	O
of	O
random	O
access	O
memory	O
,	O
which	O
must	O
hold	O
the	O
complete	O
internal	O
state	O
of	O
a	O
game	O
:	O
location	O
of	O
game	O
entities	O
,	O
timers	O
,	O
health	O
indicators	O
,	O
etc	O
.	O

The	O
RAM	Method
is	O
therefore	O
a	O
relatively	O
compact	O
representation	O
of	O
the	O
game	O
state	O
,	O
and	O
in	O
contrast	O
to	O
the	O
game	O
screen	O
,	O
it	O
is	O
also	O
Markovian	Method
.	O

The	O
purpose	O
of	O
our	O
RAM	Method
-	Method
based	Method
agent	Method
is	O
to	O
investigate	O
whether	O
features	O
generated	O
from	O
the	O
RAM	Method
affect	O
performance	O
differently	O
from	O
features	O
generated	O
from	O
game	Material
screens	Material
.	O

The	O
first	O
part	O
of	O
the	O
generated	O
feature	O
vector	O
simply	O
includes	O
the	O
1024	O
bits	O
of	O
RAM	O
.	O

Atari	Material
2600	Material
game	Material
programmers	Material
often	O
used	O
these	O
bits	O
not	O
as	O
individual	O
values	O
,	O
but	O
as	O
part	O
of	O
4	O
-	O
bit	O
or	O
8	O
-	O
bit	O
words	O
.	O

Linear	Method
function	Method
approximation	Method
on	O
the	O
individual	O
bits	O
can	O
capture	O
the	O
value	O
of	O
these	O
multi	O
-	O
bit	O
words	O
.	O

We	O
are	O
also	O
interested	O
in	O
the	O
relation	O
between	O
pairs	O
of	O
values	O
in	O
memory	O
.	O

To	O
capture	O
these	O
relations	O
,	O
the	O
logical	O
-	O
AND	O
of	O
all	O
possible	O
bit	O
pairs	O
is	O
appended	O
to	O
the	O
feature	O
vector	O
.	O

Note	O
that	O
a	O
linear	O
function	O
on	O
the	O
pairwise	O
’s	Method
can	O
capture	O
products	O
of	O
both	O
4	O
-	O
bit	O
and	O
8	O
-	O
bit	O
words	O
.	O

This	O
is	O
because	O
the	O
product	O
of	O
two	O
-	O
bit	O
words	O
can	O
be	O
expressed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
pairwise	O
products	O
of	O
their	O
bits	O
.	O

appendix	O
:	O
UCT	Material
Pseudocode	Material
UCT	O
(	O
search	O
horizon	O
)	O
,	O
(	O
simulations	O
per	O
step	O
)	O
(	O
search	O
tree	O
)	O
(	O
current	O
state	O
)	O
is	O
empty	O
or	O
optional	O
is	O
not	O
a	O
leaf	O
,	O
some	O
action	O
was	O
never	O
taken	O
in	O
run	O
model	O
for	O
one	O
step	O
c	O
is	O
necessarily	O
a	O
leaf	O
update	O
-	O
value	O
(	O
n	O
,	O
R	O
)	O
propagate	O
values	O
back	O
up	O
action	O
most	O
frequently	O
taken	O
at	O
root	O
UCT	O
Routines	O
:	O
discount	O
factor	O
children	O
of	O
Initialize	O
Monte	O
-	O
Carlo	O
return	O
to	O
0	O
Select	O
according	O
to	O
some	O
rollout	Method
policy	Method
(	O
e.g.	O
uniformly	O
randomly	O
)	O
is	O
not	O
the	O
root	O
of	O
,	O
i.e.	O
appendix	O
:	O
Experimental	O
Parameters	O
appendix	O
:	O
Detailed	O
Results	O
subsection	O
:	O
Reinforcement	Method
Learning	Method
.	O

subsection	O
:	O
Planning	Task
.	O

bibliography	O
:	O
References	O
document	O
:	O
An	O
Effective	O
Approach	O
to	O
Unsupervised	Task
Machine	Task
Translation	Task
While	O
machine	Task
translation	Task
has	O
traditionally	O
relied	O
on	O
large	O
amounts	O
of	O
parallel	O
corpora	O
,	O
a	O
recent	O
research	O
line	O
has	O
managed	O
to	O
train	O
both	O
Neural	Method
Machine	Method
Translation	Method
(	O
NMT	Method
)	O
and	O
Statistical	Method
Machine	Method
Translation	Method
(	O
SMT	Method
)	O
systems	O
using	O
monolingual	O
corpora	O
only	O
.	O

In	O
this	O
paper	O
,	O
we	O
identify	O
and	O
address	O
several	O
deficiencies	O
of	O
existing	O
unsupervised	O
SMT	Method
approaches	O
by	O
exploiting	O
subword	O
information	O
,	O
developing	O
a	O
theoretically	O
well	O
founded	O
unsupervised	Method
tuning	Method
method	Method
,	O
and	O
incorporating	O
a	O
joint	Method
refinement	Method
procedure	Method
.	O

Moreover	O
,	O
we	O
use	O
our	O
improved	O
SMT	Method
system	O
to	O
initialize	O
a	O
dual	O
NMT	Method
model	O
,	O
which	O
is	O
further	O
fine	O
-	O
tuned	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	Method
-	Method
translation	Method
.	O

Together	O
,	O
we	O
obtain	O
large	O
improvements	O
over	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	Task
machine	Task
translation	Task
.	O

For	O
instance	O
,	O
we	O
get	O
22.5	O
BLEU	Metric
points	Metric
in	O
English	Material
-	Material
to	Material
-	Material
German	Material
WMT	Material
2014	Material
,	O
5.5	O
points	O
more	O
than	O
the	O
previous	O
best	O
unsupervised	Method
system	Method
,	O
and	O
0.5	O
points	O
more	O
than	O
the	O
(	O
supervised	O
)	O
shared	O
task	O
winner	O
back	O
in	O
2014	O
.	O

section	O
:	O
Introduction	O
The	O
recent	O
advent	O
of	O
neural	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
modeling	Method
has	O
resulted	O
in	O
significant	O
progress	O
in	O
the	O
field	O
of	O
machine	Task
translation	Task
,	O
with	O
large	O
improvements	O
in	O
standard	O
benchmarks	O
vaswani2017attention	O
,	O
edunov2018understanding	O
and	O
the	O
first	O
solid	O
claims	O
of	O
human	Metric
parity	Metric
in	O
certain	O
settings	O
hassan2018achieving	O
.	O

Unfortunately	O
,	O
these	O
systems	O
rely	O
on	O
large	O
amounts	O
of	O
parallel	O
corpora	O
,	O
which	O
are	O
only	O
available	O
for	O
a	O
few	O
combinations	O
of	O
major	O
languages	O
like	O
English	Material
,	O
German	Material
and	O
French	Material
.	O

Aiming	O
to	O
remove	O
this	O
dependency	O
on	O
parallel	O
data	O
,	O
a	O
recent	O
research	O
line	O
has	O
managed	O
to	O
train	O
unsupervised	Task
machine	Task
translation	Task
systems	Task
using	O
monolingual	O
corpora	O
only	O
.	O

The	O
first	O
such	O
systems	O
were	O
based	O
on	O
Neural	Method
Machine	Method
Translation	Method
(	O
NMT	Method
)	O
,	O
and	O
combined	O
denoising	Method
autoencoding	Method
and	O
back	Method
-	Method
translation	Method
to	O
train	O
a	O
dual	Method
model	Method
initialized	O
with	O
cross	Method
-	Method
lingual	Method
embeddings	Method
artetxe2018unmt	O
,	O
lample2018unsupervised	O
.	O

Nevertheless	O
,	O
these	O
early	O
systems	O
were	O
later	O
superseded	O
by	O
Statistical	Method
Machine	Method
Translation	Method
(	O
SMT	Method
)	O
based	O
approaches	O
,	O
which	O
induced	O
an	O
initial	O
phrase	O
-	O
table	O
through	O
cross	Method
-	Method
lingual	Method
embedding	Method
mappings	Method
,	O
combined	O
it	O
with	O
an	O
n	Method
-	Method
gram	Method
language	Method
model	Method
,	O
and	O
further	O
improved	O
the	O
system	O
through	O
iterative	Method
back	Method
-	Method
translation	Method
lample2018phrase	O
,	O
artetxe2018usmt	O
.	O

In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
more	O
principled	O
approach	O
to	O
unsupervised	Task
SMT	Task
,	O
addressing	O
several	O
deficiencies	O
of	O
previous	O
systems	O
by	O
incorporating	O
subword	O
information	O
,	O
applying	O
a	O
theoretically	O
well	O
founded	O
unsupervised	Method
tuning	Method
method	Method
,	O
and	O
developing	O
a	O
joint	Method
refinement	Method
procedure	Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
use	O
our	O
improved	O
SMT	Method
approach	O
to	O
initialize	O
an	O
unsupervised	O
NMT	Method
system	O
,	O
which	O
is	O
further	O
improved	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	Method
-	Method
translation	Method
.	O

Our	O
experiments	O
on	O
WMT	Material
2014	Material
/	Material
2016	Material
French	Material
-	Material
English	Material
and	O
German	Material
-	Material
English	Material
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
as	O
our	O
proposed	O
system	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	Task
machine	Task
translation	Task
by	O
5	O
-	O
7	O
BLEU	Metric
points	Metric
in	O
all	O
these	O
datasets	O
and	O
translation	O
directions	O
.	O

Our	O
system	O
also	O
outperforms	O
the	O
supervised	O
WMT	Material
2014	O
shared	O
task	O
winner	O
in	O
English	Material
-	Material
to	Material
-	Material
German	Material
,	O
and	O
is	O
around	O
2	O
BLEU	Metric
points	Metric
behind	O
it	O
in	O
the	O
rest	O
of	O
translation	Task
directions	Task
,	O
suggesting	O
that	O
unsupervised	Task
machine	Task
translation	Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

The	O
remaining	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
first	O
discusses	O
the	O
related	O
work	O
in	O
the	O
topic	O
.	O

Section	O
[	O
reference	O
]	O
then	O
describes	O
our	O
principled	Method
unsupervised	Method
SMT	Method
method	Method
,	O
while	O
Section	O
[	O
reference	O
]	O
discusses	O
our	O
hybridization	Method
method	Method
with	O
NMT	Method
.	O

We	O
then	O
present	O
the	O
experiments	O
done	O
and	O
the	O
results	O
obtained	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
Section	O
[	O
reference	O
]	O
concludes	O
the	O
paper	O
.	O

section	O
:	O
Related	O
work	O
Early	O
attempts	O
to	O
build	O
machine	Task
translation	Task
systems	Task
with	O
monolingual	O
corpora	O
go	O
back	O
to	O
statistical	Task
decipherment	Task
ravi2011deciphering	O
,	O
dou2012large	O
.	O

These	O
methods	O
see	O
the	O
source	O
language	O
as	O
ciphertext	O
produced	O
by	O
a	O
noisy	Method
channel	Method
model	Method
that	O
first	O
generates	O
the	O
original	O
English	O
text	O
and	O
then	O
probabilistically	O
replaces	O
the	O
words	O
in	O
it	O
.	O

The	O
English	Method
generative	Method
process	Method
is	O
modeled	O
using	O
an	O
n	Method
-	Method
gram	Method
language	Method
model	Method
,	O
and	O
the	O
channel	Method
model	Method
parameters	Method
are	O
estimated	O
using	O
either	O
expectation	Method
maximization	Method
or	O
Bayesian	Method
inference	Method
.	O

This	O
basic	O
approach	O
was	O
later	O
improved	O
by	O
incorporating	O
syntactic	O
knowledge	O
dou2013dependency	O
and	O
word	O
embeddings	O
dou2015unifying	O
.	O

Nevertheless	O
,	O
these	O
methods	O
were	O
only	O
shown	O
to	O
work	O
in	O
limited	O
settings	O
,	O
being	O
most	O
often	O
evaluated	O
in	O
word	Task
-	Task
level	Task
translation	Task
.	O

More	O
recently	O
,	O
the	O
task	O
got	O
a	O
renewed	O
interest	O
after	O
the	O
concurrent	O
work	O
of	O
artetxe2018unmt	O
and	O
lample2018unsupervised	O
on	O
unsupervised	Method
NMT	Method
which	O
,	O
for	O
the	O
first	O
time	O
,	O
obtained	O
promising	O
results	O
in	O
standard	O
machine	Task
translation	Task
benchmarks	Task
using	O
monolingual	O
corpora	O
only	O
.	O

Both	O
methods	O
build	O
upon	O
the	O
recent	O
work	O
on	O
unsupervised	Task
cross	Task
-	Task
lingual	Task
embedding	Task
mappings	Task
,	O
which	O
independently	O
train	O
word	O
embeddings	O
in	O
two	O
languages	O
and	O
learn	O
a	O
linear	Method
transformation	Method
to	O
map	O
them	O
to	O
a	O
shared	O
space	O
through	O
self	Method
-	Method
learning	Method
artetxe2017learning	O
,	O
artetxe2018robust	Method
or	Method
adversarial	Method
training	Method
conneau2018word	O
.	O

The	O
resulting	O
cross	Method
-	Method
lingual	Method
embeddings	Method
are	O
used	O
to	O
initialize	O
a	O
shared	Method
encoder	Method
for	O
both	O
languages	O
,	O
and	O
the	O
entire	O
system	O
is	O
trained	O
using	O
a	O
combination	O
of	O
denoising	Method
autoencoding	Method
,	O
back	Method
-	Method
translation	Method
and	O
,	O
in	O
the	O
case	O
of	O
lample2018unsupervised	Method
,	Method
adversarial	Method
training	Method
.	O

This	O
method	O
was	O
further	O
improved	O
by	O
yang2018unsupervised	O
,	O
who	O
use	O
two	O
language	Method
-	Method
specific	Method
encoders	Method
sharing	O
only	O
a	O
subset	O
of	O
their	O
parameters	O
,	O
and	O
incorporate	O
a	O
local	Method
and	Method
a	Method
global	Method
generative	Method
adversarial	Method
network	Method
.	O

Nevertheless	O
,	O
it	O
was	O
later	O
argued	O
that	O
the	O
modular	Method
architecture	Method
of	O
phrase	O
-	O
based	O
SMT	Method
was	O
more	O
suitable	O
for	O
this	O
problem	O
,	O
and	O
lample2018phrase	O
and	O
artetxe2018usmt	O
adapted	O
the	O
same	O
principles	O
discussed	O
above	O
to	O
train	O
an	O
unsupervised	O
SMT	Method
model	O
,	O
obtaining	O
large	O
improvements	O
over	O
the	O
original	O
unsupervised	O
NMT	Method
systems	O
.	O

More	O
concretely	O
,	O
both	O
approaches	O
learn	O
cross	Method
-	Method
lingual	Method
n	Method
-	Method
gram	Method
embeddings	Method
from	O
monolingual	O
corpora	O
based	O
on	O
the	O
mapping	Method
method	Method
discussed	O
earlier	O
,	O
and	O
use	O
them	O
to	O
induce	O
an	O
initial	O
phrase	O
-	O
table	O
that	O
is	O
combined	O
with	O
an	O
n	Method
-	Method
gram	Method
language	Method
model	Method
and	O
a	O
distortion	Method
model	Method
.	O

This	O
initial	O
system	O
is	O
then	O
refined	O
through	O
iterative	Method
back	Method
-	Method
translation	Method
sennrich2016improving	O
which	O
,	O
in	O
the	O
case	O
of	O
artetxe2018usmt	O
,	O
is	O
preceded	O
by	O
an	O
unsupervised	Method
tuning	Method
step	Method
.	O

Our	O
work	O
identifies	O
some	O
deficiencies	O
in	O
these	O
previous	O
systems	O
,	O
and	O
proposes	O
a	O
more	O
principled	Method
approach	Method
to	O
unsupervised	Task
SMT	Task
that	O
incorporates	O
subword	O
information	O
,	O
uses	O
a	O
theoretically	O
better	O
founded	O
unsupervised	Method
tuning	Method
method	Method
,	O
and	O
applies	O
a	O
joint	Method
refinement	Method
procedure	Method
,	O
outperforming	O
these	O
previous	O
systems	O
by	O
a	O
substantial	O
margin	O
.	O

Very	O
recently	O
,	O
some	O
authors	O
have	O
tried	O
to	O
combine	O
both	O
SMT	Method
and	O
NMT	Method
to	O
build	O
hybrid	Task
unsupervised	Task
machine	Task
translation	Task
systems	Task
.	O

This	O
idea	O
was	O
already	O
explored	O
by	O
lample2018phrase	O
,	O
who	O
aided	O
the	O
training	O
of	O
their	O
unsupervised	O
NMT	Method
system	O
by	O
combining	O
standard	O
back	Method
-	Method
translation	Method
with	O
synthetic	O
parallel	O
data	O
generated	O
by	O
unsupervised	Task
SMT	Task
.	O

marie2018unsupervised	O
go	O
further	O
and	O
use	O
synthetic	O
parallel	O
data	O
from	O
unsupervised	Task
SMT	Task
to	O
train	O
a	O
conventional	O
NMT	Method
system	O
from	O
scratch	O
.	O

The	O
resulting	O
NMT	Method
model	O
is	O
then	O
used	O
to	O
augment	O
the	O
synthetic	O
parallel	O
corpus	O
through	O
back	Method
-	Method
translation	Method
,	O
and	O
a	O
new	O
NMT	Method
model	O
is	O
trained	O
on	O
top	O
of	O
it	O
from	O
scratch	O
,	O
repeating	O
the	O
process	O
iteratively	O
.	O

ren2019unsupervised	O
follow	O
a	O
similar	O
approach	O
,	O
but	O
use	O
SMT	Method
as	O
posterior	Method
regularization	Method
at	O
each	O
iteration	O
.	O

As	O
shown	O
later	O
in	O
our	O
experiments	O
,	O
our	O
proposed	O
NMT	Method
hybridization	O
obtains	O
substantially	O
larger	O
absolute	O
gains	O
than	O
all	O
these	O
previous	O
approaches	O
,	O
even	O
if	O
our	O
initial	O
SMT	Method
system	O
is	O
stronger	O
and	O
thus	O
more	O
challenging	O
to	O
improve	O
upon	O
.	O

section	O
:	O
Principled	O
unsupervised	O
SMT	Method
Phrase	O
-	O
based	O
SMT	Method
is	O
formulated	O
as	O
a	O
log	Method
-	Method
linear	Method
combination	Method
of	O
several	O
statistical	Method
models	Method
:	O
a	O
translation	Method
model	Method
,	O
a	O
language	Method
model	Method
,	O
a	O
reordering	Method
model	Method
and	O
a	O
word	Method
/	Method
phrase	Method
penalty	Method
.	O

As	O
such	O
,	O
building	O
an	O
unsupervised	O
SMT	Method
system	O
requires	O
learning	O
these	O
different	O
components	O
from	O
monolingual	O
corpora	O
.	O

As	O
it	O
turns	O
out	O
,	O
this	O
is	O
straightforward	O
for	O
most	O
of	O
them	O
:	O
the	O
language	Method
model	Method
is	O
learned	O
from	O
monolingual	O
corpora	O
by	O
definition	O
;	O
the	O
word	O
and	O
phrase	O
penalties	O
are	O
parameterless	O
;	O
and	O
one	O
can	O
drop	O
the	O
standard	O
lexical	Method
reordering	Method
model	Method
at	O
a	O
small	O
cost	O
and	O
do	O
with	O
the	O
distortion	Method
model	Method
alone	O
,	O
which	O
is	O
also	O
parameterless	O
.	O

This	O
way	O
,	O
the	O
main	O
challenge	O
left	O
is	O
learning	O
the	O
translation	Method
model	Method
,	O
that	O
is	O
,	O
building	O
the	O
phrase	O
-	O
table	O
.	O

Our	O
proposed	O
method	O
starts	O
by	O
building	O
an	O
initial	O
phrase	O
-	O
table	O
through	O
cross	Method
-	Method
lingual	Method
embedding	Method
mappings	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

This	O
initial	O
phrase	O
-	O
table	O
is	O
then	O
extended	O
by	O
incorporating	O
subword	O
information	O
,	O
addressing	O
one	O
of	O
the	O
main	O
limitations	O
of	O
previous	O
unsupervised	O
SMT	Method
systems	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Having	O
done	O
that	O
,	O
we	O
adjust	O
the	O
weights	O
of	O
the	O
underlying	O
log	Method
-	Method
linear	Method
model	Method
through	O
a	O
novel	O
unsupervised	Method
tuning	Method
procedure	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
we	O
further	O
improve	O
the	O
system	O
by	O
jointly	O
refining	O
two	O
models	O
in	O
opposite	O
directions	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Initial	O
phrase	O
-	O
table	O
So	O
as	O
to	O
build	O
our	O
initial	O
phrase	O
-	O
table	O
,	O
we	O
follow	O
artetxe2018usmt	O
and	O
learn	O
n	Method
-	Method
gram	Method
embeddings	Method
for	O
each	O
language	O
independently	O
,	O
map	O
them	O
to	O
a	O
shared	O
space	O
through	O
self	Method
-	Method
learning	Method
,	O
and	O
use	O
the	O
resulting	O
cross	Method
-	Method
lingual	Method
embeddings	Method
to	O
extract	O
and	O
score	O
phrase	O
pairs	O
.	O

More	O
concretely	O
,	O
we	O
train	O
our	O
n	Method
-	Method
gram	Method
embeddings	Method
using	O
phrase2vechttps:	O
//	O
github.com	O
/	O
artetxem	O
/	O
phrase2vec	O
,	O
a	O
simple	O
extension	O
of	O
skip	Method
-	Method
gram	Method
that	O
applies	O
the	O
standard	O
negative	Method
sampling	Method
loss	Method
of	O
mikolov2013distributed	O
to	O
bigram	O
-	O
context	O
and	O
trigram	O
-	O
context	O
pairs	O
in	O
addition	O
to	O
the	O
usual	O
word	O
-	O
context	O
pairs	O
.	O

Having	O
done	O
that	O
,	O
we	O
map	O
the	O
embeddings	O
to	O
a	O
cross	O
-	O
lingual	O
space	O
using	O
VecMap	Method
with	O
identical	Method
initialization	Method
artetxe2018robust	O
,	O
which	O
builds	O
an	O
initial	O
solution	O
by	O
aligning	O
identical	O
words	O
and	O
iteratively	O
improves	O
it	O
through	O
self	Method
-	Method
learning	Method
.	O

Finally	O
,	O
we	O
extract	O
translation	O
candidates	O
by	O
taking	O
the	O
100	O
nearest	O
-	O
neighbors	O
of	O
each	O
source	O
phrase	O
,	O
and	O
score	O
them	O
by	O
applying	O
the	O
softmax	Method
function	Method
over	O
their	O
cosine	O
similarities	O
:	O
where	O
the	O
temperature	O
is	O
estimated	O
using	O
maximum	Method
likelihood	Method
estimation	Method
over	O
a	O
dictionary	O
induced	O
in	O
the	O
reverse	O
direction	O
.	O

In	O
addition	O
to	O
the	O
phrase	O
translation	O
probabilities	O
in	O
both	O
directions	O
,	O
the	O
forward	O
and	O
reverse	O
lexical	O
weightings	O
are	O
also	O
estimated	O
by	O
aligning	O
each	O
word	O
in	O
the	O
target	O
phrase	O
with	O
the	O
one	O
in	O
the	O
source	O
phrase	O
most	O
likely	O
generating	O
it	O
,	O
and	O
taking	O
the	O
product	O
of	O
their	O
respective	O
translation	O
probabilities	O
.	O

The	O
reader	O
is	O
referred	O
to	O
artetxe2018usmt	O
for	O
more	O
details	O
.	O

subsection	O
:	O
Adding	O
subword	O
information	O
An	O
inherent	O
limitation	O
of	O
existing	O
unsupervised	O
SMT	Method
systems	O
is	O
that	O
words	O
are	O
taken	O
as	O
atomic	O
units	O
,	O
making	O
it	O
impossible	O
to	O
exploit	O
character	O
-	O
level	O
information	O
.	O

This	O
is	O
reflected	O
in	O
the	O
known	O
difficulty	O
of	O
these	O
models	O
to	O
translate	O
named	O
entities	O
,	O
as	O
it	O
is	O
very	O
challenging	O
to	O
discriminate	O
among	O
related	O
proper	O
nouns	O
based	O
on	O
distributional	O
information	O
alone	O
,	O
yielding	O
to	O
translation	O
errors	O
like	O
‘	O
‘	O
Sunday	O
Telegraph	O
’	O
’	O
‘	O
‘	O
The	O
Times	O
of	O
London	O
’	O
’	O
artetxe2018usmt	O
.	O

So	O
as	O
to	O
overcome	O
this	O
issue	O
,	O
we	O
propose	O
to	O
incorporate	O
subword	O
information	O
once	O
the	O
initial	O
alignment	O
is	O
done	O
at	O
the	O
word	O
/	O
phrase	O
level	O
.	O

For	O
that	O
purpose	O
,	O
we	O
add	O
two	O
additional	O
weights	O
to	O
the	O
initial	O
phrase	O
-	O
table	O
that	O
are	O
analogous	O
to	O
the	O
lexical	O
weightings	O
,	O
but	O
use	O
a	O
character	O
-	O
level	O
similarity	Method
function	Method
instead	O
of	O
word	O
translation	O
probabilities	O
:	O
where	O
guarantees	O
a	O
minimum	Metric
similarity	Metric
score	Metric
,	O
as	O
we	O
want	O
to	O
favor	O
translation	O
candidates	O
that	O
are	O
similar	O
at	O
the	O
character	O
level	O
without	O
excessively	O
penalizing	O
those	O
that	O
are	O
not	O
.	O

In	O
our	O
case	O
,	O
we	O
use	O
a	O
simple	O
similarity	Method
function	Method
that	O
normalizes	O
the	O
Levenshtein	O
distance	O
levenshtein1966binary	O
by	O
the	O
length	O
of	O
the	O
words	O
:	O
We	O
leave	O
the	O
exploration	O
of	O
more	O
elaborated	O
similarity	Method
functions	Method
and	O
,	O
in	O
particular	O
,	O
learnable	Metric
metrics	Metric
mccallum2005conditional	O
,	O
for	O
future	O
work	O
.	O

subsection	O
:	O
Unsupervised	Method
tuning	Method
Having	O
trained	O
the	O
underlying	O
statistical	Method
models	Method
independently	O
,	O
SMT	Method
tuning	O
aims	O
to	O
adjust	O
the	O
weights	O
of	O
their	O
resulting	O
log	Method
-	Method
linear	Method
combination	Method
to	O
optimize	O
some	O
evaluation	Metric
metric	Metric
like	O
BLEU	Metric
in	O
a	O
parallel	O
validation	O
corpus	O
,	O
which	O
is	O
typically	O
done	O
through	O
Minimum	Metric
Error	Metric
Rate	Metric
Training	Metric
or	O
MERT	Metric
och2003MERT	O
.	O

Needless	O
to	O
say	O
,	O
this	O
can	O
not	O
be	O
done	O
in	O
strictly	O
unsupervised	Task
settings	Task
,	O
but	O
we	O
argue	O
that	O
it	O
would	O
still	O
be	O
desirable	O
to	O
optimize	O
some	O
unsupervised	Metric
criterion	Metric
that	O
is	O
expected	O
to	O
correlate	O
well	O
with	O
test	O
performance	O
.	O

Unfortunately	O
,	O
neither	O
of	O
the	O
existing	O
unsupervised	O
SMT	Method
systems	O
do	O
so	O
:	O
artetxe2018usmt	O
use	O
a	O
heuristic	O
that	O
builds	O
two	O
initial	O
models	O
in	O
opposite	O
directions	O
,	O
uses	O
one	O
of	O
them	O
to	O
generates	O
a	O
synthetic	O
parallel	O
corpus	O
through	O
back	Method
-	Method
translation	Method
sennrich2016improving	O
,	O
and	O
applies	O
MERT	Metric
to	O
tune	O
the	O
model	O
in	O
the	O
reverse	O
direction	O
,	O
iterating	O
until	O
convergence	O
,	O
whereas	O
lample2018phrase	O
do	O
not	O
perform	O
any	O
tuning	O
at	O
all	O
.	O

In	O
what	O
follows	O
,	O
we	O
propose	O
a	O
more	O
principled	O
approach	O
to	O
tuning	Task
that	O
defines	O
an	O
unsupervised	Metric
criterion	Metric
and	O
an	O
optimization	Method
procedure	Method
that	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
of	O
it	O
.	O

Inspired	O
by	O
the	O
previous	O
work	O
on	O
CycleGANs	Method
zhu2017unpaired	O
and	O
dual	Method
learning	Method
he2016dual	O
,	O
our	O
method	O
takes	O
two	O
initial	O
models	O
in	O
opposite	O
directions	O
,	O
and	O
defines	O
an	O
unsupervised	Method
optimization	Method
objective	Method
that	O
combines	O
a	O
cyclic	Method
consistency	Method
loss	Method
and	O
a	O
language	Method
model	Method
loss	Method
over	O
the	O
two	O
monolingual	O
corpora	O
and	O
:	O
The	O
cyclic	O
consistency	O
loss	O
captures	O
the	O
intuition	O
that	O
the	O
translation	O
of	O
a	O
translation	O
should	O
be	O
close	O
to	O
the	O
original	O
text	O
.	O

So	O
as	O
to	O
quantify	O
this	O
,	O
we	O
take	O
a	O
monolingual	O
corpus	O
in	O
the	O
source	O
language	O
,	O
translate	O
it	O
to	O
the	O
target	O
language	O
and	O
back	O
to	O
the	O
source	O
language	O
,	O
and	O
compute	O
its	O
BLEU	Metric
score	Metric
taking	O
the	O
original	O
text	O
as	O
reference	O
:	O
At	O
the	O
same	O
time	O
,	O
the	O
language	Method
model	Method
loss	Method
captures	O
the	O
intuition	O
that	O
machine	Task
translation	Task
should	O
produce	O
fluent	O
text	O
in	O
the	O
target	O
language	O
.	O

For	O
that	O
purpose	O
,	O
we	O
estimate	O
the	O
per	O
-	O
word	O
entropy	O
in	O
the	O
target	O
language	O
corpus	O
using	O
an	O
n	Method
-	Method
gram	Method
language	Method
model	Method
,	O
and	O
penalize	O
higher	O
per	O
-	O
word	O
entropies	O
in	O
machine	O
translated	O
text	O
as	O
follows	O
:	O
where	O
the	O
length	O
penalty	O
penalizes	O
excessively	O
long	O
translations	O
:	O
So	O
as	O
to	O
minimize	O
the	O
combined	O
loss	O
function	O
,	O
we	O
adapt	O
MERT	Metric
to	O
jointly	O
optimize	O
the	O
parameters	O
of	O
the	O
two	O
models	O
.	O

In	O
its	O
basic	O
form	O
,	O
MERT	Metric
approximates	O
the	O
search	O
space	O
for	O
each	O
source	O
sentence	O
through	O
an	O
n	O
-	O
best	O
list	O
,	O
and	O
performs	O
a	O
form	O
of	O
coordinate	Method
descent	Method
by	O
computing	O
the	O
optimal	O
value	O
for	O
each	O
parameter	O
through	O
an	O
efficient	O
line	Method
search	Method
method	Method
and	O
greedily	O
taking	O
the	O
step	O
that	O
leads	O
to	O
the	O
largest	O
gain	O
.	O

The	O
process	O
is	O
repeated	O
iteratively	O
until	O
convergence	O
,	O
augmenting	O
the	O
n	O
-	O
best	O
list	O
with	O
the	O
updated	O
parameters	O
at	O
each	O
iteration	O
so	O
as	O
to	O
obtain	O
a	O
better	O
approximation	O
of	O
the	O
full	O
search	O
space	O
.	O

Given	O
that	O
our	O
optimization	Task
objective	Task
combines	O
two	O
translation	Method
systems	Method
,	O
this	O
would	O
require	O
generating	O
an	O
n	O
-	O
best	O
list	O
for	O
first	O
and	O
,	O
for	O
each	O
entry	O
on	O
it	O
,	O
generating	O
a	O
new	O
n	O
-	O
best	O
list	O
with	O
,	O
yielding	O
a	O
combined	O
n	O
-	O
best	O
list	O
with	O
entries	O
.	O

So	O
as	O
to	O
make	O
it	O
more	O
efficient	O
,	O
we	O
propose	O
an	O
alternating	Method
optimization	Method
approach	Method
where	O
we	O
fix	O
the	O
parameters	O
of	O
one	O
model	O
and	O
optimize	O
the	O
other	O
with	O
standard	O
MERT	Metric
.	O

Thanks	O
to	O
this	O
,	O
we	O
do	O
not	O
need	O
to	O
expand	O
the	O
search	O
space	O
of	O
the	O
fixed	Method
model	Method
,	O
so	O
we	O
can	O
do	O
with	O
an	O
n	O
-	O
best	O
list	O
of	O
entries	O
alone	O
.	O

Having	O
done	O
that	O
,	O
we	O
fix	O
the	O
parameters	O
of	O
the	O
opposite	O
model	O
and	O
optimize	O
the	O
other	O
,	O
iterating	O
until	O
convergence	O
.	O

subsection	O
:	O
Joint	Task
refinement	Task
Constrained	O
by	O
the	O
lack	O
of	O
parallel	O
corpora	O
,	O
the	O
procedure	O
described	O
so	O
far	O
makes	O
important	O
simplifications	O
that	O
could	O
compromise	O
its	O
potential	O
performance	O
:	O
its	O
phrase	O
-	O
table	O
is	O
somewhat	O
unnatural	O
(	O
e.g.	O
the	O
translation	O
probabilities	O
are	O
estimated	O
from	O
cross	Method
-	Method
lingual	Method
embeddings	Method
rather	O
than	O
actual	O
frequency	O
counts	O
)	O
and	O
it	O
lacks	O
a	O
lexical	Method
reordering	Method
model	Method
altogether	O
.	O

So	O
as	O
to	O
overcome	O
this	O
issue	O
,	O
existing	O
unsupervised	O
SMT	Method
methods	O
generate	O
a	O
synthetic	O
parallel	O
corpus	O
through	O
back	Method
-	Method
translation	Method
and	O
use	O
it	O
to	O
train	O
a	O
standard	O
SMT	Method
system	O
from	O
scratch	O
,	O
iterating	O
until	O
convergence	O
.	O

An	O
obvious	O
drawback	O
of	O
this	O
approach	O
is	O
that	O
the	O
back	O
-	O
translated	O
side	O
will	O
contain	O
ungrammatical	O
n	O
-	O
grams	O
that	O
will	O
end	O
up	O
in	O
the	O
induced	O
phrase	O
-	O
table	O
.	O

One	O
could	O
argue	O
that	O
this	O
should	O
be	O
innocuous	O
as	O
long	O
as	O
the	O
ungrammatical	O
n	O
-	O
grams	O
are	O
in	O
the	O
source	O
side	O
,	O
as	O
they	O
should	O
never	O
occur	O
in	O
real	O
text	O
and	O
their	O
corresponding	O
entries	O
in	O
the	O
phrase	O
-	O
table	O
should	O
therefore	O
not	O
be	O
used	O
.	O

However	O
,	O
ungrammatical	O
source	O
phrases	O
do	O
ultimately	O
affect	O
the	O
estimation	O
of	O
the	O
backward	O
translation	O
probabilities	O
,	O
including	O
those	O
of	O
grammatical	O
phrases	O
.	O

For	O
instance	O
,	O
let	O
’s	O
say	O
that	O
the	O
target	O
phrase	O
‘	O
‘	O
dos	O
gatos	O
’	O
’	O
has	O
been	O
aligned	O
10	O
times	O
with	O
‘	O
‘	O
two	O
cats	O
’	O
’	O
and	O
90	O
times	O
with	O
‘	O
‘	O
two	O
cat	O
’	O
’	O
.	O

While	O
the	O
ungrammatical	O
phrase	O
-	O
table	O
entry	O
two	O
cat	O
-	O
dos	O
gatos	O
should	O
never	O
be	O
picked	O
,	O
the	O
backward	Method
probability	Method
estimation	Method
of	O
two	O
cats	O
-	O
dos	O
gatos	O
is	O
still	O
affected	O
by	O
it	O
(	O
it	O
would	O
be	O
0.1	O
instead	O
of	O
1.0	O
in	O
this	O
example	O
)	O
.	O

We	O
argue	O
that	O
,	O
ultimately	O
,	O
the	O
backward	Method
probability	Method
estimations	Method
can	O
only	O
be	O
meaningful	O
when	O
all	O
source	O
phrases	O
are	O
grammatical	O
(	O
so	O
the	O
probabilities	O
of	O
all	O
plausible	O
translations	O
sum	O
to	O
one	O
)	O
and	O
,	O
similarly	O
,	O
the	O
forward	Method
probability	Method
estimations	Method
can	O
only	O
be	O
meaningful	O
when	O
all	O
target	O
phrases	O
are	O
grammatical	O
.	O

Following	O
this	O
observation	O
,	O
we	O
propose	O
an	O
alternative	O
approach	O
that	O
jointly	O
refines	O
both	O
translation	O
directions	O
.	O

More	O
concretely	O
,	O
we	O
use	O
the	O
initial	O
systems	O
to	O
build	O
two	O
synthetic	O
corpora	O
in	O
opposite	O
directions	O
.	O

Having	O
done	O
that	O
,	O
we	O
independently	O
extract	O
phrase	O
pairs	O
from	O
each	O
synthetic	O
corpus	O
,	O
and	O
build	O
a	O
phrase	O
-	O
table	O
by	O
taking	O
their	O
intersection	O
.	O

The	O
forward	O
probabilities	O
are	O
estimated	O
in	O
the	O
parallel	O
corpus	O
with	O
the	O
synthetic	O
source	O
side	O
,	O
while	O
the	O
backward	O
probabilities	O
are	O
estimated	O
in	O
the	O
one	O
with	O
the	O
synthetic	O
target	O
side	O
.	O

This	O
does	O
not	O
only	O
guarantee	O
that	O
the	O
probability	O
estimates	O
are	O
meaningful	O
as	O
discussed	O
previously	O
,	O
but	O
it	O
also	O
discards	O
the	O
ungrammatical	O
phrases	O
altogether	O
,	O
as	O
both	O
the	O
source	O
and	O
the	O
target	O
n	O
-	O
grams	O
must	O
have	O
occurred	O
in	O
the	O
original	O
monolingual	O
texts	O
to	O
be	O
present	O
in	O
the	O
resulting	O
phrase	O
-	O
table	O
.	O

We	O
repeat	O
this	O
process	O
for	O
a	O
total	O
of	O
3	O
iterations	O
.	O

section	O
:	O
NMT	Method
hybridization	O
While	O
the	O
rigid	O
and	O
modular	O
design	O
of	O
SMT	Method
provides	O
a	O
very	O
suitable	O
framework	O
for	O
unsupervised	Task
machine	Task
translation	Task
,	O
NMT	Method
has	O
shown	O
to	O
be	O
a	O
fairly	O
superior	O
paradigm	O
in	O
supervised	Task
settings	Task
,	O
outperforming	O
SMT	Method
by	O
a	O
large	O
margin	O
in	O
standard	O
benchmarks	O
.	O

As	O
such	O
,	O
the	O
choice	O
of	O
SMT	Method
over	O
NMT	Method
also	O
imposes	O
a	O
hard	O
ceiling	O
on	O
the	O
potential	O
performance	O
of	O
these	O
approaches	O
,	O
as	O
unsupervised	O
SMT	Method
systems	O
inherit	O
the	O
very	O
same	O
limitations	O
of	O
their	O
supervised	Method
counterparts	Method
(	O
e.g.	O
the	O
locality	Task
and	Task
sparsity	Task
problems	Task
)	O
.	O

For	O
that	O
reason	O
,	O
we	O
argue	O
that	O
SMT	Method
provides	O
a	O
more	O
appropriate	O
architecture	O
to	O
find	O
an	O
initial	O
alignment	O
between	O
the	O
languages	O
,	O
but	O
NMT	Method
is	O
ultimately	O
a	O
better	O
architecture	O
to	O
model	O
the	O
translation	Task
process	Task
.	O

Following	O
this	O
observation	O
,	O
we	O
propose	O
a	O
hybrid	Method
approach	Method
that	O
uses	O
unsupervised	Task
SMT	Task
to	O
warm	O
up	O
a	O
dual	O
NMT	Method
model	O
trained	O
through	O
iterative	Method
back	Method
-	Method
translation	Method
.	O

More	O
concretely	O
,	O
we	O
first	O
train	O
two	O
SMT	Method
systems	O
in	O
opposite	O
directions	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
use	O
them	O
to	O
assist	O
the	O
training	O
of	O
another	O
two	O
NMT	Method
systems	O
in	O
opposite	O
directions	O
.	O

These	O
NMT	Method
systems	O
are	O
trained	O
following	O
an	O
iterative	O
process	O
where	O
,	O
at	O
each	O
iteration	O
,	O
we	O
alternately	O
update	O
the	O
model	O
in	O
each	O
direction	O
by	O
performing	O
a	O
single	O
pass	O
over	O
a	O
synthetic	O
parallel	O
corpus	O
built	O
through	O
back	Method
-	Method
translation	Method
sennrich2016improving	O
.	O

In	O
the	O
first	O
iteration	O
,	O
the	O
synthetic	O
parallel	O
corpus	O
is	O
entirely	O
generated	O
by	O
the	O
SMT	Method
system	O
in	O
the	O
opposite	O
direction	O
but	O
,	O
as	O
training	O
progresses	O
and	O
the	O
NMT	Method
models	O
get	O
better	O
,	O
we	O
progressively	O
switch	O
to	O
a	O
synthetic	O
parallel	O
corpus	O
generated	O
by	O
the	O
reverse	O
NMT	Method
model	O
.	O

More	O
concretely	O
,	O
iteration	Method
uses	O
synthetic	O
parallel	O
sentences	O
from	O
the	O
reverse	O
SMT	Method
system	O
,	O
where	O
the	O
parameter	O
controls	O
the	O
number	O
of	O
transition	O
iterations	O
from	O
SMT	Method
to	O
NMT	Method
back	Method
-	Method
translation	Method
.	O

The	O
remaining	O
sentences	O
are	O
generated	O
by	O
the	O
reverse	O
NMT	Method
model	O
.	O

Inspired	O
by	O
edunov2018understanding	O
,	O
we	O
use	O
greedy	Method
decoding	Method
for	O
half	O
of	O
them	O
,	O
which	O
produces	O
more	O
fluent	O
and	O
predictable	O
translations	O
,	O
and	O
random	Method
sampling	Method
for	O
the	O
other	O
half	O
,	O
which	O
produces	O
more	O
varied	O
translations	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
and	O
,	O
and	O
perform	O
a	O
total	O
of	O
60	O
such	O
iterations	O
.	O

At	O
test	O
time	O
,	O
we	O
use	O
beam	Method
search	Method
decoding	Method
with	O
an	O
ensemble	O
of	O
all	O
checkpoints	O
from	O
every	O
10	O
iterations	O
.	O

section	O
:	O
Experiments	O
and	O
results	O
SMT	Method
+	O
NMT	Method
In	O
order	O
to	O
make	O
our	O
experiments	O
comparable	O
to	O
previous	O
work	O
,	O
we	O
use	O
the	O
French	Material
-	Material
English	Material
and	O
German	Material
-	Material
English	Material
datasets	O
from	O
the	O
WMT	Material
2014	Material
shared	Material
task	Material
.	O

More	O
concretely	O
,	O
our	O
training	O
data	O
consists	O
of	O
the	O
concatenation	O
of	O
all	O
News	Material
Crawl	Material
monolingual	Material
corpora	Material
from	O
2007	O
to	O
2013	O
,	O
which	O
make	O
a	O
total	O
of	O
749	O
million	O
tokens	O
in	O
French	O
,	O
1	O
,	O
606	O
millions	O
in	O
German	Material
,	O
and	O
2	O
,	O
109	O
millions	O
in	O
English	Material
,	O
from	O
which	O
we	O
take	O
a	O
random	O
subset	O
of	O
2	O
,	O
000	O
sentences	O
for	O
tuning	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Preprocessing	Task
is	O
done	O
using	O
standard	O
Moses	Method
tools	O
,	O
and	O
involves	O
punctuation	Method
normalization	Method
,	O
tokenization	Method
with	O
aggressive	Method
hyphen	Method
splitting	Method
,	O
and	O
truecasing	Method
.	O

Our	O
SMT	Method
implementation	O
is	O
based	O
on	O
Moses	Method
,	O
and	O
we	O
use	O
the	O
KenLM	Method
heafield2013scalable	O
tool	O
included	O
in	O
it	O
to	O
estimate	O
our	O
5	Method
-	Method
gram	Method
language	Method
model	Method
with	O
modified	O
Kneser	Method
-	Method
Ney	Method
smoothing	Method
.	O

Our	O
unsupervised	Method
tuning	Method
implementation	Method
is	O
based	O
on	O
Z	O
-	O
MERT	Metric
zaidan2009zmert	O
,	O
and	O
we	O
use	O
FastAlign	Method
dyer2013simple	O
for	O
word	Task
alignment	Task
within	O
the	O
joint	Method
refinement	Method
procedure	Method
.	O

Finally	O
,	O
we	O
use	O
the	O
big	Method
transformer	Method
implementation	Method
from	O
fairseq	Method
for	O
our	O
NMT	Method
system	O
,	O
training	O
with	O
a	O
total	O
batch	O
size	O
of	O
20	O
,	O
000	O
tokens	O
across	O
8	O
GPUs	O
with	O
the	O
exact	O
same	O
hyperparameters	O
as	O
ott2018scaling	O
.	O

We	O
use	O
newstest2014	O
as	O
our	O
test	O
set	O
for	O
French	Material
-	Material
English	Material
,	O
and	O
both	O
newstest2014	O
and	O
newstest2016	O
(	O
from	O
WMT	Material
2016	Material
)	O
for	O
German	Material
-	Material
English	Material
.	O

Following	O
common	O
practice	O
,	O
we	O
report	O
tokenized	Metric
BLEU	Metric
scores	Metric
as	O
computed	O
by	O
the	O
multi	Method
-	Method
bleu.perl	Method
script	Method
included	O
in	O
Moses	Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
also	O
report	O
detokenized	Metric
BLEU	Metric
scores	Metric
as	O
computed	O
by	O
SacreBLEU	O
post2018call	O
,	O
which	O
is	O
equivalent	O
to	O
the	O
official	O
mteval	O
-	O
v13a.pl	O
script	O
.	O

We	O
next	O
present	O
the	O
results	O
of	O
our	O
proposed	O
system	O
in	O
comparison	O
to	O
previous	O
work	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Section	O
[	O
reference	O
]	O
then	O
compares	O
the	O
obtained	O
results	O
to	O
those	O
of	O
different	O
supervised	Method
systems	Method
.	O

Finally	O
,	O
Section	O
[	O
reference	O
]	O
presents	O
some	O
translation	O
examples	O
from	O
our	O
system	O
.	O

subsection	O
:	O
Main	O
results	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
the	O
proposed	O
system	O
in	O
comparison	O
to	O
previous	O
work	O
.	O

As	O
it	O
can	O
be	O
seen	O
,	O
our	O
full	O
system	O
obtains	O
the	O
best	O
published	O
results	O
in	O
all	O
cases	O
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
5	O
-	O
7	O
BLEU	Metric
points	Metric
in	O
all	O
datasets	O
and	O
translation	O
directions	O
.	O

A	O
substantial	O
part	O
of	O
this	O
improvement	O
comes	O
from	O
our	O
more	O
principled	O
unsupervised	O
SMT	Method
approach	O
,	O
which	O
outperforms	O
all	O
previous	O
SMT	Method
-	O
based	O
systems	O
by	O
around	O
2	O
BLEU	Metric
points	Metric
.	O

Nevertheless	O
,	O
it	O
is	O
the	O
NMT	Method
hybridization	O
that	O
brings	O
the	O
largest	O
gains	O
,	O
improving	O
the	O
results	O
of	O
this	O
initial	O
SMT	Method
systems	O
by	O
5	O
-	O
9	O
BLEU	Metric
points	Metric
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
our	O
absolute	O
gains	O
are	O
considerably	O
larger	O
than	O
those	O
of	O
previous	O
hybridization	Method
methods	Method
,	O
even	O
if	O
our	O
initial	O
SMT	Method
system	O
is	O
substantially	O
better	O
and	O
thus	O
more	O
difficult	O
to	O
improve	O
upon	O
.	O

This	O
way	O
,	O
our	O
initial	O
SMT	Method
system	O
is	O
about	O
4	O
-	O
5	O
BLEU	Metric
points	Metric
above	O
that	O
of	O
marie2018unsupervised	O
,	O
yet	O
our	O
absolute	O
gain	O
on	O
top	O
of	O
it	O
is	O
around	O
2.5	O
BLEU	Metric
points	Metric
higher	O
.	O

When	O
compared	O
to	O
lample2018phrase	O
,	O
we	O
obtain	O
an	O
absolute	O
gain	O
of	O
5	O
-	O
6	O
BLEU	Metric
points	Metric
in	O
both	O
French	Material
-	Material
English	Material
directions	O
while	O
they	O
do	O
not	O
get	O
any	O
clear	O
improvement	O
,	O
and	O
we	O
obtain	O
an	O
improvement	O
of	O
7	O
-	O
9	O
BLEU	Metric
points	Metric
in	O
both	O
German	Material
-	Material
English	Material
directions	O
,	O
in	O
contrast	O
with	O
the	O
2.3	O
BLEU	Metric
points	Metric
they	O
obtain	O
.	O

More	O
generally	O
,	O
it	O
is	O
interesting	O
that	O
pure	O
SMT	Method
systems	O
perform	O
better	O
than	O
pure	O
NMT	Method
systems	O
,	O
yet	O
the	O
best	O
results	O
are	O
obtained	O
by	O
initializing	O
an	O
NMT	Method
system	O
with	O
an	O
SMT	Method
system	O
.	O

This	O
suggests	O
that	O
the	O
rigid	Method
and	Method
modular	Method
architecture	Method
of	O
SMT	Method
might	O
be	O
more	O
suitable	O
to	O
find	O
an	O
initial	O
alignment	O
between	O
the	O
languages	O
,	O
but	O
the	O
final	O
system	O
should	O
be	O
ultimately	O
based	O
on	O
NMT	Method
for	O
optimal	O
results	O
.	O

subsection	O
:	O
Comparison	O
with	O
supervised	Method
systems	Method
So	O
as	O
to	O
put	O
our	O
results	O
into	O
perspective	O
,	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
different	O
supervised	Method
systems	Method
in	O
the	O
same	O
WMT	Material
2014	Material
test	Material
set	Material
.	O

More	O
concretely	O
,	O
we	O
include	O
the	O
best	O
results	O
from	O
the	O
shared	O
task	O
itself	O
,	O
which	O
reflect	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
machine	Task
translation	Task
back	O
in	O
2014	O
;	O
those	O
of	O
vaswani2017attention	O
,	O
who	O
introduced	O
the	O
now	O
predominant	O
transformer	Method
architecture	Method
;	O
and	O
those	O
of	O
edunov2018understanding	Method
,	O
who	O
apply	O
back	Method
-	Method
translation	Method
at	O
a	O
large	O
scale	O
and	O
hold	O
the	O
current	O
best	O
results	O
in	O
the	O
test	O
set	O
.	O

As	O
it	O
can	O
be	O
seen	O
,	O
our	O
unsupervised	Method
system	Method
outperforms	O
the	O
WMT	Material
2014	O
shared	O
task	O
winner	O
in	O
English	Material
-	Material
to	Material
-	Material
German	Material
,	O
and	O
is	O
around	O
2	O
BLEU	Metric
points	Metric
behind	O
it	O
in	O
the	O
other	O
translation	O
directions	O
.	O

This	O
shows	O
that	O
unsupervised	Task
machine	Task
translation	Task
is	O
already	O
competitive	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
supervised	Task
machine	Task
translation	Task
in	O
2014	O
.	O

While	O
the	O
field	O
of	O
machine	Task
translation	Task
has	O
undergone	O
great	O
progress	O
in	O
the	O
last	O
5	O
years	O
,	O
and	O
the	O
gap	O
between	O
our	O
unsupervised	Method
system	Method
and	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
supervised	Task
machine	Task
translation	Task
is	O
still	O
large	O
as	O
reflected	O
by	O
the	O
other	O
results	O
,	O
this	O
suggests	O
that	O
unsupervised	Task
machine	Task
translation	Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

subsection	O
:	O
Qualitative	O
results	O
Table	O
[	O
reference	O
]	O
shows	O
some	O
translation	O
examples	O
from	O
our	O
proposed	O
system	O
in	O
comparison	O
to	O
those	O
reported	O
by	O
artetxe2018usmt	O
.	O

We	O
choose	O
the	O
exact	O
same	O
sentences	O
reported	O
by	O
artetxe2018usmt	O
,	O
which	O
were	O
randomly	O
taken	O
from	O
newstest2014	O
,	O
so	O
they	O
should	O
be	O
representative	O
of	O
the	O
general	O
behavior	O
of	O
both	O
systems	O
.	O

While	O
not	O
perfect	O
,	O
our	O
proposed	O
system	O
produces	O
generally	O
fluent	O
translations	O
that	O
accurately	O
capture	O
the	O
meaning	O
of	O
the	O
original	O
text	O
.	O

Just	O
in	O
line	O
with	O
our	O
quantitative	O
results	O
,	O
this	O
suggests	O
that	O
unsupervised	Task
machine	Task
translation	Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

Compared	O
to	O
artetxe2018usmt	O
,	O
our	O
translations	O
are	O
generally	O
more	O
fluent	O
,	O
which	O
is	O
not	O
surprising	O
given	O
that	O
they	O
are	O
produced	O
by	O
an	O
NMT	Method
system	O
rather	O
than	O
an	O
SMT	Method
system	O
.	O

In	O
addition	O
to	O
that	O
,	O
the	O
system	O
of	O
artetxe2018usmt	O
has	O
some	O
adequacy	O
issues	O
when	O
translating	O
named	O
entities	O
and	O
numerals	O
(	O
e.g.	O
34	O
32	O
,	O
Sunday	O
Telegraph	O
The	O
Times	O
of	O
London	O
)	O
,	O
which	O
we	O
do	O
not	O
observe	O
for	O
our	O
proposed	O
system	O
in	O
these	O
examples	O
.	O

section	O
:	O
Conclusions	O
and	O
future	O
work	O
In	O
this	O
paper	O
,	O
we	O
identify	O
several	O
deficiencies	O
in	O
previous	O
unsupervised	O
SMT	Method
systems	O
,	O
and	O
propose	O
a	O
more	O
principled	O
approach	O
that	O
addresses	O
them	O
by	O
incorporating	O
subword	O
information	O
,	O
using	O
a	O
theoretically	O
well	O
founded	O
unsupervised	Method
tuning	Method
method	Method
,	O
and	O
developing	O
a	O
joint	Method
refinement	Method
procedure	Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
use	O
our	O
improved	O
SMT	Method
approach	O
to	O
initialize	O
a	O
dual	O
NMT	Method
model	O
that	O
is	O
further	O
improved	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	Method
-	Method
translation	Method
.	O

Our	O
experiments	O
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
as	O
we	O
improve	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	Task
machine	Task
translation	Task
by	O
5	O
-	O
7	O
BLEU	Metric
points	Metric
in	O
French	Material
-	Material
English	Material
and	O
German	Material
-	Material
English	Material
WMT	Material
2014	Material
and	O
2016	Material
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
explore	O
learnable	O
similarity	Method
functions	Method
like	O
the	O
one	O
proposed	O
by	O
mccallum2005conditional	O
to	O
compute	O
the	O
character	O
-	O
level	O
scores	O
in	O
our	O
initial	O
phrase	O
-	O
table	O
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
would	O
like	O
to	O
incorporate	O
a	O
language	Method
modeling	Method
loss	Method
during	O
NMT	Method
training	O
similar	O
to	O
he2016dual	O
.	O

Finally	O
,	O
we	O
would	O
like	O
to	O
adapt	O
our	O
approach	O
to	O
more	O
relaxed	O
scenarios	O
with	O
multiple	O
languages	O
and	O
/	O
or	O
small	O
parallel	O
corpora	O
.	O

section	O
:	O
Acknowledgments	O
This	O
research	O
was	O
partially	O
supported	O
by	O
the	O
Spanish	O
MINECO	O
(	O
UnsupNMT	O
TIN2017‐91692‐EXP	O
,	O
cofunded	O
by	O
EU	O
FEDER	O
)	O
,	O
the	O
UPV	O
/	O
EHU	O
(	O
excellence	O
research	O
group	O
)	O
,	O
and	O
the	O
NVIDIA	O
GPU	O
grant	O
program	O
.	O

Mikel	O
Artetxe	O
enjoys	O
a	O
doctoral	O
grant	O
from	O
the	O
Spanish	O
MECD	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Frustum	Method
PointNets	Method
for	O
3D	Task
Object	Task
Detection	Task
from	O
RGB	Material
-	Material
D	Material
Data	Material
In	O
this	O
work	O
,	O
we	O
study	O
3D	O
object	O
detection	Task
from	O
RGB	Material
-	Material
D	Material
data	Material
in	O
both	O
indoor	O
and	O
outdoor	O
scenes	O
.	O

While	O
previous	O
methods	O
focus	O
on	O
images	O
or	O
3D	O
voxels	O
,	O
often	O
obscuring	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
3D	O
data	O
,	O
we	O
directly	O
operate	O
on	O
raw	O
point	O
clouds	O
by	O
popping	O
up	O
RGB	Material
-	Material
D	Material
scans	Material
.	O

However	O
,	O
a	O
key	O
challenge	O
of	O
this	O
approach	O
is	O
how	O
to	O
efficiently	O
localize	Task
objects	Task
in	O
point	Task
clouds	Task
of	Task
large	Task
-	Task
scale	Task
scenes	Task
(	O
region	Task
proposal	Task
)	O
.	O

Instead	O
of	O
solely	O
relying	O
on	O
3D	O
proposals	O
,	O
our	O
method	O
leverages	O
both	O
mature	O
2D	Method
object	Method
detectors	Method
and	O
advanced	O
3D	Method
deep	Method
learning	Method
for	O
object	Task
localization	Task
,	O
achieving	O
efficiency	O
as	O
well	O
as	O
high	O
recall	Metric
for	O
even	O
small	O
objects	O
.	O

Benefited	O
from	O
learning	O
directly	O
in	O
raw	Task
point	Task
clouds	Task
,	O
our	O
method	O
is	O
also	O
able	O
to	O
precisely	O
estimate	O
3D	Task
bounding	Task
boxes	Task
even	O
under	O
strong	O
occlusion	O
or	O
with	O
very	O
sparse	O
points	O
.	O

Evaluated	O
on	O
KITTI	Material
and	O
SUN	O
RGB	O
-	O
D	O
3D	Task
detection	Task
benchmarks	O
,	O
our	O
method	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
remarkable	O
margins	O
while	O
having	O
real	Metric
-	Metric
time	Metric
capability	Metric
.	O

section	O
:	O
Introduction	O
Recently	O
,	O
great	O
progress	O
has	O
been	O
made	O
on	O
2D	Task
image	Task
understanding	Task
tasks	Task
,	O
such	O
as	O
object	O
detection	Task
and	O
instance	Task
segmentation	Task
.	O

However	O
,	O
beyond	O
getting	O
2D	O
bounding	O
boxes	O
or	O
pixel	O
masks	O
,	O
3D	Task
understanding	Task
is	O
eagerly	O
in	O
demand	O
in	O
many	O
applications	O
such	O
as	O
autonomous	Task
driving	Task
and	O
augmented	Task
reality	Task
(	O
AR	Task
)	O
.	O

With	O
the	O
popularity	O
of	O
3D	Method
sensors	Method
deployed	O
on	O
mobile	O
devices	O
and	O
autonomous	O
vehicles	O
,	O
more	O
and	O
more	O
3D	O
data	O
is	O
captured	O
and	O
processed	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
one	O
of	O
the	O
most	O
important	O
3D	Task
perception	Task
tasks	Task
–	O
3D	O
object	O
detection	Task
,	O
which	O
classifies	O
the	O
object	Task
category	Task
and	O
estimates	O
oriented	Task
3D	Task
bounding	Task
boxes	Task
of	Task
physical	Task
objects	Task
from	O
3D	O
sensor	O
data	O
.	O

While	O
3D	O
sensor	O
data	O
is	O
often	O
in	O
the	O
form	O
of	O
point	O
clouds	O
,	O
how	O
to	O
represent	O
point	O
cloud	O
and	O
what	O
deep	Method
net	Method
architectures	Method
to	O
use	O
for	O
3D	O
object	O
detection	Task
remains	O
an	O
open	O
problem	O
.	O

Most	O
existing	O
works	O
convert	O
3D	O
point	O
clouds	O
to	O
images	O
by	O
projection	O
or	O
to	O
volumetric	O
grids	O
by	O
quantization	O
and	O
then	O
apply	O
convolutional	Method
networks	Method
.	O

This	O
data	Method
representation	Method
transformation	Method
,	O
however	O
,	O
may	O
obscure	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
the	O
data	O
.	O

Recently	O
,	O
a	O
number	O
of	O
papers	O
have	O
proposed	O
to	O
process	O
point	Task
clouds	Task
directly	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

For	O
example	O
,	O
proposed	O
new	O
types	O
of	O
deep	Method
net	Method
architectures	Method
,	O
called	O
PointNets	Method
,	O
which	O
have	O
shown	O
superior	O
performance	O
and	O
efficiency	O
in	O
several	O
3D	Task
understanding	Task
tasks	Task
such	O
as	O
object	Task
classification	Task
and	O
semantic	Task
segmentation	Task
.	O

While	O
PointNets	Method
are	O
capable	O
of	O
classifying	O
a	O
whole	Task
point	Task
cloud	Task
or	O
predicting	O
a	O
semantic	O
class	O
for	O
each	O
point	O
in	O
a	O
point	O
cloud	O
,	O
it	O
is	O
unclear	O
how	O
this	O
architecture	O
can	O
be	O
used	O
for	O
instance	O
-	O
level	O
3D	O
object	O
detection	Task
.	O

Towards	O
this	O
goal	O
,	O
we	O
have	O
to	O
address	O
one	O
key	O
challenge	O
:	O
how	O
to	O
efficiently	O
propose	O
possible	O
locations	O
of	O
3D	O
objects	O
in	O
a	O
3D	O
space	O
.	O

Imitating	O
the	O
practice	O
in	O
image	Task
detection	Task
,	O
it	O
is	O
straightforward	O
to	O
enumerate	O
candidate	O
3D	O
boxes	O
by	O
sliding	O
windows	O
or	O
by	O
3D	Method
region	Method
proposal	Method
networks	Method
such	O
as	O
.	O

However	O
,	O
the	O
computational	Metric
complexity	Metric
of	O
3D	Task
search	Task
typically	O
grows	O
cubically	O
with	O
respect	O
to	O
resolution	O
and	O
becomes	O
too	O
expensive	O
for	O
large	Task
scenes	Task
or	O
real	Task
-	Task
time	Task
applications	Task
such	O
as	O
autonomous	Task
driving	Task
.	O

Instead	O
,	O
in	O
this	O
work	O
,	O
we	O
reduce	O
the	O
search	O
space	O
following	O
the	O
dimension	Method
reduction	Method
principle	Method
:	O
we	O
take	O
the	O
advantage	O
of	O
mature	O
2D	Method
object	Method
detectors	Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

First	O
,	O
we	O
extract	O
the	O
3D	O
bounding	O
frustum	O
of	O
an	O
object	O
by	O
extruding	O
2D	O
bounding	O
boxes	O
from	O
image	Method
detectors	Method
.	O

Then	O
,	O
within	O
the	O
3D	O
space	O
trimmed	O
by	O
each	O
of	O
the	O
3D	O
frustums	O
,	O
we	O
consecutively	O
perform	O
3D	Method
object	Method
instance	Method
segmentation	Method
and	O
amodal	O
3D	Method
bounding	Method
box	Method
regression	Method
using	O
two	O
variants	O
of	O
PointNet	Method
.	O

The	O
segmentation	Method
network	Method
predicts	O
the	O
3D	O
mask	O
of	O
the	O
object	O
of	O
interest	O
(	O
i.e.	O
instance	Task
segmentation	Task
)	O
;	O
and	O
the	O
regression	Method
network	Method
estimates	O
the	O
amodal	O
3D	O
bounding	O
box	O
(	O
covering	O
the	O
entire	O
object	O
even	O
if	O
only	O
part	O
of	O
it	O
is	O
visible	O
)	O
.	O

In	O
contrast	O
to	O
previous	O
work	O
that	O
treats	O
RGB	Material
-	Material
D	Material
data	Material
as	O
2D	O
maps	O
for	O
CNNs	Method
,	O
our	O
method	O
is	O
more	O
3D	O
-	O
centric	O
as	O
we	O
lift	O
depth	O
maps	O
to	O
3D	O
point	O
clouds	O
and	O
process	O
them	O
using	O
3D	Method
tools	Method
.	O

This	O
3D	Method
-	Method
centric	Method
view	Method
enables	O
new	O
capabilities	O
for	O
exploring	O
3D	Task
data	Task
in	O
a	O
more	O
effective	O
manner	O
.	O

First	O
,	O
in	O
our	O
pipeline	O
,	O
a	O
few	O
transformations	O
are	O
applied	O
successively	O
on	O
3D	O
coordinates	O
,	O
which	O
align	O
point	O
clouds	O
into	O
a	O
sequence	O
of	O
more	O
constrained	O
and	O
canonical	O
frames	O
.	O

These	O
alignments	O
factor	O
out	O
pose	O
variations	O
in	O
data	O
,	O
and	O
thus	O
make	O
3D	O
geometry	O
pattern	O
more	O
evident	O
,	O
leading	O
to	O
an	O
easier	O
job	O
of	O
3D	Task
learners	Task
.	O

Second	O
,	O
learning	Task
in	Task
3D	Task
space	Task
can	O
better	O
exploits	O
the	O
geometric	O
and	O
topological	O
structure	O
of	O
3D	O
space	O
.	O

In	O
principle	O
,	O
all	O
objects	O
live	O
in	O
3D	O
space	O
;	O
therefore	O
,	O
we	O
believe	O
that	O
many	O
geometric	O
structures	O
,	O
such	O
as	O
repetition	O
,	O
planarity	O
,	O
and	O
symmetry	O
,	O
are	O
more	O
naturally	O
parameterized	O
and	O
captured	O
by	O
learners	Method
that	O
directly	O
operate	O
in	O
3D	O
space	O
.	O

The	O
usefulness	O
of	O
this	O
3D	Method
-	Method
centric	Method
network	Method
design	Method
philosophy	Method
has	O
been	O
supported	O
by	O
much	O
recent	O
experimental	O
evidence	O
.	O

Our	O
method	O
achieve	O
leading	O
positions	O
on	O
KITTI	Material
3D	O
object	O
detection	Task
and	O
bird	O
’s	O
eye	O
view	O
detection	Task
benchmarks	O
.	O

Compared	O
with	O
the	O
previous	O
state	O
of	O
the	O
art	O
,	O
our	O
method	O
is	O
8.04	O
%	O
better	O
on	O
3D	Metric
car	Metric
AP	Metric
with	O
high	O
efficiency	O
(	O
running	O
at	O
5	O
fps	Metric
)	O
.	O

Our	O
method	O
also	O
fits	O
well	O
to	O
indoor	Material
RGB	Material
-	Material
D	Material
data	Material
where	O
we	O
have	O
achieved	O
8.9	O
%	O
and	O
6.4	O
%	O
better	O
3D	Metric
mAP	Metric
than	O
and	O
on	O
SUN	Material
-	Material
RGBD	Material
while	O
running	O
one	O
to	O
three	O
orders	O
of	O
magnitude	O
faster	O
.	O

The	O
key	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
framework	O
for	O
RGB	O
-	O
D	O
data	O
based	O
3D	O
object	O
detection	Task
called	O
Frustum	Method
PointNets	Method
.	O

We	O
show	O
how	O
we	O
can	O
train	O
3D	Method
object	Method
detectors	Method
under	O
our	O
framework	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
standard	O
3D	O
object	O
detection	Task
benchmarks	O
.	O

We	O
provide	O
extensive	O
quantitative	O
evaluations	O
to	O
validate	O
our	O
design	O
choices	O
as	O
well	O
as	O
rich	O
qualitative	O
results	O
for	O
understanding	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
method	O
.	O

section	O
:	O
Related	O
Work	O
paragraph	O
:	O
3D	Task
Object	Task
Detection	Task
from	O
RGB	Material
-	Material
D	Material
Data	Material
Researchers	O
have	O
approached	O
the	O
3D	Task
detection	Task
problem	O
by	O
taking	O
various	O
ways	O
to	O
represent	O
RGB	Material
-	Material
D	Material
data	Material
.	O

Front	Method
view	Method
image	Method
based	Method
methods	Method
:	O
take	O
monocular	Material
RGB	Material
images	Material
and	O
shape	O
priors	O
or	O
occlusion	O
patterns	O
to	O
infer	O
3D	O
bounding	O
boxes	O
.	O

represent	O
depth	O
data	O
as	O
2D	O
maps	O
and	O
apply	O
CNNs	Method
to	O
localize	Task
objects	Task
in	O
2D	O
image	O
.	O

In	O
comparison	O
we	O
represent	O
depth	O
as	O
a	O
point	O
cloud	O
and	O
use	O
advanced	O
3D	Method
deep	Method
networks	Method
(	O
PointNets	Method
)	O
that	O
can	O
exploit	O
3D	O
geometry	O
more	O
effectively	O
.	O

Bird	Method
’s	Method
eye	Method
view	Method
based	Method
methods	Method
:	O
MV3D	Method
projects	O
LiDAR	Method
point	Method
cloud	Method
to	O
bird	O
’s	O
eye	O
view	O
and	O
trains	O
a	O
region	Method
proposal	Method
network	Method
(	O
RPN	Method
)	O
for	O
3D	Task
bounding	Task
box	Task
proposal	Task
.	O

However	O
,	O
the	O
method	O
lags	O
behind	O
in	O
detecting	Task
small	Task
objects	Task
,	O
such	O
as	O
pedestrians	O
and	O
cyclists	O
and	O
can	O
not	O
easily	O
adapt	O
to	O
scenes	O
with	O
multiple	O
objects	O
in	O
vertical	O
direction	O
.	O

3D	Method
based	Method
methods	Method
:	O
train	O
3D	Method
object	Method
classifiers	Method
by	O
SVMs	Method
on	O
hand	O
-	O
designed	O
geometry	O
features	O
extracted	O
from	O
point	O
cloud	O
and	O
then	O
localize	O
objects	O
using	O
sliding	Method
-	Method
window	Method
search	Method
.	O

extends	O
by	O
replacing	O
SVM	Method
with	O
3D	Method
CNN	Method
on	O
voxelized	O
3D	O
grids	O
.	O

designs	O
new	O
geometric	O
features	O
for	O
3D	O
object	O
detection	Task
in	O
a	O
point	O
cloud	O
.	O

convert	O
a	O
point	O
cloud	O
of	O
the	O
entire	O
scene	O
into	O
a	O
volumetric	O
grid	O
and	O
use	O
3D	Method
volumetric	Method
CNN	Method
for	O
object	Task
proposal	Task
and	Task
classification	Task
.	O

Computation	Metric
cost	Metric
for	O
those	O
method	O
is	O
usually	O
quite	O
high	O
due	O
to	O
the	O
expensive	O
cost	O
of	O
3D	O
convolutions	O
and	O
large	O
3D	O
search	O
space	O
.	O

Recently	O
,	O
proposes	O
a	O
2D	O
-	O
driven	O
3D	O
object	O
detection	Task
method	O
that	O
is	O
similar	O
to	O
ours	O
in	O
spirit	O
.	O

However	O
,	O
they	O
use	O
hand	O
-	O
crafted	O
features	O
(	O
based	O
on	O
histogram	Method
of	Method
point	Method
coordinates	Method
)	O
with	O
simple	O
fully	Method
connected	Method
networks	Method
to	O
regress	O
3D	Task
box	Task
location	Task
and	O
pose	O
,	O
which	O
is	O
sub	O
-	O
optimal	O
in	O
both	O
speed	Metric
and	O
performance	O
.	O

In	O
contrast	O
,	O
we	O
propose	O
a	O
more	O
flexible	O
and	O
effective	O
solution	O
with	O
deep	Method
3D	Method
feature	Method
learning	Method
(	O
PointNets	Method
)	O
.	O

paragraph	O
:	O
Deep	Task
Learning	Task
on	O
Point	Task
Clouds	Task
Most	O
existing	O
works	O
convert	O
point	O
clouds	O
to	O
images	O
or	O
volumetric	O
forms	O
before	O
feature	Method
learning	Method
.	O

voxelize	Task
point	Task
clouds	Task
into	O
volumetric	O
grids	O
and	O
generalize	O
image	Method
CNNs	Method
to	O
3D	Method
CNNs	Method
.	O

design	O
more	O
efficient	O
3D	Method
CNN	Method
or	Method
neural	Method
network	Method
architectures	Method
that	O
exploit	O
sparsity	O
in	O
point	O
cloud	O
.	O

However	O
,	O
these	O
CNN	Method
based	Method
methods	Method
still	O
require	O
quantitization	Task
of	Task
point	Task
clouds	Task
with	O
certain	O
voxel	O
resolution	O
.	O

Recently	O
,	O
a	O
few	O
works	O
propose	O
a	O
novel	O
type	O
of	O
network	Method
architectures	Method
(	O
PointNets	Method
)	O
that	O
directly	O
consumes	O
raw	O
point	O
clouds	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

While	O
PointNets	Method
have	O
been	O
applied	O
to	O
single	Task
object	Task
classification	Task
and	O
semantic	Task
segmentation	Task
,	O
our	O
work	O
explores	O
how	O
to	O
extend	O
the	O
architecture	O
for	O
the	O
purpose	O
of	O
3D	O
object	O
detection	Task
.	O

section	O
:	O
Problem	O
Definition	O
Given	O
RGB	Material
-	Material
D	Material
data	Material
as	O
input	O
,	O
our	O
goal	O
is	O
to	O
classify	O
and	O
localize	Task
objects	Task
in	Task
3D	Task
space	Task
.	O

The	O
depth	O
data	O
,	O
obtained	O
from	O
LiDAR	O
or	O
indoor	O
depth	O
sensors	O
,	O
is	O
represented	O
as	O
a	O
point	O
cloud	O
in	O
RGB	O
camera	O
coordinates	O
.	O

The	O
projection	O
matrix	O
is	O
also	O
known	O
so	O
that	O
we	O
can	O
get	O
a	O
3D	O
frustum	O
from	O
a	O
2D	O
image	O
region	O
.	O

Each	O
object	O
is	O
represented	O
by	O
a	O
class	O
(	O
one	O
among	O
predefined	O
classes	O
)	O
and	O
an	O
amodal	O
3D	O
bounding	O
box	O
.	O

The	O
amodal	O
box	O
bounds	O
the	O
complete	O
object	O
even	O
if	O
part	O
of	O
the	O
object	O
is	O
occluded	O
or	O
truncated	O
.	O

The	O
3D	O
box	O
is	O
parameterized	O
by	O
its	O
size	O
,	O
center	O
,	O
and	O
orientation	O
relative	O
to	O
a	O
predefined	O
canonical	O
pose	O
for	O
each	O
category	O
.	O

In	O
our	O
implementation	O
,	O
we	O
only	O
consider	O
the	O
heading	O
angle	O
around	O
the	O
up	O
-	O
axis	O
for	O
orientation	O
.	O

section	O
:	O
3D	Task
Detection	Task
with	O
Frustum	Method
PointNets	Method
As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
system	O
for	O
3D	O
object	O
detection	Task
consists	O
of	O
three	O
modules	O
:	O
frustum	Method
proposal	Method
,	O
3D	Task
instance	Task
segmentation	Task
,	O
and	O
3D	Method
amodal	Method
bounding	Method
box	Method
estimation	Method
.	O

We	O
will	O
introduce	O
each	O
module	O
in	O
the	O
following	O
subsections	O
.	O

We	O
will	O
focus	O
on	O
the	O
pipeline	O
and	O
functionality	O
of	O
each	O
module	O
,	O
and	O
refer	O
readers	O
to	O
supplementary	O
for	O
specific	O
architectures	O
of	O
the	O
deep	Method
networks	Method
involved	O
.	O

subsection	O
:	O
Frustum	Method
Proposal	Method
The	O
resolution	O
of	O
data	O
produced	O
by	O
most	O
3D	Method
sensors	Method
,	O
especially	O
real	Method
-	Method
time	Method
depth	Method
sensors	Method
,	O
is	O
still	O
lower	O
than	O
RGB	Material
images	Material
from	O
commodity	O
cameras	O
.	O

Therefore	O
,	O
we	O
leverage	O
mature	O
2D	Method
object	Method
detector	Method
to	O
propose	O
2D	O
object	O
regions	O
in	O
RGB	Material
images	Material
as	O
well	O
as	O
to	O
classify	O
objects	O
.	O

With	O
a	O
known	O
camera	O
projection	O
matrix	O
,	O
a	O
2D	O
bounding	O
box	O
can	O
be	O
lifted	O
to	O
a	O
frustum	O
(	O
with	O
near	O
and	O
far	O
planes	O
specified	O
by	O
depth	O
sensor	O
range	O
)	O
that	O
defines	O
a	O
3D	O
search	O
space	O
for	O
the	O
object	O
.	O

We	O
then	O
collect	O
all	O
points	O
within	O
the	O
frustum	O
to	O
form	O
a	O
frustum	O
point	O
cloud	O
.	O

As	O
shown	O
in	O
Fig	O
[	O
reference	O
]	O
(	O
a	O
)	O
,	O
frustums	O
may	O
orient	O
towards	O
many	O
different	O
directions	O
,	O
which	O
result	O
in	O
large	O
variation	O
in	O
the	O
placement	Task
of	Task
point	Task
clouds	Task
.	O

We	O
therefore	O
normalize	O
the	O
frustums	O
by	O
rotating	O
them	O
toward	O
a	O
center	O
view	O
such	O
that	O
the	O
center	O
axis	O
of	O
the	O
frustum	O
is	O
orthogonal	O
to	O
the	O
image	O
plane	O
.	O

This	O
normalization	O
helps	O
improve	O
the	O
rotation	O
-	O
invariance	O
of	O
the	O
algorithm	O
.	O

We	O
call	O
this	O
entire	O
procedure	O
for	O
extracting	Task
frustum	Task
point	Task
clouds	Task
from	O
RGB	Task
-	Task
D	Task
data	Task
frustum	Task
proposal	Task
generation	Task
.	O

While	O
our	O
3D	Task
detection	Task
framework	O
is	O
agnostic	O
to	O
the	O
exact	O
method	O
for	O
2D	Task
region	Task
proposal	Task
,	O
we	O
adopt	O
a	O
FPN	Method
based	Method
model	Method
.	O

We	O
pre	O
-	O
train	O
the	O
model	O
weights	O
on	O
ImageNet	Task
classification	Task
and	O
COCO	O
object	O
detection	Task
datasets	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
a	O
KITTI	Material
2D	O
object	O
detection	Task
dataset	O
to	O
classify	O
and	O
predict	O
amodal	Task
2D	Task
boxes	Task
.	O

More	O
details	O
of	O
the	O
2D	Method
detector	Method
training	Method
are	O
provided	O
in	O
the	O
supplementary	O
.	O

subsection	O
:	O
3D	Task
Instance	Task
Segmentation	Task
Given	O
a	O
2D	O
image	O
region	O
(	O
and	O
its	O
corresponding	O
3D	O
frustum	O
)	O
,	O
several	O
methods	O
might	O
be	O
used	O
to	O
obtain	O
3D	O
location	O
of	O
the	O
object	O
:	O
One	O
straightforward	O
solution	O
is	O
to	O
directly	O
regress	O
3D	O
object	O
locations	O
(	O
e.g.	O
,	O
by	O
3D	O
bounding	O
box	O
)	O
from	O
a	O
depth	Method
map	Method
using	O
2D	Method
CNNs	Method
.	O

However	O
,	O
this	O
problem	O
is	O
not	O
easy	O
as	O
occluding	O
objects	O
and	O
background	O
clutter	O
is	O
common	O
in	O
natural	O
scenes	O
(	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
which	O
may	O
severely	O
distract	O
the	O
3D	Task
localization	Task
task	Task
.	O

Because	O
objects	O
are	O
naturally	O
separated	O
in	O
physical	O
space	O
,	O
segmentation	Task
in	O
3D	Task
point	Task
cloud	Task
is	O
much	O
more	O
natural	O
and	O
easier	O
than	O
that	O
in	O
images	O
where	O
pixels	O
from	O
distant	O
objects	O
can	O
be	O
near	O
-	O
by	O
to	O
each	O
other	O
.	O

Having	O
observed	O
this	O
fact	O
,	O
we	O
propose	O
to	O
segment	O
instances	O
in	O
3D	Task
point	Task
cloud	Task
instead	O
of	O
in	O
2D	Task
image	Task
or	Task
depth	Task
map	Task
.	O

Similar	O
to	O
Mask	Method
-	Method
RCNN	Method
,	O
which	O
achieves	O
instance	Task
segmentation	Task
by	O
binary	Task
classification	Task
of	Task
pixels	Task
in	Task
image	Task
regions	Task
,	O
we	O
realize	O
3D	Method
instance	Method
segmentation	Method
using	O
a	O
PointNet	Method
-	Method
based	Method
network	Method
on	O
point	O
clouds	O
in	O
frustums	O
.	O

Based	O
on	O
3D	Task
instance	Task
segmentation	Task
,	O
we	O
are	O
able	O
to	O
achieve	O
residual	Task
based	Task
3D	Task
localization	Task
.	O

That	O
is	O
,	O
rather	O
than	O
regressing	O
the	O
absolute	O
3D	O
location	O
of	O
the	O
object	O
whose	O
offset	O
from	O
the	O
sensor	O
may	O
vary	O
in	O
large	O
ranges	O
(	O
e.g.	O
from	O
5	O
m	O
to	O
beyond	O
50	O
m	O
in	O
KITTI	Material
data	O
)	O
,	O
we	O
predict	O
the	O
3D	O
bounding	O
box	O
center	O
in	O
a	O
local	Method
coordinate	Method
system	Method
–	O
3D	O
mask	O
coordinates	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

paragraph	O
:	O
3D	Task
Instance	Task
Segmentation	Task
PointNet	Task
.	O

The	O
network	O
takes	O
a	O
point	O
cloud	O
in	O
frustum	Method
and	O
predicts	O
a	O
probability	O
score	O
for	O
each	O
point	O
that	O
indicates	O
how	O
likely	O
the	O
point	O
belongs	O
to	O
the	O
object	O
of	O
interest	O
.	O

Note	O
that	O
each	O
frustum	O
contains	O
exactly	O
one	O
object	O
of	O
interest	O
.	O

Here	O
those	O
“	O
other	O
”	O
points	O
could	O
be	O
points	O
of	O
non	O
-	O
relevant	O
areas	O
(	O
such	O
as	O
ground	O
,	O
vegetation	O
)	O
or	O
other	O
instances	O
that	O
occlude	O
or	O
are	O
behind	O
the	O
object	O
of	O
interest	O
.	O

Similar	O
to	O
the	O
case	O
in	O
2D	Task
instance	Task
segmentation	Task
,	O
depending	O
on	O
the	O
position	O
of	O
the	O
frustum	O
,	O
object	O
points	O
in	O
one	O
frustum	O
may	O
become	O
cluttered	O
or	O
occlude	O
points	O
in	O
another	O
.	O

Therefore	O
,	O
our	O
segmentation	Task
PointNet	Task
is	O
learning	O
the	O
occlusion	O
and	O
clutter	O
patterns	O
as	O
well	O
as	O
recognizing	O
the	O
geometry	O
for	O
the	O
object	O
of	O
a	O
certain	O
category	O
.	O

In	O
a	O
multi	O
-	O
class	O
detection	Task
case	O
,	O
we	O
also	O
leverage	O
the	O
semantics	O
from	O
a	O
2D	Method
detector	Method
for	O
better	O
instance	Task
segmentation	Task
.	O

For	O
example	O
,	O
if	O
we	O
know	O
the	O
object	O
of	O
interest	O
is	O
a	O
pedestrian	O
,	O
then	O
the	O
segmentation	Method
network	Method
can	O
use	O
this	O
prior	O
to	O
find	O
geometries	O
that	O
look	O
like	O
a	O
person	O
.	O

Specifically	O
,	O
in	O
our	O
architecture	O
we	O
encode	O
the	O
semantic	O
category	O
as	O
a	O
one	O
-	O
hot	O
class	O
vector	O
(	O
dimensional	O
for	O
the	O
pre	O
-	O
defined	O
categories	O
)	O
and	O
concatenate	O
the	O
one	O
-	O
hot	O
vector	O
to	O
the	O
intermediate	O
point	O
cloud	O
features	O
.	O

More	O
details	O
of	O
the	O
specific	O
architectures	O
are	O
described	O
in	O
the	O
supplementary	O
.	O

After	O
3D	Task
instance	Task
segmentation	Task
,	O
points	O
that	O
are	O
classified	O
as	O
the	O
object	O
of	O
interest	O
are	O
extracted	O
(	O
“	O
masking	O
”	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Having	O
obtained	O
these	O
segmented	O
object	O
points	O
,	O
we	O
further	O
normalize	O
its	O
coordinates	O
to	O
boost	O
the	O
translational	O
invariance	O
of	O
the	O
algorithm	O
,	O
following	O
the	O
same	O
rationale	O
as	O
in	O
the	O
frustum	Method
proposal	Method
step	Method
.	O

In	O
our	O
implementation	O
,	O
we	O
transform	O
the	O
point	O
cloud	O
into	O
a	O
local	O
coordinate	O
by	O
subtracting	O
XYZ	O
values	O
by	O
its	O
centroid	O
.	O

This	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

Note	O
that	O
we	O
intentionally	O
do	O
not	O
scale	O
the	O
point	O
cloud	O
,	O
because	O
the	O
bounding	O
sphere	O
size	O
of	O
a	O
partial	O
point	O
cloud	O
can	O
be	O
greatly	O
affected	O
by	O
viewpoints	O
and	O
the	O
real	O
size	O
of	O
the	O
point	O
cloud	O
helps	O
the	O
box	Method
size	Method
estimation	Method
.	O

In	O
our	O
experiments	O
,	O
we	O
find	O
that	O
coordinate	O
transformations	O
such	O
as	O
the	O
one	O
above	O
and	O
the	O
previous	O
frustum	O
rotation	O
are	O
critical	O
for	O
3D	Task
detection	Task
result	O
as	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Amodal	Task
3D	Task
Box	Task
Estimation	Task
Given	O
the	O
segmented	O
object	O
points	O
(	O
in	O
3D	O
mask	O
coordinate	O
)	O
,	O
this	O
module	O
estimates	O
the	O
object	O
’s	O
amodal	O
oriented	O
3D	O
bounding	O
box	O
by	O
using	O
a	O
box	Method
regression	Method
PointNet	Method
together	O
with	O
a	O
preprocessing	Method
transformer	Method
network	Method
.	O

paragraph	O
:	O
Learning	Task
-	Task
based	Task
3D	Task
Alignment	Task
by	O
T	Method
-	Method
Net	Method
Even	O
though	O
we	O
have	O
aligned	O
segmented	O
object	O
points	O
according	O
to	O
their	O
centroid	O
position	O
,	O
we	O
find	O
that	O
the	O
origin	O
of	O
the	O
mask	O
coordinate	O
frame	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
)	O
may	O
still	O
be	O
quite	O
far	O
from	O
the	O
amodal	O
box	O
center	O
.	O

We	O
therefore	O
propose	O
to	O
use	O
a	O
light	Method
-	Method
weight	Method
regression	Method
PointNet	Method
(	O
T	Method
-	Method
Net	Method
)	O
to	O
estimate	O
the	O
true	O
center	O
of	O
the	O
complete	O
object	O
and	O
then	O
transform	O
the	O
coordinate	O
such	O
that	O
the	O
predicted	O
center	O
becomes	O
the	O
origin	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
architecture	O
and	O
training	O
of	O
our	O
T	Method
-	Method
Net	Method
is	O
similar	O
to	O
the	O
T	Method
-	Method
Net	Method
in	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
type	O
of	O
spatial	Method
transformer	Method
network	Method
(	O
STN	Method
)	O
.	O

However	O
,	O
different	O
from	O
the	O
original	O
STN	Method
that	O
has	O
no	O
direct	O
supervision	O
on	O
transformation	Task
,	O
we	O
explicitly	O
supervise	O
our	O
translation	Method
network	Method
to	O
predict	O
center	O
residuals	O
from	O
the	O
mask	O
coordinate	O
origin	O
to	O
real	O
object	O
center	O
.	O

paragraph	O
:	O
Amodal	O
3D	Task
Box	Task
Estimation	Task
PointNet	Task
The	O
box	Method
estimation	Method
network	Method
predicts	O
amodal	O
bounding	O
boxes	O
(	O
for	O
entire	O
object	O
even	O
if	O
part	O
of	O
it	O
is	O
unseen	O
)	O
for	O
objects	O
given	O
an	O
object	O
point	O
cloud	O
in	O
3D	O
object	O
coordinate	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
network	Method
architecture	Method
is	O
similar	O
to	O
that	O
for	O
object	Task
classification	Task
,	O
however	O
the	O
output	O
is	O
no	O
longer	O
object	O
class	O
scores	O
but	O
parameters	O
for	O
a	O
3D	O
bounding	O
box	O
.	O

As	O
stated	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
we	O
parameterize	O
a	O
3D	O
bounding	O
box	O
by	O
its	O
center	O
(	O
,	O
,	O
)	O
,	O
size	O
(	O
,	O
,	O
)	O
and	O
heading	O
angle	O
(	O
along	O
up	O
-	O
axis	O
)	O
.	O

We	O
take	O
a	O
“	O
residual	Method
”	Method
approach	Method
for	O
box	Task
center	Task
estimation	Task
.	O

The	O
center	O
residual	O
predicted	O
by	O
the	O
box	Method
estimation	Method
network	Method
is	O
combined	O
with	O
the	O
previous	O
center	O
residual	O
from	O
the	O
T	Method
-	Method
Net	Method
and	O
the	O
masked	O
points	O
’	O
centroid	O
to	O
recover	O
an	O
absolute	O
center	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

For	O
box	O
size	O
and	O
heading	O
angle	O
,	O
we	O
follow	O
previous	O
works	O
and	O
use	O
a	O
hybrid	O
of	O
classification	Method
and	Method
regression	Method
formulations	Method
.	O

Specifically	O
we	O
pre	O
-	O
define	O
size	O
templates	O
and	O
equally	O
split	O
angle	O
bins	O
.	O

Our	O
model	O
will	O
both	O
classify	O
size	O
/	O
heading	O
(	O
scores	O
for	O
size	O
,	O
scores	O
for	O
heading	Task
)	O
to	O
those	O
pre	O
-	O
defined	O
categories	O
as	O
well	O
as	O
predict	O
residual	O
numbers	O
for	O
each	O
category	O
(	O
residual	O
dimensions	O
for	O
height	O
,	O
width	O
,	O
length	O
,	O
residual	O
angles	O
for	O
heading	O
)	O
.	O

In	O
the	O
end	O
the	O
net	O
outputs	O
numbers	O
in	O
total	O
.	O

subsection	O
:	O
Training	O
with	O
Multi	Task
-	Task
task	Task
Losses	Task
We	O
simultaneously	O
optimize	O
the	O
three	O
nets	O
involved	O
(	O
3D	Task
instance	Task
segmentation	Task
PointNet	Task
,	O
T	Method
-	Method
Net	Method
and	O
amodal	Method
box	Method
estimation	Method
PointNet	Method
)	O
with	O
multi	O
-	O
task	O
losses	O
(	O
as	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

is	O
for	O
T	Method
-	Method
Net	Method
and	O
is	O
for	O
center	Method
regression	Method
of	Method
box	Method
estimation	Method
net	Method
.	O

and	O
are	O
losses	O
for	O
heading	Task
angle	Task
prediction	Task
while	O
and	O
are	O
for	O
box	O
size	O
.	O

Softmax	Method
is	O
used	O
for	O
all	O
classification	Task
tasks	Task
and	O
smooth	Method
-	Method
(	Method
huber	Method
)	Method
loss	Method
is	O
used	O
for	O
all	O
regression	Task
cases	Task
.	O

paragraph	O
:	O
Corner	O
Loss	O
for	O
Joint	Task
Optimization	Task
of	Task
Box	Task
Parameters	Task
While	O
our	O
3D	Method
bounding	Method
box	Method
parameterization	Method
is	O
compact	O
and	O
complete	O
,	O
learning	Method
is	O
not	O
optimized	O
for	O
final	O
3D	Metric
box	Metric
accuracy	Metric
–	O
center	O
,	O
size	O
and	O
heading	O
have	O
separate	O
loss	O
terms	O
.	O

Imagine	O
cases	O
where	O
center	O
and	O
size	O
are	O
accurately	O
predicted	O
but	O
heading	O
angle	O
is	O
off	O
–	O
the	O
3D	O
IoU	O
with	O
ground	O
truth	O
box	O
will	O
then	O
be	O
dominated	O
by	O
the	O
angle	Metric
error	Metric
.	O

Ideally	O
all	O
three	O
terms	O
(	O
center	O
,	O
size	O
,	O
heading	O
)	O
should	O
be	O
jointly	O
optimized	O
for	O
best	O
3D	Task
box	Task
estimation	Task
(	O
under	O
IoU	Metric
metric	Metric
)	O
.	O

To	O
resolve	O
this	O
problem	O
we	O
propose	O
a	O
novel	O
regularization	Method
loss	Method
,	O
the	O
corner	Method
loss	Method
:	O
In	O
essence	O
,	O
the	O
corner	Metric
loss	Metric
is	O
the	O
sum	O
of	O
the	O
distances	O
between	O
the	O
eight	O
corners	O
of	O
a	O
predicted	O
box	O
and	O
a	O
ground	O
truth	O
box	O
.	O

Since	O
corner	O
positions	O
are	O
jointly	O
determined	O
by	O
center	O
,	O
size	O
and	O
heading	O
,	O
the	O
corner	Method
loss	Method
is	O
able	O
to	O
regularize	O
the	O
multi	Task
-	Task
task	Task
training	Task
for	O
those	O
parameters	O
.	O

To	O
compute	O
the	O
corner	O
loss	O
,	O
we	O
firstly	O
construct	O
“	O
anchor	O
”	O
boxes	O
from	O
all	O
size	O
templates	O
and	O
heading	O
angle	O
bins	O
.	O

The	O
anchor	O
boxes	O
are	O
then	O
translated	O
to	O
the	O
estimated	O
box	O
center	O
.	O

We	O
denote	O
the	O
anchor	O
box	O
corners	O
as	O
,	O
where	O
,	O
,	O
are	O
indices	O
for	O
the	O
size	O
class	O
,	O
heading	O
class	O
,	O
and	O
(	O
predefined	O
)	O
corner	O
order	O
,	O
respectively	O
.	O

To	O
avoid	O
large	O
penalty	O
from	O
flipped	Task
heading	Task
estimation	Task
,	O
we	O
further	O
compute	O
distances	O
to	O
corners	O
(	O
from	O
the	O
flipped	O
ground	O
truth	O
box	O
and	O
use	O
the	O
minimum	O
of	O
the	O
original	O
and	O
flipped	O
cases	O
.	O

,	O
which	O
is	O
one	O
for	O
the	O
ground	O
truth	O
size	O
/	O
heading	O
class	O
and	O
zero	O
else	O
wise	O
,	O
is	O
a	O
two	O
-	O
dimensional	O
mask	O
used	O
to	O
select	O
the	O
distance	O
term	O
we	O
care	O
about	O
.	O

section	O
:	O
Experiments	O
Experiments	O
are	O
divided	O
into	O
three	O
parts	O
.	O

First	O
we	O
compare	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
3D	O
object	O
detection	Task
on	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
we	O
provide	O
in	O
-	O
depth	O
analysis	O
to	O
validate	O
our	O
design	O
choices	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Last	O
,	O
we	O
show	O
qualitative	O
results	O
and	O
discuss	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
methods	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Comparing	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Methods	O
We	O
evaluate	O
our	O
3D	Method
object	Method
detector	Method
on	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
benchmarks	O
for	O
3D	O
object	O
detection	Task
.	O

On	O
both	O
tasks	O
we	O
have	O
achieved	O
significantly	O
better	O
results	O
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

paragraph	O
:	O
KITTI	Material
Tab	O
.	O

[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
our	O
3D	Method
detector	Method
on	O
the	O
KITTI	Material
test	O
set	O
.	O

We	O
outperform	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
by	O
a	O
large	O
margin	O
.	O

While	O
MV3D	Method
uses	O
multi	Method
-	Method
view	Method
feature	Method
aggregation	Method
and	O
sophisticated	O
multi	Method
-	Method
sensor	Method
fusion	Method
strategy	Method
,	O
our	O
method	O
based	O
on	O
the	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	O
++	O
(	O
v2	O
)	O
backbone	O
is	O
much	O
cleaner	O
in	O
design	O
.	O

While	O
out	O
of	O
the	O
scope	O
for	O
this	O
work	O
,	O
we	O
expect	O
that	O
sensor	Task
fusion	Task
(	O
esp	O
.	O

aggregation	Task
of	Task
image	Task
feature	Task
for	O
3D	Task
detection	Task
)	O
could	O
further	O
improve	O
our	O
results	O
.	O

We	O
also	O
show	O
our	O
method	O
’s	O
performance	O
on	O
3D	O
object	Task
localization	Task
(	O
bird	O
’s	O
eye	O
view	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

In	O
the	O
3D	O
localization	Task
task	O
bounding	O
boxes	O
are	O
projected	O
to	O
bird	O
’s	O
eye	O
view	O
plane	O
and	O
IoU	O
is	O
evaluated	O
on	O
oriented	O
2D	O
boxes	O
.	O

Again	O
,	O
our	O
method	O
significantly	O
outperforms	O
previous	O
works	O
which	O
include	O
DoBEM	Method
and	O
MV3D	Method
that	O
use	O
CNNs	Method
on	O
projected	Material
LiDAR	Material
images	Material
,	O
as	O
well	O
as	O
3D	Method
FCN	Method
that	O
uses	O
3D	Method
CNNs	Method
on	O
voxelized	O
point	O
cloud	O
.	O

The	O
output	O
of	O
our	O
network	O
is	O
visualized	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
where	O
we	O
observe	O
accurate	O
3D	Task
instance	Task
segmentation	Task
and	O
box	Task
prediction	Task
even	O
under	O
very	O
challenging	O
cases	O
.	O

We	O
defer	O
more	O
discussions	O
on	O
success	O
and	O
failure	O
case	O
patterns	O
to	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

We	O
also	O
report	O
performance	O
on	O
KITTI	Material
val	O
set	O
(	O
the	O
same	O
split	O
as	O
in	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
and	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
cars	O
)	O
to	O
support	O
comparison	O
with	O
more	O
published	O
works	O
,	O
and	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
pedestrians	O
and	O
cyclists	O
)	O
for	O
reference	O
.	O

paragraph	O
:	O
SUN	Material
-	Material
RGBD	Material
Most	O
previous	O
3D	Task
detection	Task
works	O
specialize	O
either	O
on	O
outdoor	Material
LiDAR	Material
scans	Material
where	O
objects	O
are	O
well	O
separated	O
in	O
space	O
and	O
the	O
point	O
cloud	O
is	O
sparse	O
(	O
so	O
that	O
it	O
’s	O
feasible	O
for	O
bird	Task
’s	Task
eye	Task
projection	Task
)	O
,	O
or	O
on	O
indoor	Task
depth	Task
maps	Task
that	O
are	O
regular	O
images	O
with	O
dense	O
pixel	O
values	O
such	O
that	O
image	Method
CNNs	Method
can	O
be	O
easily	O
applied	O
.	O

However	O
,	O
methods	O
designed	O
for	O
bird	Task
’s	Task
eye	Task
view	Task
may	O
be	O
incapable	O
for	O
indoor	O
rooms	O
where	O
multiple	O
objects	O
often	O
exist	O
together	O
in	O
vertical	O
space	O
.	O

On	O
the	O
other	O
hand	O
,	O
indoor	Method
focused	Method
methods	Method
could	O
find	O
it	O
hard	O
to	O
apply	O
to	O
sparse	Task
and	Task
large	Task
-	Task
scale	Task
point	Task
cloud	Task
from	O
LiDAR	Material
scans	Material
.	O

In	O
contrast	O
,	O
our	O
frustum	Method
-	Method
based	Method
PointNet	Method
is	O
a	O
generic	O
framework	O
for	O
both	O
outdoor	O
and	O
indoor	O
3D	O
object	O
detection	Task
.	O

By	O
applying	O
the	O
same	O
pipeline	O
we	O
used	O
for	O
KITTI	Material
data	O
set	O
,	O
we	O
’	O
ve	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
SUN	Material
-	Material
RGBD	Material
benchmark	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
significantly	O
higher	O
mAP	Metric
as	O
well	O
as	O
much	O
faster	O
(	O
10x	O
-	O
1000x	O
)	O
inference	Metric
speed	Metric
.	O

subsection	O
:	O
Architecture	Task
Design	Task
Analysis	Task
In	O
this	O
section	O
we	O
provide	O
analysis	O
and	O
ablation	O
experiments	O
to	O
validate	O
our	O
design	O
choices	O
.	O

paragraph	O
:	O
Experiment	O
setup	O
.	O

Unless	O
otherwise	O
noted	O
,	O
all	O
experiments	O
in	O
this	O
section	O
are	O
based	O
on	O
our	O
v1	Method
model	Method
on	O
KITTI	Material
data	O
using	O
train	O
/	O
val	O
split	O
as	O
in	O
.	O

To	O
decouple	O
the	O
influence	O
of	O
2D	Method
detectors	Method
,	O
we	O
use	O
ground	O
truth	O
2D	O
boxes	O
for	O
region	Task
proposals	Task
and	O
use	O
3D	Metric
box	Metric
estimation	Metric
accuracy	Metric
(	O
IoU	Metric
threshold	Metric
0.7	Metric
)	O
as	O
the	O
evaluation	Metric
metric	Metric
.	O

We	O
will	O
only	O
focus	O
on	O
the	O
car	Task
category	Task
which	O
has	O
the	O
most	O
training	O
examples	O
.	O

paragraph	O
:	O
Comparing	O
with	O
alternative	O
approaches	O
for	O
3D	Task
detection	Task
.	O

In	O
this	O
part	O
we	O
evaluate	O
a	O
few	O
CNN	Method
-	Method
based	Method
baseline	Method
approaches	Method
as	O
well	O
as	O
ablated	Method
versions	Method
and	O
variants	O
of	O
our	O
pipelines	O
using	O
2D	O
masks	O
.	O

In	O
the	O
first	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
show	O
3D	Task
box	Task
estimation	Task
results	O
from	O
two	O
CNN	Method
-	Method
based	Method
networks	Method
.	O

The	O
baseline	O
methods	O
trained	O
VGG	Method
models	Method
on	O
ground	Material
truth	Material
boxes	Material
of	Material
RGB	Material
-	Material
D	Material
images	Material
and	O
adopt	O
the	O
same	O
box	O
parameter	O
and	O
loss	O
functions	O
as	O
our	O
main	O
method	O
.	O

While	O
the	O
model	O
in	O
the	O
first	O
row	O
directly	O
estimates	O
box	O
location	O
and	O
parameters	O
from	O
vanilla	Material
RGB	Material
-	Material
D	Material
image	Material
patch	Material
,	O
the	O
other	O
one	O
(	O
second	O
row	O
)	O
uses	O
a	O
FCN	Method
trained	O
from	O
the	O
COCO	Material
dataset	Material
for	O
2D	Task
mask	Task
estimation	Task
(	O
as	O
that	O
in	O
Mask	Method
-	Method
RCNN	Method
)	O
and	O
only	O
uses	O
features	O
from	O
the	O
masked	O
region	O
for	O
prediction	Task
.	O

The	O
depth	O
values	O
are	O
also	O
translated	O
by	O
subtracting	O
the	O
median	O
depth	O
within	O
the	O
2D	O
mask	O
.	O

However	O
,	O
both	O
CNN	Method
baselines	Method
get	O
far	O
worse	O
results	O
compared	O
to	O
our	O
main	O
method	O
.	O

To	O
understand	O
why	O
CNN	Method
baselines	Method
underperform	O
,	O
we	O
visualize	O
a	O
typical	O
2D	Task
mask	Task
prediction	Task
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

While	O
the	O
estimated	O
2D	O
mask	O
appears	O
in	O
high	O
quality	O
on	O
an	O
RGB	Material
image	Material
,	O
there	O
are	O
still	O
lots	O
of	O
clutter	O
and	O
foreground	O
points	O
in	O
the	O
2D	O
mask	O
.	O

In	O
comparison	O
,	O
our	O
3D	Method
instance	Method
segmentation	Method
gets	O
much	O
cleaner	O
result	O
,	O
which	O
greatly	O
eases	O
the	O
next	O
module	O
in	O
finer	Task
localization	Task
and	O
bounding	Task
box	Task
regression	Task
.	O

In	O
the	O
third	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
experiment	O
with	O
an	O
ablated	Method
version	Method
of	O
frustum	Method
PointNet	Method
that	O
has	O
no	O
3D	Method
instance	Method
segmentation	Method
module	Method
.	O

Not	O
surprisingly	O
,	O
the	O
model	O
gets	O
much	O
worse	O
results	O
than	O
our	O
main	O
method	O
,	O
which	O
indicates	O
the	O
critical	O
effect	O
of	O
our	O
3D	Method
instance	Method
segmentation	Method
module	Method
.	O

In	O
the	O
fourth	O
row	O
,	O
instead	O
of	O
3D	Task
segmentation	Task
we	O
use	O
point	O
clouds	O
from	O
2D	O
masked	O
depth	O
maps	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
for	O
3D	Task
box	Task
estimation	Task
.	O

However	O
,	O
since	O
a	O
2D	O
mask	O
is	O
not	O
able	O
to	O
cleanly	O
segment	O
the	O
3D	O
object	O
,	O
the	O
performance	O
is	O
more	O
than	O
12	O
%	O
worse	O
than	O
that	O
with	O
the	O
3D	Method
segmentation	Method
(	O
our	O
main	O
method	O
in	O
the	O
fifth	O
row	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
combined	O
usage	O
of	O
2D	Method
and	Method
3D	Method
masks	Method
–	O
applying	O
3D	Method
segmentation	Method
on	O
point	O
cloud	O
from	O
2D	O
masked	O
depth	O
map	O
–	O
also	O
shows	O
slightly	O
worse	O
results	O
than	O
our	O
main	O
method	O
probably	O
due	O
to	O
the	O
accumulated	O
error	O
from	O
inaccurate	O
2D	Method
mask	Method
predictions	Method
.	O

paragraph	O
:	O
Effects	O
of	O
point	Method
cloud	Method
normalization	Method
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
frustum	O
PointNet	Method
takes	O
a	O
few	O
key	O
coordinate	O
transformations	O
to	O
canonicalize	O
the	O
point	O
cloud	O
for	O
more	O
effective	O
learning	Task
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
how	O
each	O
normalization	Method
step	Method
helps	O
for	O
3D	Task
detection	Task
.	O

We	O
see	O
that	O
both	O
frustum	O
rotation	O
(	O
such	O
that	O
frustum	O
points	O
have	O
more	O
similar	O
XYZ	O
distributions	O
)	O
and	O
mask	Method
centroid	Method
subtraction	Method
(	O
such	O
that	O
object	O
points	O
have	O
smaller	O
and	O
more	O
canonical	O
XYZ	O
)	O
are	O
critical	O
.	O

In	O
addition	O
,	O
extra	O
alignment	O
of	O
object	O
point	O
cloud	O
to	O
object	O
center	O
by	O
T	Method
-	Method
Net	Method
also	O
contributes	O
significantly	O
to	O
the	O
performance	O
.	O

paragraph	O
:	O
Effects	O
of	O
regression	Method
loss	Method
formulation	Method
and	O
corner	Method
loss	Method
.	O

In	O
Tab	O
.	O

[	O
reference	O
]	O
we	O
compare	O
different	O
loss	O
options	O
and	O
show	O
that	O
a	O
combination	O
of	O
“	O
cls	Method
-	Method
reg	Method
”	Method
loss	Method
(	O
the	O
classification	Method
and	Method
residual	Method
regression	Method
approach	Method
for	O
heading	Task
and	Task
size	Task
regression	Task
)	O
and	O
a	O
regularizing	Method
corner	Method
loss	Method
achieves	O
the	O
best	O
result	O
.	O

The	O
naive	O
baseline	O
using	O
regression	Method
loss	Method
only	O
(	O
first	O
row	O
)	O
achieves	O
unsatisfactory	O
result	O
because	O
the	O
regression	O
target	O
is	O
large	O
in	O
range	O
(	O
object	O
size	O
from	O
0.2	O
m	O
to	O
5	O
m	O
)	O
.	O

In	O
comparison	O
,	O
the	O
cls	Method
-	Method
reg	Method
loss	Method
and	O
a	O
normalized	Method
version	Method
(	O
residual	O
normalized	O
by	O
heading	O
bin	O
size	O
or	O
template	O
shape	O
size	O
)	O
of	O
it	O
achieve	O
much	O
better	O
performance	O
.	O

At	O
last	O
row	O
we	O
show	O
that	O
a	O
regularizing	O
corner	O
loss	O
further	O
helps	O
optimization	Task
.	O

subsection	O
:	O
Qualitative	O
Results	O
and	O
Discussion	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
representative	O
outputs	O
of	O
our	O
frustum	Method
PointNet	Method
model	Method
.	O

We	O
see	O
that	O
for	O
simple	O
cases	O
of	O
non	O
-	O
occluded	O
objects	O
in	O
reasonable	O
distance	O
(	O
so	O
we	O
get	O
enough	O
number	O
of	O
points	O
)	O
,	O
our	O
model	O
outputs	O
remarkably	O
accurate	O
3D	O
instance	O
segmentation	O
mask	O
and	O
3D	O
bounding	O
boxes	O
.	O

Second	O
,	O
we	O
are	O
surprised	O
to	O
find	O
that	O
our	O
model	O
can	O
even	O
predict	O
correctly	O
posed	O
amodal	O
3D	O
box	O
from	O
partial	O
data	O
(	O
e.g.	O
parallel	O
parked	O
cars	O
)	O
with	O
few	O
points	O
.	O

Even	O
humans	O
find	O
it	O
very	O
difficult	O
to	O
annotate	O
such	O
results	O
with	O
point	O
cloud	O
data	O
only	O
.	O

Third	O
,	O
in	O
some	O
cases	O
that	O
seem	O
very	O
challenging	O
in	O
images	O
with	O
lots	O
of	O
nearby	O
or	O
even	O
overlapping	O
2D	O
boxes	O
,	O
when	O
converted	O
to	O
3D	O
space	O
,	O
the	O
localization	Task
becomes	O
much	O
easier	O
(	O
e.g.	O
P11	O
in	O
second	O
row	O
third	O
column	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
do	O
observe	O
several	O
failure	O
patterns	O
,	O
which	O
indicate	O
possible	O
directions	O
for	O
future	O
efforts	O
.	O

The	O
first	O
common	O
mistake	O
is	O
due	O
to	O
inaccurate	Task
pose	Task
and	Task
size	Task
estimation	Task
in	O
a	O
sparse	O
point	O
cloud	O
(	O
sometimes	O
less	O
than	O
5	O
points	O
)	O
.	O

We	O
think	O
image	O
features	O
could	O
greatly	O
help	O
esp	Task
.	O

since	O
we	O
have	O
access	O
to	O
high	O
resolution	O
image	O
patch	O
even	O
for	O
far	O
-	O
away	O
objects	O
.	O

The	O
second	O
type	O
of	O
challenge	O
is	O
when	O
there	O
are	O
multiple	O
instances	O
from	O
the	O
same	O
category	O
in	O
a	O
frustum	O
(	O
like	O
two	O
persons	O
standing	O
by	O
)	O
.	O

Since	O
our	O
current	O
pipeline	O
assumes	O
a	O
single	O
object	O
of	O
interest	O
in	O
each	O
frustum	O
,	O
it	O
may	O
get	O
confused	O
when	O
multiple	O
instances	O
appear	O
and	O
thus	O
outputs	O
mixed	O
segmentation	O
results	O
.	O

This	O
problem	O
could	O
potentially	O
be	O
mitigated	O
if	O
we	O
are	O
able	O
to	O
propose	O
multiple	O
3D	O
bounding	O
boxes	O
within	O
each	O
frustum	O
.	O

Thirdly	O
,	O
sometimes	O
our	O
2D	Method
detector	Method
misses	O
objects	O
due	O
to	O
dark	O
lighting	O
or	O
strong	O
occlusion	O
.	O

Since	O
our	O
frustum	Method
proposals	Method
are	O
based	O
on	O
region	Method
proposal	Method
,	O
no	O
3D	O
object	O
will	O
be	O
detected	O
given	O
no	O
2D	Task
detection	Task
.	O

However	O
,	O
our	O
3D	Task
instance	Task
segmentation	Task
and	O
amodal	Task
3D	Task
box	Task
estimation	Task
PointNets	Task
are	O
not	O
restricted	O
to	O
RGB	O
view	O
proposals	O
.	O

As	O
shown	O
in	O
the	O
supplementary	O
,	O
the	O
same	O
framework	O
can	O
also	O
be	O
extended	O
to	O
3D	O
regions	O
proposed	O
in	O
bird	O
’s	O
eye	O
view	O
.	O

paragraph	O
:	O
Acknowledgement	O
The	O
authors	O
wish	O
to	O
thank	O
the	O
support	O
of	O
Nuro	O
Inc.	O
,	O
ONR	O
MURI	O
grant	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0341	O
,	O
NSF	O
grants	O
DMS	O
-	O
1546206	O
and	O
IIS	O
-	O
1528025	O
,	O
a	O
Samsung	O
GRO	O
award	O
,	O
and	O
gifts	O
from	O
Adobe	O
,	O
Amazon	O
,	O
and	O
Apple	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Overview	O
This	O
document	O
provides	O
additional	O
technical	O
details	O
,	O
extra	O
analysis	O
experiments	O
,	O
more	O
quantitative	O
results	O
and	O
qualitative	O
test	O
results	O
to	O
the	O
main	O
paper	O
.	O

In	O
Sec.ï½	O
[	O
reference	O
]	O
we	O
provide	O
more	O
details	O
on	O
network	Method
architectures	Method
of	O
PointNets	O
and	O
training	O
parameters	O
while	O
Sec	O
.	O

[	O
reference	O
]	O
explains	O
more	O
about	O
our	O
2D	Method
detector	Method
.	O

Sec	O
.	O

[	O
reference	O
]	O
shows	O
how	O
our	O
framework	O
can	O
be	O
extended	O
to	O
bird	O
’s	O
eye	O
view	O
(	O
BV	O
)	O
proposals	O
and	O
how	O
combining	O
BV	Method
and	Method
RGB	Method
proposals	Method
can	O
further	O
improve	O
detection	Task
performance	O
.	O

Then	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
results	O
from	O
more	O
analysis	O
experiments	O
.	O

At	O
last	O
,	O
Sec	O
.	O

[	O
reference	O
]	O
shows	O
more	O
visualization	O
results	O
for	O
3D	Task
detection	Task
on	O
SUN	Material
-	Material
RGBD	Material
dataset	O
.	O

appendix	O
:	O
Details	O
on	O
Frustum	Method
PointNets	Method
(	O
Sec	O
4.2	O
,	O
4.3	O
)	O
subsection	O
:	O
Network	Method
Architectures	Method
We	O
adopt	O
similar	O
network	Method
architectures	Method
as	O
in	O
the	O
original	O
works	O
of	O
PointNet	O
and	O
PointNet	O
++	O
for	O
our	O
v1	Method
and	Method
v2	Method
models	Method
respectively	O
.	O

What	O
is	O
different	O
is	O
that	O
we	O
add	O
an	O
extra	O
link	O
for	O
class	O
one	O
-	O
hot	O
vector	O
such	O
that	O
instance	Task
segmentation	Task
and	O
bounding	Task
box	Task
estimation	Task
can	O
leverage	O
semantics	O
predicted	O
from	O
RGB	Material
images	Material
.	O

The	O
detailed	O
network	Method
architectures	Method
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

For	O
v1	Method
model	Method
our	O
architecture	O
involves	O
point	Method
embedding	Method
layers	Method
(	O
as	O
shared	O
MLP	Method
on	O
each	O
point	O
independently	O
)	O
,	O
a	O
max	Method
pooling	Method
layer	Method
and	O
per	Method
-	Method
point	Method
classification	Method
multi	Method
-	Method
layer	Method
perceptron	Method
(	O
MLP	Method
)	O
based	O
on	O
aggregated	O
information	O
from	O
global	O
feature	O
and	O
each	O
point	O
as	O
well	O
as	O
an	O
one	O
-	O
hot	O
class	O
vector	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
transformer	Method
networks	Method
as	O
in	O
because	O
frustum	O
points	O
are	O
viewpoint	O
based	O
(	O
not	O
complete	O
point	O
cloud	O
as	O
in	O
)	O
and	O
are	O
already	O
normalized	O
by	O
frustum	O
rotation	O
.	O

In	O
addition	O
to	O
XYZ	O
,	O
we	O
also	O
leverage	O
LiDAR	O
intensity	O
as	O
a	O
fourth	O
channel	O
.	O

For	O
v2	Method
model	Method
we	O
use	O
set	Method
abstraction	Method
layers	Method
for	O
hierarchical	Task
feature	Task
learning	Task
in	O
point	Task
clouds	Task
.	O

In	O
addition	O
,	O
because	O
LiDAR	Method
point	Method
cloud	Method
gets	O
increasingly	O
sparse	O
as	O
it	O
gets	O
farther	O
,	O
feature	Method
learning	Method
has	O
to	O
be	O
robust	O
to	O
those	O
density	O
variations	O
.	O

Therefore	O
we	O
used	O
a	O
robust	O
type	O
of	O
set	Method
abstraction	Method
layers	Method
–	O
multi	Method
-	Method
scale	Method
grouping	Method
(	Method
MSG	Method
)	Method
layers	Method
as	O
introduced	O
in	O
for	O
the	O
segmentation	Method
network	Method
.	O

With	O
hierarchical	O
features	O
and	O
learned	O
robustness	O
to	O
varying	O
densities	O
,	O
our	O
v2	Method
model	Method
shows	O
superior	O
performance	O
than	O
v1	Method
model	Method
in	O
both	O
segmentation	Task
and	O
box	Task
estimation	Task
.	O

subsection	O
:	O
Data	Task
Augmentation	Task
and	O
Training	Task
paragraph	O
:	O
Data	Task
augmentation	Task
Data	Task
augmentation	Task
plays	O
an	O
important	O
role	O
in	O
preventing	O
model	Task
overfitting	Task
.	O

Our	O
augmentation	O
involves	O
two	O
branches	O
:	O
one	O
is	O
2D	Method
box	Method
augmentation	Method
and	O
the	O
other	O
is	O
frustum	Method
point	Method
cloud	Method
augmentation	Method
.	O

We	O
use	O
ground	O
truth	O
2D	O
boxes	O
to	O
generate	O
frustum	O
point	O
clouds	O
for	O
Frustum	Method
PointNets	Method
training	O
and	O
augment	O
the	O
2D	O
boxes	O
by	O
random	O
translation	O
and	O
scaling	O
.	O

Specifically	O
,	O
we	O
firstly	O
compute	O
the	O
2D	O
box	O
height	O
(	O
)	O
and	O
width	O
(	O
)	O
and	O
translate	O
the	O
2D	O
box	O
center	O
by	O
random	O
distances	O
sampled	O
from	O
Uniform	O
[	O
]	O
and	O
Uniform	O
[	O
]	O
in	O
u	O
,	O
v	O
directions	O
respectively	O
.	O

The	O
height	O
and	O
width	O
are	O
also	O
augmented	O
by	O
two	O
random	O
scaling	O
factor	O
sampled	O
from	O
Uniform	O
[	O
]	O
.	O

We	O
augment	O
each	O
frustum	O
point	O
cloud	O
by	O
three	O
ways	O
.	O

First	O
,	O
we	O
randomly	O
sample	O
a	O
subset	O
of	O
points	O
from	O
the	O
frustum	O
point	O
cloud	O
on	O
the	O
fly	O
(	O
1	O
,	O
024	O
for	O
KITTI	Material
and	O
2	O
,	O
048	O
for	O
SUN	Material
-	Material
RGBD	Material
)	O
.	O

For	O
object	O
points	O
segmented	O
from	O
our	O
predicted	O
3D	O
mask	O
,	O
we	O
randomly	O
sample	O
512	O
points	O
from	O
it	O
(	O
if	O
there	O
are	O
less	O
than	O
512	O
points	O
we	O
will	O
randomly	O
resample	O
to	O
make	O
up	O
for	O
the	O
number	O
)	O
.	O

Second	O
,	O
we	O
randomly	O
flip	O
the	O
frustum	O
point	O
cloud	O
(	O
after	O
rotating	O
the	O
frustum	O
to	O
the	O
center	O
)	O
along	O
the	O
YZ	O
plane	O
in	O
camera	O
coordinate	O
(	O
Z	O
is	O
forward	O
,	O
Y	O
is	O
pointing	O
down	O
)	O
.	O

Thirdly	O
,	O
we	O
perturb	O
the	O
points	O
by	O
shifting	O
the	O
entire	O
frustum	O
point	O
cloud	O
in	O
Z	O
-	O
axis	O
direction	O
such	O
that	O
the	O
depth	O
of	O
points	O
is	O
augmented	O
.	O

Together	O
with	O
all	O
data	Task
augmentation	Task
,	O
we	O
modify	O
the	O
ground	O
truth	O
labels	O
for	O
3D	O
mask	O
and	O
headings	O
correspondingly	O
.	O

paragraph	O
:	O
KITTI	Material
Training	O
The	O
object	O
detection	Task
benchmark	O
in	O
KITTI	Material
provides	O
synchronized	Material
RGB	Material
images	Material
and	O
LiDAR	Material
point	Material
clouds	Material
with	O
ground	O
truth	O
amodal	O
2D	O
and	O
3D	O
box	O
annotations	O
for	O
vehicles	O
,	O
pedestrians	O
and	O
cyclists	O
.	O

The	O
training	O
set	O
contains	O
7	O
,	O
481	O
frames	O
and	O
an	O
undisclosed	O
test	O
set	O
contains	O
7	O
,	O
581	O
frames	O
.	O

In	O
our	O
own	O
experiments	O
(	O
except	O
those	O
for	O
test	O
sets	O
)	O
,	O
we	O
follow	O
to	O
split	O
the	O
official	O
training	O
set	O
to	O
a	O
train	O
set	O
of	O
3	O
,	O
717	O
frames	O
and	O
a	O
val	O
set	O
of	O
3769	O
frames	O
such	O
that	O
frames	O
in	O
train	O
/	O
val	O
sets	O
belong	O
to	O
different	O
video	O
clips	O
.	O

For	O
models	O
evaluated	O
on	O
the	O
test	O
set	O
we	O
train	O
our	O
model	O
on	O
our	O
own	O
train	O
/	O
val	O
split	O
where	O
around	O
80	O
%	O
of	O
the	O
training	O
data	O
is	O
used	O
such	O
that	O
the	O
model	O
can	O
achieve	O
better	O
generalization	Task
by	O
seeing	O
more	O
examples	O
.	O

To	O
get	O
ground	O
truth	O
for	O
3D	Task
instance	Task
segmentation	Task
we	O
simply	O
consider	O
all	O
points	O
that	O
fall	O
into	O
the	O
ground	O
truth	O
3D	O
bounding	O
box	O
as	O
object	O
points	O
.	O

Although	O
there	O
are	O
sometimes	O
false	O
labels	O
from	O
ground	O
points	O
or	O
points	O
from	O
other	O
closeby	O
objects	O
(	O
e.g.	O
a	O
person	O
standing	O
by	O
)	O
,	O
the	O
auto	O
-	O
labeled	O
segmentation	O
ground	O
truth	O
is	O
in	O
general	O
acceptable	O
.	O

For	O
both	O
of	O
our	O
v1	Method
and	Method
v2	Method
models	Method
,	O
we	O
use	O
Adam	Method
optimizer	Method
with	O
starting	Metric
learning	Metric
rate	Metric
0.001	O
,	O
with	O
step	O
-	O
wise	O
decay	O
(	O
by	O
half	O
)	O
in	O
every	O
60k	O
iterations	O
.	O

For	O
all	O
trainable	O
layers	O
except	O
the	O
last	O
classification	Task
or	Task
regression	Task
ones	Task
,	O
we	O
use	O
batch	Method
normalization	Method
with	O
a	O
start	O
decay	O
rate	O
of	O
0.5	O
and	O
gradually	O
decay	O
the	O
decay	Metric
rate	Metric
to	O
0.99	O
(	O
step	O
-	O
wise	O
decay	O
with	O
rate	O
0.5	O
in	O
every	O
20k	O
iterations	O
)	O
.	O

We	O
use	O
batch	O
size	O
32	O
for	O
v1	Method
models	Method
and	O
batch	O
size	O
24	O
for	O
v2	Method
models	Method
.	O

All	O
three	O
PointNets	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Trained	O
on	O
a	O
single	O
GTX	Method
1080	Method
GPU	Method
,	O
it	O
takes	O
around	O
one	O
day	O
to	O
train	O
a	O
v1	Method
model	Method
(	O
all	O
three	O
nets	O
)	O
for	O
200	O
epochs	O
while	O
it	O
takes	O
around	O
three	O
days	O
for	O
a	O
v2	Method
model	Method
.	O

We	O
picked	O
the	O
early	O
stopped	O
(	O
200	O
epochs	O
)	O
snapshot	Method
models	Method
for	O
evaluation	O
.	O

paragraph	O
:	O
SUN	Material
-	Material
RGBD	Material
Training	Material
The	O
data	O
set	O
consists	O
of	O
10	O
,	O
355	O
RGB	Material
-	Material
D	Material
images	Material
captured	O
from	O
various	O
depth	O
sensors	O
for	O
indoor	O
scenes	O
(	O
bedrooms	O
,	O
dining	O
rooms	O
etc	O
.	O

)	O
.	O

We	O
follow	O
the	O
same	O
train	O
/	O
val	O
splits	O
as	O
for	O
experiments	O
.	O

The	O
data	Method
augmentation	Method
and	O
optimization	O
parameters	O
are	O
the	O
same	O
as	O
that	O
in	O
KITTI	Material
.	O

As	O
to	O
auto	Task
-	Task
labeling	Task
of	Task
instance	Task
segmentation	Task
mask	Task
,	O
however	O
,	O
data	Metric
quality	Metric
is	O
much	O
lower	O
than	O
that	O
in	O
KITTI	Material
because	O
of	O
strong	O
occlusions	O
and	O
tight	O
arrangement	O
of	O
objects	O
in	O
indoor	O
scenes	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
some	O
examples	O
)	O
.	O

Nonetheless	O
we	O
still	O
consider	O
all	O
points	O
within	O
the	O
ground	O
truth	O
boxes	O
as	O
object	O
points	O
for	O
our	O
training	O
.	O

For	O
3D	Task
segmentation	Task
we	O
get	O
only	O
a	O
82.7	O
%	O
accuracy	Metric
compared	O
to	O
around	O
90	O
%	O
in	O
KITTI	Material
.	O

Due	O
to	O
the	O
heavy	O
noise	O
in	O
segmentation	O
mask	O
label	O
,	O
we	O
choose	O
to	O
only	O
train	O
and	O
evaluate	O
on	O
v1	Method
models	Method
that	O
has	O
more	O
strength	O
in	O
global	Method
feature	Method
learning	Method
than	O
v2	O
ones	O
.	O

For	O
future	O
works	O
,	O
we	O
think	O
higher	O
quality	O
in	O
3D	O
mask	O
labels	O
can	O
greatly	O
help	O
the	O
instance	Task
segmentation	Task
network	Task
training	Task
.	O

appendix	O
:	O
Details	O
on	O
RGB	Method
Detector	Method
(	O
Sec	O
4.1	O
)	O
For	O
2D	Task
RGB	Task
image	Task
detector	Task
,	O
we	O
use	O
the	O
encoder	Method
-	Method
decoder	Method
structure	Method
(	O
e.g.	O
DSSD	Method
,	O
FPN	Method
)	O
to	O
generate	O
region	O
proposals	O
from	O
multiple	O
feature	O
maps	O
using	O
focal	O
loss	O
and	O
use	O
Fast	Method
R	Method
-	Method
CNN	Method
to	O
predict	O
final	O
2D	Task
detection	Task
bounding	O
boxes	O
from	O
the	O
region	O
proposals	O
.	O

To	O
make	O
the	O
detector	Task
faster	O
,	O
we	O
take	O
the	O
reduced	Method
VGG	Method
base	Method
network	Method
architecture	Method
from	O
SSD	Method
,	O
sample	O
half	O
of	O
the	O
channels	O
per	O
layer	O
and	O
change	O
all	O
max	Method
pooling	Method
layers	Method
to	O
convolution	Method
layers	Method
with	O
kernel	O
size	O
and	O
stride	O
of	O
2	O
.	O

Then	O
we	O
fine	O
-	O
tune	O
it	O
on	O
ImageNet	Material
CLS	Material
-	Material
LOC	Material
dataset	Material
for	O
400k	O
iterations	O
with	O
batch	O
size	O
of	O
260	O
on	O
10	O
GPUs	O
.	O

The	O
resulting	O
base	O
network	Method
architecture	Method
has	O
about	O
66.7	O
%	O
top	Metric
-	Metric
1	Metric
classification	Metric
accuracy	Metric
on	O
the	O
CLS	Material
-	Material
LOC	Material
validation	Material
dataset	Material
and	O
only	O
needs	O
about	O
1.2ms	O
to	O
process	O
a	O
image	O
on	O
a	O
NVIDIA	Method
GTX	Method
1080	Method
.	O

We	O
then	O
add	O
the	O
feature	Method
pyramid	Method
layers	Method
from	O
conv3_3	Method
,	O
conv4_3	O
,	O
conv5_3	Method
,	O
and	O
fc7	Method
,	O
which	O
are	O
used	O
to	O
predict	O
region	O
proposals	O
with	O
scales	O
of	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
respectively	O
.	O

We	O
also	O
add	O
an	O
extra	O
convolutional	Method
layer	Method
(	O
conv8	Method
)	O
which	O
halves	O
the	O
fc7	O
feature	O
map	O
size	O
,	O
and	O
use	O
it	O
to	O
predict	O
proposals	O
with	O
scale	O
of	O
256	O
.	O

We	O
use	O
5	O
different	O
aspect	O
ratios	O
{	O
,	O
,	O
1	O
,	O
2	O
,	O
3	O
}	O
for	O
all	O
layers	O
except	O
that	O
we	O
ignore	O
{	O
,	O
3	O
}	O
for	O
conv3_3	O
.	O

Following	O
SSD	Method
,	O
we	O
also	O
use	O
normalization	Method
layer	Method
on	O
conv3_3	Method
,	O
conv4_3	O
,	O
and	O
conv5_3	O
and	O
initialize	O
the	O
norm	O
40	O
.	O

For	O
Fast	Task
R	Task
-	Task
CNN	Task
part	Task
,	O
we	O
extract	O
features	O
from	O
conv3_3	Method
,	O
conv5_3	Method
,	O
and	O
conv8	Method
for	O
each	O
region	O
proposal	O
and	O
concatenate	O
all	O
the	O
features	O
to	O
predict	O
class	O
scores	O
and	O
further	O
adjust	O
the	O
proposals	O
.	O

We	O
train	O
this	O
detector	O
from	O
COCO	Material
dataset	Material
with	O
input	O
image	O
and	O
have	O
achieved	O
35.5	O
mAP	Metric
on	O
the	O
COCO	Material
minival	Material
dataset	Material
,	O
with	O
only	O
10ms	O
processing	Metric
time	Metric
for	O
a	O
image	O
on	O
a	O
single	O
GPU	O
.	O

Finally	O
,	O
we	O
fine	O
-	O
tune	O
the	O
detector	O
on	O
car	O
,	O
people	O
,	O
and	O
bicycle	O
from	O
COCO	Material
dataset	Material
,	O
and	O
have	O
achieved	O
48.5	O
,	O
44.1	O
,	O
and	O
40.1	O
for	O
these	O
three	O
classes	O
on	O
COCO	Material
.	O

We	O
take	O
this	O
model	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
car	Task
,	Task
pedestrian	Task
,	O
and	O
cyclist	O
from	O
KITTI	Material
dataset	O
.	O

The	O
final	O
model	O
takes	O
about	O
30ms	O
to	O
process	O
a	O
image	O
.	O

To	O
increase	O
the	O
recall	Metric
of	O
the	O
detector	O
,	O
we	O
also	O
do	O
detection	Task
from	O
the	O
center	O
crop	O
of	O
the	O
image	O
besides	O
the	O
full	O
image	O
,	O
and	O
then	O
merge	O
the	O
detections	O
using	O
non	Method
-	Method
maximum	Method
suppression	Method
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
our	O
detector	O
’s	O
AP	Metric
(	O
2D	O
)	O
on	O
KITTI	Material
test	O
set	O
.	O

Our	O
detector	O
has	O
achieved	O
competitive	O
or	O
better	O
results	O
than	O
current	O
leading	O
players	O
on	O
KITTI	Material
leader	O
board	O
.	O

We	O
’	O
ve	O
also	O
reported	O
our	O
AP	Metric
(	O
2D	O
)	O
on	O
val	O
set	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

appendix	O
:	O
Bird	O
’s	O
Eye	O
View	O
PointNets	O
(	O
Sec	O
5.3	O
)	O
In	O
this	O
section	O
,	O
we	O
show	O
that	O
our	O
3D	Task
detection	Task
framework	O
can	O
also	O
be	O
extended	O
to	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
,	O
which	O
adds	O
another	O
orthogonal	Method
proposal	Method
source	Method
to	O
achieve	O
better	O
overall	O
3D	Task
detection	Task
performance	O
.	O

We	O
evaluate	O
the	O
results	O
of	O
car	Task
detection	Task
using	O
LiDAR	O
bird	O
’s	O
eye	O
view	O
only	O
proposals	O
+	O
point	O
net	O
(	O
Ours	O
(	O
BV	O
)	O
)	O
,	O
and	O
combine	O
frustum	Method
point	Method
net	Method
and	O
bird	O
’s	O
eye	O
view	O
point	O
net	O
using	O
3D	Method
non	Method
-	Method
maximum	Method
suppression	Method
(	Method
NMS	Method
)	Method
(	O
Ours	Method
(	Method
Frustum	Method
+	Method
BV	Method
)	Method
)	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
Proposal	O
Similar	O
to	O
MV3D	O
we	O
use	O
point	O
features	O
such	O
as	O
height	O
,	O
intensity	O
and	O
density	O
,	O
and	O
train	O
the	O
bird	O
’s	O
eye	O
view	O
2D	Method
proposal	Method
net	Method
using	O
the	O
standard	O
Faster	Method
-	Method
RCNN	Method
structure	Method
.	O

The	O
net	O
outputs	O
axis	O
-	O
aligned	O
2D	O
bounding	O
boxes	O
in	O
the	O
bird	O
’s	O
eye	O
view	O
.	O

In	O
detail	O
,	O
we	O
discretize	O
the	O
projected	O
point	O
clouds	O
into	O
2D	O
grids	O
with	O
resolution	O
of	O
meter	O
and	O
with	O
the	O
depth	O
and	O
width	O
range	O
meters	O
,	O
which	O
gives	O
us	O
the	O
input	O
size	O
.	O

For	O
each	O
cell	O
,	O
we	O
take	O
the	O
intensity	O
and	O
the	O
density	O
of	O
the	O
highest	O
point	O
and	O
divide	O
the	O
heights	O
into	O
bins	O
with	O
the	O
height	O
of	O
the	O
highest	O
point	O
in	O
each	O
bin	O
,	O
which	O
gives	O
us	O
channels	O
in	O
total	O
.	O

In	O
Faster	Task
R	Task
-	Task
CNN	Task
,	O
we	O
use	O
the	O
VGG	Method
-	Method
16	Method
with	O
anchor	O
scales	O
(	O
)	O
and	O
aspect	O
ratios	O
(	O
)	O
.	O

We	O
train	O
RPN	Method
and	O
Fast	Method
R	Method
-	Method
CNN	Method
together	O
using	O
the	O
approximate	Method
joint	Method
training	Method
.	O

To	O
combine	O
3D	Task
detection	Task
boxes	O
from	O
frustum	Method
PointNets	Method
and	O
the	O
bird	O
’s	O
eye	O
view	O
PointNets	O
,	O
we	O
use	O
3D	Method
NMS	Method
with	O
IoU	O
threshold	O
.	O

We	O
also	O
apply	O
a	O
weight	O
(	O
0.5	O
)	O
to	O
3D	O
boxes	O
from	O
BV	O
PointNets	O
since	O
it	O
is	O
a	O
weaker	O
detector	O
compared	O
with	O
our	O
frustum	Method
one	Method
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
(	O
BV	O
)	O
PointNets	O
Similar	O
to	O
Frustum	Method
PointNets	Method
that	O
take	O
point	O
cloud	O
in	O
frustum	O
,	O
segment	O
point	O
cloud	O
and	O
estimate	O
amodal	O
bounding	O
box	O
,	O
we	O
can	O
apply	O
PointNets	O
to	O
points	O
in	O
bird	O
’s	O
eye	O
view	O
regions	O
.	O

Since	O
bird	O
’s	O
eye	O
view	O
is	O
based	O
on	O
orthogonal	O
projection	O
,	O
the	O
3D	O
space	O
specified	O
by	O
a	O
BV	O
2D	O
box	O
is	O
a	O
3D	O
cuboid	O
(	O
cut	O
by	O
minimum	O
and	O
maximum	O
height	O
)	O
instead	O
of	O
a	O
frustum	O
.	O

paragraph	O
:	O
Results	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
Ours	O
BV	O
)	O
shows	O
the	O
APs	O
we	O
get	O
by	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
only	O
(	O
without	O
and	O
RGB	O
information	O
)	O
.	O

We	O
compare	O
with	O
two	O
previous	O
LiDAR	Method
only	Method
methods	Method
(	O
VeloFCN	Method
and	O
MV3D	Method
(	O
BV	Method
+	Method
FV	Method
)	Method
)	O
and	O
show	O
that	O
our	O
BV	Method
proposal	Method
based	Method
detector	Method
greatly	O
outperforms	O
VeloFCN	Method
on	O
all	O
cases	O
and	O
outperforms	O
MV3D	Method
(	O
BV	Method
+	Method
FV	Method
)	O
on	O
moderate	O
and	O
hard	O
cases	O
by	O
a	O
significant	O
margin	O
.	O

More	O
importantly	O
,	O
we	O
show	O
in	O
the	O
last	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
that	O
bird	O
’s	O
eye	O
view	O
and	O
RGB	O
view	O
proposals	O
can	O
be	O
combined	O
to	O
achieve	O
an	O
even	O
better	O
performance	O
(	O
3.8	O
%	O
AP	Metric
improvement	O
on	O
hard	O
cases	O
)	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
gives	O
an	O
intuitive	O
explanation	O
of	O
why	O
bird	O
’s	O
eye	O
view	O
proposals	O
could	O
help	O
.	O

In	O
the	O
sample	O
frame	O
shown	O
:	O
while	O
our	O
2D	Method
detector	Method
misses	O
some	O
highly	O
occluded	O
cars	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
left	O
RGB	Material
image	Material
)	O
,	O
bird	O
’s	O
eye	O
view	O
based	O
RPN	Method
successfully	O
detects	O
them	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
blue	O
arrows	O
in	O
right	O
LiDAR	Material
image	Material
)	O
.	O

appendix	O
:	O
More	O
Experiments	O
(	O
Sec	O
5.2	O
)	O
subsection	O
:	O
Effects	O
of	O
PointNet	Method
Architectures	Method
Table	O
[	O
reference	O
]	O
compares	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	Method
++	Method
(	Method
v2	Method
)	O
architectures	O
for	O
instance	Task
segmentation	Task
and	O
amodal	Task
box	Task
estimation	Task
.	O

The	O
v2	Method
model	Method
outperforms	O
v1	Method
model	Method
on	O
both	O
tasks	O
because	O
1	O
)	O
v2	Method
model	Method
learns	O
hierarchical	O
features	O
that	O
are	O
richer	O
and	O
more	O
generalizable	O
;	O
2	O
)	O
v2	Method
model	Method
uses	O
multi	Method
-	Method
scale	Method
feature	Method
learning	Method
that	O
adapts	O
to	O
varying	O
point	O
densities	O
.	O

Note	O
that	O
the	O
ours	O
(	O
v1	O
)	O
model	O
corresponds	O
to	O
first	O
row	O
of	O
Table	O
[	O
reference	O
]	O
while	O
the	O
ours	O
(	O
v2	O
)	O
links	O
to	O
the	O
last	O
row	O
.	O

subsection	O
:	O
Effects	O
of	O
Training	Metric
Data	Metric
Size	Metric
Recently	O
observed	O
linear	O
improvement	O
in	O
performance	O
of	O
deep	Method
learning	Method
models	Method
with	O
exponential	O
growth	O
of	O
data	O
set	O
size	O
.	O

In	O
our	O
Frustum	Method
PointNets	Method
we	O
observe	O
similar	O
trend	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
trend	O
indicates	O
a	O
promising	O
performance	O
potential	O
of	O
our	O
methods	O
with	O
larger	O
datasets	O
.	O

We	O
train	O
three	O
separate	O
group	O
of	O
Frustum	Method
PointNets	Method
on	O
three	O
sets	O
of	O
training	O
data	O
and	O
then	O
evaluate	O
the	O
model	O
on	O
a	O
fixed	O
validation	O
set	O
(	O
1929	O
samples	O
)	O
.	O

The	O
three	O
data	O
points	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
represent	O
training	O
set	O
sizes	O
of	O
1388	O
,	O
2776	O
,	O
5552	O
samples	O
(	O
0.185x	O
,	O
0.371x	O
,	O
0.742x	O
of	O
the	O
entire	O
trainval	O
set	O
)	O
respectively	O
.	O

We	O
augment	O
the	O
training	O
data	O
such	O
that	O
the	O
total	O
amount	O
of	O
samples	O
are	O
the	O
same	O
for	O
each	O
of	O
the	O
three	O
cases	O
(	O
20x	O
,	O
10x	O
and	O
5x	O
augmentation	O
respectively	O
)	O
.	O

The	O
training	O
set	O
and	O
validation	O
set	O
are	O
chosen	O
such	O
that	O
they	O
do	O
n’t	O
share	O
frames	O
from	O
the	O
same	O
video	O
clips	O
.	O

subsection	O
:	O
Runtime	O
and	O
Model	Metric
Size	Metric
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
decomposed	O
runtime	Metric
cost	Metric
(	O
inference	Metric
time	Metric
)	O
for	O
our	O
frustum	Method
PointNets	Method
(	O
v1	O
and	O
v2	O
)	O
.	O

The	O
evaluation	O
is	O
based	O
on	O
TensorFlow	Method
with	O
a	O
NVIDIA	Method
GTX	Method
1080	Method
and	O
a	O
single	O
CPU	O
core	O
.	O

While	O
for	O
v1	Method
model	Method
frustum	Method
proposal	Method
(	O
with	O
CNN	Method
and	Method
backprojection	Method
)	O
takes	O
the	O
majority	O
time	O
,	O
for	O
v2	Method
model	Method
since	O
a	O
PointNet	Method
++	Method
model	Method
with	O
multi	Method
-	Method
scale	Method
grouping	Method
is	O
used	O
,	O
computation	O
bottleneck	O
shifts	O
to	O
instance	Task
segmentation	Task
.	O

Note	O
that	O
we	O
merge	O
batch	Method
normalization	Method
and	O
FC	Method
/	Method
convolution	Method
layers	Method
for	O
faster	O
inference	Task
(	O
since	O
they	O
are	O
both	O
linear	O
operation	O
with	O
multiply	O
and	O
sum	O
)	O
,	O
which	O
results	O
in	O
close	O
to	O
50	O
%	O
speedup	O
for	O
inference	Task
.	O

CNN	Method
model	Method
has	O
size	O
28	O
MB	O
.	O

v1	O
PointNets	Method
have	O
size	O
19	O
MB	O
.	O

v2	O
PointNets	Method
have	O
size	O
22	O
MB	O
.	O

The	O
total	O
size	O
is	O
therefore	O
47	O
MB	O
for	O
v1	Method
model	Method
and	O
50	O
MB	O
for	O
v2	Method
model	Method
.	O

appendix	O
:	O
Visualizations	Task
for	O
SUN	Material
-	Material
RGBD	Material
(	O
Sec	O
5.1	O
)	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
some	O
representative	O
detection	Task
results	O
on	O
SUN	Material
-	Material
RGBD	Material
data	O
.	O

We	O
can	O
see	O
that	O
compared	O
with	O
KITTI	Material
LiDAR	O
data	O
,	O
depth	Material
images	Material
can	O
be	O
popped	O
up	O
to	O
much	O
more	O
dense	O
point	O
clouds	O
.	O

However	O
even	O
with	O
such	O
dense	O
point	O
cloud	O
,	O
strong	O
occlusions	O
of	O
indoor	O
objects	O
as	O
well	O
as	O
the	O
tight	O
arrangement	O
present	O
new	O
challenges	O
for	O
detection	Task
in	O
indoor	O
scenes	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
report	O
the	O
3D	O
AP	Metric
curves	O
of	O
our	O
Frustum	Method
PointNets	Method
on	O
SUN	Material
-	Material
RGBD	Material
val	O
set	O
.	O

2D	Task
detection	Task
APs	O
of	O
our	O
RGB	Method
detector	Method
are	O
also	O
provided	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

document	O
:	O
Graph	Method
-	Method
Structured	Method
Representations	Method
for	O
Visual	Task
Question	Task
Answering	Task
This	O
paper	O
proposes	O
to	O
improve	O
visual	Task
question	Task
answering	Task
(	O
VQA	Task
)	O
with	O
structured	Method
representations	Method
of	O
both	O
scene	O
contents	O
and	O
questions	O
.	O

A	O
key	O
challenge	O
in	O
VQA	Task
is	O
to	O
require	O
joint	Task
reasoning	Task
over	O
the	O
visual	O
and	O
text	O
domains	O
.	O

The	O
predominant	O
CNN	Method
/	O
LSTM	O
-	O
based	O
approach	O
to	O
VQA	Task
is	O
limited	O
by	O
monolithic	Method
vector	Method
representations	Method
that	O
largely	O
ignore	O
structure	O
in	O
the	O
scene	O
and	O
in	O
the	O
question	O
.	O

CNN	Method
feature	O
vectors	O
can	O
not	O
effectively	O
capture	O
situations	O
as	O
simple	O
as	O
multiple	O
object	O
instances	O
,	O
and	O
LSTMs	Method
process	O
questions	O
as	O
series	O
of	O
words	O
,	O
which	O
do	O
not	O
reflect	O
the	O
true	O
complexity	O
of	O
language	O
structure	O
.	O

We	O
instead	O
propose	O
to	O
build	O
graphs	O
over	O
the	O
scene	O
objects	O
and	O
over	O
the	O
question	O
words	O
,	O
and	O
we	O
describe	O
a	O
deep	Method
neural	Method
network	Method
that	O
exploits	O
the	O
structure	O
in	O
these	O
representations	O
.	O

We	O
show	O
that	O
this	O
approach	O
achieves	O
significant	O
improvements	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
increasing	O
accuracy	Metric
from	O
71.2	O
%	O
to	O
74.4	O
%	O
on	O
the	O
“	O
abstract	O
scenes	O
”	O
multiple	O
-	O
choice	O
benchmark	O
,	O
and	O
from	O
34.7	O
%	O
to	O
39.1	O
%	O
for	O
the	O
more	O
challenging	O
“	O
balanced	O
”	O
scenes	O
,	O
i.e	O
.	O

image	O
pairs	O
with	O
fine	O
-	O
grained	O
differences	O
and	O
opposite	O
yes	O
/	O
no	O
answers	O
to	O
a	O
same	O
question	O
.	O

section	O
:	O
Introduction	O
The	O
task	O
of	O
Visual	Task
Question	Task
Answering	Task
has	O
received	O
growing	O
interest	O
in	O
the	O
recent	O
years	O
(	O
see	O
for	O
example	O
)	O
.	O

One	O
of	O
the	O
more	O
interesting	O
aspects	O
of	O
the	O
problem	O
is	O
that	O
it	O
combines	O
computer	Task
vision	Task
,	O
natural	Task
language	Task
processing	Task
,	O
and	O
artificial	Task
intelligence	Task
.	O

In	O
its	O
open	O
-	O
ended	O
form	O
,	O
a	O
question	O
is	O
provided	O
as	O
text	O
in	O
natural	O
language	O
together	O
with	O
an	O
image	O
,	O
and	O
a	O
correct	O
answer	O
must	O
be	O
predicted	O
,	O
typically	O
in	O
the	O
form	O
of	O
a	O
single	O
word	O
or	O
a	O
short	O
phrase	O
.	O

In	O
the	O
multiple	Method
-	Method
choice	Method
variant	Method
,	O
an	O
answer	O
is	O
selected	O
from	O
a	O
provided	O
set	O
of	O
candidates	O
,	O
alleviating	O
evaluation	O
issues	O
related	O
to	O
synonyms	O
and	O
paraphrasing	O
.	O

Multiple	O
datasets	O
for	O
VQA	Task
have	O
been	O
introduced	O
with	O
either	O
real	O
or	O
synthetic	O
images	O
.	O

Our	O
experiments	O
uses	O
the	O
latter	O
,	O
being	O
based	O
on	O
clip	O
art	O
or	O
“	O
cartoon	O
”	O
images	O
created	O
by	O
humans	O
to	O
depict	O
realistic	O
scenes	O
(	O
they	O
are	O
usually	O
referred	O
to	O
as	O
“	O
abstract	O
scenes	O
”	O
,	O
despite	O
this	O
being	O
a	O
misnomer	O
)	O
.	O

Our	O
experiments	O
focus	O
on	O
this	O
dataset	O
of	O
clip	O
art	O
scenes	O
as	O
they	O
allow	O
to	O
focus	O
on	O
semantic	Task
reasoning	Task
and	O
vision	Task
-	Task
language	Task
interactions	Task
,	O
in	O
isolation	O
from	O
the	O
performance	O
of	O
visual	Task
recognition	Task
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

They	O
also	O
allow	O
the	O
manipulation	O
of	O
the	O
image	O
data	O
so	O
as	O
to	O
better	O
illuminate	O
algorithm	O
performance	O
.	O

A	O
particularly	O
attractive	O
VQA	Task
dataset	O
was	O
introduced	O
in	O
by	O
selecting	O
only	O
the	O
questions	O
with	O
binary	O
answers	O
(	O
e.g	O
.	O

yes	O
/	O
no	O
)	O
and	O
pairing	O
each	O
(	O
synthetic	O
)	O
image	O
with	O
a	O
minimally	O
-	O
different	O
complementary	O
version	O
that	O
elicits	O
the	O
opposite	O
(	O
no	O
/	O
yes	O
)	O
answer	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
bottom	O
rows	O
)	O
.	O

This	O
strongly	O
contrasts	O
with	O
other	O
VQA	Task
datasets	O
of	O
real	O
images	O
,	O
where	O
a	O
correct	O
answer	O
is	O
often	O
obvious	O
without	O
looking	O
at	O
the	O
image	O
,	O
by	O
relying	O
on	O
systematic	O
regularities	O
of	O
frequent	O
questions	O
and	O
answers	O
.	O

Performance	O
improvements	O
reported	O
on	O
such	O
datasets	O
are	O
difficult	O
to	O
interpret	O
as	O
actual	O
progress	O
in	O
scene	Task
understanding	Task
and	O
reasoning	Task
as	O
they	O
might	O
similarly	O
be	O
taken	O
to	O
represent	O
a	O
better	O
modeling	O
of	O
the	O
language	O
prior	O
of	O
the	O
dataset	O
.	O

This	O
hampers	O
,	O
or	O
at	O
best	O
obscures	O
,	O
progress	O
toward	O
the	O
greater	O
goal	O
of	O
general	Task
VQA	Task
.	O

In	O
our	O
view	O
,	O
and	O
despite	O
obvious	O
limitations	O
of	O
synthetic	O
images	O
,	O
improvements	O
on	O
the	O
aforementioned	O
“	O
balanced	O
”	O
dataset	O
constitute	O
an	O
illuminating	O
measure	O
of	O
progress	O
in	O
scene	Task
-	Task
understanding	Task
,	O
because	O
a	O
language	Method
model	Method
alone	O
can	O
not	O
perform	O
better	O
than	O
chance	O
on	O
this	O
data	O
.	O

paragraph	O
:	O
Challenges	O
The	O
questions	O
in	O
the	O
clip	O
-	O
art	O
dataset	O
vary	O
greatly	O
in	O
their	O
complexity	O
.	O

Some	O
can	O
be	O
directly	O
answered	O
from	O
observations	O
of	O
visual	O
elements	O
,	O
e.g	O
.	O

Is	O
there	O
a	O
dog	O
in	O
the	O
room	O
?	O
,	O
or	O
Is	O
the	O
weather	O
good	O
?	O
.	O

Others	O
require	O
relating	O
multiple	O
facts	O
or	O
understanding	O
complex	O
actions	O
,	O
e.g	O
.	O

Is	O
the	O
boy	O
going	O
to	O
catch	O
the	O
ball	O
?	O
,	O
or	O
Is	O
it	O
winter	O
?	O
.	O

An	O
additional	O
challenge	O
,	O
which	O
affects	O
all	O
VQA	Task
datasets	O
,	O
is	O
the	O
sparsity	O
of	O
the	O
training	O
data	O
.	O

Even	O
a	O
large	O
number	O
of	O
training	O
questions	O
(	O
almost	O
25	O
,	O
000	O
for	O
the	O
clip	O
art	O
scenes	O
of	O
)	O
can	O
not	O
possibly	O
cover	O
the	O
combinatorial	O
diversity	O
of	O
possible	O
objects	O
and	O
concepts	O
.	O

Adding	O
to	O
this	O
challenge	O
,	O
most	O
methods	O
for	O
VQA	Task
process	O
the	O
question	O
through	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
such	O
as	O
an	O
LSTM	Method
)	O
trained	O
from	O
scratch	O
solely	O
on	O
the	O
training	O
questions	O
.	O

paragraph	O
:	O
Language	Method
representation	Method
The	O
above	O
reasons	O
motivate	O
us	O
to	O
take	O
advantage	O
of	O
the	O
extensive	O
existing	O
work	O
in	O
the	O
natural	O
language	O
community	O
to	O
aid	O
processing	O
the	O
questions	O
.	O

First	O
,	O
we	O
identify	O
the	O
syntactic	O
structure	O
of	O
the	O
question	O
using	O
a	O
dependency	Method
parser	Method
.	O

This	O
produces	O
a	O
graph	Method
representation	Method
of	O
the	O
question	O
in	O
which	O
each	O
node	O
represents	O
a	O
word	O
and	O
each	O
edge	O
a	O
particular	O
type	O
of	O
dependency	O
(	O
e.g	O
.	O

determiner	O
,	O
nominal	O
subject	O
,	O
direct	O
object	O
,	O
etc	O
.	O

)	O
.	O

Second	O
,	O
we	O
associate	O
each	O
word	O
(	O
node	O
)	O
with	O
a	O
vector	Method
embedding	Method
pretrained	Method
on	O
large	O
corpora	O
of	O
text	O
data	O
.	O

This	O
embedding	O
maps	O
the	O
words	O
to	O
a	O
space	O
in	O
which	O
distances	O
are	O
semantically	O
meaningful	O
.	O

Consequently	O
,	O
this	O
essentially	O
regularizes	O
the	O
remainder	O
of	O
the	O
network	O
to	O
share	O
learned	O
concepts	O
among	O
related	O
words	O
and	O
synonyms	O
.	O

This	O
particularly	O
helps	O
in	O
dealing	O
with	O
rare	O
words	O
,	O
and	O
also	O
allows	O
questions	O
to	O
include	O
words	O
absent	O
from	O
the	O
training	O
questions	O
/	O
answers	O
.	O

Note	O
that	O
this	O
pretraining	O
and	O
ad	Task
hoc	Task
processing	Task
of	O
the	O
language	Task
part	Task
mimics	O
a	O
practice	O
common	O
for	O
the	O
image	Task
part	Task
,	O
in	O
which	O
visual	O
features	O
are	O
usually	O
obtained	O
from	O
a	O
fixed	O
CNN	Method
,	O
itself	O
pretrained	O
on	O
a	O
larger	O
dataset	O
and	O
with	O
a	O
different	O
(	O
supervised	Metric
classification	Metric
)	Metric
objective	Metric
.	O

paragraph	O
:	O
Scene	Method
representation	Method
Each	O
object	O
in	O
the	O
scene	O
corresponds	O
to	O
a	O
node	O
in	O
the	O
scene	O
graph	O
,	O
which	O
has	O
an	O
associated	O
feature	O
vector	O
describing	O
its	O
appearance	O
.	O

The	O
graph	O
is	O
fully	O
connected	O
,	O
with	O
each	O
edge	O
representing	O
the	O
relative	O
position	O
of	O
the	O
objects	O
in	O
the	O
image	O
.	O

paragraph	O
:	O
Applying	O
Neural	Method
Networks	Method
to	O
graphs	O
The	O
two	O
graph	Method
representations	Method
feed	O
into	O
a	O
deep	Method
neural	Method
network	Method
that	O
we	O
will	O
describe	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
advantage	O
of	O
this	O
approach	O
with	O
text	Task
-	Task
and	Task
scene	Task
-	Task
graphs	Task
,	O
rather	O
than	O
more	O
typical	O
representations	O
,	O
is	O
that	O
the	O
graphs	O
can	O
capture	O
relationships	O
between	O
words	O
and	O
between	O
objects	O
which	O
are	O
of	O
semantic	O
significance	O
.	O

This	O
enables	O
the	O
GNN	Method
to	O
exploit	O
(	O
1	O
)	O
the	O
unordered	O
nature	O
of	O
scene	O
elements	O
(	O
the	O
objects	O
in	O
particular	O
)	O
and	O
(	O
2	O
)	O
the	O
semantic	O
relationships	O
between	O
elements	O
(	O
and	O
the	O
grammatical	O
relationships	O
between	O
words	O
in	O
particular	O
)	O
.	O

This	O
contrasts	O
with	O
the	O
typical	O
approach	O
of	O
representing	O
the	O
image	O
with	O
CNN	Method
activations	O
(	O
which	O
are	O
sensitive	O
to	O
individual	O
object	O
locations	O
but	O
less	O
so	O
to	O
relative	O
position	O
)	O
and	O
the	O
processing	O
words	O
of	O
the	O
question	O
serially	O
with	O
an	O
RNN	Method
(	O
despite	O
the	O
fact	O
that	O
grammatical	O
structure	O
is	O
very	O
non	O
-	O
linear	O
)	O
.	O

The	O
graph	Method
representation	Method
ignores	O
the	O
order	O
in	O
which	O
elements	O
are	O
processed	O
,	O
but	O
instead	O
represents	O
the	O
relationships	O
between	O
different	O
elements	O
using	O
different	O
edge	O
types	O
.	O

Our	O
network	O
uses	O
multiple	O
layers	O
that	O
iterate	O
over	O
the	O
features	O
associated	O
with	O
every	O
node	O
,	O
then	O
ultimately	O
identifies	O
a	O
soft	O
matching	O
between	O
nodes	O
from	O
the	O
two	O
graphs	O
.	O

This	O
matching	O
reflects	O
the	O
correspondences	O
between	O
the	O
words	O
in	O
the	O
question	O
and	O
the	O
objects	O
in	O
the	O
image	O
.	O

The	O
features	O
of	O
the	O
matched	O
nodes	O
then	O
feed	O
into	O
a	O
classifier	Method
to	O
infer	O
the	O
answer	O
to	O
the	O
question	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
four	O
-	O
fold	O
.	O

We	O
describe	O
how	O
to	O
use	O
graph	Method
representations	Method
of	Method
scene	Method
and	Method
question	Method
for	O
VQA	Task
,	O
and	O
a	O
neural	Method
network	Method
capable	O
of	O
processing	O
these	O
representations	O
to	O
infer	O
an	O
answer	O
.	O

We	O
show	O
how	O
to	O
make	O
use	O
of	O
an	O
off	O
-	O
the	O
-	O
shelf	O
language	Method
parsing	Method
tool	Method
by	O
generating	O
a	O
graph	Method
representation	Method
of	Method
text	Method
that	O
captures	O
grammatical	O
relationships	O
,	O
and	O
by	O
making	O
this	O
information	O
accessible	O
to	O
the	O
VQA	Task
model	O
.	O

This	O
representation	O
uses	O
a	O
pre	O
-	O
trained	O
word	Method
embedding	Method
to	O
form	O
node	O
features	O
,	O
and	O
encodes	O
syntactic	O
dependencies	O
between	O
words	O
as	O
edge	O
features	O
.	O

We	O
train	O
the	O
proposed	O
model	O
on	O
the	O
VQA	Task
“	O
abstract	O
scenes	O
”	O
benchmark	O
and	O
demonstrate	O
its	O
efficacy	O
by	O
raising	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
from	O
71.2	O
%	O
to	O
74.4	O
%	O
in	O
the	O
multiple	Task
-	Task
choice	Task
setting	Task
.	O

On	O
the	O
“	O
balanced	O
”	O
version	O
of	O
the	O
dataset	O
,	O
we	O
raise	O
the	O
accuracy	Metric
from	O
34.7	O
%	O
to	O
39.1	O
%	O
in	O
the	O
hardest	O
setting	O
(	O
requiring	O
a	O
correct	O
answer	O
over	O
pairs	O
of	O
scenes	O
)	O
.	O

We	O
evaluate	O
the	O
uncertainty	O
in	O
the	O
model	O
by	O
presenting	O
–	O
for	O
the	O
first	O
time	O
on	O
the	O
task	O
of	O
VQA	Task
–	O
precision	Metric
/	Metric
recall	Metric
curves	Metric
of	O
predicted	O
answers	O
.	O

Those	O
curves	O
provide	O
more	O
insight	O
than	O
the	O
single	O
accuracy	Metric
metric	Metric
and	O
show	O
that	O
the	O
uncertainty	O
estimated	O
by	O
the	O
model	O
about	O
its	O
predictions	O
correlates	O
with	O
the	O
ambiguity	O
of	O
the	O
human	O
-	O
provided	O
ground	O
truth	O
.	O

section	O
:	O
Related	O
work	O
The	O
task	O
of	O
visual	Task
question	Task
answering	Task
has	O
received	O
increasing	O
interest	O
since	O
the	O
seminal	O
paper	O
of	O
Antol	O
et	O
al	O
.	O

.	O

Most	O
recent	O
methods	O
are	O
based	O
on	O
the	O
idea	O
of	O
a	O
joint	Method
embedding	Method
of	O
the	O
image	O
and	O
the	O
question	O
using	O
a	O
deep	Method
neural	Method
network	Method
.	O

The	O
image	O
is	O
passed	O
through	O
a	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
pretrained	O
for	O
image	Task
classification	Task
,	O
from	O
which	O
intermediate	O
features	O
are	O
extracted	O
to	O
describe	O
the	O
image	O
.	O

The	O
question	O
is	O
typically	O
passed	O
through	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
such	O
as	O
an	O
LSTM	Method
,	O
which	O
produces	O
a	O
fixed	Method
-	Method
size	Method
vector	Method
representing	O
the	O
sequence	O
of	O
words	O
.	O

These	O
two	O
representations	O
are	O
mapped	O
to	O
a	O
joint	O
space	O
by	O
one	O
or	O
several	O
non	Method
-	Method
linear	Method
layers	Method
.	O

They	O
can	O
then	O
be	O
fed	O
into	O
a	O
classifier	Method
over	O
an	O
output	O
vocabulary	O
,	O
predicting	O
the	O
final	O
answer	O
.	O

Most	O
recent	O
papers	O
on	O
VQA	Task
propose	O
improvements	O
and	O
variations	O
on	O
this	O
basic	O
idea	O
.	O

Consult	O
for	O
a	O
survey	O
.	O

A	O
major	O
improvement	O
to	O
the	O
basic	O
method	O
is	O
to	O
use	O
an	O
attention	Method
mechanism	Method
.	O

It	O
models	O
interactions	O
between	O
specific	O
parts	O
of	O
the	O
inputs	O
(	O
image	O
and	O
question	O
)	O
depending	O
on	O
their	O
actual	O
contents	O
.	O

The	O
visual	O
input	O
is	O
then	O
typically	O
represented	O
a	O
spatial	Method
feature	Method
map	Method
,	O
instead	O
of	O
holistic	O
,	O
image	O
-	O
wide	O
features	O
.	O

The	O
feature	Method
map	Method
is	O
used	O
with	O
the	O
question	O
to	O
determine	O
spatial	O
weights	O
that	O
reflect	O
the	O
most	O
relevant	O
regions	O
of	O
the	O
image	O
.	O

Our	O
approach	O
uses	O
a	O
similar	O
weighting	Method
operation	Method
,	O
which	O
,	O
with	O
our	O
graph	Method
representation	Method
,	O
we	O
equate	O
to	O
a	O
subgraph	Method
matching	Method
.	O

Graph	O
nodes	O
representing	O
question	O
words	O
are	O
associated	O
with	O
graph	O
nodes	O
representing	O
scene	O
objects	O
and	O
vice	O
versa	O
.	O

Similarly	O
,	O
the	O
co	Method
-	Method
attention	Method
model	Method
of	O
Lu	O
et	O
al	O
.	O

determines	O
attention	O
weights	O
on	O
both	O
image	O
regions	O
and	O
question	O
words	O
.	O

Their	O
best	O
-	O
performing	O
approach	O
proceeds	O
in	O
a	O
sequential	O
manner	O
,	O
starting	O
with	O
question	Task
-	Task
guided	Task
visual	Task
attention	Task
followed	O
by	O
image	Method
-	Method
guided	Method
question	Method
attention	Method
.	O

In	O
our	O
case	O
,	O
we	O
found	O
that	O
a	O
joint	Method
,	Method
one	Method
-	Method
pass	Method
version	Method
performs	O
better	O
.	O

A	O
major	O
contribution	O
of	O
our	O
model	O
is	O
to	O
use	O
structured	Method
representations	Method
of	O
the	O
input	O
scene	O
and	O
the	O
question	O
.	O

This	O
contrasts	O
with	O
typical	O
CNN	Method
and	O
RNN	Method
models	O
which	O
are	O
limited	O
to	O
spatial	O
feature	O
maps	O
and	O
sequences	O
of	O
words	O
respectively	O
.	O

The	O
dynamic	Method
memory	Method
networks	Method
(	O
DMN	Method
)	O
,	O
applied	O
to	O
VQA	Task
in	O
also	O
maintain	O
a	O
set	Method
-	Method
like	Method
representation	Method
of	O
the	O
input	O
.	O

As	O
in	O
our	O
model	O
,	O
the	O
DMN	Method
models	Method
interactions	Method
between	O
different	O
parts	O
of	O
the	O
input	O
.	O

Our	O
method	O
can	O
additionally	O
take	O
,	O
as	O
input	O
,	O
features	O
characterizing	O
arbitrary	O
relations	O
between	O
parts	O
of	O
the	O
input	O
(	O
the	O
edge	O
features	O
in	O
our	O
graphs	O
)	O
.	O

This	O
specifically	O
allows	O
making	O
use	O
of	O
syntactic	O
dependencies	O
between	O
words	O
after	O
pre	O
-	O
parsing	O
the	O
question	O
.	O

Most	O
VQA	Task
systems	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
from	O
questions	O
and	O
images	O
to	O
answers	O
,	O
with	O
the	O
exception	O
of	O
the	O
visual	Method
feature	Method
extractor	Method
,	O
which	O
is	O
typically	O
a	O
CNN	Method
pretrained	O
for	O
image	Task
classification	Task
.	O

For	O
the	O
language	Task
processing	Task
part	Task
,	O
some	O
methods	O
address	O
the	O
the	O
semantic	O
aspect	O
with	O
word	Method
embeddings	Method
pretrained	O
on	O
a	O
language	Task
modeling	Task
task	Task
(	O
e.g	O
.	O

)	O
.	O

The	O
syntactic	O
relationships	O
between	O
the	O
words	O
in	O
the	O
question	O
are	O
typically	O
overlooked	O
,	O
however	O
.	O

In	O
,	O
hand	O
-	O
designed	O
rules	O
serve	O
to	O
identify	O
primary	O
and	O
secondary	O
objects	O
of	O
the	O
questions	O
.	O

In	O
the	O
Neural	Method
Module	Method
Networks	Method
,	O
the	O
question	O
is	O
processed	O
by	O
a	O
dependency	Method
parser	Method
,	O
and	O
fragments	O
of	O
the	O
parse	O
,	O
selected	O
with	O
ad	O
hoc	O
fixed	O
rules	O
are	O
associated	O
with	O
modules	O
,	O
are	O
assembled	O
into	O
a	O
full	Method
neural	Method
network	Method
.	O

In	O
contrast	O
,	O
our	O
method	O
is	O
trained	O
to	O
make	O
direct	O
use	O
of	O
the	O
output	O
of	O
a	O
syntactic	Method
parser	Method
.	O

Neural	Method
networks	Method
on	O
graphs	O
have	O
received	O
significant	O
attention	O
recently	O
.	O

The	O
approach	O
most	O
similar	O
to	O
ours	O
is	O
the	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Network	Method
,	O
which	O
associate	O
a	O
gated	Method
recurrent	Method
unit	Method
(	O
GRU	Method
)	O
to	O
each	O
node	O
,	O
and	O
updates	O
the	O
feature	O
vector	O
of	O
each	O
node	O
by	O
iteratively	O
passing	O
messages	O
between	O
neighbours	O
.	O

Also	O
related	O
is	O
the	O
work	O
of	O
Vinyals	O
et	O
al	O
.	O

for	O
embedding	O
a	O
set	O
into	O
fixed	O
-	O
size	O
vector	O
,	O
invariant	O
to	O
the	O
order	O
of	O
its	O
elements	O
.	O

They	O
do	O
so	O
by	O
feeding	O
the	O
entire	O
set	O
through	O
a	O
recurrent	Method
unit	Method
multiple	O
times	O
.	O

Each	O
iteration	O
uses	O
an	O
attention	Method
mechanism	Method
to	O
focus	O
on	O
different	O
parts	O
of	O
the	O
set	O
.	O

Our	O
formulation	O
similarly	O
incorporates	O
information	O
from	O
neighbours	O
into	O
each	O
node	O
feature	O
over	O
multiple	O
iterations	O
,	O
but	O
we	O
did	O
not	O
find	O
any	O
advantage	O
in	O
using	O
an	O
attention	Method
mechanism	Method
within	O
the	O
recurrent	Method
unit	Method
.	O

section	O
:	O
Graph	Task
representation	Task
of	Task
scenes	Task
and	Task
questions	Task
The	O
input	O
data	O
for	O
each	O
training	O
or	O
test	O
instance	O
is	O
a	O
question	O
,	O
and	O
a	O
parameterized	O
description	O
of	O
contents	O
of	O
the	O
scene	O
.	O

The	O
question	O
is	O
processed	O
with	O
the	O
Stanford	Method
dependency	Method
parser	Method
,	O
which	O
outputs	O
the	O
following	O
.	O

A	O
set	O
of	O
words	O
that	O
constitute	O
the	O
nodes	O
of	O
the	O
question	O
graph	O
.	O

Each	O
word	O
is	O
represented	O
by	O
its	O
index	O
in	O
the	O
input	O
vocabulary	O
,	O
a	O
token	O
(	O
)	O
.	O

A	O
set	O
of	O
pairwise	O
relations	O
between	O
words	O
,	O
which	O
constitute	O
the	O
edges	O
of	O
our	O
graph	O
.	O

An	O
edge	O
between	O
words	O
and	O
is	O
represented	O
by	O
,	O
an	O
index	O
among	O
the	O
possible	O
types	O
of	O
dependencies	O
.	O

The	O
dataset	O
provides	O
the	O
following	O
information	O
about	O
the	O
image	O
A	O
set	O
of	O
objects	O
that	O
constitute	O
the	O
nodes	O
of	O
the	O
scene	O
graph	O
.	O

Each	O
node	O
is	O
represented	O
by	O
a	O
vector	O
of	O
visual	O
features	O
(	O
)	O
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
implementation	O
details	O
.	O

A	O
set	O
of	O
pairwise	O
relations	O
between	O
all	O
objects	O
.	O

They	O
form	O
the	O
edges	O
of	O
a	O
fully	O
-	O
connected	O
graph	O
of	O
the	O
scene	O
.	O

The	O
edge	O
between	O
objects	O
and	O
is	O
represented	O
by	O
a	O
vector	O
that	O
encodes	O
relative	O
spatial	O
relationships	O
(	O
see	O
supp	O
.	O

mat	O
.	O

)	O
.	O

Our	O
experiments	O
are	O
carried	O
out	O
on	O
datasets	O
of	O
clip	O
art	O
scenes	O
,	O
in	O
which	O
descriptions	O
of	O
the	O
scenes	O
are	O
provided	O
in	O
the	O
form	O
of	O
lists	O
of	O
objects	O
with	O
their	O
visual	O
features	O
.	O

The	O
method	O
is	O
equally	O
applicable	O
to	O
real	O
images	O
,	O
with	O
the	O
object	O
list	O
replaced	O
by	O
candidate	O
object	O
detections	O
.	O

Our	O
experiments	O
on	O
clip	Task
art	Task
allows	O
the	O
effect	O
of	O
the	O
proposed	O
method	O
to	O
be	O
isolated	O
from	O
the	O
performance	O
of	O
the	O
object	Method
detector	Method
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
implementation	O
details	O
.	O

The	O
features	O
of	O
all	O
nodes	O
and	O
edges	O
are	O
projected	O
to	O
a	O
vector	O
space	O
of	O
common	O
dimension	O
(	O
typically	O
=	O
300	O
)	O
.	O

The	O
question	O
nodes	O
and	O
edges	O
use	O
vector	Method
embeddings	Method
implemented	O
as	O
look	O
-	O
up	O
tables	O
,	O
and	O
the	O
scene	O
nodes	O
and	O
edges	O
use	O
affine	Method
projections	Method
:	O
with	O
the	O
word	Method
embedding	Method
(	O
usually	O
pretrained	O
,	O
see	O
supplementary	O
material	O
)	O
,	O
the	O
embedding	O
of	O
dependencies	O
,	O
and	O
weight	O
matrices	O
,	O
and	O
and	O
biases	O
.	O

section	O
:	O
Processing	Task
graphs	Task
with	O
neural	Method
networks	Method
We	O
now	O
describe	O
a	O
deep	Method
neural	Method
network	Method
suitable	O
for	O
processing	O
the	O
question	Task
and	Task
scene	Task
graphs	Task
to	O
infer	O
an	O
answer	O
.	O

See	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
an	O
overview	O
.	O

The	O
two	O
graphs	O
representing	O
the	O
question	O
and	O
the	O
scene	O
are	O
processed	O
independently	O
in	O
a	O
recurrent	Method
architecture	Method
.	O

We	O
drop	O
the	O
exponents	O
and	O
for	O
this	O
paragraph	O
as	O
the	O
same	O
procedure	O
applies	O
to	O
both	O
graphs	O
.	O

Each	O
node	O
is	O
associated	O
with	O
a	O
gated	Method
recurrent	Method
unit	Method
(	O
GRU	Method
)	O
and	O
processed	O
over	O
a	O
fixed	O
number	O
of	O
iterations	O
(	O
typically	O
=	O
4	O
)	O
:	O
Square	O
brackets	O
with	O
a	O
semicolon	O
represent	O
a	O
concatenation	O
of	O
vectors	O
,	O
and	O
the	O
Hadamard	Method
(	Method
element	Method
-	Method
wise	Method
)	Method
product	Method
.	O

The	O
final	O
state	O
of	O
the	O
GRU	Method
is	O
used	O
as	O
the	O
new	O
representation	O
of	O
the	O
nodes	O
:	O
.	O

The	O
operation	O
transforms	O
features	O
from	O
a	O
variable	O
number	O
of	O
neighbours	O
(	O
i.e	O
.	O

connected	O
nodes	O
)	O
to	O
a	O
fixed	Method
-	Method
size	Method
representation	Method
.	O

Any	O
commutative	O
operation	O
can	O
be	O
used	O
(	O
e.g	O
.	O

sum	O
,	O
maximum	O
)	O
.	O

In	O
our	O
implementation	O
,	O
we	O
found	O
the	O
best	O
performance	O
with	O
the	O
average	Method
function	Method
,	O
taking	O
care	O
of	O
averaging	O
over	O
the	O
variable	O
number	O
of	O
connected	O
neighbours	O
.	O

An	O
intuitive	O
interpretation	O
of	O
the	O
recurrent	Method
processing	Method
is	O
to	O
progressively	O
integrate	O
context	O
information	O
from	O
connected	O
neighbours	O
into	O
each	O
node	O
’s	O
own	O
representation	O
.	O

A	O
node	O
corresponding	O
to	O
the	O
word	O
’	O
ball	O
’	O
,	O
for	O
instance	O
,	O
might	O
thus	O
incorporate	O
the	O
fact	O
that	O
the	O
associated	O
adjective	O
is	O
’	O
red	O
’	O
.	O

Our	O
formulation	O
is	O
similar	O
but	O
slightly	O
different	O
from	O
the	O
gated	Method
graph	Method
networks	Method
,	O
as	O
the	O
propagation	O
of	O
information	O
in	O
our	O
model	O
is	O
limited	O
to	O
the	O
first	O
order	O
.	O

Note	O
that	O
our	O
graphs	O
are	O
typically	O
densely	O
connected	O
.	O

We	O
now	O
introduce	O
a	O
form	O
of	O
attention	O
into	O
the	O
model	O
,	O
which	O
constitutes	O
an	O
essential	O
part	O
of	O
the	O
model	O
.	O

The	O
motivation	O
is	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
to	O
identify	O
parts	O
of	O
the	O
input	O
data	O
most	O
relevant	O
to	O
produce	O
the	O
answer	O
and	O
(	O
2	O
)	O
to	O
align	O
specific	O
words	O
in	O
the	O
question	O
with	O
particular	O
elements	O
of	O
the	O
scene	O
.	O

Practically	O
,	O
we	O
estimate	O
the	O
relevance	O
of	O
each	O
possible	O
pairwise	O
combination	O
of	O
words	O
and	O
objects	O
.	O

More	O
precisely	O
,	O
we	O
compute	O
scalar	O
“	O
matching	O
weights	O
”	O
between	O
node	O
sets	O
and	O
.	O

These	O
weights	O
are	O
comparable	O
to	O
the	O
“	O
attention	O
weights	O
”	O
in	O
other	O
models	O
(	O
e.g	O
.	O

)	O
.	O

Therefore	O
,	O
:	O
where	O
and	O
are	O
learned	O
weights	O
and	O
biases	O
,	O
and	O
the	O
logistic	Method
function	Method
that	O
introduces	O
a	O
non	O
-	O
linearity	O
and	O
bounds	O
the	O
weights	O
to	O
.	O

The	O
formulation	O
is	O
similar	O
to	O
a	O
cosine	O
similarity	O
with	O
learned	O
weights	O
on	O
the	O
feature	O
dimensions	O
.	O

Note	O
that	O
the	O
weights	O
are	O
computed	O
using	O
the	O
initial	O
embedding	O
of	O
the	O
node	O
features	O
(	O
pre	Method
-	Method
GRU	Method
)	O
.	O

We	O
apply	O
the	O
scalar	O
weights	O
to	O
the	O
corresponding	O
pairwise	O
combinations	O
of	O
question	O
and	O
scene	O
features	O
,	O
thereby	O
focusing	O
and	O
giving	O
more	O
importance	O
to	O
the	O
matched	O
pairs	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
sum	O
the	O
weighted	O
features	O
over	O
the	O
scene	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
then	O
over	O
the	O
question	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
,	O
interleaving	O
the	O
sums	O
with	O
affine	O
projections	O
and	O
non	O
-	O
linearities	O
to	O
obtain	O
a	O
final	O
prediction	Task
:	O
with	O
,	O
,	O
,	O
learned	O
weights	O
and	O
biases	O
,	O
a	O
ReLU	Method
,	O
and	O
a	O
softmax	Method
or	O
a	O
logistic	Method
function	Method
(	O
see	O
experiments	O
,	O
Section	O
[	O
reference	O
]	O
)	O
.	O

The	O
summations	O
over	O
the	O
scene	O
elements	O
and	O
question	O
elements	O
is	O
a	O
form	O
of	O
pooling	Method
that	O
brings	O
the	O
variable	O
number	O
of	O
features	O
(	O
due	O
to	O
the	O
variable	O
number	O
of	O
words	O
and	O
objects	O
in	O
the	O
input	O
)	O
to	O
a	O
fixed	O
-	O
size	O
output	O
.	O

The	O
final	O
output	O
vector	O
contains	O
scores	O
for	O
the	O
possible	O
answers	O
,	O
and	O
has	O
a	O
number	O
of	O
dimensions	O
equal	O
to	O
2	O
for	O
the	O
binary	O
questions	O
of	O
the	O
“	O
balanced	O
”	O
dataset	O
,	O
or	O
to	O
the	O
number	O
of	O
all	O
candidate	O
answers	O
in	O
the	O
“	O
abstract	Material
scenes	Material
”	Material
dataset	Material
.	O

The	O
candidate	O
answers	O
are	O
those	O
appearing	O
at	O
least	O
times	O
in	O
the	O
training	O
set	O
(	O
see	O
supplementary	O
material	O
for	O
details	O
)	O
.	O

section	O
:	O
Evaluation	O
paragraph	O
:	O
Datasets	O
Our	O
evaluation	O
uses	O
two	O
datasets	O
:	O
the	O
original	O
“	O
abstract	O
scenes	O
”	O
from	O
Antol	O
et	O
al	O
.	O

and	O
its	O
“	O
balanced	O
”	O
extension	O
from	O
.	O

They	O
both	O
contain	O
scenes	O
created	O
by	O
humans	O
in	O
a	O
drag	O
-	O
and	O
-	O
drop	O
interface	O
for	O
arranging	O
clip	O
art	O
objects	O
and	O
figures	O
.	O

The	O
original	O
dataset	O
contains	O
scenes	O
(	O
for	O
training	O
validation	O
test	O
respectively	O
)	O
and	O
questions	O
,	O
each	O
with	O
10	O
human	O
-	O
provided	O
ground	O
-	O
truth	O
answers	O
.	O

Questions	O
are	O
categorized	O
based	O
on	O
the	O
type	O
of	O
the	O
correct	O
answer	O
into	O
yes	O
/	O
no	O
,	O
number	O
,	O
and	O
other	O
,	O
but	O
the	O
same	O
method	O
is	O
used	O
for	O
all	O
categories	O
,	O
the	O
type	O
of	O
the	O
test	O
questions	O
being	O
unknown	O
.	O

The	O
“	O
balanced	O
”	O
version	O
of	O
the	O
dataset	O
contains	O
only	O
the	O
subset	O
of	O
questions	O
which	O
have	O
binary	O
(	O
yes	O
/	O
no	O
)	O
answers	O
and	O
,	O
in	O
addition	O
,	O
complementary	O
scenes	O
created	O
to	O
elicit	O
the	O
opposite	O
answer	O
to	O
each	O
question	O
.	O

This	O
is	O
significant	O
because	O
guessing	O
the	O
modal	O
answer	O
from	O
the	O
training	O
set	O
will	O
the	O
succeed	O
only	O
half	O
of	O
the	O
time	O
(	O
slightly	O
more	O
than	O
in	O
practice	O
because	O
of	O
disagreement	O
between	O
annotators	O
)	O
and	O
give	O
accuracy	Metric
over	O
complementary	O
pairs	O
.	O

This	O
contrasts	O
with	O
other	O
VQA	Task
datasets	O
where	O
blind	Task
guessing	Task
can	O
be	O
very	O
effective	O
.	O

The	O
pairs	O
of	O
complementary	O
scenes	O
also	O
typically	O
differ	O
by	O
only	O
one	O
or	O
two	O
objects	O
being	O
displaced	O
,	O
removed	O
,	O
or	O
slightly	O
modified	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
bottom	O
rows	O
)	O
.	O

This	O
makes	O
the	O
questions	O
very	O
challenging	O
by	O
requiring	O
to	O
take	O
into	O
account	O
subtle	O
details	O
of	O
the	O
scenes	O
.	O

paragraph	O
:	O
Metrics	O
The	O
main	O
metric	O
is	O
the	O
average	O
“	O
VQA	Task
score	O
”	O
,	O
which	O
is	O
a	O
soft	Metric
accuracy	Metric
that	O
takes	O
into	O
account	O
variability	O
of	O
ground	O
truth	O
answers	O
from	O
multiple	O
human	O
annotators	O
.	O

Let	O
us	O
refer	O
to	O
a	O
test	O
question	O
by	O
an	O
index	O
,	O
and	O
to	O
each	O
possible	O
answer	O
in	O
the	O
output	O
vocabulary	O
by	O
an	O
index	O
.	O

The	O
ground	Metric
truth	Metric
score	Metric
if	O
the	O
answer	O
was	O
provided	O
by	O
annotators	O
.	O

Otherwise	O
,	O
.	O

Our	O
method	O
outputs	O
a	O
predicted	O
score	O
for	O
each	O
question	O
and	O
answer	O
(	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
overall	O
accuracy	Metric
is	O
the	O
average	Metric
ground	Metric
truth	Metric
score	Metric
of	O
the	O
highest	O
prediction	O
per	O
question	O
,	O
i.e	O
.	O

.	O

It	O
has	O
been	O
argued	O
that	O
the	O
“	O
balanced	O
”	O
dataset	O
can	O
better	O
evaluate	O
a	O
method	O
’s	O
level	O
of	O
visual	Task
understanding	Task
than	O
other	O
datasets	O
,	O
because	O
it	O
is	O
less	O
susceptible	O
to	O
the	O
use	O
of	O
language	O
priors	O
and	O
dataset	O
regularities	O
(	O
i.e	O
.	O

guessing	O
from	O
the	O
question	O
)	O
.	O

Our	O
initial	O
experiments	O
confirmed	O
that	O
the	O
performances	O
of	O
various	O
algorithms	O
on	O
the	O
balanced	O
dataset	O
were	O
indeed	O
better	O
separated	O
,	O
and	O
we	O
used	O
it	O
for	O
our	O
ablative	Task
analysis	Task
.	O

We	O
also	O
focus	O
on	O
the	O
hardest	O
evaluation	Metric
setting	Metric
,	O
which	O
measures	O
the	O
accuracy	Metric
over	O
pairs	O
of	O
complementary	O
scenes	O
.	O

This	O
is	O
the	O
only	O
metric	O
in	O
which	O
blind	Method
models	Method
(	O
guessing	O
from	O
the	O
question	O
)	O
obtain	O
null	Metric
accuracy	Metric
.	O

This	O
setting	O
also	O
does	O
not	O
consider	O
pairs	O
of	O
test	O
scenes	O
deemed	O
ambiguous	O
because	O
of	O
disagreement	O
between	O
annotators	O
.	O

Each	O
test	O
scene	O
is	O
still	O
evaluated	O
independently	O
however	O
,	O
so	O
the	O
model	O
is	O
unable	O
to	O
increase	O
performance	O
by	O
forcing	O
opposite	O
answers	O
to	O
pairs	O
of	O
questions	O
.	O

The	O
metric	O
is	O
then	O
a	O
standard	O
“	O
hard	Metric
”	Metric
accuracy	Metric
,	O
i.e	O
.	O

all	O
ground	Metric
truth	Metric
scores	Metric
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
additional	O
details	O
.	O

subsection	O
:	O
Evaluation	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
We	O
compare	O
our	O
method	O
against	O
the	O
three	O
models	O
proposed	O
in	O
.	O

They	O
all	O
use	O
an	O
ensemble	Method
of	Method
models	Method
exploiting	O
either	O
an	O
LSTM	Method
for	O
processing	O
the	O
question	O
,	O
or	O
an	O
elaborate	O
set	O
of	O
hand	O
-	O
designed	O
rules	O
to	O
identify	O
two	O
objects	O
as	O
the	O
focus	O
of	O
the	O
question	O
.	O

The	O
visual	O
features	O
in	O
the	O
three	O
models	O
are	O
respectively	O
empty	O
(	O
blind	Method
model	Method
)	O
,	O
global	O
(	O
scene	O
-	O
wide	O
)	O
,	O
or	O
focused	O
on	O
the	O
two	O
objects	O
identified	O
from	O
the	O
question	O
.	O

These	O
models	O
are	O
specifically	O
designed	O
for	O
binary	Task
questions	Task
,	O
whereas	O
ours	O
is	O
generally	O
applicable	O
.	O

Nevertheless	O
,	O
we	O
obtain	O
significantly	O
better	O
accuracy	Metric
than	O
all	O
three	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Differences	O
in	O
performance	O
are	O
mostly	O
visible	O
in	O
the	O
“	O
pairs	O
”	O
setting	O
,	O
which	O
we	O
believe	O
is	O
more	O
reliable	O
as	O
it	O
discards	O
ambiguous	O
test	O
questions	O
on	O
which	O
human	O
annotators	O
disagreed	O
.	O

During	O
training	O
,	O
we	O
take	O
care	O
to	O
keep	O
pairs	O
of	O
complementary	O
scenes	O
together	O
when	O
forming	O
mini	O
-	O
batches	O
.	O

This	O
has	O
a	O
significant	O
positive	O
effect	O
on	O
the	O
stability	Metric
of	O
the	O
optimization	Task
.	O

Interestingly	O
,	O
we	O
did	O
not	O
notice	O
any	O
tendency	O
toward	O
overfitting	O
when	O
training	O
on	O
balanced	O
scenes	O
.	O

We	O
hypothesize	O
that	O
the	O
pairs	O
of	O
complementary	O
scenes	O
have	O
a	O
strong	O
regularizing	O
effect	O
that	O
force	O
the	O
learned	O
model	O
to	O
focus	O
on	O
relevant	O
details	O
of	O
the	O
scenes	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
and	O
in	O
the	O
supplementary	O
material	O
)	O
,	O
we	O
visualize	O
the	O
matching	O
weights	O
between	O
question	O
words	O
and	O
scene	O
objects	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

As	O
expected	O
,	O
these	O
tend	O
to	O
be	O
larger	O
between	O
semantically	O
related	O
elements	O
(	O
e.g	O
.	O

daytime	O
sun	O
,	O
dog	O
puppy	O
,	O
boy	O
human	O
)	O
although	O
some	O
are	O
more	O
difficult	O
to	O
interpret	O
.	O

Our	O
best	O
performance	O
of	O
about	O
is	O
still	O
low	O
in	O
absolute	O
terms	O
,	O
which	O
is	O
understandable	O
from	O
the	O
wide	O
range	O
of	O
concepts	O
involved	O
in	O
the	O
questions	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
in	O
the	O
supplementary	O
material	O
)	O
.	O

It	O
seems	O
unlikely	O
that	O
these	O
concepts	O
could	O
be	O
learned	O
from	O
training	O
question	O
/	O
answers	O
alone	O
,	O
and	O
we	O
suggest	O
that	O
any	O
further	O
significant	O
improvement	O
in	O
performance	O
will	O
require	O
external	O
sources	O
of	O
information	O
at	O
training	O
and	O
/	O
or	O
test	O
time	O
.	O

paragraph	O
:	O
Ablative	Task
evaluation	Task
We	O
evaluated	O
variants	O
of	O
our	O
model	O
to	O
measure	O
the	O
impact	O
of	O
various	O
design	O
choices	O
(	O
see	O
numbered	O
rows	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

On	O
the	O
question	O
side	O
,	O
we	O
evaluate	O
(	O
row	O
1	O
)	O
our	O
graph	Method
approach	Method
without	O
syntactic	Method
parsing	Method
,	O
building	O
question	Method
graphs	Method
with	O
only	O
two	O
types	O
of	O
edges	O
,	O
previous	O
/	O
next	O
and	O
linking	O
consecutive	O
nodes	O
.	O

This	O
shows	O
the	O
advantage	O
of	O
using	O
the	O
graph	Method
method	Method
together	O
with	O
syntactic	Method
parsing	Method
.	O

Optimizing	O
the	O
word	O
embeddings	O
from	O
scratch	O
(	O
row	O
2	O
)	O
rather	O
than	O
from	O
pretrained	O
Glove	O
vectors	O
produces	O
a	O
significant	O
drop	O
in	O
performance	O
.	O

On	O
the	O
scene	O
side	O
,	O
we	O
removed	O
the	O
edge	O
features	O
(	O
row	O
3	O
)	O
by	O
setting	O
.	O

It	O
confirms	O
that	O
the	O
model	O
makes	O
use	O
of	O
the	O
spatial	O
relations	O
between	O
objects	O
encoded	O
by	O
the	O
edges	O
of	O
the	O
graph	O
.	O

In	O
rows	O
4–6	O
,	O
we	O
disabled	O
the	O
recurrent	Method
graph	Method
processing	Method
(	O
)	O
for	O
the	O
either	O
the	O
question	O
,	O
the	O
scene	O
,	O
or	O
both	O
.	O

We	O
finally	O
tested	O
the	O
model	O
with	O
uniform	O
matching	O
weights	O
(	O
,	O
row	O
10	O
)	O
.	O

As	O
expected	O
,	O
it	O
performed	O
poorly	O
.	O

Our	O
weights	O
act	O
similarly	O
to	O
the	O
attention	Method
mechanisms	Method
in	O
other	O
models	O
(	O
e.g	O
.	O

)	O
and	O
our	O
observations	O
confirm	O
that	O
such	O
mechanisms	O
are	O
crucial	O
for	O
good	O
performance	O
.	O

paragraph	O
:	O
Precision	Metric
/	O
recall	Metric
We	O
are	O
interested	O
in	O
assessing	O
the	O
confidence	O
of	O
our	O
model	O
in	O
its	O
predicted	O
answers	O
.	O

Most	O
existing	O
VQA	Task
methods	O
treat	O
the	O
answering	Task
as	O
a	O
hard	O
classification	Task
over	O
candidate	O
answers	O
,	O
and	O
almost	O
all	O
reported	O
results	O
consist	O
of	O
a	O
single	O
accuracy	Metric
metric	Metric
.	O

To	O
provide	O
more	O
insight	O
,	O
we	O
produce	O
precision	Metric
/	Metric
recall	Metric
curves	Metric
for	O
predicted	O
answers	O
.	O

A	O
precision	Metric
/	Metric
recall	Metric
point	Metric
is	O
obtained	O
by	O
setting	O
a	O
threshold	O
on	O
predicted	O
scores	O
such	O
that	O
where	O
is	O
the	O
indicator	O
function	O
.	O

We	O
plot	O
precision	Metric
/	Metric
recall	Metric
curves	Metric
in	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
both	O
datasets	O
.	O

The	O
predicted	Metric
score	Metric
proves	O
to	O
be	O
a	O
reliable	O
indicator	O
of	O
the	O
model	Metric
confidence	Metric
,	O
as	O
a	O
low	O
threshold	O
can	O
achieve	O
near	O
-	O
perfect	O
accuracy	Metric
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
left	O
and	O
middle	O
)	O
by	O
filtering	O
out	O
harder	O
and	O
/	O
or	O
ambiguous	O
test	O
cases	O
.	O

We	O
compare	O
models	O
trained	O
with	O
either	O
a	O
softmax	O
or	O
a	O
sigmoid	O
as	O
the	O
final	O
non	O
-	O
linearity	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
common	O
practice	O
is	O
to	O
train	O
the	O
softmax	Method
for	O
a	O
hard	Task
classification	Task
objective	Task
,	O
using	O
a	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
and	O
the	O
answer	O
of	O
highest	O
ground	O
truth	O
score	O
as	O
the	O
target	O
.	O

In	O
an	O
attempt	O
to	O
make	O
better	O
use	O
of	O
the	O
multiple	O
human	O
-	O
provided	O
answers	O
,	O
we	O
propose	O
to	O
use	O
the	O
soft	O
ground	O
truth	O
scores	O
as	O
the	O
target	O
with	O
a	O
logarithmic	O
loss	O
.	O

This	O
shows	O
an	O
advantage	O
on	O
the	O
“	O
abstract	Material
scenes	Material
”	Material
dataset	Material
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
left	O
and	O
middle	O
)	O
.	O

In	O
that	O
dataset	O
,	O
the	O
soft	O
target	O
scores	O
reflect	O
frequent	O
ambiguities	O
in	O
the	O
questions	O
and	O
the	O
scenes	O
,	O
and	O
when	O
synonyms	O
constitute	O
multiple	O
acceptable	O
answers	O
.	O

In	O
those	O
cases	O
,	O
we	O
can	O
avoid	O
the	O
potential	O
confusion	O
induced	O
by	O
a	O
hard	O
classification	O
for	O
one	O
specific	O
answer	O
.	O

The	O
“	O
balanced	O
”	O
dataset	O
,	O
by	O
nature	O
,	O
contains	O
almost	O
no	O
such	O
ambiguities	O
,	O
and	O
there	O
is	O
no	O
significant	O
difference	O
between	O
the	O
different	O
training	O
objectives	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
right	O
)	O
.	O

paragraph	O
:	O
Effect	O
of	O
training	O
set	O
size	O
Our	O
motivation	O
for	O
introducing	O
language	Task
parsing	Task
and	O
pretrained	Task
word	Task
embeddings	Task
is	O
to	O
better	O
generalize	O
the	O
concepts	O
learned	O
from	O
the	O
limited	O
training	O
examples	O
.	O

Words	O
representing	O
semantically	O
close	O
concepts	O
ideally	O
get	O
assigned	O
close	O
word	O
embeddings	O
.	O

Similarly	O
,	O
paraphrases	O
of	O
similar	O
questions	O
should	O
produce	O
parse	O
graphs	O
with	O
more	O
similarities	O
than	O
a	O
simple	O
concatenation	O
of	O
words	O
would	O
reveal	O
(	O
as	O
in	O
the	O
input	O
to	O
traditional	O
LSTMs	Method
)	O
.	O

We	O
trained	O
our	O
model	O
with	O
limited	O
subsets	O
of	O
the	O
training	O
data	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Unsurprisingly	O
,	O
the	O
performance	O
grows	O
steadily	O
with	O
the	O
amount	O
of	O
training	O
data	O
,	O
which	O
suggests	O
that	O
larger	O
datasets	O
would	O
improve	O
performance	O
.	O

In	O
our	O
opinion	O
however	O
,	O
it	O
seems	O
unlikely	O
that	O
sufficient	O
data	O
,	O
covering	O
all	O
possible	O
concepts	O
,	O
could	O
be	O
collected	O
in	O
the	O
form	O
of	O
question	O
/	O
answer	O
examples	O
.	O

More	O
data	O
can	O
however	O
be	O
brought	O
in	O
with	O
other	O
sources	O
of	O
information	O
and	O
supervision	O
.	O

Our	O
use	O
of	O
parsing	Method
and	O
word	Method
embeddings	Method
is	O
a	O
small	O
step	O
in	O
that	O
direction	O
.	O

Both	O
techniques	O
clearly	O
improve	O
generalization	Task
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
effect	O
may	O
be	O
particularly	O
visible	O
in	O
our	O
case	O
because	O
of	O
the	O
relatively	O
small	O
number	O
of	O
training	O
examples	O
(	O
about	O
k	O
questions	O
in	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
.	O

It	O
is	O
unclear	O
whether	O
huge	O
VQA	Task
datasets	O
could	O
ultimately	O
negate	O
this	O
advantage	O
.	O

Future	O
experiments	O
on	O
larger	O
datasets	O
(	O
e.g	O
.	O

)	O
may	O
answer	O
this	O
question	O
.	O

subsection	O
:	O
Evaluation	O
on	O
the	O
“	O
abstract	Material
scenes	Material
”	Material
dataset	Material
We	O
report	O
our	O
results	O
on	O
the	O
original	O
“	O
abstract	Material
scenes	Material
”	Material
dataset	Material
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
evaluation	O
is	O
performed	O
on	O
an	O
automated	O
server	O
that	O
does	O
not	O
allow	O
for	O
an	O
extensive	O
ablative	Task
analysis	Task
.	O

Anecdotally	O
,	O
performance	O
on	O
the	O
validation	O
set	O
corroborates	O
all	O
findings	O
presented	O
above	O
,	O
in	O
particular	O
the	O
strong	O
benefit	O
of	O
pre	Method
-	Method
parsing	Method
,	O
pretrained	Method
word	Method
embeddings	Method
,	O
and	O
graph	Method
processing	Method
with	O
a	O
GRU	Method
.	O

At	O
the	O
time	O
of	O
our	O
submission	O
,	O
our	O
method	O
occupies	O
the	O
top	O
place	O
on	O
the	O
leader	O
board	O
in	O
both	O
the	O
open	O
-	O
ended	O
and	O
multiple	Task
choice	Task
settings	Task
.	O

The	O
advantage	O
over	O
existing	O
method	O
is	O
most	O
pronounced	O
on	O
the	O
binary	Task
and	Task
the	Task
counting	Task
questions	Task
.	O

Refer	O
to	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
to	O
the	O
supplementary	O
for	O
visualizations	O
of	O
the	O
results	O
.	O

section	O
:	O
Conclusions	O
We	O
presented	O
a	O
deep	Method
neural	Method
network	Method
for	O
visual	Task
question	Task
answering	Task
that	O
processes	O
graph	Task
-	Task
structured	Task
representations	Task
of	Task
scenes	Task
and	Task
questions	Task
.	O

This	O
enables	O
leveraging	O
existing	O
natural	Method
language	Method
processing	Method
tools	Method
,	O
in	O
particular	O
pretrained	Method
word	Method
embeddings	Method
and	O
syntactic	Task
parsing	Task
.	O

The	O
latter	O
showed	O
significant	O
advantage	O
over	O
a	O
traditional	O
sequential	Task
processing	Task
of	Task
the	Task
questions	Task
,	O
e.g	O
.	O

with	O
LSTMs	Method
.	O

In	O
our	O
opinion	O
,	O
VQA	Task
systems	O
are	O
unlikely	O
to	O
learn	O
everything	O
from	O
question	O
/	O
answer	O
examples	O
alone	O
.	O

We	O
believe	O
that	O
any	O
significant	O
improvement	O
in	O
performance	O
will	O
require	O
additional	O
sources	O
of	O
information	O
and	O
supervision	O
.	O

Our	O
explicit	O
processing	O
of	O
the	O
language	O
part	O
is	O
a	O
small	O
step	O
in	O
that	O
direction	O
.	O

It	O
has	O
clearly	O
shown	O
to	O
improve	O
generalization	Task
without	O
resting	O
entirely	O
on	O
VQA	Task
-	O
specific	O
annotations	O
.	O

We	O
have	O
so	O
far	O
applied	O
our	O
method	O
to	O
datasets	O
of	O
clip	O
art	O
scenes	O
.	O

Its	O
direct	O
extension	O
to	O
real	O
images	O
will	O
be	O
addressed	O
in	O
future	O
work	O
,	O
by	O
replacing	O
nodes	O
in	O
the	O
input	O
scene	O
graph	O
with	O
proposals	O
from	O
pretrained	Method
object	Method
detectors	Method
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplementary	O
material	O
appendix	O
:	O
Implementation	O
We	O
provide	O
below	O
practical	O
details	O
of	O
our	O
implementation	O
of	O
the	O
proposed	O
method	O
.	O

Size	O
of	O
vector	O
embeddings	O
of	O
node	O
features	O
,	O
edge	O
features	O
,	O
and	O
all	O
hidden	O
states	O
within	O
the	O
network	O
:	O
=	O
300	O
.	O

Note	O
that	O
smaller	O
values	O
such	O
as	O
=	O
200	O
also	O
give	O
very	O
good	O
results	O
(	O
not	O
reported	O
in	O
this	O
paper	O
)	O
at	O
a	O
fraction	O
of	O
the	O
training	O
time	O
.	O

Number	O
of	O
recurrent	O
iterations	O
to	O
update	O
graph	Method
node	Method
representations	Method
:	O
=	O
=	O
4	O
.	O

Anecdotally	O
,	O
we	O
observed	O
that	O
processing	O
the	O
scene	O
graph	O
benefits	O
from	O
more	O
iterations	O
than	O
the	O
question	O
graph	O
,	O
for	O
which	O
performance	O
nearly	O
saturates	O
with	O
2	O
or	O
more	O
iterations	O
.	O

As	O
reported	O
in	O
the	O
ablative	O
evaluation	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
the	O
use	O
of	O
at	O
least	O
a	O
single	O
iteration	O
has	O
a	O
stronger	O
influence	O
than	O
its	O
exact	O
number	O
.	O

All	O
weights	O
except	O
word	O
embeddings	O
are	O
initialized	O
randomly	O
following	O
.	O

Word	Method
embeddings	Method
are	O
initialized	O
with	O
Glove	O
vectors	O
of	O
dimension	O
300	O
available	O
publicly	O
,	O
trained	O
for	O
6	O
billion	O
words	O
on	O
Wikipedia	Material
and	O
Gigaword	Material
.	O

The	O
word	O
embeddings	O
are	O
fine	O
-	O
tuned	O
with	O
a	O
learning	Metric
rate	Metric
of	O
of	O
the	O
other	O
weights	O
.	O

Dropout	Method
with	O
ratio	O
0.3	O
is	O
applied	O
between	O
the	O
weighted	O
sum	O
over	O
scene	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
final	O
classifier	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Weights	O
are	O
optimized	O
with	O
Adadelta	Method
with	O
mini	O
-	O
batches	O
of	O
128	O
questions	O
.	O

We	O
run	O
optimization	Method
until	O
convergence	O
(	O
typically	O
20	O
epochs	O
on	O
the	O
“	O
abstract	Material
scenes	Material
”	O
,	O
100	O
epochs	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
and	O
report	O
performance	O
on	O
the	O
test	O
set	O
from	O
the	O
epoch	O
with	O
the	O
highest	O
performance	O
on	O
the	O
validation	O
set	O
(	O
measured	O
by	O
VQA	Task
score	O
on	O
the	O
“	O
abstract	Material
scenes	Material
”	Material
dataset	Material
,	O
and	O
accuracy	Metric
over	O
pairs	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
.	O

The	O
edges	O
between	O
word	O
nodes	O
in	O
the	O
input	O
question	O
graph	O
are	O
labeled	O
with	O
the	O
dependency	O
labels	O
identified	O
by	O
the	O
Stanford	Method
parser	Method
.	O

These	O
dependencies	O
are	O
directed	O
,	O
and	O
we	O
supplement	O
all	O
of	O
them	O
with	O
their	O
symmetric	O
,	O
albeit	O
tagged	O
with	O
a	O
different	O
set	O
of	O
labels	O
.	O

The	O
output	O
of	O
the	O
parser	Method
includes	O
the	O
propagation	O
of	O
conjunct	O
dependencies	O
(	O
its	O
default	O
setting	O
)	O
.	O

This	O
yields	O
quite	O
densely	O
connected	O
graphs	O
.	O

The	O
input	O
features	O
of	O
the	O
object	O
nodes	O
are	O
those	O
directly	O
available	O
in	O
the	O
datasets	O
.	O

They	O
represent	O
:	O
the	O
object	O
category	O
(	O
human	O
,	O
animal	O
,	O
small	O
or	O
large	O
object	O
)	O
as	O
one	O
one	O
-	O
hot	O
vector	O
,	O
the	O
object	O
type	O
(	O
table	O
,	O
sun	O
,	O
dog	O
window	O
,	O
…	O
)	O
as	O
a	O
one	O
-	O
hot	O
vector	O
,	O
the	O
expression	O
/	O
pose	O
/	O
type	O
(	O
various	O
depictions	O
being	O
possible	O
for	O
each	O
object	O
type	O
)	O
as	O
a	O
one	O
-	O
hot	O
vector	O
,	O
and	O
10	O
scalar	O
values	O
describing	O
the	O
pose	O
of	O
human	O
figures	O
(	O
the	O
X	O
/	O
Y	O
position	O
of	O
arms	O
,	O
legs	O
,	O
and	O
head	O
relative	O
to	O
the	O
torso	O
)	O
.	O

They	O
form	O
altogether	O
a	O
feature	O
vector	O
of	O
dimension	O
159	O
.	O

The	O
edge	O
features	O
between	O
objects	O
represent	O
:	O
the	O
signed	O
difference	O
in	O
their	O
X	O
/	O
Y	O
position	O
,	O
the	O
inverse	O
of	O
their	O
absolute	O
difference	O
in	O
X	O
/	O
Y	O
position	O
,	O
and	O
their	O
relative	O
position	O
on	O
depth	O
planes	O
as	O
+	O
1	O
if	O
closer	O
(	O
potentially	O
occluding	O
the	O
other	O
)	O
,	O
-	O
1	O
otherwise	O
.	O

All	O
input	O
features	O
are	O
normalized	O
for	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O

When	O
training	O
for	O
the	O
“	O
balanced	O
”	O
dataset	O
,	O
care	O
is	O
taken	O
to	O
keep	O
each	O
pair	O
of	O
complementary	O
scenes	O
in	O
a	O
same	O
mini	O
-	O
batch	O
when	O
shuffling	O
training	O
instances	O
.	O

This	O
has	O
a	O
noticeable	O
effect	O
on	O
the	O
stability	O
of	O
the	O
optimization	Task
.	O

In	O
the	O
open	Task
-	Task
ended	Task
setting	Task
,	O
the	O
output	O
space	O
is	O
made	O
of	O
all	O
answers	O
that	O
appear	O
at	O
least	O
5	O
times	O
in	O
the	O
training	O
set	O
.	O

These	O
correspond	O
to	O
623	O
possible	O
answers	O
,	O
which	O
cover	O
96	O
%	O
of	O
the	O
training	O
questions	O
.	O

Our	O
model	O
was	O
implemented	O
in	O
Matlab	O
from	O
scratch	O
.	O

Training	Task
takes	O
in	O
the	O
order	O
of	O
5	O
to	O
10	O
hours	O
on	O
one	O
CPU	O
,	O
depending	O
on	O
the	O
dataset	O
and	O
on	O
the	O
size	O
of	O
the	O
internal	Method
representations	Method
.	O

appendix	O
:	O
Additional	O
details	O
Why	O
do	O
we	O
choose	O
to	O
focus	O
on	O
abstract	O
scenes	O
?	O
Does	O
this	O
method	O
extend	O
to	O
real	O
images	O
?	O
The	O
balanced	Material
dataset	Material
of	Material
abstract	Material
scenes	Material
was	O
the	O
only	O
one	O
allowing	O
evaluation	O
free	O
from	O
dataset	O
biases	O
.	O

Abstract	Material
scenes	Material
also	O
enabled	O
removing	O
confounding	O
factors	O
(	O
visual	Task
recognition	Task
)	O
.	O

It	O
is	O
not	O
unreasonable	O
to	O
view	O
the	O
scene	O
descriptions	O
(	O
provided	O
with	O
abstract	Material
scenes	Material
)	O
as	O
the	O
output	O
of	O
a	O
“	O
perfect	O
”	O
vision	Method
system	Method
.	O

The	O
proposel	O
model	O
could	O
be	O
extended	O
to	O
real	O
images	O
by	O
building	O
graphs	O
of	O
the	O
images	O
where	O
scene	O
nodes	O
are	O
candidates	O
from	O
an	O
object	Method
detection	Method
algorithm	Method
.	O

The	O
multiple	O
-	O
choice	O
(	O
M.C.	O
)	O
setting	O
should	O
be	O
easier	O
than	O
open	O
-	O
ended	O
(	O
O.E.	O
)	O
.	O

Therefore	O
,	O
why	O
is	O
the	O
accuracy	Metric
not	O
better	O
for	O
binary	O
and	O
number	O
questions	O
in	O
the	O
M.C	O
setting	O
(	O
rather	O
than	O
O.E.	O
)	O
?	O
This	O
intuition	O
is	O
incorrect	O
in	O
practice	O
.	O

The	O
wording	O
of	O
binary	O
and	O
number	O
questions	O
(	O
“	O
How	O
many	O
…	O
”	O
)	O
can	O
easily	O
narrow	O
down	O
the	O
set	O
of	O
possible	O
answers	O
,	O
whether	O
evaluated	O
in	O
a	O
M.C.	O
or	O
O.E.	O
setting	O
.	O

One	O
thus	O
can	O
not	O
qualify	O
one	O
as	O
strictly	O
easier	O
than	O
the	O
other	O
.	O

Other	O
factors	O
can	O
then	O
influence	O
the	O
performance	O
either	O
way	O
.	O

Note	O
also	O
that	O
,	O
for	O
example	O
that	O
most	O
choices	O
of	O
number	O
questions	O
are	O
not	O
numbers	O
.	O

In	O
Table	O
1	O
,	O
why	O
is	O
there	O
a	O
large	O
improvement	O
of	O
the	O
metric	O
over	O
balanced	O
pairs	O
of	O
scenes	O
,	O
but	O
not	O
of	O
the	O
metric	O
over	O
individual	O
scenes	O
?	O
The	O
metric	O
over	O
pairs	O
is	O
much	O
harder	O
to	O
satisfy	O
and	O
should	O
be	O
regarded	O
as	O
more	O
meaningful	O
.	O

The	O
other	O
metric	O
(	O
over	O
scenes	O
)	O
essentially	O
saturates	O
at	O
the	O
same	O
point	O
between	O
the	O
two	O
methods	O
.	O

How	O
are	O
precison	Metric
/	Metric
recall	Metric
curves	Metric
helping	O
better	O
understand	O
model	O
compared	O
to	O
a	O
simple	O
accuracy	Metric
number	Metric
?	O
A	O
P	O
/	O
R	O
curve	O
shows	O
the	O
confidence	O
of	O
the	O
model	O
in	O
its	O
answers	O
.	O

A	O
practical	O
VQA	Task
system	O
will	O
need	O
to	O
provide	O
an	O
indication	O
of	O
certainty	O
,	O
including	O
the	O
possibility	O
of	O
“	O
I	O
do	O
n’t	O
know	O
”	O
.	O

Reporting	O
P	O
/	O
R	O
is	O
a	O
step	O
in	O
that	O
direction	O
.	O

P	O
/	O
R	O
curves	O
also	O
contain	O
more	O
information	O
and	O
can	O
show	O
differences	O
between	O
methods	O
(	O
e.g	O
.	O

Fig.3	O
left	O
)	O
that	O
may	O
otherwise	O
not	O
be	O
appreciable	O
through	O
an	O
aggregate	Metric
metric	Metric
.	O

Why	O
is	O
attention	O
computed	O
with	O
pre	O
-	O
GRU	O
node	O
features	O
?	O
This	O
performed	O
slightly	O
better	O
than	O
the	O
alternative	O
.	O

The	O
intuition	O
is	O
that	O
the	O
identity	O
of	O
each	O
node	O
is	O
sufficient	O
,	O
and	O
the	O
context	O
(	O
transfered	O
by	O
the	O
GRU	Method
from	O
neighbouring	O
nodes	O
)	O
is	O
probably	O
less	O
useful	O
to	O
compute	O
attention	O
.	O

Why	O
are	O
the	O
largest	O
performance	O
gains	O
obtained	O
with	O
“	O
number	O
”	O
questions	O
?	O
We	O
could	O
not	O
draw	O
definitive	O
conclusions	O
.	O

Competing	Method
methods	Method
seem	O
to	O
rely	O
on	O
dataset	O
biases	O
(	O
predominance	O
of	O
2	O
and	O
3	O
as	O
answers	O
)	O
.	O

Ours	O
was	O
developed	O
(	O
cross	O
-	O
validated	O
)	O
for	O
the	O
balanced	O
dataset	O
,	O
which	O
requires	O
not	O
to	O
rely	O
on	O
such	O
biases	O
,	O
and	O
may	O
simply	O
be	O
better	O
at	O
utilizing	O
the	O
input	O
and	O
not	O
biases	O
.	O

This	O
may	O
in	O
turn	O
explain	O
minimal	O
gains	O
on	O
other	O
questions	O
,	O
which	O
could	O
benefit	O
from	O
using	O
biases	O
(	O
because	O
of	O
a	O
larger	O
pool	O
of	O
reasonable	O
answers	O
)	O
.	O

appendix	O
:	O
Additional	O
results	O
We	O
provide	O
below	O
additional	O
example	O
results	O
in	O
the	O
same	O
format	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Additional	O
results	O
:	O
abstract	O
scenes	O
dataset	O
subsection	O
:	O
Additional	O
results	O
:	O
balanced	O
dataset	O
Photographic	Task
Image	Task
Synthesis	Task
with	O
Cascaded	Method
Refinement	Method
Networks	Method
section	O
:	O
The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	Task
photographic	Task
images	Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	Method
label	Method
map	Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	Method
engine	Method
that	O
takes	O
a	O
two	O
-	O
dimensional	Task
semantic	Task
specification	Task
of	Task
the	Task
scene	Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	Method
training	Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	Method
network	Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	Method
regression	Method
objective	Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
†	O
Intel	O
Labs	O
‡	O
Stanford	O
University	O
demonstrate	O
this	O
by	O
synthesizing	O
photographic	O
images	O
at	O
2	O
-	O
megapixel	O
resolution	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Extensive	O
perceptual	O
experiments	O
on	O
datasets	O
of	O
outdoor	O
and	O
indoor	O
scenes	O
demonstrate	O
that	O
images	O
synthesized	O
by	O
the	O
presented	O
approach	O
are	O
considerably	O
more	O
realistic	O
than	O
alternative	O
approaches	O
.	O

section	O
:	O
.	O

Given	O
a	O
pixelwise	Method
semantic	Method
layout	Method
,	O
the	O
presented	O
model	O
synthesizes	O
an	O
image	O
that	O
conforms	O
to	O
this	O
layout	O
.	O

(	O
a	O
)	O
Semantic	O
layouts	O
from	O
the	O
Cityscapes	Material
dataset	Material
of	Material
urban	Material
scenes	Material
;	O
semantic	O
classes	O
are	O
coded	O
by	O
color	O
.	O

(	O
b	O
)	O
Images	O
synthesized	O
by	O
our	O
model	O
for	O
these	O
layouts	O
.	O

The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	Task
photographic	Task
images	Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	Method
label	Method
map	Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	Method
engine	Method
that	O
takes	O
a	O
two	O
-	O
dimensional	Task
semantic	Task
specification	Task
of	Task
the	Task
scene	Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	Method
training	Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	Method
network	Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	Method
regression	Method
objective	Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
section	O
:	O
Introduction	O
Consider	O
the	O
semantic	O
layouts	O
in	O
Figure	O
1	O
.	O

A	O
skilled	O
painter	O
could	O
draw	O
images	O
that	O
depict	O
urban	O
scenes	O
that	O
conform	O
to	O
these	O
layouts	O
.	O

Highly	O
trained	O
craftsmen	O
can	O
even	O
create	O
paintings	O
that	O
approach	O
photorealism	O
[	O
reference	O
]	O
.	O

Can	O
we	O
train	O
computational	Method
models	Method
that	O
have	O
this	O
ability	O
?	O
Given	O
a	O
semantic	O
layout	O
of	O
a	O
novel	O
scene	O
,	O
can	O
an	O
artificial	Method
system	Method
synthesize	O
an	O
image	O
that	O
depicts	O
this	O
scene	O
and	O
looks	O
like	O
a	O
photograph	O
?	O
This	O
question	O
is	O
connected	O
to	O
central	O
problems	O
in	O
computer	Task
graphics	Task
and	O
artificial	Task
intelligence	Task
.	O

First	O
,	O
consider	O
the	O
problem	O
of	O
photorealism	Task
in	Task
computer	Task
graphics	Task
.	O

A	O
system	O
that	O
synthesizes	O
photorealistic	O
images	O
from	O
semantic	O
layouts	O
would	O
in	O
effect	O
function	O
as	O
a	O
kind	O
of	O
rendering	Method
engine	Method
that	O
bypasses	O
the	O
laborious	O
specification	O
of	O
detailed	O
threedimensional	O
geometry	O
and	O
surface	O
reflectance	O
distributions	O
,	O
and	O
avoids	O
computationally	O
intensive	O
light	Method
transport	Method
simulation	Method
[	O
reference	O
]	O
.	O

A	O
direct	Method
synthesis	Method
approach	Method
could	O
not	O
immediately	O
replace	O
modern	O
rendering	Method
engines	Method
,	O
but	O
would	O
indicate	O
that	O
an	O
alternative	O
route	O
to	O
photorealism	Task
may	O
be	O
viable	O
and	O
could	O
some	O
day	O
complement	O
existing	O
computer	Method
graphics	Method
techniques	Method
.	O

Our	O
second	O
source	O
of	O
motivation	O
is	O
the	O
role	O
of	O
mental	O
imagery	O
and	O
simulation	Task
in	O
human	Task
cognition	Task
[	O
reference	O
]	O
.	O

Mental	Task
imagery	Task
is	O
believed	O
to	O
play	O
an	O
important	O
role	O
in	O
planning	Task
and	Task
decision	Task
making	Task
.	O

The	O
level	O
of	O
detail	O
and	O
completeness	O
of	O
mental	O
imagery	O
is	O
a	O
matter	O
of	O
debate	O
,	O
but	O
its	O
role	O
in	O
human	Task
intelligence	Task
suggests	O
that	O
the	O
ability	O
to	O
synthesize	O
photorealistic	O
images	O
may	O
support	O
the	O
development	O
of	O
artificial	Method
intelligent	Method
systems	Method
[	O
reference	O
]	O
.	O

In	O
this	O
work	O
,	O
we	O
develop	O
a	O
model	O
for	O
photographic	Task
image	Task
synthesis	Task
from	O
pixelwise	Task
semantic	Task
layouts	Task
.	O

Our	O
model	O
is	O
a	O
convolutional	Method
network	Method
,	O
trained	O
in	O
a	O
supervised	Method
fashion	Method
on	O
pairs	O
of	O
photographs	O
and	O
corresponding	O
semantic	O
layouts	O
.	O

Such	O
pairs	O
are	O
provided	O
with	O
semantic	O
segmentation	O
datasets	O
[	O
reference	O
]	O
.	O

We	O
use	O
them	O
not	O
to	O
infer	O
semantic	O
layouts	O
from	O
photographs	O
,	O
but	O
to	O
synthesize	O
photographs	O
from	O
semantic	O
layouts	O
.	O

In	O
this	O
sense	O
our	O
problem	O
is	O
the	O
inverse	Task
of	Task
semantic	Task
segmentation	Task
.	O

Images	O
synthesized	O
by	O
our	O
model	O
are	O
shown	O
in	O
Figure	O
1	O
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
directly	O
by	O
a	O
single	O
feedforward	Method
convolutional	Method
network	Method
trained	O
to	O
minimize	O
a	O
regression	O
loss	O
.	O

This	O
departs	O
from	O
much	O
recent	O
and	O
contemporaneous	O
work	O
,	O
which	O
uses	O
adversarial	Method
training	Method
of	Method
generator	Method
-	Method
discriminator	Method
dyads	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
show	O
that	O
direct	O
supervised	Method
training	Method
of	O
a	O
single	Method
convolutional	Method
network	Method
can	O
yield	O
photographic	O
images	O
.	O

This	O
bypasses	O
adversarial	Method
training	Method
,	O
which	O
is	O
known	O
to	O
be	O
"	O
massively	O
unstable	O
"	O
[	O
reference	O
]	O
.	O

Furthermore	O
,	O
the	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
image	O
resolutions	O
.	O

We	O
synthesize	O
images	O
with	O
resolution	O
up	O
to	O
2	O
megapixels	O
(	O
1024×2048	O
)	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Doubling	O
the	O
output	O
resolution	O
and	O
generating	O
appropriate	O
details	O
at	O
that	O
resolution	O
amounts	O
to	O
adding	O
a	O
single	O
module	O
to	O
our	O
end	O
-	O
to	O
-	O
end	Method
model	Method
.	O

We	O
conduct	O
careful	O
perceptual	O
experiments	O
using	O
the	O
Amazon	Method
Mechanical	Method
Turk	Method
platform	O
,	O
comparing	O
the	O
presented	O
approach	O
to	O
a	O
range	O
of	O
baselines	O
.	O

These	O
experiments	O
clearly	O
indicate	O
that	O
images	O
synthesized	O
by	O
our	O
model	O
are	O
significantly	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
alternative	O
approaches	O
.	O

section	O
:	O
Related	O
Work	O
The	O
most	O
prominent	O
contemporary	O
approach	O
to	O
image	Task
synthesis	Task
is	O
based	O
on	O
generative	Method
adversarial	Method
networks	Method
(	O
GANs	Method
)	O
[	O
reference	O
]	O
.	O

In	O
the	O
original	O
work	O
of	O
Goodfellow	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
GANs	Method
were	O
used	O
to	O
synthesize	O
MNIST	Material
digits	Material
and	O
32	O
×	O
32	O
images	O
that	O
aimed	O
to	O
reproduce	O
the	O
appearance	O
of	O
different	O
classes	O
in	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
training	O
multiple	O
separate	O
GANs	Method
,	O
one	O
for	O
each	O
level	O
in	O
a	O
Laplacian	O
pyramid	O
.	O

Each	O
model	O
is	O
trained	O
independently	O
to	O
synthesize	O
details	O
at	O
its	O
scale	O
.	O

Assembling	O
separatelytrained	Method
models	Method
in	O
this	O
fashion	O
enabled	O
the	O
authors	O
to	O
synthesize	O
smoother	O
images	O
and	O
to	O
push	O
resolution	O
up	O
to	O
96×96	O
.	O

This	O
work	O
is	O
an	O
important	O
precursor	O
to	O
ours	O
in	O
that	O
multiscale	Task
refinement	Task
is	O
a	O
central	O
characteristic	O
of	O
our	O
approach	O
.	O

Key	O
differences	O
are	O
that	O
we	O
train	O
a	O
single	O
model	O
end	O
-	O
to	O
-	O
end	O
to	O
directly	O
synthesize	O
the	O
output	O
image	O
,	O
and	O
that	O
no	O
adversarial	Method
training	Method
is	O
used	O
.	O

Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
remark	O
that	O
"	O
Historical	O
attempts	O
to	O
scale	O
up	O
GANs	Method
using	O
CNNs	Method
to	O
model	O
images	O
have	O
been	O
unsuccessful	O
"	O
and	O
describe	O
a	O
number	O
of	O
modifications	O
that	O
enable	O
scaling	O
up	O
adversarial	Method
training	Method
to	O
64×64	O
images	O
.	O

Salimans	O
et	O
al	O
.	O

[	O
reference	O
]	O
also	O
tackle	O
the	O
instability	O
of	O
GAN	Method
training	Method
and	O
describe	O
a	O
number	O
of	O
heuristics	Method
that	O
encourage	O
convergence	Metric
.	O

The	O
authors	O
synthesize	O
128	O
×	O
128	O
images	O
that	O
possess	O
plausible	O
low	O
-	O
level	O
statistics	O
.	O

Nevertheless	O
,	O
as	O
observed	O
in	O
recent	O
work	O
and	O
widely	O
known	O
in	O
the	O
folklore	O
,	O
GANs	Method
"	O
remain	O
remarkably	O
difficult	O
to	O
train	O
"	O
and	O
"	O
approaches	O
to	O
attacking	O
this	O
problem	O
still	O
rely	O
on	O
heuristics	Method
that	O
are	O
extremely	O
sensitive	O
to	O
modifications	O
"	O
[	O
reference	O
]	O
.	O

(	O
See	O
also	O
[	O
reference	O
]	O
.	O

)	O
Our	O
work	O
demonstrates	O
that	O
these	O
difficulties	O
can	O
be	O
avoided	O
in	O
the	O
setting	O
we	O
consider	O
.	O

Dosovitskiy	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
ConvNet	Method
to	O
generate	O
images	Task
of	Task
3D	Task
models	Task
,	O
given	O
a	O
model	O
ID	O
and	O
viewpoint	O
.	O

The	O
network	O
thus	O
acts	O
directly	O
as	O
a	O
rendering	Method
engine	Method
for	O
the	O
3D	Method
model	Method
.	O

This	O
is	O
also	O
an	O
important	O
precursor	O
to	O
our	O
work	O
as	O
it	O
uses	O
direct	O
feedforward	Method
synthesis	Method
through	O
a	O
network	O
trained	O
with	O
a	O
regression	Method
loss	Method
.	O

Our	O
model	O
,	O
loss	Method
,	O
and	O
problem	O
setting	O
are	O
different	O
,	O
enabling	O
synthesis	Task
of	Task
sharper	Task
higherresolution	Task
images	Task
of	Task
scenes	Task
without	Task
3D	Task
models	Task
.	O

Dosovitskiy	O
and	O
Brox	O
[	O
reference	O
]	O
introduced	O
a	O
family	O
of	O
composite	Method
loss	Method
functions	Method
for	O
image	Task
synthesis	Task
,	O
which	O
combine	O
regression	Method
over	O
the	O
activations	O
of	O
a	O
fixed	O
"	O
perceiver	Method
"	Method
network	Method
with	O
a	O
GAN	Method
loss	Method
.	O

Networks	Method
trained	O
using	O
these	O
composite	O
loss	O
functions	O
were	O
applied	O
to	O
synthesize	O
preimages	O
that	O
induce	O
desired	O
excitation	O
patterns	O
in	O
image	Method
classification	Method
models	Method
[	O
reference	O
]	O
and	O
images	O
that	O
excite	O
specific	O
elements	O
in	O
such	O
models	O
[	O
reference	O
]	O
.	O

In	O
recent	O
work	O
,	O
networks	O
trained	O
using	O
these	O
losses	O
were	O
applied	O
to	O
generate	O
diverse	O
sets	O
of	O
227×227	O
images	O
,	O
to	O
synthesize	O
images	O
for	O
given	O
captions	O
,	O
and	O
to	O
inpaint	O
missing	O
regions	O
[	O
reference	O
]	O
.	O

These	O
works	O
all	O
rely	O
on	O
the	O
aforementioned	O
composite	O
losses	O
,	O
which	O
require	O
balancing	O
the	O
adversarial	O
loss	O
with	O
a	O
regression	Method
loss	Method
.	O

Our	O
work	O
differs	O
in	O
that	O
GANs	Method
are	O
not	O
used	O
,	O
which	O
simplifies	O
the	O
train	O
-	O
[	O
reference	O
]	O
.	O

Zoom	O
in	O
for	O
details	O
.	O

ing	O
procedure	O
,	O
architecture	O
,	O
and	O
loss	O
.	O

Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
consider	O
a	O
family	O
of	O
problems	O
that	O
include	O
the	O
image	Task
synthesis	Task
problem	Task
we	O
focus	O
on	O
.	O

The	O
paper	O
of	O
Isola	O
et	O
al	O
.	O

appeared	O
on	O
arXiv	O
during	O
the	O
course	O
of	O
our	O
research	O
.	O

It	O
provides	O
an	O
opportunity	O
to	O
compare	O
our	O
approach	O
to	O
a	O
credible	O
alternative	O
that	O
was	O
independently	O
tested	O
on	O
the	O
same	O
data	O
.	O

Like	O
a	O
number	O
of	O
aforementioned	O
formulations	O
,	O
Isola	O
et	O
al	O
.	O

use	O
a	O
composite	Method
loss	Method
that	O
combines	O
a	O
GAN	Method
and	O
a	O
regression	Method
term	Method
.	O

The	O
authors	O
use	O
the	O
Cityscapes	Material
dataset	O
and	O
synthesize	O
256×256	O
images	O
for	O
given	O
semantic	O
layouts	O
.	O

In	O
comparison	O
,	O
our	O
simpler	O
direct	Method
formulation	Method
yields	O
much	O
more	O
realistic	O
images	O
and	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
.	O

A	O
qualitative	O
comparison	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
64×64	O
images	O
of	O
scenes	O
that	O
are	O
described	O
by	O
given	O
sentences	O
.	O

Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
describe	O
a	O
different	O
model	O
that	O
generates	O
32×32	O
images	O
that	O
aim	O
to	O
fit	O
sentences	O
.	O

Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
generate	O
64×64	O
images	O
of	O
faces	O
and	O
birds	O
with	O
given	O
attributes	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
128×128	O
images	O
of	O
birds	O
and	O
people	O
conditioned	O
on	O
text	O
descriptions	O
and	O
on	O
spatial	O
constraints	O
such	O
as	O
bounding	O
boxes	O
or	O
keypoints	O
.	O

Wang	O
and	O
Gupta	O
[	O
reference	O
]	O
synthesize	O
128×128	Task
images	Task
of	Task
indoor	Task
scenes	Task
by	O
factorizing	O
the	O
image	Method
generation	Method
process	Method
into	O
synthesis	O
of	O
a	O
normal	Method
map	Method
and	O
subsequent	O
synthesis	O
of	O
a	O
corresponding	O
color	O
image	O
.	O

Most	O
of	O
these	O
works	O
use	O
GANs	Method
,	O
with	O
the	O
exception	O
of	O
Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
variational	Method
autoencoders	Method
and	O
Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
a	O
recurrent	Method
attention	Method
-	Method
based	Method
model	Method
[	O
reference	O
]	O
.	O

Our	O
problem	O
statement	O
is	O
different	O
in	O
that	O
our	O
input	O
is	O
a	O
pixelwise	O
semantic	O
layout	O
,	O
and	O
our	O
technical	O
approach	O
differs	O
substantially	O
in	O
that	O
a	O
single	O
feedforward	Method
convolutional	Method
network	Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
synthesize	O
a	O
high	O
-	O
resolution	O
image	O
.	O

A	O
line	O
of	O
work	O
considers	O
synthesis	Task
of	Task
future	Task
frames	Task
in	Task
video	Task
.	O

Srivastava	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
recurrent	Method
network	Method
for	O
this	O
purpose	O
.	O

Mathieu	O
et	O
al	O
.	O

[	O
reference	O
]	O
build	O
on	O
the	O
work	O
of	O
Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
use	O
a	O
composite	Method
loss	Method
that	O
combines	O
an	O
adversarial	Method
term	Method
with	O
regression	O
penalties	O
on	O
colors	O
and	O
gradients	O
.	O

Oh	O
et	O
al	O
.	O

[	O
reference	O
]	O
predict	O
future	O
frames	O
in	O
Atari	Task
games	Task
conditioned	O
on	O
the	O
player	O
's	O
action	O
.	O

Finn	O
et	O
al	O
.	O

[	O
reference	O
]	O
explicitly	O
model	O
pixel	O
motion	O
and	O
also	O
condition	O
on	O
action	O
.	O

Vondrick	O
et	O
al	O
.	O

[	O
reference	O
]	O
learn	O
a	O
model	Method
of	Method
scene	Method
dynamics	Method
and	O
use	O
it	O
to	O
synthesize	O
video	O
sequences	O
from	O
single	O
images	O
.	O

Xue	O
et	O
al	O
.	O

[	O
reference	O
]	O
develop	O
a	O
probabilistic	Method
model	Method
that	O
enables	O
synthesizing	O
multiple	O
plausible	O
video	O
sequences	O
.	O

In	O
these	O
works	O
,	O
a	O
color	O
image	O
is	O
available	O
as	O
a	O
starting	O
point	O
for	O
synthesis	Task
.	O

Video	Task
synthesis	Task
can	O
be	O
accomplished	O
by	O
advecting	O
the	O
content	O
of	O
this	O
initial	O
image	O
.	O

In	O
our	O
setting	O
,	O
photographic	Task
scene	Task
appearance	Task
must	O
be	O
synthesized	O
without	O
such	O
initialization	O
.	O

Researchers	O
have	O
also	O
studied	O
image	Task
inpainting	Task
[	O
reference	O
]	O
,	O
superresolution	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
novel	O
view	Task
synthesis	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
interactive	Task
image	Task
manipulation	Task
[	O
reference	O
]	O
.	O

In	O
these	O
problems	O
,	O
photographic	O
content	O
is	O
given	O
as	O
input	O
,	O
whereas	O
we	O
are	O
concerned	O
with	O
synthesizing	O
photographic	O
images	O
from	O
semantic	O
layouts	O
alone	O
.	O

section	O
:	O
Method	O
section	O
:	O
Preliminaries	O
Consider	O
a	O
semantic	O
layout	O
L	O
∈	O
{	O
0	O
,	O
1	O
}	O
m×n×c	O
,	O
where	O
m×n	O
is	O
the	O
pixel	O
resolution	O
and	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

Each	O
pixel	O
in	O
L	O
is	O
represented	O
by	O
a	O
one	O
-	O
hot	O
vector	O
that	O
indicates	O
its	O
semantic	O
label	O
:	O
One	O
of	O
the	O
c	O
possible	O
labels	O
is	O
'	O
void	O
'	O
,	O
which	O
indicates	O
that	O
the	O
semantic	O
class	O
of	O
the	O
pixel	O
is	O
not	O
specified	O
.	O

Our	O
goal	O
is	O
to	O
train	O
a	O
parametric	Method
mapping	Method
g	Method
that	O
given	O
a	O
semantic	O
layout	O
L	O
produces	O
a	O
color	O
image	O
I	O
∈	O
R	O
m×n×3	O
that	O
conforms	O
to	O
L.	O
In	O
the	O
course	O
of	O
this	O
project	O
we	O
have	O
experimented	O
with	O
a	O
large	O
number	O
of	O
network	Method
architectures	Method
.	O

As	O
a	O
result	O
of	O
these	O
experiments	O
,	O
we	O
have	O
identified	O
three	O
characteristics	O
that	O
are	O
important	O
for	O
synthesizing	Task
photorealistic	Task
images	Task
.	O

We	O
review	O
these	O
characteristics	O
before	O
describing	O
our	O
solution	O
.	O

Global	Task
coordination	Task
.	O

Globally	O
consistent	O
structure	O
is	O
essential	O
for	O
photorealism	Task
.	O

Many	O
objects	O
exhibit	O
nonlocal	O
structural	O
relationships	O
,	O
such	O
as	O
symmetry	O
.	O

For	O
example	O
,	O
if	O
the	O
network	O
synthesizes	O
a	O
red	O
light	O
on	O
the	O
left	O
side	O
of	O
a	O
car	O
,	O
then	O
the	O
corresponding	O
light	O
on	O
the	O
right	O
should	O
also	O
be	O
red	O
.	O

This	O
distinguishes	O
photorealistic	Task
image	Task
synthesis	Task
from	O
texture	Task
synthesis	Task
,	O
which	O
can	O
leverage	O
statistical	Method
stationarity	Method
[	O
reference	O
]	O
.	O

Our	O
model	O
is	O
based	O
on	O
multi	Method
-	Method
resolution	Method
refinement	Method
.	O

The	O
synthesis	Task
begins	O
at	O
extremely	O
low	O
resolution	O
(	O
4	O
×	O
8	O
in	O
our	O
implementation	O
)	O
.	O

Feature	O
maps	O
are	O
then	O
progressively	O
refined	O
.	O

Thus	O
global	O
structure	O
can	O
be	O
coordinated	O
at	O
lower	O
octaves	O
,	O
where	O
even	O
distant	O
object	O
parts	O
are	O
represented	O
in	O
nearby	O
feature	O
columns	O
.	O

These	O
decisions	O
are	O
then	O
refined	O
at	O
higher	O
octaves	O
.	O

High	Task
resolution	Task
.	O

To	O
produce	O
truly	O
photorealistic	O
results	O
,	O
a	O
model	O
must	O
be	O
able	O
to	O
synthesize	O
high	O
-	O
resolution	O
images	O
.	O

Low	Task
resolution	Task
is	O
akin	O
to	O
myopic	Task
vision	Task
in	O
that	O
fine	O
visual	O
features	O
are	O
not	O
discernable	O
.	O

The	O
drive	O
to	O
high	O
image	Task
and	Task
video	Task
resolutions	Task
in	O
multiple	O
industries	O
is	O
a	O
testament	O
to	O
resolution	O
's	O
importance	O
.	O

Our	O
model	O
synthesizes	O
images	O
by	O
progressive	Method
refinement	Method
,	O
and	O
going	O
up	O
an	O
octave	O
in	O
resolution	O
(	O
e.g.	O
,	O
from	O
512p	O
to	O
1024p	O
)	O
amounts	O
to	O
adding	O
a	O
single	O
refinement	Method
module	Method
.	O

The	O
entire	O
cascade	Method
of	Method
refinement	Method
modules	Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Memory	O
.	O

We	O
conjecture	O
that	O
high	O
model	Metric
capacity	Metric
is	O
essential	O
for	O
synthesizing	Task
high	Task
-	Task
resolution	Task
photorealistic	Task
images	Task
.	O

Human	O
hyperrealistic	O
painters	O
use	O
photographic	O
references	O
as	O
external	O
memory	O
of	O
detailed	O
object	O
appearance	O
[	O
reference	O
]	O
.	O

The	O
best	O
existing	O
image	Method
compression	Method
techniques	Method
require	O
millions	O
of	O
bits	O
of	O
information	O
to	O
represent	O
the	O
content	O
of	O
a	O
single	O
high	O
-	O
resolution	O
image	O
:	O
there	O
exists	O
no	O
known	O
way	O
to	O
reconstruct	O
a	O
given	O
photograph	O
at	O
high	O
fidelity	O
from	O
a	O
lowercapacity	Method
representation	Method
[	O
reference	O
]	O
.	O

In	O
order	O
for	O
our	O
model	O
to	O
be	O
able	O
to	O
synthesize	O
diverse	O
scenes	O
from	O
a	O
given	O
domain	O
given	O
only	O
semantic	O
layouts	O
as	O
input	O
,	O
the	O
capacity	O
of	O
the	O
model	O
must	O
be	O
sufficiently	O
high	O
to	O
be	O
able	O
to	O
reproduce	O
the	O
detailed	O
photographic	O
appearance	O
of	O
many	O
objects	O
.	O

We	O
expect	O
a	O
successful	O
model	O
to	O
reproduce	O
images	O
in	O
the	O
training	O
set	O
extremely	O
well	O
(	O
memorization	O
)	O
and	O
also	O
to	O
apply	O
the	O
learned	O
representations	O
to	O
novel	O
layouts	O
(	O
generalization	Task
)	O
.	O

This	O
requires	O
high	O
model	O
capacity	O
.	O

Our	O
design	O
is	O
modular	O
and	O
the	O
capacity	O
of	O
the	O
model	O
can	O
be	O
expanded	O
as	O
allowed	O
by	O
hardware	O
.	O

The	O
network	O
used	O
in	O
most	O
of	O
our	O
experiments	O
has	O
105	O
M	O
parameters	O
and	O
maximizes	O
available	O
GPU	O
memory	O
.	O

We	O
have	O
consistently	O
found	O
that	O
increasing	O
model	Metric
capacity	Metric
increases	O
image	Metric
quality	Metric
.	O

section	O
:	O
Architecture	O
The	O
Cascaded	Method
Refinement	Method
Network	Method
(	O
CRN	Method
)	O
is	O
a	O
cascade	Method
of	Method
refinement	Method
modules	Method
.	O

Each	O
module	O
M	O
i	O
operates	O
at	O
a	O
given	O
resolution	O
.	O

In	O
our	O
implementation	O
,	O
the	O
resolution	O
of	O
the	O
first	O
module	O
(	O
M	O
0	O
)	O
is	O
4×8	O
.	O

Resolution	Method
is	O
doubled	O
between	O
consecutive	O
modules	O
(	O
from	O
M	O
i−1	O
to	O
M	O
i	O
)	O
.	O

Let	O
w	O
i	O
×	O
h	O
i	O
be	O
the	O
resolution	O
of	O
module	O
i.	O
The	O
first	O
module	O
,	O
M	O
0	O
,	O
receives	O
the	O
semantic	O
layout	O
L	O
as	O
input	O
(	O
downsampled	O
to	O
w	O
0	O
×h	O
0	O
)	O
and	O
produces	O
a	O
feature	Method
layer	Method
F	O
0	O
at	O
resolution	O
w	O
0	O
×	O
h	O
0	O
as	O
output	O
.	O

All	O
other	O
modules	O
M	O
i	O
(	O
for	O
i	O
=	O
0	O
)	O
are	O
structurally	O
identical	O
:	O
M	O
i	O
receives	O
a	O
concatenation	O
of	O
the	O
layout	O
L	O
(	O
downsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
and	O
the	O
feature	O
layer	O
F	O
i−1	O
(	O
upsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
as	O
input	O
,	O
and	O
produces	O
feature	O
layer	O
F	O
i	O
as	O
output	O
.	O

We	O
denote	O
the	O
number	O
of	O
feature	O
maps	O
in	O
F	O
i	O
by	O
d	O
i	O
.	O

Each	O
module	O
M	O
i	O
consists	O
of	O
three	O
feature	Method
layers	Method
:	O
the	O
input	Method
layer	Method
,	O
an	O
intermediate	Method
layer	Method
,	O
and	O
the	O
output	O
layer	O
.	O

This	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O

The	O
input	O
layer	O
has	O
dimensionality	O
w	O
i	O
×h	O
i	O
×	O
(	O
d	O
i−1	O
+	O
c	O
)	O
and	O
is	O
a	O
concatenation	O
of	O
the	O
downsampled	Method
semantic	Method
layout	Method
L	Method
(	O
c	O
channels	O
)	O
and	O
a	O
bilinearly	Method
upsampled	Method
feature	Method
layer	Method
F	O
i−1	O
(	O
d	O
i−1	O
channels	O
)	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
upconvolutions	Method
because	O
upconvolutions	Method
tend	O
to	O
introduce	O
characteristic	O
artifacts	O
[	O
reference	O
]	O
.	O

The	O
intermediate	O
layer	O
and	O
the	O
output	O
layer	O
both	O
have	O
dimensionality	O
w	O
i	O
×h	O
i	O
×d	O
i	O
.	O

Each	O
layer	O
is	O
followed	O
by	O
3×3	O
convolutions	O
,	O
layer	Method
normalization	Method
[	O
reference	O
]	O
,	O
and	O
LReLU	Method
nonlinearity	Method
[	O
reference	O
]	O
.	O

The	O
output	O
layer	O
Fī	O
of	O
the	O
final	O
module	O
Mī	Method
is	O
not	O
followed	O
by	O
normalization	O
or	O
nonlinearity	O
.	O

Instead	O
,	O
a	O
linear	Method
projection	Method
(	O
1×1	Method
convolution	Method
)	O
is	O
applied	O
to	O
map	O
Fī	O
(	O
dimensionality	O
wī	O
×hī	O
×dī	O
)	O
to	O
the	O
output	O
color	O
image	O
(	O
dimensionality	O
wī	O
×hī	O
×3	O
)	O
.	O

The	O
total	O
number	O
of	O
refinement	O
modules	O
in	O
a	O
cascade	O
depends	O
on	O
the	O
output	O
resolution	O
.	O

section	O
:	O
Training	O
The	O
CRN	Method
is	O
trained	O
in	O
a	O
supervised	Method
fashion	Method
on	O
a	O
semantic	O
segmentation	O
dataset	O
D	O
=	O
{	O
(	O
I	O
,	O
L	O
)	O
}.	O
A	O
semantic	O
layout	O
L	O
is	O
used	O
as	O
input	O
and	O
the	O
corresponding	O
color	O
image	O
I	O
as	O
output	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
"	O
inverse	Task
semantic	Task
segmentation	Task
"	O
.	O

It	O
is	O
an	O
underconstrained	Task
one	Task
-	Task
to	Task
-	Task
many	Task
inverse	Task
problem	Task
.	O

We	O
will	O
generally	O
refer	O
to	O
I	O
as	O
a	O
"	O
reference	O
image	O
"	O
rather	O
than	O
"	O
ground	O
truth	O
"	O
,	O
since	O
many	O
valid	O
photographic	O
images	O
could	O
have	O
yielded	O
the	O
same	O
semantic	O
layout	O
.	O

Given	O
the	O
underconstrained	O
nature	O
of	O
the	O
problem	O
,	O
using	O
an	O
appropriate	O
loss	Method
function	Method
is	O
critical	O
,	O
as	O
observed	O
in	O
prior	O
work	O
on	O
image	Task
synthesis	Task
.	O

Simply	O
comparing	O
the	O
pixel	O
colors	O
of	O
the	O
synthesized	O
image	O
and	O
the	O
reference	O
image	O
could	O
severely	O
penalize	O
perfectly	O
realistic	O
outputs	O
.	O

For	O
example	O
,	O
synthesizing	O
a	O
white	O
car	O
instead	O
of	O
a	O
black	O
car	O
would	O
induce	O
a	O
very	O
high	O
loss	O
.	O

Instead	O
we	O
adopt	O
the	O
"	O
content	Method
representation	Method
"	O
of	O
Gatys	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
also	O
referred	O
to	O
as	O
a	O
perceptual	Task
loss	Task
or	O
feature	Task
matching	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

The	O
basic	O
idea	O
is	O
to	O
match	O
activations	O
in	O
a	O
visual	Method
perception	Method
network	Method
that	O
is	O
applied	O
to	O
the	O
synthesized	O
image	O
and	O
separately	O
to	O
the	O
reference	O
image	O
.	O

Let	O
Φ	O
be	O
a	O
trained	O
visual	Method
perception	Method
network	Method
(	O
we	O
use	O
VGG	Method
-	Method
19	Method
[	O
reference	O
]	O
)	O
.	O

Layers	O
in	O
the	O
network	O
represent	O
an	O
image	O
at	O
increasing	O
levels	O
of	O
abstraction	O
:	O
from	O
edges	O
and	O
colors	O
to	O
objects	O
and	O
categories	O
.	O

Matching	O
both	O
lower	O
-	O
layer	O
and	O
higher	O
-	O
layer	O
activations	O
in	O
the	O
perception	Method
network	Method
guides	O
the	O
synthesis	Method
network	Method
to	O
learn	O
both	O
fine	O
-	O
grained	O
details	O
and	O
more	O
global	O
part	O
arrangement	O
.	O

Let	O
{	O
Φ	O
l	O
}	O
be	O
a	O
collection	O
of	O
layers	O
in	O
the	O
network	Method
Φ	Method
,	O
such	O
that	O
Φ	O
0	O
denotes	O
the	O
input	O
image	O
.	O

Each	O
layer	O
is	O
a	O
threedimensional	O
tensor	O
.	O

For	O
a	O
training	O
pair	O
(	O
I	O
,	O
L	O
)	O
∈	O
D	O
,	O
our	O
loss	Metric
is	O
Here	O
g	O
is	O
the	O
image	Method
synthesis	Method
network	Method
being	O
trained	O
and	O
θ	O
is	O
the	O
set	O
of	O
parameters	O
of	O
this	O
network	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
balance	O
the	O
contribution	O
of	O
each	O
layer	O
l	O
to	O
the	O
loss	O
.	O

For	O
layers	O
Φ	O
l	O
(	O
l	O
≥	O
1	O
)	O
we	O
use	O
'	O
conv1	O
2	O
'	O
,	O
'	O
conv2	O
2	O
'	O
,	O
'	O
conv3	O
2	O
'	O
,	O
'	O
conv4	O
2	O
'	O
,	O
and	O
'	O
conv5	O
2	O
'	O
in	O
VGG	Method
-	Method
19	Method
[	O
reference	O
]	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
are	O
set	O
automatically	O
.	O

They	O
are	O
initialized	O
to	O
the	O
inverse	O
of	O
the	O
number	O
of	O
elements	O
in	O
each	O
layer	O
.	O

After	O
100	O
epochs	O
,	O
{	O
λ	O
l	O
}	O
are	O
rescaled	O
to	O
normalize	O
the	O
expected	O
contribution	O
of	O
each	O
term	O
Φ	O
l	O
(	O
I	O
)	O
−	O
Φ	O
l	O
(	O
g	O
(	O
L	O
;	O
θ	O
)	O
)	O
1	O
to	O
the	O
loss	O
.	O

section	O
:	O
Synthesizing	O
a	O
diverse	O
collection	O
The	O
architecture	O
and	O
training	Method
procedure	Method
described	O
so	O
far	O
synthesize	O
a	O
single	O
image	O
for	O
a	O
given	O
input	O
L.	O
In	O
our	O
experiments	O
this	O
already	O
yields	O
good	O
results	O
.	O

However	O
,	O
since	O
a	O
given	O
semantic	O
layout	O
can	O
correspond	O
to	O
many	O
images	O
,	O
it	O
also	O
makes	O
sense	O
to	O
generate	O
a	O
diverse	O
set	O
of	O
images	O
as	O
output	O
.	O

Conditional	Task
synthesis	Task
of	Task
diverse	Task
images	Task
can	O
be	O
approached	O
as	O
a	O
stochastic	Method
process	Method
[	O
reference	O
]	O
.	O

We	O
take	O
a	O
different	O
tack	O
and	O
modify	O
the	O
network	O
to	O
emit	O
a	O
collection	O
of	O
images	O
in	O
one	O
shot	O
,	O
with	O
a	O
modified	O
loss	O
that	O
encourages	O
diversity	O
within	O
the	O
collection	O
.	O

Specifically	O
,	O
we	O
change	O
the	O
number	O
of	O
output	O
channels	O
from	O
3	O
to	O
3k	O
,	O
where	O
k	O
is	O
the	O
desired	O
number	O
of	O
images	O
.	O

Each	O
consecutive	O
3	O
-	O
tuple	O
of	O
channels	O
forms	O
an	O
image	O
.	O

Now	O
consider	O
the	O
loss	O
.	O

If	O
loss	O
(	O
1	O
)	O
is	O
applied	O
independently	O
to	O
each	O
output	O
image	O
,	O
the	O
k	O
synthesized	O
images	O
will	O
be	O
identical	O
.	O

Our	O
first	O
modification	O
is	O
to	O
consider	O
the	O
set	O
of	O
k	O
outputs	O
together	O
and	O
define	O
the	O
loss	O
of	O
the	O
whole	O
collection	O
in	O
terms	O
of	O
the	O
best	O
synthesized	O
image	O
.	O

Let	O
g	O
u	O
(	O
L	O
;	O
θ	O
)	O
be	O
the	O
u	O
th	O
image	O
in	O
the	O
synthesized	O
collection	O
.	O

Our	O
first	O
version	O
of	O
the	O
modified	O
loss	O
is	O
based	O
on	O
the	O
hindsight	Method
loss	Method
developed	O
for	O
multiple	Task
choice	Task
learning	Task
[	O
reference	O
]	O
:	O
By	O
considering	O
only	O
the	O
best	O
synthesized	O
image	O
,	O
this	O
loss	O
encourages	O
the	O
network	O
to	O
spread	O
its	O
bets	O
and	O
cover	O
the	O
space	O
of	O
images	O
that	O
conform	O
to	O
the	O
input	O
semantic	O
layout	O
.	O

The	O
loss	O
is	O
structurally	O
akin	O
to	O
the	O
k	Method
-	Method
means	Method
clustering	Method
objective	Method
,	O
which	O
only	O
considers	O
the	O
closest	O
centroid	O
to	O
each	O
datapoint	O
and	O
thus	O
encourages	O
the	O
centroids	O
to	O
spread	O
and	O
cover	O
the	O
dataset	O
.	O

We	O
further	O
build	O
on	O
this	O
idea	O
and	O
formulate	O
a	O
loss	Method
that	O
considers	O
a	O
virtual	O
collection	O
of	O
up	O
to	O
k	O
c	O
images	O
.	O

(	O
Recall	O
that	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

)	O
Specifically	O
,	O
for	O
each	O
semantic	O
class	O
p	O
,	O
let	O
L	O
p	O
denote	O
the	O
corresponding	O
channel	O
L	O
(	O
·	O
,	O
·	O
,	O
p	O
)	O
in	O
the	O
input	O
label	O
map	O
.	O

We	O
now	O
define	O
a	O
more	O
powerful	O
diversity	O
loss	O
as	O
where	O
Φ	O
j	O
l	O
is	O
the	O
j	O
th	O
feature	O
map	O
in	O
Φ	O
l	O
,	O
L	O
l	O
p	O
is	O
the	O
mask	O
L	O
p	O
downsampled	O
to	O
match	O
the	O
resolution	O
of	O
Φ	O
l	O
,	O
and	O
is	O
the	O
Hadamard	Method
product	Method
.	O

This	O
loss	O
in	O
effect	O
constructs	O
a	O
virtual	O
image	O
by	O
adaptively	O
taking	O
the	O
best	O
synthesized	O
content	O
for	O
each	O
semantic	O
class	O
from	O
the	O
whole	O
collection	O
,	O
and	O
scoring	O
the	O
collection	O
based	O
on	O
this	O
assembled	O
image	O
.	O

section	O
:	O
Baselines	O
The	O
approach	O
presented	O
in	O
Section	O
3	O
is	O
far	O
from	O
the	O
first	O
we	O
tried	O
.	O

In	O
this	O
section	O
we	O
describe	O
a	O
number	O
of	O
alternative	O
approaches	O
that	O
will	O
be	O
used	O
as	O
baselines	O
in	O
Section	O
5	O
.	O

GAN	Method
and	O
semantic	Task
segmentation	Task
.	O

Our	O
first	O
baseline	O
is	O
consistent	O
with	O
current	O
trends	O
in	O
the	O
research	O
community	O
.	O

It	O
combines	O
a	O
GAN	Method
with	O
a	O
semantic	Method
segmentation	Method
objective	Method
.	O

The	O
generator	O
is	O
trained	O
to	O
synthesize	O
an	O
image	O
that	O
fools	O
the	O
discriminator	Method
[	O
reference	O
]	O
.	O

An	O
additional	O
term	O
in	O
the	O
loss	O
specifies	O
that	O
when	O
the	O
synthesized	O
image	O
is	O
given	O
as	O
input	O
to	O
a	O
pretrained	Method
semantic	Method
segmentation	Method
network	Method
,	O
it	O
should	O
produce	O
a	O
label	O
map	O
that	O
is	O
as	O
close	O
to	O
the	O
input	O
layout	O
L	O
as	O
possible	O
.	O

The	O
GAN	O
setup	O
follows	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

The	O
input	O
to	O
the	O
generator	O
is	O
the	O
semantic	O
layout	O
L.	O
For	O
the	O
semantic	Task
segmentation	Task
network	Task
,	O
we	O
use	O
publicly	O
available	O
networks	O
that	O
were	O
pretrained	O
for	O
the	O
Cityscapes	Material
dataset	O
[	O
reference	O
]	O
and	O
the	O
NYU	Material
dataset	Material
[	O
reference	O
]	O
.	O

The	O
training	Metric
objective	Metric
combines	O
the	O
GAN	O
loss	O
and	O
the	O
semantic	Method
segmentation	Method
(	O
pixelwise	Method
cross	Method
-	Method
entropy	Method
)	O
loss	O
.	O

Full	Method
-	Method
resolution	Method
network	Method
.	O

Our	O
second	O
baseline	O
is	O
a	O
feedforward	Method
convolutional	Method
network	Method
that	O
operates	O
at	O
full	O
resolution	O
.	O

This	O
baseline	O
uses	O
the	O
same	O
loss	O
as	O
the	O
CRN	Method
described	O
in	O
Section	O
3	O
.	O

The	O
only	O
difference	O
is	O
the	O
network	Method
architecture	Method
.	O

In	O
particular	O
,	O
we	O
have	O
experimented	O
with	O
variants	O
of	O
the	O
multi	Method
-	Method
scale	Method
context	Method
aggregation	Method
network	Method
[	O
reference	O
]	O
.	O

An	O
appealing	O
property	O
of	O
this	O
network	O
is	O
that	O
it	O
retains	O
high	O
resolution	O
in	O
the	O
intermediate	O
layers	O
,	O
which	O
we	O
hypothesized	O
to	O
be	O
helpful	O
for	O
photorealistic	Task
image	Task
synthesis	Task
.	O

The	O
original	O
architecture	O
described	O
in	O
[	O
reference	O
]	O
did	O
not	O
yield	O
good	O
results	O
and	O
is	O
not	O
well	O
-	O
suited	O
to	O
our	O
problem	O
,	O
because	O
the	O
input	O
semantic	O
layouts	O
are	O
piecewise	O
constant	O
and	O
the	O
network	O
of	O
[	O
reference	O
]	O
begins	O
with	O
a	O
small	O
receptive	O
field	O
.	O

We	O
obtained	O
much	O
better	O
results	O
with	O
the	O
inverse	Method
architecture	Method
:	O
start	O
with	O
large	O
dilation	O
and	O
decrease	O
it	O
by	O
a	O
factor	O
of	O
2	O
in	O
each	O
layer	O
.	O

This	O
can	O
be	O
viewed	O
as	O
a	O
full	Task
-	Task
resolution	Task
counterpart	Task
to	O
the	O
CRN	Method
,	O
based	O
on	O
dilating	O
the	O
filters	Method
instead	O
of	O
scaling	O
the	O
feature	O
maps	O
.	O

One	O
of	O
the	O
drawbacks	O
of	O
this	O
approach	O
is	O
that	O
all	O
intermediate	Method
feature	Method
layers	Method
are	O
at	O
full	O
image	O
resolution	O
and	O
have	O
a	O
high	O
memory	O
footprint	O
.	O

Thus	O
the	O
ratio	O
of	O
capacity	O
(	O
number	O
of	O
parameters	O
)	O
to	O
memory	Metric
footprint	Metric
is	O
much	O
lower	O
than	O
in	O
the	O
CRN	Method
.	O

This	O
high	O
memory	O
footprint	O
of	O
intermediate	O
layers	O
also	O
constrains	O
the	O
resolution	O
to	O
which	O
this	O
approach	O
can	O
scale	O
:	O
with	O
10	O
layers	O
and	O
256	O
feature	O
maps	O
per	O
layer	O
,	O
the	O
maximal	O
resolution	O
that	O
could	O
be	O
trained	O
with	O
available	O
GPU	O
memory	O
is	O
256×512	O
.	O

Encoder	Method
-	Method
decoder	Method
.	O

Our	O
third	O
baseline	O
is	O
an	O
encoder	Method
-	Method
decoder	Method
network	Method
,	O
the	O
u	Method
-	Method
net	Method
[	O
reference	O
]	O
.	O

This	O
network	O
is	O
also	O
trained	O
with	O
the	O
same	O
loss	Metric
as	O
the	O
CRN	Method
.	O

It	O
is	O
thus	O
an	O
additional	O
baseline	O
that	O
evaluates	O
the	O
effect	O
of	O
using	O
the	O
CRN	Method
versus	O
a	O
different	O
architecture	O
,	O
when	O
everything	O
else	O
(	O
loss	O
,	O
training	O
procedure	O
)	O
is	O
held	O
fixed	O
.	O

Image	Task
-	Task
space	Task
loss	Task
.	O

Our	O
next	O
baseline	O
controls	O
for	O
the	O
feature	Task
matching	Task
loss	Task
used	O
to	O
train	O
the	O
CRN	Method
.	O

Here	O
we	O
use	O
exactly	O
the	O
same	O
architecture	O
as	O
in	O
Section	O
3	O
,	O
but	O
use	O
only	O
the	O
first	O
layer	O
Φ	O
0	O
(	O
image	O
color	O
)	O
in	O
the	O
loss	O
:	O
Image	Task
-	Task
to	Task
-	Task
image	Task
translation	Task
.	O

Our	O
last	O
baseline	O
is	O
the	O
contemporaneous	Method
approach	Method
of	O
Isola	O
et	O
al	O
.	O

,	O
the	O
implementation	O
and	O
results	O
of	O
which	O
are	O
publicly	O
available	O
[	O
reference	O
]	O
.	O

This	O
approach	O
uses	O
a	O
conditional	Method
GAN	Method
and	O
is	O
representative	O
of	O
the	O
dominant	O
stream	O
of	O
research	O
in	O
image	Task
synthesis	Task
.	O

The	O
generator	O
is	O
an	O
encoder	Method
-	Method
decoder	Method
[	O
reference	O
]	O
.	O

The	O
GAN	O
setup	O
is	O
derived	O
from	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Experiments	O
section	O
:	O
Experimental	O
procedure	O
Methodology	O
.	O

The	O
most	O
reliable	O
known	O
methodology	O
for	O
evaluating	O
the	O
realism	Task
of	O
synthesized	O
images	O
is	O
perceptual	O
experiments	O
with	O
human	O
observers	O
.	O

Such	O
experiments	O
yield	O
quantitative	O
results	O
and	O
have	O
been	O
used	O
in	O
related	O
work	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

There	O
have	O
also	O
been	O
attempts	O
to	O
design	O
automatic	Metric
measures	Metric
that	O
evaluate	O
realism	O
without	O
humans	O
in	O
the	O
loop	O
.	O

For	O
example	O
,	O
Salimans	O
et	O
al	O
.	O

ran	O
a	O
pretrained	Method
image	Method
classification	Method
network	Method
on	O
synthesized	O
images	O
and	O
analyzed	O
its	O
predictions	O
[	O
reference	O
]	O
.	O

We	O
experimented	O
with	O
such	O
automatic	Metric
measures	Metric
(	O
for	O
example	O
using	O
pretrained	Method
semantic	Method
segmentation	Method
networks	Method
)	O
and	O
found	O
that	O
they	O
can	O
all	O
be	O
fooled	O
by	O
augmenting	O
any	O
baseline	O
to	O
also	O
optimize	O
for	O
the	O
evaluated	O
measure	O
;	O
the	O
resulting	O
images	O
are	O
not	O
more	O
realistic	O
but	O
score	O
very	O
highly	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Well	O
-	O
designed	O
perceptual	O
experiments	O
with	O
human	O
observers	O
are	O
more	O
reliable	O
.	O

We	O
therefore	O
use	O
carefully	O
designed	O
perceptual	O
experiments	O
for	O
quantitative	Task
evaluation	Task
.	O

We	O
will	O
release	O
our	O
complete	O
implementation	O
and	O
experimental	O
setup	O
so	O
that	O
our	O
experiments	O
can	O
be	O
replicated	O
by	O
others	O
.	O

All	O
experiments	O
use	O
pairwise	O
A	O
/	O
B	O
tests	O
deployed	O
on	O
the	O
Amazon	Method
Mechanical	Method
Turk	Method
(	O
MTurk	Method
)	O
platform	O
.	O

Similar	O
protocols	O
have	O
been	O
used	O
to	O
evaluate	O
the	O
realism	Task
of	Task
3D	Task
reconstructions	Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Each	O
MTurk	Method
job	O
involves	O
a	O
batch	O
of	O
roughly	O
100	O
pairwise	O
comparisons	O
,	O
along	O
with	O
sentinel	O
pairs	O
that	O
test	O
whether	O
the	O
worker	O
is	O
attentive	O
and	O
diligent	O
.	O

Each	O
pair	O
contains	O
two	O
images	O
synthesized	O
for	O
the	O
same	O
label	O
map	O
by	O
two	O
different	O
approaches	O
(	O
or	O
a	O
corresponding	O
reference	O
image	O
from	O
the	O
dataset	O
)	O
.	O

The	O
workers	O
are	O
asked	O
to	O
select	O
the	O
more	O
realistic	O
image	O
in	O
each	O
pair	O
.	O

The	O
images	O
are	O
all	O
shown	O
at	O
the	O
same	O
resolution	O
(	O
200×400	O
)	O
.	O

The	O
comparisons	O
are	O
randomized	O
across	O
conditions	O
and	O
both	O
the	O
left	O
-	O
right	O
order	O
and	O
the	O
order	O
within	O
a	O
job	O
are	O
randomized	O
.	O

Two	O
types	O
of	O
experiments	O
are	O
conducted	O
.	O

In	O
the	O
first	O
,	O
images	O
are	O
shown	O
for	O
unlimited	O
time	O
and	O
the	O
worker	O
is	O
free	O
to	O
spend	O
as	O
much	O
time	O
as	O
desired	O
on	O
each	O
pair	O
.	O

In	O
the	O
second	O
,	O
each	O
pair	O
is	O
shown	O
for	O
a	O
randomly	O
chosen	O
duration	O
between	O
Table	O
1	O
.	O

Results	O
of	O
pairwise	Metric
comparisons	Metric
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	Material
and	O
NYU	Material
datasets	Material
.	O

Each	O
column	O
compares	O
our	O
approach	O
with	O
one	O
of	O
the	O
baselines	O
.	O

Each	O
cell	O
lists	O
the	O
fraction	O
of	O
pairwise	O
comparisons	O
in	O
which	O
images	O
synthesized	O
by	O
our	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
corresponding	O
baseline	O
.	O

Chance	O
is	O
at	O
50	O
%	O
.	O

Datasets	O
.	O

We	O
use	O
two	O
datasets	O
with	O
pixelwise	O
semantic	O
labels	O
,	O
one	O
depicting	O
outdoor	O
scenes	O
and	O
one	O
depicting	O
indoor	O
scenes	O
.	O

Our	O
primary	O
dataset	O
is	O
Cityscapes	Material
,	O
which	O
has	O
become	O
the	O
dominant	O
semantic	O
segmentation	O
dataset	O
due	O
to	O
the	O
quality	O
of	O
the	O
data	O
[	O
reference	O
]	O
.	O

We	O
train	O
on	O
the	O
training	O
set	O
(	O
3	O
K	O
images	O
)	O
and	O
evaluate	O
on	O
the	O
validation	O
set	O
(	O
500	O
images	O
)	O
.	O

(	O
Evaluating	O
"	O
inverse	Task
semantic	Task
segmentation	Task
"	O
on	O
the	O
test	O
set	O
is	O
impossible	O
because	O
the	O
label	O
maps	O
are	O
not	O
provided	O
.	O

)	O
Our	O
second	O
dataset	O
is	O
the	O
older	O
NYU	Material
dataset	Material
of	Material
indoor	Material
scenes	Material
[	O
reference	O
]	O
.	O

This	O
dataset	O
is	O
smaller	O
and	O
the	O
images	O
are	O
VGA	Material
resolution	Material
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
depth	O
data	O
in	O
the	O
NYU	Material
dataset	Material
,	O
only	O
the	O
semantic	O
layouts	O
and	O
the	O
color	O
images	O
.	O

We	O
use	O
the	O
first	O
1200	O
of	O
the	O
1449	O
labeled	O
images	O
for	O
training	O
and	O
the	O
remaining	O
249	O
for	O
testing	O
.	O

section	O
:	O
Results	O
Primary	O
experiments	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	Material
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
four	O
alternative	O
approaches	O
.	O

Note	O
that	O
the	O
'	O
image	Method
-	Method
space	Method
loss	Method
'	Method
baseline	Method
uses	O
the	O
same	O
architecture	O
as	O
the	O
CRN	Method
and	O
controls	O
for	O
the	O
loss	O
,	O
while	O
the	O
'	O
full	Method
-	Method
resolution	Method
network	Method
'	O
and	O
the	O
'	O
encoder	Method
-	Method
decoder	Method
'	O
use	O
the	O
same	O
loss	O
as	O
the	O
CRN	Method
and	O
control	O
for	O
the	O
architecture	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Compared	O
to	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	Method
were	O
rated	O
more	O
realistic	O
in	O
97	O
%	O
of	O
the	O
comparisons	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
5	O
.	O

Figure	O
4	O
reports	O
the	O
results	O
of	O
time	O
-	O
limited	O
pairwise	O
comparisons	O
of	O
real	O
Cityscapes	Material
images	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	Method
,	O
and	O
images	O
synthesized	O
by	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
(	O
referred	O
to	O
as	O
'	O
Pix2pix	Material
'	O
following	O
the	O
public	O
implementation	O
)	O
.	O

After	O
just	O
1	O
8	O
of	O
a	O
second	O
,	O
the	O
Pix2pix	Material
images	O
are	O
clearly	O
rated	O
less	O
realistic	O
than	O
the	O
real	O
Cityscapes	Material
images	O
or	O
the	O
CRN	Method
images	O
(	O
72.5	O
%	O
Real	Material
>	O
Pix2pix	Material
,	O
73.4	O
%	O
CRN	Method
>	O
Pix2pix	Material
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
CRN	Method
images	O
are	O
on	O
par	O
with	O
real	O
images	O
at	O
that	O
time	O
,	O
as	O
seen	O
both	O
in	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
(	O
52.6	O
%	O
)	O
and	O
in	O
the	O
nearly	O
identical	O
Real	Material
>	O
Pix2pix	Material
and	O
CRN	Method
>	O
Pix2pix	Material
rates	O
.	O

At	O
250	O
milliseconds	O
(	O
1	O
4	O
of	O
a	O
second	O
)	O
,	O
the	O
Real	Material
>	O
Pix2pix	Material
rate	O
rises	O
to	O
85.0	O
%	O
while	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
is	O
at	O
57.4	O
%	O
.	O

The	O
CRN	Method
>	O
Pix2pix	Material
rate	O
is	O
84.0	O
%	O
,	O
still	O
nearly	O
identical	O
to	O
Real	Material
>	O
Pix2pix	Material
.	O

At	O
500	O
milliseconds	O
,	O
the	O
Real	Material
>	O
Pix2pix	Material
and	O
CRN	Method
>	O
Pix2pix	Material
rates	O
finally	O
diverge	O
,	O
although	O
both	O
are	O
extremely	O
high	O
(	O
95.1	O
%	O
and	O
87.4	O
%	O
,	O
respectively	O
)	O
,	O
and	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
rises	O
to	O
64.2	O
%	O
.	O

Over	O
time	O
,	O
the	O
CRN	Method
>	O
Pix2pix	Material
rate	O
rises	O
above	O
90	O
%	O
and	O
the	O
Real	Material
>	O
Pix2pix	Material
rate	O
remains	O
consistently	O
higher	O
than	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
.	O

NYU	Material
dataset	Material
.	O

We	O
conduct	O
supporting	O
experiments	O
on	O
the	O
NYU	Material
dataset	Material
.	O

This	O
dataset	O
is	O
smaller	O
and	O
lower	O
-	O
resolution	O
,	O
so	O
the	O
quality	O
of	O
images	O
synthesized	O
by	O
all	O
approaches	O
is	O
lower	O
.	O

Nevertheless	O
,	O
the	O
differences	O
are	O
still	O
clear	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
for	O
this	O
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
again	O
rated	O
consistently	O
more	O
realistic	O
than	O
the	O
baselines	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
6	O
.	O

Diversity	Task
loss	Task
.	O

For	O
all	O
preceding	O
experiments	O
we	O
have	O
used	O
the	O
feature	O
matching	O
loss	O
specified	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
models	O
produced	O
a	O
single	O
image	O
as	O
output	O
,	O
and	O
this	O
image	O
was	O
evaluated	O
against	O
baselines	O
.	O

We	O
now	O
qualitatively	O
demonstrate	O
the	O
effect	O
of	O
the	O
diversity	O
loss	O
described	O
in	O
Section	O
3.4	O
.	O

To	O
this	O
end	O
we	O
trained	O
models	O
that	O
produce	O
image	O
collections	O
as	O
output	O
(	O
9	O
images	O
at	O
a	O
time	O
)	O
.	O

Figure	O
7	O
shows	O
pairs	O
of	O
images	O
sampled	O
from	O
the	O
synthesized	O
collections	O
,	O
for	O
different	O
input	O
layouts	O
in	O
the	O
NYU	Material
validation	Material
set	Material
.	O

The	O
figure	O
illustrates	O
that	O
the	O
diversity	O
loss	O
does	O
lead	O
the	O
output	O
channels	O
to	O
spread	O
out	O
and	O
produce	O
different	O
appearances	O
.	O

section	O
:	O
Semantic	Method
layout	Method
section	O
:	O
GAN	Method
+	Method
semantic	Method
segmenation	Method
Full	Method
-	Method
resolution	Method
network	Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Encoder	Method
-	Method
decoder	Method
Semantic	Method
layout	Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Full	Method
-	Method
resolution	Method
network	Method
Encoder	Method
-	Method
decoder	Method
Figure	O
6	O
.	O

Qualitative	O
comparison	O
on	O
the	O
NYU	Material
dataset	Material
.	O

Figure	O
7	O
.	O

Synthesizing	O
a	O
diverse	O
collection	O
,	O
illustrated	O
on	O
the	O
NYU	Material
dataset	Material
.	O

Each	O
pair	O
shows	O
two	O
images	O
from	O
a	O
collection	O
synthesized	O
for	O
a	O
given	O
semantic	O
layout	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
direct	O
approach	O
to	O
photographic	Task
image	Task
synthesis	Task
conditioned	O
on	O
pixelwise	Method
semantic	Method
layouts	Method
.	O

Images	O
are	O
synthesized	O
by	O
a	O
convolutional	Method
network	Method
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
regression	Method
loss	Method
.	O

This	O
direct	O
approach	O
is	O
considerably	O
simpler	O
than	O
contemporaneous	O
work	O
,	O
and	O
produces	O
much	O
more	O
realistic	O
results	O
.	O

We	O
hope	O
that	O
the	O
simplicity	O
of	O
the	O
presented	O
approach	O
can	O
support	O
follow	O
-	O
up	O
work	O
that	O
will	O
further	O
advance	O
realism	O
and	O
explore	O
the	O
applications	O
of	O
photographic	Task
image	Task
synthesis	Task
.	O

Our	O
results	O
,	O
while	O
significantly	O
more	O
realistic	O
than	O
the	O
prior	O
state	O
of	O
the	O
art	O
,	O
are	O
clearly	O
not	O
indistinguishable	O
from	O
real	O
HD	O
images	O
.	O

Exciting	O
work	O
remains	O
to	O
be	O
done	O
to	O
achieve	O
perfect	O
photorealism	O
.	O

If	O
such	O
level	O
of	O
realism	O
is	O
ever	O
achieved	O
,	O
which	O
we	O
believe	O
to	O
be	O
possible	O
,	O
alternative	O
routes	O
for	O
image	Task
synthesis	Task
in	O
computer	Task
graphics	Task
will	O
open	O
up	O
.	O

section	O
:	O
document	O
:	O
Loss	Method
-	Method
Sensitive	Method
Generative	Method
Adversarial	Method
Networks	Method
on	O
Lipschitz	Method
Densities	Method
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
Lipschitz	Method
regularization	Method
theory	Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	Method
-	Method
Sensitive	Method
Generative	Method
Adversarial	Method
Network	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
.	O

Specifically	O
,	O
it	O
trains	O
a	O
loss	O
function	O
to	O
distinguish	O
between	O
real	O
and	O
fake	O
samples	O
by	O
designated	O
margins	O
,	O
while	O
learning	O
a	O
generator	Method
alternately	O
to	O
produce	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

The	O
LS	Method
-	Method
GAN	Method
further	O
regularizes	O
its	O
loss	Method
function	Method
with	O
a	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
density	O
of	O
real	O
data	O
,	O
yielding	O
a	O
regularized	Method
model	Method
that	O
can	O
better	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
a	O
reasonable	O
number	O
of	O
training	O
examples	O
than	O
the	O
classic	O
GAN	Method
.	O

We	O
will	O
further	O
present	O
a	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	O
-	O
GAN	Method
)	O
and	O
show	O
it	O
contains	O
a	O
large	O
family	O
of	O
regularized	O
GAN	Method
models	O
,	O
including	O
both	O
LS	Method
-	Method
GAN	Method
and	O
Wasserstein	O
GAN	Method
,	O
as	O
its	O
special	O
cases	O
.	O

Compared	O
with	O
the	O
other	O
GAN	Method
models	O
,	O
we	O
will	O
conduct	O
experiments	O
to	O
show	O
both	O
LS	Method
-	Method
GAN	Method
and	O
GLS	Method
-	Method
GAN	Method
exhibit	O
competitive	O
ability	O
in	O
generating	O
new	O
images	O
in	O
terms	O
of	O
the	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
assessed	O
on	O
a	O
separate	O
test	O
set	O
.	O

We	O
further	O
extend	O
the	O
LS	Method
-	Method
GAN	Method
to	O
a	O
conditional	Method
form	Method
for	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
problems	Task
,	O
and	O
demonstrate	O
its	O
outstanding	O
performance	O
on	O
image	O
classification	Task
tasks	Task
.	O

Keywords	O
:	O
Generative	Method
Adversarial	Method
Nets	Method
(	O
GANs	Method
)	O
,	O
Lipschitz	O
regularity	O
,	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
section	O
:	O
Introduction	O
A	O
classic	O
Generative	Method
Adversarial	Method
Net	Method
(	O
GAN	Method
)	O
learns	O
a	O
discriminator	Method
and	O
a	O
generator	Method
by	O
playing	O
a	O
two	Method
-	Method
player	Method
minimax	Method
game	Method
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
discriminator	O
is	O
trained	O
to	O
distinguish	O
real	O
samples	O
from	O
those	O
generated	O
by	O
the	O
generator	O
,	O
and	O
it	O
in	O
turn	O
guides	O
the	O
generator	O
to	O
produce	O
realistic	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

However	O
,	O
from	O
both	O
theoretical	O
and	O
practical	O
perspectives	O
,	O
a	O
critical	O
question	O
is	O
whether	O
the	O
GAN	Method
can	O
generate	O
realistic	O
samples	O
from	O
arbitrary	O
data	O
distribution	O
without	O
any	O
prior	O
?	O
If	O
not	O
,	O
what	O
kind	O
of	O
prior	O
ought	O
to	O
be	O
imposed	O
on	O
the	O
data	O
distribution	O
to	O
regularize	O
the	O
GAN	Method
?	O
Indeed	O
,	O
the	O
classic	O
GAN	Method
imposes	O
no	O
prior	O
on	O
the	O
data	O
distribution	O
.	O

This	O
represents	O
an	O
ambitious	O
goal	O
to	O
generate	O
samples	O
from	O
any	O
distributions	O
.	O

However	O
,	O
it	O
in	O
turn	O
requires	O
a	O
non	Method
-	Method
parametric	Method
discriminator	Method
to	O
prove	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
samples	O
by	O
assuming	O
the	O
model	O
has	O
infinite	O
capacity	O
(	O
see	O
Section	O
4	O
of	O
)	O
.	O

This	O
is	O
a	O
too	O
strong	O
assumption	O
to	O
establish	O
the	O
theoretical	O
basis	O
for	O
the	O
GAN	Method
.	O

Moreover	O
,	O
with	O
such	O
an	O
assumption	O
,	O
its	O
generalizability	O
becomes	O
susceptible	O
.	O

Specifically	O
,	O
one	O
could	O
argue	O
the	O
learned	Method
generator	Method
may	O
be	O
overfit	O
by	O
an	O
unregularized	Method
discriminator	Method
in	O
an	O
non	O
-	O
parametric	O
fashion	O
by	O
merely	O
memorizing	O
or	O
interpolating	O
training	O
examples	O
.	O

In	O
other	O
words	O
,	O
it	O
could	O
lack	O
the	O
generalization	O
ability	O
to	O
generate	O
new	O
samples	O
out	O
of	O
existing	O
data	O
.	O

Indeed	O
,	O
Arora	O
et	O
al	O
.	O

have	O
shown	O
that	O
the	O
GAN	Method
minimizing	O
the	O
Jensen	Metric
-	Metric
Shannon	Metric
distance	Metric
between	O
the	O
distributions	O
of	O
generated	O
and	O
real	O
data	O
could	O
fail	O
to	O
generalize	O
to	O
produce	O
new	O
samples	O
with	O
a	O
reasonable	O
size	O
of	O
training	O
set	O
.	O

Thus	O
,	O
a	O
properly	O
regularized	O
GAN	Method
is	O
demanded	O
to	O
establish	O
provable	O
generalizability	O
by	O
focusing	O
on	O
a	O
restricted	O
yet	O
still	O
sufficiently	O
large	O
family	O
of	O
data	O
distributions	O
.	O

subsection	O
:	O
Objective	O
:	O
Towards	O
Regularized	O
GANs	Method
In	O
this	O
paper	O
,	O
we	O
attempt	O
to	O
develop	O
regularization	Method
theory	Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
.	O

Specifically	O
,	O
we	O
introduce	O
a	O
loss	Method
function	Method
to	O
quantify	O
the	O
quality	Metric
of	Metric
generated	Metric
samples	Metric
.	O

A	O
constraint	O
is	O
imposed	O
so	O
that	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
counterpart	O
.	O

Specifically	O
,	O
in	O
the	O
learning	Method
algorithm	Method
,	O
we	O
will	O
define	O
margins	O
to	O
separate	O
the	O
losses	O
between	O
generated	O
and	O
real	O
samples	O
.	O

Then	O
,	O
an	O
optimal	Method
generator	Method
will	O
be	O
trained	O
to	O
produce	O
realistic	O
samples	O
with	O
minimum	O
losses	O
.	O

The	O
loss	Method
function	Method
and	O
the	O
generator	Method
will	O
be	O
trained	O
in	O
an	O
adversarial	Method
fashion	Method
until	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
real	O
ones	O
.	O

We	O
will	O
also	O
develop	O
new	O
theory	O
to	O
analyze	O
the	O
LS	Method
-	Method
GAN	Method
on	O
the	O
basis	O
of	O
Lipschitz	O
regularity	O
.	O

We	O
note	O
that	O
the	O
reason	O
of	O
making	O
non	Method
-	Method
parametric	Method
assumption	Method
of	Method
infinite	Method
capacity	Method
on	O
the	O
discriminator	Method
in	O
the	O
classic	O
GAN	Method
is	O
due	O
to	O
its	O
ambitious	O
goal	O
to	O
generate	O
data	O
from	O
any	O
arbitrary	O
distribution	O
.	O

However	O
,	O
no	O
free	O
lunch	O
principle	O
reminds	O
us	O
of	O
the	O
need	O
to	O
impose	O
a	O
suitable	O
prior	O
on	O
the	O
data	O
distribution	O
from	O
which	O
real	O
samples	O
are	O
generated	O
.	O

This	O
inspires	O
us	O
to	O
impose	O
a	O
Lipschitz	O
regularity	O
condition	O
by	O
assuming	O
the	O
data	O
density	O
does	O
not	O
change	O
abruptly	O
.	O

Based	O
on	O
this	O
mild	O
condition	O
,	O
we	O
will	O
show	O
that	O
the	O
density	O
of	O
generated	O
samples	O
by	O
LS	Method
-	Method
GAN	Method
can	O
exactly	O
match	O
that	O
of	O
real	O
data	O
.	O

More	O
importantly	O
,	O
the	O
Lipschitz	O
regularity	O
allows	O
us	O
to	O
prove	O
the	O
LS	Method
-	Method
GAN	Method
can	O
well	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
training	O
examples	O
.	O

To	O
this	O
end	O
,	O
we	O
will	O
provide	O
a	O
Probably	Metric
Approximate	Metric
Correct	Metric
(	O
PAC	Metric
)-	O
style	O
theorem	O
by	O
showing	O
the	O
empirical	Method
LS	Method
-	Method
GAN	Method
model	Method
trained	O
with	O
a	O
reasonable	O
number	O
of	O
examples	O
can	O
be	O
sufficiently	O
close	O
to	O
the	O
oracle	Method
LS	Method
-	Method
GAN	Method
trained	O
with	O
hypothetically	O
known	O
data	O
distribution	O
,	O
thereby	O
proving	O
the	O
generalizability	O
of	O
LS	Method
-	Method
GAN	Method
in	O
generating	O
samples	O
from	O
any	O
Lipschitz	O
data	O
distribution	O
.	O

We	O
will	O
also	O
make	O
a	O
non	Method
-	Method
parametric	Method
analysis	Method
of	Method
the	Method
LS	Method
-	Method
GAN	Method
.	O

It	O
does	O
not	O
rely	O
on	O
any	O
parametric	O
form	O
of	O
the	O
loss	O
function	O
to	O
characterize	O
its	O
optimality	O
in	O
the	O
space	O
of	O
Lipschtiz	O
functions	O
.	O

It	O
gives	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	Metric
loss	Metric
,	O
which	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	Task
gradient	Task
.	O

This	O
suggests	O
that	O
the	O
LS	Method
-	Method
GAN	Method
can	O
provide	O
sufficient	O
gradient	O
to	O
update	O
its	O
LS	Method
-	Method
GAN	Method
generator	O
even	O
if	O
the	O
loss	O
function	O
has	O
been	O
fully	O
optimized	O
,	O
thus	O
avoiding	O
the	O
vanishing	Task
gradient	Task
problem	O
that	O
could	O
occur	O
in	O
training	O
the	O
GAN	Method
.	O

subsection	O
:	O
Extensions	O
:	O
Generalized	Method
and	O
Conditional	Method
LS	Method
-	Method
GANs	Method
We	O
further	O
present	O
a	O
generalized	O
form	O
of	O
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
and	O
conduct	O
experiment	O
to	O
demonstrate	O
it	O
has	O
the	O
best	O
generalization	Metric
ability	Metric
.	O

We	O
will	O
show	O
this	O
is	O
not	O
a	O
surprising	O
result	O
as	O
the	O
GLS	Method
-	Method
GAN	Method
contains	O
a	O
large	O
family	O
of	O
regularized	O
GANs	Method
with	O
both	O
LS	Method
-	Method
GAN	Method
and	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
as	O
its	O
special	O
cases	O
.	O

Moreover	O
,	O
we	O
will	O
extend	O
a	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
that	O
can	O
generate	O
samples	O
from	O
given	O
conditions	O
.	O

In	O
particular	O
,	O
with	O
class	O
labels	O
being	O
conditions	O
,	O
the	O
learned	O
loss	O
function	O
can	O
be	O
used	O
as	O
a	O
classifier	Method
for	O
both	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
.	O

The	O
advantage	O
of	O
such	O
a	O
classifier	Method
arises	O
from	O
its	O
ability	O
of	O
exploring	O
generated	O
examples	O
to	O
uncover	O
intrinsic	O
variations	O
for	O
different	O
classes	O
.	O

Experiment	O
results	O
demonstrate	O
competitive	O
performance	O
of	O
the	O
CLS	Method
-	Method
GAN	Method
classifier	Method
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

subsection	O
:	O
Paper	O
Structure	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
reviews	O
the	O
related	O
work	O
,	O
and	O
the	O
proposed	O
LS	Method
-	Method
GAN	Method
is	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
analyze	O
the	O
LS	Method
-	Method
GAN	Method
by	O
proving	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
data	O
with	O
the	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
data	O
distribution	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
discuss	O
the	O
generalizability	Task
problem	Task
arising	O
from	O
using	O
sample	Method
means	Method
to	O
approximate	O
the	O
expectations	O
in	O
the	O
training	O
objectives	O
.	O

We	O
will	O
make	O
a	O
comparison	O
with	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
present	O
a	O
generalized	O
LS	Method
-	Method
GAN	Method
with	O
both	O
WGAN	Method
and	O
LS	Method
-	Method
GAN	Method
as	O
its	O
special	O
cases	O
in	O
Section	O
[	O
reference	O
]	O
.	O

A	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
algorithm	O
is	O
followed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Then	O
we	O
will	O
show	O
how	O
the	O
model	O
can	O
be	O
extended	O
to	O
a	O
conditional	Method
model	Method
for	O
both	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
in	O
Section	O
[	O
reference	O
]	O
.	O

Experiment	O
results	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
we	O
conclude	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Source	O
codes	O
.	O

The	O
source	O
codes	O
for	O
both	O
LS	Method
-	Method
GAN	Method
and	O
GLS	Method
-	Method
GAN	Method
are	O
available	O
at	O
,	O
in	O
the	O
frameworks	O
of	O
torch	O
,	O
pytorch	O
and	O
tensorflow	Method
.	O

LS	Method
-	Method
GAN	Method
is	O
also	O
supported	O
by	O
Microsoft	O
CNTK	O
at	O
.	O

section	O
:	O
Related	O
Work	O
Deep	Method
generative	Method
models	Method
,	O
especially	O
the	O
Generative	O
Adversarial	O
Net	O
(	O
GAN	Method
)	O
,	O
have	O
attracted	O
many	O
attentions	O
recently	O
due	O
to	O
their	O
demonstrated	O
abilities	O
of	O
generating	O
real	O
samples	O
following	O
the	O
underlying	O
data	O
densities	O
.	O

In	O
particular	O
,	O
the	O
GAN	Method
attempts	O
to	O
learn	O
a	O
pair	O
of	O
discriminator	Method
and	Method
generator	Method
by	O
playing	O
a	O
maximin	Method
game	Method
to	O
seek	O
an	O
equilibrium	O
,	O
in	O
which	O
the	O
discriminator	O
is	O
trained	O
by	O
distinguishing	O
real	O
samples	O
from	O
generated	O
ones	O
and	O
the	O
generator	Method
is	O
optimized	O
to	O
produce	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

A	O
family	O
of	O
GAN	Method
architectures	O
have	O
been	O
proposed	O
to	O
implement	O
this	O
idea	O
.	O

For	O
example	O
,	O
recent	O
progresses	O
have	O
shown	O
impressive	O
performances	O
on	O
synthesizing	O
photo	O
-	O
realistic	O
images	O
by	O
constructing	O
multiple	O
strided	Method
and	Method
factional	Method
-	Method
strided	Method
convolutional	Method
layers	Method
for	O
discriminators	Method
and	O
generators	Method
.	O

On	O
the	O
contrary	O
,	O
proposed	O
to	O
use	O
a	O
Laplacian	Method
pyramid	Method
to	O
produce	O
high	O
-	O
quality	O
images	O
by	O
iteratively	O
adding	O
multiple	O
layers	O
of	O
noises	O
at	O
different	O
resolutions	O
.	O

presented	O
to	O
train	O
a	O
recurrent	Method
generative	Method
model	Method
by	O
using	O
adversarial	Method
training	Method
to	O
unroll	O
gradient	Method
-	Method
based	Method
optimizations	Method
to	O
create	O
high	O
quality	O
images	O
.	O

In	O
addition	O
to	O
designing	O
different	O
GAN	Method
networks	O
,	O
research	O
efforts	O
have	O
been	O
made	O
to	O
train	O
the	O
GAN	Method
by	O
different	O
criteria	O
.	O

For	O
example	O
,	O
presented	O
an	O
energy	O
-	O
based	O
GAN	Method
by	O
minimizing	O
an	O
energy	Method
function	Method
to	O
learn	O
an	O
optimal	Method
discriminator	Method
,	O
and	O
an	O
auto	Method
-	Method
encoder	Method
structured	Method
discriminator	Method
is	O
presented	O
to	O
compute	O
the	O
energy	O
.	O

The	O
authors	O
also	O
present	O
a	O
theoretical	O
analysis	O
by	O
showing	O
this	O
variant	O
of	O
GAN	Method
can	O
generate	O
samples	O
whose	O
density	O
can	O
recover	O
the	O
underlying	O
true	O
data	O
density	O
.	O

However	O
,	O
it	O
still	O
needs	O
to	O
assume	O
the	O
discriminator	Method
has	O
infinite	O
modeling	O
capacity	O
to	O
prove	O
the	O
result	O
in	O
a	O
non	O
-	O
parametric	O
fashion	O
,	O
and	O
its	O
generalizability	O
of	O
producing	O
new	O
data	O
out	O
of	O
training	O
examples	O
is	O
unknown	O
without	O
theoretical	O
proof	O
or	O
empirical	O
evidence	O
.	O

In	O
addition	O
,	O
presented	O
to	O
analyze	O
the	O
GAN	Method
from	O
information	Task
theoretical	Task
perspective	Task
,	O
and	O
they	O
seek	O
to	O
minimize	O
the	O
variational	Method
estimate	Method
of	Method
f	Method
-	Method
divergence	Method
,	O
and	O
show	O
that	O
the	O
classic	O
GAN	Method
is	O
included	O
as	O
a	O
special	O
case	O
of	O
f	Method
-	Method
GAN	Method
.	O

In	O
contrast	O
,	O
InfoGAN	Method
proposed	O
another	O
information	O
-	O
theoretic	O
GAN	Method
to	O
learn	O
disentangled	Method
representations	Method
capturing	O
various	O
latent	O
concepts	O
and	O
factors	O
in	O
generating	O
samples	O
.	O

Most	O
recently	O
,	O
propose	O
to	O
minimize	O
the	O
Earth	O
-	O
Mover	O
distance	O
between	O
the	O
density	O
of	O
generated	O
samples	O
and	O
the	O
true	O
data	O
density	O
,	O
and	O
they	O
show	O
the	O
resultant	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	Method
can	O
address	O
the	O
vanishing	Task
gradient	Task
problem	O
that	O
the	O
classic	O
GAN	Method
suffers	O
.	O

Besides	O
the	O
class	O
of	O
GANs	Method
,	O
there	O
exist	O
other	O
models	O
that	O
also	O
attempt	O
to	O
generate	O
natural	O
images	O
.	O

For	O
example	O
,	O
rendered	O
images	O
by	O
matching	O
features	O
in	O
a	O
convolutional	Method
network	Method
with	O
respect	O
to	O
reference	O
images	O
.	O

used	O
deconvolutional	Method
network	Method
to	O
render	O
3D	Method
chair	Method
models	Method
in	O
various	O
styles	O
and	O
viewpoints	O
.	O

introduced	O
a	O
deep	Method
recurrent	Method
neutral	Method
network	Method
architecture	Method
for	O
image	Task
generation	Task
with	O
a	O
sequence	O
of	O
variational	Method
auto	Method
-	Method
encoders	Method
to	O
iteratively	O
construct	O
complex	O
images	O
.	O

Recent	O
efforts	O
have	O
also	O
been	O
made	O
on	O
leveraging	O
the	O
learned	O
representations	O
by	O
deep	Method
generative	Method
networks	Method
to	O
improve	O
the	O
classification	Metric
accuracy	Metric
when	O
it	O
is	O
too	O
difficult	O
or	O
expensive	O
to	O
label	O
sufficient	O
training	O
examples	O
.	O

For	O
example	O
,	O
presented	O
variational	Method
auto	Method
-	Method
encoders	Method
by	O
combining	O
deep	Method
generative	Method
models	Method
and	O
approximate	Method
variational	Method
inference	Method
to	O
explore	O
both	O
labeled	O
and	O
unlabeled	O
data	O
.	O

treated	O
the	O
samples	O
from	O
the	O
GAN	Method
generator	O
as	O
a	O
new	O
class	O
,	O
and	O
explore	O
unlabeled	O
examples	O
by	O
assigning	O
them	O
to	O
a	O
class	O
different	O
from	O
the	O
new	O
one	O
.	O

proposed	O
to	O
train	O
a	O
ladder	Method
network	Method
by	O
minimizing	O
the	O
sum	Metric
of	Metric
supervised	Metric
and	O
unsupervised	Metric
cost	Metric
functions	Metric
through	O
back	Method
-	Method
propagation	Method
,	O
which	O
avoids	O
the	O
conventional	O
layer	Method
-	Method
wise	Method
pre	Method
-	Method
training	Method
approach	Method
.	O

presented	O
an	O
approach	O
to	O
learning	O
a	O
discriminative	Method
classifier	Method
by	O
trading	O
-	O
off	O
mutual	O
information	O
between	O
observed	O
examples	O
and	O
their	O
predicted	O
classes	O
against	O
an	O
adversarial	Method
generative	Method
model	Method
.	O

sought	O
to	O
jointly	O
distinguish	O
between	O
not	O
only	O
real	O
and	O
generated	O
samples	O
but	O
also	O
their	O
latent	O
variables	O
in	O
an	O
adversarial	Method
process	Method
.	O

Recently	O
,	O
presented	O
a	O
novel	O
paradigm	O
of	O
localized	O
GANs	Method
to	O
explore	O
the	O
local	O
consistency	O
of	O
classifiers	O
in	O
local	O
coordinate	O
charts	O
,	O
as	O
well	O
as	O
showed	O
an	O
intrinsic	O
connection	O
with	O
Laplace	O
-	O
Beltrami	O
operator	O
along	O
the	O
manifold	O
.	O

These	O
methods	O
have	O
shown	O
promising	O
results	O
for	O
classification	Task
tasks	Task
by	O
leveraging	O
deep	Method
generative	Method
models	Method
.	O

section	O
:	O
Loss	O
-	O
Sensitive	O
GAN	Method
The	O
classic	O
GAN	Method
consists	O
of	O
two	O
players	O
–	O
a	O
generator	Method
producing	O
samples	O
from	O
random	O
noises	O
,	O
and	O
a	O
discriminator	Method
distinguishing	O
real	O
and	O
fake	O
samples	O
.	O

The	O
generator	Method
and	O
discriminator	Method
are	O
trained	O
in	O
an	O
adversarial	Method
fashion	Method
to	O
reach	O
an	O
equilibrium	O
in	O
which	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
their	O
real	O
counterparts	O
.	O

On	O
the	O
contrary	O
,	O
in	O
the	O
LS	Method
-	Method
GAN	Method
we	O
seek	O
to	O
learn	O
a	O
loss	O
function	O
parameterized	O
with	O
by	O
assuming	O
that	O
a	O
real	O
example	O
ought	O
to	O
have	O
a	O
smaller	O
loss	O
than	O
a	O
generated	O
sample	O
by	O
a	O
desired	O
margin	O
.	O

Then	O
the	O
generator	O
can	O
be	O
trained	O
to	O
generate	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

Formally	O
,	O
consider	O
a	O
generator	Method
function	Method
that	O
produces	O
a	O
sample	O
by	O
transforming	O
a	O
noise	O
input	O
drawn	O
from	O
a	O
simple	O
distribution	Method
such	O
as	O
uniform	Method
and	Method
Gaussian	Method
distributions	Method
.	O

Then	O
for	O
a	O
real	O
example	O
and	O
a	O
generated	O
sample	O
,	O
the	O
loss	Method
function	Method
can	O
be	O
trained	O
to	O
distinguish	O
them	O
with	O
the	O
following	O
constraint	O
:	O
where	O
is	O
the	O
margin	O
measuring	O
the	O
difference	O
between	O
and	O
.	O

This	O
constraint	O
requires	O
a	O
real	O
sample	O
be	O
separated	O
from	O
a	O
generated	O
counterpart	O
in	O
terms	O
of	O
their	O
losses	O
by	O
at	O
least	O
a	O
margin	O
of	O
.	O

The	O
above	O
hard	O
constraint	O
can	O
be	O
relaxed	O
by	O
introducing	O
a	O
nonnegative	O
slack	O
variable	O
that	O
quantifies	O
the	O
violation	O
of	O
the	O
above	O
constraint	O
.	O

This	O
results	O
in	O
the	O
following	O
minimization	Task
problem	Task
to	O
learn	O
the	O
loss	O
function	O
given	O
a	O
fixed	O
generator	O
,	O
where	O
is	O
a	O
positive	O
balancing	O
parameter	O
,	O
and	O
is	O
the	O
data	O
distribution	O
of	O
real	O
samples	O
.	O

The	O
first	O
term	O
minimizes	O
the	O
expected	O
loss	O
function	O
over	O
data	O
distribution	O
since	O
a	O
smaller	O
loss	O
is	O
preferred	O
on	O
real	O
samples	O
.	O

The	O
second	O
term	O
is	O
the	O
expected	Metric
error	Metric
caused	O
by	O
the	O
violation	O
of	O
the	O
constraint	O
.	O

Without	O
loss	O
of	O
generality	O
,	O
we	O
require	O
the	O
loss	O
function	O
should	O
be	O
nonnegative	O
.	O

Given	O
a	O
fixed	O
loss	O
function	O
,	O
on	O
the	O
other	O
hand	O
,	O
one	O
can	O
solve	O
the	O
following	O
minimization	Task
problem	Task
to	O
find	O
an	O
optimal	O
generator	O
.	O

We	O
can	O
use	O
and	O
to	O
denote	O
the	O
density	O
of	O
samples	O
generated	O
by	O
and	O
respectively	O
,	O
with	O
being	O
drawn	O
from	O
.	O

However	O
,	O
for	O
the	O
simplicity	O
of	O
notations	O
,	O
we	O
will	O
use	O
and	O
to	O
denote	O
and	O
without	O
explicitly	O
mentioning	O
and	O
that	O
should	O
be	O
clear	O
in	O
the	O
context	O
.	O

Finally	O
,	O
let	O
us	O
summarize	O
the	O
above	O
objectives	O
.	O

The	O
LS	Method
-	Method
GAN	Method
optimizes	O
and	O
alternately	O
by	O
seeking	O
an	O
equilibrium	O
such	O
that	O
minimizes	O
which	O
is	O
an	O
equivalent	O
form	O
of	O
(	O
[	O
reference	O
]	O
)	O
with	O
,	O
and	O
minimizes	O
In	O
the	O
next	O
section	O
,	O
we	O
will	O
show	O
the	O
consistency	O
between	O
and	O
for	O
LS	Method
-	Method
GAN	Method
.	O

section	O
:	O
Theoretical	Task
Analysis	Task
:	O
Distributional	Task
Consistency	Task
Suppose	O
is	O
a	O
Nash	Method
equilibrium	Method
that	O
jointly	O
solves	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

We	O
will	O
show	O
that	O
as	O
,	O
the	O
density	O
distribution	O
of	O
the	O
samples	O
generated	O
by	O
will	O
converge	O
to	O
the	O
real	O
data	O
density	O
.	O

First	O
,	O
we	O
have	O
the	O
following	O
definition	O
.	O

Definition	O
.	O

For	O
any	O
two	O
samples	O
x	O
and	O
z	O
,	O
the	O
loss	O
function	O
⁢F	O
(	O
x	O
)	O
is	O
Lipschitz	O
continuous	O
with	O
respect	O
to	O
a	O
distance	O
metric	O
Δ	O
if	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
κ	O
,	O
i.e	O
,	O
<	O
κ	O
+	O
∞.	O
To	O
prove	O
our	O
main	O
result	O
,	O
we	O
assume	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

The	O
data	Method
density	Method
P⁢data	Method
is	O
supported	O
in	O
a	O
compact	O
set	O
D	O
,	O
and	O
it	O
is	O
Lipschitz	O
continuous	O
wrt	O
Δ	O
with	O
a	O
bounded	O
constant	O
<	O
κ	O
+	O
∞.	O
The	O
set	O
of	O
Lipschitz	O
densities	O
with	O
a	O
compact	O
support	O
contain	O
a	O
large	O
family	O
of	O
distributions	O
that	O
are	O
dense	O
in	O
the	O
space	O
of	O
continuous	O
densities	O
.	O

For	O
example	O
,	O
the	O
density	O
of	O
natural	O
images	O
are	O
defined	O
over	O
a	O
compact	O
set	O
of	O
pixel	O
values	O
,	O
and	O
it	O
can	O
be	O
consider	O
as	O
Lipschitz	O
continuous	O
,	O
since	O
the	O
densities	O
of	O
two	O
similar	O
images	O
are	O
unlikely	O
to	O
change	O
abruptly	O
at	O
an	O
unbounded	O
rate	O
.	O

If	O
real	O
samples	O
are	O
distributed	O
on	O
a	O
manifold	O
(	O
or	O
is	O
supported	O
in	O
a	O
manifold	O
)	O
,	O
we	O
only	O
require	O
the	O
Lipschitz	O
condition	O
hold	O
on	O
this	O
manifold	O
.	O

This	O
makes	O
the	O
Lipschitz	O
regularity	O
applicable	O
to	O
the	O
data	O
densities	O
on	O
a	O
thin	O
manifold	O
embedded	O
in	O
the	O
ambient	O
space	O
.	O

Let	O
us	O
show	O
the	O
existence	O
of	O
Nash	O
equilibrium	O
such	O
that	O
both	O
the	O
loss	O
function	O
and	O
the	O
density	O
of	O
generated	O
samples	O
are	O
Lipschitz	O
.	O

Let	O
be	O
the	O
class	O
of	O
functions	O
over	O
with	O
a	O
bounded	O
yet	O
sufficiently	O
large	O
Lipschitz	O
constant	O
such	O
that	O
belongs	O
to	O
.	O

It	O
is	O
not	O
difficult	O
to	O
show	O
that	O
the	O
space	O
is	O
convex	O
and	O
compact	O
if	O
its	O
member	O
functions	O
are	O
supported	O
in	O
a	O
compact	O
set	O
.	O

In	O
addition	O
,	O
we	O
note	O
both	O
and	O
are	O
convex	O
in	O
and	O
in	O
.	O

Then	O
,	O
according	O
to	O
the	O
Sion	O
’s	O
theorem	O
,	O
with	O
and	O
being	O
optimized	O
over	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
such	O
that	O
both	O
Lθ	Method
*	Method
and	O
PG	Method
*	Method
are	O
Lipschitz	O
.	O

Now	O
we	O
can	O
prove	O
the	O
main	O
lemma	O
of	O
this	O
paper	O
.	O

The	O
Lipschitz	O
regularity	O
relaxes	O
the	O
strong	O
non	O
-	O
parametric	O
assumption	O
on	O
the	O
GAN	Method
’s	O
discriminator	O
with	O
infinite	O
capacity	O
to	O
the	O
above	O
weaker	O
Lipschitz	O
assumption	O
for	O
the	O
LS	Method
-	Method
GAN	Method
.	O

This	O
allows	O
us	O
to	O
show	O
the	O
following	O
lemma	O
that	O
establishes	O
the	O
distributional	O
consistency	O
between	O
the	O
optimal	O
by	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
–	O
(	O
[	O
reference	O
]	O
)	O
and	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
for	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
in	O
Lemma	O
,	O
we	O
have	O
Thus	O
,	O
⁢PG*	O
(	O
x	O
)	O
converges	O
to	O
⁢P⁢data	O
(	O
x	O
)	O
as	O
→λ	O
+	O
∞.	O
The	O
proof	O
of	O
this	O
lemma	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

By	O
letting	O
λ	O
go	O
infinitely	O
large	O
,	O
the	O
density	O
⁢PG*	O
(	O
x	O
)	O
of	O
generated	O
samples	O
should	O
exactly	O
match	O
the	O
real	O
data	O
density	O
⁢P⁢data	O
(	O
x	O
)	O
.	O

Equivalently	O
,	O
we	O
can	O
simply	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
(	O
)	O
as	O
it	O
plays	O
no	O
role	O
as	O
→λ	O
+	O
∞.	O
Putting	O
the	O
above	O
two	O
lemmas	O
together	O
,	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
Lθ	O
*	O
and	O
PG	O
*	O
are	O
Lipschitz.	O
(	O
ii	O
)	O
∫x⁢|	O
-	O
⁢P⁢data	O
(	O
x	O
)	O
⁢PG*	O
(	O
x	O
)	O
|dx≤2λ→0	O
,	O
as	O
→λ	O
+	O
∞.	O
section	O
:	O
Learning	Task
and	O
Generalizability	O
The	O
minimization	Task
problems	Task
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
can	O
not	O
be	O
solved	O
directly	O
since	O
the	O
expectations	O
over	O
the	O
distributions	O
of	O
true	O
data	O
and	O
noises	O
are	O
unavailable	O
or	O
intractable	O
.	O

Instead	O
,	O
one	O
can	O
approximate	O
them	O
with	O
empirical	Method
means	Method
on	O
a	O
set	O
of	O
finite	O
real	O
examples	O
and	O
noise	O
vectors	O
drawn	O
from	O
and	O
respectively	O
.	O

This	O
results	O
in	O
the	O
following	O
two	O
alternative	O
problems	O
.	O

and	O
where	O
the	O
random	O
vectors	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
different	O
from	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
sample	O
mean	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
computed	O
over	O
pairs	O
randomly	O
drawn	O
from	O
real	O
and	O
generated	O
samples	O
,	O
which	O
is	O
an	O
approximation	O
to	O
the	O
second	O
expectation	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Generalizability	O
We	O
have	O
proved	O
the	O
density	O
of	O
generated	O
samples	O
by	O
the	O
LS	Method
-	Method
GAN	Method
is	O
consistent	O
with	O
the	O
real	O
data	O
density	O
in	O
Theorem	O
[	O
reference	O
]	O
.	O

This	O
consistency	O
is	O
established	O
based	O
on	O
the	O
two	O
oracle	Metric
objectives	Metric
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

However	O
,	O
in	O
practice	O
,	O
the	O
population	O
expectations	O
in	O
these	O
two	O
objectives	O
can	O
not	O
be	O
computed	O
directly	O
over	O
and	O
.	O

Instead	O
,	O
they	O
are	O
approximated	O
in	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
by	O
sample	Method
means	Method
on	O
a	O
finite	O
set	O
of	O
real	O
and	O
generated	O
examples	O
.	O

This	O
raises	O
the	O
question	O
about	O
the	O
generalizability	O
of	O
the	O
LS	Method
-	Method
GAN	Method
model	O
.	O

We	O
wonder	O
,	O
with	O
more	O
training	O
examples	O
,	O
if	O
the	O
empirical	Method
model	Method
trained	O
with	O
finitely	O
many	O
examples	O
can	O
generalize	O
to	O
the	O
oracle	Method
model	Method
.	O

In	O
particular	O
,	O
we	O
wish	O
to	O
estimate	O
the	O
sample	Metric
complexity	Metric
of	O
how	O
many	O
examples	O
are	O
required	O
to	O
sufficiently	O
bound	O
the	O
generalization	O
difference	O
between	O
the	O
empirical	O
and	O
oracle	O
objectives	O
.	O

Arora	O
et	O
al	O
.	O

has	O
proposed	O
a	O
neural	Method
network	Method
distance	Method
to	O
analyze	O
the	O
generalization	Metric
ability	Metric
for	O
the	O
GAN	Method
.	O

However	O
,	O
this	O
neural	Method
network	Method
distance	Method
can	O
not	O
be	O
directly	O
applied	O
here	O
,	O
as	O
it	O
is	O
not	O
related	O
with	O
the	O
objectives	O
that	O
are	O
used	O
to	O
train	O
the	O
LS	Method
-	Method
GAN	Method
.	O

So	O
the	O
generalization	Metric
ability	Metric
in	O
terms	O
of	O
the	O
neural	O
network	O
distance	O
does	O
not	O
imply	O
the	O
LS	Method
-	Method
GAN	Method
could	O
also	O
generalize	O
.	O

Thus	O
,	O
a	O
direct	O
generalization	Method
analysis	Method
of	O
the	O
LS	Method
-	Method
GAN	Method
is	O
required	O
based	O
on	O
its	O
own	O
objectives	O
.	O

First	O
,	O
let	O
us	O
consider	O
the	O
generalization	Task
in	O
terms	O
of	O
.	O

This	O
objective	O
is	O
used	O
to	O
train	O
the	O
loss	Method
function	Method
to	O
distinguish	O
between	O
real	O
and	O
generated	O
samples	O
.	O

Consider	O
the	O
oracle	Metric
objective	Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
population	O
expectations	O
and	O
the	O
empirical	Metric
objective	Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
sample	O
means	O
We	O
need	O
to	O
show	O
if	O
and	O
how	O
fast	O
the	O
difference	O
would	O
eventually	O
vanish	O
as	O
the	O
number	O
of	O
training	O
examples	O
grows	O
.	O

To	O
this	O
end	O
,	O
we	O
need	O
to	O
define	O
the	O
following	O
notations	O
about	O
the	O
model	Metric
complexity	Metric
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
for	O
LS	Method
-	Method
GAN	Method
,	O
the	O
loss	Method
function	Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
the	O
distance	O
between	O
two	O
samples	O
is	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
generalization	O
theorem	O
in	O
a	O
Probably	O
Approximately	O
Correct	O
(	O
PAC	Metric
)	O
style	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
N	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
loss	O
function	O
such	O
that	O
∈θRN	O
.	O

The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

This	O
theorem	O
shows	O
the	O
sample	Metric
complexity	Metric
to	O
bound	O
the	O
difference	O
between	O
and	O
is	O
polynomial	O
in	O
the	O
model	Metric
size	Metric
,	O
as	O
well	O
as	O
both	O
Lipschitz	O
constants	O
and	O
.	O

Similarly	O
,	O
we	O
can	O
establish	O
the	O
generalizability	O
to	O
train	O
the	O
generator	Method
function	Method
by	O
considering	O
the	O
empirical	Metric
objective	Metric
and	O
the	O
oracle	O
objective	O
over	O
empirical	O
and	O
real	O
distributions	O
,	O
respectively	O
.	O

We	O
use	O
the	O
following	O
notions	O
to	O
characterize	O
the	O
complexity	Metric
of	O
the	O
generator	Method
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
The	O
generator	Method
function	Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
Also	O
,	O
we	O
have	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
;	O
The	O
samples	O
’s	O
drawn	O
from	O
are	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
theorem	O
to	O
establish	O
the	O
generalizability	O
of	O
the	O
generator	O
in	O
terms	O
of	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C′	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
M	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
generator	Method
function	Method
such	O
that	O
∈ϕRM	O
.	O

subsection	O
:	O
Bounded	O
Lipschitz	O
Constants	O
for	O
Regularization	Task
Our	O
generalization	Method
theory	Method
in	O
Theorem	O
[	O
reference	O
]	O
conjectures	O
that	O
the	O
required	O
number	O
of	O
training	O
examples	O
is	O
lower	O
bounded	O
by	O
a	O
polynomial	O
of	O
Lipschitz	O
constants	O
and	O
of	O
the	O
loss	O
function	O
wrt	O
and	O
.	O

This	O
suggests	O
us	O
to	O
bound	O
both	O
constants	O
to	O
reduce	O
the	O
sample	Metric
complexity	Metric
of	O
the	O
LS	Method
-	Method
GAN	Method
to	O
improve	O
its	O
generalization	Task
performance	O
.	O

Specifically	O
,	O
bounding	O
the	O
Lipschitz	O
constants	O
and	O
can	O
be	O
implemented	O
by	O
adding	O
two	O
gradient	O
penalties	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
to	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
as	O
the	O
surrogate	O
of	O
the	O
Lipschitz	O
constants	O
.	O

For	O
simplicity	O
,	O
we	O
ignore	O
the	O
second	O
gradient	O
penalty	O
(	O
II	O
)	O
for	O
in	O
experiments	O
,	O
as	O
the	O
sample	Metric
complexity	Metric
is	O
only	O
log	O
-	O
linear	O
in	O
it	O
,	O
whose	O
impact	O
on	O
generalization	Metric
performance	Metric
is	O
negligible	O
compared	O
with	O
that	O
of	O
.	O

Otherwise	O
,	O
penalizing	O
(	O
II	O
)	O
needs	O
to	O
compute	O
its	O
gradient	O
wrt	O
,	O
which	O
is	O
with	O
a	O
Hessian	O
matrix	O
,	O
and	O
this	O
is	O
usually	O
computationally	O
demanding	O
.	O

Note	O
that	O
the	O
above	O
gradient	Method
penalty	Method
differs	O
from	O
that	O
used	O
in	O
that	O
aims	O
to	O
constrain	O
the	O
Lipschitz	O
constant	O
close	O
to	O
one	O
as	O
in	O
the	O
definition	O
of	O
the	O
Wasserstein	O
distance	O
.	O

However	O
,	O
we	O
are	O
motivated	O
to	O
have	O
lower	O
sample	Metric
complexity	Metric
by	O
directly	O
minimizing	O
the	O
Lipschitz	O
constant	O
rather	O
than	O
constraining	O
it	O
to	O
one	O
.	O

Two	O
gradient	Method
penalty	Method
approaches	Method
are	O
thus	O
derived	O
from	O
different	O
theoretical	O
perspectives	O
,	O
and	O
also	O
make	O
practical	O
differences	O
in	O
experiments	O
.	O

section	O
:	O
Wasserstein	O
GAN	Method
and	O
Generalized	Method
LS	Method
-	Method
GAN	Method
In	O
this	O
section	O
,	O
we	O
discuss	O
two	O
issues	O
about	O
LS	Method
-	Method
GAN	Method
.	O

First	O
,	O
we	O
discuss	O
its	O
connection	O
with	O
the	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
,	O
and	O
then	O
show	O
that	O
the	O
WGAN	Method
is	O
a	O
special	O
case	O
of	O
a	O
generalized	O
form	O
of	O
LS	Method
-	Method
GAN	Method
.	O

subsection	O
:	O
Comparison	O
with	O
Wasserstein	O
GAN	Method
We	O
notice	O
that	O
the	O
recently	O
proposed	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
uses	O
the	O
Earth	Method
-	Method
Mover	Method
(	Method
EM	Method
)	Method
distance	Method
to	O
address	O
the	O
vanishing	Task
gradient	Task
and	O
saturated	Task
JS	Task
distance	Task
problems	Task
in	O
the	O
classic	O
GAN	Method
by	O
showing	O
the	O
EM	Method
distance	Method
is	O
continuous	O
and	O
differentiable	O
almost	O
everywhere	O
.	O

While	O
both	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
address	O
these	O
problems	O
from	O
different	O
perspectives	O
that	O
are	O
independently	O
developed	O
almost	O
simultaneously	O
,	O
both	O
turn	O
out	O
to	O
use	O
the	O
Lipschitz	O
regularity	O
in	O
training	O
their	O
GAN	Method
models	O
.	O

This	O
constraint	O
plays	O
vital	O
but	O
different	O
roles	O
in	O
the	O
two	O
models	O
.	O

In	O
the	O
LS	Method
-	Method
GAN	Method
,	O
the	O
Lipschitz	O
regularity	O
naturally	O
arises	O
from	O
the	O
Lipschitz	O
assumption	O
on	O
the	O
data	O
density	O
and	O
the	O
generalization	O
bound	O
.	O

Under	O
this	O
regularity	O
condition	O
,	O
we	O
have	O
proved	O
in	O
Theorem	O
[	O
reference	O
]	O
that	O
the	O
density	O
of	O
generated	O
samples	O
matches	O
the	O
underlying	O
data	O
density	O
.	O

On	O
the	O
contrary	O
,	O
the	O
WGAN	Method
introduces	O
the	O
Lipschitz	O
constraint	O
from	O
the	O
Kantorovich	Method
-	Method
Rubinstein	Method
duality	Method
of	O
the	O
EM	Method
distance	Method
but	O
it	O
is	O
not	O
proved	O
in	O
if	O
the	O
density	O
of	O
samples	O
generated	O
by	O
WGAN	Method
is	O
consistent	O
with	O
that	O
of	O
real	O
data	O
.	O

Here	O
we	O
assert	O
that	O
the	O
WGAN	Method
also	O
models	O
an	O
underlying	O
Lipschitz	Method
density	Method
.	O

To	O
prove	O
this	O
,	O
we	O
restate	O
the	O
WGAN	Method
as	O
follows	O
.	O

The	O
WGAN	Method
seeks	O
to	O
find	O
a	O
critic	Method
and	O
a	O
generator	Method
such	O
that	O
and	O
Let	O
be	O
the	O
density	O
of	O
samples	O
generated	O
by	O
.	O

Then	O
,	O
we	O
prove	O
the	O
following	O
lemma	O
about	O
the	O
WGAN	Method
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
an	O
optimal	O
solution	O
(	O
fw*	O
,	O
gϕ	O
*	O
)	O
to	O
the	O
WGAN	Method
such	O
that	O
Pgϕ	O
*	O
is	O
Lipschitz	O
,	O
we	O
have	O
This	O
lemma	O
shows	O
both	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
are	O
based	O
on	O
the	O
same	O
Lipschitz	O
regularity	O
condition	O
.	O

Although	O
both	O
methods	O
are	O
derived	O
from	O
very	O
different	O
perspectives	O
,	O
it	O
is	O
interesting	O
to	O
make	O
a	O
comparison	O
between	O
their	O
respective	O
forms	O
.	O

Formally	O
,	O
the	O
WGAN	Method
seeks	O
to	O
maximize	O
the	O
difference	O
between	O
the	O
first	O
-	O
order	O
moments	O
of	O
under	O
the	O
densities	O
of	O
real	O
and	O
generated	O
examples	O
.	O

In	O
this	O
sense	O
,	O
the	O
WGAN	Method
can	O
be	O
considered	O
as	O
a	O
kind	O
of	O
first	Method
-	Method
order	Method
moment	Method
method	Method
.	O

Numerically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
tends	O
to	O
be	O
minimized	O
to	O
be	O
arbitrarily	O
small	O
over	O
generated	O
samples	O
,	O
which	O
could	O
make	O
be	O
unbounded	O
above	O
.	O

This	O
is	O
why	O
the	O
WGAN	Method
must	O
be	O
trained	O
by	O
clipping	O
the	O
network	O
weights	O
of	O
on	O
a	O
bounded	O
box	O
to	O
prevent	O
from	O
becoming	O
unbounded	O
above	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	Method
-	Method
GAN	Method
treats	O
real	O
and	O
generated	O
examples	O
in	O
pairs	O
,	O
and	O
maximizes	O
the	O
difference	O
of	O
their	O
losses	O
up	O
to	O
a	O
data	O
-	O
dependant	O
margin	O
.	O

Specifically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
when	O
the	O
loss	O
of	O
a	O
generated	O
sample	O
becomes	O
too	O
large	O
wrt	O
that	O
of	O
a	O
paired	O
real	O
example	O
,	O
the	O
maximization	O
of	O
will	O
stop	O
if	O
the	O
difference	O
exceeds	O
.	O

This	O
prevents	O
the	O
minimization	Task
problem	Task
(	O
[	O
reference	O
]	O
)	O
unbounded	O
below	O
,	O
making	O
it	O
better	O
posed	O
to	O
solve	O
.	O

More	O
importantly	O
,	O
paring	O
real	O
and	O
generated	O
samples	O
in	O
prevents	O
their	O
losses	O
from	O
being	O
decomposed	O
into	O
two	O
separate	O
first	O
-	O
order	O
moments	O
like	O
in	O
the	O
WGAN	Method
.	O

The	O
LS	Method
-	Method
GAN	Method
makes	O
pairwise	O
comparison	O
between	O
the	O
losses	O
of	O
real	O
and	O
generated	O
samples	O
,	O
thereby	O
enforcing	O
real	O
and	O
generated	O
samples	O
to	O
coordinate	O
with	O
each	O
other	O
to	O
learn	O
the	O
optimal	O
loss	O
function	O
.	O

Specifically	O
,	O
when	O
a	O
generated	O
sample	O
becomes	O
close	O
to	O
a	O
paired	O
real	O
example	O
,	O
the	O
LS	Method
-	Method
GAN	Method
will	O
stop	O
increasing	O
the	O
difference	O
between	O
their	O
losses	O
.	O

Below	O
we	O
discuss	O
a	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
model	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
show	O
that	O
both	O
WGAN	Method
and	O
LS	Method
-	Method
GAN	Method
are	O
simply	O
two	O
special	O
cases	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
.	O

subsection	O
:	O
GLS	Method
-	Method
GAN	Method
:	O
Generalized	Method
LS	Method
-	Method
GAN	Method
In	O
proving	O
Lemma	O
[	O
reference	O
]	O
,	O
it	O
is	O
noted	O
that	O
we	O
only	O
have	O
used	O
two	O
properties	O
of	O
in	O
the	O
objective	Metric
function	Metric
training	O
the	O
loss	O
function	O
:	O
1	O
)	O
for	O
any	O
;	O
2	O
)	O
for	O
.	O

This	O
inspires	O
us	O
to	O
generalize	O
the	O
LS	Method
-	Method
GAN	Method
with	O
any	O
alternative	O
cost	Method
function	Method
satisfying	O
these	O
two	O
properties	O
,	O
and	O
this	O
will	O
yield	O
the	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
.	O

We	O
will	O
show	O
that	O
both	O
LS	Method
-	Method
GAN	Method
and	O
WGAN	Method
can	O
be	O
seen	O
as	O
two	O
extreme	O
cases	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
with	O
two	O
properly	O
defined	O
cost	O
functions	O
.	O

Formally	O
,	O
if	O
a	O
cost	O
function	O
satisfies	O
for	O
any	O
and	O
for	O
any	O
,	O
given	O
a	O
fixed	O
generator	O
,	O
we	O
use	O
the	O
following	O
objective	O
to	O
learn	O
,	O
with	O
highlighting	O
its	O
dependency	O
on	O
a	O
chosen	O
cost	O
function	O
.	O

For	O
simplicity	O
,	O
we	O
only	O
involve	O
the	O
second	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
define	O
the	O
generalized	O
objective	O
.	O

But	O
it	O
does	O
not	O
affect	O
the	O
conclusion	O
as	O
the	O
role	O
of	O
the	O
first	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
would	O
vanish	O
with	O
being	O
set	O
to	O
.	O

Following	O
the	O
proof	O
of	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
can	O
prove	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
jointly	O
minimizing	O
⁢SC	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
and	O
⁢T	O
(	O
θ*	O
,	O
ϕ	O
)	O
with	O
a	O
cost	O
function	O
C	O
satisfying	O
the	O
above	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
,	O
we	O
have	O
In	O
particular	O
,	O
we	O
can	O
choose	O
a	O
leaky	Method
rectified	Method
linear	Method
function	Method
for	O
this	O
cost	Method
function	Method
,	O
i.e.	O
,	O
with	O
a	O
slope	O
.	O

As	O
long	O
as	O
,	O
it	O
is	O
easy	O
to	O
verify	O
satisfies	O
these	O
two	O
conditions	O
.	O

Now	O
the	O
LS	Method
-	Method
GAN	Method
is	O
a	O
special	O
case	O
of	O
this	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	O
-	O
GAN	Method
)	O
when	O
,	O
as	O
.	O

We	O
denote	O
this	O
equivalence	O
as	O
LS	Method
-	Method
GAN	Method
=	O
GLS	O
-	O
GAN	Method
(	O
C0	O
)	O
What	O
is	O
more	O
interesting	O
is	O
the	O
WGAN	Method
,	O
an	O
independently	O
developed	O
GAN	Method
model	O
with	O
stable	O
training	O
performance	O
,	O
also	O
becomes	O
a	O
special	O
case	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
with	O
.	O

Indeed	O
,	O
when	O
,	O
,	O
and	O
Since	O
the	O
last	O
term	O
is	O
a	O
const	O
,	O
irrespective	O
of	O
,	O
it	O
can	O
be	O
discarded	O
without	O
affecting	O
optimization	O
over	O
.	O

Thus	O
,	O
we	O
have	O
By	O
comparing	O
this	O
with	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
WGAN	Method
is	O
equivalent	O
to	O
the	O
GLS	Method
-	Method
GAN	Method
with	O
,	O
with	O
the	O
critic	Method
function	Method
being	O
equivalent	O
to	O
.	O

Thus	O
we	O
have	O
WGAN	Method
=	O
GLS	O
-	O
GAN	Method
(	O
C1	O
)	O
Therefore	O
,	O
by	O
varying	O
the	O
slope	O
in	O
,	O
we	O
will	O
obtain	O
a	O
family	O
of	O
the	O
GLS	O
-	O
GANs	Method
with	O
varied	O
beyond	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
.	O

Of	O
course	O
,	O
it	O
is	O
unnecessary	O
to	O
limit	O
to	O
a	O
leaky	Method
rectified	Method
linear	Method
function	Method
.	O

We	O
can	O
explore	O
more	O
cost	O
functions	O
as	O
long	O
as	O
they	O
satisfy	O
the	O
two	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
.	O

In	O
experiments	O
,	O
we	O
will	O
demonstrate	O
the	O
GLS	Method
-	Method
GAN	Method
has	O
competitive	O
generalization	Task
performance	O
on	O
generating	O
new	O
images	O
(	O
c.f	O
.	O

Section	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Non	Method
-	Method
Parametric	Method
Analysis	Method
Now	O
we	O
can	O
characterize	O
the	O
optimal	O
loss	O
functions	O
learned	O
from	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
will	O
provide	O
us	O
an	O
insight	O
into	O
the	O
LS	Method
-	Method
GAN	Method
model	O
.	O

We	O
generalize	O
the	O
non	Method
-	Method
parametric	Method
maximum	Method
likelihood	Method
method	Method
in	O
and	O
consider	O
non	Method
-	Method
parametric	Method
solutions	Method
to	O
the	O
optimal	O
loss	O
function	O
by	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
over	O
the	O
whole	O
class	O
of	O
Lipschitz	O
loss	O
functions	O
.	O

Let	O
,	O
i.e.	O
,	O
the	O
first	O
data	O
points	O
are	O
real	O
examples	O
and	O
the	O
rest	O
are	O
generated	O
samples	O
.	O

Then	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

The	O
following	O
functions	O
^Lθ	O
*	O
and	O
~Lθ	O
*	O
both	O
minimize	O
⁢Sm	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
in	O
Fκ	Method
:	O
with	O
the	O
parameters	O
θ*=	O
[	O
l1*	O
,	O
⋯	O
,	O
l⁢2m*	O
]	O
∈R⁢2	O
m	O
.	O

They	O
are	O
supported	O
in	O
the	O
convex	O
hull	O
of	O
{	O
x	O
(	O
1	O
),	O
⋯	O
,	O
x	O
(	O
⁢2	O
m	O
)	O
}	O
,	O
and	O
we	O
have	O
for	O
=	O
i1	O
,	O
⋯	O
,	O
⁢2	O
m	O
,	O
i.e.	O
,	O
their	O
values	O
coincide	O
on	O
{	O
x	O
(	O
1	O
),	O
x	O
(	O
2	O
),	O
⋯	O
,	O
x	O
(	O
⁢2m	O
)	O
}.	O
The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
the	O
appendix	O
.	O

From	O
the	O
theorem	O
,	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
any	O
convex	O
combination	O
of	O
these	O
two	O
forms	O
attains	O
the	O
same	O
value	O
of	O
,	O
and	O
is	O
also	O
a	O
global	O
minimizer	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

All	O
the	O
functions	O
in	O
minimize	O
Sm	O
in	O
Fκ	O
.	O

This	O
shows	O
that	O
the	O
global	O
minimizer	O
is	O
not	O
unique	O
.	O

Moreover	O
,	O
through	O
the	O
proof	O
of	O
Theorem	O
[	O
reference	O
]	O
,	O
one	O
can	O
find	O
that	O
and	O
are	O
the	O
upper	O
and	O
lower	O
bound	O
of	O
any	O
optimal	Method
loss	Method
function	Method
solution	Method
to	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
.	O

In	O
particular	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

For	O
any	O
∈⁢Lθ*	O
(	O
x	O
)	O
Fκ	O
that	O
minimizes	O
Sm	O
,	O
the	O
corresponding	O
⁢^Lθ*	O
(	O
x	O
)	O
and	O
⁢~Lθ*	O
(	O
x	O
)	O
are	O
the	O
lower	O
and	O
upper	O
bounds	O
of	O
⁢Lθ*	O
(	O
x	O
)	O
,	O
i.e.	O
,	O
The	O
proof	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

The	O
parameters	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
sought	O
by	O
minimizing	O
where	O
is	O
short	O
for	O
,	O
and	O
the	O
constraints	O
are	O
imposed	O
to	O
ensure	O
the	O
learned	O
loss	O
functions	O
stay	O
in	O
.	O

With	O
a	O
greater	O
value	O
of	O
,	O
a	O
larger	O
class	O
of	O
loss	Metric
function	Metric
will	O
be	O
sought	O
.	O

Thus	O
,	O
one	O
can	O
control	O
the	O
modeling	O
ability	O
of	O
the	O
loss	O
function	O
by	O
setting	O
a	O
proper	O
value	O
to	O
.	O

Problem	O
(	O
[	O
reference	O
]	O
)	O
is	O
a	O
typical	O
linear	Task
programming	Task
problem	Task
.	O

In	O
principle	O
,	O
one	O
can	O
solve	O
this	O
problem	O
to	O
obtain	O
a	O
non	Method
-	Method
parametric	Method
loss	Method
function	Method
for	O
the	O
LS	Method
-	Method
GAN	Method
.	O

Unfortunately	O
,	O
it	O
consists	O
of	O
a	O
large	O
number	O
of	O
constraints	O
,	O
whose	O
scale	O
is	O
at	O
an	O
order	O
of	O
.	O

This	O
prevents	O
us	O
from	O
using	O
(	O
[	O
reference	O
]	O
)	O
directly	O
to	O
solve	O
an	O
optimal	O
non	Method
-	Method
parametric	Method
LS	Method
-	Method
GAN	Method
model	Method
with	O
a	O
very	O
large	O
number	O
of	O
training	O
examples	O
.	O

On	O
the	O
contrary	O
,	O
a	O
more	O
tractable	O
solution	O
is	O
to	O
use	O
a	O
parameterized	Method
network	Method
to	O
solve	O
the	O
optimization	Task
problem	Task
(	O
[	O
reference	O
]	O
)	O
constrained	O
in	O
,	O
and	O
iteratively	O
update	O
parameterized	Method
and	Method
with	O
the	O
gradient	Method
descent	Method
method	Method
.	O

Although	O
the	O
non	Method
-	Method
parametric	Method
solution	Method
can	O
not	O
be	O
solved	O
directly	O
,	O
it	O
is	O
valuable	O
in	O
shedding	O
some	O
light	O
on	O
what	O
kind	O
of	O
the	O
loss	O
function	O
would	O
be	O
learned	O
by	O
a	O
deep	Method
network	Method
.	O

It	O
is	O
well	O
known	O
that	O
the	O
training	Task
of	O
the	O
classic	O
GAN	Method
generator	O
suffers	O
from	O
vanishing	Task
gradient	Task
problem	O
as	O
the	O
discriminator	Method
can	O
be	O
optimized	O
very	O
quickly	O
.	O

Recent	O
study	O
has	O
revealed	O
that	O
this	O
is	O
caused	O
by	O
using	O
the	O
Jensen	Method
-	Method
Shannon	Method
(	Method
JS	Method
)	Method
distance	Method
that	O
becomes	O
locally	O
saturated	O
and	O
gets	O
vanishing	Task
gradient	Task
to	O
train	O
the	O
GAN	Method
generator	O
if	O
the	O
discriminator	Method
is	O
over	O
-	O
trained	O
.	O

Similar	O
problem	O
has	O
also	O
been	O
found	O
in	O
the	O
energy	O
-	O
based	O
GAN	Method
(	O
EBGAN	O
)	O
as	O
it	O
minimizes	O
the	O
total	O
variation	O
that	O
is	O
not	O
continuous	O
or	O
(	O
sub	O
-)	O
differentiable	O
if	O
the	O
corresponding	O
discriminator	Method
is	O
fully	O
optimized	O
.	O

On	O
the	O
contrary	O
,	O
as	O
revealed	O
in	O
Theorem	O
[	O
reference	O
]	O
and	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	O
loss	O
function	O
of	O
the	O
LS	Method
-	Method
GAN	Method
are	O
cone	O
-	O
shaped	O
(	O
in	O
terms	O
of	O
that	O
defines	O
the	O
Lipschitz	O
continuity	O
)	O
,	O
and	O
have	O
non	O
-	O
vanishing	Task
gradient	Task
almost	O
everywhere	O
.	O

Moreover	O
,	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
only	O
contains	O
linear	O
objective	O
and	O
constraints	O
;	O
this	O
is	O
contrary	O
to	O
the	O
classic	O
GAN	Method
that	O
involves	O
logistic	O
loss	O
terms	O
that	O
are	O
prone	O
to	O
saturation	O
with	O
vanishing	Task
gradient	Task
.	O

Thus	O
,	O
an	O
optimal	Method
loss	Method
function	Method
that	O
is	O
properly	O
sought	O
in	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
is	O
unlikely	O
to	O
saturate	O
between	O
these	O
two	O
bounds	O
,	O
and	O
it	O
should	O
be	O
able	O
to	O
provide	O
sufficient	O
gradient	O
to	O
update	O
the	O
generator	O
by	O
descending	O
(	O
[	O
reference	O
]	O
)	O
even	O
if	O
it	O
has	O
been	O
trained	O
till	O
optimality	O
.	O

Our	O
experiment	O
also	O
shows	O
that	O
,	O
even	O
if	O
the	O
loss	O
function	O
is	O
quickly	O
trained	O
to	O
optimality	O
,	O
it	O
can	O
still	O
provide	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	Method
in	O
the	O
LS	Method
-	Method
GAN	Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Conditional	Method
LS	Method
-	Method
GAN	Method
The	O
LS	Method
-	Method
GAN	Method
can	O
easily	O
be	O
generalized	O
to	O
produce	O
a	O
sample	O
based	O
on	O
a	O
given	O
condition	O
,	O
yielding	O
a	O
new	O
paradigm	O
of	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
.	O

For	O
example	O
,	O
if	O
the	O
condition	O
is	O
an	O
image	O
class	O
,	O
the	O
CLS	Method
-	Method
GAN	Method
seeks	O
to	O
produce	O
images	O
of	O
the	O
given	O
class	O
;	O
otherwise	O
,	O
if	O
a	O
text	O
description	O
is	O
given	O
as	O
a	O
condition	O
,	O
the	O
model	O
attempts	O
to	O
generate	O
images	O
aligned	O
with	O
the	O
given	O
description	O
.	O

This	O
gives	O
us	O
more	O
flexibility	O
in	O
controlling	O
what	O
samples	O
to	O
be	O
generated	O
.	O

Formally	O
,	O
the	O
generator	O
of	O
CLS	O
-	O
GAN	Method
takes	O
a	O
condition	O
vector	O
as	O
input	O
along	O
with	O
a	O
noise	O
vector	O
to	O
produce	O
a	O
sample	O
.	O

To	O
train	O
the	O
model	O
,	O
we	O
define	O
a	O
loss	Metric
function	Metric
to	O
measure	O
the	O
degree	O
of	O
the	O
misalignment	O
between	O
a	O
data	O
sample	O
and	O
a	O
given	O
condition	O
.	O

For	O
a	O
real	O
example	O
aligned	O
with	O
the	O
condition	O
,	O
its	O
loss	Metric
function	Metric
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
sample	O
by	O
a	O
margin	O
of	O
.	O

This	O
results	O
in	O
the	O
following	O
constraint	O
,	O
Like	O
the	O
LS	Method
-	Method
GAN	Method
,	O
this	O
type	O
of	O
constraint	O
yields	O
the	O
following	O
non	Method
-	Method
zero	Method
-	Method
sum	Method
game	Method
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
which	O
seeks	O
a	O
Nash	O
equilibrium	O
so	O
that	O
minimizes	O
and	O
minimizes	O
where	O
denotes	O
either	O
the	O
joint	O
data	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
or	O
its	O
marginal	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

Playing	O
the	O
above	O
game	O
will	O
lead	O
to	O
a	O
trained	O
pair	O
of	O
loss	O
function	O
and	O
generator	Method
.	O

We	O
can	O
show	O
that	O
the	O
learned	O
generator	Method
can	O
produce	O
samples	O
whose	O
distribution	O
follows	O
the	O
true	O
data	O
density	O
for	O
a	O
given	O
condition	O
.	O

To	O
prove	O
this	O
,	O
we	O
say	O
a	O
loss	O
function	O
is	O
Lipschitz	O
if	O
it	O
is	O
Lipschitz	O
continuous	O
in	O
its	O
first	O
argument	O
.	O

We	O
also	O
impose	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
conditional	O
density	O
.	O

theorem	O
:	O
.	O

For	O
each	O
y	O
,	O
the	O
conditional	O
density	O
P⁢data	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
,	O
and	O
is	O
supported	O
in	O
a	O
convex	O
compact	O
set	O
of	O
x.	O
Then	O
it	O
is	O
not	O
difficult	O
to	O
prove	O
the	O
following	O
theorem	O
,	O
which	O
shows	O
that	O
the	O
conditional	O
density	O
becomes	O
as	O
.	O

Here	O
denotes	O
the	O
density	O
of	O
samples	O
generated	O
by	O
with	O
sampled	O
random	O
noise	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
⁢Lθ*	O
(	O
x	O
,	O
y	O
)	O
is	O
Lipschitz	O
continuous	O
in	O
x	O
for	O
each	O
y;	O
(	O
ii	O
)	O
PG*	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
continuous;	O
(	O
iii	O
)	O
∫x|P⁢data	O
(	O
x|y	O
)-	O
PG*	O
(	O
x|y	O
)	O
|dx≤2λ	O
.	O

In	O
addition	O
,	O
similar	O
upper	O
and	O
lower	O
bounds	O
can	O
be	O
derived	O
to	O
characterize	O
the	O
learned	O
conditional	O
loss	O
function	O
following	O
the	O
same	O
idea	O
for	O
LS	Method
-	Method
GAN	Method
.	O

A	O
useful	O
byproduct	O
of	O
the	O
CLS	Method
-	Method
GAN	Method
is	O
one	O
can	O
use	O
the	O
learned	O
loss	O
function	O
to	O
predict	O
the	O
label	O
of	O
an	O
example	O
by	O
The	O
advantage	O
of	O
such	O
a	O
CLS	Method
-	Method
GAN	Method
classifier	Method
is	O
it	O
is	O
trained	O
with	O
both	O
labeled	O
and	O
generated	O
examples	O
,	O
the	O
latter	O
of	O
which	O
can	O
improve	O
the	O
training	O
of	O
the	O
classifier	Method
by	O
revealing	O
more	O
potential	O
variations	O
within	O
different	O
classes	O
of	O
samples	O
.	O

It	O
also	O
provides	O
a	O
way	O
to	O
evaluate	O
the	O
model	O
based	O
on	O
its	O
classification	Metric
performance	Metric
.	O

This	O
is	O
an	O
objective	O
metric	O
we	O
can	O
use	O
to	O
assess	O
the	O
quality	O
of	O
feature	Method
representations	Method
learned	O
by	O
the	O
model	O
.	O

For	O
a	O
classification	Task
task	Task
,	O
a	O
suitable	O
value	O
should	O
be	O
set	O
to	O
.	O

Although	O
Theorem	O
[	O
reference	O
]	O
shows	O
would	O
converge	O
to	O
the	O
true	O
conditional	O
density	O
by	O
increasing	O
,	O
it	O
only	O
ensures	O
it	O
is	O
a	O
good	O
generative	O
rather	O
than	O
classification	Method
model	Method
.	O

However	O
,	O
a	O
too	O
large	O
value	O
of	O
tends	O
to	O
ignore	O
the	O
first	Method
loss	Method
minimization	Method
term	Method
of	O
(	O
[	O
reference	O
]	O
)	O
that	O
plays	O
an	O
important	O
role	O
in	O
minimizing	Task
classification	Task
error	Task
.	O

Thus	O
,	O
a	O
trade	O
-	O
off	O
should	O
be	O
made	O
to	O
balance	O
between	O
classification	Task
and	Task
generation	Task
objectives	Task
.	O

subsection	O
:	O
Semi	O
-	O
Supervised	O
LS	Method
-	Method
GAN	Method
The	O
above	O
CLS	Method
-	Method
GAN	Method
can	O
be	O
considered	O
as	O
a	O
fully	Method
supervised	Method
model	Method
to	O
classify	O
examples	O
into	O
different	O
classes	O
.	O

It	O
can	O
also	O
be	O
extended	O
to	O
a	O
Semi	Method
-	Method
Supervised	Method
model	Method
by	O
incorporating	O
unlabeled	O
examples	O
.	O

Suppose	O
we	O
have	O
classes	O
indexed	O
by	O
.	O

In	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
for	O
each	O
class	O
,	O
we	O
choose	O
a	O
loss	O
function	O
that	O
,	O
for	O
example	O
,	O
can	O
be	O
defined	O
as	O
the	O
negative	O
log	O
-	O
softmax	O
,	O
where	O
is	O
the	O
th	O
activation	O
output	O
from	O
a	O
network	Method
layer	Method
.	O

Suppose	O
we	O
also	O
have	O
unlabeled	O
examples	O
available	O
,	O
and	O
we	O
can	O
define	O
a	O
new	O
loss	O
function	O
for	O
these	O
unlabeled	O
examples	O
so	O
that	O
they	O
can	O
be	O
involved	O
in	O
training	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

Consider	O
an	O
unlabeled	O
example	O
,	O
its	O
groundtruth	O
label	O
is	O
unknown	O
.	O

However	O
,	O
the	O
best	O
guess	O
of	O
its	O
label	O
can	O
be	O
made	O
by	O
choosing	O
the	O
one	O
that	O
minimizes	O
over	O
,	O
and	O
this	O
inspires	O
us	O
to	O
define	O
the	O
following	O
loss	O
function	O
for	O
the	O
unlabeled	O
example	O
as	O
Here	O
we	O
modify	O
to	O
so	O
can	O
be	O
viewed	O
as	O
the	O
probability	O
that	O
does	O
not	O
belong	O
to	O
any	O
known	O
label	O
.	O

Then	O
we	O
have	O
the	O
following	O
loss	Task
-	Task
sensitive	Task
objective	Task
that	O
explores	O
unlabeled	O
examples	O
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
This	O
objective	O
is	O
combined	O
with	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
train	O
the	O
loss	Method
function	Method
network	Method
by	O
minimizing	O
where	O
is	O
a	O
positive	O
hyperparameter	O
balancing	O
the	O
contributions	O
from	O
labeled	O
and	O
labeled	O
examples	O
.	O

The	O
idea	O
of	O
extending	O
the	O
GAN	Method
for	O
semi	Task
-	Task
supervised	Task
learning	Task
has	O
been	O
proposed	O
by	O
Odena	O
and	O
Salimans	O
et	O
al	O
.	O

,	O
where	O
generated	O
samples	O
are	O
assigned	O
to	O
an	O
artificial	O
class	O
,	O
and	O
unlabeled	O
examples	O
are	O
treated	O
as	O
the	O
negative	O
examples	O
.	O

Our	O
proposed	O
semi	Method
-	Method
supervised	Method
learning	Method
differs	O
in	O
creating	O
a	O
new	O
loss	O
function	O
for	O
unlabeled	O
examples	O
from	O
the	O
losses	O
for	O
existing	O
classes	O
,	O
by	O
minimizing	O
which	O
we	O
make	O
the	O
best	O
guess	O
of	O
the	O
classes	O
of	O
unlabeled	O
examples	O
.	O

The	O
guessed	O
labeled	O
will	O
provide	O
additional	O
information	O
to	O
train	O
the	O
CLS	O
-	O
GAN	Method
model	O
,	O
and	O
the	O
updated	O
model	O
will	O
in	O
turn	O
improve	O
the	O
guess	O
over	O
the	O
training	O
course	O
.	O

The	O
experiments	O
in	O
the	O
following	O
section	O
will	O
show	O
that	O
this	O
approach	O
can	O
generate	O
very	O
competitive	O
performance	O
especially	O
when	O
the	O
labeled	O
data	O
is	O
very	O
limited	O
.	O

section	O
:	O
Experiments	O
Objective	Task
evaluation	Task
of	O
a	O
data	Method
generative	Method
model	Method
is	O
not	O
an	O
easy	O
task	O
as	O
there	O
is	O
no	O
consensus	O
criteria	O
to	O
quantify	O
the	O
quality	O
of	O
generated	O
samples	O
.	O

For	O
this	O
reason	O
,	O
we	O
will	O
make	O
a	O
qualitative	Task
analysis	Task
of	Task
generated	Task
images	Task
,	O
and	O
use	O
image	Task
classification	Task
to	O
quantitatively	O
evaluate	O
the	O
resultant	O
LS	Method
-	Method
GAN	Method
model	O
.	O

First	O
,	O
we	O
will	O
assess	O
the	O
quality	O
of	O
generated	Metric
images	Metric
by	O
the	O
LS	Method
-	Method
GAN	Method
in	O
comparison	O
with	O
the	O
classic	O
GAN	Method
model	O
.	O

Then	O
,	O
we	O
will	O
make	O
an	O
objective	O
evaluation	O
on	O
the	O
CLS	Method
-	Method
GAN	Method
to	O
classify	Task
images	Task
.	O

This	O
task	O
evaluates	O
the	O
quality	O
of	O
feature	Method
representations	Method
learned	O
by	O
the	O
CLS	Method
-	Method
GAN	Method
in	O
terms	O
of	O
its	O
classification	Metric
accuracy	Metric
directly	O
.	O

Finally	O
,	O
we	O
will	O
assess	O
the	O
generalizability	O
of	O
various	O
GAN	Method
models	O
in	O
generating	O
new	O
images	O
out	O
of	O
training	O
examples	O
by	O
proposing	O
the	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
on	O
a	O
separate	O
test	O
set	O
.	O

subsection	O
:	O
Architectures	O
We	O
adopted	O
the	O
ideas	O
behind	O
the	O
network	Method
architecture	Method
for	O
the	O
DCGAN	Method
to	O
build	O
the	O
generator	Method
and	Method
the	Method
loss	Method
function	Method
networks	Method
.	O

Compared	O
with	O
the	O
conventional	O
CNNs	Method
,	O
maxpooling	Method
layers	Method
were	O
replaced	O
with	O
strided	Method
convolutions	Method
in	O
both	O
networks	O
,	O
and	O
fractionally	Method
-	Method
strided	Method
convolutions	Method
were	O
used	O
in	O
the	O
generator	Method
network	Method
to	O
upsample	O
feature	O
maps	O
across	O
layers	O
to	O
finer	O
resolutions	O
.	O

Batch	Method
-	Method
normalization	Method
layers	Method
were	O
added	O
in	O
both	O
networks	O
between	O
convolutional	Method
layers	Method
,	O
and	O
fully	Method
connected	Method
layers	Method
were	O
removed	O
from	O
these	O
networks	O
.	O

However	O
,	O
unlike	O
the	O
DCGAN	Method
,	O
the	O
LS	Method
-	Method
GAN	Method
model	O
(	O
unconditional	O
version	O
in	O
Section	O
[	O
reference	O
]	O
)	O
did	O
not	O
use	O
a	O
sigmoid	O
layer	O
as	O
the	O
output	O
for	O
the	O
loss	Method
function	Method
network	Method
.	O

Instead	O
,	O
we	O
removed	O
it	O
and	O
directly	O
output	O
the	O
activation	O
before	O
the	O
removed	O
sigmoid	Method
layer	Method
.	O

On	O
the	O
other	O
hand	O
,	O
for	O
the	O
loss	Method
function	Method
network	Method
in	O
CLS	Method
-	Method
GAN	Method
,	O
a	O
global	Method
mean	Method
-	Method
pooling	Method
layer	Method
was	O
added	O
on	O
top	O
of	O
convolutional	Method
layers	Method
.	O

This	O
produced	O
a	O
feature	O
map	O
that	O
output	O
the	O
conditional	O
loss	O
on	O
different	O
classes	O
.	O

In	O
the	O
generator	Method
network	Method
,	O
Tanh	Method
was	O
used	O
to	O
produce	O
images	O
whose	O
pixel	O
values	O
are	O
scaled	O
to	O
.	O

Thus	O
,	O
all	O
image	O
examples	O
in	O
datasets	O
were	O
preprocessed	O
to	O
have	O
their	O
pixel	O
values	O
in	O
.	O

More	O
details	O
about	O
the	O
design	O
of	O
network	Method
architectures	Method
can	O
be	O
found	O
in	O
literature	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
network	Method
architecture	Method
for	O
the	O
CLS	O
-	O
GAN	Method
model	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
datasets	Material
in	O
the	O
experiments	O
.	O

In	O
particular	O
,	O
the	O
architecture	O
of	O
the	O
loss	Method
function	Method
network	Method
was	O
adapted	O
from	O
that	O
used	O
in	O
with	O
nine	O
hidden	O
layers	O
.	O

subsection	O
:	O
Training	O
Details	O
The	O
models	O
were	O
trained	O
in	O
a	O
mini	O
-	O
batch	O
of	O
images	O
,	O
and	O
their	O
weights	O
were	O
initialized	O
from	O
a	O
zero	Method
-	Method
mean	Method
Gaussian	Method
distribution	Method
with	O
a	O
standard	O
deviation	O
of	O
.	O

The	O
Adam	Method
optimizer	Method
was	O
used	O
to	O
train	O
the	O
network	O
with	O
initial	O
learning	Metric
rate	Metric
and	O
being	O
set	O
to	O
and	O
respectively	O
,	O
while	O
the	O
learning	Metric
rate	Metric
was	O
annealed	O
every	O
epochs	O
by	O
a	O
factor	O
of	O
.	O

The	O
other	O
hyperparameters	O
such	O
as	O
and	O
were	O
chosen	O
based	O
on	O
an	O
independent	O
validation	O
set	O
held	O
out	O
from	O
training	O
examples	O
.	O

We	O
also	O
tested	O
various	O
forms	O
of	O
loss	O
margins	O
between	O
real	O
and	O
fake	O
samples	O
.	O

For	O
example	O
,	O
we	O
tried	O
the	O
distance	O
between	O
image	O
representations	O
as	O
the	O
margin	O
,	O
and	O
found	O
the	O
best	O
result	O
can	O
be	O
achieved	O
when	O
.	O

The	O
distance	O
between	O
convolutional	O
features	O
was	O
supposed	O
to	O
capture	O
perceptual	O
dissimilarity	O
between	O
images	O
.	O

But	O
we	O
should	O
avoid	O
a	O
direct	O
use	O
of	O
the	O
convolutional	O
features	O
from	O
the	O
loss	Method
function	Method
network	Method
,	O
since	O
we	O
found	O
they	O
would	O
tend	O
to	O
collapse	O
to	O
a	O
trivial	O
point	O
as	O
the	O
loss	O
margin	O
vanishes	O
.	O

The	O
feature	O
maps	O
from	O
a	O
separate	O
pretrained	Method
deep	Method
network	Method
,	O
such	O
as	O
Inception	Method
and	Method
VGG	Method
-	Method
16	Method
networks	Method
,	O
could	O
be	O
a	O
better	O
choice	O
to	O
define	O
the	O
loss	O
margin	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
images	O
generated	O
by	O
LS	Method
-	Method
GAN	Method
on	O
CelebA	O
with	O
the	O
inception	O
and	O
VGG	O
-	O
16	O
margins	O
.	O

However	O
,	O
for	O
a	O
fair	O
comparison	O
,	O
we	O
did	O
not	O
use	O
these	O
external	O
deep	Method
networks	Method
in	O
other	O
experiments	O
on	O
image	Task
generation	Task
and	O
classification	Task
tasks	Task
.	O

We	O
simply	O
used	O
the	O
distance	O
between	O
raw	O
images	O
as	O
the	O
loss	O
margin	O
,	O
and	O
it	O
still	O
achieved	O
competitive	O
results	O
.	O

This	O
demonstrates	O
the	O
robustness	O
of	O
the	O
proposed	O
method	O
without	O
having	O
to	O
choose	O
a	O
sophisticated	O
loss	O
margin	O
.	O

This	O
is	O
also	O
consistent	O
with	O
our	O
theoretical	O
analysis	O
where	O
we	O
do	O
not	O
assume	O
any	O
particular	O
form	O
of	O
loss	O
margin	O
to	O
prove	O
the	O
results	O
.	O

For	O
the	O
generator	Method
network	Method
of	O
LS	Method
-	Method
GAN	Method
,	O
it	O
took	O
a	O
-	O
dimensional	O
random	O
vector	O
drawn	O
from	O
Unif	Method
as	O
input	O
.	O

For	O
the	O
CLS	O
-	O
GAN	Method
generator	O
,	O
an	O
one	Method
-	Method
hot	Method
vector	Method
encoding	O
the	O
image	O
class	O
condition	O
was	O
concatenated	O
with	O
the	O
sampled	O
random	O
vector	O
.	O

The	O
CLS	Method
-	Method
GAN	Method
was	O
trained	O
by	O
involving	O
both	O
unlabeled	O
and	O
labeled	O
examples	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

This	O
was	O
compared	O
against	O
the	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	Method
and	Method
semi	Method
-	Method
supervised	Method
models	Method
.	O

subsection	O
:	O
Generated	O
Images	O
by	O
LS	Method
-	Method
GAN	Method
First	O
we	O
made	O
a	O
qualitative	O
comparison	O
between	O
the	O
images	O
generated	O
by	O
the	O
DCGAN	Method
and	O
the	O
LS	Method
-	Method
GAN	Method
on	O
the	O
celebA	O
dataset	O
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
visual	Metric
quality	Metric
of	Metric
images	Metric
generated	O
by	O
LS	Method
-	Method
GAN	Method
and	O
DCGAN	Method
after	O
they	O
were	O
trained	O
for	O
epochs	O
,	O
and	O
there	O
was	O
no	O
perceptible	O
difference	O
between	O
the	O
qualities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
the	O
DCGAN	Method
architecture	Method
has	O
been	O
exhaustively	O
fine	O
-	O
tuned	O
in	O
terms	O
of	O
the	O
classic	O
GAN	Method
training	O
criterion	O
to	O
maximize	O
the	O
image	Task
generation	Task
performance	O
.	O

It	O
was	O
susceptible	O
that	O
its	O
architecture	O
could	O
be	O
fragile	O
if	O
we	O
make	O
some	O
change	O
to	O
it	O
.	O

Here	O
we	O
tested	O
if	O
the	O
LS	Method
-	Method
GAN	Method
can	O
be	O
more	O
robust	O
than	O
the	O
DCGAN	Method
when	O
a	O
structure	O
change	O
was	O
made	O
.	O

For	O
example	O
,	O
one	O
of	O
the	O
most	O
key	O
components	O
in	O
the	O
DCGAN	Method
is	O
the	O
batch	Method
normalization	Method
inserted	O
between	O
the	O
fractional	Method
convolution	Method
layers	Method
in	O
the	O
generator	Method
network	Method
.	O

It	O
has	O
been	O
reported	O
in	O
literature	O
that	O
the	O
batch	Method
normalization	Method
not	O
only	O
plays	O
a	O
key	O
role	O
in	O
training	O
the	O
DCGAN	Method
model	Method
,	O
but	O
also	O
prevents	O
the	O
mode	O
collapse	O
of	O
the	O
generator	O
into	O
few	O
data	O
points	O
.	O

The	O
results	O
were	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

If	O
one	O
removed	O
the	O
batch	Method
normalization	Method
layers	Method
from	O
the	O
generator	Method
,	O
the	O
DCGAN	Method
would	O
collapse	O
without	O
producing	O
any	O
face	O
images	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	Method
-	Method
GAN	Method
still	O
performed	O
very	O
well	O
even	O
if	O
these	O
batch	Method
normalization	Method
layers	Method
were	O
removed	O
,	O
and	O
there	O
was	O
no	O
perceived	O
deterioration	O
or	O
mode	O
collapse	O
of	O
the	O
generated	O
images	O
.	O

This	O
shows	O
that	O
the	O
LS	Method
-	Method
GAN	Method
was	O
more	O
resilient	O
than	O
the	O
DCGAN	Method
.	O

We	O
also	O
analyzed	O
the	O
magnitude	O
(	O
norm	O
)	O
of	O
the	O
generator	O
’s	O
gradient	O
(	O
in	O
logarithmic	O
scale	O
)	O
in	O
Figure	O
[	O
reference	O
]	O
over	O
iterations	O
.	O

With	O
the	O
loss	O
function	O
being	O
updated	O
every	O
iteration	O
,	O
the	O
generator	Method
was	O
only	O
updated	O
every	O
,	O
,	O
and	O
iterations	O
.	O

From	O
the	O
figure	O
,	O
we	O
note	O
that	O
the	O
magnitude	O
of	O
the	O
generator	O
’s	O
gradient	O
,	O
no	O
matter	O
how	O
frequently	O
the	O
loss	O
function	O
was	O
updated	O
,	O
gradually	O
increased	O
until	O
it	O
stopped	O
at	O
the	O
same	O
level	O
.	O

This	O
implies	O
the	O
objective	O
function	O
to	O
update	O
the	O
generator	Method
tended	O
to	O
be	O
linear	O
rather	O
than	O
saturated	O
through	O
the	O
training	Method
process	Method
,	O
which	O
was	O
consistent	O
with	O
our	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
optimal	Method
loss	Method
function	Method
.	O

Thus	O
,	O
it	O
provided	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	Method
.	O

Furthermore	O
,	O
we	O
compared	O
the	O
images	O
generated	O
with	O
different	O
frequencies	O
of	O
updating	O
the	O
loss	O
function	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
there	O
was	O
no	O
noticeable	O
difference	O
in	O
the	O
visual	Metric
quality	Metric
.	O

This	O
shows	O
the	O
LS	Method
-	Method
GAN	Method
was	O
not	O
affected	O
by	O
over	O
-	O
trained	O
loss	O
function	O
in	O
experiments	O
.	O

subsection	O
:	O
Image	Task
Classification	Task
We	O
conducted	O
experiments	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
to	O
compare	O
the	O
classification	Metric
accuracy	Metric
of	O
LS	Method
-	Method
GAN	Method
with	O
the	O
other	O
approaches	O
.	O

subsubsection	O
:	O
CIFAR	Material
-	Material
10	Material
The	O
CIFAR	Material
dataset	Material
consists	O
of	O
50	O
,	O
000	O
training	O
images	O
and	O
test	O
images	O
on	O
ten	O
image	O
categories	O
.	O

We	O
tested	O
the	O
proposed	O
CLS	O
-	O
GAN	Method
model	O
with	O
class	O
labels	O
as	O
conditions	O
.	O

In	O
the	O
supervised	Task
training	Task
,	O
all	O
labeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

We	O
also	O
conducted	O
experiments	O
with	O
labeled	O
examples	O
per	O
class	O
,	O
which	O
was	O
a	O
more	O
challenging	O
task	O
as	O
much	O
fewer	O
labeled	O
examples	O
were	O
used	O
for	O
training	O
.	O

In	O
this	O
case	O
,	O
the	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
model	O
in	O
a	O
semi	Task
-	Task
supervised	Task
fashion	Task
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
each	O
mini	O
-	O
batch	O
,	O
the	O
same	O
number	O
of	O
labeled	O
and	O
unlabeled	O
examples	O
were	O
used	O
to	O
update	O
the	O
model	O
by	O
stochastic	Method
gradient	Method
descent	Method
.	O

The	O
experiment	O
results	O
on	O
this	O
task	O
were	O
reported	O
by	O
averaging	O
over	O
ten	O
subsets	O
of	O
labeled	O
examples	O
.	O

Both	O
hyperparameters	O
and	O
were	O
chosen	O
via	O
a	O
five	O
-	O
fold	Method
cross	Method
-	Method
validation	Method
on	O
the	O
labeled	O
examples	O
from	O
and	O
respectively	O
.	O

Once	O
they	O
were	O
chosen	O
,	O
the	O
model	O
was	O
trained	O
with	O
the	O
chosen	O
hyperparameters	O
on	O
the	O
whole	O
training	O
set	O
,	O
and	O
the	O
performance	O
was	O
reported	O
based	O
on	O
the	O
results	O
on	O
the	O
test	O
set	O
.	O

As	O
in	O
the	O
improved	O
GAN	Method
,	O
we	O
also	O
adopted	O
the	O
weight	Method
normalization	Method
and	O
feature	Method
matching	Method
mechanisms	Method
for	O
the	O
sake	O
of	O
the	O
fair	O
comparison	O
.	O

We	O
compared	O
the	O
proposed	O
model	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
literature	O
.	O

In	O
particular	O
,	O
we	O
compared	O
with	O
the	O
conditional	O
GAN	Method
as	O
well	O
as	O
the	O
DCGAN	Method
.	O

For	O
the	O
sake	O
of	O
fair	O
comparison	O
,	O
the	O
conditional	O
GAN	Method
shared	O
the	O
same	O
architecture	O
as	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
DCGAN	Method
algorithm	Method
max	O
-	O
pooled	O
the	O
discriminator	O
’s	O
convolution	O
features	O
from	O
all	O
layers	O
to	O
grids	O
as	O
the	O
image	O
features	O
,	O
and	O
a	O
L2	Method
-	Method
SVM	Method
was	O
then	O
trained	O
to	O
classify	O
images	O
.	O

The	O
DCGAN	Method
was	O
an	O
unsupervised	Method
model	Method
which	O
had	O
shown	O
competitive	O
performance	O
on	O
generating	O
photo	O
-	O
realistic	O
images	O
.	O

Its	O
feature	Method
representations	Method
were	O
believed	O
to	O
reach	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
modeling	Task
images	Task
with	O
no	O
supervision	O
.	O

We	O
also	O
compared	O
with	O
the	O
other	O
recently	O
developed	O
supervised	Method
and	Method
semi	Method
-	Method
supervised	Method
models	Method
in	O
literature	O
,	O
including	O
the	O
baseline	Method
1	Method
Layer	Method
K	Method
-	Method
means	Method
feature	Method
extraction	Method
pipeline	Method
,	O
a	O
multi	Method
-	Method
layer	Method
extension	Method
of	O
the	O
baseline	Method
model	Method
(	O
3	O
Layer	Method
K	Method
-	Method
means	Method
Learned	Method
RF	Method
)	O
,	O
View	Method
Invariant	Method
K	Method
-	Method
means	Method
,	O
Examplar	Method
CNN	Method
,	O
Ladder	Method
Network	Method
,	O
as	O
well	O
as	O
CatGAN	Method
.	O

In	O
particular	O
,	O
among	O
the	O
compared	O
semi	Method
-	Method
supervised	Method
algorithms	Method
,	O
the	O
improved	O
GAN	Method
had	O
recorded	O
the	O
best	O
performance	O
in	O
literature	O
.	O

Furthermore	O
,	O
we	O
also	O
compared	O
with	O
the	O
ALI	Method
that	O
extended	O
the	O
classic	O
GAN	Method
by	O
jointly	O
generating	O
data	O
and	O
inferring	O
their	O
representations	O
,	O
which	O
achieved	O
comparable	O
performance	O
to	O
the	O
Improved	O
GAN	Method
.	O

This	O
pointed	O
out	O
an	O
interesting	O
direction	O
to	O
extend	O
the	O
CLS	Method
-	Method
GAN	Method
by	O
directly	O
inferring	O
the	O
data	Method
representation	Method
,	O
and	O
we	O
will	O
leave	O
it	O
in	O
the	O
future	O
work	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
experiment	O
results	O
,	O
showing	O
the	O
CSL	O
-	O
GAN	Method
successfully	O
outperformed	O
the	O
compared	O
algorithms	O
in	O
both	O
fully	Task
-	Task
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
settings	Task
.	O

subsubsection	O
:	O
SVHN	Material
The	O
SVHN	Material
(	O
i.e.	O
,	O
Street	Material
View	Material
House	Material
Number	Material
)	O
dataset	O
contains	O
color	O
images	O
of	O
house	O
numbers	O
collected	O
by	O
Google	O
Street	O
View	O
.	O

They	O
were	O
roughly	O
centered	O
on	O
a	O
digit	O
in	O
a	O
house	O
number	O
,	O
and	O
the	O
objective	O
is	O
to	O
recognize	O
the	O
digit	O
.	O

The	O
training	O
set	O
has	O
digits	O
while	O
the	O
test	O
set	O
consists	O
of	O
.	O

To	O
test	O
the	O
model	O
,	O
labeled	O
digits	O
were	O
used	O
to	O
train	O
the	O
model	O
,	O
which	O
are	O
uniformly	O
selected	O
from	O
ten	O
digit	O
classes	O
,	O
that	O
is	O
labeled	O
examples	O
per	O
digit	O
class	O
.	O

The	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
as	O
additional	O
data	O
to	O
enhance	O
the	O
generative	Method
ability	Method
of	O
CLS	Method
-	Method
GAN	Method
in	O
semi	Task
-	Task
supervised	Task
fashion	Task
.	O

We	O
expect	O
a	O
good	O
generative	Method
model	Method
could	O
produce	O
additional	O
examples	O
to	O
augment	O
the	O
training	O
set	O
.	O

We	O
used	O
the	O
same	O
experiment	O
setup	O
and	O
network	Method
architecture	Method
for	O
CIFAR	Material
-	Material
10	Material
to	O
train	O
the	O
LS	Method
-	Method
GAN	Method
on	O
this	O
dataset	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
result	O
on	O
the	O
SVHN	Material
,	O
and	O
it	O
shows	O
that	O
the	O
LS	Method
-	Method
GAN	Method
performed	O
the	O
best	O
among	O
the	O
compared	O
algorithms	O
.	O

subsubsection	O
:	O
Analysis	Task
of	Task
Generated	Task
Images	Task
by	O
CLS	Method
-	Method
GAN	Method
Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
generated	O
images	O
by	O
CLS	Method
-	Method
GAN	Method
for	O
MNIST	O
,	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
datasets	O
.	O

On	O
each	O
dataset	O
,	O
images	O
in	O
a	O
column	O
were	O
generated	O
for	O
the	O
same	O
class	O
.	O

On	O
the	O
MNIST	O
and	O
the	O
SVHN	Material
,	O
both	O
handwritten	O
and	O
street	O
-	O
view	O
digits	O
are	O
quite	O
legible	O
.	O

Both	O
also	O
cover	O
many	O
variants	O
for	O
each	O
digit	O
class	O
.	O

For	O
example	O
,	O
the	O
synthesized	O
MNIST	O
digits	O
have	O
various	O
writing	O
styles	O
,	O
rotations	O
and	O
sizes	O
,	O
and	O
the	O
generated	O
SVHN	Material
digits	O
have	O
various	O
lighting	O
conditions	O
,	O
sizes	O
and	O
even	O
different	O
co	O
-	O
occurring	O
digits	O
in	O
the	O
cropped	O
bounding	O
boxes	O
.	O

On	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
,	O
image	O
classes	O
can	O
be	O
recognized	O
from	O
the	O
generated	O
images	O
although	O
some	O
visual	O
details	O
are	O
missing	O
.	O

This	O
is	O
because	O
the	O
images	O
in	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
have	O
very	O
low	O
resolution	O
(	O
pixels	O
)	O
,	O
and	O
most	O
details	O
are	O
even	O
missing	O
from	O
input	O
examples	O
.	O

We	O
also	O
observe	O
that	O
if	O
we	O
set	O
a	O
small	O
value	O
to	O
the	O
hyperparameter	O
,	O
the	O
generated	O
images	O
would	O
become	O
very	O
similar	O
to	O
each	O
other	O
within	O
each	O
class	O
.	O

As	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
images	O
were	O
generated	O
by	O
halving	O
used	O
for	O
generating	O
images	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
smaller	O
means	O
a	O
relatively	O
large	O
weight	O
was	O
placed	O
on	O
the	O
first	O
loss	Method
minimization	Method
term	Method
of	O
(	O
[	O
reference	O
]	O
)	O
,	O
which	O
tends	O
to	O
collapse	O
generated	O
images	O
to	O
a	O
single	O
mode	O
as	O
it	O
aggressively	O
minimizes	O
their	O
losses	O
to	O
train	O
the	O
generator	Method
.	O

This	O
is	O
also	O
consistent	O
with	O
Theorem	O
[	O
reference	O
]	O
where	O
the	O
density	O
of	O
generated	O
samples	O
with	O
a	O
smaller	O
could	O
have	O
a	O
larger	O
deviation	O
from	O
the	O
underlying	O
density	O
.	O

One	O
should	O
avoid	O
the	O
collapse	O
of	O
trained	O
generator	Method
since	O
diversifying	O
generated	O
images	O
can	O
improve	O
the	O
classification	Metric
performance	Metric
of	O
the	O
CLS	Method
-	Method
GAN	Method
by	O
revealing	O
more	O
intra	O
-	O
class	O
variations	O
.	O

This	O
will	O
help	O
improve	O
the	O
model	O
’s	O
generalization	Metric
ability	Metric
as	O
these	O
variations	O
could	O
appear	O
in	O
future	O
images	O
.	O

However	O
,	O
one	O
should	O
also	O
avoid	O
setting	O
too	O
large	O
value	O
to	O
.	O

Otherwise	O
,	O
the	O
role	O
of	O
the	O
first	Method
loss	Method
minimization	Method
term	Method
could	O
be	O
underestimated	O
,	O
which	O
can	O
also	O
adversely	O
affect	O
the	O
classification	Task
results	O
without	O
reducing	O
the	O
training	Metric
loss	Metric
to	O
a	O
satisfactory	O
level	O
.	O

Therefore	O
,	O
we	O
choose	O
a	O
proper	O
value	O
for	O
by	O
cross	Method
-	Method
validation	Method
on	O
the	O
training	O
set	O
in	O
the	O
experiments	O
.	O

In	O
brief	O
,	O
the	O
comparison	O
between	O
Figure	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
reveals	O
a	O
trade	O
-	O
off	O
between	O
image	Task
generation	Task
quality	O
and	O
classification	Metric
accuracy	Metric
through	O
the	O
hyperparameter	Method
.	O

Such	O
a	O
trade	O
-	O
off	O
is	O
intuitive	O
:	O
while	O
a	O
classification	Task
task	Task
usually	O
focuses	O
on	O
learning	O
class	Task
-	Task
invariant	Task
representations	Task
that	O
do	O
not	O
change	O
within	O
a	O
class	O
,	O
image	Task
generation	Task
should	O
be	O
able	O
to	O
capture	O
many	O
variant	O
factors	O
(	O
e.g.	O
,	O
lighting	O
conditions	O
,	O
viewing	O
angles	O
,	O
and	O
object	O
poses	O
)	O
so	O
that	O
it	O
could	O
diversify	O
generated	O
samples	O
for	O
each	O
class	O
.	O

Although	O
diversified	O
examples	O
can	O
augment	O
training	O
dataset	O
,	O
it	O
comes	O
at	O
a	O
cost	O
of	O
trading	O
class	O
-	O
invariance	O
for	O
modeling	O
variant	O
generation	O
factors	O
.	O

Perhaps	O
,	O
this	O
is	O
an	O
intrinsic	O
dilemma	O
between	O
supervised	Method
learning	Method
and	O
data	Task
generation	Task
that	O
is	O
worth	O
more	O
theoretical	O
and	O
empirical	O
studies	O
in	O
future	O
.	O

subsection	O
:	O
Evaluation	O
of	O
Generalization	Metric
Performances	Metric
Most	O
of	O
existing	O
metrics	O
like	O
Inception	Metric
Score	Metric
for	O
evaluating	O
GAN	Method
models	O
focus	O
on	O
comparing	O
the	O
qualities	O
and	O
diversities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
even	O
though	O
a	O
GAN	Method
model	O
can	O
produce	O
diverse	O
and	O
high	O
quality	O
images	O
with	O
no	O
collapsed	Method
generators	Method
,	O
it	O
is	O
still	O
unknown	O
if	O
the	O
model	O
can	O
generate	O
unseen	O
images	O
out	O
of	O
given	O
examples	O
,	O
or	O
simply	O
memorizing	O
existing	O
ones	O
.	O

While	O
one	O
of	O
our	O
main	O
pursuits	O
in	O
this	O
paper	O
is	O
a	O
generalizable	O
LS	Method
-	Method
GAN	Method
,	O
we	O
were	O
motivated	O
to	O
propose	O
the	O
following	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
to	O
compare	O
its	O
generalizability	O
with	O
various	O
GANs	Method
.	O

Specifically	O
,	O
for	O
an	O
unseen	O
test	O
image	O
,	O
we	O
aim	O
to	O
find	O
an	O
input	O
noise	O
that	O
can	O
best	O
reconstruct	O
with	O
the	O
smallest	O
error	O
,	O
i.e.	O
,	O
where	O
is	O
the	O
GAN	Method
generator	O
under	O
evaluation	O
.	O

Obviously	O
,	O
if	O
is	O
adequate	O
to	O
produce	O
new	O
images	O
,	O
it	O
should	O
have	O
a	O
small	O
reconstruction	Metric
error	Metric
on	O
a	O
separate	O
test	O
set	O
that	O
has	O
not	O
been	O
used	O
in	O
training	O
the	O
model	O
.	O

We	O
assessed	O
the	O
GAN	Method
’s	O
generalizability	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
tiny	O
ImageNet	O
datasets	O
.	O

On	O
CIFAR	Material
-	Material
10	Material
,	O
we	O
split	O
the	O
dataset	O
into	O
50	O
%	O
training	O
examples	O
,	O
25	O
%	O
validation	O
examples	O
and	O
25	O
%	O
test	O
examples	O
;	O
the	O
tiny	O
ImageNet	O
was	O
split	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
in	O
a	O
ratio	O
of	O
10:1:1	O
.	O

For	O
a	O
fair	O
comparison	O
,	O
all	O
the	O
hyperparameters	O
,	O
including	O
the	O
number	O
of	O
epochs	O
,	O
were	O
chosen	O
based	O
on	O
the	O
average	Metric
MREs	Metric
on	O
the	O
validation	O
set	O
,	O
and	O
the	O
test	O
MREs	Metric
were	O
reported	O
for	O
comparison	O
.	O

The	O
optimal	O
’s	O
were	O
iteratively	O
updated	O
on	O
the	O
validation	O
and	O
test	O
sets	O
by	O
descending	O
the	O
gradient	O
of	O
the	O
reconstruction	O
errors	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
compare	O
the	O
test	O
MREs	Metric
over	O
epochs	O
by	O
LS	Method
-	Method
GAN	Method
,	O
GLS	Method
-	Method
GAN	Method
,	O
WGAN	Method
,	O
WGAN	Method
-	O
GP	O
and	O
DCGAN	Method
on	O
CIFAR	Material
-	Material
10	Material
respectively	O
.	O

For	O
the	O
sake	O
of	O
a	O
fair	O
comparison	O
,	O
all	O
models	O
were	O
trained	O
with	O
the	O
network	Method
architecture	Method
used	O
in	O
.	O

The	O
result	O
clearly	O
shows	O
the	O
regularized	Method
models	Method
,	O
including	O
GLS	Method
-	Method
GAN	Method
,	O
LS	Method
-	Method
GAN	Method
,	O
WGAN	Method
-	O
GP	O
and	O
WGAN	Method
,	O
have	O
apparently	O
better	O
generalization	Metric
performances	Metric
than	O
the	O
unregularized	Method
DCGAN	Method
based	O
on	O
the	O
classic	O
GAN	Method
model	O
.	O

On	O
CIFAR	Material
-	Material
10	Material
,	O
the	O
test	O
MRE	Metric
was	O
reduced	O
from	O
by	O
DCGAN	Method
to	O
as	O
small	O
as	O
and	O
by	O
WGAN	Method
and	O
GLS	O
-	O
GAN	Method
respectively	O
;	O
on	O
tiny	O
ImageNet	O
,	O
the	O
GLS	Method
-	Method
GAN	Method
reaches	O
the	O
smallest	O
test	O
MRE	Metric
of	O
among	O
all	O
compared	O
regularized	O
and	O
unregularized	O
GANs	Method
.	O

In	O
addition	O
,	O
the	O
DCGAN	Method
exhibited	O
fluctuating	O
MREs	Metric
on	O
the	O
CIFAR	Material
-	Material
10	Material
,	O
while	O
the	O
regularized	Method
models	Method
steadily	O
decreased	O
the	O
MREs	Metric
over	O
epochs	O
.	O

This	O
implies	O
regularized	O
GANs	Method
have	O
more	O
stable	O
training	O
than	O
the	O
classic	O
GAN	Method
.	O

We	O
illustrate	O
some	O
examples	O
of	O
reconstructed	O
images	O
by	O
different	O
GANs	Method
on	O
the	O
test	O
set	O
along	O
with	O
their	O
test	O
MREs	Metric
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
results	O
show	O
the	O
GLS	Method
-	Method
GAN	Method
achieved	O
the	O
smallest	O
test	Metric
MRE	Metric
of	O
0.1089	O
and	O
0.2085	O
with	O
a	O
LeakyReLU	O
cost	O
function	O
of	O
slope	O
and	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
tiny	O
ImageNet	O
,	O
followed	O
by	O
the	O
other	O
regularized	O
GAN	Method
models	O
.	O

This	O
is	O
not	O
a	O
surprising	O
result	O
since	O
it	O
has	O
been	O
shown	O
in	O
Section	O
[	O
reference	O
]	O
that	O
the	O
other	O
regularized	O
GANs	Method
such	O
as	O
LS	Method
-	Method
GAN	Method
and	O
WGAN	Method
are	O
only	O
special	O
cases	O
of	O
the	O
GLS	O
-	O
GAN	Method
model	O
that	O
covers	O
larger	O
family	O
of	O
models	O
.	O

Here	O
we	O
only	O
considered	O
LeakyReLU	O
as	O
the	O
cost	Method
function	Method
for	O
GLS	Method
-	Method
GAN	Method
.	O

Of	O
course	O
,	O
there	O
exist	O
many	O
more	O
cost	O
functions	O
satisfying	O
the	O
two	O
conditions	O
in	O
Section	O
[	O
reference	O
]	O
to	O
expand	O
the	O
family	O
of	O
regularized	O
GANs	Method
,	O
which	O
should	O
have	O
potentials	O
of	O
yielding	O
even	O
better	O
generalization	Metric
performances	Metric
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
approach	O
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
LS	Method
-	Method
GAN	Method
learns	O
a	O
loss	Method
function	Method
to	O
distinguish	O
between	O
generated	O
and	O
real	O
samples	O
,	O
where	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
by	O
a	O
margin	O
than	O
that	O
of	O
a	O
generated	O
sample	O
.	O

Our	O
theoretical	O
analysis	O
shows	O
the	O
distributional	O
consistency	O
between	O
the	O
real	O
and	O
generated	O
samples	O
based	O
on	O
the	O
Lipschitz	O
regularity	O
.	O

This	O
no	O
longer	O
needs	O
a	O
non	Method
-	Method
parametric	Method
discriminator	Method
with	O
infinite	O
modeling	O
ability	O
in	O
the	O
classic	O
GAN	Method
,	O
allowing	O
us	O
to	O
search	O
for	O
the	O
optimal	O
loss	O
function	O
in	O
a	O
smaller	O
functional	O
space	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
.	O

Moreover	O
,	O
we	O
prove	O
the	O
generalizability	O
of	O
LS	Method
-	Method
GAN	Method
by	O
showing	O
its	O
required	O
number	O
of	O
training	O
examples	O
is	O
polynomial	O
in	O
its	O
complexity	Metric
.	O

This	O
suggests	O
the	O
generalization	Metric
performance	Metric
can	O
be	O
improved	O
by	O
penalizing	O
the	O
Lipschitz	O
constants	O
(	O
via	O
their	O
gradient	O
surrogates	O
)	O
of	O
the	O
loss	O
function	O
to	O
reduce	O
the	O
sample	Metric
complexity	Metric
.	O

Furthermore	O
,	O
our	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
optimal	Method
loss	Method
function	Method
shows	O
its	O
lower	O
and	O
upper	O
bounds	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	Task
gradient	Task
almost	O
everywhere	O
,	O
implying	O
the	O
generator	Method
can	O
be	O
continuously	O
updated	O
even	O
if	O
the	O
loss	O
function	O
is	O
over	O
-	O
trained	O
.	O

Finally	O
,	O
we	O
extend	O
the	O
LS	Method
-	Method
GAN	Method
to	O
a	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
for	O
semi	Task
-	Task
supervised	Task
tasks	Task
,	O
and	O
demonstrate	O
it	O
reaches	O
competitive	O
performances	O
on	O
both	O
image	Task
generation	Task
and	O
classification	Task
tasks	Task
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
To	O
prove	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
two	O
probability	O
densities	O
⁢p	O
(	O
x	O
)	O
and	O
⁢q	O
(	O
x	O
)	O
,	O
if	O
≥⁢p	O
(	O
x	O
)	O
⁢ηq	O
(	O
x	O
)	O
almost	O
everywhere	O
,	O
we	O
have	O
for	O
∈η	O
(	O
0	O
,	O
1	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

We	O
have	O
the	O
following	O
equalities	O
and	O
inequalities	O
:	O
This	O
completes	O
the	O
proof	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Lemma	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

Suppose	O
is	O
a	O
Nash	O
equilibrium	O
for	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
.	O

We	O
also	O
have	O
for	O
any	O
as	O
minimizes	O
.	O

In	O
particular	O
,	O
we	O
can	O
replace	O
in	O
with	O
,	O
which	O
yields	O
Applying	O
this	O
inequality	O
into	O
(	O
[	O
reference	O
]	O
)	O
leads	O
to	O
where	O
the	O
last	O
inequality	O
follows	O
as	O
is	O
nonnegative	O
.	O

On	O
the	O
other	O
hand	O
,	O
consider	O
a	O
particular	O
loss	Method
function	Method
When	O
is	O
a	O
sufficiently	O
small	O
positive	O
coefficient	O
,	O
is	O
a	O
nonexpansive	O
function	O
(	O
i.e.	O
,	O
a	O
function	O
with	O
Lipschitz	O
constant	O
no	O
larger	O
than	O
.	O

)	O
.	O

This	O
follows	O
from	O
the	O
assumption	O
that	O
and	O
are	O
Lipschitz	O
.	O

In	O
this	O
case	O
,	O
we	O
have	O
By	O
placing	O
this	O
into	O
,	O
one	O
can	O
show	O
that	O
where	O
the	O
first	O
equality	O
uses	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
second	O
equality	O
is	O
obtained	O
by	O
substituting	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
into	O
the	O
equation	O
.	O

Assuming	O
that	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
the	O
above	O
equation	O
would	O
be	O
strictly	O
upper	O
bounded	O
by	O
and	O
we	O
have	O
This	O
results	O
in	O
a	O
contradiction	O
with	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Therefore	O
,	O
we	O
must	O
have	O
for	O
almost	O
everywhere	O
.	O

By	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
have	O
Let	O
,	O
this	O
leads	O
to	O
This	O
proves	O
that	O
converges	O
to	O
as	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
proof	O
:	O
Proof	O
.	O

Suppose	O
a	O
pair	O
of	O
jointly	O
solve	O
the	O
WGAN	Method
problem	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
inequality	O
follows	O
from	O
by	O
replacing	O
with	O
.	O

Consider	O
a	O
particular	O
.	O

Since	O
and	O
are	O
Lipschitz	O
by	O
assumption	O
,	O
when	O
is	O
sufficiently	O
small	O
,	O
it	O
can	O
be	O
shown	O
that	O
.	O

Substituting	O
this	O
into	O
,	O
we	O
get	O
Let	O
us	O
assume	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
we	O
would	O
have	O
This	O
leads	O
to	O
a	O
contradiction	O
with	O
(	O
[	O
reference	O
]	O
)	O
,	O
so	O
we	O
must	O
have	O
almost	O
everywhere	O
.	O

Hence	O
,	O
by	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
prove	O
the	O
conclusion	O
that	O
∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
For	O
simplicity	O
,	O
throughout	O
this	O
section	O
,	O
we	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
and	O
,	O
since	O
the	O
role	O
of	O
the	O
first	O
term	O
would	O
vanish	O
as	O
goes	O
to	O
.	O

However	O
,	O
even	O
if	O
it	O
is	O
involved	O
,	O
the	O
following	O
proof	O
still	O
holds	O
with	O
only	O
some	O
minor	O
changes	O
.	O

To	O
prove	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
all	O
loss	O
functions	O
Lθ	O
,	O
with	O
at	O
least	O
the	O
probability	O
of	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
with	O
a	O
sufficiently	O
large	O
constant	O
C.	O
The	O
proof	O
of	O
this	O
lemma	O
needs	O
to	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
and	O
the	O
fact	O
that	O
is	O
an	O
1	O
-	O
Lipschitz	O
to	O
bound	O
the	O
difference	O
for	O
a	O
loss	O
function	O
.	O

Then	O
,	O
to	O
get	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
,	O
a	O
standard	O
-	O
net	O
will	O
be	O
constructed	O
to	O
yield	O
finite	O
points	O
that	O
are	O
dense	O
enough	O
to	O
cover	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
functions	O
.	O

The	O
proof	O
details	O
are	O
given	O
below	O
.	O

proof	O
:	O
Proof	O
.	O

For	O
a	O
loss	O
function	O
,	O
we	O
compute	O
over	O
a	O
set	O
of	O
samples	O
drawn	O
from	O
and	O
respectively	O
.	O

To	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
,	O
we	O
need	O
to	O
bound	O
the	O
change	O
of	O
this	O
function	O
when	O
a	O
sample	O
is	O
changed	O
.	O

Denote	O
by	O
when	O
the	O
th	O
sample	O
is	O
replaced	O
with	O
and	O
.	O

Then	O
we	O
have	O
where	O
the	O
first	O
inequality	O
uses	O
the	O
fact	O
that	O
is	O
-	O
Lipschitz	O
,	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
bounded	O
by	O
and	O
is	O
-	O
Lipschitz	O
in	O
.	O

Now	O
we	O
can	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
.	O

Noting	O
that	O
we	O
have	O
The	O
above	O
bound	O
applies	O
to	O
a	O
single	O
loss	O
function	O
.	O

To	O
get	O
the	O
union	O
bound	O
,	O
we	O
consider	O
a	O
-	O
net	O
,	O
i.e.	O
,	O
for	O
any	O
,	O
there	O
is	O
a	O
in	O
this	O
net	O
so	O
that	O
.	O

This	O
standard	O
net	O
can	O
be	O
constructed	O
to	O
contain	O
finite	O
loss	O
functions	O
such	O
that	O
,	O
where	O
is	O
the	O
number	O
of	O
parameters	O
in	O
a	O
loss	O
function	O
.	O

Note	O
that	O
we	O
implicitly	O
assume	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
function	O
is	O
bounded	O
so	O
we	O
can	O
construct	O
such	O
a	O
net	O
containing	O
finite	O
points	O
here	O
.	O

Therefore	O
,	O
we	O
have	O
the	O
following	O
union	O
bound	O
for	O
all	O
that	O
,	O
with	O
probability	O
,	O
when	O
.	O

The	O
last	O
step	O
is	O
to	O
obtain	O
the	O
union	O
bound	O
for	O
all	O
loss	O
functions	O
beyond	O
.	O

To	O
show	O
that	O
,	O
we	O
consider	O
the	O
following	O
inequality	O
where	O
the	O
first	O
inequality	O
uses	O
that	O
fact	O
that	O
is	O
-	O
Lipschitz	O
again	O
,	O
and	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
-	O
Lipschitz	O
in	O
.	O

Similarly	O
,	O
we	O
can	O
also	O
show	O
that	O
Now	O
we	O
can	O
derive	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
.	O

For	O
any	O
,	O
by	O
construction	O
we	O
can	O
find	O
a	O
such	O
that	O
.	O

Then	O
,	O
with	O
probability	O
,	O
we	O
have	O
This	O
proves	O
the	O
lemma	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Theorem	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
let	O
us	O
bound	O
.	O

Consider	O
that	O
minimizes	O
.	O

Then	O
with	O
probability	O
,	O
when	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
the	O
inequality	O
as	O
may	O
not	O
minimize	O
,	O
and	O
the	O
second	O
inequality	O
is	O
a	O
direct	O
application	O
of	O
the	O
above	O
lemma	O
.	O

Similarly	O
,	O
we	O
can	O
prove	O
the	O
other	O
direction	O
.	O

With	O
probability	O
,	O
we	O
have	O
Finally	O
,	O
a	O
more	O
rigourous	O
discussion	O
about	O
the	O
generalizability	O
should	O
consider	O
that	O
is	O
updated	O
iteratively	O
.	O

Therefore	O
we	O
have	O
a	O
sequence	O
of	O
generated	O
over	O
iterations	O
for	O
.	O

Thus	O
,	O
a	O
union	O
bound	O
over	O
all	O
generators	O
should	O
be	O
considered	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
makes	O
the	O
required	O
number	O
of	O
training	O
examples	O
become	O
However	O
,	O
the	O
iteration	O
number	O
is	O
usually	O
much	O
smaller	O
than	O
the	O
model	O
size	O
(	O
which	O
is	O
often	O
hundreds	O
of	O
thousands	O
)	O
,	O
and	O
thus	O
this	O
factor	O
will	O
not	O
affect	O
the	O
above	O
lower	O
bound	O
of	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
and	O
Corollary	O
[	O
reference	O
]	O
We	O
prove	O
Theorem	O
[	O
reference	O
]	O
as	O
follows	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
,	O
the	O
existence	O
of	O
a	O
minimizer	O
follows	O
from	O
the	O
fact	O
that	O
the	O
functions	O
in	O
form	O
a	O
compact	O
set	O
,	O
and	O
the	O
objective	O
function	O
is	O
convex	O
.	O

To	O
prove	O
the	O
minimizer	O
has	O
the	O
two	O
forms	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
for	O
each	O
,	O
let	O
us	O
consider	O
It	O
is	O
not	O
hard	O
to	O
verify	O
that	O
and	O
for	O
.	O

Indeed	O
,	O
by	O
noting	O
that	O
has	O
its	O
Lipschitz	O
constant	O
bounded	O
by	O
,	O
we	O
have	O
,	O
and	O
thus	O
Because	O
by	O
the	O
assumption	O
(	O
i.e.	O
,	O
it	O
is	O
lower	O
bounded	O
by	O
zero	O
)	O
,	O
it	O
can	O
be	O
shown	O
that	O
for	O
all	O
Hence	O
,	O
by	O
the	O
definition	O
of	O
and	O
taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
because	O
for	O
any	O
,	O
and	O
it	O
is	O
true	O
in	O
particular	O
for	O
.	O

This	O
shows	O
.	O

Similarly	O
,	O
one	O
can	O
prove	O
.	O

To	O
show	O
this	O
,	O
we	O
have	O
by	O
the	O
Lipschitz	O
continuity	O
of	O
.	O

By	O
taking	O
the	O
minimum	O
over	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
by	O
the	O
definition	O
of	O
.	O

Combining	O
these	O
two	O
inequalities	O
shows	O
that	O
.	O

Now	O
we	O
can	O
prove	O
for	O
any	O
function	O
,	O
there	O
exist	O
and	O
both	O
of	O
which	O
attain	O
the	O
same	O
value	O
of	O
as	O
,	O
since	O
only	O
depends	O
on	O
the	O
values	O
of	O
on	O
the	O
data	O
points	O
.	O

In	O
particular	O
,	O
this	O
shows	O
that	O
any	O
global	O
minimum	O
in	O
of	O
can	O
also	O
be	O
attained	O
by	O
the	O
corresponding	O
functions	O
of	O
the	O
form	O
(	O
[	O
reference	O
]	O
)	O
.	O

By	O
setting	O
for	O
,	O
this	O
completes	O
the	O
proof	O
.	O

∎	O
Finally	O
,	O
we	O
prove	O
Corollary	O
[	O
reference	O
]	O
that	O
bounds	O
with	O
and	O
constructed	O
above	O
.	O

proof	O
:	O
Proof	O
.	O

By	O
the	O
Lipschitz	O
continuity	O
,	O
we	O
have	O
Since	O
,	O
it	O
follows	O
that	O
Taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
obtain	O
This	O
proves	O
the	O
lower	O
bound	O
.	O

Similarly	O
,	O
we	O
have	O
by	O
Lipschitz	O
continuity	O
which	O
,	O
by	O
taking	O
the	O
minimum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
leads	O
to	O
This	O
shows	O
the	O
upper	O
bound	O
.	O

∎	O
document	O
:	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Networks	Method
for	O
Natural	Task
Language	Task
Understanding	Task
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Network	Method
(	O
MT	Method
-	Method
DNN	Method
)	O
for	O
learning	Task
representations	Task
across	O
multiple	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
tasks	O
.	O

MT	Method
-	Method
DNN	Method
not	O
only	O
leverages	O
large	O
amounts	O
of	O
cross	O
-	O
task	O
data	O
,	O
but	O
also	O
benefits	O
from	O
a	O
regularization	O
effect	O
that	O
leads	O
to	O
more	O
general	O
representations	O
to	O
help	O
adapt	O
to	O
new	O
tasks	O
and	O
domains	O
.	O

MT	Method
-	Method
DNN	Method
extends	O
the	O
model	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
a	O
pre	Method
-	Method
trained	Method
bidirectional	Method
transformer	Method
language	Method
model	Method
,	O
known	O
as	O
BERT	Method
bert2018	O
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	Task
tasks	O
,	O
including	O
SNLI	Material
,	O
SciTail	Material
,	O
and	O
eight	O
out	O
of	O
nine	O
GLUE	Material
tasks	O
,	O
pushing	O
the	O
GLUE	Material
benchmark	O
to	O
82.2	O
%	O
(	O
1.8	O
%	O
absolute	O
improvement	O
)	O
.	O

We	O
also	O
demonstrate	O
using	O
the	O
SNLI	Material
and	O
SciTail	Material
datasets	Material
that	O
the	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
allow	O
domain	Task
adaptation	Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	Method
representations	O
.	O

Our	O
code	O
and	O
pre	O
-	O
trained	O
models	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Introduction	O
Learning	O
vector	Method
-	Method
space	Method
representations	Method
of	Method
text	Method
,	O
e.g.	O
,	O
words	O
and	O
sentences	O
,	O
is	O
fundamental	O
to	O
many	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
tasks	O
.	O

Two	O
popular	O
approaches	O
are	O
multi	Task
-	Task
task	Task
learning	Task
and	O
language	Method
model	Method
pre	Method
-	Method
training	Method
.	O

In	O
this	O
paper	O
we	O
strive	O
to	O
combine	O
the	O
strengths	O
of	O
both	O
approaches	O
by	O
proposing	O
a	O
new	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Network	Method
(	O
MT	Method
-	Method
DNN	Method
)	O
.	O

Multi	Task
-	Task
Task	Task
Learning	Task
(	O
MTL	Task
)	Task
is	O
inspired	O
by	O
human	Task
learning	Task
activities	Task
where	O
people	O
often	O
apply	O
the	O
knowledge	O
learned	O
from	O
previous	O
tasks	O
to	O
help	O
learn	O
a	O
new	O
task	O
caruana1997multitask	O
,	O
zhang2017survey	O
.	O

For	O
example	O
,	O
it	O
is	O
easier	O
for	O
a	O
person	O
who	O
knows	O
how	O
to	O
ski	O
to	O
learn	O
skating	O
than	O
the	O
one	O
who	O
does	O
not	O
.	O

Similarly	O
,	O
it	O
is	O
useful	O
for	O
multiple	O
(	O
related	O
)	O
tasks	O
to	O
be	O
learned	O
jointly	O
so	O
that	O
the	O
knowledge	O
learned	O
in	O
one	O
task	O
can	O
benefit	O
other	O
tasks	O
.	O

Recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
applying	O
MTL	Task
to	O
representation	Task
learning	Task
using	O
deep	Method
neural	Method
networks	Method
(	O
DNNs	Method
)	O
collobert2011natural	O
,	O
liu2015mtl	O
,	O
luong2015multi	O
,	O
mt	O
-	O
mrc2018	O
for	O
two	O
reasons	O
.	O

First	O
,	O
supervised	O
learning	O
of	O
DNNs	Method
requires	O
large	O
amounts	O
of	O
task	O
-	O
specific	O
labeled	O
data	O
,	O
which	O
is	O
not	O
always	O
available	O
.	O

MTL	Task
provides	O
an	O
effective	O
way	O
of	O
leveraging	O
supervised	O
data	O
from	O
many	O
related	O
tasks	O
.	O

Second	O
,	O
the	O
use	O
of	O
multi	Method
-	Method
task	Method
learning	Method
profits	O
from	O
a	O
regularization	O
effect	O
via	O
alleviating	O
overfitting	O
to	O
a	O
specific	O
task	O
,	O
thus	O
making	O
the	O
learned	O
representations	O
universal	O
across	O
tasks	O
.	O

In	O
contrast	O
to	O
MTL	Task
,	O
language	Method
model	Method
pre	Method
-	Method
training	Method
has	O
shown	O
to	O
be	O
effective	O
for	O
learning	O
universal	Task
language	Task
representations	Task
by	O
leveraging	O
large	O
amounts	O
of	O
unlabeled	O
data	O
.	O

A	O
recent	O
survey	O
is	O
included	O
in	O
gao2018neural	O
.	O

Some	O
of	O
the	O
most	O
prominent	O
examples	O
are	O
ELMo	Method
elmo2018	O
,	O
GPT	Method
gpt2018	O
and	O
BERT	Method
bert2018	O
.	O

These	O
are	O
neural	Method
network	Method
language	Method
models	Method
trained	O
on	O
text	O
data	O
using	O
unsupervised	O
objectives	O
.	O

For	O
example	O
,	O
BERT	Method
is	O
based	O
on	O
a	O
multi	Method
-	Method
layer	Method
bidirectional	Method
Transformer	Method
,	O
and	O
is	O
trained	O
on	O
plain	O
text	O
for	O
masked	Task
word	Task
prediction	Task
and	O
next	Task
sentence	Task
prediction	Task
tasks	Task
.	O

To	O
apply	O
a	O
pre	Method
-	Method
trained	Method
model	Method
to	O
specific	O
NLU	Task
tasks	O
,	O
we	O
often	O
need	O
to	O
fine	O
-	O
tune	O
,	O
for	O
each	O
task	O
,	O
the	O
model	O
with	O
additional	O
task	O
-	O
specific	O
layers	O
using	O
task	O
-	O
specific	O
training	O
data	O
.	O

For	O
example	O
,	O
bert2018	O
shows	O
that	O
BERT	Method
can	O
be	O
fine	O
-	O
tuned	O
this	O
way	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
range	O
of	O
NLU	Task
tasks	O
,	O
such	O
as	O
question	Task
answering	Task
and	O
natural	Task
language	Task
inference	Task
.	O

We	O
argue	O
that	O
MTL	Task
and	O
language	O
model	O
pre	O
-	O
training	O
are	O
complementary	O
technologies	O
,	O
and	O
can	O
be	O
combined	O
to	O
improve	O
the	O
learning	Task
of	Task
text	Task
representations	Task
to	O
boost	O
the	O
performance	O
of	O
various	O
NLU	Task
tasks	O
.	O

To	O
this	O
end	O
,	O
we	O
extend	O
the	O
MT	Method
-	Method
DNN	Method
model	O
originally	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
BERT	Method
as	O
its	O
shared	Method
text	Method
encoding	Method
layers	Method
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
lower	O
layers	O
(	O
i.e.	O
,	O
text	O
encoding	O
layers	O
)	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
are	O
task	O
-	O
specific	O
,	O
combining	O
different	O
types	O
of	O
NLU	Task
tasks	O
such	O
as	O
single	Task
-	Task
sentence	Task
classification	Task
,	O
pairwise	Task
text	Task
classification	Task
,	O
text	Task
similarity	Task
,	O
and	O
relevance	Task
ranking	Task
.	O

Similar	O
to	O
the	O
BERT	Method
model	Method
,	O
MT	Method
-	Method
DNN	Method
is	O
trained	O
in	O
two	O
stages	O
:	O
pre	Method
-	Method
training	Method
and	O
fine	Task
-	Task
tuning	Task
.	O

Unlike	O
BERT	Method
,	O
MT	Method
-	Method
DNN	Method
uses	O
MTL	Task
in	O
the	O
fine	Method
-	Method
tuning	Method
stage	Method
with	O
multiple	O
task	O
-	O
specific	O
layers	O
in	O
its	O
model	Method
architecture	Method
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
out	O
of	O
nine	O
NLU	Task
tasks	O
used	O
in	O
the	O
General	Task
Language	Task
Understanding	Task
Evaluation	Task
(	O
GLUE	Material
)	O
benchmark	O
wang2018glue	O
,	O
pushing	O
the	O
GLUE	Material
benchmark	O
score	O
to	O
82.2	O
%	O
,	O
amounting	O
to	O
1.8	O
%	O
absolute	O
improvement	O
over	O
BERT	Method
.	O

We	O
further	O
extend	O
the	O
superiority	O
of	O
MT	Method
-	Method
DNN	Method
to	O
the	O
SNLI	Material
and	O
SciTail	Material
tasks	O
.	O

The	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
allow	O
domain	Task
adaptation	Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	Method
representations	O
.	O

For	O
example	O
,	O
our	O
adapted	O
models	O
achieve	O
the	O
accuracy	Metric
of	O
91.1	O
%	O
on	O
SNLI	Material
and	O
94.1	O
%	O
on	O
SciTail	Material
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
by	O
1.0	O
%	O
and	O
5.8	O
%	O
,	O
respectively	O
.	O

Even	O
with	O
only	O
0.1	O
%	O
or	O
1.0	O
%	O
of	O
the	O
original	O
training	O
data	O
,	O
the	O
performance	O
of	O
MT	Method
-	Method
DNN	Method
on	O
both	O
SNLI	Material
and	O
SciTail	Material
datasets	O
is	O
fairly	O
good	O
and	O
much	O
better	O
than	O
many	O
existing	O
models	O
.	O

All	O
of	O
these	O
clearly	O
demonstrate	O
MT	Method
-	Method
DNN	Method
’s	O
exceptional	O
generalization	Method
capability	Method
via	O
multi	Method
-	Method
task	Method
learning	Method
.	O

section	O
:	O
Tasks	O
The	O
MT	Method
-	Method
DNN	Method
model	O
combines	O
four	O
types	O
of	O
NLU	Task
tasks	O
:	O
single	Task
-	Task
sentence	Task
classification	Task
,	O
pairwise	Task
text	Task
classification	Task
,	O
text	Task
similarity	Task
scoring	Task
,	O
and	O
relevance	Task
ranking	Task
.	O

For	O
concreteness	O
,	O
we	O
describe	O
them	O
using	O
the	O
NLU	Task
tasks	O
defined	O
in	O
the	O
GLUE	Material
benchmark	O
as	O
examples	O
.	O

paragraph	O
:	O
Single	O
-	O
Sentence	Task
Classification	Task
:	O
Given	O
a	O
sentence	O
,	O
the	O
model	O
labels	O
it	O
using	O
one	O
of	O
the	O
pre	O
-	O
defined	O
class	O
labels	O
.	O

For	O
example	O
,	O
the	O
CoLA	Task
task	Task
is	O
to	O
predict	O
whether	O
an	O
English	Material
sentence	Material
is	O
grammatically	O
plausible	O
.	O

The	O
SST	Material
-	Material
2	Material
task	O
is	O
to	O
determine	O
whether	O
the	O
sentiment	O
of	O
a	O
sentence	O
extracted	O
from	O
movie	O
reviews	O
is	O
positive	O
or	O
negative	O
.	O

paragraph	O
:	O
Text	Metric
Similarity	Metric
:	O
This	O
is	O
a	O
regression	Task
task	Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
predicts	O
a	O
real	O
-	O
value	O
score	O
indicating	O
the	O
semantic	O
similarity	O
of	O
the	O
two	O
sentences	O
.	O

STS	Method
-	Method
B	Method
is	O
the	O
only	O
example	O
of	O
the	O
task	O
in	O
GLUE	Material
.	O

paragraph	O
:	O
Pairwise	Task
Text	Task
Classification	Task
:	O
Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
determines	O
the	O
relationship	O
of	O
the	O
two	O
sentences	O
based	O
on	O
a	O
set	O
of	O
pre	O
-	O
defined	O
labels	O
.	O

For	O
example	O
,	O
both	O
RTE	Method
and	O
MNLI	Material
are	O
language	Task
inference	Task
tasks	Task
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
a	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
other	O
.	O

QQP	Material
and	O
MRPC	Material
are	O
paragraph	O
datasets	O
that	O
consist	O
of	O
sentence	O
pairs	O
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
.	O

paragraph	O
:	O
Relevance	Task
Ranking	Task
:	O
Given	O
a	O
query	O
and	O
a	O
list	O
of	O
candidate	O
answers	O
,	O
the	O
model	O
ranks	O
all	O
the	O
candidates	O
in	O
the	O
order	O
of	O
relevance	O
to	O
the	O
query	O
.	O

QNLI	Material
is	O
a	O
version	O
of	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
rajpurkar2016squad	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
sentence	O
contains	O
the	O
correct	O
answer	O
to	O
a	O
given	O
query	O
.	O

Although	O
QNLI	Material
is	O
defined	O
as	O
a	O
binary	Task
classification	Task
task	Task
in	O
GLUE	Material
,	O
in	O
this	O
study	O
we	O
formulate	O
it	O
as	O
a	O
pairwise	Task
ranking	Task
task	Task
,	O
where	O
the	O
model	O
is	O
expected	O
to	O
rank	O
the	O
candidate	O
that	O
contains	O
the	O
correct	O
answer	O
higher	O
than	O
the	O
candidate	O
that	O
does	O
not	O
.	O

We	O
will	O
show	O
that	O
this	O
formulation	O
leads	O
to	O
a	O
significant	O
improvement	O
in	O
accuracy	Metric
over	O
binary	Task
classification	Task
.	O

section	O
:	O
The	O
Proposed	O
MT	Method
-	Method
DNN	Method
Model	O
The	O
architecture	O
of	O
the	O
MT	Method
-	Method
DNN	Method
model	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
lower	O
layers	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
represent	O
task	O
-	O
specific	O
outputs	O
.	O

The	O
input	O
,	O
which	O
is	O
a	O
word	O
sequence	O
(	O
either	O
a	O
sentence	O
or	O
a	O
pair	O
of	O
sentences	O
packed	O
together	O
)	O
is	O
first	O
represented	O
as	O
a	O
sequence	O
of	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
word	O
,	O
in	O
.	O

Then	O
the	O
transformer	Method
encoder	Method
captures	O
the	O
contextual	O
information	O
for	O
each	O
word	O
via	O
self	O
-	O
attention	O
,	O
and	O
generates	O
a	O
sequence	O
of	O
contextual	O
embeddings	O
in	O
.	O

This	O
is	O
the	O
shared	Method
semantic	Method
representation	Method
that	O
is	O
trained	O
by	O
our	O
multi	Task
-	Task
task	Task
objectives	Task
.	O

In	O
what	O
follows	O
,	O
we	O
elaborate	O
on	O
the	O
model	O
in	O
detail	O
.	O

paragraph	O
:	O
Lexicon	Method
Encoder	Method
(	O
)	O
:	O
The	O
input	O
is	O
a	O
sequence	O
of	O
tokens	O
of	O
length	O
.	O

Following	O
bert2018	O
,	O
the	O
first	O
token	O
is	O
always	O
the	O
[	O
CLS	O
]	O
token	O
.	O

If	O
is	O
packed	O
by	O
a	O
sentence	O
pair	O
,	O
we	O
separate	O
the	O
two	O
sentences	O
with	O
a	O
special	O
token	O
[	O
SEP	O
]	O
.	O

The	O
lexicon	Method
encoder	Method
maps	O
into	O
a	O
sequence	O
of	O
input	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
token	O
,	O
constructed	O
by	O
summing	O
the	O
corresponding	O
word	O
,	O
segment	O
,	O
and	O
positional	O
embeddings	O
.	O

paragraph	O
:	O
Transformer	Method
Encoder	Method
(	O
)	O
:	O
We	O
use	O
a	O
multi	Method
-	Method
layer	Method
bidirectional	Method
Transformer	Method
encoder	Method
vaswani2017attention	O
to	O
map	O
the	O
input	O
representation	O
vectors	O
(	O
)	O
into	O
a	O
sequence	O
of	O
contextual	O
embedding	O
vectors	O
.	O

This	O
is	O
the	O
shared	O
representation	O
across	O
different	O
tasks	O
.	O

Unlike	O
the	O
BERT	Method
model	Method
bert2018	O
that	O
learns	O
the	O
representation	O
via	O
pre	Method
-	Method
training	Method
and	O
adapts	O
it	O
to	O
each	O
individual	O
task	O
via	O
fine	Method
-	Method
tuning	Method
,	O
MT	Method
-	Method
DNN	Method
learns	O
the	O
representation	O
using	O
multi	O
-	O
task	O
objectives	O
.	O

paragraph	O
:	O
Single	Task
-	Task
Sentence	Task
Classification	Task
Output	O
:	O
Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
the	O
token	O
[	O
CLS	O
]	O
,	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	Method
representation	Method
of	O
input	O
sentence	O
.	O

Take	O
the	O
SST	Material
-	Material
2	Material
task	O
as	O
an	O
example	O
.	O

The	O
probability	O
that	O
is	O
labeled	O
as	O
class	O
(	O
i.e.	O
,	O
the	O
sentiment	O
)	O
is	O
predicted	O
by	O
a	O
logistic	Method
regression	Method
with	O
softmax	Method
:	O
where	O
is	O
the	O
task	O
-	O
specific	O
parameter	O
matrix	O
.	O

paragraph	O
:	O
Text	Metric
Similarity	Metric
Output	O
:	O
Take	O
the	O
STS	Task
-	Task
B	Task
task	Task
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
[	O
CLS	O
]	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	Method
representation	Method
of	O
the	O
input	O
sentence	O
pair	O
.	O

We	O
introduce	O
a	O
task	O
-	O
specific	O
parameter	O
vector	O
to	O
compute	O
the	O
similarity	Metric
score	Metric
as	O
:	O
where	O
is	O
a	O
sigmoid	Method
function	Method
that	O
maps	O
the	O
score	O
to	O
a	O
real	O
value	O
of	O
the	O
range	O
.	O

paragraph	O
:	O
Pairwise	Metric
Text	Metric
Classification	Metric
Output	O
:	O
Take	O
natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
as	O
an	O
example	O
.	O

The	O
NLI	Task
task	Task
defined	O
here	O
involves	O
a	O
premise	O
of	O
words	O
and	O
a	O
hypothesis	O
of	O
words	O
,	O
and	O
aims	O
to	O
find	O
a	O
logical	O
relationship	O
between	O
and	O
.	O

The	O
design	O
of	O
the	O
output	Method
module	Method
follows	O
the	O
answer	Method
module	Method
of	O
the	O
stochastic	Method
answer	Method
network	Method
(	O
SAN	Method
)	O
liu2018san4nli	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
NLI	Method
model	Method
.	O

SAN	Method
’s	O
answer	O
module	O
uses	O
multi	Method
-	Method
step	Method
reasoning	Method
.	O

Rather	O
than	O
directly	O
predicting	O
the	O
entailment	O
given	O
the	O
input	O
,	O
it	O
maintains	O
a	O
state	O
and	O
iteratively	O
refines	O
its	O
predictions	O
.	O

The	O
SAN	Method
answer	O
module	O
works	O
as	O
follows	O
.	O

We	O
first	O
construct	O
the	O
working	O
memory	O
of	O
premise	O
by	O
concatenating	O
the	O
contextual	O
embeddings	O
of	O
the	O
words	O
in	O
,	O
which	O
are	O
the	O
output	O
of	O
the	O
transformer	Method
encoder	Method
,	O
denoted	O
as	O
,	O
and	O
similarly	O
the	O
working	O
memory	O
of	O
hypothesis	Method
,	O
denoted	O
as	O
.	O

Then	O
,	O
we	O
perform	O
-	Method
step	Method
reasoning	Method
on	O
the	O
memory	O
to	O
output	O
the	O
relation	O
label	O
,	O
where	O
is	O
a	O
hyperparameter	O
.	O

At	O
the	O
beginning	O
,	O
the	O
initial	O
state	O
is	O
the	O
summary	O
of	O
:	O
,	O
where	O
.	O

At	O
time	O
step	O
in	O
the	O
range	O
of	O
,	O
the	O
state	O
is	O
defined	O
by	O
.	O

Here	O
,	O
is	O
computed	O
from	O
the	O
previous	O
state	O
and	O
memory	O
:	O
and	O
.	O

A	O
one	Method
-	Method
layer	Method
classifier	Method
is	O
used	O
to	O
determine	O
the	O
relation	O
at	O
each	O
step	O
:	O
At	O
last	O
,	O
we	O
utilize	O
all	O
of	O
the	O
outputs	O
by	O
averaging	O
the	O
scores	O
:	O
Each	O
is	O
a	O
probability	O
distribution	O
over	O
all	O
the	O
relations	O
.	O

During	O
training	Task
,	O
we	O
apply	O
stochastic	Method
prediction	Method
dropout	Method
liu2018san	O
before	O
the	O
above	O
averaging	Method
operation	Method
.	O

During	O
decoding	Task
,	O
we	O
average	O
all	O
outputs	O
to	O
improve	O
robustness	Metric
.	O

paragraph	O
:	O
Relevance	Metric
Ranking	Metric
Output	O
:	O
Take	O
QNLI	Material
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
vector	O
of	O
[	O
CLS	Method
]	Method
which	O
is	O
the	O
semantic	Method
representation	Method
of	O
a	O
pair	O
of	O
question	O
and	O
its	O
candidate	O
answer	O
.	O

We	O
compute	O
the	O
relevance	Metric
score	Metric
as	O
:	O
For	O
a	O
given	O
,	O
we	O
rank	O
all	O
of	O
its	O
candidate	O
answers	O
based	O
on	O
their	O
relevance	O
scores	O
computed	O
using	O
Equation	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
The	O
Training	O
Procedure	O
The	O
training	Method
procedure	Method
of	O
MT	Method
-	Method
DNN	Method
consists	O
of	O
two	O
stages	O
:	O
pretraining	Task
and	O
multi	Task
-	Task
task	Task
fine	Task
-	Task
tuning	Task
.	O

The	O
pretraining	O
stage	O
follows	O
that	O
of	O
the	O
BERT	Method
model	O
bert2018	O
.	O

The	O
parameters	O
of	O
the	O
lexicon	Method
encoder	Method
and	O
Transformer	Method
encoder	Method
are	O
learned	O
using	O
two	O
unsupervised	Task
prediction	Task
tasks	Task
:	O
masked	Task
language	Task
modeling	Task
and	O
next	Task
sentence	Task
prediction	Task
.	O

In	O
the	O
multi	Task
-	Task
task	Task
fine	Task
-	Task
tuning	Task
stage	Task
,	O
we	O
use	O
mini	Method
-	Method
batch	Method
based	Method
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
to	O
learn	O
the	O
parameters	O
of	O
our	O
model	O
(	O
i.e.	O
,	O
the	O
parameters	O
of	O
all	O
shared	O
layers	O
and	O
task	O
-	O
specific	O
layers	O
)	O
as	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

In	O
each	O
epoch	O
,	O
a	O
mini	O
-	O
batch	O
is	O
selected	O
(	O
e.g	O
.	O

,	O
among	O
all	O
9	O
GLUE	Material
tasks	O
)	O
,	O
and	O
the	O
model	O
is	O
updated	O
according	O
to	O
the	O
task	O
-	O
specific	O
objective	O
for	O
the	O
task	O
.	O

This	O
approximately	O
optimizes	O
the	O
sum	O
of	O
all	O
multi	O
-	O
task	O
objectives	O
.	O

[	O
ht	O
!	O

]	O
Initialize	O
model	O
parameters	O
randomly	O
.	O

Pre	O
-	O
train	O
the	O
shared	Method
layers	Method
(	O
i.e.	O
,	O
the	O
lexicon	Method
encoder	Method
and	O
the	O
transformer	Method
encoder	Method
)	O
.	O

Set	O
the	O
max	O
number	O
of	O
epoch	O
:	O
.	O

//	O
Prepare	O
the	O
data	O
for	O
T	O
tasks	O
.	O

in	O
Pack	O
the	O
dataset	O
into	O
mini	O
-	O
batch	O
:	O
.	O

in	O
1	O
.	O

Merge	O
all	O
the	O
datasets	O
:	O
2	O
.	O

Shuffle	Method
in	O
D	O
//	O
bt	O
is	O
a	O
mini	O
-	O
batch	O
of	O
task	O
t.	O
3	O
.	O

Compute	O
loss	Metric
:	O
Eq	O
.	O

[	O
reference	O
]	O
for	O
classification	Task
Eq	Task
.	O

[	O
reference	O
]	O
for	O
regression	Task
Eq	Task
.	O

[	O
reference	O
]	O
for	O
ranking	Task
4	O
.	O

Compute	O
gradient	O
:	O
5	O
.	O

Update	Method
model	Method
:	O
Training	O
a	O
MT	Method
-	Method
DNN	Method
model	O
.	O

For	O
the	O
classification	Task
tasks	Task
(	O
i.e.	O
,	O
single	Task
-	Task
sentence	Task
or	Task
pairwise	Task
text	Task
classification	Task
)	O
,	O
we	O
use	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
as	O
the	O
objective	O
:	O
where	O
is	O
the	O
binary	O
indicator	O
(	O
0	O
or	O
1	O
)	O
if	O
class	O
label	O
is	O
the	O
correct	O
classification	O
for	O
,	O
and	O
is	O
defined	O
by	O
e.g.	O
,	O
Equation	O
[	O
reference	O
]	O
or	O
[	O
reference	O
]	O
.	O

For	O
the	O
text	Task
similarity	Task
tasks	Task
,	O
such	O
as	O
STS	Task
-	Task
B	Task
,	O
where	O
each	O
sentence	O
pair	O
is	O
annotated	O
with	O
a	O
real	O
-	O
valued	O
score	O
,	O
we	O
use	O
the	O
mean	Metric
squared	Metric
error	Metric
as	O
the	O
objective	O
:	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
objective	O
for	O
the	O
relevance	Task
ranking	Task
tasks	Task
follows	O
the	O
pairwise	Method
learning	Method
-	Method
to	Method
-	Method
rank	Method
paradigm	Method
learning	Method
-	O
to	O
-	O
rank2005burges	O
,	O
huang2013dssm	O
.	O

Take	O
QNLI	Material
as	O
an	O
example	O
.	O

Given	O
a	O
query	O
,	O
we	O
obtain	O
a	O
list	O
of	O
candidate	O
answers	O
which	O
contains	O
a	O
positive	O
example	O
that	O
includes	O
the	O
correct	O
answer	O
,	O
and	O
negative	O
examples	O
.	O

We	O
then	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
positive	O
example	O
given	O
queries	O
across	O
the	O
training	O
data	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
and	O
is	O
a	O
tuning	O
factor	O
determined	O
on	O
held	O
-	O
out	O
data	O
.	O

In	O
our	O
experiment	O
,	O
we	O
simply	O
set	O
to	O
1	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
MT	Method
-	Method
DNN	Method
on	O
three	O
popular	O
NLU	Task
benchmarks	O
:	O
GLUE	Material
,	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	O
,	O
and	O
SciTail	Material
.	O

We	O
compare	O
MT	Method
-	Method
DNN	Method
with	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
including	O
BERT	Method
and	O
demonstrate	O
the	O
effectiveness	O
of	O
MTL	Task
for	O
model	Task
fine	Task
-	Task
tuning	Task
using	O
GLUE	Material
and	O
domain	Method
adaptation	Method
using	O
SNLI	Material
and	O
SciTail	Material
.	O

subsection	O
:	O
Datasets	O
This	O
section	O
briefly	O
describes	O
the	O
GLUE	Material
,	O
SNLI	Material
,	O
and	O
SciTail	Material
datasets	Material
,	O
as	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
GLUE	Material
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
NLU	Task
tasks	O
,	O
including	O
question	Task
answering	Task
,	O
sentiment	Task
analysis	Task
,	O
and	O
textual	Task
entailment	Task
;	O
it	O
is	O
considered	O
well	O
-	O
designed	O
for	O
evaluating	O
the	O
generalization	Metric
and	Metric
robustness	Metric
of	O
NLU	Task
models	O
.	O

Both	O
SNLI	Material
and	O
SciTail	Material
are	O
NLI	Task
tasks	Task
.	O

paragraph	O
:	O
CoLA	O
The	O
Corpus	Task
of	Task
Linguistic	Task
Acceptability	Task
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
âacceptableâ	O
or	O
not	O
.	O

It	O
uses	O
Matthews	Metric
correlation	Metric
coefficient	Metric
as	O
the	O
evaluation	Metric
metric	Metric
.	O

paragraph	O
:	O
SST	Material
-	Material
2	Material
The	O
Stanford	Material
Sentiment	Material
Treebank	Material
is	O
to	O
determine	O
the	O
sentiment	O
of	O
sentences	O
.	O

The	O
sentences	O
are	O
extracted	O
from	O
movie	O
reviews	O
with	O
human	O
annotations	O
of	O
their	O
sentiment	O
.	O

Accuracy	Metric
is	O
used	O
as	O
the	O
evaluation	Metric
metric	Metric
.	O

paragraph	O
:	O
STS	Material
-	Material
B	Material
The	O
Semantic	Task
Textual	Task
Similarity	Task
Benchmark	Task
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
collected	O
from	O
multiple	O
data	O
resources	O
including	O
news	O
headlines	O
,	O
video	O
,	O
and	O
image	O
captions	O
,	O
and	O
NLI	Material
data	Material
.	O

Each	O
pair	O
is	O
human	O
-	O
annotated	O
with	O
a	O
similarity	Metric
score	Metric
from	O
one	O
to	O
five	O
,	O
indicating	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
.	O

The	O
task	O
is	O
evaluated	O
using	O
two	O
metrics	O
:	O
the	O
Pearson	Metric
and	Metric
Spearman	Metric
correlation	Metric
coefficients	Metric
.	O

paragraph	O
:	O
QNLI	Material
This	O
is	O
derived	O
from	O
the	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
rajpurkar2016squad	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	Task
classification	Task
task	Task
in	O
GLUE	Material
.	O

A	O
query	O
-	O
candidate	O
-	O
answer	O
tuple	O
is	O
labeled	O
as	O
positive	O
if	O
the	O
candidate	O
contains	O
the	O
correct	O
answer	O
to	O
the	O
query	O
and	O
negative	O
otherwise	O
.	O

In	O
this	O
study	O
,	O
however	O
,	O
we	O
formulate	O
QNLI	Material
as	O
a	O
relevance	Task
ranking	Task
task	Task
,	O
where	O
for	O
a	O
given	O
query	O
,	O
its	O
positive	O
candidate	O
answers	O
are	O
considered	O
more	O
relevant	O
,	O
and	O
thus	O
should	O
be	O
ranked	O
higher	O
than	O
its	O
negative	O
candidates	O
.	O

paragraph	O
:	O
QQP	Material
The	O
Quora	Material
Question	Material
Pairs	Material
dataset	Material
is	O
a	O
collection	O
of	O
question	O
pairs	O
extracted	O
from	O
the	O
community	Material
question	Material
-	Material
answering	Material
website	Material
Quora	Material
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
two	O
questions	O
are	O
semantically	O
equivalent	O
.	O

As	O
the	O
distribution	O
of	O
positive	O
and	O
negative	O
labels	O
is	O
unbalanced	O
,	O
both	O
accuracy	Metric
and	O
F1	Metric
score	Metric
are	O
used	O
as	O
evaluation	Metric
metrics	Metric
.	O

paragraph	O
:	O
MRPC	Material
The	O
Microsoft	Material
Research	Material
Paraphrase	Material
Corpus	Material
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
with	O
human	O
annotations	O
denoting	O
whether	O
a	O
sentence	O
pair	O
is	O
semantically	O
equivalent	O
to	O
the	O
other	O
in	O
the	O
pair	O
.	O

Similar	O
to	O
QQP	Material
,	O
both	O
accuracy	Metric
and	O
F1	Metric
score	Metric
are	O
used	O
as	O
evaluation	Metric
metrics	Metric
.	O

paragraph	O
:	O
MNLI	Material
Multi	Task
-	Task
Genre	Task
Natural	Task
Language	Task
Inference	Task
is	O
a	O
large	Task
-	Task
scale	Task
,	Task
crowd	Task
-	Task
sourced	Task
entailment	Task
classification	Task
task	Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
(	O
i.e.	O
,	O
a	O
premise	O
-	O
hypothesis	O
pair	O
)	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
hypothesis	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
premise	O
.	O

The	O
test	O
and	O
development	O
sets	O
are	O
split	O
into	O
in	O
-	O
domain	O
(	O
matched	Metric
)	O
and	O
cross	O
-	O
domain	O
(	O
mismatched	Metric
)	O
sets	O
.	O

The	O
evaluation	Metric
metric	Metric
is	O
accuracy	Metric
.	O

paragraph	O
:	O
RTE	O
The	O
Recognizing	Material
Textual	Material
Entailment	Material
dataset	Material
is	O
collected	O
from	O
a	O
series	O
of	O
annual	O
challenges	O
on	O
textual	Task
entailment	Task
.	O

The	O
task	O
is	O
similar	O
to	O
MNLI	Material
,	O
but	O
uses	O
only	O
two	O
labels	O
:	O
entailment	O
and	O
not_entailment	O
.	O

paragraph	O
:	O
WNLI	Material
The	O
Winograd	Material
NLI	Material
(	O
WNLI	Material
)	O
is	O
a	O
natural	Task
language	Task
inference	Task
dataset	O
derived	O
from	O
the	O
Winograd	Material
Schema	Material
dataset	Material
.	O

This	O
is	O
a	O
reading	Task
comprehension	Task
task	Task
.	O

The	O
goal	O
is	O
to	O
select	O
the	O
referent	O
of	O
a	O
pronoun	O
from	O
a	O
list	O
of	O
choices	O
in	O
a	O
given	O
sentence	O
which	O
contains	O
the	O
pronoun	O
.	O

paragraph	O
:	O
SNLI	Material
The	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	O
dataset	O
contains	O
570k	O
human	O
annotated	O
sentence	O
pairs	O
,	O
in	O
which	O
the	O
premises	O
are	O
drawn	O
from	O
the	O
captions	O
of	O
the	O
Flickr30	Material
corpus	Material
and	O
hypotheses	O
are	O
manually	O
annotated	O
.	O

This	O
is	O
the	O
most	O
widely	O
used	O
entailment	O
dataset	O
for	O
NLI	Task
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	Task
adaptation	Task
in	O
this	O
study	O
.	O

paragraph	O
:	O
SciTail	Material
This	O
is	O
a	O
textual	O
entailment	O
dataset	O
derived	O
from	O
a	O
science	Material
question	Material
answering	Material
(	O
SciQ	Material
)	O
dataset	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
given	O
premise	O
entails	O
a	O
given	O
hypothesis	O
.	O

In	O
contrast	O
to	O
other	O
entailment	O
datasets	O
mentioned	O
previously	O
,	O
the	O
hypotheses	O
in	O
SciTail	Material
are	O
created	O
from	O
science	O
questions	O
while	O
the	O
corresponding	O
answer	O
candidates	O
and	O
premises	O
come	O
from	O
relevant	O
web	O
sentences	O
retrieved	O
from	O
a	O
large	O
corpus	O
.	O

As	O
a	O
result	O
,	O
these	O
sentences	O
are	O
linguistically	O
challenging	O
and	O
the	O
lexical	O
similarity	O
of	O
premise	O
and	O
hypothesis	O
is	O
often	O
high	O
,	O
thus	O
making	O
SciTail	Material
particularly	O
difficult	O
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	Task
adaptation	Task
in	O
this	O
study	O
.	O

subsection	O
:	O
Implementation	O
details	O
Our	O
implementation	O
of	O
MT	Method
-	Method
DNN	Method
is	O
based	O
on	O
the	O
PyTorch	O
implementation	O
of	O
BERT	Method
.	O

We	O
used	O
Adamax	Method
as	O
our	O
optimizer	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
5e	O
-	O
5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

The	O
maximum	O
number	O
of	O
epochs	O
was	O
set	O
to	O
5	O
.	O

A	O
linear	Method
learning	Method
rate	Method
decay	Method
schedule	Method
with	O
warm	O
-	O
up	O
over	O
0.1	O
was	O
used	O
,	O
unless	O
stated	O
otherwise	O
.	O

Following	O
,	O
we	O
set	O
the	O
number	O
of	O
steps	O
to	O
5	O
with	O
a	O
dropout	Metric
rate	Metric
of	O
0.1	O
.	O

To	O
avoid	O
the	O
exploding	Task
gradient	Task
problem	Task
,	O
we	O
clipped	O
the	O
gradient	O
norm	O
within	O
1	O
.	O

All	O
the	O
texts	O
were	O
tokenized	O
using	O
wordpieces	O
,	O
and	O
were	O
chopped	O
to	O
spans	O
no	O
longer	O
than	O
512	O
tokens	O
.	O

subsection	O
:	O
GLUE	Material
Results	O
The	O
test	O
results	O
on	O
GLUE	Material
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

MT	Method
-	Method
DNN	Method
outperforms	O
all	O
existing	O
systems	O
on	O
all	O
tasks	O
,	O
except	O
WNLI	Material
,	O
creating	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
GLUE	Material
tasks	O
and	O
pushing	O
the	O
benchmark	O
to	O
82.2	O
%	O
,	O
which	O
amounts	O
to	O
1.8	O
%	O
absolution	O
improvement	O
over	O
BERT	Method
LARGE	O
.	O

Since	O
MT	Method
-	Method
DNN	Method
uses	O
BERT	Method
LARGE	O
for	O
its	O
shared	O
layers	O
,	O
the	O
gain	O
is	O
solely	O
attributed	O
to	O
the	O
use	O
of	O
MTL	Task
in	O
fine	Task
-	Task
tuning	Task
.	O

MTL	Task
is	O
particularly	O
useful	O
for	O
the	O
tasks	O
with	O
little	O
in	O
-	O
domain	O
training	O
data	O
.	O

As	O
we	O
observe	O
in	O
the	O
table	O
,	O
on	O
the	O
same	O
type	O
of	O
tasks	O
,	O
the	O
improvements	O
over	O
BERT	Method
are	O
much	O
more	O
substantial	O
for	O
the	O
tasks	O
with	O
less	O
in	O
-	O
domain	O
training	O
data	O
e.g.	O
,	O
the	O
two	O
NLI	Task
tasks	Task
:	O
RTE	Method
vs.	O
MNLI	Material
,	O
and	O
the	O
two	O
paraphrase	Method
tasks	Method
:	O
MRPC	Material
vs.	O
QQP	Material
.	O

The	O
gain	O
of	O
MT	Method
-	Method
DNN	Method
is	O
also	O
attributed	O
to	O
its	O
flexible	O
modeling	Method
framework	Method
which	O
allows	O
us	O
to	O
incorporate	O
the	O
task	Method
-	Method
specific	Method
model	Method
structures	Method
and	O
training	Method
methods	Method
which	O
have	O
been	O
developed	O
in	O
the	O
single	Task
-	Task
task	Task
setting	Task
,	O
effectively	O
leveraging	O
the	O
existing	O
body	O
of	O
research	O
.	O

Two	O
such	O
examples	O
use	O
the	O
SAN	Method
answer	O
module	O
for	O
the	O
pairwise	Method
text	Method
classification	Method
output	Method
module	Method
,	O
and	O
the	O
pairwise	Metric
ranking	Metric
loss	Metric
for	O
the	O
QNLI	Material
task	O
which	O
by	O
design	O
is	O
a	O
binary	Task
classification	Task
problem	Task
in	O
GLUE	Material
.	O

To	O
investigate	O
the	O
relative	O
contributions	O
of	O
the	O
above	O
two	O
modeling	O
design	O
choices	O
,	O
we	O
implement	O
different	O
versions	O
of	O
MT	O
-	O
DNNs	Method
and	O
compare	O
their	O
performance	O
on	O
the	O
development	O
sets	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

BERT	Method
is	O
the	O
base	O
BERT	Method
model	Method
released	O
by	O
the	O
authors	O
,	O
which	O
we	O
used	O
as	O
a	O
baseline	O
.	O

We	O
fine	O
-	O
tuned	O
the	O
model	O
for	O
each	O
single	O
task	O
.	O

MT	Method
-	Method
DNN	Method
is	O
the	O
proposed	O
model	O
described	O
in	O
Section	O
[	O
reference	O
]	O
using	O
the	O
pre	O
-	O
trained	O
BERT	Method
BASE	O
as	O
its	O
shared	O
layers	O
.	O

We	O
then	O
fine	O
-	O
tuned	O
the	O
model	O
using	O
MTL	Task
on	O
all	O
GLUE	Material
tasks	O
.	O

Comparing	O
MT	Method
-	Method
DNN	Method
vs.	O
BERT	Method
BASE	O
,	O
we	O
see	O
that	O
the	O
results	O
on	O
dev	O
sets	O
are	O
consistent	O
with	O
the	O
GLUE	Material
test	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

ST	Method
-	Method
DNN	Method
,	O
standing	O
for	O
Single	Task
-	Task
Task	Task
DNN	Task
,	O
uses	O
the	O
same	O
model	Method
architecture	Method
as	O
MT	Method
-	Method
DNN	Method
.	O

But	O
,	O
instead	O
of	O
fine	O
-	O
tuning	O
one	O
model	O
for	O
all	O
tasks	O
using	O
MTL	Task
,	O
we	O
create	O
multiple	O
ST	O
-	O
DNNs	Method
,	O
one	O
for	O
each	O
task	O
using	O
only	O
its	O
in	O
-	O
domain	O
data	O
for	O
fine	Task
-	Task
tuning	Task
.	O

Thus	O
,	O
for	O
pairwise	Task
text	Task
classification	Task
tasks	Task
,	O
the	O
only	O
difference	O
between	O
their	O
ST	O
-	O
DNNs	Method
and	O
BERT	Method
models	O
is	O
the	O
design	O
of	O
the	O
task	Method
-	Method
specific	Method
output	Method
module	Method
.	O

The	O
results	O
show	O
that	O
on	O
three	O
out	O
of	O
four	O
tasks	O
(	O
MNLI	Material
,	O
QQP	Material
and	O
MRPC	Material
)	O
ST	O
-	O
DNNs	Method
outperform	O
their	O
BERT	Method
counterparts	O
,	O
justifying	O
the	O
effectiveness	O
of	O
the	O
SAN	Method
answer	O
module	O
.	O

We	O
also	O
compare	O
the	O
results	O
of	O
ST	Method
-	Method
DNN	Method
and	O
BERT	Method
on	O
QNLI	Material
.	O

While	O
ST	Method
-	Method
DNN	Method
is	O
fine	O
-	O
tuned	O
using	O
the	O
pairwise	Metric
ranking	Metric
loss	Metric
,	O
BERT	Method
views	O
QNLI	Material
as	O
binary	Task
classification	Task
and	O
is	O
fine	O
-	O
tuned	O
using	O
the	O
cross	Metric
entropy	Metric
loss	Metric
.	O

That	O
ST	Method
-	Method
DNN	Method
significantly	O
outperforms	O
BERT	Method
demonstrates	O
clearly	O
the	O
importance	O
of	O
problem	Task
formulation	Task
.	O

subsection	O
:	O
SNLI	Material
and	O
SciTail	Material
Results	O
In	O
Table	O
4	O
,	O
we	O
compare	O
our	O
adapted	O
models	O
,	O
using	O
all	O
in	O
-	O
domain	O
training	O
samples	O
,	O
against	O
several	O
strong	O
baselines	O
including	O
the	O
best	O
results	O
reported	O
in	O
the	O
leaderboards	O
.	O

We	O
see	O
that	O
MT	Method
-	Method
DNN	Method
generates	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
both	O
datasets	O
,	O
pushing	O
the	O
benchmarks	O
to	O
91.1	O
%	O
on	O
SNLI	Material
(	O
1.0	O
%	O
absolute	O
improvement	O
)	O
and	O
94.1	O
%	O
on	O
SciTail	Material
(	O
5.8	O
%	O
absolute	O
improvement	O
)	O
,	O
respectively	O
.	O

subsection	O
:	O
Domain	Task
Adaptation	Task
Results	O
One	O
of	O
the	O
most	O
important	O
criteria	O
for	O
building	O
practical	Method
systems	Method
is	O
fast	Task
adaptation	Task
to	O
new	O
tasks	O
and	O
domains	O
.	O

This	O
is	O
because	O
it	O
is	O
prohibitively	O
expensive	O
to	O
collect	O
labeled	O
training	O
data	O
for	O
new	O
domains	O
or	O
tasks	O
.	O

Very	O
often	O
,	O
we	O
only	O
have	O
very	O
small	O
training	O
data	O
or	O
even	O
no	O
training	O
data	O
.	O

To	O
evaluate	O
the	O
models	O
using	O
the	O
above	O
criterion	O
,	O
we	O
perform	O
domain	Task
adaptation	Task
experiments	O
on	O
two	O
NLI	Task
tasks	Task
,	O
SNLI	Material
and	O
SciTail	Material
,	O
using	O
the	O
following	O
procedure	O
:	O
fine	O
-	O
tune	O
the	O
MT	Method
-	Method
DNN	Method
model	O
on	O
eight	O
GLUE	Material
tasks	O
,	O
excluding	O
WNLI	Material
;	O
create	O
for	O
each	O
new	O
task	O
(	O
SNLI	Material
or	O
SciTail	Material
)	O
a	O
task	Method
-	Method
specific	Method
model	Method
,	O
by	O
adapting	O
the	O
trained	O
MT	Method
-	Method
DNN	Method
using	O
task	O
-	O
specific	O
training	O
data	O
;	O
evaluate	O
the	O
models	O
using	O
task	O
-	O
specific	O
test	O
data	O
.	O

We	O
denote	O
the	O
two	O
task	Method
-	Method
specific	Method
models	Method
as	O
MT	Method
-	Method
DNN	Method
.	O

For	O
comparison	O
,	O
we	O
also	O
perform	O
the	O
same	O
adaptation	Method
procedure	Method
to	O
the	O
pre	O
-	O
trained	O
BERT	Method
model	Method
,	O
creating	O
two	O
task	O
-	O
specific	O
BERT	Method
models	O
for	O
SNLI	Material
and	O
SciTail	Material
,	O
respectively	O
,	O
denoted	O
as	O
BERT	Method
.	O

We	O
split	O
the	O
training	O
data	O
of	O
SNLI	Material
and	O
SciTail	Material
,	O
and	O
randomly	O
sample	O
0.1	O
%	O
,	O
1	O
%	O
,	O
10	O
%	O
and	O
100	O
%	O
of	O
its	O
training	O
data	O
.	O

As	O
a	O
result	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SciTail	Material
,	O
which	O
includes	O
23	O
,	O
235	O
,	O
2.3k	O
and	O
23.5k	O
training	O
samples	O
.	O

Similarly	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SNLI	Material
,	O
which	O
includes	O
549	O
,	O
5.5k	O
,	O
54.9k	O
and	O
549.3k	O
training	O
samples	O
.	O

Results	O
on	O
different	O
amounts	O
of	O
training	O
data	O
of	O
SNLI	Material
and	O
SciTail	Material
are	O
reported	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

We	O
observe	O
that	O
our	O
model	O
pre	O
-	O
trained	O
on	O
GLUE	Material
via	O
multi	Method
-	Method
task	Method
learning	Method
outplays	O
the	O
BERT	Method
baseline	O
consistently	O
.	O

The	O
fewer	O
the	O
training	O
data	O
used	O
,	O
the	O
larger	O
improvement	O
MT	Method
-	Method
DNN	Method
demonstrates	O
over	O
BERT	Method
.	O

For	O
example	O
,	O
with	O
only	O
0.1	O
%	O
(	O
23	O
samples	O
)	O
of	O
the	O
SNLI	Material
training	O
data	O
,	O
MT	Method
-	Method
DNN	Method
achieves	O
82.1	O
%	O
in	O
accuracy	Metric
while	O
BERT	Method
’s	O
accuracy	Metric
is	O
52.5	O
%	O
;	O
with	O
1	O
%	O
of	O
the	O
training	O
data	O
,	O
the	O
accuracy	Metric
of	O
our	O
model	O
is	O
85.2	O
%	O
and	O
BERT	Method
is	O
78.1	O
%	O
.	O

We	O
observe	O
similar	O
results	O
on	O
SciTail	Material
.	O

The	O
results	O
indicate	O
that	O
the	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
are	O
more	O
effective	O
for	O
domain	Task
adaptation	Task
than	O
that	O
of	O
BERT	Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
proposed	O
a	O
model	O
called	O
MT	Method
-	Method
DNN	Method
to	O
combine	O
multi	Task
-	Task
task	Task
learning	Task
and	O
language	Task
model	Task
pre	Task
-	Task
training	Task
for	O
language	Task
representation	Task
learning	Task
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	Task
tasks	O
across	O
three	O
popular	O
benchmarks	O
:	O
SNLI	Material
,	O
SciTail	Material
,	O
and	O
GLUE	Material
.	O

MT	Method
-	Method
DNN	Method
also	O
demonstrates	O
an	O
exceptional	O
generalization	Metric
capability	Metric
in	O
domain	Task
adaptation	Task
experiments	O
.	O

There	O
are	O
many	O
future	O
areas	O
to	O
explore	O
to	O
improve	O
MT	Method
-	Method
DNN	Method
,	O
including	O
a	O
deeper	O
understanding	O
of	O
model	Task
structure	Task
sharing	Task
in	O
MTL	Task
,	O
a	O
more	O
effective	O
training	Method
method	Method
that	O
leverages	O
relatedness	O
among	O
multiple	O
tasks	O
,	O
and	O
ways	O
of	O
incorporating	O
the	O
linguistic	O
structure	O
of	O
text	O
in	O
a	O
more	O
explicit	O
and	O
controllable	O
manner	O
.	O

section	O
:	O
Acknowledgements	O
We	O
would	O
like	O
to	O
thanks	O
Jade	O
Huang	O
from	O
Microsoft	O
for	O
her	O
generous	O
help	O
on	O
this	O
work	O
.	O

bibliography	O
:	O
References	O
Knowledge	Method
graphs	Method
are	O
structured	O
representations	O
of	O
real	O
world	O
facts	O
.	O

However	O
,	O
they	O
typically	O
contain	O
only	O
a	O
small	O
subset	O
of	O
all	O
possible	O
facts	O
.	O

Link	Task
prediction	Task
is	O
a	O
task	O
of	O
inferring	Task
missing	Task
facts	Task
based	O
on	O
existing	O
ones	O
.	O

We	O
propose	O
TuckER	Method
,	O
a	O
relatively	O
simple	O
but	O
powerful	O
linear	Method
model	Method
based	O
on	O
Tucker	Method
decomposition	Method
of	O
the	O
binary	Method
tensor	Method
representation	Method
of	Method
knowledge	Method
graph	Method
triples	Method
.	O

TuckER	Method
outperforms	O
all	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
across	O
standard	O
link	O
prediction	O
datasets	O
.	O

We	O
prove	O
that	O
TuckER	Method
is	O
a	O
fully	Method
expressive	Method
model	Method
,	O
deriving	O
the	O
bound	O
on	O
its	O
entity	Metric
and	Metric
relation	Metric
embedding	Metric
dimensionality	Metric
for	O
full	Task
expressiveness	Task
which	O
is	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE.	O
We	O
further	O
show	O
that	O
several	O
previously	O
introduced	O
linear	Method
models	Method
can	O
be	O
viewed	O
as	O
special	O
cases	O
of	O
TuckER	Method
.	O

TuckER	Method
:	O
TensorFactorizationforKnowledgeGraphCompletion	O
section	O
:	O
Introduction	O
Vast	O
amounts	O
of	O
information	O
available	O
in	O
the	O
world	O
can	O
be	O
represented	O
succinctly	O
as	O
entities	O
and	O
relations	O
between	O
them	O
.	O

Knowledge	Method
graphs	Method
are	O
large	O
,	O
graph	O
-	O
structured	O
databases	O
which	O
store	O
facts	O
in	O
triple	O
form	O
,	O
with	O
and	O
representing	O
subject	O
and	O
object	O
entities	O
and	O
a	O
relation	O
between	O
them	O
.	O

Knowledge	Method
graphs	Method
are	O
used	O
for	O
a	O
wide	O
range	O
of	O
natural	Task
language	Task
processing	Task
and	O
information	Task
extraction	Task
tasks	Task
.	O

However	O
,	O
far	O
from	O
all	O
available	O
information	O
is	O
stored	O
in	O
existing	O
knowledge	O
graphs	O
and	O
manually	O
adding	O
new	O
information	O
is	O
costly	O
,	O
which	O
creates	O
the	O
need	O
for	O
algorithms	O
that	O
are	O
able	O
to	O
automatically	O
infer	O
missing	O
facts	O
based	O
on	O
existing	O
ones	O
.	O

Knowledge	Task
graphs	Task
can	O
be	O
represented	O
by	O
a	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
,	O
where	O
each	O
element	O
corresponds	O
to	O
a	O
triple	O
,	O
1	O
indicating	O
a	O
true	O
fact	O
and	O
0	O
indicating	O
the	O
unknown	O
(	O
either	O
a	O
false	O
or	O
a	O
missing	O
fact	O
)	O
.	O

One	O
of	O
the	O
most	O
important	O
tasks	O
in	O
relational	Task
machine	Task
learning	Task
is	O
link	Task
prediction	Task
:	O
predicting	O
whether	O
two	O
entities	O
are	O
related	O
,	O
based	O
on	O
known	O
relations	O
already	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O

The	O
task	O
of	O
link	Task
prediction	Task
is	O
thus	O
to	O
infer	O
which	O
of	O
the	O
0	O
entries	O
in	O
the	O
tensor	O
are	O
indeed	O
false	O
,	O
and	O
which	O
are	O
missing	O
but	O
actually	O
true	O
.	O

A	O
large	O
number	O
of	O
approaches	O
to	O
link	Task
prediction	Task
so	O
far	O
have	O
been	O
linear	O
,	O
based	O
on	O
various	O
methods	O
of	O
factorizing	Method
the	Method
third	Method
-	Method
order	Method
binary	Method
tensor	Method
.	O

Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
using	O
non	Method
-	Method
linear	Method
convolutional	Method
models	Method
.	O

Despite	O
achieving	O
very	O
good	O
performance	O
,	O
the	O
fundamental	O
problem	O
with	O
deep	Method
,	Method
non	Method
-	Method
linear	Method
models	Method
is	O
that	O
they	O
are	O
non	O
-	O
transparent	O
and	O
poorly	O
understood	O
,	O
as	O
opposed	O
to	O
more	O
mathematically	O
principled	O
and	O
widely	O
studied	O
tensor	Method
decomposition	Method
models	Method
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
TuckER	Method
(	O
E	O
stands	O
for	O
entities	O
,	O
R	O
for	O
relations	O
)	O
,	O
a	O
simple	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	O
knowledge	Task
graphs	Task
,	O
based	O
on	O
Tucker	Method
decomposition	Method
of	O
the	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
of	Method
triples	Method
.	O

Tucker	Method
decomposition	Method
factorizes	O
a	O
tensor	O
into	O
a	O
core	O
tensor	O
multiplied	O
by	O
a	O
matrix	O
along	O
each	O
mode	O
.	O

It	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
higher	Method
-	Method
order	Method
singular	Method
value	Method
decomposition	Method
(	O
HOSVD	Method
)	Method
in	O
the	O
special	O
case	O
where	O
matrices	O
are	O
orthogonal	O
and	O
the	O
core	O
tensor	O
is	O
“	O
all	O
-	O
orthogonal	O
”	O
.	O

In	O
our	O
case	O
,	O
rows	O
of	O
the	O
three	O
matrices	O
contain	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
while	O
entries	O
of	O
the	O
core	O
tensor	O
determine	O
the	O
level	O
of	O
interaction	O
between	O
them	O
.	O

Further	O
,	O
subject	O
and	O
object	O
entity	O
embedding	O
matrices	O
are	O
assumed	O
equivalent	O
,	O
i.e.	O
we	O
make	O
no	O
distinction	O
between	O
the	O
embeddings	O
of	O
an	O
entity	O
depending	O
on	O
whether	O
it	O
appears	O
as	O
a	O
subject	O
or	O
as	O
an	O
object	O
in	O
a	O
particular	O
triple	O
.	O

Given	O
that	O
knowledge	O
graphs	O
contain	O
several	O
relation	O
types	O
(	O
symmetric	O
,	O
asymmetric	O
,	O
transitive	O
,	O
etc	O
.	O

)	O
,	O
it	O
is	O
important	O
for	O
a	O
link	Method
prediction	Method
model	Method
to	O
have	O
enough	O
expressive	O
power	O
to	O
accurately	O
represent	O
all	O
of	O
them	O
.	O

We	O
thus	O
show	O
that	O
TuckER	Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
any	O
ground	O
truth	O
over	O
the	O
triples	O
,	O
there	O
exists	O
an	O
assignment	O
of	O
values	O
to	O
the	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separates	O
the	O
true	O
triples	O
from	O
false	O
ones	O
.	O

We	O
also	O
derive	O
a	O
bound	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
that	O
guarantees	O
full	O
expressiveness	O
,	O
finding	O
it	O
to	O
be	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
.	O

This	O
enables	O
TuckER	Method
to	O
achieve	O
better	O
results	O
with	O
much	O
smaller	O
embedding	Metric
sizes	Metric
than	O
needed	O
by	O
those	O
models	O
,	O
important	O
for	O
efficiency	O
in	O
downstream	Task
tasks	Task
.	O

Finally	O
,	O
we	O
show	O
that	O
several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
linear	Method
models	Method
,	O
RESCAL	Method
,	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
are	O
special	O
cases	O
of	O
TuckER	Method
.	O

In	O
summary	O
,	O
the	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
proposing	O
TuckER	Method
,	O
a	O
new	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	Task
knowledge	Task
graphs	Task
,	O
that	O
is	O
simple	O
,	O
expressive	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
all	O
standard	O
datasets	O
;	O
proving	O
that	O
TuckER	Method
is	O
fully	O
expressive	O
and	O
deriving	O
a	O
bound	O
on	O
the	O
entity	Metric
and	Metric
relation	Metric
embedding	Metric
dimensionality	Metric
for	O
full	O
expressiveness	O
which	O
is	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
;	O
and	O
showing	O
that	O
TuckER	Method
subsumes	O
several	O
previously	O
proposed	O
tensor	Method
factorization	Method
approaches	Method
to	O
link	Task
prediction	Task
,	O
i.e.	O
that	O
RESCAL	O
,	O
DistMult	O
,	O
ComplEx	Method
and	O
SimplE	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O

section	O
:	O
Related	O
Work	O
Several	O
linear	Method
models	Method
for	O
link	Task
prediction	Task
have	O
previously	O
been	O
proposed	O
:	O
RESCAL	O
An	O
early	O
linear	Method
model	Method
,	O
RESCAL	Method
,	O
optimizes	O
a	O
scoring	Method
function	Method
containing	O
a	O
bilinear	O
product	O
between	O
vector	Method
embeddings	Method
for	O
each	O
subject	O
and	O
object	O
entity	O
and	O
a	O
full	O
rank	O
matrix	O
for	O
each	O
relation	O
.	O

Although	O
a	O
very	O
expressive	O
and	O
powerful	O
model	O
,	O
RESCAL	Method
is	O
prone	O
to	O
overfitting	O
due	O
to	O
its	O
large	O
number	O
of	O
parameters	O
,	O
which	O
increases	O
quadratically	O
in	O
the	O
embedding	O
dimension	O
with	O
the	O
number	O
of	O
relations	O
in	O
a	O
knowledge	O
graph	O
.	O

DistMult	Method
DistMult	Method
is	O
a	O
special	O
case	O
of	O
RESCAL	O
with	O
a	O
diagonal	O
matrix	O
per	O
relation	O
,	O
so	O
the	O
number	O
of	O
parameters	O
of	O
DistMult	Method
grows	O
linearly	O
with	O
respect	O
to	O
the	O
embedding	O
dimension	O
,	O
reducing	O
overfitting	O
.	O

However	O
,	O
the	O
linear	Method
transformation	Method
performed	O
on	O
subject	O
entity	O
embedding	O
vectors	O
in	O
DistMult	Method
is	O
limited	O
to	O
a	O
stretch	O
.	O

Given	O
the	O
equivalence	O
of	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
,	O
third	O
-	O
order	O
binary	O
tensor	O
learned	O
by	O
DistMult	Method
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
and	O
thus	O
DistMult	Method
can	O
not	O
model	O
asymmetric	O
relations	O
.	O

ComplEx	Method
ComplEx	Method
extends	O
DistMult	Method
to	O
the	O
complex	O
domain	O
.	O

Even	O
though	O
each	O
relation	O
matrix	O
of	O
ComplEx	O
is	O
still	O
diagonal	O
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
no	O
longer	O
equivalent	O
,	O
but	O
complex	O
conjugates	O
,	O
which	O
introduces	O
asymmetry	O
into	O
the	O
tensor	Method
decomposition	Method
and	O
thus	O
enables	O
ComplEx	Method
to	O
model	O
asymmetric	O
relations	O
.	O

SimplE	Method
SimplE	Method
is	O
a	O
linear	Method
model	Method
based	O
on	O
Canonical	Method
Polyadic	Method
(	Method
CP	Method
)	Method
decomposition	Method
.	O

In	O
CP	Task
decomposition	Task
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
independent	O
(	O
note	O
that	O
DistMult	Method
is	O
a	O
special	O
case	O
of	O
CP	O
,	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
equivalent	O
)	O
.	O

SimplE	Method
’s	Method
scoring	Method
function	Method
alters	O
CP	O
to	O
make	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
dependent	O
on	O
each	O
other	O
,	O
i.e.	O
it	O
computes	O
the	O
average	O
of	O
two	O
terms	O
,	O
first	O
of	O
which	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	Method
embedding	Method
of	O
the	O
subject	O
entity	O
,	O
relation	Method
embedding	Method
and	O
tail	O
embedding	O
of	O
the	O
object	O
entity	O
and	O
the	O
second	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	O
embedding	O
of	O
the	O
object	O
entity	O
,	O
inverse	Method
relation	Method
embedding	Method
and	O
tail	O
embedding	O
of	O
the	O
subject	O
entity	O
.	O

Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
with	O
non	Method
-	Method
linear	Method
models	Method
:	O
ConvE	Method
ConvE	Method
is	O
the	O
first	O
non	Method
-	Method
linear	Method
model	Method
that	O
significantly	O
outperformed	O
the	O
preceding	O
linear	Method
models	Method
.	O

In	O
ConvE	O
,	O
a	O
global	Method
2D	Method
convolution	Method
operation	Method
is	O
performed	O
on	O
the	O
subject	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
after	O
they	O
are	O
reshaped	O
to	O
matrices	O
and	O
concatenated	O
.	O

The	O
obtained	O
feature	O
maps	O
are	O
flattened	O
,	O
transformed	O
through	O
a	O
fully	Method
connected	Method
layer	Method
,	O
and	O
the	O
inner	O
product	O
is	O
taken	O
with	O
all	O
object	O
entity	O
vectors	O
to	O
generate	O
a	O
score	O
for	O
each	O
triple	O
.	O

Whilst	O
results	O
achieved	O
by	O
ConvE	Method
are	O
impressive	O
,	O
its	O
reshaping	O
and	O
concatenating	Method
of	Method
vectors	Method
as	O
well	O
as	O
using	O
2D	Method
convolution	Method
on	O
word	Method
embeddings	Method
is	O
unintuitive	O
.	O

HypER	O
HypER	Method
is	O
a	O
simplified	Method
convolutional	Method
model	Method
,	O
that	O
uses	O
a	O
hypernetwork	Method
to	O
generate	O
1D	Method
convolutional	Method
filters	Method
for	O
each	O
relation	O
,	O
extracting	O
relation	O
-	O
specific	O
features	O
from	O
subject	O
entity	O
embeddings	O
.	O

The	O
authors	O
show	O
that	O
convolution	Method
is	O
a	O
way	O
of	O
introducing	O
sparsity	Task
and	Task
parameter	Task
tying	Task
and	O
that	O
HypER	Method
can	O
be	O
understood	O
in	O
terms	O
of	O
tensor	Method
factorization	Method
up	O
to	O
a	O
non	O
-	O
linearity	O
,	O
thus	O
placing	O
HypER	Method
closer	O
to	O
the	O
well	O
established	O
family	O
of	O
factorization	Method
models	Method
.	O

The	O
drawback	O
of	O
HypER	Method
is	O
that	O
it	O
sets	O
most	O
elements	O
of	O
the	O
core	O
weight	O
tensor	O
to	O
0	O
,	O
which	O
amounts	O
to	O
hard	O
regularization	O
,	O
rather	O
than	O
letting	O
the	O
model	O
learn	O
which	O
parameters	O
to	O
use	O
via	O
a	O
soft	Method
regularization	Method
approach	Method
.	O

Scoring	Metric
functions	Metric
of	O
all	O
models	O
described	O
above	O
and	O
TuckER	Method
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Background	O
Let	O
denote	O
the	O
set	O
of	O
all	O
entities	O
and	O
the	O
set	O
of	O
all	O
relations	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O

A	O
triple	O
is	O
represented	O
as	O
,	O
with	O
denoting	O
subject	O
and	O
object	O
entities	O
respectively	O
and	O
the	O
relation	O
between	O
them	O
.	O

subsection	O
:	O
Link	Task
Prediction	Task
In	O
link	Task
prediction	Task
,	O
we	O
are	O
given	O
a	O
subset	O
of	O
all	O
true	O
triples	O
and	O
the	O
aim	O
is	O
to	O
learn	O
a	O
scoring	Method
function	Method
that	O
assigns	O
a	O
score	O
to	O
each	O
triple	O
,	O
indicating	O
whether	O
that	O
triple	O
is	O
true	O
or	O
false	O
,	O
with	O
the	O
ultimate	O
goal	O
of	O
being	O
able	O
to	O
correctly	O
score	O
all	O
missing	O
triples	O
.	O

The	O
scoring	Method
function	Method
is	O
either	O
a	O
specific	O
form	O
of	O
tensor	Method
factorization	Method
in	O
the	O
case	O
of	O
linear	Method
models	Method
or	O
a	O
more	O
complex	O
(	O
deep	Method
)	Method
neural	Method
network	Method
architecture	Method
in	O
the	O
case	O
of	O
non	Method
-	Method
linear	Method
models	Method
.	O

Typically	O
,	O
a	O
positive	O
score	O
for	O
a	O
particular	O
triple	O
indicates	O
a	O
true	O
fact	O
predicted	O
by	O
the	O
model	O
,	O
while	O
a	O
negative	O
score	O
indicates	O
a	O
false	O
one	O
.	O

With	O
most	O
recent	O
models	O
,	O
a	O
non	Method
-	Method
linearity	Method
such	O
as	O
the	O
logistic	O
sigmoid	O
function	O
is	O
typically	O
applied	O
to	O
the	O
score	O
to	O
give	O
a	O
corresponding	O
probability	O
prediction	O
as	O
to	O
whether	O
a	O
certain	O
fact	O
is	O
true	O
.	O

subsection	O
:	O
Tucker	Method
Decomposition	Method
Tucker	Method
decomposition	Method
,	O
named	O
after	O
Ledyard	O
R.	O
Tucker	O
and	O
refined	O
in	O
his	O
subsequent	O
work	O
,	O
decomposes	O
a	O
tensor	O
into	O
a	O
set	O
of	O
matrices	O
and	O
a	O
smaller	O
core	O
tensor	O
.	O

In	O
a	O
three	Task
-	Task
mode	Task
case	Task
,	O
given	O
the	O
original	O
tensor	O
,	O
Tucker	Method
decomposition	Method
outputs	O
a	O
tensor	O
and	O
three	O
matrices	O
,	O
,	O
:	O
with	O
indicating	O
the	O
tensor	O
product	O
along	O
the	O
n	O
-	O
th	O
mode	O
and	O
the	O
vector	O
inner	O
product	O
.	O

Factor	O
matrices	O
,	O
and	O
,	O
when	O
orthogonal	O
,	O
can	O
be	O
thought	O
of	O
as	O
the	O
principal	O
components	O
in	O
each	O
mode	O
.	O

Elements	O
of	O
the	O
core	O
tensor	O
show	O
the	O
level	O
of	O
interaction	O
between	O
the	O
different	O
components	O
.	O

Typically	O
,	O
,	O
,	O
are	O
smaller	O
than	O
,	O
,	O
respectively	O
,	O
so	O
can	O
be	O
thought	O
of	O
as	O
a	O
compressed	O
version	O
of	O
.	O

Tucker	Method
decomposition	Method
is	O
not	O
unique	O
,	O
i.e.	O
we	O
can	O
transform	O
without	O
affecting	O
the	O
fit	O
if	O
we	O
apply	O
the	O
inverse	O
of	O
that	O
transformation	O
to	O
the	O
factor	O
matrices	O
.	O

Imposing	O
additional	O
constraints	O
on	O
the	O
structure	O
of	O
,	O
such	O
as	O
sparsity	O
,	O
making	O
its	O
elements	O
small	O
or	O
making	O
the	O
core	O
“	O
all	O
-	O
orthogonal	O
”	O
,	O
can	O
lead	O
to	O
improved	O
uniqueness	O
.	O

section	O
:	O
Tucker	Method
Decomposition	Method
for	O
Link	Task
Prediction	Task
We	O
propose	O
a	O
model	O
that	O
uses	O
Tucker	Method
decomposition	Method
for	O
link	Task
prediction	Task
on	O
the	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
representation	Method
of	O
a	O
knowledge	O
graph	O
,	O
with	O
entity	O
embedding	O
matrix	O
that	O
is	O
equivalent	O
for	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
relation	O
embedding	O
matrix	O
,	O
where	O
and	O
represent	O
the	O
number	O
of	O
entities	O
and	O
relations	O
and	O
and	O
the	O
dimensionality	O
of	O
entity	O
and	O
relation	O
embedding	O
vectors	O
respectively	O
.	O

We	O
define	O
the	O
scoring	Metric
function	Metric
for	O
TuckER	Method
as	O
:	O
where	O
are	O
the	O
rows	O
of	O
representing	O
the	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
,	O
the	O
rows	O
of	O
representing	O
the	O
relation	O
embedding	O
vector	O
,	O
is	O
the	O
core	Method
tensor	Method
of	Method
Tucker	Method
decomposition	Method
and	O
is	O
the	O
tensor	O
product	O
along	O
the	O
-	O
th	O
mode	O
.	O

We	O
apply	O
logistic	Method
sigmoid	Method
to	O
each	O
score	O
to	O
obtain	O
the	O
predicted	O
probability	O
of	O
a	O
triple	O
being	O
true	O
.	O

Visualization	O
of	O
the	O
TuckER	Method
model	O
architecture	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

As	O
proven	O
in	O
Section	O
[	O
reference	O
]	O
,	O
TuckER	Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
sufficient	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
,	O
it	O
is	O
able	O
to	O
assign	O
values	O
to	O
the	O
embeddings	O
that	O
correctly	O
separate	O
any	O
combination	O
of	O
ground	O
truth	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O

The	O
number	O
of	O
parameters	O
of	O
TuckER	Method
increases	O
linearly	O
with	O
respect	O
to	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
,	O
as	O
the	O
number	O
of	O
entities	O
and	O
relations	O
(	O
and	O
respectively	O
)	O
increases	O
,	O
since	O
the	O
number	O
of	O
parameters	O
of	O
core	Method
tensor	Method
depends	O
only	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
not	O
on	O
the	O
number	O
of	O
entities	O
or	O
relations	O
.	O

By	O
having	O
the	O
core	O
tensor	O
,	O
unlike	O
simpler	O
models	O
such	O
as	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
TuckER	Method
does	O
not	O
encode	O
all	O
the	O
learned	O
knowledge	O
into	O
the	O
embeddings	O
;	O
some	O
is	O
stored	O
in	O
the	O
core	O
tensor	O
and	O
shared	O
between	O
all	O
entities	O
and	O
relations	O
.	O

subsection	O
:	O
Training	O
Given	O
we	O
can	O
not	O
use	O
analytical	Method
methods	Method
for	O
computing	O
the	O
tensor	Task
factorization	Task
,	O
since	O
the	O
tensor	O
being	O
factorized	O
is	O
comprised	O
of	O
and	O
(	O
after	O
applying	O
the	O
inverse	O
of	O
logistic	O
sigmoid	O
)	O
,	O
we	O
use	O
numerical	Method
methods	Method
to	O
train	O
TuckER	Method
.	O

Following	O
the	O
training	O
procedure	O
introduced	O
by	O
dettmers2018convolutional	Method
with	O
the	O
goal	O
of	O
speeding	O
up	O
training	Task
and	O
increasing	O
accuracy	Metric
,	O
we	O
use	O
1	Method
-	Method
N	Method
scoring	Method
,	O
i.e.	O
we	O
simultaneously	O
score	O
a	O
pair	O
and	O
with	O
all	O
entities	O
,	O
in	O
contrast	O
to	O
1	O
-	O
1	O
scoring	O
,	O
where	O
individual	O
triples	O
are	O
trained	O
one	O
at	O
a	O
time	O
.	O

This	O
way	O
we	O
make	O
use	O
of	O
the	O
local	O
-	O
closed	O
world	O
assumption	O
,	O
where	O
we	O
assume	O
that	O
a	O
knowledge	O
graph	O
is	O
only	O
locally	O
complete	O
,	O
i.e.	O
we	O
include	O
only	O
the	O
non	O
-	O
existing	O
triples	O
and	O
of	O
the	O
observed	O
pairs	O
and	O
respectively	O
as	O
negative	O
samples	O
and	O
all	O
observed	O
triples	O
as	O
positive	O
samples	O
.	O

We	O
train	O
our	O
model	O
to	O
minimize	O
the	O
Bernoulli	O
negative	O
log	O
-	O
likelihood	O
loss	O
function	O
:	O
where	O
is	O
the	O
vector	O
of	O
probabilities	O
predicted	O
by	O
the	O
model	O
and	O
is	O
the	O
label	O
vector	O
of	O
ones	O
for	O
true	O
and	O
zeros	O
for	O
false	O
triples	O
.	O

section	O
:	O
Theoretical	O
Analysis	O
subsection	O
:	O
Bound	O
on	O
Embedding	Metric
Dimensionality	Metric
for	O
Full	Task
Expressiveness	Task
As	O
previously	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
a	O
tensor	Method
factorization	Method
model	Method
is	O
said	O
to	O
be	O
fully	O
expressive	O
if	O
for	O
any	O
ground	O
truth	O
over	O
all	O
entities	O
and	O
relations	O
,	O
there	O
exist	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separate	O
the	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O

As	O
shown	O
in	O
,	O
ComplEx	Method
is	O
fully	O
expressive	O
with	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
of	O
for	O
achieving	O
full	O
expressiveness	O
.	O

Similarly	O
to	O
ComplEx	Method
,	O
kazemi2018simple	Method
show	O
that	O
SimplE	Method
is	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embeddings	O
of	O
size	O
,	O
with	O
representing	O
the	O
number	O
of	O
true	O
facts	O
.	O

The	O
authors	O
further	O
prove	O
other	O
models	O
are	O
not	O
fully	O
expressive	O
:	O
DistMult	Method
,	O
because	O
it	O
can	O
not	O
model	O
asymmetric	O
relations	O
;	O
and	O
transitive	Method
models	Method
such	O
as	O
TransE	Method
and	O
its	O
variants	O
FTransE	Method
and	O
STransE	Method
,	O
because	O
of	O
certain	O
contradictions	O
that	O
they	O
impose	O
between	O
different	O
relation	O
types	O
.	O

By	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
establish	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
(	O
i.e.	O
rank	O
of	O
the	O
decomposition	O
)	O
that	O
guarantees	O
full	O
expressiveness	O
of	O
TuckER	Method
.	O

theorem	O
:	O
.	O

Given	O
any	O
ground	O
truth	O
over	O
a	O
set	O
of	O
entities	O
E	O
and	O
relations	O
R	O
,	O
there	O
exists	O
a	O
TuckER	Method
model	O
with	O
subject	Method
and	Method
object	Method
entity	Method
embeddings	Method
of	O
dimensionality	O
=	O
dene	O
and	O
relation	O
embeddings	O
of	O
dimensionality	O
=	O
drnr	O
,	O
where	O
=	O
ne|E|	O
is	O
the	O
number	O
of	O
entities	O
and	O
=	O
nr|R|	O
the	O
number	O
of	O
relations	O
,	O
that	O
accurately	O
represents	O
that	O
ground	O
truth	O
.	O

proof	O
:	O
Proof	O
.	O

Let	O
and	O
be	O
the	O
-	O
dimensional	O
one	O
-	O
hot	Method
binary	Method
vector	Method
representations	Method
of	Method
subject	Method
and	Method
object	Method
entities	Method
and	O
respectively	O
and	O
the	O
-	O
dimensional	O
one	Method
-	Method
hot	Method
binary	Method
vector	Method
representation	Method
of	O
a	O
relation	O
.	O

For	O
each	O
subject	O
entity	O
,	O
relation	O
and	O
object	O
entity	O
,	O
we	O
let	O
the	O
-	O
th	O
,	O
-	O
th	O
and	O
-	O
th	O
element	O
respectively	O
of	O
the	O
corresponding	O
vectors	O
,	O
and	O
be	O
1	O
and	O
all	O
other	O
elements	O
0	O
.	O

Further	O
,	O
we	O
set	O
the	O
element	O
of	O
the	O
tensor	O
to	O
1	O
if	O
the	O
fact	O
holds	O
and	O
-	O
1	O
otherwise	O
.	O

Thus	O
the	O
tensor	O
product	O
of	O
these	O
entity	Method
embeddings	Method
and	O
the	O
relation	Method
embedding	Method
with	O
the	O
core	O
tensor	O
,	O
after	O
applying	O
the	O
logistic	Method
sigmoid	Method
,	O
accurately	O
represents	O
the	O
original	O
tensor	O
.	O

∎	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
Theorem	O
[	O
reference	O
]	O
shows	O
that	O
it	O
is	O
straightforward	O
to	O
prove	O
that	O
the	O
required	O
dimensionality	Metric
of	O
TuckER	Method
embeddings	O
to	O
ensure	O
full	O
expressiveness	O
is	O
lower	O
than	O
the	O
required	O
dimensionality	Metric
for	O
SimplE	O
and	O
ComplEx	O
by	O
a	O
factor	O
of	O
for	O
entity	Method
embeddings	Method
and	O
by	O
a	O
factor	O
of	O
for	O
relation	Method
embeddings	Method
.	O

Existing	O
knowledge	O
graphs	O
usually	O
contain	O
tens	O
of	O
thousands	O
of	O
entities	O
and	O
hundreds	O
or	O
even	O
thousands	O
of	O
relations	O
.	O

This	O
allows	O
TuckER	Method
to	O
be	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embedding	O
dimensionalities	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
those	O
of	O
ComplEx	Method
and	O
SimplE.	O
In	O
practice	O
,	O
we	O
expect	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
needed	O
for	O
full	Task
reconstruction	Task
of	O
the	O
underlying	O
binary	O
tensor	O
to	O
be	O
much	O
smaller	O
than	O
the	O
bound	O
stated	O
above	O
,	O
since	O
the	O
assignment	O
of	O
values	O
to	O
the	O
tensor	O
is	O
not	O
random	O
but	O
follows	O
a	O
certain	O
structure	O
,	O
otherwise	O
nothing	O
unknown	O
could	O
be	O
predicted	O
.	O

Even	O
more	O
so	O
,	O
low	O
decomposition	O
rank	O
is	O
actually	O
a	O
desired	O
property	O
,	O
forcing	O
the	O
model	O
to	O
learn	O
that	O
structure	O
and	O
generalize	O
to	O
new	O
data	O
,	O
rather	O
than	O
simply	O
memorizing	O
the	O
input	O
.	O

We	O
expect	O
TuckER	Method
to	O
perform	O
better	O
than	O
ComplEx	Method
and	O
SimplE	O
with	O
embeddings	O
of	O
lower	O
dimensionality	O
due	O
to	O
parameter	O
sharing	O
in	O
the	O
core	O
tensor	O
(	O
shown	O
empirically	O
in	O
Section	O
[	O
reference	O
]	O
)	O
,	O
which	O
could	O
be	O
of	O
importance	O
for	O
efficiency	O
in	O
downstream	Task
tasks	Task
.	O

subsection	O
:	O
Relation	O
of	O
TuckER	Method
to	O
Previous	O
Tensor	Method
Factorization	Method
Approaches	Method
Several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
TuckER	Method
:	O
RESCAL	O
[	O
]	O
Following	O
the	O
notation	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
RESCAL	Metric
scoring	Metric
function	Metric
has	O
the	O
form	O
:	O
This	O
corresponds	O
to	O
Equation	O
[	O
reference	O
]	O
with	O
,	O
,	O
and	O
the	O
identity	O
matrix	O
,	O
i.e	O
the	O
second	O
dimension	O
of	O
original	O
tensor	O
is	O
not	O
reduced	O
by	O
.	O

This	O
is	O
also	O
known	O
as	O
Tucker2	Method
decomposition	Method
.	O

As	O
is	O
the	O
case	O
with	O
TuckER	Method
,	O
the	O
entity	O
embedding	O
matrix	O
of	O
RESCAL	O
is	O
shared	O
between	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
the	O
relation	O
matrices	O
are	O
the	O
slices	O
of	O
the	O
core	O
tensor	O
.	O

As	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
drawback	O
of	O
RESCAL	Method
compared	O
to	O
TuckER	Method
is	O
that	O
its	O
number	O
of	O
parameters	O
grows	O
quadratically	O
in	O
the	O
entity	Metric
embedding	Metric
dimension	Metric
as	O
the	O
number	O
of	O
relations	O
increases	O
.	O

Therefore	O
,	O
RESCAL	Method
tends	O
to	O
overfit	O
for	O
those	O
relations	O
for	O
which	O
only	O
a	O
small	O
number	O
of	O
training	O
triples	O
is	O
available	O
.	O

DistMult	Method
[	O
]	O
The	O
scoring	Metric
function	Metric
of	O
DistMult	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
can	O
be	O
viewed	O
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
that	O
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
superdiagonal	O
with	O
1s	O
on	O
that	O
superdiagonal	O
,	O
i.e.	O
all	O
elements	O
with	O
are	O
1	O
and	O
all	O
the	O
other	O
elements	O
are	O
0	O
(	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
diagonal	O
for	O
every	O
slice	O
,	O
i.e.	O
all	O
elements	O
apart	O
from	O
with	O
are	O
0	O
.	O

If	O
we	O
adopt	O
the	O
TuckER	Method
view	O
of	O
the	O
DistMult	Method
scoring	Method
function	Method
,	O
rows	O
of	O
contain	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
and	O
rows	O
of	O
contain	O
relation	O
embedding	O
vectors	O
.	O

By	O
adopting	O
the	O
RESCAL	O
view	O
,	O
entity	O
embedding	O
embedding	O
vectors	O
remain	O
the	O
same	O
,	O
but	O
relation	O
embedding	O
vectors	O
are	O
now	O
the	O
diagonals	O
of	O
slices	O
of	O
.	O

It	O
is	O
interesting	O
to	O
note	O
that	O
the	O
TuckER	Method
interpretation	O
of	O
the	O
DistMult	Method
scoring	Method
function	Method
,	O
given	O
that	O
matrices	O
and	O
are	O
identical	O
,	O
can	O
alternatively	O
be	O
interpreted	O
as	O
a	O
special	O
case	O
of	O
CP	Method
decomposition	Method
,	O
since	O
Tucker	Method
decomposition	Method
with	O
a	O
superdiagonal	O
core	O
tensor	O
becomes	O
equivalent	O
to	O
CP	Method
decomposition	Method
.	O

Because	O
of	O
its	O
simplicity	O
,	O
DistMult	Method
learns	O
a	O
binary	O
tensor	O
that	O
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
,	O
so	O
it	O
can	O
not	O
learn	O
to	O
represent	O
asymmetric	O
relations	O
.	O

ComplEx	O
[	O
]	O
Bilinear	Method
models	Method
are	O
a	O
family	O
of	O
models	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
represented	O
by	O
vectors	O
,	O
a	O
relation	O
is	O
represented	O
by	O
a	O
matrix	O
and	O
the	O
scoring	O
function	O
takes	O
the	O
form	O
of	O
a	O
bilinear	O
product	O
between	O
the	O
two	O
embedding	O
vectors	O
and	O
the	O
relation	O
matrix	O
,	O
i.e.	O
.	O

It	O
is	O
trivial	O
to	O
show	O
that	O
both	O
RESCAL	Method
and	O
DistMult	Method
belong	O
to	O
the	O
family	Method
of	Method
bilinear	Method
models	Method
.	O

As	O
explained	O
by	O
kazemi2018simple	O
,	O
ComplEx	Method
can	O
be	O
considered	O
a	O
bilinear	Method
model	Method
with	O
the	O
real	O
and	O
imaginary	O
part	O
of	O
an	O
embedding	O
for	O
each	O
entity	O
concatenated	O
in	O
a	O
single	O
vector	O
,	O
for	O
subject	O
,	O
for	O
object	O
,	O
and	O
a	O
relation	O
matrix	O
,	O
constrained	O
in	O
a	O
way	O
that	O
its	O
leading	O
diagonal	O
contains	O
duplicated	O
elements	O
of	O
,	O
its	O
-	O
diagonal	O
contains	O
elements	O
of	O
and	O
its	O
-	O
-	O
diagonal	O
has	O
elements	O
of	O
-	O
,	O
with	O
all	O
other	O
elements	O
set	O
to	O
0	O
,	O
where	O
and	O
-	O
represent	O
offsets	O
from	O
the	O
leading	O
diagonal	O
.	O

This	O
makes	O
the	O
scoring	Metric
function	Metric
of	O
ComplEx	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
with	O
relation	O
matrix	O
constrained	O
as	O
described	O
.	O

Therefore	O
,	O
similarly	O
to	O
DistMult	Method
,	O
we	O
can	O
regard	O
the	O
scoring	O
function	O
of	O
ComplEx	Method
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
the	O
scoring	Method
function	Method
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
different	O
tensor	O
diagonals	O
are	O
set	O
to	O
1	O
,	O
elements	O
on	O
one	O
tensor	O
diagonal	O
are	O
set	O
to	O
-	O
1	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
the	O
scoring	Metric
function	Metric
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
of	O
,	O
all	O
elements	O
on	O
the	O
leading	O
diagonal	O
are	O
set	O
to	O
,	O
the	O
-	O
diagonal	O
is	O
set	O
to	O
,	O
the	O
-	O
-	O
diagonal	O
is	O
set	O
to	O
-	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
.	O

This	O
shows	O
that	O
the	O
scoring	O
function	O
of	O
ComplEx	Method
,	O
which	O
computes	O
a	O
bilinear	O
product	O
with	O
complex	O
entity	O
and	O
relation	O
embeddings	O
and	O
disregards	O
the	O
imaginary	O
part	O
of	O
the	O
obtained	O
result	O
,	O
is	O
equivalent	O
to	O
a	O
hard	O
regularization	O
of	O
the	O
core	O
tensor	O
of	O
TuckER	Method
in	O
the	O
real	O
domain	O
.	O

SimplE	O
[	O
]	O
The	O
authors	O
show	O
that	O
SimplE	O
belongs	O
to	O
the	O
family	O
of	O
bilinear	Method
models	Method
by	O
concatenating	Method
embeddings	Method
for	O
head	O
and	O
tail	O
entities	O
for	O
both	O
subject	O
and	O
object	O
into	O
vectors	O
and	O
and	O
constraining	O
the	O
relation	O
matrix	O
so	O
that	O
it	O
contains	O
the	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
diagonal	O
and	O
the	O
inverse	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
-	O
diagonal	O
and	O
0s	O
elsewhere	O
.	O

The	O
SimplE	O
scoring	Method
function	Method
is	O
therefore	O
equivalent	O
to	O
:	O
that	O
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
two	O
tensor	O
diagonals	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
,	O
elements	O
on	O
the	O
-	O
diagonal	O
are	O
set	O
to	O
,	O
elements	O
on	O
the	O
-	O
-	O
diagonal	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
0	O
.	O

subsection	O
:	O
Representing	O
Asymmetric	O
Relations	O
Each	O
relation	O
in	O
a	O
knowledge	O
graph	O
can	O
be	O
characterized	O
by	O
a	O
certain	O
set	O
of	O
properties	O
,	O
such	O
as	O
symmetry	O
,	O
reflexivity	O
,	O
transitivity	O
,	O
etc	O
.	O

A	O
relation	O
is	O
asymmetric	O
if	O
,	O
for	O
all	O
subject	O
entities	O
that	O
are	O
related	O
to	O
their	O
corresponding	O
object	O
entities	O
through	O
,	O
the	O
reciprocal	O
necessarily	O
does	O
not	O
hold	O
,	O
i.e.	O
none	O
of	O
the	O
object	O
entities	O
are	O
related	O
to	O
the	O
subject	O
entities	O
through	O
.	O

So	O
far	O
,	O
there	O
have	O
been	O
two	O
possible	O
ways	O
in	O
which	O
linear	Method
link	Method
prediction	Method
models	Method
introduce	O
asymmetry	O
into	O
factorization	Task
of	O
the	O
binary	O
tensor	O
of	O
triples	O
.	O

One	O
is	O
to	O
have	O
distinct	O
(	O
although	O
possibly	O
related	O
)	O
embeddings	O
for	O
subject	O
and	O
object	O
entities	O
and	O
a	O
diagonal	O
matrix	O
(	O
or	O
equivalently	O
a	O
vector	O
)	O
for	O
each	O
relation	O
,	O
as	O
is	O
the	O
case	O
with	O
models	O
such	O
as	O
ComplEx	O
and	O
SimplE.	O
This	O
puts	O
a	O
strict	O
constraint	O
on	O
the	O
relation	O
matrix	O
and	O
imposes	O
a	O
hard	O
limit	O
on	O
the	O
type	O
of	O
transformation	O
applied	O
on	O
entity	O
embeddings	O
.	O

The	O
other	O
way	O
of	O
modeling	O
asymmetry	O
is	O
for	O
subject	O
and	O
object	O
entity	O
embeddings	O
to	O
be	O
equivalent	O
,	O
but	O
representing	O
a	O
relation	O
as	O
a	O
full	O
rank	O
matrix	O
,	O
which	O
is	O
the	O
case	O
with	O
RESCAL	O
.	O

The	O
drawback	O
of	O
the	O
latter	O
approach	O
is	O
quadratic	O
growth	O
of	O
parameter	O
number	O
with	O
the	O
number	O
of	O
relations	O
,	O
which	O
often	O
leads	O
to	O
overfitting	O
,	O
especially	O
for	O
relations	O
with	O
a	O
small	O
number	O
of	O
training	O
triples	O
.	O

TuckER	Method
introduces	O
a	O
novel	O
approach	O
to	O
dealing	O
with	O
asymmetry	O
:	O
by	O
representing	O
relations	O
as	O
vectors	O
,	O
which	O
makes	O
the	O
parameter	O
number	O
grow	O
linearly	O
with	O
the	O
number	O
of	O
relations	O
;	O
and	O
by	O
having	O
an	O
asymmetric	O
relation	O
-	O
agnostic	O
core	O
tensor	O
,	O
which	O
enables	O
knowledge	Task
sharing	Task
between	O
relations	O
.	O

Multiplying	O
with	O
along	O
the	O
second	O
mode	O
,	O
we	O
obtain	O
a	O
full	O
rank	O
relation	O
-	O
specific	O
matrix	O
,	O
which	O
is	O
capable	O
of	O
performing	O
all	O
possible	O
linear	O
transformations	O
on	O
the	O
entity	O
embeddings	O
,	O
i.e.	O
rotation	O
,	O
reflection	O
or	O
stretch	O
,	O
and	O
thus	O
capable	O
of	O
modeling	O
asymmetry	O
.	O

Regardless	O
of	O
what	O
kind	O
of	O
transformation	O
is	O
needed	O
for	O
modeling	O
a	O
particular	O
relation	O
,	O
TuckER	Method
is	O
capable	O
of	O
learning	O
it	O
from	O
the	O
data	O
,	O
rather	O
than	O
through	O
explicitly	O
limiting	O
the	O
relation	O
matrix	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Datasets	O
We	O
evaluate	O
TuckER	Method
using	O
four	O
standard	O
link	O
prediction	O
datasets	O
:	O
FB15k	Material
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
large	O
database	O
of	O
real	O
world	O
facts	O
containing	O
information	O
about	O
films	O
,	O
actors	O
,	O
sports	O
,	O
etc	O
.	O

FB15k	Material
-	Material
237	Material
was	O
created	O
from	O
FB15k	Material
by	O
removing	O
the	O
inverse	O
of	O
many	O
relations	O
that	O
are	O
present	O
in	O
the	O
training	O
set	O
from	O
validation	O
and	O
test	O
sets	O
,	O
making	O
it	O
more	O
difficult	O
for	O
simple	O
models	O
to	O
do	O
well	O
.	O

WN18	Material
is	O
a	O
subset	O
of	O
WordNet	O
,	O
a	O
database	O
containing	O
lexical	O
relations	O
between	O
words	O
.	O

WN18	Material
follows	O
a	O
hierarchical	O
structure	O
.	O

WN18RR	Material
is	O
a	O
subset	O
of	O
WN18	Material
,	O
created	O
by	O
removing	O
the	O
inverse	O
relations	O
from	O
validation	O
and	O
test	O
sets	O
.	O

Number	O
of	O
entities	O
and	O
relations	O
for	O
each	O
dataset	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Implementation	O
and	O
Experiments	O
We	O
implement	O
TuckER	Method
in	O
PyTorch	Method
and	O
make	O
our	O
code	O
available	O
on	O
Github	O
.	O

We	O
choose	O
all	O
hyper	O
-	O
parameters	O
by	O
random	Method
search	Method
based	O
on	O
the	O
validation	Metric
set	Metric
performance	O
.	O

For	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
,	O
we	O
set	O
both	O
entity	O
and	O
relation	O
embedding	O
dimensions	O
to	O
.	O

For	O
WN18	Material
and	O
WN18RR	Material
,	O
which	O
both	O
contain	O
a	O
significantly	O
smaller	O
number	O
of	O
relations	O
relative	O
to	O
the	O
number	O
of	O
entities	O
as	O
well	O
as	O
a	O
small	O
number	O
of	O
relations	O
compared	O
to	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
,	O
we	O
set	O
and	O
.	O

We	O
use	O
both	O
batch	Method
normalization	Method
and	O
dropout	Method
to	O
control	O
overfitting	O
and	O
improve	O
predictions	Task
.	O

We	O
choose	O
the	O
learning	Metric
rate	Metric
from	O
and	O
learning	Metric
rate	Metric
decay	Metric
from	O
.	O

We	O
find	O
the	O
following	O
combinations	O
of	O
learning	Metric
rate	Metric
and	O
learning	Metric
rate	Metric
decay	Metric
to	O
give	O
the	O
best	O
results	O
:	O
for	O
FB15k	Material
,	O
for	O
FB15k	Material
-	Material
237	Material
,	O
for	O
WN18	Material
and	O
for	O
WN18RR	Material
.	O

We	O
train	O
the	O
model	O
using	O
Adam	Method
and	O
set	O
the	O
batch	O
size	O
to	O
128	O
.	O

We	O
evaluate	O
each	O
triple	O
from	O
the	O
test	O
set	O
as	O
in	O
:	O
for	O
a	O
given	O
triple	O
,	O
we	O
generate	O
test	O
triples	O
by	O
keeping	O
the	O
subject	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
object	O
entity	O
with	O
all	O
possible	O
entities	O
and	O
by	O
keeping	O
the	O
object	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
subject	O
entity	O
with	O
all	O
entities	O
.	O

We	O
then	O
rank	O
the	O
scores	O
obtained	O
.	O

We	O
use	O
the	O
filtered	O
setting	O
only	O
,	O
i.e.	O
we	O
remove	O
all	O
other	O
true	O
triples	O
apart	O
from	O
the	O
currently	O
observed	O
test	O
triple	O
.	O

For	O
evaluation	O
,	O
we	O
use	O
two	O
evaluation	Metric
metrics	Metric
used	O
across	O
the	O
link	Task
prediction	Task
literature	Task
:	O
mean	Metric
reciprocal	Metric
rank	Metric
(	O
MRR	Metric
)	O
and	O
hits@	Metric
,	O
.	O

Mean	Metric
reciprocal	Metric
rank	Metric
is	O
the	O
average	O
of	O
the	O
inverse	O
of	O
a	O
mean	O
rank	O
assigned	O
to	O
the	O
true	O
triple	O
over	O
all	O
generated	O
triples	O
.	O

Hits@	Metric
measures	O
the	O
percentage	O
of	O
times	O
the	O
true	O
triple	O
is	O
ranked	O
in	O
the	O
top	O
of	O
the	O
generated	O
triples	O
.	O

The	O
aim	O
is	O
for	O
a	O
model	O
to	O
achieve	O
high	O
MRR	Metric
and	O
hits@	Metric
.	O

subsection	O
:	O
Link	Task
Prediction	Task
Results	O
Link	Task
prediction	Task
results	O
on	O
all	O
four	O
datasets	O
are	O
shown	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

Overall	O
,	O
TuckER	Method
outperforms	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
all	O
metrics	O
across	O
all	O
datasets	O
(	O
apart	O
from	O
hits@10	Metric
on	O
WN18	Material
where	O
a	O
non	Method
-	Method
linear	Method
model	Method
,	O
R	Method
-	Method
GCN	Method
,	O
does	O
better	O
)	O
,	O
which	O
shows	O
that	O
this	O
relatively	O
simple	O
yet	O
fully	O
flexible	O
linear	Method
model	Method
leads	O
to	O
very	O
good	O
performance	O
.	O

Results	O
achieved	O
by	O
TuckER	Method
are	O
not	O
only	O
better	O
than	O
those	O
of	O
other	O
linear	Method
models	Method
,	O
such	O
as	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
but	O
also	O
better	O
than	O
the	O
results	O
of	O
many	O
more	O
complex	O
deep	Method
neural	Method
network	Method
and	Method
reinforcement	Method
learning	Method
architectures	Method
,	O
e.g.	O
R	Method
-	Method
GCN	Method
,	O
MINERVA	Method
,	O
ConvE	Method
and	O
HypER	Method
,	O
demonstrating	O
the	O
expressive	O
power	O
of	O
linear	Method
models	Method
.	O

Even	O
though	O
TuckER	Method
has	O
more	O
parameters	O
than	O
some	O
more	O
simpler	O
linear	Method
models	Method
(	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
)	O
due	O
to	O
the	O
presence	O
of	O
core	O
tensor	O
(	O
containing	O
million	O
parameters	O
for	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
and	O
million	O
parameters	O
for	O
WN18	Material
and	O
WN18RR	Material
)	O
,	O
it	O
consistently	O
obtains	O
better	O
results	O
than	O
any	O
of	O
those	O
models	O
.	O

We	O
believe	O
this	O
is	O
achieved	O
by	O
exploiting	O
knowledge	O
sharing	O
between	O
relations	O
through	O
the	O
core	O
tensor	O
and	O
implicit	Method
regularization	Method
from	O
dropout	Method
,	O
which	O
allows	O
the	O
model	O
to	O
learn	O
which	O
parameters	O
to	O
ignore	O
rather	O
than	O
explicitly	O
setting	O
them	O
to	O
0	O
.	O

We	O
find	O
the	O
value	O
of	O
the	O
dropout	O
parameter	O
to	O
have	O
a	O
significant	O
influence	O
on	O
results	O
,	O
with	O
lower	O
dropout	O
values	O
required	O
for	O
datasets	O
with	O
a	O
higher	O
number	O
of	O
training	O
triples	O
per	O
relation	O
and	O
thus	O
less	O
risk	O
of	O
overfitting	O
(	O
WN18	Material
and	O
WN18RR	Material
)	O
and	O
higher	O
dropout	Metric
values	Metric
required	O
for	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
)	O
.	O

We	O
further	O
note	O
that	O
TuckER	Method
improves	O
the	O
results	O
of	O
all	O
previous	O
linear	Method
models	Method
by	O
a	O
larger	O
margin	O
on	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
e.g.	O
improvement	O
on	O
FB15k	Material
results	O
over	O
ComplEx	Method
,	O
improvement	O
over	O
SimplE	O
on	O
the	O
toughest	Metric
hits@1	Metric
metric	Metric
)	O
,	O
which	O
supports	O
our	O
belief	O
that	O
TuckER	Method
makes	O
use	O
of	O
the	O
parameters	O
shared	O
between	O
similar	O
relations	O
to	O
improve	O
predictions	Task
by	O
multi	Task
-	Task
task	Task
learning	Task
.	O

subsection	O
:	O
Influence	O
of	O
Embedding	Metric
Dimensionality	Metric
In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
derive	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
for	O
full	Task
expressiveness	Task
that	O
is	O
much	O
lower	O
for	O
TuckER	Method
than	O
for	O
simpler	O
linear	Method
models	Method
ComplEx	O
and	O
SimplE.	O
This	O
suggests	O
TuckER	Method
should	O
need	O
a	O
lower	O
embedding	O
dimensionality	O
(	O
i.e.	O
lower	O
rank	O
of	O
the	O
decomposition	O
)	O
for	O
obtaining	O
good	O
results	O
than	O
ComplEx	Method
or	O
SimplE.	O
To	O
test	O
this	O
,	O
we	O
train	O
ComplEx	Method
,	O
SimplE	O
and	O
TuckER	Method
on	O
FB15k	Material
-	Material
237	Material
with	O
embedding	O
sizes	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
obtained	O
MRR	Metric
on	O
the	O
test	O
set	O
for	O
each	O
of	O
the	O
models	O
.	O

We	O
can	O
see	O
from	O
Figure	O
[	O
reference	O
]	O
that	O
the	O
difference	O
between	O
the	O
MRRs	Metric
of	O
ComplEx	Method
,	O
SimplE	O
and	O
TuckER	Method
is	O
approximately	O
constant	O
for	O
embedding	O
sizes	O
100	O
and	O
200	O
.	O

However	O
,	O
for	O
lower	O
embedding	O
sizes	O
,	O
the	O
difference	O
between	O
MRRs	Metric
increases	O
by	O
for	O
embedding	Metric
size	Metric
50	O
and	O
by	O
for	O
embedding	Metric
size	Metric
20	O
for	O
ComplEx	O
and	O
by	O
for	O
embedding	Metric
size	Metric
50	O
and	O
by	O
for	O
embedding	O
size	O
20	O
for	O
SimplE.	O
At	O
embedding	Metric
size	Metric
20	O
,	O
the	O
performance	O
of	O
TuckER	Method
is	O
almost	O
as	O
good	O
as	O
the	O
performance	O
of	O
ComplEx	Method
and	O
SimplE	O
at	O
embedding	O
size	O
200	O
,	O
which	O
supports	O
our	O
initial	O
assumption	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
introduce	O
TuckER	Method
,	O
a	O
relatively	O
simple	O
yet	O
highly	O
flexible	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	O
knowledge	Task
graphs	Task
based	O
on	O
the	O
Tucker	Method
decomposition	Method
of	O
a	O
third	O
-	O
order	O
binary	O
tensor	O
of	O
training	O
set	O
triples	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
standard	O
link	Task
prediction	Task
datasets	Task
.	O

As	O
well	O
as	O
being	O
fully	O
expressive	O
,	O
TuckER	Method
’s	O
number	O
of	O
parameters	O
grows	O
linearly	O
with	O
respect	O
to	O
embedding	O
dimension	O
as	O
the	O
number	O
of	O
entities	O
or	O
relations	O
in	O
a	O
knowledge	O
graph	O
increases	O
.	O

We	O
further	O
show	O
that	O
previous	O
linear	Method
state	Method
-	Method
of	Method
-	Method
the	Method
-	Method
art	Method
models	Method
,	O
RESCAL	Method
,	O
DistMult	Method
,	O
ComplEx	Method
and	O
SimplE	O
,	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O

Future	O
work	O
might	O
include	O
exploring	O
various	O
means	O
of	O
softly	Method
regularizing	Method
the	O
model	O
other	O
than	O
dropout	Method
and	O
finding	O
a	O
way	O
to	O
incorporate	O
background	O
knowledge	O
on	O
individual	O
relation	O
properties	O
into	O
the	O
existing	O
model	O
.	O

bibliography	O
:	O
References	O
Sentence	Method
-	Method
State	Method
LSTM	Method
for	O
Text	Task
Representation	Task
section	O
:	O
Abstract	O
Bi	Method
-	Method
directional	Method
LSTMs	Method
are	O
a	O
powerful	O
tool	O
for	O
text	Task
representation	Task
.	O

On	O
the	O
other	O
hand	O
,	O
they	O
have	O
been	O
shown	O
to	O
suffer	O
various	O
limitations	O
due	O
to	O
their	O
sequential	O
nature	O
.	O

We	O
investigate	O
an	O
alternative	O
LSTM	Method
structure	O
for	O
encoding	Task
text	Task
,	O
which	O
consists	O
of	O
a	O
parallel	O
state	O
for	O
each	O
word	O
.	O

Recurrent	Method
steps	Method
are	O
used	O
to	O
perform	O
local	O
and	O
global	O
information	O
exchange	O
between	O
words	O
simultaneously	O
,	O
rather	O
than	O
incremental	O
reading	O
of	O
a	O
sequence	O
of	O
words	O
.	O

Results	O
on	O
various	O
classification	Task
and	Task
sequence	Task
labelling	Task
benchmarks	Task
show	O
that	O
the	O
proposed	O
model	O
has	O
strong	O
representation	O
power	O
,	O
giving	O
highly	O
competitive	O
performances	O
compared	O
to	O
stacked	Method
BiLSTM	Method
models	Method
with	O
similar	O
parameter	O
numbers	O
.	O

section	O
:	O
Introduction	O
Neural	Method
models	Method
have	O
become	O
the	O
dominant	O
approach	O
in	O
the	O
NLP	O
literature	O
.	O

Compared	O
to	O
handcrafted	O
indicator	O
features	O
,	O
neural	Method
sentence	Method
representations	Method
are	O
less	O
sparse	O
,	O
and	O
more	O
flexible	O
in	O
encoding	O
intricate	O
syntactic	O
and	O
semantic	O
information	O
.	O

Among	O
various	O
neural	Method
networks	Method
for	O
encoding	Task
sentences	Task
,	O
bi	Method
-	Method
directional	Method
LSTMs	Method
(	O
BiLSTM	Method
)	O
[	O
reference	O
]	O
have	O
been	O
a	O
dominant	O
method	O
,	O
giving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
language	Task
modelling	Task
[	O
reference	O
]	O
,	O
machine	Task
translation	Task
[	O
reference	O
]	O
,	O
syntactic	Task
parsing	Task
[	O
reference	O
]	O
and	O
question	Task
answering	Task
[	O
reference	O
]	O
.	O

Despite	O
their	O
success	O
,	O
BiLSTMs	Method
have	O
been	O
shown	O
to	O
suffer	O
several	O
limitations	O
.	O

For	O
example	O
,	O
their	O
inherently	O
sequential	O
nature	O
endows	O
computation	O
non	O
-	O
parallel	O
within	O
the	O
same	O
sentence	O
[	O
reference	O
]	O
,	O
which	O
can	O
lead	O
to	O
a	O
computational	Metric
bottleneck	Metric
,	O
hindering	O
their	O
use	O
in	O
the	O
in	Task
-	Task
dustry	Task
.	O

In	O
addition	O
,	O
local	Method
ngrams	Method
,	O
which	O
have	O
been	O
shown	O
a	O
highly	O
useful	O
source	O
of	O
contextual	O
information	O
for	O
NLP	Task
,	O
are	O
not	O
explicitly	O
modelled	O
[	O
reference	O
]	O
.	O

Finally	O
,	O
sequential	O
information	O
flow	O
leads	O
to	O
relatively	O
weaker	O
power	O
in	O
capturing	O
longrange	O
dependencies	O
,	O
which	O
results	O
in	O
lower	O
performance	O
in	O
encoding	O
longer	O
sentences	O
[	O
reference	O
]	O
.	O

We	O
investigate	O
an	O
alternative	O
recurrent	Method
neural	Method
network	Method
structure	Method
for	O
addressing	O
these	O
issues	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
main	O
idea	O
is	O
to	O
model	O
the	O
hidden	O
states	O
of	O
all	O
words	O
simultaneously	O
at	O
each	O
recurrent	O
step	O
,	O
rather	O
than	O
one	O
word	O
at	O
a	O
time	O
.	O

In	O
particular	O
,	O
we	O
view	O
the	O
whole	O
sentence	O
as	O
a	O
single	O
state	O
,	O
which	O
consists	O
of	O
sub	O
-	O
states	O
for	O
individual	O
words	O
and	O
an	O
overall	O
sentence	O
-	O
level	O
state	O
.	O

To	O
capture	O
local	O
and	O
non	O
-	O
local	O
contexts	O
,	O
states	O
are	O
updated	O
recurrently	O
by	O
exchanging	O
information	O
between	O
each	O
other	O
.	O

Consequently	O
,	O
we	O
refer	O
to	O
our	O
model	O
as	O
sentence	O
-	O
state	O
LSTM	Method
,	O
or	O
S	Method
-	Method
LSTM	Method
in	O
short	O
.	O

Empirically	O
,	O
S	Method
-	Method
LSTM	Method
can	O
give	O
effective	O
sentence	Task
encoding	Task
after	O
3	O
-	O
6	O
recurrent	O
steps	O
.	O

In	O
contrast	O
,	O
the	O
number	O
of	O
recurrent	O
steps	O
necessary	O
for	O
BiLSTM	Method
scales	O
with	O
the	O
size	O
of	O
the	O
sentence	O
.	O

At	O
each	O
recurrent	O
step	O
,	O
information	Task
exchange	Task
is	O
conducted	O
between	O
consecutive	O
words	O
in	O
the	O
sentence	O
,	O
and	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
each	O
word	O
.	O

In	O
particular	O
,	O
each	O
word	O
receives	O
information	O
from	O
its	O
predecessor	O
and	O
successor	O
simultaneously	O
.	O

From	O
an	O
initial	O
state	O
without	O
information	O
exchange	O
,	O
each	O
word	O
-	O
level	O
state	O
can	O
obtain	O
3	O
-	O
gram	O
,	O
5	O
-	O
gram	O
and	O
7	O
-	O
gram	O
information	O
after	O
1	O
,	O
2	O
and	O
3	O
recurrent	O
steps	O
,	O
respectively	O
.	O

Being	O
connected	O
with	O
every	O
word	O
,	O
the	O
sentence	O
-	O
level	O
state	O
vector	O
serves	O
to	O
exchange	O
non	O
-	O
local	O
information	O
with	O
each	O
word	O
.	O

In	O
addition	O
,	O
it	O
can	O
also	O
be	O
used	O
as	O
a	O
global	Method
sentence	Method
-	Method
level	Method
representation	Method
for	O
classification	Task
tasks	Task
.	O

Results	O
on	O
both	O
classification	Task
and	Task
sequence	Task
labelling	Task
show	O
that	O
S	Method
-	Method
LSTM	Method
gives	O
better	O
accuracies	Metric
compared	O
to	O
BiLSTM	Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
while	O
being	O
faster	O
.	O

We	O
release	O
our	O
code	O
and	O
models	O
at	O
https:	O
//	O
github.com	O
/	O
leuchine	O
/	O
S	O
-	O
LSTM	Method
,	O
which	O
include	O
all	O
baselines	O
and	O
the	O
final	O
model	O
.	O

section	O
:	O
Related	O
Work	O
LSTM	Method
[	O
reference	O
]	O
showed	O
its	O
early	O
potentials	O
in	O
NLP	Task
when	O
a	O
neural	Method
machine	Method
translation	Method
system	Method
that	O
leverages	O
LSTM	Method
source	O
encoding	O
gave	O
highly	O
competitive	O
results	O
compared	O
to	O
the	O
best	O
SMT	Method
models	Method
[	O
reference	O
]	O
.	O

LSTM	Method
encoders	O
have	O
since	O
been	O
explored	O
for	O
other	O
tasks	O
,	O
including	O
syntactic	Task
parsing	Task
[	O
reference	O
]	O
,	O
text	Task
classification	Task
[	O
reference	O
]	O
and	O
machine	Task
reading	Task
[	O
reference	O
]	O
.	O

Bidirectional	Method
extensions	Method
have	O
become	O
a	O
standard	O
configuration	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	Metric
among	O
various	O
tasks	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

SLSTMs	Method
are	O
similar	O
to	O
BiLSTMs	Method
in	O
their	O
recurrent	O
bi	O
-	O
directional	O
message	O
flow	O
between	O
words	O
,	O
but	O
different	O
in	O
the	O
design	O
of	O
state	Task
transition	Task
.	O

CNNs	Method
[	O
reference	O
]	O
)	O
also	O
allow	O
better	O
parallelisation	O
compared	O
to	O
LSTMs	Method
for	O
sentence	Task
encoding	Task
[	O
reference	O
]	O
,	O
thanks	O
to	O
parallelism	O
among	O
convolution	Method
filters	Method
.	O

On	O
the	O
other	O
hand	O
,	O
convolution	O
features	O
embody	O
only	O
fix	O
-	O
sized	O
local	O
ngram	O
information	O
,	O
whereas	O
sentence	Method
-	Method
level	Method
feature	Method
aggregation	Method
via	O
pooling	Method
can	O
lead	O
to	O
loss	O
of	O
information	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
uses	O
a	O
global	O
sentence	O
-	O
level	O
node	O
to	O
assemble	O
and	O
back	O
-	O
distribute	O
local	O
information	O
in	O
the	O
recurrent	Method
state	Method
transition	Method
process	Method
,	O
suffering	O
less	O
information	Metric
loss	Metric
compared	O
to	O
pooling	Task
.	O

Attention	Method
[	O
reference	O
]	O
has	O
recently	O
been	O
explored	O
as	O
a	O
standalone	O
method	O
for	O
sentence	Task
encoding	Task
,	O
giving	O
competitive	O
results	O
compared	O
to	O
Bi	O
-	O
LSTM	Method
encoders	O
for	O
neural	Task
machine	Task
translation	Task
[	O
reference	O
]	O
.	O

The	O
attention	Method
mechanism	Method
allows	O
parallelisation	Task
,	O
and	O
can	O
play	O
a	O
similar	O
role	O
to	O
the	O
sentence	O
-	O
level	O
state	O
in	O
S	Method
-	Method
LSTMs	Method
,	O
which	O
uses	O
neural	O
gates	O
to	O
integrate	O
word	O
-	O
level	O
information	O
compared	O
to	O
hierarchical	O
attention	O
.	O

S	Method
-	Method
LSTM	Method
further	O
allows	O
local	O
communication	O
between	O
neighbouring	O
words	O
.	O

Hierarchical	Method
stacking	Method
of	Method
CNN	Method
layers	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
allows	O
better	O
interaction	O
between	O
non	O
-	O
local	O
components	O
in	O
a	O
sentence	O
via	O
incremental	O
levels	O
of	O
abstraction	O
.	O

S	Method
-	Method
LSTM	Method
is	O
similar	O
to	O
hierarchical	Method
attention	Method
and	O
stacked	Method
CNN	Method
in	O
this	O
respect	O
,	O
incrementally	O
refining	O
sentence	Task
representations	Task
.	O

However	O
,	O
S	Method
-	Method
LSTM	Method
models	O
hierarchical	Method
encoding	Method
of	Method
sentence	Method
structure	Method
as	O
a	O
recurrent	Method
state	Method
transition	Method
process	Method
.	O

In	O
nature	O
,	O
our	O
work	O
belongs	O
to	O
the	O
family	O
of	O
LSTM	Method
sentence	O
representations	O
.	O

S	Method
-	Method
LSTM	Method
is	O
inspired	O
by	O
message	Method
passing	Method
over	Method
graphs	Method
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

Graph	Method
-	Method
structure	Method
neural	Method
models	Method
have	O
been	O
used	O
for	O
computer	Task
program	Task
verification	Task
[	O
reference	O
]	O
and	O
image	Task
object	Task
detection	Task
[	O
reference	O
]	O
.	O

The	O
closest	O
previous	O
work	O
in	O
NLP	Task
includes	O
the	O
use	O
of	O
convolutional	Method
neural	Method
networks	Method
[	O
reference	O
]	O
and	O
DAG	Method
LSTMs	Method
[	O
reference	O
]	O
for	O
modelling	Task
syntactic	Task
structures	Task
.	O

Compared	O
to	O
our	O
work	O
,	O
their	O
motivations	O
and	O
network	O
structures	O
are	O
highly	O
different	O
.	O

In	O
particular	O
,	O
the	O
DAG	Method
LSTM	Method
of	O
[	O
reference	O
]	O
is	O
a	O
natural	O
extension	O
of	O
tree	Method
LSTM	Method
[	O
reference	O
]	O
,	O
and	O
is	O
sequential	O
rather	O
than	O
parallel	O
in	O
nature	O
.	O

To	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
investigate	O
a	O
graph	Method
RNN	Method
for	O
encoding	Task
sentences	Task
,	O
proposing	O
parallel	O
graph	O
states	O
for	O
integrating	O
word	O
-	O
level	O
and	O
sentence	O
-	O
level	O
information	O
.	O

In	O
this	O
perspective	O
,	O
our	O
contribution	O
is	O
similar	O
to	O
that	O
of	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
introducing	O
a	O
neural	Method
representation	Method
to	O
the	O
NLP	O
literature	O
.	O

section	O
:	O
Model	O
Given	O
a	O
sentence	O
s	O
=	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O

.	O

.	O

,	O
w	O
n	O
,	O
where	O
w	O
i	O
represents	O
the	O
ith	O
word	O
and	O
n	O
is	O
the	O
sentence	O
length	O
,	O
our	O
goal	O
is	O
to	O
find	O
a	O
neural	Method
representation	Method
of	Method
s	Method
,	O
which	O
consists	O
of	O
a	O
hidden	O
vector	O
h	O
i	O
for	O
each	O
input	O
word	O
w	O
i	O
,	O
and	O
a	O
global	O
sentence	O
-	O
level	O
hidden	O
vector	O
g.	O
Here	O
h	O
i	O
represents	O
syntactic	O
and	O
semantic	O
features	O
for	O
w	O
i	O
under	O
the	O
sentential	O
context	O
,	O
while	O
g	O
represents	O
features	O
for	O
the	O
whole	O
sentence	O
.	O

Following	O
previous	O
work	O
,	O
we	O
additionally	O
add	O
s	O
and	O
/	O
s	O
to	O
the	O
two	O
ends	O
of	O
the	O
sentence	O
as	O
w	O
0	O
and	O
w	O
n	O
+	O
1	O
,	O
respectively	O
.	O

section	O
:	O
Baseline	O
BiLSTM	Method
The	O
baseline	Method
BiLSTM	Method
model	Method
consists	O
of	O
two	O
LSTM	Method
components	O
,	O
which	O
process	O
the	O
input	O
in	O
the	O
forward	O
left	O
-	O
to	O
-	O
right	O
and	O
the	O
backward	O
rightto	O
-	O
left	O
directions	O
,	O
respectively	O
.	O

In	O
each	O
direction	O
,	O
the	O
reading	O
of	O
input	O
words	O
is	O
modelled	O
as	O
a	O
recurrent	Method
process	Method
with	O
a	O
single	O
hidden	O
state	O
.	O

Given	O
an	O
initial	O
value	O
,	O
the	O
state	O
changes	O
its	O
value	O
recurrently	O
,	O
each	O
time	O
consuming	O
an	O
incoming	O
word	O
.	O

Take	O
the	O
forward	O
LSTM	Method
component	O
for	O
example	O
.	O

Denoting	O
the	O
initial	O
state	O
as	O
−	O
→	O
h	O
0	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
the	O
recurrent	O
state	O
transition	O
step	O
for	O
calculating	O
−	O
→	O
h	O
1	O
,	O
.	O

.	O

.	O

,	O
−	O
→	O
h	O
n	O
+	O
1	O
is	O
defined	O
as	O
follows	O
[	O
reference	O
]	O
:	O
where	O
x	O
t	O
denotes	O
the	O
word	Method
representation	Method
of	Method
w	Method
t	Method
;	O
i	O
t	O
,	O
o	O
t	O
,	O
f	O
t	O
and	O
u	O
t	O
represent	O
the	O
values	O
of	O
an	O
input	O
gate	O
,	O
an	O
output	O
gate	O
,	O
a	O
forget	O
gate	O
and	O
an	O
actual	O
input	O
at	O
time	O
step	O
t	O
,	O
respectively	O
,	O
which	O
controls	O
the	O
information	O
flow	O
for	O
a	O
recurrent	Method
cell	Method
−	O
→	O
c	O
t	O
and	O
the	O
state	O
vector	O
−	O
→	O
h	O
t	O
;	O
W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
i	O
,	O
o	O
,	O
f	O
,	O
u	O
}	O
)	O
are	O
model	O
parameters	O
.	O

σ	Method
is	O
the	O
sigmoid	Method
function	Method
.	O

The	O
backward	O
LSTM	Method
component	O
follows	O
the	O
same	O
recurrent	Method
state	Method
transition	Method
process	Method
as	O
described	O
in	O
Eq	O
1	O
.	O

Starting	O
from	O
an	O
initial	O
state	O
h	O
n	O
+	O
1	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
it	O
reads	O
the	O
input	O
x	O
n	O
,	O
x	O
n−1	O
,	O
.	O

.	O

.	O

,	O
x	O
0	O
,	O
changing	O
its	O
value	O
to	O
The	O
BiLSTM	Method
model	O
uses	O
the	O
concatenated	O
value	O
of	O
−	O
→	O
h	O
t	O
and	O
←	O
−	O
h	O
t	O
as	O
the	O
hidden	O
vector	O
for	O
w	O
t	O
:	O
A	O
single	O
hidden	Method
vector	Method
representation	Method
g	Method
of	O
the	O
whole	O
input	O
sentence	O
can	O
be	O
obtained	O
using	O
the	O
final	O
state	O
values	O
of	O
the	O
two	O
LSTM	Method
components	O
:	O
Stacked	Method
BiLSTM	Method
Multiple	Method
layers	Method
of	Method
BiLTMs	Method
can	O
be	O
stacked	O
for	O
increased	O
representation	O
power	O
,	O
where	O
the	O
hidden	O
vectors	O
of	O
a	O
lower	O
layer	O
are	O
used	O
as	O
inputs	O
for	O
an	O
upper	O
layer	O
.	O

Different	O
model	O
parameters	O
are	O
used	O
in	O
each	O
stacked	Method
BiLSTM	Method
layer	Method
.	O

section	O
:	O
Sentence	Method
-	Method
State	Method
LSTM	Method
Formally	O
,	O
an	O
S	O
-	O
LSTM	Method
state	O
at	O
time	O
step	O
t	O
can	O
be	O
denoted	O
by	O
:	O
which	O
consists	O
of	O
a	O
sub	O
state	O
h	O
t	O
i	O
for	O
each	O
word	O
w	O
i	O
and	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
g	O
t	O
.	O

S	Method
-	Method
LSTM	Method
uses	O
a	O
recurrent	Method
state	Method
transition	Method
process	Method
to	O
model	O
information	O
exchange	O
between	O
sub	O
states	O
,	O
which	O
enriches	O
state	Method
representations	Method
incrementally	O
.	O

For	O
the	O
initial	O
state	O
H	O
0	O
,	O
we	O
set	O
h	O
0	O
i	O
=	O
g	O
0	O
=	O
h	O
0	O
,	O
where	O
h	O
0	O
is	O
a	O
parameter	O
.	O

The	O
state	O
transition	O
from	O
H	O
t−1	O
to	O
H	O
t	O
consists	O
of	O
sub	O
state	O
transitions	O
from	O
h	O
t−1	O
i	O
to	O
h	O
t	O
i	O
and	O
from	O
g	O
t−1	O
to	O
g	O
t	O
.	O

We	O
take	O
an	O
LSTM	Method
structure	O
similar	O
to	O
the	O
baseline	O
BiLSTM	Method
for	O
modelling	Task
state	Task
transition	Task
,	O
using	O
a	O
recurrent	O
cell	O
c	O
t	O
i	O
for	O
each	O
w	O
i	O
and	O
a	O
cell	O
c	O
t	O
g	O
for	O
g.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
value	O
of	O
each	O
h	O
t	O
i	O
is	O
computed	O
based	O
on	O
the	O
values	O
of	O
x	O
i	O
,	O
h	O
i	O
+	O
1	O
and	O
g	O
t−1	O
,	O
together	O
with	O
their	O
corresponding	O
cell	O
values	O
:	O
where	O
ξ	O
t	O
i	O
is	O
the	O
concatenation	O
of	O
hidden	O
vectors	O
of	O
a	O
context	O
window	O
,	O
and	O
where	O
f	O
t	O
0	O
,	O
.	O

.	O

.	O

,	O
f	O
t	O
n	O
+	O
1	O
and	O
f	O
t	O
g	O
are	O
gates	O
controlling	O
information	O
from	O
c	O
t−1	O
0	O
,	O
.	O

.	O

.	O

,	O
c	O
t−1	O
n	O
+	O
1	O
and	O
c	O
t−1	O
g	O
,	O
respectively	O
,	O
which	O
are	O
normalised	O
.	O

o	O
t	O
is	O
an	O
output	O
gate	O
from	O
the	O
recurrent	Method
cell	Method
c	O
t	O
g	O
to	O
g	O
t	O
.	O

W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
g	O
,	O
f	O
,	O
o	O
}	O
)	O
are	O
model	O
parameters	O
.	O

section	O
:	O
Contrast	O
with	O
BiLSTM	Method
The	O
difference	O
between	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
can	O
be	O
understood	O
with	O
respect	O
to	O
their	O
recurrent	O
states	O
.	O

While	O
BiL	Method
-	Method
STM	Method
uses	O
only	O
one	O
state	O
in	O
each	O
direction	O
to	O
represent	O
the	O
subsequence	O
from	O
the	O
beginning	O
to	O
a	O
certain	O
word	O
,	O
S	Method
-	Method
LSTM	Method
uses	O
a	O
structural	O
state	O
to	O
represent	O
the	O
full	O
sentence	O
,	O
which	O
consists	O
of	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
and	O
n	O
+	O
2	O
word	O
-	O
level	O
sub	O
states	O
,	O
simultaneously	O
.	O

Different	O
from	O
BiLSTMs	Method
,	O
for	O
which	O
h	O
t	O
at	O
different	O
time	O
steps	O
are	O
used	O
to	O
represent	O
w	O
0	O
,	O
.	O

.	O

.	O

,	O
w	O
n	O
+	O
1	O
,	O
respectively	O
,	O
the	O
word	O
-	O
level	O
states	O
h	O
t	O
i	O
and	O
sentence	O
-	O
level	O
state	O
g	O
t	O
of	O
S	Method
-	Method
LSTMs	Method
directly	O
correspond	O
to	O
the	O
goal	O
outputs	O
h	O
i	O
and	O
g	O
,	O
as	O
introduced	O
in	O
the	O
beginning	O
of	O
this	O
section	O
.	O

As	O
t	O
increases	O
from	O
0	O
,	O
h	O
t	O
i	O
and	O
g	O
t	O
are	O
enriched	O
with	O
increasingly	O
deeper	O
context	O
information	O
.	O

From	O
the	O
perspective	O
of	O
information	Task
flow	Task
,	O
BiL	Method
-	Method
STM	Method
passes	O
information	O
from	O
one	O
end	O
of	O
the	O
sentence	O
to	O
the	O
other	O
.	O

As	O
a	O
result	O
,	O
the	O
number	O
of	O
time	O
steps	O
scales	O
with	O
the	O
size	O
of	O
the	O
input	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
allows	O
bi	O
-	O
directional	O
information	O
flow	O
at	O
each	O
word	O
simultaneously	O
,	O
and	O
additionally	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
every	O
wordlevel	O
state	O
.	O

At	O
each	O
step	O
,	O
each	O
h	O
i	O
captures	O
an	O
increasing	O
larger	O
ngram	O
context	O
,	O
while	O
additionally	O
communicating	O
globally	O
to	O
all	O
other	O
h	O
j	O
via	O
g.	O
The	O
optimal	O
number	O
of	O
recurrent	O
steps	O
is	O
decided	O
by	O
the	O
end	Metric
-	Metric
task	Metric
performance	Metric
,	O
and	O
does	O
not	O
necessarily	O
scale	O
with	O
the	O
sentence	O
size	O
.	O

As	O
a	O
result	O
,	O
S	Method
-	Method
LSTM	Method
can	O
potentially	O
be	O
both	O
more	O
efficient	O
and	O
more	O
accurate	O
compared	O
with	O
BiLSTMs	Method
.	O

Increasing	O
window	O
size	O
.	O

By	O
default	O
S	Method
-	Method
LSTM	Method
exchanges	O
information	O
only	O
between	O
neighbouring	O
words	O
,	O
which	O
can	O
be	O
seen	O
as	O
adopting	O
a	O
1	O
-	O
word	O
window	O
on	O
each	O
side	O
.	O

The	O
window	O
size	O
can	O
be	O
extended	O
to	O
2	O
,	O
3	O
or	O
more	O
words	O
in	O
order	O
to	O
allow	O
more	O
communication	O
in	O
a	O
state	O
transition	O
,	O
expediting	O
information	Task
exchange	Task
.	O

To	O
this	O
end	O
,	O
we	O
modify	O
Eq	O
2	O
,	O
integrating	O
additional	O
context	O
words	O
to	O
ξ	O
t	O
i	O
,	O
with	O
extended	O
gates	O
and	O
cells	O
.	O

For	O
example	O
,	O
with	O
a	O
window	O
size	O
of	O
2	O
,	O
We	O
study	O
the	O
effectiveness	O
of	O
window	O
size	O
in	O
our	O
experiments	O
.	O

Additional	O
sentence	O
-	O
level	O
nodes	O
.	O

By	O
default	O
S	Method
-	Method
LSTM	Method
uses	O
one	O
sentence	O
-	O
level	O
node	O
.	O

One	O
way	O
of	O
enriching	O
the	O
parameter	O
space	O
is	O
to	O
add	O
more	O
sentence	O
-	O
level	O
nodes	O
,	O
each	O
communicating	O
with	O
word	O
-	O
level	O
nodes	O
in	O
the	O
same	O
way	O
as	O
described	O
by	O
Eq	O
3	O
.	O

In	O
addition	O
,	O
different	O
sentence	O
-	O
level	O
nodes	O
can	O
communicate	O
with	O
each	O
other	O
during	O
state	O
transition	O
.	O

When	O
one	O
sentence	O
-	O
level	O
node	O
is	O
used	O
for	O
classification	O
outputs	O
,	O
the	O
other	O
sentencelevel	O
node	O
can	O
serve	O
as	O
hidden	O
memory	O
units	O
,	O
or	O
latent	O
features	O
.	O

We	O
study	O
the	O
effectiveness	O
of	O
multiple	O
sentence	O
-	O
level	O
nodes	O
empirically	O
.	O

section	O
:	O
Task	O
settings	O
We	O
consider	O
two	O
task	O
settings	O
,	O
namely	O
classification	Task
and	O
sequence	Task
labelling	Task
.	O

For	O
classification	Task
,	O
g	O
is	O
fed	O
to	O
a	O
softmax	Method
classification	Method
layer	Method
:	O
where	O
y	O
is	O
the	O
probability	O
distribution	O
of	O
output	O
class	O
labels	O
and	O
W	O
c	O
and	O
b	O
c	O
are	O
model	O
parameters	O
.	O

For	O
sequence	Task
labelling	Task
,	O
each	O
h	O
i	O
can	O
be	O
used	O
as	O
feature	Method
representation	Method
for	O
a	O
corresponding	O
word	O
w	O
i	O
.	O

External	O
attention	O
It	O
has	O
been	O
shown	O
that	O
summation	O
of	O
hidden	O
states	O
using	O
attention	O
[	O
reference	O
][	O
reference	O
]	O
give	O
better	O
accuracies	Metric
compared	O
to	O
using	O
the	O
end	O
states	O
of	O
BiLSTMs	Method
.	O

We	O
study	O
the	O
influence	O
of	O
attention	O
on	O
both	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
for	O
classification	Task
.	O

In	O
particular	O
,	O
additive	O
attention	O
(	O
Bahdanau	O
Here	O
W	O
α	O
,	O
u	O
and	O
b	O
α	O
are	O
model	O
parameters	O
.	O

External	O
CRF	O
For	O
sequential	Task
labelling	Task
,	O
we	O
use	O
a	O
CRF	Method
layer	Method
on	O
top	O
of	O
the	O
hidden	O
vectors	O
h	O
1	O
,	O
h	O
2	O
,	O
.	O

.	O

.	O

,	O
h	O
n	O
for	O
calculating	O
the	O
conditional	O
probabilities	O
of	O
label	O
sequences	O
[	O
reference	O
]	O
:	O
where	O
W	O
y	O
i−1	O
,	O
y	O
i	O
s	O
and	O
b	O
y	O
i−1	O
,	O
y	O
i	O
s	O
are	O
parameters	O
specific	O
to	O
two	O
consecutive	O
labels	O
y	O
i−1	O
and	O
y	O
i	O
.	O

For	O
training	Task
,	O
standard	O
log	Method
-	Method
likelihood	Method
loss	Method
is	O
used	O
with	O
L	Method
2	Method
regularization	Method
given	O
a	O
set	O
of	O
gold	O
-	O
standard	O
instances	O
.	O

section	O
:	O
Experiments	O
We	O
empirically	O
compare	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
on	O
different	O
classification	Task
and	Task
sequence	Task
labelling	Task
tasks	Task
.	O

All	O
experiments	O
are	O
conducted	O
using	O
a	O
GeForce	Method
GTX	Method
1080	Method
GPU	Method
with	O
8	O
GB	O
memory	O
.	O

[	O
reference	O
]	O
)	O
.	O

Statistics	O
of	O
the	O
four	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
.	O

Hyperparameters	Method
.	O

We	O
initialise	O
word	O
embeddings	O
using	O
GloVe	Method
[	O
reference	O
]	O
)	O
300	Method
dimensional	Method
embeddings	Method
.	O

1	O
Embeddings	Method
are	O
finetuned	O
during	O
model	Method
training	Method
for	O
all	O
tasks	O
.	O

Dropout	Method
[	O
reference	O
]	O
)	O
is	O
applied	O
to	O
embedding	O
hidden	O
states	O
,	O
with	O
a	O
rate	O
of	O
0.5	O
.	O

All	O
models	O
are	O
optimised	O
using	O
the	O
Adam	Method
optimizer	Method
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
,	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.001	O
and	O
a	O
decay	Metric
rate	Metric
of	Metric
0.97	Metric
.	O

Gradients	O
are	O
clipped	O
at	O
3	O
and	O
a	O
batch	O
size	O
of	O
10	O
is	O
adopted	O
.	O

Sentences	O
with	O
similar	O
lengths	O
are	O
batched	O
together	O
.	O

The	O
L2	O
regularization	O
parameter	O
is	O
set	O
to	O
0.001	O
.	O

section	O
:	O
Development	O
Experiments	O
We	O
use	O
the	O
movie	Material
review	Material
development	O
data	O
to	O
investigate	O
different	O
configurations	O
of	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
.	O

For	O
S	Method
-	Method
LSTMs	Method
,	O
the	O
default	O
configuration	O
uses	O
s	O
and	O
/	O
s	O
words	O
for	O
augmenting	O
words	O
Hyperparameters	O
:	O
Table	O
2	O
shows	O
the	O
development	O
results	O
of	O
various	O
S	O
-	O
LSTM	Method
settings	O
,	O
where	O
Time	O
refers	O
to	O
training	O
time	O
per	O
epoch	O
.	O

Without	O
the	O
sentence	O
-	O
level	O
node	O
,	O
the	O
accuracy	Metric
of	O
S	Method
-	Method
LSTM	Method
drops	O
to	O
81.76	O
%	O
,	O
demonstrating	O
the	O
necessity	O
of	O
global	Task
information	Task
exchange	Task
.	O

Adding	O
one	O
additional	O
sentence	O
-	O
level	O
node	O
as	O
described	O
in	O
Section	O
3.2	O
does	O
not	O
lead	O
to	O
accuracy	Metric
improvements	O
,	O
although	O
the	O
number	O
of	O
parameters	O
and	O
decoding	Metric
time	Metric
increase	O
accordingly	O
.	O

As	O
a	O
result	O
,	O
we	O
use	O
only	O
1	O
sentence	O
-	O
level	O
node	O
for	O
the	O
remaining	O
experiments	O
.	O

The	O
accuracies	Metric
of	O
S	Method
-	Method
LSTM	Method
increases	O
as	O
the	O
hidden	O
layer	O
size	O
for	O
each	O
node	O
increases	O
from	O
100	O
to	O
300	O
,	O
but	O
does	O
not	O
further	O
increase	O
when	O
the	O
size	O
increases	O
beyond	O
300	O
.	O

We	O
fix	O
the	O
hidden	O
size	O
to	O
300	O
accordingly	O
.	O

Without	O
using	O
s	O
and	O
/	Method
s	Method
,	O
the	O
performance	O
of	O
S	Method
-	Method
LSTM	Method
drops	O
from	O
82.64	O
%	O
to	O
82.36	O
%	O
,	O
showing	O
the	O
effectiveness	O
of	O
having	O
these	O
additional	O
nodes	O
.	O

Hyperparameters	Method
for	O
BiLSTM	Method
models	O
are	O
also	O
set	O
according	O
to	O
the	O
development	O
data	O
,	O
which	O
we	O
omit	O
here	O
.	O

State	O
transition	O
.	O

In	O
Table	O
2	O
,	O
the	O
number	O
of	O
recurrent	O
state	O
transition	O
steps	O
of	O
S	Method
-	Method
LSTM	Method
is	O
decided	O
according	O
to	O
the	O
best	O
development	O
performance	O
.	O

Figure	O
2	O
draws	O
the	O
development	O
accuracies	Metric
of	O
SLSTMs	Method
with	O
various	O
window	O
sizes	O
against	O
the	O
number	O
of	O
recurrent	O
steps	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
when	O
the	O
number	O
of	O
time	O
steps	O
increases	O
from	O
1	O
to	O
11	O
,	O
the	O
accuracies	Metric
generally	O
increase	O
,	O
before	O
reaching	O
a	O
maximum	O
value	O
.	O

This	O
shows	O
the	O
effectiveness	O
of	O
recurrent	Task
information	Task
exchange	Task
in	O
S	O
-	O
LSTM	Method
state	O
transition	O
.	O

On	O
the	O
other	O
hand	O
,	O
no	O
significant	O
differences	O
are	O
observed	O
on	O
the	O
peak	O
accuracies	Metric
given	O
by	O
different	O
window	O
sizes	O
,	O
although	O
a	O
larger	O
window	O
size	O
(	O
e.g.	O
4	O
)	O
generally	O
results	O
in	O
faster	O
plateauing	O
.	O

This	O
can	O
be	O
be	O
explained	O
by	O
the	O
intuition	O
that	O
information	O
exchange	O
between	O
distant	O
nodes	O
can	O
be	O
achieved	O
using	O
more	O
recurrent	O
steps	O
under	O
a	O
smaller	O
window	O
size	O
,	O
as	O
can	O
be	O
achieved	O
using	O
fewer	O
steps	O
under	O
a	O
larger	O
window	O
size	O
.	O

Considering	O
efficiency	O
,	O
we	O
choose	O
a	O
window	O
size	O
of	O
1	O
for	O
the	O
remaining	O
experiments	O
,	O
setting	O
the	O
number	O
of	O
recurrent	O
steps	O
to	O
9	O
according	O
to	O
Figure	O
2	O
.	O

S	Method
-	Method
LSTM	Method
vs	O
BiLSTM	Method
:	O
As	O
shown	O
in	O
Table	O
3	O
,	O
BiLSTM	Method
gives	O
significantly	O
better	O
accuracies	Metric
compared	O
to	O
uni	O
-	O
directional	O
LSTM	Method
2	O
,	O
with	O
the	O
training	Metric
time	Metric
per	O
epoch	O
growing	O
from	O
67	O
seconds	O
to	O
106	O
seconds	O
.	O

Stacking	O
2	O
layers	O
of	O
BiLSTM	Method
gives	O
further	O
improvements	O
to	O
development	O
results	O
,	O
with	O
a	O
larger	O
time	O
of	O
207	O
seconds	O
.	O

3	O
layers	O
of	O
stacked	O
BiLSTM	Method
does	O
not	O
further	O
improve	O
the	O
results	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
a	O
development	O
result	O
of	O
82.64	O
%	O
,	O
which	O
is	O
significantly	O
better	O
compared	O
to	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
,	O
with	O
a	O
smaller	O
number	O
of	O
model	O
parameters	O
and	O
a	O
shorter	O
time	O
of	O
65	O
seconds	O
.	O

We	O
additionally	O
make	O
comparisons	O
with	O
stacked	Method
CNNs	Method
and	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
shown	O
in	O
Table	O
3	O
(	O
the	O
CNN	O
and	O
Transformer	O
rows	O
)	O
,	O
where	O
N	O
indicates	O
the	O
number	O
of	O
attention	O
layers	O
.	O

CNN	Method
is	O
the	O
most	O
efficient	O
among	O
all	O
models	O
compared	O
,	O
with	O
the	O
smallest	O
model	O
size	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
3	Method
-	Method
layer	Method
stacked	Method
CNN	Method
gives	O
an	O
accuracy	Metric
of	O
81.46	O
%	O
,	O
which	O
is	O
also	O
the	O
lowest	O
compared	O
with	O
BiLSTM	Method
,	O
hierarchical	Method
attention	Method
and	O
S	Method
-	Method
LSTM	Method
.	O

The	O
best	O
performance	O
of	O
hierarchical	Task
attention	Task
is	O
between	O
single	Method
-	Method
layer	Method
and	O
two	Method
-	Method
layer	Method
BiLSTMs	Method
in	O
terms	O
of	O
both	O
accuracy	Metric
and	O
efficiency	Metric
.	O

S	Method
-	Method
LSTM	Method
gives	O
significantly	O
better	O
accuracies	Metric
compared	O
with	O
both	O
CNN	Method
and	O
hierarchical	Method
attention	Method
.	O

Influence	O
of	O
external	Method
attention	Method
mechanism	Method
.	O

Table	O
3	O
additionally	O
shows	O
the	O
results	O
of	O
BiLSTM	Method
and	O
S	Method
-	Method
LSTM	Method
when	O
external	O
attention	O
is	O
used	O
as	O
described	O
in	O
Section	O
3.3	O
.	O

Attention	Method
leads	O
to	O
improved	O
accuracies	Metric
for	O
both	O
BiLSTM	Method
and	O
S	Method
-	Method
LSTM	Method
in	O
classification	Task
,	O
with	O
S	Method
-	Method
LSTM	Method
still	O
outperforming	O
BiLSTM	Method
significantly	O
.	O

The	O
result	O
suggests	O
that	O
external	Method
techniques	Method
such	O
as	O
attention	O
can	O
play	O
orthogonal	O
roles	O
compared	O
with	O
internal	O
recurrent	O
structures	O
,	O
therefore	O
benefiting	O
both	O
BiLSTMs	Method
and	O
S	Method
-	Method
LSTMs	Method
.	O

Similar	O
observations	O
are	O
found	O
using	O
external	Method
CRF	Method
layers	Method
for	O
sequence	Task
labelling	Task
.	O

section	O
:	O
Final	O
Results	O
for	O
Classification	Task
The	O
final	O
results	O
on	O
the	O
movie	Material
review	Material
and	O
rich	O
text	O
classification	O
datasets	O
are	O
shown	O
in	O
Tables	O
4	O
and	O
5	O
,	O
respectively	O
.	O

In	O
addition	O
to	O
training	Metric
time	Metric
per	O
epoch	O
,	O
test	Metric
times	Metric
are	O
additionally	O
reported	O
.	O

We	O
use	O
the	O
best	O
settings	O
on	O
the	O
movie	Material
review	Material
development	O
dataset	O
for	O
both	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
.	O

The	O
step	O
number	O
for	O
S	Method
-	Method
LSTMs	Method
is	O
set	O
to	O
9	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
the	O
final	O
results	O
on	O
the	O
movie	Material
review	Material
dataset	Material
are	O
consistent	O
with	O
the	O
development	O
results	O
,	O
where	O
S	Method
-	Method
LSTM	Method
outperforms	O
BiL	Method
-	Method
STM	Method
significantly	O
,	O
with	O
a	O
faster	O
speed	O
.	O

Observations	O
on	O
CNN	Method
and	O
hierarchical	Task
attention	Task
are	O
consistent	O
with	O
the	O
development	O
results	O
.	O

S	Method
-	Method
LSTM	Method
also	O
gives	O
highly	O
competitive	O
results	O
when	O
compared	O
with	O
existing	O
methods	O
in	O
the	O
literature	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
among	O
the	O
16	O
datasets	O
of	O
[	O
reference	O
]	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
the	O
best	O
results	O
on	O
12	O
,	O
compared	O
with	O
BiLSTM	Method
and	O
2	O
layered	O
BiL	Method
-	Method
STM	Method
models	O
.	O

The	O
average	O
accuracy	Metric
of	O
S	Method
-	Method
LSTM	Method
is	O
85.6	O
%	O
,	O
significantly	O
higher	O
compared	O
with	O
84.9	O
%	O
by	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
.	O

3	Method
-	Method
layer	Method
stacked	Method
BiL	Method
-	Method
STM	Method
gives	O
an	O
average	O
accuracy	Metric
of	O
84.57	O
%	O
,	O
which	O
is	O
lower	O
compared	O
to	O
a	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
,	O
with	O
a	O
training	Metric
time	Metric
per	O
epoch	O
of	O
423.6	O
seconds	O
.	O

The	O
relative	Metric
speed	Metric
advantage	Metric
of	O
S	Method
-	Method
LSTM	Method
over	O
BiLSTM	Method
is	O
larger	O
on	O
the	O
16	O
datasets	O
as	O
compared	O
to	O
the	O
movie	Material
review	Material
test	O
test	O
.	O

This	O
is	O
because	O
the	O
average	O
length	O
of	O
inputs	O
is	O
larger	O
on	O
the	O
16	O
datasets	O
(	O
see	O
Section	O
4.5	O
)	O
.	O

section	O
:	O
Final	O
Results	O
for	O
Sequence	Task
Labelling	Task
Bi	Method
-	Method
directional	Method
RNN	Method
-	Method
CRF	Method
structures	Method
,	O
and	O
in	O
particular	O
BiLSTM	Method
-	O
CRFs	O
,	O
have	O
achieved	O
the	O
state	O
of	O
the	O
art	O
in	O
the	O
literature	O
for	O
sequence	Task
labelling	Task
tasks	Task
,	O
including	O
POS	Task
-	Task
tagging	Task
and	O
NER	Task
.	O

We	O
compare	O
S	Method
-	Method
LSTM	Method
-	Method
CRF	Method
with	O
BiLSTM	Method
-	Method
CRF	Method
for	O
sequence	Task
labelling	Task
,	O
using	O
the	O
same	O
settings	O
as	O
decided	O
on	O
the	O
movie	Material
review	Material
development	O
experiments	O
for	O
both	O
BiLSTMs	Method
and	O
S	Method
-	Method
LSTMs	Method
.	O

For	O
the	O
latter	O
,	O
we	O
decide	O
the	O
number	O
of	O
recurrent	O
steps	O
on	O
the	O
respective	O
development	O
sets	O
for	O
sequence	Task
labelling	Task
.	O

The	O
POS	Metric
accuracies	Metric
and	O
NER	Metric
F1	Metric
-	Metric
scores	Metric
against	O
the	O
number	O
of	O
recurrent	O
steps	O
are	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
respectively	O
.	O

For	O
POS	Task
tagging	Task
,	O
the	O
best	O
step	O
number	O
is	O
set	O
to	O
7	O
,	O
with	O
a	O
development	O
accuracy	Metric
of	O
97.58	O
%	O
.	O

For	O
NER	Task
,	O
the	O
step	O
number	O
is	O
set	O
to	O
9	O
,	O
with	O
a	O
development	O
F1	Metric
-	Metric
score	Metric
of	O
94.98	O
%	O
.	O

As	O
can	O
be	O
seen	O
in	O
(	O
Table	O
7	O
)	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
an	O
F1	Metric
-	Metric
score	Metric
of	O
91.57	O
%	O
on	O
the	O
CoNLL	Material
test	Material
set	Material
,	O
which	O
is	O
significantly	O
better	O
compared	O
with	O
BiLSTMs	Method
.	O

Stacking	O
more	O
layers	O
of	O
BiLSTMs	Method
leads	O
to	O
slightly	O
better	O
F1	Metric
-	Metric
scores	Metric
compared	O
with	O
a	O
single	Method
-	Method
layer	Method
BiL	Method
-	Method
STM	Method
.	O

Our	O
BiLSTM	Method
results	O
are	O
comparable	O
to	O
the	O
results	O
reported	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
who	O
also	O
use	O
bidirectional	Method
RNN	Method
-	Method
CRF	Method
structures	Method
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
the	O
best	O
reported	O
results	O
under	O
the	O
same	O
settings	O
.	O

In	O
the	O
second	O
section	O
of	O
Table	O
7	O
learning	O
using	O
additional	O
language	O
model	O
objectives	O
,	O
obtaining	O
an	O
F	Metric
-	Metric
score	Metric
of	O
86.26	O
%	O
;	O
[	O
reference	O
]	O
leverage	O
character	Method
-	Method
level	Method
language	Method
models	Method
,	O
obtaining	O
an	O
F	Metric
-	Metric
score	Metric
of	O
91.93	O
%	O
,	O
which	O
is	O
the	O
current	O
best	O
result	O
on	O
the	O
dataset	O
.	O

All	O
the	O
three	O
models	O
are	O
based	O
on	O
BiLSTM	Method
-	Method
CRF	Method
.	O

On	O
the	O
other	O
hand	O
,	O
these	O
semi	Method
-	Method
supervised	Method
learning	Method
techniques	Method
are	O
orthogonal	O
to	O
our	O
work	O
,	O
and	O
can	O
potentially	O
be	O
used	O
for	O
S	Method
-	Method
LSTM	Method
also	O
.	O

section	O
:	O
Analysis	O
Figure	O
4	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
the	O
accuracies	Metric
against	O
the	O
sentence	O
length	O
on	O
the	O
movie	Material
review	Material
and	O
CoNLL	Material
datasets	Material
,	O
respectively	O
,	O
where	O
test	O
samples	O
are	O
binned	O
in	O
batches	O
of	O
80	O
.	O

We	O
find	O
that	O
the	O
performances	O
of	O
both	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
decrease	O
as	O
the	O
sentence	O
length	O
increases	O
.	O

On	O
the	O
other	O
hand	O
,	O
S	Method
-	Method
LSTM	Method
demonstrates	O
relatively	O
better	O
robustness	Metric
compared	O
to	O
BiLSTMs	Method
.	O

This	O
confirms	O
our	O
intuition	O
that	O
a	O
sentence	O
-	O
level	O
node	O
can	O
facilitate	O
better	O
non	Task
-	Task
local	Task
communication	Task
.	O

these	O
comparisons	O
,	O
we	O
mix	O
all	O
training	O
instances	O
,	O
order	O
them	O
by	O
the	O
size	O
,	O
and	O
put	O
them	O
into	O
10	O
equal	O
groups	O
,	O
the	O
medium	O
sentence	O
lengths	O
of	O
which	O
are	O
shown	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
the	O
speed	Metric
advantage	Metric
of	O
S	Method
-	Method
LSTM	Method
is	O
larger	O
when	O
the	O
size	O
of	O
the	O
input	O
text	O
increases	O
,	O
thanks	O
to	O
a	O
fixed	O
number	O
of	O
recurrent	O
steps	O
.	O

Similar	O
to	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
there	O
is	O
a	O
relative	O
disadvantage	O
of	O
S	Method
-	Method
LSTM	Method
in	O
comparison	O
with	O
BiLSTM	Method
,	O
which	O
is	O
that	O
the	O
memory	Metric
consumption	Metric
is	O
relatively	O
larger	O
.	O

For	O
example	O
,	O
over	O
the	O
movie	Material
review	Material
development	O
set	O
,	O
the	O
actual	O
GPU	Metric
memory	Metric
consumption	Metric
by	O
S	Method
-	Method
LSTM	Method
,	O
BiLSTM	Method
,	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
and	O
4	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
are	O
252	O
M	O
,	O
89	O
M	O
,	O
146	O
M	O
and	O
253	O
M	O
,	O
respectively	O
.	O

This	O
is	O
due	O
to	O
the	O
fact	O
that	O
computation	O
is	O
performed	O
in	O
parallel	O
by	O
S	Method
-	Method
LSTM	Method
and	O
hierarchical	Method
attention	Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
investigated	O
S	Method
-	Method
LSTM	Method
,	O
a	O
recurrent	Method
neural	Method
network	Method
for	O
encoding	Task
sentences	Task
,	O
which	O
offers	O
richer	O
contextual	O
information	O
exchange	O
with	O
more	O
parallelism	O
compared	O
to	O
BiLSTMs	Method
.	O

Results	O
on	O
a	O
range	O
of	O
classification	Task
and	Task
sequence	Task
labelling	Task
tasks	Task
show	O
that	O
S	Method
-	Method
LSTM	Method
outperforms	O
BiLSTMs	Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
demonstrating	O
that	O
S	Method
-	Method
LSTM	Method
can	O
be	O
a	O
useful	O
addition	O
to	O
the	O
neural	Method
toolbox	Method
for	O
encoding	Task
sentences	Task
.	O

The	O
structural	O
nature	O
in	O
S	O
-	O
LSTM	Method
states	O
allows	O
straightforward	O
extension	O
to	O
tree	O
structures	O
,	O
resulting	O
in	O
highly	O
parallelisable	Method
tree	Method
LSTMs	Method
.	O

We	O
leave	O
such	O
investigation	O
to	O
future	O
work	O
.	O

Next	O
directions	O
also	O
include	O
the	O
investigation	O
of	O
S	Method
-	Method
LSTM	Method
to	O
more	O
NLP	Task
tasks	Task
,	O
such	O
as	O
machine	Task
translation	Task
.	O

section	O
:	O
section	O
:	O
Acknowledge	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
and	O
thoughtful	O
comments	O
.	O

section	O
:	O
document	O
:	O
The	O
Variational	Method
Fair	Method
Autoencoder	Method
We	O
investigate	O
the	O
problem	O
of	O
learning	Task
representations	Task
that	O
are	O
invariant	O
to	O
certain	O
nuisance	O
or	O
sensitive	O
factors	O
of	O
variation	O
in	O
the	O
data	O
while	O
retaining	O
as	O
much	O
of	O
the	O
remaining	O
information	O
as	O
possible	O
.	O

Our	O
model	O
is	O
based	O
on	O
a	O
variational	Method
autoencoding	Method
architecture	Method
kingma2013auto	O
,	O
rezende2014stochastic	O
with	O
priors	O
that	O
encourage	O
independence	O
between	O
sensitive	O
and	O
latent	O
factors	O
of	O
variation	O
.	O

Any	O
subsequent	O
processing	O
,	O
such	O
as	O
classification	Task
,	O
can	O
then	O
be	O
performed	O
on	O
this	O
purged	O
latent	Method
representation	Method
.	O

To	O
remove	O
any	O
remaining	O
dependencies	O
we	O
incorporate	O
an	O
additional	O
penalty	O
term	O
based	O
on	O
the	O
“	O
Maximum	Method
Mean	Method
Discrepancy	Method
”	Method
(	O
MMD	Method
)	O
gretton2006kernel	O
measure	O
.	O

We	O
discuss	O
how	O
these	O
architectures	O
can	O
be	O
efficiently	O
trained	O
on	O
data	O
and	O
show	O
in	O
experiments	O
that	O
this	O
method	O
is	O
more	O
effective	O
than	O
previous	O
work	O
in	O
removing	O
unwanted	O
sources	O
of	O
variation	O
while	O
maintaining	O
informative	Method
latent	Method
representations	Method
.	O

section	O
:	O
Introduction	O
In	O
“	O
Representation	Method
Learning	Method
”	O
one	O
tries	O
to	O
find	O
representations	O
of	O
the	O
data	O
that	O
are	O
informative	O
for	O
a	O
particular	O
task	O
while	O
removing	O
the	O
factors	O
of	O
variation	O
that	O
are	O
uninformative	O
and	O
are	O
typically	O
detrimental	O
for	O
the	O
task	O
under	O
consideration	O
.	O

Uninformative	O
dimensions	O
are	O
often	O
called	O
“	O
noise	O
”	O
or	O
“	O
nuisance	O
variables	O
”	O
while	O
informative	O
dimensions	O
are	O
usually	O
called	O
latent	O
or	O
hidden	O
factors	O
of	O
variation	O
.	O

Many	O
machine	Method
learning	Method
algorithms	Method
can	O
be	O
understood	O
in	O
this	O
way	O
:	O
principal	Method
component	Method
analysis	Method
,	O
nonlinear	Method
dimensional	Method
reduction	Method
and	O
latent	Method
Dirichlet	Method
allocation	Method
are	O
all	O
models	O
that	O
extract	O
informative	O
factors	O
(	O
dimensions	O
,	O
causes	O
,	O
topics	O
)	O
of	O
the	O
data	O
which	O
can	O
often	O
be	O
used	O
to	O
visualize	O
the	O
data	O
.	O

On	O
the	O
other	O
hand	O
,	O
linear	Method
discriminant	Method
analysis	Method
and	O
deep	Method
(	Method
convolutional	Method
)	Method
neural	Method
nets	Method
learn	O
representations	O
that	O
are	O
good	O
for	O
classification	Task
.	O

In	O
this	O
paper	O
we	O
consider	O
the	O
case	O
where	O
we	O
wish	O
to	O
learn	O
latent	Method
representations	Method
where	O
(	O
almost	O
)	O
all	O
of	O
the	O
information	O
about	O
certain	O
known	O
factors	O
of	O
variation	O
are	O
purged	O
from	O
the	O
representation	O
while	O
still	O
retaining	O
as	O
much	O
information	O
about	O
the	O
data	O
as	O
possible	O
.	O

In	O
other	O
words	O
,	O
we	O
want	O
a	O
latent	Method
representation	Method
that	O
is	O
maximally	O
informative	O
about	O
an	O
observed	O
random	O
variable	O
(	O
e.g.	O
,	O
class	O
label	O
)	O
while	O
minimally	O
informative	O
about	O
a	O
sensitive	O
or	O
nuisance	O
variable	O
.	O

By	O
treating	O
as	O
a	O
sensitive	O
variable	O
,	O
i.e.	O
is	O
correlated	O
with	O
our	O
objective	O
,	O
we	O
are	O
dealing	O
with	O
“	O
fair	Method
representations	Method
”	O
,	O
a	O
problem	O
previously	O
considered	O
by	O
.	O

If	O
we	O
instead	O
treat	O
as	O
a	O
nuisance	O
variable	O
we	O
are	O
dealing	O
with	O
“	O
domain	Task
adaptation	Task
”	O
,	O
in	O
other	O
words	O
by	O
removing	O
the	O
domain	O
from	O
our	O
representations	O
we	O
will	O
obtain	O
improved	O
performance	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
a	O
novel	O
model	O
based	O
on	O
deep	O
variational	Method
autoencoders	Method
(	O
VAE	Method
)	O
kingma2013auto	O
,	O
rezende2014stochastic	O
.	O

These	O
models	O
can	O
naturally	O
encourage	O
separation	O
between	O
latent	O
variables	O
and	O
sensitive	O
variables	O
by	O
using	O
factorized	O
priors	O
.	O

However	O
,	O
some	O
dependencies	O
may	O
still	O
remain	O
when	O
mapping	O
data	O
-	O
cases	O
to	O
their	O
hidden	Method
representation	Method
using	O
the	O
variational	O
posterior	O
,	O
which	O
we	O
stamp	O
out	O
using	O
a	O
“	O
Maximum	Method
Mean	Method
Discrepancy	Method
”	O
gretton2006kernel	O
term	O
that	O
penalizes	O
differences	O
between	O
all	O
order	O
moments	O
of	O
the	O
marginal	O
posterior	O
distributions	O
and	O
(	O
for	O
a	O
discrete	O
RV	O
)	O
.	O

In	O
experiments	O
we	O
show	O
that	O
this	O
combined	O
approach	O
is	O
highly	O
successful	O
in	O
learning	O
representations	Task
that	O
are	O
devoid	O
of	O
unwanted	O
information	O
while	O
retaining	O
as	O
much	O
information	O
as	O
possible	O
from	O
what	O
remains	O
.	O

section	O
:	O
Learning	Task
Invariant	Task
Representations	Task
[	O
scale=0.75	O
]	O
unsupervised.pdf	O
[	O
scale=0.75	O
]	O
semisupervised.pdf	O
subsection	O
:	O
Unsupervised	Method
model	Method
Factoring	O
out	O
undesired	O
variations	O
from	O
the	O
data	O
can	O
be	O
easily	O
formulated	O
as	O
a	O
general	O
probabilistic	Method
model	Method
which	O
admits	O
two	O
distinct	O
(	O
independent	O
)	O
“	O
sources	O
”	O
;	O
an	O
observed	O
variable	O
,	O
which	O
denotes	O
the	O
variations	O
that	O
we	O
want	O
to	O
remove	O
,	O
and	O
a	O
continuous	O
latent	O
variable	O
which	O
models	O
all	O
the	O
remaining	O
information	O
.	O

This	O
generative	Method
process	Method
can	O
be	O
formally	O
defined	O
as	O
:	O
where	O
is	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

With	O
this	O
formulation	O
we	O
explicitly	O
encode	O
a	O
notion	O
of	O
‘	O
invariance	O
’	O
in	O
our	O
model	O
,	O
since	O
the	O
latent	Method
representation	Method
is	O
marginally	O
independent	O
of	O
the	O
factors	O
of	O
variation	O
.	O

Therefore	O
the	O
problem	O
of	O
finding	O
an	O
invariant	Method
representation	Method
for	O
a	O
data	O
point	O
and	O
variation	O
can	O
be	O
cast	O
as	O
performing	O
inference	Task
on	O
this	O
graphical	Method
model	Method
and	O
obtaining	O
the	O
posterior	O
distribution	O
of	O
,	O
.	O

For	O
our	O
model	O
we	O
will	O
employ	O
a	O
variational	Method
autoencoder	Method
architecture	Method
kingma2013auto	O
,	O
rezende2014stochastic	O
;	O
namely	O
we	O
will	O
parametrize	O
the	O
generative	Method
model	Method
(	O
decoder	Method
)	O
and	O
the	O
variational	Method
posterior	Method
(	O
encoder	Method
)	O
as	O
(	O
deep	O
)	O
neural	Method
networks	Method
which	O
accept	O
as	O
inputs	O
and	O
respectively	O
and	O
produce	O
the	O
parameters	O
of	O
each	O
distribution	O
after	O
a	O
series	O
of	O
non	O
-	O
linear	O
transformations	O
.	O

Both	O
the	O
model	O
(	O
)	O
and	O
variational	Method
(	Method
)	O
parameters	O
will	O
be	O
jointly	O
optimized	O
with	O
the	O
SGVB	Method
kingma2013auto	O
algorithm	O
according	O
to	O
a	O
lower	O
bound	O
on	O
the	O
log	Metric
-	Metric
likelihood	Metric
.	O

This	O
parametrization	O
will	O
allow	O
us	O
to	O
capture	O
most	O
of	O
the	O
salient	O
information	O
of	O
in	O
our	O
embedding	O
.	O

Furthermore	O
the	O
distributed	Method
representation	Method
of	O
a	O
neural	Method
network	Method
would	O
allow	O
us	O
to	O
better	O
resolve	O
the	O
dependencies	O
between	O
and	O
thus	O
yielding	O
a	O
better	O
disentangling	O
between	O
the	O
independent	O
factors	O
and	O
.	O

By	O
choosing	O
a	O
Gaussian	Method
posterior	Method
and	O
standard	O
isotropic	Method
Gaussian	Method
prior	Method
we	O
can	O
obtain	O
the	O
following	O
lower	O
bound	O
:	O
with	O
and	O
with	O
being	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

subsection	O
:	O
Semi	Method
-	Method
Supervised	Method
model	Method
Factoring	O
out	O
variations	O
in	O
an	O
unsupervised	O
way	O
can	O
however	O
be	O
harmful	O
in	O
cases	O
where	O
we	O
want	O
to	O
use	O
this	O
invariant	Method
representation	Method
for	O
a	O
subsequent	O
prediction	Task
task	Task
.	O

In	O
particular	O
if	O
we	O
have	O
a	O
situation	O
where	O
the	O
nuisance	O
variable	O
and	O
the	O
actual	O
label	O
are	O
correlated	O
,	O
then	O
training	O
an	O
unsupervised	Method
model	Method
could	O
yield	O
random	O
or	O
degenerate	O
representations	O
with	O
respect	O
to	O
.	O

Therefore	O
it	O
is	O
more	O
appropriate	O
to	O
try	O
to	O
“	O
inject	O
”	O
the	O
information	O
about	O
the	O
label	O
during	O
the	O
feature	Method
extraction	Method
phase	Method
.	O

This	O
can	O
be	O
quite	O
simply	O
achieved	O
by	O
introducing	O
a	O
second	O
“	O
layer	O
”	O
of	O
latent	O
variables	O
to	O
our	O
generative	Method
model	Method
where	O
we	O
try	O
to	O
correlate	O
with	O
the	O
prediction	Task
task	Task
.	O

Assuming	O
that	O
the	O
invariant	O
features	O
are	O
now	O
called	O
we	O
enrich	O
the	O
generative	O
story	O
by	O
similarly	O
providing	O
two	O
distinct	O
(	O
independent	O
)	O
sources	O
for	O
;	O
a	O
discrete	O
(	O
in	O
case	O
of	O
classification	Task
)	Task
variable	Task
which	O
denotes	O
the	O
label	O
of	O
the	O
data	O
point	O
and	O
a	O
continuous	O
latent	O
variable	O
which	O
encodes	O
the	O
variation	O
on	O
that	O
is	O
not	O
explained	O
by	O
(	O
dependent	O
noise	O
)	O
.	O

The	O
process	O
now	O
can	O
be	O
formally	O
defined	O
as	O
:	O
Similarly	O
to	O
the	O
unsupervised	Task
case	Task
we	O
use	O
a	O
variational	Method
auto	Method
-	Method
encoder	Method
and	O
jointly	O
optimize	O
the	O
variational	O
and	O
model	O
parameters	O
.	O

The	O
lower	O
bound	O
now	O
becomes	O
:	O
where	O
we	O
assume	O
that	O
the	O
posterior	O
is	O
factorized	O
as	O
,	O
and	O
where	O
:	O
with	O
again	O
being	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

The	O
model	O
proposed	O
here	O
can	O
be	O
seen	O
as	O
an	O
extension	O
to	O
the	O
‘	O
stacked	Method
M1	Method
+	Method
M2	Method
’	Method
model	Method
originally	O
proposed	O
from	O
,	O
where	O
we	O
have	O
additionally	O
introduced	O
the	O
nuisance	O
variable	O
during	O
the	O
feature	Method
extraction	Method
.	O

Thus	O
following	O
we	O
can	O
also	O
handle	O
the	O
‘	O
semi	Task
-	Task
supervised	Task
’	Task
case	Task
,	O
i.e.	O
,	O
missing	O
labels	O
.	O

In	O
situations	O
where	O
the	O
label	O
is	O
observed	O
the	O
lower	O
bound	O
takes	O
the	O
following	O
form	O
(	O
exploiting	O
the	O
fact	O
that	O
we	O
can	O
compute	O
some	O
Kullback	O
-	O
Leibler	O
divergences	O
explicitly	O
in	O
our	O
case	O
)	O
:	O
and	O
in	O
the	O
case	O
that	O
it	O
is	O
not	O
observed	O
we	O
use	O
to	O
‘	O
impute	O
’	O
our	O
data	O
:	O
therefore	O
the	O
final	O
objective	Metric
function	Metric
is	O
:	O
where	O
the	O
last	O
term	O
is	O
introduced	O
so	O
as	O
to	O
ensure	O
that	O
the	O
predictive	O
posterior	O
learns	O
from	O
both	O
labeled	O
and	O
unlabeled	O
data	O
.	O

This	O
semi	Method
-	Method
supervised	Method
model	Method
will	O
be	O
called	O
“	O
VAE	Method
”	O
in	O
our	O
experiments	O
.	O

However	O
,	O
there	O
is	O
a	O
subtle	O
difference	O
between	O
the	O
approach	O
of	O
and	O
our	O
model	O
.	O

Instead	O
of	O
training	O
separately	O
each	O
layer	O
of	O
stochastic	O
variables	O
we	O
optimize	O
the	O
model	O
jointly	O
.	O

The	O
potential	O
advantages	O
of	O
this	O
approach	O
are	O
two	O
fold	O
:	O
as	O
we	O
previously	O
mentioned	O
if	O
the	O
label	O
and	O
the	O
nuisance	O
information	O
are	O
correlated	O
then	O
training	O
a	O
(	O
conditional	Method
)	Method
feature	Method
extractor	Method
separately	O
poses	O
the	O
danger	O
of	O
creating	O
a	O
degenerate	Method
representation	Method
with	O
respect	O
to	O
the	O
label	O
.	O

Furthermore	O
the	O
label	O
information	O
will	O
also	O
better	O
guide	O
the	O
feature	Method
extraction	Method
towards	O
the	O
more	O
salient	O
parts	O
of	O
the	O
data	O
,	O
thus	O
maintaining	O
most	O
of	O
the	O
(	O
predictive	O
)	O
information	O
.	O

subsection	O
:	O
Further	O
invariance	O
via	O
Maximum	Method
Mean	Method
Discrepancy	Method
Despite	O
the	O
fact	O
that	O
we	O
have	O
a	O
model	O
that	O
encourages	O
statistical	O
independence	O
between	O
and	O
a	O
-	O
priori	O
we	O
might	O
still	O
have	O
some	O
dependence	O
in	O
the	O
(	O
approximate	O
)	O
marginal	O
posterior	O
.	O

In	O
particular	O
,	O
this	O
can	O
happen	O
if	O
the	O
label	O
is	O
correlated	O
with	O
the	O
sensitive	O
variable	O
,	O
which	O
can	O
allow	O
information	O
about	O
to	O
“	O
leak	O
”	O
into	O
the	O
posterior	O
.	O

Thus	O
instead	O
we	O
could	O
maximize	O
a	O
“	O
penalized	Metric
”	Metric
lower	Metric
bound	Metric
where	O
we	O
impose	O
some	O
sort	O
of	O
regularization	O
on	O
the	O
marginal	O
.	O

In	O
the	O
following	O
we	O
will	O
describe	O
one	O
way	O
to	O
achieve	O
this	O
regularization	O
through	O
the	O
Maximum	Method
Mean	Method
Discrepancy	Method
(	O
MMD	Method
)	O
gretton2006kernel	O
measure	O
.	O

subsubsection	Method
:	O
Maximum	Method
Mean	Method
Discrepancy	Method
Consider	O
the	O
problem	O
of	O
determining	O
whether	O
two	O
datasets	O
and	O
are	O
drawn	O
from	O
the	O
same	O
distribution	O
,	O
i.e.	O
,	O
.	O

A	O
simple	O
test	O
is	O
to	O
consider	O
the	O
distance	O
between	O
empirical	O
statistics	O
of	O
the	O
two	O
datasets	O
:	O
Expanding	O
the	O
square	O
yields	O
an	O
estimator	O
composed	O
only	O
of	O
inner	O
products	O
on	O
which	O
the	O
kernel	Method
trick	Method
can	O
be	O
applied	O
.	O

The	O
resulting	O
estimator	O
is	O
known	O
as	O
Maximum	Method
Mean	Method
Discrepancy	Method
(	O
MMD	Method
)	O
gretton2006kernel	O
:	O
Asymptotically	O
,	O
for	O
a	O
universal	Method
kernel	Method
such	O
as	O
the	O
Gaussian	Method
kernel	Method
,	O
is	O
if	O
and	O
only	O
if	O
.	O

Equivalently	O
,	O
minimizing	O
MMD	Method
can	O
be	O
viewed	O
as	O
matching	O
all	O
of	O
the	O
moments	O
of	O
and	O
.	O

Therefore	O
,	O
we	O
can	O
use	O
it	O
as	O
an	O
extra	O
“	O
regularizer	O
”	O
and	O
force	O
the	O
model	O
to	O
try	O
to	O
match	O
the	O
moments	O
between	O
the	O
marginal	O
posterior	O
distributions	O
of	O
our	O
latent	O
variables	O
,	O
i.e.	O
,	O
and	O
(	O
in	O
the	O
case	O
of	O
binary	O
nuisance	O
information	O
)	O
.	O

By	O
adding	O
the	O
MMD	Method
penalty	O
into	O
the	O
lower	O
bound	O
of	O
our	O
aforementioned	O
VAE	Method
architecture	O
we	O
obtain	O
our	O
proposed	O
model	O
,	O
the	O
“	O
Variational	Method
Fair	Method
Autoencoder	Method
”	O
(	O
VFAE	Method
)	O
:	O
where	O
:	O
subsection	O
:	O
Fast	O
MMD	Method
via	O
Random	Method
Fourier	Method
Features	Method
A	O
naive	O
implementation	O
of	O
MMD	Method
in	O
minibatch	Method
stochastic	Method
gradient	Method
descent	Method
would	O
require	O
computing	O
the	O
Gram	O
matrix	O
for	O
each	O
minibatch	O
during	O
training	O
,	O
where	O
is	O
the	O
minibatch	O
size	O
.	O

Instead	O
,	O
we	O
can	O
use	O
random	Method
kitchen	Method
sinks	Method
rahimi2009weighted	O
to	O
compute	O
a	O
feature	Method
expansion	Method
such	O
that	O
computing	O
the	O
estimator	O
approximates	O
the	O
full	O
MMD	Method
(	O
[	O
reference	O
]	O
)	O
.	O

To	O
compute	O
this	O
,	O
we	O
draw	O
a	O
random	O
matrix	O
,	O
where	O
is	O
the	O
dimensionality	O
of	O
,	O
is	O
the	O
number	O
of	O
random	O
features	O
and	O
each	O
entry	O
of	O
is	O
drawn	O
from	O
a	O
standard	O
isotropic	Method
Gaussian	Method
.	O

The	O
feature	Method
expansion	Method
is	O
then	O
given	O
as	O
:	O
where	O
is	O
a	O
-	O
dimensional	O
uniform	O
random	O
vector	O
with	O
entries	O
in	O
.	O

have	O
successfully	O
applied	O
the	O
idea	O
of	O
using	O
random	Method
kitchen	Method
sinks	Method
to	O
approximate	O
MMD	Method
.	O

This	O
estimator	O
is	O
fairly	O
accurate	O
,	O
and	O
is	O
typically	O
much	O
faster	O
than	O
the	O
full	O
MMD	Method
penalty	O
.	O

We	O
use	O
in	O
our	O
experiments	O
.	O

section	O
:	O
Experiments	O
We	O
performed	O
experiments	O
on	O
the	O
three	O
datasets	O
that	O
correspond	O
to	O
a	O
“	O
fair	O
”	O
classification	Task
scenario	Task
and	O
were	O
previously	O
used	O
by	O
.	O

In	O
these	O
datasets	O
the	O
“	O
nuisance	O
”	O
or	O
sensitive	O
variable	O
is	O
significantly	O
correlated	O
with	O
the	O
label	O
thus	O
making	O
the	O
proper	O
removal	Task
of	Task
challenging	Task
.	O

Furthermore	O
,	O
we	O
also	O
experimented	O
with	O
the	O
Amazon	Material
reviews	Material
dataset	Material
to	O
make	O
a	O
connection	O
with	O
the	O
“	O
domain	Material
-	Material
adaptation	Material
”	Material
literature	Material
.	O

Finally	O
,	O
we	O
also	O
experimented	O
with	O
a	O
more	O
general	O
task	O
on	O
the	O
extended	Material
Yale	Material
B	Material
dataset	Material
;	O
that	O
of	O
learning	Task
invariant	Task
representations	Task
.	O

subsection	O
:	O
Datasets	O
For	O
the	O
fairness	Task
task	Task
we	O
experimented	O
with	O
three	O
datasets	O
that	O
were	O
previously	O
used	O
by	O
zemel2013learning	O
.	O

The	O
German	Material
dataset	Material
is	O
the	O
smallest	O
one	O
with	O
data	O
points	O
and	O
the	O
objective	O
is	O
to	O
predict	O
whether	O
a	O
person	O
has	O
a	O
good	O
or	O
bad	O
credit	O
rating	O
.	O

The	O
sensitive	O
variable	O
is	O
the	O
gender	O
of	O
the	O
individual	O
.	O

The	O
Adult	Material
income	Material
dataset	Material
contains	O
entries	O
and	O
describes	O
whether	O
an	O
account	O
holder	O
has	O
over	O
dollars	O
in	O
their	O
account	O
.	O

The	O
sensitive	O
variable	O
is	O
age	O
.	O

Both	O
of	O
these	O
are	O
obtained	O
from	O
the	O
UCI	Material
machine	Material
learning	Material
repository	Material
UCI	Material
.	O

The	O
health	Material
dataset	Material
is	O
derived	O
from	O
the	O
Heritage	O
Health	Material
Prize	O
.	O

It	O
is	O
the	O
largest	O
of	O
the	O
three	O
datasets	O
with	O
entries	O
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
a	O
patient	O
will	O
spend	O
any	O
days	O
in	O
the	O
hospital	O
in	O
the	O
next	O
year	O
and	O
the	O
sensitive	O
variable	O
is	O
the	O
age	O
of	O
the	O
individual	O
.	O

We	O
use	O
the	O
same	O
train	O
/	O
test	O
/	O
validation	O
splits	O
as	O
zemel2013learning	O
for	O
our	O
experiments	O
.	O

Finally	O
we	O
also	O
binarized	O
the	O
data	O
and	O
used	O
a	O
multivariate	Method
Bernoulli	Method
distribution	Method
for	O
,	O
where	O
is	O
the	O
sigmoid	O
function	O
.	O

For	O
the	O
domain	Task
adaptation	Task
task	Task
we	O
used	O
the	O
Amazon	Material
reviews	Material
dataset	Material
(	O
with	O
similar	O
preprocessing	O
)	O
that	O
was	O
also	O
employed	O
by	O
chen2012marginalized	O
and	O
2015arXiv150507818G.	O
It	O
is	O
composed	O
from	O
text	Material
reviews	Material
about	O
particular	O
products	O
,	O
where	O
each	O
product	O
belongs	O
to	O
one	O
out	O
of	O
four	O
different	O
domains	O
:	O
“	O
books	O
”	O
,	O
“	O
dvd	O
”	O
,	O
“	O
electronics	O
”	O
and	O
“	O
kitchen	O
”	O
.	O

As	O
a	O
result	O
we	O
performed	O
twelve	O
domain	Task
adaptation	Task
tasks	Task
.	O

The	O
labels	O
correspond	O
to	O
the	O
sentiment	O
of	O
each	O
review	O
,	O
i.e.	O
either	O
positive	O
or	O
negative	O
.	O

Since	O
each	O
feature	O
vector	O
is	O
composed	O
from	O
counts	O
of	O
unigrams	O
and	O
bigrams	Method
we	O
used	O
a	O
Poisson	Method
distribution	Method
for	O
.	O

It	O
is	O
also	O
worthwhile	O
to	O
mention	O
that	O
we	O
can	O
fully	O
exploit	O
the	O
semi	O
-	O
supervised	O
nature	O
of	O
our	O
model	O
in	O
this	O
dataset	O
,	O
and	O
thus	O
for	O
training	O
we	O
only	O
use	O
the	O
source	O
domain	O
labels	O
and	O
consider	O
the	O
labels	O
of	O
the	O
target	O
domain	O
as	O
“	O
missing	O
”	O
.	O

For	O
the	O
general	O
task	O
of	O
learning	Task
invariant	Task
representations	Task
we	O
used	O
the	O
Extended	Material
Yale	Material
B	Material
dataset	Material
,	O
which	O
was	O
also	O
employed	O
in	O
a	O
similar	O
fashion	O
by	O
li2014learning	O
.	O

It	O
is	O
composed	O
from	O
face	O
images	O
of	O
38	O
people	O
under	O
different	O
lighting	O
conditions	O
(	O
directions	O
of	O
the	O
light	O
source	O
)	O
.	O

Similarly	O
to	O
li2014learning	O
,	O
we	O
created	O
5	O
states	O
for	O
the	O
nuisance	O
variable	O
:	O
light	O
source	O
in	O
upper	O
right	O
,	O
lower	O
right	O
,	O
lower	O
left	O
,	O
upper	O
left	O
and	O
the	O
front	O
.	O

The	O
labels	O
correspond	O
to	O
the	O
identity	O
of	O
the	O
person	O
.	O

Following	O
li2014learning	O
,	O
we	O
used	O
the	O
same	O
training	O
,	O
test	O
set	O
and	O
no	O
validation	O
set	O
.	O

For	O
the	O
distribution	O
we	O
used	O
a	O
Gaussian	Method
with	O
means	O
constrained	O
in	O
the	O
0	O
-	O
1	O
range	O
(	O
since	O
we	O
have	O
intensity	O
images	O
)	O
by	O
a	O
sigmoid	Method
,	O
i.e.	O
.	O

subsection	O
:	O
Experimental	O
Setup	O
For	O
the	O
Adult	Material
dataset	Material
both	O
encoders	Method
,	O
for	O
and	O
,	O
and	O
both	O
decoders	Method
,	O
for	O
and	O
,	O
had	O
one	O
hidden	Method
layer	Method
of	O
100	O
units	O
.	O

For	O
the	O
Health	Material
dataset	Material
we	O
had	O
one	O
hidden	O
layer	O
of	O
300	O
units	O
for	O
the	O
encoder	Method
and	Method
decoder	Method
and	O
one	O
hidden	Method
layer	Method
of	O
150	O
units	O
for	O
the	O
encoder	Method
and	Method
decoder	Method
.	O

For	O
the	O
much	O
smaller	O
German	Material
dataset	Material
we	O
used	O
60	O
hidden	O
units	O
for	O
both	O
encoders	Method
and	O
decoders	Method
.	O

Finally	O
,	O
for	O
the	O
Amazon	Material
reviews	Material
and	O
Extended	Material
Yale	Material
B	Material
datasets	Material
we	O
had	O
one	O
hidden	Method
layer	Method
with	O
500	O
,	O
400	O
units	O
respectively	O
for	O
the	O
encoder	Method
,	O
decoder	Method
,	O
and	O
300	O
,	O
100	O
units	O
respectively	O
for	O
the	O
encoder	Method
and	Method
decoder	Method
.	O

On	O
all	O
of	O
the	O
datasets	O
we	O
used	O
50	O
latent	O
dimensions	O
for	O
and	O
,	O
except	O
for	O
the	O
small	Material
German	Material
dataset	Material
,	O
where	O
we	O
used	O
30	O
latent	O
dimensions	O
for	O
both	O
variables	O
.	O

For	O
the	O
predictive	Task
posterior	Task
we	O
used	O
a	O
simple	O
Logistic	Method
regression	Method
classifier	Method
.	O

Optimization	Task
of	O
the	O
objective	Metric
function	Metric
was	O
done	O
with	O
Adam	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
KingmaB14	O
using	O
the	O
default	O
values	O
for	O
the	O
hyperparameters	O
,	O
minibatches	O
of	O
100	O
data	O
points	O
and	O
temporal	Method
averaging	Method
.	O

The	O
MMD	Method
penalty	O
was	O
simply	O
multiplied	O
by	O
the	O
minibatch	O
size	O
so	O
as	O
to	O
keep	O
the	O
scale	O
of	O
the	O
penalty	O
similar	O
to	O
the	O
lower	O
bound	O
.	O

Furthermore	O
,	O
the	O
extra	O
strength	O
of	O
the	O
MMD	Method
,	O
,	O
was	O
tuned	O
according	O
to	O
a	O
validation	O
set	O
.	O

The	O
scaling	O
of	O
the	O
supervised	Metric
cost	Metric
was	O
low	O
(	O
)	O
for	O
the	O
Adult	Material
,	O
Health	Material
and	O
German	Material
datasets	Material
due	O
to	O
the	O
correlation	O
of	O
with	O
.	O

On	O
the	O
Amazon	Material
reviews	Material
and	O
Extended	Material
Yale	Material
B	Material
datasets	Material
however	O
the	O
scaling	O
of	O
the	O
supervised	Metric
cost	Metric
was	O
higher	O
:	O
for	O
the	O
Amazon	Material
reviews	Material
dataset	Material
(	O
empirically	O
determined	O
after	O
observing	O
the	O
classification	Metric
loss	Metric
on	O
the	O
first	O
few	O
iterations	O
on	O
the	O
first	O
source	O
-	O
target	O
pair	O
)	O
and	O
for	O
the	O
Extended	Material
Yale	Material
B	Material
dataset	Material
.	O

Similarly	O
,	O
the	O
scaling	O
of	O
the	O
MMD	Method
penalty	O
was	O
for	O
the	O
Amazon	Material
reviews	Material
dataset	Material
and	O
for	O
the	O
Extended	Material
Yale	Material
B.	Material
Our	O
evaluation	O
is	O
geared	O
towards	O
two	O
fronts	O
;	O
removing	O
information	O
about	O
and	O
classification	Metric
accuracy	Metric
for	O
.	O

To	O
measure	O
the	O
information	O
about	O
in	O
our	O
new	O
representation	O
we	O
simply	O
train	O
a	O
classifier	Method
to	O
predict	O
from	O
.	O

We	O
utilize	O
both	O
Logistic	Method
Regression	Method
(	O
LR	Method
)	O
which	O
is	O
a	O
simple	O
linear	Method
classifier	Method
,	O
and	O
Random	Method
Forest	Method
(	O
RF	Method
)	O
which	O
is	O
a	O
powerful	O
non	Method
-	Method
linear	Method
classifier	Method
.	O

Since	O
on	O
the	O
datasets	O
that	O
we	O
experimented	O
with	O
the	O
nuisance	O
variable	O
is	O
binary	O
we	O
can	O
easily	O
find	O
the	O
random	Metric
chance	Metric
accuracy	Metric
for	O
and	O
measure	O
the	O
discriminatory	Metric
information	Metric
of	Metric
in	Metric
.	O

Furthermore	O
,	O
we	O
also	O
used	O
the	O
discrimination	Method
metric	Method
from	O
as	O
well	O
a	O
more	O
“	O
informed	O
”	O
version	O
of	O
the	O
discrimination	Method
metric	Method
that	O
instead	O
of	O
the	O
predictions	O
,	O
takes	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
correct	O
class	O
.	O

They	O
are	O
provided	O
in	O
the	O
appendix	O
A.	O
Finally	O
,	O
for	O
the	O
classification	Task
performance	O
on	O
we	O
used	O
the	O
predictive	O
posterior	O
for	O
the	O
VAE	Method
/	O
VFAE	Method
and	O
a	O
simple	O
Logistic	Method
Regression	Method
for	O
the	O
original	O
representations	O
.	O

It	O
should	O
be	O
noted	O
that	O
for	O
the	O
VFAE	Method
and	O
VAE	Method
models	O
we	O
use	O
a	O
sample	O
from	O
to	O
make	O
predictions	O
,	O
instead	O
of	O
using	O
the	O
mean	O
.	O

We	O
found	O
that	O
the	O
extra	O
noise	O
helps	O
with	O
invariance	O
.	O

We	O
implemented	O
the	O
Learning	Method
Fair	Method
Representations	Method
zemel2013learning	O
method	O
(	O
LFR	Method
)	O
as	O
a	O
baseline	O
using	O
dimensions	O
for	O
the	O
latent	O
space	O
.	O

To	O
measure	O
the	O
accuracy	Metric
on	O
in	O
the	O
results	O
below	O
we	O
similarly	O
used	O
the	O
LFR	Method
model	Method
predictions	Method
.	O

subsection	O
:	O
Results	O
subsubsection	O
:	O
Fair	Task
classification	Task
The	O
results	O
for	O
all	O
three	O
datasets	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Since	O
we	O
are	O
dealing	O
with	O
the	O
“	O
fair	Task
”	Task
classification	Task
scenario	Task
here	O
,	O
low	Metric
accuracy	Metric
and	O
discrimination	Metric
against	Metric
is	O
more	O
important	O
than	O
the	O
accuracy	Metric
on	O
(	O
as	O
long	O
as	O
we	O
do	O
not	O
produce	O
degenerate	O
representations	O
)	O
.	O

.329	O
[	O
width=1.1	O
]	O
adult_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
adult_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
adult_y.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_y.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_y.pdf	O
On	O
the	O
Adult	Material
dataset	Material
,	O
the	O
highest	O
accuracy	Metric
on	O
the	O
label	O
and	O
the	O
lowest	O
discrimination	Metric
against	O
is	O
obtained	O
by	O
our	O
LFR	Method
baseline	Method
.	O

Despite	O
the	O
fact	O
that	O
LFR	Method
appears	O
to	O
give	O
the	O
best	O
tradeoff	O
between	O
accuracy	Metric
and	O
discrimination	Task
,	O
it	O
appears	O
to	O
retain	O
information	O
about	O
in	O
its	O
representation	O
,	O
which	O
is	O
discovered	O
from	O
the	O
random	Method
forest	Method
classifier	Method
.	O

In	O
that	O
sense	O
,	O
the	O
VFAE	Method
method	O
appears	O
to	O
do	O
the	O
best	O
job	O
in	O
actually	O
removing	O
the	O
sensitive	O
information	O
and	O
maintaining	O
most	O
of	O
the	O
predictive	O
information	O
.	O

Furthermore	O
,	O
the	O
introduction	O
of	O
the	O
MMD	Method
penalty	O
in	O
the	O
VFAE	Method
model	O
seems	O
to	O
provide	O
a	O
significant	O
benefit	O
with	O
respect	O
to	O
our	O
discrimination	Metric
metrics	Metric
,	O
as	O
both	O
were	O
reduced	O
considerably	O
compared	O
to	O
the	O
regular	O
VAE	Method
.	O

On	O
the	O
German	Material
dataset	Material
,	O
all	O
methods	O
appear	O
to	O
be	O
invariant	O
with	O
respect	O
to	O
the	O
sensitive	O
information	O
.	O

However	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
discrimination	Metric
metric	Metric
,	O
since	O
LFR	Method
does	O
appear	O
to	O
retain	O
information	O
compared	O
to	O
the	O
VAE	Method
and	O
VFAE	Method
.	O

The	O
MMD	Method
penalty	O
in	O
VFAE	Method
did	O
seem	O
improve	O
the	O
discrimination	Metric
scores	Metric
over	O
the	O
original	O
VAE	Method
,	O
while	O
the	O
accuracy	Metric
on	O
the	O
labels	O
remained	O
similar	O
.	O

As	O
for	O
the	O
Health	Material
dataset	Material
;	O
this	O
dataset	O
is	O
extremely	O
imbalanced	O
,	O
with	O
only	O
15	O
%	O
of	O
the	O
patients	O
being	O
admitted	O
to	O
a	O
hospital	O
.	O

Therefore	O
,	O
each	O
of	O
the	O
classifiers	Method
seems	O
to	O
predict	O
the	O
majority	O
class	O
as	O
the	O
label	O
for	O
every	O
point	O
.	O

For	O
the	O
invariance	O
against	O
however	O
,	O
the	O
results	O
were	O
more	O
interesting	O
.	O

On	O
the	O
one	O
hand	O
,	O
the	O
VAE	Method
model	O
on	O
this	O
dataset	O
did	O
maintain	O
some	O
sensitive	O
information	O
,	O
which	O
could	O
be	O
identified	O
both	O
linearly	O
and	O
non	O
-	O
linearly	O
.	O

On	O
the	O
other	O
hand	O
,	O
VFAE	Method
and	O
the	O
LFR	Method
methods	Method
were	O
able	O
to	O
retain	O
less	O
information	O
in	O
their	O
latent	Method
representation	Method
,	O
since	O
only	O
Random	Method
Forest	Method
was	O
able	O
to	O
achieve	O
higher	O
than	O
random	Metric
chance	Metric
accuracy	Metric
.	O

This	O
further	O
justifies	O
our	O
choice	O
for	O
including	O
the	O
MMD	Method
penalty	O
in	O
the	O
lower	Metric
bound	Metric
of	O
the	O
VAE	Method
.	O

.	O

In	O
order	O
to	O
further	O
assess	O
the	O
nature	O
of	O
our	O
new	O
representations	O
,	O
we	O
visualized	O
two	O
dimensional	O
Barnes	O
-	O
Hut	O
SNE	O
2013arXiv1301.3342V	O
embeddings	O
of	O
the	O
representations	O
,	O
obtained	O
from	O
the	O
model	O
trained	O
on	O
the	O
Adult	Material
dataset	Material
,	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

As	O
we	O
can	O
see	O
,	O
the	O
nuisance	O
/	O
sensitive	O
variables	O
can	O
be	O
identified	O
both	O
on	O
the	O
original	O
representation	O
and	O
on	O
a	O
latent	Method
representation	Method
that	O
does	O
not	O
have	O
the	O
MMD	Method
penalty	O
and	O
the	O
independence	O
properties	O
between	O
and	O
in	O
the	O
prior	O
.	O

By	O
introducing	O
these	O
independence	O
properties	O
as	O
well	O
as	O
the	O
MMD	Method
penalty	O
the	O
nuisance	O
variable	O
groups	O
become	O
practically	O
indistinguishable	O
.	O

.25	O
[	O
width=1.	O
]	O
tsne_adult_x.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult_s.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult_mmd_s.pdf	O
subsubsection	Method
:	O
Domain	Method
adaptation	Method
As	O
for	O
the	O
domain	Task
adaptation	Task
scenario	Task
and	O
the	O
Amazon	Material
reviews	Material
dataset	Material
,	O
the	O
results	O
of	O
our	O
VFAE	Method
model	O
can	O
be	O
seen	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
model	O
was	O
successful	O
in	O
factoring	O
out	O
the	O
domain	O
information	O
,	O
since	O
the	O
accuracy	Metric
,	O
measured	O
both	O
linearly	O
(	O
LR	Method
)	O
and	O
non	O
-	O
linearly	O
(	O
RF	Method
)	O
,	O
was	O
towards	O
random	O
chance	O
(	O
which	O
for	O
this	O
dataset	O
is	O
0.5	O
)	O
.	O

We	O
should	O
also	O
mention	O
that	O
,	O
on	O
this	O
dataset	O
at	O
least	O
,	O
completely	O
removing	O
information	O
about	O
the	O
domain	O
does	O
not	O
guarantee	O
a	O
better	O
performance	O
on	O
.	O

The	O
same	O
effect	O
was	O
also	O
observed	O
by	O
and	O
.	O

As	O
far	O
as	O
the	O
accuracy	Metric
on	O
is	O
concerned	O
,	O
we	O
compared	O
against	O
a	O
recent	O
neural	Method
network	Method
based	O
state	O
of	O
the	O
art	O
method	O
for	O
domain	Task
adaptation	Task
,	O
Domain	Method
Adversarial	Method
Neural	Method
Network	Method
(	O
DANN	Method
)	O
2015arXiv150507818G.	O
As	O
we	O
can	O
observe	O
in	O
table	O
[	O
reference	O
]	O
,	O
our	O
accuracy	Metric
on	O
the	O
labels	O
is	O
higher	O
on	O
9	O
out	O
of	O
the	O
12	O
domain	Task
adaptation	Task
tasks	Task
whereas	O
on	O
the	O
remaining	O
3	O
it	O
is	O
quite	O
similar	O
to	O
the	O
DANN	Method
architecture	Method
.	O

subsection	O
:	O
Learning	Task
Invariant	Task
Representations	Task
Regarding	O
the	O
more	O
general	O
task	O
of	O
learning	Task
invariant	Task
representations	Task
;	O
our	O
results	O
on	O
the	O
Extended	Material
Yale	Material
B	Material
dataset	Material
also	O
demonstrate	O
our	O
model	O
’s	O
ability	O
to	O
learn	O
such	O
representations	O
.	O

As	O
expected	O
,	O
on	O
the	O
original	O
representation	O
the	O
lighting	O
conditions	O
,	O
,	O
are	O
well	O
identifiable	O
with	O
almost	O
perfect	O
accuracy	Metric
from	O
both	O
RF	Method
and	O
LR	Method
.	O

This	O
can	O
also	O
be	O
seen	O
in	O
the	O
two	O
dimensional	O
embeddings	O
of	O
the	O
original	O
space	O
in	O
Figure	O
[	O
reference	O
]	O
:	O
the	O
images	O
are	O
mostly	O
clustered	O
according	O
to	O
the	O
lighting	O
conditions	O
.	O

As	O
soon	O
as	O
we	O
utilize	O
our	O
VFAE	Method
model	O
we	O
simultaneously	O
decrease	O
the	O
accuracy	Metric
on	O
,	O
from	O
96	O
%	O
to	O
about	O
50	O
%	O
,	O
and	O
increase	O
our	O
accuracy	Metric
on	O
,	O
from	O
78	O
%	O
to	O
about	O
85	O
%	O
.	O

This	O
effect	O
can	O
also	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
:	O
the	O
images	O
are	O
now	O
mostly	O
clustered	O
according	O
to	O
the	O
person	O
ID	O
(	O
the	O
label	O
)	O
.	O

It	O
is	O
clear	O
that	O
in	O
this	O
scenario	O
the	O
information	O
about	O
is	O
purely	O
“	O
nuisance	O
”	O
with	O
respect	O
to	O
the	O
labels	O
.	O

Therefore	O
,	O
by	O
using	O
our	O
VFAE	Method
model	O
we	O
are	O
able	O
to	O
obtain	O
improved	O
generalization	Metric
and	O
classification	Task
performance	O
by	O
effectively	O
removing	O
from	O
our	O
representations	O
.	O

.49	O
[	O
width=	O
]	O
yaleb_x.pdf	O
.49	O
[	O
width=	O
]	O
yaleb_z.pdf	O
section	O
:	O
Related	O
Work	O
Most	O
related	O
to	O
our	O
“	O
fair	O
”	O
representations	O
view	O
is	O
the	O
work	O
from	O
.	O

They	O
proposed	O
a	O
neural	Method
network	Method
based	Method
semi	Method
-	Method
supervised	Method
clustering	Method
model	Method
for	O
learning	Task
fair	Task
representations	Task
.	O

The	O
idea	O
is	O
to	O
learn	O
a	O
localised	Method
representation	Method
that	O
maps	O
each	O
datapoint	O
to	O
a	O
cluster	O
in	O
such	O
a	O
way	O
that	O
each	O
cluster	O
gets	O
assigned	O
roughly	O
equal	O
proportions	O
of	O
data	O
from	O
each	O
group	O
in	O
.	O

Although	O
their	O
approach	O
was	O
successfully	O
applied	O
on	O
several	O
datasets	O
,	O
the	O
restriction	O
to	O
clustering	Task
means	O
that	O
it	O
can	O
not	O
leverage	O
the	O
representational	O
power	O
of	O
a	O
distributed	Method
representation	Method
.	O

Furthermore	O
,	O
this	O
penalty	O
does	O
not	O
account	O
for	O
higher	O
order	O
moments	O
in	O
the	O
latent	O
distribution	O
.	O

For	O
example	O
,	O
if	O
always	O
returns	O
or	O
,	O
while	O
returns	O
values	O
between	O
values	O
and	O
,	O
then	O
the	O
penalty	O
could	O
still	O
be	O
satisfied	O
,	O
but	O
information	O
could	O
still	O
leak	O
through	O
.	O

We	O
addressed	O
both	O
of	O
these	O
issues	O
in	O
this	O
paper	O
.	O

Domain	Task
adaptation	Task
can	O
also	O
be	O
cast	O
as	O
learning	Method
representations	Method
that	O
are	O
“	O
invariant	O
”	O
with	O
respect	O
to	O
a	O
discrete	O
variable	O
,	O
the	O
domain	O
.	O

Most	O
similar	O
to	O
our	O
work	O
are	O
neural	Method
network	Method
approaches	Method
which	O
try	O
to	O
match	O
the	O
feature	O
distributions	O
between	O
the	O
domains	O
.	O

This	O
was	O
performed	O
in	O
an	O
unsupervised	O
way	O
with	O
mSDA	Method
chen2012marginalized	O
by	O
training	O
denoising	Method
autoencoders	Method
jointly	O
on	O
all	O
domains	O
,	O
thus	O
implicitly	O
obtaining	O
a	O
representation	O
general	O
enough	O
to	O
explain	O
both	O
the	O
domain	O
and	O
the	O
data	O
.	O

This	O
is	O
in	O
contrast	O
to	O
our	O
approach	O
where	O
we	O
instead	O
try	O
to	O
learn	O
representations	O
that	O
explicitly	O
remove	O
domain	O
information	O
during	O
the	O
learning	Method
process	Method
.	O

For	O
the	O
latter	O
we	O
find	O
more	O
similarities	O
with	O
“	O
domain	Method
-	Method
regularized	Method
”	Method
supervised	Method
approaches	Method
that	O
simultaneously	O
try	O
to	O
predict	O
the	O
label	O
for	O
a	O
data	O
point	O
and	O
remove	O
domain	O
specific	O
information	O
.	O

This	O
is	O
done	O
with	O
either	O
MMD	Method
long2015learning	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
TzengHZSD14	O
or	O
adversarial	O
2015arXiv150507818	O
G	O
penalties	O
at	O
the	O
hidden	O
layers	O
of	O
the	O
network	O
.	O

In	O
our	O
model	O
however	O
the	O
main	O
“	O
domain	O
-	O
regularizer	O
”	O
stems	O
from	O
the	O
independence	O
properties	O
of	O
the	O
prior	O
over	O
the	O
domain	O
and	O
latent	Method
representations	Method
.	O

We	O
also	O
employ	O
MMD	Method
on	O
our	O
model	O
but	O
from	O
a	O
different	O
perspective	O
since	O
we	O
consider	O
a	O
slightly	O
more	O
difficult	O
case	O
where	O
the	O
domain	O
and	O
label	O
are	O
correlated	O
;	O
we	O
need	O
to	O
ensure	O
that	O
we	O
remain	O
as	O
“	O
invariant	O
”	O
as	O
possible	O
since	O
might	O
‘	O
leak	O
’	O
information	O
about	O
.	O

section	O
:	O
Conclusion	O
We	O
introduce	O
the	O
Variational	Method
Fair	Method
Autoencoder	Method
(	O
VFAE	Method
)	O
,	O
an	O
extension	O
of	O
the	O
semi	Method
-	Method
supervised	Method
variational	Method
autoencoder	Method
in	O
order	O
to	O
learn	O
representations	O
that	O
are	O
explicitly	O
invariant	O
with	O
respect	O
to	O
some	O
known	O
aspect	O
of	O
a	O
dataset	O
while	O
retaining	O
as	O
much	O
remaining	O
information	O
as	O
possible	O
.	O

We	O
further	O
use	O
a	O
Maximum	Method
Mean	Method
Discrepancy	Method
regularizer	O
in	O
order	O
to	O
further	O
promote	O
invariance	O
in	O
the	O
posterior	O
distribution	O
over	O
latent	O
variables	O
.	O

We	O
apply	O
this	O
model	O
to	O
tasks	O
involving	O
developing	O
fair	Method
classifiers	Method
that	O
are	O
invariant	O
to	O
sensitive	O
demographic	O
information	O
and	O
show	O
that	O
it	O
produces	O
a	O
better	O
tradeoff	O
with	O
respect	O
to	O
accuracy	Metric
and	O
invariance	Metric
.	O

As	O
a	O
second	O
application	O
,	O
we	O
consider	O
the	O
task	O
of	O
domain	Task
adaptation	Task
,	O
where	O
the	O
goal	O
is	O
to	O
improve	O
classification	Task
by	O
training	O
a	O
classifier	Method
that	O
is	O
invariant	O
to	O
the	O
domain	O
.	O

We	O
find	O
that	O
our	O
model	O
is	O
competitive	O
with	O
recently	O
proposed	O
adversarial	Method
approaches	Method
.	O

Finally	O
,	O
we	O
also	O
consider	O
the	O
more	O
general	O
task	O
of	O
learning	Task
invariant	Task
representations	Task
.	O

We	O
can	O
observe	O
that	O
our	O
model	O
provides	O
a	O
clear	O
improvement	O
against	O
a	O
neural	Method
network	Method
that	O
incorporates	O
a	O
Maximum	Method
Mean	Method
Discrepancy	Method
penalty	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Discrimination	Metric
metrics	Metric
The	O
Discrimination	Metric
metric	Metric
zemel2013learning	O
and	O
the	O
Discrimination	Metric
metric	Metric
that	O
takes	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
correct	O
class	O
are	O
mathematically	O
formalized	O
as	O
:	O
where	O
for	O
the	O
predictions	O
that	O
were	O
done	O
on	O
the	O
datapoints	O
with	O
nuisance	O
variable	O
,	O
denotes	O
the	O
total	O
amount	O
of	O
datapoints	O
that	O
had	O
nuisance	O
variable	O
and	O
denotes	O
the	O
probability	O
of	O
the	O
prediction	O
for	O
the	O
datapoints	O
with	O
.	O

For	O
the	O
predictions	O
and	O
their	O
respective	O
probabilities	O
we	O
used	O
a	O
Logistic	Method
Regression	Method
classifier	Method
.	O

appendix	O
:	O
Proxy	Method
A	Method
-	Method
Distance	Method
(	O
PAD	Method
)	O
for	O
Amazon	Material
Reviews	Material
dataset	Material
Similarly	O
to	O
,	O
we	O
also	O
calculated	O
the	O
Proxy	O
A	O
-	O
distance	O
(	O
PAD	Method
)	O
ben2007analysis	O
,	O
ben2010theory	O
scores	O
for	O
the	O
raw	O
data	O
and	O
for	O
the	O
representations	O
of	O
VFAE	Method
.	O

Briefly	O
,	O
Proxy	Method
A	Method
-	Method
distance	Method
is	O
an	O
approximation	O
to	O
the	O
-	Metric
divergence	Metric
measure	Metric
of	Metric
domain	Metric
distinguishability	Metric
proposed	O
in	O
and	O
.	O

To	O
compute	O
it	O
we	O
first	O
need	O
to	O
train	O
a	O
learning	Method
algorithm	Method
on	O
the	O
task	O
of	O
discriminating	O
examples	O
from	O
the	O
source	O
and	O
target	O
domain	O
.	O

Afterwards	O
we	O
can	O
use	O
the	O
test	O
error	O
of	O
that	O
algorithm	O
in	O
the	O
following	O
formula	O
:	O
It	O
is	O
clear	O
that	O
low	O
PAD	Metric
scores	Metric
correspond	O
to	O
low	O
discrimination	O
of	O
the	O
source	O
and	O
target	O
domain	O
examples	O
from	O
the	O
classifier	Method
.	O

To	O
obtain	O
for	O
our	O
model	O
we	O
used	O
Logistic	Method
Regression	Method
.	O

The	O
resulting	O
plot	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
we	O
have	O
also	O
added	O
the	O
plot	O
from	O
DANN	O
2015arXiv150507818	O
G	O
,	O
where	O
they	O
used	O
a	O
linear	Method
Support	Method
Vector	Method
Machine	Method
for	O
the	O
classifier	Method
,	O
as	O
a	O
reference	O
.	O

It	O
can	O
be	O
seen	O
that	O
our	O
VFAE	Method
model	O
can	O
factor	O
out	O
the	O
information	O
about	O
better	O
,	O
since	O
the	O
PAD	Metric
scores	Metric
on	O
our	O
new	O
representation	O
are	O
,	O
overall	O
,	O
lower	O
than	O
the	O
ones	O
obtained	O
from	O
the	O
DANN	Method
architecture	Method
.	O

.49	O
[	O
width=1.	O
]	O
pad_vfae_x_reviews.pdf	O
.49	O
[	O
width=.9	O
]	O
PAD_DANN.pdf	O
document	O
:	O
Revisiting	O
Distributional	Method
Correspondence	Method
Indexing	Method
:	O
A	O
Python	Method
Reimplementation	Method
and	O
New	O
Experiments	O
This	O
paper	O
introduces	O
PyDCI	Method
,	O
a	O
new	O
implementation	O
of	O
Distributional	Method
Correspondence	Method
Indexing	Method
(	O
DCI	Method
)	O
written	O
in	O
Python	O
.	O

DCI	Method
is	O
a	O
transfer	Method
learning	Method
method	Method
for	O
cross	Task
-	Task
domain	Task
and	Task
cross	Task
-	Task
lingual	Task
text	Task
classification	Task
for	O
which	O
we	O
had	O
provided	O
an	O
implementation	O
(	O
here	O
called	O
JaDCI	Method
)	O
built	O
on	O
top	O
of	O
JaTeCS	Method
,	O
a	O
Java	Method
framework	Method
for	O
text	Task
classification	Task
.	O

PyDCI	Method
is	O
a	O
stand	O
-	O
alone	O
version	O
of	O
DCI	Method
that	O
exploits	O
scikit	Method
-	Method
learn	Method
and	O
the	O
SciPy	Method
stack	Method
.	O

We	O
here	O
report	O
on	O
new	O
experiments	O
that	O
we	O
have	O
carried	O
out	O
in	O
order	O
to	O
test	O
PyDCI	Method
,	O
and	O
in	O
which	O
we	O
use	O
as	O
baselines	O
new	O
high	O
-	O
performing	O
methods	O
that	O
have	O
appeared	O
after	O
DCI	Method
was	O
originally	O
proposed	O
.	O

These	O
experiments	O
show	O
that	O
,	O
thanks	O
to	O
a	O
few	O
subtle	O
ways	O
in	O
which	O
we	O
have	O
improved	O
DCI	Method
,	O
PyDCI	Method
outperforms	O
both	O
JaDCI	Method
and	O
the	O
above	O
-	O
mentioned	O
high	O
-	O
performing	O
methods	O
,	O
and	O
delivers	O
the	O
best	O
known	O
results	O
on	O
the	O
two	O
popular	O
benchmarks	O
on	O
which	O
we	O
had	O
tested	O
DCI	Method
,	O
i.e.	O
,	O
MultiDomainSentiment	Method
(	O
a.k.a	O
.	O

MDS	Method
–	O
for	O
cross	Task
-	Task
domain	Task
adaptation	Task
)	O
and	O
Webis	O
-	O
CLS	O
-	O
10	O
(	O
for	O
cross	Task
-	Task
lingual	Task
adaptation	Task
)	O
.	O

PyDCI	Method
,	O
together	O
with	O
the	O
code	O
allowing	O
to	O
replicate	O
our	O
experiments	O
,	O
is	O
available	O
at	O
.	O

Transfer	O
Learning	O
Domain	O
Adaptation	O
Text	O
Classification	O
Sentiment	O
Classification	O
Cross	O
-	O
Domain	O
Classification	O
Cross	O
-	O
Lingual	O
Classification	O
Python	O
section	O
:	O
Introduction	O
Distributional	Method
Correspondence	Method
Indexing	Method
(	O
DCI	Method
)	O
is	O
a	O
pivot	Method
-	Method
based	Method
feature	Method
-	Method
transfer	Method
domain	Method
adaptation	Method
method	Method
for	O
cross	Task
-	Task
domain	Task
and	Task
cross	Task
-	Task
lingual	Task
text	Task
classification	Task
.	O

DCI	Method
was	O
first	O
described	O
in	O
,	O
and	O
later	O
improved	O
and	O
extended	O
in	O
;	O
it	O
was	O
formerly	O
implemented	O
in	O
Java	O
as	O
part	O
of	O
the	O
JaTeCS	O
(	O
Ja	O
va	O
Te	O
xt	O
C	O
ategorization	Method
S	Method
ystem	Method
)	O
framework	O
,	O
and	O
this	O
implementation	O
(	O
henceforth	O
called	O
JaDCI	Method
)	O
was	O
made	O
publicly	O
available	O
.	O

JaTeCS	Method
is	O
a	O
complex	O
package	O
,	O
since	O
it	O
makes	O
available	O
many	O
functionalities	O
for	O
text	Task
analytics	Task
research	Task
.	O

A	O
drawback	O
of	O
JaDCI	Method
is	O
thus	O
that	O
,	O
for	O
the	O
researcher	O
wishing	O
to	O
replicate	O
the	O
results	O
of	O
or	O
simply	O
wishing	O
to	O
use	O
JaDCI	Method
,	O
a	O
substantive	O
effort	O
in	O
installing	O
and	O
properly	O
configuring	O
the	O
entire	O
JaTeCS	Method
framework	Method
is	O
thus	O
needed	O
.	O

In	O
this	O
paper	O
we	O
present	O
PyDCI	Method
,	O
a	O
new	O
implementation	O
of	O
the	O
DCI	Method
method	O
written	O
in	O
Python	O
and	O
built	O
on	O
top	O
of	O
the	O
SciPy	Method
stack	Method
and	O
scikit	Method
-	Method
learn	Method
toolkit	Method
.	O

Python	Method
has	O
become	O
the	O
preferred	O
programming	O
language	O
for	O
computer	Task
scientists	Task
.	O

In	O
the	O
fields	O
of	O
machine	Task
learning	Task
and	O
data	Task
mining	Task
its	O
use	O
has	O
also	O
been	O
promoted	O
by	O
the	O
appearance	O
of	O
Python	Method
-	Method
based	Method
environments	Method
such	O
as	O
SciPy	Method
and	O
scikit	Method
-	Method
learn	Method
,	O
whose	O
potential	O
and	O
ease	O
of	O
use	O
have	O
attracted	O
the	O
interest	O
of	O
practitioners	O
.	O

Our	O
reimplementation	O
is	O
thus	O
in	O
line	O
with	O
these	O
trends	O
.	O

With	O
respect	O
to	O
JaDCI	Method
,	O
PyDCI	Method
introduces	O
a	O
few	O
modifications	O
in	O
the	O
way	O
DCI	Method
is	O
implemented	O
that	O
,	O
although	O
subtle	O
,	O
bring	O
about	O
a	O
significant	O
improvement	O
in	O
the	O
effectiveness	O
of	O
the	O
method	O
.	O

The	O
rest	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
describe	O
the	O
main	O
modifications	O
to	O
DCI	Method
that	O
our	O
new	O
implementation	O
introduces	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
report	O
on	O
new	O
experiments	O
that	O
we	O
have	O
run	O
using	O
PyDCI	Method
,	O
and	O
show	O
that	O
,	O
thanks	O
to	O
the	O
modifications	O
above	O
,	O
PyDCI	Method
delivers	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
two	O
popular	O
benchmark	O
datasets	O
,	O
i.e.	O
,	O
MultiDomainSentiment	Method
(	O
hereafter	O
MDS	Method
–	O
for	O
cross	Task
-	Task
domain	Task
adaptation	Task
)	O
and	O
Webis	Method
-	Method
CLS	Method
-	Method
10	Method
(	O
for	O
cross	Task
-	Task
lingual	Task
adaptation	Task
)	O
.	O

These	O
results	O
represent	O
a	O
clear	O
improvement	O
over	O
the	O
ones	O
originally	O
obtained	O
with	O
JaDCI	Method
and	O
presented	O
in	O
,	O
and	O
also	O
over	O
the	O
ones	O
obtained	O
by	O
recent	O
high	O
-	O
performing	O
methods	O
that	O
have	O
appeared	O
,	O
or	O
that	O
we	O
have	O
become	O
aware	O
of	O
,	O
after	O
DCI	Method
was	O
originally	O
proposed	O
.	O

Section	O
[	O
reference	O
]	O
concludes	O
,	O
hinting	O
at	O
future	O
developments	O
.	O

We	O
make	O
PyDCI	Method
publicly	O
available	O
via	O
GitHub	O
.	O

section	O
:	O
Implementation	O
Changes	O
For	O
reasons	O
of	O
brevity	O
we	O
do	O
not	O
re	O
-	O
explain	O
DCI	Method
from	O
scratch	O
;	O
we	O
refer	O
the	O
interested	O
reader	O
to	O
for	O
a	O
concise	O
description	O
,	O
or	O
to	O
for	O
the	O
full	O
-	O
blown	O
presentation	O
.	O

The	O
main	O
modifications	O
that	O
PyDCI	Method
introduces	O
with	O
respect	O
to	O
JaDCI	Method
are	O
the	O
following	O
:	O
Document	Task
Standardization	Task
:	O
In	O
DCI	Method
,	O
feature	O
vectors	O
and	O
document	O
vectors	O
(	O
i.e.	O
,	O
the	O
vectors	O
that	O
represent	O
the	O
features	O
and	O
the	O
vectors	O
that	O
represent	O
the	O
documents	O
,	O
respectively	O
)	O
are	O
post	O
-	O
processed	O
via	O
L2	Method
-	Method
normalization	Method
.	O

In	O
we	O
had	O
witnessed	O
improvements	O
when	O
applying	O
standardization	Method
to	O
the	O
feature	O
vectors	O
(	O
i.e.	O
,	O
translating	O
and	O
scaling	O
each	O
dimension	O
so	O
that	O
it	O
is	O
approximatelly	O
normally	O
distributed	O
in	O
–	O
see	O
)	O
.	O

In	O
PyDCI	Method
we	O
give	O
the	O
user	O
the	O
option	O
to	O
apply	O
standardization	O
also	O
to	O
each	O
dimension	O
of	O
the	O
document	O
vectors	O
before	O
training	O
the	O
classifier	Method
.	O

All	O
experiments	O
we	O
report	O
in	O
this	O
paper	O
are	O
run	O
with	O
this	O
option	O
activated	O
.	O

Classifier	Task
Optimization	Task
:	O
In	O
PyDCI	Method
we	O
use	O
scikit	Method
-	Method
learn	Method
’s	Method
implementation	Method
of	O
linear	Method
SVMs	Method
(	O
LinearSVC	Method
,	O
which	O
is	O
in	O
turn	O
based	O
on	O
the	O
liblinear	Method
package	Method
)	O
,	O
instead	O
of	O
using	O
Joachims	Method
’	Method
SVM	Method
package	Method
as	O
we	O
had	O
done	O
in	O
JaDCI	Method
.	O

This	O
allows	O
us	O
to	O
leverage	O
scikit	Method
-	Method
learn	Method
’s	Method
GridSearchCV	Method
utility	Method
in	O
order	O
to	O
optimize	O
SVM	Method
’s	O
parameter	O
(	O
which	O
determines	O
the	O
trade	O
-	O
off	O
between	O
training	Metric
error	Metric
and	O
the	O
margin	O
)	O
via	O
grid	Method
search	Method
optimization	Method
,	O
which	O
allows	O
us	O
to	O
effortlessly	O
tune	O
the	O
classifier	Method
.	O

In	O
the	O
new	O
experiments	O
using	O
PyDCI	O
we	O
let	O
parameter	O
range	O
in	O
,	O
while	O
in	O
the	O
JaDCI	O
experiments	O
we	O
had	O
simply	O
relied	O
on	O
the	O
default	O
value	O
that	O
SVM	O
attributes	O
to	O
.	O

Increase	O
in	O
the	O
Number	O
of	O
Pivots	O
:	O
We	O
increase	O
the	O
number	O
of	O
pivots	O
from	O
100	O
(	O
the	O
value	O
we	O
had	O
used	O
in	O
)	O
to	O
1	O
,	O
000	O
in	O
the	O
cross	O
-	O
domain	O
experiments	O
and	O
to	O
450	O
in	O
the	O
cross	Task
-	Task
lingual	Task
experiments	Task
.	O

This	O
brings	O
about	O
a	O
significant	O
improvement	O
in	O
performance	O
,	O
that	O
does	O
not	O
come	O
at	O
a	O
significant	O
cost	O
in	O
execution	Metric
time	Metric
(	O
as	O
instead	O
had	O
happened	O
with	O
the	O
previous	O
implementation	O
)	O
.	O

We	O
limit	O
the	O
number	O
of	O
pivots	O
to	O
450	O
in	O
the	O
cross	Task
-	Task
lingual	Task
case	Task
(	O
instead	O
of	O
1	O
,	O
000	O
)	O
since	O
in	O
this	O
case	O
each	O
pivot	O
requires	O
a	O
translation	O
to	O
the	O
target	O
language	O
which	O
is	O
assumed	O
to	O
have	O
a	O
cost	O
;	O
we	O
thus	O
set	O
the	O
number	O
of	O
pivots	O
to	O
450	O
as	O
was	O
done	O
in	O
previous	O
research	O
(	O
e.g.	O
,	O
in	O
)	O
.	O

We	O
discuss	O
below	O
in	O
more	O
detail	O
the	O
impact	O
on	O
performance	O
that	O
the	O
variation	O
in	O
the	O
number	O
of	O
pivots	O
has	O
.	O

We	O
should	O
also	O
mention	O
that	O
PyDCI	Method
relies	O
on	O
scikit	Method
-	Method
learn	Method
(	O
while	O
JaDCI	Method
relied	O
on	O
JaTeCS	Method
)	O
for	O
many	O
preprocessing	Task
-	Task
related	Task
aspects	Task
(	O
e.g.	O
,	O
term	Task
weighting	Task
)	O
,	O
which	O
also	O
may	O
cause	O
some	O
(	O
hard	O
to	O
track	O
)	O
differences	O
in	O
performance	O
with	O
respect	O
to	O
JaDCI	Method
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Effectiveness	O
on	O
Cross	Task
-	Task
Domain	Task
Classification	Task
and	O
Cross	Task
-	Task
Lingual	Task
Classification	Task
In	O
this	O
section	O
we	O
report	O
the	O
results	O
we	O
have	O
obtained	O
in	O
rerunning	O
with	O
PyDCI	Method
the	O
same	O
experiments	O
we	O
had	O
run	O
with	O
JaDCI	Method
,	O
and	O
whose	O
results	O
had	O
been	O
reported	O
in	O
.	O

The	O
datasets	O
we	O
use	O
are	O
arguably	O
the	O
most	O
popular	O
benchmarks	O
in	O
the	O
domain	O
adaptation	O
literature	O
,	O
i.e.	O
,	O
MDS	Method
for	O
cross	Task
-	Task
domain	Task
adaptation	Task
and	O
Webis	Task
-	Task
CLS	Task
-	Task
10	Task
for	O
cross	Task
-	Task
lingual	Task
adaptation	Task
.	O

A	O
complete	O
description	O
of	O
the	O
datasets	O
and	O
the	O
standard	O
experimental	O
protocol	O
followed	O
in	O
each	O
case	O
can	O
be	O
found	O
either	O
in	O
the	O
original	O
publications	O
describing	O
the	O
datasets	O
or	O
in	O
.	O

Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
the	O
values	O
of	O
classification	Metric
accuracy	Metric
(	O
i.e.	O
,	O
the	O
fraction	O
of	O
correctly	O
classified	O
documents	O
)	O
we	O
obtain	O
for	O
cross	Task
-	Task
domain	Task
and	Task
cross	Task
-	Task
lingual	Task
classification	Task
experiments	O
,	O
respectively	O
.	O

We	O
focus	O
on	O
Linear	O
and	O
Cosine	Method
(	O
columns	O
9	O
-	O
10	O
)	O
,	O
two	O
parameter	O
-	O
free	O
probabilistic	O
and	O
kernel	O
-	O
based	O
distributional	Method
correspondence	Method
functions	Method
(	O
DCFs	Method
)	O
investigated	O
in	O
.	O

For	O
each	O
such	O
DCF	O
we	O
show	O
a	O
direct	O
comparison	O
against	O
the	O
values	O
we	O
had	O
obtained	O
with	O
JaDCI	Method
(	O
Columns	O
7	O
-	O
8	O
)	O
.	O

We	O
also	O
report	O
two	O
baselines	O
:	O
Lower	O
(	O
Column	O
3	O
)	O
,	O
a	O
classifier	Method
that	O
directly	O
trains	O
on	O
the	O
‘	O
‘	O
source	O
’	O
’	O
training	O
examples	O
and	O
tests	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
unlabeled	O
examples	O
without	O
performing	O
any	O
sort	O
of	O
adaptation	Task
at	O
all	O
.	O

Such	O
a	O
classifier	Method
should	O
thus	O
act	O
as	O
a	O
lower	O
bound	O
for	O
any	O
reasonable	O
adaptation	Task
endeavour	Task
.	O

Upper	O
(	O
Column	O
4	O
)	O
,	O
a	O
classifier	Method
that	O
trains	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
training	O
examples	O
and	O
tests	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
unlabeled	O
examples	O
without	O
performing	O
any	O
sort	O
of	O
adaptation	Task
at	O
all	O
.	O

Such	O
a	O
classifier	Method
should	O
thus	O
act	O
as	O
an	O
upper	O
bound	O
for	O
any	O
reasonable	O
adaptation	Task
endeavour	Task
.	O

The	O
baselines	O
use	O
exactly	O
the	O
same	O
learner	Method
we	O
use	O
for	O
PyDCI	Method
(	O
LinearSVC	Method
with	O
the	O
parameter	O
optimized	O
via	O
grid	Method
search	Method
)	O
.	O

For	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
we	O
also	O
report	O
the	O
accuracy	Metric
obtained	O
by	O
what	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
today	O
the	O
best	O
-	O
performing	O
known	O
method	O
on	O
this	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
(	O
Column	O
5	O
–	O
labelled	O
as	O
‘	O
‘	O
SOTA	O
’	O
’	O
,	O
which	O
stands	O
for	O
‘	O
‘	O
S	O
tate	O
O	O
f	O
T	O
he	O
A	O
rt	O
’	O
’	O
–	O
reports	O
the	O
name	O
of	O
the	O
method	O
and	O
Column	O
6	O
reports	O
the	O
accuracy	Metric
score	Metric
,	O
taken	O
from	O
the	O
original	O
paper	O
)	O
.	O

Boldface	O
indicates	O
the	O
best	O
score	O
for	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
;	O
shadowed	O
cells	O
indicate	O
the	O
PyDCI	O
scores	O
that	O
outperform	O
the	O
best	O
-	O
known	O
results	O
.	O

Note	O
that	O
,	O
aside	O
from	O
SDA	Method
,	O
all	O
the	O
baselines	O
in	O
the	O
‘	O
‘	O
SOTA	O
’	O
’	O
column	O
had	O
not	O
been	O
used	O
as	O
baselines	O
in	O
our	O
original	O
work	O
on	O
DCI	Method
;	O
the	O
reason	O
is	O
that	O
these	O
methods	O
were	O
published	O
after	O
DCI	Method
appeared	O
in	O
print	O
,	O
or	O
that	O
we	O
were	O
unaware	O
of	O
them	O
.	O

PyDCI	Method
outperforms	O
JaDCI	Method
in	O
most	O
cases	O
,	O
and	O
outperforms	O
also	O
the	O
best	O
-	O
performing	O
method	O
in	O
the	O
literature	O
,	O
which	O
is	O
not	O
always	O
the	O
same	O
for	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
,	O
with	O
very	O
few	O
exceptions	O
.	O

PyDCI	Method
obtains	O
7	O
out	O
of	O
13	O
best	O
results	O
on	O
MDS	Task
(	O
including	O
best	O
averaged	Metric
accuracy	Metric
)	O
when	O
equipped	O
with	O
the	O
Cosine	Method
DCF	O
,	O
and	O
5	O
out	O
of	O
10	O
best	O
results	O
in	O
Webis	Task
-	Task
CLS	Task
-	Task
10	Task
when	O
using	O
the	O
Linear	Method
DCF	Method
(	O
including	O
best	O
averaged	Metric
accuracy	Metric
)	O
.	O

In	O
agreement	O
with	O
with	O
,	O
Cosine	Method
proved	O
the	O
best	O
performing	O
DCF	Method
,	O
yielding	O
the	O
best	O
results	O
overall	O
and	O
surpassing	O
the	O
best	O
accuracy	Metric
obtained	O
by	O
any	O
other	O
method	O
in	O
17	O
cases	O
out	O
of	O
23	O
(	O
across	O
the	O
two	O
datasets	O
,	O
and	O
also	O
including	O
the	O
average	Metric
results	O
)	O
.	O

With	O
respect	O
to	O
the	O
previously	O
best	O
-	O
performing	O
system	O
,	O
PyDCI	Method
(	O
Cosine	Method
)	O
brings	O
about	O
a	O
reduction	O
in	O
error	Metric
of	O
+	O
10.2	O
%	O
on	O
MDS	Method
and	O
+	O
9.6	O
%	O
on	O
Webis	Task
-	Task
CLS	Task
-	Task
10	Task
.	O

On	O
the	O
very	O
same	O
(	O
problem	O
,	O
dataset	O
)	O
pairs	O
we	O
have	O
also	O
run	O
experiments	O
in	O
order	O
to	O
evaluate	O
the	O
impact	O
of	O
modifications	O
[	O
reference	O
]	O
(	O
Document	Task
Standardization	Task
)	O
and	O
[	O
reference	O
]	O
(	O
Classifier	Task
Optimization	Task
)	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Concerning	O
document	Task
standardization	Task
,	O
we	O
have	O
rerun	O
all	O
the	O
PyDCI	Method
experiments	O
described	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
without	O
applying	O
document	Method
standardization	Method
.	O

The	O
results	O
are	O
reported	O
in	O
the	O
first	O
two	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
,	O
and	O
indicate	O
,	O
on	O
average	Metric
,	O
a	O
relative	O
improvement	O
in	O
accuracy	Metric
of	O
+	O
0.2	O
%	O
on	O
MDS	Method
and	O
+	O
8.6	O
%	O
on	O
Webis	Task
-	Task
CLS	Task
-	Task
10	Task
;	O
document	Task
standardization	Task
thus	O
appears	O
to	O
be	O
clearly	O
beneficial	O
.	O

Concerning	O
classifier	Method
optimization	Method
,	O
we	O
have	O
rerun	O
all	O
the	O
PyDCI	Method
experiments	O
described	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
without	O
applying	O
classifier	Method
optimization	Method
.	O

The	O
results	O
are	O
reported	O
in	O
the	O
last	O
two	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
,	O
and	O
indicate	O
,	O
on	O
average	Metric
,	O
a	O
relative	O
improvement	O
in	O
accuracy	Metric
of	O
+	O
9.7	O
%	O
on	O
MDS	Method
and	O
+	O
11.8	O
%	O
on	O
Webis	Method
-	Method
CLS	Method
-	Method
10	Method
;	O
also	O
classifier	Method
optimization	Method
is	O
thus	O
(	O
unsurprisingly	O
)	O
clearly	O
beneficial	O
.	O

subsection	O
:	O
Effectiveness	O
on	O
Cross	Task
-	Task
Domain	Task
Cross	Task
-	Task
Lingual	Task
Classification	Task
Table	O
[	O
reference	O
]	O
reports	O
classification	Metric
accuracy	Metric
values	Metric
obtained	O
in	O
the	O
domain	Task
adaptation	Task
setting	Task
proposed	O
in	O
,	O
in	O
which	O
both	O
domain	O
and	O
language	O
differ	O
between	O
the	O
source	O
and	O
target	O
(	O
i.e.	O
,	O
when	O
the	O
classification	Task
task	Task
is	O
simultaneously	O
cross	O
-	O
domain	O
and	O
cross	O
-	O
lingual	O
)	O
.	O

In	O
Table	O
[	O
reference	O
]	O
we	O
include	O
the	O
results	O
we	O
had	O
obtained	O
in	O
for	O
the	O
Cross	Method
-	Method
Lingual	Method
Structural	Method
Correspondence	Method
Learning	Method
(	Method
SCL	Method
)	Method
method	Method
(	O
which	O
we	O
use	O
here	O
as	O
a	O
baseline	O
)	O
,	O
using	O
its	O
authors	O
’	O
code	O
(	O
see	O
)	O
.	O

The	O
reason	O
why	O
we	O
use	O
SCL	Method
as	O
a	O
baseline	O
is	O
that	O
,	O
although	O
newer	O
approaches	O
have	O
been	O
tested	O
in	O
this	O
setting	O
,	O
none	O
of	O
them	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
has	O
outperformed	O
SCL	Method
so	O
far	O
.	O

The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
confirm	O
the	O
superiority	O
of	O
PyDCI	Method
over	O
JaDCI	Method
.	O

In	O
this	O
case	O
,	O
though	O
,	O
the	O
differences	O
in	O
performance	O
between	O
the	O
‘	O
‘	O
Cosine	Method
’	O
’	O
counterparts	O
is	O
less	O
pronounced	O
.	O

Between	O
the	O
PyDCI	Method
variants	Method
,	O
Linear	Method
performs	O
slightly	O
better	O
than	O
Cosine	Method
.	O

subsection	O
:	O
Statistical	Metric
Significance	Metric
We	O
have	O
subjected	O
our	O
experiments	O
to	O
thorough	O
statistical	Task
significance	Task
testing	Task
,	O
by	O
running	O
a	O
two	O
-	O
tailed	O
t	O
-	O
test	O
on	O
paired	O
examples	O
across	O
all	O
runs	O
(	O
cross	O
-	O
domain	O
and	O
/	O
or	O
cross	O
-	O
lingual	O
)	O
.	O

The	O
test	O
reveals	O
that	O
the	O
PyDCI	O
versions	O
of	O
Linear	O
and	O
Cosine	Method
outperform	O
,	O
in	O
a	O
statistically	O
significant	O
sense	O
,	O
the	O
corresponding	O
JaDCI	Method
versions	Method
(	O
at	O
a	O
confidence	O
level	O
of	O
)	O
.	O

subsection	O
:	O
Efficiency	O
One	O
important	O
aspect	O
of	O
DCI	Method
in	O
general	O
,	O
and	O
of	O
PyDCI	Method
in	O
particular	O
,	O
is	O
its	O
efficiency	O
.	O

Figure	O
[	O
reference	O
]	O
reports	O
the	O
computation	Metric
times	Metric
we	O
have	O
recorded	O
in	O
order	O
to	O
measure	O
the	O
efficiency	O
of	O
PyDCI	Method
.	O

While	O
the	O
best	O
-	O
performing	O
methods	O
from	O
the	O
literature	O
rely	O
on	O
computationally	O
expensive	O
optimizations	O
(	O
most	O
of	O
them	O
are	O
deep	Method
-	Method
learning	Method
-	O
based	O
)	O
,	O
none	O
of	O
the	O
experiments	O
we	O
have	O
presented	O
so	O
far	O
required	O
more	O
than	O
35	O
seconds	O
to	O
run	O
.	O

subsection	O
:	O
Effectiveness	Metric
vs.	O
Efficiency	O
Trade	O
-	O
off	O
In	O
this	O
section	O
we	O
analyse	O
the	O
trade	O
-	O
off	O
between	O
effectiveness	Metric
(	O
in	O
terms	O
of	O
classification	Metric
accuracy	Metric
)	O
and	O
time	Metric
efficiency	Metric
(	O
in	O
terms	O
of	O
seconds	O
)	O
.	O

In	O
this	O
experiment	O
,	O
we	O
vary	O
the	O
number	O
of	O
pivots	O
in	O
the	O
range	O
.	O

For	O
the	O
Webis	Task
-	Task
CLS	Task
-	Task
10	Task
we	O
bound	O
this	O
range	O
to	O
pivots	O
since	O
,	O
for	O
some	O
tasks	O
it	O
was	O
impossible	O
to	O
extract	O
more	O
than	O
pivots	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
average	Metric
accuracy	O
(	O
left	O
)	O
and	O
computation	Metric
times	Metric
for	O
MDS	Method
and	O
Webis	Method
-	Method
CLS	Method
-	Method
10	Method
.	O

As	O
increases	O
,	O
PyDCI	Method
surpasses	O
the	O
best	O
average	Metric
accuracy	Metric
reported	O
for	O
any	O
other	O
method	O
in	O
both	O
datasets	O
.	O

In	O
particular	O
,	O
and	O
in	O
accordance	O
with	O
,	O
PyDCI	Method
equipped	O
with	O
the	O
Cosine	Method
DCF	O
does	O
so	O
with	O
only	O
100	O
pivots	O
.	O

In	O
this	O
case	O
,	O
and	O
in	O
contrast	O
with	O
JaDCI	Method
,	O
classification	Metric
accuracy	Metric
increases	O
noticeably	O
when	O
more	O
pivots	O
are	O
taken	O
into	O
account	O
;	O
this	O
might	O
be	O
a	O
side	O
effect	O
of	O
the	O
modifications	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
any	O
case	O
,	O
the	O
method	O
seems	O
to	O
reach	O
a	O
plateau	O
for	O
higher	O
values	O
of	O
,	O
allowing	O
the	O
Cosine	Method
variant	O
to	O
reach	O
new	O
peaks	O
of	O
classification	Metric
accuracy	Metric
of	O
0.839	O
(	O
when	O
)	O
in	O
MDS	Method
,	O
and	O
0.840	O
(	O
when	O
)	O
in	O
Webis	Method
-	Method
CLS	Method
-	Method
10	Method
.	O

Regarding	O
the	O
efficiency	O
of	O
the	O
method	O
,	O
PyDCI	Method
exhibits	O
a	O
quasi	O
-	O
linear	O
trend	O
in	O
time	Metric
complexity	Metric
,	O
e.g.	O
,	O
when	O
the	O
number	O
of	O
pivots	O
is	O
doubled	O
,	O
the	O
execution	Metric
time	Metric
is	O
roughly	O
doubled	O
too	O
.	O

section	O
:	O
Conclusions	O
We	O
have	O
presented	O
PyDCI	Method
,	O
a	O
(	O
Python	O
-	O
based	O
)	O
revision	O
of	O
our	O
previous	O
(	O
Java	Method
-	Method
based	Method
)	Method
implementation	Method
of	O
DCI	Method
.	O

This	O
new	O
implementation	O
incorporates	O
changes	O
that	O
,	O
although	O
subtle	O
,	O
nonetheless	O
allow	O
the	O
method	O
to	O
deliver	O
improved	O
results	O
that	O
outperform	O
the	O
currently	O
known	O
best	O
-	O
performing	O
methods	O
.	O

The	O
efficiency	O
tests	O
we	O
have	O
carried	O
out	O
speak	O
clearly	O
about	O
the	O
efficiency	O
of	O
PyDCI	Method
,	O
which	O
requires	O
roughly	O
half	O
a	O
minute	O
to	O
undertake	O
any	O
of	O
the	O
domain	Task
adaptation	Task
tasks	Task
in	O
our	O
experiments	O
.	O

In	O
a	O
preliminary	O
study	O
DCI	Method
was	O
also	O
tested	O
in	O
transductive	Task
scenarios	Task
.	O

PyDCI	Method
does	O
not	O
support	O
transductive	Method
classification	Method
;	O
this	O
is	O
something	O
we	O
plan	O
to	O
address	O
in	O
the	O
near	O
future	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Face	Method
Attention	Method
Network	Method
:	O
An	O
Effective	O
Face	Method
Detector	Method
for	O
the	O
Occluded	O
Faces	O
The	O
performance	O
of	O
face	Task
detection	Task
has	O
been	O
largely	O
improved	O
with	O
the	O
development	O
of	O
convolutional	Method
neural	Method
network	Method
.	O

However	O
,	O
the	O
occlusion	Task
issue	Task
due	O
to	O
mask	O
and	O
sunglasses	O
,	O
is	O
still	O
a	O
challenging	O
problem	O
.	O

The	O
improvement	O
on	O
the	O
recall	Metric
of	O
these	O
occluded	O
cases	O
usually	O
brings	O
the	O
risk	O
of	O
high	O
false	Metric
positives	Metric
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
face	Task
detector	O
called	O
Face	Method
Attention	Method
Network	Method
(	O
FAN	Method
)	O
,	O
which	O
can	O
significantly	O
improve	O
the	O
recall	Metric
of	O
the	O
face	Task
detection	Task
problem	Task
in	O
the	O
occluded	O
case	O
without	O
compromising	O
the	O
speed	O
.	O

More	O
specifically	O
,	O
we	O
propose	O
a	O
new	O
anchor	O
-	O
level	O
attention	O
,	O
which	O
will	O
highlight	O
the	O
features	O
from	O
the	O
face	Task
region	O
.	O

Integrated	O
with	O
our	O
anchor	Method
assign	Method
strategy	Method
and	O
data	Method
augmentation	Method
techniques	Method
,	O
we	O
obtain	O
state	O
-	O
of	O
-	O
art	O
results	O
on	O
public	O
face	Task
detection	Task
benchmarks	O
like	O
WiderFace	Material
and	O
MAFA	Material
.	O

The	O
code	O
will	O
be	O
released	O
for	O
reproduction	O
.	O

section	O
:	O
Introduction	O
Face	Task
detection	Task
is	O
a	O
fundamental	O
and	O
essential	O
step	O
for	O
many	O
face	Task
related	O
applications	O
,	O
e.g.	O
face	Task
landmark	O
and	O
face	Task
recognition	O
.	O

Starting	O
from	O
the	O
pioneering	O
work	O
of	O
Viola	Task
-	Task
Jones	Task
,	O
face	Task
detection	Task
witnessed	O
a	O
large	O
number	O
of	O
progress	O
,	O
especially	O
as	O
the	O
recent	O
development	O
of	O
convolutional	Method
neural	Method
networks	Method
.	O

However	O
,	O
the	O
occlusion	Task
problem	Task
is	O
still	O
a	O
challenging	O
problem	O
and	O
few	O
of	O
the	O
work	O
have	O
been	O
presented	O
to	O
address	O
this	O
issue	O
.	O

More	O
importantly	O
,	O
occlusion	O
caused	O
by	O
mask	O
,	O
sunglasses	O
or	O
other	O
faces	O
widely	O
exists	O
in	O
the	O
real	Task
-	Task
life	Task
applications	Task
.	O

The	O
difficulty	O
to	O
address	O
the	O
occlusion	Task
issue	Task
lies	O
at	O
the	O
risk	O
of	O
potential	O
false	Task
positive	Task
problem	Task
.	O

Considering	O
the	O
case	O
of	O
detecting	O
a	O
face	Task
occluded	O
by	O
a	O
sunglasses	O
,	O
only	O
the	O
lower	O
part	O
of	O
face	Task
is	O
available	O
.	O

The	O
models	O
,	O
which	O
can	O
recognize	O
the	O
face	Task
only	O
based	O
on	O
the	O
lower	O
part	O
,	O
will	O
be	O
easily	O
misclassified	O
at	O
the	O
positions	O
like	O
hands	O
which	O
share	O
the	O
similar	O
skin	O
color	O
.	O

How	O
to	O
successfully	O
address	O
the	O
occlusion	Task
issue	Task
and	O
meanwhile	O
prevent	O
the	O
false	Task
positive	Task
problem	Task
is	O
still	O
a	O
challenging	O
research	O
topic	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
an	O
effective	O
face	Task
detector	O
based	O
on	O
the	O
one	O
-	O
shot	O
detection	Task
pipeline	O
called	O
Face	Method
Attention	Method
Network	Method
(	O
FAN	Method
)	O
,	O
which	O
can	O
well	O
address	O
the	O
occlusion	Metric
and	Metric
false	Metric
positive	Metric
issue	Metric
.	O

More	O
specifically	O
,	O
following	O
the	O
similar	O
setting	O
as	O
RetinaNet	O
,	O
we	O
utilize	O
feature	Method
pyramid	Method
network	Method
,	O
and	O
different	O
layers	O
from	O
the	O
network	O
to	O
solve	O
the	O
faces	O
with	O
different	O
scales	O
.	O

Our	O
anchor	Method
setting	Method
is	O
designed	O
specifically	O
for	O
the	O
face	Task
application	O
and	O
an	O
anchor	O
-	O
level	O
attention	O
is	O
introduced	O
which	O
provides	O
different	O
attention	O
regions	O
for	O
different	O
feature	O
layers	O
.	O

The	O
attention	Method
is	O
supervised	O
trained	O
based	O
on	O
the	O
anchor	O
-	O
specific	O
heatmaps	O
.	O

In	O
addition	O
,	O
data	Task
augmentation	Task
like	O
random	Method
crop	Method
is	O
introduced	O
to	O
generate	O
more	O
cropped	O
(	O
occluded	O
)	O
training	O
samples	O
.	O

In	O
summary	O
,	O
there	O
are	O
three	O
contributions	O
in	O
our	O
paper	O
.	O

We	O
propose	O
a	O
anchor	Method
-	Method
level	Method
attention	Method
,	O
which	O
can	O
well	O
address	O
the	O
occlusion	Task
issue	Task
in	O
the	O
face	Task
detection	Task
task	Task
.	O

One	O
illustrative	O
example	O
for	O
our	O
detection	Task
results	O
in	O
the	O
crowd	Task
case	Task
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
practical	O
baseline	O
setting	O
is	O
introduced	O
based	O
on	O
the	O
one	Method
-	Method
shot	Method
RetinaNet	Method
detector	Method
,	O
which	O
obtains	O
comparable	O
performance	O
with	O
fast	Metric
computation	Metric
speed	Metric
.	O

Our	O
FAN	Method
which	O
integrates	O
our	O
reproduced	O
one	Method
-	Method
shot	Method
RetinaNet	Method
and	Method
anchor	Method
-	Method
level	Method
attention	Method
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	O
on	O
the	O
popular	O
face	Task
detection	Task
benchmarks	Task
including	O
WiderFace	Material
and	O
MAFA	Material
,	O
especially	O
in	O
the	O
occluded	O
cases	O
like	O
MAFA	Material
.	O

section	O
:	O
Related	O
Work	O
Face	Task
detection	Task
as	O
the	O
fundamental	O
problem	O
of	O
computer	Task
vision	Task
,	O
has	O
been	O
extensively	O
studied	O
.	O

Prior	O
to	O
the	O
renaissance	O
of	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
,	O
numerous	O
of	O
machine	Method
learning	Method
algorithms	Method
are	O
applied	O
to	O
face	Task
detection	Task
.	O

The	O
pioneering	O
work	O
of	O
Viola	Method
-	Method
Jones	Method
utilizes	O
Adaboost	Method
with	O
Haar	Method
-	Method
like	Method
feature	Method
to	O
train	O
a	O
cascade	Method
model	Method
to	O
detect	O
face	Task
and	O
get	O
a	O
real	O
-	O
time	O
performance	O
.	O

Also	O
,	O
deformable	Method
part	Method
models	Method
(	O
DPM	Method
)	Method
is	O
employed	O
for	O
face	Task
detection	Task
with	O
remarkable	O
performance	O
.	O

However	O
,	O
A	O
limitation	O
of	O
these	O
methods	O
is	O
that	O
their	O
use	O
of	O
weak	O
features	O
,	O
e.g.	O
,	O
HOG	O
or	O
Haar	O
-	O
like	O
features	O
.	O

Recently	O
,	O
deep	Method
learning	Method
based	Method
algorithm	Method
is	O
utilized	O
to	O
improve	O
both	O
the	O
feature	Method
representation	Method
and	O
classification	Task
performance	O
.	O

Based	O
on	O
the	O
whether	O
following	O
the	O
proposal	Method
and	Method
refine	Method
strategy	Method
,	O
these	O
methods	O
can	O
be	O
divided	O
into	O
:	O
single	Method
-	Method
stage	Method
detector	Method
,	O
such	O
as	O
YOLO	Method
,	O
SSD	Method
,	O
RetinaNet	Method
,	O
and	O
two	Method
-	Method
stage	Method
detector	Method
such	O
as	O
Faster	Method
R	Method
-	Method
CNN	Method
.	O

Single	Method
-	Method
stage	Method
method	Method
.	O

CascadeCNN	Method
proposes	O
a	O
cascade	Method
structure	Method
to	O
detect	O
face	Task
coarse	O
to	O
fine	O
.	O

MT	O
-	O
CNN	Method
develops	O
an	O
architecture	O
to	O
address	O
both	O
the	O
detection	Task
and	O
landmark	O
alignment	O
jointly	O
.	O

Later	O
,	O
DenseBox	Method
utilizes	O
a	O
unified	O
end	Method
-	Method
to	Method
-	Method
end	Method
fully	Method
convolutional	Method
network	Method
to	O
detect	O
confidence	O
and	O
bounding	O
box	O
directly	O
.	O

UnitBox	Method
presents	O
a	O
new	O
intersection	Metric
-	Metric
over	Metric
-	Metric
union	Metric
(	O
IoU	Metric
)	O
loss	O
to	O
directly	O
optimize	O
IOU	O
target	O
.	O

SAFD	Method
and	O
RSA	Method
unit	Method
focus	O
on	O
handling	O
scale	Task
explicitly	Task
using	O
CNN	Method
or	O
RNN	Method
.	O

RetinaNet	Method
introduces	O
a	O
new	O
focal	O
loss	O
to	O
relieve	O
the	O
class	Task
imbalance	Task
problem	Task
.	O

Two	O
-	O
stage	O
method	O
.	O

Beside	O
,	O
face	Task
detection	Task
has	O
inherited	O
some	O
achievements	O
from	O
generic	O
object	O
detection	Task
tasks	O
.	O

use	O
the	O
Faster	O
R	O
-	O
CNN	Method
framework	O
to	O
improve	O
the	O
face	Task
detection	Task
performance	O
.	O

CMS	Method
-	Method
RCNN	Method
enhances	O
Faster	O
R	O
-	O
CNN	Method
architecture	O
by	O
adding	O
body	O
context	O
information	O
.	O

Convnet	Method
joins	O
Faster	O
R	O
-	O
CNN	Method
framework	O
with	O
3D	O
face	Task
model	O
to	O
increase	O
occlusion	Metric
robustness	Metric
.	O

Additionally	O
,	O
Spatial	Method
Transformer	Method
Networks	Method
(	O
STN	Method
)	O
and	O
it	O
’s	O
variant	O
,	O
OHEM	Method
,	O
grid	Method
loss	Method
also	O
presents	O
several	O
effective	O
strategies	O
to	O
improve	O
face	Task
detection	Task
performance	O
.	O

section	O
:	O
Face	Method
Attention	Method
Network	Method
(	O
FAN	Method
)	O
Although	O
remarkable	O
improvement	O
have	O
been	O
achieved	O
for	O
the	O
face	Task
detection	Task
problem	Task
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
challenge	O
of	O
locating	Task
faces	Task
with	Task
large	Task
occlusion	Task
remains	O
.	O

Meanwhile	O
,	O
in	O
many	O
face	Task
-	O
related	O
applications	O
such	O
security	Task
surveillance	Task
,	O
faces	O
usually	O
appear	O
with	O
large	O
occlusion	O
by	O
mask	O
,	O
sunglasses	O
or	O
other	O
faces	O
,	O
which	O
need	O
to	O
be	O
detected	O
as	O
well	O
.	O

addresses	O
this	O
problem	O
by	O
merging	O
information	O
from	O
different	O
feature	O
layers	O
.	O

changes	O
anchor	Method
matching	Method
strategy	Method
in	O
order	O
to	O
increase	O
recall	Metric
rate	Metric
.	O

Inspire	O
of	O
,	O
we	O
are	O
trying	O
to	O
leverage	O
a	O
fully	Method
convolutional	Method
feature	Method
hierarchy	Method
,	O
and	O
each	O
layer	O
targets	O
to	O
handle	O
faces	O
with	O
different	O
scale	O
range	O
by	O
assigning	O
different	O
anchors	O
.	O

Our	O
algorithm	O
,	O
called	O
Face	Method
Attention	Method
Network	Method
(	O
FAN	Method
)	O
,	O
can	O
be	O
considered	O
as	O
an	O
integration	O
of	O
a	O
single	Method
-	Method
stage	Method
detector	Method
discussed	O
in	O
Section	O
[	O
reference	O
]	O
and	O
our	O
anchor	Method
-	Method
level	Method
attention	Method
in	O
Section	O
[	O
reference	O
]	O
.	O

An	O
overview	O
of	O
our	O
network	O
structure	O
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Base	O
Framework	O
Convolutional	Method
neural	Method
network	Method
has	O
different	O
semantic	O
information	O
and	O
spatial	O
resolution	O
at	O
different	O
feature	O
layers	O
.	O

The	O
shallow	O
layers	O
usually	O
have	O
high	O
spatial	O
resolution	O
,	O
which	O
is	O
good	O
for	O
spatial	Task
localization	Task
of	Task
small	Task
objects	Task
,	O
but	O
low	O
semantic	O
information	O
,	O
which	O
is	O
not	O
good	O
for	O
visual	Task
classification	Task
.	O

On	O
the	O
other	O
hand	O
,	O
deep	Method
layers	Method
obtain	O
more	O
semantic	O
information	O
but	O
the	O
spatial	O
resolution	O
is	O
compromised	O
.	O

Recent	O
work	O
like	O
Feature	Method
Pyramid	Method
Network	Method
(	O
FPN	Method
)	O
proposes	O
a	O
divide	Method
and	Method
conquer	Method
principle	Method
.	O

A	O
U	O
-	O
shape	O
structure	O
is	O
attached	O
to	O
maintain	O
both	O
the	O
high	O
spatial	O
resolution	O
and	O
semantic	O
information	O
.	O

Different	O
scales	O
of	O
objects	O
are	O
split	O
and	O
addressed	O
at	O
different	O
feature	O
layers	O
.	O

Following	O
the	O
design	O
principle	O
,	O
RetinaNet	Method
introduced	O
in	O
is	O
a	O
one	Method
-	Method
stage	Method
detector	Method
which	O
achieves	O
state	O
-	O
of	O
-	O
art	O
performance	O
on	O
COCO	O
general	O
object	O
detection	Task
.	O

It	O
employs	O
FPN	Method
with	O
ResNet	Method
as	O
backbone	O
to	O
generate	O
a	O
hierarchy	O
of	O
feature	O
pyramids	O
with	O
rich	O
semantic	O
information	O
.	O

Based	O
on	O
this	O
backbone	O
,	O
RetinaNet	Method
is	O
attached	O
with	O
two	O
subnets	O
:	O
one	O
for	O
classifying	Task
and	O
the	O
other	O
for	O
regressing	Task
.	O

We	O
borrow	O
the	O
main	O
network	Method
structure	Method
from	O
RetinaNet	Method
and	O
adapt	O
it	O
for	O
the	O
face	Task
detection	Task
task	Task
.	O

The	O
classification	Method
subnet	Method
applies	O
four	O
convolution	Method
layers	Method
each	O
with	O
256	O
filters	O
,	O
followed	O
by	O
a	O
convolution	Method
layer	Method
with	O
filters	O
where	O
means	O
the	O
number	O
of	O
classes	O
and	O
means	O
the	O
number	O
of	O
anchors	O
per	O
location	O
.	O

For	O
face	Task
detection	Task
since	O
we	O
use	O
sigmoid	Method
activation	Method
,	O
and	O
we	O
use	O
in	O
most	O
experiments	O
.	O

All	O
convolution	Method
layers	Method
in	O
this	O
subnet	O
share	O
parameters	O
across	O
all	O
pyramid	O
levels	O
just	O
like	O
the	O
original	O
RetinaNet	Method
.	O

The	O
regression	Method
subnet	Method
is	O
identical	O
to	O
the	O
classification	Method
subnet	Method
except	O
that	O
it	O
terminates	O
in	O
convolution	Method
filters	Method
with	O
linear	Method
activation	Method
.	O

Figure	O
[	O
reference	O
]	O
provides	O
an	O
overview	O
for	O
our	O
algorithm	O
.	O

Notice	O
,	O
we	O
only	O
draw	O
three	O
levels	O
pyramids	O
for	O
illustrative	O
purpose	O
.	O

subsection	O
:	O
Attention	Method
Network	Method
Compared	O
with	O
the	O
original	O
RetinaNet	Method
,	O
we	O
have	O
designed	O
our	O
anchor	O
setting	O
together	O
with	O
our	O
attention	O
function	O
.	O

There	O
are	O
three	O
design	O
principles	O
:	O
addressing	O
different	O
scales	O
of	O
the	O
faces	O
in	O
different	O
feature	O
layers	O
,	O
highlighting	O
the	O
features	O
from	O
the	O
face	Task
region	O
and	O
diminish	O
the	O
regions	O
without	O
face	Task
,	O
generating	O
more	O
occluded	O
faces	O
for	O
training	O
.	O

subsubsection	O
:	O
Anchor	Method
Assign	Method
Strategy	Method
We	O
start	O
the	O
discussion	O
of	O
our	O
anchor	O
setting	O
first	O
.	O

In	O
our	O
FAN	Method
,	O
we	O
have	O
five	O
detector	Method
layers	Method
each	O
associated	O
with	O
a	O
specific	O
scale	O
anchor	O
.	O

In	O
addition	O
,	O
the	O
aspect	O
ratio	O
for	O
our	O
anchor	O
is	O
set	O
as	O
1	O
and	O
1.5	O
,	O
because	O
most	O
of	O
frontal	O
faces	O
are	O
approximately	O
square	O
and	O
profile	O
faces	O
can	O
be	O
considered	O
as	O
a	O
1:1.5	O
rectangle	O
.	O

Besides	O
,	O
we	O
calculate	O
the	O
statistics	O
from	O
the	O
WiderFace	Material
train	O
set	O
based	O
on	O
the	O
ground	O
-	O
truth	O
face	Task
size	O
.	O

As	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
more	O
than	O
80	O
%	O
faces	O
have	O
an	O
object	O
scale	O
from	O
16	O
to	O
406	O
pixel	O
.	O

Faces	O
with	O
small	O
size	O
lack	O
sufficient	O
resolution	O
and	O
therefore	O
it	O
may	O
not	O
be	O
a	O
good	O
choice	O
to	O
include	O
in	O
the	O
training	O
data	O
.	O

Thus	O
,	O
we	O
set	O
our	O
anchors	O
from	O
areas	O
of	O
to	O
on	O
pyramid	O
levels	O
.	O

We	O
set	O
anchor	O
scale	O
step	O
to	O
,	O
which	O
ensure	O
every	O
ground	O
-	O
truth	O
boxes	O
have	O
anchor	O
with	O
IoU	Metric
.	O

Specifically	O
,	O
anchors	O
are	O
assigned	O
to	O
a	O
ground	O
-	O
truth	O
box	O
with	O
the	O
highest	O
IoU	Metric
larger	O
than	O
,	O
and	O
to	O
background	O
if	O
the	O
highest	O
IoU	Metric
is	O
less	O
than	O
.	O

Unassigned	O
anchors	O
are	O
ignored	O
during	O
training	O
.	O

subsubsection	O
:	O
Attention	Method
Function	Method
To	O
address	O
the	O
occlusion	Task
issue	Task
,	O
we	O
propose	O
a	O
novel	O
anchor	Method
-	Method
level	Method
attention	Method
based	O
on	O
the	O
network	O
structure	O
mentioned	O
above	O
.	O

Specifically	O
,	O
we	O
utilize	O
a	O
segment	O
-	O
like	O
side	O
branch	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
attention	O
supervision	O
information	O
is	O
obtained	O
by	O
filling	O
the	O
ground	O
-	O
truth	O
box	O
.	O

Meanwhile	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
supervised	O
heatmaps	O
are	O
associated	O
to	O
the	O
ground	O
-	O
truth	O
faces	O
assigned	O
to	O
the	O
anchors	O
in	O
the	O
current	O
layer	O
.	O

These	O
hierarchical	O
attention	O
maps	O
could	O
decrease	O
the	O
correlation	O
among	O
them	O
.	O

Different	O
from	O
traditional	O
usage	O
of	O
attention	Method
map	Method
,	O
which	O
naively	O
multiple	O
it	O
with	O
the	O
feature	O
maps	O
,	O
our	O
attention	Method
maps	Method
are	O
first	O
feed	O
to	O
an	O
exponential	Method
operation	Method
and	O
then	O
dot	O
with	O
feature	Method
maps	Method
.	O

It	O
is	O
able	O
to	O
keep	O
more	O
context	O
information	O
,	O
and	O
meanwhile	O
highlight	O
the	O
detection	Task
information	O
.	O

Considering	O
the	O
example	O
with	O
occluded	O
face	Task
,	O
most	O
invisible	O
parts	O
are	O
not	O
useful	O
and	O
may	O
be	O
harmful	O
for	O
detection	Task
.	O

Some	O
of	O
the	O
attention	O
results	O
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Our	O
attention	O
mask	O
can	O
enhance	O
the	O
feature	O
maps	O
in	O
the	O
facial	O
area	O
,	O
and	O
diminish	O
the	O
opposition	O
.	O

subsubsection	O
:	O
Data	Task
Augmentation	Task
We	O
find	O
that	O
the	O
number	O
of	O
occluded	O
faces	O
in	O
the	O
training	O
dataset	O
,	O
e.g.	O
,	O
WiderFace	Material
train	O
,	O
is	O
limited	O
and	O
can	O
not	O
satisfy	O
the	O
training	O
of	O
CNN	Method
network	O
.	O

Only	O
16	O
%	O
faces	O
are	O
with	O
highly	O
occlusion	O
property	O
from	O
the	O
annotation	O
.	O

Thus	O
,	O
we	O
propose	O
a	O
random	Method
crop	Method
strategy	Method
,	O
which	O
can	O
generate	O
a	O
large	O
number	O
of	O
occluded	O
faces	O
for	O
training	O
.	O

More	O
specifically	O
,	O
based	O
on	O
the	O
training	O
set	O
,	O
we	O
randomly	O
crop	O
square	O
patches	O
from	O
original	O
images	O
,	O
whose	O
range	O
between	O
[	O
0.3	O
,	O
1	O
]	O
of	O
the	O
short	O
edge	O
from	O
the	O
original	O
images	O
.	O

In	O
addition	O
,	O
We	O
keep	O
the	O
overlapped	O
part	O
of	O
the	O
ground	O
-	O
truth	O
box	O
if	O
its	O
center	O
is	O
in	O
the	O
sampled	O
patch	O
.	O

Besides	O
from	O
the	O
random	Task
crop	Task
dataset	Task
augmentation	Task
,	O
we	O
also	O
employ	O
augmentation	Method
from	O
random	O
flip	O
and	O
color	O
jitter	O
.	O

subsection	O
:	O
Loss	Method
function	Method
We	O
employ	O
a	O
multi	Method
-	Method
task	Method
loss	Method
function	Method
to	O
jointly	O
optimize	O
model	O
parameters	O
:	O
where	O
k	O
is	O
the	O
index	O
of	O
an	O
feature	O
pyramid	O
level	O
(	O
)	O
,	O
and	O
represents	O
the	O
set	O
of	O
anchors	O
defined	O
in	O
pyramid	O
level	O
.	O

The	O
ground	O
-	O
truth	O
label	O
is	O
1	O
if	O
the	O
anchor	O
is	O
positive	O
,	O
0	O
otherwise	O
.	O

is	O
the	O
predicted	O
classification	O
result	O
from	O
our	O
model	O
.	O

is	O
a	O
vector	O
representing	O
the	O
4	O
parameterized	O
coordinates	O
of	O
the	O
predicted	O
bounding	O
box	O
,	O
and	O
is	O
that	O
of	O
the	O
ground	O
-	O
truth	O
box	O
associated	O
with	O
a	O
positive	O
anchor	O
.	O

The	O
classification	Metric
loss	Metric
is	O
focal	O
loss	O
introduced	O
in	O
over	O
two	O
classes	O
(	O
face	Task
and	O
background	O
)	O
.	O

is	O
the	O
number	O
of	O
anchors	O
in	O
which	O
participate	O
in	O
the	O
classification	Task
loss	Task
computation	Task
.	O

The	O
regression	Method
loss	Method
is	O
smooth	Method
L1	Method
loss	Method
defined	O
in	O
.	O

is	O
the	O
indicator	O
function	O
that	O
limits	O
the	O
regression	O
loss	O
only	O
focusing	O
on	O
the	O
positively	O
assigned	O
anchors	O
,	O
and	O
.	O

The	O
attention	O
loss	O
is	O
pixel	Method
-	Method
wise	Method
sigmoid	Method
cross	Method
entropy	Method
.	O

is	O
the	O
attention	O
map	O
generated	O
per	O
level	O
,	O
and	O
is	O
the	O
ground	O
-	O
truth	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

and	O
are	O
used	O
to	O
balance	O
these	O
three	O
loss	O
terms	O
,	O
here	O
we	O
simply	O
set	O
.	O

section	O
:	O
Experiments	O
We	O
use	O
ResNet	Method
-	Method
50	Method
as	O
base	O
model	O
.	O

All	O
models	O
are	O
trained	O
by	O
SGD	Method
over	O
8	O
GPUs	Method
with	O
a	O
total	O
of	O
32	O
images	O
per	O
mini	O
-	O
batch	O
(	O
4	O
images	O
per	O
GPU	O
)	O
.	O

Similar	O
to	O
,	O
the	O
four	O
convolution	Method
layers	Method
attached	O
to	O
FPN	Method
are	O
initialized	O
with	O
bias	O
and	O
Gaussian	O
weight	O
variance	O
.	O

For	O
the	O
final	O
convolution	Method
layer	Method
of	O
the	O
classification	Method
subnet	Method
,	O
we	O
initiate	O
them	O
with	O
bias	O
and	O
here	O
.	O

Meanwhile	O
,	O
the	O
initial	O
learning	Metric
rate	Metric
is	O
set	O
as	O
3e	O
-	O
3	O
.	O

We	O
sample	O
10k	O
image	O
patches	O
per	O
epoch	O
.	O

Models	O
without	O
data	Method
augmentation	Method
are	O
trained	O
for	O
30	O
epochs	O
.	O

With	O
data	Method
augmentation	Method
,	O
models	O
are	O
trained	O
for	O
130	O
epochs	O
,	O
whose	O
learning	Metric
rate	Metric
is	O
dropped	O
by	O
10	O
at	O
100	O
epochs	O
and	O
again	O
at	O
120	O
epochs	O
.	O

Weight	O
decay	O
is	O
1e	O
-	O
5	O
and	O
momentum	O
is	O
0.9	O
.	O

Anchors	O
with	O
IoU	Metric
are	O
assigned	O
to	O
positive	O
class	O
and	O
anchors	O
which	O
have	O
an	O
IoU	Metric
with	O
all	O
ground	O
-	O
truth	O
are	O
assigned	O
to	O
the	O
background	O
class	O
.	O

subsection	O
:	O
Datasets	O
The	O
performance	O
of	O
FAN	Method
is	O
evaluated	O
across	O
multiple	O
face	Task
datasets	O
:	O
WiderFace	Material
and	O
MAFA	Material
.	O

WiderFace	Material
dataset	O
[	O
]	O
:	O
WiderFace	Material
dataset	O
contains	O
32	O
,	O
203	O
images	O
and	O
393	O
,	O
703	O
annotated	O
faces	O
with	O
a	O
high	O
degree	O
of	O
variability	O
in	O
scale	O
,	O
pose	O
and	O
occlusion	O
.	O

158	O
,	O
989	O
of	O
these	O
are	O
chosen	O
as	O
train	O
set	O
,	O
39	O
,	O
496	O
are	O
in	O
validation	O
set	O
and	O
the	O
rest	O
are	O
test	O
set	O
.	O

The	O
validation	O
set	O
and	O
test	O
set	O
are	O
split	O
into	O
’	O
easy	O
’	O
,	O
’	O
medium	O
’	O
,	O
’	O
hard	O
’	O
subsets	O
,	O
in	O
terms	O
of	O
the	O
difficulties	O
of	O
the	O
detection	Task
.	O

Due	O
to	O
the	O
variability	O
of	O
scale	O
,	O
pose	O
and	O
occlusion	O
,	O
WiderFace	Material
dataset	O
is	O
one	O
of	O
the	O
most	O
challenge	O
face	Task
datasets	O
.	O

Our	O
FAN	Method
is	O
trained	O
only	O
on	O
the	O
train	O
set	O
and	O
evaluate	O
on	O
both	O
validation	Metric
set	Metric
and	O
test	O
set	O
.	O

Ablation	O
studies	O
are	O
performed	O
on	O
the	O
validation	O
set	O
.	O

MAFA	Material
dataset	O
[	O
]	O
:	O
MAFA	Material
dataset	O
contains	O
30	O
,	O
811	O
images	O
with	O
35	O
,	O
806	O
masked	O
faces	O
collected	O
from	O
Internet	O
.	O

It	O
is	O
a	O
face	Task
detection	Task
benchmark	Task
for	O
masked	O
face	Task
,	O
in	O
which	O
faces	O
have	O
vast	O
various	O
orientations	O
and	O
occlusion	O
.	O

Beside	O
,	O
this	O
dataset	O
is	O
divided	O
into	O
masked	O
face	Task
subset	O
and	O
unmasked	O
face	Task
subset	O
according	O
to	O
whether	O
at	O
least	O
one	O
part	O
of	O
each	O
face	Task
is	O
occluded	O
by	O
mask	O
.	O

We	O
use	O
both	O
the	O
whole	O
dataset	O
and	O
occluded	O
subset	O
to	O
evaluate	O
our	O
method	O
.	O

subsubsection	O
:	O
Anchor	Task
setting	Task
and	O
assign	O
We	O
compare	O
three	O
anchor	O
settings	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
the	O
RetinaNet	Task
setting	Task
,	O
we	O
follow	O
the	O
setting	O
described	O
in	O
the	O
paper	O
.	O

For	O
our	O
FAN	Method
baseline	O
,	O
we	O
set	O
our	O
anchors	O
from	O
areas	O
of	O
to	O
on	O
pyramid	O
levels	O
.	O

In	O
addition	O
,	O
the	O
aspect	O
ratio	O
is	O
set	O
to	O
1	O
and	O
1.5	O
.	O

Also	O
,	O
inspire	O
of	O
,	O
we	O
choose	O
an	O
anchor	O
assign	O
rule	O
with	O
more	O
cover	Metric
rate	Metric
.	O

We	O
uses	O
8	O
anchors	O
per	O
location	O
spanning	O
4	O
scales	O
(	O
intervals	O
are	O
still	O
fixed	O
to	O
so	O
that	O
areas	O
from	O
to	O
)	O
and	O
2	O
ratios	O
1	O
,	O
1.5	O
.	O

For	O
the	O
dense	Task
setting	Task
,	O
it	O
is	O
the	O
same	O
as	O
our	O
FAN	Method
setting	O
except	O
we	O
apply	O
more	O
dense	O
scales	O
from	O
to	O
.	O

Based	O
on	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
anchor	O
scale	O
is	O
important	O
to	O
detector	Task
performance	O
and	O
we	O
can	O
see	O
that	O
our	O
setting	O
is	O
obviously	O
superior	O
to	O
the	O
setting	O
in	O
.	O

Compared	O
with	O
the	O
dense	O
setting	O
,	O
we	O
can	O
see	O
that	O
anchor	Metric
cover	Metric
rate	Metric
is	O
not	O
equal	O
to	O
the	O
final	O
detection	Task
performance	O
as	O
it	O
may	O
introduce	O
a	O
lot	O
of	O
negative	O
windows	O
due	O
to	O
the	O
dense	Method
sampling	Method
.	O

subsubsection	O
:	O
Attention	Method
mechanism	Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
apply	O
anchor	Method
-	Method
level	Method
attention	Method
mechanism	Method
to	O
enhance	O
the	O
facial	O
parts	O
.	O

We	O
compare	O
our	O
FAN	Method
baseline	O
with	O
and	O
without	O
attention	O
in	O
Table	O
[	O
reference	O
]	O
for	O
the	O
WiderFace	Material
val	O
dataset	O
.	O

For	O
the	O
MAFA	Material
dataset?the	O
results	O
can	O
be	O
found	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Based	O
on	O
the	O
experimental	O
results	O
,	O
we	O
can	O
find	O
that	O
our	O
attention	O
can	O
improve	O
1.1	O
%	O
in	O
WiderFace	Material
hard	Material
subset	Material
and	O
2	O
%	O
in	O
MAFA	Material
masked	O
subset	O
.	O

subsubsection	O
:	O
Data	Task
augmentation	Task
According	O
to	O
the	O
statistics	O
from	O
the	O
WiderFace	Material
dataset	O
,	O
there	O
are	O
around	O
26	O
%	O
of	O
faces	O
with	O
occlusion	O
.	O

Among	O
them	O
,	O
around	O
16	O
%	O
is	O
of	O
serious	O
occlusion	O
.	O

As	O
we	O
are	O
targeting	O
to	O
solve	O
the	O
occluded	O
faces	O
,	O
the	O
number	O
of	O
training	O
samples	O
with	O
occlusion	O
may	O
not	O
be	O
sufficient	O
.	O

Thus	O
,	O
we	O
employ	O
the	O
the	O
random	Method
crop	Method
data	Method
augmentation	Method
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
results	O
can	O
be	O
found	O
from	O
Table	O
[	O
reference	O
]	O
.	O

The	O
performance	O
improvement	O
is	O
significant	O
.	O

Besides	O
from	O
the	O
benefits	O
for	O
the	O
occluded	O
face	Task
,	O
our	O
random	Method
crop	Method
augmentation	Method
potentially	O
improve	O
the	O
performance	O
of	O
small	O
faces	O
as	O
more	O
small	O
faces	O
will	O
be	O
enlarged	O
after	O
augmentation	O
.	O

subsubsection	O
:	O
WiderFace	Material
Dataset	O
We	O
compare	O
our	O
FAN	Method
with	O
the	O
state	O
-	O
of	O
-	O
art	O
detectors	Method
like	O
SFD	Method
,	O
SSH	Method
,	O
HR	Method
and	O
ScaleFace	Method
.	O

Our	O
FAN	Method
is	O
trained	O
on	O
WiderFace	Material
train	O
set	O
with	O
data	O
augmentation	O
and	O
tested	O
on	O
both	O
validation	O
and	O
test	O
set	O
with	O
multi	O
-	O
scale	O
600	O
,	O
800	O
,	O
1000	O
,	O
1200	O
,	O
1400	O
.	O

The	O
precision	Metric
-	Metric
recall	Metric
curves	Metric
and	O
AP	Method
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
algorithm	O
obtains	O
the	O
best	O
result	O
in	O
all	O
subsets	O
,	O
i.e.	O
0.953	O
(	O
Easy	O
)	O
,	O
0.942	O
(	O
Medium	O
)	O
and	O
0.888	O
(	O
Hard	O
)	O
for	O
validation	O
set	O
,	O
and	O
0.946	O
(	O
Easy	O
)	O
,	O
0.936	O
(	O
Medium	O
)	O
and	O
0.885	O
(	O
Hard	O
)	O
for	O
test	O
set	O
.	O

Considering	O
the	O
hard	O
subset	O
which	O
contains	O
a	O
lot	O
of	O
occluded	O
faces	O
,	O
we	O
have	O
larger	O
margin	O
compared	O
with	O
the	O
previous	O
state	O
-	O
art	O
-	O
results	O
,	O
which	O
validates	O
the	O
effectiveness	O
of	O
our	O
algorithm	O
for	O
the	O
occluded	O
faces	O
.	O

Example	O
results	O
from	O
our	O
FAN	Method
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
MAFA	Material
Dataset	O
As	O
MAFA	Material
dataset	O
is	O
specifically	O
designed	O
for	O
the	O
occluded	Task
face	Task
detection	Task
,	O
it	O
is	O
adopted	O
to	O
evaluate	O
our	O
algorithm	O
.	O

We	O
compare	O
our	O
FAN	Method
with	O
LLE	Method
-	Method
CNNs	Method
and	O
AOFD	Method
.	O

Our	O
FAN	Method
is	O
trained	O
on	O
WiderFace	Material
train	O
set	O
with	O
data	O
augmentation	O
and	O
tested	O
on	O
MAFA	Material
test	O
set	O
with	O
scale	O
400	O
,	O
600	O
,	O
800	O
,	O
1000	O
.	O

The	O
results	O
based	O
on	O
average	Metric
precision	Metric
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

FAN	Method
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	O
on	O
MAFA	Material
test	O
set	O
with	O
standard	O
testing	Metric
(	O
IoU	Metric
threshold	O
=	O
0.5	O
)	O
,	O
which	O
shows	O
the	O
promising	O
performance	O
on	O
occluded	O
faces	O
.	O

Example	O
results	O
from	O
our	O
FAN	Method
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Inference	Metric
Time	Metric
Despite	O
great	O
performance	O
obtained	O
by	O
our	O
FAN	Method
,	O
the	O
speed	O
of	O
our	O
algorithm	O
is	O
not	O
compromised	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
our	O
FAN	Method
detector	O
can	O
not	O
only	O
obtain	O
the	O
state	O
-	O
of	O
-	O
art	O
results	O
but	O
also	O
possess	O
efficient	O
computational	Metric
speed	Metric
.	O

The	O
computational	Metric
cost	Metric
is	O
tested	O
on	O
a	O
NIVIDIA	Material
TITAN	Material
Xp	Material
.	O

The	O
min	O
size	O
means	O
the	O
shortest	O
side	O
of	O
the	O
images	O
which	O
are	O
resized	O
to	O
by	O
keeping	O
the	O
aspect	O
ratio	O
.	O

Compared	O
with	O
the	O
baseline	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
when	O
testing	O
with	O
short	O
-	O
side	O
1000	O
,	O
our	O
FAN	Method
already	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	Method
like	O
,	O
and	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
are	O
targeting	O
the	O
problem	O
of	O
face	Task
detection	Task
with	O
occluded	O
faces	O
.	O

We	O
propose	O
FAN	Method
detector	O
which	O
can	O
integrate	O
our	O
specifically	O
designed	O
single	Method
-	Method
stage	Method
base	Method
net	Method
and	O
our	O
anchor	Method
-	Method
level	Method
attention	Method
algorithm	Method
.	O

Based	O
on	O
our	O
anchor	O
-	O
level	O
attention	O
,	O
we	O
can	O
highlight	O
the	O
features	O
from	O
the	O
facial	O
regions	O
and	O
successfully	O
relieving	O
the	O
risk	O
from	O
the	O
false	O
positives	O
.	O

Experimental	O
results	O
on	O
challenging	O
benchmarks	O
like	O
WiderFace	Material
and	O
MAFA	Material
validate	O
the	O
effectiveness	O
and	O
efficiency	O
of	O
our	O
proposed	O
algorithm	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Scaling	Method
Memory	Method
-	Method
Augmented	Method
Neural	Method
Networks	Method
with	O
Sparse	Task
Reads	Task
and	Task
Writes	Task
Neural	Method
networks	Method
augmented	O
with	O
external	O
memory	O
have	O
the	O
ability	O
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
complex	O
tasks	O
.	O

These	O
models	O
appear	O
promising	O
for	O
applications	O
such	O
as	O
language	Task
modeling	Task
and	O
machine	Task
translation	Task
.	O

However	O
,	O
they	O
scale	O
poorly	O
in	O
both	O
space	Metric
and	O
time	Metric
as	O
the	O
amount	O
of	O
memory	O
grows	O
—	O
limiting	O
their	O
applicability	O
to	O
real	Task
-	Task
world	Task
domains	Task
.	O

Here	O
,	O
we	O
present	O
an	O
end	O
-	O
to	O
-	O
end	Method
differentiable	Method
memory	Method
access	Method
scheme	Method
,	O
which	O
we	O
call	O
Sparse	Method
Access	Method
Memory	Method
(	O
SAM	Method
)	O
,	O
that	O
retains	O
the	O
representational	O
power	O
of	O
the	O
original	O
approaches	O
whilst	O
training	O
efficiently	O
with	O
very	O
large	O
memories	O
.	O

We	O
show	O
that	O
SAM	Method
achieves	O
asymptotic	Metric
lower	Metric
bounds	Metric
in	O
space	Metric
and	O
time	Metric
complexity	O
,	O
and	O
find	O
that	O
an	O
implementation	O
runs	O
faster	O
and	O
with	O
less	O
physical	O
memory	O
than	O
non	Method
-	Method
sparse	Method
models	Method
.	O

SAM	Method
learns	O
with	O
comparable	O
data	Metric
efficiency	Metric
to	O
existing	O
models	O
on	O
a	O
range	O
of	O
synthetic	Task
tasks	Task
and	O
one	Task
-	Task
shot	Task
Omniglot	Task
character	Task
recognition	Task
,	O
and	O
can	O
scale	O
to	O
tasks	O
requiring	O
s	O
of	O
time	Metric
steps	O
and	O
memories	O
.	O

As	O
well	O
,	O
we	O
show	O
how	O
our	O
approach	O
can	O
be	O
adapted	O
for	O
models	O
that	O
maintain	O
temporal	O
associations	O
between	O
memories	O
,	O
as	O
with	O
the	O
recently	O
introduced	O
Differentiable	Method
Neural	Method
Computer	Method
.	O

capbtabboxtable	O
[	O
]	O
[	O
]	O
section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
,	O
such	O
as	O
the	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
,	O
have	O
proven	O
to	O
be	O
powerful	O
sequence	Method
learning	Method
models	Method
.	O

However	O
,	O
one	O
limitation	O
of	O
the	O
LSTM	Method
architecture	Method
is	O
that	O
the	O
number	O
of	O
parameters	O
grows	O
proportionally	O
to	O
the	O
square	O
of	O
the	O
size	O
of	O
the	O
memory	O
,	O
making	O
them	O
unsuitable	O
for	O
problems	O
requiring	O
large	O
amounts	O
of	O
long	O
-	O
term	O
memory	O
.	O

Recent	O
approaches	O
,	O
such	O
as	O
Neural	Method
Turing	Method
Machines	Method
(	O
NTMs	Method
)	O
and	O
Memory	Method
Networks	Method
,	O
have	O
addressed	O
this	O
issue	O
by	O
decoupling	O
the	O
memory	O
capacity	O
from	O
the	O
number	O
of	O
model	O
parameters	O
.	O

We	O
refer	O
to	O
this	O
class	O
of	O
models	O
as	O
memory	Method
augmented	Method
neural	Method
networks	Method
(	O
MANNs	Method
)	O
.	O

External	O
memory	O
allows	O
MANNs	Method
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
problems	O
that	O
have	O
eluded	O
the	O
capabilities	O
of	O
traditional	O
LSTMs	Method
,	O
and	O
to	O
generalize	O
to	O
longer	O
sequence	O
lengths	O
.	O

Nonetheless	O
,	O
MANNs	Method
have	O
had	O
limited	O
success	O
in	O
real	Task
world	Task
application	Task
.	O

A	O
significant	O
difficulty	O
in	O
training	O
these	O
models	O
results	O
from	O
their	O
smooth	O
read	O
and	O
write	O
operations	O
,	O
which	O
incur	O
linear	Metric
computational	Metric
overhead	Metric
on	O
the	O
number	O
of	O
memories	O
stored	O
per	O
time	Metric
step	O
of	O
training	O
.	O

Even	O
worse	O
,	O
they	O
require	O
duplication	O
of	O
the	O
entire	O
memory	O
at	O
each	O
time	Metric
step	O
to	O
perform	O
backpropagation	O
through	O
time	Metric
(	O
BPTT	Method
)	O
.	O

To	O
deal	O
with	O
sufficiently	O
complex	O
problems	O
,	O
such	O
as	O
processing	O
a	O
book	O
,	O
or	O
Wikipedia	Material
,	O
this	O
overhead	O
becomes	O
prohibitive	O
.	O

For	O
example	O
,	O
to	O
store	O
memories	O
,	O
a	O
straightforward	O
implementation	O
of	O
the	O
NTM	Method
trained	O
over	O
a	O
sequence	O
of	O
length	O
consumes	O
physical	O
memory	O
;	O
to	O
store	O
memories	O
the	O
overhead	O
exceeds	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
MANN	Method
named	O
SAM	Method
(	O
sparse	O
access	O
memory	O
)	O
.	O

By	O
thresholding	O
memory	O
modifications	O
to	O
a	O
sparse	O
subset	O
,	O
and	O
using	O
efficient	O
data	Method
structures	Method
for	O
content	Method
-	Method
based	Method
read	Method
operations	Method
,	O
our	O
model	O
is	O
optimal	O
in	O
space	Metric
and	O
time	Metric
with	O
respect	O
to	O
memory	Metric
size	Metric
,	O
while	O
retaining	O
end	O
-	O
to	O
-	O
end	Method
gradient	Method
based	Method
optimization	Method
.	O

To	O
test	O
whether	O
the	O
model	O
is	O
able	O
to	O
learn	O
with	O
this	O
sparse	Method
approximation	Method
,	O
we	O
examined	O
its	O
performance	O
on	O
a	O
selection	O
of	O
synthetic	O
and	O
natural	O
tasks	O
:	O
algorithmic	Task
tasks	Task
from	O
the	O
NTM	O
work	O
,	O
Babi	Material
reasoning	O
tasks	O
used	O
with	O
Memory	Method
Networks	Method
and	O
Omniglot	Task
one	Task
-	Task
shot	Task
classification	Task
.	O

We	O
also	O
tested	O
several	O
of	O
these	O
tasks	O
scaled	O
to	O
longer	O
sequences	O
via	O
curriculum	Task
learning	Task
.	O

For	O
large	O
external	O
memories	O
we	O
observed	O
improvements	O
in	O
empirical	O
run	O
-	O
time	Metric
and	O
memory	O
overhead	O
by	O
up	O
to	O
three	O
orders	O
magnitude	O
over	O
vanilla	O
NTMs	Method
,	O
while	O
maintaining	O
near	O
-	O
identical	O
data	Metric
efficiency	Metric
and	O
performance	O
.	O

Further	O
,	O
in	O
Supplementary	O
[	O
reference	O
]	O
we	O
demonstrate	O
the	O
generality	O
of	O
our	O
approach	O
by	O
describing	O
how	O
to	O
construct	O
a	O
sparse	Method
version	Method
of	O
the	O
recently	O
published	O
Differentiable	Method
Neural	Method
Computer	Method
.	O

This	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
(	O
SDNC	Method
)	O
is	O
over	O
faster	O
than	O
the	O
canonical	Method
dense	Method
variant	Method
for	O
a	O
memory	O
size	O
of	O
slots	O
,	O
and	O
achieves	O
the	O
best	O
reported	O
result	O
in	O
the	O
Babi	Material
tasks	O
without	O
supervising	O
the	O
memory	O
access	O
.	O

section	O
:	O
Background	O
subsection	O
:	O
Attention	Task
and	O
content	Method
-	Method
based	Method
addressing	Method
An	O
external	O
memory	O
is	O
a	O
collection	O
of	O
real	O
-	O
valued	O
vectors	O
,	O
or	O
words	O
,	O
of	O
fixed	O
size	O
.	O

A	O
soft	O
read	O
operation	O
is	O
defined	O
to	O
be	O
a	O
weighted	Method
average	Method
over	O
memory	O
words	O
,	O
where	O
is	O
a	O
vector	O
of	O
weights	O
with	O
non	O
-	O
negative	O
entries	O
that	O
sum	O
to	O
one	O
.	O

Attending	O
to	O
memory	O
is	O
formalized	O
as	O
the	O
problem	O
of	O
computing	Task
.	O

A	O
content	Method
addressable	Method
memory	Method
,	O
proposed	O
in	O
,	O
is	O
an	O
external	Method
memory	Method
with	O
an	O
addressing	Method
scheme	Method
which	O
selects	O
based	O
upon	O
the	O
similarity	O
of	O
memory	O
words	O
to	O
a	O
given	O
query	O
.	O

Specifically	O
,	O
for	O
the	O
th	O
read	O
weight	O
we	O
define	O
,	O
where	O
is	O
a	O
similarity	Metric
measure	Metric
,	O
typically	O
Euclidean	O
distance	O
or	O
cosine	O
similarity	O
,	O
and	O
is	O
a	O
differentiable	Method
monotonic	Method
transformation	Method
,	O
typically	O
a	O
softmax	Method
.	O

We	O
can	O
think	O
of	O
this	O
as	O
an	O
instance	O
of	O
kernel	Method
smoothing	Method
where	O
the	O
network	O
learns	O
to	O
query	O
relevant	O
points	O
.	O

Because	O
the	O
read	O
operation	O
(	O
[	O
reference	O
]	O
)	O
and	O
content	Method
-	Method
based	Method
addressing	Method
scheme	Method
(	O
[	O
reference	O
]	O
)	O
are	O
smooth	O
,	O
we	O
can	O
place	O
them	O
within	O
a	O
neural	Method
network	Method
,	O
and	O
train	O
the	O
full	Method
model	Method
using	O
backpropagation	Method
.	O

subsection	O
:	O
Memory	Method
Networks	Method
One	O
recent	O
architecture	O
,	O
Memory	Method
Networks	Method
,	O
make	O
use	O
of	O
a	O
content	Method
addressable	Method
memory	Method
that	O
is	O
accessed	O
via	O
a	O
series	O
of	O
read	Method
operations	Method
and	O
has	O
been	O
successfully	O
applied	O
to	O
a	O
number	O
of	O
question	Task
answering	Task
tasks	Task
.	O

In	O
these	O
tasks	O
,	O
the	O
memory	O
is	O
pre	O
-	O
loaded	O
using	O
a	O
learned	O
embedding	O
of	O
the	O
provided	O
context	O
,	O
such	O
as	O
a	O
paragraph	O
of	O
text	O
,	O
and	O
then	O
the	O
controller	O
,	O
given	O
an	O
embedding	O
of	O
the	O
question	O
,	O
repeatedly	O
queries	O
the	O
memory	O
by	O
content	O
-	O
based	O
reads	O
to	O
determine	O
an	O
answer	O
.	O

subsection	O
:	O
Neural	Method
Turing	Method
Machine	Method
The	O
Neural	Method
Turing	Method
Machine	Method
is	O
a	O
recurrent	Method
neural	Method
network	Method
equipped	O
with	O
a	O
content	O
-	O
addressable	O
memory	O
,	O
similar	O
to	O
Memory	Method
Networks	Method
,	O
but	O
with	O
the	O
additional	O
capability	O
to	O
write	O
to	O
memory	O
over	O
time	Metric
.	O

The	O
memory	O
is	O
accessed	O
by	O
a	O
controller	Method
network	Method
,	O
typically	O
an	O
LSTM	Method
,	O
and	O
the	O
full	Method
model	Method
is	O
differentiable	O
—	O
allowing	O
it	O
to	O
be	O
trained	O
via	O
BPTT	Method
.	O

A	O
write	O
to	O
memory	O
,	O
consists	O
of	O
a	O
copy	O
of	O
the	O
memory	O
from	O
the	O
previous	O
time	Metric
step	O
decayed	O
by	O
the	O
erase	O
matrix	O
indicating	O
obsolete	O
or	O
inaccurate	O
content	O
,	O
and	O
an	O
addition	O
of	O
new	O
or	O
updated	O
information	O
.	O

The	O
erase	O
matrix	O
is	O
constructed	O
as	O
the	O
outer	O
product	O
between	O
a	O
set	O
of	O
write	O
weights	O
and	O
erase	O
vector	O
.	O

The	O
add	O
matrix	O
is	O
the	O
outer	O
product	O
between	O
the	O
write	O
weights	O
and	O
a	O
new	O
write	O
word	O
,	O
which	O
the	O
controller	O
outputs	O
.	O

section	O
:	O
Architecture	O
This	O
paper	O
introduces	O
Sparse	Method
Access	Method
Memory	Method
(	O
SAM	Method
)	O
,	O
a	O
new	O
neural	Method
memory	Method
architecture	Method
with	O
two	O
innovations	O
.	O

Most	O
importantly	O
,	O
all	O
writes	O
to	O
and	O
reads	O
from	O
external	O
memory	O
are	O
constrained	O
to	O
a	O
sparse	O
subset	O
of	O
the	O
memory	O
words	O
,	O
providing	O
similar	O
functionality	O
as	O
the	O
NTM	Method
,	O
while	O
allowing	O
computational	Task
and	Task
memory	Task
efficient	Task
operation	Task
.	O

Secondly	O
,	O
we	O
introduce	O
a	O
sparse	Method
memory	Method
management	Method
scheme	Method
that	O
tracks	O
memory	O
usage	O
and	O
finds	O
unused	O
blocks	O
of	O
memory	O
for	O
recording	O
new	O
information	O
.	O

For	O
a	O
memory	O
containing	O
words	O
,	O
SAM	Method
executes	O
a	O
forward	O
,	O
backward	O
step	O
in	O
time	Metric
,	O
initializes	O
in	O
space	Metric
,	O
and	O
consumes	O
space	Metric
per	O
time	Metric
step	O
.	O

Under	O
some	O
reasonable	O
assumptions	O
,	O
SAM	Method
is	O
asymptotically	O
optimal	O
in	O
time	Metric
and	O
space	Metric
complexity	O
(	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Read	O
The	O
sparse	Task
read	Task
operation	Task
is	O
defined	O
to	O
be	O
a	O
weighted	Method
average	Method
over	O
a	O
selection	O
of	O
words	O
in	O
memory	O
:	O
where	O
contains	O
number	O
of	O
non	O
-	O
zero	O
entries	O
with	O
indices	O
;	O
is	O
a	O
small	O
constant	O
,	O
independent	O
of	O
,	O
typically	O
or	O
.	O

We	O
will	O
refer	O
to	O
sparse	O
analogues	O
of	O
weight	O
vectors	O
as	O
,	O
and	O
when	O
discussing	O
operations	O
that	O
are	O
used	O
in	O
both	O
the	O
sparse	O
and	O
dense	O
versions	O
of	O
our	O
model	O
use	O
.	O

We	O
wish	O
to	O
construct	O
such	O
that	O
.	O

For	O
content	Task
-	Task
based	Task
reads	Task
where	O
is	O
defined	O
by	O
(	O
[	O
reference	O
]	O
)	O
,	O
an	O
effective	O
approach	O
is	O
to	O
keep	O
the	O
largest	O
non	O
-	O
zero	O
entries	O
and	O
set	O
the	O
remaining	O
entries	O
to	O
zero	O
.	O

We	O
can	O
compute	O
naively	O
in	O
time	Metric
by	O
calculating	O
and	O
keeping	O
the	O
largest	O
values	O
.	O

However	O
,	O
linear	O
-	O
time	Metric
operation	O
can	O
be	O
avoided	O
.	O

Since	O
the	O
largest	O
values	O
in	O
correspond	O
to	O
the	O
closest	O
points	O
to	O
our	O
query	O
,	O
we	O
can	O
use	O
an	O
approximate	Method
nearest	Method
neighbor	Method
data	Method
-	Method
structure	Method
,	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
to	O
calculate	O
in	O
time	Metric
.	O

Sparse	Task
read	Task
can	O
be	O
considered	O
a	O
special	O
case	O
of	O
the	O
matrix	Method
-	Method
vector	Method
product	Method
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
with	O
two	O
key	O
distinctions	O
.	O

The	O
first	O
is	O
that	O
we	O
pass	O
gradients	O
for	O
only	O
a	O
constant	O
number	O
of	O
rows	O
of	O
memory	O
per	O
time	Metric
step	O
,	O
versus	O
,	O
which	O
results	O
in	O
a	O
negligible	O
fraction	O
of	O
non	O
-	O
zero	O
error	O
gradient	O
per	O
timestep	O
when	O
the	O
memory	O
is	O
large	O
.	O

The	O
second	O
distinction	O
is	O
in	O
implementation	O
:	O
by	O
using	O
an	O
efficient	O
sparse	Method
matrix	Method
format	Method
such	O
as	O
Compressed	Method
Sparse	Method
Rows	Method
(	O
CSR	Method
)	O
,	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
and	O
its	O
gradients	O
in	O
constant	O
time	Metric
and	O
space	Metric
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Write	O
The	O
write	Method
operation	Method
is	O
SAM	Method
is	O
an	O
instance	O
of	O
(	O
[	O
reference	O
]	O
)	O
where	O
the	O
write	O
weights	O
are	O
constrained	O
to	O
contain	O
a	O
constant	O
number	O
of	O
non	O
-	O
zero	O
entries	O
.	O

This	O
is	O
done	O
by	O
a	O
simple	O
scheme	O
where	O
the	O
controller	O
writes	O
either	O
to	O
previously	O
read	O
locations	O
,	O
in	O
order	O
to	O
update	O
contextually	O
relevant	O
memories	O
,	O
or	O
the	O
least	O
recently	O
accessed	O
location	O
,	O
in	O
order	O
to	O
overwrite	O
stale	O
or	O
unused	O
memory	O
slots	O
with	O
fresh	O
content	O
.	O

The	O
introduction	O
of	O
sparsity	O
could	O
be	O
achieved	O
via	O
other	O
write	Method
schemes	Method
.	O

For	O
example	O
,	O
we	O
could	O
use	O
a	O
sparse	Method
content	Method
-	Method
based	Method
write	Method
scheme	Method
,	O
where	O
the	O
controller	O
chooses	O
a	O
query	O
vector	O
and	O
applies	O
writes	O
to	O
similar	O
words	O
in	O
memory	O
.	O

This	O
would	O
allow	O
for	O
direct	Task
memory	Task
updates	Task
,	O
but	O
would	O
create	O
problems	O
when	O
the	O
memory	O
is	O
empty	O
(	O
and	O
shift	O
further	O
complexity	O
to	O
the	O
controller	O
)	O
.	O

We	O
decided	O
upon	O
the	O
previously	O
read	O
/	O
least	Method
recently	Method
accessed	Method
addressing	Method
scheme	Method
for	O
simplicity	O
and	O
flexibility	O
.	O

The	O
write	O
weights	O
are	O
defined	O
as	O
where	O
the	O
controller	O
outputs	O
the	O
interpolation	O
gate	O
parameter	O
and	O
the	O
write	O
gate	O
parameter	O
.	O

The	O
write	O
to	O
the	O
previously	O
read	O
locations	O
is	O
purely	O
additive	O
,	O
while	O
the	O
least	O
recently	O
accessed	O
word	O
is	O
set	O
to	O
zero	O
before	O
being	O
written	O
to	O
.	O

When	O
the	O
read	O
operation	O
is	O
sparse	O
(	O
has	O
non	O
-	O
zero	O
entries	O
)	O
,	O
it	O
follows	O
the	O
write	O
operation	O
is	O
also	O
sparse	O
.	O

We	O
define	O
to	O
be	O
an	O
indicator	O
over	O
words	O
in	O
memory	O
,	O
with	O
a	O
value	O
of	O
when	O
the	O
word	O
minimizes	O
a	O
usage	Metric
measure	Metric
If	O
there	O
are	O
several	O
words	O
that	O
minimize	O
then	O
we	O
choose	O
arbitrarily	O
between	O
them	O
.	O

We	O
tried	O
two	O
definitions	O
of	O
.	O

The	O
first	O
definition	O
is	O
a	O
time	Metric
-	O
discounted	O
sum	O
of	O
write	O
weights	O
where	O
is	O
the	O
discount	O
factor	O
.	O

This	O
usage	O
definition	O
is	O
incorporated	O
within	O
Dense	Method
Access	Method
Memory	Method
(	O
DAM	Method
)	O
,	O
a	O
dense	Method
-	Method
approximation	Method
to	O
SAM	Method
that	O
is	O
used	O
for	O
experimental	O
comparison	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
second	O
usage	O
definition	O
,	O
used	O
by	O
SAM	Method
,	O
is	O
simply	O
the	O
number	O
of	O
time	Metric
-	O
steps	O
since	O
a	O
non	O
-	O
negligible	O
memory	O
access	O
:	O
.	O

Here	O
,	O
is	O
a	O
tuning	O
parameter	O
that	O
we	O
typically	O
choose	O
to	O
be	O
.	O

We	O
maintain	O
this	O
usage	O
statistic	O
in	O
constant	O
time	Metric
using	O
a	O
custom	O
data	O
-	O
structure	O
(	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
we	O
also	O
use	O
the	O
least	O
recently	O
accessed	O
word	O
to	O
calculate	O
the	O
erase	O
matrix	O
.	O

is	O
defined	O
to	O
be	O
the	O
expansion	O
of	O
this	O
usage	O
indicator	O
where	O
is	O
a	O
vector	O
of	O
ones	O
.	O

The	O
total	O
cost	O
of	O
the	O
write	O
is	O
constant	O
in	O
time	Metric
and	O
space	Metric
for	O
both	O
the	O
forwards	O
and	O
backwards	O
pass	O
,	O
which	O
improves	O
on	O
the	O
linear	O
space	Metric
and	O
time	Metric
dense	O
write	O
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Controller	O
We	O
use	O
a	O
one	O
layer	O
LSTM	Method
for	O
the	O
controller	Method
throughout	O
.	O

At	O
each	O
time	Metric
step	O
,	O
the	O
LSTM	Method
receives	O
a	O
concatenation	O
of	O
the	O
external	O
input	O
,	O
,	O
the	O
word	O
,	O
read	O
in	O
the	O
previous	O
time	Metric
step	O
.	O

The	O
LSTM	Method
then	O
produces	O
a	O
vector	O
,	O
,	O
of	O
read	O
and	O
write	O
parameters	O
for	O
memory	O
access	O
via	O
a	O
linear	Method
layer	Method
.	O

The	O
word	O
read	O
from	O
memory	O
for	O
the	O
current	O
time	Metric
step	O
,	O
,	O
is	O
then	O
concatenated	O
with	O
the	O
output	O
of	O
the	O
LSTM	Method
,	O
and	O
this	O
vector	O
is	O
fed	O
through	O
a	O
linear	Method
layer	Method
to	O
form	O
the	O
final	O
output	O
,	O
.	O

The	O
full	O
control	O
flow	O
is	O
illustrated	O
in	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Efficient	O
backpropagation	O
through	O
time	Metric
We	O
have	O
already	O
demonstrated	O
how	O
the	O
forward	Method
operations	Method
in	O
SAM	Method
can	O
be	O
efficiently	O
computed	O
in	O
time	Metric
.	O

However	O
,	O
when	O
considering	O
space	Metric
complexity	Metric
of	O
MANNs	O
,	O
there	O
remains	O
a	O
dependence	O
on	O
for	O
the	O
computation	O
of	O
the	O
derivatives	O
at	O
the	O
corresponding	O
time	Metric
step	O
.	O

A	O
naive	O
implementation	O
requires	O
the	O
state	O
of	O
the	O
memory	O
to	O
be	O
cached	O
at	O
each	O
time	Metric
step	O
,	O
incurring	O
a	O
space	Metric
overhead	O
of	O
,	O
which	O
severely	O
limits	O
memory	O
size	O
and	O
sequence	O
length	O
.	O

Fortunately	O
,	O
this	O
can	O
be	O
remedied	O
.	O

Since	O
there	O
are	O
only	O
words	O
that	O
are	O
written	O
at	O
each	O
time	Metric
step	O
,	O
we	O
instead	O
track	O
the	O
sparse	O
modifications	O
made	O
to	O
the	O
memory	O
at	O
each	O
timestep	O
,	O
apply	O
them	O
in	O
-	O
place	O
to	O
compute	O
in	O
time	Metric
and	O
space	Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
can	O
restore	O
the	O
state	O
of	O
from	O
in	O
time	Metric
by	O
reverting	O
the	O
sparse	Method
modifications	Method
applied	O
at	O
time	Metric
step	O
.	O

As	O
such	O
the	O
memory	O
is	O
actually	O
rolled	O
back	O
to	O
previous	O
states	O
during	O
backpropagation	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

At	O
the	O
end	O
of	O
the	O
backward	O
pass	O
,	O
the	O
memory	O
ends	O
rolled	O
back	O
to	O
the	O
start	O
state	O
.	O

If	O
required	O
,	O
such	O
as	O
when	O
using	O
truncating	Method
BPTT	Method
,	O
the	O
final	O
memory	O
state	O
can	O
be	O
restored	O
by	O
making	O
a	O
copy	O
of	O
prior	O
to	O
calling	O
backwards	O
in	O
time	Metric
,	O
or	O
by	O
re	O
-	O
applying	O
the	O
sparse	Method
updates	Method
in	O
time	Metric
.	O

subsection	O
:	O
Approximate	Method
nearest	Method
neighbors	Method
When	O
querying	O
the	O
memory	O
,	O
we	O
can	O
use	O
an	O
approximate	Method
nearest	Method
neighbor	Method
index	Method
(	O
ANN	Method
)	O
to	O
search	O
over	O
the	O
external	O
memory	O
for	O
the	O
nearest	O
words	O
.	O

Where	O
a	O
linear	Method
KNN	Method
search	Method
inspects	O
every	O
element	O
in	O
memory	O
(	O
taking	O
time	Metric
)	O
,	O
an	O
ANN	Method
index	Method
maintains	O
a	O
structure	O
over	O
the	O
dataset	O
to	O
allow	O
for	O
fast	O
inspection	O
of	O
nearby	O
points	O
in	O
time	Metric
.	O

In	O
our	O
case	O
,	O
the	O
memory	O
is	O
still	O
a	O
dense	O
tensor	O
that	O
the	O
network	O
directly	O
operates	O
on	O
;	O
however	O
the	O
ANN	Method
is	O
a	O
structured	O
view	O
of	O
its	O
contents	O
.	O

Both	O
the	O
memory	O
and	O
the	O
ANN	O
index	O
are	O
passed	O
through	O
the	O
network	O
and	O
kept	O
in	O
sync	O
during	O
writes	O
.	O

However	O
there	O
are	O
no	O
gradients	O
with	O
respect	O
to	O
the	O
ANN	Method
as	O
its	O
function	O
is	O
fixed	O
.	O

We	O
considered	O
two	O
types	O
of	O
ANN	Method
indexes	Method
:	O
FLANN	Method
’s	Method
randomized	Method
k	Method
-	Method
d	Method
tree	Method
implementation	Method
that	O
arranges	O
the	O
datapoints	O
in	O
an	O
ensemble	O
of	O
structured	O
(	O
randomized	Method
k	Method
-	Method
d	Method
)	Method
trees	Method
to	O
search	O
for	O
nearby	O
points	O
via	O
comparison	Method
-	Method
based	Method
search	Method
,	O
and	O
one	O
that	O
uses	O
locality	Method
sensitive	Method
hash	Method
(	O
LSH	Method
)	O
functions	O
that	O
map	O
points	O
into	O
buckets	O
with	O
distance	O
-	O
preserving	O
guarantees	O
.	O

We	O
used	O
randomized	Method
k	Method
-	Method
d	Method
trees	Method
for	O
small	O
word	O
sizes	O
and	O
LSHs	Method
for	O
large	O
word	O
sizes	O
.	O

For	O
both	O
ANN	Method
implementations	Method
,	O
there	O
is	O
an	O
cost	O
for	O
insertion	Task
,	O
deletion	Task
and	O
query	Task
.	O

We	O
also	O
rebuild	O
the	O
ANN	Method
from	O
scratch	O
every	O
insertions	O
to	O
ensure	O
it	O
does	O
not	O
become	O
imbalanced	O
.	O

section	O
:	O
Results	O
subsection	O
:	O
Speed	Metric
and	Metric
memory	Metric
benchmarks	Metric
0.47	O
0.47	O
We	O
measured	O
the	O
forward	O
and	O
backward	O
times	O
of	O
the	O
SAM	Method
architecture	O
versus	O
the	O
dense	O
DAM	Method
variant	O
and	O
the	O
original	O
NTM	Method
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

SAM	Method
is	O
over	O
times	O
faster	O
than	O
the	O
NTM	Method
when	O
the	O
memory	O
contains	O
one	O
million	O
words	O
and	O
an	O
exact	O
linear	O
-	O
index	O
is	O
used	O
,	O
and	O
times	O
faster	O
with	O
the	O
k	Method
-	Method
d	Method
tree	Method
(	O
Figure	O
[	O
reference	O
]	O
sf	O
:	O
speed	O
)	O
.	O

With	O
an	O
ANN	Method
the	O
model	O
runs	O
in	O
sublinear	O
time	Metric
with	O
respect	O
to	O
the	O
memory	O
size	O
.	O

SAM	Method
’s	O
memory	O
usage	O
per	O
time	Metric
step	O
is	O
independent	O
of	O
the	O
number	O
of	O
memory	O
words	O
(	O
Figure	O
[	O
reference	O
]	O
sf	Method
:	O
memory	O
)	O
,	O
which	O
empirically	O
verifies	O
the	O
space	Metric
claim	O
from	O
Supplementary	O
[	O
reference	O
]	O
.	O

For	O
memory	O
words	O
SAM	Method
uses	O
of	O
physical	O
memory	O
to	O
initialize	O
the	O
network	O
and	O
to	O
run	O
a	O
100	O
step	O
forward	O
and	O
backward	O
pass	O
,	O
compared	O
with	O
the	O
NTM	Method
which	O
consumes	O
.	O

subsection	O
:	O
Learning	Task
with	O
sparse	Task
memory	Task
access	Task
We	O
have	O
established	O
that	O
SAM	Method
reaps	O
a	O
huge	O
computational	Metric
and	Metric
memory	Metric
advantage	O
of	O
previous	O
models	O
,	O
but	O
can	O
we	O
really	O
learn	O
with	O
SAM	Method
’s	O
sparse	O
approximations	O
?	O
We	O
investigated	O
the	O
learning	Metric
cost	Metric
of	O
inducing	Task
sparsity	Task
,	O
and	O
the	O
effect	O
of	O
placing	O
an	O
approximate	O
nearest	O
neighbor	O
index	O
within	O
the	O
network	O
,	O
by	O
comparing	O
SAM	Method
with	O
its	O
dense	O
variant	O
DAM	Method
and	O
some	O
established	O
models	O
,	O
the	O
NTM	Method
and	O
the	O
LSTM	Method
.	O

We	O
trained	O
each	O
model	O
on	O
three	O
of	O
the	O
original	O
NTM	Task
tasks	Task
.	O

1	O
.	O

Copy	Method
:	O
copy	O
a	O
random	O
input	O
sequence	O
of	O
length	O
1–20	O
,	O
2	O
.	O

Associative	Task
Recall	Task
:	O
given	O
3	O
-	O
6	O
random	O
(	O
key	O
,	O
value	O
)	O
pairs	O
,	O
and	O
subsequently	O
a	O
cue	O
key	O
,	O
return	O
the	O
associated	O
value	O
.	O

3	O
.	O

Priority	Method
Sort	Method
:	O
Given	O
20	O
random	O
keys	O
and	O
priority	O
values	O
,	O
return	O
the	O
top	O
16	O
keys	O
in	O
descending	O
order	O
of	O
priority	O
.	O

We	O
chose	O
these	O
tasks	O
because	O
the	O
NTM	Method
is	O
known	O
to	O
perform	O
well	O
on	O
them	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
that	O
sparse	Method
models	Method
are	O
able	O
to	O
learn	O
with	O
comparable	O
efficiency	O
to	O
the	O
dense	Method
models	Method
and	O
,	O
surprisingly	O
,	O
learn	O
more	O
effectively	O
for	O
some	O
tasks	O
—	O
notably	O
priority	Task
sort	Task
and	O
associative	Task
recall	Task
.	O

This	O
shows	O
that	O
sparse	O
reads	O
and	O
writes	O
can	O
actually	O
benefit	O
early	O
-	O
stage	O
learning	Task
in	O
some	O
cases	O
.	O

Full	O
hyperparameter	O
details	O
are	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

0.32	O
0.32	O
0.32	O
subsection	O
:	O
Scaling	O
with	O
a	O
curriculum	O
The	O
computational	Metric
efficiency	Metric
of	O
SAM	Method
opens	O
up	O
the	O
possibility	O
of	O
training	O
on	O
tasks	O
that	O
require	O
storing	O
a	O
large	O
amount	O
of	O
information	O
over	O
long	O
sequences	O
.	O

Here	O
we	O
show	O
this	O
is	O
possible	O
in	O
practice	O
,	O
by	O
scaling	Task
tasks	Task
to	O
a	O
large	O
scale	O
via	O
an	O
exponentially	Method
increasing	Method
curriculum	Method
.	O

We	O
parametrized	O
three	O
of	O
the	O
tasks	O
described	O
in	O
Section	O
[	O
reference	O
]	O
:	O
associative	Task
recall	Task
,	O
copy	O
,	O
and	O
priority	Task
sort	Task
,	O
with	O
a	O
progressively	O
increasing	O
difficulty	O
level	O
which	O
characterises	O
the	O
length	O
of	O
the	O
sequence	O
and	O
number	O
of	O
entries	O
to	O
store	O
in	O
memory	O
.	O

For	O
example	O
,	O
level	O
specifies	O
the	O
input	O
sequence	O
length	O
for	O
the	O
copy	Task
task	Task
.	O

We	O
exponentially	O
increased	O
the	O
maximum	O
level	O
when	O
the	O
network	O
begins	O
to	O
learn	O
the	O
fundamental	Method
algorithm	Method
.	O

Since	O
the	O
time	Metric
taken	O
for	O
a	O
forward	O
and	O
backward	O
pass	O
scales	O
with	O
the	O
sequence	O
length	O
,	O
following	O
a	O
standard	O
linearly	Method
increasing	Method
curriculum	Method
could	O
potentially	O
take	O
,	O
if	O
the	O
same	O
amount	O
of	O
training	O
was	O
required	O
at	O
each	O
step	O
of	O
the	O
curriculum	O
.	O

Specifically	O
,	O
was	O
doubled	O
whenever	O
the	O
average	Metric
training	Metric
loss	Metric
dropped	O
below	O
a	O
threshold	O
for	O
a	O
number	O
of	O
episodes	O
.	O

The	O
level	O
was	O
sampled	O
for	O
each	O
minibatch	O
from	O
the	O
uniform	O
distribution	O
over	O
integers	O
.	O

We	O
compared	O
the	O
dense	Method
models	Method
,	O
NTM	O
and	O
DAM	Method
,	O
with	O
both	O
SAM	Method
with	O
an	O
exact	Method
nearest	Method
neighbor	Method
index	Method
(	O
SAM	Method
linear	O
)	O
and	O
with	O
locality	Method
sensitive	Method
hashing	Method
(	O
SAM	Method
ANN	O
)	O
.	O

The	O
dense	Method
models	Method
contained	O
64	O
memory	O
words	O
,	O
while	O
the	O
sparse	Method
models	Method
had	O
words	O
.	O

These	O
sizes	O
were	O
chosen	O
to	O
ensure	O
all	O
models	O
use	O
approximately	O
the	O
same	O
amount	O
of	O
physical	O
memory	O
when	O
trained	O
over	O
100	O
steps	O
.	O

For	O
all	O
tasks	O
,	O
SAM	Method
was	O
able	O
to	O
advance	O
further	O
than	O
the	O
other	O
models	O
,	O
and	O
in	O
the	O
associative	Task
recall	Task
task	Task
,	O
SAM	Method
was	O
able	O
to	O
advance	O
through	O
the	O
curriculum	O
to	O
sequences	O
greater	O
than	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Note	O
that	O
we	O
did	O
not	O
use	O
truncated	Method
backpropagation	Method
,	O
so	O
this	O
involved	O
BPTT	Method
for	O
over	O
steps	O
with	O
a	O
memory	O
size	O
in	O
the	O
millions	O
of	O
words	O
.	O

To	O
investigate	O
whether	O
SAM	Method
was	O
able	O
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
tasks	O
,	O
we	O
investigated	O
its	O
ability	O
to	O
generalize	O
to	O
sequences	O
that	O
far	O
exceeded	O
those	O
observed	O
during	O
training	O
.	O

Namely	O
we	O
trained	O
SAM	Method
on	O
the	O
associative	Task
recall	Task
task	Task
up	O
to	O
sequences	O
of	O
length	O
,	O
and	O
found	O
it	O
was	O
then	O
able	O
to	O
generalize	O
to	O
sequences	O
of	O
length	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Question	Task
answering	Task
on	O
the	O
Babi	Material
tasks	O
introduced	O
toy	Task
tasks	Task
they	O
considered	O
a	O
prerequisite	O
to	O
agents	O
which	O
can	O
reason	O
and	O
understand	O
natural	O
language	O
.	O

They	O
are	O
synthetically	Task
generated	Task
language	Task
tasks	Task
with	O
a	O
vocab	O
of	O
about	O
150	O
words	O
that	O
test	O
various	O
aspects	O
of	O
simple	O
reasoning	Task
such	O
as	O
deduction	Task
,	O
induction	Task
and	O
coreferencing	Task
.	O

We	O
tested	O
the	O
models	O
(	O
including	O
the	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
on	O
this	O
task	O
.	O

The	O
full	O
results	O
and	O
training	O
details	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

The	O
MANNs	Method
,	O
except	O
the	O
NTM	Method
,	O
are	O
able	O
to	O
learn	O
solutions	O
comparable	O
to	O
the	O
previous	O
best	O
results	O
,	O
failing	O
at	O
only	O
2	O
of	O
the	O
tasks	O
.	O

The	O
SDNC	Method
manages	O
to	O
solve	O
all	O
but	O
1	O
of	O
the	O
tasks	O
,	O
the	O
best	O
reported	O
result	O
on	O
Babi	Material
that	O
we	O
are	O
aware	O
of	O
.	O

Notably	O
the	O
best	O
prior	O
results	O
have	O
been	O
obtained	O
by	O
using	O
supervising	O
the	O
memory	Task
retrieval	Task
(	O
during	O
training	O
the	O
model	O
is	O
provided	O
annotations	O
which	O
indicate	O
which	O
memories	O
should	O
be	O
used	O
to	O
answer	O
a	O
query	O
)	O
.	O

More	O
directly	O
comparable	O
previous	O
work	O
with	O
end	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
networks	Method
,	O
which	O
did	O
not	O
use	O
supervision	O
,	O
fails	O
at	O
6	O
of	O
the	O
tasks	O
.	O

Both	O
the	O
sparse	Method
and	O
dense	Method
perform	O
comparably	O
at	O
this	O
task	O
,	O
again	O
indicating	O
the	O
sparse	Method
approximations	Method
do	O
not	O
impair	O
learning	Task
.	O

We	O
believe	O
the	O
NTM	Method
may	O
perform	O
poorly	O
since	O
it	O
lacks	O
a	O
mechanism	O
which	O
allows	O
it	O
to	O
allocate	O
memory	O
effectively	O
.	O

subsection	O
:	O
Learning	O
on	O
real	O
world	O
data	O
Finally	O
,	O
we	O
demonstrate	O
that	O
the	O
model	O
is	O
capable	O
of	O
learning	O
in	O
a	O
non	O
-	O
synthetic	O
dataset	O
.	O

Omniglot	Method
is	O
a	O
dataset	O
of	O
1623	O
characters	O
taken	O
from	O
50	O
different	O
alphabets	O
,	O
with	O
20	O
examples	O
of	O
each	O
character	O
.	O

This	O
dataset	O
is	O
used	O
to	O
test	O
rapid	Task
,	Task
or	Task
one	Task
-	Task
shot	Task
learning	Task
,	O
since	O
there	O
are	O
few	O
examples	O
of	O
each	O
character	O
but	O
many	O
different	O
character	O
classes	O
.	O

Following	O
,	O
we	O
generate	O
episodes	O
where	O
a	O
subset	O
of	O
characters	O
are	O
randomly	O
selected	O
from	O
the	O
dataset	O
,	O
rotated	O
and	O
stretched	O
,	O
and	O
assigned	O
a	O
randomly	O
chosen	O
label	O
.	O

At	O
each	O
time	Metric
step	O
an	O
example	O
of	O
one	O
of	O
the	O
characters	O
is	O
presented	O
,	O
along	O
with	O
the	O
correct	O
label	O
of	O
the	O
proceeding	O
character	O
.	O

Each	O
character	O
is	O
presented	O
10	O
times	O
in	O
an	O
episode	O
(	O
but	O
each	O
presentation	O
may	O
be	O
any	O
one	O
of	O
the	O
20	O
examples	O
of	O
the	O
character	O
)	O
.	O

In	O
order	O
to	O
succeed	O
at	O
the	O
task	O
the	O
model	O
must	O
learn	O
to	O
rapidly	O
associate	O
a	O
novel	O
character	O
with	O
the	O
correct	O
label	O
,	O
such	O
that	O
it	O
can	O
correctly	O
classify	O
subsequent	O
examples	O
of	O
the	O
same	O
character	O
class	O
.	O

Again	O
,	O
we	O
used	O
an	O
exponential	Method
curriculum	Method
,	O
doubling	O
the	O
number	O
of	O
additional	O
characters	O
provided	O
to	O
the	O
model	O
whenever	O
the	O
cost	O
was	O
reduced	O
under	O
a	O
threshold	O
.	O

After	O
training	O
all	O
MANNs	Method
for	O
the	O
same	O
length	O
of	O
time	Metric
,	O
a	O
validation	Task
task	Task
with	O
characters	O
was	O
used	O
to	O
select	O
the	O
best	O
run	O
,	O
and	O
this	O
was	O
then	O
tested	O
on	O
a	O
test	O
set	O
,	O
containing	O
all	O
novel	O
characters	O
for	O
different	O
sequence	O
lengths	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

All	O
of	O
the	O
MANNs	Method
were	O
able	O
to	O
perform	O
much	O
better	O
than	O
chance	O
,	O
even	O
on	O
sequences	O
longer	O
than	O
seen	O
during	O
training	O
.	O

SAM	Method
outperformed	O
other	O
models	O
,	O
presumably	O
due	O
to	O
its	O
much	O
larger	O
memory	O
capacity	O
.	O

Previous	O
results	O
on	O
the	O
Omniglot	Task
curriculum	Task
task	Task
are	O
not	O
identical	O
,	O
since	O
we	O
used	O
1	O
-	O
hot	O
labels	O
throughout	O
and	O
the	O
training	O
curriculum	O
scaled	O
to	O
longer	O
sequences	O
,	O
but	O
our	O
results	O
with	O
the	O
dense	Method
models	Method
are	O
comparable	O
(	O
errors	O
with	O
characters	O
)	O
,	O
while	O
the	O
SAM	Method
is	O
significantly	O
better	O
(	O
errors	O
with	O
characters	O
)	O
.	O

section	O
:	O
Discussion	O
Scaling	Task
memory	Task
systems	Task
is	O
a	O
pressing	O
research	O
direction	O
due	O
to	O
potential	O
for	O
compelling	O
applications	O
with	O
large	O
amounts	O
of	O
memory	O
.	O

We	O
have	O
demonstrated	O
that	O
you	O
can	O
train	O
neural	Method
networks	Method
with	O
large	O
memories	O
via	O
a	O
sparse	Method
read	Method
and	Method
write	Method
scheme	Method
that	O
makes	O
use	O
of	O
efficient	O
data	O
structures	O
within	O
the	O
network	O
,	O
and	O
obtain	O
significant	O
speedups	O
during	O
training	Task
.	O

Although	O
we	O
have	O
focused	O
on	O
a	O
specific	O
MANN	Method
(	O
SAM	Method
)	Method
,	O
which	O
is	O
closely	O
related	O
to	O
the	O
NTM	Method
,	O
the	O
approach	O
taken	O
here	O
is	O
general	O
and	O
can	O
be	O
applied	O
to	O
many	O
differentiable	O
memory	Method
architectures	Method
,	O
such	O
as	O
Memory	Method
Networks	Method
.	O

It	O
should	O
be	O
noted	O
that	O
there	O
are	O
multiple	O
possible	O
routes	O
toward	O
scalable	Task
memory	Task
architectures	Task
.	O

For	O
example	O
,	O
prior	O
work	O
aimed	O
at	O
scaling	O
Neural	Method
Turing	Method
Machines	Method
used	O
reinforcement	Method
learning	Method
to	O
train	O
a	O
discrete	Method
addressing	Method
policy	Method
.	O

This	O
approach	O
also	O
touches	O
only	O
a	O
sparse	O
set	O
of	O
memories	O
at	O
each	O
time	Metric
step	O
,	O
but	O
relies	O
on	O
higher	O
variance	O
estimates	O
of	O
the	O
gradient	O
during	O
optimization	Task
.	O

Though	O
we	O
can	O
only	O
guess	O
at	O
what	O
class	O
of	O
memory	Method
models	Method
will	O
become	O
staple	O
in	O
machine	Task
learning	Task
systems	Task
of	O
the	O
future	O
,	O
we	O
argue	O
in	O
Supplementary	O
[	O
reference	O
]	O
that	O
they	O
will	O
be	O
no	O
more	O
efficient	O
than	O
SAM	Method
in	O
space	Metric
and	O
time	Metric
complexity	Metric
if	O
they	O
address	O
memories	O
based	O
on	O
content	O
.	O

We	O
have	O
experimented	O
with	O
randomized	Method
k	Method
-	Method
d	Method
trees	Method
and	O
LSH	Method
within	O
the	O
network	O
to	O
reduce	O
the	O
forward	O
pass	O
of	O
training	Task
to	O
sublinear	O
time	Metric
,	O
but	O
there	O
may	O
be	O
room	O
for	O
improvement	O
here	O
.	O

K	Method
-	Method
d	Method
trees	Method
were	O
not	O
designed	O
specifically	O
for	O
fully	O
online	Task
scenarios	Task
,	O
and	O
can	O
become	O
imbalanced	O
during	O
training	O
.	O

Recent	O
work	O
in	O
tree	Method
ensemble	Method
models	Method
,	O
such	O
as	O
Mondrian	Method
forests	Method
,	O
show	O
promising	O
results	O
in	O
maintaining	O
balanced	Task
hierarchical	Task
set	Task
coverage	Task
in	O
the	O
online	Task
setting	Task
.	O

An	O
alternative	O
approach	O
which	O
may	O
be	O
well	O
-	O
suited	O
is	O
LSH	Method
forests	O
,	O
which	O
adaptively	O
modifies	O
the	O
number	O
of	O
hashes	O
used	O
.	O

It	O
would	O
be	O
an	O
interesting	O
empirical	O
investigation	O
to	O
more	O
fully	O
assess	O
different	O
ANN	Method
approaches	Method
in	O
the	O
challenging	O
context	O
of	O
training	O
a	O
neural	Method
network	Method
.	O

Humans	O
are	O
able	O
to	O
retain	O
a	O
large	O
,	O
task	O
-	O
dependent	O
set	O
of	O
memories	O
obtained	O
in	O
one	O
pass	O
with	O
a	O
surprising	O
amount	O
of	O
fidelity	Metric
.	O

Here	O
we	O
have	O
demonstrated	O
architectures	O
that	O
may	O
one	O
day	O
compete	O
with	O
humans	O
at	O
these	O
kinds	O
of	O
tasks	O
.	O

subsection	O
:	O
Acknowledgements	O
We	O
thank	O
Vyacheslav	O
Egorov	O
,	O
Edward	O
Grefenstette	O
,	O
Malcolm	O
Reynolds	O
,	O
Fumin	O
Wang	O
and	O
Yori	O
Zwols	O
for	O
their	O
assistance	O
,	O
and	O
the	O
Google	O
DeepMind	O
family	O
for	O
helpful	O
discussions	O
and	O
encouragement	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplementary	O
Information	O
appendix	O
:	O
Time	Metric
and	O
space	Metric
complexity	Metric
Under	O
a	O
reasonable	O
class	O
of	O
content	Method
addressable	Method
memory	Method
architectures	Method
,	O
SAM	Method
is	O
optimal	O
in	O
time	Metric
and	O
space	Metric
complexity	Metric
.	O

theorem	O
:	O
.	O

Let	O
M	O
be	O
a	O
collection	O
of	O
real	O
vectors	O
m1	O
,	O
m2	O
,	O
…	O
,	O
mN	O
of	O
fixed	O
dimension	O
d.	O
Let	O
A	O
be	O
the	O
set	O
of	O
all	O
content	Method
addressable	Method
memory	Method
data	Method
structures	Method
that	O
store	O
M	O
and	O
can	O
return	O
at	O
least	O
one	O
word	O
mj	O
such	O
that	O
≤⁢D	O
(	O
q	O
,	O
mj	O
)	O
⁢c	O
(+	O
1ϵ	O
)	O
for	O
a	O
given	O
Lp	O
norm	O
D	O
,	O
query	O
vector	O
q	O
,	O
and	O
>	O
ϵ0	O
;	O
provided	O
such	O
a	O
memory	O
mc	O
exists	O
with	O
=	O
⁢D	O
(	O
q	O
,	O
mc	O
)	O
c	O
.	O

Existing	O
lower	O
bounds	O
assert	O
that	O
for	O
any	O
data	O
structure	O
,	O
requires	O
time	Metric
and	O
space	Metric
to	O
perform	O
a	O
read	O
operation	O
.	O

The	O
SAM	Method
memory	O
architecture	O
proposed	O
in	O
this	O
paper	O
is	O
contained	O
within	O
as	O
it	O
computes	O
the	O
approximate	Task
nearest	Task
neighbors	Task
problem	Task
in	O
fixed	O
dimensions	O
.	O

As	O
we	O
will	O
show	O
,	O
SAM	Method
requires	O
time	Metric
to	O
query	O
and	O
maintain	O
the	O
ANN	Method
,	O
to	O
perform	O
all	O
subsequent	O
sparse	Task
read	Task
,	Task
write	Task
,	Task
and	Task
error	Task
gradient	Task
calculations	Task
.	O

It	O
requires	O
space	Metric
to	O
initialize	O
the	O
memory	O
and	O
to	O
store	O
intermediate	O
sparse	O
tensors	O
.	O

We	O
thus	O
conclude	O
it	O
is	O
optimal	O
in	O
asymptotic	Metric
time	Metric
and	O
space	Metric
complexity	Metric
.	O

subsection	O
:	O
Initialization	O
Upon	O
initialization	Task
,	O
SAM	Method
consumes	O
space	Metric
and	O
time	Metric
to	O
instantiate	O
the	O
memory	O
and	O
the	O
memory	O
Jacobian	O
.	O

Furthermore	O
,	O
it	O
requires	O
time	Metric
and	O
space	Metric
to	O
initialize	O
auxiliary	O
data	O
structures	O
which	O
index	O
the	O
memory	O
,	O
such	O
as	O
the	O
approximate	Method
nearest	Method
neighbor	Method
which	O
provides	O
a	O
content	O
-	O
structured	O
view	O
of	O
the	O
memory	O
,	O
and	O
the	O
least	O
accessed	O
ring	O
,	O
which	O
maintains	O
the	O
temporal	O
ordering	O
in	O
which	O
memory	O
words	O
are	O
accessed	O
.	O

These	O
initializations	O
represent	O
an	O
unavoidable	O
one	Metric
-	Metric
off	Metric
cost	Metric
that	O
does	O
not	O
recur	O
per	O
step	O
of	O
training	O
,	O
and	O
ultimately	O
has	O
little	O
effect	O
on	O
training	Metric
speed	Metric
.	O

For	O
the	O
remainder	O
of	O
the	O
analysis	O
we	O
will	O
concentrate	O
on	O
the	O
space	Metric
and	O
time	Metric
cost	Metric
per	O
training	O
step	O
.	O

subsection	O
:	O
Read	O
Recall	O
the	O
sparse	O
read	O
operation	O
,	O
As	O
is	O
chosen	O
to	O
be	O
a	O
fixed	O
constant	O
,	O
it	O
is	O
clear	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
in	O
time	Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
see	O
the	O
gradients	O
are	O
sparse	O
with	O
only	O
non	O
-	O
zero	O
terms	O
,	O
and	O
where	O
is	O
a	O
vector	O
of	O
zeros	O
.	O

Thus	O
they	O
can	O
both	O
be	O
computed	O
in	O
constant	O
time	Metric
by	O
skipping	O
the	O
computation	O
of	O
zeros	O
.	O

Furthermore	O
by	O
using	O
an	O
efficient	O
sparse	Method
matrix	Method
format	Method
to	O
store	O
these	O
matrices	O
and	O
vectors	O
,	O
such	O
as	O
the	O
CSR	Method
,	O
we	O
can	O
represent	O
them	O
using	O
at	O
most	O
values	O
.	O

Since	O
the	O
read	O
word	O
and	O
its	O
respective	O
error	O
gradient	O
is	O
the	O
size	O
of	O
a	O
single	O
word	O
in	O
memory	O
(	O
elements	O
)	O
,	O
the	O
overall	O
space	Metric
complexity	Metric
is	O
per	O
time	Metric
step	O
for	O
the	O
read	O
.	O

subsection	O
:	O
Write	O
Recall	O
the	O
write	O
operation	O
,	O
where	O
is	O
the	O
add	O
matrix	O
,	O
is	O
the	O
erase	O
matrix	O
,	O
and	O
is	O
defined	O
to	O
be	O
the	O
erase	O
weight	O
matrix	O
.	O

We	O
chose	O
the	O
write	O
weights	O
to	O
be	O
an	O
interpolation	O
between	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
the	O
previously	O
read	O
locations	O
,	O
For	O
sparse	Task
reads	Task
where	O
is	O
a	O
sparse	O
vector	O
with	O
non	O
-	O
zeros	O
,	O
the	O
write	O
weights	O
is	O
also	O
sparse	O
with	O
non	O
-	O
zeros	O
:	O
for	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
for	O
the	O
previously	O
read	O
locations	O
.	O

Thus	O
the	O
sparse	Method
-	Method
dense	Method
outer	Method
product	Method
can	O
be	O
performed	O
in	O
time	Metric
as	O
is	O
a	O
fixed	O
constant	O
.	O

Since	O
can	O
be	O
represented	O
as	O
a	O
sparse	O
matrix	O
with	O
one	O
single	O
non	O
-	O
zero	O
,	O
the	O
erase	O
matrix	O
can	O
also	O
.	O

As	O
and	O
are	O
sparse	O
matrices	O
we	O
can	O
then	O
add	O
them	O
component	O
-	O
wise	O
to	O
the	O
dense	O
in	O
time	Metric
.	O

By	O
analogous	O
arguments	O
the	O
backward	Method
pass	Method
can	O
be	O
computed	O
in	O
time	Metric
and	O
each	O
sparse	O
matrix	O
can	O
be	O
represented	O
in	O
space	Metric
.	O

We	O
avoid	O
caching	O
the	O
modified	O
memory	O
,	O
and	O
thus	O
duplicating	O
it	O
,	O
by	O
applying	O
the	O
write	O
directly	O
to	O
the	O
memory	O
.	O

To	O
restore	O
its	O
prior	O
state	O
during	O
the	O
backward	O
pass	O
,	O
which	O
is	O
crucial	O
to	O
gradient	Task
calculations	Task
at	O
earlier	O
time	Metric
steps	O
,	O
we	O
roll	O
the	O
memory	O
it	O
back	O
by	O
reverting	O
the	O
sparse	Method
modifications	Method
with	O
an	O
additional	O
time	Metric
overhead	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
location	O
of	O
the	O
least	O
recently	O
accessed	O
memory	O
can	O
be	O
maintained	O
in	O
time	Metric
by	O
constructing	O
a	O
circular	Method
linked	Method
list	Method
that	O
tracks	O
the	O
indices	O
of	O
words	O
in	O
memory	O
,	O
and	O
preserves	O
a	O
strict	O
ordering	O
of	O
relative	O
temporal	O
access	O
.	O

The	O
first	O
element	O
in	O
the	O
ring	O
is	O
the	O
least	O
recently	O
accessed	O
word	O
in	O
memory	O
,	O
and	O
the	O
last	O
element	O
in	O
the	O
ring	O
is	O
the	O
most	O
recently	O
modified	O
.	O

We	O
keep	O
a	O
‘	O
‘	O
head	O
’	O
’	O
pointer	O
to	O
the	O
first	O
element	O
in	O
the	O
ring	O
.	O

When	O
a	O
memory	O
word	O
is	O
randomly	O
accessed	O
,	O
we	O
can	O
push	O
its	O
respective	O
index	O
to	O
the	O
back	O
of	O
the	O
ring	O
in	O
time	Metric
by	O
redirecting	O
a	O
small	O
number	O
of	O
pointers	O
.	O

When	O
we	O
wish	O
to	O
pop	O
the	O
least	O
recently	O
accessed	O
memory	O
(	O
and	O
write	O
to	O
it	O
)	O
we	O
move	O
the	O
head	O
to	O
the	O
next	O
element	O
in	O
the	O
ring	O
in	O
time	Metric
.	O

subsection	O
:	O
Content	Method
-	Method
based	Method
addressing	Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
we	O
can	O
calculate	O
the	O
content	O
-	O
based	O
attention	O
,	O
or	O
read	O
weights	O
,	O
in	O
time	Metric
using	O
an	O
approximate	Method
nearest	Method
neighbor	Method
index	Method
that	O
views	O
the	O
memory	O
.	O

We	O
keep	O
the	O
ANN	O
index	O
synchronized	O
with	O
the	O
memory	O
by	O
passing	O
it	O
through	O
the	O
network	O
as	O
a	O
non	O
-	O
differentiable	O
member	O
of	O
the	O
network	O
’s	O
state	O
(	O
so	O
we	O
do	O
not	O
pass	O
gradients	O
for	O
it	O
)	O
,	O
and	O
we	O
update	O
the	O
index	O
upon	O
each	O
write	O
or	O
erase	O
to	O
memory	O
in	O
time	Metric
.	O

Maintaining	Task
and	Task
querying	Task
the	O
ANN	Task
index	Task
represents	O
the	O
most	O
expensive	O
part	O
of	O
the	O
network	O
,	O
which	O
is	O
reasonable	O
as	O
content	Task
-	Task
based	Task
addressing	Task
is	O
inherently	O
expensive	O
.	O

For	O
the	O
backward	Task
pass	Task
computation	Task
,	O
specifically	O
calculating	O
and	O
with	O
respect	O
to	O
,	O
we	O
can	O
once	O
again	O
compute	O
these	O
using	O
sparse	Method
matrix	Method
operations	Method
in	O
time	Metric
.	O

This	O
is	O
because	O
the	O
non	O
-	O
zero	O
locations	O
have	O
been	O
determined	O
during	O
the	O
forward	O
pass	O
.	O

Thus	O
to	O
conclude	O
,	O
SAM	Method
consumes	O
in	O
total	O
space	Metric
for	O
both	O
the	O
forward	O
and	O
backward	O
step	O
during	O
training	O
,	O
time	Metric
per	O
forward	O
step	O
,	O
and	O
per	O
backward	O
step	O
.	O

appendix	O
:	O
Control	Method
flow	Method
appendix	O
:	O
Training	O
details	O
Here	O
we	O
provide	O
additional	O
details	O
on	O
the	O
training	O
regime	O
used	O
for	O
our	O
experiments	O
used	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

To	O
avoid	O
bias	O
in	O
our	O
results	O
,	O
we	O
chose	O
the	O
learning	Metric
rate	Metric
that	O
worked	O
best	O
for	O
DAM	Method
(	O
and	O
not	O
SAM	Method
)	O
.	O

We	O
tried	O
learning	Metric
rates	Metric
and	O
found	O
that	O
DAM	Method
trained	O
best	O
with	O
.	O

We	O
also	O
tried	O
values	O
of	O
and	O
found	O
no	O
significant	O
difference	O
in	O
performance	O
across	O
the	O
values	O
.	O

We	O
used	O
100	O
hidden	O
units	O
for	O
the	O
LSTM	Method
(	O
including	O
the	O
controller	Method
LSTMs	Method
)	O
,	O
a	O
minibatch	Method
of	O
,	O
asynchronous	Method
workers	Method
to	O
speed	O
up	O
training	Task
,	O
and	O
RMSProp	Method
to	O
optimize	O
the	O
controller	O
.	O

We	O
used	O
memory	O
access	O
heads	O
and	O
configured	O
SAM	Method
to	O
read	O
from	O
only	O
locations	O
per	O
head	O
.	O

appendix	O
:	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
Recently	O
proposed	O
a	O
novel	O
MANN	Method
the	O
Differentiable	Method
Neural	Method
Computer	Method
(	O
DNC	Method
)	O
.	O

The	O
two	O
innovations	O
proposed	O
by	O
this	O
model	O
are	O
a	O
new	O
approach	O
to	O
tracking	Task
memory	Task
freeness	Task
(	O
dynamic	Task
memory	Task
allocation	Task
)	O
and	O
a	O
mechanism	O
for	O
associating	O
memories	O
together	O
(	O
temporal	Method
memory	Method
linkage	Method
)	O
.	O

We	O
demonstrate	O
here	O
that	O
the	O
approaches	O
enumerated	O
in	O
the	O
paper	O
can	O
be	O
adapted	O
to	O
new	O
models	O
by	O
outlining	O
a	O
sparse	Method
version	Method
of	O
this	O
model	O
,	O
the	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
(	O
SDNC	Method
)	O
,	O
which	O
learns	O
with	O
similar	O
data	Metric
efficiency	Metric
while	O
retaining	O
the	O
computational	O
advantages	O
of	O
sparsity	O
.	O

subsection	O
:	O
Architecture	O
For	O
brevity	O
,	O
we	O
will	O
only	O
explain	O
the	O
sparse	O
implementations	O
of	O
these	O
two	O
items	O
,	O
for	O
the	O
full	O
model	O
details	O
refer	O
to	O
the	O
original	O
paper	O
.	O

The	O
mechanism	O
for	O
sparse	Task
memory	Task
reads	Task
and	Task
writes	Task
was	O
implemented	O
identically	O
to	O
SAM	Method
.	O

It	O
is	O
possible	O
to	O
implement	O
a	O
scalable	Method
version	Method
of	O
the	O
dynamic	Method
memory	Method
allocation	Method
system	Method
of	O
the	O
DNC	Method
avoiding	O
any	O
operations	O
by	O
using	O
a	O
heap	O
.	O

However	O
,	O
because	O
it	O
is	O
practical	O
to	O
run	O
the	O
SDNC	Method
with	O
many	O
more	O
memory	O
words	O
,	O
reusing	O
memory	O
is	O
less	O
crucial	O
so	O
we	O
did	O
not	O
implement	O
this	O
and	O
used	O
the	O
same	O
usage	O
tracking	O
as	O
in	O
SAM	Method
.	O

The	O
temporal	Method
memory	Method
linkage	Method
in	O
the	O
DNC	Method
is	O
a	O
system	O
for	O
associating	O
and	O
recalling	O
memory	O
locations	O
which	O
were	O
written	O
in	O
a	O
temporal	O
order	O
,	O
for	O
exampling	Task
storing	Task
and	Task
retrieving	Task
a	Task
list	Task
.	O

In	O
the	O
DNC	Method
this	O
is	O
done	O
by	O
maintaining	O
a	O
temporal	Method
linkage	Method
matrix	Method
.	O

represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
after	O
location	O
.	O

This	O
matrix	O
is	O
updated	O
by	O
tracking	O
the	O
precedence	O
weighting	O
,	O
where	O
represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
.	O

The	O
memory	Method
linkage	Method
is	O
updated	O
according	O
to	O
the	O
following	O
recurrence	O
The	O
temporal	Method
linkage	Method
can	O
be	O
used	O
to	O
compute	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
either	O
forward	O
or	O
backward	O
The	O
read	O
head	O
then	O
uses	O
a	O
3	O
-	O
way	O
softmax	O
to	O
select	O
between	O
a	O
content	O
-	O
based	O
read	O
or	O
following	O
the	O
forward	O
or	O
backward	O
weighting	O
.	O

Naively	O
,	O
the	O
link	Method
matrix	Method
requires	O
memory	O
and	O
computation	O
although	O
proposes	O
a	O
method	O
to	O
reduce	O
the	O
computational	Metric
cost	Metric
to	O
and	O
memory	Metric
cost	Metric
.	O

In	O
order	O
to	O
maintain	O
the	O
scaling	O
properties	O
of	O
the	O
SAM	Method
,	O
we	O
wish	O
to	O
avoid	O
any	O
computational	O
dependence	O
on	O
.	O

We	O
do	O
this	O
by	O
maintaining	O
two	O
sparse	O
matrices	O
that	O
approximate	O
and	O
respectively	O
.	O

We	O
store	O
these	O
matrices	O
in	O
Compressed	O
Sparse	O
Row	O
format	O
.	O

They	O
are	O
defined	O
by	O
the	O
following	O
updates	O
:	O
Additionally	O
,	O
is	O
,	O
as	O
with	O
the	O
other	O
weight	O
vectors	O
maintained	O
as	O
a	O
sparse	O
vector	O
with	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

This	O
means	O
that	O
the	O
outer	O
product	O
of	O
has	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

In	O
addition	O
to	O
the	O
updates	O
specified	O
above	O
,	O
we	O
also	O
constrain	O
each	O
row	O
of	O
the	O
matrices	O
and	O
to	O
have	O
at	O
most	O
non	O
-	O
zero	O
entries	O
—	O
this	O
constraint	O
can	O
be	O
applied	O
in	O
because	O
at	O
most	O
rows	O
change	O
in	O
the	O
matrix	O
.	O

Once	O
these	O
matrices	O
are	O
applied	O
the	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
can	O
be	O
computed	O
similar	O
to	O
before	O
:	O
Note	O
,	O
the	O
number	O
of	O
locations	O
we	O
read	O
from	O
,	O
,	O
does	O
not	O
have	O
to	O
equal	O
the	O
number	O
of	O
outward	O
and	O
inward	O
links	O
we	O
preserve	O
,	O
.	O

We	O
typically	O
choose	O
as	O
this	O
is	O
still	O
very	O
fast	O
to	O
compute	O
(	O
in	O
total	O
to	O
calculate	O
on	O
a	O
single	O
CPU	O
thread	O
)	O
and	O
we	O
see	O
no	O
learning	O
benefit	O
with	O
larger	O
.	O

In	O
order	O
to	O
compute	O
the	O
gradients	O
,	O
and	O
need	O
to	O
be	O
stored	O
.	O

This	O
could	O
be	O
done	O
by	O
maintaining	O
a	O
sparse	O
record	O
of	O
the	O
updates	O
applied	O
and	O
reversing	O
them	O
,	O
similar	O
to	O
that	O
performed	O
with	O
the	O
memory	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

However	O
,	O
for	O
implementation	O
simplicity	O
we	O
did	O
not	O
pass	O
gradients	O
through	O
the	O
temporal	O
linkage	O
matrices	O
.	O

subsection	O
:	O
Results	O
We	O
benchmarked	O
the	O
speed	Metric
and	Metric
memory	Metric
performance	Metric
of	O
the	O
SDNC	Method
versus	O
a	O
naive	O
DNC	Method
implementation	O
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

The	O
results	O
are	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Here	O
,	O
the	O
computational	O
benefits	O
of	O
sparsity	O
are	O
more	O
pronounced	O
due	O
to	O
the	O
expensive	O
(	O
quadratic	O
time	Metric
and	O
space	Metric
)	O
temporal	O
transition	O
table	O
operations	O
in	O
the	O
DNC	Method
.	O

We	O
were	O
only	O
able	O
to	O
run	O
comparative	O
benchmarks	O
up	O
to	O
,	O
as	O
the	O
DNC	Method
quickly	O
exceeded	O
the	O
machine	O
’s	O
physical	O
memory	O
for	O
larger	O
values	O
;	O
however	O
even	O
at	O
this	O
modest	O
memory	O
size	O
we	O
see	O
a	O
speed	O
increase	O
of	O
and	O
physical	Metric
memory	Metric
reduction	Metric
of	O
.	O

Note	O
,	O
unlike	O
the	O
SAM	Method
memory	O
benchmark	O
in	O
Section	O
[	O
reference	O
]	O
we	O
plot	O
the	O
total	Metric
memory	Metric
consumption	Metric
,	O
i.e.	O
the	O
memory	Metric
overhead	Metric
of	O
the	O
initial	O
start	O
state	O
plus	O
the	O
memory	O
overhead	O
of	O
unrolling	O
the	O
core	O
over	O
a	O
sequence	O
.	O

This	O
is	O
because	O
the	O
SDNC	Method
and	O
DNC	Method
do	O
not	O
have	O
identical	O
start	O
states	O
.	O

The	O
sparse	Method
temporal	Method
transition	Method
matrices	Method
consume	O
much	O
less	O
memory	O
than	O
the	O
corresponding	O
in	O
the	O
DNC	Method
.	O

0.47	O
0.47	O
In	O
order	O
to	O
compare	O
the	O
models	O
on	O
an	O
interesting	O
task	O
we	O
ran	O
the	O
DNC	Method
and	O
SDNC	Method
on	O
the	O
Babi	Material
task	O
(	O
this	O
task	O
is	O
described	O
more	O
fully	O
in	O
the	O
main	O
text	O
)	O
.	O

The	O
results	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
and	O
demonstrate	O
the	O
SDNC	Method
is	O
capable	O
of	O
learning	Task
competitively	O
.	O

In	O
particular	O
,	O
it	O
achieves	O
the	O
best	O
report	O
result	O
on	O
the	O
Babi	Material
task	O
.	O

appendix	O
:	O
Benchmarking	O
details	O
Each	O
model	O
contained	O
an	O
LSTM	Method
controller	O
with	O
100	O
hidden	O
units	O
,	O
an	O
external	O
memory	O
containing	O
slots	O
of	O
memory	O
,	O
with	O
word	O
size	O
and	O
access	O
heads	O
.	O

For	O
speed	Task
benchmarks	Task
,	O
a	O
minibatch	O
size	O
of	O
was	O
used	O
to	O
ensure	O
fair	O
comparison	O
-	O
as	O
many	O
dense	O
operations	O
(	O
e.g.	O
matrix	Task
multiplication	Task
)	O
can	O
be	O
batched	O
efficiently	O
.	O

For	O
memory	Task
benchmarks	Task
,	O
the	O
minibatch	O
size	O
was	O
set	O
to	O
.	O

We	O
used	O
Torch7	Method
to	O
implement	O
SAM	Method
,	O
DAM	Method
,	O
NTM	Method
,	O
DNC	Method
and	O
SDNC	Method
.	O

Eigen	Method
v3	Method
was	O
used	O
for	O
the	O
fast	O
sparse	Task
tensor	Task
operations	Task
,	O
using	O
the	O
provided	O
CSC	O
and	O
CSR	O
formats	O
.	O

All	O
benchmarks	O
were	O
run	O
on	O
a	O
Linux	O
desktop	O
running	O
Ubuntu	O
14.04.1	O
with	O
32GiB	O
of	O
RAM	O
and	O
an	O
Intel	O
Xeon	O
E5	O
-	O
1650	O
3.20GHz	O
processor	O
with	O
power	O
scaling	O
disabled	O
.	O

appendix	O
:	O
Generalization	O
on	O
associative	Task
recall	Task
appendix	O
:	O
Babi	Material
results	O
See	O
the	O
main	O
text	O
for	O
a	O
description	O
of	O
the	O
Babi	Material
task	O
and	O
its	O
relevance	O
.	O

Here	O
we	O
report	O
the	O
best	O
and	O
mean	O
results	O
for	O
all	O
of	O
the	O
models	O
on	O
this	O
task	O
.	O

The	O
task	O
was	O
encoded	O
using	O
straightforward	O
1	Method
-	Method
hot	Method
word	Method
encodings	Method
for	O
both	O
the	O
input	O
and	O
output	O
.	O

We	O
trained	O
a	O
single	O
model	O
on	O
all	O
of	O
the	O
tasks	O
,	O
and	O
used	O
the	O
10	O
,	O
000	O
examples	O
per	O
task	O
version	O
of	O
the	O
training	O
set	O
(	O
a	O
small	O
subset	O
of	O
which	O
we	O
used	O
as	O
a	O
validation	O
set	O
for	O
selecting	O
the	O
best	O
run	O
and	O
hyperparameters	O
)	O
.	O

Previous	O
work	O
has	O
reported	O
best	O
results	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
,	O
which	O
with	O
only	O
15	O
runs	O
is	O
a	O
noisy	O
comparison	O
,	O
so	O
we	O
additionally	O
report	O
the	O
mean	O
and	O
variance	O
for	O
all	O
runs	O
with	O
the	O
best	O
selected	O
hyperparameters	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
.	O

document	O
:	O
Fast	O
and	O
Accurate	O
Deep	Method
Network	Method
Learning	Method
by	O
Exponential	Method
Linear	Method
Units	Method
(	O
ELUs	Method
)	O
We	O
introduce	O
the	O
“	O
exponential	Method
linear	Method
unit	Method
”	Method
(	O
ELU	Method
)	O
which	O
speeds	O
up	O
learning	Task
in	O
deep	Method
neural	Method
networks	Method
and	O
leads	O
to	O
higher	O
classification	Metric
accuracies	Metric
.	O

Like	O
rectified	Method
linear	Method
units	Method
(	O
ReLUs	Method
)	O
,	O
leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
and	O
parametrized	Method
ReLUs	Method
(	O
PReLUs	Method
)	O
,	O
ELUs	Method
alleviate	O
the	O
vanishing	Task
gradient	Task
problem	Task
via	O
the	O
identity	O
for	O
positive	O
values	O
.	O

However	O
ELUs	Method
have	O
improved	O
learning	Metric
characteristics	Metric
compared	O
to	O
the	O
units	O
with	O
other	O
activation	O
functions	O
.	O

In	O
contrast	O
to	O
ReLUs	Method
,	O
ELUs	Method
have	O
negative	O
values	O
which	O
allows	O
them	O
to	O
push	O
mean	O
unit	O
activations	O
closer	O
to	O
zero	O
like	O
batch	Method
normalization	Method
but	O
with	O
lower	O
computational	Metric
complexity	Metric
.	O

Mean	O
shifts	O
toward	O
zero	O
speed	O
up	O
learning	Task
by	O
bringing	O
the	O
normal	O
gradient	O
closer	O
to	O
the	O
unit	O
natural	O
gradient	O
because	O
of	O
a	O
reduced	O
bias	O
shift	O
effect	O
.	O

While	O
LReLUs	Method
and	O
PReLUs	Method
have	O
negative	O
values	O
,	O
too	O
,	O
they	O
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O

ELUs	Method
saturate	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
inputs	O
and	O
thereby	O
decrease	O
the	O
forward	O
propagated	O
variation	O
and	O
information	O
.	O

Therefore	O
ELUs	Method
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
while	O
they	O
do	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O

In	O
experiments	O
,	O
ELUs	Method
lead	O
not	O
only	O
to	O
faster	O
learning	Task
,	O
but	O
also	O
to	O
significantly	O
better	O
generalization	Metric
performance	Metric
than	O
ReLUs	Method
and	O
LReLUs	Method
on	O
networks	O
with	O
more	O
than	O
5	O
layers	O
.	O

On	O
CIFAR	Method
-	Method
100	Method
ELUs	Method
networks	Method
significantly	O
outperform	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
while	O
batch	Method
normalization	Method
does	O
not	O
improve	O
ELU	Method
networks	Method
.	O

ELU	Method
networks	Method
are	O
among	O
the	O
top	O
10	O
reported	O
CIFAR	Material
-	Material
10	Material
results	O
and	O
yield	O
the	O
best	O
published	O
result	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
without	O
resorting	O
to	O
multi	Method
-	Method
view	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O

On	O
ImageNet	O
,	O
ELU	Method
networks	Method
considerably	O
speed	O
up	O
learning	Task
compared	O
to	O
a	O
ReLU	Method
network	O
with	O
the	O
same	O
architecture	O
,	O
obtaining	O
less	O
than	O
10	O
%	O
classification	Metric
error	Metric
for	O
a	O
single	O
crop	Method
,	Method
single	Method
model	Method
network	Method
.	O

section	O
:	O
Introduction	O
Currently	O
the	O
most	O
popular	O
activation	Method
function	Method
for	O
neural	Method
networks	Method
is	O
the	O
rectified	Method
linear	Method
unit	Method
(	O
ReLU	Method
)	O
,	O
which	O
was	O
first	O
proposed	O
for	O
restricted	Method
Boltzmann	Method
machines	Method
and	O
then	O
successfully	O
used	O
for	O
neural	Task
networks	Task
.	O

The	O
ReLU	Method
activation	O
function	O
is	O
the	O
identity	O
for	O
positive	O
arguments	O
and	O
zero	O
otherwise	O
.	O

Besides	O
producing	O
sparse	Method
codes	Method
,	O
the	O
main	O
advantage	O
of	O
ReLUs	Method
is	O
that	O
they	O
alleviate	O
the	O
vanishing	Task
gradient	Task
problem	Task
since	O
the	O
derivative	O
of	O
1	O
for	O
positive	O
values	O
is	O
not	O
contractive	O
.	O

However	O
ReLUs	O
are	O
non	O
-	O
negative	O
and	O
,	O
therefore	O
,	O
have	O
a	O
mean	O
activation	O
larger	O
than	O
zero	O
.	O

Units	O
that	O
have	O
a	O
non	O
-	O
zero	O
mean	O
activation	O
act	O
as	O
bias	O
for	O
the	O
next	O
layer	O
.	O

If	O
such	O
units	O
do	O
not	O
cancel	O
each	O
other	O
out	O
,	O
learning	O
causes	O
a	O
bias	O
shift	O
for	O
units	O
in	O
next	O
layer	O
.	O

The	O
more	O
the	O
units	O
are	O
correlated	O
,	O
the	O
higher	O
their	O
bias	O
shift	O
.	O

We	O
will	O
see	O
that	O
Fisher	Method
optimal	Method
learning	Method
,	O
i.e.	O
,	O
the	O
natural	O
gradient	O
,	O
would	O
correct	O
for	O
the	O
bias	O
shift	O
by	O
adjusting	O
the	O
weight	O
updates	O
.	O

Thus	O
,	O
less	O
bias	O
shift	O
brings	O
the	O
standard	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
and	O
speeds	O
up	O
learning	Task
.	O

We	O
aim	O
at	O
activation	O
functions	O
that	O
push	O
activation	O
means	O
closer	O
to	O
zero	O
to	O
decrease	O
the	O
bias	O
shift	O
effect	O
.	O

Centering	O
the	O
activations	O
at	O
zero	O
has	O
been	O
proposed	O
in	O
order	O
to	O
keep	O
the	O
off	O
-	O
diagonal	O
entries	O
of	O
the	O
Fisher	O
information	O
matrix	O
small	O
.	O

For	O
neural	Method
network	Method
it	O
is	O
known	O
that	O
centering	O
the	O
activations	O
speeds	O
up	O
learning	Task
.	O

“	O
Batch	Method
normalization	Method
”	O
also	O
centers	O
activations	O
with	O
the	O
goal	O
to	O
counter	O
the	O
internal	O
covariate	O
shift	O
.	O

Also	O
the	O
Projected	Method
Natural	Method
Gradient	Method
Descent	Method
algorithm	Method
(	O
PRONG	Method
)	O
centers	O
the	O
activations	O
by	O
implicitly	O
whitening	O
them	O
.	O

An	O
alternative	O
to	O
centering	O
is	O
to	O
push	O
the	O
mean	O
activation	O
toward	O
zero	O
by	O
an	O
appropriate	O
activation	Method
function	Method
.	O

Therefore	O
has	O
been	O
preferred	O
over	O
logistic	Method
functions	Method
.	O

Recently	O
“	O
Leaky	Method
ReLUs	Method
”	Method
(	O
LReLUs	Method
)	O
that	O
replace	O
the	O
negative	O
part	O
of	O
the	O
ReLU	Method
with	O
a	O
linear	O
function	O
have	O
been	O
shown	O
to	O
be	O
superior	O
to	O
ReLUs	Method
.	O

Parametric	Method
Rectified	Method
Linear	Method
Units	Method
(	O
PReLUs	Method
)	O
generalize	O
LReLUs	Method
by	O
learning	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
yielded	O
improved	O
learning	O
behavior	O
on	O
large	O
image	O
benchmark	O
data	O
sets	O
.	O

Another	O
variant	O
are	O
Randomized	Method
Leaky	Method
Rectified	Method
Linear	Method
Units	Method
(	O
RReLUs	Method
)	O
which	O
randomly	O
sample	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
raised	O
the	O
performance	O
on	O
image	O
benchmark	O
datasets	O
and	O
convolutional	Method
networks	Method
.	O

In	O
contrast	O
to	O
ReLUs	O
,	O
activation	O
functions	O
like	O
LReLUs	Method
,	O
PReLUs	O
,	O
and	O
RReLUs	Method
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O

We	O
propose	O
an	O
activation	Method
function	Method
that	O
has	O
negative	O
values	O
to	O
allow	O
for	O
mean	O
activations	O
close	O
to	O
zero	O
,	O
but	O
which	O
saturates	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
arguments	O
.	O

The	O
saturation	O
decreases	O
the	O
variation	O
of	O
the	O
units	O
if	O
deactivated	O
,	O
so	O
the	O
precise	O
deactivation	O
argument	O
is	O
less	O
relevant	O
.	O

Such	O
an	O
activation	Method
function	Method
can	O
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
but	O
does	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O

Therefore	O
,	O
such	O
an	O
activation	Method
function	Method
is	O
more	O
robust	O
to	O
noise	O
.	O

Consequently	O
,	O
dependencies	O
between	O
coding	O
units	O
are	O
much	O
easier	O
to	O
model	O
and	O
much	O
easier	O
to	O
interpret	O
since	O
only	O
activated	O
code	O
units	O
carry	O
much	O
information	O
.	O

Furthermore	O
,	O
distinct	O
concepts	O
are	O
much	O
less	O
likely	O
to	O
interfere	O
with	O
such	O
activation	O
functions	O
since	O
the	O
deactivation	O
state	O
is	O
non	O
-	O
informative	O
,	O
i.e.	O
variance	O
decreasing	O
.	O

section	O
:	O
Bias	Method
Shift	Method
Correction	Method
Speeds	O
Up	O
Learning	Task
To	O
derive	O
and	O
analyze	O
the	O
bias	O
shift	O
effect	O
mentioned	O
in	O
the	O
introduction	O
,	O
we	O
utilize	O
the	O
natural	O
gradient	O
.	O

The	O
natural	O
gradient	O
corrects	O
the	O
gradient	O
direction	O
with	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
and	O
,	O
thereby	O
,	O
enables	O
Fisher	Method
optimal	Method
learning	Method
,	O
which	O
ensures	O
the	O
steepest	O
descent	O
in	O
the	O
Riemannian	O
parameter	O
manifold	O
and	O
Fisher	Metric
efficiency	Metric
for	O
online	Task
learning	Task
.	O

The	O
recently	O
introduced	O
Hessian	Method
-	Method
Free	Method
Optimization	Method
technique	Method
and	O
the	O
Krylov	Method
Subspace	Method
Descent	Method
methods	Method
use	O
an	O
extended	Method
Gauss	Method
-	Method
Newton	Method
approximation	Method
of	O
the	O
Hessian	O
,	O
therefore	O
they	O
can	O
be	O
interpreted	O
as	O
versions	O
of	O
natural	Method
gradient	Method
descent	Method
.	O

Since	O
for	O
neural	Method
networks	Method
the	O
Fisher	O
information	O
matrix	O
is	O
typically	O
too	O
expensive	O
to	O
compute	O
,	O
different	O
approximations	O
of	O
the	O
natural	O
gradient	O
have	O
been	O
proposed	O
.	O

Topmoumoute	Method
Online	Method
natural	Method
Gradient	Method
Algorithm	Method
(	O
TONGA	Method
)	O
uses	O
a	O
low	Method
-	Method
rank	Method
approximation	Method
of	Method
natural	Method
gradient	Method
descent	Method
.	O

FActorized	Method
Natural	Method
Gradient	Method
(	O
FANG	Method
)	O
estimates	O
the	O
natural	O
gradient	O
via	O
an	O
approximation	Method
of	Method
the	Method
Fisher	Method
information	Method
matrix	Method
by	O
a	O
Gaussian	Method
graphical	Method
model	Method
.	O

The	O
Fisher	O
information	O
matrix	O
can	O
be	O
approximated	O
by	O
a	O
block	Method
-	Method
diagonal	Method
matrix	Method
,	O
where	O
unit	O
or	O
quasi	O
-	O
diagonal	O
natural	O
gradients	O
are	O
used	O
.	O

Unit	O
natural	O
gradients	O
or	O
“	O
Unitwise	Method
Fisher	Method
’s	Method
scoring	Method
”	Method
are	O
based	O
on	O
natural	O
gradients	O
for	O
perceptrons	Method
.	O

We	O
will	O
base	O
our	O
analysis	O
on	O
the	O
unit	O
natural	O
gradient	O
.	O

We	O
assume	O
a	O
parameterized	Method
probabilistic	Method
model	Method
with	O
parameter	O
vector	O
and	O
data	O
.	O

The	O
training	O
data	O
are	O
with	O
,	O
where	O
is	O
the	O
input	O
for	O
example	O
and	O
is	O
its	O
label	O
.	O

is	O
the	O
loss	O
of	O
example	O
using	O
model	O
.	O

The	O
average	Metric
loss	Metric
on	O
the	O
training	O
data	O
is	O
the	O
empirical	O
risk	O
.	O

Gradient	Method
descent	Method
updates	O
the	O
weight	O
vector	O
by	O
where	O
is	O
the	O
learning	Metric
rate	Metric
.	O

The	O
natural	O
gradient	O
is	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
multiplied	O
by	O
the	O
gradient	O
of	O
the	O
empirical	O
risk	O
:	O
.	O

For	O
a	O
multi	Method
-	Method
layer	Method
perceptron	Method
is	O
the	O
unit	O
activation	O
vector	O
and	O
is	O
the	O
bias	O
unit	O
activation	O
.	O

We	O
consider	O
the	O
ingoing	O
weights	O
to	O
unit	O
,	O
therefore	O
we	O
drop	O
the	O
index	O
:	O
for	O
the	O
weight	O
from	O
unit	O
to	O
unit	O
,	O
for	O
the	O
activation	O
,	O
and	O
for	O
the	O
bias	O
weight	O
of	O
unit	O
.	O

The	O
activation	Method
function	Method
maps	O
the	O
net	O
input	O
of	O
unit	O
to	O
its	O
activation	O
.	O

For	O
computing	O
the	O
Fisher	O
information	O
matrix	O
,	O
the	O
derivative	O
of	O
the	O
log	O
-	O
output	O
probability	O
is	O
required	O
.	O

Therefore	O
we	O
define	O
the	O
at	O
unit	O
as	O
,	O
which	O
can	O
be	O
computed	O
via	O
backpropagation	Method
,	O
but	O
using	O
the	O
log	O
-	O
output	O
probability	O
instead	O
of	O
the	O
conventional	O
loss	O
function	O
.	O

The	O
derivative	O
is	O
.	O

We	O
restrict	O
the	O
Fisher	O
information	O
matrix	O
to	O
weights	O
leading	O
to	O
unit	O
which	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
.	O

captures	O
only	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
.	O

Consequently	O
,	O
the	O
unit	O
natural	O
gradient	O
only	O
corrects	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
,	O
i.e.	O
considers	O
the	O
Riemannian	O
parameter	O
manifold	O
only	O
in	O
a	O
subspace	O
.	O

The	O
unit	O
Fisher	O
information	O
matrix	O
is	O
Weighting	O
the	O
activations	O
by	O
is	O
equivalent	O
to	O
adjusting	O
the	O
probability	O
of	O
drawing	O
inputs	O
.	O

Inputs	O
with	O
large	O
are	O
drawn	O
with	O
higher	O
probability	O
.	O

Since	O
,	O
we	O
can	O
define	O
a	O
distribution	O
:	O
Using	O
,	O
the	O
entries	O
of	O
can	O
be	O
expressed	O
as	O
second	O
moments	O
:	O
If	O
the	O
bias	O
unit	O
is	O
with	O
weight	O
then	O
the	O
weight	O
vector	O
can	O
be	O
divided	O
into	O
a	O
bias	O
part	O
and	O
the	O
rest	O
:	O
.	O

For	O
the	O
row	O
that	O
corresponds	O
to	O
the	O
bias	O
weight	O
,	O
we	O
have	O
:	O
The	O
next	O
Theorem	O
[	O
reference	O
]	O
gives	O
the	O
correction	O
of	O
the	O
standard	O
gradient	O
by	O
the	O
unit	O
natural	O
gradient	O
where	O
the	O
bias	O
weight	O
is	O
treated	O
separately	O
(	O
see	O
also	O
)	O
.	O

theorem	O
:	O
.	O

The	O
unit	O
natural	O
gradient	O
corrects	O
the	O
weight	O
update	O
(	O
⁢ΔwT	O
,	O
⁢Δw0	O
)	O
T	O
to	O
a	O
unit	O
i	O
by	O
following	O
affine	O
transformation	O
of	O
the	O
gradient	O
=	O
∇	O
(	O
wT	O
,	O
w0	O
)	O
TRemp	O
(	O
gT	O
,	O
g0	O
)	O
T	O
:	O
where	O
A=	O
[	O
⁢F	O
(	O
w	O
)]	O
⁢¬0	O
,	O
⁢¬0=⁢E⁢p	O
(	O
z	O
)(	O
δ2	O
)	O
E⁢q	O
(	O
z	O
)(	O
⁢aaT	O
)	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
without	O
row	O
0	O
and	O
column	O
0	O
corresponding	O
to	O
the	O
bias	O
weight	O
.	O

The	O
vector	O
=	O
b	O
[	O
⁢F	O
(	O
w	O
)]	O
0	O
is	O
the	O
zeroth	O
column	O
of	O
F	O
corresponding	O
to	O
the	O
bias	O
weight	O
,	O
and	O
the	O
positive	O
scalar	O
s	O
is	O
where	O
a	O
is	O
the	O
vector	O
of	O
activations	O
of	O
units	O
with	O
weights	O
to	O
unit	O
i	O
and	O
=	O
⁢q	O
(	O
z	O
)	O
⁢δ2	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
E⁢p	O
(	O
z	O
)-	O
1	O
(	O
δ2	O
)	O
.	O

proof	O
:	O
Proof	O
.	O

Multiplying	O
the	O
inverse	O
Fisher	O
matrix	O
with	O
the	O
separated	O
gradient	O
gives	O
the	O
weight	Method
update	Method
:	O
where	O
The	O
previous	O
formula	O
is	O
derived	O
in	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O

Using	O
in	O
the	O
update	O
gives	O
The	O
right	O
hand	O
side	O
is	O
obtained	O
by	O
inserting	O
in	O
the	O
left	O
hand	O
side	O
update	O
.	O

Since	O
,	O
,	O
and	O
,	O
we	O
obtain	O
Applying	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
gives	O
the	O
formula	O
for	O
.	O

∎	O
The	O
bias	O
shift	O
(	O
mean	O
shift	O
)	O
of	O
unit	O
is	O
the	O
change	O
of	O
unit	O
’s	O
mean	O
value	O
due	O
to	O
the	O
weight	Method
update	Method
.	O

Bias	O
shifts	O
of	O
unit	O
lead	O
to	O
oscillations	O
and	O
impede	O
learning	Task
.	O

See	O
Section	O
4.4	O
in	O
for	O
demonstrating	O
this	O
effect	O
at	O
the	O
inputs	O
and	O
in	O
for	O
explaining	O
this	O
effect	O
using	O
the	O
input	O
covariance	O
matrix	O
.	O

Such	O
bias	O
shifts	O
are	O
mitigated	O
or	O
even	O
prevented	O
by	O
the	O
unit	O
natural	O
gradient	O
.	O

The	O
bias	Task
shift	Task
correction	Task
of	O
the	O
unit	O
natural	O
gradient	O
is	O
the	O
effect	O
on	O
the	O
bias	O
shift	O
due	O
to	O
which	O
captures	O
the	O
interaction	O
between	O
the	O
bias	O
unit	O
and	O
the	O
incoming	O
units	O
.	O

Without	O
bias	Method
shift	Method
correction	Method
,	O
i.e.	O
,	O
and	O
,	O
the	O
weight	O
updates	O
are	O
and	O
.	O

As	O
only	O
the	O
activations	O
depend	O
on	O
the	O
input	O
,	O
the	O
bias	O
shift	O
can	O
be	O
computed	O
by	O
multiplying	O
the	O
weight	O
update	O
by	O
the	O
mean	O
of	O
the	O
activation	O
vector	O
.	O

Thus	O
we	O
obtain	O
the	O
bias	O
shift	O
.	O

The	O
bias	O
shift	O
strongly	O
depends	O
on	O
the	O
correlation	O
of	O
the	O
incoming	O
units	O
which	O
is	O
captured	O
by	O
.	O

Next	O
,	O
Theorem	O
[	O
reference	O
]	O
states	O
that	O
the	O
bias	Method
shift	Method
correction	Method
by	O
the	O
unit	O
natural	O
gradient	O
can	O
be	O
considered	O
to	O
correct	O
the	O
incoming	O
mean	O
proportional	O
to	O
toward	O
zero	O
.	O

theorem	O
:	O
.	O

The	O
bias	O
shift	O
correction	O
by	O
the	O
unit	O
natural	O
gradient	O
is	O
equivalent	O
to	O
an	O
additive	Method
correction	Method
of	O
the	O
incoming	O
mean	O
by	O
-	O
⁢kE⁢q	O
(	O
z	O
)(	O
a	O
)	O
and	O
a	O
multiplicative	Method
correction	Method
of	O
the	O
bias	O
unit	O
by	O
k	O
,	O
where	O
proof	O
:	O
Proof	O
.	O

Using	O
,	O
the	O
bias	O
shift	O
is	O
:	O
The	O
mean	O
correction	O
term	O
,	O
indicated	O
by	O
an	O
underbrace	O
in	O
previous	O
formula	O
,	O
is	O
The	O
expression	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
for	O
follows	O
from	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O

The	O
bias	O
unit	O
correction	O
term	O
is	O
.	O

∎	O
In	O
Theorem	O
[	O
reference	O
]	O
we	O
can	O
reformulate	O
.	O

Therefore	O
increases	O
with	O
the	O
length	O
of	O
for	O
given	O
variances	O
and	O
covariances	O
.	O

Consequently	O
the	O
bias	Task
shift	Task
correction	Task
through	O
the	O
unit	O
natural	O
gradient	O
is	O
governed	O
by	O
the	O
length	O
of	O
.	O

The	O
bias	Method
shift	Method
correction	Method
is	O
zero	O
for	O
since	O
does	O
not	O
correct	O
the	O
bias	O
unit	O
multiplicatively	O
.	O

Using	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
is	O
split	O
into	O
an	O
offset	O
and	O
an	O
information	O
containing	O
term	O
:	O
In	O
general	O
,	O
smaller	O
positive	O
⁢E⁢p	O
(	O
z	O
)(	O
a	O
)	O
lead	O
to	O
smaller	O
positive	O
⁢E⁢q	O
(	O
z	O
)(	O
a	O
)	O
,	O
therefore	O
to	O
smaller	O
corrections	O
.	O

The	O
reason	O
is	O
that	O
in	O
general	O
the	O
largest	O
absolute	O
components	O
of	O
are	O
positive	O
,	O
since	O
activated	O
inputs	O
will	O
activate	O
the	O
unit	O
which	O
in	O
turn	O
will	O
have	O
large	O
impact	O
on	O
the	O
output	O
.	O

To	O
summarize	O
,	O
the	O
unit	O
natural	O
gradient	O
corrects	O
the	O
bias	O
shift	O
of	O
unit	O
via	O
the	O
interactions	O
of	O
incoming	O
units	O
with	O
the	O
bias	O
unit	O
to	O
ensure	O
efficient	O
learning	Task
.	O

This	O
correction	O
is	O
equivalent	O
to	O
shifting	O
the	O
mean	O
activations	O
of	O
the	O
incoming	O
units	O
toward	O
zero	O
and	O
scaling	O
up	O
the	O
bias	O
unit	O
.	O

To	O
reduce	O
the	O
undesired	O
bias	O
shift	O
effect	O
without	O
the	O
natural	O
gradient	O
,	O
either	O
the	O
(	O
i	O
)	O
activation	O
of	O
incoming	O
units	O
can	O
be	O
centered	O
at	O
zero	O
or	O
(	O
ii	O
)	O
activation	O
functions	O
with	O
negative	O
values	O
can	O
be	O
used	O
.	O

We	O
introduce	O
a	O
new	O
activation	Method
function	Method
with	O
negative	O
values	O
while	O
keeping	O
the	O
identity	O
for	O
positive	O
arguments	O
where	O
it	O
is	O
not	O
contradicting	O
.	O

section	O
:	O
Exponential	Method
Linear	Method
Units	Method
(	O
ELUs	Method
)	O
The	O
exponential	Method
linear	Method
unit	Method
(	O
ELU	Method
)	O
with	O
is	O
The	O
ELU	Method
hyperparameter	O
controls	O
the	O
value	O
to	O
which	O
an	O
ELU	Method
saturates	O
for	O
negative	O
net	O
inputs	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

ELUs	Method
diminish	O
the	O
vanishing	O
gradient	O
effect	O
as	O
rectified	O
linear	O
units	O
(	O
ReLUs	Method
)	O
and	O
leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
do	O
.	O

The	O
vanishing	Task
gradient	Task
problem	Task
is	O
alleviated	O
because	O
the	O
positive	O
part	O
of	O
these	O
functions	O
is	O
the	O
identity	O
,	O
therefore	O
their	O
derivative	O
is	O
one	O
and	O
not	O
contractive	O
.	O

In	O
contrast	O
,	O
and	O
sigmoid	Method
activation	Method
functions	Method
are	O
contractive	O
almost	O
everywhere	O
.	O

In	O
contrast	O
to	O
ReLUs	O
,	O
ELUs	Method
have	O
negative	O
values	O
which	O
pushes	O
the	O
mean	O
of	O
the	O
activations	O
closer	O
to	O
zero	O
.	O

Mean	O
activations	O
that	O
are	O
closer	O
to	O
zero	O
enable	O
faster	O
learning	Task
as	O
they	O
bring	O
the	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
(	O
see	O
Theorem	O
[	O
reference	O
]	O
and	O
text	O
thereafter	O
)	O
.	O

ELUs	Method
saturate	O
to	O
a	O
negative	O
value	O
when	O
the	O
argument	O
gets	O
smaller	O
.	O

Saturation	O
means	O
a	O
small	O
derivative	O
which	O
decreases	O
the	O
variation	O
and	O
the	O
information	O
that	O
is	O
propagated	O
to	O
the	O
next	O
layer	O
.	O

Therefore	O
the	O
representation	O
is	O
both	O
noise	O
-	O
robust	O
and	O
low	O
-	O
complex	O
.	O

ELUs	Method
code	O
the	O
degree	O
of	O
presence	O
of	O
input	O
concepts	O
,	O
while	O
they	O
neither	O
quantify	O
the	O
degree	O
of	O
their	O
absence	O
nor	O
distinguish	O
the	O
causes	O
of	O
their	O
absence	O
.	O

This	O
property	O
of	O
non	O
-	O
informative	O
deactivation	O
states	O
is	O
also	O
present	O
at	O
ReLUs	Method
and	O
allowed	O
to	O
detect	O
biclusters	O
corresponding	O
to	O
biological	O
modules	O
in	O
gene	O
expression	O
datasets	O
and	O
to	O
identify	O
toxicophores	Task
in	Task
toxicity	Task
prediction	Task
.	O

The	O
enabling	O
features	O
for	O
these	O
interpretations	O
is	O
that	O
activation	Task
can	O
be	O
clearly	O
distinguished	O
from	O
deactivation	Method
and	O
that	O
only	O
active	O
units	O
carry	O
relevant	O
information	O
and	O
can	O
crosstalk	O
.	O

section	O
:	O
Experiments	O
Using	O
ELUs	Method
In	O
this	O
section	O
,	O
we	O
assess	O
the	O
performance	O
of	O
exponential	Method
linear	Method
units	Method
(	O
ELUs	Method
)	O
if	O
used	O
for	O
unsupervised	Task
and	O
supervised	Task
learning	Task
of	Task
deep	Task
autoencoders	Task
and	O
deep	Method
convolutional	Method
networks	Method
.	O

ELUs	Method
with	O
are	O
compared	O
to	O
(	O
i	O
)	O
Rectified	Method
Linear	Method
Units	Method
(	O
ReLUs	Method
)	O
with	O
activation	O
,	O
(	O
ii	O
)	O
Leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
with	O
activation	O
(	O
)	O
,	O
and	O
(	O
iii	O
)	O
Shifted	Method
ReLUs	Method
(	O
SReLUs	Method
)	O
with	O
activation	O
.	O

Comparisons	O
are	O
done	O
with	O
and	O
without	O
batch	Method
normalization	Method
.	O

The	O
following	O
benchmark	O
datasets	O
are	O
used	O
:	O
(	O
i	O
)	O
MNIST	O
(	O
gray	O
images	O
in	O
10	O
classes	O
,	O
60k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
ii	O
)	O
CIFAR	Material
-	Material
10	Material
(	O
color	O
images	O
in	O
10	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
iii	O
)	O
CIFAR	Material
-	Material
100	Material
(	O
color	O
images	O
in	O
100	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
and	O
(	O
iv	O
)	O
ImageNet	O
(	O
color	O
images	O
in	O
1	O
,	O
000	O
classes	O
,	O
1.3	O
M	O
train	O
and	O
100k	O
tests	O
)	O
.	O

subsection	O
:	O
MNIST	O
subsubsection	O
:	O
Learning	Task
Behavior	Task
We	O
first	O
want	O
to	O
verify	O
that	O
ELUs	Method
keep	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
than	O
other	O
units	O
.	O

Fully	Method
connected	Method
deep	Method
neural	Method
networks	Method
with	O
ELUs	Method
(	Method
)	Method
,	O
ReLUs	Method
,	O
and	O
LReLUs	Method
(	O
)	O
were	O
trained	O
on	O
the	O
MNIST	O
digit	O
classification	O
dataset	O
while	O
each	O
hidden	O
unit	O
’s	O
activation	O
was	O
tracked	O
.	O

Each	O
network	O
had	O
eight	O
hidden	O
layers	O
of	O
128	O
units	O
each	O
,	O
and	O
was	O
trained	O
for	O
300	O
epochs	O
by	O
stochastic	Method
gradient	Method
descent	Method
with	O
learning	Method
rate	Method
and	O
mini	O
-	O
batches	O
of	O
size	O
64	O
.	O

The	O
weights	O
have	O
been	O
initialized	O
according	O
to	O
.	O

After	O
each	O
epoch	O
we	O
calculated	O
the	O
units	O
’	O
average	O
activations	O
on	O
a	O
fixed	O
subset	O
of	O
the	O
training	O
data	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
median	O
over	O
all	O
units	O
along	O
learning	O
.	O

ELUs	Method
stay	O
have	O
smaller	O
median	O
throughout	O
the	O
training	O
process	O
.	O

The	O
training	Metric
error	Metric
of	O
ELU	Method
networks	Method
decreases	O
much	O
more	O
rapidly	O
than	O
for	O
the	O
other	O
networks	O
.	O

Section	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
compares	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	Method
and	O
ELU	Method
networks	Method
.	O

The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	Method
networks	O
.	O

This	O
indicates	O
that	O
ReLU	Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	Method
networks	Method
.	O

subsubsection	O
:	O
Autoencoder	Method
Learning	Method
To	O
evaluate	O
ELU	Method
networks	Method
at	O
unsupervised	Task
settings	O
,	O
we	O
followed	O
and	O
and	O
trained	O
a	O
deep	Method
autoencoder	Method
on	O
the	O
MNIST	O
dataset	O
.	O

The	O
encoder	Method
part	Method
consisted	O
of	O
four	O
fully	Method
connected	Method
hidden	Method
layers	Method
with	O
sizes	O
1000	O
,	O
500	O
,	O
250	O
and	O
30	O
,	O
respectively	O
.	O

The	O
decoder	O
part	O
was	O
symmetrical	O
to	O
the	O
encoder	O
.	O

For	O
learning	Task
we	O
applied	O
stochastic	Method
gradient	Method
descent	Method
with	O
mini	O
-	O
batches	O
of	O
64	O
samples	O
for	O
500	O
epochs	O
using	O
the	O
fixed	O
learning	Metric
rates	Metric
(	O
)	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
,	O
that	O
ELUs	Method
outperform	O
the	O
competing	O
activation	Method
functions	Method
in	O
terms	O
of	O
training	Metric
/	Metric
test	Metric
set	Metric
reconstruction	Metric
error	Metric
for	O
all	O
learning	Metric
rates	Metric
.	O

As	O
already	O
noted	O
by	O
,	O
higher	O
learning	Metric
rates	Metric
seem	O
to	O
perform	O
better	O
.	O

subsection	O
:	O
Comparison	O
of	O
Activation	Method
Functions	Method
In	O
this	O
subsection	O
we	O
show	O
that	O
ELUs	Method
indeed	O
possess	O
a	O
superior	O
learning	Metric
behavior	Metric
compared	O
to	O
other	O
activation	Method
functions	Method
as	O
postulated	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Furthermore	O
we	O
show	O
that	O
ELU	Method
networks	Method
perform	O
better	O
than	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
.	O

We	O
use	O
as	O
benchmark	O
dataset	O
CIFAR	Material
-	Material
100	Material
and	O
use	O
a	O
relatively	O
simple	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
architecture	O
to	O
keep	O
the	O
computational	Metric
complexity	Metric
reasonable	O
for	O
comparisons	O
.	O

[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
The	O
CNN	Method
for	O
these	O
CIFAR	Material
-	Material
100	Material
experiments	O
consists	O
of	O
11	O
convolutional	Method
layers	Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
.	O

2	O
2	O
max	Method
-	Method
pooling	Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
.	O

For	O
network	Task
regularization	Task
we	O
used	O
the	O
following	O
drop	Metric
-	Metric
out	Metric
rate	Metric
for	O
the	O
last	O
layer	O
of	O
each	O
stack	O
(	O
)	O
.	O

The	O
-	O
weight	O
decay	O
regularization	O
term	O
was	O
set	O
to	O
.	O

The	O
following	O
learning	Metric
rate	Metric
schedule	Metric
was	O
applied	O
(	O
)	O
(	O
iterations	O
[	O
learning	O
rate	O
]	O
)	O
.	O

For	O
fair	O
comparisons	O
,	O
we	O
used	O
this	O
learning	Method
rate	Method
schedule	Method
for	O
all	O
networks	O
.	O

During	O
previous	O
experiments	O
,	O
this	O
schedule	O
was	O
optimized	O
for	O
ReLU	Method
networks	O
,	O
however	O
as	O
ELUs	Method
converge	O
faster	O
they	O
would	O
benefit	O
from	O
an	O
adjusted	O
schedule	O
.	O

The	O
momentum	Metric
term	Metric
learning	Metric
rate	Metric
was	O
fixed	O
to	O
0.9	O
.	O

The	O
dataset	O
was	O
preprocessed	O
as	O
described	O
in	O
with	O
global	Method
contrast	Method
normalization	Method
and	O
ZCA	Method
whitening	Method
.	O

Additionally	O
,	O
the	O
images	O
were	O
padded	O
with	O
four	O
zero	O
pixels	O
at	O
all	O
borders	O
.	O

The	O
model	O
was	O
trained	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O

Besides	O
that	O
,	O
we	O
no	O
further	O
augmented	O
the	O
dataset	O
during	O
training	O
.	O

Each	O
network	O
was	O
run	O
10	O
times	O
with	O
different	O
weight	Method
initialization	Method
.	O

Across	O
networks	O
with	O
different	O
activation	O
functions	O
the	O
same	O
run	O
number	O
had	O
the	O
same	O
initial	O
weights	O
.	O

Mean	Metric
test	Metric
error	Metric
results	O
of	O
networks	O
with	O
different	O
activation	O
functions	O
are	O
compared	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
which	O
also	O
shows	O
the	O
standard	O
deviation	O
.	O

ELUs	Method
yield	O
on	O
average	O
a	O
test	Metric
error	Metric
of	O
28.75	O
(	O
0.24	O
)	O
%	O
,	O
while	O
SReLUs	O
,	O
ReLUs	O
and	O
LReLUs	Method
yield	O
29.35	O
(	O
0.29	O
)	O
%	O
,	O
31.56	O
(	O
0.37	O
)	O
%	O
and	O
30.59	O
(	O
0.29	O
)	O
%	O
,	O
respectively	O
.	O

ELUs	Method
achieve	O
both	O
lower	O
training	Metric
loss	Metric
and	O
lower	O
test	Metric
error	Metric
than	O
ReLUs	Method
,	O
LReLUs	Method
,	O
and	O
SReLUs	Method
.	O

Both	O
the	O
ELU	Method
training	Method
and	O
test	O
performance	O
is	O
significantly	O
better	O
than	O
for	O
other	O
activation	Method
functions	Method
(	O
Wilcoxon	O
signed	O
-	O
rank	O
test	O
with	O
-	O
value	O
0.001	O
)	O
.	O

Batch	Method
normalization	Method
improved	O
ReLU	Method
and	O
LReLU	Method
networks	Method
,	O
but	O
did	O
not	O
improve	O
ELU	Method
and	O
SReLU	Method
networks	Method
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

ELU	Method
networks	Method
significantly	O
outperform	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
(	O
Wilcoxon	Metric
signed	Metric
-	Metric
rank	Metric
test	Metric
with	O
-	O
value	O
0.001	O
)	O
.	O

subsection	O
:	O
Classification	Metric
Performance	Metric
on	O
CIFAR	Material
-	Material
100	Material
and	O
CIFAR	Material
-	Material
10	Material
The	O
following	O
experiments	O
should	O
highlight	O
the	O
generalization	Method
capabilities	Method
of	O
ELU	Method
networks	Method
.	O

The	O
CNN	Method
architecture	O
is	O
more	O
sophisticated	O
than	O
in	O
the	O
previous	O
subsection	O
and	O
consists	O
of	O
18	O
convolutional	Method
layers	Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
.	O

Initial	O
drop	Metric
-	Metric
out	Metric
rate	Metric
,	O
Max	Method
-	Method
pooling	Method
after	O
each	O
stack	O
,	O
-	O
weight	O
decay	O
,	O
momentum	O
term	O
,	O
data	Method
preprocessing	Method
,	O
padding	O
,	O
and	O
cropping	O
were	O
as	O
in	O
previous	O
section	O
.	O

The	O
initial	O
learning	Metric
rate	Metric
was	O
set	O
to	O
0.01	O
and	O
decreased	O
by	O
a	O
factor	O
of	O
10	O
after	O
35k	O
iterations	O
.	O

The	O
mini	O
-	O
batch	O
size	O
was	O
100	O
.	O

For	O
the	O
final	O
50k	O
iterations	O
fine	O
-	O
tuning	O
we	O
increased	O
the	O
drop	Metric
-	Metric
out	Metric
rate	Metric
for	O
all	O
layers	O
in	O
a	O
stack	O
to	O
(	O
)	O
,	O
thereafter	O
increased	O
the	O
drop	Metric
-	Metric
out	Metric
rate	Metric
by	O
a	O
factor	O
of	O
1.5	O
for	O
40k	O
additional	O
iterations	O
.	O

ELU	Method
networks	Method
are	O
compared	O
to	O
following	O
recent	O
successful	O
CNN	Method
architectures	O
:	O
AlexNet	Method
,	O
DSN	Method
,	O
NiN	Method
,	O
Maxout	Method
,	O
All	O
-	O
CNN	Method
,	O
Highway	Method
Network	Method
and	O
Fractional	Method
Max	Method
-	Method
Pooling	Method
.	O

The	O
test	Metric
error	Metric
in	O
percent	Metric
misclassification	Metric
are	O
given	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

ELU	Method
-	Method
networks	Method
are	O
the	O
second	O
best	O
on	O
CIFAR	Material
-	Material
10	Material
with	O
a	O
test	Metric
error	Metric
of	O
6.55	O
%	O
but	O
still	O
they	O
are	O
among	O
the	O
top	O
10	O
best	O
results	O
reported	O
for	O
CIFAR	Material
-	Material
10	Material
.	O

ELU	Method
networks	Method
performed	O
best	O
on	O
CIFAR	Material
-	Material
100	Material
with	O
a	O
test	Metric
error	Metric
of	O
24.28	O
%	O
.	O

This	O
is	O
the	O
best	O
published	O
result	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
without	O
even	O
resorting	O
to	O
multi	Method
-	Method
view	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O

subsection	O
:	O
ImageNet	O
Challenge	O
Dataset	O
Finally	O
,	O
we	O
evaluated	O
ELU	Method
-	Method
networks	Method
on	O
the	O
1000	O
-	O
class	O
ImageNet	O
dataset	O
.	O

It	O
contains	O
about	O
1.3	O
M	O
training	O
color	O
images	O
as	O
well	O
as	O
additional	O
50k	O
images	O
and	O
100k	O
images	O
for	O
validation	O
and	O
testing	O
,	O
respectively	O
.	O

For	O
this	O
task	O
,	O
we	O
designed	O
a	O
15	O
layer	O
CNN	Method
,	O
which	O
was	O
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
or	O
fully	Method
-	Method
connected	Method
(	O
FC	Method
)	O
.	O

2	O
2	O
max	Method
-	Method
pooling	Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
and	O
spatial	Method
pyramid	Method
pooling	Method
(	O
SPP	Method
)	O
with	O
3	O
levels	O
before	O
the	O
first	O
FC	Method
layer	O
.	O

For	O
network	Task
regularization	Task
we	O
set	O
the	O
-	O
weight	O
decay	O
term	O
to	O
and	O
used	O
50	O
%	O
drop	O
-	O
out	O
in	O
the	O
two	O
penultimate	O
FC	Method
layers	O
.	O

Images	O
were	O
re	O
-	O
sized	O
to	O
256	O
256	O
pixels	O
and	O
per	O
-	O
pixel	O
mean	O
subtracted	O
.	O

Trained	O
was	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O

Besides	O
that	O
,	O
we	O
did	O
not	O
augment	O
the	O
dataset	O
during	O
training	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
learning	O
behavior	O
of	O
ELU	Method
vs.	O
ReLU	Method
networks	O
.	O

Panel	O
(	O
b	O
)	O
shows	O
that	O
ELUs	Method
start	O
reducing	O
the	O
error	Metric
earlier	O
.	O

The	O
ELU	Method
-	Method
network	Method
already	O
reaches	O
the	O
20	O
%	O
top	Metric
-	Metric
5	Metric
error	Metric
after	O
160k	O
iterations	O
,	O
while	O
the	O
ReLU	Method
network	O
needs	O
200k	O
iterations	O
to	O
reach	O
the	O
same	O
error	Metric
rate	Metric
.	O

The	O
single	O
-	O
model	O
performance	O
was	O
evaluated	O
on	O
the	O
single	O
center	Task
crop	Task
with	O
no	O
further	O
augmentation	O
and	O
yielded	O
a	O
top	Metric
-	Metric
5	Metric
validation	Metric
error	Metric
below	O
10	O
%	O
.	O

Currently	O
ELU	Method
nets	Method
are	O
5	O
%	O
slower	O
on	O
ImageNet	Method
than	O
ReLU	Method
nets	O
.	O

The	O
difference	O
is	O
small	O
because	O
activation	O
functions	O
generally	O
have	O
only	O
minor	O
influence	O
on	O
the	O
overall	O
training	Metric
time	Metric
.	O

In	O
terms	O
of	O
wall	Metric
clock	Metric
time	Metric
,	O
ELUs	Method
require	O
12.15h	O
vs.	O
ReLUs	Method
with	O
11.48h	O
for	O
10k	O
iterations	O
.	O

We	O
expect	O
that	O
ELU	Method
implementations	Method
can	O
be	O
improved	O
,	O
e.g.	O
by	O
faster	O
exponential	Method
functions	Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
the	O
exponential	Method
linear	Method
units	Method
(	O
ELUs	Method
)	O
for	O
faster	O
and	O
more	O
precise	O
learning	Task
in	O
deep	Task
neural	Task
networks	Task
.	O

ELUs	Method
have	O
negative	O
values	O
,	O
which	O
allows	O
the	O
network	O
to	O
push	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
.	O

Therefore	O
ELUs	Method
decrease	O
the	O
gap	O
between	O
the	O
normal	O
gradient	O
and	O
the	O
unit	O
natural	O
gradient	O
and	O
,	O
thereby	O
speed	O
up	O
learning	Task
.	O

We	O
believe	O
that	O
this	O
property	O
is	O
also	O
the	O
reason	O
for	O
the	O
success	O
of	O
activation	O
functions	O
like	O
LReLUs	Method
and	O
PReLUs	Method
and	O
of	O
batch	Method
normalization	Method
.	O

In	O
contrast	O
to	O
LReLUs	Method
and	O
PReLUs	Method
,	O
ELUs	Method
have	O
a	O
clear	O
saturation	O
plateau	O
in	O
its	O
negative	O
regime	O
,	O
allowing	O
them	O
to	O
learn	O
a	O
more	O
robust	O
and	O
stable	O
representation	O
.	O

Experimental	O
results	O
show	O
that	O
ELUs	Method
significantly	O
outperform	O
other	O
activation	Method
functions	Method
on	O
different	O
vision	O
datasets	O
.	O

Further	O
ELU	Method
networks	Method
perform	O
significantly	O
better	O
than	O
ReLU	Method
networks	O
trained	O
with	O
batch	Method
normalization	Method
.	O

ELU	Method
networks	Method
achieved	O
one	O
of	O
the	O
top	O
10	O
best	O
reported	O
results	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
set	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
CIFAR	Material
-	Material
100	Material
without	O
the	O
need	O
for	O
multi	Method
-	Method
view	Method
test	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O

Furthermore	O
,	O
ELU	Method
networks	Method
produced	O
competitive	O
results	O
on	O
the	O
ImageNet	O
in	O
much	O
fewer	O
epochs	O
than	O
a	O
corresponding	O
ReLU	Method
network	O
.	O

Given	O
their	O
outstanding	O
performance	O
,	O
we	O
expect	O
ELU	Method
networks	Method
to	O
become	O
a	O
real	O
time	O
saver	O
in	O
convolutional	Method
networks	Method
,	O
which	O
are	O
notably	O
time	O
-	O
intensive	O
to	O
train	O
from	O
scratch	O
otherwise	O
.	O

paragraph	O
:	O
Acknowledgment	O
.	O

We	O
thank	O
the	O
NVIDIA	O
Corporation	O
for	O
supporting	O
this	O
research	O
with	O
several	O
Titan	O
X	O
GPUs	O
and	O
Roland	O
Vollgraf	O
and	O
Martin	O
Heusel	O
for	O
helpful	O
discussions	O
and	O
comments	O
on	O
this	O
work	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Inverse	Method
of	Method
Block	Method
Matrices	Method
theorem	O
:	O
.	O

The	O
positive	O
definite	O
matrix	O
M	O
is	O
in	O
block	O
format	O
with	O
matrix	O
A	O
,	O
vector	O
b	O
,	O
and	O
scalar	O
c.	O
The	O
inverse	O
of	O
M	O
is	O
where	O
proof	O
:	O
Proof	O
.	O

For	O
block	O
matrices	O
the	O
inverse	O
is	O
where	O
the	O
matrices	O
on	O
the	O
right	O
hand	O
side	O
are	O
:	O
Further	O
if	O
follows	O
that	O
We	O
now	O
use	O
this	O
formula	O
for	O
being	O
a	O
vector	O
and	O
a	O
scalar	O
.	O

We	O
obtain	O
where	O
the	O
right	O
hand	O
side	O
matrices	O
,	O
vectors	O
,	O
and	O
the	O
scalar	O
are	O
:	O
Again	O
it	O
follows	O
that	O
A	O
reformulation	O
using	O
gives	O
∎	O
appendix	O
:	O
Quadratic	O
Form	O
of	O
Mean	O
and	O
Inverse	Method
Second	Method
Moment	Method
theorem	O
:	O
.	O

For	O
a	O
random	O
variable	O
a	O
holds	O
and	O
Furthermore	O
holds	O
proof	O
:	O
Proof	O
.	O

The	O
Sherman	Method
-	Method
Morrison	Method
Theorem	Method
states	O
Therefore	O
we	O
have	O
Using	O
the	O
identity	O
for	O
the	O
second	O
moment	O
and	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
we	O
get	O
The	O
last	O
inequality	O
follows	O
from	O
the	O
fact	O
that	O
is	O
positive	O
definite	O
.	O

From	O
last	O
equation	O
,	O
we	O
obtain	O
further	O
For	O
the	O
mixed	O
quadratic	O
form	O
we	O
get	O
from	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
From	O
this	O
equation	O
follows	O
Therefore	O
we	O
get	O
∎	O
appendix	O
:	O
Variance	O
of	O
Mean	O
Activations	O
in	O
ELU	Method
and	O
ReLU	Method
Networks	O
To	O
compare	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	Method
and	O
ELU	Method
networks	Method
,	O
we	O
trained	O
a	O
neural	Method
network	Method
with	O
5	O
hidden	O
layers	O
of	O
256	O
hidden	O
units	O
for	O
200	O
epochs	O
using	O
a	O
learning	O
rate	O
of	O
0.01	O
,	O
once	O
using	O
ReLU	Method
and	O
once	O
using	O
ELU	Method
activation	Method
functions	Method
on	O
the	O
MNIST	O
dataset	O
.	O

After	O
each	O
epoch	O
,	O
we	O
calculated	O
the	O
median	O
activation	O
of	O
each	O
hidden	Method
unit	Method
on	O
the	O
whole	O
training	O
set	O
.	O

We	O
then	O
calculated	O
the	O
variance	O
of	O
these	O
changes	O
,	O
which	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	Method
networks	O
.	O

This	O
indicates	O
that	O
ReLU	Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	Method
networks	Method
.	O

Conditional	Method
Random	Method
Fields	Method
as	O
Recurrent	Method
Neural	Method
Networks	Method
section	O
:	O
Abstract	O
Pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
,	O
such	O
as	O
semantic	Task
segmentation	Task
,	O
play	O
a	O
central	O
role	O
in	O
image	Task
understanding	Task
.	O

Recent	O
approaches	O
have	O
attempted	O
to	O
harness	O
the	O
capabilities	O
of	O
deep	Method
learning	Method
techniques	Method
for	O
image	Task
recognition	Task
to	O
tackle	O
pixellevel	Task
labelling	Task
tasks	Task
.	O

One	O
central	O
issue	O
in	O
this	O
methodology	O
is	O
the	O
limited	O
capacity	O
of	O
deep	Method
learning	Method
techniques	Method
to	O
delineate	O
visual	O
objects	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
introduce	O
a	O
new	O
form	O
of	O
convolutional	Method
neural	Method
network	Method
that	O
combines	O
the	O
strengths	O
of	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
and	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRFs	Method
)-	O
based	O
probabilistic	O
graphical	O
modelling	O
.	O

To	O
this	O
end	O
,	O
we	O
formulate	O
mean	Method
-	Method
field	Method
approximate	Method
inference	Method
for	O
the	O
Conditional	Method
Random	Method
Fields	Method
with	O
Gaussian	O
pairwise	O
potentials	O
as	O
Recurrent	Method
Neural	Method
Networks	Method
.	O

This	O
network	O
,	O
called	O
CRF	Method
-	Method
RNN	Method
,	O
is	O
then	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
CNN	Method
to	O
obtain	O
a	O
deep	Method
network	Method
that	O
has	O
desirable	O
properties	O
of	O
both	O
CNNs	Method
and	O
CRFs	Method
.	O

Importantly	O
,	O
our	O
system	O
fully	O
integrates	O
CRF	Method
modelling	O
with	O
CNNs	Method
,	O
making	O
it	O
possible	O
to	O
train	O
the	O
whole	O
deep	Method
network	Method
end	O
-	O
to	O
-	O
end	O
with	O
the	O
usual	O
back	Method
-	Method
propagation	Method
algorithm	Method
,	O
avoiding	O
offline	Method
post	Method
-	Method
processing	Method
methods	Method
for	O
object	Task
delineation	Task
.	O

We	O
apply	O
the	O
proposed	O
method	O
to	O
the	O
problem	O
of	O
semantic	Task
image	Task
segmentation	Task
,	O
obtaining	O
top	O
results	O
on	O
the	O
challenging	O
Pascal	Material
VOC	Material
2012	Material
segmentation	Material
benchmark	Material
.	O

section	O
:	O
Introduction	O
Low	Task
-	Task
level	Task
computer	Task
vision	Task
problems	Task
such	O
as	O
semantic	Task
image	Task
segmentation	Task
or	O
depth	Task
estimation	Task
often	O
involve	O
assigning	O
a	O
label	O
to	O
each	O
pixel	O
in	O
an	O
image	O
.	O

While	O
the	O
feature	Method
representation	Method
used	O
to	O
classify	O
individual	O
pixels	O
plays	O
an	O
important	O
role	O
in	O
this	O
task	O
,	O
it	O
is	O
similarly	O
important	O
to	O
consider	O
factors	O
such	O
as	O
image	O
edges	O
,	O
appearance	O
consistency	O
and	O
spatial	O
consistency	O
while	O
assigning	O
labels	O
in	O
order	O
to	O
obtain	O
accurate	O
and	O
precise	O
results	O
.	O

Designing	O
a	O
strong	Method
feature	Method
representation	Method
is	O
a	O
key	O
chal	O
-	O
*	O
Authors	O
contributed	O
equally	O
.	O

†	O
Work	O
conducted	O
while	O
authors	O
at	O
the	O
University	O
of	O
Oxford	O
.	O

lenge	O
in	O
pixel	Task
-	Task
level	Task
labelling	Task
problems	Task
.	O

Work	O
on	O
this	O
topic	O
includes	O
:	O
TextonBoost	Method
[	O
reference	O
]	O
,	O
TextonForest	Method
[	O
reference	O
]	O
,	O
and	O
Random	Method
Forest	Method
-	Method
based	Method
classifiers	Method
[	O
reference	O
]	O
.	O

Recently	O
,	O
supervised	Method
deep	Method
learning	Method
approaches	Method
such	O
as	O
large	Method
-	Method
scale	Method
deep	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
have	O
been	O
immensely	O
successful	O
in	O
many	O
high	O
-	O
level	Task
computer	Task
vision	Task
tasks	Task
such	O
as	O
image	Task
recognition	Task
[	O
reference	O
]	O
and	O
object	Task
detection	Task
[	O
reference	O
]	O
.	O

This	O
motivates	O
exploring	O
the	O
use	O
of	O
CNNs	Method
for	O
pixel	Task
-	Task
level	Task
labelling	Task
problems	Task
.	O

The	O
key	O
insight	O
is	O
to	O
learn	O
a	O
strong	O
feature	Method
representation	Method
end	O
-	O
to	O
-	O
end	O
for	O
the	O
pixel	Task
-	Task
level	Task
labelling	Task
task	Task
instead	O
of	O
hand	O
-	O
crafting	O
features	Method
with	O
heuristic	Method
parameter	Method
tuning	Method
.	O

In	O
fact	O
,	O
a	O
number	O
of	O
recent	O
approaches	O
including	O
the	O
particularly	O
interesting	O
works	O
FCN	Method
[	O
reference	O
]	O
and	O
DeepLab	Method
[	O
reference	O
]	O
have	O
shown	O
a	O
significant	O
accuracy	Metric
boost	O
by	O
adapting	O
stateof	O
-	O
the	O
-	O
art	O
CNN	Method
based	O
image	O
classifiers	O
to	O
the	O
semantic	Task
segmentation	Task
problem	O
.	O

However	O
,	O
there	O
are	O
significant	O
challenges	O
in	O
adapting	O
CNNs	Method
designed	O
for	O
high	Task
level	Task
computer	Task
vision	Task
tasks	Task
such	O
as	O
object	Task
recognition	Task
to	O
pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
.	O

Firstly	O
,	O
traditional	O
CNNs	Method
have	O
convolutional	Method
filters	Method
with	O
large	O
receptive	O
fields	O
and	O
hence	O
produce	O
coarse	O
outputs	O
when	O
restructured	O
to	O
produce	O
pixel	O
-	O
level	O
labels	O
[	O
reference	O
]	O
.	O

Presence	O
of	O
maxpooling	Method
layers	Method
in	O
CNNs	Method
further	O
reduces	O
the	O
chance	O
of	O
getting	O
a	O
fine	O
segmentation	O
output	O
[	O
reference	O
]	O
.	O

This	O
,	O
for	O
instance	O
,	O
can	O
result	O
in	O
non	O
-	O
sharp	O
boundaries	O
and	O
blob	O
-	O
like	O
shapes	O
in	O
semantic	Task
segmentation	Task
tasks	O
.	O

Secondly	O
,	O
CNNs	Method
lack	O
smoothness	O
constraints	O
that	O
encourage	O
label	O
agreement	O
between	O
similar	O
pixels	O
,	O
and	O
spatial	O
and	O
appearance	O
consistency	O
of	O
the	O
labelling	O
output	O
.	O

Lack	O
of	O
such	O
smoothness	O
constraints	O
can	O
result	O
in	O
poor	O
object	Task
delineation	Task
and	O
small	O
spurious	O
regions	O
in	O
the	O
segmentation	O
output	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

On	O
a	O
separate	O
track	O
to	O
the	O
progress	O
of	O
deep	Method
learning	Method
techniques	Method
,	O
probabilistic	Method
graphical	Method
models	Method
have	O
been	O
developed	O
as	O
effective	O
methods	O
to	O
enhance	O
the	O
accuracy	Metric
of	O
pixellevel	Task
labelling	Task
tasks	Task
.	O

In	O
particular	O
,	O
Markov	Method
Random	Method
Fields	Method
(	O
MRFs	Method
)	Method
and	O
its	O
variant	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRFs	Method
)	O
have	O
observed	O
widespread	O
success	O
in	O
this	O
area	O
[	O
reference	O
][	O
reference	O
]	O
and	O
have	O
become	O
one	O
of	O
the	O
most	O
successful	O
graphical	Method
models	Method
used	O
in	O
computer	Task
vision	Task
.	O

The	O
key	O
idea	O
of	O
CRF	Method
inference	O
for	O
semantic	Task
labelling	Task
is	O
to	O
formulate	O
the	O
label	Task
assignment	Task
problem	Task
as	O
a	O
probabilistic	Task
inference	Task
problem	Task
that	O
incorporates	O
assumptions	O
such	O
as	O
the	O
label	O
agreement	O
between	O
similar	O
pixels	O
.	O

CRF	Method
inference	O
is	O
able	O
to	O
refine	O
weak	O
and	O
coarse	O
pixel	O
-	O
level	O
label	O
predictions	O
to	O
produce	O
sharp	O
boundaries	O
and	O
fine	Task
-	Task
grained	Task
segmentations	Task
.	O

Therefore	O
,	O
intuitively	O
,	O
CRFs	Method
can	O
be	O
used	O
to	O
overcome	O
the	O
drawbacks	O
in	O
utilizing	O
CNNs	Method
for	O
pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
.	O

One	O
way	O
to	O
utilize	O
CRFs	Method
to	O
improve	O
the	O
semantic	Task
labelling	Task
results	Task
produced	O
by	O
a	O
CNN	Method
is	O
to	O
apply	O
CRF	Method
inference	O
as	O
a	O
post	Method
-	Method
processing	Method
step	Method
disconnected	O
from	O
the	O
training	O
of	O
the	O
CNN	Method
[	O
reference	O
]	O
.	O

Arguably	O
,	O
this	O
does	O
not	O
fully	O
harness	O
the	O
strength	O
of	O
CRFs	Method
since	O
it	O
is	O
not	O
integrated	O
with	O
the	O
deep	Method
network	Method
.	O

In	O
this	O
setup	O
,	O
the	O
deep	Method
network	Method
is	O
unaware	O
of	O
the	O
CRF	Method
during	O
the	O
training	O
phase	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
deep	Method
learning	Method
solution	Method
for	O
the	O
pixel	Task
-	Task
level	Task
semantic	Task
image	Task
segmentation	Task
problem	Task
.	O

Our	O
formulation	O
combines	O
the	O
strengths	O
of	O
both	O
CNNs	O
and	O
CRF	Method
based	O
graphical	O
models	O
in	O
one	O
unified	O
framework	O
.	O

More	O
specifically	O
,	O
we	O
formulate	O
mean	Method
-	Method
field	Method
approximate	Method
inference	Method
for	O
the	O
dense	O
CRF	Method
with	O
Gaussian	Method
pairwise	Method
potentials	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
which	O
can	O
refine	O
coarse	O
outputs	O
from	O
a	O
traditional	O
CNN	Method
in	O
the	O
forward	O
pass	O
,	O
while	O
passing	O
error	O
differentials	O
back	O
to	O
the	O
CNN	Method
during	O
training	O
.	O

Importantly	O
,	O
with	O
our	O
formulation	O
,	O
the	O
whole	O
deep	Method
network	Method
,	O
which	O
comprises	O
a	O
traditional	O
CNN	Method
and	O
an	O
RNN	Method
for	O
CRF	Method
inference	O
,	O
can	O
be	O
trained	O
endto	O
-	O
end	O
utilizing	O
the	O
usual	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

Arguably	O
,	O
when	O
properly	O
trained	O
,	O
the	O
proposed	O
network	O
should	O
outperform	O
a	O
system	O
where	O
CRF	Method
inference	O
is	O
applied	O
as	O
a	O
post	Method
-	Method
processing	Method
method	Method
on	O
independent	O
pixel	O
-	O
level	O
predictions	O
produced	O
by	O
a	O
pre	O
-	O
trained	O
CNN	Method
.	O

Our	O
experimental	O
evaluation	O
confirms	O
that	O
this	O
indeed	O
is	O
the	O
case	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
our	O
network	O
on	O
the	O
popular	O
Pascal	Material
VOC	Material
2012	Material
benchmark	Material
,	O
achieving	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
of	O
74.7	O
%	O
.	O

Our	O
source	O
code	O
and	O
models	O
are	O
publicly	O
available	O
1	O
.	O

section	O
:	O
Related	O
Work	O
In	O
this	O
section	O
we	O
review	O
approaches	O
that	O
make	O
use	O
of	O
deep	Method
learning	Method
and	O
CNNs	Method
for	O
low	Task
-	Task
level	Task
computer	Task
vision	Task
tasks	Task
,	O
with	O
a	O
focus	O
on	O
semantic	Task
image	Task
segmentation	Task
.	O

A	O
wide	O
variety	O
of	O
approaches	O
have	O
been	O
proposed	O
to	O
tackle	O
the	O
semantic	Task
image	Task
segmentation	Task
task	Task
using	O
deep	Method
learning	Method
.	O

These	O
approaches	O
can	O
be	O
categorized	O
into	O
two	O
main	O
strategies	O
.	O

The	O
first	O
strategy	O
is	O
based	O
on	O
utilizing	O
separate	O
mechanisms	O
for	O
feature	Task
extraction	Task
,	O
and	O
image	Task
segmentation	Task
exploiting	O
the	O
edges	O
of	O
the	O
image	O
[	O
reference	O
][	O
reference	O
]	O
.	O

One	O
representative	O
instance	O
of	O
this	O
scheme	O
is	O
the	O
application	O
of	O
a	O
CNN	Method
for	O
the	O
extraction	Task
of	Task
meaningful	Task
features	Task
,	O
and	O
using	O
superpixels	Method
to	O
account	O
for	O
the	O
structural	O
pattern	O
of	O
the	O
image	O
.	O

Two	O
representative	O
examples	O
are	O
[	O
reference	O
][	O
reference	O
]	O
,	O
where	O
the	O
authors	O
first	O
ob	O
-	O
tained	O
superpixels	O
from	O
the	O
image	O
and	O
then	O
used	O
a	O
feature	Method
extraction	Method
process	Method
on	O
each	O
of	O
them	O
.	O

The	O
main	O
disadvantage	O
of	O
this	O
strategy	O
is	O
that	O
errors	O
in	O
the	O
initial	O
proposals	O
(	O
e.g	O
:	O
super	O
-	O
pixels	O
)	O
may	O
lead	O
to	O
poor	O
predictions	O
,	O
no	O
matter	O
how	O
good	O
the	O
feature	Method
extraction	Method
process	Method
is	O
.	O

Pinheiro	O
and	O
Collobert	O
[	O
reference	O
]	O
employed	O
an	O
RNN	Method
to	O
model	O
the	O
spatial	O
dependencies	O
during	O
scene	Task
parsing	Task
.	O

In	O
contrast	O
to	O
their	O
approach	O
,	O
we	O
show	O
that	O
a	O
typical	O
graphical	Method
model	Method
such	O
as	O
a	O
CRF	Method
can	O
be	O
formulated	O
as	O
an	O
RNN	Method
to	O
form	O
a	O
part	O
of	O
a	O
deep	Method
network	Method
,	O
to	O
perform	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
combined	O
with	O
a	O
CNN	Method
.	O

The	O
second	O
strategy	O
is	O
to	O
directly	O
learn	O
a	O
nonlinear	Method
model	Method
from	O
the	O
images	O
to	O
the	O
label	O
map	O
.	O

This	O
,	O
for	O
example	O
,	O
was	O
shown	O
in	O
[	O
reference	O
]	O
,	O
where	O
the	O
authors	O
replaced	O
the	O
last	O
fully	O
connected	O
layers	O
of	O
a	O
CNN	Method
by	O
convolutional	Method
layers	Method
to	O
keep	O
spatial	O
information	O
.	O

An	O
important	O
contribution	O
in	O
this	O
direction	O
is	O
[	O
reference	O
]	O
,	O
where	O
Long	O
et	O
al	O
.	O

used	O
the	O
concept	O
of	O
fully	Method
convolutional	Method
networks	Method
,	O
and	O
the	O
notion	O
that	O
top	O
layers	O
obtain	O
meaningful	O
features	O
for	O
object	Task
recognition	Task
whereas	O
low	O
layers	O
keep	O
information	O
about	O
the	O
structure	O
of	O
the	O
image	O
,	O
such	O
as	O
edges	O
.	O

In	O
their	O
work	O
,	O
connections	O
from	O
early	O
layers	O
to	O
later	O
layers	O
were	O
used	O
to	O
combine	O
these	O
cues	O
.	O

Bell	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
][	O
reference	O
]	O
used	O
a	O
CRF	Method
to	O
refine	O
segmentation	Task
results	O
obtained	O
from	O
a	O
CNN	Method
.	O

Bell	O
et	O
al	O
.	O

focused	O
on	O
material	Task
recognition	Task
and	O
segmentation	Task
,	O
whereas	O
Chen	O
et	O
al	O
.	O

reported	O
very	O
significant	O
improvements	O
on	O
semantic	Task
image	Task
segmentation	Task
.	O

In	O
contrast	O
to	O
these	O
works	O
,	O
which	O
employed	O
CRF	Method
inference	O
as	O
a	O
standalone	Task
post	Task
-	Task
processing	Task
step	Task
disconnected	O
from	O
the	O
CNN	Method
training	O
,	O
our	O
approach	O
is	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
network	Method
that	O
jointly	O
learns	O
the	O
parameters	O
of	O
the	O
CNN	Method
and	O
the	O
CRF	Method
in	O
one	O
unified	Method
deep	Method
network	Method
.	O

Works	O
that	O
use	O
neural	Method
networks	Method
to	O
predict	O
structured	O
output	O
are	O
found	O
in	O
different	O
domains	O
.	O

For	O
example	O
,	O
Do	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
an	O
approach	O
to	O
combine	O
deep	Method
neural	Method
networks	Method
and	O
Markov	Method
networks	Method
for	O
sequence	Task
labeling	Task
tasks	Task
.	O

Jain	O
et	O
al	O
.	O

[	O
reference	O
]	O
has	O
shown	O
Convolutional	Method
Neural	Method
Networks	Method
can	O
perform	O
well	O
like	O
MRFs	O
/	O
CRFs	Method
approaches	O
in	O
image	Task
restoration	Task
application	Task
.	O

Another	O
domain	O
which	O
benefits	O
from	O
the	O
combination	O
of	O
CNNs	Method
and	O
structured	Method
loss	Method
is	O
handwriting	Task
recognition	Task
.	O

In	O
natural	Task
language	Task
processing	Task
,	O
Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
shows	O
that	O
the	O
performance	O
of	O
an	O
RNN	Method
-	O
based	O
words	O
tagger	O
can	O
be	O
significantly	O
improved	O
by	O
incorporating	O
elements	O
of	O
the	O
CRF	Method
model	O
.	O

In	O
[	O
reference	O
]	O
,	O
the	O
authors	O
combined	O
a	O
CNN	Method
with	O
Hidden	Method
Markov	Method
Models	Method
for	O
that	O
purpose	O
,	O
whereas	O
more	O
recently	O
,	O
Peng	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
a	O
modified	O
version	O
of	O
CRFs	Method
.	O

Related	O
to	O
this	O
line	O
of	O
works	O
,	O
in	O
[	O
reference	O
]	O
a	O
joint	O
CNN	Method
and	O
CRF	Method
model	O
was	O
used	O
for	O
text	Task
recognition	Task
on	O
natural	O
images	O
.	O

Tompson	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
the	O
use	O
of	O
joint	Method
training	Method
of	O
a	O
CNN	Method
and	O
an	O
MRF	Method
for	O
human	Task
pose	Task
estimation	Task
,	O
while	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
focused	O
on	O
the	O
image	Task
classification	Task
problem	Task
with	O
a	O
similar	O
approach	O
.	O

Another	O
prominent	O
work	O
is	O
[	O
reference	O
]	O
,	O
in	O
which	O
the	O
authors	O
express	O
deformable	Method
part	Method
models	Method
,	O
a	O
kind	O
of	O
MRF	Method
,	O
as	O
a	O
layer	O
in	O
a	O
neural	Method
network	Method
.	O

In	O
our	O
approach	O
,	O
we	O
cast	O
a	O
different	O
graphical	Method
model	Method
as	O
a	O
neural	Method
network	Method
layer	Method
.	O

A	O
number	O
of	O
approaches	O
have	O
been	O
proposed	O
for	O
automatic	Task
learning	Task
of	Task
graphical	Task
model	Task
parameters	Task
and	O
joint	Method
training	Method
of	Method
classifiers	Method
and	O
graphical	Method
models	Method
.	O

Barbu	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
joint	Method
training	Method
of	O
a	O
MRF	O
/	O
CRF	Method
model	O
together	O
with	O
an	O
inference	Method
algorithm	Method
in	O
their	O
Active	Method
Random	Method
Field	Method
approach	Method
.	O

Domke	O
[	O
reference	O
]	O
advocated	O
back	Method
-	Method
propagation	Method
based	Method
parameter	Method
optimization	Method
in	O
graphical	Method
models	Method
when	O
approximate	Method
inference	Method
methods	Method
such	O
as	O
mean	Method
-	Method
field	Method
and	O
belief	Method
propagation	Method
are	O
used	O
.	O

This	O
idea	O
was	O
utilized	O
in	O
[	O
reference	O
]	O
,	O
where	O
a	O
binary	O
dense	O
CRF	Method
was	O
used	O
for	O
human	Task
pose	Task
estimation	Task
.	O

Similarly	O
,	O
Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Stoyanov	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
how	O
back	Method
-	Method
propagation	Method
through	O
belief	Method
propagation	Method
can	O
be	O
used	O
to	O
optimize	O
model	O
parameters	O
.	O

Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
in	O
particular	O
proposes	O
an	O
approach	O
based	O
on	O
learning	Task
messages	Task
.	O

Many	O
of	O
these	O
ideas	O
can	O
be	O
traced	O
back	O
to	O
[	O
reference	O
]	O
,	O
which	O
proposes	O
unrolling	Method
message	Method
passing	Method
algorithms	Method
as	O
simpler	O
operations	O
that	O
could	O
be	O
performed	O
within	O
a	O
CNN	Method
.	O

In	O
a	O
different	O
setup	O
,	O
Krähenbühl	O
and	O
Koltun	O
[	O
reference	O
]	O
demonstrated	O
automatic	Method
parameter	Method
tuning	Method
of	O
dense	O
CRF	Method
when	O
a	O
modified	O
mean	Method
-	Method
field	Method
algorithm	Method
is	O
used	O
for	O
inference	Task
.	O

An	O
alternative	O
inference	Method
approach	Method
for	O
dense	O
CRF	Method
,	O
not	O
based	O
on	O
mean	Method
-	Method
field	Method
,	O
is	O
proposed	O
in	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
to	O
the	O
works	O
described	O
above	O
,	O
our	O
approach	O
shows	O
that	O
it	O
is	O
possible	O
to	O
formulate	O
dense	O
CRF	Method
as	O
an	O
RNN	Method
so	O
that	O
one	O
can	O
form	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
system	Method
for	O
semantic	Task
image	Task
segmentation	Task
which	O
combines	O
the	O
strengths	O
of	O
deep	Method
learning	Method
and	O
graphical	Method
modelling	Method
.	O

After	O
our	O
initial	O
publication	O
of	O
the	O
technical	O
report	O
of	O
this	O
work	O
on	O
arXiv.org	O
,	O
a	O
number	O
of	O
independent	O
works	O
[	O
reference	O
][	O
reference	O
]	O
appeared	O
on	O
arXiv.org	O
presenting	O
similar	O
joint	Method
training	Method
approaches	Method
for	O
semantic	Task
image	Task
segmentation	Task
.	O

section	O
:	O
Conditional	Method
Random	Method
Fields	Method
In	O
this	O
section	O
we	O
provide	O
a	O
brief	O
overview	O
of	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRF	Method
)	O
for	O
pixel	Task
-	Task
wise	Task
labelling	Task
and	O
introduce	O
the	O
notation	O
used	O
in	O
the	O
paper	O
.	O

A	O
CRF	Method
,	O
used	O
in	O
the	O
context	O
of	O
pixel	Task
-	Task
wise	Task
label	Task
prediction	Task
,	O
models	O
pixel	O
labels	O
as	O
random	O
variables	O
that	O
form	O
a	O
Markov	Method
Random	Method
Field	Method
(	O
MRF	Method
)	O
when	O
conditioned	O
upon	O
a	O
global	O
observation	O
.	O

The	O
global	Task
observation	Task
is	O
usually	O
taken	O
to	O
be	O
the	O
image	O
.	O

Let	O
X	O
i	O
be	O
the	O
random	O
variable	O
associated	O
to	O
pixel	O
i	O
,	O
which	O
represents	O
the	O
label	O
assigned	O
to	O
the	O
pixel	O
i	O
and	O
can	O
take	O
any	O
value	O
from	O
a	O
pre	O
-	O
defined	O
set	O
of	O
labels	O
L	O
=	O
{	O
l	O
1	O
,	O
l	O
2	O
,	O
.	O

.	O

.	O

,	O
l	O
L	O
}	O
.	O

Let	O
X	O
be	O
the	O
vector	O
formed	O
by	O
the	O
random	O
variables	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O

.	O

,	O
X	O
N	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
in	O
the	O
image	O
.	O

Given	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
=	O
{	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O

.	O

,	O
X	O
N	O
}	O
,	O
and	O
a	O
global	O
observation	O
(	O
image	O
)	O
I	O
,	O
the	O
pair	O
(	O
I	O
,	O
X	O
)	O
can	O
be	O
modelled	O
as	O
a	O
CRF	Method
characterized	O
by	O
a	O
Gibbs	Method
distribution	Method
of	O
the	O
form	O
P	O
(	O
X	O
=	O
x|I	O
)	O
=	O
tion	O
[	O
reference	O
]	O
.	O

From	O
now	O
on	O
,	O
we	O
drop	O
the	O
conditioning	O
on	O
I	O
in	O
the	O
notation	O
for	O
convenience	O
.	O

In	O
the	O
fully	O
connected	O
pairwise	O
CRF	Method
model	O
of	O
[	O
reference	O
]	O
,	O
the	O
energy	O
of	O
a	O
label	O
assignment	O
x	O
is	O
given	O
by	O
:	O
where	O
the	O
unary	O
energy	O
components	O
ψ	O
u	O
(	O
x	O
i	O
)	O
measure	O
the	O
inverse	O
likelihood	O
(	O
and	O
therefore	O
,	O
the	O
cost	O
)	O
of	O
the	O
pixel	O
i	O
taking	O
the	O
label	O
x	O
i	O
,	O
and	O
pairwise	O
energy	O
components	O
ψ	O
p	O
(	O
x	O
i	O
,	O
x	O
j	O
)	O
measure	O
the	O
cost	O
of	O
assigning	O
labels	O
x	O
i	O
,	O
x	O
j	O
to	O
pixels	O
i	O
,	O
j	O
simultaneously	O
.	O

In	O
our	O
model	O
,	O
unary	O
energies	O
are	O
obtained	O
from	O
a	O
CNN	Method
,	O
which	O
,	O
roughly	O
speaking	O
,	O
predicts	O
labels	O
for	O
pixels	O
without	O
considering	O
the	O
smoothness	O
and	O
the	O
consistency	O
of	O
the	O
label	O
assignments	O
.	O

The	O
pairwise	O
energies	O
provide	O
an	O
image	Method
data	Method
-	Method
dependent	Method
smoothing	Method
term	Method
that	O
encourages	O
assigning	O
similar	O
labels	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

As	O
was	O
done	O
in	O
[	O
reference	O
]	O
,	O
we	O
model	O
pairwise	O
potentials	O
as	O
weighted	Method
Gaussians	Method
:	O
where	O
each	O
k	O
,	O
is	O
a	O
Gaussian	Method
kernel	Method
applied	O
on	O
feature	O
vectors	O
.	O

The	O
feature	O
vector	O
of	O
pixel	O
i	O
,	O
denoted	O
by	O
f	O
i	O
,	O
is	O
derived	O
from	O
image	O
features	O
such	O
as	O
spatial	O
location	O
and	O
RGB	O
values	O
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
same	O
features	O
as	O
in	O
[	O
reference	O
]	O
.	O

The	O
function	O
µ	O
(	O
.	O

,	O
.	O

)	O
,	O
called	O
the	O
label	O
compatibility	O
function	O
,	O
captures	O
the	O
compatibility	O
between	O
different	O
pairs	O
of	O
labels	O
as	O
the	O
name	O
implies	O
.	O

Minimizing	O
the	O
above	O
CRF	Method
energy	O
E	O
(	O
x	O
)	O
yields	O
the	O
most	O
probable	O
label	O
assignment	O
x	O
for	O
the	O
given	O
image	O
.	O

Since	O
this	O
exact	Task
minimization	Task
is	O
intractable	O
,	O
a	O
mean	Method
-	Method
field	Method
approximation	Method
to	O
the	O
CRF	Method
distribution	O
is	O
used	O
for	O
approximate	Task
maximum	Task
posterior	Task
marginal	Task
inference	Task
.	O

It	O
consists	O
in	O
approximating	O
the	O
CRF	Method
distribution	O
P	O
(	O
X	O
)	O
by	O
a	O
simpler	O
distribution	Method
Q	Method
(	Method
X	Method
)	Method
,	O
which	O
can	O
be	O
written	O
as	O
the	O
product	O
of	O
independent	O
marginal	O
distributions	O
,	O
i.e.	O
,	O
Q	O
(	O
X	O
)	O
=	O
i	O
Q	O
i	O
(	O
X	O
i	O
)	O
.	O

The	O
steps	O
of	O
the	O
iterative	Method
algorithm	Method
for	O
approximate	Task
mean	Task
-	Task
field	Task
inference	Task
and	O
its	O
reformulation	O
as	O
an	O
RNN	Method
are	O
discussed	O
next	O
.	O

[	O
reference	O
]	O
,	O
broken	O
down	O
to	O
common	O
CNN	Method
operations	O
.	O

section	O
:	O
Algorithm	O
1	O
Mean	Method
-	Method
field	Method
in	O
dense	O
CRFs	Method
Adding	O
Unary	O
Potentials	O
Normalizing	O
end	O
while	O
section	O
:	O
A	O
Mean	Method
-	Method
field	Method
Iteration	Method
as	O
a	O
Stack	O
of	O
CNN	Method
Layers	O
A	O
key	O
contribution	O
of	O
this	O
paper	O
is	O
to	O
show	O
that	O
the	O
meanfield	O
CRF	Method
inference	O
can	O
be	O
reformulated	O
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O

To	O
this	O
end	O
,	O
we	O
first	O
consider	O
individual	O
steps	O
of	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
summarized	O
in	O
Algorithm	O
1	O
[	O
reference	O
]	O
,	O
and	O
describe	O
them	O
as	O
CNN	Method
layers	O
.	O

Our	O
contribution	O
is	O
based	O
on	O
the	O
observation	O
that	O
filter	Method
-	Method
based	Method
approximate	Method
mean	Method
-	Method
field	Method
inference	Method
approach	Method
for	O
dense	O
CRFs	Method
relies	O
on	O
applying	O
Gaussian	Method
spatial	Method
and	Method
bilateral	Method
filters	Method
on	O
the	O
mean	Method
-	Method
field	Method
approximates	Method
in	O
each	O
iteration	O
.	O

Unlike	O
the	O
standard	O
convolutional	Method
layer	Method
in	O
a	O
CNN	Method
,	O
in	O
which	O
filters	O
are	O
fixed	O
after	O
the	O
training	O
stage	O
,	O
we	O
use	O
edge	Method
-	Method
preserving	Method
Gaussian	Method
filters	Method
[	O
reference	O
][	O
reference	O
]	O
,	O
coefficients	O
of	O
which	O
depend	O
on	O
the	O
original	O
spatial	O
and	O
appearance	O
information	O
of	O
the	O
image	O
.	O

These	O
filters	O
have	O
the	O
additional	O
advantages	O
of	O
requiring	O
a	O
smaller	O
set	O
of	O
parameters	O
,	O
despite	O
the	O
filter	O
size	O
being	O
potentially	O
as	O
big	O
as	O
the	O
image	O
.	O

While	O
reformulating	O
the	O
steps	O
of	O
the	O
inference	Method
algorithm	Method
as	O
CNN	Method
layers	O
,	O
it	O
is	O
essential	O
to	O
be	O
able	O
to	O
calculate	O
error	O
differentials	O
in	O
each	O
layer	O
w.r.t	O
.	O

its	O
inputs	O
in	O
order	O
to	O
be	O
able	O
to	O
back	O
-	O
propagate	O
the	O
error	O
differentials	O
to	O
previous	O
layers	O
during	O
training	O
.	O

We	O
also	O
discuss	O
how	O
to	O
calculate	O
error	O
differentials	O
with	O
respect	O
to	O
the	O
parameters	O
in	O
each	O
layer	O
,	O
enabling	O
their	O
optimization	Task
through	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

Therefore	O
,	O
in	O
our	O
formulation	O
,	O
CRF	Method
parameters	O
such	O
as	O
the	O
weights	O
of	O
the	O
Gaussian	O
kernels	O
and	O
the	O
label	O
compatibility	O
function	O
can	O
also	O
be	O
optimized	O
automatically	O
during	O
the	O
training	O
of	O
the	O
full	Method
network	Method
.	O

Once	O
the	O
individual	O
steps	O
of	O
the	O
algorithm	O
are	O
broken	O
down	O
as	O
CNN	Method
layers	O
,	O
the	O
full	Method
algorithm	Method
can	O
then	O
be	O
formulated	O
as	O
an	O
RNN	Method
.	O

We	O
explain	O
this	O
in	O
Section	O
5	O
after	O
discussing	O
the	O
steps	O
of	O
Algorithm	O
1	O
in	O
detail	O
below	O
.	O

In	O
Algorithm	O
1	O
and	O
the	O
remainder	O
of	O
this	O
paper	O
,	O
we	O
use	O
U	O
i	O
(	O
l	O
)	O
to	O
denote	O
the	O
negative	O
of	O
the	O
unary	O
energy	O
introduced	O
in	O
the	O
previous	O
section	O
,	O
i.e.	O
,	O
U	O
i	O
(	O
l	O
)	O
=	O
−ψ	O
u	O
(	O
X	O
i	O
=	O
l	O
)	O
.	O

In	O
the	O
conventional	O
CRF	Method
setting	O
,	O
this	O
input	O
U	O
i	O
(	O
l	O
)	O
to	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
is	O
obtained	O
from	O
an	O
independent	Method
classifier	Method
.	O

section	O
:	O
Initialization	O
In	O
the	O
initialization	O
step	O
of	O
the	O
algorithm	O
,	O
the	O
operation	O
,	O
is	O
performed	O
.	O

Note	O
that	O
this	O
is	O
equivalent	O
to	O
applying	O
a	O
softmax	O
function	O
over	O
the	O
unary	O
potentials	O
U	O
across	O
all	O
the	O
labels	O
at	O
each	O
pixel	O
.	O

The	O
softmax	Method
function	Method
has	O
been	O
extensively	O
used	O
in	O
CNN	Method
architectures	O
before	O
and	O
is	O
therefore	O
well	O
known	O
in	O
the	O
deep	Task
learning	Task
community	Task
.	O

This	O
operation	O
does	O
not	O
include	O
any	O
parameters	O
and	O
the	O
error	O
differentials	O
received	O
at	O
the	O
output	O
of	O
the	O
step	O
during	O
back	Method
-	Method
propagation	Method
could	O
be	O
passed	O
down	O
to	O
the	O
unary	O
potential	O
inputs	O
after	O
performing	O
usual	O
backward	Method
pass	Method
calculations	Method
of	O
the	O
softmax	Method
transformation	Method
.	O

section	O
:	O
Message	Task
Passing	Task
In	O
the	O
dense	O
CRF	Method
formulation	O
,	O
message	Task
passing	Task
is	O
implemented	O
by	O
applying	O
M	Method
Gaussian	Method
filters	Method
on	O
Q	O
values	O
.	O

Gaussian	Method
filter	Method
coefficients	Method
are	O
derived	O
based	O
on	O
image	O
features	O
such	O
as	O
the	O
pixel	O
locations	O
and	O
RGB	O
values	O
,	O
which	O
reflect	O
how	O
strongly	O
a	O
pixel	O
is	O
related	O
to	O
other	O
pixels	O
.	O

Since	O
the	O
CRF	Method
is	O
potentially	O
fully	O
connected	O
,	O
each	O
filter	O
's	O
receptive	O
field	O
spans	O
the	O
whole	O
image	O
,	O
making	O
it	O
infeasible	O
to	O
use	O
a	O
brute	O
-	O
force	O
implementation	O
of	O
the	O
filters	O
.	O

Fortunately	O
,	O
several	O
approximation	Method
techniques	Method
exist	O
to	O
make	O
computation	O
of	O
high	Task
dimensional	Task
Gaussian	Task
filtering	Task
significantly	O
faster	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
the	O
Permutohedral	Method
lattice	Method
implementation	Method
[	O
reference	O
]	O
,	O
which	O
can	O
compute	O
the	O
filter	O
response	O
in	O
O	O
(	O
N	O
)	O
time	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
of	O
the	O
image	O
[	O
reference	O
]	O
.	O

During	O
back	Method
-	Method
propagation	Method
,	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
inputs	O
are	O
calculated	O
by	O
sending	O
the	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
outputs	O
through	O
the	O
same	O
M	O
Gaussian	Method
filters	Method
in	O
reverse	O
direction	O
.	O

In	O
terms	O
of	O
permutohedral	Method
lattice	Method
operations	Method
,	O
this	O
can	O
be	O
accomplished	O
by	O
only	O
reversing	O
the	O
order	O
of	O
the	O
separable	Method
filters	Method
in	O
the	O
blur	Method
stage	Method
,	O
while	O
building	O
the	O
permutohedral	O
lattice	O
,	O
splatting	Method
,	O
and	O
slicing	O
in	O
the	O
same	O
way	O
as	O
in	O
the	O
forward	O
pass	O
.	O

Therefore	O
,	O
back	Method
-	Method
propagation	Method
through	O
this	O
filtering	Method
stage	Method
can	O
also	O
be	O
performed	O
in	O
O	O
(	O
N	O
)	O
time	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
two	O
Gaussian	Method
kernels	Method
,	O
a	O
spatial	Method
kernel	Method
and	O
a	O
bilateral	Method
kernel	Method
.	O

In	O
this	O
work	O
,	O
for	O
simplicity	O
,	O
we	O
keep	O
the	O
bandwidth	O
values	O
of	O
the	O
filters	O
fixed	O
.	O

It	O
is	O
also	O
possible	O
to	O
use	O
multiple	O
spatial	O
and	O
bilateral	O
kernels	O
with	O
different	O
bandwidth	O
values	O
and	O
learn	O
their	O
optimal	O
linear	Method
combination	Method
.	O

section	O
:	O
Weighting	Method
Filter	Method
Outputs	O
The	O
next	O
step	O
of	O
the	O
mean	Method
-	Method
field	Method
iteration	Method
is	O
taking	O
a	O
weighted	O
sum	O
of	O
the	O
M	O
filter	O
outputs	O
from	O
the	O
previous	O
step	O
,	O
for	O
each	O
class	O
label	O
l.	O
When	O
each	O
class	O
label	O
is	O
considered	O
individually	O
,	O
this	O
can	O
be	O
viewed	O
as	O
usual	O
convolution	Method
with	O
a	O
1	Method
×	Method
1	Method
filter	Method
with	O
M	O
input	O
channels	O
,	O
and	O
one	O
output	O
channel	O
.	O

Since	O
both	O
inputs	O
and	O
the	O
outputs	O
to	O
this	O
step	O
are	O
known	O
during	O
back	Method
-	Method
propagation	Method
,	O
the	O
error	O
derivative	O
w.r.t	O
.	O

the	O
filter	O
weights	O
can	O
be	O
computed	O
,	O
making	O
it	O
possible	O
to	O
automatically	O
learn	O
the	O
filter	O
weights	O
(	O
relative	O
contributions	O
from	O
each	O
Gaussian	Method
filter	Method
output	O
from	O
the	O
previous	O
stage	O
)	O
.	O

Error	O
derivative	O
w.r.t	O
.	O

the	O
inputs	O
can	O
also	O
be	O
computed	O
in	O
the	O
usual	O
manner	O
to	O
pass	O
the	O
error	O
derivatives	O
down	O
to	O
the	O
previous	O
stage	O
.	O

To	O
obtain	O
a	O
higher	O
number	O
of	O
tunable	O
parameters	O
,	O
in	O
contrast	O
to	O
[	O
reference	O
]	O
,	O
we	O
use	O
independent	O
kernel	O
weights	O
for	O
each	O
class	O
label	O
.	O

The	O
intuition	O
is	O
that	O
the	O
relative	O
importance	O
of	O
the	O
spatial	O
kernel	O
vs	O
the	O
bilateral	Method
kernel	Method
depends	O
on	O
the	O
visual	O
class	O
.	O

For	O
example	O
,	O
bilateral	O
kernels	O
may	O
have	O
on	O
the	O
one	O
hand	O
a	O
high	O
importance	O
in	O
bicycle	Task
detection	Task
,	O
because	O
similarity	O
of	O
colours	O
is	O
determinant	O
;	O
on	O
the	O
other	O
hand	O
they	O
may	O
have	O
low	O
importance	O
for	O
TV	Task
detection	Task
,	O
given	O
that	O
whatever	O
is	O
inside	O
the	O
TV	O
screen	O
may	O
have	O
many	O
different	O
colours	O
.	O

section	O
:	O
Compatibility	Method
Transform	Method
In	O
the	O
compatibility	Method
transform	Method
step	Method
,	O
outputs	O
from	O
the	O
previous	O
step	O
(	O
denoted	O
byQ	O
in	O
Algorithm	O
1	O
)	O
are	O
shared	O
between	O
the	O
labels	O
to	O
a	O
varied	O
extent	O
,	O
depending	O
on	O
the	O
compatibility	O
between	O
these	O
labels	O
.	O

Compatibility	O
between	O
the	O
two	O
labels	O
l	O
and	O
l	O
is	O
parameterized	O
by	O
the	O
label	O
compatibility	O
function	O
µ	O
(	O
l	O
,	O
l	O
)	O
.	O

The	O
Potts	Method
model	Method
,	O
given	O
by	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
[	O
l	O
=	O
l	O
]	O
,	O
where	O
[	O
.	O

]	O
is	O
the	O
Iverson	O
bracket	O
,	O
assigns	O
a	O
fixed	O
penalty	O
if	O
different	O
labels	O
are	O
assigned	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

A	O
limitation	O
of	O
this	O
model	O
is	O
that	O
it	O
assigns	O
the	O
same	O
penalty	O
for	O
all	O
different	O
pairs	O
of	O
labels	O
.	O

Intuitively	O
,	O
better	O
results	O
can	O
be	O
obtained	O
by	O
taking	O
the	O
compatibility	O
between	O
different	O
label	O
pairs	O
into	O
account	O
and	O
penalizing	O
the	O
assignments	O
accordingly	O
.	O

For	O
example	O
,	O
assigning	O
labels	O
"	O
person	O
"	O
and	O
"	O
bicycle	O
"	O
to	O
nearby	O
pixels	O
should	O
have	O
a	O
lesser	O
penalty	O
than	O
assigning	O
labels	O
"	O
sky	O
"	O
and	O
"	O
bicycle	O
"	O
.	O

Therefore	O
,	O
learning	O
the	O
function	O
µ	O
from	O
data	O
is	O
preferred	O
to	O
fixing	O
it	O
in	O
advance	O
with	O
Potts	Method
model	Method
.	O

We	O
also	O
relax	O
our	O
compatibility	Method
transform	Method
model	Method
by	O
assuming	O
that	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
µ	O
(	O
l	O
,	O
l	O
)	O
in	O
general	O
.	O

Compatibility	Task
transform	Task
step	Task
can	O
be	O
viewed	O
as	O
another	O
convolution	Method
layer	Method
where	O
the	O
spatial	O
receptive	O
field	O
of	O
the	O
filter	O
is	O
1	O
×	O
1	O
,	O
and	O
the	O
number	O
of	O
input	O
and	O
output	O
channels	O
are	O
both	O
L.	O
Learning	O
the	O
weights	O
of	O
this	O
filter	O
is	O
equivalent	O
to	O
learning	O
the	O
label	O
compatibility	O
function	O
µ.	O
Transferring	O
error	O
differentials	O
from	O
the	O
output	O
of	O
this	O
step	O
to	O
the	O
input	O
can	O
be	O
done	O
since	O
this	O
step	O
is	O
a	O
usual	O
convolution	Method
operation	Method
.	O

section	O
:	O
Adding	O
Unary	O
Potentials	O
In	O
this	O
step	O
,	O
the	O
output	O
from	O
the	O
compatibility	Method
transform	Method
stage	Method
is	O
subtracted	O
element	O
-	O
wise	O
from	O
the	O
unary	O
inputs	O
U	O
.	O

While	O
no	O
parameters	O
are	O
involved	O
in	O
this	O
step	O
,	O
transferring	O
error	O
differentials	O
can	O
be	O
done	O
trivially	O
by	O
copying	O
the	O
differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
to	O
both	O
inputs	O
with	O
the	O
appropriate	O
sign	O
.	O

section	O
:	O
Normalization	Task
Finally	O
,	O
the	O
normalization	O
step	O
of	O
the	O
iteration	O
can	O
be	O
considered	O
as	O
another	O
softmax	Method
operation	Method
with	O
no	O
parameters	O
.	O

Differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
can	O
be	O
passed	O
on	O
to	O
the	O
input	O
using	O
the	O
softmax	Method
operation	Method
's	Method
backward	Method
pass	Method
.	O

section	O
:	O
The	O
End	O
-	O
to	O
-	O
end	O
Trainable	Method
Network	Method
We	O
now	O
describe	O
our	O
end	O
-	O
to	O
-	O
end	Method
deep	Method
learning	Method
system	Method
for	O
semantic	Task
image	Task
segmentation	Task
.	O

To	O
pave	O
the	O
way	O
for	O
this	O
,	O
we	O
first	O
explain	O
how	O
repeated	O
mean	Method
-	Method
field	Method
iterations	Method
can	O
be	O
organized	O
as	O
an	O
RNN	Method
.	O

section	O
:	O
CRF	Method
as	O
RNN	Method
In	O
the	O
previous	O
section	O
,	O
it	O
was	O
shown	O
that	O
one	O
iteration	O
of	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
can	O
be	O
formulated	O
as	O
a	O
stack	O
of	O
common	O
CNN	Method
layers	O
(	O
see	O
Fig	O
.	O

1	O
)	O
.	O

We	O
use	O
the	O
function	O
f	Method
θ	Method
to	O
denote	O
the	O
transformation	O
done	O
by	O
one	O
mean	Method
-	Method
field	Method
iteration	Method
:	O
given	O
an	O
image	O
I	O
,	O
pixel	O
-	O
wise	O
unary	O
potential	O
values	O
U	O
and	O
an	O
estimation	O
of	O
marginal	O
probabilities	O
Q	O
in	O
from	O
the	O
previous	O
iteration	O
,	O
the	O
next	O
estimation	Task
of	Task
marginal	Task
distributions	Task
after	O
one	O
mean	Method
-	Method
field	Method
iteration	Method
is	O
given	O
by	O
f	Method
θ	Method
(	O
U	O
,	O
Q	O
in	O
,	O
I	O
)	O
.	O

..	O
,	O
l	O
L	O
}	O
represents	O
the	O
CRF	Method
parameters	O
described	O
in	O
Section	O
4	O
.	O

Multiple	O
mean	Method
-	Method
field	Method
iterations	Method
can	O
be	O
implemented	O
by	O
repeating	O
the	O
above	O
stack	O
of	O
layers	O
in	O
such	O
a	O
way	O
that	O
each	O
iteration	O
takes	O
Q	O
value	O
estimates	O
from	O
the	O
previous	O
iteration	O
and	O
the	O
unary	O
values	O
in	O
their	O
original	O
form	O
.	O

This	O
is	O
equivalent	O
to	O
treating	O
the	O
iterative	Method
mean	Method
-	Method
field	Method
inference	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
as	O
shown	O
in	O
Fig	O
.	O

2	O
.	O

Using	O
the	O
notation	O
in	O
the	O
figure	O
,	O
the	O
behaviour	O
of	O
the	O
network	O
is	O
given	O
by	O
the	O
following	O
equations	O
where	O
T	O
is	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
:	O
We	O
name	O
this	O
RNN	Method
structure	O
CRF	Method
-	Method
RNN	Method
.	O

Parameters	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
are	O
the	O
same	O
as	O
the	O
mean	O
-	O
field	O
parameters	O
described	O
in	O
Section	O
4	O
and	O
denoted	O
by	O
θ	O
here	O
.	O

Since	O
the	O
calculation	O
of	O
error	O
differentials	O
w.r.t	O
.	O

these	O
parameters	O
in	O
a	O
single	O
iteration	O
was	O
described	O
in	O
Section	O
4	O
,	O
they	O
can	O
be	O
learnt	O
in	O
the	O
RNN	Method
setting	O
using	O
the	O
standard	O
back	Method
-	Method
propagation	Method
through	Method
time	Method
algorithm	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

It	O
was	O
shown	O
in	O
[	O
reference	O
]	O
that	O
the	O
mean	Method
-	Method
field	Method
iterative	Method
algorithm	Method
for	O
dense	O
CRF	Method
converges	O
in	O
less	O
than	O
10	O
iterations	O
.	O

Furthermore	O
,	O
in	O
practice	O
,	O
after	O
Figure	O
2	O
.	O

The	O
CRF	Method
-	O
RNN	Method
Network	O
.	O

We	O
formulate	O
the	O
iterative	Method
mean	Method
-	Method
field	Method
algorithm	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O

Gating	O
functions	O
G1	O
and	O
G2	O
are	O
fixed	O
as	O
described	O
in	O
the	O
text	O
.	O

about	O
5	O
iterations	O
,	O
increasing	O
the	O
number	O
of	O
iterations	O
usually	O
does	O
not	O
significantly	O
improve	O
results	O
[	O
reference	O
]	O
.	O

Therefore	O
,	O
it	O
does	O
not	O
suffer	O
from	O
the	O
vanishing	Task
and	Task
exploding	Task
gradient	Task
problem	Task
inherent	O
to	O
deep	O
RNNs	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

This	O
allows	O
us	O
to	O
use	O
a	O
plain	O
RNN	Method
architecture	O
instead	O
of	O
more	O
sophisticated	O
architectures	O
such	O
as	O
LSTMs	Method
in	O
our	O
network	O
.	O

section	O
:	O
Completing	O
the	O
Picture	O
Our	O
approach	O
comprises	O
a	O
fully	Method
convolutional	Method
network	Method
stage	Method
,	O
which	O
predicts	O
pixel	O
-	O
level	O
labels	O
without	O
considering	O
structure	O
,	O
followed	O
by	O
a	O
CRF	Method
-	O
RNN	Method
stage	O
,	O
which	O
performs	O
CRF	Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
for	O
structured	Task
prediction	Task
.	O

The	O
complete	O
system	O
,	O
therefore	O
,	O
unifies	O
strengths	O
of	O
both	O
CNNs	Method
and	O
CRFs	Method
and	O
is	O
trainable	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
[	O
reference	O
]	O
and	O
the	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
procedure	O
.	O

During	O
training	Task
,	O
a	O
whole	O
image	O
(	O
or	O
many	O
of	O
them	O
)	O
can	O
be	O
used	O
as	O
the	O
mini	O
-	O
batch	O
and	O
the	O
error	O
at	O
each	O
pixel	O
output	O
of	O
the	O
network	O
can	O
be	O
computed	O
using	O
an	O
appropriate	O
loss	Method
function	Method
such	O
as	O
the	O
softmax	Method
loss	Method
with	O
respect	O
to	O
the	O
ground	Task
truth	Task
segmentation	Task
of	O
the	O
image	O
.	O

We	O
used	O
the	O
FCN	Method
-	Method
8s	Method
architecture	Method
of	O
[	O
reference	O
]	O
as	O
the	O
first	O
part	O
of	O
our	O
network	O
,	O
which	O
provides	O
unary	O
potentials	O
to	O
the	O
CRF	Method
.	O

This	O
network	O
is	O
based	O
on	O
the	O
VGG	Method
-	Method
16	Method
network	Method
[	O
reference	O
]	O
but	O
has	O
been	O
restructured	O
to	O
perform	O
pixel	Task
-	Task
wise	Task
prediction	Task
instead	O
of	O
image	Task
classification	Task
.	O

The	O
complete	O
architecture	O
of	O
our	O
network	O
,	O
including	O
the	O
FCN8s	Method
part	Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

In	O
the	O
forward	O
pass	O
through	O
the	O
network	O
,	O
once	O
the	O
computation	O
enters	O
the	O
CRF	Method
-	Method
RNN	Method
after	O
passing	O
through	O
the	O
CNN	Method
stage	O
,	O
it	O
takes	O
T	O
iterations	O
for	O
the	O
data	O
to	O
leave	O
the	O
loop	O
created	O
by	O
the	O
RNN	Method
.	O

Neither	O
the	O
CNN	Method
that	O
provides	O
unary	O
values	O
nor	O
the	O
layers	O
after	O
the	O
CRF	Method
-	Method
RNN	Method
(	O
i.e.	O
,	O
the	O
loss	O
layers	O
)	O
need	O
to	O
perform	O
any	O
computations	O
during	O
this	O
time	O
since	O
the	O
refinement	O
happens	O
only	O
inside	O
the	O
RNN	Method
's	O
loop	O
.	O

Once	O
the	O
output	O
Y	O
leaves	O
the	O
loop	O
,	O
next	O
stages	O
of	O
the	O
deep	Method
network	Method
after	O
the	O
CRF	Method
-	Method
RNN	Method
can	O
continue	O
the	O
forward	O
pass	O
.	O

In	O
our	O
setup	O
,	O
a	O
softmax	Method
loss	Method
layer	Method
directly	O
follows	O
the	O
CRF	Method
-	Method
RNN	Method
and	O
terminates	O
the	O
network	O
.	O

During	O
the	O
backward	O
pass	O
,	O
once	O
the	O
error	O
differentials	O
reach	O
the	O
CRF	Method
-	Method
RNN	Method
's	O
output	O
Y	O
,	O
they	O
similarly	O
spend	O
T	O
iterations	O
within	O
the	O
loop	O
before	O
reaching	O
the	O
RNN	Method
input	O
U	O
in	O
order	O
to	O
propagate	O
to	O
the	O
CNN	Method
which	O
provides	O
the	O
unary	O
input	O
.	O

In	O
each	O
iteration	O
inside	O
the	O
loop	O
,	O
error	O
differentials	O
are	O
computed	O
inside	O
each	O
component	O
of	O
the	O
mean	Method
-	Method
field	Method
iteration	Method
as	O
described	O
in	O
Section	O
4	O
.	O

We	O
note	O
that	O
unnecessarily	O
increasing	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
could	O
potentially	O
result	O
in	O
the	O
vanishing	Task
and	Task
exploding	Task
gradient	Task
problems	Task
in	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

We	O
,	O
however	O
,	O
did	O
not	O
experience	O
this	O
problem	O
during	O
our	O
experiments	O
.	O

section	O
:	O
Implementation	O
Details	O
In	O
the	O
present	O
section	O
we	O
describe	O
the	O
implementation	O
details	O
of	O
the	O
proposed	O
network	O
,	O
as	O
well	O
as	O
its	O
training	Method
process	Method
.	O

The	O
high	O
-	O
level	O
architecture	O
of	O
our	O
system	O
,	O
which	O
was	O
implemented	O
using	O
the	O
popular	O
Caffe	O
[	O
reference	O
]	O
deep	O
learning	O
library	O
,	O
is	O
shown	O
in	O
Fig	O
.	O

3	O
.	O

Complete	O
architecture	O
of	O
the	O
deep	Method
network	Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

The	O
full	O
source	O
code	O
and	O
the	O
trained	O
models	O
of	O
our	O
approach	O
will	O
be	O
made	O
publicly	O
available	O
.	O

We	O
initialized	O
the	O
first	O
part	O
of	O
the	O
network	O
using	O
the	O
publicly	O
available	O
weights	O
of	O
the	O
FCN	Method
-	Method
8s	Method
network	Method
[	O
reference	O
]	O
.	O

The	O
compatibility	O
transform	O
parameters	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
were	O
initialized	O
using	O
the	O
Potts	Method
model	Method
,	O
and	O
kernel	O
width	O
and	O
weight	O
parameters	O
were	O
obtained	O
from	O
a	O
cross	Method
-	Method
validation	Method
process	Method
.	O

We	O
found	O
that	O
such	O
initialization	O
results	O
in	O
faster	O
convergence	Task
of	Task
training	Task
.	O

During	O
the	O
training	O
phase	O
,	O
parameters	O
of	O
the	O
whole	O
network	O
were	O
optimized	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

In	O
particular	O
we	O
used	O
full	Task
image	Task
training	Task
described	O
in	O
[	O
reference	O
]	O
,	O
with	O
learning	Metric
rate	Metric
fixed	O
at	O
10	O
and	O
momentum	O
set	O
to	O
0.99	O
.	O

These	O
extreme	O
values	O
of	O
the	O
parameters	O
were	O
used	O
since	O
we	O
employed	O
only	O
one	O
image	O
per	O
batch	O
to	O
avoid	O
reaching	O
memory	O
limits	O
of	O
the	O
GPU	O
.	O

In	O
all	O
our	O
experiments	O
,	O
during	O
training	Task
,	O
we	O
set	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
in	O
the	O
CRF	Method
-	Method
RNN	Method
to	O
5	O
to	O
avoid	O
vanishing	Task
/	Task
exploding	Task
gradient	Task
problems	Task
and	O
to	O
reduce	O
the	O
training	Metric
time	Metric
.	O

During	O
the	O
test	O
time	O
,	O
iteration	O
count	O
was	O
increased	O
to	O
10	O
.	O

The	O
effect	O
of	O
this	O
parameter	O
value	O
on	O
the	O
accuracy	Metric
is	O
discussed	O
in	O
section	O
7.1	O
.	O

Loss	Method
function	Method
During	O
the	O
training	O
of	O
the	O
models	O
that	O
achieved	O
the	O
best	O
results	O
reported	O
in	O
this	O
paper	O
,	O
we	O
used	O
the	O
standard	O
softmax	Method
loss	Method
function	Method
,	O
that	O
is	O
,	O
the	O
log	Method
-	Method
likelihood	Method
error	Method
function	Method
described	O
in	O
[	O
reference	O
]	O
.	O

The	O
standard	O
metric	O
used	O
in	O
the	O
Pascal	Material
VOC	Material
challenge	Material
is	O
the	O
average	Metric
intersection	Metric
over	Metric
union	Metric
(	O
IU	Metric
)	O
,	O
which	O
we	O
also	O
use	O
here	O
to	O
report	O
the	O
results	O
.	O

In	O
our	O
experiments	O
we	O
found	O
that	O
high	O
values	O
of	O
IU	Metric
on	O
the	O
validation	O
set	O
were	O
associated	O
to	O
low	O
values	O
of	O
the	O
averaged	Metric
softmax	Metric
loss	Metric
,	O
to	O
a	O
large	O
extent	O
.	O

We	O
also	O
tried	O
the	O
robust	O
loglikelihood	O
in	O
[	O
reference	O
]	O
as	O
a	O
loss	O
function	O
for	O
CRF	Method
-	O
RNN	Method
training	O
.	O

However	O
,	O
this	O
did	O
not	O
result	O
in	O
increased	O
accuracy	Metric
nor	O
faster	O
convergence	Metric
.	O

Normalization	Method
techniques	Method
As	O
described	O
in	O
Section	O
4	O
,	O
we	O
use	O
the	O
exponential	O
function	O
followed	O
by	O
pixel	Method
-	Method
wise	Method
normalization	Method
across	O
channels	O
in	O
several	O
stages	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

Since	O
this	O
operation	O
has	O
a	O
tendency	O
to	O
result	O
in	O
small	O
gradients	O
with	O
respect	O
to	O
the	O
input	O
when	O
the	O
input	O
value	O
is	O
large	O
,	O
we	O
conducted	O
several	O
experiments	O
where	O
we	O
replaced	O
this	O
by	O
a	O
rectifier	Method
linear	Method
unit	Method
(	O
ReLU	Method
)	Method
operation	Method
followed	O
by	O
a	O
normalization	Method
across	O
the	O
channels	O
.	O

Our	O
hypothesis	O
was	O
that	O
this	O
approach	O
may	O
approximate	O
the	O
original	O
operation	O
adequately	O
while	O
speeding	O
up	O
the	O
training	Task
due	O
to	O
improved	O
gradients	O
.	O

Furthermore	O
,	O
ReLU	Method
would	O
induce	O
sparsity	O
on	O
the	O
probability	O
of	O
labels	O
assigned	O
to	O
pixels	O
,	O
implicitly	O
pruning	O
low	O
likelihood	O
configurations	O
,	O
which	O
could	O
have	O
a	O
positive	O
effect	O
.	O

However	O
,	O
this	O
approach	O
did	O
not	O
lead	O
to	O
better	O
results	O
,	O
obtaining	O
1	O
%	O
IU	Metric
lower	O
than	O
the	O
original	O
setting	O
performance	O
.	O

section	O
:	O
Experiments	O
We	O
present	O
experimental	O
results	O
with	O
the	O
proposed	O
CRF	Method
-	O
RNN	Method
framework	O
.	O

We	O
use	O
these	O
datasets	O
:	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
,	O
and	O
the	O
Pascal	Material
Context	Material
dataset	Material
.	O

We	O
use	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
as	O
it	O
has	O
become	O
the	O
golden	O
standard	O
to	O
comprehensively	O
evaluate	O
any	O
new	O
semantic	Task
segmentation	Task
approach	O
in	O
comparison	O
to	O
existing	O
methods	O
.	O

We	O
also	O
use	O
the	O
Pascal	Material
Context	Material
dataset	Material
to	O
assess	O
how	O
well	O
our	O
approach	O
performs	O
on	O
a	O
dataset	O
with	O
different	O
characteristics	O
.	O

section	O
:	O
Pascal	Material
VOC	Material
Datasets	Material
In	O
order	O
to	O
evaluate	O
our	O
approach	O
with	O
existing	O
methods	O
under	O
the	O
same	O
circumstances	O
,	O
we	O
conducted	O
two	O
main	O
experiments	O
with	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
,	O
followed	O
by	O
a	O
qualitative	O
experiment	O
.	O

In	O
the	O
first	O
experiment	O
,	O
following	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
we	O
used	O
a	O
training	O
set	O
consisted	O
of	O
VOC	Material
2012	Material
training	Material
data	Material
(	O
1464	O
images	O
)	O
,	O
and	O
training	O
and	O
validation	O
data	O
of	O
[	O
reference	O
]	O
,	O
which	O
amounts	O
to	O
a	O
total	O
of	O
11	O
,	O
685	O
images	O
.	O

After	O
removing	O
the	O
overlapping	O
images	O
between	O
VOC	Material
2012	Material
validation	Material
data	Material
and	O
this	O
training	O
dataset	O
,	O
we	O
were	O
left	O
with	O
346	O
images	O
from	O
the	O
original	O
VOC	Material
2012	Material
validation	Material
set	Material
to	O
validate	O
our	O
models	O
on	O
.	O

We	O
call	O
this	O
set	O
the	O
reduced	Metric
validation	Metric
set	Metric
in	O
the	O
sequel	O
.	O

Annotations	O
of	O
the	O
VOC	Material
2012	Material
test	Material
set	Material
,	O
which	O
consists	O
of	O
1456	O
images	O
,	O
are	O
not	O
publicly	O
available	O
and	O
hence	O
the	O
final	O
results	O
on	O
the	O
test	O
set	O
were	O
obtained	O
by	O
submitting	O
the	O
results	O
to	O
the	O
Pascal	Material
VOC	Material
challenge	Material
evaluation	Material
server	Material
[	O
reference	O
]	O
.	O

Regardless	O
of	O
the	O
smaller	O
number	O
of	O
images	O
,	O
we	O
found	O
that	O
the	O
relative	O
improvements	O
of	O
the	O
accuracy	Metric
on	O
our	O
validation	O
set	O
were	O
in	O
good	O
agreement	O
with	O
the	O
test	O
set	O
.	O

As	O
a	O
first	O
step	O
we	O
directly	O
compared	O
the	O
potential	O
advantage	O
of	O
learning	O
the	O
model	O
end	O
-	O
to	O
-	O
end	O
with	O
respect	O
to	O
alternative	O
learning	Method
strategies	Method
.	O

These	O
are	O
plain	O
FCN	Method
-	Method
8s	Method
without	O
applying	O
CRF	Method
,	O
and	O
with	O
CRF	Method
as	O
a	O
postprocessing	Method
method	Method
disconnected	O
from	O
the	O
training	O
of	O
FCN	Method
,	O
which	O
is	O
comparable	O
to	O
the	O
approach	O
described	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
reported	O
in	O
Table	O
1	O
and	O
show	O
a	O
clear	O
advantage	O
of	O
the	O
end	O
-	O
to	O
-	O
end	Method
strategy	Method
over	O
the	O
offline	Method
application	Method
of	O
CRF	Method
as	O
a	O
post	Method
-	Method
processing	Method
method	Method
.	O

This	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
during	O
the	O
SGD	Method
training	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
,	O
the	O
CNN	Method
component	O
and	O
the	O
CRF	Method
component	O
learn	O
how	O
to	O
co	O
-	O
operate	O
with	O
each	O
other	O
to	O
produce	O
the	O
optimum	O
output	O
of	O
the	O
whole	O
network	O
.	O

We	O
then	O
proceeded	O
to	O
compare	O
our	O
approach	O
with	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
that	O
used	O
training	O
data	O
from	O
the	O
standard	O
VOC	Material
2012	Material
training	Material
and	Material
validation	Material
sets	Material
,	O
and	O
from	O
the	O
dataset	O
published	O
with	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
2	O
,	O
above	O
the	O
bar	O
,	O
and	O
we	O
can	O
see	O
that	O
our	O
approach	O
outperforms	O
all	O
competitors	O
.	O

In	O
the	O
second	O
experiment	O
,	O
in	O
addition	O
to	O
the	O
above	O
training	O
set	O
,	O
we	O
used	O
data	O
from	O
the	O
Microsoft	O
COCO	Material
dataset	O
[	O
reference	O
]	O
as	O
was	O
done	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

We	O
selected	O
images	O
from	O
MS	O
COCO	Material
2014	O
training	O
set	O
where	O
the	O
ground	Task
truth	Task
segmentation	Task
has	O
at	O
least	O
200	O
pixels	O
marked	O
with	O
classes	O
labels	O
present	O
in	O
the	O
VOC	Material
2012	Material
dataset	Material
.	O

With	O
this	O
selection	O
,	O
we	O
ended	O
up	O
using	O
66	O
,	O
099	O
images	O
from	O
the	O
COCO	Material
dataset	O
and	O
therefore	O
a	O
total	O
of	O
66	O
,	O
099	O
+	O
11	O
,	O
685	O
=	O
77	O
,	O
784	O
training	O
images	O
were	O
used	O
in	O
the	O
second	O
experiment	O
.	O

The	O
same	O
reduced	Metric
validation	Metric
set	Metric
was	O
used	O
in	O
this	O
second	O
experiment	O
as	O
well	O
.	O

In	O
this	O
case	O
,	O
we	O
first	O
fine	O
-	O
tuned	O
the	O
plain	Method
FCN	Method
-	Method
32s	Method
network	Method
(	O
without	O
the	O
CRF	Method
-	Method
RNN	Method
part	Method
)	O
on	O
COCO	Material
data	O
,	O
then	O
we	O
built	O
an	O
FCN	Method
-	Method
8s	Method
network	Method
with	O
the	O
learnt	O
weights	O
and	O
finally	O
train	O
the	O
CRF	Method
-	O
RNN	Method
network	O
end	O
-	O
to	O
-	O
end	O
using	O
VOC	Material
2012	Material
training	Material
data	Material
only	O
.	O

Since	O
the	O
MS	O
COCO	Material
ground	Task
truth	Task
segmentation	Task
data	O
contains	O
somewhat	O
coarse	O
segmentation	O
masks	O
where	O
objects	O
are	O
not	O
delineated	O
properly	O
,	O
we	O
found	O
that	O
fine	O
-	O
tuning	O
our	O
model	O
with	O
COCO	Material
did	O
not	O
yield	O
significant	O
improvements	O
.	O

This	O
can	O
be	O
understood	O
because	O
the	O
primary	O
advantage	O
of	O
our	O
model	O
comes	O
from	O
delineating	O
the	O
objects	O
and	O
improving	O
fine	O
segmentation	O
boundaries	O
.	O

The	O
VOC	Material
2012	Material
training	Material
dataset	Material
therefore	O
helps	O
our	O
model	O
learn	O
this	O
task	O
effectively	O
.	O

The	O
results	O
of	O
this	O
experiment	O
are	O
shown	O
in	O
Table	O
2	O
,	O
below	O
the	O
bar	O
,	O
and	O
we	O
see	O
that	O
our	O
approach	O
sets	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
VOC	Material
2012	Material
dataset	Material
.	O

Note	O
that	O
in	O
both	O
setups	O
,	O
our	O
approach	O
outperforms	O
competing	O
methods	O
due	O
to	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
of	O
the	O
CNN	Method
and	O
CRF	Method
in	O
the	O
unified	O
CRF	Method
-	O
RNN	Method
framework	O
.	O

We	O
also	O
evaluated	O
our	O
models	O
on	O
the	O
VOC	Material
2010	Material
,	O
and	O
VOC	Material
2011	Material
test	Material
set	Material
(	O
see	O
Table	O
2	O
)	O
.	O

In	O
all	O
cases	O
our	O
method	O
achieves	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
order	O
to	O
have	O
a	O
qualitative	O
evidence	O
about	O
how	O
CRF	Method
-	Method
RNN	Method
learns	O
,	O
we	O
visualize	O
the	O
compatibility	O
function	O
learned	O
after	O
the	O
training	O
stage	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
as	O
a	O
matrix	Method
representation	Method
in	O
Fig	O
.	O

5	O
.	O

Element	O
(	O
i	O
,	O
j	O
)	O
of	O
this	O
matrix	O
corresponds	O
to	O
µ	O
(	O
i	O
,	O
j	O
)	O
defined	O
earlier	O
:	O
a	O
high	O
value	O
at	O
(	O
i	O
,	O
j	O
)	O
implies	O
high	O
penalty	O
for	O
assigning	O
label	O
i	O
to	O
a	O
pixel	O
when	O
a	O
similar	O
pixel	O
(	O
spatially	O
or	O
appearance	O
wise	O
)	O
is	O
assigned	O
label	O
j.	O
For	O
example	O
we	O
can	O
appreciate	O
that	O
the	O
learned	O
compatibility	O
matrix	O
assigns	O
a	O
low	O
penalty	O
to	O
pairs	O
of	O
labels	O
that	O
tend	O
to	O
appear	O
together	O
,	O
such	O
as	O
[	O
Motorbike	O
,	O
Person	O
]	O
,	O
and	O
[	O
Dining	O
[	O
reference	O
]	O
n	O
/	O
a	O
39.1	O
n	O
/	O
a	O
O2PCPMC	O
[	O
reference	O
]	O
49.6	O
48.8	O
47.8	O
Divmbest	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
48.1	O
NUS	O
-	O
UDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
50.0	O
SDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
51.6	O
MSRA	Method
-	Method
CFM	Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
61.8	O
FCN	Method
-	Method
8s	Method
[	O
reference	O
]	O
n	O
/	O
a	O
62.7	O
62.2	O
Hypercolumn	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
62.6	O
Zoomout	O
[	O
reference	O
]	O
64.4	O
64.1	O
64.4	O
Context	O
-	O
Deep	O
-	O
CNN	Method
-	O
CRF	Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
70.7	O
DeepLabMSc	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.6	O
Our	O
method	O
w	O
/	O
o	O
COCO	Material
73.6	O
72.4	O
72.0	O
BoxSup	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.0	O
DeepLab	O
[	O
reference	O
][	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
72.7	O
Our	O
method	O
with	O
COCO	Material
75.7	O
75.0	O
74.7	O
Table	O
2	O
.	O

Mean	O
IU	Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	Method
-	Method
RNN	Method
,	O
compared	O
to	O
the	O
other	O
approaches	O
on	O
the	O
Pascal	O
VOC	Material
2010	Material
-	O
2012	O
test	O
datasets	O
.	O

Methods	O
from	O
the	O
first	O
group	O
do	O
not	O
use	O
MS	O
COCO	Material
data	O
for	O
training	O
.	O

The	O
methods	O
from	O
the	O
second	O
group	O
use	O
both	O
COCO	Material
and	O
VOC	O
datasets	O
for	O
training	O
.	O

section	O
:	O
Pascal	Material
Context	Material
Dataset	Material
We	O
conducted	O
an	O
experiment	O
on	O
the	O
Pascal	Material
Context	Material
dataset	Material
[	O
reference	O
]	O
,	O
which	O
differs	O
from	O
the	O
previous	O
one	O
in	O
the	O
larger	O
number	O
of	O
classes	O
considered	O
,	O
59	O
.	O

We	O
used	O
the	O
provided	O
partitions	O
of	O
training	O
and	O
validation	O
sets	O
,	O
and	O
the	O
obtained	O
results	O
are	O
reported	O
in	O
Table	O
3	O
.	O

Table	O
3	O
.	O

Mean	O
IU	Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	Method
-	Method
RNN	Method
,	O
evaluated	O
on	O
the	O
Pascal	Material
Context	Material
validation	Material
set	Material
.	O

Figure	O
5	O
.	O

Visualization	O
of	O
the	O
learnt	O
label	O
compatibility	O
matrix	O
.	O

In	O
the	O
standard	O
Potts	Method
model	Method
,	O
diagonal	O
entries	O
are	O
equal	O
to	O
−1	O
,	O
while	O
off	O
-	O
diagonal	O
entries	O
are	O
zero	O
.	O

These	O
values	O
have	O
changed	O
after	O
the	O
end	O
-	O
to	O
-	O
end	O
training	O
of	O
our	O
network	O
.	O

Best	O
viewed	O
in	O
colour	O
.	O

section	O
:	O
Effect	O
of	O
Design	O
Choices	O
We	O
performed	O
a	O
number	O
of	O
additional	O
experiments	O
on	O
the	O
Pascal	Material
VOC	Material
2012	Material
validation	Material
set	Material
described	O
above	O
to	O
study	O
the	O
effect	O
of	O
some	O
design	O
choices	O
we	O
made	O
.	O

We	O
first	O
studied	O
the	O
performance	O
gains	O
attained	O
by	O
our	O
modifications	O
to	O
the	O
CRF	Method
over	O
the	O
CRF	Method
approach	O
proposed	O
by	O
[	O
reference	O
]	O
.	O

We	O
found	O
that	O
using	O
different	O
filter	O
weights	O
for	O
different	O
classes	O
improved	O
the	O
performance	O
by	O
1.8	O
percentage	O
points	O
,	O
and	O
that	O
introducing	O
the	O
asymmetric	Method
compatibility	Method
transform	Method
further	O
boosted	O
the	O
performance	O
by	O
0.9	O
percentage	O
points	O
.	O

Regarding	O
the	O
RNN	Method
parameter	O
iteration	O
count	O
T	O
,	O
incrementing	O
it	O
to	O
T	O
=	O
10	O
during	O
the	O
test	O
time	O
,	O
from	O
T	O
=	O
5	O
during	O
the	O
train	O
time	O
,	O
produced	O
an	O
accuracy	Metric
improvement	O
of	O
0.2	O
percentage	O
points	O
.	O

Setting	O
T	O
=	O
10	O
also	O
during	O
training	O
reduced	O
the	O
accuracy	Metric
by	O
0.7	O
percentage	O
points	O
.	O

We	O
believe	O
that	O
this	O
might	O
be	O
due	O
to	O
a	O
vanishing	O
gradient	O
effect	O
caused	O
by	O
using	O
too	O
many	O
iterations	O
.	O

In	O
practice	O
that	O
leads	O
to	O
the	O
first	O
part	O
of	O
the	O
network	O
(	O
the	O
one	O
producing	O
unary	O
potentials	O
)	O
receiving	O
a	O
very	O
weak	O
error	O
gradient	O
signal	O
during	O
training	O
,	O
thus	O
hampering	O
its	O
learning	Method
capacity	Method
.	O

End	Metric
-	Metric
to	Metric
-	Metric
end	Metric
training	Metric
after	O
the	O
initialization	O
of	O
CRF	Method
parameters	O
improved	O
performance	O
by	O
3.4	O
percentage	O
points	O
.	O

We	O
also	O
conducted	O
an	O
experiment	O
where	O
we	O
froze	O
the	O
FCN8s	Method
part	Method
and	O
fine	O
-	O
tuned	O
only	O
the	O
RNN	Method
part	O
(	O
i.e.	O
,	O
CRF	Method
parameters	O
)	O
.	O

It	O
improved	O
the	O
performance	O
over	O
initialization	Method
by	O
only	O
1	O
percentage	O
point	O
.	O

We	O
therefore	O
conclude	O
that	O
end	O
-	O
toend	O
training	O
significantly	O
contributed	O
to	O
boost	O
the	O
accuracy	Metric
of	O
the	O
system	O
.	O

Treating	O
each	O
iteration	O
of	O
mean	Method
-	Method
field	Method
inference	Method
as	O
an	O
independent	O
step	O
with	O
its	O
own	O
parameters	O
,	O
and	O
training	O
endto	O
-	O
end	O
with	O
5	O
such	O
iterations	O
yielded	O
a	O
final	O
mean	O
IU	Metric
score	O
of	O
only	O
70.9	O
,	O
supporting	O
the	O
hypothesis	O
that	O
the	O
recurrent	O
structure	O
of	O
our	O
approach	O
is	O
important	O
for	O
its	O
success	O
.	O

section	O
:	O
Conclusion	O
We	O
presented	O
CRF	Method
-	Method
RNN	Method
,	O
an	O
interpretation	O
of	O
dense	O
CRFs	Method
as	O
Recurrent	Method
Neural	Method
Networks	Method
.	O

Our	O
formulation	O
fully	O
integrates	O
CRF	Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
with	O
emerging	O
deep	Method
learning	Method
techniques	Method
.	O

In	O
particular	O
,	O
the	O
proposed	O
CRF	Method
-	Method
RNN	Method
can	O
be	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
traditional	O
deep	Method
neural	Method
network	Method
:	O
It	O
is	O
capable	O
of	O
passing	O
on	O
error	O
differentials	O
from	O
its	O
outputs	O
to	O
inputs	O
during	O
back	Method
-	Method
propagation	Method
based	Method
training	Method
of	O
the	O
deep	Method
network	Method
while	O
learning	O
CRF	Method
parameters	O
.	O

We	O
demonstrate	O
the	O
use	O
of	O
this	O
approach	O
by	O
utilizing	O
it	O
for	O
the	O
semantic	Task
segmentation	Task
task	O
:	O
we	O
form	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
deep	Method
network	Method
by	O
combining	O
a	O
fully	Method
convolutional	Method
neural	Method
network	Method
with	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

Our	O
system	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
popular	O
Pascal	Material
VOC	Material
segmentation	Material
benchmark	Material
.	O

This	O
improvement	O
can	O
be	O
attributed	O
to	O
the	O
uniting	O
of	O
the	O
strengths	O
of	O
CNNs	Method
and	O
CRFs	Method
in	O
a	O
single	O
deep	Method
network	Method
.	O

In	O
the	O
future	O
,	O
we	O
plan	O
to	O
investigate	O
the	O
advantages	O
/	O
disadvantages	O
of	O
restricting	O
the	O
capabilities	O
of	O
the	O
RNN	Method
part	O
of	O
our	O
network	O
to	O
mean	O
-	O
field	O
inference	O
of	O
dense	O
CRF	Method
.	O

A	O
sensible	O
baseline	O
to	O
the	O
work	O
presented	O
here	O
would	O
be	O
to	O
use	O
more	O
standard	O
RNNs	Method
(	O
e.g.	O
LSTMs	Method
)	O
that	O
learn	O
to	O
iteratively	O
improve	O
the	O
input	O
unary	O
potentials	O
to	O
make	O
them	O
closer	O
to	O
the	O
ground	O
-	O
truth	O
.	O

section	O
:	O
section	O
:	O
Acknowledgement	O
This	O
work	O
was	O
supported	O
by	O
grants	O
Leverhulme	O
Trust	O
,	O
EPSRC	O
EP	O
/	O
I001107	O
/	O
2	O
and	O
ERC	O
321162	O
-	O
HELIOS	O
.	O

We	O
thank	O
the	O
Caffe	O
team	O
,	O
Baidu	O
IDL	O
,	O
and	O
the	O
Oxford	O
ARC	O
team	O
for	O
their	O
support	O
.	O

We	O
gratefully	O
acknowledge	O
GPU	O
donations	O
from	O
NVIDIA	O
.	O

section	O
:	O
document	O
:	O
The	O
Reactor	Method
:	O
A	O
fast	Method
and	Method
sample	Method
-	Method
efficient	Method
Actor	Method
-	Method
Critic	Method
agent	Method
for	O
Reinforcement	Task
Learning	Task
In	O
this	O
work	O
,	O
we	O
present	O
a	O
new	O
agent	Method
architecture	Method
,	O
called	O
Reactor	Method
,	O
which	O
combines	O
multiple	O
algorithmic	Method
and	Method
architectural	Method
contributions	Method
to	O
produce	O
an	O
agent	O
with	O
higher	O
sample	Metric
-	Metric
efficiency	Metric
than	O
Prioritized	Method
Dueling	Method
DQN	Method
wang2017sample	O
and	O
Categorical	Method
DQN	Method
bellemare2017distributional	O
,	O
while	O
giving	O
better	O
run	Metric
-	Metric
time	Metric
performance	O
than	O
A3C	O
mnih2016asynchronous	O
.	O

Our	O
first	O
contribution	O
is	O
a	O
new	O
policy	Method
evaluation	Method
algorithm	Method
called	O
Distributional	Method
Retrace	Method
,	O
which	O
brings	O
multi	Task
-	Task
step	Task
off	Task
-	Task
policy	Task
updates	Task
to	O
the	O
distributional	Task
reinforcement	Task
learning	Task
setting	Task
.	O

The	O
same	O
approach	O
can	O
be	O
used	O
to	O
convert	O
several	O
classes	O
of	O
multi	Method
-	Method
step	Method
policy	Method
evaluation	Method
algorithms	Method
,	O
designed	O
for	O
expected	Task
value	Task
evaluation	Task
,	O
into	O
distributional	Method
algorithms	Method
.	O

Next	O
,	O
we	O
introduce	O
the	O
-	Method
leave	Method
-	Method
one	Method
-	Method
out	Method
policy	Method
gradient	Method
algorithm	Method
,	O
which	O
improves	O
the	O
trade	O
-	O
off	O
between	O
variance	Metric
and	O
bias	O
by	O
using	O
action	O
values	O
as	O
a	O
baseline	O
.	O

Our	O
final	O
algorithmic	O
contribution	O
is	O
a	O
new	O
prioritized	Method
replay	Method
algorithm	Method
for	O
sequences	O
,	O
which	O
exploits	O
the	O
temporal	O
locality	O
of	O
neighboring	O
observations	O
for	O
more	O
efficient	O
replay	Task
prioritization	Task
.	O

Using	O
the	O
Atari	Material
2600	Material
benchmarks	Material
,	O
we	O
show	O
that	O
each	O
of	O
these	O
innovations	O
contribute	O
to	O
both	O
sample	Metric
efficiency	Metric
and	O
final	Metric
agent	Metric
performance	Metric
.	O

Finally	O
,	O
we	O
demonstrate	O
that	O
Reactor	Method
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
after	O
200	O
million	O
frames	O
and	O
less	O
than	O
a	O
day	O
of	O
training	O
.	O

section	O
:	O
Introduction	O
Model	Method
-	Method
free	Method
deep	Method
reinforcement	Method
learning	Method
has	O
achieved	O
several	O
remarkable	O
successes	O
in	O
domains	O
ranging	O
from	O
super	Task
-	Task
human	Task
-	Task
level	Task
control	Task
in	O
video	O
games	O
mnih15human	O
and	O
the	O
game	O
of	O
Go	O
silver2016mastering	O
,	O
agzero	O
,	O
to	O
continuous	Task
motor	Task
control	Task
tasks	Task
lillicrap2015continuous	O
,	O
schulman2015trust	O
.	O

Much	O
of	O
the	O
recent	O
work	O
can	O
be	O
divided	O
into	O
two	O
categories	O
.	O

First	O
,	O
those	O
of	O
which	O
that	O
,	O
often	O
building	O
on	O
the	O
DQN	Method
framework	Method
,	O
act	O
-	O
greedily	O
according	O
to	O
an	O
action	Method
-	Method
value	Method
function	Method
and	O
train	O
using	O
mini	O
-	O
batches	O
of	O
transitions	O
sampled	O
from	O
an	O
experience	O
replay	O
buffer	O
van2016deep	O
,	O
wang2015dueling	O
,	O
he2016learning	O
,	O
anschel2017averaged	O
.	O

These	O
value	Method
-	Method
function	Method
agents	Method
benefit	O
from	O
improved	O
sample	Metric
complexity	Metric
,	O
but	O
tend	O
to	O
suffer	O
from	O
long	O
runtimes	Metric
(	O
e.g.	O
DQN	Method
requires	O
approximately	O
a	O
week	O
to	O
train	O
on	O
Atari	Material
)	O
.	O

The	O
second	O
category	O
are	O
the	O
actor	Method
-	Method
critic	Method
agents	Method
,	O
which	O
includes	O
the	O
asynchronous	Method
advantage	Method
actor	Method
-	Method
critic	Method
(	Method
A3C	Method
)	Method
algorithm	Method
,	O
introduced	O
by	O
mnih2016asynchronous	O
.	O

These	O
agents	O
train	O
on	O
transitions	O
collected	O
by	O
multiple	O
actors	O
running	O
,	O
and	O
often	O
training	O
,	O
in	O
parallel	O
schulman2017proximal	O
,	O
vezhnevets2017feudal	O
.	O

The	O
deep	Method
actor	Method
-	Method
critic	Method
agents	Method
train	O
on	O
each	O
trajectory	O
only	O
once	O
,	O
and	O
thus	O
tend	O
to	O
have	O
worse	O
sample	Metric
complexity	Metric
.	O

However	O
,	O
their	O
distributed	O
nature	O
allows	O
significantly	O
faster	O
training	Task
in	O
terms	O
of	O
wall	Metric
-	Metric
clock	Metric
time	Metric
.	O

Still	O
,	O
not	O
all	O
existing	O
algorithms	O
can	O
be	O
put	O
in	O
the	O
above	O
two	O
categories	O
and	O
various	O
hybrid	O
approaches	O
do	O
exist	O
zhao2016deep	O
,	O
o2016combining	O
,	O
gu2016q	O
,	O
wang2017sample	O
.	O

Data	Task
-	Task
efficiency	Task
and	O
off	Task
-	Task
policy	Task
learning	Task
are	O
essential	O
for	O
many	O
real	Task
-	Task
world	Task
domains	Task
where	O
interactions	O
with	O
the	O
environment	O
are	O
expensive	O
.	O

Similarly	O
,	O
wall	O
-	O
clock	O
time	O
(	O
time	Metric
-	Metric
efficiency	Metric
)	O
directly	O
impacts	O
an	O
algorithm	O
’s	O
applicability	O
through	O
resource	Metric
costs	Metric
.	O

The	O
focus	O
of	O
this	O
work	O
is	O
to	O
produce	O
an	O
agent	O
that	O
is	O
sample	O
-	O
and	O
time	O
-	O
efficient	O
.	O

To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
new	O
reinforcement	Method
learning	Method
agent	Method
,	O
called	O
Reactor	Method
(	O
Retrace	Method
-	Method
Actor	Method
)	O
,	O
which	O
takes	O
a	O
principled	O
approach	O
to	O
combining	O
the	O
sample	Metric
-	Metric
efficiency	Metric
of	O
off	Method
-	Method
policy	Method
experience	Method
replay	Method
with	O
the	O
time	O
-	O
efficiency	O
of	O
asynchronous	Method
algorithms	Method
.	O

We	O
combine	O
recent	O
advances	O
in	O
both	O
categories	O
of	O
agents	O
with	O
novel	O
contributions	O
to	O
produce	O
an	O
agent	O
that	O
inherits	O
the	O
benefits	O
of	O
both	O
and	O
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
over	O
57	O
Atari	Material
2600	O
games	O
.	O

Our	O
primary	O
contributions	O
are	O
(	O
1	O
)	O
a	O
novel	O
policy	Method
gradient	Method
algorithm	Method
,	O
-	O
LOO	O
,	O
which	O
makes	O
better	O
use	O
of	O
action	O
-	O
value	O
estimates	O
to	O
improve	O
the	O
policy	O
gradient	O
;	O
(	O
2	O
)	O
the	O
first	O
multi	Method
-	Method
step	Method
off	Method
-	Method
policy	Method
distributional	Method
reinforcement	Method
learning	Method
algorithm	Method
,	O
distributional	Method
Retrace	Method
(	O
)	O
;	O
(	O
3	O
)	O
a	O
novel	O
prioritized	Method
replay	Method
for	O
off	Task
-	Task
policy	Task
sequences	Task
of	Task
transitions	Task
;	O
and	O
(	O
4	O
)	O
an	O
optimized	Method
network	Method
and	Method
parallel	Method
training	Method
architecture	Method
.	O

We	O
begin	O
by	O
reviewing	O
background	O
material	O
,	O
including	O
relevant	O
improvements	O
to	O
both	O
value	Method
-	Method
function	Method
agents	Method
and	O
actor	Method
-	Method
critic	Method
agents	Method
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
introduce	O
each	O
of	O
our	O
primary	O
contributions	O
and	O
present	O
the	O
Reactor	Method
agent	O
.	O

Finally	O
,	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
present	O
experimental	O
results	O
on	O
the	O
57	O
Atari	Material
2600	O
games	O
from	O
the	O
Arcade	O
Learning	O
Environment	O
(	O
ALE	O
)	O
bellemare2013arcade	O
,	O
as	O
well	O
as	O
a	O
series	O
of	O
ablation	Task
studies	Task
for	O
the	O
various	O
components	O
of	O
Reactor	Method
.	O

section	O
:	O
Background	O
We	O
consider	O
a	O
Markov	Method
decision	Method
process	Method
(	Method
MDP	Method
)	Method
with	O
state	O
space	O
and	O
finite	O
action	O
space	O
.	O

A	O
(	O
stochastic	Method
)	Method
policy	Method
is	O
a	O
mapping	O
from	O
states	O
to	O
a	O
probability	O
distribution	O
over	O
actions	O
.	O

We	O
consider	O
a	O
-	O
discounted	O
infinite	O
-	O
horizon	O
criterion	O
,	O
with	O
the	O
discount	O
factor	O
,	O
and	O
define	O
for	O
policy	O
the	O
action	O
-	O
value	O
of	O
a	O
state	O
-	O
action	O
pair	O
as	O
where	O
is	O
a	O
trajectory	O
generated	O
by	O
choosing	O
in	O
and	O
following	O
thereafter	O
,	O
i.e.	O
,	O
(	O
for	O
)	O
,	O
and	O
is	O
the	O
reward	O
signal	O
.	O

The	O
objective	O
in	O
reinforcement	Task
learning	Task
is	O
to	O
find	O
an	O
optimal	Method
policy	Method
,	O
which	O
maximises	O
.	O

The	O
optimal	O
action	O
-	O
values	O
are	O
given	O
by	O
.	O

subsection	O
:	O
Value	Method
-	Method
based	Method
algorithms	Method
The	O
Deep	Method
Q	Method
-	Method
Network	Method
(	Method
DQN	Method
)	Method
framework	Method
,	O
introduced	O
by	O
mnih15human	O
,	O
popularised	O
the	O
current	O
line	O
of	O
research	O
into	O
deep	Method
reinforcement	Method
learning	Method
by	O
reaching	O
human	O
-	O
level	O
,	O
and	O
beyond	O
,	O
performance	O
across	O
57	O
Atari	Material
2600	O
games	O
in	O
the	O
ALE	O
.	O

While	O
DQN	Method
includes	O
many	O
specific	O
components	O
,	O
the	O
essence	O
of	O
the	O
framework	O
,	O
much	O
of	O
which	O
is	O
shared	O
by	O
Neural	Method
Fitted	Method
Q	Method
-	O
Learning	O
riedmiller2005neural	O
,	O
is	O
to	O
use	O
of	O
a	O
deep	Method
convolutional	Method
neural	Method
network	Method
to	O
approximate	O
an	O
action	Method
-	Method
value	Method
function	Method
,	O
training	O
this	O
approximate	Method
action	Method
-	Method
value	Method
function	Method
using	O
the	O
Q	Method
-	Method
Learning	Method
algorithm	Method
watkins1992	O
and	O
mini	O
-	O
batches	O
of	O
one	O
-	O
step	O
transitions	O
(	O
)	O
drawn	O
randomly	O
from	O
an	O
experience	O
replay	O
buffer	O
lin1992self	O
.	O

Additionally	O
,	O
the	O
next	O
-	O
state	O
action	O
-	O
values	O
are	O
taken	O
from	O
a	O
target	O
network	O
,	O
which	O
is	O
updated	O
to	O
match	O
the	O
current	O
network	O
periodically	O
.	O

Thus	O
,	O
the	O
temporal	Metric
difference	Metric
(	Metric
TD	Metric
)	Metric
error	Metric
for	O
transition	O
used	O
by	O
these	O
algorithms	O
is	O
given	O
by	O
where	O
denotes	O
the	O
parameters	O
of	O
the	O
network	O
and	O
are	O
the	O
parameters	O
of	O
the	O
target	O
network	O
.	O

Since	O
this	O
seminal	O
work	O
,	O
we	O
have	O
seen	O
numerous	O
extensions	O
and	O
improvements	O
that	O
all	O
share	O
the	O
same	O
underlying	O
framework	O
.	O

Double	Method
DQN	Method
van2016deep	O
,	O
attempts	O
to	O
correct	O
for	O
the	O
over	Task
-	Task
estimation	Task
bias	Task
inherent	O
in	O
Q	Method
-	Method
Learning	Method
by	O
changing	O
the	O
second	O
term	O
of	O
eq	O
:	O
tderr	O
to	O
.	O

The	O
dueling	Method
architecture	Method
wang2015dueling	O
,	O
changes	O
the	O
network	O
to	O
estimate	O
action	O
-	O
values	O
using	O
separate	O
network	Method
heads	Method
and	O
with	O
Recently	O
,	O
rainbow	O
introduced	O
Rainbow	Method
,	O
a	O
value	Method
-	Method
based	Method
reinforcement	Method
learning	Method
agent	Method
combining	O
many	O
of	O
these	O
improvements	O
into	O
a	O
single	O
agent	O
and	O
demonstrating	O
that	O
they	O
are	O
largely	O
complementary	O
.	O

Rainbow	Method
significantly	O
out	O
performs	O
previous	O
methods	O
,	O
but	O
also	O
inherits	O
the	O
poorer	O
time	Metric
-	Metric
efficiency	Metric
of	O
the	O
DQN	Method
framework	Method
.	O

We	O
include	O
a	O
detailed	O
comparison	O
between	O
Reactor	Method
and	O
Rainbow	O
in	O
the	O
Appendix	O
.	O

In	O
the	O
remainder	O
of	O
the	O
section	O
we	O
will	O
describe	O
in	O
more	O
depth	O
other	O
recent	O
improvements	O
to	O
DQN	Method
.	O

subsubsection	O
:	O
Prioritized	Task
experience	Task
replay	Task
The	O
experience	Method
replay	Method
buffer	Method
was	O
first	O
introduced	O
by	O
lin1992self	O
and	O
later	O
used	O
in	O
DQN	O
mnih15human	O
.	O

Typically	O
,	O
the	O
replay	Method
buffer	Method
is	O
essentially	O
a	O
first	Method
-	Method
in	Method
-	Method
first	Method
-	Method
out	Method
queue	Method
with	O
new	O
transitions	O
gradually	O
replacing	O
older	O
transitions	O
.	O

The	O
agent	O
would	O
then	O
sample	O
a	O
mini	O
-	O
batch	O
uniformly	O
at	O
random	O
from	O
the	O
replay	O
buffer	O
.	O

Drawing	O
inspiration	O
from	O
prioritized	Method
sweeping	Method
moore1993prioritized	Method
,	O
prioritized	Method
experience	Method
replay	Method
replaces	O
the	O
uniform	Method
sampling	Method
with	O
prioritized	Method
sampling	Method
proportional	O
to	O
the	O
absolute	Metric
TD	Metric
error	Metric
schaul16prioritized	O
.	O

Specifically	O
,	O
for	O
a	O
replay	O
buffer	O
of	O
size	O
,	O
prioritized	O
experience	O
replay	O
samples	O
transition	O
with	O
probability	O
,	O
and	O
applies	O
weighted	Method
importance	Method
-	Method
sampling	Method
with	O
to	O
correct	O
for	O
the	O
prioritization	O
bias	O
,	O
where	O
Prioritized	Method
DQN	Method
significantly	O
increases	O
both	O
the	O
sample	Metric
-	Metric
efficiency	Metric
and	O
final	O
performance	O
over	O
DQN	Method
on	O
the	O
Atari	Material
2600	Material
benchmarks	Material
schaul2015prioritized	O
.	O

subsubsection	O
:	O
Retrace	Method
(	O
)	O
Retrace	Method
(	Method
)	Method
is	O
a	O
convergent	Method
off	Method
-	Method
policy	Method
multi	Method
-	Method
step	Method
algorithm	Method
extending	O
the	O
DQN	Method
agent	Method
munos2016safe	O
.	O

Assume	O
that	O
some	O
trajectory	O
has	O
been	O
generated	O
according	O
to	O
behaviour	O
policy	O
,	O
i.e.	O
,	O
.	O

Now	O
,	O
we	O
aim	O
to	O
evaluate	O
the	O
value	O
of	O
a	O
different	O
target	O
policy	O
,	O
i.e.	O
we	O
want	O
to	O
estimate	O
.	O

The	O
Retrace	Method
algorithm	Method
will	O
update	O
our	O
current	O
estimate	O
of	O
in	O
the	O
direction	O
of	O
where	O
is	O
the	O
temporal	O
difference	O
at	O
time	O
under	O
,	O
and	O
The	O
Retrace	Method
algorithm	Method
comes	O
with	O
the	O
theoretical	O
guarantee	O
that	O
in	O
finite	O
state	O
and	O
action	O
spaces	O
,	O
repeatedly	O
updating	O
our	O
current	O
estimate	O
according	O
to	O
(	O
[	O
reference	O
]	O
)	O
produces	O
a	O
sequence	O
of	O
Q	Method
functions	Method
which	O
converges	O
to	O
for	O
a	O
fixed	O
or	O
to	O
if	O
we	O
consider	O
a	O
sequence	O
of	O
policies	O
which	O
become	O
increasingly	O
greedy	O
w.r.t	O
.	O

the	O
estimates	O
munos2016safe	O
.	O

subsubsection	O
:	O
Distributional	Method
RL	Method
Distributional	Method
reinforcement	Method
learning	Method
refers	O
to	O
a	O
class	O
of	O
algorithms	O
that	O
directly	O
estimate	O
the	O
distribution	O
over	O
returns	O
,	O
whose	O
expectation	O
gives	O
the	O
traditional	O
value	O
function	O
bellemare2017distributional	O
.	O

Such	O
approaches	O
can	O
be	O
made	O
tractable	O
with	O
a	O
distributional	Method
Bellman	Method
equation	Method
,	O
and	O
the	O
recently	O
proposed	O
algorithm	O
showed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
the	O
Atari	Material
2600	Material
benchmarks	Material
.	O

parameterizes	O
the	O
distribution	O
over	O
returns	O
with	O
a	O
mixture	Method
over	Method
Diracs	Method
centered	O
on	O
a	O
uniform	O
grid	O
,	O
with	O
hyperparameters	O
that	O
bound	O
the	O
distribution	O
support	O
of	O
size	O
.	O

subsection	O
:	O
Actor	Method
-	Method
critic	Method
algorithms	Method
In	O
this	O
section	O
we	O
review	O
the	O
actor	Method
-	Method
critic	Method
framework	Method
for	O
reinforcement	Method
learning	Method
algorithms	Method
and	O
then	O
discuss	O
recent	O
advances	O
in	O
actor	Method
-	Method
critic	Method
algorithms	Method
along	O
with	O
their	O
various	O
trade	O
-	O
offs	O
.	O

The	O
asynchronous	Method
advantage	Method
actor	Method
-	Method
critic	Method
(	Method
A3C	Method
)	Method
algorithm	Method
mnih2016asynchronous	Method
,	O
maintains	O
a	O
parameterized	Method
policy	Method
and	Method
value	Method
function	Method
,	O
which	O
are	O
updated	O
with	O
A3C	O
uses	O
parallel	Method
CPU	Method
workers	Method
,	O
each	O
acting	O
independently	O
in	O
the	O
environment	O
and	O
applying	O
the	O
above	O
updates	O
asynchronously	O
to	O
a	O
shared	O
set	O
of	O
parameters	O
.	O

In	O
contrast	O
to	O
the	O
previously	O
discussed	O
value	Method
-	Method
based	Method
methods	Method
,	O
A3C	Method
is	O
an	O
on	Method
-	Method
policy	Method
algorithm	Method
,	O
and	O
does	O
not	O
use	O
a	O
GPU	Method
nor	O
a	O
replay	Method
buffer	Method
.	O

Proximal	Method
Policy	Method
Optimization	Method
(	O
PPO	Method
)	O
is	O
a	O
closely	O
related	O
actor	Method
-	Method
critic	Method
algorithm	Method
schulman2017proximal	Method
,	O
which	O
replaces	O
the	O
advantage	O
pgadv	Method
with	O
,	O
where	O
is	O
as	O
defined	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Although	O
both	O
PPO	Method
and	O
A3C	O
run	O
parallel	Method
workers	Method
collecting	O
trajectories	O
independently	O
in	O
the	O
environment	O
,	O
PPO	Method
collects	O
these	O
experiences	O
to	O
perform	O
a	O
single	O
,	O
synchronous	O
,	O
update	O
in	O
contrast	O
with	O
the	O
asynchronous	Method
updates	Method
of	O
A3C.	O
Actor	Method
-	Method
Critic	Method
Experience	Method
Replay	Method
(	O
ACER	Method
)	Method
extends	O
the	O
A3C	Method
framework	Method
with	O
an	O
experience	Method
replay	Method
buffer	Method
,	O
Retrace	Method
algorithm	Method
for	O
off	Task
-	Task
policy	Task
corrections	Task
,	O
and	O
the	O
Truncated	Method
Importance	Method
Sampling	Method
Likelihood	Method
Ratio	Method
(	Method
TISLR	Method
)	Method
algorithm	Method
used	O
for	O
off	Task
-	Task
policy	Task
policy	Task
optimization	Task
wang2017sample	O
.	O

section	O
:	O
The	O
Reactor	Method
The	O
Reactor	Method
is	O
a	O
combination	O
of	O
four	O
novel	O
contributions	O
on	O
top	O
of	O
recent	O
improvements	O
to	O
both	O
deep	Method
value	Method
-	Method
based	Method
RL	Method
and	O
policy	Method
-	Method
gradient	Method
algorithms	Method
.	O

Each	O
contribution	O
moves	O
Reactor	Method
towards	O
our	O
goal	O
of	O
achieving	O
both	O
sample	Metric
and	Metric
time	Metric
efficiency	Metric
.	O

subsection	O
:	O
-	O
LOO	O
The	O
Reactor	Method
architecture	O
represents	O
both	O
a	O
policy	Method
and	Method
action	Method
-	Method
value	Method
function	Method
.	O

We	O
use	O
a	O
policy	Method
gradient	Method
algorithm	Method
to	O
train	O
the	O
actor	Method
which	O
makes	O
use	O
of	O
our	O
current	O
estimate	O
of	O
.	O

Let	O
be	O
the	O
value	O
function	O
at	O
some	O
initial	O
state	O
,	O
the	O
policy	Method
gradient	Method
theorem	Method
says	O
that	O
,	O
where	O
refers	O
to	O
the	O
gradient	O
w.r.t	O
.	O

policy	O
parameters	O
Sutton00policygradient	O
.	O

We	O
now	O
consider	O
several	O
possible	O
ways	O
to	O
estimate	O
this	O
gradient	O
.	O

To	O
simplify	O
notation	O
,	O
we	O
drop	O
the	O
dependence	O
on	O
the	O
state	O
for	O
now	O
and	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
quantity	O
In	O
the	O
off	Task
-	Task
policy	Task
case	Task
,	O
we	O
consider	O
estimating	O
using	O
a	O
single	O
action	O
drawn	O
from	O
a	O
(	O
possibly	O
different	O
from	O
)	O
behaviour	O
distribution	O
.	O

Let	O
us	O
assume	O
that	O
for	O
the	O
chosen	O
action	O
we	O
have	O
access	O
to	O
an	O
unbiased	O
estimate	O
of	O
.	O

Then	O
,	O
we	O
can	O
use	O
likelihood	Method
ratio	Method
(	Method
LR	Method
)	Method
method	Method
combined	O
with	O
an	O
importance	Method
sampling	Method
(	Method
IS	Method
)	Method
ratio	Method
(	O
which	O
we	O
call	O
ISLR	Method
)	O
to	O
build	O
an	O
unbiased	O
estimate	O
of	O
:	O
where	O
is	O
a	O
baseline	O
that	O
depends	O
on	O
the	O
state	O
but	O
not	O
on	O
the	O
chosen	O
action	O
.	O

However	O
this	O
estimate	O
suffers	O
from	O
high	O
variance	O
.	O

A	O
possible	O
way	O
for	O
reducing	O
variance	Metric
is	O
to	O
estimate	O
directly	O
from	O
(	O
[	O
reference	O
]	O
)	O
by	O
using	O
the	O
return	O
for	O
the	O
chosen	O
action	O
and	O
our	O
current	O
estimate	O
of	O
for	O
the	O
other	O
actions	O
,	O
which	O
leads	O
to	O
the	O
so	O
-	O
called	O
leave	Method
-	Method
one	Method
-	Method
out	Method
(	Method
LOO	Method
)	Method
policy	Method
-	Method
gradient	Method
estimate	Method
:	O
This	O
estimate	O
has	O
low	O
variance	O
but	O
may	O
be	O
biased	O
if	O
the	O
estimated	O
values	O
differ	O
from	O
.	O

A	O
better	O
bias	Metric
-	Metric
variance	Metric
tradeoff	Metric
may	O
be	O
obtained	O
by	O
the	O
more	O
general	Method
-	Method
LOO	Method
policy	Method
-	Method
gradient	Method
estimate	Method
:	O
where	O
can	O
be	O
a	O
function	O
of	O
both	O
policies	O
,	O
and	O
,	O
and	O
the	O
selected	O
action	O
.	O

Notice	O
that	O
when	O
,	O
(	O
[	O
reference	O
]	O
)	O
reduces	O
to	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
when	O
,	O
then	O
(	O
[	O
reference	O
]	O
)	O
is	O
This	O
estimate	O
is	O
unbiased	O
and	O
can	O
be	O
seen	O
as	O
a	O
generalization	O
of	O
where	O
instead	O
of	O
using	O
a	O
state	O
-	O
only	O
dependent	O
baseline	O
,	O
we	O
use	O
a	O
state	O
-	O
and	O
-	O
action	O
-	O
dependent	O
baseline	O
(	O
our	O
current	O
estimate	O
)	O
and	O
add	O
the	O
correction	Method
term	Method
to	O
cancel	O
the	O
bias	O
.	O

Proposition	O
[	O
reference	O
]	O
gives	O
our	O
analysis	O
of	O
the	O
bias	O
of	O
,	O
with	O
a	O
proof	O
left	O
to	O
the	O
Appendix	O
.	O

propositionpropbias	O
Assume	O
and	O
that	O
.	O

Then	O
,	O
the	O
bias	O
of	O
is	O
.	O

Thus	O
the	O
bias	O
is	O
small	O
when	O
is	O
close	O
to	O
,	O
or	O
when	O
the	O
-	O
estimates	O
are	O
close	O
to	O
the	O
true	O
values	O
,	O
and	O
unbiased	O
regardless	O
of	O
the	O
estimates	O
if	O
.	O

The	O
variance	O
is	O
low	O
when	O
is	O
small	O
,	O
therefore	O
,	O
in	O
order	O
to	O
improve	O
the	O
bias	Metric
-	Metric
variance	Metric
tradeoff	Metric
we	O
recommend	O
using	O
the	O
-	Method
LOO	Method
estimate	Method
with	O
defined	O
as	O
:	O
for	O
some	O
constant	O
.	O

This	O
truncated	O
coefficient	O
shares	O
similarities	O
with	O
the	O
truncated	Method
IS	Method
gradient	Method
estimate	Method
introduced	O
in	O
wang2017sample	O
(	O
which	O
we	O
call	O
TISLR	Method
for	O
truncated	Method
-	Method
ISLR	Method
)	O
:	O
The	O
differences	O
are	O
:	O
(	O
i	O
)	O
we	O
truncate	O
instead	O
of	O
truncating	Method
,	O
which	O
provides	O
an	O
additional	O
variance	Metric
reduction	Metric
due	O
to	O
the	O
variance	O
of	O
the	O
LR	Method
(	O
since	O
this	O
LR	Method
may	O
be	O
large	O
when	O
a	O
low	O
probability	O
action	O
is	O
chosen	O
)	O
,	O
and	O
(	O
ii	O
)	O
we	O
use	O
our	O
-	O
baseline	O
instead	O
of	O
a	O
baseline	O
,	O
reducing	O
further	O
the	O
variance	O
of	O
the	O
LR	Method
estimate	Method
.	O

subsection	O
:	O
Distributional	Task
Retrace	Task
In	O
off	Task
-	Task
policy	Task
learning	Task
it	O
is	O
very	O
difficult	O
to	O
produce	O
an	O
unbiased	O
sample	O
of	O
when	O
following	O
another	O
policy	O
.	O

This	O
would	O
require	O
using	O
full	O
importance	Method
sampling	Method
correction	Method
along	O
the	O
trajectory	O
.	O

Instead	O
,	O
we	O
use	O
the	O
off	O
-	O
policy	O
corrected	O
return	O
computed	O
by	O
the	O
Retrace	Method
algorithm	Method
,	O
which	O
produces	O
a	O
(	O
biased	O
)	O
estimate	O
of	O
but	O
whose	O
bias	O
vanishes	O
asymptotically	O
munos2016safe	O
.	O

In	O
Reactor	Method
,	O
we	O
consider	O
predicting	O
an	O
approximation	O
of	O
the	O
return	Method
distribution	Method
function	Method
from	O
any	O
state	O
-	O
action	O
pair	O
in	O
a	O
similar	O
way	O
as	O
in	O
bellemare2017distributional	O
.	O

The	O
original	O
algorithm	O
C51	O
described	O
in	O
that	O
paper	O
considered	O
single	O
-	O
step	O
Bellman	Method
updates	Method
only	O
.	O

Here	O
we	O
need	O
to	O
extend	O
this	O
idea	O
to	O
multi	Task
-	Task
step	Task
updates	Task
and	O
handle	O
the	O
off	Task
-	Task
policy	Task
correction	Task
performed	O
by	O
the	O
Retrace	Method
algorithm	Method
,	O
as	O
defined	O
in	O
eq	O
:	O
retrace	Method
.	O

Next	O
,	O
we	O
describe	O
these	O
two	O
extensions	O
.	O

paragraph	O
:	O
Multi	Method
-	Method
step	Method
distributional	Method
Bellman	Method
operator	Method
:	O
First	O
,	O
we	O
extend	O
C51	Method
to	O
multi	O
-	O
step	O
Bellman	Task
backups	Task
.	O

We	O
consider	O
return	O
-	O
distributions	O
from	O
of	O
the	O
form	O
(	O
where	O
denotes	O
a	O
Dirac	O
in	O
)	O
which	O
are	O
supported	O
on	O
a	O
finite	O
uniform	O
grid	O
,	O
,	O
,	O
.	O

The	O
coefficients	O
(	O
discrete	O
distribution	O
)	O
corresponds	O
to	O
the	O
probabilities	O
assigned	O
to	O
each	O
atom	O
of	O
the	O
grid	O
.	O

From	O
an	O
observed	O
-	O
step	O
sequence	O
,	O
generated	O
by	O
behavior	Method
policy	Method
(	O
i.e	O
,	O
for	O
)	O
,	O
we	O
build	O
the	O
-	Method
step	Method
backed	Method
-	Method
up	Method
return	Method
-	Method
distribution	Method
from	O
.	O

The	O
-	O
step	O
distributional	O
Bellman	O
target	O
,	O
whose	O
expectation	O
is	O
,	O
is	O
given	O
by	O
:	O
Since	O
this	O
distribution	O
is	O
supported	O
on	O
the	O
set	O
of	O
atoms	O
,	O
which	O
is	O
not	O
necessarily	O
aligned	O
with	O
the	O
grid	O
,	O
we	O
do	O
a	O
projection	Method
step	Method
and	O
minimize	O
the	O
KL	Metric
-	Metric
loss	Metric
between	O
the	O
projected	O
target	O
and	O
the	O
current	O
estimate	O
,	O
just	O
as	O
with	O
C51	O
except	O
with	O
a	O
different	O
target	O
distribution	O
bellemare2017distributional	O
.	O

paragraph	O
:	O
Distributional	O
Retrace	O
:	O
Now	O
,	O
the	O
Retrace	Method
algorithm	Method
defined	O
in	O
eq	O
:	O
retrace	Method
involves	O
an	O
off	Method
-	Method
policy	Method
correction	Method
which	O
is	O
not	O
handled	O
by	O
the	O
previous	O
-	O
step	O
distributional	Method
Bellman	Method
backup	Method
.	O

The	O
key	O
to	O
extending	O
this	O
distributional	Task
back	Task
-	Task
up	Task
to	O
off	Task
-	Task
policy	Task
learning	Task
is	O
to	O
rewrite	O
the	O
Retrace	Method
algorithm	Method
as	O
a	O
linear	Method
combination	Method
of	Method
-	Method
step	Method
Bellman	Method
backups	Method
,	O
weighted	O
by	O
some	O
coefficients	O
.	O

Indeed	O
,	O
notice	O
that	O
eq	O
:	O
retrace	O
rewrites	O
as	O
where	O
.	O

These	O
coefficients	O
depend	O
on	O
the	O
degree	O
of	O
off	O
-	O
policy	O
-	O
ness	O
(	O
between	O
and	O
)	O
along	O
the	O
trajectory	O
.	O

We	O
have	O
that	O
,	O
but	O
notice	O
some	O
coefficients	O
may	O
be	O
negative	O
.	O

However	O
,	O
in	O
expectation	O
(	O
over	O
the	O
behavior	Method
policy	Method
)	O
they	O
are	O
non	O
-	O
negative	O
.	O

Indeed	O
,	O
by	O
definition	O
of	O
the	O
coefficients	O
eq	O
:	O
trace.cut	O
.	O

Thus	O
in	O
expectation	O
(	O
over	O
the	O
behavior	O
policy	O
)	O
,	O
the	O
Retrace	Method
update	Method
can	O
be	O
seen	O
as	O
a	O
convex	Method
combination	Method
of	Method
-	Method
step	Method
Bellman	Method
updates	Method
.	O

Then	O
,	O
the	O
distributional	Method
Retrace	Method
algorithm	Method
can	O
be	O
defined	O
as	O
backing	O
up	O
a	O
mixture	Method
of	Method
-	Method
step	Method
distributions	Method
.	O

More	O
precisely	O
,	O
we	O
define	O
the	O
Retrace	O
target	O
distribution	O
as	O
:	O
where	O
is	O
a	O
linear	Method
interpolation	Method
kernel	Method
,	O
projecting	O
onto	O
the	O
support	O
:	O
We	O
update	O
the	O
current	O
probabilities	O
by	O
performing	O
a	O
gradient	Method
step	Method
on	O
the	O
KL	Method
-	Method
loss	Method
Again	O
,	O
notice	O
that	O
some	O
target	O
‘	O
‘	O
probabilities	O
’	O
’	O
may	O
be	O
negative	O
for	O
some	O
sample	O
trajectory	O
,	O
but	O
in	O
expectation	O
they	O
will	O
be	O
non	O
-	O
negative	O
.	O

Since	O
the	O
gradient	O
of	O
a	O
KL	Method
-	Method
loss	Method
is	O
linear	O
w.r.t	O
.	O

its	O
first	O
argument	O
,	O
our	O
update	Method
rule	Method
eq	Method
:	O
kl.gradient	Method
provides	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
of	O
the	O
KL	O
between	O
the	O
expected	O
(	O
over	O
the	O
behavior	Method
policy	Method
)	O
Retrace	O
target	O
distribution	O
and	O
the	O
current	O
predicted	O
distribution	O
.	O

paragraph	O
:	O
Remark	O
:	O
The	O
same	O
method	O
can	O
be	O
applied	O
to	O
other	O
algorithms	O
(	O
such	O
as	O
TB	Method
(	Method
)	Method
precup2000eligibility	O
and	O
importance	Method
sampling	Method
precup01offpolicy	Method
)	O
in	O
order	O
to	O
derive	O
distributional	Method
versions	Method
of	O
other	O
off	Method
-	Method
policy	Method
multi	Method
-	Method
step	Method
RL	Method
algorithms	Method
.	O

subsection	O
:	O
Prioritized	Task
sequence	Task
replay	Task
Prioritized	Task
experience	Task
replay	Task
has	O
been	O
shown	O
to	O
boost	O
both	O
statistical	Metric
efficiency	Metric
and	O
final	O
performance	O
of	O
deep	Method
RL	Method
agents	Method
schaul16prioritized	O
.	O

However	O
,	O
as	O
originally	O
defined	O
prioritized	Method
replay	Method
does	O
not	O
handle	O
sequences	O
of	O
transitions	O
and	O
weights	O
all	O
unsampled	O
transitions	O
identically	O
.	O

In	O
this	O
section	O
we	O
present	O
an	O
alternative	O
initialization	Method
strategy	Method
,	O
called	O
lazy	Method
initialization	Method
,	O
and	O
argue	O
that	O
it	O
better	O
encodes	O
prior	O
information	O
about	O
temporal	O
difference	O
errors	O
.	O

We	O
then	O
briefly	O
describe	O
our	O
computationally	O
efficient	O
prioritized	Method
sequence	Method
sampling	Method
algorithm	Method
,	O
with	O
full	O
details	O
left	O
to	O
the	O
appendix	O
.	O

It	O
is	O
widely	O
recognized	O
that	O
TD	O
errors	O
tend	O
to	O
be	O
temporally	O
correlated	O
,	O
indeed	O
the	O
need	O
to	O
break	O
this	O
temporal	O
correlation	O
has	O
been	O
one	O
of	O
the	O
primary	O
justifications	O
for	O
the	O
use	O
of	O
experience	Task
replay	Task
mnih15human	O
.	O

Our	O
proposed	O
algorithm	O
begins	O
with	O
this	O
fundamental	O
assumption	O
.	O

theorem	O
:	O
.	O

Temporal	O
differences	O
are	O
temporally	O
correlated	O
,	O
with	O
correlation	O
decaying	O
on	O
average	O
with	O
the	O
time	O
-	O
difference	O
between	O
two	O
transitions	O
.	O

Prioritized	Method
experience	Method
replay	Method
adds	O
new	O
transitions	O
to	O
the	O
replay	O
buffer	O
with	O
a	O
constant	O
priority	O
,	O
but	O
given	O
the	O
above	O
assumption	O
we	O
can	O
devise	O
a	O
better	O
method	O
.	O

Specifically	O
,	O
we	O
propose	O
to	O
add	O
experience	O
to	O
the	O
buffer	O
with	O
no	O
priority	O
,	O
inserting	O
a	O
priority	O
only	O
after	O
the	O
transition	O
has	O
been	O
sampled	O
and	O
used	O
for	O
training	O
.	O

Also	O
,	O
instead	O
of	O
sampling	O
transitions	O
,	O
we	O
assign	O
priorities	O
to	O
all	O
(	O
overlapping	O
)	O
sequences	O
of	O
length	O
.	O

When	O
sampling	O
,	O
sequences	O
with	O
an	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
that	O
priority	O
.	O

Sequences	O
with	O
no	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
the	O
average	O
priority	O
of	O
assigned	O
priority	O
sequences	O
within	O
some	O
local	O
neighbourhood	O
.	O

Averages	Method
are	O
weighted	O
to	O
compensate	O
for	O
sampling	O
biases	O
(	O
i.e.	O
more	O
samples	O
are	O
made	O
in	O
areas	O
of	O
high	O
estimated	O
priorities	O
,	O
and	O
in	O
the	O
absence	O
of	O
weighting	O
this	O
would	O
lead	O
to	O
overestimation	O
of	O
unassigned	O
priorities	O
)	O
.	O

The	O
lazy	Method
initialization	Method
scheme	Method
starts	O
with	O
priorities	O
corresponding	O
to	O
the	O
sequences	O
for	O
which	O
a	O
priority	O
was	O
already	O
assigned	O
.	O

Then	O
it	O
extrapolates	O
a	O
priority	O
of	O
all	O
other	O
sequences	O
in	O
the	O
following	O
way	O
.	O

Let	O
us	O
define	O
a	O
partition	O
of	O
the	O
states	O
ordered	O
by	O
increasing	O
time	O
such	O
that	O
each	O
cell	O
contains	O
exactly	O
one	O
state	O
with	O
already	O
assigned	O
priority	O
.	O

We	O
define	O
the	O
estimated	O
priority	O
to	O
all	O
other	O
sequences	O
as	O
,	O
where	O
is	O
a	O
collection	O
of	O
contiguous	O
cells	O
containing	O
time	O
,	O
and	O
is	O
the	O
length	O
of	O
the	O
cell	O
containing	O
.	O

For	O
already	O
defined	O
priorities	O
denote	O
.	O

Cell	O
sizes	O
work	O
as	O
estimates	O
of	O
inverse	O
local	O
density	O
and	O
are	O
used	O
as	O
importance	O
weights	O
for	O
priority	Task
estimation	Task
.	O

For	O
the	O
algorithm	O
to	O
be	O
unbiased	O
,	O
partition	O
must	O
not	O
be	O
a	O
function	O
of	O
the	O
assigned	O
priorities	O
.	O

So	O
far	O
we	O
have	O
defined	O
a	O
class	O
of	O
algorithms	O
all	O
free	O
to	O
choose	O
the	O
partition	O
and	O
the	O
collection	O
of	O
cells	O
,	O
as	O
long	O
that	O
they	O
satisfy	O
the	O
above	O
constraints	O
.	O

Figure	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
illustrates	O
the	O
above	O
description	O
.	O

Now	O
,	O
with	O
probability	O
we	O
sample	O
uniformly	O
at	O
random	O
,	O
and	O
with	O
probability	O
we	O
sample	O
proportionally	O
to	O
.	O

We	O
implemented	O
an	O
algorithm	O
satisfying	O
the	O
above	O
constraints	O
and	O
called	O
it	O
Contextual	Method
Priority	Method
Tree	Method
(	O
CPT	Method
)	O
.	O

It	O
is	O
based	O
on	O
AVL	Method
trees	Method
velskii1976avl	O
and	O
can	O
execute	O
sampling	Method
,	O
insertion	Method
,	O
deletion	Method
and	O
density	Method
evaluation	Method
in	O
time	O
.	O

We	O
describe	O
CPT	O
in	O
detail	O
in	O
the	O
Appendix	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
treated	O
prioritization	Method
as	O
purely	O
a	O
variance	Method
reduction	Method
technique	Method
.	O

Importance	O
-	O
sampling	O
weights	O
were	O
evaluated	O
as	O
in	O
prioritized	Task
experience	Task
replay	Task
,	O
with	O
fixed	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

We	O
used	O
simple	O
gradient	Method
magnitude	Method
estimates	Method
as	O
priorities	O
,	O
corresponding	O
to	O
a	O
mean	Metric
absolute	Metric
TD	Metric
error	Metric
along	O
a	O
sequence	O
for	O
Retrace	Task
,	O
as	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
for	O
the	O
classical	Task
RL	Task
case	Task
,	O
and	O
total	O
variation	O
in	O
the	O
distributional	Task
Retrace	Task
case	Task
.	O

subsection	O
:	O
Agent	Method
architecture	Method
In	O
order	O
to	O
improve	O
CPU	O
utilization	O
we	O
decoupled	O
acting	O
from	O
learning	Task
.	O

This	O
is	O
an	O
important	O
aspect	O
of	O
our	O
architecture	O
:	O
an	O
acting	Method
thread	Method
receives	O
observations	O
,	O
submits	O
actions	O
to	O
the	O
environment	O
,	O
and	O
stores	O
transitions	O
in	O
memory	O
,	O
while	O
a	O
learning	Method
thread	Method
re	O
-	O
samples	O
sequences	O
of	O
experiences	O
from	O
memory	O
and	O
trains	O
on	O
them	O
(	O
Figure	O
[	O
reference	O
]	O
,	O
left	O
)	O
.	O

We	O
typically	O
execute	O
4	O
-	O
6	O
acting	O
steps	O
per	O
each	O
learning	O
step	O
.	O

We	O
sample	O
sequences	O
of	O
length	O
in	O
batches	O
of	O
4	O
.	O

A	O
moving	Method
network	Method
is	O
unrolled	O
over	O
frames	O
1	O
-	O
32	O
while	O
the	O
target	O
network	O
is	O
unrolled	O
over	O
frames	O
2	O
-	O
33	O
.	O

We	O
allow	O
the	O
agent	O
to	O
be	O
distributed	O
over	O
multiple	O
machines	O
each	O
containing	O
action	O
-	O
learner	O
pairs	O
.	O

Each	O
worker	O
downloads	O
the	O
newest	O
network	O
parameters	O
before	O
each	O
learning	O
step	O
and	O
sends	O
delta	O
-	O
updates	O
at	O
the	O
end	O
of	O
it	O
.	O

Both	O
the	O
network	O
and	O
target	O
network	O
are	O
stored	O
on	O
a	O
shared	O
parameter	O
server	O
while	O
each	O
machine	O
contains	O
its	O
own	O
local	O
replay	O
memory	O
.	O

Training	Task
is	O
done	O
by	O
downloading	O
a	O
shared	Method
network	Method
,	O
evaluating	O
local	O
gradients	O
and	O
sending	O
them	O
to	O
be	O
applied	O
on	O
the	O
shared	Method
network	Method
.	O

While	O
the	O
agent	O
can	O
also	O
be	O
trained	O
on	O
a	O
single	O
machine	O
,	O
in	O
this	O
work	O
we	O
present	O
results	O
of	O
training	O
obtained	O
with	O
either	O
10	O
or	O
20	O
actor	Method
-	Method
learner	Method
workers	Method
and	O
one	O
parameter	Method
server	Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
(	O
right	O
)	O
we	O
compare	O
resources	O
and	O
runtimes	O
of	O
Reactor	Method
with	O
related	O
algorithms	O
.	O

subsubsection	O
:	O
Network	Method
architecture	Method
In	O
some	O
domains	O
,	O
such	O
as	O
Atari	Material
,	O
it	O
is	O
useful	O
to	O
base	O
decisions	O
on	O
a	O
short	O
history	O
of	O
past	O
observations	O
.	O

The	O
two	O
techniques	O
generally	O
used	O
to	O
achieve	O
this	O
are	O
frame	Method
stacking	Method
and	O
recurrent	Method
network	Method
architectures	Method
.	O

We	O
chose	O
the	O
latter	O
over	O
the	O
former	O
for	O
reasons	O
of	O
implementation	Metric
simplicity	Metric
and	O
computational	Metric
efficiency	Metric
.	O

As	O
the	O
Retrace	Method
algorithm	Method
requires	O
evaluating	O
action	O
-	O
values	O
over	O
contiguous	O
sequences	O
of	O
trajectories	O
,	O
using	O
a	O
recurrent	Method
architecture	Method
allowed	O
each	O
frame	O
to	O
be	O
processed	O
by	O
the	O
convolutional	Method
network	Method
only	O
once	O
,	O
as	O
opposed	O
to	O
times	O
times	O
if	O
frame	O
concatenations	O
were	O
used	O
.	O

The	O
Reactor	Method
architecture	O
uses	O
a	O
recurrent	Method
neural	Method
network	Method
which	O
takes	O
an	O
observation	O
as	O
input	O
and	O
produces	O
two	O
outputs	O
:	O
categorical	O
action	O
-	O
value	O
distributions	O
(	O
here	O
is	O
a	O
bin	O
identifier	O
)	O
,	O
and	O
policy	O
probabilities	O
.	O

We	O
use	O
an	O
architecture	O
inspired	O
by	O
the	O
duelling	Method
network	Method
architecture	Method
wang2015dueling	O
.	O

We	O
split	O
action	O
-	O
value	O
-	O
distribution	O
logits	O
into	O
state	O
-	O
value	O
logits	O
and	O
advantage	O
logits	O
,	O
which	O
in	O
turn	O
are	O
connected	O
to	O
the	O
same	O
LSTM	Method
network	Method
hochreiter1997long	O
.	O

Final	O
action	O
-	O
value	O
logits	O
are	O
produced	O
by	O
summing	O
state	O
-	O
and	O
action	O
-	O
specific	O
logits	O
,	O
as	O
in	O
wang2015dueling	O
.	O

Finally	O
,	O
a	O
softmax	Method
layer	Method
on	O
top	O
for	O
each	O
action	O
produces	O
the	O
distributions	O
over	O
discounted	O
future	O
returns	O
.	O

The	O
policy	Method
head	Method
uses	O
a	O
softmax	Method
layer	Method
mixed	O
with	O
a	O
fixed	O
uniform	O
distribution	O
over	O
actions	O
,	O
where	O
this	O
mixing	O
ratio	O
is	O
a	O
hyperparameter	O
[	O
Section	O
5.1.3	O
]	O
wiering1999explorations	O
.	O

Policy	Method
and	Method
Q	Method
-	Method
networks	Method
have	O
separate	O
LSTMs	Method
.	O

Both	O
LSTMs	Method
are	O
connected	O
to	O
a	O
shared	Method
linear	Method
layer	Method
which	O
is	O
connected	O
to	O
a	O
shared	Method
convolutional	Method
neural	Method
network	Method
krizhevsky2012imagenet	Method
.	O

The	O
precise	O
network	O
specification	O
is	O
given	O
in	O
Table	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
.	O

Gradients	O
coming	O
from	O
the	O
policy	Method
LSTM	Method
are	O
blocked	O
and	O
only	O
gradients	O
originating	O
from	O
the	O
Q	Method
-	Method
network	Method
LSTM	Method
are	O
allowed	O
to	O
back	O
-	O
propagate	O
into	O
the	O
convolutional	Method
neural	Method
network	Method
.	O

We	O
block	O
gradients	O
from	O
the	O
policy	O
head	O
for	O
increased	O
stability	O
,	O
as	O
this	O
avoids	O
positive	O
feedback	O
loops	O
between	O
and	O
caused	O
by	O
shared	O
representations	O
.	O

We	O
used	O
the	O
Adam	Method
optimiser	Method
kingma2014adam	O
,	O
with	O
a	O
learning	Metric
rate	Metric
of	O
and	O
zero	O
momentum	O
because	O
asynchronous	O
updates	O
induce	O
implicit	O
momentum	O
mitliagkas2016asynchrony	O
.	O

Further	O
discussion	O
of	O
hyperparameters	O
and	O
their	O
optimization	Task
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

section	O
:	O
Experimental	O
Results	O
We	O
trained	O
and	O
evaluated	O
Reactor	Method
on	O
57	O
Atari	Material
games	O
bellemare2013arcade	O
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
performance	O
of	O
Reactor	Method
with	O
different	O
versions	O
of	O
Reactor	Method
each	O
time	O
leaving	O
one	O
of	O
the	O
algorithmic	O
improvements	O
out	O
.	O

We	O
can	O
see	O
that	O
each	O
of	O
the	O
algorithmic	O
improvements	O
(	O
Distributional	Method
retrace	Method
,	O
beta	Method
-	Method
LOO	Method
and	O
prioritized	Method
replay	Method
)	O
contributed	O
to	O
the	O
final	O
results	O
.	O

While	O
prioritization	Method
was	O
arguably	O
the	O
most	O
important	O
component	O
,	O
Beta	Method
-	Method
LOO	Method
clearly	O
outperformed	O
TISLR	Method
algorithm	Method
.	O

Although	O
distributional	Method
and	Method
non	Method
-	Method
distributional	Method
versions	Method
performed	O
similarly	O
in	O
terms	O
of	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
,	O
distributional	Method
version	Method
of	O
the	O
algorithm	O
generalized	O
better	O
when	O
tested	O
with	O
random	O
human	O
starts	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Comparing	O
to	O
prior	O
work	O
We	O
evaluated	O
Reactor	Method
with	O
target	O
update	O
frequency	O
,	O
and	O
-	O
LOO	O
with	O
on	O
57	O
Atari	Material
games	O
trained	O
on	O
10	O
machines	O
in	O
parallel	O
.	O

We	O
averaged	O
scores	O
over	O
200	O
episodes	O
using	O
30	O
random	O
human	O
starts	O
and	O
noop	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
)	O
.	O

We	O
calculated	O
mean	O
and	O
median	O
human	O
normalised	O
scores	O
across	O
all	O
games	O
.	O

We	O
also	O
ranked	O
all	O
algorithms	O
(	O
including	O
random	O
and	O
human	O
scores	O
)	O
for	O
each	O
game	O
and	O
evaluated	O
mean	Metric
rank	Metric
of	O
each	O
algorithm	O
across	O
all	O
57	O
Atari	Material
games	O
.	O

We	O
also	O
evaluated	O
mean	O
Rank	Metric
and	O
Elo	Metric
scores	Metric
for	O
each	O
algorithm	O
for	O
both	O
human	O
and	O
noop	O
start	O
settings	O
.	O

Please	O
refer	O
to	O
Section	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
for	O
more	O
details	O
.	O

Tables	O
[	O
reference	O
]	O
&	O
[	O
reference	O
]	O
compare	O
versions	O
of	O
our	O
algorithm	O
,	O
with	O
several	O
other	O
state	O
-	O
of	O
-	O
art	O
algorithms	O
across	O
57	O
Atari	Material
games	O
for	O
a	O
fixed	O
random	O
seed	O
across	O
all	O
games	O
bellemare2013arcade	O
.	O

We	O
compare	O
Reactor	Method
against	O
are	O
:	O
DQN	Method
mnih15human	O
,	O
Double	Method
DQN	Method
van2016deep	O
,	O
DQN	Method
with	Method
prioritised	Method
experience	Method
replay	Method
schaul2015prioritized	O
,	O
dueling	Method
architecture	Method
and	Method
prioritised	Method
dueling	Method
wang2015dueling	O
,	O
ACER	Method
wang2017sample	O
,	O
A3C	Method
mnih2016asynchronous	O
,	O
and	O
Rainbow	Method
rainbow	Method
.	O

Each	O
algorithm	O
was	O
exposed	O
to	O
200	O
million	O
frames	O
of	O
experience	O
,	O
or	O
500	O
million	O
frames	O
when	O
followed	O
by	O
,	O
and	O
the	O
same	O
pre	Method
-	Method
processing	Method
pipeline	Method
including	O
4	O
action	O
repeats	O
was	O
used	O
as	O
in	O
the	O
original	O
DQN	O
paper	O
mnih15human	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
Reactor	Method
exceeds	O
the	O
performance	O
of	O
all	O
algorithms	O
across	O
all	O
metrics	O
,	O
despite	O
requiring	O
under	O
two	O
days	O
of	O
training	O
.	O

With	O
500	O
million	O
frames	O
and	O
four	O
days	O
training	O
we	O
see	O
Reactor	Method
’s	O
performance	O
continue	O
to	O
improve	O
significantly	O
.	O

The	O
difference	O
in	O
time	Metric
-	Metric
efficiency	Metric
is	O
especially	O
apparent	O
when	O
comparing	O
Reactor	Method
and	O
Rainbow	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
,	O
right	O
)	O
.	O

Additionally	O
,	O
unlike	O
Rainbow	Method
,	O
Reactor	Method
does	O
not	O
use	O
Noisy	Method
Networks	Method
fortunato2017noisy	O
,	O
which	O
was	O
reported	O
to	O
have	O
contributed	O
to	O
the	O
performance	O
gains	O
.	O

When	O
evaluating	O
under	O
the	O
no	O
-	O
op	O
starts	O
regime	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
Reactor	Method
out	O
performs	O
all	O
methods	O
except	O
for	O
Rainbow	Method
.	O

This	O
suggests	O
that	O
Rainbow	Method
is	O
more	O
sample	O
-	O
efficient	O
when	O
training	O
and	O
evaluation	O
regimes	O
match	O
exactly	O
,	O
but	O
may	O
be	O
overfitting	O
to	O
particular	O
trajectories	O
due	O
to	O
the	O
significant	O
drop	O
in	O
performance	O
when	O
evaluated	O
on	O
the	O
random	O
human	O
starts	O
.	O

Regarding	O
ACER	Method
,	O
another	O
Retrace	Method
-	Method
based	Method
actor	Method
-	Method
critic	Method
architecture	Method
,	O
both	O
classical	O
and	O
distributional	Method
versions	Method
of	O
Reactor	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
exceeded	O
the	O
best	O
reported	O
median	O
human	Metric
normalized	Metric
score	Metric
of	O
1.9	O
with	O
noop	O
starts	O
achieved	O
in	O
500	O
million	O
steps	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
presented	O
a	O
new	O
off	Method
-	Method
policy	Method
agent	Method
based	O
on	O
Retrace	Method
actor	Method
-	Method
critic	Method
architecture	Method
and	O
show	O
that	O
it	O
achieves	O
similar	O
performance	O
as	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
while	O
giving	O
significant	O
real	O
-	O
time	O
performance	O
gains	O
.	O

We	O
demonstrate	O
the	O
benefits	O
of	O
each	O
of	O
the	O
suggested	O
algorithmic	Method
improvements	Method
,	O
including	O
Distributional	Method
Retrace	Method
,	O
beta	Method
-	Method
LOO	Method
policy	Method
gradient	Method
and	O
contextual	Method
priority	Method
tree	Method
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
*	O
proof	O
:	O
Proof	O
.	O

The	O
bias	O
of	O
is	O
∎	O
subsection	O
:	O
Hyperparameter	Method
optimization	Method
As	O
we	O
believe	O
that	O
algorithms	O
should	O
be	O
robust	O
with	O
respect	O
to	O
the	O
choice	O
of	O
hyperparameters	O
,	O
we	O
spent	O
little	O
effort	O
on	O
parameter	Task
optimization	Task
.	O

In	O
total	O
,	O
we	O
explored	O
three	O
distinct	O
values	O
of	O
learning	Metric
rates	Metric
and	O
two	O
values	O
of	O
ADAM	O
momentum	O
(	O
the	O
default	O
and	O
zero	O
)	O
and	O
two	O
values	O
of	O
on	O
a	O
subset	O
of	O
7	O
Atari	Material
games	O
without	O
prioritization	Method
using	O
non	Method
-	Method
distributional	Method
version	Method
of	Method
Reactor	Method
.	O

We	O
later	O
used	O
those	O
values	O
for	O
all	O
experiments	O
.	O

We	O
did	O
not	O
optimize	O
for	O
batch	O
sizes	O
and	O
sequence	O
length	O
or	O
any	O
prioritization	Method
hyperparamters	Method
.	O

subsection	O
:	O
Rank	Metric
and	O
Elo	Metric
evaluation	Metric
Commonly	O
used	O
mean	O
and	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
have	O
several	O
disadvantages	O
.	O

A	O
mean	O
human	O
normalized	O
score	O
implicitly	O
puts	O
more	O
weight	O
on	O
games	O
that	O
computers	O
are	O
good	O
and	O
humans	O
are	O
bad	O
at	O
.	O

Comparing	O
algorithm	O
by	O
a	O
mean	O
human	Metric
normalized	Metric
score	Metric
across	O
57	O
Atari	Material
games	O
is	O
almost	O
equivalent	O
to	O
comparing	O
algorithms	O
on	O
a	O
small	O
subset	O
of	O
games	O
close	O
to	O
the	O
median	O
and	O
thus	O
dominating	O
the	O
signal	O
.	O

Typically	O
a	O
set	O
of	O
ten	O
most	O
score	O
-	O
generous	O
games	O
,	O
namely	O
Assault	O
,	O
Asterix	O
,	O
Breakout	O
,	O
Demon	O
Attack	O
,	O
Double	O
Dunk	O
,	O
Gopher	O
,	O
Pheonix	O
,	O
Stargunner	O
,	O
Up’n	O
Down	O
and	O
Video	O
Pinball	O
can	O
explain	O
more	O
than	O
half	O
of	O
inter	Metric
-	Metric
algorithm	Metric
variance	Metric
.	O

A	O
median	Metric
human	Metric
normalized	Metric
score	Metric
has	O
the	O
opposite	O
disadvantage	O
by	O
effectively	O
discarding	O
very	O
easy	O
and	O
very	O
hard	O
games	O
from	O
the	O
comparison	O
.	O

As	O
typical	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
are	O
within	O
the	O
range	O
of	O
1	O
-	O
2.5	O
,	O
an	O
algorithm	O
which	O
scores	O
zero	O
points	O
on	O
Montezuma	O
’s	O
Revenge	O
is	O
evaluated	O
equal	O
to	O
the	O
one	O
which	O
scores	O
2500	O
points	O
,	O
as	O
both	O
performance	O
levels	O
are	O
still	O
below	O
human	O
performance	O
making	O
incremental	O
improvements	O
on	O
hard	O
games	O
not	O
being	O
reflected	O
in	O
the	O
overall	O
evaluation	O
.	O

In	O
order	O
to	O
address	O
both	O
problem	O
,	O
we	O
also	O
evaluated	O
mean	Metric
rank	Metric
and	O
Elo	Metric
metrics	Metric
for	O
inter	Metric
-	Metric
algorithm	Metric
comparison	Metric
.	O

Those	O
metrics	O
implicitly	O
assign	O
the	O
same	O
weight	O
to	O
each	O
game	O
,	O
and	O
as	O
a	O
result	O
is	O
more	O
sensitive	O
of	O
relative	O
performance	O
on	O
very	O
hard	O
and	O
easy	O
games	O
:	O
swapping	O
scores	O
of	O
two	O
algorithms	O
on	O
any	O
game	O
would	O
result	O
in	O
the	O
change	O
of	O
both	O
mean	Metric
rank	Metric
and	O
Elo	Metric
metrics	Metric
.	O

We	O
calculated	O
separate	O
mean	Metric
rank	Metric
and	O
Elo	Metric
scores	Metric
for	O
each	O
algorithm	O
using	O
results	O
of	O
test	O
evaluations	O
with	O
30	O
random	O
noop	O
-	O
starts	O
and	O
30	O
random	O
human	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

All	O
algorithms	O
were	O
ranked	O
across	O
each	O
game	O
separately	O
,	O
and	O
a	O
mean	Metric
rank	Metric
was	O
evaluated	O
across	O
57	O
Atari	Material
games	O
.	O

For	O
Elo	Method
score	Method
evaluation	Method
algorithm	Method
,	O
was	O
considered	O
to	O
win	O
over	O
algorithm	O
if	O
it	O
obtained	O
more	O
scores	O
on	O
a	O
given	O
Atari	Material
.	O

We	O
produced	O
an	O
empirical	Metric
win	Metric
-	Metric
probability	Metric
matrix	Metric
by	O
summing	O
wins	O
across	O
all	O
games	O
and	O
used	O
this	O
matrix	O
to	O
evaluate	O
Elo	Metric
scores	Metric
.	O

A	O
ranking	O
difference	O
of	O
400	O
corresponds	O
to	O
the	O
odds	O
of	O
winning	O
of	O
10:1	O
under	O
the	O
Gaussian	Method
assumption	Method
.	O

subsection	O
:	O
Contextual	Method
priority	Method
tree	Method
Contextual	Method
priority	Method
tree	Method
is	O
one	O
possible	O
implementation	O
of	O
lazy	Task
prioritization	Task
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

All	O
sequence	O
keys	O
are	O
put	O
into	O
a	O
balanced	Method
binary	Method
search	Method
tree	Method
which	O
maintains	O
a	O
temporal	O
order	O
.	O

An	O
AVL	Method
tree	Method
(	O
)	O
was	O
chosen	O
due	O
to	O
the	O
ease	O
of	O
implementation	O
and	O
because	O
it	O
is	O
on	O
average	O
more	O
evenly	O
balanced	O
than	O
a	O
Red	O
-	O
Black	O
Tree	O
.	O

Each	O
tree	O
node	O
has	O
up	O
to	O
two	O
children	O
(	O
left	O
and	O
right	O
)	O
and	O
contains	O
currently	O
stored	O
key	O
and	O
a	O
priority	O
of	O
the	O
key	O
which	O
is	O
either	O
set	O
or	O
is	O
unknown	O
.	O

Some	O
trees	O
may	O
only	O
have	O
a	O
single	O
child	O
subtree	O
while	O
some	O
may	O
have	O
none	O
.	O

In	O
addition	O
to	O
this	O
information	O
,	O
we	O
were	O
tracking	O
other	O
summary	O
statistics	O
at	O
each	O
node	O
which	O
was	O
re	O
-	O
evaluated	O
after	O
each	O
tree	Method
rotation	Method
.	O

The	O
summary	Metric
statistics	Metric
was	O
evaluated	O
by	O
consuming	O
previously	O
evaluated	O
summary	Metric
statistics	Metric
of	O
both	O
children	O
and	O
a	O
priority	O
of	O
the	O
key	O
stored	O
within	O
the	O
current	O
node	O
.	O

In	O
particular	O
,	O
we	O
were	O
tracking	O
a	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
and	O
mean	O
-	O
priority	O
estimates	O
updated	O
according	O
to	O
rules	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
was	O
always	O
known	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
while	O
mean	O
priority	O
estimates	O
per	O
key	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
could	O
either	O
be	O
known	O
or	O
unknown	O
.	O

If	O
a	O
mean	O
priority	O
of	O
either	O
one	O
child	O
subtree	O
or	O
a	O
key	O
stored	O
within	O
the	O
current	O
node	O
is	O
unknown	O
then	O
it	O
can	O
be	O
estimated	O
to	O
by	O
exploiting	O
information	O
coming	O
from	O
another	O
sibling	O
subtree	O
or	O
a	O
priority	O
stored	O
within	O
the	O
parent	O
node	O
.	O

Sampling	O
was	O
done	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
up	O
while	O
sampling	O
either	O
one	O
of	O
the	O
children	O
subtrees	O
or	O
the	O
currently	O
held	O
key	O
proportionally	O
to	O
the	O
total	O
estimated	O
priority	O
masses	O
contained	O
within	O
.	O

The	O
rules	O
used	O
to	O
evaluate	O
proportions	O
are	O
shown	O
in	O
orange	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Similarly	O
,	O
probabilities	O
of	O
arbitrary	O
keys	O
can	O
be	O
queried	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
towards	O
the	O
child	O
node	O
of	O
an	O
interest	O
while	O
maintaining	O
a	O
product	O
of	O
probabilities	O
at	O
each	O
branching	O
point	O
.	O

Insertion	Method
,	O
deletion	Method
,	O
sampling	Method
and	O
probability	Method
query	Method
operations	Method
can	O
be	O
done	O
in	O
O	O
(	O
ln	O
(	O
n	O
)	O
)	O
time	O
.	O

The	O
suggested	O
algorithm	O
has	O
the	O
desired	O
property	O
that	O
it	O
becomes	O
a	O
simple	O
proportional	Method
sampling	Method
algorithm	Method
once	O
all	O
the	O
priorities	O
are	O
known	O
.	O

While	O
some	O
key	O
priorities	O
are	O
unknown	O
,	O
they	O
are	O
estimated	O
by	O
using	O
nearby	O
known	O
key	O
priorities	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Each	O
time	O
when	O
a	O
new	O
sequence	O
key	O
is	O
added	O
to	O
the	O
tree	O
,	O
it	O
was	O
set	O
to	O
have	O
an	O
unknown	O
priority	O
.	O

Any	O
priority	O
was	O
assigned	O
only	O
after	O
the	O
key	O
got	O
first	O
sampled	O
and	O
the	O
corresponding	O
sequence	O
got	O
passed	O
through	O
the	O
learner	O
.	O

When	O
a	O
priority	O
of	O
a	O
key	O
is	O
set	O
or	O
updated	O
,	O
the	O
key	O
node	O
is	O
deliberately	O
removed	O
from	O
and	O
placed	O
back	O
to	O
the	O
tree	O
in	O
order	O
to	O
become	O
a	O
leaf	O
-	O
node	O
.	O

This	O
helped	O
to	O
set	O
priorities	O
of	O
nodes	O
in	O
the	O
immediate	O
vicinity	O
more	O
accurately	O
by	O
using	O
the	O
freshest	O
information	O
available	O
.	O

subsection	O
:	O
Network	Method
architecture	Method
The	O
value	O
of	O
is	O
the	O
minimum	O
probability	O
of	O
choosing	O
a	O
random	O
action	O
and	O
it	O
is	O
hard	O
-	O
coded	O
into	O
the	O
policy	Method
network	Method
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
overall	O
network	O
topology	O
while	O
Table	O
[	O
reference	O
]	O
specifies	O
network	O
layer	O
sizes	O
.	O

subsection	O
:	O
Comparisons	O
with	O
Rainbow	O
In	O
this	O
section	O
we	O
compare	O
Reactor	Method
with	O
the	O
recently	O
published	O
Rainbow	Method
agent	Method
rainbow	Method
.	O

While	O
ACER	Method
is	O
the	O
most	O
closely	O
related	O
algorithmically	O
,	O
Rainbow	Method
is	O
most	O
closely	O
related	O
in	O
terms	O
of	O
performance	O
and	O
thus	O
a	O
deeper	O
understanding	O
of	O
the	O
trade	O
-	O
offs	O
between	O
Rainbow	Method
and	O
Reactor	Method
may	O
benefit	O
interested	O
readers	O
.	O

There	O
are	O
many	O
architectural	O
and	O
algorithmic	O
differences	O
between	O
Rainbow	Method
and	O
Reactor	Method
.	O

We	O
will	O
therefore	O
begin	O
by	O
highlighting	O
where	O
they	O
agree	O
.	O

Both	O
use	O
a	O
categorical	O
action	O
-	O
value	O
distribution	O
critic	O
bellemare2017distributional	O
,	O
factored	O
into	O
state	O
and	O
state	O
-	O
action	O
logits	O
wang2015dueling	O
,	O
Both	O
use	O
prioritized	Method
replay	Method
,	O
and	O
finally	O
,	O
both	O
perform	O
-	O
step	O
Bellman	Method
updates	Method
.	O

Despite	O
these	O
similarities	O
,	O
Reactor	Method
and	O
Rainbow	Method
are	O
fundamentally	O
different	O
algorithms	O
and	O
are	O
based	O
upon	O
different	O
lines	O
of	O
research	O
.	O

While	O
Rainbow	Method
uses	O
Q	Method
-	Method
Learning	Method
and	O
is	O
based	O
upon	O
DQN	Method
mnih15human	O
,	O
Reactor	Method
is	O
an	O
actor	Method
-	Method
critic	Method
algorithm	Method
most	O
closely	O
based	O
upon	O
A3C	Method
mnih2016asynchronous	O
.	O

Each	O
inherits	O
some	O
design	O
choices	O
from	O
their	O
predecessors	O
,	O
and	O
we	O
have	O
not	O
performed	O
an	O
extensive	O
ablation	O
comparing	O
these	O
various	O
differences	O
.	O

Instead	O
,	O
we	O
will	O
discuss	O
four	O
of	O
the	O
differences	O
we	O
believe	O
are	O
important	O
but	O
less	O
obvious	O
.	O

First	O
,	O
the	O
network	O
structures	O
are	O
substantially	O
different	O
.	O

Rainbow	Method
uses	O
noisy	Method
linear	Method
layers	Method
and	O
ReLU	O
activations	O
throughout	O
the	O
network	O
,	O
whereas	O
Reactor	Method
uses	O
standard	O
linear	Method
layers	Method
and	O
concatenated	O
ReLU	O
activations	O
throughout	O
.	O

To	O
overcome	O
partial	O
observability	O
,	O
Rainbow	Method
,	O
inheriting	O
this	O
choice	O
from	O
DQN	Method
,	O
uses	O
frame	Method
stacking	Method
.	O

On	O
the	O
other	O
hand	O
,	O
Reactor	Method
,	O
inheriting	O
its	O
choice	O
from	O
A3C	Method
,	O
uses	O
LSTMs	Method
after	O
the	O
convolutional	Method
layers	Method
of	O
the	O
network	O
.	O

It	O
is	O
also	O
difficult	O
to	O
directly	O
compare	O
the	O
number	O
of	O
parameters	O
in	O
each	O
network	O
because	O
the	O
use	O
of	O
noisy	Method
linear	Method
layers	Method
doubles	O
the	O
number	O
of	O
parameters	O
,	O
although	O
half	O
of	O
these	O
are	O
used	O
to	O
control	O
noise	O
,	O
while	O
the	O
LSTM	Method
units	Method
in	O
Reactor	Method
require	O
more	O
parameters	O
than	O
a	O
corresponding	O
linear	Method
layer	Method
would	O
.	O

Second	O
,	O
both	O
algorithms	O
perform	O
-	O
step	O
updates	O
,	O
however	O
,	O
the	O
Rainbow	Method
-	Method
step	Method
update	Method
does	O
not	O
use	O
any	O
form	O
of	O
off	Method
-	Method
policy	Method
correction	Method
.	O

Because	O
of	O
this	O
,	O
Rainbow	Method
is	O
restricted	O
to	O
using	O
only	O
small	O
values	O
of	O
(	O
e.g.	O
)	O
because	O
larger	O
values	O
would	O
make	O
sequences	O
more	O
off	O
-	O
policy	O
and	O
hurt	O
performance	O
.	O

By	O
comparison	O
,	O
Reactor	Method
uses	O
our	O
proposed	O
distributional	Method
Retrace	Method
algorithm	Method
for	O
off	Task
-	Task
policy	Task
correction	Task
of	Task
-	Task
step	Task
updates	Task
.	O

This	O
allows	O
the	O
use	O
of	O
larger	O
values	O
of	O
(	O
e.g.	O
)	O
without	O
loss	O
of	O
performance	O
.	O

Third	O
,	O
while	O
both	O
agents	O
use	O
prioritized	O
replay	O
buffers	O
schaul16prioritized	O
,	O
they	O
each	O
store	O
different	O
information	O
and	O
prioritize	O
using	O
different	O
algorithms	O
.	O

Rainbow	Method
stores	O
a	O
tuple	O
containing	O
the	O
state	O
,	O
action	O
,	O
sum	O
of	O
discounted	O
rewards	O
,	O
product	O
of	O
discount	O
factors	O
,	O
and	O
next	O
-	O
state	O
steps	O
away	O
.	O

Tuples	O
are	O
prioritized	O
based	O
upon	O
the	O
last	O
observed	O
TD	O
error	O
,	O
and	O
inserted	O
into	O
replay	O
with	O
a	O
maximum	O
priority	O
.	O

Reactor	Method
stores	O
length	O
sequences	O
of	O
tuples	O
and	O
also	O
prioritizes	O
based	O
upon	O
the	O
observed	O
TD	Metric
error	Metric
.	O

However	O
,	O
when	O
inserted	O
into	O
the	O
buffer	O
the	O
priority	O
is	O
instead	O
inferred	O
based	O
upon	O
the	O
known	O
priorities	O
of	O
neighboring	O
sequences	O
.	O

This	O
priority	Method
inference	Method
was	O
made	O
efficient	O
using	O
the	O
previously	O
introduced	O
contextual	Method
priority	Method
tree	Method
,	O
and	O
anecdotally	O
we	O
have	O
seen	O
it	O
improve	O
performance	O
over	O
a	O
simple	O
maximum	Method
priority	Method
approach	Method
.	O

Finally	O
,	O
the	O
two	O
algorithms	O
have	O
different	O
approaches	O
to	O
exploration	Task
.	O

Rainbow	Method
,	O
unlike	O
DQN	Method
,	O
does	O
not	O
use	O
-	Method
greedy	Method
exploration	Method
,	O
but	O
instead	O
replaces	O
all	O
linear	O
layers	O
with	O
noisy	O
linear	O
layers	O
which	O
induce	O
randomness	O
throughout	O
the	O
network	O
.	O

This	O
method	O
,	O
called	O
Noisy	Method
Networks	Method
fortunato2017noisy	O
,	O
creates	O
an	O
adaptive	Method
exploration	Method
integrated	O
into	O
the	O
agent	Method
’s	Method
network	Method
.	O

Reactor	Method
does	O
not	O
use	O
noisy	Method
networks	Method
,	O
but	O
instead	O
uses	O
the	O
same	O
entropy	Method
cost	Method
method	Method
used	O
by	O
A3C	O
and	O
many	O
others	O
mnih2016asynchronous	O
,	O
which	O
penalizes	O
deterministic	Method
policies	Method
thus	O
encouraging	O
indifference	O
between	O
similarly	O
valued	O
actions	O
.	O

Because	O
Rainbow	O
can	O
essentially	O
learn	O
not	O
to	O
explore	O
,	O
it	O
may	O
learn	O
to	O
become	O
entirely	O
greedy	O
in	O
the	O
early	O
parts	O
of	O
the	O
episode	O
,	O
while	O
still	O
exploring	O
in	O
states	O
not	O
as	O
frequently	O
seen	O
.	O

In	O
some	O
sense	O
,	O
this	O
is	O
precisely	O
what	O
we	O
want	O
from	O
an	O
exploration	Method
technique	Method
,	O
but	O
it	O
may	O
also	O
lead	O
to	O
highly	O
deterministic	O
trajectories	O
in	O
the	O
early	O
part	O
of	O
the	O
episode	O
and	O
an	O
increase	O
in	O
overfitting	O
to	O
those	O
trajectories	O
.	O

We	O
hypothesize	O
that	O
this	O
may	O
be	O
the	O
explanation	O
for	O
the	O
significant	O
difference	O
in	O
Rainbow	O
’s	O
performance	O
between	O
evaluation	Task
under	O
no	O
-	O
op	O
and	O
random	O
human	O
starts	O
,	O
and	O
why	O
Reactor	Method
does	O
not	O
show	O
such	O
a	O
large	O
difference	O
.	O

subsection	O
:	O
Atari	Material
results	O
Table	Task
-	Task
to	Task
-	Task
text	Task
Generation	Task
by	O
Structure	Method
-	Method
aware	Method
Seq2seq	Task
Learning	Task
section	O
:	O
Abstract	O
Table	O
-	O
to	O
-	O
text	Task
generation	Task
aims	O
to	O
generate	O
a	O
description	O
for	O
a	O
factual	O
table	O
which	O
can	O
be	O
viewed	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
.	O

To	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
novel	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
which	O
consists	O
of	O
field	Method
-	Method
gating	Method
encoder	Method
and	O
description	Method
generator	Method
with	O
dual	O
attention	O
.	O

In	O
the	O
encoding	Task
phase	Task
,	O
we	O
update	O
the	O
cell	O
memory	O
of	O
the	O
LSTM	Method
unit	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
in	O
order	O
to	O
incorporate	O
field	O
information	O
into	O
table	Method
representation	Method
.	O

In	O
the	O
decoding	Task
phase	Task
,	O
dual	Method
attention	Method
mechanism	Method
which	O
contains	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
is	O
proposed	O
to	O
model	O
the	O
semantic	O
relevance	O
between	O
the	O
generated	O
description	O
and	O
the	O
table	O
.	O

We	O
conduct	O
experiments	O
on	O
the	O
WIKIBIO	Material
dataset	Material
which	O
contains	O
over	O
700k	O
biographies	O
and	O
corresponding	O
infoboxes	O
from	O
Wikipedia	O
.	O

The	O
attention	Task
visualizations	Task
and	O
case	O
studies	O
show	O
that	O
our	O
model	O
is	O
capable	O
of	O
generating	O
coherent	O
and	O
informative	O
descriptions	O
based	O
on	O
the	O
comprehensive	O
understanding	O
of	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O

Automatic	O
evaluations	O
also	O
show	O
our	O
model	O
outperforms	O
the	O
baselines	O
by	O
a	O
great	O
margin	O
.	O

Code	O
for	O
this	O
work	O
is	O
available	O
on	O
https:	O
//	O
github.com	O
/	O
tyliupku	O
/	O
wiki2bio	O
.	O

section	O
:	O
Introduction	O
Generating	Task
natural	Task
language	Task
description	Task
for	O
a	O
structured	O
table	O
is	O
an	O
important	O
task	O
for	O
text	Task
generation	Task
from	O
structured	O
data	O
.	O

Previous	O
researches	O
include	O
weather	Task
forecast	Task
based	O
on	O
a	O
set	O
of	O
weather	O
records	O
[	O
reference	O
]	O
and	O
sportscasting	O
based	O
on	O
temporally	O
ordered	O
events	O
[	O
reference	O
]	O
.	O

However	O
,	O
previous	O
work	O
models	O
the	O
structured	O
data	O
in	O
the	O
limited	O
pre	O
-	O
defined	O
schemas	O
.	O

For	O
example	O
,	O
a	O
weather	O
record	O
rainChance	O
(	O
time:06:00	O
-	O
21:00	O
,	O
mode	O
:	O
SSE	O
,	O
value:20	O
)	O
is	O
represented	O
by	O
a	O
fixed	O
-	O
length	O
onehot	O
vector	O
by	O
its	O
record	O
type	O
,	O
record	O
time	O
,	O
record	O
value	O
and	O
record	O
value	O
.	O

To	O
this	O
end	O
,	O
we	O
focus	O
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
which	O
involves	O
comprehensive	Method
representation	Method
for	O
the	O
complex	O
structure	O
of	O
a	O
table	O
rather	O
than	O
pre	O
-	O
defined	O
schemas	O
.	O

In	O
contrast	O
to	O
previous	O
work	O
experimented	O
on	O
small	O
datasets	O
which	O
contain	O
only	O
a	O
few	O
tens	O
of	O
thousands	O
of	O
records	O
such	O
as	O
WEATHERGOV	Material
[	O
reference	O
]	O
and	O
ROBOCUP	Material
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
a	O
more	O
challenging	O
task	O
to	O
generate	O
biographies	O
The	O
Wikipedia	O
infobox	O
of	O
Charles	O
Winstead	O
,	O
the	O
corresponding	O
introduction	O
on	O
his	O
wiki	O
page	O
reads	O
"	O
Charles	O
Winstead	O
[	O
reference	O
]	O
was	O
an	O
FBI	O
agent	O
in	O
the	O
1930s	O
-	O
40s	O
,	O
famous	O
for	O
being	O
one	O
of	O
the	O
agents	O
who	O
shot	O
and	O
killed	O
John	O
Dillinger	O
.	O

"	O
.	O

based	O
on	O
the	O
Wikipedia	O
infoboxes	O
.	O

As	O
shown	O
in	O
Fig	O
1	O
,	O
a	O
biographic	O
infobox	O
is	O
a	O
fixed	O
-	O
format	O
table	O
that	O
describes	O
a	O
person	O
with	O
many	O
field	O
-	O
value	O
records	O
like	O
(	O
Name	O
,	O
Charles	O
B.	O
Winstead	O
)	O
,	O
(	O
Nationality	O
,	O
American	O
)	O
,	O
(	O
Occupation	O
,	O
FBI	O
Agent	O
)	O
,	O
etc	O
.	O

We	O
utilize	O
WIKIBIO	Material
dataset	Material
proposed	O
by	O
[	O
reference	O
]	O
which	O
contains	O
700k	O
biographies	O
from	O
Wikipedia	O
,	O
with	O
400k	O
words	O
in	O
total	O
as	O
the	O
benchmark	O
dataset	O
.	O

Previous	O
work	O
has	O
made	O
significant	O
progress	O
on	O
this	O
task	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
statistical	Method
n	Method
-	Method
gram	Method
model	Method
with	O
local	O
and	O
global	O
conditioning	O
on	O
a	O
Wikipedia	O
infobox	O
.	O

However	O
the	O
field	O
content	O
of	O
a	O
record	O
is	O
likely	O
to	O
be	O
a	O
sequence	O
of	O
words	O
,	O
the	O
statistical	Method
language	Method
model	Method
is	O
not	O
good	O
at	O
capturing	O
long	O
-	O
range	O
dependencies	O
between	O
words	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
selective	O
generation	Task
method	O
based	O
on	O
an	O
encoder	Method
-	Method
alignerdecoder	Method
framework	Method
.	O

The	O
model	O
utilizes	O
a	O
sparse	Method
one	Method
-	Method
hot	Method
vector	Method
to	O
represent	O
a	O
weather	O
record	O
.	O

However	O
it	O
's	O
inefficient	O
to	O
represent	O
the	O
complex	O
structure	O
of	O
a	O
table	O
by	O
one	O
-	O
hot	O
vectors	O
.	O

We	O
propose	O
a	O
structure	Method
-	Method
aware	Method
sequence	Method
to	Method
sequence	Method
(	O
seq2seq	Method
)	O
generation	Task
framework	O
to	O
model	O
both	O
content	O
and	O
structure	O
of	O
the	O
table	O
by	O
local	O
and	O
global	O
addressing	O
.	O

When	O
a	O
human	O
writes	O
a	O
biography	O
for	O
a	O
person	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
,	O
he	O
will	O
firstly	O
determine	O
which	O
records	O
in	O
the	O
table	O
should	O
be	O
included	O
in	O
the	O
introduction	O
and	O
how	O
to	O
arrange	O
the	O
order	O
of	O
these	O
records	O
before	O
wording	O
.	O

After	O
that	O
,	O
the	O
writer	O
will	O
further	O
consider	O
which	O
words	O
or	O
phrases	O
in	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
to	O
paraphrase	O
.	O

We	O
summarize	O
the	O
two	O
phases	O
of	O
generation	Task
as	O
two	O
scopes	O
of	O
addressing	Task
:	O
local	Task
and	Task
global	Task
addressing	Task
.	O

Local	Task
addressing	Task
determines	O
which	O
particular	O
word	O
in	O
the	O
table	O
should	O
be	O
focused	O
on	O
while	O
generating	O
a	O
piece	O
of	O
description	O
at	O
certain	O
time	O
step	O
.	O

However	O
,	O
the	O
word	Method
level	Method
addressing	Method
can	O
not	O
fully	O
address	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
problem	O
as	O
the	O
factual	O
tables	O
usually	O
have	O
complex	O
structures	O
which	O
might	O
confuse	O
the	O
generator	O
.	O

Global	Task
addressing	Task
is	O
proposed	O
to	O
determine	O
which	O
records	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
while	O
generating	O
corresponding	O
description	O
.	O

Global	Task
addressing	Task
is	O
necessary	O
as	O
the	O
description	O
of	O
a	O
table	O
may	O
not	O
cover	O
all	O
the	O
records	O
.	O

For	O
example	O
,	O
the	O
'cause	O
of	O
death	O
'	O
field	O
in	O
Fig	O
1	O
is	O
not	O
mentioned	O
in	O
the	O
description	O
.	O

Furthermore	O
,	O
the	O
order	O
of	O
records	O
in	O
the	O
tables	O
may	O
not	O
always	O
be	O
homogeneous	O
.	O

For	O
example	O
,	O
we	O
can	O
introduce	O
a	O
person	O
as	O
an	O
order	O
of	O
his	O
(	O
BirthDeath	O
-	O
Nationality	O
-	O
Occupation	O
)	O
according	O
to	O
his	O
Wikipedia	O
infobox	O
.	O

However	O
the	O
other	O
infoboxes	O
may	O
be	O
arranged	O
as	O
(	O
Occupation	O
-	O
Nationality	O
-	O
Birth	O
-	O
Death	O
)	O
.	O

Local	Task
addressing	Task
is	O
realized	O
by	O
content	O
encoding	O
of	O
the	O
LSTM	Method
encoder	O
and	O
word	Method
level	Method
attention	Method
while	O
global	Method
addressing	Method
is	O
realized	O
by	O
field	Method
encoding	Method
of	O
the	O
field	O
-	O
gating	O
LSTM	Method
variation	O
and	O
field	Method
level	Method
attention	Method
in	O
our	O
model	O
.	O

The	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
we	O
proposed	O
exploits	O
encoder	Method
-	Method
decoder	Method
framework	Method
using	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
[	O
reference	O
]	O
units	O
with	O
local	O
and	O
global	O
addressing	O
on	O
the	O
structured	O
table	O
.	O

In	O
the	O
encoding	O
phase	O
,	O
our	O
model	O
first	O
encodes	O
the	O
sets	O
of	O
fieldvalue	O
records	O
in	O
the	O
table	O
by	O
integrating	O
field	O
information	O
and	O
content	Method
representation	Method
.	O

To	O
make	O
better	O
use	O
of	O
field	O
information	O
,	O
we	O
add	O
a	O
field	O
gate	O
to	O
the	O
cell	O
state	O
of	O
the	O
encoder	O
LSTM	Method
unit	O
to	O
incorporate	O
the	O
field	Method
embedding	Method
into	O
the	O
structural	O
representation	O
of	O
the	O
table	O
.	O

The	O
model	O
next	O
employs	O
a	O
LSTM	Method
decoder	O
to	O
generate	O
natural	Task
language	Task
description	Task
by	O
the	O
structural	Method
representation	Method
of	O
the	O
table	O
.	O

In	O
the	O
decoding	Task
phase	Task
,	O
we	O
also	O
propose	O
a	O
novel	O
dual	Method
attention	Method
mechanism	Method
which	O
consists	O
of	O
two	O
parts	O
:	O
word	Method
-	Method
level	Method
attention	Method
for	O
local	Task
addressing	Task
and	O
field	Task
-	Task
level	Task
attention	Task
for	O
global	Task
addressing	Task
.	O

Our	O
contributions	O
are	O
three	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
propose	O
an	O
endto	Method
-	Method
end	Method
structure	Method
-	Method
aware	Method
encoder	Method
-	Method
decoder	Method
architecture	Method
to	O
encode	O
field	O
information	O
into	O
the	O
representation	O
of	O
a	O
structured	O
table	O
.	O

(	O
2	O
)	O
Field	Method
-	Method
gating	Method
encoder	Method
and	O
dual	Method
attention	Method
mechanism	Method
are	O
proposed	O
to	O
operate	O
local	O
and	O
global	O
addressing	O
between	O
the	O
content	O
and	O
the	O
field	O
information	O
of	O
a	O
structured	O
table	O
.	O

(	O
3	O
)	O
Experiments	O
on	O
WIKIBIO	Material
dataset	Material
show	O
that	O
our	O
model	O
achieves	O
substantial	O
improvement	O
over	O
baselines	O
.	O

section	O
:	O
Related	O
Work	O
Most	O
generation	Task
systems	O
can	O
be	O
divided	O
into	O
two	O
independent	O
modules	O
:	O
(	O
1	O
)	O
content	O
selection	Task
involves	O
choosing	O
a	O
subset	O
of	O
relevant	O
records	O
in	O
a	O
table	O
to	O
talk	O
about	O
.	O

(	O
2	Task
)	Task
surface	Task
realization	Task
is	O
concerned	O
with	O
generating	Task
natural	Task
language	Task
descriptions	Task
for	O
this	O
subset	O
.	O

Many	O
approaches	O
have	O
been	O
proposed	O
to	O
learn	O
the	O
individual	O
modules	O
.	O

For	O
content	Task
selection	Task
module	Task
,	O
one	O
approach	O
builds	O
a	O
content	Method
selection	Method
model	Method
by	O
aligning	O
records	O
and	O
sentences	O
[	O
reference	O
][	O
reference	O
]	O
.	O

A	O
hierarchical	Method
semi	Method
-	Method
Markov	Method
method	Method
is	O
proposed	O
by	O
[	O
reference	O
]	O
which	O
first	O
associates	O
the	O
text	O
sequences	O
to	O
corresponding	O
records	O
and	O
then	O
generates	O
corresponding	O
descriptions	O
from	O
these	O
records	O
.	O

Surface	Task
realization	Task
is	O
often	O
treated	O
as	O
a	O
concept	Task
-	Task
to	Task
-	Task
text	Task
generation	Task
task	Task
from	O
a	O
given	O
representation	O
.	O

[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
utilize	O
various	O
linguistic	O
features	O
to	O
train	O
sentence	Method
planners	Method
for	O
sentence	O
generation	Task
.	O

Context	Method
-	Method
free	Method
grammars	Method
are	O
also	O
used	O
to	O
generate	O
natural	O
language	O
sentences	O
from	O
formal	Method
meaning	Method
representations	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

Other	O
effective	O
approaches	O
include	O
hybrid	O
alignment	O
tree	O
[	O
reference	O
]	O
,	O
tree	Method
conditional	Method
random	Method
fields	Method
[	O
reference	O
]	O
,	O
tree	Method
adjoining	Method
grammar	Method
[	O
reference	O
]	O
and	O
template	Method
extraction	Method
in	O
a	O
log	Method
-	Method
linear	Method
framework	Method
[	O
reference	O
]	O
.	O

Recent	O
work	O
combines	O
content	Task
selection	Task
and	O
surface	Task
realization	Task
in	O
a	O
unified	O
framework	O
(	O
Ratnaparkhi	O
2002	O
;	O
[	O
reference	O
][	O
reference	O
]	O
Our	O
model	O
borrowed	O
the	O
idea	O
of	O
representing	O
a	O
structured	O
table	O
by	O
its	O
field	O
and	O
content	O
information	O
from	O
(	O
Lebret	O
,	O
Grangier	O
,	O
and	O
Auli	O
2016	O
)	O
.	O

However	O
,	O
their	O
n	Method
-	Method
gram	Method
model	Method
is	O
inefficient	O
to	O
model	O
long	O
-	O
range	O
dependencies	O
while	O
generating	O
descriptions	O
.	O

[	O
reference	O
]	O
also	O
proposed	O
a	O
seq2seq	Method
model	O
with	O
an	O
aligner	O
between	O
weather	O
records	O
and	O
weather	O
broadcast	O
.	O

The	O
model	O
used	O
one	Method
-	Method
hot	Method
encoding	Method
to	O
represent	O
the	O
weather	O
records	O
as	O
they	O
are	O
relatively	O
simple	O
and	O
highly	O
structured	O
.	O

However	O
,	O
the	O
model	O
is	O
not	O
capable	O
to	O
represent	O
the	O
tables	O
with	O
complex	O
structure	O
like	O
Wikipedia	O
infoboxes	O
.	O

section	O
:	O
Task	O
Definition	O
We	O
model	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
in	O
an	O
end	O
-	O
to	O
-	O
end	O
structure	O
-	O
aware	O
seq2seq	Method
framework	O
.	O

The	O
given	O
table	O
T	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
n	O
field	O
-	O
value	O
records	O
{	O
R	O
1	O
,	O
R	O
2	O
,	O
·	O
·	O
·	O
,	O
R	O
n	O
}	O
.	O

Each	O
record	O
R	O
i	O
consists	O
of	O
a	O
sequence	O
of	O
words	O
The	O
output	O
of	O
the	O
model	O
is	O
the	O
generated	O
description	O
S	O
for	O
table	O
T	O
which	O
contains	O
p	O
tokens	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
·	O
·	O
·	O
,	O
w	O
p	O
}	O
with	O
w	O
t	O
being	O
the	O
word	O
at	O
time	O
t.	O
We	O
formulate	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
as	O
the	O
inference	Task
over	O
a	O
probabilistic	Method
model	Method
.	O

The	O
goal	O
of	O
the	O
inference	Task
is	O
to	O
generate	O
a	O
sequence	O
w	O
*	O
1:p	O
which	O
maximizes	O
P	O
(	O
w	O
1:p	O
|R	O
1:n	O
)	O
.	O

bedding	O
for	O
a	O
segment	O
of	O
words	O
in	O
the	O
field	O
content	O
.	O

The	O
field	Method
embedding	Method
is	O
a	O
key	O
point	O
to	O
label	O
each	O
word	O
in	O
the	O
field	O
content	O
by	O
its	O
corresponding	O
field	O
name	O
and	O
occurrence	O
in	O
the	O
table	O
.	O

[	O
reference	O
]	O
represented	O
the	O
field	O
embeddding	O
Z	O
w	O
=	O
{	O
f	O
w	O
;	O
p	O
w	O
}	O
for	O
a	O
word	O
w	O
in	O
the	O
table	O
with	O
corresponding	O
field	O
name	O
f	O
w	O
and	O
position	O
information	O
p	O
w	O
.	O

The	O
position	O
information	O
can	O
be	O
further	O
represented	O
as	O
a	O
tuple	O
(	O
p	O
which	O
includes	O
the	O
positions	O
of	O
the	O
token	O
w	O
counted	O
from	O
the	O
begining	O
and	O
the	O
end	O
of	O
the	O
field	O
respectively	O
.	O

So	O
the	O
field	Method
embedding	Method
of	Method
token	Method
w	Method
is	O
extended	O
to	O
a	O
triple	O
:	O
As	O
shown	O
in	O
Fig	O
2	O
,	O
the	O
infobox	O
of	O
George	O
Mikell	O
contains	O
several	O
field	O
-	O
value	O
records	O
,	O
the	O
field	O
content	O
for	O
the	O
record	O
(	O
birthname	O
,	O
Jurgis	O
Mikelatitis	O
)	O
is	O
'	O
Jurgis	O
Mikelatitis	O
'	O
.	O

The	O
word	O
'	O
Jurgis	O
'	O
is	O
the	O
first	O
token	O
counted	O
from	O
the	O
beginning	O
of	O
the	O
field	O
'	O
birthname	O
'	O
and	O
also	O
the	O
second	O
token	O
counted	O
from	O
the	O
end	O
.	O

So	O
the	O
field	Method
embedding	Method
for	O
the	O
word	O
'	O
Jurgis	O
'	O
is	O
described	O
as	O
{	O
birthname	O
;	O
1	O
;	O
2}.	O
Each	O
token	O
in	O
the	O
table	O
has	O
an	O
unique	O
field	O
embedding	O
even	O
if	O
there	O
exists	O
two	O
same	O
words	O
in	O
the	O
same	O
field	O
due	O
to	O
the	O
unique	O
(	O
field	O
,	O
position	O
)	O
pair	O
.	O

section	O
:	O
Field	Method
-	Method
gating	Method
Table	Method
Encoder	Method
The	O
table	Method
encoder	Method
aims	O
to	O
encode	O
each	O
word	O
d	O
j	O
in	O
the	O
table	O
together	O
with	O
its	O
field	O
embedding	O
Z	O
dj	O
into	O
the	O
hidden	O
state	O
h	O
j	O
using	O
LSTM	Method
encoder	O
.	O

We	O
present	O
a	O
novel	O
field	O
-	O
gating	O
LSTM	Method
unit	O
to	O
incorporate	O
field	O
information	O
into	O
table	Task
encoding	Task
.	O

LSTM	Method
is	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
architecture	O
which	O
uses	O
a	O
vector	O
of	O
cell	O
state	O
c	O
t	O
and	O
a	O
set	O
of	O
element	O
-	O
wise	O
multiplication	O
gates	O
to	O
control	O
how	O
information	O
is	O
stored	O
,	O
forgotten	O
and	O
exploited	O
inside	O
the	O
network	O
.	O

Following	O
the	O
design	O
for	O
an	O
LSTM	Method
cell	O
in	O
[	O
reference	O
]	O
,	O
the	O
architecture	O
used	O
in	O
the	O
table	Method
encoder	Method
is	O
defined	O
by	O
following	O
equations	O
:	O
where	O
i	O
t	O
,	O
f	O
t	O
,	O
o	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
are	O
input	O
,	O
forget	O
and	O
output	O
gates	O
respectively	O
,	O
andĉ	O
t	O
and	O
c	O
t	O
are	O
proposed	O
cell	O
value	O
and	O
true	O
cell	O
value	O
in	O
time	O
t.	O
n	O
is	O
the	O
hidden	O
size	O
.	O

To	O
make	O
better	O
understanding	O
of	O
the	O
structure	O
of	O
a	O
table	O
,	O
the	O
field	O
information	O
should	O
also	O
be	O
encoded	O
into	O
the	O
encoder	Method
.	O

One	O
simple	O
way	O
is	O
to	O
take	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
and	O
corresponding	O
field	Method
embedding	Method
as	O
the	O
input	O
for	O
the	O
vanilla	O
LSTM	Method
unit	O
.	O

Actually	O
,	O
the	O
method	O
is	O
indeed	O
proved	O
to	O
be	O
useful	O
in	O
our	O
experiments	O
and	O
serves	O
as	O
a	O
baseline	O
for	O
comparison	O
.	O

However	O
,	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
and	O
field	Method
embedding	Method
only	O
treats	O
the	O
field	O
information	O
as	O
an	O
additional	O
label	O
of	O
certain	O
token	O
which	O
loses	O
the	O
structural	O
information	O
of	O
the	O
table	O
.	O

To	O
better	O
encode	O
the	O
structural	O
information	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
field	Method
-	Method
gating	Method
variation	Method
on	O
the	O
vanilla	O
LSTM	Method
unit	O
to	O
update	O
the	O
cell	O
memory	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
.	O

The	O
field	O
-	O
gating	O
cell	O
state	O
is	O
described	O
as	O
follows	O
:	O
where	O
z	O
t	O
is	O
the	O
field	O
embedding	O
described	O
before	O
,	O
l	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
is	O
the	O
field	O
gate	O
to	O
determine	O
how	O
much	O
field	O
information	O
should	O
be	O
kept	O
in	O
the	O
cell	O
memory	O
,	O
ẑ	O
t	O
is	O
the	O
proposed	O
field	O
value	O
corresponding	O
to	O
field	O
gate	O
.	O

The	O
cell	O
state	O
c	O
t	O
is	O
updated	O
from	O
the	O
original	O
c	O
t	O
by	O
incorporating	O
field	O
information	O
of	O
the	O
table	O
.	O

section	O
:	O
Description	Method
Decoder	Method
with	O
Dual	Method
Attention	Method
To	O
conduct	O
local	Task
and	Task
global	Task
addressing	Task
towards	O
the	O
structured	O
table	O
,	O
we	O
use	O
LSTM	Method
architecture	O
with	O
dual	Method
attention	Method
mechanism	Method
as	O
our	O
description	Method
generator	Method
.	O

As	O
defined	O
in	O
the	O
equation	O
1	O
,	O
the	O
generated	O
token	O
w	O
t	O
at	O
time	O
t	O
in	O
the	O
decoder	O
is	O
predicated	O
based	O
on	O
all	O
the	O
previously	O
generated	O
tokens	O
w	O
<	O
t	O
before	O
w	O
t	O
,	O
the	O
hidden	O
states	O
H	O
=	O
{	O
h	O
t	O
}	O
L	O
t=1	O
of	O
the	O
table	Method
encoder	Method
and	O
the	O
field	Method
embeddings	Method
Z	O
=	O
{	O
z	O
t	O
}	O
L	O
t=1	O
.	O

To	O
be	O
more	O
specific	O
:	O
where	O
s	O
t	O
is	O
the	O
t	O
-	O
th	O
hidden	O
state	O
of	O
the	O
decoder	O
calculated	O
by	O
the	O
LSTM	Method
unit	O
.	O

The	O
computational	O
details	O
can	O
be	O
referred	O
in	O
Equation	O
3	O
,	O
4	O
and	O
5	O
.	O

a	O
t	O
is	O
the	O
attention	O
vector	O
which	O
is	O
widely	O
used	O
in	O
many	O
applications	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Vanilla	Method
attention	Method
mechanism	Method
is	O
proposed	O
to	O
encode	O
the	O
semantic	O
relevance	O
between	O
the	O
encoder	O
states	O
{	O
h	O
t	O
}	O
L	O
t=1	O
and	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
time	O
t.	O
The	O
attention	O
vector	O
is	O
usually	O
represented	O
by	O
the	O
weighted	O
sum	O
of	O
encoder	O
hidden	O
states	O
.	O

However	O
,	O
the	O
word	Method
level	Method
attention	Method
described	O
above	O
can	O
only	O
capture	O
the	O
semantic	O
relevance	O
between	O
generated	O
tokens	O
and	O
the	O
content	O
information	O
in	O
the	O
table	O
,	O
ignoring	O
the	O
structure	O
information	O
of	O
the	O
table	O
.	O

To	O
fully	O
utilize	O
the	O
structure	O
information	O
,	O
we	O
propose	O
a	O
higher	O
level	O
attention	O
over	O
generated	O
tokens	O
and	O
the	O
field	O
embedding	O
of	O
the	O
table	O
.	O

Field	O
level	O
attention	O
can	O
locate	O
the	O
particular	O
field	O
-	O
value	O
record	O
which	O
should	O
be	O
focused	O
on	O
while	O
generating	O
next	O
token	O
in	O
the	O
description	O
by	O
modeling	O
the	O
relevance	O
between	O
all	O
field	O
embeddings	O
{	O
z	O
t	O
}	O
L	O
t=1	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
t	O
-	O
th	O
time	O
.	O

Field	O
level	O
attention	O
weight	O
β	Method
ti	Method
is	O
presented	O
as	O
Equation	O
13	O
.	O

We	O
use	O
the	O
same	O
relevant	O
score	O
function	O
g	O
(	O
s	O
t	O
,	O
z	O
i	O
)	O
as	O
equation	O
12	O
.	O

Dual	O
attention	O
weight	O
γ	O
t	O
is	O
the	O
element	O
-	O
wise	O
production	O
between	O
field	O
level	O
attention	O
weight	O
β	O
t	O
and	O
word	Method
level	Method
attention	Method
weight	O
α	O
t	O
.	O

The	O
dual	O
attention	O
vector	O
a	O
t	O
is	O
updated	O
as	O
the	O
weighted	O
sum	O
of	O
encoder	O
states	O
{	O
h	O
t	O
}	O
t=1	O
by	O
γ	O
t	O
(	O
Equation	O
15	O
)	O
:	O
Furthermore	O
,	O
we	O
utilize	O
a	O
post	Method
-	Method
process	Method
operation	Method
for	O
the	O
generated	O
unknown	O
(	O
UNK	O
)	O
tokens	O
to	O
alleviate	O
the	O
out	Task
-	Task
ofvocabulary	Task
(	O
OOV	Task
)	O
problem	O
.	O

We	O
replace	O
a	O
specific	O
generated	O
UNK	O
token	O
with	O
the	O
most	O
relevant	O
token	O
in	O
the	O
corresponding	O
table	O
according	O
to	O
the	O
related	O
dual	O
attention	O
matrix	O
.	O

section	O
:	O
Local	Task
and	Task
Global	Task
Addressing	Task
Local	O
and	O
global	O
addressing	O
determine	O
which	O
part	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
in	O
different	O
steps	O
of	O
description	O
generation	Task
.	O

The	O
two	O
scopes	O
of	O
addressings	O
play	O
a	O
very	O
important	O
role	O
in	O
understanding	O
and	O
representing	O
the	O
innerstructure	O
of	O
a	O
table	O
.	O

Next	O
we	O
will	O
introduce	O
how	O
our	O
model	O
conducts	O
local	Task
and	Task
global	Task
addressing	Task
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
with	O
the	O
help	O
of	O
Fig	O
3	O
.	O

Local	Task
addressing	Task
:	O
A	O
table	O
can	O
be	O
treated	O
as	O
a	O
set	O
of	O
fieldvalue	O
records	O
.	O

Local	Task
addressing	Task
tends	O
to	O
encode	O
the	O
table	O
content	O
inside	O
each	O
record	O
.	O

The	O
value	O
in	O
each	O
field	O
-	O
value	O
record	O
is	O
a	O
sequence	O
of	O
words	O
which	O
contains	O
2.7	O
tokens	O
on	O
average	O
.	O

Some	O
records	O
in	O
the	O
Wikipedia	O
infoboxes	O
even	O
contain	O
several	O
phrases	O
or	O
sentences	O
.	O

Previous	O
models	O
which	O
used	O
one	Method
-	Method
hot	Method
encoding	Method
or	O
statistical	Method
language	Method
model	Method
to	O
encode	O
field	O
content	O
are	O
inefficient	O
to	O
capture	O
the	O
semantic	O
relevance	O
between	O
words	O
inside	O
a	O
field	O
.	O

The	O
seq2seq	Method
structure	O
itself	O
has	O
a	O
strong	O
ability	O
to	O
model	O
the	O
context	O
of	O
a	O
piece	O
of	O
words	O
.	O

For	O
one	O
thing	O
,	O
the	O
LSTM	Method
encoder	O
can	O
capture	O
longrange	O
dependencies	O
between	O
words	O
in	O
the	O
table	O
.	O

For	O
another	O
,	O
the	O
word	Method
level	Method
attention	Method
of	O
the	O
proposed	O
dual	Method
attention	Method
mechanism	Method
can	O
also	O
build	O
a	O
connection	O
between	O
the	O
words	O
in	O
the	O
description	O
and	O
the	O
tokens	O
in	O
the	O
table	O
.	O

The	O
generated	O
word	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
refers	O
to	O
the	O
word	O
'	O
actor	O
'	O
in	O
the	O
'	O
Occupation	O
'	O
field	O
.	O

Global	Task
addressing	Task
:	O
The	O
goal	O
of	O
local	Task
addressing	Task
is	O
to	O
represent	O
inner	O
-	O
record	O
information	O
while	O
global	Task
addressing	Task
aims	O
to	O
model	O
inter	O
-	O
record	O
relevance	O
within	O
the	O
table	O
.	O

For	O
example	O
,	O
it	O
's	O
noteworthy	O
that	O
the	O
generated	O
token	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
is	O
mapped	O
to	O
the	O
'	O
occupation	O
'	O
field	O
in	O
Table	O
2	O
.	O

Field	Method
-	Method
gating	Method
[	O
reference	O
]	O
.	O

We	O
should	O
make	O
it	O
clear	O
which	O
records	O
the	O
token	O
to	O
be	O
generated	O
is	O
focused	O
on	O
by	O
global	O
addressing	O
between	O
the	O
field	O
information	O
of	O
a	O
table	O
and	O
its	O
description	O
.	O

The	O
field	O
level	O
attention	O
of	O
dual	Method
attention	Method
mechanism	Method
is	O
introduced	O
to	O
determine	O
which	O
field	O
the	O
generator	O
focused	O
on	O
in	O
certain	O
time	O
step	O
.	O

Experiments	O
show	O
that	O
our	O
dual	Method
attention	Method
mechanism	Method
is	O
of	O
great	O
help	O
to	O
generate	O
description	O
from	O
certain	O
table	O
and	O
insensible	O
to	O
different	O
orders	O
of	O
table	O
records	O
.	O

section	O
:	O
Experiments	O
We	O
first	O
introduce	O
the	O
dataset	O
,	O
evaluation	Metric
metrics	Metric
and	O
experimental	O
setups	O
in	O
our	O
experiments	O
.	O

Then	O
we	O
compare	O
our	O
model	O
with	O
several	O
baselines	O
.	O

After	O
that	O
,	O
we	O
assess	O
the	O
performance	O
of	O
our	O
model	O
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O

Furthermore	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
the	O
disordered	O
tables	O
to	O
show	O
the	O
efficiency	O
of	O
global	Method
addressing	Method
mechanism	Method
.	O

section	O
:	O
Dataset	O
and	O
Evaluation	Metric
Metrics	Metric
We	O
use	O
WIKBIO	Material
dataset	Material
proposed	O
by	O
[	O
reference	O
]	O
as	O
the	O
benchmark	O
dataset	O
.	O

WIKBIO	Material
contains	O
728	O
,	O
321	O
articles	O
from	O
English	O
Wikipedia	O
[	O
reference	O
]	O
.	O

The	O
dataset	O
uses	O
the	O
first	O
sentence	O
of	O
each	O
article	O
as	O
the	O
description	O
of	O
the	O
corresponding	O
infobox	O
.	O

Table	O
1	O
summarizes	O
the	O
dataset	O
statistics	O
:	O
on	O
average	O
,	O
the	O
tokens	O
in	O
the	O
table	O
(	O
53.1	O
)	O
are	O
twice	O
as	O
long	O
as	O
those	O
in	O
the	O
first	O
sentence	O
(	O
26.1	O
)	O
.	O

9.5	O
tokens	O
in	O
the	O
description	O
text	O
also	O
occur	O
in	O
the	O
table	O
.	O

The	O
corpus	O
has	O
been	O
divided	O
in	O
to	O
training	O
(	O
80	O
%	O
)	O
,	O
testing	O
(	O
10	O
%	O
)	O
and	O
validation	Metric
(	O
10	O
%	O
)	O
sets	O
.	O

We	O
assess	O
the	O
generation	Task
quality	O
automatically	O
with	O
BLEU	Metric
-	Metric
4	Metric
and	O
ROUGE	Metric
-	Metric
4	Metric
(	O
F	Metric
measure	Metric
)	O
1	O
.	O

section	O
:	O
Baselines	O
We	O
compare	O
the	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
with	O
several	O
statistical	Method
language	Method
models	Method
and	O
the	O
vanilla	Method
encoder	Method
-	Method
decoder	Method
model	Method
.	O

The	O
baselines	O
are	O
listed	O
as	O
follows	O
:	O
•	O
KN	Method
:	O
The	O
Kneser	Method
-	Method
Ney	Method
(	O
KN	Method
)	O
model	O
is	O
a	O
widely	O
used	O
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
KenLM	Method
toolkit	Method
to	O
train	O
5	Method
-	Method
gram	Method
models	Method
without	O
pruning	Method
.	O

•	O
Template	O
KN	Method
:	O
Template	O
KN	Method
is	O
a	O
KN	Method
model	O
over	O
templates	O
which	O
also	O
serves	O
as	O
a	O
baseline	O
in	O
[	O
reference	O
]	O
.	O

The	O
model	O
replaces	O
the	O
words	O
occurring	O
in	O
both	O
the	O
table	O
and	O
the	O
training	O
sentences	O
with	O
a	O
special	O
token	O
reflecting	O
its	O
field	O
.	O

The	O
introduction	O
section	O
of	O
the	O
table	O
in	O
Fig	O
2	O
looks	O
as	O
follows	O
under	O
this	O
scheme	O
:	O
"	O
name	O
1	O
name	O
2	O
(	O
born	O
birthname	O
1	O
...	O
birthdate	O
3	O
)	O
is	O
a	O
LithuanianAustralian	O
occupation	O
1	O
and	O
occupation	O
3	O
best	O
known	O
for	O
his	O
performances	O
in	O
known	O
for	O
1	O
...	O
known	O
for	O
4	O
(	O
1961	O
)	O
and	O
known	O
for	O
5	O
...	O
known	O
for	O
7	O
(	O
1963	O
)	O
"	O
.	O

During	O
inference	Task
,	O
the	O
decoder	Method
is	O
constrained	O
to	O
emit	O
words	O
from	O
the	O
regular	O
vocabulary	O
or	O
special	O
tokens	O
occurring	O
in	O
the	O
input	O
table	O
.	O

•	O
NLM	Method
:	O
A	O
naive	Method
statistical	Method
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
for	O
comparison	O
.	O

The	O
model	O
uses	O
only	O
the	O
field	O
content	O
as	O
input	O
without	O
field	O
and	O
position	O
information	O
.	O

•	O
Table	O
NLM	Method
:	O
The	O
most	O
competitive	O
statistical	Method
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
which	O
includes	O
local	O
and	O
global	O
conditioning	O
over	O
the	O
table	O
by	O
integrating	O
related	O
field	O
and	O
position	Method
embedding	Method
into	O
the	O
table	Method
representation	Method
.	O

•	O
Vanilla	Method
Seq2seq	Method
:	O
The	O
vanilla	O
seq2seq	Method
neural	O
architecture	O
is	O
also	O
provided	O
as	O
a	O
strong	O
baseline	O
which	O
uses	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
,	O
field	Method
embedding	Method
and	O
position	Method
embedding	Method
as	O
the	O
model	O
input	O
.	O

The	O
model	O
can	O
operate	O
local	Task
addressing	Task
over	O
the	O
table	O
by	O
the	O
natural	O
advantages	O
of	O
LSTM	Method
units	O
and	O
word	Method
level	Method
attention	Method
mechanism	O
.	O

section	O
:	O
Experiment	O
Setup	O
In	O
the	O
table	Task
encoding	Task
phase	Task
,	O
we	O
use	O
a	O
sequence	O
of	O
word	O
embeddings	O
and	O
their	O
corresponding	O
field	Method
and	Method
position	Method
embedding	Method
as	O
input	O
.	O

We	O
select	O
the	O
most	O
frequent	O
20	O
,	O
000	O
words	O
in	O
the	O
training	O
set	O
as	O
the	O
word	O
vocabulary	O
.	O

For	O
field	Task
embedding	Task
,	O
we	O
select	O
1480	O
fields	O
occurring	O
more	O
than	O
100	O
times	O
from	O
the	O
training	O
set	O
as	O
field	O
vocabulary	O
.	O

Additionally	O
,	O
we	O
filter	O
all	O
empty	O
fields	O
whose	O
values	O
are	O
none	O
while	O
feeding	O
field	O
information	O
to	O
the	O
network	O
.	O

We	O
also	O
limit	O
the	O
largest	O
position	O
number	O
as	O
30	O
.	O

Any	O
position	O
number	O
over	O
30	O
will	O
be	O
counted	O
as	O
30	O
.	O

While	O
generating	O
description	O
for	O
the	O
table	O
,	O
a	O
special	O
start	O
token	O
sos	O
is	O
feed	O
into	O
the	O
generator	O
in	O
the	O
beginning	O
of	O
the	O
Note	O
there	O
are	O
two	O
adjacent	O
'	O
belgium	O
's	O
in	O
'	O
birthplace	O
-	O
3	O
'	O
and	O
'	O
nationality	O
-	O
1	O
'	O
field	O
,	O
respectively	O
.	O

The	O
word	Method
level	Method
attention	Method
focuses	O
improperly	O
on	O
the	O
first	O
'	O
belgium	O
'	O
while	O
generating	O
'	O
a	O
belgian	O
film	O
director	O
'	O
.	O

In	O
contrast	O
,	O
the	O
field	O
level	O
attention	O
and	O
dual	O
attention	O
can	O
locate	O
the	O
second	O
'	O
belgium	O
'	O
properly	O
by	O
word	Method
-	Method
field	Method
modeling	Method
(	O
marked	O
in	O
the	O
black	O
boxes	O
)	O
.	O

Table	O
3	O
:	O
BLEU	Metric
-	Metric
4	Metric
and	O
ROUGE	Metric
-	Metric
4	Metric
for	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
(	O
last	O
three	O
rows	O
)	O
,	O
statistical	Method
language	Method
model	Method
(	O
first	O
four	O
rows	O
)	O
and	O
vanilla	O
seq2seq	Method
model	O
with	O
field	O
and	O
position	O
input	O
(	O
three	O
rows	O
in	O
the	O
middle	O
)	O
.	O

section	O
:	O
Model	O
decoding	Task
phase	Task
.	O

Then	O
we	O
use	O
the	O
last	O
generated	O
token	O
as	O
the	O
input	O
at	O
the	O
next	O
time	O
step	O
.	O

A	O
special	O
end	O
token	O
eos	O
is	O
used	O
to	O
mark	O
the	O
end	O
of	O
decoding	Task
.	O

We	O
also	O
restrict	O
the	O
generated	O
text	O
by	O
a	O
pre	O
-	O
defined	O
max	O
length	O
to	O
avoid	O
redundant	O
or	O
irrelevant	O
generation	Task
.	O

We	O
also	O
try	O
beam	Method
search	Method
with	O
beam	O
size	O
2	O
-	O
10	O
to	O
enhance	O
the	O
performance	O
.	O

We	O
use	O
grid	Method
search	Method
to	O
determine	O
the	O
parameters	O
of	O
our	O
model	O
.	O

The	O
detail	O
of	O
model	O
parameters	O
is	O
listed	O
in	O
Table	O
2	O
.	O

section	O
:	O
Generation	Task
Assessment	Task
The	O
assessment	O
for	O
description	O
generation	Task
is	O
listed	O
in	O
Table	O
3	O
.	O

We	O
have	O
following	O
observations	O
:	O
(	O
1	O
)	O
Neural	Method
network	Method
models	Method
perform	O
much	O
better	O
than	O
statistical	Method
language	Method
models	Method
.	O

Even	O
vanilla	O
seq2seq	Method
architecture	O
with	O
word	Method
level	Method
attention	Method
outperform	O
the	O
most	O
competitive	O
statistical	Method
model	Method
by	O
a	O
great	O
margin	O
.	O

(	O
2	O
)	O
The	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
can	O
further	O
improve	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
compared	O
with	O
the	O
competitive	O
vanilla	O
seq2seq	Method
.	O

Dual	Method
attention	Method
mechanism	Method
is	O
able	O
to	O
boost	O
the	O
model	O
performance	O
by	O
over	O
1	O
BLEU	Metric
compared	O
to	O
vanilla	Method
attention	Method
mechanism	Method
.	O

section	O
:	O
Research	O
on	O
Disordered	O
Tables	O
We	O
view	O
a	O
structured	O
table	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
and	O
then	O
feed	O
the	O
records	O
into	O
the	O
generator	O
sequentially	O
as	O
the	O
order	O
they	O
are	O
presented	O
in	O
the	O
table	O
.	O

The	O
order	O
of	O
records	O
can	O
guide	O
the	O
description	Method
generator	Method
to	O
produce	O
an	O
introduction	O
in	O
the	O
pre	O
-	O
defined	O
schemas	O
[	O
reference	O
]	O
.	O

However	O
,	O
not	O
all	O
the	O
tables	O
are	O
arranged	O
in	O
the	O
proper	O
order	O
.	O

So	O
global	O
addressing	O
between	O
the	O
generated	O
descriptions	O
and	O
the	O
records	O
of	O
the	O
table	O
is	O
necessary	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O

Furthermore	O
,	O
the	O
schemas	O
of	O
various	O
types	O
of	O
tables	O
differ	O
greatly	O
from	O
each	O
other	O
.	O

A	O
biography	O
about	O
a	O
politician	O
may	O
emphasize	O
his	O
or	O
her	O
social	O
activities	O
and	O
working	O
experience	O
while	O
a	O
biography	O
of	O
a	O
soccer	O
player	O
is	O
likely	O
to	O
highlight	O
which	O
team	O
he	O
or	O
she	O
used	O
to	O
serve	O
in	O
or	O
the	O
performance	O
in	O
his	O
or	O
her	O
career	O
.	O

To	O
cope	O
with	O
various	O
schemas	O
of	O
different	O
tables	O
,	O
it	O
's	O
essential	O
to	O
model	O
inter	O
-	O
record	O
information	O
within	O
the	O
tables	O
by	O
global	Method
addressing	Method
.	O

For	O
these	O
reasons	O
,	O
we	O
propose	O
a	O
pair	O
of	O
disordered	O
training	O
and	O
testing	O
set	O
based	O
on	O
WIKIBIO	Material
by	O
randomly	O
shuffling	O
the	O
records	O
of	O
a	O
infobox	O
.	O

For	O
example	O
,	O
the	O
order	O
of	O
several	O
records	O
in	O
a	O
specific	O
infobox	O
is	O
'	O
name	O
-	O
birthdateoccupation	O
-	O
spouse	O
'	O
,	O
we	O
randomly	O
shuffle	O
the	O
table	O
records	O
as	O
'	O
occupation	O
-	O
name	O
-	O
spouse	O
-	O
birthdate	O
'	O
,	O
without	O
changing	O
the	O
field	O
content	O
inside	O
the	O
'	O
occupation	O
'	O
,	O
'	O
name	O
'	O
,	O
'	O
spouse	O
'	O
and	O
'	O
birthdate	O
'	O
records	O
.	O

The	O
generated	O
descriptions	O
for	O
Binky	O
Jones	O
and	O
the	O
corresponding	O
reference	O
in	O
the	O
Wikipedia	O
.	O

Our	O
proposed	O
structaware	O
seq2seq	Method
model	O
can	O
generate	O
more	O
informative	O
and	O
accurate	O
description	O
compared	O
to	O
vanilla	O
seq2seq	Method
model	O
.	O

Table	O
4	O
shows	O
that	O
all	O
three	O
neural	Method
network	Method
models	Method
perform	O
not	O
as	O
good	O
as	O
before	O
,	O
which	O
means	O
the	O
order	O
of	O
table	O
records	O
is	O
an	O
essential	O
aspect	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O

However	O
,	O
the	O
BLEU	Metric
and	O
ROUGE	Metric
decreases	O
on	O
the	O
structureaware	O
seq2seq	Method
model	O
are	O
much	O
smaller	O
than	O
the	O
other	O
two	O
models	O
,	O
which	O
proves	O
the	O
efficiency	O
of	O
global	Method
addressing	Method
mechanism	Method
.	O

section	O
:	O
Model	O
section	O
:	O
Qualitative	Task
Analysis	Task
Analysis	Task
on	O
Dual	Task
Attention	Task
Dual	Method
attention	Method
mechanism	Method
models	O
the	O
relationship	O
between	O
the	O
generated	O
tokens	O
and	O
table	O
content	O
inside	O
each	O
record	O
by	O
word	Method
level	Method
attention	Method
while	O
encoding	O
the	O
relevance	O
of	O
generated	O
description	O
and	O
inter	O
-	O
record	O
information	O
within	O
the	O
table	O
by	O
field	O
level	O
attention	O
.	O

The	O
aggregation	O
of	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
can	O
model	O
more	O
precise	O
connection	O
between	O
the	O
table	O
and	O
its	O
generated	O
description	O
.	O

Fig	O
4	O
shows	O
an	O
example	O
of	O
the	O
three	O
attention	Method
mechanisms	Method
while	O
generating	O
a	O
piece	O
of	O
description	O
for	O
Frédéric	O
Fonteyne	O
based	O
on	O
his	O
Wikipedia	O
infobox	O
.	O

We	O
can	O
find	O
out	O
that	O
the	O
name	O
,	O
birthdate	O
,	O
nationality	O
and	O
occupation	O
information	O
contained	O
in	O
the	O
generated	O
sentence	O
can	O
properly	O
refer	O
to	O
the	O
related	O
table	O
content	O
by	O
the	O
aggregated	O
dual	O
attention	O
.	O

section	O
:	O
Case	O
Study	O
Fig	O
5	O
shows	O
the	O
generated	O
descriptions	O
for	O
different	O
variants	O
of	O
our	O
model	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
.	O

All	O
three	O
neural	Method
network	Method
generators	Method
can	O
produce	O
coherent	O
and	O
understandable	O
sentences	O
with	O
the	O
help	O
of	O
local	Method
addressing	Method
mechanism	Method
.	O

All	O
of	O
them	O
contain	O
the	O
word	O
'	O
baseball	O
'	O
which	O
is	O
not	O
directly	O
mentioned	O
in	O
the	O
infobox	O
.	O

It	O
means	O
the	O
generators	O
deduce	O
from	O
table	O
content	O
that	O
Binky	O
Jones	O
is	O
a	O
baseball	O
player	O
.	O

However	O
,	O
the	O
two	O
vanilla	O
seq2seq	Method
models	O
also	O
generate	O
'	O
major	O
league	O
baseball	O
'	O
or	O
'	O
major	O
leagues	O
'	O
which	O
are	O
not	O
mentioned	O
in	O
the	O
table	O
and	O
probably	O
not	O
correct	O
.	O

Vanilla	O
seq2seq	Method
model	O
without	O
global	Method
addressing	Method
on	O
the	O
table	O
just	O
generates	O
the	O
most	O
possible	O
league	O
in	O
Wikipedia	O
for	O
a	O
baseball	O
player	O
to	O
play	O
in	O
.	O

Furthermore	O
,	O
the	O
two	O
biographies	O
generated	O
by	O
vanilla	O
seq2seq	Method
model	O
fail	O
to	O
contain	O
the	O
information	O
from	O
the	O
infobox	O
which	O
team	O
he	O
served	O
in	O
,	O
as	O
well	O
as	O
the	O
time	O
period	O
of	O
his	O
playing	O
in	O
that	O
team	O
.	O

The	O
biography	O
generated	O
by	O
our	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
is	O
able	O
to	O
cover	O
nearly	O
all	O
the	O
information	O
mentioned	O
in	O
the	O
table	O
.	O

The	O
generated	O
segment	O
'	O
who	O
played	O
shortstop	O
from	O
april	O
15	O
to	O
april	O
27	O
for	O
the	O
brooklyn	O
robins	O
in	O
1924	O
'	O
(	O
15	O
words	O
)	O
includes	O
information	O
in	O
five	O
fields	O
of	O
the	O
table	O
:	O
'	O
position	O
'	O
,	O
'	O
debutdate	O
'	O
,	O
'	O
finaldate	O
'	O
,	O
'	O
debutteam	O
'	O
and	O
'	O
finalteam	O
'	O
,	O
which	O
is	O
achieved	O
by	O
the	O
global	O
addressing	O
between	O
the	O
fields	O
and	O
the	O
generated	O
tokens	O
.	O

section	O
:	O
Conclusions	O
We	O
propose	O
a	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
to	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O

The	O
model	O
consists	O
of	O
field	Method
-	Method
gating	Method
encoder	Method
and	O
description	Method
generator	Method
with	O
dual	Method
attention	Method
.	O

We	O
add	O
a	O
field	O
gate	O
to	O
the	O
encoder	O
LSTM	Method
unit	O
to	O
incorporate	O
the	O
field	O
information	O
.	O

Furthermore	O
,	O
dual	Method
attention	Method
mechanism	Method
which	O
contains	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
can	O
operate	O
local	O
and	O
global	O
addressing	O
to	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O

A	O
series	O
of	O
visualizations	O
,	O
case	O
studies	O
and	O
generation	Task
assessments	O
show	O
that	O
our	O
model	O
outperforms	O
the	O
competitive	O
baselines	O
by	O
a	O
large	O
margin	O
.	O

section	O
:	O
section	O
:	O
Acknowledgments	O
Our	O
work	O
is	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
Grant	O
No.2017YFB1002101	O
and	O
project	O
61772040	O
supported	O
by	O
NSFC	O
.	O

The	O
corresponding	O
authors	O
of	O
this	O
paper	O
are	O
Baobao	O
Chang	O
and	O
Zhifang	O
Sui	O
.	O

section	O
:	O
document	O
:	O
Towards	O
Faster	O
Training	O
of	O
Global	Method
Covariance	Method
Pooling	Method
Networks	Method
by	O
Iterative	Method
Matrix	Method
Square	Method
Root	Method
Normalization	Method
Global	Method
covariance	Method
pooling	Method
in	O
convolutional	Method
neural	Method
networks	Method
has	O
achieved	O
impressive	O
improvement	O
over	O
the	O
classical	O
first	Method
-	Method
order	Method
pooling	Method
.	O

Recent	O
works	O
have	O
shown	O
matrix	O
square	Method
root	Method
normalization	Method
plays	O
a	O
central	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

However	O
,	O
existing	O
methods	O
depend	O
heavily	O
on	O
eigendecomposition	Method
(	O
EIG	Method
)	O
or	O
singular	Method
value	Method
decomposition	Method
(	O
SVD	Method
)	O
,	O
suffering	O
from	O
inefficient	O
training	Task
due	O
to	O
limited	O
support	O
of	O
EIG	Method
and	O
SVD	Method
on	O
GPU	O
.	O

Towards	O
addressing	O
this	O
problem	O
,	O
we	O
propose	O
an	O
iterative	Method
matrix	Method
square	Method
root	Method
normalization	Method
method	Method
for	O
fast	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
of	Task
global	Task
covariance	Task
pooling	Task
networks	Task
.	O

At	O
the	O
core	O
of	O
our	O
method	O
is	O
a	O
meta	Method
-	Method
layer	Method
designed	O
with	O
loop	O
-	O
embedded	O
directed	O
graph	O
structure	O
.	O

The	O
meta	Method
-	Method
layer	Method
consists	O
of	O
three	O
consecutive	O
nonlinear	Method
structured	Method
layers	Method
,	O
which	O
perform	O
pre	Method
-	Method
normalization	Method
,	O
coupled	Method
matrix	Method
iteration	Method
and	O
post	Task
-	Task
compensation	Task
,	O
respectively	O
.	O

Our	O
method	O
is	O
much	O
faster	O
than	O
EIG	Method
or	O
SVD	Method
based	Method
ones	Method
,	O
since	O
it	O
involves	O
only	O
matrix	Method
multiplications	Method
,	O
suitable	O
for	O
parallel	Task
implementation	Task
on	O
GPU	O
.	O

Moreover	O
,	O
the	O
proposed	O
network	O
with	O
ResNet	Method
architecture	Method
can	O
converge	O
in	O
much	O
less	O
epochs	O
,	O
further	O
accelerating	O
network	Task
training	Task
.	O

On	O
large	O
-	O
scale	O
ImageNet	O
,	O
we	O
achieve	O
competitive	O
performance	O
superior	O
to	O
existing	O
counterparts	O
.	O

By	O
finetuning	O
our	O
models	O
pre	O
-	O
trained	O
on	O
ImageNet	Material
,	O
we	O
establish	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
three	O
challenging	O
fine	Task
-	Task
grained	Task
benchmarks	Task
.	O

The	O
source	O
code	O
and	O
network	Method
models	Method
will	O
be	O
available	O
at	O
.	O

footnote	O
[	O
page	O
]	O
section	O
:	O
Introduction	O
Deep	Method
convolutional	Method
neural	Method
networks	Method
(	O
ConvNets	Method
)	O
have	O
made	O
significant	O
progress	O
in	O
the	O
past	O
years	O
,	O
achieving	O
recognition	Metric
accuracy	Metric
surpassing	O
human	O
beings	O
in	O
large	Task
-	Task
scale	Task
object	Task
recognition	Task
.	O

The	O
ConvNet	Method
models	Method
pre	O
-	O
trained	O
on	O
ImageNet	Material
have	O
been	O
proven	O
to	O
benefit	O
a	O
multitude	O
of	O
other	O
computer	Task
vision	Task
tasks	Task
,	O
ranging	O
from	O
fine	Task
-	Task
grained	Task
visual	Task
categorization	Task
(	O
FGVC	Task
)	O
,	O
object	Task
detection	Task
,	O
semantic	Task
segmentation	Task
to	O
scene	Task
parsing	Task
,	O
where	O
labeled	O
data	O
are	O
insufficient	O
for	O
training	O
from	O
scratch	O
.	O

The	O
common	Method
layers	Method
such	O
as	O
convolution	Method
,	O
non	Method
-	Method
linear	Method
rectification	Method
,	O
pooling	Method
and	O
batch	Method
normalization	Method
have	O
become	O
off	O
-	O
the	O
-	O
shelf	O
commodities	O
,	O
widely	O
supported	O
on	O
devices	O
including	O
workstations	O
,	O
PCs	Method
and	O
embedded	Task
systems	Task
.	O

Although	O
the	O
architecture	O
of	O
ConvNet	Method
has	O
greatly	O
evolved	O
in	O
the	O
past	O
years	O
,	O
its	O
basic	O
layers	O
largely	O
keep	O
unchanged	O
.	O

Recently	O
,	O
researchers	O
have	O
shown	O
increasing	O
interests	O
in	O
exploring	O
structured	O
layers	O
to	O
enhance	O
representation	Task
capability	Task
of	Task
networks	Task
.	O

One	O
particular	O
kind	O
of	O
structured	Method
layer	Method
is	O
concerned	O
with	O
global	Method
covariance	Method
pooling	Method
after	O
the	O
last	O
convolution	Method
layer	Method
,	O
which	O
has	O
shown	O
impressive	O
improvement	O
over	O
the	O
classical	O
first	Method
-	Method
order	Method
pooling	Method
,	O
successfully	O
used	O
in	O
FGVC	Task
,	O
visual	Task
question	Task
answering	Task
and	O
video	Task
action	Task
recognition	Task
.	O

Very	O
recent	O
works	O
have	O
demonstrated	O
that	O
matrix	Method
square	Method
root	Method
normalization	Method
of	Method
global	Method
covariance	Method
pooling	Method
plays	O
a	O
key	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
both	O
large	Task
-	Task
scale	Task
visual	Task
recognition	Task
and	O
challenging	Task
FGVC	Task
.	O

CUDA	Method
support	O
Scalability	O
to	O
multi	Method
-	Method
GPUs	Method
Large	Method
-	Method
scale	Method
(	O
LS	Method
)	O
or	O
Small	Method
-	Method
scale	Method
(	O
SS	Method
)	O
EIG	Method
algorithm	Method
BP	Method
of	O
EIG	Method
limited	O
G	O
DeNet	O
SVD	Method
algorithm	O
BP	O
of	O
SVD	Method
limited	O
Improved	O
B	Method
-	Method
CNN	Method
Newton	Method
-	Method
Schulz	Method
Iter	Method
.	O

BP	Method
by	O
Lyapunov	Method
equation	Method
(	O
SCHUR	O
or	O
EIG	Method
required	O
)	O
iSQRT	Method
-	Method
COV	Method
(	O
ours	O
)	O
Newton	Method
-	Method
Schulz	Method
Iter	Method
.	O

BP	Method
of	Method
Newton	Method
-	Method
Schulz	Method
Iter	Method
.	O

For	O
computing	O
matrix	Task
square	Task
root	Task
,	O
existing	O
methods	O
depend	O
heavily	O
on	O
eigendecomposition	Method
(	O
EIG	Method
)	O
or	O
singular	Method
value	Method
decomposition	Method
(	O
SVD	Method
)	O
.	O

However	O
,	O
fast	O
implementation	O
of	O
EIG	Method
or	O
SVD	Method
on	O
GPU	Method
is	O
an	O
open	O
problem	O
,	O
which	O
is	O
limitedly	O
supported	O
on	O
NVIDIA	Method
CUDA	Method
platform	Method
,	O
significantly	O
slower	O
than	O
their	O
CPU	Method
counterparts	Method
.	O

As	O
such	O
,	O
existing	O
methods	O
opt	O
for	O
EIG	Method
or	O
SVD	Method
on	O
CPU	Method
for	O
computing	O
matrix	Task
square	Task
root	Task
.	O

Nevertheless	O
,	O
current	O
implementations	O
of	O
meta	Method
-	Method
layers	Method
depending	O
on	O
CPU	Method
are	O
far	O
from	O
ideal	O
,	O
particularly	O
for	O
multi	Task
-	Task
GPU	Task
configuration	Task
.	O

Since	O
GPUs	Method
with	O
powerful	O
parallel	O
computing	O
ability	O
have	O
to	O
be	O
interrupted	O
and	O
await	O
CPUs	O
with	O
limited	O
parallel	O
ability	O
,	O
their	O
concurrency	O
and	O
throughput	O
are	O
greatly	O
restricted	O
.	O

In	O
,	O
for	O
the	O
purpose	O
of	O
fast	Task
forward	Task
propagation	Task
(	Task
FP	Task
)	Task
,	O
Lin	O
and	O
Maji	O
use	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
(	O
called	O
modified	Method
Denman	Method
-	Method
Beavers	Method
iteration	Method
therein	O
)	O
algorithm	O
,	O
which	O
is	O
proposed	O
in	O
,	O
to	O
compute	O
matrix	O
square	O
-	O
root	O
.	O

Unfortunately	O
,	O
for	O
backward	Task
propagation	Task
(	O
BP	Method
)	O
,	O
they	O
compute	O
the	O
gradient	O
through	O
Lyapunov	Method
equation	Method
solution	Method
which	O
depends	O
on	O
the	O
GPU	Method
unfriendly	Method
Schur	Method
-	Method
decomposition	Method
(	O
SCHUR	Method
)	O
or	O
EIG	Method
.	O

Hence	O
,	O
the	O
training	O
in	O
is	O
expensive	O
though	O
FP	Method
which	O
involves	O
only	O
matrix	Method
multiplication	Method
runs	O
very	O
fast	O
.	O

Inspired	O
by	O
that	O
work	O
,	O
we	O
propose	O
a	O
fast	O
end	Method
-	Method
to	Method
-	Method
end	Method
training	Method
method	Method
,	O
called	O
iterative	Method
matrix	Method
square	Method
root	Method
normalization	Method
of	Method
covariance	Method
pooling	Method
(	O
iSQRT	Method
-	Method
COV	Method
)	O
,	O
depending	O
on	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
in	O
both	O
forward	Method
and	Method
backward	Method
propagations	Method
.	O

At	O
the	O
core	O
of	O
iSQRT	Method
-	Method
COV	Method
is	O
a	O
meta	Method
-	Method
layer	Method
with	O
loop	Method
-	Method
embedded	Method
directed	Method
graph	Method
structure	Method
,	O
specifically	O
designed	O
for	O
ensuring	O
both	O
convergence	O
of	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
and	O
performance	O
of	O
global	Method
covariance	Method
pooling	Method
networks	Method
.	O

The	O
meta	Method
-	Method
layer	Method
consists	O
of	O
three	O
consecutive	O
structured	Method
layers	Method
,	O
performing	O
pre	Method
-	Method
normalization	Method
,	O
coupled	Method
matrix	Method
iteration	Method
and	O
post	Task
-	Task
compensation	Task
,	O
respectively	O
.	O

We	O
derive	O
the	O
gradients	O
associated	O
with	O
the	O
involved	O
non	Method
-	Method
linear	Method
layers	Method
based	O
on	O
matrix	Method
backpropagation	Method
theory	Method
.	O

The	O
design	O
of	O
sandwiching	Method
Newton	Method
-	Method
Schulz	Method
iteration	Method
using	O
pre	Method
-	Method
normalization	Method
by	O
Frobenius	O
norm	O
or	O
trace	O
and	O
post	Method
-	Method
compensation	Method
is	O
essential	O
,	O
which	O
,	O
as	O
far	O
as	O
we	O
know	O
,	O
did	O
not	O
appear	O
in	O
previous	O
literature	O
(	O
e.g.	O
in	O
or	O
)	O
.	O

The	O
pre	Method
-	Method
normalization	Method
guarantees	O
convergence	O
of	O
Newton	Method
-	Method
Schulz	Method
(	Method
NS	Method
)	Method
iteration	Method
,	O
while	O
post	Method
-	Method
compensation	Method
plays	O
a	O
key	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
prevalent	O
deep	Method
ConvNet	Method
architectures	Method
,	O
e.g.	O
ResNet	Method
.	O

The	O
main	O
differences	O
between	O
our	O
method	O
and	O
other	O
related	O
works	O
are	O
summarized	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
B	Method
-	Method
CNN	Method
is	O
one	O
of	O
the	O
first	O
end	Task
-	Task
to	Task
-	Task
end	Task
covariance	Task
pooling	Task
ConvNets	Task
.	O

It	O
performs	O
element	Method
-	Method
wise	Method
square	Method
root	Method
normalization	Method
followed	O
by	O
normalization	Method
for	O
covariance	O
matrix	O
,	O
achieving	O
impressive	O
performance	O
in	O
FGVC	Task
task	Task
.	O

Improved	O
B	Method
-	Method
CNN	Method
shows	O
that	O
additional	O
matrix	Method
square	Method
root	Method
normalization	Method
before	O
element	Method
-	Method
wise	Method
square	Method
root	Method
and	O
normalization	Method
can	O
further	O
attain	O
large	O
improvement	O
.	O

In	O
training	Task
process	Task
,	O
they	O
perform	O
FP	Method
using	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
or	O
using	O
SVD	Method
,	O
and	O
perform	O
BP	Method
by	O
solving	O
Lyapunov	Method
equation	Method
or	O
compute	O
gradients	O
associated	O
with	O
SVD	Method
.	O

In	O
any	O
case	O
,	O
improved	O
B	Method
-	Method
CNN	Method
suffers	O
from	O
GPU	O
unfriendly	O
SVD	Method
,	O
SCHUR	O
or	O
EIG	Method
and	O
so	O
network	Method
training	Method
is	O
expensive	O
.	O

Our	O
iSQRT	Method
-	Method
COV	Method
differs	O
from	O
in	O
three	O
aspects	O
.	O

First	O
,	O
both	O
FP	Method
and	O
BP	Method
of	O
our	O
method	O
are	O
based	O
on	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
,	O
making	O
network	Method
training	Method
very	O
efficient	O
as	O
only	O
GPU	Method
friendly	Method
matrix	Method
multiplications	Method
are	O
involved	O
.	O

Second	O
,	O
we	O
propose	O
sandwiching	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
using	O
pre	Method
-	Method
normalization	Method
and	O
post	Method
-	Method
compensation	Method
which	O
is	O
essential	O
and	O
plays	O
a	O
key	O
role	O
in	O
training	O
extremely	O
deep	Task
ConvNets	Task
.	O

Finally	O
,	O
we	O
evaluate	O
extensively	O
on	O
both	O
large	Material
-	Material
scale	Material
ImageNet	Material
and	O
on	O
three	O
popular	O
fine	O
-	O
grained	O
benchmarks	O
.	O

In	O
,	O
matrix	Method
power	Method
normalized	Method
covariance	Method
pooling	Method
method	Method
(	O
MPN	Method
-	Method
COV	Method
)	O
is	O
proposed	O
for	O
large	Task
-	Task
scale	Task
visual	Task
recognition	Task
.	O

It	O
achieves	O
impressive	O
improvements	O
over	O
first	Method
-	Method
order	Method
pooling	Method
with	O
AlexNet	Method
,	O
VGG	Method
-	Method
Net	Method
and	O
ResNet	Method
architectures	Method
.	O

MPN	Method
-	Method
COV	Method
has	O
shown	O
that	O
,	O
given	O
a	O
small	O
number	O
of	O
high	O
-	O
dimensional	O
features	O
,	O
matrix	O
power	O
is	O
consistent	O
with	O
shrinkage	Method
principle	Method
of	Method
robust	Method
covariance	Method
estimation	Method
,	O
and	O
matrix	Method
square	Method
root	Method
can	O
be	O
derived	O
as	O
a	O
robust	Method
covariance	Method
estimator	Method
via	O
a	O
von	Method
Neumann	Method
regularized	Method
maximum	Method
likelihood	Method
estimation	Method
.	O

It	O
is	O
also	O
shown	O
that	O
matrix	Method
power	Method
normalization	Method
approximately	O
yet	O
effectively	O
exploits	O
geometry	O
of	O
the	O
manifold	O
of	O
covariance	O
matrices	O
,	O
superior	O
to	O
matrix	Method
logarithm	Method
normalization	Method
for	O
high	Task
-	Task
dimensional	Task
features	Task
.	O

All	O
computations	O
of	O
MPN	Method
-	Method
COV	Method
meta	Method
-	Method
layer	Method
are	O
implemented	O
with	O
NVIDIA	Method
cuBLAS	Method
library	Method
running	O
on	O
GPU	O
,	O
except	O
EIG	Method
which	O
runs	O
on	O
CPU	O
.	O

G	Method
DeNet	Method
is	O
concerned	O
with	O
inserting	O
global	Method
Gaussian	Method
distributions	Method
into	O
ConvNets	Method
for	O
end	Task
-	Task
to	Task
-	Task
end	Task
learning	Task
.	O

In	O
G	O
DeNet	O
,	O
each	O
Gaussian	O
is	O
identified	O
as	O
square	O
root	O
of	O
a	O
symmetric	O
positive	O
definite	O
matrix	O
based	O
on	O
Lie	O
group	O
structure	O
of	O
Gaussian	O
manifold	O
.	O

The	O
matrix	Method
square	Method
root	Method
plays	O
a	O
central	O
role	O
in	O
obtaining	O
the	O
competitive	O
performance	O
.	O

Compact	Method
bilinear	Method
pooling	Method
(	O
CBP	Method
)	O
clarifies	O
that	O
bilinear	Method
pooling	Method
is	O
closely	O
related	O
to	O
the	O
second	Method
-	Method
order	Method
polynomial	Method
kernel	Method
,	O
and	O
presents	O
two	O
compact	Method
representations	Method
via	O
low	Method
-	Method
dimensional	Method
feature	Method
maps	Method
for	O
kernel	Method
approximation	Method
.	O

Kernel	Method
pooling	Method
approximates	O
Gaussian	Method
RBF	Method
kernel	Method
to	O
a	O
given	O
order	O
through	O
compact	O
explicit	O
feature	O
maps	O
,	O
aiming	O
to	O
characterize	O
higher	O
order	O
feature	O
interactions	O
.	O

Cai	O
et	O
al	O
.	O

introduce	O
a	O
polynomial	Method
kernel	Method
based	Method
predictor	Method
to	O
model	O
higher	O
-	O
order	O
statistics	O
of	O
convolutional	O
features	O
across	O
multiple	O
layers	O
.	O

section	O
:	O
Proposed	O
iSQRT	Method
-	Method
COV	Method
Network	O
In	O
this	O
section	O
,	O
we	O
first	O
give	O
an	O
overview	O
of	O
the	O
proposed	O
iSQRT	Method
-	Method
COV	Method
network	O
.	O

Then	O
we	O
describe	O
matrix	Method
square	Method
root	Method
computation	Method
and	O
its	O
forward	Method
propagation	Method
.	O

We	O
finally	O
derive	O
the	O
corresponding	O
backward	O
gradients	O
.	O

subsection	O
:	O
Overview	O
of	O
Method	O
The	O
flowchart	O
of	O
the	O
proposed	O
network	O
is	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Let	O
output	O
of	O
the	O
last	O
convolutional	Method
layer	Method
(	O
with	O
ReLU	Method
)	O
be	O
a	O
tensor	O
with	O
spatial	O
height	O
,	O
width	O
and	O
channel	O
.	O

We	O
reshape	O
the	O
tensor	O
to	O
a	O
feature	O
matrix	O
consisting	O
of	O
features	O
of	O
dimension	O
.	O

Then	O
we	O
perform	O
second	Method
-	Method
order	Method
pooling	Method
by	O
computing	O
the	O
covariance	O
matrix	O
,	O
where	O
,	O
and	O
are	O
the	O
identity	O
matrix	O
and	O
matrix	O
of	O
all	O
ones	O
,	O
respectively	O
.	O

Our	O
meta	Method
-	Method
layer	Method
is	O
designed	O
to	O
have	O
loop	O
-	O
embedded	O
directed	O
graph	O
structure	O
,	O
consisting	O
of	O
three	O
consecutive	O
nonlinear	Method
structured	Method
layers	Method
.	O

The	O
purpose	O
of	O
the	O
first	O
layer	O
(	O
i.e.	O
,	O
pre	Method
-	Method
normalization	Method
)	O
is	O
to	O
guarantee	O
the	O
convergence	O
of	O
the	O
following	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
,	O
achieved	O
by	O
dividing	O
the	O
covariance	O
matrix	O
by	O
its	O
trace	O
(	O
or	O
Frobenius	O
norm	O
)	O
.	O

The	O
second	O
layer	O
is	O
of	O
loop	O
structure	O
,	O
repeating	O
the	O
coupled	Method
matrix	Method
equations	Method
involved	O
in	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
a	O
fixed	O
number	O
of	O
times	O
,	O
for	O
computing	O
approximate	Task
matrix	Task
square	Task
root	Task
.	O

The	O
pre	O
-	O
normalization	Task
nontrivially	O
changes	O
data	O
magnitudes	O
,	O
so	O
we	O
design	O
the	O
third	O
layer	O
(	O
i.e.	O
,	O
post	O
-	O
compensation	O
)	O
to	O
counteract	O
the	O
adverse	O
effect	O
by	O
multiplying	O
trace	O
(	O
or	O
Frobenius	O
norm	O
)	O
of	O
the	O
square	O
root	O
of	O
the	O
covariance	O
matrix	O
.	O

As	O
the	O
output	O
of	O
our	O
meta	Method
-	Method
layer	Method
is	O
a	O
symmetric	O
matrix	O
,	O
we	O
concatenate	O
its	O
upper	O
triangular	O
entries	O
forming	O
an	O
-	O
dimensional	O
vector	O
,	O
submitted	O
to	O
the	O
subsequent	O
layer	O
of	O
the	O
ConvNet	Method
.	O

subsection	O
:	O
Matrix	Method
Square	Method
Root	Method
and	O
Forward	Method
Propagation	Method
Square	O
roots	O
of	O
matrices	O
,	O
particularly	O
covariance	Method
matrices	Method
which	O
are	O
symmetric	Method
positive	Method
(	Method
semi	Method
)	Method
definite	Method
(	Method
SPD	Method
)	Method
,	O
find	O
applications	O
in	O
a	O
variety	O
of	O
fields	O
including	O
computer	Task
vision	Task
,	O
medical	Task
imaging	Task
and	O
chemical	Task
physics	Task
.	O

It	O
is	O
well	O
-	O
known	O
any	O
SPD	Method
matrix	Method
has	O
a	O
unique	O
square	O
root	O
which	O
can	O
be	O
computed	O
accurately	O
by	O
EIG	Method
or	O
SVD	Method
.	O

Briefly	O
,	O
let	O
be	O
an	O
SPD	Method
matrix	Method
and	O
it	O
has	O
EIG	Method
,	O
where	O
is	O
orthogonal	O
and	O
is	O
a	O
diagonal	O
matrix	O
of	O
eigenvalues	O
of	O
.	O

Then	O
has	O
a	O
square	O
root	O
,	O
i.e.	O
,	O
.	O

Unfortunately	O
,	O
both	O
EIG	Method
and	O
SVD	Method
are	O
not	O
well	O
supported	O
on	O
GPU	O
.	O

paragraph	O
:	O
Newton	Method
-	Method
Schulz	Method
Iteration	Method
Higham	O
studied	O
a	O
class	O
of	O
methods	O
for	O
iteratively	Task
computing	Task
matrix	Task
square	Task
root	Task
.	O

These	O
methods	O
,	O
termed	O
as	O
Newton	Method
-	Method
Padé	Method
iterations	Method
,	O
are	O
developed	O
based	O
on	O
the	O
connection	O
between	O
matrix	O
sign	O
function	O
and	O
matrix	O
square	O
root	O
,	O
together	O
with	O
rational	Method
Padé	Method
approximation	Method
.	O

Specifically	O
,	O
for	O
computing	O
the	O
square	O
root	O
of	O
,	O
given	O
and	O
,	O
for	O
,	O
the	O
coupled	Method
iteration	Method
takes	O
the	O
following	O
form	O
:	O
where	O
and	O
are	O
polynomials	O
,	O
and	O
and	O
are	O
non	O
-	O
negative	O
integers	O
.	O

Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
converges	O
only	O
locally	O
:	O
if	O
where	O
denotes	O
any	O
induced	O
(	O
or	O
consistent	O
)	O
matrix	O
norm	O
,	O
and	O
quadratically	O
converge	O
to	O
and	O
,	O
respectively	O
.	O

The	O
family	O
of	O
coupled	Method
iteration	Method
is	O
stable	O
in	O
that	O
small	O
errors	O
in	O
the	O
previous	O
iteration	O
will	O
not	O
be	O
amplified	O
.	O

The	O
case	O
of	O
called	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
fits	O
for	O
our	O
purpose	O
as	O
no	O
GPU	Method
unfriendly	Method
matrix	Method
inverse	Method
is	O
involved	O
:	O
Clearly	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
involves	O
only	O
matrix	Method
product	Method
,	O
suitable	O
for	O
parallel	Task
implementation	Task
on	O
GPU	O
.	O

Compared	O
to	O
accurate	O
square	O
root	O
computed	O
by	O
EIG	Method
,	O
one	O
can	O
only	O
obtain	O
approximate	Method
solution	Method
with	O
a	O
small	O
number	O
of	O
iterations	O
.	O

We	O
determine	O
the	O
number	O
of	O
iterations	O
by	O
cross	Method
-	Method
validation	Method
.	O

Interestingly	O
,	O
compared	O
to	O
EIG	Method
or	O
SVD	Method
based	O
methods	O
,	O
experiments	O
on	O
large	Material
-	Material
scale	Material
ImageNet	Material
show	O
that	O
we	O
can	O
obtain	O
matching	O
or	O
marginally	O
better	O
performance	O
under	O
AlexNet	Method
architecture	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
and	O
better	O
performance	O
under	O
ResNet	Method
architecture	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
using	O
no	O
more	O
than	O
5	O
iterations	O
.	O

paragraph	O
:	O
Pre	O
-	O
normalization	Task
and	O
Post	Task
-	Task
compensation	Task
As	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
only	O
converges	O
locally	O
,	O
we	O
pre	O
-	O
normalize	O
by	O
trace	O
or	O
Frobenius	O
norm	O
,	O
i.e.	O
,	O
Let	O
be	O
eigenvalues	O
of	O
,	O
arranged	O
in	O
nondecreasing	O
order	O
.	O

As	O
and	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
,	O
which	O
equals	O
to	O
the	O
largest	O
singular	O
value	O
of	O
,	O
is	O
and	O
for	O
the	O
case	O
of	O
trace	Metric
and	Metric
Frobenius	Metric
norm	Metric
,	O
respectively	O
,	O
both	O
less	O
than	O
1	O
.	O

Hence	O
,	O
the	O
convergence	O
condition	O
is	O
satisfied	O
.	O

The	O
above	O
pre	Method
-	Method
normalization	Method
of	Method
covariance	Method
matrix	Method
nontrivially	O
changes	O
the	O
data	O
magnitudes	O
such	O
that	O
it	O
produces	O
adverse	O
effect	O
on	O
network	O
.	O

Hence	O
,	O
to	O
counteract	O
this	O
change	O
,	O
after	O
the	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
,	O
we	O
accordingly	O
perform	O
post	Method
-	Method
compensation	Method
,	O
i.e.	O
,	O
An	O
alternative	O
scheme	O
to	O
counterbalance	O
the	O
influence	O
incurred	O
by	O
pre	Method
-	Method
normalization	Method
is	O
Batch	Method
Normalization	Method
(	O
BN	Method
)	O
.	O

One	O
may	O
even	O
consider	O
without	O
using	O
any	O
post	Method
-	Method
compensation	Method
.	O

However	O
,	O
our	O
experiment	O
on	O
ImageNet	Material
has	O
shown	O
that	O
,	O
without	O
post	Method
-	Method
normalization	Method
,	O
prevalent	O
ResNet	Method
fails	O
to	O
converge	O
,	O
while	O
our	O
scheme	O
outperforms	O
BN	Method
by	O
about	O
1	O
%	O
(	O
see	O
[	O
reference	O
]	O
for	O
details	O
)	O
.	O

subsection	O
:	O
Backward	Method
Propagation	Method
(	O
BP	Method
)	O
The	O
gradients	O
associated	O
with	O
the	O
structured	Method
layers	Method
are	O
derived	O
using	O
matrix	Method
backpropagation	Method
methodology	Method
,	O
which	O
establishes	O
the	O
chain	O
rule	O
of	O
a	O
general	Method
matrix	Method
function	Method
by	O
first	Method
-	Method
order	Method
Taylor	Method
approximation	Method
.	O

Below	O
we	O
take	O
pre	O
-	O
normalization	O
by	O
trace	O
as	O
an	O
example	O
,	O
deriving	O
the	O
corresponding	O
gradients	O
.	O

paragraph	O
:	O
BP	Task
of	Task
Post	Task
-	Task
compensation	Task
Given	O
where	O
is	O
the	O
loss	O
function	O
,	O
the	O
chain	Method
rule	Method
is	O
of	O
the	O
form	O
where	O
denotes	O
variation	O
of	O
.	O

After	O
some	O
manipulations	O
,	O
we	O
have	O
paragraph	O
:	O
BP	Method
of	Method
Newton	Method
-	Method
Schulz	Method
Iteration	Method
Then	O
we	O
are	O
to	O
compute	O
the	O
partial	O
derivatives	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
and	O
,	O
,	O
given	O
computed	O
by	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
.	O

As	O
the	O
covariance	O
matrix	O
is	O
symmetric	O
,	O
it	O
is	O
easy	O
to	O
see	O
from	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
that	O
and	O
are	O
both	O
symmetric	O
.	O

According	O
to	O
the	O
chain	O
rules	O
(	O
omitted	O
hereafter	O
for	O
simplicity	O
)	O
of	O
matrix	Method
backpropagation	Method
and	O
after	O
some	O
manipulations	O
,	O
,	O
we	O
can	O
derive	O
The	O
final	O
step	O
of	O
this	O
layer	O
is	O
concerned	O
with	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
,	O
which	O
is	O
given	O
by	O
paragraph	O
:	O
BP	O
of	O
Pre	Method
-	Method
normalization	Method
Note	O
that	O
here	O
we	O
need	O
to	O
combine	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
,	O
backpropagated	O
from	O
the	O
post	Method
-	Method
compensation	Method
layer	Method
.	O

As	O
such	O
,	O
by	O
referring	O
to	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
we	O
make	O
similar	O
derivations	O
as	O
before	O
and	O
obtain	O
If	O
we	O
adopt	O
pre	O
-	O
normalization	O
by	O
Frobenius	O
norm	O
,	O
the	O
gradients	O
associated	O
with	O
post	Method
-	Method
compensation	Method
become	O
and	O
that	O
with	O
respect	O
to	O
pre	O
-	O
normalization	O
is	O
while	O
the	O
backward	O
gradients	O
of	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
(	O
[	O
reference	O
]	O
)	O
keep	O
unchanged	O
.	O

Finally	O
,	O
given	O
,	O
one	O
can	O
derive	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
input	O
matrix	O
,	O
which	O
takes	O
the	O
following	O
form	O
:	O
section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
method	O
on	O
both	O
large	Task
-	Task
scale	Task
image	Task
classification	Task
and	O
challenging	Task
fine	Task
-	Task
grained	Task
visual	Task
categorization	Task
tasks	Task
.	O

We	O
make	O
experiments	O
using	O
two	O
PCs	Method
each	O
of	O
which	O
is	O
equipped	O
with	O
a	O
4	O
-	O
core	O
Intel	O
i7	O
-	O
4790k@4.0GHz	O
CPU	O
,	O
32	O
G	O
RAM	O
,	O
512	O
GB	O
Samsung	Method
PRO	Method
SSD	Method
and	O
two	O
Titan	Method
Xp	Method
GPUs	Method
.	O

We	O
implement	O
our	O
networks	O
using	O
MatConvNet	Method
and	O
Matlab2015b	Method
,	O
under	O
Ubuntu	Method
14.04.5	Method
LTS	Method
.	O

subsection	O
:	O
Datasets	O
and	O
Our	O
Meta	Method
-	Method
layer	Method
Implementation	Method
Datasets	O
For	O
large	Task
-	Task
scale	Task
image	Task
classification	Task
,	O
we	O
adopt	O
ImageNet	Material
LSVRC2012	Material
dataset	Material
with	O
1	O
,	O
000	O
object	O
categories	O
.	O

The	O
dataset	O
contains	O
1.28	O
M	O
images	O
for	O
training	O
,	O
50	O
K	O
images	O
for	O
validation	Task
and	O
100	O
K	O
images	O
for	O
testing	O
(	O
without	O
published	O
labels	O
)	O
.	O

As	O
in	O
,	O
we	O
report	O
the	O
results	O
on	O
the	O
validation	O
set	O
.	O

For	O
fine	Task
-	Task
grained	Task
categorization	Task
,	O
we	O
use	O
three	O
popular	O
fine	O
-	O
grained	O
benchmarks	O
,	O
i.e.	O
,	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
(	O
Birds	Material
)	O
,	O
FGVC	Material
-	Material
aircraft	Material
(	O
Aircrafts	Material
)	O
and	O
Stanford	Material
cars	Material
(	O
Cars	Material
)	O
.	O

The	O
Birds	Material
dataset	Material
contains	O
11	O
,	O
788	O
images	O
from	O
200	O
species	O
,	O
with	O
large	O
intra	O
-	O
class	O
variation	O
but	O
small	O
inter	O
-	O
class	O
variation	O
.	O

The	O
Aircrafts	Material
dataset	O
includes	O
100	O
aircraft	Material
classes	Material
and	O
a	O
total	O
of	O
10	O
,	O
000	O
images	O
with	O
small	O
background	O
noise	O
but	O
higher	O
inter	O
-	O
class	O
similarity	O
.	O

The	O
Cars	Material
dataset	O
consists	O
of	O
16	O
,	O
185	O
images	O
from	O
196	O
classes	O
.	O

For	O
all	O
datasets	O
,	O
we	O
adopt	O
the	O
provided	O
training	O
/	O
test	O
split	O
,	O
using	O
neither	O
bounding	O
boxes	O
nor	O
part	O
annotations	O
.	O

Implementation	O
of	O
iSQRT	Method
-	Method
COV	Method
Meta	O
-	O
layer	O
We	O
encapsulate	O
our	O
code	O
in	O
three	O
computational	O
blocks	O
,	O
which	O
implement	O
forward	Method
&	Method
backward	Method
computation	Method
of	Method
pre	Method
-	Method
normalization	Method
layer	Method
,	O
Newton	Method
-	Method
Schulz	Method
iteration	Method
layer	Method
and	O
post	Method
-	Method
compensation	Method
layer	Method
,	O
respectively	O
.	O

The	O
code	O
is	O
written	O
in	O
C	Method
++	Method
based	O
on	O
NVIDIA	Method
on	O
top	O
of	O
CUDA	Method
toolkit	Method
8.0	Method
.	O

In	O
addition	O
,	O
we	O
write	O
code	O
in	O
C	Method
++	Method
based	O
on	O
cuBLAS	Method
for	O
computing	Task
covariance	Task
matrices	Task
.	O

We	O
create	O
MEX	Method
files	Method
so	O
that	O
the	O
above	O
subroutines	O
can	O
be	O
called	O
in	O
Matlab	O
environment	O
.	O

For	O
AlexNet	Task
,	O
we	O
insert	O
our	O
meta	Method
-	Method
layer	Method
after	O
the	O
last	O
convolution	Method
layer	Method
(	O
with	O
ReLU	Method
)	O
,	O
which	O
outputs	O
an	O
tensor	O
.	O

For	O
ResNet	Method
architecture	Method
,	O
as	O
suggested	O
,	O
we	O
do	O
not	O
perform	O
downsampling	O
for	O
the	O
last	O
set	O
of	O
convolutional	O
blocks	O
,	O
and	O
add	O
one	O
convolution	Method
with	O
channels	O
after	O
the	O
last	O
sum	Method
layer	Method
(	O
with	O
ReLU	Method
)	O
.	O

The	O
added	O
convolution	Method
layer	Method
outputs	O
an	O
tensor	O
.	O

Hence	O
,	O
with	O
both	O
architectures	O
,	O
the	O
covariance	O
matrix	O
is	O
of	O
size	O
and	O
our	O
meta	Method
-	Method
layer	Method
outputs	O
an	O
-	O
dimensional	O
vector	O
as	O
the	O
image	Method
representation	Method
.	O

subsection	O
:	O
Evaluation	O
with	O
AlexNet	Method
on	O
ImageNet	Task
In	O
the	O
first	O
part	O
of	O
experiments	O
,	O
we	O
analyze	O
,	O
with	O
AlexNet	Method
architecture	Method
,	O
the	O
design	O
choices	O
of	O
our	O
iSQRT	Method
-	Method
COV	Method
method	O
,	O
including	O
the	O
number	O
of	O
Newton	Method
-	Method
Schulz	Method
iterations	Method
,	O
time	Metric
and	Metric
memory	Metric
usage	Metric
,	O
and	O
behaviors	O
of	O
different	O
pre	Method
-	Method
normalization	Method
methods	Method
.	O

We	O
select	O
AlexNet	Method
because	O
it	O
runs	O
faster	O
with	O
shallower	O
depth	O
,	O
and	O
the	O
results	O
can	O
extrapolate	O
to	O
deeper	Method
networks	Method
which	O
mostly	O
follow	O
its	O
architecture	O
design	O
.	O

We	O
follow	O
for	O
color	Task
augmentation	Task
and	O
weight	Task
initialization	Task
,	O
adopting	O
BN	Method
and	Method
no	Method
dropout	Method
.	O

We	O
use	O
SGD	Method
with	O
a	O
mini	O
-	O
batch	O
of	O
128	O
,	O
unless	O
otherwise	O
stated	O
.	O

The	O
momentum	O
is	O
0.9	O
and	O
weight	O
decay	O
is	O
0.0005	O
.	O

We	O
train	O
iSQRT	Method
-	Method
COV	Method
networks	O
from	O
scratch	O
in	O
20	O
epochs	O
where	O
learning	Metric
rate	Metric
follows	O
exponential	Method
decay	Method
.	O

All	O
training	O
and	O
test	O
images	O
are	O
uniformly	O
resized	O
with	O
shorter	O
sides	O
of	O
256	O
.	O

During	O
training	O
we	O
randomly	O
crop	O
a	O
patch	O
from	O
each	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O

We	O
make	O
inference	Task
on	O
one	O
single	O
center	O
crop	O
from	O
a	O
test	O
image	O
.	O

Impact	O
of	O
Number	O
N	O
of	O
Newton	Method
-	Method
Schulz	Method
Iterations	Method
Fig	O
.	O

[	O
reference	O
]	O
shows	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
as	O
a	O
function	O
of	O
number	O
of	O
Newton	O
-	O
Schulz	O
iterations	O
in	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Plain	Method
-	Method
COV	Method
indicates	O
simple	O
covariance	Method
pooling	Method
without	O
any	O
normalization	Method
.	O

With	O
one	O
single	O
iteration	O
,	O
our	O
method	O
outperforms	O
Plain	Method
-	Method
COV	Method
by	O
.	O

As	O
iteration	O
number	O
grows	O
,	O
the	O
error	Metric
rate	Metric
of	O
iSQRT	Method
-	Method
COV	Method
gradually	O
declines	O
.	O

With	O
3	O
iterations	O
,	O
iSQRT	Method
-	Method
COV	Method
is	O
comparable	O
to	O
MPN	Method
-	Method
COV	Method
,	O
having	O
only	O
0.3	O
%	O
higher	O
error	Metric
rate	Metric
,	O
while	O
performing	O
marginally	O
better	O
than	O
MPN	Method
-	Method
COV	Method
between	O
5	O
and	O
7	O
iterations	O
.	O

After	O
,	O
the	O
error	Metric
rate	Metric
consistently	O
increases	O
,	O
indicating	O
growth	O
of	O
iteration	O
number	O
is	O
not	O
helpful	O
for	O
improving	O
accuracy	Metric
.	O

As	O
larger	O
incurs	O
higher	O
computational	Metric
cost	Metric
,	O
to	O
balance	O
efficiency	O
and	O
accuracy	Metric
,	O
we	O
set	O
to	O
5	O
in	O
the	O
remaining	O
experiments	O
.	O

Notably	O
,	O
the	O
approximate	Method
square	Method
root	Method
normalization	Method
improves	O
a	O
little	O
over	O
the	O
accurate	O
one	O
obtained	O
via	O
EIG	Method
.	O

This	O
interesting	O
problem	O
will	O
be	O
discussed	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
where	O
iSQRT	Method
-	Method
COV	Method
is	O
further	O
evaluated	O
on	O
substantially	O
deeper	O
ResNets	O
.	O

C	Method
++	Method
N	O
/	O
A	O
Impro	O
.	O

B	O
-	O
CNN	O
SVD	Method
or	O
EIG	Method
CUDA	O
cuSOLVER	O
Matlab	O
(	O
CPU	O
function	O
)	O
Matlab	O
(	O
GPU	O
function	O
)	O
Time	Method
and	Method
Memory	Method
Analysis	Method
We	O
compare	O
time	Metric
and	O
memory	Metric
consumed	O
by	O
single	O
meta	Method
-	Method
layer	Method
of	O
different	O
methods	O
.	O

We	O
use	O
public	O
code	O
for	O
,	O
and	O
released	O
by	O
the	O
respective	O
authors	O
.	O

As	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
table	O
:	O
single	O
-	O
layer	O
,	O
iSQRT	Method
-	Method
COV	Method
(	O
)	O
and	O
iSQRT	Method
-	Method
COV	Method
(	O
)	O
are	O
3.1x	O
faster	O
and	O
1.8x	O
faster	O
than	O
MPN	Method
-	Method
COV	Method
,	O
respectively	O
.	O

Furthermore	O
,	O
iSQRT	Method
-	Method
COV	Method
(	O
)	O
is	O
five	O
times	O
more	O
efficient	O
than	O
improved	O
B	Method
-	Method
CNN	Method
and	O
G	Method
DeNet	Method
.	O

For	O
improved	O
B	Task
-	Task
CNN	Task
,	O
the	O
forward	Task
computation	Task
of	O
Newton	Method
-	Method
Schulz	Method
(	Method
NS	Method
)	Method
iteration	Method
is	O
much	O
faster	O
than	O
that	O
of	O
SVD	Method
,	O
but	O
the	O
total	O
time	O
of	O
two	O
methods	O
is	O
comparable	O
.	O

The	O
authors	O
of	O
improved	O
B	Method
-	Method
CNN	Method
also	O
proposed	O
two	O
other	O
implementations	O
,	O
i.e.	O
,	O
FP	Method
by	O
NS	Method
iteration	Method
plus	O
BP	Method
by	O
SVD	Method
and	O
FP	Method
by	O
SVD	Method
plus	O
BP	Method
by	O
Lyapunov	Method
(	O
Lyap	Method
.	O

)	O
,	O
which	O
take	O
15.31	O
(	O
2.09	O
)	O
and	O
12.21	O
(	O
11.19	O
)	O
,	O
respectively	O
.	O

We	O
observe	O
that	O
,	O
in	O
any	O
case	O
,	O
the	O
forward	O
backward	O
time	O
taken	O
by	O
single	O
meta	Method
-	Method
layer	Method
of	O
improved	Method
B	Method
-	Method
CNN	Method
is	O
significant	O
as	O
GPU	O
unfriendly	O
SVD	Method
or	O
EIG	Method
can	O
not	O
be	O
avoided	O
,	O
even	O
though	O
the	O
forward	Method
computation	Method
is	O
very	O
efficient	O
when	O
NS	Method
iteration	Method
is	O
used	O
.	O

Tab	O
.	O

[	O
reference	O
]	O
table	O
:	O
time	O
-	O
matrix	Method
-	Method
decomposition	Method
presents	O
running	Metric
time	Metric
of	O
EIG	Method
and	O
SVD	Method
of	O
an	O
covariance	Method
matrix	Method
.	O

Matlab	Method
(	O
M	O
)	O
built	O
-	O
in	O
CPU	O
functions	O
and	O
GPU	Method
functions	Method
deliver	O
over	O
10x	O
and	O
2.1x	O
speedups	O
over	O
their	O
CUDA	Method
counterparts	Method
,	O
respectively	O
.	O

Our	O
method	O
needs	O
to	O
store	O
and	O
in	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
which	O
will	O
be	O
used	O
in	O
backpropagation	Method
,	O
taking	O
up	O
more	O
memory	O
than	O
EIG	Method
or	O
SVD	Method
based	O
ones	O
.	O

Among	O
all	O
,	O
our	O
iSQRT	Method
-	Method
COV	Method
(	O
)	O
takes	O
up	O
the	O
largest	O
memory	O
of	O
1.129	O
MB	O
,	O
which	O
is	O
insignificant	O
compared	O
to	O
12	O
GB	O
memory	O
on	O
a	O
Titan	O
Xp	O
.	O

Note	O
that	O
for	O
network	Task
inference	Task
only	O
,	O
our	O
method	O
takes	O
0.125	O
MB	O
memory	O
as	O
it	O
is	O
unnecessary	O
to	O
store	O
and	O
.	O

Next	O
,	O
we	O
compare	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
speed	O
of	O
network	Method
training	Method
between	O
MPN	Method
-	Method
COV	Method
and	O
iSQRT	Method
-	Method
COV	Method
with	O
both	O
one	O
-	O
GPU	O
and	O
two	O
-	O
GPU	Method
configurations	Method
.	O

For	O
one	O
-	O
GPU	O
configuration	O
,	O
the	O
speed	Metric
gap	Metric
vs.	O
batch	Metric
size	Metric
between	O
the	O
two	O
methods	O
keeps	O
nearly	O
constant	O
.	O

For	O
two	O
-	O
GPU	O
configuration	O
,	O
their	O
speed	Metric
gap	Metric
becomes	O
more	O
significant	O
when	O
batch	O
size	O
gets	O
larger	O
.	O

As	O
can	O
be	O
seen	O
,	O
the	O
speed	O
of	O
iSQRT	Method
-	Method
COV	Method
network	O
continuously	O
grows	O
with	O
increase	O
of	O
batch	O
size	O
while	O
that	O
of	O
MPN	Method
-	Method
COV	Method
tends	O
to	O
saturate	O
when	O
batch	O
size	O
is	O
larger	O
than	O
512	O
.	O

Clearly	O
our	O
iSQRT	Method
-	Method
COV	Method
network	O
can	O
make	O
better	O
use	O
of	O
computing	O
power	O
of	O
multiple	O
GPUs	Method
than	O
MPN	Method
-	Method
COV	Method
.	O

Pre	O
-	O
normalization	O
by	O
Trace	O
vs.	O
by	O
Frobenius	O
Norm	O
Sec	O
.	O

[	O
reference	O
]	O
describes	O
two	O
pre	Method
-	Method
normalization	Method
methods	Method
.	O

Here	O
we	O
compare	O
them	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
bottom	O
rows	O
)	O
,	O
where	O
iSQRT	Method
-	Method
COV	Method
(	O
trace	Method
)	O
indicates	O
pre	Method
-	Method
normalization	Method
by	O
trace	O
.	O

We	O
can	O
see	O
that	O
pre	O
-	O
normalization	Method
by	O
trace	Method
produces	O
0.3	O
%	O
lower	O
error	Metric
rate	Metric
than	O
that	O
by	O
Frobenius	Method
norm	Method
,	O
while	O
taking	O
similar	O
time	O
with	O
the	O
latter	O
.	O

Hence	O
,	O
in	O
all	O
the	O
remaining	O
experiments	O
,	O
we	O
adopt	O
trace	Method
based	Method
pre	Method
-	Method
normalization	Method
method	Method
.	O

Comparison	O
with	O
Other	O
Covariance	Method
Pooling	Method
Methods	Method
We	O
compare	O
iSQRT	Method
-	Method
COV	Method
with	O
other	O
covariance	Method
pooling	Method
methods	Method
,	O
as	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

The	O
results	O
of	O
MPN	Method
-	Method
COV	Method
,	O
B	Method
-	Method
CNN	Method
and	O
DeepO	Method
P	Method
are	O
duplicated	O
from	O
.	O

We	O
train	O
from	O
scratch	O
G	Method
DeNet	Method
and	O
improved	O
B	Method
-	Method
CNN	Method
on	O
ImageNet	Material
.	O

We	O
use	O
the	O
most	O
efficient	O
implementation	O
of	O
improved	O
B	Method
-	Method
CNN	Method
,	O
i.e.	O
,	O
FP	Method
by	O
SVD	Method
and	O
BP	Method
by	O
Lyap	Method
.	O

,	O
and	O
we	O
mention	O
all	O
implementations	O
of	O
improved	O
B	Method
-	Method
CNN	Method
produce	O
similar	O
results	O
.	O

Our	O
iSQRT	Method
-	Method
COV	Method
using	O
pre	Method
-	Method
normalization	Method
by	O
trace	Method
is	O
marginally	O
better	O
than	O
MPN	Method
-	Method
COV	Method
.	O

All	O
matrix	Method
square	Method
root	Method
normalization	Method
methods	Method
except	O
improved	O
B	Method
-	Method
CNN	Method
outperform	O
B	Method
-	Method
CNN	Method
and	O
DeepO	Method
P.	Method
Since	O
improved	O
B	Method
-	Method
CNN	Method
is	O
identical	O
to	O
MPN	Method
-	Method
COV	Method
if	O
element	Method
-	Method
wise	Method
square	Method
root	Method
normalization	Method
and	O
normalization	Method
are	O
neglected	O
,	O
its	O
unsatisfactory	O
performance	O
suggests	O
that	O
,	O
after	O
matrix	Method
square	Method
root	Method
normalization	Method
,	O
further	O
element	Method
-	Method
wise	Method
square	Method
root	Method
normalization	Method
and	O
normalization	Method
hurt	O
large	Task
-	Task
scale	Task
ImageNet	Task
classification	Task
.	O

This	O
is	O
consistent	O
with	O
the	O
observation	O
in	O
,	O
where	O
after	O
matrix	Method
power	Method
normalization	Method
,	O
additional	O
normalization	Method
by	O
Frobenius	Method
norm	Method
or	O
matrix	Method
norm	Method
makes	O
performance	O
decline	O
.	O

subsection	O
:	O
Results	O
on	O
ImageNet	Method
with	Method
ResNet	Method
Architecture	Method
This	O
section	O
evaluates	O
iSQRT	Method
-	Method
COV	Method
with	O
ResNet	Method
architecture	Method
.	O

We	O
follow	O
for	O
color	Task
augmentation	Task
and	O
weight	Task
initialization	Task
.	O

We	O
rescale	O
each	O
training	O
image	O
with	O
its	O
shorter	O
side	O
randomly	O
sampled	O
on	O
.	O

The	O
fixed	O
-	O
size	O
patch	O
is	O
randomly	O
cropped	O
from	O
the	O
rescaled	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O

We	O
rescale	O
each	O
test	O
image	O
with	O
a	O
shorter	O
side	O
of	O
256	O
and	O
evaluate	O
a	O
single	O
center	O
crop	O
for	O
inference	Task
.	O

We	O
use	O
SGD	Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
256	O
,	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

We	O
train	O
iSQRT	Method
-	Method
COV	Method
networks	O
from	O
scratch	O
in	O
60	O
epochs	O
,	O
initializing	O
the	O
learning	Metric
rate	Metric
to	O
which	O
is	O
divided	O
by	O
10	O
at	O
epoch	O
30	O
and	O
45	O
,	O
respectively	O
.	O

Significance	O
of	O
Post	Method
-	Method
compensation	Method
Rather	O
than	O
our	O
post	Method
-	Method
compensation	Method
scheme	Method
,	O
one	O
may	O
choose	O
Batch	Method
Normalization	Method
(	O
BN	Method
)	O
or	O
simply	O
do	O
nothing	O
(	O
i.e.	O
,	O
without	O
post	Method
-	Method
compensation	Method
)	O
.	O

He	O
et	O
al	O
.	O

FBN	Method
SORT	Method
MPN	O
-	O
COV	O
iSQRT	Method
-	Method
COV	Method
He	O
et	O
al	O
.	O

iSQRT	Method
-	Method
COV	Method
He	O
et	O
al	O
.	O

Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
impact	O
of	O
different	O
schemes	O
on	O
iSQRT	Method
-	Method
COV	Method
network	O
with	O
ResNet	Method
-	Method
50	Method
architecture	Method
.	O

Without	O
post	Method
-	Method
compensation	Method
,	O
iSQRT	Method
-	Method
COV	Method
network	O
fails	O
to	O
converge	O
.	O

Careful	O
observations	O
show	O
that	O
in	O
this	O
case	O
the	O
gradients	O
are	O
very	O
small	O
(	O
on	O
the	O
order	O
of	O
)	O
,	O
and	O
largely	O
tuning	O
of	O
learning	Metric
rate	Metric
helps	O
little	O
.	O

Option	O
of	O
BN	Method
helps	O
the	O
network	O
converge	O
,	O
but	O
producing	O
about	O
1	O
%	O
higher	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
than	O
our	O
post	Method
-	Method
compensation	Method
scheme	Method
.	O

The	O
comparison	O
above	O
suggests	O
that	O
our	O
post	Method
-	Method
compensation	Method
scheme	Method
is	O
essential	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

paragraph	O
:	O
Fast	O
Convergence	O
of	O
iSQRT	Method
-	Method
COV	Method
Network	O
We	O
compare	O
convergence	O
of	O
iSQRT	Method
-	Method
COV	Method
and	O
MPN	O
-	O
COV	O
with	O
ResNet	Method
-	Method
50	Method
architecture	Method
,	O
as	O
well	O
as	O
the	O
original	O
ResNet	Method
-	Method
50	Method
in	O
which	O
global	Method
average	Method
pooling	Method
is	O
performed	O
after	O
the	O
last	O
convolution	O
layer	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
presents	O
the	O
convergence	O
curves	O
.	O

Compared	O
to	O
the	O
original	O
ResNet	Method
-	Method
50	Method
,	O
the	O
convergence	Metric
of	O
both	O
iSQRT	Method
-	Method
COV	Method
and	O
MPN	Method
-	Method
COV	Method
is	O
significantly	O
faster	O
.	O

We	O
observe	O
that	O
iSQRT	Method
-	Method
COV	Method
can	O
converge	O
well	O
within	O
60	O
epochs	O
,	O
achieving	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
of	O
22.14	O
%	O
,	O
0.6	O
%	O
lower	O
than	O
MPN	Method
-	Method
COV	Method
.	O

We	O
also	O
trained	O
iSQRT	Method
-	Method
COV	Method
with	O
90	O
epochs	O
using	O
same	O
setting	O
with	O
MPN	Method
-	Method
COV	Method
,	O
obtaining	O
top	Metric
-	Metric
5	Metric
error	Metric
of	O
6.12	O
%	O
,	O
slightly	O
lower	O
than	O
that	O
with	O
60	O
epochs	O
(	O
6.22	O
%	O
)	O
.	O

This	O
indicates	O
iSQRT	Method
-	Method
COV	Method
can	O
converge	O
in	O
less	O
epochs	O
,	O
so	O
further	O
accelerating	Task
training	Task
,	O
as	O
opposed	O
to	O
MPN	Method
-	Method
COV	Method
.	O

The	O
fast	Metric
convergence	Metric
property	Metric
of	O
iSQRT	Method
-	Method
COV	Method
is	O
appealing	O
.	O

As	O
far	O
as	O
we	O
know	O
,	O
previous	O
networks	O
with	O
ResNet	Method
-	Method
50	Method
architecture	Method
require	O
at	O
least	O
90	O
epochs	O
to	O
converge	O
to	O
competitive	O
results	O
.	O

Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
arts	O
In	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
compare	O
our	O
method	O
with	O
other	O
second	Method
-	Method
order	Method
networks	Method
,	O
as	O
well	O
as	O
the	O
original	O
ResNets	Method
.	O

With	O
ResNet	Method
-	Method
50	Method
architecture	Method
,	O
all	O
the	O
second	Method
-	Method
order	Method
networks	Method
improve	O
over	O
the	O
first	O
-	O
order	O
one	O
while	O
our	O
method	O
performing	O
best	O
.	O

MPN	Method
-	Method
COV	Method
and	O
iSQRT	Method
-	Method
COV	Method
,	O
both	O
of	O
which	O
involve	O
square	Method
root	Method
normalization	Method
,	O
are	O
superior	O
to	O
FBN	Method
which	O
uses	O
no	O
normalization	O
and	O
SORT	Method
which	O
introduces	O
dot	Method
product	Method
transform	Method
in	O
the	O
linear	Method
sum	Method
of	Method
two	Method
-	Method
branch	Method
module	Method
followed	O
by	O
element	Method
-	Method
wise	Method
normalization	Method
.	O

Moreover	O
,	O
our	O
iSQRT	Method
-	Method
COV	Method
outperforms	O
MPN	Method
-	Method
COV	Method
by	O
0.6	O
%	O
in	O
top	Metric
-	Metric
1	Metric
error	Metric
.	O

Note	O
that	O
our	O
50	O
-	O
layer	O
iSQRT	Method
-	Method
COV	Method
network	O
achieves	O
lower	O
error	Metric
rate	Metric
than	O
much	O
deeper	O
ResNet	Method
-	Method
101	Method
and	O
ResNet	Method
-	Method
152	Method
,	O
while	O
our	O
101	O
-	O
layer	O
iSQRT	Method
-	Method
COV	Method
network	O
outperforming	O
the	O
original	O
ResNet	Method
-	Method
101	Method
by	O
2.4	O
%	O
and	O
ResNet	Method
-	Method
152	Method
by	O
1.8	O
%	O
,	O
respectively	O
.	O

Why	O
Approximate	Method
Square	Method
Root	Method
Performs	O
Better	O
Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
more	O
iterations	O
which	O
lead	O
to	O
more	O
accurate	O
square	O
root	O
is	O
not	O
helpful	O
for	O
iSQRT	Method
-	Method
COV	Method
with	O
AlexNet	Method
.	O

From	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
observe	O
that	O
iSQRT	Method
-	Method
COV	Method
with	O
ResNet	Method
computing	Method
approximate	Method
square	Method
root	Method
performs	O
better	O
than	O
MPN	Method
-	Method
COV	Method
which	O
can	O
obtain	O
exact	O
square	O
root	O
by	O
EIG	Method
.	O

Recall	O
that	O
,	O
for	O
covariance	Task
pooling	Task
ConvNets	Task
,	O
we	O
face	O
the	O
problem	O
of	O
small	O
sample	O
of	O
large	O
dimensionality	O
,	O
and	O
matrix	Method
square	Method
root	Method
is	O
consistent	O
with	O
general	O
shrinkage	Method
principle	Method
of	Method
robust	Method
covariance	Method
estimation	Method
.	O

Hence	O
,	O
we	O
conjuncture	O
that	O
approximate	Method
matrix	Method
square	Method
root	Method
may	O
be	O
a	O
better	O
robust	Method
covariance	Method
estimator	Method
than	O
the	O
exact	O
square	Method
root	Method
.	O

Despite	O
this	O
analysis	O
,	O
we	O
think	O
this	O
problem	O
is	O
worth	O
future	O
research	O
.	O

Compactness	O
of	O
iSQRT	Method
-	Method
COV	Method
Our	O
iSQRT	Method
-	Method
COV	Method
outputs	O
32k	Method
-	Method
dimensional	Method
representation	Method
which	O
is	O
high	O
.	O

Here	O
we	O
consider	O
to	O
compress	O
this	O
representation	O
.	O

Compactness	Method
by	O
PCA	Method
is	O
not	O
viable	O
since	O
obtaining	O
the	O
principal	Method
components	Method
on	O
ImageNet	Material
is	O
too	O
expensive	O
.	O

CBP	Method
is	O
not	O
applicable	O
to	O
our	O
iSQRT	Method
-	Method
COV	Method
as	O
well	O
,	O
as	O
it	O
does	O
not	O
explicitly	O
estimate	O
the	O
covariance	O
matrix	O
.	O

We	O
propose	O
a	O
simple	O
scheme	O
,	O
which	O
decreases	O
the	O
dimension	Metric
(	O
dim	O
.	O

)	O
of	O
covariance	Method
representation	Method
by	O
lowering	O
the	O
number	O
of	O
channels	O
of	O
convolutional	Method
layer	Method
before	O
our	O
covariance	Method
pooling	Method
.	O

Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
results	O
of	O
compact	O
iSQRT	Method
-	Method
COV	Method
.	O

The	O
recognition	Metric
error	Metric
increases	O
slightly	O
(	O
)	O
when	O
decreases	O
from	O
256	O
to	O
128	O
(	O
correspondingly	O
,	O
dim	O
.	O

of	O
image	Method
representation	Method
)	O
.	O

The	O
error	Metric
rate	Metric
is	O
23.73	O
if	O
the	O
dimension	O
is	O
compressed	O
to	O
2	O
K	O
,	O
still	O
outperforming	O
the	O
original	O
ResNet	Method
-	Method
50	Method
which	O
performs	O
global	Method
average	Method
pooling	Method
.	O

subsection	O
:	O
Fine	Task
-	Task
grained	Task
Visual	Task
Categorization	Task
(	O
FGVC	Method
)	O
Finally	O
,	O
we	O
apply	O
iSQRT	Method
-	Method
COV	Method
models	O
pre	O
-	O
trained	O
on	O
ImageNet	Method
to	O
FGVC	Method
.	O

For	O
fair	O
comparison	O
,	O
we	O
follow	O
for	O
experimental	O
setting	O
and	O
evaluation	O
protocol	O
.	O

On	O
all	O
datasets	O
,	O
we	O
crop	O
patches	O
as	O
input	O
images	O
.	O

We	O
replace	O
1000	Method
-	Method
way	Method
softmax	Method
layer	Method
of	O
a	O
pre	O
-	O
trained	O
iSQRT	Method
-	Method
COV	Method
model	O
by	O
a	O
k	Method
-	Method
way	Method
softmax	Method
layer	Method
,	O
where	O
is	O
number	O
of	O
classes	O
in	O
the	O
fine	O
-	O
grained	O
dataset	O
,	O
and	O
finetune	O
the	O
network	O
using	O
SGD	Method
with	O
momentum	O
of	O
0.9	O
for	O
50	O
100	O
epochs	O
with	O
a	O
small	O
learning	Metric
rate	Metric
(	O
)	O
for	O
all	O
layers	O
except	O
the	O
fully	O
-	O
connected	O
layer	O
,	O
which	O
is	O
set	O
to	O
.	O

We	O
use	O
horizontal	O
flipping	O
as	O
data	Task
augmentation	Task
.	O

After	O
finetuning	O
,	O
the	O
outputs	O
of	O
iSQRT	Method
-	Method
COV	Method
layer	O
are	O
normalized	O
before	O
inputted	O
to	O
train	O
one	Method
-	Method
vs	Method
-	Method
all	Method
linear	Method
SVMs	Method
with	O
hyperparameter	Method
.	O

We	O
predict	O
the	O
label	O
of	O
a	O
test	O
image	O
by	O
averaging	O
SVM	O
scores	O
of	O
the	O
image	O
and	O
its	O
horizontal	O
flip	O
.	O

ResNet	O
-	O
50	O
iSQRT	Method
-	Method
COV	Method
VGG	O
-	O
D	O
Improved	O
B	O
-	O
CNN	O
Tab	O
.	O

[	O
reference	O
]	O
presents	O
classification	Task
results	O
of	O
different	O
methods	O
,	O
where	O
column	O
3	O
lists	O
the	O
dimension	O
of	O
the	O
corresponding	O
representation	O
.	O

With	O
ResNet	Method
-	Method
50	Method
architecture	Method
,	O
KP	Method
performs	O
much	O
better	O
than	O
CBP	Method
,	O
while	O
iSQRT	Method
-	Method
COV	Method
(	O
8	O
K	O
)	O
respectively	O
outperforms	O
KP	Method
(	O
14	O
K	O
)	O
by	O
about	O
2.6	O
%	O
,	O
3.8	O
%	O
and	O
0.6	O
%	O
on	O
Birds	Material
,	O
Aircrafts	Material
and	O
Cars	Material
,	O
and	O
iSQRT	Method
-	Method
COV	Method
(	O
32	O
K	O
)	O
further	O
improves	O
accuracy	Metric
.	O

Note	O
that	O
KP	Method
combines	O
first	O
-	O
order	O
up	O
to	O
fourth	O
-	O
order	O
statistics	O
while	O
iSQRT	Method
-	Method
COV	Method
only	O
exploits	O
second	O
-	O
order	O
one	O
.	O

With	O
VGG	Method
-	Method
D	Method
,	O
iSQRT	Method
-	Method
COV	Method
(	O
32k	O
)	O
matches	O
or	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
competitors	O
,	O
but	O
inferior	O
to	O
iSQRT	Method
-	Method
COV	Method
(	O
32k	O
)	O
with	O
ResNet	Method
-	Method
50	Method
.	O

On	O
all	O
fine	O
-	O
grained	O
datasets	O
,	O
KP	Method
and	O
CBP	Method
with	O
16	Method
-	Method
layer	Method
VGG	Method
-	Method
D	Method
perform	O
better	O
than	O
their	O
counterparts	O
with	O
50	Method
-	Method
layer	Method
ResNet	Method
,	O
despite	O
the	O
fact	O
that	O
ResNet	Method
-	Method
50	Method
significantly	O
outperforms	O
VGG	Method
-	Method
D	Method
on	O
ImageNet	Material
.	O

The	O
reason	O
may	O
be	O
that	O
the	O
last	O
convolution	Method
layer	Method
of	O
pre	O
-	O
trained	O
ResNet	Method
-	Method
50	Method
outputs	O
2048	O
-	O
dimensional	O
features	O
,	O
much	O
higher	O
than	O
512	Method
-	Method
dimensional	Method
one	Method
of	Method
VGG	Method
-	Method
D	Method
,	O
which	O
are	O
not	O
suitable	O
for	O
existing	O
second	Method
-	Method
or	Method
higher	Method
-	Method
order	Method
pooling	Method
methods	Method
.	O

Different	O
from	O
all	O
existing	O
methods	O
which	O
use	O
models	O
pre	O
-	O
trained	O
on	O
ImageNet	Material
with	O
first	O
-	O
order	O
information	O
,	O
our	O
pre	O
-	O
trained	Method
models	Method
are	O
of	O
second	O
-	O
order	O
.	O

Using	O
pre	O
-	O
trained	O
iSQRT	Method
-	Method
COV	Method
models	O
with	O
ResNet	Method
-	Method
50	Method
,	O
we	O
achieve	O
recognition	Task
results	O
superior	O
to	O
all	O
the	O
compared	O
methods	O
,	O
and	O
furthermore	O
,	O
establish	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
three	O
fine	O
-	O
grained	O
benchmarks	O
using	O
iSQRT	Method
-	Method
COV	Method
model	O
with	O
ResNet	Method
-	Method
101	Method
.	O

section	O
:	O
Conclusion	O
We	O
presented	O
an	O
iterative	Method
matrix	Method
square	Method
root	Method
normalization	Method
of	Method
covariance	Method
pooling	Method
(	O
iSQRT	Method
-	Method
COV	Method
)	O
network	O
which	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Compared	O
to	O
existing	O
works	O
depending	O
heavily	O
on	O
GPU	O
unfriendly	O
EIG	Method
or	O
SVD	Method
,	O
our	O
method	O
,	O
based	O
on	O
coupled	Method
Newton	Method
-	Method
Schulz	Method
iteration	Method
,	O
runs	O
much	O
faster	O
as	O
it	O
involves	O
only	O
matrix	Method
multiplications	Method
,	O
suitable	O
for	O
parallel	Task
implementation	Task
on	O
GPU	O
.	O

We	O
validated	O
our	O
method	O
on	O
both	O
large	Material
-	Material
scale	Material
ImageNet	Material
dataset	Material
and	O
challenging	O
fine	O
-	O
grained	O
benchmarks	O
.	O

Given	O
efficiency	O
and	O
promising	O
performance	O
of	O
our	O
iSQRT	Method
-	Method
COV	Method
,	O
we	O
hope	O
global	Method
covariance	Method
pooling	Method
will	O
be	O
a	O
promising	O
alternative	O
to	O
global	Method
average	Method
pooling	Method
in	O
other	O
deep	Method
network	Method
architectures	Method
,	O
e.g.	O
,	O
ResNeXt	Method
,	O
Inception	Method
and	O
DenseNet	Method
.	O

bibliography	O
:	O
References	O
document	O
:	O
Abstractive	Task
Text	Task
Summarization	Task
using	O
Sequence	Method
-	Method
to	Method
-	Method
sequence	Method
RNNs	Method
and	O
Beyond	O
In	O
this	O
work	O
,	O
we	O
model	O
abstractive	Task
text	Task
summarization	Task
using	O
Attentional	Method
Encoder	Method
-	Method
Decoder	Method
Recurrent	Method
Neural	Method
Networks	Method
,	O
and	O
show	O
that	O
they	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
two	O
different	O
corpora	O
.	O

We	O
propose	O
several	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	Task
that	O
are	O
not	O
adequately	O
modeled	O
by	O
the	O
basic	O
architecture	O
,	O
such	O
as	O
modeling	O
key	O
-	O
words	O
,	O
capturing	O
the	O
hierarchy	O
of	O
sentence	O
-	O
to	O
-	O
word	O
structure	O
,	O
and	O
emitting	O
words	O
that	O
are	O
rare	O
or	O
unseen	O
at	O
training	O
time	O
.	O

Our	O
work	O
shows	O
that	O
many	O
of	O
our	O
proposed	O
models	O
contribute	O
to	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
consisting	O
of	O
multi	O
-	O
sentence	O
summaries	O
,	O
and	O
establish	O
performance	O
benchmarks	O
for	O
further	O
research	O
.	O

section	O
:	O
Introduction	O
Abstractive	Task
text	Task
summarization	Task
is	O
the	O
task	O
of	O
generating	O
a	O
headline	Task
or	O
a	O
short	O
summary	O
consisting	O
of	O
a	O
few	O
sentences	O
that	O
captures	O
the	O
salient	O
ideas	O
of	O
an	O
article	O
or	O
a	O
passage	O
.	O

We	O
use	O
the	O
adjective	O
‘	O
abstractive	O
’	O
to	O
denote	O
a	O
summary	O
that	O
is	O
not	O
a	O
mere	O
selection	O
of	O
a	O
few	O
existing	O
passages	O
or	O
sentences	O
extracted	O
from	O
the	O
source	O
,	O
but	O
a	O
compressed	O
paraphrasing	O
of	O
the	O
main	O
contents	O
of	O
the	O
document	O
,	O
potentially	O
using	O
vocabulary	O
unseen	O
in	O
the	O
source	O
document	O
.	O

This	O
task	O
can	O
also	O
be	O
naturally	O
cast	O
as	O
mapping	O
an	O
input	O
sequence	O
of	O
words	O
in	O
a	O
source	O
document	O
to	O
a	O
target	O
sequence	O
of	O
words	O
called	O
summary	O
.	O

In	O
the	O
recent	O
past	O
,	O
deep	Method
-	Method
learning	Method
based	Method
models	Method
that	O
map	O
an	O
input	O
sequence	O
into	O
another	O
output	O
sequence	O
,	O
called	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
,	O
have	O
been	O
successful	O
in	O
many	O
problems	O
such	O
as	O
machine	Task
translation	Task
,	O
speech	Task
recognition	Task
and	O
video	Task
captioning	Task
.	O

In	O
the	O
framework	O
of	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
,	O
a	O
very	O
relevant	O
model	O
to	O
our	O
task	O
is	O
the	O
attentional	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
encoder	Method
-	Method
decoder	Method
model	Method
proposed	O
in	O
nmt	Method
,	O
which	O
has	O
produced	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
machine	Task
translation	Task
(	O
MT	Task
)	O
,	O
which	O
is	O
also	O
a	O
natural	Task
language	Task
task	Task
.	O

Despite	O
the	O
similarities	O
,	O
abstractive	Task
summarization	Task
is	O
a	O
very	O
different	O
problem	O
from	O
MT	Task
.	O

Unlike	O
in	O
MT	Task
,	O
the	O
target	O
(	O
summary	O
)	O
is	O
typically	O
very	O
short	O
and	O
does	O
not	O
depend	O
very	O
much	O
on	O
the	O
length	O
of	O
the	O
source	O
(	O
document	O
)	O
in	O
summarization	Task
.	O

Additionally	O
,	O
a	O
key	O
challenge	O
in	O
summarization	Task
is	O
to	O
optimally	O
compress	O
the	O
original	O
document	O
in	O
a	O
lossy	O
manner	O
such	O
that	O
the	O
key	O
concepts	O
in	O
the	O
original	O
document	O
are	O
preserved	O
,	O
whereas	O
in	O
MT	Task
,	O
the	O
translation	Task
is	O
expected	O
to	O
be	O
loss	O
-	O
less	O
.	O

In	O
translation	Task
,	O
there	O
is	O
a	O
strong	O
notion	O
of	O
almost	O
one	O
-	O
to	O
-	O
one	O
word	O
-	O
level	O
alignment	O
between	O
source	O
and	O
target	O
,	O
but	O
in	O
summarization	Task
,	O
it	O
is	O
less	O
obvious	O
.	O

We	O
make	O
the	O
following	O
main	O
contributions	O
in	O
this	O
work	O
:	O
(	O
i	O
)	O
We	O
apply	O
the	O
off	O
-	O
the	O
-	O
shelf	O
attentional	O
encoder	O
-	O
decoder	O
RNN	Method
that	O
was	O
originally	O
developed	O
for	O
machine	Task
translation	Task
to	O
summarization	Task
,	O
and	O
show	O
that	O
it	O
already	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
on	O
two	O
different	O
English	Material
corpora	Material
.	O

(	O
ii	O
)	O
Motivated	O
by	O
concrete	O
problems	O
in	O
summarization	Task
that	O
are	O
not	O
sufficiently	O
addressed	O
by	O
the	O
machine	Task
translation	Task
based	O
model	O
,	O
we	O
propose	O
novel	O
models	O
and	O
show	O
that	O
they	O
provide	O
additional	O
improvement	O
in	O
performance	O
.	O

(	O
iii	O
)	O
We	O
propose	O
a	O
new	O
dataset	O
for	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
of	O
a	O
document	O
into	O
multiple	O
sentences	O
and	O
establish	O
benchmarks	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
describe	O
each	O
specific	O
problem	O
in	O
abstractive	Task
summarization	Task
that	O
we	O
aim	O
to	O
solve	O
,	O
and	O
present	O
a	O
novel	O
model	O
that	O
addresses	O
it	O
.	O

Section	O
[	O
reference	O
]	O
contextualizes	O
our	O
models	O
with	O
respect	O
to	O
closely	O
related	O
work	O
on	O
the	O
topic	O
of	O
abstractive	Task
text	Task
summarization	Task
.	O

We	O
present	O
the	O
results	O
of	O
our	O
experiments	O
on	O
three	O
different	O
data	O
sets	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
also	O
present	O
some	O
qualitative	O
analysis	O
of	O
the	O
output	O
from	O
our	O
models	O
in	O
Section	O
[	O
reference	O
]	O
before	O
concluding	O
the	O
paper	O
with	O
remarks	O
on	O
our	O
future	O
direction	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Models	O
In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
basic	O
encoder	O
-	O
decoder	O
RNN	Method
that	O
serves	O
as	O
our	O
baseline	O
and	O
then	O
propose	O
several	O
novel	O
models	O
for	O
summarization	Task
,	O
each	O
addressing	O
a	O
specific	O
weakness	O
in	O
the	O
baseline	O
.	O

subsection	O
:	O
Encoder	O
-	O
Decoder	O
RNN	Method
with	O
Attention	Method
and	O
Large	Task
Vocabulary	Task
Trick	Task
Our	O
baseline	O
model	O
corresponds	O
to	O
the	O
neural	O
machine	Task
translation	Task
model	O
used	O
in	O
nmt	Method
.	O

The	O
encoder	Method
consists	O
of	O
a	O
bidirectional	O
GRU	O
-	O
RNN	Method
,	O
while	O
the	O
decoder	Method
consists	O
of	O
a	O
uni	O
-	O
directional	O
GRU	O
-	O
RNN	Method
with	O
the	O
same	O
hidden	O
-	O
state	O
size	O
as	O
that	O
of	O
the	O
encoder	O
,	O
and	O
an	O
attention	Method
mechanism	Method
over	O
the	O
source	O
-	O
hidden	O
states	O
and	O
a	O
soft	Method
-	Method
max	Method
layer	Method
over	O
target	O
vocabulary	O
to	O
generate	O
words	O
.	O

In	O
the	O
interest	O
of	O
space	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
original	O
paper	O
for	O
a	O
detailed	O
treatment	O
of	O
this	O
model	O
.	O

In	O
addition	O
to	O
the	O
basic	O
model	O
,	O
we	O
also	O
adapted	O
to	O
the	O
summarization	Task
problem	Task
,	O
the	O
large	Task
vocabulary	Task
‘	O
trick	O
’	O
(	O
LVT	Method
)	O
described	O
in	O
lvt	Method
.	O

In	O
our	O
approach	O
,	O
the	O
decoder	O
-	O
vocabulary	O
of	O
each	O
mini	O
-	O
batch	O
is	O
restricted	O
to	O
words	O
in	O
the	O
source	O
documents	O
of	O
that	O
batch	O
.	O

In	O
addition	O
,	O
the	O
most	O
frequent	O
words	O
in	O
the	O
target	O
dictionary	O
are	O
added	O
until	O
the	O
vocabulary	O
reaches	O
a	O
fixed	O
size	O
.	O

The	O
aim	O
of	O
this	O
technique	O
is	O
to	O
reduce	O
the	O
size	O
of	O
the	O
soft	Method
-	Method
max	Method
layer	Method
of	O
the	O
decoder	Method
which	O
is	O
the	O
main	O
computational	Metric
bottleneck	Metric
.	O

In	O
addition	O
,	O
this	O
technique	O
also	O
speeds	O
up	O
convergence	Task
by	O
focusing	O
the	O
modeling	O
effort	O
only	O
on	O
the	O
words	O
that	O
are	O
essential	O
to	O
a	O
given	O
example	O
.	O

This	O
technique	O
is	O
particularly	O
well	O
suited	O
to	O
summarization	Task
since	O
a	O
large	O
proportion	O
of	O
the	O
words	O
in	O
the	O
summary	O
come	O
from	O
the	O
source	O
document	O
in	O
any	O
case	O
.	O

subsection	O
:	O
Capturing	Task
Keywords	Task
using	O
Feature	Method
-	Method
rich	Method
Encoder	Method
In	O
summarization	Task
,	O
one	O
of	O
the	O
key	O
challenges	O
is	O
to	O
identify	O
the	O
key	O
concepts	O
and	O
key	O
entities	O
in	O
the	O
document	O
,	O
around	O
which	O
the	O
story	O
revolves	O
.	O

In	O
order	O
to	O
accomplish	O
this	O
goal	O
,	O
we	O
may	O
need	O
to	O
go	O
beyond	O
the	O
word	Method
-	Method
embeddings	Method
-	Method
based	Method
representation	Method
of	O
the	O
input	O
document	O
and	O
capture	O
additional	O
linguistic	O
features	O
such	O
as	O
parts	O
-	O
of	O
-	O
speech	O
tags	O
,	O
named	O
-	O
entity	O
tags	O
,	O
and	O
TF	O
and	O
IDF	O
statistics	O
of	O
the	O
words	O
.	O

We	O
therefore	O
create	O
additional	O
look	Method
-	Method
up	Method
based	Method
embedding	Method
matrices	Method
for	O
the	O
vocabulary	O
of	O
each	O
tag	O
-	O
type	O
,	O
similar	O
to	O
the	O
embeddings	O
for	O
words	O
.	O

For	O
continuous	O
features	O
such	O
as	O
TF	O
and	O
IDF	Method
,	O
we	O
convert	O
them	O
into	O
categorical	O
values	O
by	O
discretizing	O
them	O
into	O
a	O
fixed	O
number	O
of	O
bins	O
,	O
and	O
use	O
one	Method
-	Method
hot	Method
representations	Method
to	O
indicate	O
the	O
bin	O
number	O
they	O
fall	O
into	O
.	O

This	O
allows	O
us	O
to	O
map	O
them	O
into	O
an	O
embeddings	O
matrix	O
like	O
any	O
other	O
tag	O
-	O
type	O
.	O

Finally	O
,	O
for	O
each	O
word	O
in	O
the	O
source	O
document	O
,	O
we	O
simply	O
look	O
-	O
up	O
its	O
embeddings	O
from	O
all	O
of	O
its	O
associated	O
tags	O
and	O
concatenate	O
them	O
into	O
a	O
single	O
long	O
vector	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

On	O
the	O
target	O
side	O
,	O
we	O
continue	O
to	O
use	O
only	O
word	Method
-	Method
based	Method
embeddings	Method
as	O
the	O
representation	O
.	O

subsection	O
:	O
Modeling	Task
Rare	Task
/	Task
Unseen	Task
Words	Task
using	O
Switching	Method
Generator	Method
-	Method
Pointer	Method
Often	O
-	O
times	O
in	O
summarization	Task
,	O
the	O
keywords	O
or	O
named	O
-	O
entities	O
in	O
a	O
test	O
document	O
that	O
are	O
central	O
to	O
the	O
summary	O
may	O
actually	O
be	O
unseen	O
or	O
rare	O
with	O
respect	O
to	O
training	O
data	O
.	O

Since	O
the	O
vocabulary	O
of	O
the	O
decoder	O
is	O
fixed	O
at	O
training	O
time	O
,	O
it	O
can	O
not	O
emit	O
these	O
unseen	O
words	O
.	O

Instead	O
,	O
a	O
most	O
common	O
way	O
of	O
handling	O
these	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
is	O
to	O
emit	O
an	O
‘	O
UNK	O
’	O
token	O
as	O
a	O
placeholder	O
.	O

However	O
this	O
does	O
not	O
result	O
in	O
legible	O
summaries	O
.	O

In	O
summarization	Task
,	O
an	O
intuitive	O
way	O
to	O
handle	O
such	O
OOV	Material
words	Material
is	O
to	O
simply	O
point	O
to	O
their	O
location	O
in	O
the	O
source	O
document	O
instead	O
.	O

We	O
model	O
this	O
notion	O
using	O
our	O
novel	O
switching	Method
decoder	Method
/	O
pointer	Method
architecture	Method
which	O
is	O
graphically	O
represented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
this	O
model	O
,	O
the	O
decoder	Method
is	O
equipped	O
with	O
a	O
‘	O
switch	O
’	O
that	O
decides	O
between	O
using	O
the	O
generator	O
or	O
a	O
pointer	Method
at	O
every	O
time	O
-	O
step	O
.	O

If	O
the	O
switch	O
is	O
turned	O
on	O
,	O
the	O
decoder	O
produces	O
a	O
word	O
from	O
its	O
target	O
vocabulary	O
in	O
the	O
normal	O
fashion	O
.	O

However	O
,	O
if	O
the	O
switch	O
is	O
turned	O
off	O
,	O
the	O
decoder	Method
instead	O
generates	O
a	O
pointer	Method
to	O
one	O
of	O
the	O
word	O
-	O
positions	O
in	O
the	O
source	O
.	O

The	O
word	O
at	O
the	O
pointer	Method
-	O
location	O
is	O
then	O
copied	O
into	O
the	O
summary	O
.	O

The	O
switch	Method
is	O
modeled	O
as	O
a	O
sigmoid	Method
activation	Method
function	Method
over	O
a	O
linear	Method
layer	Method
based	O
on	O
the	O
entire	O
available	O
context	O
at	O
each	O
time	O
-	O
step	O
as	O
shown	O
below	O
.	O

where	O
is	O
the	O
probability	O
of	O
the	O
switch	O
turning	O
on	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	O
,	O
is	O
the	O
hidden	O
state	O
,	O
is	O
the	O
embedding	O
vector	O
of	O
the	O
emission	O
from	O
the	O
previous	O
time	O
step	O
,	O
is	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
,	O
and	O
and	O
are	O
the	O
switch	O
parameters	O
.	O

We	O
use	O
attention	O
distribution	O
over	O
word	O
positions	O
in	O
the	O
document	O
as	O
the	O
distribution	O
to	O
sample	O
the	O
pointer	Method
from	O
.	O

In	O
the	O
above	O
equation	O
,	O
is	O
the	O
pointer	Method
value	O
at	O
word	O
-	O
position	O
in	O
the	O
summary	O
,	O
sampled	O
from	O
the	O
attention	Method
distribution	Method
over	O
the	O
document	O
word	O
-	O
positions	O
,	O
where	O
is	O
the	O
probability	O
of	O
the	O
time	O
-	O
step	O
in	O
the	O
decoder	O
pointing	O
to	O
the	O
position	O
in	O
the	O
document	O
,	O
and	O
is	O
the	O
encoder	O
’s	O
hidden	O
state	O
at	O
position	O
.	O

At	O
training	O
time	O
,	O
we	O
provide	O
the	O
model	O
with	O
explicit	O
pointer	Method
information	O
whenever	O
the	O
summary	O
word	O
does	O
not	O
exist	O
in	O
the	O
target	O
vocabulary	O
.	O

When	O
the	O
OOV	O
word	O
in	O
summary	O
occurs	O
in	O
multiple	O
document	O
positions	O
,	O
we	O
break	O
the	O
tie	O
in	O
favor	O
of	O
its	O
first	O
occurrence	O
.	O

At	O
training	O
time	O
,	O
we	O
optimize	O
the	O
conditional	O
log	O
-	O
likelihood	O
shown	O
below	O
,	O
with	O
additional	O
regularization	O
penalties	O
.	O

where	O
and	O
are	O
the	O
summary	O
and	O
document	O
words	O
respectively	O
,	O
is	O
an	O
indicator	O
function	O
that	O
is	O
set	O
to	O
0	O
whenever	O
the	O
word	O
at	O
position	O
in	O
the	O
summary	O
is	O
OOV	O
with	O
respect	O
to	O
the	O
decoder	O
vocabulary	O
.	O

At	O
test	O
time	O
,	O
the	O
model	O
decides	O
automatically	O
at	O
each	O
time	O
-	O
step	O
whether	O
to	O
generate	O
or	O
to	O
point	O
,	O
based	O
on	O
the	O
estimated	O
switch	O
probability	O
.	O

We	O
simply	O
use	O
the	O
of	O
the	O
posterior	O
probability	O
of	O
generation	O
or	O
pointing	O
to	O
generate	O
the	O
best	O
output	O
at	O
each	O
time	O
step	O
.	O

The	O
pointer	Method
mechanism	O
may	O
be	O
more	O
robust	O
in	O
handling	O
rare	Task
words	Task
because	O
it	O
uses	O
the	O
encoder	Method
’s	Method
hidden	Method
-	Method
state	Method
representation	Method
of	O
rare	O
words	O
to	O
decide	O
which	O
word	O
from	O
the	O
document	O
to	O
point	O
to	O
.	O

Since	O
the	O
hidden	O
state	O
depends	O
on	O
the	O
entire	O
context	O
of	O
the	O
word	O
,	O
the	O
model	O
is	O
able	O
to	O
accurately	O
point	O
to	O
unseen	O
words	O
although	O
they	O
do	O
not	O
appear	O
in	O
the	O
target	O
vocabulary	O
.	O

subsection	O
:	O
Capturing	Task
Hierarchical	Task
Document	Task
Structure	Task
with	O
Hierarchical	O
Attention	O
In	O
datasets	O
where	O
the	O
source	O
document	O
is	O
very	O
long	O
,	O
in	O
addition	O
to	O
identifying	O
the	O
keywords	O
in	O
the	O
document	O
,	O
it	O
is	O
also	O
important	O
to	O
identify	O
the	O
key	O
sentences	O
from	O
which	O
the	O
summary	O
can	O
be	O
drawn	O
.	O

This	O
model	O
aims	O
to	O
capture	O
this	O
notion	O
of	O
two	O
levels	O
of	O
importance	O
using	O
two	O
bi	Method
-	Method
directional	Method
RNNs	Method
on	O
the	O
source	O
side	O
,	O
one	O
at	O
the	O
word	O
level	O
and	O
the	O
other	O
at	O
the	O
sentence	O
level	O
.	O

The	O
attention	Method
mechanism	Method
operates	O
at	O
both	O
levels	O
simultaneously	O
.	O

The	O
word	O
-	O
level	O
attention	O
is	O
further	O
re	O
-	O
weighted	O
by	O
the	O
corresponding	O
sentence	O
-	O
level	O
attention	O
and	O
re	O
-	O
normalized	O
as	O
shown	O
below	O
:	O
where	O
is	O
the	O
word	O
-	O
level	O
attention	O
weight	O
at	O
position	O
of	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
ID	O
of	O
the	O
sentence	O
at	O
word	O
position	O
,	O
is	O
the	O
sentence	O
-	O
level	O
attention	O
weight	O
for	O
the	O
sentence	O
in	O
the	O
source	O
,	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
re	O
-	O
scaled	O
attention	O
at	O
the	O
word	O
position	O
.	O

The	O
re	O
-	O
scaled	O
attention	O
is	O
then	O
used	O
to	O
compute	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
that	O
goes	O
as	O
input	O
to	O
the	O
hidden	O
state	O
of	O
the	O
decoder	O
.	O

Further	O
,	O
we	O
also	O
concatenate	O
additional	O
positional	O
embeddings	O
to	O
the	O
hidden	O
state	O
of	O
the	O
sentence	O
-	O
level	O
RNN	Method
to	O
model	O
positional	O
importance	O
of	O
sentences	O
in	O
the	O
document	O
.	O

This	O
architecture	O
therefore	O
models	O
key	O
sentences	O
as	O
well	O
as	O
keywords	O
within	O
those	O
sentences	O
jointly	O
.	O

A	O
graphical	Method
representation	Method
of	O
this	O
model	O
is	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
A	O
vast	O
majority	O
of	O
past	O
work	O
in	O
summarization	Task
has	O
been	O
extractive	Task
,	O
which	O
consists	O
of	O
identifying	O
key	O
sentences	O
or	O
passages	O
in	O
the	O
source	O
document	O
and	O
reproducing	O
them	O
as	O
summary	O
.	O

Humans	O
on	O
the	O
other	O
hand	O
,	O
tend	O
to	O
paraphrase	O
the	O
original	O
story	O
in	O
their	O
own	O
words	O
.	O

As	O
such	O
,	O
human	O
summaries	O
are	O
abstractive	O
in	O
nature	O
and	O
seldom	O
consist	O
of	O
reproduction	O
of	O
original	O
sentences	O
from	O
the	O
document	O
.	O

The	O
task	O
of	O
abstractive	Task
summarization	Task
has	O
been	O
standardized	O
using	O
the	O
DUC	Material
-	Material
2003	Material
and	O
DUC	Material
-	Material
2004	Material
competitions	O
.	O

The	O
data	O
for	O
these	O
tasks	O
consists	O
of	O
news	O
stories	O
from	O
various	O
topics	O
with	O
multiple	O
reference	O
summaries	O
per	O
story	O
generated	O
by	O
humans	O
.	O

The	O
best	O
performing	O
system	O
on	O
the	O
DUC	Material
-	Material
2004	Material
task	O
,	O
called	O
TOPIARY	Method
,	O
used	O
a	O
combination	O
of	O
linguistically	Method
motivated	Method
compression	Method
techniques	Method
,	O
and	O
an	O
unsupervised	Method
topic	Method
detection	Method
algorithm	Method
that	O
appends	O
keywords	O
extracted	O
from	O
the	O
article	O
onto	O
the	O
compressed	O
output	O
.	O

Some	O
of	O
the	O
other	O
notable	O
work	O
in	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
includes	O
using	O
traditional	O
phrase	O
-	O
table	O
based	O
machine	Task
translation	Task
approaches	O
,	O
compression	Method
using	O
weighted	Method
tree	Method
-	Method
transformation	Method
rules	Method
and	O
quasi	Method
-	Method
synchronous	Method
grammar	Method
approaches	Method
.	O

With	O
the	O
emergence	O
of	O
deep	Method
learning	Method
as	O
a	O
viable	O
alternative	O
for	O
many	O
NLP	Task
tasks	Task
,	O
researchers	O
have	O
started	O
considering	O
this	O
framework	O
as	O
an	O
attractive	O
,	O
fully	O
data	O
-	O
driven	O
alternative	O
to	O
abstractive	Task
summarization	Task
.	O

In	O
namas	Method
,	O
the	O
authors	O
use	O
convolutional	Method
models	Method
to	O
encode	O
the	O
source	O
,	O
and	O
a	O
context	Method
-	Method
sensitive	Method
attentional	Method
feed	Method
-	Method
forward	Method
neural	Method
network	Method
to	O
generate	O
the	O
summary	O
,	O
producing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
Gigaword	Material
and	O
DUC	Material
datasets	Material
.	O

In	O
an	O
extension	O
to	O
this	O
work	O
,	O
chopra	O
used	O
a	O
similar	O
convolutional	Method
model	Method
for	O
the	O
encoder	Method
,	O
but	O
replaced	O
the	O
decoder	Method
with	O
an	O
RNN	Method
,	O
producing	O
further	O
improvement	O
in	O
performance	O
on	O
both	O
datasets	O
.	O

In	O
another	O
paper	O
that	O
is	O
closely	O
related	O
to	O
our	O
work	O
,	O
hu:2015:EMNLP	Method
introduce	O
a	O
large	O
dataset	O
for	O
Chinese	Task
short	Task
text	Task
summarization	Task
.	O

They	O
show	O
promising	O
results	O
on	O
their	O
Chinese	Material
dataset	Material
using	O
an	O
encoder	O
-	O
decoder	O
RNN	Method
,	O
but	O
do	O
not	O
report	O
experiments	O
on	O
English	Material
corpora	Material
.	O

In	O
another	O
very	O
recent	O
work	O
,	O
jianpeng	O
used	O
RNN	Method
based	O
encoder	O
-	O
decoder	O
for	O
extractive	Task
summarization	Task
of	Task
documents	Task
.	O

This	O
model	O
is	O
not	O
directly	O
comparable	O
to	O
ours	O
since	O
their	O
framework	O
is	O
extractive	O
while	O
ours	O
and	O
that	O
of	O
,	O
and	O
is	O
abstractive	O
.	O

Our	O
work	O
starts	O
with	O
the	O
same	O
framework	O
as	O
,	O
where	O
we	O
use	O
RNNs	Method
for	O
both	O
source	O
and	O
target	O
,	O
but	O
we	O
go	O
beyond	O
the	O
standard	O
architecture	O
and	O
propose	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	Task
.	O

We	O
also	O
note	O
that	O
this	O
work	O
is	O
an	O
extended	O
version	O
of	O
nallapati	O
.	O

In	O
addition	O
to	O
performing	O
more	O
extensive	O
experiments	O
compared	O
to	O
that	O
work	O
,	O
we	O
also	O
propose	O
a	O
novel	O
dataset	O
for	O
document	Task
summarization	Task
on	O
which	O
we	O
establish	O
benchmark	O
numbers	O
too	O
.	O

Below	O
,	O
we	O
analyze	O
the	O
similarities	O
and	O
differences	O
of	O
our	O
proposed	O
models	O
with	O
related	O
work	O
on	O
summarization	Task
.	O

Feature	Method
-	Method
rich	Method
encoder	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Linguistic	O
features	O
such	O
as	O
POS	O
tags	O
,	O
and	O
named	O
-	O
entities	O
as	O
well	O
as	O
TF	O
and	O
IDF	O
information	O
were	O
used	O
in	O
many	O
extractive	Method
approaches	Method
to	O
summarization	Task
,	O
but	O
they	O
are	O
novel	O
in	O
the	O
context	O
of	O
deep	Method
learning	Method
approaches	Method
for	O
abstractive	Task
summarization	Task
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

Switching	O
generator	O
-	O
pointer	Method
model	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
This	O
model	O
combines	O
extractive	Method
and	Method
abstractive	Method
approaches	Method
to	O
summarization	Task
in	O
a	O
single	O
end	Method
-	Method
to	Method
-	Method
end	Method
framework	Method
.	O

namas	Method
also	O
used	O
a	O
combination	O
of	O
extractive	Method
and	Method
abstractive	Method
approaches	Method
,	O
but	O
their	O
extractive	Method
model	Method
is	O
a	O
separate	O
log	Method
-	Method
linear	Method
classifier	Method
with	O
handcrafted	O
features	O
.	O

Pointer	Method
networks	Method
have	O
also	O
been	O
used	O
earlier	O
for	O
the	O
problem	O
of	O
rare	Task
words	Task
in	O
the	O
context	O
of	O
machine	Task
translation	Task
,	O
but	O
the	O
novel	O
addition	O
of	O
switch	O
in	O
our	O
model	O
allows	O
it	O
to	O
strike	O
a	O
balance	O
between	O
when	O
to	O
be	O
faithful	O
to	O
the	O
original	O
source	O
(	O
e.g.	O
,	O
for	O
named	O
entities	O
and	O
OOV	O
)	O
and	O
when	O
it	O
is	O
allowed	O
to	O
be	O
creative	O
.	O

We	O
believe	O
such	O
a	O
process	O
arguably	O
mimics	O
how	O
human	O
produces	O
summaries	O
.	O

For	O
a	O
more	O
detailed	O
treatment	O
of	O
this	O
model	O
,	O
and	O
experiments	O
on	O
multiple	O
tasks	O
,	O
please	O
refer	O
to	O
the	O
parallel	O
work	O
published	O
by	O
some	O
of	O
the	O
authors	O
of	O
this	O
work	O
.	O

Hierarchical	Method
attention	Method
model	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Previously	O
proposed	O
hierarchical	Method
encoder	Method
-	Method
decoder	Method
models	Method
use	O
attention	O
only	O
at	O
sentence	O
-	O
level	O
.	O

The	O
novelty	O
of	O
our	O
approach	O
lies	O
in	O
joint	Task
modeling	Task
of	Task
attention	Task
at	O
both	O
sentence	O
and	O
word	O
levels	O
,	O
where	O
the	O
word	O
-	O
level	O
attention	O
is	O
further	O
influenced	O
by	O
sentence	O
-	O
level	O
attention	O
,	O
thus	O
capturing	O
the	O
notion	O
of	O
important	O
sentences	O
and	O
important	O
words	O
within	O
those	O
sentences	O
.	O

Concatenation	Method
of	Method
positional	Method
embeddings	Method
with	O
the	O
hidden	O
state	O
at	O
sentence	O
-	O
level	O
is	O
also	O
new	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Gigaword	Material
Corpus	O
In	O
this	O
series	O
of	O
experiments	O
,	O
we	O
used	O
the	O
annotated	O
Gigaword	Material
corpus	O
as	O
described	O
in	O
namas	O
.	O

We	O
used	O
the	O
scripts	O
made	O
available	O
by	O
the	O
authors	O
of	O
this	O
work	O
to	O
preprocess	O
the	O
data	O
,	O
which	O
resulted	O
in	O
about	O
3.8	O
M	O
training	O
examples	O
.	O

The	O
script	O
also	O
produces	O
about	O
400	O
K	O
validation	O
and	O
test	O
examples	O
,	O
but	O
we	O
created	O
a	O
randomly	O
sampled	O
subset	O
of	O
2000	O
examples	O
each	O
for	O
validation	Task
and	Task
testing	Task
purposes	Task
,	O
on	O
which	O
we	O
report	O
our	O
performance	O
.	O

Further	O
,	O
we	O
also	O
acquired	O
the	O
exact	O
test	O
sample	O
used	O
in	O
namas	O
to	O
make	O
precise	O
comparison	O
of	O
our	O
models	O
with	O
theirs	O
.	O

We	O
also	O
made	O
small	O
modifications	O
to	O
the	O
script	O
to	O
extract	O
not	O
only	O
the	O
tokenized	O
words	O
,	O
but	O
also	O
system	O
-	O
generated	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
.	O

Training	O
:	O
For	O
all	O
the	O
models	O
we	O
discuss	O
below	O
,	O
we	O
used	O
200	O
dimensional	O
word2vec	O
vectors	O
trained	O
on	O
the	O
same	O
corpus	O
to	O
initialize	O
the	O
model	O
embeddings	O
,	O
but	O
we	O
allowed	O
them	O
to	O
be	O
updated	O
during	O
training	O
.	O

The	O
hidden	O
state	O
dimension	O
of	O
the	O
encoder	Method
and	Method
decoder	Method
was	O
fixed	O
at	O
400	O
in	O
all	O
our	O
experiments	O
.	O

When	O
we	O
used	O
only	O
the	O
first	O
sentence	O
of	O
the	O
document	O
as	O
the	O
source	O
,	O
as	O
done	O
in	O
namas	O
,	O
the	O
encoder	Metric
vocabulary	Metric
size	Metric
was	O
119	O
,	O
505	O
and	O
that	O
of	O
the	O
decoder	Method
stood	O
at	O
68	O
,	O
885	O
.	O

We	O
used	O
Adadelta	Method
for	O
training	Task
,	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.001	O
.	O

We	O
used	O
a	O
batch	O
-	O
size	O
of	O
50	O
and	O
randomly	O
shuffled	O
the	O
training	O
data	O
at	O
every	O
epoch	O
,	O
while	O
sorting	O
every	O
10	O
batches	O
according	O
to	O
their	O
lengths	O
to	O
speed	O
up	O
training	Task
.	O

We	O
did	O
not	O
use	O
any	O
dropout	Method
or	Method
regularization	Method
,	O
but	O
applied	O
gradient	Method
clipping	Method
.	O

We	O
used	O
early	Method
stopping	Method
based	O
on	O
the	O
validation	O
set	O
and	O
used	O
the	O
best	O
model	O
on	O
the	O
validation	O
set	O
to	O
report	O
all	O
test	O
performance	O
numbers	O
.	O

For	O
all	O
our	O
models	O
,	O
we	O
employ	O
the	O
large	Method
-	Method
vocabulary	Method
trick	Method
,	O
where	O
we	O
restrict	O
the	O
decoder	O
vocabulary	O
size	O
to	O
2	O
,	O
000	O
,	O
because	O
it	O
cuts	O
down	O
the	O
training	Metric
time	Metric
per	O
epoch	O
by	O
nearly	O
three	O
times	O
,	O
and	O
helps	O
this	O
and	O
all	O
subsequent	O
models	O
converge	O
in	O
only	O
50%	O
-	O
75	O
%	O
of	O
the	O
epochs	O
needed	O
for	O
the	O
model	O
based	O
on	O
full	O
vocabulary	O
.	O

Decoding	Task
:	O
At	O
decode	O
-	O
time	O
,	O
we	O
used	O
beam	Method
search	Method
of	O
size	O
5	O
to	O
generate	O
the	O
summary	O
,	O
and	O
limited	O
the	O
size	O
of	O
summary	O
to	O
a	O
maximum	O
of	O
30	O
words	O
,	O
since	O
this	O
is	O
the	O
maximum	O
size	O
we	O
noticed	O
in	O
the	O
sampled	O
validation	O
set	O
.	O

We	O
found	O
that	O
the	O
average	O
system	Metric
summary	Metric
length	Metric
from	O
all	O
our	O
models	O
(	O
7.8	O
to	O
8.3	O
)	O
agrees	O
very	O
closely	O
with	O
that	O
of	O
the	O
ground	O
truth	O
on	O
the	O
validation	O
set	O
(	O
about	O
8.7	O
words	O
)	O
,	O
without	O
any	O
specific	O
tuning	O
.	O

Computational	Metric
costs	Metric
:	O
We	O
trained	O
all	O
our	O
models	O
on	O
a	O
single	O
Tesla	Method
K40	Method
GPU	Method
.	O

Most	O
models	O
took	O
about	O
10	O
hours	O
per	O
epoch	O
on	O
an	O
average	O
except	O
the	O
hierarchical	Method
attention	Method
model	Method
,	O
which	O
took	O
12	O
hours	O
per	O
epoch	O
.	O

All	O
models	O
typically	O
converged	O
within	O
15	O
epochs	O
using	O
our	O
early	Metric
stopping	Metric
criterion	Metric
based	O
on	O
the	O
validation	Metric
cost	Metric
.	O

The	O
wall	Metric
-	Metric
clock	Metric
training	Metric
time	Metric
until	O
convergence	Metric
therefore	O
varies	O
between	O
6	O
-	O
8	O
days	O
depending	O
on	O
the	O
model	O
.	O

Generating	Task
summaries	Task
at	O
test	O
time	O
is	O
reasonably	O
fast	O
with	O
a	O
throughput	O
of	O
about	O
20	O
summaries	O
per	O
second	O
on	O
a	O
single	O
GPU	O
,	O
using	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	O
metrics	O
:	O
Similar	O
to	O
and	O
,	O
we	O
use	O
the	O
full	Metric
length	Metric
F1	Metric
variant	Metric
of	O
Rouge	Metric
to	O
evaluate	O
our	O
system	O
.	O

Although	O
limited	Metric
length	Metric
recall	Metric
was	O
the	O
preferred	O
metric	O
for	O
most	O
previous	O
work	O
,	O
one	O
of	O
its	O
disadvantages	O
is	O
choosing	O
the	O
length	O
limit	O
which	O
varies	O
from	O
corpus	O
to	O
corpus	O
,	O
making	O
it	O
difficult	O
for	O
researchers	O
to	O
compare	O
performances	O
.	O

Full	Task
-	Task
length	Task
recall	Task
,	O
on	O
the	O
other	O
hand	O
,	O
does	O
not	O
impose	O
a	O
length	O
restriction	O
but	O
unfairly	O
favors	O
longer	O
summaries	O
.	O

Full	Method
-	Method
length	Method
F1	Method
solves	O
this	O
problem	O
since	O
it	O
can	O
penalize	O
longer	O
summaries	O
,	O
while	O
not	O
imposing	O
a	O
specific	O
length	O
restriction	O
.	O

In	O
addition	O
,	O
we	O
also	O
report	O
the	O
percentage	O
of	O
tokens	O
in	O
the	O
system	O
summary	O
that	O
occur	O
in	O
the	O
source	O
(	O
which	O
we	O
call	O
‘	O
src	O
.	O

copy	Metric
rate	Metric
’	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

We	O
describe	O
all	O
our	O
experiments	O
and	O
results	O
on	O
the	O
Gigaword	Material
corpus	O
below	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
:	O
This	O
is	O
the	O
baseline	O
attentional	O
encoder	Method
-	Method
decoder	Method
model	Method
with	O
the	O
large	Method
vocabulary	Method
trick	Method
.	O

This	O
model	O
is	O
trained	O
only	O
on	O
the	O
first	O
sentence	O
from	O
the	O
source	O
document	O
,	O
as	O
done	O
in	O
namas	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
:	O
This	O
model	O
is	O
identical	O
to	O
the	O
model	O
above	O
except	O
for	O
the	O
fact	O
that	O
it	O
is	O
trained	O
on	O
the	O
first	O
two	O
sentences	O
from	O
the	O
source	O
.	O

On	O
this	O
corpus	O
,	O
adding	O
the	O
additional	O
sentence	O
in	O
the	O
source	O
does	O
seem	O
to	O
aid	O
performance	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
tried	O
adding	O
more	O
sentences	O
,	O
but	O
the	O
performance	O
dropped	O
,	O
which	O
is	O
probably	O
because	O
the	O
latter	O
sentences	O
in	O
this	O
corpus	O
are	O
not	O
pertinent	O
to	O
the	O
summary	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
-	O
hieratt	O
:	O
Since	O
we	O
used	O
two	O
sentences	O
from	O
source	O
document	O
,	O
we	O
trained	O
the	O
hierarchical	Method
attention	Method
model	Method
proposed	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
this	O
model	O
improves	O
performance	O
compared	O
to	O
its	O
flatter	O
counterpart	O
by	O
learning	O
the	O
relative	O
importance	O
of	O
the	O
first	O
two	O
sentences	O
automatically	O
.	O

feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
:	O
Here	O
,	O
we	O
still	O
train	O
on	O
the	O
first	O
two	O
sentences	O
,	O
but	O
we	O
exploit	O
the	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
in	O
the	O
annotated	Material
gigaword	Material
corpus	Material
as	O
well	O
as	O
TF	O
,	O
IDF	O
values	O
,	O
to	O
augment	O
the	O
input	O
embeddings	O
on	O
the	O
source	O
side	O
as	O
described	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

In	O
total	O
,	O
our	O
embedding	Method
vector	Method
grew	O
from	O
the	O
original	O
100	O
to	O
155	O
,	O
and	O
produced	O
incremental	O
gains	O
compared	O
to	O
its	O
counterpart	O
words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
demonstrating	O
the	O
utility	O
of	O
syntax	Method
based	Method
features	Method
in	O
this	O
task	O
.	O

feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
-	O
ptr	O
:	O
This	O
is	O
the	O
switching	Method
generator	Method
/	O
pointer	Method
model	O
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
but	O
in	O
addition	O
,	O
we	O
also	O
use	O
feature	O
-	O
rich	O
embeddings	O
on	O
the	O
document	O
side	O
as	O
in	O
the	O
above	O
model	O
.	O

Our	O
experiments	O
indicate	O
that	O
the	O
new	O
model	O
is	O
able	O
to	O
achieve	O
the	O
best	O
performance	O
on	O
our	O
test	O
set	O
by	O
all	O
three	O
Rouge	Metric
variants	Metric
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
:	O
We	O
compared	O
the	O
performance	O
of	O
our	O
model	Method
words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
sample	O
created	O
by	O
namas	O
,	O
as	O
displayed	O
in	O
the	O
bottom	O
part	O
of	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
trained	O
another	O
system	O
which	O
we	O
call	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
which	O
has	O
a	O
larger	O
LVT	Method
vocabulary	O
size	O
of	O
5k	O
,	O
but	O
also	O
has	O
much	O
larger	O
source	O
and	O
target	O
vocabularies	O
of	O
400	O
K	O
and	O
200	O
K	O
respectively	O
.	O

The	O
reason	O
we	O
did	O
not	O
evaluate	O
our	O
best	O
validation	O
models	O
here	O
is	O
that	O
this	O
test	O
set	O
consisted	O
of	O
only	O
1	O
sentence	O
from	O
the	O
source	O
document	O
,	O
and	O
did	O
not	O
include	O
NLP	O
annotations	O
,	O
which	O
are	O
needed	O
in	O
our	O
best	O
models	O
.	O

The	O
table	O
shows	O
that	O
,	O
despite	O
this	O
fact	O
,	O
our	O
model	O
outperforms	O
the	O
ABS	Method
+	Method
model	Method
of	O
namas	Method
with	O
statistical	Metric
significance	Metric
.	O

In	O
addition	O
,	O
our	O
models	O
exhibit	O
better	O
abstractive	Metric
ability	Metric
as	O
shown	O
by	O
the	O
src	Method
.	O

copy	Metric
rate	Metric
metric	Metric
in	O
the	O
last	O
column	O
of	O
the	O
table	O
.	O

Further	O
,	O
our	O
larger	O
model	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
of	O
with	O
statistically	O
significant	O
improvement	O
on	O
Rouge	Metric
-	Metric
1	Metric
.	O

We	O
believe	O
the	O
bidirectional	O
RNN	Method
we	O
used	O
to	O
model	O
the	O
source	O
captures	O
richer	O
contextual	O
information	O
of	O
every	O
word	O
than	O
the	O
bag	Method
-	Method
of	Method
-	Method
embeddings	Method
representation	Method
used	O
by	O
namas	O
and	O
chopra	O
in	O
their	O
convolutional	Method
attentional	Method
encoders	Method
,	O
which	O
might	O
explain	O
our	O
superior	O
performance	O
.	O

Further	O
,	O
explicit	O
modeling	O
of	O
important	O
information	O
such	O
as	O
multiple	O
source	O
sentences	O
,	O
word	O
-	O
level	O
linguistic	O
features	O
,	O
using	O
the	O
switch	Method
mechanism	Method
to	O
point	O
to	O
source	O
words	O
when	O
needed	O
,	O
and	O
hierarchical	O
attention	O
,	O
solve	O
specific	O
problems	O
in	O
summarization	Task
,	O
each	O
boosting	O
performance	O
incrementally	O
.	O

subsection	O
:	O
DUC	Material
Corpus	Material
The	O
DUC	Material
corpus	Material
comes	O
in	O
two	O
parts	O
:	O
the	O
2003	Material
corpus	Material
consisting	O
of	O
624	O
document	O
,	O
summary	O
pairs	O
and	O
the	O
2004	Material
corpus	Material
consisting	O
of	O
500	O
pairs	O
.	O

Since	O
these	O
corpora	O
are	O
too	O
small	O
to	O
train	O
large	O
neural	Method
networks	Method
on	O
,	O
namas	Method
trained	O
their	O
models	O
on	O
the	O
Gigaword	Material
corpus	O
,	O
but	O
combined	O
it	O
with	O
an	O
additional	O
log	Method
-	Method
linear	Method
extractive	Method
summarization	Method
model	Method
with	O
handcrafted	O
features	O
,	O
that	O
is	O
trained	O
on	O
the	O
DUC	Material
2003	Material
corpus	Material
.	O

They	O
call	O
the	O
original	O
neural	Method
attention	Method
model	Method
the	O
ABS	Method
model	Method
,	O
and	O
the	O
combined	Method
model	Method
ABS	Method
+	Method
.	O

chopra	O
also	O
report	O
the	O
performance	O
of	O
their	O
RAS	Method
-	Method
Elman	Method
model	Method
on	O
this	O
corpus	O
and	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
since	O
it	O
outperforms	O
all	O
previously	O
published	O
baselines	O
including	O
non	Method
-	Method
neural	Method
network	Method
based	Method
extractive	Method
and	Method
abstractive	Method
systems	Method
,	O
as	O
measured	O
by	O
the	O
official	Metric
DUC	Metric
metric	Metric
of	Metric
recall	Metric
at	O
75	O
bytes	O
.	O

In	O
these	O
experiments	O
,	O
we	O
use	O
the	O
same	O
metric	O
to	O
evaluate	O
our	O
models	O
too	O
,	O
but	O
we	O
omit	O
reporting	O
numbers	O
from	O
other	O
systems	O
in	O
the	O
interest	O
of	O
space	O
.	O

In	O
our	O
work	O
,	O
we	O
simply	O
run	O
the	O
models	O
trained	O
on	O
Gigaword	Material
corpus	O
as	O
they	O
are	O
,	O
without	O
tuning	O
them	O
on	O
the	O
DUC	Material
validation	Material
set	Material
.	O

The	O
only	O
change	O
we	O
made	O
to	O
the	O
decoder	Method
is	O
to	O
suppress	O
the	O
model	O
from	O
emitting	O
the	O
end	O
-	O
of	O
-	O
summary	O
tag	O
,	O
and	O
force	O
it	O
to	O
emit	O
exactly	O
30	O
words	O
for	O
every	O
summary	O
,	O
since	O
the	O
official	O
evaluation	O
on	O
this	O
corpus	O
is	O
based	O
on	O
limited	O
-	O
length	O
Rouge	Metric
recall	O
.	O

On	O
this	O
corpus	O
too	O
,	O
since	O
we	O
have	O
only	O
a	O
single	O
sentence	O
from	O
source	O
and	O
no	O
NLP	Material
annotations	Material
,	O
we	O
ran	O
just	O
the	O
models	Method
words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
and	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
.	O

The	O
performance	O
of	O
this	O
model	O
on	O
the	O
test	O
set	O
is	O
compared	O
with	O
ABS	Method
and	Method
ABS	Method
+	Method
models	Method
,	O
RAS	Method
-	O
Elman	Method
from	O
,	O
as	O
well	O
as	O
TOPIARY	Method
,	O
the	O
top	O
performing	O
system	O
on	O
DUC	Material
-	Material
2004	Material
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
note	O
our	O
best	O
model	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
outperforms	O
RAS	Method
-	Method
Elman	Method
on	O
two	O
of	O
the	O
three	O
variants	O
of	O
Rouge	Metric
,	O
while	O
being	O
competitive	O
on	O
Rouge	Metric
-	Metric
1	Metric
.	O

subsection	O
:	O
CNN	Material
/	Material
Daily	Material
Mail	Material
Corpus	Material
The	O
existing	O
abstractive	O
text	O
summarization	O
corpora	O
including	O
Gigaword	Material
and	O
DUC	Material
consist	O
of	O
only	O
one	O
sentence	O
in	O
each	O
summary	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
a	O
new	O
corpus	O
that	O
comprises	O
multi	Material
-	Material
sentence	Material
summaries	Material
.	O

To	O
produce	O
this	O
corpus	O
,	O
we	O
modify	O
an	O
existing	O
corpus	O
that	O
has	O
been	O
used	O
for	O
the	O
task	O
of	O
passage	Task
-	Task
based	Task
question	Task
answering	Task
.	O

In	O
this	O
work	O
,	O
the	O
authors	O
used	O
the	O
human	O
generated	O
abstractive	O
summary	O
bullets	O
from	O
new	O
-	O
stories	O
in	O
CNN	Material
and	O
Daily	Material
Mail	Material
websites	Material
as	O
questions	O
(	O
with	O
one	O
of	O
the	O
entities	O
hidden	O
)	O
,	O
and	O
stories	O
as	O
the	O
corresponding	O
passages	O
from	O
which	O
the	O
system	O
is	O
expected	O
to	O
answer	O
the	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
question	O
.	O

The	O
authors	O
released	O
the	O
scripts	O
that	O
crawl	O
,	O
extract	O
and	O
generate	O
pairs	O
of	O
passages	O
and	O
questions	O
from	O
these	O
websites	O
.	O

With	O
a	O
simple	O
modification	O
of	O
the	O
script	O
,	O
we	O
restored	O
all	O
the	O
summary	O
bullets	O
of	O
each	O
story	O
in	O
the	O
original	O
order	O
to	O
obtain	O
a	O
multi	O
-	O
sentence	O
summary	O
,	O
where	O
each	O
bullet	O
is	O
treated	O
as	O
a	O
sentence	O
.	O

In	O
all	O
,	O
this	O
corpus	O
has	O
286	O
,	O
817	O
training	O
pairs	O
,	O
13	O
,	O
368	O
validation	O
pairs	O
and	O
11	O
,	O
487	O
test	O
pairs	O
,	O
as	O
defined	O
by	O
their	O
scripts	O
.	O

The	O
source	O
documents	O
in	O
the	O
training	O
set	O
have	O
766	O
words	O
spanning	O
29.74	O
sentences	O
on	O
an	O
average	O
while	O
the	O
summaries	O
consist	O
of	O
53	O
words	O
and	O
3.72	O
sentences	O
.	O

The	O
unique	O
characteristics	O
of	O
this	O
dataset	O
such	O
as	O
long	O
documents	O
,	O
and	O
ordered	O
multi	O
-	O
sentence	O
summaries	O
present	O
interesting	O
challenges	O
,	O
and	O
we	O
hope	O
will	O
attract	O
future	O
researchers	O
to	O
build	O
and	O
test	O
novel	O
models	O
on	O
it	O
.	O

The	O
dataset	O
is	O
released	O
in	O
two	O
versions	O
:	O
one	O
consisting	O
of	O
actual	O
entity	O
names	O
,	O
and	O
the	O
other	O
,	O
in	O
which	O
entity	O
occurrences	O
are	O
replaced	O
with	O
document	O
-	O
specific	O
integer	O
-	O
ids	O
beginning	O
from	O
0	O
.	O

Since	O
the	O
vocabulary	Metric
size	Metric
is	O
smaller	O
in	O
the	O
anonymized	Method
version	Method
,	O
we	O
used	O
it	O
in	O
all	O
our	O
experiments	O
below	O
.	O

We	O
limited	O
the	O
source	O
vocabulary	O
size	O
to	O
150	O
K	O
,	O
and	O
the	O
target	O
vocabulary	O
to	O
60	O
K	O
,	O
the	O
source	O
and	O
target	O
lengths	O
to	O
at	O
most	O
800	O
and	O
100	O
words	O
respectively	O
.	O

We	O
used	O
100	Method
-	Method
dimensional	Method
word2vec	Method
embeddings	Method
trained	O
on	O
this	O
dataset	O
as	O
input	O
,	O
and	O
we	O
fixed	O
the	O
model	O
hidden	O
state	O
size	O
at	O
200	O
.	O

We	O
also	O
created	O
explicit	O
pointers	O
in	O
the	O
training	O
data	O
by	O
matching	O
only	O
the	O
anonymized	O
entity	O
-	O
ids	O
between	O
source	O
and	O
target	O
on	O
similar	O
lines	O
as	O
we	O
did	O
for	O
the	O
OOV	Material
words	Material
in	O
Gigaword	Material
corpus	O
.	O

Computational	Metric
costs	Metric
:	O
We	O
used	O
a	O
single	O
Tesla	Method
K	Method
-	Method
40	Method
GPU	Method
to	O
train	O
our	O
models	O
on	O
this	O
dataset	O
as	O
well	O
.	O

While	O
the	O
flat	Method
models	Method
(	O
words	Method
-	Method
lvt2k	Method
and	Method
words	Method
-	Method
lvt2k	Method
-	Method
ptr	Method
)	O
took	O
under	O
5	O
hours	O
per	O
epoch	O
,	O
the	O
hierarchical	Method
attention	Method
model	Method
was	O
very	O
expensive	O
,	O
consuming	O
nearly	O
12.5	O
hours	O
per	O
epoch	O
.	O

Convergence	O
of	O
all	O
models	O
is	O
also	O
slower	O
on	O
this	O
dataset	O
compared	O
to	O
Gigaword	Material
,	O
taking	O
nearly	O
35	O
epochs	O
for	O
all	O
models	O
.	O

Thus	O
,	O
the	O
wall	Metric
-	Metric
clock	Metric
time	Metric
for	O
training	O
until	O
convergence	Task
is	O
about	O
7	O
days	O
for	O
the	O
flat	Method
models	Method
,	O
but	O
nearly	O
18	O
days	O
for	O
the	O
hierarchical	Method
attention	Method
model	Method
.	O

Decoding	Method
is	O
also	O
slower	O
as	O
well	O
,	O
with	O
a	O
throughput	O
of	O
2	O
examples	O
per	O
second	O
for	O
flat	Method
models	Method
and	O
1.5	O
examples	O
per	O
second	O
for	O
the	O
hierarchical	Method
attention	Method
model	Method
,	O
when	O
run	O
on	O
a	O
single	O
GPU	Method
with	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	Task
:	O
We	O
evaluated	O
our	O
models	O
using	O
the	O
full	O
-	O
length	O
Rouge	Metric
F1	O
metric	O
that	O
we	O
employed	O
for	O
the	O
Gigaword	Material
corpus	O
,	O
but	O
with	O
one	O
notable	O
difference	O
:	O
in	O
both	O
system	O
and	O
gold	O
summaries	O
,	O
we	O
considered	O
each	O
highlight	O
to	O
be	O
a	O
separate	O
sentence	O
.	O

Results	O
:	O
Results	O
from	O
the	O
basic	O
attention	Method
encoder	Method
-	Method
decoder	Method
as	O
well	O
as	O
the	O
hierarchical	Method
attention	Method
model	Method
are	O
displayed	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Although	O
this	O
dataset	O
is	O
smaller	O
and	O
more	O
complex	O
than	O
the	O
Gigaword	Material
corpus	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
Rouge	Metric
numbers	O
are	O
in	O
the	O
same	O
range	O
.	O

However	O
,	O
the	O
hierarchical	Method
attention	Method
model	Method
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
outperforms	O
the	O
baseline	O
attentional	Method
decoder	Method
only	O
marginally	O
.	O

Upon	O
visual	O
inspection	O
of	O
the	O
system	O
output	O
,	O
we	O
noticed	O
that	O
on	O
this	O
dataset	O
,	O
both	O
these	O
models	O
produced	O
summaries	O
that	O
contain	O
repetitive	O
phrases	O
or	O
even	O
repetitive	O
sentences	O
at	O
times	O
.	O

Since	O
the	O
summaries	O
in	O
this	O
dataset	O
involve	O
multiple	O
sentences	O
,	O
it	O
is	O
likely	O
that	O
the	O
decoder	Method
‘	O
forgets	O
’	O
what	O
part	O
of	O
the	O
document	O
was	O
used	O
in	O
producing	O
earlier	O
highlights	O
.	O

To	O
overcome	O
this	O
problem	O
,	O
we	O
used	O
the	O
Temporal	Method
Attention	Method
model	Method
of	O
baskaran	Method
that	O
keeps	O
track	O
of	O
past	O
attentional	O
weights	O
of	O
the	O
decoder	O
and	O
expliticly	O
discourages	O
it	O
from	O
attending	O
to	O
the	O
same	O
parts	O
of	O
the	O
document	O
in	O
future	O
time	O
steps	O
.	O

The	O
model	O
works	O
as	O
shown	O
by	O
the	O
following	O
simple	O
equations	O
:	O
where	O
is	O
the	O
unnormalized	O
attention	O
-	O
weights	O
vector	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	Method
.	O

In	O
other	O
words	O
,	O
the	O
temporal	Method
attention	Method
model	Method
down	O
-	O
weights	O
the	O
attention	O
weights	O
at	O
the	O
current	O
time	O
step	O
if	O
the	O
past	O
attention	O
weights	O
are	O
high	O
on	O
the	O
same	O
part	O
of	O
the	O
document	O
.	O

Using	O
this	O
strategy	O
,	O
the	O
temporal	Method
attention	Method
model	Method
improves	O
performance	O
significantly	O
over	O
both	O
the	O
baseline	Method
model	Method
as	O
well	O
as	O
the	O
hierarchical	Method
attention	Method
model	Method
.	O

We	O
have	O
also	O
noticed	O
that	O
there	O
are	O
fewer	O
repetitions	O
of	O
summay	O
highlights	O
produced	O
by	O
this	O
model	O
as	O
shown	O
in	O
the	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O

These	O
results	O
,	O
although	O
preliminary	O
,	O
should	O
serve	O
as	O
a	O
good	O
baseline	O
for	O
future	O
researchers	O
to	O
compare	O
their	O
models	O
against	O
.	O

section	O
:	O
Qualitative	Task
Analysis	Task
Table	O
[	O
reference	O
]	O
presents	O
a	O
few	O
high	O
quality	O
and	O
poor	O
quality	O
output	O
on	O
the	O
validation	O
set	O
from	O
feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
,	O
one	O
of	O
our	O
best	O
performing	O
models	O
.	O

Even	O
when	O
the	O
model	O
differs	O
from	O
the	O
target	O
summary	O
,	O
its	O
summaries	O
tend	O
to	O
be	O
very	O
meaningful	O
and	O
relevant	O
,	O
a	O
phenomenon	O
not	O
captured	O
by	O
word	Metric
/	Metric
phrase	Metric
matching	Metric
evaluation	Metric
metrics	Metric
such	O
as	O
Rouge	Metric
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
model	O
sometimes	O
‘	O
misinterprets	O
’	O
the	O
semantics	O
of	O
the	O
text	O
and	O
generates	O
a	O
summary	O
with	O
a	O
comical	O
interpretation	O
as	O
shown	O
in	O
the	O
poor	O
quality	O
examples	O
in	O
the	O
table	O
.	O

Clearly	O
,	O
capturing	O
the	O
‘	O
meaning	O
’	O
of	O
complex	O
sentences	O
remains	O
a	O
weakness	O
of	O
these	O
models	O
.	O

Our	O
next	O
example	O
output	O
,	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
displays	O
the	O
sample	O
output	O
from	O
the	O
switching	Method
generator	Method
/	O
pointer	Method
model	O
on	O
the	O
Gigaword	Material
corpus	O
.	O

It	O
is	O
apparent	O
from	O
the	O
examples	O
that	O
the	O
model	O
learns	O
to	O
use	O
pointers	O
very	O
accurately	O
not	O
only	O
for	O
named	O
entities	O
,	O
but	O
also	O
for	O
multi	Task
-	Task
word	Task
phrases	Task
.	O

Despite	O
its	O
accuracy	Metric
,	O
the	O
performance	O
improvement	O
of	O
the	O
overall	O
model	O
is	O
not	O
significant	O
.	O

We	O
believe	O
the	O
impact	O
of	O
this	O
model	O
may	O
be	O
more	O
pronounced	O
in	O
other	O
settings	O
with	O
a	O
heavier	O
tail	O
distribution	O
of	O
rare	O
words	O
.	O

We	O
intend	O
to	O
carry	O
out	O
more	O
experiments	O
with	O
this	O
model	O
in	O
the	O
future	O
.	O

On	O
CNN	Material
/	Material
Daily	Material
Mail	Material
data	Material
,	O
although	O
our	O
models	O
are	O
able	O
to	O
produce	O
good	O
quality	O
multi	O
-	O
sentence	O
summaries	O
,	O
we	O
notice	O
that	O
the	O
same	O
sentence	O
or	O
phrase	O
often	O
gets	O
repeated	O
in	O
the	O
summary	O
.	O

We	O
believe	O
models	O
that	O
incorporate	O
intra	Method
-	Method
attention	Method
such	O
as	O
lstmn	Method
can	O
fix	O
this	O
problem	O
by	O
encouraging	O
the	O
model	O
to	O
‘	O
remember	O
’	O
the	O
words	O
it	O
has	O
already	O
produced	O
in	O
the	O
past	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
apply	O
the	O
attentional	Method
encoder	Method
-	Method
decoder	Method
for	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
with	O
very	O
promising	O
results	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
significantly	O
on	O
two	O
different	O
datasets	O
.	O

Each	O
of	O
our	O
proposed	O
novel	O
models	O
addresses	O
a	O
specific	O
problem	O
in	O
abstractive	Task
summarization	Task
,	O
yielding	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
for	O
multi	Task
-	Task
sentence	Task
summarization	Task
and	O
establish	O
benchmark	O
numbers	O
on	O
it	O
.	O

As	O
part	O
of	O
our	O
future	O
work	O
,	O
we	O
plan	O
to	O
focus	O
our	O
efforts	O
on	O
this	O
data	O
and	O
build	O
more	O
robust	O
models	O
for	O
summaries	Task
consisting	O
of	O
multiple	O
sentences	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Training	O
with	O
Exploration	Task
Improves	O
a	O
Greedy	Method
Stack	Method
LSTM	Method
Parser	Method
We	O
adapt	O
the	O
greedy	Method
stack	Method
LSTM	Method
dependency	Method
parser	Method
of	O
lstmacl15	O
to	O
support	O
a	O
training	Task
-	Task
with	Task
-	Task
exploration	Task
procedure	Task
using	O
dynamic	O
oracles	O
instead	O
of	O
assuming	O
an	O
error	O
-	O
free	O
action	O
history	O
.	O

This	O
form	O
of	O
training	O
,	O
which	O
accounts	O
for	O
model	Task
predictions	Task
at	O
training	O
time	O
,	O
improves	O
parsing	Metric
accuracies	Metric
.	O

We	O
discuss	O
some	O
modifications	O
needed	O
in	O
order	O
to	O
get	O
training	O
with	O
exploration	Task
to	O
work	O
well	O
for	O
a	O
probabilistic	Method
neural	Method
network	Method
dependency	Method
parser	Method
.	O

section	O
:	O
Introduction	O
Natural	O
language	O
parsing	Task
can	O
be	O
formulated	O
as	O
a	O
series	O
of	O
decisions	O
that	O
read	O
words	O
in	O
sequence	O
and	O
incrementally	O
combine	O
them	O
to	O
form	O
syntactic	O
structures	O
;	O
this	O
formalization	O
is	O
known	O
as	O
transition	O
-	O
based	O
parsing	Task
,	O
and	O
is	O
often	O
coupled	O
with	O
a	O
greedy	Method
search	Method
procedure	Method
.	O

The	O
literature	O
on	O
transition	O
-	O
based	O
parsing	Task
is	O
vast	O
,	O
but	O
all	O
works	O
share	O
in	O
common	O
a	O
classification	Method
component	Method
that	O
takes	O
into	O
account	O
features	O
of	O
the	O
current	O
parser	O
state	O
and	O
predicts	O
the	O
next	O
action	O
to	O
take	O
conditioned	O
on	O
the	O
state	O
.	O

The	O
state	O
is	O
of	O
unbounded	O
size	O
.	O

Dyer	O
et	O
al	O
.	O

lstmacl15	Method
presented	O
a	O
parser	Method
in	O
which	O
the	O
parser	O
’s	O
unbounded	O
state	O
is	O
embedded	O
in	O
a	O
fixed	O
-	O
dimensional	O
continuous	O
space	O
using	O
recurrent	Method
neural	Method
networks	Method
.	O

Coupled	O
with	O
a	O
recursive	Method
tree	Method
composition	Method
function	Method
,	O
the	O
feature	Method
representation	Method
is	O
able	O
to	O
capture	O
information	O
from	O
the	O
entirety	O
of	O
the	O
state	O
,	O
without	O
resorting	O
to	O
locality	O
assumptions	O
that	O
were	O
common	O
in	O
most	O
other	O
transition	Method
-	Method
based	Method
parsers	Method
.	O

The	O
use	O
of	O
a	O
novel	O
stack	Method
LSTM	Method
data	Method
structure	Method
allows	O
the	O
parser	Method
to	O
maintain	O
a	O
constant	O
time	O
per	O
-	O
state	O
update	O
,	O
and	O
retain	O
an	O
overall	O
linear	O
parsing	Task
time	O
.	O

The	O
Dyer	O
et	O
al	O
.	O

parser	Method
was	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
gold	O
-	O
standard	O
transition	O
sequences	O
,	O
given	O
words	O
.	O

At	O
test	O
time	O
,	O
the	O
parser	Method
makes	O
greedy	O
decisions	O
according	O
to	O
the	O
learned	O
model	O
.	O

Although	O
this	O
setup	O
obtains	O
very	O
good	O
performance	O
,	O
the	O
training	O
and	O
testing	O
conditions	O
are	O
mismatched	O
in	O
the	O
following	O
way	O
:	O
at	O
training	O
time	O
the	O
historical	O
context	O
of	O
an	O
action	O
is	O
always	O
derived	O
from	O
the	O
gold	O
standard	O
(	O
i.e.	O
,	O
perfectly	O
correct	O
past	O
actions	O
)	O
,	O
but	O
at	O
test	O
time	O
,	O
it	O
will	O
be	O
a	O
model	Method
prediction	Method
.	O

In	O
this	O
work	O
,	O
we	O
adapt	O
the	O
training	O
criterion	O
so	O
as	O
to	O
explore	O
parser	O
states	O
drawn	O
not	O
only	O
from	O
the	O
training	O
data	O
,	O
but	O
also	O
from	O
the	O
model	O
as	O
it	O
is	O
being	O
learned	O
.	O

To	O
do	O
so	O
,	O
we	O
use	O
the	O
method	O
of	O
Goldberg	O
and	O
Nivre	O
goldberg12dynamic	O
,	O
goldberg2013training	O
to	O
dynamically	O
chose	O
an	O
optimal	O
(	O
relative	O
to	O
the	O
final	O
attachment	Metric
accuracy	Metric
)	O
action	O
given	O
an	O
imperfect	O
history	O
.	O

By	O
interpolating	O
between	O
algorithm	O
states	O
sampled	O
from	O
the	O
model	O
and	O
those	O
sampled	O
from	O
the	O
training	O
data	O
,	O
more	O
robust	O
predictions	O
at	O
test	O
time	O
can	O
be	O
made	O
.	O

We	O
show	O
that	O
the	O
technique	O
can	O
be	O
used	O
to	O
improve	O
the	O
strong	O
parser	O
of	O
Dyer	O
et	O
al	O
.	O

section	O
:	O
Parsing	Method
Model	Method
and	O
Parameter	Method
Learning	Method
Our	O
departure	O
point	O
is	O
the	O
parsing	Task
model	O
described	O
by	O
lstmacl15	O
.	O

We	O
do	O
not	O
describe	O
the	O
model	O
in	O
detail	O
,	O
and	O
refer	O
the	O
reader	O
to	O
the	O
original	O
work	O
.	O

At	O
each	O
stage	O
of	O
the	O
parsing	Task
process	O
,	O
the	O
parser	O
state	O
is	O
encoded	O
into	O
a	O
vector	O
,	O
which	O
is	O
used	O
to	O
compute	O
the	O
probability	O
of	O
the	O
parser	O
action	O
at	O
time	O
as	O
:	O
where	O
is	O
a	O
column	O
vector	O
representing	O
the	O
(	O
output	O
)	O
embedding	O
of	O
the	O
parser	O
action	O
,	O
and	O
is	O
a	O
bias	O
term	O
for	O
action	O
.	O

The	O
set	O
represents	O
the	O
valid	O
transition	O
actions	O
that	O
may	O
be	O
taken	O
in	O
the	O
current	O
state	O
.	O

Since	O
encodes	O
information	O
about	O
all	O
previous	O
decisions	O
made	O
by	O
the	O
parser	Method
,	O
the	O
chain	Method
rule	Method
gives	O
the	O
probability	O
of	O
any	O
valid	O
sequence	O
of	O
parse	O
transitions	O
conditional	O
on	O
the	O
input	O
:	O
The	O
parser	Method
is	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
taking	O
a	O
“	O
correct	O
”	O
action	O
at	O
each	O
parsing	Task
state	O
.	O

The	O
definition	O
of	O
what	O
constitutes	O
a	O
“	O
correct	O
”	O
action	O
is	O
the	O
major	O
difference	O
between	O
a	O
static	O
oracle	O
as	O
used	O
by	O
lstmacl15	O
and	O
the	O
dynamic	Method
oracle	Method
explored	O
here	O
.	O

Regardless	O
of	O
the	O
oracle	O
,	O
our	O
training	Method
implementation	Method
constructs	O
a	O
computation	O
graph	O
(	O
nodes	O
that	O
represent	O
values	O
,	O
linked	O
by	O
directed	O
edges	O
from	O
each	O
function	O
’s	O
inputs	O
to	O
its	O
outputs	O
)	O
for	O
the	O
negative	O
log	O
probability	O
for	O
the	O
oracle	O
transition	O
sequence	O
as	O
a	O
function	O
of	O
the	O
current	O
model	O
parameters	O
and	O
uses	O
forward	Method
-	Method
and	O
backpropagation	Method
to	O
obtain	O
the	O
gradients	O
respect	O
to	O
the	O
model	O
parameters	O
.	O

subsection	O
:	O
Training	O
with	O
Static	O
Oracles	O
With	O
a	O
static	O
oracle	O
,	O
the	O
training	Method
procedure	Method
computes	O
a	O
canonical	O
reference	O
series	O
of	O
transitions	O
for	O
each	O
gold	O
parse	O
tree	O
.	O

It	O
then	O
runs	O
the	O
parser	Method
through	O
this	O
canonical	O
sequence	O
of	O
transitions	O
,	O
while	O
keeping	O
track	O
of	O
the	O
state	Method
representation	Method
at	O
each	O
step	O
,	O
as	O
well	O
as	O
the	O
distribution	O
over	O
transitions	O
which	O
is	O
predicted	O
by	O
the	O
current	O
classifier	Method
for	O
the	O
state	Method
representation	Method
.	O

Once	O
the	O
end	O
of	O
the	O
sentence	O
is	O
reached	O
,	O
the	O
parameters	O
are	O
updated	O
towards	O
maximizing	O
the	O
likelihood	O
of	O
the	O
reference	O
transition	O
sequence	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
which	O
equates	O
to	O
maximizing	O
the	O
probability	O
of	O
the	O
correct	O
transition	O
,	O
,	O
at	O
each	O
state	O
along	O
the	O
path	O
.	O

subsection	O
:	O
Training	O
with	O
Dynamic	Method
Oracles	Method
In	O
the	O
static	Task
oracle	Task
case	Task
,	O
the	O
parser	Method
is	O
trained	O
to	O
predict	O
the	O
best	O
transition	O
to	O
take	O
at	O
each	O
parsing	Task
step	O
,	O
assuming	O
all	O
previous	O
transitions	O
were	O
correct	O
.	O

Since	O
the	O
parser	Method
is	O
likely	O
to	O
make	O
mistakes	O
at	O
test	O
time	O
and	O
encounter	O
states	O
it	O
has	O
not	O
seen	O
during	O
training	O
,	O
this	O
training	O
criterion	O
is	O
problematic	O
.	O

Instead	O
,	O
we	O
would	O
prefer	O
to	O
train	O
the	O
parser	Method
to	O
behave	O
optimally	O
even	O
after	O
making	O
a	O
mistake	O
(	O
under	O
the	O
constraint	O
that	O
it	O
can	O
not	O
backtrack	O
or	O
fix	O
any	O
previous	O
decision	O
)	O
.	O

We	O
thus	O
need	O
to	O
include	O
in	O
the	O
training	O
examples	O
states	O
that	O
result	O
from	O
wrong	O
parsing	Task
decisions	O
,	O
together	O
with	O
the	O
optimal	O
transitions	O
to	O
take	O
in	O
these	O
states	O
.	O

To	O
this	O
end	O
we	O
reconsider	O
which	O
training	O
examples	O
to	O
show	O
,	O
and	O
what	O
it	O
means	O
to	O
behave	O
optimally	O
on	O
these	O
training	O
examples	O
.	O

The	O
framework	O
of	O
training	O
with	O
exploration	Task
using	O
dynamic	O
oracles	O
suggested	O
by	O
Goldberg	O
and	O
Nivre	O
goldberg12dynamic	O
,	O
goldberg2013training	O
provides	O
answers	O
to	O
these	O
questions	O
.	O

While	O
the	O
application	O
of	O
dynamic	Method
oracle	Method
training	Method
is	O
relatively	O
straightforward	O
,	O
some	O
adaptations	O
were	O
needed	O
to	O
accommodate	O
the	O
probabilistic	Task
training	Task
objective	Task
.	O

These	O
adaptations	O
mostly	O
follow	O
Goldberg	O
goldberg2013calibrated	O
.	O

paragraph	O
:	O
Dynamic	O
Oracles	O
.	O

A	O
dynamic	Method
oracle	Method
is	O
the	O
component	O
that	O
,	O
given	O
a	O
gold	O
parse	O
tree	O
,	O
provides	O
the	O
optimal	O
set	O
of	O
possible	O
actions	O
to	O
take	O
for	O
any	O
valid	O
parser	O
state	O
.	O

In	O
contrast	O
to	O
static	Method
oracles	Method
that	O
derive	O
a	O
canonical	O
state	O
sequence	O
for	O
each	O
gold	O
parse	O
tree	O
and	O
say	O
nothing	O
about	O
states	O
that	O
deviate	O
from	O
this	O
canonical	O
path	O
,	O
the	O
dynamic	Method
oracle	Method
is	O
well	O
defined	O
for	O
states	O
that	O
result	O
from	O
parsing	Task
mistakes	O
,	O
and	O
they	O
may	O
produce	O
more	O
than	O
a	O
single	O
gold	O
action	O
for	O
a	O
given	O
state	O
.	O

Under	O
the	O
dynamic	Method
oracle	Method
framework	Method
,	O
an	O
action	O
is	O
said	O
to	O
be	O
optimal	O
for	O
a	O
state	O
if	O
the	O
best	O
tree	O
that	O
can	O
be	O
reached	O
after	O
taking	O
the	O
action	O
is	O
no	O
worse	O
(	O
in	O
terms	O
of	O
accuracy	Metric
with	O
respect	O
to	O
the	O
gold	O
tree	O
)	O
than	O
the	O
best	O
tree	O
that	O
could	O
be	O
reached	O
prior	O
to	O
taking	O
that	O
action	O
.	O

Goldberg	O
and	O
Nivre	O
goldberg2013training	O
define	O
the	O
arc	Method
-	Method
decomposition	Method
property	Method
of	Method
transition	Method
systems	Method
,	O
and	O
show	O
how	O
to	O
derive	O
efficient	O
dynamic	Method
oracles	Method
for	O
transition	Method
systems	Method
that	O
are	O
arc	O
-	O
decomposable	O
.	O

Unfortunately	O
,	O
the	O
arc	Method
-	Method
standard	Method
transition	Method
system	Method
does	O
not	O
have	O
this	O
property	O
.	O

While	O
it	O
is	O
possible	O
to	O
compute	O
dynamic	O
oracles	O
for	O
the	O
arc	Method
-	Method
standard	Method
system	Method
,	O
the	O
computation	O
relies	O
on	O
a	O
dynamic	Method
programming	Method
algorithm	Method
which	O
is	O
polynomial	O
in	O
the	O
length	O
of	O
the	O
stack	O
.	O

As	O
the	O
dynamic	Method
oracle	Method
has	O
to	O
be	O
queried	O
for	O
each	O
parser	O
state	O
seen	O
during	O
training	O
,	O
the	O
use	O
of	O
this	O
dynamic	Method
oracle	Method
will	O
make	O
the	O
training	O
runtime	O
several	O
times	O
longer	O
.	O

We	O
chose	O
instead	O
to	O
switch	O
to	O
the	O
arc	Method
-	Method
hybrid	Method
transition	O
system	O
,	O
which	O
is	O
very	O
similar	O
to	O
the	O
arc	Method
-	Method
standard	Method
system	Method
but	O
is	O
arc	O
-	O
decomposable	O
and	O
hence	O
admits	O
an	O
efficient	O
dynamic	Method
oracle	Method
,	O
resulting	O
in	O
only	O
negligible	O
increase	O
to	O
training	Metric
runtime	Metric
.	O

We	O
implemented	O
the	O
dynamic	Method
oracle	Method
to	O
the	O
arc	Method
-	Method
hybrid	Method
system	O
as	O
described	O
by	O
Goldberg	O
goldberg2013training	O
.	O

paragraph	O
:	O
Training	O
with	O
Exploration	O
.	O

In	O
order	O
to	O
expose	O
the	O
parser	Method
to	O
configurations	O
that	O
are	O
likely	O
to	O
result	O
from	O
incorrect	O
parsing	Task
decisions	O
,	O
we	O
make	O
use	O
of	O
the	O
probabilistic	O
nature	O
of	O
the	O
classifier	Method
.	O

During	O
training	O
,	O
instead	O
of	O
following	O
the	O
gold	O
action	O
,	O
we	O
sample	O
the	O
next	O
transition	O
according	O
to	O
the	O
output	O
distribution	O
the	O
classifier	Method
assigns	O
to	O
the	O
current	O
configuration	O
.	O

Another	O
option	O
,	O
taken	O
by	O
Goldberg	O
and	O
Nivre	O
,	O
is	O
to	O
follow	O
the	O
one	O
-	O
best	O
action	O
predicted	O
by	O
the	O
classifier	Method
.	O

However	O
,	O
initial	O
experiments	O
showed	O
that	O
the	O
one	O
-	O
best	O
approach	O
did	O
not	O
work	O
well	O
.	O

Because	O
the	O
neural	Method
network	Method
classifier	Method
becomes	O
accurate	O
early	O
on	O
in	O
the	O
training	O
process	O
,	O
the	O
one	O
-	O
best	O
action	O
is	O
likely	O
to	O
be	O
correct	O
,	O
and	O
the	O
parser	Method
is	O
then	O
exposed	O
to	O
very	O
few	O
error	O
states	O
in	O
its	O
training	O
process	O
.	O

By	O
sampling	O
from	O
the	O
predicted	O
distribution	O
,	O
we	O
are	O
effectively	O
increasing	O
the	O
chance	O
of	O
straying	O
from	O
the	O
gold	O
path	O
during	O
training	O
,	O
while	O
still	O
focusing	O
on	O
mistakes	O
that	O
receive	O
relatively	O
high	O
parser	Metric
scores	Metric
.	O

We	O
believe	O
further	O
formal	O
analysis	O
of	O
this	O
method	O
will	O
reveal	O
connections	O
to	O
reinforcement	Task
learning	Task
and	O
,	O
perhaps	O
,	O
other	O
methods	O
for	O
learning	Task
complex	Task
policies	Task
.	O

Taking	O
this	O
idea	O
further	O
,	O
we	O
could	O
increase	O
the	O
number	O
of	O
error	O
-	O
states	O
observed	O
in	O
the	O
training	Method
process	Method
by	O
changing	O
the	O
sampling	O
distribution	O
so	O
as	O
to	O
bias	O
it	O
toward	O
more	O
low	O
-	O
probability	O
states	O
.	O

We	O
do	O
this	O
by	O
raising	O
each	O
probability	O
to	O
the	O
power	O
of	O
(	O
)	O
and	O
re	O
-	O
normalizing	O
.	O

This	O
transformation	O
keeps	O
the	O
relative	O
ordering	O
of	O
the	O
events	O
,	O
while	O
shifting	O
probability	O
mass	O
towards	O
less	O
frequent	O
events	O
.	O

As	O
we	O
show	O
below	O
,	O
this	O
turns	O
out	O
to	O
be	O
very	O
beneficial	O
for	O
the	O
configurations	O
that	O
make	O
use	O
of	O
external	O
embeddings	O
.	O

Indeed	O
,	O
these	O
configurations	O
achieve	O
high	O
accuracies	Metric
and	O
sharp	O
class	O
distributions	O
early	O
on	O
in	O
the	O
training	O
process	O
.	O

The	O
parser	Method
is	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
a	O
correct	O
action	O
at	O
each	O
parsing	Task
state	O
according	O
to	O
Equation	O
[	O
reference	O
]	O
.	O

When	O
using	O
the	O
dynamic	O
oracle	O
,	O
a	O
state	O
may	O
admit	O
multiple	O
correct	O
actions	O
.	O

Our	O
objective	O
in	O
such	O
cases	O
is	O
the	O
marginal	O
likelihood	O
of	O
all	O
correct	O
actions	O
,	O
section	O
:	O
Experiments	O
Following	O
the	O
same	O
settings	O
of	O
Chen	O
and	O
Manning	O
chen:2014	O
and	O
Dyer	O
et	O
al	O
lstmacl15	O
we	O
report	O
results	O
in	O
the	O
English	Material
PTB	Material
and	O
Chinese	Material
CTB	Material
-	Material
5	Material
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
parser	Method
in	O
its	O
different	O
configurations	O
.	O

The	O
table	O
also	O
shows	O
the	O
best	O
result	O
obtained	O
with	O
the	O
static	O
oracle	O
(	O
obtained	O
by	O
rerunning	O
Dyer	O
et	O
al	O
.	O

parser	Method
)	O
for	O
the	O
sake	O
of	O
comparison	O
between	O
static	Method
and	Method
dynamic	Method
training	Method
strategies	Method
.	O

The	O
score	O
achieved	O
by	O
the	O
dynamic	Method
oracle	Method
for	O
English	Material
is	O
93.56	O
UAS	Metric
.	O

This	O
is	O
remarkable	O
given	O
that	O
the	O
parser	Method
uses	O
a	O
completely	O
greedy	Method
search	Method
procedure	Method
.	O

Moreover	O
,	O
the	O
Chinese	Material
score	O
establishes	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
using	O
the	O
same	O
settings	O
as	O
chen:2014	O
.	O

The	O
error	Method
-	Method
exploring	Method
dynamic	Method
-	Method
oracle	Method
training	Method
always	O
improves	O
over	O
static	Method
oracle	Method
training	Method
controlling	O
for	O
the	O
transition	Task
system	Task
,	O
but	O
the	O
arc	Method
-	Method
hybrid	Method
system	O
slightly	O
under	O
-	O
performs	O
the	O
arc	Method
-	Method
standard	Method
system	Method
when	O
trained	O
with	O
static	O
oracle	O
.	O

Flattening	O
the	O
sampling	O
distribution	O
(	O
)	O
is	O
especially	O
beneficial	O
when	O
training	O
with	O
pretrained	Task
word	Task
embeddings	Task
.	O

In	O
order	O
to	O
be	O
able	O
to	O
compare	O
with	O
similar	O
greedy	Method
parsers	Method
we	O
report	O
the	O
performance	O
of	O
the	O
parser	Method
on	O
the	O
multilingual	O
treebanks	O
of	O
the	O
CoNLL	Material
2009	Material
shared	Material
task	Material
.	O

Since	O
some	O
of	O
the	O
treebanks	O
contain	O
nonprojective	O
sentences	O
and	O
arc	Method
-	Method
hybrid	Method
does	O
not	O
allow	O
nonprojective	O
trees	O
,	O
we	O
use	O
the	O
pseudo	Method
-	Method
projective	Method
approach	Method
.	O

We	O
used	O
predicted	O
part	O
-	O
of	O
-	O
speech	O
tags	O
provided	O
by	O
the	O
CoNLL	Task
2009	Task
shared	Task
task	Task
organizers	Task
.	O

We	O
also	O
include	O
results	O
with	O
pretrained	O
word	O
embeddings	O
for	O
English	Material
,	O
Chinese	Material
,	O
German	Material
,	O
and	O
Spanish	Material
following	O
the	O
same	O
training	O
setup	O
as	O
Dyer	O
et	O
al	O
.	O

(	O
2015	O
)	O
;	O
for	O
English	Material
and	O
Chinese	Material
we	O
used	O
the	O
same	O
pretrained	O
word	O
embeddings	O
as	O
in	O
Table	O
[	O
reference	O
]	O
,	O
for	O
German	Material
we	O
used	O
the	O
monolingual	O
training	O
data	O
from	O
the	O
WMT	Material
2015	Material
dataset	Material
and	O
for	O
Spanish	Material
we	O
used	O
the	O
Spanish	Material
Gigaword	Material
version	O
3	O
.	O

See	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
Training	O
greedy	Method
parsers	Method
on	O
non	O
-	O
gold	O
outcomes	O
,	O
facilitated	O
by	O
dynamic	Method
oracles	Method
,	O
has	O
been	O
explored	O
by	O
several	O
researchers	O
in	O
different	O
ways	O
.	O

More	O
generally	O
,	O
training	O
greedy	Method
search	Method
systems	Method
by	O
paying	O
attention	O
to	O
the	O
expected	O
classifier	O
behavior	O
during	O
test	O
time	O
has	O
been	O
explored	O
under	O
the	O
imitation	Method
learning	Method
and	O
learning	Method
-	Method
to	Method
-	Method
search	Method
frameworks	Method
.	O

Directly	O
modeling	O
the	O
probability	O
of	O
making	O
a	O
mistake	O
has	O
also	O
been	O
explored	O
for	O
parsing	Task
.	O

Generally	O
,	O
the	O
use	O
of	O
RNNs	Method
to	O
conditionally	Task
predict	Task
actions	Task
in	O
sequence	O
given	O
a	O
history	O
is	O
spurring	O
increased	O
interest	O
in	O
training	O
regimens	O
that	O
make	O
the	O
learned	O
model	O
more	O
robust	O
to	O
test	O
-	O
time	O
prediction	O
errors	O
.	O

Solutions	O
based	O
on	O
curriculum	Method
learning	Method
,	O
expected	Method
loss	Method
training	Method
,	O
and	O
reinforcement	Method
learning	Method
have	O
been	O
proposed	O
.	O

Finally	O
,	O
abandoning	O
greedy	Method
search	Method
in	O
favor	O
of	O
approximate	Method
global	Method
search	Method
offers	O
an	O
alternative	O
solution	O
to	O
the	O
problems	O
with	O
greedy	Method
search	Method
,	O
and	O
has	O
been	O
analyzed	O
as	O
well	O
,	O
including	O
for	O
parsing	Task
.	O

section	O
:	O
Conclusions	O
lstmacl15	O
presented	O
stack	Method
LSTMs	Method
and	O
used	O
them	O
to	O
implement	O
a	O
transition	Method
-	Method
based	Method
dependency	Method
parser	Method
.	O

The	O
parser	Method
uses	O
a	O
greedy	Method
learning	Method
strategy	Method
which	O
potentially	O
provides	O
very	O
high	O
parsing	Task
speed	O
while	O
still	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

We	O
have	O
demonstrated	O
that	O
improvement	O
by	O
training	O
the	O
greedy	Method
parser	Method
on	O
non	O
-	O
gold	O
outcomes	O
;	O
dynamic	Method
oracles	Method
improve	O
the	O
stack	Method
LSTM	Method
parser	Method
,	O
achieving	O
93.56	O
UAS	Metric
for	O
English	Material
,	O
maintaining	O
greedy	Method
search	Method
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
U.	O
S.	O
Army	O
Research	O
Laboratory	O
and	O
the	O
U.	O
S.	O
Army	O
Research	O
Office	O
under	O
contract	O
/	O
grant	O
number	O
W911NF	O
-	O
10	O
-	O
1	O
-	O
0533	O
,	O
and	O
in	O
part	O
by	O
NSF	O
CAREER	O
grant	O
IIS	O
-	O
1054319	O
.	O

Miguel	O
Ballesteros	O
was	O
supported	O
by	O
the	O
European	O
Commission	O
under	O
the	O
contract	O
numbers	O
FP7	O
-	O
ICT	O
-	O
610411	O
(	O
project	O
MULTISENSOR	O
)	O
and	O
H2020	O
-	O
RIA	O
-	O
645012	O
(	O
project	O
KRISTINA	O
)	O
.	O

Yoav	O
Goldberg	O
is	O
supported	O
by	O
the	O
Intel	O
Collaborative	O
Research	O
Institute	O
for	O
Computational	Task
Intelligence	Task
(	O
ICRI	O
-	O
CI	O
)	O
,	O
a	O
Google	O
Research	O
Award	O
and	O
the	O
Israeli	O
Science	O
Foundation	O
(	O
grant	O
number	O
1555	O
/	O
15	O
)	O
.	O

bibliography	O
:	O
References	O
Pairwise	Method
Confusion	Method
for	O
Fine	Material
-	Material
Grained	Material
Visual	Material
Classification	Material
section	O
:	O
Abstract	O
.	O

Fine	Material
-	Material
Grained	Material
Visual	Material
Classification	Material
(	O
FGVC	Material
)	O
datasets	Material
contain	O
small	O
sample	O
sizes	O
,	O
along	O
with	O
significant	O
intra	O
-	O
class	O
variation	O
and	O
interclass	Metric
similarity	Metric
.	O

While	O
prior	O
work	O
has	O
addressed	O
intra	Task
-	Task
class	Task
variation	Task
using	O
localization	Method
and	O
segmentation	Method
techniques	Method
,	O
inter	O
-	O
class	O
similarity	O
may	O
also	O
affect	O
feature	Method
learning	Method
and	O
reduce	O
classification	Task
performance	O
.	O

In	O
this	O
work	O
,	O
we	O
address	O
this	O
problem	O
using	O
a	O
novel	O
optimization	Method
procedure	Method
for	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
neural	Task
network	Task
training	Task
on	O
FGVC	Task
tasks	Task
.	O

Our	O
procedure	O
,	O
called	O
Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
reduces	O
overfitting	O
by	O
intentionally	O
introducing	O
confusion	O
in	O
the	O
activations	O
.	O

With	O
PC	Method
regularization	Method
,	O
we	O
obtain	O
state	O
-	O
ofthe	O
-	O
art	O
performance	O
on	O
six	O
of	O
the	O
most	O
widely	O
-	O
used	O
FGVC	Material
datasets	Material
and	O
demonstrate	O
improved	O
localization	Method
ability	O
.	O

PC	Method
is	O
easy	O
to	O
implement	O
,	O
does	O
not	O
need	O
excessive	O
hyperparameter	Method
tuning	Method
during	O
training	O
,	O
and	O
does	O
not	O
add	O
significant	O
overhead	O
during	O
test	O
time	O
.	O

section	O
:	O
Introduction	O
The	O
Fine	Material
-	Material
Grained	Material
Visual	Material
Classification	Material
(	O
FGVC	Material
)	O
task	O
focuses	O
on	O
differentiating	O
between	O
hard	Task
-	Task
to	Task
-	Task
distinguish	Task
object	Task
classes	Task
,	O
such	O
as	O
species	O
of	O
birds	O
,	O
flowers	O
,	O
or	O
animals	O
;	O
and	O
identifying	O
the	O
makes	O
or	O
models	Task
of	Task
vehicles	Task
.	O

FGVC	Material
datasets	Material
depart	O
from	O
conventional	O
image	Task
classification	Task
in	O
that	O
they	O
typically	O
require	O
expert	O
knowledge	O
,	O
rather	O
than	O
crowdsourcing	O
,	O
for	O
gathering	Task
annotations	Task
.	O

FGVC	Material
datasets	Material
contain	O
images	O
with	O
much	O
higher	O
visual	O
similarity	O
than	O
those	O
in	O
large	O
-	O
scale	O
visual	O
classification	O
(	O
LSVC	O
)	O
.	O

Moreover	O
,	O
FGVC	Material
datasets	Material
have	O
minute	O
inter	O
-	O
class	O
visual	O
differences	O
in	O
addition	O
to	O
the	O
variations	O
in	O
pose	O
,	O
lighting	O
and	O
viewpoint	O
found	O
in	O
LSVC	O
[	O
reference	O
]	O
.	O

Additionally	O
,	O
FGVC	Material
datasets	Material
often	O
exhibit	O
long	O
tails	O
in	O
the	O
data	O
distribution	O
,	O
since	O
the	O
difficulty	O
of	O
obtaining	O
examples	O
of	O
different	O
classes	O
may	O
vary	O
.	O

This	O
combination	O
of	O
small	O
,	O
non	O
-	O
uniform	O
datasets	Material
and	O
subtle	O
inter	O
-	O
class	O
differences	O
makes	O
FGVC	Material
challenging	O
even	O
for	O
powerful	O
deep	Method
learning	Method
algorithms	Method
.	O

Most	O
of	O
the	O
prior	O
work	O
in	O
FGVC	Material
has	O
focused	O
on	O
tackling	O
the	O
intra	O
-	O
class	O
variation	O
in	O
pose	O
,	O
lighting	O
,	O
and	O
viewpoint	O
using	O
localization	Method
techniques	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
by	O
augmenting	O
training	O
datasets	Material
with	O
additional	O
data	O
from	O
the	O
Web	O
[	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
we	O
observe	O
that	O
prior	O
work	O
in	O
FGVC	Material
does	O
not	O
pay	O
much	O
attention	O
to	O
the	O
problems	O
that	O
may	O
arise	O
due	O
to	O
the	O
inter	O
-	O
class	O
visual	O
similarity	O
in	O
the	O
feature	Method
extraction	Method
pipeline	Method
.	O

Similar	O
to	O
LSVC	Task
tasks	Task
,	O
neural	Method
networks	Method
for	O
FGVC	Task
tasks	Task
are	O
typically	O
trained	O
with	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

In	O
LSVC	O
datasets	Material
such	O
as	O
ImageNet	Material
[	O
reference	O
]	O
,	O
strongly	O
discriminative	Method
learning	Method
using	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
is	O
successful	O
in	O
part	O
due	O
to	O
the	O
significant	O
inter	O
-	O
class	O
variation	O
(	O
compared	O
to	O
intraclass	O
variation	O
)	O
,	O
which	O
enables	O
deep	Method
networks	Method
to	O
learn	O
generalized	O
discriminatory	O
features	O
with	O
large	O
amounts	O
of	O
data	O
.	O

We	O
posit	O
that	O
this	O
formulation	O
may	O
not	O
be	O
ideal	O
for	O
FGVC	Material
,	O
which	O
shows	O
smaller	O
visual	O
differences	O
between	O
classes	O
and	O
larger	O
differences	O
within	O
each	O
class	O
than	O
LSVC	Method
.	O

For	O
instance	O
,	O
if	O
two	O
samples	O
in	O
the	O
training	O
set	O
have	O
very	O
similar	O
visual	O
content	O
but	O
different	O
class	O
labels	O
,	O
minimizing	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
will	O
force	O
the	O
neural	Method
network	Method
to	O
learn	O
features	O
that	O
distinguish	O
these	O
two	O
images	O
with	O
high	O
confidence	O
-	O
potentially	O
forcing	O
the	O
network	O
to	O
learn	O
sample	O
-	O
specific	O
artifacts	O
for	O
visually	O
confusing	O
classes	O
in	O
order	O
to	O
minimize	O
training	Metric
error	Metric
.	O

We	O
suspect	O
that	O
this	O
effect	O
would	O
be	O
especially	O
pronounced	O
in	O
FGVC	Material
,	O
since	O
there	O
are	O
fewer	O
samples	O
from	O
which	O
the	O
network	O
can	O
learn	O
generalizable	O
class	O
-	O
specific	O
features	O
.	O

Based	O
on	O
this	O
hypothesis	O
,	O
we	O
propose	O
that	O
introducing	O
confusion	O
in	O
output	O
logit	O
activations	O
during	O
training	O
for	O
an	O
FGVC	Material
task	O
will	O
force	O
the	O
network	O
to	O
learn	O
slightly	O
less	O
discriminative	O
features	O
,	O
thereby	O
preventing	O
it	O
from	O
overfitting	O
to	O
sample	O
-	O
specific	O
artifacts	O
.	O

Specifically	O
,	O
we	O
aim	O
to	O
confuse	O
the	O
network	O
,	O
by	O
minimizing	O
the	O
distance	O
between	O
the	O
predicted	O
probability	O
distributions	O
for	O
random	O
pairs	O
of	O
samples	O
from	O
the	O
training	O
set	O
.	O

To	O
do	O
so	O
,	O
we	O
propose	O
Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
[	O
reference	O
]	O
,	O
a	O
pairwise	Method
algorithm	Method
for	O
training	O
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
end	O
-	O
to	O
-	O
end	O
for	O
fine	Task
-	Task
grained	Task
visual	Task
classification	Task
.	O

In	O
Pairwise	Method
Confusion	Method
,	O
we	O
construct	O
a	O
Siamese	Method
neural	Method
network	Method
trained	O
with	O
a	O
novel	O
loss	Method
function	Method
that	O
attempts	O
to	O
bring	O
class	O
conditional	O
probability	O
distributions	O
closer	O
to	O
each	O
other	O
.	O

Using	O
Pairwise	Method
Confusion	Method
with	O
a	O
standard	O
network	Method
architecture	Method
like	O
DenseNet	Method
[	O
reference	O
]	O
or	O
ResNet	Method
[	O
reference	O
]	O
as	O
a	O
base	O
network	O
,	O
we	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
of	O
the	O
most	O
widely	O
-	O
used	O
fine	Task
-	Task
grained	Task
recognition	Task
datasets	Task
,	O
improving	O
over	O
the	O
previous	O
-	O
best	O
published	O
methods	O
by	O
1.86	O
%	O
on	O
average	O
.	O

In	O
addition	O
,	O
PC	Method
-	O
trained	O
networks	O
show	O
better	O
localization	Method
performance	O
as	O
compared	O
to	O
standard	O
networks	O
.	O

Pairwise	Method
Confusion	Method
is	O
simple	O
to	O
implement	O
,	O
has	O
no	O
added	O
overhead	O
in	O
training	O
or	O
prediction	Metric
time	Metric
,	O
and	O
provides	O
performance	O
improvements	O
both	O
in	O
FGVC	Task
tasks	Task
and	O
other	O
tasks	O
that	O
involve	O
transfer	Method
learning	Method
with	O
small	O
amounts	O
of	O
training	O
data	O
.	O

section	O
:	O
Related	O
Work	O
Fine	Material
-	Material
Grained	Material
Visual	Material
Classification	Material
:	O
Early	O
FGVC	Material
research	O
focused	O
on	O
methods	O
to	O
train	O
with	O
limited	O
labeled	O
data	O
and	O
traditional	O
image	O
features	O
.	O

Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
combined	O
strongly	O
discriminative	O
image	O
patches	O
with	O
randomization	Method
techniques	Method
to	O
prevent	O
overfitting	O
.	O

Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
subsequently	O
utilized	O
template	Method
matching	Method
to	O
avoid	O
the	O
need	O
for	O
a	O
large	O
number	O
of	O
annotations	O
.	O

Recently	O
,	O
improved	O
localization	Method
of	O
the	O
target	O
object	O
in	O
training	O
images	O
has	O
been	O
shown	O
to	O
be	O
useful	O
for	O
FGVC	Material
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Zhang	O
et	O
al	O
.	O

[	O
reference	O
]	O
utilize	O
part	Method
-	Method
based	Method
Region	Method
-	Method
CNNs	Method
[	O
reference	O
]	O
to	O
perform	O
finer	O
localization	Method
.	O

Spatial	Method
Transformer	Method
Networks	Method
[	O
reference	O
]	O
show	O
that	O
learning	O
a	O
content	Method
-	Method
based	Method
affine	Method
transformation	Method
layer	Method
improves	O
FGVC	Material
performance	O
.	O

Pose	Method
-	Method
normalized	Method
CNNs	Method
have	O
also	O
been	O
shown	O
to	O
be	O
effective	O
at	O
FGVC	Material
[	O
reference	O
][	O
reference	O
]	O
.	O

Model	Method
ensembling	Method
and	O
boosting	Method
has	O
also	O
improved	O
performance	O
on	O
FGVC	Material
[	O
reference	O
]	O
.	O

Lin	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
Bilinear	Method
Pooling	Method
,	O
which	O
combines	O
pairwise	O
local	O
feature	O
sets	O
and	O
improves	O
classification	Task
performance	O
.	O

Bilinear	Method
Pooling	Method
has	O
been	O
extended	O
by	O
Gao	O
et	O
al	O
.	O

[	O
reference	O
]	O
using	O
a	O
compact	Method
bilinear	Method
representation	Method
and	O
Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
using	O
a	O
general	O
Kernel	Method
-	Method
based	Method
pooling	Method
framework	Method
that	O
captures	O
higher	O
-	O
order	O
interactions	O
of	O
features	O
.	O

Pairwise	Task
Learning	Task
:	O
Chopra	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
a	O
Siamese	Method
neural	Method
network	Method
for	O
handwriting	Task
recognition	Task
.	O

Parikh	O
and	O
Grauman	O
[	O
reference	O
]	O
developed	O
a	O
pairwise	Method
ranking	Method
scheme	Method
for	O
relative	Task
attribute	Task
learning	Task
.	O

Subsequently	O
,	O
pairwise	Method
neural	Method
network	Method
models	Method
have	O
become	O
common	O
for	O
attribute	Task
modeling	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Learning	O
from	O
Label	O
Confusion	O
:	O
Our	O
method	O
aims	O
to	O
improve	O
classification	Task
performance	O
by	O
introducing	O
confusion	O
within	O
the	O
output	O
labels	O
.	O

Prior	O
work	O
in	O
this	O
area	O
includes	O
methods	O
that	O
utilize	O
label	O
noise	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
and	O
data	O
noise	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
in	O
training	O
.	O

Krause	O
et	O
al	O
.	O

[	O
reference	O
]	O
utilized	O
noisy	O
training	O
data	O
for	O
FGVC	Material
.	O

Neelakantan	O
et	O
al	O
.	O

[	O
reference	O
]	O
added	O
noise	O
to	O
the	O
gradient	O
during	O
training	O
to	O
improve	O
generalization	Task
performance	O
in	O
very	Task
deep	Task
networks	Task
.	O

Szegedy	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
label	Method
-	Method
smoothing	Method
regularization	Method
for	O
training	O
deep	Method
Inception	Method
models	Method
.	O

In	O
this	O
paper	O
,	O
we	O
bring	O
together	O
concepts	O
from	O
pairwise	Method
learning	Method
and	O
label	Task
confusion	Task
and	O
take	O
a	O
step	O
towards	O
solving	O
the	O
problems	O
of	O
overfitting	O
and	O
samplespecific	O
artifacts	O
when	O
training	O
neural	Method
networks	Method
for	O
FGVC	Task
tasks	Task
.	O

section	O
:	O
Method	O
FGVC	Material
datasets	Material
in	O
computer	Task
vision	Task
are	O
orders	O
of	O
magnitude	O
smaller	O
than	O
LSVC	O
datasets	Material
and	O
contain	O
greater	O
imbalance	O
across	O
classes	O
(	O
see	O
Table	O
1	O
)	O
.	O

Moreover	O
,	O
the	O
samples	O
of	O
a	O
class	O
are	O
not	O
accurately	O
representative	O
of	O
the	O
complete	O
variation	O
in	O
the	O
visual	O
class	O
itself	O
.	O

The	O
smaller	O
dataset	O
size	O
can	O
result	O
in	O
overfitting	O
when	O
training	O
deep	Method
neural	Method
architectures	Method
with	O
large	O
number	O
of	O
parameters	O
-	O
even	O
with	O
preliminary	O
layers	O
being	O
frozen	O
.	O

In	O
addition	O
,	O
the	O
training	O
data	O
may	O
not	O
be	O
completely	O
representative	O
of	O
the	O
real	O
-	O
world	O
data	O
,	O
with	O
issues	O
such	O
as	O
more	O
abundant	O
sampling	O
for	O
certain	O
classes	O
.	O

For	O
example	O
,	O
in	O
FGVC	Material
of	O
birds	O
,	O
certain	O
species	O
from	O
geographically	O
accessible	O
areas	O
may	O
be	O
overrepresented	O
in	O
the	O
training	O
dataset	O
.	O

As	O
a	O
result	O
,	O
the	O
neural	Method
network	Method
may	O
learn	O
to	O
latch	O
on	O
to	O
sample	O
-	O
specific	O
artifacts	O
in	O
the	O
image	O
,	O
instead	O
of	O
learning	O
a	O
versatile	Method
representation	Method
for	O
the	O
target	O
object	O
.	O

We	O
aim	O
to	O
solve	O
both	O
of	O
these	O
issues	O
in	O
FGVC	Material
(	O
overfitting	O
and	O
sample	Task
-	Task
specific	Task
artifacts	Task
)	O
by	O
bringing	O
the	O
different	O
class	O
-	O
conditional	O
probability	O
distributions	O
closer	O
together	O
and	O
confusing	O
the	O
deep	Method
network	Method
,	O
subsequently	O
reducing	O
its	O
prediction	Metric
over	Metric
-	Metric
confidence	Metric
,	O
thus	O
improving	O
generalization	Metric
performance	Metric
.	O

Let	O
us	O
formalize	O
the	O
idea	O
of	O
"	O
confusing	O
"	O
the	O
conditional	O
probability	O
distributions	O
.	O

Consider	O
the	O
conditional	O
probability	O
distributions	O
for	O
two	O
input	O
images	O
x	O
1	O
and	O
x	O
2	O
,	O
which	O
can	O
be	O
given	O
by	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
and	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
respectively	O
.	O

For	O
a	O
classification	Task
problem	Task
with	O
N	O
output	O
classes	O
,	O
each	O
of	O
these	O
distributions	O
is	O
an	O
N	O
-	O
dimensional	O
vector	O
,	O
with	O
each	O
element	O
i	O
denoting	O
the	O
belief	O
of	O
the	O
classifier	O
in	O
class	O
y	O
i	O
given	O
input	O
x.	O
If	O
we	O
wish	O
to	O
confuse	O
the	O
class	O
outputs	O
of	O
the	O
classifier	Method
for	O
the	O
pair	O
x	O
1	O
and	O
x	O
2	O
,	O
we	O
should	O
learn	O
parameters	O
θ	O
that	O
bring	O
these	O
conditional	O
probability	O
distributions	O
"	O
closer	O
"	O
under	O
some	O
distance	Metric
metric	Metric
,	O
that	O
is	O
,	O
make	O
the	O
predictions	O
for	O
x	O
1	O
and	O
x	O
2	O
similar	O
.	O

While	O
KL	Method
-	O
divergence	O
might	O
seem	O
to	O
be	O
a	O
reasonable	O
choice	O
to	O
design	O
a	O
loss	O
function	O
for	O
optimizing	O
the	O
distance	O
between	O
conditional	O
probability	O
distributions	O
,	O
in	O
Section	O
3.1	O
,	O
we	O
show	O
that	O
it	O
is	O
infeasible	O
to	O
train	O
a	O
neural	Method
network	Method
when	O
using	O
KL	Method
-	O
divergence	O
as	O
a	O
regularizer	Method
.	O

Therefore	O
,	O
we	O
introduce	O
the	O
Euclidean	O
Distance	O
between	O
distributions	O
as	O
a	O
metric	O
for	O
confusion	Task
in	O
Sections	O
3.2	O
and	O
3.3	O
and	O
describe	O
neural	Method
network	Method
training	Method
with	O
this	O
metric	O
in	O
Section	O
3.4	O
.	O

section	O
:	O
Symmetric	O
KL	Method
-	O
divergence	O
or	O
Jeffrey	Method
's	Method
Divergence	Method
The	O
most	O
prevalent	O
method	O
to	O
measure	O
dissimilarity	O
of	O
one	O
probability	O
distribution	O
from	O
another	O
is	O
to	O
use	O
the	O
Kullback	Method
-	Method
Liebler	Method
(	O
KL	Method
)	O
divergence	O
.	O

However	O
,	O
the	O
standard	O
KL	Method
-	O
divergence	O
can	O
not	O
serve	O
our	O
purpose	O
owing	O
to	O
its	O
asymmetric	O
nature	O
.	O

This	O
could	O
be	O
remedied	O
by	O
using	O
the	O
symmetric	O
KL	Method
-	O
divergence	O
,	O
defined	O
for	O
two	O
probability	O
distributions	O
P	O
,	O
Q	O
with	O
mass	O
functions	O
p	O
(	O
·	O
)	O
,	O
q	O
(	O
·	O
)	O
(	O
for	O
events	O
u	O
∈	O
U	O
)	O
:	O
This	O
symmetrized	O
version	O
of	O
KL	Method
-	O
divergence	O
,	O
known	O
as	O
Jeffrey	O
's	O
divergence	O
[	O
reference	O
]	O
,	O
is	O
a	O
measure	O
of	O
the	O
average	Metric
relative	Metric
entropy	Metric
between	O
two	O
probability	O
distributions	O
[	O
reference	O
]	O
.	O

For	O
our	O
model	O
parameterized	O
by	O
θ	O
,	O
for	O
samples	O
x	O
1	O
and	O
x	O
2	O
,	O
the	O
Jeffrey	O
's	O
divergence	O
can	O
be	O
written	O
as	O
:	O
Jeffrey	Method
's	Method
divergence	Method
satisfies	O
all	O
of	O
our	O
basic	O
requirements	O
of	O
a	O
symmetric	Metric
divergence	Metric
metric	Metric
between	O
probability	O
distributions	O
,	O
and	O
therefore	O
could	O
be	O
included	O
as	O
a	O
regularizing	Method
term	Method
while	O
training	O
with	O
cross	O
-	O
entropy	O
,	O
to	O
achieve	O
our	O
desired	O
confusion	O
.	O

However	O
,	O
when	O
we	O
learn	O
model	Method
parameters	Method
using	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	Method
,	O
it	O
can	O
be	O
difficult	O
to	O
train	O
,	O
especially	O
if	O
our	O
distributions	O
P	O
,	O
Q	O
have	O
mass	O
concentrated	O
on	O
different	O
events	O
.	O

This	O
can	O
be	O
seen	O
in	O
Equation	O
2	O
.	O

Consider	O
Jeffrey	Method
's	Method
divergence	Method
with	O
N	O
=	O
2	O
classes	O
,	O
and	O
that	O
x	O
1	O
belongs	O
to	O
class	O
1	O
,	O
and	O
x	O
2	O
belongs	O
to	O
class	O
2	O
.	O

If	O
the	O
model	O
parameters	O
θ	O
are	O
such	O
that	O
it	O
correctly	O
identifies	O
both	O
x	O
1	O
and	O
x	O
2	O
by	O
training	O
using	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
,	O
p	O
θ	O
(	O
y	O
1	O
|x	O
1	O
)	O
=	O
1	O
−	O
δ	O
1	O
and	O
p	O
θ	O
(	O
y	O
2	O
|x	O
2	O
)	O
=	O
1	O
−	O
δ	O
2	O
,	O
where	O
0	O
<	O
δ	O
1	O
,	O
δ	O
2	O
<	O
1	O
2	O
(	O
since	O
the	O
classifier	Method
outputs	O
correct	O
predictions	O
for	O
the	O
input	O
images	O
)	O
,	O
we	O
can	O
show	O
:	O
Please	O
see	O
the	O
supplementary	O
material	O
for	O
an	O
expanded	O
proof	O
.	O

As	O
training	O
progresses	O
with	O
these	O
labels	O
,	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
will	O
motivate	O
the	O
values	O
of	O
δ	O
1	O
and	O
δ	O
2	O
to	O
become	O
closer	O
to	O
zero	O
(	O
but	O
never	O
equaling	O
zero	O
,	O
since	O
the	O
probability	O
outputs	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
are	O
the	O
outputs	O
from	O
a	O
softmax	Method
)	Method
.	O

As	O
(	O
δ	O
1	O
,	O
δ	O
2	O
)	O
→	O
(	O
0	O
+	O
,	O
0	O
+	O
)	O
,	O
the	O
second	O
term	O
−	O
log	O
(	O
δ	O
1	O
δ	O
2	O
)	O
on	O
the	O
R.H.S.	O
of	O
inequality	O
(	O
4	O
)	O
typically	O
grows	O
whereas	O
(	O
1	O
−	O
δ	O
1	O
−	O
δ	O
2	O
)	O
approaches	O
1	O
,	O
which	O
makes	O
D	O
J	O
(	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
)	O
larger	O
as	O
the	O
predictions	O
get	O
closer	O
to	O
the	O
true	O
labels	O
.	O

In	O
practice	O
,	O
we	O
see	O
that	O
training	O
with	O
D	O
J	O
(	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
)	O
as	O
a	O
regularizer	Method
term	Method
diverges	O
,	O
unless	O
a	O
very	O
small	O
regularizing	O
parameter	O
is	O
chosen	O
,	O
which	O
removes	O
the	O
effect	O
of	O
regularization	O
altogether	O
.	O

A	O
natural	O
question	O
that	O
can	O
arise	O
from	O
this	O
analysis	O
is	O
that	O
cross	Method
-	Method
entropy	Method
training	Method
itself	O
involves	O
optimizing	O
KL	Method
-	O
divergence	O
between	O
the	O
target	O
label	O
distribution	O
and	O
the	O
model	O
's	O
predictions	O
,	O
however	O
no	O
such	O
divergence	O
occurs	O
.	O

This	O
is	O
because	O
cross	O
-	O
entropy	O
involves	O
only	O
one	O
direction	O
of	O
the	O
KL	Method
-	O
divergence	O
,	O
and	O
the	O
target	O
distribution	O
has	O
all	O
the	O
mass	O
concentrated	O
at	O
one	O
event	O
(	O
the	O
correct	O
label	O
)	O
.	O

Since	O
(	O
x	O
log	O
x	O
)	O
|	O
x=0	O
=	O
0	O
,	O
for	O
predicted	O
label	O
vector	O
y	O
with	O
correct	O
label	O
class	O
c	O
,	O
this	O
simplifies	O
the	O
cross	Metric
-	Metric
entropy	Metric
error	Metric
L	Metric
CE	Metric
(	O
p	O
θ	O
(	O
y|x	O
)	O
,	O
y	O
)	O
to	O
be	O
:	O
This	O
formulation	O
does	O
not	O
diverge	O
as	O
the	O
model	O
trains	O
,	O
i.e.	O
p	O
θ	O
(	O
y	O
c	O
|x	O
)	O
→	O
1	O
.	O

In	O
some	O
cases	O
where	O
label	O
noise	O
is	O
added	O
to	O
the	O
label	O
vector	O
(	O
such	O
as	O
label	Method
smoothing	Method
[	O
reference	O
][	O
reference	O
]	O
)	O
,	O
the	O
label	O
noise	O
is	O
a	O
fixed	O
constant	O
and	O
not	O
approaching	O
zero	O
(	O
as	O
in	O
the	O
case	O
of	O
Jeffery	O
's	O
divergence	O
between	O
model	O
predictions	O
)	O
and	O
is	O
hence	O
feasible	O
to	O
train	O
.	O

Thus	O
,	O
Jeffrey	O
's	O
Divergence	O
or	O
symmetric	O
KL	Method
-	O
divergence	O
,	O
while	O
a	O
seemingly	O
natural	O
choice	O
,	O
can	O
not	O
be	O
used	O
to	O
train	O
a	O
neural	Method
network	Method
with	O
SGD	Method
.	O

This	O
motivates	O
us	O
to	O
look	O
for	O
an	O
alternative	O
metric	O
to	O
measure	O
"	O
confusion	O
"	O
between	O
conditional	O
probability	O
distributions	O
.	O

section	O
:	O
Euclidean	O
Distance	O
as	O
Confusion	O
Since	O
the	O
conditional	O
probability	O
distribution	O
over	O
N	O
classes	O
is	O
an	O
element	O
within	O
R	O
N	O
on	O
the	O
unit	O
simplex	O
,	O
we	O
can	O
consider	O
the	O
Euclidean	O
distance	O
to	O
be	O
a	O
metric	O
of	O
"	O
confusion	O
"	O
between	O
two	O
conditional	Method
probability	Method
distributions	Method
.	O

Analogous	O
to	O
the	O
previous	O
setting	O
,	O
we	O
define	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
·	O
,	O
·	O
)	O
for	O
a	O
pair	O
of	O
inputs	O
x	O
1	O
,	O
x	O
2	O
with	O
model	O
parameters	O
θ	O
as	O
:	O
(	O
5	O
)	O
Unlike	O
Jeffrey	O
's	O
Divergence	O
,	O
Euclidean	O
Confusion	O
does	O
not	O
diverge	O
when	O
used	O
as	O
a	O
regularization	Method
term	Method
with	O
cross	O
-	O
entropy	O
.	O

However	O
,	O
to	O
verify	O
this	O
unconventional	O
choice	O
for	O
a	O
distance	O
metric	O
between	O
probability	O
distributions	O
,	O
we	O
prove	O
some	O
properties	O
that	O
relate	O
Euclidean	Metric
Confusion	Metric
to	O
existing	O
divergence	Metric
measures	Metric
.	O

Lemma	O
1	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
P	O
,	O
Q	O
)	O
is	O
a	O
lower	O
bound	O
for	O
the	O
Jeffrey	Metric
's	Metric
Divergence	Metric
D	O
J	O
(	O
P	O
,	O
Q	O
)	O
for	O
probability	O
measures	O
P	O
,	O
Q.	O
Proof	O
.	O

This	O
follows	O
from	O
Pinsker	O
's	O
Inequality	O
and	O
the	O
relationship	O
between	O
1	O
and	O
2	O
norms	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

By	O
Lemma	O
1	O
,	O
we	O
can	O
see	O
that	O
the	O
Euclidean	O
Confusion	O
is	O
a	O
conservative	O
estimate	O
for	O
Jeffrey	Metric
's	Metric
divergence	Metric
,	O
the	O
earlier	O
proposed	O
divergence	Metric
measure	Metric
.	O

For	O
finite	O
probability	O
spaces	O
,	O
the	O
Total	Metric
Variation	Metric
Distance	Metric
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
2	O
=	O
1	O
2	O
P	O
−	O
Q	O
1	O
is	O
also	O
a	O
measure	O
of	O
interest	O
.	O

However	O
,	O
due	O
to	O
its	O
non	O
-	O
differentiable	O
nature	O
,	O
it	O
is	O
unsuitable	O
for	O
our	O
case	O
.	O

Nevertheless	O
,	O
we	O
can	O
relate	O
the	O
Euclidean	Metric
Confusion	Metric
and	O
Total	Metric
Variation	Metric
Distance	Metric
by	O
the	O
following	O
result	O
.	O

Lemma	O
2	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
P	O
,	O
Q	O
)	O
is	O
bounded	O
by	O
4D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
2	O
for	O
probability	O
measures	O
P	O
,	O
Q.	O
Proof	O
.	O

This	O
follows	O
directly	O
from	O
the	O
relationship	O
between	O
1	O
and	O
2	O
norms	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

section	O
:	O
Euclidean	O
Confusion	O
for	O
Point	O
Sets	O
In	O
a	O
standard	O
classification	Task
setting	Task
with	O
N	O
classes	O
,	O
we	O
consider	O
a	O
training	O
set	O
with	O
m	O
=	O
N	O
i=1	O
m	O
i	O
training	O
examples	O
,	O
where	O
m	O
i	O
denotes	O
the	O
number	O
of	O
training	O
samples	O
for	O
class	O
i.	O
For	O
this	O
setting	O
,	O
we	O
can	O
write	O
the	O
total	O
Euclidean	O
Confusion	O
between	O
points	O
of	O
classes	O
i	O
and	O
j	O
as	O
the	O
average	O
of	O
the	O
Euclidean	O
Confusion	O
between	O
all	O
pairs	O
of	O
points	O
belonging	O
to	O
those	O
two	O
classes	O
.	O

For	O
simplicity	O
of	O
notation	O
,	O
let	O
us	O
denote	O
the	O
set	O
of	O
conditional	O
probability	O
distributions	O
of	O
all	O
training	O
points	O
belonging	O
to	O
class	O
i	O
for	O
a	O
model	O
parameterized	O
by	O
θ	O
as	O
Then	O
,	O
for	O
a	O
model	O
parameterized	O
by	O
θ	O
,	O
the	O
Euclidean	Metric
Confusion	Metric
is	O
given	O
by	O
:	O
We	O
can	O
simplify	O
this	O
equation	O
by	O
assuming	O
an	O
equal	O
number	O
of	O
points	O
n	O
per	O
class	O
:	O
This	O
form	O
of	O
the	O
Euclidean	O
Confusion	O
between	O
the	O
two	O
sets	O
of	O
points	O
gives	O
us	O
an	O
interesting	O
connection	O
with	O
another	O
popular	O
distance	Metric
metric	Metric
over	O
probability	O
distributions	O
,	O
known	O
as	O
the	O
Energy	O
Distance	O
[	O
reference	O
]	O
.	O

Introduced	O
by	O
Gabor	O
Szekely	O
[	O
reference	O
]	O
,	O
the	O
Energy	O
Distance	O
D	O
EN	O
(	O
F	O
,	O
G	O
)	O
between	O
two	O
cumulative	Method
probability	Method
distribution	Method
functions	Method
F	O
and	O
G	O
with	O
random	O
vectors	O
X	O
and	O
Y	O
in	O
R	O
N	O
can	O
be	O
given	O
by	O
where	O
(	O
X	O
,	O
X	O
,	O
Y	O
,	O
Y	O
)	O
are	O
independent	O
,	O
and	O
X	O
∼	O
F	O
,	O
X	O
∼	O
F	O
,	O
Y	O
∼	O
G	O
,	O
Y	O
∼	O
G.	O
If	O
we	O
consider	O
the	O
sets	O
S	O
i	O
and	O
S	O
j	O
,	O
with	O
a	O
uniform	O
probability	O
of	O
selecting	O
any	O
of	O
the	O
n	O
points	O
in	O
each	O
of	O
these	O
sets	O
,	O
then	O
we	O
obtain	O
the	O
following	O
results	O
.	O

Lemma	O
3	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
:	O
where	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
the	O
Energy	O
Distance	O
under	O
Euclidean	O
norm	O
between	O
S	O
i	O
and	O
S	O
j	O
(	O
parameterized	O
by	O
θ	O
)	O
,	O
and	O
random	O
vectors	O
are	O
selected	O
with	O
uniform	O
probability	O
in	O
both	O
S	O
i	O
and	O
S	O
j	O
.	O

Proof	O
.	O

This	O
follows	O
from	O
the	O
definition	O
of	O
Energy	O
Distance	O
with	O
uniform	O
probability	O
of	O
sampling	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

Corollary	O
1	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
,	O
we	O
have	O
:	O
with	O
equality	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

Proof	O
.	O

This	O
follows	O
from	O
the	O
fact	O
that	O
the	O
Energy	O
Distance	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
0	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

The	O
complete	O
version	O
of	O
the	O
proof	O
is	O
included	O
in	O
the	O
supplement	O
.	O

With	O
these	O
results	O
,	O
we	O
restrict	O
the	O
behavior	O
of	O
Euclidean	O
Confusion	O
within	O
two	O
well	O
-	O
defined	O
conventional	O
probability	Metric
distance	Metric
measures	Metric
,	O
the	O
Jeffrey	Metric
's	Metric
divergence	Metric
and	O
Energy	Method
Distance	Method
.	O

One	O
might	O
consider	O
optimizing	O
the	O
Energy	O
Distance	O
directly	O
,	O
due	O
to	O
its	O
similar	O
formulation	O
and	O
the	O
fact	O
that	O
we	O
uniformly	O
sample	O
points	O
during	O
training	O
with	O
SGD	Method
.	O

However	O
,	O
the	O
Energy	O
Distance	O
additionally	O
includes	O
the	O
two	O
terms	O
that	O
account	O
for	O
the	O
negative	O
of	O
the	O
average	O
all	O
-	O
pairs	O
distances	O
between	O
points	O
in	O
S	O
i	O
and	O
S	O
j	O
respectively	O
,	O
which	O
we	O
do	O
not	O
want	O
to	O
maximize	O
,	O
since	O
we	O
do	O
not	O
wish	O
to	O
push	O
points	O
within	O
the	O
same	O
class	O
further	O
apart	O
.	O

Therefore	O
,	O
we	O
proceed	O
with	O
our	O
measure	O
of	O
Euclidean	Metric
Confusion	Metric
.	O

section	O
:	O
Learning	Task
with	O
Gradient	Method
Descent	Method
We	O
proceed	O
to	O
learn	O
parameters	O
θ	O
*	O
for	O
a	O
neural	Method
network	Method
,	O
with	O
the	O
following	O
learning	Metric
objective	Metric
function	Metric
for	O
a	O
pair	O
of	O
input	O
points	O
,	O
motivated	O
by	O
the	O
formulation	O
.	O

We	O
employ	O
a	O
Siamese	Method
-	Method
like	Method
architecture	Method
,	O
with	O
individual	O
cross	Method
entropy	Method
calculations	Method
for	O
each	O
branch	O
,	O
followed	O
by	O
a	O
joint	Method
energy	Method
-	Method
distance	Method
minimization	Method
loss	Method
.	O

We	O
split	O
each	O
incoming	O
batch	O
of	O
samples	O
into	O
two	O
mini	O
-	O
batches	O
,	O
and	O
feed	O
the	O
network	O
pairwise	O
samples	O
.	O

of	O
Euclidean	O
Confusion	O
:	O
This	O
objective	Metric
function	Metric
can	O
be	O
explained	O
as	O
:	O
for	O
each	O
point	O
in	O
the	O
training	O
set	O
,	O
we	O
randomly	O
select	O
another	O
point	O
from	O
a	O
different	O
class	O
and	O
calculate	O
the	O
individual	O
cross	Metric
-	Metric
entropy	Metric
losses	Metric
and	O
Euclidean	Metric
Confusion	Metric
until	O
all	O
pairs	O
have	O
been	O
exhausted	O
.	O

For	O
each	O
point	O
in	O
the	O
training	O
dataset	O
,	O
there	O
are	O
n·	O
(	O
N	O
−	O
1	O
)	O
valid	O
choices	O
for	O
the	O
other	O
point	O
,	O
giving	O
us	O
a	O
total	O
of	O
n	O
2	O
·	O
N	O
·	O
(	O
N	O
−	O
1	O
)	O
possible	O
pairs	O
.	O

In	O
practice	O
,	O
we	O
find	O
that	O
we	O
do	O
not	O
need	O
to	O
exhaust	O
all	O
combinations	O
for	O
effective	Task
learning	Task
using	O
gradient	Method
descent	Method
,	O
and	O
in	O
fact	O
we	O
observe	O
that	O
convergence	Metric
is	O
achieved	O
far	O
before	O
all	O
observations	O
are	O
observed	O
.	O

We	O
simplify	O
our	O
formulation	O
instead	O
by	O
using	O
the	O
following	O
procedure	O
described	O
in	O
Algorithm	O
1	O
.	O

Training	O
Procedure	O
:	O
As	O
described	O
in	O
Algorithm	O
1	O
,	O
our	O
learning	Method
procedure	Method
is	O
a	O
slightly	O
modified	O
version	O
of	O
the	O
standard	O
SGD	Method
.	O

We	O
randomly	O
permute	O
the	O
training	O
set	O
twice	O
,	O
and	O
then	O
for	O
each	O
pair	O
of	O
points	O
in	O
the	O
training	O
set	O
,	O
add	O
Euclidean	O
Confusion	O
only	O
if	O
the	O
samples	O
belong	O
to	O
different	O
classes	O
.	O

This	O
form	O
of	O
sampling	O
approximates	O
the	O
exhaustive	O
Euclidean	O
Confusion	O
,	O
with	O
some	O
points	O
with	O
regular	Method
gradient	Method
descent	Method
,	O
which	O
in	O
practice	O
does	O
not	O
alter	O
the	O
performance	O
.	O

Moreover	O
,	O
convergence	Metric
is	O
achieved	O
after	O
only	O
a	O
fraction	O
of	O
all	O
the	O
possible	O
pairs	O
are	O
observed	O
.	O

Formally	O
,	O
we	O
wish	O
to	O
model	O
the	O
conditional	O
probability	O
distribution	O
p	O
θ	O
(	O
y|x	O
)	O
over	O
the	O
p	O
classes	O
for	O
function	O
f	O
(	O
x	O
;	O
θ	O
)	O
=	O
p	O
θ	O
(	O
y|x	O
)	O
parameterized	O
by	O
model	O
parameters	O
θ	O
.	O

Given	O
our	O
optimization	Method
procedure	Method
,	O
we	O
can	O
rewrite	O
the	O
total	O
loss	O
for	O
a	O
pair	O
of	O
section	O
:	O
Algorithm	O
1	O
Training	O
Using	O
Euclidean	Method
Confusion	Method
where	O
,	O
γ	O
(	O
y	O
1	O
,	O
y	O
2	O
)	O
=	O
1	O
when	O
y	O
i	O
=	O
y	O
j	O
,	O
and	O
0	O
otherwise	O
.	O

We	O
denote	O
training	O
with	O
this	O
general	O
architecture	O
with	O
the	O
term	O
Pairwise	Method
Confusion	Method
or	O
PC	Method
for	O
short	O
.	O

Specifically	O
,	O
we	O
train	O
a	O
Siamese	Method
-	Method
like	Method
neural	Method
network	Method
[	O
reference	O
]	O
with	O
shared	O
weights	O
,	O
training	O
each	O
network	O
individually	O
using	O
cross	Method
-	Method
entropy	Method
,	O
and	O
add	O
the	O
Euclidean	Metric
Confusion	Metric
loss	Metric
between	O
the	O
conditional	O
probability	O
distributions	O
obtained	O
from	O
each	O
network	O
(	O
Figure	O
1	O
)	O
.	O

During	O
training	Task
,	O
we	O
split	O
an	O
incoming	O
batch	O
of	O
training	O
samples	O
into	O
two	O
parts	O
,	O
and	O
evaluating	O
cross	Metric
-	Metric
entropy	Metric
on	O
each	O
sub	O
-	O
batch	O
identically	O
,	O
followed	O
by	O
a	O
pairwise	O
loss	O
term	O
calculated	O
for	O
corresponding	O
pairs	O
of	O
samples	O
across	O
batches	O
.	O

During	O
testing	O
,	O
only	O
one	O
branch	O
of	O
the	O
network	O
is	O
active	O
,	O
and	O
generates	O
output	O
predictions	O
for	O
the	O
input	O
image	O
.	O

As	O
a	O
result	O
,	O
implementing	O
this	O
method	O
does	O
not	O
introduce	O
any	O
significant	O
computational	Metric
overhead	Metric
during	O
testing	O
.	O

section	O
:	O
CNN	Method
Architectures	Method
We	O
experiment	O
with	O
VGGNet	Method
[	O
reference	O
]	O
,	O
GoogLeNet	Method
[	O
reference	O
]	O
,	O
ResNets	Method
[	O
reference	O
]	O
,	O
and	O
DenseNets	Method
[	O
reference	O
]	O
as	O
base	O
architectures	O
for	O
the	O
Siamese	Method
network	Method
trained	O
with	O
PC	Method
to	O
demonstrate	O
that	O
our	O
method	O
is	O
insensitive	O
to	O
the	O
choice	O
of	O
source	Method
architecture	Method
.	O

section	O
:	O
Experimental	O
Details	O
We	O
perform	O
all	O
experiments	O
using	O
Caffe	Method
[	O
reference	O
]	O
or	O
PyTorch	Method
[	O
reference	O
]	O
over	O
a	O
cluster	O
of	O
NVIDIA	Method
Titan	Method
X	Method
,	O
Tesla	Method
K40c	Method
and	O
GTX	Method
1080	Method
GPUs	Method
.	O

Our	O
code	O
and	O
models	O
are	O
available	O
at	O
github.com	O
/	O
abhimanyudubey	O
/	O
confusion	O
.	O

Next	O
,	O
we	O
provide	O
brief	O
descriptions	O
of	O
the	O
various	O
datasets	Material
used	O
in	O
our	O
paper	O
.	O

Table	O
2	O
.	O

Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
widelyused	O
fine	Task
-	Task
grained	Task
visual	Task
classification	Task
datasets	Material
(	O
A	O
-	O
F	O
)	O
.	O

Improvement	O
over	O
the	O
baseline	O
model	O
is	O
reported	O
as	O
(	O
∆	O
)	O
.	O

All	O
results	O
averaged	O
over	O
5	O
trials	O
.	O

(	O
A	O
)	O
CUB	Method
-	Method
200	Method
-	Method
2011	Method
Method	Method
Top	O
-	O
1	O
∆	O
Gao	O
et	O
al	O
.	O

[	O
reference	O
]	O
84.00	O
-	O
STN	O
[	O
reference	O
]	O
84.10	O
-	O
Zhang	O
et	O
al	O
.	O

[	O
reference	O
]	O
84.50	O
-	O
Lin	O
et	O
al	O
.	O

[	O
reference	O
]	O
85.80	O
-	O
Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
86	O
.	O

car	O
make	O
,	O
model	O
,	O
and	O
year	O
.	O

The	O
Aircraft	Material
dataset	Material
is	O
a	O
set	O
of	O
10	O
,	O
000	O
images	O
across	O
100	O
classes	O
denoting	O
a	O
fine	O
-	O
grained	O
set	O
of	O
airplanes	O
of	O
different	O
varieties	O
[	O
reference	O
]	O
.	O

These	O
datasets	Material
contain	O
(	O
i	O
)	O
large	O
visual	O
diversity	O
in	O
each	O
class	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
(	O
ii	O
)	O
visually	O
similar	O
,	O
often	O
confusing	O
samples	O
belonging	O
to	O
different	O
classes	O
,	O
and	O
(	O
iii	O
)	O
a	O
large	O
variation	O
in	O
the	O
number	O
of	O
samples	O
present	O
per	O
class	O
,	O
leading	O
to	O
greater	O
class	O
imbalance	O
than	O
LSVC	O
datasets	Material
like	O
ImageNet	Material
[	O
reference	O
]	O
.	O

Additionally	O
,	O
some	O
of	O
these	O
datasets	Material
have	O
densely	O
annotated	O
part	O
information	O
available	O
,	O
which	O
we	O
do	O
not	O
utilize	O
in	O
our	O
experiments	O
.	O

section	O
:	O
Results	O
section	O
:	O
Fine	Material
-	Material
Grained	Material
Visual	Material
Classification	Material
We	O
first	O
describe	O
our	O
results	O
on	O
the	O
six	O
FGVC	Material
datasets	Material
from	O
Table	O
2	O
.	O

In	O
all	O
experiments	O
,	O
we	O
average	O
results	O
over	O
5	O
trials	O
per	O
experiment	O
-	O
after	O
choosing	O
the	O
best	O
value	O
of	O
hyperparameter	O
λ	O
.	O

Please	O
see	O
the	O
supplementary	O
material	O
for	O
mean	O
and	O
standard	O
deviation	O
values	O
for	O
all	O
experiments	O
.	O

1	O
.	O

Fine	Method
-	Method
tuning	Method
from	O
Baseline	Method
Models	Method
:	O
We	O
fine	O
-	O
tune	O
from	O
three	O
baseline	O
models	O
using	O
the	O
PC	Method
optimization	O
procedure	O
:	O
ResNet	Method
-	Method
50	Method
[	O
reference	O
]	O
,	O
Bilinear	Method
CNN	Method
[	O
reference	O
]	O
,	O
and	O
DenseNet	Method
-	Method
161	Method
[	O
reference	O
]	O
.	O

As	O
Tables	O
2	O
-(	O
A	O
-	O
F	O
)	O
show	O
,	O
PC	Method
obtains	O
substantial	O
improvement	O
across	O
all	O
datasets	Material
and	O
models	O
.	O

For	O
instance	O
,	O
a	O
baseline	O
DenseNet	Method
-	Method
161	Method
architecture	O
obtains	O
an	O
average	O
accuracy	Metric
of	O
84.21	O
%	O
,	O
but	O
PC	Method
-	O
DenseNet	Method
-	Method
161	Method
obtains	O
an	O
accuracy	Metric
of	O
86.87	O
%	O
,	O
an	O
improvement	O
of	O
2.66	O
%	O
.	O

On	O
NABirds	Material
,	O
we	O
obtain	O
improvements	O
of	O
4.60	O
%	O
and	O
3.42	O
%	O
over	O
baseline	O
ResNet	Method
-	Method
50	Method
and	O
DenseNet	Method
-	Method
161	Method
architectures	O
.	O

2	O
.	O

Combining	O
PC	Method
with	O
Specialized	O
FGVC	Material
models	O
:	O
Recent	O
work	O
in	O
FGVC	Material
has	O
proposed	O
several	O
novel	O
CNN	Method
designs	Method
that	O
take	O
part	O
-	O
localization	Method
into	O
account	O
,	O
such	O
as	O
bilinear	Method
pooling	Method
techniques	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
and	O
spatial	Method
transformer	Method
networks	Method
[	O
reference	O
]	O
.	O

We	O
train	O
a	O
Bilinear	Method
CNN	Method
[	O
reference	O
]	O
with	O
PC	Method
,	O
and	O
obtain	O
an	O
average	O
improvement	O
of	O
1.7	O
%	O
on	O
the	O
6	O
datasets	Material
.	O

We	O
note	O
two	O
important	O
aspects	O
of	O
our	O
analysis	O
:	O
(	O
1	O
)	O
we	O
do	O
not	O
compare	O
with	O
ensembling	Method
and	Method
data	Method
augmentation	Method
techniques	Method
such	O
as	O
Boosted	Method
CNNs	Method
[	O
reference	O
]	O
and	O
Krause	O
,	O
et	O
al	O
.	O

[	O
reference	O
]	O
since	O
prior	O
evidence	O
indicates	O
that	O
these	O
techniques	O
invariably	O
improve	O
performance	O
,	O
and	O
(	O
2	O
)	O
we	O
evaluate	O
a	O
single	O
-	O
crop	O
,	O
single	Method
-	Method
model	Method
evaluation	Method
without	O
any	O
part	O
-	O
or	O
object	O
-	O
annotations	O
,	O
and	O
perform	O
competitively	O
with	O
methods	O
that	O
use	O
both	O
augmentations	Method
.	O

Choice	O
of	O
Hyperparameter	O
λ	O
:	O
Since	O
our	O
formulation	O
requires	O
the	O
selection	O
of	O
a	O
hyperparameter	O
λ	O
,	O
it	O
is	O
important	O
to	O
study	O
the	O
sensitivity	O
of	O
classification	Metric
performance	Metric
to	O
the	O
choice	O
of	O
λ	O
.	O

We	O
conduct	O
this	O
experiment	O
for	O
four	O
different	O
models	O
:	O
GoogLeNet	Method
[	O
reference	O
]	O
,	O
ResNet	Method
-	Method
50	Method
[	O
reference	O
]	O
and	O
VGGNet	Method
-	Method
16	Method
[	O
reference	O
]	O
and	O
Bilinear	Method
-	Method
CNN	Method
[	O
reference	O
]	O
on	O
the	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
dataset	Material
.	O

PC	Method
's	O
performance	O
is	O
not	O
very	O
sensitive	O
to	O
the	O
choice	O
of	O
λ	O
(	O
Figure	O
2	O
and	O
Supplementary	O
Tables	O
S1	O
-	O
S5	O
)	O
.	O

For	O
all	O
six	O
datasets	Material
,	O
the	O
λ	O
value	O
is	O
typically	O
between	O
the	O
range	O
[	O
reference	O
][	O
reference	O
]	O
.	O

On	O
Bilinear	Method
CNN	Method
,	O
setting	O
λ	O
=	O
10	O
for	O
all	O
datasets	Material
gives	O
average	O
performance	O
within	O
0.08	O
%	O
compared	O
to	O
the	O
reported	O
values	O
in	O
Table	O
2	O
.	O

In	O
general	O
,	O
PC	Method
obtains	O
optimum	O
performance	O
in	O
the	O
range	O
of	O
0.05N	O
and	O
0.15N	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
classes	O
.	O

section	O
:	O
Additional	O
Experiments	O
Since	O
our	O
method	O
aims	O
to	O
improve	O
classification	Task
performance	O
in	O
FGVC	Task
tasks	Task
by	O
introducing	O
confusion	O
in	O
output	O
logit	O
activations	O
,	O
we	O
would	O
expect	O
to	O
see	O
a	O
larger	O
improvement	O
in	O
datasets	Material
with	O
higher	O
inter	Metric
-	Metric
class	Metric
similarity	Metric
and	O
intra	O
-	O
class	O
variation	O
.	O

To	O
test	O
this	O
hypothesis	O
,	O
we	O
conduct	O
two	O
additional	O
experiments	O
.	O

In	O
the	O
first	O
experiment	O
,	O
we	O
construct	O
two	O
subsets	O
of	O
ImageNet	Material
-	Material
1	Material
K	Material
[	O
reference	O
]	O
.	O

The	O
first	O
dataset	O
,	O
ImageNet	Material
-	Material
Dogs	Material
is	O
a	O
subset	O
consisting	O
only	O
of	O
species	O
of	O
dogs	O
(	O
117	O
classes	O
and	O
116	O
K	O
images	O
)	O
.	O

The	O
second	O
dataset	O
,	O
ImageNet	Material
-	O
Random	O
contains	O
randomly	O
selected	O
classes	O
from	O
ImageNet	Material
-	Material
1K.	Material
Both	O
datasets	Material
contain	O
equal	O
number	O
of	O
classes	O
(	O
117	O
)	O
and	O
images	O
(	O
116	O
K	O
)	O
,	O
but	O
ImageNet	Material
-	Material
Dogs	Material
has	O
much	O
higher	O
interclass	Metric
similarity	Metric
and	O
intra	Metric
-	Metric
class	Metric
variation	Metric
,	O
as	O
compared	O
to	O
ImageNet	O
-	O
Random	O
.	O

To	O
test	O
repeatability	O
,	O
we	O
construct	O
3	O
instances	O
of	O
Imagenet	Material
-	O
Random	O
,	O
by	O
randomly	O
choosing	O
a	O
different	O
subset	O
of	O
ImageNet	Material
with	O
117	O
classes	O
each	O
time	O
.	O

For	O
both	O
experiments	O
,	O
we	O
randomly	O
construct	O
a	O
80	O
-	O
20	O
train	O
-	O
val	O
split	O
from	O
the	O
training	O
data	O
to	O
find	O
optimal	O
λ	O
by	O
cross	Metric
-	Metric
validation	Metric
,	O
and	O
report	O
the	O
performance	O
on	O
the	O
unseen	Material
ImageNet	Material
validation	Material
set	Material
of	O
the	O
subset	O
of	O
chosen	O
classes	O
.	O

In	O
Table	O
3	O
,	O
we	O
compare	O
the	O
performance	O
of	O
training	O
from	O
scratch	O
with	O
-	O
and	O
without	O
-	O
PC	Method
across	O
three	O
models	O
:	O
GoogLeNet	Method
,	O
ResNet	Method
-	Method
50	Method
,	O
and	O
DenseNet	Method
-	Method
161	Method
.	O

As	O
expected	O
,	O
PC	Method
obtains	O
a	O
larger	O
gain	O
in	O
classification	O
accuracy	Metric
(	O
1.45	O
%	O
)	O
on	O
ImageNet	Material
-	Material
Dogs	Material
as	O
compared	O
to	O
the	O
ImageNet	Material
-	Material
Random	Material
dataset	Material
(	O
0.54	O
%	O
±	O
0.28	O
)	O
.	O

In	O
the	O
second	O
experiment	O
,	O
we	O
utilize	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
,	O
which	O
contain	O
the	O
same	O
number	O
of	O
total	O
images	O
.	O

CIFAR	Material
-	Material
100	Material
has	O
10×	O
the	O
number	O
of	O
classes	O
and	O
10	O
%	O
of	O
images	O
per	O
class	O
as	O
CIFAR	Material
-	Material
10	Material
and	O
contains	O
larger	O
inter	O
-	O
class	O
similarity	O
and	O
intra	O
-	O
class	O
variation	O
.	O

We	O
train	O
networks	O
on	O
both	O
datasets	Material
from	O
scratch	O
using	O
default	O
train	O
-	O
test	O
splits	O
(	O
Table	O
3	O
)	O
.	O

As	O
expected	O
,	O
we	O
obtain	O
larger	O
average	O
gains	O
of	O
1.77	O
%	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
as	O
compared	O
to	O
0.20	O
%	O
on	O
CIFAR	Material
-	Material
10	Material
.	O

Additionally	O
,	O
when	O
training	O
with	O
λ	O
=	O
10	O
on	O
the	O
entire	O
ImageNet	Material
dataset	Material
,	O
we	O
obtain	O
a	O
top	O
-	O
1	O
accuracy	Metric
of	O
76.28	O
%	O
(	O
compared	O
to	O
a	O
baseline	O
of	O
76.15	O
%	O
)	O
,	O
which	O
is	O
a	O
smaller	O
improvement	O
,	O
which	O
is	O
in	O
line	O
with	O
what	O
we	O
would	O
expect	O
for	O
a	O
large	Task
-	Task
scale	Task
image	Task
classification	Task
problem	Task
with	O
large	O
inter	O
-	O
class	O
variation	O
.	O

Moreover	O
,	O
while	O
training	O
with	O
PC	Method
,	O
we	O
observe	O
that	O
the	O
rate	O
of	O
convergence	Metric
is	O
always	O
similar	O
to	O
or	O
faster	O
than	O
training	O
without	O
PC	Method
.	O

For	O
example	O
,	O
a	O
GoogLeNet	Method
trained	O
on	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
(	O
Figure	O
2	O
(	O
right	O
)	O
above	O
)	O
shows	O
that	O
PC	Method
converges	O
to	O
higher	O
validation	O
accuracy	Metric
faster	O
than	O
normal	Method
training	Method
using	O
identical	O
learning	Metric
rate	Metric
schedule	Metric
and	O
batch	O
size	O
.	O

Note	O
that	O
the	O
training	O
accuracy	Metric
is	O
reduced	O
when	O
training	O
with	O
PC	Method
,	O
due	O
to	O
the	O
regularization	O
effect	O
.	O

In	O
sum	O
,	O
classification	Task
problems	Task
that	O
have	O
large	O
intra	O
-	O
class	O
variation	O
and	O
high	O
inter	O
-	O
class	O
similarity	O
benefit	O
from	O
optimization	Task
with	O
pairwise	O
confusion	O
.	O

The	O
improvement	O
is	O
even	O
more	O
prominent	O
when	O
training	O
data	O
is	O
limited	O
.	O

section	O
:	O
Improvement	O
in	O
Localization	Metric
Ability	Metric
Recent	O
techniques	O
for	O
improving	O
classification	Task
performance	O
in	O
fine	Task
-	Task
grained	Task
recognition	Task
are	O
based	O
on	O
summarizing	O
and	O
extracting	O
dense	O
localization	Method
information	O
in	O
images	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Since	O
our	O
technique	O
increases	O
classification	O
accuracy	Metric
,	O
we	O
wish	O
to	O
understand	O
if	O
the	O
improvement	O
is	O
a	O
result	O
of	O
enhanced	O
CNN	O
localization	Method
abilities	O
due	O
to	O
PC	Method
.	O

To	O
measure	O
the	O
regions	O
the	O
CNN	O
localizes	O
on	O
,	O
we	O
utilize	O
Gradient	Method
-	Method
Weighted	Method
Class	Method
Activation	Method
Mapping	Method
(	O
Grad	Method
-	Method
CAM	Method
)	O
[	O
reference	O
]	O
,	O
a	O
method	O
that	O
provides	O
a	O
heatmap	O
of	O
visual	O
saliency	O
as	O
produced	O
by	O
the	O
network	O
.	O

We	O
perform	O
both	O
quantitative	O
and	O
qualitative	O
studies	O
of	O
localization	Method
ability	O
of	O
PC	Method
-	O
trained	O
models	O
.	O

Overlap	O
in	O
Localized	O
Regions	O
:	O
To	O
quantify	O
the	O
improvement	O
in	O
localization	Method
due	O
to	O
PC	Method
,	O
we	O
construct	O
bounding	O
boxes	O
around	O
object	O
regions	O
obtained	O
from	O
Grad	Method
-	Method
CAM	Method
,	O
by	O
thresholding	O
the	O
heatmap	O
values	O
at	O
0.5	O
,	O
and	O
choosing	O
the	O
largest	O
box	O
returned	O
.	O

We	O
then	O
calculate	O
the	O
mean	Metric
IoU	Metric
(	O
intersection	Metric
-	Metric
over	Metric
-	Metric
union	Metric
)	O
of	O
the	O
bounding	O
box	O
with	O
the	O
provided	O
object	O
bounding	O
boxes	O
for	O
the	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
dataset	Material
.	O

We	O
compare	O
the	O
mean	Metric
IoU	Metric
across	O
several	O
models	O
,	O
with	O
and	O
without	O
PC	Method
.	O

As	O
summarized	O
in	O
Table	O
4	O
,	O
we	O
observe	O
an	O
average	O
3.4	O
%	O
improvement	O
across	O
five	O
different	O
networks	O
,	O
implying	O
better	O
localization	Method
accuracy	Metric
.	O

Change	O
in	O
Class	Method
-	Method
Activation	Method
Mapping	Method
:	O
To	O
qualitatively	O
study	O
the	O
improvement	O
in	O
localization	Method
due	O
to	O
PC	Method
,	O
we	O
obtain	O
samples	O
from	O
the	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
dataset	Material
and	O
visualize	O
the	O
localization	Method
regions	O
returned	O
from	O
Grad	Method
-	Method
CAM	Method
for	O
both	O
the	O
baseline	Method
and	O
PC	Method
-	O
trained	O
VGG	O
-	O
16	O
model	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
PC	Method
models	O
provide	O
tighter	O
,	O
more	O
accurate	O
localization	Method
around	O
the	O
target	O
object	O
,	O
whereas	O
sometimes	O
the	O
baseline	Method
model	Method
has	O
localization	Method
driven	O
by	O
image	O
artifacts	O
.	O

Figure	O
3	O
-(	O
a	O
)	O
has	O
an	O
example	O
of	O
the	O
types	O
of	O
distractions	O
that	O
are	O
often	O
present	O
in	O
FGVC	Material
images	O
(	O
the	O
cartoon	O
bird	O
on	O
the	O
right	O
)	O
.	O

We	O
see	O
that	O
the	O
baseline	Method
VGG	Method
-	Method
16	Method
network	Method
pays	O
For	O
all	O
cases	O
,	O
we	O
consistently	O
observe	O
a	O
tighter	O
and	O
more	O
accurate	O
localization	Method
with	O
PC	Method
,	O
whereas	O
the	O
baseline	Method
VGG	Method
-	Method
16	Method
network	Method
often	O
latches	O
on	O
to	O
artifacts	O
,	O
even	O
while	O
making	O
correct	O
predictions	O
.	O

significant	O
attention	O
to	O
the	O
distraction	O
,	O
despite	O
making	O
the	O
correct	O
prediction	O
.	O

With	O
PC	Method
,	O
we	O
find	O
that	O
the	O
attention	O
is	O
limited	O
almost	O
exclusively	O
to	O
the	O
correct	O
object	O
,	O
as	O
desired	O
.	O

Similarly	O
for	O
Figure	O
3	O
-(	O
b	O
)	O
,	O
we	O
see	O
that	O
the	O
baseline	O
method	O
latches	O
on	O
to	O
the	O
incorrect	O
bird	O
category	O
,	O
which	O
is	O
corrected	O
by	O
the	O
addition	O
of	O
PC	Method
.	O

In	O
Figures	O
3	O
-(	O
c	O
-	O
d	O
)	O
,	O
we	O
see	O
that	O
the	O
baseline	Method
classifier	Method
makes	O
incorrect	O
decisions	O
due	O
to	O
poor	O
localization	Method
,	O
mistakes	O
that	O
are	O
resolved	O
by	O
PC	Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
introduce	O
Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
,	O
an	O
optimization	Method
procedure	Method
to	O
improve	O
generalizability	Task
in	O
fine	Task
-	Task
grained	Task
visual	Task
classification	Task
(	O
FGVC	Material
)	O
tasks	O
by	O
encouraging	O
confusion	O
in	O
output	O
activations	O
.	O

PC	Method
improves	O
FGVC	Material
performance	O
for	O
a	O
wide	O
class	O
of	O
convolutional	Method
architectures	Method
while	O
fine	Task
-	Task
tuning	Task
.	O

Our	O
experiments	O
indicate	O
that	O
PC	Method
-	O
trained	O
networks	O
show	O
improved	O
localization	Method
performance	O
which	O
contributes	O
to	O
the	O
gains	O
in	O
classification	O
accuracy	Metric
.	O

PC	Method
is	O
easy	O
to	O
implement	O
,	O
does	O
not	O
need	O
excessive	O
tuning	O
during	O
training	O
,	O
and	O
does	O
not	O
add	O
significant	O
overhead	O
during	O
test	O
time	O
,	O
in	O
contrast	O
to	O
methods	O
that	O
introduce	O
complex	O
localizationbased	Method
pooling	Method
steps	Method
that	O
are	O
often	O
difficult	O
to	O
implement	O
and	O
train	O
.	O

Therefore	O
,	O
our	O
technique	O
should	O
be	O
beneficial	O
to	O
a	O
wide	O
variety	O
of	O
specialized	O
neural	Method
network	Method
models	Method
for	O
applications	O
that	O
demand	O
for	O
fine	Task
-	Task
grained	Task
visual	Task
classification	Task
or	O
learning	Task
from	O
limited	O
labeled	O
data	O
.	O

Consider	O
Jeffrey	Method
's	Method
divergence	Method
with	O
N	O
=	O
2	O
classes	O
,	O
and	O
that	O
x	O
1	O
belongs	O
to	O
class	O
1	O
,	O
and	O
x	O
2	O
belongs	O
to	O
class	O
2	O
.	O

For	O
a	O
model	O
with	O
parameters	O
θ	O
that	O
correctly	O
identifies	O
both	O
x	O
1	O
and	O
x	O
2	O
by	O
training	O
using	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
,	O
p	O
θ	O
(	O
y	O
1	O
|x	O
1	O
)	O
=	O
1	O
−	O
δ	O
1	O
and	O
p	O
θ	O
(	O
y	O
2	O
|x	O
2	O
)	O
=	O
1	O
−	O
δ	O
2	O
,	O
where	O
0	O
<	O
δ	O
1	O
,	O
δ	O
2	O
<	O
1	O
2	O
(	O
since	O
the	O
classifier	Method
outputs	O
correct	O
predictions	O
for	O
the	O
input	O
images	O
)	O
,	O
we	O
get	O
:	O
section	O
:	O
S1.2	O
Lemmas	O
1	O
and	O
2	O
from	O
Main	O
Text	O
(	O
Euclidean	Metric
Confusion	Metric
Bounds	Metric
)	O
Lemma	O
1	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
for	O
probability	O
measures	O
P	O
,	O
Q	O
:	O
where	O
D	O
J	O
(	O
P	O
,	O
Q	O
)	O
is	O
the	O
Jeffrey	O
's	O
Divergence	O
between	O
P	O
and	O
Q.	O
Proof	O
.	O

By	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
For	O
a	O
finite	O
-	O
dimensional	O
vector	O
x	O
,	O
x	O
2	O
≤	O
x	O
1	O
,	O
therefore	O
:	O
Since	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
=	O
1	O
2	O
(	O
u∈U	O
|p	O
(	O
u	O
)	O
−	O
q	O
(	O
u	O
)	O
|	O
)	O
for	O
finite	O
alphabet	O
U	O
,	O
we	O
have	O
:	O
Since	O
Total	O
Variation	O
Distance	O
is	O
symmetric	O
,	O
we	O
have	O
:	O
By	O
Pinsker	O
's	O
Inequality	O
,	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
≤	O
Lemma	O
2	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
for	O
probability	O
measures	O
P	O
,	O
Q	O
:	O
where	O
D	O
TV	O
denotes	O
the	O
total	O
variation	O
distance	O
between	O
P	O
and	O
Q.	O
Proof	O
.	O

By	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
For	O
a	O
finite	O
-	O
dimensional	O
vector	O
x	O
,	O
x	O
2	O
≤	O
x	O
1	O
,	O
therefore	O
:	O
Since	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
=	O
Lemma	O
3	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
:	O
where	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
the	O
Energy	O
Distance	O
under	O
Euclidean	O
norm	O
between	O
S	O
i	O
and	O
S	O
j	O
(	O
parameterized	O
by	O
θ	O
)	O
,	O
and	O
random	O
vectors	O
are	O
selected	O
with	O
uniform	O
probability	O
in	O
both	O
S	O
i	O
and	O
S	O
j	O
.	O

Proof	O
.	O

From	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
Considering	O
X	O
i	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
,	O
then	O
we	O
get	O
:	O
Considering	O
X	O
j	O
∼	O
Uniform	O
(	O
S	O
j	O
)	O
,	O
we	O
obtain	O
:	O
Under	O
the	O
squared	Metric
Euclidean	Metric
norm	Metric
distance	Metric
,	O
the	O
Energy	Metric
Distance	Metric
can	O
be	O
given	O
by	O
:	O
Where	O
random	O
variables	O
X	O
,	O
X	O
∼	O
P	O
(	O
S	O
i	O
)	O
and	O
Y	O
,	O
Y	O
∼	O
P	O
(	O
S	O
j	O
)	O
.	O

If	O
P	O
(	O
S	O
i	O
)	O
=	O
Uniform	O
(	O
S	O
i	O
)	O
,	O
and	O
P	O
(	O
S	O
j	O
)	O
=	O
Uniform	O
(	O
S	O
j	O
)	O
,	O
we	O
have	O
by	O
substitution	O
of	O
Equation	O
(	O
18	O
)	O
:	O
Corollary	O
1	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
,	O
we	O
have	O
:	O
Proof	O
.	O

From	O
Equation	O
(	O
20	O
)	O
,	O
we	O
have	O
:	O
From	O
Equation	O
(	O
18	O
)	O
,	O
we	O
have	O
:	O
For	O
S	O
i	O
=	O
S	O
j	O
,	O
we	O
have	O
with	O
X	O
i	O
,	O
X	O
j	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
:	O
Replacing	O
this	O
in	O
Equation	O
(	O
20	O
)	O
,	O
we	O
have	O
with	O
X	O
,	O
X	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
and	O
Y	O
,	O
Y	O
∼	O
Uniform	O
(	O
S	O
j	O
)	O
:	O
From	O
Szekely	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
we	O
know	O
that	O
the	O
Energy	O
Distance	O
≥	O
0	O
with	O
equality	O
if	O
and	O
only	O
if	O
S	O
i	O
=	O
S	O
j	O
.	O

Thus	O
,	O
we	O
have	O
that	O
:	O
With	O
equality	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

section	O
:	O
S2	O
Training	O
Details	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
process	O
for	O
training	O
with	O
Pairwise	Method
Confusion	Method
for	O
different	O
base	O
architectures	O
,	O
including	O
the	O
list	O
of	O
hyperparameters	O
using	O
for	O
different	O
datasets	Material
.	O

section	O
:	O
ResNet	Method
-	Method
50	Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batch	O
-	O
size	O
8	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	Metric
rate	Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S1	O
.	O

Bilinear	O
and	O
Compact	O
Bilinear	Method
CNN	Method
:	O
In	O
all	O
experiments	O
,	O
we	O
use	O
the	O
training	O
procedure	O
described	O
by	O
the	O
authors	O
[	O
reference	O
]	O
.	O

In	O
addition	O
,	O
we	O
repeat	O
the	O
described	O
step	O
2	O
without	O
the	O
loss	O
on	O
confusion	O
from	O
the	O
obtained	O
weights	O
after	O
performing	O
Step	O
2	O
with	O
the	O
loss	O
,	O
and	O
obtain	O
an	O
additional	O
0.5	O
percent	O
gain	O
in	O
performance	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S2	O
.	O

section	O
:	O
DenseNet	Method
-	Method
161	Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batchsize	O
32	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	Metric
rate	Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S3	O
.	O

GoogLeNet	Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
300000	O
iterations	O
with	O
batch	O
-	O
size	O
32	O
,	O
with	O
a	O
step	O
size	O
of	O
30000	O
,	O
decreasing	O
it	O
by	O
a	O
ratio	O
of	O
0.96	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
is	O
given	O
in	O
Table	O
S4	O
.	O

section	O
:	O
VGGNet	Method
-	Method
16	Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batchsize	O
32	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	Metric
rate	Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
is	O
given	O
in	O
Table	O
S5	O
.	O

section	O
:	O
S3	O
Mean	O
and	O
Standard	O
Deviation	O
for	O
FGVC	Material
Results	O
In	O
Table	O
S6	O
,	O
we	O
provide	O
the	O
mean	O
and	O
standard	O
deviation	O
values	O
over	O
five	O
independent	O
runs	O
for	O
training	O
with	O
Pairwise	Method
Confusion	Method
with	O
different	O
baseline	Method
models	Method
.	O

These	O
results	O
correspond	O
to	O
Table	O
2	O
in	O
the	O
main	O
text	O
.	O

section	O
:	O
S4	O
Comparison	O
with	O
Regularization	Method
We	O
additionally	O
compare	O
the	O
performance	O
of	O
our	O
optimization	Method
technique	Method
with	O
other	O
regularization	Method
methods	Method
as	O
well	O
.	O

We	O
first	O
compare	O
Pairwise	Method
Comparison	Method
with	O
with	O
Label	Method
-	Method
Smoothing	Method
Regularization	Method
(	O
LSR	Method
)	O
on	O
all	O
six	O
FGVC	Material
datasets	Material
for	O
VGG	Material
-	Material
Net16	Material
,	O
ResNet	Method
-	Method
50	Method
and	O
DenseNet	Method
-	Method
161	Method
.	O

These	O
results	O
are	O
summarized	O
in	O
Table	O
S7	O
.	O

Next	O
,	O
in	O
Table	O
S8	O
,	O
we	O
compare	O
the	O
performance	O
of	O
Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
with	O
several	O
additional	O
regularization	Method
techniques	Method
on	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
using	O
two	O
small	O
architectures	O
:	O
CIFAR	Material
-	Material
10	Material
Quick	Material
(	O
C10Quick	Material
)	O
and	O
CIFAR	Material
-	Material
10	Material
Full	Material
(	O
C10Full	Material
)	O
,	O
which	O
are	O
standard	O
models	O
available	O
in	O
the	O
Caffe	Method
framework	Method
.	O

section	O
:	O
S5	O
Changes	O
to	O
Class	Metric
-	Metric
wise	Metric
Prediction	Metric
Accuracy	Metric
We	O
find	O
that	O
while	O
the	O
average	O
and	O
lowest	O
per	O
-	O
class	O
accuracy	Metric
increase	O
when	O
training	O
with	O
PC	Method
,	O
there	O
is	O
a	O
small	O
decline	O
in	O
top	O
-	O
performing	O
class	O
accuracy	Metric
(	O
See	O
Table	O
S9	O
)	O
.	O

Moreover	O
,	O
the	O
standard	Metric
deviation	Metric
in	O
per	O
-	O
class	O
accuracy	Metric
is	O
reduced	O
as	O
well	O
.	O

We	O
also	O
found	O
that	O
using	O
PC	Method
slightly	O
increased	O
false	Metric
positive	Metric
errors	Metric
while	O
obtaining	O
a	O
larger	O
reduction	O
in	O
false	Metric
negative	Metric
errors	Metric
.	O

For	O
example	O
,	O
on	O
CUB	Material
-	Material
200	Material
-	Material
2011	Material
with	O
ResNet	Method
-	Method
50	Method
,	O
the	O
average	Metric
false	Metric
positive	Metric
error	Metric
is	O
increased	O
by	O
0.06	O
%	O
,	O
but	O
Table	O
S6	O
.	O

Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
widelyused	O
fine	Task
-	Task
grained	Task
visual	Task
classification	Task
datasets	Material
(	O
A	O
-	O
F	O
)	O
.	O

Improvement	O
over	O
the	O
baseline	O
model	O
is	O
reported	O
as	O
(	O
∆	O
)	O
.	O

All	O
results	O
averaged	O
over	O
5	O
trials	O
with	O
standard	O
deviations	O
reported	O
in	O
parentheses	O
.	O

Table	O
S8	O
.	O

Image	Task
classification	Task
performance	O
and	O
train	Metric
-	Metric
val	Metric
gap	Metric
(	O
∆	O
)	O
)	O
for	O
Pairwise	Method
Confusion	Method
(	O
PC	Method
)	O
and	O
popular	O
regularization	Method
methods	Method
.	O

The	O
standard	O
deviation	O
across	O
trials	O
is	O
mentioned	O
in	O
parentheses	O
.	O

section	O
:	O
Method	O
the	O
average	Metric
false	Metric
negative	Metric
error	Metric
is	O
reduced	O
by	O
0.13	O
%	O
.	O

So	O
while	O
some	O
additional	O
mistakes	O
are	O
made	O
in	O
terms	O
of	O
false	Metric
positives	Metric
,	O
we	O
curb	O
/	O
reduce	O
the	O
problem	O
of	O
classifier	Task
overconfidence	Task
by	O
a	O
larger	O
margin	O
.	O

section	O
:	O
section	O
:	O
Acknowledgements	O
:	O
We	O
would	O
like	O
to	O
thank	O
Dr.	O
Ashok	O
Gupta	O
for	O
his	O
guidance	O
on	O
bird	Task
recognition	Task
,	O
and	O
Dr.	O
Sumeet	O
Agarwal	O
,	O
Spandan	O
Madan	O
and	O
Ishaan	O
Grover	O
for	O
their	O
feedback	O
at	O
various	O
stages	O
of	O
this	O
work	O
.	O

section	O
:	O
ReasoNet	Method
:	O
Learning	O
to	O
Stop	Task
Reading	Task
in	O
Machine	Task
Comprehension	Task
section	O
:	O
ABSTRACT	O
Teaching	O
a	O
computer	O
to	O
read	O
and	O
answer	O
general	O
questions	O
pertaining	O
to	O
a	O
document	O
is	O
a	O
challenging	O
yet	O
unsolved	O
problem	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
novel	O
neural	Method
network	Method
architecture	Method
called	O
the	O
Reasoning	Method
Network	Method
(	O
ReasoNet	Method
)	O
for	O
machine	Task
comprehension	Task
tasks	Task
.	O

ReasoNets	Method
make	O
use	O
of	O
multiple	O
turns	O
to	O
e	O
ectively	O
exploit	O
and	O
then	O
reason	O
over	O
the	O
relation	O
among	O
queries	O
,	O
documents	O
,	O
and	O
answers	O
.	O

Di	O
erent	O
from	O
previous	O
approaches	O
using	O
a	O
xed	O
number	O
of	O
turns	O
during	O
inference	Task
,	O
ReasoNets	Method
introduce	O
a	O
termination	O
state	O
to	O
relax	O
this	O
constraint	O
on	O
the	O
reasoning	O
depth	O
.	O

With	O
the	O
use	O
of	O
reinforcement	Task
learning	Task
,	O
ReasoNets	Method
can	O
dynamically	O
determine	O
whether	O
to	O
continue	O
the	O
comprehension	Task
process	Task
after	O
digesting	O
intermediate	O
results	O
,	O
or	O
to	O
terminate	O
reading	O
when	O
it	O
concludes	O
that	O
existing	O
information	O
is	O
adequate	O
to	O
produce	O
an	O
answer	O
.	O

ReasoNets	Method
achieve	O
superior	O
performance	O
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	Material
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
the	O
Stanford	Material
SQuAD	Material
dataset	Material
,	O
and	O
a	O
structured	Material
Graph	Material
Reachability	Material
dataset	Material
.	O

section	O
:	O
INTRODUCTION	O
Teaching	Task
machines	Task
to	O
read	O
,	O
process	O
,	O
and	O
comprehend	O
natural	O
language	O
documents	O
is	O
a	O
coveted	O
goal	O
for	O
arti	Task
cial	Task
intelligence	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Genuine	O
reading	Task
comprehension	Task
is	O
extremely	O
challenging	O
,	O
since	O
e	O
ective	Task
comprehension	Task
involves	O
thorough	O
understanding	Task
of	Task
documents	Task
and	O
sophisticated	O
inference	Task
.	O

Toward	O
solving	O
this	O
machine	Task
reading	Task
comprehension	Task
problem	Task
,	O
in	O
recent	O
years	O
,	O
several	O
works	O
have	O
collected	O
various	O
datasets	O
,	O
in	O
the	O
form	O
of	O
question	O
,	O
passage	O
,	O
and	O
answer	O
,	O
to	O
test	O
machine	O
on	O
answering	O
a	O
question	O
based	O
on	O
the	O
provided	O
passage	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Some	O
large	O
-	O
scale	O
cloze	O
-	O
style	O
datasets	O
[	O
reference	O
][	O
reference	O
]	O
have	O
gained	O
signi	O
ca	O
nt	O
attention	O
along	O
with	O
powerful	O
deep	Method
learning	Method
models	Method
.	O

Recent	O
approaches	O
on	O
cloze	O
-	O
style	O
datasets	O
can	O
be	O
separated	O
into	O
two	O
categories	O
:	O
single	O
-	O
turn	O
and	O
multi	Task
-	Task
turn	Task
reasoning	Task
.	O

Single	Method
turn	Method
reasoning	Method
models	Method
utilize	O
attention	Method
mechanisms	Method
[	O
reference	O
]	O
to	O
emphasize	O
speci	O
c	O
parts	O
of	O
the	O
document	O
which	O
are	O
relevant	O
to	O
the	O
query	O
.	O

Permission	O
to	O
make	O
digital	O
or	O
hard	O
copies	O
of	O
all	O
or	O
part	O
of	O
this	O
work	O
for	O
personal	O
or	O
classroom	O
use	O
is	O
granted	O
without	O
fee	O
provided	O
that	O
copies	O
are	O
not	O
made	O
or	O
distributed	O
for	O
pro	O
t	O
or	O
commercial	O
advantage	O
and	O
that	O
copies	O
bear	O
this	O
notice	O
and	O
the	O
full	O
citation	O
on	O
the	O
rst	O
page	O
.	O

Copyrights	O
for	O
components	O
of	O
this	O
work	O
owned	O
by	O
others	O
than	O
ACM	O
must	O
be	O
honored	O
.	O

Abstracting	O
with	O
credit	O
is	O
permitted	O
.	O

To	O
copy	O
otherwise	O
,	O
or	O
republish	O
,	O
to	O
post	O
on	O
servers	O
or	O
to	O
redistribute	O
to	O
lists	O
,	O
requires	O
prior	O
speci	O
c	O
permission	O
and	O
/	O
or	O
a	O
fee	O
.	O

Request	O
permissions	O
from	O
permissions@acm.org	O
.	O

These	O
attention	Method
models	Method
subsequently	O
calculate	O
the	O
relevance	O
between	O
a	O
query	O
and	O
the	O
corresponding	O
weighted	O
representations	O
of	O
document	O
subunits	O
(	O
e.g.	O
sentences	O
or	O
words	O
)	O
to	O
score	O
target	O
candidates	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
considering	O
the	O
sophistication	O
of	O
the	O
problem	O
,	O
after	O
a	O
single	Task
-	Task
turn	Task
comprehension	Task
,	O
readers	O
often	O
revisit	O
some	O
speci	O
c	O
passage	O
or	O
the	O
question	O
to	O
grasp	O
a	O
better	O
understanding	O
of	O
the	O
problem	O
.	O

With	O
this	O
motivation	O
,	O
recent	O
advances	O
in	O
reading	Task
comprehension	Task
have	O
made	O
use	O
of	O
multiple	O
turns	O
to	O
infer	O
the	O
relation	O
between	O
query	O
,	O
document	O
and	O
answer	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

By	O
repeatedly	O
processing	O
the	O
document	O
and	O
the	O
question	O
after	O
digesting	O
intermediate	O
information	O
,	O
multi	Method
-	Method
turn	Method
reasoning	Method
can	O
generally	O
produce	O
a	O
better	O
answer	O
and	O
these	O
existing	O
works	O
have	O
demonstrated	O
its	O
superior	O
performance	O
consistently	O
.	O

Existing	O
multi	Method
-	Method
turn	Method
models	Method
have	O
a	O
pre	O
-	O
de	O
ned	O
number	O
of	O
hops	O
or	O
iterations	O
in	O
their	O
inference	Task
without	O
regard	O
to	O
the	O
complexity	O
of	O
each	O
individual	O
query	O
or	O
document	O
.	O

However	O
,	O
when	O
human	O
read	O
a	O
document	O
with	O
a	O
question	O
in	O
mind	O
,	O
we	O
often	O
decide	O
whether	O
we	O
want	O
to	O
stop	O
reading	O
if	O
we	O
believe	O
the	O
observed	O
information	O
is	O
adequate	O
already	O
to	O
answer	O
the	O
question	O
,	O
or	O
continue	O
reading	O
after	O
digesting	O
intermediate	O
information	O
until	O
we	O
can	O
answer	O
the	O
question	O
with	O
con	O
dence	O
.	O

This	O
behavior	O
generally	O
varies	O
from	O
document	O
to	O
document	O
or	O
question	O
to	O
question	O
because	O
it	O
is	O
related	O
to	O
the	O
sophistication	O
of	O
the	O
document	O
or	O
the	O
di	O
culty	O
of	O
the	O
question	O
.	O

Meanwhile	O
,	O
the	O
analysis	O
in	O
[	O
reference	O
]	O
also	O
illustrates	O
the	O
huge	O
variations	O
in	O
the	O
di	O
culty	O
level	O
with	O
respect	O
to	O
questions	O
in	O
the	O
CNN	Material
/	Material
Daily	Material
Mail	Material
datasets	Material
[	O
reference	O
]	O
.	O

For	O
a	O
signi	O
ca	O
nt	O
part	O
of	O
the	O
datasets	O
,	O
this	O
analysis	O
shows	O
that	O
the	O
problem	O
can	O
not	O
be	O
solved	O
without	O
appropriate	O
reasoning	O
on	O
both	O
its	O
query	O
and	O
document	O
.	O

With	O
this	O
motivation	O
,	O
we	O
propose	O
a	O
novel	O
neural	Method
network	Method
architecture	Method
called	O
Reasoning	Method
Network	Method
(	O
ReasoNet	Method
)	O
.	O

which	O
tries	O
to	O
mimic	O
the	O
inference	Task
process	Task
of	O
human	Task
readers	Task
.	O

With	O
a	O
question	O
in	O
mind	O
,	O
ReasoNets	O
read	O
a	O
document	O
repeatedly	O
,	O
each	O
time	O
focusing	O
on	O
di	O
erent	O
parts	O
of	O
the	O
document	O
until	O
a	O
satisfying	O
answer	O
is	O
found	O
or	O
formed	O
.	O

This	O
reminds	O
us	O
of	O
a	O
Chinese	O
proverb	O
:	O
"	O
The	O
meaning	O
of	O
a	O
book	O
will	O
become	O
clear	O
if	O
you	O
read	O
it	O
hundreds	O
of	O
times	O
.	O

"	O
.	O

Moreover	O
,	O
unlike	O
previous	O
approaches	O
using	O
xed	O
number	O
of	O
hops	O
or	O
iterations	O
,	O
ReasoNets	Method
introduce	O
a	O
termination	O
state	O
in	O
the	O
inference	Task
.	O

This	O
state	O
can	O
decide	O
whether	O
to	O
continue	O
the	O
inference	O
to	O
the	O
next	O
turn	O
after	O
digesting	O
intermediate	O
information	O
,	O
or	O
to	O
terminate	O
the	O
whole	O
inference	O
when	O
it	O
concludes	O
that	O
existing	O
information	O
is	O
sufcient	O
to	O
yield	O
an	O
answer	O
.	O

The	O
number	O
of	O
turns	O
in	O
the	O
inference	Task
is	O
dynamically	O
modeled	O
by	O
both	O
the	O
document	O
and	O
the	O
query	O
,	O
and	O
can	O
be	O
learned	O
automatically	O
according	O
to	O
the	O
di	O
culty	O
of	O
the	O
problem	O
.	O

One	O
of	O
the	O
signi	O
ca	O
nt	O
challenges	O
ReasoNets	O
face	O
is	O
how	O
to	O
design	O
an	O
e	O
cient	O
training	Method
method	Method
,	O
since	O
the	O
termination	O
state	O
is	O
discrete	O
and	O
not	O
connected	O
to	O
the	O
nal	O
output	O
.	O

This	O
prohibits	O
canonical	Method
back	Method
-	Method
propagation	Method
method	Method
being	O
directly	O
applied	O
to	O
train	O
ReasoNets	Method
.	O

Motivated	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
tackle	O
this	O
challenge	O
by	O
proposing	O
a	O
reinforcement	Task
learning	Task
approach	O
,	O
which	O
utilizes	O
an	O
instance	Method
-	Method
dependent	Method
reward	Method
baseline	Method
,	O
to	O
successfully	O
train	O
ReasoNets	Method
.	O

Finally	O
,	O
by	O
accounting	O
for	O
a	O
dynamic	O
termination	O
state	O
during	O
inference	Task
and	O
applying	O
proposed	O
deep	O
reinforcement	Task
learning	Task
optimization	O
method	O
,	O
ReasoNets	Method
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
machine	Material
comprehension	Material
datasets	Material
,	O
including	O
unstructured	Material
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
and	O
the	O
proposed	O
structured	Material
Graph	Material
Reachability	Material
dataset	Material
,	O
when	O
the	O
paper	O
is	O
rst	O
publicly	O
available	O
on	O
arXiv	O
.	O

[	O
reference	O
]	O
At	O
the	O
time	O
of	O
the	O
paper	O
submission	O
,	O
we	O
apply	O
ReasoNet	Method
to	O
the	O
competitive	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
,	O
ReasoNets	Method
outperform	O
all	O
existing	O
published	O
approaches	O
and	O
rank	O
at	O
second	O
place	O
on	O
the	O
test	O
set	O
leaderboard	O
.	O

[	O
reference	O
]	O
This	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

In	O
Section	O
2	O
,	O
we	O
review	O
and	O
compare	O
recent	O
work	O
on	O
machine	Task
reading	Task
comprehension	Task
tasks	Task
.	O

In	O
Section	O
3	O
,	O
we	O
introduce	O
our	O
proposed	O
ReasoNet	Method
model	O
architecture	O
and	O
training	O
objectives	O
.	O

Section	O
4	O
presents	O
the	O
experimental	O
setting	O
and	O
results	O
on	O
unstructured	Task
and	Task
structured	Task
machine	Task
reading	Task
comprehension	Task
tasks	Task
.	O

section	O
:	O
RELATED	O
WORK	O
Recently	O
,	O
with	O
large	O
-	O
scale	O
datasets	O
available	O
and	O
the	O
impressive	O
advance	O
of	O
various	O
statistical	Method
models	Method
,	O
machine	Task
reading	Task
comprehension	Task
tasks	Task
have	O
attracted	O
much	O
attention	O
.	O

Here	O
we	O
mainly	O
focus	O
on	O
the	O
related	O
work	O
in	O
cloze	O
-	O
style	O
datasets	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Based	O
on	O
how	O
they	O
perform	O
the	O
inference	Task
,	O
we	O
can	O
classify	O
their	O
models	O
into	O
two	O
categories	O
:	O
single	O
-	O
turn	O
and	O
multi	Task
-	Task
turn	Task
reasoning	Task
.	O

Single	Method
-	Method
turn	Method
reasoning	Method
:	O
Single	Method
turn	Method
reasoning	Method
models	Method
utilize	O
an	O
attention	Method
mechanism	Method
to	O
emphasize	O
some	O
sections	O
of	O
a	O
document	O
which	O
are	O
relevant	O
to	O
a	O
query	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
treating	O
some	O
parts	O
unimportant	O
while	O
focusing	O
on	O
other	O
important	O
ones	O
to	O
nd	O
the	O
most	O
probable	O
answer	O
.	O

Hermann	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
attentive	Method
reader	Method
and	O
the	O
impatient	Method
reader	Method
models	Method
using	O
neural	Method
networks	Method
with	O
an	O
attention	O
over	O
passages	O
to	O
predict	O
candidates	O
.	O

Hill	O
et	O
al	O
.	O

[	O
reference	O
]	O
use	O
attention	Method
over	O
window	Method
-	Method
based	Method
memory	Method
,	O
which	O
encodes	O
a	O
window	O
of	O
words	O
around	O
entity	O
candidates	O
,	O
by	O
leveraging	O
an	O
endto	Method
-	Method
end	Method
memory	Method
network	Method
[	O
reference	O
]	O
.	O

Meanwhile	O
,	O
given	O
the	O
same	O
entity	O
candidate	O
can	O
appear	O
multiple	O
times	O
in	O
a	O
passage	O
,	O
Kadlec	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
attention	Method
-	Method
sum	Method
reader	Method
to	O
sum	O
up	O
all	O
the	O
attention	O
scores	O
for	O
the	O
same	O
entity	O
.	O

This	O
score	O
captures	O
the	O
relevance	O
between	O
a	O
query	O
and	O
a	O
candidate	O
.	O

Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
using	O
a	O
bilinear	Method
term	Method
similarity	Method
function	Method
to	O
calculate	O
attention	O
scores	O
with	O
pretrained	O
word	O
embeddings	O
.	O

Trischler	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
EpiReader	Method
which	O
uses	O
two	O
neural	Method
network	Method
structures	Method
:	O
one	O
extracts	O
candidates	O
using	O
the	O
attention	Method
-	Method
sum	Method
reader	Method
;	O
the	O
other	O
reranks	O
candidates	O
based	O
on	O
a	O
bilinear	Metric
term	Metric
similarity	Metric
score	Metric
calculated	O
from	O
query	Method
and	Method
passage	Method
representations	Method
.	O

Multi	Task
-	Task
turn	Task
reasoning	Task
:	O
For	O
complex	O
passages	O
and	O
complex	O
queries	O
,	O
human	O
readers	O
often	O
revisit	O
the	O
given	O
document	O
in	O
order	O
to	O
perform	O
deeper	Task
inference	Task
after	O
reading	O
a	O
document	O
.	O

Several	O
recent	O
studies	O
try	O
to	O
simulate	O
this	O
revisit	O
by	O
combining	O
the	O
information	O
in	O
the	O
query	O
with	O
the	O
new	O
information	O
digested	O
from	O
previous	O
iterations	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Hill	O
et	O
al	O
.	O

[	O
reference	O
]	O
use	O
multiple	Method
hops	Method
memory	Method
network	Method
to	O
augment	O
the	O
query	O
with	O
new	O
information	O
from	O
the	O
previous	O
hop	O
.	O

Gated	Method
Attention	Method
reader	Method
[	O
reference	O
]	O
is	O
an	O
extension	O
of	O
the	O
attention	Method
-	Method
sum	Method
reader	Method
with	O
multiple	O
iterations	O
by	O
pushing	O
the	O
query	Method
encoding	Method
into	O
an	O
attention	Method
-	Method
based	Method
gate	Method
in	O
each	O
iteration	O
.	O

Iterative	Method
Alternative	Method
(	O
IA	Method
)	O
reader	O
[	O
reference	O
]	O
produces	O
a	O
new	O
query	O
glimpse	O
and	O
document	O
glimpse	O
in	O
each	O
iteration	O
and	O
utilizes	O
them	O
alternatively	O
in	O
the	O
next	O
iteration	O
.	O

Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
further	O
propose	O
to	O
extend	O
the	O
query	O
-	O
speci	O
c	O
attention	O
to	O
both	O
query	Task
-	Task
to	Task
-	Task
document	Task
attention	Task
and	O
document	Task
-	Task
to	Task
-	Task
query	Task
attention	Task
,	O
which	O
is	O
built	O
from	O
the	O
intermediate	O
results	O
in	O
the	O
query	O
-	O
speci	O
c	O
attention	O
.	O

By	O
reading	O
documents	O
and	O
enriching	O
the	O
query	O
in	O
an	O
iterative	O
fashion	O
,	O
multi	Method
-	Method
turn	Method
reasoning	Method
has	O
demonstrated	O
their	O
superior	O
performance	O
consistently	O
.	O

Our	O
proposed	O
approach	O
explores	O
the	O
idea	O
of	O
using	O
both	O
attentionsum	O
to	O
aggregate	O
candidate	O
attention	O
scores	O
and	O
multiple	O
turns	O
to	O
attain	O
a	O
better	O
reasoning	O
capability	O
.	O

Unlike	O
previous	O
approaches	O
using	O
a	O
xed	O
number	O
of	O
hops	O
or	O
iterations	O
,	O
motivated	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
propose	O
a	O
termination	Method
module	Method
in	O
the	O
inference	Task
.	O

The	O
termination	Method
module	Method
can	O
decide	O
whether	O
to	O
continue	O
to	O
infer	O
the	O
next	O
turn	O
after	O
digesting	O
intermediate	O
information	O
,	O
or	O
to	O
terminate	O
the	O
whole	O
inference	Task
process	Task
when	O
it	O
concludes	O
existing	O
information	O
is	O
su	O
cient	O
to	O
yield	O
an	O
answer	O
.	O

The	O
number	O
of	O
turns	O
in	O
the	O
inference	Task
is	O
dynamically	O
modeled	O
by	O
both	O
a	O
document	O
and	O
a	O
query	O
,	O
and	O
is	O
generally	O
related	O
to	O
the	O
complexity	O
of	O
the	O
document	O
and	O
the	O
query	O
.	O

section	O
:	O
REASONING	Task
NETWORKS	Task
ReasoNets	Method
are	O
devised	O
to	O
mimic	O
the	O
inference	Task
process	Task
of	Task
human	Task
readers	Task
.	O

ReasoNets	O
read	O
a	O
document	O
repeatedly	O
with	O
attention	O
on	O
di	O
erent	O
parts	O
each	O
time	O
until	O
a	O
satisfying	O
answer	O
is	O
found	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
a	O
ReasoNet	Method
is	O
composed	O
of	O
the	O
following	O
components	O
:	O
Memory	Method
:	O
The	O
external	O
memory	O
is	O
denoted	O
as	O
M.	O
It	O
is	O
a	O
list	O
of	O
word	O
vectors	O
,	O
M	O
=	O
{	O
m	O
i	O
}	O
i=1	O
..	O
D	O
,	O
where	O
m	O
i	O
is	O
a	O
xed	O
dimensional	O
vector	O
.	O

For	O
example	O
,	O
in	O
the	O
Graph	Task
Reachability	Task
,	O
m	O
i	O
is	O
the	O
vector	Method
representation	Method
of	O
each	O
word	O
in	O
the	O
graph	O
description	O
encoded	O
by	O
a	O
bidirectional	O
-	O
RNN	Method
.	O

Please	O
refer	O
to	O
Section	O
4	O
for	O
the	O
detailed	O
setup	O
in	O
each	O
experiment	O
.	O

Attention	Method
:	O
The	O
attention	O
vector	O
x	O
t	O
is	O
generated	O
based	O
on	O
the	O
current	O
internal	O
state	O
s	O
t	O
and	O
the	O
external	O
memory	O
M	O
:	O
x	O
t	O
=	O
f	O
at	O
t	O
(	O
s	O
t	O
,	O
M	O
;	O
θ	O
x	O
)	O
.	O

Please	O
refer	O
to	O
Section	O
4	O
for	O
the	O
detailed	O
setup	O
in	O
each	O
experiment	O
.	O

Internal	O
State	O
:	O
The	O
internal	O
state	O
is	O
denoted	O
as	O
s	O
which	O
is	O
a	O
vector	Method
representation	Method
of	Method
the	Method
question	Method
state	Method
.	O

Typically	O
,	O
the	O
initial	O
state	O
s	O
1	O
is	O
the	O
last	Method
-	Method
word	Method
vector	Method
representation	Method
of	O
query	O
by	O
an	O
RNN	Method
.	O

The	O
t	O
-	O
th	O
time	O
step	O
of	O
the	O
internal	O
state	O
is	O
represented	O
by	O
s	O
t	O
.	O

The	O
sequence	O
of	O
internal	O
states	O
are	O
modeled	O
by	O
an	O
RNN	Method
:	O
s	O
t	O
+	O
1	O
=	O
RNN	Method
(	O
s	O
t	O
,	O
x	O
t	O
;	O
θ	O
s	O
)	O
,	O
where	O
x	O
t	O
is	O
the	O
attention	O
vector	O
mentioned	O
above	O
.	O

Termination	O
Gate	O
:	O
The	O
termination	O
gate	O
generates	O
a	O
random	O
variable	O
according	O
to	O
the	O
current	O
internal	O
state	O
;	O
t	O
t	O
∼	O
p	O
(	O
·|	O
f	O
t	O
(	O
s	O
t	O
;	O
θ	O
t	O
)	O
)	O
)	O
.	O

t	O
t	O
is	O
a	O
binary	O
random	O
variable	O
.	O

If	O
t	O
t	O
is	O
true	O
,	O
the	O
ReasoNet	Method
stops	O
,	O
and	O
the	O
answer	Method
module	Method
executes	O
at	O
time	O
step	O
t	O
;	O
otherwise	O
the	O
ReasoNet	Method
generates	O
an	O
attention	O
vector	O
x	O
t	O
+	O
1	O
,	O
and	O
feeds	O
the	O
vector	O
into	O
the	O
state	Method
network	Method
to	O
update	O
the	O
next	O
internal	O
state	O
s	O
t	O
+	O
1	O
.	O

Answer	O
:	O
The	O
action	O
of	O
answer	Method
module	Method
is	O
triggered	O
when	O
the	O
termination	O
gate	O
variable	O
is	O
true	O
:	O
a	O
t	O
∼	O
p	O
(	O
·|	O
f	O
a	O
(	O
s	O
t	O
;	O
θ	O
a	O
)	O
)	O
.	O

In	O
Algorithm	O
1	O
,	O
we	O
describe	O
the	O
stochastic	Method
inference	Method
process	Method
of	O
a	O
ReasoNet	Method
.	O

The	O
process	O
can	O
be	O
considered	O
as	O
solving	O
a	O
Partially	Task
Observable	Task
Markov	Task
Decision	Task
Process	Task
(	O
POMDP	Task
)	O
[	O
reference	O
]	O
in	O
the	O
reinforcement	Task
learning	Task
(	O
RL	Task
)	O
literature	O
.	O

The	O
state	O
sequence	O
s	O
1:T	O
is	O
hidden	O
Step	O
t	O
=	O
1	O
;	O
Maximum	O
Step	O
T	O
max	O
Output	O
:	O
Termination	O
Step	O
T	O
,	O
Answer	O
a	O
T	O
1	O
Sample	O
t	O
t	O
from	O
the	O
distribution	O
p	O
(	O
·|	O
f	O
t	O
(	O
s	O
t	O
;	O
θ	O
t	O
)	O
)	O
;	O
2	O
if	O
t	O
t	O
is	O
false	O
,	O
go	O
to	O
Step	O
3	O
;	O
otherwise	O
Step	O
6	O
;	O
3	O
Generate	O
attention	O
vector	O
x	O
t	O
=	O
f	O
at	O
t	O
(	O
s	O
t	O
,	O
M	O
;	O
θ	O
x	O
)	O
;	O
4	O
Update	O
internal	O
state	O
s	O
t	O
+	O
1	O
=	O
RNN	Method
(	O
s	O
t	O
,	O
x	O
t	O
;	O
θ	O
s	O
)	O
;	O
5	O
Set	O
t	O
=	O
t	O
+	O
1	O
;	O
if	O
t	O
<	O
T	O
max	O
go	O
to	O
Step	O
1	O
;	O
otherwise	O
Step	O
6	O
;	O
6	O
Generate	O
answer	O
a	O
t	O
∼	O
p	O
(	O
·|	O
f	O
a	O
(	O
s	O
t	O
;	O
θ	O
a	O
)	O
)	O
;	O
7	O
Return	O
T	O
=	O
t	O
and	O
a	O
T	O
=	O
a	O
t	O
;	O
and	O
dynamic	O
,	O
controlled	O
by	O
an	O
RNN	Method
sequence	O
model	O
.	O

The	O
ReasoNet	Method
performs	O
an	O
answer	O
action	O
a	O
T	O
at	O
the	O
T	O
-	O
th	O
step	O
,	O
which	O
implies	O
that	O
the	O
termination	O
gate	O
variables	O
t	O
1:T	O
=	O
(	O
t	O
1	O
=	O
0	O
,	O
t	O
2	O
=	O
0	O
,	O
...	O
,	O
t	O
T	O
−1	O
=	O
0	O
,	O
t	O
T	O
=	O
1	O
)	O
.	O

The	O
ReasoNet	Method
learns	O
a	O
stochastic	Method
policy	Method
π	Method
(	O
(	O
t	O
t	O
,	O
a	O
t	O
)	O
|s	O
t	O
;	O
θ	O
)	O
with	O
parameters	O
θ	O
to	O
get	O
a	O
distribution	O
of	O
termination	O
actions	O
,	O
to	O
continue	O
reading	O
or	O
to	O
stop	O
,	O
and	O
of	O
answer	O
actions	O
if	O
the	O
model	O
decides	O
to	O
stop	O
at	O
the	O
current	O
step	O
.	O

The	O
termination	O
step	O
T	O
varies	O
from	O
instance	O
to	O
instance	O
.	O

The	O
learnable	O
parameters	O
θ	O
of	O
the	O
ReasoNet	Method
are	O
the	O
embedding	O
matrices	O
θ	O
W	O
,	O
attention	Method
network	Method
θ	O
x	O
,	O
the	O
state	O
RNN	Method
network	O
θ	O
s	O
,	O
the	O
answer	Method
action	Method
network	Method
θ	O
a	O
,	O
and	O
the	O
termination	Method
gate	Method
network	Method
θ	O
t	O
.	O

The	O
parameters	O
θ	O
=	O
{	O
θ	O
W	O
,	O
θ	O
x	O
,	O
θ	O
s	O
,	O
θ	O
a	O
,	O
θ	O
t	O
}	O
are	O
trained	O
by	O
maximizing	O
the	O
total	Metric
expect	Metric
reward	Metric
.	O

The	O
expected	Metric
reward	Metric
for	O
an	O
instance	O
is	O
de	O
ned	O
as	O
:	O
The	O
reward	O
can	O
only	O
be	O
received	O
at	O
the	O
nal	O
termination	O
step	O
when	O
an	O
answer	O
action	O
a	O
T	O
is	O
performed	O
.	O

We	O
de	O
ne	O
r	O
T	O
=	O
1	O
if	O
t	O
T	O
=	O
1	O
and	O
the	O
answer	O
is	O
correct	O
,	O
and	O
r	O
T	O
=	O
0	O
otherwise	O
.	O

The	O
rewards	O
on	O
intermediate	O
steps	O
are	O
zeros	O
,	O
{	O
r	O
t	O
=	O
0	O
}	O
t	O
=	O
1	O
...	O
T	O
−1	O
.	O

can	O
be	O
maximized	O
by	O
directly	O
applying	O
gradient	Method
based	Method
optimization	Method
methods	Method
.	O

The	O
gradient	O
of	O
is	O
given	O
by	O
:	O
Motivated	O
by	O
the	O
REINFORCE	Method
algorithm	Method
[	O
reference	O
]	O
,	O
we	O
compute	O
∇	O
θ	O
(	O
θ	O
)	O
:	O
where	O
A	O
†	O
is	O
all	O
the	O
possible	O
episodes	O
,	O
T	O
,	O
t	O
1:T	O
,	O
a	O
T	O
and	O
r	O
T	O
are	O
the	O
termination	O
step	O
,	O
termination	O
action	O
,	O
answer	O
action	O
,	O
and	O
reward	O
,	O
respectively	O
,	O
for	O
the	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
episode	O
.	O

b	O
T	O
is	O
called	O
the	O
reward	O
baseline	O
in	O
the	O
RL	Task
literature	O
to	O
lower	O
the	O
variance	O
[	O
reference	O
]	O
.	O

It	O
is	O
common	O
to	O
,	O
and	O
can	O
be	O
updated	O
via	O
an	O
online	Method
moving	Method
average	Method
approach	Method
:	O
b	O
T	O
=	O
λb	O
T	O
+	O
(	O
1	O
−	O
λ	O
)	O
r	O
T	O
.	O

However	O
,	O
we	O
empirically	O
nd	O
that	O
the	O
above	O
approach	O
leads	O
to	O
slow	O
convergence	O
in	O
training	Task
ReasoNets	Task
.	O

Intuitively	O
,	O
the	O
average	O
baselines	O
{	O
b	O
T	O
;	O
T	O
=	O
1	O
..	O
T	O
max	O
}	O
are	O
global	O
variables	O
independent	O
of	O
instances	O
.	O

It	O
is	O
hard	O
for	O
these	O
baselines	O
to	O
capture	O
the	O
dynamic	O
termination	O
behavior	O
of	O
ReasoNets	O
.	O

Since	O
ReasoNets	O
may	O
stop	O
at	O
di	O
erent	O
time	O
steps	O
for	O
di	O
erent	O
instances	O
,	O
the	O
adoption	O
of	O
a	O
global	O
variable	O
without	O
considering	O
the	O
dynamic	O
variance	O
in	O
each	O
instance	O
is	O
inappropriate	O
.	O

To	O
resolve	O
this	O
weakness	O
in	O
traditional	O
methods	O
and	O
account	O
for	O
the	O
dynamic	O
characteristic	O
of	O
ReasoNets	O
,	O
we	O
propose	O
an	O
instance	Method
-	Method
dependent	Method
baseline	Method
method	Method
to	O
calculate	O
∇	O
θ	O
(	O
θ	O
)	O
,	O
as	O
illustrated	O
in	O
Section	O
3.1	O
.	O

Empirical	O
results	O
show	O
that	O
the	O
proposed	O
reward	Method
schema	Method
achieves	O
better	O
results	O
compared	O
to	O
baseline	O
approaches	O
.	O

section	O
:	O
Training	O
Details	O
In	O
the	O
machine	Task
reading	Task
comprehension	Task
tasks	Task
,	O
a	O
training	O
dataset	O
is	O
a	O
collection	O
of	O
triplets	O
of	O
query	O
q	O
,	O
passage	O
p	O
,	O
and	O
answer	O
a.	O
Say	O
q	O
n	O
,	O
p	O
n	O
,	O
a	O
n	O
is	O
the	O
n	O
-	O
th	O
training	O
instance	O
.	O

The	O
rst	O
step	O
is	O
to	O
extract	O
memory	O
M	O
from	O
p	O
n	O
by	O
mapping	O
each	O
symbolic	O
in	O
the	O
passage	O
to	O
a	O
contextual	Method
representation	Method
given	O
by	O
the	O
concatenation	O
of	O
forward	O
and	O
backward	O
RNN	Method
hidden	O
states	O
,	O
i.e.	O
,	O
,	O
and	O
extract	O
initial	O
state	O
s	O
1	O
from	O
q	O
n	O
by	O
assigning	O
.	O

Given	O
M	O
and	O
s	O
1	O
for	O
the	O
n	O
-	O
th	O
training	O
instance	O
,	O
a	O
ReasoNet	Method
executes	O
|A	O
†	O
|	O
episodes	O
,	O
where	O
all	O
possible	O
episodes	O
A	O
†	O
can	O
be	O
enumerated	O
by	O
setting	O
a	O
maximum	O
step	O
.	O

Each	O
episode	O
generates	O
actions	O
and	O
a	O
reward	O
from	O
the	O
last	O
step	O
:	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
,	O
r	O
T	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
∈A	O
†	O
.	O

Therefore	O
,	O
the	O
gradient	O
of	O
can	O
be	O
rewritten	O
as	O
:	O
where	O
the	O
baseline	O
b	O
=	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
∈A	O
†	O
π	O
(	O
t	O
1:T	O
,	O
a	O
T	O
;	O
θ	O
)	O
r	O
T	O
is	O
the	O
average	O
reward	O
on	O
the	O
|A	O
†	O
|	O
episodes	O
for	O
the	O
n	O
-	O
th	O
training	O
instance	O
.	O

It	O
allows	O
di	O
erent	O
baselines	O
for	O
di	O
erent	O
training	O
instances	O
.	O

This	O
can	O
be	O
benecial	O
since	O
the	O
complexity	O
of	O
training	O
instances	O
varies	O
signi	O
cantly	O
.	O

In	O
experiments	O
,	O
we	O
empirically	O
nd	O
using	O
(	O
r	O
T	O
b	O
−	O
1	O
)	O
in	O
replace	O
of	O
(	O
r	O
T	O
−	O
b	O
)	O
can	O
lead	O
to	O
a	O
faster	O
convergence	Metric
.	O

Therefore	O
,	O
we	O
adopt	O
this	O
approach	O
to	O
train	O
ReasoNets	Task
in	O
the	O
experiments	O
.	O

section	O
:	O
EXPERIMENTS	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
ReasoNets	Method
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	Material
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
the	O
Stanford	Material
SQuAD	Material
dataset	Material
,	O
and	O
a	O
structured	Material
Graph	Material
Reachability	Material
dataset	Material
.	O

section	O
:	O
CNN	Material
and	Material
Daily	Material
Mail	Material
Datasets	Material
We	O
examine	O
the	O
performance	O
of	O
ReasoNets	Method
on	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
.	O

[	O
reference	O
]	O
The	O
detailed	O
settings	O
of	O
the	O
ReasoNet	Method
model	O
are	O
as	O
follows	O
.	O

Vocab	Metric
Size	Metric
:	O
For	O
training	O
our	O
ReasoNet	Method
,	O
we	O
keep	O
the	O
most	O
frequent	O
|V	O
|	O
=	O
101k	O
words	O
(	O
not	O
including	O
584	O
entities	O
and	O
1	O
placeholder	O
marker	O
)	O
in	O
the	O
CNN	Material
dataset	Material
,	O
and	O
|V	O
|	O
=	O
151k	O
words	O
(	O
not	O
including	O
530	O
entities	O
and	O
1	O
placeholder	O
marker	O
)	O
in	O
the	O
Daily	Material
Mail	Material
dataset	Material
.	O

Embedding	Method
Layer	Method
:	O
We	O
choose	O
300	O
-	O
dimensional	O
word	O
embeddings	O
,	O
and	O
use	O
the	O
300	Method
-	Method
dimensional	Method
pretrained	Method
Glove	Method
word	Method
embeddings	Method
[	O
reference	O
]	O
for	O
initialization	Task
.	O

We	O
also	O
apply	O
dropout	Method
with	O
probability	O
0.2	O
to	O
the	O
embedding	Method
layer	Method
.	O

Bi	Method
-	Method
GRU	Method
Encoder	Method
:	O
We	O
apply	O
bidirectional	Method
GRU	Method
for	O
encoding	O
query	Task
and	Task
passage	Task
into	O
vector	Method
representations	Method
.	O

We	O
set	O
the	O
number	O
of	O
hidden	O
units	O
to	O
be	O
256	O
and	O
384	O
for	O
the	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
respectively	O
.	O

The	O
recurrent	O
weights	O
of	O
GRUs	Method
are	O
initialized	O
with	O
random	Method
orthogonal	Method
matrices	Method
.	O

The	O
other	O
weights	O
in	O
GRU	O
cell	O
are	O
initialized	O
from	O
a	O
uniform	O
distribution	O
between	O
−0.01	O
and	O
0.01	O
.	O

We	O
use	O
a	O
shared	O
GRU	Method
model	Method
for	O
both	O
query	Task
and	Task
passage	Task
.	O

Memory	O
and	O
Attention	Task
:	O
The	O
memory	O
of	O
the	O
ReasoNet	Method
on	O
CNN	Material
and	Material
Daily	Material
Mail	Material
dataset	Material
is	O
composed	O
of	O
query	O
memory	O
and	O
passage	O
memory	O
.	O

M	O
=	O
(	O
M	O
quer	O
,	O
M	O
doc	O
)	O
,	O
where	O
M	O
quer	O
and	O
M	O
doc	O
are	O
extracted	O
from	O
query	Method
bidirectional	Method
-	Method
GRU	Method
encoder	Method
and	O
passage	Method
bidirectional	Method
-	Method
GRU	Method
encoder	Method
respectively	O
.	O

We	O
choose	O
projected	Method
cosine	Method
similarity	Method
function	Method
as	O
the	O
attention	Method
module	Method
.	O

The	O
attention	O
score	O
a	O
doc	O
t	O
,	O
i	O
on	O
memory	O
m	O
doc	O
i	O
given	O
the	O
state	O
s	O
t	O
is	O
computed	O
as	O
follows	O
:	O
where	O
W	O
t	O
and	O
b	O
t	O
are	O
the	O
weight	O
matrix	O
and	O
bias	O
vector	O
,	O
respectively	O
.	O

Answer	Method
Module	Method
:	O
We	O
apply	O
a	O
linear	Method
projection	Method
from	O
GRU	O
outputs	O
and	O
make	O
predictions	O
on	O
the	O
entity	O
candidates	O
.	O

Following	O
the	O
[	O
reference	O
]	O
The	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
are	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
deepmind	O
/	O
rcdata	O
settings	O
in	O
AS	O
Reader	O
[	O
reference	O
]	O
,	O
we	O
sum	O
up	O
scores	O
from	O
the	O
same	O
candidate	O
and	O
make	O
a	O
prediction	O
.	O

Thus	O
,	O
AS	O
Reader	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNets	O
with	O
T	O
max	O
=	O
1	O
.	O

[	O
reference	O
]	O
Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
,	O
T	O
max	O
is	O
set	O
to	O
5	O
in	O
experiments	O
on	O
both	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
.	O

We	O
use	O
ADAM	Method
optimizer	Method
[	O
reference	O
]	O
for	O
parameter	Task
optimization	Task
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.0005	O
,	O
β	O
1	O
=	O
0.9	O
and	O
β	O
2	O
=	O
0.999	O
;	O
The	O
absolute	O
value	O
of	O
gradient	O
on	O
each	O
parameter	O
is	O
clipped	O
within	O
0.001	O
.	O

The	O
batch	O
size	O
is	O
64	O
for	O
both	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
.	O

For	O
each	O
batch	O
of	O
the	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
we	O
randomly	O
reshu	O
e	O
the	O
assignment	O
of	O
named	O
entities	O
[	O
reference	O
]	O
.	O

This	O
forces	O
the	O
model	O
to	O
treat	O
the	O
named	O
entities	O
as	O
semantically	O
meaningless	O
labels	O
.	O

In	O
the	O
prediction	Task
of	Task
test	Task
cases	Task
,	O
we	O
randomly	O
reshu	O
e	O
named	O
entities	O
up	O
to	O
4	O
times	O
,	O
and	O
report	O
the	O
averaged	O
answer	O
.	O

Models	O
are	O
trained	O
on	O
GTX	Method
TitanX	O
12	O
GB	O
.	O

It	O
takes	O
7	O
hours	O
per	O
epoch	O
to	O
train	O
on	O
the	O
Daily	Material
Mail	Material
dataset	Material
and	O
3	O
hours	O
per	O
epoch	O
to	O
train	O
on	O
the	O
CNN	Material
dataset	Material
.	O

The	O
models	O
are	O
usually	O
converged	O
within	O
6	O
epochs	O
on	O
both	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
.	O

[	O
reference	O
]	O
When	O
ReasoNet	Method
is	O
set	O
with	O
T	O
max	O
=	O
1	O
in	O
CNN	Material
and	O
Daily	Material
Mail	Material
,	O
it	O
directly	O
applies	O
s	O
0	O
to	O
make	O
predictions	O
on	O
the	O
entity	O
candidates	O
,	O
without	O
performing	O
attention	O
on	O
the	O
memory	Method
module	Method
.	O

The	O
prediction	Method
module	Method
in	O
ReasoNets	Method
is	O
the	O
same	O
as	O
in	O
AS	O
Reader	O
.	O

It	O
sums	O
up	O
the	O
scores	O
from	O
the	O
same	O
entity	O
candidates	O
,	O
where	O
the	O
scores	O
are	O
calculated	O
by	O
the	O
inner	O
product	O
between	O
s	O
t	O
and	O
m	O
d	O
oc	O
e	O
,	O
where	O
m	O
d	O
oc	O
e	O
is	O
an	O
embedding	O
vector	O
of	O
one	O
entity	O
candidate	O
in	O
the	O
passage	O
.	O

Query	O
:	O
passenger	O
@placeholder	O
,	O
36	O
,	O
died	O
at	O
the	O
scene	O
Passage	O
:	O
(	O
@entity0	O
)	O
what	O
was	O
supposed	O
to	O
be	O
a	O
fantasy	O
sports	O
car	O
ride	O
at	O
@entity3	O
turned	O
deadly	O
when	O
a	O
@entity4	O
crashed	O
into	O
a	O
guardrail	O
.	O

the	O
crash	O
took	O
place	O
sunday	O
at	O
the	O
@entity8	O
,	O
which	O
bills	O
itself	O
as	O
a	O
chance	O
to	O
drive	O
your	O
dream	O
car	O
on	O
a	O
racetrack	O
.	O

the	O
@entity4	O
's	O
passenger	O
,	O
36	O
-	O
year	O
-	O
old	O
@entity14	O
of	O
@entity15	O
,	O
@entity16	O
,	O
died	O
at	O
the	O
scene	O
,	O
@entity13	O
said	O
.	O

the	O
driver	O
of	O
the	O
@entity4	O
,	O
24	O
-	O
year	O
-	O
old	O
@entity18	O
of	O
@entity19	O
,	O
@entity16	O
,	O
lost	O
control	O
of	O
the	O
vehicle	O
,	O
the	O
@entity13	O
said	O
.	O

he	O
was	O
hospitalized	O
with	O
minor	O
injuries	O
.	O

@entity24	O
,	O
which	O
operates	O
the	O
@entity8	O
at	O
@entity3	O
,	O
released	O
a	O
statement	O
sunday	O
night	O
about	O
the	O
crash	O
.	O

"	O
on	O
behalf	O
of	O
everyone	O
in	O
the	O
organization	O
,	O
it	O
is	O
with	O
a	O
very	O
heavy	O
heart	O
that	O
we	O
extend	O
our	O
deepest	O
sympathies	O
to	O
those	O
involved	O
in	O
today	O
's	O
tragic	O
accident	O
in	O
@entity36	O
,	O
"	O
the	O
company	O
said	O
.	O

@entity24	O
also	O
operates	O
the	O
@entity3	O
--	O
a	O
chance	O
to	O
drive	O
or	O
ride	O
in	O
@entity39	O
race	O
cars	O
named	O
for	O
the	O
winningest	O
driver	O
in	O
the	O
sport	O
's	O
history	O
.	O

@entity0	O
's	O
@entity43	O
and	O
@entity44	O
contributed	O
to	O
this	O
report	O
.	O

section	O
:	O
Answer	O
:	O
@entity14	O
Step	Metric
Termination	Metric
Probability	Metric
Attention	O
Sum	O
1	O
0.0011	O
0.4916	O
2	O
0.5747	O
0.5486	O
3	O
0.9178	O
0.5577	O
Step	O
3	O
Step	O
1	O
Step	O
2	O
Figure	O
3	O
:	O
Results	O
of	O
a	O
test	O
example	O
69e1f777e41bf67d5a22b7c69ae76f0ae873cf43.story	O
from	O
the	O
CNN	Material
dataset	Material
.	O

The	O
numbers	O
next	O
to	O
the	O
underline	O
bars	O
indicate	O
the	O
rank	O
of	O
the	O
attention	O
scores	O
.	O

The	O
corresponding	O
termination	Metric
probability	Metric
and	O
the	O
sum	O
of	O
attention	O
scores	O
for	O
the	O
answer	O
entity	O
are	O
shown	O
in	O
the	O
table	O
on	O
the	O
right	O
.	O

Results	O
:	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
all	O
the	O
existing	O
single	Method
model	Method
baselines	Method
and	O
our	O
proposed	O
ReasoNet	Method
.	O

Among	O
all	O
the	O
baselines	O
,	O
AS	O
Reader	O
could	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNet	Method
with	O
T	O
max	O
=	O
1	O
.	O

Comparing	O
with	O
the	O
AS	Method
Reader	Method
,	O
ReasoNet	Method
shows	O
the	O
signi	O
ca	O
nt	O
improvement	O
by	O
capturing	O
multi	Task
-	Task
turn	Task
reasoning	Task
in	O
the	O
paragraph	O
.	O

Iterative	Method
Attention	Method
Reader	Method
,	O
EpiReader	Method
and	O
GA	Method
Reader	Method
are	O
the	O
three	O
multi	Method
-	Method
turn	Method
reasoning	Method
models	Method
with	O
xed	O
reasoning	O
steps	O
.	O

ReasoNet	Method
also	O
outperforms	O
all	O
of	O
them	O
by	O
integrating	O
termination	O
gate	O
in	O
the	O
model	O
which	O
allows	O
di	O
erent	O
reasoning	O
steps	O
for	O
di	O
erent	O
test	O
cases	O
.	O

AoA	O
Reader	Method
is	O
another	O
single	Method
-	Method
turn	Method
reasoning	Method
model	Method
,	O
it	O
captures	O
the	O
word	O
alignment	O
signals	O
between	O
query	O
and	O
passage	O
,	O
and	O
shows	O
a	O
big	O
improvement	O
over	O
AS	O
Reader	Method
.	O

ReasoNet	Method
obtains	O
comparable	O
results	O
with	O
AoA	Method
Reader	Method
on	O
CNN	Material
test	Material
set	Material
.	O

We	O
expect	O
that	O
ReasoNet	Method
could	O
be	O
improved	O
further	O
by	O
incorporating	O
the	O
word	O
alignment	O
information	O
in	O
the	O
memory	Method
module	Method
as	O
suggested	O
in	O
AoA	Method
Reader	Method
.	O

We	O
show	O
the	O
distribution	O
of	O
termination	O
step	O
distribution	O
of	O
ReasoNets	O
in	O
the	O
CNN	Material
dataset	Material
in	O
Figure	O
2	O
.	O

The	O
distributions	O
spread	O
out	O
across	O
di	O
erent	O
steps	O
.	O

Around	O
70	O
%	O
of	O
the	O
instances	O
terminate	O
in	O
the	O
last	O
step	O
.	O

Figure	O
3	O
gives	O
a	O
test	O
example	O
on	O
CNN	Material
dataset	Material
,	O
which	O
illustrates	O
the	O
inference	Task
process	Task
of	O
the	O
ReasoNet	Method
.	O

The	O
model	O
initially	O
focuses	O
on	O
wrong	O
entities	O
with	O
low	O
termination	O
probability	O
.	O

In	O
the	O
second	O
and	O
third	O
steps	O
,	O
the	O
model	O
focuses	O
on	O
the	O
right	O
clue	O
with	O
higher	O
termination	O
probability	O
.	O

Interestingly	O
,	O
we	O
also	O
nd	O
its	O
query	O
attention	O
focuses	O
on	O
the	O
placeholder	O
token	O
throughout	O
all	O
the	O
steps	O
.	O

section	O
:	O
SQuAD	Material
Dataset	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
ReasoNet	Method
model	O
on	O
the	O
task	O
of	O
question	Task
answering	Task
using	O
the	O
SQuAD	Material
dataset	O
[	O
reference	O
]	O
.	O

[	O
reference	O
]	O
SQuAD	Material
is	O
a	O
machine	O
comprehension	O
dataset	O
on	O
536	O
Wikipedia	Material
articles	Material
,	O
with	O
more	O
than	O
100	O
,	O
000	O
questions	O
.	O

Two	O
metrics	O
are	O
used	O
to	O
evaluate	O
models	O
:	O
Exact	Metric
Match	Metric
(	O
EM	Metric
)	O
and	O
a	O
softer	Metric
metric	Metric
,	O
F1	Metric
score	Metric
,	O
which	O
measures	O
the	O
weighted	O
average	O
of	O
the	O
precision	Metric
and	O
recall	Metric
rate	O
at	O
the	O
character	Metric
level	Metric
.	O

The	O
dataset	O
consists	O
of	O
90k	O
/	O
10k	O
training	O
/	O
dev	O
question	O
-	O
contextanswer	O
tuples	O
with	O
a	O
large	O
hidden	O
test	O
set	O
.	O

The	O
model	O
architecture	O
used	O
for	O
this	O
task	O
is	O
as	O
follows	O
:	O
[	O
reference	O
]	O
SQuAD	Material
Competition	O
Website	O
is	O
https:	O
//	O
rajpurkar.github.io	O
/	O
SQuAD	Material
-	O
explorer	O
/	O
Vocab	O
Size	O
:	O
We	O
use	O
the	O
python	Method
NLTK	Method
tokenizer	Method
6	O
to	O
preprocess	O
passages	O
and	O
questions	O
,	O
and	O
obtain	O
about	O
100	O
K	O
words	O
in	O
the	O
vocabulary	O
.	O

Embedding	Method
Layer	Method
:	O
We	O
use	O
the	O
100	O
-	O
dimensional	O
pretrained	O
Glove	O
vectors	O
[	O
reference	O
]	O
as	O
word	O
embeddings	O
.	O

These	O
Glove	O
vectors	O
are	O
xed	O
during	O
the	O
model	Method
training	Method
.	O

To	O
alleviate	O
the	O
out	Task
-	Task
of	Task
-	Task
vocabulary	Task
issue	Task
,	O
we	O
adopt	O
one	Method
layer	Method
100	Method
-	Method
dimensional	Method
convolutional	Method
neural	Method
network	Method
on	O
character	O
-	O
level	O
with	O
a	O
width	O
size	O
of	O
5	O
and	O
each	O
character	O
encoded	O
as	O
an	O
8	O
-	O
dimensional	O
vector	O
following	O
the	O
work	O
[	O
reference	O
]	O
.	O

The	O
100	O
-	O
dimensional	O
Glove	O
word	O
vector	O
and	O
the	O
100	O
-	O
dimensional	O
character	O
-	O
level	O
vector	O
are	O
concatenated	O
to	O
obtain	O
a	O
200	O
-	O
dimensional	O
vector	O
for	O
each	O
word	O
.	O

Bi	Method
-	Method
GRU	Method
Encoder	Method
:	O
We	O
apply	O
bidirectional	Method
GRU	Method
for	O
encoding	O
query	Task
and	Task
passage	Task
into	O
vector	Method
representations	Method
.	O

The	O
number	O
of	O
hidden	O
units	O
is	O
set	O
to	O
128	O
.	O

Memory	Method
:	O
We	O
use	O
bidirectional	Method
-	Method
GRU	Method
encoders	Method
to	O
extract	O
the	O
query	Method
representation	Method
M	Method
quer	Method
and	O
the	O
passage	Method
representation	Method
M	Method
doc	Method
,	O
given	O
a	O
query	O
and	O
a	O
passage	O
.	O

We	O
compute	O
the	O
similarity	O
matrix	O
Table	O
4	O
:	O
Small	O
and	O
large	O
random	O
graph	O
in	O
the	O
Graph	Material
Reachability	Material
dataset	Material
.	O

Note	O
that	O
"	O
A	O
→	O
B	O
"	O
represents	O
an	O
edge	O
connected	O
from	O
A	O
to	O
B	O
and	O
the	O
#	O
symbol	O
is	O
used	O
as	O
a	O
delimiter	O
between	O
di	O
erent	O
edges	O
.	O

Small	Task
Graph	Task
Large	Task
Graph	Task
No	O
Yes	O
between	O
each	O
word	O
in	O
the	O
query	O
and	O
each	O
word	O
in	O
the	O
passage	O
.	O

The	O
similarity	O
matrix	O
is	O
denoted	O
as	O
S	O
∈	O
R	O
T×	O
,	O
where	O
T	O
and	O
are	O
the	O
number	O
of	O
words	O
in	O
the	O
passage	O
and	O
query	O
,	O
respectively	O
,	O
and	O
]	O
∈	O
R	O
,	O
where	O
w	O
S	O
is	O
a	O
trainable	O
weight	O
vector	O
,	O
•	O
denotes	O
the	O
elementwise	Method
multiplication	Method
,	O
and	O
[	O
;	O
]	O
is	O
the	O
vector	O
concatenation	O
across	O
row	O
.	O

We	O
then	O
compute	O
the	O
context	Task
-	Task
to	Task
-	Task
query	Task
attention	Task
and	O
query	Task
-	Task
to	Task
-	Task
context	Task
attention	Task
from	O
the	O
similarity	O
matrix	O
S	O
by	O
following	O
recent	O
co	Method
-	Method
attention	Method
work	O
[	O
reference	O
]	O
to	O
obtain	O
the	O
query	Method
-	Method
aware	Method
passage	Method
representation	Method
G.	Method
We	O
feed	O
G	O
to	O
a	O
128	Method
-	Method
dimensional	Method
bidirectional	Method
GRU	Method
to	O
obtain	O
the	O
memory	O
M	O
=	O
bidirectional	Method
-	Method
GRU	Method
(	Method
G	Method
)	O
,	O
where	O
M	O
∈	O
R	O
256×T	O
.	O

Internal	Method
State	Method
Controller	Method
:	O
We	O
use	O
a	O
GRU	Method
model	Method
with	O
256	Method
-	Method
dimensional	Method
hidden	Method
units	Method
as	O
the	O
internal	Method
state	Method
controller	Method
.	O

The	O
initial	O
state	O
of	O
the	O
GRU	Method
controller	Method
is	O
the	O
last	Method
-	Method
word	Method
representation	Method
of	O
the	O
query	Method
bidirectional	Method
-	Method
GRU	Method
encoder	Method
.	O

Termination	Method
Module	Method
:	O
We	O
use	O
the	O
same	O
termination	Method
module	Method
as	O
in	O
the	O
CNN	Task
and	Task
Daily	Task
Mail	Task
experiments	Task
.	O

Answer	Method
Module	Method
:	O
SQuAD	Material
task	O
requires	O
the	O
model	O
to	O
nd	O
a	O
span	O
in	O
the	O
passage	O
to	O
answer	O
the	O
query	O
.	O

Thus	O
the	O
answer	Method
module	Method
requires	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
the	O
answer	O
span	O
in	O
the	O
passage	O
.	O

The	O
probability	O
distribution	O
of	O
selecting	O
the	O
start	O
index	O
over	O
the	O
passage	O
at	O
state	O
s	O
t	O
is	O
computed	O
by	O
:	O
where	O
S	O
t	O
is	O
given	O
via	O
tiling	O
s	O
t	O
by	O
T	O
times	O
across	O
the	O
column	O
and	O
w	O
p	O
1	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

The	O
probability	O
distribution	O
of	O
selecting	O
the	O
end	O
index	O
over	O
passage	O
is	O
computed	O
in	O
a	O
similar	O
manner	O
:	O
Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
T	O
max	O
is	O
set	O
to	O
10	O
in	O
SQuAD	Material
experiments	O
.	O

We	O
use	O
AdaDelta	Method
optimizer	Method
[	O
reference	O
]	O
for	O
parameter	Task
optimization	Task
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

Models	O
are	O
trained	O
on	O
GTX	Method
TitanX	O
12	O
GB	O
.	O

It	O
takes	O
about	O
40	O
minutes	O
per	O
epoch	O
for	O
training	Task
,	O
with	O
18	O
epochs	O
in	O
total	O
.	O

Results	O
:	O
In	O
the	O
Table	O
2	O
,	O
we	O
report	O
the	O
performance	O
of	O
all	O
models	O
in	O
the	O
SQuAD	Material
leaderboard	O
.	O

[	O
reference	O
]	O
In	O
the	O
upper	O
part	O
of	O
the	O
Table	O
2	O
,	O
we	O
compare	O
ReasoNet	Method
with	O
all	O
published	O
baselines	O
at	O
the	O
time	O
of	O
submission	O
.	O

Speci	O
cally	O
,	O
BiDAF	Method
model	Method
could	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNet	Method
with	O
T	O
max	O
=	O
1	O
.	O

It	O
is	O
worth	O
noting	O
that	O
this	O
SQuAD	Material
leaderboard	O
is	O
highly	O
active	O
and	O
competitive	O
.	O

The	O
test	O
set	O
is	O
hidden	O
to	O
all	O
models	O
and	O
all	O
the	O
results	O
on	O
the	O
leaderboard	O
are	O
produced	O
and	O
reported	O
by	O
the	O
organizer	O
;	O
thus	O
all	O
the	O
results	O
here	O
are	O
reproducible	O
.	O

In	O
Table	O
2	O
,	O
we	O
demonstrate	O
that	O
ReasoNet	Method
outperforms	O
all	O
existing	O
published	O
approaches	O
.	O

While	O
we	O
compare	O
ReasoNet	Method
with	O
BiDAF	Method
,	O
ReasoNet	Method
exceeds	O
BiDAF	Method
both	O
in	O
single	O
model	O
and	O
ensemble	Method
model	Method
cases	Method
.	O

This	O
demonstrates	O
the	O
importance	O
of	O
the	O
dynamic	Task
multi	Task
-	Task
turn	Task
reasoning	Task
over	O
a	O
passage	O
.	O

In	O
the	O
bottom	O
part	O
of	O
Table	O
2	O
,	O
we	O
compare	O
ReasoNet	Method
with	O
all	O
unpublished	O
methods	O
at	O
the	O
time	O
of	O
this	O
submission	O
,	O
ReasoNet	Method
holds	O
the	O
second	O
position	O
in	O
all	O
the	O
competing	O
approaches	O
in	O
the	O
SQuAD	Material
leaderboard	O
.	O

section	O
:	O
Graph	Task
Reachability	Task
Task	Task
Recent	O
analysis	O
and	O
results	O
[	O
reference	O
]	O
on	O
the	O
cloze	Task
-	Task
style	Task
machine	Task
comprehension	Task
tasks	Task
have	O
suggested	O
some	O
simple	O
models	O
without	O
multiturn	Method
reasoning	Method
can	O
achieve	O
reasonable	O
performance	O
.	O

Based	O
on	O
these	O
results	O
,	O
we	O
construct	O
a	O
synthetic	Material
structured	Material
Graph	Material
Reachability	Material
dataset	Material
[	O
reference	O
]	O
to	O
evaluate	O
longer	Task
range	Task
machine	Task
inference	Task
and	Task
reasoning	Task
capability	Task
,	O
since	O
we	O
anticipate	O
ReasoNets	Method
to	O
have	O
the	O
capability	O
to	O
handle	O
long	O
range	O
relationships	O
.	O

We	O
generate	O
two	O
synthetic	O
datasets	O
:	O
a	O
small	O
graph	O
dataset	O
and	O
a	O
large	O
graph	O
dataset	O
.	O

In	O
the	O
small	O
graph	O
dataset	O
,	O
it	O
contains	O
500	O
K	O
small	O
graphs	O
,	O
where	O
each	O
graph	O
contains	O
9	O
nodes	O
and	O
16	O
direct	O
Step	O
1	O
Step	O
2	O
Step	O
0	O
Step	O
3	O
Step	O
4	O
,	O
5	O
,	O
7	O
Step	O
6	O
,	O
8	O
Step	O
9	O
Step	O
Step	O
2	O
Step	O
3	O
Step	O
1	O
Step	O
4	O
Steps	O
5	O
,	O
6	O
,	O
8	O
Steps	O
7	O
,	O
9	O
Step	O
10	O
Figure	O
4	O
:	O
An	O
example	O
of	O
graph	Task
reachability	Task
result	O
,	O
given	O
a	O
query	O
"	O
10	O
→	O
17	O
"	O
(	O
Answer	O
:	O
Yes	O
)	O
.	O

The	O
red	O
circles	O
highlight	O
the	O
nodes	O
/	O
edges	O
which	O
have	O
the	O
highest	O
attention	O
in	O
each	O
step	O
.	O

The	O
corresponding	O
termination	Metric
probability	Metric
and	O
prediction	Task
results	O
are	O
shown	O
in	O
the	O
table	O
.	O

The	O
model	O
terminates	O
at	O
step	O
10	O
.	O

edges	O
to	O
randomly	O
connect	O
pairs	O
of	O
nodes	O
.	O

The	O
large	O
graph	O
dataset	O
contains	O
500	O
K	O
graphs	O
,	O
where	O
each	O
graph	O
contains	O
18	O
nodes	O
and	O
32	O
random	O
direct	O
edges	O
.	O

Duplicated	O
edges	O
are	O
removed	O
.	O

Table	O
3	O
shows	O
the	O
graph	O
reachability	O
statistics	O
on	O
the	O
two	O
datasets	O
.	O

In	O
Table	O
4	O
,	O
we	O
show	O
examples	O
of	O
a	O
small	O
graph	O
and	O
a	O
large	O
graph	O
in	O
the	O
synthetic	O
dataset	O
.	O

Both	O
graph	O
and	O
query	O
are	O
represented	O
by	O
a	O
sequence	O
of	O
symbols	O
.	O

The	O
details	O
settings	O
of	O
the	O
ReasoNet	Method
are	O
listed	O
as	O
follows	O
in	O
the	O
reachability	Task
tasks	Task
.	O

Embedding	Method
Layer	Method
We	O
use	O
a	O
100	O
-	O
dimensional	O
embedding	O
vector	O
for	O
each	O
symbol	O
in	O
the	O
query	Task
and	Task
graph	Task
description	Task
.	O

Bi	Method
-	Method
LSTM	Method
Encoder	Method
:	O
We	O
apply	O
a	O
bidirectional	Method
-	Method
LSTM	Method
layer	Method
with	O
128	O
and	O
256	O
cells	O
on	O
query	O
embeddings	O
in	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

The	O
last	O
states	O
of	O
bidirectional	Method
-	Method
LSTM	Method
on	O
query	O
are	O
concatenated	O
to	O
be	O
the	O
initial	O
internal	O
state	O
We	O
apply	O
another	O
bidirectional	Method
-	Method
LSTM	Method
layer	Method
with	O
128	O
and	O
256	O
cells	O
on	O
graph	Task
description	Task
embeddings	Task
in	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

It	O
maps	O
each	O
symbol	O
i	O
to	O
a	O
contextual	Method
representation	Method
given	O
by	O
the	O
concatenation	Method
of	Method
forward	Method
and	Method
backward	Method
LSTM	Method
hidden	Method
states	Method
section	O
:	O
Internal	Method
State	Method
Controller	Method
:	O
We	O
use	O
a	O
GRU	Method
model	Method
with	O
128	O
-	O
dimensional	O
and	O
256	O
-	O
dimensional	O
hidden	O
units	O
as	O
the	O
internal	Method
state	Method
controller	Method
for	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

The	O
initial	O
state	O
of	O
the	O
GRU	Method
controller	Method
is	O
s	O
1	O
.	O

Answer	Method
Module	Method
:	O
The	O
nal	O
answer	O
is	O
either	O
"	O
Yes	O
"	O
or	O
"	O
No	O
"	O
and	O
hence	O
logistical	Method
regression	Method
is	O
used	O
as	O
the	O
answer	O
module	O
:	O
Termination	Method
Module	Method
:	O
We	O
use	O
the	O
same	O
termination	Method
module	Method
as	O
in	O
the	O
CNN	Task
and	Task
Daily	Task
Mail	Task
experiments	Task
.	O

Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
T	O
max	O
is	O
set	O
to	O
15	O
and	O
25	O
for	O
the	O
small	O
graph	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
.	O

We	O
use	O
AdaDelta	Method
optimizer	Method
[	O
reference	O
]	O
for	O
parameter	Task
optimization	Task
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

We	O
denote	O
"	O
ReasoNet	Method
"	O
as	O
the	O
standard	O
ReasoNet	Method
with	O
termination	O
gate	O
,	O
as	O
described	O
in	O
Section	O
3.1	O
.	O

To	O
study	O
the	O
e	O
ectiveness	O
of	O
the	O
termination	O
gate	O
in	O
ReasoNets	O
,	O
we	O
remove	O
the	O
termination	O
gate	O
and	O
use	O
the	O
prediction	O
from	O
the	O
last	O
state	O
,	O
â	O
=	O
a	O
T	O
max	O
(	O
T	O
max	O
is	O
the	O
maximum	O
reasoning	O
step	O
)	O
,	O
denoted	O
as	O
"	O
ReasoNet	Method
-	Method
Last	Method
"	O
.	O

To	O
study	O
the	O
e	O
ectiveness	O
of	O
multi	Task
-	Task
turn	Task
reasoning	Task
,	O
we	O
choose	O
"	O
ReasoNet	Method
-	O
T	O
max	O
=	O
2	O
"	O
,	O
which	O
only	O
has	O
single	Method
-	Method
turn	Method
reasoning	Method
.	O

We	O
compare	O
ReasoNets	Method
with	O
a	O
two	Method
layer	Method
deep	Method
LSTM	Method
model	Method
[	O
reference	O
]	O
with	O
128	O
hidden	O
units	O
,	O
denoted	O
as	O
"	O
Deep	Method
LSTM	Method
Reader	Method
"	O
,	O
as	O
a	O
baseline	O
.	O

Table	O
5	O
shows	O
the	O
performance	O
of	O
these	O
models	O
on	O
the	O
graph	Material
reachability	Material
dataset	Material
.	O

Deep	Method
LSTM	Method
Reader	Method
achieves	O
90.92	O
%	O
and	O
71.55	O
%	O
accuracy	Metric
in	O
the	O
small	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
,	O
which	O
indicates	O
the	O
graph	Task
reachibility	Task
task	Task
is	O
not	O
trivial	O
.	O

The	O
results	O
of	O
ReasoNet	Method
-	O
T	O
max	O
=	O
2	O
are	O
comparable	O
with	O
the	O
results	O
of	O
Deep	Method
LSTM	Method
Reader	Method
,	O
since	O
both	O
Deep	Method
LSTM	Method
Reader	Method
and	O
ReasoNet	Method
-	O
T	O
max	O
=	O
2	O
perform	O
single	Method
-	Method
turn	Method
reasoning	Method
.	O

The	O
ReasoNet	Method
-	O
Last	O
model	O
achieves	O
100	O
%	O
accuracy	Metric
on	O
the	O
small	O
graph	O
dataset	O
,	O
while	O
the	O
ReasoNet	Method
-	O
Last	O
model	O
achieves	O
Step	O
Step	O
1	O
Step	O
2	O
Step	O
2	O
Step	O
1	O
Step	O
1	O
Step	O
1	O
Figure	O
4	O
,	O
we	O
can	O
observe	O
that	O
the	O
model	O
does	O
not	O
make	O
a	O
rm	Task
prediction	Task
till	O
step	O
9	O
.	O

The	O
highest	O
attention	O
word	O
at	O
each	O
step	O
shows	O
the	O
reasoning	O
process	O
of	O
the	O
model	O
.	O

Interestingly	O
,	O
the	O
model	O
starts	O
from	O
the	O
end	O
node	O
[	O
reference	O
]	O
,	O
traverses	O
backward	O
till	O
nding	O
the	O
starting	O
node	O
(	O
10	O
)	O
in	O
step	O
9	O
,	O
and	O
makes	O
a	O
rm	Task
termination	Task
prediction	Task
.	O

On	O
the	O
other	O
hand	O
,	O
in	O
Figure	O
5	O
,	O
the	O
model	O
learns	O
to	O
stop	O
in	O
step	O
2	O
.	O

In	O
step	O
1	O
,	O
the	O
model	O
looks	O
for	O
neighbor	O
nodes	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
to	O
4	O
and	O
9	O
.	O

Then	O
,	O
the	O
model	O
gives	O
up	O
in	O
step	O
2	O
and	O
predict	O
"	O
No	O
"	O
.	O

All	O
of	O
these	O
demonstrate	O
the	O
dynamic	O
termination	O
characteristic	O
and	O
potential	O
reasoning	O
capability	O
of	O
ReasoNets	Method
.	O

To	O
better	O
grasp	O
when	O
ReasoNets	O
stop	O
reasoning	O
,	O
we	O
show	O
the	O
distribution	O
of	O
termination	O
steps	O
in	O
ReasoNets	O
on	O
the	O
test	O
set	O
.	O

The	O
termination	O
step	O
is	O
chosen	O
with	O
the	O
maximum	O
termination	O
probability	O
p	O
(	O
k	O
)	O
=	O
t	O
k	O
k	O
−1	O
i=1	O
(	O
1	O
−	O
t	O
i	O
)	O
,	O
where	O
t	O
i	O
is	O
the	O
termination	O
probability	O
at	O
step	O
i.	O
Figure	O
6	O
shows	O
the	O
termination	O
step	O
distribution	O
of	O
ReasoNets	O
in	O
the	O
graph	Material
reachability	Material
dataset	Material
.	O

The	O
distributions	O
spread	O
out	O
across	O
di	O
erent	O
steps	O
.	O

Around	O
16	O
%	O
and	O
35	O
%	O
of	O
the	O
instances	O
terminate	O
in	O
the	O
last	O
step	O
for	O
the	O
small	O
and	O
large	O
graph	O
,	O
respectively	O
.	O

We	O
study	O
the	O
correlation	O
between	O
the	O
termination	O
steps	O
and	O
the	O
complexity	O
of	O
test	O
instances	O
in	O
Figure	O
7	O
.	O

Given	O
the	O
query	O
,	O
we	O
use	O
the	O
Breadth	Method
-	Method
First	Method
Search	Method
(	O
BFS	Method
)	O
algorithm	O
over	O
the	O
target	O
graph	O
to	O
analyze	O
the	O
complexity	O
of	O
test	O
instances	O
.	O

For	O
example	O
,	O
BFS	Method
-	O
Step	O
Figure	O
7	O
shows	O
that	O
test	O
instances	O
with	O
larger	O
BFS	Method
-	O
Steps	O
require	O
more	O
reasoning	O
steps	O
.	O

The	O
correlation	O
between	O
BFS	Method
steps	O
and	O
ReasoNet	Method
termination	O
steps	O
in	O
the	O
graph	O
reachability	O
dataset	O
,	O
where	O
T	O
max	O
is	O
set	O
to	O
15	O
and	O
25	O
in	O
the	O
small	O
graph	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
,	O
and	O
BFS	Method
-	O
Step=	O
−1	O
denotes	O
unreachable	O
cases	O
.	O

The	O
value	O
indicates	O
the	O
number	O
of	O
instances	O
in	O
each	O
case	O
.	O

section	O
:	O
CONCLUSION	O
In	O
this	O
paper	O
,	O
we	O
propose	O
ReasoNets	Method
that	O
dynamically	O
decide	O
whether	O
to	O
continue	O
or	O
to	O
terminate	O
the	O
inference	Task
process	Task
in	O
machine	Task
comprehension	Task
tasks	Task
.	O

With	O
the	O
use	O
of	O
the	O
instance	Method
-	Method
dependent	Method
baseline	Method
method	Method
,	O
our	O
proposed	O
model	O
achieves	O
superior	O
results	O
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	Material
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
,	O
the	O
Stanford	Material
SQuAD	Material
dataset	Material
,	O
and	O
a	O
proposed	O
structured	O
Graph	O
Reachability	O
dataset	O
.	O

section	O
:	O
document	O
:	O
Dynamic	Task
Integration	Task
of	Task
Background	Task
Knowledge	Task
in	O
Neural	O
NLU	Task
Systems	O
Common	O
-	O
sense	O
and	O
background	O
knowledge	O
is	O
required	O
to	O
understand	O
natural	O
language	O
,	O
but	O
in	O
most	O
neural	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
systems	O
,	O
this	O
knowledge	O
must	O
be	O
acquired	O
from	O
training	O
corpora	O
during	O
learning	O
,	O
and	O
then	O
it	O
is	O
static	O
at	O
test	O
time	O
.	O

We	O
introduce	O
a	O
new	O
architecture	O
for	O
the	O
dynamic	Task
integration	Task
of	Task
explicit	Task
background	Task
knowledge	Task
in	O
NLU	Task
models	O
.	O

A	O
general	O
-	O
purpose	O
reading	Method
module	Method
reads	O
background	O
knowledge	O
in	O
the	O
form	O
of	O
free	O
-	O
text	O
statements	O
(	O
together	O
with	O
task	O
-	O
specific	O
text	O
inputs	O
)	O
and	O
yields	O
refined	O
word	Method
representations	Method
to	O
a	O
task	O
-	O
specific	O
NLU	Task
architecture	O
that	O
reprocesses	O
the	O
task	O
inputs	O
with	O
these	O
representations	O
.	O

Experiments	O
on	O
document	Task
question	Task
answering	Task
(	O
DQA	Task
)	O
and	O
recognizing	Task
textual	Task
entailment	Task
(	O
RTE	Task
)	O
demonstrate	O
the	O
effectiveness	O
and	O
flexibility	O
of	O
the	O
approach	O
.	O

Analysis	O
shows	O
that	O
our	O
model	O
learns	O
to	O
exploit	O
knowledge	O
in	O
a	O
semantically	O
appropriate	O
way	O
.	O

section	O
:	O
Introduction	O
Understanding	Task
natural	Task
language	Task
depends	O
crucially	O
on	O
common	O
-	O
sense	O
and	O
background	O
knowledge	O
,	O
for	O
example	O
,	O
knowledge	O
about	O
what	O
concepts	O
are	O
expressed	O
by	O
the	O
words	O
being	O
read	O
(	O
lexical	O
knowledge	O
)	O
,	O
and	O
what	O
relations	O
hold	O
between	O
these	O
concepts	O
(	O
relational	O
knowledge	O
)	O
.	O

As	O
a	O
simple	O
illustration	O
,	O
if	O
an	O
agent	O
needs	O
to	O
understand	O
that	O
the	O
statement	O
“	O
King	O
Farouk	O
signed	O
his	O
abdication	O
”	O
is	O
entailed	O
by	O
“	O
King	O
Farouk	O
was	O
exiled	O
to	O
France	O
in	O
1952	O
,	O
after	O
signing	O
his	O
resignation	O
”	O
,	O
it	O
must	O
know	O
(	O
among	O
other	O
things	O
)	O
that	O
abdication	O
means	O
resignation	O
of	O
a	O
king	O
.	O

In	O
most	O
neural	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
systems	O
,	O
the	O
requisite	O
background	O
knowledge	O
is	O
implicitly	O
encoded	O
in	O
the	O
models	O
’	O
parameters	O
.	O

That	O
is	O
,	O
what	O
background	O
knowledge	O
is	O
present	O
has	O
been	O
learned	O
from	O
task	Task
supervision	Task
and	O
also	O
by	O
pre	O
-	O
training	O
word	Method
embeddings	Method
(	O
where	O
distributional	O
properties	O
correlate	O
with	O
certain	O
kinds	O
of	O
useful	O
background	O
knowledge	O
,	O
such	O
as	O
semantic	O
relatedness	O
)	O
.	O

However	O
,	O
acquisition	O
of	O
background	O
knowledge	O
from	O
static	O
training	O
corpora	O
is	O
limiting	O
for	O
two	O
reasons	O
.	O

First	O
,	O
it	O
is	O
unreasonable	O
to	O
expect	O
that	O
all	O
background	O
knowledge	O
that	O
could	O
be	O
important	O
for	O
solving	O
an	O
NLU	Task
task	O
can	O
be	O
extracted	O
from	O
a	O
limited	O
amount	O
of	O
training	O
data	O
.	O

Second	O
,	O
as	O
the	O
world	O
changes	O
,	O
the	O
facts	O
that	O
may	O
influence	O
how	O
a	O
text	O
is	O
understood	O
will	O
likewise	O
change	O
.	O

In	O
short	O
:	O
building	O
suitably	O
large	O
corpora	O
to	O
capture	O
all	O
relevant	O
information	O
,	O
and	O
keeping	O
the	O
corpus	O
and	O
derived	O
models	O
up	O
to	O
date	O
with	O
changes	O
to	O
the	O
world	O
would	O
be	O
impractical	O
.	O

