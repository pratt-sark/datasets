Canonical	B-Method
Polyadic	E-Method
(	O
CP	S-Method
)	O
decomposition	O
fails	O
on	O
both	O
relations	O
as	O
it	O
has	O
to	O
push	O
symmetric	O
and	O
antisymmetric	O
patterns	O
through	O
the	O
entity	O
embeddings	O
.	O

Surprisingly	O
,	O
only	O
our	O
model	O
succeeds	O
on	O
such	O
simple	O
data	O
.	O

subsection	O
:	O
Datasets	O
:	O
FB15	O
K	O
and	O
WN18	S-Material
We	O
next	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
FB15	O
K	O
and	O
WN18	B-Material
datasets	E-Material
.	O

FB15	O
K	O
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
curated	O
KB	O
of	O
general	O
facts	O
,	O
whereas	O
WN18	S-Material
is	O
a	O
subset	O
of	O
Wordnet	S-Material
,	O
a	O
database	O
featuring	O
lexical	O
relations	O
between	O
words	O
.	O

We	O
use	O
original	O
training	O
,	O
validation	O
and	O
test	O
set	O
splits	O
as	O
provided	O
by	O
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
the	O
metadata	O
of	O
the	O
two	O
datasets	O
.	O

Both	O
datasets	O
contain	O
only	O
positive	O
triples	O
.	O

As	O
in	O
,	O
we	O
generated	O
negatives	O
using	O
the	O
local	O
closed	O
world	O
assumption	O
.	O

That	O
is	O
,	O
for	O
a	O
triple	O
,	O
we	O
randomly	O
change	O
either	O
the	O
subject	O
or	O
the	O
object	O
at	O
random	O
,	O
to	O
form	O
a	O
negative	O
example	O
.	O

This	O
negative	B-Method
sampling	E-Method
is	O
performed	O
at	O
runtime	O
for	O
each	O
batch	O
of	O
training	O
positive	O
examples	O
.	O

For	O
evaluation	O
,	O
we	O
measure	O
the	O
quality	O
of	O
the	O
ranking	S-Metric
of	O
each	O
test	O
triple	O
among	O
all	O
possible	O
subject	O
and	O
object	O
substitutions	O
:	O
and	O
,	O
.	O

Mean	B-Metric
Reciprocal	I-Metric
Rank	E-Metric
(	O
MRR	S-Metric
)	O
and	O
Hits	S-Metric
at	O
are	O
the	O
standard	O
evaluation	B-Metric
measures	E-Metric
for	O
these	O
datasets	O
and	O
come	O
in	O
two	O
flavours	O
:	O
raw	O
and	O
filtered	O
.	O

The	O
filtered	B-Metric
metrics	E-Metric
are	O
computed	O
after	O
removing	O
all	O
the	O
other	O
positive	O
observed	O
triples	O
that	O
appear	O
in	O
either	O
training	O
,	O
validation	O
or	O
test	O
set	O
from	O
the	O
ranking	S-Task
,	O
whereas	O
the	O
raw	O
metrics	O
do	O
not	O
remove	O
these	O
.	O

Since	O
ranking	B-Metric
measures	E-Metric
are	O
used	O
,	O
previous	O
studies	O
generally	O
preferred	O
a	O
pairwise	B-Metric
ranking	I-Metric
loss	E-Metric
for	O
the	O
task	O
.	O

We	O
chose	O
to	O
use	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
the	O
logistic	B-Method
model	E-Method
,	O
as	O
it	O
is	O
a	O
continuous	O
surrogate	O
of	O
the	O
sign	O
-	O
rank	O
,	O
and	O
has	O
been	O
shown	O
to	O
learn	O
compact	B-Method
representations	E-Method
for	O
several	O
important	O
relations	O
,	O
especially	O
for	O
transitive	O
relations	O
.	O

In	O
preliminary	O
work	O
,	O
we	O
tried	O
both	O
losses	O
,	O
and	O
indeed	O
the	O
log	B-Method
-	I-Method
likelihood	E-Method
yielded	O
better	O
results	O
than	O
the	O
ranking	B-Metric
loss	E-Metric
(	O
except	O
with	O
TransE	S-Method
)	O
,	O
especially	O
on	O
FB15K.	O
We	O
report	O
both	O
filtered	S-Metric
and	O
raw	B-Metric
MRR	E-Metric
,	O
and	O
filtered	O
Hits	B-Metric
at	I-Metric
1	E-Metric
,	O
3	S-Metric
and	O
10	S-Metric
in	O
Table	O
[	O
reference	O
]	O
for	O
the	O
evaluated	O
models	O
.	O

Furthermore	O
,	O
we	O
chose	O
TransE	O
,	O
DistMult	S-Method
and	O
HolE	S-Method
as	O
baselines	O
since	O
they	O
are	O
the	O
best	O
performing	O
models	O
on	O
those	O
datasets	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

We	O
also	O
compare	O
with	O
the	O
CP	S-Method
model	O
to	O
emphasize	O
empirically	O
the	O
importance	O
of	O
learning	O
unique	O
embeddings	O
for	O
entities	O
.	O

For	O
experimental	O
fairness	O
,	O
we	O
reimplemented	O
these	O
methods	O
within	O
the	O
same	O
framework	O
as	O
the	O
ComplEx	B-Method
model	E-Method
,	O
using	O
theano	O
.	O

However	O
,	O
due	O
to	O
time	O
constraints	O
and	O
the	O
complexity	O
of	O
an	O
efficient	O
implementation	O
of	O
HolE	S-Method
,	O
we	O
record	O
the	O
original	O
results	O
for	O
HolE	S-Method
as	O
reported	O
in	O
.	O

subsection	O
:	O
Results	O
WN18	S-Material
describes	O
lexical	O
and	O
semantic	O
hierarchies	O
between	O
concepts	O
and	O
contains	O
many	O
antisymmetric	O
relations	O
such	O
as	O
hypernymy	O
,	O
hyponymy	O
,	O
or	O
being	O
”	O
part	O
of	O
”	O
.	O

Indeed	O
,	O
the	O
DistMult	S-Method
and	O
TransE	B-Method
models	E-Method
are	O
outperformed	O
here	O
by	O
ComplEx	S-Method
and	O
HolE	S-Method
,	O
which	O
are	O
on	O
par	O
with	O
respective	O
filtered	B-Metric
MRR	I-Metric
scores	E-Metric
of	O
0.941	O
and	O
0.938	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
filtered	O
test	O
set	O
MRR	S-Metric
for	O
the	O
models	O
considered	O
and	O
each	O
relation	O
of	O
WN18	S-Material
,	O
confirming	O
the	O
advantage	O
of	O
our	O
model	O
on	O
antisymmetric	O
relations	O
while	O
losing	O
nothing	O
on	O
the	O
others	O
.	O

2D	O
projections	O
of	O
the	O
relation	O
embeddings	O
provided	O
in	O
Appendix	O
[	O
reference	O
]	O
visually	O
corroborate	O
the	O
results	O
.	O

On	O
FB15	O
K	O
,	O
the	O
gap	O
is	O
much	O
more	O
pronounced	O
and	O
the	O
ComplEx	B-Method
model	E-Method
largely	O
outperforms	O
HolE	S-Method
,	O
with	O
a	O
filtered	B-Metric
MRR	E-Metric
of	O
0.692	O
and	O
59.9	O
%	O
of	O
Hits	B-Metric
at	I-Metric
1	E-Metric
,	O
compared	O
to	O
0.524	O
and	O
40.2	O
%	O
for	O
HolE.	S-Method
We	O
attribute	O
this	O
to	O
the	O
simplicity	O
of	O
our	O
model	O
and	O
the	O
different	O
loss	O
function	O
.	O

This	O
is	O
supported	O
by	O
the	O
relatively	O
small	O
gap	O
in	O
MRR	S-Metric
compared	O
to	O
DistMult	S-Method
(	O
0.654	O
)	O
;	O
our	O
model	O
can	O
in	O
fact	O
be	O
interpreted	O
as	O
a	O
complex	O
number	O
version	O
of	O
DistMult	S-Method
.	O

On	O
both	O
datasets	O
,	O
TransE	S-Method
and	O
CP	S-Method
are	O
largely	O
left	O
behind	O
.	O

This	O
illustrates	O
the	O
power	O
of	O
the	O
simple	O
dot	B-Method
product	E-Method
in	O
the	O
first	O
case	O
,	O
and	O
the	O
importance	O
of	O
learning	O
unique	O
entity	O
embeddings	O
in	O
the	O
second	O
.	O

CP	S-Method
performs	O
poorly	O
on	O
WN18	S-Material
due	O
to	O
the	O
small	O
number	O
of	O
relations	O
,	O
which	O
magnifies	O
this	O
subject	O
/	O
object	O
difference	O
.	O

Reported	O
results	O
are	O
given	O
for	O
the	O
best	O
set	O
of	O
hyper	O
-	O
parameters	O
evaluated	O
on	O
the	O
validation	O
set	O
for	O
each	O
model	O
,	O
after	O
grid	B-Method
search	E-Method
on	O
the	O
following	O
values	O
:	O
,	O
,	O
,	O
with	O
the	O
regularization	O
parameter	O
,	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
(	O
then	O
tuned	O
at	O
runtime	O
with	O
AdaGrad	S-Method
)	O
,	O
and	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
triple	O
.	O

We	O
also	O
tried	O
varying	O
the	O
batch	O
size	O
but	O
this	O
had	O
no	O
impact	O
and	O
we	O
settled	O
with	O
100	O
batches	O
per	O
epoch	O
.	O

Best	O
ranks	O
were	O
generally	O
150	O
or	O
200	O
,	O
in	O
both	O
cases	O
scores	O
were	O
always	O
very	O
close	O
for	O
all	O
models	O
.	O

The	O
number	O
of	O
negative	O
samples	O
per	O
positive	O
sample	O
also	O
had	O
a	O
large	O
influence	O
on	O
the	O
filtered	O
MRR	S-Metric
on	O
FB15	O
K	O
(	O
up	O
to	O
+	O
0.08	O
improvement	O
from	O
1	O
to	O
10	S-Metric
negatives	O
)	O
,	O
but	O
not	O
much	O
on	O
WN18	S-Material
.	O

On	O
both	O
datasets	O
regularization	S-Task
was	O
important	O
(	O
up	O
to	O
+	O
0.05	O
on	O
filtered	B-Metric
MRR	E-Metric
between	O
and	O
optimal	O
one	O
)	O
.	O

We	O
found	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
to	O
be	O
very	O
important	O
on	O
FB15	O
K	O
,	O
while	O
not	O
so	O
much	O
on	O
WN18	S-Material
.	O

We	O
think	O
this	O
may	O
also	O
explain	O
the	O
large	O
gap	O
of	O
improvement	O
our	O
model	O
provides	O
on	O
this	O
dataset	O
compared	O
to	O
previously	O
published	O
results	O
–	O
as	O
DistMult	S-Method
results	O
are	O
also	O
better	O
than	O
those	O
previously	O
reported	O
–	O
along	O
with	O
the	O
use	O
of	O
the	O
log	B-Method
-	I-Method
likelihood	I-Method
objective	E-Method
.	O

It	O
seems	O
that	O
in	O
general	O
AdaGrad	S-Method
is	O
relatively	O
insensitive	O
to	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
,	O
perhaps	O
causing	O
some	O
overconfidence	O
in	O
its	O
ability	O
to	O
tune	O
the	O
step	O
size	O
online	O
and	O
consequently	O
leading	O
to	O
less	O
efforts	O
when	O
selecting	O
the	O
initial	O
step	O
size	O
.	O

Training	S-Task
was	O
stopped	O
using	O
early	O
stopping	O
on	O
the	O
validation	O
set	O
filtered	O
MRR	S-Metric
,	O
computed	O
every	O
50	O
epochs	O
with	O
a	O
maximum	O
of	O
1000	O
epochs	O
.	O

subsection	O
:	O
Influence	O
of	O
Negative	O
Samples	O
We	O
further	O
investigated	O
the	O
influence	O
of	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
sample	O
.	O

In	O
the	O
previous	O
experiment	O
,	O
due	O
to	O
computational	O
limitations	O
,	O
the	O
number	O
of	O
negatives	O
per	O
training	O
sample	O
,	O
,	O
was	O
validated	O
among	O
the	O
possible	O
numbers	O
.	O

We	O
want	O
to	O
explore	O
here	O
whether	O
increasing	O
these	O
numbers	O
could	O
lead	O
to	O
better	O
results	O
.	O

To	O
do	O
so	O
,	O
we	O
focused	O
on	O
FB15	O
K	O
,	O
with	O
the	O
best	O
validated	O
,	O
obtained	O
from	O
the	O
previous	O
experiment	O
.	O

We	O
then	O
let	O
vary	O
in	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
influence	O
of	O
the	O
number	O
of	O
generated	O
negatives	O
per	O
positive	O
training	O
triple	O
on	O
the	O
performance	O
of	O
our	O
model	O
on	O
FB15K.	O
Generating	O
more	O
negatives	O
clearly	O
improves	O
the	O
results	O
,	O
with	O
a	O
filtered	B-Metric
MRR	E-Metric
of	O
0.737	O
with	O
100	O
negative	O
triples	O
(	O
and	O
64.8	O
%	O
of	O
Hits@1	S-Metric
)	O
,	O
before	O
decreasing	O
again	O
with	O
200	O
negatives	O
.	O

The	O
model	O
also	O
converges	O
with	O
fewer	O
epochs	O
,	O
which	O
compensates	O
partially	O
for	O
the	O
additional	O
training	O
time	O
per	O
epoch	O
,	O
up	O
to	O
50	O
negatives	O
.	O

It	O
then	O
grows	O
linearly	O
as	O
the	O
number	O
of	O
negatives	O
increases	O
,	O
making	O
50	O
a	O
good	O
trade	O
-	O
off	O
between	O
accuracy	S-Metric
and	O
training	B-Metric
time	E-Metric
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
early	O
age	O
of	O
spectral	B-Method
theory	E-Method
in	O
linear	B-Task
algebra	E-Task
,	O
complex	O
numbers	O
were	O
not	O
used	O
for	O
matrix	B-Task
factorization	E-Task
and	O
mathematicians	O
mostly	O
focused	O
on	O
bi	O
-	O
linear	O
forms	O
.	O

The	O
eigen	B-Method
-	I-Method
decomposition	E-Method
in	O
the	O
complex	O
domain	O
as	O
taught	O
today	O
in	O
linear	B-Task
algebra	I-Task
courses	E-Task
came	O
40	O
years	O
later	O
.	O

Similarly	O
,	O
most	O
of	O
the	O
existing	O
approaches	O
for	O
tensor	B-Task
factorization	E-Task
were	O
based	O
on	O
decompositions	O
in	O
the	O
real	O
domain	O
,	O
such	O
as	O
the	O
Canonical	B-Method
Polyadic	E-Method
(	O
CP	S-Method
)	O
decomposition	O
.	O

These	O
methods	O
are	O
very	O
effective	O
in	O
many	O
applications	O
that	O
use	O
different	O
modes	O
of	O
the	O
tensor	O
for	O
different	O
types	O
of	O
entities	O
.	O

But	O
in	O
the	O
link	B-Task
prediction	I-Task
problem	E-Task
,	O
antisymmetry	B-Task
of	I-Task
relations	E-Task
was	O
quickly	O
seen	O
as	O
a	O
problem	O
and	O
asymmetric	O
extensions	O
of	O
tensors	O
were	O
studied	O
,	O
mostly	O
by	O
either	O
considering	O
independent	B-Method
embeddings	E-Method
or	O
considering	O
relations	O
as	O
matrices	O
instead	O
of	O
vectors	O
in	O
the	O
RESCAL	B-Method
model	E-Method
.	O

Direct	O
extensions	O
were	O
based	O
on	O
uni	O
-,	O
bi	O
-	O
and	O
trigram	O
latent	O
factors	O
for	O
triple	O
data	O
,	O
as	O
well	O
as	O
a	O
low	O
-	O
rank	O
relation	O
matrix	O
.	O

Pairwise	B-Method
interaction	I-Method
models	E-Method
were	O
also	O
considered	O
to	O
improve	O
prediction	S-Task
performances	O
.	O

For	O
example	O
,	O
the	O
Universal	B-Method
Schema	I-Method
approach	E-Method
factorizes	O
a	O
2D	B-Method
unfolding	I-Method
of	I-Method
the	I-Method
tensor	E-Method
(	O
a	O
matrix	O
of	O
entity	O
pairs	O
vs.	O
relations	O
)	O
while	O
extend	O
this	O
also	O
to	O
other	O
pairs	O
.	O

In	O
the	O
Neural	B-Method
Tensor	I-Method
Network	E-Method
(	O
NTN	S-Method
)	O
model	O
,	O
combine	O
linear	O
transformations	O
and	O
multiple	O
bilinear	O
forms	O
of	O
subject	O
and	O
object	O
embeddings	O
to	O
jointly	O
feed	O
them	O
into	O
a	O
nonlinear	B-Method
neural	I-Method
layer	E-Method
.	O

Its	O
non	O
-	O
linearity	O
and	O
multiple	O
ways	O
of	O
including	O
interactions	O
between	O
embeddings	O
gives	O
it	O
an	O
advantage	O
in	O
expressiveness	O
over	O
models	O
with	O
simpler	O
scoring	O
function	O
like	O
DistMult	S-Method
or	O
RESCAL	S-Method
.	O

As	O
a	O
downside	O
,	O
its	O
very	O
large	O
number	O
of	O
parameters	O
can	O
make	O
the	O
NTN	S-Method
model	O
harder	O
to	O
train	O
and	O
overfit	O
more	O
easily	O
.	O

The	O
original	O
multi	O
-	O
linear	O
DistMult	S-Method
model	O
is	O
symmetric	O
in	O
subject	O
and	O
object	O
for	O
every	O
relation	O
and	O
achieves	O
good	O
performance	O
,	O
presumably	O
due	O
to	O
its	O
simplicity	O
.	O

The	O
TransE	B-Method
model	E-Method
from	O
also	O
embeds	O
entities	O
and	O
relations	O
in	O
the	O
same	O
space	O
and	O
imposes	O
a	O
geometrical	O
structural	O
bias	O
into	O
the	O
model	O
:	O
the	O
subject	O
entity	O
vector	O
should	O
be	O
close	O
to	O
the	O
object	O
entity	O
vector	O
once	O
translated	O
by	O
the	O
relation	O
vector	O
.	O

A	O
recent	O
novel	O
way	O
to	O
handle	O
antisymmetry	O
is	O
via	O
the	O
Holographic	B-Method
Embeddings	E-Method
(	O
HolE	S-Method
)	O
model	O
by	O
.	O

In	O
HolE	S-Method
the	O
circular	O
correlation	O
is	O
used	O
for	O
combining	O
entity	O
embeddings	O
,	O
measuring	O
the	O
covariance	O
between	O
embeddings	O
at	O
different	O
dimension	O
shifts	O
.	O

This	O
generally	O
suggests	O
that	O
other	O
composition	O
functions	O
than	O
the	O
classical	O
tensor	B-Method
product	E-Method
can	O
be	O
helpful	O
as	O
they	O
allow	O
for	O
a	O
richer	O
interaction	O
of	O
embeddings	O
.	O

However	O
,	O
the	O
asymmetry	O
in	O
the	O
composition	O
function	O
in	O
HolE	S-Method
stems	O
from	O
the	O
asymmetry	O
of	O
circular	O
correlation	O
,	O
an	O
operation	O
,	O
whereas	O
ours	O
is	O
inherited	O
from	O
the	O
complex	O
inner	O
product	O
,	O
in	O
.	O

section	O
:	O
Conclusion	O
We	O
described	O
a	O
simple	O
approach	O
to	O
matrix	B-Task
and	I-Task
tensor	I-Task
factorization	E-Task
for	O
link	B-Task
prediction	I-Task
data	E-Task
that	O
uses	O
vectors	O
with	O
complex	O
values	O
and	O
retains	O
the	O
mathematical	O
definition	O
of	O
the	O
dot	O
product	O
.	O

The	O
class	B-Method
of	I-Method
normal	I-Method
matrices	E-Method
is	O
a	O
natural	O
fit	O
for	O
binary	O
relations	O
,	O
and	O
using	O
the	O
real	O
part	O
allows	O
for	O
efficient	O
approximation	O
of	O
any	O
learnable	O
relation	O
.	O

Results	O
on	O
standard	O
benchmarks	O
show	O
that	O
no	O
more	O
modifications	O
are	O
needed	O
to	O
improve	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

There	O
are	O
several	O
directions	O
in	O
which	O
this	O
work	O
can	O
be	O
extended	O
.	O

An	O
obvious	O
one	O
is	O
to	O
merge	O
our	O
approach	O
with	O
known	O
extensions	O
to	O
tensor	B-Method
factorization	E-Method
in	O
order	O
to	O
further	O
improve	O
predictive	S-Task
performance	O
.	O

For	O
example	O
,	O
the	O
use	O
of	O
pairwise	O
embeddings	O
together	O
with	O
complex	O
numbers	O
might	O
lead	O
to	O
improved	O
results	O
in	O
many	O
situations	O
that	O
involve	O
non	O
-	O
compositionality	O
.	O

Another	O
direction	O
would	O
be	O
to	O
develop	O
a	O
more	O
intelligent	O
negative	B-Method
sampling	I-Method
procedure	E-Method
,	O
to	O
generate	O
more	O
informative	O
negatives	O
with	O
respect	O
to	O
the	O
positive	O
sample	O
from	O
which	O
they	O
have	O
been	O
sampled	O
.	O

It	O
would	O
reduce	O
the	O
number	O
of	O
negatives	O
required	O
to	O
reach	O
good	O
performance	O
,	O
thus	O
accelerating	O
training	B-Metric
time	E-Metric
.	O

Also	O
,	O
if	O
we	O
were	O
to	O
use	O
complex	O
embeddings	O
every	O
time	O
a	O
model	O
includes	O
a	O
dot	O
product	O
,	O
e.g.	O
in	O
deep	B-Method
neural	I-Method
networks	E-Method
,	O
would	O
it	O
lead	O
to	O
a	O
similar	O
systematic	O
improvement	O
?	O
section	O
:	O
Acknowledgements	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
Paul	O
Allen	O
Foundation	O
through	O
an	O
Allen	O
Distinguished	O
Investigator	O
grant	O
and	O
in	O
part	O
by	O
a	O
Google	O
Focused	O
Research	O
Award	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
SGD	B-Method
algorithm	E-Method
We	O
describe	O
the	O
algorithm	O
to	O
learn	O
the	O
ComplEx	B-Method
model	E-Method
with	O
Stochastic	B-Method
Gradient	I-Method
Descent	E-Method
using	O
only	O
real	O
-	O
valued	O
vectors	O
.	O

Let	O
us	O
rewrite	O
equation	O
[	O
reference	O
]	O
,	O
by	O
denoting	O
the	O
real	O
part	O
of	O
embeddings	O
with	O
primes	O
and	O
the	O
imaginary	O
part	O
with	O
double	O
primes	O
:	O
,	O
,	O
,	O
.	O

The	O
set	O
of	O
parameters	O
is	O
,	O
and	O
the	O
scoring	B-Method
function	E-Method
involves	O
only	O
real	O
vectors	O
:	O
where	O
each	O
entity	O
and	O
each	O
relation	O
has	O
two	O
real	O
embeddings	O
.	O

Gradients	O
are	O
now	O
easy	O
to	O
write	O
:	O
where	O
is	O
the	O
element	O
-	O
wise	O
(	O
Hadamard	O
)	O
product	O
.	O

As	O
stated	O
in	O
equation	O
[	O
reference	O
]	O
we	O
use	O
the	O
sigmoid	O
link	O
function	O
,	O
and	O
minimize	O
the	O
-	O
regularized	O
negative	O
log	O
-	O
likelihood	O
:	O
To	O
handle	O
regularization	S-Task
,	O
note	O
that	O
the	O
squared	O
-	O
norm	O
of	O
a	O
complex	O
vector	O
is	O
the	O
sum	O
of	O
the	O
squared	O
modulus	O
of	O
each	O
entry	O
:	O
which	O
is	O
actually	O
the	O
sum	O
of	O
the	O
-	O
norms	O
of	O
the	O
vectors	O
of	O
the	O
real	O
and	O
imaginary	O
parts	O
.	O

We	O
can	O
finally	O
write	O
the	O
gradient	O
of	O
with	O
respect	O
to	O
a	O
real	O
embedding	O
for	O
one	O
triple	O
:	O
where	O
is	O
the	O
sigmoid	O
function	O
.	O

Algorithm	O
[	O
reference	O
]	O
describes	O
SGD	S-Method
for	O
this	O
formulation	O
of	O
the	O
scoring	O
function	O
.	O

When	O
contains	O
only	O
positive	O
triples	O
,	O
we	O
generate	O
negatives	O
per	O
positive	O
train	O
triple	O
,	O
by	O
corrupting	O
either	O
the	O
subject	O
or	O
the	O
object	O
of	O
the	O
positive	O
triple	O
,	O
as	O
described	O
in	O
.	O

[	O
t	O
]	O
SGD	S-Method
for	O
the	O
ComplEx	B-Method
model	E-Method
Training	O
set	O
,	O
Validation	O
set	O
,	O
learning	B-Metric
rate	E-Metric
,	O
embedding	O
dim	O
.	O

,	O
regularization	O
factor	O
,	O
negative	O
ratio	O
,	O
batch	O
size	O
,	O
max	O
iter	O
,	O
early	O
stopping	O
.	O

,	O
for	O
each	O
,	O
for	O
each	O
sample	O
Update	O
embeddings	O
w.r.t	O
.	O

:	O
Update	O
learning	B-Metric
rate	E-Metric
using	O
Adagrad	S-Method
break	O
if	O
filteredMRR	O
or	O
AP	O
on	O
decreased	O
appendix	O
:	O
WN18	B-Task
embeddings	I-Task
visualization	E-Task
We	O
used	O
principal	B-Method
component	I-Method
analysis	E-Method
(	O
PCA	S-Method
)	O
to	O
visualize	O
embeddings	O
of	O
the	O
relations	O
of	O
the	O
wordnet	B-Material
dataset	E-Material
(	O
WN18	S-Material
)	O
.	O

We	O
plotted	O
the	O
four	O
first	O
components	O
of	O
the	O
best	O
DistMult	S-Method
and	O
ComplEx	B-Method
model	E-Method
’s	O
embeddings	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

For	O
the	O
ComplEx	B-Method
model	E-Method
,	O
we	O
simply	O
concatenated	O
the	O
real	O
and	O
imaginary	O
parts	O
of	O
each	O
embedding	O
.	O

Most	O
of	O
WN18	S-Material
relations	O
describe	O
hierarchies	O
,	O
and	O
are	O
thus	O
antisymmetric	O
.	O

Each	O
of	O
these	O
hierarchic	O
relations	O
has	O
its	O
inverse	O
relation	O
in	O
the	O
dataset	O
.	O

For	O
example	O
:	O
hypernym	O
/	O
hyponym	O
,	O
part_of	O
/	O
has_part	O
,	O
synset_domain_topic_of	O
/	O
member_of_domain_topic	O
.	O

Since	O
DistMult	S-Method
is	O
unable	O
to	O
model	O
antisymmetry	O
,	O
it	O
will	O
correctly	O
represent	O
the	O
nature	O
of	O
each	O
pair	O
of	O
opposite	O
relations	O
,	O
but	O
not	O
the	O
direction	O
of	O
the	O
relations	O
.	O

Loosely	O
speaking	O
,	O
in	O
the	O
hypernym	O
/	O
hyponym	O
pair	O
the	O
nature	O
is	O
sharing	O
semantics	O
,	O
and	O
the	O
direction	O
is	O
that	O
one	O
entity	O
generalizes	O
the	O
semantics	O
of	O
the	O
other	O
.	O

This	O
makes	O
DistMult	S-Method
reprensenting	O
the	O
opposite	O
relations	O
with	O
very	O
close	O
embeddings	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
.	O

It	O
is	O
especially	O
striking	O
for	O
the	O
third	O
and	O
fourth	O
principal	O
component	O
(	O
bottom	O
-	O
left	O
)	O
.	O

Conversely	O
,	O
ComplEx	S-Method
manages	O
to	O
oppose	O
spatially	O
the	O
opposite	O
relations	O
.	O

DeepWalk	S-Method
:	O
Online	B-Task
Learning	I-Task
of	I-Task
Social	I-Task
Representations	E-Task
section	O
:	O
ABSTRACT	O
We	O
present	O
DeepWalk	S-Method
,	O
a	O
novel	O
approach	O
for	O
learning	B-Task
latent	I-Task
representations	I-Task
of	I-Task
vertices	I-Task
in	I-Task
a	I-Task
network	E-Task
.	O

These	O
latent	B-Method
representations	E-Method
encode	O
social	O
relations	O
in	O
a	O
continuous	O
vector	O
space	O
,	O
which	O
is	O
easily	O
exploited	O
by	O
statistical	B-Method
models	E-Method
.	O

DeepWalk	S-Method
generalizes	O
recent	O
advancements	O
in	O
language	B-Task
modeling	E-Task
and	O
unsupervised	B-Task
feature	I-Task
learning	E-Task
(	O
or	O
deep	B-Task
learning	E-Task
)	O
from	O
sequences	O
of	O
words	O
to	O
graphs	O
.	O

DeepWalk	S-Method
uses	O
local	O
information	O
obtained	O
from	O
truncated	O
random	O
walks	O
to	O
learn	O
latent	B-Method
representations	E-Method
by	O
treating	O
walks	O
as	O
the	O
equivalent	O
of	O
sentences	O
.	O

We	O
demonstrate	O
DeepWalk	S-Method
's	O
latent	O
representations	O
on	O
several	O
multi	B-Task
-	I-Task
label	I-Task
network	I-Task
classification	I-Task
tasks	E-Task
for	O
social	O
networks	O
such	O
as	O
BlogCatalog	S-Material
,	O
Flickr	S-Material
,	O
and	O
YouTube	S-Material
.	O

Our	O
results	O
show	O
that	O
DeepWalk	S-Method
outperforms	O
challenging	O
baselines	O
which	O
are	O
allowed	O
a	O
global	O
view	O
of	O
the	O
network	O
,	O
especially	O
in	O
the	O
presence	O
of	O
missing	O
information	O
.	O

DeepWalk	S-Method
's	O
representations	O
can	O
provide	O
F1	B-Metric
scores	E-Metric
up	O
to	O
10	O
%	O
higher	O
than	O
competing	O
methods	O
when	O
labeled	O
data	O
is	O
sparse	O
.	O

In	O
some	O
experiments	O
,	O
DeepWalk	S-Method
's	O
representations	O
are	O
able	O
to	O
outperform	O
all	O
baseline	O
methods	O
while	O
using	O
60	O
%	O
less	O
training	O
data	O
.	O

DeepWalk	S-Method
is	O
also	O
scalable	O
.	O

It	O
is	O
an	O
online	B-Method
learning	I-Method
algorithm	E-Method
which	O
builds	O
useful	O
incremental	O
results	O
,	O
and	O
is	O
trivially	O
parallelizable	O
.	O

These	O
qualities	O
make	O
it	O
suitable	O
for	O
a	O
broad	O
class	O
of	O
real	B-Task
world	I-Task
applications	E-Task
such	O
as	O
network	B-Task
classification	E-Task
,	O
and	O
anomaly	B-Task
detection	E-Task
.	O

section	O
:	O
INTRODUCTION	O
The	O
sparsity	O
of	O
a	O
network	B-Method
representation	E-Method
is	O
both	O
a	O
strength	O
and	O
a	O
weakness	O
.	O

Sparsity	S-Method
enables	O
the	O
design	O
of	O
efficient	O
discrete	B-Method
algorithms	E-Method
,	O
but	O
can	O
make	O
it	O
harder	O
to	O
generalize	O
in	O
statistical	B-Task
learning	E-Task
.	O

Machine	B-Task
learning	I-Task
applications	E-Task
in	O
networks	S-Task
(	O
such	O
as	O
network	B-Task
classification	E-Task
[	O
reference	O
][	O
reference	O
]	O
,	O
content	O
rec	O
-	O
The	O
learned	O
representation	O
encodes	O
community	O
structure	O
so	O
it	O
can	O
be	O
easily	O
exploited	O
by	O
standard	O
classification	S-Task
methods	O
.	O

Here	O
,	O
our	O
method	O
is	O
used	O
on	O
Zachary	B-Method
's	I-Method
Karate	I-Method
network	E-Method
[	O
reference	O
]	O
to	O
generate	O
a	O
latent	B-Method
representation	E-Method
in	O
R	O
2	O
.	O

Note	O
the	O
correspondence	O
between	O
community	O
structure	O
in	O
the	O
input	O
graph	O
and	O
the	O
embedding	O
.	O

Vertex	O
colors	O
represent	O
a	O
modularity	B-Method
-	I-Method
based	I-Method
clustering	E-Method
of	O
the	O
input	O
graph	O
.	O

ommendation	S-Method
[	O
reference	O
]	O
,	O
anomaly	B-Task
detection	E-Task
[	O
reference	O
]	O
,	O
and	O
missing	B-Task
link	I-Task
prediction	E-Task
[	O
reference	O
]	O
)	O
must	O
be	O
able	O
to	O
deal	O
with	O
this	O
sparsity	O
in	O
order	O
to	O
survive	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
deep	B-Method
learning	E-Method
(	O
unsupervised	B-Method
feature	I-Method
learning	E-Method
)	O
[	O
reference	O
]	O
techniques	O
,	O
which	O
have	O
proven	O
successful	O
in	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
into	O
network	B-Task
analysis	E-Task
for	O
the	O
first	O
time	O
.	O

We	O
develop	O
an	O
algorithm	O
(	O
DeepWalk	S-Method
)	O
that	O
learns	O
social	B-Method
representations	E-Method
of	O
a	O
graph	O
's	O
vertices	O
,	O
by	O
modeling	O
a	O
stream	B-Method
of	I-Method
short	I-Method
random	I-Method
walks	E-Method
.	O

Social	B-Method
representations	E-Method
are	O
latent	O
features	O
of	O
the	O
vertices	O
that	O
capture	O
neighborhood	O
similarity	O
and	O
community	O
membership	O
.	O

These	O
latent	B-Method
representations	E-Method
encode	O
social	O
relations	O
in	O
a	O
continuous	O
vector	O
space	O
with	O
a	O
relatively	O
small	O
number	O
of	O
dimensions	O
.	O

DeepWalk	S-Method
generalizes	O
neural	B-Method
language	I-Method
models	E-Method
to	O
process	O
a	O
special	O
language	O
composed	O
of	O
a	O
set	O
of	O
randomly	O
-	O
generated	O
walks	O
.	O

These	O
neural	B-Method
language	I-Method
models	E-Method
have	O
been	O
used	O
to	O
capture	O
the	O
semantic	O
and	O
syntactic	O
structure	O
of	O
human	O
language	O
[	O
reference	O
]	O
,	O
and	O
even	O
logical	O
analogies	O
[	O
reference	O
]	O
.	O

DeepWalk	S-Method
takes	O
a	O
graph	O
as	O
input	O
and	O
produces	O
a	O
latent	B-Method
representation	E-Method
as	O
an	O
output	O
.	O

The	O
result	O
of	O
applying	O
our	O
method	O
to	O
the	O
well	O
-	O
studied	O
Karate	B-Task
network	E-Task
is	O
shown	O
in	O
Figure	O
1	O
.	O

The	O
graph	O
,	O
as	O
typically	O
presented	O
by	O
force	B-Method
-	I-Method
directed	I-Method
layouts	E-Method
,	O
is	O
shown	O
in	O
Figure	O
1a	O
.	O

Figure	O
1b	O
shows	O
the	O
output	O
of	O
our	O
method	O
with	O
2	O
latent	O
dimensions	O
.	O

Beyond	O
the	O
striking	O
similarity	O
,	O
we	O
note	O
that	O
linearly	O
separable	O
portions	O
of	O
(	O
1b	O
)	O
correspond	O
to	O
clusters	O
found	O
through	O
modularity	B-Method
maximization	E-Method
in	O
the	O
input	O
graph	O
(	O
1a	O
)	O
(	O
shown	O
as	O
vertex	O
colors	O
)	O
.	O

To	O
demonstrate	O
DeepWalk	S-Method
's	O
potential	O
in	O
real	B-Task
world	I-Task
sce	I-Task
-	I-Task
narios	E-Task
,	O
we	O
evaluate	O
its	O
performance	O
on	O
challenging	O
multilabel	O
network	B-Task
classification	E-Task
problems	O
in	O
large	O
heterogeneous	O
graphs	O
.	O

In	O
the	O
relational	B-Task
classification	I-Task
problem	E-Task
,	O
the	O
links	O
between	O
feature	O
vectors	O
violate	O
the	O
traditional	O
i.i.d	O
.	O

assumption	O
.	O

Techniques	O
to	O
address	O
this	O
problem	O
typically	O
use	O
approximate	B-Method
inference	I-Method
techniques	E-Method
[	O
reference	O
][	O
reference	O
]	O
to	O
leverage	O
the	O
dependency	O
information	O
to	O
improve	O
classification	S-Task
results	O
.	O

We	O
distance	O
ourselves	O
from	O
these	O
approaches	O
by	O
learning	O
labelindependent	B-Method
representations	I-Method
of	I-Method
the	I-Method
graph	E-Method
.	O

Our	O
representation	B-Metric
quality	E-Metric
is	O
not	O
influenced	O
by	O
the	O
choice	O
of	O
labeled	O
vertices	O
,	O
so	O
they	O
can	O
be	O
shared	O
among	O
tasks	O
.	O

DeepWalk	S-Method
outperforms	O
other	O
latent	B-Method
representation	I-Method
methods	E-Method
for	O
creating	B-Task
social	I-Task
dimensions	E-Task
[	O
reference	O
][	O
reference	O
]	O
,	O
especially	O
when	O
labeled	O
nodes	O
are	O
scarce	O
.	O

Strong	O
performance	O
with	O
our	O
representations	O
is	O
possible	O
with	O
very	O
simple	O
linear	B-Method
classifiers	E-Method
(	O
e.g.	O
logistic	B-Method
regression	E-Method
)	O
.	O

Our	O
representations	O
are	O
general	O
,	O
and	O
can	O
be	O
combined	O
with	O
any	O
classification	S-Task
method	O
(	O
including	O
iterative	B-Method
inference	I-Method
methods	E-Method
)	O
.	O

DeepWalk	S-Method
achieves	O
all	O
of	O
that	O
while	O
being	O
an	O
online	B-Method
algorithm	E-Method
that	O
is	O
trivially	O
parallelizable	O
.	O

Our	O
contributions	O
are	O
as	O
follows	O
:	O
•	O
We	O
introduce	O
deep	B-Method
learning	E-Method
as	O
a	O
tool	O
to	O
analyze	O
graphs	S-Task
,	O
to	O
build	O
robust	B-Method
representations	E-Method
that	O
are	O
suitable	O
for	O
statistical	B-Task
modeling	E-Task
.	O

DeepWalk	S-Method
learns	O
structural	O
regularities	O
present	O
within	O
short	O
random	O
walks	O
.	O

•	O
We	O
extensively	O
evaluate	O
our	O
representations	O
on	O
multilabel	B-Task
classification	I-Task
tasks	E-Task
on	O
several	O
social	O
networks	O
.	O

We	O
show	O
significantly	O
increased	O
classification	S-Task
performance	O
in	O
the	O
presence	O
of	O
label	O
sparsity	O
,	O
getting	O
improvements	O
5%	O
-	O
10	O
%	O
of	O
Micro	B-Metric
F1	E-Metric
,	O
on	O
the	O
sparsest	B-Task
problems	E-Task
we	O
consider	O
.	O

In	O
some	O
cases	O
,	O
DeepWalk	S-Method
's	O
representations	O
can	O
outperform	O
its	O
competitors	O
even	O
when	O
given	O
60	O
%	O
less	O
training	O
data	O
.	O

•	O
We	O
demonstrate	O
the	O
scalability	O
of	O
our	O
algorithm	O
by	O
building	O
representations	B-Method
of	I-Method
web	I-Method
-	I-Method
scale	I-Method
graphs	E-Method
,	O
(	O
such	O
as	O
YouTube	S-Material
)	O
using	O
a	O
parallel	B-Method
implementation	E-Method
.	O

Moreover	O
,	O
we	O
describe	O
the	O
minimal	O
changes	O
necessary	O
to	O
build	O
a	O
streaming	B-Method
version	E-Method
of	O
our	O
approach	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
arranged	O
as	O
follows	O
.	O

In	O
Sections	O
2	O
and	O
3	O
,	O
we	O
discuss	O
the	O
problem	B-Task
formulation	E-Task
of	O
classification	S-Task
in	O
data	B-Task
networks	E-Task
,	O
and	O
how	O
it	O
relates	O
to	O
our	O
work	O
.	O

In	O
Section	O
4	O
we	O
present	O
DeepWalk	S-Method
,	O
our	O
approach	O
for	O
Social	B-Task
Representation	I-Task
Learning	E-Task
.	O

We	O
outline	O
ours	O
experiments	O
in	O
Section	O
5	O
,	O
and	O
present	O
their	O
results	O
in	O
Section	O
6	O
.	O

We	O
close	O
with	O
a	O
discussion	O
of	O
related	O
work	O
in	O
Section	O
7	O
,	O
and	O
our	O
conclusions	O
.	O

section	O
:	O
PROBLEM	O
DEFINITION	O
We	O
consider	O
the	O
problem	O
of	O
classifying	O
members	O
of	O
a	O
social	B-Task
network	E-Task
into	O
one	O
or	O
more	O
categories	O
.	O

More	O
formally	O
,	O
let	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
are	O
the	O
members	O
of	O
the	O
network	O
,	O
and	O
E	O
be	O
its	O
edges	O
,	O
E	O
⊆	O
(	O
V	O
×	O
V	O
)	O
.	O

Given	O
a	O
partially	O
labeled	O
social	O
network	O
GL	O
=	O
(	O
V	O
,	O
E	O
,	O
X	O
,	O
Y	O
)	O
,	O
with	O
attributes	O
X	O
∈	O
R	O
section	O
:	O
|V	O
|×S	O
where	O
S	O
is	O
the	O
size	O
of	O
the	O
feature	O
space	O
for	O
each	O
attribute	O
vector	O
,	O
and	O
Y	O
∈	O
R	O
|V	O
|×|Y|	O
,	O
Y	O
is	O
the	O
set	O
of	O
labels	O
.	O

In	O
a	O
traditional	O
machine	B-Task
learning	I-Task
classification	I-Task
setting	E-Task
,	O
we	O
aim	O
to	O
learn	O
a	O
hypothesis	O
H	O
that	O
maps	O
elements	O
of	O
X	O
to	O
the	O
labels	O
set	O
Y.	O
In	O
our	O
case	O
,	O
we	O
can	O
utilize	O
the	O
significant	O
information	O
about	O
the	O
dependence	O
of	O
the	O
examples	O
embedded	O
in	O
the	O
structure	O
of	O
G	O
to	O
achieve	O
superior	O
performance	O
.	O

In	O
the	O
literature	O
,	O
this	O
is	O
known	O
as	O
the	O
relational	B-Task
classification	E-Task
(	O
or	O
the	O
collective	B-Task
classification	I-Task
problem	E-Task
[	O
reference	O
]	O
)	O
.	O

Traditional	O
approaches	O
to	O
relational	B-Task
classification	E-Task
pose	O
the	O
problem	O
as	O
an	O
inference	S-Task
in	O
an	O
undirected	B-Method
Markov	I-Method
network	E-Method
,	O
and	O
then	O
use	O
iterative	B-Method
approximate	I-Method
inference	I-Method
algorithms	E-Method
(	O
such	O
as	O
the	O
iterative	O
classification	S-Task
algorithm	O
[	O
reference	O
]	O
,	O
Gibbs	B-Method
Sampling	E-Method
[	O
reference	O
]	O
,	O
or	O
label	B-Method
relaxation	E-Method
[	O
reference	O
]	O
)	O
to	O
compute	O
the	O
posterior	O
distribution	O
of	O
labels	O
given	O
the	O
network	O
structure	O
.	O

We	O
propose	O
a	O
different	O
approach	O
to	O
capture	O
the	O
network	O
topology	O
information	O
.	O

Instead	O
of	O
mixing	O
the	O
label	O
space	O
as	O
part	O
of	O
the	O
feature	O
space	O
,	O
we	O
propose	O
an	O
unsupervised	B-Method
method	E-Method
which	O
learns	O
features	O
that	O
capture	O
the	O
graph	O
structure	O
independent	O
of	O
the	O
labels	O
'	O
distribution	O
.	O

This	O
separation	O
between	O
the	O
structural	B-Method
representation	E-Method
and	O
the	O
labeling	B-Task
task	E-Task
avoids	O
cascading	O
errors	O
,	O
which	O
can	O
occur	O
in	O
iterative	B-Method
methods	E-Method
[	O
reference	O
]	O
.	O

Moreover	O
,	O
the	O
same	O
representation	O
can	O
be	O
used	O
for	O
multiple	O
classification	B-Task
problems	E-Task
concerning	O
that	O
network	O
.	O

Our	O
goal	O
is	O
to	O
learn	O
XE	O
∈	O
R	O
|V	O
|×d	O
,	O
where	O
d	O
is	O
small	O
number	O
of	O
latent	O
dimensions	O
.	O

These	O
low	B-Method
-	I-Method
dimensional	I-Method
representations	E-Method
are	O
distributed	O
;	O
meaning	O
each	O
social	O
phenomena	O
is	O
expressed	O
by	O
a	O
subset	O
of	O
the	O
dimensions	O
and	O
each	O
dimension	O
contributes	O
to	O
a	O
subset	O
of	O
the	O
social	O
concepts	O
expressed	O
by	O
the	O
space	O
.	O

Using	O
these	O
structural	O
features	O
,	O
we	O
will	O
augment	O
the	O
attributes	O
space	O
to	O
help	O
the	O
classification	B-Task
decision	E-Task
.	O

These	O
features	O
are	O
general	O
,	O
and	O
can	O
be	O
used	O
with	O
any	O
classification	S-Task
algorithm	O
(	O
including	O
iterative	B-Method
methods	E-Method
)	O
.	O

However	O
,	O
we	O
believe	O
that	O
the	O
greatest	O
utility	O
of	O
these	O
features	O
is	O
their	O
easy	O
integration	O
with	O
simple	O
machine	B-Method
learning	I-Method
algorithms	E-Method
.	O

They	O
scale	O
appropriately	O
in	O
real	B-Task
-	I-Task
world	I-Task
networks	E-Task
,	O
as	O
we	O
will	O
show	O
in	O
Section	O
6	O
.	O

section	O
:	O
LEARNING	B-Task
SOCIAL	I-Task
REPRESENTATIONS	E-Task
We	O
seek	O
learning	B-Task
social	I-Task
representations	E-Task
with	O
the	O
following	O
characteristics	O
:	O
•	O
Adaptability	B-Task
-	I-Task
Real	I-Task
social	I-Task
networks	E-Task
are	O
constantly	O
evolving	O
;	O
new	O
social	O
relations	O
should	O
not	O
require	O
repeating	O
the	O
learning	B-Method
process	E-Method
all	O
over	O
again	O
.	O

•	O
Community	O
aware	O
-	O
The	O
distance	O
between	O
latent	O
dimensions	O
should	O
represent	O
a	O
metric	O
for	O
evaluating	B-Task
social	I-Task
similarity	E-Task
between	O
the	O
corresponding	O
members	O
of	O
the	O
network	O
.	O

This	O
allows	O
generalization	S-Task
in	O
networks	S-Task
with	O
homophily	O
.	O

•	O
Low	B-Material
dimensional	I-Material
-	I-Material
When	I-Material
labeled	I-Material
data	E-Material
is	O
scarce	O
,	O
lowdimensional	B-Method
models	E-Method
generalize	O
better	O
,	O
and	O
speed	O
up	O
convergence	S-Task
and	O
inference	S-Task
.	O

•	O
Continuous	O
-	O
We	O
require	O
latent	B-Method
representations	E-Method
to	O
model	O
partial	O
community	O
membership	O
in	O
continuous	O
space	O
.	O

In	O
addition	O
to	O
providing	O
a	O
nuanced	O
view	O
of	O
community	O
membership	O
,	O
a	O
continuous	B-Method
representation	E-Method
has	O
smooth	O
decision	O
boundaries	O
between	O
communities	O
which	O
allows	O
more	O
robust	O
classification	S-Task
.	O

Our	O
method	O
for	O
satisfying	O
these	O
requirements	O
learns	O
representation	O
for	O
vertices	O
from	O
a	O
stream	O
of	O
short	B-Method
random	I-Method
walks	E-Method
,	O
using	O
optimization	B-Method
techniques	E-Method
originally	O
designed	O
for	O
language	B-Task
modeling	E-Task
.	O

Here	O
,	O
we	O
review	O
the	O
basics	O
of	O
both	O
random	B-Method
walks	E-Method
and	O
language	B-Method
modeling	E-Method
,	O
and	O
describe	O
how	O
their	O
combination	O
satisfies	O
our	O
requirements	O
.	O

section	O
:	O
Random	O
Walks	O
We	O
denote	O
a	O
random	O
walk	O
rooted	O
at	O
vertex	O
vi	O
as	O
Wv	O
i	O
.	O

It	O
is	O
a	O
stochastic	B-Method
process	E-Method
with	O
random	O
variables	O
W	O
is	O
a	O
vertex	O
chosen	O
at	O
random	O
from	O
the	O
neighbors	O
of	O
vertex	O
v	O
k	O
.	O

Random	B-Method
walks	E-Method
have	O
been	O
used	O
as	O
a	O
similarity	B-Metric
measure	E-Metric
for	O
a	O
variety	O
of	O
problems	O
in	O
content	B-Task
recommendation	E-Task
[	O
reference	O
]	O
and	O
community	B-Task
detection	E-Task
[	O
reference	O
]	O
.	O

They	O
are	O
also	O
the	O
foundation	O
of	O
a	O
class	O
of	O
output	B-Method
sensitive	I-Method
algorithms	E-Method
which	O
use	O
them	O
to	O
compute	O
local	O
community	O
structure	O
information	O
in	O
time	O
sublinear	O
to	O
the	O
size	O
of	O
the	O
input	O
graph	O
[	O
reference	O
]	O
.	O

It	O
is	O
this	O
connection	O
to	O
local	O
structure	O
that	O
motivates	O
us	O
to	O
use	O
a	O
stream	B-Method
of	I-Method
short	I-Method
random	I-Method
walks	E-Method
as	O
our	O
basic	O
tool	O
for	O
extracting	B-Task
information	E-Task
from	O
a	O
network	S-Task
.	O

In	O
addition	O
to	O
capturing	O
community	O
information	O
,	O
using	O
random	O
walks	O
as	O
the	O
basis	O
for	O
our	O
algorithm	O
gives	O
us	O
two	O
other	O
desirable	O
properties	O
.	O

First	O
,	O
local	B-Task
exploration	E-Task
is	O
easy	O
to	O
parallelize	O
.	O

Several	O
random	B-Method
walkers	E-Method
(	O
in	O
different	O
threads	O
,	O
processes	O
,	O
or	O
machines	O
)	O
can	O
simultaneously	O
explore	O
different	O
parts	O
of	O
the	O
same	O
graph	O
.	O

Secondly	O
,	O
relying	O
on	O
information	O
obtained	O
from	O
short	O
random	O
walks	O
make	O
it	O
possible	O
to	O
accommodate	O
small	O
changes	O
in	O
the	O
graph	O
structure	O
without	O
the	O
need	O
for	O
global	O
recomputation	O
.	O

We	O
can	O
iteratively	O
update	O
the	O
learned	O
model	O
with	O
new	O
random	O
walks	O
from	O
the	O
changed	O
region	O
in	O
time	O
sub	O
-	O
linear	O
to	O
the	O
entire	O
graph	O
.	O

section	O
:	O
Connection	O
:	O
Power	O
laws	O
Having	O
chosen	O
online	O
random	O
walks	O
as	O
our	O
primitive	O
for	O
capturing	B-Task
graph	I-Task
structure	E-Task
,	O
we	O
now	O
need	O
a	O
suitable	O
method	O
to	O
capture	O
this	O
information	O
.	O

If	O
the	O
degree	O
distribution	O
of	O
a	O
connected	O
graph	O
follows	O
a	O
power	O
law	O
(	O
is	O
scale	O
-	O
free	O
)	O
,	O
we	O
observe	O
that	O
the	O
frequency	O
which	O
vertices	O
appear	O
in	O
the	O
short	O
random	O
walks	O
will	O
also	O
follow	O
a	O
power	B-Method
-	I-Method
law	I-Method
distribution	E-Method
.	O

Word	O
frequency	O
in	O
natural	O
language	O
follows	O
a	O
similar	O
distribution	O
,	O
and	O
techniques	O
from	O
language	B-Method
modeling	E-Method
account	O
for	O
this	O
distributional	O
behavior	O
.	O

To	O
emphasize	O
this	O
similarity	O
we	O
show	O
two	O
different	O
power	O
-	O
law	O
distributions	O
in	O
Figure	O
2	O
.	O

The	O
first	O
comes	O
from	O
a	O
series	O
of	O
short	B-Method
random	I-Method
walks	E-Method
on	O
a	O
scale	B-Method
-	I-Method
free	I-Method
graph	E-Method
,	O
and	O
the	O
second	O
comes	O
from	O
the	O
text	O
of	O
100	O
,	O
000	O
articles	O
from	O
the	O
English	B-Material
Wikipedia	E-Material
.	O

A	O
core	O
contribution	O
of	O
our	O
work	O
is	O
the	O
idea	O
that	O
techniques	O
which	O
have	O
been	O
used	O
to	O
model	O
natural	O
language	O
(	O
where	O
the	O
symbol	O
frequency	O
follows	O
a	O
power	O
law	O
distribution	O
(	O
or	O
Zipf	B-Method
's	I-Method
law	E-Method
)	O
)	O
can	O
be	O
re	O
-	O
purposed	O
to	O
model	O
community	B-Task
structure	I-Task
in	I-Task
networks	E-Task
.	O

We	O
spend	O
the	O
rest	O
of	O
this	O
section	O
reviewing	O
the	O
growing	O
work	O
in	O
language	B-Task
modeling	E-Task
,	O
and	O
transforming	O
it	O
to	O
learn	O
representations	O
of	O
vertices	O
which	O
satisfy	O
our	O
criteria	O
.	O

section	O
:	O
Language	B-Method
Modeling	E-Method
The	O
goal	O
of	O
language	B-Task
modeling	E-Task
is	O
estimate	O
the	O
likelihood	O
of	O
a	O
specific	O
sequence	O
of	O
words	O
appearing	O
in	O
a	O
corpus	O
.	O

More	O
formally	O
,	O
given	O
a	O
sequence	O
of	O
words	O
where	O
wi	O
∈	O
V	O
(	O
V	O
is	O
the	O
vocabulary	O
)	O
,	O
we	O
would	O
like	O
to	O
maximize	O
the	O
Pr	O
(	O
wn|w0	O
,	O
w1	O
,	O
·	O
·	O
·	O
,	O
wn−1	O
)	O
over	O
all	O
the	O
training	O
corpus	O
.	O

Recent	O
work	O
in	O
representation	B-Task
learning	E-Task
has	O
focused	O
on	O
using	O
probabilistic	B-Method
neural	I-Method
networks	E-Method
to	O
build	O
general	B-Method
representations	E-Method
of	O
words	O
which	O
extend	O
the	O
scope	O
of	O
language	B-Method
modeling	E-Method
beyond	O
its	O
original	O
goals	O
.	O

In	O
this	O
work	O
,	O
we	O
present	O
a	O
generalization	B-Method
of	I-Method
language	I-Method
modeling	E-Method
to	O
explore	O
the	O
graph	O
through	O
a	O
stream	B-Method
of	I-Method
short	I-Method
random	I-Method
walks	E-Method
.	O

These	O
walks	O
can	O
be	O
thought	O
of	O
short	O
sentences	O
and	O
phrases	O
in	O
a	O
special	O
language	O
.	O

The	O
direct	O
analog	O
is	O
to	O
estimate	O
the	O
likelihood	O
of	O
observing	O
vertex	O
vi	O
given	O
all	O
the	O
previous	O
vertices	O
visited	O
so	O
far	O
in	O
the	O
random	O
walk	O
.	O

Our	O
goal	O
is	O
to	O
learn	O
a	O
latent	B-Method
representation	E-Method
,	O
not	O
only	O
a	O
probability	O
distribution	O
of	O
node	O
co	O
-	O
occurrences	O
,	O
and	O
so	O
we	O
introduce	O
a	O
mapping	O
function	O
Φ	O
:	O
v	O
∈	O
V	O
→	O
R	O
|V	O
|×d	O
.	O

This	O
mapping	B-Method
Φ	E-Method
represents	O
the	O
latent	B-Method
social	I-Method
representation	E-Method
associated	O
with	O
each	O
vertex	O
v	O
in	O
the	O
graph	O
.	O

(	O
In	O
practice	O
,	O
we	O
represent	O
Φ	O
by	O
a	O
|V	O
|	O
×	O
d	O
matrix	O
of	O
free	O
parameters	O
,	O
which	O
will	O
serve	O
later	O
on	O
as	O
our	O
XE	O
.	O

)	O
The	O
problem	O
then	O
,	O
is	O
to	O
estimate	O
the	O
likelihood	O
:	O
However	O
as	O
the	O
walk	O
length	O
grows	O
,	O
computing	O
this	O
objective	O
function	O
becomes	O
unfeasible	O
.	O

A	O
recent	O
relaxation	O
in	O
language	B-Task
modeling	E-Task
[	O
reference	O
][	O
reference	O
]	O
turns	O
the	O
prediction	B-Task
problem	E-Task
on	O
its	O
head	O
.	O

First	O
,	O
instead	O
of	O
using	O
the	O
context	O
to	O
predict	O
a	O
missing	O
word	O
,	O
it	O
uses	O
one	O
word	O
to	O
predict	O
the	O
context	O
.	O

Secondly	O
,	O
the	O
context	O
is	O
composed	O
of	O
the	O
words	O
appearing	O
to	O
right	O
side	O
of	O
the	O
given	O
word	O
as	O
well	O
as	O
the	O
left	O
side	O
.	O

Finally	O
,	O
it	O
removes	O
the	O
ordering	O
constraint	O
on	O
the	O
problem	O
.	O

Instead	O
,	O
the	O
model	O
is	O
required	O
to	O
maximize	O
the	O
probability	O
of	O
any	O
word	O
appearing	O
in	O
the	O
context	O
without	O
the	O
knowledge	O
of	O
its	O
offset	O
from	O
the	O
given	O
word	O
.	O

In	O
terms	O
of	O
vertex	B-Method
representation	I-Method
modeling	E-Method
,	O
this	O
yields	O
the	O
optimization	B-Task
problem	E-Task
:	O
We	O
find	O
these	O
relaxations	O
are	O
particularly	O
desirable	O
for	O
social	B-Task
representation	I-Task
learning	E-Task
.	O

First	O
,	O
the	O
order	O
independence	O
assumption	O
better	O
captures	O
a	O
sense	O
of	O
'	O
nearness	O
'	O
that	O
is	O
provided	O
by	O
random	O
walks	O
.	O

Moreover	O
,	O
this	O
relaxation	O
is	O
quite	O
useful	O
for	O
speeding	O
up	O
the	O
training	B-Metric
time	E-Metric
by	O
building	O
small	B-Method
models	E-Method
as	O
one	O
vertex	O
is	O
given	O
at	O
a	O
time	O
.	O

Solving	O
the	O
optimization	B-Task
problem	E-Task
from	O
Eq	O
.	O

2	O
builds	O
representations	O
that	O
capture	O
the	O
shared	O
similarities	O
in	O
local	O
graph	O
structure	O
between	O
vertices	O
.	O

Vertices	O
which	O
have	O
similar	O
neighborhoods	O
will	O
acquire	O
similar	O
representations	O
(	O
encoding	O
cocitation	O
similarity	O
)	O
,	O
and	O
allowing	O
generalization	S-Task
on	O
machine	B-Task
learning	I-Task
tasks	E-Task
.	O

By	O
combining	O
both	O
truncated	B-Method
random	I-Method
walks	E-Method
and	O
neural	B-Method
language	I-Method
models	E-Method
we	O
formulate	O
a	O
method	O
which	O
satisfies	O
all	O
window	O
size	O
w	O
embedding	O
size	O
d	O
walks	O
per	O
vertex	O
γ	O
walk	O
length	O
t	O
Output	O
:	O
matrix	B-Method
of	I-Method
vertex	I-Method
representations	E-Method
Φ	O
∈	O
R	O
|V	O
|×d	O
1	O
:	O
Initialization	O
:	O
Sample	O
Φ	O
from	O
U	O
|V	O
|×d	O
2	O
:	O
Build	O
a	O
binary	O
Tree	O
T	O
from	O
V	O
3	O
:	O
for	O
i	O
=	O
0	O
to	O
γ	O
do	O
4	O
:	O
for	O
each	O
vi	O
∈	O
O	O
do	O
6	O
:	O
end	O
for	O
9	O
:	O
end	O
for	O
of	O
our	O
desired	O
properties	O
.	O

This	O
method	O
generates	O
representations	B-Task
of	I-Task
social	I-Task
networks	E-Task
that	O
are	O
low	O
-	O
dimensional	O
,	O
and	O
exist	O
in	O
a	O
continuous	O
vector	O
space	O
.	O

Its	O
representations	O
encode	O
latent	O
forms	O
of	O
community	O
membership	O
,	O
and	O
because	O
the	O
method	O
outputs	O
useful	O
intermediate	B-Method
representations	E-Method
,	O
it	O
can	O
adapt	O
to	O
changing	O
network	O
topology	O
.	O

section	O
:	O
METHOD	O
In	O
this	O
section	O
we	O
discuss	O
the	O
main	O
components	O
of	O
our	O
algorithm	O
.	O

We	O
also	O
present	O
several	O
variants	O
of	O
our	O
approach	O
and	O
discuss	O
their	O
merits	O
.	O

section	O
:	O
Overview	O
As	O
in	O
any	O
language	B-Method
modeling	I-Method
algorithm	E-Method
,	O
the	O
only	O
required	O
input	O
is	O
a	O
corpus	O
and	O
a	O
vocabulary	O
V.	O
DeepWalk	S-Method
considers	O
a	O
set	O
of	O
short	O
truncated	O
random	O
walks	O
its	O
own	O
corpus	O
,	O
and	O
the	O
graph	O
vertices	O
as	O
its	O
own	O
vocabulary	O
(	O
V	O
=	O
V	O
)	O
.	O

While	O
it	O
is	O
beneficial	O
to	O
know	O
the	O
V	O
and	O
the	O
frequency	O
distribution	O
of	O
vertices	O
in	O
the	O
random	O
walks	O
ahead	O
of	O
the	O
training	O
,	O
it	O
is	O
not	O
necessary	O
for	O
the	O
algorithm	O
to	O
work	O
as	O
we	O
will	O
show	O
in	O
4.2.2	O
.	O

section	O
:	O
Algorithm	O
:	O
DeepWalk	S-Method
The	O
algorithm	O
consists	O
of	O
two	O
main	O
components	O
;	O
first	O
a	O
random	B-Method
walk	I-Method
generator	E-Method
and	O
second	O
an	O
update	B-Method
procedure	E-Method
.	O

The	O
random	B-Method
walk	I-Method
generator	E-Method
takes	O
a	O
graph	O
G	O
and	O
samples	O
uniformly	O
a	O
random	O
vertex	O
vi	O
as	O
the	O
root	O
of	O
the	O
random	B-Method
walk	E-Method
Wv	O
i	O
.	O

A	O
walk	O
samples	O
uniformly	O
from	O
the	O
neighbors	O
of	O
the	O
last	O
vertex	O
visited	O
until	O
the	O
maximum	O
length	O
(	O
t	O
)	O
is	O
reached	O
.	O

While	O
we	O
set	O
the	O
length	O
of	O
our	O
random	O
walks	O
in	O
the	O
experiments	O
to	O
be	O
fixed	O
,	O
there	O
is	O
no	O
restriction	O
for	O
the	O
random	O
walks	O
to	O
be	O
of	O
the	O
same	O
length	O
.	O

These	O
walks	O
could	O
have	O
restarts	O
(	O
i.e.	O
a	O
teleport	O
probability	O
of	O
returning	O
back	O
to	O
their	O
root	O
)	O
,	O
but	O
our	O
preliminary	O
results	O
did	O
not	O
show	O
any	O
advantage	O
of	O
using	O
restarts	S-Method
.	O

In	O
practice	O
,	O
our	O
implementation	O
specifies	O
a	O
number	O
of	O
random	O
walks	O
γ	O
of	O
length	O
t	O
to	O
start	O
at	O
each	O
vertex	O
.	O

Lines	O
3	O
-	O
9	O
in	O
Algorithm	O
1	O
shows	O
the	O
core	O
of	O
our	O
approach	O
.	O

The	O
outer	O
loop	O
specifies	O
the	O
number	O
of	O
times	O
,	O
γ	O
,	O
which	O
we	O
should	O
start	O
random	O
walks	O
at	O
each	O
vertex	O
.	O

We	O
think	O
of	O
each	O
iteration	O
as	O
making	O
a	O
'	O
pass	O
'	O
over	O
the	O
data	O
and	O
sample	O
one	O
walk	O
per	O
node	O
during	O
this	O
pass	O
.	O

At	O
the	O
start	O
of	O
each	O
pass	O
we	O
generate	O
a	O
random	O
ordering	O
to	O
traverse	O
the	O
vertices	O
.	O

This	O
is	O
not	O
strictly	O
required	O
,	O
but	O
is	O
well	O
-	O
known	O
to	O
speed	O
up	O
the	O
convergence	O
of	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
.	O

section	O
:	O
Algorithm	O
2	O
5	O
:	O
end	O
for	O
6	O
:	O
end	O
for	O
In	O
the	O
inner	O
loop	O
,	O
we	O
iterate	O
over	O
all	O
the	O
vertices	O
of	O
the	O
graph	O
.	O

For	O
each	O
vertex	O
vi	O
we	O
generate	O
a	O
random	B-Method
walk	E-Method
|Wv	O
i	O
|	O
=	O
t	O
,	O
and	O
then	O
use	O
it	O
to	O
update	O
our	O
representations	O
(	O
Line	O
7	O
)	O
.	O

We	O
use	O
the	O
SkipGram	B-Method
algorithm	E-Method
[	O
reference	O
]	O
to	O
update	O
these	O
representations	O
in	O
accordance	O
with	O
our	O
objective	B-Metric
function	E-Metric
in	O
Eq	O
.	O

2	O
.	O

section	O
:	O
SkipGram	B-Method
SkipGram	E-Method
is	O
a	O
language	B-Method
model	E-Method
that	O
maximizes	O
the	O
cooccurrence	O
probability	O
among	O
the	O
words	O
that	O
appear	O
within	O
a	O
window	O
,	O
w	O
,	O
in	O
a	O
sentence	O
[	O
reference	O
]	O
.	O

Algorithm	O
2	O
iterates	O
over	O
all	O
possible	O
collocations	O
in	O
random	O
walk	O
that	O
appear	O
within	O
the	O
window	O
w	O
(	O
lines	O
1	O
-	O
2	O
)	O
.	O

For	O
each	O
,	O
we	O
map	O
each	O
vertex	O
vj	O
to	O
its	O
current	O
representation	O
vector	O
Φ	O
(	O
vj	O
)	O
∈	O
R	O
d	O
(	O
See	O
Figure	O
3b	O
)	O
.	O

Given	O
the	O
representation	O
of	O
vj	O
,	O
we	O
would	O
like	O
to	O
maximize	O
the	O
probability	O
of	O
its	O
neighbors	O
in	O
the	O
walk	O
(	O
line	O
3	O
)	O
.	O

We	O
can	O
learn	O
such	O
posterior	O
distribution	O
using	O
several	O
choices	O
of	O
classifiers	S-Method
.	O

For	O
example	O
,	O
modeling	O
the	O
previous	B-Task
problem	E-Task
using	O
logistic	B-Method
regression	E-Method
would	O
result	O
in	O
a	O
huge	O
number	O
of	O
labels	O
that	O
is	O
equal	O
to	O
|V	O
|	O
which	O
could	O
be	O
in	O
millions	O
or	O
billions	O
.	O

Such	O
models	O
require	O
large	O
amount	O
of	O
computational	O
resources	O
that	O
could	O
span	O
a	O
whole	O
cluster	O
of	O
computers	O
[	O
reference	O
]	O
.	O

To	O
speed	O
the	O
training	B-Metric
time	E-Metric
,	O
Hierarchical	O
Softmax	O
[	O
reference	O
][	O
reference	O
]	O
can	O
be	O
used	O
to	O
approximate	O
the	O
probability	O
distribution	O
.	O

section	O
:	O
Hierarchical	B-Method
Softmax	E-Method
Given	O
that	O
u	O
k	O
∈	O
V	O
,	O
calculating	O
Pr	O
(	O
u	O
k	O
|	O
Φ	O
(	O
vj	O
)	O
)	O
in	O
line	O
3	O
is	O
not	O
feasible	O
.	O

Computing	O
the	O
partition	O
function	O
(	O
normalization	O
factor	O
)	O
is	O
expensive	O
.	O

If	O
we	O
assign	O
the	O
vertices	O
to	O
the	O
leaves	O
of	O
a	O
binary	O
tree	O
,	O
the	O
prediction	B-Task
problem	E-Task


turns	O
into	O
maximizing	O
the	O
probability	O
of	O
a	O
specific	O
path	O
in	O
the	O
tree	O
(	O
See	O
Figure	O
3c	O
)	O
.	O

If	O
the	O
path	O
to	O
vertex	O
u	O
k	O
is	O
identified	O
by	O
a	O
sequence	O
of	O
tree	O
nodes	O
(	O
b0	O
,	O
b1	O
,	O
.	O

.	O


.	O


,	O
b	O
log	O
|V	O
|	O
)	O
,	O
(	O
b0	O
=	O
root	O
,	O
Now	O
,	O
Pr	O
(	O
b	O
l	O
|	O
Φ	O
(	O
vj	O
)	O
)	O
could	O
be	O
modeled	O
by	O
a	O
binary	B-Method
classifier	E-Method
that	O
is	O
assigned	O
to	O
the	O
parent	O
of	O
the	O
node	O
b	O
l	O
.	O

This	O
reduces	O
the	O
computational	B-Metric
complexity	E-Metric
of	O
calculating	O
Pr	S-Metric
(	O
We	O
can	O
speed	O
up	O
the	O
training	O
process	O
further	O
,	O
by	O
assigning	O
shorter	O
paths	O
to	O
the	O
frequent	O
vertices	O
in	O
the	O
random	O
walks	O
.	O

Huffman	B-Method
coding	E-Method
is	O
used	O
to	O
reduce	O
the	O
access	O
time	O
of	O
frequent	O
elements	O
in	O
the	O
tree	O
.	O

section	O
:	O
Optimization	S-Task
The	O
model	O
parameter	O
set	O
is	O
{	O
Φ	O
,	O
T	O
}	O
where	O
the	O
size	O
of	O
each	O
is	O
O	O
(	O
d|V	O
|	O
)	O
.	O

Stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
[	O
reference	O
]	O
is	O
used	O
to	O
optimize	O
these	O
parameters	O
(	O
Line	O
4	O
,	O
Algorithm	O
2	O
)	O
.	O

The	O
derivatives	O
are	O
estimated	O
using	O
the	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
.	O

The	O
learning	B-Metric
rate	I-Metric
α	E-Metric
for	O
SGD	S-Method
is	O
initially	O
set	O
to	O
2.5	O
%	O
at	O
the	O
beginning	O
of	O
the	O
training	O
and	O
then	O
decreased	O
linearly	O
(	O
v1	O
)	O
)	O
and	O
Pr	O
(	O
v5	O
|	O
Φ	O
(	O
v1	O
)	O
)	O
over	O
sequences	O
of	O
probability	O
distributions	O
corresponding	O
to	O
the	O
paths	O
starting	O
at	O
the	O
root	O
and	O
ending	O
at	O
v3	O
and	O
v5	O
.	O

The	O
representation	B-Method
Φ	E-Method
is	O
updated	O
to	O
maximize	O
the	O
probability	O
of	O
v1	O
co	O
-	O
occurring	O
with	O
its	O
context	O
{	O
v3	O
,	O
v5}.	O
with	O
the	O
number	O
of	O
vertices	O
that	O
are	O
seen	O
so	O
far	O
.	O

section	O
:	O
Parallelizability	O
As	O
shown	O
in	O
Figure	O
2	O
the	O
frequency	O
distribution	O
of	O
vertices	O
in	O
random	O
walks	O
of	O
social	O
network	O
and	O
words	O
in	O
a	O
language	O
both	O
follow	O
a	O
power	B-Method
law	E-Method
.	O

This	O
results	O
in	O
a	O
long	O
tail	O
of	O
infrequent	O
vertices	O
,	O
therefore	O
,	O
the	O
updates	O
that	O
affect	O
Φ	O
will	O
be	O
sparse	O
in	O
nature	O
.	O

This	O
allows	O
us	O
to	O
use	O
asynchronous	B-Method
version	I-Method
of	I-Method
stochastic	I-Method
gradient	I-Method
descent	E-Method
(	O
ASGD	B-Method
)	E-Method
,	O
in	O
the	O
multi	B-Task
-	I-Task
worker	I-Task
case	E-Task
.	O

Given	O
that	O
our	O
updates	O
are	O
sparse	O
and	O
we	O
do	O
not	O
acquire	O
a	O
lock	O
to	O
access	O
the	O
model	O
shared	O
parameters	O
,	O
ASGD	O
will	O
achieve	O
an	O
optimal	O
rate	O
of	O
convergence	S-Metric
[	O
reference	O
]	O
.	O

While	O
we	O
run	O
experiments	O
on	O
one	O
machine	O
using	O
multiple	O
threads	O
,	O
it	O
has	O
been	O
demonstrated	O
that	O
this	O
technique	O
is	O
highly	O
scalable	O
,	O
and	O
can	O
be	O
used	O
in	O
very	O
large	B-Task
scale	I-Task
machine	I-Task
learning	E-Task
[	O
reference	O
]	O
.	O

Figure	O
4	O
presents	O
the	O
effects	O
of	O
parallelizing	O
DeepWalk	S-Method
.	O

It	O
shows	O
the	O
speed	O
up	O
in	O
processing	O
BlogCatalog	S-Material
and	O
Flickr	O
networks	O
is	O
consistent	O
as	O
we	O
increase	O
the	O
number	O
of	O
workers	O
to	O
8	O
(	O
Figure	O
4a	O
)	O
.	O

It	O
also	O
shows	O
that	O
there	O
is	O
no	O
loss	O
of	O
predictive	B-Metric
performance	E-Metric
relative	O
to	O
the	O
running	O
DeepWalk	S-Method
serially	O
(	O
Figure	O
4b	O
)	O
.	O

section	O
:	O
Algorithm	O
Variants	O
Here	O
we	O
discuss	O
some	O
variants	O
of	O
our	O
proposed	O
method	O
,	O
which	O
we	O
believe	O
may	O
be	O
of	O
interest	O
.	O

section	O
:	O
Streaming	S-Task
One	O
interesting	O
variant	O
of	O
this	O
method	O
is	O
a	O
streaming	B-Method
approach	E-Method
,	O
which	O
could	O
be	O
implemented	O
without	O
knowledge	O
of	O
the	O
entire	O
graph	O
.	O

In	O
this	O
variant	O
small	O
walks	O
from	O
the	O
graph	O
are	O
passed	O
directly	O
to	O
the	O
representation	B-Method
learning	I-Method
code	E-Method
,	O
and	O
the	O
model	O
is	O
updated	O
directly	O
.	O

Some	O
modifications	O
to	O
the	O
learning	B-Method
process	E-Method
will	O
also	O
be	O
necessary	O
.	O

First	O
,	O
using	O
a	O
decaying	O
learning	O
rate	O
will	O
no	O
longer	O
be	O
possible	O
.	O

Instead	O
,	O
we	O
can	O
initialize	O
the	O
learning	B-Metric
rate	I-Metric
α	E-Metric
to	O
a	O
small	O
constant	O
value	O
.	O

This	O
will	O
take	O
longer	O
to	O
learn	O
,	O
but	O
may	O
be	O
worth	O
it	O
in	O
some	O
applications	O
.	O

Second	O
,	O
we	O
can	O
not	O
necessarily	O
build	O
a	O
tree	O
of	O
parameters	O
any	O
more	O
.	O

If	O
the	O
cardinality	O
of	O
V	O
is	O
known	O
(	O
or	O
can	O
be	O
bounded	O
)	O
,	O
we	O
can	O
build	O
the	O
Hierarchical	O
Softmax	O
tree	O
for	O
that	O
maximum	O
value	O
.	O

Vertices	O
can	O
be	O
assigned	O
to	O
one	O
of	O
the	O
remaining	O
leaves	O
when	O
they	O
are	O
first	O
seen	O
.	O

If	O
we	O
have	O
the	O
ability	O
to	O
estimate	O
the	O
vertex	O
frequency	O
a	O
priori	O
,	O
we	O
can	O
section	O
:	O
Non	B-Task
-	I-Task
random	I-Task
walks	E-Task
Some	O
graphs	O
are	O
created	O
as	O
a	O
by	O
-	O
product	O
of	O
agents	O
interacting	O
with	O
a	O
sequence	O
of	O
elements	O
(	O
e.g.	O
users	O
'	O
navigation	O
of	O
pages	O
on	O
a	O
website	O
)	O
.	O

When	O
a	O
graph	O
is	O
created	O
by	O
such	O
a	O
stream	O
of	O
non	B-Method
-	I-Method
random	I-Method
walks	E-Method
,	O
we	O
can	O
use	O
this	O
process	O
to	O
feed	O
the	O
modeling	B-Task
phase	E-Task
directly	O
.	O

Graphs	O
sampled	O
in	O
this	O
way	O
will	O
not	O
only	O
capture	O
information	O
related	O
to	O
network	O
structure	O
,	O
but	O
also	O
to	O
the	O
frequency	O
at	O
which	O
paths	O
are	O
traversed	O
.	O

In	O
our	O
view	O
,	O
this	O
variant	O
also	O
encompasses	O
language	B-Method
modeling	E-Method
.	O

Sentences	O
can	O
be	O
viewed	O
as	O
purposed	O
walks	O
through	O
an	O
appropriately	O
designed	O
language	B-Method
network	E-Method
,	O
and	O
language	B-Method
models	E-Method
like	O
SkipGram	S-Method
are	O
designed	O
to	O
capture	O
this	O
behavior	O
.	O

This	O
approach	O
can	O
be	O
combined	O
with	O
the	O
streaming	B-Method
variant	E-Method
(	O
Section	O
4.4.1	O
)	O
to	O
train	O
features	O
on	O
a	O
continually	B-Method
evolving	I-Method
network	E-Method
without	O
ever	O
explicitly	O
constructing	O
the	O
entire	O
graph	O
.	O

Maintaining	B-Method
representations	E-Method
with	O
this	O
technique	O
could	O
enable	O
web	B-Task
-	I-Task
scale	I-Task
classification	E-Task
without	O
the	O
hassles	O
of	O
dealing	O
with	O
a	O
web	O
-	O
scale	O
graph	O
.	O

section	O
:	O
EXPERIMENTAL	O
DESIGN	O
In	O
this	O
section	O
we	O
provide	O
an	O
overview	O
of	O
the	O
datasets	O
and	O
methods	O
which	O
we	O
will	O
use	O
in	O
our	O
experiments	O
.	O

Code	O
and	O
data	O
to	O
reproduce	O
our	O
results	O
will	O
be	O
available	O
at	O
the	O
first	O
author	O
's	O
website	O
.	O

An	O
overview	O
of	O
the	O
graphs	O
we	O
consider	O
in	O
our	O
experiments	O
is	O
given	O
in	O
Figure	O
1	O
.	O

section	O
:	O
Datasets	O
•	O
BlogCatalog	S-Material
[	O
reference	O
]	O
is	O
a	O
network	O
of	O
social	O
relationships	O
provided	O
by	O
blogger	O
authors	O
.	O

The	O
labels	O
represent	O
the	O
topic	O
categories	O
provided	O
by	O
the	O
authors	O
.	O

•	O
Flickr	S-Material
[	O
reference	O
]	O
is	O
a	O
network	O
of	O
the	O
contacts	O
between	O
users	O
of	O
the	O
photo	O
sharing	O
website	O
.	O

The	O
labels	O
represent	O
the	O
interest	O
groups	O
of	O
the	O
users	O
such	O
as	O
'	O
black	O
and	O
white	O
photos	O
'	O
.	O

•	O
YouTube	S-Material
[	O
reference	O
]	O
is	O
a	O
social	O
network	O
between	O
users	O
of	O
the	O
popular	O
video	O
sharing	O
website	O
.	O

The	O
labels	O
here	O
represent	O
groups	O
of	O
viewers	O
that	O
enjoy	O
common	O
video	O
genres	O
(	O
e.g.	O
anime	O
and	O
wrestling	O
)	O
.	O

section	O
:	O
Baseline	O
Methods	O
To	O
validate	O
the	O
performance	O
of	O
our	O
approach	O
we	O
compare	O
it	O
against	O
a	O
number	O
of	O
baselines	O
:	O
•	O
SpectralClustering	S-Method
[	O
reference	O
]	O
:	O
This	O
method	O
generates	O
a	O
representation	O
in	O
R	O
d	O
from	O
the	O
d	O
-	O
smallest	O
eigenvectors	O
of	O
L	O
,	O
the	O
normalized	O
graph	O
Laplacian	O
of	O
G.	O
Utilizing	O
the	O
eigenvectors	O
of	O
L	O
implicitly	O
assumes	O
that	O
graph	O
cuts	O
will	O
be	O
useful	O
for	O
classification	S-Task
.	O

•	O
Modularity	O
[	O
reference	O
]	O
:	O
This	O
method	O
generates	O
a	O
representation	O
in	O
R	O
d	O
from	O
the	O
top	O
-	O
d	O
eigenvectors	O
of	O
B	O
,	O
the	O
Modularity	O
matrix	O
of	O
G.	O
The	O
eigenvectors	O
of	O
B	O
encode	O
information	O
about	O
modular	O
graph	O
partitions	O
of	O
G	O
[	O
reference	O
]	O
.	O

Using	O
them	O
as	O
features	O
assumes	O
that	O
modular	O
graph	O
partitions	O
will	O
be	O
useful	O
for	O
classification	S-Task
.	O

•	O
EdgeCluster	S-Method
[	O
reference	O
]	O
:	O
This	O
method	O
uses	O
k	B-Method
-	I-Method
means	I-Method
clustering	E-Method
to	O
cluster	O
the	O
adjacency	O
matrix	O
of	O
G.	O
Its	O
has	O
been	O
shown	O
to	O
perform	O
comparably	O
to	O
the	O
Modularity	B-Method
method	E-Method
,	O
with	O
the	O
added	O
advantage	O
of	O
scaling	O
to	O
graphs	O
which	O
are	O
too	O
large	O
for	O
spectral	B-Method
decomposition	E-Method
.	O

•	O
wvRN	S-Method
[	O
reference	O
]	O
:	O
The	O
weighted	B-Method
-	I-Method
vote	I-Method
Relational	I-Method
Neighbor	E-Method
is	O
a	O
relational	B-Method
classifier	E-Method
.	O

Given	O
the	O
neighborhood	O
Ni	O
of	O
vertex	O
vi	O
,	O
wvRN	O
estimates	O
Pr	O
(	O
yi|Ni	O
)	O
with	O
the	O
(	O
appropriately	O
normalized	O
)	O
weighted	O
mean	O
of	O
its	O
neighbors	O
(	O
i.e	O
Pr	O
(	O
yi|Ni	O
)	O
=	O
wij	O
Pr	O
(	O
yj	O
|	O
Nj	O
)	O
)	O
.	O

It	O
has	O
shown	O
surprisingly	O
good	O
performance	O
in	O
real	B-Task
networks	E-Task
,	O
and	O
has	O
been	O
advocated	O
as	O
a	O
sensible	O
relational	O
classification	S-Task
baseline	O
[	O
reference	O
]	O
.	O

•	O
Majority	O
:	O
This	O
naïve	O
method	O
simply	O
chooses	O
the	O
most	O
frequent	O
labels	O
in	O
the	O
training	O
set	O
.	O

section	O
:	O
EXPERIMENTS	O
In	O
this	O
section	O
we	O
present	O
an	O
experimental	O
analysis	O
of	O
our	O
method	O
.	O

We	O
thoroughly	O
evaluate	O
it	O
on	O
a	O
number	O
of	O
multilabel	B-Task
classification	I-Task
tasks	E-Task
,	O
and	O
analyze	O
its	O
sensitivity	O
across	O
several	O
parameters	O
.	O

section	O
:	O
Multi	B-Task
-	I-Task
Label	I-Task
Classification	E-Task
To	O
facilitate	O
the	O
comparison	O
between	O
our	O
method	O
and	O
the	O
relevant	O
baselines	O
,	O
we	O
use	O
the	O
exact	O
same	O
datasets	O
and	O
experimental	O
procedure	O
as	O
in	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Specifically	O
,	O
we	O
randomly	O
sample	O
a	O
portion	O
(	O
TR	O
)	O
of	O
the	O
labeled	O
nodes	O
,	O
and	O
use	O
them	O
as	O
training	O
data	O
.	O

The	O
rest	O
of	O
the	O
nodes	O
are	O
used	O
as	O
test	O
.	O

We	O
repeat	O
this	O
process	O
10	O
times	O
,	O
and	O
report	O
the	O
average	O
performance	O
in	O
terms	O
of	O
both	O
Macro	B-Metric
-	I-Metric
F1	E-Metric
and	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
.	O

When	O
possible	O
we	O
report	O
the	O
original	O
results	O
[	O
reference	O
][	O
reference	O
]	O
here	O
directly	O
.	O

For	O
all	O
models	O
we	O
use	O
a	O
one	B-Method
-	I-Method
vs	I-Method
-	I-Method
rest	I-Method
logistic	I-Method
regression	E-Method
implemented	O
by	O
LibLinear	S-Method
[	O
reference	O
]	O
section	O
:	O
BlogCatalog	S-Material
In	O
this	O
experiment	O
we	O
increase	O
the	O
training	B-Metric
ratio	E-Metric
(	O
TR	S-Metric
)	O
on	O
the	O
BlogCatalog	S-Material
network	O
from	O
10	O
%	O
to	O
90	O
%	O
.	O

Our	O
results	O
are	O
presented	O
in	O
Table	O
2	O
.	O

Numbers	O
in	O
bold	O
represent	O
the	O
highest	O
performance	O
in	O
each	O
column	O
.	O

DeepWalk	S-Method
performs	O
consistently	O
better	O
than	O
EdgeCluster	S-Method
,	O
Modularity	S-Method
,	O
and	O
wvRN	S-Method
.	O

In	O
fact	O
,	O
when	O
trained	O
with	O
only	O
20	O
%	O
of	O
the	O
nodes	O
labeled	O
,	O
DeepWalk	S-Method
performs	O
better	O
than	O
these	O
approaches	O
when	O
they	O
are	O
given	O
90	O
%	O
of	O
the	O
data	O
.	O

The	O
performance	O
of	O
SpectralClustering	S-Method
proves	O
much	O
more	O
competitive	O
,	O
but	O
DeepWalk	S-Method
still	O
outperforms	O
when	O
labeled	O
data	O
is	O
sparse	O
on	O
both	O
Macro	B-Metric
-	I-Metric
F1	E-Metric
(	O
TR	S-Metric
≤	O
20	O
%	O
)	O
and	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
(	O
TR	S-Metric
≤	O
60	O
%	O
)	O
.	O

This	O
strong	O
performance	O
when	O
only	O
small	O
fractions	O
of	O
the	O
graph	O
are	O
labeled	O
is	O
a	O
core	O
strength	O
of	O
our	O
approach	O
.	O

In	O
the	O
following	O
experiments	O
,	O
we	O
investigate	O
the	O
performance	O
of	O
our	O
representations	O
on	O
even	O
more	O
sparsely	O
labeled	O
graphs	O
.	O

section	O
:	O
Flickr	S-Material
In	O
this	O
experiment	O
we	O
vary	O
the	O
training	B-Metric
ratio	E-Metric
(	O
TR	S-Metric
)	O
on	O
the	O
Flickr	B-Method
network	E-Method
from	O
1	O
%	O
to	O
10	O
%	O
.	O

This	O
corresponds	O
to	O
having	O
approximately	O
800	O
to	O
8	O
,	O
000	O
nodes	O
labeled	O
for	O
classification	S-Task
in	O
the	O
entire	O
network	O
.	O

Table	O
3	O
presents	O
our	O
results	O
,	O
which	O
are	O
consistent	O
with	O
the	O
previous	O
experiment	O
.	O

DeepWalk	S-Method
outperforms	O
all	O
baselines	O
by	O
at	O
least	O
3	O
%	O
with	O
respect	O
to	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
.	O

Additionally	O
,	O
its	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
performance	O
when	O
only	O
3	O
%	O
of	O
the	O
graph	O
is	O
labeled	O
beats	O
all	O
other	O
methods	O
even	O
when	O
they	O
have	O
been	O
given	O
10	O
%	O
of	O
the	O
data	O
.	O

In	O
other	O
words	O
,	O
DeepWalk	S-Method
can	O
outperform	O
the	O
baselines	O
with	O
60	O
%	O
less	O
training	O
data	O
.	O

It	O
also	O
performs	O
quite	O
well	O
in	O
Macro	B-Metric
-	I-Metric
F1	E-Metric
,	O
initially	O
performing	O
close	O
to	O
SpectralClustering	S-Method
,	O
but	O
distancing	O
itself	O
to	O
a	O
1	O
%	O
improvement	O
.	O

section	O
:	O
YouTube	S-Material
The	O
YouTube	B-Material
network	E-Material
is	O
considerably	O
larger	O
than	O
the	O
previous	O
ones	O
we	O
have	O
experimented	O
on	O
,	O
and	O
its	O
size	O
prevents	O
two	O
of	O
our	O
baseline	O
methods	O
(	O
SpectralClustering	S-Method
and	O
Modularity	S-Method
)	O
from	O
running	O
on	O
it	O
.	O

It	O
is	O
much	O
closer	O
to	O
a	O
real	O
world	O
graph	O
than	O
those	O
we	O
have	O
previously	O
considered	O
.	O

The	O
results	O
of	O
varying	O
the	O
training	B-Metric
ratio	E-Metric
(	O
TR	S-Metric
)	O
from	O
1	O
%	O
to	O
10	O
%	O
are	O
presented	O
in	O
Table	O
4	O
.	O

They	O
show	O
that	O
DeepWalk	S-Method
significantly	O
outperforms	O
the	O
scalable	O
baseline	O
for	O
creating	B-Task
graph	I-Task
representations	E-Task
,	O
EdgeCluster	O
.	O

When	O
1	O
%	O
of	O
the	O
labeled	O
nodes	O
are	O
used	O
for	O
test	O
,	O
the	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
improves	O
by	O
14	O
%	O
.	O

The	O
Macro	B-Metric
-	I-Metric
F1	E-Metric
shows	O
a	O
corresponding	O
10	O
%	O
increase	O
.	O

This	O
lead	O
narrows	O
as	O
the	O
training	O
data	O
increases	O
,	O
but	O
DeepWalk	S-Method
ends	O
with	O
a	O
3	O
%	O
lead	O
in	O
Micro	B-Metric
-	I-Metric
F1	E-Metric
,	O
and	O
an	O
impressive	O
5	O
%	O
improvement	O
in	O
Macro	B-Metric
-	I-Metric
F1	E-Metric
.	O

This	O
experiment	O
showcases	O
the	O
performance	O
benefits	O
that	O
can	O
occur	O
from	O
using	O
social	B-Method
representation	I-Method
learning	E-Method
for	O
multilabel	B-Task
classification	E-Task
.	O

DeepWalk	S-Method
,	O
can	O
scale	O
to	O
large	O
graphs	O
,	O
and	O
performs	O
exceedingly	O
well	O
in	O
such	O
a	O
sparsely	O
labeled	O
environment	O
.	O

section	O
:	O
Parameter	B-Metric
Sensitivity	E-Metric
In	O
order	O
to	O
evaluate	O
how	O
changes	O
to	O
the	O
parameterization	O
of	O
DeepWalk	S-Method
effect	O
its	O
performance	O
on	O
classification	B-Task
tasks	E-Task
,	O
we	O
conducted	O
experiments	O
on	O
two	O
multi	B-Task
-	I-Task
label	I-Task
classifications	I-Task
tasks	E-Task
(	O
Flickr	S-Material
,	O
and	O
BlogCatalog	S-Material
)	O
.	O

For	O
this	O
test	O
,	O
we	O
have	O
fixed	O
the	O
window	O
size	O
and	O
the	O
walk	O
length	O
to	O
sensible	O
values	O
(	O
w	O
=	O
10	O
,	O
t	O
=	O
40	O
)	O
which	O
should	O
emphasize	O
local	O
structure	O
.	O

We	O
then	O
vary	O
the	O
number	O
of	O
latent	O
dimensions	O
(	O
d	O
)	O
,	O
the	O
number	O
of	O
walks	O
started	O
per	O
vertex	O
(	O
γ	O
)	O
,	O
and	O
the	O
amount	O
of	O
training	O
data	O
available	O
(	O
TR	O
)	O
to	O
determine	O
their	O
impact	O
on	O
the	O
network	B-Task
classification	E-Task
performance	O
.	O

Figure	O
5a	O
shows	O
the	O
effects	O
of	O
increasing	O
the	O
number	O
of	O
latent	O
dimensions	O
available	O
to	O
our	O
model	O
.	O

section	O
:	O
Effect	O
of	O
Dimensionality	S-Metric
Figures	O
5a1	O
and	O
5a3	O
examine	O
the	O
effects	O
of	O
varying	O
the	O
dimensionality	S-Metric
and	O
training	B-Metric
rate	E-Metric
.	O

The	O
performance	O
is	O
quite	O
consistent	O
between	O
both	O
Flickr	S-Material
and	O
BlogCatalog	S-Material
and	O
show	O
that	O
the	O
optimal	O
dimensionality	S-Metric
for	O
a	O
model	O
is	O
dependent	O
on	O
the	O
number	O
of	O
training	O
examples	O
.	O

(	O
Note	O
that	O
1	O
%	O
of	O
Flickr	S-Material
has	O
approximately	O
as	O
many	O
labeled	O
examples	O
as	O
10	O
%	O
of	O
BlogCatalog	S-Material
)	O
.	O

Figures	O
5a2	O
and	O
5a3	O
examine	O
the	O
effects	O
of	O
varying	O
the	O
dimensionality	O
and	O
number	O
of	O
walks	O
per	O
vertex	O
.	O

The	O
relative	O
performance	O
between	O
dimensions	O
is	O
relatively	O
stable	O
across	O
different	O
values	O
of	O
γ	O
.	O

These	O
charts	O
have	O
two	O
interesting	O
observations	O
.	O

The	O
first	O
is	O
that	O
there	O
is	O
most	O
of	O
the	O
benefit	O
is	O
accomplished	O
by	O
starting	O
γ	O
=	O
30	O
walks	O
per	O
node	O
in	O
both	O
graphs	O
.	O

The	O
second	O
is	O
that	O
the	O
relative	O
difference	O
between	O
different	O
values	O
of	O
γ	O
is	O
quite	O
consistent	O
between	O
the	O
two	O
graphs	O
.	O

Flickr	S-Method
has	O
an	O
order	O
of	O
magnitude	O
more	O
edges	O
than	O
BlogCatalog	S-Material
,	O
and	O
we	O
find	O
this	O
behavior	O
interesting	O
.	O

These	O
experiments	O
show	O
that	O
our	O
method	O
can	O
make	O
useful	O
models	O
of	O
various	O
sizes	O
.	O

They	O
also	O
show	O
that	O
the	O
performance	O
of	O
the	O
model	O
depends	O
on	O
the	O
number	O
of	O
random	O
walks	O
it	O
has	O
seen	O
,	O
and	O
the	O
appropriate	O
dimensionality	O
of	O
the	O
model	O
depends	O
on	O
the	O
training	O
examples	O
available	O
.	O

Figure	O
5a	O
shows	O
the	O
effects	O
of	O
increasing	O
γ	O
,	O
the	O
number	O
of	O
random	O
walks	O
that	O
we	O
start	O
from	O
each	O
vertex	O
.	O

section	O
:	O
Effect	O
of	O
sampling	O
frequency	O
The	O
results	O
are	O
very	O
consistent	O
for	O
different	O
dimensions	O
(	O
Fig	O
.	O

5b1	O
,	O
Fig	O
.	O

5b3	O
)	O
and	O
the	O
amount	O
of	O
training	O
data	O
(	O
Fig	O
.	O

5b2	O
,	O
Fig	O
.	O

5b4	O
)	O
.	O

Initially	O
,	O
increasing	O
γ	O
has	O
a	O
big	O
effect	O
in	O
the	O
results	O
,	O
but	O
this	O
effect	O
quickly	O
slows	O
(	O
γ	O
>	O
10	O
)	O
.	O

These	O
results	O
demonstrate	O
that	O
we	O
are	O
able	O
to	O
learn	O
meaningful	O
latent	O
representations	O
for	O
vertices	O
after	O
only	O
a	O
small	O
number	O
of	O
random	O
walks	O
.	O

section	O
:	O
RELATED	O
WORK	O
The	O
main	O
differences	O
between	O
our	O
proposed	O
method	O
and	O
previous	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
1	O
.	O

We	O
learn	O
our	O
latent	B-Method
social	I-Method
representations	E-Method
,	O
instead	O
of	O
computing	O
statistics	O
related	O
to	O
centrality	O
[	O
reference	O
]	O
or	O
partitioning	O
[	O
reference	O
]	O
.	O

2	O
.	O

We	O
do	O
not	O
attempt	O
to	O
extend	O
the	O
classification	S-Task
procedure	O
itself	O
(	O
through	O
collective	B-Method
inference	E-Method
[	O
reference	O
]	O
or	O
graph	B-Method
kernels	E-Method
[	O
reference	O
]	O
)	O
.	O

3	O
.	O

We	O
propose	O
a	O
scalable	B-Method
online	I-Method
method	E-Method
which	O
uses	O
only	O
local	O
information	O
.	O

Most	O
methods	O
require	O
global	O
information	O
and	O
are	O
offline	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

4	O
.	O

We	O
apply	O
unsupervised	B-Method
representation	I-Method
learning	E-Method
to	O
graphs	O
.	O

In	O
this	O
section	O
we	O
discuss	O
related	O
work	O
in	O
network	B-Task
classification	E-Task
and	O
unsupervised	B-Task
feature	I-Task
learning	E-Task
.	O

section	O
:	O
Relational	B-Task
Learning	E-Task
Relational	O
classification	S-Task
(	O
or	O
collective	O
classification	S-Task
)	O
methods	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
use	O
links	O
between	O
data	O
items	O
as	O
part	O
of	O
the	O
classification	B-Task
process	E-Task
.	O

Exact	B-Task
inference	E-Task
in	O
the	O
collective	B-Task
classification	I-Task
problem	E-Task
is	O
NP	B-Task
-	I-Task
hard	E-Task
,	O
and	O
solutions	O
have	O
focused	O
on	O
the	O
use	O
of	O
approximate	B-Method
inference	I-Method
algorithm	E-Method
which	O
may	O
not	O
be	O
guaranteed	O
to	O
converge	O
[	O
reference	O
]	O
.	O

The	O
most	O
relevant	O
relational	O
classification	S-Task
algorithms	O
to	O
our	O
work	O
incorporate	O
community	O
information	O
by	O
learning	O
clusters	O
[	O
reference	O
]	O
,	O
by	O
adding	O
edges	O
between	O
nearby	O
nodes	O
[	O
reference	O
]	O
,	O
by	O
using	O
PageRank	S-Method
[	O
reference	O
]	O
,	O
or	O
by	O
extending	O
relational	B-Task
classification	E-Task
to	O
take	O
additional	O
features	O
into	O
account	O
[	O
reference	O
]	O
.	O

Our	O
work	O
takes	O
a	O
substantially	O
different	O
approach	O
.	O

Instead	O
of	O
a	O
new	O
approximation	B-Method
inference	I-Method
algorithm	E-Method
,	O
we	O
propose	O
a	O
procedure	O
which	O
learns	O
representations	B-Method
of	I-Method
network	I-Method
structure	E-Method
which	O
can	O
then	O
be	O
used	O
by	O
existing	O
inference	B-Method
procedure	E-Method
(	O
including	O
iterative	B-Method
ones	E-Method
)	O
.	O

A	O
number	O
of	O
techniques	O
for	O
generating	B-Task
features	E-Task
from	O
graphs	O
have	O
also	O
been	O
proposed	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

In	O
contrast	O
to	O
these	O
methods	O
,	O
we	O
frame	O
the	O
feature	B-Task
creation	I-Task
procedure	E-Task
as	O
a	O
representation	B-Task
learning	I-Task
problem	E-Task
.	O

Graph	B-Method
Kernels	E-Method
[	O
reference	O
]	O
have	O
been	O
proposed	O
as	O
a	O
way	O
to	O
use	O
relational	O
data	O
as	O
part	O
of	O
the	O
classification	B-Task
process	E-Task
,	O
but	O
are	O
quite	O
slow	O
unless	O
approximated	O
[	O
reference	O
]	O
.	O

Our	O
approach	O
is	O
complementary	O
;	O
instead	O
of	O
encoding	O
the	O
structure	O
as	O
part	O
of	O
a	O
kernel	B-Method
function	E-Method
,	O
we	O
learn	O
a	O
representation	O
which	O
allows	O
them	O
to	O
be	O
used	O
directly	O
as	O
features	O
for	O
any	O
classification	S-Task
method	O
.	O

section	O
:	O
Unsupervised	B-Task
Feature	I-Task
Learning	E-Task
Distributed	B-Method
representations	E-Method
have	O
been	O
proposed	O
to	O
model	O
structural	O
relationship	O
between	O
concepts	O
[	O
reference	O
]	O
.	O

These	O
representations	O
are	O
trained	O
by	O
the	O
back	B-Method
-	I-Method
propagation	E-Method
and	O
gradient	B-Method
descent	E-Method
.	O

Computational	B-Metric
costs	E-Metric
and	O
numerical	B-Metric
instability	E-Metric
led	O
to	O
these	O
techniques	O
to	O
be	O
abandoned	O
for	O
almost	O
a	O
decade	O
.	O

Recently	O
,	O
distributed	B-Task
computing	E-Task
allowed	O
for	O
larger	O
models	O
to	O
be	O
trained	O
[	O
reference	O
]	O
,	O
and	O
the	O
growth	O
of	O
data	O
for	O
unsupervised	B-Method
learning	I-Method
algorithms	E-Method
to	O
emerge	O
[	O
reference	O
]	O
.	O

Distributed	B-Method
representations	E-Method
usually	O
are	O
trained	O
through	O
neural	B-Method
networks	E-Method
,	O
these	O
networks	O
have	O
made	O
advancements	O
in	O
diverse	O
fields	O
such	O
as	O
computer	B-Task
vision	E-Task
[	O
reference	O
]	O
,	O
speech	B-Task
recognition	E-Task
[	O
reference	O
]	O
,	O
and	O
natural	B-Task
language	I-Task
processing	E-Task
[	O
reference	O
]	O
.	O

section	O
:	O
CONCLUSIONS	O
We	O
propose	O
DeepWalk	S-Method
,	O
a	O
novel	O
approach	O
for	O
learning	O
latent	B-Task
social	I-Task
representations	I-Task
of	I-Task
vertices	E-Task
.	O

Using	O
local	O
information	O
from	O
truncated	O
random	O
walks	O
as	O
input	O
,	O
our	O
method	O
learns	O
a	O
representation	O
which	O
encodes	O
structural	O
regularities	O
.	O

Experiments	O
on	O
a	O
variety	O
of	O
different	O
graphs	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
on	O
challenging	O
multi	B-Task
-	I-Task
label	I-Task
classification	I-Task
tasks	E-Task
.	O

As	O
an	O
online	B-Method
algorithm	E-Method
,	O
DeepWalk	S-Method
is	O
also	O
scalable	O
.	O

Our	O
results	O
show	O
that	O
we	O
can	O
create	O
meaningful	B-Method
representations	E-Method
for	O
graphs	S-Task
too	O
large	O
to	O
run	O
spectral	B-Method
methods	E-Method
on	O
.	O

On	O
such	O
large	O
graphs	O
,	O
our	O
method	O
significantly	O
outperforms	O
other	O
methods	O
designed	O
to	O
operate	O
for	O
sparsity	S-Task
.	O

We	O
also	O
show	O
that	O
our	O
approach	O
is	O
parallelizable	O
,	O
allowing	O
workers	O
to	O
update	O
different	O
parts	O
of	O
the	O
model	O
concurrently	O
.	O

In	O
addition	O
to	O
being	O
effective	O
and	O
scalable	O
,	O
our	O
approach	O
is	O
also	O
an	O
appealing	O
generalization	B-Method
of	I-Method
language	I-Method
modeling	E-Method
.	O

This	O
connection	O
is	O
mutually	O
beneficial	O
.	O

Advances	O
in	O
language	B-Method
modeling	E-Method
may	O
continue	O
to	O
generate	O
improved	O
latent	B-Method
representations	E-Method
for	O
networks	S-Task
.	O

In	O
our	O
view	O
,	O
language	B-Task
modeling	E-Task
is	O
actually	O
sampling	O
from	O
an	O
unobservable	O
language	O
graph	O
.	O

We	O
believe	O
that	O
insights	O
obtained	O
from	O
modeling	O
observable	O
graphs	O
may	O
in	O
turn	O
yield	O
improvements	O
to	O
modeling	O
unobservable	O
ones	O
.	O

Our	O
future	O
work	O
in	O
the	O
area	O
will	O
focus	O
on	O
investigating	O
this	O
duality	O
further	O
,	O
using	O
our	O
results	O
to	O
improve	O
language	B-Task
modeling	E-Task
,	O
and	O
strengthening	O
the	O
theoretical	O
justifications	O
of	O
the	O
method	O
.	O

section	O
:	O
Feature	B-Method
Pyramid	I-Method
Networks	E-Method
for	O
Object	B-Task
Detection	E-Task
section	O
:	O
Abstract	O
Feature	B-Method
pyramids	E-Method
are	O
a	O
basic	O
component	O
in	O
recognition	B-Method
systems	E-Method
for	O
detecting	B-Task
objects	E-Task
at	O
different	O
scales	O
.	O

But	O
recent	O
deep	B-Method
learning	I-Method
object	I-Method
detectors	E-Method
have	O
avoided	O
pyramid	B-Method
representations	E-Method
,	O
in	O
part	O
because	O
they	O
are	O
compute	O
and	O
memory	O
intensive	O
.	O

In	O
this	O
paper	O
,	O
we	O
exploit	O
the	O
inherent	O
multi	O
-	O
scale	O
,	O
pyramidal	O
hierarchy	O
of	O
deep	B-Method
convolutional	I-Method
networks	E-Method
to	O
construct	O
feature	O
pyramids	O
with	O
marginal	O
extra	O
cost	O
.	O

A	O
topdown	B-Method
architecture	E-Method
with	O
lateral	O
connections	O
is	O
developed	O
for	O
building	O
high	B-Task
-	I-Task
level	I-Task
semantic	I-Task
feature	I-Task
maps	E-Task
at	O
all	O
scales	O
.	O

This	O
architecture	O
,	O
called	O
a	O
Feature	B-Method
Pyramid	I-Method
Network	E-Method
(	O
FPN	S-Method
)	O
,	O
shows	O
significant	O
improvement	O
as	O
a	O
generic	O
feature	B-Method
extractor	E-Method
in	O
several	O
applications	O
.	O

Using	O
FPN	S-Method
in	O
a	O
basic	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	I-Method
system	E-Method
,	O
our	O
method	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
singlemodel	O
results	O
on	O
the	O
COCO	B-Material
detection	I-Material
benchmark	E-Material
without	O
bells	O
and	O
whistles	O
,	O
surpassing	O
all	O
existing	O
single	O
-	O
model	O
entries	O
including	O
those	O
from	O
the	O
COCO	S-Material
2016	O
challenge	O
winners	O
.	O

In	O
addition	O
,	O
our	O
method	O
can	O
run	O
at	O
6	O
FPS	O
on	O
a	O
GPU	O
and	O
thus	O
is	O
a	O
practical	O
and	O
accurate	O
solution	O
to	O
multi	B-Task
-	I-Task
scale	I-Task
object	I-Task
detection	E-Task
.	O

Code	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Introduction	O
Recognizing	B-Task
objects	E-Task
at	O
vastly	O
different	O
scales	O
is	O
a	O
fundamental	O
challenge	O
in	O
computer	B-Task
vision	E-Task
.	O

Feature	B-Method
pyramids	E-Method
built	O
upon	O
image	B-Method
pyramids	E-Method
(	O
for	O
short	O
we	O
call	O
these	O
featurized	B-Method
image	I-Method
pyramids	E-Method
)	O
form	O
the	O
basis	O
of	O
a	O
standard	O
solution	O
[	O
reference	O
]	O
(	O
Fig	O
.	O

1	O
(	O
a	O
)	O
)	O
.	O

These	O
pyramids	O
are	O
scale	O
-	O
invariant	O
in	O
the	O
sense	O
that	O
an	O
object	O
's	O
scale	O
change	O
is	O
offset	O
by	O
shifting	O
its	O
level	O
in	O
the	O
pyramid	O
.	O

Intuitively	O
,	O
this	O
property	O
enables	O
a	O
model	O
to	O
detect	O
objects	O
across	O
a	O
large	O
range	O
of	O
scales	O
by	O
scanning	O
the	O
model	O
over	O
both	O
positions	O
and	O
pyramid	O
levels	O
.	O

Featurized	B-Method
image	I-Method
pyramids	E-Method
were	O
heavily	O
used	O
in	O
the	O
era	O
of	O
hand	O
-	O
engineered	O
features	O
[	O
reference	O
][	O
reference	O
]	O
.	O

They	O
were	O
so	O
critical	O
that	O
object	B-Method
detectors	E-Method
like	O
DPM	S-Method
[	O
reference	O
]	O
required	O
dense	B-Method
scale	I-Method
sampling	E-Method
to	O
achieve	O
good	O
results	O
(	O
e.g.	O
,	O
10	O
scales	O
per	O
octave	O
)	O
.	O

For	O
recognition	B-Task
tasks	E-Task
,	O
engineered	O
features	O
have	O
largely	O
been	O
replaced	O
with	O
features	O
computed	O
by	O
deep	B-Method
convolutional	I-Method
networks	E-Method
(	O
ConvNets	S-Method
)	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Aside	O
from	O
being	O
capable	O
of	O
representing	O
higher	O
-	O
level	O
semantics	O
,	O
ConvNets	S-Method
are	O
also	O
more	O
robust	O
to	O
variance	O
in	O
scale	O
and	O
thus	O
facilitate	O
recognition	S-Task
from	O
features	O
computed	O
on	O
a	O
single	O
input	O
scale	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
.	O

But	O
even	O
with	O
this	O
robustness	O
,	O
pyramids	O
are	O
still	O
needed	O
to	O
get	O
the	O
most	O
accurate	O
results	O
.	O

All	O
recent	O
top	O
entries	O
in	O
the	O
ImageNet	S-Material
[	O
reference	O
]	O
and	O
COCO	S-Material
[	O
reference	O
]	O
detection	B-Task
challenges	E-Task
use	O
multi	B-Method
-	I-Method
scale	I-Method
testing	E-Method
on	O
featurized	B-Method
image	I-Method
pyramids	E-Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

The	O
principle	O
advantage	O
of	O
featurizing	O
each	O
level	O
of	O
an	O
image	B-Method
pyramid	E-Method
is	O
that	O
it	O
produces	O
a	O
multi	B-Method
-	I-Method
scale	I-Method
feature	I-Method
representation	E-Method
in	O
which	O
all	O
levels	O
are	O
semantically	O
strong	O
,	O
including	O
the	O
high	O
-	O
resolution	O
levels	O
.	O

Nevertheless	O
,	O
featurizing	O
each	O
level	O
of	O
an	O
image	B-Method
pyramid	E-Method
has	O
obvious	O
limitations	O
.	O

Inference	B-Metric
time	E-Metric
increases	O
considerably	O
(	O
e.g.	O
,	O
by	O
four	O
times	O
[	O
reference	O
]	O
)	O
,	O
making	O
this	O
approach	O
impractical	O
for	O
real	O
applications	O
.	O

Moreover	O
,	O
training	O
deep	B-Method
networks	E-Method
end	O
-	O
to	O
-	O
end	O
on	O
an	O
image	O
pyramid	O
is	O
infeasible	O
in	O
terms	O
of	O
memory	O
,	O
and	O
so	O
,	O
if	O
exploited	O
,	O
image	O
pyramids	O
are	O
used	O
only	O
at	O
test	O
time	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
which	O
creates	O
an	O
inconsistency	O
between	O
train	O
/	O
test	O
-	O
time	B-Method
inference	E-Method
.	O

For	O
these	O
reasons	O
,	O
Fast	S-Method
and	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
][	O
reference	O
]	O
opt	O
to	O
not	O
use	O
featurized	O
image	O
pyramids	O
under	O
default	O
settings	O
.	O

However	O
,	O
image	B-Method
pyramids	E-Method
are	O
not	O
the	O
only	O
way	O
to	O
compute	O
a	O
multi	B-Method
-	I-Method
scale	I-Method
feature	I-Method
representation	E-Method
.	O

A	O
deep	B-Method
ConvNet	E-Method
computes	O
a	O
feature	B-Method
hierarchy	I-Method
layer	I-Method
by	I-Method
layer	E-Method
,	O
and	O
with	O
subsampling	B-Method
layers	E-Method
the	O
feature	O
hierarchy	O
has	O
an	O
inherent	O
multiscale	O
,	O
pyramidal	O
shape	O
.	O

This	O
in	O
-	O
network	O
feature	O
hierarchy	O
produces	O
feature	O
maps	O
of	O
different	O
spatial	O
resolutions	O
,	O
but	O
introduces	O
large	O
semantic	O
gaps	O
caused	O
by	O
different	O
depths	O
.	O

The	O
high	O
-	O
resolution	O
maps	O
have	O
low	O
-	O
level	O
features	O
that	O
harm	O
their	O
representational	B-Method
capacity	E-Method
for	O
object	B-Task
recognition	E-Task
.	O

The	O
Single	B-Method
Shot	I-Method
Detector	E-Method
(	O
SSD	S-Method
)	O
[	O
reference	O
]	O
is	O
one	O
of	O
the	O
first	O
attempts	O
at	O
using	O
a	O
ConvNet	O
's	O
pyramidal	O
feature	O
hierarchy	O
as	O
if	O
it	O
were	O
a	O
featurized	O
image	O
pyramid	O
(	O
Fig	O
.	O

1	O
(	O
c	O
)	O
)	O
.	O

Ideally	O
,	O
the	O
SSD	B-Method
-	I-Method
style	I-Method
pyramid	E-Method
would	O
reuse	O
the	O
multi	O
-	O
scale	O
feature	O
maps	O
from	O
different	O
layers	O
computed	O
in	O
the	O
forward	O
pass	O
and	O
thus	O
come	O
free	O
of	O
cost	O
.	O

But	O
to	O
avoid	O
using	O
low	O
-	O
level	O
features	O
SSD	O
foregoes	O
reusing	O
already	O
computed	O
layers	O
and	O
instead	O
builds	O
the	O
pyramid	O
starting	O
from	O
high	O
up	O
in	O
the	O
network	O
(	O
e.g.	O
,	O
conv4	O
3	O
of	O
VGG	B-Method
nets	E-Method
[	O
reference	O
]	O
)	O
and	O
then	O
by	O
adding	O
several	O
new	O
layers	O
.	O

Thus	O
it	O
misses	O
the	O
opportunity	O
to	O
reuse	O
the	O
higher	O
-	O
resolution	O
maps	O
of	O
the	O
feature	O
hierarchy	O
.	O

We	O
show	O
that	O
these	O
are	O
important	O
for	O
detecting	B-Task
small	I-Task
objects	E-Task
.	O

The	O
goal	O
of	O
this	O
paper	O
is	O
to	O
naturally	O
leverage	O
the	O
pyramidal	O
shape	O
of	O
a	O
ConvNet	O
's	O
feature	O
hierarchy	O
while	O
creating	O
a	O
feature	B-Method
pyramid	E-Method
that	O
has	O
strong	O
semantics	O
at	O
all	O
scales	O
.	O

To	O
achieve	O
this	O
goal	O
,	O
we	O
rely	O
on	O
an	O
architecture	O
that	O
combines	O
low	O
-	O
resolution	O
,	O
semantically	O
strong	O
features	O
with	O
high	O
-	O
resolution	O
,	O
semantically	O
weak	O
features	O
via	O
a	O
top	B-Method
-	I-Method
down	I-Method
pathway	E-Method
and	O
lateral	B-Method
connections	E-Method
(	O
Fig	O
.	O

1	O
(	O
d	O
)	O
)	O
.	O

The	O
result	O
is	O
a	O
feature	B-Method
pyramid	E-Method
that	O
has	O
rich	O
semantics	O
at	O
all	O
levels	O
and	O
is	O
built	O
quickly	O
from	O
a	O
single	O
input	O
image	O
scale	O
.	O

In	O
other	O
words	O
,	O
we	O
show	O
how	O
to	O
create	O
in	B-Method
-	I-Method
network	I-Method
feature	I-Method
pyramids	E-Method
that	O
can	O
be	O
used	O
to	O
replace	O
featurized	O
image	O
pyramids	O
without	O
sacrificing	O
representational	O
power	O
,	O
speed	O
,	O
or	O
memory	O
.	O

Similar	O
architectures	O
adopting	O
top	B-Method
-	I-Method
down	I-Method
and	I-Method
skip	I-Method
connections	E-Method
are	O
popular	O
in	O
recent	O
research	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Their	O
goals	O
are	O
to	O
produce	O
a	O
single	O
high	O
-	O
level	O
feature	O
map	O
of	O
a	O
fine	O
resolution	O
on	O
which	O
the	O
predictions	O
are	O
to	O
be	O
made	O
(	O
Fig	O
.	O

2	O
top	O
)	O
.	O

On	O
the	O
contrary	O
,	O
our	O
method	O
leverages	O
the	O
architecture	O
as	O
a	O
feature	O
pyramid	O
where	O
predictions	O
(	O
e.g.	O
,	O
object	O
detections	O
)	O
are	O
independently	O
made	O
on	O
each	O
level	O
(	O
Fig	O
.	O

2	O
bottom	O
)	O
.	O

Our	O
model	O
echoes	O
a	O
featurized	B-Method
image	I-Method
pyramid	E-Method
,	O
which	O
has	O
not	O
been	O
explored	O
in	O
these	O
works	O
.	O

We	O
evaluate	O
our	O
method	O
,	O
called	O
a	O
Feature	B-Method
Pyramid	I-Method
Network	E-Method
(	O
FPN	S-Method
)	O
,	O
in	O
various	O
systems	O
for	O
detection	B-Task
and	I-Task
segmentation	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Without	O
bells	O
and	O
whistles	O
,	O
we	O
report	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
-	O
model	O
result	O
on	O
the	O
challenging	O
COCO	B-Material
detection	I-Material
benchmark	E-Material
[	O
reference	O
]	O
simply	O
based	O
on	O
FPN	S-Method
and	O
predict	O
predict	O
predict	O
predict	O
Figure	O
2	O
.	O

Top	O
:	O
a	O
top	B-Method
-	I-Method
down	I-Method
architecture	E-Method
with	O
skip	O
connections	O
,	O
where	O
predictions	O
are	O
made	O
on	O
the	O
finest	O
level	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
.	O

Bottom	O
:	O
our	O
model	O
that	O
has	O
a	O
similar	O
structure	O
but	O
leverages	O
it	O
as	O
a	O
feature	O
pyramid	O
,	O
with	O
predictions	O
made	O
independently	O
at	O
all	O
levels	O
.	O

a	O
basic	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
detector	O
[	O
reference	O
]	O
,	O
surpassing	O
all	O
existing	O
heavily	O
-	O
engineered	O
single	O
-	O
model	O
entries	O
of	O
competition	O
winners	O
.	O

In	O
ablation	O
experiments	O
,	O
we	O
find	O
that	O
for	O
bounding	B-Task
box	I-Task
proposals	E-Task
,	O
FPN	S-Method
significantly	O
increases	O
the	O
Average	B-Metric
Recall	E-Metric
(	O
AR	S-Metric
)	O
by	O
8.0	O
points	O
;	O
for	O
object	B-Task
detection	E-Task
,	O
it	O
improves	O
the	O
COCO	S-Material
-	O
style	O
Average	B-Metric
Precision	E-Metric
(	O
AP	S-Metric
)	O
by	O
2.3	O
points	O
and	O
PASCAL	O
-	O
style	O
AP	S-Metric
by	O
3.8	O
points	O
,	O
over	O
a	O
strong	O
single	O
-	O
scale	O
baseline	O
of	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
on	O
ResNets	S-Method
[	O
reference	O
]	O
.	O

Our	O
method	O
is	O
also	O
easily	O
extended	O
to	O
mask	B-Task
proposals	E-Task
and	O
improves	O
both	O
instance	O
segmentation	O
AR	S-Metric
and	O
speed	S-Metric
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
that	O
heavily	O
depend	O
on	O
image	O
pyramids	O
.	O

In	O
addition	O
,	O
our	O
pyramid	B-Method
structure	E-Method
can	O
be	O
trained	O
end	O
-	O
toend	O
with	O
all	O
scales	O
and	O
is	O
used	O
consistently	O
at	O
train	B-Metric
/	I-Metric
test	I-Metric
time	E-Metric
,	O
which	O
would	O
be	O
memory	O
-	O
infeasible	O
using	O
image	B-Method
pyramids	E-Method
.	O

As	O
a	O
result	O
,	O
FPNs	S-Method
are	O
able	O
to	O
achieve	O
higher	O
accuracy	S-Metric
than	O
all	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Moreover	O
,	O
this	O
improvement	O
is	O
achieved	O
without	O
increasing	O
testing	B-Metric
time	E-Metric
over	O
the	O
single	O
-	O
scale	O
baseline	O
.	O

We	O
believe	O
these	O
advances	O
will	O
facilitate	O
future	O
research	O
and	O
applications	O
.	O

Our	O
code	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Related	O
Work	O
Hand	O
-	O
engineered	O
features	O
and	O
early	B-Method
neural	I-Method
networks	E-Method
.	O

SIFT	O
features	O
[	O
reference	O
]	O
were	O
originally	O
extracted	O
at	O
scale	O
-	O
space	O
extrema	O
and	O
used	O
for	O
feature	B-Task
point	I-Task
matching	E-Task
.	O

HOG	O
features	O
[	O
reference	O
]	O
,	O
and	O
later	O
SIFT	O
features	O
as	O
well	O
,	O
were	O
computed	O
densely	O
over	O
entire	O
image	O
pyramids	O
.	O

These	O
HOG	B-Method
and	I-Method
SIFT	I-Method
pyramids	E-Method
have	O
been	O
used	O
in	O
numerous	O
works	O
for	O
image	O
classification	S-Method
,	O
object	B-Task
detection	E-Task
,	O
human	B-Task
pose	I-Task
estimation	E-Task
,	O
and	O
more	O
.	O

There	O
has	O
also	O
been	O
significant	O
interest	O
in	O
computing	O
featurized	B-Task
image	I-Task
pyramids	E-Task
quickly	O
.	O

Dollár	O
et	O
al	O
.	O

[	O
reference	O
]	O
demonstrated	O
fast	O
pyramid	B-Task
computation	E-Task
by	O
first	O
computing	O
a	O
sparsely	B-Method
sampled	I-Method
(	I-Method
in	I-Method
scale	I-Method
)	I-Method
pyramid	E-Method
and	O
then	O
interpolating	O
missing	O
levels	O
.	O

Before	O
HOG	S-Method
and	O
SIFT	S-Method
,	O
early	O
work	O
on	O
face	B-Task
detection	E-Task
with	O
ConvNets	S-Method
[	O
reference	O
][	O
reference	O
]	O
computed	O
shallow	B-Method
networks	E-Method
over	O
image	O
pyramids	O
to	O
detect	O
faces	O
across	O
scales	O
.	O

Deep	B-Method
ConvNet	I-Method
object	I-Method
detectors	E-Method
.	O

With	O
the	O
development	O
of	O
modern	O
deep	B-Method
ConvNets	E-Method
[	O
reference	O
]	O
,	O
object	B-Method
detectors	E-Method
like	O
OverFeat	S-Method
[	O
reference	O
]	O
and	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
showed	O
dramatic	O
improvements	O
in	O
accuracy	S-Metric
.	O

OverFeat	O
adopted	O
a	O
strategy	O
similar	O
to	O
early	O
neural	B-Method
network	I-Method
face	I-Method
detectors	E-Method
by	O
applying	O
a	O
ConvNet	S-Method
as	O
a	O
sliding	B-Method
window	I-Method
detector	E-Method
on	O
an	O
image	O
pyramid	O
.	O

R	B-Method
-	I-Method
CNN	E-Method
adopted	O
a	O
region	B-Method
proposal	I-Method
-	I-Method
based	I-Method
strategy	E-Method
[	O
reference	O
]	O
in	O
which	O
each	O
proposal	O
was	O
scale	O
-	O
normalized	O
before	O
classifying	O
with	O
a	O
ConvNet	S-Method
.	O

SPPnet	S-Method
[	O
reference	O
]	O
demonstrated	O
that	O
such	O
region	B-Method
-	I-Method
based	I-Method
detectors	E-Method
could	O
be	O
applied	O
much	O
more	O
efficiently	O
on	O
feature	O
maps	O
extracted	O
on	O
a	O
single	O
image	O
scale	O
.	O

Recent	O
and	O
more	O
accurate	O
detection	B-Method
methods	E-Method
like	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
and	O
Faster	O
R	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
advocate	O
using	O
features	O
computed	O
from	O
a	O
single	O
scale	O
,	O
because	O
it	O
offers	O
a	O
good	O
trade	O
-	O
off	O
between	O
accuracy	S-Metric
and	O
speed	S-Metric
.	O

Multi	B-Task
-	I-Task
scale	I-Task
detection	E-Task
,	O
however	O
,	O
still	O
performs	O
better	O
,	O
especially	O
for	O
small	O
objects	O
.	O

Methods	O
using	O
multiple	O
layers	O
.	O

A	O
number	O
of	O
recent	O
approaches	O
improve	O
detection	B-Task
and	I-Task
segmentation	E-Task
by	O
using	O
different	O
layers	O
in	O
a	O
ConvNet	S-Method
.	O

FCN	S-Method
[	O
reference	O
]	O
sums	O
partial	O
scores	O
for	O
each	O
category	O
over	O
multiple	O
scales	O
to	O
compute	O
semantic	B-Task
segmentations	E-Task
.	O

Hypercolumns	S-Method
[	O
reference	O
]	O
uses	O
a	O
similar	O
method	O
for	O
object	B-Task
instance	I-Task
segmentation	E-Task
.	O

Several	O
other	O
approaches	O
(	O
HyperNet	S-Method
[	O
reference	O
]	O
,	O
ParseNet	S-Method
[	O
reference	O
]	O
,	O
and	O
ION	S-Method
[	O
reference	O
]	O
)	O
concatenate	O
features	O
of	O
multiple	O
layers	O
before	O
computing	O
predictions	S-Task
,	O
which	O
is	O
equivalent	O
to	O
summing	O
transformed	O
features	O
.	O

SSD	S-Method
[	O
reference	O
]	O
and	O
MS	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
predict	O
objects	O
at	O
multiple	O
layers	O
of	O
the	O
feature	O
hierarchy	O
without	O
combining	O
features	O
or	O
scores	O
.	O

There	O
are	O
recent	O
methods	O
exploiting	O
lateral	O
/	O
skip	O
connections	O
that	O
associate	O
low	O
-	O
level	O
feature	O
maps	O
across	O
resolutions	O
and	O
semantic	O
levels	O
,	O
including	O
U	B-Method
-	I-Method
Net	E-Method
[	O
reference	O
]	O
and	O
SharpMask	S-Method
[	O
reference	O
]	O
for	O
segmentation	S-Task
,	O
Recombinator	B-Method
networks	E-Method
[	O
reference	O
]	O
for	O
face	B-Task
detection	E-Task
,	O
and	O
Stacked	B-Method
Hourglass	I-Method
networks	E-Method
[	O
reference	O
]	O
for	O
keypoint	B-Task
estimation	E-Task
.	O

Ghiasi	O
et	O
al	O
.	O

[	O
reference	O
]	O
present	O
a	O
Laplacian	B-Method
pyramid	I-Method
presentation	E-Method
for	O
FCNs	S-Method
to	O
progressively	O
refine	O
segmentation	S-Task
.	O

Although	O
these	O
methods	O
adopt	O
architectures	O
with	O
pyramidal	O
shapes	O
,	O
they	O
are	O
unlike	O
featurized	B-Method
image	I-Method
pyramids	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
where	O
predictions	O
are	O
made	O
independently	O
at	O
all	O
levels	O
,	O
see	O
Fig	O
.	O

2	O
.	O

In	O
fact	O
,	O
for	O
the	O
pyramidal	O
architecture	O
in	O
Fig	O
.	O

2	O
(	O
top	O
)	O
,	O
image	O
pyramids	O
are	O
still	O
needed	O
to	O
recognize	O
objects	O
across	O
multiple	O
scales	O
[	O
reference	O
]	O
.	O

section	O
:	O
Feature	B-Method
Pyramid	I-Method
Networks	E-Method
Our	O
goal	O
is	O
to	O
leverage	O
a	O
ConvNet	O
's	O
pyramidal	O
feature	O
hierarchy	O
,	O
which	O
has	O
semantics	O
from	O
low	O
to	O
high	O
levels	O
,	O
and	O
build	O
a	O
feature	O
pyramid	O
with	O
high	O
-	O
level	O
semantics	O
throughout	O
.	O

The	O
resulting	O
Feature	B-Method
Pyramid	I-Method
Network	E-Method
is	O
generalpurpose	O
and	O
in	O
this	O
paper	O
we	O
focus	O
on	O
sliding	B-Method
window	I-Method
proposers	E-Method
(	O
Region	B-Method
Proposal	I-Method
Network	E-Method
,	O
RPN	S-Method
for	O
short	O
)	O
[	O
reference	O
]	O
and	O
region	B-Method
-	I-Method
based	I-Method
detectors	E-Method
(	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
)	O
[	O
reference	O
]	O
.	O

We	O
also	O
generalize	O
FPNs	S-Method
to	O
instance	B-Task
segmentation	I-Task
proposals	E-Task
in	O
Sec	O
.	O

6	O
.	O

Our	O
method	O
takes	O
a	O
single	O
-	O
scale	O
image	O
of	O
an	O
arbitrary	O
size	O
as	O
input	O
,	O
and	O
outputs	O
proportionally	O
sized	O
feature	O
maps	O
at	O
multiple	O
levels	O
,	O
in	O
a	O
fully	B-Method
convolutional	I-Method
fashion	E-Method
.	O

This	O
process	O
is	O
independent	O
of	O
the	O
backbone	B-Method
convolutional	I-Method
architectures	E-Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
,	O
and	O
in	O
this	O
paper	O
we	O
present	O
results	O
using	O
ResNets	S-Method
[	O
reference	O
]	O
.	O

The	O
construction	O
of	O
our	O
pyramid	O
involves	O
a	O
bottom	O
-	O
up	O
pathway	O
,	O
a	O
top	O
-	O
down	O
pathway	O
,	O
and	O
lateral	O
connections	O
,	O
as	O
introduced	O
in	O
the	O
following	O
.	O

Bottom	O
-	O
up	O
pathway	O
.	O

The	O
bottom	O
-	O
up	O
pathway	O
is	O
the	O
feedforward	B-Method
computation	E-Method
of	O
the	O
backbone	B-Method
ConvNet	E-Method
,	O
which	O
computes	O
a	O
feature	O
hierarchy	O
consisting	O
of	O
feature	O
maps	O
at	O
several	O
scales	O
with	O
a	O
scaling	O
step	O
of	O
2	O
.	O

There	O
are	O
often	O
many	O
layers	O
producing	O
output	O
maps	O
of	O
the	O
same	O
size	O
and	O
we	O
say	O
these	O
layers	O
are	O
in	O
the	O
same	O
network	O
stage	O
.	O

For	O
our	O
feature	O
pyramid	O
,	O
we	O
define	O
one	O
pyramid	O
level	O
for	O
each	O
stage	O
.	O

We	O
choose	O
the	O
output	O
of	O
the	O
last	O
layer	O
of	O
each	O
stage	O
as	O
our	O
reference	O
set	O
of	O
feature	O
maps	O
,	O
which	O
we	O
will	O
enrich	O
to	O
create	O
our	O
pyramid	O
.	O

This	O
choice	O
is	O
natural	O
since	O
the	O
deepest	O
layer	O
of	O
each	O
stage	O
should	O
have	O
the	O
strongest	O
features	O
.	O

Specifically	O
,	O
for	O
ResNets	O
[	O
reference	O
]	O
we	O
use	O
the	O
feature	O
activations	O
output	O
by	O
each	O
stage	O
's	O
last	O
residual	O
block	O
.	O

We	O
denote	O
the	O
output	O
of	O
these	O
last	O
residual	O
blocks	O
as	O
{	O
C	O
2	O
,	O
C	O
3	O
,	O
C	O
4	O
,	O
C	O
5	O
}	O
for	O
conv2	S-Method
,	O
conv3	O
,	O
conv4	S-Method
,	O
and	O
conv5	O
outputs	O
,	O
and	O
note	O
that	O
they	O
have	O
strides	O
of	O
{	O
4	O
,	O
8	O
,	O
16	O
,	O
32	O
}	O
pixels	O
with	O
respect	O
to	O
the	O
input	O
image	O
.	O

We	O
do	O
not	O
include	O
conv1	O
into	O
the	O
pyramid	O
due	O
to	O
its	O
large	O
memory	O
footprint	O
.	O

Top	O
-	O
down	O
pathway	O
and	O
lateral	O
connections	O
.	O

The	O
topdown	B-Method
pathway	E-Method
hallucinates	O
higher	O
resolution	O
features	O
by	O
upsampling	O
spatially	O
coarser	O
,	O
but	O
semantically	O
stronger	O
,	O
feature	O
maps	O
from	O
higher	O
pyramid	O
levels	O
.	O

These	O
features	O
are	O
then	O
enhanced	O
with	O
features	O
from	O
the	O
bottom	B-Method
-	I-Method
up	I-Method
pathway	E-Method
via	O
lateral	B-Method
connections	E-Method
.	O

Each	O
lateral	O
connection	O
merges	O
feature	O
maps	O
of	O
the	O
same	O
spatial	O
size	O
from	O
the	O
bottom	O
-	O
up	O
pathway	O
and	O
the	O
top	O
-	O
down	O
pathway	O
.	O

The	O
bottom	B-Method
-	I-Method
up	I-Method
feature	I-Method
map	E-Method
is	O
of	O
lower	O
-	O
level	O
semantics	O
,	O
but	O
its	O
activations	O
are	O
more	O
accurately	O
localized	O
as	O
it	O
was	O
subsampled	O
fewer	O
times	O
.	O

Fig	O
.	O

3	O
shows	O
the	O
building	O
block	O
that	O
constructs	O
our	O
topdown	B-Method
feature	I-Method
maps	E-Method
.	O

With	O
a	O
coarser	O
-	O
resolution	O
feature	O
map	O
,	O
we	O
upsample	O
the	O
spatial	O
resolution	O
by	O
a	O
factor	O
of	O
2	O
(	O
using	O
nearest	B-Method
neighbor	I-Method
upsampling	E-Method
for	O
simplicity	O
)	O
.	O

The	O
upsam	B-Method
-	I-Method
pled	I-Method
map	E-Method
is	O
then	O
merged	O
with	O
the	O
corresponding	O
bottom	O
-	O
up	O
map	O
(	O
which	O
undergoes	O
a	O
1×1	B-Method
convolutional	I-Method
layer	E-Method
to	O
reduce	O
channel	O
dimensions	O
)	O
by	O
element	O
-	O
wise	O
addition	O
.	O

This	O
process	O
is	O
iterated	O
until	O
the	O
finest	O
resolution	O
map	O
is	O
generated	O
.	O

To	O
start	O
the	O
iteration	O
,	O
we	O
simply	O
attach	O
a	O
1×1	B-Method
convolutional	I-Method
layer	E-Method
on	O
C	O
5	O
to	O
produce	O
the	O
coarsest	O
resolution	O
map	O
.	O

Finally	O
,	O
we	O
append	O
a	O
3×3	B-Method
convolution	E-Method
on	O
each	O
merged	O
map	O
to	O
generate	O
the	O
final	O
feature	O
map	O
,	O
which	O
is	O
to	O
reduce	O
the	O
aliasing	O
effect	O
of	O
upsampling	O
.	O

This	O
final	O
set	O
of	O
feature	O
maps	O
is	O
called	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
}	O
,	O
corresponding	O
to	O
{	O
C	O
2	O
,	O
C	O
3	O
,	O
C	O
4	O
,	O
C	O
5	O
}	O
that	O
are	O
respectively	O
of	O
the	O
same	O
spatial	O
sizes	O
.	O

Because	O
all	O
levels	O
of	O
the	O
pyramid	O
use	O
shared	B-Method
classifiers	I-Method
/	I-Method
regressors	E-Method
as	O
in	O
a	O
traditional	O
featurized	B-Method
image	I-Method
pyramid	E-Method
,	O
we	O
fix	O
the	O
feature	O
dimension	O
(	O
numbers	O
of	O
channels	O
,	O
denoted	O
as	O
d	O
)	O
in	O
all	O
the	O
feature	O
maps	O
.	O

We	O
set	O
d	O
=	O
256	O
in	O
this	O
paper	O
and	O
thus	O
all	O
extra	O
convolutional	B-Method
layers	E-Method
have	O
256	O
-	O
channel	O
outputs	O
.	O

There	O
are	O
no	O
non	O
-	O
linearities	O
in	O
these	O
extra	O
layers	O
,	O
which	O
we	O
have	O
empirically	O
found	O
to	O
have	O
minor	O
impacts	O
.	O

Simplicity	O
is	O
central	O
to	O
our	O
design	O
and	O
we	O
have	O
found	O
that	O
our	O
model	O
is	O
robust	O
to	O
many	O
design	O
choices	O
.	O

We	O
have	O
experimented	O
with	O
more	O
sophisticated	O
blocks	O
(	O
e.g.	O
,	O
using	O
multilayer	O
residual	O
blocks	O
[	O
reference	O
]	O
as	O
the	O
connections	O
)	O
and	O
observed	O
marginally	O
better	O
results	O
.	O

Designing	O
better	O
connection	O
modules	O
is	O
not	O
the	O
focus	O
of	O
this	O
paper	O
,	O
so	O
we	O
opt	O
for	O
the	O
simple	O
design	O
described	O
above	O
.	O

section	O
:	O
Applications	O
Our	O
method	O
is	O
a	O
generic	O
solution	O
for	O
building	O
feature	B-Task
pyramids	E-Task
inside	O
deep	B-Task
ConvNets	E-Task
.	O

In	O
the	O
following	O
we	O
adopt	O
our	O
method	O
in	O
RPN	S-Method
[	O
reference	O
]	O
for	O
bounding	B-Task
box	I-Task
proposal	I-Task
generation	E-Task
and	O
in	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
for	O
object	B-Task
detection	E-Task
.	O

To	O
demonstrate	O
the	O
simplicity	O
and	O
effectiveness	O
of	O
our	O
method	O
,	O
we	O
make	O
minimal	O
modifications	O
to	O
the	O
original	O
systems	O
of	O
[	O
reference	O
][	O
reference	O
]	O
when	O
adapting	O
them	O
to	O
our	O
feature	B-Method
pyramid	E-Method
.	O

section	O
:	O
Feature	B-Method
Pyramid	I-Method
Networks	E-Method
for	O
RPN	B-Method
RPN	E-Method
[	O
reference	O
]	O
is	O
a	O
sliding	B-Method
-	I-Method
window	I-Method
class	I-Method
-	I-Method
agnostic	I-Method
object	I-Method
detector	E-Method
.	O

In	O
the	O
original	O
RPN	S-Method
design	O
,	O
a	O
small	O
subnetwork	O
is	O
evaluated	O
on	O
dense	O
3×3	O
sliding	O
windows	O
,	O
on	O
top	O
of	O
a	O
singlescale	B-Method
convolutional	I-Method
feature	I-Method
map	E-Method
,	O
performing	O
object	O
/	O
nonobject	O
binary	O
classification	S-Method
and	O
bounding	B-Method
box	I-Method
regression	E-Method
.	O

This	O
is	O
realized	O
by	O
a	O
3×3	B-Method
convolutional	I-Method
layer	E-Method
followed	O
by	O
two	O
sibling	B-Method
1×1	I-Method
convolutions	E-Method
for	O
classification	S-Method
and	O
regression	S-Task
,	O
which	O
we	O
refer	O
to	O
as	O
a	O
network	O
head	O
.	O

The	O
object	B-Task
/	I-Task
nonobject	I-Task
criterion	E-Task
and	O
bounding	O
box	O
regression	O
target	O
are	O
defined	O
with	O
respect	O
to	O
a	O
set	O
of	O
reference	O
boxes	O
called	O
anchors	O
[	O
reference	O
]	O
.	O

The	O
anchors	O
are	O
of	O
multiple	O
pre	O
-	O
defined	O
scales	O
and	O
aspect	O
ratios	O
in	O
order	O
to	O
cover	O
objects	O
of	O
different	O
shapes	O
.	O

We	O
adapt	O
RPN	S-Method
by	O
replacing	O
the	O
single	B-Method
-	I-Method
scale	I-Method
feature	I-Method
map	E-Method
with	O
our	O
FPN	S-Method
.	O

We	O
attach	O
a	O
head	O
of	O
the	O
same	O
design	O
(	O
3×3	O
conv	O
and	O
two	O
sibling	O
1×1	O
convs	O
)	O
to	O
each	O
level	O
on	O
our	O
feature	O
pyramid	O
.	O

Because	O
the	O
head	O
slides	O
densely	O
over	O
all	O
locations	O
in	O
all	O
pyramid	O
levels	O
,	O
it	O
is	O
not	O
necessary	O
to	O
have	O
multi	O
-	O
scale	O
anchors	O
on	O
a	O
specific	O
level	O
.	O

Instead	O
,	O
we	O
assign	O
anchors	O
of	O
a	O
single	O
scale	O
to	O
each	O
level	O
.	O

Formally	O
,	O
we	O
define	O
the	O
anchors	O
to	O
have	O
areas	O
of	O
{	O
32	O
2	O
,	O
64	O
2	O
,	O
128	O
2	O
,	O
256	O
2	O
,	O
512	O
2	O
}	O
pixels	O
on	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
respectively	O
.	O

[	O
reference	O
]	O
As	O
in	O
[	O
reference	O
]	O
we	O
also	O
use	O
anchors	O
of	O
multiple	O
aspect	O
ratios	O
{	O
1:2	O
,	O
1:1	O
,	O
2:1	O
}	O
at	O
each	O
level	O
.	O

So	O
in	O
total	O
there	O
are	O
15	O
anchors	O
over	O
the	O
pyramid	O
.	O

We	O
assign	O
training	O
labels	O
to	O
the	O
anchors	O
based	O
on	O
their	O
Intersection	B-Metric
-	I-Metric
over	I-Metric
-	I-Metric
Union	E-Metric
(	O
IoU	S-Metric
)	O
ratios	O
with	O
ground	O
-	O
truth	O
bounding	O
boxes	O
as	O
in	O
[	O
reference	O
]	O
.	O

Formally	O
,	O
an	O
anchor	O
is	O
assigned	O
a	O
positive	O
label	O
if	O
it	O
has	O
the	O
highest	O
IoU	S-Metric
for	O
a	O
given	O
groundtruth	O
box	O
or	O
an	O
IoU	S-Metric
over	O
0.7	O
with	O
any	O
ground	O
-	O
truth	O
box	O
,	O
and	O
a	O
negative	O
label	O
if	O
it	O
has	O
IoU	S-Metric
lower	O
than	O
0.3	O
for	O
all	O
ground	O
-	O
truth	O
boxes	O
.	O

Note	O
that	O
scales	O
of	O
ground	O
-	O
truth	O
boxes	O
are	O
not	O
explicitly	O
used	O
to	O
assign	O
them	O
to	O
the	O
levels	O
of	O
the	O
pyramid	O
;	O
instead	O
,	O
ground	O
-	O
truth	O
boxes	O
are	O
associated	O
with	O
anchors	O
,	O
which	O
have	O
been	O
assigned	O
to	O
pyramid	O
levels	O
.	O

As	O
such	O
,	O
we	O
introduce	O
no	O
extra	O
rules	O
in	O
addition	O
to	O
those	O
in	O
[	O
reference	O
]	O
.	O

We	O
note	O
that	O
the	O
parameters	O
of	O
the	O
heads	O
are	O
shared	O
across	O
all	O
feature	O
pyramid	O
levels	O
;	O
we	O
have	O
also	O
evaluated	O
the	O
alternative	O
without	O
sharing	O
parameters	O
and	O
observed	O
similar	O
accuracy	S-Metric
.	O

The	O
good	O
performance	O
of	O
sharing	O
parameters	O
indicates	O
that	O
all	O
levels	O
of	O
our	O
pyramid	O
share	O
similar	O
semantic	O
levels	O
.	O

This	O
advantage	O
is	O
analogous	O
to	O
that	O
of	O
using	O
a	O
featurized	B-Method
image	I-Method
pyramid	E-Method
,	O
where	O
a	O
common	B-Method
head	I-Method
classifier	E-Method
can	O
be	O
applied	O
to	O
features	O
computed	O
at	O
any	O
image	O
scale	O
.	O

With	O
the	O
above	O
adaptations	O
,	O
RPN	S-Method
can	O
be	O
naturally	O
trained	O
and	O
tested	O
with	O
our	O
FPN	S-Method
,	O
in	O
the	O
same	O
fashion	O
as	O
in	O
[	O
reference	O
]	O
.	O

We	O
elaborate	O
on	O
the	O
implementation	O
details	O
in	O
the	O
experiments	O
.	O

section	O
:	O
Feature	B-Method
Pyramid	I-Method
Networks	E-Method
for	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	I-Method
Fast	I-Method
R	I-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
is	O
a	O
region	B-Method
-	I-Method
based	I-Method
object	I-Method
detector	E-Method
in	O
which	O
Region	B-Method
-	I-Method
of	I-Method
-	I-Method
Interest	E-Method
(	O
RoI	S-Method
)	O
pooling	O
is	O
used	O
to	O
extract	O
features	O
.	O

Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
is	O
most	O
commonly	O
performed	O
on	O
a	O
single	O
-	O
scale	O
feature	O
map	O
.	O

To	O
use	O
it	O
with	O
our	O
FPN	S-Method
,	O
we	O
need	O
to	O
assign	O
RoIs	O
of	O
different	O
scales	O
to	O
the	O
pyramid	O
levels	O
.	O

We	O
view	O
our	O
feature	B-Method
pyramid	E-Method
as	O
if	O
it	O
were	O
produced	O
from	O
an	O
image	B-Method
pyramid	E-Method
.	O

Thus	O
we	O
can	O
adapt	O
the	O
assignment	B-Method
strategy	E-Method
of	O
region	B-Method
-	I-Method
based	I-Method
detectors	E-Method
[	O
reference	O
][	O
reference	O
]	O
in	O
the	O
case	O
when	O
they	O
are	O
run	O
on	O
image	O
pyramids	O
.	O

Formally	O
,	O
we	O
assign	O
an	O
RoI	S-Method
of	O
width	O
w	O
and	O
height	O
h	O
(	O
on	O
the	O
input	O
image	O
to	O
the	O
network	O
)	O
to	O
the	O
level	O
P	O
k	O
of	O
our	O
feature	B-Method
pyramid	E-Method
by	O
:	O
Here	O
224	O
is	O
the	O
canonical	B-Material
ImageNet	I-Material
pre	I-Material
-	I-Material
training	I-Material
size	E-Material
,	O
and	O
k	O
0	O
is	O
the	O
target	O
level	O
on	O
which	O
an	O
RoI	S-Method
with	O
w	O
×	O
h	O
=	O
224	O
2	O
should	O
be	O
mapped	O
into	O
.	O

Analogous	O
to	O
the	O
ResNet	O
-	O
based	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
system	O
[	O
reference	O
]	O
that	O
uses	O
C	O
4	O
as	O
the	O
single	O
-	O
scale	O
feature	O
map	O
,	O
we	O
set	O
k	O
0	O
to	O
4	O
.	O

Intuitively	O
,	O
Eqn	O
.	O

(	O
1	O
)	O
means	O
that	O
if	O
the	O
RoI	S-Method
's	O
scale	O
becomes	O
smaller	O
(	O
say	O
,	O
1	O
/	O
2	O
of	O
224	O
)	O
,	O
it	O
should	O
be	O
mapped	O
into	O
a	O
finer	O
-	O
resolution	O
level	O
(	O
say	O
,	O
k	O
=	O
3	O
)	O
.	O

We	O
attach	O
predictor	O
heads	O
(	O
in	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
the	O
heads	O
are	O
class	B-Method
-	I-Method
specific	I-Method
classifiers	E-Method
and	O
bounding	B-Method
box	I-Method
regressors	E-Method
)	O
to	O
all	O
RoIs	O
of	O
all	O
levels	O
.	O

Again	O
,	O
the	O
heads	O
all	O
share	O
parameters	O
,	O
regardless	O
of	O
their	O
levels	O
.	O

In	O
[	O
reference	O
]	O
,	O
a	O
ResNet	O
's	O
conv5	O
layers	O
(	O
a	O
9	B-Method
-	I-Method
layer	I-Method
deep	I-Method
subnetwork	E-Method
)	O
are	O
adopted	O
as	O
the	O
head	O
on	O
top	O
of	O
the	O
conv4	O
features	O
,	O
but	O
our	O
method	O
has	O
already	O
harnessed	O
conv5	S-Method
to	O
construct	O
the	O
feature	O
pyramid	O
.	O

So	O
unlike	O
[	O
reference	O
]	O
,	O
we	O
simply	O
adopt	O
RoI	S-Method
pooling	O
to	O
extract	O
7×7	O
features	O
,	O
and	O
attach	O
two	O
hidden	O
1	O
,	O
024	O
-	O
d	O
fully	B-Method
-	I-Method
connected	E-Method
(	O
fc	S-Method
)	O
layers	O
(	O
each	O
followed	O
by	O
ReLU	S-Method
)	O
before	O
the	O
final	O
classification	S-Method
and	O
bounding	B-Method
box	I-Method
regression	I-Method
layers	E-Method
.	O

These	O
layers	O
are	O
randomly	O
initialized	O
,	O
as	O
there	O
are	O
no	O
pre	O
-	O
trained	O
fc	S-Method
layers	O
available	O
in	O
ResNets	S-Material
.	O

Note	O
that	O
compared	O
to	O
the	O
standard	O
conv5	B-Method
head	E-Method
,	O
our	O
2	O
-	O
fc	S-Method
MLP	O
head	O
is	O
lighter	O
weight	O
and	O
faster	O
.	O

Based	O
on	O
these	O
adaptations	O
,	O
we	O
can	O
train	O
and	O
test	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
on	O
top	O
of	O
the	O
feature	O
pyramid	O
.	O

Implementation	O
details	O
are	O
given	O
in	O
the	O
experimental	O
section	O
.	O

section	O
:	O
Experiments	O
on	O
Object	B-Task
Detection	E-Task
We	O
perform	O
experiments	O
on	O
the	O
80	O
category	O
COCO	S-Material
detection	O
dataset	O
[	O
reference	O
]	O
.	O

We	O
train	O
using	O
the	O
union	O
of	O
80k	O
train	O
images	O
and	O
a	O
35k	O
subset	O
of	O
val	O
images	O
(	O
trainval35k	O
[	O
reference	O
]	O
)	O
,	O
and	O
report	O
ablations	S-Method
on	O
a	O
5k	O
subset	O
of	O
val	O
images	O
(	O
minival	O
)	O
.	O

We	O
also	O
report	O
final	O
results	O
on	O
the	O
standard	O
test	O
set	O
(	O
test	O
-	O
std	O
)	O
[	O
reference	O
]	O
which	O
has	O
no	O
disclosed	O
labels	O
.	O

As	O
is	O
common	O
practice	O
[	O
reference	O
]	O
,	O
all	O
network	B-Method
backbones	E-Method
are	O
pre	O
-	O
trained	O
on	O
the	O
ImageNet1k	O
classification	S-Method
set	O
[	O
reference	O
]	O
and	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
detection	O
dataset	O
.	O

We	O
use	O
the	O
pre	O
-	O
trained	O
ResNet	B-Method
-	I-Method
50	E-Method
and	O
ResNet	B-Method
-	I-Method
101	I-Method
models	E-Method
that	O
are	O
publicly	O
available	O
.	O

[	O
reference	O
]	O
Our	O
code	O
is	O
a	O
reimplementation	O
of	O
py	B-Method
-	I-Method
faster	I-Method
-	I-Method
rcnn	I-Method
3	E-Method
using	O
Caffe2	S-Method
.	O

[	O
reference	O
]	O
section	O
:	O
Region	B-Method
Proposal	E-Method
with	O
RPN	S-Method
We	O
evaluate	O
the	O
COCO	S-Material
-	O
style	O
Average	B-Metric
Recall	E-Metric
(	O
AR	S-Metric
)	O
and	O
AR	S-Metric
on	O
small	O
,	O
medium	O
,	O
and	O
large	O
objects	O
(	O
AR	S-Metric
s	O
,	O
AR	S-Metric
m	O
,	O
and	O
AR	S-Metric
l	O
)	O
following	O
the	O
definitions	O
in	O
[	O
reference	O
]	O
.	O

We	O
report	O
results	O
for	O
100	O
and	O
1000	O
proposals	O
per	O
images	O
(	O
AR	S-Metric
100	O
and	O
AR	S-Metric
1k	O
)	O
.	O

Implementation	O
details	O
.	O

All	O
architectures	O
in	O
Table	O
1	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

The	O
input	O
image	O
is	O
resized	O
such	O
that	O
its	O
shorter	O
side	O
has	O
800	O
pixels	O
.	O

We	O
adopt	O
synchronized	B-Method
SGD	I-Method
training	E-Method
on	O
8	O
GPUs	S-Method
.	O

A	O
mini	O
-	O
batch	O
involves	O
2	O
images	O
per	O
GPU	O
and	O
256	O
anchors	O
per	O
image	O
.	O

We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

The	O
learning	B-Metric
rate	E-Metric
is	O
0.02	O
for	O
the	O
first	O
30k	O
mini	O
-	O
batches	O
and	O
0.002	O
for	O
the	O
next	O
10k	O
.	O

For	O
all	O
RPN	S-Method
experiments	O
(	O
including	O
baselines	O
)	O
,	O
we	O
include	O
the	O
anchor	O
boxes	O
that	O
are	O
outside	O
the	O
image	O
for	O
training	O
,	O
which	O
is	O
unlike	O
[	O
reference	O
]	O
where	O
these	O
anchor	O
boxes	O
are	O
ignored	O
.	O

Other	O
implementation	O
details	O
are	O
as	O
in	O
[	O
reference	O
]	O
.	O

Training	O
RPN	S-Method
with	O
FPN	S-Method
on	O
8	O
GPUs	S-Method
takes	O
about	O
8	O
hours	O
on	O
COCO	S-Material
.	O

1	O
(	O
a	O
)	O
)	O
.	O

In	O
addition	O
,	O
the	O
performance	O
on	O
small	O
objects	O
(	O
AR	S-Metric
1k	O
s	O
)	O
is	O
boosted	O
by	O
a	O
large	O
margin	O
of	O
12.9	O
points	O
.	O

Our	O
pyramid	B-Method
representation	E-Method
greatly	O
improves	O
RPN	S-Method
's	O
robustness	S-Metric
to	O
object	O
scale	O
variation	O
.	O

How	O
important	O
is	O
top	O
-	O
down	O
enrichment	O
?	O
Table	O
1	O
(	O
d	O
)	O
shows	O
the	O
results	O
of	O
our	O
feature	B-Method
pyramid	E-Method
without	O
the	O
topdown	B-Method
pathway	E-Method
.	O

With	O
this	O
modification	O
,	O
the	O
1×1	O
lateral	O
connections	O
followed	O
by	O
3×3	B-Method
convolutions	E-Method
are	O
attached	O
to	O
the	O
bottom	O
-	O
up	O
pyramid	O
.	O

This	O
architecture	O
simulates	O
the	O
effect	O
of	O
reusing	O
the	O
pyramidal	O
feature	O
hierarchy	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
.	O

The	O
results	O
in	O
Table	O
1	O
(	O
d	O
)	O
are	O
just	O
on	O
par	O
with	O
the	O
RPN	S-Method
baseline	O
and	O
lag	O
far	O
behind	O
ours	O
.	O

We	O
conjecture	O
that	O
this	O
is	O
because	O
there	O
are	O
large	O
semantic	O
gaps	O
between	O
different	O
levels	O
on	O
the	O
bottom	O
-	O
up	O
pyramid	O
(	O
Fig	O
.	O

1	O
(	O
b	O
)	O
)	O
,	O
especially	O
for	O
very	O
deep	B-Task
ResNets	E-Task
.	O

We	O
have	O
also	O
evaluated	O
a	O
variant	O
of	O
Table	B-Method
1	I-Method
(	I-Method
d	E-Method
)	O
without	O
sharing	O
the	O
parameters	O
of	O
the	O
heads	O
,	O
but	O
observed	O
similarly	O
degraded	O
performance	O
.	O

This	O
issue	O
can	O
not	O
be	O
simply	O
remedied	O
by	O
level	O
-	O
specific	O
heads	O
.	O

How	O
important	O
are	O
lateral	O
connections	O
?	O
Table	O
1	O
(	O
e	O
)	O
shows	O
the	O
ablation	O
results	O
of	O
a	O
top	B-Method
-	I-Method
down	I-Method
feature	I-Method
pyramid	E-Method
without	O
the	O
1×1	O
lateral	O
connections	O
.	O

This	O
top	B-Method
-	I-Method
down	I-Method
pyramid	E-Method
has	O
strong	O
semantic	O
features	O
and	O
fine	O
resolutions	O
.	O

But	O
we	O
argue	O
that	O
the	O
locations	O
of	O
these	O
features	O
are	O
not	O
precise	O
,	O
because	O
these	O
maps	O
have	O
been	O
downsampled	O
and	O
upsampled	O
several	O
times	O
.	O

More	O
precise	O
locations	O
of	O
features	O
can	O
be	O
directly	O
passed	O
from	O
the	O
finer	O
levels	O
of	O
the	O
bottom	O
-	O
up	O
maps	O
via	O
the	O
lateral	O
connections	O
to	O
the	O
top	O
-	O
down	O
maps	O
.	O

As	O
a	O
results	O
,	O
FPN	S-Method
has	O
an	O
AR	S-Metric
1k	O
score	O
10	O
points	O
higher	O
than	O
Table	O
1	O
section	O
:	O
(	O
e	O
)	O
.	O

How	O
important	O
are	O
pyramid	B-Method
representations	E-Method
?	O
Instead	O
of	O
resorting	O
to	O
pyramid	B-Method
representations	E-Method
,	O
one	O
can	O
attach	O
the	O
head	O
to	O
the	O
highest	O
-	O
resolution	O
,	O
strongly	O
semantic	O
feature	O
maps	O
of	O
P	O
2	O
(	O
i.e.	O
,	O
the	O
finest	O
level	O
in	O
our	O
pyramids	O
)	O
.	O

Similar	O
to	O
the	O
single	O
-	O
scale	B-Method
baselines	E-Method
,	O
we	O
assign	O
all	O
anchors	O
to	O
the	O
P	O
2	O
feature	O
map	O
.	O

This	O
variant	O
(	O
Table	O
1	O
(	O
f	O
)	O
)	O
is	O
better	O
than	O
the	O
baseline	O
but	O
inferior	O
to	O
our	O
approach	O
.	O

RPN	S-Method
is	O
a	O
sliding	B-Method
window	I-Method
detector	E-Method
with	O
a	O
fixed	O
window	O
size	O
,	O
so	O
scanning	O
over	O
pyramid	O
levels	O
can	O
increase	O
its	O
robustness	S-Metric
to	O
scale	O
variance	O
.	O

In	O
addition	O
,	O
we	O
note	O
that	O
using	O
P	B-Method
2	E-Method
alone	O
leads	O
to	O
more	O
anchors	O
(	O
750k	O
,	O
Table	O
1	O
.	O

Bounding	B-Method
box	I-Method
proposal	E-Method
results	O
using	O
RPN	S-Method
[	O
reference	O
]	O
,	O
evaluated	O
on	O
the	O
COCO	S-Material
minival	O
set	O
.	O

All	O
models	O
are	O
trained	O
on	O
trainval35k	O
.	O

The	O
columns	O
"	O
lateral	O
"	O
and	O
"	O
top	O
-	O
down	O
"	O
denote	O
the	O
presence	O
of	O
lateral	O
and	O
top	O
-	O
down	O
connections	O
,	O
respectively	O
.	O

The	O
column	O
"	O
feature	O
"	O
denotes	O
the	O
feature	O
maps	O
on	O
which	O
the	O
heads	O
are	O
attached	O
.	O

All	O
results	O
are	O
based	O
on	O
ResNet	B-Method
-	I-Method
50	E-Method
and	O
share	O
the	O
same	O
hyper	O
-	O
parameters	O
.	O

section	O
:	O
Object	B-Task
Detection	E-Task
with	O
Fast	S-Method
/	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
Next	O
we	O
investigate	O
FPN	S-Method
for	O
region	B-Method
-	I-Method
based	I-Method
(	I-Method
non	I-Method
-	I-Method
sliding	I-Method
window	I-Method
)	I-Method
detectors	E-Method
.	O

We	O
evaluate	O
object	B-Task
detection	E-Task
by	O
the	O
COCO	B-Metric
-	I-Metric
style	I-Metric
Average	I-Metric
Precision	E-Metric
(	O
AP	S-Metric
)	O
and	O
PASCAL	O
-	O
style	O
AP	S-Metric
(	O
at	O
a	O
single	O
IoU	S-Metric
threshold	O
of	O
0.5	O
)	O
.	O

We	O
also	O
report	O
COCO	B-Metric
AP	E-Metric
on	O
objects	O
of	O
small	O
,	O
medium	O
,	O
and	O
large	O
sizes	O
(	O
namely	O
,	O
AP	S-Metric
s	O
,	O
AP	S-Metric
m	O
,	O
and	O
AP	S-Metric
l	O
)	O
following	O
the	O
definitions	O
in	O
[	O
reference	O
]	O
.	O

Implementation	O
details	O
.	O

The	O
input	O
image	O
is	O
resized	O
such	O
that	O
its	O
shorter	O
side	O
has	O
800	O
pixels	O
.	O

Synchronized	B-Method
SGD	E-Method
is	O
used	O
to	O
train	O
the	O
model	O
on	O
8	O
GPUs	O
.	O

Each	O
mini	O
-	O
batch	O
involves	O
2	O
image	O
per	O
GPU	O
and	O
512	O
RoIs	O
per	O
image	O
.	O

We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

The	O
learning	B-Metric
rate	E-Metric
is	O
0.02	O
for	O
the	O
first	O
60k	O
mini	O
-	O
batches	O
and	O
0.002	O
for	O
the	O
next	O
20k	O
.	O

We	O
use	O
2000	O
RoIs	O
per	O
image	O
for	O
training	O
and	O
1000	O
for	O
testing	O
.	O

Training	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
with	O
FPN	S-Method
takes	O
about	O
10	O
hours	O
on	O
the	O
COCO	S-Material
dataset	O
.	O

section	O
:	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
(	O
on	O
fixed	O
proposals	O
)	O
To	O
better	O
investigate	O
FPN	S-Method
's	O
effects	O
on	O
the	O
region	B-Method
-	I-Method
based	I-Method
detector	E-Method
alone	O
,	O
we	O
conduct	O
ablations	O
of	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
on	O
a	O
fixed	O
set	O
of	O
proposals	O
.	O

We	O
choose	O
to	O
freeze	O
the	O
proposals	O
as	O
computed	O
by	O
RPN	S-Method
on	O
FPN	S-Method
(	O
Table	O
1	O
(	O
c	O
)	O
)	O
,	O
because	O
it	O
has	O
good	O
performance	O
on	O
small	O
objects	O
that	O
are	O
to	O
be	O
recognized	O
by	O
the	O
detector	O
.	O

For	O
simplicity	O
we	O
do	O
not	O
share	O
features	O
between	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
and	O
RPN	S-Method
,	O
except	O
when	O
specified	O
.	O

As	O
a	O
ResNet	O
-	O
based	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
baseline	O
,	O
following	O
[	O
reference	O
]	O
,	O
we	O
adopt	O
RoI	S-Method
pooling	O
with	O
an	O
output	O
size	O
of	O
14×14	O
and	O
attach	O
all	O
conv5	B-Method
layers	E-Method
as	O
the	O
hidden	O
layers	O
of	O
the	O
head	O
.	O

This	O
gives	O
an	O
AP	S-Metric
of	O
31.9	O
in	O
Table	O
2	O
(	O
a	O
)	O
.	O

Table	O
2	O
(	O
b	O
)	O
is	O
a	O
baseline	O
exploiting	O
an	O
MLP	B-Method
head	E-Method
with	O
2	O
hidden	O
fc	S-Method
layers	O
,	O
similar	O
to	O
the	O
head	O
in	O
our	O
architecture	O
.	O

It	O
gets	O
an	O
AP	S-Metric
of	O
28.8	O
,	O
indicating	O
that	O
the	O
2	O
-	O
fc	S-Method
head	O
does	O
not	O
give	O
us	O
any	O
orthogonal	O
advantage	O
over	O
the	O
baseline	O
in	O
Table	O
2	O
(	O
a	O
)	O
.	O

Table	O
2	O
(	O
c	O
)	O
shows	O
the	O
results	O
of	O
our	O
FPN	S-Method
in	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
.	O

Comparing	O
with	O
the	O
baseline	O
in	O
Table	O
2	O
(	O
a	O
)	O
,	O
our	O
method	O
improves	O
AP	S-Metric
by	O
2.0	O
points	O
and	O
small	O
object	O
AP	S-Metric
by	O
2.1	O
points	O
.	O

Comparing	O
with	O
the	O
baseline	O
that	O
also	O
adopts	O
a	O
2fc	O
head	O
(	O
Table	O
2	O
(	O
b	O
)	O
)	O
,	O
our	O
method	O
improves	O
AP	S-Metric
by	O
5.1	O
points	O
.	O

[	O
reference	O
]	O
These	O
comparisons	O
indicate	O
that	O
our	O
feature	B-Method
pyramid	E-Method
is	O
superior	O
to	O
single	B-Method
-	I-Method
scale	I-Method
features	E-Method
for	O
a	O
region	B-Method
-	I-Method
based	I-Method
object	I-Method
detector	E-Method
.	O

nections	O
or	O
removing	O
lateral	O
connections	O
leads	O
to	O
inferior	O
results	O
,	O
similar	O
to	O
what	O
we	O
have	O
observed	O
in	O
the	O
above	O
subsection	O
for	O
RPN	S-Method
.	O

It	O
is	O
noteworthy	O
that	O
removing	O
top	O
-	O
down	O
connections	O
(	O
Table	O
2	O
(	O
d	O
)	O
)	O
significantly	O
degrades	O
the	O
accuracy	S-Metric
,	O
suggesting	O
that	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
suffers	O
from	O
using	O
the	O
low	O
-	O
level	O
features	O
at	O
the	O
high	O
-	O
resolution	O
maps	O
.	O

In	O
Table	O
2	O
(	O
f	O
)	O
,	O
we	O
adopt	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
on	O
the	O
single	O
finest	O
scale	O
feature	O
map	O
of	O
P	O
2	O
.	O

Its	O
result	O
(	O
33.4	O
AP	S-Metric
)	O
is	O
marginally	O
worse	O
than	O
that	O
of	O
using	O
all	O
pyramid	O
levels	O
(	O
33.9	O
AP	S-Metric
,	O
Table	O
2	O
(	O
c	O
)	O
)	O
.	O

We	O
argue	O
that	O
this	O
is	O
because	O
RoI	S-Method
pooling	O
is	O
a	O
warping	B-Method
-	I-Method
like	I-Method
operation	E-Method
,	O
which	O
is	O
less	O
sensitive	O
to	O
the	O
region	O
's	O
scales	O
.	O

Despite	O
the	O
good	O
accuracy	S-Metric
of	O
this	O
variant	O
,	O
it	O
is	O
based	O
on	O
the	O
RPN	S-Method
proposals	O
of	O
{	O
P	O
k	O
}	O
and	O
has	O
thus	O
already	O
benefited	O
from	O
the	O
pyramid	B-Method
representation	E-Method
.	O

section	O
:	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
(	O
on	O
consistent	O
proposals	O
)	O
In	O
the	O
above	O
we	O
used	O
a	O
fixed	O
set	O
of	O
proposals	O
to	O
investigate	O
the	O
detectors	O
.	O

But	O
in	O
a	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	I-Method
system	E-Method
[	O
reference	O
]	O
,	O
the	O
RPN	S-Method
and	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
must	O
use	O
the	O
same	O
network	O
backbone	O
in	O
order	O
to	O
make	O
feature	B-Task
sharing	E-Task
possible	O
.	O

Table	O
3	O
shows	O
the	O
comparisons	O
between	O
our	O
method	O
and	O
two	O
baselines	O
,	O
all	O
using	O
consistent	B-Method
backbone	I-Method
architectures	E-Method
for	O
RPN	S-Method
and	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
.	O

Table	O
3	O
(	O
a	O
)	O
shows	O
our	O
reproduction	O
of	O
the	O
baseline	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
system	O
as	O
described	O
in	O
[	O
reference	O
]	O
.	O

Under	O
controlled	O
settings	O
,	O
our	O
FPN	S-Method
(	O
Table	O
3	O
(	O
c	O
)	O
)	O
is	O
better	O
than	O
this	O
strong	O
baseline	O
by	O
2.3	O
points	O
AP	S-Metric
and	O
3.8	O
points	O
AP@0.5	O
.	O

Note	O
that	O
Table	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
are	O
baselines	O
that	O
are	O
much	O
stronger	O
than	O
the	O
baseline	O
provided	O
by	O
He	O
et	O
al	O
.	O

[	O
reference	O
]	O
in	O
Table	O
3	O
(	O
*	O
)	O
.	O

We	O
find	O
the	O
following	O
implementations	O
contribute	O
to	O
the	O
gap	O
:	O
(	O
i	O
)	O
We	O
use	O
an	O
image	O
scale	O
of	O
800	O
pixels	O
instead	O
of	O
600	O
in	O
[	O
reference	O
][	O
reference	O
]	O
;	O
(	O
ii	O
)	O
We	O
train	O
with	O
512	O
RoIs	O
per	O
image	O
which	O
accelerate	O
convergence	S-Task
,	O
in	O
contrast	O
to	O
64	O
RoIs	O
in	O
[	O
reference	O
][	O
reference	O
]	O
;	O
(	O
iii	O
)	O
We	O
use	O
5	O
scale	O
anchors	O
instead	O
of	O
4	O
in	O
[	O
reference	O
]	O
(	O
adding	O
32	O
2	O
)	O
;	O
(	O
iv	O
)	O
At	O
test	O
time	O
we	O
use	O
1000	O
proposals	O
per	O
image	O
instead	O
of	O
300	O
in	O
[	O
reference	O
]	O
.	O

So	O
comparing	O
with	O
He	O
et	O
al	O
.	O

's	O
ResNet	B-Method
-	I-Method
50	I-Method
Faster	I-Method
R	I-Method
-	I-Method
CNN	I-Method
baseline	E-Method
in	O
Table	O
3	O
(	O
*	O
)	O
,	O
our	O
method	O
improves	O
AP	S-Metric
by	O
7.6	O
points	O
and	O
AP@0.5	O
by	O
9.6	O
points	O
.	O

Sharing	O
features	O
.	O

In	O
the	O
above	O
,	O
for	O
simplicity	O
we	O
do	O
not	O
share	O
the	O
features	O
between	O
RPN	S-Method
and	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
.	O

In	O
Ta	O
-	O
Table	O
5	O
.	O

More	O
object	B-Task
detection	E-Task
results	O
using	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
and	O
our	O
FPNs	S-Method
,	O
evaluated	O
on	O
minival	S-Material
.	O

Sharing	O
features	O
increases	O
train	B-Metric
time	E-Metric
by	O
1.5×	O
(	O
using	O
4	B-Method
-	I-Method
step	I-Method
training	E-Method
[	O
reference	O
]	O
)	O
,	O
but	O
reduces	O
test	B-Metric
time	E-Metric
.	O

ble	O
5	O
,	O
we	O
evaluate	O
sharing	O
features	O
following	O
the	O
4	B-Method
-	I-Method
step	I-Method
training	E-Method
described	O
in	O
[	O
reference	O
]	O
.	O

Similar	O
to	O
[	O
reference	O
]	O
,	O
we	O
find	O
that	O
sharing	O
features	O
improves	O
accuracy	S-Metric
by	O
a	O
small	O
margin	O
.	O

Feature	B-Method
sharing	E-Method
also	O
reduces	O
the	O
testing	B-Metric
time	E-Metric
.	O

Running	B-Metric
time	E-Metric
.	O

With	O
feature	B-Method
sharing	E-Method
,	O
our	O
FPN	S-Method
-	O
based	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
system	O
has	O
inference	B-Metric
time	E-Metric
of	O
0.148	O
seconds	O
per	O
image	O
on	O
a	O
single	O
NVIDIA	O
M40	O
GPU	O
for	O
ResNet	B-Method
-	I-Method
50	E-Method
,	O
and	O
0.172	O
seconds	O
for	O
ResNet	B-Task
-	I-Task
101	E-Task
.	O

[	O
reference	O
]	O
As	O
a	O
comparison	O
,	O
the	O
single	O
-	O
scale	O
ResNet	B-Method
-	I-Method
50	E-Method
baseline	O
in	O
Table	O
3	O
(	O
a	O
)	O
runs	O
at	O
0.32	O
seconds	O
.	O

Our	O
method	O
introduces	O
small	O
extra	O
cost	O
by	O
the	O
extra	O
layers	O
in	O
the	O
FPN	S-Method
,	O
but	O
has	O
a	O
lighter	O
weight	O
head	O
.	O

Overall	O
our	O
system	O
is	O
faster	O
than	O
the	O
ResNet	O
-	O
based	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
counterpart	O
.	O

We	O
believe	O
the	O
efficiency	O
and	O
simplicity	O
of	O
our	O
method	O
will	O
benefit	O
future	O
research	O
and	O
applications	O
.	O

section	O
:	O
Comparing	O
with	O
COCO	S-Material
Competition	O
Winners	O
We	O
find	O
that	O
our	O
ResNet	B-Method
-	I-Method
101	I-Method
model	E-Method
in	O
Table	O
5	O
is	O
not	O
sufficiently	O
trained	O
with	O
the	O
default	B-Metric
learning	I-Metric
rate	I-Metric
schedule	E-Metric
.	O

So	O
we	O
increase	O
the	O
number	O
of	O
mini	O
-	O
batches	O
by	O
2×	O
at	O
each	O
learning	B-Metric
rate	E-Metric
when	O
training	O
the	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
step	O
.	O

This	O
increases	O
AP	S-Metric
on	O
minival	O
to	O
35.6	O
,	O
without	O
sharing	O
features	O
.	O

This	O
model	O
is	O
the	O
one	O
we	O
submitted	O
to	O
the	O
COCO	S-Material
detection	O
leaderboard	O
,	O
shown	O
in	O
Table	O
4	O
.	O

We	O
have	O
not	O
evaluated	O
its	O
feature	B-Method
-	I-Method
sharing	I-Method
version	E-Method
due	O
to	O
limited	O
time	O
,	O
which	O
should	O
be	O
slightly	O
better	O
as	O
implied	O
by	O
Table	O
5	O
.	O

Table	O
4	O
compares	O
our	O
method	O
with	O
the	O
single	O
-	O
model	O
results	O
of	O
the	O
COCO	S-Material
competition	O
winners	O
,	O
including	O
the	O
2016	O
winner	O
G	B-Method
-	I-Method
RMI	E-Method
and	O
the	O
2015	O
winner	O
Faster	O
R	B-Method
-	I-Method
CNN	I-Method
+++	E-Method
.	O

Without	O
adding	O
bells	O
and	O
whistles	O
,	O
our	O
single	O
-	O
model	O
entry	O
has	O
surpassed	O
these	O
strong	O
,	O
heavily	O
engineered	O
competitors	O
.	O

On	O
the	O
test	O
-	O
dev	O
set	O
,	O
our	O
method	O
increases	O
over	O
the	O
existing	O
best	O
results	O
by	O
0.5	O
points	O
of	O
AP	S-Metric
(	O
36.2	O
vs.	O
35.7	O
)	O
and	O
3.4	O
points	O
of	O
AP@0.5	O
(	O
59.1	O
vs.	O
55.7	O
)	O
.	O

It	O
is	O
worth	O
noting	O
that	O
our	O
method	O
does	O
not	O
rely	O
on	O
image	B-Method
pyramids	E-Method
and	O
only	O
uses	O
a	O
single	O
input	O
image	O
scale	O
,	O
but	O
still	O
has	O
outstanding	O
AP	S-Metric
on	O
small	O
-	O
scale	O
objects	O
.	O

This	O
could	O
only	O
be	O
achieved	O
by	O
highresolution	O
image	O
inputs	O
with	O
previous	O
methods	O
.	O

Moreover	O
,	O
our	O
method	O
does	O
not	O
exploit	O
many	O
popular	O
improvements	O
,	O
such	O
as	O
iterative	B-Method
regression	E-Method
[	O
reference	O
]	O
,	O
hard	B-Method
negative	I-Method
mining	E-Method
[	O
reference	O
]	O
,	O
context	B-Method
modeling	E-Method
[	O
reference	O
]	O
,	O
stronger	O
data	B-Method
augmentation	E-Method
[	O
reference	O
]	O
,	O
etc	O
.	O

These	O
improvements	O
are	O
complementary	O
to	O
FPNs	S-Method
and	O
should	O
boost	O
accuracy	S-Metric
further	O
.	O

Recently	O
,	O
FPN	S-Method
has	O
enabled	O
new	O
top	O
results	O
in	O
all	O
tracks	O
of	O
the	O
COCO	S-Material
competition	O
,	O
including	O
detection	S-Task
,	O
instance	B-Task
segmentation	E-Task
,	O
and	O
keypoint	B-Task
estimation	E-Task
.	O

See	O
[	O
reference	O
]	O
for	O
details	O
.	O

section	O
:	O
Extensions	O
:	O
Segmentation	B-Task
Proposals	E-Task
Our	O
method	O
is	O
a	O
generic	B-Method
pyramid	I-Method
representation	E-Method
and	O
can	O
be	O
used	O
in	O
applications	O
other	O
than	O
object	B-Task
detection	E-Task
.	O

In	O
this	O
section	O
we	O
use	O
FPNs	S-Method
to	O
generate	O
segmentation	B-Task
proposals	E-Task
,	O
following	O
the	O
DeepMask	O
/	O
SharpMask	S-Method
framework	O
[	O
reference	O
][	O
reference	O
]	O
.	O

DeepMask	O
/	O
SharpMask	S-Method
were	O
trained	O
on	O
image	O
crops	O
for	O
predicting	B-Task
instance	I-Task
segments	E-Task
and	O
object	B-Task
/	I-Task
non	I-Task
-	I-Task
object	I-Task
scores	E-Task
.	O

At	O
inference	O
time	O
,	O
these	O
models	O
are	O
run	O
convolutionally	S-Method
to	O
generate	O
dense	B-Task
proposals	E-Task
in	O
an	O
image	O
.	O

To	O
generate	O
segments	O
at	O
multiple	O
scales	O
,	O
image	O
pyramids	O
are	O
necessary	O
[	O
reference	O
][	O
reference	O
]	O
.	O

It	O
is	O
easy	O
to	O
adapt	O
FPN	S-Method
to	O
generate	O
mask	O
proposals	O
.	O

We	O
use	O
a	O
fully	B-Method
convolutional	I-Method
setup	E-Method
for	O
both	O
training	S-Task
and	O
inference	S-Task
.	O

We	O
construct	O
our	O
feature	B-Method
pyramid	E-Method
as	O
in	O
Sec	O
.	O

5.1	O
and	O
set	O
d	O
=	O
128	O
.	O

On	O
top	O
of	O
each	O
level	O
of	O
the	O
feature	O
pyramid	O
,	O
we	O
apply	O
a	O
small	O
5×5	B-Method
MLP	E-Method
to	O
predict	O
14×14	O
masks	O
and	O
object	O
scores	O
in	O
a	O
fully	B-Method
convolutional	I-Method
fashion	E-Method
,	O
see	O
Fig	O
.	O

4	O
.	O

Additionally	O
,	O
motivated	O
by	O
the	O
use	O
of	O
2	O
scales	O
per	O
octave	O
in	O
the	O
image	O
pyramid	O
of	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
use	O
a	O
second	O
MLP	S-Method
of	O
input	O
size	O
7×7	O
to	O
handle	O
half	O
octaves	O
.	O

The	O
two	O
MLPs	S-Method
play	O
a	O
similar	O
role	O
as	O
anchors	O
in	O
RPN	S-Method
.	O

The	O
architecture	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
;	O
full	O
implementation	O
details	O
are	O
given	O
in	O
the	O
appendix	O
.	O

section	O
:	O
Segmentation	O
Proposal	O
Results	O
Results	O
are	O
shown	O
in	O
Table	O
6	O
.	O

We	O
report	O
segment	O
AR	S-Metric
and	O
segment	O
AR	S-Metric
on	O
small	O
,	O
medium	O
,	O
and	O
large	O
objects	O
,	O
always	O
for	O
1000	O
proposals	O
.	O

Our	O
baseline	O
FPN	S-Method
model	O
with	O
a	O
single	O
5×5	B-Method
MLP	E-Method
achieves	O
an	O
AR	S-Metric
of	O
43.4	O
.	O

Switching	O
to	O
a	O
slightly	O
larger	O
7×7	O
MLP	S-Method
leaves	O
accuracy	S-Metric
largely	O
unchanged	O
.	O

Using	O
both	O
MLPs	S-Method
together	O
increases	O
accuracy	S-Metric
to	O
45.7	O
AR	S-Metric
.	O

Increasing	O
mask	O
output	O
size	O
from	O
14×14	O
to	O
28×28	O
increases	O
AR	S-Metric
another	O
point	O
(	O
larger	O
sizes	O
begin	O
to	O
degrade	O
accuracy	S-Metric
)	O
.	O

Finally	O
,	O
doubling	O
the	O
training	O
iterations	O
increases	O
AR	S-Metric
to	O
48.1	O
.	O

We	O
also	O
report	O
comparisons	O
to	O
DeepMask	S-Method
[	O
reference	O
]	O
,	O
SharpMask	S-Method
[	O
reference	O
]	O
,	O
and	O
InstanceFCN	S-Method
[	O
reference	O
]	O
,	O
the	O
previous	O
state	O
of	O
the	O
art	O
methods	O
in	O
mask	B-Task
proposal	I-Task
generation	E-Task
.	O

We	O
outperform	O
the	O
accuracy	S-Metric
of	O
these	O
approaches	O
by	O
over	O
8.3	O
points	O
AR	S-Metric
.	O

In	O
particular	O
,	O
we	O
nearly	O
double	O
the	O
accuracy	S-Metric
on	O
small	O
objects	O
.	O

Existing	O
mask	B-Method
proposal	I-Method
methods	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
are	O
based	O
on	O
densely	O
sampled	O
image	O
pyramids	O
(	O
e.g.	O
,	O
scaled	O
by	O
2	O
{	O
−2:0.5:1	O
}	O
in	O
[	O
reference	O
][	O
reference	O
]	O
)	O
,	O
making	O
them	O
computationally	O
expensive	O
.	O

Our	O
approach	O
,	O
based	O
on	O
FPNs	S-Method
,	O
is	O
substantially	O
faster	O
(	O
our	O
models	O
run	O
at	O
6	O
to	O
7	O
FPS	S-Metric
)	O
.	O

These	O
results	O
demonstrate	O
that	O
our	O
model	O
is	O
a	O
generic	B-Method
feature	I-Method
extractor	E-Method
and	O
can	O
replace	O
image	B-Method
pyramids	E-Method
for	O
other	O
multi	B-Task
-	I-Task
scale	I-Task
detection	I-Task
problems	E-Task
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
clean	O
and	O
simple	O
framework	O
for	O
building	O
feature	B-Task
pyramids	E-Task
inside	O
ConvNets	S-Method
.	O

Our	O
method	O
shows	O
significant	O
improvements	O
over	O
several	O
strong	O
baselines	O
and	O
competition	O
winners	O
.	O

Thus	O
,	O
it	O
provides	O
a	O
practical	O
solution	O
for	O
research	B-Task
and	I-Task
applications	I-Task
of	I-Task
feature	I-Task
pyramids	E-Task
,	O
without	O
the	O
need	O
of	O
computing	O
image	O
pyramids	O
.	O

Finally	O
,	O
our	O
study	O
suggests	O
that	O
despite	O
the	O
strong	O
representational	O
power	O
of	O
deep	B-Method
ConvNets	E-Method
and	O
their	O
implicit	O
robustness	O
to	O
scale	O
variation	O
,	O
it	O
is	O
still	O
critical	O
to	O
explicitly	O
address	O
multiscale	B-Task
problems	E-Task
using	O
pyramid	B-Method
representations	E-Method
.	O

section	O
:	O
A.	O
Implementation	O
of	O
Segmentation	B-Task
Proposals	E-Task
We	O
use	O
our	O
feature	B-Method
pyramid	I-Method
networks	E-Method
to	O
efficiently	O
generate	O
object	B-Task
segment	I-Task
proposals	E-Task
,	O
adopting	O
an	O
image	B-Method
-	I-Method
centric	I-Method
training	I-Method
strategy	E-Method
popular	O
for	O
object	B-Task
detection	E-Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Our	O
FPN	S-Method
mask	O
generation	O
model	O
inherits	O
many	O
of	O
the	O
ideas	O
and	O
motivations	O
from	O
DeepMask	O
/	O
SharpMask	S-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
in	O
contrast	O
to	O
these	O
models	O
,	O
which	O
were	O
trained	O
on	O
image	O
crops	O
and	O
used	O
a	O
densely	O
sampled	O
image	O
pyramid	O
for	O
inference	S-Task
,	O
we	O
perform	O
fully	B-Method
-	I-Method
convolutional	I-Method
training	E-Method
for	O
mask	B-Task
prediction	E-Task
on	O
a	O
feature	O
pyramid	O
.	O

While	O
this	O
requires	O
changing	O
many	O
of	O
the	O
specifics	O
,	O
our	O
implementation	O
remains	O
similar	O
in	O
spirit	O
to	O
DeepMask	S-Method
.	O

Specifically	O
,	O
to	O
define	O
the	O
label	O
of	O
a	O
mask	O
instance	O
at	O
each	O
sliding	O
window	O
,	O
we	O
think	O
of	O
this	O
window	O
as	O
being	O
a	O
crop	O
on	O
the	O
input	O
image	O
,	O
allowing	O
us	O
to	O
inherit	O
definitions	O
of	O
positives	O
/	O
negatives	O
from	O
DeepMask	S-Method
.	O

We	O
give	O
more	O
details	O
next	O
,	O
see	O
also	O
Fig	O
.	O

4	O
for	O
a	O
visualization	O
.	O

We	O
construct	O
the	O
feature	O
pyramid	O
with	O
P	O
2−6	O
using	O
the	O
same	O
architecture	O
as	O
described	O
in	O
Sec	O
.	O

5.1	O
.	O

We	O
set	O
d	O
=	O
128	O
.	O

Each	O
level	O
of	O
our	O
feature	B-Method
pyramid	E-Method
is	O
used	O
for	O
predicting	O
masks	O
at	O
a	O
different	O
scale	O
.	O

As	O
in	O
DeepMask	S-Method
,	O
we	O
define	O
the	O
scale	O
of	O
a	O
mask	O
as	O
the	O
max	O
of	O
its	O
width	O
and	O
height	O
.	O

Masks	O
with	O
scales	O
of	O
{	O
32	O
,	O
64	O
,	O
128	O
,	O
256	O
,	O
512	O
}	O
pixels	O
map	O
to	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
,	O
respectively	O
,	O
and	O
are	O
handled	O
by	O
a	O
5×5	O
MLP	S-Method
.	O

As	O
DeepMask	S-Method
uses	O
a	O
pyramid	O
with	O
half	O
octaves	O
,	O
we	O
use	O
a	O
second	O
slightly	O
larger	O
MLP	S-Method
of	O
size	O
7×7	O
(	O
7	O
≈	O
5	O
√	O
2	O
)	O
to	O
handle	O
half	O
-	O
octaves	O
in	O
our	O
model	O
(	O
e.g.	O
,	O
a	O
128	O
√	O
2	O
scale	O
mask	O
is	O
predicted	O
by	O
the	O
7×7	O
MLP	S-Method
on	O
P	O
4	O
)	O
.	O

Objects	O
at	O
intermediate	O
scales	O
are	O
mapped	O
to	O
the	O
nearest	O
scale	O
in	O
log	O
space	O
.	O

As	O
the	O
MLP	S-Method
must	O
predict	O
objects	O
at	O
a	O
range	O
of	O
scales	O
for	O
each	O
pyramid	O
level	O
(	O
specifically	O
a	O
half	O
octave	O
range	O
)	O
,	O
some	O
padding	O
must	O
be	O
given	O
around	O
the	O
canonical	O
object	O
size	O
.	O

We	O
use	O
25	O
%	O
padding	S-Method
.	O

This	O
means	O
that	O
the	O
mask	O
output	O
over	O
{	O
P	O
2	O
,	O
P	O
3	O
,	O
P	O
4	O
,	O
P	O
5	O
,	O
P	O
6	O
}	O
maps	O
to	O
{	O
40	O
,	O
80	O
,	O
160	O
,	O
320	O
,	O
640	O
}	O
sized	O
image	O
regions	O
for	O
the	O
5×5	O
MLP	S-Method
(	O
and	O
to	O
√	O
2	O
larger	O
corresponding	O
sizes	O
for	O
the	O
7×7	B-Method
MLP	E-Method
)	O
.	O

Each	O
spatial	O
position	O
in	O
the	O
feature	O
map	O
is	O
used	O
to	O
predict	O
a	O
mask	O
at	O
a	O
different	O
location	O
.	O

Specifically	O
,	O
at	O
scale	O
P	O
k	O
,	O
each	O
spatial	O
position	O
in	O
the	O
feature	O
map	O
is	O
used	O
to	O
predict	O
the	O
mask	O
whose	O
center	O
falls	O
within	O
2	O
k	O
pixels	O
of	O
that	O
location	O
(	O
corresponding	O
to	O
±1	O
cell	O
offset	O
in	O
the	O
feature	O
map	O
)	O
.	O

If	O
no	O
object	O
center	O
falls	O
within	O
this	O
range	O
,	O
the	O
location	O
is	O
considered	O
a	O
negative	O
,	O
and	O
,	O
as	O
in	O
DeepMask	S-Method
,	O
is	O
used	O
only	O
for	O
training	O
the	O
score	O
branch	O
and	O
not	O
the	O
mask	O
branch	O
.	O

The	O
MLP	S-Method
we	O
use	O
for	O
predicting	O
the	O
mask	B-Metric
and	I-Metric
score	E-Metric
is	O
fairly	O
simple	O
.	O

We	O
apply	O
a	O
5×5	B-Method
kernel	E-Method
with	O
512	O
outputs	O
,	O
followed	O
by	O
sibling	O
fully	B-Method
connected	I-Method
layers	E-Method
to	O
predict	O
a	O
14×14	O
mask	O
(	O
14	O
2	O
outputs	O
)	O
and	O
object	O
score	O
(	O
1	O
output	O
)	O
.	O

The	O
model	O
is	O
implemented	O
in	O
a	O
fully	B-Method
convolutional	I-Method
manner	E-Method
(	O
using	O
1×1	B-Method
convolutions	E-Method
in	O
place	O
of	O
fully	B-Method
connected	I-Method
layers	E-Method
)	O
.	O

The	O
7×7	B-Method
MLP	E-Method
for	O
handling	O
objects	O
at	O
half	O
octave	O
scales	O
is	O
identical	O
to	O
the	O
5×5	O
MLP	S-Method
except	O
for	O
its	O
larger	O
input	O
region	O
.	O

During	O
training	S-Task
,	O
we	O
randomly	O
sample	O
2048	O
examples	O
per	O
mini	O
-	O
batch	O
(	O
128	O
examples	O
per	O
image	O
from	O
16	O
images	O
)	O
with	O
a	O
positive	O
/	O
negative	O
sampling	O
ratio	O
of	O
1:3	O
.	O

The	O
mask	O
loss	O
is	O
given	O
10×	O
higher	O
weight	O
than	O
the	O
score	B-Metric
loss	E-Metric
.	O

This	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
on	O
8	O
GPUs	S-Method
using	O
synchronized	B-Method
SGD	E-Method
(	O
2	O
images	O
per	O
GPU	O
)	O
.	O

We	O
start	O
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
0.03	O
and	O
train	O
for	O
80k	O
mini	O
-	O
batches	O
,	O
dividing	O
the	O
learning	B-Metric
rate	E-Metric
by	O
10	O
after	O
60k	O
mini	O
-	O
batches	O
.	O

The	O
image	O
scale	O
is	O
set	O
to	O
800	O
pixels	O
during	O
training	O
and	O
testing	O
(	O
we	O
do	O
not	O
use	O
scale	O
jitter	O
)	O
.	O

During	O
inference	S-Task
our	O
fully	B-Method
-	I-Method
convolutional	I-Method
model	E-Method
predicts	O
scores	O
at	O
all	O
positions	O
and	O
scales	O
and	O
masks	O
at	O
the	O
1000	O
highest	O
scoring	O
locations	O
.	O

We	O
do	O
not	O
perform	O
any	O
non	O
-	O
maximum	B-Method
suppression	E-Method
or	O
post	B-Task
-	I-Task
processing	E-Task
.	O

section	O
:	O
document	O
:	O
Improved	O
Language	B-Task
Modeling	E-Task
by	O
Decoding	O
the	O
Past	O
Highly	O
regularized	O
LSTMs	S-Method
achieve	O
impressive	O
results	O
on	O
several	O
benchmark	O
datasets	O
in	O
language	B-Task
modeling	E-Task
.	O

We	O
propose	O
a	O
new	O
regularization	B-Method
method	E-Method
based	O
on	O
decoding	O
the	O
last	O
token	O
in	O
the	O
context	O
using	O
the	O
predicted	O
distribution	O
of	O
the	O
next	O
token	O
.	O

This	O
biases	O
the	O
model	O
towards	O
retaining	O
more	O
contextual	O
information	O
,	O
in	O
turn	O
improving	O
its	O
ability	O
to	O
predict	O
the	O
next	O
token	O
.	O

With	O
negligible	O
overhead	O
in	O
the	O
number	O
of	O
parameters	O
and	O
training	S-Material
time	O
,	O
our	O
Past	B-Method
Decode	I-Method
Regularization	E-Method
(	O
PDR	S-Method
)	O
method	O
achieves	O
a	O
word	B-Metric
level	I-Metric
perplexity	E-Metric
of	O
55.6	O
on	O
the	O
Penn	B-Material
Treebank	E-Material
and	O
63.5	O
on	O
the	O
WikiText	B-Material
-	I-Material
2	I-Material
datasets	E-Material
using	O
a	O
single	O
softmax	S-Method
.	O

We	O
also	O
show	O
gains	O
by	O
using	O
PDR	S-Method
in	O
combination	O
with	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
,	O
achieving	O
a	O
word	B-Metric
level	I-Metric
perplexity	E-Metric
of	O
53.8	O
and	O
60.5	O
on	O
these	O
datasets	O
.	O

In	O
addition	O
,	O
our	O
method	O
achieves	O
1.169	O
bits	B-Metric
-	I-Metric
per	I-Metric
-	I-Metric
character	E-Metric
on	O
the	O
Penn	B-Material
Treebank	I-Material
Character	I-Material
dataset	E-Material
for	O
character	B-Task
level	I-Task
language	I-Task
modeling	E-Task
.	O

These	O
results	O
constitute	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
their	O
respective	O
settings	O
.	O

tickpos	O
=	O
left	O
colorbrewer	O
section	O
:	O
Introduction	O
Language	B-Task
modeling	E-Task
is	O
a	O
fundamental	O
task	O
in	O
natural	B-Task
language	I-Task
processing	E-Task
.	O

Given	O
a	O
sequence	O
of	O
tokens	O
,	O
its	O
joint	O
probability	O
distribution	O
can	O
be	O
modeled	O
using	O
the	O
auto	B-Method
-	I-Method
regressive	I-Method
conditional	I-Method
factorization	E-Method
.	O

This	O
leads	O
to	O
a	O
convenient	O
formulation	O
where	O
a	O
language	B-Method
model	E-Method
has	O
to	O
predict	O
the	O
next	O
token	O
given	O
a	O
sequence	O
of	O
tokens	O
as	O
context	O
.	O

Recurrent	B-Method
neural	I-Method
networks	E-Method
are	O
an	O
effective	O
way	O
to	O
compute	O
distributed	B-Task
representations	I-Task
of	I-Task
the	I-Task
context	E-Task
by	O
sequentially	O
operating	O
on	O
the	O
embeddings	O
of	O
the	O
tokens	O
.	O

These	O
representations	O
can	O
then	O
be	O
used	O
to	O
predict	O
the	O
next	O
token	O
as	O
a	O
probability	O
distribution	O
over	O
a	O
fixed	O
vocabulary	O
using	O
a	O
linear	O
decoder	S-Method
followed	O
by	O
Softmax	S-Method
.	O

Starting	O
from	O
the	O
work	O
of	O
,	O
there	O
has	O
been	O
a	O
long	O
list	O
of	O
works	O
that	O
seek	O
to	O
improve	O
language	B-Task
modeling	E-Task
performance	O
using	O
more	O
sophisticated	O
recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
(	O
)	O
.	O

However	O
,	O
in	O
more	O
recent	O
work	O
vanilla	O
LSTMs	S-Method
(	O
)	O
with	O
relatively	O
large	O
number	O
of	O
parameters	O
have	O
been	O
shown	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
several	O
standard	O
benchmark	O
datasets	O
both	O
in	O
word	B-Metric
-	I-Metric
level	E-Metric
and	O
character	B-Task
-	I-Task
level	I-Task
perplexity	E-Task
(	O
)	O
.	O

A	O
key	O
component	O
in	O
these	O
models	O
is	O
the	O
use	O
of	O
several	O
forms	O
of	O
regularization	S-Method
e.g.	O
variational	B-Method
dropout	E-Method
on	O
the	O
token	O
embeddings	O
(	O
)	O
,	O
dropout	S-Method
on	O
the	O
hidden	O
-	O
to	O
-	O
hidden	O
weights	O
in	O
the	O
LSTM	S-Method
(	O
)	O
,	O
norm	B-Method
regularization	E-Method
on	O
the	O
outputs	O
of	O
the	O
LSTM	S-Method
and	O
classical	B-Method
dropout	E-Method
(	O
)	O
.	O

By	O
carefully	O
tuning	O
the	O
hyperparameters	O
associated	O
with	O
these	O
regularizers	S-Method
combined	O
with	O
optimization	B-Method
algorithms	E-Method
like	O
NT	B-Method
-	I-Method
ASGD	E-Method
(	O
a	O
variant	O
of	O
the	O
Averaged	O
SGD	S-Method
)	O
,	O
it	O
is	O
possible	O
to	O
achieve	O
very	O
good	O
performance	O
.	O

Each	O
of	O
these	O
regularizations	O
address	O
different	O
parts	O
of	O
the	O
LSTM	B-Method
model	E-Method
and	O
are	O
general	O
techniques	O
that	O
could	O
be	O
applied	O
to	O
any	O
other	O
sequence	B-Task
modeling	I-Task
problem	E-Task
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
regularization	B-Method
technique	E-Method
that	O
is	O
specific	O
to	O
language	B-Task
modeling	E-Task
.	O

One	O
unique	O
aspect	O
of	O
language	B-Method
modeling	E-Method
using	O
LSTMs	S-Method
(	O
or	O
any	O
RNN	S-Method
)	O
is	O
that	O
at	O
each	O
time	O
step	O
,	O
the	O
model	O
takes	O
as	O
input	O
a	O
particular	O
token	O
from	O
a	O
vocabulary	O
and	O
using	O
the	O
hidden	O
state	O
of	O
the	O
LSTM	S-Method
(	O
which	O
encodes	O
the	O
context	O
till	O
)	O
predicts	O
a	O
probability	O
distribution	O
on	O
the	O
next	O
token	O
over	O
the	O
same	O
vocabulary	O
as	O
output	O
.	O

Since	O
can	O
be	O
mapped	O
to	O
a	O
trivial	O
probability	O
distribution	O
over	O
,	O
this	O
operation	O
can	O
be	O
interpreted	O
as	O
transforming	O
distributions	O
over	O
(	O
)	O
.	O

Clearly	O
,	O
the	O
output	O
distribution	O
is	O
dependent	O
on	O
and	O
is	O
a	O
function	O
of	O
and	O
the	O
context	O
further	O
in	O
the	O
past	O
and	O
encodes	O
information	O
about	O
it	O
.	O

We	O
ask	O
the	O
following	O
question	O
–	O
How	O
much	O
information	O
is	O
it	O
possible	O
to	O
decode	O
about	O
the	O
input	O
distribution	O
(	O
and	O
hence	O
)	O
from	O
the	O
output	O
distribution	O
?	O
In	O
general	O
,	O
it	O
is	O
impossible	O
to	O
decode	O
unambiguously	O
.	O

Even	O
if	O
the	O
language	B-Method
model	E-Method
is	O
perfect	O
and	O
correctly	O
predicts	O
with	O
probability	O
1	O
,	O
there	O
could	O
be	O
many	O
tokens	O
preceding	O
it	O
.	O

However	O
,	O
in	O
this	O
case	O
the	O
number	O
of	O
possibilities	O
for	O
will	O
be	O
limited	O
,	O
as	O
dictated	O
by	O
the	O
bigram	O
statistics	O
of	O
the	O
corpus	O
and	O
the	O
language	O
in	O
general	O
.	O

We	O
argue	O
that	O
biasing	O
the	O
language	B-Method
model	E-Method
such	O
that	O
it	O
is	O
possible	O
to	O
decode	O
more	O
information	O
about	O
the	O
past	O
tokens	O
from	O
the	O
predicted	O
next	O
token	O
distribution	O
is	O
beneficial	O
.	O

We	O
incorporate	O
this	O
intuition	O
into	O
a	O
regularization	B-Method
term	E-Method
in	O
the	O
loss	O
function	O
of	O
the	O
language	B-Method
model	E-Method
.	O

The	O
symmetry	O
in	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
language	B-Method
model	E-Method
at	O
each	O
step	O
lends	O
itself	O
to	O
a	O
simple	O
decoding	B-Method
operation	E-Method
.	O

It	O
can	O
be	O
cast	O
as	O
a	O
(	O
pseudo	B-Task
)	I-Task
language	I-Task
modeling	I-Task
problem	E-Task
in	O
“	O
reverse	B-Task
”	E-Task
,	O
where	O
the	O
future	B-Task
prediction	E-Task
acts	O
as	O
the	O
input	O
and	O
the	O
last	O
token	O
acts	O
as	O
the	O
target	O
of	O
prediction	S-Task
.	O

The	O
token	O
embedding	O
matrix	O
and	O
weights	O
of	O
the	O
linear	O
decoder	S-Method
of	O
the	O
main	B-Method
language	I-Method
model	E-Method
can	O
be	O
reused	O
in	O
the	O
past	O
decoding	B-Task
operation	E-Task
.	O

We	O
only	O
need	O
a	O
few	O
extra	O
parameters	O
to	O
model	O
the	O
nonlinear	O
transformation	O
performed	O
by	O
the	O
LSTM	S-Method
,	O
which	O
we	O
do	O
by	O
using	O
a	O
simple	O
stateless	B-Method
layer	E-Method
.	O

We	O
compute	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
between	O
the	O
decoded	O
distribution	O
for	O
the	O
past	O
token	O
and	O
and	O
add	O
it	O
to	O
the	O
main	O
loss	O
function	O
after	O
suitable	O
weighting	O
.	O

The	O
extra	O
parameters	O
used	O
in	O
the	O
past	O
decoding	O
are	O
discarded	O
during	O
inference	O
time	O
.	O

We	O
call	O
our	O
method	O
Past	B-Method
Decode	I-Method
Regularization	E-Method
or	O
PDR	S-Method
for	O
short	O
.	O

We	O
conduct	O
extensive	O
experiments	O
on	O
four	O
benchmark	O
datasets	O
for	O
word	B-Task
level	I-Task
and	I-Task
character	I-Task
level	I-Task
language	I-Task
modeling	E-Task
by	O
combining	O
PDR	S-Method
with	O
existing	O
LSTM	S-Method
based	O
language	O
models	O
and	O
achieve	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
three	O
of	O
them	O
.	O

section	O
:	O
Past	B-Method
Decode	I-Method
Regularization	E-Method
(	O
PDR	S-Method
)	O
Let	O
be	O
a	O
sequence	O
of	O
tokens	O
.	O

In	O
this	O
paper	O
,	O
we	O
will	O
experiment	O
with	O
both	O
word	B-Method
level	I-Method
and	I-Method
character	I-Method
level	I-Method
language	I-Method
modeling	E-Method
.	O

Therefore	O
,	O
tokens	O
can	O
be	O
either	O
words	O
or	O
characters	O
.	O

The	O
joint	O
probability	O
factorizes	O
into	O
Let	O
denote	O
the	O
context	O
available	O
to	O
the	O
language	B-Method
model	E-Method
for	O
.	O

Let	O
denote	O
the	O
vocabulary	O
of	O
tokens	O
,	O
each	O
of	O
which	O
is	O
embedded	O
into	O
a	O
vector	O
of	O
dimension	O
.	O

Let	O
denote	O
the	O
token	O
embedding	O
matrix	O
of	O
dimension	O
and	O
denote	O
the	O
embedding	O
of	O
.	O

An	O
LSTM	S-Method
computes	O
a	O
distributed	B-Method
representation	E-Method
of	O
in	O
the	O
form	O
of	O
its	O
hidden	O
state	O
,	O
which	O
we	O
assume	O
has	O
dimension	O
as	O
well	O
.	O

The	O
probability	O
that	O
the	O
next	O
token	O
is	O
can	O
then	O
be	O
calculated	O
using	O
a	O
linear	O
decoder	S-Method
followed	O
by	O
a	O
Softmax	B-Method
layer	E-Method
as	O
where	O
is	O
the	O
entry	O
corresponding	O
to	O
in	O
a	O
bias	O
vector	O
of	O
dimension	O
and	O
represents	O
projection	O
onto	O
.	O

Here	O
we	O
assume	O
that	O
the	O
weights	O
of	O
the	O
decoder	S-Method
are	O
tied	O
with	O
the	O
token	B-Method
embedding	I-Method
matrix	E-Method
(	O
)	O
.	O

To	O
optimize	O
the	O
parameters	O
of	O
the	O
language	B-Method
model	E-Method
,	O
the	O
loss	O
function	O
to	O
be	O
minimized	O
during	O
training	S-Material
is	O
set	O
as	O
the	O
cross	O
-	O
entropy	O
between	O
the	O
predicted	O
distribution	O
and	O
the	O
actual	O
token	O
.	O

Note	O
that	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
when	O
applied	O
to	O
all	O
produces	O
a	O
vector	O
,	O
encapsulating	O
the	O
prediction	O
the	O
language	B-Method
model	E-Method
has	O
about	O
the	O
next	O
token	O
.	O

Since	O
this	O
is	O
dependent	O
on	O
and	O
conditioned	O
on	O
,	O
clearly	O
encodes	O
information	O
about	O
it	O
;	O
in	O
particular	O
about	O
the	O
last	O
token	O
in	O
.	O

In	O
turn	O
,	O
it	O
should	O
be	O
possible	O
to	O
infer	O
or	O
decode	O
some	O
limited	O
information	O
about	O
from	O
.	O

We	O
argue	O
that	O
by	O
biasing	O
the	O
model	O
to	O
be	O
more	O
accurate	O
in	O
recalling	O
information	O
about	O
past	O
tokens	O
,	O
we	O
can	O
help	O
it	O
in	O
predicting	O
the	O
next	O
token	O
better	O
.	O

To	O
this	O
end	O
,	O
we	O
define	O
the	O
following	O
decoding	B-Method
operation	E-Method
to	O
compute	O
a	O
probability	O
distribution	O
over	O
as	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

Here	O
is	O
a	O
non	B-Method
-	I-Method
linear	I-Method
function	E-Method
that	O
maps	O
vectors	O
in	O
to	O
vectors	O
in	O
and	O
is	O
a	O
bias	O
vector	O
of	O
dimension	O
,	O
together	O
with	O
parameters	O
.	O

In	O
effect	O
,	O
we	O
are	O
decoding	O
the	O
past	O
–	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

This	O
produces	O
a	O
vector	O
of	O
dimension	O
.	O

The	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
with	O
respect	O
to	O
the	O
actual	O
last	O
token	O
can	O
then	O
be	O
computed	O
as	O
Here	O
stands	O
for	O
Past	B-Method
Decode	I-Method
Regularization	E-Method
.	O

captures	O
the	O
extent	O
to	O
which	O
the	O
decoded	O
distribution	O
of	O
tokens	O
differs	O
from	O
the	O
actual	O
tokens	O
in	O
the	O
context	O
.	O

Note	O
the	O
symmetry	O
between	O
Eqs	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
“	O
input	O
”	O
in	O
the	O
latter	O
case	O
is	O
and	O
the	O
“	O
context	O
”	O
is	O
provided	O
by	O
a	O
nonlinear	B-Method
transformation	E-Method
of	O
.	O

Different	O
from	O
the	O
former	O
,	O
the	O
context	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
does	O
not	O
preserve	O
any	O
state	O
information	O
across	O
time	O
steps	O
as	O
we	O
want	O
to	O
decode	O
only	O
using	O
.	O

The	O
term	O
can	O
be	O
interpreted	O
as	O
a	O
“	O
soft	B-Method
”	I-Method
token	I-Method
embedding	I-Method
lookup	E-Method
,	O
where	O
the	O
token	O
vector	O
is	O
a	O
probability	O
distribution	O
instead	O
of	O
a	O
unit	O
vector	O
.	O

We	O
add	O
to	O
the	O
loss	O
function	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
as	O
a	O
regularization	O
term	O
,	O
where	O
is	O
a	O
positive	O
weighting	O
coefficient	O
,	O
to	O
construct	O
the	O
following	O
new	O
loss	B-Method
function	E-Method
for	O
the	O
language	B-Method
model	E-Method
.	O

Thus	O
equivalently	O
PDR	S-Method
can	O
also	O
be	O
viewed	O
as	O
a	O
method	O
of	O
defining	O
an	O
augmented	B-Method
loss	I-Method
function	E-Method
for	O
language	B-Task
modeling	E-Task
.	O

The	O
choice	O
of	O
dictates	O
the	O
degree	O
to	O
which	O
we	O
want	O
the	O
language	B-Method
model	E-Method
to	O
incorporate	O
our	O
inductive	O
bias	O
i.e.	O
decodability	O
of	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

If	O
it	O
is	O
too	O
large	O
,	O
the	O
model	O
will	O
fail	O
to	O
predict	O
the	O
next	O
token	O
,	O
which	O
is	O
its	O
primary	O
task	O
.	O

If	O
it	O
is	O
zero	O
or	O
too	O
small	O
,	O
the	O
model	O
will	O
retain	O
less	O
information	O
about	O
the	O
last	O
token	O
which	O
hampers	O
its	O
predictive	O
performance	O
.	O

In	O
practice	O
,	O
we	O
choose	O
by	O
a	O
search	O
based	O
on	O
validation	B-Metric
set	I-Metric
performance	E-Metric
.	O

Note	O
that	O
the	O
trainable	O
parameters	O
associated	O
with	O
PDR	S-Method
are	O
used	O
only	O
during	O
training	S-Material
to	O
bias	O
the	O
language	B-Method
model	E-Method
and	O
are	O
not	O
used	O
at	O
inference	B-Task
time	E-Task
.	O

This	O
also	O
means	O
that	O
it	O
is	O
important	O
to	O
control	O
the	O
complexity	O
of	O
the	O
nonlinear	O
function	O
so	O
as	O
not	O
to	O
overly	O
bias	O
the	O
training	S-Material
.	O

As	O
a	O
simple	O
choice	O
,	O
we	O
use	O
a	O
single	O
fully	B-Method
connected	I-Method
layer	I-Method
of	I-Method
size	E-Method
followed	O
by	O
a	O
Tanh	O
nonlinearity	O
as	O
.	O

This	O
introduces	O
few	O
extra	O
parameters	O
and	O
a	O
small	O
increase	O
in	O
training	S-Material
time	O
as	O
compared	O
to	O
a	O
model	O
not	O
using	O
PDR	S-Method
.	O

section	O
:	O
Experiments	O
We	O
present	O
extensive	O
experimental	O
results	O
to	O
show	O
the	O
efficacy	O
of	O
using	O
PDR	S-Method
for	O
language	B-Task
modeling	E-Task
on	O
four	O
standard	O
benchmark	O
datasets	O
–	O
two	O
each	O
for	O
word	B-Task
level	I-Task
and	I-Task
character	I-Task
level	I-Task
language	I-Task
modeling	E-Task
.	O

For	O
the	O
former	O
,	O
we	O
evaluate	O
our	O
method	O
on	O
the	O
Penn	B-Material
Treebank	E-Material
(	O
PTB	S-Material
)	O
(	O
)	O
and	O
the	O
WikiText	B-Material
-	I-Material
2	E-Material
(	O
WT2	S-Material
)	O
(	O
)	O
datasets	O
.	O

For	O
the	O
latter	O
,	O
we	O
use	O
the	O
Penn	B-Material
Treebank	I-Material
Character	E-Material
(	O
PTBC	S-Material
)	O
(	O
)	O
and	O
the	O
Hutter	B-Material
Prize	I-Material
Wikipedia	I-Material
Prize	E-Material
(	O
)	O
(	O
also	O
known	O
as	O
Enwik8	S-Material
)	O
datasets	O
.	O

Key	O
statistics	O
for	O
these	O
datasets	O
is	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
mentioned	O
in	O
the	O
introduction	O
,	O
some	O
of	O
the	O
best	O
existing	O
results	O
on	O
these	O
datasets	O
are	O
obtained	O
by	O
using	O
extensive	O
regularization	B-Method
techniques	E-Method
on	O
relatively	O
large	O
LSTMs	S-Method
(	O
)	O
.	O

We	O
apply	O
our	O
regularization	B-Method
technique	E-Method
to	O
these	O
models	O
,	O
the	O
so	O
called	O
AWD	B-Method
-	I-Method
LSTM	E-Method
.	O

We	O
consider	O
two	O
versions	O
of	O
the	O
model	O
–	O
one	O
with	O
a	O
single	O
softmax	B-Method
(	I-Method
AWD	I-Method
-	I-Method
LSTM	E-Method
)	O
and	O
one	O
with	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
(	O
AWD	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
MoS	E-Method
)	O
.	O

The	O
PDR	S-Method
regularization	O
term	O
is	O
computed	O
according	O
to	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

We	O
call	O
our	O
model	B-Method
AWD	I-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	E-Method
when	O
using	O
a	O
single	O
softmax	S-Method
and	O
AWD	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
MoS	I-Method
+	I-Method
PDR	E-Method
when	O
using	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
.	O

We	O
largely	O
follow	O
the	O
experimental	O
procedure	O
of	O
the	O
original	O
models	O
and	O
incorporate	O
their	O
dropouts	S-Method
and	O
regularizations	S-Method
in	O
our	O
experiments	O
.	O

The	O
relative	O
contribution	O
of	O
these	O
existing	O
regularizations	S-Method
and	O
PDR	S-Method
will	O
be	O
analyzed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

There	O
are	O
7	O
hyperparameters	O
associated	O
with	O
the	O
regularizations	S-Method
used	O
in	O
AWD	B-Method
-	I-Method
LSTM	E-Method
(	O
and	O
one	O
extra	O
with	O
MoS	S-Method
)	O
.	O

PDR	S-Method
also	O
has	O
an	O
associated	O
weighting	O
coefficient	O
.	O

For	O
our	O
experiments	O
,	O
we	O
set	O
which	O
was	O
determined	O
by	O
a	O
coarse	B-Method
search	E-Method
on	O
the	O
PTB	S-Material
and	O
WT2	S-Material
validation	O
sets	O
.	O

For	O
the	O
remaining	O
ones	O
,	O
we	O
perform	O
light	O
hyperparameter	B-Method
search	E-Method
in	O
the	O
vicinity	O
of	O
those	O
reported	O
for	O
AWD	B-Method
-	I-Method
LSTM	I-Method
in	E-Method
and	O
for	O
AWD	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
MoS	E-Method
in	O
.	O

subsection	O
:	O
Model	O
and	O
training	S-Material
for	O
PTB	S-Material
and	O
WikiText	B-Material
-	I-Material
2	E-Material
For	O
the	O
single	B-Method
softmax	I-Method
model	E-Method
(	O
AWD	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	I-Method
)	E-Method
,	O
for	O
both	O
PTB	S-Material
and	O
WT2	S-Material
,	O
we	O
use	O
a	O
3	O
-	O
layered	O
LSTM	S-Method
with	O
1150	O
,	O
1150	O
and	O
400	O
hidden	O
dimensions	O
.	O

The	O
word	O
embedding	O
dimension	O
is	O
set	O
to	O
.	O

For	O
the	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmax	I-Method
model	E-Method
,	O
we	O
use	O
a	O
3	O
-	O
layer	O
LSTM	S-Method
with	O
dimensions	O
960	O
,	O
960	O
and	O
620	O
,	O
embedding	O
dimension	O
of	O
280	O
and	O
15	O
experts	O
for	O
PTB	S-Material
and	O
a	O
3	O
-	O
layer	O
LSTM	S-Method
with	O
dimensions	O
1150	O
,	O
1150	O
and	O
650	O
,	O
embedding	O
dimension	O
of	O
and	O
15	O
experts	O
for	O
WT2	S-Material
.	O

Weight	B-Method
tying	E-Method
is	O
used	O
in	O
all	O
the	O
models	O
.	O

For	O
training	S-Material
the	O
models	O
,	O
we	O
follow	O
the	O
same	O
procedure	O
as	O
AWD	B-Method
-	I-Method
LSTM	I-Method
i.e.	E-Method
a	O
combination	O
of	O
SGD	S-Method
and	O
NT	B-Method
-	I-Method
ASGD	E-Method
,	O
followed	O
by	O
finetuning	S-Method
.	O

We	O
adopt	O
the	O
learning	O
rate	O
schedules	O
and	O
batch	O
sizes	O
of	O
and	O
in	O
our	O
experiments	O
.	O

subsection	O
:	O
Model	O
and	O
training	S-Material
for	O
PTBC	S-Material
and	O
Enwik8	S-Material
For	O
PTBC	S-Material
,	O
we	O
use	O
a	O
3	O
-	O
layer	O
LSTM	S-Method
with	O
1000	O
,	O
1000	O
and	O
200	O
hidden	O
dimensions	O
and	O
a	O
character	O
embedding	O
dimension	O
of	O
.	O

For	O
Enwik8	S-Material
,	O
we	O
use	O
a	O
LSTM	S-Method
with	O
1850	O
,	O
1850	O
and	O
400	O
hidden	O
dimensions	O
and	O
the	O
characters	O
are	O
embedded	O
in	O
dimensions	O
.	O

For	O
training	S-Material
,	O
we	O
largely	O
follow	O
the	O
procedure	O
laid	O
out	O
in	O
.	O

For	O
each	O
of	O
the	O
datasets	O
,	O
AWD	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	E-Method
has	O
less	O
than	O
1	O
%	O
more	O
parameters	O
than	O
the	O
corresponding	O
AWD	B-Method
-	I-Method
LSTM	I-Method
model	E-Method
(	O
during	O
training	S-Material
only	O
)	O
.	O

The	O
maximum	O
observed	B-Metric
time	I-Metric
overhead	E-Metric
due	O
to	O
the	O
additional	O
computation	O
is	O
less	O
than	O
3	O
%	O
.	O

section	O
:	O
Results	O
on	O
Word	O
Level	O
Language	B-Task
Modeling	E-Task
The	O
results	O
for	O
PTB	S-Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

With	O
a	O
single	O
softmax	S-Method
,	O
our	O
method	O
(	O
AWD	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	E-Method
)	O
achieves	O
a	O
perplexity	S-Metric
of	O
55.6	O
on	O
the	O
PTB	B-Material
test	I-Material
set	E-Material
,	O
which	O
improves	O
on	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
a	O
single	O
softmax	O
by	O
an	O
absolute	O
1.7	O
points	O
.	O

The	O
advantages	O
of	O
better	O
information	B-Task
retention	E-Task
due	O
to	O
PDR	S-Method
are	O
maintained	O
when	O
combined	O
with	O
a	O
continuous	O
cache	O
pointer	O
(	O
)	O
,	O
where	O
our	O
method	O
yields	O
an	O
absolute	O
improvement	O
of	O
1.2	O
over	O
AWD	B-Method
-	I-Method
LSTM	E-Method
.	O

Notably	O
,	O
when	O
coupled	O
with	O
dynamic	B-Method
evaluation	E-Method
(	O
)	O
,	O
the	O
perplexity	S-Metric
is	O
decreased	O
further	O
to	O
49.3	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
ours	O
is	O
the	O
first	O
method	O
to	O
achieve	O
a	O
sub	B-Task
50	I-Task
perplexity	E-Task
on	O
the	O
PTB	B-Material
test	I-Material
set	E-Material
with	O
a	O
single	O
softmax	S-Method
.	O

Note	O
that	O
,	O
for	O
both	O
cache	B-Task
pointer	I-Task
and	I-Task
dynamic	I-Task
evaluation	E-Task
,	O
we	O
coarsely	O
tune	O
the	O
associated	O
hyperparameters	O
on	O
the	O
validation	O
set	O
.	O

Using	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
,	O
our	O
method	O
(	O
AWD	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
MoS	I-Method
+	I-Method
PDR	E-Method
)	O
achieves	O
a	O
test	B-Metric
perplexity	E-Metric
of	O
53.8	O
,	O
an	O
improvement	O
of	O
0.6	O
points	O
over	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

The	O
use	O
of	O
dynamic	B-Method
evaluation	E-Method
pushes	O
the	O
perplexity	O
further	O
down	O
to	O
47.3	O
.	O

PTB	S-Material
is	O
a	O
restrictive	O
dataset	O
with	O
a	O
vocabulary	O
of	O
10	O
K	O
words	O
.	O

Achieving	O
good	O
perplexity	S-Task
requires	O
considerable	O
regularization	O
.	O

The	O
fact	O
that	O
PDR	S-Method
can	O
improve	O
upon	O
existing	O
heavily	B-Method
regularized	I-Method
models	E-Method
is	O
empirical	O
evidence	O
of	O
its	O
distinctive	O
nature	O
and	O
its	O
effectiveness	O
in	O
improving	O
language	B-Method
models	E-Method
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
perplexities	O
achieved	O
by	O
our	O
model	O
on	O
WT2	S-Material
.	O

This	O
dataset	O
is	O
considerably	O
more	O
complex	O
than	O
PTB	S-Material
with	O
a	O
vocabulary	O
of	O
more	O
than	O
33	O
K	O
words	O
.	O

AWD	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	E-Method
improves	O
over	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
a	O
single	O
softmax	S-Method
by	O
a	O
significant	O
2.3	O
points	O
,	O
achieving	O
a	O
perplexity	S-Metric
of	O
63.5	O
.	O

The	O
gains	O
are	O
maintained	O
with	O
the	O
use	O
of	O
cache	O
pointer	O
(	O
2.4	O
points	O
)	O
and	O
with	O
the	O
use	O
of	O
dynamic	O
evaluation	O
(	O
1.7	O
points	O
)	O
.	O

Using	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
,	O
AWD	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
MoS	I-Method
+	I-Method
PDR	E-Method
achieves	O
perplexities	S-Method
of	O
60.5	O
and	O
40.3	O
(	O
with	O
dynamic	B-Metric
evaluation	E-Metric
)	O
on	O
the	O
WT2	B-Material
test	I-Material
set	E-Material
,	O
improving	O
upon	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
1.0	O
and	O
0.4	O
points	O
respectively	O
.	O

subsection	O
:	O
Performance	O
on	O
Larger	O
Datasets	O
We	O
consider	O
the	O
Gigaword	O
dataset	O
with	O
a	O
truncated	O
vocabulary	O
of	O
about	O
100	O
K	O
tokens	O
with	O
the	O
highest	O
frequency	O
and	O
apply	O
PDR	S-Method
to	O
a	O
baseline	O
2	O
-	O
layer	O
LSTM	S-Method
language	O
model	O
with	O
embedding	O
and	O
hidden	O
dimensions	O
set	O
to	O
1024	O
.	O

We	O
use	O
all	O
the	O
shards	O
from	O
the	O
training	S-Material
set	O
for	O
training	S-Material
and	O
a	O
few	O
shards	O
from	O
the	O
heldout	O
set	O
for	O
validation	S-Task
(	O
heldout	O
-	O
0	O
,	O
10	O
)	O
and	O
test	O
(	O
heldout	O
-	O
20	O
,	O
30	O
,	O
40	O
)	O
.	O

We	O
tuned	O
the	O
PDR	S-Method
coefficient	O
coarsely	O
in	O
the	O
vicinity	O
of	O
0.001	O
.	O

While	O
the	O
baseline	O
model	O
achieved	O
a	O
validation	O
(	O
test	O
)	O
perplexity	S-Metric
of	O
44.3	O
(	O
43.1	O
)	O
,	O
on	O
applying	O
PDR	S-Method
,	O
the	O
model	O
achieved	O
a	O
perplexity	S-Metric
of	O
44.0	O
(	O
42.5	O
)	O
.	O

Thus	O
,	O
PDR	S-Method
is	O
relatively	O
less	O
effective	O
on	O
larger	O
datasets	O
,	O
a	O
fact	O
also	O
observed	O
for	O
other	O
regularization	B-Method
techniques	E-Method
on	O
such	O
datasets	O
(	O
)	O
.	O

section	O
:	O
Results	O
on	O
Character	O
Level	O
Language	B-Task
Modeling	E-Task
The	O
results	O
on	O
PTBC	S-Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
method	O
achieves	O
a	O
bits	B-Metric
-	I-Metric
per	I-Metric
-	I-Metric
character	E-Metric
(	O
BPC	S-Metric
)	O
performance	O
of	O
1.169	O
on	O
the	O
PTBC	B-Material
test	I-Material
set	E-Material
,	O
improving	O
on	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
0.006	O
or	O
0.5	O
%	O
.	O

It	O
is	O
notable	O
that	O
even	O
with	O
this	O
highly	O
processed	O
dataset	O
and	O
a	O
small	O
vocabulary	O
of	O
only	O
51	O
tokens	O
,	O
our	O
method	O
improves	O
on	O
already	O
highly	O
regularized	B-Method
models	E-Method
.	O

Finally	O
,	O
we	O
present	O
results	O
on	O
Enwik8	S-Material
in	O
Table	O
[	O
reference	O
]	O
.	O

AWD	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
PDR	E-Method
achieves	O
1.245	O
BPC	S-Metric
.	O

This	O
is	O
0.012	O
or	O
about	O
1	O
%	O
less	O
than	O
the	O
1.257	O
BPC	S-Metric
achieved	O
by	O
AWD	B-Method
-	I-Method
LSTM	E-Method
in	O
our	O
experiments	O
(	O
with	O
hyperparameters	O
from	O
)	O
.	O

section	O
:	O
Analysis	O
of	O
PDR	S-Method
In	O
this	O
section	O
,	O
we	O
analyze	O
PDR	S-Method
by	O
probing	O
its	O
performance	O
in	O
several	O
ways	O
and	O
comparing	O
it	O
with	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
that	O
do	O
not	O
use	O
PDR	S-Method
.	O

subsection	O
:	O
A	O
Valid	O
Regularization	S-Method
To	O
verify	O
that	O
indeed	O
PDR	S-Method
can	O
act	O
as	O
a	O
form	O
of	O
regularization	S-Method
,	O
we	O
perform	O
the	O
following	O
experiment	O
.	O

We	O
take	O
the	O
models	O
for	O
PTB	S-Material
and	O
WT2	S-Material
and	O
turn	O
off	O
all	O
dropouts	S-Method
and	O
regularization	S-Method
and	O
compare	O
its	O
performance	O
with	O
only	O
PDR	S-Method
turned	O
on	O
.	O

The	O
results	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
validate	O
the	O
premise	O
of	O
PDR	S-Method
.	O

The	O
model	O
with	O
only	O
PDR	S-Method
turned	O
on	O
achieves	O
2.4	O
and	O
5.1	O
better	O
validation	B-Metric
perplexity	E-Metric
on	O
PTB	S-Material
and	O
WT2	S-Material
as	O
compared	O
to	O
the	O
model	O
without	O
any	O
regularization	S-Method
.	O

Thus	O
,	O
biasing	O
the	O
LSTM	S-Method
by	O
decoding	O
the	O
distribution	O
of	O
past	O
tokens	O
from	O
the	O
predicted	O
next	O
-	O
token	O
distribution	O
can	O
indeed	O
act	O
as	O
a	O
regularizer	S-Method
leading	O
to	O
better	O
generalization	S-Task
performance	O
.	O

Next	O
,	O
we	O
plot	O
histograms	O
of	O
the	O
negative	O
log	O
-	O
likelihoods	O
of	O
the	O
correct	O
context	O
tokens	O
in	O
the	O
past	O
decoded	O
vector	O
computed	O
using	O
our	O
best	O
models	O
on	O
the	O
PTB	S-Material
and	O
WT2	S-Material
validation	O
sets	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
.	O

The	O
NLL	O
values	O
are	O
significantly	O
peaked	O
near	O
0	O
,	O
which	O
means	O
that	O
the	O
past	B-Method
decoding	I-Method
operation	E-Method
is	O
able	O
to	O
decode	O
significant	O
amount	O
of	O
information	O
about	O
the	O
last	O
token	O
in	O
the	O
context	O
.	O

To	O
investigate	O
the	O
effect	O
of	O
hyperparameters	O
on	O
PDR	S-Method
,	O
we	O
pick	O
60	O
sets	O
of	O
random	O
hyperparameters	O
in	O
the	O
vicinity	O
of	O
those	O
reported	O
by	O
and	O
compute	O
the	O
validation	B-Metric
set	I-Metric
perplexity	E-Metric
after	O
training	S-Material
(	O
without	O
finetuning	S-Method
)	O
on	O
PTB	S-Material
,	O
for	O
both	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
and	O
AWD	B-Method
-	I-Method
LSTM	E-Method
.	O

Their	O
histograms	O
are	O
plotted	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
.	O

The	O
perplexities	O
for	O
models	O
with	O
PDR	S-Method
are	O
distributed	O
slightly	O
to	O
the	O
left	O
of	O
those	O
without	O
PDR	S-Method
.	O

There	O
appears	O
to	O
be	O
more	O
instances	O
of	O
perplexities	O
in	O
the	O
higher	O
range	O
for	O
models	O
without	O
PDR	S-Method
.	O

Note	O
that	O
there	O
are	O
certainly	O
hyperparameter	O
settings	O
where	O
adding	O
PDR	S-Method
leads	O
to	O
lower	O
validation	B-Metric
complexity	E-Metric
,	O
as	O
is	O
generally	O
the	O
case	O
for	O
any	O
regularization	B-Method
method	E-Method
.	O

[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Negative	O
log	O
-	O
likelihood	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.05	O
,	O
0.5	O
)	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
legend	O
pos=	O
north	O
east	O
,	O
legend	O
entries	O
=	O
PTB	S-Material
-	O
Valid	O
,	O
WT2	S-Material
-	O
Valid	O
,	O
]	O
[	O
magenta	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.316464626105	O
)(	O
1.0	O
,	O
0.0851737740793	O
)(	O
1.67	O
,	O
0.0652768579573	O
)(	O
2.33	O
,	O
0.057819113709	O
)(	O
3.0	O
,	O
0.0520170462726	O
)(	O
3.67	O
,	O
0.0518154856172	O
)(	O
4.33	O
,	O
0.050044631288	O
)(	O
5.0	O
,	O
0.0517866912379	O
)(	O
5.67	O
,	O
0.0515851305825	O
)(	O
6.33	O
,	O
0.0497854818739	O
)(	O
7.0	O
,	O
0.0443865357482	O
)(	O
7.67	O
,	O
0.0388580149155	O
)(	O
8.33	O
,	O
0.0341789282732	O
)(	O
9.0	O
,	O
0.0278297676294	O
)(	O
9.67	O
,	O
0.022977914711	O
)	O
;	O
[	O
black	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.30860908651	O
)(	O
1.0	O
,	O
0.0787424358073	O
)(	O
1.67	O
,	O
0.0643498909958	O
)(	O
2.33	O
,	O
0.059959790672	O
)(	O
3.0	O
,	O
0.0559238916244	O
)(	O
3.67	O
,	O
0.0553451965817	O
)(	O
4.33	O
,	O
0.0522072726003	O
)(	O
5.0	O
,	O
0.0524417438676	O
)(	O
5.67	O
,	O
0.0500670986924	O
)(	O
6.33	O
,	O
0.0488797761049	O
)(	O
7.0	O
,	O
0.0446293607914	O
)(	O
7.67	O
,	O
0.0396455991739	O
)(	O
8.33	O
,	O
0.0349711400791	O
)(	O
9.0	O
,	O
0.0289646846361	O
)(	O
9.67	O
,	O
0.0252630318631	O
)	O
;	O
[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
ymax=0.3	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Perplexity	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.02	O
,	O
0.5	O
)	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
y	O
tick	O
label	O
style=	O
/	O
pgf	O
/	O
number	O
format	O
/	O
.cd	O
,	O
fixed	O
,	O
fixed	O
zerofill	O
,	O
precision=2	O
,	O
legend	O
pos=	O
north	O
west	O
,	O
legend	O
entries	O
=	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
,	O
AWD	B-Method
-	I-Method
LSTM	E-Method
,	O
]	O
[	O
red	O
]	O
coordinates	O
(	O
60.17	O
,	O
0.016393442623	O
)(	O
60.32	O
,	O
0.0	O
)(	O
60.47	O
,	O
0.016393442623	O
)(	O
60.61	O
,	O
0.0655737704918	O
)(	O
60.76	O
,	O
0.114754098361	O
)(	O
60.91	O
,	O
0.131147540984	O
)(	O
61.05	O
,	O
0.114754098361	O
)(	O
61.2	O
,	O
0.147540983607	O
)(	O
61.35	O
,	O
0.229508196721	O
)(	O
61.49	O
,	O
0.0655737704918	O
)(	O
61.64	O
,	O
0.0655737704918	O
)(	O
61.79	O
,	O
0.0	O
)(	O
61.93	O
,	O
0.016393442623	O
)(	O
62.08	O
,	O
0.0	O
)(	O
62.23	O
,	O
0.016393442623	O
)	O
;	O
[	O
black!60!green	O
]	O
coordinates	O
(	O
60.17	O
,	O
0.0	O
)(	O
60.32	O
,	O
0.0	O
)(	O
60.47	O
,	O
0.0	O
)(	O
60.61	O
,	O
0.0655737704918	O
)(	O
60.76	O
,	O
0.0819672131148	O
)(	O
60.91	O
,	O
0.16393442623	O
)(	O
61.05	O
,	O
0.180327868852	O
)(	O
61.2	O
,	O
0.131147540984	O
)(	O
61.35	O
,	O
0.0983606557377	O
)(	O
61.49	O
,	O
0.114754098361	O
)(	O
61.64	O
,	O
0.131147540984	O
)(	O
61.79	O
,	O
0.0327868852459	O
)(	O
61.93	O
,	O
0.0	O
)(	O
62.08	O
,	O
0.0	O
)(	O
62.23	O
,	O
0.0	O
)	O
;	O
subsection	O
:	O
Comparison	O
with	O
AWD	B-Method
-	I-Method
LSTM	E-Method
cycle	O
list	O
/	O
Set1	O
-	O
4	O
[	O
scale=0.80	O
]	O
patterns	O
[	O
ybar	O
,	O
ymin=0	O
,	O
bar	O
width=2	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
Predicted	O
token	O
entropy	O
,	O
ylabel	O
=	O
Normalized	O
frequency	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.02	O
,	O
0.5	O
)	O
,	O
y	O
tick	O
label	O
style=	O
/	O
pgf	O
/	O
number	O
format	O
/	O
.cd	O
,	O
fixed	O
,	O
fixed	O
zerofill	O
,	O
precision=2	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
every	O
axis	O
plot	O
/	O
.append	O
style	O
=	O
fill	O
,	O
legend	O
pos=	O
north	O
west	O
,	O
legend	O
entries	O
=	O
AWD	O
-	O
LSTM	S-Method
+	O
PDR	S-Method
,	O
AWD	O
-	O
LSTM	S-Method
,	O
]	O
[	O
red	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.0668799739693	O
)(	O
1.0	O
,	O
0.031575807698	O
)(	O
1.67	O
,	O
0.0282406214835	O
)(	O
2.33	O
,	O
0.0319960953917	O
)(	O
3.0	O
,	O
0.037310701067	O
)(	O
3.67	O
,	O
0.045865589284	O
)(	O
4.33	O
,	O
0.064331132472	O
)(	O
5.0	O
,	O
0.0876367629713	O
)(	O
5.67	O
,	O
0.108068167952	O
)(	O
6.33	O
,	O
0.133922639949	O
)(	O
7.0	O
,	O
0.145975406391	O
)(	O
7.67	O
,	O
0.117490746892	O
)(	O
8.33	O
,	O
0.0765737062596	O
)(	O
9.0	O
,	O
0.0230073618135	O
)(	O
9.67	O
,	O
0.00112528640573	O
)	O
;	O
[	O
black!60!green	O
]	O
coordinates	O
(	O
0.33	O
,	O
0.0684582220475	O
)(	O
1.0	O
,	O
0.0332840738079	O
)(	O
1.67	O
,	O
0.031372442685	O
)(	O
2.33	O
,	O
0.0344093602137	O
)(	O
3.0	O
,	O
0.0409305982999	O
)(	O
3.67	O
,	O
0.0500684662211	O
)(	O
4.33	O
,	O
0.0660529562494	O
)(	O
5.0	O
,	O
0.084057538741	O
)(	O
5.67	O
,	O
0.103553464662	O
)(	O
6.33	O
,	O
0.123117178921	O
)(	O
7.0	O
,	O
0.133583698261	O
)(	O
7.67	O
,	O
0.11149825784	O
)(	O
8.33	O
,	O
0.083824279071	O
)(	O
9.0	O
,	O
0.0334982171667	O
)(	O
9.67	O
,	O
0.00229124581407	O
)	O
;	O
[	O
scale=0.80	O
]	O
[	O
xmax=1200	O
,	O
xmin=50	O
,	O
ymax=85	O
,	O
x	O
tick	O
label	O
style	O
=	O
rotate=0	O
,	O
xlabel	O
=	O
No	O
.	O

of	O
epochs	O
,	O
ylabel	O
=	O
Perplexity	O
,	O
y	O
label	O
style	O
=	O
at=	O
(	O
0.05	O
,	O
0.5	O
)	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
legend	O
entries	O
=	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
(	O
Train	O
),	O
AWD	B-Method
-	I-Method
LSTM	E-Method
(	O
Train	O
),	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
(	O
Valid	O
),	O
AWD	B-Method
-	I-Method
LSTM	E-Method
(	O
Valid	O
)	O
]	O
[	O
red	O
,	O
thick	O
,	O
dashed	O
]	O
coordinates	O
(	O
10	O
,	O
155.13	O
)(	O
20	O
,	O
107.08	O
)(	O
30	O
,	O
88.67	O
)(	O
40	O
,	O
78.86	O
)(	O
50	O
,	O
72.14	O
)(	O
60	O
,	O
68.24	O
)(	O
70	O
,	O
66.26	O
)(	O
80	O
,	O
62.77	O
)(	O
90	O
,	O
61.43	O
)(	O
100	O
,	O
59.24	O
)(	O
110	O
,	O
57.73	O
)(	O
120	O
,	O
56.89	O
)(	O
130	O
,	O
55.47	O
)(	O
140	O
,	O
54.97	O
)(	O
150	O
,	O
53.56	O
)(	O
160	O
,	O
53.39	O
)(	O
170	O
,	O
51.40	O
)(	O
180	O
,	O
50.83	O
)(	O
190	O
,	O
51.02	O
)(	O
200	O
,	O
50.42	O
)(	O
210	O
,	O
49.81	O
)(	O
220	O
,	O
49.46	O
)(	O
230	O
,	O
49.05	O
)(	O
240	O
,	O
48.19	O
)(	O
250	O
,	O
48.29	O
)(	O
260	O
,	O
48.05	O
)(	O
270	O
,	O
47.82	O
)(	O
280	O
,	O
47.44	O
)(	O
290	O
,	O
47.52	O
)(	O
300	O
,	O
46.70	O
)(	O
310	O
,	O
46.44	O
)(	O
320	O
,	O
46.24	O
)(	O
330	O
,	O
46.49	O
)(	O
340	O
,	O
45.73	O
)(	O
350	O
,	O
45.62	O
)(	O
360	O
,	O
45.44	O
)(	O
370	O
,	O
44.94	O
)(	O
380	O
,	O
45.05	O
)(	O
390	O
,	O
44.54	O
)(	O
400	O
,	O
44.91	O
)(	O
410	O
,	O
44.52	O
)(	O
420	O
,	O
44.00	O
)(	O
430	O
,	O
43.94	O
)(	O
440	O
,	O
43.95	O
)(	O
450	O
,	O
44.13	O
)(	O
460	O
,	O
43.84	O
)(	O
470	O
,	O
43.88	O
)(	O
480	O
,	O
43.63	O
)(	O
490	O
,	O
43.73	O
)(	O
500	O
,	O
43.57	O
)(	O
510	O
,	O
42.99	O
)(	O
520	O
,	O
42.78	O
)(	O
530	O
,	O
42.89	O
)(	O
540	O
,	O
43.10	O
)(	O
550	O
,	O
43.06	O
)(	O
560	O
,	O
42.55	O
)(	O
570	O
,	O
42.55	O
)(	O
580	O
,	O
42.27	O
)(	O
590	O
,	O
41.90	O
)(	O
600	O
,	O
42.71	O
)(	O
610	O
,	O
42.08	O
)(	O
620	O
,	O
42.40	O
)(	O
630	O
,	O
42.15	O
)(	O
640	O
,	O
42.14	O
)(	O
650	O
,	O
42.57	O
)(	O
660	O
,	O
41.67	O
)(	O
670	O
,	O
41.60	O
)(	O
680	O
,	O
41.43	O
)(	O
690	O
,	O
41.55	O
)(	O
700	O
,	O
41.68	O
)(	O
710	O
,	O
41.68	O
)(	O
720	O
,	O
41.44	O
)(	O
730	O
,	O
41.21	O
)(	O
740	O
,	O
41.70	O
)(	O
750	O
,	O
41.69	O
)(	O
760	O
,	O
39.79	O
)(	O
770	O
,	O
41.44	O
)(	O
780	O
,	O
41.96	O
)(	O
790	O
,	O
42.13	O
)(	O
800	O
,	O
41.67	O
)(	O
810	O
,	O
41.85	O
)(	O
820	O
,	O
42.46	O
)(	O
830	O
,	O
41.82	O
)(	O
840	O
,	O
41.82	O
)(	O
850	O
,	O
41.76	O
)(	O
860	O
,	O
41.43	O
)(	O
870	O
,	O
41.65	O
)(	O
880	O
,	O
41.61	O
)(	O
890	O
,	O
41.94	O
)(	O
900	O
,	O
41.35	O
)(	O
910	O
,	O
41.76	O
)(	O
920	O
,	O
41.30	O
)(	O
930	O
,	O
41.34	O
)(	O
940	O
,	O
41.24	O
)(	O
950	O
,	O
41.09	O
)(	O
960	O
,	O
41.11	O
)(	O
970	O
,	O
40.72	O
)(	O
980	O
,	O
41.14	O
)(	O
990	O
,	O
40.53	O
)(	O
1000	O
,	O
41.03	O
)(	O
1010	O
,	O
41.11	O
)(	O
1020	O
,	O
41.06	O
)(	O
1030	O
,	O
40.65	O
)(	O
1040	O
,	O
41.14	O
)(	O
1050	O
,	O
40.73	O
)(	O
1060	O
,	O
40.62	O
)(	O
1070	O
,	O
40.48	O
)(	O
1080	O
,	O
40.83	O
)(	O
1090	O
,	O
41.12	O
)(	O
1100	O
,	O
40.41	O
)(	O
1110	O
,	O
40.65	O
)(	O
1120	O
,	O
40.17	O
)(	O
1130	O
,	O
40.44	O
)(	O
1140	O
,	O
40.28	O
)(	O
1150	O
,	O
40.05	O
)(	O
1160	O
,	O
40.44	O
)(	O
1170	O
,	O
39.93	O
)(	O
1180	O
,	O
39.90	O
)(	O
1190	O
,	O
39.59	O
)(	O
1200	O
,	O
40.16	O
)(	O
1210	O
,	O
40.28	O
)(	O
1220	O
,	O
39.82	O
)(	O
1230	O
,	O
40.00	O
)(	O
1240	O
,	O
40.08	O
)(	O
1250	O
,	O
39.93	O
)(	O
1260	O
,	O
39.62	O
)(	O
1270	O
,	O
39.45	O
)(	O
1280	O
,	O
39.85	O
)(	O
1290	O
,	O
39.82	O
)(	O
1300	O
,	O
40.09	O
)(	O
1310	O
,	O
39.63	O
)(	O
1320	O
,	O
39.64	O
)(	O
1330	O
,	O
39.34	O
)(	O
1340	O
,	O
39.05	O
)(	O
1350	O
,	O
39.43	O
)(	O
1360	O
,	O
39.12	O
)(	O
1370	O
,	O
39.32	O
)(	O
1380	O
,	O
39.37	O
)(	O
1390	O
,	O
39.63	O
)(	O
1400	O
,	O
39.71	O
)	O
;	O
[	O
black!60!green	O
,	O
thick	O
,	O
dashed	O
]	O
coordinates	O
(	O
10	O
,	O
140.66	O
)(	O
20	O
,	O
95.19	O
)(	O
30	O
,	O
79.04	O
)(	O
40	O
,	O
69.99	O
)(	O
50	O
,	O
64.20	O
)(	O
60	O
,	O
60.38	O
)(	O
70	O
,	O
58.07	O
)(	O
80	O
,	O
55.14	O
)(	O
90	O
,	O
53.71	O
)(	O
100	O
,	O
51.55	O
)(	O
110	O
,	O
50.41	O
)(	O
120	O
,	O
49.34	O
)(	O
130	O
,	O
48.23	O
)(	O
140	O
,	O
47.76	O
)(	O
150	O
,	O
46.76	O
)(	O
160	O
,	O
46.68	O
)(	O
170	O
,	O
44.84	O
)(	O
180	O
,	O
44.30	O
)(	O
190	O
,	O
44.08	O
)(	O
200	O
,	O
43.60	O
)(	O
210	O
,	O
43.48	O
)(	O
220	O
,	O
43.03	O
)(	O
230	O
,	O
42.56	O
)(	O
240	O
,	O
41.78	O
)(	O
250	O
,	O
41.93	O
)(	O
260	O
,	O
41.61	O
)(	O
270	O
,	O
41.81	O
)(	O
280	O
,	O
41.40	O
)(	O
290	O
,	O
41.20	O
)(	O
300	O
,	O
40.69	O
)(	O
310	O
,	O
40.43	O
)(	O
320	O
,	O
40.17	O
)(	O
330	O
,	O
40.40	O
)(	O
340	O
,	O
39.86	O
)(	O
350	O
,	O
39.66	O
)(	O
360	O
,	O
39.46	O
)(	O
370	O
,	O
39.03	O
)(	O
380	O
,	O
38.83	O
)(	O
390	O
,	O
38.53	O
)(	O
400	O
,	O
38.91	O
)(	O
410	O
,	O
38.74	O
)(	O
420	O
,	O
38.21	O
)(	O
430	O
,	O
37.93	O
)(	O
440	O
,	O
38.15	O
)(	O
450	O
,	O
38.09	O
)(	O
460	O
,	O
37.83	O
)(	O
470	O
,	O
37.85	O
)(	O
480	O
,	O
37.75	O
)(	O
490	O
,	O
37.72	O
)(	O
500	O
,	O
37.27	O
)(	O
510	O
,	O
37.09	O
)(	O
520	O
,	O
37.19	O
)(	O
530	O
,	O
36.76	O
)(	O
540	O
,	O
37.15	O
)(	O
550	O
,	O
37.02	O
)(	O
560	O
,	O
36.77	O
)(	O
570	O
,	O
36.77	O
)(	O
580	O
,	O
36.53	O
)(	O
590	O
,	O
36.16	O
)(	O
600	O
,	O
36.76	O
)(	O
610	O
,	O
36.43	O
)(	O
620	O
,	O
36.24	O
)(	O
630	O
,	O
36.26	O
)(	O
640	O
,	O
36.12	O
)(	O
650	O
,	O
36.44	O
)(	O
660	O
,	O
35.96	O
)(	O
670	O
,	O
35.79	O
)(	O
680	O
,	O
35.75	O
)(	O
690	O
,	O
35.62	O
)(	O
700	O
,	O
35.79	O
)(	O
710	O
,	O
35.62	O
)(	O
720	O
,	O
35.97	O
)(	O
730	O
,	O
35.30	O
)(	O
740	O
,	O
35.43	O
)(	O
750	O
,	O
35.72	O
)(	O
760	O
,	O
36.88	O
)(	O
770	O
,	O
36.87	O
)(	O
780	O
,	O
37.12	O
)(	O
790	O
,	O
36.91	O
)(	O
800	O
,	O
36.36	O
)(	O
810	O
,	O
36.07	O
)(	O
820	O
,	O
36.33	O
)(	O
830	O
,	O
35.65	O
)(	O
840	O
,	O
35.58	O
)(	O
850	O
,	O
35.48	O
)(	O
860	O
,	O
35.18	O
)(	O
870	O
,	O
35.29	O
)(	O
880	O
,	O
34.86	O
)(	O
890	O
,	O
35.3	O
)(	O
900	O
,	O
34.87	O
)(	O
910	O
,	O
35.37	O
)(	O
920	O
,	O
34.76	O
)(	O
930	O
,	O
34.68	O
)(	O
940	O
,	O
34.69	O
)(	O
950	O
,	O
34.31	O
)(	O
960	O
,	O
34.56	O
)(	O
970	O
,	O
34.15	O
)(	O
980	O
,	O
34.4	O
)(	O
990	O
,	O
33.95	O
)(	O
1000	O
,	O
34.02	O
)(	O
1010	O
,	O
34.12	O
)(	O
1020	O
,	O
34.24	O
)(	O
1030	O
,	O
33.9	O
)(	O
1040	O
,	O
34.11	O
)(	O
1050	O
,	O
33.79	O
)(	O
1060	O
,	O
33.91	O
)(	O
1070	O
,	O
33.84	O
)(	O
1080	O
,	O
34.06	O
)(	O
1090	O
,	O
34.3	O
)(	O
1100	O
,	O
33.75	O
)(	O
1110	O
,	O
33.54	O
)(	O
1120	O
,	O
33.57	O
)(	O
1130	O
,	O
33.55	O
)(	O
1140	O
,	O
33.42	O
)(	O
1150	O
,	O
33.17	O
)(	O
1160	O
,	O
33.78	O
)(	O
1170	O
,	O
33.1	O
)(	O
1180	O
,	O
33.01	O
)(	O
1190	O
,	O
32.9	O
)(	O
1200	O
,	O
33.38	O
)(	O
1210	O
,	O
33.12	O
)(	O
1220	O
,	O
32.83	O
)(	O
1230	O
,	O
32.94	O
)(	O
1240	O
,	O
32.95	O
)(	O
1250	O
,	O
33.04	O
)(	O
1260	O
,	O
32.57	O
)(	O
1270	O
,	O
32.57	O
)(	O
1280	O
,	O
32.63	O
)(	O
1290	O
,	O
32.93	O
)(	O
1300	O
,	O
32.83	O
)(	O
1310	O
,	O
32.68	O
)(	O
1320	O
,	O
32.54	O
)(	O
1330	O
,	O
32.72	O
)(	O
1340	O
,	O
32.24	O
)(	O
1350	O
,	O
32.65	O
)(	O
1360	O
,	O
32.42	O
)(	O
1370	O
,	O
32.24	O
)(	O
1380	O
,	O
32.53	O
)(	O
1390	O
,	O
32.42	O
)(	O
1400	O
,	O
32.45	O
)	O
;	O
[	O
red	O
,	O
thick	O
]	O
coordinates	O
(	O
10	O
,	O
121.40	O
)(	O
20	O
,	O
89.75	O
)(	O
30	O
,	O
81.34	O
)(	O
40	O
,	O
75.91	O
)(	O
50	O
,	O
73.53	O
)(	O
60	O
,	O
68.18	O
)(	O
70	O
,	O
66.82	O
)(	O
80	O
,	O
66.04	O
)(	O
90	O
,	O
65.49	O
)(	O
100	O
,	O
65.03	O
)(	O
110	O
,	O
64.64	O
)(	O
120	O
,	O
64.29	O
)(	O
130	O
,	O
63.97	O
)(	O
140	O
,	O
63.70	O
)(	O
150	O
,	O
63.46	O
)(	O
160	O
,	O
63.25	O
)(	O
170	O
,	O
63.04	O
)(	O
180	O
,	O
62.85	O
)(	O
190	O
,	O
62.69	O
)(	O
200	O
,	O
62.54	O
)(	O
210	O
,	O
62.40	O
)(	O
220	O
,	O
62.27	O
)(	O
230	O
,	O
62.15	O
)(	O
240	O
,	O
62.04	O
)(	O
250	O
,	O
61.93	O
)(	O
260	O
,	O
61.84	O
)(	O
270	O
,	O
61.76	O
)(	O
280	O
,	O
61.68	O
)(	O
290	O
,	O
61.60	O
)(	O
300	O
,	O
61.53	O
)(	O
310	O
,	O
61.46	O
)(	O
320	O
,	O
61.40	O
)(	O
330	O
,	O
61.35	O
)(	O
340	O
,	O
61.30	O
)(	O
350	O
,	O
61.25	O
)(	O
360	O
,	O
61.20	O
)(	O
370	O
,	O
61.15	O
)(	O
380	O
,	O
61.11	O
)(	O
390	O
,	O
61.06	O
)(	O
400	O
,	O
61.03	O
)(	O
410	O
,	O
60.99	O
)(	O
420	O
,	O
60.95	O
)(	O
430	O
,	O
60.92	O
)(	O
440	O
,	O
60.88	O
)(	O
450	O
,	O
60.85	O
)(	O
460	O
,	O
60.82	O
)(	O
470	O
,	O
60.80	O
)(	O
480	O
,	O
60.77	O
)(	O
490	O
,	O
60.75	O
)(	O
500	O
,	O
60.73	O
)(	O
510	O
,	O
60.71	O
)(	O
520	O
,	O
60.68	O
)(	O
530	O
,	O
60.66	O
)(	O
540	O
,	O
60.65	O
)(	O
550	O
,	O
60.63	O
)(	O
560	O
,	O
60.61	O
)(	O
570	O
,	O
60.59	O
)(	O
580	O
,	O
60.58	O
)(	O
590	O
,	O
60.56	O
)(	O
600	O
,	O
60.54	O
)(	O
610	O
,	O
60.53	O
)(	O
620	O
,	O
60.51	O
)(	O
630	O
,	O
60.50	O
)(	O
640	O
,	O
60.49	O
)(	O
650	O
,	O
60.48	O
)(	O
660	O
,	O
60.47	O
)(	O
670	O
,	O
60.46	O
)(	O
680	O
,	O
60.44	O
)(	O
690	O
,	O
60.43	O
)(	O
700	O
,	O
60.42	O
)(	O
710	O
,	O
60.41	O
)(	O
720	O
,	O
60.40	O
)(	O
730	O
,	O
60.40	O
)(	O
740	O
,	O
60.39	O
)(	O
750	O
,	O
60.39	O
)(	O
760	O
,	O
59.97	O
)(	O
770	O
,	O
59.78	O
)(	O
780	O
,	O
59.64	O
)(	O
790	O
,	O
59.51	O
)(	O
800	O
,	O
59.41	O
)(	O
810	O
,	O
59.32	O
)(	O
820	O
,	O
59.25	O
)(	O
830	O
,	O
59.18	O
)(	O
840	O
,	O
59.13	O
)(	O
850	O
,	O
59.08	O
)(	O
860	O
,	O
59.03	O
)(	O
870	O
,	O
59.00	O
)(	O
880	O
,	O
58.96	O
)(	O
890	O
,	O
58.93	O
)(	O
900	O
,	O
58.90	O
)(	O
910	O
,	O
58.88	O
)(	O
920	O
,	O
58.85	O
)(	O
930	O
,	O
58.83	O
)(	O
940	O
,	O
58.80	O
)(	O
950	O
,	O
58.78	O
)(	O
960	O
,	O
58.75	O
)(	O
970	O
,	O
58.73	O
)(	O
980	O
,	O
58.71	O
)(	O
990	O
,	O
58.69	O
)(	O
1000	O
,	O
58.67	O
)(	O
1010	O
,	O
58.65	O
)(	O
1020	O
,	O
58.63	O
)(	O
1030	O
,	O
58.61	O
)(	O
1040	O
,	O
58.59	O
)(	O
1050	O
,	O
58.57	O
)(	O
1060	O
,	O
58.56	O
)(	O
1070	O
,	O
58.54	O
)(	O
1080	O
,	O
58.52	O
)(	O
1090	O
,	O
58.50	O
)(	O
1100	O
,	O
58.48	O
)(	O
1110	O
,	O
58.47	O
)(	O
1120	O
,	O
58.45	O
)(	O
1130	O
,	O
58.44	O
)(	O
1140	O
,	O
58.42	O
)(	O
1150	O
,	O
58.41	O
)(	O
1160	O
,	O
58.39	O
)(	O
1170	O
,	O
58.38	O
)(	O
1180	O
,	O
58.37	O
)(	O
1190	O
,	O
58.35	O
)(	O
1200	O
,	O
58.34	O
)(	O
1210	O
,	O
58.33	O
)(	O
1220	O
,	O
58.31	O
)(	O
1230	O
,	O
58.30	O
)(	O
1240	O
,	O
58.29	O
)(	O
1250	O
,	O
58.28	O
)(	O
1260	O
,	O
58.27	O
)(	O
1270	O
,	O
58.26	O
)(	O
1280	O
,	O
58.25	O
)(	O
1290	O
,	O
58.24	O
)(	O
1300	O
,	O
58.23	O
)(	O
1310	O
,	O
58.22	O
)(	O
1320	O
,	O
58.21	O
)(	O
1330	O
,	O
58.21	O
)(	O
1340	O
,	O
58.20	O
)(	O
1350	O
,	O
58.19	O
)(	O
1360	O
,	O
58.18	O
)(	O
1370	O
,	O
58.18	O
)(	O
1380	O
,	O
58.17	O
)(	O
1390	O
,	O
58.16	O
)(	O
1400	O
,	O
58.15	O
)	O
;	O
[	O
black!60!green	O
,	O
thick	O
]	O
coordinates	O
(	O
10	O
,	O
115.36	O
)(	O
20	O
,	O
87.02	O
)(	O
30	O
,	O
79.04	O
)(	O
40	O
,	O
74.46	O
)(	O
50	O
,	O
72.82	O
)(	O
60	O
,	O
67.61	O
)(	O
70	O
,	O
66.66	O
)(	O
80	O
,	O
65.95	O
)(	O
90	O
,	O
65.42	O
)(	O
100	O
,	O
64.98	O
)(	O
110	O
,	O
64.60	O
)(	O
120	O
,	O
64.27	O
)(	O
130	O
,	O
63.96	O
)(	O
140	O
,	O
63.70	O
)(	O
150	O
,	O
63.48	O
)(	O
160	O
,	O
63.29	O
)(	O
170	O
,	O
63.12	O
)(	O
180	O
,	O
62.96	O
)(	O
190	O
,	O
62.81	O
)(	O
200	O
,	O
62.68	O
)(	O
210	O
,	O
62.56	O
)(	O
220	O
,	O
62.45	O
)(	O
230	O
,	O
62.35	O
)(	O
240	O
,	O
62.26	O
)(	O
250	O
,	O
62.17	O
)(	O
260	O
,	O
62.09	O
)(	O
270	O
,	O
62.02	O
)(	O
280	O
,	O
61.95	O
)(	O
290	O
,	O
61.89	O
)(	O
300	O
,	O
61.83	O
)(	O
310	O
,	O
61.77	O
)(	O
320	O
,	O
61.72	O
)(	O
330	O
,	O
61.66	O
)(	O
340	O
,	O
61.62	O
)(	O
350	O
,	O
61.57	O
)(	O
360	O
,	O
61.53	O
)(	O
370	O
,	O
61.48	O
)(	O
380	O
,	O
61.45	O
)(	O
390	O
,	O
61.41	O
)(	O
400	O
,	O
61.38	O
)(	O
410	O
,	O
61.35	O
)(	O
420	O
,	O
61.32	O
)(	O
430	O
,	O
61.29	O
)(	O
440	O
,	O
61.26	O
)(	O
450	O
,	O
61.23	O
)(	O
460	O
,	O
61.21	O
)(	O
470	O
,	O
61.18	O
)(	O
480	O
,	O
61.16	O
)(	O
490	O
,	O
61.14	O
)(	O
500	O
,	O
61.13	O
)(	O
510	O
,	O
61.11	O
)(	O
520	O
,	O
61.09	O
)(	O
530	O
,	O
61.07	O
)(	O
540	O
,	O
61.06	O
)(	O
550	O
,	O
61.04	O
)(	O
560	O
,	O
61.02	O
)(	O
570	O
,	O
61.01	O
)(	O
580	O
,	O
61.00	O
)(	O
590	O
,	O
60.98	O
)(	O
600	O
,	O
60.97	O
)(	O
610	O
,	O
60.96	O
)(	O
620	O
,	O
60.94	O
)(	O
630	O
,	O
60.93	O
)(	O
640	O
,	O
60.92	O
)(	O
650	O
,	O
60.91	O
)(	O
660	O
,	O
60.90	O
)(	O
670	O
,	O
60.89	O
)(	O
680	O
,	O
60.88	O
)(	O
690	O
,	O
60.87	O
)(	O
700	O
,	O
60.86	O
)(	O
710	O
,	O
60.85	O
)(	O
720	O
,	O
60.84	O
)(	O
730	O
,	O
60.83	O
)(	O
740	O
,	O
60.83	O
)(	O
750	O
,	O
60.82	O
)(	O
760	O
,	O
60.37	O
)(	O
770	O
,	O
60.30	O
)(	O
780	O
,	O
60.25	O
)(	O
790	O
,	O
60.18	O
)(	O
800	O
,	O
60.13	O
)(	O
810	O
,	O
60.07	O
)(	O
820	O
,	O
60.01	O
)(	O
830	O
,	O
59.97	O
)(	O
840	O
,	O
59.93	O
)(	O
850	O
,	O
59.91	O
)(	O
860	O
,	O
59.89	O
)(	O
870	O
,	O
59.86	O
)(	O
880	O
,	O
59.83	O
)(	O
890	O
,	O
59.81	O
)(	O
900	O
,	O
59.78	O
)(	O
910	O
,	O
59.76	O
)(	O
920	O
,	O
59.75	O
)(	O
930	O
,	O
59.73	O
)(	O
940	O
,	O
59.72	O
)(	O
950	O
,	O
59.70	O
)(	O
960	O
,	O
59.69	O
)(	O
970	O
,	O
59.68	O
)(	O
980	O
,	O
59.67	O
)(	O
990	O
,	O
59.66	O
)(	O
1000	O
,	O
59.65	O
)(	O
1010	O
,	O
59.63	O
)(	O
1020	O
,	O
59.62	O
)(	O
1030	O
,	O
59.61	O
)(	O
1040	O
,	O
59.60	O
)(	O
1050	O
,	O
59.58	O
)(	O
1060	O
,	O
59.57	O
)(	O
1070	O
,	O
59.56	O
)(	O
1080	O
,	O
59.55	O
)(	O
1090	O
,	O
59.54	O
)(	O
1100	O
,	O
59.52	O
)(	O
1110	O
,	O
59.51	O
)(	O
1120	O
,	O
59.49	O
)(	O
1130	O
,	O
59.48	O
)(	O
1140	O
,	O
59.47	O
)(	O
1150	O
,	O
59.46	O
)(	O
1160	O
,	O
59.45	O
)(	O
1170	O
,	O
59.44	O
)(	O
1180	O
,	O
59.43	O
)(	O
1190	O
,	O
59.43	O
)(	O
1200	O
,	O
59.42	O
)(	O
1210	O
,	O
59.41	O
)(	O
1220	O
,	O
59.40	O
)(	O
1230	O
,	O
59.39	O
)(	O
1240	O
,	O
59.39	O
)(	O
1250	O
,	O
59.38	O
)(	O
1260	O
,	O
59.37	O
)(	O
1270	O
,	O
59.37	O
)(	O
1280	O
,	O
59.36	O
)(	O
1290	O
,	O
59.36	O
)(	O
1300	O
,	O
59.35	O
)(	O
1310	O
,	O
59.35	O
)(	O
1320	O
,	O
59.34	O
)(	O
1330	O
,	O
59.34	O
)(	O
1340	O
,	O
59.34	O
)(	O
1350	O
,	O
59.33	O
)(	O
1360	O
,	O
59.33	O
)(	O
1370	O
,	O
59.33	O
)(	O
1380	O
,	O
59.32	O
)(	O
1390	O
,	O
59.32	O
)(	O
1400	O
,	O
59.31	O
)	O
;	O
To	O
show	O
the	O
qualitative	O
difference	O
between	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
and	O
AWD	B-Method
-	I-Method
LSTM	E-Method
,	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
,	O
we	O
plot	O
a	O
histogram	O
of	O
the	O
entropy	S-Metric
of	O
the	O
predicted	O
next	O
token	O
distribution	O
for	O
all	O
the	O
tokens	O
in	O
the	O
validation	O
set	O
of	O
PTB	S-Material
achieved	O
by	O
their	O
respective	O
best	O
models	O
.	O

The	O
distributions	O
for	O
the	O
two	O
models	O
is	O
slightly	O
different	O
,	O
with	O
some	O
identifiable	O
patterns	O
.	O

The	O
use	O
of	O
PDR	S-Method
has	O
the	O
effect	O
of	O
reducing	O
the	O
entropy	S-Metric
of	O
the	O
predicted	O
distribution	O
when	O
it	O
is	O
in	O
the	O
higher	O
range	O
of	O
8	O
and	O
above	O
,	O
pushing	O
it	O
into	O
the	O
range	O
of	O
5	O
-	O
8	O
.	O

This	O
shows	O
that	O
one	O
way	O
PDR	S-Method
biases	O
the	O
language	B-Method
model	E-Method
is	O
by	O
reducing	O
the	O
entropy	O
of	O
the	O
predicted	O
next	O
token	O
distribution	O
.	O

Indeed	O
,	O
one	O
way	O
to	O
reduce	O
the	O
cross	B-Metric
-	I-Metric
entropy	E-Metric
between	O
and	O
is	O
by	O
making	O
less	O
spread	O
out	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

This	O
tends	O
to	O
benefits	O
the	O
language	B-Method
model	E-Method
when	O
the	O
predictions	O
are	O
correct	O
.	O

We	O
also	O
compare	O
the	O
training	S-Material
curves	O
for	O
the	O
two	O
models	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
on	O
PTB	S-Material
.	O

Although	O
the	O
two	O
models	O
use	O
slightly	O
different	O
hyperparameters	O
,	O
the	O
regularization	O
effect	O
of	O
PDR	S-Method
is	O
apparent	O
with	O
a	O
lower	O
validation	B-Metric
perplexity	E-Metric
but	O
higher	O
training	S-Material
perplexity	O
.	O

The	O
corresponding	O
trends	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
,	O
b	O
)	O
for	O
WT2	S-Material
have	O
similar	O
characteristics	O
.	O

subsection	O
:	O
Ablation	B-Task
Studies	E-Task
We	O
perform	O
a	O
set	O
of	O
ablation	S-Task
experiments	O
on	O
the	O
best	O
AWD	B-Method
-	I-Method
LSTM	E-Method
+	O
PDR	S-Method
models	O
for	O
PTB	S-Material
and	O
WT2	S-Material
to	O
understand	O
the	O
relative	O
contribution	O
of	O
PDR	S-Method
and	O
the	O
other	O
regularizations	S-Method
used	O
in	O
the	O
model	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
both	O
cases	O
,	O
PDR	S-Method
has	O
a	O
significant	O
effect	O
in	O
decreasing	O
the	O
validation	B-Metric
set	I-Metric
performance	E-Metric
,	O
albeit	O
lesser	O
than	O
the	O
other	O
forms	O
of	O
regularization	S-Method
.	O

This	O
is	O
not	O
surprising	O
as	O
PDR	S-Method
does	O
not	O
influence	O
the	O
LSTM	S-Method
directly	O
.	O

section	O
:	O
Related	O
Work	O
Our	O
method	O
builds	O
on	O
the	O
work	O
of	O
using	O
sophisticated	O
regularization	B-Method
techniques	E-Method
to	O
train	O
LSTMs	S-Method
for	O
language	B-Task
modeling	E-Task
.	O

In	O
particular	O
,	O
the	O
AWD	B-Method
-	I-Method
LSTM	I-Method
model	E-Method
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
a	O
single	O
softmax	S-Method
on	O
the	O
four	O
datasets	O
considered	O
in	O
this	O
paper	O
(	O
)	O
.	O

also	O
achieve	O
similar	O
results	O
with	O
highly	O
regularized	O
LSTMs	S-Method
.	O

By	O
addressing	O
the	O
so	O
-	O
called	O
softmax	B-Method
bottleneck	E-Method
in	O
single	O
softmax	B-Method
models	E-Method
,	O
use	O
a	O
mixture	B-Method
-	I-Method
of	I-Method
-	I-Method
softmaxes	E-Method
to	O
achieve	O
significantly	O
lower	O
perplexities	S-Metric
.	O

PDR	S-Method
utilizes	O
the	O
symmetry	O
between	O
the	O
inputs	O
and	O
outputs	O
of	O
a	O
language	B-Method
model	E-Method
,	O
a	O
fact	O
that	O
is	O
also	O
exploited	O
in	O
weight	B-Method
tying	E-Method
(	O
)	O
.	O

Our	O
method	O
can	O
be	O
used	O
with	O
untied	O
weights	O
as	O
well	O
.	O

Although	O
motivated	O
by	O
language	B-Method
modeling	E-Method
,	O
PDR	S-Method
can	O
also	O
be	O
applied	O
to	O
seq2seq	B-Method
models	E-Method
with	O
shared	O
input	O
-	O
output	O
vocabularies	O
,	O
such	O
as	O
those	O
used	O
for	O
text	B-Task
summarization	E-Task
and	O
neural	B-Task
machine	I-Task
translation	E-Task
(	O
with	O
byte	O
pair	O
encoding	O
of	O
words	O
)	O
(	O
)	O
.	O

Regularizing	O
the	O
training	S-Material
of	O
an	O
LSTM	S-Method
by	O
combining	O
the	O
main	O
objective	O
function	O
with	O
auxiliary	B-Task
tasks	E-Task
has	O
been	O
successfully	O
applied	O
to	O
several	O
tasks	O
in	O
NLP	B-Task
(	E-Task
)	O
.	O

In	O
fact	O
,	O
a	O
popular	O
choice	O
for	O
the	O
auxiliary	B-Task
task	E-Task
is	O
language	B-Method
modeling	E-Method
itself	O
.	O

This	O
in	O
turn	O
is	O
related	O
to	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
(	O
)	O
.	O

Specialized	B-Method
architectures	E-Method
like	O
Recurrent	B-Method
Highway	I-Method
Networks	E-Method
(	O
)	O
and	O
NAS	S-Method
(	O
)	O
have	O
been	O
successfully	O
used	O
to	O
achieve	O
competitive	O
performance	O
in	O
language	B-Task
modeling	E-Task
.	O

The	O
former	O
one	O
makes	O
the	O
hidden	O
-	O
to	O
-	O
hidden	O
transition	O
function	O
more	O
complex	O
allowing	O
for	O
more	O
refined	O
information	O
flow	O
.	O

Such	O
architectures	O
are	O
especially	O
important	O
for	O
character	B-Task
level	I-Task
language	I-Task
modeling	E-Task
where	O
strong	O
results	O
have	O
been	O
shown	O
using	O
Fast	B-Method
-	I-Method
Slow	I-Method
RNNs	E-Method
(	O
)	O
,	O
a	O
two	B-Method
level	I-Method
architecture	E-Method
where	O
the	O
slowly	B-Method
changing	I-Method
recurrent	I-Method
network	E-Method
tries	O
to	O
capture	O
more	O
long	O
range	O
dependencies	O
.	O

The	O
use	O
of	O
historical	O
information	O
can	O
greatly	O
help	O
language	B-Method
models	E-Method
deal	O
with	O
long	O
range	O
dependencies	O
as	O
shown	O
by	O
.	O

Finally	O
,	O
in	O
a	O
recent	O
paper	O
,	O
achieve	O
improved	O
performance	O
for	O
language	B-Task
modeling	E-Task
by	O
using	O
frequency	B-Method
agnostic	I-Method
word	I-Method
embeddings	E-Method
,	O
a	O
technique	O
orthogonal	O
to	O
and	O
combinable	O
with	O
PDR	S-Method
.	O

bibliography	O
:	O
References	O
document	O
:	O
DialogueRNN	S-Method
:	O
An	O
Attentive	O
RNN	S-Method
for	O
Emotion	B-Task
Detection	E-Task
in	O
Conversations	B-Task
Emotion	I-Task
detection	E-Task
in	O
conversations	O
is	O
a	O
necessary	O
step	O
for	O
a	O
number	O
of	O
applications	O
,	O
including	O
opinion	B-Task
mining	E-Task
over	O
chat	O
history	O
,	O
social	O
media	O
threads	O
,	O
debates	O
,	O
argumentation	B-Task
mining	E-Task
,	O
understanding	B-Task
consumer	I-Task
feedback	E-Task
in	O
live	O
conversations	O
,	O
etc	O
.	O

Currently	O
systems	O
do	O
not	O
treat	O
the	O
parties	O
in	O
the	O
conversation	O
individually	O
by	O
adapting	O
to	O
the	O
speaker	O
of	O
each	O
utterance	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
new	O
method	O
based	O
on	O
recurrent	B-Method
neural	I-Method
networks	E-Method
that	O
keeps	O
track	O
of	O
the	O
individual	O
party	O
states	O
throughout	O
the	O
conversation	O
and	O
uses	O
this	O
information	O
for	O
emotion	B-Task
classification	E-Task
.	O

Our	O
model	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
a	O
significant	O
margin	O
on	O
two	O
different	O
datasets	O
.	O

section	O
:	O
Introduction	O
Emotion	B-Task
detection	E-Task
in	O
conversation	O
attracts	O
increasing	O
attention	O
of	O
the	O
community	O
due	O
to	O
its	O
applications	O
in	O
many	O
important	O
tasks	O
such	O
as	O
opinion	B-Task
mining	E-Task
over	O
chat	O
history	O
and	O
social	O
media	O
threads	O
in	O
YouTube	O
,	O
Facebook	O
,	O
Twitter	O
,	O
etc	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
method	O
based	O
on	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
that	O
can	O
cater	O
to	O
these	O
needs	O
by	O
processing	O
the	O
huge	O
amount	O
of	O
available	O
conversational	O
data	O
.	O

Current	O
systems	O
,	O
including	O
the	O
state	O
of	O
the	O
art	O
,	O
do	O
not	O
distinguish	O
different	O
parties	O
in	O
a	O
conversation	O
in	O
a	O
meaningful	O
way	O
.	O

They	O
are	O
not	O
aware	O
of	O
the	O
speaker	O
of	O
a	O
given	O
utterance	O
.	O

In	O
contrast	O
,	O
we	O
model	O
individual	O
party	O
with	O
party	O
states	O
,	O
as	O
the	O
conversation	O
flows	O
,	O
basing	O
on	O
the	O
utterance	O
,	O
the	O
context	O
,	O
and	O
current	O
party	O
state	O
.	O

Our	O
model	O
is	O
based	O
on	O
the	O
assumption	O
that	O
there	O
are	O
three	O
major	O
aspects	O
relevant	O
to	O
the	O
emotion	O
in	O
a	O
conversation	O
:	O
the	O
speaker	O
,	O
the	O
context	O
from	O
the	O
preceding	O
utterances	O
,	O
and	O
the	O
emotion	O
of	O
the	O
preceding	O
utterances	O
.	O

These	O
three	O
aspects	O
are	O
not	O
necessarily	O
independent	O
,	O
but	O
their	O
separate	O
modeling	O
significantly	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
(	O
tab	O
:	O
results	O
-	O
text	O
)	O
.	O

In	O
dyadic	O
conversations	O
,	O
the	O
parties	O
have	O
distinct	O
roles	O
.	O

Hence	O
,	O
to	O
extract	O
the	O
context	O
,	O
it	O
is	O
crucial	O
to	O
consider	O
the	O
preceding	O
turns	O
of	O
both	O
speaker	O
and	O
listener	O
at	O
a	O
given	O
moment	O
(	O
fig	O
:	O
example	O
)	O
.	O

Our	O
DialogueRNN	S-Method
employs	O
three	O
gated	B-Method
recurrent	I-Method
units	E-Method
(	O
GRU	S-Method
)	O
to	O
model	O
these	O
aspects	O
.	O

The	O
incoming	O
utterance	O
is	O
fed	O
into	O
two	O
GRUs	S-Method
called	O
global	B-Method
GRU	E-Method
and	O
party	B-Method
GRU	E-Method
to	O
update	O
the	O
context	O
and	O
party	O
state	O
,	O
respectively	O
.	O

The	O
global	B-Method
GRU	E-Method
encodes	O
corresponding	O
party	O
information	O
while	O
encoding	O
an	O
utterance	O
.	O

Attending	O
over	O
this	O
GRU	S-Method
gives	O
contextual	B-Method
representation	E-Method
that	O
has	O
information	O
of	O
all	O
preceding	O
utterances	O
by	O
different	O
parties	O
in	O
the	O
conversation	O
.	O

The	O
speaker	O
state	O
depends	O
on	O
this	O
context	O
through	O
attention	O
and	O
the	O
speaker	O
’s	O
previous	O
state	O
.	O

This	O
ensures	O
that	O
at	O
time	O
,	O
the	O
speaker	O
state	O
directly	O
gets	O
information	O
from	O
the	O
speaker	O
’s	O
previous	O
state	O
and	O
global	O
GRU	S-Method
which	O
has	O
information	O
on	O
the	O
preceding	O
parties	O
.	O

Finally	O
,	O
the	O
updated	O
speaker	O
state	O
is	O
fed	O
into	O
the	O
emotion	O
GRU	S-Method
to	O
decode	O
the	O
emotion	B-Method
representation	E-Method
of	O
the	O
given	O
utterance	O
,	O
which	O
is	O
used	O
for	O
emotion	B-Task
classification	E-Task
.	O

At	O
time	O
,	O
emotion	B-Method
GRU	I-Method
cell	E-Method
gets	O
the	O
emotion	O
representation	O
of	O
and	O
speaker	O
state	O
of	O
.	O

The	O
emotion	O
GRU	S-Method
,	O
along	O
with	O
the	O
global	B-Method
GRU	E-Method
,	O
plays	O
a	O
pivotal	O
role	O
in	O
inter	B-Task
-	I-Task
party	I-Task
relation	I-Task
modeling	E-Task
.	O

On	O
the	O
other	O
hand	O
,	O
party	B-Method
GRU	E-Method
models	O
relation	O
between	O
two	O
sequential	O
states	O
of	O
the	O
same	O
party	O
.	O

In	O
DialogueRNN	S-Method
,	O
all	O
these	O
three	O
different	O
types	O
of	O
GRUs	S-Method
are	O
connected	O
in	O
a	O
recurrent	O
manner	O
.	O

We	O
believe	O
that	O
DialogueRNN	S-Method
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
contextual	B-Method
emotion	I-Method
classifiers	E-Method
such	O
as	O
because	O
of	O
better	O
context	B-Method
representation	E-Method
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
sec	O
:	O
related	O
-	O
works	O
discusses	O
related	O
work	O
;	O
sec	O
:	O
method	O
provides	O
detailed	O
description	O
of	O
our	O
model	O
;	O
sec	O
:	O
experiments	O
,	O
sec	O
:	O
results	O
-	O
discussion	O
present	O
the	O
experimental	O
results	O
;	O
finally	O
,	O
sec	O
:	O
conclusion	O
concludes	O
the	O
paper	O
.	O

section	O
:	O
Related	O
Work	O
Emotion	B-Task
recognition	E-Task
has	O
attracted	O
attention	O
in	O
various	O
fields	O
such	O
as	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
psychology	S-Task
,	O
cognitive	B-Task
science	E-Task
,	O
and	O
so	O
on	O
.	O

ekman1993facial	O
ekman1993facial	O
found	O
correlation	O
between	O
emotion	O
and	O
facial	O
cues	O
.	O

datcu2008semantic	O
datcu2008semantic	O
fused	O
acoustic	O
information	O
with	O
visual	O
cues	O
for	O
emotion	B-Task
recognition	E-Task
.	O

alm2005emotions	O
alm2005emotions	O
introduced	O
text	B-Task
-	I-Task
based	I-Task
emotion	I-Task
recognition	E-Task
,	O
developed	O
in	O
the	O
work	O
of	O
strapparava2010annotating	O
strapparava2010annotating	O
.	O

wollmer2010context	O
wollmer2010context	O
used	O
contextual	O
information	O
for	O
emotion	B-Task
recognition	E-Task
in	O
multimodal	B-Task
setting	E-Task
.	O

Recently	O
,	O
poria	O
-	O
EtAl:2017:Long	O
poria	O
-	O
EtAl:2017:Long	O
successfully	O
used	O
RNN	S-Method
-	O
based	O
deep	O
networks	O
for	O
multimodal	B-Task
emotion	I-Task
recognition	E-Task
,	O
which	O
was	O
followed	O
by	O
other	O
works	O
.	O

Reproducing	B-Task
human	I-Task
interaction	E-Task
requires	O
deep	B-Task
understanding	I-Task
of	I-Task
conversation	E-Task
.	O

ruusuvuori2013emotion	O
ruusuvuori2013emotion	O
states	O
that	O
emotion	O
plays	O
a	O
pivotal	O
role	O
in	O
conversations	O
.	O

It	O
has	O
been	O
argued	O
that	O
emotional	O
dynamics	O
in	O
a	O
conversation	O
is	O
an	O
inter	O
-	O
personal	O
phenomenon	O
.	O

Hence	O
,	O
our	O
model	O
incorporates	O
inter	O
-	O
personal	O
interactions	O
in	O
an	O
effective	O
way	O
.	O

Further	O
,	O
since	O
conversations	O
have	O
a	O
natural	O
temporal	O
nature	O
,	O
we	O
adopt	O
the	O
temporal	O
nature	O
through	O
recurrent	B-Method
network	E-Method
.	O

Memory	B-Method
networks	E-Method
has	O
been	O
successful	O
in	O
several	O
NLP	B-Task
areas	E-Task
,	O
including	O
question	B-Task
answering	E-Task
,	O
machine	B-Task
translation	E-Task
,	O
speech	B-Task
recognition	E-Task
,	O
and	O
so	O
on	O
.	O

Thus	O
,	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
used	O
memory	B-Method
networks	E-Method
for	O
emotion	B-Task
recognition	E-Task
in	O
dyadic	O
conversations	O
,	O
where	O
two	O
distinct	O
memory	B-Method
networks	E-Method
enabled	O
inter	B-Task
-	I-Task
speaker	I-Task
interaction	E-Task
,	O
yielding	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

section	O
:	O
Methodology	O
subsection	O
:	O
Problem	O
Definition	O
Let	O
there	O
be	O
parties	O
/	O
participants	O
(	O
for	O
the	O
datasets	O
we	O
used	O
)	O
in	O
a	O
conversation	O
.	O

The	O
task	O
is	O
to	O
predict	O
the	O
emotion	O
labels	O
(	O
happy	O
,	O
sad	O
,	O
neutral	O
,	O
angry	O
,	O
excited	O
,	O
and	O
frustrated	O
)	O
of	O
the	O
constituent	O
utterances	O
,	O
where	O
utterance	O
is	O
uttered	O
by	O
party	O
,	O
while	O
being	O
the	O
mapping	O
between	O
utterance	O
and	O
index	O
of	O
its	O
corresponding	O
party	O
.	O

Also	O
,	O
is	O
the	O
utterance	B-Method
representation	E-Method
,	O
obtained	O
using	O
feature	B-Method
extractors	E-Method
described	O
below	O
.	O

subsection	O
:	O
Unimodal	B-Task
Feature	I-Task
Extraction	E-Task
For	O
a	O
fair	O
comparison	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
,	O
conversational	B-Method
memory	I-Method
networks	E-Method
(	O
CMN	S-Method
)	O
,	O
we	O
follow	O
identical	O
feature	B-Method
extraction	I-Method
procedures	E-Method
.	O

subsubsection	O
:	O
Textual	B-Method
Feature	I-Method
Extraction	E-Method
We	O
employ	O
convolutional	B-Method
neural	I-Method
networks	E-Method
(	O
CNN	S-Method
)	O
for	O
textual	S-Task
feature	O
extraction	O
.	O

Following	O
kim2014convolutional	O
kim2014convolutional	O
,	O
we	O
obtain	O
n	O
-	O
gram	O
features	O
from	O
each	O
utterance	O
using	O
three	O
distinct	O
convolution	B-Method
filters	E-Method
of	O
sizes	O
3	O
,	O
4	O
,	O
and	O
5	O
respectively	O
,	O
each	O
having	O
50	O
feature	O
-	O
maps	O
.	O

Outputs	O
are	O
then	O
subjected	O
to	O
max	B-Method
-	I-Method
pooling	E-Method
followed	O
by	O
rectified	B-Method
linear	I-Method
unit	E-Method
(	O
ReLU	S-Method
)	O
activation	O
.	O

These	O
activations	O
are	O
concatenated	O
and	O
fed	O
to	O
a	O
dimensional	B-Method
dense	I-Method
layer	E-Method
,	O
which	O
is	O
regarded	O
as	O
the	O
textual	S-Task
utterance	O
representation	O
.	O

This	O
network	O
is	O
trained	O
at	O
utterance	O
level	O
with	O
the	O
emotion	O
labels	O
.	O

subsubsection	O
:	O
Audio	B-Task
and	I-Task
Visual	I-Task
Feature	I-Task
Extraction	E-Task
Identical	O
to	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
we	O
use	O
3D	B-Method
-	I-Method
CNN	E-Method
and	O
openSMILE	S-Method
for	O
visual	B-Task
and	I-Task
acoustic	I-Task
feature	I-Task
extraction	E-Task
,	O
respectively	O
.	O

subsection	O
:	O
Our	O
Model	O
We	O
assume	O
that	O
the	O
emotion	O
of	O
an	O
utterance	O
in	O
a	O
conversation	O
depends	O
on	O
three	O
major	O
factors	O
:	O
the	O
speaker	O
.	O

the	O
context	O
given	O
by	O
the	O
preceding	O
utterances	O
.	O

the	O
emotion	O
behind	O
the	O
preceding	O
utterances	O
.	O

Our	O
model	O
DialogueRNN	S-Method
,	O
shown	O
in	O
fig	O
:	O
architecture	O
,	O
models	O
these	O
three	O
factors	O
as	O
follows	O
:	O
each	O
party	O
is	O
modeled	O
using	O
a	O
party	O
state	O
which	O
changes	O
as	O
and	O
when	O
that	O
party	O
utters	O
an	O
utterance	O
.	O

This	O
enables	O
the	O
model	O
to	O
track	O
the	O
parties	O
’	O
emotion	O
dynamics	O
through	O
the	O
conversations	O
,	O
which	O
is	O
related	O
to	O
the	O
emotion	O
behind	O
the	O
utterances	O
.	O

Furthermore	O
,	O
the	O
context	O
of	O
an	O
utterance	O
is	O
modeled	O
using	O
a	O
global	O
state	O
(	O
called	O
global	O
,	O
because	O
of	O
being	O
shared	O
among	O
the	O
parties	O
)	O
,	O
where	O
the	O
preceding	O
utterances	O
and	O
the	O
party	O
states	O
are	O
jointly	O
encoded	O
for	O
context	B-Method
representation	E-Method
,	O
necessary	O
for	O
accurate	O
party	B-Method
state	I-Method
representation	E-Method
.	O

Finally	O
,	O
the	O
model	O
infers	O
emotion	B-Method
representation	E-Method
from	O
the	O
party	O
state	O
of	O
the	O
speaker	O
along	O
with	O
the	O
preceding	O
speakers	O
’	O
states	O
as	O
context	O
.	O

This	O
emotion	B-Method
representation	E-Method
is	O
used	O
for	O
the	O
final	O
emotion	B-Task
classification	E-Task
.	O

We	O
use	O
GRU	O
cells	O
to	O
update	O
the	O
states	O
and	O
representations	O
.	O

Each	O
GRU	B-Method
cell	E-Method
computes	O
a	O
hidden	O
state	O
defined	O
as	O
,	O
where	O
is	O
the	O
current	O
input	O
and	O
is	O
the	O
previous	O
GRU	O
state	O
.	O

also	O
serves	O
as	O
the	O
current	O
GRU	O
output	O
.	O

We	O
provide	O
the	O
GRU	O
computation	O
details	O
in	O
the	O
supplementary	O
.	O

GRUs	S-Method
are	O
efficient	O
networks	O
with	O
trainable	O
parameters	O
:	O
and	O
.	O

We	O
model	O
the	O
emotion	B-Method
representation	E-Method
of	O
the	O
current	O
utterance	O
as	O
a	O
function	O
of	O
the	O
emotion	B-Method
representation	E-Method
of	O
the	O
previous	O
utterance	O
and	O
the	O
state	O
of	O
the	O
current	O
speaker	O
.	O

Finally	O
,	O
this	O
emotion	B-Method
representation	E-Method
is	O
sent	O
to	O
a	O
softmax	B-Method
layer	E-Method
for	O
emotion	B-Task
classification	E-Task
.	O

0.73	O
0.25	O
subsubsection	O
:	O
Global	O
State	O
(	O
Global	O
GRU	O
)	O
Global	O
state	O
aims	O
to	O
capture	O
the	O
context	O
of	O
a	O
given	O
utterance	O
by	O
jointly	O
encoding	O
utterance	O
and	O
speaker	O
state	O
.	O

Each	O
state	O
also	O
serves	O
as	O
speaker	B-Method
-	I-Method
specific	I-Method
utterance	I-Method
representation	E-Method
.	O

Attending	O
on	O
these	O
states	O
facilitates	O
the	O
inter	O
-	O
speaker	O
and	O
inter	O
-	O
utterance	O
dependencies	O
to	O
produce	O
improved	O
context	B-Method
representation	E-Method
.	O

The	O
current	O
utterance	O
changes	O
the	O
speaker	O
’s	O
state	O
from	O
to	O
.	O

We	O
capture	O
this	O
change	O
with	O
GRU	O
cell	O
with	O
output	O
size	O
,	O
using	O
and	O
:	O
where	O
is	O
the	O
size	O
of	O
global	O
state	O
vector	O
,	O
is	O
the	O
size	O
of	O
party	O
state	O
vector	O
,	O
,	O
,	O
,	O
,	O
,	O
is	O
party	O
state	O
size	O
,	O
and	O
represents	O
concatenation	O
.	O

subsubsection	O
:	O
Party	O
State	O
(	O
Party	O
GRU	O
)	O
DialogueRNN	S-Method
keeps	O
track	O
of	O
the	O
state	O
of	O
individual	O
speakers	O
using	O
fixed	O
size	O
vectors	O
through	O
out	O
the	O
conversation	O
.	O

These	O
states	O
are	O
representative	O
of	O
the	O
speakers	O
’	O
state	O
in	O
the	O
conversation	O
,	O
relevant	O
to	O
emotion	B-Task
classification	E-Task
.	O

We	O
update	O
these	O
states	O
based	O
on	O
the	O
current	O
(	O
at	O
time	O
)	O
role	O
of	O
a	O
participant	O
in	O
the	O
conversation	O
,	O
which	O
is	O
either	O
speaker	O
or	O
listener	O
,	O
and	O
the	O
incoming	O
utterance	O
.	O

These	O
state	O
vectors	O
are	O
initialized	O
with	O
null	O
vectors	O
for	O
all	O
the	O
participants	O
.	O

The	O
main	O
purpose	O
of	O
this	O
module	O
is	O
to	O
ensure	O
that	O
the	O
model	O
is	O
aware	O
of	O
the	O
speaker	O
of	O
each	O
utterance	O
and	O
handle	O
it	O
accordingly	O
.	O

subsubsection	O
:	O
Speaker	B-Method
Update	E-Method
(	O
Speaker	B-Method
GRU	E-Method
)	O
:	O
Speaker	O
usually	O
frames	O
the	O
response	O
based	O
on	O
the	O
context	O
,	O
which	O
is	O
the	O
preceding	O
utterances	O
in	O
the	O
conversation	O
.	O

Hence	O
,	O
we	O
capture	O
context	O
relevant	O
to	O
the	O
utterance	O
as	O
follows	O
:	O
where	O
are	O
preceding	O
global	O
states	O
(	O
)	O
,	O
,	O
,	O
and	O
.	O

In	O
eq:7	O
,	O
we	O
calculate	O
attention	O
scores	O
over	O
the	O
previous	O
global	O
states	O
representative	O
of	O
the	O
previous	O
utterances	O
.	O

This	O
assigns	O
higher	O
attention	O
scores	O
to	O
the	O
utterances	O
emotionally	O
relevant	O
to	O
.	O

Finally	O
,	O
in	O
eq:6	O
the	O
context	O
vector	O
is	O
calculated	O
by	O
pooling	O
the	O
previous	O
global	O
states	O
with	O
.	O

Now	O
,	O
we	O
employ	O
a	O
GRU	B-Method
cell	E-Method
to	O
update	O
the	O
current	O
speaker	O
state	O
to	O
the	O
new	O
state	O
based	O
on	O
incoming	O
utterance	O
and	O
context	O
using	O
GRU	O
cell	O
of	O
output	O
size	O
where	O
,	O
,	O
,	O
and	O
.	O

This	O
encodes	O
the	O
information	O
on	O
the	O
current	O
utterance	O
along	O
with	O
its	O
context	O
from	O
the	O
global	O
GRU	O
into	O
the	O
speaker	O
’s	O
state	O
,	O
which	O
helps	O
in	O
emotion	B-Task
classification	E-Task
down	O
the	O
line	O
.	O

subsubsection	O
:	O
Listener	O
Update	O
:	O
Listener	O
state	O
models	O
the	O
listeners	O
’	O
change	O
of	O
state	O
due	O
to	O
the	O
speaker	O
’s	O
utterance	O
.	O

We	O
tried	O
two	O
listener	B-Method
state	I-Method
update	I-Method
mechanisms	E-Method
:	O
Simply	O
keep	O
the	O
state	O
of	O
the	O
listener	O
unchanged	O
,	O
that	O
is	O
Employ	O
another	O
GRU	B-Method
cell	E-Method
to	O
update	O
the	O
listener	O
state	O
based	O
on	O
listener	O
visual	O
cues	O
(	O
facial	O
expression	O
)	O
and	O
its	O
context	O
,	O
as	O
where	O
,	O
,	O
,	O
and	O
.	O

Listener	O
visual	O
features	O
of	O
party	O
at	O
time	O
are	O
extracted	O
using	O
the	O
model	O
introduced	O
by	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1710	O
-	O
07557	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1710	O
-	O
07557	O
,	O
pretrained	O
on	O
FER2013	B-Material
dataset	E-Material
,	O
where	O
feature	B-Metric
size	E-Metric
.	O

The	O
simpler	O
first	O
approach	O
turns	O
out	O
to	O
be	O
sufficient	O
,	O
since	O
the	O
second	O
approach	O
yields	O
very	O
similar	O
result	O
while	O
increasing	O
number	O
of	O
parameters	O
.	O

This	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
listener	O
becomes	O
relevant	O
to	O
the	O
conversation	O
only	O
when	O
he	O
/	O
she	O
speaks	O
.	O

In	O
other	O
words	O
,	O
a	O
silent	O
party	O
has	O
no	O
influence	O
in	O
a	O
conversation	O
.	O

Now	O
,	O
when	O
a	O
party	O
speaks	O
,	O
we	O
update	O
his	O
/	O
her	O
state	O
with	O
context	O
which	O
contains	O
relevant	O
information	O
on	O
all	O
the	O
preceding	O
utterances	O
,	O
rendering	O
explicit	O
listener	O
state	O
update	O
unnecessary	O
.	O

This	O
is	O
shown	O
in	O
tab	O
:	O
results	O
-	O
text	O
.	O

subsubsection	O
:	O
Emotion	B-Method
Representation	E-Method
(	O
Emotion	B-Method
GRU	E-Method
)	O
We	O
infer	O
the	O
emotionally	B-Method
relevant	I-Method
representation	E-Method
of	O
utterance	O
from	O
the	O
speaker	O
’s	O
state	O
and	O
the	O
emotion	B-Method
representation	E-Method
of	O
the	O
previous	O
utterance	O
.	O

Since	O
context	O
is	O
important	O
to	O
the	O
emotion	O
of	O
the	O
incoming	O
utterance	O
,	O
feeds	O
fine	O
-	O
tuned	O
emotionally	O
relevant	O
contextual	O
information	O
from	O
other	O
the	O
party	O
states	O
into	O
the	O
emotion	B-Method
representation	E-Method
.	O

This	O
establishes	O
a	O
connection	O
between	O
the	O
speaker	O
state	O
and	O
the	O
other	O
party	O
states	O
.	O

Hence	O
,	O
we	O
model	O
with	O
a	O
GRU	B-Method
cell	E-Method
(	O
)	O
with	O
output	O
size	O
as	O
where	O
is	O
the	O
size	O
of	O
emotion	O
representation	O
vector	O
,	O
,	O
,	O
,	O
and	O
.	O

Since	O
speaker	O
state	O
gets	O
information	O
from	O
global	O
states	O
,	O
which	O
serve	O
as	O
speaker	B-Method
-	I-Method
specific	I-Method
utterance	I-Method
representation	E-Method
,	O
one	O
may	O
claim	O
that	O
this	O
way	O
the	O
model	O
already	O
has	O
access	O
to	O
the	O
information	O
on	O
other	O
parties	O
.	O

However	O
,	O
as	O
shown	O
in	O
the	O
ablation	O
study	O
(	O
sec	O
:	O
ablation	O
-	O
study	O
)	O
emotion	B-Method
GRU	E-Method
helps	O
to	O
improve	O
the	O
performance	O
by	O
directly	O
linking	O
states	O
of	O
preceding	O
parties	O
.	O

Further	O
,	O
we	O
believe	O
that	O
speaker	O
and	O
global	O
GRUs	O
(	O
,	O
)	O
jointly	O
act	O
similar	O
to	O
an	O
encoder	S-Method
,	O
whereas	O
emotion	B-Method
GRU	E-Method
serves	O
as	O
a	O
decoder	S-Method
.	O

subsubsection	O
:	O
Emotion	B-Task
Classification	E-Task
We	O
use	O
a	O
two	B-Method
-	I-Method
layer	I-Method
perceptron	E-Method
with	O
a	O
final	O
softmax	B-Method
layer	E-Method
to	O
calculate	O
emotion	O
-	O
class	O
probabilities	O
from	O
emotion	B-Method
representation	E-Method
of	O
utterance	O
and	O
then	O
we	O
pick	O
the	O
most	O
likely	O
emotion	O
class	O
:	O
where	O
,	O
,	O
,	O
,	O
,	O
and	O
is	O
the	O
predicted	O
label	O
for	O
utterance	O
.	O

subsubsection	O
:	O
Training	O
We	O
use	O
categorical	B-Metric
cross	I-Metric
-	I-Metric
entropy	E-Metric
along	O
with	O
L2	B-Method
-	I-Method
regularization	E-Method
as	O
the	O
measure	B-Metric
of	I-Metric
loss	I-Metric
(	E-Metric
)	O
during	O
training	O
:	O
where	O
is	O
the	O
number	O
of	O
samples	O
/	O
dialogues	O
,	O
is	O
the	O
number	O
of	O
utterances	O
in	O
sample	O
,	O
is	O
the	O
probability	O
distribution	O
of	O
emotion	O
labels	O
for	O
utterance	O
of	O
dialogue	O
,	O
is	O
the	O
expected	O
class	O
label	O
of	O
utterance	O
of	O
dialogue	O
,	O
is	O
the	O
L2	O
-	O
regularizer	O
weight	O
,	O
and	O
is	O
the	O
set	O
of	O
trainable	O
parameters	O
where	O
We	O
used	O
stochastic	B-Method
gradient	I-Method
descent	I-Method
based	I-Method
Adam	I-Method
optimizer	E-Method
to	O
train	O
our	O
network	O
.	O

Hyperparameters	S-Method
are	O
optimized	O
using	O
grid	B-Method
search	E-Method
(	O
values	O
are	O
added	O
to	O
the	O
supplementary	O
material	O
)	O
.	O

subsection	O
:	O
DialogueRNN	B-Method
Variants	E-Method
We	O
use	O
DialogueRNN	S-Method
(	O
sec	O
:	O
model	O
)	O
as	O
the	O
basis	O
for	O
the	O
following	O
models	O
:	O
subsubsection	O
:	O
DialogueRNN	S-Method
+	O
Listener	O
State	O
Update	O
(	O
DialogueRNN	S-Method
)	O
:	O
This	O
variant	O
updates	O
the	O
listener	O
state	O
based	O
on	O
the	O
the	O
resulting	O
speaker	O
state	O
,	O
as	O
described	O
in	O
eq:8	O
.	O

subsubsection	O
:	O
Bidirectional	B-Method
DialogueRNN	E-Method
(	O
BiDialogueRNN	S-Method
)	O
:	O
Bidirectional	B-Method
DialogueRNN	E-Method
is	O
analogous	O
to	O
bidirectional	B-Method
RNNs	E-Method
,	O
where	O
two	O
different	O
RNNs	S-Method
are	O
used	O
for	O
forward	O
and	O
backward	O
passes	O
of	O
the	O
input	O
sequence	O
.	O

Outputs	O
from	O
the	O
RNNs	S-Method
are	O
concatenated	O
in	O
sequence	O
level	O
.	O

Similarly	O
,	O
in	O
BiDialogueRNN	S-Method
,	O
the	O
final	O
emotion	B-Method
representation	E-Method
contains	O
information	O
from	O
both	O
past	O
and	O
future	O
utterances	O
in	O
the	O
dialogue	O
through	O
forward	O
and	O
backward	O
DialogueRNNs	O
respectively	O
,	O
which	O
provides	O
better	O
context	O
for	O
emotion	B-Task
classification	E-Task
.	O

subsubsection	O
:	O
DialogueRNN	S-Method
+	O
attention	O
(	O
DialogueRNN	S-Method
+	O
Att	O
)	O
:	O
For	O
each	O
emotion	B-Method
representation	E-Method
,	O
attention	O
is	O
applied	O
over	O
all	O
surrounding	O
emotion	O
representations	O
in	O
the	O
dialogue	O
by	O
matching	O
them	O
with	O
(	O
eq	O
:	O
beta	O
,	O
eq	O
:	O
beta	O
-	O
2	O
)	O
.	O

This	O
provides	O
context	O
from	O
the	O
relevant	O
(	O
based	O
on	O
attention	O
score	O
)	O
future	O
and	O
preceding	O
utterances	O
.	O

subsubsection	O
:	O
Bidirectional	O
DialogueRNN	S-Method
+	O
Emotional	O
attention	O
(	O
BiDialogueRNN	S-Method
+	O
Att	O
)	O
:	O
For	O
each	O
emotion	B-Method
representation	E-Method
of	O
BiDialogueRNN	S-Method
,	O
attention	O
is	O
applied	O
over	O
all	O
the	O
emotion	B-Method
representations	E-Method
in	O
the	O
dialogue	O
to	O
capture	O
context	O
from	O
the	O
other	O
utterances	O
in	O
dialogue	O
:	O
where	O
,	O
,	O
,	O
and	O
.	O

Further	O
,	O
are	O
fed	O
to	O
a	O
two	B-Method
-	I-Method
layer	I-Method
perceptron	E-Method
for	O
emotion	B-Task
classification	E-Task
,	O
as	O
in	O
eq:5	O
,	O
eq	O
:	O
c	O
-	O
6	O
,	O
eq	O
:	O
c	O
-	O
7	O
.	O

section	O
:	O
Experimental	O
Setting	O
subsection	O
:	O
Datasets	O
Used	O
We	O
use	O
two	O
emotion	O
detection	O
datasets	O
IEMOCAP	S-Material
and	O
AVEC	S-Material
to	O
evaluate	O
DialogueRNN	S-Method
.	O

We	O
partition	O
both	O
datasets	O
into	O
train	O
and	O
test	O
sets	O
with	O
roughly	O
ratio	O
such	O
that	O
the	O
partitions	O
do	O
not	O
share	O
any	O
speaker	O
.	O

table	O
:	O
dataset	O
shows	O
the	O
distribution	O
of	O
train	O
and	O
test	O
samples	O
for	O
both	O
dataset	O
.	O

subsubsection	O
:	O
IEMOCAP	S-Material
:	O
IEMOCAP	S-Material
dataset	O
contains	O
videos	O
of	O
two	O
-	O
way	O
conversations	O
of	O
ten	O
unique	O
speakers	O
,	O
where	O
only	O
the	O
first	O
eight	O
speakers	O
from	O
session	O
one	O
to	O
four	O
belong	O
to	O
the	O
train	O
-	O
set	O
.	O

Each	O
video	O
contains	O
a	O
single	O
dyadic	O
dialogue	O
,	O
segmented	O
into	O
utterances	O
.	O

The	O
utterances	O
are	O
annotated	O
with	O
one	O
of	O
six	O
emotion	O
labels	O
,	O
which	O
are	O
happy	O
,	O
sad	O
,	O
neutral	O
,	O
angry	O
,	O
excited	O
,	O
and	O
frustrated	O
.	O

subsubsection	O
:	O
AVEC	S-Material
:	O
AVEC	B-Material
dataset	E-Material
is	O
a	O
modification	O
of	O
SEMAINE	O
database	O
containing	O
interactions	O
between	O
humans	O
and	O
artificially	O
intelligent	O
agents	O
.	O

Each	O
utterance	O
of	O
a	O
dialogue	O
is	O
annotated	O
with	O
four	O
real	O
valued	O
affective	O
attributes	O
:	O
valence	O
(	O
)	O
,	O
arousal	O
(	O
)	O
,	O
expectancy	O
(	O
)	O
,	O
and	O
power	O
(	O
)	O
.	O

The	O
annotations	O
are	O
available	O
every	O
0.2	O
seconds	O
in	O
the	O
original	O
database	O
.	O

However	O
,	O
in	O
order	O
to	O
adapt	O
the	O
annotations	O
to	O
our	O
need	O
of	O
utterance	B-Task
-	I-Task
level	I-Task
annotation	E-Task
,	O
we	O
averaged	O
the	O
attributes	O
over	O
the	O
span	O
of	O
an	O
utterance	O
.	O

subsection	O
:	O
Baselines	O
and	O
State	O
of	O
the	O
Art	O
For	O
a	O
comprehensive	O
evaluation	O
of	O
DialogueRNN	S-Method
,	O
we	O
compare	O
our	O
model	O
with	O
the	O
following	O
baseline	O
methods	O
:	O
subsubsection	O
:	O
c	B-Method
-	I-Method
LSTM	E-Method
:	O
Biredectional	B-Method
LSTM	E-Method
is	O
used	O
to	O
capture	O
the	O
context	O
from	O
the	O
surrounding	O
utterances	O
to	O
generate	O
context	B-Method
-	I-Method
aware	I-Method
utterance	I-Method
representation	E-Method
.	O

However	O
,	O
this	O
model	O
does	O
not	O
differentiate	O
among	O
the	O
speakers	O
.	O

subsubsection	O
:	O
c	B-Method
-	I-Method
LSTM	I-Method
+	I-Method
Att	E-Method
:	O
In	O
this	O
variant	O
attention	O
is	O
applied	O
applied	O
to	O
the	O
c	B-Method
-	I-Method
LSTM	I-Method
output	E-Method
at	O
each	O
timestamp	O
by	O
following	O
eq	O
:	O
beta	O
,	O
eq	O
:	O
beta	O
-	O
2	O
.	O

This	O
provides	O
better	O
context	O
to	O
the	O
final	O
utterance	B-Method
representation	E-Method
.	O

subsubsection	O
:	O
TFN	S-Method
:	O
This	O
is	O
specific	O
to	O
multimodal	B-Task
scenario	E-Task
.	O

Tensor	B-Method
outer	I-Method
product	E-Method
is	O
used	O
to	O
capture	O
inter	O
-	O
modality	O
and	O
intra	O
-	O
modality	O
interactions	O
.	O

This	O
model	O
does	O
not	O
capture	O
context	O
from	O
surrounding	O
utterances	O
.	O

subsubsection	O
:	O
MFN	S-Method
:	O
Specific	O
to	O
multimodal	B-Task
scenario	E-Task
,	O
this	O
model	O
utilizes	O
multi	B-Method
-	I-Method
view	I-Method
learning	E-Method
by	O
modeling	O
view	O
-	O
specific	O
and	O
cross	O
-	O
view	O
interactions	O
.	O

Similar	O
to	O
TFN	S-Method
,	O
this	O
model	O
does	O
not	O
use	O
contextual	O
information	O
.	O

subsubsection	O
:	O
CNN	S-Method
:	O
This	O
is	O
identical	O
to	O
our	O
textual	S-Task
feature	O
extractor	O
network	O
(	O
sec	O
:	O
feature	B-Method
-	I-Method
extraction	E-Method
)	O
and	O
it	O
does	O
not	O
use	O
contextual	O
information	O
from	O
the	O
surrounding	O
utterances	O
.	O

subsubsection	O
:	O
Memnet	S-Method
:	O
As	O
described	O
in	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
the	O
current	O
utterance	O
is	O
fed	O
to	O
a	O
memory	B-Method
network	E-Method
,	O
where	O
the	O
memories	O
correspond	O
to	O
preceding	O
utterances	O
.	O

The	O
output	O
from	O
the	O
memory	B-Method
network	E-Method
is	O
used	O
as	O
the	O
final	O
utterance	B-Method
representation	E-Method
for	O
emotion	B-Task
classification	E-Task
.	O

subsubsection	O
:	O
CMN	S-Method
:	O
This	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
models	O
utterance	O
context	O
from	O
dialogue	O
history	O
using	O
two	O
distinct	O
GRUs	S-Method
for	O
two	O
speakers	O
.	O

Finally	O
,	O
utterance	B-Method
representation	E-Method
is	O
obtained	O
by	O
feeding	O
the	O
current	O
utterance	O
as	O
query	O
to	O
two	O
distinct	O
memory	B-Method
networks	E-Method
for	O
both	O
speakers	O
.	O

subsection	O
:	O
Modalities	O
We	O
evaluated	O
our	O
model	O
primarily	O
on	O
textual	S-Task
modality	O
.	O

However	O
,	O
to	O
substantiate	O
efficacy	O
of	O
our	O
model	O
in	O
multimodal	B-Task
scenario	E-Task
,	O
we	O
also	O
experimented	O
with	O
multimodal	O
features	O
.	O

section	O
:	O
Results	O
and	O
Discussion	O
We	O
compare	O
DialogueRNN	S-Method
and	O
its	O
variants	O
with	O
the	O
baselines	O
for	O
textual	S-Task
data	O
in	O
tab	O
:	O
results	O
-	O
text	O
.	O

As	O
expected	O
,	O
on	O
average	O
DialogueRNN	S-Method
outperforms	O
all	O
the	O
baseline	O
methods	O
,	O
including	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	S-Method
,	O
on	O
both	O
of	O
the	O
datasets	O
.	O

subsection	O
:	O
Comparison	O
with	O
the	O
State	O
of	O
the	O
Art	O
We	O
compare	O
the	O
performance	O
of	O
DialogueRNN	S-Method
against	O
the	O
performance	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	S-Method
on	O
IEMOCAP	S-Material
and	O
AVEC	B-Method
datasets	E-Method
for	O
textual	S-Task
modality	O
.	O

subsubsection	O
:	O
IEMOCAP	S-Material
As	O
evidenced	O
by	O
tab	O
:	O
results	O
-	O
text	O
,	O
for	O
IEMOCAP	S-Material
dataset	O
,	O
our	O
model	O
surpasses	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	S-Method
by	O
accuracy	S-Metric
and	O
f1	B-Metric
-	I-Metric
score	E-Metric
on	O
average	O
.	O

We	O
think	O
that	O
this	O
enhancement	O
is	O
caused	O
by	O
the	O
fundamental	O
differences	O
between	O
CMN	S-Method
and	O
DialogueRNN	S-Method
,	O
which	O
are	O
party	B-Method
state	I-Method
modeling	E-Method
with	O
in	O
eq:2	S-Method
,	O
speaker	O
specific	O
utterance	O
treatment	O
in	O
eq:2	O
,	O
eq:3	O
,	O
and	O
global	B-Method
state	I-Method
capturing	E-Method
with	O
in	O
eq:3	O
.	O

Since	O
we	O
deal	O
with	O
six	O
unbalanced	O
emotion	O
labels	O
,	O
we	O
also	O
explored	O
the	O
model	O
performance	O
for	O
individual	O
labels	O
.	O

DialogueRNN	S-Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	S-Method
in	O
five	O
out	O
of	O
six	O
emotion	O
classes	O
by	O
significant	O
margin	O
.	O

For	O
frustrated	O
class	O
,	O
DialogueRNN	S-Method
lags	O
behind	O
CMN	S-Method
by	O
f1	B-Metric
-	I-Metric
score	E-Metric
.	O

We	O
think	O
that	O
DialogueRNN	S-Method
may	O
surpass	O
CMN	S-Method
using	O
a	O
standalone	B-Method
classifier	E-Method
for	O
frustrated	O
class	O
.	O

However	O
,	O
it	O
can	O
be	O
observed	O
in	O
tab	O
:	O
results	O
-	O
text	O
that	O
some	O
of	O
the	O
other	O
variants	O
of	O
DialogueRNN	S-Method
,	O
like	O
BiDialogueRNN	S-Method
has	O
already	O
outperformed	O
CMN	S-Method
for	O
frustrated	O
class	O
.	O

subsubsection	O
:	O
AVEC	S-Material
DialogueRNN	S-Method
outperforms	O
CMN	S-Method
for	O
valence	O
,	O
arousal	O
,	O
expectancy	O
,	O
and	O
power	O
attributes	O
;	O
see	O
tab	O
:	O
results	O
-	O
text	O
.	O

It	O
yields	O
significantly	O
lower	O
mean	B-Metric
absolute	I-Metric
error	E-Metric
(	O
)	O
and	O
higher	O
Pearson	B-Metric
correlation	I-Metric
coefficient	E-Metric
(	O
)	O
for	O
all	O
four	O
attributes	O
.	O

We	O
believe	O
this	O
to	O
be	O
due	O
to	O
the	O
incorporation	O
of	O
party	O
state	O
and	O
emotion	O
GRU	O
,	O
which	O
are	O
missing	O
from	O
CMN	S-Method
.	O

subsection	O
:	O
DialogueRNN	S-Method
vs.	O
DialogueRNN	B-Method
Variants	E-Method
We	O
discuss	O
the	O
performance	O
of	O
different	O
DialogueRNN	S-Method
variants	O
on	O
IEMOCAP	S-Material
and	O
AVEC	B-Method
datasets	E-Method
for	O
textual	S-Task
modality	O
.	O

subsubsection	O
:	O
DialogueRNN	S-Method
:	O
Following	O
tab	O
:	O
results	O
-	O
text	O
,	O
using	O
explicit	B-Method
listener	I-Method
state	I-Method
update	E-Method
yields	O
slightly	O
worse	O
performance	O
than	O
regular	O
DialogueRNN	S-Method
.	O

This	O
is	O
true	O
for	O
both	O
IEMOCAP	S-Material
and	O
AVEC	B-Method
datasets	E-Method
in	O
general	O
.	O

However	O
,	O
the	O
only	O
exception	O
to	O
this	O
trend	O
is	O
for	O
happy	O
emotion	O
label	O
for	O
IEMOCAP	S-Material
,	O
where	O
DialogueRNN	S-Method
outperforms	O
DialogueRNN	S-Method
by	O
f1	B-Metric
-	I-Metric
score	E-Metric
.	O

We	O
surmise	O
that	O
,	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
listener	O
becomes	O
relevant	O
to	O
the	O
conversation	O
only	O
when	O
he	O
/	O
she	O
speaks	O
.	O

Now	O
,	O
in	O
DialogueRNN	S-Method
,	O
when	O
a	O
party	O
speaks	O
,	O
we	O
update	O
his	O
/	O
her	O
state	O
with	O
context	O
which	O
contains	O
relevant	O
information	O
on	O
all	O
the	O
preceding	O
utterances	O
,	O
rendering	O
explicit	O
listener	O
state	O
update	O
of	O
DialogueRNN	S-Method
unnecessary	O
.	O

subsubsection	O
:	O
BiDialogueRNN	S-Method
:	O
Since	O
BiDialogueRNN	S-Method
captures	O
context	O
from	O
the	O
future	O
utterances	O
,	O
we	O
expect	O
improved	O
performance	O
from	O
it	O
over	O
DialogueRNN	S-Method
.	O

This	O
is	O
confirmed	O
in	O
tab	O
:	O
results	O
-	O
text	O
,	O
where	O
BiDialogueRNN	S-Method
outperforms	O
DialogueRNN	S-Method
on	O
average	O
on	O
both	O
datasets	O
.	O

subsubsection	O
:	O
DialogueRNN	B-Method
+	I-Method
Attn	E-Method
:	O
DialogueRNN	B-Method
+	I-Method
Attn	E-Method
also	O
uses	O
information	O
from	O
the	O
future	O
utterances	O
.	O

However	O
,	O
here	O
we	O
take	O
information	O
from	O
both	O
past	O
and	O
future	O
utterances	O
by	O
matching	O
them	O
with	O
the	O
current	O
utterance	O
and	O
calculating	O
attention	O
score	O
over	O
them	O
.	O

This	O
provides	O
relevance	O
to	O
emotionally	O
important	O
context	O
utterances	O
,	O
yielding	O
better	O
performance	O
than	O
BiDialogueRNN	S-Method
.	O

The	O
improvement	O
over	O
BiDialogueRNN	S-Method
is	O
f1	B-Metric
-	I-Metric
score	E-Metric
for	O
IEMOCAP	S-Material
and	O
consistently	O
lower	O
and	O
higher	O
in	O
AVEC	S-Material
.	O

subsubsection	O
:	O
BiDialogueRNN	B-Method
+	I-Method
Attn	E-Method
:	O
Since	O
this	O
setting	O
generates	O
the	O
final	O
emotion	B-Method
representation	E-Method
by	O
attending	O
over	O
the	O
emotion	B-Method
representation	E-Method
from	O
BiDialogueRNN	S-Method
,	O
we	O
expect	O
better	O
performance	O
than	O
both	O
BiDialogueRNN	S-Method
and	O
DialogueRNN	B-Method
+	I-Method
Attn	E-Method
.	O

This	O
is	O
confirmed	O
in	O
tab	O
:	O
results	O
-	O
text	O
,	O
where	O
this	O
setting	O
performs	O
the	O
best	O
in	O
general	O
than	O
any	O
other	O
methods	O
discussed	O
,	O
on	O
both	O
datasets	O
.	O

This	O
setting	O
yields	O
higher	O
f1	B-Metric
-	I-Metric
score	E-Metric
on	O
average	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CMN	S-Method
and	O
higher	O
f1	B-Metric
-	I-Metric
score	E-Metric
than	O
vanilla	O
DialogueRNN	S-Method
for	O
IEMOCAP	S-Material
dataset	O
.	O

For	O
AVEC	B-Material
dataset	E-Material
also	O
,	O
this	O
setting	O
gives	O
the	O
best	O
performance	O
across	O
all	O
the	O
four	O
attributes	O
.	O

subsection	O
:	O
Multimodal	B-Task
Setting	E-Task
As	O
both	O
IEMOCAP	S-Material
and	O
AVEC	B-Material
dataset	E-Material
contain	O
multimodal	O
information	O
,	O
we	O
have	O
evaluated	O
DialogueRNN	S-Method
on	O
multimodal	O
features	O
as	O
used	O
and	O
provided	O
by	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
.	O

We	O
use	O
concatenation	O
of	O
the	O
unimodal	O
features	O
as	O
a	O
fusion	B-Method
method	E-Method
by	O
following	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
hazarika	O
-	O
EtAl:2018:N18	O
-	O
1	O
,	O
since	O
fusion	B-Method
mechanism	E-Method
is	O
not	O
a	O
focus	O
of	O
this	O
paper	O
.	O

Now	O
,	O
as	O
we	O
can	O
see	O
in	O
tab	O
:	O
results	O
-	O
multimodal	O
,	O
DialogueRNN	S-Method
significantly	O
outperforms	O
the	O
strong	O
baselines	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
CMN	S-Method
.	O

subsection	O
:	O
Case	O
Studies	O
subsubsection	O
:	O
Dependency	O
on	O
preceding	O
utterances	O
(	O
DialogueRNN	S-Method
)	O
One	O
of	O
the	O
crucial	O
components	O
of	O
DialogueRNN	S-Method
is	O
its	O
attention	B-Method
module	E-Method
over	O
the	O
outputs	O
of	O
global	B-Method
GRU	I-Method
(	E-Method
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
attention	O
vector	O
(	O
eq:7	O
)	O
over	O
the	O
history	O
of	O
a	O
given	O
test	O
utterance	O
compared	O
with	O
the	O
attention	O
vector	O
from	O
the	O
CMN	B-Method
model	E-Method
.	O

The	O
attention	O
of	O
our	O
model	O
is	O
more	O
focused	O
compared	O
with	O
CMN	S-Method
:	O
the	O
latter	O
gives	O
diluted	O
attention	B-Metric
scores	E-Metric
leading	O
to	O
misclassifications	O
.	O

We	O
observe	O
this	O
trend	O
of	O
focused	O
-	O
attention	O
across	O
cases	O
and	O
posit	O
that	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
confidence	B-Metric
indicator	E-Metric
.	O

Further	O
in	O
this	O
example	O
,	O
the	O
test	O
utterance	O
by	O
(	O
turn	O
44	O
)	O
comprises	O
of	O
a	O
change	O
in	O
emotion	O
from	O
neutral	O
to	O
frustrated	O
.	O

DialogueRNN	S-Method
anticipates	O
this	O
correctly	O
by	O
attending	O
to	O
turn	O
41	O
and	O
42	O
that	O
are	O
spoken	O
by	O
and	O
,	O
respectively	O
.	O

These	O
two	O
utterances	O
provide	O
self	O
and	O
inter	O
-	O
party	O
influences	O
that	O
trigger	O
the	O
emotional	O
shift	O
.	O

CMN	S-Method
,	O
however	O
,	O
fails	O
to	O
capture	O
such	O
dependencies	O
and	O
wrongly	O
predicts	O
neutral	O
emotion	O
.	O

subsubsection	O
:	O
Dependency	O
on	O
future	O
utterances	O
(	O
BiDialogueRNN	S-Method
+	O
Att	O
)	O
fig	O
:	O
case	O
-	O
study	O
visualizes	O
the	O
(	O
eq	O
:	O
beta	O
)	O
attention	O
over	O
the	O
emotion	B-Method
representations	E-Method
for	O
a	O
segment	O
of	O
a	O
conversation	O
between	O
a	O
couple	O
.	O

In	O
the	O
discussion	O
,	O
the	O
woman	O
(	O
)	O
is	O
initially	O
at	O
a	O
neutral	O
state	O
,	O
whereas	O
the	O
man	O
(	O
)	O
is	O
angry	O
throughout	O
.	O

The	O
figure	O
reveals	O
that	O
the	O
emotional	O
attention	O
of	O
the	O
woman	O
is	O
localized	O
to	O
the	O
duration	O
of	O
her	O
neutral	O
state	O
(	O
turns	O
1	O
-	O
16	O
approximately	O
)	O
.	O

For	O
example	O
,	O
in	O
the	O
dialogue	O
,	O
turns	O
,	O
and	O
strongly	O
attend	O
to	O
turn	O
.	O

Interestingly	O
,	O
turn	O
attends	O
to	O
both	O
past	O
(	O
turn	O
)	O
and	O
future	O
(	O
turn	O
)	O
utterances	O
.	O

Similar	O
trend	O
across	O
other	O
utterances	O
establish	O
inter	O
-	O
dependence	O
between	O
emotional	O
states	O
of	O
future	O
and	O
past	O
utterances	O
.	O

0.49	O
0.49	O
0.49	O
0.49	O
The	O
beneficial	O
consideration	O
of	O
future	O
utterances	O
through	O
is	O
also	O
apparent	O
through	O
turns	O
.	O

These	O
utterances	O
focus	O
on	O
the	O
distant	O
future	O
(	O
turn	O
)	O
where	O
the	O
man	O
is	O
at	O
an	O
enraged	O
state	O
,	O
thus	O
capturing	O
emotional	O
correlations	O
across	O
time	O
.	O

Although	O
,	O
turn	O
is	O
misclassified	O
by	O
our	O
model	O
,	O
it	O
still	O
manages	O
to	O
infer	O
a	O
related	O
emotional	O
state	O
(	O
anger	O
)	O
against	O
the	O
correct	O
state	O
(	O
frustrated	O
)	O
.	O

We	O
analyze	O
more	O
of	O
this	O
trend	O
in	O
section	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Dependency	O
on	O
distant	O
context	O
For	O
all	O
correct	O
predictions	O
in	O
the	O
IEMOCAP	S-Material
test	O
set	O
in	O
fig	O
:	O
DeltaTTrend	O
we	O
summarize	O
the	O
distribution	O
over	O
the	O
relative	O
distance	O
between	O
test	O
utterance	O
and	O
(	O
)	O
highest	O
attended	O
utterance	O
–	O
either	O
in	O
the	O
history	O
or	O
future	O
–	O
in	O
the	O
conversation	O
.	O

This	O
reveals	O
a	O
decreasing	O
trend	O
with	O
the	O
highest	O
dependence	O
being	O
within	O
the	O
local	O
context	O
.	O

However	O
,	O
a	O
significant	O
portion	O
of	O
the	O
test	O
utterances	O
(	O
)	O
,	O
attend	O
to	O
utterances	O
that	O
are	O
to	O
turns	O
away	O
from	O
themselves	O
,	O
which	O
highlights	O
the	O
important	O
role	O
of	O
long	O
-	O
term	O
emotional	O
dependencies	O
.	O

Such	O
cases	O
primarily	O
occur	O
in	O
conversations	O
that	O
maintain	O
a	O
specific	O
affective	O
tone	O
and	O
do	O
not	O
incur	O
frequent	O
emotional	O
shifts	O
.	O

fig	O
:	O
DistantAttention	O
demonstrates	O
a	O
case	O
of	O
long	B-Task
-	I-Task
term	I-Task
context	I-Task
dependency	E-Task
.	O

The	O
presented	O
conversation	O
maintains	O
a	O
happy	O
mood	O
throughout	O
the	O
dialogue	O
.	O

Although	O
the	O
turn	O
comprising	O
the	O
sentence	O
Horrible	O
thing	O
.	O

I	O
hated	O
it	O
.	O

seems	O
to	O
be	O
a	O
negative	O
expression	O
,	O
when	O
seen	O
with	O
the	O
global	O
context	O
,	O
it	O
reveals	O
the	O
excitement	O
present	O
in	O
the	O
speaker	O
.	O

To	O
disambiguate	O
such	O
cases	O
,	O
our	O
model	O
attends	O
to	O
distant	O
utterances	O
in	O
the	O
past	O
(	O
turn	O
,	O
)	O
which	O
serve	O
as	O
prototypes	O
of	O
the	O
emotional	O
tonality	O
of	O
the	O
overall	O
conversation	O
.	O

subsection	O
:	O
Error	B-Method
Analysis	E-Method
A	O
noticeable	O
trend	O
in	O
the	O
predictions	O
is	O
the	O
high	O
level	O
of	O
cross	O
-	O
predictions	O
amongst	O
related	O
emotions	O
.	O

Most	O
of	O
the	O
misclassifications	O
by	O
the	O
model	O
for	O
happy	O
emotion	O
are	O
for	O
excited	O
class	O
.	O

Also	O
,	O
anger	O
and	O
frustrated	O
share	O
misclassifications	O
amongst	O
each	O
other	O
.	O

We	O
suspect	O
this	O
is	O
due	O
to	O
subtle	O
difference	O
between	O
those	O
emotion	O
pairs	O
,	O
resulting	O
in	O
harder	O
disambiguation	S-Task
.	O

Another	O
class	O
with	O
high	O
rate	O
of	O
false	B-Metric
-	I-Metric
positives	E-Metric
is	O
the	O
neutral	O
class	O
.	O

Primary	O
reason	O
for	O
this	O
could	O
be	O
its	O
majority	O
in	O
the	O
class	O
distribution	O
over	O
the	O
considered	O
emotions	O
.	O

At	O
the	O
dialogue	O
level	O
,	O
we	O
observe	O
that	O
a	O
significant	O
amount	O
of	O
errors	O
occur	O
at	O
turns	O
having	O
a	O
change	O
of	O
emotion	O
from	O
the	O
previous	O
turn	O
of	O
the	O
same	O
party	O
.	O

Across	O
all	O
the	O
occurrences	O
of	O
these	O
emotional	O
-	O
shifts	O
in	O
the	O
testing	O
set	O
,	O
our	O
model	O
correctly	O
predicts	O
instances	O
.	O

This	O
stands	O
less	O
as	O
compared	O
to	O
the	O
success	O
that	O
it	O
achieves	O
at	O
regions	O
of	O
no	O
emotional	O
-	O
shift	O
.	O

Changes	O
in	O
emotions	O
in	O
a	O
dialogue	O
is	O
a	O
complex	O
phenomenon	O
governed	O
by	O
latent	O
dynamics	O
.	O

Further	O
improvement	O
of	O
these	O
cases	O
remain	O
as	O
an	O
open	O
area	O
of	O
research	O
.	O

subsection	O
:	O
Ablation	B-Task
Study	E-Task
The	O
main	O
novelty	O
of	O
our	O
method	O
is	O
the	O
introduction	O
of	O
party	O
state	O
and	O
emotion	B-Method
GRU	I-Method
(	E-Method
)	O
.	O

To	O
comprehensively	O
study	O
the	O
impact	O
of	O
these	O
two	O
components	O
,	O
we	O
remove	O
them	O
one	O
at	O
a	O
time	O
and	O
evaluate	O
their	O
impact	O
on	O
IEMOCAP	S-Material
.	O

As	O
expected	O
,	O
following	O
tab	O
:	O
ablation	S-Task
,	O
party	O
state	O
stands	O
very	O
important	O
,	O
as	O
without	O
its	O
presence	O
the	O
performance	O
falls	O
by	O
.	O

We	O
suspect	O
that	O
party	O
state	O
helps	O
in	O
extracting	O
useful	O
contextual	O
information	O
relevant	O
to	O
parties	O
’	O
emotion	O
.	O

Emotion	B-Method
GRU	E-Method
is	O
also	O
impactful	O
,	O
but	O
less	O
than	O
party	O
state	O
,	O
as	O
its	O
absence	O
causes	O
performance	O
to	O
fall	O
by	O
only	O
.	O

We	O
believe	O
the	O
reason	O
to	O
be	O
the	O
lack	O
of	O
context	O
flow	O
from	O
the	O
other	O
parties	O
’	O
states	O
through	O
the	O
emotion	B-Method
representation	E-Method
of	O
the	O
preceding	O
utterances	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
an	O
RNN	S-Method
-	O
based	O
neural	O
architecture	O
for	O
emotion	B-Task
detection	E-Task
in	O
a	O
conversation	O
.	O

In	O
contrast	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
,	O
CMN	S-Method
,	O
our	O
method	O
treats	O
each	O
incoming	O
utterance	O
taking	O
into	O
account	O
characteristics	O
of	O
the	O
speaker	O
,	O
which	O
gives	O
finer	O
context	O
to	O
the	O
utterance	O
.	O

Our	O
model	O
outperforms	O
the	O
current	O
state	O
of	O
the	O
art	O
on	O
two	O
distinct	O
datasets	O
in	O
both	O
textual	S-Task
and	O
multimodal	B-Task
setting	E-Task
.	O

Our	O
method	O
is	O
designed	O
to	O
be	O
scalable	O
to	O
multi	B-Task
-	I-Task
party	I-Task
setting	E-Task
with	O
more	O
than	O
two	O
speakers	O
,	O
though	O
we	O
could	O
not	O
test	O
it	O
due	O
to	O
unavailability	O
of	O
a	O
multi	O
-	O
party	O
conversation	O
dataset	O
with	O
emotion	O
labels	O
.	O

This	O
is	O
left	O
to	O
our	O
future	O
work	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Supplementary	O
Material	O
subsection	O
:	O
GRU	S-Method
Details	O
We	O
use	O
GRU	O
cells	O
are	O
defined	O
as	O
,	O
where	O
:	O
is	O
the	O
current	O
input	O
,	O
is	O
the	O
previous	O
GRU	O
output	O
,	O
and	O
is	O
the	O
current	O
GRU	O
output	O
.	O

and	O
are	O
GRU	O
parameters	O
.	O

,	O
are	O
refresh	O
gate	O
and	O
update	O
gate	O
respectively	O
.	O

is	O
the	O
candidate	O
output	O
.	O

stands	O
for	O
hadamard	O
product	O
.	O

[	O
b	O
]	O
DialogueRNN	S-Method
algorithm	O
[	O
1	O
]	O
DialogueRNN	S-Method
,	O
=	O
utterances	O
in	O
the	O
conversation	O
,	O
S	O
=	O
speakers	O
Initialize	O
the	O
participant	O
states	O
with	O
null	O
vector	O
:	O
i:	O
[	O
1	O
,	O
M	O
]	O
Set	O
the	O
initial	O
global	O
and	O
emotional	O
state	O
as	O
null	O
vector	O
:	O
Pass	O
the	O
dialogue	O
through	O
RNN	S-Method
:	O
t:	O
[	O
1	O
,	O
N	O
]	O
return	O
DialogueCell	O
,	O
,	O
,	O
,	O
Update	O
global	O
state	O
:	O
Get	O
context	O
from	O
preceding	O
global	O
states	O
:	O
Update	O
participant	O
states	O
:	O
i:	O
[	O
1	O
,	O
M	O
]	O
Update	O
speaker	O
state	O
:	O
Update	O
listener	O
state	O
:	O
Update	O
emotion	B-Method
representation	E-Method
:	O
return	O
document	O
:	O
Deep	O
Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
for	O
Face	B-Task
Alignment	E-Task
Facial	O
landmarks	O
are	O
highly	O
correlated	O
with	O
each	O
other	O
since	O
a	O
certain	O
landmark	O
can	O
be	O
estimated	O
by	O
its	O
neighboring	O
landmarks	O
.	O

Most	O
of	O
the	O
existing	O
deep	B-Method
learning	I-Method
methods	E-Method
only	O
use	O
one	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
called	O
shape	B-Method
prediction	I-Method
layer	E-Method
to	O
estimate	O
the	O
locations	O
of	O
facial	O
landmarks	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
deep	B-Method
learning	I-Method
framework	E-Method
named	O
Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
with	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
for	O
face	B-Task
alignment	E-Task
.	O

In	O
particular	O
,	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
semantically	O
relevant	O
landmarks	O
respectively	O
.	O

Challenging	O
landmarks	O
are	O
focused	O
firstly	O
,	O
and	O
each	O
cluster	O
of	O
landmarks	O
is	O
further	O
optimized	O
respectively	O
.	O

Moreover	O
,	O
to	O
reduce	O
the	O
model	B-Metric
complexity	E-Metric
,	O
we	O
propose	O
a	O
model	B-Method
assembling	I-Method
method	E-Method
to	O
integrate	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
into	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
.	O

Extensive	O
experiments	O
demonstrate	O
that	O
our	O
method	O
is	O
effective	O
for	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
with	O
real	O
-	O
time	O
performance	O
.	O

The	O
code	O
for	O
our	O
method	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
ZhiwenShao	O
/	O
MCNet	O
-	O
Extension	O
.	O

Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
,	O
Model	B-Task
Assembling	E-Task
,	O
Face	B-Task
Alignment	E-Task
section	O
:	O
Introduction	O
Face	B-Task
alignment	E-Task
refers	O
to	O
detecting	B-Task
facial	I-Task
landmarks	E-Task
such	O
as	O
eye	O
centers	O
,	O
nose	O
tip	O
,	O
and	O
mouth	O
corners	O
.	O

It	O
is	O
the	O
preprocessor	O
stage	O
of	O
many	O
face	B-Task
analysis	I-Task
tasks	E-Task
like	O
face	B-Task
animation	E-Task
,	O
face	B-Task
beautification	E-Task
,	O
and	O
face	B-Task
recognition	E-Task
.	O

A	O
robust	B-Task
and	I-Task
accurate	I-Task
face	I-Task
alignment	E-Task
is	O
still	O
challenging	O
in	O
unconstrained	B-Task
scenarios	E-Task
,	O
owing	O
to	O
severe	O
occlusions	O
and	O
large	O
appearance	O
variations	O
.	O

Most	O
conventional	O
methods	O
only	O
use	O
low	O
-	O
level	O
handcrafted	O
features	O
and	O
are	O
not	O
based	O
on	O
the	O
prevailing	O
deep	B-Method
neural	I-Method
networks	E-Method
,	O
which	O
limits	O
their	O
capacity	O
to	O
represent	O
highly	O
complex	O
faces	O
.	O

Recently	O
,	O
several	O
methods	O
use	O
deep	B-Method
networks	E-Method
to	O
estimate	O
shapes	O
from	O
input	O
faces	O
.	O

Sun	O
et	O
al	O
.	O

,	O
Zhou	O
et	O
al	O
.	O

,	O
and	O
Zhang	O
et	O
al	O
.	O

employed	O
cascaded	B-Method
deep	I-Method
networks	E-Method
to	O
refine	O
predicted	O
shapes	O
successively	O
.	O

Due	O
to	O
the	O
use	O
of	O
multiple	B-Method
networks	E-Method
,	O
these	O
methods	O
have	O
high	O
model	B-Metric
complexity	E-Metric
with	O
complicated	O
training	B-Method
processes	E-Method
.	O

Taking	O
this	O
into	O
account	O
,	O
Zhang	O
et	O
al	O
.	O

proposed	O
a	O
Tasks	B-Method
-	I-Method
Constrained	I-Method
Deep	I-Method
Convolutional	I-Method
Network	E-Method
(	O
TCDCN	S-Method
)	O
,	O
which	O
uses	O
only	O
one	O
deep	B-Method
network	E-Method
with	O
excellent	O
performance	O
.	O

However	O
,	O
it	O
needs	O
extra	O
labels	O
of	O
facial	O
attributes	O
for	O
training	O
samples	O
,	O
which	O
limits	O
its	O
universality	O
.	O

Each	O
facial	O
landmark	O
is	O
not	O
isolated	O
but	O
highly	O
correlated	O
with	O
adjacent	O
landmarks	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
facial	O
landmarks	O
along	O
the	O
chin	O
are	O
all	O
occluded	O
,	O
and	O
landmarks	O
around	O
the	O
mouth	O
are	O
partially	O
occluded	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
landmarks	O
on	O
the	O
right	O
side	O
of	O
face	O
are	O
almost	O
invisible	O
.	O

Therefore	O
,	O
landmarks	O
in	O
the	O
same	O
local	O
face	O
region	O
have	O
similar	O
properties	O
including	O
occlusion	O
and	O
visibility	O
.	O

It	O
is	O
observed	O
that	O
the	O
nose	O
can	O
be	O
localized	O
roughly	O
with	O
the	O
locations	O
of	O
eyes	O
and	O
mouth	O
.	O

There	O
are	O
also	O
structural	O
correlations	O
among	O
different	O
facial	O
parts	O
.	O

Motivated	O
by	O
this	O
fact	O
,	O
facial	O
landmarks	O
are	O
divided	O
into	O
several	O
clusters	O
based	O
on	O
their	O
semantic	O
relevance	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
deep	B-Method
learning	I-Method
framework	E-Method
named	O
Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
(	O
MCL	S-Method
)	O
to	O
exploit	O
the	O
strong	O
correlations	O
among	O
landmarks	O
.	O

In	O
particular	O
,	O
our	O
network	O
uses	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
to	O
predict	O
the	O
locations	O
of	O
landmarks	O
,	O
and	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
landmarks	O
respectively	O
.	O

By	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
,	O
challenging	O
landmarks	O
are	O
focused	O
firstly	O
,	O
and	O
each	O
cluster	O
of	O
landmarks	O
is	O
further	O
optimized	O
respectively	O
.	O

Moreover	O
,	O
to	O
decrease	O
the	O
model	B-Metric
complexity	E-Metric
,	O
we	O
propose	O
a	O
model	B-Method
assembling	I-Method
method	E-Method
to	O
integrate	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
into	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
.	O

The	O
entire	O
framework	O
reinforces	O
the	O
learning	B-Method
process	E-Method
of	O
each	O
landmark	O
with	O
a	O
low	O
model	B-Metric
complexity	E-Metric
.	O

The	O
main	O
contributions	O
of	O
this	O
study	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
multi	B-Method
-	I-Method
center	I-Method
learning	I-Method
framework	E-Method
for	O
exploiting	O
the	O
strong	O
correlations	O
among	O
landmarks	O
.	O

We	O
propose	O
a	O
model	B-Method
assembling	I-Method
method	E-Method
which	O
ensures	O
a	O
low	O
model	B-Metric
complexity	E-Metric
.	O

Extensive	O
experiments	O
demonstrate	O
that	O
our	O
method	O
is	O
effective	O
for	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
with	O
real	O
-	O
time	O
performance	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
is	O
structured	O
as	O
below	O
.	O

We	O
discuss	O
related	O
works	O
in	O
the	O
next	O
section	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
illuminate	O
the	O
structure	O
of	O
our	O
network	O
and	O
the	O
learning	B-Method
algorithm	E-Method
.	O

Extensive	O
experiments	O
are	O
carried	O
out	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Section	O
[	O
reference	O
]	O
concludes	O
this	O
work	O
.	O

section	O
:	O
Related	O
Work	O
We	O
review	O
researches	O
from	O
three	O
aspects	O
related	O
to	O
our	O
method	O
:	O
conventional	O
face	B-Task
alignment	E-Task
,	O
unconstrained	B-Task
face	I-Task
alignment	E-Task
,	O
face	B-Task
alignment	E-Task
via	O
deep	B-Method
learning	E-Method
.	O

subsection	O
:	O
Conventional	O
Face	B-Task
Alignment	E-Task
Conventional	O
face	B-Method
alignment	I-Method
methods	E-Method
can	O
be	O
classified	O
as	O
two	O
categories	O
:	O
template	B-Method
fitting	E-Method
and	O
regression	B-Method
-	I-Method
based	E-Method
.	O

Template	B-Method
fitting	I-Method
methods	E-Method
match	O
faces	O
by	O
constructing	O
shape	B-Method
templates	E-Method
.	O

Cootes	O
et	O
al	O
.	O

proposed	O
a	O
typical	O
template	B-Method
fitting	I-Method
method	E-Method
named	O
Active	B-Method
Appearance	I-Method
Model	E-Method
(	O
AAM	S-Method
)	O
,	O
which	O
minimizes	O
the	O
texture	O
residual	O
to	O
estimate	O
the	O
shape	O
.	O

Asthana	O
et	O
al	O
.	O

used	O
regression	B-Method
techniques	E-Method
to	O
learn	O
functions	O
from	O
response	O
maps	O
to	O
shapes	O
,	O
in	O
which	O
the	O
response	O
map	O
has	O
stronger	O
robustness	S-Metric
and	O
generalization	B-Metric
ability	E-Metric
than	O
texture	O
based	O
features	O
of	O
AAM	S-Method
.	O

Pedersoli	O
et	O
al	O
.	O

developed	O
the	O
mixture	B-Method
of	I-Method
trees	I-Method
of	I-Method
parts	I-Method
method	E-Method
by	O
extending	O
the	O
mixtures	O
from	O
trees	O
to	O
graphs	O
,	O
and	O
learned	O
a	O
deformable	B-Method
detector	E-Method
to	O
align	O
its	O
parts	O
to	O
faces	O
.	O

However	O
,	O
these	O
templates	O
are	O
not	O
complete	O
enough	O
to	O
cover	O
complex	O
variations	O
,	O
which	O
are	O
difficult	O
to	O
be	O
generalized	O
to	O
unseen	O
faces	O
.	O

Regression	B-Method
-	I-Method
based	I-Method
methods	E-Method
predict	O
the	O
locations	O
of	O
facial	O
landmarks	O
by	O
learning	O
a	O
regression	B-Method
function	E-Method
from	O
face	O
features	O
to	O
shapes	O
.	O

Cao	O
et	O
al	O
.	O

proposed	O
an	O
Explicit	B-Method
Shape	I-Method
Regression	E-Method
(	O
ESR	S-Method
)	O
method	O
to	O
predict	O
the	O
shape	O
increment	O
with	O
pixel	O
-	O
difference	O
features	O
.	O

Xiong	O
et	O
al	O
.	O

proposed	O
a	O
Supervised	B-Method
Descent	I-Method
Method	E-Method
(	O
SDM	S-Method
)	O
to	O
detect	B-Task
landmarks	E-Task
by	O
solving	O
the	O
nonlinear	B-Task
least	I-Task
squares	I-Task
problem	E-Task
,	O
with	O
Scale	O
-	O
Invariant	O
Feature	O
Transform	O
(	O
SIFT	O
)	O
features	O
and	O
linear	B-Method
regressors	E-Method
being	O
applied	O
.	O

Ren	O
et	O
al	O
.	O

used	O
a	O
locality	B-Method
principle	E-Method
to	O
extract	O
a	O
set	O
of	O
Local	O
Binary	O
Features	O
(	O
LBF	B-Method
)	E-Method
,	O
in	O
which	O
a	O
linear	B-Method
regression	E-Method
is	O
utilized	O
for	O
localizing	B-Task
landmarks	E-Task
.	O

Lee	O
et	O
al	O
.	O

employs	O
Cascade	B-Method
Gaussian	I-Method
Process	I-Method
Regression	I-Method
Trees	E-Method
(	O
cGPRT	S-Method
)	O
with	O
shape	B-Method
-	I-Method
indexed	I-Method
difference	I-Method
of	I-Method
Gaussian	I-Method
features	E-Method
to	O
achieve	O
face	B-Task
alignment	E-Task
.	O

It	O
has	O
a	O
better	O
generalization	B-Metric
ability	E-Metric
than	O
cascade	B-Method
regression	I-Method
trees	E-Method
,	O
and	O
shows	O
strong	O
robustness	S-Metric
against	O
geometric	O
variations	O
of	O
faces	O
.	O

Most	O
of	O
these	O
methods	O
give	O
an	O
initial	O
shape	O
and	O
refine	O
the	O
shape	O
in	O
an	O
iterative	O
manner	O
,	O
where	O
the	O
final	O
solutions	O
are	O
prone	O
to	O
getting	O
trapped	O
in	O
a	O
local	O
optimum	O
with	O
a	O
poor	O
initialization	O
.	O

In	O
contrast	O
,	O
our	O
method	O
uses	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
to	O
regress	O
from	O
raw	O
face	O
patches	O
to	O
the	O
locations	O
of	O
landmarks	O
.	O

subsection	O
:	O
Unconstrained	O
Face	B-Task
Alignment	E-Task
Large	O
pose	O
variations	O
and	O
severe	O
occlusions	O
are	O
major	O
challenges	O
in	O
unconstrained	O
environments	O
.	O

Unconstrained	B-Method
face	I-Method
alignment	I-Method
methods	E-Method
are	O
based	O
on	O
3D	B-Method
models	E-Method
or	O
deal	O
with	O
occlusions	O
explicitly	O
.	O

Many	O
methods	O
utilize	O
3D	B-Method
shape	I-Method
models	E-Method
to	O
solve	O
large	B-Task
-	I-Task
pose	I-Task
face	I-Task
alignment	E-Task
.	O

Nair	O
et	O
al	O
.	O

refined	O
the	O
fit	O
of	O
a	O
3D	B-Method
point	I-Method
distribution	I-Method
model	E-Method
to	O
perform	O
landmark	B-Task
detection	E-Task
.	O

Yu	O
et	O
al	O
.	O

used	O
a	O
cascaded	B-Method
deformable	I-Method
shape	I-Method
model	E-Method
to	O
detect	O
landmarks	B-Task
of	I-Task
large	I-Task
-	I-Task
pose	I-Task
faces	E-Task
.	O

Cao	O
et	O
al	O
.	O

employed	O
a	O
displaced	B-Method
dynamic	I-Method
expression	I-Method
regression	E-Method
to	O
estimate	O
the	O
3D	O
face	O
shape	O
and	O
2D	O
facial	O
landmarks	O
.	O

The	O
predicted	O
2D	O
landmarks	O
are	O
used	O
to	O
adjust	O
the	O
model	O
parameters	O
to	O
better	O
fit	O
the	O
current	O
user	O
.	O

Jeni	O
et	O
al	O
.	O

proposed	O
a	O
3D	B-Method
cascade	I-Method
regression	I-Method
method	E-Method
to	O
implement	O
3D	B-Task
face	I-Task
alignment	E-Task
,	O
which	O
can	O
maintain	O
the	O
pose	O
invariance	O
of	O
facial	O
landmarks	O
within	O
the	O
range	O
of	O
around	O
degrees	O
.	O

There	O
are	O
several	O
occlusion	B-Method
-	I-Method
free	I-Method
face	I-Method
alignment	I-Method
methods	E-Method
.	O

Burgos	O
-	O
Artizzu	O
et	O
al	O
.	O

developed	O
a	O
Robust	B-Method
Cascaded	I-Method
Pose	I-Method
Regression	E-Method
(	O
RCPR	S-Method
)	O
method	O
to	O
detect	O
occlusions	O
explicitly	O
,	O
and	O
uses	O
shape	O
-	O
indexed	O
features	O
to	O
regress	O
the	O
shape	O
increment	O
.	O

Yu	O
et	O
al	O
.	O

utilizes	O
a	O
Bayesian	B-Method
model	E-Method
to	O
merge	O
the	O
estimation	S-Task
results	O
from	O
multiple	O
regressors	O
,	O
in	O
which	O
each	O
regressor	O
is	O
trained	O
to	O
localize	O
facial	O
landmarks	O
with	O
a	O
specific	O
pre	O
-	O
defined	O
facial	O
part	O
being	O
occluded	O
.	O

Wu	O
et	O
al	O
.	O

proposed	O
a	O
Robust	B-Method
Facial	I-Method
Landmark	I-Method
Detection	E-Method
(	O
RFLD	S-Method
)	O
method	O
,	O
which	O
uses	O
a	O
robust	B-Method
cascaded	I-Method
regressor	E-Method
to	O
handle	O
complex	O
occlusions	O
and	O
large	O
head	O
poses	O
.	O

To	O
improve	O
the	O
performance	O
of	O
occlusion	B-Task
estimation	E-Task
,	O
landmark	O
visibility	O
probabilities	O
are	O
estimated	O
with	O
an	O
explicit	O
occlusion	O
constraint	O
.	O

Different	O
from	O
these	O
methods	O
,	O
our	O
method	O
is	O
not	O
based	O
on	O
3D	B-Method
models	E-Method
and	O
does	O
not	O
process	O
occlusions	O
explicitly	O
.	O

subsection	O
:	O
Face	B-Task
Alignment	E-Task
via	O
Deep	B-Method
Learning	I-Method
Deep	I-Method
learning	I-Method
methods	E-Method
can	O
be	O
divided	O
into	O
two	O
classes	O
:	O
single	B-Method
network	I-Method
based	E-Method
and	O
multiple	B-Method
networks	E-Method
based	O
.	O

Sun	O
et	O
al	O
.	O

estimated	O
the	O
locations	O
of	O
facial	O
landmarks	O
using	O
Cascaded	B-Method
Convolutional	I-Method
Neural	I-Method
Networks	E-Method
(	O
Cascaded	B-Method
CNN	E-Method
)	O
,	O
in	O
which	O
each	O
level	O
computes	O
averaged	O
estimated	O
shape	O
and	O
the	O
shape	O
is	O
refined	O
level	O
by	O
level	O
.	O

Zhou	O
et	O
al	O
.	O

used	O
multi	B-Method
-	I-Method
level	I-Method
deep	I-Method
networks	E-Method
to	O
detect	O
facial	O
landmarks	O
from	O
coarse	O
to	O
fine	O
.	O

Similarly	O
,	O
Zhang	O
et	O
al	O
.	O

proposed	O
Coarse	B-Method
-	I-Method
to	I-Method
-	I-Method
Fine	I-Method
Auto	I-Method
-	I-Method
encoder	I-Method
Networks	E-Method
(	O
CFAN	B-Method
)	E-Method
.	O

These	O
methods	O
all	O
use	O
multi	B-Method
-	I-Method
stage	I-Method
deep	I-Method
networks	E-Method
to	O
localize	O
landmarks	O
in	O
a	O
coarse	O
-	O
to	O
-	O
fine	O
manner	O
.	O

Instead	O
of	O
using	O
cascaded	B-Method
networks	E-Method
,	O
Honari	O
et	O
al	O
.	O

proposed	O
Recombinator	B-Method
Networks	E-Method
(	O
RecNet	S-Method
)	O
for	O
learning	O
coarse	B-Task
-	I-Task
to	I-Task
-	I-Task
fine	I-Task
feature	I-Task
aggregation	E-Task
with	O
multi	O
-	O
scale	O
input	O
maps	O
,	O
where	O
each	O
branch	O
extracts	O
features	O
based	O
on	O
current	O
maps	O
and	O
the	O
feature	O
maps	O
of	O
coarser	O
branches	O
.	O

A	O
few	O
methods	O
employ	O
a	O
single	O
network	O
to	O
solve	O
the	O
face	B-Task
alignment	I-Task
problem	E-Task
.	O

Shao	O
et	O
al	O
.	O

proposed	O
a	O
Coarse	B-Method
-	I-Method
to	I-Method
-	I-Method
Fine	I-Method
Training	E-Method
(	O
CFT	S-Method
)	O
method	O
to	O
learn	O
the	O
mapping	O
from	O
input	O
face	O
patches	O
to	O
estimated	O
shapes	O
,	O
which	O
searches	O
the	O
solutions	O
smoothly	O
by	O
adjusting	O
the	O
relative	O
weights	O
between	O
principal	O
landmarks	O
and	O
elaborate	O
landmarks	O
.	O

Zhang	O
et	O
al	O
.	O

used	O
the	O
TCDCN	S-Method
with	O
auxiliary	B-Method
facial	I-Method
attribute	I-Method
recognition	E-Method
to	O
predict	O
correlative	O
facial	O
properties	O
like	O
expression	O
and	O
pose	O
,	O
which	O
improves	O
the	O
performance	O
of	O
face	B-Task
alignment	E-Task
.	O

Xiao	O
et	O
al	O
.	O

proposed	O
a	O
Recurrent	B-Method
Attentive	I-Method
-	I-Method
Refinement	I-Method
(	I-Method
RAR	I-Method
)	I-Method
network	E-Method
for	O
face	B-Task
alignment	E-Task
under	O
unconstrained	O
conditions	O
,	O
where	O
shape	O
-	O
indexed	O
deep	O
features	O
and	O
temporal	O
information	O
are	O
taken	O
as	O
inputs	O
and	O
shape	O
predictions	O
are	O
recurrently	O
revised	O
.	O

Compared	O
to	O
these	O
methods	O
,	O
our	O
method	O
uses	O
only	O
one	O
network	O
and	O
is	O
independent	O
of	O
additional	O
facial	O
attributes	O
.	O

section	O
:	O
Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
for	O
Face	B-Task
Alignment	E-Task
subsection	O
:	O
Network	B-Method
Architecture	E-Method
The	O
architecture	O
of	O
our	O
network	O
MCL	S-Method
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

MCL	S-Method
contains	O
three	O
max	B-Method
-	I-Method
pooling	I-Method
layers	E-Method
,	O
each	O
of	O
which	O
follows	O
a	O
stack	O
of	O
two	O
convolutional	B-Method
layers	E-Method
proposed	O
by	O
VGGNet	S-Method
.	O

In	O
the	O
fourth	O
stack	O
of	O
convolutional	O
layers	O
,	O
we	O
use	O
a	O
convolutional	B-Method
layer	E-Method
with	O
feature	O
maps	O
above	O
two	O
convolutional	B-Method
layers	E-Method
.	O

We	O
perform	O
Batch	B-Method
Normalization	E-Method
(	O
BN	S-Method
)	O
and	O
Rectified	B-Method
Linear	I-Method
Unit	E-Method
(	O
ReLU	S-Method
)	O
after	O
each	O
convolution	S-Method
to	O
accelerate	O
the	O
convergence	O
of	O
our	O
network	O
.	O

Most	O
of	O
the	O
existing	O
deep	B-Method
learning	I-Method
methods	E-Method
such	O
as	O
TCDCN	S-Method
use	O
the	O
fully	B-Method
-	I-Method
connected	I-Method
layer	E-Method
to	O
extract	O
features	O
,	O
which	O
is	O
apt	O
to	O
overfit	O
and	O
hamper	O
the	O
generalization	B-Metric
ability	E-Metric
of	O
the	O
network	O
.	O

To	O
sidestep	O
these	O
problems	O
,	O
we	O
operate	O
Global	B-Method
Average	I-Method
Pooling	E-Method
on	O
the	O
last	O
convolutional	B-Method
layer	E-Method
to	O
extract	O
a	O
high	B-Method
-	I-Method
level	I-Method
feature	I-Method
representation	E-Method
,	O
which	O
computes	O
the	O
average	O
of	O
each	O
feature	O
map	O
.	O

With	O
this	O
improvement	O
,	O
our	O
MCL	S-Method
acquires	O
a	O
higher	O
representation	O
power	O
with	O
fewer	O
parameters	O
.	O

Face	B-Task
alignment	E-Task
can	O
be	O
regarded	O
as	O
a	O
nonlinear	B-Task
regression	I-Task
problem	E-Task
,	O
which	O
transforms	O
appearance	O
to	O
shape	O
.	O

A	O
transformation	O
is	O
used	O
for	O
modeling	O
this	O
highly	O
nonlinear	O
function	O
,	O
which	O
extracts	O
the	O
feature	O
from	O
the	O
input	O
face	O
image	O
,	O
formulated	O
as	O
where	O
,	O
corresponds	O
to	O
the	O
bias	O
,	O
and	O
is	O
a	O
composite	O
function	O
of	O
operations	O
including	O
convolution	S-Method
,	O
BN	S-Method
,	O
ReLU	S-Method
,	O
and	O
pooling	S-Method
.	O

Traditionally	O
,	O
only	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
is	O
used	O
,	O
which	O
limits	O
the	O
performance	O
.	O

In	O
contrast	O
,	O
our	O
MCL	S-Method
uses	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
,	O
each	O
of	O
which	O
emphasizes	O
on	O
the	O
detection	O
of	O
a	O
certain	O
cluster	O
of	O
landmarks	O
.	O

The	O
first	O
several	O
layers	O
are	O
shared	O
by	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
,	O
which	O
are	O
called	O
shared	O
layers	O
forming	O
the	O
composite	O
function	O
.	O

For	O
the	O
-	O
th	O
shape	O
prediction	O
layer	O
,	O
,	O
a	O
weight	O
matrix	O
is	O
used	O
to	O
connect	O
the	O
feature	O
,	O
where	O
and	O
are	O
the	O
number	O
of	O
shape	O
prediction	O
layers	O
and	O
landmarks	O
,	O
respectively	O
.	O

The	O
reason	O
why	O
we	O
train	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
to	O
predict	O
landmarks	O
instead	O
of	O
one	O
cluster	O
of	O
landmarks	O
is	O
that	O
different	O
facial	O
parts	O
have	O
correlations	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

To	O
decrease	O
the	O
model	B-Metric
complexity	E-Metric
,	O
we	O
use	O
a	O
model	B-Method
assembling	I-Method
function	E-Method
to	O
integrate	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
into	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
,	O
which	O
is	O
formulated	O
as	O
where	O
is	O
the	O
assembled	O
weight	O
matrix	O
.	O

Specifically	O
,	O
,	O
,	O
,	O
,	O
where	O
is	O
the	O
-	O
th	O
cluster	O
of	O
indexes	O
of	O
landmarks	O
.	O

The	O
final	O
prediction	S-Task
is	O
defined	O
as	O
where	O
and	O
denote	O
the	O
predicted	O
x	O
-	O
coordinate	O
and	O
y	O
-	O
coordinate	O
of	O
the	O
-	O
th	O
landmark	O
respectively	O
.	O

Compared	O
to	O
other	O
typical	O
convolutional	B-Method
networks	E-Method
like	O
VGGNet	S-Method
,	O
GoogLe	B-Method
-	I-Method
Net	E-Method
,	O
and	O
ResNet	S-Method
,	O
our	O
network	O
MCL	S-Method
is	O
substantially	O
smaller	O
and	O
shallower	O
.	O

We	O
believe	O
that	O
such	O
a	O
concise	B-Method
structure	E-Method
is	O
efficient	O
for	O
estimating	B-Task
the	I-Task
locations	I-Task
of	I-Task
facial	I-Task
landmarks	E-Task
.	O

Firstly	O
,	O
face	B-Task
alignment	E-Task
aims	O
to	O
regress	O
coordinates	O
of	O
fewer	O
than	O
facial	O
landmarks	O
generally	O
,	O
which	O
demands	O
much	O
lower	O
model	B-Metric
complexity	E-Metric
than	O
visual	B-Task
recognition	I-Task
problems	E-Task
with	O
more	O
than	O
classes	O
.	O

Secondly	O
,	O
a	O
very	O
deep	B-Method
network	E-Method
may	O
fail	O
to	O
work	O
well	O
for	O
landmark	B-Task
detection	E-Task
owing	O
to	O
the	O
reduction	O
of	O
spatial	O
information	O
layer	O
by	O
layer	O
.	O

Other	O
visual	B-Task
localization	I-Task
tasks	E-Task
,	O
like	O
face	B-Task
detection	E-Task
,	O
usually	O
use	O
multiple	O
cascaded	B-Method
shallow	I-Method
networks	E-Method
rather	O
than	O
a	O
single	O
very	B-Method
deep	I-Method
network	E-Method
.	O

Finally	O
,	O
common	O
face	B-Task
alignment	I-Task
benchmarks	E-Task
only	O
contain	O
thousands	O
of	O
training	O
images	O
.	O

A	O
simple	O
network	O
is	O
not	O
easy	O
to	O
overfit	O
given	O
a	O
small	O
amount	O
of	O
raw	O
training	O
data	O
.	O

[	O
!	O
htb	O
]	O
Multi	B-Method
-	I-Method
Center	I-Method
Learning	E-Method
Algorithm	O
.	O

[	O
1	O
]	O
A	O
network	O
MCL	S-Method

,	O
,	O
,	O
initialized	O
.	O

.	O


Pre	O
-	O
train	O
shared	B-Method
layers	E-Method
and	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
until	O
convergence	O
;	O
Fix	O
the	O
parameters	O
of	O
the	O
first	O
six	O
convolutional	B-Method
layers	E-Method
and	O
fine	O
-	O
tune	O
subsequent	O
layers	O
until	O
convergence	O
;	O
Fine	O
-	O
tune	O
all	O
the	O
layers	O
until	O
convergence	O
;	O
to	O
Fix	O
and	O
fine	O
-	O
tune	O
the	O
-	O
th	O
shape	B-Method
prediction	I-Method
layer	E-Method
until	O
convergence	O
;	O
;	O
Return	O
.	O

subsection	O
:	O
Learning	B-Method
Algorithm	E-Method
The	O
overview	O
of	O
our	O
learning	B-Method
algorithm	E-Method
is	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

and	O
are	O
the	O
training	O
set	O
and	O
the	O
validation	O
set	O
respectively	O
.	O

is	O
the	O
set	O
of	O
parameters	O
including	O
weights	O
and	O
biases	O
of	O
our	O
network	O
MCL	S-Method
,	O
which	O
is	O
updated	O
using	O
Mini	B-Method
-	I-Method
Batch	I-Method
Stochastic	I-Method
Gradient	I-Method
Descent	E-Method
(	O
SGD	S-Method
)	O
at	O
each	O
iteration	O
.	O

The	O
face	B-Task
alignment	I-Task
loss	E-Task
is	O
defined	O
as	O
where	O
is	O
the	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
,	O
and	O
denote	O
the	O
ground	O
-	O
truth	O
x	O
-	O
coordinate	O
and	O
y	O
-	O
coordinate	O
of	O
the	O
-	O
th	O
landmark	O
respectively	O
,	O
and	O
is	O
the	O
ground	O
truth	O
inter	O
-	O
ocular	O
distance	O
between	O
the	O
eye	O
centers	O
.	O

Inter	B-Method
-	I-Method
ocular	I-Method
distance	I-Method
normalization	E-Method
provides	O
fair	O
comparisons	O
among	O
faces	O
with	O
different	O
size	O
,	O
and	O
reduces	O
the	O
magnitude	O
of	O
loss	S-Metric
to	O
speed	O
up	O
the	O
learning	B-Method
process	E-Method
.	O

During	O
training	S-Task
,	O
a	O
too	O
high	O
learning	B-Metric
rate	E-Metric
may	O
cause	O
the	O
missing	O
of	O
optimum	O
so	O
far	O
as	O
to	O
the	O
divergence	O
of	O
network	O
,	O
and	O
a	O
too	O
low	O
learning	B-Metric
rate	E-Metric
may	O
lead	O
to	O
falling	O
into	O
a	O
local	O
optimum	O
.	O

We	O
employ	O
a	O
low	O
initial	B-Metric
learning	I-Metric
rate	E-Metric
to	O
avoid	O
the	O
divergence	O
,	O
and	O
increase	O
the	O
learning	B-Metric
rate	E-Metric
when	O
the	O
loss	S-Metric
is	O
reduced	O
significantly	O
and	O
continue	O
the	O
training	O
procedure	O
.	O

subsubsection	S-Method
:	O
Pre	B-Method
-	I-Method
Training	E-Method
and	O
Weighting	B-Method
Fine	I-Method
-	I-Method
Tuning	E-Method
In	O
Step	O
[	O
reference	O
]	O
,	O
a	O
basic	B-Method
model	E-Method
(	O
BM	S-Method
)	O
with	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
is	O
pre	O
-	O
trained	O
to	O
learn	O
a	O
good	O
initial	O
solution	O
.	O

In	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
for	O
all	O
.	O

The	O
average	B-Metric
alignment	I-Metric
error	E-Metric
of	O
each	O
landmark	O
of	O
BM	O
on	O
are	O
respectively	O
,	O
which	O
are	O
averaged	O
over	O
all	O
the	O
images	O
.	O

The	O
landmarks	O
with	O
larger	O
errors	O
than	O
remaining	O
landmarks	O
are	O
treated	O
as	O
challenging	O
landmarks	O
.	O

In	O
Steps	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
the	O
detection	B-Task
of	I-Task
challenging	I-Task
landmarks	E-Task
by	O
assigning	O
them	O
larger	O
weights	O
.	O

The	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
is	O
proportional	O
to	O
its	O
alignment	B-Metric
error	E-Metric
as	O
Instead	O
of	O
fine	O
-	O
tuning	O
all	O
the	O
layers	O
from	O
BM	S-Method
directly	O
,	O
we	O
use	O
two	O
steps	O
to	O
search	O
the	O
solution	O
smoothly	O
.	O

Step	O
[	O
reference	O
]	O
searches	O
the	O
solution	O
without	O
deviating	O
from	O
BM	O
overly	O
.	O

Step	O
[	O
reference	O
]	O
searches	O
the	O
solution	O
within	O
a	O
larger	O
range	O
on	O
the	O
basis	O
of	O
the	O
previous	O
step	O
.	O

This	O
stage	O
is	O
named	O
weighting	B-Task
fine	I-Task
-	I-Task
tuning	E-Task
,	O
which	O
learns	O
a	O
weighting	B-Method
model	E-Method
(	O
WM	S-Method
)	O
with	O
higher	O
localization	B-Metric
accuracy	E-Metric
of	O
challenging	O
landmarks	O
.	O

subsubsection	O
:	O
Multi	B-Method
-	I-Method
Center	I-Method
Fine	I-Method
-	I-Method
Tuning	E-Method
and	O
Model	B-Method
Assembling	E-Method
The	O
face	O
is	O
partitioned	O
into	O
seven	O
parts	O
according	O
to	O
its	O
semantic	O
structure	O
:	O
left	O
eye	O
,	O
right	O
eye	O
,	O
nose	O
,	O
mouth	O
,	O
left	O
contour	O
,	O
chin	O
,	O
and	O
right	O
contour	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
different	O
labeling	O
patterns	O
of	O
,	O
,	O
and	O
facial	O
landmarks	O
are	O
partitioned	O
into	O
,	O
,	O
and	O
clusters	O
respectively	O
.	O

For	O
the	O
-	B-Task
th	I-Task
shape	I-Task
prediction	I-Task
layer	E-Task
,	O
the	O
-	O
th	O
cluster	O
of	O
landmarks	O
are	O
treated	O
as	O
the	O
optimized	O
center	O
,	O
and	O
the	O
set	O
of	O
indexes	O
of	O
remaining	O
landmarks	O
is	O
denoted	O
as	O
.	O

From	O
Steps	O
[	O
reference	O
]	O
to	O
[	O
reference	O
]	O
,	O
the	O
parameters	O
of	O
shared	O
layers	O
are	O
fixed	O
,	O
and	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
is	O
initialized	O
with	O
the	O
parameters	O
of	O
the	O
shape	B-Method
prediction	I-Method
layer	E-Method
of	O
WM	S-Method
.	O

When	O
fine	O
-	O
tuning	O
the	O
-	B-Method
th	I-Method
shape	I-Method
prediction	I-Method
layer	E-Method
,	O
the	O
weights	O
of	O
landmarks	O
in	O
and	O
are	O
defined	O
as	O
where	O
is	O
a	O
coefficient	O
to	O
make	O
the	O
-	B-Method
th	I-Method
shape	I-Method
prediction	I-Method
layer	E-Method
emphasize	O
on	O
the	O
detection	O
of	O
the	O
-	O
th	O
cluster	O
of	O
landmarks	O
.	O

The	O
constraint	O
between	O
and	O
is	O
formulated	O
as	O
where	O
refers	O
to	O
the	O
number	O
of	O
elements	O
in	O
a	O
cluster	O
.	O

With	O
Eqs	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
the	O
solved	O
weights	O
are	O
formulated	O
as	O
The	O
average	B-Metric
alignment	I-Metric
error	E-Metric
of	O
each	O
landmark	O
of	O
WM	S-Method
on	O
are	O
respectively	O
.	O

Similar	O
to	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
the	O
weight	O
of	O
the	O
-	O
th	O
landmark	O
is	O
Although	O
the	O
landmarks	O
in	O
are	O
mainly	O
optimized	O
,	O
remaining	O
landmarks	O
are	O
still	O
considered	O
with	O
very	O
small	O
weights	O
rather	O
than	O
zero	O
.	O

This	O
is	O
beneficial	O
for	O
utilizing	O
implicit	O
structural	O
correlations	O
of	O
different	O
facial	O
parts	O
and	O
searching	O
the	O
solutions	O
smoothly	O
.	O

This	O
stage	O
is	O
called	O
multi	B-Method
-	I-Method
center	I-Method
fine	I-Method
-	I-Method
tuning	E-Method
which	O
learns	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
.	O

In	O
Step	O
[	O
reference	O
]	O
,	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
are	O
assembled	O
into	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
by	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

With	O
this	O
model	B-Method
assembling	I-Method
stage	E-Method
,	O
our	O
method	O
learns	O
an	O
assembling	B-Method
model	E-Method
(	O
AM	S-Method
)	O
.	O

There	O
is	O
no	O
increase	O
of	O
model	B-Metric
complexity	E-Metric
in	O
the	O
assembling	B-Task
process	E-Task
,	O
so	O
AM	S-Method
has	O
a	O
low	O
computational	B-Metric
cost	E-Metric
.	O

It	O
improves	O
the	O
detection	B-Metric
precision	E-Metric
of	O
each	O
facial	O
landmark	O
by	O
integrating	O
the	O
advantage	O
of	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
.	O

subsubsection	O
:	O
Analysis	O
of	O
Model	B-Method
Learning	E-Method
To	O
investigate	O
the	O
influence	O
from	O
the	O
weights	O
of	O
landmarks	O
on	O
learning	B-Task
procedure	E-Task
,	O
we	O
calculate	O
the	O
derivative	O
of	O
Eq	O
.	O

[	O
reference	O
]	O
with	O
respect	O
to	O
:	O
where	O
,	O
.	O

During	O
the	O
learning	B-Task
process	E-Task
,	O
the	O
assembled	O
weight	O
matrix	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
is	O
updated	O
by	O
SGD	S-Method
.	O

Specifically	O
,	O
.	O

In	O
summary	O
,	O
is	O
updated	O
as	O
where	O
is	O
the	O
learning	B-Metric
rate	E-Metric
.	O

If	O
the	O
-	O
th	O
landmark	O
is	O
given	O
a	O
larger	O
weight	O
,	O
its	O
corresponding	O
parameters	O
will	O
be	O
updated	O
with	O
a	O
larger	O
step	O
towards	O
the	O
optimal	O
solution	O
.	O

Therefore	O
,	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
ensures	O
that	O
the	O
landmarks	O
with	O
larger	O
weights	O
are	O
mainly	O
optimized	O
.	O

Our	O
method	O
first	O
uses	O
the	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
to	O
optimize	O
challenging	O
landmarks	O
,	O
and	O
further	O
uses	O
the	O
multi	B-Method
-	I-Method
center	I-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
to	O
optimize	O
each	O
cluster	O
of	O
landmarks	O
respectively	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Datasets	O
and	O
Settings	O
subsubsection	O
:	O
Datasets	O
There	O
are	O
three	O
challenging	O
benchmarks	O
AFLW	S-Material
,	O
COFW	S-Method
,	O
and	O
IBUG	S-Method
,	O
which	O
are	O
used	O
for	O
evaluating	B-Task
face	I-Task
alignment	E-Task
with	O
severe	O
occlusion	O
and	O
large	O
variations	O
of	O
pose	O
,	O
expression	O
,	O
and	O
illumination	O
.	O

The	O
provided	O
face	O
bounding	O
boxes	O
are	O
employed	O
to	O
crop	O
face	O
patches	O
during	O
testing	O
.	O

AFLW	S-Material
[	O
]	O
contains	O
faces	O
under	O
real	O
-	O
world	O
conditions	O
gathered	O
from	O
Flickr	S-Material
.	O

Compared	O
with	O
other	O
datasets	O
like	O
MUCT	S-Material
and	O
LFPW	S-Material
,	O
AFLW	S-Material
exhibits	O
larger	O
pose	O
variations	O
and	O
extreme	O
partial	O
occlusions	O
.	O

Following	O
the	O
settings	O
of	O
,	O
images	O
are	O
used	O
for	O
testing	O
,	O
and	O
images	O
annotated	O
with	O
landmarks	O
are	O
used	O
for	O
training	S-Task
,	O
which	O
includes	O
LFW	B-Material
images	E-Material
and	O
web	O
images	O
.	O

COFW	O
[	O
]	O
is	O
an	O
occluded	O
face	O
dataset	O
in	O
the	O
wild	O
,	O
in	O
which	O
the	O
faces	O
are	O
designed	O
with	O
severe	O
occlusions	O
using	O
accessories	O
and	O
interactions	O
with	O
objects	O
.	O

It	O
contains	O
images	O
annotated	O
with	O
landmarks	O
.	O

The	O
training	O
set	O
includes	O
LFPW	S-Material
faces	O
and	O
COFW	B-Material
faces	E-Material
,	O
and	O
the	O
testing	O
set	O
includes	O
remaining	O
COFW	O
faces	O
.	O

IBUG	S-Method
[	O
]	O
contains	O
testing	O
images	O
which	O
present	O
large	O
variations	O
in	O
pose	O
,	O
expression	O
,	O
illumination	O
,	O
and	O
occlusion	O
.	O

The	O
training	O
set	O
consists	O
of	O
AFW	S-Method
,	O
the	O
training	O
set	O
of	O
LFPW	S-Material
,	O
and	O
the	O
training	O
set	O
of	O
Helen	S-Method
,	O
which	O
are	O
from	O
300	O
-	O
W	O
with	O
images	O
labeled	O
with	O
landmarks	O
.	O

subsubsection	O
:	O
Implementation	O
Details	O
We	O
enhance	O
the	O
diversity	O
of	O
raw	O
training	O
data	O
on	O
account	O
of	O
their	O
limited	O
variation	O
patterns	O
,	O
using	O
five	O
steps	O
:	O
rotation	O
,	O
uniform	O
scaling	O
,	O
translation	O
,	O
horizontal	O
flip	O
,	O
and	O
JPEG	B-Method
compression	E-Method
.	O

In	O
particular	O
,	O
for	O
each	O
training	O
face	O
,	O
we	O
firstly	O
perform	O
multiple	O
rotations	O
,	O
and	O
attain	O
a	O
tight	O
face	O
bounding	O
box	O
covering	O
the	O
ground	O
truth	O
locations	O
of	O
landmarks	O
of	O
each	O
rotated	O
result	O
respectively	O
.	O

Uniform	O
scaling	O
and	O
translation	O
with	O
different	O
extents	O
on	O
face	O
bounding	O
boxes	O
are	O
further	O
conducted	O
,	O
in	O
which	O
each	O
newly	O
generated	O
face	O
bounding	O
box	O
is	O
used	O
to	O
crop	O
the	O
face	O
.	O

Finally	O
training	O
samples	O
are	O
augmented	O
through	O
horizontal	B-Method
flip	E-Method
and	O
JPEG	B-Method
compression	E-Method
.	O

It	O
is	O
beneficial	O
for	O
avoiding	O
overfitting	O
and	O
improving	O
the	O
robustness	S-Metric
of	O
learned	B-Method
models	E-Method
by	O
covering	O
various	O
patterns	O
.	O

We	O
train	O
our	O
MCL	S-Method
using	O
an	O
open	B-Method
source	I-Method
deep	I-Method
learning	I-Method
framework	I-Method
Caffe	E-Method
.	O

The	O
input	O
face	O
patch	O
is	O
a	O
grayscale	O
image	O
,	O
and	O
each	O
pixel	O
value	O
is	O
normalized	O
to	O
by	O
subtracting	O
and	O
multiplying	O
.	O

A	O
more	O
complex	O
model	O
is	O
needed	O
for	O
a	O
labeling	O
pattern	O
with	O
more	O
facial	O
landmarks	O
,	O
so	O
is	O
set	O
to	O
be	O
for	O
facial	O
landmarks	O
.	O

The	O
type	O
of	O
solver	O
is	O
SGD	S-Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
,	O
a	O
momentum	O
of	O
,	O
and	O
a	O
weight	O
decay	O
of	O
.	O

The	O
maximum	O
learning	O
iterations	O
of	O
pre	O
-	O
training	O
and	O
each	O
fine	B-Method
-	I-Method
tuning	I-Method
step	E-Method
are	O
and	O
respectively	O
,	O
and	O
the	O
initial	O
learning	B-Metric
rates	E-Metric
of	O
pre	O
-	O
training	O
and	O
each	O
fine	B-Method
-	I-Method
tuning	I-Method
step	E-Method
are	O
and	O
respectively	O
.	O

Note	O
that	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
fine	B-Method
-	I-Method
tuning	E-Method
should	O
be	O
low	O
to	O
preserve	O
some	O
representational	O
structures	O
learned	O
in	O
the	O
pre	O
-	O
training	O
stage	O
and	O
avoid	O
missing	O
good	O
intermediate	O
solutions	O
.	O

The	O
learning	B-Metric
rate	E-Metric
is	O
multiplied	O
by	O
a	O
factor	O
of	O
at	O
every	O
iterations	O
,	O
and	O
the	O
remaining	O
parameter	O
is	O
set	O
to	O
be	O
.	O

subsubsection	O
:	O
Evaluation	B-Metric
Metric	E-Metric
Similar	O
to	O
previous	O
methods	O
,	O
we	O
report	O
the	O
inter	O
-	O
ocular	O
distance	O
normalized	O
mean	B-Metric
error	E-Metric
,	O
and	O
treat	O
the	O
mean	B-Metric
error	E-Metric
larger	O
than	O
as	O
a	O
failure	O
.	O

To	O
conduct	O
a	O
more	O
comprehensive	O
comparison	O
,	O
the	O
cumulative	B-Metric
errors	I-Metric
distribution	I-Metric
(	I-Metric
CED	I-Metric
)	I-Metric
curves	E-Metric
are	O
plotted	O
.	O

To	O
measure	O
the	O
time	B-Metric
efficiency	E-Metric
,	O
the	O
average	B-Metric
running	I-Metric
speed	E-Metric
(	O
Frame	B-Metric
per	I-Metric
Second	E-Metric
,	O
FPS	S-Metric
)	O
on	O
a	O
single	O
core	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
is	O
also	O
reported	O
.	O

A	O
single	O
image	O
is	O
fed	O
into	O
the	O
model	O
at	O
a	O
time	O
when	O
computing	O
the	O
speed	O
.	O

In	O
other	O
words	O
,	O
we	O
evaluate	O
methods	O
on	O
four	O
popular	O
metrics	S-Metric
:	O
mean	B-Metric
error	E-Metric
(	O
%	O
)	O
,	O
failure	B-Metric
rate	E-Metric
(	O
%	O
)	O
,	O
CED	B-Metric
curves	E-Metric
,	O
and	O
average	B-Metric
running	I-Metric
speed	E-Metric
.	O

In	O
the	O
next	O
sections	O
,	O
%	O
in	O
all	O
the	O
results	O
are	O
omitted	O
for	O
simplicity	O
.	O

subsection	O
:	O
Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Methods	O
We	O
compare	O
our	O
work	O
MCL	S-Method
against	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
including	O
ESR	S-Method
,	O
SDM	S-Method
,	O
Cascaded	B-Method
CNN	E-Method
,	O
RCPR	S-Method
,	O
CFAN	S-Method
,	O
LBF	S-Method
,	O
cGPRT	S-Method
,	O
CFSS	S-Method
,	O
TCDCN	S-Method
,	O
ALR	S-Method
,	O
CFT	S-Method
,	O
RFLD	S-Method
,	O
RecNet	S-Method
,	O
RAR	S-Method
,	O
and	O
FLD	B-Method
+	I-Method
PDE	E-Method
.	O

All	O
the	O
methods	O
are	O
evaluated	O
on	O
testing	O
images	O
using	O
the	O
face	O
bounding	O
boxes	O
provided	O
by	O
benchmarks	O
.	O

In	O
addition	O
to	O
given	O
training	O
images	O
,	O
TCDCN	S-Method
uses	O
outside	O
training	O
data	O
labeled	O
with	O
facial	O
attributes	O
.	O

RAR	S-Method
augments	O
training	O
images	O
with	O
occlusions	O
incurred	O
by	O
outside	O
natural	O
objects	O
like	O
sunglasses	O
,	O
phones	O
,	O
and	O
hands	O
.	O

FLD	B-Method
+	I-Method
PDE	E-Method
performs	O
facial	B-Task
landmark	I-Task
detection	E-Task
,	O
pose	B-Task
and	I-Task
deformation	I-Task
estimation	E-Task
simultaneously	O
,	O
in	O
which	O
the	O
training	O
data	O
of	O
pose	B-Task
and	I-Task
deformation	I-Task
estimation	E-Task
are	O
used	O
.	O

Other	O
methods	O
including	O
our	O
MCL	S-Method
only	O
utilize	O
given	O
training	O
images	O
from	O
the	O
benchmarks	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
our	O
method	O
and	O
previous	O
works	O
on	O
three	O
benchmarks	O
.	O

Our	O
method	O
MCL	S-Method
outperforms	O
most	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
especially	O
on	O
AFLW	B-Material
dataset	E-Material
where	O
a	O
relative	B-Metric
error	I-Metric
reduction	E-Metric
of	O
is	O
achieved	O
compared	O
to	O
RecNet	S-Method
.	O

Cascaded	B-Method
CNN	E-Method
estimates	O
the	O
location	O
of	O
each	O
landmark	O
separately	O
in	O
the	O
second	O
and	O
third	O
level	O
,	O
and	O
every	O
two	O
networks	O
are	O
used	O
to	O
detect	O
one	O
landmark	O
.	O

It	O
is	O
difficult	O
to	O
be	O
extended	O
to	O
dense	O
landmarks	O
owing	O
to	O
the	O
explosion	O
of	O
the	O
number	O
of	O
networks	O
.	O

TCDCN	S-Method
relies	O
on	O
outside	O
training	O
data	O
for	O
auxiliary	B-Task
facial	I-Task
attribute	I-Task
recognition	E-Task
,	O
which	O
limits	O
the	O
universality	O
.	O

It	O
can	O
be	O
seen	O
that	O
MCL	S-Method
outperforms	O
Cascaded	B-Method
CNN	E-Method
and	O
TCDCN	S-Method
on	O
all	O
the	O
benchmarks	O
.	O

Moreover	O
,	O
MCL	S-Method
is	O
robust	O
to	O
occlusions	O
with	O
the	O
performance	O
on	O
par	O
with	O
RFLD	S-Method
,	O
benefiting	O
from	O
utilizing	O
semantical	O
correlations	O
among	O
different	O
landmarks	O
.	O

RecNet	S-Method
and	O
RAR	S-Method
show	O
significant	O
results	O
,	O
but	O
their	O
models	O
are	O
very	O
complex	O
with	O
high	O
computational	B-Metric
costs	E-Metric
.	O

We	O
compare	O
with	O
other	O
methods	O
on	O
several	O
challenging	O
images	O
from	O
AFLW	S-Material
and	O
COFW	S-Material
respectively	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Our	O
method	O
MCL	S-Method
indicates	O
higher	O
accuracy	S-Metric
in	O
the	O
details	O
than	O
previous	O
works	O
.	O

More	O
examples	O
on	O
challenging	O
IBUG	S-Task
are	O
presented	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

MCL	S-Method
demonstrates	O
a	O
superior	O
capability	O
of	O
handling	O
severe	O
occlusions	O
and	O
complex	O
variations	O
of	O
pose	O
,	O
expression	O
,	O
illumination	O
.	O

The	O
CED	B-Metric
curves	E-Metric
of	O
MCL	S-Method
and	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

It	O
is	O
observed	O
that	O
MCL	S-Method
achieves	O
competitive	O
performance	O
on	O
all	O
three	O
benchmarks	O
.	O

The	O
average	B-Metric
running	I-Metric
speed	E-Metric
of	O
deep	B-Method
learning	I-Method
methods	E-Method
for	O
detecting	B-Task
facial	I-Task
landmarks	E-Task
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Except	O
for	O
the	O
methods	O
tested	O
on	O
the	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
,	O
other	O
methods	O
are	O
reported	O
with	O
the	O
results	O
in	O
the	O
original	O
papers	O
.	O

Since	O
CFAN	S-Method
utilizes	O
multiple	O
networks	O
,	O
it	O
costs	O
more	O
running	B-Metric
time	E-Metric
.	O

RAR	S-Method
achieves	O
only	O
FPS	O
on	O
a	O
Titan	B-Method
-	I-Method
Z	I-Method
GPU	E-Method
,	O
which	O
can	O
not	O
be	O
applied	O
to	O
practical	O
scenarios	O
.	O

Both	O
TCDCN	S-Method
and	O
our	O
method	O
MCL	S-Method
are	O
based	O
on	O
only	O
one	O
network	O
,	O
so	O
they	O
show	O
higher	O
speed	O
.	O

Our	O
method	O
only	O
takes	O
ms	O
per	O
face	O
on	O
a	O
single	O
core	O
i5	O
-	O
6200U	O
2.3GHz	O
CPU	O
.	O

This	O
profits	O
from	O
low	O
model	B-Metric
complexity	E-Metric
and	O
computational	B-Metric
costs	E-Metric
of	O
our	O
network	O
.	O

It	O
can	O
be	O
concluded	O
that	O
our	O
method	O
is	O
able	O
to	O
be	O
extended	O
to	O
real	B-Task
-	I-Task
time	I-Task
facial	I-Task
landmark	I-Task
tracking	E-Task
in	O
unconstrained	B-Task
environments	E-Task
.	O

subsection	O
:	O
Ablation	B-Task
Study	E-Task
subsubsection	O
:	O
Global	B-Method
Average	I-Method
Pooling	E-Method
vs.	O
Full	B-Method
Connection	E-Method
Based	O
on	O
the	O
previous	O
version	O
of	O
our	O
work	O
,	O
the	O
last	O
max	B-Method
-	I-Method
pooling	I-Method
layer	E-Method
and	O
the	O
-	B-Method
dimensional	I-Method
fully	I-Method
-	I-Method
connected	I-Method
layer	E-Method
are	O
replaced	O
with	O
a	O
convolutional	B-Method
layer	E-Method
and	O
a	O
Global	B-Method
Average	I-Method
Pooling	I-Method
layer	E-Method
.	O

The	O
results	O
of	O
the	O
mean	B-Metric
error	E-Metric
of	O
BM	S-Method
and	O
the	O
previous	O
version	O
(	O
pre	B-Method
-	I-Method
BM	E-Method
)	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
BM	S-Method
performs	O
better	O
on	O
IBUG	S-Method
and	O
COFW	S-Method
but	O
worse	O
on	O
AFLW	S-Material
than	O
pre	O
-	O
BM	S-Method
.	O

It	O
demonstrates	O
that	O
Global	B-Method
Average	I-Method
Pooling	E-Method
is	O
more	O
advantageous	O
for	O
more	O
complex	O
problems	O
with	O
more	O
facial	O
landmarks	O
.	O

There	O
are	O
higher	O
requirements	O
for	O
learned	O
features	O
when	O
localizing	O
more	O
facial	O
landmarks	O
.	O

For	O
simple	O
problems	O
especially	O
for	O
localizing	O
landmarks	O
of	O
AFLW	S-Material
,	O
a	O
plain	B-Method
network	E-Method
with	O
full	B-Method
connection	E-Method
is	O
more	O
prone	O
to	O
being	O
trained	O
.	O

The	O
difference	O
between	O
pre	B-Method
-	I-Method
BM	E-Method
and	O
BM	S-Method
is	O
the	O
structure	O
of	O
learning	O
the	O
feature	O
.	O

The	O
number	O
of	O
parameters	O
for	O
this	O
part	O
of	O
pre	B-Method
-	I-Method
BM	E-Method
and	O
BM	S-Method
are	O
and	O
respectively	O
,	O
where	O
the	O
three	O
terms	O
for	O
BM	S-Method
correspond	O
to	O
the	O
convolution	O
,	O
the	O
expectation	O
and	O
variance	O
of	O
BN	O
,	O
and	O
the	O
scaling	B-Method
and	I-Method
shifting	I-Method
of	I-Method
BN	E-Method
.	O

Therefore	O
,	O
BM	S-Method
has	O
a	O
stronger	O
feature	O
learning	O
ability	O
with	O
fewer	O
parameters	O
than	O
pre	O
-	O
BM	S-Method
.	O

subsubsection	O
:	O
Robustness	O
of	O
Weighting	O
To	O
verify	O
the	O
robustness	O
of	O
weighting	S-Task
,	O
random	O
perturbations	O
are	O
added	O
to	O
the	O
weights	O
of	O
landmarks	O
.	O

In	O
particular	O
,	O
we	O
plus	O
a	O
perturbation	O
to	O
the	O
weight	O
of	O
each	O
of	O
random	O
landmarks	O
and	O
minus	O
to	O
the	O
weight	O
of	O
each	O
of	O
remaining	O
landmarks	O
,	O
where	O
refers	O
to	O
rounding	O
down	O
to	O
the	O
nearest	O
integer	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
variations	O
of	O
mean	B-Metric
error	I-Metric
of	I-Metric
WM	E-Metric
with	O
the	O
increase	O
of	O
.	O

When	O
is	O
,	O
WM	S-Method
can	O
still	O
achieves	O
good	O
performance	O
.	O

Therefore	O
,	O
weighting	O
the	O
loss	O
of	O
each	O
landmark	O
is	O
robust	O
to	O
random	O
perturbations	O
.	O

Even	O
if	O
different	O
weights	O
are	O
obtained	O
,	O
the	O
results	O
will	O
not	O
be	O
affected	O
as	O
long	O
as	O
the	O
relative	O
sizes	O
of	O
weights	O
are	O
identical	O
.	O

subsubsection	O
:	O
Analysis	B-Method
of	I-Method
Shape	I-Method
Prediction	I-Method
Layers	E-Method
Our	O
method	O
learns	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
respectively	O
with	O
a	O
certain	O
cluster	O
of	O
landmarks	O
being	O
emphasized	O
.	O

The	O
results	O
of	O
WM	S-Method
and	O
two	O
shape	B-Method
prediction	I-Method
layers	E-Method
with	O
respect	O
to	O
the	O
left	O
eye	O
and	O
the	O
right	O
eye	O
on	O
IBUG	B-Material
benchmark	E-Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Compared	O
to	O
WM	S-Method
,	O
the	O
left	B-Method
eye	I-Method
model	E-Method
and	O
the	O
right	B-Method
eye	I-Method
model	E-Method
both	O
reduce	O
the	O
alignment	B-Metric
errors	E-Metric
of	O
their	O
corresponding	O
clusters	O
.	O

As	O
a	O
result	O
,	O
the	O
assembled	O
AM	S-Method
can	O
improve	O
the	O
detection	B-Metric
accuracy	E-Metric
of	O
landmarks	O
of	O
the	O
left	O
eye	O
and	O
the	O
right	O
eye	O
on	O
the	O
basis	O
of	O
WM	S-Method
.	O

Note	O
that	O
the	O
two	O
models	O
also	O
improve	O
the	O
localization	B-Metric
precision	E-Metric
of	O
other	O
clusters	O
.	O

Taking	O
the	O
left	B-Method
eye	I-Method
model	E-Method
as	O
an	O
example	O
,	O
it	O
additionally	O
reduces	O
the	O
errors	O
of	O
landmarks	O
of	O
right	O
eye	O
,	O
mouth	O
,	O
and	O
chin	O
,	O
which	O
is	O
due	O
to	O
the	O
correlations	O
among	O
different	O
facial	O
parts	O
.	O

Moreover	O
,	O
for	O
the	O
right	O
eye	O
cluster	O
,	O
the	O
right	B-Method
eye	I-Method
model	E-Method
improves	O
the	O
accuracy	S-Metric
more	O
significantly	O
than	O
the	O
left	B-Method
eye	I-Method
model	E-Method
.	O

It	O
can	O
be	O
concluded	O
that	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
emphasizes	O
on	O
the	O
corresponding	O
cluster	O
respectively	O
.	O

subsubsection	O
:	O
Integration	O
of	O
Weighting	B-Method
Fine	I-Method
-	I-Method
Tuning	E-Method
and	O
Multi	B-Method
-	I-Method
Center	I-Method
Fine	I-Method
-	I-Method
Tuning	E-Method
Here	O
we	O
validate	O
the	O
effectiveness	O
of	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	E-Method
by	O
removing	O
the	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
to	O
learn	O
a	O
Simplified	O
AM	S-Method
from	O
BM	S-Method
.	O

Table	O
[	O
reference	O
]	O
presents	O
the	O
results	O
of	O
mean	B-Metric
error	E-Metric
of	O
Simplified	O
AM	S-Method
and	O
AM	S-Method
respectively	O
on	O
COFW	S-Method
and	O
IBUG	S-Method
.	O

Note	O
that	O
Simplified	O
AM	S-Method
has	O
already	O
acquired	O
good	O
results	O
,	O
which	O
verifies	O
the	O
effectiveness	O
of	O
the	O
multi	B-Method
-	I-Method
center	I-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
.	O

The	O
accuracy	S-Metric
of	O
AM	S-Method
is	O
superior	O
to	O
that	O
of	O
Simplified	O
AM	S-Method
especially	O
on	O
challenging	O
IBUG	S-Task
,	O
which	O
is	O
attributed	O
to	O
the	O
integration	O
of	O
two	O
stages	O
.	O

A	O
Weighting	O
Simplified	O
AM	S-Method
from	O
Simplified	O
AM	S-Method
using	O
the	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
is	O
also	O
learned	O
,	O
whose	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
Weighting	O
Simplified	O
AM	S-Method
improves	O
slightly	O
on	O
COFW	S-Method
but	O
fails	O
to	O
search	O
a	O
better	O
solution	O
on	O
IBUG	S-Method
.	O

Therefore	O
,	O
we	O
choose	O
to	O
use	O
the	O
multi	B-Method
-	I-Method
center	I-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
after	O
the	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	I-Method
stage	E-Method
.	O

subsubsection	O
:	O
Discussion	O
of	O
All	O
Stages	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
results	O
of	O
mean	B-Metric
error	E-Metric
and	O
failure	B-Metric
rate	E-Metric
of	O
BM	S-Method
,	O
WM	S-Method
,	O
and	O
AM	S-Method
.	O

It	O
can	O
be	O
observed	O
that	O
AM	S-Method
has	O
higher	O
accuracy	S-Metric
and	O
stronger	O
robustness	S-Metric
than	O
BM	S-Method
and	O
WM	S-Method
.	O

Fig	O
.	O

[	O
reference	O
]	O
depicts	O
the	O
enhancement	O
from	O
WM	S-Method
to	O
AM	S-Method
for	O
several	O
examples	O
of	O
COFW	S-Task
.	O

The	O
localization	B-Metric
accuracy	E-Metric
of	O
facial	O
landmarks	O
from	O
each	O
cluster	O
is	O
improved	O
in	O
the	O
details	O
.	O

It	O
is	O
because	O
each	O
shape	B-Method
prediction	I-Method
layer	E-Method
increases	O
the	O
detection	B-Metric
precision	E-Metric
of	O
corresponding	O
cluster	O
respectively	O
.	O

subsection	O
:	O
MCL	S-Method
for	O
Partially	B-Task
Occluded	I-Task
Faces	E-Task
The	O
correlations	O
among	O
different	O
facial	O
parts	O
are	O
very	O
useful	O
for	O
face	B-Task
alignment	E-Task
especially	O
for	O
partially	B-Task
occluded	I-Task
faces	E-Task
.	O

To	O
investigate	O
the	O
influence	O
of	O
occlusions	O
,	O
we	O
directly	O
use	O
trained	O
WM	S-Method
and	O
AM	S-Method
without	O
any	O
additional	O
processing	O
for	O
partially	B-Task
occluded	I-Task
faces	E-Task
.	O

Randomly	O
testing	O
faces	O
from	O
COFW	S-Material
are	O
processed	O
with	O
left	O
eyes	O
being	O
occluded	O
,	O
where	O
the	O
tight	O
bounding	O
box	O
covering	O
landmarks	O
of	O
left	O
eye	O
is	O
filled	O
with	O
gray	O
color	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
mean	B-Metric
error	E-Metric
results	O
for	O
the	O
left	O
eye	O
cluster	O
and	O
other	O
clusters	O
of	O
WM	S-Method
and	O
AM	S-Method
on	O
COFW	B-Material
benchmark	E-Material
,	O
where	O
“	O
with	O
(	O
w	O
/	O
)	O
occlusion	O
(	O
occlu	O
.	O

)	O
”	O
denotes	O
that	O
left	O
eyes	O
of	O
the	O
testing	O
faces	O
are	O
processed	O
with	O
handcrafted	O
occlusions	O
as	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
and	O
“	O
without	O
(	O
w	O
/	O
o	O
)	O
occlu	O
.	O

”	O
denotes	O
that	O
the	O
testing	O
faces	O
are	O
kept	O
unchanged	O
.	O

Note	O
that	O
our	O
method	O
does	O
not	O
process	O
occlusions	O
explicitly	O
,	O
in	O
which	O
the	O
training	O
data	O
is	O
not	O
performed	O
handcrafted	O
occlusions	O
.	O

After	O
processing	O
testing	O
faces	O
with	O
occlusions	O
,	O
the	O
mean	B-Metric
error	E-Metric
results	O
of	O
both	O
WM	S-Method
and	O
AM	S-Method
increase	O
.	O

Besides	O
the	O
results	O
of	O
landmarks	O
from	O
the	O
left	O
eye	O
cluster	O
,	O
the	O
results	O
of	O
remaining	O
landmarks	O
from	O
other	O
clusters	O
become	O
worse	O
slightly	O
.	O

This	O
is	O
because	O
different	O
facial	O
parts	O
have	O
correlations	O
and	O
the	O
occlusions	O
of	O
the	O
left	O
eye	O
influences	O
results	O
of	O
other	O
facial	O
parts	O
.	O

Note	O
that	O
WM	S-Method
and	O
AM	S-Method
still	O
perform	O
well	O
on	O
occluded	O
left	O
eyes	O
with	O
the	O
mean	B-Metric
error	E-Metric
of	O
and	O
respectively	O
,	O
due	O
to	O
the	O
following	O
reasons	O
.	O

First	O
,	O
WM	S-Method
weights	O
each	O
landmark	O
proportional	O
to	O
its	O
alignment	B-Metric
error	E-Metric
,	O
which	O
exploits	O
correlations	O
among	O
landmarks	O
.	O

Second	O
,	O
AM	S-Method
uses	O
an	O
independent	B-Method
shape	I-Method
prediction	I-Method
layer	E-Method
focusing	O
on	O
a	O
certain	O
cluster	O
of	O
landmarks	O
with	O
small	O
weights	O
,	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
for	O
remaining	O
landmarks	O
,	O
respectively	O
,	O
where	O
correlations	O
among	O
landmarks	O
are	O
further	O
exploited	O
.	O

subsection	O
:	O
Weighting	B-Task
Fine	I-Task
-	I-Task
Tuning	E-Task
for	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Frameworks	O
Most	O
recently	O
,	O
there	O
are	O
a	O
few	O
well	O
-	O
designed	O
and	O
well	O
-	O
trained	O
deep	B-Method
learning	I-Method
frameworks	E-Method
advancing	O
the	O
performance	O
of	O
face	B-Task
alignment	E-Task
,	O
in	O
which	O
DAN	S-Task
is	O
a	O
typical	O
work	O
.	O

DAN	S-Method
uses	O
cascaded	B-Method
deep	I-Method
neural	I-Method
networks	E-Method
to	O
refine	O
the	O
localization	B-Task
accuracy	I-Task
of	I-Task
landmarks	E-Task
iteratively	O
,	O
where	O
the	O
entire	O
face	O
image	O
and	O
the	O
landmark	O
heatmap	O
generated	O
from	O
the	O
previous	O
stage	O
are	O
used	O
in	O
each	O
stage	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
method	O
extended	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frameworks	O
,	O
we	O
conduct	O
experiments	O
with	O
our	O
proposed	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	E-Method
being	O
applied	O
to	O
DAN	S-Method
.	O

In	O
particular	O
,	O
each	O
stage	O
of	O
DAN	S-Method
is	O
first	O
pre	O
-	O
trained	O
and	O
further	O
weighting	O
fine	O
-	O
tuned	O
,	O
where	O
DAN	S-Method
with	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	E-Method
is	O
named	O
DAN	O
-	O
WM	S-Method
.	O

Note	O
that	O
the	O
results	O
of	O
retrained	B-Method
DAN	E-Method
(	O
re	B-Method
-	I-Method
DAN	E-Method
)	O
using	O
the	O
published	O
code	O
are	O
slightly	O
worse	O
than	O
reported	O
results	O
of	O
DAN	S-Method
.	O

For	O
a	O
fair	O
comparison	O
,	O
the	O
results	O
of	O
mean	B-Metric
error	E-Metric
of	O
DAN	S-Method
,	O
re	B-Method
-	I-Method
DAN	E-Method
,	O
and	O
DAN	O
-	O
WM	S-Method
on	O
IBUG	B-Task
benchmark	E-Task
are	O
all	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
can	O
be	O
seen	O
that	O
the	O
mean	B-Metric
error	E-Metric
of	O
re	B-Metric
-	I-Metric
DAN	E-Metric
is	O
reduced	O
from	O
to	O
after	O
using	O
our	O
proposed	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	E-Method
.	O

Note	O
that	O
our	O
method	O
uses	O
only	O
a	O
single	O
neural	B-Method
network	E-Method
,	O
which	O
has	O
a	O
concise	O
structure	O
with	O
low	B-Metric
model	I-Metric
complexity	E-Metric
.	O

Our	O
network	O
can	O
be	O
replaced	O
with	O
a	O
more	O
powerful	O
one	O
such	O
as	O
cascaded	B-Method
deep	I-Method
neural	I-Method
networks	E-Method
,	O
which	O
could	O
further	O
improve	O
the	O
performance	O
of	O
face	B-Task
alignment	E-Task
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
have	O
developed	O
a	O
novel	O
multi	B-Method
-	I-Method
center	I-Method
learning	I-Method
framework	E-Method
with	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
for	O
face	B-Task
alignment	E-Task
.	O

The	O
structure	O
of	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
is	O
beneficial	O
for	O
reinforcing	O
the	O
learning	B-Task
process	E-Task
of	O
each	O
cluster	O
of	O
landmarks	O
.	O

In	O
addition	O
,	O
we	O
have	O
proposed	O
the	O
model	B-Method
assembling	I-Method
method	E-Method
to	O
integrate	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
into	O
one	O
shape	B-Method
prediction	I-Method
layer	E-Method
so	O
as	O
to	O
ensure	O
a	O
low	O
model	B-Metric
complexity	E-Metric
.	O

Extensive	O
experiments	O
have	O
demonstrated	O
the	O
effectiveness	O
of	O
our	O
method	O
including	O
handling	O
complex	O
occlusions	O
and	O
appearance	O
variations	O
.	O

First	O
,	O
each	O
component	O
of	O
our	O
framework	O
including	O
Global	B-Method
Average	I-Method
Pooling	E-Method
,	O
multiple	B-Method
shape	I-Method
prediction	I-Method
layers	E-Method
,	O
weighting	B-Method
fine	I-Method
-	I-Method
tuning	E-Method
,	O
and	O
multi	B-Method
-	I-Method
center	I-Method
fine	I-Method
-	I-Method
tuning	E-Method
contributes	O
to	O
face	B-Task
alignment	E-Task
.	O

Second	O
,	O
our	O
proposed	O
neural	B-Method
network	E-Method
and	O
model	B-Method
assembling	I-Method
method	E-Method
allow	O
real	O
-	O
time	O
performance	O
.	O

Third	O
,	O
we	O
have	O
extended	O
our	O
method	O
for	O
detecting	B-Task
partially	I-Task
occluded	I-Task
faces	E-Task
and	O
integrating	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	B-Method
frameworks	E-Method
,	O
and	O
have	O
shown	O
that	O
our	O
method	O
exploits	O
correlations	O
among	O
landmarks	O
and	O
can	O
further	O
improve	O
the	O
performance	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frameworks	O
.	O

The	O
proposed	O
framework	O
is	O
also	O
promising	O
to	O
be	O
applied	O
for	O
other	O
face	B-Task
analysis	I-Task
tasks	E-Task
and	O
multi	B-Task
-	I-Task
label	I-Task
problems	E-Task
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
No	O
.	O

61472245	O
)	O
,	O
and	O
the	O
Science	O
and	O
Technology	O
Commission	O
of	O
Shanghai	O
Municipality	O
Program	O
(	O
No	O
.	O

16511101300	O
)	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Quasi	B-Method
-	I-Method
Recurrent	I-Method
Neural	I-Method
Networks	I-Method
Recurrent	I-Method
neural	I-Method
networks	E-Method
are	O
a	O
powerful	O
tool	O
for	O
modeling	O
sequential	O
data	O
,	O
but	O
the	O
dependence	O
of	O
each	O
timestep	O
’s	O
computation	O
on	O
the	O
previous	O
timestep	O
’s	O
output	O
limits	O
parallelism	O
and	O
makes	O
RNNs	S-Method
unwieldy	O
for	O
very	O
long	O
sequences	O
.	O

We	O
introduce	O
quasi	B-Method
-	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
(	O
QRNNs	S-Method
)	O
,	O
an	O
approach	O
to	O
neural	B-Task
sequence	I-Task
modeling	E-Task
that	O
alternates	O
convolutional	B-Method
layers	E-Method
,	O
which	O
apply	O
in	O
parallel	O
across	O
timesteps	O
,	O
and	O
a	O
minimalist	B-Method
recurrent	I-Method
pooling	I-Method
function	E-Method
that	O
applies	O
in	O
parallel	O
across	O
channels	O
.	O

Despite	O
lacking	O
trainable	O
recurrent	B-Method
layers	E-Method
,	O
stacked	O
QRNNs	S-Method
have	O
better	O
predictive	B-Metric
accuracy	E-Metric
than	O
stacked	B-Method
LSTMs	E-Method
of	O
the	O
same	O
hidden	O
size	O
.	O

Due	O
to	O
their	O
increased	O
parallelism	O
,	O
they	O
are	O
up	O
to	O
16	O
times	O
faster	O
at	O
train	B-Metric
and	I-Metric
test	I-Metric
time	E-Metric
.	O

Experiments	O
on	O
language	B-Task
modeling	E-Task
,	O
sentiment	B-Task
classification	E-Task
,	O
and	O
character	O
-	O
level	O
neural	O
machine	B-Task
translation	E-Task
demonstrate	O
these	O
advantages	O
and	O
underline	O
the	O
viability	O
of	O
QRNNs	S-Method
as	O
a	O
basic	O
building	O
block	O
for	O
a	O
variety	O
of	O
sequence	B-Task
tasks	E-Task
.	O

section	O
:	O
Introduction	O
Recurrent	B-Method
neural	I-Method
networks	E-Method
(	O
RNNs	S-Method
)	O
,	O
including	O
gated	B-Method
variants	E-Method
such	O
as	O
the	O
long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	E-Method
(	O
LSTM	S-Method
)	O
Hochreiter1997	O
have	O
become	O
the	O
standard	O
model	B-Method
architecture	E-Method
for	O
deep	B-Method
learning	I-Method
approaches	E-Method
to	O
sequence	B-Task
modeling	I-Task
tasks	E-Task
.	O

RNNs	S-Method
repeatedly	O
apply	O
a	O
function	O
with	O
trainable	O
parameters	O
to	O
a	O
hidden	O
state	O
.	O

Recurrent	B-Method
layers	E-Method
can	O
also	O
be	O
stacked	O
,	O
increasing	O
network	O
depth	O
,	O
representational	O
power	O
and	O
often	O
accuracy	S-Metric
.	O

RNN	B-Method
applications	E-Method
in	O
the	O
natural	B-Task
language	I-Task
domain	E-Task
range	O
from	O
sentence	B-Task
classification	E-Task
Wang2015	O
to	O
word	B-Task
-	I-Task
and	I-Task
character	I-Task
-	I-Task
level	I-Task
language	I-Task
modeling	E-Task
Zaremba2014	O
.	O

RNNs	S-Method
are	O
also	O
commonly	O
the	O
basic	O
building	O
block	O
for	O
more	O
complex	O
models	O
for	O
tasks	O
such	O
as	O
machine	B-Task
translation	E-Task
Bahdanau2015	O
,	O
Luong2015	O
,	O
Bradbury2016	O
or	O
question	B-Task
answering	E-Task
Kumar2016	O
,	O
Xiong2016	O
.	O

Unfortunately	O
standard	O
RNNs	S-Method
,	O
including	O
LSTMs	S-Method
,	O
are	O
limited	O
in	O
their	O
capability	O
to	O
handle	O
tasks	O
involving	O
very	O
long	O
sequences	O
,	O
such	O
as	O
document	B-Task
classification	E-Task
or	O
character	O
-	O
level	O
machine	B-Task
translation	E-Task
,	O
as	O
the	O
computation	O
of	O
features	O
or	O
states	O
for	O
different	O
parts	O
of	O
the	O
document	O
can	O
not	O
occur	O
in	O
parallel	O
.	O

Convolutional	B-Method
neural	I-Method
networks	E-Method
(	O
CNNs	S-Method
)	O
Krizhevsky2012	O
,	O
though	O
more	O
popular	O
on	O
tasks	O
involving	O
image	O
data	O
,	O
have	O
also	O
been	O
applied	O
to	O
sequence	B-Task
encoding	I-Task
tasks	E-Task
Zhang2015	O
.	O

Such	O
models	O
apply	O
time	B-Method
-	I-Method
invariant	I-Method
filter	I-Method
functions	E-Method
in	O
parallel	O
to	O
windows	O
along	O
the	O
input	O
sequence	O
.	O

CNNs	S-Method
possess	O
several	O
advantages	O
over	O
recurrent	B-Method
models	E-Method
,	O
including	O
increased	O
parallelism	O
and	O
better	O
scaling	O
to	O
long	O
sequences	O
such	O
as	O
those	O
often	O
seen	O
with	O
character	O
-	O
level	O
language	O
data	O
.	O

Convolutional	B-Method
models	E-Method
for	O
sequence	B-Task
processing	E-Task
have	O
been	O
more	O
successful	O
when	O
combined	O
with	O
RNN	B-Method
layers	E-Method
in	O
a	O
hybrid	B-Method
architecture	E-Method
Lee2016	O
,	O
because	O
traditional	O
max	B-Method
-	I-Method
and	I-Method
average	I-Method
-	I-Method
pooling	I-Method
approaches	E-Method
to	O
combining	O
convolutional	O
features	O
across	O
timesteps	O
assume	O
time	O
invariance	O
and	O
hence	O
can	O
not	O
make	O
full	O
use	O
of	O
large	O
-	O
scale	O
sequence	O
order	O
information	O
.	O

We	O
present	O
quasi	B-Method
-	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
for	O
neural	B-Task
sequence	I-Task
modeling	E-Task
.	O

QRNNs	S-Method
address	O
both	O
drawbacks	O
of	O
standard	O
models	O
:	O
like	O
CNNs	S-Method
,	O
QRNNs	S-Method
allow	O
for	O
parallel	B-Task
computation	E-Task
across	O
both	O
timestep	O
and	O
minibatch	O
dimensions	O
,	O
enabling	O
high	O
throughput	S-Metric
and	O
good	O
scaling	O
to	O
long	O
sequences	O
.	O

Like	O
RNNs	S-Method
,	O
QRNNs	S-Method
allow	O
the	O
output	O
to	O
depend	O
on	O
the	O
overall	O
order	O
of	O
elements	O
in	O
the	O
sequence	O
.	O

We	O
describe	O
QRNN	B-Method
variants	E-Method
tailored	O
to	O
several	O
natural	B-Task
language	I-Task
tasks	E-Task
,	O
including	O
document	B-Task
-	I-Task
level	I-Task
sentiment	I-Task
classification	E-Task
,	O
language	B-Task
modeling	E-Task
,	O
and	O
character	O
-	O
level	O
machine	B-Task
translation	E-Task
.	O

These	O
models	O
outperform	O
strong	O
LSTM	S-Method
baselines	O
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
reducing	O
computation	B-Metric
time	E-Metric
.	O

section	O
:	O
Model	O
Each	O
layer	O
of	O
a	O
quasi	B-Method
-	I-Method
recurrent	I-Method
neural	I-Method
network	E-Method
consists	O
of	O
two	O
kinds	O
of	O
subcomponents	O
,	O
analogous	O
to	O
convolution	B-Method
and	I-Method
pooling	I-Method
layers	E-Method
in	O
CNNs	S-Method
.	O

The	O
convolutional	B-Method
component	E-Method
,	O
like	O
convolutional	B-Method
layers	E-Method
in	O
CNNs	S-Method
,	O
allows	O
fully	O
parallel	B-Task
computation	E-Task
across	O
both	O
minibatches	O
and	O
spatial	O
dimensions	O
,	O
in	O
this	O
case	O
the	O
sequence	O
dimension	O
.	O

The	O
pooling	B-Method
component	E-Method
,	O
like	O
pooling	B-Method
layers	E-Method
in	O
CNNs	S-Method
,	O
lacks	O
trainable	O
parameters	O
and	O
allows	O
fully	O
parallel	O
computation	O
across	O
minibatch	O
and	O
feature	O
dimensions	O
.	O

Given	O
an	O
input	O
sequence	O
of	O
-	O
dimensional	O
vectors	O
,	O
the	O
convolutional	B-Method
subcomponent	E-Method
of	O
a	O
QRNN	S-Method
performs	O
convolutions	S-Method
in	O
the	O
timestep	O
dimension	O
with	O
a	O
bank	B-Method
of	I-Method
filters	E-Method
,	O
producing	O
a	O
sequence	O
of	O
-	O
dimensional	O
candidate	O
vectors	O
.	O

In	O
order	O
to	O
be	O
useful	O
for	O
tasks	O
that	O
include	O
prediction	B-Task
of	I-Task
the	I-Task
next	I-Task
token	E-Task
,	O
the	O
filters	S-Method
must	O
not	O
allow	O
the	O
computation	O
for	O
any	O
given	O
timestep	O
to	O
access	O
information	O
from	O
future	O
timesteps	O
.	O

That	O
is	O
,	O
with	O
filters	O
of	O
width	O
,	O
each	O
depends	O
only	O
on	O
through	O
.	O

This	O
concept	O
,	O
known	O
as	O
a	O
masked	B-Method
convolution	E-Method
vandenOord2016	O
,	O
is	O
implemented	O
by	O
padding	O
the	O
input	O
to	O
the	O
left	O
by	O
the	O
convolution	O
’s	O
filter	O
size	O
minus	O
one	O
.	O

We	O
apply	O
additional	O
convolutions	S-Method
with	O
separate	O
filter	B-Method
banks	E-Method
to	O
obtain	O
sequences	O
of	O
vectors	O
for	O
the	O
elementwise	O
gates	O
that	O
are	O
needed	O
for	O
the	O
pooling	O
function	O
.	O

While	O
the	O
candidate	O
vectors	O
are	O
passed	O
through	O
a	O
nonlinearity	S-Method
,	O
the	O
gates	O
use	O
an	O
elementwise	B-Method
sigmoid	E-Method
.	O

If	O
the	O
pooling	B-Method
function	E-Method
requires	O
a	O
forget	O
gate	O
and	O
an	O
output	O
gate	O
at	O
each	O
timestep	O
,	O
the	O
full	O
set	O
of	O
computations	O
in	O
the	O
convolutional	B-Method
component	E-Method
is	O
then	O
:	O
where	O
,	O
,	O
and	O
,	O
each	O
in	O
,	O
are	O
the	O
convolutional	B-Method
filter	I-Method
banks	E-Method
and	O
denotes	O
a	O
masked	B-Method
convolution	E-Method
along	O
the	O
timestep	O
dimension	O
.	O

Note	O
that	O
if	O
the	O
filter	O
width	O
is	O
2	O
,	O
these	O
equations	O
reduce	O
to	O
the	O
LSTM	S-Method
-	O
like	O
Convolution	O
filters	O
of	O
larger	O
width	O
effectively	O
compute	O
higher	O
-	O
gram	O
features	O
at	O
each	O
timestep	O
;	O
thus	O
larger	O
widths	O
are	O
especially	O
important	O
for	O
character	B-Task
-	I-Task
level	I-Task
tasks	E-Task
.	O

Suitable	O
functions	O
for	O
the	O
pooling	O
subcomponent	O
can	O
be	O
constructed	O
from	O
the	O
familiar	O
elementwise	O
gates	O
of	O
the	O
traditional	O
LSTM	S-Method
cell	O
.	O

We	O
seek	O
a	O
function	O
controlled	O
by	O
gates	O
that	O
can	O
mix	O
states	O
across	O
timesteps	O
,	O
but	O
which	O
acts	O
independently	O
on	O
each	O
channel	O
of	O
the	O
state	O
vector	O
.	O

The	O
simplest	O
option	O
,	O
which	O
term	O
“	O
dynamic	B-Method
average	I-Method
pooling	E-Method
”	O
,	O
uses	O
only	O
a	O
forget	O
gate	O
:	O
where	O
denotes	O
elementwise	O
multiplication	O
.	O

The	O
function	O
may	O
also	O
include	O
an	O
output	O
gate	O
:	O
Or	O
the	O
recurrence	O
relation	O
may	O
include	O
an	O
independent	O
input	O
and	O
forget	O
gate	O
:	O
We	O
term	O
these	O
three	O
options	O
f	O
-	O
pooling	O
,	O
fo	B-Method
-	I-Method
pooling	E-Method
,	O
and	O
ifo	O
-	O
pooling	O
respectively	O
;	O
in	O
each	O
case	O
we	O
initialize	O
or	O
to	O
zero	O
.	O

Although	O
the	O
recurrent	O
parts	O
of	O
these	O
functions	O
must	O
be	O
calculated	O
for	O
each	O
timestep	O
in	O
sequence	O
,	O
their	O
simplicity	O
and	O
parallelism	O
along	O
feature	O
dimensions	O
means	O
that	O
,	O
in	O
practice	O
,	O
evaluating	O
them	O
over	O
even	O
long	O
sequences	O
requires	O
a	O
negligible	O
amount	O
of	O
computation	B-Metric
time	E-Metric
.	O

A	O
single	O
QRNN	B-Method
layer	E-Method
thus	O
performs	O
an	O
input	B-Method
-	I-Method
dependent	I-Method
pooling	E-Method
,	O
followed	O
by	O
a	O
gated	B-Method
linear	I-Method
combination	I-Method
of	I-Method
convolutional	I-Method
features	E-Method
.	O

As	O
with	O
convolutional	B-Method
neural	I-Method
networks	E-Method
,	O
two	O
or	O
more	O
QRNN	B-Method
layers	E-Method
should	O
be	O
stacked	O
to	O
create	O
a	O
model	O
with	O
the	O
capacity	O
to	O
approximate	O
more	O
complex	O
functions	O
.	O

subsection	O
:	O
Variants	O
Motivated	O
by	O
several	O
common	O
natural	B-Task
language	I-Task
tasks	E-Task
,	O
and	O
the	O
long	O
history	O
of	O
work	O
on	O
related	O
architectures	O
,	O
we	O
introduce	O
several	O
extensions	O
to	O
the	O
stacked	B-Method
QRNN	E-Method
described	O
above	O
.	O

Notably	O
,	O
many	O
extensions	O
to	O
both	O
recurrent	B-Method
and	I-Method
convolutional	I-Method
models	E-Method
can	O
be	O
applied	O
directly	O
to	O
the	O
QRNN	S-Method
as	O
it	O
combines	O
elements	O
of	O
both	O
model	O
types	O
.	O

Regularization	S-Task
An	O
important	O
extension	O
to	O
the	O
stacked	B-Method
QRNN	E-Method
is	O
a	O
robust	B-Method
regularization	I-Method
scheme	E-Method
inspired	O
by	O
recent	O
work	O
in	O
regularizing	B-Method
LSTMs	E-Method
.	O

The	O
need	O
for	O
an	O
effective	O
regularization	B-Method
method	E-Method
for	O
LSTMs	S-Method
,	O
and	O
dropout	S-Method
’s	O
relative	O
lack	O
of	O
efficacy	O
when	O
applied	O
to	O
recurrent	O
connections	O
,	O
led	O
to	O
the	O
development	O
of	O
recurrent	B-Method
dropout	I-Method
schemes	E-Method
,	O
including	O
variational	B-Method
inference	E-Method
–	O
based	O
dropout	O
Gal2015	S-Method
and	O
zoneout	S-Method
Krueger2016	O
.	O

These	O
schemes	O
extend	O
dropout	S-Method
to	O
the	O
recurrent	B-Task
setting	E-Task
by	O
taking	O
advantage	O
of	O
the	O
repeating	B-Method
structure	I-Method
of	I-Method
recurrent	I-Method
networks	E-Method
,	O
providing	O
more	O
powerful	O
and	O
less	O
destructive	B-Method
regularization	E-Method
.	O

Variational	B-Method
inference	I-Method
–	I-Method
based	I-Method
dropout	E-Method
locks	O
the	O
dropout	O
mask	O
used	O
for	O
the	O
recurrent	O
connections	O
across	O
timesteps	O
,	O
so	O
a	O
single	O
RNN	B-Method
pass	E-Method
uses	O
a	O
single	O
stochastic	O
subset	O
of	O
the	O
recurrent	O
weights	O
.	O

Zoneout	S-Method
stochastically	O
chooses	O
a	O
new	O
subset	O
of	O
channels	O
to	O
“	O
zone	O
out	O
”	O
at	O
each	O
timestep	O
;	O
for	O
these	O
channels	O
the	O
network	O
copies	O
states	O
from	O
one	O
timestep	O
to	O
the	O
next	O
without	O
modification	O
.	O

As	O
QRNNs	S-Method
lack	O
recurrent	O
weights	O
,	O
the	O
variational	B-Method
inference	I-Method
approach	E-Method
does	O
not	O
apply	O
.	O

Thus	O
we	O
extended	O
zoneout	S-Method
to	O
the	O
QRNN	B-Method
architecture	E-Method
by	O
modifying	O
the	O
pooling	B-Method
function	E-Method
to	O
keep	O
the	O
previous	O
pooling	O
state	O
for	O
a	O
stochastic	O
subset	O
of	O
channels	O
.	O

Conveniently	O
,	O
this	O
is	O
equivalent	O
to	O
stochastically	O
setting	O
a	O
subset	O
of	O
the	O
QRNN	O
’s	O
gate	O
channels	O
to	O
1	O
,	O
or	O
applying	O
dropout	S-Method
on	O
:	O
Thus	O
the	O
pooling	B-Method
function	E-Method
itself	O
need	O
not	O
be	O
modified	O
at	O
all	O
.	O

We	O
note	O
that	O
when	O
using	O
an	O
off	O
-	O
the	O
-	O
shelf	O
dropout	B-Method
layer	E-Method
in	O
this	O
context	O
,	O
it	O
is	O
important	O
to	O
remove	O
automatic	O
rescaling	O
functionality	O
from	O
the	O
implementation	O
if	O
it	O
is	O
present	O
.	O

In	O
many	O
experiments	O
,	O
we	O
also	O
apply	O
ordinary	B-Method
dropout	I-Method
between	I-Method
layers	E-Method
,	O
including	O
between	O
word	B-Method
embeddings	E-Method
and	O
the	O
first	B-Method
QRNN	I-Method
layer	E-Method
.	O

Densely	B-Method
-	I-Method
Connected	I-Method
Layers	E-Method
We	O
can	O
also	O
extend	O
the	O
QRNN	B-Method
architecture	E-Method
using	O
techniques	O
introduced	O
for	O
convolutional	B-Method
networks	E-Method
.	O

For	O
sequence	B-Task
classification	I-Task
tasks	E-Task
,	O
we	O
found	O
it	O
helpful	O
to	O
use	O
skip	O
-	O
connections	O
between	O
every	O
QRNN	B-Method
layer	E-Method
,	O
a	O
technique	O
termed	O
“	O
dense	B-Method
convolution	E-Method
”	O
by	O
.	O

Where	O
traditional	O
feed	B-Method
-	I-Method
forward	I-Method
or	I-Method
convolutional	I-Method
networks	E-Method
have	O
connections	O
only	O
between	O
subsequent	O
layers	O
,	O
a	O
“	O
DenseNet	B-Method
”	E-Method
with	O
layers	O
has	O
feed	O
-	O
forward	O
or	O
convolutional	O
connections	O
between	O
every	O
pair	O
of	O
layers	O
,	O
for	O
a	O
total	O
of	O
.	O

This	O
can	O
improve	O
gradient	B-Metric
flow	E-Metric
and	O
convergence	B-Metric
properties	E-Metric
,	O
especially	O
in	O
deeper	B-Task
networks	E-Task
,	O
although	O
it	O
requires	O
a	O
parameter	O
count	O
that	O
is	O
quadratic	O
in	O
the	O
number	O
of	O
layers	O
.	O

When	O
applying	O
this	O
technique	O
to	O
the	O
QRNN	S-Method
,	O
we	O
include	O
connections	O
between	O
the	O
input	O
embeddings	O
and	O
every	O
QRNN	B-Method
layer	E-Method
and	O
between	O
every	O
pair	O
of	O
QRNN	B-Method
layers	E-Method
.	O

This	O
is	O
equivalent	O
to	O
concatenating	O
each	O
QRNN	B-Method
layer	E-Method
’s	O
input	O
to	O
its	O
output	O
along	O
the	O
channel	O
dimension	O
before	O
feeding	O
the	O
state	O
into	O
the	O
next	O
layer	O
.	O

The	O
output	O
of	O
the	O
last	O
layer	O
alone	O
is	O
then	O
used	O
as	O
the	O
overall	O
encoding	O
result	O
.	O

Encoder	B-Method
–	I-Method
Decoder	I-Method
Models	E-Method
To	O
demonstrate	O
the	O
generality	O
of	O
QRNNs	S-Method
,	O
we	O
extend	O
the	O
model	B-Method
architecture	E-Method
to	O
sequence	B-Task
-	I-Task
to	I-Task
-	I-Task
sequence	I-Task
tasks	E-Task
,	O
such	O
as	O
machine	B-Task
translation	E-Task
,	O
by	O
using	O
a	O
QRNN	S-Method
as	O
encoder	S-Method
and	O
a	O
modified	O
QRNN	S-Method
,	O
enhanced	O
with	O
attention	S-Method
,	O
as	O
decoder	S-Method
.	O

The	O
motivation	O
for	O
modifying	O
the	O
decoder	O
is	O
that	O
simply	O
feeding	O
the	O
last	O
encoder	O
hidden	O
state	O
(	O
the	O
output	O
of	O
the	O
encoder	B-Method
’s	I-Method
pooling	I-Method
layer	E-Method
)	O
into	O
the	O
decoder	B-Method
’s	I-Method
recurrent	I-Method
pooling	I-Method
layer	E-Method
,	O
analogously	O
to	O
conventional	O
recurrent	B-Method
encoder	I-Method
–	I-Method
decoder	I-Method
architectures	E-Method
,	O
would	O
not	O
allow	O
the	O
encoder	O
state	O
to	O
affect	O
the	O
gate	O
or	O
update	O
values	O
that	O
are	O
provided	O
to	O
the	O
decoder	B-Method
’s	I-Method
pooling	I-Method
layer	E-Method
.	O

This	O
would	O
substantially	O
limit	O
the	O
representational	O
power	O
of	O
the	O
decoder	S-Method
.	O

Instead	O
,	O
the	O
output	O
of	O
each	O
decoder	B-Method
QRNN	I-Method
layer	I-Method
’s	I-Method
convolution	I-Method
functions	E-Method
is	O
supplemented	O
at	O
every	O
timestep	O
with	O
the	O
final	O
encoder	O
hidden	O
state	O
.	O

This	O
is	O
accomplished	O
by	O
adding	O
the	O
result	O
of	O
the	O
convolution	O
for	O
layer	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
with	O
broadcasting	O
to	O
a	O
linearly	O
projected	O
copy	O
of	O
layer	O
’s	O
last	O
encoder	O
state	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
:	O
where	O
the	O
tilde	O
denotes	O
that	O
is	O
an	O
encoder	O
variable	O
.	O

Encoder	B-Method
–	I-Method
decoder	I-Method
models	E-Method
which	O
operate	O
on	O
long	O
sequences	O
are	O
made	O
significantly	O
more	O
powerful	O
with	O
the	O
addition	O
of	O
soft	O
attention	O
Bahdanau2015	O
,	O
which	O
removes	O
the	O
need	O
for	O
the	O
entire	O
input	O
representation	O
to	O
fit	O
into	O
a	O
fixed	O
-	O
length	O
encoding	O
vector	O
.	O

In	O
our	O
experiments	O
,	O
we	O
computed	O
an	O
attentional	O
sum	O
of	O
the	O
encoder	S-Method
’s	O
last	O
layer	O
’s	O
hidden	O
states	O
.	O

We	O
used	O
the	O
dot	O
products	O
of	O
these	O
encoder	O
hidden	O
states	O
with	O
the	O
decoder	O
’s	O
last	O
layer	O
’s	O
un	O
-	O
gated	O
hidden	O
states	O
,	O
applying	O
a	O
along	O
the	O
encoder	O
timesteps	O
,	O
to	O
weight	O
the	O
encoder	O
states	O
into	O
an	O
attentional	O
sum	O
for	O
each	O
decoder	O
timestep	O
.	O

This	O
context	O
,	O
and	O
the	O
decoder	O
state	O
,	O
are	O
then	O
fed	O
into	O
a	O
linear	B-Method
layer	E-Method
followed	O
by	O
the	O
output	O
gate	O
:	O
where	O
is	O
the	O
last	O
layer	O
.	O

While	O
the	O
first	O
step	O
of	O
this	O
attention	B-Method
procedure	E-Method
is	O
quadratic	O
in	O
the	O
sequence	O
length	O
,	O
in	O
practice	O
it	O
takes	O
significantly	O
less	O
computation	B-Metric
time	E-Metric
than	O
the	O
model	O
’s	O
linear	B-Method
and	I-Method
convolutional	I-Method
layers	E-Method
due	O
to	O
the	O
simple	O
and	O
highly	O
parallel	O
dot	O
-	O
product	O
scoring	O
function	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
performance	O
of	O
the	O
QRNN	S-Method
on	O
three	O
different	O
natural	B-Task
language	I-Task
tasks	E-Task
:	O
document	B-Task
-	I-Task
level	I-Task
sentiment	I-Task
classification	E-Task
,	O
language	B-Task
modeling	E-Task
,	O
and	O
character	O
-	O
based	O
neural	O
machine	B-Task
translation	E-Task
.	O

Our	O
QRNN	B-Method
models	E-Method
outperform	O
LSTM	S-Method
-	O
based	O
models	O
of	O
equal	B-Method
hidden	I-Method
size	E-Method
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
improving	O
computation	B-Metric
speed	E-Metric
.	O

Experiments	O
were	O
implemented	O
in	O
Chainer	O
Tokui2015	O
.	O

subsection	O
:	O
Sentiment	B-Task
Classification	E-Task
We	O
evaluate	O
the	O
QRNN	B-Method
architecture	E-Method
on	O
a	O
popular	O
document	B-Task
-	I-Task
level	I-Task
sentiment	I-Task
classification	I-Task
benchmark	E-Task
,	O
the	O
IMDb	B-Material
movie	I-Material
review	I-Material
dataset	E-Material
Maas2011	O
.	O

The	O
dataset	O
consists	O
of	O
a	O
balanced	O
sample	O
of	O
25	O
,	O
000	O
positive	O
and	O
25	O
,	O
000	O
negative	O
reviews	O
,	O
divided	O
into	O
equal	O
-	O
size	O
train	O
and	O
test	O
sets	O
,	O
with	O
an	O
average	O
document	O
length	O
of	O
231	O
words	O
Wang2012	O
.	O

We	O
compare	O
only	O
to	O
other	O
results	O
that	O
do	O
not	O
make	O
use	O
of	O
additional	O
unlabeled	O
data	O
(	O
thus	O
excluding	O
e.g.	O
,	O
Miyato2016	O
)	O
.	O

Our	O
best	O
performance	O
on	O
a	O
held	O
-	O
out	O
development	O
set	O
was	O
achieved	O
using	O
a	O
four	B-Method
-	I-Method
layer	I-Method
densely	I-Method
-	I-Method
connected	I-Method
QRNN	E-Method
with	O
256	O
units	O
per	O
layer	O
and	O
word	O
vectors	O
initialized	O
using	O
300	B-Method
-	I-Method
dimensional	I-Method
cased	I-Method
GloVe	I-Method
embeddings	E-Method
Pennington2014	O
.	O

Dropout	O
of	O
0.3	O
was	O
applied	O
between	O
layers	O
,	O
and	O
we	O
used	O
regularization	B-Method
of	E-Method
.	O

Optimization	S-Task
was	O
performed	O
on	O
minibatches	O
of	O
24	O
examples	O
using	O
RMSprop	S-Method
Tieleman2012	O
with	O
learning	B-Metric
rate	E-Metric
of	O
,	O
,	O
and	O
.	O

Small	O
batch	O
sizes	O
and	O
long	O
sequence	O
lengths	O
provide	O
an	O
ideal	O
situation	O
for	O
demonstrating	O
the	O
QRNN	B-Method
’s	E-Method
performance	O
advantages	O
over	O
traditional	O
recurrent	B-Method
architectures	E-Method
.	O

We	O
observed	O
a	O
speedup	O
of	O
3.2x	O
on	O
IMDb	B-Metric
train	I-Metric
time	E-Metric
per	O
epoch	O
compared	O
to	O
the	O
optimized	O
LSTM	S-Method
implementation	O
provided	O
in	O
NVIDIA	B-Method
’s	I-Method
cuDNN	I-Method
library	E-Method
.	O

For	O
specific	O
batch	O
sizes	O
and	O
sequence	O
lengths	O
,	O
a	O
16x	O
speed	O
gain	O
is	O
possible	O
.	O

Figure	O
[	O
reference	O
]	O
provides	O
extensive	O
speed	O
comparisons	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
visualize	O
the	O
hidden	O
state	O
vectors	O
of	O
the	O
final	O
QRNN	B-Method
layer	E-Method
on	O
part	O
of	O
an	O
example	O
from	O
the	O
IMDb	B-Material
dataset	E-Material
.	O

Even	O
without	O
any	O
post	B-Method
-	I-Method
processing	E-Method
,	O
changes	O
in	O
the	O
hidden	O
state	O
are	O
visible	O
and	O
interpretable	O
in	O
regards	O
to	O
the	O
input	O
.	O

This	O
is	O
a	O
consequence	O
of	O
the	O
elementwise	O
nature	O
of	O
the	O
recurrent	B-Method
pooling	I-Method
function	E-Method
,	O
which	O
delays	O
direct	O
interaction	O
between	O
different	O
channels	O
of	O
the	O
hidden	O
state	O
until	O
the	O
computation	O
of	O
the	O
next	O
QRNN	B-Method
layer	E-Method
.	O

subsection	O
:	O
Language	B-Method
Modeling	E-Method
We	O
replicate	O
the	O
language	B-Method
modeling	E-Method
experiment	O
of	O
Zaremba2014	O
and	O
Gal2015	O
to	O
benchmark	O
the	O
QRNN	B-Method
architecture	E-Method
for	O
natural	B-Task
language	I-Task
sequence	I-Task
prediction	E-Task
.	O

The	O
experiment	O
uses	O
a	O
standard	O
preprocessed	B-Method
version	E-Method
of	O
the	O
Penn	B-Material
Treebank	E-Material
(	O
PTB	S-Material
)	O
by	O
Mikolov2010	O
.	O

We	O
implemented	O
a	O
gated	B-Method
QRNN	I-Method
model	E-Method
with	O
medium	O
hidden	O
size	O
:	O
2	O
layers	O
with	O
640	O
units	O
in	O
each	O
layer	O
.	O

Both	O
QRNN	B-Method
layers	E-Method
use	O
a	O
convolutional	B-Method
filter	I-Method
width	E-Method
of	O
two	O
timesteps	O
.	O

While	O
the	O
“	O
medium	B-Method
”	I-Method
models	E-Method
used	O
in	O
other	O
work	O
Zaremba2014	O
,	O
Gal2015	O
consist	O
of	O
650	O
units	O
in	O
each	O
layer	O
,	O
it	O
was	O
more	O
computationally	O
convenient	O
to	O
use	O
a	O
multiple	O
of	O
32	O
.	O

As	O
the	O
Penn	B-Material
Treebank	E-Material
is	O
a	O
relatively	O
small	O
dataset	O
,	O
preventing	O
overfitting	S-Task
is	O
of	O
considerable	O
importance	O
and	O
a	O
major	O
focus	O
of	O
recent	O
research	O
.	O

It	O
is	O
not	O
obvious	O
in	O
advance	O
which	O
of	O
the	O
many	O
RNN	B-Method
regularization	I-Method
schemes	E-Method
would	O
perform	O
well	O
when	O
applied	O
to	O
the	O
QRNN	S-Method
.	O

Our	O
tests	O
showed	O
encouraging	O
results	O
from	O
zoneout	S-Method
applied	O
to	O
the	O
QRNN	B-Method
’s	I-Method
recurrent	I-Method
pooling	I-Method
layer	E-Method
,	O
implemented	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
experimental	O
settings	O
largely	O
followed	O
the	O
“	O
medium	O
”	O
setup	O
of	O
Zaremba2014	O
.	O

Optimization	S-Task
was	O
performed	O
by	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
without	O
momentum	S-Method
.	O

The	O
learning	B-Metric
rate	E-Metric
was	O
set	O
at	O
1	O
for	O
six	O
epochs	O
,	O
then	O
decayed	O
by	O
0.95	O
for	O
each	O
subsequent	O
epoch	O
,	O
for	O
a	O
total	O
of	O
72	O
epochs	O
.	O

We	O
additionally	O
used	O
regularization	O
of	O
and	O
rescaled	O
gradients	O
with	O
norm	O
above	O
10	O
.	O

Zoneout	S-Method
was	O
applied	O
by	O
performing	O
dropout	S-Method
with	O
ratio	O
0.1	O
on	O
the	O
forget	O
gates	O
of	O
the	O
QRNN	S-Method
,	O
without	O
rescaling	O
the	O
output	O
of	O
the	O
dropout	O
function	O
.	O

Batches	O
consist	O
of	O
20	O
examples	O
,	O
each	O
105	O
timesteps	O
.	O

Comparing	O
our	O
results	O
on	O
the	O
gated	B-Method
QRNN	E-Method
with	O
zoneout	S-Method
to	O
the	O
results	O
of	O
LSTMs	S-Method
with	O
both	O
ordinary	B-Method
and	I-Method
variational	I-Method
dropout	E-Method
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
the	O
QRNN	S-Method
is	O
highly	O
competitive	O
.	O

The	O
QRNN	O
without	O
zoneout	S-Method
strongly	O
outperforms	O
both	O
our	O
medium	O
LSTM	S-Method
and	O
the	O
medium	O
LSTM	S-Method
of	O
Zaremba2014	S-Method
which	O
do	O
not	O
use	O
recurrent	B-Method
dropout	E-Method
and	O
is	O
even	O
competitive	O
with	O
variational	B-Method
LSTMs	E-Method
.	O

This	O
may	O
be	O
due	O
to	O
the	O
limited	O
computational	O
capacity	O
that	O
the	O
QRNN	B-Method
’s	I-Method
pooling	I-Method
layer	E-Method
has	O
relative	O
to	O
the	O
LSTM	S-Method
’s	O
recurrent	O
weights	O
,	O
providing	O
structural	O
regularization	O
over	O
the	O
recurrence	O
.	O

Without	O
zoneout	S-Method
,	O
early	O
stopping	O
based	O
upon	O
validation	B-Metric
loss	E-Metric
was	O
required	O
as	O
the	O
QRNN	S-Method
would	O
begin	O
overfitting	O
.	O

By	O
applying	O
a	O
small	O
amount	O
of	O
zoneout	S-Method
(	O
)	O
,	O
no	O
early	O
stopping	O
is	O
required	O
and	O
the	O
QRNN	S-Method
achieves	O
competitive	O
levels	O
of	O
perplexity	S-Metric
to	O
the	O
variational	O
LSTM	S-Method
of	O
Gal2015	O
,	O
which	O
had	O
variational	B-Method
inference	I-Method
based	I-Method
dropout	I-Method
of	I-Method
0.2	E-Method
applied	O
recurrently	O
.	O

Their	O
best	O
performing	O
variation	O
also	O
used	O
Monte	B-Method
Carlo	I-Method
(	I-Method
MC	I-Method
)	I-Method
dropout	I-Method
averaging	E-Method
at	O
test	O
time	O
of	O
1000	O
different	O
masks	O
,	O
making	O
it	O
computationally	O
more	O
expensive	O
to	O
run	O
.	O

When	O
training	O
on	O
the	O
PTB	S-Material
dataset	O
with	O
an	O
NVIDIA	B-Method
K40	I-Method
GPU	E-Method
,	O
we	O
found	O
that	O
the	O
QRNN	S-Method
is	O
substantially	O
faster	O
than	O
a	O
standard	O
LSTM	S-Method
,	O
even	O
when	O
comparing	O
against	O
the	O
optimized	O
cuDNN	O
LSTM	S-Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
provide	O
a	O
breakdown	O
of	O
the	O
time	O
taken	O
for	O
Chainer	O
’s	O
default	O
LSTM	S-Method
,	O
the	O
cuDNN	O
LSTM	S-Method
,	O
and	O
QRNN	S-Method
to	O
perform	O
a	O
full	O
forward	B-Method
and	I-Method
backward	I-Method
pass	E-Method
on	O
a	O
single	O
batch	O
during	O
training	O
of	O
the	O
RNN	B-Method
LM	E-Method
on	O
PTB	S-Material
.	O

For	O
both	O
LSTM	S-Method
implementations	O
,	O
running	B-Metric
time	E-Metric
was	O
dominated	O
by	O
the	O
RNN	B-Method
computations	E-Method
,	O
even	O
with	O
the	O
highly	O
optimized	O
cuDNN	B-Method
implementation	E-Method
.	O

For	O
the	O
QRNN	B-Method
implementation	E-Method
,	O
however	O
,	O
the	O
“	O
RNN	B-Method
”	I-Method
layers	E-Method
are	O
no	O
longer	O
the	O
bottleneck	O
.	O

Indeed	O
,	O
there	O
are	O
diminishing	O
returns	O
from	O
further	O
optimization	S-Task
of	O
the	O
QRNN	B-Method
itself	E-Method
as	O
the	O
softmax	O
and	O
optimization	O
overhead	O
take	O
equal	O
or	O
greater	O
time	O
.	O

Note	O
that	O
the	O
softmax	S-Method
,	O
over	O
a	O
vocabulary	O
size	O
of	O
only	O
10	O
,	O
000	O
words	O
,	O
is	O
relatively	O
small	O
;	O
for	O
tasks	O
with	O
larger	O
vocabularies	O
,	O
the	O
softmax	S-Method
would	O
likely	O
dominate	O
computation	B-Metric
time	E-Metric
.	O

It	O
is	O
also	O
important	O
to	O
note	O
that	O
the	O
cuDNN	B-Method
library	I-Method
’s	I-Method
RNN	I-Method
primitives	E-Method
do	O
not	O
natively	O
support	O
any	O
form	O
of	O
recurrent	B-Method
dropout	E-Method
.	O

That	O
is	O
,	O
running	O
an	O
LSTM	S-Method
that	O
uses	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
regularization	B-Method
scheme	E-Method
at	O
cuDNN	S-Method
-	O
like	O
speeds	O
would	O
likely	O
require	O
an	O
entirely	O
custom	O
kernel	O
.	O

Batch	O
size	O
subsection	O
:	O
Character	B-Task
-	I-Task
level	I-Task
Neural	I-Task
Machine	I-Task
Translation	E-Task
We	O
evaluate	O
the	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
QRNN	I-Method
architecture	E-Method
described	O
in	O
[	O
reference	O
]	O
on	O
a	O
challenging	O
neural	O
machine	B-Task
translation	E-Task
task	O
,	O
IWSLT	B-Task
German	I-Task
–	I-Task
English	I-Task
spoken	I-Task
-	I-Task
domain	I-Task
translation	E-Task
,	O
applying	O
fully	B-Task
character	I-Task
-	I-Task
level	I-Task
segmentation	E-Task
.	O

This	O
dataset	O
consists	O
of	O
209	O
,	O
772	O
sentence	O
pairs	O
of	O
parallel	O
training	O
data	O
from	O
transcribed	O
TED	O
and	O
TEDx	O
presentations	O
,	O
with	O
a	O
mean	O
sentence	O
length	O
of	O
103	O
characters	O
for	O
German	S-Material
and	O
93	O
for	O
English	S-Material
.	O

We	O
remove	O
training	O
sentences	O
with	O
more	O
than	O
300	O
characters	O
in	O
English	O
or	O
German	O
,	O
and	O
use	O
a	O
unified	O
vocabulary	O
of	O
187	O
Unicode	O
code	O
points	O
.	O

Our	O
best	O
performance	O
on	O
a	O
development	O
set	O
(	O
TED.tst2013	S-Material
)	O
was	O
achieved	O
using	O
a	O
four	B-Method
-	I-Method
layer	I-Method
encoder	I-Method
–	I-Method
decoder	I-Method
QRNN	E-Method
with	O
320	O
units	O
per	O
layer	O
,	O
no	O
dropout	O
or	O
regularization	O
,	O
and	O
gradient	B-Method
rescaling	E-Method
to	O
a	O
maximum	O
magnitude	O
of	O
5	O
.	O

Inputs	O
were	O
supplied	O
to	O
the	O
encoder	O
reversed	O
,	O
while	O
the	O
encoder	B-Method
convolutions	E-Method
were	O
not	O
masked	O
.	O

The	O
first	O
encoder	B-Method
layer	E-Method
used	O
convolutional	O
filter	O
width	O
,	O
while	O
the	O
other	O
encoder	B-Method
layers	E-Method
used	O
.	O

Optimization	S-Task
was	O
performed	O
for	O
10	O
epochs	O
on	O
minibatches	O
of	O
16	O
examples	O
using	O
Adam	B-Method
kingma2014adam	E-Method
with	O
,	O
,	O
,	O
and	O
.	O

Decoding	S-Task
was	O
performed	O
using	O
beam	B-Method
search	E-Method
with	O
beam	O
width	O
8	O
and	O
length	B-Method
normalization	E-Method
.	O

The	O
modified	O
log	B-Metric
-	I-Metric
probability	I-Metric
ranking	I-Metric
criterion	E-Metric
is	O
provided	O
in	O
the	O
appendix	O
.	O

Results	O
using	O
this	O
architecture	O
were	O
compared	O
to	O
an	O
equal	O
-	O
sized	O
four	O
-	O
layer	O
encoder	O
–	O
decoder	O
LSTM	S-Method
with	O
attention	S-Method
,	O
applying	O
dropout	B-Method
of	I-Method
0.2	E-Method
.	O

We	O
again	O
optimized	O
using	O
Adam	S-Method
;	O
other	O
hyperparameters	O
were	O
equal	O
to	O
their	O
values	O
for	O
the	O
QRNN	S-Method
and	O
the	O
same	O
beam	B-Method
search	I-Method
procedure	E-Method
was	O
applied	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
QRNN	S-Method
outperformed	O
the	O
character	O
-	O
level	O
LSTM	S-Method
,	O
almost	O
matching	O
the	O
performance	O
of	O
a	O
word	B-Method
-	I-Method
level	I-Method
attentional	I-Method
baseline	E-Method
.	O

section	O
:	O
Related	O
Work	O
Exploring	O
alternatives	O
to	O
traditional	O
RNNs	S-Method
for	O
sequence	B-Task
tasks	E-Task
is	O
a	O
major	O
area	O
of	O
current	O
research	O
.	O

Quasi	B-Method
-	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
are	O
related	O
to	O
several	O
such	O
recently	O
described	O
models	O
,	O
especially	O
the	O
strongly	B-Method
-	I-Method
typed	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
(	O
T	B-Method
-	I-Method
RNN	E-Method
)	O
introduced	O
by	O
.	O

While	O
the	O
motivation	O
and	O
constraints	O
described	O
in	O
that	O
work	O
are	O
different	O
,	O
’s	O
concepts	O
of	O
“	O
learnware	O
”	O
and	O
“	O
firmware	S-Method
”	O
parallel	O
our	O
discussion	O
of	O
convolution	B-Method
-	I-Method
like	I-Method
and	I-Method
pooling	I-Method
-	I-Method
like	I-Method
subcomponents	E-Method
.	O

As	O
the	O
use	O
of	O
a	O
fully	O
connected	O
layer	O
for	O
recurrent	O
connections	O
violates	O
the	O
constraint	O
of	O
“	O
strong	O
typing	O
”	O
,	O
all	O
strongly	B-Method
-	I-Method
typed	I-Method
RNN	I-Method
architectures	E-Method
(	O
including	O
the	O
T	B-Method
-	I-Method
RNN	E-Method
,	O
T	B-Method
-	I-Method
GRU	E-Method
,	O
and	O
T	O
-	O
LSTM	S-Method
)	O
are	O
also	O
quasi	B-Method
-	I-Method
recurrent	E-Method
.	O

However	O
,	O
some	O
QRNN	B-Method
models	E-Method
(	O
including	O
those	O
with	O
attention	O
or	O
skip	O
-	O
connections	O
)	O
are	O
not	O
“	O
strongly	O
typed	O
”	O
.	O

In	O
particular	O
,	O
a	O
T	B-Method
-	I-Method
RNN	E-Method
differs	O
from	O
a	O
QRNN	S-Method
as	O
described	O
in	O
this	O
paper	O
with	O
filter	O
size	O
1	O
and	O
f	B-Method
-	I-Method
pooling	E-Method
only	O
in	O
the	O
absence	O
of	O
an	O
activation	O
function	O
on	O
.	O

Similarly	O
,	O
T	B-Method
-	I-Method
GRUs	E-Method
and	O
T	B-Method
-	I-Method
LSTMs	E-Method
differ	O
from	O
QRNNs	S-Method
with	O
filter	O
size	O
2	O
and	O
fo	B-Method
-	I-Method
or	I-Method
ifo	I-Method
-	I-Method
pooling	E-Method
respectively	O
in	O
that	O
they	O
lack	O
on	O
and	O
use	O
rather	O
than	O
sigmoid	O
on	O
.	O

The	O
QRNN	S-Method
is	O
also	O
related	O
to	O
work	O
in	O
hybrid	B-Method
convolutional	I-Method
–	I-Method
recurrent	I-Method
models	E-Method
.	O

Zhou2015b	O
apply	O
CNNs	S-Method
at	O
the	O
word	O
level	O
to	O
generate	O
-	O
gram	O
features	O
used	O
by	O
an	O
LSTM	S-Method
for	O
text	B-Task
classification	E-Task
.	O

Xiao2016	O
also	O
tackle	O
text	B-Task
classification	E-Task
by	O
applying	O
convolutions	S-Method
at	O
the	O
character	O
level	O
,	O
with	O
a	O
stride	O
to	O
reduce	O
sequence	O
length	O
,	O
then	O
feeding	O
these	O
features	O
into	O
a	O
bidirectional	O
LSTM	S-Method
.	O

A	O
similar	O
approach	O
was	O
taken	O
by	O
Lee2016	O
for	O
character	O
-	O
level	O
machine	B-Task
translation	E-Task
.	O

Their	O
model	O
’s	O
encoder	S-Method
uses	O
a	O
convolutional	B-Method
layer	E-Method
followed	O
by	O
max	B-Method
-	I-Method
pooling	E-Method
to	O
reduce	O
sequence	O
length	O
,	O
a	O
four	B-Method
-	I-Method
layer	I-Method
highway	I-Method
network	E-Method
,	O
and	O
a	O
bidirectional	B-Method
GRU	E-Method
.	O

The	O
parallelism	O
of	O
the	O
convolutional	B-Method
,	I-Method
pooling	I-Method
,	I-Method
and	I-Method
highway	I-Method
layers	E-Method
allows	O
training	O
speed	O
comparable	O
to	O
subword	B-Method
-	I-Method
level	I-Method
models	E-Method
without	O
hard	B-Task
-	I-Task
coded	I-Task
text	I-Task
segmentation	E-Task
.	O

The	O
QRNN	B-Method
encoder	I-Method
–	I-Method
decoder	I-Method
model	E-Method
shares	O
the	O
favorable	O
parallelism	O
and	O
path	O
-	O
length	O
properties	O
exhibited	O
by	O
the	O
ByteNet	S-Method
Kalchbrenner2016	O
,	O
an	O
architecture	O
for	O
character	O
-	O
level	O
machine	B-Task
translation	E-Task
based	O
on	O
residual	B-Method
convolutions	E-Method
over	O
binary	O
trees	O
.	O

Their	O
model	O
was	O
constructed	O
to	O
achieve	O
three	O
desired	O
properties	O
:	O
parallelism	O
,	O
linear	B-Metric
-	I-Metric
time	I-Metric
computational	I-Metric
complexity	E-Metric
,	O
and	O
short	O
paths	O
between	O
any	O
pair	O
of	O
words	O
in	O
order	O
to	O
better	O
propagate	O
gradient	O
signals	O
.	O

section	O
:	O
Conclusion	O
Intuitively	O
,	O
many	O
aspects	O
of	O
the	O
semantics	O
of	O
long	O
sequences	O
are	O
context	O
-	O
invariant	O
and	O
can	O
be	O
computed	O
in	O
parallel	O
(	O
e.g.	O
,	O
convolutionally	O
)	O
,	O
but	O
some	O
aspects	O
require	O
long	O
-	O
distance	O
context	O
and	O
must	O
be	O
computed	O
recurrently	O
.	O

Many	O
existing	O
neural	B-Method
network	I-Method
architectures	E-Method
either	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
contextual	O
information	O
or	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
parallelism	O
.	O

QRNNs	S-Method
exploit	O
both	O
parallelism	O
and	O
context	O
,	O
exhibiting	O
advantages	O
from	O
both	O
convolutional	B-Method
and	I-Method
recurrent	I-Method
neural	I-Method
networks	E-Method
.	O

QRNNs	S-Method
have	O
better	O
predictive	B-Metric
accuracy	E-Metric
than	O
LSTM	S-Method
-	O
based	O
models	O
of	O
equal	B-Method
hidden	I-Method
size	E-Method
,	O
even	O
though	O
they	O
use	O
fewer	O
parameters	O
and	O
run	O
substantially	O
faster	O
.	O

Our	O
experiments	O
show	O
that	O
the	O
speed	B-Metric
and	I-Metric
accuracy	I-Metric
advantages	E-Metric
remain	O
consistent	O
across	O
tasks	O
and	O
at	O
both	O
word	O
and	O
character	O
levels	O
.	O

Extensions	O
to	O
both	O
CNNs	S-Method
and	O
RNNs	S-Method
are	O
often	O
directly	O
applicable	O
to	O
the	O
QRNN	S-Method
,	O
while	O
the	O
model	O
’s	O
hidden	O
states	O
are	O
more	O
interpretable	O
than	O
those	O
of	O
other	O
recurrent	B-Method
architectures	E-Method
as	O
its	O
channels	O
maintain	O
their	O
independence	O
across	O
timesteps	O
.	O

We	O
believe	O
that	O
QRNNs	S-Method
can	O
serve	O
as	O
a	O
building	O
block	O
for	O
long	B-Task
-	I-Task
sequence	I-Task
tasks	E-Task
that	O
were	O
previously	O
impractical	O
with	O
traditional	O
RNNs	S-Method
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
Beam	B-Metric
search	I-Metric
ranking	I-Metric
criterion	E-Metric
The	O
modified	O
log	B-Metric
-	I-Metric
probability	I-Metric
ranking	I-Metric
criterion	E-Metric
we	O
used	O
in	O
beam	B-Task
search	E-Task
for	O
translation	B-Task
experiments	E-Task
is	O
:	O
where	O
is	O
a	O
length	O
normalization	O
parameter	O
Wu2016	O
,	O
is	O
the	O
th	O
output	O
character	O
,	O
and	O
is	O
a	O
“	O
target	O
length	O
”	O
equal	O
to	O
the	O
source	O
sentence	O
length	O
plus	O
five	O
characters	O
.	O

This	O
reduces	O
at	O
to	O
ordinary	O
beam	B-Method
search	E-Method
with	O
probabilities	O
:	O
and	O
at	O
to	O
beam	B-Method
search	E-Method
with	O
probabilities	O
normalized	O
by	O
length	O
(	O
up	O
to	O
the	O
target	O
length	O
)	O
:	O
Conveniently	O
,	O
this	O
ranking	B-Metric
criterion	E-Metric
can	O
be	O
computed	O
at	O
intermediate	O
beam	O
-	O
search	O
timesteps	O
,	O
obviating	O
the	O
need	O
to	O
apply	O
a	O
separate	O
reranking	S-Method
on	O
complete	O
hypotheses	O
.	O

document	O
:	O
The	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
:	O
An	O
Evaluation	B-Method
Platform	E-Method
for	O
General	B-Task
Agents	E-Task
In	O
this	O
article	O
we	O
introduce	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
(	O
ALE	S-Method
)	O
:	O
both	O
a	O
challenge	O
problem	O
and	O
a	O
platform	O
and	O
methodology	O
for	O
evaluating	O
the	O
development	O
of	O
general	B-Task
,	I-Task
domain	I-Task
-	I-Task
independent	I-Task
AI	I-Task
technology	E-Task
.	O

ALE	S-Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	B-Material
2600	I-Material
game	I-Material
environments	E-Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

ALE	S-Method
presents	O
significant	O
research	O
challenges	O
for	O
reinforcement	B-Task
learning	E-Task
,	O
model	B-Method
learning	E-Method
,	O
model	B-Method
-	I-Method
based	I-Method
planning	E-Method
,	O
imitation	B-Task
learning	E-Task
,	O
transfer	B-Task
learning	E-Task
,	O
and	O
intrinsic	B-Task
motivation	E-Task
.	O

Most	O
importantly	O
,	O
it	O
provides	O
a	O
rigorous	O
testbed	O
for	O
evaluating	O
and	O
comparing	O
approaches	O
to	O
these	O
problems	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	S-Method
by	O
developing	O
and	O
benchmarking	O
domain	B-Method
-	I-Method
independent	I-Method
agents	E-Method
designed	O
using	O
well	O
-	O
established	O
AI	B-Method
techniques	E-Method
for	O
both	O
reinforcement	B-Task
learning	E-Task
and	O
planning	S-Task
.	O

In	O
doing	O
so	O
,	O
we	O
also	O
propose	O
an	O
evaluation	O
methodology	O
made	O
possible	O
by	O
ALE	S-Method
,	O
reporting	O
empirical	O
results	O
on	O
over	O
55	O
different	O
games	O
.	O

All	O
of	O
the	O
software	O
,	O
including	O
the	O
benchmark	O
agents	O
,	O
is	O
publicly	O
available	O
.	O

472013253–27902	O
/	O
1306	O
/	O
13	O
TheArcadeLearningEnvironment	O
:	O
AnEvaluationPlatformforGeneralAgents	O
Bellemare	O
,	O
Naddaf	O
,	O
Veness	O
,&	O
Bowling	O
253	O
section	O
:	O
Introduction	O
A	O
longstanding	O
goal	O
of	O
artificial	B-Task
intelligence	E-Task
is	O
the	O
development	O
of	O
algorithms	O
capable	O
of	O
general	B-Task
competency	E-Task
in	O
a	O
variety	O
of	O
tasks	O
and	O
domains	O
without	O
the	O
need	O
for	O
domain	O
-	O
specific	O
tailoring	O
.	O

To	O
this	O
end	O
,	O
different	O
theoretical	O
frameworks	O
have	O
been	O
proposed	O
to	O
formalize	O
the	O
notion	O
of	O
“	O
big	O
”	O
artificial	O
intelligence	O
e.g.	O
,	O
¿Russell97rationalityand	O
,	O
Hutter:04uaibook	O
,	O
legg08machine	O
.	O

Similar	O
ideas	O
have	O
been	O
developed	O
around	O
the	O
theme	O
of	O
lifelong	B-Task
learning	E-Task
:	O
learning	O
a	O
reusable	B-Task
,	I-Task
high	I-Task
-	I-Task
level	I-Task
understanding	I-Task
of	I-Task
the	I-Task
world	E-Task
from	O
raw	B-Material
sensory	I-Material
data	E-Material
thrun95lifelong	O
,	O
pierce_kuipers_97	O
,	O
stober08pixels	O
,	O
sutton11horde	O
.	O

The	O
growing	O
interest	O
in	O
competitions	O
such	O
as	O
the	O
General	B-Task
Game	I-Task
Playing	I-Task
competition	E-Task
,	O
Reinforcement	B-Task
Learning	I-Task
competition	E-Task
,	O
and	O
the	O
International	B-Task
Planning	I-Task
competition	E-Task
coles_12	O
also	O
suggests	O
the	O
artificial	B-Task
intelligence	E-Task
community	O
’s	O
desire	O
for	O
the	O
emergence	O
of	O
algorithms	O
that	O
provide	O
general	O
competency	O
.	O

Designing	O
generally	O
competent	O
agents	O
raises	O
the	O
question	O
of	O
how	O
to	O
best	O
evaluate	O
them	O
.	O

Empirically	O
evaluating	O
general	B-Task
competency	E-Task
on	O
a	O
handful	O
of	O
parametrized	B-Material
benchmark	I-Material
problems	E-Material
is	O
,	O
by	O
definition	O
,	O
flawed	O
.	O

Such	O
an	O
evaluation	O
is	O
prone	O
to	O
method	O
overfitting	O
and	O
discounts	O
the	O
amount	O
of	O
expert	O
effort	O
necessary	O
to	O
transfer	O
the	O
algorithm	O
to	O
new	O
domains	O
.	O

Ideally	O
,	O
the	O
algorithm	O
should	O
be	O
compared	O
across	O
domains	O
that	O
are	O
(	O
i	O
)	O
varied	O
enough	O
to	O
claim	O
generality	O
,	O
(	O
ii	O
)	O
each	O
interesting	O
enough	O
to	O
be	O
representative	O
of	O
settings	O
that	O
might	O
be	O
faced	O
in	O
practice	O
,	O
and	O
(	O
iii	O
)	O
each	O
created	O
by	O
an	O
independent	O
party	O
to	O
be	O
free	O
of	O
experimenter	O
’s	O
bias	O
.	O

In	O
this	O
article	O
,	O
we	O
introduce	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
(	O
ALE	S-Method
)	O
:	O
a	O
new	O
challenge	O
problem	O
,	O
platform	O
,	O
and	O
experimental	O
methodology	O
for	O
empirically	B-Task
assessing	I-Task
agents	E-Task
designed	O
for	O
general	B-Task
competency	E-Task
.	O

ALE	S-Method
is	O
a	O
software	B-Method
framework	E-Method
for	O
interfacing	O
with	O
emulated	O
Atari	B-Material
2600	I-Material
game	I-Material
environments	E-Material
.	O

The	O
Atari	B-Method
2600	E-Method
,	O
a	O
second	B-Method
generation	I-Method
game	I-Method
console	E-Method
,	O
was	O
originally	O
released	O
in	O
1977	O
and	O
remained	O
massively	O
popular	O
for	O
over	O
a	O
decade	O
.	O

Over	O
500	O
games	O
were	O
developed	O
for	O
the	O
Atari	B-Material
2600	E-Material
,	O
spanning	O
a	O
diverse	O
range	O
of	O
genres	O
such	O
as	O
shooters	S-Material
,	O
beat’em	B-Material
ups	E-Material
,	O
puzzle	S-Material
,	O
sports	S-Material
,	O
and	O
action	B-Task
-	I-Task
adventure	I-Task
games	E-Task
;	O
many	O
game	O
genres	O
were	O
pioneered	O
on	O
the	O
console	O
.	O

While	O
modern	O
game	O
consoles	O
involve	O
visuals	O
,	O
controls	O
,	O
and	O
a	O
general	O
complexity	O
that	O
rivals	O
the	O
real	O
world	O
,	O
Atari	B-Material
2600	I-Material
games	E-Material
are	O
far	O
simpler	O
.	O

In	O
spite	O
of	O
this	O
,	O
they	O
still	O
pose	O
a	O
variety	O
of	O
challenging	O
and	O
interesting	O
situations	O
for	O
human	O
players	O
.	O

ALE	S-Method
is	O
both	O
an	O
experimental	O
methodology	O
and	O
a	O
challenge	O
problem	O
for	O
general	B-Task
AI	I-Task
competency	E-Task
.	O

In	O
machine	B-Task
learning	E-Task
,	O
it	O
is	O
considered	O
poor	O
experimental	O
practice	O
to	O
both	O
train	O
and	O
evaluate	O
an	O
algorithm	O
on	O
the	O
same	O
data	O
set	O
,	O
as	O
it	O
can	O
grossly	O
over	O
-	O
estimate	O
the	O
algorithm	O
’s	O
performance	O
.	O

The	O
typical	O
practice	O
is	O
instead	O
to	O
train	O
on	O
a	O
training	O
set	O
then	O
evaluate	O
on	O
a	O
disjoint	O
test	O
set	O
.	O

With	O
the	O
large	O
number	O
of	O
available	O
games	O
in	O
ALE	S-Method
,	O
we	O
propose	O
that	O
a	O
similar	O
methodology	O
can	O
be	O
used	O
to	O
the	O
same	O
effect	O
:	O
an	O
approach	O
’s	O
domain	B-Method
representation	E-Method
and	O
parametrization	S-Method
should	O
be	O
first	O
tuned	O
on	O
a	O
small	O
number	O
of	O
training	O
games	O
,	O
before	O
testing	O
the	O
approach	O
on	O
unseen	B-Material
testing	I-Material
games	E-Material
.	O

Ideally	O
,	O
agents	O
designed	O
in	O
this	O
fashion	O
are	O
evaluated	O
on	O
the	O
testing	O
games	O
only	O
once	O
,	O
with	O
no	O
possibility	O
for	O
subsequent	O
modifications	O
to	O
the	O
algorithm	O
.	O

While	O
general	B-Method
competency	E-Method
remains	O
the	O
long	O
-	O
term	O
goal	O
for	O
artificial	B-Task
intelligence	E-Task
,	O
ALE	S-Method
proposes	O
an	O
achievable	O
stepping	O
stone	O
:	O
techniques	O
for	O
general	B-Task
competency	E-Task
across	O
the	O
gamut	O
of	O
Atari	B-Material
2600	I-Material
games	E-Material
.	O

We	O
believe	O
this	O
represents	O
a	O
goal	O
that	O
is	O
attainable	O
in	O
a	O
short	O
time	O
-	O
frame	O
yet	O
formidable	O
enough	O
to	O
require	O
new	O
technological	O
breakthroughs	O
.	O

section	O
:	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
We	O
begin	O
by	O
describing	O
our	O
main	O
contribution	O
,	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
(	O
ALE	S-Method
)	O
.	O

ALE	S-Method
is	O
a	O
software	B-Method
framework	E-Method
designed	O
to	O
make	O
it	O
easy	O
to	O
develop	O
agents	O
that	O
play	O
arbitrary	O
Atari	B-Material
2600	I-Material
games	E-Material
.	O

subsection	O
:	O
The	O
Atari	B-Material
2600	E-Material
The	O
Atari	B-Method
2600	E-Method
is	O
a	O
home	B-Method
video	I-Method
game	I-Method
console	E-Method
developed	O
in	O
1977	O
and	O
sold	O
for	O
over	O
a	O
decade	O
.	O

It	O
popularized	O
the	O
use	O
of	O
general	B-Method
purpose	I-Method
CPUs	E-Method
in	O
game	O
console	O
hardware	O
,	O
with	O
game	O
code	O
distributed	O
through	O
cartridges	O
.	O

Over	O
500	O
original	O
games	O
were	O
released	O
for	O
the	O
console	O
;	O
“	O
homebrew	B-Material
”	I-Material
games	E-Material
continue	O
to	O
be	O
developed	O
today	O
,	O
over	O
thirty	O
years	O
later	O
.	O

The	O
console	O
’s	O
joystick	O
,	O
as	O
well	O
as	O
some	O
of	O
the	O
original	O
games	O
such	O
as	O
Adventure	O
and	O
Pitfall	B-Material
!	E-Material
,	O
are	O
iconic	O
symbols	O
of	O
early	O
video	B-Material
games	E-Material
.	O

Nearly	O
all	O
arcade	B-Method
games	E-Method
of	O
the	O
time	O
–	O
Pac	B-Method
-	I-Method
Man	E-Method
and	O
Space	B-Task
Invaders	E-Task
are	O
two	O
well	O
-	O
known	O
examples	O
–	O
were	O
ported	O
to	O
the	O
console	O
.	O

Despite	O
the	O
number	O
and	O
variety	O
of	O
games	O
developed	O
for	O
the	O
Atari	B-Material
2600	E-Material
,	O
the	O
hardware	O
is	O
relatively	O
simple	O
.	O

It	O
has	O
a	O
1.19Mhz	B-Method
CPU	E-Method
and	O
can	O
be	O
emulated	O
much	O
faster	O
than	O
real	O
-	O
time	O
on	O
modern	O
hardware	O
.	O

The	O
cartridge	O
ROM	O
(	O
typically	O
2–4kB	O
)	O
holds	O
the	O
game	O
code	O
,	O
while	O
the	O
console	O
RAM	O
itself	O
only	O
holds	O
128	O
bytes	O
(	O
1024	O
bits	O
)	O
.	O

A	O
single	O
game	O
screen	O
is	O
160	O
pixels	O
wide	O
and	O
210	O
pixels	O
high	O
,	O
with	O
a	O
128	O
-	O
colour	O
palette	O
;	O
18	O
“	O
actions	O
”	O
can	O
be	O
input	O
to	O
the	O
game	O
via	O
a	O
digital	O
joystick	O
:	O
three	O
positions	O
of	O
the	O
joystick	O
for	O
each	O
axis	O
,	O
plus	O
a	O
single	O
button	O
.	O

The	O
Atari	B-Method
2600	I-Method
hardware	E-Method
limits	O
the	O
possible	O
complexity	O
of	O
games	O
,	O
which	O
we	O
believe	O
strikes	O
the	O
perfect	O
balance	O
:	O
a	O
challenging	O
platform	O
offering	O
conceivable	O
near	O
-	O
term	O
advancements	O
in	O
learning	S-Task
,	O
modelling	S-Task
,	O
and	O
planning	S-Task
.	O

subsection	O
:	O
Interface	O
ALE	S-Method
is	O
built	O
on	O
top	O
of	O
Stella	S-Method
,	O
an	O
open	B-Method
-	I-Method
source	I-Method
Atari	I-Method
2600	I-Method
emulator	E-Method
.	O

It	O
allows	O
the	O
user	O
to	O
interface	O
with	O
the	O
Atari	B-Material
2600	E-Material
by	O
receiving	O
joystick	O
motions	O
,	O
sending	O
screen	O
and	O
/	O
or	O
RAM	O
information	O
,	O
and	O
emulating	O
the	O
platform	O
.	O

ALE	S-Method
also	O
provides	O
a	O
game	B-Method
-	I-Method
handling	I-Method
layer	E-Method
which	O
transforms	O
each	O
game	O
into	O
a	O
standard	O
reinforcement	B-Task
learning	I-Task
problem	E-Task
by	O
identifying	O
the	O
accumulated	O
score	O
and	O
whether	O
the	O
game	O
has	O
ended	O
.	O

By	O
default	O
,	O
each	O
observation	O
consists	O
of	O
a	O
single	O
game	O
screen	O
(	O
frame	O
)	O
:	O
a	O
2D	O
array	O
of	O
7	O
-	O
bit	O
pixels	O
,	O
160	O
pixels	O
wide	O
by	O
210	O
pixels	O
high	O
.	O

The	O
action	O
space	O
consists	O
of	O
the	O
18	O
discrete	O
actions	O
defined	O
by	O
the	O
joystick	B-Method
controller	E-Method
.	O

The	O
game	B-Method
-	I-Method
handling	I-Method
layer	E-Method
also	O
specifies	O
the	O
minimal	O
set	O
of	O
actions	O
needed	O
to	O
play	O
a	O
particular	O
game	O
,	O
although	O
none	O
of	O
the	O
results	O
in	O
this	O
paper	O
make	O
use	O
of	O
this	O
information	O
.	O

When	O
running	O
in	O
real	O
-	O
time	O
,	O
the	O
simulator	O
generates	O
60	O
frames	O
per	O
second	O
,	O
and	O
at	O
full	O
speed	O
emulates	O
up	O
to	O
6000	O
frames	O
per	O
second	O
.	O

The	O
reward	O
at	O
each	O
time	O
-	O
step	O
is	O
defined	O
on	O
a	O
game	O
by	O
game	O
basis	O
,	O
typically	O
by	O
taking	O
the	O
difference	O
in	O
score	O
or	O
points	O
between	O
frames	O
.	O

An	O
episode	O
begins	O
on	O
the	O
first	O
frame	O
after	O
a	O
reset	O
command	O
is	O
issued	O
,	O
and	O
terminates	O
when	O
the	O
game	O
ends	O
.	O

The	O
game	B-Method
-	I-Method
handling	I-Method
layer	E-Method
also	O
offers	O
the	O
ability	O
to	O
end	O
the	O
episode	O
after	O
a	O
predefined	O
number	O
of	O
frames	O
.	O

The	O
user	O
therefore	O
has	O
access	O
to	O
several	O
dozen	O
games	O
through	O
a	O
single	O
common	O
interface	O
,	O
and	O
adding	O
support	O
for	O
new	O
games	O
is	O
relatively	O
straightforward	O
.	O

ALE	S-Method
further	O
provides	O
the	O
functionality	O
to	O
save	O
and	O
restore	O
the	O
state	O
of	O
the	O
emulator	O
.	O

When	O
issued	O
a	O
save	O
-	O
state	O
command	O
,	O
ALE	S-Method
saves	O
all	O
the	O
relevant	O
data	O
about	O
the	O
current	O
game	O
,	O
including	O
the	O
contents	O
of	O
the	O
RAM	O
,	O
registers	O
,	O
and	O
address	O
counters	O
.	O

The	O
restore	O
-	O
state	O
command	O
similarly	O
resets	O
the	O
game	O
to	O
a	O
previously	O
saved	O
state	O
.	O

This	O
allows	O
the	O
use	O
of	O
ALE	S-Method
as	O
a	O
generative	B-Method
model	E-Method
to	O
study	O
topics	O
such	O
as	O
planning	S-Task
and	O
model	B-Task
-	I-Task
based	I-Task
reinforcement	I-Task
learning	E-Task
.	O

subsection	O
:	O
Source	O
Code	O
ALE	S-Method
is	O
released	O
as	O
free	O
,	O
open	O
-	O
source	O
software	O
under	O
the	O
terms	O
of	O
the	O
GNU	O
General	O
Public	O
License	O
.	O

The	O
latest	O
version	O
of	O
the	O
source	O
code	O
is	O
publicly	O
available	O
at	O
:	O
The	O
source	O
code	O
for	O
the	O
agents	O
used	O
in	O
the	O
benchmark	O
experiments	O
below	O
is	O
also	O
available	O
on	O
the	O
publication	O
page	O
for	O
this	O
article	O
on	O
the	O
same	O
website	O
.	O

While	O
ALE	S-Method
itself	O
is	O
written	O
in	O
C	B-Method
++	E-Method
,	O
a	O
variety	O
of	O
interfaces	O
are	O
available	O
that	O
allow	O
users	O
to	O
interact	O
with	O
ALE	S-Method
in	O
the	O
programming	O
language	O
of	O
their	O
choice	O
.	O

Support	O
for	O
new	O
games	O
is	O
easily	O
added	O
by	O
implementing	O
a	O
derived	B-Method
class	E-Method
representing	O
the	O
game	O
’s	O
particular	O
reward	O
and	O
termination	O
functions	O
.	O

section	O
:	O
Benchmark	O
Results	O
Planning	S-Task
and	O
reinforcement	B-Method
learning	E-Method
are	O
two	O
different	O
AI	B-Task
problem	I-Task
formulations	E-Task
that	O
can	O
naturally	O
be	O
investigated	O
within	O
the	O
ALE	S-Method
framework	O
.	O

Our	O
purpose	O
in	O
presenting	O
benchmark	O
results	O
for	O
both	O
of	O
these	O
formulations	O
is	O
two	O
-	O
fold	O
.	O

First	O
,	O
these	O
results	O
provide	O
a	O
baseline	O
performance	O
for	O
traditional	O
techniques	O
,	O
establishing	O
a	O
point	O
of	O
comparison	O
with	O
future	O
,	O
more	O
advanced	O
,	O
approaches	O
.	O

Second	O
,	O
in	O
describing	O
these	O
results	O
we	O
illustrate	O
our	O
proposed	O
methodology	O
for	O
doing	O
empirical	B-Task
validation	E-Task
with	O
ALE	S-Method
.	O

subsection	O
:	O
Reinforcement	B-Method
Learning	E-Method
We	O
begin	O
by	O
providing	O
benchmark	O
results	O
using	O
SARSA	S-Method
,	O
a	O
traditional	O
technique	O
for	O
model	B-Method
-	I-Method
free	I-Method
reinforcement	I-Method
learning	E-Method
.	O

Note	O
that	O
in	O
the	O
reinforcement	B-Task
learning	I-Task
setting	E-Task
,	O
the	O
agent	O
does	O
not	O
have	O
access	O
to	O
a	O
model	O
of	O
the	O
game	O
dynamics	O
.	O

At	O
each	O
time	O
step	O
,	O
the	O
agent	O
selects	O
an	O
action	O
and	O
receives	O
a	O
reward	O
and	O
an	O
observation	O
,	O
and	O
the	O
agent	O
’s	O
aim	O
is	O
to	O
maximize	O
its	O
accumulated	O
reward	O
.	O

In	O
these	O
experiments	O
,	O
we	O
augmented	O
the	O
SARSA	B-Method
(	I-Method
)	I-Method
algorithm	E-Method
with	O
linear	B-Method
function	I-Method
approximation	E-Method
,	O
replacing	B-Method
traces	E-Method
,	O
and	O
-	B-Method
greedy	I-Method
exploration	E-Method
.	O

A	O
detailed	O
explanation	O
of	O
SARSA	B-Method
(	E-Method
)	O
and	O
its	O
extensions	O
can	O
be	O
found	O
in	O
the	O
work	O
of	O
sutton_barto_98	O
.	O

subsubsection	S-Method
:	O
Feature	B-Method
Construction	E-Method
In	O
our	O
approach	O
to	O
the	O
reinforcement	B-Task
learning	I-Task
setting	E-Task
,	O
the	O
most	O
important	O
design	O
issue	O
is	O
the	O
choice	O
of	O
features	O
to	O
use	O
with	O
linear	B-Method
function	I-Method
approximation	E-Method
.	O

We	O
ran	O
experiments	O
using	O
five	O
different	O
sets	O
of	O
features	O
,	O
which	O
we	O
now	O
briefly	O
explain	O
;	O
a	O
complete	O
description	O
of	O
these	O
feature	O
sets	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Of	O
these	O
sets	O
of	O
features	O
,	O
BASS	O
,	O
DISCO	S-Method
and	O
RAM	S-Method
were	O
originally	O
introduced	O
by	O
naddaf2010	O
,	O
while	O
the	O
rest	O
are	O
novel	O
.	O

paragraph	O
:	O
Basic	O
.	O

The	O
Basic	O
method	O
,	O
derived	O
from	O
naddaf2010	B-Method
’s	I-Method
BASS	I-Method
naddaf2010	E-Method
,	O
encodes	O
the	O
presence	O
of	O
colours	O
on	O
the	O
Atari	B-Material
2600	I-Material
screen	E-Material
.	O

The	O
Basic	O
method	O
first	O
removes	O
the	O
image	O
background	O
by	O
storing	O
the	O
frequency	O
of	O
colours	O
at	O
each	O
pixel	O
location	O
within	O
a	O
histogram	O
.	O

Each	O
game	B-Material
background	E-Material
is	O
precomputed	O
offline	O
,	O
using	O
18	O
,	O
000	O
observations	O
collected	O
from	O
sample	O
trajectories	O
.	O

The	O
sample	O
trajectories	O
are	O
generated	O
by	O
following	O
a	O
human	O
-	O
provided	O
trajectory	O
for	O
a	O
random	O
number	O
of	O
steps	O
and	O
subsequently	O
selecting	O
actions	O
uniformly	O
at	O
random	O
.	O

The	O
screen	O
is	O
then	O
divided	O
into	O
tiles	O
.	O

Basic	O
generates	O
one	O
binary	O
feature	O
for	O
each	O
of	O
the	O
colours	O
and	O
each	O
of	O
the	O
tiles	O
,	O
giving	O
a	O
total	O
of	O
28	O
,	O
672	O
features	O
.	O

paragraph	O
:	O
BASS	O
.	O

The	O
BASS	B-Method
method	E-Method
behaves	O
identically	O
to	O
the	O
Basic	O
method	O
save	O
in	O
two	O
respects	O
.	O

First	O
,	O
BASS	S-Method
augments	O
the	O
Basic	O
feature	O
set	O
with	O
pairwise	O
combinations	O
of	O
its	O
features	O
.	O

Second	O
,	O
BASS	S-Method
uses	O
a	O
smaller	O
,	O
8	B-Method
-	I-Method
colour	I-Method
encoding	E-Method
to	O
ensure	O
that	O
the	O
number	O
of	O
pairwise	O
combinations	O
remains	O
tractable	O
.	O

paragraph	O
:	O
DISCO	S-Method
.	O

The	O
DISCO	B-Method
method	E-Method
aims	O
to	O
detect	O
objects	O
within	O
the	O
Atari	B-Material
2600	I-Material
screen	E-Material
.	O

To	O
do	O
so	O
,	O
it	O
first	O
preprocesses	O
36	O
,	O
000	O
observations	O
from	O
sample	O
trajectories	O
generated	O
as	O
in	O
the	O
Basic	O
method	O
.	O

DISCO	S-Method
also	O
performs	O
the	O
background	B-Method
subtraction	I-Method
steps	E-Method
as	O
in	O
Basic	O
and	O
BASS	O
.	O

Extracted	O
objects	O
are	O
then	O
labelled	O
into	O
classes	O
.	O

During	O
the	O
actual	O
training	O
,	O
DISCO	S-Method
infers	O
the	O
class	O
label	O
of	O
detected	O
objects	O
and	O
encodes	O
their	O
position	O
and	O
velocity	O
using	O
tile	B-Method
coding	E-Method
.	O

paragraph	O
:	O
LSH	O
.	O

The	O
LSH	B-Method
method	E-Method
maps	O
raw	O
Atari	B-Material
2600	I-Material
screens	E-Material
into	O
a	O
small	O
set	O
of	O
binary	O
features	O
using	O
Locally	B-Method
Sensitive	I-Method
Hashing	E-Method
.	O

The	O
screens	O
are	O
mapped	O
using	O
random	O
projections	O
,	O
such	O
that	O
visually	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
generate	O
the	O
same	O
features	O
.	O

paragraph	O
:	O
RAM	O
.	O

The	O
RAM	B-Method
method	E-Method
works	O
on	O
an	O
entirely	O
different	O
observation	O
space	O
than	O
the	O
other	O
four	O
methods	O
.	O

Rather	O
than	O
receiving	O
in	O
Atari	B-Material
2600	I-Material
screen	E-Material
as	O
an	O
observation	O
,	O
it	O
directly	O
observes	O
the	O
Atari	O
2600	O
’s	O
1024	O
bits	O
of	O
memory	O
.	O

Each	O
bit	O
of	O
RAM	O
is	O
provided	O
as	O
a	O
binary	O
feature	O
together	O
with	O
the	O
pairwise	O
logical	O
-	O
AND	O
of	O
every	O
pair	O
of	O
bits	O
.	O

subsubsection	O
:	O
Evaluation	O
Methodology	O
We	O
first	O
constructed	O
two	O
sets	O
of	O
games	O
,	O
one	O
for	O
training	O
and	O
the	O
other	O
for	O
testing	O
.	O

We	O
used	O
the	O
training	O
games	O
for	O
parameter	B-Task
tuning	E-Task
as	O
well	O
as	O
design	B-Task
refinements	E-Task
,	O
and	O
the	O
testing	O
games	O
for	O
the	O
final	O
evaluation	O
of	O
our	O
methods	O
.	O

Our	O
training	O
set	O
consisted	O
of	O
five	O
games	O
:	O
Asterix	O
,	O
Beam	B-Task
Rider	E-Task
,	O
Freeway	S-Material
,	O
Seaquest	S-Material
and	O
Space	O
Invaders	O
.	O

The	O
parameter	B-Task
search	E-Task
involved	O
finding	O
suitable	O
values	O
for	O
the	O
parameters	O
to	O
the	O
SARSA	B-Method
(	I-Method
)	I-Method
algorithm	E-Method
,	O
i.e.	O
the	O
learning	B-Metric
rate	E-Metric
,	O
exploration	B-Metric
rate	E-Metric
,	O
discount	O
factor	O
,	O
and	O
the	O
decay	B-Metric
rate	E-Metric
.	O

We	O
also	O
searched	O
the	O
space	O
of	O
feature	O
generation	O
parameters	O
,	O
for	O
example	O
the	O
abstraction	O
level	O
for	O
the	O
BASS	B-Method
agent	E-Method
.	O

The	O
results	O
of	O
our	O
parameter	B-Method
search	E-Method
are	O
summarized	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Our	O
testing	O
set	O
was	O
constructed	O
by	O
choosing	O
semi	O
-	O
randomly	O
from	O
the	O
381	O
games	O
listed	O
on	O
Wikipedia	S-Material
at	O
the	O
time	O
of	O
writing	O
.	O

Of	O
these	O
games	O
,	O
123	O
games	O
have	O
their	O
own	O
Wikipedia	B-Material
page	E-Material
,	O
have	O
a	O
single	O
player	O
mode	O
,	O
are	O
not	O
adult	O
-	O
themed	O
or	O
prototypes	O
,	O
and	O
can	O
be	O
emulated	O
in	O
ALE	S-Method
.	O

From	O
this	O
list	O
,	O
50	O
games	O
were	O
chosen	O
at	O
random	O
to	O
form	O
the	O
test	O
set	O
.	O

Evaluation	O
of	O
each	O
method	O
on	O
each	O
game	O
was	O
performed	O
as	O
follows	O
.	O

An	O
episode	O
starts	O
on	O
the	O
frame	O
that	O
follows	O
the	O
reset	O
command	O
,	O
and	O
terminates	O
when	O
the	O
end	O
-	O
of	O
-	O
game	O
condition	O
is	O
detected	O
or	O
after	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
whichever	O
comes	O
first	O
.	O

During	O
an	O
episode	O
,	O
the	O
agent	O
acts	O
every	O
5	O
frames	O
,	O
or	O
equivalently	O
12	O
times	O
per	O
second	O
of	O
gameplay	O
.	O

A	O
reinforcement	B-Method
learning	I-Method
trial	E-Method
consists	O
of	O
5	O
,	O
000	O
training	O
episodes	O
,	O
followed	O
by	O
500	O
evaluation	O
episodes	O
during	O
which	O
no	O
learning	O
takes	O
place	O
.	O

The	O
agent	O
’s	O
performance	O
is	O
measured	O
as	O
the	O
average	B-Metric
score	E-Metric
achieved	O
during	O
the	O
evaluation	O
episodes	O
.	O

For	O
each	O
game	O
,	O
we	O
report	O
our	O
methods	O
’	O
average	O
performance	O
across	O
30	O
trials	O
.	O

For	O
purposes	O
of	O
comparison	O
,	O
we	O
also	O
provide	O
performance	O
measures	O
for	O
three	O
simple	O
baseline	O
agents	O
–	O
Random	O
,	O
Const	O
and	O
Perturb	S-Method
–	O
as	O
well	O
as	O
the	O
performance	O
of	O
a	O
non	O
-	O
expert	B-Method
human	I-Method
player	E-Method
.	O

The	O
Random	B-Method
agent	E-Method
picks	O
a	O
random	O
action	O
on	O
every	O
frame	O
.	O

The	O
Const	B-Method
agent	E-Method
selects	O
a	O
single	O
fixed	O
action	O
throughout	O
an	O
episode	O
;	O
our	O
results	O
reflect	O
the	O
highest	O
score	O
achieved	O
by	O
any	O
single	O
action	O
within	O
each	O
game	O
.	O

The	O
Perturb	B-Method
agent	E-Method
selects	O
a	O
fixed	O
action	O
with	O
probability	O
0.95	O
and	O
otherwise	O
acts	O
uniformly	O
randomly	O
;	O
for	O
each	O
game	O
,	O
we	O
report	O
the	O
performance	O
of	O
the	O
best	O
policy	O
of	O
this	O
type	O
.	O

Additionally	O
,	O
we	O
provide	O
human	O
player	O
results	O
that	O
report	O
the	O
five	O
-	O
episode	B-Metric
average	I-Metric
score	E-Metric
obtained	O
by	O
a	O
beginner	O
(	O
who	O
had	O
never	O
previously	O
played	O
Atari	O
2600	O
games	O
)	O
playing	O
selected	O
games	O
.	O

Our	O
aim	O
is	O
not	O
to	O
provide	O
exhaustive	O
or	O
accurate	O
human	B-Metric
-	I-Metric
level	I-Metric
benchmarks	E-Metric
,	O
which	O
would	O
be	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
,	O
but	O
rather	O
to	O
offer	O
insight	O
into	O
the	O
performance	O
level	O
achieved	O
by	O
our	O
agents	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
reinforcement	B-Method
learning	E-Method
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
a	O
small	O
subset	O
of	O
results	O
from	O
two	O
training	O
games	O
and	O
three	O
test	O
games	O
.	O

In	O
40	O
games	O
out	O
of	O
55	O
,	O
learning	B-Method
agents	E-Method
perform	O
better	O
than	O
the	O
baseline	O
agents	O
.	O

In	O
some	O
games	O
,	O
e.g.	O
,	O
Double	B-Task
Dunk	E-Task
,	O
Journey	B-Task
Escape	E-Task
and	O
Tennis	S-Task
,	O
the	O
no	B-Method
-	I-Method
action	I-Method
baseline	I-Method
policy	E-Method
performs	O
the	O
best	O
by	O
essentially	O
refusing	O
to	O
play	O
and	O
thus	O
incurring	O
no	O
negative	O
reward	O
.	O

Within	O
the	O
40	O
games	O
for	O
which	O
learning	S-Task
occurs	O
,	O
the	O
BASS	B-Method
method	E-Method
generally	O
performs	O
best	O
.	O

DISCO	S-Method
performed	O
particularly	O
poorly	O
compared	O
to	O
the	O
other	O
learning	B-Method
methods	E-Method
.	O

The	O
RAM	B-Method
-	I-Method
based	I-Method
agent	E-Method
,	O
surprisingly	O
,	O
did	O
not	O
outperform	O
image	B-Method
-	I-Method
based	I-Method
methods	E-Method
,	O
despite	O
building	O
its	O
representation	O
from	O
raw	O
game	O
state	O
.	O

It	O
appears	O
the	O
screen	B-Material
image	E-Material
carries	O
structural	O
information	O
that	O
is	O
not	O
easily	O
extracted	O
from	O
the	O
RAM	O
bits	O
.	O

Our	O
reinforcement	B-Method
learning	E-Method
results	O
show	O
that	O
while	O
some	O
learning	S-Task
progress	O
is	O
already	O
possible	O
in	O
Atari	B-Material
2600	I-Material
games	E-Material
,	O
much	O
more	O
work	O
remains	O
to	O
be	O
done	O
.	O

Different	O
methods	O
perform	O
well	O
on	O
different	O
games	O
,	O
and	O
no	O
single	O
method	O
performs	O
well	O
on	O
all	O
games	O
.	O

Some	O
games	O
are	O
particularly	O
challenging	O
.	O

For	O
example	O
,	O
platformers	O
such	O
as	O
Montezuma	B-Material
’s	I-Material
Revenge	E-Material
seem	O
to	O
require	O
high	O
-	O
level	O
planning	O
far	O
beyond	O
what	O
our	O
current	O
,	O
domain	B-Method
-	I-Method
independent	I-Method
methods	E-Method
provide	O
.	O

Tennis	S-Method
requires	O
fairly	O
elaborate	O
behaviour	O
before	O
observing	O
any	O
positive	O
reward	O
,	O
but	O
simple	O
behaviour	O
can	O
avoid	O
negative	O
rewards	O
.	O

Our	O
results	O
also	O
highlight	O
the	O
value	O
of	O
ALE	S-Method
as	O
an	O
experimental	O
methodology	O
.	O

For	O
example	O
,	O
the	O
DISCO	B-Method
approach	E-Method
performs	O
reasonably	O
well	O
on	O
the	O
training	B-Material
set	E-Material
,	O
but	O
suffers	O
a	O
dramatic	O
reduction	O
in	O
performance	O
when	O
applied	O
to	O
unseen	B-Material
games	E-Material
.	O

This	O
suggests	O
the	O
method	O
is	O
less	O
robust	O
than	O
the	O
other	O
methods	O
we	O
studied	O
.	O

After	O
a	O
quick	O
glance	O
at	O
the	O
full	O
table	O
of	O
results	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
it	O
is	O
clear	O
that	O
summarizing	O
results	O
across	O
such	O
varied	O
domains	O
needs	O
further	O
attention	O
;	O
we	O
explore	O
this	O
issue	O
further	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Planning	S-Task
The	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
can	O
naturally	O
be	O
used	O
to	O
study	O
planning	B-Method
techniques	E-Method
by	O
using	O
the	O
emulator	B-Method
itself	E-Method
as	O
a	O
generative	B-Method
model	E-Method
.	O

Initially	O
it	O
may	O
seem	O
that	O
allowing	O
the	O
agent	O
to	O
plan	O
into	O
the	O
future	O
with	O
a	O
perfect	O
model	O
trivializes	O
the	O
problem	O
.	O

However	O
,	O
this	O
is	O
not	O
the	O
case	O
:	O
the	O
size	O
of	O
state	O
space	O
in	O
Atari	B-Material
2600	I-Material
games	E-Material
prohibits	O
exhaustive	B-Method
search	E-Method
.	O

Eighteen	O
different	O
actions	O
are	O
available	O
at	O
every	O
frame	O
;	O
at	O
60	O
frames	O
per	O
second	O
,	O
looking	O
ahead	O
one	O
second	O
requires	O
simulation	O
steps	O
.	O

Furthermore	O
,	O
rewards	O
are	O
often	O
sparsely	O
distributed	O
,	O
which	O
causes	O
significant	O
horizon	O
effects	O
in	O
many	O
search	B-Method
algorithms	E-Method
.	O

subsubsection	S-Method
:	O
Search	B-Method
Methods	E-Method
We	O
now	O
provide	O
benchmark	O
ALE	S-Method
results	O
for	O
two	O
traditional	O
search	B-Method
methods	E-Method
.	O

Each	O
method	O
was	O
applied	O
online	O
to	O
select	O
an	O
action	O
at	O
every	O
time	O
step	O
(	O
every	O
five	O
frames	O
)	O
until	O
the	O
game	O
was	O
over	O
.	O

paragraph	O
:	O
Breadth	B-Method
-	I-Method
first	I-Method
Search	E-Method
.	O

Our	O
first	O
approach	O
builds	O
a	O
search	O
tree	O
in	O
a	O
breadth	O
-	O
first	O
fashion	O
until	O
a	O
node	O
limit	O
is	O
reached	O
.	O

Once	O
the	O
tree	O
is	O
expanded	O
,	O
node	O
values	O
are	O
updated	O
recursively	O
from	O
the	O
bottom	O
of	O
the	O
tree	O
to	O
the	O
root	O
.	O

The	O
agent	O
then	O
selects	O
the	O
action	O
corresponding	O
to	O
the	O
branch	O
with	O
the	O
highest	O
discounted	O
sum	O
of	O
rewards	O
.	O

Expanding	O
the	O
full	O
search	O
tree	O
requires	O
a	O
large	O
number	O
of	O
simulation	O
steps	O
.	O

For	O
instance	O
,	O
selecting	O
an	O
action	O
every	O
5	O
frames	O
and	O
allowing	O
a	O
maximum	O
of	O
100	O
,	O
000	O
simulation	O
steps	O
per	O
frame	O
,	O
the	O
agent	O
can	O
only	O
look	O
ahead	O
about	O
a	O
third	O
of	O
a	O
second	O
.	O

In	O
many	O
games	O
,	O
this	O
allows	O
the	O
agent	O
to	O
collect	O
immediate	O
rewards	O
and	O
avoid	O
death	O
but	O
little	O
else	O
.	O

For	O
example	O
,	O
in	O
Seaquest	O
the	O
agent	O
must	O
collect	O
a	O
swimmer	O
and	O
return	O
to	O
the	O
surface	O
before	O
running	O
out	O
of	O
air	O
,	O
which	O
involves	O
planning	O
far	O
beyond	O
one	O
second	O
.	O

paragraph	O
:	O
UCT	S-Method
:	O
Upper	B-Method
Confidence	I-Method
Bounds	E-Method
Applied	O
to	O
Trees	O
.	O

A	O
preferable	O
alternative	O
to	O
exhaustively	O
expanding	O
the	O
tree	O
is	O
to	O
simulate	O
deeper	O
into	O
the	O
more	O
promising	O
branches	O
.	O

To	O
do	O
this	O
,	O
we	O
need	O
to	O
find	O
a	O
balance	O
between	O
expanding	O
the	O
higher	O
-	O
valued	O
branches	O
and	O
spending	O
simulation	O
steps	O
on	O
the	O
lower	O
-	O
valued	O
branches	O
to	O
get	O
a	O
better	O
estimate	O
of	O
their	O
values	O
.	O

The	O
UCT	B-Method
algorithm	E-Method
,	O
developed	O
by	O
kocsis_06	S-Method
,	O
deals	O
with	O
the	O
exploration	B-Task
-	I-Task
exploitation	I-Task
dilemma	E-Task
by	O
treating	O
each	O
node	O
of	O
a	O
search	O
tree	O
as	O
a	O
multi	B-Task
-	I-Task
armed	I-Task
bandit	I-Task
problem	E-Task
.	O

UCT	S-Method
uses	O
a	O
variation	O
of	O
UCB1	S-Method
,	O
a	O
bandit	B-Method
algorithm	E-Method
,	O
to	O
choose	O
which	O
child	O
node	O
to	O
visit	O
next	O
.	O

A	O
common	O
practice	O
is	O
to	O
apply	O
a	O
-	O
step	O
random	B-Method
simulation	E-Method
at	O
the	O
end	O
of	O
each	O
leaf	O
node	O
to	O
obtain	O
an	O
estimate	O
from	O
a	O
longer	O
trajectory	O
.	O

By	O
expanding	O
the	O
more	O
valuable	O
branches	O
of	O
the	O
tree	O
and	O
carrying	O
out	O
a	O
random	O
simulation	O
at	O
the	O
leaf	O
nodes	O
,	O
UCT	S-Method
is	O
known	O
to	O
perform	O
well	O
in	O
many	O
different	O
settings	O
mcts_survery2012	S-Method
.	O

Our	O
UCT	B-Method
implementation	E-Method
was	O
entirely	O
standard	O
,	O
except	O
for	O
one	O
optimization	S-Task
.	O

Few	O
Atari	B-Method
games	E-Method
actually	O
distinguish	O
between	O
all	O
18	O
actions	O
at	O
every	O
time	O
step	O
.	O

In	O
Beam	B-Task
Rider	E-Task
,	O
for	O
example	O
,	O
the	O
down	O
action	O
does	O
nothing	O
,	O
and	O
pressing	O
the	O
button	O
when	O
a	O
bullet	O
has	O
already	O
been	O
shot	O
has	O
no	O
effect	O
.	O

We	O
exploit	O
this	O
fact	O
as	O
follows	O
:	O
after	O
expanding	O
the	O
children	O
of	O
a	O
node	O
in	O
the	O
search	O
tree	O
,	O
we	O
compare	O
the	O
resulting	O
emulator	O
states	O
.	O

Actions	O
that	O
result	O
in	O
the	O
same	O
state	O
are	O
treated	O
as	O
duplicates	O
and	O
only	O
one	O
of	O
the	O
actions	O
is	O
considered	O
in	O
the	O
search	O
tree	O
.	O

This	O
reduces	O
the	O
branching	O
factor	O
,	O
thus	O
allowing	O
deeper	B-Task
search	E-Task
.	O

At	O
every	O
step	O
,	O
we	O
also	O
reuse	O
the	O
part	O
of	O
our	O
search	O
tree	O
corresponding	O
to	O
the	O
selected	O
action	O
.	O

Pseudocode	O
for	O
our	O
implementation	O
of	O
the	O
UCT	B-Method
algorithm	E-Method
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Experimental	O
Setup	O
We	O
designed	O
and	O
tuned	O
our	O
algorithms	O
based	O
on	O
the	O
same	O
five	O
training	O
games	O
used	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
subsequently	O
evaluated	O
the	O
methods	O
on	O
the	O
fifty	O
games	O
of	O
the	O
testing	O
set	O
.	O

The	O
training	B-Material
games	E-Material
were	O
used	O
to	O
determine	O
the	O
length	O
of	O
the	O
search	O
horizon	O
as	O
well	O
as	O
the	O
constant	O
controlling	O
the	O
amount	O
of	O
exploration	O
at	O
internal	O
nodes	O
of	O
the	O
tree	O
.	O

Each	O
episode	O
was	O
set	O
to	O
last	O
up	O
to	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
with	O
actions	O
selected	O
every	O
5	O
frames	O
,	O
matching	O
our	O
settings	O
in	O
Section	O
[	O
reference	O
]	O
.	O

On	O
average	O
,	O
each	O
action	B-Method
selection	E-Method
step	O
took	O
on	O
the	O
order	O
of	O
15	O
seconds	O
.	O

We	O
also	O
used	O
the	O
same	O
discount	O
factor	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
ran	O
our	O
algorithms	O
for	O
10	O
episodes	O
per	O
game	O
.	O

Details	O
of	O
the	O
algorithmic	O
parameters	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
search	O
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
results	O
on	O
a	O
selected	O
subset	O
of	O
games	O
.	O

For	O
reference	O
purposes	O
,	O
we	O
also	O
include	O
the	O
performance	O
of	O
the	O
best	O
learning	B-Method
agent	E-Method
and	O
the	O
best	O
baseline	O
policy	O
from	O
Table	O
[	O
reference	O
]	O
.	O

Together	O
,	O
our	O
two	O
search	B-Method
methods	E-Method
performed	O
better	O
than	O
both	O
learning	B-Method
agents	E-Method
and	O
the	O
baseline	B-Method
policies	E-Method
on	O
49	O
of	O
55	O
games	O
.	O

In	O
most	O
cases	O
,	O
UCT	S-Method
performs	O
significantly	O
better	O
than	O
breadth	B-Method
-	I-Method
first	I-Method
search	E-Method
.	O

Four	O
of	O
the	O
six	O
games	O
for	O
which	O
search	B-Method
methods	E-Method
do	O
not	O
perform	O
best	O
are	O
games	O
where	O
rewards	O
are	O
sparse	O
and	O
require	O
long	O
-	O
term	O
planning	O
.	O

These	O
are	O
Freeway	O
,	O
Private	O
Eye	O
,	O
Montezuma	O
’s	O
Revenge	O
and	O
Venture	O
.	O

section	O
:	O
Evaluation	B-Metric
Metrics	E-Metric
for	O
General	O
Atari	B-Task
2600	I-Task
Agents	E-Task
Applying	O
algorithms	O
to	O
a	O
large	O
set	O
of	O
games	O
as	O
we	O
did	O
in	O
Sections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
presents	O
difficulties	O
when	O
interpreting	O
the	O
results	O
.	O

While	O
the	O
agent	O
’s	O
goal	O
in	O
all	O
games	O
is	O
to	O
maximize	O
its	O
score	O
,	O
scores	O
for	O
two	O
different	O
games	O
can	O
not	O
be	O
easily	O
compared	O
.	O

Each	O
game	O
uses	O
its	O
own	O
scale	O
for	O
scores	O
,	O
and	O
different	O
game	O
mechanics	O
make	O
some	O
games	O
harder	O
to	O
learn	O
than	O
others	O
.	O

The	O
challenges	O
associated	O
with	O
comparing	O
general	B-Method
agents	E-Method
has	O
been	O
previously	O
highlighted	O
by	O
whiteson11	O
.	O

Although	O
we	O
can	O
always	O
report	O
full	O
performance	O
tables	O
,	O
as	O
we	O
did	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
some	O
more	O
compact	O
summary	B-Metric
statistics	E-Metric
are	O
also	O
desirable	O
.	O

We	O
now	O
introduce	O
some	O
simple	O
metrics	O
that	O
help	O
compare	O
agents	O
across	O
a	O
diverse	O
set	O
of	O
domains	O
,	O
such	O
as	O
our	O
test	O
set	O
of	O
Atari	B-Material
2600	I-Material
games	E-Material
.	O

subsection	O
:	O
Normalized	B-Metric
Scores	E-Metric
Consider	O
the	O
scores	O
and	O
achieved	O
by	O
two	O
algorithms	O
in	O
game	O
.	O

Our	O
goal	O
here	O
is	O
to	O
explore	O
methods	O
that	O
allow	O
us	O
to	O
compare	O
two	O
sets	O
of	O
scores	O
and	O
.	O

The	O
approach	O
we	O
take	O
is	O
to	O
transform	O
into	O
a	O
normalized	O
score	O
with	O
the	O
aim	O
of	O
comparing	O
normalized	O
scores	O
across	O
games	O
;	O
in	O
the	O
ideal	O
case	O
,	O
implies	O
that	O
algorithm	O
performs	O
as	O
well	O
on	O
game	O
as	O
on	O
game	O
.	O

In	O
order	O
to	O
compare	O
algorithms	O
over	O
a	O
set	O
of	O
games	O
,	O
we	O
aggregate	O
normalized	O
scores	O
for	O
each	O
game	O
and	O
each	O
algorithm	O
.	O

The	O
most	O
natural	O
way	O
to	O
compare	O
games	O
with	O
different	O
scoring	O
scales	O
is	O
to	O
normalize	O
scores	O
so	O
that	O
the	O
numerical	O
values	O
become	O
comparable	O
.	O

All	O
of	O
our	O
normalization	B-Method
methods	E-Method
are	O
defined	O
using	O
the	O
notion	O
of	O
a	O
score	O
range	O
computed	O
for	O
each	O
game	O
.	O

Given	O
such	O
a	O
score	O
range	O
,	O
score	O
is	O
normalized	O
by	O
computing	O
.	O

subsubsection	S-Method
:	O
Normalization	S-Method
to	O
a	O
Reference	O
Score	O
One	O
straightforward	O
method	O
is	O
to	O
normalize	O
to	O
a	O
score	O
range	O
defined	O
by	O
repeated	O
runs	O
of	O
a	O
random	O
agent	O
across	O
each	O
game	O
.	O

Here	O
,	O
is	O
the	O
absolute	O
value	O
of	O
the	O
average	B-Metric
score	E-Metric
achieved	O
by	O
the	O
random	B-Method
agent	E-Method
,	O
and	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
depicts	O
the	O
random	B-Metric
-	I-Metric
normalized	I-Metric
scores	E-Metric
achieved	O
by	O
BASS	S-Method
and	O
RAM	S-Method
on	O
three	O
games	O
.	O

Two	O
issues	O
arise	O
with	O
this	O
approach	O
:	O
the	O
scale	O
of	O
normalized	O
scores	O
may	O
be	O
excessively	O
large	O
and	O
normalized	O
scores	O
are	O
generally	O
not	O
translation	O
invariant	O
.	O

The	O
issue	O
of	O
scale	O
is	O
best	O
seen	O
in	O
a	O
game	O
such	O
as	O
Freeway	S-Material
,	O
for	O
which	O
the	O
random	B-Method
agent	E-Method
achieves	O
a	O
score	O
close	O
to	O
0	O
:	O
scores	O
achieved	O
by	O
learning	B-Method
agents	E-Method
,	O
in	O
the	O
10	O
-	O
20	O
range	O
,	O
are	O
normalized	O
into	O
thousands	O
.	O

By	O
contrast	O
,	O
no	O
learning	B-Method
agent	E-Method
achieves	O
a	O
random	B-Metric
-	I-Metric
normalized	I-Metric
score	E-Metric
greater	O
than	O
1	O
in	O
Asteroids	O
.	O

subsubsection	S-Method
:	O
Normalizing	O
to	O
a	O
Baseline	O
Set	O
Rather	O
than	O
normalizing	O
to	O
a	O
single	O
reference	O
we	O
may	O
normalize	O
to	O
the	O
score	O
range	O
implied	O
by	O
a	O
set	O
of	O
references	O
.	O

Let	O
be	O
a	O
set	O
of	O
reference	O
scores	O
.	O

A	O
method	O
’s	O
baseline	B-Metric
score	E-Metric
is	O
computed	O
using	O
the	O
score	O
range	O
.	O

Given	O
a	O
sufficiently	O
rich	O
set	O
of	O
reference	O
scores	O
,	O
baseline	B-Method
normalization	E-Method
allows	O
us	O
to	O
reduce	O
the	O
scores	O
for	O
most	O
games	O
to	O
comparable	O
quantities	O
,	O
and	O
lets	O
us	O
know	O
whether	O
meaningful	O
performance	O
was	O
obtained	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
example	O
baseline	O
scores	O
.	O

The	O
score	O
range	O
for	O
these	O
scores	O
corresponds	O
to	O
the	O
scores	O
achieved	O
by	O
37	O
baseline	O
agents	O
(	O
Section	O
[	O
reference	O
]	O
)	O
:	O
Random	O
,	O
Const	O
(	O
one	O
policy	O
per	O
action	O
)	O
,	O
and	O
Perturb	O
(	O
one	O
policy	O
per	O
action	O
)	O
.	O

A	O
natural	O
idea	O
is	O
to	O
also	O
include	O
scores	O
achieved	O
by	O
human	O
players	O
into	O
the	O
baseline	O
set	O
.	O

For	O
example	O
,	O
one	O
may	O
include	O
the	O
score	O
achieved	O
by	O
an	O
expert	O
as	O
well	O
as	O
the	O
score	O
achieved	O
by	O
a	O
beginner	O
.	O

However	O
,	O
using	O
human	O
scores	O
raises	O
its	O
own	O
set	O
of	O
issues	O
.	O

For	O
example	O
,	O
humans	O
often	O
play	O
games	O
without	O
seeking	O
to	O
maximize	O
score	O
;	O
humans	O
also	O
benefit	O
from	O
prior	O
knowledge	O
that	O
is	O
difficult	O
to	O
incorporate	O
into	O
domain	B-Method
-	I-Method
independent	I-Method
agents	E-Method
.	O

subsubsection	S-Method
:	O
Inter	B-Method
-	I-Method
Algorithm	I-Method
Normalization	E-Method
A	O
third	O
alternative	O
is	O
to	O
normalize	O
using	O
the	O
scores	O
achieved	O
by	O
the	O
algorithms	O
themselves	O
.	O

Given	O
algorithms	O
,	O
each	O
achieving	O
score	O
on	O
game	O
,	O
we	O
define	O
the	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
score	E-Metric
using	O
the	O
score	O
range	O
.	O

By	O
definition	O
,	O
.	O

A	O
special	O
case	O
of	O
this	O
is	O
when	O
n=2	O
,	O
where	O
indicates	O
which	O
algorithm	O
is	O
better	O
than	O
the	O
other	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
shows	O
example	O
inter	O
-	O
algorithm	O
scores	O
;	O
the	O
relevant	O
score	O
ranges	O
are	O
constructed	O
from	O
the	O
performance	O
of	O
all	O
five	O
learning	B-Method
agents	E-Method
.	O

Because	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
scores	E-Metric
are	O
bounded	O
,	O
this	O
type	O
of	O
normalization	S-Method
is	O
an	O
appealing	O
solution	O
to	O
compare	O
the	O
relative	O
performance	O
of	O
different	O
methods	O
.	O

Its	O
main	O
drawback	O
is	O
that	O
it	O
gives	O
no	O
indication	O
of	O
the	O
objective	O
performance	O
of	O
the	O
best	O
algorithm	O
.	O

A	O
good	O
example	O
of	O
this	O
is	O
Venture	O
:	O
the	O
inter	O
-	O
algorithm	B-Metric
score	E-Metric
of	O
1.0	O
achieved	O
by	O
BASS	S-Method
does	O
not	O
reflect	O
the	O
fact	O
that	O
none	O
of	O
our	O
agents	O
achieved	O
a	O
score	O
remotely	O
comparable	O
to	O
a	O
human	O
’s	O
performance	O
.	O

The	O
lack	O
of	O
objective	O
reference	O
in	O
inter	O
-	O
algorithm	B-Method
normalization	E-Method
suggests	O
that	O
it	O
should	O
be	O
used	O
to	O
complement	O
other	O
scoring	B-Metric
metrics	E-Metric
.	O

subsection	O
:	O
Aggregating	O
Scores	O
Once	O
normalized	O
scores	O
are	O
obtained	O
for	O
each	O
game	O
,	O
the	O
next	O
step	O
is	O
to	O
produce	O
a	O
measure	O
that	O
reflects	O
how	O
well	O
each	O
agent	O
performs	O
across	O
the	O
set	O
of	O
games	O
.	O

As	O
illustrated	O
by	O
Table	O
[	O
reference	O
]	O
,	O
a	O
large	O
table	O
of	O
numbers	O
does	O
not	O
easily	O
permit	O
comparison	O
between	O
algorithms	O
.	O

We	O
now	O
describe	O
three	O
methods	O
to	O
aggregate	O
normalized	O
scores	O
.	O

subsubsection	S-Method
:	O
Average	B-Metric
Score	E-Metric
The	O
most	O
straightforward	O
method	O
of	O
aggregating	O
normalized	O
scores	O
is	O
to	O
compute	O
their	O
average	O
.	O

Without	O
perfect	B-Method
score	I-Method
normalization	E-Method
,	O
however	O
,	O
score	B-Metric
averages	E-Metric
tend	O
to	O
be	O
heavily	O
influenced	O
by	O
games	O
such	O
as	O
Zaxxon	O
for	O
which	O
baseline	O
scores	O
are	O
high	O
.	O

Averaging	B-Metric
inter	I-Metric
-	I-Metric
algorithm	I-Metric
scores	E-Metric
obviates	O
this	O
issue	O
as	O
all	O
scores	O
are	O
bounded	O
between	O
0	O
and	O
1	O
.	O

Figure	O
[	O
reference	O
]	O
displays	O
average	O
baseline	O
and	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
scores	E-Metric
for	O
our	O
learning	B-Method
agents	E-Method
.	O

subsubsection	O
:	O
Median	B-Method
Score	E-Method
Median	B-Metric
scores	E-Metric
are	O
generally	O
more	O
robust	O
to	O
outliers	O
than	O
average	O
scores	O
.	O

The	O
median	O
is	O
obtained	O
by	O
sorting	O
all	O
normalized	O
scores	O
and	O
selecting	O
the	O
middle	O
element	O
(	O
the	O
average	O
of	O
the	O
two	O
middle	O
elements	O
is	O
used	O
if	O
the	O
number	O
of	O
scores	O
is	O
even	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
median	O
baseline	O
and	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
scores	E-Metric
for	O
our	O
learning	B-Method
agents	E-Method
.	O

Comparing	O
medians	S-Metric
and	O
averages	O
in	O
the	O
baseline	B-Metric
score	E-Metric
(	O
upper	O
two	O
graphs	O
)	O
illustrates	O
exactly	O
the	O
outlier	B-Metric
sensitivity	E-Metric
of	O
the	O
average	B-Metric
score	E-Metric
,	O
where	O
the	O
LSH	B-Method
method	E-Method
appears	O
dramatically	O
superior	O
due	O
entirely	O
to	O
its	O
performance	O
in	O
Zaxxon	S-Task
.	O

subsubsection	O
:	O
Score	B-Method
Distribution	E-Method
The	O
score	B-Method
distribution	I-Method
aggregate	E-Method
is	O
a	O
natural	O
generalization	O
of	O
the	O
median	B-Metric
score	E-Metric
:	O
it	O
shows	O
the	O
fraction	O
of	O
games	O
on	O
which	O
an	O
algorithm	O
achieves	O
a	O
certain	O
normalized	O
score	O
or	O
better	O
.	O

It	O
is	O
essentially	O
a	O
quantile	B-Method
plot	E-Method
or	O
inverse	B-Method
empirical	I-Method
CDF	E-Method
.	O

Unlike	O
the	O
average	B-Metric
and	I-Metric
median	I-Metric
scores	E-Metric
,	O
the	O
score	B-Metric
distribution	E-Metric
accurately	O
represents	O
the	O
performance	O
of	O
an	O
agent	O
irrespective	O
of	O
how	O
individual	O
scores	O
are	O
distributed	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
baseline	O
and	O
inter	O
-	O
algorithm	B-Metric
score	I-Metric
distributions	E-Metric
.	O

Score	O
distributions	O
allow	O
us	O
to	O
compare	O
different	O
algorithms	O
at	O
a	O
glance	O
–	O
if	O
one	O
curve	O
is	O
above	O
another	O
,	O
the	O
corresponding	O
method	O
generally	O
obtains	O
higher	O
scores	O
.	O

Using	O
the	O
baseline	B-Metric
score	I-Metric
distribution	E-Metric
,	O
we	O
can	O
easily	O
determine	O
the	O
proportion	O
of	O
games	O
for	O
which	O
methods	O
perform	O
better	O
than	O
the	O
baseline	O
policies	O
(	O
scores	O
above	O
1	O
)	O
.	O

The	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
score	I-Metric
distribution	E-Metric
,	O
on	O
the	O
other	O
hand	O
,	O
effectively	O
conveys	O
the	O
relative	O
performance	O
of	O
each	O
method	O
.	O

In	O
particular	O
,	O
it	O
allows	O
us	O
to	O
conclude	O
that	O
BASS	S-Method
performs	O
slightly	O
better	O
than	O
Basic	S-Method
and	O
RAM	S-Method
,	O
and	O
that	O
DISCO	S-Method
performs	O
significantly	O
worse	O
than	O
the	O
other	O
methods	O
.	O

subsection	O
:	O
Paired	O
Tests	O
An	O
alternate	O
evaluation	B-Metric
metric	E-Metric
,	O
especially	O
useful	O
when	O
comparing	O
only	O
a	O
few	O
algorithms	O
,	O
is	O
to	O
perform	O
paired	O
tests	O
over	O
the	O
raw	O
scores	O
.	O

For	O
each	O
game	O
,	O
we	O
performed	O
a	O
two	O
-	O
tailed	O
Welsh	O
’s	O
-	O
test	O
with	O
99	O
%	O
confidence	O
intervals	O
to	O
determine	O
whether	O
one	O
algorithm	O
’s	O
score	O
was	O
statistically	O
different	O
than	O
the	O
other	O
’s	O
.	O

Table	O
[	O
reference	O
]	O
provides	O
,	O
for	O
each	O
pair	O
of	O
algorithms	O
,	O
the	O
number	O
of	O
games	O
for	O
which	O
one	O
algorithm	O
performs	O
statistically	O
better	O
or	O
worse	O
than	O
the	O
other	O
.	O

Because	O
of	O
their	O
ternary	O
nature	O
,	O
paired	B-Method
tests	E-Method
tend	O
to	O
magnify	O
small	O
but	O
significant	O
differences	O
in	O
scores	O
.	O

section	O
:	O
Related	O
Work	O
We	O
now	O
briefly	O
survey	O
recent	O
research	O
related	O
to	O
Atari	B-Material
2600	I-Material
games	E-Material
and	O
some	O
prior	O
work	O
on	O
the	O
construction	O
of	O
empirical	B-Metric
benchmarks	E-Metric
for	O
measuring	B-Task
general	I-Task
competency	E-Task
.	O

subsection	O
:	O
Atari	B-Task
Games	E-Task
There	O
has	O
been	O
some	O
attention	O
devoted	O
to	O
Atari	B-Task
2600	I-Task
game	I-Task
playing	E-Task
within	O
the	O
reinforcement	B-Task
learning	I-Task
community	E-Task
.	O

For	O
the	O
most	O
part	O
,	O
prior	O
work	O
has	O
focused	O
on	O
the	O
challenge	O
of	O
finding	O
good	O
state	O
features	O
for	O
this	O
domain	O
.	O

diuk2008	O
applied	O
their	O
DOORMAX	B-Method
algorithm	E-Method
to	O
a	O
restricted	O
version	O
of	O
the	O
game	B-Task
of	I-Task
Pitfall	I-Task
!	E-Task
.	O

Their	O
method	O
extracts	O
objects	O
from	O
the	O
displayed	B-Material
image	E-Material
with	O
game	B-Task
-	I-Task
specific	I-Task
object	I-Task
detection	E-Task
.	O

These	O
objects	O
are	O
then	O
converted	O
into	O
a	O
first	B-Method
-	I-Method
order	I-Method
logic	I-Method
representation	I-Method
of	I-Method
the	I-Method
world	E-Method
,	O
the	O
Object	B-Method
-	I-Method
Oriented	I-Method
Markov	I-Method
Decision	I-Method
Process	E-Method
(	O
OO	B-Method
-	I-Method
MDP	E-Method
)	O
.	O

Their	O
results	O
show	O
that	O
DOORMAX	S-Method
can	O
discover	O
the	O
optimal	O
behaviour	O
for	O
this	O
OO	B-Task
-	I-Task
MDP	E-Task
within	O
one	O
episode	O
.	O

wintermute2010	O
proposed	O
a	O
method	O
that	O
also	O
extracts	O
objects	O
from	O
the	O
displayed	B-Material
image	E-Material
and	O
embeds	O
them	O
into	O
a	O
logic	B-Method
-	I-Method
based	I-Method
architecture	E-Method
,	O
SOAR	S-Method
.	O

Their	O
method	O
uses	O
a	O
forward	B-Method
model	E-Method
of	O
the	O
scene	O
to	O
improve	O
the	O
performance	O
of	O
the	O
Q	B-Method
-	I-Method
Learning	I-Method
algorithm	E-Method
.	O

They	O
showed	O
that	O
by	O
using	O
such	O
a	O
model	O
,	O
a	O
reinforcement	B-Method
learning	I-Method
agent	E-Method
could	O
learn	O
to	O
play	O
a	O
restricted	O
version	O
of	O
the	O
game	O
of	O
Frogger	S-Material
.	O

cobo2011	S-Method
investigated	O
automatic	B-Task
feature	I-Task
discovery	E-Task
in	O
the	O
games	O
of	O
Pong	S-Task
and	O
Frogger	S-Task
,	O
using	O
their	O
own	O
simulator	O
.	O

Their	O
proposed	O
method	O
takes	O
advantage	O
of	O
human	O
trajectories	O
to	O
identify	O
state	O
features	O
that	O
are	O
important	O
for	O
playing	B-Task
console	I-Task
games	E-Task
.	O

Recently	O
,	O
hausknecht_12	O
proposed	O
HyperNEAT	B-Method
-	I-Method
GGP	E-Method
,	O
an	O
evolutionary	B-Method
approach	E-Method
for	O
finding	B-Task
policies	E-Task
to	O
play	O
Atari	B-Task
2600	I-Task
games	E-Task
.	O

Although	O
HyperNEAT	B-Method
-	I-Method
GGP	E-Method
is	O
presented	O
as	O
a	O
general	O
game	B-Method
playing	I-Method
approach	E-Method
,	O
it	O
is	O
currently	O
difficult	O
to	O
assess	O
its	O
general	O
performance	O
as	O
the	O
reported	O
results	O
were	O
limited	O
to	O
only	O
two	O
games	O
.	O

Finally	O
,	O
some	O
of	O
the	O
authors	O
of	O
this	O
paper	O
recently	O
presented	O
a	O
domain	B-Method
-	I-Method
independent	I-Method
feature	I-Method
generation	I-Method
technique	E-Method
that	O
attempts	O
to	O
focus	O
its	O
effort	O
around	O
the	O
location	O
of	O
the	O
player	O
avatar	O
.	O

This	O
work	O
used	O
the	O
evaluation	O
methodology	O
advocated	O
here	O
and	O
is	O
the	O
only	O
one	O
to	O
demonstrate	O
the	O
technique	O
across	O
a	O
large	O
set	O
of	O
testing	O
games	O
.	O

subsection	O
:	O
Evaluation	B-Method
Frameworks	E-Method
for	O
General	B-Task
Agents	E-Task
Although	O
the	O
idea	O
of	O
using	O
games	S-Method
to	O
evaluate	O
the	O
performance	B-Task
of	I-Task
agents	E-Task
has	O
a	O
long	O
history	O
in	O
artificial	B-Task
intelligence	E-Task
,	O
it	O
is	O
only	O
more	O
recently	O
that	O
an	O
emphasis	O
on	O
generality	O
has	O
assumed	O
a	O
more	O
prominent	O
role	O
.	O

pell93strategy	S-Method
advocated	O
the	O
design	O
of	O
agents	O
that	O
,	O
given	O
an	O
abstract	O
description	O
of	O
a	O
game	O
,	O
could	O
automatically	O
play	O
them	O
.	O

His	O
work	O
strongly	O
influenced	O
the	O
design	O
of	O
the	O
now	O
annual	O
General	B-Task
Game	I-Task
Playing	I-Task
competition	E-Task
.	O

Our	O
framework	O
differs	O
in	O
that	O
we	O
do	O
not	O
assume	O
to	O
have	O
access	O
to	O
a	O
compact	O
logical	O
description	O
of	O
the	O
game	O
semantics	O
.	O

schaul11	O
also	O
recently	O
presented	O
an	O
interesting	O
proposal	O
for	O
using	O
games	S-Method
to	O
measure	O
the	O
general	O
capabilities	O
of	O
an	O
agent	O
.	O

whiteson11	O
discuss	O
a	O
number	O
of	O
challenges	O
in	O
designing	O
empirical	B-Task
tests	E-Task
to	O
measure	O
general	B-Task
reinforcement	I-Task
learning	E-Task
performance	O
;	O
this	O
work	O
can	O
be	O
seen	O
as	O
attempting	O
to	O
address	O
their	O
important	O
concerns	O
.	O

Starting	O
in	O
2004	O
as	O
a	O
conference	O
workshop	O
,	O
the	O
Reinforcement	B-Task
Learning	I-Task
competition	E-Task
was	O
held	O
until	O
2009	O
(	O
a	O
new	O
iteration	O
of	O
the	O
competition	O
has	O
been	O
announced	O
for	O
2013	O
)	O
.	O

Each	O
year	O
new	O
domains	O
are	O
proposed	O
,	O
including	O
standard	O
RL	B-Method
benchmarks	E-Method
,	O
Tetris	S-Method
,	O
and	O
Infinite	B-Material
Mario	E-Material
.	O

In	O
a	O
typical	O
competition	B-Material
domain	E-Material
,	O
the	O
agent	O
’s	O
state	O
information	O
is	O
summarized	O
through	O
a	O
series	O
of	O
high	O
-	O
level	O
state	O
variables	O
rather	O
than	O
direct	O
sensory	O
information	O
.	O

Infinite	B-Task
Mario	E-Task
,	O
for	O
example	O
,	O
provides	O
the	O
agent	O
with	O
an	O
object	O
-	O
oriented	O
observation	O
space	O
.	O

In	O
the	O
past	O
,	O
organizers	O
have	O
provided	O
a	O
special	O
‘	O
Polyathlon	O
’	O
track	O
in	O
which	O
agents	O
must	O
behave	O
in	O
a	O
medley	O
of	O
continuous	O
-	O
observation	O
,	O
discrete	O
-	O
action	O
domains	O
.	O

Another	O
longstanding	O
competition	O
,	O
the	O
International	B-Task
Planning	I-Task
Competition	E-Task
(	O
IPC	S-Task
)	O
,	O
has	O
been	O
organized	O
since	O
1998	O
,	O
and	O
aims	O
to	O
“	O
produce	O
new	O
benchmarks	O
,	O
and	O
to	O
gather	O
and	O
disseminate	O
data	O
about	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
”	O
.	O

The	O
IPC	S-Method
is	O
composed	O
of	O
different	O
tracks	O
corresponding	O
to	O
different	O
types	O
of	O
planning	B-Task
problems	E-Task
,	O
including	O
factory	B-Task
optimization	E-Task
,	O
elevator	B-Task
control	E-Task
and	O
agent	B-Task
coordination	E-Task
.	O

For	O
example	O
,	O
one	O
of	O
the	O
problems	O
in	O
the	O
2011	O
competition	O
consists	O
in	O
coordinating	O
a	O
set	O
of	O
robots	O
around	O
a	O
two	O
-	O
dimensional	O
gridworld	O
so	O
that	O
every	O
tile	O
is	O
painted	O
with	O
a	O
specific	O
colour	O
.	O

Domains	S-Material
are	O
described	O
using	O
either	O
relational	B-Method
reinforcement	I-Method
learning	E-Method
,	O
yielding	O
parametrized	B-Method
Markov	I-Method
Decision	I-Method
Processes	E-Method
(	O
MDPs	S-Method
)	O
and	O
Partially	B-Method
Observable	I-Method
MDPs	E-Method
,	O
or	O
using	O
logic	O
predicates	O
,	O
e.g.	O
in	O
STRIPS	O
notation	O
.	O

One	O
indication	O
of	O
how	O
much	O
these	O
competitions	O
value	O
domain	O
variety	O
can	O
be	O
seen	O
in	O
the	O
time	O
spent	O
on	O
finding	O
a	O
good	O
specification	B-Method
language	E-Method
.	O

The	O
2008	O
-	O
2009	O
RL	B-Material
competitions	E-Material
,	O
for	O
example	O
,	O
used	O
RL	B-Method
-	I-Method
Glue	E-Method
specifically	O
for	O
this	O
purpose	O
;	O
the	O
2011	O
planning	O
under	O
uncertainty	O
track	O
of	O
the	O
IPC	O
similar	O
employed	O
the	O
Relation	B-Method
Dynamic	I-Method
Influence	I-Method
Diagram	I-Method
Language	E-Method
.	O

While	O
competitions	O
seek	O
to	O
spur	O
new	O
research	O
and	O
evaluate	O
existing	O
algorithms	O
through	O
a	O
standardized	O
set	O
of	O
benchmarks	O
,	O
they	O
are	O
not	O
independently	O
developed	O
,	O
in	O
the	O
sense	O
that	O
the	O
vast	O
majority	O
of	O
domains	O
are	O
provided	O
by	O
the	O
research	O
community	O
.	O

Thus	O
a	O
typical	O
competition	O
domain	O
reflects	O
existing	O
research	O
directions	O
:	O
Mountain	B-Material
Car	E-Material
and	O
Acrobot	S-Material
remain	O
staples	O
of	O
the	O
RL	B-Task
competition	E-Task
.	O

These	O
competitions	O
also	O
focus	O
their	O
research	O
effort	O
on	O
domains	O
that	O
provide	O
high	O
-	O
level	O
state	O
variables	O
,	O
for	O
example	O
the	O
location	B-Task
of	I-Task
robots	E-Task
in	O
the	O
floor	B-Task
-	I-Task
painting	I-Task
domain	E-Task
described	O
above	O
.	O

By	O
contrast	O
,	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
and	O
the	O
domain	O
-	O
independent	O
setting	O
force	O
us	O
to	O
consider	O
the	O
question	O
of	O
perceptual	B-Task
grounding	E-Task
:	O
how	O
to	O
extract	O
meaningful	O
state	O
information	O
from	O
raw	O
game	O
screens	O
(	O
or	O
RAM	O
information	O
)	O
.	O

In	O
turn	O
,	O
this	O
emphasizes	O
the	O
design	O
of	O
algorithms	O
that	O
can	O
be	O
applied	O
to	O
sensor	B-Material
-	I-Material
rich	I-Material
domains	E-Material
without	O
significant	O
expert	O
knowledge	O
.	O

There	O
have	O
also	O
been	O
a	O
number	O
of	O
attempts	O
to	O
define	O
formal	O
agent	B-Metric
performance	I-Metric
metrics	E-Metric
based	O
on	O
algorithmic	B-Method
information	I-Method
theory	E-Method
.	O

The	O
first	O
such	O
attempts	O
were	O
due	O
to	O
Hernandez	O
-	O
orallo98aformal	O
and	O
to	O
DoweHajek98	O
.	O

More	O
recently	O
,	O
the	O
approaches	O
of	O
Hern10	S-Method
and	O
of	O
legg11	S-Method
appear	O
to	O
have	O
some	O
potential	O
.	O

Although	O
these	O
frameworks	O
are	O
general	O
and	O
conceptually	O
clean	O
,	O
the	O
key	O
challenge	O
remains	O
how	O
to	O
specify	O
sufficiently	O
interesting	O
classes	O
of	O
environments	O
.	O

In	O
our	O
opinion	O
,	O
much	O
more	O
work	O
is	O
required	O
before	O
these	O
approaches	O
can	O
claim	O
to	O
rival	O
the	O
practicality	O
of	O
using	O
a	O
large	O
set	O
of	O
existing	O
human	B-Material
-	I-Material
designed	I-Material
environments	E-Material
for	O
agent	B-Task
evaluation	E-Task
.	O

section	O
:	O
Final	O
Remarks	O
The	O
Atari	B-Material
2600	I-Material
games	E-Material
were	O
developed	O
for	O
humans	O
and	O
as	O
such	O
exhibit	O
many	O
idiosyncrasies	O
that	O
make	O
them	O
both	O
challenging	O
and	O
exciting	O
.	O

Consider	O
,	O
for	O
example	O
,	O
the	O
game	O
Pong	O
.	O

Pong	S-Method
has	O
been	O
studied	O
in	O
a	O
variety	O
of	O
contexts	O
as	O
an	O
interesting	O
reinforcement	B-Task
learning	I-Task
domain	E-Task
.	O

The	O
Atari	B-Method
2600	I-Method
Pong	E-Method
,	O
however	O
,	O
is	O
significantly	O
more	O
complex	O
than	O
Pong	O
domains	O
developed	O
for	O
research	O
.	O

Games	S-Material
can	O
easily	O
last	O
10	O
,	O
000	O
time	O
steps	O
(	O
compared	O
to	O
200–1000	O
in	O
other	O
domains	O
)	O
;	O
observations	O
are	O
composed	O
of	O
7	B-Material
-	I-Material
bit	I-Material
images	E-Material
(	O
compared	O
to	O
black	B-Material
and	I-Material
white	I-Material
images	E-Material
in	O
the	O
work	O
of	O
stober08pixels	O
,	O
or	O
5	O
-	O
6	O
input	O
features	O
elsewhere	O
)	O
;	O
observations	O
are	O
also	O
more	O
complex	O
,	O
containing	O
the	O
two	O
players	O
’	O
score	O
and	O
side	O
walls	O
.	O

In	O
sheer	O
size	O
,	O
the	O
Atari	B-Material
2600	I-Material
Pong	E-Material
is	O
thus	O
a	O
larger	O
domain	O
.	O

Its	O
dynamics	O
are	O
also	O
more	O
complicated	O
.	O

In	O
research	O
implementations	O
of	O
Pong	B-Task
object	I-Task
motion	E-Task
is	O
implemented	O
using	O
first	B-Method
-	I-Method
order	I-Method
mechanics	E-Method
.	O

However	O
,	O
in	O
Atari	B-Task
2600	I-Task
Pong	I-Task
paddle	I-Task
control	E-Task
is	O
nonlinear	O
:	O
simple	O
experimentation	O
shows	O
that	O
fully	O
predicting	O
the	O
player	O
’s	O
paddle	O
requires	O
knowledge	O
of	O
the	O
last	O
18	O
actions	O
.	O

As	O
with	O
many	O
other	O
Atari	B-Task
games	E-Task
,	O
the	O
player	O
paddle	O
also	O
moves	O
every	O
other	O
frame	O
,	O
adding	O
a	O
degree	O
of	O
temporal	O
aliasing	O
to	O
the	O
domain	O
.	O

While	O
Atari	O
2600	O
Pong	O
may	O
appear	O
unnecessarily	O
contrived	O
,	O
it	O
in	O
fact	O
reflects	O
the	O
unexpected	O
complexity	O
of	O
the	O
problems	O
with	O
which	O
humans	O
are	O
faced	O
.	O

Most	O
,	O
if	O
not	O
all	O
Atari	B-Material
2600	I-Material
games	E-Material
are	O
subject	O
to	O
similar	O
programming	O
artifacts	O
:	O
in	O
Space	O
Invaders	O
,	O
for	O
example	O
,	O
the	O
invaders	O
’	O
velocity	O
increases	O
nonlinearly	O
with	O
the	O
number	O
of	O
remaining	O
invaders	O
.	O

In	O
this	O
way	O
the	O
Atari	B-Method
2600	I-Method
platform	E-Method
provides	O
AI	O
researchers	O
with	O
something	O
unique	O
:	O
clean	O
,	O
easily	O
-	O
emulated	O
domains	O
which	O
nevertheless	O
provide	O
many	O
of	O
the	O
challenges	O
typically	O
associated	O
with	O
real	B-Task
-	I-Task
world	I-Task
applications	E-Task
.	O

Should	O
technology	O
advance	O
so	O
as	O
to	O
render	O
general	O
Atari	B-Task
2600	I-Task
game	I-Task
playing	E-Task
achievable	O
,	O
our	O
challenge	O
problem	O
can	O
always	O
be	O
extended	O
to	O
use	O
more	O
recent	O
video	B-Method
game	I-Method
platforms	E-Method
.	O

A	O
natural	O
progression	O
,	O
for	O
example	O
,	O
would	O
be	O
to	O
move	O
on	O
to	O
the	O
Commodore	O
64	O
,	O
then	O
to	O
the	O
Nintendo	O
,	O
and	O
so	O
forth	O
towards	O
current	O
generation	O
consoles	O
.	O

All	O
of	O
these	O
consoles	O
have	O
hundreds	O
of	O
released	B-Material
games	E-Material
,	O
and	O
older	O
platforms	O
have	O
readily	O
available	O
emulators	S-Method
.	O

With	O
the	O
ultra	O
-	O
realism	O
of	O
current	O
generation	O
consoles	O
,	O
each	O
console	O
represents	O
a	O
natural	O
stepping	O
stone	O
toward	O
general	O
real	B-Task
-	I-Task
world	I-Task
competency	E-Task
.	O

Our	O
hope	O
is	O
that	O
by	O
using	O
the	O
methodology	O
advocated	O
in	O
this	O
paper	O
,	O
we	O
can	O
work	O
in	O
a	O
bottom	O
-	O
up	O
fashion	O
towards	O
developing	O
more	O
sophisticated	O
AI	B-Method
technology	E-Method
while	O
still	O
maintaining	O
empirical	O
rigor	O
.	O

section	O
:	O
Conclusion	O
This	O
article	O
has	O
introduced	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
,	O
a	O
platform	O
for	O
evaluating	O
the	O
development	B-Task
of	I-Task
general	I-Task
,	I-Task
domain	I-Task
-	I-Task
independent	I-Task
agents	E-Task
.	O

ALE	S-Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	B-Material
2600	I-Material
game	I-Material
environments	E-Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	S-Method
as	O
a	O
challenge	O
problem	O
by	O
benchmarking	O
several	O
domain	B-Method
-	I-Method
independent	I-Method
agents	E-Method
that	O
use	O
well	O
-	O
established	O
reinforcement	B-Method
learning	I-Method
and	I-Method
planning	I-Method
techniques	E-Method
.	O

Our	O
results	O
suggest	O
that	O
general	B-Task
Atari	I-Task
game	I-Task
playing	E-Task
is	O
a	O
challenging	O
but	O
not	O
intractable	O
problem	O
domain	O
with	O
the	O
potential	O
to	O
aid	O
the	O
development	O
and	O
evaluation	B-Task
of	I-Task
general	I-Task
agents	E-Task
.	O

We	O
would	O
like	O
to	O
thank	O
Marc	O
Lanctot	O
,	O
Erik	O
Talvitie	O
,	O
and	O
Matthew	O
Hausknecht	O
for	O
providing	O
suggestions	O
on	O
helping	O
debug	O
and	O
improving	O
the	O
Arcade	B-Method
Learning	I-Method
Environment	E-Method
source	O
code	O
.	O

We	O
would	O
also	O
like	O
to	O
thank	O
our	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
enthusiasm	O
about	O
the	O
Atari	B-Material
2600	E-Material
as	O
a	O
research	O
platform	O
.	O

The	O
work	O
presented	O
here	O
was	O
supported	O
by	O
the	O
Alberta	B-Material
Innovates	I-Material
Technology	I-Material
Futures	E-Material
,	O
the	O
Alberta	O
Innovates	O
Centre	O
for	O
Machine	B-Task
Learning	E-Task
at	O
the	O
University	O
of	O
Alberta	O
,	O
and	O
the	O
Natural	B-Material
Science	I-Material
and	I-Material
Engineering	I-Material
Research	I-Material
Council	E-Material
of	O
Canada	O
.	O

Invaluable	B-Material
computational	I-Material
resources	E-Material
were	O
provided	O
by	O
Compute	O
/	O
Calcul	O
Canada	O
.	O

appendix	O
:	O
Feature	B-Method
Set	I-Method
Construction	E-Method
This	O
section	O
gives	O
a	O
detailed	O
description	O
of	O
the	O
five	O
feature	B-Method
generation	I-Method
techniques	E-Method
from	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Basic	O
Abstraction	O
of	O
the	O
ScreenShots	O
(	O
BASS	O
)	O
The	O
idea	O
behind	O
BASS	S-Method
is	O
to	O
directly	O
encode	O
colours	O
present	O
on	O
the	O
screen	O
.	O

This	O
method	O
is	O
motivated	O
by	O
three	O
observations	O
on	O
the	O
Atari	B-Material
2600	I-Material
hardware	E-Material
and	O
games	S-Material
:	O
While	O
the	O
Atari	B-Method
2600	I-Method
hardware	E-Method
supports	O
a	O
screen	O
resolution	O
of	O
,	O
game	O
objects	O
are	O
usually	O
larger	O
than	O
a	O
few	O
pixels	O
.	O

Overall	O
,	O
important	O
game	O
events	O
happen	O
at	O
a	O
much	O
lower	O
resolution	O
.	O

Many	O
Atari	B-Material
2600	I-Material
games	E-Material
have	O
a	O
static	O
background	O
,	O
with	O
a	O
few	O
important	O
objects	O
moving	O
on	O
the	O
screen	O
.	O

While	O
the	O
screen	O
matrix	O
is	O
densely	O
populated	O
,	O
the	O
actual	O
interesting	O
features	O
on	O
the	O
screen	O
are	O
often	O
sparse	O
.	O

While	O
the	O
hardware	O
can	O
show	O
up	O
to	O
128	O
colours	O
in	O
the	O
NTSC	O
mode	O
,	O
it	O
is	O
limited	O
to	O
only	O
8	O
colours	O
in	O
the	O
SECAM	B-Method
mode	E-Method
.	O

Consequently	O
,	O
most	O
games	O
use	O
a	O
few	O
number	O
of	O
colours	O
to	O
distinguish	O
important	O
objects	O
on	O
the	O
screen	O
.	O

The	O
game	B-Material
screen	E-Material
is	O
first	O
preprocessed	O
by	O
subtracting	O
its	O
background	O
,	O
detected	O
using	O
a	O
simple	O
histogram	B-Method
method	E-Method
.	O

BASS	S-Method
then	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
eight	O
SECAM	O
palette	O
colours	O
at	O
a	O
low	O
resolution	O
,	O
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Intuitively	O
,	O
BASS	S-Method
seeks	O
to	O
capture	O
the	O
presence	O
of	O
objects	O
of	O
certain	O
colours	O
at	O
different	O
screen	O
locations	O
.	O

BASS	S-Method
also	O
encodes	O
relations	O
between	O
objects	O
by	O
constructing	O
all	O
pairwise	O
combinations	O
of	O
its	O
encoded	O
colour	O
features	O
.	O

In	O
Asterix	S-Material
,	O
for	O
example	O
,	O
it	O
is	O
important	O
to	O
know	O
if	O
there	O
is	O
a	O
green	O
object	O
(	O
player	O
character	O
)	O
and	O
a	O
red	O
object	O
(	O
collectable	O
object	O
)	O
in	O
its	O
vicinity	O
.	O

Pairwise	O
features	O
allow	O
us	O
to	O
capture	O
such	O
object	O
relations	O
.	O

subsection	O
:	O
Basic	O
The	O
Basic	O
method	O
generates	O
the	O
same	O
set	O
of	O
features	O
as	O
BASS	O
,	O
but	O
omits	O
the	O
pairwise	O
combinations	O
.	O

This	O
allows	O
us	O
to	O
study	O
whether	O
the	O
additional	O
features	O
are	O
beneficial	O
or	O
harmful	O
to	O
learning	S-Task
.	O

Because	O
the	O
Basic	O
method	O
has	O
fewer	O
features	O
than	O
BASS	O
,	O
it	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
128	O
colours	O
.	O

In	O
comparison	O
to	O
BASS	O
,	O
Basic	O
therefore	O
represents	O
colour	O
more	O
accurately	O
,	O
but	O
can	O
not	O
represent	O
object	O
interactions	O
.	O

subsection	O
:	O
Detecting	B-Task
Instances	I-Task
of	I-Task
Classes	I-Task
of	I-Task
Objects	E-Task
(	O
DISCO	S-Method
)	O
This	O
feature	B-Method
generation	I-Method
method	E-Method
is	O
based	O
on	O
detecting	O
a	O
set	O
of	O
classes	O
representing	O
game	O
entities	O
and	O
locating	O
instances	O
of	O
these	O
classes	O
on	O
the	O
screen	O
.	O

DISCO	S-Method
is	O
motivated	O
by	O
the	O
following	O
additional	O
observations	O
on	O
Atari	B-Task
2600	I-Task
games	E-Task
:	O
The	O
game	O
entities	O
are	O
often	O
instances	O
of	O
a	O
few	O
classes	O
of	O
objects	O
.	O

For	O
instance	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
while	O
there	O
are	O
many	O
objects	O
in	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Freeway	O
,	O
all	O
of	O
these	O
objects	O
are	O
instances	O
of	O
only	O
two	O
classes	O
:	O
Chicken	O
and	O
Car	O
.	O

Similarly	O
,	O
all	O
the	O
objects	O
on	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Seaquest	O
are	O
instances	O
of	O
one	O
of	O
these	O
six	O
classes	O
:	O
Fish	O
,	O
Swimmer	O
,	O
Player	O
Submarine	O
,	O
Enemy	O
Submarine	O
,	O
Player	O
Bullet	O
,	O
and	O
Enemy	O
Bullet	O
.	O

The	O
interaction	O
between	O
two	O
objects	O
can	O
often	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
their	O
respective	O
classes	O
.	O

As	O
an	O
example	O
,	O
consider	O
Car	B-Task
-	I-Task
Chicken	I-Task
object	I-Task
interactions	E-Task
in	O
Freeway	S-Material
:	O
learning	O
that	O
there	O
is	O
lower	O
value	O
associated	O
with	O
one	O
Chicken	O
instance	O
hitting	O
a	O
Car	O
instance	O
can	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
those	O
two	O
classes	O
.	O

DISCO	S-Method
first	O
performs	O
a	O
series	O
of	O
preprocessing	B-Method
steps	E-Method
to	O
discover	O
classes	O
,	O
during	O
which	O
no	O
value	B-Method
function	I-Method
learning	E-Method
is	O
performed	O
.	O

When	O
the	O
agent	O
subsequently	O
learns	O
to	O
play	O
the	O
game	O
,	O
DISCO	O
generates	O
features	O
by	O
detecting	O
objects	O
on	O
the	O
screen	O
and	O
classifying	O
them	O
.	O

The	O
DISCO	B-Method
process	E-Method
is	O
summarized	O
by	O
the	O
following	O
steps	O
:	O
Locally	B-Method
Sensitive	I-Method
Hashing	I-Method
(	I-Method
LSH	I-Method
)	I-Method
Feature	I-Method
Generation	E-Method
(	O
hash	O
table	O
size	O
)	O
,	O
(	O
screen	O
bit	O
vector	O
size	O
)	O
(	O
number	O
of	O
random	O
bit	O
vectors	O
)	O
,	O
(	O
number	O
of	O
non	O
-	O
zero	O
entries	O
)	O
A	O
screen	O
matrix	O
with	O
elements	O
(	O
has	O
length	O
)	O
Initialize	O
hash	O
the	O
projection	O
of	O
onto	O
one	O
binary	O
feature	O
per	O
random	O
bit	O
vector	O
Initialize	O
,	O
(	O
)	O
Initialize	O
Select	O
distinct	O
coordinates	O
between	O
1	O
and	O
uniformly	O
at	O
random	O
;	O
;	O
…	O
;	O
Initialize	O
,	O
(	O
uniformly	O
random	O
coordinate	O
between	O
1	O
and	O
M	O
)	O
Preprocessing	S-Task
:	O
Background	B-Task
detection	E-Task
:	O
The	O
static	O
background	O
matrix	O
is	O
extracted	O
using	O
a	O
histogram	B-Method
method	E-Method
,	O
as	O
with	O
BASS	S-Method
.	O

Blob	B-Task
extraction	E-Task
:	O
A	O
list	O
of	O
moving	O
blob	O
(	O
foreground	O
)	O
objects	O
is	O
detected	O
in	O
each	O
game	O
screen	O
.	O

Class	B-Task
discovery	E-Task
:	O
A	O
set	O
of	O
classes	O
is	O
detected	O
from	O
the	O
extracted	O
blob	O
objects	O
.	O

Class	B-Method
filtering	E-Method
:	O
Classes	O
that	O
appear	O
infrequently	O
or	O
are	O
restricted	O
to	O
small	O
region	O
of	O
the	O
screen	O
are	O
removed	O
from	O
the	O
set	O
.	O

Class	B-Task
merging	E-Task
:	O
Classes	O
that	O
have	O
similar	O
shapes	O
are	O
merged	O
together	O
.	O

Feature	B-Task
generation	E-Task
:	O
Class	B-Method
instance	I-Method
detection	E-Method
:	O
At	O
each	O
time	O
step	O
,	O
class	O
instances	O
are	O
detected	O
from	O
the	O
current	O
screen	O
matrix	O
.	O

Feature	B-Method
vector	I-Method
generation	E-Method
:	O
A	O
feature	O
vector	O
is	O
generated	O
from	O
the	O
detected	O
instances	O
by	O
tile	O
-	O
coding	O
their	O
absolute	O
position	O
as	O
well	O
as	O
the	O
relative	O
position	O
and	O
velocity	O
of	O
every	O
pair	O
of	O
instances	O
from	O
different	O
classes	O
.	O

Multiple	O
instances	O
of	O
the	O
same	O
objects	O
are	O
combined	O
additively	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
discovered	O
objects	O
in	O
a	O
Seaquest	B-Material
frame	E-Material
.	O

This	O
image	O
illustrates	O
the	O
difficulties	O
in	O
detecting	B-Task
objects	E-Task
:	O
although	O
DISCO	S-Method
correctly	O
classifies	O
the	O
different	O
fish	O
as	O
part	O
of	O
the	O
same	O
class	O
,	O
it	O
also	O
detects	O
a	O
life	O
icon	O
and	O
the	O
oxygen	O
bar	O
as	O
part	O
of	O
that	O
class	O
.	O

subsection	O
:	O
Locality	B-Method
Sensitive	I-Method
Hashing	E-Method
(	O
LSH	S-Method
)	O
An	O
alternative	O
approach	O
to	O
BASS	S-Task
and	O
DISCO	S-Task
is	O
to	O
use	O
well	O
-	O
established	O
feature	B-Method
generation	I-Method
methods	E-Method
that	O
are	O
agnostic	O
about	O
the	O
type	O
of	O
input	O
they	O
receive	O
.	O

Such	O
methods	O
include	O
polynomial	B-Method
bases	E-Method
,	O
sparse	B-Method
distributed	I-Method
memories	E-Method
and	O
locality	B-Method
sensitive	I-Method
hashing	I-Method
(	I-Method
LSH	I-Method
)	E-Method
.	O

In	O
this	O
paper	O
we	O
consider	O
the	O
latter	O
as	O
a	O
simple	O
mean	O
of	O
reducing	O
the	O
large	B-Task
image	I-Task
space	E-Task
to	O
a	O
smaller	O
,	O
more	O
manageable	O
set	O
of	O
features	O
.	O

The	O
input	O
–	O
here	O
,	O
a	O
game	B-Material
screen	E-Material
–	O
is	O
first	O
mapped	O
to	O
a	O
bit	O
vector	O
of	O
size	O
.	O

The	O
resulting	O
vector	O
is	O
then	O
hashed	O
down	O
into	O
a	O
smaller	O
set	O
of	O
features	O
.	O

LSH	S-Method
performs	O
an	O
additional	O
random	B-Method
projection	I-Method
step	E-Method
to	O
ensure	O
that	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
be	O
binned	O
together	O
.	O

The	O
LSH	B-Method
generation	I-Method
method	E-Method
is	O
detailed	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
RAM	B-Method
-	I-Method
based	I-Method
Feature	I-Method
Generation	E-Method
Unlike	O
the	O
previous	O
three	O
methods	O
,	O
which	O
generate	O
feature	O
vectors	O
based	O
on	O
the	O
game	B-Material
screen	E-Material
,	O
the	O
RAM	B-Method
-	I-Method
based	I-Method
feature	I-Method
generation	I-Method
method	E-Method
relies	O
on	O
the	O
contents	O
of	O
the	O
console	O
memory	O
.	O

The	O
Atari	B-Method
2600	E-Method
has	O
only	O
bits	O
of	O
random	O
access	O
memory	O
,	O
which	O
must	O
hold	O
the	O
complete	O
internal	O
state	O
of	O
a	O
game	O
:	O
location	O
of	O
game	O
entities	O
,	O
timers	O
,	O
health	O
indicators	O
,	O
etc	O
.	O

The	O
RAM	S-Method
is	O
therefore	O
a	O
relatively	O
compact	O
representation	O
of	O
the	O
game	O
state	O
,	O
and	O
in	O
contrast	O
to	O
the	O
game	O
screen	O
,	O
it	O
is	O
also	O
Markovian	S-Method
.	O

The	O
purpose	O
of	O
our	O
RAM	B-Method
-	I-Method
based	I-Method
agent	E-Method
is	O
to	O
investigate	O
whether	O
features	O
generated	O
from	O
the	O
RAM	S-Method
affect	O
performance	O
differently	O
from	O
features	O
generated	O
from	O
game	B-Material
screens	E-Material
.	O

The	O
first	O
part	O
of	O
the	O
generated	O
feature	O
vector	O
simply	O
includes	O
the	O
1024	O
bits	O
of	O
RAM	O
.	O

Atari	B-Material
2600	I-Material
game	I-Material
programmers	E-Material
often	O
used	O
these	O
bits	O
not	O
as	O
individual	O
values	O
,	O
but	O
as	O
part	O
of	O
4	O
-	O
bit	O
or	O
8	O
-	O
bit	O
words	O
.	O

Linear	B-Method
function	I-Method
approximation	E-Method
on	O
the	O
individual	O
bits	O
can	O
capture	O
the	O
value	O
of	O
these	O
multi	O
-	O
bit	O
words	O
.	O

We	O
are	O
also	O
interested	O
in	O
the	O
relation	O
between	O
pairs	O
of	O
values	O
in	O
memory	O
.	O

To	O
capture	O
these	O
relations	O
,	O
the	O
logical	O
-	O
AND	O
of	O
all	O
possible	O
bit	O
pairs	O
is	O
appended	O
to	O
the	O
feature	O
vector	O
.	O

Note	O
that	O
a	O
linear	O
function	O
on	O
the	O
pairwise	O
’s	S-Method
can	O
capture	O
products	O
of	O
both	O
4	O
-	O
bit	O
and	O
8	O
-	O
bit	O
words	O
.	O

This	O
is	O
because	O
the	O
product	O
of	O
two	O
-	O
bit	O
words	O
can	O
be	O
expressed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
pairwise	O
products	O
of	O
their	O
bits	O
.	O

appendix	O
:	O
UCT	B-Material
Pseudocode	E-Material
UCT	O
(	O
search	O
horizon	O
)	O
,	O
(	O
simulations	O
per	O
step	O
)	O
(	O
search	O
tree	O
)	O
(	O
current	O
state	O
)	O
is	O
empty	O
or	O
optional	O
is	O
not	O
a	O
leaf	O
,	O
some	O
action	O
was	O
never	O
taken	O
in	O
run	O
model	O
for	O
one	O
step	O
c	O
is	O
necessarily	O
a	O
leaf	O
update	O
-	O
value	O
(	O
n	O
,	O
R	O
)	O
propagate	O
values	O
back	O
up	O
action	O
most	O
frequently	O
taken	O
at	O
root	O
UCT	O
Routines	O
:	O
discount	O
factor	O
children	O
of	O
Initialize	O
Monte	O
-	O
Carlo	O
return	O
to	O
0	O
Select	O
according	O
to	O
some	O
rollout	B-Method
policy	E-Method
(	O
e.g.	O
uniformly	O
randomly	O
)	O
is	O
not	O
the	O
root	O
of	O
,	O
i.e.	O
appendix	O
:	O
Experimental	O
Parameters	O
appendix	O
:	O
Detailed	O
Results	O
subsection	O
:	O
Reinforcement	B-Method
Learning	E-Method
.	O

subsection	O
:	O
Planning	S-Task
.	O

bibliography	O
:	O
References	O
document	O
:	O
An	O
Effective	O
Approach	O
to	O
Unsupervised	B-Task
Machine	I-Task
Translation	E-Task
While	O
machine	B-Task
translation	E-Task
has	O
traditionally	O
relied	O
on	O
large	O
amounts	O
of	O
parallel	O
corpora	O
,	O
a	O
recent	O
research	O
line	O
has	O
managed	O
to	O
train	O
both	O
Neural	B-Method
Machine	I-Method
Translation	E-Method
(	O
NMT	S-Method
)	O
and	O
Statistical	B-Method
Machine	I-Method
Translation	E-Method
(	O
SMT	S-Method
)	O
systems	O
using	O
monolingual	O
corpora	O
only	O
.	O

In	O
this	O
paper	O
,	O
we	O
identify	O
and	O
address	O
several	O
deficiencies	O
of	O
existing	O
unsupervised	O
SMT	S-Method
approaches	O
by	O
exploiting	O
subword	O
information	O
,	O
developing	O
a	O
theoretically	O
well	O
founded	O
unsupervised	B-Method
tuning	I-Method
method	E-Method
,	O
and	O
incorporating	O
a	O
joint	B-Method
refinement	I-Method
procedure	E-Method
.	O

Moreover	O
,	O
we	O
use	O
our	O
improved	O
SMT	S-Method
system	O
to	O
initialize	O
a	O
dual	O
NMT	S-Method
model	O
,	O
which	O
is	O
further	O
fine	O
-	O
tuned	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	B-Method
-	I-Method
translation	E-Method
.	O

Together	O
,	O
we	O
obtain	O
large	O
improvements	O
over	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
.	O

For	O
instance	O
,	O
we	O
get	O
22.5	O
BLEU	B-Metric
points	E-Metric
in	O
English	B-Material
-	I-Material
to	I-Material
-	I-Material
German	I-Material
WMT	I-Material
2014	E-Material
,	O
5.5	O
points	O
more	O
than	O
the	O
previous	O
best	O
unsupervised	B-Method
system	E-Method
,	O
and	O
0.5	O
points	O
more	O
than	O
the	O
(	O
supervised	O
)	O
shared	O
task	O
winner	O
back	O
in	O
2014	O
.	O

section	O
:	O
Introduction	O
The	O
recent	O
advent	O
of	O
neural	B-Method
sequence	I-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
modeling	E-Method
has	O
resulted	O
in	O
significant	O
progress	O
in	O
the	O
field	O
of	O
machine	B-Task
translation	E-Task
,	O
with	O
large	O
improvements	O
in	O
standard	O
benchmarks	O
vaswani2017attention	O
,	O
edunov2018understanding	O
and	O
the	O
first	O
solid	O
claims	O
of	O
human	B-Metric
parity	E-Metric
in	O
certain	O
settings	O
hassan2018achieving	O
.	O

Unfortunately	O
,	O
these	O
systems	O
rely	O
on	O
large	O
amounts	O
of	O
parallel	O
corpora	O
,	O
which	O
are	O
only	O
available	O
for	O
a	O
few	O
combinations	O
of	O
major	O
languages	O
like	O
English	S-Material
,	O
German	S-Material
and	O
French	S-Material
.	O

Aiming	O
to	O
remove	O
this	O
dependency	O
on	O
parallel	O
data	O
,	O
a	O
recent	O
research	O
line	O
has	O
managed	O
to	O
train	O
unsupervised	B-Task
machine	I-Task
translation	I-Task
systems	E-Task
using	O
monolingual	O
corpora	O
only	O
.	O

The	O
first	O
such	O
systems	O
were	O
based	O
on	O
Neural	B-Method
Machine	I-Method
Translation	E-Method
(	O
NMT	S-Method
)	O
,	O
and	O
combined	O
denoising	B-Method
autoencoding	E-Method
and	O
back	B-Method
-	I-Method
translation	E-Method
to	O
train	O
a	O
dual	B-Method
model	E-Method
initialized	O
with	O
cross	B-Method
-	I-Method
lingual	I-Method
embeddings	E-Method
artetxe2018unmt	O
,	O
lample2018unsupervised	O
.	O

Nevertheless	O
,	O
these	O
early	O
systems	O
were	O
later	O
superseded	O
by	O
Statistical	B-Method
Machine	I-Method
Translation	E-Method
(	O
SMT	S-Method
)	O
based	O
approaches	O
,	O
which	O
induced	O
an	O
initial	O
phrase	O
-	O
table	O
through	O
cross	B-Method
-	I-Method
lingual	I-Method
embedding	I-Method
mappings	E-Method
,	O
combined	O
it	O
with	O
an	O
n	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
,	O
and	O
further	O
improved	O
the	O
system	O
through	O
iterative	B-Method
back	I-Method
-	I-Method
translation	E-Method
lample2018phrase	O
,	O
artetxe2018usmt	O
.	O

In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
more	O
principled	O
approach	O
to	O
unsupervised	B-Task
SMT	E-Task
,	O
addressing	O
several	O
deficiencies	O
of	O
previous	O
systems	O
by	O
incorporating	O
subword	O
information	O
,	O
applying	O
a	O
theoretically	O
well	O
founded	O
unsupervised	B-Method
tuning	I-Method
method	E-Method
,	O
and	O
developing	O
a	O
joint	B-Method
refinement	I-Method
procedure	E-Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
use	O
our	O
improved	O
SMT	S-Method
approach	O
to	O
initialize	O
an	O
unsupervised	O
NMT	S-Method
system	O
,	O
which	O
is	O
further	O
improved	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	B-Method
-	I-Method
translation	E-Method
.	O

Our	O
experiments	O
on	O
WMT	B-Material
2014	I-Material
/	I-Material
2016	I-Material
French	I-Material
-	I-Material
English	E-Material
and	O
German	B-Material
-	I-Material
English	E-Material
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
as	O
our	O
proposed	O
system	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
by	O
5	O
-	O
7	O
BLEU	B-Metric
points	E-Metric
in	O
all	O
these	O
datasets	O
and	O
translation	O
directions	O
.	O

Our	O
system	O
also	O
outperforms	O
the	O
supervised	O
WMT	S-Material
2014	O
shared	O
task	O
winner	O
in	O
English	B-Material
-	I-Material
to	I-Material
-	I-Material
German	E-Material
,	O
and	O
is	O
around	O
2	O
BLEU	B-Metric
points	E-Metric
behind	O
it	O
in	O
the	O
rest	O
of	O
translation	B-Task
directions	E-Task
,	O
suggesting	O
that	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

The	O
remaining	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
first	O
discusses	O
the	O
related	O
work	O
in	O
the	O
topic	O
.	O

Section	O
[	O
reference	O
]	O
then	O
describes	O
our	O
principled	B-Method
unsupervised	I-Method
SMT	I-Method
method	E-Method
,	O
while	O
Section	O
[	O
reference	O
]	O
discusses	O
our	O
hybridization	B-Method
method	E-Method
with	O
NMT	S-Method
.	O

We	O
then	O
present	O
the	O
experiments	O
done	O
and	O
the	O
results	O
obtained	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
Section	O
[	O
reference	O
]	O
concludes	O
the	O
paper	O
.	O

section	O
:	O
Related	O
work	O
Early	O
attempts	O
to	O
build	O
machine	B-Task
translation	I-Task
systems	E-Task
with	O
monolingual	O
corpora	O
go	O
back	O
to	O
statistical	B-Task
decipherment	E-Task
ravi2011deciphering	O
,	O
dou2012large	O
.	O

These	O
methods	O
see	O
the	O
source	O
language	O
as	O
ciphertext	O
produced	O
by	O
a	O
noisy	B-Method
channel	I-Method
model	E-Method
that	O
first	O
generates	O
the	O
original	O
English	O
text	O
and	O
then	O
probabilistically	O
replaces	O
the	O
words	O
in	O
it	O
.	O

The	O
English	B-Method
generative	I-Method
process	E-Method
is	O
modeled	O
using	O
an	O
n	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
,	O
and	O
the	O
channel	B-Method
model	I-Method
parameters	E-Method
are	O
estimated	O
using	O
either	O
expectation	B-Method
maximization	E-Method
or	O
Bayesian	B-Method
inference	E-Method
.	O

This	O
basic	O
approach	O
was	O
later	O
improved	O
by	O
incorporating	O
syntactic	O
knowledge	O
dou2013dependency	O
and	O
word	O
embeddings	O
dou2015unifying	O
.	O

Nevertheless	O
,	O
these	O
methods	O
were	O
only	O
shown	O
to	O
work	O
in	O
limited	O
settings	O
,	O
being	O
most	O
often	O
evaluated	O
in	O
word	B-Task
-	I-Task
level	I-Task
translation	E-Task
.	O

More	O
recently	O
,	O
the	O
task	O
got	O
a	O
renewed	O
interest	O
after	O
the	O
concurrent	O
work	O
of	O
artetxe2018unmt	O
and	O
lample2018unsupervised	O
on	O
unsupervised	B-Method
NMT	E-Method
which	O
,	O
for	O
the	O
first	O
time	O
,	O
obtained	O
promising	O
results	O
in	O
standard	O
machine	B-Task
translation	I-Task
benchmarks	E-Task
using	O
monolingual	O
corpora	O
only	O
.	O

Both	O
methods	O
build	O
upon	O
the	O
recent	O
work	O
on	O
unsupervised	B-Task
cross	I-Task
-	I-Task
lingual	I-Task
embedding	I-Task
mappings	E-Task
,	O
which	O
independently	O
train	O
word	O
embeddings	O
in	O
two	O
languages	O
and	O
learn	O
a	O
linear	B-Method
transformation	E-Method
to	O
map	O
them	O
to	O
a	O
shared	O
space	O
through	O
self	B-Method
-	I-Method
learning	E-Method
artetxe2017learning	O
,	O
artetxe2018robust	B-Method
or	I-Method
adversarial	I-Method
training	E-Method
conneau2018word	O
.	O

The	O
resulting	O
cross	B-Method
-	I-Method
lingual	I-Method
embeddings	E-Method
are	O
used	O
to	O
initialize	O
a	O
shared	B-Method
encoder	E-Method
for	O
both	O
languages	O
,	O
and	O
the	O
entire	O
system	O
is	O
trained	O
using	O
a	O
combination	O
of	O
denoising	B-Method
autoencoding	E-Method
,	O
back	B-Method
-	I-Method
translation	E-Method
and	O
,	O
in	O
the	O
case	O
of	O
lample2018unsupervised	B-Method
,	I-Method
adversarial	I-Method
training	E-Method
.	O

This	O
method	O
was	O
further	O
improved	O
by	O
yang2018unsupervised	O
,	O
who	O
use	O
two	O
language	B-Method
-	I-Method
specific	I-Method
encoders	E-Method
sharing	O
only	O
a	O
subset	O
of	O
their	O
parameters	O
,	O
and	O
incorporate	O
a	O
local	B-Method
and	I-Method
a	I-Method
global	I-Method
generative	I-Method
adversarial	I-Method
network	E-Method
.	O

Nevertheless	O
,	O
it	O
was	O
later	O
argued	O
that	O
the	O
modular	B-Method
architecture	E-Method
of	O
phrase	O
-	O
based	O
SMT	S-Method
was	O
more	O
suitable	O
for	O
this	O
problem	O
,	O
and	O
lample2018phrase	O
and	O
artetxe2018usmt	O
adapted	O
the	O
same	O
principles	O
discussed	O
above	O
to	O
train	O
an	O
unsupervised	O
SMT	S-Method
model	O
,	O
obtaining	O
large	O
improvements	O
over	O
the	O
original	O
unsupervised	O
NMT	S-Method
systems	O
.	O

More	O
concretely	O
,	O
both	O
approaches	O
learn	O
cross	B-Method
-	I-Method
lingual	I-Method
n	I-Method
-	I-Method
gram	I-Method
embeddings	E-Method
from	O
monolingual	O
corpora	O
based	O
on	O
the	O
mapping	B-Method
method	E-Method
discussed	O
earlier	O
,	O
and	O
use	O
them	O
to	O
induce	O
an	O
initial	O
phrase	O
-	O
table	O
that	O
is	O
combined	O
with	O
an	O
n	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
and	O
a	O
distortion	B-Method
model	E-Method
.	O

This	O
initial	O
system	O
is	O
then	O
refined	O
through	O
iterative	B-Method
back	I-Method
-	I-Method
translation	E-Method
sennrich2016improving	O
which	O
,	O
in	O
the	O
case	O
of	O
artetxe2018usmt	O
,	O
is	O
preceded	O
by	O
an	O
unsupervised	B-Method
tuning	I-Method
step	E-Method
.	O

Our	O
work	O
identifies	O
some	O
deficiencies	O
in	O
these	O
previous	O
systems	O
,	O
and	O
proposes	O
a	O
more	O
principled	B-Method
approach	E-Method
to	O
unsupervised	B-Task
SMT	E-Task
that	O
incorporates	O
subword	O
information	O
,	O
uses	O
a	O
theoretically	O
better	O
founded	O
unsupervised	B-Method
tuning	I-Method
method	E-Method
,	O
and	O
applies	O
a	O
joint	B-Method
refinement	I-Method
procedure	E-Method
,	O
outperforming	O
these	O
previous	O
systems	O
by	O
a	O
substantial	O
margin	O
.	O

Very	O
recently	O
,	O
some	O
authors	O
have	O
tried	O
to	O
combine	O
both	O
SMT	S-Method
and	O
NMT	S-Method
to	O
build	O
hybrid	B-Task
unsupervised	I-Task
machine	I-Task
translation	I-Task
systems	E-Task
.	O

This	O
idea	O
was	O
already	O
explored	O
by	O
lample2018phrase	O
,	O
who	O
aided	O
the	O
training	O
of	O
their	O
unsupervised	O
NMT	S-Method
system	O
by	O
combining	O
standard	O
back	B-Method
-	I-Method
translation	E-Method
with	O
synthetic	O
parallel	O
data	O
generated	O
by	O
unsupervised	B-Task
SMT	E-Task
.	O

marie2018unsupervised	O
go	O
further	O
and	O
use	O
synthetic	O
parallel	O
data	O
from	O
unsupervised	B-Task
SMT	E-Task
to	O
train	O
a	O
conventional	O
NMT	S-Method
system	O
from	O
scratch	O
.	O

The	O
resulting	O
NMT	S-Method
model	O
is	O
then	O
used	O
to	O
augment	O
the	O
synthetic	O
parallel	O
corpus	O
through	O
back	B-Method
-	I-Method
translation	E-Method
,	O
and	O
a	O
new	O
NMT	S-Method
model	O
is	O
trained	O
on	O
top	O
of	O
it	O
from	O
scratch	O
,	O
repeating	O
the	O
process	O
iteratively	O
.	O

ren2019unsupervised	O
follow	O
a	O
similar	O
approach	O
,	O
but	O
use	O
SMT	S-Method
as	O
posterior	B-Method
regularization	E-Method
at	O
each	O
iteration	O
.	O

As	O
shown	O
later	O
in	O
our	O
experiments	O
,	O
our	O
proposed	O
NMT	S-Method
hybridization	O
obtains	O
substantially	O
larger	O
absolute	O
gains	O
than	O
all	O
these	O
previous	O
approaches	O
,	O
even	O
if	O
our	O
initial	O
SMT	S-Method
system	O
is	O
stronger	O
and	O
thus	O
more	O
challenging	O
to	O
improve	O
upon	O
.	O

section	O
:	O
Principled	O
unsupervised	O
SMT	S-Method
Phrase	O
-	O
based	O
SMT	S-Method
is	O
formulated	O
as	O
a	O
log	B-Method
-	I-Method
linear	I-Method
combination	E-Method
of	O
several	O
statistical	B-Method
models	E-Method
:	O
a	O
translation	B-Method
model	E-Method
,	O
a	O
language	B-Method
model	E-Method
,	O
a	O
reordering	B-Method
model	E-Method
and	O
a	O
word	B-Method
/	I-Method
phrase	I-Method
penalty	E-Method
.	O

As	O
such	O
,	O
building	O
an	O
unsupervised	O
SMT	S-Method
system	O
requires	O
learning	O
these	O
different	O
components	O
from	O
monolingual	O
corpora	O
.	O

As	O
it	O
turns	O
out	O
,	O
this	O
is	O
straightforward	O
for	O
most	O
of	O
them	O
:	O
the	O
language	B-Method
model	E-Method
is	O
learned	O
from	O
monolingual	O
corpora	O
by	O
definition	O
;	O
the	O
word	O
and	O
phrase	O
penalties	O
are	O
parameterless	O
;	O
and	O
one	O
can	O
drop	O
the	O
standard	O
lexical	B-Method
reordering	I-Method
model	E-Method
at	O
a	O
small	O
cost	O
and	O
do	O
with	O
the	O
distortion	B-Method
model	E-Method
alone	O
,	O
which	O
is	O
also	O
parameterless	O
.	O

This	O
way	O
,	O
the	O
main	O
challenge	O
left	O
is	O
learning	O
the	O
translation	B-Method
model	E-Method
,	O
that	O
is	O
,	O
building	O
the	O
phrase	O
-	O
table	O
.	O

Our	O
proposed	O
method	O
starts	O
by	O
building	O
an	O
initial	O
phrase	O
-	O
table	O
through	O
cross	B-Method
-	I-Method
lingual	I-Method
embedding	I-Method
mappings	E-Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

This	O
initial	O
phrase	O
-	O
table	O
is	O
then	O
extended	O
by	O
incorporating	O
subword	O
information	O
,	O
addressing	O
one	O
of	O
the	O
main	O
limitations	O
of	O
previous	O
unsupervised	O
SMT	S-Method
systems	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Having	O
done	O
that	O
,	O
we	O
adjust	O
the	O
weights	O
of	O
the	O
underlying	O
log	B-Method
-	I-Method
linear	I-Method
model	E-Method
through	O
a	O
novel	O
unsupervised	B-Method
tuning	I-Method
procedure	E-Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
we	O
further	O
improve	O
the	O
system	O
by	O
jointly	O
refining	O
two	O
models	O
in	O
opposite	O
directions	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Initial	O
phrase	O
-	O
table	O
So	O
as	O
to	O
build	O
our	O
initial	O
phrase	O
-	O
table	O
,	O
we	O
follow	O
artetxe2018usmt	O
and	O
learn	O
n	B-Method
-	I-Method
gram	I-Method
embeddings	E-Method
for	O
each	O
language	O
independently	O
,	O
map	O
them	O
to	O
a	O
shared	O
space	O
through	O
self	B-Method
-	I-Method
learning	E-Method
,	O
and	O
use	O
the	O
resulting	O
cross	B-Method
-	I-Method
lingual	I-Method
embeddings	E-Method
to	O
extract	O
and	O
score	O
phrase	O
pairs	O
.	O

More	O
concretely	O
,	O
we	O
train	O
our	O
n	B-Method
-	I-Method
gram	I-Method
embeddings	E-Method
using	O
phrase2vechttps:	O
//	O
github.com	O
/	O
artetxem	O
/	O
phrase2vec	O
,	O
a	O
simple	O
extension	O
of	O
skip	B-Method
-	I-Method
gram	E-Method
that	O
applies	O
the	O
standard	O
negative	B-Method
sampling	I-Method
loss	E-Method
of	O
mikolov2013distributed	O
to	O
bigram	O
-	O
context	O
and	O
trigram	O
-	O
context	O
pairs	O
in	O
addition	O
to	O
the	O
usual	O
word	O
-	O
context	O
pairs	O
.	O

Having	O
done	O
that	O
,	O
we	O
map	O
the	O
embeddings	O
to	O
a	O
cross	O
-	O
lingual	O
space	O
using	O
VecMap	S-Method
with	O
identical	B-Method
initialization	E-Method
artetxe2018robust	O
,	O
which	O
builds	O
an	O
initial	O
solution	O
by	O
aligning	O
identical	O
words	O
and	O
iteratively	O
improves	O
it	O
through	O
self	B-Method
-	I-Method
learning	E-Method
.	O

Finally	O
,	O
we	O
extract	O
translation	O
candidates	O
by	O
taking	O
the	O
100	O
nearest	O
-	O
neighbors	O
of	O
each	O
source	O
phrase	O
,	O
and	O
score	O
them	O
by	O
applying	O
the	O
softmax	B-Method
function	E-Method
over	O
their	O
cosine	O
similarities	O
:	O
where	O
the	O
temperature	O
is	O
estimated	O
using	O
maximum	B-Method
likelihood	I-Method
estimation	E-Method
over	O
a	O
dictionary	O
induced	O
in	O
the	O
reverse	O
direction	O
.	O

In	O
addition	O
to	O
the	O
phrase	O
translation	O
probabilities	O
in	O
both	O
directions	O
,	O
the	O
forward	O
and	O
reverse	O
lexical	O
weightings	O
are	O
also	O
estimated	O
by	O
aligning	O
each	O
word	O
in	O
the	O
target	O
phrase	O
with	O
the	O
one	O
in	O
the	O
source	O
phrase	O
most	O
likely	O
generating	O
it	O
,	O
and	O
taking	O
the	O
product	O
of	O
their	O
respective	O
translation	O
probabilities	O
.	O

The	O
reader	O
is	O
referred	O
to	O
artetxe2018usmt	O
for	O
more	O
details	O
.	O

subsection	O
:	O
Adding	O
subword	O
information	O
An	O
inherent	O
limitation	O
of	O
existing	O
unsupervised	O
SMT	S-Method
systems	O
is	O
that	O
words	O
are	O
taken	O
as	O
atomic	O
units	O
,	O
making	O
it	O
impossible	O
to	O
exploit	O
character	O
-	O
level	O
information	O
.	O

This	O
is	O
reflected	O
in	O
the	O
known	O
difficulty	O
of	O
these	O
models	O
to	O
translate	O
named	O
entities	O
,	O
as	O
it	O
is	O
very	O
challenging	O
to	O
discriminate	O
among	O
related	O
proper	O
nouns	O
based	O
on	O
distributional	O
information	O
alone	O
,	O
yielding	O
to	O
translation	O
errors	O
like	O
‘	O
‘	O
Sunday	O
Telegraph	O
’	O
’	O
‘	O
‘	O
The	O
Times	O
of	O
London	O
’	O
’	O
artetxe2018usmt	O
.	O

So	O
as	O
to	O
overcome	O
this	O
issue	O
,	O
we	O
propose	O
to	O
incorporate	O
subword	O
information	O
once	O
the	O
initial	O
alignment	O
is	O
done	O
at	O
the	O
word	O
/	O
phrase	O
level	O
.	O

For	O
that	O
purpose	O
,	O
we	O
add	O
two	O
additional	O
weights	O
to	O
the	O
initial	O
phrase	O
-	O
table	O
that	O
are	O
analogous	O
to	O
the	O
lexical	O
weightings	O
,	O
but	O
use	O
a	O
character	O
-	O
level	O
similarity	B-Method
function	E-Method
instead	O
of	O
word	O
translation	O
probabilities	O
:	O
where	O
guarantees	O
a	O
minimum	B-Metric
similarity	I-Metric
score	E-Metric
,	O
as	O
we	O
want	O
to	O
favor	O
translation	O
candidates	O
that	O
are	O
similar	O
at	O
the	O
character	O
level	O
without	O
excessively	O
penalizing	O
those	O
that	O
are	O
not	O
.	O

In	O
our	O
case	O
,	O
we	O
use	O
a	O
simple	O
similarity	B-Method
function	E-Method
that	O
normalizes	O
the	O
Levenshtein	O
distance	O
levenshtein1966binary	O
by	O
the	O
length	O
of	O
the	O
words	O
:	O
We	O
leave	O
the	O
exploration	O
of	O
more	O
elaborated	O
similarity	B-Method
functions	E-Method
and	O
,	O
in	O
particular	O
,	O
learnable	B-Metric
metrics	E-Metric
mccallum2005conditional	O
,	O
for	O
future	O
work	O
.	O

subsection	O
:	O
Unsupervised	B-Method
tuning	E-Method
Having	O
trained	O
the	O
underlying	O
statistical	B-Method
models	E-Method
independently	O
,	O
SMT	S-Method
tuning	O
aims	O
to	O
adjust	O
the	O
weights	O
of	O
their	O
resulting	O
log	B-Method
-	I-Method
linear	I-Method
combination	E-Method
to	O
optimize	O
some	O
evaluation	B-Metric
metric	E-Metric
like	O
BLEU	S-Metric
in	O
a	O
parallel	O
validation	O
corpus	O
,	O
which	O
is	O
typically	O
done	O
through	O
Minimum	B-Metric
Error	I-Metric
Rate	I-Metric
Training	E-Metric
or	O
MERT	S-Metric
och2003MERT	O
.	O

Needless	O
to	O
say	O
,	O
this	O
can	O
not	O
be	O
done	O
in	O
strictly	O
unsupervised	B-Task
settings	E-Task
,	O
but	O
we	O
argue	O
that	O
it	O
would	O
still	O
be	O
desirable	O
to	O
optimize	O
some	O
unsupervised	B-Metric
criterion	E-Metric
that	O
is	O
expected	O
to	O
correlate	O
well	O
with	O
test	O
performance	O
.	O

Unfortunately	O
,	O
neither	O
of	O
the	O
existing	O
unsupervised	O
SMT	S-Method
systems	O
do	O
so	O
:	O
artetxe2018usmt	O
use	O
a	O
heuristic	O
that	O
builds	O
two	O
initial	O
models	O
in	O
opposite	O
directions	O
,	O
uses	O
one	O
of	O
them	O
to	O
generates	O
a	O
synthetic	O
parallel	O
corpus	O
through	O
back	B-Method
-	I-Method
translation	E-Method
sennrich2016improving	O
,	O
and	O
applies	O
MERT	S-Metric
to	O
tune	O
the	O
model	O
in	O
the	O
reverse	O
direction	O
,	O
iterating	O
until	O
convergence	O
,	O
whereas	O
lample2018phrase	O
do	O
not	O
perform	O
any	O
tuning	O
at	O
all	O
.	O

In	O
what	O
follows	O
,	O
we	O
propose	O
a	O
more	O
principled	O
approach	O
to	O
tuning	S-Task
that	O
defines	O
an	O
unsupervised	B-Metric
criterion	E-Metric
and	O
an	O
optimization	B-Method
procedure	E-Method
that	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
local	O
optimum	O
of	O
it	O
.	O

Inspired	O
by	O
the	O
previous	O
work	O
on	O
CycleGANs	S-Method
zhu2017unpaired	O
and	O
dual	B-Method
learning	E-Method
he2016dual	O
,	O
our	O
method	O
takes	O
two	O
initial	O
models	O
in	O
opposite	O
directions	O
,	O
and	O
defines	O
an	O
unsupervised	B-Method
optimization	I-Method
objective	E-Method
that	O
combines	O
a	O
cyclic	B-Method
consistency	I-Method
loss	E-Method
and	O
a	O
language	B-Method
model	I-Method
loss	E-Method
over	O
the	O
two	O
monolingual	O
corpora	O
and	O
:	O
The	O
cyclic	O
consistency	O
loss	O
captures	O
the	O
intuition	O
that	O
the	O
translation	O
of	O
a	O
translation	O
should	O
be	O
close	O
to	O
the	O
original	O
text	O
.	O

So	O
as	O
to	O
quantify	O
this	O
,	O
we	O
take	O
a	O
monolingual	O
corpus	O
in	O
the	O
source	O
language	O
,	O
translate	O
it	O
to	O
the	O
target	O
language	O
and	O
back	O
to	O
the	O
source	O
language	O
,	O
and	O
compute	O
its	O
BLEU	B-Metric
score	E-Metric
taking	O
the	O
original	O
text	O
as	O
reference	O
:	O
At	O
the	O
same	O
time	O
,	O
the	O
language	B-Method
model	I-Method
loss	E-Method
captures	O
the	O
intuition	O
that	O
machine	B-Task
translation	E-Task
should	O
produce	O
fluent	O
text	O
in	O
the	O
target	O
language	O
.	O

For	O
that	O
purpose	O
,	O
we	O
estimate	O
the	O
per	O
-	O
word	O
entropy	O
in	O
the	O
target	O
language	O
corpus	O
using	O
an	O
n	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
,	O
and	O
penalize	O
higher	O
per	O
-	O
word	O
entropies	O
in	O
machine	O
translated	O
text	O
as	O
follows	O
:	O
where	O
the	O
length	O
penalty	O
penalizes	O
excessively	O
long	O
translations	O
:	O
So	O
as	O
to	O
minimize	O
the	O
combined	O
loss	O
function	O
,	O
we	O
adapt	O
MERT	S-Metric
to	O
jointly	O
optimize	O
the	O
parameters	O
of	O
the	O
two	O
models	O
.	O

In	O
its	O
basic	O
form	O
,	O
MERT	S-Metric
approximates	O
the	O
search	O
space	O
for	O
each	O
source	O
sentence	O
through	O
an	O
n	O
-	O
best	O
list	O
,	O
and	O
performs	O
a	O
form	O
of	O
coordinate	B-Method
descent	E-Method
by	O
computing	O
the	O
optimal	O
value	O
for	O
each	O
parameter	O
through	O
an	O
efficient	O
line	B-Method
search	I-Method
method	E-Method
and	O
greedily	O
taking	O
the	O
step	O
that	O
leads	O
to	O
the	O
largest	O
gain	O
.	O

The	O
process	O
is	O
repeated	O
iteratively	O
until	O
convergence	O
,	O
augmenting	O
the	O
n	O
-	O
best	O
list	O
with	O
the	O
updated	O
parameters	O
at	O
each	O
iteration	O
so	O
as	O
to	O
obtain	O
a	O
better	O
approximation	O
of	O
the	O
full	O
search	O
space	O
.	O

Given	O
that	O
our	O
optimization	B-Task
objective	E-Task
combines	O
two	O
translation	B-Method
systems	E-Method
,	O
this	O
would	O
require	O
generating	O
an	O
n	O
-	O
best	O
list	O
for	O
first	O
and	O
,	O
for	O
each	O
entry	O
on	O
it	O
,	O
generating	O
a	O
new	O
n	O
-	O
best	O
list	O
with	O
,	O
yielding	O
a	O
combined	O
n	O
-	O
best	O
list	O
with	O
entries	O
.	O

So	O
as	O
to	O
make	O
it	O
more	O
efficient	O
,	O
we	O
propose	O
an	O
alternating	B-Method
optimization	I-Method
approach	E-Method
where	O
we	O
fix	O
the	O
parameters	O
of	O
one	O
model	O
and	O
optimize	O
the	O
other	O
with	O
standard	O
MERT	S-Metric
.	O

Thanks	O
to	O
this	O
,	O
we	O
do	O
not	O
need	O
to	O
expand	O
the	O
search	O
space	O
of	O
the	O
fixed	B-Method
model	E-Method
,	O
so	O
we	O
can	O
do	O
with	O
an	O
n	O
-	O
best	O
list	O
of	O
entries	O
alone	O
.	O

Having	O
done	O
that	O
,	O
we	O
fix	O
the	O
parameters	O
of	O
the	O
opposite	O
model	O
and	O
optimize	O
the	O
other	O
,	O
iterating	O
until	O
convergence	O
.	O

subsection	O
:	O
Joint	B-Task
refinement	E-Task
Constrained	O
by	O
the	O
lack	O
of	O
parallel	O
corpora	O
,	O
the	O
procedure	O
described	O
so	O
far	O
makes	O
important	O
simplifications	O
that	O
could	O
compromise	O
its	O
potential	O
performance	O
:	O
its	O
phrase	O
-	O
table	O
is	O
somewhat	O
unnatural	O
(	O
e.g.	O
the	O
translation	O
probabilities	O
are	O
estimated	O
from	O
cross	B-Method
-	I-Method
lingual	I-Method
embeddings	E-Method
rather	O
than	O
actual	O
frequency	O
counts	O
)	O
and	O
it	O
lacks	O
a	O
lexical	B-Method
reordering	I-Method
model	E-Method
altogether	O
.	O

So	O
as	O
to	O
overcome	O
this	O
issue	O
,	O
existing	O
unsupervised	O
SMT	S-Method
methods	O
generate	O
a	O
synthetic	O
parallel	O
corpus	O
through	O
back	B-Method
-	I-Method
translation	E-Method
and	O
use	O
it	O
to	O
train	O
a	O
standard	O
SMT	S-Method
system	O
from	O
scratch	O
,	O
iterating	O
until	O
convergence	O
.	O

An	O
obvious	O
drawback	O
of	O
this	O
approach	O
is	O
that	O
the	O
back	O
-	O
translated	O
side	O
will	O
contain	O
ungrammatical	O
n	O
-	O
grams	O
that	O
will	O
end	O
up	O
in	O
the	O
induced	O
phrase	O
-	O
table	O
.	O

One	O
could	O
argue	O
that	O
this	O
should	O
be	O
innocuous	O
as	O
long	O
as	O
the	O
ungrammatical	O
n	O
-	O
grams	O
are	O
in	O
the	O
source	O
side	O
,	O
as	O
they	O
should	O
never	O
occur	O
in	O
real	O
text	O
and	O
their	O
corresponding	O
entries	O
in	O
the	O
phrase	O
-	O
table	O
should	O
therefore	O
not	O
be	O
used	O
.	O

However	O
,	O
ungrammatical	O
source	O
phrases	O
do	O
ultimately	O
affect	O
the	O
estimation	O
of	O
the	O
backward	O
translation	O
probabilities	O
,	O
including	O
those	O
of	O
grammatical	O
phrases	O
.	O

For	O
instance	O
,	O
let	O
’s	O
say	O
that	O
the	O
target	O
phrase	O
‘	O
‘	O
dos	O
gatos	O
’	O
’	O
has	O
been	O
aligned	O
10	O
times	O
with	O
‘	O
‘	O
two	O
cats	O
’	O
’	O
and	O
90	O
times	O
with	O
‘	O
‘	O
two	O
cat	O
’	O
’	O
.	O

While	O
the	O
ungrammatical	O
phrase	O
-	O
table	O
entry	O
two	O
cat	O
-	O
dos	O
gatos	O
should	O
never	O
be	O
picked	O
,	O
the	O
backward	B-Method
probability	I-Method
estimation	E-Method
of	O
two	O
cats	O
-	O
dos	O
gatos	O
is	O
still	O
affected	O
by	O
it	O
(	O
it	O
would	O
be	O
0.1	O
instead	O
of	O
1.0	O
in	O
this	O
example	O
)	O
.	O

We	O
argue	O
that	O
,	O
ultimately	O
,	O
the	O
backward	B-Method
probability	I-Method
estimations	E-Method
can	O
only	O
be	O
meaningful	O
when	O
all	O
source	O
phrases	O
are	O
grammatical	O
(	O
so	O
the	O
probabilities	O
of	O
all	O
plausible	O
translations	O
sum	O
to	O
one	O
)	O
and	O
,	O
similarly	O
,	O
the	O
forward	B-Method
probability	I-Method
estimations	E-Method
can	O
only	O
be	O
meaningful	O
when	O
all	O
target	O
phrases	O
are	O
grammatical	O
.	O

Following	O
this	O
observation	O
,	O
we	O
propose	O
an	O
alternative	O
approach	O
that	O
jointly	O
refines	O
both	O
translation	O
directions	O
.	O

More	O
concretely	O
,	O
we	O
use	O
the	O
initial	O
systems	O
to	O
build	O
two	O
synthetic	O
corpora	O
in	O
opposite	O
directions	O
.	O

Having	O
done	O
that	O
,	O
we	O
independently	O
extract	O
phrase	O
pairs	O
from	O
each	O
synthetic	O
corpus	O
,	O
and	O
build	O
a	O
phrase	O
-	O
table	O
by	O
taking	O
their	O
intersection	O
.	O

The	O
forward	O
probabilities	O
are	O
estimated	O
in	O
the	O
parallel	O
corpus	O
with	O
the	O
synthetic	O
source	O
side	O
,	O
while	O
the	O
backward	O
probabilities	O
are	O
estimated	O
in	O
the	O
one	O
with	O
the	O
synthetic	O
target	O
side	O
.	O

This	O
does	O
not	O
only	O
guarantee	O
that	O
the	O
probability	O
estimates	O
are	O
meaningful	O
as	O
discussed	O
previously	O
,	O
but	O
it	O
also	O
discards	O
the	O
ungrammatical	O
phrases	O
altogether	O
,	O
as	O
both	O
the	O
source	O
and	O
the	O
target	O
n	O
-	O
grams	O
must	O
have	O
occurred	O
in	O
the	O
original	O
monolingual	O
texts	O
to	O
be	O
present	O
in	O
the	O
resulting	O
phrase	O
-	O
table	O
.	O

We	O
repeat	O
this	O
process	O
for	O
a	O
total	O
of	O
3	O
iterations	O
.	O

section	O
:	O
NMT	S-Method
hybridization	O
While	O
the	O
rigid	O
and	O
modular	O
design	O
of	O
SMT	S-Method
provides	O
a	O
very	O
suitable	O
framework	O
for	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
,	O
NMT	S-Method
has	O
shown	O
to	O
be	O
a	O
fairly	O
superior	O
paradigm	O
in	O
supervised	B-Task
settings	E-Task
,	O
outperforming	O
SMT	S-Method
by	O
a	O
large	O
margin	O
in	O
standard	O
benchmarks	O
.	O

As	O
such	O
,	O
the	O
choice	O
of	O
SMT	S-Method
over	O
NMT	S-Method
also	O
imposes	O
a	O
hard	O
ceiling	O
on	O
the	O
potential	O
performance	O
of	O
these	O
approaches	O
,	O
as	O
unsupervised	O
SMT	S-Method
systems	O
inherit	O
the	O
very	O
same	O
limitations	O
of	O
their	O
supervised	B-Method
counterparts	E-Method
(	O
e.g.	O
the	O
locality	B-Task
and	I-Task
sparsity	I-Task
problems	E-Task
)	O
.	O

For	O
that	O
reason	O
,	O
we	O
argue	O
that	O
SMT	S-Method
provides	O
a	O
more	O
appropriate	O
architecture	O
to	O
find	O
an	O
initial	O
alignment	O
between	O
the	O
languages	O
,	O
but	O
NMT	S-Method
is	O
ultimately	O
a	O
better	O
architecture	O
to	O
model	O
the	O
translation	B-Task
process	E-Task
.	O

Following	O
this	O
observation	O
,	O
we	O
propose	O
a	O
hybrid	B-Method
approach	E-Method
that	O
uses	O
unsupervised	B-Task
SMT	E-Task
to	O
warm	O
up	O
a	O
dual	O
NMT	S-Method
model	O
trained	O
through	O
iterative	B-Method
back	I-Method
-	I-Method
translation	E-Method
.	O

More	O
concretely	O
,	O
we	O
first	O
train	O
two	O
SMT	S-Method
systems	O
in	O
opposite	O
directions	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
use	O
them	O
to	O
assist	O
the	O
training	O
of	O
another	O
two	O
NMT	S-Method
systems	O
in	O
opposite	O
directions	O
.	O

These	O
NMT	S-Method
systems	O
are	O
trained	O
following	O
an	O
iterative	O
process	O
where	O
,	O
at	O
each	O
iteration	O
,	O
we	O
alternately	O
update	O
the	O
model	O
in	O
each	O
direction	O
by	O
performing	O
a	O
single	O
pass	O
over	O
a	O
synthetic	O
parallel	O
corpus	O
built	O
through	O
back	B-Method
-	I-Method
translation	E-Method
sennrich2016improving	O
.	O

In	O
the	O
first	O
iteration	O
,	O
the	O
synthetic	O
parallel	O
corpus	O
is	O
entirely	O
generated	O
by	O
the	O
SMT	S-Method
system	O
in	O
the	O
opposite	O
direction	O
but	O
,	O
as	O
training	O
progresses	O
and	O
the	O
NMT	S-Method
models	O
get	O
better	O
,	O
we	O
progressively	O
switch	O
to	O
a	O
synthetic	O
parallel	O
corpus	O
generated	O
by	O
the	O
reverse	O
NMT	S-Method
model	O
.	O

More	O
concretely	O
,	O
iteration	S-Method
uses	O
synthetic	O
parallel	O
sentences	O
from	O
the	O
reverse	O
SMT	S-Method
system	O
,	O
where	O
the	O
parameter	O
controls	O
the	O
number	O
of	O
transition	O
iterations	O
from	O
SMT	S-Method
to	O
NMT	B-Method
back	I-Method
-	I-Method
translation	E-Method
.	O

The	O
remaining	O
sentences	O
are	O
generated	O
by	O
the	O
reverse	O
NMT	S-Method
model	O
.	O

Inspired	O
by	O
edunov2018understanding	O
,	O
we	O
use	O
greedy	B-Method
decoding	E-Method
for	O
half	O
of	O
them	O
,	O
which	O
produces	O
more	O
fluent	O
and	O
predictable	O
translations	O
,	O
and	O
random	B-Method
sampling	E-Method
for	O
the	O
other	O
half	O
,	O
which	O
produces	O
more	O
varied	O
translations	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
and	O
,	O
and	O
perform	O
a	O
total	O
of	O
60	O
such	O
iterations	O
.	O

At	O
test	O
time	O
,	O
we	O
use	O
beam	B-Method
search	I-Method
decoding	E-Method
with	O
an	O
ensemble	O
of	O
all	O
checkpoints	O
from	O
every	O
10	O
iterations	O
.	O

section	O
:	O
Experiments	O
and	O
results	O
SMT	S-Method
+	O
NMT	S-Method
In	O
order	O
to	O
make	O
our	O
experiments	O
comparable	O
to	O
previous	O
work	O
,	O
we	O
use	O
the	O
French	B-Material
-	I-Material
English	E-Material
and	O
German	B-Material
-	I-Material
English	E-Material
datasets	O
from	O
the	O
WMT	B-Material
2014	I-Material
shared	I-Material
task	E-Material
.	O

More	O
concretely	O
,	O
our	O
training	O
data	O
consists	O
of	O
the	O
concatenation	O
of	O
all	O
News	B-Material
Crawl	I-Material
monolingual	I-Material
corpora	E-Material
from	O
2007	O
to	O
2013	O
,	O
which	O
make	O
a	O
total	O
of	O
749	O
million	O
tokens	O
in	O
French	O
,	O
1	O
,	O
606	O
millions	O
in	O
German	S-Material
,	O
and	O
2	O
,	O
109	O
millions	O
in	O
English	S-Material
,	O
from	O
which	O
we	O
take	O
a	O
random	O
subset	O
of	O
2	O
,	O
000	O
sentences	O
for	O
tuning	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Preprocessing	S-Task
is	O
done	O
using	O
standard	O
Moses	S-Method
tools	O
,	O
and	O
involves	O
punctuation	B-Method
normalization	E-Method
,	O
tokenization	S-Method
with	O
aggressive	B-Method
hyphen	I-Method
splitting	E-Method
,	O
and	O
truecasing	S-Method
.	O

Our	O
SMT	S-Method
implementation	O
is	O
based	O
on	O
Moses	S-Method
,	O
and	O
we	O
use	O
the	O
KenLM	S-Method
heafield2013scalable	O
tool	O
included	O
in	O
it	O
to	O
estimate	O
our	O
5	B-Method
-	I-Method
gram	I-Method
language	I-Method
model	E-Method
with	O
modified	O
Kneser	B-Method
-	I-Method
Ney	I-Method
smoothing	E-Method
.	O

Our	O
unsupervised	B-Method
tuning	I-Method
implementation	E-Method
is	O
based	O
on	O
Z	O
-	O
MERT	S-Metric
zaidan2009zmert	O
,	O
and	O
we	O
use	O
FastAlign	S-Method
dyer2013simple	O
for	O
word	B-Task
alignment	E-Task
within	O
the	O
joint	B-Method
refinement	I-Method
procedure	E-Method
.	O

Finally	O
,	O
we	O
use	O
the	O
big	B-Method
transformer	I-Method
implementation	E-Method
from	O
fairseq	S-Method
for	O
our	O
NMT	S-Method
system	O
,	O
training	O
with	O
a	O
total	O
batch	O
size	O
of	O
20	O
,	O
000	O
tokens	O
across	O
8	O
GPUs	O
with	O
the	O
exact	O
same	O
hyperparameters	O
as	O
ott2018scaling	O
.	O

We	O
use	O
newstest2014	O
as	O
our	O
test	O
set	O
for	O
French	B-Material
-	I-Material
English	E-Material
,	O
and	O
both	O
newstest2014	O
and	O
newstest2016	O
(	O
from	O
WMT	B-Material
2016	E-Material
)	O
for	O
German	B-Material
-	I-Material
English	E-Material
.	O

Following	O
common	O
practice	O
,	O
we	O
report	O
tokenized	B-Metric
BLEU	I-Metric
scores	E-Metric
as	O
computed	O
by	O
the	O
multi	B-Method
-	I-Method
bleu.perl	I-Method
script	E-Method
included	O
in	O
Moses	S-Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
also	O
report	O
detokenized	B-Metric
BLEU	I-Metric
scores	E-Metric
as	O
computed	O
by	O
SacreBLEU	O
post2018call	O
,	O
which	O
is	O
equivalent	O
to	O
the	O
official	O
mteval	O
-	O
v13a.pl	O
script	O
.	O

We	O
next	O
present	O
the	O
results	O
of	O
our	O
proposed	O
system	O
in	O
comparison	O
to	O
previous	O
work	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Section	O
[	O
reference	O
]	O
then	O
compares	O
the	O
obtained	O
results	O
to	O
those	O
of	O
different	O
supervised	B-Method
systems	E-Method
.	O

Finally	O
,	O
Section	O
[	O
reference	O
]	O
presents	O
some	O
translation	O
examples	O
from	O
our	O
system	O
.	O

subsection	O
:	O
Main	O
results	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
the	O
proposed	O
system	O
in	O
comparison	O
to	O
previous	O
work	O
.	O

As	O
it	O
can	O
be	O
seen	O
,	O
our	O
full	O
system	O
obtains	O
the	O
best	O
published	O
results	O
in	O
all	O
cases	O
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
5	O
-	O
7	O
BLEU	B-Metric
points	E-Metric
in	O
all	O
datasets	O
and	O
translation	O
directions	O
.	O

A	O
substantial	O
part	O
of	O
this	O
improvement	O
comes	O
from	O
our	O
more	O
principled	O
unsupervised	O
SMT	S-Method
approach	O
,	O
which	O
outperforms	O
all	O
previous	O
SMT	S-Method
-	O
based	O
systems	O
by	O
around	O
2	O
BLEU	B-Metric
points	E-Metric
.	O

Nevertheless	O
,	O
it	O
is	O
the	O
NMT	S-Method
hybridization	O
that	O
brings	O
the	O
largest	O
gains	O
,	O
improving	O
the	O
results	O
of	O
this	O
initial	O
SMT	S-Method
systems	O
by	O
5	O
-	O
9	O
BLEU	B-Metric
points	E-Metric
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
our	O
absolute	O
gains	O
are	O
considerably	O
larger	O
than	O
those	O
of	O
previous	O
hybridization	B-Method
methods	E-Method
,	O
even	O
if	O
our	O
initial	O
SMT	S-Method
system	O
is	O
substantially	O
better	O
and	O
thus	O
more	O
difficult	O
to	O
improve	O
upon	O
.	O

This	O
way	O
,	O
our	O
initial	O
SMT	S-Method
system	O
is	O
about	O
4	O
-	O
5	O
BLEU	B-Metric
points	E-Metric
above	O
that	O
of	O
marie2018unsupervised	O
,	O
yet	O
our	O
absolute	O
gain	O
on	O
top	O
of	O
it	O
is	O
around	O
2.5	O
BLEU	B-Metric
points	E-Metric
higher	O
.	O

When	O
compared	O
to	O
lample2018phrase	O
,	O
we	O
obtain	O
an	O
absolute	O
gain	O
of	O
5	O
-	O
6	O
BLEU	B-Metric
points	E-Metric
in	O
both	O
French	B-Material
-	I-Material
English	E-Material
directions	O
while	O
they	O
do	O
not	O
get	O
any	O
clear	O
improvement	O
,	O
and	O
we	O
obtain	O
an	O
improvement	O
of	O
7	O
-	O
9	O
BLEU	B-Metric
points	E-Metric
in	O
both	O
German	B-Material
-	I-Material
English	E-Material
directions	O
,	O
in	O
contrast	O
with	O
the	O
2.3	O
BLEU	B-Metric
points	E-Metric
they	O
obtain	O
.	O

More	O
generally	O
,	O
it	O
is	O
interesting	O
that	O
pure	O
SMT	S-Method
systems	O
perform	O
better	O
than	O
pure	O
NMT	S-Method
systems	O
,	O
yet	O
the	O
best	O
results	O
are	O
obtained	O
by	O
initializing	O
an	O
NMT	S-Method
system	O
with	O
an	O
SMT	S-Method
system	O
.	O

This	O
suggests	O
that	O
the	O
rigid	B-Method
and	I-Method
modular	I-Method
architecture	E-Method
of	O
SMT	S-Method
might	O
be	O
more	O
suitable	O
to	O
find	O
an	O
initial	O
alignment	O
between	O
the	O
languages	O
,	O
but	O
the	O
final	O
system	O
should	O
be	O
ultimately	O
based	O
on	O
NMT	S-Method
for	O
optimal	O
results	O
.	O

subsection	O
:	O
Comparison	O
with	O
supervised	B-Method
systems	E-Method
So	O
as	O
to	O
put	O
our	O
results	O
into	O
perspective	O
,	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
of	O
different	O
supervised	B-Method
systems	E-Method
in	O
the	O
same	O
WMT	B-Material
2014	I-Material
test	I-Material
set	E-Material
.	O

More	O
concretely	O
,	O
we	O
include	O
the	O
best	O
results	O
from	O
the	O
shared	O
task	O
itself	O
,	O
which	O
reflect	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
machine	B-Task
translation	E-Task
back	O
in	O
2014	O
;	O
those	O
of	O
vaswani2017attention	O
,	O
who	O
introduced	O
the	O
now	O
predominant	O
transformer	B-Method
architecture	E-Method
;	O
and	O
those	O
of	O
edunov2018understanding	S-Method
,	O
who	O
apply	O
back	B-Method
-	I-Method
translation	E-Method
at	O
a	O
large	O
scale	O
and	O
hold	O
the	O
current	O
best	O
results	O
in	O
the	O
test	O
set	O
.	O

As	O
it	O
can	O
be	O
seen	O
,	O
our	O
unsupervised	B-Method
system	E-Method
outperforms	O
the	O
WMT	S-Material
2014	O
shared	O
task	O
winner	O
in	O
English	B-Material
-	I-Material
to	I-Material
-	I-Material
German	E-Material
,	O
and	O
is	O
around	O
2	O
BLEU	B-Metric
points	E-Metric
behind	O
it	O
in	O
the	O
other	O
translation	O
directions	O
.	O

This	O
shows	O
that	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
is	O
already	O
competitive	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
supervised	B-Task
machine	I-Task
translation	E-Task
in	O
2014	O
.	O

While	O
the	O
field	O
of	O
machine	B-Task
translation	E-Task
has	O
undergone	O
great	O
progress	O
in	O
the	O
last	O
5	O
years	O
,	O
and	O
the	O
gap	O
between	O
our	O
unsupervised	B-Method
system	E-Method
and	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
supervised	B-Task
machine	I-Task
translation	E-Task
is	O
still	O
large	O
as	O
reflected	O
by	O
the	O
other	O
results	O
,	O
this	O
suggests	O
that	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

subsection	O
:	O
Qualitative	O
results	O
Table	O
[	O
reference	O
]	O
shows	O
some	O
translation	O
examples	O
from	O
our	O
proposed	O
system	O
in	O
comparison	O
to	O
those	O
reported	O
by	O
artetxe2018usmt	O
.	O

We	O
choose	O
the	O
exact	O
same	O
sentences	O
reported	O
by	O
artetxe2018usmt	O
,	O
which	O
were	O
randomly	O
taken	O
from	O
newstest2014	O
,	O
so	O
they	O
should	O
be	O
representative	O
of	O
the	O
general	O
behavior	O
of	O
both	O
systems	O
.	O

While	O
not	O
perfect	O
,	O
our	O
proposed	O
system	O
produces	O
generally	O
fluent	O
translations	O
that	O
accurately	O
capture	O
the	O
meaning	O
of	O
the	O
original	O
text	O
.	O

Just	O
in	O
line	O
with	O
our	O
quantitative	O
results	O
,	O
this	O
suggests	O
that	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
can	O
be	O
a	O
usable	O
alternative	O
in	O
practical	O
settings	O
.	O

Compared	O
to	O
artetxe2018usmt	O
,	O
our	O
translations	O
are	O
generally	O
more	O
fluent	O
,	O
which	O
is	O
not	O
surprising	O
given	O
that	O
they	O
are	O
produced	O
by	O
an	O
NMT	S-Method
system	O
rather	O
than	O
an	O
SMT	S-Method
system	O
.	O

In	O
addition	O
to	O
that	O
,	O
the	O
system	O
of	O
artetxe2018usmt	O
has	O
some	O
adequacy	O
issues	O
when	O
translating	O
named	O
entities	O
and	O
numerals	O
(	O
e.g.	O
34	O
32	O
,	O
Sunday	O
Telegraph	O
The	O
Times	O
of	O
London	O
)	O
,	O
which	O
we	O
do	O
not	O
observe	O
for	O
our	O
proposed	O
system	O
in	O
these	O
examples	O
.	O

section	O
:	O
Conclusions	O
and	O
future	O
work	O
In	O
this	O
paper	O
,	O
we	O
identify	O
several	O
deficiencies	O
in	O
previous	O
unsupervised	O
SMT	S-Method
systems	O
,	O
and	O
propose	O
a	O
more	O
principled	O
approach	O
that	O
addresses	O
them	O
by	O
incorporating	O
subword	O
information	O
,	O
using	O
a	O
theoretically	O
well	O
founded	O
unsupervised	B-Method
tuning	I-Method
method	E-Method
,	O
and	O
developing	O
a	O
joint	B-Method
refinement	I-Method
procedure	E-Method
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
use	O
our	O
improved	O
SMT	S-Method
approach	O
to	O
initialize	O
a	O
dual	O
NMT	S-Method
model	O
that	O
is	O
further	O
improved	O
through	O
on	O
-	O
the	O
-	O
fly	O
back	B-Method
-	I-Method
translation	E-Method
.	O

Our	O
experiments	O
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
as	O
we	O
improve	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
unsupervised	B-Task
machine	I-Task
translation	E-Task
by	O
5	O
-	O
7	O
BLEU	B-Metric
points	E-Metric
in	O
French	B-Material
-	I-Material
English	E-Material
and	O
German	B-Material
-	I-Material
English	I-Material
WMT	I-Material
2014	E-Material
and	O
2016	S-Material
.	O

In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
explore	O
learnable	O
similarity	B-Method
functions	E-Method
like	O
the	O
one	O
proposed	O
by	O
mccallum2005conditional	O
to	O
compute	O
the	O
character	O
-	O
level	O
scores	O
in	O
our	O
initial	O
phrase	O
-	O
table	O
.	O

In	O
addition	O
to	O
that	O
,	O
we	O
would	O
like	O
to	O
incorporate	O
a	O
language	B-Method
modeling	I-Method
loss	E-Method
during	O
NMT	S-Method
training	O
similar	O
to	O
he2016dual	O
.	O

Finally	O
,	O
we	O
would	O
like	O
to	O
adapt	O
our	O
approach	O
to	O
more	O
relaxed	O
scenarios	O
with	O
multiple	O
languages	O
and	O
/	O
or	O
small	O
parallel	O
corpora	O
.	O

section	O
:	O
Acknowledgments	O
This	O
research	O
was	O
partially	O
supported	O
by	O
the	O
Spanish	O
MINECO	O
(	O
UnsupNMT	O
TIN2017‐91692‐EXP	O
,	O
cofunded	O
by	O
EU	O
FEDER	O
)	O
,	O
the	O
UPV	O
/	O
EHU	O
(	O
excellence	O
research	O
group	O
)	O
,	O
and	O
the	O
NVIDIA	O
GPU	O
grant	O
program	O
.	O

Mikel	O
Artetxe	O
enjoys	O
a	O
doctoral	O
grant	O
from	O
the	O
Spanish	O
MECD	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Frustum	B-Method
PointNets	E-Method
for	O
3D	B-Task
Object	I-Task
Detection	E-Task
from	O
RGB	B-Material
-	I-Material
D	I-Material
Data	E-Material
In	O
this	O
work	O
,	O
we	O
study	O
3D	O
object	O
detection	S-Task
from	O
RGB	B-Material
-	I-Material
D	I-Material
data	E-Material
in	O
both	O
indoor	O
and	O
outdoor	O
scenes	O
.	O

While	O
previous	O
methods	O
focus	O
on	O
images	O
or	O
3D	O
voxels	O
,	O
often	O
obscuring	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
3D	O
data	O
,	O
we	O
directly	O
operate	O
on	O
raw	O
point	O
clouds	O
by	O
popping	O
up	O
RGB	B-Material
-	I-Material
D	I-Material
scans	E-Material
.	O

However	O
,	O
a	O
key	O
challenge	O
of	O
this	O
approach	O
is	O
how	O
to	O
efficiently	O
localize	B-Task
objects	E-Task
in	O
point	B-Task
clouds	I-Task
of	I-Task
large	I-Task
-	I-Task
scale	I-Task
scenes	E-Task
(	O
region	B-Task
proposal	E-Task
)	O
.	O

Instead	O
of	O
solely	O
relying	O
on	O
3D	O
proposals	O
,	O
our	O
method	O
leverages	O
both	O
mature	O
2D	B-Method
object	I-Method
detectors	E-Method
and	O
advanced	O
3D	B-Method
deep	I-Method
learning	E-Method
for	O
object	B-Task
localization	E-Task
,	O
achieving	O
efficiency	O
as	O
well	O
as	O
high	O
recall	S-Metric
for	O
even	O
small	O
objects	O
.	O

Benefited	O
from	O
learning	O
directly	O
in	O
raw	B-Task
point	I-Task
clouds	E-Task
,	O
our	O
method	O
is	O
also	O
able	O
to	O
precisely	O
estimate	O
3D	B-Task
bounding	I-Task
boxes	E-Task
even	O
under	O
strong	O
occlusion	O
or	O
with	O
very	O
sparse	O
points	O
.	O

Evaluated	O
on	O
KITTI	S-Material
and	O
SUN	O
RGB	O
-	O
D	O
3D	B-Task
detection	E-Task
benchmarks	O
,	O
our	O
method	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
remarkable	O
margins	O
while	O
having	O
real	B-Metric
-	I-Metric
time	I-Metric
capability	E-Metric
.	O

section	O
:	O
Introduction	O
Recently	O
,	O
great	O
progress	O
has	O
been	O
made	O
on	O
2D	B-Task
image	I-Task
understanding	I-Task
tasks	E-Task
,	O
such	O
as	O
object	O
detection	S-Task
and	O
instance	B-Task
segmentation	E-Task
.	O

However	O
,	O
beyond	O
getting	O
2D	O
bounding	O
boxes	O
or	O
pixel	O
masks	O
,	O
3D	B-Task
understanding	E-Task
is	O
eagerly	O
in	O
demand	O
in	O
many	O
applications	O
such	O
as	O
autonomous	B-Task
driving	E-Task
and	O
augmented	B-Task
reality	E-Task
(	O
AR	S-Task
)	O
.	O

With	O
the	O
popularity	O
of	O
3D	B-Method
sensors	E-Method
deployed	O
on	O
mobile	O
devices	O
and	O
autonomous	O
vehicles	O
,	O
more	O
and	O
more	O
3D	O
data	O
is	O
captured	O
and	O
processed	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
one	O
of	O
the	O
most	O
important	O
3D	B-Task
perception	I-Task
tasks	E-Task
–	O
3D	O
object	O
detection	S-Task
,	O
which	O
classifies	O
the	O
object	B-Task
category	E-Task
and	O
estimates	O
oriented	B-Task
3D	I-Task
bounding	I-Task
boxes	I-Task
of	I-Task
physical	I-Task
objects	E-Task
from	O
3D	O
sensor	O
data	O
.	O

While	O
3D	O
sensor	O
data	O
is	O
often	O
in	O
the	O
form	O
of	O
point	O
clouds	O
,	O
how	O
to	O
represent	O
point	O
cloud	O
and	O
what	O
deep	B-Method
net	I-Method
architectures	E-Method
to	O
use	O
for	O
3D	O
object	O
detection	S-Task
remains	O
an	O
open	O
problem	O
.	O

Most	O
existing	O
works	O
convert	O
3D	O
point	O
clouds	O
to	O
images	O
by	O
projection	O
or	O
to	O
volumetric	O
grids	O
by	O
quantization	O
and	O
then	O
apply	O
convolutional	B-Method
networks	E-Method
.	O

This	O
data	B-Method
representation	I-Method
transformation	E-Method
,	O
however	O
,	O
may	O
obscure	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
the	O
data	O
.	O

Recently	O
,	O
a	O
number	O
of	O
papers	O
have	O
proposed	O
to	O
process	O
point	B-Task
clouds	E-Task
directly	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

For	O
example	O
,	O
proposed	O
new	O
types	O
of	O
deep	B-Method
net	I-Method
architectures	E-Method
,	O
called	O
PointNets	S-Method
,	O
which	O
have	O
shown	O
superior	O
performance	O
and	O
efficiency	O
in	O
several	O
3D	B-Task
understanding	I-Task
tasks	E-Task
such	O
as	O
object	B-Task
classification	E-Task
and	O
semantic	B-Task
segmentation	E-Task
.	O

While	O
PointNets	S-Method
are	O
capable	O
of	O
classifying	O
a	O
whole	B-Task
point	I-Task
cloud	E-Task
or	O
predicting	O
a	O
semantic	O
class	O
for	O
each	O
point	O
in	O
a	O
point	O
cloud	O
,	O
it	O
is	O
unclear	O
how	O
this	O
architecture	O
can	O
be	O
used	O
for	O
instance	O
-	O
level	O
3D	O
object	O
detection	S-Task
.	O

Towards	O
this	O
goal	O
,	O
we	O
have	O
to	O
address	O
one	O
key	O
challenge	O
:	O
how	O
to	O
efficiently	O
propose	O
possible	O
locations	O
of	O
3D	O
objects	O
in	O
a	O
3D	O
space	O
.	O

Imitating	O
the	O
practice	O
in	O
image	B-Task
detection	E-Task
,	O
it	O
is	O
straightforward	O
to	O
enumerate	O
candidate	O
3D	O
boxes	O
by	O
sliding	O
windows	O
or	O
by	O
3D	B-Method
region	I-Method
proposal	I-Method
networks	E-Method
such	O
as	O
.	O

However	O
,	O
the	O
computational	B-Metric
complexity	E-Metric
of	O
3D	B-Task
search	E-Task
typically	O
grows	O
cubically	O
with	O
respect	O
to	O
resolution	O
and	O
becomes	O
too	O
expensive	O
for	O
large	B-Task
scenes	E-Task
or	O
real	B-Task
-	I-Task
time	I-Task
applications	E-Task
such	O
as	O
autonomous	B-Task
driving	E-Task
.	O

Instead	O
,	O
in	O
this	O
work	O
,	O
we	O
reduce	O
the	O
search	O
space	O
following	O
the	O
dimension	B-Method
reduction	I-Method
principle	E-Method
:	O
we	O
take	O
the	O
advantage	O
of	O
mature	O
2D	B-Method
object	I-Method
detectors	E-Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

First	O
,	O
we	O
extract	O
the	O
3D	O
bounding	O
frustum	O
of	O
an	O
object	O
by	O
extruding	O
2D	O
bounding	O
boxes	O
from	O
image	B-Method
detectors	E-Method
.	O

Then	O
,	O
within	O
the	O
3D	O
space	O
trimmed	O
by	O
each	O
of	O
the	O
3D	O
frustums	O
,	O
we	O
consecutively	O
perform	O
3D	B-Method
object	I-Method
instance	I-Method
segmentation	E-Method
and	O
amodal	O
3D	B-Method
bounding	I-Method
box	I-Method
regression	E-Method
using	O
two	O
variants	O
of	O
PointNet	S-Method
.	O

The	O
segmentation	B-Method
network	E-Method
predicts	O
the	O
3D	O
mask	O
of	O
the	O
object	O
of	O
interest	O
(	O
i.e.	O
instance	B-Task
segmentation	E-Task
)	O
;	O
and	O
the	O
regression	B-Method
network	E-Method
estimates	O
the	O
amodal	O
3D	O
bounding	O
box	O
(	O
covering	O
the	O
entire	O
object	O
even	O
if	O
only	O
part	O
of	O
it	O
is	O
visible	O
)	O
.	O

In	O
contrast	O
to	O
previous	O
work	O
that	O
treats	O
RGB	B-Material
-	I-Material
D	I-Material
data	E-Material
as	O
2D	O
maps	O
for	O
CNNs	S-Method
,	O
our	O
method	O
is	O
more	O
3D	O
-	O
centric	O
as	O
we	O
lift	O
depth	O
maps	O
to	O
3D	O
point	O
clouds	O
and	O
process	O
them	O
using	O
3D	B-Method
tools	E-Method
.	O

This	O
3D	B-Method
-	I-Method
centric	I-Method
view	E-Method
enables	O
new	O
capabilities	O
for	O
exploring	O
3D	B-Task
data	E-Task
in	O
a	O
more	O
effective	O
manner	O
.	O

First	O
,	O
in	O
our	O
pipeline	O
,	O
a	O
few	O
transformations	O
are	O
applied	O
successively	O
on	O
3D	O
coordinates	O
,	O
which	O
align	O
point	O
clouds	O
into	O
a	O
sequence	O
of	O
more	O
constrained	O
and	O
canonical	O
frames	O
.	O

These	O
alignments	O
factor	O
out	O
pose	O
variations	O
in	O
data	O
,	O
and	O
thus	O
make	O
3D	O
geometry	O
pattern	O
more	O
evident	O
,	O
leading	O
to	O
an	O
easier	O
job	O
of	O
3D	B-Task
learners	E-Task
.	O

Second	O
,	O
learning	B-Task
in	I-Task
3D	I-Task
space	E-Task
can	O
better	O
exploits	O
the	O
geometric	O
and	O
topological	O
structure	O
of	O
3D	O
space	O
.	O

In	O
principle	O
,	O
all	O
objects	O
live	O
in	O
3D	O
space	O
;	O
therefore	O
,	O
we	O
believe	O
that	O
many	O
geometric	O
structures	O
,	O
such	O
as	O
repetition	O
,	O
planarity	O
,	O
and	O
symmetry	O
,	O
are	O
more	O
naturally	O
parameterized	O
and	O
captured	O
by	O
learners	S-Method
that	O
directly	O
operate	O
in	O
3D	O
space	O
.	O

The	O
usefulness	O
of	O
this	O
3D	B-Method
-	I-Method
centric	I-Method
network	I-Method
design	I-Method
philosophy	E-Method
has	O
been	O
supported	O
by	O
much	O
recent	O
experimental	O
evidence	O
.	O

Our	O
method	O
achieve	O
leading	O
positions	O
on	O
KITTI	S-Material
3D	O
object	O
detection	S-Task
and	O
bird	O
’s	O
eye	O
view	O
detection	S-Task
benchmarks	O
.	O

Compared	O
with	O
the	O
previous	O
state	O
of	O
the	O
art	O
,	O
our	O
method	O
is	O
8.04	O
%	O
better	O
on	O
3D	B-Metric
car	I-Metric
AP	E-Metric
with	O
high	O
efficiency	O
(	O
running	O
at	O
5	O
fps	S-Metric
)	O
.	O

Our	O
method	O
also	O
fits	O
well	O
to	O
indoor	B-Material
RGB	I-Material
-	I-Material
D	I-Material
data	E-Material
where	O
we	O
have	O
achieved	O
8.9	O
%	O
and	O
6.4	O
%	O
better	O
3D	B-Metric
mAP	E-Metric
than	O
and	O
on	O
SUN	B-Material
-	I-Material
RGBD	E-Material
while	O
running	O
one	O
to	O
three	O
orders	O
of	O
magnitude	O
faster	O
.	O

The	O
key	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
framework	O
for	O
RGB	O
-	O
D	O
data	O
based	O
3D	O
object	O
detection	S-Task
called	O
Frustum	B-Method
PointNets	E-Method
.	O

We	O
show	O
how	O
we	O
can	O
train	O
3D	B-Method
object	I-Method
detectors	E-Method
under	O
our	O
framework	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
standard	O
3D	O
object	O
detection	S-Task
benchmarks	O
.	O

We	O
provide	O
extensive	O
quantitative	O
evaluations	O
to	O
validate	O
our	O
design	O
choices	O
as	O
well	O
as	O
rich	O
qualitative	O
results	O
for	O
understanding	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
method	O
.	O

section	O
:	O
Related	O
Work	O
paragraph	O
:	O
3D	B-Task
Object	I-Task
Detection	E-Task
from	O
RGB	B-Material
-	I-Material
D	I-Material
Data	E-Material
Researchers	O
have	O
approached	O
the	O
3D	B-Task
detection	E-Task
problem	O
by	O
taking	O
various	O
ways	O
to	O
represent	O
RGB	B-Material
-	I-Material
D	I-Material
data	E-Material
.	O

Front	B-Method
view	I-Method
image	I-Method
based	I-Method
methods	E-Method
:	O
take	O
monocular	B-Material
RGB	I-Material
images	E-Material
and	O
shape	O
priors	O
or	O
occlusion	O
patterns	O
to	O
infer	O
3D	O
bounding	O
boxes	O
.	O

represent	O
depth	O
data	O
as	O
2D	O
maps	O
and	O
apply	O
CNNs	S-Method
to	O
localize	B-Task
objects	E-Task
in	O
2D	O
image	O
.	O

In	O
comparison	O
we	O
represent	O
depth	O
as	O
a	O
point	O
cloud	O
and	O
use	O
advanced	O
3D	B-Method
deep	I-Method
networks	E-Method
(	O
PointNets	S-Method
)	O
that	O
can	O
exploit	O
3D	O
geometry	O
more	O
effectively	O
.	O

Bird	B-Method
’s	I-Method
eye	I-Method
view	I-Method
based	I-Method
methods	E-Method
:	O
MV3D	S-Method
projects	O
LiDAR	B-Method
point	I-Method
cloud	E-Method
to	O
bird	O
’s	O
eye	O
view	O
and	O
trains	O
a	O
region	B-Method
proposal	I-Method
network	E-Method
(	O
RPN	S-Method
)	O
for	O
3D	B-Task
bounding	I-Task
box	I-Task
proposal	E-Task
.	O

However	O
,	O
the	O
method	O
lags	O
behind	O
in	O
detecting	B-Task
small	I-Task
objects	E-Task
,	O
such	O
as	O
pedestrians	O
and	O
cyclists	O
and	O
can	O
not	O
easily	O
adapt	O
to	O
scenes	O
with	O
multiple	O
objects	O
in	O
vertical	O
direction	O
.	O

3D	B-Method
based	I-Method
methods	E-Method
:	O
train	O
3D	B-Method
object	I-Method
classifiers	E-Method
by	O
SVMs	S-Method
on	O
hand	O
-	O
designed	O
geometry	O
features	O
extracted	O
from	O
point	O
cloud	O
and	O
then	O
localize	O
objects	O
using	O
sliding	B-Method
-	I-Method
window	I-Method
search	E-Method
.	O

extends	O
by	O
replacing	O
SVM	S-Method
with	O
3D	B-Method
CNN	E-Method
on	O
voxelized	O
3D	O
grids	O
.	O

designs	O
new	O
geometric	O
features	O
for	O
3D	O
object	O
detection	S-Task
in	O
a	O
point	O
cloud	O
.	O

convert	O
a	O
point	O
cloud	O
of	O
the	O
entire	O
scene	O
into	O
a	O
volumetric	O
grid	O
and	O
use	O
3D	B-Method
volumetric	I-Method
CNN	E-Method
for	O
object	B-Task
proposal	I-Task
and	I-Task
classification	E-Task
.	O

Computation	B-Metric
cost	E-Metric
for	O
those	O
method	O
is	O
usually	O
quite	O
high	O
due	O
to	O
the	O
expensive	O
cost	O
of	O
3D	O
convolutions	O
and	O
large	O
3D	O
search	O
space	O
.	O

Recently	O
,	O
proposes	O
a	O
2D	O
-	O
driven	O
3D	O
object	O
detection	S-Task
method	O
that	O
is	O
similar	O
to	O
ours	O
in	O
spirit	O
.	O

However	O
,	O
they	O
use	O
hand	O
-	O
crafted	O
features	O
(	O
based	O
on	O
histogram	B-Method
of	I-Method
point	I-Method
coordinates	E-Method
)	O
with	O
simple	O
fully	B-Method
connected	I-Method
networks	E-Method
to	O
regress	O
3D	B-Task
box	I-Task
location	E-Task
and	O
pose	O
,	O
which	O
is	O
sub	O
-	O
optimal	O
in	O
both	O
speed	S-Metric
and	O
performance	O
.	O

In	O
contrast	O
,	O
we	O
propose	O
a	O
more	O
flexible	O
and	O
effective	O
solution	O
with	O
deep	B-Method
3D	I-Method
feature	I-Method
learning	E-Method
(	O
PointNets	S-Method
)	O
.	O

paragraph	O
:	O
Deep	B-Task
Learning	E-Task
on	O
Point	B-Task
Clouds	E-Task
Most	O
existing	O
works	O
convert	O
point	O
clouds	O
to	O
images	O
or	O
volumetric	O
forms	O
before	O
feature	B-Method
learning	E-Method
.	O

voxelize	B-Task
point	I-Task
clouds	E-Task
into	O
volumetric	O
grids	O
and	O
generalize	O
image	B-Method
CNNs	E-Method
to	O
3D	B-Method
CNNs	E-Method
.	O

design	O
more	O
efficient	O
3D	B-Method
CNN	I-Method
or	I-Method
neural	I-Method
network	I-Method
architectures	E-Method
that	O
exploit	O
sparsity	O
in	O
point	O
cloud	O
.	O

However	O
,	O
these	O
CNN	B-Method
based	I-Method
methods	E-Method
still	O
require	O
quantitization	B-Task
of	I-Task
point	I-Task
clouds	E-Task
with	O
certain	O
voxel	O
resolution	O
.	O

Recently	O
,	O
a	O
few	O
works	O
propose	O
a	O
novel	O
type	O
of	O
network	B-Method
architectures	E-Method
(	O
PointNets	S-Method
)	O
that	O
directly	O
consumes	O
raw	O
point	O
clouds	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

While	O
PointNets	S-Method
have	O
been	O
applied	O
to	O
single	B-Task
object	I-Task
classification	E-Task
and	O
semantic	B-Task
segmentation	E-Task
,	O
our	O
work	O
explores	O
how	O
to	O
extend	O
the	O
architecture	O
for	O
the	O
purpose	O
of	O
3D	O
object	O
detection	S-Task
.	O

section	O
:	O
Problem	O
Definition	O
Given	O
RGB	B-Material
-	I-Material
D	I-Material
data	E-Material
as	O
input	O
,	O
our	O
goal	O
is	O
to	O
classify	O
and	O
localize	B-Task
objects	I-Task
in	I-Task
3D	I-Task
space	E-Task
.	O

The	O
depth	O
data	O
,	O
obtained	O
from	O
LiDAR	O
or	O
indoor	O
depth	O
sensors	O
,	O
is	O
represented	O
as	O
a	O
point	O
cloud	O
in	O
RGB	O
camera	O
coordinates	O
.	O

The	O
projection	O
matrix	O
is	O
also	O
known	O
so	O
that	O
we	O
can	O
get	O
a	O
3D	O
frustum	O
from	O
a	O
2D	O
image	O
region	O
.	O

Each	O
object	O
is	O
represented	O
by	O
a	O
class	O
(	O
one	O
among	O
predefined	O
classes	O
)	O
and	O
an	O
amodal	O
3D	O
bounding	O
box	O
.	O

The	O
amodal	O
box	O
bounds	O
the	O
complete	O
object	O
even	O
if	O
part	O
of	O
the	O
object	O
is	O
occluded	O
or	O
truncated	O
.	O

The	O
3D	O
box	O
is	O
parameterized	O
by	O
its	O
size	O
,	O
center	O
,	O
and	O
orientation	O
relative	O
to	O
a	O
predefined	O
canonical	O
pose	O
for	O
each	O
category	O
.	O

In	O
our	O
implementation	O
,	O
we	O
only	O
consider	O
the	O
heading	O
angle	O
around	O
the	O
up	O
-	O
axis	O
for	O
orientation	O
.	O

section	O
:	O
3D	B-Task
Detection	E-Task
with	O
Frustum	B-Method
PointNets	E-Method
As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
system	O
for	O
3D	O
object	O
detection	S-Task
consists	O
of	O
three	O
modules	O
:	O
frustum	B-Method
proposal	E-Method
,	O
3D	B-Task
instance	I-Task
segmentation	E-Task
,	O
and	O
3D	B-Method
amodal	I-Method
bounding	I-Method
box	I-Method
estimation	E-Method
.	O

We	O
will	O
introduce	O
each	O
module	O
in	O
the	O
following	O
subsections	O
.	O

We	O
will	O
focus	O
on	O
the	O
pipeline	O
and	O
functionality	O
of	O
each	O
module	O
,	O
and	O
refer	O
readers	O
to	O
supplementary	O
for	O
specific	O
architectures	O
of	O
the	O
deep	B-Method
networks	E-Method
involved	O
.	O

subsection	O
:	O
Frustum	B-Method
Proposal	E-Method
The	O
resolution	O
of	O
data	O
produced	O
by	O
most	O
3D	B-Method
sensors	E-Method
,	O
especially	O
real	B-Method
-	I-Method
time	I-Method
depth	I-Method
sensors	E-Method
,	O
is	O
still	O
lower	O
than	O
RGB	B-Material
images	E-Material
from	O
commodity	O
cameras	O
.	O

Therefore	O
,	O
we	O
leverage	O
mature	O
2D	B-Method
object	I-Method
detector	E-Method
to	O
propose	O
2D	O
object	O
regions	O
in	O
RGB	B-Material
images	E-Material
as	O
well	O
as	O
to	O
classify	O
objects	O
.	O

With	O
a	O
known	O
camera	O
projection	O
matrix	O
,	O
a	O
2D	O
bounding	O
box	O
can	O
be	O
lifted	O
to	O
a	O
frustum	O
(	O
with	O
near	O
and	O
far	O
planes	O
specified	O
by	O
depth	O
sensor	O
range	O
)	O
that	O
defines	O
a	O
3D	O
search	O
space	O
for	O
the	O
object	O
.	O

We	O
then	O
collect	O
all	O
points	O
within	O
the	O
frustum	O
to	O
form	O
a	O
frustum	O
point	O
cloud	O
.	O

As	O
shown	O
in	O
Fig	O
[	O
reference	O
]	O
(	O
a	O
)	O
,	O
frustums	O
may	O
orient	O
towards	O
many	O
different	O
directions	O
,	O
which	O
result	O
in	O
large	O
variation	O
in	O
the	O
placement	B-Task
of	I-Task
point	I-Task
clouds	E-Task
.	O

We	O
therefore	O
normalize	O
the	O
frustums	O
by	O
rotating	O
them	O
toward	O
a	O
center	O
view	O
such	O
that	O
the	O
center	O
axis	O
of	O
the	O
frustum	O
is	O
orthogonal	O
to	O
the	O
image	O
plane	O
.	O

This	O
normalization	O
helps	O
improve	O
the	O
rotation	O
-	O
invariance	O
of	O
the	O
algorithm	O
.	O

We	O
call	O
this	O
entire	O
procedure	O
for	O
extracting	B-Task
frustum	I-Task
point	I-Task
clouds	E-Task
from	O
RGB	B-Task
-	I-Task
D	I-Task
data	I-Task
frustum	I-Task
proposal	I-Task
generation	E-Task
.	O

While	O
our	O
3D	B-Task
detection	E-Task
framework	O
is	O
agnostic	O
to	O
the	O
exact	O
method	O
for	O
2D	B-Task
region	I-Task
proposal	E-Task
,	O
we	O
adopt	O
a	O
FPN	B-Method
based	I-Method
model	E-Method
.	O

We	O
pre	O
-	O
train	O
the	O
model	O
weights	O
on	O
ImageNet	B-Task
classification	E-Task
and	O
COCO	O
object	O
detection	S-Task
datasets	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
a	O
KITTI	S-Material
2D	O
object	O
detection	S-Task
dataset	O
to	O
classify	O
and	O
predict	O
amodal	B-Task
2D	I-Task
boxes	E-Task
.	O

More	O
details	O
of	O
the	O
2D	B-Method
detector	I-Method
training	E-Method
are	O
provided	O
in	O
the	O
supplementary	O
.	O

subsection	O
:	O
3D	B-Task
Instance	I-Task
Segmentation	E-Task
Given	O
a	O
2D	O
image	O
region	O
(	O
and	O
its	O
corresponding	O
3D	O
frustum	O
)	O
,	O
several	O
methods	O
might	O
be	O
used	O
to	O
obtain	O
3D	O
location	O
of	O
the	O
object	O
:	O
One	O
straightforward	O
solution	O
is	O
to	O
directly	O
regress	O
3D	O
object	O
locations	O
(	O
e.g.	O
,	O
by	O
3D	O
bounding	O
box	O
)	O
from	O
a	O
depth	B-Method
map	E-Method
using	O
2D	B-Method
CNNs	E-Method
.	O

However	O
,	O
this	O
problem	O
is	O
not	O
easy	O
as	O
occluding	O
objects	O
and	O
background	O
clutter	O
is	O
common	O
in	O
natural	O
scenes	O
(	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
which	O
may	O
severely	O
distract	O
the	O
3D	B-Task
localization	I-Task
task	E-Task
.	O

Because	O
objects	O
are	O
naturally	O
separated	O
in	O
physical	O
space	O
,	O
segmentation	S-Task
in	O
3D	B-Task
point	I-Task
cloud	E-Task
is	O
much	O
more	O
natural	O
and	O
easier	O
than	O
that	O
in	O
images	O
where	O
pixels	O
from	O
distant	O
objects	O
can	O
be	O
near	O
-	O
by	O
to	O
each	O
other	O
.	O

Having	O
observed	O
this	O
fact	O
,	O
we	O
propose	O
to	O
segment	O
instances	O
in	O
3D	B-Task
point	I-Task
cloud	E-Task
instead	O
of	O
in	O
2D	B-Task
image	I-Task
or	I-Task
depth	I-Task
map	E-Task
.	O

Similar	O
to	O
Mask	B-Method
-	I-Method
RCNN	E-Method
,	O
which	O
achieves	O
instance	B-Task
segmentation	E-Task
by	O
binary	B-Task
classification	I-Task
of	I-Task
pixels	I-Task
in	I-Task
image	I-Task
regions	E-Task
,	O
we	O
realize	O
3D	B-Method
instance	I-Method
segmentation	E-Method
using	O
a	O
PointNet	B-Method
-	I-Method
based	I-Method
network	E-Method
on	O
point	O
clouds	O
in	O
frustums	O
.	O

Based	O
on	O
3D	B-Task
instance	I-Task
segmentation	E-Task
,	O
we	O
are	O
able	O
to	O
achieve	O
residual	B-Task
based	I-Task
3D	I-Task
localization	E-Task
.	O

That	O
is	O
,	O
rather	O
than	O
regressing	O
the	O
absolute	O
3D	O
location	O
of	O
the	O
object	O
whose	O
offset	O
from	O
the	O
sensor	O
may	O
vary	O
in	O
large	O
ranges	O
(	O
e.g.	O
from	O
5	O
m	O
to	O
beyond	O
50	O
m	O
in	O
KITTI	S-Material
data	O
)	O
,	O
we	O
predict	O
the	O
3D	O
bounding	O
box	O
center	O
in	O
a	O
local	B-Method
coordinate	I-Method
system	E-Method
–	O
3D	O
mask	O
coordinates	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

paragraph	O
:	O
3D	B-Task
Instance	I-Task
Segmentation	I-Task
PointNet	E-Task
.	O

The	O
network	O
takes	O
a	O
point	O
cloud	O
in	O
frustum	S-Method
and	O
predicts	O
a	O
probability	O
score	O
for	O
each	O
point	O
that	O
indicates	O
how	O
likely	O
the	O
point	O
belongs	O
to	O
the	O
object	O
of	O
interest	O
.	O

Note	O
that	O
each	O
frustum	O
contains	O
exactly	O
one	O
object	O
of	O
interest	O
.	O

Here	O
those	O
“	O
other	O
”	O
points	O
could	O
be	O
points	O
of	O
non	O
-	O
relevant	O
areas	O
(	O
such	O
as	O
ground	O
,	O
vegetation	O
)	O
or	O
other	O
instances	O
that	O
occlude	O
or	O
are	O
behind	O
the	O
object	O
of	O
interest	O
.	O

Similar	O
to	O
the	O
case	O
in	O
2D	B-Task
instance	I-Task
segmentation	E-Task
,	O
depending	O
on	O
the	O
position	O
of	O
the	O
frustum	O
,	O
object	O
points	O
in	O
one	O
frustum	O
may	O
become	O
cluttered	O
or	O
occlude	O
points	O
in	O
another	O
.	O

Therefore	O
,	O
our	O
segmentation	B-Task
PointNet	E-Task
is	O
learning	O
the	O
occlusion	O
and	O
clutter	O
patterns	O
as	O
well	O
as	O
recognizing	O
the	O
geometry	O
for	O
the	O
object	O
of	O
a	O
certain	O
category	O
.	O

In	O
a	O
multi	O
-	O
class	O
detection	S-Task
case	O
,	O
we	O
also	O
leverage	O
the	O
semantics	O
from	O
a	O
2D	B-Method
detector	E-Method
for	O
better	O
instance	B-Task
segmentation	E-Task
.	O

For	O
example	O
,	O
if	O
we	O
know	O
the	O
object	O
of	O
interest	O
is	O
a	O
pedestrian	O
,	O
then	O
the	O
segmentation	B-Method
network	E-Method
can	O
use	O
this	O
prior	O
to	O
find	O
geometries	O
that	O
look	O
like	O
a	O
person	O
.	O

Specifically	O
,	O
in	O
our	O
architecture	O
we	O
encode	O
the	O
semantic	O
category	O
as	O
a	O
one	O
-	O
hot	O
class	O
vector	O
(	O
dimensional	O
for	O
the	O
pre	O
-	O
defined	O
categories	O
)	O
and	O
concatenate	O
the	O
one	O
-	O
hot	O
vector	O
to	O
the	O
intermediate	O
point	O
cloud	O
features	O
.	O

More	O
details	O
of	O
the	O
specific	O
architectures	O
are	O
described	O
in	O
the	O
supplementary	O
.	O

After	O
3D	B-Task
instance	I-Task
segmentation	E-Task
,	O
points	O
that	O
are	O
classified	O
as	O
the	O
object	O
of	O
interest	O
are	O
extracted	O
(	O
“	O
masking	O
”	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Having	O
obtained	O
these	O
segmented	O
object	O
points	O
,	O
we	O
further	O
normalize	O
its	O
coordinates	O
to	O
boost	O
the	O
translational	O
invariance	O
of	O
the	O
algorithm	O
,	O
following	O
the	O
same	O
rationale	O
as	O
in	O
the	O
frustum	B-Method
proposal	I-Method
step	E-Method
.	O

In	O
our	O
implementation	O
,	O
we	O
transform	O
the	O
point	O
cloud	O
into	O
a	O
local	O
coordinate	O
by	O
subtracting	O
XYZ	O
values	O
by	O
its	O
centroid	O
.	O

This	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

Note	O
that	O
we	O
intentionally	O
do	O
not	O
scale	O
the	O
point	O
cloud	O
,	O
because	O
the	O
bounding	O
sphere	O
size	O
of	O
a	O
partial	O
point	O
cloud	O
can	O
be	O
greatly	O
affected	O
by	O
viewpoints	O
and	O
the	O
real	O
size	O
of	O
the	O
point	O
cloud	O
helps	O
the	O
box	B-Method
size	I-Method
estimation	E-Method
.	O

In	O
our	O
experiments	O
,	O
we	O
find	O
that	O
coordinate	O
transformations	O
such	O
as	O
the	O
one	O
above	O
and	O
the	O
previous	O
frustum	O
rotation	O
are	O
critical	O
for	O
3D	B-Task
detection	E-Task
result	O
as	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Amodal	B-Task
3D	I-Task
Box	I-Task
Estimation	E-Task
Given	O
the	O
segmented	O
object	O
points	O
(	O
in	O
3D	O
mask	O
coordinate	O
)	O
,	O
this	O
module	O
estimates	O
the	O
object	O
’s	O
amodal	O
oriented	O
3D	O
bounding	O
box	O
by	O
using	O
a	O
box	B-Method
regression	I-Method
PointNet	E-Method
together	O
with	O
a	O
preprocessing	B-Method
transformer	I-Method
network	E-Method
.	O

paragraph	O
:	O
Learning	B-Task
-	I-Task
based	I-Task
3D	I-Task
Alignment	E-Task
by	O
T	B-Method
-	I-Method
Net	E-Method
Even	O
though	O
we	O
have	O
aligned	O
segmented	O
object	O
points	O
according	O
to	O
their	O
centroid	O
position	O
,	O
we	O
find	O
that	O
the	O
origin	O
of	O
the	O
mask	O
coordinate	O
frame	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
)	O
may	O
still	O
be	O
quite	O
far	O
from	O
the	O
amodal	O
box	O
center	O
.	O

We	O
therefore	O
propose	O
to	O
use	O
a	O
light	B-Method
-	I-Method
weight	I-Method
regression	I-Method
PointNet	E-Method
(	O
T	B-Method
-	I-Method
Net	E-Method
)	O
to	O
estimate	O
the	O
true	O
center	O
of	O
the	O
complete	O
object	O
and	O
then	O
transform	O
the	O
coordinate	O
such	O
that	O
the	O
predicted	O
center	O
becomes	O
the	O
origin	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
architecture	O
and	O
training	O
of	O
our	O
T	B-Method
-	I-Method
Net	E-Method
is	O
similar	O
to	O
the	O
T	B-Method
-	I-Method
Net	E-Method
in	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
type	O
of	O
spatial	B-Method
transformer	I-Method
network	E-Method
(	O
STN	S-Method
)	O
.	O

However	O
,	O
different	O
from	O
the	O
original	O
STN	S-Method
that	O
has	O
no	O
direct	O
supervision	O
on	O
transformation	S-Task
,	O
we	O
explicitly	O
supervise	O
our	O
translation	B-Method
network	E-Method
to	O
predict	O
center	O
residuals	O
from	O
the	O
mask	O
coordinate	O
origin	O
to	O
real	O
object	O
center	O
.	O

paragraph	O
:	O
Amodal	O
3D	B-Task
Box	I-Task
Estimation	I-Task
PointNet	E-Task
The	O
box	B-Method
estimation	I-Method
network	E-Method
predicts	O
amodal	O
bounding	O
boxes	O
(	O
for	O
entire	O
object	O
even	O
if	O
part	O
of	O
it	O
is	O
unseen	O
)	O
for	O
objects	O
given	O
an	O
object	O
point	O
cloud	O
in	O
3D	O
object	O
coordinate	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
network	B-Method
architecture	E-Method
is	O
similar	O
to	O
that	O
for	O
object	B-Task
classification	E-Task
,	O
however	O
the	O
output	O
is	O
no	O
longer	O
object	O
class	O
scores	O
but	O
parameters	O
for	O
a	O
3D	O
bounding	O
box	O
.	O

As	O
stated	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
we	O
parameterize	O
a	O
3D	O
bounding	O
box	O
by	O
its	O
center	O
(	O
,	O
,	O
)	O
,	O
size	O
(	O
,	O
,	O
)	O
and	O
heading	O
angle	O
(	O
along	O
up	O
-	O
axis	O
)	O
.	O

We	O
take	O
a	O
“	O
residual	B-Method
”	I-Method
approach	E-Method
for	O
box	B-Task
center	I-Task
estimation	E-Task
.	O

The	O
center	O
residual	O
predicted	O
by	O
the	O
box	B-Method
estimation	I-Method
network	E-Method
is	O
combined	O
with	O
the	O
previous	O
center	O
residual	O
from	O
the	O
T	B-Method
-	I-Method
Net	E-Method
and	O
the	O
masked	O
points	O
’	O
centroid	O
to	O
recover	O
an	O
absolute	O
center	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

For	O
box	O
size	O
and	O
heading	O
angle	O
,	O
we	O
follow	O
previous	O
works	O
and	O
use	O
a	O
hybrid	O
of	O
classification	B-Method
and	I-Method
regression	I-Method
formulations	E-Method
.	O

Specifically	O
we	O
pre	O
-	O
define	O
size	O
templates	O
and	O
equally	O
split	O
angle	O
bins	O
.	O

Our	O
model	O
will	O
both	O
classify	O
size	O
/	O
heading	O
(	O
scores	O
for	O
size	O
,	O
scores	O
for	O
heading	S-Task
)	O
to	O
those	O
pre	O
-	O
defined	O
categories	O
as	O
well	O
as	O
predict	O
residual	O
numbers	O
for	O
each	O
category	O
(	O
residual	O
dimensions	O
for	O
height	O
,	O
width	O
,	O
length	O
,	O
residual	O
angles	O
for	O
heading	O
)	O
.	O

In	O
the	O
end	O
the	O
net	O
outputs	O
numbers	O
in	O
total	O
.	O

subsection	O
:	O
Training	O
with	O
Multi	B-Task
-	I-Task
task	I-Task
Losses	E-Task
We	O
simultaneously	O
optimize	O
the	O
three	O
nets	O
involved	O
(	O
3D	B-Task
instance	I-Task
segmentation	I-Task
PointNet	E-Task
,	O
T	B-Method
-	I-Method
Net	E-Method
and	O
amodal	B-Method
box	I-Method
estimation	I-Method
PointNet	E-Method
)	O
with	O
multi	O
-	O
task	O
losses	O
(	O
as	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

is	O
for	O
T	B-Method
-	I-Method
Net	E-Method
and	O
is	O
for	O
center	B-Method
regression	I-Method
of	I-Method
box	I-Method
estimation	I-Method
net	E-Method
.	O

and	O
are	O
losses	O
for	O
heading	B-Task
angle	I-Task
prediction	E-Task
while	O
and	O
are	O
for	O
box	O
size	O
.	O

Softmax	S-Method
is	O
used	O
for	O
all	O
classification	B-Task
tasks	E-Task
and	O
smooth	B-Method
-	I-Method
(	I-Method
huber	I-Method
)	I-Method
loss	E-Method
is	O
used	O
for	O
all	O
regression	B-Task
cases	E-Task
.	O

paragraph	O
:	O
Corner	O
Loss	O
for	O
Joint	B-Task
Optimization	I-Task
of	I-Task
Box	I-Task
Parameters	E-Task
While	O
our	O
3D	B-Method
bounding	I-Method
box	I-Method
parameterization	E-Method
is	O
compact	O
and	O
complete	O
,	O
learning	S-Method
is	O
not	O
optimized	O
for	O
final	O
3D	B-Metric
box	I-Metric
accuracy	E-Metric
–	O
center	O
,	O
size	O
and	O
heading	O
have	O
separate	O
loss	O
terms	O
.	O

Imagine	O
cases	O
where	O
center	O
and	O
size	O
are	O
accurately	O
predicted	O
but	O
heading	O
angle	O
is	O
off	O
–	O
the	O
3D	O
IoU	O
with	O
ground	O
truth	O
box	O
will	O
then	O
be	O
dominated	O
by	O
the	O
angle	B-Metric
error	E-Metric
.	O

Ideally	O
all	O
three	O
terms	O
(	O
center	O
,	O
size	O
,	O
heading	O
)	O
should	O
be	O
jointly	O
optimized	O
for	O
best	O
3D	B-Task
box	I-Task
estimation	E-Task
(	O
under	O
IoU	B-Metric
metric	E-Metric
)	O
.	O

To	O
resolve	O
this	O
problem	O
we	O
propose	O
a	O
novel	O
regularization	B-Method
loss	E-Method
,	O
the	O
corner	B-Method
loss	E-Method
:	O
In	O
essence	O
,	O
the	O
corner	B-Metric
loss	E-Metric
is	O
the	O
sum	O
of	O
the	O
distances	O
between	O
the	O
eight	O
corners	O
of	O
a	O
predicted	O
box	O
and	O
a	O
ground	O
truth	O
box	O
.	O

Since	O
corner	O
positions	O
are	O
jointly	O
determined	O
by	O
center	O
,	O
size	O
and	O
heading	O
,	O
the	O
corner	B-Method
loss	E-Method
is	O
able	O
to	O
regularize	O
the	O
multi	B-Task
-	I-Task
task	I-Task
training	E-Task
for	O
those	O
parameters	O
.	O

To	O
compute	O
the	O
corner	O
loss	O
,	O
we	O
firstly	O
construct	O
“	O
anchor	O
”	O
boxes	O
from	O
all	O
size	O
templates	O
and	O
heading	O
angle	O
bins	O
.	O

The	O
anchor	O
boxes	O
are	O
then	O
translated	O
to	O
the	O
estimated	O
box	O
center	O
.	O

We	O
denote	O
the	O
anchor	O
box	O
corners	O
as	O
,	O
where	O
,	O
,	O
are	O
indices	O
for	O
the	O
size	O
class	O
,	O
heading	O
class	O
,	O
and	O
(	O
predefined	O
)	O
corner	O
order	O
,	O
respectively	O
.	O

To	O
avoid	O
large	O
penalty	O
from	O
flipped	B-Task
heading	I-Task
estimation	E-Task
,	O
we	O
further	O
compute	O
distances	O
to	O
corners	O
(	O
from	O
the	O
flipped	O
ground	O
truth	O
box	O
and	O
use	O
the	O
minimum	O
of	O
the	O
original	O
and	O
flipped	O
cases	O
.	O

,	O
which	O
is	O
one	O
for	O
the	O
ground	O
truth	O
size	O
/	O
heading	O
class	O
and	O
zero	O
else	O
wise	O
,	O
is	O
a	O
two	O
-	O
dimensional	O
mask	O
used	O
to	O
select	O
the	O
distance	O
term	O
we	O
care	O
about	O
.	O

section	O
:	O
Experiments	O
Experiments	O
are	O
divided	O
into	O
three	O
parts	O
.	O

First	O
we	O
compare	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
3D	O
object	O
detection	S-Task
on	O
KITTI	S-Material
and	O
SUN	B-Material
-	I-Material
RGBD	E-Material
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
we	O
provide	O
in	O
-	O
depth	O
analysis	O
to	O
validate	O
our	O
design	O
choices	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Last	O
,	O
we	O
show	O
qualitative	O
results	O
and	O
discuss	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
methods	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Comparing	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Methods	O
We	O
evaluate	O
our	O
3D	B-Method
object	I-Method
detector	E-Method
on	O
KITTI	S-Material
and	O
SUN	B-Material
-	I-Material
RGBD	E-Material
benchmarks	O
for	O
3D	O
object	O
detection	S-Task
.	O

On	O
both	O
tasks	O
we	O
have	O
achieved	O
significantly	O
better	O
results	O
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

paragraph	O
:	O
KITTI	S-Material
Tab	O
.	O

[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
our	O
3D	B-Method
detector	E-Method
on	O
the	O
KITTI	S-Material
test	O
set	O
.	O

We	O
outperform	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
by	O
a	O
large	O
margin	O
.	O

While	O
MV3D	S-Method
uses	O
multi	B-Method
-	I-Method
view	I-Method
feature	I-Method
aggregation	E-Method
and	O
sophisticated	O
multi	B-Method
-	I-Method
sensor	I-Method
fusion	I-Method
strategy	E-Method
,	O
our	O
method	O
based	O
on	O
the	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	O
++	O
(	O
v2	O
)	O
backbone	O
is	O
much	O
cleaner	O
in	O
design	O
.	O

While	O
out	O
of	O
the	O
scope	O
for	O
this	O
work	O
,	O
we	O
expect	O
that	O
sensor	B-Task
fusion	E-Task
(	O
esp	O
.	O

aggregation	B-Task
of	I-Task
image	I-Task
feature	E-Task
for	O
3D	B-Task
detection	E-Task
)	O
could	O
further	O
improve	O
our	O
results	O
.	O

We	O
also	O
show	O
our	O
method	O
’s	O
performance	O
on	O
3D	O
object	B-Task
localization	E-Task
(	O
bird	O
’s	O
eye	O
view	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

In	O
the	O
3D	O
localization	S-Task
task	O
bounding	O
boxes	O
are	O
projected	O
to	O
bird	O
’s	O
eye	O
view	O
plane	O
and	O
IoU	O
is	O
evaluated	O
on	O
oriented	O
2D	O
boxes	O
.	O

Again	O
,	O
our	O
method	O
significantly	O
outperforms	O
previous	O
works	O
which	O
include	O
DoBEM	S-Method
and	O
MV3D	S-Method
that	O
use	O
CNNs	S-Method
on	O
projected	B-Material
LiDAR	I-Material
images	E-Material
,	O
as	O
well	O
as	O
3D	B-Method
FCN	E-Method
that	O
uses	O
3D	B-Method
CNNs	E-Method
on	O
voxelized	O
point	O
cloud	O
.	O

The	O
output	O
of	O
our	O
network	O
is	O
visualized	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
where	O
we	O
observe	O
accurate	O
3D	B-Task
instance	I-Task
segmentation	E-Task
and	O
box	B-Task
prediction	E-Task
even	O
under	O
very	O
challenging	O
cases	O
.	O

We	O
defer	O
more	O
discussions	O
on	O
success	O
and	O
failure	O
case	O
patterns	O
to	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

We	O
also	O
report	O
performance	O
on	O
KITTI	S-Material
val	O
set	O
(	O
the	O
same	O
split	O
as	O
in	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
and	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
cars	O
)	O
to	O
support	O
comparison	O
with	O
more	O
published	O
works	O
,	O
and	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
pedestrians	O
and	O
cyclists	O
)	O
for	O
reference	O
.	O

paragraph	O
:	O
SUN	B-Material
-	I-Material
RGBD	E-Material
Most	O
previous	O
3D	B-Task
detection	E-Task
works	O
specialize	O
either	O
on	O
outdoor	B-Material
LiDAR	I-Material
scans	E-Material
where	O
objects	O
are	O
well	O
separated	O
in	O
space	O
and	O
the	O
point	O
cloud	O
is	O
sparse	O
(	O
so	O
that	O
it	O
’s	O
feasible	O
for	O
bird	B-Task
’s	I-Task
eye	I-Task
projection	E-Task
)	O
,	O
or	O
on	O
indoor	B-Task
depth	I-Task
maps	E-Task
that	O
are	O
regular	O
images	O
with	O
dense	O
pixel	O
values	O
such	O
that	O
image	B-Method
CNNs	E-Method
can	O
be	O
easily	O
applied	O
.	O

However	O
,	O
methods	O
designed	O
for	O
bird	B-Task
’s	I-Task
eye	I-Task
view	E-Task
may	O
be	O
incapable	O
for	O
indoor	O
rooms	O
where	O
multiple	O
objects	O
often	O
exist	O
together	O
in	O
vertical	O
space	O
.	O

On	O
the	O
other	O
hand	O
,	O
indoor	B-Method
focused	I-Method
methods	E-Method
could	O
find	O
it	O
hard	O
to	O
apply	O
to	O
sparse	B-Task
and	I-Task
large	I-Task
-	I-Task
scale	I-Task
point	I-Task
cloud	E-Task
from	O
LiDAR	B-Material
scans	E-Material
.	O

In	O
contrast	O
,	O
our	O
frustum	B-Method
-	I-Method
based	I-Method
PointNet	E-Method
is	O
a	O
generic	O
framework	O
for	O
both	O
outdoor	O
and	O
indoor	O
3D	O
object	O
detection	S-Task
.	O

By	O
applying	O
the	O
same	O
pipeline	O
we	O
used	O
for	O
KITTI	S-Material
data	O
set	O
,	O
we	O
’	O
ve	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
SUN	B-Material
-	I-Material
RGBD	E-Material
benchmark	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
significantly	O
higher	O
mAP	S-Metric
as	O
well	O
as	O
much	O
faster	O
(	O
10x	O
-	O
1000x	O
)	O
inference	B-Metric
speed	E-Metric
.	O

subsection	O
:	O
Architecture	B-Task
Design	I-Task
Analysis	E-Task
In	O
this	O
section	O
we	O
provide	O
analysis	O
and	O
ablation	O
experiments	O
to	O
validate	O
our	O
design	O
choices	O
.	O

paragraph	O
:	O
Experiment	O
setup	O
.	O

Unless	O
otherwise	O
noted	O
,	O
all	O
experiments	O
in	O
this	O
section	O
are	O
based	O
on	O
our	O
v1	B-Method
model	E-Method
on	O
KITTI	S-Material
data	O
using	O
train	O
/	O
val	O
split	O
as	O
in	O
.	O

To	O
decouple	O
the	O
influence	O
of	O
2D	B-Method
detectors	E-Method
,	O
we	O
use	O
ground	O
truth	O
2D	O
boxes	O
for	O
region	B-Task
proposals	E-Task
and	O
use	O
3D	B-Metric
box	I-Metric
estimation	I-Metric
accuracy	E-Metric
(	O
IoU	B-Metric
threshold	I-Metric
0.7	E-Metric
)	O
as	O
the	O
evaluation	B-Metric
metric	E-Metric
.	O

We	O
will	O
only	O
focus	O
on	O
the	O
car	B-Task
category	E-Task
which	O
has	O
the	O
most	O
training	O
examples	O
.	O

paragraph	O
:	O
Comparing	O
with	O
alternative	O
approaches	O
for	O
3D	B-Task
detection	E-Task
.	O

In	O
this	O
part	O
we	O
evaluate	O
a	O
few	O
CNN	B-Method
-	I-Method
based	I-Method
baseline	I-Method
approaches	E-Method
as	O
well	O
as	O
ablated	B-Method
versions	E-Method
and	O
variants	O
of	O
our	O
pipelines	O
using	O
2D	O
masks	O
.	O

In	O
the	O
first	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
show	O
3D	B-Task
box	I-Task
estimation	E-Task
results	O
from	O
two	O
CNN	B-Method
-	I-Method
based	I-Method
networks	E-Method
.	O

The	O
baseline	O
methods	O
trained	O
VGG	B-Method
models	E-Method
on	O
ground	B-Material
truth	I-Material
boxes	I-Material
of	I-Material
RGB	I-Material
-	I-Material
D	I-Material
images	E-Material
and	O
adopt	O
the	O
same	O
box	O
parameter	O
and	O
loss	O
functions	O
as	O
our	O
main	O
method	O
.	O

While	O
the	O
model	O
in	O
the	O
first	O
row	O
directly	O
estimates	O
box	O
location	O
and	O
parameters	O
from	O
vanilla	B-Material
RGB	I-Material
-	I-Material
D	I-Material
image	I-Material
patch	E-Material
,	O
the	O
other	O
one	O
(	O
second	O
row	O
)	O
uses	O
a	O
FCN	S-Method
trained	O
from	O
the	O
COCO	B-Material
dataset	E-Material
for	O
2D	B-Task
mask	I-Task
estimation	E-Task
(	O
as	O
that	O
in	O
Mask	B-Method
-	I-Method
RCNN	E-Method
)	O
and	O
only	O
uses	O
features	O
from	O
the	O
masked	O
region	O
for	O
prediction	S-Task
.	O

The	O
depth	O
values	O
are	O
also	O
translated	O
by	O
subtracting	O
the	O
median	O
depth	O
within	O
the	O
2D	O
mask	O
.	O

However	O
,	O
both	O
CNN	B-Method
baselines	E-Method
get	O
far	O
worse	O
results	O
compared	O
to	O
our	O
main	O
method	O
.	O

To	O
understand	O
why	O
CNN	B-Method
baselines	E-Method
underperform	O
,	O
we	O
visualize	O
a	O
typical	O
2D	B-Task
mask	I-Task
prediction	E-Task
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

While	O
the	O
estimated	O
2D	O
mask	O
appears	O
in	O
high	O
quality	O
on	O
an	O
RGB	B-Material
image	E-Material
,	O
there	O
are	O
still	O
lots	O
of	O
clutter	O
and	O
foreground	O
points	O
in	O
the	O
2D	O
mask	O
.	O

In	O
comparison	O
,	O
our	O
3D	B-Method
instance	I-Method
segmentation	E-Method
gets	O
much	O
cleaner	O
result	O
,	O
which	O
greatly	O
eases	O
the	O
next	O
module	O
in	O
finer	B-Task
localization	E-Task
and	O
bounding	B-Task
box	I-Task
regression	E-Task
.	O

In	O
the	O
third	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
experiment	O
with	O
an	O
ablated	B-Method
version	E-Method
of	O
frustum	B-Method
PointNet	E-Method
that	O
has	O
no	O
3D	B-Method
instance	I-Method
segmentation	I-Method
module	E-Method
.	O

Not	O
surprisingly	O
,	O
the	O
model	O
gets	O
much	O
worse	O
results	O
than	O
our	O
main	O
method	O
,	O
which	O
indicates	O
the	O
critical	O
effect	O
of	O
our	O
3D	B-Method
instance	I-Method
segmentation	I-Method
module	E-Method
.	O

In	O
the	O
fourth	O
row	O
,	O
instead	O
of	O
3D	B-Task
segmentation	E-Task
we	O
use	O
point	O
clouds	O
from	O
2D	O
masked	O
depth	O
maps	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
for	O
3D	B-Task
box	I-Task
estimation	E-Task
.	O

However	O
,	O
since	O
a	O
2D	O
mask	O
is	O
not	O
able	O
to	O
cleanly	O
segment	O
the	O
3D	O
object	O
,	O
the	O
performance	O
is	O
more	O
than	O
12	O
%	O
worse	O
than	O
that	O
with	O
the	O
3D	B-Method
segmentation	E-Method
(	O
our	O
main	O
method	O
in	O
the	O
fifth	O
row	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
combined	O
usage	O
of	O
2D	B-Method
and	I-Method
3D	I-Method
masks	E-Method
–	O
applying	O
3D	B-Method
segmentation	E-Method
on	O
point	O
cloud	O
from	O
2D	O
masked	O
depth	O
map	O
–	O
also	O
shows	O
slightly	O
worse	O
results	O
than	O
our	O
main	O
method	O
probably	O
due	O
to	O
the	O
accumulated	O
error	O
from	O
inaccurate	O
2D	B-Method
mask	I-Method
predictions	E-Method
.	O

paragraph	O
:	O
Effects	O
of	O
point	B-Method
cloud	I-Method
normalization	E-Method
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
frustum	O
PointNet	S-Method
takes	O
a	O
few	O
key	O
coordinate	O
transformations	O
to	O
canonicalize	O
the	O
point	O
cloud	O
for	O
more	O
effective	O
learning	S-Task
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
how	O
each	O
normalization	B-Method
step	E-Method
helps	O
for	O
3D	B-Task
detection	E-Task
.	O

We	O
see	O
that	O
both	O
frustum	O
rotation	O
(	O
such	O
that	O
frustum	O
points	O
have	O
more	O
similar	O
XYZ	O
distributions	O
)	O
and	O
mask	B-Method
centroid	I-Method
subtraction	E-Method
(	O
such	O
that	O
object	O
points	O
have	O
smaller	O
and	O
more	O
canonical	O
XYZ	O
)	O
are	O
critical	O
.	O

In	O
addition	O
,	O
extra	O
alignment	O
of	O
object	O
point	O
cloud	O
to	O
object	O
center	O
by	O
T	B-Method
-	I-Method
Net	E-Method
also	O
contributes	O
significantly	O
to	O
the	O
performance	O
.	O

paragraph	O
:	O
Effects	O
of	O
regression	B-Method
loss	I-Method
formulation	E-Method
and	O
corner	B-Method
loss	E-Method
.	O

In	O
Tab	O
.	O

[	O
reference	O
]	O
we	O
compare	O
different	O
loss	O
options	O
and	O
show	O
that	O
a	O
combination	O
of	O
“	O
cls	B-Method
-	I-Method
reg	I-Method
”	I-Method
loss	E-Method
(	O
the	O
classification	B-Method
and	I-Method
residual	I-Method
regression	I-Method
approach	E-Method
for	O
heading	B-Task
and	I-Task
size	I-Task
regression	E-Task
)	O
and	O
a	O
regularizing	B-Method
corner	I-Method
loss	E-Method
achieves	O
the	O
best	O
result	O
.	O

The	O
naive	O
baseline	O
using	O
regression	B-Method
loss	E-Method
only	O
(	O
first	O
row	O
)	O
achieves	O
unsatisfactory	O
result	O
because	O
the	O
regression	O
target	O
is	O
large	O
in	O
range	O
(	O
object	O
size	O
from	O
0.2	O
m	O
to	O
5	O
m	O
)	O
.	O

In	O
comparison	O
,	O
the	O
cls	B-Method
-	I-Method
reg	I-Method
loss	E-Method
and	O
a	O
normalized	B-Method
version	E-Method
(	O
residual	O
normalized	O
by	O
heading	O
bin	O
size	O
or	O
template	O
shape	O
size	O
)	O
of	O
it	O
achieve	O
much	O
better	O
performance	O
.	O

At	O
last	O
row	O
we	O
show	O
that	O
a	O
regularizing	O
corner	O
loss	O
further	O
helps	O
optimization	S-Task
.	O

subsection	O
:	O
Qualitative	O
Results	O
and	O
Discussion	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
representative	O
outputs	O
of	O
our	O
frustum	B-Method
PointNet	I-Method
model	E-Method
.	O

We	O
see	O
that	O
for	O
simple	O
cases	O
of	O
non	O
-	O
occluded	O
objects	O
in	O
reasonable	O
distance	O
(	O
so	O
we	O
get	O
enough	O
number	O
of	O
points	O
)	O
,	O
our	O
model	O
outputs	O
remarkably	O
accurate	O
3D	O
instance	O
segmentation	O
mask	O
and	O
3D	O
bounding	O
boxes	O
.	O

Second	O
,	O
we	O
are	O
surprised	O
to	O
find	O
that	O
our	O
model	O
can	O
even	O
predict	O
correctly	O
posed	O
amodal	O
3D	O
box	O
from	O
partial	O
data	O
(	O
e.g.	O
parallel	O
parked	O
cars	O
)	O
with	O
few	O
points	O
.	O

Even	O
humans	O
find	O
it	O
very	O
difficult	O
to	O
annotate	O
such	O
results	O
with	O
point	O
cloud	O
data	O
only	O
.	O

Third	O
,	O
in	O
some	O
cases	O
that	O
seem	O
very	O
challenging	O
in	O
images	O
with	O
lots	O
of	O
nearby	O
or	O
even	O
overlapping	O
2D	O
boxes	O
,	O
when	O
converted	O
to	O
3D	O
space	O
,	O
the	O
localization	S-Task
becomes	O
much	O
easier	O
(	O
e.g.	O
P11	O
in	O
second	O
row	O
third	O
column	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
do	O
observe	O
several	O
failure	O
patterns	O
,	O
which	O
indicate	O
possible	O
directions	O
for	O
future	O
efforts	O
.	O

The	O
first	O
common	O
mistake	O
is	O
due	O
to	O
inaccurate	B-Task
pose	I-Task
and	I-Task
size	I-Task
estimation	E-Task
in	O
a	O
sparse	O
point	O
cloud	O
(	O
sometimes	O
less	O
than	O
5	O
points	O
)	O
.	O

We	O
think	O
image	O
features	O
could	O
greatly	O
help	O
esp	S-Task
.	O

since	O
we	O
have	O
access	O
to	O
high	O
resolution	O
image	O
patch	O
even	O
for	O
far	O
-	O
away	O
objects	O
.	O

The	O
second	O
type	O
of	O
challenge	O
is	O
when	O
there	O
are	O
multiple	O
instances	O
from	O
the	O
same	O
category	O
in	O
a	O
frustum	O
(	O
like	O
two	O
persons	O
standing	O
by	O
)	O
.	O

Since	O
our	O
current	O
pipeline	O
assumes	O
a	O
single	O
object	O
of	O
interest	O
in	O
each	O
frustum	O
,	O
it	O
may	O
get	O
confused	O
when	O
multiple	O
instances	O
appear	O
and	O
thus	O
outputs	O
mixed	O
segmentation	O
results	O
.	O

This	O
problem	O
could	O
potentially	O
be	O
mitigated	O
if	O
we	O
are	O
able	O
to	O
propose	O
multiple	O
3D	O
bounding	O
boxes	O
within	O
each	O
frustum	O
.	O

Thirdly	O
,	O
sometimes	O
our	O
2D	B-Method
detector	E-Method
misses	O
objects	O
due	O
to	O
dark	O
lighting	O
or	O
strong	O
occlusion	O
.	O

Since	O
our	O
frustum	B-Method
proposals	E-Method
are	O
based	O
on	O
region	B-Method
proposal	E-Method
,	O
no	O
3D	O
object	O
will	O
be	O
detected	O
given	O
no	O
2D	B-Task
detection	E-Task
.	O

However	O
,	O
our	O
3D	B-Task
instance	I-Task
segmentation	E-Task
and	O
amodal	B-Task
3D	I-Task
box	I-Task
estimation	I-Task
PointNets	E-Task
are	O
not	O
restricted	O
to	O
RGB	O
view	O
proposals	O
.	O

As	O
shown	O
in	O
the	O
supplementary	O
,	O
the	O
same	O
framework	O
can	O
also	O
be	O
extended	O
to	O
3D	O
regions	O
proposed	O
in	O
bird	O
’s	O
eye	O
view	O
.	O

paragraph	O
:	O
Acknowledgement	O
The	O
authors	O
wish	O
to	O
thank	O
the	O
support	O
of	O
Nuro	O
Inc.	O
,	O
ONR	O
MURI	O
grant	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0341	O
,	O
NSF	O
grants	O
DMS	O
-	O
1546206	O
and	O
IIS	O
-	O
1528025	O
,	O
a	O
Samsung	O
GRO	O
award	O
,	O
and	O
gifts	O
from	O
Adobe	O
,	O
Amazon	O
,	O
and	O
Apple	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Overview	O
This	O
document	O
provides	O
additional	O
technical	O
details	O
,	O
extra	O
analysis	O
experiments	O
,	O
more	O
quantitative	O
results	O
and	O
qualitative	O
test	O
results	O
to	O
the	O
main	O
paper	O
.	O

In	O
Sec.ï½	O
[	O
reference	O
]	O
we	O
provide	O
more	O
details	O
on	O
network	B-Method
architectures	E-Method
of	O
PointNets	O
and	O
training	O
parameters	O
while	O
Sec	O
.	O

[	O
reference	O
]	O
explains	O
more	O
about	O
our	O
2D	B-Method
detector	E-Method
.	O

Sec	O
.	O

[	O
reference	O
]	O
shows	O
how	O
our	O
framework	O
can	O
be	O
extended	O
to	O
bird	O
’s	O
eye	O
view	O
(	O
BV	O
)	O
proposals	O
and	O
how	O
combining	O
BV	B-Method
and	I-Method
RGB	I-Method
proposals	E-Method
can	O
further	O
improve	O
detection	S-Task
performance	O
.	O

Then	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
results	O
from	O
more	O
analysis	O
experiments	O
.	O

At	O
last	O
,	O
Sec	O
.	O

[	O
reference	O
]	O
shows	O
more	O
visualization	O
results	O
for	O
3D	B-Task
detection	E-Task
on	O
SUN	B-Material
-	I-Material
RGBD	E-Material
dataset	O
.	O

appendix	O
:	O
Details	O
on	O
Frustum	B-Method
PointNets	E-Method
(	O
Sec	O
4.2	O
,	O
4.3	O
)	O
subsection	O
:	O
Network	B-Method
Architectures	E-Method
We	O
adopt	O
similar	O
network	B-Method
architectures	E-Method
as	O
in	O
the	O
original	O
works	O
of	O
PointNet	O
and	O
PointNet	O
++	O
for	O
our	O
v1	B-Method
and	I-Method
v2	I-Method
models	E-Method
respectively	O
.	O

What	O
is	O
different	O
is	O
that	O
we	O
add	O
an	O
extra	O
link	O
for	O
class	O
one	O
-	O
hot	O
vector	O
such	O
that	O
instance	B-Task
segmentation	E-Task
and	O
bounding	B-Task
box	I-Task
estimation	E-Task
can	O
leverage	O
semantics	O
predicted	O
from	O
RGB	B-Material
images	E-Material
.	O

The	O
detailed	O
network	B-Method
architectures	E-Method
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

For	O
v1	B-Method
model	E-Method
our	O
architecture	O
involves	O
point	B-Method
embedding	I-Method
layers	E-Method
(	O
as	O
shared	O
MLP	S-Method
on	O
each	O
point	O
independently	O
)	O
,	O
a	O
max	B-Method
pooling	I-Method
layer	E-Method
and	O
per	B-Method
-	I-Method
point	I-Method
classification	I-Method
multi	I-Method
-	I-Method
layer	I-Method
perceptron	E-Method
(	O
MLP	S-Method
)	O
based	O
on	O
aggregated	O
information	O
from	O
global	O
feature	O
and	O
each	O
point	O
as	O
well	O
as	O
an	O
one	O
-	O
hot	O
class	O
vector	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
transformer	B-Method
networks	E-Method
as	O
in	O
because	O
frustum	O
points	O
are	O
viewpoint	O
based	O
(	O
not	O
complete	O
point	O
cloud	O
as	O
in	O
)	O
and	O
are	O
already	O
normalized	O
by	O
frustum	O
rotation	O
.	O

In	O
addition	O
to	O
XYZ	O
,	O
we	O
also	O
leverage	O
LiDAR	O
intensity	O
as	O
a	O
fourth	O
channel	O
.	O

For	O
v2	B-Method
model	E-Method
we	O
use	O
set	B-Method
abstraction	I-Method
layers	E-Method
for	O
hierarchical	B-Task
feature	I-Task
learning	E-Task
in	O
point	B-Task
clouds	E-Task
.	O

In	O
addition	O
,	O
because	O
LiDAR	B-Method
point	I-Method
cloud	E-Method
gets	O
increasingly	O
sparse	O
as	O
it	O
gets	O
farther	O
,	O
feature	B-Method
learning	E-Method
has	O
to	O
be	O
robust	O
to	O
those	O
density	O
variations	O
.	O

Therefore	O
we	O
used	O
a	O
robust	O
type	O
of	O
set	B-Method
abstraction	I-Method
layers	E-Method
–	O
multi	B-Method
-	I-Method
scale	I-Method
grouping	I-Method
(	I-Method
MSG	I-Method
)	I-Method
layers	E-Method
as	O
introduced	O
in	O
for	O
the	O
segmentation	B-Method
network	E-Method
.	O

With	O
hierarchical	O
features	O
and	O
learned	O
robustness	O
to	O
varying	O
densities	O
,	O
our	O
v2	B-Method
model	E-Method
shows	O
superior	O
performance	O
than	O
v1	B-Method
model	E-Method
in	O
both	O
segmentation	S-Task
and	O
box	B-Task
estimation	E-Task
.	O

subsection	O
:	O
Data	B-Task
Augmentation	E-Task
and	O
Training	S-Task
paragraph	O
:	O
Data	B-Task
augmentation	I-Task
Data	I-Task
augmentation	E-Task
plays	O
an	O
important	O
role	O
in	O
preventing	O
model	B-Task
overfitting	E-Task
.	O

Our	O
augmentation	O
involves	O
two	O
branches	O
:	O
one	O
is	O
2D	B-Method
box	I-Method
augmentation	E-Method
and	O
the	O
other	O
is	O
frustum	B-Method
point	I-Method
cloud	I-Method
augmentation	E-Method
.	O

We	O
use	O
ground	O
truth	O
2D	O
boxes	O
to	O
generate	O
frustum	O
point	O
clouds	O
for	O
Frustum	B-Method
PointNets	E-Method
training	O
and	O
augment	O
the	O
2D	O
boxes	O
by	O
random	O
translation	O
and	O
scaling	O
.	O

Specifically	O
,	O
we	O
firstly	O
compute	O
the	O
2D	O
box	O
height	O
(	O
)	O
and	O
width	O
(	O
)	O
and	O
translate	O
the	O
2D	O
box	O
center	O
by	O
random	O
distances	O
sampled	O
from	O
Uniform	O
[	O
]	O
and	O
Uniform	O
[	O
]	O
in	O
u	O
,	O
v	O
directions	O
respectively	O
.	O

The	O
height	O
and	O
width	O
are	O
also	O
augmented	O
by	O
two	O
random	O
scaling	O
factor	O
sampled	O
from	O
Uniform	O
[	O
]	O
.	O

We	O
augment	O
each	O
frustum	O
point	O
cloud	O
by	O
three	O
ways	O
.	O

First	O
,	O
we	O
randomly	O
sample	O
a	O
subset	O
of	O
points	O
from	O
the	O
frustum	O
point	O
cloud	O
on	O
the	O
fly	O
(	O
1	O
,	O
024	O
for	O
KITTI	S-Material
and	O
2	O
,	O
048	O
for	O
SUN	B-Material
-	I-Material
RGBD	E-Material
)	O
.	O

For	O
object	O
points	O
segmented	O
from	O
our	O
predicted	O
3D	O
mask	O
,	O
we	O
randomly	O
sample	O
512	O
points	O
from	O
it	O
(	O
if	O
there	O
are	O
less	O
than	O
512	O
points	O
we	O
will	O
randomly	O
resample	O
to	O
make	O
up	O
for	O
the	O
number	O
)	O
.	O

Second	O
,	O
we	O
randomly	O
flip	O
the	O
frustum	O
point	O
cloud	O
(	O
after	O
rotating	O
the	O
frustum	O
to	O
the	O
center	O
)	O
along	O
the	O
YZ	O
plane	O
in	O
camera	O
coordinate	O
(	O
Z	O
is	O
forward	O
,	O
Y	O
is	O
pointing	O
down	O
)	O
.	O

Thirdly	O
,	O
we	O
perturb	O
the	O
points	O
by	O
shifting	O
the	O
entire	O
frustum	O
point	O
cloud	O
in	O
Z	O
-	O
axis	O
direction	O
such	O
that	O
the	O
depth	O
of	O
points	O
is	O
augmented	O
.	O

Together	O
with	O
all	O
data	B-Task
augmentation	E-Task
,	O
we	O
modify	O
the	O
ground	O
truth	O
labels	O
for	O
3D	O
mask	O
and	O
headings	O
correspondingly	O
.	O

paragraph	O
:	O
KITTI	S-Material
Training	O
The	O
object	O
detection	S-Task
benchmark	O
in	O
KITTI	S-Material
provides	O
synchronized	B-Material
RGB	I-Material
images	E-Material
and	O
LiDAR	B-Material
point	I-Material
clouds	E-Material
with	O
ground	O
truth	O
amodal	O
2D	O
and	O
3D	O
box	O
annotations	O
for	O
vehicles	O
,	O
pedestrians	O
and	O
cyclists	O
.	O

The	O
training	O
set	O
contains	O
7	O
,	O
481	O
frames	O
and	O
an	O
undisclosed	O
test	O
set	O
contains	O
7	O
,	O
581	O
frames	O
.	O

In	O
our	O
own	O
experiments	O
(	O
except	O
those	O
for	O
test	O
sets	O
)	O
,	O
we	O
follow	O
to	O
split	O
the	O
official	O
training	O
set	O
to	O
a	O
train	O
set	O
of	O
3	O
,	O
717	O
frames	O
and	O
a	O
val	O
set	O
of	O
3769	O
frames	O
such	O
that	O
frames	O
in	O
train	O
/	O
val	O
sets	O
belong	O
to	O
different	O
video	O
clips	O
.	O

For	O
models	O
evaluated	O
on	O
the	O
test	O
set	O
we	O
train	O
our	O
model	O
on	O
our	O
own	O
train	O
/	O
val	O
split	O
where	O
around	O
80	O
%	O
of	O
the	O
training	O
data	O
is	O
used	O
such	O
that	O
the	O
model	O
can	O
achieve	O
better	O
generalization	S-Task
by	O
seeing	O
more	O
examples	O
.	O

To	O
get	O
ground	O
truth	O
for	O
3D	B-Task
instance	I-Task
segmentation	E-Task
we	O
simply	O
consider	O
all	O
points	O
that	O
fall	O
into	O
the	O
ground	O
truth	O
3D	O
bounding	O
box	O
as	O
object	O
points	O
.	O

Although	O
there	O
are	O
sometimes	O
false	O
labels	O
from	O
ground	O
points	O
or	O
points	O
from	O
other	O
closeby	O
objects	O
(	O
e.g.	O
a	O
person	O
standing	O
by	O
)	O
,	O
the	O
auto	O
-	O
labeled	O
segmentation	O
ground	O
truth	O
is	O
in	O
general	O
acceptable	O
.	O

For	O
both	O
of	O
our	O
v1	B-Method
and	I-Method
v2	I-Method
models	E-Method
,	O
we	O
use	O
Adam	B-Method
optimizer	E-Method
with	O
starting	B-Metric
learning	I-Metric
rate	E-Metric
0.001	O
,	O
with	O
step	O
-	O
wise	O
decay	O
(	O
by	O
half	O
)	O
in	O
every	O
60k	O
iterations	O
.	O

For	O
all	O
trainable	O
layers	O
except	O
the	O
last	O
classification	B-Task
or	I-Task
regression	I-Task
ones	E-Task
,	O
we	O
use	O
batch	B-Method
normalization	E-Method
with	O
a	O
start	O
decay	O
rate	O
of	O
0.5	O
and	O
gradually	O
decay	O
the	O
decay	B-Metric
rate	E-Metric
to	O
0.99	O
(	O
step	O
-	O
wise	O
decay	O
with	O
rate	O
0.5	O
in	O
every	O
20k	O
iterations	O
)	O
.	O

We	O
use	O
batch	O
size	O
32	O
for	O
v1	B-Method
models	E-Method
and	O
batch	O
size	O
24	O
for	O
v2	B-Method
models	E-Method
.	O

All	O
three	O
PointNets	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Trained	O
on	O
a	O
single	O
GTX	B-Method
1080	I-Method
GPU	E-Method
,	O
it	O
takes	O
around	O
one	O
day	O
to	O
train	O
a	O
v1	B-Method
model	E-Method
(	O
all	O
three	O
nets	O
)	O
for	O
200	O
epochs	O
while	O
it	O
takes	O
around	O
three	O
days	O
for	O
a	O
v2	B-Method
model	E-Method
.	O

We	O
picked	O
the	O
early	O
stopped	O
(	O
200	O
epochs	O
)	O
snapshot	B-Method
models	E-Method
for	O
evaluation	O
.	O

paragraph	O
:	O
SUN	B-Material
-	I-Material
RGBD	I-Material
Training	E-Material
The	O
data	O
set	O
consists	O
of	O
10	O
,	O
355	O
RGB	B-Material
-	I-Material
D	I-Material
images	E-Material
captured	O
from	O
various	O
depth	O
sensors	O
for	O
indoor	O
scenes	O
(	O
bedrooms	O
,	O
dining	O
rooms	O
etc	O
.	O

)	O
.	O

We	O
follow	O
the	O
same	O
train	O
/	O
val	O
splits	O
as	O
for	O
experiments	O
.	O

The	O
data	B-Method
augmentation	E-Method
and	O
optimization	O
parameters	O
are	O
the	O
same	O
as	O
that	O
in	O
KITTI	S-Material
.	O

As	O
to	O
auto	B-Task
-	I-Task
labeling	I-Task
of	I-Task
instance	I-Task
segmentation	I-Task
mask	E-Task
,	O
however	O
,	O
data	B-Metric
quality	E-Metric
is	O
much	O
lower	O
than	O
that	O
in	O
KITTI	S-Material
because	O
of	O
strong	O
occlusions	O
and	O
tight	O
arrangement	O
of	O
objects	O
in	O
indoor	O
scenes	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
some	O
examples	O
)	O
.	O

Nonetheless	O
we	O
still	O
consider	O
all	O
points	O
within	O
the	O
ground	O
truth	O
boxes	O
as	O
object	O
points	O
for	O
our	O
training	O
.	O

For	O
3D	B-Task
segmentation	E-Task
we	O
get	O
only	O
a	O
82.7	O
%	O
accuracy	S-Metric
compared	O
to	O
around	O
90	O
%	O
in	O
KITTI	S-Material
.	O

Due	O
to	O
the	O
heavy	O
noise	O
in	O
segmentation	O
mask	O
label	O
,	O
we	O
choose	O
to	O
only	O
train	O
and	O
evaluate	O
on	O
v1	B-Method
models	E-Method
that	O
has	O
more	O
strength	O
in	O
global	B-Method
feature	I-Method
learning	E-Method
than	O
v2	O
ones	O
.	O

For	O
future	O
works	O
,	O
we	O
think	O
higher	O
quality	O
in	O
3D	O
mask	O
labels	O
can	O
greatly	O
help	O
the	O
instance	B-Task
segmentation	I-Task
network	I-Task
training	E-Task
.	O

appendix	O
:	O
Details	O
on	O
RGB	B-Method
Detector	E-Method
(	O
Sec	O
4.1	O
)	O
For	O
2D	B-Task
RGB	I-Task
image	I-Task
detector	E-Task
,	O
we	O
use	O
the	O
encoder	B-Method
-	I-Method
decoder	I-Method
structure	E-Method
(	O
e.g.	O
DSSD	S-Method
,	O
FPN	S-Method
)	O
to	O
generate	O
region	O
proposals	O
from	O
multiple	O
feature	O
maps	O
using	O
focal	O
loss	O
and	O
use	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
to	O
predict	O
final	O
2D	B-Task
detection	E-Task
bounding	O
boxes	O
from	O
the	O
region	O
proposals	O
.	O

To	O
make	O
the	O
detector	S-Task
faster	O
,	O
we	O
take	O
the	O
reduced	B-Method
VGG	I-Method
base	I-Method
network	I-Method
architecture	E-Method
from	O
SSD	S-Method
,	O
sample	O
half	O
of	O
the	O
channels	O
per	O
layer	O
and	O
change	O
all	O
max	B-Method
pooling	I-Method
layers	E-Method
to	O
convolution	B-Method
layers	E-Method
with	O
kernel	O
size	O
and	O
stride	O
of	O
2	O
.	O

Then	O
we	O
fine	O
-	O
tune	O
it	O
on	O
ImageNet	B-Material
CLS	I-Material
-	I-Material
LOC	I-Material
dataset	E-Material
for	O
400k	O
iterations	O
with	O
batch	O
size	O
of	O
260	O
on	O
10	O
GPUs	O
.	O

The	O
resulting	O
base	O
network	B-Method
architecture	E-Method
has	O
about	O
66.7	O
%	O
top	B-Metric
-	I-Metric
1	I-Metric
classification	I-Metric
accuracy	E-Metric
on	O
the	O
CLS	B-Material
-	I-Material
LOC	I-Material
validation	I-Material
dataset	E-Material
and	O
only	O
needs	O
about	O
1.2ms	O
to	O
process	O
a	O
image	O
on	O
a	O
NVIDIA	B-Method
GTX	I-Method
1080	E-Method
.	O

We	O
then	O
add	O
the	O
feature	B-Method
pyramid	I-Method
layers	E-Method
from	O
conv3_3	S-Method
,	O
conv4_3	O
,	O
conv5_3	S-Method
,	O
and	O
fc7	S-Method
,	O
which	O
are	O
used	O
to	O
predict	O
region	O
proposals	O
with	O
scales	O
of	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
respectively	O
.	O

We	O
also	O
add	O
an	O
extra	O
convolutional	B-Method
layer	E-Method
(	O
conv8	S-Method
)	O
which	O
halves	O
the	O
fc7	O
feature	O
map	O
size	O
,	O
and	O
use	O
it	O
to	O
predict	O
proposals	O
with	O
scale	O
of	O
256	O
.	O

We	O
use	O
5	O
different	O
aspect	O
ratios	O
{	O
,	O
,	O
1	O
,	O
2	O
,	O
3	O
}	O
for	O
all	O
layers	O
except	O
that	O
we	O
ignore	O
{	O
,	O
3	O
}	O
for	O
conv3_3	O
.	O

Following	O
SSD	S-Method
,	O
we	O
also	O
use	O
normalization	B-Method
layer	E-Method
on	O
conv3_3	S-Method
,	O
conv4_3	O
,	O
and	O
conv5_3	O
and	O
initialize	O
the	O
norm	O
40	O
.	O

For	O
Fast	B-Task
R	I-Task
-	I-Task
CNN	I-Task
part	E-Task
,	O
we	O
extract	O
features	O
from	O
conv3_3	S-Method
,	O
conv5_3	S-Method
,	O
and	O
conv8	S-Method
for	O
each	O
region	O
proposal	O
and	O
concatenate	O
all	O
the	O
features	O
to	O
predict	O
class	O
scores	O
and	O
further	O
adjust	O
the	O
proposals	O
.	O

We	O
train	O
this	O
detector	O
from	O
COCO	B-Material
dataset	E-Material
with	O
input	O
image	O
and	O
have	O
achieved	O
35.5	O
mAP	S-Metric
on	O
the	O
COCO	B-Material
minival	I-Material
dataset	E-Material
,	O
with	O
only	O
10ms	O
processing	B-Metric
time	E-Metric
for	O
a	O
image	O
on	O
a	O
single	O
GPU	O
.	O

Finally	O
,	O
we	O
fine	O
-	O
tune	O
the	O
detector	O
on	O
car	O
,	O
people	O
,	O
and	O
bicycle	O
from	O
COCO	B-Material
dataset	E-Material
,	O
and	O
have	O
achieved	O
48.5	O
,	O
44.1	O
,	O
and	O
40.1	O
for	O
these	O
three	O
classes	O
on	O
COCO	S-Material
.	O

We	O
take	O
this	O
model	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
car	B-Task
,	I-Task
pedestrian	E-Task
,	O
and	O
cyclist	O
from	O
KITTI	S-Material
dataset	O
.	O

The	O
final	O
model	O
takes	O
about	O
30ms	O
to	O
process	O
a	O
image	O
.	O

To	O
increase	O
the	O
recall	S-Metric
of	O
the	O
detector	O
,	O
we	O
also	O
do	O
detection	S-Task
from	O
the	O
center	O
crop	O
of	O
the	O
image	O
besides	O
the	O
full	O
image	O
,	O
and	O
then	O
merge	O
the	O
detections	O
using	O
non	B-Method
-	I-Method
maximum	I-Method
suppression	E-Method
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
our	O
detector	O
’s	O
AP	S-Metric
(	O
2D	O
)	O
on	O
KITTI	S-Material
test	O
set	O
.	O

Our	O
detector	O
has	O
achieved	O
competitive	O
or	O
better	O
results	O
than	O
current	O
leading	O
players	O
on	O
KITTI	S-Material
leader	O
board	O
.	O

We	O
’	O
ve	O
also	O
reported	O
our	O
AP	S-Metric
(	O
2D	O
)	O
on	O
val	O
set	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

appendix	O
:	O
Bird	O
’s	O
Eye	O
View	O
PointNets	O
(	O
Sec	O
5.3	O
)	O
In	O
this	O
section	O
,	O
we	O
show	O
that	O
our	O
3D	B-Task
detection	E-Task
framework	O
can	O
also	O
be	O
extended	O
to	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
,	O
which	O
adds	O
another	O
orthogonal	B-Method
proposal	I-Method
source	E-Method
to	O
achieve	O
better	O
overall	O
3D	B-Task
detection	E-Task
performance	O
.	O

We	O
evaluate	O
the	O
results	O
of	O
car	B-Task
detection	E-Task
using	O
LiDAR	O
bird	O
’s	O
eye	O
view	O
only	O
proposals	O
+	O
point	O
net	O
(	O
Ours	O
(	O
BV	O
)	O
)	O
,	O
and	O
combine	O
frustum	B-Method
point	I-Method
net	E-Method
and	O
bird	O
’s	O
eye	O
view	O
point	O
net	O
using	O
3D	B-Method
non	I-Method
-	I-Method
maximum	I-Method
suppression	I-Method
(	I-Method
NMS	I-Method
)	E-Method
(	O
Ours	B-Method
(	I-Method
Frustum	I-Method
+	I-Method
BV	I-Method
)	E-Method
)	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
Proposal	O
Similar	O
to	O
MV3D	O
we	O
use	O
point	O
features	O
such	O
as	O
height	O
,	O
intensity	O
and	O
density	O
,	O
and	O
train	O
the	O
bird	O
’s	O
eye	O
view	O
2D	B-Method
proposal	I-Method
net	E-Method
using	O
the	O
standard	O
Faster	B-Method
-	I-Method
RCNN	I-Method
structure	E-Method
.	O

The	O
net	O
outputs	O
axis	O
-	O
aligned	O
2D	O
bounding	O
boxes	O
in	O
the	O
bird	O
’s	O
eye	O
view	O
.	O

In	O
detail	O
,	O
we	O
discretize	O
the	O
projected	O
point	O
clouds	O
into	O
2D	O
grids	O
with	O
resolution	O
of	O
meter	O
and	O
with	O
the	O
depth	O
and	O
width	O
range	O
meters	O
,	O
which	O
gives	O
us	O
the	O
input	O
size	O
.	O

For	O
each	O
cell	O
,	O
we	O
take	O
the	O
intensity	O
and	O
the	O
density	O
of	O
the	O
highest	O
point	O
and	O
divide	O
the	O
heights	O
into	O
bins	O
with	O
the	O
height	O
of	O
the	O
highest	O
point	O
in	O
each	O
bin	O
,	O
which	O
gives	O
us	O
channels	O
in	O
total	O
.	O

In	O
Faster	B-Task
R	I-Task
-	I-Task
CNN	E-Task
,	O
we	O
use	O
the	O
VGG	B-Method
-	I-Method
16	E-Method
with	O
anchor	O
scales	O
(	O
)	O
and	O
aspect	O
ratios	O
(	O
)	O
.	O

We	O
train	O
RPN	S-Method
and	O
Fast	B-Method
R	I-Method
-	I-Method
CNN	E-Method
together	O
using	O
the	O
approximate	B-Method
joint	I-Method
training	E-Method
.	O

To	O
combine	O
3D	B-Task
detection	E-Task
boxes	O
from	O
frustum	B-Method
PointNets	E-Method
and	O
the	O
bird	O
’s	O
eye	O
view	O
PointNets	O
,	O
we	O
use	O
3D	B-Method
NMS	E-Method
with	O
IoU	O
threshold	O
.	O

We	O
also	O
apply	O
a	O
weight	O
(	O
0.5	O
)	O
to	O
3D	O
boxes	O
from	O
BV	O
PointNets	O
since	O
it	O
is	O
a	O
weaker	O
detector	O
compared	O
with	O
our	O
frustum	B-Method
one	E-Method
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
(	O
BV	O
)	O
PointNets	O
Similar	O
to	O
Frustum	B-Method
PointNets	E-Method
that	O
take	O
point	O
cloud	O
in	O
frustum	O
,	O
segment	O
point	O
cloud	O
and	O
estimate	O
amodal	O
bounding	O
box	O
,	O
we	O
can	O
apply	O
PointNets	O
to	O
points	O
in	O
bird	O
’s	O
eye	O
view	O
regions	O
.	O

Since	O
bird	O
’s	O
eye	O
view	O
is	O
based	O
on	O
orthogonal	O
projection	O
,	O
the	O
3D	O
space	O
specified	O
by	O
a	O
BV	O
2D	O
box	O
is	O
a	O
3D	O
cuboid	O
(	O
cut	O
by	O
minimum	O
and	O
maximum	O
height	O
)	O
instead	O
of	O
a	O
frustum	O
.	O

paragraph	O
:	O
Results	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
Ours	O
BV	O
)	O
shows	O
the	O
APs	O
we	O
get	O
by	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
only	O
(	O
without	O
and	O
RGB	O
information	O
)	O
.	O

We	O
compare	O
with	O
two	O
previous	O
LiDAR	B-Method
only	I-Method
methods	E-Method
(	O
VeloFCN	S-Method
and	O
MV3D	S-Method
(	O
BV	B-Method
+	I-Method
FV	I-Method
)	E-Method
)	O
and	O
show	O
that	O
our	O
BV	B-Method
proposal	I-Method
based	I-Method
detector	E-Method
greatly	O
outperforms	O
VeloFCN	S-Method
on	O
all	O
cases	O
and	O
outperforms	O
MV3D	S-Method
(	O
BV	B-Method
+	I-Method
FV	E-Method
)	O
on	O
moderate	O
and	O
hard	O
cases	O
by	O
a	O
significant	O
margin	O
.	O

More	O
importantly	O
,	O
we	O
show	O
in	O
the	O
last	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
that	O
bird	O
’s	O
eye	O
view	O
and	O
RGB	O
view	O
proposals	O
can	O
be	O
combined	O
to	O
achieve	O
an	O
even	O
better	O
performance	O
(	O
3.8	O
%	O
AP	S-Metric
improvement	O
on	O
hard	O
cases	O
)	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
gives	O
an	O
intuitive	O
explanation	O
of	O
why	O
bird	O
’s	O
eye	O
view	O
proposals	O
could	O
help	O
.	O

In	O
the	O
sample	O
frame	O
shown	O
:	O
while	O
our	O
2D	B-Method
detector	E-Method
misses	O
some	O
highly	O
occluded	O
cars	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
left	O
RGB	B-Material
image	E-Material
)	O
,	O
bird	O
’s	O
eye	O
view	O
based	O
RPN	S-Method
successfully	O
detects	O
them	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
blue	O
arrows	O
in	O
right	O
LiDAR	B-Material
image	E-Material
)	O
.	O

appendix	O
:	O
More	O
Experiments	O
(	O
Sec	O
5.2	O
)	O
subsection	O
:	O
Effects	O
of	O
PointNet	B-Method
Architectures	E-Method
Table	O
[	O
reference	O
]	O
compares	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	B-Method
++	I-Method
(	I-Method
v2	E-Method
)	O
architectures	O
for	O
instance	B-Task
segmentation	E-Task
and	O
amodal	B-Task
box	I-Task
estimation	E-Task
.	O

The	O
v2	B-Method
model	E-Method
outperforms	O
v1	B-Method
model	E-Method
on	O
both	O
tasks	O
because	O
1	O
)	O
v2	B-Method
model	E-Method
learns	O
hierarchical	O
features	O
that	O
are	O
richer	O
and	O
more	O
generalizable	O
;	O
2	O
)	O
v2	B-Method
model	E-Method
uses	O
multi	B-Method
-	I-Method
scale	I-Method
feature	I-Method
learning	E-Method
that	O
adapts	O
to	O
varying	O
point	O
densities	O
.	O

Note	O
that	O
the	O
ours	O
(	O
v1	O
)	O
model	O
corresponds	O
to	O
first	O
row	O
of	O
Table	O
[	O
reference	O
]	O
while	O
the	O
ours	O
(	O
v2	O
)	O
links	O
to	O
the	O
last	O
row	O
.	O

subsection	O
:	O
Effects	O
of	O
Training	B-Metric
Data	I-Metric
Size	E-Metric
Recently	O
observed	O
linear	O
improvement	O
in	O
performance	O
of	O
deep	B-Method
learning	I-Method
models	E-Method
with	O
exponential	O
growth	O
of	O
data	O
set	O
size	O
.	O

In	O
our	O
Frustum	B-Method
PointNets	E-Method
we	O
observe	O
similar	O
trend	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
trend	O
indicates	O
a	O
promising	O
performance	O
potential	O
of	O
our	O
methods	O
with	O
larger	O
datasets	O
.	O

We	O
train	O
three	O
separate	O
group	O
of	O
Frustum	B-Method
PointNets	E-Method
on	O
three	O
sets	O
of	O
training	O
data	O
and	O
then	O
evaluate	O
the	O
model	O
on	O
a	O
fixed	O
validation	O
set	O
(	O
1929	O
samples	O
)	O
.	O

The	O
three	O
data	O
points	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
represent	O
training	O
set	O
sizes	O
of	O
1388	O
,	O
2776	O
,	O
5552	O
samples	O
(	O
0.185x	O
,	O
0.371x	O
,	O
0.742x	O
of	O
the	O
entire	O
trainval	O
set	O
)	O
respectively	O
.	O

We	O
augment	O
the	O
training	O
data	O
such	O
that	O
the	O
total	O
amount	O
of	O
samples	O
are	O
the	O
same	O
for	O
each	O
of	O
the	O
three	O
cases	O
(	O
20x	O
,	O
10x	O
and	O
5x	O
augmentation	O
respectively	O
)	O
.	O

The	O
training	O
set	O
and	O
validation	O
set	O
are	O
chosen	O
such	O
that	O
they	O
do	O
n’t	O
share	O
frames	O
from	O
the	O
same	O
video	O
clips	O
.	O

subsection	O
:	O
Runtime	O
and	O
Model	B-Metric
Size	E-Metric
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
decomposed	O
runtime	B-Metric
cost	E-Metric
(	O
inference	B-Metric
time	E-Metric
)	O
for	O
our	O
frustum	B-Method
PointNets	E-Method
(	O
v1	O
and	O
v2	O
)	O
.	O

The	O
evaluation	O
is	O
based	O
on	O
TensorFlow	S-Method
with	O
a	O
NVIDIA	B-Method
GTX	I-Method
1080	E-Method
and	O
a	O
single	O
CPU	O
core	O
.	O

While	O
for	O
v1	B-Method
model	I-Method
frustum	I-Method
proposal	E-Method
(	O
with	O
CNN	B-Method
and	I-Method
backprojection	E-Method
)	O
takes	O
the	O
majority	O
time	O
,	O
for	O
v2	B-Method
model	E-Method
since	O
a	O
PointNet	B-Method
++	I-Method
model	E-Method
with	O
multi	B-Method
-	I-Method
scale	I-Method
grouping	E-Method
is	O
used	O
,	O
computation	O
bottleneck	O
shifts	O
to	O
instance	B-Task
segmentation	E-Task
.	O

Note	O
that	O
we	O
merge	O
batch	B-Method
normalization	E-Method
and	O
FC	B-Method
/	I-Method
convolution	I-Method
layers	E-Method
for	O
faster	O
inference	S-Task
(	O
since	O
they	O
are	O
both	O
linear	O
operation	O
with	O
multiply	O
and	O
sum	O
)	O
,	O
which	O
results	O
in	O
close	O
to	O
50	O
%	O
speedup	O
for	O
inference	S-Task
.	O

CNN	B-Method
model	E-Method
has	O
size	O
28	O
MB	O
.	O

v1	O
PointNets	S-Method
have	O
size	O
19	O
MB	O
.	O

v2	O
PointNets	S-Method
have	O
size	O
22	O
MB	O
.	O

The	O
total	O
size	O
is	O
therefore	O
47	O
MB	O
for	O
v1	B-Method
model	E-Method
and	O
50	O
MB	O
for	O
v2	B-Method
model	E-Method
.	O

appendix	O
:	O
Visualizations	S-Task
for	O
SUN	B-Material
-	I-Material
RGBD	E-Material
(	O
Sec	O
5.1	O
)	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
some	O
representative	O
detection	S-Task
results	O
on	O
SUN	B-Material
-	I-Material
RGBD	E-Material
data	O
.	O

We	O
can	O
see	O
that	O
compared	O
with	O
KITTI	S-Material
LiDAR	O
data	O
,	O
depth	B-Material
images	E-Material
can	O
be	O
popped	O
up	O
to	O
much	O
more	O
dense	O
point	O
clouds	O
.	O

However	O
even	O
with	O
such	O
dense	O
point	O
cloud	O
,	O
strong	O
occlusions	O
of	O
indoor	O
objects	O
as	O
well	O
as	O
the	O
tight	O
arrangement	O
present	O
new	O
challenges	O
for	O
detection	S-Task
in	O
indoor	O
scenes	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
report	O
the	O
3D	O
AP	S-Metric
curves	O
of	O
our	O
Frustum	B-Method
PointNets	E-Method
on	O
SUN	B-Material
-	I-Material
RGBD	E-Material
val	O
set	O
.	O

2D	B-Task
detection	E-Task
APs	O
of	O
our	O
RGB	B-Method
detector	E-Method
are	O
also	O
provided	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

document	O
:	O
Graph	B-Method
-	I-Method
Structured	I-Method
Representations	E-Method
for	O
Visual	B-Task
Question	I-Task
Answering	E-Task
This	O
paper	O
proposes	O
to	O
improve	O
visual	B-Task
question	I-Task
answering	E-Task
(	O
VQA	S-Task
)	O
with	O
structured	B-Method
representations	E-Method
of	O
both	O
scene	O
contents	O
and	O
questions	O
.	O

A	O
key	O
challenge	O
in	O
VQA	S-Task
is	O
to	O
require	O
joint	B-Task
reasoning	E-Task
over	O
the	O
visual	O
and	O
text	O
domains	O
.	O

The	O
predominant	O
CNN	S-Method
/	O
LSTM	O
-	O
based	O
approach	O
to	O
VQA	S-Task
is	O
limited	O
by	O
monolithic	B-Method
vector	I-Method
representations	E-Method
that	O
largely	O
ignore	O
structure	O
in	O
the	O
scene	O
and	O
in	O
the	O
question	O
.	O

CNN	S-Method
feature	O
vectors	O
can	O
not	O
effectively	O
capture	O
situations	O
as	O
simple	O
as	O
multiple	O
object	O
instances	O
,	O
and	O
LSTMs	S-Method
process	O
questions	O
as	O
series	O
of	O
words	O
,	O
which	O
do	O
not	O
reflect	O
the	O
true	O
complexity	O
of	O
language	O
structure	O
.	O

We	O
instead	O
propose	O
to	O
build	O
graphs	O
over	O
the	O
scene	O
objects	O
and	O
over	O
the	O
question	O
words	O
,	O
and	O
we	O
describe	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
that	O
exploits	O
the	O
structure	O
in	O
these	O
representations	O
.	O

We	O
show	O
that	O
this	O
approach	O
achieves	O
significant	O
improvements	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
increasing	O
accuracy	S-Metric
from	O
71.2	O
%	O
to	O
74.4	O
%	O
on	O
the	O
“	O
abstract	O
scenes	O
”	O
multiple	O
-	O
choice	O
benchmark	O
,	O
and	O
from	O
34.7	O
%	O
to	O
39.1	O
%	O
for	O
the	O
more	O
challenging	O
“	O
balanced	O
”	O
scenes	O
,	O
i.e	O
.	O

image	O
pairs	O
with	O
fine	O
-	O
grained	O
differences	O
and	O
opposite	O
yes	O
/	O
no	O
answers	O
to	O
a	O
same	O
question	O
.	O

section	O
:	O
Introduction	O
The	O
task	O
of	O
Visual	B-Task
Question	I-Task
Answering	E-Task
has	O
received	O
growing	O
interest	O
in	O
the	O
recent	O
years	O
(	O
see	O
for	O
example	O
)	O
.	O

One	O
of	O
the	O
more	O
interesting	O
aspects	O
of	O
the	O
problem	O
is	O
that	O
it	O
combines	O
computer	B-Task
vision	E-Task
,	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
and	O
artificial	B-Task
intelligence	E-Task
.	O

In	O
its	O
open	O
-	O
ended	O
form	O
,	O
a	O
question	O
is	O
provided	O
as	O
text	O
in	O
natural	O
language	O
together	O
with	O
an	O
image	O
,	O
and	O
a	O
correct	O
answer	O
must	O
be	O
predicted	O
,	O
typically	O
in	O
the	O
form	O
of	O
a	O
single	O
word	O
or	O
a	O
short	O
phrase	O
.	O

In	O
the	O
multiple	B-Method
-	I-Method
choice	I-Method
variant	E-Method
,	O
an	O
answer	O
is	O
selected	O
from	O
a	O
provided	O
set	O
of	O
candidates	O
,	O
alleviating	O
evaluation	O
issues	O
related	O
to	O
synonyms	O
and	O
paraphrasing	O
.	O

Multiple	O
datasets	O
for	O
VQA	S-Task
have	O
been	O
introduced	O
with	O
either	O
real	O
or	O
synthetic	O
images	O
.	O

Our	O
experiments	O
uses	O
the	O
latter	O
,	O
being	O
based	O
on	O
clip	O
art	O
or	O
“	O
cartoon	O
”	O
images	O
created	O
by	O
humans	O
to	O
depict	O
realistic	O
scenes	O
(	O
they	O
are	O
usually	O
referred	O
to	O
as	O
“	O
abstract	O
scenes	O
”	O
,	O
despite	O
this	O
being	O
a	O
misnomer	O
)	O
.	O

Our	O
experiments	O
focus	O
on	O
this	O
dataset	O
of	O
clip	O
art	O
scenes	O
as	O
they	O
allow	O
to	O
focus	O
on	O
semantic	B-Task
reasoning	E-Task
and	O
vision	B-Task
-	I-Task
language	I-Task
interactions	E-Task
,	O
in	O
isolation	O
from	O
the	O
performance	O
of	O
visual	B-Task
recognition	E-Task
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

They	O
also	O
allow	O
the	O
manipulation	O
of	O
the	O
image	O
data	O
so	O
as	O
to	O
better	O
illuminate	O
algorithm	O
performance	O
.	O

A	O
particularly	O
attractive	O
VQA	S-Task
dataset	O
was	O
introduced	O
in	O
by	O
selecting	O
only	O
the	O
questions	O
with	O
binary	O
answers	O
(	O
e.g	O
.	O

yes	O
/	O
no	O
)	O
and	O
pairing	O
each	O
(	O
synthetic	O
)	O
image	O
with	O
a	O
minimally	O
-	O
different	O
complementary	O
version	O
that	O
elicits	O
the	O
opposite	O
(	O
no	O
/	O
yes	O
)	O
answer	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
bottom	O
rows	O
)	O
.	O

This	O
strongly	O
contrasts	O
with	O
other	O
VQA	S-Task
datasets	O
of	O
real	O
images	O
,	O
where	O
a	O
correct	O
answer	O
is	O
often	O
obvious	O
without	O
looking	O
at	O
the	O
image	O
,	O
by	O
relying	O
on	O
systematic	O
regularities	O
of	O
frequent	O
questions	O
and	O
answers	O
.	O

Performance	O
improvements	O
reported	O
on	O
such	O
datasets	O
are	O
difficult	O
to	O
interpret	O
as	O
actual	O
progress	O
in	O
scene	B-Task
understanding	E-Task
and	O
reasoning	S-Task
as	O
they	O
might	O
similarly	O
be	O
taken	O
to	O
represent	O
a	O
better	O
modeling	O
of	O
the	O
language	O
prior	O
of	O
the	O
dataset	O
.	O

This	O
hampers	O
,	O
or	O
at	O
best	O
obscures	O
,	O
progress	O
toward	O
the	O
greater	O
goal	O
of	O
general	B-Task
VQA	E-Task
.	O

In	O
our	O
view	O
,	O
and	O
despite	O
obvious	O
limitations	O
of	O
synthetic	O
images	O
,	O
improvements	O
on	O
the	O
aforementioned	O
“	O
balanced	O
”	O
dataset	O
constitute	O
an	O
illuminating	O
measure	O
of	O
progress	O
in	O
scene	B-Task
-	I-Task
understanding	E-Task
,	O
because	O
a	O
language	B-Method
model	E-Method
alone	O
can	O
not	O
perform	O
better	O
than	O
chance	O
on	O
this	O
data	O
.	O

paragraph	O
:	O
Challenges	O
The	O
questions	O
in	O
the	O
clip	O
-	O
art	O
dataset	O
vary	O
greatly	O
in	O
their	O
complexity	O
.	O

Some	O
can	O
be	O
directly	O
answered	O
from	O
observations	O
of	O
visual	O
elements	O
,	O
e.g	O
.	O

Is	O
there	O
a	O
dog	O
in	O
the	O
room	O
?	O
,	O
or	O
Is	O
the	O
weather	O
good	O
?	O
.	O

Others	O
require	O
relating	O
multiple	O
facts	O
or	O
understanding	O
complex	O
actions	O
,	O
e.g	O
.	O

Is	O
the	O
boy	O
going	O
to	O
catch	O
the	O
ball	O
?	O
,	O
or	O
Is	O
it	O
winter	O
?	O
.	O

An	O
additional	O
challenge	O
,	O
which	O
affects	O
all	O
VQA	S-Task
datasets	O
,	O
is	O
the	O
sparsity	O
of	O
the	O
training	O
data	O
.	O

Even	O
a	O
large	O
number	O
of	O
training	O
questions	O
(	O
almost	O
25	O
,	O
000	O
for	O
the	O
clip	O
art	O
scenes	O
of	O
)	O
can	O
not	O
possibly	O
cover	O
the	O
combinatorial	O
diversity	O
of	O
possible	O
objects	O
and	O
concepts	O
.	O

Adding	O
to	O
this	O
challenge	O
,	O
most	O
methods	O
for	O
VQA	S-Task
process	O
the	O
question	O
through	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
such	O
as	O
an	O
LSTM	S-Method
)	O
trained	O
from	O
scratch	O
solely	O
on	O
the	O
training	O
questions	O
.	O

paragraph	O
:	O
Language	B-Method
representation	E-Method
The	O
above	O
reasons	O
motivate	O
us	O
to	O
take	O
advantage	O
of	O
the	O
extensive	O
existing	O
work	O
in	O
the	O
natural	O
language	O
community	O
to	O
aid	O
processing	O
the	O
questions	O
.	O

First	O
,	O
we	O
identify	O
the	O
syntactic	O
structure	O
of	O
the	O
question	O
using	O
a	O
dependency	B-Method
parser	E-Method
.	O

This	O
produces	O
a	O
graph	B-Method
representation	E-Method
of	O
the	O
question	O
in	O
which	O
each	O
node	O
represents	O
a	O
word	O
and	O
each	O
edge	O
a	O
particular	O
type	O
of	O
dependency	O
(	O
e.g	O
.	O

determiner	O
,	O
nominal	O
subject	O
,	O
direct	O
object	O
,	O
etc	O
.	O

)	O
.	O

Second	O
,	O
we	O
associate	O
each	O
word	O
(	O
node	O
)	O
with	O
a	O
vector	B-Method
embedding	I-Method
pretrained	E-Method
on	O
large	O
corpora	O
of	O
text	O
data	O
.	O

This	O
embedding	O
maps	O
the	O
words	O
to	O
a	O
space	O
in	O
which	O
distances	O
are	O
semantically	O
meaningful	O
.	O

Consequently	O
,	O
this	O
essentially	O
regularizes	O
the	O
remainder	O
of	O
the	O
network	O
to	O
share	O
learned	O
concepts	O
among	O
related	O
words	O
and	O
synonyms	O
.	O

This	O
particularly	O
helps	O
in	O
dealing	O
with	O
rare	O
words	O
,	O
and	O
also	O
allows	O
questions	O
to	O
include	O
words	O
absent	O
from	O
the	O
training	O
questions	O
/	O
answers	O
.	O

Note	O
that	O
this	O
pretraining	O
and	O
ad	B-Task
hoc	I-Task
processing	E-Task
of	O
the	O
language	B-Task
part	E-Task
mimics	O
a	O
practice	O
common	O
for	O
the	O
image	B-Task
part	E-Task
,	O
in	O
which	O
visual	O
features	O
are	O
usually	O
obtained	O
from	O
a	O
fixed	O
CNN	S-Method
,	O
itself	O
pretrained	O
on	O
a	O
larger	O
dataset	O
and	O
with	O
a	O
different	O
(	O
supervised	B-Metric
classification	I-Metric
)	I-Metric
objective	E-Metric
.	O

paragraph	O
:	O
Scene	B-Method
representation	E-Method
Each	O
object	O
in	O
the	O
scene	O
corresponds	O
to	O
a	O
node	O
in	O
the	O
scene	O
graph	O
,	O
which	O
has	O
an	O
associated	O
feature	O
vector	O
describing	O
its	O
appearance	O
.	O

The	O
graph	O
is	O
fully	O
connected	O
,	O
with	O
each	O
edge	O
representing	O
the	O
relative	O
position	O
of	O
the	O
objects	O
in	O
the	O
image	O
.	O

paragraph	O
:	O
Applying	O
Neural	B-Method
Networks	E-Method
to	O
graphs	O
The	O
two	O
graph	B-Method
representations	E-Method
feed	O
into	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
that	O
we	O
will	O
describe	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
advantage	O
of	O
this	O
approach	O
with	O
text	B-Task
-	I-Task
and	I-Task
scene	I-Task
-	I-Task
graphs	E-Task
,	O
rather	O
than	O
more	O
typical	O
representations	O
,	O
is	O
that	O
the	O
graphs	O
can	O
capture	O
relationships	O
between	O
words	O
and	O
between	O
objects	O
which	O
are	O
of	O
semantic	O
significance	O
.	O

This	O
enables	O
the	O
GNN	S-Method
to	O
exploit	O
(	O
1	O
)	O
the	O
unordered	O
nature	O
of	O
scene	O
elements	O
(	O
the	O
objects	O
in	O
particular	O
)	O
and	O
(	O
2	O
)	O
the	O
semantic	O
relationships	O
between	O
elements	O
(	O
and	O
the	O
grammatical	O
relationships	O
between	O
words	O
in	O
particular	O
)	O
.	O

This	O
contrasts	O
with	O
the	O
typical	O
approach	O
of	O
representing	O
the	O
image	O
with	O
CNN	S-Method
activations	O
(	O
which	O
are	O
sensitive	O
to	O
individual	O
object	O
locations	O
but	O
less	O
so	O
to	O
relative	O
position	O
)	O
and	O
the	O
processing	O
words	O
of	O
the	O
question	O
serially	O
with	O
an	O
RNN	S-Method
(	O
despite	O
the	O
fact	O
that	O
grammatical	O
structure	O
is	O
very	O
non	O
-	O
linear	O
)	O
.	O

The	O
graph	B-Method
representation	E-Method
ignores	O
the	O
order	O
in	O
which	O
elements	O
are	O
processed	O
,	O
but	O
instead	O
represents	O
the	O
relationships	O
between	O
different	O
elements	O
using	O
different	O
edge	O
types	O
.	O

Our	O
network	O
uses	O
multiple	O
layers	O
that	O
iterate	O
over	O
the	O
features	O
associated	O
with	O
every	O
node	O
,	O
then	O
ultimately	O
identifies	O
a	O
soft	O
matching	O
between	O
nodes	O
from	O
the	O
two	O
graphs	O
.	O

This	O
matching	O
reflects	O
the	O
correspondences	O
between	O
the	O
words	O
in	O
the	O
question	O
and	O
the	O
objects	O
in	O
the	O
image	O
.	O

The	O
features	O
of	O
the	O
matched	O
nodes	O
then	O
feed	O
into	O
a	O
classifier	S-Method
to	O
infer	O
the	O
answer	O
to	O
the	O
question	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
four	O
-	O
fold	O
.	O

We	O
describe	O
how	O
to	O
use	O
graph	B-Method
representations	I-Method
of	I-Method
scene	I-Method
and	I-Method
question	E-Method
for	O
VQA	S-Task
,	O
and	O
a	O
neural	B-Method
network	E-Method
capable	O
of	O
processing	O
these	O
representations	O
to	O
infer	O
an	O
answer	O
.	O

We	O
show	O
how	O
to	O
make	O
use	O
of	O
an	O
off	O
-	O
the	O
-	O
shelf	O
language	B-Method
parsing	I-Method
tool	E-Method
by	O
generating	O
a	O
graph	B-Method
representation	I-Method
of	I-Method
text	E-Method
that	O
captures	O
grammatical	O
relationships	O
,	O
and	O
by	O
making	O
this	O
information	O
accessible	O
to	O
the	O
VQA	S-Task
model	O
.	O

This	O
representation	O
uses	O
a	O
pre	O
-	O
trained	O
word	B-Method
embedding	E-Method
to	O
form	O
node	O
features	O
,	O
and	O
encodes	O
syntactic	O
dependencies	O
between	O
words	O
as	O
edge	O
features	O
.	O

We	O
train	O
the	O
proposed	O
model	O
on	O
the	O
VQA	S-Task
“	O
abstract	O
scenes	O
”	O
benchmark	O
and	O
demonstrate	O
its	O
efficacy	O
by	O
raising	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	S-Metric
from	O
71.2	O
%	O
to	O
74.4	O
%	O
in	O
the	O
multiple	B-Task
-	I-Task
choice	I-Task
setting	E-Task
.	O

On	O
the	O
“	O
balanced	O
”	O
version	O
of	O
the	O
dataset	O
,	O
we	O
raise	O
the	O
accuracy	S-Metric
from	O
34.7	O
%	O
to	O
39.1	O
%	O
in	O
the	O
hardest	O
setting	O
(	O
requiring	O
a	O
correct	O
answer	O
over	O
pairs	O
of	O
scenes	O
)	O
.	O

We	O
evaluate	O
the	O
uncertainty	O
in	O
the	O
model	O
by	O
presenting	O
–	O
for	O
the	O
first	O
time	O
on	O
the	O
task	O
of	O
VQA	S-Task
–	O
precision	B-Metric
/	I-Metric
recall	I-Metric
curves	E-Metric
of	O
predicted	O
answers	O
.	O

Those	O
curves	O
provide	O
more	O
insight	O
than	O
the	O
single	O
accuracy	B-Metric
metric	E-Metric
and	O
show	O
that	O
the	O
uncertainty	O
estimated	O
by	O
the	O
model	O
about	O
its	O
predictions	O
correlates	O
with	O
the	O
ambiguity	O
of	O
the	O
human	O
-	O
provided	O
ground	O
truth	O
.	O

section	O
:	O
Related	O
work	O
The	O
task	O
of	O
visual	B-Task
question	I-Task
answering	E-Task

has	O
received	O
increasing	O
interest	O
since	O
the	O
seminal	O
paper	O
of	O
Antol	O
et	O
al	O
.	O

.	O


Most	O
recent	O
methods	O
are	O
based	O
on	O
the	O
idea	O
of	O
a	O
joint	B-Method
embedding	E-Method
of	O
the	O
image	O
and	O
the	O
question	O
using	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
.	O

The	O
image	O
is	O
passed	O
through	O
a	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
pretrained	O
for	O
image	B-Task
classification	E-Task
,	O
from	O
which	O
intermediate	O
features	O
are	O
extracted	O
to	O
describe	O
the	O
image	O
.	O

The	O
question	O
is	O
typically	O
passed	O
through	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
such	O
as	O
an	O
LSTM	S-Method
,	O
which	O
produces	O
a	O
fixed	B-Method
-	I-Method
size	I-Method
vector	E-Method
representing	O
the	O
sequence	O
of	O
words	O
.	O

These	O
two	O
representations	O
are	O
mapped	O
to	O
a	O
joint	O
space	O
by	O
one	O
or	O
several	O
non	B-Method
-	I-Method
linear	I-Method
layers	E-Method
.	O

They	O
can	O
then	O
be	O
fed	O
into	O
a	O
classifier	S-Method
over	O
an	O
output	O
vocabulary	O
,	O
predicting	O
the	O
final	O
answer	O
.	O

Most	O
recent	O
papers	O
on	O
VQA	S-Task
propose	O
improvements	O
and	O
variations	O
on	O
this	O
basic	O
idea	O
.	O

Consult	O
for	O
a	O
survey	O
.	O

A	O
major	O
improvement	O
to	O
the	O
basic	O
method	O
is	O
to	O
use	O
an	O
attention	B-Method
mechanism	E-Method
.	O

It	O
models	O
interactions	O
between	O
specific	O
parts	O
of	O
the	O
inputs	O
(	O
image	O
and	O
question	O
)	O
depending	O
on	O
their	O
actual	O
contents	O
.	O

The	O
visual	O
input	O
is	O
then	O
typically	O
represented	O
a	O
spatial	B-Method
feature	I-Method
map	E-Method
,	O
instead	O
of	O
holistic	O
,	O
image	O
-	O
wide	O
features	O
.	O

The	O
feature	B-Method
map	E-Method
is	O
used	O
with	O
the	O
question	O
to	O
determine	O
spatial	O
weights	O
that	O
reflect	O
the	O
most	O
relevant	O
regions	O
of	O
the	O
image	O
.	O

Our	O
approach	O
uses	O
a	O
similar	O
weighting	B-Method
operation	E-Method
,	O
which	O
,	O
with	O
our	O
graph	B-Method
representation	E-Method
,	O
we	O
equate	O
to	O
a	O
subgraph	B-Method
matching	E-Method
.	O

Graph	O
nodes	O
representing	O
question	O
words	O
are	O
associated	O
with	O
graph	O
nodes	O
representing	O
scene	O
objects	O
and	O
vice	O
versa	O
.	O

Similarly	O
,	O
the	O
co	B-Method
-	I-Method
attention	I-Method
model	E-Method
of	O
Lu	O
et	O
al	O
.	O

determines	O
attention	O
weights	O
on	O
both	O
image	O
regions	O
and	O
question	O
words	O
.	O

Their	O
best	O
-	O
performing	O
approach	O
proceeds	O
in	O
a	O
sequential	O
manner	O
,	O
starting	O
with	O
question	B-Task
-	I-Task
guided	I-Task
visual	I-Task
attention	E-Task
followed	O
by	O
image	B-Method
-	I-Method
guided	I-Method
question	I-Method
attention	E-Method
.	O

In	O
our	O
case	O
,	O
we	O
found	O
that	O
a	O
joint	B-Method
,	I-Method
one	I-Method
-	I-Method
pass	I-Method
version	E-Method
performs	O
better	O
.	O

A	O
major	O
contribution	O
of	O
our	O
model	O
is	O
to	O
use	O
structured	B-Method
representations	E-Method
of	O
the	O
input	O
scene	O
and	O
the	O
question	O
.	O

This	O
contrasts	O
with	O
typical	O
CNN	S-Method
and	O
RNN	S-Method
models	O
which	O
are	O
limited	O
to	O
spatial	O
feature	O
maps	O
and	O
sequences	O
of	O
words	O
respectively	O
.	O

The	O
dynamic	B-Method
memory	I-Method
networks	E-Method
(	O
DMN	S-Method
)	O
,	O
applied	O
to	O
VQA	S-Task
in	O
also	O
maintain	O
a	O
set	B-Method
-	I-Method
like	I-Method
representation	E-Method
of	O
the	O
input	O
.	O

As	O
in	O
our	O
model	O
,	O
the	O
DMN	B-Method
models	I-Method
interactions	E-Method
between	O
different	O
parts	O
of	O
the	O
input	O
.	O

Our	O
method	O
can	O
additionally	O
take	O
,	O
as	O
input	O
,	O
features	O
characterizing	O
arbitrary	O
relations	O
between	O
parts	O
of	O
the	O
input	O
(	O
the	O
edge	O
features	O
in	O
our	O
graphs	O
)	O
.	O

This	O
specifically	O
allows	O
making	O
use	O
of	O
syntactic	O
dependencies	O
between	O
words	O
after	O
pre	O
-	O
parsing	O
the	O
question	O
.	O

Most	O
VQA	S-Task
systems	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
from	O
questions	O
and	O
images	O
to	O
answers	O
,	O
with	O
the	O
exception	O
of	O
the	O
visual	B-Method
feature	I-Method
extractor	E-Method
,	O
which	O
is	O
typically	O
a	O
CNN	S-Method
pretrained	O
for	O
image	B-Task
classification	E-Task
.	O

For	O
the	O
language	B-Task
processing	I-Task
part	E-Task
,	O
some	O
methods	O
address	O
the	O
the	O
semantic	O
aspect	O
with	O
word	B-Method
embeddings	E-Method
pretrained	O
on	O
a	O
language	B-Task
modeling	I-Task
task	E-Task
(	O
e.g	O
.	O

)	O
.	O

The	O
syntactic	O
relationships	O
between	O
the	O
words	O
in	O
the	O
question	O
are	O
typically	O
overlooked	O
,	O
however	O
.	O

In	O
,	O
hand	O
-	O
designed	O
rules	O
serve	O
to	O
identify	O
primary	O
and	O
secondary	O
objects	O
of	O
the	O
questions	O
.	O

In	O
the	O
Neural	B-Method
Module	I-Method
Networks	E-Method
,	O
the	O
question	O
is	O
processed	O
by	O
a	O
dependency	B-Method
parser	E-Method
,	O
and	O
fragments	O
of	O
the	O
parse	O
,	O
selected	O
with	O
ad	O
hoc	O
fixed	O
rules	O
are	O
associated	O
with	O
modules	O
,	O
are	O
assembled	O
into	O
a	O
full	B-Method
neural	I-Method
network	E-Method
.	O

In	O
contrast	O
,	O
our	O
method	O
is	O
trained	O
to	O
make	O
direct	O
use	O
of	O
the	O
output	O
of	O
a	O
syntactic	B-Method
parser	E-Method
.	O

Neural	B-Method
networks	E-Method
on	O
graphs	O
have	O
received	O
significant	O
attention	O
recently	O
.	O

The	O
approach	O
most	O
similar	O
to	O
ours	O
is	O
the	O
Gated	B-Method
Graph	I-Method
Sequence	I-Method
Neural	I-Method
Network	E-Method
,	O
which	O
associate	O
a	O
gated	B-Method
recurrent	I-Method
unit	E-Method
(	O
GRU	S-Method
)	O
to	O
each	O
node	O
,	O
and	O
updates	O
the	O
feature	O
vector	O
of	O
each	O
node	O
by	O
iteratively	O
passing	O
messages	O
between	O
neighbours	O
.	O

Also	O
related	O
is	O
the	O
work	O
of	O
Vinyals	O
et	O
al	O
.	O

for	O
embedding	O
a	O
set	O
into	O
fixed	O
-	O
size	O
vector	O
,	O
invariant	O
to	O
the	O
order	O
of	O
its	O
elements	O
.	O

They	O
do	O
so	O
by	O
feeding	O
the	O
entire	O
set	O
through	O
a	O
recurrent	B-Method
unit	E-Method
multiple	O
times	O
.	O

Each	O
iteration	O
uses	O
an	O
attention	B-Method
mechanism	E-Method
to	O
focus	O
on	O
different	O
parts	O
of	O
the	O
set	O
.	O

Our	O
formulation	O
similarly	O
incorporates	O
information	O
from	O
neighbours	O
into	O
each	O
node	O
feature	O
over	O
multiple	O
iterations	O
,	O
but	O
we	O
did	O
not	O
find	O
any	O
advantage	O
in	O
using	O
an	O
attention	B-Method
mechanism	E-Method
within	O
the	O
recurrent	B-Method
unit	E-Method
.	O

section	O
:	O
Graph	B-Task
representation	I-Task
of	I-Task
scenes	I-Task
and	I-Task
questions	E-Task
The	O
input	O
data	O
for	O
each	O
training	O
or	O
test	O
instance	O
is	O
a	O
question	O
,	O
and	O
a	O
parameterized	O
description	O
of	O
contents	O
of	O
the	O
scene	O
.	O

The	O
question	O
is	O
processed	O
with	O
the	O
Stanford	B-Method
dependency	I-Method
parser	E-Method
,	O
which	O
outputs	O
the	O
following	O
.	O

A	O
set	O
of	O
words	O
that	O
constitute	O
the	O
nodes	O
of	O
the	O
question	O
graph	O
.	O

Each	O
word	O
is	O
represented	O
by	O
its	O
index	O
in	O
the	O
input	O
vocabulary	O
,	O
a	O
token	O
(	O
)	O
.	O

A	O
set	O
of	O
pairwise	O
relations	O
between	O
words	O
,	O
which	O
constitute	O
the	O
edges	O
of	O
our	O
graph	O
.	O

An	O
edge	O
between	O
words	O
and	O
is	O
represented	O
by	O
,	O
an	O
index	O
among	O
the	O
possible	O
types	O
of	O
dependencies	O
.	O

The	O
dataset	O
provides	O
the	O
following	O
information	O
about	O
the	O
image	O
A	O
set	O
of	O
objects	O
that	O
constitute	O
the	O
nodes	O
of	O
the	O
scene	O
graph	O
.	O

Each	O
node	O
is	O
represented	O
by	O
a	O
vector	O
of	O
visual	O
features	O
(	O
)	O
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
implementation	O
details	O
.	O

A	O
set	O
of	O
pairwise	O
relations	O
between	O
all	O
objects	O
.	O

They	O
form	O
the	O
edges	O
of	O
a	O
fully	O
-	O
connected	O
graph	O
of	O
the	O
scene	O
.	O

The	O
edge	O
between	O
objects	O
and	O
is	O
represented	O
by	O
a	O
vector	O
that	O
encodes	O
relative	O
spatial	O
relationships	O
(	O
see	O
supp	O
.	O

mat	O
.	O

)	O
.	O

Our	O
experiments	O
are	O
carried	O
out	O
on	O
datasets	O
of	O
clip	O
art	O
scenes	O
,	O
in	O
which	O
descriptions	O
of	O
the	O
scenes	O
are	O
provided	O
in	O
the	O
form	O
of	O
lists	O
of	O
objects	O
with	O
their	O
visual	O
features	O
.	O

The	O
method	O
is	O
equally	O
applicable	O
to	O
real	O
images	O
,	O
with	O
the	O
object	O
list	O
replaced	O
by	O
candidate	O
object	O
detections	O
.	O

Our	O
experiments	O
on	O
clip	B-Task
art	E-Task
allows	O
the	O
effect	O
of	O
the	O
proposed	O
method	O
to	O
be	O
isolated	O
from	O
the	O
performance	O
of	O
the	O
object	B-Method
detector	E-Method
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
implementation	O
details	O
.	O

The	O
features	O
of	O
all	O
nodes	O
and	O
edges	O
are	O
projected	O
to	O
a	O
vector	O
space	O
of	O
common	O
dimension	O
(	O
typically	O
=	O
300	O
)	O
.	O

The	O
question	O
nodes	O
and	O
edges	O
use	O
vector	B-Method
embeddings	E-Method
implemented	O
as	O
look	O
-	O
up	O
tables	O
,	O
and	O
the	O
scene	O
nodes	O
and	O
edges	O
use	O
affine	B-Method
projections	E-Method
:	O
with	O
the	O
word	B-Method
embedding	E-Method
(	O
usually	O
pretrained	O
,	O
see	O
supplementary	O
material	O
)	O
,	O
the	O
embedding	O
of	O
dependencies	O
,	O
and	O
weight	O
matrices	O
,	O
and	O
and	O
biases	O
.	O

section	O
:	O
Processing	B-Task
graphs	E-Task
with	O
neural	B-Method
networks	E-Method
We	O
now	O
describe	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
suitable	O
for	O
processing	O
the	O
question	B-Task
and	I-Task
scene	I-Task
graphs	E-Task
to	O
infer	O
an	O
answer	O
.	O

See	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
an	O
overview	O
.	O

The	O
two	O
graphs	O
representing	O
the	O
question	O
and	O
the	O
scene	O
are	O
processed	O
independently	O
in	O
a	O
recurrent	B-Method
architecture	E-Method
.	O

We	O
drop	O
the	O
exponents	O
and	O
for	O
this	O
paragraph	O
as	O
the	O
same	O
procedure	O
applies	O
to	O
both	O
graphs	O
.	O

Each	O
node	O
is	O
associated	O
with	O
a	O
gated	B-Method
recurrent	I-Method
unit	E-Method
(	O
GRU	S-Method
)	O
and	O
processed	O
over	O
a	O
fixed	O
number	O
of	O
iterations	O
(	O
typically	O
=	O
4	O
)	O
:	O
Square	O
brackets	O
with	O
a	O
semicolon	O
represent	O
a	O
concatenation	O
of	O
vectors	O
,	O
and	O
the	O
Hadamard	B-Method
(	I-Method
element	I-Method
-	I-Method
wise	I-Method
)	I-Method
product	E-Method
.	O

The	O
final	O
state	O
of	O
the	O
GRU	S-Method
is	O
used	O
as	O
the	O
new	O
representation	O
of	O
the	O
nodes	O
:	O
.	O

The	O
operation	O
transforms	O
features	O
from	O
a	O
variable	O
number	O
of	O
neighbours	O
(	O
i.e	O
.	O

connected	O
nodes	O
)	O
to	O
a	O
fixed	B-Method
-	I-Method
size	I-Method
representation	E-Method
.	O

Any	O
commutative	O
operation	O
can	O
be	O
used	O
(	O
e.g	O
.	O

sum	O
,	O
maximum	O
)	O
.	O

In	O
our	O
implementation	O
,	O
we	O
found	O
the	O
best	O
performance	O
with	O
the	O
average	B-Method
function	E-Method
,	O
taking	O
care	O
of	O
averaging	O
over	O
the	O
variable	O
number	O
of	O
connected	O
neighbours	O
.	O

An	O
intuitive	O
interpretation	O
of	O
the	O
recurrent	B-Method
processing	E-Method
is	O
to	O
progressively	O
integrate	O
context	O
information	O
from	O
connected	O
neighbours	O
into	O
each	O
node	O
’s	O
own	O
representation	O
.	O

A	O
node	O
corresponding	O
to	O
the	O
word	O
’	O
ball	O
’	O
,	O
for	O
instance	O
,	O
might	O
thus	O
incorporate	O
the	O
fact	O
that	O
the	O
associated	O
adjective	O
is	O
’	O
red	O
’	O
.	O

Our	O
formulation	O
is	O
similar	O
but	O
slightly	O
different	O
from	O
the	O
gated	B-Method
graph	I-Method
networks	E-Method
,	O
as	O
the	O
propagation	O
of	O
information	O
in	O
our	O
model	O
is	O
limited	O
to	O
the	O
first	O
order	O
.	O

Note	O
that	O
our	O
graphs	O
are	O
typically	O
densely	O
connected	O
.	O

We	O
now	O
introduce	O
a	O
form	O
of	O
attention	O
into	O
the	O
model	O
,	O
which	O
constitutes	O
an	O
essential	O
part	O
of	O
the	O
model	O
.	O

The	O
motivation	O
is	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
to	O
identify	O
parts	O
of	O
the	O
input	O
data	O
most	O
relevant	O
to	O
produce	O
the	O
answer	O
and	O
(	O
2	O
)	O
to	O
align	O
specific	O
words	O
in	O
the	O
question	O
with	O
particular	O
elements	O
of	O
the	O
scene	O
.	O

Practically	O
,	O
we	O
estimate	O
the	O
relevance	O
of	O
each	O
possible	O
pairwise	O
combination	O
of	O
words	O
and	O
objects	O
.	O

More	O
precisely	O
,	O
we	O
compute	O
scalar	O
“	O
matching	O
weights	O
”	O
between	O
node	O
sets	O
and	O
.	O

These	O
weights	O
are	O
comparable	O
to	O
the	O
“	O
attention	O
weights	O
”	O
in	O
other	O
models	O
(	O
e.g	O
.	O

)	O
.	O

Therefore	O
,	O
:	O
where	O
and	O
are	O
learned	O
weights	O
and	O
biases	O
,	O
and	O
the	O
logistic	B-Method
function	E-Method
that	O
introduces	O
a	O
non	O
-	O
linearity	O
and	O
bounds	O
the	O
weights	O
to	O
.	O

The	O
formulation	O
is	O
similar	O
to	O
a	O
cosine	O
similarity	O
with	O
learned	O
weights	O
on	O
the	O
feature	O
dimensions	O
.	O

Note	O
that	O
the	O
weights	O
are	O
computed	O
using	O
the	O
initial	O
embedding	O
of	O
the	O
node	O
features	O
(	O
pre	B-Method
-	I-Method
GRU	E-Method
)	O
.	O

We	O
apply	O
the	O
scalar	O
weights	O
to	O
the	O
corresponding	O
pairwise	O
combinations	O
of	O
question	O
and	O
scene	O
features	O
,	O
thereby	O
focusing	O
and	O
giving	O
more	O
importance	O
to	O
the	O
matched	O
pairs	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
sum	O
the	O
weighted	O
features	O
over	O
the	O
scene	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
then	O
over	O
the	O
question	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
,	O
interleaving	O
the	O
sums	O
with	O
affine	O
projections	O
and	O
non	O
-	O
linearities	O
to	O
obtain	O
a	O
final	O
prediction	S-Task
:	O
with	O
,	O
,	O
,	O
learned	O
weights	O
and	O
biases	O
,	O
a	O
ReLU	S-Method
,	O
and	O
a	O
softmax	S-Method
or	O
a	O
logistic	B-Method
function	E-Method
(	O
see	O
experiments	O
,	O
Section	O
[	O
reference	O
]	O
)	O
.	O

The	O
summations	O
over	O
the	O
scene	O
elements	O
and	O
question	O
elements	O
is	O
a	O
form	O
of	O
pooling	S-Method
that	O
brings	O
the	O
variable	O
number	O
of	O
features	O
(	O
due	O
to	O
the	O
variable	O
number	O
of	O
words	O
and	O
objects	O
in	O
the	O
input	O
)	O
to	O
a	O
fixed	O
-	O
size	O
output	O
.	O

The	O
final	O
output	O
vector	O
contains	O
scores	O
for	O
the	O
possible	O
answers	O
,	O
and	O
has	O
a	O
number	O
of	O
dimensions	O
equal	O
to	O
2	O
for	O
the	O
binary	O
questions	O
of	O
the	O
“	O
balanced	O
”	O
dataset	O
,	O
or	O
to	O
the	O
number	O
of	O
all	O
candidate	O
answers	O
in	O
the	O
“	O
abstract	B-Material
scenes	I-Material
”	I-Material
dataset	E-Material
.	O

The	O
candidate	O
answers	O
are	O
those	O
appearing	O
at	O
least	O
times	O
in	O
the	O
training	O
set	O
(	O
see	O
supplementary	O
material	O
for	O
details	O
)	O
.	O

section	O
:	O
Evaluation	O
paragraph	O
:	O
Datasets	O
Our	O
evaluation	O
uses	O
two	O
datasets	O
:	O
the	O
original	O
“	O
abstract	O
scenes	O
”	O
from	O
Antol	O
et	O
al	O
.	O

and	O
its	O
“	O
balanced	O
”	O
extension	O
from	O
.	O

They	O
both	O
contain	O
scenes	O
created	O
by	O
humans	O
in	O
a	O
drag	O
-	O
and	O
-	O
drop	O
interface	O
for	O
arranging	O
clip	O
art	O
objects	O
and	O
figures	O
.	O

The	O
original	O
dataset	O
contains	O
scenes	O
(	O
for	O
training	O
validation	O
test	O
respectively	O
)	O
and	O
questions	O
,	O
each	O
with	O
10	O
human	O
-	O
provided	O
ground	O
-	O
truth	O
answers	O
.	O

Questions	O
are	O
categorized	O
based	O
on	O
the	O
type	O
of	O
the	O
correct	O
answer	O
into	O
yes	O
/	O
no	O
,	O
number	O
,	O
and	O
other	O
,	O
but	O
the	O
same	O
method	O
is	O
used	O
for	O
all	O
categories	O
,	O
the	O
type	O
of	O
the	O
test	O
questions	O
being	O
unknown	O
.	O

The	O
“	O
balanced	O
”	O
version	O
of	O
the	O
dataset	O
contains	O
only	O
the	O
subset	O
of	O
questions	O
which	O
have	O
binary	O
(	O
yes	O
/	O
no	O
)	O
answers	O
and	O
,	O
in	O
addition	O
,	O
complementary	O
scenes	O
created	O
to	O
elicit	O
the	O
opposite	O
answer	O
to	O
each	O
question	O
.	O

This	O
is	O
significant	O
because	O
guessing	O
the	O
modal	O
answer	O
from	O
the	O
training	O
set	O
will	O
the	O
succeed	O
only	O
half	O
of	O
the	O
time	O
(	O
slightly	O
more	O
than	O
in	O
practice	O
because	O
of	O
disagreement	O
between	O
annotators	O
)	O
and	O
give	O
accuracy	S-Metric
over	O
complementary	O
pairs	O
.	O

This	O
contrasts	O
with	O
other	O
VQA	S-Task
datasets	O
where	O
blind	B-Task
guessing	E-Task
can	O
be	O
very	O
effective	O
.	O

The	O
pairs	O
of	O
complementary	O
scenes	O
also	O
typically	O
differ	O
by	O
only	O
one	O
or	O
two	O
objects	O
being	O
displaced	O
,	O
removed	O
,	O
or	O
slightly	O
modified	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
bottom	O
rows	O
)	O
.	O

This	O
makes	O
the	O
questions	O
very	O
challenging	O
by	O
requiring	O
to	O
take	O
into	O
account	O
subtle	O
details	O
of	O
the	O
scenes	O
.	O

paragraph	O
:	O
Metrics	O
The	O
main	O
metric	O
is	O
the	O
average	O
“	O
VQA	S-Task
score	O
”	O
,	O
which	O
is	O
a	O
soft	B-Metric
accuracy	E-Metric
that	O
takes	O
into	O
account	O
variability	O
of	O
ground	O
truth	O
answers	O
from	O
multiple	O
human	O
annotators	O
.	O

Let	O
us	O
refer	O
to	O
a	O
test	O
question	O
by	O
an	O
index	O
,	O
and	O
to	O
each	O
possible	O
answer	O
in	O
the	O
output	O
vocabulary	O
by	O
an	O
index	O
.	O

The	O
ground	B-Metric
truth	I-Metric
score	E-Metric
if	O
the	O
answer	O
was	O
provided	O
by	O
annotators	O
.	O

Otherwise	O
,	O
.	O

Our	O
method	O
outputs	O
a	O
predicted	O
score	O
for	O
each	O
question	O
and	O
answer	O
(	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
overall	O
accuracy	S-Metric
is	O
the	O
average	B-Metric
ground	I-Metric
truth	I-Metric
score	E-Metric

of	O
the	O
highest	O
prediction	O
per	O
question	O
,	O
i.e	O
.	O

.	O


It	O
has	O
been	O
argued	O
that	O
the	O
“	O
balanced	O
”	O
dataset	O
can	O
better	O
evaluate	O
a	O
method	O
’s	O
level	O
of	O
visual	B-Task
understanding	E-Task
than	O
other	O
datasets	O
,	O
because	O
it	O
is	O
less	O
susceptible	O
to	O
the	O
use	O
of	O
language	O
priors	O
and	O
dataset	O
regularities	O
(	O
i.e	O
.	O

guessing	O
from	O
the	O
question	O
)	O
.	O

Our	O
initial	O
experiments	O
confirmed	O
that	O
the	O
performances	O
of	O
various	O
algorithms	O
on	O
the	O
balanced	O
dataset	O
were	O
indeed	O
better	O
separated	O
,	O
and	O
we	O
used	O
it	O
for	O
our	O
ablative	B-Task
analysis	E-Task
.	O

We	O
also	O
focus	O
on	O
the	O
hardest	O
evaluation	B-Metric
setting	E-Metric
,	O
which	O
measures	O
the	O
accuracy	S-Metric
over	O
pairs	O
of	O
complementary	O
scenes	O
.	O

This	O
is	O
the	O
only	O
metric	O
in	O
which	O
blind	B-Method
models	E-Method
(	O
guessing	O
from	O
the	O
question	O
)	O
obtain	O
null	B-Metric
accuracy	E-Metric
.	O

This	O
setting	O
also	O
does	O
not	O
consider	O
pairs	O
of	O
test	O
scenes	O
deemed	O
ambiguous	O
because	O
of	O
disagreement	O
between	O
annotators	O
.	O

Each	O
test	O
scene	O
is	O
still	O
evaluated	O
independently	O
however	O
,	O
so	O
the	O
model	O
is	O
unable	O
to	O
increase	O
performance	O
by	O
forcing	O
opposite	O
answers	O
to	O
pairs	O
of	O
questions	O
.	O

The	O
metric	O
is	O
then	O
a	O
standard	O
“	O
hard	B-Metric
”	I-Metric
accuracy	E-Metric
,	O
i.e	O
.	O

all	O
ground	B-Metric
truth	I-Metric
scores	E-Metric
.	O

Please	O
refer	O
to	O
the	O
supplementary	O
material	O
for	O
additional	O
details	O
.	O

subsection	O
:	O
Evaluation	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
We	O
compare	O
our	O
method	O
against	O
the	O
three	O
models	O
proposed	O
in	O
.	O

They	O
all	O
use	O
an	O
ensemble	B-Method
of	I-Method
models	E-Method
exploiting	O
either	O
an	O
LSTM	S-Method
for	O
processing	O
the	O
question	O
,	O
or	O
an	O
elaborate	O
set	O
of	O
hand	O
-	O
designed	O
rules	O
to	O
identify	O
two	O
objects	O
as	O
the	O
focus	O
of	O
the	O
question	O
.	O

The	O
visual	O
features	O
in	O
the	O
three	O
models	O
are	O
respectively	O
empty	O
(	O
blind	B-Method
model	E-Method
)	O
,	O
global	O
(	O
scene	O
-	O
wide	O
)	O
,	O
or	O
focused	O
on	O
the	O
two	O
objects	O
identified	O
from	O
the	O
question	O
.	O

These	O
models	O
are	O
specifically	O
designed	O
for	O
binary	B-Task
questions	E-Task
,	O
whereas	O
ours	O
is	O
generally	O
applicable	O
.	O

Nevertheless	O
,	O
we	O
obtain	O
significantly	O
better	O
accuracy	S-Metric
than	O
all	O
three	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Differences	O
in	O
performance	O
are	O
mostly	O
visible	O
in	O
the	O
“	O
pairs	O
”	O
setting	O
,	O
which	O
we	O
believe	O
is	O
more	O
reliable	O
as	O
it	O
discards	O
ambiguous	O
test	O
questions	O
on	O
which	O
human	O
annotators	O
disagreed	O
.	O

During	O
training	O
,	O
we	O
take	O
care	O
to	O
keep	O
pairs	O
of	O
complementary	O
scenes	O
together	O
when	O
forming	O
mini	O
-	O
batches	O
.	O

This	O
has	O
a	O
significant	O
positive	O
effect	O
on	O
the	O
stability	S-Metric
of	O
the	O
optimization	S-Task
.	O

Interestingly	O
,	O
we	O
did	O
not	O
notice	O
any	O
tendency	O
toward	O
overfitting	O
when	O
training	O
on	O
balanced	O
scenes	O
.	O

We	O
hypothesize	O
that	O
the	O
pairs	O
of	O
complementary	O
scenes	O
have	O
a	O
strong	O
regularizing	O
effect	O
that	O
force	O
the	O
learned	O
model	O
to	O
focus	O
on	O
relevant	O
details	O
of	O
the	O
scenes	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
and	O
in	O
the	O
supplementary	O
material	O
)	O
,	O
we	O
visualize	O
the	O
matching	O
weights	O
between	O
question	O
words	O
and	O
scene	O
objects	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

As	O
expected	O
,	O
these	O
tend	O
to	O
be	O
larger	O
between	O
semantically	O
related	O
elements	O
(	O
e.g	O
.	O

daytime	O
sun	O
,	O
dog	O
puppy	O
,	O
boy	O
human	O
)	O
although	O
some	O
are	O
more	O
difficult	O
to	O
interpret	O
.	O

Our	O
best	O
performance	O
of	O
about	O
is	O
still	O
low	O
in	O
absolute	O
terms	O
,	O
which	O
is	O
understandable	O
from	O
the	O
wide	O
range	O
of	O
concepts	O
involved	O
in	O
the	O
questions	O
(	O
see	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
in	O
the	O
supplementary	O
material	O
)	O
.	O

It	O
seems	O
unlikely	O
that	O
these	O
concepts	O
could	O
be	O
learned	O
from	O
training	O
question	O
/	O
answers	O
alone	O
,	O
and	O
we	O
suggest	O
that	O
any	O
further	O
significant	O
improvement	O
in	O
performance	O
will	O
require	O
external	O
sources	O
of	O
information	O
at	O
training	O
and	O
/	O
or	O
test	O
time	O
.	O

paragraph	O
:	O
Ablative	B-Task
evaluation	E-Task
We	O
evaluated	O
variants	O
of	O
our	O
model	O
to	O
measure	O
the	O
impact	O
of	O
various	O
design	O
choices	O
(	O
see	O
numbered	O
rows	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

On	O
the	O
question	O
side	O
,	O
we	O
evaluate	O
(	O
row	O
1	O
)	O
our	O
graph	B-Method
approach	E-Method
without	O
syntactic	B-Method
parsing	E-Method
,	O
building	O
question	B-Method
graphs	E-Method
with	O
only	O
two	O
types	O
of	O
edges	O
,	O
previous	O
/	O
next	O
and	O
linking	O
consecutive	O
nodes	O
.	O

This	O
shows	O
the	O
advantage	O
of	O
using	O
the	O
graph	B-Method
method	E-Method
together	O
with	O
syntactic	B-Method
parsing	E-Method
.	O

Optimizing	O
the	O
word	O
embeddings	O
from	O
scratch	O
(	O
row	O
2	O
)	O
rather	O
than	O
from	O
pretrained	O
Glove	O
vectors	O
produces	O
a	O
significant	O
drop	O
in	O
performance	O
.	O

On	O
the	O
scene	O
side	O
,	O
we	O
removed	O
the	O
edge	O
features	O
(	O
row	O
3	O
)	O
by	O
setting	O
.	O

It	O
confirms	O
that	O
the	O
model	O
makes	O
use	O
of	O
the	O
spatial	O
relations	O
between	O
objects	O
encoded	O
by	O
the	O
edges	O
of	O
the	O
graph	O
.	O

In	O
rows	O
4–6	O
,	O
we	O
disabled	O
the	O
recurrent	B-Method
graph	I-Method
processing	E-Method
(	O
)	O
for	O
the	O
either	O
the	O
question	O
,	O
the	O
scene	O
,	O
or	O
both	O
.	O

We	O
finally	O
tested	O
the	O
model	O
with	O
uniform	O
matching	O
weights	O
(	O
,	O
row	O
10	O
)	O
.	O

As	O
expected	O
,	O
it	O
performed	O
poorly	O
.	O

Our	O
weights	O
act	O
similarly	O
to	O
the	O
attention	B-Method
mechanisms	E-Method
in	O
other	O
models	O
(	O
e.g	O
.	O

)	O
and	O
our	O
observations	O
confirm	O
that	O
such	O
mechanisms	O
are	O
crucial	O
for	O
good	O
performance	O
.	O

paragraph	O
:	O
Precision	S-Metric
/	O
recall	S-Metric
We	O
are	O
interested	O
in	O
assessing	O
the	O
confidence	O
of	O
our	O
model	O
in	O
its	O
predicted	O
answers	O
.	O

Most	O
existing	O
VQA	S-Task
methods	O
treat	O
the	O
answering	S-Task
as	O
a	O
hard	O
classification	S-Task
over	O
candidate	O
answers	O
,	O
and	O
almost	O
all	O
reported	O
results	O
consist	O
of	O
a	O
single	O
accuracy	B-Metric
metric	E-Metric
.	O

To	O
provide	O
more	O
insight	O
,	O
we	O
produce	O
precision	B-Metric
/	I-Metric
recall	I-Metric
curves	E-Metric
for	O
predicted	O
answers	O
.	O

A	O
precision	B-Metric
/	I-Metric
recall	I-Metric
point	E-Metric
is	O
obtained	O
by	O
setting	O
a	O
threshold	O
on	O
predicted	O
scores	O
such	O
that	O
where	O
is	O
the	O
indicator	O
function	O
.	O

We	O
plot	O
precision	B-Metric
/	I-Metric
recall	I-Metric
curves	E-Metric
in	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
both	O
datasets	O
.	O

The	O
predicted	B-Metric
score	E-Metric
proves	O
to	O
be	O
a	O
reliable	O
indicator	O
of	O
the	O
model	B-Metric
confidence	E-Metric
,	O
as	O
a	O
low	O
threshold	O
can	O
achieve	O
near	O
-	O
perfect	O
accuracy	S-Metric
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
left	O
and	O
middle	O
)	O
by	O
filtering	O
out	O
harder	O
and	O
/	O
or	O
ambiguous	O
test	O
cases	O
.	O

We	O
compare	O
models	O
trained	O
with	O
either	O
a	O
softmax	O
or	O
a	O
sigmoid	O
as	O
the	O
final	O
non	O
-	O
linearity	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
common	O
practice	O
is	O
to	O
train	O
the	O
softmax	S-Method
for	O
a	O
hard	B-Task
classification	I-Task
objective	E-Task
,	O
using	O
a	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
and	O
the	O
answer	O
of	O
highest	O
ground	O
truth	O
score	O
as	O
the	O
target	O
.	O

In	O
an	O
attempt	O
to	O
make	O
better	O
use	O
of	O
the	O
multiple	O
human	O
-	O
provided	O
answers	O
,	O
we	O
propose	O
to	O
use	O
the	O
soft	O
ground	O
truth	O
scores	O
as	O
the	O
target	O
with	O
a	O
logarithmic	O
loss	O
.	O

This	O
shows	O
an	O
advantage	O
on	O
the	O
“	O
abstract	B-Material
scenes	I-Material
”	I-Material
dataset	E-Material
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
left	O
and	O
middle	O
)	O
.	O

In	O
that	O
dataset	O
,	O
the	O
soft	O
target	O
scores	O
reflect	O
frequent	O
ambiguities	O
in	O
the	O
questions	O
and	O
the	O
scenes	O
,	O
and	O
when	O
synonyms	O
constitute	O
multiple	O
acceptable	O
answers	O
.	O

In	O
those	O
cases	O
,	O
we	O
can	O
avoid	O
the	O
potential	O
confusion	O
induced	O
by	O
a	O
hard	O
classification	O
for	O
one	O
specific	O
answer	O
.	O

The	O
“	O
balanced	O
”	O
dataset	O
,	O
by	O
nature	O
,	O
contains	O
almost	O
no	O
such	O
ambiguities	O
,	O
and	O
there	O
is	O
no	O
significant	O
difference	O
between	O
the	O
different	O
training	O
objectives	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
right	O
)	O
.	O

paragraph	O
:	O
Effect	O
of	O
training	O
set	O
size	O
Our	O
motivation	O
for	O
introducing	O
language	B-Task
parsing	E-Task
and	O
pretrained	B-Task
word	I-Task
embeddings	E-Task
is	O
to	O
better	O
generalize	O
the	O
concepts	O
learned	O
from	O
the	O
limited	O
training	O
examples	O
.	O

Words	O
representing	O
semantically	O
close	O
concepts	O
ideally	O
get	O
assigned	O
close	O
word	O
embeddings	O
.	O

Similarly	O
,	O
paraphrases	O
of	O
similar	O
questions	O
should	O
produce	O
parse	O
graphs	O
with	O
more	O
similarities	O
than	O
a	O
simple	O
concatenation	O
of	O
words	O
would	O
reveal	O
(	O
as	O
in	O
the	O
input	O
to	O
traditional	O
LSTMs	S-Method
)	O
.	O

We	O
trained	O
our	O
model	O
with	O
limited	O
subsets	O
of	O
the	O
training	O
data	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Unsurprisingly	O
,	O
the	O
performance	O
grows	O
steadily	O
with	O
the	O
amount	O
of	O
training	O
data	O
,	O
which	O
suggests	O
that	O
larger	O
datasets	O
would	O
improve	O
performance	O
.	O

In	O
our	O
opinion	O
however	O
,	O
it	O
seems	O
unlikely	O
that	O
sufficient	O
data	O
,	O
covering	O
all	O
possible	O
concepts	O
,	O
could	O
be	O
collected	O
in	O
the	O
form	O
of	O
question	O
/	O
answer	O
examples	O
.	O

More	O
data	O
can	O
however	O
be	O
brought	O
in	O
with	O
other	O
sources	O
of	O
information	O
and	O
supervision	O
.	O

Our	O
use	O
of	O
parsing	S-Method
and	O
word	B-Method
embeddings	E-Method
is	O
a	O
small	O
step	O
in	O
that	O
direction	O
.	O

Both	O
techniques	O
clearly	O
improve	O
generalization	S-Task
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
effect	O
may	O
be	O
particularly	O
visible	O
in	O
our	O
case	O
because	O
of	O
the	O
relatively	O
small	O
number	O
of	O
training	O
examples	O
(	O
about	O
k	O
questions	O
in	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
.	O

It	O
is	O
unclear	O
whether	O
huge	O
VQA	S-Task
datasets	O
could	O
ultimately	O
negate	O
this	O
advantage	O
.	O

Future	O
experiments	O
on	O
larger	O
datasets	O
(	O
e.g	O
.	O

)	O
may	O
answer	O
this	O
question	O
.	O

subsection	O
:	O
Evaluation	O
on	O
the	O
“	O
abstract	B-Material
scenes	I-Material
”	I-Material
dataset	E-Material
We	O
report	O
our	O
results	O
on	O
the	O
original	O
“	O
abstract	B-Material
scenes	I-Material
”	I-Material
dataset	E-Material
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
evaluation	O
is	O
performed	O
on	O
an	O
automated	O
server	O
that	O
does	O
not	O
allow	O
for	O
an	O
extensive	O
ablative	B-Task
analysis	E-Task
.	O

Anecdotally	O
,	O
performance	O
on	O
the	O
validation	O
set	O
corroborates	O
all	O
findings	O
presented	O
above	O
,	O
in	O
particular	O
the	O
strong	O
benefit	O
of	O
pre	B-Method
-	I-Method
parsing	E-Method
,	O
pretrained	B-Method
word	I-Method
embeddings	E-Method
,	O
and	O
graph	B-Method
processing	E-Method
with	O
a	O
GRU	S-Method
.	O

At	O
the	O
time	O
of	O
our	O
submission	O
,	O
our	O
method	O
occupies	O
the	O
top	O
place	O
on	O
the	O
leader	O
board	O
in	O
both	O
the	O
open	O
-	O
ended	O
and	O
multiple	B-Task
choice	I-Task
settings	E-Task
.	O

The	O
advantage	O
over	O
existing	O
method	O
is	O
most	O
pronounced	O
on	O
the	O
binary	B-Task
and	I-Task
the	I-Task
counting	I-Task
questions	E-Task
.	O

Refer	O
to	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
to	O
the	O
supplementary	O
for	O
visualizations	O
of	O
the	O
results	O
.	O

section	O
:	O
Conclusions	O
We	O
presented	O
a	O
deep	B-Method
neural	I-Method
network	E-Method
for	O
visual	B-Task
question	I-Task
answering	E-Task
that	O
processes	O
graph	B-Task
-	I-Task
structured	I-Task
representations	I-Task
of	I-Task
scenes	I-Task
and	I-Task
questions	E-Task
.	O

This	O
enables	O
leveraging	O
existing	O
natural	B-Method
language	I-Method
processing	I-Method
tools	E-Method
,	O
in	O
particular	O
pretrained	B-Method
word	I-Method
embeddings	E-Method
and	O
syntactic	B-Task
parsing	E-Task
.	O

The	O
latter	O
showed	O
significant	O
advantage	O
over	O
a	O
traditional	O
sequential	B-Task
processing	I-Task
of	I-Task
the	I-Task
questions	E-Task
,	O
e.g	O
.	O

with	O
LSTMs	S-Method
.	O

In	O
our	O
opinion	O
,	O
VQA	S-Task
systems	O
are	O
unlikely	O
to	O
learn	O
everything	O
from	O
question	O
/	O
answer	O
examples	O
alone	O
.	O

We	O
believe	O
that	O
any	O
significant	O
improvement	O
in	O
performance	O
will	O
require	O
additional	O
sources	O
of	O
information	O
and	O
supervision	O
.	O

Our	O
explicit	O
processing	O
of	O
the	O
language	O
part	O
is	O
a	O
small	O
step	O
in	O
that	O
direction	O
.	O

It	O
has	O
clearly	O
shown	O
to	O
improve	O
generalization	S-Task
without	O
resting	O
entirely	O
on	O
VQA	S-Task
-	O
specific	O
annotations	O
.	O

We	O
have	O
so	O
far	O
applied	O
our	O
method	O
to	O
datasets	O
of	O
clip	O
art	O
scenes	O
.	O

Its	O
direct	O
extension	O
to	O
real	O
images	O
will	O
be	O
addressed	O
in	O
future	O
work	O
,	O
by	O
replacing	O
nodes	O
in	O
the	O
input	O
scene	O
graph	O
with	O
proposals	O
from	O
pretrained	B-Method
object	I-Method
detectors	E-Method
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplementary	O
material	O
appendix	O
:	O
Implementation	O
We	O
provide	O
below	O
practical	O
details	O
of	O
our	O
implementation	O
of	O
the	O
proposed	O
method	O
.	O

Size	O
of	O
vector	O
embeddings	O
of	O
node	O
features	O
,	O
edge	O
features	O
,	O
and	O
all	O
hidden	O
states	O
within	O
the	O
network	O
:	O
=	O
300	O
.	O

Note	O
that	O
smaller	O
values	O
such	O
as	O
=	O
200	O
also	O
give	O
very	O
good	O
results	O
(	O
not	O
reported	O
in	O
this	O
paper	O
)	O
at	O
a	O
fraction	O
of	O
the	O
training	O
time	O
.	O

Number	O
of	O
recurrent	O
iterations	O
to	O
update	O
graph	B-Method
node	I-Method
representations	E-Method
:	O
=	O
=	O
4	O
.	O

Anecdotally	O
,	O
we	O
observed	O
that	O
processing	O
the	O
scene	O
graph	O
benefits	O
from	O
more	O
iterations	O
than	O
the	O
question	O
graph	O
,	O
for	O
which	O
performance	O
nearly	O
saturates	O
with	O
2	O
or	O
more	O
iterations	O
.	O

As	O
reported	O
in	O
the	O
ablative	O
evaluation	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
the	O
use	O
of	O
at	O
least	O
a	O
single	O
iteration	O
has	O
a	O
stronger	O
influence	O
than	O
its	O
exact	O
number	O
.	O

All	O
weights	O
except	O
word	O
embeddings	O
are	O
initialized	O
randomly	O
following	O
.	O

Word	B-Method
embeddings	E-Method
are	O
initialized	O
with	O
Glove	O
vectors	O
of	O
dimension	O
300	O
available	O
publicly	O
,	O
trained	O
for	O
6	O
billion	O
words	O
on	O
Wikipedia	S-Material
and	O
Gigaword	S-Material
.	O

The	O
word	O
embeddings	O
are	O
fine	O
-	O
tuned	O
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
of	O
the	O
other	O
weights	O
.	O

Dropout	S-Method
with	O
ratio	O
0.3	O
is	O
applied	O
between	O
the	O
weighted	O
sum	O
over	O
scene	O
elements	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
final	O
classifier	S-Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Weights	O
are	O
optimized	O
with	O
Adadelta	S-Method
with	O
mini	O
-	O
batches	O
of	O
128	O
questions	O
.	O

We	O
run	O
optimization	S-Method
until	O
convergence	O
(	O
typically	O
20	O
epochs	O
on	O
the	O
“	O
abstract	B-Material
scenes	E-Material
”	O
,	O
100	O
epochs	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
and	O
report	O
performance	O
on	O
the	O
test	O
set	O
from	O
the	O
epoch	O
with	O
the	O
highest	O
performance	O
on	O
the	O
validation	O
set	O
(	O
measured	O
by	O
VQA	S-Task
score	O
on	O
the	O
“	O
abstract	B-Material
scenes	I-Material
”	I-Material
dataset	E-Material
,	O
and	O
accuracy	S-Metric
over	O
pairs	O
on	O
the	O
“	O
balanced	O
”	O
dataset	O
)	O
.	O

The	O
edges	O
between	O
word	O
nodes	O
in	O
the	O
input	O
question	O
graph	O
are	O
labeled	O
with	O
the	O
dependency	O
labels	O
identified	O
by	O
the	O
Stanford	B-Method
parser	E-Method
.	O

These	O
dependencies	O
are	O
directed	O
,	O
and	O
we	O
supplement	O
all	O
of	O
them	O
with	O
their	O
symmetric	O
,	O
albeit	O
tagged	O
with	O
a	O
different	O
set	O
of	O
labels	O
.	O

The	O
output	O
of	O
the	O
parser	S-Method
includes	O
the	O
propagation	O
of	O
conjunct	O
dependencies	O
(	O
its	O
default	O
setting	O
)	O
.	O

This	O
yields	O
quite	O
densely	O
connected	O
graphs	O
.	O

The	O
input	O
features	O
of	O
the	O
object	O
nodes	O
are	O
those	O
directly	O
available	O
in	O
the	O
datasets	O
.	O

They	O
represent	O
:	O
the	O
object	O
category	O
(	O
human	O
,	O
animal	O
,	O
small	O
or	O
large	O
object	O
)	O
as	O
one	O
one	O
-	O
hot	O
vector	O
,	O
the	O
object	O
type	O
(	O
table	O
,	O
sun	O
,	O
dog	O
window	O
,	O
…	O
)	O
as	O
a	O
one	O
-	O
hot	O
vector	O
,	O
the	O
expression	O
/	O
pose	O
/	O
type	O
(	O
various	O
depictions	O
being	O
possible	O
for	O
each	O
object	O
type	O
)	O
as	O
a	O
one	O
-	O
hot	O
vector	O
,	O
and	O
10	O
scalar	O
values	O
describing	O
the	O
pose	O
of	O
human	O
figures	O
(	O
the	O
X	O
/	O
Y	O
position	O
of	O
arms	O
,	O
legs	O
,	O
and	O
head	O
relative	O
to	O
the	O
torso	O
)	O
.	O

They	O
form	O
altogether	O
a	O
feature	O
vector	O
of	O
dimension	O
159	O
.	O

The	O
edge	O
features	O
between	O
objects	O
represent	O
:	O
the	O
signed	O
difference	O
in	O
their	O
X	O
/	O
Y	O
position	O
,	O
the	O
inverse	O
of	O
their	O
absolute	O
difference	O
in	O
X	O
/	O
Y	O
position	O
,	O
and	O
their	O
relative	O
position	O
on	O
depth	O
planes	O
as	O
+	O
1	O
if	O
closer	O
(	O
potentially	O
occluding	O
the	O
other	O
)	O
,	O
-	O
1	O
otherwise	O
.	O

All	O
input	O
features	O
are	O
normalized	O
for	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O

When	O
training	O
for	O
the	O
“	O
balanced	O
”	O
dataset	O
,	O
care	O
is	O
taken	O
to	O
keep	O
each	O
pair	O
of	O
complementary	O
scenes	O
in	O
a	O
same	O
mini	O
-	O
batch	O
when	O
shuffling	O
training	O
instances	O
.	O

This	O
has	O
a	O
noticeable	O
effect	O
on	O
the	O
stability	O
of	O
the	O
optimization	S-Task
.	O

In	O
the	O
open	B-Task
-	I-Task
ended	I-Task
setting	E-Task
,	O
the	O
output	O
space	O
is	O
made	O
of	O
all	O
answers	O
that	O
appear	O
at	O
least	O
5	O
times	O
in	O
the	O
training	O
set	O
.	O

These	O
correspond	O
to	O
623	O
possible	O
answers	O
,	O
which	O
cover	O
96	O
%	O
of	O
the	O
training	O
questions	O
.	O

Our	O
model	O
was	O
implemented	O
in	O
Matlab	O
from	O
scratch	O
.	O

Training	S-Task
takes	O
in	O
the	O
order	O
of	O
5	O
to	O
10	O
hours	O
on	O
one	O
CPU	O
,	O
depending	O
on	O
the	O
dataset	O
and	O
on	O
the	O
size	O
of	O
the	O
internal	B-Method
representations	E-Method
.	O

appendix	O
:	O
Additional	O
details	O
Why	O
do	O
we	O
choose	O
to	O
focus	O
on	O
abstract	O
scenes	O
?	O
Does	O
this	O
method	O
extend	O
to	O
real	O
images	O
?	O
The	O
balanced	B-Material
dataset	I-Material
of	I-Material
abstract	I-Material
scenes	E-Material
was	O
the	O
only	O
one	O
allowing	O
evaluation	O
free	O
from	O
dataset	O
biases	O
.	O

Abstract	B-Material
scenes	E-Material
also	O
enabled	O
removing	O
confounding	O
factors	O
(	O
visual	B-Task
recognition	E-Task
)	O
.	O

It	O
is	O
not	O
unreasonable	O
to	O
view	O
the	O
scene	O
descriptions	O
(	O
provided	O
with	O
abstract	B-Material
scenes	E-Material
)	O
as	O
the	O
output	O
of	O
a	O
“	O
perfect	O
”	O
vision	B-Method
system	E-Method
.	O

The	O
proposel	O
model	O
could	O
be	O
extended	O
to	O
real	O
images	O
by	O
building	O
graphs	O
of	O
the	O
images	O
where	O
scene	O
nodes	O
are	O
candidates	O
from	O
an	O
object	B-Method
detection	I-Method
algorithm	E-Method
.	O

The	O
multiple	O
-	O
choice	O
(	O
M.C.	O
)	O
setting	O
should	O
be	O
easier	O
than	O
open	O
-	O
ended	O
(	O
O.E.	O
)	O
.	O

Therefore	O
,	O
why	O
is	O
the	O
accuracy	S-Metric
not	O
better	O
for	O
binary	O
and	O
number	O
questions	O
in	O
the	O
M.C	O
setting	O
(	O
rather	O
than	O
O.E.	O
)	O
?	O
This	O
intuition	O
is	O
incorrect	O
in	O
practice	O
.	O

The	O
wording	O
of	O
binary	O
and	O
number	O
questions	O
(	O
“	O
How	O
many	O
…	O
”	O
)	O
can	O
easily	O
narrow	O
down	O
the	O
set	O
of	O
possible	O
answers	O
,	O
whether	O
evaluated	O
in	O
a	O
M.C.	O
or	O
O.E.	O
setting	O
.	O

One	O
thus	O
can	O
not	O
qualify	O
one	O
as	O
strictly	O
easier	O
than	O
the	O
other	O
.	O

Other	O
factors	O
can	O
then	O
influence	O
the	O
performance	O
either	O
way	O
.	O

Note	O
also	O
that	O
,	O
for	O
example	O
that	O
most	O
choices	O
of	O
number	O
questions	O
are	O
not	O
numbers	O
.	O

In	O
Table	O
1	O
,	O
why	O
is	O
there	O
a	O
large	O
improvement	O
of	O
the	O
metric	O
over	O
balanced	O
pairs	O
of	O
scenes	O
,	O
but	O
not	O
of	O
the	O
metric	O
over	O
individual	O
scenes	O
?	O
The	O
metric	O
over	O
pairs	O
is	O
much	O
harder	O
to	O
satisfy	O
and	O
should	O
be	O
regarded	O
as	O
more	O
meaningful	O
.	O

The	O
other	O
metric	O
(	O
over	O
scenes	O
)	O
essentially	O
saturates	O
at	O
the	O
same	O
point	O
between	O
the	O
two	O
methods	O
.	O

How	O
are	O
precison	B-Metric
/	I-Metric
recall	I-Metric
curves	E-Metric
helping	O
better	O
understand	O
model	O
compared	O
to	O
a	O
simple	O
accuracy	B-Metric
number	E-Metric
?	O
A	O
P	O
/	O
R	O
curve	O
shows	O
the	O
confidence	O
of	O
the	O
model	O
in	O
its	O
answers	O
.	O

A	O
practical	O
VQA	S-Task
system	O
will	O
need	O
to	O
provide	O
an	O
indication	O
of	O
certainty	O
,	O
including	O
the	O
possibility	O
of	O
“	O
I	O
do	O
n’t	O
know	O
”	O
.	O

Reporting	O
P	O
/	O
R	O
is	O
a	O
step	O
in	O
that	O
direction	O
.	O

P	O
/	O
R	O
curves	O
also	O
contain	O
more	O
information	O
and	O
can	O
show	O
differences	O
between	O
methods	O
(	O
e.g	O
.	O

Fig.3	O
left	O
)	O
that	O
may	O
otherwise	O
not	O
be	O
appreciable	O
through	O
an	O
aggregate	B-Metric
metric	E-Metric
.	O

Why	O
is	O
attention	O
computed	O
with	O
pre	O
-	O
GRU	O
node	O
features	O
?	O
This	O
performed	O
slightly	O
better	O
than	O
the	O
alternative	O
.	O

The	O
intuition	O
is	O
that	O
the	O
identity	O
of	O
each	O
node	O
is	O
sufficient	O
,	O
and	O
the	O
context	O
(	O
transfered	O
by	O
the	O
GRU	S-Method
from	O
neighbouring	O
nodes	O
)	O
is	O
probably	O
less	O
useful	O
to	O
compute	O
attention	O
.	O

Why	O
are	O
the	O
largest	O
performance	O
gains	O
obtained	O
with	O
“	O
number	O
”	O
questions	O
?	O
We	O
could	O
not	O
draw	O
definitive	O
conclusions	O
.	O

Competing	B-Method
methods	E-Method
seem	O
to	O
rely	O
on	O
dataset	O
biases	O
(	O
predominance	O
of	O
2	O
and	O
3	O
as	O
answers	O
)	O
.	O

Ours	O
was	O
developed	O
(	O
cross	O
-	O
validated	O
)	O
for	O
the	O
balanced	O
dataset	O
,	O
which	O
requires	O
not	O
to	O
rely	O
on	O
such	O
biases	O
,	O
and	O
may	O
simply	O
be	O
better	O
at	O
utilizing	O
the	O
input	O
and	O
not	O
biases	O
.	O

This	O
may	O
in	O
turn	O
explain	O
minimal	O
gains	O
on	O
other	O
questions	O
,	O
which	O
could	O
benefit	O
from	O
using	O
biases	O
(	O
because	O
of	O
a	O
larger	O
pool	O
of	O
reasonable	O
answers	O
)	O
.	O

appendix	O
:	O
Additional	O
results	O
We	O
provide	O
below	O
additional	O
example	O
results	O
in	O
the	O
same	O
format	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Additional	O
results	O
:	O
abstract	O
scenes	O
dataset	O
subsection	O
:	O
Additional	O
results	O
:	O
balanced	O
dataset	O
Photographic	B-Task
Image	I-Task
Synthesis	E-Task
with	O
Cascaded	B-Method
Refinement	I-Method
Networks	E-Method
section	O
:	O
The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	B-Task
photographic	I-Task
images	E-Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	B-Method
label	I-Method
map	E-Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	B-Method
engine	E-Method
that	O
takes	O
a	O
two	O
-	O
dimensional	B-Task
semantic	I-Task
specification	I-Task
of	I-Task
the	I-Task
scene	E-Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	B-Method
training	E-Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	B-Method
network	E-Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	B-Method
regression	I-Method
objective	E-Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
†	O
Intel	O
Labs	O
‡	O
Stanford	O
University	O
demonstrate	O
this	O
by	O
synthesizing	O
photographic	O
images	O
at	O
2	O
-	O
megapixel	O
resolution	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Extensive	O
perceptual	O
experiments	O
on	O
datasets	O
of	O
outdoor	O
and	O
indoor	O
scenes	O
demonstrate	O
that	O
images	O
synthesized	O
by	O
the	O
presented	O
approach	O
are	O
considerably	O
more	O
realistic	O
than	O
alternative	O
approaches	O
.	O

section	O
:	O
.	O

Given	O
a	O
pixelwise	B-Method
semantic	I-Method
layout	E-Method
,	O
the	O
presented	O
model	O
synthesizes	O
an	O
image	O
that	O
conforms	O
to	O
this	O
layout	O
.	O

(	O
a	O
)	O
Semantic	O
layouts	O
from	O
the	O
Cityscapes	B-Material
dataset	I-Material
of	I-Material
urban	I-Material
scenes	E-Material
;	O
semantic	O
classes	O
are	O
coded	O
by	O
color	O
.	O

(	O
b	O
)	O
Images	O
synthesized	O
by	O
our	O
model	O
for	O
these	O
layouts	O
.	O

The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	B-Task
photographic	I-Task
images	E-Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	B-Method
label	I-Method
map	E-Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	B-Method
engine	E-Method
that	O
takes	O
a	O
two	O
-	O
dimensional	B-Task
semantic	I-Task
specification	I-Task
of	I-Task
the	I-Task
scene	E-Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	B-Method
training	E-Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	B-Method
network	E-Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	B-Method
regression	I-Method
objective	E-Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
section	O
:	O
Introduction	O
Consider	O
the	O
semantic	O
layouts	O
in	O
Figure	O
1	O
.	O

A	O
skilled	O
painter	O
could	O
draw	O
images	O
that	O
depict	O
urban	O
scenes	O
that	O
conform	O
to	O
these	O
layouts	O
.	O

Highly	O
trained	O
craftsmen	O
can	O
even	O
create	O
paintings	O
that	O
approach	O
photorealism	O
[	O
reference	O
]	O
.	O

Can	O
we	O
train	O
computational	B-Method
models	E-Method
that	O
have	O
this	O
ability	O
?	O
Given	O
a	O
semantic	O
layout	O
of	O
a	O
novel	O
scene	O
,	O
can	O
an	O
artificial	B-Method
system	E-Method
synthesize	O
an	O
image	O
that	O
depicts	O
this	O
scene	O
and	O
looks	O
like	O
a	O
photograph	O
?	O
This	O
question	O
is	O
connected	O
to	O
central	O
problems	O
in	O
computer	B-Task
graphics	E-Task
and	O
artificial	B-Task
intelligence	E-Task
.	O

First	O
,	O
consider	O
the	O
problem	O
of	O
photorealism	B-Task
in	I-Task
computer	I-Task
graphics	E-Task
.	O

A	O
system	O
that	O
synthesizes	O
photorealistic	O
images	O
from	O
semantic	O
layouts	O
would	O
in	O
effect	O
function	O
as	O
a	O
kind	O
of	O
rendering	B-Method
engine	E-Method
that	O
bypasses	O
the	O
laborious	O
specification	O
of	O
detailed	O
threedimensional	O
geometry	O
and	O
surface	O
reflectance	O
distributions	O
,	O
and	O
avoids	O
computationally	O
intensive	O
light	B-Method
transport	I-Method
simulation	E-Method
[	O
reference	O
]	O
.	O

A	O
direct	B-Method
synthesis	I-Method
approach	E-Method
could	O
not	O
immediately	O
replace	O
modern	O
rendering	B-Method
engines	E-Method
,	O
but	O
would	O
indicate	O
that	O
an	O
alternative	O
route	O
to	O
photorealism	S-Task
may	O
be	O
viable	O
and	O
could	O
some	O
day	O
complement	O
existing	O
computer	B-Method
graphics	I-Method
techniques	E-Method
.	O

Our	O
second	O
source	O
of	O
motivation	O
is	O
the	O
role	O
of	O
mental	O
imagery	O
and	O
simulation	S-Task
in	O
human	B-Task
cognition	E-Task
[	O
reference	O
]	O
.	O

Mental	B-Task
imagery	E-Task
is	O
believed	O
to	O
play	O
an	O
important	O
role	O
in	O
planning	B-Task
and	I-Task
decision	I-Task
making	E-Task
.	O

The	O
level	O
of	O
detail	O
and	O
completeness	O
of	O
mental	O
imagery	O
is	O
a	O
matter	O
of	O
debate	O
,	O
but	O
its	O
role	O
in	O
human	B-Task
intelligence	E-Task
suggests	O
that	O
the	O
ability	O
to	O
synthesize	O
photorealistic	O
images	O
may	O
support	O
the	O
development	O
of	O
artificial	B-Method
intelligent	I-Method
systems	E-Method
[	O
reference	O
]	O
.	O

In	O
this	O
work	O
,	O
we	O
develop	O
a	O
model	O
for	O
photographic	B-Task
image	I-Task
synthesis	E-Task
from	O
pixelwise	B-Task
semantic	I-Task
layouts	E-Task
.	O

Our	O
model	O
is	O
a	O
convolutional	B-Method
network	E-Method
,	O
trained	O
in	O
a	O
supervised	B-Method
fashion	E-Method
on	O
pairs	O
of	O
photographs	O
and	O
corresponding	O
semantic	O
layouts	O
.	O

Such	O
pairs	O
are	O
provided	O
with	O
semantic	O
segmentation	O
datasets	O
[	O
reference	O
]	O
.	O

We	O
use	O
them	O
not	O
to	O
infer	O
semantic	O
layouts	O
from	O
photographs	O
,	O
but	O
to	O
synthesize	O
photographs	O
from	O
semantic	O
layouts	O
.	O

In	O
this	O
sense	O
our	O
problem	O
is	O
the	O
inverse	B-Task
of	I-Task
semantic	I-Task
segmentation	E-Task
.	O

Images	O
synthesized	O
by	O
our	O
model	O
are	O
shown	O
in	O
Figure	O
1	O
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
directly	O
by	O
a	O
single	O
feedforward	B-Method
convolutional	I-Method
network	E-Method
trained	O
to	O
minimize	O
a	O
regression	O
loss	O
.	O

This	O
departs	O
from	O
much	O
recent	O
and	O
contemporaneous	O
work	O
,	O
which	O
uses	O
adversarial	B-Method
training	I-Method
of	I-Method
generator	I-Method
-	I-Method
discriminator	I-Method
dyads	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
show	O
that	O
direct	O
supervised	B-Method
training	E-Method
of	O
a	O
single	B-Method
convolutional	I-Method
network	E-Method
can	O
yield	O
photographic	O
images	O
.	O

This	O
bypasses	O
adversarial	B-Method
training	E-Method
,	O
which	O
is	O
known	O
to	O
be	O
"	O
massively	O
unstable	O
"	O
[	O
reference	O
]	O
.	O

Furthermore	O
,	O
the	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
image	O
resolutions	O
.	O

We	O
synthesize	O
images	O
with	O
resolution	O
up	O
to	O
2	O
megapixels	O
(	O
1024×2048	O
)	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Doubling	O
the	O
output	O
resolution	O
and	O
generating	O
appropriate	O
details	O
at	O
that	O
resolution	O
amounts	O
to	O
adding	O
a	O
single	O
module	O
to	O
our	O
end	O
-	O
to	O
-	O
end	B-Method
model	E-Method
.	O

We	O
conduct	O
careful	O
perceptual	O
experiments	O
using	O
the	O
Amazon	B-Method
Mechanical	I-Method
Turk	E-Method
platform	O
,	O
comparing	O
the	O
presented	O
approach	O
to	O
a	O
range	O
of	O
baselines	O
.	O

These	O
experiments	O
clearly	O
indicate	O
that	O
images	O
synthesized	O
by	O
our	O
model	O
are	O
significantly	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
alternative	O
approaches	O
.	O

section	O
:	O
Related	O
Work	O
The	O
most	O
prominent	O
contemporary	O
approach	O
to	O
image	B-Task
synthesis	E-Task
is	O
based	O
on	O
generative	B-Method
adversarial	I-Method
networks	E-Method
(	O
GANs	S-Method
)	O
[	O
reference	O
]	O
.	O

In	O
the	O
original	O
work	O
of	O
Goodfellow	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
GANs	S-Method
were	O
used	O
to	O
synthesize	O
MNIST	B-Material
digits	E-Material
and	O
32	O
×	O
32	O
images	O
that	O
aimed	O
to	O
reproduce	O
the	O
appearance	O
of	O
different	O
classes	O
in	O
the	O
CIFAR	B-Material
-	I-Material
10	I-Material
dataset	E-Material
.	O

Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
training	O
multiple	O
separate	O
GANs	S-Method
,	O
one	O
for	O
each	O
level	O
in	O
a	O
Laplacian	O
pyramid	O
.	O

Each	O
model	O
is	O
trained	O
independently	O
to	O
synthesize	O
details	O
at	O
its	O
scale	O
.	O

Assembling	O
separatelytrained	B-Method
models	E-Method
in	O
this	O
fashion	O
enabled	O
the	O
authors	O
to	O
synthesize	O
smoother	O
images	O
and	O
to	O
push	O
resolution	O
up	O
to	O
96×96	O
.	O

This	O
work	O
is	O
an	O
important	O
precursor	O
to	O
ours	O
in	O
that	O
multiscale	B-Task
refinement	E-Task
is	O
a	O
central	O
characteristic	O
of	O
our	O
approach	O
.	O

Key	O
differences	O
are	O
that	O
we	O
train	O
a	O
single	O
model	O
end	O
-	O
to	O
-	O
end	O
to	O
directly	O
synthesize	O
the	O
output	O
image	O
,	O
and	O
that	O
no	O
adversarial	B-Method
training	E-Method
is	O
used	O
.	O

Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
remark	O
that	O
"	O
Historical	O
attempts	O
to	O
scale	O
up	O
GANs	S-Method
using	O
CNNs	S-Method
to	O
model	O
images	O
have	O
been	O
unsuccessful	O
"	O
and	O
describe	O
a	O
number	O
of	O
modifications	O
that	O
enable	O
scaling	O
up	O
adversarial	B-Method
training	E-Method
to	O
64×64	O
images	O
.	O

Salimans	O
et	O
al	O
.	O

[	O
reference	O
]	O
also	O
tackle	O
the	O
instability	O
of	O
GAN	B-Method
training	E-Method
and	O
describe	O
a	O
number	O
of	O
heuristics	S-Method
that	O
encourage	O
convergence	S-Metric
.	O

The	O
authors	O
synthesize	O
128	O
×	O
128	O
images	O
that	O
possess	O
plausible	O
low	O
-	O
level	O
statistics	O
.	O

Nevertheless	O
,	O
as	O
observed	O
in	O
recent	O
work	O
and	O
widely	O
known	O
in	O
the	O
folklore	O
,	O
GANs	S-Method
"	O
remain	O
remarkably	O
difficult	O
to	O
train	O
"	O
and	O
"	O
approaches	O
to	O
attacking	O
this	O
problem	O
still	O
rely	O
on	O
heuristics	S-Method
that	O
are	O
extremely	O
sensitive	O
to	O
modifications	O
"	O
[	O
reference	O
]	O
.	O

(	O
See	O
also	O
[	O
reference	O
]	O
.	O

)	O
Our	O
work	O
demonstrates	O
that	O
these	O
difficulties	O
can	O
be	O
avoided	O
in	O
the	O
setting	O
we	O
consider	O
.	O

Dosovitskiy	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
ConvNet	S-Method
to	O
generate	O
images	B-Task
of	I-Task
3D	I-Task
models	E-Task
,	O
given	O
a	O
model	O
ID	O
and	O
viewpoint	O
.	O

The	O
network	O
thus	O
acts	O
directly	O
as	O
a	O
rendering	B-Method
engine	E-Method
for	O
the	O
3D	B-Method
model	E-Method
.	O

This	O
is	O
also	O
an	O
important	O
precursor	O
to	O
our	O
work	O
as	O
it	O
uses	O
direct	O
feedforward	B-Method
synthesis	E-Method
through	O
a	O
network	O
trained	O
with	O
a	O
regression	B-Method
loss	E-Method
.	O

Our	O
model	O
,	O
loss	S-Method
,	O
and	O
problem	O
setting	O
are	O
different	O
,	O
enabling	O
synthesis	B-Task
of	I-Task
sharper	I-Task
higherresolution	I-Task
images	I-Task
of	I-Task
scenes	I-Task
without	I-Task
3D	I-Task
models	E-Task
.	O

Dosovitskiy	O
and	O
Brox	O
[	O
reference	O
]	O
introduced	O
a	O
family	O
of	O
composite	B-Method
loss	I-Method
functions	E-Method
for	O
image	B-Task
synthesis	E-Task
,	O
which	O
combine	O
regression	S-Method
over	O
the	O
activations	O
of	O
a	O
fixed	O
"	O
perceiver	B-Method
"	I-Method
network	E-Method
with	O
a	O
GAN	B-Method
loss	E-Method
.	O

Networks	S-Method
trained	O
using	O
these	O
composite	O
loss	O
functions	O
were	O
applied	O
to	O
synthesize	O
preimages	O
that	O
induce	O
desired	O
excitation	O
patterns	O
in	O
image	B-Method
classification	I-Method
models	E-Method
[	O
reference	O
]	O
and	O
images	O
that	O
excite	O
specific	O
elements	O
in	O
such	O
models	O
[	O
reference	O
]	O
.	O

In	O
recent	O
work	O
,	O
networks	O
trained	O
using	O
these	O
losses	O
were	O
applied	O
to	O
generate	O
diverse	O
sets	O
of	O
227×227	O
images	O
,	O
to	O
synthesize	O
images	O
for	O
given	O
captions	O
,	O
and	O
to	O
inpaint	O
missing	O
regions	O
[	O
reference	O
]	O
.	O

These	O
works	O
all	O
rely	O
on	O
the	O
aforementioned	O
composite	O
losses	O
,	O
which	O
require	O
balancing	O
the	O
adversarial	O
loss	O
with	O
a	O
regression	B-Method
loss	E-Method
.	O

Our	O
work	O
differs	O
in	O
that	O
GANs	S-Method
are	O
not	O
used	O
,	O
which	O
simplifies	O
the	O
train	O
-	O
[	O
reference	O
]	O
.	O

Zoom	O
in	O
for	O
details	O
.	O

ing	O
procedure	O
,	O
architecture	O
,	O
and	O
loss	O
.	O

Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
consider	O
a	O
family	O
of	O
problems	O
that	O
include	O
the	O
image	B-Task
synthesis	I-Task
problem	E-Task
we	O
focus	O
on	O
.	O

The	O
paper	O
of	O
Isola	O
et	O
al	O
.	O

appeared	O
on	O
arXiv	O
during	O
the	O
course	O
of	O
our	O
research	O
.	O

It	O
provides	O
an	O
opportunity	O
to	O
compare	O
our	O
approach	O
to	O
a	O
credible	O
alternative	O
that	O
was	O
independently	O
tested	O
on	O
the	O
same	O
data	O
.	O

Like	O
a	O
number	O
of	O
aforementioned	O
formulations	O
,	O
Isola	O
et	O
al	O
.	O

use	O
a	O
composite	B-Method
loss	E-Method
that	O
combines	O
a	O
GAN	S-Method
and	O
a	O
regression	B-Method
term	E-Method
.	O

The	O
authors	O
use	O
the	O
Cityscapes	S-Material
dataset	O
and	O
synthesize	O
256×256	O
images	O
for	O
given	O
semantic	O
layouts	O
.	O

In	O
comparison	O
,	O
our	O
simpler	O
direct	B-Method
formulation	E-Method
yields	O
much	O
more	O
realistic	O
images	O
and	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
.	O

A	O
qualitative	O
comparison	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
64×64	O
images	O
of	O
scenes	O
that	O
are	O
described	O
by	O
given	O
sentences	O
.	O

Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
describe	O
a	O
different	O
model	O
that	O
generates	O
32×32	O
images	O
that	O
aim	O
to	O
fit	O
sentences	O
.	O

Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
generate	O
64×64	O
images	O
of	O
faces	O
and	O
birds	O
with	O
given	O
attributes	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
128×128	O
images	O
of	O
birds	O
and	O
people	O
conditioned	O
on	O
text	O
descriptions	O
and	O
on	O
spatial	O
constraints	O
such	O
as	O
bounding	O
boxes	O
or	O
keypoints	O
.	O

Wang	O
and	O
Gupta	O
[	O
reference	O
]	O
synthesize	O
128×128	B-Task
images	I-Task
of	I-Task
indoor	I-Task
scenes	E-Task
by	O
factorizing	O
the	O
image	B-Method
generation	I-Method
process	E-Method
into	O
synthesis	O
of	O
a	O
normal	B-Method
map	E-Method
and	O
subsequent	O
synthesis	O
of	O
a	O
corresponding	O
color	O
image	O
.	O

Most	O
of	O
these	O
works	O
use	O
GANs	S-Method
,	O
with	O
the	O
exception	O
of	O
Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
variational	B-Method
autoencoders	E-Method
and	O
Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
a	O
recurrent	B-Method
attention	I-Method
-	I-Method
based	I-Method
model	E-Method
[	O
reference	O
]	O
.	O

Our	O
problem	O
statement	O
is	O
different	O
in	O
that	O
our	O
input	O
is	O
a	O
pixelwise	O
semantic	O
layout	O
,	O
and	O
our	O
technical	O
approach	O
differs	O
substantially	O
in	O
that	O
a	O
single	O
feedforward	B-Method
convolutional	I-Method
network	E-Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
synthesize	O
a	O
high	O
-	O
resolution	O
image	O
.	O

A	O
line	O
of	O
work	O
considers	O
synthesis	B-Task
of	I-Task
future	I-Task
frames	I-Task
in	I-Task
video	E-Task
.	O

Srivastava	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
recurrent	B-Method
network	E-Method
for	O
this	O
purpose	O
.	O

Mathieu	O
et	O
al	O
.	O

[	O
reference	O
]	O
build	O
on	O
the	O
work	O
of	O
Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
use	O
a	O
composite	B-Method
loss	E-Method
that	O
combines	O
an	O
adversarial	B-Method
term	E-Method
with	O
regression	O
penalties	O
on	O
colors	O
and	O
gradients	O
.	O

Oh	O
et	O
al	O
.	O

[	O
reference	O
]	O
predict	O
future	O
frames	O
in	O
Atari	B-Task
games	E-Task
conditioned	O
on	O
the	O
player	O
's	O
action	O
.	O

Finn	O
et	O
al	O
.	O

[	O
reference	O
]	O
explicitly	O
model	O
pixel	O
motion	O
and	O
also	O
condition	O
on	O
action	O
.	O

Vondrick	O
et	O
al	O
.	O

[	O
reference	O
]	O
learn	O
a	O
model	B-Method
of	I-Method
scene	I-Method
dynamics	E-Method
and	O
use	O
it	O
to	O
synthesize	O
video	O
sequences	O
from	O
single	O
images	O
.	O

Xue	O
et	O
al	O
.	O

[	O
reference	O
]	O
develop	O
a	O
probabilistic	B-Method
model	E-Method
that	O
enables	O
synthesizing	O
multiple	O
plausible	O
video	O
sequences	O
.	O

In	O
these	O
works	O
,	O
a	O
color	O
image	O
is	O
available	O
as	O
a	O
starting	O
point	O
for	O
synthesis	S-Task
.	O

Video	B-Task
synthesis	E-Task
can	O
be	O
accomplished	O
by	O
advecting	O
the	O
content	O
of	O
this	O
initial	O
image	O
.	O

In	O
our	O
setting	O
,	O
photographic	B-Task
scene	I-Task
appearance	E-Task
must	O
be	O
synthesized	O
without	O
such	O
initialization	O
.	O

Researchers	O
have	O
also	O
studied	O
image	B-Task
inpainting	E-Task
[	O
reference	O
]	O
,	O
superresolution	S-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
novel	O
view	B-Task
synthesis	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
interactive	B-Task
image	I-Task
manipulation	E-Task
[	O
reference	O
]	O
.	O

In	O
these	O
problems	O
,	O
photographic	O
content	O
is	O
given	O
as	O
input	O
,	O
whereas	O
we	O
are	O
concerned	O
with	O
synthesizing	O
photographic	O
images	O
from	O
semantic	O
layouts	O
alone	O
.	O

section	O
:	O
Method	O
section	O
:	O
Preliminaries	O
Consider	O
a	O
semantic	O
layout	O
L	O
∈	O
{	O
0	O
,	O
1	O
}	O
m×n×c	O
,	O
where	O
m×n	O
is	O
the	O
pixel	O
resolution	O
and	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

Each	O
pixel	O
in	O
L	O
is	O
represented	O
by	O
a	O
one	O
-	O
hot	O
vector	O
that	O
indicates	O
its	O
semantic	O
label	O
:	O
One	O
of	O
the	O
c	O
possible	O
labels	O
is	O
'	O
void	O
'	O
,	O
which	O
indicates	O
that	O
the	O
semantic	O
class	O
of	O
the	O
pixel	O
is	O
not	O
specified	O
.	O

Our	O
goal	O
is	O
to	O
train	O
a	O
parametric	B-Method
mapping	I-Method
g	E-Method
that	O
given	O
a	O
semantic	O
layout	O
L	O
produces	O
a	O
color	O
image	O
I	O
∈	O
R	O
m×n×3	O
that	O
conforms	O
to	O
L.	O
In	O
the	O
course	O
of	O
this	O
project	O
we	O
have	O
experimented	O
with	O
a	O
large	O
number	O
of	O
network	B-Method
architectures	E-Method
.	O

As	O
a	O
result	O
of	O
these	O
experiments	O
,	O
we	O
have	O
identified	O
three	O
characteristics	O
that	O
are	O
important	O
for	O
synthesizing	B-Task
photorealistic	I-Task
images	E-Task
.	O

We	O
review	O
these	O
characteristics	O
before	O
describing	O
our	O
solution	O
.	O

Global	B-Task
coordination	E-Task
.	O

Globally	O
consistent	O
structure	O
is	O
essential	O
for	O
photorealism	S-Task
.	O

Many	O
objects	O
exhibit	O
nonlocal	O
structural	O
relationships	O
,	O
such	O
as	O
symmetry	O
.	O

For	O
example	O
,	O
if	O
the	O
network	O
synthesizes	O
a	O
red	O
light	O
on	O
the	O
left	O
side	O
of	O
a	O
car	O
,	O
then	O
the	O
corresponding	O
light	O
on	O
the	O
right	O
should	O
also	O
be	O
red	O
.	O

This	O
distinguishes	O
photorealistic	B-Task
image	I-Task
synthesis	E-Task
from	O
texture	B-Task
synthesis	E-Task
,	O
which	O
can	O
leverage	O
statistical	B-Method
stationarity	E-Method
[	O
reference	O
]	O
.	O

Our	O
model	O
is	O
based	O
on	O
multi	B-Method
-	I-Method
resolution	I-Method
refinement	E-Method
.	O

The	O
synthesis	S-Task
begins	O
at	O
extremely	O
low	O
resolution	O
(	O
4	O
×	O
8	O
in	O
our	O
implementation	O
)	O
.	O

Feature	O
maps	O
are	O
then	O
progressively	O
refined	O
.	O

Thus	O
global	O
structure	O
can	O
be	O
coordinated	O
at	O
lower	O
octaves	O
,	O
where	O
even	O
distant	O
object	O
parts	O
are	O
represented	O
in	O
nearby	O
feature	O
columns	O
.	O

These	O
decisions	O
are	O
then	O
refined	O
at	O
higher	O
octaves	O
.	O

High	B-Task
resolution	E-Task
.	O

To	O
produce	O
truly	O
photorealistic	O
results	O
,	O
a	O
model	O
must	O
be	O
able	O
to	O
synthesize	O
high	O
-	O
resolution	O
images	O
.	O

Low	B-Task
resolution	E-Task
is	O
akin	O
to	O
myopic	B-Task
vision	E-Task
in	O
that	O
fine	O
visual	O
features	O
are	O
not	O
discernable	O
.	O

The	O
drive	O
to	O
high	O
image	B-Task
and	I-Task
video	I-Task
resolutions	E-Task
in	O
multiple	O
industries	O
is	O
a	O
testament	O
to	O
resolution	O
's	O
importance	O
.	O

Our	O
model	O
synthesizes	O
images	O
by	O
progressive	B-Method
refinement	E-Method
,	O
and	O
going	O
up	O
an	O
octave	O
in	O
resolution	O
(	O
e.g.	O
,	O
from	O
512p	O
to	O
1024p	O
)	O
amounts	O
to	O
adding	O
a	O
single	O
refinement	B-Method
module	E-Method
.	O

The	O
entire	O
cascade	B-Method
of	I-Method
refinement	I-Method
modules	E-Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Memory	O
.	O

We	O
conjecture	O
that	O
high	O
model	B-Metric
capacity	E-Metric
is	O
essential	O
for	O
synthesizing	B-Task
high	I-Task
-	I-Task
resolution	I-Task
photorealistic	I-Task
images	E-Task
.	O

Human	O
hyperrealistic	O
painters	O
use	O
photographic	O
references	O
as	O
external	O
memory	O
of	O
detailed	O
object	O
appearance	O
[	O
reference	O
]	O
.	O

The	O
best	O
existing	O
image	B-Method
compression	I-Method
techniques	E-Method
require	O
millions	O
of	O
bits	O
of	O
information	O
to	O
represent	O
the	O
content	O
of	O
a	O
single	O
high	O
-	O
resolution	O
image	O
:	O
there	O
exists	O
no	O
known	O
way	O
to	O
reconstruct	O
a	O
given	O
photograph	O
at	O
high	O
fidelity	O
from	O
a	O
lowercapacity	B-Method
representation	E-Method
[	O
reference	O
]	O
.	O

In	O
order	O
for	O
our	O
model	O
to	O
be	O
able	O
to	O
synthesize	O
diverse	O
scenes	O
from	O
a	O
given	O
domain	O
given	O
only	O
semantic	O
layouts	O
as	O
input	O
,	O
the	O
capacity	O
of	O
the	O
model	O
must	O
be	O
sufficiently	O
high	O
to	O
be	O
able	O
to	O
reproduce	O
the	O
detailed	O
photographic	O
appearance	O
of	O
many	O
objects	O
.	O

We	O
expect	O
a	O
successful	O
model	O
to	O
reproduce	O
images	O
in	O
the	O
training	O
set	O
extremely	O
well	O
(	O
memorization	O
)	O
and	O
also	O
to	O
apply	O
the	O
learned	O
representations	O
to	O
novel	O
layouts	O
(	O
generalization	S-Task
)	O
.	O

This	O
requires	O
high	O
model	O
capacity	O
.	O

Our	O
design	O
is	O
modular	O
and	O
the	O
capacity	O
of	O
the	O
model	O
can	O
be	O
expanded	O
as	O
allowed	O
by	O
hardware	O
.	O

The	O
network	O
used	O
in	O
most	O
of	O
our	O
experiments	O
has	O
105	O
M	O
parameters	O
and	O
maximizes	O
available	O
GPU	O
memory	O
.	O

We	O
have	O
consistently	O
found	O
that	O
increasing	O
model	B-Metric
capacity	E-Metric
increases	O
image	B-Metric
quality	E-Metric
.	O

section	O
:	O
Architecture	O
The	O
Cascaded	B-Method
Refinement	I-Method
Network	E-Method
(	O
CRN	S-Method
)	O
is	O
a	O
cascade	B-Method
of	I-Method
refinement	I-Method
modules	E-Method
.	O

Each	O
module	O
M	O
i	O
operates	O
at	O
a	O
given	O
resolution	O
.	O

In	O
our	O
implementation	O
,	O
the	O
resolution	O
of	O
the	O
first	O
module	O
(	O
M	O
0	O
)	O
is	O
4×8	O
.	O

Resolution	S-Method
is	O
doubled	O
between	O
consecutive	O
modules	O
(	O
from	O
M	O
i−1	O
to	O
M	O
i	O
)	O
.	O

Let	O
w	O
i	O
×	O
h	O
i	O
be	O
the	O
resolution	O
of	O
module	O
i.	O
The	O
first	O
module	O
,	O
M	O
0	O
,	O
receives	O
the	O
semantic	O
layout	O
L	O
as	O
input	O
(	O
downsampled	O
to	O
w	O
0	O
×h	O
0	O
)	O
and	O
produces	O
a	O
feature	B-Method
layer	E-Method
F	O
0	O
at	O
resolution	O
w	O
0	O
×	O
h	O
0	O
as	O
output	O
.	O

All	O
other	O
modules	O
M	O
i	O
(	O
for	O
i	O
=	O
0	O
)	O
are	O
structurally	O
identical	O
:	O
M	O
i	O
receives	O
a	O
concatenation	O
of	O
the	O
layout	O
L	O
(	O
downsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
and	O
the	O
feature	O
layer	O
F	O
i−1	O
(	O
upsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
as	O
input	O
,	O
and	O
produces	O
feature	O
layer	O
F	O
i	O
as	O
output	O
.	O

We	O
denote	O
the	O
number	O
of	O
feature	O
maps	O
in	O
F	O
i	O
by	O
d	O
i	O
.	O

Each	O
module	O
M	O
i	O
consists	O
of	O
three	O
feature	B-Method
layers	E-Method
:	O
the	O
input	B-Method
layer	E-Method
,	O
an	O
intermediate	B-Method
layer	E-Method
,	O
and	O
the	O
output	O
layer	O
.	O

This	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O

The	O
input	O
layer	O
has	O
dimensionality	O
w	O
i	O
×h	O
i	O
×	O
(	O
d	O
i−1	O
+	O
c	O
)	O
and	O
is	O
a	O
concatenation	O
of	O
the	O
downsampled	B-Method
semantic	I-Method
layout	I-Method
L	E-Method
(	O
c	O
channels	O
)	O
and	O
a	O
bilinearly	B-Method
upsampled	I-Method
feature	I-Method
layer	E-Method
F	O
i−1	O
(	O
d	O
i−1	O
channels	O
)	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
upconvolutions	S-Method
because	O
upconvolutions	S-Method
tend	O
to	O
introduce	O
characteristic	O
artifacts	O
[	O
reference	O
]	O
.	O

The	O
intermediate	O
layer	O
and	O
the	O
output	O
layer	O
both	O
have	O
dimensionality	O
w	O
i	O
×h	O
i	O
×d	O
i	O
.	O

Each	O
layer	O
is	O
followed	O
by	O
3×3	O
convolutions	O
,	O
layer	B-Method
normalization	E-Method
[	O
reference	O
]	O
,	O
and	O
LReLU	B-Method
nonlinearity	E-Method
[	O
reference	O
]	O
.	O

The	O
output	O
layer	O
Fī	O
of	O
the	O
final	O
module	O
Mī	S-Method
is	O
not	O
followed	O
by	O
normalization	O
or	O
nonlinearity	O
.	O

Instead	O
,	O
a	O
linear	B-Method
projection	E-Method
(	O
1×1	B-Method
convolution	E-Method
)	O
is	O
applied	O
to	O
map	O
Fī	O
(	O
dimensionality	O
wī	O
×hī	O
×dī	O
)	O
to	O
the	O
output	O
color	O
image	O
(	O
dimensionality	O
wī	O
×hī	O
×3	O
)	O
.	O

The	O
total	O
number	O
of	O
refinement	O
modules	O
in	O
a	O
cascade	O
depends	O
on	O
the	O
output	O
resolution	O
.	O

section	O
:	O
Training	O
The	O
CRN	S-Method
is	O
trained	O
in	O
a	O
supervised	B-Method
fashion	E-Method
on	O
a	O
semantic	O
segmentation	O
dataset	O
D	O
=	O
{	O
(	O
I	O
,	O
L	O
)	O
}.	O
A	O
semantic	O
layout	O
L	O
is	O
used	O
as	O
input	O
and	O
the	O
corresponding	O
color	O
image	O
I	O
as	O
output	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
"	O
inverse	B-Task
semantic	I-Task
segmentation	E-Task
"	O
.	O

It	O
is	O
an	O
underconstrained	B-Task
one	I-Task
-	I-Task
to	I-Task
-	I-Task
many	I-Task
inverse	I-Task
problem	E-Task
.	O

We	O
will	O
generally	O
refer	O
to	O
I	O
as	O
a	O
"	O
reference	O
image	O
"	O
rather	O
than	O
"	O
ground	O
truth	O
"	O
,	O
since	O
many	O
valid	O
photographic	O
images	O
could	O
have	O
yielded	O
the	O
same	O
semantic	O
layout	O
.	O

Given	O
the	O
underconstrained	O
nature	O
of	O
the	O
problem	O
,	O
using	O
an	O
appropriate	O
loss	B-Method
function	E-Method
is	O
critical	O
,	O
as	O
observed	O
in	O
prior	O
work	O
on	O
image	B-Task
synthesis	E-Task
.	O

Simply	O
comparing	O
the	O
pixel	O
colors	O
of	O
the	O
synthesized	O
image	O
and	O
the	O
reference	O
image	O
could	O
severely	O
penalize	O
perfectly	O
realistic	O
outputs	O
.	O

For	O
example	O
,	O
synthesizing	O
a	O
white	O
car	O
instead	O
of	O
a	O
black	O
car	O
would	O
induce	O
a	O
very	O
high	O
loss	O
.	O

Instead	O
we	O
adopt	O
the	O
"	O
content	B-Method
representation	E-Method
"	O
of	O
Gatys	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
also	O
referred	O
to	O
as	O
a	O
perceptual	B-Task
loss	E-Task
or	O
feature	B-Task
matching	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

The	O
basic	O
idea	O
is	O
to	O
match	O
activations	O
in	O
a	O
visual	B-Method
perception	I-Method
network	E-Method
that	O
is	O
applied	O
to	O
the	O
synthesized	O
image	O
and	O
separately	O
to	O
the	O
reference	O
image	O
.	O

Let	O
Φ	O
be	O
a	O
trained	O
visual	B-Method
perception	I-Method
network	E-Method
(	O
we	O
use	O
VGG	B-Method
-	I-Method
19	E-Method
[	O
reference	O
]	O
)	O
.	O

Layers	O
in	O
the	O
network	O
represent	O
an	O
image	O
at	O
increasing	O
levels	O
of	O
abstraction	O
:	O
from	O
edges	O
and	O
colors	O
to	O
objects	O
and	O
categories	O
.	O

Matching	O
both	O
lower	O
-	O
layer	O
and	O
higher	O
-	O
layer	O
activations	O
in	O
the	O
perception	B-Method
network	E-Method
guides	O
the	O
synthesis	B-Method
network	E-Method
to	O
learn	O
both	O
fine	O
-	O
grained	O
details	O
and	O
more	O
global	O
part	O
arrangement	O
.	O

Let	O
{	O
Φ	O
l	O
}	O
be	O
a	O
collection	O
of	O
layers	O
in	O
the	O
network	B-Method
Φ	E-Method
,	O
such	O
that	O
Φ	O
0	O
denotes	O
the	O
input	O
image	O
.	O

Each	O
layer	O
is	O
a	O
threedimensional	O
tensor	O
.	O

For	O
a	O
training	O
pair	O
(	O
I	O
,	O
L	O
)	O
∈	O
D	O
,	O
our	O
loss	S-Metric
is	O
Here	O
g	O
is	O
the	O
image	B-Method
synthesis	I-Method
network	E-Method
being	O
trained	O
and	O
θ	O
is	O
the	O
set	O
of	O
parameters	O
of	O
this	O
network	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
balance	O
the	O
contribution	O
of	O
each	O
layer	O
l	O
to	O
the	O
loss	O
.	O

For	O
layers	O
Φ	O
l	O
(	O
l	O
≥	O
1	O
)	O
we	O
use	O
'	O
conv1	O
2	O
'	O
,	O
'	O
conv2	O
2	O
'	O
,	O
'	O
conv3	O
2	O
'	O
,	O
'	O
conv4	O
2	O
'	O
,	O
and	O
'	O
conv5	O
2	O
'	O
in	O
VGG	B-Method
-	I-Method
19	E-Method
[	O
reference	O
]	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
are	O
set	O
automatically	O
.	O

They	O
are	O
initialized	O
to	O
the	O
inverse	O
of	O
the	O
number	O
of	O
elements	O
in	O
each	O
layer	O
.	O

After	O
100	O
epochs	O
,	O
{	O
λ	O
l	O
}	O
are	O
rescaled	O
to	O
normalize	O
the	O
expected	O
contribution	O
of	O
each	O
term	O
Φ	O
l	O
(	O
I	O
)	O
−	O
Φ	O
l	O
(	O
g	O
(	O
L	O
;	O
θ	O
)	O
)	O
1	O
to	O
the	O
loss	O
.	O

section	O
:	O
Synthesizing	O
a	O
diverse	O
collection	O
The	O
architecture	O
and	O
training	B-Method
procedure	E-Method
described	O
so	O
far	O
synthesize	O
a	O
single	O
image	O
for	O
a	O
given	O
input	O
L.	O
In	O
our	O
experiments	O
this	O
already	O
yields	O
good	O
results	O
.	O

However	O
,	O
since	O
a	O
given	O
semantic	O
layout	O
can	O
correspond	O
to	O
many	O
images	O
,	O
it	O
also	O
makes	O
sense	O
to	O
generate	O
a	O
diverse	O
set	O
of	O
images	O
as	O
output	O
.	O

Conditional	B-Task
synthesis	I-Task
of	I-Task
diverse	I-Task
images	E-Task
can	O
be	O
approached	O
as	O
a	O
stochastic	B-Method
process	E-Method
[	O
reference	O
]	O
.	O

We	O
take	O
a	O
different	O
tack	O
and	O
modify	O
the	O
network	O
to	O
emit	O
a	O
collection	O
of	O
images	O
in	O
one	O
shot	O
,	O
with	O
a	O
modified	O
loss	O
that	O
encourages	O
diversity	O
within	O
the	O
collection	O
.	O

Specifically	O
,	O
we	O
change	O
the	O
number	O
of	O
output	O
channels	O
from	O
3	O
to	O
3k	O
,	O
where	O
k	O
is	O
the	O
desired	O
number	O
of	O
images	O
.	O

Each	O
consecutive	O
3	O
-	O
tuple	O
of	O
channels	O
forms	O
an	O
image	O
.	O

Now	O
consider	O
the	O
loss	O
.	O

If	O
loss	O
(	O
1	O
)	O
is	O
applied	O
independently	O
to	O
each	O
output	O
image	O
,	O
the	O
k	O
synthesized	O
images	O
will	O
be	O
identical	O
.	O

Our	O
first	O
modification	O
is	O
to	O
consider	O
the	O
set	O
of	O
k	O
outputs	O
together	O
and	O
define	O
the	O
loss	O
of	O
the	O
whole	O
collection	O
in	O
terms	O
of	O
the	O
best	O
synthesized	O
image	O
.	O

Let	O
g	O
u	O
(	O
L	O
;	O
θ	O
)	O
be	O
the	O
u	O
th	O
image	O
in	O
the	O
synthesized	O
collection	O
.	O

Our	O
first	O
version	O
of	O
the	O
modified	O
loss	O
is	O
based	O
on	O
the	O
hindsight	B-Method
loss	E-Method
developed	O
for	O
multiple	B-Task
choice	I-Task
learning	E-Task
[	O
reference	O
]	O
:	O
By	O
considering	O
only	O
the	O
best	O
synthesized	O
image	O
,	O
this	O
loss	O
encourages	O
the	O
network	O
to	O
spread	O
its	O
bets	O
and	O
cover	O
the	O
space	O
of	O
images	O
that	O
conform	O
to	O
the	O
input	O
semantic	O
layout	O
.	O

The	O
loss	O
is	O
structurally	O
akin	O
to	O
the	O
k	B-Method
-	I-Method
means	I-Method
clustering	I-Method
objective	E-Method
,	O
which	O
only	O
considers	O
the	O
closest	O
centroid	O
to	O
each	O
datapoint	O
and	O
thus	O
encourages	O
the	O
centroids	O
to	O
spread	O
and	O
cover	O
the	O
dataset	O
.	O

We	O
further	O
build	O
on	O
this	O
idea	O
and	O
formulate	O
a	O
loss	S-Method
that	O
considers	O
a	O
virtual	O
collection	O
of	O
up	O
to	O
k	O
c	O
images	O
.	O

(	O
Recall	O
that	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

)	O
Specifically	O
,	O
for	O
each	O
semantic	O
class	O
p	O
,	O
let	O
L	O
p	O
denote	O
the	O
corresponding	O
channel	O
L	O
(	O
·	O
,	O
·	O
,	O
p	O
)	O
in	O
the	O
input	O
label	O
map	O
.	O

We	O
now	O
define	O
a	O
more	O
powerful	O
diversity	O
loss	O
as	O
where	O
Φ	O
j	O
l	O
is	O
the	O
j	O
th	O
feature	O
map	O
in	O
Φ	O
l	O
,	O
L	O
l	O
p	O
is	O
the	O
mask	O
L	O
p	O
downsampled	O
to	O
match	O
the	O
resolution	O
of	O
Φ	O
l	O
,	O
and	O
is	O
the	O
Hadamard	B-Method
product	E-Method
.	O

This	O
loss	O
in	O
effect	O
constructs	O
a	O
virtual	O
image	O
by	O
adaptively	O
taking	O
the	O
best	O
synthesized	O
content	O
for	O
each	O
semantic	O
class	O
from	O
the	O
whole	O
collection	O
,	O
and	O
scoring	O
the	O
collection	O
based	O
on	O
this	O
assembled	O
image	O
.	O

section	O
:	O
Baselines	O
The	O
approach	O
presented	O
in	O
Section	O
3	O
is	O
far	O
from	O
the	O
first	O
we	O
tried	O
.	O

In	O
this	O
section	O
we	O
describe	O
a	O
number	O
of	O
alternative	O
approaches	O
that	O
will	O
be	O
used	O
as	O
baselines	O
in	O
Section	O
5	O
.	O

GAN	S-Method
and	O
semantic	B-Task
segmentation	E-Task
.	O

Our	O
first	O
baseline	O
is	O
consistent	O
with	O
current	O
trends	O
in	O
the	O
research	O
community	O
.	O

It	O
combines	O
a	O
GAN	S-Method
with	O
a	O
semantic	B-Method
segmentation	I-Method
objective	E-Method
.	O

The	O
generator	O
is	O
trained	O
to	O
synthesize	O
an	O
image	O
that	O
fools	O
the	O
discriminator	S-Method
[	O
reference	O
]	O
.	O

An	O
additional	O
term	O
in	O
the	O
loss	O
specifies	O
that	O
when	O
the	O
synthesized	O
image	O
is	O
given	O
as	O
input	O
to	O
a	O
pretrained	B-Method
semantic	I-Method
segmentation	I-Method
network	E-Method
,	O
it	O
should	O
produce	O
a	O
label	O
map	O
that	O
is	O
as	O
close	O
to	O
the	O
input	O
layout	O
L	O
as	O
possible	O
.	O

The	O
GAN	O
setup	O
follows	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

The	O
input	O
to	O
the	O
generator	O
is	O
the	O
semantic	O
layout	O
L.	O
For	O
the	O
semantic	B-Task
segmentation	I-Task
network	E-Task
,	O
we	O
use	O
publicly	O
available	O
networks	O
that	O
were	O
pretrained	O
for	O
the	O
Cityscapes	S-Material
dataset	O
[	O
reference	O
]	O
and	O
the	O
NYU	B-Material
dataset	E-Material
[	O
reference	O
]	O
.	O

The	O
training	B-Metric
objective	E-Metric
combines	O
the	O
GAN	O
loss	O
and	O
the	O
semantic	B-Method
segmentation	E-Method
(	O
pixelwise	B-Method
cross	I-Method
-	I-Method
entropy	E-Method
)	O
loss	O
.	O

Full	B-Method
-	I-Method
resolution	I-Method
network	E-Method
.	O

Our	O
second	O
baseline	O
is	O
a	O
feedforward	B-Method
convolutional	I-Method
network	E-Method
that	O
operates	O
at	O
full	O
resolution	O
.	O

This	O
baseline	O
uses	O
the	O
same	O
loss	O
as	O
the	O
CRN	S-Method
described	O
in	O
Section	O
3	O
.	O

The	O
only	O
difference	O
is	O
the	O
network	B-Method
architecture	E-Method
.	O

In	O
particular	O
,	O
we	O
have	O
experimented	O
with	O
variants	O
of	O
the	O
multi	B-Method
-	I-Method
scale	I-Method
context	I-Method
aggregation	I-Method
network	E-Method
[	O
reference	O
]	O
.	O

An	O
appealing	O
property	O
of	O
this	O
network	O
is	O
that	O
it	O
retains	O
high	O
resolution	O
in	O
the	O
intermediate	O
layers	O
,	O
which	O
we	O
hypothesized	O
to	O
be	O
helpful	O
for	O
photorealistic	B-Task
image	I-Task
synthesis	E-Task
.	O

The	O
original	O
architecture	O
described	O
in	O
[	O
reference	O
]	O
did	O
not	O
yield	O
good	O
results	O
and	O
is	O
not	O
well	O
-	O
suited	O
to	O
our	O
problem	O
,	O
because	O
the	O
input	O
semantic	O
layouts	O
are	O
piecewise	O
constant	O
and	O
the	O
network	O
of	O
[	O
reference	O
]	O
begins	O
with	O
a	O
small	O
receptive	O
field	O
.	O

We	O
obtained	O
much	O
better	O
results	O
with	O
the	O
inverse	B-Method
architecture	E-Method
:	O
start	O
with	O
large	O
dilation	O
and	O
decrease	O
it	O
by	O
a	O
factor	O
of	O
2	O
in	O
each	O
layer	O
.	O

This	O
can	O
be	O
viewed	O
as	O
a	O
full	B-Task
-	I-Task
resolution	I-Task
counterpart	E-Task
to	O
the	O
CRN	S-Method
,	O
based	O
on	O
dilating	O
the	O
filters	S-Method
instead	O
of	O
scaling	O
the	O
feature	O
maps	O
.	O

One	O
of	O
the	O
drawbacks	O
of	O
this	O
approach	O
is	O
that	O
all	O
intermediate	B-Method
feature	I-Method
layers	E-Method
are	O
at	O
full	O
image	O
resolution	O
and	O
have	O
a	O
high	O
memory	O
footprint	O
.	O

Thus	O
the	O
ratio	O
of	O
capacity	O
(	O
number	O
of	O
parameters	O
)	O
to	O
memory	B-Metric
footprint	E-Metric
is	O
much	O
lower	O
than	O
in	O
the	O
CRN	S-Method
.	O

This	O
high	O
memory	O
footprint	O
of	O
intermediate	O
layers	O
also	O
constrains	O
the	O
resolution	O
to	O
which	O
this	O
approach	O
can	O
scale	O
:	O
with	O
10	O
layers	O
and	O
256	O
feature	O
maps	O
per	O
layer	O
,	O
the	O
maximal	O
resolution	O
that	O
could	O
be	O
trained	O
with	O
available	O
GPU	O
memory	O
is	O
256×512	O
.	O

Encoder	B-Method
-	I-Method
decoder	E-Method
.	O

Our	O
third	O
baseline	O
is	O
an	O
encoder	B-Method
-	I-Method
decoder	I-Method
network	E-Method
,	O
the	O
u	B-Method
-	I-Method
net	E-Method
[	O
reference	O
]	O
.	O

This	O
network	O
is	O
also	O
trained	O
with	O
the	O
same	O
loss	S-Metric
as	O
the	O
CRN	S-Method
.	O

It	O
is	O
thus	O
an	O
additional	O
baseline	O
that	O
evaluates	O
the	O
effect	O
of	O
using	O
the	O
CRN	S-Method
versus	O
a	O
different	O
architecture	O
,	O
when	O
everything	O
else	O
(	O
loss	O
,	O
training	O
procedure	O
)	O
is	O
held	O
fixed	O
.	O

Image	B-Task
-	I-Task
space	I-Task
loss	E-Task
.	O

Our	O
next	O
baseline	O
controls	O
for	O
the	O
feature	B-Task
matching	I-Task
loss	E-Task
used	O
to	O
train	O
the	O
CRN	S-Method
.	O

Here	O
we	O
use	O
exactly	O
the	O
same	O
architecture	O
as	O
in	O
Section	O
3	O
,	O
but	O
use	O
only	O
the	O
first	O
layer	O
Φ	O
0	O
(	O
image	O
color	O
)	O
in	O
the	O
loss	O
:	O
Image	B-Task
-	I-Task
to	I-Task
-	I-Task
image	I-Task
translation	E-Task
.	O

Our	O
last	O
baseline	O
is	O
the	O
contemporaneous	B-Method
approach	E-Method
of	O
Isola	O
et	O
al	O
.	O

,	O
the	O
implementation	O
and	O
results	O
of	O
which	O
are	O
publicly	O
available	O
[	O
reference	O
]	O
.	O

This	O
approach	O
uses	O
a	O
conditional	B-Method
GAN	E-Method
and	O
is	O
representative	O
of	O
the	O
dominant	O
stream	O
of	O
research	O
in	O
image	B-Task
synthesis	E-Task
.	O

The	O
generator	O
is	O
an	O
encoder	B-Method
-	I-Method
decoder	E-Method
[	O
reference	O
]	O
.	O

The	O
GAN	O
setup	O
is	O
derived	O
from	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Experiments	O
section	O
:	O
Experimental	O
procedure	O
Methodology	O
.	O

The	O
most	O
reliable	O
known	O
methodology	O
for	O
evaluating	O
the	O
realism	S-Task
of	O
synthesized	O
images	O
is	O
perceptual	O
experiments	O
with	O
human	O
observers	O
.	O

Such	O
experiments	O
yield	O
quantitative	O
results	O
and	O
have	O
been	O
used	O
in	O
related	O
work	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

There	O
have	O
also	O
been	O
attempts	O
to	O
design	O
automatic	B-Metric
measures	E-Metric
that	O
evaluate	O
realism	O
without	O
humans	O
in	O
the	O
loop	O
.	O

For	O
example	O
,	O
Salimans	O
et	O
al	O
.	O

ran	O
a	O
pretrained	B-Method
image	I-Method
classification	I-Method
network	E-Method
on	O
synthesized	O
images	O
and	O
analyzed	O
its	O
predictions	O
[	O
reference	O
]	O
.	O

We	O
experimented	O
with	O
such	O
automatic	B-Metric
measures	E-Metric
(	O
for	O
example	O
using	O
pretrained	B-Method
semantic	I-Method
segmentation	I-Method
networks	E-Method
)	O
and	O
found	O
that	O
they	O
can	O
all	O
be	O
fooled	O
by	O
augmenting	O
any	O
baseline	O
to	O
also	O
optimize	O
for	O
the	O
evaluated	O
measure	O
;	O
the	O
resulting	O
images	O
are	O
not	O
more	O
realistic	O
but	O
score	O
very	O
highly	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Well	O
-	O
designed	O
perceptual	O
experiments	O
with	O
human	O
observers	O
are	O
more	O
reliable	O
.	O

We	O
therefore	O
use	O
carefully	O
designed	O
perceptual	O
experiments	O
for	O
quantitative	B-Task
evaluation	E-Task
.	O

We	O
will	O
release	O
our	O
complete	O
implementation	O
and	O
experimental	O
setup	O
so	O
that	O
our	O
experiments	O
can	O
be	O
replicated	O
by	O
others	O
.	O

All	O
experiments	O
use	O
pairwise	O
A	O
/	O
B	O
tests	O
deployed	O
on	O
the	O
Amazon	B-Method
Mechanical	I-Method
Turk	E-Method
(	O
MTurk	S-Method
)	O
platform	O
.	O

Similar	O
protocols	O
have	O
been	O
used	O
to	O
evaluate	O
the	O
realism	B-Task
of	I-Task
3D	I-Task
reconstructions	E-Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Each	O
MTurk	S-Method
job	O
involves	O
a	O
batch	O
of	O
roughly	O
100	O
pairwise	O
comparisons	O
,	O
along	O
with	O
sentinel	O
pairs	O
that	O
test	O
whether	O
the	O
worker	O
is	O
attentive	O
and	O
diligent	O
.	O

Each	O
pair	O
contains	O
two	O
images	O
synthesized	O
for	O
the	O
same	O
label	O
map	O
by	O
two	O
different	O
approaches	O
(	O
or	O
a	O
corresponding	O
reference	O
image	O
from	O
the	O
dataset	O
)	O
.	O

The	O
workers	O
are	O
asked	O
to	O
select	O
the	O
more	O
realistic	O
image	O
in	O
each	O
pair	O
.	O

The	O
images	O
are	O
all	O
shown	O
at	O
the	O
same	O
resolution	O
(	O
200×400	O
)	O
.	O

The	O
comparisons	O
are	O
randomized	O
across	O
conditions	O
and	O
both	O
the	O
left	O
-	O
right	O
order	O
and	O
the	O
order	O
within	O
a	O
job	O
are	O
randomized	O
.	O

Two	O
types	O
of	O
experiments	O
are	O
conducted	O
.	O

In	O
the	O
first	O
,	O
images	O
are	O
shown	O
for	O
unlimited	O
time	O
and	O
the	O
worker	O
is	O
free	O
to	O
spend	O
as	O
much	O
time	O
as	O
desired	O
on	O
each	O
pair	O
.	O

In	O
the	O
second	O
,	O
each	O
pair	O
is	O
shown	O
for	O
a	O
randomly	O
chosen	O
duration	O
between	O
Table	O
1	O
.	O

Results	O
of	O
pairwise	B-Metric
comparisons	E-Metric
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	S-Material
and	O
NYU	B-Material
datasets	E-Material
.	O

Each	O
column	O
compares	O
our	O
approach	O
with	O
one	O
of	O
the	O
baselines	O
.	O

Each	O
cell	O
lists	O
the	O
fraction	O
of	O
pairwise	O
comparisons	O
in	O
which	O
images	O
synthesized	O
by	O
our	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
corresponding	O
baseline	O
.	O

Chance	O
is	O
at	O
50	O
%	O
.	O

Datasets	O
.	O

We	O
use	O
two	O
datasets	O
with	O
pixelwise	O
semantic	O
labels	O
,	O
one	O
depicting	O
outdoor	O
scenes	O
and	O
one	O
depicting	O
indoor	O
scenes	O
.	O

Our	O
primary	O
dataset	O
is	O
Cityscapes	S-Material
,	O
which	O
has	O
become	O
the	O
dominant	O
semantic	O
segmentation	O
dataset	O
due	O
to	O
the	O
quality	O
of	O
the	O
data	O
[	O
reference	O
]	O
.	O

We	O
train	O
on	O
the	O
training	O
set	O
(	O
3	O
K	O
images	O
)	O
and	O
evaluate	O
on	O
the	O
validation	O
set	O
(	O
500	O
images	O
)	O
.	O

(	O
Evaluating	O
"	O
inverse	B-Task
semantic	I-Task
segmentation	E-Task
"	O
on	O
the	O
test	O
set	O
is	O
impossible	O
because	O
the	O
label	O
maps	O
are	O
not	O
provided	O
.	O

)	O
Our	O
second	O
dataset	O
is	O
the	O
older	O
NYU	B-Material
dataset	I-Material
of	I-Material
indoor	I-Material
scenes	E-Material
[	O
reference	O
]	O
.	O

This	O
dataset	O
is	O
smaller	O
and	O
the	O
images	O
are	O
VGA	B-Material
resolution	E-Material
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
depth	O
data	O
in	O
the	O
NYU	B-Material
dataset	E-Material
,	O
only	O
the	O
semantic	O
layouts	O
and	O
the	O
color	O
images	O
.	O

We	O
use	O
the	O
first	O
1200	O
of	O
the	O
1449	O
labeled	O
images	O
for	O
training	O
and	O
the	O
remaining	O
249	O
for	O
testing	O
.	O

section	O
:	O
Results	O
Primary	O
experiments	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	S-Material
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
four	O
alternative	O
approaches	O
.	O

Note	O
that	O
the	O
'	O
image	B-Method
-	I-Method
space	I-Method
loss	I-Method
'	I-Method
baseline	E-Method
uses	O
the	O
same	O
architecture	O
as	O
the	O
CRN	S-Method
and	O
controls	O
for	O
the	O
loss	O
,	O
while	O
the	O
'	O
full	B-Method
-	I-Method
resolution	I-Method
network	E-Method
'	O
and	O
the	O
'	O
encoder	B-Method
-	I-Method
decoder	E-Method
'	O
use	O
the	O
same	O
loss	O
as	O
the	O
CRN	S-Method
and	O
control	O
for	O
the	O
architecture	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Compared	O
to	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	S-Method
were	O
rated	O
more	O
realistic	O
in	O
97	O
%	O
of	O
the	O
comparisons	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
5	O
.	O

Figure	O
4	O
reports	O
the	O
results	O
of	O
time	O
-	O
limited	O
pairwise	O
comparisons	O
of	O
real	O
Cityscapes	S-Material
images	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	S-Method
,	O
and	O
images	O
synthesized	O
by	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
(	O
referred	O
to	O
as	O
'	O
Pix2pix	S-Material
'	O
following	O
the	O
public	O
implementation	O
)	O
.	O

After	O
just	O
1	O
8	O
of	O
a	O
second	O
,	O
the	O
Pix2pix	S-Material
images	O
are	O
clearly	O
rated	O
less	O
realistic	O
than	O
the	O
real	O
Cityscapes	S-Material
images	O
or	O
the	O
CRN	S-Method
images	O
(	O
72.5	O
%	O
Real	S-Material
>	O
Pix2pix	S-Material
,	O
73.4	O
%	O
CRN	S-Method
>	O
Pix2pix	S-Material
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
CRN	S-Method
images	O
are	O
on	O
par	O
with	O
real	O
images	O
at	O
that	O
time	O
,	O
as	O
seen	O
both	O
in	O
the	O
Real	B-Metric
>	I-Metric
CRN	I-Metric
rate	E-Metric
(	O
52.6	O
%	O
)	O
and	O
in	O
the	O
nearly	O
identical	O
Real	S-Material
>	O
Pix2pix	S-Material
and	O
CRN	S-Method
>	O
Pix2pix	S-Material
rates	O
.	O

At	O
250	O
milliseconds	O
(	O
1	O
4	O
of	O
a	O
second	O
)	O
,	O
the	O
Real	S-Material
>	O
Pix2pix	S-Material
rate	O
rises	O
to	O
85.0	O
%	O
while	O
the	O
Real	B-Metric
>	I-Metric
CRN	I-Metric
rate	E-Metric
is	O
at	O
57.4	O
%	O
.	O

The	O
CRN	S-Method
>	O
Pix2pix	S-Material
rate	O
is	O
84.0	O
%	O
,	O
still	O
nearly	O
identical	O
to	O
Real	S-Material
>	O
Pix2pix	S-Material
.	O

At	O
500	O
milliseconds	O
,	O
the	O
Real	S-Material
>	O
Pix2pix	S-Material
and	O
CRN	S-Method
>	O
Pix2pix	S-Material
rates	O
finally	O
diverge	O
,	O
although	O
both	O
are	O
extremely	O
high	O
(	O
95.1	O
%	O
and	O
87.4	O
%	O
,	O
respectively	O
)	O
,	O
and	O
the	O
Real	B-Metric
>	I-Metric
CRN	I-Metric
rate	E-Metric
rises	O
to	O
64.2	O
%	O
.	O

Over	O
time	O
,	O
the	O
CRN	S-Method
>	O
Pix2pix	S-Material
rate	O
rises	O
above	O
90	O
%	O
and	O
the	O
Real	S-Material
>	O
Pix2pix	S-Material
rate	O
remains	O
consistently	O
higher	O
than	O
the	O
Real	B-Metric
>	I-Metric
CRN	I-Metric
rate	E-Metric
.	O

NYU	B-Material
dataset	E-Material
.	O

We	O
conduct	O
supporting	O
experiments	O
on	O
the	O
NYU	B-Material
dataset	E-Material
.	O

This	O
dataset	O
is	O
smaller	O
and	O
lower	O
-	O
resolution	O
,	O
so	O
the	O
quality	O
of	O
images	O
synthesized	O
by	O
all	O
approaches	O
is	O
lower	O
.	O

Nevertheless	O
,	O
the	O
differences	O
are	O
still	O
clear	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
for	O
this	O
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
again	O
rated	O
consistently	O
more	O
realistic	O
than	O
the	O
baselines	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
6	O
.	O

Diversity	B-Task
loss	E-Task
.	O

For	O
all	O
preceding	O
experiments	O
we	O
have	O
used	O
the	O
feature	O
matching	O
loss	O
specified	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
models	O
produced	O
a	O
single	O
image	O
as	O
output	O
,	O
and	O
this	O
image	O
was	O
evaluated	O
against	O
baselines	O
.	O

We	O
now	O
qualitatively	O
demonstrate	O
the	O
effect	O
of	O
the	O
diversity	O
loss	O
described	O
in	O
Section	O
3.4	O
.	O

To	O
this	O
end	O
we	O
trained	O
models	O
that	O
produce	O
image	O
collections	O
as	O
output	O
(	O
9	O
images	O
at	O
a	O
time	O
)	O
.	O

Figure	O
7	O
shows	O
pairs	O
of	O
images	O
sampled	O
from	O
the	O
synthesized	O
collections	O
,	O
for	O
different	O
input	O
layouts	O
in	O
the	O
NYU	B-Material
validation	I-Material
set	E-Material
.	O

The	O
figure	O
illustrates	O
that	O
the	O
diversity	O
loss	O
does	O
lead	O
the	O
output	O
channels	O
to	O
spread	O
out	O
and	O
produce	O
different	O
appearances	O
.	O

section	O
:	O
Semantic	B-Method
layout	E-Method
section	O
:	O
GAN	B-Method
+	I-Method
semantic	I-Method
segmenation	I-Method
Full	I-Method
-	I-Method
resolution	I-Method
network	E-Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Encoder	B-Method
-	I-Method
decoder	I-Method
Semantic	I-Method
layout	E-Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Full	B-Method
-	I-Method
resolution	I-Method
network	I-Method
Encoder	I-Method
-	I-Method
decoder	E-Method
Figure	O
6	O
.	O

Qualitative	O
comparison	O
on	O
the	O
NYU	B-Material
dataset	E-Material
.	O

Figure	O
7	O
.	O

Synthesizing	O
a	O
diverse	O
collection	O
,	O
illustrated	O
on	O
the	O
NYU	B-Material
dataset	E-Material
.	O

Each	O
pair	O
shows	O
two	O
images	O
from	O
a	O
collection	O
synthesized	O
for	O
a	O
given	O
semantic	O
layout	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
direct	O
approach	O
to	O
photographic	B-Task
image	I-Task
synthesis	E-Task
conditioned	O
on	O
pixelwise	B-Method
semantic	I-Method
layouts	E-Method
.	O

Images	O
are	O
synthesized	O
by	O
a	O
convolutional	B-Method
network	E-Method
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
regression	B-Method
loss	E-Method
.	O

This	O
direct	O
approach	O
is	O
considerably	O
simpler	O
than	O
contemporaneous	O
work	O
,	O
and	O
produces	O
much	O
more	O
realistic	O
results	O
.	O

We	O
hope	O
that	O
the	O
simplicity	O
of	O
the	O
presented	O
approach	O
can	O
support	O
follow	O
-	O
up	O
work	O
that	O
will	O
further	O
advance	O
realism	O
and	O
explore	O
the	O
applications	O
of	O
photographic	B-Task
image	I-Task
synthesis	E-Task
.	O

Our	O
results	O
,	O
while	O
significantly	O
more	O
realistic	O
than	O
the	O
prior	O
state	O
of	O
the	O
art	O
,	O
are	O
clearly	O
not	O
indistinguishable	O
from	O
real	O
HD	O
images	O
.	O

Exciting	O
work	O
remains	O
to	O
be	O
done	O
to	O
achieve	O
perfect	O
photorealism	O
.	O

If	O
such	O
level	O
of	O
realism	O
is	O
ever	O
achieved	O
,	O
which	O
we	O
believe	O
to	O
be	O
possible	O
,	O
alternative	O
routes	O
for	O
image	B-Task
synthesis	E-Task
in	O
computer	B-Task
graphics	E-Task
will	O
open	O
up	O
.	O

section	O
:	O
document	O
:	O
Loss	B-Method
-	I-Method
Sensitive	I-Method
Generative	I-Method
Adversarial	I-Method
Networks	E-Method
on	O
Lipschitz	B-Method
Densities	E-Method
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
Lipschitz	B-Method
regularization	I-Method
theory	E-Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	B-Method
-	I-Method
Sensitive	I-Method
Generative	I-Method
Adversarial	I-Method
Network	E-Method
(	O
LS	B-Method
-	I-Method
GAN	E-Method
)	O
.	O

Specifically	O
,	O
it	O
trains	O
a	O
loss	O
function	O
to	O
distinguish	O
between	O
real	O
and	O
fake	O
samples	O
by	O
designated	O
margins	O
,	O
while	O
learning	O
a	O
generator	S-Method
alternately	O
to	O
produce	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

The	O
LS	B-Method
-	I-Method
GAN	E-Method
further	O
regularizes	O
its	O
loss	B-Method
function	E-Method
with	O
a	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
density	O
of	O
real	O
data	O
,	O
yielding	O
a	O
regularized	B-Method
model	E-Method
that	O
can	O
better	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
a	O
reasonable	O
number	O
of	O
training	O
examples	O
than	O
the	O
classic	O
GAN	S-Method
.	O

We	O
will	O
further	O
present	O
a	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
GLS	O
-	O
GAN	S-Method
)	O
and	O
show	O
it	O
contains	O
a	O
large	O
family	O
of	O
regularized	O
GAN	S-Method
models	O
,	O
including	O
both	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
Wasserstein	O
GAN	S-Method
,	O
as	O
its	O
special	O
cases	O
.	O

Compared	O
with	O
the	O
other	O
GAN	S-Method
models	O
,	O
we	O
will	O
conduct	O
experiments	O
to	O
show	O
both	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
GLS	B-Method
-	I-Method
GAN	E-Method
exhibit	O
competitive	O
ability	O
in	O
generating	O
new	O
images	O
in	O
terms	O
of	O
the	O
Minimum	B-Metric
Reconstruction	I-Metric
Error	E-Metric
(	O
MRE	S-Metric
)	O
assessed	O
on	O
a	O
separate	O
test	O
set	O
.	O

We	O
further	O
extend	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
to	O
a	O
conditional	B-Method
form	E-Method
for	O
supervised	B-Task
and	I-Task
semi	I-Task
-	I-Task
supervised	I-Task
learning	I-Task
problems	E-Task
,	O
and	O
demonstrate	O
its	O
outstanding	O
performance	O
on	O
image	O
classification	B-Task
tasks	E-Task
.	O

Keywords	O
:	O
Generative	B-Method
Adversarial	I-Method
Nets	E-Method
(	O
GANs	S-Method
)	O
,	O
Lipschitz	O
regularity	O
,	O
Minimum	B-Metric
Reconstruction	I-Metric
Error	E-Metric
(	O
MRE	S-Metric
)	O
section	O
:	O
Introduction	O
A	O
classic	O
Generative	B-Method
Adversarial	I-Method
Net	E-Method
(	O
GAN	S-Method
)	O
learns	O
a	O
discriminator	S-Method
and	O
a	O
generator	S-Method
by	O
playing	O
a	O
two	B-Method
-	I-Method
player	I-Method
minimax	I-Method
game	E-Method
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
discriminator	O
is	O
trained	O
to	O
distinguish	O
real	O
samples	O
from	O
those	O
generated	O
by	O
the	O
generator	O
,	O
and	O
it	O
in	O
turn	O
guides	O
the	O
generator	O
to	O
produce	O
realistic	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

However	O
,	O
from	O
both	O
theoretical	O
and	O
practical	O
perspectives	O
,	O
a	O
critical	O
question	O
is	O
whether	O
the	O
GAN	S-Method
can	O
generate	O
realistic	O
samples	O
from	O
arbitrary	O
data	O
distribution	O
without	O
any	O
prior	O
?	O
If	O
not	O
,	O
what	O
kind	O
of	O
prior	O
ought	O
to	O
be	O
imposed	O
on	O
the	O
data	O
distribution	O
to	O
regularize	O
the	O
GAN	S-Method
?	O
Indeed	O
,	O
the	O
classic	O
GAN	S-Method
imposes	O
no	O
prior	O
on	O
the	O
data	O
distribution	O
.	O

This	O
represents	O
an	O
ambitious	O
goal	O
to	O
generate	O
samples	O
from	O
any	O
distributions	O
.	O

However	O
,	O
it	O
in	O
turn	O
requires	O
a	O
non	B-Method
-	I-Method
parametric	I-Method
discriminator	E-Method
to	O
prove	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
samples	O
by	O
assuming	O
the	O
model	O
has	O
infinite	O
capacity	O
(	O
see	O
Section	O
4	O
of	O
)	O
.	O

This	O
is	O
a	O
too	O
strong	O
assumption	O
to	O
establish	O
the	O
theoretical	O
basis	O
for	O
the	O
GAN	S-Method
.	O

Moreover	O
,	O
with	O
such	O
an	O
assumption	O
,	O
its	O
generalizability	O
becomes	O
susceptible	O
.	O

Specifically	O
,	O
one	O
could	O
argue	O
the	O
learned	B-Method
generator	E-Method
may	O
be	O
overfit	O
by	O
an	O
unregularized	B-Method
discriminator	E-Method
in	O
an	O
non	O
-	O
parametric	O
fashion	O
by	O
merely	O
memorizing	O
or	O
interpolating	O
training	O
examples	O
.	O

In	O
other	O
words	O
,	O
it	O
could	O
lack	O
the	O
generalization	O
ability	O
to	O
generate	O
new	O
samples	O
out	O
of	O
existing	O
data	O
.	O

Indeed	O
,	O
Arora	O
et	O
al	O
.	O

have	O
shown	O
that	O
the	O
GAN	S-Method
minimizing	O
the	O
Jensen	B-Metric
-	I-Metric
Shannon	I-Metric
distance	E-Metric
between	O
the	O
distributions	O
of	O
generated	O
and	O
real	O
data	O
could	O
fail	O
to	O
generalize	O
to	O
produce	O
new	O
samples	O
with	O
a	O
reasonable	O
size	O
of	O
training	O
set	O
.	O

Thus	O
,	O
a	O
properly	O
regularized	O
GAN	S-Method
is	O
demanded	O
to	O
establish	O
provable	O
generalizability	O
by	O
focusing	O
on	O
a	O
restricted	O
yet	O
still	O
sufficiently	O
large	O
family	O
of	O
data	O
distributions	O
.	O

subsection	O
:	O
Objective	O
:	O
Towards	O
Regularized	O
GANs	S-Method
In	O
this	O
paper	O
,	O
we	O
attempt	O
to	O
develop	O
regularization	B-Method
theory	E-Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	S-Method
(	O
LS	B-Method
-	I-Method
GAN	E-Method
)	O
.	O

Specifically	O
,	O
we	O
introduce	O
a	O
loss	B-Method
function	E-Method
to	O
quantify	O
the	O
quality	B-Metric
of	I-Metric
generated	I-Metric
samples	E-Metric
.	O

A	O
constraint	O
is	O
imposed	O
so	O
that	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
counterpart	O
.	O

Specifically	O
,	O
in	O
the	O
learning	B-Method
algorithm	E-Method
,	O
we	O
will	O
define	O
margins	O
to	O
separate	O
the	O
losses	O
between	O
generated	O
and	O
real	O
samples	O
.	O

Then	O
,	O
an	O
optimal	B-Method
generator	E-Method
will	O
be	O
trained	O
to	O
produce	O
realistic	O
samples	O
with	O
minimum	O
losses	O
.	O

The	O
loss	B-Method
function	E-Method
and	O
the	O
generator	S-Method
will	O
be	O
trained	O
in	O
an	O
adversarial	B-Method
fashion	E-Method
until	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
real	O
ones	O
.	O

We	O
will	O
also	O
develop	O
new	O
theory	O
to	O
analyze	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
on	O
the	O
basis	O
of	O
Lipschitz	O
regularity	O
.	O

We	O
note	O
that	O
the	O
reason	O
of	O
making	O
non	B-Method
-	I-Method
parametric	I-Method
assumption	I-Method
of	I-Method
infinite	I-Method
capacity	E-Method
on	O
the	O
discriminator	S-Method
in	O
the	O
classic	O
GAN	S-Method
is	O
due	O
to	O
its	O
ambitious	O
goal	O
to	O
generate	O
data	O
from	O
any	O
arbitrary	O
distribution	O
.	O

However	O
,	O
no	O
free	O
lunch	O
principle	O
reminds	O
us	O
of	O
the	O
need	O
to	O
impose	O
a	O
suitable	O
prior	O
on	O
the	O
data	O
distribution	O
from	O
which	O
real	O
samples	O
are	O
generated	O
.	O

This	O
inspires	O
us	O
to	O
impose	O
a	O
Lipschitz	O
regularity	O
condition	O
by	O
assuming	O
the	O
data	O
density	O
does	O
not	O
change	O
abruptly	O
.	O

Based	O
on	O
this	O
mild	O
condition	O
,	O
we	O
will	O
show	O
that	O
the	O
density	O
of	O
generated	O
samples	O
by	O
LS	B-Method
-	I-Method
GAN	E-Method
can	O
exactly	O
match	O
that	O
of	O
real	O
data	O
.	O

More	O
importantly	O
,	O
the	O
Lipschitz	O
regularity	O
allows	O
us	O
to	O
prove	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
can	O
well	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
training	O
examples	O
.	O

To	O
this	O
end	O
,	O
we	O
will	O
provide	O
a	O
Probably	B-Metric
Approximate	I-Metric
Correct	E-Metric
(	O
PAC	S-Metric
)-	O
style	O
theorem	O
by	O
showing	O
the	O
empirical	B-Method
LS	I-Method
-	I-Method
GAN	I-Method
model	E-Method
trained	O
with	O
a	O
reasonable	O
number	O
of	O
examples	O
can	O
be	O
sufficiently	O
close	O
to	O
the	O
oracle	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
trained	O
with	O
hypothetically	O
known	O
data	O
distribution	O
,	O
thereby	O
proving	O
the	O
generalizability	O
of	O
LS	B-Method
-	I-Method
GAN	E-Method
in	O
generating	O
samples	O
from	O
any	O
Lipschitz	O
data	O
distribution	O
.	O

We	O
will	O
also	O
make	O
a	O
non	B-Method
-	I-Method
parametric	I-Method
analysis	I-Method
of	I-Method
the	I-Method
LS	I-Method
-	I-Method
GAN	E-Method
.	O

It	O
does	O
not	O
rely	O
on	O
any	O
parametric	O
form	O
of	O
the	O
loss	O
function	O
to	O
characterize	O
its	O
optimality	O
in	O
the	O
space	O
of	O
Lipschtiz	O
functions	O
.	O

It	O
gives	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	B-Metric
loss	E-Metric
,	O
which	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	B-Task
gradient	E-Task
.	O

This	O
suggests	O
that	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
can	O
provide	O
sufficient	O
gradient	O
to	O
update	O
its	O
LS	B-Method
-	I-Method
GAN	E-Method
generator	O
even	O
if	O
the	O
loss	O
function	O
has	O
been	O
fully	O
optimized	O
,	O
thus	O
avoiding	O
the	O
vanishing	B-Task
gradient	E-Task
problem	O
that	O
could	O
occur	O
in	O
training	O
the	O
GAN	S-Method
.	O

subsection	O
:	O
Extensions	O
:	O
Generalized	S-Method
and	O
Conditional	B-Method
LS	I-Method
-	I-Method
GANs	E-Method
We	O
further	O
present	O
a	O
generalized	O
form	O
of	O
LS	B-Method
-	I-Method
GAN	E-Method
(	O
GLS	B-Method
-	I-Method
GAN	E-Method
)	O
and	O
conduct	O
experiment	O
to	O
demonstrate	O
it	O
has	O
the	O
best	O
generalization	B-Metric
ability	E-Metric
.	O

We	O
will	O
show	O
this	O
is	O
not	O
a	O
surprising	O
result	O
as	O
the	O
GLS	B-Method
-	I-Method
GAN	E-Method
contains	O
a	O
large	O
family	O
of	O
regularized	O
GANs	S-Method
with	O
both	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
Wasserstein	O
GAN	S-Method
(	O
WGAN	S-Method
)	O
as	O
its	O
special	O
cases	O
.	O

Moreover	O
,	O
we	O
will	O
extend	O
a	O
Conditional	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
CLS	B-Method
-	I-Method
GAN	E-Method
)	O
that	O
can	O
generate	O
samples	O
from	O
given	O
conditions	O
.	O

In	O
particular	O
,	O
with	O
class	O
labels	O
being	O
conditions	O
,	O
the	O
learned	O
loss	O
function	O
can	O
be	O
used	O
as	O
a	O
classifier	S-Method
for	O
both	O
supervised	B-Task
and	I-Task
semi	I-Task
-	I-Task
supervised	I-Task
learning	E-Task
.	O

The	O
advantage	O
of	O
such	O
a	O
classifier	S-Method
arises	O
from	O
its	O
ability	O
of	O
exploring	O
generated	O
examples	O
to	O
uncover	O
intrinsic	O
variations	O
for	O
different	O
classes	O
.	O

Experiment	O
results	O
demonstrate	O
competitive	O
performance	O
of	O
the	O
CLS	B-Method
-	I-Method
GAN	I-Method
classifier	E-Method
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

subsection	O
:	O
Paper	O
Structure	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
reviews	O
the	O
related	O
work	O
,	O
and	O
the	O
proposed	O
LS	B-Method
-	I-Method
GAN	E-Method
is	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
analyze	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
by	O
proving	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
data	O
with	O
the	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
data	O
distribution	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
discuss	O
the	O
generalizability	B-Task
problem	E-Task
arising	O
from	O
using	O
sample	B-Method
means	E-Method
to	O
approximate	O
the	O
expectations	O
in	O
the	O
training	O
objectives	O
.	O

We	O
will	O
make	O
a	O
comparison	O
with	O
Wasserstein	O
GAN	S-Method
(	O
WGAN	S-Method
)	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
present	O
a	O
generalized	O
LS	B-Method
-	I-Method
GAN	E-Method
with	O
both	O
WGAN	S-Method
and	O
LS	B-Method
-	I-Method
GAN	E-Method
as	O
its	O
special	O
cases	O
in	O
Section	O
[	O
reference	O
]	O
.	O

A	O
non	B-Method
-	I-Method
parametric	I-Method
analysis	E-Method
of	O
the	O
algorithm	O
is	O
followed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Then	O
we	O
will	O
show	O
how	O
the	O
model	O
can	O
be	O
extended	O
to	O
a	O
conditional	B-Method
model	E-Method
for	O
both	O
supervised	B-Task
and	I-Task
semi	I-Task
-	I-Task
supervised	I-Task
learning	E-Task
in	O
Section	O
[	O
reference	O
]	O
.	O

Experiment	O
results	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
we	O
conclude	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Source	O
codes	O
.	O

The	O
source	O
codes	O
for	O
both	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
GLS	B-Method
-	I-Method
GAN	E-Method
are	O
available	O
at	O
,	O
in	O
the	O
frameworks	O
of	O
torch	O
,	O
pytorch	O
and	O
tensorflow	S-Method
.	O

LS	B-Method
-	I-Method
GAN	E-Method
is	O
also	O
supported	O
by	O
Microsoft	O
CNTK	O
at	O
.	O

section	O
:	O
Related	O
Work	O
Deep	B-Method
generative	I-Method
models	E-Method
,	O
especially	O
the	O
Generative	O
Adversarial	O
Net	O
(	O
GAN	S-Method
)	O
,	O
have	O
attracted	O
many	O
attentions	O
recently	O
due	O
to	O
their	O
demonstrated	O
abilities	O
of	O
generating	O
real	O
samples	O
following	O
the	O
underlying	O
data	O
densities	O
.	O

In	O
particular	O
,	O
the	O
GAN	S-Method
attempts	O
to	O
learn	O
a	O
pair	O
of	O
discriminator	B-Method
and	I-Method
generator	E-Method
by	O
playing	O
a	O
maximin	B-Method
game	E-Method
to	O
seek	O
an	O
equilibrium	O
,	O
in	O
which	O
the	O
discriminator	O
is	O
trained	O
by	O
distinguishing	O
real	O
samples	O
from	O
generated	O
ones	O
and	O
the	O
generator	S-Method
is	O
optimized	O
to	O
produce	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

A	O
family	O
of	O
GAN	S-Method
architectures	O
have	O
been	O
proposed	O
to	O
implement	O
this	O
idea	O
.	O

For	O
example	O
,	O
recent	O
progresses	O
have	O
shown	O
impressive	O
performances	O
on	O
synthesizing	O
photo	O
-	O
realistic	O
images	O
by	O
constructing	O
multiple	O
strided	B-Method
and	I-Method
factional	I-Method
-	I-Method
strided	I-Method
convolutional	I-Method
layers	E-Method
for	O
discriminators	S-Method
and	O
generators	S-Method
.	O

On	O
the	O
contrary	O
,	O
proposed	O
to	O
use	O
a	O
Laplacian	B-Method
pyramid	E-Method
to	O
produce	O
high	O
-	O
quality	O
images	O
by	O
iteratively	O
adding	O
multiple	O
layers	O
of	O
noises	O
at	O
different	O
resolutions	O
.	O

presented	O
to	O
train	O
a	O
recurrent	B-Method
generative	I-Method
model	E-Method
by	O
using	O
adversarial	B-Method
training	E-Method
to	O
unroll	O
gradient	B-Method
-	I-Method
based	I-Method
optimizations	E-Method
to	O
create	O
high	O
quality	O
images	O
.	O

In	O
addition	O
to	O
designing	O
different	O
GAN	S-Method
networks	O
,	O
research	O
efforts	O
have	O
been	O
made	O
to	O
train	O
the	O
GAN	S-Method
by	O
different	O
criteria	O
.	O

For	O
example	O
,	O
presented	O
an	O
energy	O
-	O
based	O
GAN	S-Method
by	O
minimizing	O
an	O
energy	B-Method
function	E-Method
to	O
learn	O
an	O
optimal	B-Method
discriminator	E-Method
,	O
and	O
an	O
auto	B-Method
-	I-Method
encoder	I-Method
structured	I-Method
discriminator	E-Method
is	O
presented	O
to	O
compute	O
the	O
energy	O
.	O

The	O
authors	O
also	O
present	O
a	O
theoretical	O
analysis	O
by	O
showing	O
this	O
variant	O
of	O
GAN	S-Method
can	O
generate	O
samples	O
whose	O
density	O
can	O
recover	O
the	O
underlying	O
true	O
data	O
density	O
.	O

However	O
,	O
it	O
still	O
needs	O
to	O
assume	O
the	O
discriminator	S-Method
has	O
infinite	O
modeling	O
capacity	O
to	O
prove	O
the	O
result	O
in	O
a	O
non	O
-	O
parametric	O
fashion	O
,	O
and	O
its	O
generalizability	O
of	O
producing	O
new	O
data	O
out	O
of	O
training	O
examples	O
is	O
unknown	O
without	O
theoretical	O
proof	O
or	O
empirical	O
evidence	O
.	O

In	O
addition	O
,	O
presented	O
to	O
analyze	O
the	O
GAN	S-Method
from	O
information	B-Task
theoretical	I-Task
perspective	E-Task
,	O
and	O
they	O
seek	O
to	O
minimize	O
the	O
variational	B-Method
estimate	I-Method
of	I-Method
f	I-Method
-	I-Method
divergence	E-Method
,	O
and	O
show	O
that	O
the	O
classic	O
GAN	S-Method
is	O
included	O
as	O
a	O
special	O
case	O
of	O
f	B-Method
-	I-Method
GAN	E-Method
.	O

In	O
contrast	O
,	O
InfoGAN	S-Method
proposed	O
another	O
information	O
-	O
theoretic	O
GAN	S-Method
to	O
learn	O
disentangled	B-Method
representations	E-Method
capturing	O
various	O
latent	O
concepts	O
and	O
factors	O
in	O
generating	O
samples	O
.	O

Most	O
recently	O
,	O
propose	O
to	O
minimize	O
the	O
Earth	O
-	O
Mover	O
distance	O
between	O
the	O
density	O
of	O
generated	O
samples	O
and	O
the	O
true	O
data	O
density	O
,	O
and	O
they	O
show	O
the	O
resultant	O
Wasserstein	O
GAN	S-Method
(	O
WGAN	B-Method
)	E-Method
can	O
address	O
the	O
vanishing	B-Task
gradient	E-Task
problem	O
that	O
the	O
classic	O
GAN	S-Method
suffers	O
.	O

Besides	O
the	O
class	O
of	O
GANs	S-Method
,	O
there	O
exist	O
other	O
models	O
that	O
also	O
attempt	O
to	O
generate	O
natural	O
images	O
.	O

For	O
example	O
,	O
rendered	O
images	O
by	O
matching	O
features	O
in	O
a	O
convolutional	B-Method
network	E-Method
with	O
respect	O
to	O
reference	O
images	O
.	O

used	O
deconvolutional	B-Method
network	E-Method
to	O
render	O
3D	B-Method
chair	I-Method
models	E-Method
in	O
various	O
styles	O
and	O
viewpoints	O
.	O

introduced	O
a	O
deep	B-Method
recurrent	I-Method
neutral	I-Method
network	I-Method
architecture	E-Method
for	O
image	B-Task
generation	E-Task
with	O
a	O
sequence	O
of	O
variational	B-Method
auto	I-Method
-	I-Method
encoders	E-Method
to	O
iteratively	O
construct	O
complex	O
images	O
.	O

Recent	O
efforts	O
have	O
also	O
been	O
made	O
on	O
leveraging	O
the	O
learned	O
representations	O
by	O
deep	B-Method
generative	I-Method
networks	E-Method
to	O
improve	O
the	O
classification	B-Metric
accuracy	E-Metric
when	O
it	O
is	O
too	O
difficult	O
or	O
expensive	O
to	O
label	O
sufficient	O
training	O
examples	O
.	O

For	O
example	O
,	O
presented	O
variational	B-Method
auto	I-Method
-	I-Method
encoders	E-Method
by	O
combining	O
deep	B-Method
generative	I-Method
models	E-Method
and	O
approximate	B-Method
variational	I-Method
inference	E-Method
to	O
explore	O
both	O
labeled	O
and	O
unlabeled	O
data	O
.	O

treated	O
the	O
samples	O
from	O
the	O
GAN	S-Method
generator	O
as	O
a	O
new	O
class	O
,	O
and	O
explore	O
unlabeled	O
examples	O
by	O
assigning	O
them	O
to	O
a	O
class	O
different	O
from	O
the	O
new	O
one	O
.	O

proposed	O
to	O
train	O
a	O
ladder	B-Method
network	E-Method
by	O
minimizing	O
the	O
sum	B-Metric
of	I-Metric
supervised	E-Metric
and	O
unsupervised	B-Metric
cost	I-Metric
functions	E-Metric
through	O
back	B-Method
-	I-Method
propagation	E-Method
,	O
which	O
avoids	O
the	O
conventional	O
layer	B-Method
-	I-Method
wise	I-Method
pre	I-Method
-	I-Method
training	I-Method
approach	E-Method
.	O

presented	O
an	O
approach	O
to	O
learning	O
a	O
discriminative	B-Method
classifier	E-Method
by	O
trading	O
-	O
off	O
mutual	O
information	O
between	O
observed	O
examples	O
and	O
their	O
predicted	O
classes	O
against	O
an	O
adversarial	B-Method
generative	I-Method
model	E-Method
.	O

sought	O
to	O
jointly	O
distinguish	O
between	O
not	O
only	O
real	O
and	O
generated	O
samples	O
but	O
also	O
their	O
latent	O
variables	O
in	O
an	O
adversarial	B-Method
process	E-Method
.	O

Recently	O
,	O
presented	O
a	O
novel	O
paradigm	O
of	O
localized	O
GANs	S-Method
to	O
explore	O
the	O
local	O
consistency	O
of	O
classifiers	O
in	O
local	O
coordinate	O
charts	O
,	O
as	O
well	O
as	O
showed	O
an	O
intrinsic	O
connection	O
with	O
Laplace	O
-	O
Beltrami	O
operator	O
along	O
the	O
manifold	O
.	O

These	O
methods	O
have	O
shown	O
promising	O
results	O
for	O
classification	B-Task
tasks	E-Task
by	O
leveraging	O
deep	B-Method
generative	I-Method
models	E-Method
.	O

section	O
:	O
Loss	O
-	O
Sensitive	O
GAN	S-Method
The	O
classic	O
GAN	S-Method
consists	O
of	O
two	O
players	O
–	O
a	O
generator	S-Method
producing	O
samples	O
from	O
random	O
noises	O
,	O
and	O
a	O
discriminator	S-Method
distinguishing	O
real	O
and	O
fake	O
samples	O
.	O

The	O
generator	S-Method
and	O
discriminator	S-Method
are	O
trained	O
in	O
an	O
adversarial	B-Method
fashion	E-Method
to	O
reach	O
an	O
equilibrium	O
in	O
which	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
their	O
real	O
counterparts	O
.	O

On	O
the	O
contrary	O
,	O
in	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
we	O
seek	O
to	O
learn	O
a	O
loss	O
function	O
parameterized	O
with	O
by	O
assuming	O
that	O
a	O
real	O
example	O
ought	O
to	O
have	O
a	O
smaller	O
loss	O
than	O
a	O
generated	O
sample	O
by	O
a	O
desired	O
margin	O
.	O

Then	O
the	O
generator	O
can	O
be	O
trained	O
to	O
generate	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

Formally	O
,	O
consider	O
a	O
generator	B-Method
function	E-Method
that	O
produces	O
a	O
sample	O
by	O
transforming	O
a	O
noise	O
input	O
drawn	O
from	O
a	O
simple	O
distribution	S-Method
such	O
as	O
uniform	B-Method
and	I-Method
Gaussian	I-Method
distributions	E-Method
.	O

Then	O
for	O
a	O
real	O
example	O
and	O
a	O
generated	O
sample	O
,	O
the	O
loss	B-Method
function	E-Method
can	O
be	O
trained	O
to	O
distinguish	O
them	O
with	O
the	O
following	O
constraint	O
:	O
where	O
is	O
the	O
margin	O
measuring	O
the	O
difference	O
between	O
and	O
.	O

This	O
constraint	O
requires	O
a	O
real	O
sample	O
be	O
separated	O
from	O
a	O
generated	O
counterpart	O
in	O
terms	O
of	O
their	O
losses	O
by	O
at	O
least	O
a	O
margin	O
of	O
.	O

The	O
above	O
hard	O
constraint	O
can	O
be	O
relaxed	O
by	O
introducing	O
a	O
nonnegative	O
slack	O
variable	O
that	O
quantifies	O
the	O
violation	O
of	O
the	O
above	O
constraint	O
.	O

This	O
results	O
in	O
the	O
following	O
minimization	B-Task
problem	E-Task
to	O
learn	O
the	O
loss	O
function	O
given	O
a	O
fixed	O
generator	O
,	O
where	O
is	O
a	O
positive	O
balancing	O
parameter	O
,	O
and	O
is	O
the	O
data	O
distribution	O
of	O
real	O
samples	O
.	O

The	O
first	O
term	O
minimizes	O
the	O
expected	O
loss	O
function	O
over	O
data	O
distribution	O
since	O
a	O
smaller	O
loss	O
is	O
preferred	O
on	O
real	O
samples	O
.	O

The	O
second	O
term	O
is	O
the	O
expected	B-Metric
error	E-Metric
caused	O
by	O
the	O
violation	O
of	O
the	O
constraint	O
.	O

Without	O
loss	O
of	O
generality	O
,	O
we	O
require	O
the	O
loss	O
function	O
should	O
be	O
nonnegative	O
.	O

Given	O
a	O
fixed	O
loss	O
function	O
,	O
on	O
the	O
other	O
hand	O
,	O
one	O
can	O
solve	O
the	O
following	O
minimization	B-Task
problem	E-Task
to	O
find	O
an	O
optimal	O
generator	O
.	O

We	O
can	O
use	O
and	O
to	O
denote	O
the	O
density	O
of	O
samples	O
generated	O
by	O
and	O
respectively	O
,	O
with	O
being	O
drawn	O
from	O
.	O

However	O
,	O
for	O
the	O
simplicity	O
of	O
notations	O
,	O
we	O
will	O
use	O
and	O
to	O
denote	O
and	O
without	O
explicitly	O
mentioning	O
and	O
that	O
should	O
be	O
clear	O
in	O
the	O
context	O
.	O

Finally	O
,	O
let	O
us	O
summarize	O
the	O
above	O
objectives	O
.	O

The	O
LS	B-Method
-	I-Method
GAN	E-Method
optimizes	O
and	O
alternately	O
by	O
seeking	O
an	O
equilibrium	O
such	O
that	O
minimizes	O
which	O
is	O
an	O
equivalent	O
form	O
of	O
(	O
[	O
reference	O
]	O
)	O
with	O
,	O
and	O
minimizes	O
In	O
the	O
next	O
section	O
,	O
we	O
will	O
show	O
the	O
consistency	O
between	O
and	O
for	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

section	O
:	O
Theoretical	B-Task
Analysis	E-Task
:	O
Distributional	B-Task
Consistency	E-Task
Suppose	O
is	O
a	O
Nash	B-Method
equilibrium	E-Method
that	O
jointly	O
solves	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

We	O
will	O
show	O
that	O
as	O
,	O
the	O
density	O
distribution	O
of	O
the	O
samples	O
generated	O
by	O
will	O
converge	O
to	O
the	O
real	O
data	O
density	O
.	O

First	O
,	O
we	O
have	O
the	O
following	O
definition	O
.	O

Definition	O
.	O

For	O
any	O
two	O
samples	O
x	O
and	O
z	O
,	O
the	O
loss	O
function	O
⁢F	O
(	O
x	O
)	O
is	O
Lipschitz	O
continuous	O
with	O
respect	O
to	O
a	O
distance	O
metric	O
Δ	O
if	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
κ	O
,	O
i.e	O
,	O
<	O
κ	O
+	O
∞.	O
To	O
prove	O
our	O
main	O
result	O
,	O
we	O
assume	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

The	O
data	B-Method
density	I-Method
P⁢data	E-Method
is	O
supported	O
in	O
a	O
compact	O
set	O
D	O
,	O
and	O
it	O
is	O
Lipschitz	O
continuous	O
wrt	O
Δ	O
with	O
a	O
bounded	O
constant	O
<	O
κ	O
+	O
∞.	O
The	O
set	O
of	O
Lipschitz	O
densities	O
with	O
a	O
compact	O
support	O
contain	O
a	O
large	O
family	O
of	O
distributions	O
that	O
are	O
dense	O
in	O
the	O
space	O
of	O
continuous	O
densities	O
.	O

For	O
example	O
,	O
the	O
density	O
of	O
natural	O
images	O
are	O
defined	O
over	O
a	O
compact	O
set	O
of	O
pixel	O
values	O
,	O
and	O
it	O
can	O
be	O
consider	O
as	O
Lipschitz	O
continuous	O
,	O
since	O
the	O
densities	O
of	O
two	O
similar	O
images	O
are	O
unlikely	O
to	O
change	O
abruptly	O
at	O
an	O
unbounded	O
rate	O
.	O

If	O
real	O
samples	O
are	O
distributed	O
on	O
a	O
manifold	O
(	O
or	O
is	O
supported	O
in	O
a	O
manifold	O
)	O
,	O
we	O
only	O
require	O
the	O
Lipschitz	O
condition	O
hold	O
on	O
this	O
manifold	O
.	O

This	O
makes	O
the	O
Lipschitz	O
regularity	O
applicable	O
to	O
the	O
data	O
densities	O
on	O
a	O
thin	O
manifold	O
embedded	O
in	O
the	O
ambient	O
space	O
.	O

Let	O
us	O
show	O
the	O
existence	O
of	O
Nash	O
equilibrium	O
such	O
that	O
both	O
the	O
loss	O
function	O
and	O
the	O
density	O
of	O
generated	O
samples	O
are	O
Lipschitz	O
.	O

Let	O
be	O
the	O
class	O
of	O
functions	O
over	O
with	O
a	O
bounded	O
yet	O
sufficiently	O
large	O
Lipschitz	O
constant	O
such	O
that	O
belongs	O
to	O
.	O

It	O
is	O
not	O
difficult	O
to	O
show	O
that	O
the	O
space	O
is	O
convex	O
and	O
compact	O
if	O
its	O
member	O
functions	O
are	O
supported	O
in	O
a	O
compact	O
set	O
.	O

In	O
addition	O
,	O
we	O
note	O
both	O
and	O
are	O
convex	O
in	O
and	O
in	O
.	O

Then	O
,	O
according	O
to	O
the	O
Sion	O
’s	O
theorem	O
,	O
with	O
and	O
being	O
optimized	O
over	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
such	O
that	O
both	O
Lθ	B-Method
*	E-Method
and	O
PG	B-Method
*	E-Method
are	O
Lipschitz	O
.	O

Now	O
we	O
can	O
prove	O
the	O
main	O
lemma	O
of	O
this	O
paper	O
.	O

The	O
Lipschitz	O
regularity	O
relaxes	O
the	O
strong	O
non	O
-	O
parametric	O
assumption	O
on	O
the	O
GAN	S-Method
’s	O
discriminator	O
with	O
infinite	O
capacity	O
to	O
the	O
above	O
weaker	O
Lipschitz	O
assumption	O
for	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

This	O
allows	O
us	O
to	O
show	O
the	O
following	O
lemma	O
that	O
establishes	O
the	O
distributional	O
consistency	O
between	O
the	O
optimal	O
by	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
–	O
(	O
[	O
reference	O
]	O
)	O
and	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
for	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
in	O
Lemma	O
,	O
we	O
have	O
Thus	O
,	O
⁢PG*	O
(	O
x	O
)	O
converges	O
to	O
⁢P⁢data	O
(	O
x	O
)	O
as	O
→λ	O
+	O
∞.	O
The	O
proof	O
of	O
this	O
lemma	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

By	O
letting	O
λ	O
go	O
infinitely	O
large	O
,	O
the	O
density	O
⁢PG*	O
(	O
x	O
)	O
of	O
generated	O
samples	O
should	O
exactly	O
match	O
the	O
real	O
data	O
density	O
⁢P⁢data	O
(	O
x	O
)	O
.	O

Equivalently	O
,	O
we	O
can	O
simply	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
(	O
)	O
as	O
it	O
plays	O
no	O
role	O
as	O
→λ	O
+	O
∞.	O
Putting	O
the	O
above	O
two	O
lemmas	O
together	O
,	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	B-Method
equilibrium	E-Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
Lθ	O
*	O
and	O
PG	O
*	O
are	O
Lipschitz.	O
(	O
ii	O
)	O
∫x⁢|	O
-	O
⁢P⁢data	O
(	O
x	O
)	O
⁢PG*	O
(	O
x	O
)	O
|dx≤2λ→0	O
,	O
as	O
→λ	O
+	O
∞.	O
section	O
:	O
Learning	S-Task
and	O
Generalizability	O
The	O
minimization	B-Task
problems	E-Task
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
can	O
not	O
be	O
solved	O
directly	O
since	O
the	O
expectations	O
over	O
the	O
distributions	O
of	O
true	O
data	O
and	O
noises	O
are	O
unavailable	O
or	O
intractable	O
.	O

Instead	O
,	O
one	O
can	O
approximate	O
them	O
with	O
empirical	B-Method
means	E-Method
on	O
a	O
set	O
of	O
finite	O
real	O
examples	O
and	O
noise	O
vectors	O
drawn	O
from	O
and	O
respectively	O
.	O

This	O
results	O
in	O
the	O
following	O
two	O
alternative	O
problems	O
.	O

and	O
where	O
the	O
random	O
vectors	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
different	O
from	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
sample	O
mean	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
computed	O
over	O
pairs	O
randomly	O
drawn	O
from	O
real	O
and	O
generated	O
samples	O
,	O
which	O
is	O
an	O
approximation	O
to	O
the	O
second	O
expectation	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Generalizability	O
We	O
have	O
proved	O
the	O
density	O
of	O
generated	O
samples	O
by	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
is	O
consistent	O
with	O
the	O
real	O
data	O
density	O
in	O
Theorem	O
[	O
reference	O
]	O
.	O

This	O
consistency	O
is	O
established	O
based	O
on	O
the	O
two	O
oracle	B-Metric
objectives	E-Metric
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

However	O
,	O
in	O
practice	O
,	O
the	O
population	O
expectations	O
in	O
these	O
two	O
objectives	O
can	O
not	O
be	O
computed	O
directly	O
over	O
and	O
.	O

Instead	O
,	O
they	O
are	O
approximated	O
in	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
by	O
sample	B-Method
means	E-Method
on	O
a	O
finite	O
set	O
of	O
real	O
and	O
generated	O
examples	O
.	O

This	O
raises	O
the	O
question	O
about	O
the	O
generalizability	O
of	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
model	O
.	O

We	O
wonder	O
,	O
with	O
more	O
training	O
examples	O
,	O
if	O
the	O
empirical	B-Method
model	E-Method
trained	O
with	O
finitely	O
many	O
examples	O
can	O
generalize	O
to	O
the	O
oracle	B-Method
model	E-Method
.	O

In	O
particular	O
,	O
we	O
wish	O
to	O
estimate	O
the	O
sample	B-Metric
complexity	E-Metric
of	O
how	O
many	O
examples	O
are	O
required	O
to	O
sufficiently	O
bound	O
the	O
generalization	O
difference	O
between	O
the	O
empirical	O
and	O
oracle	O
objectives	O
.	O

Arora	O
et	O
al	O
.	O

has	O
proposed	O
a	O
neural	B-Method
network	I-Method
distance	E-Method
to	O
analyze	O
the	O
generalization	B-Metric
ability	E-Metric
for	O
the	O
GAN	S-Method
.	O

However	O
,	O
this	O
neural	B-Method
network	I-Method
distance	E-Method
can	O
not	O
be	O
directly	O
applied	O
here	O
,	O
as	O
it	O
is	O
not	O
related	O
with	O
the	O
objectives	O
that	O
are	O
used	O
to	O
train	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

So	O
the	O
generalization	B-Metric
ability	E-Metric
in	O
terms	O
of	O
the	O
neural	O
network	O
distance	O
does	O
not	O
imply	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
could	O
also	O
generalize	O
.	O

Thus	O
,	O
a	O
direct	O
generalization	B-Method
analysis	E-Method
of	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
is	O
required	O
based	O
on	O
its	O
own	O
objectives	O
.	O

First	O
,	O
let	O
us	O
consider	O
the	O
generalization	S-Task
in	O
terms	O
of	O
.	O

This	O
objective	O
is	O
used	O
to	O
train	O
the	O
loss	B-Method
function	E-Method
to	O
distinguish	O
between	O
real	O
and	O
generated	O
samples	O
.	O

Consider	O
the	O
oracle	B-Metric
objective	E-Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
population	O
expectations	O
and	O
the	O
empirical	B-Metric
objective	E-Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
sample	O
means	O
We	O
need	O
to	O
show	O
if	O
and	O
how	O
fast	O
the	O
difference	O
would	O
eventually	O
vanish	O
as	O
the	O
number	O
of	O
training	O
examples	O
grows	O
.	O

To	O
this	O
end	O
,	O
we	O
need	O
to	O
define	O
the	O
following	O
notations	O
about	O
the	O
model	B-Metric
complexity	E-Metric
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
for	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
the	O
loss	B-Method
function	E-Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
the	O
distance	O
between	O
two	O
samples	O
is	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
generalization	O
theorem	O
in	O
a	O
Probably	O
Approximately	O
Correct	O
(	O
PAC	S-Metric
)	O
style	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
N	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
loss	O
function	O
such	O
that	O
∈θRN	O
.	O

The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

This	O
theorem	O
shows	O
the	O
sample	B-Metric
complexity	E-Metric
to	O
bound	O
the	O
difference	O
between	O
and	O
is	O
polynomial	O
in	O
the	O
model	B-Metric
size	E-Metric
,	O
as	O
well	O
as	O
both	O
Lipschitz	O
constants	O
and	O
.	O

Similarly	O
,	O
we	O
can	O
establish	O
the	O
generalizability	O
to	O
train	O
the	O
generator	B-Method
function	E-Method
by	O
considering	O
the	O
empirical	B-Metric
objective	E-Metric
and	O
the	O
oracle	O
objective	O
over	O
empirical	O
and	O
real	O
distributions	O
,	O
respectively	O
.	O

We	O
use	O
the	O
following	O
notions	O
to	O
characterize	O
the	O
complexity	S-Metric
of	O
the	O
generator	S-Method
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
The	O
generator	B-Method
function	E-Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
Also	O
,	O
we	O
have	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
;	O
The	O
samples	O
’s	O
drawn	O
from	O
are	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
theorem	O
to	O
establish	O
the	O
generalizability	O
of	O
the	O
generator	O
in	O
terms	O
of	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C′	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
M	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
generator	B-Method
function	E-Method
such	O
that	O
∈ϕRM	O
.	O

subsection	O
:	O
Bounded	O
Lipschitz	O
Constants	O
for	O
Regularization	S-Task
Our	O
generalization	B-Method
theory	E-Method
in	O
Theorem	O
[	O
reference	O
]	O
conjectures	O
that	O
the	O
required	O
number	O
of	O
training	O
examples	O
is	O
lower	O
bounded	O
by	O
a	O
polynomial	O
of	O
Lipschitz	O
constants	O
and	O
of	O
the	O
loss	O
function	O
wrt	O
and	O
.	O

This	O
suggests	O
us	O
to	O
bound	O
both	O
constants	O
to	O
reduce	O
the	O
sample	B-Metric
complexity	E-Metric
of	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
to	O
improve	O
its	O
generalization	S-Task
performance	O
.	O

Specifically	O
,	O
bounding	O
the	O
Lipschitz	O
constants	O
and	O
can	O
be	O
implemented	O
by	O
adding	O
two	O
gradient	O
penalties	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
to	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
as	O
the	O
surrogate	O
of	O
the	O
Lipschitz	O
constants	O
.	O

For	O
simplicity	O
,	O
we	O
ignore	O
the	O
second	O
gradient	O
penalty	O
(	O
II	O
)	O
for	O
in	O
experiments	O
,	O
as	O
the	O
sample	B-Metric
complexity	E-Metric
is	O
only	O
log	O
-	O
linear	O
in	O
it	O
,	O
whose	O
impact	O
on	O
generalization	B-Metric
performance	E-Metric
is	O
negligible	O
compared	O
with	O
that	O
of	O
.	O

Otherwise	O
,	O
penalizing	O
(	O
II	O
)	O
needs	O
to	O
compute	O
its	O
gradient	O
wrt	O
,	O
which	O
is	O
with	O
a	O
Hessian	O
matrix	O
,	O
and	O
this	O
is	O
usually	O
computationally	O
demanding	O
.	O

Note	O
that	O
the	O
above	O
gradient	B-Method
penalty	E-Method
differs	O
from	O
that	O
used	O
in	O
that	O
aims	O
to	O
constrain	O
the	O
Lipschitz	O
constant	O
close	O
to	O
one	O
as	O
in	O
the	O
definition	O
of	O
the	O
Wasserstein	O
distance	O
.	O

However	O
,	O
we	O
are	O
motivated	O
to	O
have	O
lower	O
sample	B-Metric
complexity	E-Metric
by	O
directly	O
minimizing	O
the	O
Lipschitz	O
constant	O
rather	O
than	O
constraining	O
it	O
to	O
one	O
.	O

Two	O
gradient	B-Method
penalty	I-Method
approaches	E-Method
are	O
thus	O
derived	O
from	O
different	O
theoretical	O
perspectives	O
,	O
and	O
also	O
make	O
practical	O
differences	O
in	O
experiments	O
.	O

section	O
:	O
Wasserstein	O
GAN	S-Method
and	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
In	O
this	O
section	O
,	O
we	O
discuss	O
two	O
issues	O
about	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

First	O
,	O
we	O
discuss	O
its	O
connection	O
with	O
the	O
Wasserstein	O
GAN	S-Method
(	O
WGAN	S-Method
)	O
,	O
and	O
then	O
show	O
that	O
the	O
WGAN	S-Method
is	O
a	O
special	O
case	O
of	O
a	O
generalized	O
form	O
of	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

subsection	O
:	O
Comparison	O
with	O
Wasserstein	O
GAN	S-Method
We	O
notice	O
that	O
the	O
recently	O
proposed	O
Wasserstein	O
GAN	S-Method
(	O
WGAN	S-Method
)	O
uses	O
the	O
Earth	B-Method
-	I-Method
Mover	I-Method
(	I-Method
EM	I-Method
)	I-Method
distance	E-Method
to	O
address	O
the	O
vanishing	B-Task
gradient	E-Task
and	O
saturated	B-Task
JS	I-Task
distance	I-Task
problems	E-Task
in	O
the	O
classic	O
GAN	S-Method
by	O
showing	O
the	O
EM	B-Method
distance	E-Method
is	O
continuous	O
and	O
differentiable	O
almost	O
everywhere	O
.	O

While	O
both	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
the	O
WGAN	S-Method
address	O
these	O
problems	O
from	O
different	O
perspectives	O
that	O
are	O
independently	O
developed	O
almost	O
simultaneously	O
,	O
both	O
turn	O
out	O
to	O
use	O
the	O
Lipschitz	O
regularity	O
in	O
training	O
their	O
GAN	S-Method
models	O
.	O

This	O
constraint	O
plays	O
vital	O
but	O
different	O
roles	O
in	O
the	O
two	O
models	O
.	O

In	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
the	O
Lipschitz	O
regularity	O
naturally	O
arises	O
from	O
the	O
Lipschitz	O
assumption	O
on	O
the	O
data	O
density	O
and	O
the	O
generalization	O
bound	O
.	O

Under	O
this	O
regularity	O
condition	O
,	O
we	O
have	O
proved	O
in	O
Theorem	O
[	O
reference	O
]	O
that	O
the	O
density	O
of	O
generated	O
samples	O
matches	O
the	O
underlying	O
data	O
density	O
.	O

On	O
the	O
contrary	O
,	O
the	O
WGAN	S-Method
introduces	O
the	O
Lipschitz	O
constraint	O
from	O
the	O
Kantorovich	B-Method
-	I-Method
Rubinstein	I-Method
duality	E-Method
of	O
the	O
EM	B-Method
distance	E-Method
but	O
it	O
is	O
not	O
proved	O
in	O
if	O
the	O
density	O
of	O
samples	O
generated	O
by	O
WGAN	S-Method
is	O
consistent	O
with	O
that	O
of	O
real	O
data	O
.	O

Here	O
we	O
assert	O
that	O
the	O
WGAN	S-Method
also	O
models	O
an	O
underlying	O
Lipschitz	B-Method
density	E-Method
.	O

To	O
prove	O
this	O
,	O
we	O
restate	O
the	O
WGAN	S-Method
as	O
follows	O
.	O

The	O
WGAN	S-Method
seeks	O
to	O
find	O
a	O
critic	S-Method
and	O
a	O
generator	S-Method
such	O
that	O
and	O
Let	O
be	O
the	O
density	O
of	O
samples	O
generated	O
by	O
.	O

Then	O
,	O
we	O
prove	O
the	O
following	O
lemma	O
about	O
the	O
WGAN	S-Method
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
an	O
optimal	O
solution	O
(	O
fw*	O
,	O
gϕ	O
*	O
)	O
to	O
the	O
WGAN	S-Method
such	O
that	O
Pgϕ	O
*	O
is	O
Lipschitz	O
,	O
we	O
have	O
This	O
lemma	O
shows	O
both	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
the	O
WGAN	S-Method
are	O
based	O
on	O
the	O
same	O
Lipschitz	O
regularity	O
condition	O
.	O

Although	O
both	O
methods	O
are	O
derived	O
from	O
very	O
different	O
perspectives	O
,	O
it	O
is	O
interesting	O
to	O
make	O
a	O
comparison	O
between	O
their	O
respective	O
forms	O
.	O

Formally	O
,	O
the	O
WGAN	S-Method
seeks	O
to	O
maximize	O
the	O
difference	O
between	O
the	O
first	O
-	O
order	O
moments	O
of	O
under	O
the	O
densities	O
of	O
real	O
and	O
generated	O
examples	O
.	O

In	O
this	O
sense	O
,	O
the	O
WGAN	S-Method
can	O
be	O
considered	O
as	O
a	O
kind	O
of	O
first	B-Method
-	I-Method
order	I-Method
moment	I-Method
method	E-Method
.	O

Numerically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
tends	O
to	O
be	O
minimized	O
to	O
be	O
arbitrarily	O
small	O
over	O
generated	O
samples	O
,	O
which	O
could	O
make	O
be	O
unbounded	O
above	O
.	O

This	O
is	O
why	O
the	O
WGAN	S-Method
must	O
be	O
trained	O
by	O
clipping	O
the	O
network	O
weights	O
of	O
on	O
a	O
bounded	O
box	O
to	O
prevent	O
from	O
becoming	O
unbounded	O
above	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
treats	O
real	O
and	O
generated	O
examples	O
in	O
pairs	O
,	O
and	O
maximizes	O
the	O
difference	O
of	O
their	O
losses	O
up	O
to	O
a	O
data	O
-	O
dependant	O
margin	O
.	O

Specifically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
when	O
the	O
loss	O
of	O
a	O
generated	O
sample	O
becomes	O
too	O
large	O
wrt	O
that	O
of	O
a	O
paired	O
real	O
example	O
,	O
the	O
maximization	O
of	O
will	O
stop	O
if	O
the	O
difference	O
exceeds	O
.	O

This	O
prevents	O
the	O
minimization	B-Task
problem	E-Task
(	O
[	O
reference	O
]	O
)	O
unbounded	O
below	O
,	O
making	O
it	O
better	O
posed	O
to	O
solve	O
.	O

More	O
importantly	O
,	O
paring	O
real	O
and	O
generated	O
samples	O
in	O
prevents	O
their	O
losses	O
from	O
being	O
decomposed	O
into	O
two	O
separate	O
first	O
-	O
order	O
moments	O
like	O
in	O
the	O
WGAN	S-Method
.	O

The	O
LS	B-Method
-	I-Method
GAN	E-Method
makes	O
pairwise	O
comparison	O
between	O
the	O
losses	O
of	O
real	O
and	O
generated	O
samples	O
,	O
thereby	O
enforcing	O
real	O
and	O
generated	O
samples	O
to	O
coordinate	O
with	O
each	O
other	O
to	O
learn	O
the	O
optimal	O
loss	O
function	O
.	O

Specifically	O
,	O
when	O
a	O
generated	O
sample	O
becomes	O
close	O
to	O
a	O
paired	O
real	O
example	O
,	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
will	O
stop	O
increasing	O
the	O
difference	O
between	O
their	O
losses	O
.	O

Below	O
we	O
discuss	O
a	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
GLS	B-Method
-	I-Method
GAN	E-Method
)	O
model	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
show	O
that	O
both	O
WGAN	S-Method
and	O
LS	B-Method
-	I-Method
GAN	E-Method
are	O
simply	O
two	O
special	O
cases	O
of	O
this	O
GLS	B-Method
-	I-Method
GAN	E-Method
.	O

subsection	O
:	O
GLS	B-Method
-	I-Method
GAN	E-Method
:	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
In	O
proving	O
Lemma	O
[	O
reference	O
]	O
,	O
it	O
is	O
noted	O
that	O
we	O
only	O
have	O
used	O
two	O
properties	O
of	O
in	O
the	O
objective	B-Metric
function	E-Metric
training	O
the	O
loss	O
function	O
:	O
1	O
)	O
for	O
any	O
;	O
2	O
)	O
for	O
.	O

This	O
inspires	O
us	O
to	O
generalize	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
with	O
any	O
alternative	O
cost	B-Method
function	E-Method
satisfying	O
these	O
two	O
properties	O
,	O
and	O
this	O
will	O
yield	O
the	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
GLS	B-Method
-	I-Method
GAN	E-Method
)	O
.	O

We	O
will	O
show	O
that	O
both	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
WGAN	S-Method
can	O
be	O
seen	O
as	O
two	O
extreme	O
cases	O
of	O
this	O
GLS	B-Method
-	I-Method
GAN	E-Method
with	O
two	O
properly	O
defined	O
cost	O
functions	O
.	O

Formally	O
,	O
if	O
a	O
cost	O
function	O
satisfies	O
for	O
any	O
and	O
for	O
any	O
,	O
given	O
a	O
fixed	O
generator	O
,	O
we	O
use	O
the	O
following	O
objective	O
to	O
learn	O
,	O
with	O
highlighting	O
its	O
dependency	O
on	O
a	O
chosen	O
cost	O
function	O
.	O

For	O
simplicity	O
,	O
we	O
only	O
involve	O
the	O
second	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
define	O
the	O
generalized	O
objective	O
.	O

But	O
it	O
does	O
not	O
affect	O
the	O
conclusion	O
as	O
the	O
role	O
of	O
the	O
first	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
would	O
vanish	O
with	O
being	O
set	O
to	O
.	O

Following	O
the	O
proof	O
of	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
can	O
prove	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
a	O
Nash	B-Method
equilibrium	E-Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
jointly	O
minimizing	O
⁢SC	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
and	O
⁢T	O
(	O
θ*	O
,	O
ϕ	O
)	O
with	O
a	O
cost	O
function	O
C	O
satisfying	O
the	O
above	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
,	O
we	O
have	O
In	O
particular	O
,	O
we	O
can	O
choose	O
a	O
leaky	B-Method
rectified	I-Method
linear	I-Method
function	E-Method
for	O
this	O
cost	B-Method
function	E-Method
,	O
i.e.	O
,	O
with	O
a	O
slope	O
.	O

As	O
long	O
as	O
,	O
it	O
is	O
easy	O
to	O
verify	O
satisfies	O
these	O
two	O
conditions	O
.	O

Now	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
is	O
a	O
special	O
case	O
of	O
this	O
Generalized	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
GLS	O
-	O
GAN	S-Method
)	O
when	O
,	O
as	O
.	O

We	O
denote	O
this	O
equivalence	O
as	O
LS	B-Method
-	I-Method
GAN	E-Method
=	O
GLS	O
-	O
GAN	S-Method
(	O
C0	O
)	O
What	O
is	O
more	O
interesting	O
is	O
the	O
WGAN	S-Method
,	O
an	O
independently	O
developed	O
GAN	S-Method
model	O
with	O
stable	O
training	O
performance	O
,	O
also	O
becomes	O
a	O
special	O
case	O
of	O
this	O
GLS	B-Method
-	I-Method
GAN	E-Method
with	O
.	O

Indeed	O
,	O
when	O
,	O
,	O
and	O
Since	O
the	O
last	O
term	O
is	O
a	O
const	O
,	O
irrespective	O
of	O
,	O
it	O
can	O
be	O
discarded	O
without	O
affecting	O
optimization	O
over	O
.	O

Thus	O
,	O
we	O
have	O
By	O
comparing	O
this	O
with	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
WGAN	S-Method
is	O
equivalent	O
to	O
the	O
GLS	B-Method
-	I-Method
GAN	E-Method
with	O
,	O
with	O
the	O
critic	B-Method
function	E-Method
being	O
equivalent	O
to	O
.	O

Thus	O
we	O
have	O
WGAN	S-Method
=	O
GLS	O
-	O
GAN	S-Method
(	O
C1	O
)	O
Therefore	O
,	O
by	O
varying	O
the	O
slope	O
in	O
,	O
we	O
will	O
obtain	O
a	O
family	O
of	O
the	O
GLS	O
-	O
GANs	S-Method
with	O
varied	O
beyond	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
the	O
WGAN	S-Method
.	O

Of	O
course	O
,	O
it	O
is	O
unnecessary	O
to	O
limit	O
to	O
a	O
leaky	B-Method
rectified	I-Method
linear	I-Method
function	E-Method
.	O

We	O
can	O
explore	O
more	O
cost	O
functions	O
as	O
long	O
as	O
they	O
satisfy	O
the	O
two	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
.	O

In	O
experiments	O
,	O
we	O
will	O
demonstrate	O
the	O
GLS	B-Method
-	I-Method
GAN	E-Method
has	O
competitive	O
generalization	S-Task
performance	O
on	O
generating	O
new	O
images	O
(	O
c.f	O
.	O

Section	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Non	B-Method
-	I-Method
Parametric	I-Method
Analysis	E-Method
Now	O
we	O
can	O
characterize	O
the	O
optimal	O
loss	O
functions	O
learned	O
from	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
will	O
provide	O
us	O
an	O
insight	O
into	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
model	O
.	O

We	O
generalize	O
the	O
non	B-Method
-	I-Method
parametric	I-Method
maximum	I-Method
likelihood	I-Method
method	E-Method
in	O
and	O
consider	O
non	B-Method
-	I-Method
parametric	I-Method
solutions	E-Method
to	O
the	O
optimal	O
loss	O
function	O
by	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
over	O
the	O
whole	O
class	O
of	O
Lipschitz	O
loss	O
functions	O
.	O

Let	O
,	O
i.e.	O
,	O
the	O
first	O
data	O
points	O
are	O
real	O
examples	O
and	O
the	O
rest	O
are	O
generated	O
samples	O
.	O

Then	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

The	O
following	O
functions	O
^Lθ	O
*	O
and	O
~Lθ	O
*	O
both	O
minimize	O
⁢Sm	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
in	O
Fκ	S-Method
:	O
with	O
the	O
parameters	O
θ*=	O
[	O
l1*	O
,	O
⋯	O
,	O
l⁢2m*	O
]	O
∈R⁢2	O
m	O
.	O

They	O
are	O
supported	O
in	O
the	O
convex	O
hull	O
of	O
{	O
x	O
(	O
1	O
),	O
⋯	O
,	O
x	O
(	O
⁢2	O
m	O
)	O
}	O
,	O
and	O
we	O
have	O
for	O
=	O
i1	O
,	O
⋯	O
,	O
⁢2	O
m	O
,	O
i.e.	O
,	O
their	O
values	O
coincide	O
on	O
{	O
x	O
(	O
1	O
),	O
x	O
(	O
2	O
),	O
⋯	O
,	O
x	O
(	O
⁢2m	O
)	O
}.	O
The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
the	O
appendix	O
.	O

From	O
the	O
theorem	O
,	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
any	O
convex	O
combination	O
of	O
these	O
two	O
forms	O
attains	O
the	O
same	O
value	O
of	O
,	O
and	O
is	O
also	O
a	O
global	O
minimizer	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

All	O
the	O
functions	O
in	O
minimize	O
Sm	O
in	O
Fκ	O
.	O

This	O
shows	O
that	O
the	O
global	O
minimizer	O
is	O
not	O
unique	O
.	O

Moreover	O
,	O
through	O
the	O
proof	O
of	O
Theorem	O
[	O
reference	O
]	O
,	O
one	O
can	O
find	O
that	O
and	O
are	O
the	O
upper	O
and	O
lower	O
bound	O
of	O
any	O
optimal	B-Method
loss	I-Method
function	I-Method
solution	E-Method
to	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
.	O

In	O
particular	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

For	O
any	O
∈⁢Lθ*	O
(	O
x	O
)	O
Fκ	O
that	O
minimizes	O
Sm	O
,	O
the	O
corresponding	O
⁢^Lθ*	O
(	O
x	O
)	O
and	O
⁢~Lθ*	O
(	O
x	O
)	O
are	O
the	O
lower	O
and	O
upper	O
bounds	O
of	O
⁢Lθ*	O
(	O
x	O
)	O
,	O
i.e.	O
,	O
The	O
proof	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

The	O
parameters	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
sought	O
by	O
minimizing	O
where	O
is	O
short	O
for	O
,	O
and	O
the	O
constraints	O
are	O
imposed	O
to	O
ensure	O
the	O
learned	O
loss	O
functions	O
stay	O
in	O
.	O

With	O
a	O
greater	O
value	O
of	O
,	O
a	O
larger	O
class	O
of	O
loss	B-Metric
function	E-Metric
will	O
be	O
sought	O
.	O

Thus	O
,	O
one	O
can	O
control	O
the	O
modeling	O
ability	O
of	O
the	O
loss	O
function	O
by	O
setting	O
a	O
proper	O
value	O
to	O
.	O

Problem	O
(	O
[	O
reference	O
]	O
)	O
is	O
a	O
typical	O
linear	B-Task
programming	I-Task
problem	E-Task
.	O

In	O
principle	O
,	O
one	O
can	O
solve	O
this	O
problem	O
to	O
obtain	O
a	O
non	B-Method
-	I-Method
parametric	I-Method
loss	I-Method
function	E-Method
for	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

Unfortunately	O
,	O
it	O
consists	O
of	O
a	O
large	O
number	O
of	O
constraints	O
,	O
whose	O
scale	O
is	O
at	O
an	O
order	O
of	O
.	O

This	O
prevents	O
us	O
from	O
using	O
(	O
[	O
reference	O
]	O
)	O
directly	O
to	O
solve	O
an	O
optimal	O
non	B-Method
-	I-Method
parametric	I-Method
LS	I-Method
-	I-Method
GAN	I-Method
model	E-Method
with	O
a	O
very	O
large	O
number	O
of	O
training	O
examples	O
.	O

On	O
the	O
contrary	O
,	O
a	O
more	O
tractable	O
solution	O
is	O
to	O
use	O
a	O
parameterized	B-Method
network	E-Method
to	O
solve	O
the	O
optimization	B-Task
problem	E-Task
(	O
[	O
reference	O
]	O
)	O
constrained	O
in	O
,	O
and	O
iteratively	O
update	O
parameterized	B-Method
and	E-Method
with	O
the	O
gradient	B-Method
descent	I-Method
method	E-Method
.	O

Although	O
the	O
non	B-Method
-	I-Method
parametric	I-Method
solution	E-Method
can	O
not	O
be	O
solved	O
directly	O
,	O
it	O
is	O
valuable	O
in	O
shedding	O
some	O
light	O
on	O
what	O
kind	O
of	O
the	O
loss	O
function	O
would	O
be	O
learned	O
by	O
a	O
deep	B-Method
network	E-Method
.	O

It	O
is	O
well	O
known	O
that	O
the	O
training	S-Task
of	O
the	O
classic	O
GAN	S-Method
generator	O
suffers	O
from	O
vanishing	B-Task
gradient	E-Task
problem	O
as	O
the	O
discriminator	S-Method
can	O
be	O
optimized	O
very	O
quickly	O
.	O

Recent	O
study	O
has	O
revealed	O
that	O
this	O
is	O
caused	O
by	O
using	O
the	O
Jensen	B-Method
-	I-Method
Shannon	I-Method
(	I-Method
JS	I-Method
)	I-Method
distance	E-Method
that	O
becomes	O
locally	O
saturated	O
and	O
gets	O
vanishing	B-Task
gradient	E-Task
to	O
train	O
the	O
GAN	S-Method
generator	O
if	O
the	O
discriminator	S-Method
is	O
over	O
-	O
trained	O
.	O

Similar	O
problem	O
has	O
also	O
been	O
found	O
in	O
the	O
energy	O
-	O
based	O
GAN	S-Method
(	O
EBGAN	O
)	O
as	O
it	O
minimizes	O
the	O
total	O
variation	O
that	O
is	O
not	O
continuous	O
or	O
(	O
sub	O
-)	O
differentiable	O
if	O
the	O
corresponding	O
discriminator	S-Method
is	O
fully	O
optimized	O
.	O

On	O
the	O
contrary	O
,	O
as	O
revealed	O
in	O
Theorem	O
[	O
reference	O
]	O
and	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	O
loss	O
function	O
of	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
are	O
cone	O
-	O
shaped	O
(	O
in	O
terms	O
of	O
that	O
defines	O
the	O
Lipschitz	O
continuity	O
)	O
,	O
and	O
have	O
non	O
-	O
vanishing	B-Task
gradient	E-Task
almost	O
everywhere	O
.	O

Moreover	O
,	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
only	O
contains	O
linear	O
objective	O
and	O
constraints	O
;	O
this	O
is	O
contrary	O
to	O
the	O
classic	O
GAN	S-Method
that	O
involves	O
logistic	O
loss	O
terms	O
that	O
are	O
prone	O
to	O
saturation	O
with	O
vanishing	B-Task
gradient	E-Task
.	O

Thus	O
,	O
an	O
optimal	B-Method
loss	I-Method
function	E-Method
that	O
is	O
properly	O
sought	O
in	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
is	O
unlikely	O
to	O
saturate	O
between	O
these	O
two	O
bounds	O
,	O
and	O
it	O
should	O
be	O
able	O
to	O
provide	O
sufficient	O
gradient	O
to	O
update	O
the	O
generator	O
by	O
descending	O
(	O
[	O
reference	O
]	O
)	O
even	O
if	O
it	O
has	O
been	O
trained	O
till	O
optimality	O
.	O

Our	O
experiment	O
also	O
shows	O
that	O
,	O
even	O
if	O
the	O
loss	O
function	O
is	O
quickly	O
trained	O
to	O
optimality	O
,	O
it	O
can	O
still	O
provide	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	S-Method
in	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Conditional	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
The	O
LS	B-Method
-	I-Method
GAN	E-Method
can	O
easily	O
be	O
generalized	O
to	O
produce	O
a	O
sample	O
based	O
on	O
a	O
given	O
condition	O
,	O
yielding	O
a	O
new	O
paradigm	O
of	O
Conditional	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
CLS	B-Method
-	I-Method
GAN	E-Method
)	O
.	O

For	O
example	O
,	O
if	O
the	O
condition	O
is	O
an	O
image	O
class	O
,	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
seeks	O
to	O
produce	O
images	O
of	O
the	O
given	O
class	O
;	O
otherwise	O
,	O
if	O
a	O
text	O
description	O
is	O
given	O
as	O
a	O
condition	O
,	O
the	O
model	O
attempts	O
to	O
generate	O
images	O
aligned	O
with	O
the	O
given	O
description	O
.	O

This	O
gives	O
us	O
more	O
flexibility	O
in	O
controlling	O
what	O
samples	O
to	O
be	O
generated	O
.	O

Formally	O
,	O
the	O
generator	O
of	O
CLS	O
-	O
GAN	S-Method
takes	O
a	O
condition	O
vector	O
as	O
input	O
along	O
with	O
a	O
noise	O
vector	O
to	O
produce	O
a	O
sample	O
.	O

To	O
train	O
the	O
model	O
,	O
we	O
define	O
a	O
loss	B-Metric
function	E-Metric
to	O
measure	O
the	O
degree	O
of	O
the	O
misalignment	O
between	O
a	O
data	O
sample	O
and	O
a	O
given	O
condition	O
.	O

For	O
a	O
real	O
example	O
aligned	O
with	O
the	O
condition	O
,	O
its	O
loss	B-Metric
function	E-Metric
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
sample	O
by	O
a	O
margin	O
of	O
.	O

This	O
results	O
in	O
the	O
following	O
constraint	O
,	O
Like	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
this	O
type	O
of	O
constraint	O
yields	O
the	O
following	O
non	B-Method
-	I-Method
zero	I-Method
-	I-Method
sum	I-Method
game	E-Method
to	O
train	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
,	O
which	O
seeks	O
a	O
Nash	O
equilibrium	O
so	O
that	O
minimizes	O
and	O
minimizes	O
where	O
denotes	O
either	O
the	O
joint	O
data	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
or	O
its	O
marginal	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

Playing	O
the	O
above	O
game	O
will	O
lead	O
to	O
a	O
trained	O
pair	O
of	O
loss	O
function	O
and	O
generator	S-Method
.	O

We	O
can	O
show	O
that	O
the	O
learned	O
generator	S-Method
can	O
produce	O
samples	O
whose	O
distribution	O
follows	O
the	O
true	O
data	O
density	O
for	O
a	O
given	O
condition	O
.	O

To	O
prove	O
this	O
,	O
we	O
say	O
a	O
loss	O
function	O
is	O
Lipschitz	O
if	O
it	O
is	O
Lipschitz	O
continuous	O
in	O
its	O
first	O
argument	O
.	O

We	O
also	O
impose	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
conditional	O
density	O
.	O

theorem	O
:	O
.	O

For	O
each	O
y	O
,	O
the	O
conditional	O
density	O
P⁢data	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
,	O
and	O
is	O
supported	O
in	O
a	O
convex	O
compact	O
set	O
of	O
x.	O
Then	O
it	O
is	O
not	O
difficult	O
to	O
prove	O
the	O
following	O
theorem	O
,	O
which	O
shows	O
that	O
the	O
conditional	O
density	O
becomes	O
as	O
.	O

Here	O
denotes	O
the	O
density	O
of	O
samples	O
generated	O
by	O
with	O
sampled	O
random	O
noise	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	B-Method
equilibrium	E-Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
⁢Lθ*	O
(	O
x	O
,	O
y	O
)	O
is	O
Lipschitz	O
continuous	O
in	O
x	O
for	O
each	O
y;	O
(	O
ii	O
)	O
PG*	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
continuous;	O
(	O
iii	O
)	O
∫x|P⁢data	O
(	O
x|y	O
)-	O
PG*	O
(	O
x|y	O
)	O
|dx≤2λ	O
.	O

In	O
addition	O
,	O
similar	O
upper	O
and	O
lower	O
bounds	O
can	O
be	O
derived	O
to	O
characterize	O
the	O
learned	O
conditional	O
loss	O
function	O
following	O
the	O
same	O
idea	O
for	O
LS	B-Method
-	I-Method
GAN	E-Method
.	O

A	O
useful	O
byproduct	O
of	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
is	O
one	O
can	O
use	O
the	O
learned	O
loss	O
function	O
to	O
predict	O
the	O
label	O
of	O
an	O
example	O
by	O
The	O
advantage	O
of	O
such	O
a	O
CLS	B-Method
-	I-Method
GAN	I-Method
classifier	E-Method
is	O
it	O
is	O
trained	O
with	O
both	O
labeled	O
and	O
generated	O
examples	O
,	O
the	O
latter	O
of	O
which	O
can	O
improve	O
the	O
training	O
of	O
the	O
classifier	S-Method
by	O
revealing	O
more	O
potential	O
variations	O
within	O
different	O
classes	O
of	O
samples	O
.	O

It	O
also	O
provides	O
a	O
way	O
to	O
evaluate	O
the	O
model	O
based	O
on	O
its	O
classification	B-Metric
performance	E-Metric
.	O

This	O
is	O
an	O
objective	O
metric	O
we	O
can	O
use	O
to	O
assess	O
the	O
quality	O
of	O
feature	B-Method
representations	E-Method
learned	O
by	O
the	O
model	O
.	O

For	O
a	O
classification	B-Task
task	E-Task
,	O
a	O
suitable	O
value	O
should	O
be	O
set	O
to	O
.	O

Although	O
Theorem	O
[	O
reference	O
]	O
shows	O
would	O
converge	O
to	O
the	O
true	O
conditional	O
density	O
by	O
increasing	O
,	O
it	O
only	O
ensures	O
it	O
is	O
a	O
good	O
generative	O
rather	O
than	O
classification	B-Method
model	E-Method
.	O

However	O
,	O
a	O
too	O
large	O
value	O
of	O
tends	O
to	O
ignore	O
the	O
first	B-Method
loss	I-Method
minimization	I-Method
term	E-Method
of	O
(	O
[	O
reference	O
]	O
)	O
that	O
plays	O
an	O
important	O
role	O
in	O
minimizing	B-Task
classification	I-Task
error	E-Task
.	O

Thus	O
,	O
a	O
trade	O
-	O
off	O
should	O
be	O
made	O
to	O
balance	O
between	O
classification	B-Task
and	I-Task
generation	I-Task
objectives	E-Task
.	O

subsection	O
:	O
Semi	O
-	O
Supervised	O
LS	B-Method
-	I-Method
GAN	E-Method
The	O
above	O
CLS	B-Method
-	I-Method
GAN	E-Method
can	O
be	O
considered	O
as	O
a	O
fully	B-Method
supervised	I-Method
model	E-Method
to	O
classify	O
examples	O
into	O
different	O
classes	O
.	O

It	O
can	O
also	O
be	O
extended	O
to	O
a	O
Semi	B-Method
-	I-Method
Supervised	I-Method
model	E-Method
by	O
incorporating	O
unlabeled	O
examples	O
.	O

Suppose	O
we	O
have	O
classes	O
indexed	O
by	O
.	O

In	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
,	O
for	O
each	O
class	O
,	O
we	O
choose	O
a	O
loss	O
function	O
that	O
,	O
for	O
example	O
,	O
can	O
be	O
defined	O
as	O
the	O
negative	O
log	O
-	O
softmax	O
,	O
where	O
is	O
the	O
th	O
activation	O
output	O
from	O
a	O
network	B-Method
layer	E-Method
.	O

Suppose	O
we	O
also	O
have	O
unlabeled	O
examples	O
available	O
,	O
and	O
we	O
can	O
define	O
a	O
new	O
loss	O
function	O
for	O
these	O
unlabeled	O
examples	O
so	O
that	O
they	O
can	O
be	O
involved	O
in	O
training	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
.	O

Consider	O
an	O
unlabeled	O
example	O
,	O
its	O
groundtruth	O
label	O
is	O
unknown	O
.	O

However	O
,	O
the	O
best	O
guess	O
of	O
its	O
label	O
can	O
be	O
made	O
by	O
choosing	O
the	O
one	O
that	O
minimizes	O
over	O
,	O
and	O
this	O
inspires	O
us	O
to	O
define	O
the	O
following	O
loss	O
function	O
for	O
the	O
unlabeled	O
example	O
as	O
Here	O
we	O
modify	O
to	O
so	O
can	O
be	O
viewed	O
as	O
the	O
probability	O
that	O
does	O
not	O
belong	O
to	O
any	O
known	O
label	O
.	O

Then	O
we	O
have	O
the	O
following	O
loss	B-Task
-	I-Task
sensitive	I-Task
objective	E-Task
that	O
explores	O
unlabeled	O
examples	O
to	O
train	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
,	O
This	O
objective	O
is	O
combined	O
with	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
train	O
the	O
loss	B-Method
function	I-Method
network	E-Method
by	O
minimizing	O
where	O
is	O
a	O
positive	O
hyperparameter	O
balancing	O
the	O
contributions	O
from	O
labeled	O
and	O
labeled	O
examples	O
.	O

The	O
idea	O
of	O
extending	O
the	O
GAN	S-Method
for	O
semi	B-Task
-	I-Task
supervised	I-Task
learning	E-Task
has	O
been	O
proposed	O
by	O
Odena	O
and	O
Salimans	O
et	O
al	O
.	O

,	O
where	O
generated	O
samples	O
are	O
assigned	O
to	O
an	O
artificial	O
class	O
,	O
and	O
unlabeled	O
examples	O
are	O
treated	O
as	O
the	O
negative	O
examples	O
.	O

Our	O
proposed	O
semi	B-Method
-	I-Method
supervised	I-Method
learning	E-Method
differs	O
in	O
creating	O
a	O
new	O
loss	O
function	O
for	O
unlabeled	O
examples	O
from	O
the	O
losses	O
for	O
existing	O
classes	O
,	O
by	O
minimizing	O
which	O
we	O
make	O
the	O
best	O
guess	O
of	O
the	O
classes	O
of	O
unlabeled	O
examples	O
.	O

The	O
guessed	O
labeled	O
will	O
provide	O
additional	O
information	O
to	O
train	O
the	O
CLS	O
-	O
GAN	S-Method
model	O
,	O
and	O
the	O
updated	O
model	O
will	O
in	O
turn	O
improve	O
the	O
guess	O
over	O
the	O
training	O
course	O
.	O

The	O
experiments	O
in	O
the	O
following	O
section	O
will	O
show	O
that	O
this	O
approach	O
can	O
generate	O
very	O
competitive	O
performance	O
especially	O
when	O
the	O
labeled	O
data	O
is	O
very	O
limited	O
.	O

section	O
:	O
Experiments	O
Objective	B-Task
evaluation	E-Task
of	O
a	O
data	B-Method
generative	I-Method
model	E-Method
is	O
not	O
an	O
easy	O
task	O
as	O
there	O
is	O
no	O
consensus	O
criteria	O
to	O
quantify	O
the	O
quality	O
of	O
generated	O
samples	O
.	O

For	O
this	O
reason	O
,	O
we	O
will	O
make	O
a	O
qualitative	B-Task
analysis	I-Task
of	I-Task
generated	I-Task
images	E-Task
,	O
and	O
use	O
image	B-Task
classification	E-Task
to	O
quantitatively	O
evaluate	O
the	O
resultant	O
LS	B-Method
-	I-Method
GAN	E-Method
model	O
.	O

First	O
,	O
we	O
will	O
assess	O
the	O
quality	O
of	O
generated	B-Metric
images	E-Metric
by	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
in	O
comparison	O
with	O
the	O
classic	O
GAN	S-Method
model	O
.	O

Then	O
,	O
we	O
will	O
make	O
an	O
objective	O
evaluation	O
on	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
to	O
classify	B-Task
images	E-Task
.	O

This	O
task	O
evaluates	O
the	O
quality	O
of	O
feature	B-Method
representations	E-Method
learned	O
by	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
in	O
terms	O
of	O
its	O
classification	B-Metric
accuracy	E-Metric
directly	O
.	O

Finally	O
,	O
we	O
will	O
assess	O
the	O
generalizability	O
of	O
various	O
GAN	S-Method
models	O
in	O
generating	O
new	O
images	O
out	O
of	O
training	O
examples	O
by	O
proposing	O
the	O
Minimum	B-Metric
Reconstruction	I-Metric
Error	E-Metric
(	O
MRE	S-Metric
)	O
on	O
a	O
separate	O
test	O
set	O
.	O

subsection	O
:	O
Architectures	O
We	O
adopted	O
the	O
ideas	O
behind	O
the	O
network	B-Method
architecture	E-Method
for	O
the	O
DCGAN	S-Method
to	O
build	O
the	O
generator	B-Method
and	I-Method
the	I-Method
loss	I-Method
function	I-Method
networks	E-Method
.	O

Compared	O
with	O
the	O
conventional	O
CNNs	S-Method
,	O
maxpooling	B-Method
layers	E-Method
were	O
replaced	O
with	O
strided	B-Method
convolutions	E-Method
in	O
both	O
networks	O
,	O
and	O
fractionally	B-Method
-	I-Method
strided	I-Method
convolutions	E-Method
were	O
used	O
in	O
the	O
generator	B-Method
network	E-Method
to	O
upsample	O
feature	O
maps	O
across	O
layers	O
to	O
finer	O
resolutions	O
.	O

Batch	B-Method
-	I-Method
normalization	I-Method
layers	E-Method
were	O
added	O
in	O
both	O
networks	O
between	O
convolutional	B-Method
layers	E-Method
,	O
and	O
fully	B-Method
connected	I-Method
layers	E-Method
were	O
removed	O
from	O
these	O
networks	O
.	O

However	O
,	O
unlike	O
the	O
DCGAN	S-Method
,	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
model	O
(	O
unconditional	O
version	O
in	O
Section	O
[	O
reference	O
]	O
)	O
did	O
not	O
use	O
a	O
sigmoid	O
layer	O
as	O
the	O
output	O
for	O
the	O
loss	B-Method
function	I-Method
network	E-Method
.	O

Instead	O
,	O
we	O
removed	O
it	O
and	O
directly	O
output	O
the	O
activation	O
before	O
the	O
removed	O
sigmoid	B-Method
layer	E-Method
.	O

On	O
the	O
other	O
hand	O
,	O
for	O
the	O
loss	B-Method
function	I-Method
network	E-Method
in	O
CLS	B-Method
-	I-Method
GAN	E-Method
,	O
a	O
global	B-Method
mean	I-Method
-	I-Method
pooling	I-Method
layer	E-Method
was	O
added	O
on	O
top	O
of	O
convolutional	B-Method
layers	E-Method
.	O

This	O
produced	O
a	O
feature	O
map	O
that	O
output	O
the	O
conditional	O
loss	O
on	O
different	O
classes	O
.	O

In	O
the	O
generator	B-Method
network	E-Method
,	O
Tanh	S-Method
was	O
used	O
to	O
produce	O
images	O
whose	O
pixel	O
values	O
are	O
scaled	O
to	O
.	O

Thus	O
,	O
all	O
image	O
examples	O
in	O
datasets	O
were	O
preprocessed	O
to	O
have	O
their	O
pixel	O
values	O
in	O
.	O

More	O
details	O
about	O
the	O
design	O
of	O
network	B-Method
architectures	E-Method
can	O
be	O
found	O
in	O
literature	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
network	B-Method
architecture	E-Method
for	O
the	O
CLS	O
-	O
GAN	S-Method
model	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
SVHN	B-Material
datasets	E-Material
in	O
the	O
experiments	O
.	O

In	O
particular	O
,	O
the	O
architecture	O
of	O
the	O
loss	B-Method
function	I-Method
network	E-Method
was	O
adapted	O
from	O
that	O
used	O
in	O
with	O
nine	O
hidden	O
layers	O
.	O

subsection	O
:	O
Training	O
Details	O
The	O
models	O
were	O
trained	O
in	O
a	O
mini	O
-	O
batch	O
of	O
images	O
,	O
and	O
their	O
weights	O
were	O
initialized	O
from	O
a	O
zero	B-Method
-	I-Method
mean	I-Method
Gaussian	I-Method
distribution	E-Method
with	O
a	O
standard	O
deviation	O
of	O
.	O

The	O
Adam	B-Method
optimizer	E-Method
was	O
used	O
to	O
train	O
the	O
network	O
with	O
initial	O
learning	B-Metric
rate	E-Metric
and	O
being	O
set	O
to	O
and	O
respectively	O
,	O
while	O
the	O
learning	B-Metric
rate	E-Metric
was	O
annealed	O
every	O
epochs	O
by	O
a	O
factor	O
of	O
.	O

The	O
other	O
hyperparameters	O
such	O
as	O
and	O
were	O
chosen	O
based	O
on	O
an	O
independent	O
validation	O
set	O
held	O
out	O
from	O
training	O
examples	O
.	O

We	O
also	O
tested	O
various	O
forms	O
of	O
loss	O
margins	O
between	O
real	O
and	O
fake	O
samples	O
.	O

For	O
example	O
,	O
we	O
tried	O
the	O
distance	O
between	O
image	O
representations	O
as	O
the	O
margin	O
,	O
and	O
found	O
the	O
best	O
result	O
can	O
be	O
achieved	O
when	O
.	O

The	O
distance	O
between	O
convolutional	O
features	O
was	O
supposed	O
to	O
capture	O
perceptual	O
dissimilarity	O
between	O
images	O
.	O

But	O
we	O
should	O
avoid	O
a	O
direct	O
use	O
of	O
the	O
convolutional	O
features	O
from	O
the	O
loss	B-Method
function	I-Method
network	E-Method
,	O
since	O
we	O
found	O
they	O
would	O
tend	O
to	O
collapse	O
to	O
a	O
trivial	O
point	O
as	O
the	O
loss	O
margin	O
vanishes	O
.	O

The	O
feature	O
maps	O
from	O
a	O
separate	O
pretrained	B-Method
deep	I-Method
network	E-Method
,	O
such	O
as	O
Inception	B-Method
and	I-Method
VGG	I-Method
-	I-Method
16	I-Method
networks	E-Method
,	O
could	O
be	O
a	O
better	O
choice	O
to	O
define	O
the	O
loss	O
margin	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
images	O
generated	O
by	O
LS	B-Method
-	I-Method
GAN	E-Method
on	O
CelebA	O
with	O
the	O
inception	O
and	O
VGG	O
-	O
16	O
margins	O
.	O

However	O
,	O
for	O
a	O
fair	O
comparison	O
,	O
we	O
did	O
not	O
use	O
these	O
external	O
deep	B-Method
networks	E-Method
in	O
other	O
experiments	O
on	O
image	B-Task
generation	E-Task
and	O
classification	B-Task
tasks	E-Task
.	O

We	O
simply	O
used	O
the	O
distance	O
between	O
raw	O
images	O
as	O
the	O
loss	O
margin	O
,	O
and	O
it	O
still	O
achieved	O
competitive	O
results	O
.	O

This	O
demonstrates	O
the	O
robustness	O
of	O
the	O
proposed	O
method	O
without	O
having	O
to	O
choose	O
a	O
sophisticated	O
loss	O
margin	O
.	O

This	O
is	O
also	O
consistent	O
with	O
our	O
theoretical	O
analysis	O
where	O
we	O
do	O
not	O
assume	O
any	O
particular	O
form	O
of	O
loss	O
margin	O
to	O
prove	O
the	O
results	O
.	O

For	O
the	O
generator	B-Method
network	E-Method
of	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
it	O
took	O
a	O
-	O
dimensional	O
random	O
vector	O
drawn	O
from	O
Unif	S-Method
as	O
input	O
.	O

For	O
the	O
CLS	O
-	O
GAN	S-Method
generator	O
,	O
an	O
one	B-Method
-	I-Method
hot	I-Method
vector	E-Method
encoding	O
the	O
image	O
class	O
condition	O
was	O
concatenated	O
with	O
the	O
sampled	O
random	O
vector	O
.	O

The	O
CLS	B-Method
-	I-Method
GAN	E-Method
was	O
trained	O
by	O
involving	O
both	O
unlabeled	O
and	O
labeled	O
examples	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

This	O
was	O
compared	O
against	O
the	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	B-Method
and	I-Method
semi	I-Method
-	I-Method
supervised	I-Method
models	E-Method
.	O

subsection	O
:	O
Generated	O
Images	O
by	O
LS	B-Method
-	I-Method
GAN	E-Method
First	O
we	O
made	O
a	O
qualitative	O
comparison	O
between	O
the	O
images	O
generated	O
by	O
the	O
DCGAN	S-Method
and	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
on	O
the	O
celebA	O
dataset	O
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
visual	B-Metric
quality	I-Metric
of	I-Metric
images	E-Metric
generated	O
by	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
DCGAN	S-Method
after	O
they	O
were	O
trained	O
for	O
epochs	O
,	O
and	O
there	O
was	O
no	O
perceptible	O
difference	O
between	O
the	O
qualities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
the	O
DCGAN	B-Method
architecture	E-Method
has	O
been	O
exhaustively	O
fine	O
-	O
tuned	O
in	O
terms	O
of	O
the	O
classic	O
GAN	S-Method
training	O
criterion	O
to	O
maximize	O
the	O
image	B-Task
generation	E-Task
performance	O
.	O

It	O
was	O
susceptible	O
that	O
its	O
architecture	O
could	O
be	O
fragile	O
if	O
we	O
make	O
some	O
change	O
to	O
it	O
.	O

Here	O
we	O
tested	O
if	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
can	O
be	O
more	O
robust	O
than	O
the	O
DCGAN	S-Method
when	O
a	O
structure	O
change	O
was	O
made	O
.	O

For	O
example	O
,	O
one	O
of	O
the	O
most	O
key	O
components	O
in	O
the	O
DCGAN	S-Method
is	O
the	O
batch	B-Method
normalization	E-Method
inserted	O
between	O
the	O
fractional	B-Method
convolution	I-Method
layers	E-Method
in	O
the	O
generator	B-Method
network	E-Method
.	O

It	O
has	O
been	O
reported	O
in	O
literature	O
that	O
the	O
batch	B-Method
normalization	E-Method
not	O
only	O
plays	O
a	O
key	O
role	O
in	O
training	O
the	O
DCGAN	B-Method
model	E-Method
,	O
but	O
also	O
prevents	O
the	O
mode	O
collapse	O
of	O
the	O
generator	O
into	O
few	O
data	O
points	O
.	O

The	O
results	O
were	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

If	O
one	O
removed	O
the	O
batch	B-Method
normalization	I-Method
layers	E-Method
from	O
the	O
generator	S-Method
,	O
the	O
DCGAN	S-Method
would	O
collapse	O
without	O
producing	O
any	O
face	O
images	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
still	O
performed	O
very	O
well	O
even	O
if	O
these	O
batch	B-Method
normalization	I-Method
layers	E-Method
were	O
removed	O
,	O
and	O
there	O
was	O
no	O
perceived	O
deterioration	O
or	O
mode	O
collapse	O
of	O
the	O
generated	O
images	O
.	O

This	O
shows	O
that	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
was	O
more	O
resilient	O
than	O
the	O
DCGAN	S-Method
.	O

We	O
also	O
analyzed	O
the	O
magnitude	O
(	O
norm	O
)	O
of	O
the	O
generator	O
’s	O
gradient	O
(	O
in	O
logarithmic	O
scale	O
)	O
in	O
Figure	O
[	O
reference	O
]	O
over	O
iterations	O
.	O

With	O
the	O
loss	O
function	O
being	O
updated	O
every	O
iteration	O
,	O
the	O
generator	S-Method
was	O
only	O
updated	O
every	O
,	O
,	O
and	O
iterations	O
.	O

From	O
the	O
figure	O
,	O
we	O
note	O
that	O
the	O
magnitude	O
of	O
the	O
generator	O
’s	O
gradient	O
,	O
no	O
matter	O
how	O
frequently	O
the	O
loss	O
function	O
was	O
updated	O
,	O
gradually	O
increased	O
until	O
it	O
stopped	O
at	O
the	O
same	O
level	O
.	O

This	O
implies	O
the	O
objective	O
function	O
to	O
update	O
the	O
generator	S-Method
tended	O
to	O
be	O
linear	O
rather	O
than	O
saturated	O
through	O
the	O
training	B-Method
process	E-Method
,	O
which	O
was	O
consistent	O
with	O
our	O
non	B-Method
-	I-Method
parametric	I-Method
analysis	E-Method
of	O
the	O
optimal	B-Method
loss	I-Method
function	E-Method
.	O

Thus	O
,	O
it	O
provided	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	S-Method
.	O

Furthermore	O
,	O
we	O
compared	O
the	O
images	O
generated	O
with	O
different	O
frequencies	O
of	O
updating	O
the	O
loss	O
function	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
there	O
was	O
no	O
noticeable	O
difference	O
in	O
the	O
visual	B-Metric
quality	E-Metric
.	O

This	O
shows	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
was	O
not	O
affected	O
by	O
over	O
-	O
trained	O
loss	O
function	O
in	O
experiments	O
.	O

subsection	O
:	O
Image	B-Task
Classification	E-Task
We	O
conducted	O
experiments	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
SVHN	S-Material
to	O
compare	O
the	O
classification	B-Metric
accuracy	E-Metric
of	O
LS	B-Method
-	I-Method
GAN	E-Method
with	O
the	O
other	O
approaches	O
.	O

subsubsection	O
:	O
CIFAR	B-Material
-	I-Material
10	E-Material
The	O
CIFAR	B-Material
dataset	E-Material
consists	O
of	O
50	O
,	O
000	O
training	O
images	O
and	O
test	O
images	O
on	O
ten	O
image	O
categories	O
.	O

We	O
tested	O
the	O
proposed	O
CLS	O
-	O
GAN	S-Method
model	O
with	O
class	O
labels	O
as	O
conditions	O
.	O

In	O
the	O
supervised	B-Task
training	E-Task
,	O
all	O
labeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
.	O

We	O
also	O
conducted	O
experiments	O
with	O
labeled	O
examples	O
per	O
class	O
,	O
which	O
was	O
a	O
more	O
challenging	O
task	O
as	O
much	O
fewer	O
labeled	O
examples	O
were	O
used	O
for	O
training	O
.	O

In	O
this	O
case	O
,	O
the	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
model	O
in	O
a	O
semi	B-Task
-	I-Task
supervised	I-Task
fashion	E-Task
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
each	O
mini	O
-	O
batch	O
,	O
the	O
same	O
number	O
of	O
labeled	O
and	O
unlabeled	O
examples	O
were	O
used	O
to	O
update	O
the	O
model	O
by	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
.	O

The	O
experiment	O
results	O
on	O
this	O
task	O
were	O
reported	O
by	O
averaging	O
over	O
ten	O
subsets	O
of	O
labeled	O
examples	O
.	O

Both	O
hyperparameters	O
and	O
were	O
chosen	O
via	O
a	O
five	O
-	O
fold	B-Method
cross	I-Method
-	I-Method
validation	E-Method
on	O
the	O
labeled	O
examples	O
from	O
and	O
respectively	O
.	O

Once	O
they	O
were	O
chosen	O
,	O
the	O
model	O
was	O
trained	O
with	O
the	O
chosen	O
hyperparameters	O
on	O
the	O
whole	O
training	O
set	O
,	O
and	O
the	O
performance	O
was	O
reported	O
based	O
on	O
the	O
results	O
on	O
the	O
test	O
set	O
.	O

As	O
in	O
the	O
improved	O
GAN	S-Method
,	O
we	O
also	O
adopted	O
the	O
weight	B-Method
normalization	E-Method
and	O
feature	B-Method
matching	I-Method
mechanisms	E-Method
for	O
the	O
sake	O
of	O
the	O
fair	O
comparison	O
.	O

We	O
compared	O
the	O
proposed	O
model	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
literature	O
.	O

In	O
particular	O
,	O
we	O
compared	O
with	O
the	O
conditional	O
GAN	S-Method
as	O
well	O
as	O
the	O
DCGAN	S-Method
.	O

For	O
the	O
sake	O
of	O
fair	O
comparison	O
,	O
the	O
conditional	O
GAN	S-Method
shared	O
the	O
same	O
architecture	O
as	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
DCGAN	B-Method
algorithm	E-Method
max	O
-	O
pooled	O
the	O
discriminator	O
’s	O
convolution	O
features	O
from	O
all	O
layers	O
to	O
grids	O
as	O
the	O
image	O
features	O
,	O
and	O
a	O
L2	B-Method
-	I-Method
SVM	E-Method
was	O
then	O
trained	O
to	O
classify	O
images	O
.	O

The	O
DCGAN	S-Method
was	O
an	O
unsupervised	B-Method
model	E-Method
which	O
had	O
shown	O
competitive	O
performance	O
on	O
generating	O
photo	O
-	O
realistic	O
images	O
.	O

Its	O
feature	B-Method
representations	E-Method
were	O
believed	O
to	O
reach	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
modeling	B-Task
images	E-Task
with	O
no	O
supervision	O
.	O

We	O
also	O
compared	O
with	O
the	O
other	O
recently	O
developed	O
supervised	B-Method
and	I-Method
semi	I-Method
-	I-Method
supervised	I-Method
models	E-Method
in	O
literature	O
,	O
including	O
the	O
baseline	B-Method
1	I-Method
Layer	I-Method
K	I-Method
-	I-Method
means	I-Method
feature	I-Method
extraction	I-Method
pipeline	E-Method
,	O
a	O
multi	B-Method
-	I-Method
layer	I-Method
extension	E-Method
of	O
the	O
baseline	B-Method
model	E-Method
(	O
3	O
Layer	B-Method
K	I-Method
-	I-Method
means	I-Method
Learned	I-Method
RF	E-Method
)	O
,	O
View	B-Method
Invariant	I-Method
K	I-Method
-	I-Method
means	E-Method
,	O
Examplar	B-Method
CNN	E-Method
,	O
Ladder	B-Method
Network	E-Method
,	O
as	O
well	O
as	O
CatGAN	S-Method
.	O

In	O
particular	O
,	O
among	O
the	O
compared	O
semi	B-Method
-	I-Method
supervised	I-Method
algorithms	E-Method
,	O
the	O
improved	O
GAN	S-Method
had	O
recorded	O
the	O
best	O
performance	O
in	O
literature	O
.	O

Furthermore	O
,	O
we	O
also	O
compared	O
with	O
the	O
ALI	S-Method
that	O
extended	O
the	O
classic	O
GAN	S-Method
by	O
jointly	O
generating	O
data	O
and	O
inferring	O
their	O
representations	O
,	O
which	O
achieved	O
comparable	O
performance	O
to	O
the	O
Improved	O
GAN	S-Method
.	O

This	O
pointed	O
out	O
an	O
interesting	O
direction	O
to	O
extend	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
by	O
directly	O
inferring	O
the	O
data	B-Method
representation	E-Method
,	O
and	O
we	O
will	O
leave	O
it	O
in	O
the	O
future	O
work	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
experiment	O
results	O
,	O
showing	O
the	O
CSL	O
-	O
GAN	S-Method
successfully	O
outperformed	O
the	O
compared	O
algorithms	O
in	O
both	O
fully	B-Task
-	I-Task
supervised	I-Task
and	I-Task
semi	I-Task
-	I-Task
supervised	I-Task
settings	E-Task
.	O

subsubsection	O
:	O
SVHN	S-Material
The	O
SVHN	S-Material
(	O
i.e.	O
,	O
Street	B-Material
View	I-Material
House	I-Material
Number	E-Material
)	O
dataset	O
contains	O
color	O
images	O
of	O
house	O
numbers	O
collected	O
by	O
Google	O
Street	O
View	O
.	O

They	O
were	O
roughly	O
centered	O
on	O
a	O
digit	O
in	O
a	O
house	O
number	O
,	O
and	O
the	O
objective	O
is	O
to	O
recognize	O
the	O
digit	O
.	O

The	O
training	O
set	O
has	O
digits	O
while	O
the	O
test	O
set	O
consists	O
of	O
.	O

To	O
test	O
the	O
model	O
,	O
labeled	O
digits	O
were	O
used	O
to	O
train	O
the	O
model	O
,	O
which	O
are	O
uniformly	O
selected	O
from	O
ten	O
digit	O
classes	O
,	O
that	O
is	O
labeled	O
examples	O
per	O
digit	O
class	O
.	O

The	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
as	O
additional	O
data	O
to	O
enhance	O
the	O
generative	B-Method
ability	E-Method
of	O
CLS	B-Method
-	I-Method
GAN	E-Method
in	O
semi	B-Task
-	I-Task
supervised	I-Task
fashion	E-Task
.	O

We	O
expect	O
a	O
good	O
generative	B-Method
model	E-Method
could	O
produce	O
additional	O
examples	O
to	O
augment	O
the	O
training	O
set	O
.	O

We	O
used	O
the	O
same	O
experiment	O
setup	O
and	O
network	B-Method
architecture	E-Method
for	O
CIFAR	B-Material
-	I-Material
10	E-Material
to	O
train	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
on	O
this	O
dataset	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
result	O
on	O
the	O
SVHN	S-Material
,	O
and	O
it	O
shows	O
that	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
performed	O
the	O
best	O
among	O
the	O
compared	O
algorithms	O
.	O

subsubsection	O
:	O
Analysis	B-Task
of	I-Task
Generated	I-Task
Images	E-Task
by	O
CLS	B-Method
-	I-Method
GAN	E-Method
Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
generated	O
images	O
by	O
CLS	B-Method
-	I-Method
GAN	E-Method
for	O
MNIST	O
,	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
SVHN	S-Material
datasets	O
.	O

On	O
each	O
dataset	O
,	O
images	O
in	O
a	O
column	O
were	O
generated	O
for	O
the	O
same	O
class	O
.	O

On	O
the	O
MNIST	O
and	O
the	O
SVHN	S-Material
,	O
both	O
handwritten	O
and	O
street	O
-	O
view	O
digits	O
are	O
quite	O
legible	O
.	O

Both	O
also	O
cover	O
many	O
variants	O
for	O
each	O
digit	O
class	O
.	O

For	O
example	O
,	O
the	O
synthesized	O
MNIST	O
digits	O
have	O
various	O
writing	O
styles	O
,	O
rotations	O
and	O
sizes	O
,	O
and	O
the	O
generated	O
SVHN	S-Material
digits	O
have	O
various	O
lighting	O
conditions	O
,	O
sizes	O
and	O
even	O
different	O
co	O
-	O
occurring	O
digits	O
in	O
the	O
cropped	O
bounding	O
boxes	O
.	O

On	O
the	O
CIFAR	B-Material
-	I-Material
10	E-Material
dataset	O
,	O
image	O
classes	O
can	O
be	O
recognized	O
from	O
the	O
generated	O
images	O
although	O
some	O
visual	O
details	O
are	O
missing	O
.	O

This	O
is	O
because	O
the	O
images	O
in	O
the	O
CIFAR	B-Material
-	I-Material
10	E-Material
dataset	O
have	O
very	O
low	O
resolution	O
(	O
pixels	O
)	O
,	O
and	O
most	O
details	O
are	O
even	O
missing	O
from	O
input	O
examples	O
.	O

We	O
also	O
observe	O
that	O
if	O
we	O
set	O
a	O
small	O
value	O
to	O
the	O
hyperparameter	O
,	O
the	O
generated	O
images	O
would	O
become	O
very	O
similar	O
to	O
each	O
other	O
within	O
each	O
class	O
.	O

As	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
images	O
were	O
generated	O
by	O
halving	O
used	O
for	O
generating	O
images	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
smaller	O
means	O
a	O
relatively	O
large	O
weight	O
was	O
placed	O
on	O
the	O
first	O
loss	B-Method
minimization	I-Method
term	E-Method
of	O
(	O
[	O
reference	O
]	O
)	O
,	O
which	O
tends	O
to	O
collapse	O
generated	O
images	O
to	O
a	O
single	O
mode	O
as	O
it	O
aggressively	O
minimizes	O
their	O
losses	O
to	O
train	O
the	O
generator	S-Method
.	O

This	O
is	O
also	O
consistent	O
with	O
Theorem	O
[	O
reference	O
]	O
where	O
the	O
density	O
of	O
generated	O
samples	O
with	O
a	O
smaller	O
could	O
have	O
a	O
larger	O
deviation	O
from	O
the	O
underlying	O
density	O
.	O

One	O
should	O
avoid	O
the	O
collapse	O
of	O
trained	O
generator	S-Method
since	O
diversifying	O
generated	O
images	O
can	O
improve	O
the	O
classification	B-Metric
performance	E-Metric
of	O
the	O
CLS	B-Method
-	I-Method
GAN	E-Method
by	O
revealing	O
more	O
intra	O
-	O
class	O
variations	O
.	O

This	O
will	O
help	O
improve	O
the	O
model	O
’s	O
generalization	B-Metric
ability	E-Metric
as	O
these	O
variations	O
could	O
appear	O
in	O
future	O
images	O
.	O

However	O
,	O
one	O
should	O
also	O
avoid	O
setting	O
too	O
large	O
value	O
to	O
.	O

Otherwise	O
,	O
the	O
role	O
of	O
the	O
first	B-Method
loss	I-Method
minimization	I-Method
term	E-Method
could	O
be	O
underestimated	O
,	O
which	O
can	O
also	O
adversely	O
affect	O
the	O
classification	S-Task
results	O
without	O
reducing	O
the	O
training	B-Metric
loss	E-Metric
to	O
a	O
satisfactory	O
level	O
.	O

Therefore	O
,	O
we	O
choose	O
a	O
proper	O
value	O
for	O
by	O
cross	B-Method
-	I-Method
validation	E-Method
on	O
the	O
training	O
set	O
in	O
the	O
experiments	O
.	O

In	O
brief	O
,	O
the	O
comparison	O
between	O
Figure	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
reveals	O
a	O
trade	O
-	O
off	O
between	O
image	B-Task
generation	E-Task
quality	O
and	O
classification	B-Metric
accuracy	E-Metric
through	O
the	O
hyperparameter	S-Method
.	O

Such	O
a	O
trade	O
-	O
off	O
is	O
intuitive	O
:	O
while	O
a	O
classification	B-Task
task	E-Task
usually	O
focuses	O
on	O
learning	O
class	B-Task
-	I-Task
invariant	I-Task
representations	E-Task
that	O
do	O
not	O
change	O
within	O
a	O
class	O
,	O
image	B-Task
generation	E-Task
should	O
be	O
able	O
to	O
capture	O
many	O
variant	O
factors	O
(	O
e.g.	O
,	O
lighting	O
conditions	O
,	O
viewing	O
angles	O
,	O
and	O
object	O
poses	O
)	O
so	O
that	O
it	O
could	O
diversify	O
generated	O
samples	O
for	O
each	O
class	O
.	O

Although	O
diversified	O
examples	O
can	O
augment	O
training	O
dataset	O
,	O
it	O
comes	O
at	O
a	O
cost	O
of	O
trading	O
class	O
-	O
invariance	O
for	O
modeling	O
variant	O
generation	O
factors	O
.	O

Perhaps	O
,	O
this	O
is	O
an	O
intrinsic	O
dilemma	O
between	O
supervised	B-Method
learning	E-Method
and	O
data	B-Task
generation	E-Task
that	O
is	O
worth	O
more	O
theoretical	O
and	O
empirical	O
studies	O
in	O
future	O
.	O

subsection	O
:	O
Evaluation	O
of	O
Generalization	B-Metric
Performances	E-Metric
Most	O
of	O
existing	O
metrics	O
like	O
Inception	B-Metric
Score	E-Metric
for	O
evaluating	O
GAN	S-Method
models	O
focus	O
on	O
comparing	O
the	O
qualities	O
and	O
diversities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
even	O
though	O
a	O
GAN	S-Method
model	O
can	O
produce	O
diverse	O
and	O
high	O
quality	O
images	O
with	O
no	O
collapsed	B-Method
generators	E-Method
,	O
it	O
is	O
still	O
unknown	O
if	O
the	O
model	O
can	O
generate	O
unseen	O
images	O
out	O
of	O
given	O
examples	O
,	O
or	O
simply	O
memorizing	O
existing	O
ones	O
.	O

While	O
one	O
of	O
our	O
main	O
pursuits	O
in	O
this	O
paper	O
is	O
a	O
generalizable	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
we	O
were	O
motivated	O
to	O
propose	O
the	O
following	O
Minimum	B-Metric
Reconstruction	I-Metric
Error	E-Metric
(	O
MRE	S-Metric
)	O
to	O
compare	O
its	O
generalizability	O
with	O
various	O
GANs	S-Method
.	O

Specifically	O
,	O
for	O
an	O
unseen	O
test	O
image	O
,	O
we	O
aim	O
to	O
find	O
an	O
input	O
noise	O
that	O
can	O
best	O
reconstruct	O
with	O
the	O
smallest	O
error	O
,	O
i.e.	O
,	O
where	O
is	O
the	O
GAN	S-Method
generator	O
under	O
evaluation	O
.	O

Obviously	O
,	O
if	O
is	O
adequate	O
to	O
produce	O
new	O
images	O
,	O
it	O
should	O
have	O
a	O
small	O
reconstruction	B-Metric
error	E-Metric
on	O
a	O
separate	O
test	O
set	O
that	O
has	O
not	O
been	O
used	O
in	O
training	O
the	O
model	O
.	O

We	O
assessed	O
the	O
GAN	S-Method
’s	O
generalizability	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
tiny	O
ImageNet	O
datasets	O
.	O

On	O
CIFAR	B-Material
-	I-Material
10	E-Material
,	O
we	O
split	O
the	O
dataset	O
into	O
50	O
%	O
training	O
examples	O
,	O
25	O
%	O
validation	O
examples	O
and	O
25	O
%	O
test	O
examples	O
;	O
the	O
tiny	O
ImageNet	O
was	O
split	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
in	O
a	O
ratio	O
of	O
10:1:1	O
.	O

For	O
a	O
fair	O
comparison	O
,	O
all	O
the	O
hyperparameters	O
,	O
including	O
the	O
number	O
of	O
epochs	O
,	O
were	O
chosen	O
based	O
on	O
the	O
average	B-Metric
MREs	E-Metric
on	O
the	O
validation	O
set	O
,	O
and	O
the	O
test	O
MREs	S-Metric
were	O
reported	O
for	O
comparison	O
.	O

The	O
optimal	O
’s	O
were	O
iteratively	O
updated	O
on	O
the	O
validation	O
and	O
test	O
sets	O
by	O
descending	O
the	O
gradient	O
of	O
the	O
reconstruction	O
errors	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
compare	O
the	O
test	O
MREs	S-Metric
over	O
epochs	O
by	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
GLS	B-Method
-	I-Method
GAN	E-Method
,	O
WGAN	S-Method
,	O
WGAN	S-Method
-	O
GP	O
and	O
DCGAN	S-Method
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
respectively	O
.	O

For	O
the	O
sake	O
of	O
a	O
fair	O
comparison	O
,	O
all	O
models	O
were	O
trained	O
with	O
the	O
network	B-Method
architecture	E-Method
used	O
in	O
.	O

The	O
result	O
clearly	O
shows	O
the	O
regularized	B-Method
models	E-Method
,	O
including	O
GLS	B-Method
-	I-Method
GAN	E-Method
,	O
LS	B-Method
-	I-Method
GAN	E-Method
,	O
WGAN	S-Method
-	O
GP	O
and	O
WGAN	S-Method
,	O
have	O
apparently	O
better	O
generalization	B-Metric
performances	E-Metric
than	O
the	O
unregularized	B-Method
DCGAN	E-Method
based	O
on	O
the	O
classic	O
GAN	S-Method
model	O
.	O

On	O
CIFAR	B-Material
-	I-Material
10	E-Material
,	O
the	O
test	O
MRE	S-Metric
was	O
reduced	O
from	O
by	O
DCGAN	S-Method
to	O
as	O
small	O
as	O
and	O
by	O
WGAN	S-Method
and	O
GLS	O
-	O
GAN	S-Method
respectively	O
;	O
on	O
tiny	O
ImageNet	O
,	O
the	O
GLS	B-Method
-	I-Method
GAN	E-Method
reaches	O
the	O
smallest	O
test	O
MRE	S-Metric
of	O
among	O
all	O
compared	O
regularized	O
and	O
unregularized	O
GANs	S-Method
.	O

In	O
addition	O
,	O
the	O
DCGAN	S-Method
exhibited	O
fluctuating	O
MREs	S-Metric
on	O
the	O
CIFAR	B-Material
-	I-Material
10	E-Material
,	O
while	O
the	O
regularized	B-Method
models	E-Method
steadily	O
decreased	O
the	O
MREs	S-Metric
over	O
epochs	O
.	O

This	O
implies	O
regularized	O
GANs	S-Method
have	O
more	O
stable	O
training	O
than	O
the	O
classic	O
GAN	S-Method
.	O

We	O
illustrate	O
some	O
examples	O
of	O
reconstructed	O
images	O
by	O
different	O
GANs	S-Method
on	O
the	O
test	O
set	O
along	O
with	O
their	O
test	O
MREs	S-Metric
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
results	O
show	O
the	O
GLS	B-Method
-	I-Method
GAN	E-Method
achieved	O
the	O
smallest	O
test	B-Metric
MRE	E-Metric
of	O
0.1089	O
and	O
0.2085	O
with	O
a	O
LeakyReLU	O
cost	O
function	O
of	O
slope	O
and	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
tiny	O
ImageNet	O
,	O
followed	O
by	O
the	O
other	O
regularized	O
GAN	S-Method
models	O
.	O

This	O
is	O
not	O
a	O
surprising	O
result	O
since	O
it	O
has	O
been	O
shown	O
in	O
Section	O
[	O
reference	O
]	O
that	O
the	O
other	O
regularized	O
GANs	S-Method
such	O
as	O
LS	B-Method
-	I-Method
GAN	E-Method
and	O
WGAN	S-Method
are	O
only	O
special	O
cases	O
of	O
the	O
GLS	O
-	O
GAN	S-Method
model	O
that	O
covers	O
larger	O
family	O
of	O
models	O
.	O

Here	O
we	O
only	O
considered	O
LeakyReLU	O
as	O
the	O
cost	B-Method
function	E-Method
for	O
GLS	B-Method
-	I-Method
GAN	E-Method
.	O

Of	O
course	O
,	O
there	O
exist	O
many	O
more	O
cost	O
functions	O
satisfying	O
the	O
two	O
conditions	O
in	O
Section	O
[	O
reference	O
]	O
to	O
expand	O
the	O
family	O
of	O
regularized	O
GANs	S-Method
,	O
which	O
should	O
have	O
potentials	O
of	O
yielding	O
even	O
better	O
generalization	B-Metric
performances	E-Metric
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	S-Method
(	O
LS	B-Method
-	I-Method
GAN	E-Method
)	O
approach	O
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
LS	B-Method
-	I-Method
GAN	E-Method
learns	O
a	O
loss	B-Method
function	E-Method
to	O
distinguish	O
between	O
generated	O
and	O
real	O
samples	O
,	O
where	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
by	O
a	O
margin	O
than	O
that	O
of	O
a	O
generated	O
sample	O
.	O

Our	O
theoretical	O
analysis	O
shows	O
the	O
distributional	O
consistency	O
between	O
the	O
real	O
and	O
generated	O
samples	O
based	O
on	O
the	O
Lipschitz	O
regularity	O
.	O

This	O
no	O
longer	O
needs	O
a	O
non	B-Method
-	I-Method
parametric	I-Method
discriminator	E-Method
with	O
infinite	O
modeling	O
ability	O
in	O
the	O
classic	O
GAN	S-Method
,	O
allowing	O
us	O
to	O
search	O
for	O
the	O
optimal	O
loss	O
function	O
in	O
a	O
smaller	O
functional	O
space	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
.	O

Moreover	O
,	O
we	O
prove	O
the	O
generalizability	O
of	O
LS	B-Method
-	I-Method
GAN	E-Method
by	O
showing	O
its	O
required	O
number	O
of	O
training	O
examples	O
is	O
polynomial	O
in	O
its	O
complexity	S-Metric
.	O

This	O
suggests	O
the	O
generalization	B-Metric
performance	E-Metric
can	O
be	O
improved	O
by	O
penalizing	O
the	O
Lipschitz	O
constants	O
(	O
via	O
their	O
gradient	O
surrogates	O
)	O
of	O
the	O
loss	O
function	O
to	O
reduce	O
the	O
sample	B-Metric
complexity	E-Metric
.	O

Furthermore	O
,	O
our	O
non	B-Method
-	I-Method
parametric	I-Method
analysis	E-Method
of	O
the	O
optimal	B-Method
loss	I-Method
function	E-Method
shows	O
its	O
lower	O
and	O
upper	O
bounds	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	B-Task
gradient	E-Task
almost	O
everywhere	O
,	O
implying	O
the	O
generator	S-Method
can	O
be	O
continuously	O
updated	O
even	O
if	O
the	O
loss	O
function	O
is	O
over	O
-	O
trained	O
.	O

Finally	O
,	O
we	O
extend	O
the	O
LS	B-Method
-	I-Method
GAN	E-Method
to	O
a	O
Conditional	B-Method
LS	I-Method
-	I-Method
GAN	E-Method
(	O
CLS	B-Method
-	I-Method
GAN	E-Method
)	O
for	O
semi	B-Task
-	I-Task
supervised	I-Task
tasks	E-Task
,	O
and	O
demonstrate	O
it	O
reaches	O
competitive	O
performances	O
on	O
both	O
image	B-Task
generation	E-Task
and	O
classification	B-Task
tasks	E-Task
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
To	O
prove	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
two	O
probability	O
densities	O
⁢p	O
(	O
x	O
)	O
and	O
⁢q	O
(	O
x	O
)	O
,	O
if	O
≥⁢p	O
(	O
x	O
)	O
⁢ηq	O
(	O
x	O
)	O
almost	O
everywhere	O
,	O
we	O
have	O
for	O
∈η	O
(	O
0	O
,	O
1	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

We	O
have	O
the	O
following	O
equalities	O
and	O
inequalities	O
:	O
This	O
completes	O
the	O
proof	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Lemma	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

Suppose	O
is	O
a	O
Nash	O
equilibrium	O
for	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
.	O

We	O
also	O
have	O
for	O
any	O
as	O
minimizes	O
.	O

In	O
particular	O
,	O
we	O
can	O
replace	O
in	O
with	O
,	O
which	O
yields	O
Applying	O
this	O
inequality	O
into	O
(	O
[	O
reference	O
]	O
)	O
leads	O
to	O
where	O
the	O
last	O
inequality	O
follows	O
as	O
is	O
nonnegative	O
.	O

On	O
the	O
other	O
hand	O
,	O
consider	O
a	O
particular	O
loss	B-Method
function	E-Method
When	O
is	O
a	O
sufficiently	O
small	O
positive	O
coefficient	O
,	O
is	O
a	O
nonexpansive	O
function	O
(	O
i.e.	O
,	O
a	O
function	O
with	O
Lipschitz	O
constant	O
no	O
larger	O
than	O
.	O

)	O
.	O

This	O
follows	O
from	O
the	O
assumption	O
that	O
and	O
are	O
Lipschitz	O
.	O

In	O
this	O
case	O
,	O
we	O
have	O
By	O
placing	O
this	O
into	O
,	O
one	O
can	O
show	O
that	O
where	O
the	O
first	O
equality	O
uses	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
second	O
equality	O
is	O
obtained	O
by	O
substituting	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
into	O
the	O
equation	O
.	O

Assuming	O
that	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
the	O
above	O
equation	O
would	O
be	O
strictly	O
upper	O
bounded	O
by	O
and	O
we	O
have	O
This	O
results	O
in	O
a	O
contradiction	O
with	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Therefore	O
,	O
we	O
must	O
have	O
for	O
almost	O
everywhere	O
.	O

By	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
have	O
Let	O
,	O
this	O
leads	O
to	O
This	O
proves	O
that	O
converges	O
to	O
as	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
proof	O
:	O
Proof	O
.	O

Suppose	O
a	O
pair	O
of	O
jointly	O
solve	O
the	O
WGAN	S-Method
problem	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
inequality	O
follows	O
from	O
by	O
replacing	O
with	O
.	O

Consider	O
a	O
particular	O
.	O

Since	O
and	O
are	O
Lipschitz	O
by	O
assumption	O
,	O
when	O
is	O
sufficiently	O
small	O
,	O
it	O
can	O
be	O
shown	O
that	O
.	O

Substituting	O
this	O
into	O
,	O
we	O
get	O
Let	O
us	O
assume	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
we	O
would	O
have	O
This	O
leads	O
to	O
a	O
contradiction	O
with	O
(	O
[	O
reference	O
]	O
)	O
,	O
so	O
we	O
must	O
have	O
almost	O
everywhere	O
.	O

Hence	O
,	O
by	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
prove	O
the	O
conclusion	O
that	O
∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
For	O
simplicity	O
,	O
throughout	O
this	O
section	O
,	O
we	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
and	O
,	O
since	O
the	O
role	O
of	O
the	O
first	O
term	O
would	O
vanish	O
as	O
goes	O
to	O
.	O

However	O
,	O
even	O
if	O
it	O
is	O
involved	O
,	O
the	O
following	O
proof	O
still	O
holds	O
with	O
only	O
some	O
minor	O
changes	O
.	O

To	O
prove	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
all	O
loss	O
functions	O
Lθ	O
,	O
with	O
at	O
least	O
the	O
probability	O
of	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
with	O
a	O
sufficiently	O
large	O
constant	O
C.	O
The	O
proof	O
of	O
this	O
lemma	O
needs	O
to	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
and	O
the	O
fact	O
that	O
is	O
an	O
1	O
-	O
Lipschitz	O
to	O
bound	O
the	O
difference	O
for	O
a	O
loss	O
function	O
.	O

Then	O
,	O
to	O
get	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
,	O
a	O
standard	O
-	O
net	O
will	O
be	O
constructed	O
to	O
yield	O
finite	O
points	O
that	O
are	O
dense	O
enough	O
to	O
cover	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
functions	O
.	O

The	O
proof	O
details	O
are	O
given	O
below	O
.	O

proof	O
:	O
Proof	O
.	O

For	O
a	O
loss	O
function	O
,	O
we	O
compute	O
over	O
a	O
set	O
of	O
samples	O
drawn	O
from	O
and	O
respectively	O
.	O

To	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
,	O
we	O
need	O
to	O
bound	O
the	O
change	O
of	O
this	O
function	O
when	O
a	O
sample	O
is	O
changed	O
.	O

Denote	O
by	O
when	O
the	O
th	O
sample	O
is	O
replaced	O
with	O
and	O
.	O

Then	O
we	O
have	O
where	O
the	O
first	O
inequality	O
uses	O
the	O
fact	O
that	O
is	O
-	O
Lipschitz	O
,	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
bounded	O
by	O
and	O
is	O
-	O
Lipschitz	O
in	O
.	O

Now	O
we	O
can	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
.	O

Noting	O
that	O
we	O
have	O
The	O
above	O
bound	O
applies	O
to	O
a	O
single	O
loss	O
function	O
.	O

To	O
get	O
the	O
union	O
bound	O
,	O
we	O
consider	O
a	O
-	O
net	O
,	O
i.e.	O
,	O
for	O
any	O
,	O
there	O
is	O
a	O
in	O
this	O
net	O
so	O
that	O
.	O

This	O
standard	O
net	O
can	O
be	O
constructed	O
to	O
contain	O
finite	O
loss	O
functions	O
such	O
that	O
,	O
where	O
is	O
the	O
number	O
of	O
parameters	O
in	O
a	O
loss	O
function	O
.	O

Note	O
that	O
we	O
implicitly	O
assume	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
function	O
is	O
bounded	O
so	O
we	O
can	O
construct	O
such	O
a	O
net	O
containing	O
finite	O
points	O
here	O
.	O

Therefore	O
,	O
we	O
have	O
the	O
following	O
union	O
bound	O
for	O
all	O
that	O
,	O
with	O
probability	O
,	O
when	O
.	O

The	O
last	O
step	O
is	O
to	O
obtain	O
the	O
union	O
bound	O
for	O
all	O
loss	O
functions	O
beyond	O
.	O

To	O
show	O
that	O
,	O
we	O
consider	O
the	O
following	O
inequality	O
where	O
the	O
first	O
inequality	O
uses	O
that	O
fact	O
that	O
is	O
-	O
Lipschitz	O
again	O
,	O
and	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
-	O
Lipschitz	O
in	O
.	O

Similarly	O
,	O
we	O
can	O
also	O
show	O
that	O
Now	O
we	O
can	O
derive	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
.	O

For	O
any	O
,	O
by	O
construction	O
we	O
can	O
find	O
a	O
such	O
that	O
.	O

Then	O
,	O
with	O
probability	O
,	O
we	O
have	O
This	O
proves	O
the	O
lemma	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Theorem	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
let	O
us	O
bound	O
.	O

Consider	O
that	O
minimizes	O
.	O

Then	O
with	O
probability	O
,	O
when	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
the	O
inequality	O
as	O
may	O
not	O
minimize	O
,	O
and	O
the	O
second	O
inequality	O
is	O
a	O
direct	O
application	O
of	O
the	O
above	O
lemma	O
.	O

Similarly	O
,	O
we	O
can	O
prove	O
the	O
other	O
direction	O
.	O

With	O
probability	O
,	O
we	O
have	O
Finally	O
,	O
a	O
more	O
rigourous	O
discussion	O
about	O
the	O
generalizability	O
should	O
consider	O
that	O
is	O
updated	O
iteratively	O
.	O

Therefore	O
we	O
have	O
a	O
sequence	O
of	O
generated	O
over	O
iterations	O
for	O
.	O

Thus	O
,	O
a	O
union	O
bound	O
over	O
all	O
generators	O
should	O
be	O
considered	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
makes	O
the	O
required	O
number	O
of	O
training	O
examples	O
become	O
However	O
,	O
the	O
iteration	O
number	O
is	O
usually	O
much	O
smaller	O
than	O
the	O
model	O
size	O
(	O
which	O
is	O
often	O
hundreds	O
of	O
thousands	O
)	O
,	O
and	O
thus	O
this	O
factor	O
will	O
not	O
affect	O
the	O
above	O
lower	O
bound	O
of	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
and	O
Corollary	O
[	O
reference	O
]	O
We	O
prove	O
Theorem	O
[	O
reference	O
]	O
as	O
follows	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
,	O
the	O
existence	O
of	O
a	O
minimizer	O
follows	O
from	O
the	O
fact	O
that	O
the	O
functions	O
in	O
form	O
a	O
compact	O
set	O
,	O
and	O
the	O
objective	O
function	O
is	O
convex	O
.	O

To	O
prove	O
the	O
minimizer	O
has	O
the	O
two	O
forms	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
for	O
each	O
,	O
let	O
us	O
consider	O
It	O
is	O
not	O
hard	O
to	O
verify	O
that	O
and	O
for	O
.	O

Indeed	O
,	O
by	O
noting	O
that	O
has	O
its	O
Lipschitz	O
constant	O
bounded	O
by	O
,	O
we	O
have	O
,	O
and	O
thus	O
Because	O
by	O
the	O
assumption	O
(	O
i.e.	O
,	O
it	O
is	O
lower	O
bounded	O
by	O
zero	O
)	O
,	O
it	O
can	O
be	O
shown	O
that	O
for	O
all	O
Hence	O
,	O
by	O
the	O
definition	O
of	O
and	O
taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
because	O
for	O
any	O
,	O
and	O
it	O
is	O
true	O
in	O
particular	O
for	O
.	O

This	O
shows	O
.	O

Similarly	O
,	O
one	O
can	O
prove	O
.	O

To	O
show	O
this	O
,	O
we	O
have	O
by	O
the	O
Lipschitz	O
continuity	O
of	O
.	O

By	O
taking	O
the	O
minimum	O
over	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
by	O
the	O
definition	O
of	O
.	O

Combining	O
these	O
two	O
inequalities	O
shows	O
that	O
.	O

Now	O
we	O
can	O
prove	O
for	O
any	O
function	O
,	O
there	O
exist	O
and	O
both	O
of	O
which	O
attain	O
the	O
same	O
value	O
of	O
as	O
,	O
since	O
only	O
depends	O
on	O
the	O
values	O
of	O
on	O
the	O
data	O
points	O
.	O

In	O
particular	O
,	O
this	O
shows	O
that	O
any	O
global	O
minimum	O
in	O
of	O
can	O
also	O
be	O
attained	O
by	O
the	O
corresponding	O
functions	O
of	O
the	O
form	O
(	O
[	O
reference	O
]	O
)	O
.	O

By	O
setting	O
for	O
,	O
this	O
completes	O
the	O
proof	O
.	O

∎	O
Finally	O
,	O
we	O
prove	O
Corollary	O
[	O
reference	O
]	O
that	O
bounds	O
with	O
and	O
constructed	O
above	O
.	O

proof	O
:	O
Proof	O
.	O

By	O
the	O
Lipschitz	O
continuity	O
,	O
we	O
have	O
Since	O
,	O
it	O
follows	O
that	O
Taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
obtain	O
This	O
proves	O
the	O
lower	O
bound	O
.	O

Similarly	O
,	O
we	O
have	O
by	O
Lipschitz	O
continuity	O
which	O
,	O
by	O
taking	O
the	O
minimum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
leads	O
to	O
This	O
shows	O
the	O
upper	O
bound	O
.	O

∎	O
document	O
:	O
Multi	B-Method
-	I-Method
Task	I-Method
Deep	I-Method
Neural	I-Method
Networks	E-Method
for	O
Natural	B-Task
Language	I-Task
Understanding	E-Task
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
Multi	B-Method
-	I-Method
Task	I-Method
Deep	I-Method
Neural	I-Method
Network	E-Method
(	O
MT	B-Method
-	I-Method
DNN	E-Method
)	O
for	O
learning	B-Task
representations	E-Task
across	O
multiple	O
natural	B-Task
language	I-Task
understanding	E-Task
(	O
NLU	S-Task
)	O
tasks	O
.	O

MT	B-Method
-	I-Method
DNN	E-Method
not	O
only	O
leverages	O
large	O
amounts	O
of	O
cross	O
-	O
task	O
data	O
,	O
but	O
also	O
benefits	O
from	O
a	O
regularization	O
effect	O
that	O
leads	O
to	O
more	O
general	O
representations	O
to	O
help	O
adapt	O
to	O
new	O
tasks	O
and	O
domains	O
.	O

MT	B-Method
-	I-Method
DNN	E-Method
extends	O
the	O
model	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
a	O
pre	B-Method
-	I-Method
trained	I-Method
bidirectional	I-Method
transformer	I-Method
language	I-Method
model	E-Method
,	O
known	O
as	O
BERT	S-Method
bert2018	O
.	O

MT	B-Method
-	I-Method
DNN	E-Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	S-Task
tasks	O
,	O
including	O
SNLI	S-Material
,	O
SciTail	S-Material
,	O
and	O
eight	O
out	O
of	O
nine	O
GLUE	S-Material
tasks	O
,	O
pushing	O
the	O
GLUE	S-Material
benchmark	O
to	O
82.2	O
%	O
(	O
1.8	O
%	O
absolute	O
improvement	O
)	O
.	O

We	O
also	O
demonstrate	O
using	O
the	O
SNLI	S-Material
and	O
SciTail	B-Material
datasets	E-Material
that	O
the	O
representations	O
learned	O
by	O
MT	B-Method
-	I-Method
DNN	E-Method
allow	O
domain	B-Task
adaptation	E-Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	S-Method
representations	O
.	O

Our	O
code	O
and	O
pre	O
-	O
trained	O
models	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Introduction	O
Learning	O
vector	B-Method
-	I-Method
space	I-Method
representations	I-Method
of	I-Method
text	E-Method
,	O
e.g.	O
,	O
words	O
and	O
sentences	O
,	O
is	O
fundamental	O
to	O
many	O
natural	B-Task
language	I-Task
understanding	E-Task
(	O
NLU	S-Task
)	O
tasks	O
.	O

Two	O
popular	O
approaches	O
are	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
and	O
language	B-Method
model	I-Method
pre	I-Method
-	I-Method
training	E-Method
.	O

In	O
this	O
paper	O
we	O
strive	O
to	O
combine	O
the	O
strengths	O
of	O
both	O
approaches	O
by	O
proposing	O
a	O
new	O
Multi	B-Method
-	I-Method
Task	I-Method
Deep	I-Method
Neural	I-Method
Network	E-Method
(	O
MT	B-Method
-	I-Method
DNN	E-Method
)	O
.	O

Multi	B-Task
-	I-Task
Task	I-Task
Learning	E-Task
(	O
MTL	B-Task
)	E-Task
is	O
inspired	O
by	O
human	B-Task
learning	I-Task
activities	E-Task
where	O
people	O
often	O
apply	O
the	O
knowledge	O
learned	O
from	O
previous	O
tasks	O
to	O
help	O
learn	O
a	O
new	O
task	O
caruana1997multitask	O
,	O
zhang2017survey	O
.	O

For	O
example	O
,	O
it	O
is	O
easier	O
for	O
a	O
person	O
who	O
knows	O
how	O
to	O
ski	O
to	O
learn	O
skating	O
than	O
the	O
one	O
who	O
does	O
not	O
.	O

Similarly	O
,	O
it	O
is	O
useful	O
for	O
multiple	O
(	O
related	O
)	O
tasks	O
to	O
be	O
learned	O
jointly	O
so	O
that	O
the	O
knowledge	O
learned	O
in	O
one	O
task	O
can	O
benefit	O
other	O
tasks	O
.	O

Recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
applying	O
MTL	S-Task
to	O
representation	B-Task
learning	E-Task
using	O
deep	B-Method
neural	I-Method
networks	E-Method
(	O
DNNs	S-Method
)	O
collobert2011natural	O
,	O
liu2015mtl	O
,	O
luong2015multi	O
,	O
mt	O
-	O
mrc2018	O
for	O
two	O
reasons	O
.	O

First	O
,	O
supervised	O
learning	O
of	O
DNNs	S-Method
requires	O
large	O
amounts	O
of	O
task	O
-	O
specific	O
labeled	O
data	O
,	O
which	O
is	O
not	O
always	O
available	O
.	O

MTL	S-Task
provides	O
an	O
effective	O
way	O
of	O
leveraging	O
supervised	O
data	O
from	O
many	O
related	O
tasks	O
.	O

Second	O
,	O
the	O
use	O
of	O
multi	B-Method
-	I-Method
task	I-Method
learning	E-Method
profits	O
from	O
a	O
regularization	O
effect	O
via	O
alleviating	O
overfitting	O
to	O
a	O
specific	O
task	O
,	O
thus	O
making	O
the	O
learned	O
representations	O
universal	O
across	O
tasks	O
.	O

In	O
contrast	O
to	O
MTL	S-Task
,	O
language	B-Method
model	I-Method
pre	I-Method
-	I-Method
training	E-Method
has	O
shown	O
to	O
be	O
effective	O
for	O
learning	O
universal	B-Task
language	I-Task
representations	E-Task
by	O
leveraging	O
large	O
amounts	O
of	O
unlabeled	O
data	O
.	O

A	O
recent	O
survey	O
is	O
included	O
in	O
gao2018neural	O
.	O

Some	O
of	O
the	O
most	O
prominent	O
examples	O
are	O
ELMo	S-Method
elmo2018	O
,	O
GPT	S-Method
gpt2018	O
and	O
BERT	S-Method
bert2018	O
.	O

These	O
are	O
neural	B-Method
network	I-Method
language	I-Method
models	E-Method
trained	O
on	O
text	O
data	O
using	O
unsupervised	O
objectives	O
.	O

For	O
example	O
,	O
BERT	S-Method
is	O
based	O
on	O
a	O
multi	B-Method
-	I-Method
layer	I-Method
bidirectional	I-Method
Transformer	E-Method
,	O
and	O
is	O
trained	O
on	O
plain	O
text	O
for	O
masked	B-Task
word	I-Task
prediction	E-Task
and	O
next	B-Task
sentence	I-Task
prediction	I-Task
tasks	E-Task
.	O

To	O
apply	O
a	O
pre	B-Method
-	I-Method
trained	I-Method
model	E-Method
to	O
specific	O
NLU	S-Task
tasks	O
,	O
we	O
often	O
need	O
to	O
fine	O
-	O
tune	O
,	O
for	O
each	O
task	O
,	O
the	O
model	O
with	O
additional	O
task	O
-	O
specific	O
layers	O
using	O
task	O
-	O
specific	O
training	O
data	O
.	O

For	O
example	O
,	O
bert2018	O
shows	O
that	O
BERT	S-Method
can	O
be	O
fine	O
-	O
tuned	O
this	O
way	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
range	O
of	O
NLU	S-Task
tasks	O
,	O
such	O
as	O
question	B-Task
answering	E-Task
and	O
natural	B-Task
language	I-Task
inference	E-Task
.	O

We	O
argue	O
that	O
MTL	S-Task
and	O
language	O
model	O
pre	O
-	O
training	O
are	O
complementary	O
technologies	O
,	O
and	O
can	O
be	O
combined	O
to	O
improve	O
the	O
learning	B-Task
of	I-Task
text	I-Task
representations	E-Task
to	O
boost	O
the	O
performance	O
of	O
various	O
NLU	S-Task
tasks	O
.	O

To	O
this	O
end	O
,	O
we	O
extend	O
the	O
MT	B-Method
-	I-Method
DNN	E-Method
model	O
originally	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
BERT	S-Method
as	O
its	O
shared	B-Method
text	I-Method
encoding	I-Method
layers	E-Method
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
lower	O
layers	O
(	O
i.e.	O
,	O
text	O
encoding	O
layers	O
)	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
are	O
task	O
-	O
specific	O
,	O
combining	O
different	O
types	O
of	O
NLU	S-Task
tasks	O
such	O
as	O
single	B-Task
-	I-Task
sentence	I-Task
classification	E-Task
,	O
pairwise	B-Task
text	I-Task
classification	E-Task
,	O
text	B-Task
similarity	E-Task
,	O
and	O
relevance	B-Task
ranking	E-Task
.	O

Similar	O
to	O
the	O
BERT	B-Method
model	E-Method
,	O
MT	B-Method
-	I-Method
DNN	E-Method
is	O
trained	O
in	O
two	O
stages	O
:	O
pre	B-Method
-	I-Method
training	E-Method
and	O
fine	B-Task
-	I-Task
tuning	E-Task
.	O

Unlike	O
BERT	S-Method
,	O
MT	B-Method
-	I-Method
DNN	E-Method
uses	O
MTL	S-Task
in	O
the	O
fine	B-Method
-	I-Method
tuning	I-Method
stage	E-Method
with	O
multiple	O
task	O
-	O
specific	O
layers	O
in	O
its	O
model	B-Method
architecture	E-Method
.	O

MT	B-Method
-	I-Method
DNN	E-Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
out	O
of	O
nine	O
NLU	S-Task
tasks	O
used	O
in	O
the	O
General	B-Task
Language	I-Task
Understanding	I-Task
Evaluation	E-Task
(	O
GLUE	S-Material
)	O
benchmark	O
wang2018glue	O
,	O
pushing	O
the	O
GLUE	S-Material
benchmark	O
score	O
to	O
82.2	O
%	O
,	O
amounting	O
to	O
1.8	O
%	O
absolute	O
improvement	O
over	O
BERT	S-Method
.	O

We	O
further	O
extend	O
the	O
superiority	O
of	O
MT	B-Method
-	I-Method
DNN	E-Method
to	O
the	O
SNLI	S-Material
and	O
SciTail	S-Material
tasks	O
.	O

The	O
representations	O
learned	O
by	O
MT	B-Method
-	I-Method
DNN	E-Method
allow	O
domain	B-Task
adaptation	E-Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	S-Method
representations	O
.	O

For	O
example	O
,	O
our	O
adapted	O
models	O
achieve	O
the	O
accuracy	S-Metric
of	O
91.1	O
%	O
on	O
SNLI	S-Material
and	O
94.1	O
%	O
on	O
SciTail	S-Material
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
by	O
1.0	O
%	O
and	O
5.8	O
%	O
,	O
respectively	O
.	O

Even	O
with	O
only	O
0.1	O
%	O
or	O
1.0	O
%	O
of	O
the	O
original	O
training	O
data	O
,	O
the	O
performance	O
of	O
MT	B-Method
-	I-Method
DNN	E-Method
on	O
both	O
SNLI	S-Material
and	O
SciTail	S-Material
datasets	O
is	O
fairly	O
good	O
and	O
much	O
better	O
than	O
many	O
existing	O
models	O
.	O

All	O
of	O
these	O
clearly	O
demonstrate	O
MT	B-Method
-	I-Method
DNN	E-Method
’s	O
exceptional	O
generalization	B-Method
capability	E-Method
via	O
multi	B-Method
-	I-Method
task	I-Method
learning	E-Method
.	O

section	O
:	O
Tasks	O
The	O
MT	B-Method
-	I-Method
DNN	E-Method
model	O
combines	O
four	O
types	O
of	O
NLU	S-Task
tasks	O
:	O
single	B-Task
-	I-Task
sentence	I-Task
classification	E-Task
,	O
pairwise	B-Task
text	I-Task
classification	E-Task
,	O
text	B-Task
similarity	I-Task
scoring	E-Task
,	O
and	O
relevance	B-Task
ranking	E-Task
.	O

For	O
concreteness	O
,	O
we	O
describe	O
them	O
using	O
the	O
NLU	S-Task
tasks	O
defined	O
in	O
the	O
GLUE	S-Material
benchmark	O
as	O
examples	O
.	O

paragraph	O
:	O
Single	O
-	O
Sentence	B-Task
Classification	E-Task
:	O
Given	O
a	O
sentence	O
,	O
the	O
model	O
labels	O
it	O
using	O
one	O
of	O
the	O
pre	O
-	O
defined	O
class	O
labels	O
.	O

For	O
example	O
,	O
the	O
CoLA	B-Task
task	E-Task
is	O
to	O
predict	O
whether	O
an	O
English	B-Material
sentence	E-Material
is	O
grammatically	O
plausible	O
.	O

The	O
SST	B-Material
-	I-Material
2	E-Material
task	O
is	O
to	O
determine	O
whether	O
the	O
sentiment	O
of	O
a	O
sentence	O
extracted	O
from	O
movie	O
reviews	O
is	O
positive	O
or	O
negative	O
.	O

paragraph	O
:	O
Text	B-Metric
Similarity	E-Metric
:	O
This	O
is	O
a	O
regression	B-Task
task	E-Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
predicts	O
a	O
real	O
-	O
value	O
score	O
indicating	O
the	O
semantic	O
similarity	O
of	O
the	O
two	O
sentences	O
.	O

STS	B-Method
-	I-Method
B	E-Method
is	O
the	O
only	O
example	O
of	O
the	O
task	O
in	O
GLUE	S-Material
.	O

paragraph	O
:	O
Pairwise	B-Task
Text	I-Task
Classification	E-Task
:	O
Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
determines	O
the	O
relationship	O
of	O
the	O
two	O
sentences	O
based	O
on	O
a	O
set	O
of	O
pre	O
-	O
defined	O
labels	O
.	O

For	O
example	O
,	O
both	O
RTE	S-Method
and	O
MNLI	S-Material
are	O
language	B-Task
inference	I-Task
tasks	E-Task
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
a	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
other	O
.	O

QQP	S-Material
and	O
MRPC	S-Material
are	O
paragraph	O
datasets	O
that	O
consist	O
of	O
sentence	O
pairs	O
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
.	O

paragraph	O
:	O
Relevance	B-Task
Ranking	E-Task
:	O
Given	O
a	O
query	O
and	O
a	O
list	O
of	O
candidate	O
answers	O
,	O
the	O
model	O
ranks	O
all	O
the	O
candidates	O
in	O
the	O
order	O
of	O
relevance	O
to	O
the	O
query	O
.	O

QNLI	S-Material
is	O
a	O
version	O
of	O
Stanford	B-Material
Question	I-Material
Answering	I-Material
Dataset	E-Material
rajpurkar2016squad	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
sentence	O
contains	O
the	O
correct	O
answer	O
to	O
a	O
given	O
query	O
.	O

Although	O
QNLI	S-Material
is	O
defined	O
as	O
a	O
binary	B-Task
classification	I-Task
task	E-Task
in	O
GLUE	S-Material
,	O
in	O
this	O
study	O
we	O
formulate	O
it	O
as	O
a	O
pairwise	B-Task
ranking	I-Task
task	E-Task
,	O
where	O
the	O
model	O
is	O
expected	O
to	O
rank	O
the	O
candidate	O
that	O
contains	O
the	O
correct	O
answer	O
higher	O
than	O
the	O
candidate	O
that	O
does	O
not	O
.	O

We	O
will	O
show	O
that	O
this	O
formulation	O
leads	O
to	O
a	O
significant	O
improvement	O
in	O
accuracy	S-Metric
over	O
binary	B-Task
classification	E-Task
.	O

section	O
:	O
The	O
Proposed	O
MT	B-Method
-	I-Method
DNN	E-Method
Model	O
The	O
architecture	O
of	O
the	O
MT	B-Method
-	I-Method
DNN	E-Method
model	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
lower	O
layers	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
represent	O
task	O
-	O
specific	O
outputs	O
.	O

The	O
input	O
,	O
which	O
is	O
a	O
word	O
sequence	O
(	O
either	O
a	O
sentence	O
or	O
a	O
pair	O
of	O
sentences	O
packed	O
together	O
)	O
is	O
first	O
represented	O
as	O
a	O
sequence	O
of	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
word	O
,	O
in	O
.	O

Then	O
the	O
transformer	B-Method
encoder	E-Method
captures	O
the	O
contextual	O
information	O
for	O
each	O
word	O
via	O
self	O
-	O
attention	O
,	O
and	O
generates	O
a	O
sequence	O
of	O
contextual	O
embeddings	O
in	O
.	O

This	O
is	O
the	O
shared	B-Method
semantic	I-Method
representation	E-Method
that	O
is	O
trained	O
by	O
our	O
multi	B-Task
-	I-Task
task	I-Task
objectives	E-Task
.	O

In	O
what	O
follows	O
,	O
we	O
elaborate	O
on	O
the	O
model	O
in	O
detail	O
.	O

paragraph	O
:	O
Lexicon	B-Method
Encoder	E-Method
(	O
)	O
:	O
The	O
input	O
is	O
a	O
sequence	O
of	O
tokens	O
of	O
length	O
.	O

Following	O
bert2018	O
,	O
the	O
first	O
token	O
is	O
always	O
the	O
[	O
CLS	O
]	O
token	O
.	O

If	O
is	O
packed	O
by	O
a	O
sentence	O
pair	O
,	O
we	O
separate	O
the	O
two	O
sentences	O
with	O
a	O
special	O
token	O
[	O
SEP	O
]	O
.	O

The	O
lexicon	B-Method
encoder	E-Method
maps	O
into	O
a	O
sequence	O
of	O
input	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
token	O
,	O
constructed	O
by	O
summing	O
the	O
corresponding	O
word	O
,	O
segment	O
,	O
and	O
positional	O
embeddings	O
.	O

paragraph	O
:	O
Transformer	B-Method
Encoder	E-Method
(	O
)	O
:	O
We	O
use	O
a	O
multi	B-Method
-	I-Method
layer	I-Method
bidirectional	I-Method
Transformer	I-Method
encoder	E-Method
vaswani2017attention	O
to	O
map	O
the	O
input	O
representation	O
vectors	O
(	O
)	O
into	O
a	O
sequence	O
of	O
contextual	O
embedding	O
vectors	O
.	O

This	O
is	O
the	O
shared	O
representation	O
across	O
different	O
tasks	O
.	O

Unlike	O
the	O
BERT	B-Method
model	E-Method
bert2018	O
that	O
learns	O
the	O
representation	O
via	O
pre	B-Method
-	I-Method
training	E-Method
and	O
adapts	O
it	O
to	O
each	O
individual	O
task	O
via	O
fine	B-Method
-	I-Method
tuning	E-Method
,	O
MT	B-Method
-	I-Method
DNN	E-Method
learns	O
the	O
representation	O
using	O
multi	O
-	O
task	O
objectives	O
.	O

paragraph	O
:	O
Single	B-Task
-	I-Task
Sentence	I-Task
Classification	E-Task
Output	O
:	O
Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
the	O
token	O
[	O
CLS	O
]	O
,	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	B-Method
representation	E-Method
of	O
input	O
sentence	O
.	O

Take	O
the	O
SST	B-Material
-	I-Material
2	E-Material
task	O
as	O
an	O
example	O
.	O

The	O
probability	O
that	O
is	O
labeled	O
as	O
class	O
(	O
i.e.	O
,	O
the	O
sentiment	O
)	O
is	O
predicted	O
by	O
a	O
logistic	B-Method
regression	E-Method
with	O
softmax	S-Method
:	O
where	O
is	O
the	O
task	O
-	O
specific	O
parameter	O
matrix	O
.	O

paragraph	O
:	O
Text	B-Metric
Similarity	E-Metric
Output	O
:	O
Take	O
the	O
STS	B-Task
-	I-Task
B	I-Task
task	E-Task
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
[	O
CLS	O
]	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	B-Method
representation	E-Method
of	O
the	O
input	O
sentence	O
pair	O
.	O

We	O
introduce	O
a	O
task	O
-	O
specific	O
parameter	O
vector	O
to	O
compute	O
the	O
similarity	B-Metric
score	E-Metric
as	O
:	O
where	O
is	O
a	O
sigmoid	B-Method
function	E-Method
that	O
maps	O
the	O
score	O
to	O
a	O
real	O
value	O
of	O
the	O
range	O
.	O

paragraph	O
:	O
Pairwise	B-Metric
Text	I-Metric
Classification	E-Metric
Output	O
:	O
Take	O
natural	B-Task
language	I-Task
inference	E-Task
(	O
NLI	S-Task
)	O
as	O
an	O
example	O
.	O

The	O
NLI	B-Task
task	E-Task
defined	O
here	O
involves	O
a	O
premise	O
of	O
words	O
and	O
a	O
hypothesis	O
of	O
words	O
,	O
and	O
aims	O
to	O
find	O
a	O
logical	O
relationship	O
between	O
and	O
.	O

The	O
design	O
of	O
the	O
output	B-Method
module	E-Method
follows	O
the	O
answer	B-Method
module	E-Method
of	O
the	O
stochastic	B-Method
answer	I-Method
network	E-Method
(	O
SAN	S-Method
)	O
liu2018san4nli	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	B-Method
NLI	I-Method
model	E-Method
.	O

SAN	S-Method
’s	O
answer	O
module	O
uses	O
multi	B-Method
-	I-Method
step	I-Method
reasoning	E-Method
.	O

Rather	O
than	O
directly	O
predicting	O
the	O
entailment	O
given	O
the	O
input	O
,	O
it	O
maintains	O
a	O
state	O
and	O
iteratively	O
refines	O
its	O
predictions	O
.	O

The	O
SAN	S-Method
answer	O
module	O
works	O
as	O
follows	O
.	O

We	O
first	O
construct	O
the	O
working	O
memory	O
of	O
premise	O
by	O
concatenating	O
the	O
contextual	O
embeddings	O
of	O
the	O
words	O
in	O
,	O
which	O
are	O
the	O
output	O
of	O
the	O
transformer	B-Method
encoder	E-Method
,	O
denoted	O
as	O
,	O
and	O
similarly	O
the	O
working	O
memory	O
of	O
hypothesis	S-Method
,	O
denoted	O
as	O
.	O

Then	O
,	O
we	O
perform	O
-	B-Method
step	I-Method
reasoning	E-Method
on	O
the	O
memory	O
to	O
output	O
the	O
relation	O
label	O
,	O
where	O
is	O
a	O
hyperparameter	O
.	O

At	O
the	O
beginning	O
,	O
the	O
initial	O
state	O
is	O
the	O
summary	O
of	O
:	O
,	O
where	O
.	O

At	O
time	O
step	O
in	O
the	O
range	O
of	O
,	O
the	O
state	O
is	O
defined	O
by	O
.	O

Here	O
,	O
is	O
computed	O
from	O
the	O
previous	O
state	O
and	O
memory	O
:	O
and	O
.	O

A	O
one	B-Method
-	I-Method
layer	I-Method
classifier	E-Method
is	O
used	O
to	O
determine	O
the	O
relation	O
at	O
each	O
step	O
:	O
At	O
last	O
,	O
we	O
utilize	O
all	O
of	O
the	O
outputs	O
by	O
averaging	O
the	O
scores	O
:	O
Each	O
is	O
a	O
probability	O
distribution	O
over	O
all	O
the	O
relations	O
.	O

During	O
training	S-Task
,	O
we	O
apply	O
stochastic	B-Method
prediction	I-Method
dropout	E-Method
liu2018san	O
before	O
the	O
above	O
averaging	B-Method
operation	E-Method
.	O

During	O
decoding	S-Task
,	O
we	O
average	O
all	O
outputs	O
to	O
improve	O
robustness	S-Metric
.	O

paragraph	O
:	O
Relevance	B-Metric
Ranking	E-Metric
Output	O
:	O
Take	O
QNLI	S-Material
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
vector	O
of	O
[	O
CLS	B-Method
]	E-Method
which	O
is	O
the	O
semantic	B-Method
representation	E-Method
of	O
a	O
pair	O
of	O
question	O
and	O
its	O
candidate	O
answer	O
.	O

We	O
compute	O
the	O
relevance	B-Metric
score	E-Metric
as	O
:	O
For	O
a	O
given	O
,	O
we	O
rank	O
all	O
of	O
its	O
candidate	O
answers	O
based	O
on	O
their	O
relevance	O
scores	O
computed	O
using	O
Equation	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
The	O
Training	O
Procedure	O
The	O
training	B-Method
procedure	E-Method
of	O
MT	B-Method
-	I-Method
DNN	E-Method
consists	O
of	O
two	O
stages	O
:	O
pretraining	S-Task
and	O
multi	B-Task
-	I-Task
task	I-Task
fine	I-Task
-	I-Task
tuning	E-Task
.	O

The	O
pretraining	O
stage	O
follows	O
that	O
of	O
the	O
BERT	S-Method
model	O
bert2018	O
.	O

The	O
parameters	O
of	O
the	O
lexicon	B-Method
encoder	E-Method
and	O
Transformer	B-Method
encoder	E-Method
are	O
learned	O
using	O
two	O
unsupervised	B-Task
prediction	I-Task
tasks	E-Task
:	O
masked	B-Task
language	I-Task
modeling	E-Task
and	O
next	B-Task
sentence	I-Task
prediction	E-Task
.	O

In	O
the	O
multi	B-Task
-	I-Task
task	I-Task
fine	I-Task
-	I-Task
tuning	I-Task
stage	E-Task
,	O
we	O
use	O
mini	B-Method
-	I-Method
batch	I-Method
based	I-Method
stochastic	I-Method
gradient	I-Method
descent	E-Method
(	O
SGD	S-Method
)	O
to	O
learn	O
the	O
parameters	O
of	O
our	O
model	O
(	O
i.e.	O
,	O
the	O
parameters	O
of	O
all	O
shared	O
layers	O
and	O
task	O
-	O
specific	O
layers	O
)	O
as	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

In	O
each	O
epoch	O
,	O
a	O
mini	O
-	O
batch	O
is	O
selected	O
(	O
e.g	O
.	O

,	O
among	O
all	O
9	O
GLUE	S-Material
tasks	O
)	O
,	O
and	O
the	O
model	O
is	O
updated	O
according	O
to	O
the	O
task	O
-	O
specific	O
objective	O
for	O
the	O
task	O
.	O

This	O
approximately	O
optimizes	O
the	O
sum	O
of	O
all	O
multi	O
-	O
task	O
objectives	O
.	O

[	O
ht	O
!	O
]	O
Initialize	O
model	O
parameters	O
randomly	O
.	O

Pre	O
-	O
train	O
the	O
shared	B-Method
layers	E-Method
(	O
i.e.	O
,	O
the	O
lexicon	B-Method
encoder	E-Method
and	O
the	O
transformer	B-Method
encoder	E-Method
)	O
.	O

Set	O
the	O
max	O
number	O
of	O
epoch	O
:	O
.	O

//	O
Prepare	O
the	O
data	O
for	O
T	O
tasks	O
.	O

in	O
Pack	O
the	O
dataset	O
into	O
mini	O
-	O
batch	O
:	O
.	O

in	O
1	O
.	O

Merge	O
all	O
the	O
datasets	O
:	O
2	O
.	O

Shuffle	S-Method
in	O
D	O
//	O
bt	O
is	O
a	O
mini	O
-	O
batch	O
of	O
task	O
t.	O
3	O
.	O

Compute	O
loss	S-Metric
:	O
Eq	O
.	O

[	O
reference	O
]	O
for	O
classification	B-Task
Eq	E-Task
.	O

[	O
reference	O
]	O
for	O
regression	B-Task
Eq	E-Task
.	O

[	O
reference	O
]	O
for	O
ranking	S-Task
4	O
.	O

Compute	O
gradient	O
:	O
5	O
.	O

Update	B-Method
model	E-Method
:	O
Training	O
a	O
MT	B-Method
-	I-Method
DNN	E-Method
model	O
.	O

For	O
the	O
classification	B-Task
tasks	E-Task
(	O
i.e.	O
,	O
single	B-Task
-	I-Task
sentence	I-Task
or	I-Task
pairwise	I-Task
text	I-Task
classification	E-Task
)	O
,	O
we	O
use	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
as	O
the	O
objective	O
:	O
where	O
is	O
the	O
binary	O
indicator	O
(	O
0	O
or	O
1	O
)	O
if	O
class	O
label	O
is	O
the	O
correct	O
classification	O
for	O
,	O
and	O
is	O
defined	O
by	O
e.g.	O
,	O
Equation	O
[	O
reference	O
]	O
or	O
[	O
reference	O
]	O
.	O

For	O
the	O
text	B-Task
similarity	I-Task
tasks	E-Task
,	O
such	O
as	O
STS	B-Task
-	I-Task
B	E-Task
,	O
where	O
each	O
sentence	O
pair	O
is	O
annotated	O
with	O
a	O
real	O
-	O
valued	O
score	O
,	O
we	O
use	O
the	O
mean	B-Metric
squared	I-Metric
error	E-Metric
as	O
the	O
objective	O
:	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
objective	O
for	O
the	O
relevance	B-Task
ranking	I-Task
tasks	E-Task
follows	O
the	O
pairwise	B-Method
learning	I-Method
-	I-Method
to	I-Method
-	I-Method
rank	I-Method
paradigm	I-Method
learning	E-Method
-	O
to	O
-	O
rank2005burges	O
,	O
huang2013dssm	O
.	O

Take	O
QNLI	S-Material
as	O
an	O
example	O
.	O

Given	O
a	O
query	O
,	O
we	O
obtain	O
a	O
list	O
of	O
candidate	O
answers	O
which	O
contains	O
a	O
positive	O
example	O
that	O
includes	O
the	O
correct	O
answer	O
,	O
and	O
negative	O
examples	O
.	O

We	O
then	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
positive	O
example	O
given	O
queries	O
across	O
the	O
training	O
data	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
and	O
is	O
a	O
tuning	O
factor	O
determined	O
on	O
held	O
-	O
out	O
data	O
.	O

In	O
our	O
experiment	O
,	O
we	O
simply	O
set	O
to	O
1	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
MT	B-Method
-	I-Method
DNN	E-Method
on	O
three	O
popular	O
NLU	S-Task
benchmarks	O
:	O
GLUE	S-Material
,	O
Stanford	B-Material
Natural	I-Material
Language	I-Material
Inference	E-Material
(	O
SNLI	S-Material
)	O
,	O
and	O
SciTail	S-Material
.	O

We	O
compare	O
MT	B-Method
-	I-Method
DNN	E-Method
with	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
including	O
BERT	S-Method
and	O
demonstrate	O
the	O
effectiveness	O
of	O
MTL	S-Task
for	O
model	B-Task
fine	I-Task
-	I-Task
tuning	E-Task
using	O
GLUE	S-Material
and	O
domain	B-Method
adaptation	E-Method
using	O
SNLI	S-Material
and	O
SciTail	S-Material
.	O

subsection	O
:	O
Datasets	O
This	O
section	O
briefly	O
describes	O
the	O
GLUE	S-Material
,	O
SNLI	S-Material
,	O
and	O
SciTail	B-Material
datasets	E-Material
,	O
as	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
GLUE	S-Material
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
NLU	S-Task
tasks	O
,	O
including	O
question	B-Task
answering	E-Task
,	O
sentiment	B-Task
analysis	E-Task
,	O
and	O
textual	B-Task
entailment	E-Task
;	O
it	O
is	O
considered	O
well	O
-	O
designed	O
for	O
evaluating	O
the	O
generalization	B-Metric
and	I-Metric
robustness	E-Metric
of	O
NLU	S-Task
models	O
.	O

Both	O
SNLI	S-Material
and	O
SciTail	S-Material
are	O
NLI	B-Task
tasks	E-Task
.	O

paragraph	O
:	O
CoLA	O
The	O
Corpus	B-Task
of	I-Task
Linguistic	I-Task
Acceptability	E-Task
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
âacceptableâ	O
or	O
not	O
.	O

It	O
uses	O
Matthews	B-Metric
correlation	I-Metric
coefficient	E-Metric
as	O
the	O
evaluation	B-Metric
metric	E-Metric
.	O

paragraph	O
:	O
SST	B-Material
-	I-Material
2	E-Material
The	O
Stanford	B-Material
Sentiment	I-Material
Treebank	E-Material
is	O
to	O
determine	O
the	O
sentiment	O
of	O
sentences	O
.	O

The	O
sentences	O
are	O
extracted	O
from	O
movie	O
reviews	O
with	O
human	O
annotations	O
of	O
their	O
sentiment	O
.	O

Accuracy	S-Metric
is	O
used	O
as	O
the	O
evaluation	B-Metric
metric	E-Metric
.	O

paragraph	O
:	O
STS	B-Material
-	I-Material
B	E-Material
The	O
Semantic	B-Task
Textual	I-Task
Similarity	I-Task
Benchmark	E-Task
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
collected	O
from	O
multiple	O
data	O
resources	O
including	O
news	O
headlines	O
,	O
video	O
,	O
and	O
image	O
captions	O
,	O
and	O
NLI	B-Material
data	E-Material
.	O

Each	O
pair	O
is	O
human	O
-	O
annotated	O
with	O
a	O
similarity	B-Metric
score	E-Metric
from	O
one	O
to	O
five	O
,	O
indicating	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
.	O

The	O
task	O
is	O
evaluated	O
using	O
two	O
metrics	O
:	O
the	O
Pearson	B-Metric
and	I-Metric
Spearman	I-Metric
correlation	I-Metric
coefficients	E-Metric
.	O

paragraph	O
:	O
QNLI	S-Material
This	O
is	O
derived	O
from	O
the	O
Stanford	B-Material
Question	I-Material
Answering	I-Material
Dataset	E-Material
rajpurkar2016squad	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	B-Task
classification	I-Task
task	E-Task
in	O
GLUE	S-Material
.	O

A	O
query	O
-	O
candidate	O
-	O
answer	O
tuple	O
is	O
labeled	O
as	O
positive	O
if	O
the	O
candidate	O
contains	O
the	O
correct	O
answer	O
to	O
the	O
query	O
and	O
negative	O
otherwise	O
.	O

In	O
this	O
study	O
,	O
however	O
,	O
we	O
formulate	O
QNLI	S-Material
as	O
a	O
relevance	B-Task
ranking	I-Task
task	E-Task
,	O
where	O
for	O
a	O
given	O
query	O
,	O
its	O
positive	O
candidate	O
answers	O
are	O
considered	O
more	O
relevant	O
,	O
and	O
thus	O
should	O
be	O
ranked	O
higher	O
than	O
its	O
negative	O
candidates	O
.	O

paragraph	O
:	O
QQP	S-Material
The	O
Quora	B-Material
Question	I-Material
Pairs	I-Material
dataset	E-Material
is	O
a	O
collection	O
of	O
question	O
pairs	O
extracted	O
from	O
the	O
community	B-Material
question	I-Material
-	I-Material
answering	I-Material
website	I-Material
Quora	E-Material
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
two	O
questions	O
are	O
semantically	O
equivalent	O
.	O

As	O
the	O
distribution	O
of	O
positive	O
and	O
negative	O
labels	O
is	O
unbalanced	O
,	O
both	O
accuracy	S-Metric
and	O
F1	B-Metric
score	E-Metric
are	O
used	O
as	O
evaluation	B-Metric
metrics	E-Metric
.	O

paragraph	O
:	O
MRPC	S-Material
The	O
Microsoft	B-Material
Research	I-Material
Paraphrase	I-Material
Corpus	E-Material
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
with	O
human	O
annotations	O
denoting	O
whether	O
a	O
sentence	O
pair	O
is	O
semantically	O
equivalent	O
to	O
the	O
other	O
in	O
the	O
pair	O
.	O

Similar	O
to	O
QQP	S-Material
,	O
both	O
accuracy	S-Metric
and	O
F1	B-Metric
score	E-Metric
are	O
used	O
as	O
evaluation	B-Metric
metrics	E-Metric
.	O

paragraph	O
:	O
MNLI	S-Material
Multi	B-Task
-	I-Task
Genre	I-Task
Natural	I-Task
Language	I-Task
Inference	E-Task
is	O
a	O
large	B-Task
-	I-Task
scale	I-Task
,	I-Task
crowd	I-Task
-	I-Task
sourced	I-Task
entailment	I-Task
classification	I-Task
task	E-Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
(	O
i.e.	O
,	O
a	O
premise	O
-	O
hypothesis	O
pair	O
)	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
hypothesis	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
premise	O
.	O

The	O
test	O
and	O
development	O
sets	O
are	O
split	O
into	O
in	O
-	O
domain	O
(	O
matched	S-Metric
)	O
and	O
cross	O
-	O
domain	O
(	O
mismatched	S-Metric
)	O
sets	O
.	O

The	O
evaluation	B-Metric
metric	E-Metric
is	O
accuracy	S-Metric
.	O

paragraph	O
:	O
RTE	O
The	O
Recognizing	B-Material
Textual	I-Material
Entailment	I-Material
dataset	E-Material
is	O
collected	O
from	O
a	O
series	O
of	O
annual	O
challenges	O
on	O
textual	B-Task
entailment	E-Task
.	O

The	O
task	O
is	O
similar	O
to	O
MNLI	S-Material
,	O
but	O
uses	O
only	O
two	O
labels	O
:	O
entailment	O
and	O
not_entailment	O
.	O

paragraph	O
:	O
WNLI	S-Material
The	O
Winograd	B-Material
NLI	E-Material
(	O
WNLI	S-Material
)	O
is	O
a	O
natural	B-Task
language	I-Task
inference	E-Task
dataset	O
derived	O
from	O
the	O
Winograd	B-Material
Schema	I-Material
dataset	E-Material
.	O

This	O
is	O
a	O
reading	B-Task
comprehension	I-Task
task	E-Task
.	O

The	O
goal	O
is	O
to	O
select	O
the	O
referent	O
of	O
a	O
pronoun	O
from	O
a	O
list	O
of	O
choices	O
in	O
a	O
given	O
sentence	O
which	O
contains	O
the	O
pronoun	O
.	O

paragraph	O
:	O
SNLI	S-Material
The	O
Stanford	B-Material
Natural	I-Material
Language	I-Material
Inference	E-Material
(	O
SNLI	S-Material
)	O
dataset	O
contains	O
570k	O
human	O
annotated	O
sentence	O
pairs	O
,	O
in	O
which	O
the	O
premises	O
are	O
drawn	O
from	O
the	O
captions	O
of	O
the	O
Flickr30	B-Material
corpus	E-Material
and	O
hypotheses	O
are	O
manually	O
annotated	O
.	O

This	O
is	O
the	O
most	O
widely	O
used	O
entailment	O
dataset	O
for	O
NLI	S-Task
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	B-Task
adaptation	E-Task
in	O
this	O
study	O
.	O

paragraph	O
:	O
SciTail	S-Material
This	O
is	O
a	O
textual	O
entailment	O
dataset	O
derived	O
from	O
a	O
science	B-Material
question	I-Material
answering	E-Material
(	O
SciQ	S-Material
)	O
dataset	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
given	O
premise	O
entails	O
a	O
given	O
hypothesis	O
.	O

In	O
contrast	O
to	O
other	O
entailment	O
datasets	O
mentioned	O
previously	O
,	O
the	O
hypotheses	O
in	O
SciTail	S-Material
are	O
created	O
from	O
science	O
questions	O
while	O
the	O
corresponding	O
answer	O
candidates	O
and	O
premises	O
come	O
from	O
relevant	O
web	O
sentences	O
retrieved	O
from	O
a	O
large	O
corpus	O
.	O

As	O
a	O
result	O
,	O
these	O
sentences	O
are	O
linguistically	O
challenging	O
and	O
the	O
lexical	O
similarity	O
of	O
premise	O
and	O
hypothesis	O
is	O
often	O
high	O
,	O
thus	O
making	O
SciTail	S-Material
particularly	O
difficult	O
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	B-Task
adaptation	E-Task
in	O
this	O
study	O
.	O

subsection	O
:	O
Implementation	O
details	O
Our	O
implementation	O
of	O
MT	B-Method
-	I-Method
DNN	E-Method
is	O
based	O
on	O
the	O
PyTorch	O
implementation	O
of	O
BERT	S-Method
.	O

We	O
used	O
Adamax	S-Method
as	O
our	O
optimizer	S-Method
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
5e	O
-	O
5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

The	O
maximum	O
number	O
of	O
epochs	O
was	O
set	O
to	O
5	O
.	O

A	O
linear	B-Method
learning	I-Method
rate	I-Method
decay	I-Method
schedule	E-Method
with	O
warm	O
-	O
up	O
over	O
0.1	O
was	O
used	O
,	O
unless	O
stated	O
otherwise	O
.	O

Following	O
,	O
we	O
set	O
the	O
number	O
of	O
steps	O
to	O
5	O
with	O
a	O
dropout	B-Metric
rate	E-Metric
of	O
0.1	O
.	O

To	O
avoid	O
the	O
exploding	B-Task
gradient	I-Task
problem	E-Task
,	O
we	O
clipped	O
the	O
gradient	O
norm	O
within	O
1	O
.	O

All	O
the	O
texts	O
were	O
tokenized	O
using	O
wordpieces	O
,	O
and	O
were	O
chopped	O
to	O
spans	O
no	O
longer	O
than	O
512	O
tokens	O
.	O

subsection	O
:	O
GLUE	S-Material
Results	O
The	O
test	O
results	O
on	O
GLUE	S-Material
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

MT	B-Method
-	I-Method
DNN	E-Method
outperforms	O
all	O
existing	O
systems	O
on	O
all	O
tasks	O
,	O
except	O
WNLI	S-Material
,	O
creating	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
GLUE	S-Material
tasks	O
and	O
pushing	O
the	O
benchmark	O
to	O
82.2	O
%	O
,	O
which	O
amounts	O
to	O
1.8	O
%	O
absolution	O
improvement	O
over	O
BERT	S-Method
LARGE	O
.	O

Since	O
MT	B-Method
-	I-Method
DNN	E-Method
uses	O
BERT	S-Method
LARGE	O
for	O
its	O
shared	O
layers	O
,	O
the	O
gain	O
is	O
solely	O
attributed	O
to	O
the	O
use	O
of	O
MTL	S-Task
in	O
fine	B-Task
-	I-Task
tuning	E-Task
.	O

MTL	S-Task
is	O
particularly	O
useful	O
for	O
the	O
tasks	O
with	O
little	O
in	O
-	O
domain	O
training	O
data	O
.	O

As	O
we	O
observe	O
in	O
the	O
table	O
,	O
on	O
the	O
same	O
type	O
of	O
tasks	O
,	O
the	O
improvements	O
over	O
BERT	S-Method
are	O
much	O
more	O
substantial	O
for	O
the	O
tasks	O
with	O
less	O
in	O
-	O
domain	O
training	O
data	O
e.g.	O
,	O
the	O
two	O
NLI	B-Task
tasks	E-Task
:	O
RTE	S-Method
vs.	O
MNLI	S-Material
,	O
and	O
the	O
two	O
paraphrase	B-Method
tasks	E-Method
:	O
MRPC	S-Material
vs.	O
QQP	S-Material
.	O

The	O
gain	O
of	O
MT	B-Method
-	I-Method
DNN	E-Method
is	O
also	O
attributed	O
to	O
its	O
flexible	O
modeling	B-Method
framework	E-Method
which	O
allows	O
us	O
to	O
incorporate	O
the	O
task	B-Method
-	I-Method
specific	I-Method
model	I-Method
structures	E-Method
and	O
training	B-Method
methods	E-Method
which	O
have	O
been	O
developed	O
in	O
the	O
single	B-Task
-	I-Task
task	I-Task
setting	E-Task
,	O
effectively	O
leveraging	O
the	O
existing	O
body	O
of	O
research	O
.	O

Two	O
such	O
examples	O
use	O
the	O
SAN	S-Method
answer	O
module	O
for	O
the	O
pairwise	B-Method
text	I-Method
classification	I-Method
output	I-Method
module	E-Method
,	O
and	O
the	O
pairwise	B-Metric
ranking	I-Metric
loss	E-Metric
for	O
the	O
QNLI	S-Material
task	O
which	O
by	O
design	O
is	O
a	O
binary	B-Task
classification	I-Task
problem	E-Task
in	O
GLUE	S-Material
.	O

To	O
investigate	O
the	O
relative	O
contributions	O
of	O
the	O
above	O
two	O
modeling	O
design	O
choices	O
,	O
we	O
implement	O
different	O
versions	O
of	O
MT	O
-	O
DNNs	S-Method
and	O
compare	O
their	O
performance	O
on	O
the	O
development	O
sets	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

BERT	S-Method
is	O
the	O
base	O
BERT	B-Method
model	E-Method
released	O
by	O
the	O
authors	O
,	O
which	O
we	O
used	O
as	O
a	O
baseline	O
.	O

We	O
fine	O
-	O
tuned	O
the	O
model	O
for	O
each	O
single	O
task	O
.	O

MT	B-Method
-	I-Method
DNN	E-Method
is	O
the	O
proposed	O
model	O
described	O
in	O
Section	O
[	O
reference	O
]	O
using	O
the	O
pre	O
-	O
trained	O
BERT	S-Method
BASE	O
as	O
its	O
shared	O
layers	O
.	O

We	O
then	O
fine	O
-	O
tuned	O
the	O
model	O
using	O
MTL	S-Task
on	O
all	O
GLUE	S-Material
tasks	O
.	O

Comparing	O
MT	B-Method
-	I-Method
DNN	E-Method
vs.	O
BERT	S-Method
BASE	O
,	O
we	O
see	O
that	O
the	O
results	O
on	O
dev	O
sets	O
are	O
consistent	O
with	O
the	O
GLUE	S-Material
test	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

ST	B-Method
-	I-Method
DNN	E-Method
,	O
standing	O
for	O
Single	B-Task
-	I-Task
Task	I-Task
DNN	E-Task
,	O
uses	O
the	O
same	O
model	B-Method
architecture	E-Method
as	O
MT	B-Method
-	I-Method
DNN	E-Method
.	O

But	O
,	O
instead	O
of	O
fine	O
-	O
tuning	O
one	O
model	O
for	O
all	O
tasks	O
using	O
MTL	S-Task
,	O
we	O
create	O
multiple	O
ST	O
-	O
DNNs	S-Method
,	O
one	O
for	O
each	O
task	O
using	O
only	O
its	O
in	O
-	O
domain	O
data	O
for	O
fine	B-Task
-	I-Task
tuning	E-Task
.	O

Thus	O
,	O
for	O
pairwise	B-Task
text	I-Task
classification	I-Task
tasks	E-Task
,	O
the	O
only	O
difference	O
between	O
their	O
ST	O
-	O
DNNs	S-Method
and	O
BERT	S-Method
models	O
is	O
the	O
design	O
of	O
the	O
task	B-Method
-	I-Method
specific	I-Method
output	I-Method
module	E-Method
.	O

The	O
results	O
show	O
that	O
on	O
three	O
out	O
of	O
four	O
tasks	O
(	O
MNLI	S-Material
,	O
QQP	S-Material
and	O
MRPC	S-Material
)	O
ST	O
-	O
DNNs	S-Method
outperform	O
their	O
BERT	S-Method
counterparts	O
,	O
justifying	O
the	O
effectiveness	O
of	O
the	O
SAN	S-Method
answer	O
module	O
.	O

We	O
also	O
compare	O
the	O
results	O
of	O
ST	B-Method
-	I-Method
DNN	E-Method
and	O
BERT	S-Method
on	O
QNLI	S-Material
.	O

While	O
ST	B-Method
-	I-Method
DNN	E-Method
is	O
fine	O
-	O
tuned	O
using	O
the	O
pairwise	B-Metric
ranking	I-Metric
loss	E-Metric
,	O
BERT	S-Method
views	O
QNLI	S-Material
as	O
binary	B-Task
classification	E-Task
and	O
is	O
fine	O
-	O
tuned	O
using	O
the	O
cross	B-Metric
entropy	I-Metric
loss	E-Metric
.	O

That	O
ST	B-Method
-	I-Method
DNN	E-Method
significantly	O
outperforms	O
BERT	S-Method
demonstrates	O
clearly	O
the	O
importance	O
of	O
problem	B-Task
formulation	E-Task
.	O

subsection	O
:	O
SNLI	S-Material
and	O
SciTail	S-Material
Results	O
In	O
Table	O
4	O
,	O
we	O
compare	O
our	O
adapted	O
models	O
,	O
using	O
all	O
in	O
-	O
domain	O
training	O
samples	O
,	O
against	O
several	O
strong	O
baselines	O
including	O
the	O
best	O
results	O
reported	O
in	O
the	O
leaderboards	O
.	O

We	O
see	O
that	O
MT	B-Method
-	I-Method
DNN	E-Method
generates	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
both	O
datasets	O
,	O
pushing	O
the	O
benchmarks	O
to	O
91.1	O
%	O
on	O
SNLI	S-Material
(	O
1.0	O
%	O
absolute	O
improvement	O
)	O
and	O
94.1	O
%	O
on	O
SciTail	S-Material
(	O
5.8	O
%	O
absolute	O
improvement	O
)	O
,	O
respectively	O
.	O

subsection	O
:	O
Domain	B-Task
Adaptation	E-Task
Results	O
One	O
of	O
the	O
most	O
important	O
criteria	O
for	O
building	O
practical	B-Method
systems	E-Method
is	O
fast	B-Task
adaptation	E-Task
to	O
new	O
tasks	O
and	O
domains	O
.	O

This	O
is	O
because	O
it	O
is	O
prohibitively	O
expensive	O
to	O
collect	O
labeled	O
training	O
data	O
for	O
new	O
domains	O
or	O
tasks	O
.	O

Very	O
often	O
,	O
we	O
only	O
have	O
very	O
small	O
training	O
data	O
or	O
even	O
no	O
training	O
data	O
.	O

To	O
evaluate	O
the	O
models	O
using	O
the	O
above	O
criterion	O
,	O
we	O
perform	O
domain	B-Task
adaptation	E-Task
experiments	O
on	O
two	O
NLI	B-Task
tasks	E-Task
,	O
SNLI	S-Material
and	O
SciTail	S-Material
,	O
using	O
the	O
following	O
procedure	O
:	O
fine	O
-	O
tune	O
the	O
MT	B-Method
-	I-Method
DNN	E-Method
model	O
on	O
eight	O
GLUE	S-Material
tasks	O
,	O
excluding	O
WNLI	S-Material
;	O
create	O
for	O
each	O
new	O
task	O
(	O
SNLI	S-Material
or	O
SciTail	S-Material
)	O
a	O
task	B-Method
-	I-Method
specific	I-Method
model	E-Method
,	O
by	O
adapting	O
the	O
trained	O
MT	B-Method
-	I-Method
DNN	E-Method
using	O
task	O
-	O
specific	O
training	O
data	O
;	O
evaluate	O
the	O
models	O
using	O
task	O
-	O
specific	O
test	O
data	O
.	O

We	O
denote	O
the	O
two	O
task	B-Method
-	I-Method
specific	I-Method
models	E-Method
as	O
MT	B-Method
-	I-Method
DNN	E-Method
.	O

For	O
comparison	O
,	O
we	O
also	O
perform	O
the	O
same	O
adaptation	B-Method
procedure	E-Method
to	O
the	O
pre	O
-	O
trained	O
BERT	B-Method
model	E-Method
,	O
creating	O
two	O
task	O
-	O
specific	O
BERT	S-Method
models	O
for	O
SNLI	S-Material
and	O
SciTail	S-Material
,	O
respectively	O
,	O
denoted	O
as	O
BERT	S-Method
.	O

We	O
split	O
the	O
training	O
data	O
of	O
SNLI	S-Material
and	O
SciTail	S-Material
,	O
and	O
randomly	O
sample	O
0.1	O
%	O
,	O
1	O
%	O
,	O
10	O
%	O
and	O
100	O
%	O
of	O
its	O
training	O
data	O
.	O

As	O
a	O
result	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SciTail	S-Material
,	O
which	O
includes	O
23	O
,	O
235	O
,	O
2.3k	O
and	O
23.5k	O
training	O
samples	O
.	O

Similarly	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SNLI	S-Material
,	O
which	O
includes	O
549	O
,	O
5.5k	O
,	O
54.9k	O
and	O
549.3k	O
training	O
samples	O
.	O

Results	O
on	O
different	O
amounts	O
of	O
training	O
data	O
of	O
SNLI	S-Material
and	O
SciTail	S-Material
are	O
reported	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

We	O
observe	O
that	O
our	O
model	O
pre	O
-	O
trained	O
on	O
GLUE	S-Material
via	O
multi	B-Method
-	I-Method
task	I-Method
learning	E-Method
outplays	O
the	O
BERT	S-Method
baseline	O
consistently	O
.	O

The	O
fewer	O
the	O
training	O
data	O
used	O
,	O
the	O
larger	O
improvement	O
MT	B-Method
-	I-Method
DNN	E-Method
demonstrates	O
over	O
BERT	S-Method
.	O

For	O
example	O
,	O
with	O
only	O
0.1	O
%	O
(	O
23	O
samples	O
)	O
of	O
the	O
SNLI	S-Material
training	O
data	O
,	O
MT	B-Method
-	I-Method
DNN	E-Method
achieves	O
82.1	O
%	O
in	O
accuracy	S-Metric
while	O
BERT	S-Method
’s	O
accuracy	S-Metric
is	O
52.5	O
%	O
;	O
with	O
1	O
%	O
of	O
the	O
training	O
data	O
,	O
the	O
accuracy	S-Metric
of	O
our	O
model	O
is	O
85.2	O
%	O
and	O
BERT	S-Method
is	O
78.1	O
%	O
.	O

We	O
observe	O
similar	O
results	O
on	O
SciTail	S-Material
.	O

The	O
results	O
indicate	O
that	O
the	O
representations	O
learned	O
by	O
MT	B-Method
-	I-Method
DNN	E-Method
are	O
more	O
effective	O
for	O
domain	B-Task
adaptation	E-Task
than	O
that	O
of	O
BERT	S-Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
proposed	O
a	O
model	O
called	O
MT	B-Method
-	I-Method
DNN	E-Method
to	O
combine	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
and	O
language	B-Task
model	I-Task
pre	I-Task
-	I-Task
training	E-Task
for	O
language	B-Task
representation	I-Task
learning	E-Task
.	O

MT	B-Method
-	I-Method
DNN	E-Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	S-Task
tasks	O
across	O
three	O
popular	O
benchmarks	O
:	O
SNLI	S-Material
,	O
SciTail	S-Material
,	O
and	O
GLUE	S-Material
.	O

MT	B-Method
-	I-Method
DNN	E-Method
also	O
demonstrates	O
an	O
exceptional	O
generalization	B-Metric
capability	E-Metric
in	O
domain	B-Task
adaptation	E-Task
experiments	O
.	O

There	O
are	O
many	O
future	O
areas	O
to	O
explore	O
to	O
improve	O
MT	B-Method
-	I-Method
DNN	E-Method
,	O
including	O
a	O
deeper	O
understanding	O
of	O
model	B-Task
structure	I-Task
sharing	E-Task
in	O
MTL	S-Task
,	O
a	O
more	O
effective	O
training	B-Method
method	E-Method
that	O
leverages	O
relatedness	O
among	O
multiple	O
tasks	O
,	O
and	O
ways	O
of	O
incorporating	O
the	O
linguistic	O
structure	O
of	O
text	O
in	O
a	O
more	O
explicit	O
and	O
controllable	O
manner	O
.	O

section	O
:	O
Acknowledgements	O
We	O
would	O
like	O
to	O
thanks	O
Jade	O
Huang	O
from	O
Microsoft	O
for	O
her	O
generous	O
help	O
on	O
this	O
work	O
.	O

bibliography	O
:	O
References	O
Knowledge	B-Method
graphs	E-Method
are	O
structured	O
representations	O
of	O
real	O
world	O
facts	O
.	O

However	O
,	O
they	O
typically	O
contain	O
only	O
a	O
small	O
subset	O
of	O
all	O
possible	O
facts	O
.	O

Link	B-Task
prediction	E-Task
is	O
a	O
task	O
of	O
inferring	B-Task
missing	I-Task
facts	E-Task
based	O
on	O
existing	O
ones	O
.	O

We	O
propose	O
TuckER	S-Method
,	O
a	O
relatively	O
simple	O
but	O
powerful	O
linear	B-Method
model	E-Method
based	O
on	O
Tucker	B-Method
decomposition	E-Method
of	O
the	O
binary	B-Method
tensor	I-Method
representation	I-Method
of	I-Method
knowledge	I-Method
graph	I-Method
triples	E-Method
.	O

TuckER	S-Method
outperforms	O
all	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
across	O
standard	O
link	O
prediction	O
datasets	O
.	O

We	O
prove	O
that	O
TuckER	S-Method
is	O
a	O
fully	B-Method
expressive	I-Method
model	E-Method
,	O
deriving	O
the	O
bound	O
on	O
its	O
entity	B-Metric
and	I-Metric
relation	I-Metric
embedding	I-Metric
dimensionality	E-Metric
for	O
full	B-Task
expressiveness	E-Task
which	O
is	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE.	O
We	O
further	O
show	O
that	O
several	O
previously	O
introduced	O
linear	B-Method
models	E-Method
can	O
be	O
viewed	O
as	O
special	O
cases	O
of	O
TuckER	S-Method
.	O

TuckER	S-Method
:	O
TensorFactorizationforKnowledgeGraphCompletion	O
section	O
:	O
Introduction	O
Vast	O
amounts	O
of	O
information	O
available	O
in	O
the	O
world	O
can	O
be	O
represented	O
succinctly	O
as	O
entities	O
and	O
relations	O
between	O
them	O
.	O

Knowledge	B-Method
graphs	E-Method
are	O
large	O
,	O
graph	O
-	O
structured	O
databases	O
which	O
store	O
facts	O
in	O
triple	O
form	O
,	O
with	O
and	O
representing	O
subject	O
and	O
object	O
entities	O
and	O
a	O
relation	O
between	O
them	O
.	O

Knowledge	B-Method
graphs	E-Method
are	O
used	O
for	O
a	O
wide	O
range	O
of	O
natural	B-Task
language	I-Task
processing	E-Task
and	O
information	B-Task
extraction	I-Task
tasks	E-Task
.	O

However	O
,	O
far	O
from	O
all	O
available	O
information	O
is	O
stored	O
in	O
existing	O
knowledge	O
graphs	O
and	O
manually	O
adding	O
new	O
information	O
is	O
costly	O
,	O
which	O
creates	O
the	O
need	O
for	O
algorithms	O
that	O
are	O
able	O
to	O
automatically	O
infer	O
missing	O
facts	O
based	O
on	O
existing	O
ones	O
.	O

Knowledge	B-Task
graphs	E-Task
can	O
be	O
represented	O
by	O
a	O
third	B-Method
-	I-Method
order	I-Method
binary	I-Method
tensor	E-Method
,	O
where	O
each	O
element	O
corresponds	O
to	O
a	O
triple	O
,	O
1	O
indicating	O
a	O
true	O
fact	O
and	O
0	O
indicating	O
the	O
unknown	O
(	O
either	O
a	O
false	O
or	O
a	O
missing	O
fact	O
)	O
.	O

One	O
of	O
the	O
most	O
important	O
tasks	O
in	O
relational	B-Task
machine	I-Task
learning	E-Task
is	O
link	B-Task
prediction	E-Task
:	O
predicting	O
whether	O
two	O
entities	O
are	O
related	O
,	O
based	O
on	O
known	O
relations	O
already	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O

The	O
task	O
of	O
link	B-Task
prediction	E-Task
is	O
thus	O
to	O
infer	O
which	O
of	O
the	O
0	O
entries	O
in	O
the	O
tensor	O
are	O
indeed	O
false	O
,	O
and	O
which	O
are	O
missing	O
but	O
actually	O
true	O
.	O

A	O
large	O
number	O
of	O
approaches	O
to	O
link	B-Task
prediction	E-Task
so	O
far	O
have	O
been	O
linear	O
,	O
based	O
on	O
various	O
methods	O
of	O
factorizing	B-Method
the	I-Method
third	I-Method
-	I-Method
order	I-Method
binary	I-Method
tensor	E-Method
.	O

Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
using	O
non	B-Method
-	I-Method
linear	I-Method
convolutional	I-Method
models	E-Method
.	O

Despite	O
achieving	O
very	O
good	O
performance	O
,	O
the	O
fundamental	O
problem	O
with	O
deep	B-Method
,	I-Method
non	I-Method
-	I-Method
linear	I-Method
models	E-Method
is	O
that	O
they	O
are	O
non	O
-	O
transparent	O
and	O
poorly	O
understood	O
,	O
as	O
opposed	O
to	O
more	O
mathematically	O
principled	O
and	O
widely	O
studied	O
tensor	B-Method
decomposition	I-Method
models	E-Method
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
TuckER	S-Method
(	O
E	O
stands	O
for	O
entities	O
,	O
R	O
for	O
relations	O
)	O
,	O
a	O
simple	O
linear	B-Method
model	E-Method
for	O
link	B-Task
prediction	E-Task
in	O
knowledge	B-Task
graphs	E-Task
,	O
based	O
on	O
Tucker	B-Method
decomposition	E-Method
of	O
the	O
third	B-Method
-	I-Method
order	I-Method
binary	I-Method
tensor	I-Method
of	I-Method
triples	E-Method
.	O

Tucker	B-Method
decomposition	E-Method
factorizes	O
a	O
tensor	O
into	O
a	O
core	O
tensor	O
multiplied	O
by	O
a	O
matrix	O
along	O
each	O
mode	O
.	O

It	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
higher	B-Method
-	I-Method
order	I-Method
singular	I-Method
value	I-Method
decomposition	E-Method
(	O
HOSVD	B-Method
)	E-Method
in	O
the	O
special	O
case	O
where	O
matrices	O
are	O
orthogonal	O
and	O
the	O
core	O
tensor	O
is	O
“	O
all	O
-	O
orthogonal	O
”	O
.	O

In	O
our	O
case	O
,	O
rows	O
of	O
the	O
three	O
matrices	O
contain	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
while	O
entries	O
of	O
the	O
core	O
tensor	O
determine	O
the	O
level	O
of	O
interaction	O
between	O
them	O
.	O

Further	O
,	O
subject	O
and	O
object	O
entity	O
embedding	O
matrices	O
are	O
assumed	O
equivalent	O
,	O
i.e.	O
we	O
make	O
no	O
distinction	O
between	O
the	O
embeddings	O
of	O
an	O
entity	O
depending	O
on	O
whether	O
it	O
appears	O
as	O
a	O
subject	O
or	O
as	O
an	O
object	O
in	O
a	O
particular	O
triple	O
.	O

Given	O
that	O
knowledge	O
graphs	O
contain	O
several	O
relation	O
types	O
(	O
symmetric	O
,	O
asymmetric	O
,	O
transitive	O
,	O
etc	O
.	O

)	O
,	O
it	O
is	O
important	O
for	O
a	O
link	B-Method
prediction	I-Method
model	E-Method
to	O
have	O
enough	O
expressive	O
power	O
to	O
accurately	O
represent	O
all	O
of	O
them	O
.	O

We	O
thus	O
show	O
that	O
TuckER	S-Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
any	O
ground	O
truth	O
over	O
the	O
triples	O
,	O
there	O
exists	O
an	O
assignment	O
of	O
values	O
to	O
the	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separates	O
the	O
true	O
triples	O
from	O
false	O
ones	O
.	O

We	O
also	O
derive	O
a	O
bound	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
that	O
guarantees	O
full	O
expressiveness	O
,	O
finding	O
it	O
to	O
be	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
.	O

This	O
enables	O
TuckER	S-Method
to	O
achieve	O
better	O
results	O
with	O
much	O
smaller	O
embedding	B-Metric
sizes	E-Metric
than	O
needed	O
by	O
those	O
models	O
,	O
important	O
for	O
efficiency	O
in	O
downstream	B-Task
tasks	E-Task
.	O

Finally	O
,	O
we	O
show	O
that	O
several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
linear	B-Method
models	E-Method
,	O
RESCAL	S-Method
,	O
DistMult	S-Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
are	O
special	O
cases	O
of	O
TuckER	S-Method
.	O

In	O
summary	O
,	O
the	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
proposing	O
TuckER	S-Method
,	O
a	O
new	O
linear	B-Method
model	E-Method
for	O
link	B-Task
prediction	I-Task
in	I-Task
knowledge	I-Task
graphs	E-Task
,	O
that	O
is	O
simple	O
,	O
expressive	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
all	O
standard	O
datasets	O
;	O
proving	O
that	O
TuckER	S-Method
is	O
fully	O
expressive	O
and	O
deriving	O
a	O
bound	O
on	O
the	O
entity	B-Metric
and	I-Metric
relation	I-Metric
embedding	I-Metric
dimensionality	E-Metric
for	O
full	O
expressiveness	O
which	O
is	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
;	O
and	O
showing	O
that	O
TuckER	S-Method
subsumes	O
several	O
previously	O
proposed	O
tensor	B-Method
factorization	I-Method
approaches	E-Method
to	O
link	B-Task
prediction	E-Task
,	O
i.e.	O
that	O
RESCAL	O
,	O
DistMult	O
,	O
ComplEx	S-Method
and	O
SimplE	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O

section	O
:	O
Related	O
Work	O
Several	O
linear	B-Method
models	E-Method
for	O
link	B-Task
prediction	E-Task
have	O
previously	O
been	O
proposed	O
:	O
RESCAL	O
An	O
early	O
linear	B-Method
model	E-Method
,	O
RESCAL	S-Method
,	O
optimizes	O
a	O
scoring	B-Method
function	E-Method
containing	O
a	O
bilinear	O
product	O
between	O
vector	B-Method
embeddings	E-Method
for	O
each	O
subject	O
and	O
object	O
entity	O
and	O
a	O
full	O
rank	O
matrix	O
for	O
each	O
relation	O
.	O

Although	O
a	O
very	O
expressive	O
and	O
powerful	O
model	O
,	O
RESCAL	S-Method
is	O
prone	O
to	O
overfitting	O
due	O
to	O
its	O
large	O
number	O
of	O
parameters	O
,	O
which	O
increases	O
quadratically	O
in	O
the	O
embedding	O
dimension	O
with	O
the	O
number	O
of	O
relations	O
in	O
a	O
knowledge	O
graph	O
.	O

DistMult	B-Method
DistMult	E-Method
is	O
a	O
special	O
case	O
of	O
RESCAL	O
with	O
a	O
diagonal	O
matrix	O
per	O
relation	O
,	O
so	O
the	O
number	O
of	O
parameters	O
of	O
DistMult	S-Method
grows	O
linearly	O
with	O
respect	O
to	O
the	O
embedding	O
dimension	O
,	O
reducing	O
overfitting	O
.	O

However	O
,	O
the	O
linear	B-Method
transformation	E-Method
performed	O
on	O
subject	O
entity	O
embedding	O
vectors	O
in	O
DistMult	S-Method
is	O
limited	O
to	O
a	O
stretch	O
.	O

Given	O
the	O
equivalence	O
of	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
,	O
third	O
-	O
order	O
binary	O
tensor	O
learned	O
by	O
DistMult	S-Method
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
and	O
thus	O
DistMult	S-Method
can	O
not	O
model	O
asymmetric	O
relations	O
.	O

ComplEx	B-Method
ComplEx	E-Method
extends	O
DistMult	S-Method
to	O
the	O
complex	O
domain	O
.	O

Even	O
though	O
each	O
relation	O
matrix	O
of	O
ComplEx	O
is	O
still	O
diagonal	O
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
no	O
longer	O
equivalent	O
,	O
but	O
complex	O
conjugates	O
,	O
which	O
introduces	O
asymmetry	O
into	O
the	O
tensor	B-Method
decomposition	E-Method
and	O
thus	O
enables	O
ComplEx	S-Method
to	O
model	O
asymmetric	O
relations	O
.	O

SimplE	B-Method
SimplE	E-Method
is	O
a	O
linear	B-Method
model	E-Method
based	O
on	O
Canonical	B-Method
Polyadic	I-Method
(	I-Method
CP	I-Method
)	I-Method
decomposition	E-Method
.	O

In	O
CP	B-Task
decomposition	E-Task
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
independent	O
(	O
note	O
that	O
DistMult	S-Method
is	O
a	O
special	O
case	O
of	O
CP	O
,	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
equivalent	O
)	O
.	O

SimplE	B-Method
’s	I-Method
scoring	I-Method
function	E-Method
alters	O
CP	O
to	O
make	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
dependent	O
on	O
each	O
other	O
,	O
i.e.	O
it	O
computes	O
the	O
average	O
of	O
two	O
terms	O
,	O
first	O
of	O
which	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	B-Method
embedding	E-Method
of	O
the	O
subject	O
entity	O
,	O
relation	B-Method
embedding	E-Method
and	O
tail	O
embedding	O
of	O
the	O
object	O
entity	O
and	O
the	O
second	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	O
embedding	O
of	O
the	O
object	O
entity	O
,	O
inverse	B-Method
relation	I-Method
embedding	E-Method
and	O
tail	O
embedding	O
of	O
the	O
subject	O
entity	O
.	O

Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
with	O
non	B-Method
-	I-Method
linear	I-Method
models	E-Method
:	O
ConvE	B-Method
ConvE	E-Method
is	O
the	O
first	O
non	B-Method
-	I-Method
linear	I-Method
model	E-Method
that	O
significantly	O
outperformed	O
the	O
preceding	O
linear	B-Method
models	E-Method
.	O

In	O
ConvE	O
,	O
a	O
global	B-Method
2D	I-Method
convolution	I-Method
operation	E-Method
is	O
performed	O
on	O
the	O
subject	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
after	O
they	O
are	O
reshaped	O
to	O
matrices	O
and	O
concatenated	O
.	O

The	O
obtained	O
feature	O
maps	O
are	O
flattened	O
,	O
transformed	O
through	O
a	O
fully	B-Method
connected	I-Method
layer	E-Method
,	O
and	O
the	O
inner	O
product	O
is	O
taken	O
with	O
all	O
object	O
entity	O
vectors	O
to	O
generate	O
a	O
score	O
for	O
each	O
triple	O
.	O

Whilst	O
results	O
achieved	O
by	O
ConvE	S-Method
are	O
impressive	O
,	O
its	O
reshaping	O
and	O
concatenating	B-Method
of	I-Method
vectors	E-Method
as	O
well	O
as	O
using	O
2D	B-Method
convolution	E-Method
on	O
word	B-Method
embeddings	E-Method
is	O
unintuitive	O
.	O

HypER	O
HypER	S-Method
is	O
a	O
simplified	B-Method
convolutional	I-Method
model	E-Method
,	O
that	O
uses	O
a	O
hypernetwork	S-Method
to	O
generate	O
1D	B-Method
convolutional	I-Method
filters	E-Method
for	O
each	O
relation	O
,	O
extracting	O
relation	O
-	O
specific	O
features	O
from	O
subject	O
entity	O
embeddings	O
.	O

The	O
authors	O
show	O
that	O
convolution	S-Method
is	O
a	O
way	O
of	O
introducing	O
sparsity	B-Task
and	I-Task
parameter	I-Task
tying	E-Task
and	O
that	O
HypER	S-Method
can	O
be	O
understood	O
in	O
terms	O
of	O
tensor	B-Method
factorization	E-Method
up	O
to	O
a	O
non	O
-	O
linearity	O
,	O
thus	O
placing	O
HypER	S-Method
closer	O
to	O
the	O
well	O
established	O
family	O
of	O
factorization	B-Method
models	E-Method
.	O

The	O
drawback	O
of	O
HypER	S-Method
is	O
that	O
it	O
sets	O
most	O
elements	O
of	O
the	O
core	O
weight	O
tensor	O
to	O
0	O
,	O
which	O
amounts	O
to	O
hard	O
regularization	O
,	O
rather	O
than	O
letting	O
the	O
model	O
learn	O
which	O
parameters	O
to	O
use	O
via	O
a	O
soft	B-Method
regularization	I-Method
approach	E-Method
.	O

Scoring	B-Metric
functions	E-Metric
of	O
all	O
models	O
described	O
above	O
and	O
TuckER	S-Method
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Background	O
Let	O
denote	O
the	O
set	O
of	O
all	O
entities	O
and	O
the	O
set	O
of	O
all	O
relations	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O

A	O
triple	O
is	O
represented	O
as	O
,	O
with	O
denoting	O
subject	O
and	O
object	O
entities	O
respectively	O
and	O
the	O
relation	O
between	O
them	O
.	O

subsection	O
:	O
Link	B-Task
Prediction	E-Task
In	O
link	B-Task
prediction	E-Task
,	O
we	O
are	O
given	O
a	O
subset	O
of	O
all	O
true	O
triples	O
and	O
the	O
aim	O
is	O
to	O
learn	O
a	O
scoring	B-Method
function	E-Method
that	O
assigns	O
a	O
score	O
to	O
each	O
triple	O
,	O
indicating	O
whether	O
that	O
triple	O
is	O
true	O
or	O
false	O
,	O
with	O
the	O
ultimate	O
goal	O
of	O
being	O
able	O
to	O
correctly	O
score	O
all	O
missing	O
triples	O
.	O

The	O
scoring	B-Method
function	E-Method
is	O
either	O
a	O
specific	O
form	O
of	O
tensor	B-Method
factorization	E-Method
in	O
the	O
case	O
of	O
linear	B-Method
models	E-Method
or	O
a	O
more	O
complex	O
(	O
deep	B-Method
)	I-Method
neural	I-Method
network	I-Method
architecture	E-Method
in	O
the	O
case	O
of	O
non	B-Method
-	I-Method
linear	I-Method
models	E-Method
.	O

Typically	O
,	O
a	O
positive	O
score	O
for	O
a	O
particular	O
triple	O
indicates	O
a	O
true	O
fact	O
predicted	O
by	O
the	O
model	O
,	O
while	O
a	O
negative	O
score	O
indicates	O
a	O
false	O
one	O
.	O

With	O
most	O
recent	O
models	O
,	O
a	O
non	B-Method
-	I-Method
linearity	E-Method
such	O
as	O
the	O
logistic	O
sigmoid	O
function	O
is	O
typically	O
applied	O
to	O
the	O
score	O
to	O
give	O
a	O
corresponding	O
probability	O
prediction	O
as	O
to	O
whether	O
a	O
certain	O
fact	O
is	O
true	O
.	O

subsection	O
:	O
Tucker	B-Method
Decomposition	I-Method
Tucker	I-Method
decomposition	E-Method
,	O
named	O
after	O
Ledyard	O
R.	O
Tucker	O
and	O
refined	O
in	O
his	O
subsequent	O
work	O
,	O
decomposes	O
a	O
tensor	O
into	O
a	O
set	O
of	O
matrices	O
and	O
a	O
smaller	O
core	O
tensor	O
.	O

In	O
a	O
three	B-Task
-	I-Task
mode	I-Task
case	E-Task
,	O
given	O
the	O
original	O
tensor	O
,	O
Tucker	B-Method
decomposition	E-Method
outputs	O
a	O
tensor	O
and	O
three	O
matrices	O
,	O
,	O
:	O
with	O
indicating	O
the	O
tensor	O
product	O
along	O
the	O
n	O
-	O
th	O
mode	O
and	O
the	O
vector	O
inner	O
product	O
.	O

Factor	O
matrices	O
,	O
and	O
,	O
when	O
orthogonal	O
,	O
can	O
be	O
thought	O
of	O
as	O
the	O
principal	O
components	O
in	O
each	O
mode	O
.	O

Elements	O
of	O
the	O
core	O
tensor	O
show	O
the	O
level	O
of	O
interaction	O
between	O
the	O
different	O
components	O
.	O

Typically	O
,	O
,	O
,	O
are	O
smaller	O
than	O
,	O
,	O
respectively	O
,	O
so	O
can	O
be	O
thought	O
of	O
as	O
a	O
compressed	O
version	O
of	O
.	O

Tucker	B-Method
decomposition	E-Method
is	O
not	O
unique	O
,	O
i.e.	O
we	O
can	O
transform	O
without	O
affecting	O
the	O
fit	O
if	O
we	O
apply	O
the	O
inverse	O
of	O
that	O
transformation	O
to	O
the	O
factor	O
matrices	O
.	O

Imposing	O
additional	O
constraints	O
on	O
the	O
structure	O
of	O
,	O
such	O
as	O
sparsity	O
,	O
making	O
its	O
elements	O
small	O
or	O
making	O
the	O
core	O
“	O
all	O
-	O
orthogonal	O
”	O
,	O
can	O
lead	O
to	O
improved	O
uniqueness	O
.	O

section	O
:	O
Tucker	B-Method
Decomposition	E-Method
for	O
Link	B-Task
Prediction	E-Task
We	O
propose	O
a	O
model	O
that	O
uses	O
Tucker	B-Method
decomposition	E-Method
for	O
link	B-Task
prediction	E-Task
on	O
the	O
third	B-Method
-	I-Method
order	I-Method
binary	I-Method
tensor	I-Method
representation	E-Method
of	O
a	O
knowledge	O
graph	O
,	O
with	O
entity	O
embedding	O
matrix	O
that	O
is	O
equivalent	O
for	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
relation	O
embedding	O
matrix	O
,	O
where	O
and	O
represent	O
the	O
number	O
of	O
entities	O
and	O
relations	O
and	O
and	O
the	O
dimensionality	O
of	O
entity	O
and	O
relation	O
embedding	O
vectors	O
respectively	O
.	O

We	O
define	O
the	O
scoring	B-Metric
function	E-Metric
for	O
TuckER	S-Method
as	O
:	O
where	O
are	O
the	O
rows	O
of	O
representing	O
the	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
,	O
the	O
rows	O
of	O
representing	O
the	O
relation	O
embedding	O
vector	O
,	O
is	O
the	O
core	B-Method
tensor	I-Method
of	I-Method
Tucker	I-Method
decomposition	E-Method
and	O
is	O
the	O
tensor	O
product	O
along	O
the	O
-	O
th	O
mode	O
.	O

We	O
apply	O
logistic	B-Method
sigmoid	E-Method
to	O
each	O
score	O
to	O
obtain	O
the	O
predicted	O
probability	O
of	O
a	O
triple	O
being	O
true	O
.	O

Visualization	O
of	O
the	O
TuckER	S-Method
model	O
architecture	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

As	O
proven	O
in	O
Section	O
[	O
reference	O
]	O
,	O
TuckER	S-Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
sufficient	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
,	O
it	O
is	O
able	O
to	O
assign	O
values	O
to	O
the	O
embeddings	O
that	O
correctly	O
separate	O
any	O
combination	O
of	O
ground	O
truth	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O

The	O
number	O
of	O
parameters	O
of	O
TuckER	S-Method
increases	O
linearly	O
with	O
respect	O
to	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
,	O
as	O
the	O
number	O
of	O
entities	O
and	O
relations	O
(	O
and	O
respectively	O
)	O
increases	O
,	O
since	O
the	O
number	O
of	O
parameters	O
of	O
core	B-Method
tensor	E-Method
depends	O
only	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
not	O
on	O
the	O
number	O
of	O
entities	O
or	O
relations	O
.	O

By	O
having	O
the	O
core	O
tensor	O
,	O
unlike	O
simpler	O
models	O
such	O
as	O
DistMult	S-Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
TuckER	S-Method
does	O
not	O
encode	O
all	O
the	O
learned	O
knowledge	O
into	O
the	O
embeddings	O
;	O
some	O
is	O
stored	O
in	O
the	O
core	O
tensor	O
and	O
shared	O
between	O
all	O
entities	O
and	O
relations	O
.	O

subsection	O
:	O
Training	O
Given	O
we	O
can	O
not	O
use	O
analytical	B-Method
methods	E-Method
for	O
computing	O
the	O
tensor	B-Task
factorization	E-Task
,	O
since	O
the	O
tensor	O
being	O
factorized	O
is	O
comprised	O
of	O
and	O
(	O
after	O
applying	O
the	O
inverse	O
of	O
logistic	O
sigmoid	O
)	O
,	O
we	O
use	O
numerical	B-Method
methods	E-Method
to	O
train	O
TuckER	S-Method
.	O

Following	O
the	O
training	O
procedure	O
introduced	O
by	O
dettmers2018convolutional	S-Method
with	O
the	O
goal	O
of	O
speeding	O
up	O
training	S-Task
and	O
increasing	O
accuracy	S-Metric
,	O
we	O
use	O
1	B-Method
-	I-Method
N	I-Method
scoring	E-Method
,	O
i.e.	O
we	O
simultaneously	O
score	O
a	O
pair	O
and	O
with	O
all	O
entities	O
,	O
in	O
contrast	O
to	O
1	O
-	O
1	O
scoring	O
,	O
where	O
individual	O
triples	O
are	O
trained	O
one	O
at	O
a	O
time	O
.	O

This	O
way	O
we	O
make	O
use	O
of	O
the	O
local	O
-	O
closed	O
world	O
assumption	O
,	O
where	O
we	O
assume	O
that	O
a	O
knowledge	O
graph	O
is	O
only	O
locally	O
complete	O
,	O
i.e.	O
we	O
include	O
only	O
the	O
non	O
-	O
existing	O
triples	O
and	O
of	O
the	O
observed	O
pairs	O
and	O
respectively	O
as	O
negative	O
samples	O
and	O
all	O
observed	O
triples	O
as	O
positive	O
samples	O
.	O

We	O
train	O
our	O
model	O
to	O
minimize	O
the	O
Bernoulli	O
negative	O
log	O
-	O
likelihood	O
loss	O
function	O
:	O
where	O
is	O
the	O
vector	O
of	O
probabilities	O
predicted	O
by	O
the	O
model	O
and	O
is	O
the	O
label	O
vector	O
of	O
ones	O
for	O
true	O
and	O
zeros	O
for	O
false	O
triples	O
.	O

section	O
:	O
Theoretical	O
Analysis	O
subsection	O
:	O
Bound	O
on	O
Embedding	B-Metric
Dimensionality	E-Metric
for	O
Full	B-Task
Expressiveness	E-Task
As	O
previously	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
a	O
tensor	B-Method
factorization	I-Method
model	E-Method
is	O
said	O
to	O
be	O
fully	O
expressive	O
if	O
for	O
any	O
ground	O
truth	O
over	O
all	O
entities	O
and	O
relations	O
,	O
there	O
exist	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separate	O
the	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O

As	O
shown	O
in	O
,	O
ComplEx	S-Method
is	O
fully	O
expressive	O
with	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
of	O
for	O
achieving	O
full	O
expressiveness	O
.	O

Similarly	O
to	O
ComplEx	S-Method
,	O
kazemi2018simple	S-Method
show	O
that	O
SimplE	S-Method
is	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embeddings	O
of	O
size	O
,	O
with	O
representing	O
the	O
number	O
of	O
true	O
facts	O
.	O

The	O
authors	O
further	O
prove	O
other	O
models	O
are	O
not	O
fully	O
expressive	O
:	O
DistMult	S-Method
,	O
because	O
it	O
can	O
not	O
model	O
asymmetric	O
relations	O
;	O
and	O
transitive	B-Method
models	E-Method
such	O
as	O
TransE	S-Method
and	O
its	O
variants	O
FTransE	S-Method
and	O
STransE	S-Method
,	O
because	O
of	O
certain	O
contradictions	O
that	O
they	O
impose	O
between	O
different	O
relation	O
types	O
.	O

By	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
establish	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
(	O
i.e.	O
rank	O
of	O
the	O
decomposition	O
)	O
that	O
guarantees	O
full	O
expressiveness	O
of	O
TuckER	S-Method
.	O

theorem	O
:	O
.	O

Given	O
any	O
ground	O
truth	O
over	O
a	O
set	O
of	O
entities	O
E	O
and	O
relations	O
R	O
,	O
there	O
exists	O
a	O
TuckER	S-Method
model	O
with	O
subject	B-Method
and	I-Method
object	I-Method
entity	I-Method
embeddings	E-Method
of	O
dimensionality	O
=	O
dene	O
and	O
relation	O
embeddings	O
of	O
dimensionality	O
=	O
drnr	O
,	O
where	O
=	O
ne|E|	O
is	O
the	O
number	O
of	O
entities	O
and	O
=	O
nr|R|	O
the	O
number	O
of	O
relations	O
,	O
that	O
accurately	O
represents	O
that	O
ground	O
truth	O
.	O

proof	O
:	O
Proof	O
.	O

Let	O
and	O
be	O
the	O
-	O
dimensional	O
one	O
-	O
hot	B-Method
binary	I-Method
vector	I-Method
representations	I-Method
of	I-Method
subject	I-Method
and	I-Method
object	I-Method
entities	E-Method
and	O
respectively	O
and	O
the	O
-	O
dimensional	O
one	B-Method
-	I-Method
hot	I-Method
binary	I-Method
vector	I-Method
representation	E-Method
of	O
a	O
relation	O
.	O

For	O
each	O
subject	O
entity	O
,	O
relation	O
and	O
object	O
entity	O
,	O
we	O
let	O
the	O
-	O
th	O
,	O
-	O
th	O
and	O
-	O
th	O
element	O
respectively	O
of	O
the	O
corresponding	O
vectors	O
,	O
and	O
be	O
1	O
and	O
all	O
other	O
elements	O
0	O
.	O

Further	O
,	O
we	O
set	O
the	O
element	O
of	O
the	O
tensor	O
to	O
1	O
if	O
the	O
fact	O
holds	O
and	O
-	O
1	O
otherwise	O
.	O

Thus	O
the	O
tensor	O
product	O
of	O
these	O
entity	B-Method
embeddings	E-Method
and	O
the	O
relation	B-Method
embedding	E-Method
with	O
the	O
core	O
tensor	O
,	O
after	O
applying	O
the	O
logistic	B-Method
sigmoid	E-Method
,	O
accurately	O
represents	O
the	O
original	O
tensor	O
.	O

∎	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
Theorem	O
[	O
reference	O
]	O
shows	O
that	O
it	O
is	O
straightforward	O
to	O
prove	O
that	O
the	O
required	O
dimensionality	S-Metric
of	O
TuckER	S-Method
embeddings	O
to	O
ensure	O
full	O
expressiveness	O
is	O
lower	O
than	O
the	O
required	O
dimensionality	S-Metric
for	O
SimplE	O
and	O
ComplEx	O
by	O
a	O
factor	O
of	O
for	O
entity	B-Method
embeddings	E-Method
and	O
by	O
a	O
factor	O
of	O
for	O
relation	B-Method
embeddings	E-Method
.	O

Existing	O
knowledge	O
graphs	O
usually	O
contain	O
tens	O
of	O
thousands	O
of	O
entities	O
and	O
hundreds	O
or	O
even	O
thousands	O
of	O
relations	O
.	O

This	O
allows	O
TuckER	S-Method
to	O
be	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embedding	O
dimensionalities	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
those	O
of	O
ComplEx	S-Method
and	O
SimplE.	O
In	O
practice	O
,	O
we	O
expect	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
needed	O
for	O
full	B-Task
reconstruction	E-Task
of	O
the	O
underlying	O
binary	O
tensor	O
to	O
be	O
much	O
smaller	O
than	O
the	O
bound	O
stated	O
above	O
,	O
since	O
the	O
assignment	O
of	O
values	O
to	O
the	O
tensor	O
is	O
not	O
random	O
but	O
follows	O
a	O
certain	O
structure	O
,	O
otherwise	O
nothing	O
unknown	O
could	O
be	O
predicted	O
.	O

Even	O
more	O
so	O
,	O
low	O
decomposition	O
rank	O
is	O
actually	O
a	O
desired	O
property	O
,	O
forcing	O
the	O
model	O
to	O
learn	O
that	O
structure	O
and	O
generalize	O
to	O
new	O
data	O
,	O
rather	O
than	O
simply	O
memorizing	O
the	O
input	O
.	O

We	O
expect	O
TuckER	S-Method
to	O
perform	O
better	O
than	O
ComplEx	S-Method
and	O
SimplE	O
with	O
embeddings	O
of	O
lower	O
dimensionality	O
due	O
to	O
parameter	O
sharing	O
in	O
the	O
core	O
tensor	O
(	O
shown	O
empirically	O
in	O
Section	O
[	O
reference	O
]	O
)	O
,	O
which	O
could	O
be	O
of	O
importance	O
for	O
efficiency	O
in	O
downstream	B-Task
tasks	E-Task
.	O

subsection	O
:	O
Relation	O
of	O
TuckER	S-Method
to	O
Previous	O
Tensor	B-Method
Factorization	I-Method
Approaches	E-Method
Several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
TuckER	S-Method
:	O
RESCAL	O
[	O
]	O
Following	O
the	O
notation	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
RESCAL	B-Metric
scoring	I-Metric
function	E-Metric
has	O
the	O
form	O
:	O
This	O
corresponds	O
to	O
Equation	O
[	O
reference	O
]	O
with	O
,	O
,	O
and	O
the	O
identity	O
matrix	O
,	O
i.e	O
the	O
second	O
dimension	O
of	O
original	O
tensor	O
is	O
not	O
reduced	O
by	O
.	O

This	O
is	O
also	O
known	O
as	O
Tucker2	B-Method
decomposition	E-Method
.	O

As	O
is	O
the	O
case	O
with	O
TuckER	S-Method
,	O
the	O
entity	O
embedding	O
matrix	O
of	O
RESCAL	O
is	O
shared	O
between	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
the	O
relation	O
matrices	O
are	O
the	O
slices	O
of	O
the	O
core	O
tensor	O
.	O

As	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
drawback	O
of	O
RESCAL	S-Method
compared	O
to	O
TuckER	S-Method
is	O
that	O
its	O
number	O
of	O
parameters	O
grows	O
quadratically	O
in	O
the	O
entity	B-Metric
embedding	I-Metric
dimension	E-Metric
as	O
the	O
number	O
of	O
relations	O
increases	O
.	O

Therefore	O
,	O
RESCAL	S-Method
tends	O
to	O
overfit	O
for	O
those	O
relations	O
for	O
which	O
only	O
a	O
small	O
number	O
of	O
training	O
triples	O
is	O
available	O
.	O

DistMult	S-Method
[	O
]	O
The	O
scoring	B-Metric
function	E-Metric
of	O
DistMult	S-Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
can	O
be	O
viewed	O
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
that	O
of	O
TuckER	S-Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
superdiagonal	O
with	O
1s	O
on	O
that	O
superdiagonal	O
,	O
i.e.	O
all	O
elements	O
with	O
are	O
1	O
and	O
all	O
the	O
other	O
elements	O
are	O
0	O
(	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
diagonal	O
for	O
every	O
slice	O
,	O
i.e.	O
all	O
elements	O
apart	O
from	O
with	O
are	O
0	O
.	O

If	O
we	O
adopt	O
the	O
TuckER	S-Method
view	O
of	O
the	O
DistMult	B-Method
scoring	I-Method
function	E-Method
,	O
rows	O
of	O
contain	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
and	O
rows	O
of	O
contain	O
relation	O
embedding	O
vectors	O
.	O

By	O
adopting	O
the	O
RESCAL	O
view	O
,	O
entity	O
embedding	O
embedding	O
vectors	O
remain	O
the	O
same	O
,	O
but	O
relation	O
embedding	O
vectors	O
are	O
now	O
the	O
diagonals	O
of	O
slices	O
of	O
.	O

It	O
is	O
interesting	O
to	O
note	O
that	O
the	O
TuckER	S-Method
interpretation	O
of	O
the	O
DistMult	B-Method
scoring	I-Method
function	E-Method
,	O
given	O
that	O
matrices	O
and	O
are	O
identical	O
,	O
can	O
alternatively	O
be	O
interpreted	O
as	O
a	O
special	O
case	O
of	O
CP	B-Method
decomposition	E-Method
,	O
since	O
Tucker	B-Method
decomposition	E-Method
with	O
a	O
superdiagonal	O
core	O
tensor	O
becomes	O
equivalent	O
to	O
CP	B-Method
decomposition	E-Method
.	O

Because	O
of	O
its	O
simplicity	O
,	O
DistMult	S-Method
learns	O
a	O
binary	O
tensor	O
that	O
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
,	O
so	O
it	O
can	O
not	O
learn	O
to	O
represent	O
asymmetric	O
relations	O
.	O

ComplEx	O
[	O
]	O
Bilinear	B-Method
models	E-Method

are	O
a	O
family	O
of	O
models	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
represented	O
by	O
vectors	O
,	O
a	O
relation	O
is	O
represented	O
by	O
a	O
matrix	O
and	O
the	O
scoring	O
function	O
takes	O
the	O
form	O
of	O
a	O
bilinear	O
product	O
between	O
the	O
two	O
embedding	O
vectors	O
and	O
the	O
relation	O
matrix	O
,	O
i.e.	O
.	O


It	O
is	O
trivial	O
to	O
show	O
that	O
both	O
RESCAL	S-Method
and	O
DistMult	S-Method
belong	O
to	O
the	O
family	B-Method
of	I-Method
bilinear	I-Method
models	E-Method
.	O

As	O
explained	O
by	O
kazemi2018simple	O
,	O
ComplEx	S-Method
can	O
be	O
considered	O
a	O
bilinear	B-Method
model	E-Method
with	O
the	O
real	O
and	O
imaginary	O
part	O
of	O
an	O
embedding	O
for	O
each	O
entity	O
concatenated	O
in	O
a	O
single	O
vector	O
,	O
for	O
subject	O
,	O
for	O
object	O
,	O
and	O
a	O
relation	O
matrix	O
,	O
constrained	O
in	O
a	O
way	O
that	O
its	O
leading	O
diagonal	O
contains	O
duplicated	O
elements	O
of	O
,	O
its	O
-	O
diagonal	O
contains	O
elements	O
of	O
and	O
its	O
-	O
-	O
diagonal	O
has	O
elements	O
of	O
-	O
,	O
with	O
all	O
other	O
elements	O
set	O
to	O
0	O
,	O
where	O
and	O
-	O
represent	O
offsets	O
from	O
the	O
leading	O
diagonal	O
.	O

This	O
makes	O
the	O
scoring	B-Metric
function	E-Metric
of	O
ComplEx	S-Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
with	O
relation	O
matrix	O
constrained	O
as	O
described	O
.	O

Therefore	O
,	O
similarly	O
to	O
DistMult	S-Method
,	O
we	O
can	O
regard	O
the	O
scoring	O
function	O
of	O
ComplEx	S-Method
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
the	O
scoring	B-Method
function	E-Method
of	O
TuckER	S-Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
different	O
tensor	O
diagonals	O
are	O
set	O
to	O
1	O
,	O
elements	O
on	O
one	O
tensor	O
diagonal	O
are	O
set	O
to	O
-	O
1	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
the	O
scoring	B-Metric
function	E-Metric
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
of	O
,	O
all	O
elements	O
on	O
the	O
leading	O
diagonal	O
are	O
set	O
to	O
,	O
the	O
-	O
diagonal	O
is	O
set	O
to	O
,	O
the	O
-	O
-	O
diagonal	O
is	O
set	O
to	O
-	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
.	O

This	O
shows	O
that	O
the	O
scoring	O
function	O
of	O
ComplEx	S-Method
,	O
which	O
computes	O
a	O
bilinear	O
product	O
with	O
complex	O
entity	O
and	O
relation	O
embeddings	O
and	O
disregards	O
the	O
imaginary	O
part	O
of	O
the	O
obtained	O
result	O
,	O
is	O
equivalent	O
to	O
a	O
hard	O
regularization	O
of	O
the	O
core	O
tensor	O
of	O
TuckER	S-Method
in	O
the	O
real	O
domain	O
.	O

SimplE	O
[	O
]	O
The	O
authors	O
show	O
that	O
SimplE	O
belongs	O
to	O
the	O
family	O
of	O
bilinear	B-Method
models	E-Method
by	O
concatenating	B-Method
embeddings	E-Method
for	O
head	O
and	O
tail	O
entities	O
for	O
both	O
subject	O
and	O
object	O
into	O
vectors	O
and	O
and	O
constraining	O
the	O
relation	O
matrix	O
so	O
that	O
it	O
contains	O
the	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
diagonal	O
and	O
the	O
inverse	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
-	O
diagonal	O
and	O
0s	O
elsewhere	O
.	O

The	O
SimplE	O
scoring	B-Method
function	E-Method
is	O
therefore	O
equivalent	O
to	O
:	O
that	O
of	O
TuckER	S-Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
two	O
tensor	O
diagonals	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
,	O
elements	O
on	O
the	O
-	O
diagonal	O
are	O
set	O
to	O
,	O
elements	O
on	O
the	O
-	O
-	O
diagonal	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
0	O
.	O

subsection	O
:	O
Representing	O
Asymmetric	O
Relations	O
Each	O
relation	O
in	O
a	O
knowledge	O
graph	O
can	O
be	O
characterized	O
by	O
a	O
certain	O
set	O
of	O
properties	O
,	O
such	O
as	O
symmetry	O
,	O
reflexivity	O
,	O
transitivity	O
,	O
etc	O
.	O

A	O
relation	O
is	O
asymmetric	O
if	O
,	O
for	O
all	O
subject	O
entities	O
that	O
are	O
related	O
to	O
their	O
corresponding	O
object	O
entities	O
through	O
,	O
the	O
reciprocal	O
necessarily	O
does	O
not	O
hold	O
,	O
i.e.	O
none	O
of	O
the	O
object	O
entities	O
are	O
related	O
to	O
the	O
subject	O
entities	O
through	O
.	O

So	O
far	O
,	O
there	O
have	O
been	O
two	O
possible	O
ways	O
in	O
which	O
linear	B-Method
link	I-Method
prediction	I-Method
models	E-Method
introduce	O
asymmetry	O
into	O
factorization	S-Task
of	O
the	O
binary	O
tensor	O
of	O
triples	O
.	O

One	O
is	O
to	O
have	O
distinct	O
(	O
although	O
possibly	O
related	O
)	O
embeddings	O
for	O
subject	O
and	O
object	O
entities	O
and	O
a	O
diagonal	O
matrix	O
(	O
or	O
equivalently	O
a	O
vector	O
)	O
for	O
each	O
relation	O
,	O
as	O
is	O
the	O
case	O
with	O
models	O
such	O
as	O
ComplEx	O
and	O
SimplE.	O
This	O
puts	O
a	O
strict	O
constraint	O
on	O
the	O
relation	O
matrix	O
and	O
imposes	O
a	O
hard	O
limit	O
on	O
the	O
type	O
of	O
transformation	O
applied	O
on	O
entity	O
embeddings	O
.	O

The	O
other	O
way	O
of	O
modeling	O
asymmetry	O
is	O
for	O
subject	O
and	O
object	O
entity	O
embeddings	O
to	O
be	O
equivalent	O
,	O
but	O
representing	O
a	O
relation	O
as	O
a	O
full	O
rank	O
matrix	O
,	O
which	O
is	O
the	O
case	O
with	O
RESCAL	O
.	O

The	O
drawback	O
of	O
the	O
latter	O
approach	O
is	O
quadratic	O
growth	O
of	O
parameter	O
number	O
with	O
the	O
number	O
of	O
relations	O
,	O
which	O
often	O
leads	O
to	O
overfitting	O
,	O
especially	O
for	O
relations	O
with	O
a	O
small	O
number	O
of	O
training	O
triples	O
.	O

TuckER	S-Method
introduces	O
a	O
novel	O
approach	O
to	O
dealing	O
with	O
asymmetry	O
:	O
by	O
representing	O
relations	O
as	O
vectors	O
,	O
which	O
makes	O
the	O
parameter	O
number	O
grow	O
linearly	O
with	O
the	O
number	O
of	O
relations	O
;	O
and	O
by	O
having	O
an	O
asymmetric	O
relation	O
-	O
agnostic	O
core	O
tensor	O
,	O
which	O
enables	O
knowledge	B-Task
sharing	E-Task
between	O
relations	O
.	O

Multiplying	O
with	O
along	O
the	O
second	O
mode	O
,	O
we	O
obtain	O
a	O
full	O
rank	O
relation	O
-	O
specific	O
matrix	O
,	O
which	O
is	O
capable	O
of	O
performing	O
all	O
possible	O
linear	O
transformations	O
on	O
the	O
entity	O
embeddings	O
,	O
i.e.	O
rotation	O
,	O
reflection	O
or	O
stretch	O
,	O
and	O
thus	O
capable	O
of	O
modeling	O
asymmetry	O
.	O

Regardless	O
of	O
what	O
kind	O
of	O
transformation	O
is	O
needed	O
for	O
modeling	O
a	O
particular	O
relation	O
,	O
TuckER	S-Method
is	O
capable	O
of	O
learning	O
it	O
from	O
the	O
data	O
,	O
rather	O
than	O
through	O
explicitly	O
limiting	O
the	O
relation	O
matrix	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Datasets	O
We	O
evaluate	O
TuckER	S-Method
using	O
four	O
standard	O
link	O
prediction	O
datasets	O
:	O
FB15k	S-Material
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
large	O
database	O
of	O
real	O
world	O
facts	O
containing	O
information	O
about	O
films	O
,	O
actors	O
,	O
sports	O
,	O
etc	O
.	O

FB15k	B-Material
-	I-Material
237	E-Material
was	O
created	O
from	O
FB15k	S-Material
by	O
removing	O
the	O
inverse	O
of	O
many	O
relations	O
that	O
are	O
present	O
in	O
the	O
training	O
set	O
from	O
validation	O
and	O
test	O
sets	O
,	O
making	O
it	O
more	O
difficult	O
for	O
simple	O
models	O
to	O
do	O
well	O
.	O

WN18	S-Material
is	O
a	O
subset	O
of	O
WordNet	O
,	O
a	O
database	O
containing	O
lexical	O
relations	O
between	O
words	O
.	O

WN18	S-Material
follows	O
a	O
hierarchical	O
structure	O
.	O

WN18RR	S-Material
is	O
a	O
subset	O
of	O
WN18	S-Material
,	O
created	O
by	O
removing	O
the	O
inverse	O
relations	O
from	O
validation	O
and	O
test	O
sets	O
.	O

Number	O
of	O
entities	O
and	O
relations	O
for	O
each	O
dataset	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Implementation	O
and	O
Experiments	O
We	O
implement	O
TuckER	S-Method
in	O
PyTorch	S-Method
and	O
make	O
our	O
code	O
available	O
on	O
Github	O
.	O

We	O
choose	O
all	O
hyper	O
-	O
parameters	O
by	O
random	B-Method
search	E-Method
based	O
on	O
the	O
validation	B-Metric
set	E-Metric
performance	O
.	O

For	O
FB15k	S-Material
and	O
FB15k	B-Material
-	I-Material
237	E-Material
,	O
we	O
set	O
both	O
entity	O
and	O
relation	O
embedding	O
dimensions	O
to	O
.	O

For	O
WN18	S-Material
and	O
WN18RR	S-Material
,	O
which	O
both	O
contain	O
a	O
significantly	O
smaller	O
number	O
of	O
relations	O
relative	O
to	O
the	O
number	O
of	O
entities	O
as	O
well	O
as	O
a	O
small	O
number	O
of	O
relations	O
compared	O
to	O
FB15k	S-Material
and	O
FB15k	B-Material
-	I-Material
237	E-Material
,	O
we	O
set	O
and	O
.	O

We	O
use	O
both	O
batch	B-Method
normalization	E-Method
and	O
dropout	S-Method
to	O
control	O
overfitting	O
and	O
improve	O
predictions	S-Task
.	O

We	O
choose	O
the	O
learning	B-Metric
rate	E-Metric
from	O
and	O
learning	B-Metric
rate	I-Metric
decay	E-Metric
from	O
.	O

We	O
find	O
the	O
following	O
combinations	O
of	O
learning	B-Metric
rate	E-Metric
and	O
learning	B-Metric
rate	I-Metric
decay	E-Metric
to	O
give	O
the	O
best	O
results	O
:	O
for	O
FB15k	S-Material
,	O
for	O
FB15k	B-Material
-	I-Material
237	E-Material
,	O
for	O
WN18	S-Material
and	O
for	O
WN18RR	S-Material
.	O

We	O
train	O
the	O
model	O
using	O
Adam	S-Method
and	O
set	O
the	O
batch	O
size	O
to	O
128	O
.	O

We	O
evaluate	O
each	O
triple	O
from	O
the	O
test	O
set	O
as	O
in	O
:	O
for	O
a	O
given	O
triple	O
,	O
we	O
generate	O
test	O
triples	O
by	O
keeping	O
the	O
subject	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
object	O
entity	O
with	O
all	O
possible	O
entities	O
and	O
by	O
keeping	O
the	O
object	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
subject	O
entity	O
with	O
all	O
entities	O
.	O

We	O
then	O
rank	O
the	O
scores	O
obtained	O
.	O

We	O
use	O
the	O
filtered	O
setting	O
only	O
,	O
i.e.	O
we	O
remove	O
all	O
other	O
true	O
triples	O
apart	O
from	O
the	O
currently	O
observed	O
test	O
triple	O
.	O

For	O
evaluation	O
,	O
we	O
use	O
two	O
evaluation	B-Metric
metrics	E-Metric
used	O
across	O
the	O
link	B-Task
prediction	I-Task
literature	E-Task
:	O
mean	B-Metric
reciprocal	I-Metric
rank	E-Metric
(	O
MRR	S-Metric
)	O
and	O
hits@	S-Metric
,	O
.	O

Mean	B-Metric
reciprocal	I-Metric
rank	E-Metric
is	O
the	O
average	O
of	O
the	O
inverse	O
of	O
a	O
mean	O
rank	O
assigned	O
to	O
the	O
true	O
triple	O
over	O
all	O
generated	O
triples	O
.	O

Hits@	S-Metric
measures	O
the	O
percentage	O
of	O
times	O
the	O
true	O
triple	O
is	O
ranked	O
in	O
the	O
top	O
of	O
the	O
generated	O
triples	O
.	O

The	O
aim	O
is	O
for	O
a	O
model	O
to	O
achieve	O
high	O
MRR	S-Metric
and	O
hits@	S-Metric
.	O

subsection	O
:	O
Link	B-Task
Prediction	E-Task
Results	O
Link	B-Task
prediction	E-Task
results	O
on	O
all	O
four	O
datasets	O
are	O
shown	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

Overall	O
,	O
TuckER	S-Method
outperforms	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
all	O
metrics	O
across	O
all	O
datasets	O
(	O
apart	O
from	O
hits@10	S-Metric
on	O
WN18	S-Material
where	O
a	O
non	B-Method
-	I-Method
linear	I-Method
model	E-Method
,	O
R	B-Method
-	I-Method
GCN	E-Method
,	O
does	O
better	O
)	O
,	O
which	O
shows	O
that	O
this	O
relatively	O
simple	O
yet	O
fully	O
flexible	O
linear	B-Method
model	E-Method
leads	O
to	O
very	O
good	O
performance	O
.	O

Results	O
achieved	O
by	O
TuckER	S-Method
are	O
not	O
only	O
better	O
than	O
those	O
of	O
other	O
linear	B-Method
models	E-Method
,	O
such	O
as	O
DistMult	S-Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
but	O
also	O
better	O
than	O
the	O
results	O
of	O
many	O
more	O
complex	O
deep	B-Method
neural	I-Method
network	I-Method
and	I-Method
reinforcement	I-Method
learning	I-Method
architectures	E-Method
,	O
e.g.	O
R	B-Method
-	I-Method
GCN	E-Method
,	O
MINERVA	S-Method
,	O
ConvE	S-Method
and	O
HypER	S-Method
,	O
demonstrating	O
the	O
expressive	O
power	O
of	O
linear	B-Method
models	E-Method
.	O

Even	O
though	O
TuckER	S-Method
has	O
more	O
parameters	O
than	O
some	O
more	O
simpler	O
linear	B-Method
models	E-Method
(	O
DistMult	S-Method
,	O
ComplEx	O
and	O
SimplE	O
)	O
due	O
to	O
the	O
presence	O
of	O
core	O
tensor	O
(	O
containing	O
million	O
parameters	O
for	O
FB15k	S-Material
and	O
FB15k	B-Material
-	I-Material
237	E-Material
and	O
million	O
parameters	O
for	O
WN18	S-Material
and	O
WN18RR	S-Material
)	O
,	O
it	O
consistently	O
obtains	O
better	O
results	O
than	O
any	O
of	O
those	O
models	O
.	O

We	O
believe	O
this	O
is	O
achieved	O
by	O
exploiting	O
knowledge	O
sharing	O
between	O
relations	O
through	O
the	O
core	O
tensor	O
and	O
implicit	B-Method
regularization	E-Method
from	O
dropout	S-Method
,	O
which	O
allows	O
the	O
model	O
to	O
learn	O
which	O
parameters	O
to	O
ignore	O
rather	O
than	O
explicitly	O
setting	O
them	O
to	O
0	O
.	O

We	O
find	O
the	O
value	O
of	O
the	O
dropout	O
parameter	O
to	O
have	O
a	O
significant	O
influence	O
on	O
results	O
,	O
with	O
lower	O
dropout	O
values	O
required	O
for	O
datasets	O
with	O
a	O
higher	O
number	O
of	O
training	O
triples	O
per	O
relation	O
and	O
thus	O
less	O
risk	O
of	O
overfitting	O
(	O
WN18	S-Material
and	O
WN18RR	S-Material
)	O
and	O
higher	O
dropout	B-Metric
values	E-Metric
required	O
for	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
FB15k	S-Material
and	O
FB15k	B-Material
-	I-Material
237	E-Material
)	O
.	O

We	O
further	O
note	O
that	O
TuckER	S-Method
improves	O
the	O
results	O
of	O
all	O
previous	O
linear	B-Method
models	E-Method
by	O
a	O
larger	O
margin	O
on	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
e.g.	O
improvement	O
on	O
FB15k	S-Material
results	O
over	O
ComplEx	S-Method
,	O
improvement	O
over	O
SimplE	O
on	O
the	O
toughest	B-Metric
hits@1	I-Metric
metric	E-Metric
)	O
,	O
which	O
supports	O
our	O
belief	O
that	O
TuckER	S-Method
makes	O
use	O
of	O
the	O
parameters	O
shared	O
between	O
similar	O
relations	O
to	O
improve	O
predictions	S-Task
by	O
multi	B-Task
-	I-Task
task	I-Task
learning	E-Task
.	O

subsection	O
:	O
Influence	O
of	O
Embedding	B-Metric
Dimensionality	E-Metric
In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
derive	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
for	O
full	B-Task
expressiveness	E-Task
that	O
is	O
much	O
lower	O
for	O
TuckER	S-Method
than	O
for	O
simpler	O
linear	B-Method
models	E-Method
ComplEx	O
and	O
SimplE.	O
This	O
suggests	O
TuckER	S-Method
should	O
need	O
a	O
lower	O
embedding	O
dimensionality	O
(	O
i.e.	O
lower	O
rank	O
of	O
the	O
decomposition	O
)	O
for	O
obtaining	O
good	O
results	O
than	O
ComplEx	S-Method
or	O
SimplE.	O
To	O
test	O
this	O
,	O
we	O
train	O
ComplEx	S-Method
,	O
SimplE	O
and	O
TuckER	S-Method
on	O
FB15k	B-Material
-	I-Material
237	E-Material
with	O
embedding	O
sizes	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
obtained	O
MRR	S-Metric
on	O
the	O
test	O
set	O
for	O
each	O
of	O
the	O
models	O
.	O

We	O
can	O
see	O
from	O
Figure	O
[	O
reference	O
]	O
that	O
the	O
difference	O
between	O
the	O
MRRs	S-Metric
of	O
ComplEx	S-Method
,	O
SimplE	O
and	O
TuckER	S-Method
is	O
approximately	O
constant	O
for	O
embedding	O
sizes	O
100	O
and	O
200	O
.	O

However	O
,	O
for	O
lower	O
embedding	O
sizes	O
,	O
the	O
difference	O
between	O
MRRs	S-Metric
increases	O
by	O
for	O
embedding	B-Metric
size	E-Metric
50	O
and	O
by	O
for	O
embedding	B-Metric
size	E-Metric
20	O
for	O
ComplEx	O
and	O
by	O
for	O
embedding	B-Metric
size	E-Metric
50	O
and	O
by	O
for	O
embedding	O
size	O
20	O
for	O
SimplE.	O
At	O
embedding	B-Metric
size	E-Metric
20	O
,	O
the	O
performance	O
of	O
TuckER	S-Method
is	O
almost	O
as	O
good	O
as	O
the	O
performance	O
of	O
ComplEx	S-Method
and	O
SimplE	O
at	O
embedding	O
size	O
200	O
,	O
which	O
supports	O
our	O
initial	O
assumption	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
introduce	O
TuckER	S-Method
,	O
a	O
relatively	O
simple	O
yet	O
highly	O
flexible	O
linear	B-Method
model	E-Method
for	O
link	B-Task
prediction	E-Task
in	O
knowledge	B-Task
graphs	E-Task
based	O
on	O
the	O
Tucker	B-Method
decomposition	E-Method
of	O
a	O
third	O
-	O
order	O
binary	O
tensor	O
of	O
training	O
set	O
triples	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
standard	O
link	B-Task
prediction	I-Task
datasets	E-Task
.	O

As	O
well	O
as	O
being	O
fully	O
expressive	O
,	O
TuckER	S-Method
’s	O
number	O
of	O
parameters	O
grows	O
linearly	O
with	O
respect	O
to	O
embedding	O
dimension	O
as	O
the	O
number	O
of	O
entities	O
or	O
relations	O
in	O
a	O
knowledge	O
graph	O
increases	O
.	O

We	O
further	O
show	O
that	O
previous	O
linear	B-Method
state	I-Method
-	I-Method
of	I-Method
-	I-Method
the	I-Method
-	I-Method
art	I-Method
models	E-Method
,	O
RESCAL	S-Method
,	O
DistMult	S-Method
,	O
ComplEx	S-Method
and	O
SimplE	O
,	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O

Future	O
work	O
might	O
include	O
exploring	O
various	O
means	O
of	O
softly	B-Method
regularizing	E-Method
the	O
model	O
other	O
than	O
dropout	S-Method
and	O
finding	O
a	O
way	O
to	O
incorporate	O
background	O
knowledge	O
on	O
individual	O
relation	O
properties	O
into	O
the	O
existing	O
model	O
.	O

bibliography	O
:	O
References	O
Sentence	B-Method
-	I-Method
State	I-Method
LSTM	E-Method
for	O
Text	B-Task
Representation	E-Task
section	O
:	O
Abstract	O
Bi	B-Method
-	I-Method
directional	I-Method
LSTMs	E-Method
are	O
a	O
powerful	O
tool	O
for	O
text	B-Task
representation	E-Task
.	O

On	O
the	O
other	O
hand	O
,	O
they	O
have	O
been	O
shown	O
to	O
suffer	O
various	O
limitations	O
due	O
to	O
their	O
sequential	O
nature	O
.	O

We	O
investigate	O
an	O
alternative	O
LSTM	S-Method
structure	O
for	O
encoding	B-Task
text	E-Task
,	O
which	O
consists	O
of	O
a	O
parallel	O
state	O
for	O
each	O
word	O
.	O

Recurrent	B-Method
steps	E-Method
are	O
used	O
to	O
perform	O
local	O
and	O
global	O
information	O
exchange	O
between	O
words	O
simultaneously	O
,	O
rather	O
than	O
incremental	O
reading	O
of	O
a	O
sequence	O
of	O
words	O
.	O

Results	O
on	O
various	O
classification	B-Task
and	I-Task
sequence	I-Task
labelling	I-Task
benchmarks	E-Task
show	O
that	O
the	O
proposed	O
model	O
has	O
strong	O
representation	O
power	O
,	O
giving	O
highly	O
competitive	O
performances	O
compared	O
to	O
stacked	B-Method
BiLSTM	I-Method
models	E-Method
with	O
similar	O
parameter	O
numbers	O
.	O

section	O
:	O
Introduction	O
Neural	B-Method
models	E-Method
have	O
become	O
the	O
dominant	O
approach	O
in	O
the	O
NLP	O
literature	O
.	O

Compared	O
to	O
handcrafted	O
indicator	O
features	O
,	O
neural	B-Method
sentence	I-Method
representations	E-Method
are	O
less	O
sparse	O
,	O
and	O
more	O
flexible	O
in	O
encoding	O
intricate	O
syntactic	O
and	O
semantic	O
information	O
.	O

Among	O
various	O
neural	B-Method
networks	E-Method
for	O
encoding	B-Task
sentences	E-Task
,	O
bi	B-Method
-	I-Method
directional	I-Method
LSTMs	E-Method
(	O
BiLSTM	S-Method
)	O
[	O
reference	O
]	O
have	O
been	O
a	O
dominant	O
method	O
,	O
giving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
language	B-Task
modelling	E-Task
[	O
reference	O
]	O
,	O
machine	B-Task
translation	E-Task
[	O
reference	O
]	O
,	O
syntactic	B-Task
parsing	E-Task
[	O
reference	O
]	O
and	O
question	B-Task
answering	E-Task
[	O
reference	O
]	O
.	O

Despite	O
their	O
success	O
,	O
BiLSTMs	S-Method
have	O
been	O
shown	O
to	O
suffer	O
several	O
limitations	O
.	O

For	O
example	O
,	O
their	O
inherently	O
sequential	O
nature	O
endows	O
computation	O
non	O
-	O
parallel	O
within	O
the	O
same	O
sentence	O
[	O
reference	O
]	O
,	O
which	O
can	O
lead	O
to	O
a	O
computational	B-Metric
bottleneck	E-Metric
,	O
hindering	O
their	O
use	O
in	O
the	O
in	B-Task
-	I-Task
dustry	E-Task
.	O

In	O
addition	O
,	O
local	B-Method
ngrams	E-Method
,	O
which	O
have	O
been	O
shown	O
a	O
highly	O
useful	O
source	O
of	O
contextual	O
information	O
for	O
NLP	S-Task
,	O
are	O
not	O
explicitly	O
modelled	O
[	O
reference	O
]	O
.	O

Finally	O
,	O
sequential	O
information	O
flow	O
leads	O
to	O
relatively	O
weaker	O
power	O
in	O
capturing	O
longrange	O
dependencies	O
,	O
which	O
results	O
in	O
lower	O
performance	O
in	O
encoding	O
longer	O
sentences	O
[	O
reference	O
]	O
.	O

We	O
investigate	O
an	O
alternative	O
recurrent	B-Method
neural	I-Method
network	I-Method
structure	E-Method
for	O
addressing	O
these	O
issues	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
main	O
idea	O
is	O
to	O
model	O
the	O
hidden	O
states	O
of	O
all	O
words	O
simultaneously	O
at	O
each	O
recurrent	O
step	O
,	O
rather	O
than	O
one	O
word	O
at	O
a	O
time	O
.	O

In	O
particular	O
,	O
we	O
view	O
the	O
whole	O
sentence	O
as	O
a	O
single	O
state	O
,	O
which	O
consists	O
of	O
sub	O
-	O
states	O
for	O
individual	O
words	O
and	O
an	O
overall	O
sentence	O
-	O
level	O
state	O
.	O

To	O
capture	O
local	O
and	O
non	O
-	O
local	O
contexts	O
,	O
states	O
are	O
updated	O
recurrently	O
by	O
exchanging	O
information	O
between	O
each	O
other	O
.	O

Consequently	O
,	O
we	O
refer	O
to	O
our	O
model	O
as	O
sentence	O
-	O
state	O
LSTM	S-Method
,	O
or	O
S	B-Method
-	I-Method
LSTM	E-Method
in	O
short	O
.	O

Empirically	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
can	O
give	O
effective	O
sentence	B-Task
encoding	E-Task
after	O
3	O
-	O
6	O
recurrent	O
steps	O
.	O

In	O
contrast	O
,	O
the	O
number	O
of	O
recurrent	O
steps	O
necessary	O
for	O
BiLSTM	S-Method
scales	O
with	O
the	O
size	O
of	O
the	O
sentence	O
.	O

At	O
each	O
recurrent	O
step	O
,	O
information	B-Task
exchange	E-Task
is	O
conducted	O
between	O
consecutive	O
words	O
in	O
the	O
sentence	O
,	O
and	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
each	O
word	O
.	O

In	O
particular	O
,	O
each	O
word	O
receives	O
information	O
from	O
its	O
predecessor	O
and	O
successor	O
simultaneously	O
.	O

From	O
an	O
initial	O
state	O
without	O
information	O
exchange	O
,	O
each	O
word	O
-	O
level	O
state	O
can	O
obtain	O
3	O
-	O
gram	O
,	O
5	O
-	O
gram	O
and	O
7	O
-	O
gram	O
information	O
after	O
1	O
,	O
2	O
and	O
3	O
recurrent	O
steps	O
,	O
respectively	O
.	O

Being	O
connected	O
with	O
every	O
word	O
,	O
the	O
sentence	O
-	O
level	O
state	O
vector	O
serves	O
to	O
exchange	O
non	O
-	O
local	O
information	O
with	O
each	O
word	O
.	O

In	O
addition	O
,	O
it	O
can	O
also	O
be	O
used	O
as	O
a	O
global	B-Method
sentence	I-Method
-	I-Method
level	I-Method
representation	E-Method
for	O
classification	B-Task
tasks	E-Task
.	O

Results	O
on	O
both	O
classification	B-Task
and	I-Task
sequence	I-Task
labelling	E-Task
show	O
that	O
S	B-Method
-	I-Method
LSTM	E-Method
gives	O
better	O
accuracies	S-Metric
compared	O
to	O
BiLSTM	S-Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
while	O
being	O
faster	O
.	O

We	O
release	O
our	O
code	O
and	O
models	O
at	O
https:	O
//	O
github.com	O
/	O
leuchine	O
/	O
S	O
-	O
LSTM	S-Method
,	O
which	O
include	O
all	O
baselines	O
and	O
the	O
final	O
model	O
.	O

section	O
:	O
Related	O
Work	O
LSTM	S-Method
[	O
reference	O
]	O
showed	O
its	O
early	O
potentials	O
in	O
NLP	S-Task
when	O
a	O
neural	B-Method
machine	I-Method
translation	I-Method
system	E-Method
that	O
leverages	O
LSTM	S-Method
source	O
encoding	O
gave	O
highly	O
competitive	O
results	O
compared	O
to	O
the	O
best	O
SMT	B-Method
models	E-Method
[	O
reference	O
]	O
.	O

LSTM	S-Method
encoders	O
have	O
since	O
been	O
explored	O
for	O
other	O
tasks	O
,	O
including	O
syntactic	B-Task
parsing	E-Task
[	O
reference	O
]	O
,	O
text	B-Task
classification	E-Task
[	O
reference	O
]	O
and	O
machine	B-Task
reading	E-Task
[	O
reference	O
]	O
.	O

Bidirectional	B-Method
extensions	E-Method
have	O
become	O
a	O
standard	O
configuration	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	S-Metric
among	O
various	O
tasks	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

SLSTMs	S-Method
are	O
similar	O
to	O
BiLSTMs	S-Method
in	O
their	O
recurrent	O
bi	O
-	O
directional	O
message	O
flow	O
between	O
words	O
,	O
but	O
different	O
in	O
the	O
design	O
of	O
state	B-Task
transition	E-Task
.	O

CNNs	S-Method
[	O
reference	O
]	O
)	O
also	O
allow	O
better	O
parallelisation	O
compared	O
to	O
LSTMs	S-Method
for	O
sentence	B-Task
encoding	E-Task
[	O
reference	O
]	O
,	O
thanks	O
to	O
parallelism	O
among	O
convolution	B-Method
filters	E-Method
.	O

On	O
the	O
other	O
hand	O
,	O
convolution	O
features	O
embody	O
only	O
fix	O
-	O
sized	O
local	O
ngram	O
information	O
,	O
whereas	O
sentence	B-Method
-	I-Method
level	I-Method
feature	I-Method
aggregation	E-Method
via	O
pooling	S-Method
can	O
lead	O
to	O
loss	O
of	O
information	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
uses	O
a	O
global	O
sentence	O
-	O
level	O
node	O
to	O
assemble	O
and	O
back	O
-	O
distribute	O
local	O
information	O
in	O
the	O
recurrent	B-Method
state	I-Method
transition	I-Method
process	E-Method
,	O
suffering	O
less	O
information	B-Metric
loss	E-Metric
compared	O
to	O
pooling	S-Task
.	O

Attention	S-Method
[	O
reference	O
]	O
has	O
recently	O
been	O
explored	O
as	O
a	O
standalone	O
method	O
for	O
sentence	B-Task
encoding	E-Task
,	O
giving	O
competitive	O
results	O
compared	O
to	O
Bi	O
-	O
LSTM	S-Method
encoders	O
for	O
neural	B-Task
machine	I-Task
translation	E-Task
[	O
reference	O
]	O
.	O

The	O
attention	B-Method
mechanism	E-Method
allows	O
parallelisation	S-Task
,	O
and	O
can	O
play	O
a	O
similar	O
role	O
to	O
the	O
sentence	O
-	O
level	O
state	O
in	O
S	B-Method
-	I-Method
LSTMs	E-Method
,	O
which	O
uses	O
neural	O
gates	O
to	O
integrate	O
word	O
-	O
level	O
information	O
compared	O
to	O
hierarchical	O
attention	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
further	O
allows	O
local	O
communication	O
between	O
neighbouring	O
words	O
.	O

Hierarchical	B-Method
stacking	I-Method
of	I-Method
CNN	I-Method
layers	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
allows	O
better	O
interaction	O
between	O
non	O
-	O
local	O
components	O
in	O
a	O
sentence	O
via	O
incremental	O
levels	O
of	O
abstraction	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
is	O
similar	O
to	O
hierarchical	B-Method
attention	E-Method
and	O
stacked	B-Method
CNN	E-Method
in	O
this	O
respect	O
,	O
incrementally	O
refining	O
sentence	B-Task
representations	E-Task
.	O

However	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
models	O
hierarchical	B-Method
encoding	I-Method
of	I-Method
sentence	I-Method
structure	E-Method
as	O
a	O
recurrent	B-Method
state	I-Method
transition	I-Method
process	E-Method
.	O

In	O
nature	O
,	O
our	O
work	O
belongs	O
to	O
the	O
family	O
of	O
LSTM	S-Method
sentence	O
representations	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
is	O
inspired	O
by	O
message	B-Method
passing	I-Method
over	I-Method
graphs	E-Method
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

Graph	B-Method
-	I-Method
structure	I-Method
neural	I-Method
models	E-Method
have	O
been	O
used	O
for	O
computer	B-Task
program	I-Task
verification	E-Task
[	O
reference	O
]	O
and	O
image	B-Task
object	I-Task
detection	E-Task
[	O
reference	O
]	O
.	O

The	O
closest	O
previous	O
work	O
in	O
NLP	S-Task
includes	O
the	O
use	O
of	O
convolutional	B-Method
neural	I-Method
networks	E-Method
[	O
reference	O
]	O
and	O
DAG	B-Method
LSTMs	E-Method
[	O
reference	O
]	O
for	O
modelling	B-Task
syntactic	I-Task
structures	E-Task
.	O

Compared	O
to	O
our	O
work	O
,	O
their	O
motivations	O
and	O
network	O
structures	O
are	O
highly	O
different	O
.	O

In	O
particular	O
,	O
the	O
DAG	B-Method
LSTM	E-Method
of	O
[	O
reference	O
]	O
is	O
a	O
natural	O
extension	O
of	O
tree	B-Method
LSTM	E-Method
[	O
reference	O
]	O
,	O
and	O
is	O
sequential	O
rather	O
than	O
parallel	O
in	O
nature	O
.	O

To	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
investigate	O
a	O
graph	B-Method
RNN	E-Method
for	O
encoding	B-Task
sentences	E-Task
,	O
proposing	O
parallel	O
graph	O
states	O
for	O
integrating	O
word	O
-	O
level	O
and	O
sentence	O
-	O
level	O
information	O
.	O

In	O
this	O
perspective	O
,	O
our	O
contribution	O
is	O
similar	O
to	O
that	O
of	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
introducing	O
a	O
neural	B-Method
representation	E-Method


to	O
the	O
NLP	O
literature	O
.	O

section	O
:	O
Model	O
Given	O
a	O
sentence	O
s	O
=	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O

.	O


.	O


,	O
w	O
n	O
,	O
where	O
w	O
i	O
represents	O
the	O
ith	O
word	O
and	O
n	O
is	O
the	O
sentence	O
length	O
,	O
our	O
goal	O
is	O
to	O
find	O
a	O
neural	B-Method
representation	I-Method
of	I-Method
s	E-Method
,	O
which	O
consists	O
of	O
a	O
hidden	O
vector	O
h	O
i	O
for	O
each	O
input	O
word	O
w	O
i	O
,	O
and	O
a	O
global	O
sentence	O
-	O
level	O
hidden	O
vector	O
g.	O
Here	O
h	O
i	O
represents	O
syntactic	O
and	O
semantic	O
features	O
for	O
w	O
i	O
under	O
the	O
sentential	O
context	O
,	O
while	O
g	O
represents	O
features	O
for	O
the	O
whole	O
sentence	O
.	O

Following	O
previous	O
work	O
,	O
we	O
additionally	O
add	O
s	O
and	O
/	O
s	O
to	O
the	O
two	O
ends	O
of	O
the	O
sentence	O
as	O
w	O
0	O
and	O
w	O
n	O
+	O
1	O
,	O
respectively	O
.	O

section	O
:	O
Baseline	O
BiLSTM	S-Method
The	O
baseline	B-Method
BiLSTM	I-Method
model	E-Method
consists	O
of	O
two	O
LSTM	S-Method
components	O
,	O
which	O
process	O
the	O
input	O
in	O
the	O
forward	O
left	O
-	O
to	O
-	O
right	O
and	O
the	O
backward	O
rightto	O
-	O
left	O
directions	O
,	O
respectively	O
.	O

In	O
each	O
direction	O
,	O
the	O
reading	O
of	O
input	O
words	O
is	O
modelled	O
as	O
a	O
recurrent	B-Method
process	E-Method
with	O
a	O
single	O
hidden	O
state	O
.	O

Given	O
an	O
initial	O
value	O
,	O
the	O
state	O
changes	O
its	O
value	O
recurrently	O
,	O
each	O
time	O
consuming	O
an	O
incoming	O
word	O
.	O

Take	O
the	O
forward	O
LSTM	S-Method


component	O
for	O
example	O
.	O

Denoting	O
the	O
initial	O
state	O
as	O
−	O
→	O
h	O
0	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
the	O
recurrent	O
state	O
transition	O
step	O
for	O
calculating	O
−	O
→	O
h	O
1	O
,	O
.	O

.	O


.	O


,	O
−	O
→	O
h	O
n	O
+	O
1	O
is	O
defined	O
as	O
follows	O
[	O
reference	O
]	O
:	O
where	O
x	O
t	O
denotes	O
the	O
word	B-Method
representation	I-Method
of	I-Method
w	I-Method
t	E-Method
;	O
i	O
t	O
,	O
o	O
t	O
,	O
f	O
t	O
and	O
u	O
t	O
represent	O
the	O
values	O
of	O
an	O
input	O
gate	O
,	O
an	O
output	O
gate	O
,	O
a	O
forget	O
gate	O
and	O
an	O
actual	O
input	O
at	O
time	O
step	O
t	O
,	O
respectively	O
,	O
which	O
controls	O
the	O
information	O
flow	O
for	O
a	O
recurrent	B-Method
cell	E-Method
−	O
→	O
c	O
t	O
and	O
the	O
state	O
vector	O
−	O
→	O
h	O
t	O
;	O
W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
i	O
,	O
o	O
,	O
f	O
,	O
u	O
}	O
)	O
are	O
model	O
parameters	O
.	O

σ	S-Method
is	O
the	O
sigmoid	B-Method
function	E-Method
.	O

The	O
backward	O
LSTM	S-Method
component	O
follows	O
the	O
same	O
recurrent	B-Method
state	I-Method
transition	I-Method
process	E-Method


as	O
described	O
in	O
Eq	O
1	O
.	O

Starting	O
from	O
an	O
initial	O
state	O
h	O
n	O
+	O
1	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
it	O
reads	O
the	O
input	O
x	O
n	O
,	O
x	O
n−1	O
,	O
.	O

.	O


.	O


,	O
x	O
0	O
,	O
changing	O
its	O
value	O
to	O
The	O
BiLSTM	S-Method
model	O
uses	O
the	O
concatenated	O
value	O
of	O
−	O
→	O
h	O
t	O
and	O
←	O
−	O
h	O
t	O
as	O
the	O
hidden	O
vector	O
for	O
w	O
t	O
:	O
A	O
single	O
hidden	B-Method
vector	I-Method
representation	I-Method
g	E-Method
of	O
the	O
whole	O
input	O
sentence	O
can	O
be	O
obtained	O
using	O
the	O
final	O
state	O
values	O
of	O
the	O
two	O
LSTM	S-Method
components	O
:	O
Stacked	B-Method
BiLSTM	I-Method
Multiple	I-Method
layers	I-Method
of	I-Method
BiLTMs	E-Method
can	O
be	O
stacked	O
for	O
increased	O
representation	O
power	O
,	O
where	O
the	O
hidden	O
vectors	O
of	O
a	O
lower	O
layer	O
are	O
used	O
as	O
inputs	O
for	O
an	O
upper	O
layer	O
.	O

Different	O
model	O
parameters	O
are	O
used	O
in	O
each	O
stacked	B-Method
BiLSTM	I-Method
layer	E-Method
.	O

section	O
:	O
Sentence	B-Method
-	I-Method
State	I-Method
LSTM	E-Method
Formally	O
,	O
an	O
S	O
-	O
LSTM	S-Method
state	O
at	O
time	O
step	O
t	O
can	O
be	O
denoted	O
by	O
:	O
which	O
consists	O
of	O
a	O
sub	O
state	O
h	O
t	O
i	O
for	O
each	O
word	O
w	O
i	O
and	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
g	O
t	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
uses	O
a	O
recurrent	B-Method
state	I-Method
transition	I-Method
process	E-Method
to	O
model	O
information	O
exchange	O
between	O
sub	O
states	O
,	O
which	O
enriches	O
state	B-Method
representations	E-Method
incrementally	O
.	O

For	O
the	O
initial	O
state	O
H	O
0	O
,	O
we	O
set	O
h	O
0	O
i	O
=	O
g	O
0	O
=	O
h	O
0	O
,	O
where	O
h	O
0	O
is	O
a	O
parameter	O
.	O

The	O
state	O
transition	O
from	O
H	O
t−1	O
to	O
H	O
t	O
consists	O
of	O
sub	O
state	O
transitions	O
from	O
h	O
t−1	O
i	O
to	O
h	O
t	O
i	O
and	O
from	O
g	O
t−1	O
to	O
g	O
t	O
.	O

We	O
take	O
an	O
LSTM	S-Method
structure	O
similar	O
to	O
the	O
baseline	O
BiLSTM	S-Method
for	O
modelling	B-Task
state	I-Task
transition	E-Task




,	O
using	O
a	O
recurrent	O
cell	O
c	O
t	O
i	O
for	O
each	O
w	O
i	O
and	O
a	O
cell	O
c	O
t	O
g	O
for	O
g.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
value	O
of	O
each	O
h	O
t	O
i	O
is	O
computed	O
based	O
on	O
the	O
values	O
of	O
x	O
i	O
,	O
h	O
i	O
+	O
1	O
and	O
g	O
t−1	O
,	O
together	O
with	O
their	O
corresponding	O
cell	O
values	O
:	O
where	O
ξ	O
t	O
i	O
is	O
the	O
concatenation	O
of	O
hidden	O
vectors	O
of	O
a	O
context	O
window	O
,	O
and	O
where	O
f	O
t	O
0	O
,	O
.	O

.	O


.	O


,	O
f	O
t	O
n	O
+	O
1	O
and	O
f	O
t	O
g	O
are	O
gates	O
controlling	O
information	O
from	O
c	O
t−1	O
0	O
,	O
.	O

.	O


.	O


,	O
c	O
t−1	O
n	O
+	O
1	O
and	O
c	O
t−1	O
g	O
,	O
respectively	O
,	O
which	O
are	O
normalised	O
.	O

o	O
t	O
is	O
an	O
output	O
gate	O
from	O
the	O
recurrent	B-Method
cell	E-Method
c	O
t	O
g	O
to	O
g	O
t	O
.	O

W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
g	O
,	O
f	O
,	O
o	O
}	O
)	O
are	O
model	O
parameters	O
.	O

section	O
:	O
Contrast	O
with	O
BiLSTM	S-Method
The	O
difference	O
between	O
S	B-Method
-	I-Method
LSTM	E-Method
and	O
BiLSTM	S-Method
can	O
be	O
understood	O
with	O
respect	O
to	O
their	O
recurrent	O
states	O
.	O

While	O
BiL	B-Method
-	I-Method
STM	E-Method
uses	O
only	O
one	O
state	O
in	O
each	O
direction	O
to	O
represent	O
the	O
subsequence	O
from	O
the	O
beginning	O
to	O
a	O
certain	O
word	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
uses	O
a	O
structural	O
state	O
to	O
represent	O
the	O
full	O
sentence	O
,	O
which	O
consists	O
of	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
and	O
n	O
+	O
2	O
word	O
-	O
level	O
sub	O
states	O
,	O
simultaneously	O
.	O

Different	O
from	O
BiLSTMs	S-Method


,	O
for	O
which	O
h	O
t	O
at	O
different	O
time	O
steps	O
are	O
used	O
to	O
represent	O
w	O
0	O
,	O
.	O

.	O


.	O


,	O
w	O
n	O
+	O
1	O
,	O
respectively	O
,	O
the	O
word	O
-	O
level	O
states	O
h	O
t	O
i	O
and	O
sentence	O
-	O
level	O
state	O
g	O
t	O
of	O
S	B-Method
-	I-Method
LSTMs	E-Method
directly	O
correspond	O
to	O
the	O
goal	O
outputs	O
h	O
i	O
and	O
g	O
,	O
as	O
introduced	O
in	O
the	O
beginning	O
of	O
this	O
section	O
.	O

As	O
t	O
increases	O
from	O
0	O
,	O
h	O
t	O
i	O
and	O
g	O
t	O
are	O
enriched	O
with	O
increasingly	O
deeper	O
context	O
information	O
.	O

From	O
the	O
perspective	O
of	O
information	B-Task
flow	E-Task
,	O
BiL	B-Method
-	I-Method
STM	E-Method
passes	O
information	O
from	O
one	O
end	O
of	O
the	O
sentence	O
to	O
the	O
other	O
.	O

As	O
a	O
result	O
,	O
the	O
number	O
of	O
time	O
steps	O
scales	O
with	O
the	O
size	O
of	O
the	O
input	O
.	O

In	O
contrast	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
allows	O
bi	O
-	O
directional	O
information	O
flow	O
at	O
each	O
word	O
simultaneously	O
,	O
and	O
additionally	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
every	O
wordlevel	O
state	O
.	O

At	O
each	O
step	O
,	O
each	O
h	O
i	O
captures	O
an	O
increasing	O
larger	O
ngram	O
context	O
,	O
while	O
additionally	O
communicating	O
globally	O
to	O
all	O
other	O
h	O
j	O
via	O
g.	O
The	O
optimal	O
number	O
of	O
recurrent	O
steps	O
is	O
decided	O
by	O
the	O
end	B-Metric
-	I-Metric
task	I-Metric
performance	E-Metric
,	O
and	O
does	O
not	O
necessarily	O
scale	O
with	O
the	O
sentence	O
size	O
.	O

As	O
a	O
result	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
can	O
potentially	O
be	O
both	O
more	O
efficient	O
and	O
more	O
accurate	O
compared	O
with	O
BiLSTMs	S-Method
.	O

Increasing	O
window	O
size	O
.	O

By	O
default	O
S	B-Method
-	I-Method
LSTM	E-Method
exchanges	O
information	O
only	O
between	O
neighbouring	O
words	O
,	O
which	O
can	O
be	O
seen	O
as	O
adopting	O
a	O
1	O
-	O
word	O
window	O
on	O
each	O
side	O
.	O

The	O
window	O
size	O
can	O
be	O
extended	O
to	O
2	O
,	O
3	O
or	O
more	O
words	O
in	O
order	O
to	O
allow	O
more	O
communication	O
in	O
a	O
state	O
transition	O
,	O
expediting	O
information	B-Task
exchange	E-Task
.	O

To	O
this	O
end	O
,	O
we	O
modify	O
Eq	O
2	O
,	O
integrating	O
additional	O
context	O
words	O
to	O
ξ	O
t	O
i	O
,	O
with	O
extended	O
gates	O
and	O
cells	O
.	O

For	O
example	O
,	O
with	O
a	O
window	O
size	O
of	O
2	O
,	O
We	O
study	O
the	O
effectiveness	O
of	O
window	O
size	O
in	O
our	O
experiments	O
.	O

Additional	O
sentence	O
-	O
level	O
nodes	O
.	O

By	O
default	O
S	B-Method
-	I-Method
LSTM	E-Method
uses	O
one	O
sentence	O
-	O
level	O
node	O
.	O

One	O
way	O
of	O
enriching	O
the	O
parameter	O
space	O
is	O
to	O
add	O
more	O
sentence	O
-	O
level	O
nodes	O
,	O
each	O
communicating	O
with	O
word	O
-	O
level	O
nodes	O
in	O
the	O
same	O
way	O
as	O
described	O
by	O
Eq	O
3	O
.	O

In	O
addition	O
,	O
different	O
sentence	O
-	O
level	O
nodes	O
can	O
communicate	O
with	O
each	O
other	O
during	O
state	O
transition	O
.	O

When	O
one	O
sentence	O
-	O
level	O
node	O
is	O
used	O
for	O
classification	O
outputs	O
,	O
the	O
other	O
sentencelevel	O
node	O
can	O
serve	O
as	O
hidden	O
memory	O
units	O
,	O
or	O
latent	O
features	O
.	O

We	O
study	O
the	O
effectiveness	O
of	O
multiple	O
sentence	O
-	O
level	O
nodes	O
empirically	O
.	O

section	O
:	O
Task	O
settings	O
We	O
consider	O
two	O
task	O
settings	O
,	O
namely	O
classification	S-Task
and	O
sequence	B-Task
labelling	E-Task
.	O

For	O
classification	S-Task
,	O
g	O
is	O
fed	O
to	O
a	O
softmax	B-Method
classification	I-Method
layer	E-Method
:	O
where	O
y	O
is	O
the	O
probability	O
distribution	O
of	O
output	O
class	O
labels	O
and	O
W	O
c	O
and	O
b	O
c	O
are	O
model	O
parameters	O
.	O

For	O
sequence	B-Task
labelling	E-Task
,	O
each	O
h	O
i	O
can	O
be	O
used	O
as	O
feature	B-Method
representation	E-Method
for	O
a	O
corresponding	O
word	O
w	O
i	O
.	O

External	O
attention	O
It	O
has	O
been	O
shown	O
that	O
summation	O
of	O
hidden	O
states	O
using	O
attention	O
[	O
reference	O
][	O
reference	O
]	O
give	O
better	O
accuracies	S-Metric
compared	O
to	O
using	O
the	O
end	O
states	O
of	O
BiLSTMs	S-Method
.	O

We	O
study	O
the	O
influence	O
of	O
attention	O
on	O
both	O
S	B-Method
-	I-Method
LSTM	E-Method
and	O
BiLSTM	S-Method
for	O
classification	S-Task
.	O

In	O
particular	O
,	O
additive	O
attention	O
(	O
Bahdanau	O
Here	O
W	O
α	O
,	O
u	O
and	O
b	O
α	O
are	O
model	O
parameters	O
.	O

External	O
CRF	O
For	O
sequential	B-Task
labelling	E-Task
,	O
we	O
use	O
a	O
CRF	B-Method
layer	E-Method


on	O
top	O
of	O
the	O
hidden	O
vectors	O
h	O
1	O
,	O
h	O
2	O
,	O
.	O

.	O


.	O


,	O
h	O
n	O
for	O
calculating	O
the	O
conditional	O
probabilities	O
of	O
label	O
sequences	O
[	O
reference	O
]	O
:	O
where	O
W	O
y	O
i−1	O
,	O
y	O
i	O
s	O
and	O
b	O
y	O
i−1	O
,	O
y	O
i	O
s	O
are	O
parameters	O
specific	O
to	O
two	O
consecutive	O
labels	O
y	O
i−1	O
and	O
y	O
i	O
.	O

For	O
training	S-Task
,	O
standard	O
log	B-Method
-	I-Method
likelihood	I-Method
loss	E-Method
is	O
used	O
with	O
L	B-Method
2	I-Method
regularization	E-Method
given	O
a	O
set	O
of	O
gold	O
-	O
standard	O
instances	O
.	O

section	O
:	O
Experiments	O
We	O
empirically	O
compare	O
S	B-Method
-	I-Method
LSTMs	E-Method
and	O
BiLSTMs	S-Method
on	O
different	O
classification	B-Task
and	I-Task
sequence	I-Task
labelling	I-Task
tasks	E-Task
.	O

All	O
experiments	O
are	O
conducted	O
using	O
a	O
GeForce	B-Method
GTX	I-Method
1080	I-Method
GPU	E-Method
with	O
8	O
GB	O
memory	O
.	O

[	O
reference	O
]	O
)	O
.	O

Statistics	O
of	O
the	O
four	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
.	O

Hyperparameters	S-Method
.	O

We	O
initialise	O
word	O
embeddings	O
using	O
GloVe	S-Method
[	O
reference	O
]	O
)	O
300	B-Method
dimensional	I-Method
embeddings	E-Method
.	O

1	O
Embeddings	S-Method
are	O
finetuned	O
during	O
model	B-Method
training	E-Method
for	O
all	O
tasks	O
.	O

Dropout	S-Method
[	O
reference	O
]	O
)	O
is	O
applied	O
to	O
embedding	O
hidden	O
states	O
,	O
with	O
a	O
rate	O
of	O
0.5	O
.	O

All	O
models	O
are	O
optimised	O
using	O
the	O
Adam	B-Method
optimizer	E-Method
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
,	O
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.001	O
and	O
a	O
decay	B-Metric
rate	I-Metric
of	I-Metric
0.97	E-Metric
.	O

Gradients	O
are	O
clipped	O
at	O
3	O
and	O
a	O
batch	O
size	O
of	O
10	O
is	O
adopted	O
.	O

Sentences	O
with	O
similar	O
lengths	O
are	O
batched	O
together	O
.	O

The	O
L2	O
regularization	O
parameter	O
is	O
set	O
to	O
0.001	O
.	O

section	O
:	O
Development	O
Experiments	O
We	O
use	O
the	O
movie	B-Material
review	E-Material
development	O
data	O
to	O
investigate	O
different	O
configurations	O
of	O
S	B-Method
-	I-Method
LSTMs	E-Method
and	O
BiLSTMs	S-Method
.	O

For	O
S	B-Method
-	I-Method
LSTMs	E-Method
,	O
the	O
default	O
configuration	O
uses	O
s	O
and	O
/	O
s	O
words	O
for	O
augmenting	O
words	O
Hyperparameters	O
:	O
Table	O
2	O
shows	O
the	O
development	O
results	O
of	O
various	O
S	O
-	O
LSTM	S-Method
settings	O
,	O
where	O
Time	O
refers	O
to	O
training	O
time	O
per	O
epoch	O
.	O

Without	O
the	O
sentence	O
-	O
level	O
node	O
,	O
the	O
accuracy	S-Metric
of	O
S	B-Method
-	I-Method
LSTM	E-Method
drops	O
to	O
81.76	O
%	O
,	O
demonstrating	O
the	O
necessity	O
of	O
global	B-Task
information	I-Task
exchange	E-Task
.	O

Adding	O
one	O
additional	O
sentence	O
-	O
level	O
node	O
as	O
described	O
in	O
Section	O
3.2	O
does	O
not	O
lead	O
to	O
accuracy	S-Metric
improvements	O
,	O
although	O
the	O
number	O
of	O
parameters	O
and	O
decoding	B-Metric
time	E-Metric
increase	O
accordingly	O
.	O

As	O
a	O
result	O
,	O
we	O
use	O
only	O
1	O
sentence	O
-	O
level	O
node	O
for	O
the	O
remaining	O
experiments	O
.	O

The	O
accuracies	S-Metric
of	O
S	B-Method
-	I-Method
LSTM	E-Method
increases	O
as	O
the	O
hidden	O
layer	O
size	O
for	O
each	O
node	O
increases	O
from	O
100	O
to	O
300	O
,	O
but	O
does	O
not	O
further	O
increase	O
when	O
the	O
size	O
increases	O
beyond	O
300	O
.	O

We	O
fix	O
the	O
hidden	O
size	O
to	O
300	O
accordingly	O
.	O

Without	O
using	O
s	O
and	O
/	B-Method
s	E-Method
,	O
the	O
performance	O
of	O
S	B-Method
-	I-Method
LSTM	E-Method
drops	O
from	O
82.64	O
%	O
to	O
82.36	O
%	O
,	O
showing	O
the	O
effectiveness	O
of	O
having	O
these	O
additional	O
nodes	O
.	O

Hyperparameters	S-Method
for	O
BiLSTM	S-Method
models	O
are	O
also	O
set	O
according	O
to	O
the	O
development	O
data	O
,	O
which	O
we	O
omit	O
here	O
.	O

State	O
transition	O
.	O

In	O
Table	O
2	O
,	O
the	O
number	O
of	O
recurrent	O
state	O
transition	O
steps	O
of	O
S	B-Method
-	I-Method
LSTM	E-Method
is	O
decided	O
according	O
to	O
the	O
best	O
development	O
performance	O
.	O

Figure	O
2	O
draws	O
the	O
development	O
accuracies	S-Metric
of	O
SLSTMs	S-Method
with	O
various	O
window	O
sizes	O
against	O
the	O
number	O
of	O
recurrent	O
steps	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
when	O
the	O
number	O
of	O
time	O
steps	O
increases	O
from	O
1	O
to	O
11	O
,	O
the	O
accuracies	S-Metric
generally	O
increase	O
,	O
before	O
reaching	O
a	O
maximum	O
value	O
.	O

This	O
shows	O
the	O
effectiveness	O
of	O
recurrent	B-Task
information	I-Task
exchange	E-Task
in	O
S	O
-	O
LSTM	S-Method
state	O
transition	O
.	O

On	O
the	O
other	O
hand	O
,	O
no	O
significant	O
differences	O
are	O
observed	O
on	O
the	O
peak	O
accuracies	S-Metric
given	O
by	O
different	O
window	O
sizes	O
,	O
although	O
a	O
larger	O
window	O
size	O
(	O
e.g.	O
4	O
)	O
generally	O
results	O
in	O
faster	O
plateauing	O
.	O

This	O
can	O
be	O
be	O
explained	O
by	O
the	O
intuition	O
that	O
information	O
exchange	O
between	O
distant	O
nodes	O
can	O
be	O
achieved	O
using	O
more	O
recurrent	O
steps	O
under	O
a	O
smaller	O
window	O
size	O
,	O
as	O
can	O
be	O
achieved	O
using	O
fewer	O
steps	O
under	O
a	O
larger	O
window	O
size	O
.	O

Considering	O
efficiency	O
,	O
we	O
choose	O
a	O
window	O
size	O
of	O
1	O
for	O
the	O
remaining	O
experiments	O
,	O
setting	O
the	O
number	O
of	O
recurrent	O
steps	O
to	O
9	O
according	O
to	O
Figure	O
2	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
vs	O
BiLSTM	S-Method
:	O
As	O
shown	O
in	O
Table	O
3	O
,	O
BiLSTM	S-Method
gives	O
significantly	O
better	O
accuracies	S-Metric
compared	O
to	O
uni	O
-	O
directional	O
LSTM	S-Method
2	O
,	O
with	O
the	O
training	B-Metric
time	E-Metric
per	O
epoch	O
growing	O
from	O
67	O
seconds	O
to	O
106	O
seconds	O
.	O

Stacking	O
2	O
layers	O
of	O
BiLSTM	S-Method
gives	O
further	O
improvements	O
to	O
development	O
results	O
,	O
with	O
a	O
larger	O
time	O
of	O
207	O
seconds	O
.	O

3	O
layers	O
of	O
stacked	O
BiLSTM	S-Method
does	O
not	O
further	O
improve	O
the	O
results	O
.	O

In	O
contrast	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
gives	O
a	O
development	O
result	O
of	O
82.64	O
%	O
,	O
which	O
is	O
significantly	O
better	O
compared	O
to	O
2	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiLSTM	E-Method
,	O
with	O
a	O
smaller	O
number	O
of	O
model	O
parameters	O
and	O
a	O
shorter	O
time	O
of	O
65	O
seconds	O
.	O

We	O
additionally	O
make	O
comparisons	O
with	O
stacked	B-Method
CNNs	E-Method
and	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
shown	O
in	O
Table	O
3	O
(	O
the	O
CNN	O
and	O
Transformer	O
rows	O
)	O
,	O
where	O
N	O
indicates	O
the	O
number	O
of	O
attention	O
layers	O
.	O

CNN	S-Method
is	O
the	O
most	O
efficient	O
among	O
all	O
models	O
compared	O
,	O
with	O
the	O
smallest	O
model	O
size	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
3	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
CNN	E-Method
gives	O
an	O
accuracy	S-Metric
of	O
81.46	O
%	O
,	O
which	O
is	O
also	O
the	O
lowest	O
compared	O
with	O
BiLSTM	S-Method
,	O
hierarchical	B-Method
attention	E-Method
and	O
S	B-Method
-	I-Method
LSTM	E-Method
.	O

The	O
best	O
performance	O
of	O
hierarchical	B-Task
attention	E-Task
is	O
between	O
single	B-Method
-	I-Method
layer	E-Method
and	O
two	B-Method
-	I-Method
layer	I-Method
BiLSTMs	E-Method
in	O
terms	O
of	O
both	O
accuracy	S-Metric
and	O
efficiency	S-Metric
.	O

S	B-Method
-	I-Method
LSTM	E-Method
gives	O
significantly	O
better	O
accuracies	S-Metric
compared	O
with	O
both	O
CNN	S-Method
and	O
hierarchical	B-Method
attention	E-Method
.	O

Influence	O
of	O
external	B-Method
attention	I-Method
mechanism	E-Method
.	O

Table	O
3	O
additionally	O
shows	O
the	O
results	O
of	O
BiLSTM	S-Method
and	O
S	B-Method
-	I-Method
LSTM	E-Method
when	O
external	O
attention	O
is	O
used	O
as	O
described	O
in	O
Section	O
3.3	O
.	O

Attention	S-Method
leads	O
to	O
improved	O
accuracies	S-Metric
for	O
both	O
BiLSTM	S-Method
and	O
S	B-Method
-	I-Method
LSTM	E-Method
in	O
classification	S-Task
,	O
with	O
S	B-Method
-	I-Method
LSTM	E-Method
still	O
outperforming	O
BiLSTM	S-Method
significantly	O
.	O

The	O
result	O
suggests	O
that	O
external	B-Method
techniques	E-Method
such	O
as	O
attention	O
can	O
play	O
orthogonal	O
roles	O
compared	O
with	O
internal	O
recurrent	O
structures	O
,	O
therefore	O
benefiting	O
both	O
BiLSTMs	S-Method
and	O
S	B-Method
-	I-Method
LSTMs	E-Method
.	O

Similar	O
observations	O
are	O
found	O
using	O
external	B-Method
CRF	I-Method
layers	E-Method
for	O
sequence	B-Task
labelling	E-Task
.	O

section	O
:	O
Final	O
Results	O
for	O
Classification	S-Task
The	O
final	O
results	O
on	O
the	O
movie	B-Material
review	E-Material
and	O
rich	O
text	O
classification	O
datasets	O
are	O
shown	O
in	O
Tables	O
4	O
and	O
5	O
,	O
respectively	O
.	O

In	O
addition	O
to	O
training	B-Metric
time	E-Metric
per	O
epoch	O
,	O
test	B-Metric
times	E-Metric
are	O
additionally	O
reported	O
.	O

We	O
use	O
the	O
best	O
settings	O
on	O
the	O
movie	B-Material
review	E-Material
development	O
dataset	O
for	O
both	O
S	B-Method
-	I-Method
LSTMs	E-Method
and	O
BiLSTMs	S-Method
.	O

The	O
step	O
number	O
for	O
S	B-Method
-	I-Method
LSTMs	E-Method
is	O
set	O
to	O
9	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
the	O
final	O
results	O
on	O
the	O
movie	B-Material
review	I-Material
dataset	E-Material
are	O
consistent	O
with	O
the	O
development	O
results	O
,	O
where	O
S	B-Method
-	I-Method
LSTM	E-Method
outperforms	O
BiL	B-Method
-	I-Method
STM	E-Method
significantly	O
,	O
with	O
a	O
faster	O
speed	O
.	O

Observations	O
on	O
CNN	S-Method
and	O
hierarchical	B-Task
attention	E-Task
are	O
consistent	O
with	O
the	O
development	O
results	O
.	O

S	B-Method
-	I-Method
LSTM	E-Method
also	O
gives	O
highly	O
competitive	O
results	O
when	O
compared	O
with	O
existing	O
methods	O
in	O
the	O
literature	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
among	O
the	O
16	O
datasets	O
of	O
[	O
reference	O
]	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
gives	O
the	O
best	O
results	O
on	O
12	O
,	O
compared	O
with	O
BiLSTM	S-Method
and	O
2	O
layered	O
BiL	B-Method
-	I-Method
STM	E-Method
models	O
.	O

The	O
average	O
accuracy	S-Metric
of	O
S	B-Method
-	I-Method
LSTM	E-Method
is	O
85.6	O
%	O
,	O
significantly	O
higher	O
compared	O
with	O
84.9	O
%	O
by	O
2	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiLSTM	E-Method
.	O

3	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiL	I-Method
-	I-Method
STM	E-Method
gives	O
an	O
average	O
accuracy	S-Metric
of	O
84.57	O
%	O
,	O
which	O
is	O
lower	O
compared	O
to	O
a	O
2	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiLSTM	E-Method
,	O
with	O
a	O
training	B-Metric
time	E-Metric
per	O
epoch	O
of	O
423.6	O
seconds	O
.	O

The	O
relative	B-Metric
speed	I-Metric
advantage	E-Metric
of	O
S	B-Method
-	I-Method
LSTM	E-Method
over	O
BiLSTM	S-Method
is	O
larger	O
on	O
the	O
16	O
datasets	O
as	O
compared	O
to	O
the	O
movie	B-Material
review	E-Material
test	O
test	O
.	O

This	O
is	O
because	O
the	O
average	O
length	O
of	O
inputs	O
is	O
larger	O
on	O
the	O
16	O
datasets	O
(	O
see	O
Section	O
4.5	O
)	O
.	O

section	O
:	O
Final	O
Results	O
for	O
Sequence	B-Task
Labelling	E-Task
Bi	B-Method
-	I-Method
directional	I-Method
RNN	I-Method
-	I-Method
CRF	I-Method
structures	E-Method
,	O
and	O
in	O
particular	O
BiLSTM	S-Method
-	O
CRFs	O
,	O
have	O
achieved	O
the	O
state	O
of	O
the	O
art	O
in	O
the	O
literature	O
for	O
sequence	B-Task
labelling	I-Task
tasks	E-Task
,	O
including	O
POS	B-Task
-	I-Task
tagging	E-Task
and	O
NER	S-Task
.	O

We	O
compare	O
S	B-Method
-	I-Method
LSTM	I-Method
-	I-Method
CRF	E-Method
with	O
BiLSTM	B-Method
-	I-Method
CRF	E-Method
for	O
sequence	B-Task
labelling	E-Task
,	O
using	O
the	O
same	O
settings	O
as	O
decided	O
on	O
the	O
movie	B-Material
review	E-Material
development	O
experiments	O
for	O
both	O
BiLSTMs	S-Method
and	O
S	B-Method
-	I-Method
LSTMs	E-Method
.	O

For	O
the	O
latter	O
,	O
we	O
decide	O
the	O
number	O
of	O
recurrent	O
steps	O
on	O
the	O
respective	O
development	O
sets	O
for	O
sequence	B-Task
labelling	E-Task
.	O

The	O
POS	B-Metric
accuracies	E-Metric
and	O
NER	B-Metric
F1	I-Metric
-	I-Metric
scores	E-Metric
against	O
the	O
number	O
of	O
recurrent	O
steps	O
are	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
respectively	O
.	O

For	O
POS	B-Task
tagging	E-Task
,	O
the	O
best	O
step	O
number	O
is	O
set	O
to	O
7	O
,	O
with	O
a	O
development	O
accuracy	S-Metric
of	O
97.58	O
%	O
.	O

For	O
NER	S-Task
,	O
the	O
step	O
number	O
is	O
set	O
to	O
9	O
,	O
with	O
a	O
development	O
F1	B-Metric
-	I-Metric
score	E-Metric
of	O
94.98	O
%	O
.	O

As	O
can	O
be	O
seen	O
in	O
(	O
Table	O
7	O
)	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
gives	O
an	O
F1	B-Metric
-	I-Metric
score	E-Metric
of	O
91.57	O
%	O
on	O
the	O
CoNLL	B-Material
test	I-Material
set	E-Material
,	O
which	O
is	O
significantly	O
better	O
compared	O
with	O
BiLSTMs	S-Method
.	O

Stacking	O
more	O
layers	O
of	O
BiLSTMs	S-Method
leads	O
to	O
slightly	O
better	O
F1	B-Metric
-	I-Metric
scores	E-Metric
compared	O
with	O
a	O
single	B-Method
-	I-Method
layer	I-Method
BiL	I-Method
-	I-Method
STM	E-Method
.	O

Our	O
BiLSTM	S-Method
results	O
are	O
comparable	O
to	O
the	O
results	O
reported	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
who	O
also	O
use	O
bidirectional	B-Method
RNN	I-Method
-	I-Method
CRF	I-Method
structures	E-Method
.	O

In	O
contrast	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
gives	O
the	O
best	O
reported	O
results	O
under	O
the	O
same	O
settings	O
.	O

In	O
the	O
second	O
section	O
of	O
Table	O
7	O
learning	O
using	O
additional	O
language	O
model	O
objectives	O
,	O
obtaining	O
an	O
F	B-Metric
-	I-Metric
score	E-Metric
of	O
86.26	O
%	O
;	O
[	O
reference	O
]	O
leverage	O
character	B-Method
-	I-Method
level	I-Method
language	I-Method
models	E-Method
,	O
obtaining	O
an	O
F	B-Metric
-	I-Metric
score	E-Metric
of	O
91.93	O
%	O
,	O
which	O
is	O
the	O
current	O
best	O
result	O
on	O
the	O
dataset	O
.	O

All	O
the	O
three	O
models	O
are	O
based	O
on	O
BiLSTM	B-Method
-	I-Method
CRF	E-Method
.	O

On	O
the	O
other	O
hand	O
,	O
these	O
semi	B-Method
-	I-Method
supervised	I-Method
learning	I-Method
techniques	E-Method
are	O
orthogonal	O
to	O
our	O
work	O
,	O
and	O
can	O
potentially	O
be	O
used	O
for	O
S	B-Method
-	I-Method
LSTM	E-Method
also	O
.	O

section	O
:	O
Analysis	O
Figure	O
4	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
the	O
accuracies	S-Metric
against	O
the	O
sentence	O
length	O
on	O
the	O
movie	B-Material
review	E-Material
and	O
CoNLL	B-Material
datasets	E-Material
,	O
respectively	O
,	O
where	O
test	O
samples	O
are	O
binned	O
in	O
batches	O
of	O
80	O
.	O

We	O
find	O
that	O
the	O
performances	O
of	O
both	O
S	B-Method
-	I-Method
LSTM	E-Method
and	O
BiLSTM	S-Method
decrease	O
as	O
the	O
sentence	O
length	O
increases	O
.	O

On	O
the	O
other	O
hand	O
,	O
S	B-Method
-	I-Method
LSTM	E-Method
demonstrates	O
relatively	O
better	O
robustness	S-Metric
compared	O
to	O
BiLSTMs	S-Method
.	O

This	O
confirms	O
our	O
intuition	O
that	O
a	O
sentence	O
-	O
level	O
node	O
can	O
facilitate	O
better	O
non	B-Task
-	I-Task
local	I-Task
communication	E-Task
.	O

these	O
comparisons	O
,	O
we	O
mix	O
all	O
training	O
instances	O
,	O
order	O
them	O
by	O
the	O
size	O
,	O
and	O
put	O
them	O
into	O
10	O
equal	O
groups	O
,	O
the	O
medium	O
sentence	O
lengths	O
of	O
which	O
are	O
shown	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
the	O
speed	B-Metric
advantage	E-Metric
of	O
S	B-Method
-	I-Method
LSTM	E-Method
is	O
larger	O
when	O
the	O
size	O
of	O
the	O
input	O
text	O
increases	O
,	O
thanks	O
to	O
a	O
fixed	O
number	O
of	O
recurrent	O
steps	O
.	O

Similar	O
to	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
there	O
is	O
a	O
relative	O
disadvantage	O
of	O
S	B-Method
-	I-Method
LSTM	E-Method
in	O
comparison	O
with	O
BiLSTM	S-Method
,	O
which	O
is	O
that	O
the	O
memory	B-Metric
consumption	E-Metric
is	O
relatively	O
larger	O
.	O

For	O
example	O
,	O
over	O
the	O
movie	B-Material
review	E-Material
development	O
set	O
,	O
the	O
actual	O
GPU	B-Metric
memory	I-Metric
consumption	E-Metric
by	O
S	B-Method
-	I-Method
LSTM	E-Method
,	O
BiLSTM	S-Method
,	O
2	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiLSTM	E-Method
and	O
4	B-Method
-	I-Method
layer	I-Method
stacked	I-Method
BiLSTM	E-Method
are	O
252	O
M	O
,	O
89	O
M	O
,	O
146	O
M	O
and	O
253	O
M	O
,	O
respectively	O
.	O

This	O
is	O
due	O
to	O
the	O
fact	O
that	O
computation	O
is	O
performed	O
in	O
parallel	O
by	O
S	B-Method
-	I-Method
LSTM	E-Method
and	O
hierarchical	B-Method
attention	E-Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
investigated	O
S	B-Method
-	I-Method
LSTM	E-Method
,	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
for	O
encoding	B-Task
sentences	E-Task
,	O
which	O
offers	O
richer	O
contextual	O
information	O
exchange	O
with	O
more	O
parallelism	O
compared	O
to	O
BiLSTMs	S-Method
.	O

Results	O
on	O
a	O
range	O
of	O
classification	B-Task
and	I-Task
sequence	I-Task
labelling	I-Task
tasks	E-Task
show	O
that	O
S	B-Method
-	I-Method
LSTM	E-Method
outperforms	O
BiLSTMs	S-Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
demonstrating	O
that	O
S	B-Method
-	I-Method
LSTM	E-Method
can	O
be	O
a	O
useful	O
addition	O
to	O
the	O
neural	B-Method
toolbox	E-Method
for	O
encoding	B-Task
sentences	E-Task
.	O

The	O
structural	O
nature	O
in	O
S	O
-	O
LSTM	S-Method
states	O
allows	O
straightforward	O
extension	O
to	O
tree	O
structures	O
,	O
resulting	O
in	O
highly	O
parallelisable	B-Method
tree	I-Method
LSTMs	E-Method
.	O

We	O
leave	O
such	O
investigation	O
to	O
future	O
work	O
.	O

Next	O
directions	O
also	O
include	O
the	O
investigation	O
of	O
S	B-Method
-	I-Method
LSTM	E-Method
to	O
more	O
NLP	B-Task
tasks	E-Task
,	O
such	O
as	O
machine	B-Task
translation	E-Task
.	O

section	O
:	O
section	O
:	O
Acknowledge	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
and	O
thoughtful	O
comments	O
.	O

section	O
:	O
document	O
:	O
The	O
Variational	B-Method
Fair	I-Method
Autoencoder	E-Method
We	O
investigate	O
the	O
problem	O
of	O
learning	B-Task
representations	E-Task
that	O
are	O
invariant	O
to	O
certain	O
nuisance	O
or	O
sensitive	O
factors	O
of	O
variation	O
in	O
the	O
data	O
while	O
retaining	O
as	O
much	O
of	O
the	O
remaining	O
information	O
as	O
possible	O
.	O

Our	O
model	O
is	O
based	O
on	O
a	O
variational	B-Method
autoencoding	I-Method
architecture	E-Method
kingma2013auto	O
,	O
rezende2014stochastic	O
with	O
priors	O
that	O
encourage	O
independence	O
between	O
sensitive	O
and	O
latent	O
factors	O
of	O
variation	O
.	O

Any	O
subsequent	O
processing	O
,	O
such	O
as	O
classification	S-Task
,	O
can	O
then	O
be	O
performed	O
on	O
this	O
purged	O
latent	B-Method
representation	E-Method
.	O

To	O
remove	O
any	O
remaining	O
dependencies	O
we	O
incorporate	O
an	O
additional	O
penalty	O
term	O
based	O
on	O
the	O
“	O
Maximum	B-Method
Mean	I-Method
Discrepancy	I-Method
”	E-Method
(	O
MMD	S-Method
)	O
gretton2006kernel	O
measure	O
.	O

We	O
discuss	O
how	O
these	O
architectures	O
can	O
be	O
efficiently	O
trained	O
on	O
data	O
and	O
show	O
in	O
experiments	O
that	O
this	O
method	O
is	O
more	O
effective	O
than	O
previous	O
work	O
in	O
removing	O
unwanted	O
sources	O
of	O
variation	O
while	O
maintaining	O
informative	B-Method
latent	I-Method
representations	E-Method
.	O

section	O
:	O
Introduction	O
In	O
“	O
Representation	B-Method
Learning	E-Method
”	O
one	O
tries	O
to	O
find	O
representations	O
of	O
the	O
data	O
that	O
are	O
informative	O
for	O
a	O
particular	O
task	O
while	O
removing	O
the	O
factors	O
of	O
variation	O
that	O
are	O
uninformative	O
and	O
are	O
typically	O
detrimental	O
for	O
the	O
task	O
under	O
consideration	O
.	O

Uninformative	O
dimensions	O
are	O
often	O
called	O
“	O
noise	O
”	O
or	O
“	O
nuisance	O
variables	O
”	O
while	O
informative	O
dimensions	O
are	O
usually	O
called	O
latent	O
or	O
hidden	O
factors	O
of	O
variation	O
.	O

Many	O
machine	B-Method
learning	I-Method
algorithms	E-Method
can	O
be	O
understood	O
in	O
this	O
way	O
:	O
principal	B-Method
component	I-Method
analysis	E-Method
,	O
nonlinear	B-Method
dimensional	I-Method
reduction	E-Method
and	O
latent	B-Method
Dirichlet	I-Method
allocation	E-Method
are	O
all	O
models	O
that	O
extract	O
informative	O
factors	O
(	O
dimensions	O
,	O
causes	O
,	O
topics	O
)	O
of	O
the	O
data	O
which	O
can	O
often	O
be	O
used	O
to	O
visualize	O
the	O
data	O
.	O

On	O
the	O
other	O
hand	O
,	O
linear	B-Method
discriminant	I-Method
analysis	E-Method
and	O
deep	B-Method
(	I-Method
convolutional	I-Method
)	I-Method
neural	I-Method
nets	E-Method
learn	O
representations	O
that	O
are	O
good	O
for	O
classification	S-Task
.	O

In	O
this	O
paper	O
we	O
consider	O
the	O
case	O
where	O
we	O
wish	O
to	O
learn	O
latent	B-Method
representations	E-Method
where	O
(	O
almost	O
)	O
all	O
of	O
the	O
information	O
about	O
certain	O
known	O
factors	O
of	O
variation	O
are	O
purged	O
from	O
the	O
representation	O
while	O
still	O
retaining	O
as	O
much	O
information	O
about	O
the	O
data	O
as	O
possible	O
.	O

In	O
other	O
words	O
,	O
we	O
want	O
a	O
latent	B-Method
representation	E-Method
that	O
is	O
maximally	O
informative	O
about	O
an	O
observed	O
random	O
variable	O
(	O
e.g.	O
,	O
class	O
label	O
)	O
while	O
minimally	O
informative	O
about	O
a	O
sensitive	O
or	O
nuisance	O
variable	O
.	O

By	O
treating	O
as	O
a	O
sensitive	O
variable	O
,	O
i.e.	O
is	O
correlated	O
with	O
our	O
objective	O
,	O
we	O
are	O
dealing	O
with	O
“	O
fair	B-Method
representations	E-Method
”	O
,	O
a	O
problem	O
previously	O
considered	O
by	O
.	O

If	O
we	O
instead	O
treat	O
as	O
a	O
nuisance	O
variable	O
we	O
are	O
dealing	O
with	O
“	O
domain	B-Task
adaptation	E-Task
”	O
,	O
in	O
other	O
words	O
by	O
removing	O
the	O
domain	O
from	O
our	O
representations	O
we	O
will	O
obtain	O
improved	O
performance	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
a	O
novel	O
model	O
based	O
on	O
deep	O
variational	B-Method
autoencoders	E-Method
(	O
VAE	S-Method
)	O
kingma2013auto	O
,	O
rezende2014stochastic	O
.	O

These	O
models	O
can	O
naturally	O
encourage	O
separation	O
between	O
latent	O
variables	O
and	O
sensitive	O
variables	O
by	O
using	O
factorized	O
priors	O
.	O

However	O
,	O
some	O
dependencies	O
may	O
still	O
remain	O
when	O
mapping	O
data	O
-	O
cases	O
to	O
their	O
hidden	B-Method
representation	E-Method
using	O
the	O
variational	O
posterior	O
,	O
which	O
we	O
stamp	O
out	O
using	O
a	O
“	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
”	O
gretton2006kernel	O
term	O
that	O
penalizes	O
differences	O
between	O
all	O
order	O
moments	O
of	O
the	O
marginal	O
posterior	O
distributions	O
and	O
(	O
for	O
a	O
discrete	O
RV	O
)	O
.	O

In	O
experiments	O
we	O
show	O
that	O
this	O
combined	O
approach	O
is	O
highly	O
successful	O
in	O
learning	O
representations	S-Task
that	O
are	O
devoid	O
of	O
unwanted	O
information	O
while	O
retaining	O
as	O
much	O
information	O
as	O
possible	O
from	O
what	O
remains	O
.	O

section	O
:	O
Learning	B-Task
Invariant	I-Task
Representations	E-Task
[	O
scale=0.75	O
]	O
unsupervised.pdf	O
[	O
scale=0.75	O
]	O
semisupervised.pdf	O
subsection	O
:	O
Unsupervised	B-Method
model	E-Method
Factoring	O
out	O
undesired	O
variations	O
from	O
the	O
data	O
can	O
be	O
easily	O
formulated	O
as	O
a	O
general	O
probabilistic	B-Method
model	E-Method
which	O
admits	O
two	O
distinct	O
(	O
independent	O
)	O
“	O
sources	O
”	O
;	O
an	O
observed	O
variable	O
,	O
which	O
denotes	O
the	O
variations	O
that	O
we	O
want	O
to	O
remove	O
,	O
and	O
a	O
continuous	O
latent	O
variable	O
which	O
models	O
all	O
the	O
remaining	O
information	O
.	O

This	O
generative	B-Method
process	E-Method
can	O
be	O
formally	O
defined	O
as	O
:	O
where	O
is	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

With	O
this	O
formulation	O
we	O
explicitly	O
encode	O
a	O
notion	O
of	O
‘	O
invariance	O
’	O
in	O
our	O
model	O
,	O
since	O
the	O
latent	B-Method
representation	E-Method
is	O
marginally	O
independent	O
of	O
the	O
factors	O
of	O
variation	O
.	O

Therefore	O
the	O
problem	O
of	O
finding	O
an	O
invariant	B-Method
representation	E-Method
for	O
a	O
data	O
point	O
and	O
variation	O
can	O
be	O
cast	O
as	O
performing	O
inference	S-Task
on	O
this	O
graphical	B-Method
model	E-Method
and	O
obtaining	O
the	O
posterior	O
distribution	O
of	O
,	O
.	O

For	O
our	O
model	O
we	O
will	O
employ	O
a	O
variational	B-Method
autoencoder	I-Method
architecture	E-Method
kingma2013auto	O
,	O
rezende2014stochastic	O
;	O
namely	O
we	O
will	O
parametrize	O
the	O
generative	B-Method
model	E-Method
(	O
decoder	S-Method
)	O
and	O
the	O
variational	B-Method
posterior	E-Method
(	O
encoder	S-Method
)	O
as	O
(	O
deep	O
)	O
neural	B-Method
networks	E-Method
which	O
accept	O
as	O
inputs	O
and	O
respectively	O
and	O
produce	O
the	O
parameters	O
of	O
each	O
distribution	O
after	O
a	O
series	O
of	O
non	O
-	O
linear	O
transformations	O
.	O

Both	O
the	O
model	O
(	O
)	O
and	O
variational	B-Method
(	E-Method
)	O
parameters	O
will	O
be	O
jointly	O
optimized	O
with	O
the	O
SGVB	S-Method
kingma2013auto	O
algorithm	O
according	O
to	O
a	O
lower	O
bound	O
on	O
the	O
log	B-Metric
-	I-Metric
likelihood	E-Metric
.	O

This	O
parametrization	O
will	O
allow	O
us	O
to	O
capture	O
most	O
of	O
the	O
salient	O
information	O
of	O
in	O
our	O
embedding	O
.	O

Furthermore	O
the	O
distributed	B-Method
representation	E-Method
of	O
a	O
neural	B-Method
network	E-Method
would	O
allow	O
us	O
to	O
better	O
resolve	O
the	O
dependencies	O
between	O
and	O
thus	O
yielding	O
a	O
better	O
disentangling	O
between	O
the	O
independent	O
factors	O
and	O
.	O

By	O
choosing	O
a	O
Gaussian	B-Method
posterior	E-Method
and	O
standard	O
isotropic	B-Method
Gaussian	I-Method
prior	E-Method
we	O
can	O
obtain	O
the	O
following	O
lower	O
bound	O
:	O
with	O
and	O
with	O
being	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

subsection	O
:	O
Semi	B-Method
-	I-Method
Supervised	I-Method
model	E-Method
Factoring	O
out	O
variations	O
in	O
an	O
unsupervised	O
way	O
can	O
however	O
be	O
harmful	O
in	O
cases	O
where	O
we	O
want	O
to	O
use	O
this	O
invariant	B-Method
representation	E-Method
for	O
a	O
subsequent	O
prediction	B-Task
task	E-Task
.	O

In	O
particular	O
if	O
we	O
have	O
a	O
situation	O
where	O
the	O
nuisance	O
variable	O
and	O
the	O
actual	O
label	O
are	O
correlated	O
,	O
then	O
training	O
an	O
unsupervised	B-Method
model	E-Method
could	O
yield	O
random	O
or	O
degenerate	O
representations	O
with	O
respect	O
to	O
.	O

Therefore	O
it	O
is	O
more	O
appropriate	O
to	O
try	O
to	O
“	O
inject	O
”	O
the	O
information	O
about	O
the	O
label	O
during	O
the	O
feature	B-Method
extraction	I-Method
phase	E-Method
.	O

This	O
can	O
be	O
quite	O
simply	O
achieved	O
by	O
introducing	O
a	O
second	O
“	O
layer	O
”	O
of	O
latent	O
variables	O
to	O
our	O
generative	B-Method
model	E-Method
where	O
we	O
try	O
to	O
correlate	O
with	O
the	O
prediction	B-Task
task	E-Task
.	O

Assuming	O
that	O
the	O
invariant	O
features	O
are	O
now	O
called	O
we	O
enrich	O
the	O
generative	O
story	O
by	O
similarly	O
providing	O
two	O
distinct	O
(	O
independent	O
)	O
sources	O
for	O
;	O
a	O
discrete	O
(	O
in	O
case	O
of	O
classification	B-Task
)	I-Task
variable	E-Task
which	O
denotes	O
the	O
label	O
of	O
the	O
data	O
point	O
and	O
a	O
continuous	O
latent	O
variable	O
which	O
encodes	O
the	O
variation	O
on	O
that	O
is	O
not	O
explained	O
by	O
(	O
dependent	O
noise	O
)	O
.	O

The	O
process	O
now	O
can	O
be	O
formally	O
defined	O
as	O
:	O
Similarly	O
to	O
the	O
unsupervised	B-Task
case	E-Task
we	O
use	O
a	O
variational	B-Method
auto	I-Method
-	I-Method
encoder	E-Method
and	O
jointly	O
optimize	O
the	O
variational	O
and	O
model	O
parameters	O
.	O

The	O
lower	O
bound	O
now	O
becomes	O
:	O
where	O
we	O
assume	O
that	O
the	O
posterior	O
is	O
factorized	O
as	O
,	O
and	O
where	O
:	O
with	O
again	O
being	O
an	O
appropriate	O
probability	O
distribution	O
for	O
the	O
data	O
we	O
are	O
modelling	O
.	O

The	O
model	O
proposed	O
here	O
can	O
be	O
seen	O
as	O
an	O
extension	O
to	O
the	O
‘	O
stacked	B-Method
M1	I-Method
+	I-Method
M2	I-Method
’	I-Method
model	E-Method
originally	O
proposed	O
from	O
,	O
where	O
we	O
have	O
additionally	O
introduced	O
the	O
nuisance	O
variable	O
during	O
the	O
feature	B-Method
extraction	E-Method
.	O

Thus	O
following	O
we	O
can	O
also	O
handle	O
the	O
‘	O
semi	B-Task
-	I-Task
supervised	I-Task
’	I-Task
case	E-Task
,	O
i.e.	O
,	O
missing	O
labels	O
.	O

In	O
situations	O
where	O
the	O
label	O
is	O
observed	O
the	O
lower	O
bound	O
takes	O
the	O
following	O
form	O
(	O
exploiting	O
the	O
fact	O
that	O
we	O
can	O
compute	O
some	O
Kullback	O
-	O
Leibler	O
divergences	O
explicitly	O
in	O
our	O
case	O
)	O
:	O
and	O
in	O
the	O
case	O
that	O
it	O
is	O
not	O
observed	O
we	O
use	O
to	O
‘	O
impute	O
’	O
our	O
data	O
:	O
therefore	O
the	O
final	O
objective	B-Metric
function	E-Metric
is	O
:	O
where	O
the	O
last	O
term	O
is	O
introduced	O
so	O
as	O
to	O
ensure	O
that	O
the	O
predictive	O
posterior	O
learns	O
from	O
both	O
labeled	O
and	O
unlabeled	O
data	O
.	O

This	O
semi	B-Method
-	I-Method
supervised	I-Method
model	E-Method
will	O
be	O
called	O
“	O
VAE	S-Method
”	O
in	O
our	O
experiments	O
.	O

However	O
,	O
there	O
is	O
a	O
subtle	O
difference	O
between	O
the	O
approach	O
of	O
and	O
our	O
model	O
.	O

Instead	O
of	O
training	O
separately	O
each	O
layer	O
of	O
stochastic	O
variables	O
we	O
optimize	O
the	O
model	O
jointly	O
.	O

The	O
potential	O
advantages	O
of	O
this	O
approach	O
are	O
two	O
fold	O
:	O
as	O
we	O
previously	O
mentioned	O
if	O
the	O
label	O
and	O
the	O
nuisance	O
information	O
are	O
correlated	O
then	O
training	O
a	O
(	O
conditional	B-Method
)	I-Method
feature	I-Method
extractor	E-Method
separately	O
poses	O
the	O
danger	O
of	O
creating	O
a	O
degenerate	B-Method
representation	E-Method
with	O
respect	O
to	O
the	O
label	O
.	O

Furthermore	O
the	O
label	O
information	O
will	O
also	O
better	O
guide	O
the	O
feature	B-Method
extraction	E-Method
towards	O
the	O
more	O
salient	O
parts	O
of	O
the	O
data	O
,	O
thus	O
maintaining	O
most	O
of	O
the	O
(	O
predictive	O
)	O
information	O
.	O

subsection	O
:	O
Further	O
invariance	O
via	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
Despite	O
the	O
fact	O
that	O
we	O
have	O
a	O
model	O
that	O
encourages	O
statistical	O
independence	O
between	O
and	O
a	O
-	O
priori	O
we	O
might	O
still	O
have	O
some	O
dependence	O
in	O
the	O
(	O
approximate	O
)	O
marginal	O
posterior	O
.	O

In	O
particular	O
,	O
this	O
can	O
happen	O
if	O
the	O
label	O
is	O
correlated	O
with	O
the	O
sensitive	O
variable	O
,	O
which	O
can	O
allow	O
information	O
about	O
to	O
“	O
leak	O
”	O
into	O
the	O
posterior	O
.	O

Thus	O
instead	O
we	O
could	O
maximize	O
a	O
“	O
penalized	B-Metric
”	I-Metric
lower	I-Metric
bound	E-Metric
where	O
we	O
impose	O
some	O
sort	O
of	O
regularization	O
on	O
the	O
marginal	O
.	O

In	O
the	O
following	O
we	O
will	O
describe	O
one	O
way	O
to	O
achieve	O
this	O
regularization	O
through	O
the	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
(	O
MMD	S-Method
)	O
gretton2006kernel	O
measure	O
.	O

subsubsection	S-Method
:	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
Consider	O
the	O
problem	O
of	O
determining	O
whether	O
two	O
datasets	O
and	O
are	O
drawn	O
from	O
the	O
same	O
distribution	O
,	O
i.e.	O
,	O
.	O

A	O
simple	O
test	O
is	O
to	O
consider	O
the	O
distance	O
between	O
empirical	O
statistics	O
of	O
the	O
two	O
datasets	O
:	O
Expanding	O
the	O
square	O
yields	O
an	O
estimator	O
composed	O
only	O
of	O
inner	O
products	O
on	O
which	O
the	O
kernel	B-Method
trick	E-Method
can	O
be	O
applied	O
.	O

The	O
resulting	O
estimator	O
is	O
known	O
as	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
(	O
MMD	S-Method
)	O
gretton2006kernel	O
:	O
Asymptotically	O
,	O
for	O
a	O
universal	B-Method
kernel	E-Method
such	O
as	O
the	O
Gaussian	B-Method
kernel	E-Method
,	O
is	O
if	O
and	O
only	O
if	O
.	O

Equivalently	O
,	O
minimizing	O
MMD	S-Method
can	O
be	O
viewed	O
as	O
matching	O
all	O
of	O
the	O
moments	O
of	O
and	O
.	O

Therefore	O
,	O
we	O
can	O
use	O
it	O
as	O
an	O
extra	O
“	O
regularizer	O
”	O
and	O
force	O
the	O
model	O
to	O
try	O
to	O
match	O
the	O
moments	O
between	O
the	O
marginal	O
posterior	O
distributions	O
of	O
our	O
latent	O
variables	O
,	O
i.e.	O
,	O
and	O
(	O
in	O
the	O
case	O
of	O
binary	O
nuisance	O
information	O
)	O
.	O

By	O
adding	O
the	O
MMD	S-Method
penalty	O
into	O
the	O
lower	O
bound	O
of	O
our	O
aforementioned	O
VAE	S-Method
architecture	O
we	O
obtain	O
our	O
proposed	O
model	O
,	O
the	O
“	O
Variational	B-Method
Fair	I-Method
Autoencoder	E-Method
”	O
(	O
VFAE	S-Method
)	O
:	O
where	O
:	O
subsection	O
:	O
Fast	O
MMD	S-Method
via	O
Random	B-Method
Fourier	I-Method
Features	E-Method
A	O
naive	O
implementation	O
of	O
MMD	S-Method
in	O
minibatch	B-Method
stochastic	I-Method
gradient	I-Method
descent	E-Method
would	O
require	O
computing	O
the	O
Gram	O
matrix	O
for	O
each	O
minibatch	O
during	O
training	O
,	O
where	O
is	O
the	O
minibatch	O
size	O
.	O

Instead	O
,	O
we	O
can	O
use	O
random	B-Method
kitchen	I-Method
sinks	E-Method
rahimi2009weighted	O
to	O
compute	O
a	O
feature	B-Method
expansion	E-Method
such	O
that	O
computing	O
the	O
estimator	O
approximates	O
the	O
full	O
MMD	S-Method
(	O
[	O
reference	O
]	O
)	O
.	O

To	O
compute	O
this	O
,	O
we	O
draw	O
a	O
random	O
matrix	O
,	O
where	O
is	O
the	O
dimensionality	O
of	O
,	O
is	O
the	O
number	O
of	O
random	O
features	O
and	O
each	O
entry	O
of	O
is	O
drawn	O
from	O
a	O
standard	O
isotropic	B-Method
Gaussian	E-Method
.	O

The	O
feature	B-Method
expansion	E-Method
is	O
then	O
given	O
as	O
:	O
where	O
is	O
a	O
-	O
dimensional	O
uniform	O
random	O
vector	O
with	O
entries	O
in	O
.	O

have	O
successfully	O
applied	O
the	O
idea	O
of	O
using	O
random	B-Method
kitchen	I-Method
sinks	E-Method
to	O
approximate	O
MMD	S-Method
.	O

This	O
estimator	O
is	O
fairly	O
accurate	O
,	O
and	O
is	O
typically	O
much	O
faster	O
than	O
the	O
full	O
MMD	S-Method
penalty	O
.	O

We	O
use	O
in	O
our	O
experiments	O
.	O

section	O
:	O
Experiments	O
We	O
performed	O
experiments	O
on	O
the	O
three	O
datasets	O
that	O
correspond	O
to	O
a	O
“	O
fair	O
”	O
classification	B-Task
scenario	E-Task
and	O
were	O
previously	O
used	O
by	O
.	O

In	O
these	O
datasets	O
the	O
“	O
nuisance	O
”	O
or	O
sensitive	O
variable	O
is	O
significantly	O
correlated	O
with	O
the	O
label	O
thus	O
making	O
the	O
proper	O
removal	B-Task
of	I-Task
challenging	E-Task
.	O

Furthermore	O
,	O
we	O
also	O
experimented	O
with	O
the	O
Amazon	B-Material
reviews	I-Material
dataset	E-Material
to	O
make	O
a	O
connection	O
with	O
the	O
“	O
domain	B-Material
-	I-Material
adaptation	I-Material
”	I-Material
literature	E-Material
.	O

Finally	O
,	O
we	O
also	O
experimented	O
with	O
a	O
more	O
general	O
task	O
on	O
the	O
extended	B-Material
Yale	I-Material
B	I-Material
dataset	E-Material
;	O
that	O
of	O
learning	B-Task
invariant	I-Task
representations	E-Task
.	O

subsection	O
:	O
Datasets	O
For	O
the	O
fairness	B-Task
task	E-Task
we	O
experimented	O
with	O
three	O
datasets	O
that	O
were	O
previously	O
used	O
by	O
zemel2013learning	O
.	O

The	O
German	B-Material
dataset	E-Material
is	O
the	O
smallest	O
one	O
with	O
data	O
points	O
and	O
the	O
objective	O
is	O
to	O
predict	O
whether	O
a	O
person	O
has	O
a	O
good	O
or	O
bad	O
credit	O
rating	O
.	O

The	O
sensitive	O
variable	O
is	O
the	O
gender	O
of	O
the	O
individual	O
.	O

The	O
Adult	B-Material
income	I-Material
dataset	E-Material
contains	O
entries	O
and	O
describes	O
whether	O
an	O
account	O
holder	O
has	O
over	O
dollars	O
in	O
their	O
account	O
.	O

The	O
sensitive	O
variable	O
is	O
age	O
.	O

Both	O
of	O
these	O
are	O
obtained	O
from	O
the	O
UCI	B-Material
machine	I-Material
learning	I-Material
repository	I-Material
UCI	E-Material
.	O

The	O
health	B-Material
dataset	E-Material
is	O
derived	O
from	O
the	O
Heritage	O
Health	S-Material
Prize	O
.	O

It	O
is	O
the	O
largest	O
of	O
the	O
three	O
datasets	O
with	O
entries	O
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
a	O
patient	O
will	O
spend	O
any	O
days	O
in	O
the	O
hospital	O
in	O
the	O
next	O
year	O
and	O
the	O
sensitive	O
variable	O
is	O
the	O
age	O
of	O
the	O
individual	O
.	O

We	O
use	O
the	O
same	O
train	O
/	O
test	O
/	O
validation	O
splits	O
as	O
zemel2013learning	O
for	O
our	O
experiments	O
.	O

Finally	O
we	O
also	O
binarized	O
the	O
data	O
and	O
used	O
a	O
multivariate	B-Method
Bernoulli	I-Method
distribution	E-Method
for	O
,	O
where	O
is	O
the	O
sigmoid	O
function	O
.	O

For	O
the	O
domain	B-Task
adaptation	I-Task
task	E-Task
we	O
used	O
the	O
Amazon	B-Material
reviews	I-Material
dataset	E-Material
(	O
with	O
similar	O
preprocessing	O
)	O
that	O
was	O
also	O
employed	O
by	O
chen2012marginalized	O
and	O
2015arXiv150507818G.	O
It	O
is	O
composed	O
from	O
text	B-Material
reviews	E-Material
about	O
particular	O
products	O
,	O
where	O
each	O
product	O
belongs	O
to	O
one	O
out	O
of	O
four	O
different	O
domains	O
:	O
“	O
books	O
”	O
,	O
“	O
dvd	O
”	O
,	O
“	O
electronics	O
”	O
and	O
“	O
kitchen	O
”	O
.	O

As	O
a	O
result	O
we	O
performed	O
twelve	O
domain	B-Task
adaptation	I-Task
tasks	E-Task
.	O

The	O
labels	O
correspond	O
to	O
the	O
sentiment	O
of	O
each	O
review	O
,	O
i.e.	O
either	O
positive	O
or	O
negative	O
.	O

Since	O
each	O
feature	O
vector	O
is	O
composed	O
from	O
counts	O
of	O
unigrams	O
and	O
bigrams	S-Method
we	O
used	O
a	O
Poisson	B-Method
distribution	E-Method
for	O
.	O

It	O
is	O
also	O
worthwhile	O
to	O
mention	O
that	O
we	O
can	O
fully	O
exploit	O
the	O
semi	O
-	O
supervised	O
nature	O
of	O
our	O
model	O
in	O
this	O
dataset	O
,	O
and	O
thus	O
for	O
training	O
we	O
only	O
use	O
the	O
source	O
domain	O
labels	O
and	O
consider	O
the	O
labels	O
of	O
the	O
target	O
domain	O
as	O
“	O
missing	O
”	O
.	O

For	O
the	O
general	O
task	O
of	O
learning	B-Task
invariant	I-Task
representations	E-Task
we	O
used	O
the	O
Extended	B-Material
Yale	I-Material
B	I-Material
dataset	E-Material
,	O
which	O
was	O
also	O
employed	O
in	O
a	O
similar	O
fashion	O
by	O
li2014learning	O
.	O

It	O
is	O
composed	O
from	O
face	O
images	O
of	O
38	O
people	O
under	O
different	O
lighting	O
conditions	O
(	O
directions	O
of	O
the	O
light	O
source	O
)	O
.	O

Similarly	O
to	O
li2014learning	O
,	O
we	O
created	O
5	O
states	O
for	O
the	O
nuisance	O
variable	O
:	O
light	O
source	O
in	O
upper	O
right	O
,	O
lower	O
right	O
,	O
lower	O
left	O
,	O
upper	O
left	O
and	O
the	O
front	O
.	O

The	O
labels	O
correspond	O
to	O
the	O
identity	O
of	O
the	O
person	O
.	O

Following	O
li2014learning	O
,	O
we	O
used	O
the	O
same	O
training	O
,	O
test	O
set	O
and	O
no	O
validation	O
set	O
.	O

For	O
the	O
distribution	O
we	O
used	O
a	O
Gaussian	S-Method
with	O
means	O
constrained	O
in	O
the	O
0	O
-	O
1	O
range	O
(	O
since	O
we	O
have	O
intensity	O
images	O
)	O
by	O
a	O
sigmoid	S-Method

,	O
i.e.	O
.	O


subsection	O
:	O
Experimental	O
Setup	O
For	O
the	O
Adult	B-Material
dataset	E-Material
both	O
encoders	S-Method
,	O
for	O
and	O
,	O
and	O
both	O
decoders	S-Method
,	O
for	O
and	O
,	O
had	O
one	O
hidden	B-Method
layer	E-Method
of	O
100	O
units	O
.	O

For	O
the	O
Health	B-Material
dataset	E-Material
we	O
had	O
one	O
hidden	O
layer	O
of	O
300	O
units	O
for	O
the	O
encoder	B-Method
and	I-Method
decoder	E-Method
and	O
one	O
hidden	B-Method
layer	E-Method
of	O
150	O
units	O
for	O
the	O
encoder	B-Method
and	I-Method
decoder	E-Method
.	O

For	O
the	O
much	O
smaller	O
German	B-Material
dataset	E-Material
we	O
used	O
60	O
hidden	O
units	O
for	O
both	O
encoders	S-Method
and	O
decoders	S-Method
.	O

Finally	O
,	O
for	O
the	O
Amazon	B-Material
reviews	E-Material
and	O
Extended	B-Material
Yale	I-Material
B	I-Material
datasets	E-Material
we	O
had	O
one	O
hidden	B-Method
layer	E-Method
with	O
500	O
,	O
400	O
units	O
respectively	O
for	O
the	O
encoder	S-Method
,	O
decoder	S-Method
,	O
and	O
300	O
,	O
100	O
units	O
respectively	O
for	O
the	O
encoder	B-Method
and	I-Method
decoder	E-Method
.	O

On	O
all	O
of	O
the	O
datasets	O
we	O
used	O
50	O
latent	O
dimensions	O
for	O
and	O
,	O
except	O
for	O
the	O
small	B-Material
German	I-Material
dataset	E-Material
,	O
where	O
we	O
used	O
30	O
latent	O
dimensions	O
for	O
both	O
variables	O
.	O

For	O
the	O
predictive	B-Task
posterior	E-Task
we	O
used	O
a	O
simple	O
Logistic	B-Method
regression	I-Method
classifier	E-Method
.	O

Optimization	S-Task
of	O
the	O
objective	B-Metric
function	E-Metric
was	O
done	O
with	O
Adam	O
DBLP	S-Method
:	O
journals	O
/	O
corr	O
/	O
KingmaB14	O
using	O
the	O
default	O
values	O
for	O
the	O
hyperparameters	O
,	O
minibatches	O
of	O
100	O
data	O
points	O
and	O
temporal	B-Method
averaging	E-Method
.	O

The	O
MMD	S-Method
penalty	O
was	O
simply	O
multiplied	O
by	O
the	O
minibatch	O
size	O
so	O
as	O
to	O
keep	O
the	O
scale	O
of	O
the	O
penalty	O
similar	O
to	O
the	O
lower	O
bound	O
.	O

Furthermore	O
,	O
the	O
extra	O
strength	O
of	O
the	O
MMD	S-Method
,	O
,	O
was	O
tuned	O
according	O
to	O
a	O
validation	O
set	O
.	O

The	O
scaling	O
of	O
the	O
supervised	B-Metric
cost	E-Metric
was	O
low	O
(	O
)	O
for	O
the	O
Adult	S-Material
,	O
Health	S-Material
and	O
German	B-Material
datasets	E-Material
due	O
to	O
the	O
correlation	O
of	O
with	O
.	O

On	O
the	O
Amazon	B-Material
reviews	E-Material
and	O
Extended	B-Material
Yale	I-Material
B	I-Material
datasets	E-Material
however	O
the	O
scaling	O
of	O
the	O
supervised	B-Metric
cost	E-Metric
was	O
higher	O
:	O
for	O
the	O
Amazon	B-Material
reviews	I-Material
dataset	E-Material
(	O
empirically	O
determined	O
after	O
observing	O
the	O
classification	B-Metric
loss	E-Metric
on	O
the	O
first	O
few	O
iterations	O
on	O
the	O
first	O
source	O
-	O
target	O
pair	O
)	O
and	O
for	O
the	O
Extended	B-Material
Yale	I-Material
B	I-Material
dataset	E-Material
.	O

Similarly	O
,	O
the	O
scaling	O
of	O
the	O
MMD	S-Method
penalty	O
was	O
for	O
the	O
Amazon	B-Material
reviews	I-Material
dataset	E-Material
and	O
for	O
the	O
Extended	B-Material
Yale	I-Material
B.	E-Material
Our	O
evaluation	O
is	O
geared	O
towards	O
two	O
fronts	O
;	O
removing	O
information	O
about	O
and	O
classification	B-Metric
accuracy	E-Metric
for	O
.	O

To	O
measure	O
the	O
information	O
about	O
in	O
our	O
new	O
representation	O
we	O
simply	O
train	O
a	O
classifier	S-Method
to	O
predict	O
from	O
.	O

We	O
utilize	O
both	O
Logistic	B-Method
Regression	E-Method
(	O
LR	S-Method
)	O
which	O
is	O
a	O
simple	O
linear	B-Method
classifier	E-Method
,	O
and	O
Random	B-Method
Forest	E-Method
(	O
RF	S-Method
)	O
which	O
is	O
a	O
powerful	O
non	B-Method
-	I-Method
linear	I-Method
classifier	E-Method
.	O

Since	O
on	O
the	O
datasets	O
that	O
we	O
experimented	O
with	O
the	O
nuisance	O
variable	O
is	O
binary	O
we	O
can	O
easily	O
find	O
the	O
random	B-Metric
chance	I-Metric
accuracy	E-Metric
for	O
and	O
measure	O
the	O
discriminatory	B-Metric
information	I-Metric
of	I-Metric
in	E-Metric
.	O

Furthermore	O
,	O
we	O
also	O
used	O
the	O
discrimination	B-Method
metric	E-Method
from	O
as	O
well	O
a	O
more	O
“	O
informed	O
”	O
version	O
of	O
the	O
discrimination	B-Method
metric	E-Method
that	O
instead	O
of	O
the	O
predictions	O
,	O
takes	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
correct	O
class	O
.	O

They	O
are	O
provided	O
in	O
the	O
appendix	O
A.	O
Finally	O
,	O
for	O
the	O
classification	S-Task
performance	O
on	O
we	O
used	O
the	O
predictive	O
posterior	O
for	O
the	O
VAE	S-Method
/	O
VFAE	S-Method
and	O
a	O
simple	O
Logistic	B-Method
Regression	E-Method
for	O
the	O
original	O
representations	O
.	O

It	O
should	O
be	O
noted	O
that	O
for	O
the	O
VFAE	S-Method
and	O
VAE	S-Method
models	O
we	O
use	O
a	O
sample	O
from	O
to	O
make	O
predictions	O
,	O
instead	O
of	O
using	O
the	O
mean	O
.	O

We	O
found	O
that	O
the	O
extra	O
noise	O
helps	O
with	O
invariance	O
.	O

We	O
implemented	O
the	O
Learning	B-Method
Fair	I-Method
Representations	E-Method
zemel2013learning	O
method	O
(	O
LFR	S-Method
)	O
as	O
a	O
baseline	O
using	O
dimensions	O
for	O
the	O
latent	O
space	O
.	O

To	O
measure	O
the	O
accuracy	S-Metric
on	O
in	O
the	O
results	O
below	O
we	O
similarly	O
used	O
the	O
LFR	B-Method
model	I-Method
predictions	E-Method
.	O

subsection	O
:	O
Results	O
subsubsection	O
:	O
Fair	B-Task
classification	E-Task
The	O
results	O
for	O
all	O
three	O
datasets	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Since	O
we	O
are	O
dealing	O
with	O
the	O
“	O
fair	B-Task
”	I-Task
classification	I-Task
scenario	E-Task
here	O
,	O
low	B-Metric
accuracy	E-Metric
and	O
discrimination	B-Metric
against	E-Metric
is	O
more	O
important	O
than	O
the	O
accuracy	S-Metric
on	O
(	O
as	O
long	O
as	O
we	O
do	O
not	O
produce	O
degenerate	O
representations	O
)	O
.	O

.329	O
[	O
width=1.1	O
]	O
adult_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
adult_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
adult_y.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
german_y.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_s.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_discr.pdf	O
.329	O
[	O
width=1.1	O
]	O
health_y.pdf	O
On	O
the	O
Adult	B-Material
dataset	E-Material
,	O
the	O
highest	O
accuracy	S-Metric
on	O
the	O
label	O
and	O
the	O
lowest	O
discrimination	S-Metric
against	O
is	O
obtained	O
by	O
our	O
LFR	B-Method
baseline	E-Method
.	O

Despite	O
the	O
fact	O
that	O
LFR	S-Method
appears	O
to	O
give	O
the	O
best	O
tradeoff	O
between	O
accuracy	S-Metric
and	O
discrimination	S-Task
,	O
it	O
appears	O
to	O
retain	O
information	O
about	O
in	O
its	O
representation	O
,	O
which	O
is	O
discovered	O
from	O
the	O
random	B-Method
forest	I-Method
classifier	E-Method
.	O

In	O
that	O
sense	O
,	O
the	O
VFAE	S-Method
method	O
appears	O
to	O
do	O
the	O
best	O
job	O
in	O
actually	O
removing	O
the	O
sensitive	O
information	O
and	O
maintaining	O
most	O
of	O
the	O
predictive	O
information	O
.	O

Furthermore	O
,	O
the	O
introduction	O
of	O
the	O
MMD	S-Method
penalty	O
in	O
the	O
VFAE	S-Method
model	O
seems	O
to	O
provide	O
a	O
significant	O
benefit	O
with	O
respect	O
to	O
our	O
discrimination	B-Metric
metrics	E-Metric
,	O
as	O
both	O
were	O
reduced	O
considerably	O
compared	O
to	O
the	O
regular	O
VAE	S-Method
.	O

On	O
the	O
German	B-Material
dataset	E-Material
,	O
all	O
methods	O
appear	O
to	O
be	O
invariant	O
with	O
respect	O
to	O
the	O
sensitive	O
information	O
.	O

However	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
discrimination	B-Metric
metric	E-Metric
,	O
since	O
LFR	S-Method
does	O
appear	O
to	O
retain	O
information	O
compared	O
to	O
the	O
VAE	S-Method
and	O
VFAE	S-Method
.	O

The	O
MMD	S-Method
penalty	O
in	O
VFAE	S-Method
did	O
seem	O
improve	O
the	O
discrimination	B-Metric
scores	E-Metric
over	O
the	O
original	O
VAE	S-Method
,	O
while	O
the	O
accuracy	S-Metric
on	O
the	O
labels	O
remained	O
similar	O
.	O

As	O
for	O
the	O
Health	B-Material
dataset	E-Material
;	O
this	O
dataset	O
is	O
extremely	O
imbalanced	O
,	O
with	O
only	O
15	O
%	O
of	O
the	O
patients	O
being	O
admitted	O
to	O
a	O
hospital	O
.	O

Therefore	O
,	O
each	O
of	O
the	O
classifiers	S-Method
seems	O
to	O
predict	O
the	O
majority	O
class	O
as	O
the	O
label	O
for	O
every	O
point	O
.	O

For	O
the	O
invariance	O
against	O
however	O
,	O
the	O
results	O
were	O
more	O
interesting	O
.	O

On	O
the	O
one	O
hand	O
,	O
the	O
VAE	S-Method
model	O
on	O
this	O
dataset	O
did	O
maintain	O
some	O
sensitive	O
information	O
,	O
which	O
could	O
be	O
identified	O
both	O
linearly	O
and	O
non	O
-	O
linearly	O
.	O

On	O
the	O
other	O
hand	O
,	O
VFAE	S-Method
and	O
the	O
LFR	B-Method
methods	E-Method
were	O
able	O
to	O
retain	O
less	O
information	O
in	O
their	O
latent	B-Method
representation	E-Method
,	O
since	O
only	O
Random	B-Method
Forest	E-Method
was	O
able	O
to	O
achieve	O
higher	O
than	O
random	B-Metric
chance	I-Metric
accuracy	E-Metric
.	O

This	O
further	O
justifies	O
our	O
choice	O
for	O
including	O
the	O
MMD	S-Method
penalty	O
in	O
the	O
lower	B-Metric
bound	E-Metric
of	O
the	O
VAE	S-Method

.	O

.	O


In	O
order	O
to	O
further	O
assess	O
the	O
nature	O
of	O
our	O
new	O
representations	O
,	O
we	O
visualized	O
two	O
dimensional	O
Barnes	O
-	O
Hut	O
SNE	O
2013arXiv1301.3342V	O
embeddings	O
of	O
the	O
representations	O
,	O
obtained	O
from	O
the	O
model	O
trained	O
on	O
the	O
Adult	B-Material
dataset	E-Material
,	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

As	O
we	O
can	O
see	O
,	O
the	O
nuisance	O
/	O
sensitive	O
variables	O
can	O
be	O
identified	O
both	O
on	O
the	O
original	O
representation	O
and	O
on	O
a	O
latent	B-Method
representation	E-Method
that	O
does	O
not	O
have	O
the	O
MMD	S-Method
penalty	O
and	O
the	O
independence	O
properties	O
between	O
and	O
in	O
the	O
prior	O
.	O

By	O
introducing	O
these	O
independence	O
properties	O
as	O
well	O
as	O
the	O
MMD	S-Method
penalty	O
the	O
nuisance	O
variable	O
groups	O
become	O
practically	O
indistinguishable	O
.	O

.25	O
[	O
width=1.	O
]	O
tsne_adult_x.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult_s.pdf	O
.25	O
[	O
width=1.	O
]	O
tsne_adult_mmd_s.pdf	O
subsubsection	S-Method
:	O
Domain	B-Method
adaptation	E-Method
As	O
for	O
the	O
domain	B-Task
adaptation	I-Task
scenario	E-Task
and	O
the	O
Amazon	B-Material
reviews	I-Material
dataset	E-Material
,	O
the	O
results	O
of	O
our	O
VFAE	S-Method
model	O
can	O
be	O
seen	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
model	O
was	O
successful	O
in	O
factoring	O
out	O
the	O
domain	O
information	O
,	O
since	O
the	O
accuracy	S-Metric
,	O
measured	O
both	O
linearly	O
(	O
LR	S-Method
)	O
and	O
non	O
-	O
linearly	O
(	O
RF	S-Method
)	O
,	O
was	O
towards	O
random	O
chance	O
(	O
which	O
for	O
this	O
dataset	O
is	O
0.5	O
)	O
.	O

We	O
should	O
also	O
mention	O
that	O
,	O
on	O
this	O
dataset	O
at	O
least	O
,	O
completely	O
removing	O
information	O
about	O
the	O
domain	O
does	O
not	O
guarantee	O
a	O
better	O
performance	O
on	O
.	O

The	O
same	O
effect	O
was	O
also	O
observed	O
by	O
and	O
.	O

As	O
far	O
as	O
the	O
accuracy	S-Metric
on	O
is	O
concerned	O
,	O
we	O
compared	O
against	O
a	O
recent	O
neural	B-Method
network	E-Method
based	O
state	O
of	O
the	O
art	O
method	O
for	O
domain	B-Task
adaptation	E-Task
,	O
Domain	B-Method
Adversarial	I-Method
Neural	I-Method
Network	E-Method
(	O
DANN	S-Method
)	O
2015arXiv150507818G.	O
As	O
we	O
can	O
observe	O
in	O
table	O
[	O
reference	O
]	O
,	O
our	O
accuracy	S-Metric
on	O
the	O
labels	O
is	O
higher	O
on	O
9	O
out	O
of	O
the	O
12	O
domain	B-Task
adaptation	I-Task
tasks	E-Task
whereas	O
on	O
the	O
remaining	O
3	O
it	O
is	O
quite	O
similar	O
to	O
the	O
DANN	B-Method
architecture	E-Method
.	O

subsection	O
:	O
Learning	B-Task
Invariant	I-Task
Representations	E-Task
Regarding	O
the	O
more	O
general	O
task	O
of	O
learning	B-Task
invariant	I-Task
representations	E-Task
;	O
our	O
results	O
on	O
the	O
Extended	B-Material
Yale	I-Material
B	I-Material
dataset	E-Material
also	O
demonstrate	O
our	O
model	O
’s	O
ability	O
to	O
learn	O
such	O
representations	O
.	O

As	O
expected	O
,	O
on	O
the	O
original	O
representation	O
the	O
lighting	O
conditions	O
,	O
,	O
are	O
well	O
identifiable	O
with	O
almost	O
perfect	O
accuracy	S-Metric
from	O
both	O
RF	S-Method
and	O
LR	S-Method
.	O

This	O
can	O
also	O
be	O
seen	O
in	O
the	O
two	O
dimensional	O
embeddings	O
of	O
the	O
original	O
space	O
in	O
Figure	O
[	O
reference	O
]	O
:	O
the	O
images	O
are	O
mostly	O
clustered	O
according	O
to	O
the	O
lighting	O
conditions	O
.	O

As	O
soon	O
as	O
we	O
utilize	O
our	O
VFAE	S-Method
model	O
we	O
simultaneously	O
decrease	O
the	O
accuracy	S-Metric
on	O
,	O
from	O
96	O
%	O
to	O
about	O
50	O
%	O
,	O
and	O
increase	O
our	O
accuracy	S-Metric
on	O
,	O
from	O
78	O
%	O
to	O
about	O
85	O
%	O
.	O

This	O
effect	O
can	O
also	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
:	O
the	O
images	O
are	O
now	O
mostly	O
clustered	O
according	O
to	O
the	O
person	O
ID	O
(	O
the	O
label	O
)	O
.	O

It	O
is	O
clear	O
that	O
in	O
this	O
scenario	O
the	O
information	O
about	O
is	O
purely	O
“	O
nuisance	O
”	O
with	O
respect	O
to	O
the	O
labels	O
.	O

Therefore	O
,	O
by	O
using	O
our	O
VFAE	S-Method
model	O
we	O
are	O
able	O
to	O
obtain	O
improved	O
generalization	S-Metric
and	O
classification	S-Task
performance	O
by	O
effectively	O
removing	O
from	O
our	O
representations	O
.	O

.49	O
[	O
width=	O
]	O
yaleb_x.pdf	O
.49	O
[	O
width=	O
]	O
yaleb_z.pdf	O
section	O
:	O
Related	O
Work	O
Most	O
related	O
to	O
our	O
“	O
fair	O
”	O
representations	O
view	O
is	O
the	O
work	O
from	O
.	O

They	O
proposed	O
a	O
neural	B-Method
network	I-Method
based	I-Method
semi	I-Method
-	I-Method
supervised	I-Method
clustering	I-Method
model	E-Method
for	O
learning	B-Task
fair	I-Task
representations	E-Task
.	O

The	O
idea	O
is	O
to	O
learn	O
a	O
localised	B-Method
representation	E-Method
that	O
maps	O
each	O
datapoint	O
to	O
a	O
cluster	O
in	O
such	O
a	O
way	O
that	O
each	O
cluster	O
gets	O
assigned	O
roughly	O
equal	O
proportions	O
of	O
data	O
from	O
each	O
group	O
in	O
.	O

Although	O
their	O
approach	O
was	O
successfully	O
applied	O
on	O
several	O
datasets	O
,	O
the	O
restriction	O
to	O
clustering	S-Task
means	O
that	O
it	O
can	O
not	O
leverage	O
the	O
representational	O
power	O
of	O
a	O
distributed	B-Method
representation	E-Method
.	O

Furthermore	O
,	O
this	O
penalty	O
does	O
not	O
account	O
for	O
higher	O
order	O
moments	O
in	O
the	O
latent	O
distribution	O
.	O

For	O
example	O
,	O
if	O
always	O
returns	O
or	O
,	O
while	O
returns	O
values	O
between	O
values	O
and	O
,	O
then	O
the	O
penalty	O
could	O
still	O
be	O
satisfied	O
,	O
but	O
information	O
could	O
still	O
leak	O
through	O
.	O

We	O
addressed	O
both	O
of	O
these	O
issues	O
in	O
this	O
paper	O
.	O

Domain	B-Task
adaptation	E-Task
can	O
also	O
be	O
cast	O
as	O
learning	B-Method
representations	E-Method
that	O
are	O
“	O
invariant	O
”	O
with	O
respect	O
to	O
a	O
discrete	O
variable	O
,	O
the	O
domain	O
.	O

Most	O
similar	O
to	O
our	O
work	O
are	O
neural	B-Method
network	I-Method
approaches	E-Method
which	O
try	O
to	O
match	O
the	O
feature	O
distributions	O
between	O
the	O
domains	O
.	O

This	O
was	O
performed	O
in	O
an	O
unsupervised	O
way	O
with	O
mSDA	S-Method
chen2012marginalized	O
by	O
training	O
denoising	B-Method
autoencoders	E-Method
jointly	O
on	O
all	O
domains	O
,	O
thus	O
implicitly	O
obtaining	O
a	O
representation	O
general	O
enough	O
to	O
explain	O
both	O
the	O
domain	O
and	O
the	O
data	O
.	O

This	O
is	O
in	O
contrast	O
to	O
our	O
approach	O
where	O
we	O
instead	O
try	O
to	O
learn	O
representations	O
that	O
explicitly	O
remove	O
domain	O
information	O
during	O
the	O
learning	B-Method
process	E-Method
.	O

For	O
the	O
latter	O
we	O
find	O
more	O
similarities	O
with	O
“	O
domain	B-Method
-	I-Method
regularized	I-Method
”	I-Method
supervised	I-Method
approaches	E-Method
that	O
simultaneously	O
try	O
to	O
predict	O
the	O
label	O
for	O
a	O
data	O
point	O
and	O
remove	O
domain	O
specific	O
information	O
.	O

This	O
is	O
done	O
with	O
either	O
MMD	S-Method
long2015learning	O
,	O
DBLP	S-Method
:	O
journals	O
/	O
corr	O
/	O
TzengHZSD14	O
or	O
adversarial	O
2015arXiv150507818	O
G	O
penalties	O
at	O
the	O
hidden	O
layers	O
of	O
the	O
network	O
.	O

In	O
our	O
model	O
however	O
the	O
main	O
“	O
domain	O
-	O
regularizer	O
”	O
stems	O
from	O
the	O
independence	O
properties	O
of	O
the	O
prior	O
over	O
the	O
domain	O
and	O
latent	B-Method
representations	E-Method
.	O

We	O
also	O
employ	O
MMD	S-Method
on	O
our	O
model	O
but	O
from	O
a	O
different	O
perspective	O
since	O
we	O
consider	O
a	O
slightly	O
more	O
difficult	O
case	O
where	O
the	O
domain	O
and	O
label	O
are	O
correlated	O
;	O
we	O
need	O
to	O
ensure	O
that	O
we	O
remain	O
as	O
“	O
invariant	O
”	O
as	O
possible	O
since	O
might	O
‘	O
leak	O
’	O
information	O
about	O
.	O

section	O
:	O
Conclusion	O
We	O
introduce	O
the	O
Variational	B-Method
Fair	I-Method
Autoencoder	E-Method
(	O
VFAE	S-Method
)	O
,	O
an	O
extension	O
of	O
the	O
semi	B-Method
-	I-Method
supervised	I-Method
variational	I-Method
autoencoder	E-Method
in	O
order	O
to	O
learn	O
representations	O
that	O
are	O
explicitly	O
invariant	O
with	O
respect	O
to	O
some	O
known	O
aspect	O
of	O
a	O
dataset	O
while	O
retaining	O
as	O
much	O
remaining	O
information	O
as	O
possible	O
.	O

We	O
further	O
use	O
a	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
regularizer	O
in	O
order	O
to	O
further	O
promote	O
invariance	O
in	O
the	O
posterior	O
distribution	O
over	O
latent	O
variables	O
.	O

We	O
apply	O
this	O
model	O
to	O
tasks	O
involving	O
developing	O
fair	B-Method
classifiers	E-Method
that	O
are	O
invariant	O
to	O
sensitive	O
demographic	O
information	O
and	O
show	O
that	O
it	O
produces	O
a	O
better	O
tradeoff	O
with	O
respect	O
to	O
accuracy	S-Metric
and	O
invariance	S-Metric
.	O

As	O
a	O
second	O
application	O
,	O
we	O
consider	O
the	O
task	O
of	O
domain	B-Task
adaptation	E-Task
,	O
where	O
the	O
goal	O
is	O
to	O
improve	O
classification	S-Task
by	O
training	O
a	O
classifier	S-Method
that	O
is	O
invariant	O
to	O
the	O
domain	O
.	O

We	O
find	O
that	O
our	O
model	O
is	O
competitive	O
with	O
recently	O
proposed	O
adversarial	B-Method
approaches	E-Method
.	O

Finally	O
,	O
we	O
also	O
consider	O
the	O
more	O
general	O
task	O
of	O
learning	B-Task
invariant	I-Task
representations	E-Task
.	O

We	O
can	O
observe	O
that	O
our	O
model	O
provides	O
a	O
clear	O
improvement	O
against	O
a	O
neural	B-Method
network	E-Method
that	O
incorporates	O
a	O
Maximum	B-Method
Mean	I-Method
Discrepancy	E-Method
penalty	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Discrimination	B-Metric
metrics	E-Metric
The	O
Discrimination	B-Metric
metric	E-Metric
zemel2013learning	O
and	O
the	O
Discrimination	B-Metric
metric	E-Metric
that	O
takes	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
correct	O
class	O
are	O
mathematically	O
formalized	O
as	O
:	O
where	O
for	O
the	O
predictions	O
that	O
were	O
done	O
on	O
the	O
datapoints	O
with	O
nuisance	O
variable	O
,	O
denotes	O
the	O
total	O
amount	O
of	O
datapoints	O
that	O
had	O
nuisance	O
variable	O
and	O
denotes	O
the	O
probability	O
of	O
the	O
prediction	O
for	O
the	O
datapoints	O
with	O
.	O

For	O
the	O
predictions	O
and	O
their	O
respective	O
probabilities	O
we	O
used	O
a	O
Logistic	B-Method
Regression	I-Method
classifier	E-Method
.	O

appendix	O
:	O
Proxy	B-Method
A	I-Method
-	I-Method
Distance	E-Method
(	O
PAD	S-Method
)	O
for	O
Amazon	B-Material
Reviews	I-Material
dataset	E-Material
Similarly	O
to	O
,	O
we	O
also	O
calculated	O
the	O
Proxy	O
A	O
-	O
distance	O
(	O
PAD	S-Method
)	O
ben2007analysis	O
,	O
ben2010theory	O
scores	O
for	O
the	O
raw	O
data	O
and	O
for	O
the	O
representations	O
of	O
VFAE	S-Method
.	O

Briefly	O
,	O
Proxy	B-Method
A	I-Method
-	I-Method
distance	E-Method
is	O
an	O
approximation	O
to	O
the	O
-	B-Metric
divergence	I-Metric
measure	I-Metric
of	I-Metric
domain	I-Metric
distinguishability	E-Metric
proposed	O
in	O
and	O
.	O

To	O
compute	O
it	O
we	O
first	O
need	O
to	O
train	O
a	O
learning	B-Method
algorithm	E-Method
on	O
the	O
task	O
of	O
discriminating	O
examples	O
from	O
the	O
source	O
and	O
target	O
domain	O
.	O

Afterwards	O
we	O
can	O
use	O
the	O
test	O
error	O
of	O
that	O
algorithm	O
in	O
the	O
following	O
formula	O
:	O
It	O
is	O
clear	O
that	O
low	O
PAD	B-Metric
scores	E-Metric
correspond	O
to	O
low	O
discrimination	O
of	O
the	O
source	O
and	O
target	O
domain	O
examples	O
from	O
the	O
classifier	S-Method
.	O

To	O
obtain	O
for	O
our	O
model	O
we	O
used	O
Logistic	B-Method
Regression	E-Method
.	O

The	O
resulting	O
plot	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
we	O
have	O
also	O
added	O
the	O
plot	O
from	O
DANN	O
2015arXiv150507818	O
G	O
,	O
where	O
they	O
used	O
a	O
linear	B-Method
Support	I-Method
Vector	I-Method
Machine	E-Method
for	O
the	O
classifier	S-Method
,	O
as	O
a	O
reference	O
.	O

It	O
can	O
be	O
seen	O
that	O
our	O
VFAE	S-Method
model	O
can	O
factor	O
out	O
the	O
information	O
about	O
better	O
,	O
since	O
the	O
PAD	B-Metric
scores	E-Metric
on	O
our	O
new	O
representation	O
are	O
,	O
overall	O
,	O
lower	O
than	O
the	O
ones	O
obtained	O
from	O
the	O
DANN	B-Method
architecture	E-Method
.	O

.49	O
[	O
width=1.	O
]	O
pad_vfae_x_reviews.pdf	O
.49	O
[	O
width=.9	O
]	O
PAD_DANN.pdf	O
document	O
:	O
Revisiting	O
Distributional	B-Method
Correspondence	I-Method
Indexing	E-Method
:	O
A	O
Python	B-Method
Reimplementation	E-Method
and	O
New	O
Experiments	O
This	O
paper	O
introduces	O
PyDCI	S-Method
,	O
a	O
new	O
implementation	O
of	O
Distributional	B-Method
Correspondence	I-Method
Indexing	E-Method
(	O
DCI	S-Method
)	O
written	O
in	O
Python	O
.	O

DCI	S-Method
is	O
a	O
transfer	B-Method
learning	I-Method
method	E-Method
for	O
cross	B-Task
-	I-Task
domain	I-Task
and	I-Task
cross	I-Task
-	I-Task
lingual	I-Task
text	I-Task
classification	E-Task
for	O
which	O
we	O
had	O
provided	O
an	O
implementation	O
(	O
here	O
called	O
JaDCI	S-Method
)	O
built	O
on	O
top	O
of	O
JaTeCS	S-Method
,	O
a	O
Java	B-Method
framework	E-Method
for	O
text	B-Task
classification	E-Task
.	O

PyDCI	S-Method
is	O
a	O
stand	O
-	O
alone	O
version	O
of	O
DCI	S-Method
that	O
exploits	O
scikit	B-Method
-	I-Method
learn	E-Method
and	O
the	O
SciPy	B-Method
stack	E-Method
.	O

We	O
here	O
report	O
on	O
new	O
experiments	O
that	O
we	O
have	O
carried	O
out	O
in	O
order	O
to	O
test	O
PyDCI	S-Method
,	O
and	O
in	O
which	O
we	O
use	O
as	O
baselines	O
new	O
high	O
-	O
performing	O
methods	O
that	O
have	O
appeared	O
after	O
DCI	S-Method
was	O
originally	O
proposed	O
.	O

These	O
experiments	O
show	O
that	O
,	O
thanks	O
to	O
a	O
few	O
subtle	O
ways	O
in	O
which	O
we	O
have	O
improved	O
DCI	S-Method
,	O
PyDCI	S-Method
outperforms	O
both	O
JaDCI	S-Method
and	O
the	O
above	O
-	O
mentioned	O
high	O
-	O
performing	O
methods	O
,	O
and	O
delivers	O
the	O
best	O
known	O
results	O
on	O
the	O
two	O
popular	O
benchmarks	O
on	O
which	O
we	O
had	O
tested	O
DCI	S-Method
,	O
i.e.	O
,	O
MultiDomainSentiment	S-Method
(	O
a.k.a	O
.	O

MDS	S-Method
–	O
for	O
cross	B-Task
-	I-Task
domain	I-Task
adaptation	E-Task
)	O
and	O
Webis	O
-	O
CLS	O
-	O
10	O
(	O
for	O
cross	B-Task
-	I-Task
lingual	I-Task
adaptation	E-Task
)	O
.	O

PyDCI	S-Method
,	O
together	O
with	O
the	O
code	O
allowing	O
to	O
replicate	O
our	O
experiments	O
,	O
is	O
available	O
at	O
.	O

Transfer	O
Learning	O
Domain	O
Adaptation	O
Text	O
Classification	O
Sentiment	O
Classification	O
Cross	O
-	O
Domain	O
Classification	O
Cross	O
-	O
Lingual	O
Classification	O
Python	O
section	O
:	O
Introduction	O
Distributional	B-Method
Correspondence	I-Method
Indexing	E-Method
(	O
DCI	S-Method
)	O
is	O
a	O
pivot	B-Method
-	I-Method
based	I-Method
feature	I-Method
-	I-Method
transfer	I-Method
domain	I-Method
adaptation	I-Method
method	E-Method
for	O
cross	B-Task
-	I-Task
domain	I-Task
and	I-Task
cross	I-Task
-	I-Task
lingual	I-Task
text	I-Task
classification	E-Task
.	O

DCI	S-Method
was	O
first	O
described	O
in	O
,	O
and	O
later	O
improved	O
and	O
extended	O
in	O
;	O
it	O
was	O
formerly	O
implemented	O
in	O
Java	O
as	O
part	O
of	O
the	O
JaTeCS	O
(	O
Ja	O
va	O
Te	O
xt	O
C	O
ategorization	B-Method
S	I-Method
ystem	E-Method
)	O
framework	O
,	O
and	O
this	O
implementation	O
(	O
henceforth	O
called	O
JaDCI	S-Method
)	O
was	O
made	O
publicly	O
available	O
.	O

JaTeCS	S-Method
is	O
a	O
complex	O
package	O
,	O
since	O
it	O
makes	O
available	O
many	O
functionalities	O
for	O
text	B-Task
analytics	I-Task
research	E-Task
.	O

A	O
drawback	O
of	O
JaDCI	S-Method
is	O
thus	O
that	O
,	O
for	O
the	O
researcher	O
wishing	O
to	O
replicate	O
the	O
results	O
of	O
or	O
simply	O
wishing	O
to	O
use	O
JaDCI	S-Method
,	O
a	O
substantive	O
effort	O
in	O
installing	O
and	O
properly	O
configuring	O
the	O
entire	O
JaTeCS	B-Method
framework	E-Method
is	O
thus	O
needed	O
.	O

In	O
this	O
paper	O
we	O
present	O
PyDCI	S-Method
,	O
a	O
new	O
implementation	O
of	O
the	O
DCI	S-Method
method	O
written	O
in	O
Python	O
and	O
built	O
on	O
top	O
of	O
the	O
SciPy	B-Method
stack	E-Method
and	O
scikit	B-Method
-	I-Method
learn	I-Method
toolkit	E-Method
.	O

Python	S-Method
has	O
become	O
the	O
preferred	O
programming	O
language	O
for	O
computer	B-Task
scientists	E-Task
.	O

In	O
the	O
fields	O
of	O
machine	B-Task
learning	E-Task
and	O
data	B-Task
mining	E-Task
its	O
use	O
has	O
also	O
been	O
promoted	O
by	O
the	O
appearance	O
of	O
Python	B-Method
-	I-Method
based	I-Method
environments	E-Method
such	O
as	O
SciPy	S-Method
and	O
scikit	B-Method
-	I-Method
learn	E-Method
,	O
whose	O
potential	O
and	O
ease	O
of	O
use	O
have	O
attracted	O
the	O
interest	O
of	O
practitioners	O
.	O

Our	O
reimplementation	O
is	O
thus	O
in	O
line	O
with	O
these	O
trends	O
.	O

With	O
respect	O
to	O
JaDCI	S-Method
,	O
PyDCI	S-Method
introduces	O
a	O
few	O
modifications	O
in	O
the	O
way	O
DCI	S-Method
is	O
implemented	O
that	O
,	O
although	O
subtle	O
,	O
bring	O
about	O
a	O
significant	O
improvement	O
in	O
the	O
effectiveness	O
of	O
the	O
method	O
.	O

The	O
rest	O
of	O
this	O
paper	O
is	O
structured	O
as	O
follows	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
describe	O
the	O
main	O
modifications	O
to	O
DCI	S-Method
that	O
our	O
new	O
implementation	O
introduces	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
report	O
on	O
new	O
experiments	O
that	O
we	O
have	O
run	O
using	O
PyDCI	S-Method
,	O
and	O
show	O
that	O
,	O
thanks	O
to	O
the	O
modifications	O
above	O
,	O
PyDCI	S-Method
delivers	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
two	O
popular	O
benchmark	O
datasets	O
,	O
i.e.	O
,	O
MultiDomainSentiment	S-Method
(	O
hereafter	O
MDS	S-Method
–	O
for	O
cross	B-Task
-	I-Task
domain	I-Task
adaptation	E-Task
)	O
and	O
Webis	B-Method
-	I-Method
CLS	I-Method
-	I-Method
10	E-Method
(	O
for	O
cross	B-Task
-	I-Task
lingual	I-Task
adaptation	E-Task
)	O
.	O

These	O
results	O
represent	O
a	O
clear	O
improvement	O
over	O
the	O
ones	O
originally	O
obtained	O
with	O
JaDCI	S-Method
and	O
presented	O
in	O
,	O
and	O
also	O
over	O
the	O
ones	O
obtained	O
by	O
recent	O
high	O
-	O
performing	O
methods	O
that	O
have	O
appeared	O
,	O
or	O
that	O
we	O
have	O
become	O
aware	O
of	O
,	O
after	O
DCI	S-Method
was	O
originally	O
proposed	O
.	O

Section	O
[	O
reference	O
]	O
concludes	O
,	O
hinting	O
at	O
future	O
developments	O
.	O

We	O
make	O
PyDCI	S-Method
publicly	O
available	O
via	O
GitHub	O
.	O

section	O
:	O
Implementation	O
Changes	O
For	O
reasons	O
of	O
brevity	O
we	O
do	O
not	O
re	O
-	O
explain	O
DCI	S-Method
from	O
scratch	O
;	O
we	O
refer	O
the	O
interested	O
reader	O
to	O
for	O
a	O
concise	O
description	O
,	O
or	O
to	O
for	O
the	O
full	O
-	O
blown	O
presentation	O
.	O

The	O
main	O
modifications	O
that	O
PyDCI	S-Method
introduces	O
with	O
respect	O
to	O
JaDCI	S-Method
are	O
the	O
following	O
:	O
Document	B-Task
Standardization	E-Task
:	O
In	O
DCI	S-Method
,	O
feature	O
vectors	O
and	O
document	O
vectors	O
(	O
i.e.	O
,	O
the	O
vectors	O
that	O
represent	O
the	O
features	O
and	O
the	O
vectors	O
that	O
represent	O
the	O
documents	O
,	O
respectively	O
)	O
are	O
post	O
-	O
processed	O
via	O
L2	B-Method
-	I-Method
normalization	E-Method
.	O

In	O
we	O
had	O
witnessed	O
improvements	O
when	O
applying	O
standardization	S-Method
to	O
the	O
feature	O
vectors	O
(	O
i.e.	O
,	O
translating	O
and	O
scaling	O
each	O
dimension	O
so	O
that	O
it	O
is	O
approximatelly	O
normally	O
distributed	O
in	O
–	O
see	O
)	O
.	O

In	O
PyDCI	S-Method
we	O
give	O
the	O
user	O
the	O
option	O
to	O
apply	O
standardization	O
also	O
to	O
each	O
dimension	O
of	O
the	O
document	O
vectors	O
before	O
training	O
the	O
classifier	S-Method
.	O

All	O
experiments	O
we	O
report	O
in	O
this	O
paper	O
are	O
run	O
with	O
this	O
option	O
activated	O
.	O

Classifier	B-Task
Optimization	E-Task
:	O
In	O
PyDCI	S-Method
we	O
use	O
scikit	B-Method
-	I-Method
learn	I-Method
’s	I-Method
implementation	E-Method
of	O
linear	B-Method
SVMs	E-Method
(	O
LinearSVC	S-Method
,	O
which	O
is	O
in	O
turn	O
based	O
on	O
the	O
liblinear	B-Method
package	E-Method
)	O
,	O
instead	O
of	O
using	O
Joachims	B-Method
’	I-Method
SVM	I-Method
package	E-Method
as	O
we	O
had	O
done	O
in	O
JaDCI	S-Method
.	O

This	O
allows	O
us	O
to	O
leverage	O
scikit	B-Method
-	I-Method
learn	I-Method
’s	I-Method
GridSearchCV	I-Method
utility	E-Method
in	O
order	O
to	O
optimize	O
SVM	S-Method
’s	O
parameter	O
(	O
which	O
determines	O
the	O
trade	O
-	O
off	O
between	O
training	B-Metric
error	E-Metric
and	O
the	O
margin	O
)	O
via	O
grid	B-Method
search	I-Method
optimization	E-Method
,	O
which	O
allows	O
us	O
to	O
effortlessly	O
tune	O
the	O
classifier	S-Method
.	O

In	O
the	O
new	O
experiments	O
using	O
PyDCI	O
we	O
let	O
parameter	O
range	O
in	O
,	O
while	O
in	O
the	O
JaDCI	O
experiments	O
we	O
had	O
simply	O
relied	O
on	O
the	O
default	O
value	O
that	O
SVM	O
attributes	O
to	O
.	O

Increase	O
in	O
the	O
Number	O
of	O
Pivots	O
:	O
We	O
increase	O
the	O
number	O
of	O
pivots	O
from	O
100	O
(	O
the	O
value	O
we	O
had	O
used	O
in	O
)	O
to	O
1	O
,	O
000	O
in	O
the	O
cross	O
-	O
domain	O
experiments	O
and	O
to	O
450	O
in	O
the	O
cross	B-Task
-	I-Task
lingual	I-Task
experiments	E-Task
.	O

This	O
brings	O
about	O
a	O
significant	O
improvement	O
in	O
performance	O
,	O
that	O
does	O
not	O
come	O
at	O
a	O
significant	O
cost	O
in	O
execution	B-Metric
time	E-Metric
(	O
as	O
instead	O
had	O
happened	O
with	O
the	O
previous	O
implementation	O
)	O
.	O

We	O
limit	O
the	O
number	O
of	O
pivots	O
to	O
450	O
in	O
the	O
cross	B-Task
-	I-Task
lingual	I-Task
case	E-Task
(	O
instead	O
of	O
1	O
,	O
000	O
)	O
since	O
in	O
this	O
case	O
each	O
pivot	O
requires	O
a	O
translation	O
to	O
the	O
target	O
language	O
which	O
is	O
assumed	O
to	O
have	O
a	O
cost	O
;	O
we	O
thus	O
set	O
the	O
number	O
of	O
pivots	O
to	O
450	O
as	O
was	O
done	O
in	O
previous	O
research	O
(	O
e.g.	O
,	O
in	O
)	O
.	O

We	O
discuss	O
below	O
in	O
more	O
detail	O
the	O
impact	O
on	O
performance	O
that	O
the	O
variation	O
in	O
the	O
number	O
of	O
pivots	O
has	O
.	O

We	O
should	O
also	O
mention	O
that	O
PyDCI	S-Method
relies	O
on	O
scikit	B-Method
-	I-Method
learn	E-Method
(	O
while	O
JaDCI	S-Method
relied	O
on	O
JaTeCS	S-Method
)	O
for	O
many	O
preprocessing	B-Task
-	I-Task
related	I-Task
aspects	E-Task
(	O
e.g.	O
,	O
term	B-Task
weighting	E-Task
)	O
,	O
which	O
also	O
may	O
cause	O
some	O
(	O
hard	O
to	O
track	O
)	O
differences	O
in	O
performance	O
with	O
respect	O
to	O
JaDCI	S-Method
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Effectiveness	O
on	O
Cross	B-Task
-	I-Task
Domain	I-Task
Classification	E-Task
and	O
Cross	B-Task
-	I-Task
Lingual	I-Task
Classification	E-Task
In	O
this	O
section	O
we	O
report	O
the	O
results	O
we	O
have	O
obtained	O
in	O
rerunning	O
with	O
PyDCI	S-Method
the	O
same	O
experiments	O
we	O
had	O
run	O
with	O
JaDCI	S-Method
,	O
and	O
whose	O
results	O
had	O
been	O
reported	O
in	O
.	O

The	O
datasets	O
we	O
use	O
are	O
arguably	O
the	O
most	O
popular	O
benchmarks	O
in	O
the	O
domain	O
adaptation	O
literature	O
,	O
i.e.	O
,	O
MDS	S-Method
for	O
cross	B-Task
-	I-Task
domain	I-Task
adaptation	E-Task
and	O
Webis	B-Task
-	I-Task
CLS	I-Task
-	I-Task
10	E-Task
for	O
cross	B-Task
-	I-Task
lingual	I-Task
adaptation	E-Task
.	O

A	O
complete	O
description	O
of	O
the	O
datasets	O
and	O
the	O
standard	O
experimental	O
protocol	O
followed	O
in	O
each	O
case	O
can	O
be	O
found	O
either	O
in	O
the	O
original	O
publications	O
describing	O
the	O
datasets	O
or	O
in	O
.	O

Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
the	O
values	O
of	O
classification	B-Metric
accuracy	E-Metric
(	O
i.e.	O
,	O
the	O
fraction	O
of	O
correctly	O
classified	O
documents	O
)	O
we	O
obtain	O
for	O
cross	B-Task
-	I-Task
domain	I-Task
and	I-Task
cross	I-Task
-	I-Task
lingual	I-Task
classification	E-Task
experiments	O
,	O
respectively	O
.	O

We	O
focus	O
on	O
Linear	O
and	O
Cosine	S-Method
(	O
columns	O
9	O
-	O
10	O
)	O
,	O
two	O
parameter	O
-	O
free	O
probabilistic	O
and	O
kernel	O
-	O
based	O
distributional	B-Method
correspondence	I-Method
functions	E-Method
(	O
DCFs	S-Method
)	O
investigated	O
in	O
.	O

For	O
each	O
such	O
DCF	O
we	O
show	O
a	O
direct	O
comparison	O
against	O
the	O
values	O
we	O
had	O
obtained	O
with	O
JaDCI	S-Method
(	O
Columns	O
7	O
-	O
8	O
)	O
.	O

We	O
also	O
report	O
two	O
baselines	O
:	O
Lower	O
(	O
Column	O
3	O
)	O
,	O
a	O
classifier	S-Method
that	O
directly	O
trains	O
on	O
the	O
‘	O
‘	O
source	O
’	O
’	O
training	O
examples	O
and	O
tests	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
unlabeled	O
examples	O
without	O
performing	O
any	O
sort	O
of	O
adaptation	S-Task
at	O
all	O
.	O

Such	O
a	O
classifier	S-Method
should	O
thus	O
act	O
as	O
a	O
lower	O
bound	O
for	O
any	O
reasonable	O
adaptation	B-Task
endeavour	E-Task
.	O

Upper	O
(	O
Column	O
4	O
)	O
,	O
a	O
classifier	S-Method
that	O
trains	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
training	O
examples	O
and	O
tests	O
on	O
the	O
‘	O
‘	O
target	O
’	O
’	O
unlabeled	O
examples	O
without	O
performing	O
any	O
sort	O
of	O
adaptation	S-Task
at	O
all	O
.	O

Such	O
a	O
classifier	S-Method
should	O
thus	O
act	O
as	O
an	O
upper	O
bound	O
for	O
any	O
reasonable	O
adaptation	B-Task
endeavour	E-Task
.	O

The	O
baselines	O
use	O
exactly	O
the	O
same	O
learner	S-Method
we	O
use	O
for	O
PyDCI	S-Method
(	O
LinearSVC	S-Method
with	O
the	O
parameter	O
optimized	O
via	O
grid	B-Method
search	E-Method
)	O
.	O

For	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
we	O
also	O
report	O
the	O
accuracy	S-Metric
obtained	O
by	O
what	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
today	O
the	O
best	O
-	O
performing	O
known	O
method	O
on	O
this	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
(	O
Column	O
5	O
–	O
labelled	O
as	O
‘	O
‘	O
SOTA	O
’	O
’	O
,	O
which	O
stands	O
for	O
‘	O
‘	O
S	O
tate	O
O	O
f	O
T	O
he	O
A	O
rt	O
’	O
’	O
–	O
reports	O
the	O
name	O
of	O
the	O
method	O
and	O
Column	O
6	O
reports	O
the	O
accuracy	B-Metric
score	E-Metric
,	O
taken	O
from	O
the	O
original	O
paper	O
)	O
.	O

Boldface	O
indicates	O
the	O
best	O
score	O
for	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
;	O
shadowed	O
cells	O
indicate	O
the	O
PyDCI	O
scores	O
that	O
outperform	O
the	O
best	O
-	O
known	O
results	O
.	O

Note	O
that	O
,	O
aside	O
from	O
SDA	S-Method
,	O
all	O
the	O
baselines	O
in	O
the	O
‘	O
‘	O
SOTA	O
’	O
’	O
column	O
had	O
not	O
been	O
used	O
as	O
baselines	O
in	O
our	O
original	O
work	O
on	O
DCI	S-Method
;	O
the	O
reason	O
is	O
that	O
these	O
methods	O
were	O
published	O
after	O
DCI	S-Method
appeared	O
in	O
print	O
,	O
or	O
that	O
we	O
were	O
unaware	O
of	O
them	O
.	O

PyDCI	S-Method
outperforms	O
JaDCI	S-Method
in	O
most	O
cases	O
,	O
and	O
outperforms	O
also	O
the	O
best	O
-	O
performing	O
method	O
in	O
the	O
literature	O
,	O
which	O
is	O
not	O
always	O
the	O
same	O
for	O
each	O
(	O
problem	O
,	O
dataset	O
)	O
pair	O
,	O
with	O
very	O
few	O
exceptions	O
.	O

PyDCI	S-Method
obtains	O
7	O
out	O
of	O
13	O
best	O
results	O
on	O
MDS	S-Task
(	O
including	O
best	O
averaged	B-Metric
accuracy	E-Metric
)	O
when	O
equipped	O
with	O
the	O
Cosine	S-Method
DCF	O
,	O
and	O
5	O
out	O
of	O
10	O
best	O
results	O
in	O
Webis	B-Task
-	I-Task
CLS	I-Task
-	I-Task
10	E-Task
when	O
using	O
the	O
Linear	B-Method
DCF	E-Method
(	O
including	O
best	O
averaged	B-Metric
accuracy	E-Metric
)	O
.	O

In	O
agreement	O
with	O
with	O
,	O
Cosine	S-Method
proved	O
the	O
best	O
performing	O
DCF	S-Method
,	O
yielding	O
the	O
best	O
results	O
overall	O
and	O
surpassing	O
the	O
best	O
accuracy	S-Metric
obtained	O
by	O
any	O
other	O
method	O
in	O
17	O
cases	O
out	O
of	O
23	O
(	O
across	O
the	O
two	O
datasets	O
,	O
and	O
also	O
including	O
the	O
average	S-Metric
results	O
)	O
.	O

With	O
respect	O
to	O
the	O
previously	O
best	O
-	O
performing	O
system	O
,	O
PyDCI	S-Method
(	O
Cosine	S-Method
)	O
brings	O
about	O
a	O
reduction	O
in	O
error	S-Metric
of	O
+	O
10.2	O
%	O
on	O
MDS	S-Method
and	O
+	O
9.6	O
%	O
on	O
Webis	B-Task
-	I-Task
CLS	I-Task
-	I-Task
10	E-Task
.	O

On	O
the	O
very	O
same	O
(	O
problem	O
,	O
dataset	O
)	O
pairs	O
we	O
have	O
also	O
run	O
experiments	O
in	O
order	O
to	O
evaluate	O
the	O
impact	O
of	O
modifications	O
[	O
reference	O
]	O
(	O
Document	B-Task
Standardization	E-Task
)	O
and	O
[	O
reference	O
]	O
(	O
Classifier	B-Task
Optimization	E-Task
)	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Concerning	O
document	B-Task
standardization	E-Task
,	O
we	O
have	O
rerun	O
all	O
the	O
PyDCI	S-Method
experiments	O
described	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
without	O
applying	O
document	B-Method
standardization	E-Method
.	O

The	O
results	O
are	O
reported	O
in	O
the	O
first	O
two	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
,	O
and	O
indicate	O
,	O
on	O
average	S-Metric
,	O
a	O
relative	O
improvement	O
in	O
accuracy	S-Metric
of	O
+	O
0.2	O
%	O
on	O
MDS	S-Method
and	O
+	O
8.6	O
%	O
on	O
Webis	B-Task
-	I-Task
CLS	I-Task
-	I-Task
10	E-Task
;	O
document	B-Task
standardization	E-Task
thus	O
appears	O
to	O
be	O
clearly	O
beneficial	O
.	O

Concerning	O
classifier	B-Method
optimization	E-Method
,	O
we	O
have	O
rerun	O
all	O
the	O
PyDCI	S-Method
experiments	O
described	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
without	O
applying	O
classifier	B-Method
optimization	E-Method
.	O

The	O
results	O
are	O
reported	O
in	O
the	O
last	O
two	O
rows	O
of	O
Table	O
[	O
reference	O
]	O
,	O
and	O
indicate	O
,	O
on	O
average	S-Metric
,	O
a	O
relative	O
improvement	O
in	O
accuracy	S-Metric
of	O
+	O
9.7	O
%	O
on	O
MDS	S-Method
and	O
+	O
11.8	O
%	O
on	O
Webis	B-Method
-	I-Method
CLS	I-Method
-	I-Method
10	E-Method
;	O
also	O
classifier	B-Method
optimization	E-Method
is	O
thus	O
(	O
unsurprisingly	O
)	O
clearly	O
beneficial	O
.	O

subsection	O
:	O
Effectiveness	O
on	O
Cross	B-Task
-	I-Task
Domain	I-Task
Cross	I-Task
-	I-Task
Lingual	I-Task
Classification	E-Task
Table	O
[	O
reference	O
]	O
reports	O
classification	B-Metric
accuracy	I-Metric
values	E-Metric
obtained	O
in	O
the	O
domain	B-Task
adaptation	I-Task
setting	E-Task
proposed	O
in	O
,	O
in	O
which	O
both	O
domain	O
and	O
language	O
differ	O
between	O
the	O
source	O
and	O
target	O
(	O
i.e.	O
,	O
when	O
the	O
classification	B-Task
task	E-Task
is	O
simultaneously	O
cross	O
-	O
domain	O
and	O
cross	O
-	O
lingual	O
)	O
.	O

In	O
Table	O
[	O
reference	O
]	O
we	O
include	O
the	O
results	O
we	O
had	O
obtained	O
in	O
for	O
the	O
Cross	B-Method
-	I-Method
Lingual	I-Method
Structural	I-Method
Correspondence	I-Method
Learning	I-Method
(	I-Method
SCL	I-Method
)	I-Method
method	E-Method
(	O
which	O
we	O
use	O
here	O
as	O
a	O
baseline	O
)	O
,	O
using	O
its	O
authors	O
’	O
code	O
(	O
see	O
)	O
.	O

The	O
reason	O
why	O
we	O
use	O
SCL	S-Method
as	O
a	O
baseline	O
is	O
that	O
,	O
although	O
newer	O
approaches	O
have	O
been	O
tested	O
in	O
this	O
setting	O
,	O
none	O
of	O
them	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
has	O
outperformed	O
SCL	S-Method
so	O
far	O
.	O

The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
confirm	O
the	O
superiority	O
of	O
PyDCI	S-Method
over	O
JaDCI	S-Method
.	O

In	O
this	O
case	O
,	O
though	O
,	O
the	O
differences	O
in	O
performance	O
between	O
the	O
‘	O
‘	O
Cosine	S-Method
’	O
’	O
counterparts	O
is	O
less	O
pronounced	O
.	O

Between	O
the	O
PyDCI	B-Method
variants	E-Method
,	O
Linear	S-Method
performs	O
slightly	O
better	O
than	O
Cosine	S-Method
.	O

subsection	O
:	O
Statistical	B-Metric
Significance	E-Metric
We	O
have	O
subjected	O
our	O
experiments	O
to	O
thorough	O
statistical	B-Task
significance	I-Task
testing	E-Task
,	O
by	O
running	O
a	O
two	O
-	O
tailed	O
t	O
-	O
test	O
on	O
paired	O
examples	O
across	O
all	O
runs	O
(	O
cross	O
-	O
domain	O
and	O
/	O
or	O
cross	O
-	O
lingual	O
)	O
.	O

The	O
test	O
reveals	O
that	O
the	O
PyDCI	O
versions	O
of	O
Linear	O
and	O
Cosine	S-Method
outperform	O
,	O
in	O
a	O
statistically	O
significant	O
sense	O
,	O
the	O
corresponding	O
JaDCI	B-Method
versions	E-Method
(	O
at	O
a	O
confidence	O
level	O
of	O
)	O
.	O

subsection	O
:	O
Efficiency	O
One	O
important	O
aspect	O
of	O
DCI	S-Method
in	O
general	O
,	O
and	O
of	O
PyDCI	S-Method
in	O
particular	O
,	O
is	O
its	O
efficiency	O
.	O

Figure	O
[	O
reference	O
]	O
reports	O
the	O
computation	B-Metric
times	E-Metric
we	O
have	O
recorded	O
in	O
order	O
to	O
measure	O
the	O
efficiency	O
of	O
PyDCI	S-Method
.	O

While	O
the	O
best	O
-	O
performing	O
methods	O
from	O
the	O
literature	O
rely	O
on	O
computationally	O
expensive	O
optimizations	O
(	O
most	O
of	O
them	O
are	O
deep	B-Method
-	I-Method
learning	E-Method
-	O
based	O
)	O
,	O
none	O
of	O
the	O
experiments	O
we	O
have	O
presented	O
so	O
far	O
required	O
more	O
than	O
35	O
seconds	O
to	O
run	O
.	O

subsection	O
:	O
Effectiveness	S-Metric
vs.	O
Efficiency	O
Trade	O
-	O
off	O
In	O
this	O
section	O
we	O
analyse	O
the	O
trade	O
-	O
off	O
between	O
effectiveness	S-Metric
(	O
in	O
terms	O
of	O
classification	B-Metric
accuracy	E-Metric
)	O
and	O
time	B-Metric
efficiency	E-Metric
(	O
in	O
terms	O
of	O
seconds	O
)	O
.	O

In	O
this	O
experiment	O
,	O
we	O
vary	O
the	O
number	O
of	O
pivots	O
in	O
the	O
range	O
.	O

For	O
the	O
Webis	B-Task
-	I-Task
CLS	I-Task
-	I-Task
10	E-Task
we	O
bound	O
this	O
range	O
to	O
pivots	O
since	O
,	O
for	O
some	O
tasks	O
it	O
was	O
impossible	O
to	O
extract	O
more	O
than	O
pivots	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
average	S-Metric
accuracy	O
(	O
left	O
)	O
and	O
computation	B-Metric
times	E-Metric
for	O
MDS	S-Method
and	O
Webis	B-Method
-	I-Method
CLS	I-Method
-	I-Method
10	E-Method
.	O

As	O
increases	O
,	O
PyDCI	S-Method
surpasses	O
the	O
best	O
average	B-Metric
accuracy	E-Metric
reported	O
for	O
any	O
other	O
method	O
in	O
both	O
datasets	O
.	O

In	O
particular	O
,	O
and	O
in	O
accordance	O
with	O
,	O
PyDCI	S-Method
equipped	O
with	O
the	O
Cosine	S-Method
DCF	O
does	O
so	O
with	O
only	O
100	O
pivots	O
.	O

In	O
this	O
case	O
,	O
and	O
in	O
contrast	O
with	O
JaDCI	S-Method
,	O
classification	B-Metric
accuracy	E-Metric
increases	O
noticeably	O
when	O
more	O
pivots	O
are	O
taken	O
into	O
account	O
;	O
this	O
might	O
be	O
a	O
side	O
effect	O
of	O
the	O
modifications	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
any	O
case	O
,	O
the	O
method	O
seems	O
to	O
reach	O
a	O
plateau	O
for	O
higher	O
values	O
of	O
,	O
allowing	O
the	O
Cosine	S-Method
variant	O
to	O
reach	O
new	O
peaks	O
of	O
classification	B-Metric
accuracy	E-Metric
of	O
0.839	O
(	O
when	O
)	O
in	O
MDS	S-Method
,	O
and	O
0.840	O
(	O
when	O
)	O
in	O
Webis	B-Method
-	I-Method
CLS	I-Method
-	I-Method
10	E-Method
.	O

Regarding	O
the	O
efficiency	O
of	O
the	O
method	O
,	O
PyDCI	S-Method
exhibits	O
a	O
quasi	O
-	O
linear	O
trend	O
in	O
time	B-Metric
complexity	E-Metric
,	O
e.g.	O
,	O
when	O
the	O
number	O
of	O
pivots	O
is	O
doubled	O
,	O
the	O
execution	B-Metric
time	E-Metric
is	O
roughly	O
doubled	O
too	O
.	O

section	O
:	O
Conclusions	O
We	O
have	O
presented	O
PyDCI	S-Method
,	O
a	O
(	O
Python	O
-	O
based	O
)	O
revision	O
of	O
our	O
previous	O
(	O
Java	B-Method
-	I-Method
based	I-Method
)	I-Method
implementation	E-Method
of	O
DCI	S-Method
.	O

This	O
new	O
implementation	O
incorporates	O
changes	O
that	O
,	O
although	O
subtle	O
,	O
nonetheless	O
allow	O
the	O
method	O
to	O
deliver	O
improved	O
results	O
that	O
outperform	O
the	O
currently	O
known	O
best	O
-	O
performing	O
methods	O
.	O

The	O
efficiency	O
tests	O
we	O
have	O
carried	O
out	O
speak	O
clearly	O
about	O
the	O
efficiency	O
of	O
PyDCI	S-Method
,	O
which	O
requires	O
roughly	O
half	O
a	O
minute	O
to	O
undertake	O
any	O
of	O
the	O
domain	B-Task
adaptation	I-Task
tasks	E-Task
in	O
our	O
experiments	O
.	O

In	O
a	O
preliminary	O
study	O
DCI	S-Method
was	O
also	O
tested	O
in	O
transductive	B-Task
scenarios	E-Task
.	O

PyDCI	S-Method
does	O
not	O
support	O
transductive	B-Method
classification	E-Method
;	O
this	O
is	O
something	O
we	O
plan	O
to	O
address	O
in	O
the	O
near	O
future	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Face	B-Method
Attention	I-Method
Network	E-Method
:	O
An	O
Effective	O
Face	B-Method
Detector	E-Method
for	O
the	O
Occluded	O
Faces	O
The	O
performance	O
of	O
face	B-Task
detection	E-Task
has	O
been	O
largely	O
improved	O
with	O
the	O
development	O
of	O
convolutional	B-Method
neural	I-Method
network	E-Method
.	O

However	O
,	O
the	O
occlusion	B-Task
issue	E-Task
due	O
to	O
mask	O
and	O
sunglasses	O
,	O
is	O
still	O
a	O
challenging	O
problem	O
.	O

The	O
improvement	O
on	O
the	O
recall	S-Metric
of	O
these	O
occluded	O
cases	O
usually	O
brings	O
the	O
risk	O
of	O
high	O
false	B-Metric
positives	E-Metric
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
face	S-Task
detector	O
called	O
Face	B-Method
Attention	I-Method
Network	E-Method
(	O
FAN	S-Method
)	O
,	O
which	O
can	O
significantly	O
improve	O
the	O
recall	S-Metric
of	O
the	O
face	B-Task
detection	I-Task
problem	E-Task
in	O
the	O
occluded	O
case	O
without	O
compromising	O
the	O
speed	O
.	O

More	O
specifically	O
,	O
we	O
propose	O
a	O
new	O
anchor	O
-	O
level	O
attention	O
,	O
which	O
will	O
highlight	O
the	O
features	O
from	O
the	O
face	S-Task
region	O
.	O

Integrated	O
with	O
our	O
anchor	B-Method
assign	I-Method
strategy	E-Method
and	O
data	B-Method
augmentation	I-Method
techniques	E-Method
,	O
we	O
obtain	O
state	O
-	O
of	O
-	O
art	O
results	O
on	O
public	O
face	B-Task
detection	E-Task
benchmarks	O
like	O
WiderFace	S-Material
and	O
MAFA	S-Material
.	O

The	O
code	O
will	O
be	O
released	O
for	O
reproduction	O
.	O

section	O
:	O
Introduction	O
Face	B-Task
detection	E-Task
is	O
a	O
fundamental	O
and	O
essential	O
step	O
for	O
many	O
face	S-Task
related	O
applications	O
,	O
e.g.	O
face	S-Task
landmark	O
and	O
face	S-Task
recognition	O
.	O

Starting	O
from	O
the	O
pioneering	O
work	O
of	O
Viola	B-Task
-	I-Task
Jones	E-Task
,	O
face	B-Task
detection	E-Task
witnessed	O
a	O
large	O
number	O
of	O
progress	O
,	O
especially	O
as	O
the	O
recent	O
development	O
of	O
convolutional	B-Method
neural	I-Method
networks	E-Method
.	O

However	O
,	O
the	O
occlusion	B-Task
problem	E-Task
is	O
still	O
a	O
challenging	O
problem	O
and	O
few	O
of	O
the	O
work	O
have	O
been	O
presented	O
to	O
address	O
this	O
issue	O
.	O

More	O
importantly	O
,	O
occlusion	O
caused	O
by	O
mask	O
,	O
sunglasses	O
or	O
other	O
faces	O
widely	O
exists	O
in	O
the	O
real	B-Task
-	I-Task
life	I-Task
applications	E-Task
.	O

The	O
difficulty	O
to	O
address	O
the	O
occlusion	B-Task
issue	E-Task
lies	O
at	O
the	O
risk	O
of	O
potential	O
false	B-Task
positive	I-Task
problem	E-Task
.	O

Considering	O
the	O
case	O
of	O
detecting	O
a	O
face	S-Task
occluded	O
by	O
a	O
sunglasses	O
,	O
only	O
the	O
lower	O
part	O
of	O
face	S-Task
is	O
available	O
.	O

The	O
models	O
,	O
which	O
can	O
recognize	O
the	O
face	S-Task
only	O
based	O
on	O
the	O
lower	O
part	O
,	O
will	O
be	O
easily	O
misclassified	O
at	O
the	O
positions	O
like	O
hands	O
which	O
share	O
the	O
similar	O
skin	O
color	O
.	O

How	O
to	O
successfully	O
address	O
the	O
occlusion	B-Task
issue	E-Task
and	O
meanwhile	O
prevent	O
the	O
false	B-Task
positive	I-Task
problem	E-Task
is	O
still	O
a	O
challenging	O
research	O
topic	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
an	O
effective	O
face	S-Task
detector	O
based	O
on	O
the	O
one	O
-	O
shot	O
detection	S-Task
pipeline	O
called	O
Face	B-Method
Attention	I-Method
Network	E-Method
(	O
FAN	S-Method
)	O
,	O
which	O
can	O
well	O
address	O
the	O
occlusion	B-Metric
and	I-Metric
false	I-Metric
positive	I-Metric
issue	E-Metric
.	O

More	O
specifically	O
,	O
following	O
the	O
similar	O
setting	O
as	O
RetinaNet	O
,	O
we	O
utilize	O
feature	B-Method
pyramid	I-Method
network	E-Method
,	O
and	O
different	O
layers	O
from	O
the	O
network	O
to	O
solve	O
the	O
faces	O
with	O
different	O
scales	O
.	O

Our	O
anchor	B-Method
setting	E-Method
is	O
designed	O
specifically	O
for	O
the	O
face	S-Task
application	O
and	O
an	O
anchor	O
-	O
level	O
attention	O
is	O
introduced	O
which	O
provides	O
different	O
attention	O
regions	O
for	O
different	O
feature	O
layers	O
.	O

The	O
attention	S-Method
is	O
supervised	O
trained	O
based	O
on	O
the	O
anchor	O
-	O
specific	O
heatmaps	O
.	O

In	O
addition	O
,	O
data	B-Task
augmentation	E-Task
like	O
random	B-Method
crop	E-Method
is	O
introduced	O
to	O
generate	O
more	O
cropped	O
(	O
occluded	O
)	O
training	O
samples	O
.	O

In	O
summary	O
,	O
there	O
are	O
three	O
contributions	O
in	O
our	O
paper	O
.	O

We	O
propose	O
a	O
anchor	B-Method
-	I-Method
level	I-Method
attention	E-Method
,	O
which	O
can	O
well	O
address	O
the	O
occlusion	B-Task
issue	E-Task
in	O
the	O
face	B-Task
detection	I-Task
task	E-Task
.	O

One	O
illustrative	O
example	O
for	O
our	O
detection	S-Task
results	O
in	O
the	O
crowd	B-Task
case	E-Task
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
practical	O
baseline	O
setting	O
is	O
introduced	O
based	O
on	O
the	O
one	B-Method
-	I-Method
shot	I-Method
RetinaNet	I-Method
detector	E-Method
,	O
which	O
obtains	O
comparable	O
performance	O
with	O
fast	B-Metric
computation	I-Metric
speed	E-Metric
.	O

Our	O
FAN	S-Method
which	O
integrates	O
our	O
reproduced	O
one	B-Method
-	I-Method
shot	I-Method
RetinaNet	I-Method
and	I-Method
anchor	I-Method
-	I-Method
level	I-Method
attention	E-Method
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	O
on	O
the	O
popular	O
face	B-Task
detection	I-Task
benchmarks	E-Task
including	O
WiderFace	S-Material
and	O
MAFA	S-Material
,	O
especially	O
in	O
the	O
occluded	O
cases	O
like	O
MAFA	S-Material
.	O

section	O
:	O
Related	O
Work	O
Face	B-Task
detection	E-Task
as	O
the	O
fundamental	O
problem	O
of	O
computer	B-Task
vision	E-Task
,	O
has	O
been	O
extensively	O
studied	O
.	O

Prior	O
to	O
the	O
renaissance	O
of	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
,	O
numerous	O
of	O
machine	B-Method
learning	I-Method
algorithms	E-Method
are	O
applied	O
to	O
face	B-Task
detection	E-Task
.	O

The	O
pioneering	O
work	O
of	O
Viola	B-Method
-	I-Method
Jones	E-Method
utilizes	O
Adaboost	S-Method
with	O
Haar	B-Method
-	I-Method
like	I-Method
feature	E-Method
to	O
train	O
a	O
cascade	B-Method
model	E-Method
to	O
detect	O
face	S-Task
and	O
get	O
a	O
real	O
-	O
time	O
performance	O
.	O

Also	O
,	O
deformable	B-Method
part	I-Method
models	E-Method
(	O
DPM	B-Method
)	E-Method
is	O
employed	O
for	O
face	B-Task
detection	E-Task
with	O
remarkable	O
performance	O
.	O

However	O
,	O
A	O
limitation	O
of	O
these	O
methods	O
is	O
that	O
their	O
use	O
of	O
weak	O
features	O
,	O
e.g.	O
,	O
HOG	O
or	O
Haar	O
-	O
like	O
features	O
.	O

Recently	O
,	O
deep	B-Method
learning	I-Method
based	I-Method
algorithm	E-Method
is	O
utilized	O
to	O
improve	O
both	O
the	O
feature	B-Method
representation	E-Method
and	O
classification	S-Task
performance	O
.	O

Based	O
on	O
the	O
whether	O
following	O
the	O
proposal	B-Method
and	I-Method
refine	I-Method
strategy	E-Method
,	O
these	O
methods	O
can	O
be	O
divided	O
into	O
:	O
single	B-Method
-	I-Method
stage	I-Method
detector	E-Method
,	O
such	O
as	O
YOLO	S-Method
,	O
SSD	S-Method
,	O
RetinaNet	S-Method
,	O
and	O
two	B-Method
-	I-Method
stage	I-Method
detector	E-Method
such	O
as	O
Faster	B-Method
R	I-Method
-	I-Method
CNN	E-Method
.	O

Single	B-Method
-	I-Method
stage	I-Method
method	E-Method
.	O

CascadeCNN	S-Method
proposes	O
a	O
cascade	B-Method
structure	E-Method
to	O
detect	O
face	S-Task
coarse	O
to	O
fine	O
.	O

MT	O
-	O
CNN	S-Method
develops	O
an	O
architecture	O
to	O
address	O
both	O
the	O
detection	S-Task
and	O
landmark	O
alignment	O
jointly	O
.	O

Later	O
,	O
DenseBox	S-Method
utilizes	O
a	O
unified	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
fully	I-Method
convolutional	I-Method
network	E-Method
to	O
detect	O
confidence	O
and	O
bounding	O
box	O
directly	O
.	O

UnitBox	S-Method
presents	O
a	O
new	O
intersection	B-Metric
-	I-Metric
over	I-Metric
-	I-Metric
union	E-Metric
(	O
IoU	S-Metric
)	O
loss	O
to	O
directly	O
optimize	O
IOU	O
target	O
.	O

SAFD	S-Method
and	O
RSA	B-Method
unit	E-Method
focus	O
on	O
handling	O
scale	B-Task
explicitly	E-Task
using	O
CNN	S-Method
or	O
RNN	S-Method
.	O

RetinaNet	S-Method
introduces	O
a	O
new	O
focal	O
loss	O
to	O
relieve	O
the	O
class	B-Task
imbalance	I-Task
problem	E-Task
.	O

Two	O
-	O
stage	O
method	O
.	O

Beside	O
,	O
face	B-Task
detection	E-Task
has	O
inherited	O
some	O
achievements	O
from	O
generic	O
object	O
detection	S-Task
tasks	O
.	O

use	O
the	O
Faster	O
R	O
-	O
CNN	S-Method
framework	O
to	O
improve	O
the	O
face	B-Task
detection	E-Task
performance	O
.	O

CMS	B-Method
-	I-Method
RCNN	E-Method
enhances	O
Faster	O
R	O
-	O
CNN	S-Method
architecture	O
by	O
adding	O
body	O
context	O
information	O
.	O

Convnet	S-Method
joins	O
Faster	O
R	O
-	O
CNN	S-Method
framework	O
with	O
3D	O
face	S-Task
model	O
to	O
increase	O
occlusion	B-Metric
robustness	E-Metric
.	O

Additionally	O
,	O
Spatial	B-Method
Transformer	I-Method
Networks	E-Method
(	O
STN	S-Method
)	O
and	O
it	O
’s	O
variant	O
,	O
OHEM	S-Method
,	O
grid	B-Method
loss	E-Method
also	O
presents	O
several	O
effective	O
strategies	O
to	O
improve	O
face	B-Task
detection	E-Task
performance	O
.	O

section	O
:	O
Face	B-Method
Attention	I-Method
Network	E-Method
(	O
FAN	S-Method
)	O
Although	O
remarkable	O
improvement	O
have	O
been	O
achieved	O
for	O
the	O
face	B-Task
detection	I-Task
problem	E-Task
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
challenge	O
of	O
locating	B-Task
faces	I-Task
with	I-Task
large	I-Task
occlusion	E-Task
remains	O
.	O

Meanwhile	O
,	O
in	O
many	O
face	S-Task
-	O
related	O
applications	O
such	O
security	B-Task
surveillance	E-Task
,	O
faces	O
usually	O
appear	O
with	O
large	O
occlusion	O
by	O
mask	O
,	O
sunglasses	O
or	O
other	O
faces	O
,	O
which	O
need	O
to	O
be	O
detected	O
as	O
well	O
.	O

addresses	O
this	O
problem	O
by	O
merging	O
information	O
from	O
different	O
feature	O
layers	O
.	O

changes	O
anchor	B-Method
matching	I-Method
strategy	E-Method
in	O
order	O
to	O
increase	O
recall	B-Metric
rate	E-Metric
.	O

Inspire	O
of	O
,	O
we	O
are	O
trying	O
to	O
leverage	O
a	O
fully	B-Method
convolutional	I-Method
feature	I-Method
hierarchy	E-Method
,	O
and	O
each	O
layer	O
targets	O
to	O
handle	O
faces	O
with	O
different	O
scale	O
range	O
by	O
assigning	O
different	O
anchors	O
.	O

Our	O
algorithm	O
,	O
called	O
Face	B-Method
Attention	I-Method
Network	E-Method
(	O
FAN	S-Method
)	O
,	O
can	O
be	O
considered	O
as	O
an	O
integration	O
of	O
a	O
single	B-Method
-	I-Method
stage	I-Method
detector	E-Method
discussed	O
in	O
Section	O
[	O
reference	O
]	O
and	O
our	O
anchor	B-Method
-	I-Method
level	I-Method
attention	E-Method
in	O
Section	O
[	O
reference	O
]	O
.	O

An	O
overview	O
of	O
our	O
network	O
structure	O
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Base	O
Framework	O
Convolutional	B-Method
neural	I-Method
network	E-Method
has	O
different	O
semantic	O
information	O
and	O
spatial	O
resolution	O
at	O
different	O
feature	O
layers	O
.	O

The	O
shallow	O
layers	O
usually	O
have	O
high	O
spatial	O
resolution	O
,	O
which	O
is	O
good	O
for	O
spatial	B-Task
localization	I-Task
of	I-Task
small	I-Task
objects	E-Task
,	O
but	O
low	O
semantic	O
information	O
,	O
which	O
is	O
not	O
good	O
for	O
visual	B-Task
classification	E-Task
.	O

On	O
the	O
other	O
hand	O
,	O
deep	B-Method
layers	E-Method
obtain	O
more	O
semantic	O
information	O
but	O
the	O
spatial	O
resolution	O
is	O
compromised	O
.	O

Recent	O
work	O
like	O
Feature	B-Method
Pyramid	I-Method
Network	E-Method
(	O
FPN	S-Method
)	O
proposes	O
a	O
divide	B-Method
and	I-Method
conquer	I-Method
principle	E-Method
.	O

A	O
U	O
-	O
shape	O
structure	O
is	O
attached	O
to	O
maintain	O
both	O
the	O
high	O
spatial	O
resolution	O
and	O
semantic	O
information	O
.	O

Different	O
scales	O
of	O
objects	O
are	O
split	O
and	O
addressed	O
at	O
different	O
feature	O
layers	O
.	O

Following	O
the	O
design	O
principle	O
,	O
RetinaNet	S-Method
introduced	O
in	O
is	O
a	O
one	B-Method
-	I-Method
stage	I-Method
detector	E-Method
which	O
achieves	O
state	O
-	O
of	O
-	O
art	O
performance	O
on	O
COCO	O
general	O
object	O
detection	S-Task
.	O

It	O
employs	O
FPN	S-Method
with	O
ResNet	S-Method
as	O
backbone	O
to	O
generate	O
a	O
hierarchy	O
of	O
feature	O
pyramids	O
with	O
rich	O
semantic	O
information	O
.	O

Based	O
on	O
this	O
backbone	O
,	O
RetinaNet	S-Method
is	O
attached	O
with	O
two	O
subnets	O
:	O
one	O
for	O
classifying	S-Task
and	O
the	O
other	O
for	O
regressing	S-Task
.	O

We	O
borrow	O
the	O
main	O
network	B-Method
structure	E-Method
from	O
RetinaNet	S-Method
and	O
adapt	O
it	O
for	O
the	O
face	B-Task
detection	I-Task
task	E-Task
.	O

The	O
classification	B-Method
subnet	E-Method
applies	O
four	O
convolution	B-Method
layers	E-Method
each	O
with	O
256	O
filters	O
,	O
followed	O
by	O
a	O
convolution	B-Method
layer	E-Method
with	O
filters	O
where	O
means	O
the	O
number	O
of	O
classes	O
and	O
means	O
the	O
number	O
of	O
anchors	O
per	O
location	O
.	O

For	O
face	B-Task
detection	E-Task
since	O
we	O
use	O
sigmoid	B-Method
activation	E-Method
,	O
and	O
we	O
use	O
in	O
most	O
experiments	O
.	O

All	O
convolution	B-Method
layers	E-Method
in	O
this	O
subnet	O
share	O
parameters	O
across	O
all	O
pyramid	O
levels	O
just	O
like	O
the	O
original	O
RetinaNet	S-Method
.	O

The	O
regression	B-Method
subnet	E-Method
is	O
identical	O
to	O
the	O
classification	B-Method
subnet	E-Method
except	O
that	O
it	O
terminates	O
in	O
convolution	B-Method
filters	E-Method
with	O
linear	B-Method
activation	E-Method
.	O

Figure	O
[	O
reference	O
]	O
provides	O
an	O
overview	O
for	O
our	O
algorithm	O
.	O

Notice	O
,	O
we	O
only	O
draw	O
three	O
levels	O
pyramids	O
for	O
illustrative	O
purpose	O
.	O

subsection	O
:	O
Attention	B-Method
Network	E-Method
Compared	O
with	O
the	O
original	O
RetinaNet	S-Method
,	O
we	O
have	O
designed	O
our	O
anchor	O
setting	O
together	O
with	O
our	O
attention	O
function	O
.	O

There	O
are	O
three	O
design	O
principles	O
:	O
addressing	O
different	O
scales	O
of	O
the	O
faces	O
in	O
different	O
feature	O
layers	O
,	O
highlighting	O
the	O
features	O
from	O
the	O
face	S-Task
region	O
and	O
diminish	O
the	O
regions	O
without	O
face	S-Task
,	O
generating	O
more	O
occluded	O
faces	O
for	O
training	O
.	O

subsubsection	O
:	O
Anchor	B-Method
Assign	I-Method
Strategy	E-Method
We	O
start	O
the	O
discussion	O
of	O
our	O
anchor	O
setting	O
first	O
.	O

In	O
our	O
FAN	S-Method
,	O
we	O
have	O
five	O
detector	B-Method
layers	E-Method
each	O
associated	O
with	O
a	O
specific	O
scale	O
anchor	O
.	O

In	O
addition	O
,	O
the	O
aspect	O
ratio	O
for	O
our	O
anchor	O
is	O
set	O
as	O
1	O
and	O
1.5	O
,	O
because	O
most	O
of	O
frontal	O
faces	O
are	O
approximately	O
square	O
and	O
profile	O
faces	O
can	O
be	O
considered	O
as	O
a	O
1:1.5	O
rectangle	O
.	O

Besides	O
,	O
we	O
calculate	O
the	O
statistics	O
from	O
the	O
WiderFace	S-Material
train	O
set	O
based	O
on	O
the	O
ground	O
-	O
truth	O
face	S-Task
size	O
.	O

As	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
more	O
than	O
80	O
%	O
faces	O
have	O
an	O
object	O
scale	O
from	O
16	O
to	O
406	O
pixel	O
.	O

Faces	O
with	O
small	O
size	O
lack	O
sufficient	O
resolution	O
and	O
therefore	O
it	O
may	O
not	O
be	O
a	O
good	O
choice	O
to	O
include	O
in	O
the	O
training	O
data	O
.	O

Thus	O
,	O
we	O
set	O
our	O
anchors	O
from	O
areas	O
of	O
to	O
on	O
pyramid	O
levels	O
.	O

We	O
set	O
anchor	O
scale	O
step	O
to	O
,	O
which	O
ensure	O
every	O
ground	O
-	O
truth	O
boxes	O
have	O
anchor	O
with	O
IoU	S-Metric
.	O

Specifically	O
,	O
anchors	O
are	O
assigned	O
to	O
a	O
ground	O
-	O
truth	O
box	O
with	O
the	O
highest	O
IoU	S-Metric
larger	O
than	O
,	O
and	O
to	O
background	O
if	O
the	O
highest	O
IoU	S-Metric
is	O
less	O
than	O
.	O

Unassigned	O
anchors	O
are	O
ignored	O
during	O
training	O
.	O

subsubsection	O
:	O
Attention	B-Method
Function	E-Method
To	O
address	O
the	O
occlusion	B-Task
issue	E-Task
,	O
we	O
propose	O
a	O
novel	O
anchor	B-Method
-	I-Method
level	I-Method
attention	E-Method
based	O
on	O
the	O
network	O
structure	O
mentioned	O
above	O
.	O

Specifically	O
,	O
we	O
utilize	O
a	O
segment	O
-	O
like	O
side	O
branch	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
attention	O
supervision	O
information	O
is	O
obtained	O
by	O
filling	O
the	O
ground	O
-	O
truth	O
box	O
.	O

Meanwhile	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
supervised	O
heatmaps	O
are	O
associated	O
to	O
the	O
ground	O
-	O
truth	O
faces	O
assigned	O
to	O
the	O
anchors	O
in	O
the	O
current	O
layer	O
.	O

These	O
hierarchical	O
attention	O
maps	O
could	O
decrease	O
the	O
correlation	O
among	O
them	O
.	O

Different	O
from	O
traditional	O
usage	O
of	O
attention	B-Method
map	E-Method
,	O
which	O
naively	O
multiple	O
it	O
with	O
the	O
feature	O
maps	O
,	O
our	O
attention	B-Method
maps	E-Method
are	O
first	O
feed	O
to	O
an	O
exponential	B-Method
operation	E-Method
and	O
then	O
dot	O
with	O
feature	B-Method
maps	E-Method
.	O

It	O
is	O
able	O
to	O
keep	O
more	O
context	O
information	O
,	O
and	O
meanwhile	O
highlight	O
the	O
detection	S-Task
information	O
.	O

Considering	O
the	O
example	O
with	O
occluded	O
face	S-Task
,	O
most	O
invisible	O
parts	O
are	O
not	O
useful	O
and	O
may	O
be	O
harmful	O
for	O
detection	S-Task
.	O

Some	O
of	O
the	O
attention	O
results	O
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Our	O
attention	O
mask	O
can	O
enhance	O
the	O
feature	O
maps	O
in	O
the	O
facial	O
area	O
,	O
and	O
diminish	O
the	O
opposition	O
.	O

subsubsection	O
:	O
Data	B-Task
Augmentation	E-Task
We	O
find	O
that	O
the	O
number	O
of	O
occluded	O
faces	O
in	O
the	O
training	O
dataset	O
,	O
e.g.	O
,	O
WiderFace	S-Material
train	O
,	O
is	O
limited	O
and	O
can	O
not	O
satisfy	O
the	O
training	O
of	O
CNN	S-Method
network	O
.	O

Only	O
16	O
%	O
faces	O
are	O
with	O
highly	O
occlusion	O
property	O
from	O
the	O
annotation	O
.	O

Thus	O
,	O
we	O
propose	O
a	O
random	B-Method
crop	I-Method
strategy	E-Method
,	O
which	O
can	O
generate	O
a	O
large	O
number	O
of	O
occluded	O
faces	O
for	O
training	O
.	O

More	O
specifically	O
,	O
based	O
on	O
the	O
training	O
set	O
,	O
we	O
randomly	O
crop	O
square	O
patches	O
from	O
original	O
images	O
,	O
whose	O
range	O
between	O
[	O
0.3	O
,	O
1	O
]	O
of	O
the	O
short	O
edge	O
from	O
the	O
original	O
images	O
.	O

In	O
addition	O
,	O
We	O
keep	O
the	O
overlapped	O
part	O
of	O
the	O
ground	O
-	O
truth	O
box	O
if	O
its	O
center	O
is	O
in	O
the	O
sampled	O
patch	O
.	O

Besides	O
from	O
the	O
random	B-Task
crop	I-Task
dataset	I-Task
augmentation	E-Task
,	O
we	O
also	O
employ	O
augmentation	S-Method
from	O
random	O
flip	O
and	O
color	O
jitter	O
.	O

subsection	O
:	O
Loss	B-Method
function	E-Method
We	O
employ	O
a	O
multi	B-Method
-	I-Method
task	I-Method
loss	I-Method
function	E-Method
to	O
jointly	O
optimize	O
model	O
parameters	O
:	O
where	O
k	O
is	O
the	O
index	O
of	O
an	O
feature	O
pyramid	O
level	O
(	O
)	O
,	O
and	O
represents	O
the	O
set	O
of	O
anchors	O
defined	O
in	O
pyramid	O
level	O
.	O

The	O
ground	O
-	O
truth	O
label	O
is	O
1	O
if	O
the	O
anchor	O
is	O
positive	O
,	O
0	O
otherwise	O
.	O

is	O
the	O
predicted	O
classification	O
result	O
from	O
our	O
model	O
.	O

is	O
a	O
vector	O
representing	O
the	O
4	O
parameterized	O
coordinates	O
of	O
the	O
predicted	O
bounding	O
box	O
,	O
and	O
is	O
that	O
of	O
the	O
ground	O
-	O
truth	O
box	O
associated	O
with	O
a	O
positive	O
anchor	O
.	O

The	O
classification	B-Metric
loss	E-Metric
is	O
focal	O
loss	O
introduced	O
in	O
over	O
two	O
classes	O
(	O
face	S-Task
and	O
background	O
)	O
.	O

is	O
the	O
number	O
of	O
anchors	O
in	O
which	O
participate	O
in	O
the	O
classification	B-Task
loss	I-Task
computation	E-Task
.	O

The	O
regression	B-Method
loss	E-Method
is	O
smooth	B-Method
L1	I-Method
loss	E-Method
defined	O
in	O
.	O

is	O
the	O
indicator	O
function	O
that	O
limits	O
the	O
regression	O
loss	O
only	O
focusing	O
on	O
the	O
positively	O
assigned	O
anchors	O
,	O
and	O
.	O

The	O
attention	O
loss	O
is	O
pixel	B-Method
-	I-Method
wise	I-Method
sigmoid	I-Method
cross	I-Method
entropy	E-Method
.	O

is	O
the	O
attention	O
map	O
generated	O
per	O
level	O
,	O
and	O
is	O
the	O
ground	O
-	O
truth	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

and	O
are	O
used	O
to	O
balance	O
these	O
three	O
loss	O
terms	O
,	O
here	O
we	O
simply	O
set	O
.	O

section	O
:	O
Experiments	O
We	O
use	O
ResNet	B-Method
-	I-Method
50	E-Method
as	O
base	O
model	O
.	O

All	O
models	O
are	O
trained	O
by	O
SGD	S-Method
over	O
8	O
GPUs	S-Method
with	O
a	O
total	O
of	O
32	O
images	O
per	O
mini	O
-	O
batch	O
(	O
4	O
images	O
per	O
GPU	O
)	O
.	O

Similar	O
to	O
,	O
the	O
four	O
convolution	B-Method
layers	E-Method
attached	O
to	O
FPN	S-Method
are	O
initialized	O
with	O
bias	O
and	O
Gaussian	O
weight	O
variance	O
.	O

For	O
the	O
final	O
convolution	B-Method
layer	E-Method
of	O
the	O
classification	B-Method
subnet	E-Method
,	O
we	O
initiate	O
them	O
with	O
bias	O
and	O
here	O
.	O

Meanwhile	O
,	O
the	O
initial	O
learning	B-Metric
rate	E-Metric
is	O
set	O
as	O
3e	O
-	O
3	O
.	O

We	O
sample	O
10k	O
image	O
patches	O
per	O
epoch	O
.	O

Models	O
without	O
data	B-Method
augmentation	E-Method
are	O
trained	O
for	O
30	O
epochs	O
.	O

With	O
data	B-Method
augmentation	E-Method
,	O
models	O
are	O
trained	O
for	O
130	O
epochs	O
,	O
whose	O
learning	B-Metric
rate	E-Metric
is	O
dropped	O
by	O
10	O
at	O
100	O
epochs	O
and	O
again	O
at	O
120	O
epochs	O
.	O

Weight	O
decay	O
is	O
1e	O
-	O
5	O
and	O
momentum	O
is	O
0.9	O
.	O

Anchors	O
with	O
IoU	S-Metric
are	O
assigned	O
to	O
positive	O
class	O
and	O
anchors	O
which	O
have	O
an	O
IoU	S-Metric
with	O
all	O
ground	O
-	O
truth	O
are	O
assigned	O
to	O
the	O
background	O
class	O
.	O

subsection	O
:	O
Datasets	O
The	O
performance	O
of	O
FAN	S-Method
is	O
evaluated	O
across	O
multiple	O
face	S-Task
datasets	O
:	O
WiderFace	S-Material
and	O
MAFA	S-Material
.	O

WiderFace	S-Material
dataset	O
[	O
]	O
:	O
WiderFace	S-Material
dataset	O
contains	O
32	O
,	O
203	O
images	O
and	O
393	O
,	O
703	O
annotated	O
faces	O
with	O
a	O
high	O
degree	O
of	O
variability	O
in	O
scale	O
,	O
pose	O
and	O
occlusion	O
.	O

158	O
,	O
989	O
of	O
these	O
are	O
chosen	O
as	O
train	O
set	O
,	O
39	O
,	O
496	O
are	O
in	O
validation	O
set	O
and	O
the	O
rest	O
are	O
test	O
set	O
.	O

The	O
validation	O
set	O
and	O
test	O
set	O
are	O
split	O
into	O
’	O
easy	O
’	O
,	O
’	O
medium	O
’	O
,	O
’	O
hard	O
’	O
subsets	O
,	O
in	O
terms	O
of	O
the	O
difficulties	O
of	O
the	O
detection	S-Task
.	O

Due	O
to	O
the	O
variability	O
of	O
scale	O
,	O
pose	O
and	O
occlusion	O
,	O
WiderFace	S-Material
dataset	O
is	O
one	O
of	O
the	O
most	O
challenge	O
face	S-Task
datasets	O
.	O

Our	O
FAN	S-Method
is	O
trained	O
only	O
on	O
the	O
train	O
set	O
and	O
evaluate	O
on	O
both	O
validation	B-Metric
set	E-Metric
and	O
test	O
set	O
.	O

Ablation	O
studies	O
are	O
performed	O
on	O
the	O
validation	O
set	O
.	O

MAFA	S-Material
dataset	O
[	O
]	O
:	O
MAFA	S-Material
dataset	O
contains	O
30	O
,	O
811	O
images	O
with	O
35	O
,	O
806	O
masked	O
faces	O
collected	O
from	O
Internet	O
.	O

It	O
is	O
a	O
face	B-Task
detection	I-Task
benchmark	E-Task
for	O
masked	O
face	S-Task
,	O
in	O
which	O
faces	O
have	O
vast	O
various	O
orientations	O
and	O
occlusion	O
.	O

Beside	O
,	O
this	O
dataset	O
is	O
divided	O
into	O
masked	O
face	S-Task
subset	O
and	O
unmasked	O
face	S-Task
subset	O
according	O
to	O
whether	O
at	O
least	O
one	O
part	O
of	O
each	O
face	S-Task
is	O
occluded	O
by	O
mask	O
.	O

We	O
use	O
both	O
the	O
whole	O
dataset	O
and	O
occluded	O
subset	O
to	O
evaluate	O
our	O
method	O
.	O

subsubsection	O
:	O
Anchor	B-Task
setting	E-Task
and	O
assign	O
We	O
compare	O
three	O
anchor	O
settings	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
the	O
RetinaNet	B-Task
setting	E-Task
,	O
we	O
follow	O
the	O
setting	O
described	O
in	O
the	O
paper	O
.	O

For	O
our	O
FAN	S-Method
baseline	O
,	O
we	O
set	O
our	O
anchors	O
from	O
areas	O
of	O
to	O
on	O
pyramid	O
levels	O
.	O

In	O
addition	O
,	O
the	O
aspect	O
ratio	O
is	O
set	O
to	O
1	O
and	O
1.5	O
.	O

Also	O
,	O
inspire	O
of	O
,	O
we	O
choose	O
an	O
anchor	O
assign	O
rule	O
with	O
more	O
cover	B-Metric
rate	E-Metric
.	O

We	O
uses	O
8	O
anchors	O
per	O
location	O
spanning	O
4	O
scales	O
(	O
intervals	O
are	O
still	O
fixed	O
to	O
so	O
that	O
areas	O
from	O
to	O
)	O
and	O
2	O
ratios	O
1	O
,	O
1.5	O
.	O

For	O
the	O
dense	B-Task
setting	E-Task
,	O
it	O
is	O
the	O
same	O
as	O
our	O
FAN	S-Method
setting	O
except	O
we	O
apply	O
more	O
dense	O
scales	O
from	O
to	O
.	O

Based	O
on	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
anchor	O
scale	O
is	O
important	O
to	O
detector	S-Task
performance	O
and	O
we	O
can	O
see	O
that	O
our	O
setting	O
is	O
obviously	O
superior	O
to	O
the	O
setting	O
in	O
.	O

Compared	O
with	O
the	O
dense	O
setting	O
,	O
we	O
can	O
see	O
that	O
anchor	B-Metric
cover	I-Metric
rate	E-Metric
is	O
not	O
equal	O
to	O
the	O
final	O
detection	S-Task
performance	O
as	O
it	O
may	O
introduce	O
a	O
lot	O
of	O
negative	O
windows	O
due	O
to	O
the	O
dense	B-Method
sampling	E-Method
.	O

subsubsection	O
:	O
Attention	B-Method
mechanism	E-Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
apply	O
anchor	B-Method
-	I-Method
level	I-Method
attention	I-Method
mechanism	E-Method
to	O
enhance	O
the	O
facial	O
parts	O
.	O

We	O
compare	O
our	O
FAN	S-Method
baseline	O
with	O
and	O
without	O
attention	O
in	O
Table	O
[	O
reference	O
]	O
for	O
the	O
WiderFace	S-Material
val	O
dataset	O
.	O

For	O
the	O
MAFA	S-Material
dataset?the	O
results	O
can	O
be	O
found	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Based	O
on	O
the	O
experimental	O
results	O
,	O
we	O
can	O
find	O
that	O
our	O
attention	O
can	O
improve	O
1.1	O
%	O
in	O
WiderFace	B-Material
hard	I-Material
subset	E-Material
and	O
2	O
%	O
in	O
MAFA	S-Material
masked	O
subset	O
.	O

subsubsection	O
:	O
Data	B-Task
augmentation	E-Task
According	O
to	O
the	O
statistics	O
from	O
the	O
WiderFace	S-Material
dataset	O
,	O
there	O
are	O
around	O
26	O
%	O
of	O
faces	O
with	O
occlusion	O
.	O

Among	O
them	O
,	O
around	O
16	O
%	O
is	O
of	O
serious	O
occlusion	O
.	O

As	O
we	O
are	O
targeting	O
to	O
solve	O
the	O
occluded	O
faces	O
,	O
the	O
number	O
of	O
training	O
samples	O
with	O
occlusion	O
may	O
not	O
be	O
sufficient	O
.	O

Thus	O
,	O
we	O
employ	O
the	O
the	O
random	B-Method
crop	I-Method
data	I-Method
augmentation	E-Method
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
results	O
can	O
be	O
found	O
from	O
Table	O
[	O
reference	O
]	O
.	O

The	O
performance	O
improvement	O
is	O
significant	O
.	O

Besides	O
from	O
the	O
benefits	O
for	O
the	O
occluded	O
face	S-Task
,	O
our	O
random	B-Method
crop	I-Method
augmentation	E-Method
potentially	O
improve	O
the	O
performance	O
of	O
small	O
faces	O
as	O
more	O
small	O
faces	O
will	O
be	O
enlarged	O
after	O
augmentation	O
.	O

subsubsection	O
:	O
WiderFace	S-Material
Dataset	O
We	O
compare	O
our	O
FAN	S-Method
with	O
the	O
state	O
-	O
of	O
-	O
art	O
detectors	S-Method
like	O
SFD	S-Method
,	O
SSH	S-Method
,	O
HR	S-Method
and	O
ScaleFace	S-Method
.	O

Our	O
FAN	S-Method
is	O
trained	O
on	O
WiderFace	S-Material
train	O
set	O
with	O
data	O
augmentation	O
and	O
tested	O
on	O
both	O
validation	O
and	O
test	O
set	O
with	O
multi	O
-	O
scale	O
600	O
,	O
800	O
,	O
1000	O
,	O
1200	O
,	O
1400	O
.	O

The	O
precision	B-Metric
-	I-Metric
recall	I-Metric
curves	E-Metric
and	O
AP	S-Method
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
algorithm	O
obtains	O
the	O
best	O
result	O
in	O
all	O
subsets	O
,	O
i.e.	O
0.953	O
(	O
Easy	O
)	O
,	O
0.942	O
(	O
Medium	O
)	O
and	O
0.888	O
(	O
Hard	O
)	O
for	O
validation	O
set	O
,	O
and	O
0.946	O
(	O
Easy	O
)	O
,	O
0.936	O
(	O
Medium	O
)	O
and	O
0.885	O
(	O
Hard	O
)	O
for	O
test	O
set	O
.	O

Considering	O
the	O
hard	O
subset	O
which	O
contains	O
a	O
lot	O
of	O
occluded	O
faces	O
,	O
we	O
have	O
larger	O
margin	O
compared	O
with	O
the	O
previous	O
state	O
-	O
art	O
-	O
results	O
,	O
which	O
validates	O
the	O
effectiveness	O
of	O
our	O
algorithm	O
for	O
the	O
occluded	O
faces	O
.	O

Example	O
results	O
from	O
our	O
FAN	S-Method
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
MAFA	S-Material
Dataset	O
As	O
MAFA	S-Material
dataset	O
is	O
specifically	O
designed	O
for	O
the	O
occluded	B-Task
face	I-Task
detection	E-Task
,	O
it	O
is	O
adopted	O
to	O
evaluate	O
our	O
algorithm	O
.	O

We	O
compare	O
our	O
FAN	S-Method
with	O
LLE	B-Method
-	I-Method
CNNs	E-Method
and	O
AOFD	S-Method
.	O

Our	O
FAN	S-Method
is	O
trained	O
on	O
WiderFace	S-Material
train	O
set	O
with	O
data	O
augmentation	O
and	O
tested	O
on	O
MAFA	S-Material
test	O
set	O
with	O
scale	O
400	O
,	O
600	O
,	O
800	O
,	O
1000	O
.	O

The	O
results	O
based	O
on	O
average	B-Metric
precision	E-Metric
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

FAN	S-Method
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	O
on	O
MAFA	S-Material
test	O
set	O
with	O
standard	O
testing	S-Metric
(	O
IoU	S-Metric
threshold	O
=	O
0.5	O
)	O
,	O
which	O
shows	O
the	O
promising	O
performance	O
on	O
occluded	O
faces	O
.	O

Example	O
results	O
from	O
our	O
FAN	S-Method
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Inference	B-Metric
Time	E-Metric
Despite	O
great	O
performance	O
obtained	O
by	O
our	O
FAN	S-Method
,	O
the	O
speed	O
of	O
our	O
algorithm	O
is	O
not	O
compromised	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
our	O
FAN	S-Method
detector	O
can	O
not	O
only	O
obtain	O
the	O
state	O
-	O
of	O
-	O
art	O
results	O
but	O
also	O
possess	O
efficient	O
computational	B-Metric
speed	E-Metric
.	O

The	O
computational	B-Metric
cost	E-Metric
is	O
tested	O
on	O
a	O
NIVIDIA	B-Material
TITAN	I-Material
Xp	E-Material
.	O

The	O
min	O
size	O
means	O
the	O
shortest	O
side	O
of	O
the	O
images	O
which	O
are	O
resized	O
to	O
by	O
keeping	O
the	O
aspect	O
ratio	O
.	O

Compared	O
with	O
the	O
baseline	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
when	O
testing	O
with	O
short	O
-	O
side	O
1000	O
,	O
our	O
FAN	S-Method
already	O
outperforms	O
state	O
-	O
of	O
-	O
art	O
detectors	S-Method
like	O
,	O
and	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
are	O
targeting	O
the	O
problem	O
of	O
face	B-Task
detection	E-Task
with	O
occluded	O
faces	O
.	O

We	O
propose	O
FAN	S-Method
detector	O
which	O
can	O
integrate	O
our	O
specifically	O
designed	O
single	B-Method
-	I-Method
stage	I-Method
base	I-Method
net	E-Method
and	O
our	O
anchor	B-Method
-	I-Method
level	I-Method
attention	I-Method
algorithm	E-Method
.	O

Based	O
on	O
our	O
anchor	O
-	O
level	O
attention	O
,	O
we	O
can	O
highlight	O
the	O
features	O
from	O
the	O
facial	O
regions	O
and	O
successfully	O
relieving	O
the	O
risk	O
from	O
the	O
false	O
positives	O
.	O

Experimental	O
results	O
on	O
challenging	O
benchmarks	O
like	O
WiderFace	S-Material
and	O
MAFA	S-Material
validate	O
the	O
effectiveness	O
and	O
efficiency	O
of	O
our	O
proposed	O
algorithm	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Scaling	B-Method
Memory	I-Method
-	I-Method
Augmented	I-Method
Neural	I-Method
Networks	E-Method
with	O
Sparse	B-Task
Reads	I-Task
and	I-Task
Writes	E-Task
Neural	B-Method
networks	E-Method
augmented	O
with	O
external	O
memory	O
have	O
the	O
ability	O
to	O
learn	O
algorithmic	B-Task
solutions	E-Task
to	O
complex	O
tasks	O
.	O

These	O
models	O
appear	O
promising	O
for	O
applications	O
such	O
as	O
language	B-Task
modeling	E-Task
and	O
machine	B-Task
translation	E-Task
.	O

However	O
,	O
they	O
scale	O
poorly	O
in	O
both	O
space	S-Metric
and	O
time	S-Metric
as	O
the	O
amount	O
of	O
memory	O
grows	O
—	O
limiting	O
their	O
applicability	O
to	O
real	B-Task
-	I-Task
world	I-Task
domains	E-Task
.	O

Here	O
,	O
we	O
present	O
an	O
end	O
-	O
to	O
-	O
end	B-Method
differentiable	I-Method
memory	I-Method
access	I-Method
scheme	E-Method
,	O
which	O
we	O
call	O
Sparse	B-Method
Access	I-Method
Memory	E-Method
(	O
SAM	S-Method
)	O
,	O
that	O
retains	O
the	O
representational	O
power	O
of	O
the	O
original	O
approaches	O
whilst	O
training	O
efficiently	O
with	O
very	O
large	O
memories	O
.	O

We	O
show	O
that	O
SAM	S-Method
achieves	O
asymptotic	B-Metric
lower	I-Metric
bounds	E-Metric
in	O
space	S-Metric
and	O
time	S-Metric
complexity	O
,	O
and	O
find	O
that	O
an	O
implementation	O
runs	O
faster	O
and	O
with	O
less	O
physical	O
memory	O
than	O
non	B-Method
-	I-Method
sparse	I-Method
models	E-Method
.	O

SAM	S-Method
learns	O
with	O
comparable	O
data	B-Metric
efficiency	E-Metric
to	O
existing	O
models	O
on	O
a	O
range	O
of	O
synthetic	B-Task
tasks	E-Task
and	O
one	B-Task
-	I-Task
shot	I-Task
Omniglot	I-Task
character	I-Task
recognition	E-Task
,	O
and	O
can	O
scale	O
to	O
tasks	O
requiring	O
s	O
of	O
time	S-Metric
steps	O
and	O
memories	O
.	O

As	O
well	O
,	O
we	O
show	O
how	O
our	O
approach	O
can	O
be	O
adapted	O
for	O
models	O
that	O
maintain	O
temporal	O
associations	O
between	O
memories	O
,	O
as	O
with	O
the	O
recently	O
introduced	O
Differentiable	B-Method
Neural	I-Method
Computer	E-Method
.	O

capbtabboxtable	O
[	O
]	O
[	O
]	O
section	O
:	O
Introduction	O
Recurrent	B-Method
neural	I-Method
networks	E-Method
,	O
such	O
as	O
the	O
Long	B-Method
Short	I-Method
-	I-Method
Term	I-Method
Memory	E-Method
(	O
LSTM	S-Method
)	O
,	O
have	O
proven	O
to	O
be	O
powerful	O
sequence	B-Method
learning	I-Method
models	E-Method
.	O

However	O
,	O
one	O
limitation	O
of	O
the	O
LSTM	B-Method
architecture	E-Method
is	O
that	O
the	O
number	O
of	O
parameters	O
grows	O
proportionally	O
to	O
the	O
square	O
of	O
the	O
size	O
of	O
the	O
memory	O
,	O
making	O
them	O
unsuitable	O
for	O
problems	O
requiring	O
large	O
amounts	O
of	O
long	O
-	O
term	O
memory	O
.	O

Recent	O
approaches	O
,	O
such	O
as	O
Neural	B-Method
Turing	I-Method
Machines	E-Method
(	O
NTMs	S-Method
)	O
and	O
Memory	B-Method
Networks	E-Method
,	O
have	O
addressed	O
this	O
issue	O
by	O
decoupling	O
the	O
memory	O
capacity	O
from	O
the	O
number	O
of	O
model	O
parameters	O
.	O

We	O
refer	O
to	O
this	O
class	O
of	O
models	O
as	O
memory	B-Method
augmented	I-Method
neural	I-Method
networks	E-Method
(	O
MANNs	S-Method
)	O
.	O

External	O
memory	O
allows	O
MANNs	S-Method
to	O
learn	O
algorithmic	B-Task
solutions	E-Task
to	O
problems	O
that	O
have	O
eluded	O
the	O
capabilities	O
of	O
traditional	O
LSTMs	S-Method
,	O
and	O
to	O
generalize	O
to	O
longer	O
sequence	O
lengths	O
.	O

Nonetheless	O
,	O
MANNs	S-Method
have	O
had	O
limited	O
success	O
in	O
real	B-Task
world	I-Task
application	E-Task
.	O

A	O
significant	O
difficulty	O
in	O
training	O
these	O
models	O
results	O
from	O
their	O
smooth	O
read	O
and	O
write	O
operations	O
,	O
which	O
incur	O
linear	B-Metric
computational	I-Metric
overhead	E-Metric
on	O
the	O
number	O
of	O
memories	O
stored	O
per	O
time	S-Metric
step	O
of	O
training	O
.	O

Even	O
worse	O
,	O
they	O
require	O
duplication	O
of	O
the	O
entire	O
memory	O
at	O
each	O
time	S-Metric
step	O
to	O
perform	O
backpropagation	O
through	O
time	S-Metric
(	O
BPTT	S-Method
)	O
.	O

To	O
deal	O
with	O
sufficiently	O
complex	O
problems	O
,	O
such	O
as	O
processing	O
a	O
book	O
,	O
or	O
Wikipedia	S-Material
,	O
this	O
overhead	O
becomes	O
prohibitive	O
.	O

For	O
example	O
,	O
to	O
store	O
memories	O
,	O
a	O
straightforward	O
implementation	O
of	O
the	O
NTM	S-Method
trained	O
over	O
a	O
sequence	O
of	O
length	O
consumes	O
physical	O
memory	O
;	O
to	O
store	O
memories	O
the	O
overhead	O
exceeds	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
MANN	S-Method
named	O
SAM	S-Method
(	O
sparse	O
access	O
memory	O
)	O
.	O

By	O
thresholding	O
memory	O
modifications	O
to	O
a	O
sparse	O
subset	O
,	O
and	O
using	O
efficient	O
data	B-Method
structures	E-Method
for	O
content	B-Method
-	I-Method
based	I-Method
read	I-Method
operations	E-Method
,	O
our	O
model	O
is	O
optimal	O
in	O
space	S-Metric
and	O
time	S-Metric
with	O
respect	O
to	O
memory	B-Metric
size	E-Metric
,	O
while	O
retaining	O
end	O
-	O
to	O
-	O
end	B-Method
gradient	I-Method
based	I-Method
optimization	E-Method
.	O

To	O
test	O
whether	O
the	O
model	O
is	O
able	O
to	O
learn	O
with	O
this	O
sparse	B-Method
approximation	E-Method
,	O
we	O
examined	O
its	O
performance	O
on	O
a	O
selection	O
of	O
synthetic	O
and	O
natural	O
tasks	O
:	O
algorithmic	B-Task
tasks	E-Task
from	O
the	O
NTM	O
work	O
,	O
Babi	S-Material
reasoning	O
tasks	O
used	O
with	O
Memory	B-Method
Networks	E-Method
and	O
Omniglot	B-Task
one	I-Task
-	I-Task
shot	I-Task
classification	E-Task
.	O

We	O
also	O
tested	O
several	O
of	O
these	O
tasks	O
scaled	O
to	O
longer	O
sequences	O
via	O
curriculum	B-Task
learning	E-Task
.	O

For	O
large	O
external	O
memories	O
we	O
observed	O
improvements	O
in	O
empirical	O
run	O
-	O
time	S-Metric
and	O
memory	O
overhead	O
by	O
up	O
to	O
three	O
orders	O
magnitude	O
over	O
vanilla	O
NTMs	S-Method
,	O
while	O
maintaining	O
near	O
-	O
identical	O
data	B-Metric
efficiency	E-Metric
and	O
performance	O
.	O

Further	O
,	O
in	O
Supplementary	O
[	O
reference	O
]	O
we	O
demonstrate	O
the	O
generality	O
of	O
our	O
approach	O
by	O
describing	O
how	O
to	O
construct	O
a	O
sparse	B-Method
version	E-Method
of	O
the	O
recently	O
published	O
Differentiable	B-Method
Neural	I-Method
Computer	E-Method
.	O

This	O
Sparse	B-Method
Differentiable	I-Method
Neural	I-Method
Computer	E-Method
(	O
SDNC	S-Method
)	O
is	O
over	O
faster	O
than	O
the	O
canonical	B-Method
dense	I-Method
variant	E-Method
for	O
a	O
memory	O
size	O
of	O
slots	O
,	O
and	O
achieves	O
the	O
best	O
reported	O
result	O
in	O
the	O
Babi	S-Material
tasks	O
without	O
supervising	O
the	O
memory	O
access	O
.	O

section	O
:	O
Background	O
subsection	O
:	O
Attention	S-Task
and	O
content	B-Method
-	I-Method
based	I-Method
addressing	E-Method
An	O
external	O
memory	O
is	O
a	O
collection	O
of	O
real	O
-	O
valued	O
vectors	O
,	O
or	O
words	O
,	O
of	O
fixed	O
size	O
.	O

A	O
soft	O
read	O
operation	O
is	O
defined	O
to	O
be	O
a	O
weighted	B-Method
average	E-Method
over	O
memory	O
words	O
,	O
where	O
is	O
a	O
vector	O
of	O
weights	O
with	O
non	O
-	O
negative	O
entries	O
that	O
sum	O
to	O
one	O
.	O

Attending	O
to	O
memory	O
is	O
formalized	O
as	O
the	O
problem	O
of	O
computing	S-Task
.	O

A	O
content	B-Method
addressable	I-Method
memory	E-Method
,	O
proposed	O
in	O
,	O
is	O
an	O
external	B-Method
memory	E-Method
with	O
an	O
addressing	B-Method
scheme	E-Method
which	O
selects	O
based	O
upon	O
the	O
similarity	O
of	O
memory	O
words	O
to	O
a	O
given	O
query	O
.	O

Specifically	O
,	O
for	O
the	O
th	O
read	O
weight	O
we	O
define	O
,	O
where	O
is	O
a	O
similarity	B-Metric
measure	E-Metric
,	O
typically	O
Euclidean	O
distance	O
or	O
cosine	O
similarity	O
,	O
and	O
is	O
a	O
differentiable	B-Method
monotonic	I-Method
transformation	E-Method
,	O
typically	O
a	O
softmax	S-Method
.	O

We	O
can	O
think	O
of	O
this	O
as	O
an	O
instance	O
of	O
kernel	B-Method
smoothing	E-Method
where	O
the	O
network	O
learns	O
to	O
query	O
relevant	O
points	O
.	O

Because	O
the	O
read	O
operation	O
(	O
[	O
reference	O
]	O
)	O
and	O
content	B-Method
-	I-Method
based	I-Method
addressing	I-Method
scheme	E-Method
(	O
[	O
reference	O
]	O
)	O
are	O
smooth	O
,	O
we	O
can	O
place	O
them	O
within	O
a	O
neural	B-Method
network	E-Method
,	O
and	O
train	O
the	O
full	B-Method
model	E-Method
using	O
backpropagation	S-Method
.	O

subsection	O
:	O
Memory	B-Method
Networks	E-Method
One	O
recent	O
architecture	O
,	O
Memory	B-Method
Networks	E-Method
,	O
make	O
use	O
of	O
a	O
content	B-Method
addressable	I-Method
memory	E-Method
that	O
is	O
accessed	O
via	O
a	O
series	O
of	O
read	B-Method
operations	E-Method
and	O
has	O
been	O
successfully	O
applied	O
to	O
a	O
number	O
of	O
question	B-Task
answering	I-Task
tasks	E-Task
.	O

In	O
these	O
tasks	O
,	O
the	O
memory	O
is	O
pre	O
-	O
loaded	O
using	O
a	O
learned	O
embedding	O
of	O
the	O
provided	O
context	O
,	O
such	O
as	O
a	O
paragraph	O
of	O
text	O
,	O
and	O
then	O
the	O
controller	O
,	O
given	O
an	O
embedding	O
of	O
the	O
question	O
,	O
repeatedly	O
queries	O
the	O
memory	O
by	O
content	O
-	O
based	O
reads	O
to	O
determine	O
an	O
answer	O
.	O

subsection	O
:	O
Neural	B-Method
Turing	I-Method
Machine	E-Method
The	O
Neural	B-Method
Turing	I-Method
Machine	E-Method
is	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
equipped	O
with	O
a	O
content	O
-	O
addressable	O
memory	O
,	O
similar	O
to	O
Memory	B-Method
Networks	E-Method
,	O
but	O
with	O
the	O
additional	O
capability	O
to	O
write	O
to	O
memory	O
over	O
time	S-Metric
.	O

The	O
memory	O
is	O
accessed	O
by	O
a	O
controller	B-Method
network	E-Method
,	O
typically	O
an	O
LSTM	S-Method
,	O
and	O
the	O
full	B-Method
model	E-Method
is	O
differentiable	O
—	O
allowing	O
it	O
to	O
be	O
trained	O
via	O
BPTT	S-Method
.	O

A	O
write	O
to	O
memory	O
,	O
consists	O
of	O
a	O
copy	O
of	O
the	O
memory	O
from	O
the	O
previous	O
time	S-Metric
step	O
decayed	O
by	O
the	O
erase	O
matrix	O
indicating	O
obsolete	O
or	O
inaccurate	O
content	O
,	O
and	O
an	O
addition	O
of	O
new	O
or	O
updated	O
information	O
.	O

The	O
erase	O
matrix	O
is	O
constructed	O
as	O
the	O
outer	O
product	O
between	O
a	O
set	O
of	O
write	O
weights	O
and	O
erase	O
vector	O
.	O

The	O
add	O
matrix	O
is	O
the	O
outer	O
product	O
between	O
the	O
write	O
weights	O
and	O
a	O
new	O
write	O
word	O
,	O
which	O
the	O
controller	O
outputs	O
.	O

section	O
:	O
Architecture	O
This	O
paper	O
introduces	O
Sparse	B-Method
Access	I-Method
Memory	E-Method
(	O
SAM	S-Method
)	O
,	O
a	O
new	O
neural	B-Method
memory	I-Method
architecture	E-Method
with	O
two	O
innovations	O
.	O

Most	O
importantly	O
,	O
all	O
writes	O
to	O
and	O
reads	O
from	O
external	O
memory	O
are	O
constrained	O
to	O
a	O
sparse	O
subset	O
of	O
the	O
memory	O
words	O
,	O
providing	O
similar	O
functionality	O
as	O
the	O
NTM	S-Method
,	O
while	O
allowing	O
computational	B-Task
and	I-Task
memory	I-Task
efficient	I-Task
operation	E-Task
.	O

Secondly	O
,	O
we	O
introduce	O
a	O
sparse	B-Method
memory	I-Method
management	I-Method
scheme	E-Method
that	O
tracks	O
memory	O
usage	O
and	O
finds	O
unused	O
blocks	O
of	O
memory	O
for	O
recording	O
new	O
information	O
.	O

For	O
a	O
memory	O
containing	O
words	O
,	O
SAM	S-Method
executes	O
a	O
forward	O
,	O
backward	O
step	O
in	O
time	S-Metric
,	O
initializes	O
in	O
space	S-Metric
,	O
and	O
consumes	O
space	S-Metric
per	O
time	S-Metric
step	O
.	O

Under	O
some	O
reasonable	O
assumptions	O
,	O
SAM	S-Method
is	O
asymptotically	O
optimal	O
in	O
time	S-Metric
and	O
space	S-Metric
complexity	O
(	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Read	O
The	O
sparse	B-Task
read	I-Task
operation	E-Task
is	O
defined	O
to	O
be	O
a	O
weighted	B-Method
average	E-Method
over	O
a	O
selection	O
of	O
words	O
in	O
memory	O
:	O
where	O
contains	O
number	O
of	O
non	O
-	O
zero	O
entries	O
with	O
indices	O
;	O
is	O
a	O
small	O
constant	O
,	O
independent	O
of	O
,	O
typically	O
or	O
.	O

We	O
will	O
refer	O
to	O
sparse	O
analogues	O
of	O
weight	O
vectors	O
as	O
,	O
and	O
when	O
discussing	O
operations	O
that	O
are	O
used	O
in	O
both	O
the	O
sparse	O
and	O
dense	O
versions	O
of	O
our	O
model	O
use	O
.	O

We	O
wish	O
to	O
construct	O
such	O
that	O
.	O

For	O
content	B-Task
-	I-Task
based	I-Task
reads	E-Task
where	O
is	O
defined	O
by	O
(	O
[	O
reference	O
]	O
)	O
,	O
an	O
effective	O
approach	O
is	O
to	O
keep	O
the	O
largest	O
non	O
-	O
zero	O
entries	O
and	O
set	O
the	O
remaining	O
entries	O
to	O
zero	O
.	O

We	O
can	O
compute	O
naively	O
in	O
time	S-Metric
by	O
calculating	O
and	O
keeping	O
the	O
largest	O
values	O
.	O

However	O
,	O
linear	O
-	O
time	S-Metric
operation	O
can	O
be	O
avoided	O
.	O

Since	O
the	O
largest	O
values	O
in	O
correspond	O
to	O
the	O
closest	O
points	O
to	O
our	O
query	O
,	O
we	O
can	O
use	O
an	O
approximate	B-Method
nearest	I-Method
neighbor	I-Method
data	I-Method
-	I-Method
structure	E-Method
,	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
to	O
calculate	O
in	O
time	S-Metric
.	O

Sparse	B-Task
read	E-Task
can	O
be	O
considered	O
a	O
special	O
case	O
of	O
the	O
matrix	B-Method
-	I-Method
vector	I-Method
product	E-Method
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
with	O
two	O
key	O
distinctions	O
.	O

The	O
first	O
is	O
that	O
we	O
pass	O
gradients	O
for	O
only	O
a	O
constant	O
number	O
of	O
rows	O
of	O
memory	O
per	O
time	S-Metric
step	O
,	O
versus	O
,	O
which	O
results	O
in	O
a	O
negligible	O
fraction	O
of	O
non	O
-	O
zero	O
error	O
gradient	O
per	O
timestep	O
when	O
the	O
memory	O
is	O
large	O
.	O

The	O
second	O
distinction	O
is	O
in	O
implementation	O
:	O
by	O
using	O
an	O
efficient	O
sparse	B-Method
matrix	I-Method
format	E-Method
such	O
as	O
Compressed	B-Method
Sparse	I-Method
Rows	E-Method
(	O
CSR	S-Method
)	O
,	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
and	O
its	O
gradients	O
in	O
constant	O
time	S-Metric
and	O
space	S-Metric
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Write	O
The	O
write	B-Method
operation	E-Method
is	O
SAM	S-Method
is	O
an	O
instance	O
of	O
(	O
[	O
reference	O
]	O
)	O
where	O
the	O
write	O
weights	O
are	O
constrained	O
to	O
contain	O
a	O
constant	O
number	O
of	O
non	O
-	O
zero	O
entries	O
.	O

This	O
is	O
done	O
by	O
a	O
simple	O
scheme	O
where	O
the	O
controller	O
writes	O
either	O
to	O
previously	O
read	O
locations	O
,	O
in	O
order	O
to	O
update	O
contextually	O
relevant	O
memories	O
,	O
or	O
the	O
least	O
recently	O
accessed	O
location	O
,	O
in	O
order	O
to	O
overwrite	O
stale	O
or	O
unused	O
memory	O
slots	O
with	O
fresh	O
content	O
.	O

The	O
introduction	O
of	O
sparsity	O
could	O
be	O
achieved	O
via	O
other	O
write	B-Method
schemes	E-Method
.	O

For	O
example	O
,	O
we	O
could	O
use	O
a	O
sparse	B-Method
content	I-Method
-	I-Method
based	I-Method
write	I-Method
scheme	E-Method
,	O
where	O
the	O
controller	O
chooses	O
a	O
query	O
vector	O
and	O
applies	O
writes	O
to	O
similar	O
words	O
in	O
memory	O
.	O

This	O
would	O
allow	O
for	O
direct	B-Task
memory	I-Task
updates	E-Task
,	O
but	O
would	O
create	O
problems	O
when	O
the	O
memory	O
is	O
empty	O
(	O
and	O
shift	O
further	O
complexity	O
to	O
the	O
controller	O
)	O
.	O

We	O
decided	O
upon	O
the	O
previously	O
read	O
/	O
least	B-Method
recently	I-Method
accessed	I-Method
addressing	I-Method
scheme	E-Method
for	O
simplicity	O
and	O
flexibility	O
.	O

The	O
write	O
weights	O
are	O
defined	O
as	O
where	O
the	O
controller	O
outputs	O
the	O
interpolation	O
gate	O
parameter	O
and	O
the	O
write	O
gate	O
parameter	O
.	O

The	O
write	O
to	O
the	O
previously	O
read	O
locations	O
is	O
purely	O
additive	O
,	O
while	O
the	O
least	O
recently	O
accessed	O
word	O
is	O
set	O
to	O
zero	O
before	O
being	O
written	O
to	O
.	O

When	O
the	O
read	O
operation	O
is	O
sparse	O
(	O
has	O
non	O
-	O
zero	O
entries	O
)	O
,	O
it	O
follows	O
the	O
write	O
operation	O
is	O
also	O
sparse	O
.	O

We	O
define	O
to	O
be	O
an	O
indicator	O
over	O
words	O
in	O
memory	O
,	O
with	O
a	O
value	O
of	O
when	O
the	O
word	O
minimizes	O
a	O
usage	B-Metric
measure	E-Metric
If	O
there	O
are	O
several	O
words	O
that	O
minimize	O
then	O
we	O
choose	O
arbitrarily	O
between	O
them	O
.	O

We	O
tried	O
two	O
definitions	O
of	O
.	O

The	O
first	O
definition	O
is	O
a	O
time	S-Metric
-	O
discounted	O
sum	O
of	O
write	O
weights	O
where	O
is	O
the	O
discount	O
factor	O
.	O

This	O
usage	O
definition	O
is	O
incorporated	O
within	O
Dense	B-Method
Access	I-Method
Memory	E-Method
(	O
DAM	S-Method
)	O
,	O
a	O
dense	B-Method
-	I-Method
approximation	E-Method
to	O
SAM	S-Method
that	O
is	O
used	O
for	O
experimental	O
comparison	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
second	O
usage	O
definition	O
,	O
used	O
by	O
SAM	S-Method
,	O
is	O
simply	O
the	O
number	O
of	O
time	S-Metric
-	O
steps	O
since	O
a	O
non	O
-	O
negligible	O
memory	O
access	O
:	O
.	O

Here	O
,	O
is	O
a	O
tuning	O
parameter	O
that	O
we	O
typically	O
choose	O
to	O
be	O
.	O

We	O
maintain	O
this	O
usage	O
statistic	O
in	O
constant	O
time	S-Metric
using	O
a	O
custom	O
data	O
-	O
structure	O
(	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
we	O
also	O
use	O
the	O
least	O
recently	O
accessed	O
word	O
to	O
calculate	O
the	O
erase	O
matrix	O
.	O

is	O
defined	O
to	O
be	O
the	O
expansion	O
of	O
this	O
usage	O
indicator	O
where	O
is	O
a	O
vector	O
of	O
ones	O
.	O

The	O
total	O
cost	O
of	O
the	O
write	O
is	O
constant	O
in	O
time	S-Metric
and	O
space	S-Metric
for	O
both	O
the	O
forwards	O
and	O
backwards	O
pass	O
,	O
which	O
improves	O
on	O
the	O
linear	O
space	S-Metric
and	O
time	S-Metric
dense	O
write	O
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Controller	O
We	O
use	O
a	O
one	O
layer	O
LSTM	S-Method
for	O
the	O
controller	S-Method
throughout	O
.	O

At	O
each	O
time	S-Metric
step	O
,	O
the	O
LSTM	S-Method
receives	O
a	O
concatenation	O
of	O
the	O
external	O
input	O
,	O
,	O
the	O
word	O
,	O
read	O
in	O
the	O
previous	O
time	S-Metric
step	O
.	O

The	O
LSTM	S-Method
then	O
produces	O
a	O
vector	O
,	O
,	O
of	O
read	O
and	O
write	O
parameters	O
for	O
memory	O
access	O
via	O
a	O
linear	B-Method
layer	E-Method
.	O

The	O
word	O
read	O
from	O
memory	O
for	O
the	O
current	O
time	S-Metric
step	O
,	O
,	O
is	O
then	O
concatenated	O
with	O
the	O
output	O
of	O
the	O
LSTM	S-Method
,	O
and	O
this	O
vector	O
is	O
fed	O
through	O
a	O
linear	B-Method
layer	E-Method
to	O
form	O
the	O
final	O
output	O
,	O
.	O

The	O
full	O
control	O
flow	O
is	O
illustrated	O
in	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Efficient	O
backpropagation	O
through	O
time	S-Metric
We	O
have	O
already	O
demonstrated	O
how	O
the	O
forward	B-Method
operations	E-Method
in	O
SAM	S-Method
can	O
be	O
efficiently	O
computed	O
in	O
time	S-Metric
.	O

However	O
,	O
when	O
considering	O
space	B-Metric
complexity	E-Metric
of	O
MANNs	O
,	O
there	O
remains	O
a	O
dependence	O
on	O
for	O
the	O
computation	O
of	O
the	O
derivatives	O
at	O
the	O
corresponding	O
time	S-Metric
step	O
.	O

A	O
naive	O
implementation	O
requires	O
the	O
state	O
of	O
the	O
memory	O
to	O
be	O
cached	O
at	O
each	O
time	S-Metric
step	O
,	O
incurring	O
a	O
space	S-Metric
overhead	O
of	O
,	O
which	O
severely	O
limits	O
memory	O
size	O
and	O
sequence	O
length	O
.	O

Fortunately	O
,	O
this	O
can	O
be	O
remedied	O
.	O

Since	O
there	O
are	O
only	O
words	O
that	O
are	O
written	O
at	O
each	O
time	S-Metric
step	O
,	O
we	O
instead	O
track	O
the	O
sparse	O
modifications	O
made	O
to	O
the	O
memory	O
at	O
each	O
timestep	O
,	O
apply	O
them	O
in	O
-	O
place	O
to	O
compute	O
in	O
time	S-Metric
and	O
space	S-Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
can	O
restore	O
the	O
state	O
of	O
from	O
in	O
time	S-Metric
by	O
reverting	O
the	O
sparse	B-Method
modifications	E-Method
applied	O
at	O
time	S-Metric
step	O
.	O

As	O
such	O
the	O
memory	O
is	O
actually	O
rolled	O
back	O
to	O
previous	O
states	O
during	O
backpropagation	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

At	O
the	O
end	O
of	O
the	O
backward	O
pass	O
,	O
the	O
memory	O
ends	O
rolled	O
back	O
to	O
the	O
start	O
state	O
.	O

If	O
required	O
,	O
such	O
as	O
when	O
using	O
truncating	B-Method
BPTT	E-Method
,	O
the	O
final	O
memory	O
state	O
can	O
be	O
restored	O
by	O
making	O
a	O
copy	O
of	O
prior	O
to	O
calling	O
backwards	O
in	O
time	S-Metric
,	O
or	O
by	O
re	O
-	O
applying	O
the	O
sparse	B-Method
updates	E-Method
in	O
time	S-Metric
.	O

subsection	O
:	O
Approximate	B-Method
nearest	I-Method
neighbors	E-Method
When	O
querying	O
the	O
memory	O
,	O
we	O
can	O
use	O
an	O
approximate	B-Method
nearest	I-Method
neighbor	I-Method
index	E-Method
(	O
ANN	S-Method
)	O
to	O
search	O
over	O
the	O
external	O
memory	O
for	O
the	O
nearest	O
words	O
.	O

Where	O
a	O
linear	B-Method
KNN	I-Method
search	E-Method
inspects	O
every	O
element	O
in	O
memory	O
(	O
taking	O
time	S-Metric
)	O
,	O
an	O
ANN	B-Method
index	E-Method
maintains	O
a	O
structure	O
over	O
the	O
dataset	O
to	O
allow	O
for	O
fast	O
inspection	O
of	O
nearby	O
points	O
in	O
time	S-Metric
.	O

In	O
our	O
case	O
,	O
the	O
memory	O
is	O
still	O
a	O
dense	O
tensor	O
that	O
the	O
network	O
directly	O
operates	O
on	O
;	O
however	O
the	O
ANN	S-Method
is	O
a	O
structured	O
view	O
of	O
its	O
contents	O
.	O

Both	O
the	O
memory	O
and	O
the	O
ANN	O
index	O
are	O
passed	O
through	O
the	O
network	O
and	O
kept	O
in	O
sync	O
during	O
writes	O
.	O

However	O
there	O
are	O
no	O
gradients	O
with	O
respect	O
to	O
the	O
ANN	S-Method
as	O
its	O
function	O
is	O
fixed	O
.	O

We	O
considered	O
two	O
types	O
of	O
ANN	B-Method
indexes	E-Method
:	O
FLANN	B-Method
’s	I-Method
randomized	I-Method
k	I-Method
-	I-Method
d	I-Method
tree	I-Method
implementation	E-Method
that	O
arranges	O
the	O
datapoints	O
in	O
an	O
ensemble	O
of	O
structured	O
(	O
randomized	B-Method
k	I-Method
-	I-Method
d	I-Method
)	I-Method
trees	E-Method
to	O
search	O
for	O
nearby	O
points	O
via	O
comparison	B-Method
-	I-Method
based	I-Method
search	E-Method
,	O
and	O
one	O
that	O
uses	O
locality	B-Method
sensitive	I-Method
hash	E-Method
(	O
LSH	S-Method
)	O
functions	O
that	O
map	O
points	O
into	O
buckets	O
with	O
distance	O
-	O
preserving	O
guarantees	O
.	O

We	O
used	O
randomized	B-Method
k	I-Method
-	I-Method
d	I-Method
trees	E-Method
for	O
small	O
word	O
sizes	O
and	O
LSHs	S-Method
for	O
large	O
word	O
sizes	O
.	O

For	O
both	O
ANN	B-Method
implementations	E-Method
,	O
there	O
is	O
an	O
cost	O
for	O
insertion	S-Task
,	O
deletion	S-Task
and	O
query	S-Task
.	O

We	O
also	O
rebuild	O
the	O
ANN	S-Method
from	O
scratch	O
every	O
insertions	O
to	O
ensure	O
it	O
does	O
not	O
become	O
imbalanced	O
.	O

section	O
:	O
Results	O
subsection	O
:	O
Speed	B-Metric
and	I-Metric
memory	I-Metric
benchmarks	E-Metric
0.47	O
0.47	O
We	O
measured	O
the	O
forward	O
and	O
backward	O
times	O
of	O
the	O
SAM	S-Method
architecture	O
versus	O
the	O
dense	O
DAM	S-Method
variant	O
and	O
the	O
original	O
NTM	S-Method
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

SAM	S-Method
is	O
over	O
times	O
faster	O
than	O
the	O
NTM	S-Method
when	O
the	O
memory	O
contains	O
one	O
million	O
words	O
and	O
an	O
exact	O
linear	O
-	O
index	O
is	O
used	O
,	O
and	O
times	O
faster	O
with	O
the	O
k	B-Method
-	I-Method
d	I-Method
tree	E-Method
(	O
Figure	O
[	O
reference	O
]	O
sf	O
:	O
speed	O
)	O
.	O

With	O
an	O
ANN	S-Method
the	O
model	O
runs	O
in	O
sublinear	O
time	S-Metric
with	O
respect	O
to	O
the	O
memory	O
size	O
.	O

SAM	S-Method
’s	O
memory	O
usage	O
per	O
time	S-Metric
step	O
is	O
independent	O
of	O
the	O
number	O
of	O
memory	O
words	O
(	O
Figure	O
[	O
reference	O
]	O
sf	S-Method
:	O
memory	O
)	O
,	O
which	O
empirically	O
verifies	O
the	O
space	S-Metric
claim	O
from	O
Supplementary	O
[	O
reference	O
]	O
.	O

For	O
memory	O
words	O
SAM	S-Method
uses	O
of	O
physical	O
memory	O
to	O
initialize	O
the	O
network	O
and	O
to	O
run	O
a	O
100	O
step	O
forward	O
and	O
backward	O
pass	O
,	O
compared	O
with	O
the	O
NTM	S-Method
which	O
consumes	O
.	O

subsection	O
:	O
Learning	S-Task
with	O
sparse	B-Task
memory	I-Task
access	E-Task
We	O
have	O
established	O
that	O
SAM	S-Method
reaps	O
a	O
huge	O
computational	B-Metric
and	I-Metric
memory	E-Metric
advantage	O
of	O
previous	O
models	O
,	O
but	O
can	O
we	O
really	O
learn	O
with	O
SAM	S-Method
’s	O
sparse	O
approximations	O
?	O
We	O
investigated	O
the	O
learning	B-Metric
cost	E-Metric
of	O
inducing	B-Task
sparsity	E-Task
,	O
and	O
the	O
effect	O
of	O
placing	O
an	O
approximate	O
nearest	O
neighbor	O
index	O
within	O
the	O
network	O
,	O
by	O
comparing	O
SAM	S-Method
with	O
its	O
dense	O
variant	O
DAM	S-Method
and	O
some	O
established	O
models	O
,	O
the	O
NTM	S-Method
and	O
the	O
LSTM	S-Method
.	O

We	O
trained	O
each	O
model	O
on	O
three	O
of	O
the	O
original	O
NTM	B-Task
tasks	E-Task
.	O

1	O
.	O

Copy	S-Method
:	O
copy	O
a	O
random	O
input	O
sequence	O
of	O
length	O
1–20	O
,	O
2	O
.	O

Associative	B-Task
Recall	E-Task
:	O
given	O
3	O
-	O
6	O
random	O
(	O
key	O
,	O
value	O
)	O
pairs	O
,	O
and	O
subsequently	O
a	O
cue	O
key	O
,	O
return	O
the	O
associated	O
value	O
.	O

3	O
.	O

Priority	B-Method
Sort	E-Method
:	O
Given	O
20	O
random	O
keys	O
and	O
priority	O
values	O
,	O
return	O
the	O
top	O
16	O
keys	O
in	O
descending	O
order	O
of	O
priority	O
.	O

We	O
chose	O
these	O
tasks	O
because	O
the	O
NTM	S-Method
is	O
known	O
to	O
perform	O
well	O
on	O
them	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
that	O
sparse	B-Method
models	E-Method
are	O
able	O
to	O
learn	O
with	O
comparable	O
efficiency	O
to	O
the	O
dense	B-Method
models	E-Method
and	O
,	O
surprisingly	O
,	O
learn	O
more	O
effectively	O
for	O
some	O
tasks	O
—	O
notably	O
priority	B-Task
sort	E-Task
and	O
associative	B-Task
recall	E-Task
.	O

This	O
shows	O
that	O
sparse	O
reads	O
and	O
writes	O
can	O
actually	O
benefit	O
early	O
-	O
stage	O
learning	S-Task
in	O
some	O
cases	O
.	O

Full	O
hyperparameter	O
details	O
are	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

0.32	O
0.32	O
0.32	O
subsection	O
:	O
Scaling	O
with	O
a	O
curriculum	O
The	O
computational	B-Metric
efficiency	E-Metric
of	O
SAM	S-Method
opens	O
up	O
the	O
possibility	O
of	O
training	O
on	O
tasks	O
that	O
require	O
storing	O
a	O
large	O
amount	O
of	O
information	O
over	O
long	O
sequences	O
.	O

Here	O
we	O
show	O
this	O
is	O
possible	O
in	O
practice	O
,	O
by	O
scaling	B-Task
tasks	E-Task
to	O
a	O
large	O
scale	O
via	O
an	O
exponentially	B-Method
increasing	I-Method
curriculum	E-Method
.	O

We	O
parametrized	O
three	O
of	O
the	O
tasks	O
described	O
in	O
Section	O
[	O
reference	O
]	O
:	O
associative	B-Task
recall	E-Task
,	O
copy	O
,	O
and	O
priority	B-Task
sort	E-Task
,	O
with	O
a	O
progressively	O
increasing	O
difficulty	O
level	O
which	O
characterises	O
the	O
length	O
of	O
the	O
sequence	O
and	O
number	O
of	O
entries	O
to	O
store	O
in	O
memory	O
.	O

For	O
example	O
,	O
level	O
specifies	O
the	O
input	O
sequence	O
length	O
for	O
the	O
copy	B-Task
task	E-Task
.	O

We	O
exponentially	O
increased	O
the	O
maximum	O
level	O
when	O
the	O
network	O
begins	O
to	O
learn	O
the	O
fundamental	B-Method
algorithm	E-Method
.	O

Since	O
the	O
time	S-Metric
taken	O
for	O
a	O
forward	O
and	O
backward	O
pass	O
scales	O
with	O
the	O
sequence	O
length	O
,	O
following	O
a	O
standard	O
linearly	B-Method
increasing	I-Method
curriculum	E-Method
could	O
potentially	O
take	O
,	O
if	O
the	O
same	O
amount	O
of	O
training	O
was	O
required	O
at	O
each	O
step	O
of	O
the	O
curriculum	O
.	O

Specifically	O
,	O
was	O
doubled	O
whenever	O
the	O
average	B-Metric
training	I-Metric
loss	E-Metric
dropped	O
below	O
a	O
threshold	O
for	O
a	O
number	O
of	O
episodes	O
.	O

The	O
level	O
was	O
sampled	O
for	O
each	O
minibatch	O
from	O
the	O
uniform	O
distribution	O
over	O
integers	O
.	O

We	O
compared	O
the	O
dense	B-Method
models	E-Method
,	O
NTM	O
and	O
DAM	S-Method
,	O
with	O
both	O
SAM	S-Method
with	O
an	O
exact	B-Method
nearest	I-Method
neighbor	I-Method
index	E-Method
(	O
SAM	S-Method
linear	O
)	O
and	O
with	O
locality	B-Method
sensitive	I-Method
hashing	E-Method
(	O
SAM	S-Method
ANN	O
)	O
.	O

The	O
dense	B-Method
models	E-Method
contained	O
64	O
memory	O
words	O
,	O
while	O
the	O
sparse	B-Method
models	E-Method
had	O
words	O
.	O

These	O
sizes	O
were	O
chosen	O
to	O
ensure	O
all	O
models	O
use	O
approximately	O
the	O
same	O
amount	O
of	O
physical	O
memory	O
when	O
trained	O
over	O
100	O
steps	O
.	O

For	O
all	O
tasks	O
,	O
SAM	S-Method
was	O
able	O
to	O
advance	O
further	O
than	O
the	O
other	O
models	O
,	O
and	O
in	O
the	O
associative	B-Task
recall	I-Task
task	E-Task
,	O
SAM	S-Method
was	O
able	O
to	O
advance	O
through	O
the	O
curriculum	O
to	O
sequences	O
greater	O
than	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Note	O
that	O
we	O
did	O
not	O
use	O
truncated	B-Method
backpropagation	E-Method
,	O
so	O
this	O
involved	O
BPTT	S-Method
for	O
over	O
steps	O
with	O
a	O
memory	O
size	O
in	O
the	O
millions	O
of	O
words	O
.	O

To	O
investigate	O
whether	O
SAM	S-Method
was	O
able	O
to	O
learn	O
algorithmic	B-Task
solutions	E-Task
to	O
tasks	O
,	O
we	O
investigated	O
its	O
ability	O
to	O
generalize	O
to	O
sequences	O
that	O
far	O
exceeded	O
those	O
observed	O
during	O
training	O
.	O

Namely	O
we	O
trained	O
SAM	S-Method
on	O
the	O
associative	B-Task
recall	I-Task
task	E-Task
up	O
to	O
sequences	O
of	O
length	O
,	O
and	O
found	O
it	O
was	O
then	O
able	O
to	O
generalize	O
to	O
sequences	O
of	O
length	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Question	B-Task
answering	E-Task
on	O
the	O
Babi	S-Material
tasks	O
introduced	O
toy	B-Task
tasks	E-Task
they	O
considered	O
a	O
prerequisite	O
to	O
agents	O
which	O
can	O
reason	O
and	O
understand	O
natural	O
language	O
.	O

They	O
are	O
synthetically	B-Task
generated	I-Task
language	I-Task
tasks	E-Task
with	O
a	O
vocab	O
of	O
about	O
150	O
words	O
that	O
test	O
various	O
aspects	O
of	O
simple	O
reasoning	S-Task
such	O
as	O
deduction	S-Task
,	O
induction	S-Task
and	O
coreferencing	S-Task
.	O

We	O
tested	O
the	O
models	O
(	O
including	O
the	O
Sparse	B-Method
Differentiable	I-Method
Neural	I-Method
Computer	E-Method
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
on	O
this	O
task	O
.	O

The	O
full	O
results	O
and	O
training	O
details	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

The	O
MANNs	S-Method
,	O
except	O
the	O
NTM	S-Method
,	O
are	O
able	O
to	O
learn	O
solutions	O
comparable	O
to	O
the	O
previous	O
best	O
results	O
,	O
failing	O
at	O
only	O
2	O
of	O
the	O
tasks	O
.	O

The	O
SDNC	S-Method
manages	O
to	O
solve	O
all	O
but	O
1	O
of	O
the	O
tasks	O
,	O
the	O
best	O
reported	O
result	O
on	O
Babi	S-Material
that	O
we	O
are	O
aware	O
of	O
.	O

Notably	O
the	O
best	O
prior	O
results	O
have	O
been	O
obtained	O
by	O
using	O
supervising	O
the	O
memory	B-Task
retrieval	E-Task
(	O
during	O
training	O
the	O
model	O
is	O
provided	O
annotations	O
which	O
indicate	O
which	O
memories	O
should	O
be	O
used	O
to	O
answer	O
a	O
query	O
)	O
.	O

More	O
directly	O
comparable	O
previous	O
work	O
with	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
memory	I-Method
networks	E-Method
,	O
which	O
did	O
not	O
use	O
supervision	O
,	O
fails	O
at	O
6	O
of	O
the	O
tasks	O
.	O

Both	O
the	O
sparse	S-Method
and	O
dense	S-Method
perform	O
comparably	O
at	O
this	O
task	O
,	O
again	O
indicating	O
the	O
sparse	B-Method
approximations	E-Method
do	O
not	O
impair	O
learning	S-Task
.	O

We	O
believe	O
the	O
NTM	S-Method
may	O
perform	O
poorly	O
since	O
it	O
lacks	O
a	O
mechanism	O
which	O
allows	O
it	O
to	O
allocate	O
memory	O
effectively	O
.	O

subsection	O
:	O
Learning	O
on	O
real	O
world	O
data	O
Finally	O
,	O
we	O
demonstrate	O
that	O
the	O
model	O
is	O
capable	O
of	O
learning	O
in	O
a	O
non	O
-	O
synthetic	O
dataset	O
.	O

Omniglot	S-Method
is	O
a	O
dataset	O
of	O
1623	O
characters	O
taken	O
from	O
50	O
different	O
alphabets	O
,	O
with	O
20	O
examples	O
of	O
each	O
character	O
.	O

This	O
dataset	O
is	O
used	O
to	O
test	O
rapid	B-Task
,	I-Task
or	I-Task
one	I-Task
-	I-Task
shot	I-Task
learning	E-Task
,	O
since	O
there	O
are	O
few	O
examples	O
of	O
each	O
character	O
but	O
many	O
different	O
character	O
classes	O
.	O

Following	O
,	O
we	O
generate	O
episodes	O
where	O
a	O
subset	O
of	O
characters	O
are	O
randomly	O
selected	O
from	O
the	O
dataset	O
,	O
rotated	O
and	O
stretched	O
,	O
and	O
assigned	O
a	O
randomly	O
chosen	O
label	O
.	O

At	O
each	O
time	S-Metric
step	O
an	O
example	O
of	O
one	O
of	O
the	O
characters	O
is	O
presented	O
,	O
along	O
with	O
the	O
correct	O
label	O
of	O
the	O
proceeding	O
character	O
.	O

Each	O
character	O
is	O
presented	O
10	O
times	O
in	O
an	O
episode	O
(	O
but	O
each	O
presentation	O
may	O
be	O
any	O
one	O
of	O
the	O
20	O
examples	O
of	O
the	O
character	O
)	O
.	O

In	O
order	O
to	O
succeed	O
at	O
the	O
task	O
the	O
model	O
must	O
learn	O
to	O
rapidly	O
associate	O
a	O
novel	O
character	O
with	O
the	O
correct	O
label	O
,	O
such	O
that	O
it	O
can	O
correctly	O
classify	O
subsequent	O
examples	O
of	O
the	O
same	O
character	O
class	O
.	O

Again	O
,	O
we	O
used	O
an	O
exponential	B-Method
curriculum	E-Method
,	O
doubling	O
the	O
number	O
of	O
additional	O
characters	O
provided	O
to	O
the	O
model	O
whenever	O
the	O
cost	O
was	O
reduced	O
under	O
a	O
threshold	O
.	O

After	O
training	O
all	O
MANNs	S-Method
for	O
the	O
same	O
length	O
of	O
time	S-Metric
,	O
a	O
validation	B-Task
task	E-Task
with	O
characters	O
was	O
used	O
to	O
select	O
the	O
best	O
run	O
,	O
and	O
this	O
was	O
then	O
tested	O
on	O
a	O
test	O
set	O
,	O
containing	O
all	O
novel	O
characters	O
for	O
different	O
sequence	O
lengths	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

All	O
of	O
the	O
MANNs	S-Method
were	O
able	O
to	O
perform	O
much	O
better	O
than	O
chance	O
,	O
even	O
on	O
sequences	O
longer	O
than	O
seen	O
during	O
training	O
.	O

SAM	S-Method
outperformed	O
other	O
models	O
,	O
presumably	O
due	O
to	O
its	O
much	O
larger	O
memory	O
capacity	O
.	O

Previous	O
results	O
on	O
the	O
Omniglot	B-Task
curriculum	I-Task
task	E-Task
are	O
not	O
identical	O
,	O
since	O
we	O
used	O
1	O
-	O
hot	O
labels	O
throughout	O
and	O
the	O
training	O
curriculum	O
scaled	O
to	O
longer	O
sequences	O
,	O
but	O
our	O
results	O
with	O
the	O
dense	B-Method
models	E-Method
are	O
comparable	O
(	O
errors	O
with	O
characters	O
)	O
,	O
while	O
the	O
SAM	S-Method
is	O
significantly	O
better	O
(	O
errors	O
with	O
characters	O
)	O
.	O

section	O
:	O
Discussion	O
Scaling	B-Task
memory	I-Task
systems	E-Task
is	O
a	O
pressing	O
research	O
direction	O
due	O
to	O
potential	O
for	O
compelling	O
applications	O
with	O
large	O
amounts	O
of	O
memory	O
.	O

We	O
have	O
demonstrated	O
that	O
you	O
can	O
train	O
neural	B-Method
networks	E-Method
with	O
large	O
memories	O
via	O
a	O
sparse	B-Method
read	I-Method
and	I-Method
write	I-Method
scheme	E-Method
that	O
makes	O
use	O
of	O
efficient	O
data	O
structures	O
within	O
the	O
network	O
,	O
and	O
obtain	O
significant	O
speedups	O
during	O
training	S-Task
.	O

Although	O
we	O
have	O
focused	O
on	O
a	O
specific	O
MANN	S-Method
(	O
SAM	B-Method
)	E-Method
,	O
which	O
is	O
closely	O
related	O
to	O
the	O
NTM	S-Method
,	O
the	O
approach	O
taken	O
here	O
is	O
general	O
and	O
can	O
be	O
applied	O
to	O
many	O
differentiable	O
memory	B-Method
architectures	E-Method
,	O
such	O
as	O
Memory	B-Method
Networks	E-Method
.	O

It	O
should	O
be	O
noted	O
that	O
there	O
are	O
multiple	O
possible	O
routes	O
toward	O
scalable	B-Task
memory	I-Task
architectures	E-Task
.	O

For	O
example	O
,	O
prior	O
work	O
aimed	O
at	O
scaling	O
Neural	B-Method
Turing	I-Method
Machines	E-Method
used	O
reinforcement	B-Method
learning	E-Method
to	O
train	O
a	O
discrete	B-Method
addressing	I-Method
policy	E-Method
.	O

This	O
approach	O
also	O
touches	O
only	O
a	O
sparse	O
set	O
of	O
memories	O
at	O
each	O
time	S-Metric
step	O
,	O
but	O
relies	O
on	O
higher	O
variance	O
estimates	O
of	O
the	O
gradient	O
during	O
optimization	S-Task
.	O

Though	O
we	O
can	O
only	O
guess	O
at	O
what	O
class	O
of	O
memory	B-Method
models	E-Method
will	O
become	O
staple	O
in	O
machine	B-Task
learning	I-Task
systems	E-Task
of	O
the	O
future	O
,	O
we	O
argue	O
in	O
Supplementary	O
[	O
reference	O
]	O
that	O
they	O
will	O
be	O
no	O
more	O
efficient	O
than	O
SAM	S-Method
in	O
space	S-Metric
and	O
time	B-Metric
complexity	E-Metric
if	O
they	O
address	O
memories	O
based	O
on	O
content	O
.	O

We	O
have	O
experimented	O
with	O
randomized	B-Method
k	I-Method
-	I-Method
d	I-Method
trees	E-Method
and	O
LSH	S-Method
within	O
the	O
network	O
to	O
reduce	O
the	O
forward	O
pass	O
of	O
training	S-Task
to	O
sublinear	O
time	S-Metric
,	O
but	O
there	O
may	O
be	O
room	O
for	O
improvement	O
here	O
.	O

K	B-Method
-	I-Method
d	I-Method
trees	E-Method
were	O
not	O
designed	O
specifically	O
for	O
fully	O
online	B-Task
scenarios	E-Task
,	O
and	O
can	O
become	O
imbalanced	O
during	O
training	O
.	O

Recent	O
work	O
in	O
tree	B-Method
ensemble	I-Method
models	E-Method
,	O
such	O
as	O
Mondrian	B-Method
forests	E-Method
,	O
show	O
promising	O
results	O
in	O
maintaining	O
balanced	B-Task
hierarchical	I-Task
set	I-Task
coverage	E-Task
in	O
the	O
online	B-Task
setting	E-Task
.	O

An	O
alternative	O
approach	O
which	O
may	O
be	O
well	O
-	O
suited	O
is	O
LSH	S-Method
forests	O
,	O
which	O
adaptively	O
modifies	O
the	O
number	O
of	O
hashes	O
used	O
.	O

It	O
would	O
be	O
an	O
interesting	O
empirical	O
investigation	O
to	O
more	O
fully	O
assess	O
different	O
ANN	B-Method
approaches	E-Method
in	O
the	O
challenging	O
context	O
of	O
training	O
a	O
neural	B-Method
network	E-Method
.	O

Humans	O
are	O
able	O
to	O
retain	O
a	O
large	O
,	O
task	O
-	O
dependent	O
set	O
of	O
memories	O
obtained	O
in	O
one	O
pass	O
with	O
a	O
surprising	O
amount	O
of	O
fidelity	S-Metric
.	O

Here	O
we	O
have	O
demonstrated	O
architectures	O
that	O
may	O
one	O
day	O
compete	O
with	O
humans	O
at	O
these	O
kinds	O
of	O
tasks	O
.	O

subsection	O
:	O
Acknowledgements	O
We	O
thank	O
Vyacheslav	O
Egorov	O
,	O
Edward	O
Grefenstette	O
,	O
Malcolm	O
Reynolds	O
,	O
Fumin	O
Wang	O
and	O
Yori	O
Zwols	O
for	O
their	O
assistance	O
,	O
and	O
the	O
Google	O
DeepMind	O
family	O
for	O
helpful	O
discussions	O
and	O
encouragement	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplementary	O
Information	O
appendix	O
:	O
Time	S-Metric
and	O
space	B-Metric
complexity	E-Metric
Under	O
a	O
reasonable	O
class	O
of	O
content	B-Method
addressable	I-Method
memory	I-Method
architectures	E-Method
,	O
SAM	S-Method
is	O
optimal	O
in	O
time	S-Metric
and	O
space	B-Metric
complexity	E-Metric
.	O

theorem	O
:	O
.	O

Let	O
M	O
be	O
a	O
collection	O
of	O
real	O
vectors	O
m1	O
,	O
m2	O
,	O
…	O
,	O
mN	O
of	O
fixed	O
dimension	O
d.	O
Let	O
A	O
be	O
the	O
set	O
of	O
all	O
content	B-Method
addressable	I-Method
memory	I-Method
data	I-Method
structures	E-Method
that	O
store	O
M	O
and	O
can	O
return	O
at	O
least	O
one	O
word	O
mj	O
such	O
that	O
≤⁢D	O
(	O
q	O
,	O
mj	O
)	O
⁢c	O
(+	O
1ϵ	O
)	O
for	O
a	O
given	O
Lp	O
norm	O
D	O
,	O
query	O
vector	O
q	O
,	O
and	O
>	O
ϵ0	O
;	O
provided	O
such	O
a	O
memory	O
mc	O
exists	O
with	O
=	O
⁢D	O
(	O
q	O
,	O
mc	O
)	O
c	O
.	O

Existing	O
lower	O
bounds	O
assert	O
that	O
for	O
any	O
data	O
structure	O
,	O
requires	O
time	S-Metric
and	O
space	S-Metric
to	O
perform	O
a	O
read	O
operation	O
.	O

The	O
SAM	S-Method
memory	O
architecture	O
proposed	O
in	O
this	O
paper	O
is	O
contained	O
within	O
as	O
it	O
computes	O
the	O
approximate	B-Task
nearest	I-Task
neighbors	I-Task
problem	E-Task
in	O
fixed	O
dimensions	O
.	O

As	O
we	O
will	O
show	O
,	O
SAM	S-Method
requires	O
time	S-Metric
to	O
query	O
and	O
maintain	O
the	O
ANN	S-Method
,	O
to	O
perform	O
all	O
subsequent	O
sparse	B-Task
read	I-Task
,	I-Task
write	I-Task
,	I-Task
and	I-Task
error	I-Task
gradient	I-Task
calculations	E-Task
.	O

It	O
requires	O
space	S-Metric
to	O
initialize	O
the	O
memory	O
and	O
to	O
store	O
intermediate	O
sparse	O
tensors	O
.	O

We	O
thus	O
conclude	O
it	O
is	O
optimal	O
in	O
asymptotic	B-Metric
time	E-Metric
and	O
space	B-Metric
complexity	E-Metric
.	O

subsection	O
:	O
Initialization	O
Upon	O
initialization	S-Task
,	O
SAM	S-Method
consumes	O
space	S-Metric
and	O
time	S-Metric
to	O
instantiate	O
the	O
memory	O
and	O
the	O
memory	O
Jacobian	O
.	O

Furthermore	O
,	O
it	O
requires	O
time	S-Metric
and	O
space	S-Metric
to	O
initialize	O
auxiliary	O
data	O
structures	O
which	O
index	O
the	O
memory	O
,	O
such	O
as	O
the	O
approximate	B-Method
nearest	I-Method
neighbor	E-Method
which	O
provides	O
a	O
content	O
-	O
structured	O
view	O
of	O
the	O
memory	O
,	O
and	O
the	O
least	O
accessed	O
ring	O
,	O
which	O
maintains	O
the	O
temporal	O
ordering	O
in	O
which	O
memory	O
words	O
are	O
accessed	O
.	O

These	O
initializations	O
represent	O
an	O
unavoidable	O
one	B-Metric
-	I-Metric
off	I-Metric
cost	E-Metric
that	O
does	O
not	O
recur	O
per	O
step	O
of	O
training	O
,	O
and	O
ultimately	O
has	O
little	O
effect	O
on	O
training	B-Metric
speed	E-Metric
.	O

For	O
the	O
remainder	O
of	O
the	O
analysis	O
we	O
will	O
concentrate	O
on	O
the	O
space	S-Metric
and	O
time	B-Metric
cost	E-Metric
per	O
training	O
step	O
.	O

subsection	O
:	O
Read	O
Recall	O
the	O
sparse	O
read	O
operation	O
,	O
As	O
is	O
chosen	O
to	O
be	O
a	O
fixed	O
constant	O
,	O
it	O
is	O
clear	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
in	O
time	S-Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
see	O
the	O
gradients	O
are	O
sparse	O
with	O
only	O
non	O
-	O
zero	O
terms	O
,	O
and	O
where	O
is	O
a	O
vector	O
of	O
zeros	O
.	O

Thus	O
they	O
can	O
both	O
be	O
computed	O
in	O
constant	O
time	S-Metric
by	O
skipping	O
the	O
computation	O
of	O
zeros	O
.	O

Furthermore	O
by	O
using	O
an	O
efficient	O
sparse	B-Method
matrix	I-Method
format	E-Method
to	O
store	O
these	O
matrices	O
and	O
vectors	O
,	O
such	O
as	O
the	O
CSR	S-Method
,	O
we	O
can	O
represent	O
them	O
using	O
at	O
most	O
values	O
.	O

Since	O
the	O
read	O
word	O
and	O
its	O
respective	O
error	O
gradient	O
is	O
the	O
size	O
of	O
a	O
single	O
word	O
in	O
memory	O
(	O
elements	O
)	O
,	O
the	O
overall	O
space	B-Metric
complexity	E-Metric
is	O
per	O
time	S-Metric
step	O
for	O
the	O
read	O
.	O

subsection	O
:	O
Write	O
Recall	O
the	O
write	O
operation	O
,	O
where	O
is	O
the	O
add	O
matrix	O
,	O
is	O
the	O
erase	O
matrix	O
,	O
and	O
is	O
defined	O
to	O
be	O
the	O
erase	O
weight	O
matrix	O
.	O

We	O
chose	O
the	O
write	O
weights	O
to	O
be	O
an	O
interpolation	O
between	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
the	O
previously	O
read	O
locations	O
,	O
For	O
sparse	B-Task
reads	E-Task
where	O
is	O
a	O
sparse	O
vector	O
with	O
non	O
-	O
zeros	O
,	O
the	O
write	O
weights	O
is	O
also	O
sparse	O
with	O
non	O
-	O
zeros	O
:	O
for	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
for	O
the	O
previously	O
read	O
locations	O
.	O

Thus	O
the	O
sparse	B-Method
-	I-Method
dense	I-Method
outer	I-Method
product	E-Method
can	O
be	O
performed	O
in	O
time	S-Metric
as	O
is	O
a	O
fixed	O
constant	O
.	O

Since	O
can	O
be	O
represented	O
as	O
a	O
sparse	O
matrix	O
with	O
one	O
single	O
non	O
-	O
zero	O
,	O
the	O
erase	O
matrix	O
can	O
also	O
.	O

As	O
and	O
are	O
sparse	O
matrices	O
we	O
can	O
then	O
add	O
them	O
component	O
-	O
wise	O
to	O
the	O
dense	O
in	O
time	S-Metric
.	O

By	O
analogous	O
arguments	O
the	O
backward	B-Method
pass	E-Method
can	O
be	O
computed	O
in	O
time	S-Metric
and	O
each	O
sparse	O
matrix	O
can	O
be	O
represented	O
in	O
space	S-Metric
.	O

We	O
avoid	O
caching	O
the	O
modified	O
memory	O
,	O
and	O
thus	O
duplicating	O
it	O
,	O
by	O
applying	O
the	O
write	O
directly	O
to	O
the	O
memory	O
.	O

To	O
restore	O
its	O
prior	O
state	O
during	O
the	O
backward	O
pass	O
,	O
which	O
is	O
crucial	O
to	O
gradient	B-Task
calculations	E-Task
at	O
earlier	O
time	S-Metric
steps	O
,	O
we	O
roll	O
the	O
memory	O
it	O
back	O
by	O
reverting	O
the	O
sparse	B-Method
modifications	E-Method
with	O
an	O
additional	O
time	S-Metric
overhead	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
location	O
of	O
the	O
least	O
recently	O
accessed	O
memory	O
can	O
be	O
maintained	O
in	O
time	S-Metric
by	O
constructing	O
a	O
circular	B-Method
linked	I-Method
list	E-Method
that	O
tracks	O
the	O
indices	O
of	O
words	O
in	O
memory	O
,	O
and	O
preserves	O
a	O
strict	O
ordering	O
of	O
relative	O
temporal	O
access	O
.	O

The	O
first	O
element	O
in	O
the	O
ring	O
is	O
the	O
least	O
recently	O
accessed	O
word	O
in	O
memory	O
,	O
and	O
the	O
last	O
element	O
in	O
the	O
ring	O
is	O
the	O
most	O
recently	O
modified	O
.	O

We	O
keep	O
a	O
‘	O
‘	O
head	O
’	O
’	O
pointer	O
to	O
the	O
first	O
element	O
in	O
the	O
ring	O
.	O

When	O
a	O
memory	O
word	O
is	O
randomly	O
accessed	O
,	O
we	O
can	O
push	O
its	O
respective	O
index	O
to	O
the	O
back	O
of	O
the	O
ring	O
in	O
time	S-Metric
by	O
redirecting	O
a	O
small	O
number	O
of	O
pointers	O
.	O

When	O
we	O
wish	O
to	O
pop	O
the	O
least	O
recently	O
accessed	O
memory	O
(	O
and	O
write	O
to	O
it	O
)	O
we	O
move	O
the	O
head	O
to	O
the	O
next	O
element	O
in	O
the	O
ring	O
in	O
time	S-Metric
.	O

subsection	O
:	O
Content	B-Method
-	I-Method
based	I-Method
addressing	E-Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
we	O
can	O
calculate	O
the	O
content	O
-	O
based	O
attention	O
,	O
or	O
read	O
weights	O
,	O
in	O
time	S-Metric
using	O
an	O
approximate	B-Method
nearest	I-Method
neighbor	I-Method
index	E-Method
that	O
views	O
the	O
memory	O
.	O

We	O
keep	O
the	O
ANN	O
index	O
synchronized	O
with	O
the	O
memory	O
by	O
passing	O
it	O
through	O
the	O
network	O
as	O
a	O
non	O
-	O
differentiable	O
member	O
of	O
the	O
network	O
’s	O
state	O
(	O
so	O
we	O
do	O
not	O
pass	O
gradients	O
for	O
it	O
)	O
,	O
and	O
we	O
update	O
the	O
index	O
upon	O
each	O
write	O
or	O
erase	O
to	O
memory	O
in	O
time	S-Metric
.	O

Maintaining	B-Task
and	I-Task
querying	E-Task
the	O
ANN	B-Task
index	E-Task
represents	O
the	O
most	O
expensive	O
part	O
of	O
the	O
network	O
,	O
which	O
is	O
reasonable	O
as	O
content	B-Task
-	I-Task
based	I-Task
addressing	E-Task
is	O
inherently	O
expensive	O
.	O

For	O
the	O
backward	B-Task
pass	I-Task
computation	E-Task
,	O
specifically	O
calculating	O
and	O
with	O
respect	O
to	O
,	O
we	O
can	O
once	O
again	O
compute	O
these	O
using	O
sparse	B-Method
matrix	I-Method
operations	E-Method
in	O
time	S-Metric
.	O

This	O
is	O
because	O
the	O
non	O
-	O
zero	O
locations	O
have	O
been	O
determined	O
during	O
the	O
forward	O
pass	O
.	O

Thus	O
to	O
conclude	O
,	O
SAM	S-Method
consumes	O
in	O
total	O
space	S-Metric
for	O
both	O
the	O
forward	O
and	O
backward	O
step	O
during	O
training	O
,	O
time	S-Metric
per	O
forward	O
step	O
,	O
and	O
per	O
backward	O
step	O
.	O

appendix	O
:	O
Control	B-Method
flow	E-Method
appendix	O
:	O
Training	O
details	O
Here	O
we	O
provide	O
additional	O
details	O
on	O
the	O
training	O
regime	O
used	O
for	O
our	O
experiments	O
used	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

To	O
avoid	O
bias	O
in	O
our	O
results	O
,	O
we	O
chose	O
the	O
learning	B-Metric
rate	E-Metric
that	O
worked	O
best	O
for	O
DAM	S-Method
(	O
and	O
not	O
SAM	S-Method
)	O
.	O

We	O
tried	O
learning	B-Metric
rates	E-Metric
and	O
found	O
that	O
DAM	S-Method
trained	O
best	O
with	O
.	O

We	O
also	O
tried	O
values	O
of	O
and	O
found	O
no	O
significant	O
difference	O
in	O
performance	O
across	O
the	O
values	O
.	O

We	O
used	O
100	O
hidden	O
units	O
for	O
the	O
LSTM	S-Method
(	O
including	O
the	O
controller	B-Method
LSTMs	E-Method
)	O
,	O
a	O
minibatch	S-Method
of	O
,	O
asynchronous	B-Method
workers	E-Method
to	O
speed	O
up	O
training	S-Task
,	O
and	O
RMSProp	S-Method
to	O
optimize	O
the	O
controller	O
.	O

We	O
used	O
memory	O
access	O
heads	O
and	O
configured	O
SAM	S-Method
to	O
read	O
from	O
only	O
locations	O
per	O
head	O
.	O

appendix	O
:	O
Sparse	B-Method
Differentiable	I-Method
Neural	I-Method
Computer	E-Method
Recently	O
proposed	O
a	O
novel	O
MANN	S-Method
the	O
Differentiable	B-Method
Neural	I-Method
Computer	E-Method
(	O
DNC	S-Method
)	O
.	O

The	O
two	O
innovations	O
proposed	O
by	O
this	O
model	O
are	O
a	O
new	O
approach	O
to	O
tracking	B-Task
memory	I-Task
freeness	E-Task
(	O
dynamic	B-Task
memory	I-Task
allocation	E-Task
)	O
and	O
a	O
mechanism	O
for	O
associating	O
memories	O
together	O
(	O
temporal	B-Method
memory	I-Method
linkage	E-Method
)	O
.	O

We	O
demonstrate	O
here	O
that	O
the	O
approaches	O
enumerated	O
in	O
the	O
paper	O
can	O
be	O
adapted	O
to	O
new	O
models	O
by	O
outlining	O
a	O
sparse	B-Method
version	E-Method
of	O
this	O
model	O
,	O
the	O
Sparse	B-Method
Differentiable	I-Method
Neural	I-Method
Computer	E-Method
(	O
SDNC	S-Method
)	O
,	O
which	O
learns	O
with	O
similar	O
data	B-Metric
efficiency	E-Metric
while	O
retaining	O
the	O
computational	O
advantages	O
of	O
sparsity	O
.	O

subsection	O
:	O
Architecture	O
For	O
brevity	O
,	O
we	O
will	O
only	O
explain	O
the	O
sparse	O
implementations	O
of	O
these	O
two	O
items	O
,	O
for	O
the	O
full	O
model	O
details	O
refer	O
to	O
the	O
original	O
paper	O
.	O

The	O
mechanism	O
for	O
sparse	B-Task
memory	I-Task
reads	I-Task
and	I-Task
writes	E-Task
was	O
implemented	O
identically	O
to	O
SAM	S-Method
.	O

It	O
is	O
possible	O
to	O
implement	O
a	O
scalable	B-Method
version	E-Method
of	O
the	O
dynamic	B-Method
memory	I-Method
allocation	I-Method
system	E-Method
of	O
the	O
DNC	S-Method
avoiding	O
any	O
operations	O
by	O
using	O
a	O
heap	O
.	O

However	O
,	O
because	O
it	O
is	O
practical	O
to	O
run	O
the	O
SDNC	S-Method
with	O
many	O
more	O
memory	O
words	O
,	O
reusing	O
memory	O
is	O
less	O
crucial	O
so	O
we	O
did	O
not	O
implement	O
this	O
and	O
used	O
the	O
same	O
usage	O
tracking	O
as	O
in	O
SAM	S-Method
.	O

The	O
temporal	B-Method
memory	I-Method
linkage	E-Method
in	O
the	O
DNC	S-Method
is	O
a	O
system	O
for	O
associating	O
and	O
recalling	O
memory	O
locations	O
which	O
were	O
written	O
in	O
a	O
temporal	O
order	O
,	O
for	O
exampling	B-Task
storing	I-Task
and	I-Task
retrieving	I-Task
a	I-Task
list	E-Task
.	O

In	O
the	O
DNC	S-Method
this	O
is	O
done	O
by	O
maintaining	O
a	O
temporal	B-Method
linkage	I-Method
matrix	E-Method
.	O

represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
after	O
location	O
.	O

This	O
matrix	O
is	O
updated	O
by	O
tracking	O
the	O
precedence	O
weighting	O
,	O
where	O
represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
.	O

The	O
memory	B-Method
linkage	E-Method
is	O
updated	O
according	O
to	O
the	O
following	O
recurrence	O
The	O
temporal	B-Method
linkage	E-Method
can	O
be	O
used	O
to	O
compute	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
either	O
forward	O
or	O
backward	O
The	O
read	O
head	O
then	O
uses	O
a	O
3	O
-	O
way	O
softmax	O
to	O
select	O
between	O
a	O
content	O
-	O
based	O
read	O
or	O
following	O
the	O
forward	O
or	O
backward	O
weighting	O
.	O

Naively	O
,	O
the	O
link	B-Method
matrix	E-Method
requires	O
memory	O
and	O
computation	O
although	O
proposes	O
a	O
method	O
to	O
reduce	O
the	O
computational	B-Metric
cost	E-Metric
to	O
and	O
memory	B-Metric
cost	E-Metric
.	O

In	O
order	O
to	O
maintain	O
the	O
scaling	O
properties	O
of	O
the	O
SAM	S-Method
,	O
we	O
wish	O
to	O
avoid	O
any	O
computational	O
dependence	O
on	O
.	O

We	O
do	O
this	O
by	O
maintaining	O
two	O
sparse	O
matrices	O
that	O
approximate	O
and	O
respectively	O
.	O

We	O
store	O
these	O
matrices	O
in	O
Compressed	O
Sparse	O
Row	O
format	O
.	O

They	O
are	O
defined	O
by	O
the	O
following	O
updates	O
:	O
Additionally	O
,	O
is	O
,	O
as	O
with	O
the	O
other	O
weight	O
vectors	O
maintained	O
as	O
a	O
sparse	O
vector	O
with	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

This	O
means	O
that	O
the	O
outer	O
product	O
of	O
has	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

In	O
addition	O
to	O
the	O
updates	O
specified	O
above	O
,	O
we	O
also	O
constrain	O
each	O
row	O
of	O
the	O
matrices	O
and	O
to	O
have	O
at	O
most	O
non	O
-	O
zero	O
entries	O
—	O
this	O
constraint	O
can	O
be	O
applied	O
in	O
because	O
at	O
most	O
rows	O
change	O
in	O
the	O
matrix	O
.	O

Once	O
these	O
matrices	O
are	O
applied	O
the	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
can	O
be	O
computed	O
similar	O
to	O
before	O
:	O
Note	O
,	O
the	O
number	O
of	O
locations	O
we	O
read	O
from	O
,	O
,	O
does	O
not	O
have	O
to	O
equal	O
the	O
number	O
of	O
outward	O
and	O
inward	O
links	O
we	O
preserve	O
,	O
.	O

We	O
typically	O
choose	O
as	O
this	O
is	O
still	O
very	O
fast	O
to	O
compute	O
(	O
in	O
total	O
to	O
calculate	O
on	O
a	O
single	O
CPU	O
thread	O
)	O
and	O
we	O
see	O
no	O
learning	O
benefit	O
with	O
larger	O
.	O

In	O
order	O
to	O
compute	O
the	O
gradients	O
,	O
and	O
need	O
to	O
be	O
stored	O
.	O

This	O
could	O
be	O
done	O
by	O
maintaining	O
a	O
sparse	O
record	O
of	O
the	O
updates	O
applied	O
and	O
reversing	O
them	O
,	O
similar	O
to	O
that	O
performed	O
with	O
the	O
memory	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

However	O
,	O
for	O
implementation	O
simplicity	O
we	O
did	O
not	O
pass	O
gradients	O
through	O
the	O
temporal	O
linkage	O
matrices	O
.	O

subsection	O
:	O
Results	O
We	O
benchmarked	O
the	O
speed	B-Metric
and	I-Metric
memory	I-Metric
performance	E-Metric
of	O
the	O
SDNC	S-Method
versus	O
a	O
naive	O
DNC	S-Method
implementation	O
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

The	O
results	O
are	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Here	O
,	O
the	O
computational	O
benefits	O
of	O
sparsity	O
are	O
more	O
pronounced	O
due	O
to	O
the	O
expensive	O
(	O
quadratic	O
time	S-Metric
and	O
space	S-Metric
)	O
temporal	O
transition	O
table	O
operations	O
in	O
the	O
DNC	S-Method
.	O

We	O
were	O
only	O
able	O
to	O
run	O
comparative	O
benchmarks	O
up	O
to	O
,	O
as	O
the	O
DNC	S-Method
quickly	O
exceeded	O
the	O
machine	O
’s	O
physical	O
memory	O
for	O
larger	O
values	O
;	O
however	O
even	O
at	O
this	O
modest	O
memory	O
size	O
we	O
see	O
a	O
speed	O
increase	O
of	O
and	O
physical	B-Metric
memory	I-Metric
reduction	E-Metric
of	O
.	O

Note	O
,	O
unlike	O
the	O
SAM	S-Method
memory	O
benchmark	O
in	O
Section	O
[	O
reference	O
]	O
we	O
plot	O
the	O
total	B-Metric
memory	I-Metric
consumption	E-Metric
,	O
i.e.	O
the	O
memory	B-Metric
overhead	E-Metric
of	O
the	O
initial	O
start	O
state	O
plus	O
the	O
memory	O
overhead	O
of	O
unrolling	O
the	O
core	O
over	O
a	O
sequence	O
.	O

This	O
is	O
because	O
the	O
SDNC	S-Method
and	O
DNC	S-Method
do	O
not	O
have	O
identical	O
start	O
states	O
.	O

The	O
sparse	B-Method
temporal	I-Method
transition	I-Method
matrices	E-Method
consume	O
much	O
less	O
memory	O
than	O
the	O
corresponding	O
in	O
the	O
DNC	S-Method
.	O

0.47	O
0.47	O
In	O
order	O
to	O
compare	O
the	O
models	O
on	O
an	O
interesting	O
task	O
we	O
ran	O
the	O
DNC	S-Method
and	O
SDNC	S-Method
on	O
the	O
Babi	S-Material
task	O
(	O
this	O
task	O
is	O
described	O
more	O
fully	O
in	O
the	O
main	O
text	O
)	O
.	O

The	O
results	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
and	O
demonstrate	O
the	O
SDNC	S-Method
is	O
capable	O
of	O
learning	S-Task
competitively	O
.	O

In	O
particular	O
,	O
it	O
achieves	O
the	O
best	O
report	O
result	O
on	O
the	O
Babi	S-Material
task	O
.	O

appendix	O
:	O
Benchmarking	O
details	O
Each	O
model	O
contained	O
an	O
LSTM	S-Method
controller	O
with	O
100	O
hidden	O
units	O
,	O
an	O
external	O
memory	O
containing	O
slots	O
of	O
memory	O
,	O
with	O
word	O
size	O
and	O
access	O
heads	O
.	O

For	O
speed	B-Task
benchmarks	E-Task
,	O
a	O
minibatch	O
size	O
of	O
was	O
used	O
to	O
ensure	O
fair	O
comparison	O
-	O
as	O
many	O
dense	O
operations	O
(	O
e.g.	O
matrix	B-Task
multiplication	E-Task
)	O
can	O
be	O
batched	O
efficiently	O
.	O

For	O
memory	B-Task
benchmarks	E-Task
,	O
the	O
minibatch	O
size	O
was	O
set	O
to	O
.	O

We	O
used	O
Torch7	S-Method
to	O
implement	O
SAM	S-Method
,	O
DAM	S-Method
,	O
NTM	S-Method
,	O
DNC	S-Method
and	O
SDNC	S-Method
.	O

Eigen	B-Method
v3	E-Method
was	O
used	O
for	O
the	O
fast	O
sparse	B-Task
tensor	I-Task
operations	E-Task
,	O
using	O
the	O
provided	O
CSC	O
and	O
CSR	O
formats	O
.	O

All	O
benchmarks	O
were	O
run	O
on	O
a	O
Linux	O
desktop	O
running	O
Ubuntu	O
14.04.1	O
with	O
32GiB	O
of	O
RAM	O
and	O
an	O
Intel	O
Xeon	O
E5	O
-	O
1650	O
3.20GHz	O
processor	O
with	O
power	O
scaling	O
disabled	O
.	O

appendix	O
:	O
Generalization	O
on	O
associative	B-Task
recall	E-Task
appendix	O
:	O
Babi	S-Material
results	O
See	O
the	O
main	O
text	O
for	O
a	O
description	O
of	O
the	O
Babi	S-Material
task	O
and	O
its	O
relevance	O
.	O

Here	O
we	O
report	O
the	O
best	O
and	O
mean	O
results	O
for	O
all	O
of	O
the	O
models	O
on	O
this	O
task	O
.	O

The	O
task	O
was	O
encoded	O
using	O
straightforward	O
1	B-Method
-	I-Method
hot	I-Method
word	I-Method
encodings	E-Method
for	O
both	O
the	O
input	O
and	O
output	O
.	O

We	O
trained	O
a	O
single	O
model	O
on	O
all	O
of	O
the	O
tasks	O
,	O
and	O
used	O
the	O
10	O
,	O
000	O
examples	O
per	O
task	O
version	O
of	O
the	O
training	O
set	O
(	O
a	O
small	O
subset	O
of	O
which	O
we	O
used	O
as	O
a	O
validation	O
set	O
for	O
selecting	O
the	O
best	O
run	O
and	O
hyperparameters	O
)	O
.	O

Previous	O
work	O
has	O
reported	O
best	O
results	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
,	O
which	O
with	O
only	O
15	O
runs	O
is	O
a	O
noisy	O
comparison	O
,	O
so	O
we	O
additionally	O
report	O
the	O
mean	O
and	O
variance	O
for	O
all	O
runs	O
with	O
the	O
best	O
selected	O
hyperparameters	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
.	O

document	O
:	O
Fast	O
and	O
Accurate	O
Deep	B-Method
Network	I-Method
Learning	E-Method
by	O
Exponential	B-Method
Linear	I-Method
Units	E-Method
(	O
ELUs	S-Method
)	O
We	O
introduce	O
the	O
“	O
exponential	B-Method
linear	I-Method
unit	I-Method
”	E-Method
(	O
ELU	S-Method
)	O
which	O
speeds	O
up	O
learning	S-Task
in	O
deep	B-Method
neural	I-Method
networks	E-Method
and	O
leads	O
to	O
higher	O
classification	B-Metric
accuracies	E-Metric
.	O

Like	O
rectified	B-Method
linear	I-Method
units	E-Method
(	O
ReLUs	S-Method
)	O
,	O
leaky	B-Method
ReLUs	E-Method
(	O
LReLUs	S-Method
)	O
and	O
parametrized	B-Method
ReLUs	E-Method
(	O
PReLUs	S-Method
)	O
,	O
ELUs	S-Method
alleviate	O
the	O
vanishing	B-Task
gradient	I-Task
problem	E-Task
via	O
the	O
identity	O
for	O
positive	O
values	O
.	O

However	O
ELUs	S-Method
have	O
improved	O
learning	B-Metric
characteristics	E-Metric
compared	O
to	O
the	O
units	O
with	O
other	O
activation	O
functions	O
.	O

In	O
contrast	O
to	O
ReLUs	S-Method
,	O
ELUs	S-Method
have	O
negative	O
values	O
which	O
allows	O
them	O
to	O
push	O
mean	O
unit	O
activations	O
closer	O
to	O
zero	O
like	O
batch	B-Method
normalization	E-Method
but	O
with	O
lower	O
computational	B-Metric
complexity	E-Metric
.	O

Mean	O
shifts	O
toward	O
zero	O
speed	O
up	O
learning	S-Task
by	O
bringing	O
the	O
normal	O
gradient	O
closer	O
to	O
the	O
unit	O
natural	O
gradient	O
because	O
of	O
a	O
reduced	O
bias	O
shift	O
effect	O
.	O

While	O
LReLUs	S-Method
and	O
PReLUs	S-Method
have	O
negative	O
values	O
,	O
too	O
,	O
they	O
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O

ELUs	S-Method
saturate	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
inputs	O
and	O
thereby	O
decrease	O
the	O
forward	O
propagated	O
variation	O
and	O
information	O
.	O

Therefore	O
ELUs	S-Method
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
while	O
they	O
do	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O

In	O
experiments	O
,	O
ELUs	S-Method
lead	O
not	O
only	O
to	O
faster	O
learning	S-Task
,	O
but	O
also	O
to	O
significantly	O
better	O
generalization	B-Metric
performance	E-Metric
than	O
ReLUs	S-Method
and	O
LReLUs	S-Method
on	O
networks	O
with	O
more	O
than	O
5	O
layers	O
.	O

On	O
CIFAR	B-Method
-	I-Method
100	I-Method
ELUs	I-Method
networks	E-Method
significantly	O
outperform	O
ReLU	S-Method
networks	O
with	O
batch	B-Method
normalization	E-Method
while	O
batch	B-Method
normalization	E-Method
does	O
not	O
improve	O
ELU	B-Method
networks	E-Method
.	O

ELU	B-Method
networks	E-Method
are	O
among	O
the	O
top	O
10	O
reported	O
CIFAR	B-Material
-	I-Material
10	E-Material
results	O
and	O
yield	O
the	O
best	O
published	O
result	O
on	O
CIFAR	B-Material
-	I-Material
100	E-Material
,	O
without	O
resorting	O
to	O
multi	B-Method
-	I-Method
view	I-Method
evaluation	E-Method
or	O
model	B-Method
averaging	E-Method
.	O

On	O
ImageNet	O
,	O
ELU	B-Method
networks	E-Method
considerably	O
speed	O
up	O
learning	S-Task
compared	O
to	O
a	O
ReLU	S-Method
network	O
with	O
the	O
same	O
architecture	O
,	O
obtaining	O
less	O
than	O
10	O
%	O
classification	B-Metric
error	E-Metric
for	O
a	O
single	O
crop	B-Method
,	I-Method
single	I-Method
model	I-Method
network	E-Method
.	O

section	O
:	O
Introduction	O
Currently	O
the	O
most	O
popular	O
activation	B-Method
function	E-Method
for	O
neural	B-Method
networks	E-Method
is	O
the	O
rectified	B-Method
linear	I-Method
unit	E-Method
(	O
ReLU	S-Method
)	O
,	O
which	O
was	O
first	O
proposed	O
for	O
restricted	B-Method
Boltzmann	I-Method
machines	E-Method
and	O
then	O
successfully	O
used	O
for	O
neural	B-Task
networks	E-Task
.	O

The	O
ReLU	S-Method
activation	O
function	O
is	O
the	O
identity	O
for	O
positive	O
arguments	O
and	O
zero	O
otherwise	O
.	O

Besides	O
producing	O
sparse	B-Method
codes	E-Method
,	O
the	O
main	O
advantage	O
of	O
ReLUs	S-Method
is	O
that	O
they	O
alleviate	O
the	O
vanishing	B-Task
gradient	I-Task
problem	E-Task
since	O
the	O
derivative	O
of	O
1	O
for	O
positive	O
values	O
is	O
not	O
contractive	O
.	O

However	O
ReLUs	O
are	O
non	O
-	O
negative	O
and	O
,	O
therefore	O
,	O
have	O
a	O
mean	O
activation	O
larger	O
than	O
zero	O
.	O

Units	O
that	O
have	O
a	O
non	O
-	O
zero	O
mean	O
activation	O
act	O
as	O
bias	O
for	O
the	O
next	O
layer	O
.	O

If	O
such	O
units	O
do	O
not	O
cancel	O
each	O
other	O
out	O
,	O
learning	O
causes	O
a	O
bias	O
shift	O
for	O
units	O
in	O
next	O
layer	O
.	O

The	O
more	O
the	O
units	O
are	O
correlated	O
,	O
the	O
higher	O
their	O
bias	O
shift	O
.	O

We	O
will	O
see	O
that	O
Fisher	B-Method
optimal	I-Method
learning	E-Method
,	O
i.e.	O
,	O
the	O
natural	O
gradient	O
,	O
would	O
correct	O
for	O
the	O
bias	O
shift	O
by	O
adjusting	O
the	O
weight	O
updates	O
.	O

Thus	O
,	O
less	O
bias	O
shift	O
brings	O
the	O
standard	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
and	O
speeds	O
up	O
learning	S-Task
.	O

We	O
aim	O
at	O
activation	O
functions	O
that	O
push	O
activation	O
means	O
closer	O
to	O
zero	O
to	O
decrease	O
the	O
bias	O
shift	O
effect	O
.	O

Centering	O
the	O
activations	O
at	O
zero	O
has	O
been	O
proposed	O
in	O
order	O
to	O
keep	O
the	O
off	O
-	O
diagonal	O
entries	O
of	O
the	O
Fisher	O
information	O
matrix	O
small	O
.	O

For	O
neural	B-Method
network	E-Method
it	O
is	O
known	O
that	O
centering	O
the	O
activations	O
speeds	O
up	O
learning	S-Task
.	O

“	O
Batch	B-Method
normalization	E-Method
”	O
also	O
centers	O
activations	O
with	O
the	O
goal	O
to	O
counter	O
the	O
internal	O
covariate	O
shift	O
.	O

Also	O
the	O
Projected	B-Method
Natural	I-Method
Gradient	I-Method
Descent	I-Method
algorithm	E-Method
(	O
PRONG	S-Method
)	O
centers	O
the	O
activations	O
by	O
implicitly	O
whitening	O
them	O
.	O

An	O
alternative	O
to	O
centering	O
is	O
to	O
push	O
the	O
mean	O
activation	O
toward	O
zero	O
by	O
an	O
appropriate	O
activation	B-Method
function	E-Method
.	O

Therefore	O
has	O
been	O
preferred	O
over	O
logistic	B-Method
functions	E-Method
.	O

Recently	O
“	O
Leaky	B-Method
ReLUs	I-Method
”	E-Method
(	O
LReLUs	S-Method
)	O
that	O
replace	O
the	O
negative	O
part	O
of	O
the	O
ReLU	S-Method
with	O
a	O
linear	O
function	O
have	O
been	O
shown	O
to	O
be	O
superior	O
to	O
ReLUs	S-Method
.	O

Parametric	B-Method
Rectified	I-Method
Linear	I-Method
Units	E-Method
(	O
PReLUs	S-Method
)	O
generalize	O
LReLUs	S-Method
by	O
learning	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
yielded	O
improved	O
learning	O
behavior	O
on	O
large	O
image	O
benchmark	O
data	O
sets	O
.	O

Another	O
variant	O
are	O
Randomized	B-Method
Leaky	I-Method
Rectified	I-Method
Linear	I-Method
Units	E-Method
(	O
RReLUs	S-Method
)	O
which	O
randomly	O
sample	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
raised	O
the	O
performance	O
on	O
image	O
benchmark	O
datasets	O
and	O
convolutional	B-Method
networks	E-Method
.	O

In	O
contrast	O
to	O
ReLUs	O
,	O
activation	O
functions	O
like	O
LReLUs	S-Method
,	O
PReLUs	O
,	O
and	O
RReLUs	S-Method
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O

We	O
propose	O
an	O
activation	B-Method
function	E-Method
that	O
has	O
negative	O
values	O
to	O
allow	O
for	O
mean	O
activations	O
close	O
to	O
zero	O
,	O
but	O
which	O
saturates	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
arguments	O
.	O

The	O
saturation	O
decreases	O
the	O
variation	O
of	O
the	O
units	O
if	O
deactivated	O
,	O
so	O
the	O
precise	O
deactivation	O
argument	O
is	O
less	O
relevant	O
.	O

Such	O
an	O
activation	B-Method
function	E-Method
can	O
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
but	O
does	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O

Therefore	O
,	O
such	O
an	O
activation	B-Method
function	E-Method
is	O
more	O
robust	O
to	O
noise	O
.	O

Consequently	O
,	O
dependencies	O
between	O
coding	O
units	O
are	O
much	O
easier	O
to	O
model	O
and	O
much	O
easier	O
to	O
interpret	O
since	O
only	O
activated	O
code	O
units	O
carry	O
much	O
information	O
.	O

Furthermore	O
,	O
distinct	O
concepts	O
are	O
much	O
less	O
likely	O
to	O
interfere	O
with	O
such	O
activation	O
functions	O
since	O
the	O
deactivation	O
state	O
is	O
non	O
-	O
informative	O
,	O
i.e.	O
variance	O
decreasing	O
.	O

section	O
:	O
Bias	B-Method
Shift	I-Method
Correction	E-Method
Speeds	O
Up	O
Learning	S-Task
To	O
derive	O
and	O
analyze	O
the	O
bias	O
shift	O
effect	O
mentioned	O
in	O
the	O
introduction	O
,	O
we	O
utilize	O
the	O
natural	O
gradient	O
.	O

The	O
natural	O
gradient	O
corrects	O
the	O
gradient	O
direction	O
with	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
and	O
,	O
thereby	O
,	O
enables	O
Fisher	B-Method
optimal	I-Method
learning	E-Method
,	O
which	O
ensures	O
the	O
steepest	O
descent	O
in	O
the	O
Riemannian	O
parameter	O
manifold	O
and	O
Fisher	B-Metric
efficiency	E-Metric
for	O
online	B-Task
learning	E-Task
.	O

The	O
recently	O
introduced	O
Hessian	B-Method
-	I-Method
Free	I-Method
Optimization	I-Method
technique	E-Method
and	O
the	O
Krylov	B-Method
Subspace	I-Method
Descent	I-Method
methods	E-Method
use	O
an	O
extended	B-Method
Gauss	I-Method
-	I-Method
Newton	I-Method
approximation	E-Method
of	O
the	O
Hessian	O
,	O
therefore	O
they	O
can	O
be	O
interpreted	O
as	O
versions	O
of	O
natural	B-Method
gradient	I-Method
descent	E-Method
.	O

Since	O
for	O
neural	B-Method
networks	E-Method
the	O
Fisher	O
information	O
matrix	O
is	O
typically	O
too	O
expensive	O
to	O
compute	O
,	O
different	O
approximations	O
of	O
the	O
natural	O
gradient	O
have	O
been	O
proposed	O
.	O

Topmoumoute	B-Method
Online	I-Method
natural	I-Method
Gradient	I-Method
Algorithm	E-Method
(	O
TONGA	S-Method
)	O
uses	O
a	O
low	B-Method
-	I-Method
rank	I-Method
approximation	I-Method
of	I-Method
natural	I-Method
gradient	I-Method
descent	E-Method
.	O

FActorized	B-Method
Natural	I-Method
Gradient	E-Method
(	O
FANG	S-Method
)	O
estimates	O
the	O
natural	O
gradient	O
via	O
an	O
approximation	B-Method
of	I-Method
the	I-Method
Fisher	I-Method
information	I-Method
matrix	E-Method
by	O
a	O
Gaussian	B-Method
graphical	I-Method
model	E-Method
.	O

The	O
Fisher	O
information	O
matrix	O
can	O
be	O
approximated	O
by	O
a	O
block	B-Method
-	I-Method
diagonal	I-Method
matrix	E-Method
,	O
where	O
unit	O
or	O
quasi	O
-	O
diagonal	O
natural	O
gradients	O
are	O
used	O
.	O

Unit	O
natural	O
gradients	O
or	O
“	O
Unitwise	B-Method
Fisher	I-Method
’s	I-Method
scoring	I-Method
”	E-Method
are	O
based	O
on	O
natural	O
gradients	O
for	O
perceptrons	S-Method
.	O

We	O
will	O
base	O
our	O
analysis	O
on	O
the	O
unit	O
natural	O
gradient	O
.	O

We	O
assume	O
a	O
parameterized	B-Method
probabilistic	I-Method
model	E-Method
with	O
parameter	O
vector	O
and	O
data	O
.	O

The	O
training	O
data	O
are	O
with	O
,	O
where	O
is	O
the	O
input	O
for	O
example	O
and	O
is	O
its	O
label	O
.	O

is	O
the	O
loss	O
of	O
example	O
using	O
model	O
.	O

The	O
average	B-Metric
loss	E-Metric
on	O
the	O
training	O
data	O
is	O
the	O
empirical	O
risk	O
.	O

Gradient	B-Method
descent	E-Method
updates	O
the	O
weight	O
vector	O
by	O
where	O
is	O
the	O
learning	B-Metric
rate	E-Metric
.	O

The	O
natural	O
gradient	O
is	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
multiplied	O
by	O
the	O
gradient	O
of	O
the	O
empirical	O
risk	O
:	O
.	O

For	O
a	O
multi	B-Method
-	I-Method
layer	I-Method
perceptron	E-Method
is	O
the	O
unit	O
activation	O
vector	O
and	O
is	O
the	O
bias	O
unit	O
activation	O
.	O

We	O
consider	O
the	O
ingoing	O
weights	O
to	O
unit	O
,	O
therefore	O
we	O
drop	O
the	O
index	O
:	O
for	O
the	O
weight	O
from	O
unit	O
to	O
unit	O
,	O
for	O
the	O
activation	O
,	O
and	O
for	O
the	O
bias	O
weight	O
of	O
unit	O
.	O

The	O
activation	B-Method
function	E-Method
maps	O
the	O
net	O
input	O
of	O
unit	O
to	O
its	O
activation	O
.	O

For	O
computing	O
the	O
Fisher	O
information	O
matrix	O
,	O
the	O
derivative	O
of	O
the	O
log	O
-	O
output	O
probability	O
is	O
required	O
.	O

Therefore	O
we	O
define	O
the	O
at	O
unit	O
as	O
,	O
which	O
can	O
be	O
computed	O
via	O
backpropagation	S-Method
,	O
but	O
using	O
the	O
log	O
-	O
output	O
probability	O
instead	O
of	O
the	O
conventional	O
loss	O
function	O
.	O

The	O
derivative	O
is	O
.	O

We	O
restrict	O
the	O
Fisher	O
information	O
matrix	O
to	O
weights	O
leading	O
to	O
unit	O
which	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
.	O

captures	O
only	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
.	O

Consequently	O
,	O
the	O
unit	O
natural	O
gradient	O
only	O
corrects	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
,	O
i.e.	O
considers	O
the	O
Riemannian	O
parameter	O
manifold	O
only	O
in	O
a	O
subspace	O
.	O

The	O
unit	O
Fisher	O
information	O
matrix	O
is	O
Weighting	O
the	O
activations	O
by	O
is	O
equivalent	O
to	O
adjusting	O
the	O
probability	O
of	O
drawing	O
inputs	O
.	O

Inputs	O
with	O
large	O
are	O
drawn	O
with	O
higher	O
probability	O
.	O

Since	O
,	O
we	O
can	O
define	O
a	O
distribution	O
:	O
Using	O
,	O
the	O
entries	O
of	O
can	O
be	O
expressed	O
as	O
second	O
moments	O
:	O
If	O
the	O
bias	O
unit	O
is	O
with	O
weight	O
then	O
the	O
weight	O
vector	O
can	O
be	O
divided	O
into	O
a	O
bias	O
part	O
and	O
the	O
rest	O
:	O
.	O

For	O
the	O
row	O
that	O
corresponds	O
to	O
the	O
bias	O
weight	O
,	O
we	O
have	O
:	O
The	O
next	O
Theorem	O
[	O
reference	O
]	O
gives	O
the	O
correction	O
of	O
the	O
standard	O
gradient	O
by	O
the	O
unit	O
natural	O
gradient	O
where	O
the	O
bias	O
weight	O
is	O
treated	O
separately	O
(	O
see	O
also	O
)	O
.	O

theorem	O
:	O
.	O

The	O
unit	O
natural	O
gradient	O
corrects	O
the	O
weight	O
update	O
(	O
⁢ΔwT	O
,	O
⁢Δw0	O
)	O
T	O
to	O
a	O
unit	O
i	O
by	O
following	O
affine	O
transformation	O
of	O
the	O
gradient	O
=	O
∇	O
(	O
wT	O
,	O
w0	O
)	O
TRemp	O
(	O
gT	O
,	O
g0	O
)	O
T	O
:	O
where	O
A=	O
[	O
⁢F	O
(	O
w	O
)]	O
⁢¬0	O
,	O
⁢¬0=⁢E⁢p	O
(	O
z	O
)(	O
δ2	O
)	O
E⁢q	O
(	O
z	O
)(	O
⁢aaT	O
)	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
without	O
row	O
0	O
and	O
column	O
0	O
corresponding	O
to	O
the	O
bias	O
weight	O
.	O

The	O
vector	O
=	O
b	O
[	O
⁢F	O
(	O
w	O
)]	O
0	O
is	O
the	O
zeroth	O
column	O
of	O
F	O
corresponding	O
to	O
the	O
bias	O
weight	O
,	O
and	O
the	O
positive	O
scalar	O
s	O
is	O
where	O
a	O
is	O
the	O
vector	O
of	O
activations	O
of	O
units	O
with	O
weights	O
to	O
unit	O
i	O
and	O
=	O
⁢q	O
(	O
z	O
)	O
⁢δ2	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
E⁢p	O
(	O
z	O
)-	O
1	O
(	O
δ2	O
)	O
.	O

proof	O
:	O
Proof	O
.	O

Multiplying	O
the	O
inverse	O
Fisher	O
matrix	O
with	O
the	O
separated	O
gradient	O
gives	O
the	O
weight	B-Method
update	E-Method
:	O
where	O
The	O
previous	O
formula	O
is	O
derived	O
in	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O

Using	O
in	O
the	O
update	O
gives	O
The	O
right	O
hand	O
side	O
is	O
obtained	O
by	O
inserting	O
in	O
the	O
left	O
hand	O
side	O
update	O
.	O

Since	O
,	O
,	O
and	O
,	O
we	O
obtain	O
Applying	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
gives	O
the	O
formula	O
for	O
.	O

∎	O
The	O
bias	O
shift	O
(	O
mean	O
shift	O
)	O
of	O
unit	O
is	O
the	O
change	O
of	O
unit	O
’s	O
mean	O
value	O
due	O
to	O
the	O
weight	B-Method
update	E-Method
.	O

Bias	O
shifts	O
of	O
unit	O
lead	O
to	O
oscillations	O
and	O
impede	O
learning	S-Task
.	O

See	O
Section	O
4.4	O
in	O
for	O
demonstrating	O
this	O
effect	O
at	O
the	O
inputs	O
and	O
in	O
for	O
explaining	O
this	O
effect	O
using	O
the	O
input	O
covariance	O
matrix	O
.	O

Such	O
bias	O
shifts	O
are	O
mitigated	O
or	O
even	O
prevented	O
by	O
the	O
unit	O
natural	O
gradient	O
.	O

The	O
bias	B-Task
shift	I-Task
correction	E-Task
of	O
the	O
unit	O
natural	O
gradient	O
is	O
the	O
effect	O
on	O
the	O
bias	O
shift	O
due	O
to	O
which	O
captures	O
the	O
interaction	O
between	O
the	O
bias	O
unit	O
and	O
the	O
incoming	O
units	O
.	O

Without	O
bias	B-Method
shift	I-Method
correction	E-Method
,	O
i.e.	O
,	O
and	O
,	O
the	O
weight	O
updates	O
are	O
and	O
.	O

As	O
only	O
the	O
activations	O
depend	O
on	O
the	O
input	O
,	O
the	O
bias	O
shift	O
can	O
be	O
computed	O
by	O
multiplying	O
the	O
weight	O
update	O
by	O
the	O
mean	O
of	O
the	O
activation	O
vector	O
.	O

Thus	O
we	O
obtain	O
the	O
bias	O
shift	O
.	O

The	O
bias	O
shift	O
strongly	O
depends	O
on	O
the	O
correlation	O
of	O
the	O
incoming	O
units	O
which	O
is	O
captured	O
by	O
.	O

Next	O
,	O
Theorem	O
[	O
reference	O
]	O
states	O
that	O
the	O
bias	B-Method
shift	I-Method
correction	E-Method
by	O
the	O
unit	O
natural	O
gradient	O
can	O
be	O
considered	O
to	O
correct	O
the	O
incoming	O
mean	O
proportional	O
to	O
toward	O
zero	O
.	O

theorem	O
:	O
.	O

The	O
bias	O
shift	O
correction	O
by	O
the	O
unit	O
natural	O
gradient	O
is	O
equivalent	O
to	O
an	O
additive	B-Method
correction	E-Method
of	O
the	O
incoming	O
mean	O
by	O
-	O
⁢kE⁢q	O
(	O
z	O
)(	O
a	O
)	O
and	O
a	O
multiplicative	B-Method
correction	E-Method
of	O
the	O
bias	O
unit	O
by	O
k	O
,	O
where	O
proof	O
:	O
Proof	O
.	O

Using	O
,	O
the	O
bias	O
shift	O
is	O
:	O
The	O
mean	O
correction	O
term	O
,	O
indicated	O
by	O
an	O
underbrace	O
in	O
previous	O
formula	O
,	O
is	O
The	O
expression	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
for	O
follows	O
from	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O

The	O
bias	O
unit	O
correction	O
term	O
is	O
.	O

∎	O
In	O
Theorem	O
[	O
reference	O
]	O
we	O
can	O
reformulate	O
.	O

Therefore	O
increases	O
with	O
the	O
length	O
of	O
for	O
given	O
variances	O
and	O
covariances	O
.	O

Consequently	O
the	O
bias	B-Task
shift	I-Task
correction	E-Task
through	O
the	O
unit	O
natural	O
gradient	O
is	O
governed	O
by	O
the	O
length	O
of	O
.	O

The	O
bias	B-Method
shift	I-Method
correction	E-Method
is	O
zero	O
for	O
since	O
does	O
not	O
correct	O
the	O
bias	O
unit	O
multiplicatively	O
.	O

Using	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
is	O
split	O
into	O
an	O
offset	O
and	O
an	O
information	O
containing	O
term	O
:	O
In	O
general	O
,	O
smaller	O
positive	O
⁢E⁢p	O
(	O
z	O
)(	O
a	O
)	O
lead	O
to	O
smaller	O
positive	O
⁢E⁢q	O
(	O
z	O
)(	O
a	O
)	O
,	O
therefore	O
to	O
smaller	O
corrections	O
.	O

The	O
reason	O
is	O
that	O
in	O
general	O
the	O
largest	O
absolute	O
components	O
of	O
are	O
positive	O
,	O
since	O
activated	O
inputs	O
will	O
activate	O
the	O
unit	O
which	O
in	O
turn	O
will	O
have	O
large	O
impact	O
on	O
the	O
output	O
.	O

To	O
summarize	O
,	O
the	O
unit	O
natural	O
gradient	O
corrects	O
the	O
bias	O
shift	O
of	O
unit	O
via	O
the	O
interactions	O
of	O
incoming	O
units	O
with	O
the	O
bias	O
unit	O
to	O
ensure	O
efficient	O
learning	S-Task
.	O

This	O
correction	O
is	O
equivalent	O
to	O
shifting	O
the	O
mean	O
activations	O
of	O
the	O
incoming	O
units	O
toward	O
zero	O
and	O
scaling	O
up	O
the	O
bias	O
unit	O
.	O

To	O
reduce	O
the	O
undesired	O
bias	O
shift	O
effect	O
without	O
the	O
natural	O
gradient	O
,	O
either	O
the	O
(	O
i	O
)	O
activation	O
of	O
incoming	O
units	O
can	O
be	O
centered	O
at	O
zero	O
or	O
(	O
ii	O
)	O
activation	O
functions	O
with	O
negative	O
values	O
can	O
be	O
used	O
.	O

We	O
introduce	O
a	O
new	O
activation	B-Method
function	E-Method
with	O
negative	O
values	O
while	O
keeping	O
the	O
identity	O
for	O
positive	O
arguments	O
where	O
it	O
is	O
not	O
contradicting	O
.	O

section	O
:	O
Exponential	B-Method
Linear	I-Method
Units	E-Method
(	O
ELUs	S-Method
)	O
The	O
exponential	B-Method
linear	I-Method
unit	E-Method
(	O
ELU	S-Method
)	O
with	O
is	O
The	O
ELU	S-Method
hyperparameter	O
controls	O
the	O
value	O
to	O
which	O
an	O
ELU	S-Method
saturates	O
for	O
negative	O
net	O
inputs	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

ELUs	S-Method
diminish	O
the	O
vanishing	O
gradient	O
effect	O
as	O
rectified	O
linear	O
units	O
(	O
ReLUs	S-Method
)	O
and	O
leaky	B-Method
ReLUs	E-Method
(	O
LReLUs	S-Method
)	O
do	O
.	O

The	O
vanishing	B-Task
gradient	I-Task
problem	E-Task
is	O
alleviated	O
because	O
the	O
positive	O
part	O
of	O
these	O
functions	O
is	O
the	O
identity	O
,	O
therefore	O
their	O
derivative	O
is	O
one	O
and	O
not	O
contractive	O
.	O

In	O
contrast	O
,	O
and	O
sigmoid	B-Method
activation	I-Method
functions	E-Method
are	O
contractive	O
almost	O
everywhere	O
.	O

In	O
contrast	O
to	O
ReLUs	O
,	O
ELUs	S-Method
have	O
negative	O
values	O
which	O
pushes	O
the	O
mean	O
of	O
the	O
activations	O
closer	O
to	O
zero	O
.	O

Mean	O
activations	O
that	O
are	O
closer	O
to	O
zero	O
enable	O
faster	O
learning	S-Task
as	O
they	O
bring	O
the	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
(	O
see	O
Theorem	O
[	O
reference	O
]	O
and	O
text	O
thereafter	O
)	O
.	O

ELUs	S-Method
saturate	O
to	O
a	O
negative	O
value	O
when	O
the	O
argument	O
gets	O
smaller	O
.	O

Saturation	O
means	O
a	O
small	O
derivative	O
which	O
decreases	O
the	O
variation	O
and	O
the	O
information	O
that	O
is	O
propagated	O
to	O
the	O
next	O
layer	O
.	O

Therefore	O
the	O
representation	O
is	O
both	O
noise	O
-	O
robust	O
and	O
low	O
-	O
complex	O
.	O

ELUs	S-Method
code	O
the	O
degree	O
of	O
presence	O
of	O
input	O
concepts	O
,	O
while	O
they	O
neither	O
quantify	O
the	O
degree	O
of	O
their	O
absence	O
nor	O
distinguish	O
the	O
causes	O
of	O
their	O
absence	O
.	O

This	O
property	O
of	O
non	O
-	O
informative	O
deactivation	O
states	O
is	O
also	O
present	O
at	O
ReLUs	S-Method
and	O
allowed	O
to	O
detect	O
biclusters	O
corresponding	O
to	O
biological	O
modules	O
in	O
gene	O
expression	O
datasets	O
and	O
to	O
identify	O
toxicophores	B-Task
in	I-Task
toxicity	I-Task
prediction	E-Task
.	O

The	O
enabling	O
features	O
for	O
these	O
interpretations	O
is	O
that	O
activation	S-Task
can	O
be	O
clearly	O
distinguished	O
from	O
deactivation	S-Method
and	O
that	O
only	O
active	O
units	O
carry	O
relevant	O
information	O
and	O
can	O
crosstalk	O
.	O

section	O
:	O
Experiments	O
Using	O
ELUs	S-Method
In	O
this	O
section	O
,	O
we	O
assess	O
the	O
performance	O
of	O
exponential	B-Method
linear	I-Method
units	E-Method
(	O
ELUs	S-Method
)	O
if	O
used	O
for	O
unsupervised	S-Task
and	O
supervised	B-Task
learning	I-Task
of	I-Task
deep	I-Task
autoencoders	E-Task
and	O
deep	B-Method
convolutional	I-Method
networks	E-Method
.	O

ELUs	S-Method
with	O
are	O
compared	O
to	O
(	O
i	O
)	O
Rectified	B-Method
Linear	I-Method
Units	E-Method
(	O
ReLUs	S-Method
)	O
with	O
activation	O
,	O
(	O
ii	O
)	O
Leaky	B-Method
ReLUs	E-Method
(	O
LReLUs	S-Method
)	O
with	O
activation	O
(	O
)	O
,	O
and	O
(	O
iii	O
)	O
Shifted	B-Method
ReLUs	E-Method
(	O
SReLUs	S-Method
)	O
with	O
activation	O
.	O

Comparisons	O
are	O
done	O
with	O
and	O
without	O
batch	B-Method
normalization	E-Method
.	O

The	O
following	O
benchmark	O
datasets	O
are	O
used	O
:	O
(	O
i	O
)	O
MNIST	O
(	O
gray	O
images	O
in	O
10	O
classes	O
,	O
60k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
ii	O
)	O
CIFAR	B-Material
-	I-Material
10	E-Material
(	O
color	O
images	O
in	O
10	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
iii	O
)	O
CIFAR	B-Material
-	I-Material
100	E-Material
(	O
color	O
images	O
in	O
100	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
and	O
(	O
iv	O
)	O
ImageNet	O
(	O
color	O
images	O
in	O
1	O
,	O
000	O
classes	O
,	O
1.3	O
M	O
train	O
and	O
100k	O
tests	O
)	O
.	O

subsection	O
:	O
MNIST	O
subsubsection	O
:	O
Learning	B-Task
Behavior	E-Task
We	O
first	O
want	O
to	O
verify	O
that	O
ELUs	S-Method
keep	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
than	O
other	O
units	O
.	O

Fully	B-Method
connected	I-Method
deep	I-Method
neural	I-Method
networks	E-Method
with	O
ELUs	B-Method
(	I-Method
)	E-Method
,	O
ReLUs	S-Method
,	O
and	O
LReLUs	S-Method
(	O
)	O
were	O
trained	O
on	O
the	O
MNIST	O
digit	O
classification	O
dataset	O
while	O
each	O
hidden	O
unit	O
’s	O
activation	O
was	O
tracked	O
.	O

Each	O
network	O
had	O
eight	O
hidden	O
layers	O
of	O
128	O
units	O
each	O
,	O
and	O
was	O
trained	O
for	O
300	O
epochs	O
by	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
with	O
learning	B-Method
rate	E-Method
and	O
mini	O
-	O
batches	O
of	O
size	O
64	O
.	O

The	O
weights	O
have	O
been	O
initialized	O
according	O
to	O
.	O

After	O
each	O
epoch	O
we	O
calculated	O
the	O
units	O
’	O
average	O
activations	O
on	O
a	O
fixed	O
subset	O
of	O
the	O
training	O
data	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
median	O
over	O
all	O
units	O
along	O
learning	O
.	O

ELUs	S-Method
stay	O
have	O
smaller	O
median	O
throughout	O
the	O
training	O
process	O
.	O

The	O
training	B-Metric
error	E-Metric
of	O
ELU	B-Method
networks	E-Method
decreases	O
much	O
more	O
rapidly	O
than	O
for	O
the	O
other	O
networks	O
.	O

Section	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
compares	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	S-Method
and	O
ELU	B-Method
networks	E-Method
.	O

The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	S-Method
networks	O
.	O

This	O
indicates	O
that	O
ReLU	S-Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	B-Method
networks	E-Method
.	O

subsubsection	O
:	O
Autoencoder	B-Method
Learning	E-Method
To	O
evaluate	O
ELU	B-Method
networks	E-Method
at	O
unsupervised	S-Task
settings	O
,	O
we	O
followed	O
and	O
and	O
trained	O
a	O
deep	B-Method
autoencoder	E-Method
on	O
the	O
MNIST	O
dataset	O
.	O

The	O
encoder	B-Method
part	E-Method
consisted	O
of	O
four	O
fully	B-Method
connected	I-Method
hidden	I-Method
layers	E-Method
with	O
sizes	O
1000	O
,	O
500	O
,	O
250	O
and	O
30	O
,	O
respectively	O
.	O

The	O
decoder	O
part	O
was	O
symmetrical	O
to	O
the	O
encoder	O
.	O

For	O
learning	S-Task
we	O
applied	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
with	O
mini	O
-	O
batches	O
of	O
64	O
samples	O
for	O
500	O
epochs	O
using	O
the	O
fixed	O
learning	B-Metric
rates	E-Metric
(	O
)	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
,	O
that	O
ELUs	S-Method
outperform	O
the	O
competing	O
activation	B-Method
functions	E-Method
in	O
terms	O
of	O
training	B-Metric
/	I-Metric
test	I-Metric
set	I-Metric
reconstruction	I-Metric
error	E-Metric
for	O
all	O
learning	B-Metric
rates	E-Metric
.	O

As	O
already	O
noted	O
by	O
,	O
higher	O
learning	B-Metric
rates	E-Metric
seem	O
to	O
perform	O
better	O
.	O

subsection	O
:	O
Comparison	O
of	O
Activation	B-Method
Functions	E-Method
In	O
this	O
subsection	O
we	O
show	O
that	O
ELUs	S-Method
indeed	O
possess	O
a	O
superior	O
learning	B-Metric
behavior	E-Metric
compared	O
to	O
other	O
activation	B-Method
functions	E-Method
as	O
postulated	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Furthermore	O
we	O
show	O
that	O
ELU	B-Method
networks	E-Method
perform	O
better	O
than	O
ReLU	S-Method
networks	O
with	O
batch	B-Method
normalization	E-Method
.	O

We	O
use	O
as	O
benchmark	O
dataset	O
CIFAR	B-Material
-	I-Material
100	E-Material
and	O
use	O
a	O
relatively	O
simple	O
convolutional	B-Method
neural	I-Method
network	E-Method
(	O
CNN	S-Method
)	O
architecture	O
to	O
keep	O
the	O
computational	B-Metric
complexity	E-Metric
reasonable	O
for	O
comparisons	O
.	O

[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
The	O
CNN	S-Method
for	O
these	O
CIFAR	B-Material
-	I-Material
100	E-Material
experiments	O
consists	O
of	O
11	O
convolutional	B-Method
layers	E-Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
.	O

2	O
2	O
max	B-Method
-	I-Method
pooling	E-Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
.	O

For	O
network	B-Task
regularization	E-Task
we	O
used	O
the	O
following	O
drop	B-Metric
-	I-Metric
out	I-Metric
rate	E-Metric
for	O
the	O
last	O
layer	O
of	O
each	O
stack	O
(	O
)	O
.	O

The	O
-	O
weight	O
decay	O
regularization	O
term	O
was	O
set	O
to	O
.	O

The	O
following	O
learning	B-Metric
rate	I-Metric
schedule	E-Metric
was	O
applied	O
(	O
)	O
(	O
iterations	O
[	O
learning	O
rate	O
]	O
)	O
.	O

For	O
fair	O
comparisons	O
,	O
we	O
used	O
this	O
learning	B-Method
rate	I-Method
schedule	E-Method
for	O
all	O
networks	O
.	O

During	O
previous	O
experiments	O
,	O
this	O
schedule	O
was	O
optimized	O
for	O
ReLU	S-Method
networks	O
,	O
however	O
as	O
ELUs	S-Method
converge	O
faster	O
they	O
would	O
benefit	O
from	O
an	O
adjusted	O
schedule	O
.	O

The	O
momentum	B-Metric
term	I-Metric
learning	I-Metric
rate	E-Metric
was	O
fixed	O
to	O
0.9	O
.	O

The	O
dataset	O
was	O
preprocessed	O
as	O
described	O
in	O
with	O
global	B-Method
contrast	I-Method
normalization	E-Method
and	O
ZCA	B-Method
whitening	E-Method
.	O

Additionally	O
,	O
the	O
images	O
were	O
padded	O
with	O
four	O
zero	O
pixels	O
at	O
all	O
borders	O
.	O

The	O
model	O
was	O
trained	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O

Besides	O
that	O
,	O
we	O
no	O
further	O
augmented	O
the	O
dataset	O
during	O
training	O
.	O

Each	O
network	O
was	O
run	O
10	O
times	O
with	O
different	O
weight	B-Method
initialization	E-Method
.	O

Across	O
networks	O
with	O
different	O
activation	O
functions	O
the	O
same	O
run	O
number	O
had	O
the	O
same	O
initial	O
weights	O
.	O

Mean	B-Metric
test	I-Metric
error	E-Metric
results	O
of	O
networks	O
with	O
different	O
activation	O
functions	O
are	O
compared	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
which	O
also	O
shows	O
the	O
standard	O
deviation	O
.	O

ELUs	S-Method
yield	O
on	O
average	O
a	O
test	B-Metric
error	E-Metric
of	O
28.75	O
(	O
0.24	O
)	O
%	O
,	O
while	O
SReLUs	O
,	O
ReLUs	O
and	O
LReLUs	S-Method
yield	O
29.35	O
(	O
0.29	O
)	O
%	O
,	O
31.56	O
(	O
0.37	O
)	O
%	O
and	O
30.59	O
(	O
0.29	O
)	O
%	O
,	O
respectively	O
.	O

ELUs	S-Method
achieve	O
both	O
lower	O
training	B-Metric
loss	E-Metric
and	O
lower	O
test	B-Metric
error	E-Metric
than	O
ReLUs	S-Method
,	O
LReLUs	S-Method
,	O
and	O
SReLUs	S-Method
.	O

Both	O
the	O
ELU	B-Method
training	E-Method
and	O
test	O
performance	O
is	O
significantly	O
better	O
than	O
for	O
other	O
activation	B-Method
functions	E-Method
(	O
Wilcoxon	O
signed	O
-	O
rank	O
test	O
with	O
-	O
value	O
0.001	O
)	O
.	O

Batch	B-Method
normalization	E-Method
improved	O
ReLU	S-Method
and	O
LReLU	B-Method
networks	E-Method
,	O
but	O
did	O
not	O
improve	O
ELU	S-Method
and	O
SReLU	B-Method
networks	E-Method
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

ELU	B-Method
networks	E-Method
significantly	O
outperform	O
ReLU	S-Method
networks	O
with	O
batch	B-Method
normalization	E-Method
(	O
Wilcoxon	B-Metric
signed	I-Metric
-	I-Metric
rank	I-Metric
test	E-Metric
with	O
-	O
value	O
0.001	O
)	O
.	O

subsection	O
:	O
Classification	B-Metric
Performance	E-Metric
on	O
CIFAR	B-Material
-	I-Material
100	E-Material
and	O
CIFAR	B-Material
-	I-Material
10	E-Material
The	O
following	O
experiments	O
should	O
highlight	O
the	O
generalization	B-Method
capabilities	E-Method
of	O
ELU	B-Method
networks	E-Method
.	O

The	O
CNN	S-Method
architecture	O
is	O
more	O
sophisticated	O
than	O
in	O
the	O
previous	O
subsection	O
and	O
consists	O
of	O
18	O
convolutional	B-Method
layers	E-Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
.	O

Initial	O
drop	B-Metric
-	I-Metric
out	I-Metric
rate	E-Metric
,	O
Max	B-Method
-	I-Method
pooling	E-Method
after	O
each	O
stack	O
,	O
-	O
weight	O
decay	O
,	O
momentum	O
term	O
,	O
data	B-Method
preprocessing	E-Method
,	O
padding	O
,	O
and	O
cropping	O
were	O
as	O
in	O
previous	O
section	O
.	O

The	O
initial	O
learning	B-Metric
rate	E-Metric
was	O
set	O
to	O
0.01	O
and	O
decreased	O
by	O
a	O
factor	O
of	O
10	O
after	O
35k	O
iterations	O
.	O

The	O
mini	O
-	O
batch	O
size	O
was	O
100	O
.	O

For	O
the	O
final	O
50k	O
iterations	O
fine	O
-	O
tuning	O
we	O
increased	O
the	O
drop	B-Metric
-	I-Metric
out	I-Metric
rate	E-Metric
for	O
all	O
layers	O
in	O
a	O
stack	O
to	O
(	O
)	O
,	O
thereafter	O
increased	O
the	O
drop	B-Metric
-	I-Metric
out	I-Metric
rate	E-Metric
by	O
a	O
factor	O
of	O
1.5	O
for	O
40k	O
additional	O
iterations	O
.	O

ELU	B-Method
networks	E-Method
are	O
compared	O
to	O
following	O
recent	O
successful	O
CNN	S-Method
architectures	O
:	O
AlexNet	S-Method
,	O
DSN	S-Method
,	O
NiN	S-Method
,	O
Maxout	S-Method
,	O
All	O
-	O
CNN	S-Method
,	O
Highway	B-Method
Network	E-Method
and	O
Fractional	B-Method
Max	I-Method
-	I-Method
Pooling	E-Method
.	O

The	O
test	B-Metric
error	E-Metric
in	O
percent	B-Metric
misclassification	E-Metric
are	O
given	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

ELU	B-Method
-	I-Method
networks	E-Method
are	O
the	O
second	O
best	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
with	O
a	O
test	B-Metric
error	E-Metric
of	O
6.55	O
%	O
but	O
still	O
they	O
are	O
among	O
the	O
top	O
10	O
best	O
results	O
reported	O
for	O
CIFAR	B-Material
-	I-Material
10	E-Material
.	O

ELU	B-Method
networks	E-Method
performed	O
best	O
on	O
CIFAR	B-Material
-	I-Material
100	E-Material
with	O
a	O
test	B-Metric
error	E-Metric
of	O
24.28	O
%	O
.	O

This	O
is	O
the	O
best	O
published	O
result	O
on	O
CIFAR	B-Material
-	I-Material
100	E-Material
,	O
without	O
even	O
resorting	O
to	O
multi	B-Method
-	I-Method
view	I-Method
evaluation	E-Method
or	O
model	B-Method
averaging	E-Method
.	O

subsection	O
:	O
ImageNet	O
Challenge	O
Dataset	O
Finally	O
,	O
we	O
evaluated	O
ELU	B-Method
-	I-Method
networks	E-Method
on	O
the	O
1000	O
-	O
class	O
ImageNet	O
dataset	O
.	O

It	O
contains	O
about	O
1.3	O
M	O
training	O
color	O
images	O
as	O
well	O
as	O
additional	O
50k	O
images	O
and	O
100k	O
images	O
for	O
validation	O
and	O
testing	O
,	O
respectively	O
.	O

For	O
this	O
task	O
,	O
we	O
designed	O
a	O
15	O
layer	O
CNN	S-Method
,	O
which	O
was	O
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
or	O
fully	B-Method
-	I-Method
connected	E-Method
(	O
FC	S-Method
)	O
.	O

2	O
2	O
max	B-Method
-	I-Method
pooling	E-Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
and	O
spatial	B-Method
pyramid	I-Method
pooling	E-Method
(	O
SPP	S-Method
)	O
with	O
3	O
levels	O
before	O
the	O
first	O
FC	S-Method
layer	O
.	O

For	O
network	B-Task
regularization	E-Task
we	O
set	O
the	O
-	O
weight	O
decay	O
term	O
to	O
and	O
used	O
50	O
%	O
drop	O
-	O
out	O
in	O
the	O
two	O
penultimate	O
FC	S-Method
layers	O
.	O

Images	O
were	O
re	O
-	O
sized	O
to	O
256	O
256	O
pixels	O
and	O
per	O
-	O
pixel	O
mean	O
subtracted	O
.	O

Trained	O
was	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O

Besides	O
that	O
,	O
we	O
did	O
not	O
augment	O
the	O
dataset	O
during	O
training	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
learning	O
behavior	O
of	O
ELU	S-Method
vs.	O
ReLU	S-Method
networks	O
.	O

Panel	O
(	O
b	O
)	O
shows	O
that	O
ELUs	S-Method
start	O
reducing	O
the	O
error	S-Metric
earlier	O
.	O

The	O
ELU	B-Method
-	I-Method
network	E-Method
already	O
reaches	O
the	O
20	O
%	O
top	B-Metric
-	I-Metric
5	I-Metric
error	E-Metric
after	O
160k	O
iterations	O
,	O
while	O
the	O
ReLU	S-Method
network	O
needs	O
200k	O
iterations	O
to	O
reach	O
the	O
same	O
error	B-Metric
rate	E-Metric
.	O

The	O
single	O
-	O
model	O
performance	O
was	O
evaluated	O
on	O
the	O
single	O
center	B-Task
crop	E-Task
with	O
no	O
further	O
augmentation	O
and	O
yielded	O
a	O
top	B-Metric
-	I-Metric
5	I-Metric
validation	I-Metric
error	E-Metric
below	O
10	O
%	O
.	O

Currently	O
ELU	B-Method
nets	E-Method
are	O
5	O
%	O
slower	O
on	O
ImageNet	S-Method
than	O
ReLU	S-Method
nets	O
.	O

The	O
difference	O
is	O
small	O
because	O
activation	O
functions	O
generally	O
have	O
only	O
minor	O
influence	O
on	O
the	O
overall	O
training	B-Metric
time	E-Metric
.	O

In	O
terms	O
of	O
wall	B-Metric
clock	I-Metric
time	E-Metric
,	O
ELUs	S-Method
require	O
12.15h	O
vs.	O
ReLUs	S-Method
with	O
11.48h	O
for	O
10k	O
iterations	O
.	O

We	O
expect	O
that	O
ELU	B-Method
implementations	E-Method
can	O
be	O
improved	O
,	O
e.g.	O
by	O
faster	O
exponential	B-Method
functions	E-Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
the	O
exponential	B-Method
linear	I-Method
units	E-Method
(	O
ELUs	S-Method
)	O
for	O
faster	O
and	O
more	O
precise	O
learning	S-Task
in	O
deep	B-Task
neural	I-Task
networks	E-Task
.	O

ELUs	S-Method
have	O
negative	O
values	O
,	O
which	O
allows	O
the	O
network	O
to	O
push	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
.	O

Therefore	O
ELUs	S-Method
decrease	O
the	O
gap	O
between	O
the	O
normal	O
gradient	O
and	O
the	O
unit	O
natural	O
gradient	O
and	O
,	O
thereby	O
speed	O
up	O
learning	S-Task
.	O

We	O
believe	O
that	O
this	O
property	O
is	O
also	O
the	O
reason	O
for	O
the	O
success	O
of	O
activation	O
functions	O
like	O
LReLUs	S-Method
and	O
PReLUs	S-Method
and	O
of	O
batch	B-Method
normalization	E-Method
.	O

In	O
contrast	O
to	O
LReLUs	S-Method
and	O
PReLUs	S-Method
,	O
ELUs	S-Method
have	O
a	O
clear	O
saturation	O
plateau	O
in	O
its	O
negative	O
regime	O
,	O
allowing	O
them	O
to	O
learn	O
a	O
more	O
robust	O
and	O
stable	O
representation	O
.	O

Experimental	O
results	O
show	O
that	O
ELUs	S-Method
significantly	O
outperform	O
other	O
activation	B-Method
functions	E-Method
on	O
different	O
vision	O
datasets	O
.	O

Further	O
ELU	B-Method
networks	E-Method
perform	O
significantly	O
better	O
than	O
ReLU	S-Method
networks	O
trained	O
with	O
batch	B-Method
normalization	E-Method
.	O

ELU	B-Method
networks	E-Method
achieved	O
one	O
of	O
the	O
top	O
10	O
best	O
reported	O
results	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
set	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
CIFAR	B-Material
-	I-Material
100	E-Material
without	O
the	O
need	O
for	O
multi	B-Method
-	I-Method
view	I-Method
test	I-Method
evaluation	E-Method
or	O
model	B-Method
averaging	E-Method
.	O

Furthermore	O
,	O
ELU	B-Method
networks	E-Method
produced	O
competitive	O
results	O
on	O
the	O
ImageNet	O
in	O
much	O
fewer	O
epochs	O
than	O
a	O
corresponding	O
ReLU	S-Method
network	O
.	O

Given	O
their	O
outstanding	O
performance	O
,	O
we	O
expect	O
ELU	B-Method
networks	E-Method
to	O
become	O
a	O
real	O
time	O
saver	O
in	O
convolutional	B-Method
networks	E-Method
,	O
which	O
are	O
notably	O
time	O
-	O
intensive	O
to	O
train	O
from	O
scratch	O
otherwise	O
.	O

paragraph	O
:	O
Acknowledgment	O
.	O

We	O
thank	O
the	O
NVIDIA	O
Corporation	O
for	O
supporting	O
this	O
research	O
with	O
several	O
Titan	O
X	O
GPUs	O
and	O
Roland	O
Vollgraf	O
and	O
Martin	O
Heusel	O
for	O
helpful	O
discussions	O
and	O
comments	O
on	O
this	O
work	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Inverse	B-Method
of	I-Method
Block	I-Method
Matrices	E-Method
theorem	O
:	O
.	O

The	O
positive	O
definite	O
matrix	O
M	O
is	O
in	O
block	O
format	O
with	O
matrix	O
A	O
,	O
vector	O
b	O
,	O
and	O
scalar	O
c.	O
The	O
inverse	O
of	O
M	O
is	O
where	O
proof	O
:	O
Proof	O
.	O

For	O
block	O
matrices	O
the	O
inverse	O
is	O
where	O
the	O
matrices	O
on	O
the	O
right	O
hand	O
side	O
are	O
:	O
Further	O
if	O
follows	O
that	O
We	O
now	O
use	O
this	O
formula	O
for	O
being	O
a	O
vector	O
and	O
a	O
scalar	O
.	O

We	O
obtain	O
where	O
the	O
right	O
hand	O
side	O
matrices	O
,	O
vectors	O
,	O
and	O
the	O
scalar	O
are	O
:	O
Again	O
it	O
follows	O
that	O
A	O
reformulation	O
using	O
gives	O
∎	O
appendix	O
:	O
Quadratic	O
Form	O
of	O
Mean	O
and	O
Inverse	B-Method
Second	I-Method
Moment	E-Method
theorem	O
:	O
.	O

For	O
a	O
random	O
variable	O
a	O
holds	O
and	O
Furthermore	O
holds	O
proof	O
:	O
Proof	O
.	O

The	O
Sherman	B-Method
-	I-Method
Morrison	I-Method
Theorem	E-Method
states	O
Therefore	O
we	O
have	O
Using	O
the	O
identity	O
for	O
the	O
second	O
moment	O
and	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
we	O
get	O
The	O
last	O
inequality	O
follows	O
from	O
the	O
fact	O
that	O
is	O
positive	O
definite	O
.	O

From	O
last	O
equation	O
,	O
we	O
obtain	O
further	O
For	O
the	O
mixed	O
quadratic	O
form	O
we	O
get	O
from	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
From	O
this	O
equation	O
follows	O
Therefore	O
we	O
get	O
∎	O
appendix	O
:	O
Variance	O
of	O
Mean	O
Activations	O
in	O
ELU	S-Method
and	O
ReLU	S-Method
Networks	O
To	O
compare	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	S-Method
and	O
ELU	B-Method
networks	E-Method
,	O
we	O
trained	O
a	O
neural	B-Method
network	E-Method
with	O
5	O
hidden	O
layers	O
of	O
256	O
hidden	O
units	O
for	O
200	O
epochs	O
using	O
a	O
learning	O
rate	O
of	O
0.01	O
,	O
once	O
using	O
ReLU	S-Method
and	O
once	O
using	O
ELU	B-Method
activation	I-Method
functions	E-Method
on	O
the	O
MNIST	O
dataset	O
.	O

After	O
each	O
epoch	O
,	O
we	O
calculated	O
the	O
median	O
activation	O
of	O
each	O
hidden	B-Method
unit	E-Method
on	O
the	O
whole	O
training	O
set	O
.	O

We	O
then	O
calculated	O
the	O
variance	O
of	O
these	O
changes	O
,	O
which	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	S-Method
networks	O
.	O

This	O
indicates	O
that	O
ReLU	S-Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	B-Method
networks	E-Method
.	O

Conditional	B-Method
Random	I-Method
Fields	E-Method
as	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
section	O
:	O
Abstract	O
Pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
tasks	E-Task
,	O
such	O
as	O
semantic	B-Task
segmentation	E-Task
,	O
play	O
a	O
central	O
role	O
in	O
image	B-Task
understanding	E-Task
.	O

Recent	O
approaches	O
have	O
attempted	O
to	O
harness	O
the	O
capabilities	O
of	O
deep	B-Method
learning	I-Method
techniques	E-Method
for	O
image	B-Task
recognition	E-Task
to	O
tackle	O
pixellevel	B-Task
labelling	I-Task
tasks	E-Task
.	O

One	O
central	O
issue	O
in	O
this	O
methodology	O
is	O
the	O
limited	O
capacity	O
of	O
deep	B-Method
learning	I-Method
techniques	E-Method
to	O
delineate	O
visual	O
objects	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
introduce	O
a	O
new	O
form	O
of	O
convolutional	B-Method
neural	I-Method
network	E-Method
that	O
combines	O
the	O
strengths	O
of	O
Convolutional	B-Method
Neural	I-Method
Networks	E-Method
(	O
CNNs	S-Method
)	O
and	O
Conditional	B-Method
Random	I-Method
Fields	E-Method
(	O
CRFs	S-Method
)-	O
based	O
probabilistic	O
graphical	O
modelling	O
.	O

To	O
this	O
end	O
,	O
we	O
formulate	O
mean	B-Method
-	I-Method
field	I-Method
approximate	I-Method
inference	E-Method
for	O
the	O
Conditional	B-Method
Random	I-Method
Fields	E-Method
with	O
Gaussian	O
pairwise	O
potentials	O
as	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
.	O

This	O
network	O
,	O
called	O
CRF	B-Method
-	I-Method
RNN	E-Method
,	O
is	O
then	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
CNN	S-Method
to	O
obtain	O
a	O
deep	B-Method
network	E-Method
that	O
has	O
desirable	O
properties	O
of	O
both	O
CNNs	S-Method
and	O
CRFs	S-Method
.	O

Importantly	O
,	O
our	O
system	O
fully	O
integrates	O
CRF	S-Method
modelling	O
with	O
CNNs	S-Method
,	O
making	O
it	O
possible	O
to	O
train	O
the	O
whole	O
deep	B-Method
network	E-Method
end	O
-	O
to	O
-	O
end	O
with	O
the	O
usual	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
,	O
avoiding	O
offline	B-Method
post	I-Method
-	I-Method
processing	I-Method
methods	E-Method
for	O
object	B-Task
delineation	E-Task
.	O

We	O
apply	O
the	O
proposed	O
method	O
to	O
the	O
problem	O
of	O
semantic	B-Task
image	I-Task
segmentation	E-Task
,	O
obtaining	O
top	O
results	O
on	O
the	O
challenging	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
segmentation	I-Material
benchmark	E-Material
.	O

section	O
:	O
Introduction	O
Low	B-Task
-	I-Task
level	I-Task
computer	I-Task
vision	I-Task
problems	E-Task
such	O
as	O
semantic	B-Task
image	I-Task
segmentation	E-Task
or	O
depth	B-Task
estimation	E-Task
often	O
involve	O
assigning	O
a	O
label	O
to	O
each	O
pixel	O
in	O
an	O
image	O
.	O

While	O
the	O
feature	B-Method
representation	E-Method
used	O
to	O
classify	O
individual	O
pixels	O
plays	O
an	O
important	O
role	O
in	O
this	O
task	O
,	O
it	O
is	O
similarly	O
important	O
to	O
consider	O
factors	O
such	O
as	O
image	O
edges	O
,	O
appearance	O
consistency	O
and	O
spatial	O
consistency	O
while	O
assigning	O
labels	O
in	O
order	O
to	O
obtain	O
accurate	O
and	O
precise	O
results	O
.	O

Designing	O
a	O
strong	B-Method
feature	I-Method
representation	E-Method
is	O
a	O
key	O
chal	O
-	O
*	O
Authors	O
contributed	O
equally	O
.	O

†	O
Work	O
conducted	O
while	O
authors	O
at	O
the	O
University	O
of	O
Oxford	O
.	O

lenge	O
in	O
pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
problems	E-Task
.	O

Work	O
on	O
this	O
topic	O
includes	O
:	O
TextonBoost	S-Method
[	O
reference	O
]	O
,	O
TextonForest	S-Method
[	O
reference	O
]	O
,	O
and	O
Random	B-Method
Forest	I-Method
-	I-Method
based	I-Method
classifiers	E-Method
[	O
reference	O
]	O
.	O

Recently	O
,	O
supervised	B-Method
deep	I-Method
learning	I-Method
approaches	E-Method
such	O
as	O
large	B-Method
-	I-Method
scale	I-Method
deep	I-Method
Convolutional	I-Method
Neural	I-Method
Networks	E-Method
(	O
CNNs	S-Method
)	O
have	O
been	O
immensely	O
successful	O
in	O
many	O
high	O
-	O
level	B-Task
computer	I-Task
vision	I-Task
tasks	E-Task
such	O
as	O
image	B-Task
recognition	E-Task
[	O
reference	O
]	O
and	O
object	B-Task
detection	E-Task
[	O
reference	O
]	O
.	O

This	O
motivates	O
exploring	O
the	O
use	O
of	O
CNNs	S-Method
for	O
pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
problems	E-Task
.	O

The	O
key	O
insight	O
is	O
to	O
learn	O
a	O
strong	O
feature	B-Method
representation	E-Method
end	O
-	O
to	O
-	O
end	O
for	O
the	O
pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
task	E-Task
instead	O
of	O
hand	O
-	O
crafting	O
features	S-Method
with	O
heuristic	B-Method
parameter	I-Method
tuning	E-Method
.	O

In	O
fact	O
,	O
a	O
number	O
of	O
recent	O
approaches	O
including	O
the	O
particularly	O
interesting	O
works	O
FCN	S-Method
[	O
reference	O
]	O
and	O
DeepLab	S-Method
[	O
reference	O
]	O
have	O
shown	O
a	O
significant	O
accuracy	S-Metric
boost	O
by	O
adapting	O
stateof	O
-	O
the	O
-	O
art	O
CNN	S-Method
based	O
image	O
classifiers	O
to	O
the	O
semantic	B-Task
segmentation	E-Task
problem	O
.	O

However	O
,	O
there	O
are	O
significant	O
challenges	O
in	O
adapting	O
CNNs	S-Method
designed	O
for	O
high	B-Task
level	I-Task
computer	I-Task
vision	I-Task
tasks	E-Task
such	O
as	O
object	B-Task
recognition	E-Task
to	O
pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
tasks	E-Task
.	O

Firstly	O
,	O
traditional	O
CNNs	S-Method
have	O
convolutional	B-Method
filters	E-Method
with	O
large	O
receptive	O
fields	O
and	O
hence	O
produce	O
coarse	O
outputs	O
when	O
restructured	O
to	O
produce	O
pixel	O
-	O
level	O
labels	O
[	O
reference	O
]	O
.	O

Presence	O
of	O
maxpooling	B-Method
layers	E-Method
in	O
CNNs	S-Method
further	O
reduces	O
the	O
chance	O
of	O
getting	O
a	O
fine	O
segmentation	O
output	O
[	O
reference	O
]	O
.	O

This	O
,	O
for	O
instance	O
,	O
can	O
result	O
in	O
non	O
-	O
sharp	O
boundaries	O
and	O
blob	O
-	O
like	O
shapes	O
in	O
semantic	B-Task
segmentation	E-Task
tasks	O
.	O

Secondly	O
,	O
CNNs	S-Method
lack	O
smoothness	O
constraints	O
that	O
encourage	O
label	O
agreement	O
between	O
similar	O
pixels	O
,	O
and	O
spatial	O
and	O
appearance	O
consistency	O
of	O
the	O
labelling	O
output	O
.	O

Lack	O
of	O
such	O
smoothness	O
constraints	O
can	O
result	O
in	O
poor	O
object	B-Task
delineation	E-Task
and	O
small	O
spurious	O
regions	O
in	O
the	O
segmentation	O
output	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

On	O
a	O
separate	O
track	O
to	O
the	O
progress	O
of	O
deep	B-Method
learning	I-Method
techniques	E-Method
,	O
probabilistic	B-Method
graphical	I-Method
models	E-Method
have	O
been	O
developed	O
as	O
effective	O
methods	O
to	O
enhance	O
the	O
accuracy	S-Metric
of	O
pixellevel	B-Task
labelling	I-Task
tasks	E-Task
.	O

In	O
particular	O
,	O
Markov	B-Method
Random	I-Method
Fields	E-Method
(	O
MRFs	B-Method
)	E-Method
and	O
its	O
variant	O
Conditional	B-Method
Random	I-Method
Fields	E-Method
(	O
CRFs	S-Method
)	O
have	O
observed	O
widespread	O
success	O
in	O
this	O
area	O
[	O
reference	O
][	O
reference	O
]	O
and	O
have	O
become	O
one	O
of	O
the	O
most	O
successful	O
graphical	B-Method
models	E-Method
used	O
in	O
computer	B-Task
vision	E-Task
.	O

The	O
key	O
idea	O
of	O
CRF	S-Method
inference	O
for	O
semantic	B-Task
labelling	E-Task
is	O
to	O
formulate	O
the	O
label	B-Task
assignment	I-Task
problem	E-Task
as	O
a	O
probabilistic	B-Task
inference	I-Task
problem	E-Task
that	O
incorporates	O
assumptions	O
such	O
as	O
the	O
label	O
agreement	O
between	O
similar	O
pixels	O
.	O

CRF	S-Method
inference	O
is	O
able	O
to	O
refine	O
weak	O
and	O
coarse	O
pixel	O
-	O
level	O
label	O
predictions	O
to	O
produce	O
sharp	O
boundaries	O
and	O
fine	B-Task
-	I-Task
grained	I-Task
segmentations	E-Task
.	O

Therefore	O
,	O
intuitively	O
,	O
CRFs	S-Method
can	O
be	O
used	O
to	O
overcome	O
the	O
drawbacks	O
in	O
utilizing	O
CNNs	S-Method
for	O
pixel	B-Task
-	I-Task
level	I-Task
labelling	I-Task
tasks	E-Task
.	O

One	O
way	O
to	O
utilize	O
CRFs	S-Method
to	O
improve	O
the	O
semantic	B-Task
labelling	I-Task
results	E-Task
produced	O
by	O
a	O
CNN	S-Method
is	O
to	O
apply	O
CRF	S-Method
inference	O
as	O
a	O
post	B-Method
-	I-Method
processing	I-Method
step	E-Method
disconnected	O
from	O
the	O
training	O
of	O
the	O
CNN	S-Method
[	O
reference	O
]	O
.	O

Arguably	O
,	O
this	O
does	O
not	O
fully	O
harness	O
the	O
strength	O
of	O
CRFs	S-Method
since	O
it	O
is	O
not	O
integrated	O
with	O
the	O
deep	B-Method
network	E-Method
.	O

In	O
this	O
setup	O
,	O
the	O
deep	B-Method
network	E-Method
is	O
unaware	O
of	O
the	O
CRF	S-Method
during	O
the	O
training	O
phase	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
deep	I-Method
learning	I-Method
solution	E-Method
for	O
the	O
pixel	B-Task
-	I-Task
level	I-Task
semantic	I-Task
image	I-Task
segmentation	I-Task
problem	E-Task
.	O

Our	O
formulation	O
combines	O
the	O
strengths	O
of	O
both	O
CNNs	O
and	O
CRF	S-Method
based	O
graphical	O
models	O
in	O
one	O
unified	O
framework	O
.	O

More	O
specifically	O
,	O
we	O
formulate	O
mean	B-Method
-	I-Method
field	I-Method
approximate	I-Method
inference	E-Method
for	O
the	O
dense	O
CRF	S-Method
with	O
Gaussian	B-Method
pairwise	I-Method
potentials	E-Method
as	O
a	O
Recurrent	B-Method
Neural	I-Method
Network	E-Method
(	O
RNN	S-Method
)	O
which	O
can	O
refine	O
coarse	O
outputs	O
from	O
a	O
traditional	O
CNN	S-Method
in	O
the	O
forward	O
pass	O
,	O
while	O
passing	O
error	O
differentials	O
back	O
to	O
the	O
CNN	S-Method
during	O
training	O
.	O

Importantly	O
,	O
with	O
our	O
formulation	O
,	O
the	O
whole	O
deep	B-Method
network	E-Method
,	O
which	O
comprises	O
a	O
traditional	O
CNN	S-Method
and	O
an	O
RNN	S-Method
for	O
CRF	S-Method
inference	O
,	O
can	O
be	O
trained	O
endto	O
-	O
end	O
utilizing	O
the	O
usual	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
.	O

Arguably	O
,	O
when	O
properly	O
trained	O
,	O
the	O
proposed	O
network	O
should	O
outperform	O
a	O
system	O
where	O
CRF	S-Method
inference	O
is	O
applied	O
as	O
a	O
post	B-Method
-	I-Method
processing	I-Method
method	E-Method
on	O
independent	O
pixel	O
-	O
level	O
predictions	O
produced	O
by	O
a	O
pre	O
-	O
trained	O
CNN	S-Method
.	O

Our	O
experimental	O
evaluation	O
confirms	O
that	O
this	O
indeed	O
is	O
the	O
case	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
our	O
network	O
on	O
the	O
popular	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
benchmark	E-Material
,	O
achieving	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	S-Metric
of	O
74.7	O
%	O
.	O

Our	O
source	O
code	O
and	O
models	O
are	O
publicly	O
available	O
1	O
.	O

section	O
:	O
Related	O
Work	O
In	O
this	O
section	O
we	O
review	O
approaches	O
that	O
make	O
use	O
of	O
deep	B-Method
learning	E-Method
and	O
CNNs	S-Method
for	O
low	B-Task
-	I-Task
level	I-Task
computer	I-Task
vision	I-Task
tasks	E-Task
,	O
with	O
a	O
focus	O
on	O
semantic	B-Task
image	I-Task
segmentation	E-Task
.	O

A	O
wide	O
variety	O
of	O
approaches	O
have	O
been	O
proposed	O
to	O
tackle	O
the	O
semantic	B-Task
image	I-Task
segmentation	I-Task
task	E-Task
using	O
deep	B-Method
learning	E-Method
.	O

These	O
approaches	O
can	O
be	O
categorized	O
into	O
two	O
main	O
strategies	O
.	O

The	O
first	O
strategy	O
is	O
based	O
on	O
utilizing	O
separate	O
mechanisms	O
for	O
feature	B-Task
extraction	E-Task
,	O
and	O
image	B-Task
segmentation	E-Task
exploiting	O
the	O
edges	O
of	O
the	O
image	O
[	O
reference	O
][	O
reference	O
]	O
.	O

One	O
representative	O
instance	O
of	O
this	O
scheme	O
is	O
the	O
application	O
of	O
a	O
CNN	S-Method
for	O
the	O
extraction	B-Task
of	I-Task
meaningful	I-Task
features	E-Task
,	O
and	O
using	O
superpixels	S-Method
to	O
account	O
for	O
the	O
structural	O
pattern	O
of	O
the	O
image	O
.	O

Two	O
representative	O
examples	O
are	O
[	O
reference	O
][	O
reference	O
]	O
,	O
where	O
the	O
authors	O
first	O
ob	O
-	O
tained	O
superpixels	O
from	O
the	O
image	O
and	O
then	O
used	O
a	O
feature	B-Method
extraction	I-Method
process	E-Method
on	O
each	O
of	O
them	O
.	O

The	O
main	O
disadvantage	O
of	O
this	O
strategy	O
is	O
that	O
errors	O
in	O
the	O
initial	O
proposals	O
(	O
e.g	O
:	O
super	O
-	O
pixels	O
)	O
may	O
lead	O
to	O
poor	O
predictions	O
,	O
no	O
matter	O
how	O
good	O
the	O
feature	B-Method
extraction	I-Method
process	E-Method
is	O
.	O

Pinheiro	O
and	O
Collobert	O
[	O
reference	O
]	O
employed	O
an	O
RNN	S-Method
to	O
model	O
the	O
spatial	O
dependencies	O
during	O
scene	B-Task
parsing	E-Task
.	O

In	O
contrast	O
to	O
their	O
approach	O
,	O
we	O
show	O
that	O
a	O
typical	O
graphical	B-Method
model	E-Method
such	O
as	O
a	O
CRF	S-Method
can	O
be	O
formulated	O
as	O
an	O
RNN	S-Method
to	O
form	O
a	O
part	O
of	O
a	O
deep	B-Method
network	E-Method
,	O
to	O
perform	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
training	E-Task
combined	O
with	O
a	O
CNN	S-Method
.	O

The	O
second	O
strategy	O
is	O
to	O
directly	O
learn	O
a	O
nonlinear	B-Method
model	E-Method
from	O
the	O
images	O
to	O
the	O
label	O
map	O
.	O

This	O
,	O
for	O
example	O
,	O
was	O
shown	O
in	O
[	O
reference	O
]	O
,	O
where	O
the	O
authors	O
replaced	O
the	O
last	O
fully	O
connected	O
layers	O
of	O
a	O
CNN	S-Method
by	O
convolutional	B-Method
layers	E-Method
to	O
keep	O
spatial	O
information	O
.	O

An	O
important	O
contribution	O
in	O
this	O
direction	O
is	O
[	O
reference	O
]	O
,	O
where	O
Long	O
et	O
al	O
.	O

used	O
the	O
concept	O
of	O
fully	B-Method
convolutional	I-Method
networks	E-Method
,	O
and	O
the	O
notion	O
that	O
top	O
layers	O
obtain	O
meaningful	O
features	O
for	O
object	B-Task
recognition	E-Task
whereas	O
low	O
layers	O
keep	O
information	O
about	O
the	O
structure	O
of	O
the	O
image	O
,	O
such	O
as	O
edges	O
.	O

In	O
their	O
work	O
,	O
connections	O
from	O
early	O
layers	O
to	O
later	O
layers	O
were	O
used	O
to	O
combine	O
these	O
cues	O
.	O

Bell	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
][	O
reference	O
]	O
used	O
a	O
CRF	S-Method
to	O
refine	O
segmentation	S-Task
results	O
obtained	O
from	O
a	O
CNN	S-Method
.	O

Bell	O
et	O
al	O
.	O

focused	O
on	O
material	B-Task
recognition	E-Task
and	O
segmentation	S-Task
,	O
whereas	O
Chen	O
et	O
al	O
.	O

reported	O
very	O
significant	O
improvements	O
on	O
semantic	B-Task
image	I-Task
segmentation	E-Task
.	O

In	O
contrast	O
to	O
these	O
works	O
,	O
which	O
employed	O
CRF	S-Method
inference	O
as	O
a	O
standalone	B-Task
post	I-Task
-	I-Task
processing	I-Task
step	E-Task
disconnected	O
from	O
the	O
CNN	S-Method
training	O
,	O
our	O
approach	O
is	O
an	O
end	O
-	O
to	O
-	O
end	B-Method
trainable	I-Method
network	E-Method
that	O
jointly	O
learns	O
the	O
parameters	O
of	O
the	O
CNN	S-Method
and	O
the	O
CRF	S-Method
in	O
one	O
unified	B-Method
deep	I-Method
network	E-Method
.	O

Works	O
that	O
use	O
neural	B-Method
networks	E-Method
to	O
predict	O
structured	O
output	O
are	O
found	O
in	O
different	O
domains	O
.	O

For	O
example	O
,	O
Do	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
an	O
approach	O
to	O
combine	O
deep	B-Method
neural	I-Method
networks	E-Method
and	O
Markov	B-Method
networks	E-Method
for	O
sequence	B-Task
labeling	I-Task
tasks	E-Task
.	O

Jain	O
et	O
al	O
.	O

[	O
reference	O
]	O
has	O
shown	O
Convolutional	B-Method
Neural	I-Method
Networks	E-Method
can	O
perform	O
well	O
like	O
MRFs	O
/	O
CRFs	S-Method
approaches	O
in	O
image	B-Task
restoration	I-Task
application	E-Task
.	O

Another	O
domain	O
which	O
benefits	O
from	O
the	O
combination	O
of	O
CNNs	S-Method
and	O
structured	B-Method
loss	E-Method
is	O
handwriting	B-Task
recognition	E-Task
.	O

In	O
natural	B-Task
language	I-Task
processing	E-Task
,	O
Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
shows	O
that	O
the	O
performance	O
of	O
an	O
RNN	S-Method
-	O
based	O
words	O
tagger	O
can	O
be	O
significantly	O
improved	O
by	O
incorporating	O
elements	O
of	O
the	O
CRF	S-Method
model	O
.	O

In	O
[	O
reference	O
]	O
,	O
the	O
authors	O
combined	O
a	O
CNN	S-Method
with	O
Hidden	B-Method
Markov	I-Method
Models	E-Method
for	O
that	O
purpose	O
,	O
whereas	O
more	O
recently	O
,	O
Peng	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
a	O
modified	O
version	O
of	O
CRFs	S-Method
.	O

Related	O
to	O
this	O
line	O
of	O
works	O
,	O
in	O
[	O
reference	O
]	O
a	O
joint	O
CNN	S-Method
and	O
CRF	S-Method
model	O
was	O
used	O
for	O
text	B-Task
recognition	E-Task
on	O
natural	O
images	O
.	O

Tompson	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
the	O
use	O
of	O
joint	B-Method
training	E-Method
of	O
a	O
CNN	S-Method
and	O
an	O
MRF	S-Method
for	O
human	B-Task
pose	I-Task
estimation	E-Task
,	O
while	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
focused	O
on	O
the	O
image	B-Task
classification	I-Task
problem	E-Task
with	O
a	O
similar	O
approach	O
.	O

Another	O
prominent	O
work	O
is	O
[	O
reference	O
]	O
,	O
in	O
which	O
the	O
authors	O
express	O
deformable	B-Method
part	I-Method
models	E-Method
,	O
a	O
kind	O
of	O
MRF	S-Method
,	O
as	O
a	O
layer	O
in	O
a	O
neural	B-Method
network	E-Method
.	O

In	O
our	O
approach	O
,	O
we	O
cast	O
a	O
different	O
graphical	B-Method
model	E-Method
as	O
a	O
neural	B-Method
network	I-Method
layer	E-Method
.	O

A	O
number	O
of	O
approaches	O
have	O
been	O
proposed	O
for	O
automatic	B-Task
learning	I-Task
of	I-Task
graphical	I-Task
model	I-Task
parameters	E-Task
and	O
joint	B-Method
training	I-Method
of	I-Method
classifiers	E-Method
and	O
graphical	B-Method
models	E-Method
.	O

Barbu	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
joint	B-Method
training	E-Method
of	O
a	O
MRF	O
/	O
CRF	S-Method
model	O
together	O
with	O
an	O
inference	B-Method
algorithm	E-Method
in	O
their	O
Active	B-Method
Random	I-Method
Field	I-Method
approach	E-Method
.	O

Domke	O
[	O
reference	O
]	O
advocated	O
back	B-Method
-	I-Method
propagation	I-Method
based	I-Method
parameter	I-Method
optimization	E-Method
in	O
graphical	B-Method
models	E-Method
when	O
approximate	B-Method
inference	I-Method
methods	E-Method
such	O
as	O
mean	B-Method
-	I-Method
field	E-Method
and	O
belief	B-Method
propagation	E-Method
are	O
used	O
.	O

This	O
idea	O
was	O
utilized	O
in	O
[	O
reference	O
]	O
,	O
where	O
a	O
binary	O
dense	O
CRF	S-Method
was	O
used	O
for	O
human	B-Task
pose	I-Task
estimation	E-Task
.	O

Similarly	O
,	O
Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Stoyanov	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
how	O
back	B-Method
-	I-Method
propagation	E-Method
through	O
belief	B-Method
propagation	E-Method
can	O
be	O
used	O
to	O
optimize	O
model	O
parameters	O
.	O

Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
in	O
particular	O
proposes	O
an	O
approach	O
based	O
on	O
learning	B-Task
messages	E-Task
.	O

Many	O
of	O
these	O
ideas	O
can	O
be	O
traced	O
back	O
to	O
[	O
reference	O
]	O
,	O
which	O
proposes	O
unrolling	B-Method
message	I-Method
passing	I-Method
algorithms	E-Method
as	O
simpler	O
operations	O
that	O
could	O
be	O
performed	O
within	O
a	O
CNN	S-Method
.	O

In	O
a	O
different	O
setup	O
,	O
Krähenbühl	O
and	O
Koltun	O
[	O
reference	O
]	O
demonstrated	O
automatic	B-Method
parameter	I-Method
tuning	E-Method
of	O
dense	O
CRF	S-Method
when	O
a	O
modified	O
mean	B-Method
-	I-Method
field	I-Method
algorithm	E-Method
is	O
used	O
for	O
inference	S-Task
.	O

An	O
alternative	O
inference	B-Method
approach	E-Method
for	O
dense	O
CRF	S-Method
,	O
not	O
based	O
on	O
mean	B-Method
-	I-Method
field	E-Method
,	O
is	O
proposed	O
in	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
to	O
the	O
works	O
described	O
above	O
,	O
our	O
approach	O
shows	O
that	O
it	O
is	O
possible	O
to	O
formulate	O
dense	O
CRF	S-Method
as	O
an	O
RNN	S-Method
so	O
that	O
one	O
can	O
form	O
an	O
end	O
-	O
to	O
-	O
end	B-Method
trainable	I-Method
system	E-Method
for	O
semantic	B-Task
image	I-Task
segmentation	E-Task
which	O
combines	O
the	O
strengths	O
of	O
deep	B-Method
learning	E-Method
and	O
graphical	B-Method
modelling	E-Method
.	O

After	O
our	O
initial	O
publication	O
of	O
the	O
technical	O
report	O
of	O
this	O
work	O
on	O
arXiv.org	O
,	O
a	O
number	O
of	O
independent	O
works	O
[	O
reference	O
][	O
reference	O
]	O
appeared	O
on	O
arXiv.org	O
presenting	O
similar	O
joint	B-Method
training	I-Method
approaches	E-Method
for	O
semantic	B-Task
image	I-Task
segmentation	E-Task
.	O

section	O
:	O
Conditional	B-Method
Random	I-Method
Fields	E-Method
In	O
this	O
section	O
we	O
provide	O
a	O
brief	O
overview	O
of	O
Conditional	B-Method
Random	I-Method
Fields	E-Method
(	O
CRF	S-Method
)	O
for	O
pixel	B-Task
-	I-Task
wise	I-Task
labelling	E-Task
and	O
introduce	O
the	O
notation	O
used	O
in	O
the	O
paper	O
.	O

A	O
CRF	S-Method
,	O
used	O
in	O
the	O
context	O
of	O
pixel	B-Task
-	I-Task
wise	I-Task
label	I-Task
prediction	E-Task
,	O
models	O
pixel	O
labels	O
as	O
random	O
variables	O
that	O
form	O
a	O
Markov	B-Method
Random	I-Method
Field	E-Method
(	O
MRF	S-Method
)	O
when	O
conditioned	O
upon	O
a	O
global	O
observation	O
.	O

The	O
global	B-Task
observation	E-Task






is	O
usually	O
taken	O
to	O
be	O
the	O
image	O
.	O

Let	O
X	O
i	O
be	O
the	O
random	O
variable	O
associated	O
to	O
pixel	O
i	O
,	O
which	O
represents	O
the	O
label	O
assigned	O
to	O
the	O
pixel	O
i	O
and	O
can	O
take	O
any	O
value	O
from	O
a	O
pre	O
-	O
defined	O
set	O
of	O
labels	O
L	O
=	O
{	O
l	O
1	O
,	O
l	O
2	O
,	O
.	O

.	O


.	O


,	O
l	O
L	O
}	O
.	O

Let	O
X	O
be	O
the	O
vector	O
formed	O
by	O
the	O
random	O
variables	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O


.	O


,	O
X	O
N	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
in	O
the	O
image	O
.	O

Given	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
=	O
{	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O


.	O


,	O
X	O
N	O
}	O
,	O
and	O
a	O
global	O
observation	O
(	O
image	O
)	O
I	O
,	O
the	O
pair	O
(	O
I	O
,	O
X	O
)	O
can	O
be	O
modelled	O
as	O
a	O
CRF	S-Method
characterized	O
by	O
a	O
Gibbs	B-Method
distribution	E-Method
of	O
the	O
form	O
P	O
(	O
X	O
=	O
x|I	O
)	O
=	O
tion	O
[	O
reference	O
]	O
.	O

From	O
now	O
on	O
,	O
we	O
drop	O
the	O
conditioning	O
on	O
I	O
in	O
the	O
notation	O
for	O
convenience	O
.	O

In	O
the	O
fully	O
connected	O
pairwise	O
CRF	S-Method
model	O
of	O
[	O
reference	O
]	O
,	O
the	O
energy	O
of	O
a	O
label	O
assignment	O
x	O
is	O
given	O
by	O
:	O
where	O
the	O
unary	O
energy	O
components	O
ψ	O
u	O
(	O
x	O
i	O
)	O
measure	O
the	O
inverse	O
likelihood	O
(	O
and	O
therefore	O
,	O
the	O
cost	O
)	O
of	O
the	O
pixel	O
i	O
taking	O
the	O
label	O
x	O
i	O
,	O
and	O
pairwise	O
energy	O
components	O
ψ	O
p	O
(	O
x	O
i	O
,	O
x	O
j	O
)	O
measure	O
the	O
cost	O
of	O
assigning	O
labels	O
x	O
i	O
,	O
x	O
j	O
to	O
pixels	O
i	O
,	O
j	O
simultaneously	O
.	O

In	O
our	O
model	O
,	O
unary	O
energies	O
are	O
obtained	O
from	O
a	O
CNN	S-Method
,	O
which	O
,	O
roughly	O
speaking	O
,	O
predicts	O
labels	O
for	O
pixels	O
without	O
considering	O
the	O
smoothness	O
and	O
the	O
consistency	O
of	O
the	O
label	O
assignments	O
.	O

The	O
pairwise	O
energies	O
provide	O
an	O
image	B-Method
data	I-Method
-	I-Method
dependent	I-Method
smoothing	I-Method
term	E-Method
that	O
encourages	O
assigning	O
similar	O
labels	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

As	O
was	O
done	O
in	O
[	O
reference	O
]	O
,	O
we	O
model	O
pairwise	O
potentials	O
as	O
weighted	B-Method
Gaussians	E-Method
:	O
where	O
each	O
k	O
,	O
is	O
a	O
Gaussian	B-Method
kernel	E-Method
applied	O
on	O
feature	O
vectors	O
.	O

The	O
feature	O
vector	O
of	O
pixel	O
i	O
,	O
denoted	O
by	O
f	O
i	O
,	O
is	O
derived	O
from	O
image	O
features	O
such	O
as	O
spatial	O
location	O
and	O
RGB	O
values	O
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
same	O
features	O
as	O
in	O
[	O
reference	O
]	O
.	O

The	O
function	O
µ	O
(	O
.	O

,	O
.	O

)	O
,	O
called	O
the	O
label	O
compatibility	O
function	O
,	O
captures	O
the	O
compatibility	O
between	O
different	O
pairs	O
of	O
labels	O
as	O
the	O
name	O
implies	O
.	O

Minimizing	O
the	O
above	O
CRF	S-Method
energy	O
E	O
(	O
x	O
)	O
yields	O
the	O
most	O
probable	O
label	O
assignment	O
x	O
for	O
the	O
given	O
image	O
.	O

Since	O
this	O
exact	B-Task
minimization	E-Task
is	O
intractable	O
,	O
a	O
mean	B-Method
-	I-Method
field	I-Method
approximation	E-Method
to	O
the	O
CRF	S-Method
distribution	O
is	O
used	O
for	O
approximate	B-Task
maximum	I-Task
posterior	I-Task
marginal	I-Task
inference	E-Task
.	O

It	O
consists	O
in	O
approximating	O
the	O
CRF	S-Method
distribution	O
P	O
(	O
X	O
)	O
by	O
a	O
simpler	O
distribution	B-Method
Q	I-Method
(	I-Method
X	I-Method
)	E-Method
,	O
which	O
can	O
be	O
written	O
as	O
the	O
product	O
of	O
independent	O
marginal	O
distributions	O
,	O
i.e.	O
,	O
Q	O
(	O
X	O
)	O
=	O
i	O
Q	O
i	O
(	O
X	O
i	O
)	O
.	O

The	O
steps	O
of	O
the	O
iterative	B-Method
algorithm	E-Method
for	O
approximate	B-Task
mean	I-Task
-	I-Task
field	I-Task
inference	E-Task
and	O
its	O
reformulation	O
as	O
an	O
RNN	S-Method
are	O
discussed	O
next	O
.	O

[	O
reference	O
]	O
,	O
broken	O
down	O
to	O
common	O
CNN	S-Method
operations	O
.	O

section	O
:	O
Algorithm	O
1	O
Mean	B-Method
-	I-Method
field	E-Method
in	O
dense	O
CRFs	S-Method
Adding	O
Unary	O
Potentials	O
Normalizing	O
end	O
while	O
section	O
:	O
A	O
Mean	B-Method
-	I-Method
field	I-Method
Iteration	E-Method
as	O
a	O
Stack	O
of	O
CNN	S-Method
Layers	O
A	O
key	O
contribution	O
of	O
this	O
paper	O
is	O
to	O
show	O
that	O
the	O
meanfield	O
CRF	S-Method
inference	O
can	O
be	O
reformulated	O
as	O
a	O
Recurrent	B-Method
Neural	I-Method
Network	E-Method
(	O
RNN	S-Method
)	O
.	O

To	O
this	O
end	O
,	O
we	O
first	O
consider	O
individual	O
steps	O
of	O
the	O
mean	B-Method
-	I-Method
field	I-Method
algorithm	E-Method
summarized	O
in	O
Algorithm	O
1	O
[	O
reference	O
]	O
,	O
and	O
describe	O
them	O
as	O
CNN	S-Method
layers	O
.	O

Our	O
contribution	O
is	O
based	O
on	O
the	O
observation	O
that	O
filter	B-Method
-	I-Method
based	I-Method
approximate	I-Method
mean	I-Method
-	I-Method
field	I-Method
inference	I-Method
approach	E-Method
for	O
dense	O
CRFs	S-Method
relies	O
on	O
applying	O
Gaussian	B-Method
spatial	I-Method
and	I-Method
bilateral	I-Method
filters	E-Method
on	O
the	O
mean	B-Method
-	I-Method
field	I-Method
approximates	E-Method
in	O
each	O
iteration	O
.	O

Unlike	O
the	O
standard	O
convolutional	B-Method
layer	E-Method
in	O
a	O
CNN	S-Method
,	O
in	O
which	O
filters	O
are	O
fixed	O
after	O
the	O
training	O
stage	O
,	O
we	O
use	O
edge	B-Method
-	I-Method
preserving	I-Method
Gaussian	I-Method
filters	E-Method
[	O
reference	O
][	O
reference	O
]	O
,	O
coefficients	O
of	O
which	O
depend	O
on	O
the	O
original	O
spatial	O
and	O
appearance	O
information	O
of	O
the	O
image	O
.	O

These	O
filters	O
have	O
the	O
additional	O
advantages	O
of	O
requiring	O
a	O
smaller	O
set	O
of	O
parameters	O
,	O
despite	O
the	O
filter	O
size	O
being	O
potentially	O
as	O
big	O
as	O
the	O
image	O
.	O

While	O
reformulating	O
the	O
steps	O
of	O
the	O
inference	B-Method
algorithm	E-Method
as	O
CNN	S-Method
layers	O
,	O
it	O
is	O
essential	O
to	O
be	O
able	O
to	O
calculate	O
error	O
differentials	O
in	O
each	O
layer	O
w.r.t	O
.	O

its	O
inputs	O
in	O
order	O
to	O
be	O
able	O
to	O
back	O
-	O
propagate	O
the	O
error	O
differentials	O
to	O
previous	O
layers	O
during	O
training	O
.	O

We	O
also	O
discuss	O
how	O
to	O
calculate	O
error	O
differentials	O
with	O
respect	O
to	O
the	O
parameters	O
in	O
each	O
layer	O
,	O
enabling	O
their	O
optimization	S-Task
through	O
the	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
.	O

Therefore	O
,	O
in	O
our	O
formulation	O
,	O
CRF	S-Method
parameters	O
such	O
as	O
the	O
weights	O
of	O
the	O
Gaussian	O
kernels	O
and	O
the	O
label	O
compatibility	O
function	O
can	O
also	O
be	O
optimized	O
automatically	O
during	O
the	O
training	O
of	O
the	O
full	B-Method
network	E-Method
.	O

Once	O
the	O
individual	O
steps	O
of	O
the	O
algorithm	O
are	O
broken	O
down	O
as	O
CNN	S-Method
layers	O
,	O
the	O
full	B-Method
algorithm	E-Method
can	O
then	O
be	O
formulated	O
as	O
an	O
RNN	S-Method
.	O

We	O
explain	O
this	O
in	O
Section	O
5	O
after	O
discussing	O
the	O
steps	O
of	O
Algorithm	O
1	O
in	O
detail	O
below	O
.	O

In	O
Algorithm	O
1	O
and	O
the	O
remainder	O
of	O
this	O
paper	O
,	O
we	O
use	O
U	O
i	O
(	O
l	O
)	O
to	O
denote	O
the	O
negative	O
of	O
the	O
unary	O
energy	O
introduced	O
in	O
the	O
previous	O
section	O
,	O
i.e.	O
,	O
U	O
i	O
(	O
l	O
)	O
=	O
−ψ	O
u	O
(	O
X	O
i	O
=	O
l	O
)	O
.	O

In	O
the	O
conventional	O
CRF	S-Method
setting	O
,	O
this	O
input	O
U	O
i	O
(	O
l	O
)	O
to	O
the	O
mean	B-Method
-	I-Method
field	I-Method
algorithm	E-Method
is	O
obtained	O
from	O
an	O
independent	B-Method
classifier	E-Method
.	O

section	O
:	O
Initialization	O
In	O
the	O
initialization	O
step	O
of	O
the	O
algorithm	O
,	O
the	O
operation	O
,	O
is	O
performed	O
.	O

Note	O
that	O
this	O
is	O
equivalent	O
to	O
applying	O
a	O
softmax	O
function	O
over	O
the	O
unary	O
potentials	O
U	O
across	O
all	O
the	O
labels	O
at	O
each	O
pixel	O
.	O

The	O
softmax	B-Method
function	E-Method
has	O
been	O
extensively	O
used	O
in	O
CNN	S-Method
architectures	O
before	O
and	O
is	O
therefore	O
well	O
known	O
in	O
the	O
deep	B-Task
learning	I-Task
community	E-Task
.	O

This	O
operation	O
does	O
not	O
include	O
any	O
parameters	O
and	O
the	O
error	O
differentials	O
received	O
at	O
the	O
output	O
of	O
the	O
step	O
during	O
back	B-Method
-	I-Method
propagation	E-Method
could	O
be	O
passed	O
down	O
to	O
the	O
unary	O
potential	O
inputs	O
after	O
performing	O
usual	O
backward	B-Method
pass	I-Method
calculations	E-Method
of	O
the	O
softmax	B-Method
transformation	E-Method
.	O

section	O
:	O
Message	B-Task
Passing	E-Task
In	O
the	O
dense	O
CRF	S-Method
formulation	O
,	O
message	B-Task
passing	E-Task
is	O
implemented	O
by	O
applying	O
M	B-Method
Gaussian	I-Method
filters	E-Method
on	O
Q	O
values	O
.	O

Gaussian	B-Method
filter	I-Method
coefficients	E-Method
are	O
derived	O
based	O
on	O
image	O
features	O
such	O
as	O
the	O
pixel	O
locations	O
and	O
RGB	O
values	O
,	O
which	O
reflect	O
how	O
strongly	O
a	O
pixel	O
is	O
related	O
to	O
other	O
pixels	O
.	O

Since	O
the	O
CRF	S-Method
is	O
potentially	O
fully	O
connected	O
,	O
each	O
filter	O
's	O
receptive	O
field	O
spans	O
the	O
whole	O
image	O
,	O
making	O
it	O
infeasible	O
to	O
use	O
a	O
brute	O
-	O
force	O
implementation	O
of	O
the	O
filters	O
.	O

Fortunately	O
,	O
several	O
approximation	B-Method
techniques	E-Method
exist	O
to	O
make	O
computation	O
of	O
high	B-Task
dimensional	I-Task
Gaussian	I-Task
filtering	E-Task
significantly	O
faster	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
the	O
Permutohedral	B-Method
lattice	I-Method
implementation	E-Method
[	O
reference	O
]	O
,	O
which	O
can	O
compute	O
the	O
filter	O
response	O
in	O
O	O
(	O
N	O
)	O
time	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
of	O
the	O
image	O
[	O
reference	O
]	O
.	O

During	O
back	B-Method
-	I-Method
propagation	E-Method
,	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
inputs	O
are	O
calculated	O
by	O
sending	O
the	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
outputs	O
through	O
the	O
same	O
M	O
Gaussian	B-Method
filters	E-Method
in	O
reverse	O
direction	O
.	O

In	O
terms	O
of	O
permutohedral	B-Method
lattice	I-Method
operations	E-Method
,	O
this	O
can	O
be	O
accomplished	O
by	O
only	O
reversing	O
the	O
order	O
of	O
the	O
separable	B-Method
filters	E-Method
in	O
the	O
blur	B-Method
stage	E-Method
,	O
while	O
building	O
the	O
permutohedral	O
lattice	O
,	O
splatting	S-Method
,	O
and	O
slicing	O
in	O
the	O
same	O
way	O
as	O
in	O
the	O
forward	O
pass	O
.	O

Therefore	O
,	O
back	B-Method
-	I-Method
propagation	E-Method
through	O
this	O
filtering	B-Method
stage	E-Method
can	O
also	O
be	O
performed	O
in	O
O	O
(	O
N	O
)	O
time	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
two	O
Gaussian	B-Method
kernels	E-Method
,	O
a	O
spatial	B-Method
kernel	E-Method
and	O
a	O
bilateral	B-Method
kernel	E-Method
.	O

In	O
this	O
work	O
,	O
for	O
simplicity	O
,	O
we	O
keep	O
the	O
bandwidth	O
values	O
of	O
the	O
filters	O
fixed	O
.	O

It	O
is	O
also	O
possible	O
to	O
use	O
multiple	O
spatial	O
and	O
bilateral	O
kernels	O
with	O
different	O
bandwidth	O
values	O
and	O
learn	O
their	O
optimal	O
linear	B-Method
combination	E-Method
.	O

section	O
:	O
Weighting	B-Method
Filter	E-Method
Outputs	O
The	O
next	O
step	O
of	O
the	O
mean	B-Method
-	I-Method
field	I-Method
iteration	E-Method
is	O
taking	O
a	O
weighted	O
sum	O
of	O
the	O
M	O
filter	O
outputs	O
from	O
the	O
previous	O
step	O
,	O
for	O
each	O
class	O
label	O
l.	O
When	O
each	O
class	O
label	O
is	O
considered	O
individually	O
,	O
this	O
can	O
be	O
viewed	O
as	O
usual	O
convolution	S-Method
with	O
a	O
1	B-Method
×	I-Method
1	I-Method
filter	E-Method
with	O
M	O
input	O
channels	O
,	O
and	O
one	O
output	O
channel	O
.	O

Since	O
both	O
inputs	O
and	O
the	O
outputs	O
to	O
this	O
step	O
are	O
known	O
during	O
back	B-Method
-	I-Method
propagation	E-Method
,	O
the	O
error	O
derivative	O
w.r.t	O
.	O

the	O
filter	O
weights	O
can	O
be	O
computed	O
,	O
making	O
it	O
possible	O
to	O
automatically	O
learn	O
the	O
filter	O
weights	O
(	O
relative	O
contributions	O
from	O
each	O
Gaussian	B-Method
filter	E-Method
output	O
from	O
the	O
previous	O
stage	O
)	O
.	O

Error	O
derivative	O
w.r.t	O
.	O

the	O
inputs	O
can	O
also	O
be	O
computed	O
in	O
the	O
usual	O
manner	O
to	O
pass	O
the	O
error	O
derivatives	O
down	O
to	O
the	O
previous	O
stage	O
.	O

To	O
obtain	O
a	O
higher	O
number	O
of	O
tunable	O
parameters	O
,	O
in	O
contrast	O
to	O
[	O
reference	O
]	O
,	O
we	O
use	O
independent	O
kernel	O
weights	O
for	O
each	O
class	O
label	O
.	O

The	O
intuition	O
is	O
that	O
the	O
relative	O
importance	O
of	O
the	O
spatial	O
kernel	O
vs	O
the	O
bilateral	B-Method
kernel	E-Method
depends	O
on	O
the	O
visual	O
class	O
.	O

For	O
example	O
,	O
bilateral	O
kernels	O
may	O
have	O
on	O
the	O
one	O
hand	O
a	O
high	O
importance	O
in	O
bicycle	B-Task
detection	E-Task
,	O
because	O
similarity	O
of	O
colours	O
is	O
determinant	O
;	O
on	O
the	O
other	O
hand	O
they	O
may	O
have	O
low	O
importance	O
for	O
TV	B-Task
detection	E-Task
,	O
given	O
that	O
whatever	O
is	O
inside	O
the	O
TV	O
screen	O
may	O
have	O
many	O
different	O
colours	O
.	O

section	O
:	O
Compatibility	B-Method
Transform	E-Method
In	O
the	O
compatibility	B-Method
transform	I-Method
step	E-Method
,	O
outputs	O
from	O
the	O
previous	O
step	O
(	O
denoted	O
byQ	O
in	O
Algorithm	O
1	O
)	O
are	O
shared	O
between	O
the	O
labels	O
to	O
a	O
varied	O
extent	O
,	O
depending	O
on	O
the	O
compatibility	O
between	O
these	O
labels	O
.	O

Compatibility	O
between	O
the	O
two	O
labels	O
l	O
and	O
l	O
is	O
parameterized	O
by	O
the	O
label	O
compatibility	O
function	O
µ	O
(	O
l	O
,	O
l	O
)	O
.	O

The	O
Potts	B-Method
model	E-Method
,	O
given	O
by	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
[	O
l	O
=	O
l	O
]	O
,	O
where	O
[	O
.	O

]	O
is	O
the	O
Iverson	O
bracket	O
,	O
assigns	O
a	O
fixed	O
penalty	O
if	O
different	O
labels	O
are	O
assigned	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

A	O
limitation	O
of	O
this	O
model	O
is	O
that	O
it	O
assigns	O
the	O
same	O
penalty	O
for	O
all	O
different	O
pairs	O
of	O
labels	O
.	O

Intuitively	O
,	O
better	O
results	O
can	O
be	O
obtained	O
by	O
taking	O
the	O
compatibility	O
between	O
different	O
label	O
pairs	O
into	O
account	O
and	O
penalizing	O
the	O
assignments	O
accordingly	O
.	O

For	O
example	O
,	O
assigning	O
labels	O
"	O
person	O
"	O
and	O
"	O
bicycle	O
"	O
to	O
nearby	O
pixels	O
should	O
have	O
a	O
lesser	O
penalty	O
than	O
assigning	O
labels	O
"	O
sky	O
"	O
and	O
"	O
bicycle	O
"	O
.	O

Therefore	O
,	O
learning	O
the	O
function	O
µ	O
from	O
data	O
is	O
preferred	O
to	O
fixing	O
it	O
in	O
advance	O
with	O
Potts	B-Method
model	E-Method
.	O

We	O
also	O
relax	O
our	O
compatibility	B-Method
transform	I-Method
model	E-Method
by	O
assuming	O
that	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
µ	O
(	O
l	O
,	O
l	O
)	O
in	O
general	O
.	O

Compatibility	B-Task
transform	I-Task
step	E-Task
can	O
be	O
viewed	O
as	O
another	O
convolution	B-Method
layer	E-Method
where	O
the	O
spatial	O
receptive	O
field	O
of	O
the	O
filter	O
is	O
1	O
×	O
1	O
,	O
and	O
the	O
number	O
of	O
input	O
and	O
output	O
channels	O
are	O
both	O
L.	O
Learning	O
the	O
weights	O
of	O
this	O
filter	O
is	O
equivalent	O
to	O
learning	O
the	O
label	O
compatibility	O
function	O
µ.	O
Transferring	O
error	O
differentials	O
from	O
the	O
output	O
of	O
this	O
step	O
to	O
the	O
input	O
can	O
be	O
done	O
since	O
this	O
step	O
is	O
a	O
usual	O
convolution	B-Method
operation	E-Method
.	O

section	O
:	O
Adding	O
Unary	O
Potentials	O
In	O
this	O
step	O
,	O
the	O
output	O
from	O
the	O
compatibility	B-Method
transform	I-Method
stage	E-Method
is	O
subtracted	O
element	O
-	O
wise	O
from	O
the	O
unary	O
inputs	O
U	O
.	O

While	O
no	O
parameters	O
are	O
involved	O
in	O
this	O
step	O
,	O
transferring	O
error	O
differentials	O
can	O
be	O
done	O
trivially	O
by	O
copying	O
the	O
differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
to	O
both	O
inputs	O
with	O
the	O
appropriate	O
sign	O
.	O

section	O
:	O
Normalization	S-Task
Finally	O
,	O
the	O
normalization	O
step	O
of	O
the	O
iteration	O
can	O
be	O
considered	O
as	O
another	O
softmax	B-Method
operation	E-Method
with	O
no	O
parameters	O
.	O

Differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
can	O
be	O
passed	O
on	O
to	O
the	O
input	O
using	O
the	O
softmax	B-Method
operation	I-Method
's	I-Method
backward	I-Method
pass	E-Method
.	O

section	O
:	O
The	O
End	O
-	O
to	O
-	O
end	O
Trainable	B-Method
Network	E-Method
We	O
now	O
describe	O
our	O
end	O
-	O
to	O
-	O
end	B-Method
deep	I-Method
learning	I-Method
system	E-Method
for	O
semantic	B-Task
image	I-Task
segmentation	E-Task
.	O

To	O
pave	O
the	O
way	O
for	O
this	O
,	O
we	O
first	O
explain	O
how	O
repeated	O
mean	B-Method
-	I-Method
field	I-Method
iterations	E-Method
can	O
be	O
organized	O
as	O
an	O
RNN	S-Method
.	O

section	O
:	O
CRF	S-Method
as	O
RNN	S-Method
In	O
the	O
previous	O
section	O
,	O
it	O
was	O
shown	O
that	O
one	O
iteration	O
of	O
the	O
mean	B-Method
-	I-Method
field	I-Method
algorithm	E-Method
can	O
be	O
formulated	O
as	O
a	O
stack	O
of	O
common	O
CNN	S-Method
layers	O
(	O
see	O
Fig	O
.	O

1	O
)	O
.	O

We	O
use	O
the	O
function	O
f	B-Method
θ	E-Method
to	O
denote	O
the	O
transformation	O
done	O
by	O
one	O
mean	B-Method
-	I-Method
field	I-Method
iteration	E-Method
:	O
given	O
an	O
image	O
I	O
,	O
pixel	O
-	O
wise	O
unary	O
potential	O
values	O
U	O
and	O
an	O
estimation	O
of	O
marginal	O
probabilities	O
Q	O
in	O
from	O
the	O
previous	O
iteration	O
,	O
the	O
next	O
estimation	B-Task
of	I-Task
marginal	I-Task
distributions	E-Task
after	O
one	O
mean	B-Method
-	I-Method
field	I-Method
iteration	E-Method
is	O
given	O
by	O
f	B-Method
θ	E-Method

(	O
U	O
,	O
Q	O
in	O
,	O
I	O
)	O
.	O

..	O

,	O
l	O
L	O
}	O
represents	O
the	O
CRF	S-Method
parameters	O
described	O
in	O
Section	O
4	O
.	O

Multiple	O
mean	B-Method
-	I-Method
field	I-Method
iterations	E-Method
can	O
be	O
implemented	O
by	O
repeating	O
the	O
above	O
stack	O
of	O
layers	O
in	O
such	O
a	O
way	O
that	O
each	O
iteration	O
takes	O
Q	O
value	O
estimates	O
from	O
the	O
previous	O
iteration	O
and	O
the	O
unary	O
values	O
in	O
their	O
original	O
form	O
.	O

This	O
is	O
equivalent	O
to	O
treating	O
the	O
iterative	B-Method
mean	I-Method
-	I-Method
field	I-Method
inference	E-Method
as	O
a	O
Recurrent	B-Method
Neural	I-Method
Network	E-Method
(	O
RNN	S-Method
)	O
as	O
shown	O
in	O
Fig	O
.	O

2	O
.	O

Using	O
the	O
notation	O
in	O
the	O
figure	O
,	O
the	O
behaviour	O
of	O
the	O
network	O
is	O
given	O
by	O
the	O
following	O
equations	O
where	O
T	O
is	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
:	O
We	O
name	O
this	O
RNN	S-Method
structure	O
CRF	B-Method
-	I-Method
RNN	E-Method
.	O

Parameters	O
of	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
are	O
the	O
same	O
as	O
the	O
mean	O
-	O
field	O
parameters	O
described	O
in	O
Section	O
4	O
and	O
denoted	O
by	O
θ	O
here	O
.	O

Since	O
the	O
calculation	O
of	O
error	O
differentials	O
w.r.t	O
.	O

these	O
parameters	O
in	O
a	O
single	O
iteration	O
was	O
described	O
in	O
Section	O
4	O
,	O
they	O
can	O
be	O
learnt	O
in	O
the	O
RNN	S-Method
setting	O
using	O
the	O
standard	O
back	B-Method
-	I-Method
propagation	I-Method
through	I-Method
time	I-Method
algorithm	E-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

It	O
was	O
shown	O
in	O
[	O
reference	O
]	O
that	O
the	O
mean	B-Method
-	I-Method
field	I-Method
iterative	I-Method
algorithm	E-Method
for	O
dense	O
CRF	S-Method
converges	O
in	O
less	O
than	O
10	O
iterations	O
.	O

Furthermore	O
,	O
in	O
practice	O
,	O
after	O
Figure	O
2	O
.	O

The	O
CRF	S-Method
-	O
RNN	S-Method
Network	O
.	O

We	O
formulate	O
the	O
iterative	B-Method
mean	I-Method
-	I-Method
field	I-Method
algorithm	E-Method
as	O
a	O
Recurrent	B-Method
Neural	I-Method
Network	E-Method
(	O
RNN	S-Method
)	O
.	O

Gating	O
functions	O
G1	O
and	O
G2	O
are	O
fixed	O
as	O
described	O
in	O
the	O
text	O
.	O

about	O
5	O
iterations	O
,	O
increasing	O
the	O
number	O
of	O
iterations	O
usually	O
does	O
not	O
significantly	O
improve	O
results	O
[	O
reference	O
]	O
.	O

Therefore	O
,	O
it	O
does	O
not	O
suffer	O
from	O
the	O
vanishing	B-Task
and	I-Task
exploding	I-Task
gradient	I-Task
problem	E-Task
inherent	O
to	O
deep	O
RNNs	S-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

This	O
allows	O
us	O
to	O
use	O
a	O
plain	O
RNN	S-Method
architecture	O
instead	O
of	O
more	O
sophisticated	O
architectures	O
such	O
as	O
LSTMs	S-Method
in	O
our	O
network	O
.	O

section	O
:	O
Completing	O
the	O
Picture	O
Our	O
approach	O
comprises	O
a	O
fully	B-Method
convolutional	I-Method
network	I-Method
stage	E-Method
,	O
which	O
predicts	O
pixel	O
-	O
level	O
labels	O
without	O
considering	O
structure	O
,	O
followed	O
by	O
a	O
CRF	S-Method
-	O
RNN	S-Method
stage	O
,	O
which	O
performs	O
CRF	S-Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
for	O
structured	B-Task
prediction	E-Task
.	O

The	O
complete	O
system	O
,	O
therefore	O
,	O
unifies	O
strengths	O
of	O
both	O
CNNs	S-Method
and	O
CRFs	S-Method
and	O
is	O
trainable	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
[	O
reference	O
]	O
and	O
the	O
Stochastic	B-Method
Gradient	I-Method
Descent	E-Method
(	O
SGD	S-Method
)	O
procedure	O
.	O

During	O
training	S-Task
,	O
a	O
whole	O
image	O
(	O
or	O
many	O
of	O
them	O
)	O
can	O
be	O
used	O
as	O
the	O
mini	O
-	O
batch	O
and	O
the	O
error	O
at	O
each	O
pixel	O
output	O
of	O
the	O
network	O
can	O
be	O
computed	O
using	O
an	O
appropriate	O
loss	B-Method
function	E-Method
such	O
as	O
the	O
softmax	B-Method
loss	E-Method
with	O
respect	O
to	O
the	O
ground	B-Task
truth	I-Task
segmentation	E-Task
of	O
the	O
image	O
.	O

We	O
used	O
the	O
FCN	B-Method
-	I-Method
8s	I-Method
architecture	E-Method
of	O
[	O
reference	O
]	O
as	O
the	O
first	O
part	O
of	O
our	O
network	O
,	O
which	O
provides	O
unary	O
potentials	O
to	O
the	O
CRF	S-Method
.	O

This	O
network	O
is	O
based	O
on	O
the	O
VGG	B-Method
-	I-Method
16	I-Method
network	E-Method
[	O
reference	O
]	O
but	O
has	O
been	O
restructured	O
to	O
perform	O
pixel	B-Task
-	I-Task
wise	I-Task
prediction	E-Task
instead	O
of	O
image	B-Task
classification	E-Task
.	O

The	O
complete	O
architecture	O
of	O
our	O
network	O
,	O
including	O
the	O
FCN8s	B-Method
part	E-Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

In	O
the	O
forward	O
pass	O
through	O
the	O
network	O
,	O
once	O
the	O
computation	O
enters	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
after	O
passing	O
through	O
the	O
CNN	S-Method
stage	O
,	O
it	O
takes	O
T	O
iterations	O
for	O
the	O
data	O
to	O
leave	O
the	O
loop	O
created	O
by	O
the	O
RNN	S-Method
.	O

Neither	O
the	O
CNN	S-Method
that	O
provides	O
unary	O
values	O
nor	O
the	O
layers	O
after	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
(	O
i.e.	O
,	O
the	O
loss	O
layers	O
)	O
need	O
to	O
perform	O
any	O
computations	O
during	O
this	O
time	O
since	O
the	O
refinement	O
happens	O
only	O
inside	O
the	O
RNN	S-Method
's	O
loop	O
.	O

Once	O
the	O
output	O
Y	O
leaves	O
the	O
loop	O
,	O
next	O
stages	O
of	O
the	O
deep	B-Method
network	E-Method
after	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
can	O
continue	O
the	O
forward	O
pass	O
.	O

In	O
our	O
setup	O
,	O
a	O
softmax	B-Method
loss	I-Method
layer	E-Method
directly	O
follows	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
and	O
terminates	O
the	O
network	O
.	O

During	O
the	O
backward	O
pass	O
,	O
once	O
the	O
error	O
differentials	O
reach	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
's	O
output	O
Y	O
,	O
they	O
similarly	O
spend	O
T	O
iterations	O
within	O
the	O
loop	O
before	O
reaching	O
the	O
RNN	S-Method
input	O
U	O
in	O
order	O
to	O
propagate	O
to	O
the	O
CNN	S-Method
which	O
provides	O
the	O
unary	O
input	O
.	O

In	O
each	O
iteration	O
inside	O
the	O
loop	O
,	O
error	O
differentials	O
are	O
computed	O
inside	O
each	O
component	O
of	O
the	O
mean	B-Method
-	I-Method
field	I-Method
iteration	E-Method
as	O
described	O
in	O
Section	O
4	O
.	O

We	O
note	O
that	O
unnecessarily	O
increasing	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
could	O
potentially	O
result	O
in	O
the	O
vanishing	B-Task
and	I-Task
exploding	I-Task
gradient	I-Task
problems	E-Task
in	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
.	O

We	O
,	O
however	O
,	O
did	O
not	O
experience	O
this	O
problem	O
during	O
our	O
experiments	O
.	O

section	O
:	O
Implementation	O
Details	O
In	O
the	O
present	O
section	O
we	O
describe	O
the	O
implementation	O
details	O
of	O
the	O
proposed	O
network	O
,	O
as	O
well	O
as	O
its	O
training	B-Method
process	E-Method
.	O

The	O
high	O
-	O
level	O
architecture	O
of	O
our	O
system	O
,	O
which	O
was	O
implemented	O
using	O
the	O
popular	O
Caffe	O
[	O
reference	O
]	O
deep	O
learning	O
library	O
,	O
is	O
shown	O
in	O
Fig	O
.	O

3	O
.	O

Complete	O
architecture	O
of	O
the	O
deep	B-Method
network	E-Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

The	O
full	O
source	O
code	O
and	O
the	O
trained	O
models	O
of	O
our	O
approach	O
will	O
be	O
made	O
publicly	O
available	O
.	O

We	O
initialized	O
the	O
first	O
part	O
of	O
the	O
network	O
using	O
the	O
publicly	O
available	O
weights	O
of	O
the	O
FCN	B-Method
-	I-Method
8s	I-Method
network	E-Method
[	O
reference	O
]	O
.	O

The	O
compatibility	O
transform	O
parameters	O
of	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
were	O
initialized	O
using	O
the	O
Potts	B-Method
model	E-Method
,	O
and	O
kernel	O
width	O
and	O
weight	O
parameters	O
were	O
obtained	O
from	O
a	O
cross	B-Method
-	I-Method
validation	I-Method
process	E-Method
.	O

We	O
found	O
that	O
such	O
initialization	O
results	O
in	O
faster	O
convergence	B-Task
of	I-Task
training	E-Task
.	O

During	O
the	O
training	O
phase	O
,	O
parameters	O
of	O
the	O
whole	O
network	O
were	O
optimized	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	B-Method
-	I-Method
propagation	I-Method
algorithm	E-Method
.	O

In	O
particular	O
we	O
used	O
full	B-Task
image	I-Task
training	E-Task
described	O
in	O
[	O
reference	O
]	O
,	O
with	O
learning	B-Metric
rate	E-Metric
fixed	O
at	O
10	O
and	O
momentum	O
set	O
to	O
0.99	O
.	O

These	O
extreme	O
values	O
of	O
the	O
parameters	O
were	O
used	O
since	O
we	O
employed	O
only	O
one	O
image	O
per	O
batch	O
to	O
avoid	O
reaching	O
memory	O
limits	O
of	O
the	O
GPU	O
.	O

In	O
all	O
our	O
experiments	O
,	O
during	O
training	S-Task
,	O
we	O
set	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
in	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
to	O
5	O
to	O
avoid	O
vanishing	B-Task
/	I-Task
exploding	I-Task
gradient	I-Task
problems	E-Task
and	O
to	O
reduce	O
the	O
training	B-Metric
time	E-Metric
.	O

During	O
the	O
test	O
time	O
,	O
iteration	O
count	O
was	O
increased	O
to	O
10	O
.	O

The	O
effect	O
of	O
this	O
parameter	O
value	O
on	O
the	O
accuracy	S-Metric
is	O
discussed	O
in	O
section	O
7.1	O
.	O

Loss	B-Method
function	E-Method
During	O
the	O
training	O
of	O
the	O
models	O
that	O
achieved	O
the	O
best	O
results	O
reported	O
in	O
this	O
paper	O
,	O
we	O
used	O
the	O
standard	O
softmax	B-Method
loss	I-Method
function	E-Method
,	O
that	O
is	O
,	O
the	O
log	B-Method
-	I-Method
likelihood	I-Method
error	I-Method
function	E-Method
described	O
in	O
[	O
reference	O
]	O
.	O

The	O
standard	O
metric	O
used	O
in	O
the	O
Pascal	B-Material
VOC	I-Material
challenge	E-Material
is	O
the	O
average	B-Metric
intersection	I-Metric
over	I-Metric
union	E-Metric
(	O
IU	S-Metric
)	O
,	O
which	O
we	O
also	O
use	O
here	O
to	O
report	O
the	O
results	O
.	O

In	O
our	O
experiments	O
we	O
found	O
that	O
high	O
values	O
of	O
IU	S-Metric
on	O
the	O
validation	O
set	O
were	O
associated	O
to	O
low	O
values	O
of	O
the	O
averaged	B-Metric
softmax	I-Metric
loss	E-Metric
,	O
to	O
a	O
large	O
extent	O
.	O

We	O
also	O
tried	O
the	O
robust	O
loglikelihood	O
in	O
[	O
reference	O
]	O
as	O
a	O
loss	O
function	O
for	O
CRF	S-Method
-	O
RNN	S-Method
training	O
.	O

However	O
,	O
this	O
did	O
not	O
result	O
in	O
increased	O
accuracy	S-Metric
nor	O
faster	O
convergence	S-Metric
.	O

Normalization	B-Method
techniques	E-Method
As	O
described	O
in	O
Section	O
4	O
,	O
we	O
use	O
the	O
exponential	O
function	O
followed	O
by	O
pixel	B-Method
-	I-Method
wise	I-Method
normalization	E-Method
across	O
channels	O
in	O
several	O
stages	O
of	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
.	O

Since	O
this	O
operation	O
has	O
a	O
tendency	O
to	O
result	O
in	O
small	O
gradients	O
with	O
respect	O
to	O
the	O
input	O
when	O
the	O
input	O
value	O
is	O
large	O
,	O
we	O
conducted	O
several	O
experiments	O
where	O
we	O
replaced	O
this	O
by	O
a	O
rectifier	B-Method
linear	I-Method
unit	E-Method
(	O
ReLU	B-Method
)	I-Method
operation	E-Method
followed	O
by	O
a	O
normalization	S-Method
across	O
the	O
channels	O
.	O

Our	O
hypothesis	O
was	O
that	O
this	O
approach	O
may	O
approximate	O
the	O
original	O
operation	O
adequately	O
while	O
speeding	O
up	O
the	O
training	S-Task
due	O
to	O
improved	O
gradients	O
.	O

Furthermore	O
,	O
ReLU	S-Method
would	O
induce	O
sparsity	O
on	O
the	O
probability	O
of	O
labels	O
assigned	O
to	O
pixels	O
,	O
implicitly	O
pruning	O
low	O
likelihood	O
configurations	O
,	O
which	O
could	O
have	O
a	O
positive	O
effect	O
.	O

However	O
,	O
this	O
approach	O
did	O
not	O
lead	O
to	O
better	O
results	O
,	O
obtaining	O
1	O
%	O
IU	S-Metric
lower	O
than	O
the	O
original	O
setting	O
performance	O
.	O

section	O
:	O
Experiments	O
We	O
present	O
experimental	O
results	O
with	O
the	O
proposed	O
CRF	S-Method
-	O
RNN	S-Method
framework	O
.	O

We	O
use	O
these	O
datasets	O
:	O
the	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
dataset	E-Material
,	O
and	O
the	O
Pascal	B-Material
Context	I-Material
dataset	E-Material
.	O

We	O
use	O
the	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
dataset	E-Material
as	O
it	O
has	O
become	O
the	O
golden	O
standard	O
to	O
comprehensively	O
evaluate	O
any	O
new	O
semantic	B-Task
segmentation	E-Task
approach	O
in	O
comparison	O
to	O
existing	O
methods	O
.	O

We	O
also	O
use	O
the	O
Pascal	B-Material
Context	I-Material
dataset	E-Material
to	O
assess	O
how	O
well	O
our	O
approach	O
performs	O
on	O
a	O
dataset	O
with	O
different	O
characteristics	O
.	O

section	O
:	O
Pascal	B-Material
VOC	I-Material
Datasets	E-Material
In	O
order	O
to	O
evaluate	O
our	O
approach	O
with	O
existing	O
methods	O
under	O
the	O
same	O
circumstances	O
,	O
we	O
conducted	O
two	O
main	O
experiments	O
with	O
the	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
dataset	E-Material
,	O
followed	O
by	O
a	O
qualitative	O
experiment	O
.	O

In	O
the	O
first	O
experiment	O
,	O
following	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
we	O
used	O
a	O
training	O
set	O
consisted	O
of	O
VOC	B-Material
2012	I-Material
training	I-Material
data	E-Material
(	O
1464	O
images	O
)	O
,	O
and	O
training	O
and	O
validation	O
data	O
of	O
[	O
reference	O
]	O
,	O
which	O
amounts	O
to	O
a	O
total	O
of	O
11	O
,	O
685	O
images	O
.	O

After	O
removing	O
the	O
overlapping	O
images	O
between	O
VOC	B-Material
2012	I-Material
validation	I-Material
data	E-Material
and	O
this	O
training	O
dataset	O
,	O
we	O
were	O
left	O
with	O
346	O
images	O
from	O
the	O
original	O
VOC	B-Material
2012	I-Material
validation	I-Material
set	E-Material
to	O
validate	O
our	O
models	O
on	O
.	O

We	O
call	O
this	O
set	O
the	O
reduced	B-Metric
validation	I-Metric
set	E-Metric
in	O
the	O
sequel	O
.	O

Annotations	O
of	O
the	O
VOC	B-Material
2012	I-Material
test	I-Material
set	E-Material
,	O
which	O
consists	O
of	O
1456	O
images	O
,	O
are	O
not	O
publicly	O
available	O
and	O
hence	O
the	O
final	O
results	O
on	O
the	O
test	O
set	O
were	O
obtained	O
by	O
submitting	O
the	O
results	O
to	O
the	O
Pascal	B-Material
VOC	I-Material
challenge	I-Material
evaluation	I-Material
server	E-Material
[	O
reference	O
]	O
.	O

Regardless	O
of	O
the	O
smaller	O
number	O
of	O
images	O
,	O
we	O
found	O
that	O
the	O
relative	O
improvements	O
of	O
the	O
accuracy	S-Metric
on	O
our	O
validation	O
set	O
were	O
in	O
good	O
agreement	O
with	O
the	O
test	O
set	O
.	O

As	O
a	O
first	O
step	O
we	O
directly	O
compared	O
the	O
potential	O
advantage	O
of	O
learning	O
the	O
model	O
end	O
-	O
to	O
-	O
end	O
with	O
respect	O
to	O
alternative	O
learning	B-Method
strategies	E-Method
.	O

These	O
are	O
plain	O
FCN	B-Method
-	I-Method
8s	E-Method
without	O
applying	O
CRF	S-Method
,	O
and	O
with	O
CRF	S-Method
as	O
a	O
postprocessing	B-Method
method	E-Method
disconnected	O
from	O
the	O
training	O
of	O
FCN	S-Method
,	O
which	O
is	O
comparable	O
to	O
the	O
approach	O
described	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
reported	O
in	O
Table	O
1	O
and	O
show	O
a	O
clear	O
advantage	O
of	O
the	O
end	O
-	O
to	O
-	O
end	B-Method
strategy	E-Method
over	O
the	O
offline	B-Method
application	E-Method
of	O
CRF	S-Method
as	O
a	O
post	B-Method
-	I-Method
processing	I-Method
method	E-Method
.	O

This	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
during	O
the	O
SGD	S-Method
training	O
of	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
,	O
the	O
CNN	S-Method
component	O
and	O
the	O
CRF	S-Method
component	O
learn	O
how	O
to	O
co	O
-	O
operate	O
with	O
each	O
other	O
to	O
produce	O
the	O
optimum	O
output	O
of	O
the	O
whole	O
network	O
.	O

We	O
then	O
proceeded	O
to	O
compare	O
our	O
approach	O
with	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
that	O
used	O
training	O
data	O
from	O
the	O
standard	O
VOC	B-Material
2012	I-Material
training	I-Material
and	I-Material
validation	I-Material
sets	E-Material
,	O
and	O
from	O
the	O
dataset	O
published	O
with	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
2	O
,	O
above	O
the	O
bar	O
,	O
and	O
we	O
can	O
see	O
that	O
our	O
approach	O
outperforms	O
all	O
competitors	O
.	O

In	O
the	O
second	O
experiment	O
,	O
in	O
addition	O
to	O
the	O
above	O
training	O
set	O
,	O
we	O
used	O
data	O
from	O
the	O
Microsoft	O
COCO	S-Material
dataset	O
[	O
reference	O
]	O
as	O
was	O
done	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

We	O
selected	O
images	O
from	O
MS	O
COCO	S-Material
2014	O
training	O
set	O
where	O
the	O
ground	B-Task
truth	I-Task
segmentation	E-Task
has	O
at	O
least	O
200	O
pixels	O
marked	O
with	O
classes	O
labels	O
present	O
in	O
the	O
VOC	B-Material
2012	I-Material
dataset	E-Material
.	O

With	O
this	O
selection	O
,	O
we	O
ended	O
up	O
using	O
66	O
,	O
099	O
images	O
from	O
the	O
COCO	S-Material
dataset	O
and	O
therefore	O
a	O
total	O
of	O
66	O
,	O
099	O
+	O
11	O
,	O
685	O
=	O
77	O
,	O
784	O
training	O
images	O
were	O
used	O
in	O
the	O
second	O
experiment	O
.	O

The	O
same	O
reduced	B-Metric
validation	I-Metric
set	E-Metric
was	O
used	O
in	O
this	O
second	O
experiment	O
as	O
well	O
.	O

In	O
this	O
case	O
,	O
we	O
first	O
fine	O
-	O
tuned	O
the	O
plain	B-Method
FCN	I-Method
-	I-Method
32s	I-Method
network	E-Method
(	O
without	O
the	O
CRF	B-Method
-	I-Method
RNN	I-Method
part	E-Method
)	O
on	O
COCO	S-Material
data	O
,	O
then	O
we	O
built	O
an	O
FCN	B-Method
-	I-Method
8s	I-Method
network	E-Method
with	O
the	O
learnt	O
weights	O
and	O
finally	O
train	O
the	O
CRF	S-Method
-	O
RNN	S-Method
network	O
end	O
-	O
to	O
-	O
end	O
using	O
VOC	B-Material
2012	I-Material
training	I-Material
data	E-Material
only	O
.	O

Since	O
the	O
MS	O
COCO	S-Material
ground	B-Task
truth	I-Task
segmentation	E-Task
data	O
contains	O
somewhat	O
coarse	O
segmentation	O
masks	O
where	O
objects	O
are	O
not	O
delineated	O
properly	O
,	O
we	O
found	O
that	O
fine	O
-	O
tuning	O
our	O
model	O
with	O
COCO	S-Material
did	O
not	O
yield	O
significant	O
improvements	O
.	O

This	O
can	O
be	O
understood	O
because	O
the	O
primary	O
advantage	O
of	O
our	O
model	O
comes	O
from	O
delineating	O
the	O
objects	O
and	O
improving	O
fine	O
segmentation	O
boundaries	O
.	O

The	O
VOC	B-Material
2012	I-Material
training	I-Material
dataset	E-Material
therefore	O
helps	O
our	O
model	O
learn	O
this	O
task	O
effectively	O
.	O

The	O
results	O
of	O
this	O
experiment	O
are	O
shown	O
in	O
Table	O
2	O
,	O
below	O
the	O
bar	O
,	O
and	O
we	O
see	O
that	O
our	O
approach	O
sets	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
VOC	B-Material
2012	I-Material
dataset	E-Material
.	O

Note	O
that	O
in	O
both	O
setups	O
,	O
our	O
approach	O
outperforms	O
competing	O
methods	O
due	O
to	O
the	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
training	E-Task
of	O
the	O
CNN	S-Method
and	O
CRF	S-Method
in	O
the	O
unified	O
CRF	S-Method
-	O
RNN	S-Method
framework	O
.	O

We	O
also	O
evaluated	O
our	O
models	O
on	O
the	O
VOC	B-Material
2010	E-Material
,	O
and	O
VOC	B-Material
2011	I-Material
test	I-Material
set	E-Material
(	O
see	O
Table	O
2	O
)	O
.	O

In	O
all	O
cases	O
our	O
method	O
achieves	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
order	O
to	O
have	O
a	O
qualitative	O
evidence	O
about	O
how	O
CRF	B-Method
-	I-Method
RNN	E-Method
learns	O
,	O
we	O
visualize	O
the	O
compatibility	O
function	O
learned	O
after	O
the	O
training	O
stage	O
of	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
as	O
a	O
matrix	B-Method
representation	E-Method
in	O
Fig	O
.	O

5	O
.	O

Element	O
(	O
i	O
,	O
j	O
)	O
of	O
this	O
matrix	O
corresponds	O
to	O
µ	O
(	O
i	O
,	O
j	O
)	O
defined	O
earlier	O
:	O
a	O
high	O
value	O
at	O
(	O
i	O
,	O
j	O
)	O
implies	O
high	O
penalty	O
for	O
assigning	O
label	O
i	O
to	O
a	O
pixel	O
when	O
a	O
similar	O
pixel	O
(	O
spatially	O
or	O
appearance	O
wise	O
)	O
is	O
assigned	O
label	O
j.	O
For	O
example	O
we	O
can	O
appreciate	O
that	O
the	O
learned	O
compatibility	O
matrix	O
assigns	O
a	O
low	O
penalty	O
to	O
pairs	O
of	O
labels	O
that	O
tend	O
to	O
appear	O
together	O
,	O
such	O
as	O
[	O
Motorbike	O
,	O
Person	O
]	O
,	O
and	O
[	O
Dining	O
[	O
reference	O
]	O
n	O
/	O
a	O
39.1	O
n	O
/	O
a	O
O2PCPMC	O
[	O
reference	O
]	O
49.6	O
48.8	O
47.8	O
Divmbest	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
48.1	O
NUS	O
-	O
UDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
50.0	O
SDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
51.6	O
MSRA	B-Method
-	I-Method
CFM	E-Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
61.8	O
FCN	B-Method
-	I-Method
8s	E-Method
[	O
reference	O
]	O
n	O
/	O
a	O
62.7	O
62.2	O
Hypercolumn	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
62.6	O
Zoomout	O
[	O
reference	O
]	O
64.4	O
64.1	O
64.4	O
Context	O
-	O
Deep	O
-	O
CNN	S-Method
-	O
CRF	S-Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
70.7	O
DeepLabMSc	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.6	O
Our	O
method	O
w	O
/	O
o	O
COCO	S-Material
73.6	O
72.4	O
72.0	O
BoxSup	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.0	O
DeepLab	O
[	O
reference	O
][	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
72.7	O
Our	O
method	O
with	O
COCO	S-Material
75.7	O
75.0	O
74.7	O
Table	O
2	O
.	O

Mean	O
IU	S-Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	B-Method
-	I-Method
RNN	E-Method
,	O
compared	O
to	O
the	O
other	O
approaches	O
on	O
the	O
Pascal	O
VOC	B-Material
2010	E-Material
-	O
2012	O
test	O
datasets	O
.	O

Methods	O
from	O
the	O
first	O
group	O
do	O
not	O
use	O
MS	O
COCO	S-Material
data	O
for	O
training	O
.	O

The	O
methods	O
from	O
the	O
second	O
group	O
use	O
both	O
COCO	S-Material
and	O
VOC	O
datasets	O
for	O
training	O
.	O

section	O
:	O
Pascal	B-Material
Context	I-Material
Dataset	E-Material
We	O
conducted	O
an	O
experiment	O
on	O
the	O
Pascal	B-Material
Context	I-Material
dataset	E-Material
[	O
reference	O
]	O
,	O
which	O
differs	O
from	O
the	O
previous	O
one	O
in	O
the	O
larger	O
number	O
of	O
classes	O
considered	O
,	O
59	O
.	O

We	O
used	O
the	O
provided	O
partitions	O
of	O
training	O
and	O
validation	O
sets	O
,	O
and	O
the	O
obtained	O
results	O
are	O
reported	O
in	O
Table	O
3	O
.	O

Table	O
3	O
.	O

Mean	O
IU	S-Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	B-Method
-	I-Method
RNN	E-Method
,	O
evaluated	O
on	O
the	O
Pascal	B-Material
Context	I-Material
validation	I-Material
set	E-Material
.	O

Figure	O
5	O
.	O

Visualization	O
of	O
the	O
learnt	O
label	O
compatibility	O
matrix	O
.	O

In	O
the	O
standard	O
Potts	B-Method
model	E-Method
,	O
diagonal	O
entries	O
are	O
equal	O
to	O
−1	O
,	O
while	O
off	O
-	O
diagonal	O
entries	O
are	O
zero	O
.	O

These	O
values	O
have	O
changed	O
after	O
the	O
end	O
-	O
to	O
-	O
end	O
training	O
of	O
our	O
network	O
.	O

Best	O
viewed	O
in	O
colour	O
.	O

section	O
:	O
Effect	O
of	O
Design	O
Choices	O
We	O
performed	O
a	O
number	O
of	O
additional	O
experiments	O
on	O
the	O
Pascal	B-Material
VOC	I-Material
2012	I-Material
validation	I-Material
set	E-Material
described	O
above	O
to	O
study	O
the	O
effect	O
of	O
some	O
design	O
choices	O
we	O
made	O
.	O

We	O
first	O
studied	O
the	O
performance	O
gains	O
attained	O
by	O
our	O
modifications	O
to	O
the	O
CRF	S-Method
over	O
the	O
CRF	S-Method
approach	O
proposed	O
by	O
[	O
reference	O
]	O
.	O

We	O
found	O
that	O
using	O
different	O
filter	O
weights	O
for	O
different	O
classes	O
improved	O
the	O
performance	O
by	O
1.8	O
percentage	O
points	O
,	O
and	O
that	O
introducing	O
the	O
asymmetric	B-Method
compatibility	I-Method
transform	E-Method
further	O
boosted	O
the	O
performance	O
by	O
0.9	O
percentage	O
points	O
.	O

Regarding	O
the	O
RNN	S-Method
parameter	O
iteration	O
count	O
T	O
,	O
incrementing	O
it	O
to	O
T	O
=	O
10	O
during	O
the	O
test	O
time	O
,	O
from	O
T	O
=	O
5	O
during	O
the	O
train	O
time	O
,	O
produced	O
an	O
accuracy	S-Metric
improvement	O
of	O
0.2	O
percentage	O
points	O
.	O

Setting	O
T	O
=	O
10	O
also	O
during	O
training	O
reduced	O
the	O
accuracy	S-Metric
by	O
0.7	O
percentage	O
points	O
.	O

We	O
believe	O
that	O
this	O
might	O
be	O
due	O
to	O
a	O
vanishing	O
gradient	O
effect	O
caused	O
by	O
using	O
too	O
many	O
iterations	O
.	O

In	O
practice	O
that	O
leads	O
to	O
the	O
first	O
part	O
of	O
the	O
network	O
(	O
the	O
one	O
producing	O
unary	O
potentials	O
)	O
receiving	O
a	O
very	O
weak	O
error	O
gradient	O
signal	O
during	O
training	O
,	O
thus	O
hampering	O
its	O
learning	B-Method
capacity	E-Method
.	O

End	B-Metric
-	I-Metric
to	I-Metric
-	I-Metric
end	I-Metric
training	E-Metric
after	O
the	O
initialization	O
of	O
CRF	S-Method
parameters	O
improved	O
performance	O
by	O
3.4	O
percentage	O
points	O
.	O

We	O
also	O
conducted	O
an	O
experiment	O
where	O
we	O
froze	O
the	O
FCN8s	B-Method
part	E-Method
and	O
fine	O
-	O
tuned	O
only	O
the	O
RNN	S-Method
part	O
(	O
i.e.	O
,	O
CRF	S-Method
parameters	O
)	O
.	O

It	O
improved	O
the	O
performance	O
over	O
initialization	S-Method
by	O
only	O
1	O
percentage	O
point	O
.	O

We	O
therefore	O
conclude	O
that	O
end	O
-	O
toend	O
training	O
significantly	O
contributed	O
to	O
boost	O
the	O
accuracy	S-Metric
of	O
the	O
system	O
.	O

Treating	O
each	O
iteration	O
of	O
mean	B-Method
-	I-Method
field	I-Method
inference	E-Method
as	O
an	O
independent	O
step	O
with	O
its	O
own	O
parameters	O
,	O
and	O
training	O
endto	O
-	O
end	O
with	O
5	O
such	O
iterations	O
yielded	O
a	O
final	O
mean	O
IU	S-Metric
score	O
of	O
only	O
70.9	O
,	O
supporting	O
the	O
hypothesis	O
that	O
the	O
recurrent	O
structure	O
of	O
our	O
approach	O
is	O
important	O
for	O
its	O
success	O
.	O

section	O
:	O
Conclusion	O
We	O
presented	O
CRF	B-Method
-	I-Method
RNN	E-Method
,	O
an	O
interpretation	O
of	O
dense	O
CRFs	S-Method
as	O
Recurrent	B-Method
Neural	I-Method
Networks	E-Method
.	O

Our	O
formulation	O
fully	O
integrates	O
CRF	S-Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
with	O
emerging	O
deep	B-Method
learning	I-Method
techniques	E-Method
.	O

In	O
particular	O
,	O
the	O
proposed	O
CRF	B-Method
-	I-Method
RNN	E-Method
can	O
be	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
traditional	O
deep	B-Method
neural	I-Method
network	E-Method
:	O
It	O
is	O
capable	O
of	O
passing	O
on	O
error	O
differentials	O
from	O
its	O
outputs	O
to	O
inputs	O
during	O
back	B-Method
-	I-Method
propagation	I-Method
based	I-Method
training	E-Method
of	O
the	O
deep	B-Method
network	E-Method
while	O
learning	O
CRF	S-Method
parameters	O
.	O

We	O
demonstrate	O
the	O
use	O
of	O
this	O
approach	O
by	O
utilizing	O
it	O
for	O
the	O
semantic	B-Task
segmentation	E-Task
task	O
:	O
we	O
form	O
an	O
end	O
-	O
to	O
-	O
end	B-Method
trainable	I-Method
deep	I-Method
network	E-Method
by	O
combining	O
a	O
fully	B-Method
convolutional	I-Method
neural	I-Method
network	E-Method
with	O
the	O
CRF	B-Method
-	I-Method
RNN	E-Method
.	O

Our	O
system	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
popular	O
Pascal	B-Material
VOC	I-Material
segmentation	I-Material
benchmark	E-Material
.	O

This	O
improvement	O
can	O
be	O
attributed	O
to	O
the	O
uniting	O
of	O
the	O
strengths	O
of	O
CNNs	S-Method
and	O
CRFs	S-Method
in	O
a	O
single	O
deep	B-Method
network	E-Method
.	O

In	O
the	O
future	O
,	O
we	O
plan	O
to	O
investigate	O
the	O
advantages	O
/	O
disadvantages	O
of	O
restricting	O
the	O
capabilities	O
of	O
the	O
RNN	S-Method
part	O
of	O
our	O
network	O
to	O
mean	O
-	O
field	O
inference	O
of	O
dense	O
CRF	S-Method
.	O

A	O
sensible	O
baseline	O
to	O
the	O
work	O
presented	O
here	O
would	O
be	O
to	O
use	O
more	O
standard	O
RNNs	S-Method
(	O
e.g.	O
LSTMs	S-Method
)	O
that	O
learn	O
to	O
iteratively	O
improve	O
the	O
input	O
unary	O
potentials	O
to	O
make	O
them	O
closer	O
to	O
the	O
ground	O
-	O
truth	O
.	O

section	O
:	O
section	O
:	O
Acknowledgement	O
This	O
work	O
was	O
supported	O
by	O
grants	O
Leverhulme	O
Trust	O
,	O
EPSRC	O
EP	O
/	O
I001107	O
/	O
2	O
and	O
ERC	O
321162	O
-	O
HELIOS	O
.	O

We	O
thank	O
the	O
Caffe	O
team	O
,	O
Baidu	O
IDL	O
,	O
and	O
the	O
Oxford	O
ARC	O
team	O
for	O
their	O
support	O
.	O

We	O
gratefully	O
acknowledge	O
GPU	O
donations	O
from	O
NVIDIA	O
.	O

section	O
:	O
document	O
:	O
The	O
Reactor	S-Method
:	O
A	O
fast	B-Method
and	I-Method
sample	I-Method
-	I-Method
efficient	I-Method
Actor	I-Method
-	I-Method
Critic	I-Method
agent	E-Method
for	O
Reinforcement	B-Task
Learning	E-Task
In	O
this	O
work	O
,	O
we	O
present	O
a	O
new	O
agent	B-Method
architecture	E-Method
,	O
called	O
Reactor	S-Method
,	O
which	O
combines	O
multiple	O
algorithmic	B-Method
and	I-Method
architectural	I-Method
contributions	E-Method
to	O
produce	O
an	O
agent	O
with	O
higher	O
sample	B-Metric
-	I-Metric
efficiency	E-Metric
than	O
Prioritized	B-Method
Dueling	I-Method
DQN	E-Method
wang2017sample	O
and	O
Categorical	B-Method
DQN	E-Method
bellemare2017distributional	O
,	O
while	O
giving	O
better	O
run	B-Metric
-	I-Metric
time	E-Metric
performance	O
than	O
A3C	O
mnih2016asynchronous	O
.	O

Our	O
first	O
contribution	O
is	O
a	O
new	O
policy	B-Method
evaluation	I-Method
algorithm	E-Method
called	O
Distributional	B-Method
Retrace	E-Method
,	O
which	O
brings	O
multi	B-Task
-	I-Task
step	I-Task
off	I-Task
-	I-Task
policy	I-Task
updates	E-Task
to	O
the	O
distributional	B-Task
reinforcement	I-Task
learning	I-Task
setting	E-Task
.	O

The	O
same	O
approach	O
can	O
be	O
used	O
to	O
convert	O
several	O
classes	O
of	O
multi	B-Method
-	I-Method
step	I-Method
policy	I-Method
evaluation	I-Method
algorithms	E-Method
,	O
designed	O
for	O
expected	B-Task
value	I-Task
evaluation	E-Task
,	O
into	O
distributional	B-Method
algorithms	E-Method
.	O

Next	O
,	O
we	O
introduce	O
the	O
-	B-Method
leave	I-Method
-	I-Method
one	I-Method
-	I-Method
out	I-Method
policy	I-Method
gradient	I-Method
algorithm	E-Method
,	O
which	O
improves	O
the	O
trade	O
-	O
off	O
between	O
variance	S-Metric
and	O
bias	O
by	O
using	O
action	O
values	O
as	O
a	O
baseline	O
.	O

Our	O
final	O
algorithmic	O
contribution	O
is	O
a	O
new	O
prioritized	B-Method
replay	I-Method
algorithm	E-Method
for	O
sequences	O
,	O
which	O
exploits	O
the	O
temporal	O
locality	O
of	O
neighboring	O
observations	O
for	O
more	O
efficient	O
replay	B-Task
prioritization	E-Task
.	O

Using	O
the	O
Atari	B-Material
2600	I-Material
benchmarks	E-Material
,	O
we	O
show	O
that	O
each	O
of	O
these	O
innovations	O
contribute	O
to	O
both	O
sample	B-Metric
efficiency	E-Metric
and	O
final	B-Metric
agent	I-Metric
performance	E-Metric
.	O

Finally	O
,	O
we	O
demonstrate	O
that	O
Reactor	S-Method
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
after	O
200	O
million	O
frames	O
and	O
less	O
than	O
a	O
day	O
of	O
training	O
.	O

section	O
:	O
Introduction	O
Model	B-Method
-	I-Method
free	I-Method
deep	I-Method
reinforcement	I-Method
learning	E-Method
has	O
achieved	O
several	O
remarkable	O
successes	O
in	O
domains	O
ranging	O
from	O
super	B-Task
-	I-Task
human	I-Task
-	I-Task
level	I-Task
control	E-Task
in	O
video	O
games	O
mnih15human	O
and	O
the	O
game	O
of	O
Go	O
silver2016mastering	O
,	O
agzero	O
,	O
to	O
continuous	B-Task
motor	I-Task
control	I-Task
tasks	E-Task
lillicrap2015continuous	O
,	O
schulman2015trust	O
.	O

Much	O
of	O
the	O
recent	O
work	O
can	O
be	O
divided	O
into	O
two	O
categories	O
.	O

First	O
,	O
those	O
of	O
which	O
that	O
,	O
often	O
building	O
on	O
the	O
DQN	B-Method
framework	E-Method
,	O
act	O
-	O
greedily	O
according	O
to	O
an	O
action	B-Method
-	I-Method
value	I-Method
function	E-Method
and	O
train	O
using	O
mini	O
-	O
batches	O
of	O
transitions	O
sampled	O
from	O
an	O
experience	O
replay	O
buffer	O
van2016deep	O
,	O
wang2015dueling	O
,	O
he2016learning	O
,	O
anschel2017averaged	O
.	O

These	O
value	B-Method
-	I-Method
function	I-Method
agents	E-Method
benefit	O
from	O
improved	O
sample	B-Metric
complexity	E-Metric
,	O
but	O
tend	O
to	O
suffer	O
from	O
long	O
runtimes	S-Metric
(	O
e.g.	O
DQN	S-Method
requires	O
approximately	O
a	O
week	O
to	O
train	O
on	O
Atari	S-Material
)	O
.	O

The	O
second	O
category	O
are	O
the	O
actor	B-Method
-	I-Method
critic	I-Method
agents	E-Method
,	O
which	O
includes	O
the	O
asynchronous	B-Method
advantage	I-Method
actor	I-Method
-	I-Method
critic	I-Method
(	I-Method
A3C	I-Method
)	I-Method
algorithm	E-Method
,	O
introduced	O
by	O
mnih2016asynchronous	O
.	O

These	O
agents	O
train	O
on	O
transitions	O
collected	O
by	O
multiple	O
actors	O
running	O
,	O
and	O
often	O
training	O
,	O
in	O
parallel	O
schulman2017proximal	O
,	O
vezhnevets2017feudal	O
.	O

The	O
deep	B-Method
actor	I-Method
-	I-Method
critic	I-Method
agents	E-Method
train	O
on	O
each	O
trajectory	O
only	O
once	O
,	O
and	O
thus	O
tend	O
to	O
have	O
worse	O
sample	B-Metric
complexity	E-Metric
.	O

However	O
,	O
their	O
distributed	O
nature	O
allows	O
significantly	O
faster	O
training	S-Task
in	O
terms	O
of	O
wall	B-Metric
-	I-Metric
clock	I-Metric
time	E-Metric
.	O

Still	O
,	O
not	O
all	O
existing	O
algorithms	O
can	O
be	O
put	O
in	O
the	O
above	O
two	O
categories	O
and	O
various	O
hybrid	O
approaches	O
do	O
exist	O
zhao2016deep	O
,	O
o2016combining	O
,	O
gu2016q	O
,	O
wang2017sample	O
.	O

Data	B-Task
-	I-Task
efficiency	E-Task
and	O
off	B-Task
-	I-Task
policy	I-Task
learning	E-Task
are	O
essential	O
for	O
many	O
real	B-Task
-	I-Task
world	I-Task
domains	E-Task
where	O
interactions	O
with	O
the	O
environment	O
are	O
expensive	O
.	O

Similarly	O
,	O
wall	O
-	O
clock	O
time	O
(	O
time	B-Metric
-	I-Metric
efficiency	E-Metric
)	O
directly	O
impacts	O
an	O
algorithm	O
’s	O
applicability	O
through	O
resource	B-Metric
costs	E-Metric
.	O

The	O
focus	O
of	O
this	O
work	O
is	O
to	O
produce	O
an	O
agent	O
that	O
is	O
sample	O
-	O
and	O
time	O
-	O
efficient	O
.	O

To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
new	O
reinforcement	B-Method
learning	I-Method
agent	E-Method
,	O
called	O
Reactor	S-Method
(	O
Retrace	B-Method
-	I-Method
Actor	E-Method
)	O
,	O
which	O
takes	O
a	O
principled	O
approach	O
to	O
combining	O
the	O
sample	B-Metric
-	I-Metric
efficiency	E-Metric
of	O
off	B-Method
-	I-Method
policy	I-Method
experience	I-Method
replay	E-Method
with	O
the	O
time	O
-	O
efficiency	O
of	O
asynchronous	B-Method
algorithms	E-Method
.	O

We	O
combine	O
recent	O
advances	O
in	O
both	O
categories	O
of	O
agents	O
with	O
novel	O
contributions	O
to	O
produce	O
an	O
agent	O
that	O
inherits	O
the	O
benefits	O
of	O
both	O
and	O
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
over	O
57	O
Atari	S-Material
2600	O
games	O
.	O

Our	O
primary	O
contributions	O
are	O
(	O
1	O
)	O
a	O
novel	O
policy	B-Method
gradient	I-Method
algorithm	E-Method
,	O
-	O
LOO	O
,	O
which	O
makes	O
better	O
use	O
of	O
action	O
-	O
value	O
estimates	O
to	O
improve	O
the	O
policy	O
gradient	O
;	O
(	O
2	O
)	O
the	O
first	O
multi	B-Method
-	I-Method
step	I-Method
off	I-Method
-	I-Method
policy	I-Method
distributional	I-Method
reinforcement	I-Method
learning	I-Method
algorithm	E-Method
,	O
distributional	B-Method
Retrace	E-Method
(	O
)	O
;	O
(	O
3	O
)	O
a	O
novel	O
prioritized	B-Method
replay	E-Method
for	O
off	B-Task
-	I-Task
policy	I-Task
sequences	I-Task
of	I-Task
transitions	E-Task
;	O
and	O
(	O
4	O
)	O
an	O
optimized	B-Method
network	I-Method
and	I-Method
parallel	I-Method
training	I-Method
architecture	E-Method
.	O

We	O
begin	O
by	O
reviewing	O
background	O
material	O
,	O
including	O
relevant	O
improvements	O
to	O
both	O
value	B-Method
-	I-Method
function	I-Method
agents	E-Method
and	O
actor	B-Method
-	I-Method
critic	I-Method
agents	E-Method
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
introduce	O
each	O
of	O
our	O
primary	O
contributions	O
and	O
present	O
the	O
Reactor	S-Method
agent	O
.	O

Finally	O
,	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
present	O
experimental	O
results	O
on	O
the	O
57	O
Atari	S-Material
2600	O
games	O
from	O
the	O
Arcade	O
Learning	O
Environment	O
(	O
ALE	O
)	O
bellemare2013arcade	O
,	O
as	O
well	O
as	O
a	O
series	O
of	O
ablation	B-Task
studies	E-Task
for	O
the	O
various	O
components	O
of	O
Reactor	S-Method
.	O

section	O
:	O
Background	O
We	O
consider	O
a	O
Markov	B-Method
decision	I-Method
process	I-Method
(	I-Method
MDP	I-Method
)	E-Method
with	O
state	O
space	O
and	O
finite	O
action	O
space	O
.	O

A	O
(	O
stochastic	B-Method
)	I-Method
policy	E-Method
is	O
a	O
mapping	O
from	O
states	O
to	O
a	O
probability	O
distribution	O
over	O
actions	O
.	O

We	O
consider	O
a	O
-	O
discounted	O
infinite	O
-	O
horizon	O
criterion	O
,	O
with	O
the	O
discount	O
factor	O
,	O
and	O
define	O
for	O
policy	O
the	O
action	O
-	O
value	O
of	O
a	O
state	O
-	O
action	O
pair	O
as	O
where	O
is	O
a	O
trajectory	O
generated	O
by	O
choosing	O
in	O
and	O
following	O
thereafter	O
,	O
i.e.	O
,	O
(	O
for	O
)	O
,	O
and	O
is	O
the	O
reward	O
signal	O
.	O

The	O
objective	O
in	O
reinforcement	B-Task
learning	E-Task
is	O
to	O
find	O
an	O
optimal	B-Method
policy	E-Method
,	O
which	O
maximises	O
.	O

The	O
optimal	O
action	O
-	O
values	O
are	O
given	O
by	O
.	O

subsection	O
:	O
Value	B-Method
-	I-Method
based	I-Method
algorithms	E-Method
The	O
Deep	B-Method
Q	I-Method
-	I-Method
Network	I-Method
(	I-Method
DQN	I-Method
)	I-Method
framework	E-Method
,	O
introduced	O
by	O
mnih15human	O
,	O
popularised	O
the	O
current	O
line	O
of	O
research	O
into	O
deep	B-Method
reinforcement	I-Method
learning	E-Method
by	O
reaching	O
human	O
-	O
level	O
,	O
and	O
beyond	O
,	O
performance	O
across	O
57	O
Atari	S-Material
2600	O
games	O
in	O
the	O
ALE	O
.	O

While	O
DQN	S-Method
includes	O
many	O
specific	O
components	O
,	O
the	O
essence	O
of	O
the	O
framework	O
,	O
much	O
of	O
which	O
is	O
shared	O
by	O
Neural	B-Method
Fitted	I-Method
Q	E-Method
-	O
Learning	O
riedmiller2005neural	O
,	O
is	O
to	O
use	O
of	O
a	O
deep	B-Method
convolutional	I-Method
neural	I-Method
network	E-Method
to	O
approximate	O
an	O
action	B-Method
-	I-Method
value	I-Method
function	E-Method
,	O
training	O
this	O
approximate	B-Method
action	I-Method
-	I-Method
value	I-Method
function	E-Method
using	O
the	O
Q	B-Method
-	I-Method
Learning	I-Method
algorithm	E-Method
watkins1992	O
and	O
mini	O
-	O
batches	O
of	O
one	O
-	O
step	O
transitions	O
(	O
)	O
drawn	O
randomly	O
from	O
an	O
experience	O
replay	O
buffer	O
lin1992self	O
.	O

Additionally	O
,	O
the	O
next	O
-	O
state	O
action	O
-	O
values	O
are	O
taken	O
from	O
a	O
target	O
network	O
,	O
which	O
is	O
updated	O
to	O
match	O
the	O
current	O
network	O
periodically	O
.	O

Thus	O
,	O
the	O
temporal	B-Metric
difference	I-Metric
(	I-Metric
TD	I-Metric
)	I-Metric
error	E-Metric
for	O
transition	O
used	O
by	O
these	O
algorithms	O
is	O
given	O
by	O
where	O
denotes	O
the	O
parameters	O
of	O
the	O
network	O
and	O
are	O
the	O
parameters	O
of	O
the	O
target	O
network	O
.	O

Since	O
this	O
seminal	O
work	O
,	O
we	O
have	O
seen	O
numerous	O
extensions	O
and	O
improvements	O
that	O
all	O
share	O
the	O
same	O
underlying	O
framework	O
.	O

Double	B-Method
DQN	E-Method
van2016deep	O
,	O
attempts	O
to	O
correct	O
for	O
the	O
over	B-Task
-	I-Task
estimation	I-Task
bias	E-Task
inherent	O
in	O
Q	B-Method
-	I-Method
Learning	E-Method
by	O
changing	O
the	O
second	O
term	O
of	O
eq	O
:	O
tderr	O
to	O
.	O

The	O
dueling	B-Method
architecture	E-Method
wang2015dueling	O
,	O
changes	O
the	O
network	O
to	O
estimate	O
action	O
-	O
values	O
using	O
separate	O
network	B-Method
heads	E-Method
and	O
with	O
Recently	O
,	O
rainbow	O
introduced	O
Rainbow	S-Method
,	O
a	O
value	B-Method
-	I-Method
based	I-Method
reinforcement	I-Method
learning	I-Method
agent	E-Method
combining	O
many	O
of	O
these	O
improvements	O
into	O
a	O
single	O
agent	O
and	O
demonstrating	O
that	O
they	O
are	O
largely	O
complementary	O
.	O

Rainbow	S-Method
significantly	O
out	O
performs	O
previous	O
methods	O
,	O
but	O
also	O
inherits	O
the	O
poorer	O
time	B-Metric
-	I-Metric
efficiency	E-Metric
of	O
the	O
DQN	B-Method
framework	E-Method
.	O

We	O
include	O
a	O
detailed	O
comparison	O
between	O
Reactor	S-Method
and	O
Rainbow	O
in	O
the	O
Appendix	O
.	O

In	O
the	O
remainder	O
of	O
the	O
section	O
we	O
will	O
describe	O
in	O
more	O
depth	O
other	O
recent	O
improvements	O
to	O
DQN	S-Method
.	O

subsubsection	O
:	O
Prioritized	B-Task
experience	I-Task
replay	E-Task
The	O
experience	B-Method
replay	I-Method
buffer	E-Method
was	O
first	O
introduced	O
by	O
lin1992self	O
and	O
later	O
used	O
in	O
DQN	O
mnih15human	O
.	O

Typically	O
,	O
the	O
replay	B-Method
buffer	E-Method
is	O
essentially	O
a	O
first	B-Method
-	I-Method
in	I-Method
-	I-Method
first	I-Method
-	I-Method
out	I-Method
queue	E-Method
with	O
new	O
transitions	O
gradually	O
replacing	O
older	O
transitions	O
.	O

The	O
agent	O
would	O
then	O
sample	O
a	O
mini	O
-	O
batch	O
uniformly	O
at	O
random	O
from	O
the	O
replay	O
buffer	O
.	O

Drawing	O
inspiration	O
from	O
prioritized	B-Method
sweeping	I-Method
moore1993prioritized	E-Method
,	O
prioritized	B-Method
experience	I-Method
replay	E-Method
replaces	O
the	O
uniform	B-Method
sampling	E-Method
with	O
prioritized	B-Method
sampling	E-Method
proportional	O
to	O
the	O
absolute	B-Metric
TD	I-Metric
error	E-Metric
schaul16prioritized	O
.	O

Specifically	O
,	O
for	O
a	O
replay	O
buffer	O
of	O
size	O
,	O
prioritized	O
experience	O
replay	O
samples	O
transition	O
with	O
probability	O
,	O
and	O
applies	O
weighted	B-Method
importance	I-Method
-	I-Method
sampling	E-Method
with	O
to	O
correct	O
for	O
the	O
prioritization	O
bias	O
,	O
where	O
Prioritized	B-Method
DQN	E-Method
significantly	O
increases	O
both	O
the	O
sample	B-Metric
-	I-Metric
efficiency	E-Metric
and	O
final	O
performance	O
over	O
DQN	S-Method
on	O
the	O
Atari	B-Material
2600	I-Material
benchmarks	E-Material
schaul2015prioritized	O
.	O

subsubsection	O
:	O
Retrace	S-Method
(	O
)	O
Retrace	B-Method
(	I-Method
)	E-Method
is	O
a	O
convergent	B-Method
off	I-Method
-	I-Method
policy	I-Method
multi	I-Method
-	I-Method
step	I-Method
algorithm	E-Method
extending	O
the	O
DQN	B-Method
agent	E-Method
munos2016safe	O
.	O

Assume	O
that	O
some	O
trajectory	O
has	O
been	O
generated	O
according	O
to	O
behaviour	O
policy	O
,	O
i.e.	O
,	O
.	O

Now	O
,	O
we	O
aim	O
to	O
evaluate	O
the	O
value	O
of	O
a	O
different	O
target	O
policy	O
,	O
i.e.	O
we	O
want	O
to	O
estimate	O
.	O

The	O
Retrace	B-Method
algorithm	E-Method
will	O
update	O
our	O
current	O
estimate	O
of	O
in	O
the	O
direction	O
of	O
where	O
is	O
the	O
temporal	O
difference	O
at	O
time	O
under	O
,	O
and	O
The	O
Retrace	B-Method
algorithm	E-Method
comes	O
with	O
the	O
theoretical	O
guarantee	O
that	O
in	O
finite	O
state	O
and	O
action	O
spaces	O
,	O
repeatedly	O
updating	O
our	O
current	O
estimate	O
according	O
to	O
(	O
[	O
reference	O
]	O
)	O
produces	O
a	O
sequence	O
of	O
Q	B-Method
functions	E-Method
which	O
converges	O
to	O
for	O
a	O
fixed	O
or	O
to	O
if	O
we	O
consider	O
a	O
sequence	O
of	O
policies	O
which	O
become	O
increasingly	O
greedy	O
w.r.t	O
.	O

the	O
estimates	O
munos2016safe	O
.	O

subsubsection	O
:	O
Distributional	B-Method
RL	I-Method
Distributional	I-Method
reinforcement	I-Method
learning	E-Method
refers	O
to	O
a	O
class	O
of	O
algorithms	O
that	O
directly	O
estimate	O
the	O
distribution	O
over	O
returns	O
,	O
whose	O
expectation	O
gives	O
the	O
traditional	O
value	O
function	O
bellemare2017distributional	O
.	O

Such	O
approaches	O
can	O
be	O
made	O
tractable	O
with	O
a	O
distributional	B-Method
Bellman	I-Method
equation	E-Method
,	O
and	O
the	O
recently	O
proposed	O
algorithm	O
showed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
the	O
Atari	B-Material
2600	I-Material
benchmarks	E-Material
.	O

parameterizes	O
the	O
distribution	O
over	O
returns	O
with	O
a	O
mixture	B-Method
over	I-Method
Diracs	E-Method
centered	O
on	O
a	O
uniform	O
grid	O
,	O
with	O
hyperparameters	O
that	O
bound	O
the	O
distribution	O
support	O
of	O
size	O
.	O

subsection	O
:	O
Actor	B-Method
-	I-Method
critic	I-Method
algorithms	E-Method
In	O
this	O
section	O
we	O
review	O
the	O
actor	B-Method
-	I-Method
critic	I-Method
framework	E-Method
for	O
reinforcement	B-Method
learning	I-Method
algorithms	E-Method
and	O
then	O
discuss	O
recent	O
advances	O
in	O
actor	B-Method
-	I-Method
critic	I-Method
algorithms	E-Method
along	O
with	O
their	O
various	O
trade	O
-	O
offs	O
.	O

The	O
asynchronous	B-Method
advantage	I-Method
actor	I-Method
-	I-Method
critic	I-Method
(	I-Method
A3C	I-Method
)	I-Method
algorithm	I-Method
mnih2016asynchronous	E-Method
,	O
maintains	O
a	O
parameterized	B-Method
policy	I-Method
and	I-Method
value	I-Method
function	E-Method
,	O
which	O
are	O
updated	O
with	O
A3C	O
uses	O
parallel	B-Method
CPU	I-Method
workers	E-Method
,	O
each	O
acting	O
independently	O
in	O
the	O
environment	O
and	O
applying	O
the	O
above	O
updates	O
asynchronously	O
to	O
a	O
shared	O
set	O
of	O
parameters	O
.	O

In	O
contrast	O
to	O
the	O
previously	O
discussed	O
value	B-Method
-	I-Method
based	I-Method
methods	E-Method
,	O
A3C	S-Method
is	O
an	O
on	B-Method
-	I-Method
policy	I-Method
algorithm	E-Method
,	O
and	O
does	O
not	O
use	O
a	O
GPU	S-Method
nor	O
a	O
replay	B-Method
buffer	E-Method
.	O

Proximal	B-Method
Policy	I-Method
Optimization	E-Method
(	O
PPO	S-Method
)	O
is	O
a	O
closely	O
related	O
actor	B-Method
-	I-Method
critic	I-Method
algorithm	I-Method
schulman2017proximal	E-Method
,	O
which	O
replaces	O
the	O
advantage	O
pgadv	S-Method
with	O
,	O
where	O
is	O
as	O
defined	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Although	O
both	O
PPO	S-Method
and	O
A3C	O
run	O
parallel	B-Method
workers	E-Method
collecting	O
trajectories	O
independently	O
in	O
the	O
environment	O
,	O
PPO	S-Method
collects	O
these	O
experiences	O
to	O
perform	O
a	O
single	O
,	O
synchronous	O
,	O
update	O
in	O
contrast	O
with	O
the	O
asynchronous	B-Method
updates	E-Method
of	O
A3C.	O
Actor	B-Method
-	I-Method
Critic	I-Method
Experience	I-Method
Replay	E-Method
(	O
ACER	B-Method
)	E-Method
extends	O
the	O
A3C	B-Method
framework	E-Method
with	O
an	O
experience	B-Method
replay	I-Method
buffer	E-Method
,	O
Retrace	B-Method
algorithm	E-Method
for	O
off	B-Task
-	I-Task
policy	I-Task
corrections	E-Task
,	O
and	O
the	O
Truncated	B-Method
Importance	I-Method
Sampling	I-Method
Likelihood	I-Method
Ratio	I-Method
(	I-Method
TISLR	I-Method
)	I-Method
algorithm	E-Method
used	O
for	O
off	B-Task
-	I-Task
policy	I-Task
policy	I-Task
optimization	E-Task
wang2017sample	O
.	O

section	O
:	O
The	O
Reactor	S-Method
The	O
Reactor	S-Method
is	O
a	O
combination	O
of	O
four	O
novel	O
contributions	O
on	O
top	O
of	O
recent	O
improvements	O
to	O
both	O
deep	B-Method
value	I-Method
-	I-Method
based	I-Method
RL	E-Method
and	O
policy	B-Method
-	I-Method
gradient	I-Method
algorithms	E-Method
.	O

Each	O
contribution	O
moves	O
Reactor	S-Method
towards	O
our	O
goal	O
of	O
achieving	O
both	O
sample	B-Metric
and	I-Metric
time	I-Metric
efficiency	E-Metric
.	O

subsection	O
:	O
-	O
LOO	O
The	O
Reactor	S-Method
architecture	O
represents	O
both	O
a	O
policy	B-Method
and	I-Method
action	I-Method
-	I-Method
value	I-Method
function	E-Method
.	O

We	O
use	O
a	O
policy	B-Method
gradient	I-Method
algorithm	E-Method
to	O
train	O
the	O
actor	S-Method
which	O
makes	O
use	O
of	O
our	O
current	O
estimate	O
of	O
.	O

Let	O
be	O
the	O
value	O
function	O
at	O
some	O
initial	O
state	O
,	O
the	O
policy	B-Method
gradient	I-Method
theorem	E-Method
says	O
that	O
,	O
where	O
refers	O
to	O
the	O
gradient	O
w.r.t	O
.	O

policy	O
parameters	O
Sutton00policygradient	O
.	O

We	O
now	O
consider	O
several	O
possible	O
ways	O
to	O
estimate	O
this	O
gradient	O
.	O

To	O
simplify	O
notation	O
,	O
we	O
drop	O
the	O
dependence	O
on	O
the	O
state	O
for	O
now	O
and	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
quantity	O
In	O
the	O
off	B-Task
-	I-Task
policy	I-Task
case	E-Task
,	O
we	O
consider	O
estimating	O
using	O
a	O
single	O
action	O
drawn	O
from	O
a	O
(	O
possibly	O
different	O
from	O
)	O
behaviour	O
distribution	O
.	O

Let	O
us	O
assume	O
that	O
for	O
the	O
chosen	O
action	O
we	O
have	O
access	O
to	O
an	O
unbiased	O
estimate	O
of	O
.	O

Then	O
,	O
we	O
can	O
use	O
likelihood	B-Method
ratio	I-Method
(	I-Method
LR	I-Method
)	I-Method
method	E-Method
combined	O
with	O
an	O
importance	B-Method
sampling	I-Method
(	I-Method
IS	I-Method
)	I-Method
ratio	E-Method
(	O
which	O
we	O
call	O
ISLR	S-Method
)	O
to	O
build	O
an	O
unbiased	O
estimate	O
of	O
:	O
where	O
is	O
a	O
baseline	O
that	O
depends	O
on	O
the	O
state	O
but	O
not	O
on	O
the	O
chosen	O
action	O
.	O

However	O
this	O
estimate	O
suffers	O
from	O
high	O
variance	O
.	O

A	O
possible	O
way	O
for	O
reducing	O
variance	S-Metric
is	O
to	O
estimate	O
directly	O
from	O
(	O
[	O
reference	O
]	O
)	O
by	O
using	O
the	O
return	O
for	O
the	O
chosen	O
action	O
and	O
our	O
current	O
estimate	O
of	O
for	O
the	O
other	O
actions	O
,	O
which	O
leads	O
to	O
the	O
so	O
-	O
called	O
leave	B-Method
-	I-Method
one	I-Method
-	I-Method
out	I-Method
(	I-Method
LOO	I-Method
)	I-Method
policy	I-Method
-	I-Method
gradient	I-Method
estimate	E-Method
:	O
This	O
estimate	O
has	O
low	O
variance	O
but	O
may	O
be	O
biased	O
if	O
the	O
estimated	O
values	O
differ	O
from	O
.	O

A	O
better	O
bias	B-Metric
-	I-Metric
variance	I-Metric
tradeoff	E-Metric
may	O
be	O
obtained	O
by	O
the	O
more	O
general	B-Method
-	I-Method
LOO	I-Method
policy	I-Method
-	I-Method
gradient	I-Method
estimate	E-Method
:	O
where	O
can	O
be	O
a	O
function	O
of	O
both	O
policies	O
,	O
and	O
,	O
and	O
the	O
selected	O
action	O
.	O

Notice	O
that	O
when	O
,	O
(	O
[	O
reference	O
]	O
)	O
reduces	O
to	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
when	O
,	O
then	O
(	O
[	O
reference	O
]	O
)	O
is	O
This	O
estimate	O
is	O
unbiased	O
and	O
can	O
be	O
seen	O
as	O
a	O
generalization	O
of	O
where	O
instead	O
of	O
using	O
a	O
state	O
-	O
only	O
dependent	O
baseline	O
,	O
we	O
use	O
a	O
state	O
-	O
and	O
-	O
action	O
-	O
dependent	O
baseline	O
(	O
our	O
current	O
estimate	O
)	O
and	O
add	O
the	O
correction	B-Method
term	E-Method
to	O
cancel	O
the	O
bias	O
.	O

Proposition	O
[	O
reference	O
]	O
gives	O
our	O
analysis	O
of	O
the	O
bias	O
of	O
,	O
with	O
a	O
proof	O
left	O
to	O
the	O
Appendix	O
.	O

propositionpropbias	O
Assume	O
and	O
that	O
.	O

Then	O
,	O
the	O
bias	O
of	O
is	O
.	O

Thus	O
the	O
bias	O
is	O
small	O
when	O
is	O
close	O
to	O
,	O
or	O
when	O
the	O
-	O
estimates	O
are	O
close	O
to	O
the	O
true	O
values	O
,	O
and	O
unbiased	O
regardless	O
of	O
the	O
estimates	O
if	O
.	O

The	O
variance	O
is	O
low	O
when	O
is	O
small	O
,	O
therefore	O
,	O
in	O
order	O
to	O
improve	O
the	O
bias	B-Metric
-	I-Metric
variance	I-Metric
tradeoff	E-Metric
we	O
recommend	O
using	O
the	O
-	B-Method
LOO	I-Method
estimate	E-Method
with	O
defined	O
as	O
:	O
for	O
some	O
constant	O
.	O

This	O
truncated	O
coefficient	O
shares	O
similarities	O
with	O
the	O
truncated	B-Method
IS	I-Method
gradient	I-Method
estimate	E-Method
introduced	O
in	O
wang2017sample	O
(	O
which	O
we	O
call	O
TISLR	S-Method
for	O
truncated	B-Method
-	I-Method
ISLR	E-Method
)	O
:	O
The	O
differences	O
are	O
:	O
(	O
i	O
)	O
we	O
truncate	O
instead	O
of	O
truncating	S-Method
,	O
which	O
provides	O
an	O
additional	O
variance	B-Metric
reduction	E-Metric
due	O
to	O
the	O
variance	O
of	O
the	O
LR	S-Method
(	O
since	O
this	O
LR	S-Method
may	O
be	O
large	O
when	O
a	O
low	O
probability	O
action	O
is	O
chosen	O
)	O
,	O
and	O
(	O
ii	O
)	O
we	O
use	O
our	O
-	O
baseline	O
instead	O
of	O
a	O
baseline	O
,	O
reducing	O
further	O
the	O
variance	O
of	O
the	O
LR	B-Method
estimate	E-Method
.	O

subsection	O
:	O
Distributional	B-Task
Retrace	E-Task
In	O
off	B-Task
-	I-Task
policy	I-Task
learning	E-Task
it	O
is	O
very	O
difficult	O
to	O
produce	O
an	O
unbiased	O
sample	O
of	O
when	O
following	O
another	O
policy	O
.	O

This	O
would	O
require	O
using	O
full	O
importance	B-Method
sampling	I-Method
correction	E-Method
along	O
the	O
trajectory	O
.	O

Instead	O
,	O
we	O
use	O
the	O
off	O
-	O
policy	O
corrected	O
return	O
computed	O
by	O
the	O
Retrace	B-Method
algorithm	E-Method
,	O
which	O
produces	O
a	O
(	O
biased	O
)	O
estimate	O
of	O
but	O
whose	O
bias	O
vanishes	O
asymptotically	O
munos2016safe	O
.	O

In	O
Reactor	S-Method
,	O
we	O
consider	O
predicting	O
an	O
approximation	O
of	O
the	O
return	B-Method
distribution	I-Method
function	E-Method
from	O
any	O
state	O
-	O
action	O
pair	O
in	O
a	O
similar	O
way	O
as	O
in	O
bellemare2017distributional	O
.	O

The	O
original	O
algorithm	O
C51	O
described	O
in	O
that	O
paper	O
considered	O
single	O
-	O
step	O
Bellman	B-Method
updates	E-Method
only	O
.	O

Here	O
we	O
need	O
to	O
extend	O
this	O
idea	O
to	O
multi	B-Task
-	I-Task
step	I-Task
updates	E-Task
and	O
handle	O
the	O
off	B-Task
-	I-Task
policy	I-Task
correction	E-Task
performed	O
by	O
the	O
Retrace	B-Method
algorithm	E-Method
,	O
as	O
defined	O
in	O
eq	O
:	O
retrace	S-Method
.	O

Next	O
,	O
we	O
describe	O
these	O
two	O
extensions	O
.	O

paragraph	O
:	O
Multi	B-Method
-	I-Method
step	I-Method
distributional	I-Method
Bellman	I-Method
operator	E-Method
:	O
First	O
,	O
we	O
extend	O
C51	S-Method
to	O
multi	O
-	O
step	O
Bellman	B-Task
backups	E-Task
.	O

We	O
consider	O
return	O
-	O
distributions	O
from	O
of	O
the	O
form	O
(	O
where	O
denotes	O
a	O
Dirac	O
in	O
)	O
which	O
are	O
supported	O
on	O
a	O
finite	O
uniform	O
grid	O
,	O
,	O
,	O
.	O

The	O
coefficients	O
(	O
discrete	O
distribution	O
)	O
corresponds	O
to	O
the	O
probabilities	O
assigned	O
to	O
each	O
atom	O
of	O
the	O
grid	O
.	O

From	O
an	O
observed	O
-	O
step	O
sequence	O
,	O
generated	O
by	O
behavior	B-Method
policy	E-Method
(	O
i.e	O
,	O
for	O
)	O
,	O
we	O
build	O
the	O
-	B-Method
step	I-Method
backed	I-Method
-	I-Method
up	I-Method
return	I-Method
-	I-Method
distribution	E-Method
from	O
.	O

The	O
-	O
step	O
distributional	O
Bellman	O
target	O
,	O
whose	O
expectation	O
is	O
,	O
is	O
given	O
by	O
:	O
Since	O
this	O
distribution	O
is	O
supported	O
on	O
the	O
set	O
of	O
atoms	O
,	O
which	O
is	O
not	O
necessarily	O
aligned	O
with	O
the	O
grid	O
,	O
we	O
do	O
a	O
projection	B-Method
step	E-Method
and	O
minimize	O
the	O
KL	B-Metric
-	I-Metric
loss	E-Metric
between	O
the	O
projected	O
target	O
and	O
the	O
current	O
estimate	O
,	O
just	O
as	O
with	O
C51	O
except	O
with	O
a	O
different	O
target	O
distribution	O
bellemare2017distributional	O
.	O

paragraph	O
:	O
Distributional	O
Retrace	O
:	O
Now	O
,	O
the	O
Retrace	B-Method
algorithm	E-Method
defined	O
in	O
eq	O
:	O
retrace	S-Method
involves	O
an	O
off	B-Method
-	I-Method
policy	I-Method
correction	E-Method
which	O
is	O
not	O
handled	O
by	O
the	O
previous	O
-	O
step	O
distributional	B-Method
Bellman	I-Method
backup	E-Method
.	O

The	O
key	O
to	O
extending	O
this	O
distributional	B-Task
back	I-Task
-	I-Task
up	E-Task
to	O
off	B-Task
-	I-Task
policy	I-Task
learning	E-Task
is	O
to	O
rewrite	O
the	O
Retrace	B-Method
algorithm	E-Method
as	O
a	O
linear	B-Method
combination	I-Method
of	I-Method
-	I-Method
step	I-Method
Bellman	I-Method
backups	E-Method
,	O
weighted	O
by	O
some	O
coefficients	O
.	O

Indeed	O
,	O
notice	O
that	O
eq	O
:	O
retrace	O
rewrites	O
as	O
where	O
.	O

These	O
coefficients	O
depend	O
on	O
the	O
degree	O
of	O
off	O
-	O
policy	O
-	O
ness	O
(	O
between	O
and	O
)	O
along	O
the	O
trajectory	O
.	O

We	O
have	O
that	O
,	O
but	O
notice	O
some	O
coefficients	O
may	O
be	O
negative	O
.	O

However	O
,	O
in	O
expectation	O
(	O
over	O
the	O
behavior	B-Method
policy	E-Method
)	O
they	O
are	O
non	O
-	O
negative	O
.	O

Indeed	O
,	O
by	O
definition	O
of	O
the	O
coefficients	O
eq	O
:	O
trace.cut	O
.	O

Thus	O
in	O
expectation	O
(	O
over	O
the	O
behavior	O
policy	O
)	O
,	O
the	O
Retrace	B-Method
update	E-Method
can	O
be	O
seen	O
as	O
a	O
convex	B-Method
combination	I-Method
of	I-Method
-	I-Method
step	I-Method
Bellman	I-Method
updates	E-Method
.	O

Then	O
,	O
the	O
distributional	B-Method
Retrace	I-Method
algorithm	E-Method
can	O
be	O
defined	O
as	O
backing	O
up	O
a	O
mixture	B-Method
of	I-Method
-	I-Method
step	I-Method
distributions	E-Method
.	O

More	O
precisely	O
,	O
we	O
define	O
the	O
Retrace	O
target	O
distribution	O
as	O
:	O
where	O
is	O
a	O
linear	B-Method
interpolation	I-Method
kernel	E-Method
,	O
projecting	O
onto	O
the	O
support	O
:	O
We	O
update	O
the	O
current	O
probabilities	O
by	O
performing	O
a	O
gradient	B-Method
step	E-Method
on	O
the	O
KL	B-Method
-	I-Method
loss	E-Method
Again	O
,	O
notice	O
that	O
some	O
target	O
‘	O
‘	O
probabilities	O
’	O
’	O
may	O
be	O
negative	O
for	O
some	O
sample	O
trajectory	O
,	O
but	O
in	O
expectation	O
they	O
will	O
be	O
non	O
-	O
negative	O
.	O

Since	O
the	O
gradient	O
of	O
a	O
KL	B-Method
-	I-Method
loss	E-Method
is	O
linear	O
w.r.t	O
.	O

its	O
first	O
argument	O
,	O
our	O
update	B-Method
rule	I-Method
eq	E-Method
:	O
kl.gradient	S-Method
provides	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
of	O
the	O
KL	O
between	O
the	O
expected	O
(	O
over	O
the	O
behavior	B-Method
policy	E-Method
)	O
Retrace	O
target	O
distribution	O
and	O
the	O
current	O
predicted	O
distribution	O
.	O

paragraph	O
:	O
Remark	O
:	O
The	O
same	O
method	O
can	O
be	O
applied	O
to	O
other	O
algorithms	O
(	O
such	O
as	O
TB	B-Method
(	I-Method
)	E-Method
precup2000eligibility	O
and	O
importance	B-Method
sampling	I-Method
precup01offpolicy	E-Method
)	O
in	O
order	O
to	O
derive	O
distributional	B-Method
versions	E-Method
of	O
other	O
off	B-Method
-	I-Method
policy	I-Method
multi	I-Method
-	I-Method
step	I-Method
RL	I-Method
algorithms	E-Method
.	O

subsection	O
:	O
Prioritized	B-Task
sequence	I-Task
replay	I-Task
Prioritized	I-Task
experience	I-Task
replay	E-Task
has	O
been	O
shown	O
to	O
boost	O
both	O
statistical	B-Metric
efficiency	E-Metric
and	O
final	O
performance	O
of	O
deep	B-Method
RL	I-Method
agents	E-Method
schaul16prioritized	O
.	O

However	O
,	O
as	O
originally	O
defined	O
prioritized	B-Method
replay	E-Method
does	O
not	O
handle	O
sequences	O
of	O
transitions	O
and	O
weights	O
all	O
unsampled	O
transitions	O
identically	O
.	O

In	O
this	O
section	O
we	O
present	O
an	O
alternative	O
initialization	B-Method
strategy	E-Method
,	O
called	O
lazy	B-Method
initialization	E-Method
,	O
and	O
argue	O
that	O
it	O
better	O
encodes	O
prior	O
information	O
about	O
temporal	O
difference	O
errors	O
.	O

We	O
then	O
briefly	O
describe	O
our	O
computationally	O
efficient	O
prioritized	B-Method
sequence	I-Method
sampling	I-Method
algorithm	E-Method
,	O
with	O
full	O
details	O
left	O
to	O
the	O
appendix	O
.	O

It	O
is	O
widely	O
recognized	O
that	O
TD	O
errors	O
tend	O
to	O
be	O
temporally	O
correlated	O
,	O
indeed	O
the	O
need	O
to	O
break	O
this	O
temporal	O
correlation	O
has	O
been	O
one	O
of	O
the	O
primary	O
justifications	O
for	O
the	O
use	O
of	O
experience	B-Task
replay	E-Task
mnih15human	O
.	O

Our	O
proposed	O
algorithm	O
begins	O
with	O
this	O
fundamental	O
assumption	O
.	O

theorem	O
:	O
.	O

Temporal	O
differences	O
are	O
temporally	O
correlated	O
,	O
with	O
correlation	O
decaying	O
on	O
average	O
with	O
the	O
time	O
-	O
difference	O
between	O
two	O
transitions	O
.	O

Prioritized	B-Method
experience	I-Method
replay	E-Method
adds	O
new	O
transitions	O
to	O
the	O
replay	O
buffer	O
with	O
a	O
constant	O
priority	O
,	O
but	O
given	O
the	O
above	O
assumption	O
we	O
can	O
devise	O
a	O
better	O
method	O
.	O

Specifically	O
,	O
we	O
propose	O
to	O
add	O
experience	O
to	O
the	O
buffer	O
with	O
no	O
priority	O
,	O
inserting	O
a	O
priority	O
only	O
after	O
the	O
transition	O
has	O
been	O
sampled	O
and	O
used	O
for	O
training	O
.	O

Also	O
,	O
instead	O
of	O
sampling	O
transitions	O
,	O
we	O
assign	O
priorities	O
to	O
all	O
(	O
overlapping	O
)	O
sequences	O
of	O
length	O
.	O

When	O
sampling	O
,	O
sequences	O
with	O
an	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
that	O
priority	O
.	O

Sequences	O
with	O
no	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
the	O
average	O
priority	O
of	O
assigned	O
priority	O
sequences	O
within	O
some	O
local	O
neighbourhood	O
.	O

Averages	S-Method
are	O
weighted	O
to	O
compensate	O
for	O
sampling	O
biases	O
(	O
i.e.	O
more	O
samples	O
are	O
made	O
in	O
areas	O
of	O
high	O
estimated	O
priorities	O
,	O
and	O
in	O
the	O
absence	O
of	O
weighting	O
this	O
would	O
lead	O
to	O
overestimation	O
of	O
unassigned	O
priorities	O
)	O
.	O

The	O
lazy	B-Method
initialization	I-Method
scheme	E-Method
starts	O
with	O
priorities	O
corresponding	O
to	O
the	O
sequences	O
for	O
which	O
a	O
priority	O
was	O
already	O
assigned	O
.	O

Then	O
it	O
extrapolates	O
a	O
priority	O
of	O
all	O
other	O
sequences	O
in	O
the	O
following	O
way	O
.	O

Let	O
us	O
define	O
a	O
partition	O
of	O
the	O
states	O
ordered	O
by	O
increasing	O
time	O
such	O
that	O
each	O
cell	O
contains	O
exactly	O
one	O
state	O
with	O
already	O
assigned	O
priority	O
.	O

We	O
define	O
the	O
estimated	O
priority	O
to	O
all	O
other	O
sequences	O
as	O
,	O
where	O
is	O
a	O
collection	O
of	O
contiguous	O
cells	O
containing	O
time	O
,	O
and	O
is	O
the	O
length	O
of	O
the	O
cell	O
containing	O
.	O

For	O
already	O
defined	O
priorities	O
denote	O
.	O

Cell	O
sizes	O
work	O
as	O
estimates	O
of	O
inverse	O
local	O
density	O
and	O
are	O
used	O
as	O
importance	O
weights	O
for	O
priority	B-Task
estimation	E-Task
.	O

For	O
the	O
algorithm	O
to	O
be	O
unbiased	O
,	O
partition	O
must	O
not	O
be	O
a	O
function	O
of	O
the	O
assigned	O
priorities	O
.	O

So	O
far	O
we	O
have	O
defined	O
a	O
class	O
of	O
algorithms	O
all	O
free	O
to	O
choose	O
the	O
partition	O
and	O
the	O
collection	O
of	O
cells	O
,	O
as	O
long	O
that	O
they	O
satisfy	O
the	O
above	O
constraints	O
.	O

Figure	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
illustrates	O
the	O
above	O
description	O
.	O

Now	O
,	O
with	O
probability	O
we	O
sample	O
uniformly	O
at	O
random	O
,	O
and	O
with	O
probability	O
we	O
sample	O
proportionally	O
to	O
.	O

We	O
implemented	O
an	O
algorithm	O
satisfying	O
the	O
above	O
constraints	O
and	O
called	O
it	O
Contextual	B-Method
Priority	I-Method
Tree	E-Method
(	O
CPT	S-Method
)	O
.	O

It	O
is	O
based	O
on	O
AVL	B-Method
trees	E-Method
velskii1976avl	O
and	O
can	O
execute	O
sampling	S-Method
,	O
insertion	S-Method
,	O
deletion	S-Method
and	O
density	B-Method
evaluation	E-Method
in	O
time	O
.	O

We	O
describe	O
CPT	O
in	O
detail	O
in	O
the	O
Appendix	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
treated	O
prioritization	S-Method
as	O
purely	O
a	O
variance	B-Method
reduction	I-Method
technique	E-Method
.	O

Importance	O
-	O
sampling	O
weights	O
were	O
evaluated	O
as	O
in	O
prioritized	B-Task
experience	I-Task
replay	E-Task
,	O
with	O
fixed	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

We	O
used	O
simple	O
gradient	B-Method
magnitude	I-Method
estimates	E-Method
as	O
priorities	O
,	O
corresponding	O
to	O
a	O
mean	B-Metric
absolute	I-Metric
TD	I-Metric
error	E-Metric
along	O
a	O
sequence	O
for	O
Retrace	S-Task
,	O
as	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
for	O
the	O
classical	B-Task
RL	I-Task
case	E-Task
,	O
and	O
total	O
variation	O
in	O
the	O
distributional	B-Task
Retrace	I-Task
case	E-Task
.	O

subsection	O
:	O
Agent	B-Method
architecture	E-Method
In	O
order	O
to	O
improve	O
CPU	O
utilization	O
we	O
decoupled	O
acting	O
from	O
learning	S-Task
.	O

This	O
is	O
an	O
important	O
aspect	O
of	O
our	O
architecture	O
:	O
an	O
acting	B-Method
thread	E-Method
receives	O
observations	O
,	O
submits	O
actions	O
to	O
the	O
environment	O
,	O
and	O
stores	O
transitions	O
in	O
memory	O
,	O
while	O
a	O
learning	B-Method
thread	E-Method
re	O
-	O
samples	O
sequences	O
of	O
experiences	O
from	O
memory	O
and	O
trains	O
on	O
them	O
(	O
Figure	O
[	O
reference	O
]	O
,	O
left	O
)	O
.	O

We	O
typically	O
execute	O
4	O
-	O
6	O
acting	O
steps	O
per	O
each	O
learning	O
step	O
.	O

We	O
sample	O
sequences	O
of	O
length	O
in	O
batches	O
of	O
4	O
.	O

A	O
moving	B-Method
network	E-Method
is	O
unrolled	O
over	O
frames	O
1	O
-	O
32	O
while	O
the	O
target	O
network	O
is	O
unrolled	O
over	O
frames	O
2	O
-	O
33	O
.	O

We	O
allow	O
the	O
agent	O
to	O
be	O
distributed	O
over	O
multiple	O
machines	O
each	O
containing	O
action	O
-	O
learner	O
pairs	O
.	O

Each	O
worker	O
downloads	O
the	O
newest	O
network	O
parameters	O
before	O
each	O
learning	O
step	O
and	O
sends	O
delta	O
-	O
updates	O
at	O
the	O
end	O
of	O
it	O
.	O

Both	O
the	O
network	O
and	O
target	O
network	O
are	O
stored	O
on	O
a	O
shared	O
parameter	O
server	O
while	O
each	O
machine	O
contains	O
its	O
own	O
local	O
replay	O
memory	O
.	O

Training	S-Task
is	O
done	O
by	O
downloading	O
a	O
shared	B-Method
network	E-Method
,	O
evaluating	O
local	O
gradients	O
and	O
sending	O
them	O
to	O
be	O
applied	O
on	O
the	O
shared	B-Method
network	E-Method
.	O

While	O
the	O
agent	O
can	O
also	O
be	O
trained	O
on	O
a	O
single	O
machine	O
,	O
in	O
this	O
work	O
we	O
present	O
results	O
of	O
training	O
obtained	O
with	O
either	O
10	O
or	O
20	O
actor	B-Method
-	I-Method
learner	I-Method
workers	E-Method
and	O
one	O
parameter	B-Method
server	E-Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
(	O
right	O
)	O
we	O
compare	O
resources	O
and	O
runtimes	O
of	O
Reactor	S-Method
with	O
related	O
algorithms	O
.	O

subsubsection	O
:	O
Network	B-Method
architecture	E-Method
In	O
some	O
domains	O
,	O
such	O
as	O
Atari	S-Material
,	O
it	O
is	O
useful	O
to	O
base	O
decisions	O
on	O
a	O
short	O
history	O
of	O
past	O
observations	O
.	O

The	O
two	O
techniques	O
generally	O
used	O
to	O
achieve	O
this	O
are	O
frame	B-Method
stacking	E-Method
and	O
recurrent	B-Method
network	I-Method
architectures	E-Method
.	O

We	O
chose	O
the	O
latter	O
over	O
the	O
former	O
for	O
reasons	O
of	O
implementation	B-Metric
simplicity	E-Metric
and	O
computational	B-Metric
efficiency	E-Metric
.	O

As	O
the	O
Retrace	B-Method
algorithm	E-Method
requires	O
evaluating	O
action	O
-	O
values	O
over	O
contiguous	O
sequences	O
of	O
trajectories	O
,	O
using	O
a	O
recurrent	B-Method
architecture	E-Method
allowed	O
each	O
frame	O
to	O
be	O
processed	O
by	O
the	O
convolutional	B-Method
network	E-Method
only	O
once	O
,	O
as	O
opposed	O
to	O
times	O
times	O
if	O
frame	O
concatenations	O
were	O
used	O
.	O

The	O
Reactor	S-Method
architecture	O
uses	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
which	O
takes	O
an	O
observation	O
as	O
input	O
and	O
produces	O
two	O
outputs	O
:	O
categorical	O
action	O
-	O
value	O
distributions	O
(	O
here	O
is	O
a	O
bin	O
identifier	O
)	O
,	O
and	O
policy	O
probabilities	O
.	O

We	O
use	O
an	O
architecture	O
inspired	O
by	O
the	O
duelling	B-Method
network	I-Method
architecture	E-Method
wang2015dueling	O
.	O

We	O
split	O
action	O
-	O
value	O
-	O
distribution	O
logits	O
into	O
state	O
-	O
value	O
logits	O
and	O
advantage	O
logits	O
,	O
which	O
in	O
turn	O
are	O
connected	O
to	O
the	O
same	O
LSTM	B-Method
network	E-Method
hochreiter1997long	O
.	O

Final	O
action	O
-	O
value	O
logits	O
are	O
produced	O
by	O
summing	O
state	O
-	O
and	O
action	O
-	O
specific	O
logits	O
,	O
as	O
in	O
wang2015dueling	O
.	O

Finally	O
,	O
a	O
softmax	B-Method
layer	E-Method
on	O
top	O
for	O
each	O
action	O
produces	O
the	O
distributions	O
over	O
discounted	O
future	O
returns	O
.	O

The	O
policy	B-Method
head	E-Method
uses	O
a	O
softmax	B-Method
layer	E-Method
mixed	O
with	O
a	O
fixed	O
uniform	O
distribution	O
over	O
actions	O
,	O
where	O
this	O
mixing	O
ratio	O
is	O
a	O
hyperparameter	O
[	O
Section	O
5.1.3	O
]	O
wiering1999explorations	O
.	O

Policy	B-Method
and	I-Method
Q	I-Method
-	I-Method
networks	E-Method
have	O
separate	O
LSTMs	S-Method
.	O

Both	O
LSTMs	S-Method
are	O
connected	O
to	O
a	O
shared	B-Method
linear	I-Method
layer	E-Method
which	O
is	O
connected	O
to	O
a	O
shared	B-Method
convolutional	I-Method
neural	I-Method
network	I-Method
krizhevsky2012imagenet	E-Method
.	O

The	O
precise	O
network	O
specification	O
is	O
given	O
in	O
Table	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
.	O

Gradients	O
coming	O
from	O
the	O
policy	B-Method
LSTM	E-Method
are	O
blocked	O
and	O
only	O
gradients	O
originating	O
from	O
the	O
Q	B-Method
-	I-Method
network	I-Method
LSTM	E-Method
are	O
allowed	O
to	O
back	O
-	O
propagate	O
into	O
the	O
convolutional	B-Method
neural	I-Method
network	E-Method
.	O

We	O
block	O
gradients	O
from	O
the	O
policy	O
head	O
for	O
increased	O
stability	O
,	O
as	O
this	O
avoids	O
positive	O
feedback	O
loops	O
between	O
and	O
caused	O
by	O
shared	O
representations	O
.	O

We	O
used	O
the	O
Adam	B-Method
optimiser	E-Method
kingma2014adam	O
,	O
with	O
a	O
learning	B-Metric
rate	E-Metric
of	O
and	O
zero	O
momentum	O
because	O
asynchronous	O
updates	O
induce	O
implicit	O
momentum	O
mitliagkas2016asynchrony	O
.	O

Further	O
discussion	O
of	O
hyperparameters	O
and	O
their	O
optimization	S-Task
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

section	O
:	O
Experimental	O
Results	O
We	O
trained	O
and	O
evaluated	O
Reactor	S-Method
on	O
57	O
Atari	S-Material
games	O
bellemare2013arcade	O
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
performance	O
of	O
Reactor	S-Method
with	O
different	O
versions	O
of	O
Reactor	S-Method
each	O
time	O
leaving	O
one	O
of	O
the	O
algorithmic	O
improvements	O
out	O
.	O

We	O
can	O
see	O
that	O
each	O
of	O
the	O
algorithmic	O
improvements	O
(	O
Distributional	B-Method
retrace	E-Method
,	O
beta	B-Method
-	I-Method
LOO	E-Method
and	O
prioritized	B-Method
replay	E-Method
)	O
contributed	O
to	O
the	O
final	O
results	O
.	O

While	O
prioritization	S-Method
was	O
arguably	O
the	O
most	O
important	O
component	O
,	O
Beta	B-Method
-	I-Method
LOO	E-Method
clearly	O
outperformed	O
TISLR	B-Method
algorithm	E-Method
.	O

Although	O
distributional	B-Method
and	I-Method
non	I-Method
-	I-Method
distributional	I-Method
versions	E-Method
performed	O
similarly	O
in	O
terms	O
of	O
median	B-Metric
human	I-Metric
normalized	I-Metric
scores	E-Metric
,	O
distributional	B-Method
version	E-Method
of	O
the	O
algorithm	O
generalized	O
better	O
when	O
tested	O
with	O
random	O
human	O
starts	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Comparing	O
to	O
prior	O
work	O
We	O
evaluated	O
Reactor	S-Method
with	O
target	O
update	O
frequency	O
,	O
and	O
-	O
LOO	O
with	O
on	O
57	O
Atari	S-Material
games	O
trained	O
on	O
10	O
machines	O
in	O
parallel	O
.	O

We	O
averaged	O
scores	O
over	O
200	O
episodes	O
using	O
30	O
random	O
human	O
starts	O
and	O
noop	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
)	O
.	O

We	O
calculated	O
mean	O
and	O
median	O
human	O
normalised	O
scores	O
across	O
all	O
games	O
.	O

We	O
also	O
ranked	O
all	O
algorithms	O
(	O
including	O
random	O
and	O
human	O
scores	O
)	O
for	O
each	O
game	O
and	O
evaluated	O
mean	B-Metric
rank	E-Metric
of	O
each	O
algorithm	O
across	O
all	O
57	O
Atari	S-Material
games	O
.	O

We	O
also	O
evaluated	O
mean	O
Rank	S-Metric
and	O
Elo	B-Metric
scores	E-Metric
for	O
each	O
algorithm	O
for	O
both	O
human	O
and	O
noop	O
start	O
settings	O
.	O

Please	O
refer	O
to	O
Section	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
for	O
more	O
details	O
.	O

Tables	O
[	O
reference	O
]	O
&	O
[	O
reference	O
]	O
compare	O
versions	O
of	O
our	O
algorithm	O
,	O
with	O
several	O
other	O
state	O
-	O
of	O
-	O
art	O
algorithms	O
across	O
57	O
Atari	S-Material
games	O
for	O
a	O
fixed	O
random	O
seed	O
across	O
all	O
games	O
bellemare2013arcade	O
.	O

We	O
compare	O
Reactor	S-Method
against	O
are	O
:	O
DQN	S-Method
mnih15human	O
,	O
Double	B-Method
DQN	E-Method
van2016deep	O
,	O
DQN	B-Method
with	I-Method
prioritised	I-Method
experience	I-Method
replay	E-Method
schaul2015prioritized	O
,	O
dueling	B-Method
architecture	I-Method
and	I-Method
prioritised	I-Method
dueling	E-Method
wang2015dueling	O
,	O
ACER	S-Method
wang2017sample	O
,	O
A3C	S-Method
mnih2016asynchronous	O
,	O
and	O
Rainbow	B-Method
rainbow	E-Method
.	O

Each	O
algorithm	O
was	O
exposed	O
to	O
200	O
million	O
frames	O
of	O
experience	O
,	O
or	O
500	O
million	O
frames	O
when	O
followed	O
by	O
,	O
and	O
the	O
same	O
pre	B-Method
-	I-Method
processing	I-Method
pipeline	E-Method
including	O
4	O
action	O
repeats	O
was	O
used	O
as	O
in	O
the	O
original	O
DQN	O
paper	O
mnih15human	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
Reactor	S-Method
exceeds	O
the	O
performance	O
of	O
all	O
algorithms	O
across	O
all	O
metrics	O
,	O
despite	O
requiring	O
under	O
two	O
days	O
of	O
training	O
.	O

With	O
500	O
million	O
frames	O
and	O
four	O
days	O
training	O
we	O
see	O
Reactor	S-Method
’s	O
performance	O
continue	O
to	O
improve	O
significantly	O
.	O

The	O
difference	O
in	O
time	B-Metric
-	I-Metric
efficiency	E-Metric
is	O
especially	O
apparent	O
when	O
comparing	O
Reactor	S-Method
and	O
Rainbow	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
,	O
right	O
)	O
.	O

Additionally	O
,	O
unlike	O
Rainbow	S-Method
,	O
Reactor	S-Method
does	O
not	O
use	O
Noisy	B-Method
Networks	E-Method
fortunato2017noisy	O
,	O
which	O
was	O
reported	O
to	O
have	O
contributed	O
to	O
the	O
performance	O
gains	O
.	O

When	O
evaluating	O
under	O
the	O
no	O
-	O
op	O
starts	O
regime	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
Reactor	S-Method
out	O
performs	O
all	O
methods	O
except	O
for	O
Rainbow	S-Method
.	O

This	O
suggests	O
that	O
Rainbow	S-Method
is	O
more	O
sample	O
-	O
efficient	O
when	O
training	O
and	O
evaluation	O
regimes	O
match	O
exactly	O
,	O
but	O
may	O
be	O
overfitting	O
to	O
particular	O
trajectories	O
due	O
to	O
the	O
significant	O
drop	O
in	O
performance	O
when	O
evaluated	O
on	O
the	O
random	O
human	O
starts	O
.	O

Regarding	O
ACER	S-Method
,	O
another	O
Retrace	B-Method
-	I-Method
based	I-Method
actor	I-Method
-	I-Method
critic	I-Method
architecture	E-Method
,	O
both	O
classical	O
and	O
distributional	B-Method
versions	E-Method
of	O
Reactor	S-Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
exceeded	O
the	O
best	O
reported	O
median	O
human	B-Metric
normalized	I-Metric
score	E-Metric
of	O
1.9	O
with	O
noop	O
starts	O
achieved	O
in	O
500	O
million	O
steps	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
presented	O
a	O
new	O
off	B-Method
-	I-Method
policy	I-Method
agent	E-Method
based	O
on	O
Retrace	B-Method
actor	I-Method
-	I-Method
critic	I-Method
architecture	E-Method
and	O
show	O
that	O
it	O
achieves	O
similar	O
performance	O
as	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
while	O
giving	O
significant	O
real	O
-	O
time	O
performance	O
gains	O
.	O

We	O
demonstrate	O
the	O
benefits	O
of	O
each	O
of	O
the	O
suggested	O
algorithmic	B-Method
improvements	E-Method
,	O
including	O
Distributional	B-Method
Retrace	E-Method
,	O
beta	B-Method
-	I-Method
LOO	I-Method
policy	I-Method
gradient	E-Method
and	O
contextual	B-Method
priority	I-Method
tree	E-Method
.	O

bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
*	O
proof	O
:	O
Proof	O
.	O

The	O
bias	O
of	O
is	O
∎	O
subsection	O
:	O
Hyperparameter	B-Method
optimization	E-Method
As	O
we	O
believe	O
that	O
algorithms	O
should	O
be	O
robust	O
with	O
respect	O
to	O
the	O
choice	O
of	O
hyperparameters	O
,	O
we	O
spent	O
little	O
effort	O
on	O
parameter	B-Task
optimization	E-Task
.	O

In	O
total	O
,	O
we	O
explored	O
three	O
distinct	O
values	O
of	O
learning	B-Metric
rates	E-Metric
and	O
two	O
values	O
of	O
ADAM	O
momentum	O
(	O
the	O
default	O
and	O
zero	O
)	O
and	O
two	O
values	O
of	O
on	O
a	O
subset	O
of	O
7	O
Atari	S-Material
games	O
without	O
prioritization	S-Method
using	O
non	B-Method
-	I-Method
distributional	I-Method
version	I-Method
of	I-Method
Reactor	E-Method
.	O

We	O
later	O
used	O
those	O
values	O
for	O
all	O
experiments	O
.	O

We	O
did	O
not	O
optimize	O
for	O
batch	O
sizes	O
and	O
sequence	O
length	O
or	O
any	O
prioritization	B-Method
hyperparamters	E-Method
.	O

subsection	O
:	O
Rank	S-Metric
and	O
Elo	B-Metric
evaluation	E-Metric
Commonly	O
used	O
mean	O
and	O
median	B-Metric
human	I-Metric
normalized	I-Metric
scores	E-Metric
have	O
several	O
disadvantages	O
.	O

A	O
mean	O
human	O
normalized	O
score	O
implicitly	O
puts	O
more	O
weight	O
on	O
games	O
that	O
computers	O
are	O
good	O
and	O
humans	O
are	O
bad	O
at	O
.	O

Comparing	O
algorithm	O
by	O
a	O
mean	O
human	B-Metric
normalized	I-Metric
score	E-Metric
across	O
57	O
Atari	S-Material
games	O
is	O
almost	O
equivalent	O
to	O
comparing	O
algorithms	O
on	O
a	O
small	O
subset	O
of	O
games	O
close	O
to	O
the	O
median	O
and	O
thus	O
dominating	O
the	O
signal	O
.	O

Typically	O
a	O
set	O
of	O
ten	O
most	O
score	O
-	O
generous	O
games	O
,	O
namely	O
Assault	O
,	O
Asterix	O
,	O
Breakout	O
,	O
Demon	O
Attack	O
,	O
Double	O
Dunk	O
,	O
Gopher	O
,	O
Pheonix	O
,	O
Stargunner	O
,	O
Up’n	O
Down	O
and	O
Video	O
Pinball	O
can	O
explain	O
more	O
than	O
half	O
of	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
variance	E-Metric
.	O

A	O
median	B-Metric
human	I-Metric
normalized	I-Metric
score	E-Metric
has	O
the	O
opposite	O
disadvantage	O
by	O
effectively	O
discarding	O
very	O
easy	O
and	O
very	O
hard	O
games	O
from	O
the	O
comparison	O
.	O

As	O
typical	O
median	B-Metric
human	I-Metric
normalized	I-Metric
scores	E-Metric
are	O
within	O
the	O
range	O
of	O
1	O
-	O
2.5	O
,	O
an	O
algorithm	O
which	O
scores	O
zero	O
points	O
on	O
Montezuma	O
’s	O
Revenge	O
is	O
evaluated	O
equal	O
to	O
the	O
one	O
which	O
scores	O
2500	O
points	O
,	O
as	O
both	O
performance	O
levels	O
are	O
still	O
below	O
human	O
performance	O
making	O
incremental	O
improvements	O
on	O
hard	O
games	O
not	O
being	O
reflected	O
in	O
the	O
overall	O
evaluation	O
.	O

In	O
order	O
to	O
address	O
both	O
problem	O
,	O
we	O
also	O
evaluated	O
mean	B-Metric
rank	E-Metric
and	O
Elo	B-Metric
metrics	E-Metric
for	O
inter	B-Metric
-	I-Metric
algorithm	I-Metric
comparison	E-Metric
.	O

Those	O
metrics	O
implicitly	O
assign	O
the	O
same	O
weight	O
to	O
each	O
game	O
,	O
and	O
as	O
a	O
result	O
is	O
more	O
sensitive	O
of	O
relative	O
performance	O
on	O
very	O
hard	O
and	O
easy	O
games	O
:	O
swapping	O
scores	O
of	O
two	O
algorithms	O
on	O
any	O
game	O
would	O
result	O
in	O
the	O
change	O
of	O
both	O
mean	B-Metric
rank	E-Metric
and	O
Elo	B-Metric
metrics	E-Metric
.	O

We	O
calculated	O
separate	O
mean	B-Metric
rank	E-Metric
and	O
Elo	B-Metric
scores	E-Metric
for	O
each	O
algorithm	O
using	O
results	O
of	O
test	O
evaluations	O
with	O
30	O
random	O
noop	O
-	O
starts	O
and	O
30	O
random	O
human	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

All	O
algorithms	O
were	O
ranked	O
across	O
each	O
game	O
separately	O
,	O
and	O
a	O
mean	B-Metric
rank	E-Metric
was	O
evaluated	O
across	O
57	O
Atari	S-Material
games	O
.	O

For	O
Elo	B-Method
score	I-Method
evaluation	I-Method
algorithm	E-Method
,	O
was	O
considered	O
to	O
win	O
over	O
algorithm	O
if	O
it	O
obtained	O
more	O
scores	O
on	O
a	O
given	O
Atari	S-Material
.	O

We	O
produced	O
an	O
empirical	B-Metric
win	I-Metric
-	I-Metric
probability	I-Metric
matrix	E-Metric
by	O
summing	O
wins	O
across	O
all	O
games	O
and	O
used	O
this	O
matrix	O
to	O
evaluate	O
Elo	B-Metric
scores	E-Metric
.	O

A	O
ranking	O
difference	O
of	O
400	O
corresponds	O
to	O
the	O
odds	O
of	O
winning	O
of	O
10:1	O
under	O
the	O
Gaussian	B-Method
assumption	E-Method
.	O

subsection	O
:	O
Contextual	B-Method
priority	I-Method
tree	I-Method
Contextual	I-Method
priority	I-Method
tree	E-Method
is	O
one	O
possible	O
implementation	O
of	O
lazy	B-Task
prioritization	E-Task
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

All	O
sequence	O
keys	O
are	O
put	O
into	O
a	O
balanced	B-Method
binary	I-Method
search	I-Method
tree	E-Method
which	O
maintains	O
a	O
temporal	O
order	O
.	O

An	O
AVL	B-Method
tree	E-Method
(	O
)	O
was	O
chosen	O
due	O
to	O
the	O
ease	O
of	O
implementation	O
and	O
because	O
it	O
is	O
on	O
average	O
more	O
evenly	O
balanced	O
than	O
a	O
Red	O
-	O
Black	O
Tree	O
.	O

Each	O
tree	O
node	O
has	O
up	O
to	O
two	O
children	O
(	O
left	O
and	O
right	O
)	O
and	O
contains	O
currently	O
stored	O
key	O
and	O
a	O
priority	O
of	O
the	O
key	O
which	O
is	O
either	O
set	O
or	O
is	O
unknown	O
.	O

Some	O
trees	O
may	O
only	O
have	O
a	O
single	O
child	O
subtree	O
while	O
some	O
may	O
have	O
none	O
.	O

In	O
addition	O
to	O
this	O
information	O
,	O
we	O
were	O
tracking	O
other	O
summary	O
statistics	O
at	O
each	O
node	O
which	O
was	O
re	O
-	O
evaluated	O
after	O
each	O
tree	B-Method
rotation	E-Method
.	O

The	O
summary	B-Metric
statistics	E-Metric
was	O
evaluated	O
by	O
consuming	O
previously	O
evaluated	O
summary	B-Metric
statistics	E-Metric
of	O
both	O
children	O
and	O
a	O
priority	O
of	O
the	O
key	O
stored	O
within	O
the	O
current	O
node	O
.	O

In	O
particular	O
,	O
we	O
were	O
tracking	O
a	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
and	O
mean	O
-	O
priority	O
estimates	O
updated	O
according	O
to	O
rules	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
was	O
always	O
known	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
while	O
mean	O
priority	O
estimates	O
per	O
key	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
could	O
either	O
be	O
known	O
or	O
unknown	O
.	O

If	O
a	O
mean	O
priority	O
of	O
either	O
one	O
child	O
subtree	O
or	O
a	O
key	O
stored	O
within	O
the	O
current	O
node	O
is	O
unknown	O
then	O
it	O
can	O
be	O
estimated	O
to	O
by	O
exploiting	O
information	O
coming	O
from	O
another	O
sibling	O
subtree	O
or	O
a	O
priority	O
stored	O
within	O
the	O
parent	O
node	O
.	O

Sampling	O
was	O
done	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
up	O
while	O
sampling	O
either	O
one	O
of	O
the	O
children	O
subtrees	O
or	O
the	O
currently	O
held	O
key	O
proportionally	O
to	O
the	O
total	O
estimated	O
priority	O
masses	O
contained	O
within	O
.	O

The	O
rules	O
used	O
to	O
evaluate	O
proportions	O
are	O
shown	O
in	O
orange	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Similarly	O
,	O
probabilities	O
of	O
arbitrary	O
keys	O
can	O
be	O
queried	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
towards	O
the	O
child	O
node	O
of	O
an	O
interest	O
while	O
maintaining	O
a	O
product	O
of	O
probabilities	O
at	O
each	O
branching	O
point	O
.	O

Insertion	S-Method
,	O
deletion	S-Method
,	O
sampling	S-Method
and	O
probability	B-Method
query	I-Method
operations	E-Method
can	O
be	O
done	O
in	O
O	O
(	O
ln	O
(	O
n	O
)	O
)	O
time	O
.	O

The	O
suggested	O
algorithm	O
has	O
the	O
desired	O
property	O
that	O
it	O
becomes	O
a	O
simple	O
proportional	B-Method
sampling	I-Method
algorithm	E-Method
once	O
all	O
the	O
priorities	O
are	O
known	O
.	O

While	O
some	O
key	O
priorities	O
are	O
unknown	O
,	O
they	O
are	O
estimated	O
by	O
using	O
nearby	O
known	O
key	O
priorities	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Each	O
time	O
when	O
a	O
new	O
sequence	O
key	O
is	O
added	O
to	O
the	O
tree	O
,	O
it	O
was	O
set	O
to	O
have	O
an	O
unknown	O
priority	O
.	O

Any	O
priority	O
was	O
assigned	O
only	O
after	O
the	O
key	O
got	O
first	O
sampled	O
and	O
the	O
corresponding	O
sequence	O
got	O
passed	O
through	O
the	O
learner	O
.	O

When	O
a	O
priority	O
of	O
a	O
key	O
is	O
set	O
or	O
updated	O
,	O
the	O
key	O
node	O
is	O
deliberately	O
removed	O
from	O
and	O
placed	O
back	O
to	O
the	O
tree	O
in	O
order	O
to	O
become	O
a	O
leaf	O
-	O
node	O
.	O

This	O
helped	O
to	O
set	O
priorities	O
of	O
nodes	O
in	O
the	O
immediate	O
vicinity	O
more	O
accurately	O
by	O
using	O
the	O
freshest	O
information	O
available	O
.	O

subsection	O
:	O
Network	B-Method
architecture	E-Method
The	O
value	O
of	O
is	O
the	O
minimum	O
probability	O
of	O
choosing	O
a	O
random	O
action	O
and	O
it	O
is	O
hard	O
-	O
coded	O
into	O
the	O
policy	B-Method
network	E-Method
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
overall	O
network	O
topology	O
while	O
Table	O
[	O
reference	O
]	O
specifies	O
network	O
layer	O
sizes	O
.	O

subsection	O
:	O
Comparisons	O
with	O
Rainbow	O
In	O
this	O
section	O
we	O
compare	O
Reactor	S-Method
with	O
the	O
recently	O
published	O
Rainbow	B-Method
agent	I-Method
rainbow	E-Method
.	O

While	O
ACER	S-Method
is	O
the	O
most	O
closely	O
related	O
algorithmically	O
,	O
Rainbow	S-Method
is	O
most	O
closely	O
related	O
in	O
terms	O
of	O
performance	O
and	O
thus	O
a	O
deeper	O
understanding	O
of	O
the	O
trade	O
-	O
offs	O
between	O
Rainbow	S-Method
and	O
Reactor	S-Method
may	O
benefit	O
interested	O
readers	O
.	O

There	O
are	O
many	O
architectural	O
and	O
algorithmic	O
differences	O
between	O
Rainbow	S-Method
and	O
Reactor	S-Method
.	O

We	O
will	O
therefore	O
begin	O
by	O
highlighting	O
where	O
they	O
agree	O
.	O

Both	O
use	O
a	O
categorical	O
action	O
-	O
value	O
distribution	O
critic	O
bellemare2017distributional	O
,	O
factored	O
into	O
state	O
and	O
state	O
-	O
action	O
logits	O
wang2015dueling	O
,	O
Both	O
use	O
prioritized	B-Method
replay	E-Method
,	O
and	O
finally	O
,	O
both	O
perform	O
-	O
step	O
Bellman	B-Method
updates	E-Method
.	O

Despite	O
these	O
similarities	O
,	O
Reactor	S-Method
and	O
Rainbow	S-Method
are	O
fundamentally	O
different	O
algorithms	O
and	O
are	O
based	O
upon	O
different	O
lines	O
of	O
research	O
.	O

While	O
Rainbow	S-Method
uses	O
Q	B-Method
-	I-Method
Learning	E-Method
and	O
is	O
based	O
upon	O
DQN	S-Method
mnih15human	O
,	O
Reactor	S-Method
is	O
an	O
actor	B-Method
-	I-Method
critic	I-Method
algorithm	E-Method
most	O
closely	O
based	O
upon	O
A3C	S-Method
mnih2016asynchronous	O
.	O

Each	O
inherits	O
some	O
design	O
choices	O
from	O
their	O
predecessors	O
,	O
and	O
we	O
have	O
not	O
performed	O
an	O
extensive	O
ablation	O
comparing	O
these	O
various	O
differences	O
.	O

Instead	O
,	O
we	O
will	O
discuss	O
four	O
of	O
the	O
differences	O
we	O
believe	O
are	O
important	O
but	O
less	O
obvious	O
.	O

First	O
,	O
the	O
network	O
structures	O
are	O
substantially	O
different	O
.	O

Rainbow	S-Method
uses	O
noisy	B-Method
linear	I-Method
layers	E-Method
and	O
ReLU	O
activations	O
throughout	O
the	O
network	O
,	O
whereas	O
Reactor	S-Method
uses	O
standard	O
linear	B-Method
layers	E-Method
and	O
concatenated	O
ReLU	O
activations	O
throughout	O
.	O

To	O
overcome	O
partial	O
observability	O
,	O
Rainbow	S-Method
,	O
inheriting	O
this	O
choice	O
from	O
DQN	S-Method
,	O
uses	O
frame	B-Method
stacking	E-Method
.	O

On	O
the	O
other	O
hand	O
,	O
Reactor	S-Method
,	O
inheriting	O
its	O
choice	O
from	O
A3C	S-Method
,	O
uses	O
LSTMs	S-Method
after	O
the	O
convolutional	B-Method
layers	E-Method
of	O
the	O
network	O
.	O

It	O
is	O
also	O
difficult	O
to	O
directly	O
compare	O
the	O
number	O
of	O
parameters	O
in	O
each	O
network	O
because	O
the	O
use	O
of	O
noisy	B-Method
linear	I-Method
layers	E-Method
doubles	O
the	O
number	O
of	O
parameters	O
,	O
although	O
half	O
of	O
these	O
are	O
used	O
to	O
control	O
noise	O
,	O
while	O
the	O
LSTM	B-Method
units	E-Method
in	O
Reactor	S-Method
require	O
more	O
parameters	O
than	O
a	O
corresponding	O
linear	B-Method
layer	E-Method
would	O
.	O

Second	O
,	O
both	O
algorithms	O
perform	O
-	O
step	O
updates	O
,	O
however	O
,	O
the	O
Rainbow	B-Method
-	I-Method
step	I-Method
update	E-Method
does	O
not	O
use	O
any	O
form	O
of	O
off	B-Method
-	I-Method
policy	I-Method
correction	E-Method
.	O

Because	O
of	O
this	O
,	O
Rainbow	S-Method
is	O
restricted	O
to	O
using	O
only	O
small	O
values	O
of	O
(	O
e.g.	O
)	O
because	O
larger	O
values	O
would	O
make	O
sequences	O
more	O
off	O
-	O
policy	O
and	O
hurt	O
performance	O
.	O

By	O
comparison	O
,	O
Reactor	S-Method
uses	O
our	O
proposed	O
distributional	B-Method
Retrace	I-Method
algorithm	E-Method
for	O
off	B-Task
-	I-Task
policy	I-Task
correction	I-Task
of	I-Task
-	I-Task
step	I-Task
updates	E-Task
.	O

This	O
allows	O
the	O
use	O
of	O
larger	O
values	O
of	O
(	O
e.g.	O
)	O
without	O
loss	O
of	O
performance	O
.	O

Third	O
,	O
while	O
both	O
agents	O
use	O
prioritized	O
replay	O
buffers	O
schaul16prioritized	O
,	O
they	O
each	O
store	O
different	O
information	O
and	O
prioritize	O
using	O
different	O
algorithms	O
.	O

Rainbow	S-Method
stores	O
a	O
tuple	O
containing	O
the	O
state	O
,	O
action	O
,	O
sum	O
of	O
discounted	O
rewards	O
,	O
product	O
of	O
discount	O
factors	O
,	O
and	O
next	O
-	O
state	O
steps	O
away	O
.	O

Tuples	O
are	O
prioritized	O
based	O
upon	O
the	O
last	O
observed	O
TD	O
error	O
,	O
and	O
inserted	O
into	O
replay	O
with	O
a	O
maximum	O
priority	O
.	O

Reactor	S-Method
stores	O
length	O
sequences	O
of	O
tuples	O
and	O
also	O
prioritizes	O
based	O
upon	O
the	O
observed	O
TD	B-Metric
error	E-Metric
.	O

However	O
,	O
when	O
inserted	O
into	O
the	O
buffer	O
the	O
priority	O
is	O
instead	O
inferred	O
based	O
upon	O
the	O
known	O
priorities	O
of	O
neighboring	O
sequences	O
.	O

This	O
priority	B-Method
inference	E-Method
was	O
made	O
efficient	O
using	O
the	O
previously	O
introduced	O
contextual	B-Method
priority	I-Method
tree	E-Method
,	O
and	O
anecdotally	O
we	O
have	O
seen	O
it	O
improve	O
performance	O
over	O
a	O
simple	O
maximum	B-Method
priority	I-Method
approach	E-Method
.	O

Finally	O
,	O
the	O
two	O
algorithms	O
have	O
different	O
approaches	O
to	O
exploration	S-Task
.	O

Rainbow	S-Method
,	O
unlike	O
DQN	S-Method
,	O
does	O
not	O
use	O
-	B-Method
greedy	I-Method
exploration	E-Method
,	O
but	O
instead	O
replaces	O
all	O
linear	O
layers	O
with	O
noisy	O
linear	O
layers	O
which	O
induce	O
randomness	O
throughout	O
the	O
network	O
.	O

This	O
method	O
,	O
called	O
Noisy	B-Method
Networks	E-Method
fortunato2017noisy	O
,	O
creates	O
an	O
adaptive	B-Method
exploration	E-Method
integrated	O
into	O
the	O
agent	B-Method
’s	I-Method
network	E-Method
.	O

Reactor	S-Method
does	O
not	O
use	O
noisy	B-Method
networks	E-Method
,	O
but	O
instead	O
uses	O
the	O
same	O
entropy	B-Method
cost	I-Method
method	E-Method
used	O
by	O
A3C	O
and	O
many	O
others	O
mnih2016asynchronous	O
,	O
which	O
penalizes	O
deterministic	B-Method
policies	E-Method
thus	O
encouraging	O
indifference	O
between	O
similarly	O
valued	O
actions	O
.	O

Because	O
Rainbow	O
can	O
essentially	O
learn	O
not	O
to	O
explore	O
,	O
it	O
may	O
learn	O
to	O
become	O
entirely	O
greedy	O
in	O
the	O
early	O
parts	O
of	O
the	O
episode	O
,	O
while	O
still	O
exploring	O
in	O
states	O
not	O
as	O
frequently	O
seen	O
.	O

In	O
some	O
sense	O
,	O
this	O
is	O
precisely	O
what	O
we	O
want	O
from	O
an	O
exploration	B-Method
technique	E-Method
,	O
but	O
it	O
may	O
also	O
lead	O
to	O
highly	O
deterministic	O
trajectories	O
in	O
the	O
early	O
part	O
of	O
the	O
episode	O
and	O
an	O
increase	O
in	O
overfitting	O
to	O
those	O
trajectories	O
.	O

We	O
hypothesize	O
that	O
this	O
may	O
be	O
the	O
explanation	O
for	O
the	O
significant	O
difference	O
in	O
Rainbow	O
’s	O
performance	O
between	O
evaluation	S-Task
under	O
no	O
-	O
op	O
and	O
random	O
human	O
starts	O
,	O
and	O
why	O
Reactor	S-Method
does	O
not	O
show	O
such	O
a	O
large	O
difference	O
.	O

subsection	O
:	O
Atari	S-Material
results	O
Table	B-Task
-	I-Task
to	I-Task
-	I-Task
text	I-Task
Generation	E-Task
by	O
Structure	B-Method
-	I-Method
aware	E-Method
Seq2seq	B-Task
Learning	E-Task
section	O
:	O
Abstract	O
Table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
aims	O
to	O
generate	O
a	O
description	O
for	O
a	O
factual	O
table	O
which	O
can	O
be	O
viewed	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
.	O

To	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
novel	O
structure	O
-	O
aware	O
seq2seq	S-Method
architecture	O
which	O
consists	O
of	O
field	B-Method
-	I-Method
gating	I-Method
encoder	E-Method
and	O
description	B-Method
generator	E-Method
with	O
dual	O
attention	O
.	O

In	O
the	O
encoding	B-Task
phase	E-Task
,	O
we	O
update	O
the	O
cell	O
memory	O
of	O
the	O
LSTM	S-Method
unit	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
in	O
order	O
to	O
incorporate	O
field	O
information	O
into	O
table	B-Method
representation	E-Method
.	O

In	O
the	O
decoding	B-Task
phase	E-Task
,	O
dual	B-Method
attention	I-Method
mechanism	E-Method
which	O
contains	O
word	B-Method
level	I-Method
attention	E-Method
and	O
field	O
level	O
attention	O
is	O
proposed	O
to	O
model	O
the	O
semantic	O
relevance	O
between	O
the	O
generated	O
description	O
and	O
the	O
table	O
.	O

We	O
conduct	O
experiments	O
on	O
the	O
WIKIBIO	B-Material
dataset	E-Material
which	O
contains	O
over	O
700k	O
biographies	O
and	O
corresponding	O
infoboxes	O
from	O
Wikipedia	O
.	O

The	O
attention	B-Task
visualizations	E-Task
and	O
case	O
studies	O
show	O
that	O
our	O
model	O
is	O
capable	O
of	O
generating	O
coherent	O
and	O
informative	O
descriptions	O
based	O
on	O
the	O
comprehensive	O
understanding	O
of	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O

Automatic	O
evaluations	O
also	O
show	O
our	O
model	O
outperforms	O
the	O
baselines	O
by	O
a	O
great	O
margin	O
.	O

Code	O
for	O
this	O
work	O
is	O
available	O
on	O
https:	O
//	O
github.com	O
/	O
tyliupku	O
/	O
wiki2bio	O
.	O

section	O
:	O
Introduction	O
Generating	B-Task
natural	I-Task
language	I-Task
description	E-Task
for	O
a	O
structured	O
table	O
is	O
an	O
important	O
task	O
for	O
text	B-Task
generation	E-Task
from	O
structured	O
data	O
.	O

Previous	O
researches	O
include	O
weather	B-Task
forecast	E-Task
based	O
on	O
a	O
set	O
of	O
weather	O
records	O
[	O
reference	O
]	O
and	O
sportscasting	O
based	O
on	O
temporally	O
ordered	O
events	O
[	O
reference	O
]	O
.	O

However	O
,	O
previous	O
work	O
models	O
the	O
structured	O
data	O
in	O
the	O
limited	O
pre	O
-	O
defined	O
schemas	O
.	O

For	O
example	O
,	O
a	O
weather	O
record	O
rainChance	O
(	O
time:06:00	O
-	O
21:00	O
,	O
mode	O
:	O
SSE	O
,	O
value:20	O
)	O
is	O
represented	O
by	O
a	O
fixed	O
-	O
length	O
onehot	O
vector	O
by	O
its	O
record	O
type	O
,	O
record	O
time	O
,	O
record	O
value	O
and	O
record	O
value	O
.	O

To	O
this	O
end	O
,	O
we	O
focus	O
on	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
which	O
involves	O
comprehensive	B-Method
representation	E-Method
for	O
the	O
complex	O
structure	O
of	O
a	O
table	O
rather	O
than	O
pre	O
-	O
defined	O
schemas	O
.	O

In	O
contrast	O
to	O
previous	O
work	O
experimented	O
on	O
small	O
datasets	O
which	O
contain	O
only	O
a	O
few	O
tens	O
of	O
thousands	O
of	O
records	O
such	O
as	O
WEATHERGOV	S-Material
[	O
reference	O
]	O
and	O
ROBOCUP	S-Material
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
a	O
more	O
challenging	O
task	O
to	O
generate	O
biographies	O
The	O
Wikipedia	O
infobox	O
of	O
Charles	O
Winstead	O
,	O
the	O
corresponding	O
introduction	O
on	O
his	O
wiki	O
page	O
reads	O
"	O
Charles	O
Winstead	O
[	O
reference	O
]	O
was	O
an	O
FBI	O
agent	O
in	O
the	O
1930s	O
-	O
40s	O
,	O
famous	O
for	O
being	O
one	O
of	O
the	O
agents	O
who	O
shot	O
and	O
killed	O
John	O
Dillinger	O
.	O

"	O
.	O

based	O
on	O
the	O
Wikipedia	O
infoboxes	O
.	O

As	O
shown	O
in	O
Fig	O
1	O
,	O
a	O
biographic	O
infobox	O
is	O
a	O
fixed	O
-	O
format	O
table	O
that	O
describes	O
a	O
person	O
with	O
many	O
field	O
-	O
value	O
records	O
like	O
(	O
Name	O
,	O
Charles	O
B.	O
Winstead	O
)	O
,	O
(	O
Nationality	O
,	O
American	O
)	O
,	O
(	O
Occupation	O
,	O
FBI	O
Agent	O
)	O
,	O
etc	O
.	O

We	O
utilize	O
WIKIBIO	B-Material
dataset	E-Material
proposed	O
by	O
[	O
reference	O
]	O
which	O
contains	O
700k	O
biographies	O
from	O
Wikipedia	O
,	O
with	O
400k	O
words	O
in	O
total	O
as	O
the	O
benchmark	O
dataset	O
.	O

Previous	O
work	O
has	O
made	O
significant	O
progress	O
on	O
this	O
task	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
statistical	B-Method
n	I-Method
-	I-Method
gram	I-Method
model	E-Method
with	O
local	O
and	O
global	O
conditioning	O
on	O
a	O
Wikipedia	O
infobox	O
.	O

However	O
the	O
field	O
content	O
of	O
a	O
record	O
is	O
likely	O
to	O
be	O
a	O
sequence	O
of	O
words	O
,	O
the	O
statistical	B-Method
language	I-Method
model	E-Method
is	O
not	O
good	O
at	O
capturing	O
long	O
-	O
range	O
dependencies	O
between	O
words	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
selective	O
generation	S-Task
method	O
based	O
on	O
an	O
encoder	B-Method
-	I-Method
alignerdecoder	I-Method
framework	E-Method
.	O

The	O
model	O
utilizes	O
a	O
sparse	B-Method
one	I-Method
-	I-Method
hot	I-Method
vector	E-Method
to	O
represent	O
a	O
weather	O
record	O
.	O

However	O
it	O
's	O
inefficient	O
to	O
represent	O
the	O
complex	O
structure	O
of	O
a	O
table	O
by	O
one	O
-	O
hot	O
vectors	O
.	O

We	O
propose	O
a	O
structure	B-Method
-	I-Method
aware	I-Method
sequence	I-Method
to	I-Method
sequence	E-Method
(	O
seq2seq	S-Method
)	O
generation	S-Task
framework	O
to	O
model	O
both	O
content	O
and	O
structure	O
of	O
the	O
table	O
by	O
local	O
and	O
global	O
addressing	O
.	O

When	O
a	O
human	O
writes	O
a	O
biography	O
for	O
a	O
person	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
,	O
he	O
will	O
firstly	O
determine	O
which	O
records	O
in	O
the	O
table	O
should	O
be	O
included	O
in	O
the	O
introduction	O
and	O
how	O
to	O
arrange	O
the	O
order	O
of	O
these	O
records	O
before	O
wording	O
.	O

After	O
that	O
,	O
the	O
writer	O
will	O
further	O
consider	O
which	O
words	O
or	O
phrases	O
in	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
to	O
paraphrase	O
.	O

We	O
summarize	O
the	O
two	O
phases	O
of	O
generation	S-Task
as	O
two	O
scopes	O
of	O
addressing	S-Task
:	O
local	B-Task
and	I-Task
global	I-Task
addressing	E-Task
.	O

Local	B-Task
addressing	E-Task
determines	O
which	O
particular	O
word	O
in	O
the	O
table	O
should	O
be	O
focused	O
on	O
while	O
generating	O
a	O
piece	O
of	O
description	O
at	O
certain	O
time	O
step	O
.	O

However	O
,	O
the	O
word	B-Method
level	I-Method
addressing	E-Method
can	O
not	O
fully	O
address	O
the	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
problem	O
as	O
the	O
factual	O
tables	O
usually	O
have	O
complex	O
structures	O
which	O
might	O
confuse	O
the	O
generator	O
.	O

Global	B-Task
addressing	E-Task
is	O
proposed	O
to	O
determine	O
which	O
records	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
while	O
generating	O
corresponding	O
description	O
.	O

Global	B-Task
addressing	E-Task
is	O
necessary	O
as	O
the	O
description	O
of	O
a	O
table	O
may	O
not	O
cover	O
all	O
the	O
records	O
.	O

For	O
example	O
,	O
the	O
'cause	O
of	O
death	O
'	O
field	O
in	O
Fig	O
1	O
is	O
not	O
mentioned	O
in	O
the	O
description	O
.	O

Furthermore	O
,	O
the	O
order	O
of	O
records	O
in	O
the	O
tables	O
may	O
not	O
always	O
be	O
homogeneous	O
.	O

For	O
example	O
,	O
we	O
can	O
introduce	O
a	O
person	O
as	O
an	O
order	O
of	O
his	O
(	O
BirthDeath	O
-	O
Nationality	O
-	O
Occupation	O
)	O
according	O
to	O
his	O
Wikipedia	O
infobox	O
.	O

However	O
the	O
other	O
infoboxes	O
may	O
be	O
arranged	O
as	O
(	O
Occupation	O
-	O
Nationality	O
-	O
Birth	O
-	O
Death	O
)	O
.	O

Local	B-Task
addressing	E-Task
is	O
realized	O
by	O
content	O
encoding	O
of	O
the	O
LSTM	S-Method
encoder	O
and	O
word	B-Method
level	I-Method
attention	E-Method
while	O
global	B-Method
addressing	E-Method
is	O
realized	O
by	O
field	B-Method
encoding	E-Method
of	O
the	O
field	O
-	O
gating	O
LSTM	S-Method
variation	O
and	O
field	B-Method
level	I-Method
attention	E-Method
in	O
our	O
model	O
.	O

The	O
structure	O
-	O
aware	O
seq2seq	S-Method
architecture	O
we	O
proposed	O
exploits	O
encoder	B-Method
-	I-Method
decoder	I-Method
framework	E-Method
using	O
long	B-Method
short	I-Method
-	I-Method
term	I-Method
memory	E-Method
(	O
LSTM	S-Method
)	O
[	O
reference	O
]	O
units	O
with	O
local	O
and	O
global	O
addressing	O
on	O
the	O
structured	O
table	O
.	O

In	O
the	O
encoding	O
phase	O
,	O
our	O
model	O
first	O
encodes	O
the	O
sets	O
of	O
fieldvalue	O
records	O
in	O
the	O
table	O
by	O
integrating	O
field	O
information	O
and	O
content	B-Method
representation	E-Method
.	O

To	O
make	O
better	O
use	O
of	O
field	O
information	O
,	O
we	O
add	O
a	O
field	O
gate	O
to	O
the	O
cell	O
state	O
of	O
the	O
encoder	O
LSTM	S-Method
unit	O
to	O
incorporate	O
the	O
field	B-Method
embedding	E-Method
into	O
the	O
structural	O
representation	O
of	O
the	O
table	O
.	O

The	O
model	O
next	O
employs	O
a	O
LSTM	S-Method
decoder	O
to	O
generate	O
natural	B-Task
language	I-Task
description	E-Task
by	O
the	O
structural	B-Method
representation	E-Method
of	O
the	O
table	O
.	O

In	O
the	O
decoding	B-Task
phase	E-Task
,	O
we	O
also	O
propose	O
a	O
novel	O
dual	B-Method
attention	I-Method
mechanism	E-Method
which	O
consists	O
of	O
two	O
parts	O
:	O
word	B-Method
-	I-Method
level	I-Method
attention	E-Method
for	O
local	B-Task
addressing	E-Task
and	O
field	B-Task
-	I-Task
level	I-Task
attention	E-Task
for	O
global	B-Task
addressing	E-Task
.	O

Our	O
contributions	O
are	O
three	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
propose	O
an	O
endto	B-Method
-	I-Method
end	I-Method
structure	I-Method
-	I-Method
aware	I-Method
encoder	I-Method
-	I-Method
decoder	I-Method
architecture	E-Method
to	O
encode	O
field	O
information	O
into	O
the	O
representation	O
of	O
a	O
structured	O
table	O
.	O

(	O
2	O
)	O
Field	B-Method
-	I-Method
gating	I-Method
encoder	E-Method
and	O
dual	B-Method
attention	I-Method
mechanism	E-Method
are	O
proposed	O
to	O
operate	O
local	O
and	O
global	O
addressing	O
between	O
the	O
content	O
and	O
the	O
field	O
information	O
of	O
a	O
structured	O
table	O
.	O

(	O
3	O
)	O
Experiments	O
on	O
WIKIBIO	B-Material
dataset	E-Material
show	O
that	O
our	O
model	O
achieves	O
substantial	O
improvement	O
over	O
baselines	O
.	O

section	O
:	O
Related	O
Work	O
Most	O
generation	S-Task
systems	O
can	O
be	O
divided	O
into	O
two	O
independent	O
modules	O
:	O
(	O
1	O
)	O
content	O
selection	S-Task
involves	O
choosing	O
a	O
subset	O
of	O
relevant	O
records	O
in	O
a	O
table	O
to	O
talk	O
about	O
.	O

(	O
2	B-Task
)	I-Task
surface	I-Task
realization	E-Task
is	O
concerned	O
with	O
generating	B-Task
natural	I-Task
language	I-Task
descriptions	E-Task
for	O
this	O
subset	O
.	O

Many	O
approaches	O
have	O
been	O
proposed	O
to	O
learn	O
the	O
individual	O
modules	O
.	O

For	O
content	B-Task
selection	I-Task
module	E-Task
,	O
one	O
approach	O
builds	O
a	O
content	B-Method
selection	I-Method
model	E-Method
by	O
aligning	O
records	O
and	O
sentences	O
[	O
reference	O
][	O
reference	O
]	O
.	O

A	O
hierarchical	B-Method
semi	I-Method
-	I-Method
Markov	I-Method
method	E-Method
is	O
proposed	O
by	O
[	O
reference	O
]	O
which	O
first	O
associates	O
the	O
text	O
sequences	O
to	O
corresponding	O
records	O
and	O
then	O
generates	O
corresponding	O
descriptions	O
from	O
these	O
records	O
.	O

Surface	B-Task
realization	E-Task
is	O
often	O
treated	O
as	O
a	O
concept	B-Task
-	I-Task
to	I-Task
-	I-Task
text	I-Task
generation	I-Task
task	E-Task
from	O
a	O
given	O
representation	O
.	O

[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
utilize	O
various	O
linguistic	O
features	O
to	O
train	O
sentence	B-Method
planners	E-Method
for	O
sentence	O
generation	S-Task
.	O

Context	B-Method
-	I-Method
free	I-Method
grammars	E-Method
are	O
also	O
used	O
to	O
generate	O
natural	O
language	O
sentences	O
from	O
formal	B-Method
meaning	I-Method
representations	E-Method
[	O
reference	O
][	O
reference	O
]	O
.	O

Other	O
effective	O
approaches	O
include	O
hybrid	O
alignment	O
tree	O
[	O
reference	O
]	O
,	O
tree	B-Method
conditional	I-Method
random	I-Method
fields	E-Method
[	O
reference	O
]	O
,	O
tree	B-Method
adjoining	I-Method
grammar	E-Method
[	O
reference	O
]	O
and	O
template	B-Method
extraction	E-Method
in	O
a	O
log	B-Method
-	I-Method
linear	I-Method
framework	E-Method
[	O
reference	O
]	O
.	O

Recent	O
work	O
combines	O
content	B-Task
selection	E-Task
and	O
surface	B-Task
realization	E-Task
in	O
a	O
unified	O
framework	O
(	O
Ratnaparkhi	O
2002	O
;	O
[	O
reference	O
][	O
reference	O
]	O
Our	O
model	O
borrowed	O
the	O
idea	O
of	O
representing	O
a	O
structured	O
table	O
by	O
its	O
field	O
and	O
content	O
information	O
from	O
(	O
Lebret	O
,	O
Grangier	O
,	O
and	O
Auli	O
2016	O
)	O
.	O

However	O
,	O
their	O
n	B-Method
-	I-Method
gram	I-Method
model	E-Method
is	O
inefficient	O
to	O
model	O
long	O
-	O
range	O
dependencies	O
while	O
generating	O
descriptions	O
.	O

[	O
reference	O
]	O
also	O
proposed	O
a	O
seq2seq	S-Method
model	O
with	O
an	O
aligner	O
between	O
weather	O
records	O
and	O
weather	O
broadcast	O
.	O

The	O
model	O
used	O
one	B-Method
-	I-Method
hot	I-Method
encoding	E-Method
to	O
represent	O
the	O
weather	O
records	O
as	O
they	O
are	O
relatively	O
simple	O
and	O
highly	O
structured	O
.	O

However	O
,	O
the	O
model	O
is	O
not	O
capable	O
to	O
represent	O
the	O
tables	O
with	O
complex	O
structure	O
like	O
Wikipedia	O
infoboxes	O
.	O

section	O
:	O
Task	O
Definition	O
We	O
model	O
the	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
in	O
an	O
end	O
-	O
to	O
-	O
end	O
structure	O
-	O
aware	O
seq2seq	S-Method
framework	O
.	O

The	O
given	O
table	O
T	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
n	O
field	O
-	O
value	O
records	O
{	O
R	O
1	O
,	O
R	O
2	O
,	O
·	O
·	O
·	O
,	O
R	O
n	O
}	O
.	O

Each	O
record	O
R	O
i	O
consists	O
of	O
a	O
sequence	O
of	O
words	O
The	O
output	O
of	O
the	O
model	O
is	O
the	O
generated	O
description	O
S	O
for	O
table	O
T	O
which	O
contains	O
p	O
tokens	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
·	O
·	O
·	O
,	O
w	O
p	O
}	O
with	O
w	O
t	O
being	O
the	O
word	O
at	O
time	O
t.	O
We	O
formulate	O
the	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
as	O
the	O
inference	S-Task
over	O
a	O
probabilistic	B-Method
model	E-Method
.	O

The	O
goal	O
of	O
the	O
inference	S-Task
is	O
to	O
generate	O
a	O
sequence	O
w	O
*	O
1:p	O
which	O
maximizes	O
P	O
(	O
w	O
1:p	O
|R	O
1:n	O
)	O
.	O

bedding	O
for	O
a	O
segment	O
of	O
words	O
in	O
the	O
field	O
content	O
.	O

The	O
field	B-Method
embedding	E-Method
is	O
a	O
key	O
point	O
to	O
label	O
each	O
word	O
in	O
the	O
field	O
content	O
by	O
its	O
corresponding	O
field	O
name	O
and	O
occurrence	O
in	O
the	O
table	O
.	O

[	O
reference	O
]	O
represented	O
the	O
field	O
embeddding	O
Z	O
w	O
=	O
{	O
f	O
w	O
;	O
p	O
w	O
}	O
for	O
a	O
word	O
w	O
in	O
the	O
table	O
with	O
corresponding	O
field	O
name	O
f	O
w	O
and	O
position	O
information	O
p	O
w	O
.	O

The	O
position	O
information	O
can	O
be	O
further	O
represented	O
as	O
a	O
tuple	O
(	O
p	O
which	O
includes	O
the	O
positions	O
of	O
the	O
token	O
w	O
counted	O
from	O
the	O
begining	O
and	O
the	O
end	O
of	O
the	O
field	O
respectively	O
.	O

So	O
the	O
field	B-Method
embedding	I-Method
of	I-Method
token	I-Method
w	E-Method
is	O
extended	O
to	O
a	O
triple	O
:	O
As	O
shown	O
in	O
Fig	O
2	O
,	O
the	O
infobox	O
of	O
George	O
Mikell	O
contains	O
several	O
field	O
-	O
value	O
records	O
,	O
the	O
field	O
content	O
for	O
the	O
record	O
(	O
birthname	O
,	O
Jurgis	O
Mikelatitis	O
)	O
is	O
'	O
Jurgis	O
Mikelatitis	O
'	O
.	O

The	O
word	O
'	O
Jurgis	O
'	O
is	O
the	O
first	O
token	O
counted	O
from	O
the	O
beginning	O
of	O
the	O
field	O
'	O
birthname	O
'	O
and	O
also	O
the	O
second	O
token	O
counted	O
from	O
the	O
end	O
.	O

So	O
the	O
field	B-Method
embedding	E-Method
for	O
the	O
word	O
'	O
Jurgis	O
'	O
is	O
described	O
as	O
{	O
birthname	O
;	O
1	O
;	O
2}.	O
Each	O
token	O
in	O
the	O
table	O
has	O
an	O
unique	O
field	O
embedding	O
even	O
if	O
there	O
exists	O
two	O
same	O
words	O
in	O
the	O
same	O
field	O
due	O
to	O
the	O
unique	O
(	O
field	O
,	O
position	O
)	O
pair	O
.	O

section	O
:	O
Field	B-Method
-	I-Method
gating	I-Method
Table	I-Method
Encoder	E-Method
The	O
table	B-Method
encoder	E-Method
aims	O
to	O
encode	O
each	O
word	O
d	O
j	O
in	O
the	O
table	O
together	O
with	O
its	O
field	O
embedding	O
Z	O
dj	O
into	O
the	O
hidden	O
state	O
h	O
j	O
using	O
LSTM	S-Method
encoder	O
.	O

We	O
present	O
a	O
novel	O
field	O
-	O
gating	O
LSTM	S-Method
unit	O
to	O
incorporate	O
field	O
information	O
into	O
table	B-Task
encoding	E-Task
.	O

LSTM	S-Method
is	O
a	O
recurrent	B-Method
neural	I-Method
network	E-Method
(	O
RNN	S-Method
)	O
architecture	O
which	O
uses	O
a	O
vector	O
of	O
cell	O
state	O
c	O
t	O
and	O
a	O
set	O
of	O
element	O
-	O
wise	O
multiplication	O
gates	O
to	O
control	O
how	O
information	O
is	O
stored	O
,	O
forgotten	O
and	O
exploited	O
inside	O
the	O
network	O
.	O

Following	O
the	O
design	O
for	O
an	O
LSTM	S-Method
cell	O
in	O
[	O
reference	O
]	O
,	O
the	O
architecture	O
used	O
in	O
the	O
table	B-Method
encoder	E-Method
is	O
defined	O
by	O
following	O
equations	O
:	O
where	O
i	O
t	O
,	O
f	O
t	O
,	O
o	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
are	O
input	O
,	O
forget	O
and	O
output	O
gates	O
respectively	O
,	O
andĉ	O
t	O
and	O
c	O
t	O
are	O
proposed	O
cell	O
value	O
and	O
true	O
cell	O
value	O
in	O
time	O
t.	O
n	O
is	O
the	O
hidden	O
size	O
.	O

To	O
make	O
better	O
understanding	O
of	O
the	O
structure	O
of	O
a	O
table	O
,	O
the	O
field	O
information	O
should	O
also	O
be	O
encoded	O
into	O
the	O
encoder	S-Method
.	O

One	O
simple	O
way	O
is	O
to	O
take	O
the	O
concatenation	B-Method
of	I-Method
word	I-Method
embedding	E-Method
and	O
corresponding	O
field	B-Method
embedding	E-Method
as	O
the	O
input	O
for	O
the	O
vanilla	O
LSTM	S-Method
unit	O
.	O

Actually	O
,	O
the	O
method	O
is	O
indeed	O
proved	O
to	O
be	O
useful	O
in	O
our	O
experiments	O
and	O
serves	O
as	O
a	O
baseline	O
for	O
comparison	O
.	O

However	O
,	O
the	O
concatenation	B-Method
of	I-Method
word	I-Method
embedding	E-Method
and	O
field	B-Method
embedding	E-Method
only	O
treats	O
the	O
field	O
information	O
as	O
an	O
additional	O
label	O
of	O
certain	O
token	O
which	O
loses	O
the	O
structural	O
information	O
of	O
the	O
table	O
.	O

To	O
better	O
encode	O
the	O
structural	O
information	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
field	B-Method
-	I-Method
gating	I-Method
variation	E-Method
on	O
the	O
vanilla	O
LSTM	S-Method
unit	O
to	O
update	O
the	O
cell	O
memory	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
.	O

The	O
field	O
-	O
gating	O
cell	O
state	O
is	O
described	O
as	O
follows	O
:	O
where	O
z	O
t	O
is	O
the	O
field	O
embedding	O
described	O
before	O
,	O
l	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
is	O
the	O
field	O
gate	O
to	O
determine	O
how	O
much	O
field	O
information	O
should	O
be	O
kept	O
in	O
the	O
cell	O
memory	O
,	O
ẑ	O
t	O
is	O
the	O
proposed	O
field	O
value	O
corresponding	O
to	O
field	O
gate	O
.	O

The	O
cell	O
state	O
c	O
t	O
is	O
updated	O
from	O
the	O
original	O
c	O
t	O
by	O
incorporating	O
field	O
information	O
of	O
the	O
table	O
.	O

section	O
:	O
Description	B-Method
Decoder	E-Method
with	O
Dual	B-Method
Attention	E-Method
To	O
conduct	O
local	B-Task
and	I-Task
global	I-Task
addressing	E-Task
towards	O
the	O
structured	O
table	O
,	O
we	O
use	O
LSTM	S-Method
architecture	O
with	O
dual	B-Method
attention	I-Method
mechanism	E-Method
as	O
our	O
description	B-Method
generator	E-Method
.	O

As	O
defined	O
in	O
the	O
equation	O
1	O
,	O
the	O
generated	O
token	O
w	O
t	O
at	O
time	O
t	O
in	O
the	O
decoder	O
is	O
predicated	O
based	O
on	O
all	O
the	O
previously	O
generated	O
tokens	O
w	O
<	O
t	O
before	O
w	O
t	O
,	O
the	O
hidden	O
states	O
H	O
=	O
{	O
h	O
t	O
}	O
L	O
t=1	O
of	O
the	O
table	B-Method
encoder	E-Method
and	O
the	O
field	B-Method
embeddings	E-Method
Z	O
=	O
{	O
z	O
t	O
}	O
L	O
t=1	O
.	O

To	O
be	O
more	O
specific	O
:	O
where	O
s	O
t	O
is	O
the	O
t	O
-	O
th	O
hidden	O
state	O
of	O
the	O
decoder	O
calculated	O
by	O
the	O
LSTM	S-Method
unit	O
.	O

The	O
computational	O
details	O
can	O
be	O
referred	O
in	O
Equation	O
3	O
,	O
4	O
and	O
5	O
.	O

a	O
t	O
is	O
the	O
attention	O
vector	O
which	O
is	O
widely	O
used	O
in	O
many	O
applications	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Vanilla	B-Method
attention	I-Method
mechanism	E-Method
is	O
proposed	O
to	O
encode	O
the	O
semantic	O
relevance	O
between	O
the	O
encoder	O
states	O
{	O
h	O
t	O
}	O
L	O
t=1	O
and	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
time	O
t.	O
The	O
attention	O
vector	O
is	O
usually	O
represented	O
by	O
the	O
weighted	O
sum	O
of	O
encoder	O
hidden	O
states	O
.	O

However	O
,	O
the	O
word	B-Method
level	I-Method
attention	E-Method
described	O
above	O
can	O
only	O
capture	O
the	O
semantic	O
relevance	O
between	O
generated	O
tokens	O
and	O
the	O
content	O
information	O
in	O
the	O
table	O
,	O
ignoring	O
the	O
structure	O
information	O
of	O
the	O
table	O
.	O

To	O
fully	O
utilize	O
the	O
structure	O
information	O
,	O
we	O
propose	O
a	O
higher	O
level	O
attention	O
over	O
generated	O
tokens	O
and	O
the	O
field	O
embedding	O
of	O
the	O
table	O
.	O

Field	O
level	O
attention	O
can	O
locate	O
the	O
particular	O
field	O
-	O
value	O
record	O
which	O
should	O
be	O
focused	O
on	O
while	O
generating	O
next	O
token	O
in	O
the	O
description	O
by	O
modeling	O
the	O
relevance	O
between	O
all	O
field	O
embeddings	O
{	O
z	O
t	O
}	O
L	O
t=1	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
t	O
-	O
th	O
time	O
.	O

Field	O
level	O
attention	O
weight	O
β	B-Method
ti	E-Method
is	O
presented	O
as	O
Equation	O
13	O
.	O

We	O
use	O
the	O
same	O
relevant	O
score	O
function	O
g	O
(	O
s	O
t	O
,	O
z	O
i	O
)	O
as	O
equation	O
12	O
.	O

Dual	O
attention	O
weight	O
γ	O
t	O
is	O
the	O
element	O
-	O
wise	O
production	O
between	O
field	O
level	O
attention	O
weight	O
β	O
t	O
and	O
word	B-Method
level	I-Method
attention	E-Method
weight	O
α	O
t	O
.	O

The	O
dual	O
attention	O
vector	O
a	O
t	O
is	O
updated	O
as	O
the	O
weighted	O
sum	O
of	O
encoder	O
states	O
{	O
h	O
t	O
}	O
t=1	O
by	O
γ	O
t	O
(	O
Equation	O
15	O
)	O
:	O
Furthermore	O
,	O
we	O
utilize	O
a	O
post	B-Method
-	I-Method
process	I-Method
operation	E-Method
for	O
the	O
generated	O
unknown	O
(	O
UNK	O
)	O
tokens	O
to	O
alleviate	O
the	O
out	B-Task
-	I-Task
ofvocabulary	E-Task
(	O
OOV	S-Task
)	O
problem	O
.	O

We	O
replace	O
a	O
specific	O
generated	O
UNK	O
token	O
with	O
the	O
most	O
relevant	O
token	O
in	O
the	O
corresponding	O
table	O
according	O
to	O
the	O
related	O
dual	O
attention	O
matrix	O
.	O

section	O
:	O
Local	B-Task
and	I-Task
Global	I-Task
Addressing	E-Task
Local	O
and	O
global	O
addressing	O
determine	O
which	O
part	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
in	O
different	O
steps	O
of	O
description	O
generation	S-Task
.	O

The	O
two	O
scopes	O
of	O
addressings	O
play	O
a	O
very	O
important	O
role	O
in	O
understanding	O
and	O
representing	O
the	O
innerstructure	O
of	O
a	O
table	O
.	O

Next	O
we	O
will	O
introduce	O
how	O
our	O
model	O
conducts	O
local	B-Task
and	I-Task
global	I-Task
addressing	E-Task
on	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
with	O
the	O
help	O
of	O
Fig	O
3	O
.	O

Local	B-Task
addressing	E-Task
:	O
A	O
table	O
can	O
be	O
treated	O
as	O
a	O
set	O
of	O
fieldvalue	O
records	O
.	O

Local	B-Task
addressing	E-Task
tends	O
to	O
encode	O
the	O
table	O
content	O
inside	O
each	O
record	O
.	O

The	O
value	O
in	O
each	O
field	O
-	O
value	O
record	O
is	O
a	O
sequence	O
of	O
words	O
which	O
contains	O
2.7	O
tokens	O
on	O
average	O
.	O

Some	O
records	O
in	O
the	O
Wikipedia	O
infoboxes	O
even	O
contain	O
several	O
phrases	O
or	O
sentences	O
.	O

Previous	O
models	O
which	O
used	O
one	B-Method
-	I-Method
hot	I-Method
encoding	E-Method
or	O
statistical	B-Method
language	I-Method
model	E-Method
to	O
encode	O
field	O
content	O
are	O
inefficient	O
to	O
capture	O
the	O
semantic	O
relevance	O
between	O
words	O
inside	O
a	O
field	O
.	O

The	O
seq2seq	S-Method
structure	O
itself	O
has	O
a	O
strong	O
ability	O
to	O
model	O
the	O
context	O
of	O
a	O
piece	O
of	O
words	O
.	O

For	O
one	O
thing	O
,	O
the	O
LSTM	S-Method
encoder	O
can	O
capture	O
longrange	O
dependencies	O
between	O
words	O
in	O
the	O
table	O
.	O

For	O
another	O
,	O
the	O
word	B-Method
level	I-Method
attention	E-Method
of	O
the	O
proposed	O
dual	B-Method
attention	I-Method
mechanism	E-Method
can	O
also	O
build	O
a	O
connection	O
between	O
the	O
words	O
in	O
the	O
description	O
and	O
the	O
tokens	O
in	O
the	O
table	O
.	O

The	O
generated	O
word	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
refers	O
to	O
the	O
word	O
'	O
actor	O
'	O
in	O
the	O
'	O
Occupation	O
'	O
field	O
.	O

Global	B-Task
addressing	E-Task
:	O
The	O
goal	O
of	O
local	B-Task
addressing	E-Task
is	O
to	O
represent	O
inner	O
-	O
record	O
information	O
while	O
global	B-Task
addressing	E-Task
aims	O
to	O
model	O
inter	O
-	O
record	O
relevance	O
within	O
the	O
table	O
.	O

For	O
example	O
,	O
it	O
's	O
noteworthy	O
that	O
the	O
generated	O
token	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
is	O
mapped	O
to	O
the	O
'	O
occupation	O
'	O
field	O
in	O
Table	O
2	O
.	O

Field	B-Method
-	I-Method
gating	E-Method
[	O
reference	O
]	O
.	O

We	O
should	O
make	O
it	O
clear	O
which	O
records	O
the	O
token	O
to	O
be	O
generated	O
is	O
focused	O
on	O
by	O
global	O
addressing	O
between	O
the	O
field	O
information	O
of	O
a	O
table	O
and	O
its	O
description	O
.	O

The	O
field	O
level	O
attention	O
of	O
dual	B-Method
attention	I-Method
mechanism	E-Method
is	O
introduced	O
to	O
determine	O
which	O
field	O
the	O
generator	O
focused	O
on	O
in	O
certain	O
time	O
step	O
.	O

Experiments	O
show	O
that	O
our	O
dual	B-Method
attention	I-Method
mechanism	E-Method
is	O
of	O
great	O
help	O
to	O
generate	O
description	O
from	O
certain	O
table	O
and	O
insensible	O
to	O
different	O
orders	O
of	O
table	O
records	O
.	O

section	O
:	O
Experiments	O
We	O
first	O
introduce	O
the	O
dataset	O
,	O
evaluation	B-Metric
metrics	E-Metric
and	O
experimental	O
setups	O
in	O
our	O
experiments	O
.	O

Then	O
we	O
compare	O
our	O
model	O
with	O
several	O
baselines	O
.	O

After	O
that	O
,	O
we	O
assess	O
the	O
performance	O
of	O
our	O
model	O
on	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
.	O

Furthermore	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
the	O
disordered	O
tables	O
to	O
show	O
the	O
efficiency	O
of	O
global	B-Method
addressing	I-Method
mechanism	E-Method
.	O

section	O
:	O
Dataset	O
and	O
Evaluation	B-Metric
Metrics	E-Metric
We	O
use	O
WIKBIO	B-Material
dataset	E-Material
proposed	O
by	O
[	O
reference	O
]	O
as	O
the	O
benchmark	O
dataset	O
.	O

WIKBIO	S-Material
contains	O
728	O
,	O
321	O
articles	O
from	O
English	O
Wikipedia	O
[	O
reference	O
]	O
.	O

The	O
dataset	O
uses	O
the	O
first	O
sentence	O
of	O
each	O
article	O
as	O
the	O
description	O
of	O
the	O
corresponding	O
infobox	O
.	O

Table	O
1	O
summarizes	O
the	O
dataset	O
statistics	O
:	O
on	O
average	O
,	O
the	O
tokens	O
in	O
the	O
table	O
(	O
53.1	O
)	O
are	O
twice	O
as	O
long	O
as	O
those	O
in	O
the	O
first	O
sentence	O
(	O
26.1	O
)	O
.	O

9.5	O
tokens	O
in	O
the	O
description	O
text	O
also	O
occur	O
in	O
the	O
table	O
.	O

The	O
corpus	O
has	O
been	O
divided	O
in	O
to	O
training	O
(	O
80	O
%	O
)	O
,	O
testing	O
(	O
10	O
%	O
)	O
and	O
validation	S-Metric
(	O
10	O
%	O
)	O
sets	O
.	O

We	O
assess	O
the	O
generation	S-Task
quality	O
automatically	O
with	O
BLEU	B-Metric
-	I-Metric
4	E-Metric
and	O
ROUGE	B-Metric
-	I-Metric
4	E-Metric
(	O
F	B-Metric
measure	E-Metric
)	O
1	O
.	O

section	O
:	O
Baselines	O
We	O
compare	O
the	O
proposed	O
structure	O
-	O
aware	O
seq2seq	S-Method
model	O
with	O
several	O
statistical	B-Method
language	I-Method
models	E-Method
and	O
the	O
vanilla	B-Method
encoder	I-Method
-	I-Method
decoder	I-Method
model	E-Method
.	O

The	O
baselines	O
are	O
listed	O
as	O
follows	O
:	O
•	O
KN	S-Method
:	O
The	O
Kneser	B-Method
-	I-Method
Ney	E-Method
(	O
KN	S-Method
)	O
model	O
is	O
a	O
widely	O
used	O
language	B-Method
model	E-Method
proposed	O
by	O
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
KenLM	B-Method
toolkit	E-Method
to	O
train	O
5	B-Method
-	I-Method
gram	I-Method
models	E-Method
without	O
pruning	S-Method
.	O

•	O
Template	O
KN	S-Method
:	O
Template	O
KN	S-Method
is	O
a	O
KN	S-Method
model	O
over	O
templates	O
which	O
also	O
serves	O
as	O
a	O
baseline	O
in	O
[	O
reference	O
]	O
.	O

The	O
model	O
replaces	O
the	O
words	O
occurring	O
in	O
both	O
the	O
table	O
and	O
the	O
training	O
sentences	O
with	O
a	O
special	O
token	O
reflecting	O
its	O
field	O
.	O

The	O
introduction	O
section	O
of	O
the	O
table	O
in	O
Fig	O
2	O
looks	O
as	O
follows	O
under	O
this	O
scheme	O
:	O
"	O
name	O
1	O
name	O
2	O
(	O
born	O
birthname	O
1	O
...	O
birthdate	O
3	O
)	O
is	O
a	O
LithuanianAustralian	O
occupation	O
1	O
and	O
occupation	O
3	O
best	O
known	O
for	O
his	O
performances	O
in	O
known	O
for	O
1	O
...	O
known	O
for	O
4	O
(	O
1961	O
)	O
and	O
known	O
for	O
5	O
...	O
known	O
for	O
7	O
(	O
1963	O
)	O
"	O
.	O

During	O
inference	S-Task
,	O
the	O
decoder	S-Method
is	O
constrained	O
to	O
emit	O
words	O
from	O
the	O
regular	O
vocabulary	O
or	O
special	O
tokens	O
occurring	O
in	O
the	O
input	O
table	O
.	O

•	O
NLM	S-Method
:	O
A	O
naive	B-Method
statistical	I-Method
language	I-Method
model	E-Method
proposed	O
by	O
[	O
reference	O
]	O
for	O
comparison	O
.	O

The	O
model	O
uses	O
only	O
the	O
field	O
content	O
as	O
input	O
without	O
field	O
and	O
position	O
information	O
.	O

•	O
Table	O
NLM	S-Method
:	O
The	O
most	O
competitive	O
statistical	B-Method
language	I-Method
model	E-Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
which	O
includes	O
local	O
and	O
global	O
conditioning	O
over	O
the	O
table	O
by	O
integrating	O
related	O
field	O
and	O
position	B-Method
embedding	E-Method
into	O
the	O
table	B-Method
representation	E-Method
.	O

•	O
Vanilla	B-Method
Seq2seq	E-Method
:	O
The	O
vanilla	O
seq2seq	S-Method
neural	O
architecture	O
is	O
also	O
provided	O
as	O
a	O
strong	O
baseline	O
which	O
uses	O
the	O
concatenation	B-Method
of	I-Method
word	I-Method
embedding	E-Method
,	O
field	B-Method
embedding	E-Method
and	O
position	B-Method
embedding	E-Method
as	O
the	O
model	O
input	O
.	O

The	O
model	O
can	O
operate	O
local	B-Task
addressing	E-Task
over	O
the	O
table	O
by	O
the	O
natural	O
advantages	O
of	O
LSTM	S-Method
units	O
and	O
word	B-Method
level	I-Method
attention	E-Method
mechanism	O
.	O

section	O
:	O
Experiment	O
Setup	O
In	O
the	O
table	B-Task
encoding	I-Task
phase	E-Task
,	O
we	O
use	O
a	O
sequence	O
of	O
word	O
embeddings	O
and	O
their	O
corresponding	O
field	B-Method
and	I-Method
position	I-Method
embedding	E-Method
as	O
input	O
.	O

We	O
select	O
the	O
most	O
frequent	O
20	O
,	O
000	O
words	O
in	O
the	O
training	O
set	O
as	O
the	O
word	O
vocabulary	O
.	O

For	O
field	B-Task
embedding	E-Task
,	O
we	O
select	O
1480	O
fields	O
occurring	O
more	O
than	O
100	O
times	O
from	O
the	O
training	O
set	O
as	O
field	O
vocabulary	O
.	O

Additionally	O
,	O
we	O
filter	O
all	O
empty	O
fields	O
whose	O
values	O
are	O
none	O
while	O
feeding	O
field	O
information	O
to	O
the	O
network	O
.	O

We	O
also	O
limit	O
the	O
largest	O
position	O
number	O
as	O
30	O
.	O

Any	O
position	O
number	O
over	O
30	O
will	O
be	O
counted	O
as	O
30	O
.	O

While	O
generating	O
description	O
for	O
the	O
table	O
,	O
a	O
special	O
start	O
token	O
sos	O
is	O
feed	O
into	O
the	O
generator	O
in	O
the	O
beginning	O
of	O
the	O
Note	O
there	O
are	O
two	O
adjacent	O
'	O
belgium	O
's	O
in	O
'	O
birthplace	O
-	O
3	O
'	O
and	O
'	O
nationality	O
-	O
1	O
'	O
field	O
,	O
respectively	O
.	O

The	O
word	B-Method
level	I-Method
attention	E-Method
focuses	O
improperly	O
on	O
the	O
first	O
'	O
belgium	O
'	O
while	O
generating	O
'	O
a	O
belgian	O
film	O
director	O
'	O
.	O

In	O
contrast	O
,	O
the	O
field	O
level	O
attention	O
and	O
dual	O
attention	O
can	O
locate	O
the	O
second	O
'	O
belgium	O
'	O
properly	O
by	O
word	B-Method
-	I-Method
field	I-Method
modeling	E-Method
(	O
marked	O
in	O
the	O
black	O
boxes	O
)	O
.	O

Table	O
3	O
:	O
BLEU	B-Metric
-	I-Metric
4	E-Metric
and	O
ROUGE	B-Metric
-	I-Metric
4	E-Metric
for	O
structure	O
-	O
aware	O
seq2seq	S-Method
model	O
(	O
last	O
three	O
rows	O
)	O
,	O
statistical	B-Method
language	I-Method
model	E-Method
(	O
first	O
four	O
rows	O
)	O
and	O
vanilla	O
seq2seq	S-Method
model	O
with	O
field	O
and	O
position	O
input	O
(	O
three	O
rows	O
in	O
the	O
middle	O
)	O
.	O

section	O
:	O
Model	O
decoding	B-Task
phase	E-Task
.	O

Then	O
we	O
use	O
the	O
last	O
generated	O
token	O
as	O
the	O
input	O
at	O
the	O
next	O
time	O
step	O
.	O

A	O
special	O
end	O
token	O
eos	O
is	O
used	O
to	O
mark	O
the	O
end	O
of	O
decoding	S-Task
.	O

We	O
also	O
restrict	O
the	O
generated	O
text	O
by	O
a	O
pre	O
-	O
defined	O
max	O
length	O
to	O
avoid	O
redundant	O
or	O
irrelevant	O
generation	S-Task
.	O

We	O
also	O
try	O
beam	B-Method
search	E-Method
with	O
beam	O
size	O
2	O
-	O
10	O
to	O
enhance	O
the	O
performance	O
.	O

We	O
use	O
grid	B-Method
search	E-Method
to	O
determine	O
the	O
parameters	O
of	O
our	O
model	O
.	O

The	O
detail	O
of	O
model	O
parameters	O
is	O
listed	O
in	O
Table	O
2	O
.	O

section	O
:	O
Generation	B-Task
Assessment	E-Task
The	O
assessment	O
for	O
description	O
generation	S-Task
is	O
listed	O
in	O
Table	O
3	O
.	O

We	O
have	O
following	O
observations	O
:	O
(	O
1	O
)	O
Neural	B-Method
network	I-Method
models	E-Method
perform	O
much	O
better	O
than	O
statistical	B-Method
language	I-Method
models	E-Method
.	O

Even	O
vanilla	O
seq2seq	S-Method
architecture	O
with	O
word	B-Method
level	I-Method
attention	E-Method
outperform	O
the	O
most	O
competitive	O
statistical	B-Method
model	E-Method
by	O
a	O
great	O
margin	O
.	O

(	O
2	O
)	O
The	O
proposed	O
structure	O
-	O
aware	O
seq2seq	S-Method
architecture	O
can	O
further	O
improve	O
the	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
compared	O
with	O
the	O
competitive	O
vanilla	O
seq2seq	S-Method
.	O

Dual	B-Method
attention	I-Method
mechanism	E-Method
is	O
able	O
to	O
boost	O
the	O
model	O
performance	O
by	O
over	O
1	O
BLEU	S-Metric
compared	O
to	O
vanilla	B-Method
attention	I-Method
mechanism	E-Method
.	O

section	O
:	O
Research	O
on	O
Disordered	O
Tables	O
We	O
view	O
a	O
structured	O
table	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
and	O
then	O
feed	O
the	O
records	O
into	O
the	O
generator	O
sequentially	O
as	O
the	O
order	O
they	O
are	O
presented	O
in	O
the	O
table	O
.	O

The	O
order	O
of	O
records	O
can	O
guide	O
the	O
description	B-Method
generator	E-Method
to	O
produce	O
an	O
introduction	O
in	O
the	O
pre	O
-	O
defined	O
schemas	O
[	O
reference	O
]	O
.	O

However	O
,	O
not	O
all	O
the	O
tables	O
are	O
arranged	O
in	O
the	O
proper	O
order	O
.	O

So	O
global	O
addressing	O
between	O
the	O
generated	O
descriptions	O
and	O
the	O
records	O
of	O
the	O
table	O
is	O
necessary	O
for	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
.	O

Furthermore	O
,	O
the	O
schemas	O
of	O
various	O
types	O
of	O
tables	O
differ	O
greatly	O
from	O
each	O
other	O
.	O

A	O
biography	O
about	O
a	O
politician	O
may	O
emphasize	O
his	O
or	O
her	O
social	O
activities	O
and	O
working	O
experience	O
while	O
a	O
biography	O
of	O
a	O
soccer	O
player	O
is	O
likely	O
to	O
highlight	O
which	O
team	O
he	O
or	O
she	O
used	O
to	O
serve	O
in	O
or	O
the	O
performance	O
in	O
his	O
or	O
her	O
career	O
.	O

To	O
cope	O
with	O
various	O
schemas	O
of	O
different	O
tables	O
,	O
it	O
's	O
essential	O
to	O
model	O
inter	O
-	O
record	O
information	O
within	O
the	O
tables	O
by	O
global	B-Method
addressing	E-Method
.	O

For	O
these	O
reasons	O
,	O
we	O
propose	O
a	O
pair	O
of	O
disordered	O
training	O
and	O
testing	O
set	O
based	O
on	O
WIKIBIO	S-Material
by	O
randomly	O
shuffling	O
the	O
records	O
of	O
a	O
infobox	O
.	O

For	O
example	O
,	O
the	O
order	O
of	O
several	O
records	O
in	O
a	O
specific	O
infobox	O
is	O
'	O
name	O
-	O
birthdateoccupation	O
-	O
spouse	O
'	O
,	O
we	O
randomly	O
shuffle	O
the	O
table	O
records	O
as	O
'	O
occupation	O
-	O
name	O
-	O
spouse	O
-	O
birthdate	O
'	O
,	O
without	O
changing	O
the	O
field	O
content	O
inside	O
the	O
'	O
occupation	O
'	O
,	O
'	O
name	O
'	O
,	O
'	O
spouse	O
'	O
and	O
'	O
birthdate	O
'	O
records	O
.	O

The	O
generated	O
descriptions	O
for	O
Binky	O
Jones	O
and	O
the	O
corresponding	O
reference	O
in	O
the	O
Wikipedia	O
.	O

Our	O
proposed	O
structaware	O
seq2seq	S-Method
model	O
can	O
generate	O
more	O
informative	O
and	O
accurate	O
description	O
compared	O
to	O
vanilla	O
seq2seq	S-Method
model	O
.	O

Table	O
4	O
shows	O
that	O
all	O
three	O
neural	B-Method
network	I-Method
models	E-Method
perform	O
not	O
as	O
good	O
as	O
before	O
,	O
which	O
means	O
the	O
order	O
of	O
table	O
records	O
is	O
an	O
essential	O
aspect	O
for	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
.	O

However	O
,	O
the	O
BLEU	S-Metric
and	O
ROUGE	S-Metric
decreases	O
on	O
the	O
structureaware	O
seq2seq	S-Method
model	O
are	O
much	O
smaller	O
than	O
the	O
other	O
two	O
models	O
,	O
which	O
proves	O
the	O
efficiency	O
of	O
global	B-Method
addressing	I-Method
mechanism	E-Method
.	O

section	O
:	O
Model	O
section	O
:	O
Qualitative	B-Task
Analysis	I-Task
Analysis	E-Task
on	O
Dual	B-Task
Attention	E-Task
Dual	B-Method
attention	I-Method
mechanism	E-Method
models	O
the	O
relationship	O
between	O
the	O
generated	O
tokens	O
and	O
table	O
content	O
inside	O
each	O
record	O
by	O
word	B-Method
level	I-Method
attention	E-Method
while	O
encoding	O
the	O
relevance	O
of	O
generated	O
description	O
and	O
inter	O
-	O
record	O
information	O
within	O
the	O
table	O
by	O
field	O
level	O
attention	O
.	O

The	O
aggregation	O
of	O
word	B-Method
level	I-Method
attention	E-Method
and	O
field	O
level	O
attention	O
can	O
model	O
more	O
precise	O
connection	O
between	O
the	O
table	O
and	O
its	O
generated	O
description	O
.	O

Fig	O
4	O
shows	O
an	O
example	O
of	O
the	O
three	O
attention	B-Method
mechanisms	E-Method
while	O
generating	O
a	O
piece	O
of	O
description	O
for	O
Frédéric	O
Fonteyne	O
based	O
on	O
his	O
Wikipedia	O
infobox	O
.	O

We	O
can	O
find	O
out	O
that	O
the	O
name	O
,	O
birthdate	O
,	O
nationality	O
and	O
occupation	O
information	O
contained	O
in	O
the	O
generated	O
sentence	O
can	O
properly	O
refer	O
to	O
the	O
related	O
table	O
content	O
by	O
the	O
aggregated	O
dual	O
attention	O
.	O

section	O
:	O
Case	O
Study	O
Fig	O
5	O
shows	O
the	O
generated	O
descriptions	O
for	O
different	O
variants	O
of	O
our	O
model	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
.	O

All	O
three	O
neural	B-Method
network	I-Method
generators	E-Method
can	O
produce	O
coherent	O
and	O
understandable	O
sentences	O
with	O
the	O
help	O
of	O
local	B-Method
addressing	I-Method
mechanism	E-Method
.	O

All	O
of	O
them	O
contain	O
the	O
word	O
'	O
baseball	O
'	O
which	O
is	O
not	O
directly	O
mentioned	O
in	O
the	O
infobox	O
.	O

It	O
means	O
the	O
generators	O
deduce	O
from	O
table	O
content	O
that	O
Binky	O
Jones	O
is	O
a	O
baseball	O
player	O
.	O

However	O
,	O
the	O
two	O
vanilla	O
seq2seq	S-Method
models	O
also	O
generate	O
'	O
major	O
league	O
baseball	O
'	O
or	O
'	O
major	O
leagues	O
'	O
which	O
are	O
not	O
mentioned	O
in	O
the	O
table	O
and	O
probably	O
not	O
correct	O
.	O

Vanilla	O
seq2seq	S-Method
model	O
without	O
global	B-Method
addressing	E-Method
on	O
the	O
table	O
just	O
generates	O
the	O
most	O
possible	O
league	O
in	O
Wikipedia	O
for	O
a	O
baseball	O
player	O
to	O
play	O
in	O
.	O

Furthermore	O
,	O
the	O
two	O
biographies	O
generated	O
by	O
vanilla	O
seq2seq	S-Method
model	O
fail	O
to	O
contain	O
the	O
information	O
from	O
the	O
infobox	O
which	O
team	O
he	O
served	O
in	O
,	O
as	O
well	O
as	O
the	O
time	O
period	O
of	O
his	O
playing	O
in	O
that	O
team	O
.	O

The	O
biography	O
generated	O
by	O
our	O
proposed	O
structure	O
-	O
aware	O
seq2seq	S-Method
model	O
is	O
able	O
to	O
cover	O
nearly	O
all	O
the	O
information	O
mentioned	O
in	O
the	O
table	O
.	O

The	O
generated	O
segment	O
'	O
who	O
played	O
shortstop	O
from	O
april	O
15	O
to	O
april	O
27	O
for	O
the	O
brooklyn	O
robins	O
in	O
1924	O
'	O
(	O
15	O
words	O
)	O
includes	O
information	O
in	O
five	O
fields	O
of	O
the	O
table	O
:	O
'	O
position	O
'	O
,	O
'	O
debutdate	O
'	O
,	O
'	O
finaldate	O
'	O
,	O
'	O
debutteam	O
'	O
and	O
'	O
finalteam	O
'	O
,	O
which	O
is	O
achieved	O
by	O
the	O
global	O
addressing	O
between	O
the	O
fields	O
and	O
the	O
generated	O
tokens	O
.	O

section	O
:	O
Conclusions	O
We	O
propose	O
a	O
structure	O
-	O
aware	O
seq2seq	S-Method
architecture	O
to	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
for	O
table	O
-	O
to	O
-	O
text	B-Task
generation	E-Task
.	O

The	O
model	O
consists	O
of	O
field	B-Method
-	I-Method
gating	I-Method
encoder	E-Method
and	O
description	B-Method
generator	E-Method
with	O
dual	B-Method
attention	E-Method
.	O

We	O
add	O
a	O
field	O
gate	O
to	O
the	O
encoder	O
LSTM	S-Method
unit	O
to	O
incorporate	O
the	O
field	O
information	O
.	O

Furthermore	O
,	O
dual	B-Method
attention	I-Method
mechanism	E-Method
which	O
contains	O
word	B-Method
level	I-Method
attention	E-Method
and	O
field	O
level	O
attention	O
can	O
operate	O
local	O
and	O
global	O
addressing	O
to	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O

A	O
series	O
of	O
visualizations	O
,	O
case	O
studies	O
and	O
generation	S-Task
assessments	O
show	O
that	O
our	O
model	O
outperforms	O
the	O
competitive	O
baselines	O
by	O
a	O
large	O
margin	O
.	O

section	O
:	O
section	O
:	O
Acknowledgments	O
Our	O
work	O
is	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
Grant	O
No.2017YFB1002101	O
and	O
project	O
61772040	O
supported	O
by	O
NSFC	O
.	O

The	O
corresponding	O
authors	O
of	O
this	O
paper	O
are	O
Baobao	O
Chang	O
and	O
Zhifang	O
Sui	O
.	O

section	O
:	O
document	O
:	O
Towards	O
Faster	O
Training	O
of	O
Global	B-Method
Covariance	I-Method
Pooling	I-Method
Networks	E-Method
by	O
Iterative	B-Method
Matrix	I-Method
Square	I-Method
Root	I-Method
Normalization	I-Method
Global	I-Method
covariance	I-Method
pooling	E-Method
in	O
convolutional	B-Method
neural	I-Method
networks	E-Method
has	O
achieved	O
impressive	O
improvement	O
over	O
the	O
classical	O
first	B-Method
-	I-Method
order	I-Method
pooling	E-Method
.	O

Recent	O
works	O
have	O
shown	O
matrix	O
square	B-Method
root	I-Method
normalization	E-Method
plays	O
a	O
central	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

However	O
,	O
existing	O
methods	O
depend	O
heavily	O
on	O
eigendecomposition	S-Method
(	O
EIG	S-Method
)	O
or	O
singular	B-Method
value	I-Method
decomposition	E-Method
(	O
SVD	S-Method
)	O
,	O
suffering	O
from	O
inefficient	O
training	S-Task
due	O
to	O
limited	O
support	O
of	O
EIG	S-Method
and	O
SVD	S-Method
on	O
GPU	O
.	O

Towards	O
addressing	O
this	O
problem	O
,	O
we	O
propose	O
an	O
iterative	B-Method
matrix	I-Method
square	I-Method
root	I-Method
normalization	I-Method
method	E-Method
for	O
fast	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
training	I-Task
of	I-Task
global	I-Task
covariance	I-Task
pooling	I-Task
networks	E-Task
.	O

At	O
the	O
core	O
of	O
our	O
method	O
is	O
a	O
meta	B-Method
-	I-Method
layer	E-Method
designed	O
with	O
loop	O
-	O
embedded	O
directed	O
graph	O
structure	O
.	O

The	O
meta	B-Method
-	I-Method
layer	E-Method
consists	O
of	O
three	O
consecutive	O
nonlinear	B-Method
structured	I-Method
layers	E-Method
,	O
which	O
perform	O
pre	B-Method
-	I-Method
normalization	E-Method
,	O
coupled	B-Method
matrix	I-Method
iteration	E-Method
and	O
post	B-Task
-	I-Task
compensation	E-Task
,	O
respectively	O
.	O

Our	O
method	O
is	O
much	O
faster	O
than	O
EIG	S-Method
or	O
SVD	B-Method
based	I-Method
ones	E-Method
,	O
since	O
it	O
involves	O
only	O
matrix	B-Method
multiplications	E-Method
,	O
suitable	O
for	O
parallel	B-Task
implementation	E-Task
on	O
GPU	O
.	O

Moreover	O
,	O
the	O
proposed	O
network	O
with	O
ResNet	B-Method
architecture	E-Method
can	O
converge	O
in	O
much	O
less	O
epochs	O
,	O
further	O
accelerating	O
network	B-Task
training	E-Task
.	O

On	O
large	O
-	O
scale	O
ImageNet	O
,	O
we	O
achieve	O
competitive	O
performance	O
superior	O
to	O
existing	O
counterparts	O
.	O

By	O
finetuning	O
our	O
models	O
pre	O
-	O
trained	O
on	O
ImageNet	S-Material
,	O
we	O
establish	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
three	O
challenging	O
fine	B-Task
-	I-Task
grained	I-Task
benchmarks	E-Task
.	O

The	O
source	O
code	O
and	O
network	B-Method
models	E-Method
will	O
be	O
available	O
at	O
.	O

footnote	O
[	O
page	O
]	O
section	O
:	O
Introduction	O
Deep	B-Method
convolutional	I-Method
neural	I-Method
networks	E-Method
(	O
ConvNets	S-Method
)	O
have	O
made	O
significant	O
progress	O
in	O
the	O
past	O
years	O
,	O
achieving	O
recognition	B-Metric
accuracy	E-Metric
surpassing	O
human	O
beings	O
in	O
large	B-Task
-	I-Task
scale	I-Task
object	I-Task
recognition	E-Task
.	O

The	O
ConvNet	B-Method
models	E-Method
pre	O
-	O
trained	O
on	O
ImageNet	S-Material
have	O
been	O
proven	O
to	O
benefit	O
a	O
multitude	O
of	O
other	O
computer	B-Task
vision	I-Task
tasks	E-Task
,	O
ranging	O
from	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
categorization	E-Task
(	O
FGVC	S-Task
)	O
,	O
object	B-Task
detection	E-Task
,	O
semantic	B-Task
segmentation	E-Task
to	O
scene	B-Task
parsing	E-Task
,	O
where	O
labeled	O
data	O
are	O
insufficient	O
for	O
training	O
from	O
scratch	O
.	O

The	O
common	B-Method
layers	E-Method
such	O
as	O
convolution	S-Method
,	O
non	B-Method
-	I-Method
linear	I-Method
rectification	E-Method
,	O
pooling	S-Method
and	O
batch	B-Method
normalization	E-Method
have	O
become	O
off	O
-	O
the	O
-	O
shelf	O
commodities	O
,	O
widely	O
supported	O
on	O
devices	O
including	O
workstations	O
,	O
PCs	S-Method
and	O
embedded	B-Task
systems	E-Task
.	O

Although	O
the	O
architecture	O
of	O
ConvNet	S-Method
has	O
greatly	O
evolved	O
in	O
the	O
past	O
years	O
,	O
its	O
basic	O
layers	O
largely	O
keep	O
unchanged	O
.	O

Recently	O
,	O
researchers	O
have	O
shown	O
increasing	O
interests	O
in	O
exploring	O
structured	O
layers	O
to	O
enhance	O
representation	B-Task
capability	I-Task
of	I-Task
networks	E-Task
.	O

One	O
particular	O
kind	O
of	O
structured	B-Method
layer	E-Method
is	O
concerned	O
with	O
global	B-Method
covariance	I-Method
pooling	E-Method
after	O
the	O
last	O
convolution	B-Method
layer	E-Method
,	O
which	O
has	O
shown	O
impressive	O
improvement	O
over	O
the	O
classical	O
first	B-Method
-	I-Method
order	I-Method
pooling	E-Method
,	O
successfully	O
used	O
in	O
FGVC	S-Task
,	O
visual	B-Task
question	I-Task
answering	E-Task
and	O
video	B-Task
action	I-Task
recognition	E-Task
.	O

Very	O
recent	O
works	O
have	O
demonstrated	O
that	O
matrix	B-Method
square	I-Method
root	I-Method
normalization	I-Method
of	I-Method
global	I-Method
covariance	I-Method
pooling	E-Method
plays	O
a	O
key	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
both	O
large	B-Task
-	I-Task
scale	I-Task
visual	I-Task
recognition	E-Task
and	O
challenging	B-Task
FGVC	E-Task
.	O

CUDA	S-Method
support	O
Scalability	O
to	O
multi	B-Method
-	I-Method
GPUs	I-Method
Large	I-Method
-	I-Method
scale	E-Method
(	O
LS	S-Method
)	O
or	O
Small	B-Method
-	I-Method
scale	E-Method
(	O
SS	S-Method
)	O
EIG	B-Method
algorithm	I-Method
BP	E-Method
of	O
EIG	S-Method
limited	O
G	O
DeNet	O
SVD	S-Method
algorithm	O
BP	O
of	O
SVD	S-Method
limited	O
Improved	O
B	B-Method
-	I-Method
CNN	I-Method
Newton	I-Method
-	I-Method
Schulz	I-Method
Iter	E-Method
.	O

BP	S-Method
by	O
Lyapunov	B-Method
equation	E-Method
(	O
SCHUR	O
or	O
EIG	S-Method
required	O
)	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
ours	O
)	O
Newton	B-Method
-	I-Method
Schulz	I-Method
Iter	E-Method
.	O

BP	B-Method
of	I-Method
Newton	I-Method
-	I-Method
Schulz	I-Method
Iter	E-Method
.	O

For	O
computing	O
matrix	B-Task
square	I-Task
root	E-Task
,	O
existing	O
methods	O
depend	O
heavily	O
on	O
eigendecomposition	S-Method
(	O
EIG	S-Method
)	O
or	O
singular	B-Method
value	I-Method
decomposition	E-Method
(	O
SVD	S-Method
)	O
.	O

However	O
,	O
fast	O
implementation	O
of	O
EIG	S-Method
or	O
SVD	S-Method
on	O
GPU	S-Method
is	O
an	O
open	O
problem	O
,	O
which	O
is	O
limitedly	O
supported	O
on	O
NVIDIA	B-Method
CUDA	I-Method
platform	E-Method
,	O
significantly	O
slower	O
than	O
their	O
CPU	B-Method
counterparts	E-Method
.	O

As	O
such	O
,	O
existing	O
methods	O
opt	O
for	O
EIG	S-Method
or	O
SVD	S-Method
on	O
CPU	S-Method
for	O
computing	O
matrix	B-Task
square	I-Task
root	E-Task
.	O

Nevertheless	O
,	O
current	O
implementations	O
of	O
meta	B-Method
-	I-Method
layers	E-Method
depending	O
on	O
CPU	S-Method
are	O
far	O
from	O
ideal	O
,	O
particularly	O
for	O
multi	B-Task
-	I-Task
GPU	I-Task
configuration	E-Task
.	O

Since	O
GPUs	S-Method
with	O
powerful	O
parallel	O
computing	O
ability	O
have	O
to	O
be	O
interrupted	O
and	O
await	O
CPUs	O
with	O
limited	O
parallel	O
ability	O
,	O
their	O
concurrency	O
and	O
throughput	O
are	O
greatly	O
restricted	O
.	O

In	O
,	O
for	O
the	O
purpose	O
of	O
fast	B-Task
forward	I-Task
propagation	I-Task
(	I-Task
FP	I-Task
)	E-Task
,	O
Lin	O
and	O
Maji	O
use	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
(	O
called	O
modified	B-Method
Denman	I-Method
-	I-Method
Beavers	I-Method
iteration	E-Method
therein	O
)	O
algorithm	O
,	O
which	O
is	O
proposed	O
in	O
,	O
to	O
compute	O
matrix	O
square	O
-	O
root	O
.	O

Unfortunately	O
,	O
for	O
backward	B-Task
propagation	E-Task
(	O
BP	S-Method
)	O
,	O
they	O
compute	O
the	O
gradient	O
through	O
Lyapunov	B-Method
equation	I-Method
solution	E-Method
which	O
depends	O
on	O
the	O
GPU	B-Method
unfriendly	I-Method
Schur	I-Method
-	I-Method
decomposition	E-Method
(	O
SCHUR	S-Method
)	O
or	O
EIG	S-Method
.	O

Hence	O
,	O
the	O
training	O
in	O
is	O
expensive	O
though	O
FP	S-Method
which	O
involves	O
only	O
matrix	B-Method
multiplication	E-Method
runs	O
very	O
fast	O
.	O

Inspired	O
by	O
that	O
work	O
,	O
we	O
propose	O
a	O
fast	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
training	I-Method
method	E-Method
,	O
called	O
iterative	B-Method
matrix	I-Method
square	I-Method
root	I-Method
normalization	I-Method
of	I-Method
covariance	I-Method
pooling	E-Method
(	O
iSQRT	B-Method
-	I-Method
COV	E-Method
)	O
,	O
depending	O
on	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
in	O
both	O
forward	B-Method
and	I-Method
backward	I-Method
propagations	E-Method
.	O

At	O
the	O
core	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
is	O
a	O
meta	B-Method
-	I-Method
layer	E-Method
with	O
loop	B-Method
-	I-Method
embedded	I-Method
directed	I-Method
graph	I-Method
structure	E-Method
,	O
specifically	O
designed	O
for	O
ensuring	O
both	O
convergence	O
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
and	O
performance	O
of	O
global	B-Method
covariance	I-Method
pooling	I-Method
networks	E-Method
.	O

The	O
meta	B-Method
-	I-Method
layer	E-Method
consists	O
of	O
three	O
consecutive	O
structured	B-Method
layers	E-Method
,	O
performing	O
pre	B-Method
-	I-Method
normalization	E-Method
,	O
coupled	B-Method
matrix	I-Method
iteration	E-Method
and	O
post	B-Task
-	I-Task
compensation	E-Task
,	O
respectively	O
.	O

We	O
derive	O
the	O
gradients	O
associated	O
with	O
the	O
involved	O
non	B-Method
-	I-Method
linear	I-Method
layers	E-Method
based	O
on	O
matrix	B-Method
backpropagation	I-Method
theory	E-Method
.	O

The	O
design	O
of	O
sandwiching	B-Method
Newton	I-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
using	O
pre	B-Method
-	I-Method
normalization	E-Method
by	O
Frobenius	O
norm	O
or	O
trace	O
and	O
post	B-Method
-	I-Method
compensation	E-Method
is	O
essential	O
,	O
which	O
,	O
as	O
far	O
as	O
we	O
know	O
,	O
did	O
not	O
appear	O
in	O
previous	O
literature	O
(	O
e.g.	O
in	O
or	O
)	O
.	O

The	O
pre	B-Method
-	I-Method
normalization	E-Method
guarantees	O
convergence	O
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
(	I-Method
NS	I-Method
)	I-Method
iteration	E-Method
,	O
while	O
post	B-Method
-	I-Method
compensation	E-Method
plays	O
a	O
key	O
role	O
in	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
prevalent	O
deep	B-Method
ConvNet	I-Method
architectures	E-Method
,	O
e.g.	O
ResNet	S-Method
.	O

The	O
main	O
differences	O
between	O
our	O
method	O
and	O
other	O
related	O
works	O
are	O
summarized	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
B	B-Method
-	I-Method
CNN	E-Method
is	O
one	O
of	O
the	O
first	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
covariance	I-Task
pooling	I-Task
ConvNets	E-Task
.	O

It	O
performs	O
element	B-Method
-	I-Method
wise	I-Method
square	I-Method
root	I-Method
normalization	E-Method
followed	O
by	O
normalization	S-Method
for	O
covariance	O
matrix	O
,	O
achieving	O
impressive	O
performance	O
in	O
FGVC	B-Task
task	E-Task
.	O

Improved	O
B	B-Method
-	I-Method
CNN	E-Method
shows	O
that	O
additional	O
matrix	B-Method
square	I-Method
root	I-Method
normalization	E-Method
before	O
element	B-Method
-	I-Method
wise	I-Method
square	I-Method
root	E-Method
and	O
normalization	S-Method
can	O
further	O
attain	O
large	O
improvement	O
.	O

In	O
training	B-Task
process	E-Task
,	O
they	O
perform	O
FP	S-Method
using	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
or	O
using	O
SVD	S-Method
,	O
and	O
perform	O
BP	S-Method
by	O
solving	O
Lyapunov	B-Method
equation	E-Method
or	O
compute	O
gradients	O
associated	O
with	O
SVD	S-Method
.	O

In	O
any	O
case	O
,	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
suffers	O
from	O
GPU	O
unfriendly	O
SVD	S-Method
,	O
SCHUR	O
or	O
EIG	S-Method
and	O
so	O
network	B-Method
training	E-Method
is	O
expensive	O
.	O

Our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
differs	O
from	O
in	O
three	O
aspects	O
.	O

First	O
,	O
both	O
FP	S-Method
and	O
BP	S-Method
of	O
our	O
method	O
are	O
based	O
on	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
,	O
making	O
network	B-Method
training	E-Method
very	O
efficient	O
as	O
only	O
GPU	B-Method
friendly	I-Method
matrix	I-Method
multiplications	E-Method
are	O
involved	O
.	O

Second	O
,	O
we	O
propose	O
sandwiching	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
using	O
pre	B-Method
-	I-Method
normalization	E-Method
and	O
post	B-Method
-	I-Method
compensation	E-Method
which	O
is	O
essential	O
and	O
plays	O
a	O
key	O
role	O
in	O
training	O
extremely	O
deep	B-Task
ConvNets	E-Task
.	O

Finally	O
,	O
we	O
evaluate	O
extensively	O
on	O
both	O
large	B-Material
-	I-Material
scale	I-Material
ImageNet	E-Material
and	O
on	O
three	O
popular	O
fine	O
-	O
grained	O
benchmarks	O
.	O

In	O
,	O
matrix	B-Method
power	I-Method
normalized	I-Method
covariance	I-Method
pooling	I-Method
method	E-Method
(	O
MPN	B-Method
-	I-Method
COV	E-Method
)	O
is	O
proposed	O
for	O
large	B-Task
-	I-Task
scale	I-Task
visual	I-Task
recognition	E-Task
.	O

It	O
achieves	O
impressive	O
improvements	O
over	O
first	B-Method
-	I-Method
order	I-Method
pooling	E-Method
with	O
AlexNet	S-Method
,	O
VGG	B-Method
-	I-Method
Net	E-Method
and	O
ResNet	B-Method
architectures	E-Method
.	O

MPN	B-Method
-	I-Method
COV	E-Method
has	O
shown	O
that	O
,	O
given	O
a	O
small	O
number	O
of	O
high	O
-	O
dimensional	O
features	O
,	O
matrix	O
power	O
is	O
consistent	O
with	O
shrinkage	B-Method
principle	I-Method
of	I-Method
robust	I-Method
covariance	I-Method
estimation	E-Method
,	O
and	O
matrix	B-Method
square	I-Method
root	E-Method
can	O
be	O
derived	O
as	O
a	O
robust	B-Method
covariance	I-Method
estimator	E-Method
via	O
a	O
von	B-Method
Neumann	I-Method
regularized	I-Method
maximum	I-Method
likelihood	I-Method
estimation	E-Method
.	O

It	O
is	O
also	O
shown	O
that	O
matrix	B-Method
power	I-Method
normalization	E-Method
approximately	O
yet	O
effectively	O
exploits	O
geometry	O
of	O
the	O
manifold	O
of	O
covariance	O
matrices	O
,	O
superior	O
to	O
matrix	B-Method
logarithm	I-Method
normalization	E-Method
for	O
high	B-Task
-	I-Task
dimensional	I-Task
features	E-Task
.	O

All	O
computations	O
of	O
MPN	B-Method
-	I-Method
COV	I-Method
meta	I-Method
-	I-Method
layer	E-Method
are	O
implemented	O
with	O
NVIDIA	B-Method
cuBLAS	I-Method
library	E-Method
running	O
on	O
GPU	O
,	O
except	O
EIG	S-Method
which	O
runs	O
on	O
CPU	O
.	O

G	B-Method
DeNet	E-Method
is	O
concerned	O
with	O
inserting	O
global	B-Method
Gaussian	I-Method
distributions	E-Method
into	O
ConvNets	S-Method
for	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
learning	E-Task
.	O

In	O
G	O
DeNet	O
,	O
each	O
Gaussian	O
is	O
identified	O
as	O
square	O
root	O
of	O
a	O
symmetric	O
positive	O
definite	O
matrix	O
based	O
on	O
Lie	O
group	O
structure	O
of	O
Gaussian	O
manifold	O
.	O

The	O
matrix	B-Method
square	I-Method
root	E-Method
plays	O
a	O
central	O
role	O
in	O
obtaining	O
the	O
competitive	O
performance	O
.	O

Compact	B-Method
bilinear	I-Method
pooling	E-Method
(	O
CBP	S-Method
)	O
clarifies	O
that	O
bilinear	B-Method
pooling	E-Method
is	O
closely	O
related	O
to	O
the	O
second	B-Method
-	I-Method
order	I-Method
polynomial	I-Method
kernel	E-Method
,	O
and	O
presents	O
two	O
compact	B-Method
representations	E-Method
via	O
low	B-Method
-	I-Method
dimensional	I-Method
feature	I-Method
maps	E-Method
for	O
kernel	B-Method
approximation	E-Method
.	O

Kernel	B-Method
pooling	E-Method
approximates	O
Gaussian	B-Method
RBF	I-Method
kernel	E-Method
to	O
a	O
given	O
order	O
through	O
compact	O
explicit	O
feature	O
maps	O
,	O
aiming	O
to	O
characterize	O
higher	O
order	O
feature	O
interactions	O
.	O

Cai	O
et	O
al	O
.	O

introduce	O
a	O
polynomial	B-Method
kernel	I-Method
based	I-Method
predictor	E-Method
to	O
model	O
higher	O
-	O
order	O
statistics	O
of	O
convolutional	O
features	O
across	O
multiple	O
layers	O
.	O

section	O
:	O
Proposed	O
iSQRT	B-Method
-	I-Method
COV	E-Method
Network	O
In	O
this	O
section	O
,	O
we	O
first	O
give	O
an	O
overview	O
of	O
the	O
proposed	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
.	O

Then	O
we	O
describe	O
matrix	B-Method
square	I-Method
root	I-Method
computation	E-Method
and	O
its	O
forward	B-Method
propagation	E-Method
.	O

We	O
finally	O
derive	O
the	O
corresponding	O
backward	O
gradients	O
.	O

subsection	O
:	O
Overview	O
of	O
Method	O
The	O
flowchart	O
of	O
the	O
proposed	O
network	O
is	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Let	O
output	O
of	O
the	O
last	O
convolutional	B-Method
layer	E-Method
(	O
with	O
ReLU	S-Method
)	O
be	O
a	O
tensor	O
with	O
spatial	O
height	O
,	O
width	O
and	O
channel	O
.	O

We	O
reshape	O
the	O
tensor	O
to	O
a	O
feature	O
matrix	O
consisting	O
of	O
features	O
of	O
dimension	O
.	O

Then	O
we	O
perform	O
second	B-Method
-	I-Method
order	I-Method
pooling	E-Method
by	O
computing	O
the	O
covariance	O
matrix	O
,	O
where	O
,	O
and	O
are	O
the	O
identity	O
matrix	O
and	O
matrix	O
of	O
all	O
ones	O
,	O
respectively	O
.	O

Our	O
meta	B-Method
-	I-Method
layer	E-Method
is	O
designed	O
to	O
have	O
loop	O
-	O
embedded	O
directed	O
graph	O
structure	O
,	O
consisting	O
of	O
three	O
consecutive	O
nonlinear	B-Method
structured	I-Method
layers	E-Method
.	O

The	O
purpose	O
of	O
the	O
first	O
layer	O
(	O
i.e.	O
,	O
pre	B-Method
-	I-Method
normalization	E-Method
)	O
is	O
to	O
guarantee	O
the	O
convergence	O
of	O
the	O
following	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
,	O
achieved	O
by	O
dividing	O
the	O
covariance	O
matrix	O
by	O
its	O
trace	O
(	O
or	O
Frobenius	O
norm	O
)	O
.	O

The	O
second	O
layer	O
is	O
of	O
loop	O
structure	O
,	O
repeating	O
the	O
coupled	B-Method
matrix	I-Method
equations	E-Method
involved	O
in	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
a	O
fixed	O
number	O
of	O
times	O
,	O
for	O
computing	O
approximate	B-Task
matrix	I-Task
square	I-Task
root	E-Task
.	O

The	O
pre	O
-	O
normalization	S-Task
nontrivially	O
changes	O
data	O
magnitudes	O
,	O
so	O
we	O
design	O
the	O
third	O
layer	O
(	O
i.e.	O
,	O
post	O
-	O
compensation	O
)	O
to	O
counteract	O
the	O
adverse	O
effect	O
by	O
multiplying	O
trace	O
(	O
or	O
Frobenius	O
norm	O
)	O
of	O
the	O
square	O
root	O
of	O
the	O
covariance	O
matrix	O
.	O

As	O
the	O
output	O
of	O
our	O
meta	B-Method
-	I-Method
layer	E-Method
is	O
a	O
symmetric	O
matrix	O
,	O
we	O
concatenate	O
its	O
upper	O
triangular	O
entries	O
forming	O
an	O
-	O
dimensional	O
vector	O
,	O
submitted	O
to	O
the	O
subsequent	O
layer	O
of	O
the	O
ConvNet	S-Method
.	O

subsection	O
:	O
Matrix	B-Method
Square	I-Method
Root	E-Method
and	O
Forward	B-Method
Propagation	E-Method
Square	O
roots	O
of	O
matrices	O
,	O
particularly	O
covariance	B-Method
matrices	E-Method
which	O
are	O
symmetric	B-Method
positive	I-Method
(	I-Method
semi	I-Method
)	I-Method
definite	I-Method
(	I-Method
SPD	I-Method
)	E-Method
,	O
find	O
applications	O
in	O
a	O
variety	O
of	O
fields	O
including	O
computer	B-Task
vision	E-Task
,	O
medical	B-Task
imaging	E-Task
and	O
chemical	B-Task
physics	E-Task
.	O

It	O
is	O
well	O
-	O
known	O
any	O
SPD	B-Method
matrix	E-Method
has	O
a	O
unique	O
square	O
root	O
which	O
can	O
be	O
computed	O
accurately	O
by	O
EIG	S-Method
or	O
SVD	S-Method
.	O

Briefly	O
,	O
let	O
be	O
an	O
SPD	B-Method
matrix	E-Method
and	O
it	O
has	O
EIG	S-Method
,	O
where	O
is	O
orthogonal	O
and	O
is	O
a	O
diagonal	O
matrix	O
of	O
eigenvalues	O
of	O
.	O

Then	O
has	O
a	O
square	O
root	O
,	O
i.e.	O
,	O
.	O

Unfortunately	O
,	O
both	O
EIG	S-Method
and	O
SVD	S-Method
are	O
not	O
well	O
supported	O
on	O
GPU	O
.	O

paragraph	O
:	O
Newton	B-Method
-	I-Method
Schulz	I-Method
Iteration	E-Method
Higham	O
studied	O
a	O
class	O
of	O
methods	O
for	O
iteratively	B-Task
computing	I-Task
matrix	I-Task
square	I-Task
root	E-Task
.	O

These	O
methods	O
,	O
termed	O
as	O
Newton	B-Method
-	I-Method
Padé	I-Method
iterations	E-Method
,	O
are	O
developed	O
based	O
on	O
the	O
connection	O
between	O
matrix	O
sign	O
function	O
and	O
matrix	O
square	O
root	O
,	O
together	O
with	O
rational	B-Method
Padé	I-Method
approximation	E-Method
.	O

Specifically	O
,	O
for	O
computing	O
the	O
square	O
root	O
of	O
,	O
given	O
and	O
,	O
for	O
,	O
the	O
coupled	B-Method
iteration	E-Method
takes	O
the	O
following	O
form	O
:	O
where	O
and	O
are	O
polynomials	O
,	O
and	O
and	O
are	O
non	O
-	O
negative	O
integers	O
.	O

Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
converges	O
only	O
locally	O
:	O
if	O
where	O
denotes	O
any	O
induced	O
(	O
or	O
consistent	O
)	O
matrix	O
norm	O
,	O
and	O
quadratically	O
converge	O
to	O
and	O
,	O
respectively	O
.	O

The	O
family	O
of	O
coupled	B-Method
iteration	E-Method
is	O
stable	O
in	O
that	O
small	O
errors	O
in	O
the	O
previous	O
iteration	O
will	O
not	O
be	O
amplified	O
.	O

The	O
case	O
of	O
called	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
fits	O
for	O
our	O
purpose	O
as	O
no	O
GPU	B-Method
unfriendly	I-Method
matrix	I-Method
inverse	E-Method
is	O
involved	O
:	O
Clearly	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
involves	O
only	O
matrix	B-Method
product	E-Method
,	O
suitable	O
for	O
parallel	B-Task
implementation	E-Task
on	O
GPU	O
.	O

Compared	O
to	O
accurate	O
square	O
root	O
computed	O
by	O
EIG	S-Method
,	O
one	O
can	O
only	O
obtain	O
approximate	B-Method
solution	E-Method
with	O
a	O
small	O
number	O
of	O
iterations	O
.	O

We	O
determine	O
the	O
number	O
of	O
iterations	O
by	O
cross	B-Method
-	I-Method
validation	E-Method
.	O

Interestingly	O
,	O
compared	O
to	O
EIG	S-Method
or	O
SVD	S-Method
based	O
methods	O
,	O
experiments	O
on	O
large	B-Material
-	I-Material
scale	I-Material
ImageNet	E-Material
show	O
that	O
we	O
can	O
obtain	O
matching	O
or	O
marginally	O
better	O
performance	O
under	O
AlexNet	B-Method
architecture	E-Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
and	O
better	O
performance	O
under	O
ResNet	B-Method
architecture	E-Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
using	O
no	O
more	O
than	O
5	O
iterations	O
.	O

paragraph	O
:	O
Pre	O
-	O
normalization	S-Task
and	O
Post	B-Task
-	I-Task
compensation	E-Task
As	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
only	O
converges	O
locally	O
,	O
we	O
pre	O
-	O
normalize	O
by	O
trace	O
or	O
Frobenius	O
norm	O
,	O
i.e.	O
,	O
Let	O
be	O
eigenvalues	O
of	O
,	O
arranged	O
in	O
nondecreasing	O
order	O
.	O

As	O
and	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
,	O
which	O
equals	O
to	O
the	O
largest	O
singular	O
value	O
of	O
,	O
is	O
and	O
for	O
the	O
case	O
of	O
trace	B-Metric
and	I-Metric
Frobenius	I-Metric
norm	E-Metric
,	O
respectively	O
,	O
both	O
less	O
than	O
1	O
.	O

Hence	O
,	O
the	O
convergence	O
condition	O
is	O
satisfied	O
.	O

The	O
above	O
pre	B-Method
-	I-Method
normalization	I-Method
of	I-Method
covariance	I-Method
matrix	E-Method
nontrivially	O
changes	O
the	O
data	O
magnitudes	O
such	O
that	O
it	O
produces	O
adverse	O
effect	O
on	O
network	O
.	O

Hence	O
,	O
to	O
counteract	O
this	O
change	O
,	O
after	O
the	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
,	O
we	O
accordingly	O
perform	O
post	B-Method
-	I-Method
compensation	E-Method
,	O
i.e.	O
,	O
An	O
alternative	O
scheme	O
to	O
counterbalance	O
the	O
influence	O
incurred	O
by	O
pre	B-Method
-	I-Method
normalization	E-Method
is	O
Batch	B-Method
Normalization	E-Method
(	O
BN	S-Method
)	O
.	O

One	O
may	O
even	O
consider	O
without	O
using	O
any	O
post	B-Method
-	I-Method
compensation	E-Method
.	O

However	O
,	O
our	O
experiment	O
on	O
ImageNet	S-Material
has	O
shown	O
that	O
,	O
without	O
post	B-Method
-	I-Method
normalization	E-Method
,	O
prevalent	O
ResNet	S-Method
fails	O
to	O
converge	O
,	O
while	O
our	O
scheme	O
outperforms	O
BN	S-Method
by	O
about	O
1	O
%	O
(	O
see	O
[	O
reference	O
]	O
for	O
details	O
)	O
.	O

subsection	O
:	O
Backward	B-Method
Propagation	E-Method
(	O
BP	S-Method
)	O
The	O
gradients	O
associated	O
with	O
the	O
structured	B-Method
layers	E-Method
are	O
derived	O
using	O
matrix	B-Method
backpropagation	I-Method
methodology	E-Method
,	O
which	O
establishes	O
the	O
chain	O
rule	O
of	O
a	O
general	B-Method
matrix	I-Method
function	E-Method
by	O
first	B-Method
-	I-Method
order	I-Method
Taylor	I-Method
approximation	E-Method
.	O

Below	O
we	O
take	O
pre	O
-	O
normalization	O
by	O
trace	O
as	O
an	O
example	O
,	O
deriving	O
the	O
corresponding	O
gradients	O
.	O

paragraph	O
:	O
BP	B-Task
of	I-Task
Post	I-Task
-	I-Task
compensation	E-Task
Given	O
where	O
is	O
the	O
loss	O
function	O
,	O
the	O
chain	B-Method
rule	E-Method
is	O
of	O
the	O
form	O
where	O
denotes	O
variation	O
of	O
.	O

After	O
some	O
manipulations	O
,	O
we	O
have	O
paragraph	O
:	O
BP	B-Method
of	I-Method
Newton	I-Method
-	I-Method
Schulz	I-Method
Iteration	E-Method
Then	O
we	O
are	O
to	O
compute	O
the	O
partial	O
derivatives	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
and	O
,	O
,	O
given	O
computed	O
by	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
.	O

As	O
the	O
covariance	O
matrix	O
is	O
symmetric	O
,	O
it	O
is	O
easy	O
to	O
see	O
from	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
that	O
and	O
are	O
both	O
symmetric	O
.	O

According	O
to	O
the	O
chain	O
rules	O
(	O
omitted	O
hereafter	O
for	O
simplicity	O
)	O
of	O
matrix	B-Method
backpropagation	E-Method
and	O
after	O
some	O
manipulations	O
,	O
,	O
we	O
can	O
derive	O
The	O
final	O
step	O
of	O
this	O
layer	O
is	O
concerned	O
with	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
,	O
which	O
is	O
given	O
by	O
paragraph	O
:	O
BP	O
of	O
Pre	B-Method
-	I-Method
normalization	E-Method
Note	O
that	O
here	O
we	O
need	O
to	O
combine	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
,	O
backpropagated	O
from	O
the	O
post	B-Method
-	I-Method
compensation	I-Method
layer	E-Method
.	O

As	O
such	O
,	O
by	O
referring	O
to	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
we	O
make	O
similar	O
derivations	O
as	O
before	O
and	O
obtain	O
If	O
we	O
adopt	O
pre	O
-	O
normalization	O
by	O
Frobenius	O
norm	O
,	O
the	O
gradients	O
associated	O
with	O
post	B-Method
-	I-Method
compensation	E-Method
become	O
and	O
that	O
with	O
respect	O
to	O
pre	O
-	O
normalization	O
is	O
while	O
the	O
backward	O
gradients	O
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
(	O
[	O
reference	O
]	O
)	O
keep	O
unchanged	O
.	O

Finally	O
,	O
given	O
,	O
one	O
can	O
derive	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
with	O
respect	O
to	O
input	O
matrix	O
,	O
which	O
takes	O
the	O
following	O
form	O
:	O
section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
method	O
on	O
both	O
large	B-Task
-	I-Task
scale	I-Task
image	I-Task
classification	E-Task
and	O
challenging	B-Task
fine	I-Task
-	I-Task
grained	I-Task
visual	I-Task
categorization	I-Task
tasks	E-Task
.	O

We	O
make	O
experiments	O
using	O
two	O
PCs	S-Method
each	O
of	O
which	O
is	O
equipped	O
with	O
a	O
4	O
-	O
core	O
Intel	O
i7	O
-	O
4790k@4.0GHz	O
CPU	O
,	O
32	O
G	O
RAM	O
,	O
512	O
GB	O
Samsung	B-Method
PRO	I-Method
SSD	E-Method
and	O
two	O
Titan	B-Method
Xp	I-Method
GPUs	E-Method
.	O

We	O
implement	O
our	O
networks	O
using	O
MatConvNet	S-Method
and	O
Matlab2015b	S-Method
,	O
under	O
Ubuntu	B-Method
14.04.5	I-Method
LTS	E-Method
.	O

subsection	O
:	O
Datasets	O
and	O
Our	O
Meta	B-Method
-	I-Method
layer	I-Method
Implementation	E-Method
Datasets	O
For	O
large	B-Task
-	I-Task
scale	I-Task
image	I-Task
classification	E-Task
,	O
we	O
adopt	O
ImageNet	B-Material
LSVRC2012	I-Material
dataset	E-Material
with	O
1	O
,	O
000	O
object	O
categories	O
.	O

The	O
dataset	O
contains	O
1.28	O
M	O
images	O
for	O
training	O
,	O
50	O
K	O
images	O
for	O
validation	S-Task
and	O
100	O
K	O
images	O
for	O
testing	O
(	O
without	O
published	O
labels	O
)	O
.	O

As	O
in	O
,	O
we	O
report	O
the	O
results	O
on	O
the	O
validation	O
set	O
.	O

For	O
fine	B-Task
-	I-Task
grained	I-Task
categorization	E-Task
,	O
we	O
use	O
three	O
popular	O
fine	O
-	O
grained	O
benchmarks	O
,	O
i.e.	O
,	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	E-Material
(	O
Birds	S-Material
)	O
,	O
FGVC	B-Material
-	I-Material
aircraft	E-Material
(	O
Aircrafts	S-Material
)	O
and	O
Stanford	B-Material
cars	E-Material
(	O
Cars	S-Material
)	O
.	O

The	O
Birds	B-Material
dataset	E-Material
contains	O
11	O
,	O
788	O
images	O
from	O
200	O
species	O
,	O
with	O
large	O
intra	O
-	O
class	O
variation	O
but	O
small	O
inter	O
-	O
class	O
variation	O
.	O

The	O
Aircrafts	S-Material
dataset	O
includes	O
100	O
aircraft	B-Material
classes	E-Material
and	O
a	O
total	O
of	O
10	O
,	O
000	O
images	O
with	O
small	O
background	O
noise	O
but	O
higher	O
inter	O
-	O
class	O
similarity	O
.	O

The	O
Cars	S-Material
dataset	O
consists	O
of	O
16	O
,	O
185	O
images	O
from	O
196	O
classes	O
.	O

For	O
all	O
datasets	O
,	O
we	O
adopt	O
the	O
provided	O
training	O
/	O
test	O
split	O
,	O
using	O
neither	O
bounding	O
boxes	O
nor	O
part	O
annotations	O
.	O

Implementation	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
Meta	O
-	O
layer	O
We	O
encapsulate	O
our	O
code	O
in	O
three	O
computational	O
blocks	O
,	O
which	O
implement	O
forward	B-Method
&	I-Method
backward	I-Method
computation	I-Method
of	I-Method
pre	I-Method
-	I-Method
normalization	I-Method
layer	E-Method
,	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iteration	I-Method
layer	E-Method
and	O
post	B-Method
-	I-Method
compensation	I-Method
layer	E-Method
,	O
respectively	O
.	O

The	O
code	O
is	O
written	O
in	O
C	B-Method
++	E-Method
based	O
on	O
NVIDIA	S-Method
on	O
top	O
of	O
CUDA	B-Method
toolkit	I-Method
8.0	E-Method
.	O

In	O
addition	O
,	O
we	O
write	O
code	O
in	O
C	B-Method
++	E-Method
based	O
on	O
cuBLAS	S-Method
for	O
computing	B-Task
covariance	I-Task
matrices	E-Task
.	O

We	O
create	O
MEX	B-Method
files	E-Method
so	O
that	O
the	O
above	O
subroutines	O
can	O
be	O
called	O
in	O
Matlab	O
environment	O
.	O

For	O
AlexNet	S-Task
,	O
we	O
insert	O
our	O
meta	B-Method
-	I-Method
layer	E-Method
after	O
the	O
last	O
convolution	B-Method
layer	E-Method
(	O
with	O
ReLU	S-Method
)	O
,	O
which	O
outputs	O
an	O
tensor	O
.	O

For	O
ResNet	B-Method
architecture	E-Method
,	O
as	O
suggested	O
,	O
we	O
do	O
not	O
perform	O
downsampling	O
for	O
the	O
last	O
set	O
of	O
convolutional	O
blocks	O
,	O
and	O
add	O
one	O
convolution	S-Method
with	O
channels	O
after	O
the	O
last	O
sum	B-Method
layer	E-Method
(	O
with	O
ReLU	S-Method
)	O
.	O

The	O
added	O
convolution	B-Method
layer	E-Method
outputs	O
an	O
tensor	O
.	O

Hence	O
,	O
with	O
both	O
architectures	O
,	O
the	O
covariance	O
matrix	O
is	O
of	O
size	O
and	O
our	O
meta	B-Method
-	I-Method
layer	E-Method
outputs	O
an	O
-	O
dimensional	O
vector	O
as	O
the	O
image	B-Method
representation	E-Method
.	O

subsection	O
:	O
Evaluation	O
with	O
AlexNet	S-Method
on	O
ImageNet	S-Task
In	O
the	O
first	O
part	O
of	O
experiments	O
,	O
we	O
analyze	O
,	O
with	O
AlexNet	B-Method
architecture	E-Method
,	O
the	O
design	O
choices	O
of	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
method	O
,	O
including	O
the	O
number	O
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
iterations	E-Method
,	O
time	B-Metric
and	I-Metric
memory	I-Metric
usage	E-Metric
,	O
and	O
behaviors	O
of	O
different	O
pre	B-Method
-	I-Method
normalization	I-Method
methods	E-Method
.	O

We	O
select	O
AlexNet	S-Method
because	O
it	O
runs	O
faster	O
with	O
shallower	O
depth	O
,	O
and	O
the	O
results	O
can	O
extrapolate	O
to	O
deeper	B-Method
networks	E-Method
which	O
mostly	O
follow	O
its	O
architecture	O
design	O
.	O

We	O
follow	O
for	O
color	B-Task
augmentation	E-Task
and	O
weight	B-Task
initialization	E-Task
,	O
adopting	O
BN	B-Method
and	I-Method
no	I-Method
dropout	E-Method
.	O

We	O
use	O
SGD	S-Method
with	O
a	O
mini	O
-	O
batch	O
of	O
128	O
,	O
unless	O
otherwise	O
stated	O
.	O

The	O
momentum	O
is	O
0.9	O
and	O
weight	O
decay	O
is	O
0.0005	O
.	O

We	O
train	O
iSQRT	B-Method
-	I-Method
COV	E-Method
networks	O
from	O
scratch	O
in	O
20	O
epochs	O
where	O
learning	B-Metric
rate	E-Metric
follows	O
exponential	B-Method
decay	E-Method
.	O

All	O
training	O
and	O
test	O
images	O
are	O
uniformly	O
resized	O
with	O
shorter	O
sides	O
of	O
256	O
.	O

During	O
training	O
we	O
randomly	O
crop	O
a	O
patch	O
from	O
each	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O

We	O
make	O
inference	S-Task
on	O
one	O
single	O
center	O
crop	O
from	O
a	O
test	O
image	O
.	O

Impact	O
of	O
Number	O
N	O
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
Iterations	E-Method
Fig	O
.	O

[	O
reference	O
]	O
shows	O
top	B-Metric
-	I-Metric
1	I-Metric
error	I-Metric
rate	E-Metric
as	O
a	O
function	O
of	O
number	O
of	O
Newton	O
-	O
Schulz	O
iterations	O
in	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Plain	B-Method
-	I-Method
COV	E-Method
indicates	O
simple	O
covariance	B-Method
pooling	E-Method
without	O
any	O
normalization	S-Method
.	O

With	O
one	O
single	O
iteration	O
,	O
our	O
method	O
outperforms	O
Plain	B-Method
-	I-Method
COV	E-Method
by	O
.	O

As	O
iteration	O
number	O
grows	O
,	O
the	O
error	B-Metric
rate	E-Metric
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
gradually	O
declines	O
.	O

With	O
3	O
iterations	O
,	O
iSQRT	B-Method
-	I-Method
COV	E-Method
is	O
comparable	O
to	O
MPN	B-Method
-	I-Method
COV	E-Method
,	O
having	O
only	O
0.3	O
%	O
higher	O
error	B-Metric
rate	E-Metric
,	O
while	O
performing	O
marginally	O
better	O
than	O
MPN	B-Method
-	I-Method
COV	E-Method
between	O
5	O
and	O
7	O
iterations	O
.	O

After	O
,	O
the	O
error	B-Metric
rate	E-Metric
consistently	O
increases	O
,	O
indicating	O
growth	O
of	O
iteration	O
number	O
is	O
not	O
helpful	O
for	O
improving	O
accuracy	S-Metric
.	O

As	O
larger	O
incurs	O
higher	O
computational	B-Metric
cost	E-Metric
,	O
to	O
balance	O
efficiency	O
and	O
accuracy	S-Metric
,	O
we	O
set	O
to	O
5	O
in	O
the	O
remaining	O
experiments	O
.	O

Notably	O
,	O
the	O
approximate	B-Method
square	I-Method
root	I-Method
normalization	E-Method
improves	O
a	O
little	O
over	O
the	O
accurate	O
one	O
obtained	O
via	O
EIG	S-Method
.	O

This	O
interesting	O
problem	O
will	O
be	O
discussed	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
where	O
iSQRT	B-Method
-	I-Method
COV	E-Method
is	O
further	O
evaluated	O
on	O
substantially	O
deeper	O
ResNets	O
.	O

C	B-Method
++	E-Method
N	O
/	O
A	O
Impro	O
.	O

B	O
-	O
CNN	O
SVD	S-Method
or	O
EIG	S-Method
CUDA	O
cuSOLVER	O
Matlab	O
(	O
CPU	O
function	O
)	O
Matlab	O
(	O
GPU	O
function	O
)	O
Time	B-Method
and	I-Method
Memory	I-Method
Analysis	E-Method
We	O
compare	O
time	S-Metric
and	O
memory	S-Metric
consumed	O
by	O
single	O
meta	B-Method
-	I-Method
layer	E-Method
of	O
different	O
methods	O
.	O

We	O
use	O
public	O
code	O
for	O
,	O
and	O
released	O
by	O
the	O
respective	O
authors	O
.	O

As	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
table	O
:	O
single	O
-	O
layer	O
,	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
)	O
and	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
)	O
are	O
3.1x	O
faster	O
and	O
1.8x	O
faster	O
than	O
MPN	B-Method
-	I-Method
COV	E-Method
,	O
respectively	O
.	O

Furthermore	O
,	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
)	O
is	O
five	O
times	O
more	O
efficient	O
than	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
and	O
G	B-Method
DeNet	E-Method
.	O

For	O
improved	O
B	B-Task
-	I-Task
CNN	E-Task
,	O
the	O
forward	B-Task
computation	E-Task
of	O
Newton	B-Method
-	I-Method
Schulz	I-Method
(	I-Method
NS	I-Method
)	I-Method
iteration	E-Method
is	O
much	O
faster	O
than	O
that	O
of	O
SVD	S-Method
,	O
but	O
the	O
total	O
time	O
of	O
two	O
methods	O
is	O
comparable	O
.	O

The	O
authors	O
of	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
also	O
proposed	O
two	O
other	O
implementations	O
,	O
i.e.	O
,	O
FP	S-Method
by	O
NS	B-Method
iteration	E-Method
plus	O
BP	S-Method
by	O
SVD	S-Method
and	O
FP	S-Method
by	O
SVD	S-Method
plus	O
BP	S-Method
by	O
Lyapunov	S-Method
(	O
Lyap	S-Method
.	O

)	O
,	O
which	O
take	O
15.31	O
(	O
2.09	O
)	O
and	O
12.21	O
(	O
11.19	O
)	O
,	O
respectively	O
.	O

We	O
observe	O
that	O
,	O
in	O
any	O
case	O
,	O
the	O
forward	O
backward	O
time	O
taken	O
by	O
single	O
meta	B-Method
-	I-Method
layer	E-Method
of	O
improved	B-Method
B	I-Method
-	I-Method
CNN	E-Method
is	O
significant	O
as	O
GPU	O
unfriendly	O
SVD	S-Method
or	O
EIG	S-Method
can	O
not	O
be	O
avoided	O
,	O
even	O
though	O
the	O
forward	B-Method
computation	E-Method
is	O
very	O
efficient	O
when	O
NS	B-Method
iteration	E-Method
is	O
used	O
.	O

Tab	O
.	O

[	O
reference	O
]	O
table	O
:	O
time	O
-	O
matrix	B-Method
-	I-Method
decomposition	E-Method
presents	O
running	B-Metric
time	E-Metric
of	O
EIG	S-Method
and	O
SVD	S-Method
of	O
an	O
covariance	B-Method
matrix	E-Method
.	O

Matlab	S-Method
(	O
M	O
)	O
built	O
-	O
in	O
CPU	O
functions	O
and	O
GPU	B-Method
functions	E-Method
deliver	O
over	O
10x	O
and	O
2.1x	O
speedups	O
over	O
their	O
CUDA	B-Method
counterparts	E-Method
,	O
respectively	O
.	O

Our	O
method	O
needs	O
to	O
store	O
and	O
in	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
which	O
will	O
be	O
used	O
in	O
backpropagation	S-Method
,	O
taking	O
up	O
more	O
memory	O
than	O
EIG	S-Method
or	O
SVD	S-Method
based	O
ones	O
.	O

Among	O
all	O
,	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
)	O
takes	O
up	O
the	O
largest	O
memory	O
of	O
1.129	O
MB	O
,	O
which	O
is	O
insignificant	O
compared	O
to	O
12	O
GB	O
memory	O
on	O
a	O
Titan	O
Xp	O
.	O

Note	O
that	O
for	O
network	B-Task
inference	E-Task
only	O
,	O
our	O
method	O
takes	O
0.125	O
MB	O
memory	O
as	O
it	O
is	O
unnecessary	O
to	O
store	O
and	O
.	O

Next	O
,	O
we	O
compare	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
speed	O
of	O
network	B-Method
training	E-Method
between	O
MPN	B-Method
-	I-Method
COV	E-Method
and	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
both	O
one	O
-	O
GPU	O
and	O
two	O
-	O
GPU	B-Method
configurations	E-Method
.	O

For	O
one	O
-	O
GPU	O
configuration	O
,	O
the	O
speed	B-Metric
gap	E-Metric
vs.	O
batch	B-Metric
size	E-Metric
between	O
the	O
two	O
methods	O
keeps	O
nearly	O
constant	O
.	O

For	O
two	O
-	O
GPU	O
configuration	O
,	O
their	O
speed	B-Metric
gap	E-Metric
becomes	O
more	O
significant	O
when	O
batch	O
size	O
gets	O
larger	O
.	O

As	O
can	O
be	O
seen	O
,	O
the	O
speed	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
continuously	O
grows	O
with	O
increase	O
of	O
batch	O
size	O
while	O
that	O
of	O
MPN	B-Method
-	I-Method
COV	E-Method
tends	O
to	O
saturate	O
when	O
batch	O
size	O
is	O
larger	O
than	O
512	O
.	O

Clearly	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
can	O
make	O
better	O
use	O
of	O
computing	O
power	O
of	O
multiple	O
GPUs	S-Method
than	O
MPN	B-Method
-	I-Method
COV	E-Method
.	O

Pre	O
-	O
normalization	O
by	O
Trace	O
vs.	O
by	O
Frobenius	O
Norm	O
Sec	O
.	O

[	O
reference	O
]	O
describes	O
two	O
pre	B-Method
-	I-Method
normalization	I-Method
methods	E-Method
.	O

Here	O
we	O
compare	O
them	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
bottom	O
rows	O
)	O
,	O
where	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
trace	S-Method
)	O
indicates	O
pre	B-Method
-	I-Method
normalization	E-Method
by	O
trace	O
.	O

We	O
can	O
see	O
that	O
pre	O
-	O
normalization	S-Method
by	O
trace	S-Method
produces	O
0.3	O
%	O
lower	O
error	B-Metric
rate	E-Metric
than	O
that	O
by	O
Frobenius	B-Method
norm	E-Method
,	O
while	O
taking	O
similar	O
time	O
with	O
the	O
latter	O
.	O

Hence	O
,	O
in	O
all	O
the	O
remaining	O
experiments	O
,	O
we	O
adopt	O
trace	B-Method
based	I-Method
pre	I-Method
-	I-Method
normalization	I-Method
method	E-Method
.	O

Comparison	O
with	O
Other	O
Covariance	B-Method
Pooling	I-Method
Methods	E-Method
We	O
compare	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
other	O
covariance	B-Method
pooling	I-Method
methods	E-Method
,	O
as	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

The	O
results	O
of	O
MPN	B-Method
-	I-Method
COV	E-Method
,	O
B	B-Method
-	I-Method
CNN	E-Method
and	O
DeepO	B-Method
P	E-Method
are	O
duplicated	O
from	O
.	O

We	O
train	O
from	O
scratch	O
G	B-Method
DeNet	E-Method
and	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
on	O
ImageNet	S-Material
.	O

We	O
use	O
the	O
most	O
efficient	O
implementation	O
of	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
,	O
i.e.	O
,	O
FP	S-Method
by	O
SVD	S-Method
and	O
BP	S-Method
by	O
Lyap	S-Method
.	O

,	O
and	O
we	O
mention	O
all	O
implementations	O
of	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
produce	O
similar	O
results	O
.	O

Our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
using	O
pre	B-Method
-	I-Method
normalization	E-Method
by	O
trace	S-Method
is	O
marginally	O
better	O
than	O
MPN	B-Method
-	I-Method
COV	E-Method
.	O

All	O
matrix	B-Method
square	I-Method
root	I-Method
normalization	I-Method
methods	E-Method
except	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
outperform	O
B	B-Method
-	I-Method
CNN	E-Method
and	O
DeepO	B-Method
P.	E-Method
Since	O
improved	O
B	B-Method
-	I-Method
CNN	E-Method
is	O
identical	O
to	O
MPN	B-Method
-	I-Method
COV	E-Method
if	O
element	B-Method
-	I-Method
wise	I-Method
square	I-Method
root	I-Method
normalization	E-Method
and	O
normalization	S-Method
are	O
neglected	O
,	O
its	O
unsatisfactory	O
performance	O
suggests	O
that	O
,	O
after	O
matrix	B-Method
square	I-Method
root	I-Method
normalization	E-Method
,	O
further	O
element	B-Method
-	I-Method
wise	I-Method
square	I-Method
root	I-Method
normalization	E-Method
and	O
normalization	S-Method
hurt	O
large	B-Task
-	I-Task
scale	I-Task
ImageNet	I-Task
classification	E-Task
.	O

This	O
is	O
consistent	O
with	O
the	O
observation	O
in	O
,	O
where	O
after	O
matrix	B-Method
power	I-Method
normalization	E-Method
,	O
additional	O
normalization	S-Method
by	O
Frobenius	B-Method
norm	E-Method
or	O
matrix	B-Method
norm	E-Method
makes	O
performance	O
decline	O
.	O

subsection	O
:	O
Results	O
on	O
ImageNet	B-Method
with	I-Method
ResNet	I-Method
Architecture	E-Method
This	O
section	O
evaluates	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
ResNet	B-Method
architecture	E-Method
.	O

We	O
follow	O
for	O
color	B-Task
augmentation	E-Task
and	O
weight	B-Task
initialization	E-Task
.	O

We	O
rescale	O
each	O
training	O
image	O
with	O
its	O
shorter	O
side	O
randomly	O
sampled	O
on	O
.	O

The	O
fixed	O
-	O
size	O
patch	O
is	O
randomly	O
cropped	O
from	O
the	O
rescaled	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O

We	O
rescale	O
each	O
test	O
image	O
with	O
a	O
shorter	O
side	O
of	O
256	O
and	O
evaluate	O
a	O
single	O
center	O
crop	O
for	O
inference	S-Task
.	O

We	O
use	O
SGD	S-Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
256	O
,	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O

We	O
train	O
iSQRT	B-Method
-	I-Method
COV	E-Method
networks	O
from	O
scratch	O
in	O
60	O
epochs	O
,	O
initializing	O
the	O
learning	B-Metric
rate	E-Metric
to	O
which	O
is	O
divided	O
by	O
10	O
at	O
epoch	O
30	O
and	O
45	O
,	O
respectively	O
.	O

Significance	O
of	O
Post	B-Method
-	I-Method
compensation	E-Method
Rather	O
than	O
our	O
post	B-Method
-	I-Method
compensation	I-Method
scheme	E-Method
,	O
one	O
may	O
choose	O
Batch	B-Method
Normalization	E-Method
(	O
BN	S-Method
)	O
or	O
simply	O
do	O
nothing	O
(	O
i.e.	O
,	O
without	O
post	B-Method
-	I-Method
compensation	E-Method
)	O
.	O

He	O
et	O
al	O
.	O

FBN	B-Method
SORT	E-Method
MPN	O
-	O
COV	O
iSQRT	B-Method
-	I-Method
COV	E-Method
He	O
et	O
al	O
.	O

iSQRT	B-Method
-	I-Method
COV	E-Method
He	O
et	O
al	O
.	O

Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
impact	O
of	O
different	O
schemes	O
on	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
with	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
.	O

Without	O
post	B-Method
-	I-Method
compensation	E-Method
,	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
fails	O
to	O
converge	O
.	O

Careful	O
observations	O
show	O
that	O
in	O
this	O
case	O
the	O
gradients	O
are	O
very	O
small	O
(	O
on	O
the	O
order	O
of	O
)	O
,	O
and	O
largely	O
tuning	O
of	O
learning	B-Metric
rate	E-Metric
helps	O
little	O
.	O

Option	O
of	O
BN	S-Method
helps	O
the	O
network	O
converge	O
,	O
but	O
producing	O
about	O
1	O
%	O
higher	O
top	B-Metric
-	I-Metric
1	I-Metric
error	I-Metric
rate	E-Metric
than	O
our	O
post	B-Method
-	I-Method
compensation	I-Method
scheme	E-Method
.	O

The	O
comparison	O
above	O
suggests	O
that	O
our	O
post	B-Method
-	I-Method
compensation	I-Method
scheme	E-Method
is	O
essential	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

paragraph	O
:	O
Fast	O
Convergence	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
Network	O
We	O
compare	O
convergence	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
and	O
MPN	O
-	O
COV	O
with	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
,	O
as	O
well	O
as	O
the	O
original	O
ResNet	B-Method
-	I-Method
50	E-Method
in	O
which	O
global	B-Method
average	I-Method
pooling	E-Method
is	O
performed	O
after	O
the	O
last	O
convolution	O
layer	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
presents	O
the	O
convergence	O
curves	O
.	O

Compared	O
to	O
the	O
original	O
ResNet	B-Method
-	I-Method
50	E-Method
,	O
the	O
convergence	S-Metric
of	O
both	O
iSQRT	B-Method
-	I-Method
COV	E-Method
and	O
MPN	B-Method
-	I-Method
COV	E-Method
is	O
significantly	O
faster	O
.	O

We	O
observe	O
that	O
iSQRT	B-Method
-	I-Method
COV	E-Method
can	O
converge	O
well	O
within	O
60	O
epochs	O
,	O
achieving	O
top	B-Metric
-	I-Metric
1	I-Metric
error	I-Metric
rate	E-Metric
of	O
22.14	O
%	O
,	O
0.6	O
%	O
lower	O
than	O
MPN	B-Method
-	I-Method
COV	E-Method
.	O

We	O
also	O
trained	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
90	O
epochs	O
using	O
same	O
setting	O
with	O
MPN	B-Method
-	I-Method
COV	E-Method
,	O
obtaining	O
top	B-Metric
-	I-Metric
5	I-Metric
error	E-Metric
of	O
6.12	O
%	O
,	O
slightly	O
lower	O
than	O
that	O
with	O
60	O
epochs	O
(	O
6.22	O
%	O
)	O
.	O

This	O
indicates	O
iSQRT	B-Method
-	I-Method
COV	E-Method
can	O
converge	O
in	O
less	O
epochs	O
,	O
so	O
further	O
accelerating	B-Task
training	E-Task
,	O
as	O
opposed	O
to	O
MPN	B-Method
-	I-Method
COV	E-Method
.	O

The	O
fast	B-Metric
convergence	I-Metric
property	E-Metric
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
is	O
appealing	O
.	O

As	O
far	O
as	O
we	O
know	O
,	O
previous	O
networks	O
with	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
require	O
at	O
least	O
90	O
epochs	O
to	O
converge	O
to	O
competitive	O
results	O
.	O

Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
arts	O
In	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
compare	O
our	O
method	O
with	O
other	O
second	B-Method
-	I-Method
order	I-Method
networks	E-Method
,	O
as	O
well	O
as	O
the	O
original	O
ResNets	S-Method
.	O

With	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
,	O
all	O
the	O
second	B-Method
-	I-Method
order	I-Method
networks	E-Method
improve	O
over	O
the	O
first	O
-	O
order	O
one	O
while	O
our	O
method	O
performing	O
best	O
.	O

MPN	B-Method
-	I-Method
COV	E-Method
and	O
iSQRT	B-Method
-	I-Method
COV	E-Method
,	O
both	O
of	O
which	O
involve	O
square	B-Method
root	I-Method
normalization	E-Method
,	O
are	O
superior	O
to	O
FBN	S-Method
which	O
uses	O
no	O
normalization	O
and	O
SORT	S-Method
which	O
introduces	O
dot	B-Method
product	I-Method
transform	E-Method
in	O
the	O
linear	B-Method
sum	I-Method
of	I-Method
two	I-Method
-	I-Method
branch	I-Method
module	E-Method
followed	O
by	O
element	B-Method
-	I-Method
wise	I-Method
normalization	E-Method
.	O

Moreover	O
,	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
outperforms	O
MPN	B-Method
-	I-Method
COV	E-Method
by	O
0.6	O
%	O
in	O
top	B-Metric
-	I-Metric
1	I-Metric
error	E-Metric
.	O

Note	O
that	O
our	O
50	O
-	O
layer	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
achieves	O
lower	O
error	B-Metric
rate	E-Metric
than	O
much	O
deeper	O
ResNet	B-Method
-	I-Method
101	E-Method
and	O
ResNet	B-Method
-	I-Method
152	E-Method
,	O
while	O
our	O
101	O
-	O
layer	O
iSQRT	B-Method
-	I-Method
COV	E-Method
network	O
outperforming	O
the	O
original	O
ResNet	B-Method
-	I-Method
101	E-Method
by	O
2.4	O
%	O
and	O
ResNet	B-Method
-	I-Method
152	E-Method
by	O
1.8	O
%	O
,	O
respectively	O
.	O

Why	O
Approximate	B-Method
Square	I-Method
Root	E-Method
Performs	O
Better	O
Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
more	O
iterations	O
which	O
lead	O
to	O
more	O
accurate	O
square	O
root	O
is	O
not	O
helpful	O
for	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
AlexNet	S-Method
.	O

From	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
observe	O
that	O
iSQRT	B-Method
-	I-Method
COV	E-Method
with	O
ResNet	B-Method
computing	I-Method
approximate	I-Method
square	I-Method
root	E-Method
performs	O
better	O
than	O
MPN	B-Method
-	I-Method
COV	E-Method
which	O
can	O
obtain	O
exact	O
square	O
root	O
by	O
EIG	S-Method
.	O

Recall	O
that	O
,	O
for	O
covariance	B-Task
pooling	I-Task
ConvNets	E-Task
,	O
we	O
face	O
the	O
problem	O
of	O
small	O
sample	O
of	O
large	O
dimensionality	O
,	O
and	O
matrix	B-Method
square	I-Method
root	E-Method
is	O
consistent	O
with	O
general	O
shrinkage	B-Method
principle	I-Method
of	I-Method
robust	I-Method
covariance	I-Method
estimation	E-Method
.	O

Hence	O
,	O
we	O
conjuncture	O
that	O
approximate	B-Method
matrix	I-Method
square	I-Method
root	E-Method
may	O
be	O
a	O
better	O
robust	B-Method
covariance	I-Method
estimator	E-Method
than	O
the	O
exact	O
square	B-Method
root	E-Method
.	O

Despite	O
this	O
analysis	O
,	O
we	O
think	O
this	O
problem	O
is	O
worth	O
future	O
research	O
.	O

Compactness	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
Our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
outputs	O
32k	B-Method
-	I-Method
dimensional	I-Method
representation	E-Method
which	O
is	O
high	O
.	O

Here	O
we	O
consider	O
to	O
compress	O
this	O
representation	O
.	O

Compactness	S-Method
by	O
PCA	S-Method
is	O
not	O
viable	O
since	O
obtaining	O
the	O
principal	B-Method
components	E-Method
on	O
ImageNet	S-Material
is	O
too	O
expensive	O
.	O

CBP	S-Method
is	O
not	O
applicable	O
to	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
as	O
well	O
,	O
as	O
it	O
does	O
not	O
explicitly	O
estimate	O
the	O
covariance	O
matrix	O
.	O

We	O
propose	O
a	O
simple	O
scheme	O
,	O
which	O
decreases	O
the	O
dimension	S-Metric
(	O
dim	O
.	O

)	O
of	O
covariance	B-Method
representation	E-Method
by	O
lowering	O
the	O
number	O
of	O
channels	O
of	O
convolutional	B-Method
layer	E-Method
before	O
our	O
covariance	B-Method
pooling	E-Method
.	O

Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
results	O
of	O
compact	O
iSQRT	B-Method
-	I-Method
COV	E-Method
.	O

The	O
recognition	B-Metric
error	E-Metric
increases	O
slightly	O
(	O
)	O
when	O
decreases	O
from	O
256	O
to	O
128	O
(	O
correspondingly	O
,	O
dim	O
.	O

of	O
image	B-Method
representation	E-Method
)	O
.	O

The	O
error	B-Metric
rate	E-Metric
is	O
23.73	O
if	O
the	O
dimension	O
is	O
compressed	O
to	O
2	O
K	O
,	O
still	O
outperforming	O
the	O
original	O
ResNet	B-Method
-	I-Method
50	E-Method
which	O
performs	O
global	B-Method
average	I-Method
pooling	E-Method
.	O

subsection	O
:	O
Fine	B-Task
-	I-Task
grained	I-Task
Visual	I-Task
Categorization	E-Task
(	O
FGVC	S-Method
)	O
Finally	O
,	O
we	O
apply	O
iSQRT	B-Method
-	I-Method
COV	E-Method
models	O
pre	O
-	O
trained	O
on	O
ImageNet	S-Method
to	O
FGVC	S-Method
.	O

For	O
fair	O
comparison	O
,	O
we	O
follow	O
for	O
experimental	O
setting	O
and	O
evaluation	O
protocol	O
.	O

On	O
all	O
datasets	O
,	O
we	O
crop	O
patches	O
as	O
input	O
images	O
.	O

We	O
replace	O
1000	B-Method
-	I-Method
way	I-Method
softmax	I-Method
layer	E-Method
of	O
a	O
pre	O
-	O
trained	O
iSQRT	B-Method
-	I-Method
COV	E-Method
model	O
by	O
a	O
k	B-Method
-	I-Method
way	I-Method
softmax	I-Method
layer	E-Method
,	O
where	O
is	O
number	O
of	O
classes	O
in	O
the	O
fine	O
-	O
grained	O
dataset	O
,	O
and	O
finetune	O
the	O
network	O
using	O
SGD	S-Method
with	O
momentum	O
of	O
0.9	O
for	O
50	O
100	O
epochs	O
with	O
a	O
small	O
learning	B-Metric
rate	E-Metric
(	O
)	O
for	O
all	O
layers	O
except	O
the	O
fully	O
-	O
connected	O
layer	O
,	O
which	O
is	O
set	O
to	O
.	O

We	O
use	O
horizontal	O
flipping	O
as	O
data	B-Task
augmentation	E-Task
.	O

After	O
finetuning	O
,	O
the	O
outputs	O
of	O
iSQRT	B-Method
-	I-Method
COV	E-Method
layer	O
are	O
normalized	O
before	O
inputted	O
to	O
train	O
one	B-Method
-	I-Method
vs	I-Method
-	I-Method
all	I-Method
linear	I-Method
SVMs	E-Method
with	O
hyperparameter	S-Method
.	O

We	O
predict	O
the	O
label	O
of	O
a	O
test	O
image	O
by	O
averaging	O
SVM	O
scores	O
of	O
the	O
image	O
and	O
its	O
horizontal	O
flip	O
.	O

ResNet	O
-	O
50	O
iSQRT	B-Method
-	I-Method
COV	E-Method
VGG	O
-	O
D	O
Improved	O
B	O
-	O
CNN	O
Tab	O
.	O

[	O
reference	O
]	O
presents	O
classification	S-Task
results	O
of	O
different	O
methods	O
,	O
where	O
column	O
3	O
lists	O
the	O
dimension	O
of	O
the	O
corresponding	O
representation	O
.	O

With	O
ResNet	B-Method
-	I-Method
50	I-Method
architecture	E-Method
,	O
KP	S-Method
performs	O
much	O
better	O
than	O
CBP	S-Method
,	O
while	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
8	O
K	O
)	O
respectively	O
outperforms	O
KP	S-Method
(	O
14	O
K	O
)	O
by	O
about	O
2.6	O
%	O
,	O
3.8	O
%	O
and	O
0.6	O
%	O
on	O
Birds	S-Material
,	O
Aircrafts	S-Material
and	O
Cars	S-Material
,	O
and	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
32	O
K	O
)	O
further	O
improves	O
accuracy	S-Metric
.	O

Note	O
that	O
KP	S-Method
combines	O
first	O
-	O
order	O
up	O
to	O
fourth	O
-	O
order	O
statistics	O
while	O
iSQRT	B-Method
-	I-Method
COV	E-Method
only	O
exploits	O
second	O
-	O
order	O
one	O
.	O

With	O
VGG	B-Method
-	I-Method
D	E-Method
,	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
32k	O
)	O
matches	O
or	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
competitors	O
,	O
but	O
inferior	O
to	O
iSQRT	B-Method
-	I-Method
COV	E-Method
(	O
32k	O
)	O
with	O
ResNet	B-Method
-	I-Method
50	E-Method
.	O

On	O
all	O
fine	O
-	O
grained	O
datasets	O
,	O
KP	S-Method
and	O
CBP	S-Method
with	O
16	B-Method
-	I-Method
layer	I-Method
VGG	I-Method
-	I-Method
D	E-Method
perform	O
better	O
than	O
their	O
counterparts	O
with	O
50	B-Method
-	I-Method
layer	I-Method
ResNet	E-Method
,	O
despite	O
the	O
fact	O
that	O
ResNet	B-Method
-	I-Method
50	E-Method
significantly	O
outperforms	O
VGG	B-Method
-	I-Method
D	E-Method
on	O
ImageNet	S-Material
.	O

The	O
reason	O
may	O
be	O
that	O
the	O
last	O
convolution	B-Method
layer	E-Method
of	O
pre	O
-	O
trained	O
ResNet	B-Method
-	I-Method
50	E-Method
outputs	O
2048	O
-	O
dimensional	O
features	O
,	O
much	O
higher	O
than	O
512	B-Method
-	I-Method
dimensional	I-Method
one	I-Method
of	I-Method
VGG	I-Method
-	I-Method
D	E-Method
,	O
which	O
are	O
not	O
suitable	O
for	O
existing	O
second	B-Method
-	I-Method
or	I-Method
higher	I-Method
-	I-Method
order	I-Method
pooling	I-Method
methods	E-Method
.	O

Different	O
from	O
all	O
existing	O
methods	O
which	O
use	O
models	O
pre	O
-	O
trained	O
on	O
ImageNet	S-Material
with	O
first	O
-	O
order	O
information	O
,	O
our	O
pre	O
-	O
trained	B-Method
models	E-Method
are	O
of	O
second	O
-	O
order	O
.	O

Using	O
pre	O
-	O
trained	O
iSQRT	B-Method
-	I-Method
COV	E-Method
models	O
with	O
ResNet	B-Method
-	I-Method
50	E-Method
,	O
we	O
achieve	O
recognition	S-Task
results	O
superior	O
to	O
all	O
the	O
compared	O
methods	O
,	O
and	O
furthermore	O
,	O
establish	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
three	O
fine	O
-	O
grained	O
benchmarks	O
using	O
iSQRT	B-Method
-	I-Method
COV	E-Method
model	O
with	O
ResNet	B-Method
-	I-Method
101	E-Method
.	O

section	O
:	O
Conclusion	O
We	O
presented	O
an	O
iterative	B-Method
matrix	I-Method
square	I-Method
root	I-Method
normalization	I-Method
of	I-Method
covariance	I-Method
pooling	E-Method
(	O
iSQRT	B-Method
-	I-Method
COV	E-Method
)	O
network	O
which	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Compared	O
to	O
existing	O
works	O
depending	O
heavily	O
on	O
GPU	O
unfriendly	O
EIG	S-Method
or	O
SVD	S-Method
,	O
our	O
method	O
,	O
based	O
on	O
coupled	B-Method
Newton	I-Method
-	I-Method
Schulz	I-Method
iteration	E-Method
,	O
runs	O
much	O
faster	O
as	O
it	O
involves	O
only	O
matrix	B-Method
multiplications	E-Method
,	O
suitable	O
for	O
parallel	B-Task
implementation	E-Task
on	O
GPU	O
.	O

We	O
validated	O
our	O
method	O
on	O
both	O
large	B-Material
-	I-Material
scale	I-Material
ImageNet	I-Material
dataset	E-Material
and	O
challenging	O
fine	O
-	O
grained	O
benchmarks	O
.	O

Given	O
efficiency	O
and	O
promising	O
performance	O
of	O
our	O
iSQRT	B-Method
-	I-Method
COV	E-Method
,	O
we	O
hope	O
global	B-Method
covariance	I-Method
pooling	E-Method
will	O
be	O
a	O
promising	O
alternative	O
to	O
global	B-Method
average	I-Method
pooling	E-Method
in	O
other	O
deep	B-Method
network	I-Method
architectures	E-Method
,	O
e.g.	O
,	O
ResNeXt	S-Method
,	O
Inception	S-Method
and	O
DenseNet	S-Method
.	O

bibliography	O
:	O
References	O
document	O
:	O
Abstractive	B-Task
Text	I-Task
Summarization	E-Task
using	O
Sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
RNNs	E-Method
and	O
Beyond	O
In	O
this	O
work	O
,	O
we	O
model	O
abstractive	B-Task
text	I-Task
summarization	E-Task
using	O
Attentional	B-Method
Encoder	I-Method
-	I-Method
Decoder	I-Method
Recurrent	I-Method
Neural	I-Method
Networks	E-Method
,	O
and	O
show	O
that	O
they	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
two	O
different	O
corpora	O
.	O

We	O
propose	O
several	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	S-Task
that	O
are	O
not	O
adequately	O
modeled	O
by	O
the	O
basic	O
architecture	O
,	O
such	O
as	O
modeling	O
key	O
-	O
words	O
,	O
capturing	O
the	O
hierarchy	O
of	O
sentence	O
-	O
to	O
-	O
word	O
structure	O
,	O
and	O
emitting	O
words	O
that	O
are	O
rare	O
or	O
unseen	O
at	O
training	O
time	O
.	O

Our	O
work	O
shows	O
that	O
many	O
of	O
our	O
proposed	O
models	O
contribute	O
to	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
consisting	O
of	O
multi	O
-	O
sentence	O
summaries	O
,	O
and	O
establish	O
performance	O
benchmarks	O
for	O
further	O
research	O
.	O

section	O
:	O
Introduction	O
Abstractive	B-Task
text	I-Task
summarization	E-Task
is	O
the	O
task	O
of	O
generating	O
a	O
headline	S-Task
or	O
a	O
short	O
summary	O
consisting	O
of	O
a	O
few	O
sentences	O
that	O
captures	O
the	O
salient	O
ideas	O
of	O
an	O
article	O
or	O
a	O
passage	O
.	O

We	O
use	O
the	O
adjective	O
‘	O
abstractive	O
’	O
to	O
denote	O
a	O
summary	O
that	O
is	O
not	O
a	O
mere	O
selection	O
of	O
a	O
few	O
existing	O
passages	O
or	O
sentences	O
extracted	O
from	O
the	O
source	O
,	O
but	O
a	O
compressed	O
paraphrasing	O
of	O
the	O
main	O
contents	O
of	O
the	O
document	O
,	O
potentially	O
using	O
vocabulary	O
unseen	O
in	O
the	O
source	O
document	O
.	O

This	O
task	O
can	O
also	O
be	O
naturally	O
cast	O
as	O
mapping	O
an	O
input	O
sequence	O
of	O
words	O
in	O
a	O
source	O
document	O
to	O
a	O
target	O
sequence	O
of	O
words	O
called	O
summary	O
.	O

In	O
the	O
recent	O
past	O
,	O
deep	B-Method
-	I-Method
learning	I-Method
based	I-Method
models	E-Method
that	O
map	O
an	O
input	O
sequence	O
into	O
another	O
output	O
sequence	O
,	O
called	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
models	E-Method
,	O
have	O
been	O
successful	O
in	O
many	O
problems	O
such	O
as	O
machine	B-Task
translation	E-Task
,	O
speech	B-Task
recognition	E-Task
and	O
video	B-Task
captioning	E-Task
.	O

In	O
the	O
framework	O
of	O
sequence	B-Method
-	I-Method
to	I-Method
-	I-Method
sequence	I-Method
models	E-Method
,	O
a	O
very	O
relevant	O
model	O
to	O
our	O
task	O
is	O
the	O
attentional	O
Recurrent	B-Method
Neural	I-Method
Network	E-Method
(	O
RNN	S-Method
)	O
encoder	B-Method
-	I-Method
decoder	I-Method
model	E-Method
proposed	O
in	O
nmt	S-Method
,	O
which	O
has	O
produced	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
machine	B-Task
translation	E-Task
(	O
MT	S-Task
)	O
,	O
which	O
is	O
also	O
a	O
natural	B-Task
language	I-Task
task	E-Task
.	O

Despite	O
the	O
similarities	O
,	O
abstractive	B-Task
summarization	E-Task
is	O
a	O
very	O
different	O
problem	O
from	O
MT	S-Task
.	O

Unlike	O
in	O
MT	S-Task
,	O
the	O
target	O
(	O
summary	O
)	O
is	O
typically	O
very	O
short	O
and	O
does	O
not	O
depend	O
very	O
much	O
on	O
the	O
length	O
of	O
the	O
source	O
(	O
document	O
)	O
in	O
summarization	S-Task
.	O

Additionally	O
,	O
a	O
key	O
challenge	O
in	O
summarization	S-Task
is	O
to	O
optimally	O
compress	O
the	O
original	O
document	O
in	O
a	O
lossy	O
manner	O
such	O
that	O
the	O
key	O
concepts	O
in	O
the	O
original	O
document	O
are	O
preserved	O
,	O
whereas	O
in	O
MT	S-Task
,	O
the	O
translation	S-Task
is	O
expected	O
to	O
be	O
loss	O
-	O
less	O
.	O

In	O
translation	S-Task
,	O
there	O
is	O
a	O
strong	O
notion	O
of	O
almost	O
one	O
-	O
to	O
-	O
one	O
word	O
-	O
level	O
alignment	O
between	O
source	O
and	O
target	O
,	O
but	O
in	O
summarization	S-Task
,	O
it	O
is	O
less	O
obvious	O
.	O

We	O
make	O
the	O
following	O
main	O
contributions	O
in	O
this	O
work	O
:	O
(	O
i	O
)	O
We	O
apply	O
the	O
off	O
-	O
the	O
-	O
shelf	O
attentional	O
encoder	O
-	O
decoder	O
RNN	S-Method
that	O
was	O
originally	O
developed	O
for	O
machine	B-Task
translation	E-Task
to	O
summarization	S-Task
,	O
and	O
show	O
that	O
it	O
already	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
on	O
two	O
different	O
English	B-Material
corpora	E-Material
.	O

(	O
ii	O
)	O
Motivated	O
by	O
concrete	O
problems	O
in	O
summarization	S-Task
that	O
are	O
not	O
sufficiently	O
addressed	O
by	O
the	O
machine	B-Task
translation	E-Task
based	O
model	O
,	O
we	O
propose	O
novel	O
models	O
and	O
show	O
that	O
they	O
provide	O
additional	O
improvement	O
in	O
performance	O
.	O

(	O
iii	O
)	O
We	O
propose	O
a	O
new	O
dataset	O
for	O
the	O
task	O
of	O
abstractive	B-Task
summarization	E-Task
of	O
a	O
document	O
into	O
multiple	O
sentences	O
and	O
establish	O
benchmarks	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
describe	O
each	O
specific	O
problem	O
in	O
abstractive	B-Task
summarization	E-Task
that	O
we	O
aim	O
to	O
solve	O
,	O
and	O
present	O
a	O
novel	O
model	O
that	O
addresses	O
it	O
.	O

Section	O
[	O
reference	O
]	O
contextualizes	O
our	O
models	O
with	O
respect	O
to	O
closely	O
related	O
work	O
on	O
the	O
topic	O
of	O
abstractive	B-Task
text	I-Task
summarization	E-Task
.	O

We	O
present	O
the	O
results	O
of	O
our	O
experiments	O
on	O
three	O
different	O
data	O
sets	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
also	O
present	O
some	O
qualitative	O
analysis	O
of	O
the	O
output	O
from	O
our	O
models	O
in	O
Section	O
[	O
reference	O
]	O
before	O
concluding	O
the	O
paper	O
with	O
remarks	O
on	O
our	O
future	O
direction	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Models	O
In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
basic	O
encoder	O
-	O
decoder	O
RNN	S-Method
that	O
serves	O
as	O
our	O
baseline	O
and	O
then	O
propose	O
several	O
novel	O
models	O
for	O
summarization	S-Task
,	O
each	O
addressing	O
a	O
specific	O
weakness	O
in	O
the	O
baseline	O
.	O

subsection	O
:	O
Encoder	O
-	O
Decoder	O
RNN	S-Method
with	O
Attention	S-Method
and	O
Large	B-Task
Vocabulary	I-Task
Trick	E-Task
Our	O
baseline	O
model	O
corresponds	O
to	O
the	O
neural	O
machine	B-Task
translation	E-Task
model	O
used	O
in	O
nmt	S-Method
.	O

The	O
encoder	S-Method
consists	O
of	O
a	O
bidirectional	O
GRU	O
-	O
RNN	S-Method
,	O
while	O
the	O
decoder	S-Method
consists	O
of	O
a	O
uni	O
-	O
directional	O
GRU	O
-	O
RNN	S-Method
with	O
the	O
same	O
hidden	O
-	O
state	O
size	O
as	O
that	O
of	O
the	O
encoder	O
,	O
and	O
an	O
attention	B-Method
mechanism	E-Method
over	O
the	O
source	O
-	O
hidden	O
states	O
and	O
a	O
soft	B-Method
-	I-Method
max	I-Method
layer	E-Method
over	O
target	O
vocabulary	O
to	O
generate	O
words	O
.	O

In	O
the	O
interest	O
of	O
space	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
original	O
paper	O
for	O
a	O
detailed	O
treatment	O
of	O
this	O
model	O
.	O

In	O
addition	O
to	O
the	O
basic	O
model	O
,	O
we	O
also	O
adapted	O
to	O
the	O
summarization	B-Task
problem	E-Task
,	O
the	O
large	B-Task
vocabulary	E-Task
‘	O
trick	O
’	O
(	O
LVT	S-Method
)	O
described	O
in	O
lvt	S-Method
.	O

In	O
our	O
approach	O
,	O
the	O
decoder	O
-	O
vocabulary	O
of	O
each	O
mini	O
-	O
batch	O
is	O
restricted	O
to	O
words	O
in	O
the	O
source	O
documents	O
of	O
that	O
batch	O
.	O

In	O
addition	O
,	O
the	O
most	O
frequent	O
words	O
in	O
the	O
target	O
dictionary	O
are	O
added	O
until	O
the	O
vocabulary	O
reaches	O
a	O
fixed	O
size	O
.	O

The	O
aim	O
of	O
this	O
technique	O
is	O
to	O
reduce	O
the	O
size	O
of	O
the	O
soft	B-Method
-	I-Method
max	I-Method
layer	E-Method
of	O
the	O
decoder	S-Method
which	O
is	O
the	O
main	O
computational	B-Metric
bottleneck	E-Metric
.	O

In	O
addition	O
,	O
this	O
technique	O
also	O
speeds	O
up	O
convergence	S-Task
by	O
focusing	O
the	O
modeling	O
effort	O
only	O
on	O
the	O
words	O
that	O
are	O
essential	O
to	O
a	O
given	O
example	O
.	O

This	O
technique	O
is	O
particularly	O
well	O
suited	O
to	O
summarization	S-Task
since	O
a	O
large	O
proportion	O
of	O
the	O
words	O
in	O
the	O
summary	O
come	O
from	O
the	O
source	O
document	O
in	O
any	O
case	O
.	O

subsection	O
:	O
Capturing	B-Task
Keywords	E-Task
using	O
Feature	B-Method
-	I-Method
rich	I-Method
Encoder	E-Method
In	O
summarization	S-Task
,	O
one	O
of	O
the	O
key	O
challenges	O
is	O
to	O
identify	O
the	O
key	O
concepts	O
and	O
key	O
entities	O
in	O
the	O
document	O
,	O
around	O
which	O
the	O
story	O
revolves	O
.	O

In	O
order	O
to	O
accomplish	O
this	O
goal	O
,	O
we	O
may	O
need	O
to	O
go	O
beyond	O
the	O
word	B-Method
-	I-Method
embeddings	I-Method
-	I-Method
based	I-Method
representation	E-Method
of	O
the	O
input	O
document	O
and	O
capture	O
additional	O
linguistic	O
features	O
such	O
as	O
parts	O
-	O
of	O
-	O
speech	O
tags	O
,	O
named	O
-	O
entity	O
tags	O
,	O
and	O
TF	O
and	O
IDF	O
statistics	O
of	O
the	O
words	O
.	O

We	O
therefore	O
create	O
additional	O
look	B-Method
-	I-Method
up	I-Method
based	I-Method
embedding	I-Method
matrices	E-Method
for	O
the	O
vocabulary	O
of	O
each	O
tag	O
-	O
type	O
,	O
similar	O
to	O
the	O
embeddings	O
for	O
words	O
.	O

For	O
continuous	O
features	O
such	O
as	O
TF	O
and	O
IDF	S-Method
,	O
we	O
convert	O
them	O
into	O
categorical	O
values	O
by	O
discretizing	O
them	O
into	O
a	O
fixed	O
number	O
of	O
bins	O
,	O
and	O
use	O
one	B-Method
-	I-Method
hot	I-Method
representations	E-Method
to	O
indicate	O
the	O
bin	O
number	O
they	O
fall	O
into	O
.	O

This	O
allows	O
us	O
to	O
map	O
them	O
into	O
an	O
embeddings	O
matrix	O
like	O
any	O
other	O
tag	O
-	O
type	O
.	O

Finally	O
,	O
for	O
each	O
word	O
in	O
the	O
source	O
document	O
,	O
we	O
simply	O
look	O
-	O
up	O
its	O
embeddings	O
from	O
all	O
of	O
its	O
associated	O
tags	O
and	O
concatenate	O
them	O
into	O
a	O
single	O
long	O
vector	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

On	O
the	O
target	O
side	O
,	O
we	O
continue	O
to	O
use	O
only	O
word	B-Method
-	I-Method
based	I-Method
embeddings	E-Method
as	O
the	O
representation	O
.	O

subsection	O
:	O
Modeling	B-Task
Rare	I-Task
/	I-Task
Unseen	I-Task
Words	E-Task
using	O
Switching	B-Method
Generator	I-Method
-	I-Method
Pointer	E-Method
Often	O
-	O
times	O
in	O
summarization	S-Task
,	O
the	O
keywords	O
or	O
named	O
-	O
entities	O
in	O
a	O
test	O
document	O
that	O
are	O
central	O
to	O
the	O
summary	O
may	O
actually	O
be	O
unseen	O
or	O
rare	O
with	O
respect	O
to	O
training	O
data	O
.	O

Since	O
the	O
vocabulary	O
of	O
the	O
decoder	O
is	O
fixed	O
at	O
training	O
time	O
,	O
it	O
can	O
not	O
emit	O
these	O
unseen	O
words	O
.	O

Instead	O
,	O
a	O
most	O
common	O
way	O
of	O
handling	O
these	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
is	O
to	O
emit	O
an	O
‘	O
UNK	O
’	O
token	O
as	O
a	O
placeholder	O
.	O

However	O
this	O
does	O
not	O
result	O
in	O
legible	O
summaries	O
.	O

In	O
summarization	S-Task
,	O
an	O
intuitive	O
way	O
to	O
handle	O
such	O
OOV	B-Material
words	E-Material
is	O
to	O
simply	O
point	O
to	O
their	O
location	O
in	O
the	O
source	O
document	O
instead	O
.	O

We	O
model	O
this	O
notion	O
using	O
our	O
novel	O
switching	B-Method
decoder	E-Method
/	O
pointer	B-Method
architecture	E-Method
which	O
is	O
graphically	O
represented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
this	O
model	O
,	O
the	O
decoder	S-Method
is	O
equipped	O
with	O
a	O
‘	O
switch	O
’	O
that	O
decides	O
between	O
using	O
the	O
generator	O
or	O
a	O
pointer	S-Method
at	O
every	O
time	O
-	O
step	O
.	O

If	O
the	O
switch	O
is	O
turned	O
on	O
,	O
the	O
decoder	O
produces	O
a	O
word	O
from	O
its	O
target	O
vocabulary	O
in	O
the	O
normal	O
fashion	O
.	O

However	O
,	O
if	O
the	O
switch	O
is	O
turned	O
off	O
,	O
the	O
decoder	S-Method
instead	O
generates	O
a	O
pointer	S-Method
to	O
one	O
of	O
the	O
word	O
-	O
positions	O
in	O
the	O
source	O
.	O

The	O
word	O
at	O
the	O
pointer	S-Method
-	O
location	O
is	O
then	O
copied	O
into	O
the	O
summary	O
.	O

The	O
switch	S-Method
is	O
modeled	O
as	O
a	O
sigmoid	B-Method
activation	I-Method
function	E-Method
over	O
a	O
linear	B-Method
layer	E-Method
based	O
on	O
the	O
entire	O
available	O
context	O
at	O
each	O
time	O
-	O
step	O
as	O
shown	O
below	O
.	O

where	O
is	O
the	O
probability	O
of	O
the	O
switch	O
turning	O
on	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	O
,	O
is	O
the	O
hidden	O
state	O
,	O
is	O
the	O
embedding	O
vector	O
of	O
the	O
emission	O
from	O
the	O
previous	O
time	O
step	O
,	O
is	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
,	O
and	O
and	O
are	O
the	O
switch	O
parameters	O
.	O

We	O
use	O
attention	O
distribution	O
over	O
word	O
positions	O
in	O
the	O
document	O
as	O
the	O
distribution	O
to	O
sample	O
the	O
pointer	S-Method
from	O
.	O

In	O
the	O
above	O
equation	O
,	O
is	O
the	O
pointer	S-Method
value	O
at	O
word	O
-	O
position	O
in	O
the	O
summary	O
,	O
sampled	O
from	O
the	O
attention	B-Method
distribution	E-Method
over	O
the	O
document	O
word	O
-	O
positions	O
,	O
where	O
is	O
the	O
probability	O
of	O
the	O
time	O
-	O
step	O
in	O
the	O
decoder	O
pointing	O
to	O
the	O
position	O
in	O
the	O
document	O
,	O
and	O
is	O
the	O
encoder	O
’s	O
hidden	O
state	O
at	O
position	O
.	O

At	O
training	O
time	O
,	O
we	O
provide	O
the	O
model	O
with	O
explicit	O
pointer	S-Method
information	O
whenever	O
the	O
summary	O
word	O
does	O
not	O
exist	O
in	O
the	O
target	O
vocabulary	O
.	O

When	O
the	O
OOV	O
word	O
in	O
summary	O
occurs	O
in	O
multiple	O
document	O
positions	O
,	O
we	O
break	O
the	O
tie	O
in	O
favor	O
of	O
its	O
first	O
occurrence	O
.	O

At	O
training	O
time	O
,	O
we	O
optimize	O
the	O
conditional	O
log	O
-	O
likelihood	O
shown	O
below	O
,	O
with	O
additional	O
regularization	O
penalties	O
.	O

where	O
and	O
are	O
the	O
summary	O
and	O
document	O
words	O
respectively	O
,	O
is	O
an	O
indicator	O
function	O
that	O
is	O
set	O
to	O
0	O
whenever	O
the	O
word	O
at	O
position	O
in	O
the	O
summary	O
is	O
OOV	O
with	O
respect	O
to	O
the	O
decoder	O
vocabulary	O
.	O

At	O
test	O
time	O
,	O
the	O
model	O
decides	O
automatically	O
at	O
each	O
time	O
-	O
step	O
whether	O
to	O
generate	O
or	O
to	O
point	O
,	O
based	O
on	O
the	O
estimated	O
switch	O
probability	O
.	O

We	O
simply	O
use	O
the	O
of	O
the	O
posterior	O
probability	O
of	O
generation	O
or	O
pointing	O
to	O
generate	O
the	O
best	O
output	O
at	O
each	O
time	O
step	O
.	O

The	O
pointer	S-Method
mechanism	O
may	O
be	O
more	O
robust	O
in	O
handling	O
rare	B-Task
words	E-Task
because	O
it	O
uses	O
the	O
encoder	B-Method
’s	I-Method
hidden	I-Method
-	I-Method
state	I-Method
representation	E-Method
of	O
rare	O
words	O
to	O
decide	O
which	O
word	O
from	O
the	O
document	O
to	O
point	O
to	O
.	O

Since	O
the	O
hidden	O
state	O
depends	O
on	O
the	O
entire	O
context	O
of	O
the	O
word	O
,	O
the	O
model	O
is	O
able	O
to	O
accurately	O
point	O
to	O
unseen	O
words	O
although	O
they	O
do	O
not	O
appear	O
in	O
the	O
target	O
vocabulary	O
.	O

subsection	O
:	O
Capturing	B-Task
Hierarchical	I-Task
Document	I-Task
Structure	E-Task
with	O
Hierarchical	O
Attention	O
In	O
datasets	O
where	O
the	O
source	O
document	O
is	O
very	O
long	O
,	O
in	O
addition	O
to	O
identifying	O
the	O
keywords	O
in	O
the	O
document	O
,	O
it	O
is	O
also	O
important	O
to	O
identify	O
the	O
key	O
sentences	O
from	O
which	O
the	O
summary	O
can	O
be	O
drawn	O
.	O

This	O
model	O
aims	O
to	O
capture	O
this	O
notion	O
of	O
two	O
levels	O
of	O
importance	O
using	O
two	O
bi	B-Method
-	I-Method
directional	I-Method
RNNs	E-Method
on	O
the	O
source	O
side	O
,	O
one	O
at	O
the	O
word	O
level	O
and	O
the	O
other	O
at	O
the	O
sentence	O
level	O
.	O

The	O
attention	B-Method
mechanism	E-Method
operates	O
at	O
both	O
levels	O
simultaneously	O
.	O

The	O
word	O
-	O
level	O
attention	O
is	O
further	O
re	O
-	O
weighted	O
by	O
the	O
corresponding	O
sentence	O
-	O
level	O
attention	O
and	O
re	O
-	O
normalized	O
as	O
shown	O
below	O
:	O
where	O
is	O
the	O
word	O
-	O
level	O
attention	O
weight	O
at	O
position	O
of	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
ID	O
of	O
the	O
sentence	O
at	O
word	O
position	O
,	O
is	O
the	O
sentence	O
-	O
level	O
attention	O
weight	O
for	O
the	O
sentence	O
in	O
the	O
source	O
,	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
re	O
-	O
scaled	O
attention	O
at	O
the	O
word	O
position	O
.	O

The	O
re	O
-	O
scaled	O
attention	O
is	O
then	O
used	O
to	O
compute	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
that	O
goes	O
as	O
input	O
to	O
the	O
hidden	O
state	O
of	O
the	O
decoder	O
.	O

Further	O
,	O
we	O
also	O
concatenate	O
additional	O
positional	O
embeddings	O
to	O
the	O
hidden	O
state	O
of	O
the	O
sentence	O
-	O
level	O
RNN	S-Method
to	O
model	O
positional	O
importance	O
of	O
sentences	O
in	O
the	O
document	O
.	O

This	O
architecture	O
therefore	O
models	O
key	O
sentences	O
as	O
well	O
as	O
keywords	O
within	O
those	O
sentences	O
jointly	O
.	O

A	O
graphical	B-Method
representation	E-Method
of	O
this	O
model	O
is	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
A	O
vast	O
majority	O
of	O
past	O
work	O
in	O
summarization	S-Task
has	O
been	O
extractive	S-Task
,	O
which	O
consists	O
of	O
identifying	O
key	O
sentences	O
or	O
passages	O
in	O
the	O
source	O
document	O
and	O
reproducing	O
them	O
as	O
summary	O
.	O

Humans	O
on	O
the	O
other	O
hand	O
,	O
tend	O
to	O
paraphrase	O
the	O
original	O
story	O
in	O
their	O
own	O
words	O
.	O

As	O
such	O
,	O
human	O
summaries	O
are	O
abstractive	O
in	O
nature	O
and	O
seldom	O
consist	O
of	O
reproduction	O
of	O
original	O
sentences	O
from	O
the	O
document	O
.	O

The	O
task	O
of	O
abstractive	B-Task
summarization	E-Task
has	O
been	O
standardized	O
using	O
the	O
DUC	B-Material
-	I-Material
2003	E-Material
and	O
DUC	B-Material
-	I-Material
2004	E-Material
competitions	O
.	O

The	O
data	O
for	O
these	O
tasks	O
consists	O
of	O
news	O
stories	O
from	O
various	O
topics	O
with	O
multiple	O
reference	O
summaries	O
per	O
story	O
generated	O
by	O
humans	O
.	O

The	O
best	O
performing	O
system	O
on	O
the	O
DUC	B-Material
-	I-Material
2004	E-Material
task	O
,	O
called	O
TOPIARY	S-Method
,	O
used	O
a	O
combination	O
of	O
linguistically	B-Method
motivated	I-Method
compression	I-Method
techniques	E-Method
,	O
and	O
an	O
unsupervised	B-Method
topic	I-Method
detection	I-Method
algorithm	E-Method
that	O
appends	O
keywords	O
extracted	O
from	O
the	O
article	O
onto	O
the	O
compressed	O
output	O
.	O

Some	O
of	O
the	O
other	O
notable	O
work	O
in	O
the	O
task	O
of	O
abstractive	B-Task
summarization	E-Task
includes	O
using	O
traditional	O
phrase	O
-	O
table	O
based	O
machine	B-Task
translation	E-Task
approaches	O
,	O
compression	S-Method
using	O
weighted	B-Method
tree	I-Method
-	I-Method
transformation	I-Method
rules	E-Method
and	O
quasi	B-Method
-	I-Method
synchronous	I-Method
grammar	I-Method
approaches	E-Method
.	O

With	O
the	O
emergence	O
of	O
deep	B-Method
learning	E-Method
as	O
a	O
viable	O
alternative	O
for	O
many	O
NLP	B-Task
tasks	E-Task
,	O
researchers	O
have	O
started	O
considering	O
this	O
framework	O
as	O
an	O
attractive	O
,	O
fully	O
data	O
-	O
driven	O
alternative	O
to	O
abstractive	B-Task
summarization	E-Task
.	O

In	O
namas	S-Method
,	O
the	O
authors	O
use	O
convolutional	B-Method
models	E-Method
to	O
encode	O
the	O
source	O
,	O
and	O
a	O
context	B-Method
-	I-Method
sensitive	I-Method
attentional	I-Method
feed	I-Method
-	I-Method
forward	I-Method
neural	I-Method
network	E-Method
to	O
generate	O
the	O
summary	O
,	O
producing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
Gigaword	S-Material
and	O
DUC	B-Material
datasets	E-Material
.	O

In	O
an	O
extension	O
to	O
this	O
work	O
,	O
chopra	O
used	O
a	O
similar	O
convolutional	B-Method
model	E-Method
for	O
the	O
encoder	S-Method
,	O
but	O
replaced	O
the	O
decoder	S-Method
with	O
an	O
RNN	S-Method
,	O
producing	O
further	O
improvement	O
in	O
performance	O
on	O
both	O
datasets	O
.	O

In	O
another	O
paper	O
that	O
is	O
closely	O
related	O
to	O
our	O
work	O
,	O
hu:2015:EMNLP	S-Method
introduce	O
a	O
large	O
dataset	O
for	O
Chinese	B-Task
short	I-Task
text	I-Task
summarization	E-Task
.	O

They	O
show	O
promising	O
results	O
on	O
their	O
Chinese	B-Material
dataset	E-Material
using	O
an	O
encoder	O
-	O
decoder	O
RNN	S-Method
,	O
but	O
do	O
not	O
report	O
experiments	O
on	O
English	B-Material
corpora	E-Material
.	O

In	O
another	O
very	O
recent	O
work	O
,	O
jianpeng	O
used	O
RNN	S-Method
based	O
encoder	O
-	O
decoder	O
for	O
extractive	B-Task
summarization	I-Task
of	I-Task
documents	E-Task
.	O

This	O
model	O
is	O
not	O
directly	O
comparable	O
to	O
ours	O
since	O
their	O
framework	O
is	O
extractive	O
while	O
ours	O
and	O
that	O
of	O
,	O
and	O
is	O
abstractive	O
.	O

Our	O
work	O
starts	O
with	O
the	O
same	O
framework	O
as	O
,	O
where	O
we	O
use	O
RNNs	S-Method
for	O
both	O
source	O
and	O
target	O
,	O
but	O
we	O
go	O
beyond	O
the	O
standard	O
architecture	O
and	O
propose	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	S-Task
.	O

We	O
also	O
note	O
that	O
this	O
work	O
is	O
an	O
extended	O
version	O
of	O
nallapati	O
.	O

In	O
addition	O
to	O
performing	O
more	O
extensive	O
experiments	O
compared	O
to	O
that	O
work	O
,	O
we	O
also	O
propose	O
a	O
novel	O
dataset	O
for	O
document	B-Task
summarization	E-Task
on	O
which	O
we	O
establish	O
benchmark	O
numbers	O
too	O
.	O

Below	O
,	O
we	O
analyze	O
the	O
similarities	O
and	O
differences	O
of	O
our	O
proposed	O
models	O
with	O
related	O
work	O
on	O
summarization	S-Task
.	O

Feature	B-Method
-	I-Method
rich	I-Method
encoder	E-Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Linguistic	O
features	O
such	O
as	O
POS	O
tags	O
,	O
and	O
named	O
-	O
entities	O
as	O
well	O
as	O
TF	O
and	O
IDF	O
information	O
were	O
used	O
in	O
many	O
extractive	B-Method
approaches	E-Method
to	O
summarization	S-Task
,	O
but	O
they	O
are	O
novel	O
in	O
the	O
context	O
of	O
deep	B-Method
learning	I-Method
approaches	E-Method
for	O
abstractive	B-Task
summarization	E-Task
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

Switching	O
generator	O
-	O
pointer	S-Method
model	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
This	O
model	O
combines	O
extractive	B-Method
and	I-Method
abstractive	I-Method
approaches	E-Method
to	O
summarization	S-Task
in	O
a	O
single	O
end	B-Method
-	I-Method
to	I-Method
-	I-Method
end	I-Method
framework	E-Method
.	O

namas	S-Method
also	O
used	O
a	O
combination	O
of	O
extractive	B-Method
and	I-Method
abstractive	I-Method
approaches	E-Method
,	O
but	O
their	O
extractive	B-Method
model	E-Method
is	O
a	O
separate	O
log	B-Method
-	I-Method
linear	I-Method
classifier	E-Method
with	O
handcrafted	O
features	O
.	O

Pointer	B-Method
networks	E-Method
have	O
also	O
been	O
used	O
earlier	O
for	O
the	O
problem	O
of	O
rare	B-Task
words	E-Task
in	O
the	O
context	O
of	O
machine	B-Task
translation	E-Task
,	O
but	O
the	O
novel	O
addition	O
of	O
switch	O
in	O
our	O
model	O
allows	O
it	O
to	O
strike	O
a	O
balance	O
between	O
when	O
to	O
be	O
faithful	O
to	O
the	O
original	O
source	O
(	O
e.g.	O
,	O
for	O
named	O
entities	O
and	O
OOV	O
)	O
and	O
when	O
it	O
is	O
allowed	O
to	O
be	O
creative	O
.	O

We	O
believe	O
such	O
a	O
process	O
arguably	O
mimics	O
how	O
human	O
produces	O
summaries	O
.	O

For	O
a	O
more	O
detailed	O
treatment	O
of	O
this	O
model	O
,	O
and	O
experiments	O
on	O
multiple	O
tasks	O
,	O
please	O
refer	O
to	O
the	O
parallel	O
work	O
published	O
by	O
some	O
of	O
the	O
authors	O
of	O
this	O
work	O
.	O

Hierarchical	B-Method
attention	I-Method
model	E-Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Previously	O
proposed	O
hierarchical	B-Method
encoder	I-Method
-	I-Method
decoder	I-Method
models	E-Method
use	O
attention	O
only	O
at	O
sentence	O
-	O
level	O
.	O

The	O
novelty	O
of	O
our	O
approach	O
lies	O
in	O
joint	B-Task
modeling	I-Task
of	I-Task
attention	E-Task
at	O
both	O
sentence	O
and	O
word	O
levels	O
,	O
where	O
the	O
word	O
-	O
level	O
attention	O
is	O
further	O
influenced	O
by	O
sentence	O
-	O
level	O
attention	O
,	O
thus	O
capturing	O
the	O
notion	O
of	O
important	O
sentences	O
and	O
important	O
words	O
within	O
those	O
sentences	O
.	O

Concatenation	B-Method
of	I-Method
positional	I-Method
embeddings	E-Method
with	O
the	O
hidden	O
state	O
at	O
sentence	O
-	O
level	O
is	O
also	O
new	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Gigaword	S-Material
Corpus	O
In	O
this	O
series	O
of	O
experiments	O
,	O
we	O
used	O
the	O
annotated	O
Gigaword	S-Material
corpus	O
as	O
described	O
in	O
namas	O
.	O

We	O
used	O
the	O
scripts	O
made	O
available	O
by	O
the	O
authors	O
of	O
this	O
work	O
to	O
preprocess	O
the	O
data	O
,	O
which	O
resulted	O
in	O
about	O
3.8	O
M	O
training	O
examples	O
.	O

The	O
script	O
also	O
produces	O
about	O
400	O
K	O
validation	O
and	O
test	O
examples	O
,	O
but	O
we	O
created	O
a	O
randomly	O
sampled	O
subset	O
of	O
2000	O
examples	O
each	O
for	O
validation	B-Task
and	I-Task
testing	I-Task
purposes	E-Task
,	O
on	O
which	O
we	O
report	O
our	O
performance	O
.	O

Further	O
,	O
we	O
also	O
acquired	O
the	O
exact	O
test	O
sample	O
used	O
in	O
namas	O
to	O
make	O
precise	O
comparison	O
of	O
our	O
models	O
with	O
theirs	O
.	O

We	O
also	O
made	O
small	O
modifications	O
to	O
the	O
script	O
to	O
extract	O
not	O
only	O
the	O
tokenized	O
words	O
,	O
but	O
also	O
system	O
-	O
generated	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
.	O

Training	O
:	O
For	O
all	O
the	O
models	O
we	O
discuss	O
below	O
,	O
we	O
used	O
200	O
dimensional	O
word2vec	O
vectors	O
trained	O
on	O
the	O
same	O
corpus	O
to	O
initialize	O
the	O
model	O
embeddings	O
,	O
but	O
we	O
allowed	O
them	O
to	O
be	O
updated	O
during	O
training	O
.	O

The	O
hidden	O
state	O
dimension	O
of	O
the	O
encoder	B-Method
and	I-Method
decoder	E-Method
was	O
fixed	O
at	O
400	O
in	O
all	O
our	O
experiments	O
.	O

When	O
we	O
used	O
only	O
the	O
first	O
sentence	O
of	O
the	O
document	O
as	O
the	O
source	O
,	O
as	O
done	O
in	O
namas	O
,	O
the	O
encoder	B-Metric
vocabulary	I-Metric
size	E-Metric
was	O
119	O
,	O
505	O
and	O
that	O
of	O
the	O
decoder	S-Method
stood	O
at	O
68	O
,	O
885	O
.	O

We	O
used	O
Adadelta	S-Method
for	O
training	S-Task
,	O
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.001	O
.	O

We	O
used	O
a	O
batch	O
-	O
size	O
of	O
50	O
and	O
randomly	O
shuffled	O
the	O
training	O
data	O
at	O
every	O
epoch	O
,	O
while	O
sorting	O
every	O
10	O
batches	O
according	O
to	O
their	O
lengths	O
to	O
speed	O
up	O
training	S-Task
.	O

We	O
did	O
not	O
use	O
any	O
dropout	B-Method
or	I-Method
regularization	E-Method
,	O
but	O
applied	O
gradient	B-Method
clipping	E-Method
.	O

We	O
used	O
early	B-Method
stopping	E-Method
based	O
on	O
the	O
validation	O
set	O
and	O
used	O
the	O
best	O
model	O
on	O
the	O
validation	O
set	O
to	O
report	O
all	O
test	O
performance	O
numbers	O
.	O

For	O
all	O
our	O
models	O
,	O
we	O
employ	O
the	O
large	B-Method
-	I-Method
vocabulary	I-Method
trick	E-Method
,	O
where	O
we	O
restrict	O
the	O
decoder	O
vocabulary	O
size	O
to	O
2	O
,	O
000	O
,	O
because	O
it	O
cuts	O
down	O
the	O
training	B-Metric
time	E-Metric
per	O
epoch	O
by	O
nearly	O
three	O
times	O
,	O
and	O
helps	O
this	O
and	O
all	O
subsequent	O
models	O
converge	O
in	O
only	O
50%	O
-	O
75	O
%	O
of	O
the	O
epochs	O
needed	O
for	O
the	O
model	O
based	O
on	O
full	O
vocabulary	O
.	O

Decoding	S-Task
:	O
At	O
decode	O
-	O
time	O
,	O
we	O
used	O
beam	B-Method
search	E-Method
of	O
size	O
5	O
to	O
generate	O
the	O
summary	O
,	O
and	O
limited	O
the	O
size	O
of	O
summary	O
to	O
a	O
maximum	O
of	O
30	O
words	O
,	O
since	O
this	O
is	O
the	O
maximum	O
size	O
we	O
noticed	O
in	O
the	O
sampled	O
validation	O
set	O
.	O

We	O
found	O
that	O
the	O
average	O
system	B-Metric
summary	I-Metric
length	E-Metric
from	O
all	O
our	O
models	O
(	O
7.8	O
to	O
8.3	O
)	O
agrees	O
very	O
closely	O
with	O
that	O
of	O
the	O
ground	O
truth	O
on	O
the	O
validation	O
set	O
(	O
about	O
8.7	O
words	O
)	O
,	O
without	O
any	O
specific	O
tuning	O
.	O

Computational	B-Metric
costs	E-Metric
:	O
We	O
trained	O
all	O
our	O
models	O
on	O
a	O
single	O
Tesla	B-Method
K40	I-Method
GPU	E-Method
.	O

Most	O
models	O
took	O
about	O
10	O
hours	O
per	O
epoch	O
on	O
an	O
average	O
except	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
,	O
which	O
took	O
12	O
hours	O
per	O
epoch	O
.	O

All	O
models	O
typically	O
converged	O
within	O
15	O
epochs	O
using	O
our	O
early	B-Metric
stopping	I-Metric
criterion	E-Metric
based	O
on	O
the	O
validation	B-Metric
cost	E-Metric
.	O

The	O
wall	B-Metric
-	I-Metric
clock	I-Metric
training	I-Metric
time	E-Metric
until	O
convergence	S-Metric
therefore	O
varies	O
between	O
6	O
-	O
8	O
days	O
depending	O
on	O
the	O
model	O
.	O

Generating	B-Task
summaries	E-Task
at	O
test	O
time	O
is	O
reasonably	O
fast	O
with	O
a	O
throughput	O
of	O
about	O
20	O
summaries	O
per	O
second	O
on	O
a	O
single	O
GPU	O
,	O
using	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	O
metrics	O
:	O
Similar	O
to	O
and	O
,	O
we	O
use	O
the	O
full	B-Metric
length	I-Metric
F1	I-Metric
variant	E-Metric
of	O
Rouge	S-Metric
to	O
evaluate	O
our	O
system	O
.	O

Although	O
limited	B-Metric
length	I-Metric
recall	E-Metric
was	O
the	O
preferred	O
metric	O
for	O
most	O
previous	O
work	O
,	O
one	O
of	O
its	O
disadvantages	O
is	O
choosing	O
the	O
length	O
limit	O
which	O
varies	O
from	O
corpus	O
to	O
corpus	O
,	O
making	O
it	O
difficult	O
for	O
researchers	O
to	O
compare	O
performances	O
.	O

Full	B-Task
-	I-Task
length	I-Task
recall	E-Task
,	O
on	O
the	O
other	O
hand	O
,	O
does	O
not	O
impose	O
a	O
length	O
restriction	O
but	O
unfairly	O
favors	O
longer	O
summaries	O
.	O

Full	B-Method
-	I-Method
length	I-Method
F1	E-Method
solves	O
this	O
problem	O
since	O
it	O
can	O
penalize	O
longer	O
summaries	O
,	O
while	O
not	O
imposing	O
a	O
specific	O
length	O
restriction	O
.	O

In	O
addition	O
,	O
we	O
also	O
report	O
the	O
percentage	O
of	O
tokens	O
in	O
the	O
system	O
summary	O
that	O
occur	O
in	O
the	O
source	O
(	O
which	O
we	O
call	O
‘	O
src	O
.	O

copy	B-Metric
rate	E-Metric
’	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

We	O
describe	O
all	O
our	O
experiments	O
and	O
results	O
on	O
the	O
Gigaword	S-Material
corpus	O
below	O
.	O

words	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
1sent	E-Method
:	O
This	O
is	O
the	O
baseline	O
attentional	O
encoder	B-Method
-	I-Method
decoder	I-Method
model	E-Method
with	O
the	O
large	B-Method
vocabulary	I-Method
trick	E-Method
.	O

This	O
model	O
is	O
trained	O
only	O
on	O
the	O
first	O
sentence	O
from	O
the	O
source	O
document	O
,	O
as	O
done	O
in	O
namas	O
.	O

words	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
:	O
This	O
model	O
is	O
identical	O
to	O
the	O
model	O
above	O
except	O
for	O
the	O
fact	O
that	O
it	O
is	O
trained	O
on	O
the	O
first	O
two	O
sentences	O
from	O
the	O
source	O
.	O

On	O
this	O
corpus	O
,	O
adding	O
the	O
additional	O
sentence	O
in	O
the	O
source	O
does	O
seem	O
to	O
aid	O
performance	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
tried	O
adding	O
more	O
sentences	O
,	O
but	O
the	O
performance	O
dropped	O
,	O
which	O
is	O
probably	O
because	O
the	O
latter	O
sentences	O
in	O
this	O
corpus	O
are	O
not	O
pertinent	O
to	O
the	O
summary	O
.	O

words	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
-	O
hieratt	O
:	O
Since	O
we	O
used	O
two	O
sentences	O
from	O
source	O
document	O
,	O
we	O
trained	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
proposed	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
this	O
model	O
improves	O
performance	O
compared	O
to	O
its	O
flatter	O
counterpart	O
by	O
learning	O
the	O
relative	O
importance	O
of	O
the	O
first	O
two	O
sentences	O
automatically	O
.	O

feats	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
:	O
Here	O
,	O
we	O
still	O
train	O
on	O
the	O
first	O
two	O
sentences	O
,	O
but	O
we	O
exploit	O
the	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
in	O
the	O
annotated	B-Material
gigaword	I-Material
corpus	E-Material
as	O
well	O
as	O
TF	O
,	O
IDF	O
values	O
,	O
to	O
augment	O
the	O
input	O
embeddings	O
on	O
the	O
source	O
side	O
as	O
described	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

In	O
total	O
,	O
our	O
embedding	B-Method
vector	E-Method
grew	O
from	O
the	O
original	O
100	O
to	O
155	O
,	O
and	O
produced	O
incremental	O
gains	O
compared	O
to	O
its	O
counterpart	O
words	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
demonstrating	O
the	O
utility	O
of	O
syntax	B-Method
based	I-Method
features	E-Method
in	O
this	O
task	O
.	O

feats	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
-	O
ptr	O
:	O
This	O
is	O
the	O
switching	B-Method
generator	E-Method
/	O
pointer	S-Method
model	O
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
but	O
in	O
addition	O
,	O
we	O
also	O
use	O
feature	O
-	O
rich	O
embeddings	O
on	O
the	O
document	O
side	O
as	O
in	O
the	O
above	O
model	O
.	O

Our	O
experiments	O
indicate	O
that	O
the	O
new	O
model	O
is	O
able	O
to	O
achieve	O
the	O
best	O
performance	O
on	O
our	O
test	O
set	O
by	O
all	O
three	O
Rouge	B-Metric
variants	E-Metric
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
:	O
We	O
compared	O
the	O
performance	O
of	O
our	O
model	B-Method
words	I-Method
-	I-Method
lvt2k	I-Method
-	I-Method
1sent	E-Method
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
sample	O
created	O
by	O
namas	O
,	O
as	O
displayed	O
in	O
the	O
bottom	O
part	O
of	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
trained	O
another	O
system	O
which	O
we	O
call	O
words	B-Method
-	I-Method
lvt5k	I-Method
-	I-Method
1sent	E-Method
which	O
has	O
a	O
larger	O
LVT	S-Method
vocabulary	O
size	O
of	O
5k	O
,	O
but	O
also	O
has	O
much	O
larger	O
source	O
and	O
target	O
vocabularies	O
of	O
400	O
K	O
and	O
200	O
K	O
respectively	O
.	O

The	O
reason	O
we	O
did	O
not	O
evaluate	O
our	O
best	O
validation	O
models	O
here	O
is	O
that	O
this	O
test	O
set	O
consisted	O
of	O
only	O
1	O
sentence	O
from	O
the	O
source	O
document	O
,	O
and	O
did	O
not	O
include	O
NLP	O
annotations	O
,	O
which	O
are	O
needed	O
in	O
our	O
best	O
models	O
.	O

The	O
table	O
shows	O
that	O
,	O
despite	O
this	O
fact	O
,	O
our	O
model	O
outperforms	O
the	O
ABS	B-Method
+	I-Method
model	E-Method
of	O
namas	S-Method
with	O
statistical	B-Metric
significance	E-Metric
.	O

In	O
addition	O
,	O
our	O
models	O
exhibit	O
better	O
abstractive	B-Metric
ability	E-Metric
as	O
shown	O
by	O
the	O
src	S-Method
.	O

copy	B-Metric
rate	I-Metric
metric	E-Metric
in	O
the	O
last	O
column	O
of	O
the	O
table	O
.	O

Further	O
,	O
our	O
larger	O
model	O
words	B-Method
-	I-Method
lvt5k	I-Method
-	I-Method
1sent	E-Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
of	O
with	O
statistically	O
significant	O
improvement	O
on	O
Rouge	B-Metric
-	I-Metric
1	E-Metric
.	O

We	O
believe	O
the	O
bidirectional	O
RNN	S-Method
we	O
used	O
to	O
model	O
the	O
source	O
captures	O
richer	O
contextual	O
information	O
of	O
every	O
word	O
than	O
the	O
bag	B-Method
-	I-Method
of	I-Method
-	I-Method
embeddings	I-Method
representation	E-Method
used	O
by	O
namas	O
and	O
chopra	O
in	O
their	O
convolutional	B-Method
attentional	I-Method
encoders	E-Method
,	O
which	O
might	O
explain	O
our	O
superior	O
performance	O
.	O

Further	O
,	O
explicit	O
modeling	O
of	O
important	O
information	O
such	O
as	O
multiple	O
source	O
sentences	O
,	O
word	O
-	O
level	O
linguistic	O
features	O
,	O
using	O
the	O
switch	B-Method
mechanism	E-Method
to	O
point	O
to	O
source	O
words	O
when	O
needed	O
,	O
and	O
hierarchical	O
attention	O
,	O
solve	O
specific	O
problems	O
in	O
summarization	S-Task
,	O
each	O
boosting	O
performance	O
incrementally	O
.	O

subsection	O
:	O
DUC	B-Material
Corpus	E-Material
The	O
DUC	B-Material
corpus	E-Material
comes	O
in	O
two	O
parts	O
:	O
the	O
2003	B-Material
corpus	E-Material
consisting	O
of	O
624	O
document	O
,	O
summary	O
pairs	O
and	O
the	O
2004	B-Material
corpus	E-Material
consisting	O
of	O
500	O
pairs	O
.	O

Since	O
these	O
corpora	O
are	O
too	O
small	O
to	O
train	O
large	O
neural	B-Method
networks	E-Method
on	O
,	O
namas	S-Method
trained	O
their	O
models	O
on	O
the	O
Gigaword	S-Material
corpus	O
,	O
but	O
combined	O
it	O
with	O
an	O
additional	O
log	B-Method
-	I-Method
linear	I-Method
extractive	I-Method
summarization	I-Method
model	E-Method
with	O
handcrafted	O
features	O
,	O
that	O
is	O
trained	O
on	O
the	O
DUC	B-Material
2003	I-Material
corpus	E-Material
.	O

They	O
call	O
the	O
original	O
neural	B-Method
attention	I-Method
model	E-Method
the	O
ABS	B-Method
model	E-Method
,	O
and	O
the	O
combined	B-Method
model	I-Method
ABS	I-Method
+	E-Method
.	O

chopra	O
also	O
report	O
the	O
performance	O
of	O
their	O
RAS	B-Method
-	I-Method
Elman	I-Method
model	E-Method
on	O
this	O
corpus	O
and	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
since	O
it	O
outperforms	O
all	O
previously	O
published	O
baselines	O
including	O
non	B-Method
-	I-Method
neural	I-Method
network	I-Method
based	I-Method
extractive	I-Method
and	I-Method
abstractive	I-Method
systems	E-Method
,	O
as	O
measured	O
by	O
the	O
official	B-Metric
DUC	I-Metric
metric	I-Metric
of	I-Metric
recall	E-Metric
at	O
75	O
bytes	O
.	O

In	O
these	O
experiments	O
,	O
we	O
use	O
the	O
same	O
metric	O
to	O
evaluate	O
our	O
models	O
too	O
,	O
but	O
we	O
omit	O
reporting	O
numbers	O
from	O
other	O
systems	O
in	O
the	O
interest	O
of	O
space	O
.	O

In	O
our	O
work	O
,	O
we	O
simply	O
run	O
the	O
models	O
trained	O
on	O
Gigaword	S-Material
corpus	O
as	O
they	O
are	O
,	O
without	O
tuning	O
them	O
on	O
the	O
DUC	B-Material
validation	I-Material
set	E-Material
.	O

The	O
only	O
change	O
we	O
made	O
to	O
the	O
decoder	S-Method
is	O
to	O
suppress	O
the	O
model	O
from	O
emitting	O
the	O
end	O
-	O
of	O
-	O
summary	O
tag	O
,	O
and	O
force	O
it	O
to	O
emit	O
exactly	O
30	O
words	O
for	O
every	O
summary	O
,	O
since	O
the	O
official	O
evaluation	O
on	O
this	O
corpus	O
is	O
based	O
on	O
limited	O
-	O
length	O
Rouge	S-Metric
recall	O
.	O

On	O
this	O
corpus	O
too	O
,	O
since	O
we	O
have	O
only	O
a	O
single	O
sentence	O
from	O
source	O
and	O
no	O
NLP	B-Material
annotations	E-Material
,	O
we	O
ran	O
just	O
the	O
models	B-Method
words	I-Method
-	I-Method
lvt2k	I-Method
-	I-Method
1sent	E-Method
and	O
words	B-Method
-	I-Method
lvt5k	I-Method
-	I-Method
1sent	E-Method
.	O

The	O
performance	O
of	O
this	O
model	O
on	O
the	O
test	O
set	O
is	O
compared	O
with	O
ABS	B-Method
and	I-Method
ABS	I-Method
+	I-Method
models	E-Method
,	O
RAS	S-Method
-	O
Elman	S-Method
from	O
,	O
as	O
well	O
as	O
TOPIARY	S-Method
,	O
the	O
top	O
performing	O
system	O
on	O
DUC	B-Material
-	I-Material
2004	E-Material
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
note	O
our	O
best	O
model	O
words	B-Method
-	I-Method
lvt5k	I-Method
-	I-Method
1sent	E-Method
outperforms	O
RAS	B-Method
-	I-Method
Elman	E-Method
on	O
two	O
of	O
the	O
three	O
variants	O
of	O
Rouge	S-Metric
,	O
while	O
being	O
competitive	O
on	O
Rouge	B-Metric
-	I-Metric
1	E-Metric
.	O

subsection	O
:	O
CNN	B-Material
/	I-Material
Daily	I-Material
Mail	I-Material
Corpus	E-Material
The	O
existing	O
abstractive	O
text	O
summarization	O
corpora	O
including	O
Gigaword	S-Material
and	O
DUC	S-Material
consist	O
of	O
only	O
one	O
sentence	O
in	O
each	O
summary	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
a	O
new	O
corpus	O
that	O
comprises	O
multi	B-Material
-	I-Material
sentence	I-Material
summaries	E-Material
.	O

To	O
produce	O
this	O
corpus	O
,	O
we	O
modify	O
an	O
existing	O
corpus	O
that	O
has	O
been	O
used	O
for	O
the	O
task	O
of	O
passage	B-Task
-	I-Task
based	I-Task
question	I-Task
answering	E-Task
.	O

In	O
this	O
work	O
,	O
the	O
authors	O
used	O
the	O
human	O
generated	O
abstractive	O
summary	O
bullets	O
from	O
new	O
-	O
stories	O
in	O
CNN	S-Material
and	O
Daily	B-Material
Mail	I-Material
websites	E-Material
as	O
questions	O
(	O
with	O
one	O
of	O
the	O
entities	O
hidden	O
)	O
,	O
and	O
stories	O
as	O
the	O
corresponding	O
passages	O
from	O
which	O
the	O
system	O
is	O
expected	O
to	O
answer	O
the	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
question	O
.	O

The	O
authors	O
released	O
the	O
scripts	O
that	O
crawl	O
,	O
extract	O
and	O
generate	O
pairs	O
of	O
passages	O
and	O
questions	O
from	O
these	O
websites	O
.	O

With	O
a	O
simple	O
modification	O
of	O
the	O
script	O
,	O
we	O
restored	O
all	O
the	O
summary	O
bullets	O
of	O
each	O
story	O
in	O
the	O
original	O
order	O
to	O
obtain	O
a	O
multi	O
-	O
sentence	O
summary	O
,	O
where	O
each	O
bullet	O
is	O
treated	O
as	O
a	O
sentence	O
.	O

In	O
all	O
,	O
this	O
corpus	O
has	O
286	O
,	O
817	O
training	O
pairs	O
,	O
13	O
,	O
368	O
validation	O
pairs	O
and	O
11	O
,	O
487	O
test	O
pairs	O
,	O
as	O
defined	O
by	O
their	O
scripts	O
.	O

The	O
source	O
documents	O
in	O
the	O
training	O
set	O
have	O
766	O
words	O
spanning	O
29.74	O
sentences	O
on	O
an	O
average	O
while	O
the	O
summaries	O
consist	O
of	O
53	O
words	O
and	O
3.72	O
sentences	O
.	O

The	O
unique	O
characteristics	O
of	O
this	O
dataset	O
such	O
as	O
long	O
documents	O
,	O
and	O
ordered	O
multi	O
-	O
sentence	O
summaries	O
present	O
interesting	O
challenges	O
,	O
and	O
we	O
hope	O
will	O
attract	O
future	O
researchers	O
to	O
build	O
and	O
test	O
novel	O
models	O
on	O
it	O
.	O

The	O
dataset	O
is	O
released	O
in	O
two	O
versions	O
:	O
one	O
consisting	O
of	O
actual	O
entity	O
names	O
,	O
and	O
the	O
other	O
,	O
in	O
which	O
entity	O
occurrences	O
are	O
replaced	O
with	O
document	O
-	O
specific	O
integer	O
-	O
ids	O
beginning	O
from	O
0	O
.	O

Since	O
the	O
vocabulary	B-Metric
size	E-Metric
is	O
smaller	O
in	O
the	O
anonymized	B-Method
version	E-Method
,	O
we	O
used	O
it	O
in	O
all	O
our	O
experiments	O
below	O
.	O

We	O
limited	O
the	O
source	O
vocabulary	O
size	O
to	O
150	O
K	O
,	O
and	O
the	O
target	O
vocabulary	O
to	O
60	O
K	O
,	O
the	O
source	O
and	O
target	O
lengths	O
to	O
at	O
most	O
800	O
and	O
100	O
words	O
respectively	O
.	O

We	O
used	O
100	B-Method
-	I-Method
dimensional	I-Method
word2vec	I-Method
embeddings	E-Method
trained	O
on	O
this	O
dataset	O
as	O
input	O
,	O
and	O
we	O
fixed	O
the	O
model	O
hidden	O
state	O
size	O
at	O
200	O
.	O

We	O
also	O
created	O
explicit	O
pointers	O
in	O
the	O
training	O
data	O
by	O
matching	O
only	O
the	O
anonymized	O
entity	O
-	O
ids	O
between	O
source	O
and	O
target	O
on	O
similar	O
lines	O
as	O
we	O
did	O
for	O
the	O
OOV	B-Material
words	E-Material
in	O
Gigaword	S-Material
corpus	O
.	O

Computational	B-Metric
costs	E-Metric
:	O
We	O
used	O
a	O
single	O
Tesla	B-Method
K	I-Method
-	I-Method
40	I-Method
GPU	E-Method
to	O
train	O
our	O
models	O
on	O
this	O
dataset	O
as	O
well	O
.	O

While	O
the	O
flat	B-Method
models	E-Method
(	O
words	B-Method
-	I-Method
lvt2k	I-Method
and	I-Method
words	I-Method
-	I-Method
lvt2k	I-Method
-	I-Method
ptr	E-Method
)	O
took	O
under	O
5	O
hours	O
per	O
epoch	O
,	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
was	O
very	O
expensive	O
,	O
consuming	O
nearly	O
12.5	O
hours	O
per	O
epoch	O
.	O

Convergence	O
of	O
all	O
models	O
is	O
also	O
slower	O
on	O
this	O
dataset	O
compared	O
to	O
Gigaword	S-Material
,	O
taking	O
nearly	O
35	O
epochs	O
for	O
all	O
models	O
.	O

Thus	O
,	O
the	O
wall	B-Metric
-	I-Metric
clock	I-Metric
time	E-Metric
for	O
training	O
until	O
convergence	S-Task
is	O
about	O
7	O
days	O
for	O
the	O
flat	B-Method
models	E-Method
,	O
but	O
nearly	O
18	O
days	O
for	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
.	O

Decoding	S-Method
is	O
also	O
slower	O
as	O
well	O
,	O
with	O
a	O
throughput	O
of	O
2	O
examples	O
per	O
second	O
for	O
flat	B-Method
models	E-Method
and	O
1.5	O
examples	O
per	O
second	O
for	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
,	O
when	O
run	O
on	O
a	O
single	O
GPU	S-Method
with	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	S-Task
:	O
We	O
evaluated	O
our	O
models	O
using	O
the	O
full	O
-	O
length	O
Rouge	S-Metric
F1	O
metric	O
that	O
we	O
employed	O
for	O
the	O
Gigaword	S-Material
corpus	O
,	O
but	O
with	O
one	O
notable	O
difference	O
:	O
in	O
both	O
system	O
and	O
gold	O
summaries	O
,	O
we	O
considered	O
each	O
highlight	O
to	O
be	O
a	O
separate	O
sentence	O
.	O

Results	O
:	O
Results	O
from	O
the	O
basic	O
attention	B-Method
encoder	I-Method
-	I-Method
decoder	E-Method
as	O
well	O
as	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
are	O
displayed	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Although	O
this	O
dataset	O
is	O
smaller	O
and	O
more	O
complex	O
than	O
the	O
Gigaword	S-Material
corpus	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
Rouge	S-Metric
numbers	O
are	O
in	O
the	O
same	O
range	O
.	O

However	O
,	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
outperforms	O
the	O
baseline	O
attentional	B-Method
decoder	E-Method
only	O
marginally	O
.	O

Upon	O
visual	O
inspection	O
of	O
the	O
system	O
output	O
,	O
we	O
noticed	O
that	O
on	O
this	O
dataset	O
,	O
both	O
these	O
models	O
produced	O
summaries	O
that	O
contain	O
repetitive	O
phrases	O
or	O
even	O
repetitive	O
sentences	O
at	O
times	O
.	O

Since	O
the	O
summaries	O
in	O
this	O
dataset	O
involve	O
multiple	O
sentences	O
,	O
it	O
is	O
likely	O
that	O
the	O
decoder	S-Method
‘	O
forgets	O
’	O
what	O
part	O
of	O
the	O
document	O
was	O
used	O
in	O
producing	O
earlier	O
highlights	O
.	O

To	O
overcome	O
this	O
problem	O
,	O
we	O
used	O
the	O
Temporal	B-Method
Attention	I-Method
model	E-Method
of	O
baskaran	S-Method
that	O
keeps	O
track	O
of	O
past	O
attentional	O
weights	O
of	O
the	O
decoder	O
and	O
expliticly	O
discourages	O
it	O
from	O
attending	O
to	O
the	O
same	O
parts	O
of	O
the	O
document	O
in	O
future	O
time	O
steps	O
.	O

The	O
model	O
works	O
as	O
shown	O
by	O
the	O
following	O
simple	O
equations	O
:	O
where	O
is	O
the	O
unnormalized	O
attention	O
-	O
weights	O
vector	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	S-Method
.	O

In	O
other	O
words	O
,	O
the	O
temporal	B-Method
attention	I-Method
model	E-Method
down	O
-	O
weights	O
the	O
attention	O
weights	O
at	O
the	O
current	O
time	O
step	O
if	O
the	O
past	O
attention	O
weights	O
are	O
high	O
on	O
the	O
same	O
part	O
of	O
the	O
document	O
.	O

Using	O
this	O
strategy	O
,	O
the	O
temporal	B-Method
attention	I-Method
model	E-Method
improves	O
performance	O
significantly	O
over	O
both	O
the	O
baseline	B-Method
model	E-Method
as	O
well	O
as	O
the	O
hierarchical	B-Method
attention	I-Method
model	E-Method
.	O

We	O
have	O
also	O
noticed	O
that	O
there	O
are	O
fewer	O
repetitions	O
of	O
summay	O
highlights	O
produced	O
by	O
this	O
model	O
as	O
shown	O
in	O
the	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O

These	O
results	O
,	O
although	O
preliminary	O
,	O
should	O
serve	O
as	O
a	O
good	O
baseline	O
for	O
future	O
researchers	O
to	O
compare	O
their	O
models	O
against	O
.	O

section	O
:	O
Qualitative	B-Task
Analysis	E-Task
Table	O
[	O
reference	O
]	O
presents	O
a	O
few	O
high	O
quality	O
and	O
poor	O
quality	O
output	O
on	O
the	O
validation	O
set	O
from	O
feats	B-Method
-	I-Method
lvt2k	I-Method
-	I-Method
2sent	E-Method
,	O
one	O
of	O
our	O
best	O
performing	O
models	O
.	O

Even	O
when	O
the	O
model	O
differs	O
from	O
the	O
target	O
summary	O
,	O
its	O
summaries	O
tend	O
to	O
be	O
very	O
meaningful	O
and	O
relevant	O
,	O
a	O
phenomenon	O
not	O
captured	O
by	O
word	B-Metric
/	I-Metric
phrase	I-Metric
matching	I-Metric
evaluation	I-Metric
metrics	E-Metric
such	O
as	O
Rouge	S-Metric
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
model	O
sometimes	O
‘	O
misinterprets	O
’	O
the	O
semantics	O
of	O
the	O
text	O
and	O
generates	O
a	O
summary	O
with	O
a	O
comical	O
interpretation	O
as	O
shown	O
in	O
the	O
poor	O
quality	O
examples	O
in	O
the	O
table	O
.	O

Clearly	O
,	O
capturing	O
the	O
‘	O
meaning	O
’	O
of	O
complex	O
sentences	O
remains	O
a	O
weakness	O
of	O
these	O
models	O
.	O

Our	O
next	O
example	O
output	O
,	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
displays	O
the	O
sample	O
output	O
from	O
the	O
switching	B-Method
generator	E-Method
/	O
pointer	S-Method
model	O
on	O
the	O
Gigaword	S-Material
corpus	O
.	O

It	O
is	O
apparent	O
from	O
the	O
examples	O
that	O
the	O
model	O
learns	O
to	O
use	O
pointers	O
very	O
accurately	O
not	O
only	O
for	O
named	O
entities	O
,	O
but	O
also	O
for	O
multi	B-Task
-	I-Task
word	I-Task
phrases	E-Task
.	O

Despite	O
its	O
accuracy	S-Metric
,	O
the	O
performance	O
improvement	O
of	O
the	O
overall	O
model	O
is	O
not	O
significant	O
.	O

We	O
believe	O
the	O
impact	O
of	O
this	O
model	O
may	O
be	O
more	O
pronounced	O
in	O
other	O
settings	O
with	O
a	O
heavier	O
tail	O
distribution	O
of	O
rare	O
words	O
.	O

We	O
intend	O
to	O
carry	O
out	O
more	O
experiments	O
with	O
this	O
model	O
in	O
the	O
future	O
.	O

On	O
CNN	B-Material
/	I-Material
Daily	I-Material
Mail	I-Material
data	E-Material
,	O
although	O
our	O
models	O
are	O
able	O
to	O
produce	O
good	O
quality	O
multi	O
-	O
sentence	O
summaries	O
,	O
we	O
notice	O
that	O
the	O
same	O
sentence	O
or	O
phrase	O
often	O
gets	O
repeated	O
in	O
the	O
summary	O
.	O

We	O
believe	O
models	O
that	O
incorporate	O
intra	B-Method
-	I-Method
attention	E-Method
such	O
as	O
lstmn	S-Method
can	O
fix	O
this	O
problem	O
by	O
encouraging	O
the	O
model	O
to	O
‘	O
remember	O
’	O
the	O
words	O
it	O
has	O
already	O
produced	O
in	O
the	O
past	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
apply	O
the	O
attentional	B-Method
encoder	I-Method
-	I-Method
decoder	E-Method
for	O
the	O
task	O
of	O
abstractive	B-Task
summarization	E-Task
with	O
very	O
promising	O
results	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
significantly	O
on	O
two	O
different	O
datasets	O
.	O

Each	O
of	O
our	O
proposed	O
novel	O
models	O
addresses	O
a	O
specific	O
problem	O
in	O
abstractive	B-Task
summarization	E-Task
,	O
yielding	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
for	O
multi	B-Task
-	I-Task
sentence	I-Task
summarization	E-Task
and	O
establish	O
benchmark	O
numbers	O
on	O
it	O
.	O

As	O
part	O
of	O
our	O
future	O
work	O
,	O
we	O
plan	O
to	O
focus	O
our	O
efforts	O
on	O
this	O
data	O
and	O
build	O
more	O
robust	O
models	O
for	O
summaries	S-Task
consisting	O
of	O
multiple	O
sentences	O
.	O

bibliography	O
:	O
References	O
document	O
:	O
Training	O
with	O
Exploration	S-Task
Improves	O
a	O
Greedy	B-Method
Stack	I-Method
LSTM	I-Method
Parser	E-Method
We	O
adapt	O
the	O
greedy	B-Method
stack	I-Method
LSTM	I-Method
dependency	I-Method
parser	E-Method
of	O
lstmacl15	O
to	O
support	O
a	O
training	B-Task
-	I-Task
with	I-Task
-	I-Task
exploration	I-Task
procedure	E-Task
using	O
dynamic	O
oracles	O
instead	O
of	O
assuming	O
an	O
error	O
-	O
free	O
action	O
history	O
.	O

This	O
form	O
of	O
training	O
,	O
which	O
accounts	O
for	O
model	B-Task
predictions	E-Task
at	O
training	O
time	O
,	O
improves	O
parsing	B-Metric
accuracies	E-Metric
.	O

We	O
discuss	O
some	O
modifications	O
needed	O
in	O
order	O
to	O
get	O
training	O
with	O
exploration	S-Task
to	O
work	O
well	O
for	O
a	O
probabilistic	B-Method
neural	I-Method
network	I-Method
dependency	I-Method
parser	E-Method
.	O

section	O
:	O
Introduction	O
Natural	O
language	O
parsing	S-Task
can	O
be	O
formulated	O
as	O
a	O
series	O
of	O
decisions	O
that	O
read	O
words	O
in	O
sequence	O
and	O
incrementally	O
combine	O
them	O
to	O
form	O
syntactic	O
structures	O
;	O
this	O
formalization	O
is	O
known	O
as	O
transition	O
-	O
based	O
parsing	S-Task
,	O
and	O
is	O
often	O
coupled	O
with	O
a	O
greedy	B-Method
search	I-Method
procedure	E-Method
.	O

The	O
literature	O
on	O
transition	O
-	O
based	O
parsing	S-Task
is	O
vast	O
,	O
but	O
all	O
works	O
share	O
in	O
common	O
a	O
classification	B-Method
component	E-Method
that	O
takes	O
into	O
account	O
features	O
of	O
the	O
current	O
parser	O
state	O
and	O
predicts	O
the	O
next	O
action	O
to	O
take	O
conditioned	O
on	O
the	O
state	O
.	O

The	O
state	O
is	O
of	O
unbounded	O
size	O
.	O

Dyer	O
et	O
al	O
.	O

lstmacl15	S-Method
presented	O
a	O
parser	S-Method
in	O
which	O
the	O
parser	O
’s	O
unbounded	O
state	O
is	O
embedded	O
in	O
a	O
fixed	O
-	O
dimensional	O
continuous	O
space	O
using	O
recurrent	B-Method
neural	I-Method
networks	E-Method
.	O

Coupled	O
with	O
a	O
recursive	B-Method
tree	I-Method
composition	I-Method
function	E-Method
,	O
the	O
feature	B-Method
representation	E-Method
is	O
able	O
to	O
capture	O
information	O
from	O
the	O
entirety	O
of	O
the	O
state	O
,	O
without	O
resorting	O
to	O
locality	O
assumptions	O
that	O
were	O
common	O
in	O
most	O
other	O
transition	B-Method
-	I-Method
based	I-Method
parsers	E-Method
.	O

The	O
use	O
of	O
a	O
novel	O
stack	B-Method
LSTM	I-Method
data	I-Method
structure	E-Method
allows	O
the	O
parser	S-Method
to	O
maintain	O
a	O
constant	O
time	O
per	O
-	O
state	O
update	O
,	O
and	O
retain	O
an	O
overall	O
linear	O
parsing	S-Task
time	O
.	O

The	O
Dyer	O
et	O
al	O
.	O

parser	S-Method
was	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
gold	O
-	O
standard	O
transition	O
sequences	O
,	O
given	O
words	O
.	O

At	O
test	O
time	O
,	O
the	O
parser	S-Method
makes	O
greedy	O
decisions	O
according	O
to	O
the	O
learned	O
model	O
.	O

Although	O
this	O
setup	O
obtains	O
very	O
good	O
performance	O
,	O
the	O
training	O
and	O
testing	O
conditions	O
are	O
mismatched	O
in	O
the	O
following	O
way	O
:	O
at	O
training	O
time	O
the	O
historical	O
context	O
of	O
an	O
action	O
is	O
always	O
derived	O
from	O
the	O
gold	O
standard	O
(	O
i.e.	O
,	O
perfectly	O
correct	O
past	O
actions	O
)	O
,	O
but	O
at	O
test	O
time	O
,	O
it	O
will	O
be	O
a	O
model	B-Method
prediction	E-Method
.	O

In	O
this	O
work	O
,	O
we	O
adapt	O
the	O
training	O
criterion	O
so	O
as	O
to	O
explore	O
parser	O
states	O
drawn	O
not	O
only	O
from	O
the	O
training	O
data	O
,	O
but	O
also	O
from	O
the	O
model	O
as	O
it	O
is	O
being	O
learned	O
.	O

To	O
do	O
so	O
,	O
we	O
use	O
the	O
method	O
of	O
Goldberg	O
and	O
Nivre	O
goldberg12dynamic	O
,	O
goldberg2013training	O
to	O
dynamically	O
chose	O
an	O
optimal	O
(	O
relative	O
to	O
the	O
final	O
attachment	B-Metric
accuracy	E-Metric
)	O
action	O
given	O
an	O
imperfect	O
history	O
.	O

By	O
interpolating	O
between	O
algorithm	O
states	O
sampled	O
from	O
the	O
model	O
and	O
those	O
sampled	O
from	O
the	O
training	O
data	O
,	O
more	O
robust	O
predictions	O
at	O
test	O
time	O
can	O
be	O
made	O
.	O

We	O
show	O
that	O
the	O
technique	O
can	O
be	O
used	O
to	O
improve	O
the	O
strong	O
parser	O
of	O
Dyer	O
et	O
al	O
.	O

section	O
:	O
Parsing	B-Method
Model	E-Method
and	O
Parameter	B-Method
Learning	E-Method
Our	O
departure	O
point	O
is	O
the	O
parsing	S-Task
model	O
described	O
by	O
lstmacl15	O
.	O

We	O
do	O
not	O
describe	O
the	O
model	O
in	O
detail	O
,	O
and	O
refer	O
the	O
reader	O
to	O
the	O
original	O
work	O
.	O

At	O
each	O
stage	O
of	O
the	O
parsing	S-Task
process	O
,	O
the	O
parser	O
state	O
is	O
encoded	O
into	O
a	O
vector	O
,	O
which	O
is	O
used	O
to	O
compute	O
the	O
probability	O
of	O
the	O
parser	O
action	O
at	O
time	O
as	O
:	O
where	O
is	O
a	O
column	O
vector	O
representing	O
the	O
(	O
output	O
)	O
embedding	O
of	O
the	O
parser	O
action	O
,	O
and	O
is	O
a	O
bias	O
term	O
for	O
action	O
.	O

The	O
set	O
represents	O
the	O
valid	O
transition	O
actions	O
that	O
may	O
be	O
taken	O
in	O
the	O
current	O
state	O
.	O

Since	O
encodes	O
information	O
about	O
all	O
previous	O
decisions	O
made	O
by	O
the	O
parser	S-Method
,	O
the	O
chain	B-Method
rule	E-Method
gives	O
the	O
probability	O
of	O
any	O
valid	O
sequence	O
of	O
parse	O
transitions	O
conditional	O
on	O
the	O
input	O
:	O
The	O
parser	S-Method
is	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
taking	O
a	O
“	O
correct	O
”	O
action	O
at	O
each	O
parsing	S-Task
state	O
.	O

The	O
definition	O
of	O
what	O
constitutes	O
a	O
“	O
correct	O
”	O
action	O
is	O
the	O
major	O
difference	O
between	O
a	O
static	O
oracle	O
as	O
used	O
by	O
lstmacl15	O
and	O
the	O
dynamic	B-Method
oracle	E-Method
explored	O
here	O
.	O

Regardless	O
of	O
the	O
oracle	O
,	O
our	O
training	B-Method
implementation	E-Method
constructs	O
a	O
computation	O
graph	O
(	O
nodes	O
that	O
represent	O
values	O
,	O
linked	O
by	O
directed	O
edges	O
from	O
each	O
function	O
’s	O
inputs	O
to	O
its	O
outputs	O
)	O
for	O
the	O
negative	O
log	O
probability	O
for	O
the	O
oracle	O
transition	O
sequence	O
as	O
a	O
function	O
of	O
the	O
current	O
model	O
parameters	O
and	O
uses	O
forward	B-Method
-	E-Method
and	O
backpropagation	S-Method
to	O
obtain	O
the	O
gradients	O
respect	O
to	O
the	O
model	O
parameters	O
.	O

subsection	O
:	O
Training	O
with	O
Static	O
Oracles	O
With	O
a	O
static	O
oracle	O
,	O
the	O
training	B-Method
procedure	E-Method
computes	O
a	O
canonical	O
reference	O
series	O
of	O
transitions	O
for	O
each	O
gold	O
parse	O
tree	O
.	O

It	O
then	O
runs	O
the	O
parser	S-Method
through	O
this	O
canonical	O
sequence	O
of	O
transitions	O
,	O
while	O
keeping	O
track	O
of	O
the	O
state	B-Method
representation	E-Method
at	O
each	O
step	O
,	O
as	O
well	O
as	O
the	O
distribution	O
over	O
transitions	O
which	O
is	O
predicted	O
by	O
the	O
current	O
classifier	S-Method
for	O
the	O
state	B-Method
representation	E-Method
.	O

Once	O
the	O
end	O
of	O
the	O
sentence	O
is	O
reached	O
,	O
the	O
parameters	O
are	O
updated	O
towards	O
maximizing	O
the	O
likelihood	O
of	O
the	O
reference	O
transition	O
sequence	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
which	O
equates	O
to	O
maximizing	O
the	O
probability	O
of	O
the	O
correct	O
transition	O
,	O
,	O
at	O
each	O
state	O
along	O
the	O
path	O
.	O

subsection	O
:	O
Training	O
with	O
Dynamic	B-Method
Oracles	E-Method
In	O
the	O
static	B-Task
oracle	I-Task
case	E-Task
,	O
the	O
parser	S-Method
is	O
trained	O
to	O
predict	O
the	O
best	O
transition	O
to	O
take	O
at	O
each	O
parsing	S-Task
step	O
,	O
assuming	O
all	O
previous	O
transitions	O
were	O
correct	O
.	O

Since	O
the	O
parser	S-Method
is	O
likely	O
to	O
make	O
mistakes	O
at	O
test	O
time	O
and	O
encounter	O
states	O
it	O
has	O
not	O
seen	O
during	O
training	O
,	O
this	O
training	O
criterion	O
is	O
problematic	O
.	O

Instead	O
,	O
we	O
would	O
prefer	O
to	O
train	O
the	O
parser	S-Method
to	O
behave	O
optimally	O
even	O
after	O
making	O
a	O
mistake	O
(	O
under	O
the	O
constraint	O
that	O
it	O
can	O
not	O
backtrack	O
or	O
fix	O
any	O
previous	O
decision	O
)	O
.	O

We	O
thus	O
need	O
to	O
include	O
in	O
the	O
training	O
examples	O
states	O
that	O
result	O
from	O
wrong	O
parsing	S-Task
decisions	O
,	O
together	O
with	O
the	O
optimal	O
transitions	O
to	O
take	O
in	O
these	O
states	O
.	O

To	O
this	O
end	O
we	O
reconsider	O
which	O
training	O
examples	O
to	O
show	O
,	O
and	O
what	O
it	O
means	O
to	O
behave	O
optimally	O
on	O
these	O
training	O
examples	O
.	O

The	O
framework	O
of	O
training	O
with	O
exploration	S-Task
using	O
dynamic	O
oracles	O
suggested	O
by	O
Goldberg	O
and	O
Nivre	O
goldberg12dynamic	O
,	O
goldberg2013training	O
provides	O
answers	O
to	O
these	O
questions	O
.	O

While	O
the	O
application	O
of	O
dynamic	B-Method
oracle	I-Method
training	E-Method
is	O
relatively	O
straightforward	O
,	O
some	O
adaptations	O
were	O
needed	O
to	O
accommodate	O
the	O
probabilistic	B-Task
training	I-Task
objective	E-Task
.	O

These	O
adaptations	O
mostly	O
follow	O
Goldberg	O
goldberg2013calibrated	O
.	O

paragraph	O
:	O
Dynamic	O
Oracles	O
.	O

A	O
dynamic	B-Method
oracle	E-Method
is	O
the	O
component	O
that	O
,	O
given	O
a	O
gold	O
parse	O
tree	O
,	O
provides	O
the	O
optimal	O
set	O
of	O
possible	O
actions	O
to	O
take	O
for	O
any	O
valid	O
parser	O
state	O
.	O

In	O
contrast	O
to	O
static	B-Method
oracles	E-Method
that	O
derive	O
a	O
canonical	O
state	O
sequence	O
for	O
each	O
gold	O
parse	O
tree	O
and	O
say	O
nothing	O
about	O
states	O
that	O
deviate	O
from	O
this	O
canonical	O
path	O
,	O
the	O
dynamic	B-Method
oracle	E-Method
is	O
well	O
defined	O
for	O
states	O
that	O
result	O
from	O
parsing	S-Task
mistakes	O
,	O
and	O
they	O
may	O
produce	O
more	O
than	O
a	O
single	O
gold	O
action	O
for	O
a	O
given	O
state	O
.	O

Under	O
the	O
dynamic	B-Method
oracle	I-Method
framework	E-Method
,	O
an	O
action	O
is	O
said	O
to	O
be	O
optimal	O
for	O
a	O
state	O
if	O
the	O
best	O
tree	O
that	O
can	O
be	O
reached	O
after	O
taking	O
the	O
action	O
is	O
no	O
worse	O
(	O
in	O
terms	O
of	O
accuracy	S-Metric
with	O
respect	O
to	O
the	O
gold	O
tree	O
)	O
than	O
the	O
best	O
tree	O
that	O
could	O
be	O
reached	O
prior	O
to	O
taking	O
that	O
action	O
.	O

Goldberg	O
and	O
Nivre	O
goldberg2013training	O
define	O
the	O
arc	B-Method
-	I-Method
decomposition	I-Method
property	I-Method
of	I-Method
transition	I-Method
systems	E-Method
,	O
and	O
show	O
how	O
to	O
derive	O
efficient	O
dynamic	B-Method
oracles	E-Method
for	O
transition	B-Method
systems	E-Method
that	O
are	O
arc	O
-	O
decomposable	O
.	O

Unfortunately	O
,	O
the	O
arc	B-Method
-	I-Method
standard	I-Method
transition	I-Method
system	E-Method
does	O
not	O
have	O
this	O
property	O
.	O

While	O
it	O
is	O
possible	O
to	O
compute	O
dynamic	O
oracles	O
for	O
the	O
arc	B-Method
-	I-Method
standard	I-Method
system	E-Method
,	O
the	O
computation	O
relies	O
on	O
a	O
dynamic	B-Method
programming	I-Method
algorithm	E-Method
which	O
is	O
polynomial	O
in	O
the	O
length	O
of	O
the	O
stack	O
.	O

As	O
the	O
dynamic	B-Method
oracle	E-Method
has	O
to	O
be	O
queried	O
for	O
each	O
parser	O
state	O
seen	O
during	O
training	O
,	O
the	O
use	O
of	O
this	O
dynamic	B-Method
oracle	E-Method
will	O
make	O
the	O
training	O
runtime	O
several	O
times	O
longer	O
.	O

We	O
chose	O
instead	O
to	O
switch	O
to	O
the	O
arc	B-Method
-	I-Method
hybrid	E-Method
transition	O
system	O
,	O
which	O
is	O
very	O
similar	O
to	O
the	O
arc	B-Method
-	I-Method
standard	I-Method
system	E-Method
but	O
is	O
arc	O
-	O
decomposable	O
and	O
hence	O
admits	O
an	O
efficient	O
dynamic	B-Method
oracle	E-Method
,	O
resulting	O
in	O
only	O
negligible	O
increase	O
to	O
training	B-Metric
runtime	E-Metric
.	O

We	O
implemented	O
the	O
dynamic	B-Method
oracle	E-Method
to	O
the	O
arc	B-Method
-	I-Method
hybrid	E-Method
system	O
as	O
described	O
by	O
Goldberg	O
goldberg2013training	O
.	O

paragraph	O
:	O
Training	O
with	O
Exploration	O
.	O

In	O
order	O
to	O
expose	O
the	O
parser	S-Method
to	O
configurations	O
that	O
are	O
likely	O
to	O
result	O
from	O
incorrect	O
parsing	S-Task
decisions	O
,	O
we	O
make	O
use	O
of	O
the	O
probabilistic	O
nature	O
of	O
the	O
classifier	S-Method
.	O

During	O
training	O
,	O
instead	O
of	O
following	O
the	O
gold	O
action	O
,	O
we	O
sample	O
the	O
next	O
transition	O
according	O
to	O
the	O
output	O
distribution	O
the	O
classifier	S-Method
assigns	O
to	O
the	O
current	O
configuration	O
.	O

Another	O
option	O
,	O
taken	O
by	O
Goldberg	O
and	O
Nivre	O
,	O
is	O
to	O
follow	O
the	O
one	O
-	O
best	O
action	O
predicted	O
by	O
the	O
classifier	S-Method
.	O

However	O
,	O
initial	O
experiments	O
showed	O
that	O
the	O
one	O
-	O
best	O
approach	O
did	O
not	O
work	O
well	O
.	O

Because	O
the	O
neural	B-Method
network	I-Method
classifier	E-Method
becomes	O
accurate	O
early	O
on	O
in	O
the	O
training	O
process	O
,	O
the	O
one	O
-	O
best	O
action	O
is	O
likely	O
to	O
be	O
correct	O
,	O
and	O
the	O
parser	S-Method
is	O
then	O
exposed	O
to	O
very	O
few	O
error	O
states	O
in	O
its	O
training	O
process	O
.	O

By	O
sampling	O
from	O
the	O
predicted	O
distribution	O
,	O
we	O
are	O
effectively	O
increasing	O
the	O
chance	O
of	O
straying	O
from	O
the	O
gold	O
path	O
during	O
training	O
,	O
while	O
still	O
focusing	O
on	O
mistakes	O
that	O
receive	O
relatively	O
high	O
parser	B-Metric
scores	E-Metric
.	O

We	O
believe	O
further	O
formal	O
analysis	O
of	O
this	O
method	O
will	O
reveal	O
connections	O
to	O
reinforcement	B-Task
learning	E-Task
and	O
,	O
perhaps	O
,	O
other	O
methods	O
for	O
learning	B-Task
complex	I-Task
policies	E-Task
.	O

Taking	O
this	O
idea	O
further	O
,	O
we	O
could	O
increase	O
the	O
number	O
of	O
error	O
-	O
states	O
observed	O
in	O
the	O
training	B-Method
process	E-Method
by	O
changing	O
the	O
sampling	O
distribution	O
so	O
as	O
to	O
bias	O
it	O
toward	O
more	O
low	O
-	O
probability	O
states	O
.	O

We	O
do	O
this	O
by	O
raising	O
each	O
probability	O
to	O
the	O
power	O
of	O
(	O
)	O
and	O
re	O
-	O
normalizing	O
.	O

This	O
transformation	O
keeps	O
the	O
relative	O
ordering	O
of	O
the	O
events	O
,	O
while	O
shifting	O
probability	O
mass	O
towards	O
less	O
frequent	O
events	O
.	O

As	O
we	O
show	O
below	O
,	O
this	O
turns	O
out	O
to	O
be	O
very	O
beneficial	O
for	O
the	O
configurations	O
that	O
make	O
use	O
of	O
external	O
embeddings	O
.	O

Indeed	O
,	O
these	O
configurations	O
achieve	O
high	O
accuracies	S-Metric
and	O
sharp	O
class	O
distributions	O
early	O
on	O
in	O
the	O
training	O
process	O
.	O

The	O
parser	S-Method
is	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
a	O
correct	O
action	O
at	O
each	O
parsing	S-Task
state	O
according	O
to	O
Equation	O
[	O
reference	O
]	O
.	O

When	O
using	O
the	O
dynamic	O
oracle	O
,	O
a	O
state	O
may	O
admit	O
multiple	O
correct	O
actions	O
.	O

Our	O
objective	O
in	O
such	O
cases	O
is	O
the	O
marginal	O
likelihood	O
of	O
all	O
correct	O
actions	O
,	O
section	O
:	O
Experiments	O
Following	O
the	O
same	O
settings	O
of	O
Chen	O
and	O
Manning	O
chen:2014	O
and	O
Dyer	O
et	O
al	O
lstmacl15	O
we	O
report	O
results	O
in	O
the	O
English	B-Material
PTB	E-Material
and	O
Chinese	B-Material
CTB	I-Material
-	I-Material
5	E-Material
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
parser	S-Method
in	O
its	O
different	O
configurations	O
.	O

The	O
table	O
also	O
shows	O
the	O
best	O
result	O
obtained	O
with	O
the	O
static	O
oracle	O
(	O
obtained	O
by	O
rerunning	O
Dyer	O
et	O
al	O
.	O

parser	S-Method
)	O
for	O
the	O
sake	O
of	O
comparison	O
between	O
static	B-Method
and	I-Method
dynamic	I-Method
training	I-Method
strategies	E-Method
.	O

The	O
score	O
achieved	O
by	O
the	O
dynamic	B-Method
oracle	E-Method
for	O
English	S-Material
is	O
93.56	O
UAS	S-Metric
.	O

This	O
is	O
remarkable	O
given	O
that	O
the	O
parser	S-Method
uses	O
a	O
completely	O
greedy	B-Method
search	I-Method
procedure	E-Method
.	O

Moreover	O
,	O
the	O
Chinese	S-Material
score	O
establishes	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
using	O
the	O
same	O
settings	O
as	O
chen:2014	O
.	O

The	O
error	B-Method
-	I-Method
exploring	I-Method
dynamic	I-Method
-	I-Method
oracle	I-Method
training	E-Method
always	O
improves	O
over	O
static	B-Method
oracle	I-Method
training	E-Method
controlling	O
for	O
the	O
transition	B-Task
system	E-Task
,	O
but	O
the	O
arc	B-Method
-	I-Method
hybrid	E-Method
system	O
slightly	O
under	O
-	O
performs	O
the	O
arc	B-Method
-	I-Method
standard	I-Method
system	E-Method
when	O
trained	O
with	O
static	O
oracle	O
.	O

Flattening	O
the	O
sampling	O
distribution	O
(	O
)	O
is	O
especially	O
beneficial	O
when	O
training	O
with	O
pretrained	B-Task
word	I-Task
embeddings	E-Task
.	O

In	O
order	O
to	O
be	O
able	O
to	O
compare	O
with	O
similar	O
greedy	B-Method
parsers	E-Method
we	O
report	O
the	O
performance	O
of	O
the	O
parser	S-Method
on	O
the	O
multilingual	O
treebanks	O
of	O
the	O
CoNLL	B-Material
2009	I-Material
shared	I-Material
task	E-Material
.	O

Since	O
some	O
of	O
the	O
treebanks	O
contain	O
nonprojective	O
sentences	O
and	O
arc	B-Method
-	I-Method
hybrid	E-Method
does	O
not	O
allow	O
nonprojective	O
trees	O
,	O
we	O
use	O
the	O
pseudo	B-Method
-	I-Method
projective	I-Method
approach	E-Method
.	O

We	O
used	O
predicted	O
part	O
-	O
of	O
-	O
speech	O
tags	O
provided	O
by	O
the	O
CoNLL	B-Task
2009	I-Task
shared	I-Task
task	I-Task
organizers	E-Task
.	O

We	O
also	O
include	O
results	O
with	O
pretrained	O
word	O
embeddings	O
for	O
English	S-Material
,	O
Chinese	S-Material
,	O
German	S-Material
,	O
and	O
Spanish	S-Material
following	O
the	O
same	O
training	O
setup	O
as	O
Dyer	O
et	O
al	O
.	O

(	O
2015	O
)	O
;	O
for	O
English	S-Material
and	O
Chinese	S-Material
we	O
used	O
the	O
same	O
pretrained	O
word	O
embeddings	O
as	O
in	O
Table	O
[	O
reference	O
]	O
,	O
for	O
German	S-Material
we	O
used	O
the	O
monolingual	O
training	O
data	O
from	O
the	O
WMT	B-Material
2015	I-Material
dataset	E-Material
and	O
for	O
Spanish	S-Material
we	O
used	O
the	O
Spanish	B-Material
Gigaword	E-Material
version	O
3	O
.	O

See	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
Training	O
greedy	B-Method
parsers	E-Method
on	O
non	O
-	O
gold	O
outcomes	O
,	O
facilitated	O
by	O
dynamic	B-Method
oracles	E-Method
,	O
has	O
been	O
explored	O
by	O
several	O
researchers	O
in	O
different	O
ways	O
.	O

More	O
generally	O
,	O
training	O
greedy	B-Method
search	I-Method
systems	E-Method
by	O
paying	O
attention	O
to	O
the	O
expected	O
classifier	O
behavior	O
during	O
test	O
time	O
has	O
been	O
explored	O
under	O
the	O
imitation	B-Method
learning	E-Method
and	O
learning	B-Method
-	I-Method
to	I-Method
-	I-Method
search	I-Method
frameworks	E-Method
.	O

Directly	O
modeling	O
the	O
probability	O
of	O
making	O
a	O
mistake	O
has	O
also	O
been	O
explored	O
for	O
parsing	S-Task
.	O

Generally	O
,	O
the	O
use	O
of	O
RNNs	S-Method
to	O
conditionally	B-Task
predict	I-Task
actions	E-Task
in	O
sequence	O
given	O
a	O
history	O
is	O
spurring	O
increased	O
interest	O
in	O
training	O
regimens	O
that	O
make	O
the	O
learned	O
model	O
more	O
robust	O
to	O
test	O
-	O
time	O
prediction	O
errors	O
.	O

Solutions	O
based	O
on	O
curriculum	B-Method
learning	E-Method
,	O
expected	B-Method
loss	I-Method
training	E-Method
,	O
and	O
reinforcement	B-Method
learning	E-Method
have	O
been	O
proposed	O
.	O

Finally	O
,	O
abandoning	O
greedy	B-Method
search	E-Method
in	O
favor	O
of	O
approximate	B-Method
global	I-Method
search	E-Method
offers	O
an	O
alternative	O
solution	O
to	O
the	O
problems	O
with	O
greedy	B-Method
search	E-Method
,	O
and	O
has	O
been	O
analyzed	O
as	O
well	O
,	O
including	O
for	O
parsing	S-Task
.	O

section	O
:	O
Conclusions	O
lstmacl15	O
presented	O
stack	B-Method
LSTMs	E-Method
and	O
used	O
them	O
to	O
implement	O
a	O
transition	B-Method
-	I-Method
based	I-Method
dependency	I-Method
parser	E-Method
.	O

The	O
parser	S-Method
uses	O
a	O
greedy	B-Method
learning	I-Method
strategy	E-Method
which	O
potentially	O
provides	O
very	O
high	O
parsing	S-Task
speed	O
while	O
still	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

We	O
have	O
demonstrated	O
that	O
improvement	O
by	O
training	O
the	O
greedy	B-Method
parser	E-Method
on	O
non	O
-	O
gold	O
outcomes	O
;	O
dynamic	B-Method
oracles	E-Method
improve	O
the	O
stack	B-Method
LSTM	I-Method
parser	E-Method
,	O
achieving	O
93.56	O
UAS	S-Metric
for	O
English	S-Material
,	O
maintaining	O
greedy	B-Method
search	E-Method


.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
U.	O
S.	O

Army	O
Research	O
Laboratory	O
and	O
the	O
U.	O
S.	O

Army	O
Research	O
Office	O
under	O
contract	O
/	O
grant	O
number	O
W911NF	O
-	O
10	O
-	O
1	O
-	O
0533	O
,	O
and	O
in	O
part	O
by	O
NSF	O
CAREER	O
grant	O
IIS	O
-	O
1054319	O
.	O

Miguel	O
Ballesteros	O
was	O
supported	O
by	O
the	O
European	O
Commission	O
under	O
the	O
contract	O
numbers	O
FP7	O
-	O
ICT	O
-	O
610411	O
(	O
project	O
MULTISENSOR	O
)	O
and	O
H2020	O
-	O
RIA	O
-	O
645012	O
(	O
project	O
KRISTINA	O
)	O
.	O

Yoav	O
Goldberg	O
is	O
supported	O
by	O
the	O
Intel	O
Collaborative	O
Research	O
Institute	O
for	O
Computational	B-Task
Intelligence	E-Task
(	O
ICRI	O
-	O
CI	O
)	O
,	O
a	O
Google	O
Research	O
Award	O
and	O
the	O
Israeli	O
Science	O
Foundation	O
(	O
grant	O
number	O
1555	O
/	O
15	O
)	O
.	O

bibliography	O
:	O
References	O
Pairwise	B-Method
Confusion	E-Method
for	O
Fine	B-Material
-	I-Material
Grained	I-Material
Visual	I-Material
Classification	E-Material
section	O
:	O
Abstract	O
.	O

Fine	B-Material
-	I-Material
Grained	I-Material
Visual	I-Material
Classification	E-Material
(	O
FGVC	S-Material
)	O
datasets	S-Material
contain	O
small	O
sample	O
sizes	O
,	O
along	O
with	O
significant	O
intra	O
-	O
class	O
variation	O
and	O
interclass	B-Metric
similarity	E-Metric
.	O

While	O
prior	O
work	O
has	O
addressed	O
intra	B-Task
-	I-Task
class	I-Task
variation	E-Task
using	O
localization	S-Method
and	O
segmentation	B-Method
techniques	E-Method
,	O
inter	O
-	O
class	O
similarity	O
may	O
also	O
affect	O
feature	B-Method
learning	E-Method
and	O
reduce	O
classification	S-Task
performance	O
.	O

In	O
this	O
work	O
,	O
we	O
address	O
this	O
problem	O
using	O
a	O
novel	O
optimization	B-Method
procedure	E-Method
for	O
the	O
end	B-Task
-	I-Task
to	I-Task
-	I-Task
end	I-Task
neural	I-Task
network	I-Task
training	E-Task
on	O
FGVC	B-Task
tasks	E-Task
.	O

Our	O
procedure	O
,	O
called	O
Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
reduces	O
overfitting	O
by	O
intentionally	O
introducing	O
confusion	O
in	O
the	O
activations	O
.	O

With	O
PC	B-Method
regularization	E-Method
,	O
we	O
obtain	O
state	O
-	O
ofthe	O
-	O
art	O
performance	O
on	O
six	O
of	O
the	O
most	O
widely	O
-	O
used	O
FGVC	B-Material
datasets	E-Material
and	O
demonstrate	O
improved	O
localization	S-Method
ability	O
.	O

PC	S-Method
is	O
easy	O
to	O
implement	O
,	O
does	O
not	O
need	O
excessive	O
hyperparameter	B-Method
tuning	E-Method
during	O
training	O
,	O
and	O
does	O
not	O
add	O
significant	O
overhead	O
during	O
test	O
time	O
.	O

section	O
:	O
Introduction	O
The	O
Fine	B-Material
-	I-Material
Grained	I-Material
Visual	I-Material
Classification	E-Material
(	O
FGVC	S-Material
)	O
task	O
focuses	O
on	O
differentiating	O
between	O
hard	B-Task
-	I-Task
to	I-Task
-	I-Task
distinguish	I-Task
object	I-Task
classes	E-Task
,	O
such	O
as	O
species	O
of	O
birds	O
,	O
flowers	O
,	O
or	O
animals	O
;	O
and	O
identifying	O
the	O
makes	O
or	O
models	B-Task
of	I-Task
vehicles	E-Task
.	O

FGVC	B-Material
datasets	E-Material
depart	O
from	O
conventional	O
image	B-Task
classification	E-Task
in	O
that	O
they	O
typically	O
require	O
expert	O
knowledge	O
,	O
rather	O
than	O
crowdsourcing	O
,	O
for	O
gathering	B-Task
annotations	E-Task
.	O

FGVC	B-Material
datasets	E-Material
contain	O
images	O
with	O
much	O
higher	O
visual	O
similarity	O
than	O
those	O
in	O
large	O
-	O
scale	O
visual	O
classification	O
(	O
LSVC	O
)	O
.	O

Moreover	O
,	O
FGVC	B-Material
datasets	E-Material
have	O
minute	O
inter	O
-	O
class	O
visual	O
differences	O
in	O
addition	O
to	O
the	O
variations	O
in	O
pose	O
,	O
lighting	O
and	O
viewpoint	O
found	O
in	O
LSVC	O
[	O
reference	O
]	O
.	O

Additionally	O
,	O
FGVC	B-Material
datasets	E-Material
often	O
exhibit	O
long	O
tails	O
in	O
the	O
data	O
distribution	O
,	O
since	O
the	O
difficulty	O
of	O
obtaining	O
examples	O
of	O
different	O
classes	O
may	O
vary	O
.	O

This	O
combination	O
of	O
small	O
,	O
non	O
-	O
uniform	O
datasets	S-Material
and	O
subtle	O
inter	O
-	O
class	O
differences	O
makes	O
FGVC	S-Material
challenging	O
even	O
for	O
powerful	O
deep	B-Method
learning	I-Method
algorithms	E-Method
.	O

Most	O
of	O
the	O
prior	O
work	O
in	O
FGVC	S-Material
has	O
focused	O
on	O
tackling	O
the	O
intra	O
-	O
class	O
variation	O
in	O
pose	O
,	O
lighting	O
,	O
and	O
viewpoint	O
using	O
localization	S-Method
techniques	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
by	O
augmenting	O
training	O
datasets	S-Material
with	O
additional	O
data	O
from	O
the	O
Web	O
[	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
we	O
observe	O
that	O
prior	O
work	O
in	O
FGVC	S-Material
does	O
not	O
pay	O
much	O
attention	O
to	O
the	O
problems	O
that	O
may	O
arise	O
due	O
to	O
the	O
inter	O
-	O
class	O
visual	O
similarity	O
in	O
the	O
feature	B-Method
extraction	I-Method
pipeline	E-Method
.	O

Similar	O
to	O
LSVC	B-Task
tasks	E-Task
,	O
neural	B-Method
networks	E-Method
for	O
FGVC	B-Task
tasks	E-Task
are	O
typically	O
trained	O
with	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

In	O
LSVC	O
datasets	S-Material
such	O
as	O
ImageNet	S-Material
[	O
reference	O
]	O
,	O
strongly	O
discriminative	B-Method
learning	E-Method
using	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
is	O
successful	O
in	O
part	O
due	O
to	O
the	O
significant	O
inter	O
-	O
class	O
variation	O
(	O
compared	O
to	O
intraclass	O
variation	O
)	O
,	O
which	O
enables	O
deep	B-Method
networks	E-Method
to	O
learn	O
generalized	O
discriminatory	O
features	O
with	O
large	O
amounts	O
of	O
data	O
.	O

We	O
posit	O
that	O
this	O
formulation	O
may	O
not	O
be	O
ideal	O
for	O
FGVC	S-Material
,	O
which	O
shows	O
smaller	O
visual	O
differences	O
between	O
classes	O
and	O
larger	O
differences	O
within	O
each	O
class	O
than	O
LSVC	S-Method
.	O

For	O
instance	O
,	O
if	O
two	O
samples	O
in	O
the	O
training	O
set	O
have	O
very	O
similar	O
visual	O
content	O
but	O
different	O
class	O
labels	O
,	O
minimizing	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
will	O
force	O
the	O
neural	B-Method
network	E-Method
to	O
learn	O
features	O
that	O
distinguish	O
these	O
two	O
images	O
with	O
high	O
confidence	O
-	O
potentially	O
forcing	O
the	O
network	O
to	O
learn	O
sample	O
-	O
specific	O
artifacts	O
for	O
visually	O
confusing	O
classes	O
in	O
order	O
to	O
minimize	O
training	B-Metric
error	E-Metric
.	O

We	O
suspect	O
that	O
this	O
effect	O
would	O
be	O
especially	O
pronounced	O
in	O
FGVC	S-Material
,	O
since	O
there	O
are	O
fewer	O
samples	O
from	O
which	O
the	O
network	O
can	O
learn	O
generalizable	O
class	O
-	O
specific	O
features	O
.	O

Based	O
on	O
this	O
hypothesis	O
,	O
we	O
propose	O
that	O
introducing	O
confusion	O
in	O
output	O
logit	O
activations	O
during	O
training	O
for	O
an	O
FGVC	S-Material
task	O
will	O
force	O
the	O
network	O
to	O
learn	O
slightly	O
less	O
discriminative	O
features	O
,	O
thereby	O
preventing	O
it	O
from	O
overfitting	O
to	O
sample	O
-	O
specific	O
artifacts	O
.	O

Specifically	O
,	O
we	O
aim	O
to	O
confuse	O
the	O
network	O
,	O
by	O
minimizing	O
the	O
distance	O
between	O
the	O
predicted	O
probability	O
distributions	O
for	O
random	O
pairs	O
of	O
samples	O
from	O
the	O
training	O
set	O
.	O

To	O
do	O
so	O
,	O
we	O
propose	O
Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
[	O
reference	O
]	O
,	O
a	O
pairwise	B-Method
algorithm	E-Method
for	O
training	O
convolutional	B-Method
neural	I-Method
networks	E-Method
(	O
CNNs	S-Method
)	O
end	O
-	O
to	O
-	O
end	O
for	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
classification	E-Task
.	O

In	O
Pairwise	B-Method
Confusion	E-Method
,	O
we	O
construct	O
a	O
Siamese	B-Method
neural	I-Method
network	E-Method
trained	O
with	O
a	O
novel	O
loss	B-Method
function	E-Method
that	O
attempts	O
to	O
bring	O
class	O
conditional	O
probability	O
distributions	O
closer	O
to	O
each	O
other	O
.	O

Using	O
Pairwise	B-Method
Confusion	E-Method
with	O
a	O
standard	O
network	B-Method
architecture	E-Method
like	O
DenseNet	S-Method
[	O
reference	O
]	O
or	O
ResNet	S-Method
[	O
reference	O
]	O
as	O
a	O
base	O
network	O
,	O
we	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
of	O
the	O
most	O
widely	O
-	O
used	O
fine	B-Task
-	I-Task
grained	I-Task
recognition	I-Task
datasets	E-Task
,	O
improving	O
over	O
the	O
previous	O
-	O
best	O
published	O
methods	O
by	O
1.86	O
%	O
on	O
average	O
.	O

In	O
addition	O
,	O
PC	S-Method
-	O
trained	O
networks	O
show	O
better	O
localization	S-Method
performance	O
as	O
compared	O
to	O
standard	O
networks	O
.	O

Pairwise	B-Method
Confusion	E-Method
is	O
simple	O
to	O
implement	O
,	O
has	O
no	O
added	O
overhead	O
in	O
training	O
or	O
prediction	B-Metric
time	E-Metric
,	O
and	O
provides	O
performance	O
improvements	O
both	O
in	O
FGVC	B-Task
tasks	E-Task
and	O
other	O
tasks	O
that	O
involve	O
transfer	B-Method
learning	E-Method
with	O
small	O
amounts	O
of	O
training	O
data	O
.	O

section	O
:	O
Related	O
Work	O
Fine	B-Material
-	I-Material
Grained	I-Material
Visual	I-Material
Classification	E-Material
:	O
Early	O
FGVC	S-Material
research	O
focused	O
on	O
methods	O
to	O
train	O
with	O
limited	O
labeled	O
data	O
and	O
traditional	O
image	O
features	O
.	O

Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
combined	O
strongly	O
discriminative	O
image	O
patches	O
with	O
randomization	B-Method
techniques	E-Method
to	O
prevent	O
overfitting	O
.	O

Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
subsequently	O
utilized	O
template	B-Method
matching	E-Method
to	O
avoid	O
the	O
need	O
for	O
a	O
large	O
number	O
of	O
annotations	O
.	O

Recently	O
,	O
improved	O
localization	S-Method
of	O
the	O
target	O
object	O
in	O
training	O
images	O
has	O
been	O
shown	O
to	O
be	O
useful	O
for	O
FGVC	S-Material
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Zhang	O
et	O
al	O
.	O

[	O
reference	O
]	O
utilize	O
part	B-Method
-	I-Method
based	I-Method
Region	I-Method
-	I-Method
CNNs	E-Method
[	O
reference	O
]	O
to	O
perform	O
finer	O
localization	S-Method
.	O

Spatial	B-Method
Transformer	I-Method
Networks	E-Method
[	O
reference	O
]	O
show	O
that	O
learning	O
a	O
content	B-Method
-	I-Method
based	I-Method
affine	I-Method
transformation	I-Method
layer	E-Method
improves	O
FGVC	S-Material
performance	O
.	O

Pose	B-Method
-	I-Method
normalized	I-Method
CNNs	E-Method
have	O
also	O
been	O
shown	O
to	O
be	O
effective	O
at	O
FGVC	S-Material
[	O
reference	O
][	O
reference	O
]	O
.	O

Model	B-Method
ensembling	E-Method
and	O
boosting	S-Method
has	O
also	O
improved	O
performance	O
on	O
FGVC	S-Material
[	O
reference	O
]	O
.	O

Lin	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
Bilinear	B-Method
Pooling	E-Method
,	O
which	O
combines	O
pairwise	O
local	O
feature	O
sets	O
and	O
improves	O
classification	S-Task
performance	O
.	O

Bilinear	B-Method
Pooling	E-Method
has	O
been	O
extended	O
by	O
Gao	O
et	O
al	O
.	O

[	O
reference	O
]	O
using	O
a	O
compact	B-Method
bilinear	I-Method
representation	E-Method
and	O
Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
using	O
a	O
general	O
Kernel	B-Method
-	I-Method
based	I-Method
pooling	I-Method
framework	E-Method
that	O
captures	O
higher	O
-	O
order	O
interactions	O
of	O
features	O
.	O

Pairwise	B-Task
Learning	E-Task
:	O
Chopra	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
a	O
Siamese	B-Method
neural	I-Method
network	E-Method
for	O
handwriting	B-Task
recognition	E-Task
.	O

Parikh	O
and	O
Grauman	O
[	O
reference	O
]	O
developed	O
a	O
pairwise	B-Method
ranking	I-Method
scheme	E-Method
for	O
relative	B-Task
attribute	I-Task
learning	E-Task
.	O

Subsequently	O
,	O
pairwise	B-Method
neural	I-Method
network	I-Method
models	E-Method
have	O
become	O
common	O
for	O
attribute	B-Task
modeling	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Learning	O
from	O
Label	O
Confusion	O
:	O
Our	O
method	O
aims	O
to	O
improve	O
classification	S-Task
performance	O
by	O
introducing	O
confusion	O
within	O
the	O
output	O
labels	O
.	O

Prior	O
work	O
in	O
this	O
area	O
includes	O
methods	O
that	O
utilize	O
label	O
noise	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
and	O
data	O
noise	O
(	O
e.g.	O
,	O
[	O
reference	O
]	O
)	O
in	O
training	O
.	O

Krause	O
et	O
al	O
.	O

[	O
reference	O
]	O
utilized	O
noisy	O
training	O
data	O
for	O
FGVC	S-Material
.	O

Neelakantan	O
et	O
al	O
.	O

[	O
reference	O
]	O
added	O
noise	O
to	O
the	O
gradient	O
during	O
training	O
to	O
improve	O
generalization	S-Task
performance	O
in	O
very	B-Task
deep	I-Task
networks	E-Task
.	O

Szegedy	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
label	B-Method
-	I-Method
smoothing	I-Method
regularization	E-Method
for	O
training	O
deep	B-Method
Inception	I-Method
models	E-Method
.	O

In	O
this	O
paper	O
,	O
we	O
bring	O
together	O
concepts	O
from	O
pairwise	B-Method
learning	E-Method
and	O
label	B-Task
confusion	E-Task
and	O
take	O
a	O
step	O
towards	O
solving	O
the	O
problems	O
of	O
overfitting	O
and	O
samplespecific	O
artifacts	O
when	O
training	O
neural	B-Method
networks	E-Method
for	O
FGVC	B-Task
tasks	E-Task
.	O

section	O
:	O
Method	O
FGVC	B-Material
datasets	E-Material
in	O
computer	B-Task
vision	E-Task
are	O
orders	O
of	O
magnitude	O
smaller	O
than	O
LSVC	O
datasets	S-Material
and	O
contain	O
greater	O
imbalance	O
across	O
classes	O
(	O
see	O
Table	O
1	O
)	O
.	O

Moreover	O
,	O
the	O
samples	O
of	O
a	O
class	O
are	O
not	O
accurately	O
representative	O
of	O
the	O
complete	O
variation	O
in	O
the	O
visual	O
class	O
itself	O
.	O

The	O
smaller	O
dataset	O
size	O
can	O
result	O
in	O
overfitting	O
when	O
training	O
deep	B-Method
neural	I-Method
architectures	E-Method
with	O
large	O
number	O
of	O
parameters	O
-	O
even	O
with	O
preliminary	O
layers	O
being	O
frozen	O
.	O

In	O
addition	O
,	O
the	O
training	O
data	O
may	O
not	O
be	O
completely	O
representative	O
of	O
the	O
real	O
-	O
world	O
data	O
,	O
with	O
issues	O
such	O
as	O
more	O
abundant	O
sampling	O
for	O
certain	O
classes	O
.	O

For	O
example	O
,	O
in	O
FGVC	S-Material
of	O
birds	O
,	O
certain	O
species	O
from	O
geographically	O
accessible	O
areas	O
may	O
be	O
overrepresented	O
in	O
the	O
training	O
dataset	O
.	O

As	O
a	O
result	O
,	O
the	O
neural	B-Method
network	E-Method
may	O
learn	O
to	O
latch	O
on	O
to	O
sample	O
-	O
specific	O
artifacts	O
in	O
the	O
image	O
,	O
instead	O
of	O
learning	O
a	O
versatile	B-Method
representation	E-Method
for	O
the	O
target	O
object	O
.	O

We	O
aim	O
to	O
solve	O
both	O
of	O
these	O
issues	O
in	O
FGVC	S-Material
(	O
overfitting	O
and	O
sample	B-Task
-	I-Task
specific	I-Task
artifacts	E-Task
)	O
by	O
bringing	O
the	O
different	O
class	O
-	O
conditional	O
probability	O
distributions	O
closer	O
together	O
and	O
confusing	O
the	O
deep	B-Method
network	E-Method
,	O
subsequently	O
reducing	O
its	O
prediction	B-Metric
over	I-Metric
-	I-Metric
confidence	E-Metric
,	O
thus	O
improving	O
generalization	B-Metric
performance	E-Metric
.	O

Let	O
us	O
formalize	O
the	O
idea	O
of	O
"	O
confusing	O
"	O
the	O
conditional	O
probability	O
distributions	O
.	O

Consider	O
the	O
conditional	O
probability	O
distributions	O
for	O
two	O
input	O
images	O
x	O
1	O
and	O
x	O
2	O
,	O
which	O
can	O
be	O
given	O
by	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
and	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
respectively	O
.	O

For	O
a	O
classification	B-Task
problem	E-Task
with	O
N	O
output	O
classes	O
,	O
each	O
of	O
these	O
distributions	O
is	O
an	O
N	O
-	O
dimensional	O
vector	O
,	O
with	O
each	O
element	O
i	O
denoting	O
the	O
belief	O
of	O
the	O
classifier	O
in	O
class	O
y	O
i	O
given	O
input	O
x.	O
If	O
we	O
wish	O
to	O
confuse	O
the	O
class	O
outputs	O
of	O
the	O
classifier	S-Method
for	O
the	O
pair	O
x	O
1	O
and	O
x	O
2	O
,	O
we	O
should	O
learn	O
parameters	O
θ	O
that	O
bring	O
these	O
conditional	O
probability	O
distributions	O
"	O
closer	O
"	O
under	O
some	O
distance	B-Metric
metric	E-Metric
,	O
that	O
is	O
,	O
make	O
the	O
predictions	O
for	O
x	O
1	O
and	O
x	O
2	O
similar	O
.	O

While	O
KL	S-Method
-	O
divergence	O
might	O
seem	O
to	O
be	O
a	O
reasonable	O
choice	O
to	O
design	O
a	O
loss	O
function	O
for	O
optimizing	O
the	O
distance	O
between	O
conditional	O
probability	O
distributions	O
,	O
in	O
Section	O
3.1	O
,	O
we	O
show	O
that	O
it	O
is	O
infeasible	O
to	O
train	O
a	O
neural	B-Method
network	E-Method
when	O
using	O
KL	S-Method
-	O
divergence	O
as	O
a	O
regularizer	S-Method
.	O

Therefore	O
,	O
we	O
introduce	O
the	O
Euclidean	O
Distance	O
between	O
distributions	O
as	O
a	O
metric	O
for	O
confusion	S-Task
in	O
Sections	O
3.2	O
and	O
3.3	O
and	O
describe	O
neural	B-Method
network	I-Method
training	E-Method
with	O
this	O
metric	O
in	O
Section	O
3.4	O
.	O

section	O
:	O
Symmetric	O
KL	S-Method
-	O
divergence	O
or	O
Jeffrey	B-Method
's	I-Method
Divergence	E-Method
The	O
most	O
prevalent	O
method	O
to	O
measure	O
dissimilarity	O
of	O
one	O
probability	O
distribution	O
from	O
another	O
is	O
to	O
use	O
the	O
Kullback	B-Method
-	I-Method
Liebler	E-Method
(	O
KL	S-Method
)	O
divergence	O
.	O

However	O
,	O
the	O
standard	O
KL	S-Method
-	O
divergence	O
can	O
not	O
serve	O
our	O
purpose	O
owing	O
to	O
its	O
asymmetric	O
nature	O
.	O

This	O
could	O
be	O
remedied	O
by	O
using	O
the	O
symmetric	O
KL	S-Method
-	O
divergence	O
,	O
defined	O
for	O
two	O
probability	O
distributions	O
P	O
,	O
Q	O
with	O
mass	O
functions	O
p	O
(	O
·	O
)	O
,	O
q	O
(	O
·	O
)	O
(	O
for	O
events	O
u	O
∈	O
U	O
)	O
:	O
This	O
symmetrized	O
version	O
of	O
KL	S-Method
-	O
divergence	O
,	O
known	O
as	O
Jeffrey	O
's	O
divergence	O
[	O
reference	O
]	O
,	O
is	O
a	O
measure	O
of	O
the	O
average	B-Metric
relative	I-Metric
entropy	E-Metric
between	O
two	O
probability	O
distributions	O
[	O
reference	O
]	O
.	O

For	O
our	O
model	O
parameterized	O
by	O
θ	O
,	O
for	O
samples	O
x	O
1	O
and	O
x	O
2	O
,	O
the	O
Jeffrey	O
's	O
divergence	O
can	O
be	O
written	O
as	O
:	O
Jeffrey	B-Method
's	I-Method
divergence	E-Method
satisfies	O
all	O
of	O
our	O
basic	O
requirements	O
of	O
a	O
symmetric	B-Metric
divergence	I-Metric
metric	E-Metric
between	O
probability	O
distributions	O
,	O
and	O
therefore	O
could	O
be	O
included	O
as	O
a	O
regularizing	B-Method
term	E-Method
while	O
training	O
with	O
cross	O
-	O
entropy	O
,	O
to	O
achieve	O
our	O
desired	O
confusion	O
.	O

However	O
,	O
when	O
we	O
learn	O
model	B-Method
parameters	E-Method
using	O
stochastic	B-Method
gradient	I-Method
descent	E-Method
(	O
SGD	B-Method
)	E-Method
,	O
it	O
can	O
be	O
difficult	O
to	O
train	O
,	O
especially	O
if	O
our	O
distributions	O
P	O
,	O
Q	O
have	O
mass	O
concentrated	O
on	O
different	O
events	O
.	O

This	O
can	O
be	O
seen	O
in	O
Equation	O
2	O
.	O

Consider	O
Jeffrey	B-Method
's	I-Method
divergence	E-Method
with	O
N	O
=	O
2	O
classes	O
,	O
and	O
that	O
x	O
1	O
belongs	O
to	O
class	O
1	O
,	O
and	O
x	O
2	O
belongs	O
to	O
class	O
2	O
.	O

If	O
the	O
model	O
parameters	O
θ	O
are	O
such	O
that	O
it	O
correctly	O
identifies	O
both	O
x	O
1	O
and	O
x	O
2	O
by	O
training	O
using	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
,	O
p	O
θ	O
(	O
y	O
1	O
|x	O
1	O
)	O
=	O
1	O
−	O
δ	O
1	O
and	O
p	O
θ	O
(	O
y	O
2	O
|x	O
2	O
)	O
=	O
1	O
−	O
δ	O
2	O
,	O
where	O
0	O
<	O
δ	O
1	O
,	O
δ	O
2	O
<	O
1	O
2	O
(	O
since	O
the	O
classifier	S-Method
outputs	O
correct	O
predictions	O
for	O
the	O
input	O
images	O
)	O
,	O
we	O
can	O
show	O
:	O
Please	O
see	O
the	O
supplementary	O
material	O
for	O
an	O
expanded	O
proof	O
.	O

As	O
training	O
progresses	O
with	O
these	O
labels	O
,	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
will	O
motivate	O
the	O
values	O
of	O
δ	O
1	O
and	O
δ	O
2	O
to	O
become	O
closer	O
to	O
zero	O
(	O
but	O
never	O
equaling	O
zero	O
,	O
since	O
the	O
probability	O
outputs	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
are	O
the	O
outputs	O
from	O
a	O
softmax	B-Method
)	E-Method
.	O

As	O
(	O
δ	O
1	O
,	O
δ	O
2	O
)	O
→	O
(	O
0	O
+	O
,	O
0	O
+	O
)	O
,	O
the	O
second	O
term	O
−	O
log	O
(	O
δ	O
1	O
δ	O
2	O
)	O
on	O
the	O
R.H.S.	O
of	O
inequality	O
(	O
4	O
)	O
typically	O
grows	O
whereas	O
(	O
1	O
−	O
δ	O
1	O
−	O
δ	O
2	O
)	O
approaches	O
1	O
,	O
which	O
makes	O
D	O
J	O
(	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
)	O
larger	O
as	O
the	O
predictions	O
get	O
closer	O
to	O
the	O
true	O
labels	O
.	O

In	O
practice	O
,	O
we	O
see	O
that	O
training	O
with	O
D	O
J	O
(	O
p	O
θ	O
(	O
y|x	O
1	O
)	O
,	O
p	O
θ	O
(	O
y|x	O
2	O
)	O
)	O
as	O
a	O
regularizer	B-Method
term	E-Method
diverges	O
,	O
unless	O
a	O
very	O
small	O
regularizing	O
parameter	O
is	O
chosen	O
,	O
which	O
removes	O
the	O
effect	O
of	O
regularization	O
altogether	O
.	O

A	O
natural	O
question	O
that	O
can	O
arise	O
from	O
this	O
analysis	O
is	O
that	O
cross	B-Method
-	I-Method
entropy	I-Method
training	E-Method
itself	O
involves	O
optimizing	O
KL	S-Method
-	O
divergence	O
between	O
the	O
target	O
label	O
distribution	O
and	O
the	O
model	O
's	O
predictions	O
,	O
however	O
no	O
such	O
divergence	O
occurs	O
.	O

This	O
is	O
because	O
cross	O
-	O
entropy	O
involves	O
only	O
one	O
direction	O
of	O
the	O
KL	S-Method
-	O
divergence	O
,	O
and	O
the	O
target	O
distribution	O
has	O
all	O
the	O
mass	O
concentrated	O
at	O
one	O
event	O
(	O
the	O
correct	O
label	O
)	O
.	O

Since	O
(	O
x	O
log	O
x	O
)	O
|	O
x=0	O
=	O
0	O
,	O
for	O
predicted	O
label	O
vector	O
y	O
with	O
correct	O
label	O
class	O
c	O
,	O
this	O
simplifies	O
the	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
error	I-Metric
L	I-Metric
CE	E-Metric
(	O
p	O
θ	O
(	O
y|x	O
)	O
,	O
y	O
)	O
to	O
be	O
:	O
This	O
formulation	O
does	O
not	O
diverge	O
as	O
the	O
model	O
trains	O
,	O
i.e.	O
p	O
θ	O
(	O
y	O
c	O
|x	O
)	O
→	O
1	O
.	O

In	O
some	O
cases	O
where	O
label	O
noise	O
is	O
added	O
to	O
the	O
label	O
vector	O
(	O
such	O
as	O
label	B-Method
smoothing	E-Method
[	O
reference	O
][	O
reference	O
]	O
)	O
,	O
the	O
label	O
noise	O
is	O
a	O
fixed	O
constant	O
and	O
not	O
approaching	O
zero	O
(	O
as	O
in	O
the	O
case	O
of	O
Jeffery	O
's	O
divergence	O
between	O
model	O
predictions	O
)	O
and	O
is	O
hence	O
feasible	O
to	O
train	O
.	O

Thus	O
,	O
Jeffrey	O
's	O
Divergence	O
or	O
symmetric	O
KL	S-Method
-	O
divergence	O
,	O
while	O
a	O
seemingly	O
natural	O
choice	O
,	O
can	O
not	O
be	O
used	O
to	O
train	O
a	O
neural	B-Method
network	E-Method
with	O
SGD	S-Method
.	O

This	O
motivates	O
us	O
to	O
look	O
for	O
an	O
alternative	O
metric	O
to	O
measure	O
"	O
confusion	O
"	O
between	O
conditional	O
probability	O
distributions	O
.	O

section	O
:	O
Euclidean	O
Distance	O
as	O
Confusion	O
Since	O
the	O
conditional	O
probability	O
distribution	O
over	O
N	O
classes	O
is	O
an	O
element	O
within	O
R	O
N	O
on	O
the	O
unit	O
simplex	O
,	O
we	O
can	O
consider	O
the	O
Euclidean	O
distance	O
to	O
be	O
a	O
metric	O
of	O
"	O
confusion	O
"	O
between	O
two	O
conditional	B-Method
probability	I-Method
distributions	E-Method
.	O

Analogous	O
to	O
the	O
previous	O
setting	O
,	O
we	O
define	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
·	O
,	O
·	O
)	O
for	O
a	O
pair	O
of	O
inputs	O
x	O
1	O
,	O
x	O
2	O
with	O
model	O
parameters	O
θ	O
as	O
:	O
(	O
5	O
)	O
Unlike	O
Jeffrey	O
's	O
Divergence	O
,	O
Euclidean	O
Confusion	O
does	O
not	O
diverge	O
when	O
used	O
as	O
a	O
regularization	B-Method
term	E-Method
with	O
cross	O
-	O
entropy	O
.	O

However	O
,	O
to	O
verify	O
this	O
unconventional	O
choice	O
for	O
a	O
distance	O
metric	O
between	O
probability	O
distributions	O
,	O
we	O
prove	O
some	O
properties	O
that	O
relate	O
Euclidean	B-Metric
Confusion	E-Metric
to	O
existing	O
divergence	B-Metric
measures	E-Metric
.	O

Lemma	O
1	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
P	O
,	O
Q	O
)	O
is	O
a	O
lower	O
bound	O
for	O
the	O
Jeffrey	B-Metric
's	I-Metric
Divergence	E-Metric
D	O
J	O
(	O
P	O
,	O
Q	O
)	O
for	O
probability	O
measures	O
P	O
,	O
Q.	O
Proof	O
.	O

This	O
follows	O
from	O
Pinsker	O
's	O
Inequality	O
and	O
the	O
relationship	O
between	O
1	O
and	O
2	O
norms	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

By	O
Lemma	O
1	O
,	O
we	O
can	O
see	O
that	O
the	O
Euclidean	O
Confusion	O
is	O
a	O
conservative	O
estimate	O
for	O
Jeffrey	B-Metric
's	I-Metric
divergence	E-Metric
,	O
the	O
earlier	O
proposed	O
divergence	B-Metric
measure	E-Metric
.	O

For	O
finite	O
probability	O
spaces	O
,	O
the	O
Total	B-Metric
Variation	I-Metric
Distance	E-Metric
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
2	O
=	O
1	O
2	O
P	O
−	O
Q	O
1	O
is	O
also	O
a	O
measure	O
of	O
interest	O
.	O

However	O
,	O
due	O
to	O
its	O
non	O
-	O
differentiable	O
nature	O
,	O
it	O
is	O
unsuitable	O
for	O
our	O
case	O
.	O

Nevertheless	O
,	O
we	O
can	O
relate	O
the	O
Euclidean	B-Metric
Confusion	E-Metric
and	O
Total	B-Metric
Variation	I-Metric
Distance	E-Metric
by	O
the	O
following	O
result	O
.	O

Lemma	O
2	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
the	O
Euclidean	O
Confusion	O
D	O
EC	O
(	O
P	O
,	O
Q	O
)	O
is	O
bounded	O
by	O
4D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
2	O
for	O
probability	O
measures	O
P	O
,	O
Q.	O
Proof	O
.	O

This	O
follows	O
directly	O
from	O
the	O
relationship	O
between	O
1	O
and	O
2	O
norms	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

section	O
:	O
Euclidean	O
Confusion	O
for	O
Point	O
Sets	O
In	O
a	O
standard	O
classification	B-Task
setting	E-Task
with	O
N	O
classes	O
,	O
we	O
consider	O
a	O
training	O
set	O
with	O
m	O
=	O
N	O
i=1	O
m	O
i	O
training	O
examples	O
,	O
where	O
m	O
i	O
denotes	O
the	O
number	O
of	O
training	O
samples	O
for	O
class	O
i.	O
For	O
this	O
setting	O
,	O
we	O
can	O
write	O
the	O
total	O
Euclidean	O
Confusion	O
between	O
points	O
of	O
classes	O
i	O
and	O
j	O
as	O
the	O
average	O
of	O
the	O
Euclidean	O
Confusion	O
between	O
all	O
pairs	O
of	O
points	O
belonging	O
to	O
those	O
two	O
classes	O
.	O

For	O
simplicity	O
of	O
notation	O
,	O
let	O
us	O
denote	O
the	O
set	O
of	O
conditional	O
probability	O
distributions	O
of	O
all	O
training	O
points	O
belonging	O
to	O
class	O
i	O
for	O
a	O
model	O
parameterized	O
by	O
θ	O
as	O
Then	O
,	O
for	O
a	O
model	O
parameterized	O
by	O
θ	O
,	O
the	O
Euclidean	B-Metric
Confusion	E-Metric
is	O
given	O
by	O
:	O
We	O
can	O
simplify	O
this	O
equation	O
by	O
assuming	O
an	O
equal	O
number	O
of	O
points	O
n	O
per	O
class	O
:	O
This	O
form	O
of	O
the	O
Euclidean	O
Confusion	O
between	O
the	O
two	O
sets	O
of	O
points	O
gives	O
us	O
an	O
interesting	O
connection	O
with	O
another	O
popular	O
distance	B-Metric
metric	E-Metric
over	O
probability	O
distributions	O
,	O
known	O
as	O
the	O
Energy	O
Distance	O
[	O
reference	O
]	O
.	O

Introduced	O
by	O
Gabor	O
Szekely	O
[	O
reference	O
]	O
,	O
the	O
Energy	O
Distance	O
D	O
EN	O
(	O
F	O
,	O
G	O
)	O
between	O
two	O
cumulative	B-Method
probability	I-Method
distribution	I-Method
functions	E-Method
F	O
and	O
G	O
with	O
random	O
vectors	O
X	O
and	O
Y	O
in	O
R	O
N	O
can	O
be	O
given	O
by	O
where	O
(	O
X	O
,	O
X	O
,	O
Y	O
,	O
Y	O
)	O
are	O
independent	O
,	O
and	O
X	O
∼	O
F	O
,	O
X	O
∼	O
F	O
,	O
Y	O
∼	O
G	O
,	O
Y	O
∼	O
G.	O
If	O
we	O
consider	O
the	O
sets	O
S	O
i	O
and	O
S	O
j	O
,	O
with	O
a	O
uniform	O
probability	O
of	O
selecting	O
any	O
of	O
the	O
n	O
points	O
in	O
each	O
of	O
these	O
sets	O
,	O
then	O
we	O
obtain	O
the	O
following	O
results	O
.	O

Lemma	O
3	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
:	O
where	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
the	O
Energy	O
Distance	O
under	O
Euclidean	O
norm	O
between	O
S	O
i	O
and	O
S	O
j	O
(	O
parameterized	O
by	O
θ	O
)	O
,	O
and	O
random	O
vectors	O
are	O
selected	O
with	O
uniform	O
probability	O
in	O
both	O
S	O
i	O
and	O
S	O
j	O
.	O

Proof	O
.	O

This	O
follows	O
from	O
the	O
definition	O
of	O
Energy	O
Distance	O
with	O
uniform	O
probability	O
of	O
sampling	O
.	O

Complete	O
proof	O
is	O
provided	O
in	O
the	O
supplementary	O
material	O
.	O

Corollary	O
1	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
,	O
we	O
have	O
:	O
with	O
equality	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

Proof	O
.	O

This	O
follows	O
from	O
the	O
fact	O
that	O
the	O
Energy	O
Distance	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
0	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

The	O
complete	O
version	O
of	O
the	O
proof	O
is	O
included	O
in	O
the	O
supplement	O
.	O

With	O
these	O
results	O
,	O
we	O
restrict	O
the	O
behavior	O
of	O
Euclidean	O
Confusion	O
within	O
two	O
well	O
-	O
defined	O
conventional	O
probability	B-Metric
distance	I-Metric
measures	E-Metric
,	O
the	O
Jeffrey	B-Metric
's	I-Metric
divergence	E-Metric
and	O
Energy	B-Method
Distance	E-Method
.	O

One	O
might	O
consider	O
optimizing	O
the	O
Energy	O
Distance	O
directly	O
,	O
due	O
to	O
its	O
similar	O
formulation	O
and	O
the	O
fact	O
that	O
we	O
uniformly	O
sample	O
points	O
during	O
training	O
with	O
SGD	S-Method
.	O

However	O
,	O
the	O
Energy	O
Distance	O
additionally	O
includes	O
the	O
two	O
terms	O
that	O
account	O
for	O
the	O
negative	O
of	O
the	O
average	O
all	O
-	O
pairs	O
distances	O
between	O
points	O
in	O
S	O
i	O
and	O
S	O
j	O
respectively	O
,	O
which	O
we	O
do	O
not	O
want	O
to	O
maximize	O
,	O
since	O
we	O
do	O
not	O
wish	O
to	O
push	O
points	O
within	O
the	O
same	O
class	O
further	O
apart	O
.	O

Therefore	O
,	O
we	O
proceed	O
with	O
our	O
measure	O
of	O
Euclidean	B-Metric
Confusion	E-Metric
.	O

section	O
:	O
Learning	S-Task
with	O
Gradient	B-Method
Descent	E-Method
We	O
proceed	O
to	O
learn	O
parameters	O
θ	O
*	O
for	O
a	O
neural	B-Method
network	E-Method
,	O
with	O
the	O
following	O
learning	B-Metric
objective	I-Metric
function	E-Metric
for	O
a	O
pair	O
of	O
input	O
points	O
,	O
motivated	O
by	O
the	O
formulation	O
.	O

We	O
employ	O
a	O
Siamese	B-Method
-	I-Method
like	I-Method
architecture	E-Method
,	O
with	O
individual	O
cross	B-Method
entropy	I-Method
calculations	E-Method
for	O
each	O
branch	O
,	O
followed	O
by	O
a	O
joint	B-Method
energy	I-Method
-	I-Method
distance	I-Method
minimization	I-Method
loss	E-Method
.	O

We	O
split	O
each	O
incoming	O
batch	O
of	O
samples	O
into	O
two	O
mini	O
-	O
batches	O
,	O
and	O
feed	O
the	O
network	O
pairwise	O
samples	O
.	O

of	O
Euclidean	O
Confusion	O
:	O
This	O
objective	B-Metric
function	E-Metric
can	O
be	O
explained	O
as	O
:	O
for	O
each	O
point	O
in	O
the	O
training	O
set	O
,	O
we	O
randomly	O
select	O
another	O
point	O
from	O
a	O
different	O
class	O
and	O
calculate	O
the	O
individual	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
losses	E-Metric
and	O
Euclidean	B-Metric
Confusion	E-Metric
until	O
all	O
pairs	O
have	O
been	O
exhausted	O
.	O

For	O
each	O
point	O
in	O
the	O
training	O
dataset	O
,	O
there	O
are	O
n·	O
(	O
N	O
−	O
1	O
)	O
valid	O
choices	O
for	O
the	O
other	O
point	O
,	O
giving	O
us	O
a	O
total	O
of	O
n	O
2	O
·	O
N	O
·	O
(	O
N	O
−	O
1	O
)	O
possible	O
pairs	O
.	O

In	O
practice	O
,	O
we	O
find	O
that	O
we	O
do	O
not	O
need	O
to	O
exhaust	O
all	O
combinations	O
for	O
effective	B-Task
learning	E-Task
using	O
gradient	B-Method
descent	E-Method
,	O
and	O
in	O
fact	O
we	O
observe	O
that	O
convergence	S-Metric
is	O
achieved	O
far	O
before	O
all	O
observations	O
are	O
observed	O
.	O

We	O
simplify	O
our	O
formulation	O
instead	O
by	O
using	O
the	O
following	O
procedure	O
described	O
in	O
Algorithm	O
1	O
.	O

Training	O
Procedure	O
:	O
As	O
described	O
in	O
Algorithm	O
1	O
,	O
our	O
learning	B-Method
procedure	E-Method
is	O
a	O
slightly	O
modified	O
version	O
of	O
the	O
standard	O
SGD	S-Method
.	O

We	O
randomly	O
permute	O
the	O
training	O
set	O
twice	O
,	O
and	O
then	O
for	O
each	O
pair	O
of	O
points	O
in	O
the	O
training	O
set	O
,	O
add	O
Euclidean	O
Confusion	O
only	O
if	O
the	O
samples	O
belong	O
to	O
different	O
classes	O
.	O

This	O
form	O
of	O
sampling	O
approximates	O
the	O
exhaustive	O
Euclidean	O
Confusion	O
,	O
with	O
some	O
points	O
with	O
regular	B-Method
gradient	I-Method
descent	E-Method
,	O
which	O
in	O
practice	O
does	O
not	O
alter	O
the	O
performance	O
.	O

Moreover	O
,	O
convergence	S-Metric
is	O
achieved	O
after	O
only	O
a	O
fraction	O
of	O
all	O
the	O
possible	O
pairs	O
are	O
observed	O
.	O

Formally	O
,	O
we	O
wish	O
to	O
model	O
the	O
conditional	O
probability	O
distribution	O
p	O
θ	O
(	O
y|x	O
)	O
over	O
the	O
p	O
classes	O
for	O
function	O
f	O
(	O
x	O
;	O
θ	O
)	O
=	O
p	O
θ	O
(	O
y|x	O
)	O
parameterized	O
by	O
model	O
parameters	O
θ	O
.	O

Given	O
our	O
optimization	B-Method
procedure	E-Method
,	O
we	O
can	O
rewrite	O
the	O
total	O
loss	O
for	O
a	O
pair	O
of	O
section	O
:	O
Algorithm	O
1	O
Training	O
Using	O
Euclidean	B-Method
Confusion	E-Method
where	O
,	O
γ	O
(	O
y	O
1	O
,	O
y	O
2	O
)	O
=	O
1	O
when	O
y	O
i	O
=	O
y	O
j	O
,	O
and	O
0	O
otherwise	O
.	O

We	O
denote	O
training	O
with	O
this	O
general	O
architecture	O
with	O
the	O
term	O
Pairwise	B-Method
Confusion	E-Method
or	O
PC	S-Method
for	O
short	O
.	O

Specifically	O
,	O
we	O
train	O
a	O
Siamese	B-Method
-	I-Method
like	I-Method
neural	I-Method
network	E-Method
[	O
reference	O
]	O
with	O
shared	O
weights	O
,	O
training	O
each	O
network	O
individually	O
using	O
cross	B-Method
-	I-Method
entropy	E-Method
,	O
and	O
add	O
the	O
Euclidean	B-Metric
Confusion	I-Metric
loss	E-Metric
between	O
the	O
conditional	O
probability	O
distributions	O
obtained	O
from	O
each	O
network	O
(	O
Figure	O
1	O
)	O
.	O

During	O
training	S-Task
,	O
we	O
split	O
an	O
incoming	O
batch	O
of	O
training	O
samples	O
into	O
two	O
parts	O
,	O
and	O
evaluating	O
cross	B-Metric
-	I-Metric
entropy	E-Metric
on	O
each	O
sub	O
-	O
batch	O
identically	O
,	O
followed	O
by	O
a	O
pairwise	O
loss	O
term	O
calculated	O
for	O
corresponding	O
pairs	O
of	O
samples	O
across	O
batches	O
.	O

During	O
testing	O
,	O
only	O
one	O
branch	O
of	O
the	O
network	O
is	O
active	O
,	O
and	O
generates	O
output	O
predictions	O
for	O
the	O
input	O
image	O
.	O

As	O
a	O
result	O
,	O
implementing	O
this	O
method	O
does	O
not	O
introduce	O
any	O
significant	O
computational	B-Metric
overhead	E-Metric
during	O
testing	O
.	O

section	O
:	O
CNN	B-Method
Architectures	E-Method
We	O
experiment	O
with	O
VGGNet	S-Method
[	O
reference	O
]	O
,	O
GoogLeNet	S-Method
[	O
reference	O
]	O
,	O
ResNets	S-Method
[	O
reference	O
]	O
,	O
and	O
DenseNets	S-Method
[	O
reference	O
]	O
as	O
base	O
architectures	O
for	O
the	O
Siamese	B-Method
network	E-Method
trained	O
with	O
PC	S-Method
to	O
demonstrate	O
that	O
our	O
method	O
is	O
insensitive	O
to	O
the	O
choice	O
of	O
source	B-Method
architecture	E-Method
.	O

section	O
:	O
Experimental	O
Details	O
We	O
perform	O
all	O
experiments	O
using	O
Caffe	S-Method
[	O
reference	O
]	O
or	O
PyTorch	S-Method
[	O
reference	O
]	O
over	O
a	O
cluster	O
of	O
NVIDIA	B-Method
Titan	I-Method
X	E-Method
,	O
Tesla	B-Method
K40c	E-Method
and	O
GTX	B-Method
1080	I-Method
GPUs	E-Method
.	O

Our	O
code	O
and	O
models	O
are	O
available	O
at	O
github.com	O
/	O
abhimanyudubey	O
/	O
confusion	O
.	O

Next	O
,	O
we	O
provide	O
brief	O
descriptions	O
of	O
the	O
various	O
datasets	S-Material
used	O
in	O
our	O
paper	O
.	O

Table	O
2	O
.	O

Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
widelyused	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
classification	E-Task
datasets	S-Material
(	O
A	O
-	O
F	O
)	O
.	O

Improvement	O
over	O
the	O
baseline	O
model	O
is	O
reported	O
as	O
(	O
∆	O
)	O
.	O

All	O
results	O
averaged	O
over	O
5	O
trials	O
.	O

(	O
A	O
)	O
CUB	B-Method
-	I-Method
200	I-Method
-	I-Method
2011	I-Method
Method	E-Method
Top	O
-	O
1	O
∆	O
Gao	O
et	O
al	O
.	O

[	O
reference	O
]	O
84.00	O
-	O
STN	O
[	O
reference	O
]	O
84.10	O
-	O
Zhang	O
et	O
al	O
.	O

[	O
reference	O
]	O
84.50	O
-	O
Lin	O
et	O
al	O
.	O

[	O
reference	O
]	O
85.80	O
-	O
Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
86	O
.	O

car	O
make	O
,	O
model	O
,	O
and	O
year	O
.	O

The	O
Aircraft	B-Material
dataset	E-Material
is	O
a	O
set	O
of	O
10	O
,	O
000	O
images	O
across	O
100	O
classes	O
denoting	O
a	O
fine	O
-	O
grained	O
set	O
of	O
airplanes	O
of	O
different	O
varieties	O
[	O
reference	O
]	O
.	O

These	O
datasets	S-Material
contain	O
(	O
i	O
)	O
large	O
visual	O
diversity	O
in	O
each	O
class	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
(	O
ii	O
)	O
visually	O
similar	O
,	O
often	O
confusing	O
samples	O
belonging	O
to	O
different	O
classes	O
,	O
and	O
(	O
iii	O
)	O
a	O
large	O
variation	O
in	O
the	O
number	O
of	O
samples	O
present	O
per	O
class	O
,	O
leading	O
to	O
greater	O
class	O
imbalance	O
than	O
LSVC	O
datasets	S-Material
like	O
ImageNet	S-Material
[	O
reference	O
]	O
.	O

Additionally	O
,	O
some	O
of	O
these	O
datasets	S-Material
have	O
densely	O
annotated	O
part	O
information	O
available	O
,	O
which	O
we	O
do	O
not	O
utilize	O
in	O
our	O
experiments	O
.	O

section	O
:	O
Results	O
section	O
:	O
Fine	B-Material
-	I-Material
Grained	I-Material
Visual	I-Material
Classification	E-Material
We	O
first	O
describe	O
our	O
results	O
on	O
the	O
six	O
FGVC	B-Material
datasets	E-Material
from	O
Table	O
2	O
.	O

In	O
all	O
experiments	O
,	O
we	O
average	O
results	O
over	O
5	O
trials	O
per	O
experiment	O
-	O
after	O
choosing	O
the	O
best	O
value	O
of	O
hyperparameter	O
λ	O
.	O

Please	O
see	O
the	O
supplementary	O
material	O
for	O
mean	O
and	O
standard	O
deviation	O
values	O
for	O
all	O
experiments	O
.	O

1	O
.	O

Fine	B-Method
-	I-Method
tuning	E-Method
from	O
Baseline	B-Method
Models	E-Method
:	O
We	O
fine	O
-	O
tune	O
from	O
three	O
baseline	O
models	O
using	O
the	O
PC	S-Method
optimization	O
procedure	O
:	O
ResNet	B-Method
-	I-Method
50	E-Method
[	O
reference	O
]	O
,	O
Bilinear	B-Method
CNN	E-Method
[	O
reference	O
]	O
,	O
and	O
DenseNet	B-Method
-	I-Method
161	E-Method
[	O
reference	O
]	O
.	O

As	O
Tables	O
2	O
-(	O
A	O
-	O
F	O
)	O
show	O
,	O
PC	S-Method
obtains	O
substantial	O
improvement	O
across	O
all	O
datasets	S-Material
and	O
models	O
.	O

For	O
instance	O
,	O
a	O
baseline	O
DenseNet	B-Method
-	I-Method
161	E-Method
architecture	O
obtains	O
an	O
average	O
accuracy	S-Metric
of	O
84.21	O
%	O
,	O
but	O
PC	S-Method
-	O
DenseNet	B-Method
-	I-Method
161	E-Method
obtains	O
an	O
accuracy	S-Metric
of	O
86.87	O
%	O
,	O
an	O
improvement	O
of	O
2.66	O
%	O
.	O

On	O
NABirds	S-Material
,	O
we	O
obtain	O
improvements	O
of	O
4.60	O
%	O
and	O
3.42	O
%	O
over	O
baseline	O
ResNet	B-Method
-	I-Method
50	E-Method
and	O
DenseNet	B-Method
-	I-Method
161	E-Method
architectures	O
.	O

2	O
.	O

Combining	O
PC	S-Method
with	O
Specialized	O
FGVC	S-Material
models	O
:	O
Recent	O
work	O
in	O
FGVC	S-Material
has	O
proposed	O
several	O
novel	O
CNN	B-Method
designs	E-Method
that	O
take	O
part	O
-	O
localization	S-Method
into	O
account	O
,	O
such	O
as	O
bilinear	B-Method
pooling	I-Method
techniques	E-Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
and	O
spatial	B-Method
transformer	I-Method
networks	E-Method
[	O
reference	O
]	O
.	O

We	O
train	O
a	O
Bilinear	B-Method
CNN	E-Method
[	O
reference	O
]	O
with	O
PC	S-Method
,	O
and	O
obtain	O
an	O
average	O
improvement	O
of	O
1.7	O
%	O
on	O
the	O
6	O
datasets	S-Material
.	O

We	O
note	O
two	O
important	O
aspects	O
of	O
our	O
analysis	O
:	O
(	O
1	O
)	O
we	O
do	O
not	O
compare	O
with	O
ensembling	B-Method
and	I-Method
data	I-Method
augmentation	I-Method
techniques	E-Method
such	O
as	O
Boosted	B-Method
CNNs	E-Method
[	O
reference	O
]	O
and	O
Krause	O
,	O
et	O
al	O
.	O

[	O
reference	O
]	O
since	O
prior	O
evidence	O
indicates	O
that	O
these	O
techniques	O
invariably	O
improve	O
performance	O
,	O
and	O
(	O
2	O
)	O
we	O
evaluate	O
a	O
single	O
-	O
crop	O
,	O
single	B-Method
-	I-Method
model	I-Method
evaluation	E-Method
without	O
any	O
part	O
-	O
or	O
object	O
-	O
annotations	O
,	O
and	O
perform	O
competitively	O
with	O
methods	O
that	O
use	O
both	O
augmentations	S-Method
.	O

Choice	O
of	O
Hyperparameter	O
λ	O
:	O
Since	O
our	O
formulation	O
requires	O
the	O
selection	O
of	O
a	O
hyperparameter	O
λ	O
,	O
it	O
is	O
important	O
to	O
study	O
the	O
sensitivity	O
of	O
classification	B-Metric
performance	E-Metric
to	O
the	O
choice	O
of	O
λ	O
.	O

We	O
conduct	O
this	O
experiment	O
for	O
four	O
different	O
models	O
:	O
GoogLeNet	S-Method
[	O
reference	O
]	O
,	O
ResNet	B-Method
-	I-Method
50	E-Method
[	O
reference	O
]	O
and	O
VGGNet	B-Method
-	I-Method
16	E-Method
[	O
reference	O
]	O
and	O
Bilinear	B-Method
-	I-Method
CNN	E-Method
[	O
reference	O
]	O
on	O
the	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	I-Material
dataset	E-Material
.	O

PC	S-Method
's	O
performance	O
is	O
not	O
very	O
sensitive	O
to	O
the	O
choice	O
of	O
λ	O
(	O
Figure	O
2	O
and	O
Supplementary	O
Tables	O
S1	O
-	O
S5	O
)	O
.	O

For	O
all	O
six	O
datasets	S-Material
,	O
the	O
λ	O
value	O
is	O
typically	O
between	O
the	O
range	O
[	O
reference	O
][	O
reference	O
]	O
.	O

On	O
Bilinear	B-Method
CNN	E-Method
,	O
setting	O
λ	O
=	O
10	O
for	O
all	O
datasets	S-Material
gives	O
average	O
performance	O
within	O
0.08	O
%	O
compared	O
to	O
the	O
reported	O
values	O
in	O
Table	O
2	O
.	O

In	O
general	O
,	O
PC	S-Method
obtains	O
optimum	O
performance	O
in	O
the	O
range	O
of	O
0.05N	O
and	O
0.15N	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
classes	O
.	O

section	O
:	O
Additional	O
Experiments	O
Since	O
our	O
method	O
aims	O
to	O
improve	O
classification	S-Task
performance	O
in	O
FGVC	B-Task
tasks	E-Task
by	O
introducing	O
confusion	O
in	O
output	O
logit	O
activations	O
,	O
we	O
would	O
expect	O
to	O
see	O
a	O
larger	O
improvement	O
in	O
datasets	S-Material
with	O
higher	O
inter	B-Metric
-	I-Metric
class	I-Metric
similarity	E-Metric
and	O
intra	O
-	O
class	O
variation	O
.	O

To	O
test	O
this	O
hypothesis	O
,	O
we	O
conduct	O
two	O
additional	O
experiments	O
.	O

In	O
the	O
first	O
experiment	O
,	O
we	O
construct	O
two	O
subsets	O
of	O
ImageNet	B-Material
-	I-Material
1	I-Material
K	E-Material
[	O
reference	O
]	O
.	O

The	O
first	O
dataset	O
,	O
ImageNet	B-Material
-	I-Material
Dogs	E-Material
is	O
a	O
subset	O
consisting	O
only	O
of	O
species	O
of	O
dogs	O
(	O
117	O
classes	O
and	O
116	O
K	O
images	O
)	O
.	O

The	O
second	O
dataset	O
,	O
ImageNet	S-Material
-	O
Random	O
contains	O
randomly	O
selected	O
classes	O
from	O
ImageNet	B-Material
-	I-Material
1K.	E-Material
Both	O
datasets	S-Material
contain	O
equal	O
number	O
of	O
classes	O
(	O
117	O
)	O
and	O
images	O
(	O
116	O
K	O
)	O
,	O
but	O
ImageNet	B-Material
-	I-Material
Dogs	E-Material
has	O
much	O
higher	O
interclass	B-Metric
similarity	E-Metric
and	O
intra	B-Metric
-	I-Metric
class	I-Metric
variation	E-Metric
,	O
as	O
compared	O
to	O
ImageNet	O
-	O
Random	O
.	O

To	O
test	O
repeatability	O
,	O
we	O
construct	O
3	O
instances	O
of	O
Imagenet	S-Material
-	O
Random	O
,	O
by	O
randomly	O
choosing	O
a	O
different	O
subset	O
of	O
ImageNet	S-Material
with	O
117	O
classes	O
each	O
time	O
.	O

For	O
both	O
experiments	O
,	O
we	O
randomly	O
construct	O
a	O
80	O
-	O
20	O
train	O
-	O
val	O
split	O
from	O
the	O
training	O
data	O
to	O
find	O
optimal	O
λ	O
by	O
cross	B-Metric
-	I-Metric
validation	E-Metric
,	O
and	O
report	O
the	O
performance	O
on	O
the	O
unseen	B-Material
ImageNet	I-Material
validation	I-Material
set	E-Material
of	O
the	O
subset	O
of	O
chosen	O
classes	O
.	O

In	O
Table	O
3	O
,	O
we	O
compare	O
the	O
performance	O
of	O
training	O
from	O
scratch	O
with	O
-	O
and	O
without	O
-	O
PC	S-Method
across	O
three	O
models	O
:	O
GoogLeNet	S-Method
,	O
ResNet	B-Method
-	I-Method
50	E-Method
,	O
and	O
DenseNet	B-Method
-	I-Method
161	E-Method
.	O

As	O
expected	O
,	O
PC	S-Method
obtains	O
a	O
larger	O
gain	O
in	O
classification	O
accuracy	S-Metric
(	O
1.45	O
%	O
)	O
on	O
ImageNet	B-Material
-	I-Material
Dogs	E-Material
as	O
compared	O
to	O
the	O
ImageNet	B-Material
-	I-Material
Random	I-Material
dataset	E-Material
(	O
0.54	O
%	O
±	O
0.28	O
)	O
.	O

In	O
the	O
second	O
experiment	O
,	O
we	O
utilize	O
the	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
CIFAR	B-Material
-	I-Material
100	I-Material
datasets	E-Material
,	O
which	O
contain	O
the	O
same	O
number	O
of	O
total	O
images	O
.	O

CIFAR	B-Material
-	I-Material
100	E-Material
has	O
10×	O
the	O
number	O
of	O
classes	O
and	O
10	O
%	O
of	O
images	O
per	O
class	O
as	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
contains	O
larger	O
inter	O
-	O
class	O
similarity	O
and	O
intra	O
-	O
class	O
variation	O
.	O

We	O
train	O
networks	O
on	O
both	O
datasets	S-Material
from	O
scratch	O
using	O
default	O
train	O
-	O
test	O
splits	O
(	O
Table	O
3	O
)	O
.	O

As	O
expected	O
,	O
we	O
obtain	O
larger	O
average	O
gains	O
of	O
1.77	O
%	O
on	O
CIFAR	B-Material
-	I-Material
100	E-Material
,	O
as	O
compared	O
to	O
0.20	O
%	O
on	O
CIFAR	B-Material
-	I-Material
10	E-Material
.	O

Additionally	O
,	O
when	O
training	O
with	O
λ	O
=	O
10	O
on	O
the	O
entire	O
ImageNet	B-Material
dataset	E-Material
,	O
we	O
obtain	O
a	O
top	O
-	O
1	O
accuracy	S-Metric
of	O
76.28	O
%	O
(	O
compared	O
to	O
a	O
baseline	O
of	O
76.15	O
%	O
)	O
,	O
which	O
is	O
a	O
smaller	O
improvement	O
,	O
which	O
is	O
in	O
line	O
with	O
what	O
we	O
would	O
expect	O
for	O
a	O
large	B-Task
-	I-Task
scale	I-Task
image	I-Task
classification	I-Task
problem	E-Task
with	O
large	O
inter	O
-	O
class	O
variation	O
.	O

Moreover	O
,	O
while	O
training	O
with	O
PC	S-Method
,	O
we	O
observe	O
that	O
the	O
rate	O
of	O
convergence	S-Metric
is	O
always	O
similar	O
to	O
or	O
faster	O
than	O
training	O
without	O
PC	S-Method
.	O

For	O
example	O
,	O
a	O
GoogLeNet	S-Method
trained	O
on	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	E-Material
(	O
Figure	O
2	O
(	O
right	O
)	O
above	O
)	O
shows	O
that	O
PC	S-Method
converges	O
to	O
higher	O
validation	O
accuracy	S-Metric
faster	O
than	O
normal	B-Method
training	E-Method
using	O
identical	O
learning	B-Metric
rate	I-Metric
schedule	E-Metric
and	O
batch	O
size	O
.	O

Note	O
that	O
the	O
training	O
accuracy	S-Metric
is	O
reduced	O
when	O
training	O
with	O
PC	S-Method
,	O
due	O
to	O
the	O
regularization	O
effect	O
.	O

In	O
sum	O
,	O
classification	B-Task
problems	E-Task
that	O
have	O
large	O
intra	O
-	O
class	O
variation	O
and	O
high	O
inter	O
-	O
class	O
similarity	O
benefit	O
from	O
optimization	S-Task
with	O
pairwise	O
confusion	O
.	O

The	O
improvement	O
is	O
even	O
more	O
prominent	O
when	O
training	O
data	O
is	O
limited	O
.	O

section	O
:	O
Improvement	O
in	O
Localization	B-Metric
Ability	E-Metric
Recent	O
techniques	O
for	O
improving	O
classification	S-Task
performance	O
in	O
fine	B-Task
-	I-Task
grained	I-Task
recognition	E-Task
are	O
based	O
on	O
summarizing	O
and	O
extracting	O
dense	O
localization	S-Method
information	O
in	O
images	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Since	O
our	O
technique	O
increases	O
classification	O
accuracy	S-Metric
,	O
we	O
wish	O
to	O
understand	O
if	O
the	O
improvement	O
is	O
a	O
result	O
of	O
enhanced	O
CNN	O
localization	S-Method
abilities	O
due	O
to	O
PC	S-Method
.	O

To	O
measure	O
the	O
regions	O
the	O
CNN	O
localizes	O
on	O
,	O
we	O
utilize	O
Gradient	B-Method
-	I-Method
Weighted	I-Method
Class	I-Method
Activation	I-Method
Mapping	E-Method
(	O
Grad	B-Method
-	I-Method
CAM	E-Method
)	O
[	O
reference	O
]	O
,	O
a	O
method	O
that	O
provides	O
a	O
heatmap	O
of	O
visual	O
saliency	O
as	O
produced	O
by	O
the	O
network	O
.	O

We	O
perform	O
both	O
quantitative	O
and	O
qualitative	O
studies	O
of	O
localization	S-Method
ability	O
of	O
PC	S-Method
-	O
trained	O
models	O
.	O

Overlap	O
in	O
Localized	O
Regions	O
:	O
To	O
quantify	O
the	O
improvement	O
in	O
localization	S-Method
due	O
to	O
PC	S-Method
,	O
we	O
construct	O
bounding	O
boxes	O
around	O
object	O
regions	O
obtained	O
from	O
Grad	B-Method
-	I-Method
CAM	E-Method
,	O
by	O
thresholding	O
the	O
heatmap	O
values	O
at	O
0.5	O
,	O
and	O
choosing	O
the	O
largest	O
box	O
returned	O
.	O

We	O
then	O
calculate	O
the	O
mean	B-Metric
IoU	E-Metric
(	O
intersection	B-Metric
-	I-Metric
over	I-Metric
-	I-Metric
union	E-Metric
)	O
of	O
the	O
bounding	O
box	O
with	O
the	O
provided	O
object	O
bounding	O
boxes	O
for	O
the	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	I-Material
dataset	E-Material
.	O

We	O
compare	O
the	O
mean	B-Metric
IoU	E-Metric
across	O
several	O
models	O
,	O
with	O
and	O
without	O
PC	S-Method
.	O

As	O
summarized	O
in	O
Table	O
4	O
,	O
we	O
observe	O
an	O
average	O
3.4	O
%	O
improvement	O
across	O
five	O
different	O
networks	O
,	O
implying	O
better	O
localization	S-Method
accuracy	S-Metric
.	O

Change	O
in	O
Class	B-Method
-	I-Method
Activation	I-Method
Mapping	E-Method
:	O
To	O
qualitatively	O
study	O
the	O
improvement	O
in	O
localization	S-Method
due	O
to	O
PC	S-Method
,	O
we	O
obtain	O
samples	O
from	O
the	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	I-Material
dataset	E-Material
and	O
visualize	O
the	O
localization	S-Method
regions	O
returned	O
from	O
Grad	B-Method
-	I-Method
CAM	E-Method
for	O
both	O
the	O
baseline	S-Method
and	O
PC	S-Method
-	O
trained	O
VGG	O
-	O
16	O
model	O
.	O

As	O
shown	O
in	O
Figure	O
3	O
,	O
PC	S-Method
models	O
provide	O
tighter	O
,	O
more	O
accurate	O
localization	S-Method
around	O
the	O
target	O
object	O
,	O
whereas	O
sometimes	O
the	O
baseline	B-Method
model	E-Method
has	O
localization	S-Method
driven	O
by	O
image	O
artifacts	O
.	O

Figure	O
3	O
-(	O
a	O
)	O
has	O
an	O
example	O
of	O
the	O
types	O
of	O
distractions	O
that	O
are	O
often	O
present	O
in	O
FGVC	S-Material
images	O
(	O
the	O
cartoon	O
bird	O
on	O
the	O
right	O
)	O
.	O

We	O
see	O
that	O
the	O
baseline	B-Method
VGG	I-Method
-	I-Method
16	I-Method
network	E-Method
pays	O
For	O
all	O
cases	O
,	O
we	O
consistently	O
observe	O
a	O
tighter	O
and	O
more	O
accurate	O
localization	S-Method
with	O
PC	S-Method
,	O
whereas	O
the	O
baseline	B-Method
VGG	I-Method
-	I-Method
16	I-Method
network	E-Method
often	O
latches	O
on	O
to	O
artifacts	O
,	O
even	O
while	O
making	O
correct	O
predictions	O
.	O

significant	O
attention	O
to	O
the	O
distraction	O
,	O
despite	O
making	O
the	O
correct	O
prediction	O
.	O

With	O
PC	S-Method
,	O
we	O
find	O
that	O
the	O
attention	O
is	O
limited	O
almost	O
exclusively	O
to	O
the	O
correct	O
object	O
,	O
as	O
desired	O
.	O

Similarly	O
for	O
Figure	O
3	O
-(	O
b	O
)	O
,	O
we	O
see	O
that	O
the	O
baseline	O
method	O
latches	O
on	O
to	O
the	O
incorrect	O
bird	O
category	O
,	O
which	O
is	O
corrected	O
by	O
the	O
addition	O
of	O
PC	S-Method
.	O

In	O
Figures	O
3	O
-(	O
c	O
-	O
d	O
)	O
,	O
we	O
see	O
that	O
the	O
baseline	B-Method
classifier	E-Method
makes	O
incorrect	O
decisions	O
due	O
to	O
poor	O
localization	S-Method
,	O
mistakes	O
that	O
are	O
resolved	O
by	O
PC	S-Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
introduce	O
Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
,	O
an	O
optimization	B-Method
procedure	E-Method
to	O
improve	O
generalizability	S-Task
in	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
classification	E-Task
(	O
FGVC	S-Material
)	O
tasks	O
by	O
encouraging	O
confusion	O
in	O
output	O
activations	O
.	O

PC	S-Method
improves	O
FGVC	S-Material
performance	O
for	O
a	O
wide	O
class	O
of	O
convolutional	B-Method
architectures	E-Method
while	O
fine	B-Task
-	I-Task
tuning	E-Task
.	O

Our	O
experiments	O
indicate	O
that	O
PC	S-Method
-	O
trained	O
networks	O
show	O
improved	O
localization	S-Method
performance	O
which	O
contributes	O
to	O
the	O
gains	O
in	O
classification	O
accuracy	S-Metric
.	O

PC	S-Method
is	O
easy	O
to	O
implement	O
,	O
does	O
not	O
need	O
excessive	O
tuning	O
during	O
training	O
,	O
and	O
does	O
not	O
add	O
significant	O
overhead	O
during	O
test	O
time	O
,	O
in	O
contrast	O
to	O
methods	O
that	O
introduce	O
complex	O
localizationbased	B-Method
pooling	I-Method
steps	E-Method
that	O
are	O
often	O
difficult	O
to	O
implement	O
and	O
train	O
.	O

Therefore	O
,	O
our	O
technique	O
should	O
be	O
beneficial	O
to	O
a	O
wide	O
variety	O
of	O
specialized	O
neural	B-Method
network	I-Method
models	E-Method
for	O
applications	O
that	O
demand	O
for	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
classification	E-Task
or	O
learning	S-Task
from	O
limited	O
labeled	O
data	O
.	O

Consider	O
Jeffrey	B-Method
's	I-Method
divergence	E-Method
with	O
N	O
=	O
2	O
classes	O
,	O
and	O
that	O
x	O
1	O
belongs	O
to	O
class	O
1	O
,	O
and	O
x	O
2	O
belongs	O
to	O
class	O
2	O
.	O

For	O
a	O
model	O
with	O
parameters	O
θ	O
that	O
correctly	O
identifies	O
both	O
x	O
1	O
and	O
x	O
2	O
by	O
training	O
using	O
cross	B-Metric
-	I-Metric
entropy	I-Metric
loss	E-Metric
,	O
p	O
θ	O
(	O
y	O
1	O
|x	O
1	O
)	O
=	O
1	O
−	O
δ	O
1	O
and	O
p	O
θ	O
(	O
y	O
2	O
|x	O
2	O
)	O
=	O
1	O
−	O
δ	O
2	O
,	O
where	O
0	O
<	O
δ	O
1	O
,	O
δ	O
2	O
<	O
1	O
2	O
(	O
since	O
the	O
classifier	S-Method
outputs	O
correct	O
predictions	O
for	O
the	O
input	O
images	O
)	O
,	O
we	O
get	O
:	O
section	O
:	O
S1.2	O
Lemmas	O
1	O
and	O
2	O
from	O
Main	O
Text	O
(	O
Euclidean	B-Metric
Confusion	I-Metric
Bounds	E-Metric
)	O
Lemma	O
1	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
for	O
probability	O
measures	O
P	O
,	O
Q	O
:	O
where	O
D	O
J	O
(	O
P	O
,	O
Q	O
)	O
is	O
the	O
Jeffrey	O
's	O
Divergence	O
between	O
P	O
and	O
Q.	O
Proof	O
.	O

By	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
For	O
a	O
finite	O
-	O
dimensional	O
vector	O
x	O
,	O
x	O
2	O
≤	O
x	O
1	O
,	O
therefore	O
:	O
Since	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
=	O
1	O
2	O
(	O
u∈U	O
|p	O
(	O
u	O
)	O
−	O
q	O
(	O
u	O
)	O
|	O
)	O
for	O
finite	O
alphabet	O
U	O
,	O
we	O
have	O
:	O
Since	O
Total	O
Variation	O
Distance	O
is	O
symmetric	O
,	O
we	O
have	O
:	O
By	O
Pinsker	O
's	O
Inequality	O
,	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
≤	O
Lemma	O
2	O
.	O

On	O
a	O
finite	O
probability	O
space	O
,	O
for	O
probability	O
measures	O
P	O
,	O
Q	O
:	O
where	O
D	O
TV	O
denotes	O
the	O
total	O
variation	O
distance	O
between	O
P	O
and	O
Q.	O
Proof	O
.	O

By	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
For	O
a	O
finite	O
-	O
dimensional	O
vector	O
x	O
,	O
x	O
2	O
≤	O
x	O
1	O
,	O
therefore	O
:	O
Since	O
D	O
TV	O
(	O
P	O
,	O
Q	O
)	O
=	O
Lemma	O
3	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
:	O
where	O
D	O
EN	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
is	O
the	O
Energy	O
Distance	O
under	O
Euclidean	O
norm	O
between	O
S	O
i	O
and	O
S	O
j	O
(	O
parameterized	O
by	O
θ	O
)	O
,	O
and	O
random	O
vectors	O
are	O
selected	O
with	O
uniform	O
probability	O
in	O
both	O
S	O
i	O
and	O
S	O
j	O
.	O

Proof	O
.	O

From	O
the	O
definition	O
of	O
Euclidean	O
Confusion	O
,	O
we	O
have	O
:	O
Considering	O
X	O
i	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
,	O
then	O
we	O
get	O
:	O
Considering	O
X	O
j	O
∼	O
Uniform	O
(	O
S	O
j	O
)	O
,	O
we	O
obtain	O
:	O
Under	O
the	O
squared	B-Metric
Euclidean	I-Metric
norm	I-Metric
distance	E-Metric
,	O
the	O
Energy	B-Metric
Distance	E-Metric
can	O
be	O
given	O
by	O
:	O
Where	O
random	O
variables	O
X	O
,	O
X	O
∼	O
P	O
(	O
S	O
i	O
)	O
and	O
Y	O
,	O
Y	O
∼	O
P	O
(	O
S	O
j	O
)	O
.	O

If	O
P	O
(	O
S	O
i	O
)	O
=	O
Uniform	O
(	O
S	O
i	O
)	O
,	O
and	O
P	O
(	O
S	O
j	O
)	O
=	O
Uniform	O
(	O
S	O
j	O
)	O
,	O
we	O
have	O
by	O
substitution	O
of	O
Equation	O
(	O
18	O
)	O
:	O
Corollary	O
1	O
.	O

For	O
sets	O
S	O
i	O
,	O
S	O
j	O
and	O
D	O
EC	O
(	O
S	O
i	O
,	O
S	O
j	O
;	O
θ	O
)	O
as	O
defined	O
in	O
Equation	O
(	O
14	O
)	O
,	O
we	O
have	O
:	O
Proof	O
.	O

From	O
Equation	O
(	O
20	O
)	O
,	O
we	O
have	O
:	O
From	O
Equation	O
(	O
18	O
)	O
,	O
we	O
have	O
:	O
For	O
S	O
i	O
=	O
S	O
j	O
,	O
we	O
have	O
with	O
X	O
i	O
,	O
X	O
j	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
:	O
Replacing	O
this	O
in	O
Equation	O
(	O
20	O
)	O
,	O
we	O
have	O
with	O
X	O
,	O
X	O
∼	O
Uniform	O
(	O
S	O
i	O
)	O
and	O
Y	O
,	O
Y	O
∼	O
Uniform	O
(	O
S	O
j	O
)	O
:	O
From	O
Szekely	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
we	O
know	O
that	O
the	O
Energy	O
Distance	O
≥	O
0	O
with	O
equality	O
if	O
and	O
only	O
if	O
S	O
i	O
=	O
S	O
j	O
.	O

Thus	O
,	O
we	O
have	O
that	O
:	O
With	O
equality	O
only	O
when	O
S	O
i	O
=	O
S	O
j	O
.	O

section	O
:	O
S2	O
Training	O
Details	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
process	O
for	O
training	O
with	O
Pairwise	B-Method
Confusion	E-Method
for	O
different	O
base	O
architectures	O
,	O
including	O
the	O
list	O
of	O
hyperparameters	O
using	O
for	O
different	O
datasets	S-Material
.	O

section	O
:	O
ResNet	B-Method
-	I-Method
50	E-Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batch	O
-	O
size	O
8	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	B-Metric
rate	E-Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S1	O
.	O

Bilinear	O
and	O
Compact	O
Bilinear	B-Method
CNN	E-Method
:	O
In	O
all	O
experiments	O
,	O
we	O
use	O
the	O
training	O
procedure	O
described	O
by	O
the	O
authors	O
[	O
reference	O
]	O
.	O

In	O
addition	O
,	O
we	O
repeat	O
the	O
described	O
step	O
2	O
without	O
the	O
loss	O
on	O
confusion	O
from	O
the	O
obtained	O
weights	O
after	O
performing	O
Step	O
2	O
with	O
the	O
loss	O
,	O
and	O
obtain	O
an	O
additional	O
0.5	O
percent	O
gain	O
in	O
performance	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S2	O
.	O

section	O
:	O
DenseNet	B-Method
-	I-Method
161	E-Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batchsize	O
32	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	B-Metric
rate	E-Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
for	O
each	O
dataset	O
is	O
given	O
in	O
Table	O
S3	O
.	O

GoogLeNet	S-Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
300000	O
iterations	O
with	O
batch	O
-	O
size	O
32	O
,	O
with	O
a	O
step	O
size	O
of	O
30000	O
,	O
decreasing	O
it	O
by	O
a	O
ratio	O
of	O
0.96	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
is	O
given	O
in	O
Table	O
S4	O
.	O

section	O
:	O
VGGNet	B-Method
-	I-Method
16	E-Method
:	O
In	O
all	O
experiments	O
,	O
we	O
train	O
for	O
40000	O
iterations	O
with	O
batchsize	O
32	O
,	O
with	O
a	O
linear	O
decay	O
of	O
the	O
learning	B-Metric
rate	E-Metric
from	O
an	O
initial	O
value	O
of	O
0.1	O
.	O

The	O
hyperparameter	O
for	O
the	O
confusion	O
term	O
is	O
given	O
in	O
Table	O
S5	O
.	O

section	O
:	O
S3	O
Mean	O
and	O
Standard	O
Deviation	O
for	O
FGVC	S-Material
Results	O
In	O
Table	O
S6	O
,	O
we	O
provide	O
the	O
mean	O
and	O
standard	O
deviation	O
values	O
over	O
five	O
independent	O
runs	O
for	O
training	O
with	O
Pairwise	B-Method
Confusion	E-Method
with	O
different	O
baseline	B-Method
models	E-Method
.	O

These	O
results	O
correspond	O
to	O
Table	O
2	O
in	O
the	O
main	O
text	O
.	O

section	O
:	O
S4	O
Comparison	O
with	O
Regularization	S-Method
We	O
additionally	O
compare	O
the	O
performance	O
of	O
our	O
optimization	B-Method
technique	E-Method
with	O
other	O
regularization	B-Method
methods	E-Method
as	O
well	O
.	O

We	O
first	O
compare	O
Pairwise	B-Method
Comparison	E-Method
with	O
with	O
Label	B-Method
-	I-Method
Smoothing	I-Method
Regularization	E-Method
(	O
LSR	S-Method
)	O
on	O
all	O
six	O
FGVC	B-Material
datasets	E-Material
for	O
VGG	B-Material
-	I-Material
Net16	E-Material
,	O
ResNet	B-Method
-	I-Method
50	E-Method
and	O
DenseNet	B-Method
-	I-Method
161	E-Method
.	O

These	O
results	O
are	O
summarized	O
in	O
Table	O
S7	O
.	O

Next	O
,	O
in	O
Table	O
S8	O
,	O
we	O
compare	O
the	O
performance	O
of	O
Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
with	O
several	O
additional	O
regularization	B-Method
techniques	E-Method
on	O
the	O
CIFAR	B-Material
-	I-Material
10	E-Material
and	O
CIFAR	B-Material
-	I-Material
100	I-Material
datasets	E-Material
using	O
two	O
small	O
architectures	O
:	O
CIFAR	B-Material
-	I-Material
10	I-Material
Quick	E-Material
(	O
C10Quick	S-Material
)	O
and	O
CIFAR	B-Material
-	I-Material
10	I-Material
Full	E-Material
(	O
C10Full	S-Material
)	O
,	O
which	O
are	O
standard	O
models	O
available	O
in	O
the	O
Caffe	B-Method
framework	E-Method
.	O

section	O
:	O
S5	O
Changes	O
to	O
Class	B-Metric
-	I-Metric
wise	I-Metric
Prediction	I-Metric
Accuracy	E-Metric
We	O
find	O
that	O
while	O
the	O
average	O
and	O
lowest	O
per	O
-	O
class	O
accuracy	S-Metric
increase	O
when	O
training	O
with	O
PC	S-Method
,	O
there	O
is	O
a	O
small	O
decline	O
in	O
top	O
-	O
performing	O
class	O
accuracy	S-Metric
(	O
See	O
Table	O
S9	O
)	O
.	O

Moreover	O
,	O
the	O
standard	B-Metric
deviation	E-Metric
in	O
per	O
-	O
class	O
accuracy	S-Metric
is	O
reduced	O
as	O
well	O
.	O

We	O
also	O
found	O
that	O
using	O
PC	S-Method
slightly	O
increased	O
false	B-Metric
positive	I-Metric
errors	E-Metric
while	O
obtaining	O
a	O
larger	O
reduction	O
in	O
false	B-Metric
negative	I-Metric
errors	E-Metric
.	O

For	O
example	O
,	O
on	O
CUB	B-Material
-	I-Material
200	I-Material
-	I-Material
2011	E-Material
with	O
ResNet	B-Method
-	I-Method
50	E-Method
,	O
the	O
average	B-Metric
false	I-Metric
positive	I-Metric
error	E-Metric
is	O
increased	O
by	O
0.06	O
%	O
,	O
but	O
Table	O
S6	O
.	O

Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
six	O
widelyused	O
fine	B-Task
-	I-Task
grained	I-Task
visual	I-Task
classification	E-Task
datasets	S-Material
(	O
A	O
-	O
F	O
)	O
.	O

Improvement	O
over	O
the	O
baseline	O
model	O
is	O
reported	O
as	O
(	O
∆	O
)	O
.	O

All	O
results	O
averaged	O
over	O
5	O
trials	O
with	O
standard	O
deviations	O
reported	O
in	O
parentheses	O
.	O

Table	O
S8	O
.	O

Image	B-Task
classification	E-Task
performance	O
and	O
train	B-Metric
-	I-Metric
val	I-Metric
gap	E-Metric
(	O
∆	O
)	O
)	O
for	O
Pairwise	B-Method
Confusion	E-Method
(	O
PC	S-Method
)	O
and	O
popular	O
regularization	B-Method
methods	E-Method
.	O

The	O
standard	O
deviation	O
across	O
trials	O
is	O
mentioned	O
in	O
parentheses	O
.	O

section	O
:	O
Method	O
the	O
average	B-Metric
false	I-Metric
negative	I-Metric
error	E-Metric
is	O
reduced	O
by	O
0.13	O
%	O
.	O

So	O
while	O
some	O
additional	O
mistakes	O
are	O
made	O
in	O
terms	O
of	O
false	B-Metric
positives	E-Metric
,	O
we	O
curb	O
/	O
reduce	O
the	O
problem	O
of	O
classifier	B-Task
overconfidence	E-Task
by	O
a	O
larger	O
margin	O
.	O

section	O
:	O
section	O
:	O
Acknowledgements	O
:	O
We	O
would	O
like	O
to	O
thank	O
Dr.	O
Ashok	O
Gupta	O
for	O
his	O
guidance	O
on	O
bird	B-Task
recognition	E-Task
,	O
and	O
Dr.	O
Sumeet	O
Agarwal	O
,	O
Spandan	O
Madan	O
and	O
Ishaan	O
Grover	O
for	O
their	O
feedback	O
at	O
various	O
stages	O
of	O
this	O
work	O
.	O

section	O
:	O
ReasoNet	S-Method
:	O
Learning	O
to	O
Stop	B-Task
Reading	E-Task
in	O
Machine	B-Task
Comprehension	E-Task
section	O
:	O
ABSTRACT	O
Teaching	O
a	O
computer	O
to	O
read	O
and	O
answer	O
general	O
questions	O
pertaining	O
to	O
a	O
document	O
is	O
a	O
challenging	O
yet	O
unsolved	O
problem	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
novel	O
neural	B-Method
network	I-Method
architecture	E-Method
called	O
the	O
Reasoning	B-Method
Network	E-Method
(	O
ReasoNet	S-Method
)	O
for	O
machine	B-Task
comprehension	I-Task
tasks	E-Task
.	O

ReasoNets	S-Method
make	O
use	O
of	O
multiple	O
turns	O
to	O
e	O
ectively	O
exploit	O
and	O
then	O
reason	O
over	O
the	O
relation	O
among	O
queries	O
,	O
documents	O
,	O
and	O
answers	O
.	O

Di	O
erent	O
from	O
previous	O
approaches	O
using	O
a	O
xed	O
number	O
of	O
turns	O
during	O
inference	S-Task
,	O
ReasoNets	S-Method
introduce	O
a	O
termination	O
state	O
to	O
relax	O
this	O
constraint	O
on	O
the	O
reasoning	O
depth	O
.	O

With	O
the	O
use	O
of	O
reinforcement	B-Task
learning	E-Task
,	O
ReasoNets	S-Method
can	O
dynamically	O
determine	O
whether	O
to	O
continue	O
the	O
comprehension	B-Task
process	E-Task
after	O
digesting	O
intermediate	O
results	O
,	O
or	O
to	O
terminate	O
reading	O
when	O
it	O
concludes	O
that	O
existing	O
information	O
is	O
adequate	O
to	O
produce	O
an	O
answer	O
.	O

ReasoNets	S-Method
achieve	O
superior	O
performance	O
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	B-Material
CNN	I-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
the	O
Stanford	B-Material
SQuAD	I-Material
dataset	E-Material
,	O
and	O
a	O
structured	B-Material
Graph	I-Material
Reachability	I-Material
dataset	E-Material
.	O

section	O
:	O
INTRODUCTION	O
Teaching	B-Task
machines	E-Task
to	O
read	O
,	O
process	O
,	O
and	O
comprehend	O
natural	O
language	O
documents	O
is	O
a	O
coveted	O
goal	O
for	O
arti	B-Task
cial	I-Task
intelligence	E-Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Genuine	O
reading	B-Task
comprehension	E-Task
is	O
extremely	O
challenging	O
,	O
since	O
e	O
ective	B-Task
comprehension	E-Task
involves	O
thorough	O
understanding	B-Task
of	I-Task
documents	E-Task
and	O
sophisticated	O
inference	S-Task
.	O

Toward	O
solving	O
this	O
machine	B-Task
reading	I-Task
comprehension	I-Task
problem	E-Task
,	O
in	O
recent	O
years	O
,	O
several	O
works	O
have	O
collected	O
various	O
datasets	O
,	O
in	O
the	O
form	O
of	O
question	O
,	O
passage	O
,	O
and	O
answer	O
,	O
to	O
test	O
machine	O
on	O
answering	O
a	O
question	O
based	O
on	O
the	O
provided	O
passage	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Some	O
large	O
-	O
scale	O
cloze	O
-	O
style	O
datasets	O
[	O
reference	O
][	O
reference	O
]	O
have	O
gained	O
signi	O
ca	O
nt	O
attention	O
along	O
with	O
powerful	O
deep	B-Method
learning	I-Method
models	E-Method
.	O

Recent	O
approaches	O
on	O
cloze	O
-	O
style	O
datasets	O
can	O
be	O
separated	O
into	O
two	O
categories	O
:	O
single	O
-	O
turn	O
and	O
multi	B-Task
-	I-Task
turn	I-Task
reasoning	E-Task
.	O

Single	B-Method
turn	I-Method
reasoning	I-Method
models	E-Method
utilize	O
attention	B-Method
mechanisms	E-Method
[	O
reference	O
]	O
to	O
emphasize	O
speci	O
c	O
parts	O
of	O
the	O
document	O
which	O
are	O
relevant	O
to	O
the	O
query	O
.	O

Permission	O
to	O
make	O
digital	O
or	O
hard	O
copies	O
of	O
all	O
or	O
part	O
of	O
this	O
work	O
for	O
personal	O
or	O
classroom	O
use	O
is	O
granted	O
without	O
fee	O
provided	O
that	O
copies	O
are	O
not	O
made	O
or	O
distributed	O
for	O
pro	O
t	O
or	O
commercial	O
advantage	O
and	O
that	O
copies	O
bear	O
this	O
notice	O
and	O
the	O
full	O
citation	O
on	O
the	O
rst	O
page	O
.	O

Copyrights	O
for	O
components	O
of	O
this	O
work	O
owned	O
by	O
others	O
than	O
ACM	O
must	O
be	O
honored	O
.	O

Abstracting	O
with	O
credit	O
is	O
permitted	O
.	O

To	O
copy	O
otherwise	O
,	O
or	O
republish	O
,	O
to	O
post	O
on	O
servers	O
or	O
to	O
redistribute	O
to	O
lists	O
,	O
requires	O
prior	O
speci	O
c	O
permission	O
and	O
/	O
or	O
a	O
fee	O
.	O

Request	O
permissions	O
from	O
permissions@acm.org	O
.	O

These	O
attention	B-Method
models	E-Method
subsequently	O
calculate	O
the	O
relevance	O
between	O
a	O
query	O
and	O
the	O
corresponding	O
weighted	O
representations	O
of	O
document	O
subunits	O
(	O
e.g.	O
sentences	O
or	O
words	O
)	O
to	O
score	O
target	O
candidates	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
considering	O
the	O
sophistication	O
of	O
the	O
problem	O
,	O
after	O
a	O
single	B-Task
-	I-Task
turn	I-Task
comprehension	E-Task
,	O
readers	O
often	O
revisit	O
some	O
speci	O
c	O
passage	O
or	O
the	O
question	O
to	O
grasp	O
a	O
better	O
understanding	O
of	O
the	O
problem	O
.	O

With	O
this	O
motivation	O
,	O
recent	O
advances	O
in	O
reading	B-Task
comprehension	E-Task
have	O
made	O
use	O
of	O
multiple	O
turns	O
to	O
infer	O
the	O
relation	O
between	O
query	O
,	O
document	O
and	O
answer	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

By	O
repeatedly	O
processing	O
the	O
document	O
and	O
the	O
question	O
after	O
digesting	O
intermediate	O
information	O
,	O
multi	B-Method
-	I-Method
turn	I-Method
reasoning	E-Method
can	O
generally	O
produce	O
a	O
better	O
answer	O
and	O
these	O
existing	O
works	O
have	O
demonstrated	O
its	O
superior	O
performance	O
consistently	O
.	O

Existing	O
multi	B-Method
-	I-Method
turn	I-Method
models	E-Method
have	O
a	O
pre	O
-	O
de	O
ned	O
number	O
of	O
hops	O
or	O
iterations	O
in	O
their	O
inference	S-Task
without	O
regard	O
to	O
the	O
complexity	O
of	O
each	O
individual	O
query	O
or	O
document	O
.	O

However	O
,	O
when	O
human	O
read	O
a	O
document	O
with	O
a	O
question	O
in	O
mind	O
,	O
we	O
often	O
decide	O
whether	O
we	O
want	O
to	O
stop	O
reading	O
if	O
we	O
believe	O
the	O
observed	O
information	O
is	O
adequate	O
already	O
to	O
answer	O
the	O
question	O
,	O
or	O
continue	O
reading	O
after	O
digesting	O
intermediate	O
information	O
until	O
we	O
can	O
answer	O
the	O
question	O
with	O
con	O
dence	O
.	O

This	O
behavior	O
generally	O
varies	O
from	O
document	O
to	O
document	O
or	O
question	O
to	O
question	O
because	O
it	O
is	O
related	O
to	O
the	O
sophistication	O
of	O
the	O
document	O
or	O
the	O
di	O
culty	O
of	O
the	O
question	O
.	O

Meanwhile	O
,	O
the	O
analysis	O
in	O
[	O
reference	O
]	O
also	O
illustrates	O
the	O
huge	O
variations	O
in	O
the	O
di	O
culty	O
level	O
with	O
respect	O
to	O
questions	O
in	O
the	O
CNN	B-Material
/	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
[	O
reference	O
]	O
.	O

For	O
a	O
signi	O
ca	O
nt	O
part	O
of	O
the	O
datasets	O
,	O
this	O
analysis	O
shows	O
that	O
the	O
problem	O
can	O
not	O
be	O
solved	O
without	O
appropriate	O
reasoning	O
on	O
both	O
its	O
query	O
and	O
document	O
.	O

With	O
this	O
motivation	O
,	O
we	O
propose	O
a	O
novel	O
neural	B-Method
network	I-Method
architecture	E-Method
called	O
Reasoning	B-Method
Network	E-Method
(	O
ReasoNet	S-Method
)	O
.	O

which	O
tries	O
to	O
mimic	O
the	O
inference	B-Task
process	E-Task
of	O
human	B-Task
readers	E-Task
.	O

With	O
a	O
question	O
in	O
mind	O
,	O
ReasoNets	O
read	O
a	O
document	O
repeatedly	O
,	O
each	O
time	O
focusing	O
on	O
di	O
erent	O
parts	O
of	O
the	O
document	O
until	O
a	O
satisfying	O
answer	O
is	O
found	O
or	O
formed	O
.	O

This	O
reminds	O
us	O
of	O
a	O
Chinese	O
proverb	O
:	O
"	O
The	O
meaning	O
of	O
a	O
book	O
will	O
become	O
clear	O
if	O
you	O
read	O
it	O
hundreds	O
of	O
times	O
.	O

"	O
.	O

Moreover	O
,	O
unlike	O
previous	O
approaches	O
using	O
xed	O
number	O
of	O
hops	O
or	O
iterations	O
,	O
ReasoNets	S-Method
introduce	O
a	O
termination	O
state	O
in	O
the	O
inference	S-Task
.	O

This	O
state	O
can	O
decide	O
whether	O
to	O
continue	O
the	O
inference	O
to	O
the	O
next	O
turn	O
after	O
digesting	O
intermediate	O
information	O
,	O
or	O
to	O
terminate	O
the	O
whole	O
inference	O
when	O
it	O
concludes	O
that	O
existing	O
information	O
is	O
sufcient	O
to	O
yield	O
an	O
answer	O
.	O

The	O
number	O
of	O
turns	O
in	O
the	O
inference	S-Task
is	O
dynamically	O
modeled	O
by	O
both	O
the	O
document	O
and	O
the	O
query	O
,	O
and	O
can	O
be	O
learned	O
automatically	O
according	O
to	O
the	O
di	O
culty	O
of	O
the	O
problem	O
.	O

One	O
of	O
the	O
signi	O
ca	O
nt	O
challenges	O
ReasoNets	O
face	O
is	O
how	O
to	O
design	O
an	O
e	O
cient	O
training	B-Method
method	E-Method
,	O
since	O
the	O
termination	O
state	O
is	O
discrete	O
and	O
not	O
connected	O
to	O
the	O
nal	O
output	O
.	O

This	O
prohibits	O
canonical	B-Method
back	I-Method
-	I-Method
propagation	I-Method
method	E-Method
being	O
directly	O
applied	O
to	O
train	O
ReasoNets	S-Method
.	O

Motivated	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
tackle	O
this	O
challenge	O
by	O
proposing	O
a	O
reinforcement	B-Task
learning	E-Task
approach	O
,	O
which	O
utilizes	O
an	O
instance	B-Method
-	I-Method
dependent	I-Method
reward	I-Method
baseline	E-Method
,	O
to	O
successfully	O
train	O
ReasoNets	S-Method
.	O

Finally	O
,	O
by	O
accounting	O
for	O
a	O
dynamic	O
termination	O
state	O
during	O
inference	S-Task
and	O
applying	O
proposed	O
deep	O
reinforcement	B-Task
learning	E-Task
optimization	O
method	O
,	O
ReasoNets	S-Method
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
machine	B-Material
comprehension	I-Material
datasets	E-Material
,	O
including	O
unstructured	B-Material
CNN	I-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
and	O
the	O
proposed	O
structured	B-Material
Graph	I-Material
Reachability	I-Material
dataset	E-Material
,	O
when	O
the	O
paper	O
is	O
rst	O
publicly	O
available	O
on	O
arXiv	O
.	O

[	O
reference	O
]	O
At	O
the	O
time	O
of	O
the	O
paper	O
submission	O
,	O
we	O
apply	O
ReasoNet	S-Method
to	O
the	O
competitive	O
Stanford	B-Material
Question	I-Material
Answering	I-Material
Dataset	E-Material
(	O
SQuAD	S-Material
)	O
,	O
ReasoNets	S-Method
outperform	O
all	O
existing	O
published	O
approaches	O
and	O
rank	O
at	O
second	O
place	O
on	O
the	O
test	O
set	O
leaderboard	O
.	O

[	O
reference	O
]	O
This	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

In	O
Section	O
2	O
,	O
we	O
review	O
and	O
compare	O
recent	O
work	O
on	O
machine	B-Task
reading	I-Task
comprehension	I-Task
tasks	E-Task
.	O

In	O
Section	O
3	O
,	O
we	O
introduce	O
our	O
proposed	O
ReasoNet	S-Method
model	O
architecture	O
and	O
training	O
objectives	O
.	O

Section	O
4	O
presents	O
the	O
experimental	O
setting	O
and	O
results	O
on	O
unstructured	B-Task
and	I-Task
structured	I-Task
machine	I-Task
reading	I-Task
comprehension	I-Task
tasks	E-Task
.	O

section	O
:	O
RELATED	O
WORK	O
Recently	O
,	O
with	O
large	O
-	O
scale	O
datasets	O
available	O
and	O
the	O
impressive	O
advance	O
of	O
various	O
statistical	B-Method
models	E-Method
,	O
machine	B-Task
reading	I-Task
comprehension	I-Task
tasks	E-Task
have	O
attracted	O
much	O
attention	O
.	O

Here	O
we	O
mainly	O
focus	O
on	O
the	O
related	O
work	O
in	O
cloze	O
-	O
style	O
datasets	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Based	O
on	O
how	O
they	O
perform	O
the	O
inference	S-Task
,	O
we	O
can	O
classify	O
their	O
models	O
into	O
two	O
categories	O
:	O
single	O
-	O
turn	O
and	O
multi	B-Task
-	I-Task
turn	I-Task
reasoning	E-Task
.	O

Single	B-Method
-	I-Method
turn	I-Method
reasoning	E-Method
:	O
Single	B-Method
turn	I-Method
reasoning	I-Method
models	E-Method
utilize	O
an	O
attention	B-Method
mechanism	E-Method
to	O
emphasize	O
some	O
sections	O
of	O
a	O
document	O
which	O
are	O
relevant	O
to	O
a	O
query	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
treating	O
some	O
parts	O
unimportant	O
while	O
focusing	O
on	O
other	O
important	O
ones	O
to	O
nd	O
the	O
most	O
probable	O
answer	O
.	O

Hermann	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
attentive	B-Method
reader	E-Method
and	O
the	O
impatient	B-Method
reader	I-Method
models	E-Method
using	O
neural	B-Method
networks	E-Method
with	O
an	O
attention	O
over	O
passages	O
to	O
predict	O
candidates	O
.	O

Hill	O
et	O
al	O
.	O

[	O
reference	O
]	O
use	O
attention	S-Method
over	O
window	B-Method
-	I-Method
based	I-Method
memory	E-Method
,	O
which	O
encodes	O
a	O
window	O
of	O
words	O
around	O
entity	O
candidates	O
,	O
by	O
leveraging	O
an	O
endto	B-Method
-	I-Method
end	I-Method
memory	I-Method
network	E-Method
[	O
reference	O
]	O
.	O

Meanwhile	O
,	O
given	O
the	O
same	O
entity	O
candidate	O
can	O
appear	O
multiple	O
times	O
in	O
a	O
passage	O
,	O
Kadlec	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
attention	B-Method
-	I-Method
sum	I-Method
reader	E-Method
to	O
sum	O
up	O
all	O
the	O
attention	O
scores	O
for	O
the	O
same	O
entity	O
.	O

This	O
score	O
captures	O
the	O
relevance	O
between	O
a	O
query	O
and	O
a	O
candidate	O
.	O

Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
using	O
a	O
bilinear	B-Method
term	I-Method
similarity	I-Method
function	E-Method
to	O
calculate	O
attention	O
scores	O
with	O
pretrained	O
word	O
embeddings	O
.	O

Trischler	O
et	O
al	O
.	O

[	O
reference	O
]	O
propose	O
the	O
EpiReader	S-Method
which	O
uses	O
two	O
neural	B-Method
network	I-Method
structures	E-Method
:	O
one	O
extracts	O
candidates	O
using	O
the	O
attention	B-Method
-	I-Method
sum	I-Method
reader	E-Method
;	O
the	O
other	O
reranks	O
candidates	O
based	O
on	O
a	O
bilinear	B-Metric
term	I-Metric
similarity	I-Metric
score	E-Metric
calculated	O
from	O
query	B-Method
and	I-Method
passage	I-Method
representations	E-Method
.	O

Multi	B-Task
-	I-Task
turn	I-Task
reasoning	E-Task
:	O
For	O
complex	O
passages	O
and	O
complex	O
queries	O
,	O
human	O
readers	O
often	O
revisit	O
the	O
given	O
document	O
in	O
order	O
to	O
perform	O
deeper	B-Task
inference	E-Task
after	O
reading	O
a	O
document	O
.	O

Several	O
recent	O
studies	O
try	O
to	O
simulate	O
this	O
revisit	O
by	O
combining	O
the	O
information	O
in	O
the	O
query	O
with	O
the	O
new	O
information	O
digested	O
from	O
previous	O
iterations	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Hill	O
et	O
al	O
.	O

[	O
reference	O
]	O
use	O
multiple	B-Method
hops	I-Method
memory	I-Method
network	E-Method
to	O
augment	O
the	O
query	O
with	O
new	O
information	O
from	O
the	O
previous	O
hop	O
.	O

Gated	B-Method
Attention	I-Method
reader	E-Method
[	O
reference	O
]	O
is	O
an	O
extension	O
of	O
the	O
attention	B-Method
-	I-Method
sum	I-Method
reader	E-Method
with	O
multiple	O
iterations	O
by	O
pushing	O
the	O
query	B-Method
encoding	E-Method
into	O
an	O
attention	B-Method
-	I-Method
based	I-Method
gate	E-Method
in	O
each	O
iteration	O
.	O

Iterative	B-Method
Alternative	E-Method
(	O
IA	S-Method
)	O
reader	O
[	O
reference	O
]	O
produces	O
a	O
new	O
query	O
glimpse	O
and	O
document	O
glimpse	O
in	O
each	O
iteration	O
and	O
utilizes	O
them	O
alternatively	O
in	O
the	O
next	O
iteration	O
.	O

Cui	O
et	O
al	O
.	O

[	O
reference	O
]	O
further	O
propose	O
to	O
extend	O
the	O
query	O
-	O
speci	O
c	O
attention	O
to	O
both	O
query	B-Task
-	I-Task
to	I-Task
-	I-Task
document	I-Task
attention	E-Task
and	O
document	B-Task
-	I-Task
to	I-Task
-	I-Task
query	I-Task
attention	E-Task
,	O
which	O
is	O
built	O
from	O
the	O
intermediate	O
results	O
in	O
the	O
query	O
-	O
speci	O
c	O
attention	O
.	O

By	O
reading	O
documents	O
and	O
enriching	O
the	O
query	O
in	O
an	O
iterative	O
fashion	O
,	O
multi	B-Method
-	I-Method
turn	I-Method
reasoning	E-Method
has	O
demonstrated	O
their	O
superior	O
performance	O
consistently	O
.	O

Our	O
proposed	O
approach	O
explores	O
the	O
idea	O
of	O
using	O
both	O
attentionsum	O
to	O
aggregate	O
candidate	O
attention	O
scores	O
and	O
multiple	O
turns	O
to	O
attain	O
a	O
better	O
reasoning	O
capability	O
.	O

Unlike	O
previous	O
approaches	O
using	O
a	O
xed	O
number	O
of	O
hops	O
or	O
iterations	O
,	O
motivated	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
propose	O
a	O
termination	B-Method
module	E-Method
in	O
the	O
inference	S-Task
.	O

The	O
termination	B-Method
module	E-Method
can	O
decide	O
whether	O
to	O
continue	O
to	O
infer	O
the	O
next	O
turn	O
after	O
digesting	O
intermediate	O
information	O
,	O
or	O
to	O
terminate	O
the	O
whole	O
inference	B-Task
process	E-Task
when	O
it	O
concludes	O
existing	O
information	O
is	O
su	O
cient	O
to	O
yield	O
an	O
answer	O
.	O

The	O
number	O
of	O
turns	O
in	O
the	O
inference	S-Task
is	O
dynamically	O
modeled	O
by	O
both	O
a	O
document	O
and	O
a	O
query	O
,	O
and	O
is	O
generally	O
related	O
to	O
the	O
complexity	O
of	O
the	O
document	O
and	O
the	O
query	O
.	O

section	O
:	O
REASONING	B-Task
NETWORKS	E-Task
ReasoNets	S-Method
are	O
devised	O
to	O
mimic	O
the	O
inference	B-Task
process	I-Task
of	I-Task
human	I-Task
readers	E-Task
.	O

ReasoNets	O
read	O
a	O
document	O
repeatedly	O
with	O
attention	O
on	O
di	O
erent	O
parts	O
each	O
time	O
until	O
a	O
satisfying	O
answer	O
is	O
found	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
a	O
ReasoNet	S-Method
is	O
composed	O
of	O
the	O
following	O
components	O
:	O
Memory	S-Method
:	O
The	O
external	O
memory	O
is	O
denoted	O
as	O
M.	O
It	O
is	O
a	O
list	O
of	O
word	O
vectors	O
,	O
M	O
=	O
{	O
m	O
i	O
}	O
i=1	O
..	O
D	O
,	O
where	O
m	O
i	O
is	O
a	O
xed	O
dimensional	O
vector	O
.	O

For	O
example	O
,	O
in	O
the	O
Graph	B-Task
Reachability	E-Task
,	O
m	O
i	O
is	O
the	O
vector	B-Method
representation	E-Method
of	O
each	O
word	O
in	O
the	O
graph	O
description	O
encoded	O
by	O
a	O
bidirectional	O
-	O
RNN	S-Method
.	O

Please	O
refer	O
to	O
Section	O
4	O
for	O
the	O
detailed	O
setup	O
in	O
each	O
experiment	O
.	O

Attention	S-Method
:	O
The	O
attention	O
vector	O
x	O
t	O
is	O
generated	O
based	O
on	O
the	O
current	O
internal	O
state	O
s	O
t	O
and	O
the	O
external	O
memory	O
M	O
:	O
x	O
t	O
=	O
f	O
at	O
t	O
(	O
s	O
t	O
,	O
M	O
;	O
θ	O
x	O
)	O
.	O

Please	O
refer	O
to	O
Section	O
4	O
for	O
the	O
detailed	O
setup	O
in	O
each	O
experiment	O
.	O

Internal	O
State	O
:	O
The	O
internal	O
state	O
is	O
denoted	O
as	O
s	O
which	O
is	O
a	O
vector	B-Method
representation	I-Method
of	I-Method
the	I-Method
question	I-Method
state	E-Method
.	O

Typically	O
,	O
the	O
initial	O
state	O
s	O
1	O
is	O
the	O
last	B-Method
-	I-Method
word	I-Method
vector	I-Method
representation	E-Method
of	O
query	O
by	O
an	O
RNN	S-Method
.	O

The	O
t	O
-	O
th	O
time	O
step	O
of	O
the	O
internal	O
state	O
is	O
represented	O
by	O
s	O
t	O
.	O

The	O
sequence	O
of	O
internal	O
states	O
are	O
modeled	O
by	O
an	O
RNN	S-Method
:	O
s	O
t	O
+	O
1	O
=	O
RNN	S-Method
(	O
s	O
t	O
,	O
x	O
t	O
;	O
θ	O
s	O
)	O
,	O
where	O
x	O
t	O
is	O
the	O
attention	O
vector	O
mentioned	O
above	O
.	O

Termination	O
Gate	O
:	O
The	O
termination	O
gate	O
generates	O
a	O
random	O
variable	O
according	O
to	O
the	O
current	O
internal	O
state	O
;	O
t	O
t	O
∼	O
p	O
(	O
·|	O
f	O
t	O
(	O
s	O
t	O
;	O
θ	O
t	O
)	O
)	O
)	O
.	O

t	O
t	O
is	O
a	O
binary	O
random	O
variable	O
.	O

If	O
t	O
t	O
is	O
true	O
,	O
the	O
ReasoNet	S-Method
stops	O
,	O
and	O
the	O
answer	B-Method
module	E-Method
executes	O
at	O
time	O
step	O
t	O
;	O
otherwise	O
the	O
ReasoNet	S-Method
generates	O
an	O
attention	O
vector	O
x	O
t	O
+	O
1	O
,	O
and	O
feeds	O
the	O
vector	O
into	O
the	O
state	B-Method
network	E-Method
to	O
update	O
the	O
next	O
internal	O
state	O
s	O
t	O
+	O
1	O
.	O

Answer	O
:	O
The	O
action	O
of	O
answer	B-Method
module	E-Method
is	O
triggered	O
when	O
the	O
termination	O
gate	O
variable	O
is	O
true	O
:	O
a	O
t	O
∼	O
p	O
(	O
·|	O
f	O
a	O
(	O
s	O
t	O
;	O
θ	O
a	O
)	O
)	O
.	O

In	O
Algorithm	O
1	O
,	O
we	O
describe	O
the	O
stochastic	B-Method
inference	I-Method
process	E-Method
of	O
a	O
ReasoNet	S-Method
.	O

The	O
process	O
can	O
be	O
considered	O
as	O
solving	O
a	O
Partially	B-Task
Observable	I-Task
Markov	I-Task
Decision	I-Task
Process	E-Task
(	O
POMDP	S-Task
)	O
[	O
reference	O
]	O
in	O
the	O
reinforcement	B-Task
learning	E-Task
(	O
RL	S-Task
)	O
literature	O
.	O

The	O
state	O
sequence	O
s	O
1:T	O
is	O
hidden	O
Step	O
t	O
=	O
1	O
;	O
Maximum	O
Step	O
T	O
max	O
Output	O
:	O
Termination	O
Step	O
T	O
,	O
Answer	O
a	O
T	O
1	O
Sample	O
t	O
t	O
from	O
the	O
distribution	O
p	O
(	O
·|	O
f	O
t	O
(	O
s	O
t	O
;	O
θ	O
t	O
)	O
)	O
;	O
2	O
if	O
t	O
t	O
is	O
false	O
,	O
go	O
to	O
Step	O
3	O
;	O
otherwise	O
Step	O
6	O
;	O
3	O
Generate	O
attention	O
vector	O
x	O
t	O
=	O
f	O
at	O
t	O
(	O
s	O
t	O
,	O
M	O
;	O
θ	O
x	O
)	O
;	O
4	O
Update	O
internal	O
state	O
s	O
t	O
+	O
1	O
=	O
RNN	S-Method
(	O
s	O
t	O
,	O
x	O
t	O
;	O
θ	O
s	O
)	O
;	O
5	O
Set	O
t	O
=	O
t	O
+	O
1	O
;	O
if	O
t	O
<	O
T	O
max	O
go	O
to	O
Step	O
1	O
;	O
otherwise	O
Step	O
6	O
;	O
6	O
Generate	O
answer	O
a	O
t	O
∼	O
p	O
(	O
·|	O
f	O
a	O
(	O
s	O
t	O
;	O
θ	O
a	O
)	O
)	O
;	O
7	O
Return	O
T	O
=	O
t	O
and	O
a	O
T	O
=	O
a	O
t	O
;	O
and	O
dynamic	O
,	O
controlled	O
by	O
an	O
RNN	S-Method
sequence	O
model	O
.	O

The	O
ReasoNet	S-Method
performs	O
an	O
answer	O
action	O
a	O
T	O
at	O
the	O
T	O
-	O
th	O
step	O
,	O
which	O
implies	O
that	O
the	O
termination	O
gate	O
variables	O
t	O
1:T	O
=	O
(	O
t	O
1	O
=	O
0	O
,	O
t	O
2	O
=	O
0	O
,	O
...	O
,	O
t	O
T	O
−1	O
=	O
0	O
,	O
t	O
T	O
=	O
1	O
)	O
.	O

The	O
ReasoNet	S-Method
learns	O
a	O
stochastic	B-Method
policy	I-Method
π	E-Method
(	O
(	O
t	O
t	O
,	O
a	O
t	O
)	O
|s	O
t	O
;	O
θ	O
)	O
with	O
parameters	O
θ	O
to	O
get	O
a	O
distribution	O
of	O
termination	O
actions	O
,	O
to	O
continue	O
reading	O
or	O
to	O
stop	O
,	O
and	O
of	O
answer	O
actions	O
if	O
the	O
model	O
decides	O
to	O
stop	O
at	O
the	O
current	O
step	O
.	O

The	O
termination	O
step	O
T	O
varies	O
from	O
instance	O
to	O
instance	O
.	O

The	O
learnable	O
parameters	O
θ	O
of	O
the	O
ReasoNet	S-Method
are	O
the	O
embedding	O
matrices	O
θ	O
W	O
,	O
attention	B-Method
network	E-Method
θ	O
x	O
,	O
the	O
state	O
RNN	S-Method
network	O
θ	O
s	O
,	O
the	O
answer	B-Method
action	I-Method
network	E-Method
θ	O
a	O
,	O
and	O
the	O
termination	B-Method
gate	I-Method
network	E-Method
θ	O
t	O
.	O

The	O
parameters	O
θ	O
=	O
{	O
θ	O
W	O
,	O
θ	O
x	O
,	O
θ	O
s	O
,	O
θ	O
a	O
,	O
θ	O
t	O
}	O
are	O
trained	O
by	O
maximizing	O
the	O
total	B-Metric
expect	I-Metric
reward	E-Metric
.	O

The	O
expected	B-Metric
reward	E-Metric
for	O
an	O
instance	O
is	O
de	O
ned	O
as	O
:	O
The	O
reward	O
can	O
only	O
be	O
received	O
at	O
the	O
nal	O
termination	O
step	O
when	O
an	O
answer	O
action	O
a	O
T	O
is	O
performed	O
.	O

We	O
de	O
ne	O
r	O
T	O
=	O
1	O
if	O
t	O
T	O
=	O
1	O
and	O
the	O
answer	O
is	O
correct	O
,	O
and	O
r	O
T	O
=	O
0	O
otherwise	O
.	O

The	O
rewards	O
on	O
intermediate	O
steps	O
are	O
zeros	O
,	O
{	O
r	O
t	O
=	O
0	O
}	O
t	O
=	O
1	O
...	O
T	O
−1	O
.	O

can	O
be	O
maximized	O
by	O
directly	O
applying	O
gradient	B-Method
based	I-Method
optimization	I-Method
methods	E-Method
.	O

The	O
gradient	O
of	O
is	O
given	O
by	O
:	O
Motivated	O
by	O
the	O
REINFORCE	B-Method
algorithm	E-Method
[	O
reference	O
]	O
,	O
we	O
compute	O
∇	O
θ	O
(	O
θ	O
)	O
:	O
where	O
A	O
†	O
is	O
all	O
the	O
possible	O
episodes	O
,	O
T	O
,	O
t	O
1:T	O
,	O
a	O
T	O
and	O
r	O
T	O
are	O
the	O
termination	O
step	O
,	O
termination	O
action	O
,	O
answer	O
action	O
,	O
and	O
reward	O
,	O
respectively	O
,	O
for	O
the	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
episode	O
.	O

b	O
T	O
is	O
called	O
the	O
reward	O
baseline	O
in	O
the	O
RL	S-Task
literature	O
to	O
lower	O
the	O
variance	O
[	O
reference	O
]	O
.	O

It	O
is	O
common	O
to	O
,	O
and	O
can	O
be	O
updated	O
via	O
an	O
online	B-Method
moving	I-Method
average	I-Method
approach	E-Method
:	O
b	O
T	O
=	O
λb	O
T	O
+	O
(	O
1	O
−	O
λ	O
)	O
r	O
T	O
.	O

However	O
,	O
we	O
empirically	O
nd	O
that	O
the	O
above	O
approach	O
leads	O
to	O
slow	O
convergence	O
in	O
training	B-Task
ReasoNets	E-Task
.	O

Intuitively	O
,	O
the	O
average	O
baselines	O
{	O
b	O
T	O
;	O
T	O
=	O
1	O
..	O
T	O
max	O
}	O
are	O
global	O
variables	O
independent	O
of	O
instances	O
.	O

It	O
is	O
hard	O
for	O
these	O
baselines	O
to	O
capture	O
the	O
dynamic	O
termination	O
behavior	O
of	O
ReasoNets	O
.	O

Since	O
ReasoNets	O
may	O
stop	O
at	O
di	O
erent	O
time	O
steps	O
for	O
di	O
erent	O
instances	O
,	O
the	O
adoption	O
of	O
a	O
global	O
variable	O
without	O
considering	O
the	O
dynamic	O
variance	O
in	O
each	O
instance	O
is	O
inappropriate	O
.	O

To	O
resolve	O
this	O
weakness	O
in	O
traditional	O
methods	O
and	O
account	O
for	O
the	O
dynamic	O
characteristic	O
of	O
ReasoNets	O
,	O
we	O
propose	O
an	O
instance	B-Method
-	I-Method
dependent	I-Method
baseline	I-Method
method	E-Method
to	O
calculate	O
∇	O
θ	O
(	O
θ	O
)	O
,	O
as	O
illustrated	O
in	O
Section	O
3.1	O
.	O

Empirical	O
results	O
show	O
that	O
the	O
proposed	O
reward	B-Method
schema	E-Method
achieves	O
better	O
results	O
compared	O
to	O
baseline	O
approaches	O
.	O

section	O
:	O
Training	O
Details	O
In	O
the	O
machine	B-Task
reading	I-Task
comprehension	I-Task
tasks	E-Task
,	O
a	O
training	O
dataset	O
is	O
a	O
collection	O
of	O
triplets	O
of	O
query	O
q	O
,	O
passage	O
p	O
,	O
and	O
answer	O
a.	O
Say	O
q	O
n	O
,	O
p	O
n	O
,	O
a	O
n	O
is	O
the	O
n	O
-	O
th	O
training	O
instance	O
.	O

The	O
rst	O
step	O
is	O
to	O
extract	O
memory	O
M	O
from	O
p	O
n	O
by	O
mapping	O
each	O
symbolic	O
in	O
the	O
passage	O
to	O
a	O
contextual	B-Method
representation	E-Method
given	O
by	O
the	O
concatenation	O
of	O
forward	O
and	O
backward	O
RNN	S-Method
hidden	O
states	O
,	O
i.e.	O
,	O
,	O
and	O
extract	O
initial	O
state	O
s	O
1	O
from	O
q	O
n	O
by	O
assigning	O
.	O

Given	O
M	O
and	O
s	O
1	O
for	O
the	O
n	O
-	O
th	O
training	O
instance	O
,	O
a	O
ReasoNet	S-Method
executes	O
|A	O
†	O
|	O
episodes	O
,	O
where	O
all	O
possible	O
episodes	O
A	O
†	O
can	O
be	O
enumerated	O
by	O
setting	O
a	O
maximum	O
step	O
.	O

Each	O
episode	O
generates	O
actions	O
and	O
a	O
reward	O
from	O
the	O
last	O
step	O
:	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
,	O
r	O
T	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
∈A	O
†	O
.	O

Therefore	O
,	O
the	O
gradient	O
of	O
can	O
be	O
rewritten	O
as	O
:	O
where	O
the	O
baseline	O
b	O
=	O
(	O
t	O
1:T	O
,	O
a	O
T	O
)	O
∈A	O
†	O
π	O
(	O
t	O
1:T	O
,	O
a	O
T	O
;	O
θ	O
)	O
r	O
T	O
is	O
the	O
average	O
reward	O
on	O
the	O
|A	O
†	O
|	O
episodes	O
for	O
the	O
n	O
-	O
th	O
training	O
instance	O
.	O

It	O
allows	O
di	O
erent	O
baselines	O
for	O
di	O
erent	O
training	O
instances	O
.	O

This	O
can	O
be	O
benecial	O
since	O
the	O
complexity	O
of	O
training	O
instances	O
varies	O
signi	O
cantly	O
.	O

In	O
experiments	O
,	O
we	O
empirically	O
nd	O
using	O
(	O
r	O
T	O
b	O
−	O
1	O
)	O
in	O
replace	O
of	O
(	O
r	O
T	O
−	O
b	O
)	O
can	O
lead	O
to	O
a	O
faster	O
convergence	S-Metric
.	O

Therefore	O
,	O
we	O
adopt	O
this	O
approach	O
to	O
train	O
ReasoNets	S-Task
in	O
the	O
experiments	O
.	O

section	O
:	O
EXPERIMENTS	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
ReasoNets	S-Method
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	B-Material
CNN	I-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
the	O
Stanford	B-Material
SQuAD	I-Material
dataset	E-Material
,	O
and	O
a	O
structured	B-Material
Graph	I-Material
Reachability	I-Material
dataset	E-Material
.	O

section	O
:	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
Datasets	E-Material
We	O
examine	O
the	O
performance	O
of	O
ReasoNets	S-Method
on	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
.	O

[	O
reference	O
]	O
The	O
detailed	O
settings	O
of	O
the	O
ReasoNet	S-Method
model	O
are	O
as	O
follows	O
.	O

Vocab	B-Metric
Size	E-Metric
:	O
For	O
training	O
our	O
ReasoNet	S-Method
,	O
we	O
keep	O
the	O
most	O
frequent	O
|V	O
|	O
=	O
101k	O
words	O
(	O
not	O
including	O
584	O
entities	O
and	O
1	O
placeholder	O
marker	O
)	O
in	O
the	O
CNN	B-Material
dataset	E-Material
,	O
and	O
|V	O
|	O
=	O
151k	O
words	O
(	O
not	O
including	O
530	O
entities	O
and	O
1	O
placeholder	O
marker	O
)	O
in	O
the	O
Daily	B-Material
Mail	I-Material
dataset	E-Material
.	O

Embedding	B-Method
Layer	E-Method
:	O
We	O
choose	O
300	O
-	O
dimensional	O
word	O
embeddings	O
,	O
and	O
use	O
the	O
300	B-Method
-	I-Method
dimensional	I-Method
pretrained	I-Method
Glove	I-Method
word	I-Method
embeddings	E-Method
[	O
reference	O
]	O
for	O
initialization	S-Task
.	O

We	O
also	O
apply	O
dropout	S-Method
with	O
probability	O
0.2	O
to	O
the	O
embedding	B-Method
layer	E-Method
.	O

Bi	B-Method
-	I-Method
GRU	I-Method
Encoder	E-Method
:	O
We	O
apply	O
bidirectional	B-Method
GRU	E-Method
for	O
encoding	O
query	B-Task
and	I-Task
passage	E-Task
into	O
vector	B-Method
representations	E-Method
.	O

We	O
set	O
the	O
number	O
of	O
hidden	O
units	O
to	O
be	O
256	O
and	O
384	O
for	O
the	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
respectively	O
.	O

The	O
recurrent	O
weights	O
of	O
GRUs	S-Method
are	O
initialized	O
with	O
random	B-Method
orthogonal	I-Method
matrices	E-Method
.	O

The	O
other	O
weights	O
in	O
GRU	O
cell	O
are	O
initialized	O
from	O
a	O
uniform	O
distribution	O
between	O
−0.01	O
and	O
0.01	O
.	O

We	O
use	O
a	O
shared	O
GRU	B-Method
model	E-Method
for	O
both	O
query	B-Task
and	I-Task
passage	E-Task
.	O

Memory	O
and	O
Attention	S-Task
:	O
The	O
memory	O
of	O
the	O
ReasoNet	S-Method
on	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
dataset	E-Material
is	O
composed	O
of	O
query	O
memory	O
and	O
passage	O
memory	O
.	O

M	O
=	O
(	O
M	O
quer	O
,	O
M	O
doc	O
)	O
,	O
where	O
M	O
quer	O
and	O
M	O
doc	O
are	O
extracted	O
from	O
query	B-Method
bidirectional	I-Method
-	I-Method
GRU	I-Method
encoder	E-Method
and	O
passage	B-Method
bidirectional	I-Method
-	I-Method
GRU	I-Method
encoder	E-Method
respectively	O
.	O

We	O
choose	O
projected	B-Method
cosine	I-Method
similarity	I-Method
function	E-Method
as	O
the	O
attention	B-Method
module	E-Method
.	O

The	O
attention	O
score	O
a	O
doc	O
t	O
,	O
i	O
on	O
memory	O
m	O
doc	O
i	O
given	O
the	O
state	O
s	O
t	O
is	O
computed	O
as	O
follows	O
:	O
where	O
W	O
t	O
and	O
b	O
t	O
are	O
the	O
weight	O
matrix	O
and	O
bias	O
vector	O
,	O
respectively	O
.	O

Answer	B-Method
Module	E-Method
:	O
We	O
apply	O
a	O
linear	B-Method
projection	E-Method
from	O
GRU	O
outputs	O
and	O
make	O
predictions	O
on	O
the	O
entity	O
candidates	O
.	O

Following	O
the	O
[	O
reference	O
]	O
The	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
are	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
deepmind	O
/	O
rcdata	O
settings	O
in	O
AS	O
Reader	O
[	O
reference	O
]	O
,	O
we	O
sum	O
up	O
scores	O
from	O
the	O
same	O
candidate	O
and	O
make	O
a	O
prediction	O
.	O

Thus	O
,	O
AS	O
Reader	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNets	O
with	O
T	O
max	O
=	O
1	O
.	O

[	O
reference	O
]	O
Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
,	O
T	O
max	O
is	O
set	O
to	O
5	O
in	O
experiments	O
on	O
both	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
.	O

We	O
use	O
ADAM	B-Method
optimizer	E-Method
[	O
reference	O
]	O
for	O
parameter	B-Task
optimization	E-Task
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.0005	O
,	O
β	O
1	O
=	O
0.9	O
and	O
β	O
2	O
=	O
0.999	O
;	O
The	O
absolute	O
value	O
of	O
gradient	O
on	O
each	O
parameter	O
is	O
clipped	O
within	O
0.001	O
.	O

The	O
batch	O
size	O
is	O
64	O
for	O
both	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
.	O

For	O
each	O
batch	O
of	O
the	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
we	O
randomly	O
reshu	O
e	O
the	O
assignment	O
of	O
named	O
entities	O
[	O
reference	O
]	O
.	O

This	O
forces	O
the	O
model	O
to	O
treat	O
the	O
named	O
entities	O
as	O
semantically	O
meaningless	O
labels	O
.	O

In	O
the	O
prediction	B-Task
of	I-Task
test	I-Task
cases	E-Task
,	O
we	O
randomly	O
reshu	O
e	O
named	O
entities	O
up	O
to	O
4	O
times	O
,	O
and	O
report	O
the	O
averaged	O
answer	O
.	O

Models	O
are	O
trained	O
on	O
GTX	S-Method
TitanX	O
12	O
GB	O
.	O

It	O
takes	O
7	O
hours	O
per	O
epoch	O
to	O
train	O
on	O
the	O
Daily	B-Material
Mail	I-Material
dataset	E-Material
and	O
3	O
hours	O
per	O
epoch	O
to	O
train	O
on	O
the	O
CNN	B-Material
dataset	E-Material
.	O

The	O
models	O
are	O
usually	O
converged	O
within	O
6	O
epochs	O
on	O
both	O
CNN	B-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
.	O

[	O
reference	O
]	O
When	O
ReasoNet	S-Method
is	O
set	O
with	O
T	O
max	O
=	O
1	O
in	O
CNN	S-Material
and	O
Daily	B-Material
Mail	E-Material
,	O
it	O
directly	O
applies	O
s	O
0	O
to	O
make	O
predictions	O
on	O
the	O
entity	O
candidates	O
,	O
without	O
performing	O
attention	O
on	O
the	O
memory	B-Method
module	E-Method
.	O

The	O
prediction	B-Method
module	E-Method
in	O
ReasoNets	S-Method
is	O
the	O
same	O
as	O
in	O
AS	O
Reader	O
.	O

It	O
sums	O
up	O
the	O
scores	O
from	O
the	O
same	O
entity	O
candidates	O
,	O
where	O
the	O
scores	O
are	O
calculated	O
by	O
the	O
inner	O
product	O
between	O
s	O
t	O
and	O
m	O
d	O
oc	O
e	O
,	O
where	O
m	O
d	O
oc	O
e	O
is	O
an	O
embedding	O
vector	O
of	O
one	O
entity	O
candidate	O
in	O
the	O
passage	O
.	O

Query	O
:	O
passenger	O
@placeholder	O
,	O
36	O
,	O
died	O
at	O
the	O
scene	O
Passage	O
:	O
(	O
@entity0	O
)	O
what	O
was	O
supposed	O
to	O
be	O
a	O
fantasy	O
sports	O
car	O
ride	O
at	O
@entity3	O
turned	O
deadly	O
when	O
a	O
@entity4	O
crashed	O
into	O
a	O
guardrail	O
.	O

the	O
crash	O
took	O
place	O
sunday	O
at	O
the	O
@entity8	O
,	O
which	O
bills	O
itself	O
as	O
a	O
chance	O
to	O
drive	O
your	O
dream	O
car	O
on	O
a	O
racetrack	O
.	O

the	O
@entity4	O
's	O
passenger	O
,	O
36	O
-	O
year	O
-	O
old	O
@entity14	O
of	O
@entity15	O
,	O
@entity16	O
,	O
died	O
at	O
the	O
scene	O
,	O
@entity13	O
said	O
.	O

the	O
driver	O
of	O
the	O
@entity4	O
,	O
24	O
-	O
year	O
-	O
old	O
@entity18	O
of	O
@entity19	O
,	O
@entity16	O
,	O
lost	O
control	O
of	O
the	O
vehicle	O
,	O
the	O
@entity13	O
said	O
.	O

he	O
was	O
hospitalized	O
with	O
minor	O
injuries	O
.	O

@entity24	O
,	O
which	O
operates	O
the	O
@entity8	O
at	O
@entity3	O
,	O
released	O
a	O
statement	O
sunday	O
night	O
about	O
the	O
crash	O
.	O

"	O
on	O
behalf	O
of	O
everyone	O
in	O
the	O
organization	O
,	O
it	O
is	O
with	O
a	O
very	O
heavy	O
heart	O
that	O
we	O
extend	O
our	O
deepest	O
sympathies	O
to	O
those	O
involved	O
in	O
today	O
's	O
tragic	O
accident	O
in	O
@entity36	O
,	O
"	O
the	O
company	O
said	O
.	O

@entity24	O
also	O
operates	O
the	O
@entity3	O
--	O
a	O
chance	O
to	O
drive	O
or	O
ride	O
in	O
@entity39	O
race	O
cars	O
named	O
for	O
the	O
winningest	O
driver	O
in	O
the	O
sport	O
's	O
history	O
.	O

@entity0	O
's	O
@entity43	O
and	O
@entity44	O
contributed	O
to	O
this	O
report	O
.	O

section	O
:	O
Answer	O
:	O
@entity14	O
Step	B-Metric
Termination	I-Metric
Probability	E-Metric
Attention	O
Sum	O
1	O
0.0011	O
0.4916	O
2	O
0.5747	O
0.5486	O
3	O
0.9178	O
0.5577	O
Step	O
3	O
Step	O
1	O
Step	O
2	O
Figure	O
3	O
:	O
Results	O
of	O
a	O
test	O
example	O
69e1f777e41bf67d5a22b7c69ae76f0ae873cf43.story	O
from	O
the	O
CNN	B-Material
dataset	E-Material
.	O

The	O
numbers	O
next	O
to	O
the	O
underline	O
bars	O
indicate	O
the	O
rank	O
of	O
the	O
attention	O
scores	O
.	O

The	O
corresponding	O
termination	B-Metric
probability	E-Metric
and	O
the	O
sum	O
of	O
attention	O
scores	O
for	O
the	O
answer	O
entity	O
are	O
shown	O
in	O
the	O
table	O
on	O
the	O
right	O
.	O

Results	O
:	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
all	O
the	O
existing	O
single	B-Method
model	I-Method
baselines	E-Method
and	O
our	O
proposed	O
ReasoNet	S-Method
.	O

Among	O
all	O
the	O
baselines	O
,	O
AS	O
Reader	O
could	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNet	S-Method
with	O
T	O
max	O
=	O
1	O
.	O

Comparing	O
with	O
the	O
AS	B-Method
Reader	E-Method
,	O
ReasoNet	S-Method
shows	O
the	O
signi	O
ca	O
nt	O
improvement	O
by	O
capturing	O
multi	B-Task
-	I-Task
turn	I-Task
reasoning	E-Task
in	O
the	O
paragraph	O
.	O

Iterative	B-Method
Attention	I-Method
Reader	E-Method
,	O
EpiReader	S-Method
and	O
GA	B-Method
Reader	E-Method
are	O
the	O
three	O
multi	B-Method
-	I-Method
turn	I-Method
reasoning	I-Method
models	E-Method
with	O
xed	O
reasoning	O
steps	O
.	O

ReasoNet	S-Method
also	O
outperforms	O
all	O
of	O
them	O
by	O
integrating	O
termination	O
gate	O
in	O
the	O
model	O
which	O
allows	O
di	O
erent	O
reasoning	O
steps	O
for	O
di	O
erent	O
test	O
cases	O
.	O

AoA	O
Reader	S-Method
is	O
another	O
single	B-Method
-	I-Method
turn	I-Method
reasoning	I-Method
model	E-Method
,	O
it	O
captures	O
the	O
word	O
alignment	O
signals	O
between	O
query	O
and	O
passage	O
,	O
and	O
shows	O
a	O
big	O
improvement	O
over	O
AS	O
Reader	S-Method
.	O

ReasoNet	S-Method
obtains	O
comparable	O
results	O
with	O
AoA	B-Method
Reader	E-Method
on	O
CNN	B-Material
test	I-Material
set	E-Material
.	O

We	O
expect	O
that	O
ReasoNet	S-Method
could	O
be	O
improved	O
further	O
by	O
incorporating	O
the	O
word	O
alignment	O
information	O
in	O
the	O
memory	B-Method
module	E-Method
as	O
suggested	O
in	O
AoA	B-Method
Reader	E-Method
.	O

We	O
show	O
the	O
distribution	O
of	O
termination	O
step	O
distribution	O
of	O
ReasoNets	O
in	O
the	O
CNN	B-Material
dataset	E-Material
in	O
Figure	O
2	O
.	O

The	O
distributions	O
spread	O
out	O
across	O
di	O
erent	O
steps	O
.	O

Around	O
70	O
%	O
of	O
the	O
instances	O
terminate	O
in	O
the	O
last	O
step	O
.	O

Figure	O
3	O
gives	O
a	O
test	O
example	O
on	O
CNN	B-Material
dataset	E-Material
,	O
which	O
illustrates	O
the	O
inference	B-Task
process	E-Task
of	O
the	O
ReasoNet	S-Method
.	O

The	O
model	O
initially	O
focuses	O
on	O
wrong	O
entities	O
with	O
low	O
termination	O
probability	O
.	O

In	O
the	O
second	O
and	O
third	O
steps	O
,	O
the	O
model	O
focuses	O
on	O
the	O
right	O
clue	O
with	O
higher	O
termination	O
probability	O
.	O

Interestingly	O
,	O
we	O
also	O
nd	O
its	O
query	O
attention	O
focuses	O
on	O
the	O
placeholder	O
token	O
throughout	O
all	O
the	O
steps	O
.	O

section	O
:	O
SQuAD	S-Material
Dataset	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
ReasoNet	S-Method
model	O
on	O
the	O
task	O
of	O
question	B-Task
answering	E-Task
using	O
the	O
SQuAD	S-Material
dataset	O
[	O
reference	O
]	O
.	O

[	O
reference	O
]	O
SQuAD	S-Material
is	O
a	O
machine	O
comprehension	O
dataset	O
on	O
536	O
Wikipedia	B-Material
articles	E-Material
,	O
with	O
more	O
than	O
100	O
,	O
000	O
questions	O
.	O

Two	O
metrics	O
are	O
used	O
to	O
evaluate	O
models	O
:	O
Exact	B-Metric
Match	E-Metric
(	O
EM	S-Metric
)	O
and	O
a	O
softer	B-Metric
metric	E-Metric
,	O
F1	B-Metric
score	E-Metric
,	O
which	O
measures	O
the	O
weighted	O
average	O
of	O
the	O
precision	S-Metric
and	O
recall	S-Metric
rate	O
at	O
the	O
character	B-Metric
level	E-Metric
.	O

The	O
dataset	O
consists	O
of	O
90k	O
/	O
10k	O
training	O
/	O
dev	O
question	O
-	O
contextanswer	O
tuples	O
with	O
a	O
large	O
hidden	O
test	O
set	O
.	O

The	O
model	O
architecture	O
used	O
for	O
this	O
task	O
is	O
as	O
follows	O
:	O
[	O
reference	O
]	O
SQuAD	S-Material
Competition	O
Website	O
is	O
https:	O
//	O
rajpurkar.github.io	O
/	O
SQuAD	S-Material
-	O
explorer	O
/	O
Vocab	O
Size	O
:	O
We	O
use	O
the	O
python	B-Method
NLTK	I-Method
tokenizer	E-Method
6	O
to	O
preprocess	O
passages	O
and	O
questions	O
,	O
and	O
obtain	O
about	O
100	O
K	O
words	O
in	O
the	O
vocabulary	O
.	O

Embedding	B-Method
Layer	E-Method
:	O
We	O
use	O
the	O
100	O
-	O
dimensional	O
pretrained	O
Glove	O
vectors	O
[	O
reference	O
]	O
as	O
word	O
embeddings	O
.	O

These	O
Glove	O
vectors	O
are	O
xed	O
during	O
the	O
model	B-Method
training	E-Method
.	O

To	O
alleviate	O
the	O
out	B-Task
-	I-Task
of	I-Task
-	I-Task
vocabulary	I-Task
issue	E-Task
,	O
we	O
adopt	O
one	B-Method
layer	I-Method
100	I-Method
-	I-Method
dimensional	I-Method
convolutional	I-Method
neural	I-Method
network	E-Method
on	O
character	O
-	O
level	O
with	O
a	O
width	O
size	O
of	O
5	O
and	O
each	O
character	O
encoded	O
as	O
an	O
8	O
-	O
dimensional	O
vector	O
following	O
the	O
work	O
[	O
reference	O
]	O
.	O

The	O
100	O
-	O
dimensional	O
Glove	O
word	O
vector	O
and	O
the	O
100	O
-	O
dimensional	O
character	O
-	O
level	O
vector	O
are	O
concatenated	O
to	O
obtain	O
a	O
200	O
-	O
dimensional	O
vector	O
for	O
each	O
word	O
.	O

Bi	B-Method
-	I-Method
GRU	I-Method
Encoder	E-Method
:	O
We	O
apply	O
bidirectional	B-Method
GRU	E-Method
for	O
encoding	O
query	B-Task
and	I-Task
passage	E-Task
into	O
vector	B-Method
representations	E-Method
.	O

The	O
number	O
of	O
hidden	O
units	O
is	O
set	O
to	O
128	O
.	O

Memory	S-Method
:	O
We	O
use	O
bidirectional	B-Method
-	I-Method
GRU	I-Method
encoders	E-Method
to	O
extract	O
the	O
query	B-Method
representation	I-Method
M	I-Method
quer	E-Method
and	O
the	O
passage	B-Method
representation	I-Method
M	I-Method
doc	E-Method
,	O
given	O
a	O
query	O
and	O
a	O
passage	O
.	O

We	O
compute	O
the	O
similarity	O
matrix	O
Table	O
4	O
:	O
Small	O
and	O
large	O
random	O
graph	O
in	O
the	O
Graph	B-Material
Reachability	I-Material
dataset	E-Material
.	O

Note	O
that	O
"	O
A	O
→	O
B	O
"	O
represents	O
an	O
edge	O
connected	O
from	O
A	O
to	O
B	O
and	O
the	O
#	O
symbol	O
is	O
used	O
as	O
a	O
delimiter	O
between	O
di	O
erent	O
edges	O
.	O

Small	B-Task
Graph	I-Task
Large	I-Task
Graph	E-Task
No	O
Yes	O
between	O
each	O
word	O
in	O
the	O
query	O
and	O
each	O
word	O
in	O
the	O
passage	O
.	O

The	O
similarity	O
matrix	O
is	O
denoted	O
as	O
S	O
∈	O
R	O
T×	O
,	O
where	O
T	O
and	O
are	O
the	O
number	O
of	O
words	O
in	O
the	O
passage	O
and	O
query	O
,	O
respectively	O
,	O
and	O
]	O
∈	O
R	O
,	O
where	O
w	O
S	O
is	O
a	O
trainable	O
weight	O
vector	O
,	O
•	O
denotes	O
the	O
elementwise	B-Method
multiplication	E-Method
,	O
and	O
[	O
;	O
]	O
is	O
the	O
vector	O
concatenation	O
across	O
row	O
.	O

We	O
then	O
compute	O
the	O
context	B-Task
-	I-Task
to	I-Task
-	I-Task
query	I-Task
attention	E-Task
and	O
query	B-Task
-	I-Task
to	I-Task
-	I-Task
context	I-Task
attention	E-Task
from	O
the	O
similarity	O
matrix	O
S	O
by	O
following	O
recent	O
co	B-Method
-	I-Method
attention	E-Method
work	O
[	O
reference	O
]	O
to	O
obtain	O
the	O
query	B-Method
-	I-Method
aware	I-Method
passage	I-Method
representation	I-Method
G.	E-Method
We	O
feed	O
G	O
to	O
a	O
128	B-Method
-	I-Method
dimensional	I-Method
bidirectional	I-Method
GRU	E-Method
to	O
obtain	O
the	O
memory	O
M	O
=	O
bidirectional	B-Method
-	I-Method
GRU	I-Method
(	I-Method
G	E-Method
)	O
,	O
where	O
M	O
∈	O
R	O
256×T	O
.	O

Internal	B-Method
State	I-Method
Controller	E-Method
:	O
We	O
use	O
a	O
GRU	B-Method
model	E-Method
with	O
256	B-Method
-	I-Method
dimensional	I-Method
hidden	I-Method
units	E-Method
as	O
the	O
internal	B-Method
state	I-Method
controller	E-Method
.	O

The	O
initial	O
state	O
of	O
the	O
GRU	B-Method
controller	E-Method
is	O
the	O
last	B-Method
-	I-Method
word	I-Method
representation	E-Method
of	O
the	O
query	B-Method
bidirectional	I-Method
-	I-Method
GRU	I-Method
encoder	E-Method
.	O

Termination	B-Method
Module	E-Method
:	O
We	O
use	O
the	O
same	O
termination	B-Method
module	E-Method
as	O
in	O
the	O
CNN	B-Task
and	I-Task
Daily	I-Task
Mail	I-Task
experiments	E-Task
.	O

Answer	B-Method
Module	E-Method
:	O
SQuAD	S-Material
task	O
requires	O
the	O
model	O
to	O
nd	O
a	O
span	O
in	O
the	O
passage	O
to	O
answer	O
the	O
query	O
.	O

Thus	O
the	O
answer	B-Method
module	E-Method
requires	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
the	O
answer	O
span	O
in	O
the	O
passage	O
.	O

The	O
probability	O
distribution	O
of	O
selecting	O
the	O
start	O
index	O
over	O
the	O
passage	O
at	O
state	O
s	O
t	O
is	O
computed	O
by	O
:	O
where	O
S	O
t	O
is	O
given	O
via	O
tiling	O
s	O
t	O
by	O
T	O
times	O
across	O
the	O
column	O
and	O
w	O
p	O
1	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

The	O
probability	O
distribution	O
of	O
selecting	O
the	O
end	O
index	O
over	O
passage	O
is	O
computed	O
in	O
a	O
similar	O
manner	O
:	O
Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
T	O
max	O
is	O
set	O
to	O
10	O
in	O
SQuAD	S-Material
experiments	O
.	O

We	O
use	O
AdaDelta	B-Method
optimizer	E-Method
[	O
reference	O
]	O
for	O
parameter	B-Task
optimization	E-Task
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

Models	O
are	O
trained	O
on	O
GTX	S-Method
TitanX	O
12	O
GB	O
.	O

It	O
takes	O
about	O
40	O
minutes	O
per	O
epoch	O
for	O
training	S-Task
,	O
with	O
18	O
epochs	O
in	O
total	O
.	O

Results	O
:	O
In	O
the	O
Table	O
2	O
,	O
we	O
report	O
the	O
performance	O
of	O
all	O
models	O
in	O
the	O
SQuAD	S-Material
leaderboard	O
.	O

[	O
reference	O
]	O
In	O
the	O
upper	O
part	O
of	O
the	O
Table	O
2	O
,	O
we	O
compare	O
ReasoNet	S-Method
with	O
all	O
published	O
baselines	O
at	O
the	O
time	O
of	O
submission	O
.	O

Speci	O
cally	O
,	O
BiDAF	B-Method
model	E-Method
could	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
ReasoNet	S-Method
with	O
T	O
max	O
=	O
1	O
.	O

It	O
is	O
worth	O
noting	O
that	O
this	O
SQuAD	S-Material
leaderboard	O
is	O
highly	O
active	O
and	O
competitive	O
.	O

The	O
test	O
set	O
is	O
hidden	O
to	O
all	O
models	O
and	O
all	O
the	O
results	O
on	O
the	O
leaderboard	O
are	O
produced	O
and	O
reported	O
by	O
the	O
organizer	O
;	O
thus	O
all	O
the	O
results	O
here	O
are	O
reproducible	O
.	O

In	O
Table	O
2	O
,	O
we	O
demonstrate	O
that	O
ReasoNet	S-Method
outperforms	O
all	O
existing	O
published	O
approaches	O
.	O

While	O
we	O
compare	O
ReasoNet	S-Method
with	O
BiDAF	S-Method
,	O
ReasoNet	S-Method
exceeds	O
BiDAF	S-Method
both	O
in	O
single	O
model	O
and	O
ensemble	B-Method
model	I-Method
cases	E-Method
.	O

This	O
demonstrates	O
the	O
importance	O
of	O
the	O
dynamic	B-Task
multi	I-Task
-	I-Task
turn	I-Task
reasoning	E-Task
over	O
a	O
passage	O
.	O

In	O
the	O
bottom	O
part	O
of	O
Table	O
2	O
,	O
we	O
compare	O
ReasoNet	S-Method
with	O
all	O
unpublished	O
methods	O
at	O
the	O
time	O
of	O
this	O
submission	O
,	O
ReasoNet	S-Method
holds	O
the	O
second	O
position	O
in	O
all	O
the	O
competing	O
approaches	O
in	O
the	O
SQuAD	S-Material
leaderboard	O
.	O

section	O
:	O
Graph	B-Task
Reachability	I-Task
Task	E-Task
Recent	O
analysis	O
and	O
results	O
[	O
reference	O
]	O
on	O
the	O
cloze	B-Task
-	I-Task
style	I-Task
machine	I-Task
comprehension	I-Task
tasks	E-Task
have	O
suggested	O
some	O
simple	O
models	O
without	O
multiturn	B-Method
reasoning	E-Method
can	O
achieve	O
reasonable	O
performance	O
.	O

Based	O
on	O
these	O
results	O
,	O
we	O
construct	O
a	O
synthetic	B-Material
structured	I-Material
Graph	I-Material
Reachability	I-Material
dataset	E-Material
[	O
reference	O
]	O
to	O
evaluate	O
longer	B-Task
range	I-Task
machine	I-Task
inference	I-Task
and	I-Task
reasoning	I-Task
capability	E-Task
,	O
since	O
we	O
anticipate	O
ReasoNets	S-Method
to	O
have	O
the	O
capability	O
to	O
handle	O
long	O
range	O
relationships	O
.	O

We	O
generate	O
two	O
synthetic	O
datasets	O
:	O
a	O
small	O
graph	O
dataset	O
and	O
a	O
large	O
graph	O
dataset	O
.	O

In	O
the	O
small	O
graph	O
dataset	O
,	O
it	O
contains	O
500	O
K	O
small	O
graphs	O
,	O
where	O
each	O
graph	O
contains	O
9	O
nodes	O
and	O
16	O
direct	O
Step	O
1	O
Step	O
2	O
Step	O
0	O
Step	O
3	O
Step	O
4	O
,	O
5	O
,	O
7	O
Step	O
6	O
,	O
8	O
Step	O
9	O
Step	O
Step	O
2	O
Step	O
3	O
Step	O
1	O
Step	O
4	O
Steps	O
5	O
,	O
6	O
,	O
8	O
Steps	O
7	O
,	O
9	O
Step	O
10	O
Figure	O
4	O
:	O
An	O
example	O
of	O
graph	B-Task
reachability	E-Task
result	O
,	O
given	O
a	O
query	O
"	O
10	O
→	O
17	O
"	O
(	O
Answer	O
:	O
Yes	O
)	O
.	O

The	O
red	O
circles	O
highlight	O
the	O
nodes	O
/	O
edges	O
which	O
have	O
the	O
highest	O
attention	O
in	O
each	O
step	O
.	O

The	O
corresponding	O
termination	B-Metric
probability	E-Metric
and	O
prediction	S-Task
results	O
are	O
shown	O
in	O
the	O
table	O
.	O

The	O
model	O
terminates	O
at	O
step	O
10	O
.	O

edges	O
to	O
randomly	O
connect	O
pairs	O
of	O
nodes	O
.	O

The	O
large	O
graph	O
dataset	O
contains	O
500	O
K	O
graphs	O
,	O
where	O
each	O
graph	O
contains	O
18	O
nodes	O
and	O
32	O
random	O
direct	O
edges	O
.	O

Duplicated	O
edges	O
are	O
removed	O
.	O

Table	O
3	O
shows	O
the	O
graph	O
reachability	O
statistics	O
on	O
the	O
two	O
datasets	O
.	O

In	O
Table	O
4	O
,	O
we	O
show	O
examples	O
of	O
a	O
small	O
graph	O
and	O
a	O
large	O
graph	O
in	O
the	O
synthetic	O
dataset	O
.	O

Both	O
graph	O
and	O
query	O
are	O
represented	O
by	O
a	O
sequence	O
of	O
symbols	O
.	O

The	O
details	O
settings	O
of	O
the	O
ReasoNet	S-Method
are	O
listed	O
as	O
follows	O
in	O
the	O
reachability	B-Task
tasks	E-Task
.	O

Embedding	B-Method
Layer	E-Method
We	O
use	O
a	O
100	O
-	O
dimensional	O
embedding	O
vector	O
for	O
each	O
symbol	O
in	O
the	O
query	B-Task
and	I-Task
graph	I-Task
description	E-Task
.	O

Bi	B-Method
-	I-Method
LSTM	I-Method
Encoder	E-Method
:	O
We	O
apply	O
a	O
bidirectional	B-Method
-	I-Method
LSTM	I-Method
layer	E-Method
with	O
128	O
and	O
256	O
cells	O
on	O
query	O
embeddings	O
in	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

The	O
last	O
states	O
of	O
bidirectional	B-Method
-	I-Method
LSTM	E-Method
on	O
query	O
are	O
concatenated	O
to	O
be	O
the	O
initial	O
internal	O
state	O
We	O
apply	O
another	O
bidirectional	B-Method
-	I-Method
LSTM	I-Method
layer	E-Method
with	O
128	O
and	O
256	O
cells	O
on	O
graph	B-Task
description	I-Task
embeddings	E-Task
in	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

It	O
maps	O
each	O
symbol	O
i	O
to	O
a	O
contextual	B-Method
representation	E-Method
given	O
by	O
the	O
concatenation	B-Method
of	I-Method
forward	I-Method
and	I-Method
backward	I-Method
LSTM	I-Method
hidden	I-Method
states	E-Method
section	O
:	O
Internal	B-Method
State	I-Method
Controller	E-Method
:	O
We	O
use	O
a	O
GRU	B-Method
model	E-Method
with	O
128	O
-	O
dimensional	O
and	O
256	O
-	O
dimensional	O
hidden	O
units	O
as	O
the	O
internal	B-Method
state	I-Method
controller	E-Method
for	O
the	O
small	O
and	O
large	O
graph	O
datasets	O
,	O
respectively	O
.	O

The	O
initial	O
state	O
of	O
the	O
GRU	B-Method
controller	E-Method
is	O
s	O
1	O
.	O

Answer	B-Method
Module	E-Method
:	O
The	O
nal	O
answer	O
is	O
either	O
"	O
Yes	O
"	O
or	O
"	O
No	O
"	O
and	O
hence	O
logistical	B-Method
regression	E-Method
is	O
used	O
as	O
the	O
answer	O
module	O
:	O
Termination	B-Method
Module	E-Method
:	O
We	O
use	O
the	O
same	O
termination	B-Method
module	E-Method
as	O
in	O
the	O
CNN	B-Task
and	I-Task
Daily	I-Task
Mail	I-Task
experiments	E-Task
.	O

Other	O
Details	O
:	O
The	O
maximum	O
reasoning	O
step	O
T	O
max	O
is	O
set	O
to	O
15	O
and	O
25	O
for	O
the	O
small	O
graph	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
.	O

We	O
use	O
AdaDelta	B-Method
optimizer	E-Method
[	O
reference	O
]	O
for	O
parameter	B-Task
optimization	E-Task
with	O
an	O
initial	O
learning	B-Metric
rate	E-Metric
of	O
0.5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

We	O
denote	O
"	O
ReasoNet	S-Method
"	O
as	O
the	O
standard	O
ReasoNet	S-Method
with	O
termination	O
gate	O
,	O
as	O
described	O
in	O
Section	O
3.1	O
.	O

To	O
study	O
the	O
e	O
ectiveness	O
of	O
the	O
termination	O
gate	O
in	O
ReasoNets	O
,	O
we	O
remove	O
the	O
termination	O
gate	O
and	O
use	O
the	O
prediction	O
from	O
the	O
last	O
state	O
,	O
â	O
=	O
a	O
T	O
max	O
(	O
T	O
max	O
is	O
the	O
maximum	O
reasoning	O
step	O
)	O
,	O
denoted	O
as	O
"	O
ReasoNet	B-Method
-	I-Method
Last	E-Method
"	O
.	O

To	O
study	O
the	O
e	O
ectiveness	O
of	O
multi	B-Task
-	I-Task
turn	I-Task
reasoning	E-Task
,	O
we	O
choose	O
"	O
ReasoNet	S-Method
-	O
T	O
max	O
=	O
2	O
"	O
,	O
which	O
only	O
has	O
single	B-Method
-	I-Method
turn	I-Method
reasoning	E-Method
.	O

We	O
compare	O
ReasoNets	S-Method
with	O
a	O
two	B-Method
layer	I-Method
deep	I-Method
LSTM	I-Method
model	E-Method
[	O
reference	O
]	O
with	O
128	O
hidden	O
units	O
,	O
denoted	O
as	O
"	O
Deep	B-Method
LSTM	I-Method
Reader	E-Method
"	O
,	O
as	O
a	O
baseline	O
.	O

Table	O
5	O
shows	O
the	O
performance	O
of	O
these	O
models	O
on	O
the	O
graph	B-Material
reachability	I-Material
dataset	E-Material
.	O

Deep	B-Method
LSTM	I-Method
Reader	E-Method
achieves	O
90.92	O
%	O
and	O
71.55	O
%	O
accuracy	S-Metric
in	O
the	O
small	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
,	O
which	O
indicates	O
the	O
graph	B-Task
reachibility	I-Task
task	E-Task
is	O
not	O
trivial	O
.	O

The	O
results	O
of	O
ReasoNet	S-Method
-	O
T	O
max	O
=	O
2	O
are	O
comparable	O
with	O
the	O
results	O
of	O
Deep	B-Method
LSTM	I-Method
Reader	E-Method
,	O
since	O
both	O
Deep	B-Method
LSTM	I-Method
Reader	E-Method
and	O
ReasoNet	S-Method
-	O
T	O
max	O
=	O
2	O
perform	O
single	B-Method
-	I-Method
turn	I-Method
reasoning	E-Method
.	O

The	O
ReasoNet	S-Method
-	O
Last	O
model	O
achieves	O
100	O
%	O
accuracy	S-Metric
on	O
the	O
small	O
graph	O
dataset	O
,	O
while	O
the	O
ReasoNet	S-Method
-	O
Last	O
model	O
achieves	O
Step	O
Step	O
1	O
Step	O
2	O
Step	O
2	O
Step	O
1	O
Step	O
1	O
Step	O
1	O
Figure	O
4	O
,	O
we	O
can	O
observe	O
that	O
the	O
model	O
does	O
not	O
make	O
a	O
rm	B-Task
prediction	E-Task
till	O
step	O
9	O
.	O

The	O
highest	O
attention	O
word	O
at	O
each	O
step	O
shows	O
the	O
reasoning	O
process	O
of	O
the	O
model	O
.	O

Interestingly	O
,	O
the	O
model	O
starts	O
from	O
the	O
end	O
node	O
[	O
reference	O
]	O
,	O
traverses	O
backward	O
till	O
nding	O
the	O
starting	O
node	O
(	O
10	O
)	O
in	O
step	O
9	O
,	O
and	O
makes	O
a	O
rm	B-Task
termination	I-Task
prediction	E-Task
.	O

On	O
the	O
other	O
hand	O
,	O
in	O
Figure	O
5	O
,	O
the	O
model	O
learns	O
to	O
stop	O
in	O
step	O
2	O
.	O

In	O
step	O
1	O
,	O
the	O
model	O
looks	O
for	O
neighbor	O
nodes	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
to	O
4	O
and	O
9	O
.	O

Then	O
,	O
the	O
model	O
gives	O
up	O
in	O
step	O
2	O
and	O
predict	O
"	O
No	O
"	O
.	O

All	O
of	O
these	O
demonstrate	O
the	O
dynamic	O
termination	O
characteristic	O
and	O
potential	O
reasoning	O
capability	O
of	O
ReasoNets	S-Method
.	O

To	O
better	O
grasp	O
when	O
ReasoNets	O
stop	O
reasoning	O
,	O
we	O
show	O
the	O
distribution	O
of	O
termination	O
steps	O
in	O
ReasoNets	O
on	O
the	O
test	O
set	O
.	O

The	O
termination	O
step	O
is	O
chosen	O
with	O
the	O
maximum	O
termination	O
probability	O
p	O
(	O
k	O
)	O
=	O
t	O
k	O
k	O
−1	O
i=1	O
(	O
1	O
−	O
t	O
i	O
)	O
,	O
where	O
t	O
i	O
is	O
the	O
termination	O
probability	O
at	O
step	O
i.	O
Figure	O
6	O
shows	O
the	O
termination	O
step	O
distribution	O
of	O
ReasoNets	O
in	O
the	O
graph	B-Material
reachability	I-Material
dataset	E-Material
.	O

The	O
distributions	O
spread	O
out	O
across	O
di	O
erent	O
steps	O
.	O

Around	O
16	O
%	O
and	O
35	O
%	O
of	O
the	O
instances	O
terminate	O
in	O
the	O
last	O
step	O
for	O
the	O
small	O
and	O
large	O
graph	O
,	O
respectively	O
.	O

We	O
study	O
the	O
correlation	O
between	O
the	O
termination	O
steps	O
and	O
the	O
complexity	O
of	O
test	O
instances	O
in	O
Figure	O
7	O
.	O

Given	O
the	O
query	O
,	O
we	O
use	O
the	O
Breadth	B-Method
-	I-Method
First	I-Method
Search	E-Method
(	O
BFS	S-Method
)	O
algorithm	O
over	O
the	O
target	O
graph	O
to	O
analyze	O
the	O
complexity	O
of	O
test	O
instances	O
.	O

For	O
example	O
,	O
BFS	S-Method
-	O
Step	O
Figure	O
7	O
shows	O
that	O
test	O
instances	O
with	O
larger	O
BFS	S-Method
-	O
Steps	O
require	O
more	O
reasoning	O
steps	O
.	O

The	O
correlation	O
between	O
BFS	S-Method
steps	O
and	O
ReasoNet	S-Method
termination	O
steps	O
in	O
the	O
graph	O
reachability	O
dataset	O
,	O
where	O
T	O
max	O
is	O
set	O
to	O
15	O
and	O
25	O
in	O
the	O
small	O
graph	O
and	O
large	O
graph	O
dataset	O
,	O
respectively	O
,	O
and	O
BFS	S-Method
-	O
Step=	O
−1	O
denotes	O
unreachable	O
cases	O
.	O

The	O
value	O
indicates	O
the	O
number	O
of	O
instances	O
in	O
each	O
case	O
.	O

section	O
:	O
CONCLUSION	O
In	O
this	O
paper	O
,	O
we	O
propose	O
ReasoNets	S-Method
that	O
dynamically	O
decide	O
whether	O
to	O
continue	O
or	O
to	O
terminate	O
the	O
inference	B-Task
process	E-Task
in	O
machine	B-Task
comprehension	I-Task
tasks	E-Task
.	O

With	O
the	O
use	O
of	O
the	O
instance	B-Method
-	I-Method
dependent	I-Method
baseline	I-Method
method	E-Method
,	O
our	O
proposed	O
model	O
achieves	O
superior	O
results	O
in	O
machine	O
comprehension	O
datasets	O
,	O
including	O
unstructured	B-Material
CNN	I-Material
and	I-Material
Daily	I-Material
Mail	I-Material
datasets	E-Material
,	O
the	O
Stanford	B-Material
SQuAD	I-Material
dataset	E-Material
,	O
and	O
a	O
proposed	O
structured	O
Graph	O
Reachability	O
dataset	O
.	O

section	O
:	O
document	O
:	O
Dynamic	B-Task
Integration	I-Task
of	I-Task
Background	I-Task
Knowledge	E-Task
in	O
Neural	O
NLU	S-Task
Systems	O
Common	O
-	O
sense	O
and	O
background	O
knowledge	O
is	O
required	O
to	O
understand	O
natural	O
language	O
,	O
but	O
in	O
most	O
neural	O
natural	B-Task
language	I-Task
understanding	E-Task
(	O
NLU	S-Task
)	O
systems	O
,	O
this	O
knowledge	O
must	O
be	O
acquired	O
from	O
training	O
corpora	O
during	O
learning	O
,	O
and	O
then	O
it	O
is	O
static	O
at	O
test	O
time	O
.	O

We	O
introduce	O
a	O
new	O
architecture	O
for	O
the	O
dynamic	B-Task
integration	I-Task
of	I-Task
explicit	I-Task
background	I-Task
knowledge	E-Task
in	O
NLU	S-Task
models	O
.	O

A	O
general	O
-	O
purpose	O
reading	B-Method
module	E-Method
reads	O
background	O
knowledge	O
in	O
the	O
form	O
of	O
free	O
-	O
text	O
statements	O
(	O
together	O
with	O
task	O
-	O
specific	O
text	O
inputs	O
)	O
and	O
yields	O
refined	O
word	B-Method
representations	E-Method
to	O
a	O
task	O
-	O
specific	O
NLU	S-Task
architecture	O
that	O
reprocesses	O
the	O
task	O
inputs	O
with	O
these	O
representations	O
.	O

Experiments	O
on	O
document	B-Task
question	I-Task
answering	E-Task
(	O
DQA	S-Task
)	O
and	O
recognizing	B-Task
textual	I-Task
entailment	E-Task
(	O
RTE	S-Task
)	O
demonstrate	O
the	O
effectiveness	O
and	O
flexibility	O
of	O
the	O
approach	O
.	O

Analysis	O
shows	O
that	O
our	O
model	O
learns	O
to	O
exploit	O
knowledge	O
in	O
a	O
semantically	O
appropriate	O
way	O
.	O

section	O
:	O
Introduction	O
Understanding	B-Task
natural	I-Task
language	E-Task
depends	O
crucially	O
on	O
common	O
-	O
sense	O
and	O
background	O
knowledge	O
,	O
for	O
example	O
,	O
knowledge	O
about	O
what	O
concepts	O
are	O
expressed	O
by	O
the	O
words	O
being	O
read	O
(	O
lexical	O
knowledge	O
)	O
,	O
and	O
what	O
relations	O
hold	O
between	O
these	O
concepts	O
(	O
relational	O
knowledge	O
)	O
.	O

As	O
a	O
simple	O
illustration	O
,	O
if	O
an	O
agent	O
needs	O
to	O
understand	O
that	O
the	O
statement	O
“	O
King	O
Farouk	O
signed	O
his	O
abdication	O
”	O
is	O
entailed	O
by	O
“	O
King	O
Farouk	O
was	O
exiled	O
to	O
France	O
in	O
1952	O
,	O
after	O
signing	O
his	O
resignation	O
”	O
,	O
it	O
must	O
know	O
(	O
among	O
other	O
things	O
)	O
that	O
abdication	O
means	O
resignation	O
of	O
a	O
king	O
.	O

In	O
most	O
neural	O
natural	B-Task
language	I-Task
understanding	E-Task
(	O
NLU	S-Task
)	O
systems	O
,	O
the	O
requisite	O
background	O
knowledge	O
is	O
implicitly	O
encoded	O
in	O
the	O
models	O
’	O
parameters	O
.	O

That	O
is	O
,	O
what	O
background	O
knowledge	O
is	O
present	O
has	O
been	O
learned	O
from	O
task	B-Task
supervision	E-Task
and	O
also	O
by	O
pre	O
-	O
training	O
word	B-Method
embeddings	E-Method
(	O
where	O
distributional	O
properties	O
correlate	O
with	O
certain	O
kinds	O
of	O
useful	O
background	O
knowledge	O
,	O
such	O
as	O
semantic	O
relatedness	O
)	O
.	O

However	O
,	O
acquisition	O
of	O
background	O
knowledge	O
from	O
static	O
training	O
corpora	O
is	O
limiting	O
for	O
two	O
reasons	O
.	O

First	O
,	O
it	O
is	O
unreasonable	O
to	O
expect	O
that	O
all	O
background	O
knowledge	O
that	O
could	O
be	O
important	O
for	O
solving	O
an	O
NLU	S-Task
task	O
can	O
be	O
extracted	O
from	O
a	O
limited	O
amount	O
of	O
training	O
data	O
.	O

Second	O
,	O
as	O
the	O
world	O
changes	O
,	O
the	O
facts	O
that	O
may	O
influence	O
how	O
a	O
text	O
is	O
understood	O
will	O
likewise	O
change	O
.	O

In	O
short	O
:	O
building	O
suitably	O
large	O
corpora	O
to	O
capture	O
all	O
relevant	O
information	O
,	O
and	O
keeping	O
the	O
corpus	O
and	O
derived	O
models	O
up	O
to	O
date	O
with	O
changes	O
to	O
the	O
world	O
would	O
be	O
impractical	O
.	O

