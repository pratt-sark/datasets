document O
: O
Quantized Method
Densely Method
Connected Method
U Method
- Method
Nets Method
for O
Efficient O
Landmark Task
Localization Task
In O
this O
paper O
, O
we O
propose O
quantized O
densely O
connected O
U Method
- Method
Nets Method
for O
efficient O
visual Task
landmark Task
localization Task
. O
The O
idea O
is O
that O
features O
of O
the O
same O
semantic O
meanings O
are O
globally O
reused O
across O
the O
stacked Method
U Method
- Method
Nets Method
. O
This O
dense O
connectivity O
largely O
improves O
the O
information O
flow O
, O
yielding O
improved O
localization Metric
accuracy Metric
. O
However O
, O
a O
vanilla Method
dense Method
design Method
would O
suffer O
from O
critical Metric
efficiency Metric
issue Metric
in O
both O
training Task
and O
testing Task
. O
To O
solve O
this O
problem O
, O
we O
first O
propose O
order Method
- Method
K Method
dense Method
connectivity Method
to O
trim O
off O
long O
- O
distance O
shortcuts O
; O
then O
, O
we O
use O
a O
memory O
- O
efficient O
implementation O
to O
significantly O
boost O
the O
training Task
efficiency O
and O
investigate O
an O
iterative Method
refinement Method
that O
may O
slice O
the O
model O
size O
in O
half O
. O
Finally O
, O
to O
reduce O
the O
memory Metric
consumption Metric
and O
high O
precision O
operations O
both O
in O
training Task
and O
testing Task
, O
we O
further O
quantize O
weights O
, O
inputs O
, O
and O
gradients O
of O
our O
localization Method
network Method
to O
low O
bit O
- O
width O
numbers O
. O
We O
validate O
our O
approach O
in O
two O
tasks O
: O
human Task
pose Task
estimation Task
and O
face Task
alignment Task
. O
The O
results O
show O
that O
our O
approach O
achieves O
state O
- O
of O
- O
the O
- O
art O
localization Metric
accuracy Metric
, O
but O
using O
70 O
% O
fewer O
parameters O
, O
98 O
% O
less O
model Metric
size Metric
and O
saving O
75 O
% O
training Task
memory O
compared O
with O
other O
benchmark O
localizers O
. O
The O
code O
is O
available O
at O
. O
section O
: O
Introduction O
Locating Task
visual Task
landmarks Task
, O
such O
as O
human O
body O
joints O
and O
facial O
key O
points O
, O
is O
an O
important O
yet O
challenging O
problem O
. O
The O
stacked Method
U Method
- Method
Nets Method
, O
e.g. O
hourglasses Method
( O
HGs Method
) O
, O
are O
widely O
used O
in O
landmark Task
localization Task
. O
Generally O
speaking O
, O
their O
success O
can O
be O
attributed O
to O
design O
patterns O
: O
1 O
) O
within O
each O
U Method
- Method
Net Method
, O
connect O
the O
top O
- O
down O
and O
bottom O
- O
up O
feature O
blocks O
to O
encourage O
gradient O
flow O
; O
and O
2 O
) O
stack O
multiple O
U Method
- Method
Nets Method
in O
a O
cascade O
to O
refine O
prediction Task
stage O
by O
stage O
. O
However O
, O
the O
shortcut O
connection O
exists O
only O
“ O
locally O
” O
inside O
each O
U Method
- Method
Net Method
. O
There O
is O
no O
“ O
global O
” O
connection O
across O
U Method
- Method
Nets Method
except O
the O
cascade O
. O
Blocks O
in O
different O
U Method
- Method
Nets Method
can O
not O
share O
features O
, O
which O
may O
impede O
the O
information O
flow O
and O
lead O
to O
redundant O
parameters O
. O
We O
propose O
densely O
connected O
U Method
- Method
Nets Method
( O
DU Method
- Method
Net Method
) O
to O
address O
this O
issue O
. O
The O
key O
idea O
is O
to O
directly O
connect O
blocks O
of O
the O
same O
semantic O
meanings O
, O
i.e. O
having O
the O
same O
resolution O
in O
either O
top O
- O
down O
or O
bottom O
- O
up O
context O
, O
from O
any O
U Method
- Method
Net Method
to O
all O
subsequent O
U Method
- Method
Nets Method
. O
Please O
refer O
to O
Fig O
. O
[ O
reference O
] O
for O
an O
illustration O
. O
The O
dense O
connectivity O
is O
similar O
to O
DenseNet Method
but O
generalizing O
the O
design O
philosophy O
from O
feature O
to O
semantic O
level O
. O
It O
encourages O
information O
flow O
as O
well O
as O
feature O
reuse O
“ O
globally O
” O
across O
the O
stacked Method
U Method
- Method
Nets Method
, O
yielding O
improved O
localization Metric
accuracy Metric
. O
Yet O
there O
are O
critical O
issues O
in O
designing O
DU Method
- Method
Net Method
: O
1 O
) O
The O
number O
of O
parameters O
would O
have O
a O
quadratic O
growth O
since O
stacked Method
U Method
- Method
Nets Method
could O
generate O
connections O
. O
2 O
) O
A O
naive O
implementation O
may O
allocate O
new O
memory O
for O
every O
connection O
, O
making O
the O
training Task
highly O
expensive O
and O
limiting O
the O
maximum O
depth O
of O
DU Method
- Method
Nets Method
. O
Our O
solution O
to O
those O
efficiency O
issues O
is O
threefold O
. O
First O
, O
instead O
of O
connecting O
all O
stacked Method
U Method
- Method
Nets Method
, O
we O
only O
connect O
a O
U Method
- Method
Net Method
to O
its O
successors O
. O
We O
name O
it O
as O
the O
- O
connectivity O
, O
which O
aims O
to O
balance O
the O
fitting Metric
accuracy Metric
and O
parameter Metric
efficiency Metric
by O
cutting O
off O
long O
- O
distance O
connections O
. O
Second O
, O
we O
employ O
a O
memory Method
- Method
efficient Method
implementation Method
in O
training Task
. O
The O
key O
idea O
is O
to O
reuse O
a O
pre O
- O
allocated O
memory O
so O
all O
connected O
blocks O
could O
share O
the O
same O
memory O
. O
Compared O
with O
the O
naive O
implementation O
, O
this O
strategy O
makes O
it O
possible O
to O
train O
a O
very O
deep Method
DU Method
- Method
Net Method
( O
actually O
, O
deeper O
) O
. O
Third O
, O
to O
further O
improve O
the O
efficiency O
, O
we O
investigate O
an O
iterative Method
design Method
that O
may O
reduce O
the O
model Metric
size Metric
to O
one O
half O
. O
More O
specifically O
, O
the O
output O
of O
the O
first O
pass O
of O
the O
DU Method
- Method
Net Method
is O
used O
as O
the O
input O
of O
the O
second O
pass O
, O
where O
detection Method
or Method
regression Method
loss Method
is O
applied O
as O
supervision O
. O
Besides O
shrinking O
the O
number O
of O
network O
parameters O
, O
we O
also O
study O
to O
further O
quantize O
each O
parameter O
. O
This O
motivates O
from O
the O
ubiquitous Task
mobile Task
applications Task
. O
Although O
current O
mobile O
devices O
could O
carry O
models O
of O
dozens O
of O
MBs O
, O
deploying O
such O
networks O
requires O
high O
- O
end O
GPUs Method
. O
However O
, O
quantized Method
models Method
could O
be O
accelerated O
by O
some O
specifically O
designed O
low Method
- Method
cost Method
hardwares Method
. O
Beyond O
only O
deploying O
models O
on O
mobile O
devices O
, O
training Task
deep Method
neural Method
networks Method
on O
distributed O
mobile O
devices O
emerges O
recently O
. O
To O
this O
end O
, O
we O
also O
try O
to O
quantize O
not O
only O
the O
model O
parameters O
but O
also O
its O
inputs O
( O
intermediate O
features O
) O
and O
gradients O
in O
training Task
. O
This O
is O
the O
first O
attempt O
to O
investigate O
training Task
landmark Task
localizers Task
using O
quantized O
inputs O
and O
gradients O
. O
In O
summary O
, O
our O
key O
contributions O
are O
: O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
propose O
quantized O
densely O
connected O
U Method
- Method
Nets Method
for O
visual Task
landmark Task
localization Task
, O
which O
largely O
improves O
the O
information Metric
flow Metric
and O
feature Metric
reuse Metric
at O
the O
semantic O
level O
. O
We O
propose O
the O
- O
connectivity O
to O
balance O
accuracy Metric
and O
efficiency Metric
. O
It O
decreases O
the O
growth O
of O
model Metric
size Metric
from O
quadratic O
to O
linear O
by O
removing O
trivial O
connections O
. O
Experiments O
show O
it O
could O
reduce O
70 O
% O
parameters O
of O
state O
- O
of O
- O
the O
- O
art O
landmark Method
localizers Method
. O
Very O
deep O
U Method
- Method
Nets Method
can O
be O
trained O
using O
a O
memory Method
- Method
efficient Method
implementation Method
, O
where O
pre O
- O
allocated O
memory O
is O
reused O
by O
all O
connected O
blocks O
. O
We O
further O
investigate O
an O
iterative Method
refinement Method
that O
may O
cut O
down O
half O
of O
the O
model Metric
size Metric
, O
by O
forwarding O
DU Method
- Method
Net Method
twice O
using O
either O
detection Method
or Method
regression Method
supervision Method
. O
Different O
from O
previous O
efforts O
of O
quantizing O
only O
the O
model O
parameters O
, O
we O
are O
the O
first O
to O
quantize O
their O
inputs O
and O
gradients O
for O
better O
training Task
efficiency O
on O
landmark Task
localization Task
tasks Task
. O
By O
choosing O
appropriate O
quantization O
bit O
- O
widths O
for O
weights O
, O
inputs O
and O
gradients O
, O
quantized Method
DU Method
- Method
Net Method
achieves O
75 O
% O
training Task
memory O
saving O
with O
comparable O
performance O
. O
Exhaustive O
experiments O
are O
performed O
to O
validate O
DU Method
- Method
Net Method
in O
different O
aspects O
. O
In O
both O
human Task
pose Task
estimation Task
and O
face Task
alignment Task
, O
DU Method
- Method
Net Method
demonstrates O
comparable O
localization Metric
accuracy Metric
and O
use O
2 O
% O
model Metric
size Metric
compared O
with O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
section O
: O
Related O
Work O
In O
this O
section O
, O
we O
review O
the O
recent O
developments O
on O
designing O
convolutional Method
network Method
architectures Method
, O
quantizing Method
the Method
neural Method
networks Method
, O
human Task
pose Task
estimation Task
and O
facial Task
landmark Task
localization Task
. O
Network Method
Architecture Method
. O
The O
identity O
mappings O
make O
it O
possible O
to O
train O
very O
deep Task
ResNet Task
. O
The O
popular O
stacked O
U Method
- Method
Nets Method
are O
designed O
based O
on O
the O
residual Method
modules Method
. O
More O
recently O
, O
the O
DenseNet Method
outperforms O
the O
ResNet Method
in O
the O
image Task
classification Task
task Task
, O
benefitting O
from O
its O
dense O
connections O
. O
We O
would O
like O
to O
use O
the O
dense O
connectivity O
into O
multiple O
U Method
- Method
Nets Method
. O
Network Task
Quantization Task
. O
Training O
deep Method
neural Method
networks Method
usually O
consumes O
a O
large O
amount O
of O
computational O
resources O
, O
which O
makes O
it O
hard O
to O
deploy O
on O
mobile O
devices O
. O
Recently O
, O
network Method
quantization Method
approaches Method
offer O
an O
efficient O
solution O
to O
reduce O
the O
size O
of O
network O
through O
cutting O
down O
high O
precision O
operations O
and O
operands O
. O
In O
the O
recent O
binarized Method
convolutional Method
landmark Method
localizer Method
( O
BCLL Method
) O
architecture O
, O
XNOR Method
- Method
Net Method
was O
utilized O
for O
network Task
binarization Task
. O
However O
, O
BCLL Method
only O
quantizes O
weights O
for O
inference Task
and O
bring O
in O
real O
- O
value O
scaling O
factors O
. O
Due O
to O
its O
high O
precision O
demand O
in O
training Task
, O
it O
can O
not O
save O
training Task
memory O
and O
improve O
training Task
efficiency O
. O
To O
this O
end O
, O
we O
explore O
to O
quantize O
our O
DU Method
- Method
Net Method
in O
training Task
and O
inference Task
simultaneously O
. O
Human Task
Pose Task
Estimation Task
. O
Starting O
from O
the O
DeepPose Method
, O
CNNs Method
based Method
approaches Method
become O
the O
mainstream O
in O
human Task
pose Task
estimation Task
and O
prediction O
. O
Recently O
, O
the O
architecture O
of O
stacked O
hourglasses Method
has O
obviously O
beaten O
all O
the O
previous O
ones O
in O
terms O
of O
usability Metric
and O
accuracy Metric
. O
Therefore O
, O
all O
recent O
state O
- O
of O
- O
the O
- O
art O
methods O
build O
on O
its O
architecture O
. O
They O
replace O
the O
residual Method
modules Method
with O
more O
sophisticated O
ones O
, O
add O
graphical Method
models Method
to O
get O
better O
inference Task
, O
or O
use O
an O
additional O
network O
to O
provide O
adversarial O
supervisions O
or O
do O
adversarial Task
data Task
augmentation Task
. O
In O
contrast O
, O
we O
design O
a O
simple O
yet O
very O
effective O
connectivity O
pattern O
for O
stacked Method
U Method
- Method
Nets Method
. O
Facial Task
Landmark Task
Localization Task
. O
Similarly O
, O
CNNs Method
have O
largely O
reshaped O
the O
field O
of O
facial Task
landmark Task
localization Task
. O
Traditional O
methods O
could O
be O
easily O
outperformed O
by O
the O
CNNs Method
based Method
. O
In O
the O
recent O
Menpo Task
Facial Task
Landmark Task
Localization Task
Challenge Task
, O
stacked O
hourglasses Method
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
The O
proposed Method
- Method
connected Method
U Method
- Method
Nets Method
could O
produce O
even O
better O
results O
but O
with O
much O
fewer O
parameters O
. O
section O
: O
Our O
Method O
In O
this O
section O
, O
we O
first O
introduce O
the O
DU Method
- Method
Net Method
after O
recapping O
the O
stacked Method
U Method
- Method
Nets Method
. O
Then O
we O
present O
the O
- Method
connectivity Method
to O
improve O
its O
parameter Metric
efficiency Metric
, O
an O
efficient O
implementation O
to O
reduce O
its O
training Task
memory O
, O
and O
an O
iterative Method
refinement Method
to O
make O
it O
more O
parameter O
efficient O
. O
Finally O
, O
network Method
quantization Method
is O
utilized O
to O
further O
reduce O
training Task
memory O
and O
model Metric
size Metric
. O
subsection O
: O
DU Method
- Method
Net Method
A O
U Method
- Method
Net Method
contains O
top O
- O
down O
, O
bottom O
- O
up O
blocks O
and O
skip O
connections O
between O
them O
. O
Suppose O
multiple O
U Method
- Method
Nets Method
are O
stacked O
together O
, O
for O
the O
top O
- O
down O
and O
bottom O
- O
up O
blocks O
in O
the O
U Method
- Method
Net Method
, O
we O
use O
and O
to O
denote O
their O
non Method
- Method
linear Method
transformations Method
. O
Their O
outputs O
are O
represented O
by O
and O
. O
and O
comprise O
operations O
of O
Convolution Method
( O
Conv Method
) O
, O
Batch Method
Normalization Method
( O
BN Method
) O
, O
rectified Method
linear Method
units Method
( O
ReLU Method
) Method
, O
and O
pooling Method
. O
Stacked Method
U Method
- Method
Nets Method
. O
The O
feature O
transitions O
at O
the O
top O
- O
down O
and O
bottom O
- O
up O
blocks O
of O
the O
U Method
- Method
Net Method
are O
: O
The O
skip O
connections O
only O
exist O
locally O
within O
each O
U Method
- Method
Net Method
, O
which O
may O
restrict O
that O
information O
flows O
across O
U Method
- Method
Nets Method
. O
DU Method
- Method
Net Method
. O
To O
make O
information O
flow O
efficiently O
across O
stacked Method
U Method
- Method
Nets Method
, O
we O
propose O
a O
global O
connectivity O
pattern O
. O
Blocks O
at O
the O
same O
locations O
of O
different O
U Method
- Method
Nets Method
have O
direct O
connections O
. O
Hence O
, O
we O
refer O
to O
this O
densely O
connected O
U Method
- Method
Nets Method
architecture O
as O
DU Method
- Method
Net Method
. O
Figure O
[ O
reference O
] O
gives O
an O
illustration O
. O
Mathematically O
, O
the O
feature O
transitions O
at O
the O
top O
- O
down O
and O
bottom O
- O
up O
blocks O
of O
the O
U Method
- Method
Net Method
can O
be O
formulated O
as O
: O
where O
are O
the O
outputs O
of O
the O
top O
- O
down O
blocks O
in O
all O
preceding O
U Method
- Method
Nets Method
. O
Similarly O
, O
represent O
the O
outputs O
from O
the O
bottom O
- O
up O
blocks O
. O
denotes O
the O
feature O
concatenation O
, O
which O
could O
make O
information O
flow O
more O
efficiently O
than O
the O
summation O
operation O
in O
Equation O
[ O
reference O
] O
. O
According O
to O
Equation O
[ O
reference O
] O
, O
a O
block O
receives O
features O
not O
only O
from O
connected O
blocks O
in O
the O
current O
U Method
- Method
Net Method
but O
also O
the O
output O
features O
of O
the O
same O
semantic O
blocks O
from O
all O
its O
preceding O
U Method
- Method
Nets Method
. O
Note O
that O
this O
semantic O
level O
dense O
connectivity O
is O
a O
generalization O
of O
the O
dense O
connectivity O
in O
DenseNet Method
that O
connects O
layers O
only O
within O
each O
block O
. O
subsection O
: O
- O
Connectivity O
In O
the O
above O
formulation O
of O
DU Method
- Method
Net Method
, O
we O
connect O
blocks O
with O
the O
same O
semantic O
meanings O
across O
all O
U Method
- Method
Nets Method
. O
The O
connections O
would O
have O
quadratic O
growth O
depth O
- O
wise O
. O
To O
make O
DU Method
- Method
Net Method
parameter O
efficient O
, O
we O
propose O
to O
cut O
off O
some O
trivial O
connections O
. O
For O
compensation Task
, O
we O
add O
an O
intermediate O
supervision O
at O
the O
end O
of O
each O
U Method
- Method
Net Method
. O
The O
intermediate O
supervisions O
, O
as O
the O
skip O
connections O
, O
could O
also O
alleviate O
the O
gradient Task
vanish Task
problem Task
. O
Mathematically O
, O
the O
features O
and O
in O
Equation O
[ O
reference O
] O
turns O
into O
where O
represents O
how O
many O
preceding O
nearby O
U Method
- Method
Nets Method
connect O
with O
the O
current O
one O
. O
or O
would O
result O
in O
the O
stacked Method
U Method
- Method
Nets Method
or O
fully O
densely O
connected O
U Method
- Method
Nets Method
. O
A O
medium O
order O
could O
reduce O
the O
growth O
of O
DU Method
- Method
Net Method
parameters O
from O
quadratic O
to O
linear O
. O
Therefore O
, O
it O
largely O
improves O
the O
parameter Metric
efficiency Metric
of O
DU Method
- Method
Net Method
and O
could O
make O
DU Method
- Method
Net Method
grow O
several O
times O
deeper O
. O
The O
proposed O
- O
connection O
has O
similar O
philosophy O
as O
the O
Variable Method
Order Method
Markov Method
( Method
VOM Method
) Method
models Method
. O
Each O
U Method
- Method
Net Method
can O
be O
viewed O
as O
a O
state O
in O
the O
Markov Method
model Method
. O
The O
current O
U Method
- Method
Net Method
depends O
on O
a O
fixed O
number O
of O
preceding O
nearby O
U Method
- Method
Nets Method
, O
instead O
of O
preceding O
either O
only O
one O
or O
all O
U Method
- Method
Nets Method
. O
In O
this O
way O
, O
the O
long O
- O
range O
connections O
are O
cut O
off O
. O
Figure O
[ O
reference O
] O
illustrates O
connections O
of O
three O
different O
orders O
. O
In O
Figure O
[ O
reference O
] O
, O
the O
connections O
above O
the O
central O
axes O
follow O
VOM O
patterns O
of O
- O
, O
- O
and O
- O
whereas O
the O
central O
axes O
together O
with O
connections O
below O
them O
follow O
VOM O
patterns O
of O
- O
, O
- O
and O
- O
. O
Dense O
connectivity O
is O
a O
special O
case O
of O
- O
connectivity O
on O
the O
limit O
of O
. O
For O
small O
, O
- O
connectivity O
is O
much O
more O
parameter O
efficient O
. O
But O
fewer O
connections O
may O
affect O
the O
prediction Metric
accuracy Metric
of O
very O
deep Method
DU Method
- Method
Net Method
. O
To O
make O
DU Method
- Method
Net Method
have O
both O
high O
parameter Metric
efficiency Metric
and O
prediction Metric
accuracy Metric
, O
we O
propose O
to O
use O
- O
connectivity O
in O
conjunction O
with O
intermediate O
supervisions O
. O
In O
contrast O
, O
DenseNet Method
has O
only O
one O
supervision O
at O
the O
end O
. O
Thus O
, O
it O
can O
not O
effectively O
take O
advantage O
of O
- O
connectivity O
. O
subsection O
: O
Memory Method
Efficient Method
Implementation Method
Benefitting O
from O
the O
- O
connectivity O
, O
our O
DU Method
- Method
Net Method
is O
quite O
parameter O
efficient O
. O
However O
, O
a O
naive O
implementation O
would O
prevent O
from O
training Task
very O
deep Method
DU Method
- Method
Net Method
, O
since O
every O
connection O
would O
make O
a O
copy O
of O
input O
features O
. O
To O
reduce O
the O
training Task
memory O
, O
we O
follow O
the O
efficient O
implementation O
. O
More O
specifically O
, O
concatenation O
operations O
of O
the O
same O
semantic O
blocks O
in O
all O
U Method
- Method
Nets Method
share O
a O
memory Method
allocation Method
and O
their O
subsequent O
batch Method
norm Method
operations Method
share O
another O
memory Method
allocation Method
. O
Suppose O
a O
DU Method
- Method
Net Method
includes O
U Method
- Method
Nets Method
each O
of O
which O
has O
top O
- O
down O
blocks O
and O
bottom O
- O
up O
blocks O
. O
We O
need O
to O
pre O
- O
allocate O
two O
memory O
space O
for O
each O
of O
semantic O
blocks O
. O
For O
the O
top O
- O
down O
blocks O
, O
the O
concatenated O
features O
share O
the O
same O
memory O
space O
. O
Similarly O
, O
the O
concatenated O
features O
in O
the O
bottom O
- O
up O
blocks O
share O
the O
same O
memory O
space O
. O
In O
one O
shared Task
memory Task
allocation Task
, O
later O
produced O
features O
would O
overlay O
the O
former O
features O
. O
Thus O
, O
the O
concatenations O
and O
their O
subsequent O
batch Method
norm Method
operations Method
require O
to O
be O
re O
- O
computed O
in O
backward Task
phase Task
. O
Figure O
[ O
reference O
] O
illustrates O
naive O
and O
efficient O
implementations O
. O
subsection O
: O
Iterative Method
Refinement Method
In O
order O
to O
further O
improve O
the O
parameter Metric
efficiency Metric
of O
DU Method
- Method
Net Method
, O
we O
consider O
an O
iterative Method
refinement Method
. O
It O
uses O
only O
half O
of O
a O
DU Method
- Method
Net Method
but O
may O
achieve O
comparable O
performance O
. O
In O
the O
iterative Method
refinement Method
, O
a O
DU Method
- Method
Net Method
has O
two O
forward O
passes O
. O
In O
the O
first O
pass O
, O
we O
concatenate O
the O
inputs O
of O
the O
first O
and O
last O
U Method
- Method
Nets Method
and O
merge O
them O
in O
a O
small O
dense O
block O
. O
Then O
the O
refined O
input O
is O
fed O
forward O
in O
the O
DU Method
- Method
Net Method
again O
. O
Better O
output O
is O
expected O
because O
of O
the O
refined O
input O
. O
In O
this O
iterative Method
pipeline Method
, O
the O
DU Method
- Method
Net Method
has O
two O
groups O
of O
supervisions O
in O
the O
first O
and O
second O
iterations O
. O
Both O
the O
detection Method
and Method
regression Method
supervisions Method
are O
already O
used O
in O
the O
landmark Task
detection Task
tasks Task
. O
However O
, O
there O
is O
no O
investigation O
how O
they O
compare O
with O
each O
other O
. O
To O
this O
end O
, O
we O
could O
try O
different O
combinations O
of O
detection Method
and Method
regression Method
supervisions Method
for O
two O
iterations O
. O
Our O
comparison O
could O
give O
some O
guidance O
for O
future O
research O
. O
subsection O
: O
Network Method
Quantization Method
We O
aim O
at O
cutting O
down O
high O
precision O
operations O
and O
parameters O
both O
in O
training Task
and O
inference Task
stages Task
of Task
DU Task
- Task
Net Task
. O
The O
bit O
- O
width O
of O
weights O
can O
be O
reduced O
to O
one O
or O
two O
bits O
through O
sign O
function O
or O
symmetrical O
threshold O
, O
whereas O
the O
layerwise O
gradients O
and O
inputs O
are O
quantized O
with O
linear Method
mapping Method
. O
In O
previous O
XNOR Method
- Method
Net Method
, O
a O
scaling O
factor O
was O
introduced O
to O
approximate O
the O
real O
- O
value O
weight O
. O
However O
, O
calculating O
these O
float O
factor O
costs O
additional O
computational Metric
resources Metric
. O
To O
further O
decrease O
memory Metric
usage Metric
and O
model Metric
size Metric
, O
we O
try O
to O
remove O
the O
scaling O
factor O
and O
follow O
WAGE O
to O
quantize O
dataflow O
during O
training Task
. O
More O
specifically O
, O
weights O
are O
binarized O
to O
- O
1 O
and O
1 O
by O
the O
following O
equation O
: O
or O
ternarized O
to O
- O
1 O
, O
0 O
and O
- O
1 O
by O
the O
a O
positive O
threshold O
as O
presented O
, O
where O
provided O
that O
is O
initialized O
by O
Gaussian Method
distributions Method
. O
The O
dataflows O
, O
i.e. O
gradients O
and O
inputs O
, O
are O
quantized O
to O
- O
bit O
values O
by O
the O
following O
linear Method
mapping Method
function Method
: O
Here O
, O
the O
unit O
distance O
is O
calculated O
by O
. O
In O
the O
following O
experiments O
, O
we O
explore O
different O
combinations O
of O
bit O
- O
widths O
to O
balance O
performance O
and O
memory Metric
consumption Metric
. O
section O
: O
Experiments O
In O
this O
section O
, O
we O
first O
demonstrate O
the O
effectiveness O
of O
DU Method
- Method
Net Method
through O
its O
comparison O
with O
the O
stacked Method
U Method
- Method
Nets Method
. O
Then O
we O
explore O
the O
relation O
between O
the O
prediction Metric
accuracy Metric
and O
- O
connectivity O
. O
After O
that O
, O
we O
evaluate O
the O
iterative Method
refinement Method
to O
halve O
DU Method
- Method
Net Method
parameters O
. O
Finally O
, O
we O
test O
the O
network Method
quantization Method
. O
Different O
combinations O
of O
bit O
- O
widths O
to O
find O
appropriate O
ones O
which O
balance O
accuracy Metric
, O
model Metric
size Metric
and O
memory Metric
consumption Metric
. O
The O
general O
comparisons O
are O
given O
at O
last O
. O
Some O
qualitative O
results O
are O
shown O
in O
Figure O
[ O
reference O
] O
. O
Network Method
. O
The O
input O
resolution O
is O
normalized O
to O
256 O
256 O
. O
Before O
the O
DU Method
- Method
Net Method
, O
a O
Conv Method
( Method
) Method
filter Method
with O
stride O
2 O
and O
a O
max Method
pooling Method
would O
produce O
128 O
features O
with O
resolution O
64 O
64 O
. O
Hence O
, O
the O
maximum O
resolution O
of O
DU Method
- Method
Net Method
is O
64 O
64 O
. O
Each O
block O
in O
DU Method
- Method
Net Method
has O
a O
bottleneck O
structure O
as O
shown O
on O
the O
right O
side O
of O
Figure O
[ O
reference O
] O
. O
At O
the O
beginning O
of O
each O
bottleneck O
, O
features O
from O
different O
connections O
are O
concatenated O
and O
stored O
in O
a O
shared O
memory O
. O
Then O
the O
concatenated O
features O
are O
compressed O
by O
the O
Conv Method
( Method
) Method
to O
128 O
features O
. O
At O
last O
, O
the O
Conv Method
( O
) O
further O
produces O
32 O
new O
features O
. O
The O
batch Method
norm Method
and O
ReLU Method
are O
used O
before O
the O
convolutions Method
. O
Training Task
. O
We O
implement O
the O
DU Method
- Method
Net Method
using O
the O
PyTorch Method
. O
The O
DU Method
- Method
Net Method
is O
trained O
by O
the O
optimizer Method
RMSprop Method
. O
When O
training Task
human Task
pose Task
estimators Task
, O
the O
initial O
learning Metric
rate Metric
is O
which O
is O
decayed O
to O
after O
100 O
epochs O
. O
The O
whole O
training Task
takes O
200 O
epochs O
. O
The O
facial Method
landmark Method
localizers Method
are O
easier O
to O
train O
. O
Also O
starting O
from O
, O
its O
learning Metric
rate Metric
is O
divided O
by O
5 O
, O
2 O
and O
2 O
at O
epoch O
30 O
, O
60 O
and O
90 O
respectively O
. O
The O
above O
settings O
remain O
the O
same O
for O
quantized Method
DU Method
- Method
Net Method
. O
In O
order O
to O
match O
the O
pace O
of O
dataflow O
, O
we O
set O
the O
same O
bit O
- O
width O
for O
gradients O
and O
inputs O
. O
We O
quantize O
dataflows O
and O
parameters O
all O
over O
the O
DU Method
- Method
Net Method
except O
the O
first O
and O
last O
convolutional O
layers O
, O
since O
localization Task
is O
a O
fine Task
- Task
grained Task
task Task
requires O
high O
precision Metric
of O
heatmaps O
. O
Human O
Pose O
Datasets O
. O
We O
use O
two O
benchmark O
human Task
pose Task
estimation Task
datasets O
: O
MPII Material
Human Material
Pose Material
and O
Leeds Material
Sports Material
Pose Material
( O
LSP Material
) O
. O
The O
MPII Material
is O
collected O
from O
YouTube Material
videos Material
with O
a O
broad O
range O
of O
human O
activities O
. O
It O
has O
25 O
K O
images O
and O
40 O
K O
annotated O
persons O
, O
which O
are O
split O
into O
a O
training Task
set O
of O
29 O
K O
and O
a O
test O
set O
of O
11K. O
Following O
, O
3 O
K O
samples O
are O
chosen O
from O
the O
training Task
set O
as O
validation O
set O
. O
Each O
person O
has O
16 O
labeled O
joints O
. O
The O
LSP Material
dataset O
contains O
images O
from O
many O
sport O
scenes O
. O
Its O
extended O
version O
has O
11 O
K O
training Task
samples O
and O
1 O
K O
testing O
samples O
. O
Each O
person O
in O
LSP Material
has O
14 O
labeled O
joints O
. O
Since O
there O
are O
usually O
multiple O
people O
in O
one O
image O
, O
we O
crop O
around O
each O
person O
and O
resize O
it O
to O
256x256 O
. O
We O
also O
use O
scaling O
( O
0.75 O
- O
1.25 O
) O
, O
rotation O
( O
-/+ O
30 O
) O
and O
random O
flip O
to O
augment O
the O
data O
. O
Facial O
Landmark O
Datasets O
. O
The O
experiments O
of O
the O
facial Task
lanmark Task
localization Task
are O
conducted O
on O
the O
composite O
of O
HELEN Material
, O
AFW Material
, O
LFPW Material
and O
IBUG Material
which O
are O
re O
- O
annotated O
in O
the O
300 Task
- Task
W Task
challenge Task
. O
Each O
face O
has O
68 O
landmarks O
. O
Following O
and O
, O
we O
use O
the O
training Task
images O
of O
HELEN Material
, O
LFPW Material
and O
all O
images O
of O
AFW Material
, O
totally O
3148 O
images O
, O
as O
the O
training Task
set O
. O
The O
testing O
is O
done O
on O
the O
common O
subset O
( O
testing O
images O
of O
HELEN Material
and O
LFPW Material
) O
, O
challenge O
subset O
( O
all O
images O
from O
IBUG Material
) O
and O
their O
union O
. O
We O
use O
the O
provided O
bounding O
boxes O
from O
the O
300 Task
- Task
W Task
challenge Task
to O
crop O
faces O
. O
The O
same O
augmentations O
of O
scaling O
and O
rotation O
as O
in O
human Task
pose Task
estimation Task
are O
applied O
. O
Metric O
. O
We O
use O
the O
standard O
metrics O
in O
both O
human Task
pose Task
estimation Task
and O
face Task
alignment Task
. O
Specifically O
, O
Percentage Metric
of Metric
Correct Metric
Keypoints Metric
( O
PCK Metric
) O
is O
used O
to O
evaluate O
approaches O
for O
human Task
pose Task
estimation Task
. O
And O
the O
normalized Metric
mean Metric
error Metric
( O
NME Metric
) O
is O
employed O
to O
measure O
the O
performance O
of O
localizing O
facial O
landmarks O
. O
Following O
the O
convention O
of O
300 Task
- Task
W Task
challenge Task
, O
we O
use O
the O
inter Metric
- Metric
ocular Metric
distance Metric
to O
normalize O
mean Metric
error Metric
. O
For O
network Task
quantization Task
, O
we O
propose O
the O
balance Metric
index Metric
( O
BI Metric
) O
to O
examine O
the O
trade O
- O
off O
between O
performance O
and O
efficiency Metric
. O
width=1 O
width=1 O
subsection O
: O
DU Method
- Method
Net Method
vs. O
Stacked Method
U Method
- Method
Nets Method
To O
demonstrate O
the O
advantages O
of O
DU Method
- Method
Net Method
, O
we O
first O
compare O
it O
with O
traditional O
stacked Method
U Method
- Method
Nets Method
. O
This O
experiment O
is O
done O
on O
the O
MPII Material
validation O
set O
. O
All O
DU Method
- Method
Nets Method
use O
the O
- O
connectivity O
and O
intermediate O
supervisions O
. O
Table O
[ O
reference O
] O
shows O
three O
pairs O
of O
comparisons O
with O
4 O
, O
8 O
and O
16 O
U Method
- Method
Nets Method
. O
Both O
their O
PCKh Method
and O
number O
of O
convolution O
parameters O
are O
reported O
. O
We O
could O
observe O
that O
, O
with O
the O
same O
number O
of O
U Method
- Method
Nets Method
, O
DU Method
- Method
Net Method
could O
obtain O
comparable O
or O
even O
better O
accuracy Metric
. O
More O
importantly O
, O
the O
number O
of O
parameters O
in O
DU Method
- Method
Net Method
is O
decreased O
by O
about O
70 O
% O
of O
that O
in O
stacked Method
U Method
- Method
Nets Method
. O
The O
feature O
reuse O
across O
U Method
- Method
Nets Method
make O
each O
U Method
- Method
Net Method
in O
DU Method
- Method
Net Method
become O
light O
- O
weighted O
. O
Besides O
, O
the O
high O
parameter Metric
efficiency Metric
makes O
it O
possible O
to O
train O
16 O
- O
connected O
U Method
- Method
Nets Method
in O
a O
12 O
G O
GPU O
with O
batch O
size O
16 O
. O
In O
contrast O
, O
training Task
16 O
stacked Method
U Method
- Method
Nets Method
is O
infeasible O
. O
Thus O
, O
- O
together O
with O
intermediate O
supervisions O
could O
make O
DU Method
- Method
Net Method
obtain O
accurate O
prediction Task
as O
well O
as O
high O
parameter Metric
efficiency Metric
, O
compared O
with O
stacked Method
U Method
- Method
Nets Method
. O
subsection O
: O
Evaluation Task
of Task
- Task
connectivity Task
The O
proposed O
- Method
connectivity Method
is O
key O
to O
improve O
the O
parameter Metric
efficiency Metric
of O
DU Method
- Method
Net Method
. O
In O
this O
experiment O
, O
we O
investigate O
how O
the O
PCKh O
and O
convolution O
parameter O
number O
change O
along O
with O
the O
order O
value O
. O
Figure O
[ O
reference O
] O
gives O
the O
results O
from O
MPII Material
validation O
set O
. O
The O
left O
and O
right O
figures O
show O
results O
of O
DU Method
- Method
Net Method
with O
8 O
and O
16 O
U Method
- Method
Nets Method
. O
It O
is O
clear O
that O
the O
convolution O
parameter O
number O
increases O
as O
the O
order O
becomes O
larger O
. O
However O
, O
the O
left O
and O
right O
PCKh O
curves O
have O
a O
similar O
shape O
of O
first O
increasing O
and O
then O
decreasing O
. O
- O
connectivity O
is O
always O
better O
than O
- O
. O
However O
, O
very O
dense O
connections O
may O
not O
be O
a O
good O
choice O
, O
which O
is O
kind O
of O
counter O
- O
intuitive O
. O
This O
is O
because O
the O
intermediate O
supervisions O
already O
provide O
additional O
gradients O
. O
Too O
dense O
connections O
make O
gradients O
accumulate O
too O
much O
, O
causing O
the O
overfitting O
of O
training Task
set O
. O
Further O
evidence O
of O
overfitting O
is O
shown O
in O
Table O
[ O
reference O
] O
. O
The O
- O
7 O
connectivity O
has O
the O
higher O
training Task
PCKh O
the O
- O
1 O
in O
all O
training Task
epochs O
. O
But O
its O
validation Method
PCKh Method
is O
a O
little O
lower O
in O
the O
last O
training Task
epochs O
. O
Thus O
, O
small O
orders O
are O
recommended O
in O
DU Method
- Method
Net Method
. O
width=1 O
width=1 O
subsection O
: O
Evaluation O
of O
Efficient O
Implementation O
The O
memory O
- O
efficient O
implementation O
makes O
it O
possible O
to O
train O
very O
deep Method
DU Method
- Method
Net Method
. O
Figure O
[ O
reference O
] O
shows O
the O
training Task
memory O
consumption O
of O
both O
naive O
and O
memory O
- O
efficient O
implementations O
of O
DU Method
- Method
Net Method
with O
order O
- O
1 O
connectivity O
. O
The O
linear O
growths O
of O
training Task
memory O
along O
with O
number O
of O
U Method
- Method
Nets Method
is O
because O
of O
the O
fixed O
order O
connectivity O
. O
But O
the O
memory Metric
growth Metric
of O
efficient O
implementation O
is O
much O
slower O
than O
that O
of O
the O
naive O
one O
. O
With O
batch O
size O
16 O
, O
we O
could O
train O
a O
DU Method
- Method
Net Method
with O
16 O
U Method
- Method
Nets Method
in O
12 O
GB O
GPU O
. O
Under O
the O
same O
setting O
, O
the O
naive O
implementation O
could O
accept O
only O
9 O
U Method
- Method
Nets Method
. O
subsection O
: O
Evaluation O
of O
Iterative Task
Refinement Task
The O
iterative Method
refinement Method
is O
designed O
to O
make O
DU Method
- Method
Net Method
more O
parameter O
efficient O
. O
First O
, O
experiments O
are O
done O
on O
the O
300 Material
- Material
W Material
dataset Material
using O
DU Method
- Method
Net Method
( O
4 O
) O
. O
Results O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
For O
both O
detection Task
and Task
regression Task
supervisions Task
, O
adding O
an O
iteration O
could O
lower O
the O
localization Metric
errors Metric
, O
demonstrating O
effectiveness O
of O
the O
iterative Method
refinement Method
. O
Meanwhile O
, O
the O
model O
parameters O
only O
increase O
0.2 O
M O
, O
making O
DU Method
- Method
Net Method
even O
more O
parameter O
efficient O
. O
Besides O
, O
the O
regression Method
supervision Method
outperforms O
the O
detection Method
one Method
no O
matter O
in O
the O
iterative Task
or Task
non Task
- Task
iterative Task
setting Task
, O
making O
it O
a O
better O
choice O
for O
landmark Task
localization Task
. O
Further O
, O
we O
compare O
iterative O
DU Method
- Method
Net Method
( O
4 O
) O
with O
non O
- O
iterative O
DU Method
- Method
Net Method
( O
8 O
) O
. O
Table O
[ O
reference O
] O
gives O
the O
comparison O
. O
We O
could O
find O
that O
, O
the O
iterative O
DU Method
- Method
Net Method
( O
4 O
) O
could O
obtain O
comparable O
NME Metric
as O
DU Method
- Method
Net Method
( O
8 O
) O
. O
However O
, O
DU Method
- Method
Net Method
( O
8 O
) O
has O
double O
parameters O
of O
DU Method
- Method
Net Method
( O
4 O
) O
whereas O
iterative O
DU Method
- Method
Net Method
( O
4 O
) O
increases O
only O
0.2 O
M O
additional O
parameters O
on O
DU Method
- Method
Net Method
( O
4 O
) O
. O
subsection O
: O
Evaluation O
of O
Network Task
Quantization Task
Through O
network Method
quantization Method
, O
high O
precision O
operations O
and O
parameters O
can O
be O
efficiently O
represented O
by O
a O
few O
discrete O
values O
. O
In O
order O
to O
find O
appropriate O
choices O
of O
bit O
- O
widths O
, O
we O
try O
a O
series O
of O
bit O
- O
width O
combinations O
on O
the O
300 Material
- Material
W Material
dataset Material
based O
on O
- O
DU Method
- Method
Net Method
( O
4 O
) O
. O
The O
performance O
and O
balance O
ability O
of O
these O
combinations O
on O
several O
methods O
are O
shown O
in O
Table O
[ O
reference O
] O
, O
where O
DU Method
- Method
Net Method
( O
4 O
) O
is O
DU Method
- Method
Net Method
with O
4 O
blocks O
, O
BW O
and O
TW O
respectively O
represents O
binarized O
weight O
and O
ternarized O
weight O
without O
, O
BW O
- O
is O
binarized O
weight O
with O
float O
scaling O
factor O
, O
the O
suffix O
QIG O
means O
quantized O
inputs O
and O
gradients O
. O
For O
mobile O
devices O
with O
limited O
computational O
resources O
, O
slightly O
performance O
drop O
is O
tolerable O
provided O
that O
corresponding O
large O
efficiency Metric
enhancement Metric
. O
For O
the O
evaluation O
purpose O
, O
we O
propose O
a O
balance Metric
index Metric
( O
BI Metric
) O
to O
better O
examine O
the O
trade O
- O
off O
between O
performance O
and O
efficiency Metric
: O
where O
and O
is O
respectively O
short O
for O
training Task
memory O
and O
model Metric
size Metric
compression Metric
ratios Metric
to O
the O
original O
network O
without O
quantization Method
. O
The O
square O
of O
is O
calculated O
in O
the O
above O
formula O
to O
emphasize O
the O
prior O
importance O
of O
performance O
. O
For O
BI Metric
, O
the O
smaller O
the O
value O
, O
the O
better O
the O
ability O
of O
balance O
. O
According O
to O
Table O
[ O
reference O
] O
, O
BW O
- O
QIG Method
( Method
818 Method
) O
could O
achieve O
the O
best O
balance O
between O
performance O
and O
model Metric
efficiency Metric
among O
all O
the O
combinations O
. O
BW Method
- Method
QIG Method
( Method
818 Method
) Method
could O
reduce O
more O
than O
4 O
training Task
memory O
and O
32 O
model O
size O
while O
reach O
a O
better O
performance O
than O
TSR Method
. O
Besides O
, O
BW O
- O
- O
QIG O
( O
818 O
) O
, O
BW O
- O
QIG O
( O
616 O
) O
and O
TW O
- O
QIG O
( O
626 O
) O
also O
have O
small Metric
balance Metric
index Metric
. O
Among O
all O
the O
combinations O
, O
the O
binarized Method
network Method
with O
scaling O
factor O
, O
i.e. O
BW O
- O
gets O
the O
closest O
error O
to O
the O
original O
network O
DU Method
- Method
Net Method
( O
4 O
) O
. O
For O
BW Method
- Method
- O
QIG O
( O
818 O
) O
, O
the O
performance O
is O
not O
better O
than O
BW Method
- O
QIG O
( O
818 O
) O
. O
This O
is O
mainly O
because O
that O
BW Method
- Method
is O
heavily O
rely O
on O
the O
parameter O
. O
However O
, O
the O
quantization Method
of Method
dataflow Method
could O
reduce O
the O
approximation Metric
ability Metric
of O
. O
TW Method
and O
TW O
- O
QIG Method
usually O
gets O
better O
results O
than O
BW Method
and O
BW Method
- Method
QIG Method
, O
since O
they O
have O
more O
choices O
in O
terms O
of O
weight O
value O
. O
The O
above O
results O
proves O
the O
effectiveness O
of O
network Method
quantization Method
, O
yet O
a O
correct O
combination O
of O
bit O
- O
widths O
is O
a O
crucial O
factor O
. O
subsection O
: O
Comparison O
with O
State O
- O
of O
- O
the O
- O
art O
Methods O
Human Task
Pose Task
Estimation Task
. O
Tables O
[ O
reference O
] O
and O
[ O
reference O
] O
show O
comparisons O
of O
human Task
pose Task
estimation Task
on O
MPII Material
and O
LSP Material
test Material
sets Material
. O
The O
- O
DU Method
- Method
Net Method
- O
BW O
- O
( O
16 O
) O
achieves O
comparable O
state O
- O
of O
- O
the O
- O
art O
performances O
. O
In O
contrast O
, O
as O
shown O
in O
Table O
[ O
reference O
] O
, O
it O
has O
only O
27% O
- O
62 O
% O
parameters O
and O
less O
than O
2 O
% O
model Metric
size Metric
of O
other O
recent O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
The O
DU Method
- Method
Net Method
is O
concise O
and O
simple O
. O
Other O
state O
- O
of O
- O
the O
- O
art O
methods O
use O
stacked Method
U Method
- Method
Nets Method
with O
either O
sophisticated Method
modules Method
, O
graphical Method
models Method
or O
adversarial Method
networks Method
. O
Facial Task
Landmark Task
Localization Task
. O
The O
DU Method
- Method
Net Method
is O
also O
compared O
with O
other O
state O
- O
of O
- O
the O
- O
art O
facial Method
landmark Method
localization Method
methods Method
on O
300 O
- O
W. O
Please O
refer O
to O
Table O
[ O
reference O
] O
. O
We O
uses O
a O
smaller O
network O
- O
1 O
DU Method
- Method
Net Method
( O
8 O
) O
than O
that O
in O
human Task
pose Task
estimation Task
, O
since O
localizing O
the O
facial O
landmarks O
is O
easier O
. O
The O
- O
1 O
DU Method
- Method
Net Method
- Method
BW Method
- Method
( O
8 O
) O
gets O
comparable O
errors O
state O
- O
of O
- O
the O
- O
art O
method O
. O
However O
, O
- O
1 O
DU Method
- Method
Net Method
- Method
BW Method
- O
( O
8 O
) O
has O
only O
2 O
% O
model O
size O
. O
section O
: O
Conclusion O
We O
have O
generalized O
the O
dense O
connectivity O
into O
the O
stacked Method
U Method
- Method
Nets Method
, O
resulting O
in O
a O
novel O
, O
simple O
and O
effective O
DU Method
- Method
Net Method
. O
It O
connects O
blocks O
with O
the O
same O
semantic O
meanings O
in O
different O
U Method
- Method
Nets Method
. O
- Method
connectivity Method
is O
proposed O
to O
improve O
its O
parameter Metric
efficiency Metric
. O
An O
iterative Method
refinement Method
is O
also O
introduced O
make O
it O
more O
parameter O
efficient O
. O
It O
could O
halve O
a O
DU Method
- Method
Net Method
but O
achieves O
comparable O
accuracy Metric
. O
Through O
network Method
quantization Method
, O
the O
training Task
memory O
consumption O
and O
model Metric
size Metric
can O
further O
be O
reduced O
simultaneously O
. O
Experiments O
show O
the O
DU Method
- Method
Net Method
could O
achieve O
state O
- O
of O
- O
the O
- O
art O
performances O
as O
other O
landmark Method
localizers Method
but O
with O
only O
30 O
% O
parameters O
, O
2 O
% O
model Metric
size Metric
and O
25 O
% O
training Task
memory O
. O
section O
: O
Acknowledgment O
This O
work O
is O
partly O
supported O
by O
the O
Air O
Force O
Office O
of O
Scientific O
Research O
( O
AFOSR O
) O
under O
the O
Dynamic O
Data O
- O
Driven O
Application O
Systems O
Program O
, O
NSF O
1763523 O
, O
1747778 O
, O
1733843 O
and O
1703883 O
Awards O
. O
bibliography O
: O
References O
