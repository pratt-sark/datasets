document O
: O
DR Method
- Method
BiLSTM Method
: O
Dependent Method
Reading Method
Bidirectional Method
LSTM Method
for O
Natural Task
Language Task
Inference Task
We O
present O
a O
novel O
deep Method
learning Method
architecture Method
to O
address O
the O
natural Task
language Task
inference Task
( O
NLI Task
) O
task O
. O
Existing O
approaches O
mostly O
rely O
on O
simple O
reading Method
mechanisms Method
for O
independent O
encoding O
of O
the O
premise O
and O
hypothesis O
. O
Instead O
, O
we O
propose O
a O
novel O
dependent Method
reading Method
bidirectional Method
LSTM Method
network Method
( O
DR Method
- Method
BiLSTM Method
) O
to O
efficiently O
model O
the O
relationship O
between O
a O
premise O
and O
a O
hypothesis O
during O
encoding Task
and O
inference Task
. O
We O
also O
introduce O
a O
sophisticated O
ensemble Method
strategy Method
to O
combine O
our O
proposed O
models O
, O
which O
noticeably O
improves O
final O
predictions O
. O
Finally O
, O
we O
demonstrate O
how O
the O
results O
can O
be O
improved O
further O
with O
an O
additional O
preprocessing O
step O
. O
Our O
evaluation O
shows O
that O
DR Method
- Method
BiLSTM Method
obtains O
the O
best O
single Method
model Method
and O
ensemble Method
model Method
results O
achieving O
the O
new O
state O
- O
of O
- O
the O
- O
art O
scores O
on O
the O
Stanford Material
NLI Material
dataset Material
. O
section O
: O
Introduction O
Natural Task
Language Task
Inference Task
( O
NLI Task
; O
a.k.a O
. O
Recognizing Task
Textual Task
Entailment Task
, O
or O
RTE Task
) Task
is O
an O
important O
and O
challenging O
task O
for O
natural Task
language Task
understanding Task
. O
The O
goal O
of O
NLI Task
is O
to O
identify O
the O
logical O
relationship O
( O
entailment O
, O
neutral O
, O
or O
contradiction O
) O
between O
a O
premise O
and O
a O
corresponding O
hypothesis O
. O
Table O
[ O
reference O
] O
shows O
few O
example O
relationships O
from O
the O
Stanford Material
Natural Material
Language Material
Inference Material
( O
SNLI Material
) O
dataset O
. O
Recently O
, O
NLI Task
has O
received O
a O
lot O
of O
attention O
from O
the O
researchers O
, O
especially O
due O
to O
the O
availability O
of O
large O
annotated O
datasets O
like O
SNLI Material
. O
Various O
deep Method
learning Method
models Method
have O
been O
proposed O
that O
achieve O
successful O
results O
for O
this O
task O
. O
Most O
of O
these O
existing O
NLI Task
models O
use O
attention Method
mechanism Method
to O
jointly O
interpret O
and O
align O
the O
premise O
and O
hypothesis O
. O
Such O
models O
use O
simple O
reading Method
mechanisms Method
to O
encode O
the O
premise O
and O
hypothesis O
independently O
. O
However O
, O
such O
a O
complex O
task O
require O
explicit O
modeling O
of O
dependency O
relationships O
between O
the O
premise O
and O
the O
hypothesis O
during O
the O
encoding Task
and Task
inference Task
processes Task
to O
prevent O
the O
network O
from O
the O
loss O
of O
relevant O
, O
contextual O
information O
. O
In O
this O
paper O
, O
we O
refer O
to O
such O
strategies O
as O
dependent Task
reading Task
. O
There O
are O
some O
alternative O
reading Method
mechanisms Method
available O
in O
the O
literature O
that O
consider O
dependency O
aspects O
of O
the O
premise O
- O
hypothesis O
relationships O
. O
However O
, O
these O
mechanisms O
have O
two O
major O
limitations O
: O
So O
far O
, O
they O
have O
only O
explored O
dependency O
aspects O
during O
the O
encoding Task
stage Task
, O
while O
ignoring O
its O
benefit O
during O
inference Task
. O
Such O
models O
only O
consider O
encoding O
a O
hypothesis O
depending O
on O
the O
premise O
, O
disregarding O
the O
dependency O
aspects O
in O
the O
opposite O
direction O
. O
We O
propose O
a O
dependent Method
reading Method
bidirectional Method
LSTM Method
( O
DR Method
- Method
BiLSTM Method
) O
model O
to O
address O
these O
limitations O
. O
Given O
a O
premise O
and O
a O
hypothesis O
, O
our O
model O
first O
encodes O
them O
considering O
dependency O
on O
each O
other O
( O
and O
) O
. O
Next O
, O
the O
model O
employs O
a O
soft Method
attention Method
mechanism Method
to O
extract O
relevant O
information O
from O
these O
encodings O
. O
The O
augmented Method
sentence Method
representations Method
are O
then O
passed O
to O
the O
inference Method
stage Method
, O
which O
uses O
a O
similar O
dependent Method
reading Method
strategy Method
in O
both O
directions O
, O
i.e. O
and O
. O
Finally O
, O
a O
decision O
is O
made O
through O
a O
multi Method
- Method
layer Method
perceptron Method
( Method
MLP Method
) O
based O
on O
the O
aggregated O
information O
. O
Our O
experiments O
on O
the O
SNLI Material
dataset O
show O
that O
DR Method
- Method
BiLSTM Method
achieves O
the O
best O
single Method
model Method
and O
ensemble Method
model Method
performance O
obtaining O
improvements O
of O
a O
considerable O
margin O
of O
and O
over O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
single Method
and Method
ensemble Method
models Method
, O
respectively O
. O
Furthermore O
, O
we O
demonstrate O
the O
importance O
of O
a O
simple O
preprocessing Method
step Method
performed O
on O
the O
SNLI Material
dataset O
. O
Evaluation O
results O
show O
that O
such O
preprocessing O
allows O
our O
single O
model O
to O
achieve O
the O
same O
accuracy Metric
as O
the O
state O
- O
of O
- O
the O
- O
art O
ensemble Method
model Method
and O
improves O
our O
ensemble Method
model Method
to O
outperform O
the O
state O
- O
of O
- O
the O
- O
art O
ensemble Method
model Method
by O
a O
remarkable O
margin O
of O
. O
Finally O
, O
we O
perform O
an O
extensive O
analysis O
to O
clarify O
the O
strengths O
and O
weaknesses O
of O
our O
models O
. O
section O
: O
Related O
Work O
Early O
studies O
use O
small O
datasets O
while O
leveraging O
lexical O
and O
syntactic O
features O
for O
NLI Task
. O
The O
recent O
availability O
of O
large O
- O
scale O
annotated O
datasets O
has O
enabled O
researchers O
to O
develop O
various O
deep Method
learning Method
- Method
based Method
architectures Method
for O
NLI Task
. O
google2016 O
propose O
an O
attention Method
- Method
based Method
model Method
that O
decomposes O
the O
NLI Task
task Task
into O
sub Task
- Task
problems Task
to O
solve O
them O
in O
parallel O
. O
They O
further O
show O
the O
benefit O
of O
adding O
intra O
- O
sentence O
attention O
to O
input O
representations O
. O
him2017 O
explore O
sequential Method
inference Method
models Method
based O
on O
chain Method
LSTMs Method
with O
attentional Method
input Method
encoding Method
and O
demonstrate O
the O
effectiveness O
of O
syntactic O
information O
. O
We O
also O
use O
similar O
attention Method
mechanisms Method
. O
However O
, O
our O
model O
is O
distinct O
from O
these O
models O
as O
they O
do O
not O
benefit O
from O
dependent Method
reading Method
strategies Method
. O
Rocktaschel2015 O
use O
a O
word Method
- Method
by Method
- Method
word Method
neural Method
attention Method
mechanism Method
while O
re O
- O
read O
propose O
re O
- O
read O
LSTM Method
units Method
by O
considering O
the O
dependency O
of O
a O
hypothesis O
on O
the O
information O
of O
its O
premise O
( O
) O
to O
achieve O
promising O
results O
. O
However O
, O
these O
models O
suffer O
from O
weak Method
inferencing Method
methods Method
by O
disregarding O
the O
dependency O
aspects O
from O
the O
opposite O
direction O
( O
) O
. O
Intuitively O
, O
when O
a O
human O
judges O
a O
premise O
- O
hypothesis O
relationship O
, O
s O
/ O
he O
might O
consider O
back O
- O
and O
- O
forth O
reading O
of O
both O
sentences O
before O
coming O
to O
a O
conclusion O
. O
Therefore O
, O
it O
is O
essential O
to O
encode O
the O
premise O
- O
hypothesis O
dependency O
relations O
from O
both O
directions O
to O
optimize O
the O
understanding O
of O
their O
relationship O
. O
ibm2017 O
propose O
a O
bilateral Method
multi Method
- Method
perspective Method
matching Method
( O
BiMPM Method
) O
model O
, O
which O
resembles O
the O
concept O
of O
matching O
a O
premise O
and O
hypothesis O
from O
both O
directions O
. O
Their O
matching Method
strategy Method
is O
essentially O
similar O
to O
our O
attention Method
mechanism Method
that O
utilizes O
relevant O
information O
from O
the O
other O
sentence O
for O
each O
word O
sequence O
. O
They O
use O
similar O
methods O
as O
him2017 O
for O
encoding Task
and Task
inference Task
, O
without O
any O
dependent Method
reading Method
mechanism Method
. O
Although O
NLI Task
is O
well O
studied O
in O
the O
literature O
, O
the O
potential O
of O
dependent O
reading O
and O
interaction O
between O
a O
premise O
and O
hypothesis O
is O
not O
rigorously O
explored O
. O
In O
this O
paper O
, O
we O
address O
this O
gap O
by O
proposing O
a O
novel O
deep Method
learning Method
model Method
( O
DR Method
- Method
BiLSTM Method
) O
. O
Experimental O
results O
demonstrate O
the O
effectiveness O
of O
our O
model O
. O
section O
: O
Model O
Our O
proposed O
model O
( O
DR Method
- Method
BiLSTM Method
) O
is O
composed O
of O
the O
following O
major O
components O
: O
input Task
encoding Task
, O
attention Task
, O
inference Task
, O
and O
classification Task
. O
Figure O
[ O
reference O
] O
demonstrates O
a O
high O
- O
level O
view O
of O
our O
proposed O
NLI Task
framework O
. O
Let O
and O
be O
the O
given O
premise O
with O
length O
and O
hypothesis O
with O
length O
respectively O
, O
where O
is O
an O
word O
embedding O
of O
- O
dimensional O
vector O
. O
The O
task O
is O
to O
predict O
a O
label O
that O
indicates O
the O
logical O
relationship O
between O
premise O
and O
hypothesis O
. O
subsection O
: O
Input Method
Encoding Method
RNNs Method
are O
the O
natural O
solution O
for O
variable Task
length Task
sequence Task
modeling Task
, O
consequently O
, O
we O
utilize O
a O
bidirectional Method
LSTM Method
( O
BiLSTM Method
) O
for O
encoding O
the O
given O
sentences O
. O
For O
ease O
of O
presentation O
, O
we O
only O
describe O
how O
we O
encode O
depending O
on O
. O
The O
same O
procedure O
is O
utilized O
for O
the O
reverse O
direction O
( O
) O
. O
To O
dependently O
encode O
, O
we O
first O
process O
using O
the O
BiLSTM Method
. O
Then O
we O
read O
through O
the O
BiLSTM Method
that O
is O
initialized O
with O
previous O
reading O
final O
states O
( O
memory O
cell O
and O
hidden O
state O
) O
. O
Here O
we O
represent O
a O
word O
( O
e.g. O
) O
and O
its O
context O
depending O
on O
the O
other O
sentence O
( O
e.g. O
) O
. O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
formally O
represent O
this O
component O
. O
where O
and O
are O
the O
independent O
reading O
sequences O
, O
dependent O
reading O
sequences O
, O
and O
BiLSTM Method
final O
state O
of O
independent O
reading O
of O
and O
respectively O
. O
Note O
that O
, O
“ O
” O
in O
these O
equations O
means O
that O
we O
do O
not O
care O
about O
the O
associated O
variable O
and O
its O
value O
. O
BiLSTM Method
inputs O
are O
the O
word O
embedding O
sequences O
and O
initial O
state O
vectors O
. O
and O
are O
passed O
to O
the O
next O
layer O
as O
the O
output O
of O
the O
input Method
encoding Method
component Method
. O
The O
proposed O
encoding Method
mechanism Method
yields O
a O
richer O
representation O
for O
both O
premise O
and O
hypothesis O
by O
taking O
the O
history O
of O
each O
other O
into O
account O
. O
Using O
a O
max Method
or Method
average Method
pooling Method
over O
the O
independent O
and O
dependent O
readings O
does O
not O
further O
improve O
our O
model O
. O
This O
was O
expected O
since O
dependent Task
reading Task
produces O
more O
promising O
and O
relevant O
encodings O
. O
subsection O
: O
Attention Task
We O
employ O
a O
soft Method
alignment Method
method Method
to O
associate O
the O
relevant O
sub O
- O
components O
between O
the O
given O
premise O
and O
hypothesis O
. O
In O
deep Method
learning Method
models Method
, O
such O
purpose O
is O
often O
achieved O
with O
a O
soft Method
attention Method
mechanism Method
. O
Here O
we O
compute O
the O
unnormalized O
attention O
weights O
as O
the O
similarity O
of O
hidden O
states O
of O
the O
premise O
and O
hypothesis O
with O
Equation O
[ O
reference O
] O
( O
energy O
function O
) O
. O
where O
and O
are O
the O
dependent O
reading O
hidden O
representations O
of O
and O
respectively O
which O
are O
computed O
earlier O
in O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Next O
, O
for O
each O
word O
in O
either O
premise O
or O
hypothesis O
, O
the O
relevant O
semantics O
in O
the O
other O
sentence O
is O
extracted O
and O
composed O
according O
to O
. O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
provide O
formal O
and O
specific O
details O
of O
this O
procedure O
. O
where O
represents O
the O
extracted O
relevant O
information O
of O
by O
attending O
to O
while O
represents O
the O
extracted O
relevant O
information O
of O
by O
attending O
to O
. O
To O
further O
enrich O
the O
collected O
attentional O
information O
, O
a O
trivial O
next O
step O
would O
be O
to O
pass O
the O
concatenation O
of O
the O
tuples O
or O
which O
provides O
a O
linear O
relationship O
between O
them O
. O
However O
, O
the O
model O
would O
suffer O
from O
the O
absence O
of O
similarity Metric
and Metric
closeness Metric
measures Metric
. O
Therefore O
, O
we O
calculate O
the O
difference O
and O
element O
- O
wise O
product O
for O
the O
tuples O
and O
that O
represent O
the O
similarity O
and O
closeness O
information O
respectively O
. O
The O
difference O
and O
element O
- O
wise O
product O
are O
then O
concatenated O
with O
the O
computed O
vectors O
, O
or O
, O
respectively O
. O
Finally O
, O
a O
feedforward Method
neural Method
layer Method
with O
ReLU Method
activation Method
function Method
projects O
the O
concatenated O
vectors O
from O
- O
dimensional O
vector O
space O
into O
a O
- O
dimensional O
vector O
space O
( O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
) O
. O
This O
helps O
the O
model O
to O
capture O
deeper O
dependencies O
between O
the O
sentences O
besides O
lowering O
the O
complexity Metric
of O
vector Method
representations Method
. O
Here O
stands O
for O
element O
- O
wise O
product O
while O
and O
are O
the O
trainable O
weights O
and O
biases O
of O
the O
projector Method
layer Method
respectively O
. O
subsection O
: O
Inference Task
During O
this O
phase O
, O
we O
use O
another O
BiLSTM Method
to O
aggregate O
the O
two O
sequences O
of O
computed O
matching O
vectors O
, O
and O
from O
the O
attention Method
stage Method
( O
Section O
[ O
reference O
] O
) O
. O
This O
aggregation O
is O
performed O
in O
a O
sequential O
manner O
to O
avoid O
losing O
effect O
of O
latent O
variables O
that O
might O
rely O
on O
the O
sequence O
of O
matching O
vectors O
. O
Instead O
of O
aggregating O
the O
sequences O
of O
matching O
vectors O
individually O
, O
we O
propose O
a O
similar O
dependent Method
reading Method
approach Method
for O
the O
inference Task
stage Task
. O
We O
employ O
a O
BiLSTM Method
reading O
process O
( O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
) O
similar O
to O
the O
input O
encoding O
step O
discussed O
in O
Section O
[ O
reference O
] O
. O
But O
rather O
than O
passing O
just O
the O
dependent O
reading O
information O
to O
the O
next O
step O
, O
we O
feed O
both O
independent O
reading O
( O
and O
) O
and O
dependent O
reading O
( O
and O
) O
to O
a O
max Method
pooling Method
layer Method
, O
which O
selects O
maximum O
values O
from O
each O
sequence O
of O
independent O
and O
dependent O
readings O
( O
and O
) O
as O
shown O
in O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
The O
main O
intuition O
behind O
this O
architecture O
is O
to O
maximize O
the O
inferencing O
ability O
of O
the O
model O
by O
considering O
both O
independent O
and O
dependent O
readings O
. O
Here O
and O
are O
the O
independent O
reading O
sequences O
, O
dependent O
reading O
sequences O
, O
and O
BiLSTM Method
final O
state O
of O
independent O
reading O
of O
and O
respectively O
. O
BiLSTM Method
inputs O
are O
the O
word O
embedding O
sequences O
and O
initial O
state O
vectors O
. O
Finally O
, O
we O
convert O
and O
to O
fixed O
- O
length O
vectors O
with O
pooling O
, O
and O
. O
As O
shown O
in O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
, O
we O
employ O
both O
max Method
and Method
average Method
pooling Method
and O
describe O
the O
overall O
inference O
relationship O
with O
concatenation O
of O
their O
outputs O
. O
subsection O
: O
Classification Task
Here O
, O
we O
feed O
the O
concatenation O
of O
and O
( O
) O
into O
a O
multilayer Method
perceptron Method
( O
MLP Method
) O
classifier O
that O
includes O
a O
hidden Method
layer Method
with O
tanh O
activation O
and O
softmax O
output O
layer O
. O
The O
model O
is O
trained O
in O
an O
end O
- O
to O
- O
end O
manner O
. O
section O
: O
Experiments O
and O
Evaluation O
subsection O
: O
Dataset O
The O
Stanford Material
Natural Material
Language Material
Inference Material
( O
SNLI Material
) O
dataset O
contains O
human O
annotated O
sentence O
pairs O
. O
The O
premises O
are O
drawn O
from O
the O
Flickr30k Material
corpus Material
, O
and O
then O
the O
hypotheses O
are O
manually O
composed O
for O
each O
relationship O
class O
( O
entailment O
, O
neutral O
, O
contradiction O
, O
and O
- O
) O
. O
The O
“ O
- O
” O
class O
indicates O
that O
there O
is O
no O
consensus O
decision O
among O
the O
annotators O
, O
consequently O
, O
we O
remove O
them O
during O
the O
training O
and O
evaluation Task
following O
the O
literature O
. O
We O
use O
the O
same O
data O
split O
as O
provided O
in O
snli Material
to O
report O
comparable O
results O
with O
other O
models O
. O
subsection O
: O
Experimental O
Setup O
We O
use O
pre O
- O
trained O
- O
Glove O
vectors O
to O
initialize O
our O
word Method
embedding Method
vectors Method
. O
All O
hidden O
states O
of O
BiLSTMs Method
during O
input Task
encoding Task
and O
inference Task
have O
dimensions O
( O
and O
) O
. O
The O
weights O
are O
learned O
by O
minimizing O
the O
log Metric
- Metric
loss Metric
on O
the O
training O
data O
via O
the O
Adam Method
optimizer Method
. O
The O
initial O
learning Metric
rate Metric
is O
0.0004 O
. O
To O
avoid O
overfitting O
, O
we O
use O
dropout Method
with O
the O
rate O
of O
0.4 O
for O
regularization Task
, O
which O
is O
applied O
to O
all O
feedforward O
connections O
. O
During O
training O
, O
the O
word O
embeddings O
are O
updated O
to O
learn O
effective Method
representations Method
for O
the O
NLI Task
task Task
. O
We O
use O
a O
fairly O
small O
batch O
size O
of O
32 O
to O
provide O
more O
exploration O
power O
to O
the O
model O
. O
Our O
observation O
indicates O
that O
using O
larger O
batch O
sizes O
hurts O
the O
performance O
of O
our O
model O
. O
subsection O
: O
Ensemble Method
Strategy Method
Ensemble Method
methods Method
use O
multiple O
models O
to O
obtain O
better O
predictive Task
performance O
. O
Previous O
works O
typically O
utilize O
trivial O
ensemble Method
strategies Method
by O
either O
using O
majority O
votes O
or O
averaging O
the O
probability O
distributions O
over O
the O
same O
model O
with O
different O
initialization O
seeds O
. O
By O
contrast O
, O
we O
use O
weighted Method
averaging Method
of Method
the Method
probability Method
distributions Method
where O
the O
weight O
of O
each O
model O
is O
learned O
through O
its O
performance O
on O
the O
SNLI Material
development O
set O
. O
Furthermore O
, O
the O
differences O
between O
our O
models O
in O
the O
ensemble O
originate O
from O
: O
1 O
) O
variations O
in O
the O
number O
of O
dependent O
readings O
( O
i.e. O
1 O
and O
3 O
rounds O
of O
dependent O
reading O
) O
, O
2 O
) O
projection Method
layer Method
activation Method
( O
tanh Method
and O
ReLU Method
in O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
) O
, O
and O
3 O
) O
different O
initialization O
seeds O
. O
The O
main O
intuition O
behind O
this O
design O
is O
that O
the O
effectiveness O
of O
a O
model O
may O
depend O
on O
the O
complexity O
of O
a O
premise O
- O
hypothesis O
instance O
. O
For O
a O
simple O
instance O
, O
a O
simple O
model O
could O
perform O
better O
than O
a O
complex O
one O
, O
while O
a O
complex O
instance O
may O
need O
further O
consideration O
toward O
disambiguation Task
. O
Consequently O
, O
using O
models O
with O
different O
rounds O
of O
dependent O
readings O
in O
the O
encoding O
stage O
should O
be O
beneficial O
. O
Figure O
[ O
reference O
] O
demonstrates O
the O
observed O
performance O
of O
our O
ensemble Method
method Method
with O
different O
number O
of O
models O
. O
The O
performance O
of O
the O
models O
are O
reported O
based O
on O
the O
best O
obtained O
accuracy Metric
on O
the O
development O
set O
. O
We O
also O
study O
the O
effectiveness O
of O
other O
ensemble Method
strategies Method
e.g. O
majority Method
voting Method
, O
and O
averaging Method
the Method
probability Method
distributions Method
. O
But O
, O
our O
ensemble Method
strategy Method
performs O
the O
best O
among O
them O
( O
see O
Section O
[ O
reference O
] O
in O
the O
Appendix O
for O
additional O
details O
) O
. O
subsection O
: O
Preprocessing O
We O
perform O
a O
trivial O
preprocessing O
step O
on O
SNLI Material
to O
recover O
some O
out O
- O
of O
- O
vocabulary O
words O
found O
in O
the O
development O
set O
and O
test O
set O
. O
Note O
that O
our O
vocabulary O
contains O
all O
words O
that O
are O
seen O
in O
the O
training O
set O
, O
so O
there O
is O
no O
out O
- O
of O
- O
vocabulary O
word O
in O
it O
. O
The O
SNLI Material
dataset O
is O
not O
immune O
to O
human O
errors O
, O
specifically O
, O
misspelled O
words O
. O
We O
noticed O
that O
misspelling O
is O
the O
main O
reason O
for O
some O
of O
the O
observed O
out O
- O
of O
- O
vocabulary O
words O
. O
Consequently O
, O
we O
simply O
fix O
the O
unseen O
misspelled O
words O
using O
Microsoft Method
spell Method
- Method
checker Method
( O
other O
approaches O
like O
edit Method
distance Method
can O
also O
be O
used O
) O
. O
Moreover O
, O
while O
dealing O
with O
an O
unseen O
word O
during O
evaluation O
, O
we O
try O
to O
: O
1 O
) O
replace O
it O
with O
its O
lower O
case O
, O
or O
2 O
) O
split O
the O
word O
when O
it O
contains O
a O
“ O
- O
” O
( O
e.g. O
“ O
marsh O
- O
like O
” O
) O
or O
starts O
with O
“ O
un O
” O
( O
e.g. O
“ O
unloading O
” O
) O
. O
If O
we O
still O
could O
not O
find O
the O
word O
in O
our O
vocabulary O
, O
we O
consider O
it O
as O
an O
unknown O
word O
. O
In O
the O
next O
subsection O
, O
we O
demonstrate O
the O
importance O
and O
impact O
of O
such O
trivial O
preprocessing O
( O
see O
Section O
[ O
reference O
] O
in O
the O
Appendix O
for O
additional O
details O
) O
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
shows O
the O
accuracy Metric
of O
the O
models O
on O
training O
and O
test O
sets O
of O
SNLI Material
. O
The O
first O
row O
represents O
a O
baseline Method
classifier Method
presented O
by O
snli Material
that O
utilizes O
handcrafted O
features O
. O
All O
other O
listed O
models O
are O
deep Method
- Method
learning Method
based Method
. O
The O
gap O
between O
the O
traditional O
model O
and O
deep Method
learning Method
models Method
demonstrates O
the O
effectiveness O
of O
deep Method
learning Method
methods Method
for O
this O
task O
. O
We O
also O
report O
the O
estimated O
human Metric
performance Metric
on O
the O
SNLI Material
dataset O
, O
which O
is O
the O
average O
accuracy Metric
of O
five O
annotators Metric
in O
comparison O
to O
the O
gold O
labels O
. O
It O
is O
noteworthy O
that O
recent O
deep Method
learning Method
models Method
surpass O
the O
human O
performance O
in O
the O
NLI Task
task Task
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
previous O
deep Method
learning Method
models Method
( O
rows O
2 O
- O
19 O
) O
can O
be O
divided O
into O
three O
categories O
: O
1 O
) O
sentence Method
encoding Method
based Method
models Method
( O
rows O
2 O
- O
7 O
) O
, O
2 O
) O
single Method
inter Method
- Method
sentence Method
attention Method
- Method
based Method
models Method
( O
rows O
8 O
- O
16 O
) O
, O
and O
3 O
) O
ensemble Method
inter Method
- Method
sentence Method
attention Method
- Method
based Method
models Method
( O
rows O
17 O
- O
19 O
) O
. O
We O
can O
see O
that O
inter Method
- Method
sentence Method
attention Method
- Method
based Method
models Method
perform O
better O
than O
sentence Method
encoding Method
based Method
models Method
, O
which O
supports O
our O
intuition O
. O
Natural Task
language Task
inference Task
requires O
a O
deep O
interaction O
between O
the O
premise O
and O
hypothesis O
. O
Inter Method
- Method
sentence Method
attention Method
- Method
based Method
approaches Method
can O
provide O
such O
interaction O
while O
sentence Method
encoding Method
based Method
models Method
fail O
to O
do O
so O
. O
To O
further O
enhance O
the O
modeling O
of O
interaction O
between O
the O
premise O
and O
hypothesis O
for O
efficient O
disambiguation Task
of O
their O
relationship O
, O
we O
introduce O
the O
dependent Method
reading Method
strategy Method
in O
our O
proposed O
DR Method
- Method
BiLSTM Method
model O
. O
The O
results O
demonstrate O
the O
effectiveness O
of O
our O
model O
. O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
achieves O
accuracy Metric
on O
the O
test O
set O
which O
is O
noticeably O
the O
best O
reported O
result O
among O
the O
existing O
single O
models O
for O
this O
task O
. O
Note O
that O
the O
difference O
between O
DR Method
- Method
BiLSTM Method
and O
him2017 O
is O
statistically O
significant O
with O
a O
p O
- O
value O
of O
over O
the O
Chi Metric
- Metric
square Metric
test Metric
. O
To O
further O
improve O
the O
performance O
of O
NLI Task
systems O
, O
researchers O
have O
built O
ensemble Method
models Method
. O
Previously O
, O
ensemble Method
systems Method
obtained O
the O
best O
performance O
on O
SNLI Material
with O
a O
huge O
margin O
. O
Table O
[ O
reference O
] O
shows O
that O
our O
proposed O
single O
model O
achieves O
competitive O
results O
compared O
to O
these O
reported O
ensemble Method
models Method
. O
Our O
ensemble Method
model Method
considerably O
outperforms O
the O
current O
state O
- O
of O
- O
the O
- O
art O
by O
obtaining O
accuracy Metric
. O
Up O
until O
this O
point O
, O
we O
discussed O
the O
performance O
of O
our O
models O
where O
we O
have O
not O
considered O
preprocessing Method
for O
recovering O
the O
out O
- O
of O
- O
vocabulary O
words O
. O
In O
Table O
[ O
reference O
] O
, O
“ O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
Process O
” O
, O
and O
“ O
DR Method
- Method
BiLSTM Method
( O
Ensem Method
. O
) O
Process O
” O
represent O
the O
performance O
of O
our O
models O
on O
the O
preprocessed O
dataset O
. O
We O
can O
see O
that O
our O
preprocessing Method
mechanism Method
leads O
to O
further O
improvements O
of O
and O
on O
the O
SNLI Material
test O
set O
for O
our O
single Method
and Method
ensemble Method
models Method
respectively O
. O
In O
fact O
, O
our O
single O
model O
( O
“ O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
Process O
” O
) O
obtains O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
over O
both O
reported O
single Method
and Method
ensemble Method
models Method
by O
performing O
a O
simple O
preprocessing Method
step Method
. O
Furthermore O
, O
“ O
DR Method
- Method
BiLSTM Method
( O
Ensem O
. O
) O
Process Method
” O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
remarkably O
( O
improvement O
) O
. O
For O
more O
comparison O
and O
analyses O
, O
we O
use O
“ O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
” O
and O
“ O
DR Method
- Method
BiLSTM Method
( O
Ensemble O
) O
” O
as O
our O
single Method
and Method
ensemble Method
models Method
in O
the O
rest O
of O
the O
paper O
. O
subsection O
: O
Ablation Task
and O
Configuration Task
Study Task
We O
conducted O
an O
ablation O
study O
on O
our O
model O
to O
examine O
the O
importance O
and O
effect O
of O
each O
major O
component O
. O
Then O
, O
we O
study O
the O
impact O
of O
BiLSTM Method
dimensionality O
on O
the O
performance O
of O
the O
development O
set O
and O
training O
set O
of O
SNLI Material
. O
We O
investigate O
all O
settings O
on O
the O
development O
set O
of O
the O
SNLI Material
dataset O
. O
Table O
[ O
reference O
] O
shows O
the O
ablation O
study O
results O
on O
the O
development O
set O
of O
SNLI Material
along O
with O
the O
statistical Metric
significance Metric
test Metric
results O
in O
comparison O
to O
the O
proposed O
model O
, O
DR Method
- Method
BiLSTM Method
. O
We O
can O
see O
that O
all O
modifications O
lead O
to O
a O
new O
model O
and O
their O
differences O
are O
statistically O
significant O
with O
a O
p O
- O
value O
of O
over O
Chi Metric
square Metric
test Metric
. O
Table O
[ O
reference O
] O
shows O
that O
removing O
any O
part O
from O
our O
model O
hurts O
the O
development O
set O
accuracy Metric
which O
indicates O
the O
effectiveness O
of O
these O
components O
. O
Among O
all O
components O
, O
three O
of O
them O
have O
noticeable O
influences O
: O
max Method
pooling Method
, O
difference O
in O
the O
attention O
stage O
, O
and O
dependent Task
reading Task
. O
Most O
importantly O
, O
the O
last O
four O
study O
cases O
in O
Table O
[ O
reference O
] O
( O
rows O
8 O
- O
11 O
) O
verify O
the O
main O
intuitions O
behind O
our O
proposed O
model O
. O
They O
illustrate O
the O
importance O
of O
our O
proposed O
dependent Method
reading Method
strategy Method
which O
leads O
to O
significant O
improvement O
, O
specifically O
in O
the O
encoding Task
stage Task
. O
We O
are O
convinced O
that O
the O
importance O
of O
dependent Task
reading Task
in O
the O
encoding Task
stage Task
originates O
from O
its O
ability O
to O
focus O
on O
more O
important O
and O
relevant O
aspects O
of O
the O
sentences O
due O
to O
its O
prior O
knowledge O
of O
the O
other O
sentence O
during O
the O
encoding O
procedure O
. O
Figure O
[ O
reference O
] O
shows O
the O
behavior O
of O
the O
proposed O
model O
accuracy Metric
on O
the O
training O
set O
and O
development O
set O
of O
SNLI Material
. O
Since O
the O
models O
are O
selected O
based O
on O
the O
best O
observed O
development O
set O
accuracy Metric
during O
the O
training O
procedure O
, O
the O
training O
accuracy Metric
curve O
( O
red O
, O
top O
) O
is O
not O
strictly O
increasing O
. O
Figure O
[ O
reference O
] O
demonstrates O
that O
we O
achieve O
the O
best O
performance O
with O
450 Task
- Task
dimensional Task
BiLSTMs Task
. O
In O
other O
words O
, O
using O
BiLSTMs Method
with O
lower O
dimensionality O
causes O
the O
model O
to O
suffer O
from O
the O
lack O
of O
space O
for O
capturing O
proper O
information O
and O
dependencies O
. O
On O
the O
other O
hand O
, O
using O
higher O
dimensionality O
leads O
to O
overfitting O
which O
hurts O
the O
performance O
on O
the O
development O
set O
. O
Hence O
, O
we O
use O
450 O
- O
dimensional O
BiLSTM Method
in O
our O
proposed O
model O
. O
subsection O
: O
Analysis O
We O
first O
investigate O
the O
performance O
of O
our O
models O
categorically O
. O
Then O
, O
we O
show O
a O
visualization O
of O
the O
energy O
function O
in O
the O
attention Method
stage Method
( O
Equation O
[ O
reference O
] O
) O
for O
an O
instance O
from O
the O
SNLI Material
test O
set O
. O
To O
qualitatively O
evaluate O
the O
performance O
of O
our O
models O
, O
we O
design O
a O
set O
of O
annotation O
tags O
that O
can O
be O
extracted O
automatically O
. O
This O
design O
is O
inspired O
by O
the O
reported O
annotation O
tags O
in O
multinli Material
. O
The O
specifications O
of O
our O
annotation O
tags O
are O
as O
follows O
: O
High O
Overlap O
: O
premise O
and O
hypothesis O
sentences O
share O
more O
than O
tokens O
. O
Regular O
Overlap O
: O
sentences O
share O
between O
and O
tokens O
. O
Low O
Overlap O
: O
sentences O
share O
less O
than O
tokens O
. O
Long O
Sentence O
: O
either O
sentence O
is O
longer O
than O
20 O
tokens O
. O
Regular O
Sentence O
: O
premise O
or O
hypothesis O
length O
is O
between O
5 O
and O
20 O
tokens O
. O
Short O
Sentence O
: O
either O
sentence O
is O
shorter O
than O
5 O
tokens O
. O
Negation O
: O
negation O
is O
present O
in O
a O
sentence O
. O
Quantifier O
: O
either O
of O
the O
sentences O
contains O
one O
of O
the O
following O
quantifiers O
: O
much O
, O
enough O
, O
more O
, O
most O
, O
less O
, O
least O
, O
no O
, O
none O
, O
some O
, O
any O
, O
many O
, O
few O
, O
several O
, O
almost O
, O
nearly O
. O
Belief O
: O
either O
of O
the O
sentences O
contains O
one O
of O
the O
following O
belief O
verbs O
: O
know O
, O
believe O
, O
understand O
, O
doubt O
, O
think O
, O
suppose O
, O
recognize O
, O
forget O
, O
remember O
, O
imagine O
, O
mean O
, O
agree O
, O
disagree O
, O
deny O
, O
promise O
. O
Table O
[ O
reference O
] O
shows O
the O
frequency O
of O
aforementioned O
annotation O
tags O
in O
the O
SNLI Material
test O
set O
along O
with O
the O
performance O
( O
accuracy Metric
) O
of O
ESIM Method
, O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
, O
and O
DR Method
- Method
BiLSTM Method
( O
Ensemble Method
) O
. O
Table O
[ O
reference O
] O
can O
be O
divided O
into O
four O
major O
categories O
: O
1 O
) O
gold O
label O
data O
, O
2 O
) O
word O
overlap O
, O
3 O
) O
sentence O
length O
, O
and O
4 O
) O
occurrence O
of O
special O
words O
. O
We O
can O
see O
that O
DR Method
- Method
BiLSTM Method
( O
Ensemble Method
) O
performs O
the O
best O
in O
all O
categories O
which O
matches O
our O
expectation O
. O
Moreover O
, O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
performs O
noticeably O
better O
than O
ESIM Method
in O
most O
of O
the O
categories O
except O
“ O
Entailment O
” O
, O
“ O
High O
Overlap O
” O
, O
and O
“ O
Long O
Sentence O
” O
, O
for O
which O
our O
model O
is O
not O
far O
behind O
( O
gaps O
of O
, O
, O
and O
, O
respectively O
) O
. O
It O
is O
noteworthy O
that O
DR Method
- Method
BiLSTM Method
( O
Single O
) O
performs O
better O
than O
ESIM Method
in O
more O
frequent O
categories O
. O
Specifically O
, O
the O
performance O
of O
our O
model O
in O
“ O
Neutral O
” O
, O
“ O
Negation O
” O
, O
and O
“ O
Quantifier O
” O
categories O
( O
improvements O
of O
, O
, O
and O
, O
respectively O
) O
indicates O
the O
superiority O
of O
our O
model O
in O
understanding Task
and Task
disambiguating Task
complex Task
samples Task
. O
Our O
investigations O
indicate O
that O
ESIM Method
generates O
somewhat O
uniform O
attention O
for O
most O
of O
the O
word O
pairs O
while O
our O
model O
could O
effectively O
attend O
to O
specific O
parts O
of O
the O
given O
sentences O
and O
provide O
more O
meaningful O
attention O
. O
In O
other O
words O
, O
the O
dependent Method
reading Method
strategy Method
enables O
our O
model O
to O
achieve O
meaningful Method
representations Method
, O
which O
leads O
to O
better O
attention O
to O
obtain O
further O
gains O
on O
such O
categories O
like O
Negation O
and O
Quantifier O
sentences O
( O
see O
Section O
[ O
reference O
] O
in O
the O
Appendix O
for O
additional O
details O
) O
. O
Finally O
, O
we O
show O
a O
visualization O
of O
the O
normalized O
attention O
weights O
( O
energy O
function O
, O
Equation O
[ O
reference O
] O
) O
of O
our O
model O
in O
Figure O
[ O
reference O
] O
. O
We O
show O
a O
sentence O
pair O
, O
where O
the O
premise O
is O
“ O
Male O
in O
a O
blue O
jacket O
decides O
to O
lay O
the O
grass O
. O
” O
, O
and O
the O
hypothesis O
is O
“ O
The O
guy O
in O
yellow O
is O
rolling O
on O
the O
grass O
. O
” O
, O
and O
its O
logical O
relationship O
is O
contradiction O
. O
Figure O
[ O
reference O
] O
indicates O
the O
model O
’s O
ability O
in O
attending O
to O
critical O
pairs O
of O
words O
like O
Male O
, O
guy O
, O
decides O
, O
rolling O
, O
and O
lay O
, O
rolling O
. O
Finally O
, O
high O
attention O
between O
{ O
decides O
, O
lay O
} O
and O
{ O
rolling O
} O
, O
and O
{ O
Male O
} O
and O
{ O
guy O
} O
leads O
the O
model O
to O
correctly O
classify O
the O
sentence O
pair O
as O
contradiction O
( O
for O
more O
samples O
with O
attention O
visualizations O
, O
see O
Section O
[ O
reference O
] O
of O
the O
Appendix O
) O
. O
section O
: O
Conclusion O
We O
propose O
a O
novel O
natural Task
language Task
inference Task
model O
( O
DR Method
- Method
BiLSTM Method
) O
that O
benefits O
from O
a O
dependent Method
reading Method
strategy Method
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
SNLI Material
dataset O
. O
We O
also O
introduce O
a O
sophisticated O
ensemble Method
strategy Method
and O
illustrate O
its O
effectiveness O
through O
experimentation O
. O
Moreover O
, O
we O
demonstrate O
the O
importance O
of O
a O
simple O
preprocessing O
step O
on O
the O
performance O
of O
our O
proposed O
models O
. O
Evaluation O
results O
show O
that O
the O
preprocessing Method
step Method
allows O
our O
DR Method
- Method
BiLSTM Method
( O
single O
) O
model O
to O
outperform O
all O
previous O
single Method
and Method
ensemble Method
methods Method
. O
Similar O
superior O
performance O
is O
also O
observed O
for O
our O
DR Method
- Method
BiLSTM Method
( Method
ensemble Method
) Method
model Method
. O
We O
show O
that O
our O
ensemble Method
model Method
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
by O
a O
considerable O
margin O
of O
. O
Finally O
, O
we O
perform O
an O
extensive O
analysis O
to O
demonstrate O
the O
strength O
and O
weakness O
of O
the O
proposed O
model O
, O
which O
would O
pave O
the O
way O
for O
further O
improvements O
in O
this O
domain O
. O
bibliography O
: O
References O
appendix O
: O
Ensemble Method
Strategy Method
Study Method
We O
use O
the O
following O
configurations O
in O
our O
ensemble Method
model Method
study O
: O
DR Method
- Method
BiLSTM Method
( O
with O
different O
initialization O
seeds O
) O
: O
here O
, O
we O
consider O
6 O
DR Method
- Method
BiLSTMs Method
with O
different O
initialization O
seeds O
. O
tanh Method
- Method
Projection Method
: O
same O
configuration O
as O
DR Method
- Method
BiLSTM Method
, O
but O
we O
use O
tanh Method
instead O
of O
ReLU O
as O
the O
activation O
function O
in O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
in O
the O
paper O
: O
DR Method
- Method
BiLSTM Method
( O
with O
1 O
round O
of O
dependent Method
reading Method
) O
: O
same O
configuration O
as O
DR Method
- Method
BiLSTM Method
, O
but O
we O
do O
not O
use O
dependent O
reading O
during O
the O
inference Method
process Method
. O
In O
other O
words O
, O
we O
use O
and O
instead O
of O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
in O
the O
paper O
respectively O
. O
DR Method
- Method
BiLSTM Method
( O
with O
3 O
rounds O
of O
dependent Method
reading Method
) O
: O
same O
configuration O
as O
the O
above O
, O
but O
we O
use O
3 O
rounds O
of O
dependent O
reading O
. O
Formally O
, O
we O
replace O
Equations O
[ O
reference O
] O
and O
[ O
reference O
] O
in O
the O
paper O
with O
the O
following O
equations O
respectively O
: O
Our O
final O
ensemble Method
model Method
, O
DR Method
- Method
BiLSTM Method
( O
Ensemble Method
) O
is O
the O
combination O
of O
the O
following O
6 O
models O
: O
tanh Method
- Method
Projection Method
, O
DR Method
- Method
BiLSTM Method
( O
with O
1 O
round O
of O
dependent Method
reading Method
) O
, O
DR Method
- Method
BiLSTM Method
( O
with O
3 O
rounds O
of O
dependent O
reading O
) O
, O
and O
3 O
DR Method
- Method
BiLSTMs Method
with O
different O
initialization O
seeds O
. O
We O
also O
experiment O
with O
majority Method
voting Method
and O
averaging O
the O
probability Method
distribution Method
strategies Method
for O
ensemble Method
models Method
using O
the O
same O
set O
of O
models O
as O
our O
weighted Method
averaging Method
ensemble Method
method Method
( O
as O
described O
above O
) O
. O
Figure O
[ O
reference O
] O
shows O
the O
behavior O
of O
the O
majority Method
voting Method
strategy Method
with O
different O
number O
of O
models O
. O
Interestingly O
, O
the O
best O
development O
accuracy Metric
is O
also O
observed O
using O
6 O
individual O
models O
including O
tanh Method
- Method
Projection Method
, O
DR Method
- Method
BiLSTM Method
( O
with O
1 O
round O
of O
dependent O
reading O
) O
, O
DR Method
- Method
BiLSTM Method
( O
with O
3 O
rounds O
of O
dependent O
reading O
) O
, O
and O
3 O
DR Method
- Method
BiLSTMs Method
with O
varying O
initialization O
seeds O
that O
are O
different O
from O
our O
DR Method
- Method
BiLSTM Method
( O
Ensemble O
) O
model O
. O
We O
should O
note O
that O
our O
weighted Method
averaging Method
ensemble Method
strategy Method
performs O
better O
than O
the O
majority Method
voting Method
method Method
in O
both O
development O
set O
and O
test O
set O
of O
SNLI Material
, O
which O
indicates O
the O
effectiveness O
of O
our O
approach O
. O
Furthermore O
, O
our O
method O
could O
show O
more O
consistent O
behavior O
for O
training O
and O
test O
sets O
when O
we O
increased O
the O
number O
of O
models O
( O
see O
Figure O
[ O
reference O
] O
in O
Section O
[ O
reference O
] O
of O
the O
paper O
) O
. O
According O
to O
our O
observations O
, O
averaging O
the O
probability O
distributions O
fails O
to O
improve O
the O
development O
set O
accuracy Metric
using O
two O
and O
three O
models O
, O
so O
we O
did O
not O
study O
it O
further O
. O
appendix O
: O
Preprocessing O
Study O
Table O
[ O
reference O
] O
shows O
some O
erroneous O
sentences O
from O
the O
SNLI Material
test O
set O
along O
with O
their O
corrected O
equivalents O
( O
after O
preprocessing O
) O
. O
Furthermore O
, O
we O
show O
the O
energy O
function O
( O
Equation O
[ O
reference O
] O
in O
the O
paper O
) O
visualizations O
of O
6 O
examples O
from O
the O
aforementioned O
data O
samples O
in O
Figures O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
. O
Each O
figure O
presents O
the O
visualization O
of O
an O
original O
erroneous O
sample O
along O
its O
corrected O
version O
. O
These O
figures O
clearly O
illustrate O
that O
fixing O
the O
erroneous O
words O
leads O
to O
producing O
correct O
attentions O
over O
the O
sentences O
. O
This O
can O
be O
observed O
by O
comparing O
the O
attention O
for O
the O
erroneous O
words O
and O
corrected O
words O
, O
e.g. O
“ O
daschunds O
” O
and O
“ O
dachshunds O
” O
in O
the O
premise O
of O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Note O
that O
we O
add O
two O
dummy O
notations O
( O
i.e. O
_ O
FOL O
_ O
, O
and O
_ O
EOL O
_ O
) O
to O
all O
sentences O
which O
indicate O
their O
beginning O
and O
end O
. O
appendix O
: O
Category O
Study O
Here O
we O
investigate O
the O
normalized O
attention O
weights O
of O
DR Method
- Method
BiLSTM Method
and O
ESIM Method
for O
four O
samples O
that O
belong O
to O
Negation O
and O
/ O
or O
Quantifier O
categories O
( O
Figures O
[ O
reference O
] O
- O
[ O
reference O
] O
) O
. O
Each O
figure O
illustrates O
the O
normalized O
energy O
function O
of O
DR Method
- Method
BiLSTM Method
( O
left O
diagram O
) O
and O
ESIM Method
( O
right O
diagram O
) O
respectively O
. O
Provided O
figures O
indicate O
that O
ESIM Method
assigns O
somewhat O
similar O
attention O
to O
most O
of O
the O
pairs O
while O
DR Method
- Method
BiLSTM Method
focuses O
on O
specific O
parts O
of O
the O
given O
premise O
and O
hypothesis O
. O
appendix O
: O
Attention Task
Study Task
In O
this O
section O
, O
we O
show O
visualizations O
of O
18 O
samples O
of O
normalized O
attention O
weights O
( O
energy O
function O
, O
see O
Equation O
[ O
reference O
] O
in O
the O
paper O
) O
. O
Each O
column O
in O
Figures O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
, O
represents O
three O
data O
samples O
that O
share O
the O
same O
premise O
but O
differ O
in O
hypothesis O
. O
Also O
, O
each O
row O
is O
allocated O
to O
a O
specific O
logical O
relationship O
( O
Top O
: O
Entailment O
, O
Middle O
: O
Neutral O
, O
and O
Bottom O
: O
Contradiction O
) O
. O
DR Method
- Method
BiLSTM Method
classifies O
all O
data O
samples O
reported O
in O
these O
figures O
correctly O
. O
