document O
: O
A O
Character Method
- Method
Level Method
Decoder Method
without O
Explicit Method
Segmentation Method
for O
Neural Task
Machine Task
Translation Task
The O
existing O
machine Task
translation Task
systems O
, O
whether O
phrase Method
- Method
based Method
or Method
neural Method
, O
have O
relied O
almost O
exclusively O
on O
word Method
- Method
level Method
modelling Method
with O
explicit Method
segmentation Method
. O
In O
this O
paper O
, O
we O
ask O
a O
fundamental O
question O
: O
can O
neural O
machine Task
translation Task
generate O
a O
character O
sequence O
without O
any O
explicit Method
segmentation Method
? O
To O
answer O
this O
question O
, O
we O
evaluate O
an O
attention Method
- Method
based Method
encoder Method
– Method
decoder Method
with O
a O
subword Method
- Method
level Method
encoder O
and O
a O
character Method
- Method
level Method
decoder Method
on O
four O
language O
pairs O
– O
En O
- O
Cs O
, O
En O
- O
De O
, O
En O
- O
Ru O
and O
En O
- O
Fi– O
using O
the O
parallel O
corpora O
from O
WMT’15 Material
. O
Our O
experiments O
show O
that O
the O
models O
with O
a O
character Method
- Method
level Method
decoder Method
outperform O
the O
ones O
with O
a O
subword Method
- Method
level Method
decoder O
on O
all O
of O
the O
four O
language O
pairs O
. O
Furthermore O
, O
the O
ensembles Method
of Method
neural Method
models Method
with O
a O
character Method
- Method
level Method
decoder Method
outperform O
the O
state O
- O
of O
- O
the O
- O
art O
non O
- O
neural O
machine Task
translation Task
systems O
on O
En O
- O
Cs O
, O
En O
- O
De O
and O
En O
- O
Fi O
and O
perform O
comparably O
on O
En O
- O
Ru O
. O
section O
: O
Introduction O
The O
existing O
machine Task
translation Task
systems O
have O
relied O
almost O
exclusively O
on O
word Method
- Method
level Method
modelling Method
with O
explicit Method
segmentation Method
. O
This O
is O
mainly O
due O
to O
the O
issue O
of O
data Task
sparsity Task
which O
becomes O
much O
more O
severe O
, O
especially O
for O
- O
grams O
, O
when O
a O
sentence O
is O
represented O
as O
a O
sequence O
of O
characters O
rather O
than O
words O
, O
as O
the O
length O
of O
the O
sequence O
grows O
significantly O
. O
In O
addition O
to O
data O
sparsity O
, O
we O
often O
have O
a O
priori O
belief O
that O
a O
word O
, O
or O
its O
segmented O
- O
out O
lexeme O
, O
is O
a O
basic O
unit O
of O
meaning O
, O
making O
it O
natural O
to O
approach O
translation Task
as O
mapping Task
from O
a O
sequence O
of O
source O
- O
language O
words O
to O
a O
sequence O
of O
target O
- O
language O
words O
. O
This O
has O
continued O
with O
the O
more O
recently O
proposed O
paradigm O
of O
neural O
machine Task
translation Task
, O
although O
neural Method
networks Method
do O
not O
suffer O
from O
character Method
- Method
level Method
modelling Method
and O
rather O
suffer O
from O
the O
issues O
specific O
to O
word Method
- Method
level Method
modelling Method
, O
such O
as O
the O
increased O
computational Metric
complexity Metric
from O
a O
very O
large O
target O
vocabulary O
. O
Therefore O
, O
in O
this O
paper O
, O
we O
address O
a O
question O
of O
whether O
neural O
machine Task
translation Task
can O
be O
done O
directly O
on O
a O
sequence O
of O
characters O
without O
any O
explicit O
word Method
segmentation Method
. O
To O
answer O
this O
question O
, O
we O
focus O
on O
representing O
the O
target O
side O
as O
a O
character O
sequence O
. O
We O
evaluate O
neural O
machine Task
translation Task
models O
with O
a O
character Method
- Method
level Method
decoder Method
on O
four O
language O
pairs O
from O
WMT’15 Material
to O
make O
our O
evaluation O
as O
convincing O
as O
possible O
. O
We O
represent O
the O
source O
side O
as O
a O
sequence O
of O
subwords O
extracted O
using O
byte Method
- Method
pair Method
encoding Method
from O
sennrich2015neural O
, O
and O
vary O
the O
target O
side O
to O
be O
either O
a O
sequence O
of O
subwords O
or O
characters O
. O
On O
the O
target O
side O
, O
we O
further O
design O
a O
novel O
recurrent Method
neural Method
network Method
( Method
RNN Method
) Method
, O
called O
bi Method
- Method
scale Method
recurrent O
network O
, O
that O
better O
handles O
multiple O
timescales O
in O
a O
sequence O
, O
and O
test O
it O
in O
addition O
to O
a O
naive Method
, Method
stacked Method
recurrent Method
neural Method
network Method
. O
On O
all O
of O
the O
four O
language O
pairs O
– O
En O
- O
Cs O
, O
En O
- O
De O
, O
En O
- O
Ru O
and O
En O
- O
Fi– O
, O
the O
models O
with O
a O
character Method
- Method
level Method
decoder Method
outperformed O
the O
ones O
with O
a O
subword Method
- Method
level Method
decoder O
. O
We O
observed O
a O
similar O
trend O
with O
the O
ensemble O
of O
each O
of O
these O
configurations O
, O
outperforming O
both O
the O
previous O
best O
neural O
and O
non O
- O
neural O
translation Task
systems O
on O
En O
- O
Cs O
, O
En O
- O
De O
and O
En O
- O
Fi O
, O
while O
achieving O
a O
comparable O
result O
on O
En O
- O
Ru O
. O
We O
find O
these O
results O
to O
be O
a O
strong O
evidence O
that O
neural O
machine Task
translation Task
can O
indeed O
learn O
to O
translate O
at O
the O
character O
- O
level O
and O
that O
in O
fact O
, O
it O
benefits O
from O
doing O
so O
. O
section O
: O
Neural Task
Machine Task
Translation Task
Neural O
machine Task
translation Task
refers O
to O
a O
recently O
proposed O
approach O
to O
machine Task
translation Task
. O
This O
approach O
aims O
at O
building O
an O
end Method
- Method
to Method
- Method
end Method
neural Method
network Method
that O
takes O
as O
input O
a O
source O
sentence O
and O
outputs O
its O
translation Task
, O
where O
and O
are O
respectively O
source O
and O
target O
symbols O
. O
This O
neural Method
network Method
is O
constructed O
as O
a O
composite O
of O
an O
encoder Method
network Method
and O
a O
decoder Method
network Method
. O
The O
encoder Method
network Method
encodes O
the O
input O
sentence O
into O
its O
continuous Method
representation Method
. O
In O
this O
paper O
, O
we O
closely O
follow O
the O
neural O
translation Task
model O
proposed O
in O
bahdanau2014neural O
and O
use O
a O
bidirectional Method
recurrent Method
neural Method
network Method
, O
which O
consists O
of O
two O
recurrent Method
neural Method
networks Method
. O
The O
forward Method
network Method
reads O
the O
input O
sentence O
in O
a O
forward O
direction O
: O
where O
is O
a O
continuous O
embedding O
of O
the O
- O
th O
input O
symbol O
, O
and O
is O
a O
recurrent O
activation O
function O
. O
Similarly O
, O
the O
reverse Method
network Method
reads O
the O
sentence O
in O
a O
reverse O
direction O
( O
right O
to O
left O
) O
: O
At O
each O
location O
in O
the O
input O
sentence O
, O
we O
concatenate O
the O
hidden O
states O
from O
the O
forward Method
and Method
reverse Method
RNNs Method
to O
form O
a O
context O
set O
where O
. O
Then O
the O
decoder Method
computes O
the O
conditional O
distribution O
over O
all O
possible O
translations O
based O
on O
this O
context O
set O
. O
This O
is O
done O
by O
first O
rewriting O
the O
conditional O
probability O
of O
a O
translation Task
: O
For O
each O
conditional O
term O
in O
the O
summation O
, O
the O
decoder Method
RNN Method
updates O
its O
hidden O
state O
by O
where O
is O
the O
continuous O
embedding O
of O
a O
target O
symbol O
. O
is O
a O
context O
vector O
computed O
by O
a O
soft Method
- Method
alignment Method
mechanism Method
: O
The O
soft Method
- Method
alignment Method
mechanism Method
weights O
each O
vector O
in O
the O
context O
set O
according O
to O
its O
relevance O
given O
what O
has O
been O
translated O
. O
The O
weight O
of O
each O
vector O
is O
computed O
by O
where O
is O
a O
parametric Method
function Method
returning O
an O
unnormalized O
score O
for O
given O
and O
. O
We O
use O
a O
feedforward Method
network Method
with O
a O
single O
hidden O
layer O
in O
this O
paper O
. O
is O
a O
normalization O
constant O
: O
This O
procedure O
can O
be O
understood O
as O
computing O
the O
alignment O
probability O
between O
the O
- O
th O
target O
symbol O
and O
- O
th O
source O
symbol O
. O
The O
hidden O
state O
, O
together O
with O
the O
previous O
target O
symbol O
and O
the O
context O
vector O
, O
is O
fed O
into O
a O
feedforward Method
neural Method
network Method
to O
result O
in O
the O
conditional O
distribution O
: O
The O
whole O
model O
, O
consisting O
of O
the O
encoder Method
, Method
decoder Method
and O
soft Method
- Method
alignment Method
mechanism Method
, O
is O
then O
tuned O
end O
- O
to O
- O
end O
to O
minimize O
the O
negative O
log O
- O
likelihood O
using O
stochastic Method
gradient Method
descent Method
. O
section O
: O
Towards O
Character Task
- Task
Level Task
Translation Task
subsection O
: O
Motivation O
Let O
us O
revisit O
how O
the O
source O
and O
target O
sentences O
( O
and O
) O
are O
represented O
in O
neural O
machine Task
translation Task
. O
For O
the O
source O
side O
of O
any O
given O
training O
corpus O
, O
we O
scan O
through O
the O
whole O
corpus O
to O
build O
a O
vocabulary O
of O
unique O
tokens O
to O
which O
we O
assign O
integer O
indices O
. O
A O
source O
sentence O
is O
then O
built O
as O
a O
sequence O
of O
the O
indices O
of O
such O
tokens O
belonging O
to O
the O
sentence O
, O
i.e. O
, O
, O
where O
. O
The O
target O
sentence O
is O
similarly O
transformed O
into O
a O
target O
sequence O
of O
integer O
indices O
. O
Each O
token O
, O
or O
its O
index O
, O
is O
then O
transformed O
into O
a O
so O
- O
called O
one O
- O
hot O
vector O
of O
dimensionality O
. O
All O
but O
one O
elements O
of O
this O
vector O
are O
set O
to O
0 O
. O
The O
only O
element O
whose O
index O
corresponds O
to O
the O
token O
’s O
index O
is O
set O
to O
1 O
. O
This O
one O
- O
hot O
vector O
is O
the O
one O
which O
any O
neural O
machine Task
translation Task
model O
sees O
. O
The O
embedding O
function O
, O
or O
, O
is O
simply O
the O
result O
of O
applying O
a O
linear Method
transformation Method
( O
the O
embedding O
matrix O
) O
to O
this O
one O
- O
hot O
vector O
. O
The O
important O
property O
of O
this O
approach O
based O
on O
one Method
- Method
hot Method
vectors Method
is O
that O
the O
neural Method
network Method
is O
oblivious O
to O
the O
underlying O
semantics O
of O
the O
tokens O
. O
To O
the O
neural Method
network Method
, O
each O
and O
every O
token O
in O
the O
vocabulary O
is O
equal O
distance O
away O
from O
every O
other O
token O
. O
The O
semantics O
of O
those O
tokens O
are O
simply O
learned O
( O
into O
the O
embeddings O
) O
to O
maximize O
the O
translation Task
quality O
, O
or O
the O
log O
- O
likelihood O
of O
the O
model O
. O
This O
property O
allows O
us O
great O
freedom O
in O
the O
choice O
of O
tokens O
’ O
unit O
. O
Neural Method
networks Method
have O
been O
shown O
to O
work O
well O
with O
word O
tokens O
but O
also O
with O
finer O
units O
, O
such O
as O
subwords O
as O
well O
as O
symbols O
resulting O
from O
compression Method
/ Method
encoding Method
. O
Although O
there O
have O
been O
a O
number O
of O
previous O
research O
reporting O
the O
use O
of O
neural Method
networks Method
with O
characters O
( O
see O
, O
e.g. O
, O
mikolov2012subword O
and O
santos2014learning O
) O
, O
the O
dominant O
approach O
has O
been O
to O
preprocess O
the O
text O
into O
a O
sequence O
of O
symbols O
, O
each O
associated O
with O
a O
sequence O
of O
characters O
, O
after O
which O
the O
neural Method
network Method
is O
presented O
with O
those O
symbols O
rather O
than O
with O
characters O
. O
More O
recently O
in O
the O
context O
of O
neural O
machine Task
translation Task
, O
two O
research O
groups O
have O
proposed O
to O
directly O
use O
characters O
. O
kim2015character O
proposed O
to O
represent O
each O
word O
not O
as O
a O
single O
integer O
index O
as O
before O
, O
but O
as O
a O
sequence O
of O
characters O
, O
and O
use O
a O
convolutional Method
network Method
followed O
by O
a O
highway Method
network Method
to O
extract O
a O
continuous Method
representation Method
of O
the O
word O
. O
This O
approach O
, O
which O
effectively O
replaces O
the O
embedding O
function O
, O
was O
adopted O
by O
costa2016character O
for O
neural O
machine Task
translation Task
. O
Similarly O
, O
ling2015character O
use O
a O
bidirectional Method
recurrent Method
neural Method
network Method
to O
replace O
the O
embedding O
functions O
and O
to O
respectively O
encode O
a O
character O
sequence O
to O
and O
from O
the O
corresponding O
continuous Method
word Method
representation Method
. O
A O
similar O
, O
but O
slightly O
different O
approach O
was O
proposed O
by O
lee2015naver O
, O
where O
they O
explicitly O
mark O
each O
character O
with O
its O
relative O
location O
in O
a O
word O
( O
e.g. O
, O
“ O
B”eginning O
and O
“ O
I”ntermediate O
) O
. O
Despite O
the O
fact O
that O
these O
recent O
approaches O
work O
at O
the O
level O
of O
characters O
, O
it O
is O
less O
satisfying O
that O
they O
all O
rely O
on O
knowing O
how O
to O
segment O
characters O
into O
words O
. O
Although O
it O
is O
generally O
easy O
for O
languages O
like O
English O
, O
this O
is O
not O
always O
the O
case O
. O
This O
word Method
segmentation Method
procedure Method
can O
be O
as O
simple O
as O
tokenization Task
followed O
by O
some O
punctuation Method
normalization Method
, O
but O
also O
can O
be O
as O
complicated O
as O
morpheme Task
segmentation Task
requiring O
a O
separate O
model O
to O
be O
trained O
in O
advance O
. O
Furthermore O
, O
these O
segmentation Method
steps Method
are O
often O
tuned O
or O
designed O
separately O
from O
the O
ultimate O
objective O
of O
translation Task
quality O
, O
potentially O
contributing O
to O
a O
suboptimal Metric
quality Metric
. O
Based O
on O
this O
observation O
and O
analysis O
, O
in O
this O
paper O
, O
we O
ask O
ourselves O
and O
the O
readers O
a O
question O
which O
should O
have O
been O
asked O
much O
earlier O
: O
Is O
it O
possible O
to O
do O
character O
- O
level O
translation Task
without O
any O
explicit Method
segmentation Method
? O
subsection O
: O
Why O
Word Task
- Task
Level Task
Translation Task
? O
paragraph O
: O
( O
1 O
) O
Word O
as O
a O
Basic O
Unit O
of O
Meaning O
A O
word O
can O
be O
understood O
in O
two O
different O
senses O
. O
In O
the O
abstract O
sense O
, O
a O
word O
is O
a O
basic O
unit O
of O
meaning O
( O
lexeme O
) O
, O
and O
in O
the O
other O
sense O
, O
can O
be O
understood O
as O
a O
“ O
concrete O
word O
as O
used O
in O
a O
sentence O
. O
” O
. O
A O
word O
in O
the O
former O
sense O
turns O
into O
that O
in O
the O
latter O
sense O
via O
a O
process O
of O
morphology O
, O
including O
inflection O
, O
compounding Task
and O
derivation Task
. O
These O
three O
processes O
do O
alter O
the O
meaning O
of O
the O
lexeme O
, O
but O
often O
it O
stays O
close O
to O
the O
original O
meaning O
. O
Because O
of O
this O
view O
of O
words O
as O
basic O
units O
of O
meaning O
( O
either O
in O
the O
form O
of O
lexemes O
or O
derived O
form O
) O
from O
linguistics Method
, O
much O
of O
previous O
work O
in O
natural Task
language Task
processing Task
has O
focused O
on O
using O
words O
as O
basic O
units O
of O
which O
a O
sentence O
is O
encoded O
as O
a O
sequence O
. O
Also O
, O
the O
potential O
difficulty O
in O
finding O
a O
mapping O
between O
a O
word O
’s O
character O
sequence O
and O
meaning O
has O
likely O
contributed O
to O
this O
trend O
toward O
word Method
- Method
level Method
modelling Method
. O
paragraph O
: O
( O
2 O
) O
Data O
Sparsity O
There O
is O
a O
further O
technical O
reason O
why O
much O
of O
previous O
research O
on O
machine Task
translation Task
has O
considered O
words O
as O
a O
basic O
unit O
. O
This O
is O
mainly O
due O
to O
the O
fact O
that O
major O
components O
in O
the O
existing O
translation Task
systems O
, O
such O
as O
language Method
models Method
and O
phrase Method
tables Method
, O
are O
a O
count Method
- Method
based Method
estimator Method
of Method
probabilities Method
. O
In O
other O
words O
, O
a O
probability O
of O
a O
subsequence O
of O
symbols O
, O
or O
pairs O
of O
symbols O
, O
is O
estimated O
by O
counting O
the O
number O
of O
its O
occurrences O
in O
a O
training O
corpus O
. O
This O
approach O
severely O
suffers O
from O
the O
issue O
of O
data O
sparsity O
, O
which O
is O
due O
to O
a O
large O
state O
space O
which O
grows O
exponentially O
w.r.t O
. O
the O
length O
of O
subsequences O
while O
growing O
only O
linearly O
w.r.t O
. O
the O
corpus O
size O
. O
This O
poses O
a O
great O
challenge O
to O
character Method
- Method
level Method
modelling Method
, O
as O
any O
subsequence O
will O
be O
on O
average O
4–5 O
times O
longer O
when O
characters O
, O
instead O
of O
words O
, O
are O
used O
. O
Indeed O
, O
vilar2007can O
reported O
worse O
performance O
when O
the O
character O
sequence O
was O
directly O
used O
by O
a O
phrase O
- O
based O
machine Task
translation Task
system O
. O
More O
recently O
, O
neubig2013substring O
proposed O
a O
method O
to O
improve O
character O
- O
level O
translation Task
with O
phrase O
- O
based O
translation Task
systems O
, O
however O
, O
with O
only O
a O
limited O
success O
. O
paragraph O
: O
( O
3 O
) O
Vanishing O
Gradient O
Specifically O
to O
neural O
machine Task
translation Task
, O
a O
major O
reason O
behind O
the O
wide O
adoption O
of O
word Method
- Method
level Method
modelling Method
is O
due O
to O
the O
difficulty O
in O
modelling Task
long Task
- Task
term Task
dependencies Task
with O
recurrent Method
neural Method
networks Method
. O
As O
the O
lengths O
of O
the O
sentences O
on O
both O
sides O
grow O
when O
they O
are O
represented O
in O
characters O
, O
it O
is O
easy O
to O
believe O
that O
there O
will O
be O
more O
long O
- O
term O
dependencies O
that O
must O
be O
captured O
by O
the O
recurrent Method
neural Method
network Method
for O
successful O
translation Task
. O
subsection O
: O
Why O
Character Task
- Task
Level Task
Translation Task
? O
paragraph O
: O
Why O
not O
Word Task
- Task
Level Task
Translation Task
? O
The O
most O
pressing O
issue O
with O
word Task
- Task
level Task
processing Task
is O
that O
we O
do O
not O
have O
a O
perfect O
word Method
segmentation Method
algorithm Method
for O
any O
one O
language O
. O
A O
perfect O
segmentation Method
algorithm Method
needs O
to O
be O
able O
to O
segment O
any O
given O
sentence O
into O
a O
sequence O
of O
lexemes O
and O
morphemes O
. O
This O
problem O
is O
however O
a O
difficult O
problem O
on O
its O
own O
and O
often O
requires O
decades O
of O
research O
( O
see O
, O
e.g. O
, O
creutz2005unsupervised O
for O
Finnish O
and O
other O
morphologically O
rich O
languages O
and O
huang2007chinese O
for O
Chinese O
) O
. O
Therefore O
, O
many O
opt O
to O
using O
either O
a O
rule Method
- Method
based Method
tokenization Method
approach Method
or O
a O
suboptimal O
, O
but O
still O
available O
, O
learning Method
based Method
segmentation Method
algorithm Method
. O
The O
outcome O
of O
this O
naive O
, O
sub Task
- Task
optimal Task
segmentation Task
is O
that O
the O
vocabulary O
is O
often O
filled O
with O
many O
similar O
words O
that O
share O
a O
lexeme O
but O
have O
different O
morphology O
. O
For O
instance O
, O
if O
we O
apply O
a O
simple O
tokenization Method
script Method
to O
an O
English O
corpus O
, O
“ O
run O
” O
, O
“ O
runs O
” O
, O
“ O
ran O
” O
and O
“ O
running O
” O
are O
all O
separate O
entries O
in O
the O
vocabulary O
, O
while O
they O
clearly O
share O
the O
same O
lexeme O
“ O
run O
” O
. O
This O
prevents O
any O
machine Task
translation Task
system O
, O
in O
particular O
neural O
machine Task
translation Task
, O
from O
modelling O
these O
morphological O
variants O
efficiently O
. O
More O
specifically O
in O
the O
case O
of O
neural O
machine Task
translation Task
, O
each O
of O
these O
morphological O
variants–“run O
” O
, O
“ O
runs O
” O
, O
“ O
ran O
” O
and O
“ O
running”– O
will O
be O
assigned O
a O
- O
dimensional O
word O
vector O
, O
leading O
to O
four O
independent O
vectors O
, O
while O
it O
is O
clear O
that O
if O
we O
can O
segment O
those O
variants O
into O
a O
lexeme O
and O
other O
morphemes O
, O
we O
can O
model O
them O
more O
efficiently O
. O
For O
instance O
, O
we O
can O
have O
a O
- O
dimensional O
vector O
for O
the O
lexeme O
“ O
run O
” O
and O
much O
smaller O
vectors O
for O
“ O
s O
” O
and“ing O
” O
. O
Each O
of O
those O
variants O
will O
be O
then O
a O
composite O
of O
the O
lexeme O
vector O
( O
shared O
across O
these O
variants O
) O
and O
morpheme O
vectors O
( O
shared O
across O
words O
sharing O
the O
same O
suffix O
, O
for O
example O
) O
. O
This O
makes O
use O
of O
distributed Method
representation Method
, O
which O
generally O
yields O
better O
generalization Task
, O
but O
seems O
to O
require O
an O
optimal O
segmentation O
, O
which O
is O
unfortunately O
almost O
never O
available O
. O
In O
addition O
to O
inefficiency O
in O
modelling Task
, O
there O
are O
two O
additional O
negative O
consequences O
from O
using O
( O
unsegmented O
) O
words O
. O
First O
, O
the O
translation Task
system O
can O
not O
generalize O
well O
to O
novel O
words O
, O
which O
are O
often O
mapped O
to O
a O
token O
reserved O
for O
an O
unknown O
word O
. O
This O
effectively O
ignores O
any O
meaning O
or O
structure O
of O
the O
word O
to O
be O
incorporated O
when O
translating Task
. O
Second O
, O
even O
when O
a O
lexeme O
is O
common O
and O
frequently O
observed O
in O
the O
training O
corpus O
, O
its O
morphological O
variant O
may O
not O
be O
. O
This O
implies O
that O
the O
model O
sees O
this O
specific O
, O
rare O
morphological O
variant O
much O
less O
and O
will O
not O
be O
able O
to O
translate O
it O
well O
. O
However O
, O
if O
this O
rare O
morphological Method
variant Method
shares O
a O
large O
part O
of O
its O
spelling O
with O
other O
more O
common O
words O
, O
it O
is O
desirable O
for O
a O
machine Task
translation Task
system O
to O
exploit O
those O
common O
words O
when O
translating O
those O
rare O
variants O
. O
paragraph O
: O
Why O
Character Task
- Task
Level Task
Translation Task
? O
All O
of O
these O
issues O
can O
be O
addressed O
to O
certain O
extent O
by O
directly O
modelling O
characters O
. O
Although O
the O
issue O
of O
data Task
sparsity Task
arises O
in O
character O
- O
level O
translation Task
, O
it O
is O
elegantly O
addressed O
by O
using O
a O
parametric Method
approach Method
based O
on O
recurrent Method
neural Method
networks Method
instead O
of O
a O
non Method
- Method
parametric Method
count Method
- Method
based Method
approach Method
. O
Furthermore O
, O
in O
recent O
years O
, O
we O
have O
learned O
how O
to O
build O
and O
train O
a O
recurrent Method
neural Method
network Method
that O
can O
well O
capture O
long O
- O
term O
dependencies O
by O
using O
more O
sophisticated O
activation O
functions O
, O
such O
as O
long Method
short Method
- Method
term Method
memory Method
( Method
LSTM Method
) Method
units Method
and O
gated Method
recurrent Method
units Method
. O
kim2015character O
and O
ling2015finding O
recently O
showed O
that O
by O
having O
a O
neural Method
network Method
that O
converts O
a O
character O
sequence O
into O
a O
word O
vector O
, O
we O
avoid O
the O
issues O
from O
having O
many O
morphological O
variants O
appearing O
as O
separate O
entities O
in O
a O
vocabulary O
. O
This O
is O
made O
possible O
by O
sharing O
the O
character Method
- Method
to Method
- Method
word Method
neural Method
network Method
across O
all O
the O
unique O
tokens O
. O
A O
similar O
approach O
was O
applied O
to O
machine Task
translation Task
by O
ling2015character O
. O
These O
recent O
approaches O
, O
however O
, O
still O
rely O
on O
the O
availability O
of O
a O
good O
, O
if O
not O
optimal O
, O
segmentation Method
algorithm Method
. O
ling2015character O
indeed O
states O
that O
“ O
[ O
m O
] O
uch O
of O
the O
prior O
information O
regarding O
morphology O
, O
cognates O
and O
rare O
word O
translation Task
among O
others O
, O
should O
be O
incorporated O
” O
. O
It O
however O
becomes O
unnecessary O
to O
consider O
these O
prior O
information O
, O
if O
we O
use O
a O
neural Method
network Method
, O
be O
it O
recurrent Method
, O
convolution Method
or O
their O
combination O
, O
directly O
on O
the O
unsegmented O
character O
sequence O
. O
The O
possibility O
of O
using O
a O
sequence O
of O
unsegmented O
characters O
has O
been O
studied O
over O
many O
years O
in O
the O
field O
of O
deep Task
learning Task
. O
For O
instance O
, O
mikolov2012subword O
and O
sutskever2011generating O
trained O
a O
recurrent Method
neural Method
network Method
language Method
model Method
( O
RNN Method
- Method
LM Method
) O
on O
character O
sequences O
. O
The O
latter O
showed O
that O
it O
is O
possible O
to O
generate O
sensible O
text O
sequences O
by O
simply O
sampling O
a O
character O
at O
a O
time O
from O
this O
model O
. O
More O
recently O
, O
zhang2015character O
and O
xiao2016efficient O
successfully O
applied O
a O
convolutional Method
net Method
and O
a O
convolutional Method
- Method
recurrent Method
net Method
respectively O
to O
character Task
- Task
level Task
document Task
classification Task
without O
any O
explicit Method
segmentation Method
. O
gillick2015multilingual O
further O
showed O
that O
it O
is O
possible O
to O
train O
a O
recurrent Method
neural Method
network Method
on O
unicode O
bytes O
, O
instead O
of O
characters O
or O
words O
, O
to O
perform O
part Task
- Task
of Task
- Task
speech Task
tagging Task
and O
named Task
entity Task
recognition Task
. O
These O
previous O
works O
suggest O
the O
possibility O
of O
applying O
neural Method
networks Method
for O
the O
task O
of O
machine Task
translation Task
, O
which O
is O
often O
considered O
a O
substantially O
more O
difficult O
problem O
compared O
to O
document Task
classification Task
and O
language Task
modelling Task
. O
subsection O
: O
Challenges O
and O
Questions O
There O
are O
two O
overlapping O
sets O
of O
challenges O
for O
the O
source O
and O
target O
sides O
. O
On O
the O
source O
side O
, O
it O
is O
unclear O
how O
to O
build O
a O
neural Method
network Method
that O
learns O
a O
highly O
nonlinear O
mapping O
from O
a O
spelling O
to O
the O
meaning O
of O
a O
sentence O
. O
On O
the O
target O
side O
, O
there O
are O
two O
challenges O
. O
The O
first O
challenge O
is O
the O
same O
one O
from O
the O
source O
side O
, O
as O
the O
decoder Method
neural Method
network Method
needs O
to O
summarize O
what O
has O
been O
translated O
. O
In O
addition O
to O
this O
, O
the O
character Method
- Method
level Method
modelling Method
on O
the O
target O
side O
is O
more O
challenging O
, O
as O
the O
decoder Method
network Method
must O
be O
able O
to O
generate O
a O
long O
, O
coherent O
sequence O
of O
characters O
. O
This O
is O
a O
great O
challenge O
, O
as O
the O
size O
of O
the O
state O
space O
grows O
exponentially O
w.r.t O
. O
the O
number O
of O
symbols O
, O
and O
in O
the O
case O
of O
characters O
, O
it O
is O
often O
300 O
- O
1000 O
symbols O
long O
. O
All O
these O
challenges O
should O
first O
be O
framed O
as O
questions O
; O
whether O
the O
current O
recurrent Method
neural Method
networks Method
, O
which O
are O
already O
widely O
used O
in O
neural O
machine Task
translation Task
, O
are O
able O
to O
address O
these O
challenges O
as O
they O
are O
. O
In O
this O
paper O
, O
we O
aim O
at O
answering O
these O
questions O
empirically O
and O
focus O
on O
the O
challenges O
on O
the O
target O
side O
( O
as O
the O
target O
side O
shows O
both O
of O
the O
challenges O
) O
. O
section O
: O
Character Task
- Task
Level Task
Translation Task
In O
this O
paper O
, O
we O
try O
to O
answer O
the O
questions O
posed O
earlier O
by O
testing O
two O
different O
types O
of O
recurrent Method
neural Method
networks Method
on O
the O
target O
side O
( O
decoder O
) O
. O
First O
, O
we O
test O
an O
existing O
recurrent Method
neural Method
network Method
with O
gated Method
recurrent Method
units Method
( O
GRUs Method
) O
. O
We O
call O
this O
decoder O
a O
base Method
decoder O
. O
Second O
, O
we O
build O
a O
novel O
two Method
- Method
layer Method
recurrent Method
neural Method
network Method
, O
inspired O
by O
the O
gated Method
- Method
feedback Method
network Method
from O
chung2015gated O
, O
called O
a O
bi Method
- Method
scale Method
recurrent O
neural O
network O
. O
We O
design O
this O
network O
to O
facilitate O
capturing O
two O
timescales O
, O
motivated O
by O
the O
fact O
that O
characters O
and O
words O
may O
work O
at O
two O
separate O
timescales O
. O
We O
choose O
to O
test O
these O
two O
alternatives O
for O
the O
following O
purposes O
. O
Experiments O
with O
the O
base Method
decoder O
will O
clearly O
answer O
whether O
the O
existing O
neural Method
network Method
is O
enough O
to O
handle O
character Method
- Method
level Method
decoding Method
, O
which O
has O
not O
been O
properly O
answered O
in O
the O
context O
of O
machine Task
translation Task
. O
The O
alternative O
, O
the O
bi Method
- Method
scale Method
decoder O
, O
is O
tested O
in O
order O
to O
see O
whether O
it O
is O
possible O
to O
design O
a O
better O
decoder O
, O
if O
the O
answer O
to O
the O
first O
question O
is O
positive O
. O
( O
a O
) O
Gating Method
units Method
( O
b O
) O
One Method
- Method
step Method
processing Method
subsection O
: O
Bi Method
- Method
Scale Method
Recurrent Method
Neural Method
Network Method
In O
this O
proposed O
bi Method
- Method
scale Method
recurrent O
neural O
network O
, O
there O
are O
two O
sets O
of O
hidden O
units O
, O
and O
. O
They O
contain O
the O
same O
number O
of O
units O
, O
i.e. O
, O
. O
The O
first O
set O
models O
a O
fast O
- O
changing O
timescale O
( O
thereby O
, O
a O
faster O
layer O
) O
, O
and O
a O
slower O
timescale O
( O
thereby O
, O
a O
slower O
layer O
) O
. O
For O
each O
hidden O
unit O
, O
there O
is O
an O
associated O
gating O
unit O
, O
to O
which O
we O
refer O
by O
and O
. O
For O
the O
description O
below O
, O
we O
use O
and O
for O
the O
previous O
target O
symbol O
and O
the O
context O
vector O
( O
see O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
, O
respectively O
. O
Let O
us O
start O
with O
the O
faster O
layer O
. O
The O
faster Method
layer Method
outputs O
two O
sets O
of O
activations O
, O
a O
normal O
output O
and O
its O
gated Method
version Method
. O
The O
activation O
of O
the O
faster O
layer O
is O
computed O
by O
where O
and O
are O
the O
gated O
activations O
of O
the O
faster O
and O
slower O
layers O
respectively O
. O
These O
gated O
activations O
are O
computed O
by O
In O
other O
words O
, O
the O
faster O
layer O
’s O
activation O
is O
based O
on O
the O
adaptive Method
combination Method
of O
the O
faster O
and O
slower O
layers O
’ O
activations O
from O
the O
previous O
time O
step O
. O
Whenever O
the O
faster O
layer O
determines O
that O
it O
needs O
to O
reset O
, O
i.e. O
, O
, O
the O
next O
activation O
will O
be O
determined O
based O
more O
on O
the O
slower O
layer O
’s O
activation O
. O
The O
faster Method
layer Method
’s Method
gating Method
unit Method
is O
computed O
by O
where O
is O
a O
sigmoid Method
function Method
. O
The O
slower Method
layer Method
also O
outputs O
two O
sets O
of O
activations O
, O
a O
normal O
output O
and O
its O
gated Method
version Method
. O
These O
activations O
are O
computed O
as O
follows O
: O
where O
is O
a O
candidate O
activation O
. O
The O
slower O
layer O
’s O
gating O
unit O
is O
computed O
by O
This O
adaptive Method
leaky Method
integration Method
based O
on O
the O
gating Method
unit Method
from O
the O
faster O
layer O
has O
a O
consequence O
that O
the O
slower O
layer O
updates O
its O
activation O
only O
when O
the O
faster O
layer O
resets O
. O
This O
puts O
a O
soft O
constraint O
that O
the O
faster Method
layer Method
runs O
at O
a O
faster O
rate O
by O
preventing O
the O
slower O
layer O
from O
updating O
while O
the O
faster Method
layer Method
is O
processing O
a O
current O
chunk O
. O
The O
candidate O
activation O
is O
then O
computed O
by O
indicates O
the O
reset O
activation O
from O
the O
previous O
time O
step O
, O
similarly O
to O
what O
happened O
in O
the O
faster Method
layer Method
, O
and O
is O
the O
input O
from O
the O
context O
. O
According O
to O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
, O
the O
faster O
layer O
influences O
the O
slower O
layer O
, O
only O
when O
the O
faster O
layer O
has O
finished O
processing O
the O
current O
chunk O
and O
is O
about O
to O
reset O
itself O
( O
) O
. O
In O
other O
words O
, O
the O
slower Method
layer Method
does O
not O
receive O
any O
input O
from O
the O
faster O
layer O
, O
until O
the O
faster O
layer O
has O
quickly O
processed O
the O
current O
chunk O
, O
thereby O
running O
at O
a O
slower O
rate O
than O
the O
faster Method
layer Method
does O
. O
At O
each O
time O
step O
, O
the O
final O
output O
of O
the O
proposed O
bi Method
- Method
scale Method
recurrent O
neural O
network O
is O
the O
concatenation O
of O
the O
output O
vectors O
of O
the O
faster O
and O
slower O
layers O
, O
i.e. O
, O
. O
This O
concatenated O
vector O
is O
used O
to O
compute O
the O
probability O
distribution O
over O
all O
the O
symbols O
in O
the O
vocabulary O
, O
as O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
. O
See O
Fig O
. O
[ O
reference O
] O
for O
graphical O
illustration O
. O
Src Method
Depth Method
Model Method
En O
- O
De O
BPE Method
En O
- O
Ru O
BPE Method
En O
- O
Fi O
BPE Method
section O
: O
Experiment O
Settings O
For O
evaluation O
, O
we O
represent O
a O
source O
sentence O
as O
a O
sequence O
of O
subword O
symbols O
extracted O
by O
byte Method
- Method
pair Method
encoding Method
( O
BPE Method
, O
sennrich2015neural O
) O
and O
a O
target O
sentence O
either O
as O
a O
sequence O
of O
BPE Method
- O
based O
symbols O
or O
as O
a O
sequence O
of O
characters O
. O
paragraph O
: O
Corpora O
and O
Preprocessing O
We O
use O
all O
available O
parallel O
corpora O
for O
four O
language O
pairs O
from O
WMT’15 Material
: O
En O
- O
Cs O
, O
En O
- O
De O
, O
En O
- O
Ru O
and O
En O
- O
Fi O
. O
They O
consist O
of O
12.1 O
M O
, O
4.5 O
M O
, O
2.3 O
M O
and O
2 O
M O
sentence O
pairs O
, O
respectively O
. O
We O
tokenize O
each O
corpus O
using O
a O
tokenization Method
script Method
included O
in O
Moses O
. O
We O
only O
use O
the O
sentence O
pairs O
, O
when O
the O
source O
side O
is O
up O
to O
50 O
subword O
symbols O
long O
and O
the O
target O
side O
is O
either O
up O
to O
100 O
subword O
symbols O
or O
500 O
characters O
. O
We O
do O
not O
use O
any O
monolingual O
corpus O
. O
For O
all O
the O
pairs O
other O
than O
En O
- O
Fi O
, O
we O
use O
newstest O
- O
2013 O
as O
a O
development O
set O
, O
and O
newstest O
- O
2014 O
( O
Test O
) O
and O
newstest O
- O
2015 O
( O
Test O
) O
as O
test O
sets O
. O
For O
En O
- O
Fi O
, O
we O
use O
newsdev O
- O
2015 O
and O
newstest O
- O
2015 O
as O
development O
and O
test O
sets O
, O
respectively O
. O
paragraph O
: O
Models O
and O
Training O
We O
test O
three O
models O
settings O
: O
( O
1 O
) O
BPE Method
BPE Method
, O
( O
2 O
) O
BPE Method
Char Method
( O
base Method
) O
and O
( O
3 O
) O
BPE Method
Char Method
( O
bi Method
- Method
scale Method
) O
. O
The O
latter O
two O
differ O
by O
the O
type O
of O
recurrent Method
neural Method
network Method
we O
use O
. O
We O
use O
GRUs Method
for O
the O
encoder O
in O
all O
the O
settings O
. O
We O
used O
GRUs Method
for O
the O
decoders Method
in O
the O
first O
two O
settings O
, O
( O
1 O
) O
and O
( O
2 O
) O
, O
while O
the O
proposed O
bi Method
- Method
scale Method
recurrent O
network O
was O
used O
in O
the O
last O
setting O
, O
( O
3 O
) O
. O
The O
encoder Method
has O
hidden O
units O
for O
each O
direction O
( O
forward O
and O
reverse O
) O
, O
and O
the O
decoder Method
has O
hidden O
units O
per O
layer O
. O
We O
train O
each O
model O
using O
stochastic Method
gradient Method
descent Method
with O
Adam Method
. O
Each O
update O
is O
computed O
using O
a O
minibatch O
of O
128 O
sentence O
pairs O
. O
The O
norm O
of O
the O
gradient O
is O
clipped O
with O
a O
threshold O
. O
paragraph O
: O
Decoding Task
and O
Evaluation O
We O
use O
beamsearch Method
to O
approximately O
find O
the O
most O
likely O
translation Task
given O
a O
source O
sentence O
. O
The O
beam O
widths O
are O
and O
respectively O
for O
the O
subword Method
- Method
level Method
and O
character Method
- Method
level Method
decoders Method
. O
They O
were O
chosen O
based O
on O
the O
translation Task
quality O
on O
the O
development O
set O
. O
The O
translations O
are O
evaluated O
using O
BLEU Metric
. O
paragraph O
: O
Multilayer Method
Decoder Method
and O
Soft Method
- Method
Alignment Method
Mechanism Method
When O
the O
decoder Method
is O
a O
multilayer Method
recurrent Method
neural Method
network Method
( O
including O
a O
stacked Method
network Method
as O
well O
as O
the O
proposed O
bi Method
- Method
scale Method
network O
) O
, O
the O
decoder O
outputs O
multiple O
hidden O
vectors– O
for O
layers O
, O
at O
a O
time O
. O
This O
allows O
an O
extra O
degree O
of O
freedom O
in O
the O
soft O
- O
alignment O
mechanism O
( O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
. O
We O
evaluate O
using O
alternatives O
, O
including O
( O
1 O
) O
using O
only O
( O
slower O
layer O
) O
and O
( O
2 O
) O
using O
all O
of O
them O
( O
concatenated O
) O
. O
paragraph O
: O
Ensembles O
We O
also O
evaluate O
an O
ensemble O
of O
neural O
machine Task
translation Task
models O
and O
compare O
its O
performance O
against O
the O
state O
- O
of O
- O
the O
- O
art O
phrase O
- O
based O
translation Task
systems O
on O
all O
four O
language O
pairs O
. O
We O
decode O
from O
an O
ensemble O
by O
taking O
the O
average O
of O
the O
output O
probabilities O
at O
each O
step O
. O
section O
: O
Quantitative Task
Analysis Task
paragraph O
: O
Slower Method
Layer Method
for O
Alignment Task
On O
En O
- O
De O
, O
we O
test O
which O
layer O
of O
the O
decoder Method
should O
be O
used O
for O
computing O
soft Task
- Task
alignments Task
. O
In O
the O
case O
of O
subword Method
- Method
level Method
decoder O
, O
we O
observed O
no O
difference O
between O
choosing O
any O
of O
the O
two O
layers O
of O
the O
decoder O
against O
using O
the O
concatenation O
of O
all O
the O
layers O
( O
Table O
[ O
reference O
] O
( O
a O
– O
b O
) O
) O
On O
the O
other O
hand O
, O
with O
the O
character Method
- Method
level Method
decoder Method
, O
we O
noticed O
an O
improvement O
when O
only O
the O
slower O
layer O
( O
) O
was O
used O
for O
the O
soft Method
- Method
alignment Method
mechanism Method
( O
Table O
[ O
reference O
] O
( O
c O
– O
g O
) O
) O
. O
This O
suggests O
that O
the O
soft Method
- Method
alignment Method
mechanism Method
benefits O
by O
aligning O
a O
larger O
chunk O
in O
the O
target O
with O
a O
subword O
unit O
in O
the O
source O
, O
and O
we O
use O
only O
the O
slower Method
layer Method
for O
all O
the O
other O
language O
pairs O
. O
paragraph O
: O
Single O
Models O
In O
Table O
[ O
reference O
] O
, O
we O
present O
a O
comprehensive O
report O
of O
the O
translation Task
qualities O
of O
( O
1 O
) O
subword Method
- Method
level Method
decoder O
, O
( O
2 O
) O
character Method
- Method
level Method
base Method
decoder Method
and O
( O
3 O
) O
character Method
- Method
level Method
bi Method
- Method
scale Method
decoder Method
, O
for O
all O
the O
language O
pairs O
. O
We O
see O
that O
the O
both O
types O
of O
character Method
- Method
level Method
decoder Method
outperform O
the O
subword Method
- Method
level Method
decoder O
for O
En O
- O
Cs O
and O
En O
- O
Fi O
quite O
significantly O
. O
On O
En O
- O
De O
, O
the O
character Method
- Method
level Method
base Method
decoder Method
outperforms O
both O
the O
subword Method
- Method
level Method
decoder O
and O
the O
character Method
- Method
level Method
bi Method
- Method
scale Method
decoder Method
, O
validating O
the O
effectiveness O
of O
the O
character Method
- Method
level Method
modelling Method
. O
On O
En O
- O
Ru O
, O
among O
the O
single O
models O
, O
the O
character Method
- Method
level Method
decoders Method
outperform O
the O
subword Method
- Method
level Method
decoder O
, O
but O
in O
general O
, O
we O
observe O
that O
all O
the O
three O
alternatives O
work O
comparable O
to O
each O
other O
. O
These O
results O
clearly O
suggest O
that O
it O
is O
indeed O
possible O
to O
do O
character O
- O
level O
translation Task
without O
explicit Method
segmentation Method
. O
In O
fact O
, O
what O
we O
observed O
is O
that O
character O
- O
level O
translation Task
often O
surpasses O
the O
translation Task
quality O
of O
word O
- O
level O
translation Task
. O
Of O
course O
, O
we O
note O
once O
again O
that O
our O
experiment O
is O
restricted O
to O
using O
an O
unsegmented O
character O
sequence O
at O
the O
decoder O
only O
, O
and O
a O
further O
exploration O
toward O
replacing O
the O
source O
sentence O
with O
an O
unsegmented O
character O
sequence O
is O
needed O
. O
paragraph O
: O
Ensembles O
Each O
ensemble O
was O
built O
using O
eight O
independent Method
models Method
. O
The O
first O
observation O
we O
make O
is O
that O
in O
all O
the O
language O
pairs O
, O
neural O
machine Task
translation Task
performs O
comparably O
to O
, O
or O
often O
better O
than O
, O
the O
state O
- O
of O
- O
the O
- O
art O
non O
- O
neural O
translation Task
system O
. O
Furthermore O
, O
the O
character Method
- Method
level Method
decoders Method
outperform O
the O
subword Method
- Method
level Method
decoder O
in O
all O
the O
cases O
. O
section O
: O
Qualitative Task
Analysis Task
paragraph O
: O
( O
1 O
) O
Can O
the O
character Method
- Method
level Method
decoder Method
generate O
a O
long O
, O
coherent O
sentence O
? O
The O
translation Task
in O
characters O
is O
dramatically O
longer O
than O
that O
in O
words O
, O
likely O
making O
it O
more O
difficult O
for O
a O
recurrent Method
neural Method
network Method
to O
generate O
a O
coherent O
sentence O
in O
characters O
. O
This O
belief O
turned O
out O
to O
be O
false O
. O
As O
shown O
in O
Fig O
. O
[ O
reference O
] O
( O
left O
) O
, O
there O
is O
no O
significant O
difference O
between O
the O
subword Method
- Method
level Method
and O
character Method
- Method
level Method
decoders Method
, O
even O
though O
the O
lengths O
of O
the O
generated O
translations O
are O
generally O
5–10 O
times O
longer O
in O
characters O
. O
paragraph O
: O
( O
2 O
) O
Does O
the O
character Method
- Method
level Method
decoder Method
help O
with O
rare O
words O
? O
One O
advantage O
of O
character Method
- Method
level Method
modelling Method
is O
that O
it O
can O
model O
the O
composition O
of O
any O
character O
sequence O
, O
thereby O
better O
modelling O
rare O
morphological O
variants O
. O
We O
empirically O
confirm O
this O
by O
observing O
the O
growing O
gap O
in O
the O
average Metric
negative Metric
log Metric
- Metric
probability Metric
of Metric
words Metric
between O
the O
subword Method
- Method
level Method
and O
character Method
- Method
level Method
decoders Method
as O
the O
frequency O
of O
the O
words O
decreases O
. O
This O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
( O
right O
) O
and O
explains O
one O
potential O
cause O
behind O
the O
success O
of O
character Method
- Method
level Method
decoding Method
in O
our O
experiments O
( O
we O
define O
) O
. O
paragraph O
: O
( O
3 O
) O
Can O
the O
character Method
- Method
level Method
decoder Method
soft O
- O
align O
between O
a O
source O
word O
and O
a O
target O
character O
? O
In O
Fig O
. O
[ O
reference O
] O
( O
left O
) O
, O
we O
show O
an O
example O
soft Task
- Task
alignment Task
of O
a O
source O
sentence O
, O
“ O
Two O
sets O
of O
light O
so O
close O
to O
one O
another O
” O
. O
It O
is O
clear O
that O
the O
character Method
- Method
level Method
translation Method
model Method
well O
captured O
the O
alignment O
between O
the O
source O
subwords O
and O
target O
characters O
. O
We O
observe O
that O
the O
character Method
- Method
level Method
decoder Method
correctly O
aligns O
to O
“ O
lights O
” O
and O
“ O
sets O
of O
” O
when O
generating O
a O
German O
compound O
word O
“ O
Lichtersets O
” O
( O
see O
Fig O
. O
[ O
reference O
] O
( O
right O
) O
for O
the O
zoomed O
- O
in O
version O
) O
. O
This O
type O
of O
behaviour O
happens O
similarly O
between O
“ O
one O
another O
” O
and O
“ O
einander O
” O
. O
Of O
course O
, O
this O
does O
not O
mean O
that O
there O
exists O
an O
alignment O
between O
a O
source O
word O
and O
a O
target O
character O
. O
Rather O
, O
this O
suggests O
that O
the O
internal O
state O
of O
the O
character Method
- Method
level Method
decoder Method
, O
the O
base Method
or O
bi Method
- Method
scale Method
, O
well O
captures O
the O
meaningful O
chunk O
of O
characters O
, O
allowing O
the O
model O
to O
map O
it O
to O
a O
larger O
chunk O
( O
subword O
) O
in O
the O
source O
. O
paragraph O
: O
( O
4 O
) O
How O
fast O
is O
the O
decoding Metric
speed Metric
of O
the O
character Method
- Method
level Method
decoder Method
? O
We O
evaluate O
the O
decoding Metric
speed Metric
of O
subword Method
- Method
level Method
base Method
, O
character Method
- Method
level Method
base Method
and O
character Method
- Method
level Method
bi Method
- Method
scale Method
decoders Method
on O
newstest O
- O
2013 O
corpus O
( O
En O
- O
De O
) O
with O
a O
single O
Titan Method
X Method
GPU Method
. O
The O
subword Method
- Method
level Method
base Method
decoder O
generates O
31.9 O
words O
per O
second O
, O
and O
the O
character Method
- Method
level Method
base Method
decoder Method
and O
character Method
- Method
level Method
bi Method
- Method
scale Method
decoder Method
generate O
27.5 O
words O
per O
second O
and O
25.6 O
words O
per O
second O
, O
respectively O
. O
Note O
that O
this O
is O
evaluated O
in O
an O
online Task
setting Task
, O
performing O
consecutive Task
translation Task
, O
where O
only O
one O
sentence O
is O
translated O
at O
a O
time O
. O
Translating O
in O
a O
batch O
setting O
could O
differ O
from O
these O
results O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
addressed O
a O
fundamental O
question O
on O
whether O
a O
recently O
proposed O
neural O
machine Task
translation Task
system O
can O
directly O
handle O
translation Task
at O
the O
level O
of O
characters O
without O
any O
word Method
segmentation Method
. O
We O
focused O
on O
the O
target O
side O
, O
in O
which O
a O
decoder O
was O
asked O
to O
generate O
one O
character O
at O
a O
time O
, O
while O
soft O
- O
aligning O
between O
a O
target O
character O
and O
a O
source O
subword O
. O
Our O
extensive O
experiments O
, O
on O
four O
language O
pairs O
– O
En O
- O
Cs O
, O
En O
- O
De O
, O
En O
- O
Ru O
and O
En O
- O
Fi– O
strongly O
suggest O
that O
it O
is O
indeed O
possible O
for O
neural O
machine Task
translation Task
to O
translate O
at O
the O
level O
of O
characters O
, O
and O
that O
it O
actually O
benefits O
from O
doing O
so O
. O
Our O
result O
has O
one O
limitation O
that O
we O
used O
subword O
symbols O
in O
the O
source O
side O
. O
However O
, O
this O
has O
allowed O
us O
a O
more O
fine Task
- Task
grained Task
analysis Task
, O
but O
in O
the O
future O
, O
a O
setting O
where O
the O
source O
side O
is O
also O
represented O
as O
a O
character O
sequence O
must O
be O
investigated O
. O
section O
: O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
the O
developers O
of O
Theano O
. O
We O
acknowledge O
the O
support O
of O
the O
following O
agencies O
for O
research O
funding O
and O
computing O
support O
: O
NSERC O
, O
Calcul O
Québec O
, O
Compute O
Canada O
, O
the O
Canada O
Research O
Chairs O
, O
CIFAR O
and O
Samsung O
. O
KC O
thanks O
the O
support O
by O
Facebook O
, O
Google O
( O
Google O
Faculty O
Award O
2016 O
) O
and O
NVIDIA O
( O
GPU O
Center O
of O
Excellence O
2015 O
- O
2016 O
) O
. O
JC O
thanks O
Orhan O
Firat O
for O
his O
constructive O
feedbacks O
. O
bibliography O
: O
References O
