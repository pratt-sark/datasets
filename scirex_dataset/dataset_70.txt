Supervised Method
learning Method
on O
molecules O
has O
incredible O
potential O
to O
be O
useful O
in O
chemistry Task
, O
drug Task
discovery Task
, O
and O
materials Task
science Task
. O
Luckily O
, O
several O
promising O
and O
closely O
related O
neural Method
network Method
models Method
invariant O
to O
molecular O
symmetries O
have O
already O
been O
described O
in O
the O
literature O
. O
These O
models O
learn O
a O
message Method
passing Method
algorithm Method
and O
aggregation Method
procedure Method
to O
compute O
a O
function O
of O
their O
entire O
input O
graph O
. O
At O
this O
point O
, O
the O
next O
step O
is O
to O
find O
a O
particularly O
effective O
variant O
of O
this O
general O
approach O
and O
apply O
it O
to O
chemical Task
prediction Task
benchmarks Task
until O
we O
either O
solve O
them O
or O
reach O
the O
limits O
of O
the O
approach O
. O
In O
this O
paper O
, O
we O
reformulate O
existing O
models O
into O
a O
single O
common O
framework O
we O
call O
Message Method
Passing Method
Neural Method
Networks Method
( O
MPNNs Method
) O
and O
explore O
additional O
novel O
variations O
within O
this O
framework O
. O
Using O
MPNNs Method
we O
demonstrate O
state O
of O
the O
art O
results O
on O
an O
important O
molecular Task
property Task
prediction Task
benchmark Task
; O
these O
results O
are O
strong O
enough O
that O
we O
believe O
future O
work O
should O
focus O
on O
datasets O
with O
larger O
molecules O
or O
more O
accurate O
ground O
truth O
labels O
. O
NeuralMessagePassingforQuantumChemistry O
section O
: O
Introduction O
The O
past O
decade O
has O
seen O
remarkable O
success O
in O
the O
use O
of O
deep Method
neural Method
networks Method
to O
understand O
and O
translate O
natural O
language O
, O
generate O
and O
decode O
complex O
audio O
signals O
, O
and O
infer O
features O
from O
real O
- O
world O
images O
and O
videos O
. O
Although O
chemists O
have O
applied O
machine Method
learning Method
to O
many O
problems O
over O
the O
years O
, O
predicting O
the O
properties Task
of Task
molecules Task
and Task
materials Task
using O
machine Method
learning Method
( O
and O
especially O
deep Method
learning Method
) O
is O
still O
in O
its O
infancy O
. O
To O
date O
, O
most O
research O
applying O
machine Method
learning Method
to O
chemistry Task
tasks Task
has O
revolved O
around O
feature Method
engineering Method
. O
While O
neural Method
networks Method
have O
been O
applied O
in O
a O
variety O
of O
situations O
, O
they O
have O
yet O
to O
become O
widely O
adopted O
. O
This O
situation O
is O
reminiscent O
of O
the O
state O
of O
image Method
models Method
before O
the O
broad O
adoption O
of O
convolutional Method
neural Method
networks Method
and O
is O
due O
, O
in O
part O
, O
to O
a O
dearth O
of O
empirical O
evidence O
that O
neural Method
architectures Method
with O
the O
appropriate O
inductive O
bias O
can O
be O
successful O
in O
this O
domain O
. O
Recently O
, O
large Task
scale Task
quantum Task
chemistry Task
calculation Task
and O
molecular Method
dynamics Method
simulations Method
coupled O
with O
advances O
in O
high O
throughput O
experiments O
have O
begun O
to O
generate O
data O
at O
an O
unprecedented O
rate O
. O
Most O
classical O
techniques O
do O
not O
make O
effective O
use O
of O
the O
larger O
amounts O
of O
data O
that O
are O
now O
available O
. O
The O
time O
is O
ripe O
to O
apply O
more O
powerful O
and O
flexible O
machine Method
learning Method
methods Method
to O
these O
problems O
, O
assuming O
we O
can O
find O
models O
with O
suitable O
inductive O
biases O
. O
The O
symmetries O
of O
atomic Method
systems Method
suggest O
neural Method
networks Method
that O
operate O
on O
graph O
structured O
data O
and O
are O
invariant O
to O
graph O
isomorphism O
might O
also O
be O
appropriate O
for O
molecules O
. O
Sufficiently O
successful O
models O
could O
someday O
help O
automate O
challenging O
chemical Task
search Task
problems Task
in O
drug Task
discovery Task
or O
materials Task
science Task
. O
In O
this O
paper O
, O
our O
goal O
is O
to O
demonstrate O
effective O
machine Method
learning Method
models Method
for O
chemical Task
prediction Task
problems Task
that O
are O
capable O
of O
learning O
their O
own O
features O
from O
molecular O
graphs O
directly O
and O
are O
invariant O
to O
graph O
isomorphism O
. O
To O
that O
end O
, O
we O
describe O
a O
general O
framework O
for O
supervised Task
learning Task
on Task
graphs Task
called O
Message Method
Passing Method
Neural Method
Networks Method
( O
MPNNs Method
) O
that O
simply O
abstracts O
the O
commonalities O
between O
several O
of O
the O
most O
promising O
existing O
neural Method
models Method
for O
graph O
structured O
data O
, O
in O
order O
to O
make O
it O
easier O
to O
understand O
the O
relationships O
between O
them O
and O
come O
up O
with O
novel O
variations O
. O
Given O
how O
many O
researchers O
have O
published O
models O
that O
fit O
into O
the O
MPNN Method
framework Method
, O
we O
believe O
that O
the O
community O
should O
push O
this O
general O
approach O
as O
far O
as O
possible O
on O
practically O
important O
graph Task
problems Task
and O
only O
suggest O
new O
variations O
that O
are O
well O
motivated O
by O
applications O
, O
such O
as O
the O
application O
we O
consider O
here O
: O
predicting O
the O
quantum Task
mechanical Task
properties Task
of Task
small Task
organic Task
molecules Task
( O
see O
task O
schematic O
in O
figure O
[ O
reference O
] O
) O
. O
In O
general O
, O
the O
search O
for O
practically O
effective O
machine Method
learning Method
( Method
ML Method
) Method
models Method
in O
a O
given O
domain O
proceeds O
through O
a O
sequence O
of O
increasingly O
realistic O
and O
interesting O
benchmarks O
. O
Here O
we O
focus O
on O
the O
QM9 Material
dataset Material
as O
such O
a O
benchmark O
. O
QM9 Material
consists O
of O
130k O
molecules O
with O
13 O
properties O
for O
each O
molecule O
which O
are O
approximated O
by O
an O
expensive O
quantum Method
mechanical Method
simulation Method
method Method
( O
DFT Method
) O
, O
to O
yield O
13 O
corresponding O
regression Task
tasks Task
. O
These O
tasks O
are O
plausibly O
representative O
of O
many O
important O
chemical Task
prediction Task
problems Task
and O
are O
( O
currently O
) O
difficult O
for O
many O
existing O
methods O
. O
Additionally O
, O
QM9 Material
also O
includes O
complete O
spatial O
information O
for O
the O
single O
low O
energy O
conformation O
of O
the O
atoms O
in O
the O
molecule O
that O
was O
used O
in O
calculating O
the O
chemical O
properties O
. O
QM9 Material
therefore O
lets O
us O
consider O
both O
the O
setting O
where O
the O
complete O
molecular O
geometry O
is O
known O
( O
atomic O
distances O
, O
bond O
angles O
, O
etc O
. O
) O
and O
the O
setting O
where O
we O
need O
to O
compute O
properties O
that O
might O
still O
be O
defined O
in O
terms O
of O
the O
spatial O
positions O
of O
atoms O
, O
but O
where O
only O
the O
atom O
and O
bond O
information O
( O
i.e. O
graph O
) O
is O
available O
as O
input O
. O
In O
the O
latter O
case O
, O
the O
model O
must O
implicitly O
fit O
something O
about O
the O
computation O
used O
to O
determine O
a O
low O
energy O
3D O
conformation O
and O
hopefully O
would O
still O
work O
on O
problems O
where O
it O
is O
not O
clear O
how O
to O
compute O
a O
reasonable O
3D O
conformation O
. O
When O
measuring O
the O
performance O
of O
our O
models O
on O
QM9 Material
, O
there O
are O
two O
important O
benchmark Metric
error Metric
levels Metric
. O
The O
first O
is O
the O
estimated O
average Metric
error Metric
of O
the O
DFT Method
approximation Method
to O
nature O
, O
which O
we O
refer O
to O
as O
“ O
DFT Metric
error Metric
. O
” O
The O
second O
, O
known O
as O
“ O
chemical Metric
accuracy Metric
, O
” O
is O
a O
target Metric
error Metric
that O
has O
been O
established O
by O
the O
chemistry O
community O
. O
Estimates O
of O
DFT Metric
error Metric
and O
chemical Metric
accuracy Metric
are O
provided O
for O
each O
of O
the O
13 O
targets O
in O
. O
One O
important O
goal O
of O
this O
line O
of O
research O
is O
to O
produce O
a O
model O
which O
can O
achieve O
chemical Metric
accuracy Metric
with O
respect O
to O
the O
true O
targets O
as O
measured O
by O
an O
extremely O
precise O
experiment O
. O
The O
dataset O
containing O
the O
true O
targets O
on O
all O
134k O
molecules O
does O
not O
currently O
exist O
. O
However O
, O
the O
ability O
to O
fit O
the O
DFT Method
approximation Method
to O
within O
chemical Metric
accuracy Metric
would O
be O
an O
encouraging O
step O
in O
this O
direction O
. O
For O
all O
13 O
targets O
, O
achieving O
chemical Metric
accuracy Metric
is O
at O
least O
as O
hard O
as O
achieving O
DFT Metric
error Metric
. O
In O
the O
rest O
of O
this O
paper O
when O
we O
talk O
about O
chemical Metric
accuracy Metric
we O
generally O
mean O
with O
respect O
to O
our O
available O
ground O
truth O
labels O
. O
In O
this O
paper O
, O
by O
exploring O
novel O
variations O
of O
models O
in O
the O
MPNN Method
family Method
, O
we O
are O
able O
to O
both O
achieve O
a O
new O
state O
of O
the O
art O
on O
the O
QM9 Material
dataset Material
and O
to O
predict O
the O
DFT Task
calculation Task
to O
within O
chemical Metric
accuracy Metric
on O
all O
but O
two O
targets O
. O
In O
particular O
, O
we O
provide O
the O
following O
key O
contributions O
: O
We O
develop O
an O
MPNN Method
which O
achieves O
state O
of O
the O
art O
results O
on O
all O
13 O
targets O
and O
predicts O
DFT Task
to O
within O
chemical Metric
accuracy Metric
on O
11 O
out O
of O
13 O
targets O
. O
We O
develop O
several O
different O
MPNNs Method
which O
predict O
DFT Method
to O
within O
chemical Metric
accuracy Metric
on O
5 O
out O
of O
13 O
targets O
while O
operating O
on O
the O
topology O
of O
the O
molecule O
alone O
( O
with O
no O
spatial O
information O
as O
input O
) O
. O
We O
develop O
a O
general O
method O
to O
train O
MPNNs Method
with O
larger O
node Method
representations Method
without O
a O
corresponding O
increase O
in O
computation Metric
time Metric
or O
memory Metric
, O
yielding O
a O
substantial O
savings O
over O
previous O
MPNNs Method
for O
high Method
dimensional Method
node Method
representations Method
. O
We O
believe O
our O
work O
is O
an O
important O
step O
towards O
making O
well O
- O
designed O
MPNNâs Method
the O
default O
for O
supervised Task
learning Task
on O
modestly Task
sized Task
molecules Task
. O
In O
order O
for O
this O
to O
happen O
, O
researchers O
need O
to O
perform O
careful O
empirical O
studies O
to O
find O
the O
proper O
way O
to O
use O
these O
types O
of O
models O
and O
to O
make O
any O
necessary O
improvements O
to O
them O
, O
it O
is O
not O
sufficient O
for O
these O
models O
to O
have O
been O
described O
in O
the O
literature O
if O
there O
is O
only O
limited O
accompanying O
empirical O
work O
in O
the O
chemical O
domain O
. O
Indeed O
convolutional Method
neural Method
networks Method
existed O
for O
decades O
before O
careful O
empirical O
work O
applying O
them O
to O
image Task
classification Task
helped O
them O
displace O
SVMs Method
on O
top O
of O
hand O
- O
engineered O
features O
for O
a O
host O
of O
computer Task
vision Task
problems Task
. O
section O
: O
Message Method
Passing Method
Neural Method
Networks Method
There O
are O
at O
least O
eight O
notable O
examples O
of O
models O
from O
the O
literature O
that O
we O
can O
describe O
using O
our O
Message Method
Passing Method
Neural Method
Networks Method
( O
MPNN Method
) Method
framework Method
. O
For O
simplicity O
we O
describe O
MPNNs Method
which O
operate O
on O
undirected O
graphs O
with O
node O
features O
and O
edge O
features O
. O
It O
is O
trivial O
to O
extend O
the O
formalism O
to O
directed Task
multigraphs Task
. O
The O
forward Method
pass Method
has O
two O
phases O
, O
a O
message Method
passing Method
phase Method
and O
a O
readout Method
phase Method
. O
The O
message Method
passing Method
phase Method
runs O
for O
time O
steps O
and O
is O
defined O
in O
terms O
of O
message O
functions O
and O
vertex Method
update Method
functions Method
. O
During O
the O
message Task
passing Task
phase Task
, O
hidden O
states O
at O
each O
node O
in O
the O
graph O
are O
updated O
based O
on O
messages O
according O
to O
where O
in O
the O
sum O
, O
denotes O
the O
neighbors O
of O
in O
graph O
. O
The O
readout Method
phase Method
computes O
a O
feature O
vector O
for O
the O
whole O
graph O
using O
some O
readout O
function O
according O
to O
The O
message O
functions O
, O
vertex O
update O
functions O
, O
and O
readout O
function O
are O
all O
learned O
differentiable O
functions O
. O
operates O
on O
the O
set O
of O
node O
states O
and O
must O
be O
invariant O
to O
permutations O
of O
the O
node O
states O
in O
order O
for O
the O
MPNN Method
to O
be O
invariant O
to O
graph Method
isomorphism Method
. O
In O
what O
follows O
, O
we O
define O
previous O
models O
in O
the O
literature O
by O
specifying O
the O
message O
function O
, O
vertex O
update O
function O
, O
and O
readout O
function O
used O
. O
Note O
one O
could O
also O
learn O
edge O
features O
in O
an O
MPNN Method
by O
introducing O
hidden O
states O
for O
all O
edges O
in O
the O
graph O
and O
updating O
them O
analogously O
to O
equations O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Of O
the O
existing O
MPNNs Method
, O
only O
has O
used O
this O
idea O
. O
Convolutional Method
Networks Method
for O
Learning Task
Molecular Task
Fingerprints Task
, O
( O
) O
The O
message O
function O
used O
is O
where O
denotes O
concatenation O
. O
The O
vertex O
update O
function O
used O
is O
, O
where O
is O
the O
sigmoid O
function O
, O
deg O
is O
the O
degree O
of O
vertex O
and O
is O
a O
learned O
matrix O
for O
each O
time O
step O
and O
vertex O
degree O
. O
has O
skip O
connections O
to O
all O
previous O
hidden O
states O
and O
is O
equal O
to O
, O
where O
is O
a O
neural Method
network Method
and O
are O
learned O
readout O
matrices O
, O
one O
for O
each O
time O
step O
. O
This O
message Method
passing Method
scheme Method
may O
be O
problematic O
since O
the O
resulting O
message O
vector O
is O
which O
separately O
sums O
over O
connected O
nodes O
and O
connected O
edges O
. O
It O
follows O
that O
the O
message Method
passing Method
implemented O
in O
is O
unable O
to O
identify O
correlations O
between O
edge O
states O
and O
node O
states O
. O
Gated Method
Graph Method
Neural Method
Networks Method
( O
GG Method
- Method
NN Method
) O
, O
( O
) O
The O
message O
function O
used O
is O
, O
where O
is O
a O
learned O
matrix O
, O
one O
for O
each O
edge O
label O
( O
the O
model O
assumes O
discrete O
edge O
types O
) O
. O
The O
update O
function O
is O
, O
where O
GRU Method
is O
the O
Gated Method
Recurrent Method
Unit Method
introduced O
in O
. O
This O
work O
used O
weight Method
tying Method
, O
so O
the O
same O
update O
function O
is O
used O
at O
each O
time O
step O
. O
Finally O
, O
where O
and O
are O
neural Method
networks Method
, O
and O
denotes O
element Method
- Method
wise Method
multiplication Method
. O
Interaction O
Networks O
, O
( O
) O
This O
work O
considered O
both O
the O
case O
where O
there O
is O
a O
target O
at O
each O
node O
in O
the O
graph O
, O
and O
where O
there O
is O
a O
graph O
level O
target O
. O
It O
also O
considered O
the O
case O
where O
there O
are O
node O
level O
effects O
applied O
at O
each O
time O
step O
, O
in O
such O
a O
case O
the O
update Method
function Method
takes O
as O
input O
the O
concatenation O
where O
is O
an O
external O
vector O
representing O
some O
outside O
influence O
on O
the O
vertex O
. O
The O
message Method
function Method
is O
a O
neural Method
network Method
which O
takes O
the O
concatenation O
. O
The O
vertex Method
update Method
function Method
is O
a O
neural Method
network Method
which O
takes O
as O
input O
the O
concatenation O
. O
Finally O
, O
in O
the O
case O
where O
there O
is O
a O
graph O
level O
output O
, O
where O
is O
a O
neural Method
network Method
which O
takes O
the O
sum O
of O
the O
final O
hidden O
states O
. O
Note O
the O
original O
work O
only O
defined O
the O
model O
for O
. O
Molecular Method
Graph Method
Convolutions Method
, O
( O
) O
This O
work O
deviates O
slightly O
from O
other O
MPNNs Method
in O
that O
it O
introduces O
edge Method
representations Method
which O
are O
updated O
during O
the O
message Method
passing Method
phase Method
. O
The O
message O
function O
used O
for O
node O
messages O
is O
. O
The O
vertex O
update O
function O
is O
where O
denotes O
concatenation O
, O
is O
the O
ReLU O
activation O
and O
are O
learned O
weight O
matrices O
. O
The O
edge O
state O
update O
is O
defined O
by O
where O
the O
are O
also O
learned O
weight O
matrices O
. O
Deep Method
Tensor Method
Neural Method
Networks Method
, O
( O
) O
The O
message O
from O
to O
is O
computed O
by O
where O
, O
, O
are O
matrices O
and O
, O
are O
bias O
vectors O
. O
The O
update O
function O
used O
is O
. O
The O
readout Method
function Method
passes O
each O
node O
independently O
through O
a O
single O
hidden Method
layer Method
neural Method
network Method
and O
sums O
the O
outputs O
, O
in O
particular O
Laplacian Method
Based Method
Methods Method
, O
( O
) O
These O
methods O
generalize O
the O
notion O
of O
the O
convolution O
operation O
typically O
applied O
to O
image O
datasets O
to O
an O
operation O
that O
operates O
on O
an O
arbitrary O
graph O
with O
a O
real O
valued O
adjacency O
matrix O
. O
The O
operations O
defined O
in O
result O
in O
message O
functions O
of O
the O
form O
, O
where O
the O
matrices O
are O
parameterized O
by O
the O
eigenvectors O
of O
the O
graph O
laplacian O
, O
and O
the O
learned O
parameters O
of O
the O
model O
. O
The O
vertex O
update O
function O
used O
is O
where O
is O
some O
pointwise O
non O
- O
linearity O
( O
such O
as O
ReLU Method
) O
. O
The O
model O
results O
in O
a O
message O
function O
where O
. O
The O
vertex O
update O
function O
is O
. O
For O
the O
exact O
expressions O
for O
the O
and O
the O
derivation O
of O
the O
reformulation O
of O
these O
models O
as O
MPNNs Method
, O
see O
the O
supplementary O
material O
. O
subsection O
: O
Moving O
Forward O
Given O
how O
many O
instances O
of O
MPNNâs Method
have O
appeared O
in O
the O
literature O
, O
we O
should O
focus O
on O
pushing O
this O
general O
family O
as O
far O
as O
possible O
in O
a O
specific O
application O
of O
substantial O
practical O
importance O
. O
This O
way O
we O
can O
determine O
the O
most O
crucial O
implementation O
details O
and O
potentially O
reach O
the O
limits O
of O
these O
models O
to O
guide O
us O
towards O
future O
modeling O
improvements O
. O
One O
downside O
of O
all O
of O
these O
approaches O
is O
computation Metric
time Metric
. O
Recent O
work O
has O
adapted O
the O
GG Method
- Method
NN Method
architecture Method
to O
larger O
graphs O
by O
passing O
messages O
on O
only O
subsets O
of O
the O
graph O
at O
each O
time O
step O
. O
In O
this O
work O
we O
also O
present O
a O
MPNN Method
modification Method
that O
can O
improve O
the O
computational Metric
costs Metric
. O
section O
: O
Related O
Work O
Although O
in O
principle O
quantum O
mechanics O
lets O
us O
compute O
the O
properties O
of O
molecules O
, O
the O
laws O
of O
physics O
lead O
to O
equations O
that O
are O
far O
too O
difficult O
to O
solve O
exactly O
. O
Therefore O
scientists O
have O
developed O
a O
hierarchy O
of O
approximations O
to O
quantum Task
mechanics Task
with O
varying O
tradeoffs O
of O
speed Metric
and O
accuracy Metric
, O
such O
as O
Density Method
Functional Method
Theory Method
( O
DFT Method
) O
with O
a O
variety O
of O
functionals O
, O
the O
GW Method
approximation Method
, O
and O
Quantum Method
Monte Method
- Method
Carlo Method
. O
Despite O
being O
widely O
used O
, O
DFT Method
is O
simultaneously O
still O
too O
slow O
to O
be O
applied O
to O
large O
systems O
( O
scaling O
as O
where O
is O
the O
number O
of O
electrons O
) O
and O
exhibits O
systematic O
as O
well O
as O
random O
errors O
relative O
to O
exact Method
solutions Method
to O
Schrödinger Task
’s Task
equation Task
. O
For O
example O
, O
to O
run O
the O
DFT Task
calculation Task
on O
a O
single O
9 O
heavy O
atom O
molecule O
in O
QM9 Material
takes O
around O
an O
hour O
on O
a O
single O
core O
of O
a O
Xeon O
E5 O
- O
2660 O
( O
2.2 O
GHz O
) O
using O
a O
version O
of O
Gaussian Method
G09 Method
( O
ES64L Method
- Method
G09RevD.01 Method
) O
. O
For O
a O
17 O
heavy O
atom O
molecule O
, O
computation Metric
time Metric
is O
up O
to O
8 O
hours O
. O
Empirical Method
potentials Method
have O
been O
developed O
, O
such O
as O
the O
Stillinger Method
- Method
Weber Method
potential Method
, O
that O
are O
fast O
and O
accurate O
but O
must O
be O
created O
from O
scratch O
, O
from O
first O
principles O
, O
for O
every O
new O
composition O
of O
atoms O
. O
used O
neural Method
networks Method
to O
approximate O
a O
particularly O
troublesome O
term O
in O
DFT Task
called O
the O
exchange O
correlation O
potential O
to O
improve O
the O
accuracy Metric
of O
DFT Task
. O
However O
, O
their O
method O
fails O
to O
improve O
upon O
the O
efficiency O
of O
DFT Method
and O
relies O
on O
a O
large O
set O
of O
ad Method
hoc Method
atomic Method
descriptors Method
. O
Two O
more O
recent O
approaches O
by O
and O
attempt O
to O
approximate O
solutions O
to O
quantum Task
mechanics Task
directly O
without O
appealing O
to O
DFT Method
. O
In O
the O
first O
case O
single Method
- Method
hidden Method
- Method
layer Method
neural Method
networks Method
were O
used O
to O
approximate O
the O
energy O
and O
forces O
for O
configurations O
of O
a O
Silicon O
melt O
with O
the O
goal O
of O
speeding O
up O
molecular Task
dynamics Task
simulations Task
. O
The O
second O
paper O
used O
Kernel Method
Ridge Method
Regression Method
( O
KRR Method
) O
to O
infer O
atomization O
energies O
over O
a O
wide O
range O
of O
molecules O
. O
In O
both O
cases O
hand O
engineered O
features O
were O
used O
( O
symmetry O
functions O
and O
the O
Coulomb O
matrix O
, O
respectively O
) O
that O
built O
physical O
symmetries O
into O
the O
input O
representation O
. O
Subsequent O
papers O
have O
replaced O
KRR Method
by O
a O
neural Method
network Method
. O
Both O
of O
these O
lines O
of O
research O
used O
hand O
engineered O
features O
that O
have O
intrinsic O
limitations O
. O
The O
work O
of O
used O
a O
representation O
that O
was O
manifestly O
invariant O
to O
graph Method
isomorphism Method
, O
but O
has O
difficulty O
when O
applied O
to O
systems O
with O
more O
than O
three O
species O
of O
atoms O
and O
fails O
to O
generalize O
to O
novel O
compositions O
. O
The O
representation O
used O
in O
is O
not O
invariant O
to O
graph Method
isomorphism Method
. O
Instead O
, O
this O
invariance O
must O
be O
learned O
by O
the O
downstream Method
model Method
through O
dataset Task
augmentation Task
. O
In O
addition O
to O
the O
eight O
MPNNâs Method
discussed O
in O
Section O
[ O
reference O
] O
there O
have O
been O
a O
number O
of O
other O
approaches O
to O
machine Task
learning Task
on O
graphical O
data O
which O
take O
advantage O
of O
the O
symmetries O
in O
a O
number O
of O
ways O
. O
One O
such O
family O
of O
approaches O
define O
a O
preprocessing Method
step Method
which O
constructs O
a O
canonical Method
graph Method
representation Method
which O
can O
then O
be O
fed O
into O
into O
a O
standard O
classifier Method
. O
Examples O
in O
this O
family O
include O
and O
. O
Finally O
define O
a O
message Method
passing Method
process Method
on O
graphs Method
which O
is O
run O
until O
convergence O
, O
instead O
of O
for O
a O
finite O
number O
of O
time O
steps O
as O
in O
MPNNs Method
. O
section O
: O
QM9 Material
Dataset Material
To O
investigate O
the O
success O
of O
MPNNs Method
on O
predicting Task
chemical Task
properties Task
, O
we O
use O
the O
publicly O
available O
QM9 Material
dataset Material
. O
Molecules O
in O
the O
dataset O
consist O
of O
Hydrogen O
( O
H O
) O
, O
Carbon O
( O
C O
) O
, O
Oxygen O
( O
O O
) O
, O
Nitrogen O
( O
N O
) O
, O
and O
Flourine O
( O
F O
) O
atoms O
and O
contain O
up O
to O
9 O
heavy O
( O
non O
Hydrogen O
) O
atoms O
. O
In O
all O
, O
this O
results O
in O
about O
134k O
drug O
- O
like O
organic O
molecules O
that O
span O
a O
wide O
range O
of O
chemistry O
. O
For O
each O
molecule O
DFT Method
is O
used O
to O
find O
a O
reasonable O
low O
energy O
structure O
and O
hence O
atom O
“ O
positions O
” O
are O
available O
. O
Additionally O
a O
wide O
range O
of O
interesting O
and O
fundamental O
chemical O
properties O
are O
computed O
. O
Given O
how O
fundamental O
some O
of O
the O
QM9 Material
properties O
are O
, O
it O
is O
hard O
to O
believe O
success O
on O
more O
challenging O
chemical Task
tasks Task
will O
be O
possible O
if O
we O
ca O
n’t O
make O
accurate O
statistical O
predictions O
for O
the O
properties O
computed O
in O
QM9 Material
. O
We O
can O
group O
the O
different O
properties O
we O
try O
to O
predict O
into O
four O
broad O
categories O
. O
First O
, O
we O
have O
four O
properties O
related O
to O
how O
tightly O
bound O
together O
the O
atoms O
in O
a O
molecule O
are O
. O
These O
measure O
the O
energy O
required O
to O
break O
up O
the O
molecule O
at O
different O
temperatures O
and O
pressures O
. O
These O
include O
the O
atomization O
energy O
at O
, O
( O
eV O
) O
, O
atomization O
energy O
at O
room O
temperature O
, O
( O
eV O
) O
, O
enthalpy O
of O
atomization O
at O
room O
temperature O
, O
( O
eV O
) O
, O
and O
free O
energy O
of O
atomization O
, O
( O
eV O
) O
. O
Next O
there O
are O
properties O
related O
to O
fundamental O
vibrations O
of O
the O
molecule O
, O
including O
the O
highest O
fundamental O
vibrational O
frequency O
( O
) O
and O
the O
zero O
point O
vibrational O
energy O
( O
ZPVE O
) O
( O
eV O
) O
. O
Additionally O
, O
there O
are O
a O
number O
of O
properties O
that O
concern O
the O
states O
of O
the O
electrons O
in O
the O
molecule O
. O
They O
include O
the O
energy O
of O
the O
electron O
in O
the O
highest O
occupied O
molecular O
orbital O
( O
HOMO O
) O
( O
eV O
) O
, O
the O
energy O
of O
the O
lowest O
unoccupied O
molecular O
orbital O
( O
LUMO O
) O
( O
eV O
) O
, O
and O
the O
electron O
energy O
gap O
( O
( O
eV O
) O
) O
. O
The O
electron O
energy O
gap O
is O
simply O
the O
difference O
. O
Finally O
, O
there O
are O
several O
measures O
of O
the O
spatial O
distribution O
of O
electrons O
in O
the O
molecule O
. O
These O
include O
the O
electronic O
spatial O
extent O
( O
Bohr O
) O
, O
the O
norm O
of O
the O
dipole O
moment O
( O
Debye O
) O
, O
and O
the O
norm O
of O
static O
polarizability O
( O
Bohr O
) O
. O
For O
a O
more O
detailed O
description O
of O
these O
properties O
, O
see O
the O
supplementary O
material O
. O
section O
: O
MPNN Method
Variants Method
We O
began O
our O
exploration O
of O
MPNNs Method
around O
the O
GG Method
- Method
NN Method
model Method
which O
we O
believe O
to O
be O
a O
strong O
baseline O
. O
We O
focused O
on O
trying O
different O
message O
functions O
, O
output O
functions O
, O
finding O
the O
appropriate O
input O
representation O
, O
and O
properly O
tuning O
hyperparameters O
. O
For O
the O
rest O
of O
the O
paper O
we O
use O
to O
denote O
the O
dimension O
of O
the O
internal Method
hidden Method
representation Method
of O
each O
node O
in O
the O
graph O
, O
and O
to O
denote O
the O
number O
of O
nodes O
in O
the O
graph O
. O
Our O
implementation O
of O
MPNNs Method
in O
general O
operates O
on O
directed O
graphs O
with O
a O
separate O
message O
channel O
for O
incoming O
and O
outgoing O
edges O
, O
in O
which O
case O
the O
incoming O
message O
is O
the O
concatenation O
of O
and O
, O
this O
was O
also O
used O
in O
. O
When O
we O
apply O
this O
to O
undirected Task
chemical Task
graphs Task
we O
treat O
the O
graph O
as O
directed O
, O
where O
each O
original O
edge O
becomes O
both O
an O
incoming O
and O
outgoing O
edge O
with O
the O
same O
label O
. O
Note O
there O
is O
nothing O
special O
about O
the O
direction O
of O
the O
edge O
, O
it O
is O
only O
relevant O
for O
parameter Task
tying Task
. O
Treating O
undirected O
graphs O
as O
directed O
means O
that O
the O
size O
of O
the O
message O
channel O
is O
instead O
of O
. O
The O
input O
to O
our O
MPNN Method
model Method
is O
a O
set O
of O
feature O
vectors O
for O
the O
nodes O
of O
the O
graph O
, O
, O
and O
an O
adjacency O
matrix O
with O
vector O
valued O
entries O
to O
indicate O
different O
bonds O
in O
the O
molecule O
as O
well O
as O
pairwise O
spatial O
distance O
between O
two O
atoms O
. O
We O
experimented O
as O
well O
with O
the O
message Method
function Method
used O
in O
the O
GG Method
- Method
NN Method
family Method
, O
which O
assumes O
discrete O
edge O
labels O
, O
in O
which O
case O
the O
matrix O
has O
entries O
in O
a O
discrete O
alphabet O
of O
size O
. O
The O
initial O
hidden O
states O
are O
set O
to O
be O
the O
atom O
input O
feature O
vectors O
and O
are O
padded O
up O
to O
some O
larger O
dimension O
. O
All O
of O
our O
experiments O
used O
weight Method
tying Method
at O
each O
time O
step O
, O
and O
a O
GRU Method
for O
the O
update Method
function Method
as O
in O
the O
GG Method
- Method
NN Method
family Method
. O
subsection O
: O
Message Method
Functions Method
Matrix Method
Multiplication Method
: O
We O
started O
with O
the O
message Method
function Method
used O
in O
GG Method
- Method
NN Method
which O
is O
defined O
by O
the O
equation O
. O
Edge Method
Network Method
: O
To O
allow O
vector O
valued O
edge O
features O
we O
propose O
the O
message Method
function Method
where O
is O
a O
neural Method
network Method
which O
maps O
the O
edge O
vector O
to O
a O
matrix O
. O
Pair O
Message O
: O
One O
property O
that O
the O
matrix Method
multiplication Method
rule Method
has O
is O
that O
the O
message O
from O
node O
to O
node O
is O
a O
function O
only O
of O
the O
hidden O
state O
and O
the O
edge O
. O
In O
particular O
, O
it O
does O
not O
depend O
on O
the O
hidden O
state O
. O
In O
theory O
, O
a O
network O
may O
be O
able O
to O
use O
the O
message O
channel O
more O
efficiently O
if O
the O
node O
messages O
are O
allowed O
to O
depend O
on O
both O
the O
source O
and O
destination O
node O
. O
Thus O
we O
also O
tried O
using O
a O
variant O
on O
the O
message O
function O
as O
described O
in O
. O
Here O
the O
message O
from O
to O
along O
edge O
is O
where O
is O
a O
neural Method
network Method
. O
When O
we O
apply O
the O
above O
message Method
functions Method
to O
directed O
graphs O
, O
there O
are O
two O
separate O
functions O
used O
, O
and O
an O
. O
Which O
function O
is O
applied O
to O
a O
particular O
edge O
depends O
on O
the O
direction O
of O
that O
edge O
. O
subsection O
: O
Virtual O
Graph O
Elements O
We O
explored O
two O
different O
ways O
to O
change O
how O
the O
messages O
are O
passed O
throughout O
the O
model O
. O
The O
simplest O
modification O
involves O
adding O
a O
separate O
“ O
virtual O
” O
edge O
type O
for O
pairs O
of O
nodes O
that O
are O
not O
connected O
. O
This O
can O
be O
implemented O
as O
a O
data Task
preprocessing Task
step Task
and O
allows O
information O
to O
travel O
long O
distances O
during O
the O
propagation O
phase O
. O
We O
also O
experimented O
with O
using O
a O
latent O
“ O
master O
” O
node O
, O
which O
is O
connected O
to O
every O
input O
node O
in O
the O
graph O
with O
a O
special O
edge O
type O
. O
The O
master O
node O
serves O
as O
a O
global O
scratch O
space O
that O
each O
node O
both O
reads O
from O
and O
writes O
to O
in O
every O
step O
of O
message Task
passing Task
. O
We O
allow O
the O
master O
node O
to O
have O
a O
separate O
node O
dimension O
, O
as O
well O
as O
separate O
weights O
for O
the O
internal O
update O
function O
( O
in O
our O
case O
a O
GRU Method
) O
. O
This O
allows O
information O
to O
travel O
long O
distances O
during O
the O
propagation O
phase O
. O
It O
also O
, O
in O
theory O
, O
allows O
additional O
model O
capacity O
( O
e.g. O
large O
values O
of O
) O
without O
a O
substantial O
hit O
in O
performance O
, O
as O
the O
complexity Metric
of O
the O
master Method
node Method
model Method
is O
. O
subsection O
: O
Readout Method
Functions Method
We O
experimented O
with O
two O
readout Method
functions Method
. O
First O
is O
the O
readout Method
function Method
used O
in O
GG Method
- Method
NN Method
, O
which O
is O
defined O
by O
equation O
[ O
reference O
] O
. O
Second O
is O
a O
set2set Method
model Method
from O
. O
The O
set2set Method
model Method
is O
specifically O
designed O
to O
operate O
on O
sets O
and O
should O
have O
more O
expressive O
power O
than O
simply O
summing O
the O
final O
node O
states O
. O
This O
model O
first O
applies O
a O
linear Method
projection Method
to O
each O
tuple O
( O
) O
and O
then O
takes O
as O
input O
the O
set O
of O
projected O
tuples O
. O
Then O
, O
after O
steps O
of O
computation O
, O
the O
set2set Method
model Method
produces O
a O
graph Method
level Method
embedding Method
which O
is O
invariant O
to O
the O
order O
of O
the O
of O
the O
tuples O
. O
We O
feed O
this O
embedding O
through O
a O
neural Method
network Method
to O
produce O
the O
output O
. O
subsection O
: O
Multiple O
Towers O
One O
issue O
with O
MPNNâs Method
is O
scalability O
. O
In O
particular O
, O
a O
single O
step O
of O
the O
message Method
passing Method
phase Method
for O
a O
dense Task
graph Task
requires O
floating Task
point Task
multiplications Task
. O
As O
or O
get O
large O
this O
can O
be O
computationally O
expensive O
. O
To O
address O
this O
issue O
we O
break O
the O
dimensional O
node O
embeddings O
into O
different O
dimensional O
embeddings O
and O
run O
a O
propagation Method
step Method
on O
each O
of O
the O
copies O
separately O
to O
get O
temporary O
embeddings O
, O
using O
separate O
message Method
and Method
update Method
functions Method
for O
each O
copy O
. O
The O
temporary O
embeddings O
of O
each O
node O
are O
then O
mixed O
together O
according O
to O
the O
equation O
where O
denotes O
a O
neural Method
network Method
and O
denotes O
concatenation Method
, O
with O
shared O
across O
all O
nodes O
in O
the O
graph O
. O
This O
mixing O
preserves O
the O
invariance O
to O
permutations O
of O
the O
nodes O
, O
while O
allowing O
the O
different O
copies O
of O
the O
graph O
to O
communicate O
with O
each O
other O
during O
the O
propagation O
phase O
. O
This O
can O
be O
advantageous O
in O
that O
it O
allows O
larger O
hidden O
states O
for O
the O
same O
number O
of O
parameters O
, O
which O
yields O
a O
computational O
speedup O
in O
practice O
. O
For O
example O
, O
when O
the O
message O
function O
is O
matrix Method
multiplication Method
( O
as O
in O
GG Method
- Method
NN Method
) O
a O
propagation Method
step Method
of O
a O
single O
copy O
takes O
time O
, O
and O
there O
are O
copies O
, O
therefore O
the O
overall O
time Metric
complexity Metric
is O
, O
with O
some O
additional O
overhead O
due O
to O
the O
mixing Method
network Method
. O
For O
, O
and O
we O
see O
a O
factor O
of O
2 O
speedup O
in O
inference Metric
time Metric
over O
a O
, O
, O
and O
architecture O
. O
This O
variation O
would O
be O
most O
useful O
for O
larger O
molecules O
, O
for O
instance O
molecules O
from O
GDB O
- O
17 O
. O
section O
: O
Input Method
Representation Method
There O
are O
a O
number O
of O
features O
available O
for O
each O
atom O
in O
a O
molecule O
which O
capture O
both O
properties O
of O
the O
electrons O
in O
the O
atom O
as O
well O
as O
the O
bonds O
that O
the O
atom O
participates O
in O
. O
For O
a O
list O
of O
all O
of O
the O
features O
see O
table O
[ O
reference O
] O
. O
We O
experimented O
with O
making O
the O
hydrogen O
atoms O
explicit O
nodes O
in O
the O
graph O
( O
as O
opposed O
to O
simply O
including O
the O
count O
as O
a O
node O
feature O
) O
, O
in O
which O
case O
graphs O
have O
up O
to O
29 O
nodes O
. O
Note O
that O
having O
larger O
graphs O
significantly O
slows O
training Metric
time Metric
, O
in O
this O
case O
by O
a O
factor O
of O
roughly O
10 O
. O
For O
the O
adjacency O
matrix O
there O
are O
three O
edge Method
representations Method
used O
depending O
on O
the O
model O
. O
Chemical Method
Graph Method
: O
In O
the O
abscence O
of O
distance O
information O
, O
adjacency O
matrix O
entries O
are O
discrete O
bond O
types O
: O
single O
, O
double O
, O
triple O
, O
or O
aromatic O
. O
Distance O
bins O
: O
The O
matrix Method
multiply Method
message Method
function Method
assumes O
discrete O
edge O
types O
, O
so O
to O
include O
distance O
information O
we O
bin O
bond O
distances O
into O
10 O
bins O
, O
the O
bins O
are O
obtained O
by O
uniformly O
partitioning O
the O
interval O
into O
8 O
bins O
, O
followed O
by O
adding O
a O
bin O
and O
. O
These O
bins O
were O
hand O
chosen O
by O
looking O
at O
a O
histogram O
of O
all O
distances O
. O
The O
adjacency O
matrix O
then O
has O
entries O
in O
an O
alphabet O
of O
size O
14 O
, O
indicating O
bond O
type O
for O
bonded O
atoms O
and O
distance O
bin O
for O
atoms O
that O
are O
not O
bonded O
. O
We O
found O
the O
distance O
for O
bonded O
atoms O
to O
be O
almost O
completely O
determined O
by O
bond O
type O
. O
Raw O
distance O
feature O
: O
When O
using O
a O
message Method
function Method
which O
operates O
on O
vector O
valued O
edges O
, O
the O
entries O
of O
the O
adjacency O
matrix O
are O
then O
5 O
dimensional O
, O
where O
the O
first O
dimension O
indicates O
the O
euclidean O
distance O
between O
the O
pair O
of O
atoms O
, O
and O
the O
remaining O
four O
are O
a O
one O
- O
hot O
encoding O
of O
the O
bond O
type O
. O
section O
: O
Training O
Each O
model O
and O
target O
combination O
was O
trained O
using O
a O
uniform Method
random Method
hyper Method
parameter Method
search Method
with O
50 O
trials O
. O
was O
constrained O
to O
be O
in O
the O
range O
( O
in O
practice O
, O
any O
works O
) O
. O
The O
number O
of O
set2set O
computations O
was O
chosen O
from O
the O
range O
. O
All O
models O
were O
trained O
using O
SGD Method
with O
the O
ADAM Method
optimizer Method
( O
) O
, O
with O
batch O
size O
20 O
for O
3 O
million O
steps O
( O
540 O
epochs O
) O
. O
The O
initial O
learning Metric
rate Metric
was O
chosen O
uniformly O
between O
and O
. O
We O
used O
a O
linear Method
learning Method
rate Method
decay Method
that O
began O
between O
10 O
% O
and O
90 O
% O
of O
the O
way O
through O
training O
and O
the O
initial O
learning Metric
rate Metric
decayed O
to O
a O
final O
learning Metric
rate Metric
, O
using O
a O
decay O
factor O
in O
the O
range O
. O
The O
QM Material
- Material
9 Material
dataset Material
has O
130462 O
molecules O
in O
it O
. O
We O
randomly O
chose O
10000 O
samples O
for O
validation O
, O
10000 O
samples O
for O
testing O
, O
and O
used O
the O
rest O
for O
training O
. O
We O
use O
the O
validation O
set O
to O
do O
early Method
stopping Method
and O
model Method
selection Method
and O
we O
report O
scores O
on O
the O
test O
set O
. O
All O
targets O
were O
normalized O
to O
have O
mean O
0 O
and O
variance O
1 O
. O
We O
minimize O
the O
mean Metric
squared Metric
error Metric
between O
the O
model O
output O
and O
the O
target O
, O
although O
we O
evaluate O
mean Metric
absolute Metric
error Metric
. O
section O
: O
Results O
In O
all O
of O
our O
tables O
we O
report O
the O
ratio O
of O
the O
mean Metric
absolute Metric
error Metric
( O
MAE Metric
) O
of O
our O
models O
with O
the O
provided O
estimate O
of O
chemical Metric
accuracy Metric
for O
that O
target O
. O
Thus O
any O
model O
with O
error Metric
ratio Metric
less O
than O
1 O
has O
achieved O
chemical Metric
accuracy Metric
for O
that O
target O
. O
In O
the O
supplementary O
material O
we O
list O
the O
chemical Metric
accuracy Metric
estimates Metric
for O
each O
target O
, O
these O
are O
the O
same O
estimates O
that O
were O
given O
in O
. O
In O
this O
way O
, O
the O
MAE Metric
of O
our O
models O
can O
be O
calculated O
as O
. O
Note O
, O
unless O
otherwise O
indicated O
, O
all O
tables O
display O
result O
of O
models O
trained O
individually O
on O
each O
target O
( O
as O
opposed O
to O
training O
one O
model O
to O
predict O
all O
13 O
) O
. O
We O
performed O
numerous O
experiments O
in O
order O
to O
find O
the O
best O
possible O
MPNN Method
on O
this O
dataset O
as O
well O
as O
the O
proper O
input O
representation O
. O
In O
our O
experiments O
, O
we O
found O
that O
including O
the O
complete O
edge O
feature O
vector O
( O
bond O
type O
, O
spatial O
distance O
) O
and O
treating O
hydrogen O
atoms O
as O
explicit O
nodes O
in O
the O
graph O
to O
be O
very O
important O
for O
a O
number O
of O
targets O
. O
We O
also O
found O
that O
training O
one O
model O
per O
target O
consistently O
outperformed O
jointly O
training O
on O
all O
13 O
targets O
. O
In O
some O
cases O
the O
improvement O
was O
up O
to O
40 O
% O
. O
Our O
best O
MPNN Method
variant Method
used O
the O
edge O
network O
message O
function O
, O
set2set O
output O
, O
and O
operated O
on O
graphs O
with O
explicit O
hydrogens O
. O
We O
were O
able O
to O
further O
improve O
performance O
on O
the O
test O
set O
by O
ensembling O
the O
predictions O
of O
the O
five O
models O
with O
lowest O
validation Metric
error Metric
. O
In O
table O
[ O
reference O
] O
we O
compare O
the O
performance O
of O
our O
best O
MPNN Method
variant Method
( O
denoted O
with O
enn Method
- Method
s2s Method
) O
and O
the O
corresponding O
ensemble Method
( O
denoted O
with O
enn Method
- Method
s2s Method
- Method
ens5 Method
) O
with O
the O
previous O
state O
of O
the O
art O
on O
this O
dataset O
as O
reported O
in O
. O
For O
clarity O
the O
error Metric
ratios Metric
of O
the O
best O
non Method
- Method
ensemble Method
models Method
are O
shown O
in O
bold O
. O
This O
previous O
work O
performed O
a O
comparison O
study O
of O
several O
existing O
ML Method
models Method
for O
QM9 Material
and O
we O
have O
taken O
care O
to O
use O
the O
same O
train O
, O
validation O
, O
and O
test O
split O
. O
These O
baselines O
include O
5 O
different O
hand Method
engineered Method
molecular Method
representations Method
, O
which O
then O
get O
fed O
through O
a O
standard O
, O
off Method
- Method
the Method
- Method
shelf Method
classifier Method
. O
These O
input O
representations O
include O
the O
Coulomb O
Matrix O
( O
CM Method
, O
) O
, O
Bag O
of O
Bonds O
( O
BoB Method
, O
) O
, O
Bonds O
Angles O
, O
Machine Method
Learning Method
( O
BAML Method
, O
) O
, O
Extended O
Connectivity O
Fingerprints O
( O
ECPF4 Method
, O
) O
, O
and O
“ O
Projected Method
Histograms Method
” O
( O
HDAD Method
, O
) O
representations O
. O
In O
addition O
to O
these O
hand O
engineered O
features O
we O
include O
two O
existing O
baseline O
MPNNs Method
, O
the O
Molecular Method
Graph Method
Convolutions Method
model Method
( O
GC Method
) Method
from O
, O
and O
the O
original O
GG Method
- Method
NN Method
model Method
trained O
with O
distance O
bins O
. O
Overall O
, O
our O
new O
MPNN Method
achieves O
chemical Metric
accuracy Metric
on O
11 O
out O
of O
13 O
targets O
and O
state O
of O
the O
art O
on O
all O
13 O
targets O
. O
Training O
Without O
Spatial O
Information O
: O
We O
also O
experimented O
in O
the O
setting O
where O
spatial O
information O
is O
not O
included O
in O
the O
input O
. O
In O
general O
, O
we O
find O
that O
augmenting O
the O
MPNN Method
with O
some O
means O
of O
capturing O
long O
range O
interactions O
between O
nodes O
in O
the O
graph O
greatly O
improves O
performance O
in O
this O
setting O
. O
To O
demonstrate O
this O
we O
performed O
4 O
experiments O
, O
one O
where O
we O
train O
the O
GG Method
- Method
NN Method
model Method
on O
the O
sparse O
graph O
, O
one O
where O
we O
add O
virtual O
edges O
, O
one O
where O
we O
add O
a O
master O
node O
, O
and O
one O
where O
we O
change O
the O
graph O
level O
output O
to O
a O
set2set O
output O
. O
The O
error Metric
ratios Metric
averaged O
across O
the O
13 O
targets O
are O
shown O
in O
table O
[ O
reference O
] O
. O
Overall O
, O
these O
three O
modifications O
help O
on O
all O
13 O
targets O
, O
and O
the O
Set2Set O
output O
achieves O
chemical O
accuracy Metric
on O
5 O
out O
of O
13 O
targets O
. O
For O
more O
details O
, O
consult O
the O
supplementary O
material O
. O
The O
experiments O
shown O
tables O
[ O
reference O
] O
and O
[ O
reference O
] O
were O
run O
with O
a O
partial O
charge O
feature O
as O
a O
node O
input O
. O
This O
feature O
is O
an O
output O
of O
the O
DFT Method
calculation Method
and O
thus O
could O
not O
be O
used O
in O
an O
applied O
setting O
. O
The O
state O
of O
art O
numbers O
we O
report O
in O
table O
[ O
reference O
] O
do O
not O
use O
this O
feature O
. O
Towers O
: O
Our O
original O
intent O
in O
developing O
the O
towers Method
variant Method
was O
to O
improve O
training Metric
time Metric
, O
as O
well O
as O
to O
allow O
the O
model O
to O
be O
trained O
on O
larger O
graphs O
. O
However O
, O
we O
also O
found O
some O
evidence O
that O
the O
multi O
- O
tower O
structure O
improves O
generalization Task
performance O
. O
In O
table O
[ O
reference O
] O
we O
compare O
GG Method
- Method
NN Method
+ Method
towers Method
+ Method
set2set Method
output Method
vs O
a O
baseline Method
GG Method
- Method
NN Method
+ O
set2set O
output O
when O
distance O
bins O
are O
used O
. O
We O
do O
this O
comparison O
in O
both O
the O
joint Task
training Task
regime Task
and O
when O
training O
one O
model O
per O
target O
. O
The O
towers Method
model Method
outperforms O
the O
baseline O
model O
on O
12 O
out O
of O
13 O
targets O
in O
both O
individual Task
and Task
joint Task
target Task
training Task
. O
We O
believe O
the O
benefit O
of O
towers O
is O
that O
it O
resembles O
training O
an O
ensemble Method
of Method
models Method
. O
Unfortunately O
, O
our O
attempts O
so O
far O
at O
combining O
the O
towers O
and O
edge Method
network Method
message Method
function Method
have O
failed O
to O
further O
improve O
performance O
, O
possibly O
because O
the O
combination O
makes O
training Task
more O
difficult O
. O
Further O
training O
details O
, O
and O
error Metric
ratios Metric
on O
all O
targets O
can O
be O
found O
in O
the O
supplementary O
material O
. O
Additional O
Experiments O
: O
In O
preliminary O
experiments O
, O
we O
tried O
disabling O
weight Method
tying Method
across O
different O
time O
steps O
. O
However O
, O
we O
found O
that O
the O
most O
effective O
way O
to O
increase O
performance O
was O
to O
tie O
the O
weights O
and O
use O
a O
larger O
hidden O
dimension O
. O
We O
also O
early O
on O
found O
the O
pair Method
message Method
function Method
to O
perform O
worse O
than O
the O
edge Method
network Method
function Method
. O
This O
included O
a O
toy Task
pathfinding Task
problem Task
which O
was O
originally O
designed O
to O
benefit O
from O
using O
pair O
messages O
. O
Also O
, O
when O
trained O
jointly O
on O
the O
13 O
targets O
the O
edge Method
network Method
function Method
outperforms O
pair O
message O
on O
11 O
out O
of O
13 O
targets O
, O
and O
has O
an O
average O
error Metric
ratio Metric
of O
1.53 O
compared O
to O
3.98 O
for O
pair Task
message Task
. O
Given O
the O
difficulties O
with O
training O
this O
function O
we O
did O
not O
pursue O
it O
further O
. O
For O
performance O
on O
smaller O
sized O
training O
sets O
, O
consult O
the O
supplementary O
material O
. O
section O
: O
Conclusions O
and O
Future O
Work O
Our O
results O
show O
that O
MPNNâs Method
with O
the O
appropriate O
message O
, O
update O
, O
and O
output O
functions O
have O
a O
useful O
inductive Metric
bias Metric
for O
predicting Task
molecular Task
properties Task
, O
outperforming O
several O
strong O
baselines O
and O
eliminating O
the O
need O
for O
complicated O
feature Method
engineering Method
. O
Moreover O
, O
our O
results O
also O
reveal O
the O
importance O
of O
allowing O
long O
range O
interactions O
between O
nodes O
in O
the O
graph O
with O
either O
the O
master O
node O
or O
the O
set2set O
output O
. O
The O
towers O
variation O
makes O
these O
models O
more O
scalable O
, O
but O
additional O
improvements O
will O
be O
needed O
to O
scale O
to O
much O
larger O
graphs O
. O
An O
important O
future O
direction O
is O
to O
design O
MPNNs Method
that O
can O
generalize O
effectively O
to O
larger O
graphs O
than O
those O
appearing O
in O
the O
training O
set O
or O
at O
least O
work O
with O
benchmarks O
designed O
to O
expose O
issues O
with O
generalization Task
across O
graph O
sizes O
. O
Generalizing O
to O
larger O
molecule O
sizes O
seems O
particularly O
challenging O
when O
using O
spatial O
information O
. O
First O
of O
all O
, O
the O
pairwise O
distance O
distribution O
depends O
heavily O
on O
the O
number O
of O
atoms O
. O
Second O
, O
our O
most O
successful O
ways O
of O
using O
spatial O
information O
create O
a O
fully O
connected O
graph O
where O
the O
number O
of O
incoming O
messages O
also O
depends O
on O
the O
number O
of O
nodes O
. O
To O
address O
the O
second O
issue O
, O
we O
believe O
that O
adding O
an O
attention Method
mechanism Method
over O
the O
incoming O
message O
vectors O
could O
be O
an O
interesting O
direction O
to O
explore O
. O
section O
: O
Acknowledgements O
We O
would O
like O
to O
thank O
Lukasz O
Kaiser O
, O
Geoffrey O
Irving O
, O
Alex O
Graves O
, O
and O
Yujia O
Li O
for O
helpful O
discussions O
. O
Thank O
you O
to O
Adrian O
Roitberg O
for O
pointing O
out O
an O
issue O
with O
the O
use O
of O
partial O
charges O
in O
an O
earlier O
version O
of O
this O
paper O
. O
bibliography O
: O
References O
section O
: O
Appendix O
subsection O
: O
Interpretation O
of O
Laplacian Method
based Method
models Method
as O
MPNNs Method
Another O
family O
of O
models O
defined O
in O
, O
, O
can O
be O
interpreted O
as O
MPNNs Method
. O
These O
models O
generalize O
the O
notion O
of O
convolutions Method
a O
general O
graph O
with O
nodes O
. O
In O
the O
language O
of O
MPNNs Method
, O
these O
models O
tend O
to O
have O
very O
simple O
message Method
functions Method
and O
are O
typically O
applied O
to O
much O
larger O
graphs O
such O
as O
social O
network O
data O
. O
We O
closely O
follow O
the O
notation O
defined O
in O
equation O
( O
3.2 O
) O
. O
The O
model O
discussed O
in O
( O
equation O
5 O
) O
and O
can O
be O
viewed O
as O
special O
cases O
. O
Given O
an O
adjacency O
matrix O
we O
define O
the O
graph O
Laplacian O
to O
be O
where O
is O
the O
diagonal O
degree O
matrix O
with O
. O
Let O
denote O
the O
eigenvectors O
of O
, O
ordered O
by O
eigenvalue O
. O
Let O
be O
a O
real O
valued O
nonlinearity O
( O
such O
as O
ReLU Method
) O
. O
We O
now O
define O
an O
operation O
which O
transforms O
an O
input O
vector O
of O
size O
to O
a O
vector O
of O
size O
( O
the O
full Method
model Method
can O
defined O
as O
stacking O
these O
operations O
) O
. O
Here O
and O
are O
all O
dimensional O
vectors O
corresponding O
to O
a O
scalar O
feature O
at O
each O
node O
. O
The O
matrices O
are O
all O
diagonal O
matrices O
and O
contain O
all O
of O
the O
learned O
parameters O
in O
the O
layer O
. O
We O
now O
expand O
equation O
[ O
reference O
] O
in O
terms O
of O
the O
full O
vector O
and O
vector O
, O
using O
and O
to O
index O
nodes O
in O
the O
graph O
and O
, O
to O
index O
the O
dimensions O
of O
the O
node O
states O
. O
In O
this O
way O
denotes O
the O
’ O
th O
dimension O
of O
node O
, O
and O
denotes O
the O
’ O
th O
dimension O
of O
node O
, O
furthermore O
we O
use O
to O
denote O
the O
dimensional O
vector O
for O
node O
state O
, O
and O
to O
denote O
the O
dimensional O
vector O
for O
node O
. O
Define O
the O
rank O
4 O
tensor O
of O
dimension O
where O
. O
We O
will O
use O
to O
denote O
the O
dimensional O
matrix O
where O
and O
to O
denote O
the O
dimensional O
matrix O
where O
. O
Writing O
equation O
[ O
reference O
] O
in O
this O
notation O
we O
have O
Relabelling O
as O
and O
as O
this O
last O
line O
can O
be O
interpreted O
as O
the O
message Task
passing Task
update Task
in O
an O
MPNN Method
where O
and O
. O
subsubsection O
: O
The O
special O
case O
of O
Kipf O
and O
Welling O
( O
2016 O
) O
Motivated O
as O
a O
first Method
order Method
approximation Method
of O
the O
graph Method
laplacian Method
methods Method
, O
propose O
the O
following O
layer Method
- Method
wise Method
propagation Method
rule Method
: O
Here O
where O
is O
the O
real O
valued O
adjacency O
matrix O
for O
an O
undirected O
graph O
. O
Adding O
the O
identity O
matrix O
corresponds O
to O
adding O
self O
loops O
to O
the O
graph O
. O
Also O
denotes O
the O
degree O
matrix O
for O
the O
graph O
with O
self O
loops O
, O
is O
a O
layer O
- O
specific O
trainable O
weight O
matrix O
, O
and O
denotes O
a O
real O
valued O
nonlinearity O
. O
Each O
is O
a O
dimensional O
matrix O
indicating O
the O
dimensional O
node O
states O
for O
the O
nodes O
in O
the O
graph O
. O
In O
what O
follows O
, O
given O
a O
matrix O
we O
use O
to O
denote O
the O
row O
in O
indexed O
by O
( O
will O
always O
correspond O
to O
a O
node O
in O
the O
graph O
) O
. O
Let O
. O
To O
get O
the O
updated O
node O
state O
for O
node O
we O
have O
Relabelling O
the O
row O
vector O
as O
an O
dimensional O
column O
vector O
the O
above O
equation O
is O
equivalent O
to O
which O
is O
equivalent O
to O
a O
message O
function O
and O
update Method
function Method
Note O
that O
the O
are O
all O
scalar O
valued O
, O
so O
this O
model O
corresponds O
to O
taking O
a O
certain O
weighted O
average O
of O
neighboring O
nodes O
at O
each O
time O
step O
. O
subsection O
: O
A O
More O
Detailed O
Description O
of O
the O
Quantum Metric
Properties Metric
First O
there O
the O
four O
atomization O
energies O
. O
Atomization O
energy O
at O
( O
eV O
) O
: O
This O
is O
the O
energy O
required O
to O
break O
up O
the O
molecule O
into O
all O
of O
its O
constituent O
atoms O
if O
the O
molecule O
is O
at O
absolute O
zero O
. O
This O
calculation O
assumes O
that O
the O
molecules O
are O
held O
at O
fixed O
volume O
. O
Atomization O
energy O
at O
room O
temperature O
( O
eV O
) O
: O
Like O
, O
this O
is O
the O
energy O
required O
to O
break O
up O
the O
molecule O
if O
it O
is O
at O
room O
temperature O
. O
Enthalpy O
of O
atomization O
at O
room O
temperature O
( O
eV O
) O
: O
The O
enthalpy O
of O
atomization Task
is O
similar O
in O
spirit O
to O
the O
energy O
of O
atomization O
, O
. O
However O
, O
unlike O
the O
energy O
this O
calculation O
assumes O
that O
the O
constituent O
molecules O
are O
held O
at O
fixed O
pressure O
. O
Free Metric
energy Metric
of Metric
atomization Metric
( O
eV O
) O
: O
Once O
again O
this O
is O
similar O
to O
and O
, O
but O
assumes O
that O
the O
system O
is O
held O
at O
fixed O
temperature O
and O
pressure O
during O
the O
dissociation Task
. O
Next O
there O
are O
properties O
related O
to O
fundamental O
vibrations O
of O
the O
molecule O
: O
Highest O
fundamental O
vibrational O
frequency O
( O
) O
: O
Every O
molecule O
has O
fundamental O
vibrational O
modes O
that O
it O
can O
naturally O
oscillate O
at O
. O
is O
the O
mode O
that O
requires O
the O
most O
energy O
. O
Zero O
Point O
Vibrational O
Energy O
( O
ZPVE O
) O
( O
eV O
) O
: O
Even O
at O
zero O
temperature O
quantum O
mechanical O
uncertainty O
implies O
that O
atoms O
vibrate O
. O
This O
is O
known O
as O
the O
zero O
point O
vibrational O
energy O
and O
can O
be O
calculated O
once O
the O
allowed O
vibrational O
modes O
of O
a O
molecule O
are O
known O
. O
Additionally O
, O
there O
are O
a O
number O
of O
properties O
that O
concern O
the O
states O
of O
the O
electrons O
in O
the O
molecule O
: O
Highest O
Occupied O
Molecular O
Orbital O
( O
HOMO O
) O
( O
eV O
) O
: O
Quantum O
mechanics O
dictates O
that O
the O
allowed O
states O
that O
electrons O
can O
occupy O
in O
a O
molecule O
are O
discrete O
. O
The O
Pauli O
exclusion O
principle O
states O
that O
no O
two O
electrons O
may O
occupy O
the O
same O
state O
. O
At O
zero O
temperature O
, O
therefore O
, O
electrons O
stack O
in O
states O
from O
lowest O
energy O
to O
highest O
energy O
. O
HOMO O
is O
the O
energy O
of O
the O
highest O
occupied O
electronic O
state O
. O
Lowest O
Unoccupied O
Molecular O
Orbital O
( O
LUMO O
) O
( O
eV O
) O
: O
Like O
HOMO O
, O
LUMO O
is O
the O
lowest O
energy O
electronic O
state O
that O
is O
unoccupied O
. O
Electron O
energy O
gap O
( O
eV O
) O
: O
This O
is O
the O
difference O
in O
energy O
between O
LUMO O
and O
HOMO O
. O
It O
is O
the O
lowest O
energy O
transition O
that O
can O
occur O
when O
an O
electron O
is O
excited O
from O
an O
occupied O
state O
to O
an O
unoccupied O
state O
. O
also O
dictates O
the O
longest O
wavelength O
of O
light O
that O
the O
molecule O
can O
absorb O
. O
Finally O
, O
there O
are O
several O
measures O
of O
the O
spatial O
distribution O
of O
electrons O
in O
the O
molecule O
: O
Electronic O
Spatial O
Extent O
( O
Bohr O
) O
: O
The O
electronic O
spatial O
extent O
is O
the O
second O
moment O
of O
the O
charge O
distribution O
, O
, O
or O
in O
other O
words O
. O
Norm O
of O
the O
dipole O
moment O
( O
Debye O
) O
: O
The O
dipole O
moment O
, O
, O
approximates O
the O
electric O
field O
far O
from O
a O
molecule O
. O
The O
norm O
of O
the O
dipole O
moment O
is O
related O
to O
how O
anisotropically O
the O
charge O
is O
distributed O
( O
and O
hence O
the O
strength O
of O
the O
field O
far O
from O
the O
molecule O
) O
. O
This O
degree O
of O
anisotropy O
is O
in O
turn O
related O
to O
a O
number O
of O
material O
properties O
( O
for O
example O
hydrogen O
bonding O
in O
water O
causes O
the O
dipole O
moment O
to O
be O
large O
which O
has O
a O
large O
effect O
on O
dynamics Task
and O
surface Task
tension Task
) O
. O
Norm O
of O
the O
static O
polarizability O
( O
Bohr Metric
) O
: O
measures O
the O
extent O
to O
which O
a O
molecule O
can O
spontaneously O
incur O
a O
dipole O
moment O
in O
response O
to O
an O
external O
field O
. O
This O
is O
in O
turn O
related O
to O
the O
degree O
to O
which O
i.e. O
Van O
der O
Waals O
interactions O
play O
a O
role O
in O
the O
dynamics O
of O
the O
medium O
. O
subsection O
: O
Chemical Metric
Accuracy Metric
and O
DFT Metric
Error Metric
In O
Table O
[ O
reference O
] O
we O
list O
the O
mean Metric
absolute Metric
error Metric
numbers O
for O
chemical Metric
accuracy Metric
. O
These O
are O
the O
numbers O
used O
to O
compute O
the O
error Metric
ratios Metric
of O
all O
models O
in O
the O
tables O
. O
The O
mean Metric
absolute Metric
errors Metric
of O
our O
models O
can O
thus O
be O
calculated O
as O
. O
We O
also O
include O
some O
estimates O
of O
the O
mean Metric
absolute Metric
error Metric
for O
the O
DFT Task
calculation Task
to O
the O
ground O
truth O
. O
Both O
the O
estimates O
of O
chemical Metric
accruacy Metric
and O
DFT Metric
error Metric
were O
provided O
in O
. O
subsection O
: O
Additional O
Results O
In O
Table O
[ O
reference O
] O
we O
compare O
the O
performance O
of O
the O
best O
architecture O
( O
edge Method
network Method
+ O
set2set O
output O
) O
on O
different O
sized O
training O
sets O
. O
It O
is O
surprising O
how O
data O
efficient O
this O
model O
is O
on O
some O
targets O
. O
For O
example O
, O
on O
both O
R2 O
and O
Omega O
our O
models O
are O
equal O
or O
better O
with O
11k O
samples O
than O
the O
best O
baseline O
is O
with O
110k O
samples O
. O
In O
Table O
[ O
reference O
] O
we O
compare O
the O
performance O
of O
several O
models O
trained O
without O
spatial O
information O
. O
The O
left O
4 O
columns O
show O
the O
results O
of O
4 O
experiments O
, O
one O
where O
we O
train O
the O
GG Method
- Method
NN Method
model Method
on O
the O
sparse O
graph O
, O
one O
where O
we O
add O
virtual O
edges O
( O
ve O
) O
, O
one O
where O
we O
add O
a O
master O
node O
( O
mn O
) O
, O
and O
one O
where O
we O
change O
the O
graph O
level O
output O
to O
a O
set2set O
output O
( O
s2s O
) O
. O
In O
general O
, O
we O
find O
that O
it O
’s O
important O
to O
allow O
the O
model O
to O
capture O
long O
range O
interactions O
in O
these O
graphs O
. O
In O
Table O
[ O
reference O
] O
we O
compare O
GG Method
- Method
NN Method
+ Method
towers Method
+ Method
set2set Method
output Method
( O
tow8 Method
) O
vs O
a O
baseline Method
GG Method
- Method
NN Method
+ Method
set2set Method
output Method
( O
GG Method
- Method
NN Method
) O
when O
distance O
bins O
are O
used O
. O
We O
do O
this O
comparison O
in O
both O
the O
joint Task
training Task
regime Task
( O
j O
) O
and O
when O
training O
one O
model O
per O
target O
( O
i O
) O
. O
For O
joint Task
training Task
of O
the O
baseline O
we O
used O
100 O
trials O
with O
as O
well O
as O
200 O
trials O
where O
was O
chosen O
randomly O
in O
the O
set O
, O
we O
report O
the O
minimum O
test O
error O
across O
all O
300 O
trials O
. O
For O
individual Task
training Task
of O
the O
baseline O
we O
used O
100 O
trials O
where O
was O
chosen O
uniformly O
in O
the O
range O
. O
The O
towers Method
model Method
was O
always O
trained O
with O
and O
, O
with O
100 O
tuning O
trials O
for O
joint Method
training Method
and O
50 O
trials O
for O
individual Task
training Task
. O
The O
towers Method
model Method
outperforms O
the O
baseline O
model O
on O
12 O
out O
of O
13 O
targets O
in O
both O
individual Task
and Task
joint Task
target Task
training Task
. O
In O
Table O
[ O
reference O
] O
right O
2 O
columns O
compare O
the O
edge Method
network Method
( O
enn Method
) O
with O
the O
pair Method
message Method
network Method
( O
pm Method
) O
in O
the O
joint O
training O
regime O
( O
j O
) O
. O
The O
edge Method
network Method
consistently O
outperforms O
the O
pair Method
message Method
function Method
across O
most O
targets O
. O
In O
Table O
[ O
reference O
] O
we O
compare O
our O
MPNNs Method
with O
different O
input O
featurizations O
. O
All O
models O
use O
the O
Set2Set O
output O
and O
GRU Method
update Method
functions Method
. O
The O
no Method
distance Method
model Method
uses O
the O
matrix Method
multiply Method
message Method
function Method
, O
the O
distance Method
models Method
use O
the O
edge Method
neural Method
network Method
message Method
function Method
. O
