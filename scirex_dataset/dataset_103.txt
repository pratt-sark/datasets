document	O
:	O
MEMEN	Method
:	O
Multi	Method
-	Method
layer	Method
Embedding	Method
with	Method
Memory	Method
Networks	Method
for	O
Machine	Task
Comprehension	Task
Machine	Task
comprehension	Task
(	O
MC	Task
)	O
style	O
question	O
answering	O
is	O
a	O
representative	O
problem	O
in	O
natural	Task
language	Task
processing	Task
.	O
Previous	O
methods	O
rarely	O
spend	O
time	O
on	O
the	O
improvement	O
of	O
encoding	Task
layer	Task
,	O
especially	O
the	O
embedding	O
of	O
syntactic	O
information	O
and	O
name	O
entity	O
of	O
the	O
words	O
,	O
which	O
are	O
very	O
crucial	O
to	O
the	O
quality	O
of	O
encoding	Task
.	O
Moreover	O
,	O
existing	O
attention	Method
methods	Method
represent	O
each	O
query	O
word	O
as	O
a	O
vector	O
or	O
use	O
a	O
single	O
vector	O
to	O
represent	O
the	O
whole	O
query	O
sentence	O
,	O
neither	O
of	O
them	O
can	O
handle	O
the	O
proper	O
weight	O
of	O
the	O
key	O
words	O
in	O
query	O
sentence	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
novel	O
neural	Method
network	Method
architecture	Method
called	O
Multi	Method
-	Method
layer	Method
Embedding	Method
with	Method
Memory	Method
Network	Method
(	O
MEMEN	Method
)	O
for	O
machine	Task
reading	Task
task	Task
.	O
In	O
the	O
encoding	Method
layer	Method
,	O
we	O
employ	O
classic	O
skip	Method
-	Method
gram	Method
model	Method
to	O
the	O
syntactic	O
and	O
semantic	O
information	O
of	O
the	O
words	O
to	O
train	O
a	O
new	O
kind	O
of	O
embedding	Method
layer	Method
.	O
We	O
also	O
propose	O
a	O
memory	Method
network	Method
of	Method
full	Method
-	Method
orientation	Method
matching	Method
of	O
the	O
query	O
and	O
passage	O
to	O
catch	O
more	O
pivotal	O
information	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
has	O
competitive	O
results	O
both	O
from	O
the	O
perspectives	O
of	O
precision	Metric
and	O
efficiency	Metric
in	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
among	O
all	O
published	O
results	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
TriviaQA	Material
dataset	Material
.	O
section	O
:	O
Introduction	O
Machine	Task
comprehension	Task
(	O
MC	Task
)	O
has	O
gained	O
significant	O
popularity	O
over	O
the	O
past	O
few	O
years	O
and	O
it	O
is	O
a	O
coveted	O
goal	O
in	O
the	O
field	O
of	O
natural	Task
language	Task
processing	Task
and	O
artificial	Task
intelligence	Task
.	O
Its	O
task	O
is	O
to	O
teach	O
machine	O
to	O
understand	O
the	O
content	O
of	O
a	O
given	O
passage	O
and	O
then	O
answer	O
the	O
question	O
related	O
to	O
it	O
.	O
Figure	O
1	O
shows	O
a	O
simple	O
example	O
from	O
the	O
popular	O
dataset	O
SQuAD	Material
.	O
Many	O
significant	O
works	O
are	O
based	O
on	O
this	O
task	O
,	O
and	O
most	O
of	O
them	O
focus	O
on	O
the	O
improvement	O
of	O
a	O
sequence	Method
model	Method
that	O
is	O
augmented	O
with	O
an	O
attention	Method
mechanism	Method
.	O
However	O
,	O
the	O
encoding	O
of	O
the	O
words	O
is	O
also	O
crucial	O
and	O
a	O
better	O
encoding	Method
layer	Method
can	O
lead	O
to	O
substantial	O
difference	O
to	O
the	O
final	O
performance	O
.	O
Many	O
powerful	O
methods	O
only	O
represent	O
their	O
words	O
in	O
two	O
ways	O
,	O
word	Method
-	Method
level	Method
embeddings	Method
and	O
character	Method
-	Method
level	Method
embeddings	Method
.	O
They	O
use	O
pre	O
-	O
train	O
vectors	O
,	O
like	O
GloVe	Method
,	O
to	O
do	O
the	O
word	O
-	O
level	O
embeddings	O
,	O
which	O
ignore	O
syntactic	O
information	O
and	O
name	O
entity	O
of	O
the	O
words	O
.	O
construct	O
a	O
sequence	O
of	O
syntactic	O
nodes	O
for	O
the	O
words	O
and	O
encodes	O
the	O
sequence	O
into	O
a	O
vector	Method
representation	Method
.	O
However	O
,	O
they	O
neglected	O
the	O
optimization	O
of	O
the	O
initial	O
embedding	O
and	O
did	O
n’t	O
take	O
the	O
semantic	O
information	O
of	O
the	O
words	O
into	O
account	O
,	O
which	O
are	O
very	O
important	O
parts	O
in	O
the	O
vector	Method
representations	Method
of	O
the	O
words	O
.	O
For	O
example	O
,	O
the	O
word	O
“	O
Apple	O
”	O
is	O
a	O
fixed	O
vector	O
in	O
GloVe	O
and	O
noun	O
in	O
syntactics	O
whatever	O
it	O
represents	O
the	O
fruit	O
or	O
the	O
company	O
,	O
but	O
name	O
entity	O
tags	O
can	O
help	O
recognize	O
.	O
Moreover	O
,	O
the	O
attention	Method
mechanism	Method
can	O
be	O
divided	O
into	O
two	O
categories	O
:	O
one	Method
dimensional	Method
attention	Method
and	O
two	Method
dimensional	Method
attention	Method
.	O
In	O
one	Task
dimensional	Task
attention	Task
,	O
the	O
whole	O
query	O
is	O
represented	O
by	O
one	O
embedding	O
vector	O
,	O
which	O
is	O
usually	O
the	O
last	O
hidden	O
state	O
in	O
the	O
neural	Method
network	Method
.	O
However	O
,	O
using	O
only	O
one	O
vector	O
to	O
represent	O
the	O
whole	O
query	O
will	O
attenuate	O
the	O
attention	O
of	O
key	O
words	O
.	O
On	O
the	O
contrary	O
,	O
every	O
word	O
in	O
the	O
query	O
has	O
its	O
own	O
embedding	O
vector	O
in	O
the	O
situation	O
of	O
two	O
dimensional	O
attention	O
,	O
but	O
many	O
words	O
in	O
the	O
question	O
sentence	O
are	O
useless	O
even	O
if	O
disturbing	O
,	O
such	O
as	O
the	O
stopwords	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
the	O
Multi	Method
-	Method
layer	Method
Embedding	Method
with	Method
Memory	Method
Networks	Method
(	O
MEMEN	Method
)	O
,	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
for	O
machine	Task
comprehension	Task
task	Task
.	O
Our	O
model	O
consists	O
of	O
three	O
parts	O
:	O
1	O
)	O
the	O
encoding	Task
of	Task
context	Task
and	Task
query	Task
,	O
in	O
which	O
we	O
add	O
useful	O
syntactic	O
and	O
semantic	O
information	O
in	O
the	O
embedding	O
of	O
every	O
word	O
,	O
2	O
)	O
the	O
high	O
-	O
efficiency	O
multi	Method
-	Method
layer	Method
memory	Method
network	Method
of	O
full	Method
-	Method
orientation	Method
matching	Method
to	O
match	O
the	O
question	O
and	O
context	O
,	O
3	O
)	O
the	O
pointer	Method
-	Method
network	Method
based	Method
answer	Method
boundary	Method
prediction	Method
layer	Method
to	O
get	O
the	O
location	O
of	O
the	O
answer	O
in	O
the	O
passage	O
.	O
The	O
contributions	O
of	O
this	O
paper	O
can	O
be	O
summarized	O
as	O
follows	O
.	O
First	O
,	O
we	O
propose	O
a	O
novel	O
multi	Method
-	Method
layer	Method
embedding	Method
of	O
the	O
words	O
in	O
the	O
passage	O
and	O
query	O
.	O
We	O
use	O
skip	Method
-	Method
gram	Method
model	Method
to	O
train	O
the	O
part	Task
-	Task
of	Task
-	Task
speech	Task
(	O
POS	Material
)	O
tags	O
and	O
name	O
-	O
entity	O
recognition	O
(	O
NER	Task
)	O
tags	Method
embedding	Method
that	O
represent	O
the	O
syntactic	O
and	O
semantic	O
information	O
of	O
the	O
words	O
respectively	O
.	O
The	O
analogy	Task
inference	Task
provided	O
by	O
skip	Method
-	Method
gram	Method
model	Method
can	O
make	O
the	O
similar	O
attributes	O
close	O
in	O
their	O
embedding	O
space	O
such	O
that	O
more	O
adept	O
at	O
helping	O
find	O
the	O
answer	O
.	O
Second	O
,	O
we	O
introduce	O
a	O
memory	Method
networks	Method
of	Method
full	Method
-	Method
orientation	Method
matching	Method
.	O
To	O
combines	O
the	O
advantages	O
of	O
one	Method
dimensional	Method
attention	Method
and	O
two	O
dimensional	Method
attention	Method
,	O
our	O
novel	O
hierarchical	O
attention	O
vectors	O
contain	O
both	O
of	O
them	O
.	O
Because	O
key	O
words	O
in	O
query	O
often	O
appear	O
at	O
ends	O
of	O
the	O
sentence	O
,	O
one	O
-	O
dimensional	Method
attention	Method
,	O
in	O
which	O
the	O
bi	O
-	O
directional	O
last	O
hidden	O
states	O
are	O
regarded	O
as	O
representation	O
,	O
is	O
able	O
to	O
capture	O
more	O
useful	O
information	O
compared	O
to	O
only	O
applying	O
two	Method
dimensional	Method
attention	Method
.	O
In	O
order	O
to	O
deepen	O
the	O
memory	O
and	O
better	O
understand	O
the	O
passage	O
according	O
to	O
the	O
query	O
,	O
we	O
employ	O
the	O
structure	O
of	O
multi	O
-	O
hops	O
to	O
repeatedly	O
read	O
the	O
passage	O
.	O
Moreover	O
,	O
we	O
add	O
a	O
gate	O
to	O
the	O
end	O
of	O
each	O
memory	O
to	O
improve	O
the	O
speed	O
of	O
convergence	Metric
.	O
Finally	O
,	O
the	O
proposed	O
method	O
yields	O
competitive	O
results	O
on	O
the	O
large	O
machine	O
comprehension	O
bench	O
marks	O
SQuAD	Material
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
TriviaQA	Material
dataset	Material
.	O
On	O
SQuAD	Material
,	O
our	O
model	O
achieves	O
75.37	O
%	O
exact	Metric
match	Metric
and	O
82.66	O
%	O
F1	Metric
score	Metric
.	O
Moreover	O
,	O
our	O
model	O
avoids	O
the	O
high	O
computation	Method
complexity	Method
self	Method
-	Method
matching	Method
mechanism	Method
which	O
is	O
popular	O
in	O
many	O
previous	O
works	O
,	O
thus	O
we	O
spend	O
much	O
less	O
time	O
and	O
memory	O
when	O
training	O
the	O
model	O
.	O
section	O
:	O
Model	O
Structure	O
As	O
Figure	O
2	O
shows	O
,	O
our	O
machine	Method
reading	Method
model	Method
consists	O
of	O
three	O
parts	O
.	O
First	O
,	O
we	O
concatenate	O
several	O
layers	O
of	O
embedding	O
of	O
questions	O
and	O
contexts	O
and	O
pass	O
them	O
into	O
a	O
bi	Method
-	Method
directional	Method
RNN	Method
.	O
Then	O
we	O
obtain	O
the	O
relationship	O
between	O
query	O
and	O
context	O
through	O
a	O
novel	O
full	Method
-	Method
orientation	Method
matching	Method
and	O
apply	O
memory	Method
networks	Method
in	O
order	O
to	O
deeply	O
understand	O
.	O
In	O
the	O
end	O
,	O
the	O
output	O
layer	O
helps	O
locate	O
the	O
answer	O
in	O
the	O
passage	O
.	O
subsection	O
:	O
Encoding	Task
of	Task
Context	Task
and	Task
Query	Task
In	O
the	O
encoding	Method
layer	Method
,	O
we	O
represent	O
all	O
tokens	O
in	O
the	O
context	O
and	O
question	O
as	O
a	O
sequence	O
of	O
embeddings	O
and	O
pass	O
them	O
as	O
the	O
input	O
to	O
a	O
recurrent	Method
neural	Method
network	Method
.	O
Word	O
-	O
level	O
embeddings	O
and	O
character	Method
-	Method
level	Method
embeddings	Method
are	O
first	O
applied	O
.	O
We	O
use	O
pre	O
-	O
trained	O
word	Method
vectors	Method
GloVe	Method
to	O
obtain	O
the	O
fixed	O
word	O
embedding	O
of	O
each	O
word	O
.	O
The	O
character	O
-	O
level	O
embeddings	O
are	O
generated	O
by	O
using	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNN	Method
)	O
which	O
is	O
applied	O
to	O
the	O
characters	O
of	O
each	O
word	O
(	O
Kim	O
,	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O
This	O
layer	O
maps	O
each	O
token	O
to	O
a	O
high	O
dimensional	O
vector	O
space	O
and	O
is	O
proved	O
to	O
be	O
helpful	O
in	O
handling	O
out	O
-	O
of	O
-	O
vocab	O
(	O
OOV	O
)	O
words	O
.	O
We	O
also	O
use	O
skip	Method
-	Method
gram	Method
model	Method
to	O
train	O
the	O
embeddings	O
of	O
part	Task
-	Task
of	Task
-	Task
speech	Task
(	O
POS	Material
)	O
tags	O
and	O
named	O
-	O
entity	O
recognition	O
(	O
NER	Task
)	O
tags	O
.	O
We	O
first	O
transform	O
all	O
of	O
the	O
given	O
training	O
set	O
into	O
their	O
part	Task
-	Task
of	Task
-	Task
speech	Task
(	O
POS	Material
)	O
tags	O
and	O
named	O
-	O
entity	O
recognition	O
(	O
NER	Task
)	O
tags	O
,	O
which	O
can	O
be	O
showed	O
in	O
Figure	O
3	O
.	O
Then	O
we	O
employ	O
skip	Method
-	Method
sram	Method
model	Method
,	O
which	O
is	O
one	O
of	O
the	O
core	O
algorithms	O
in	O
the	O
popular	O
off	O
-	O
the	O
-	O
shelf	O
embedding	Task
word2vec	Task
,	O
to	O
the	O
transformed	O
“	O
passage	O
”	O
just	O
like	O
it	O
works	O
in	O
word2vec	Material
for	O
the	O
normal	O
passage	O
.	O
Given	O
a	O
sequence	O
of	O
training	O
words	O
in	O
the	O
transformed	O
passage	O
:	O
,	O
the	O
objective	O
of	O
the	O
skip	Method
-	Method
gram	Method
model	Method
is	O
to	O
maximize	O
the	O
average	Metric
log	Metric
probability	Metric
:	O
where	O
is	O
the	O
size	O
of	O
the	O
context	O
which	O
can	O
be	O
set	O
manually	O
,	O
a	O
large	O
means	O
more	O
accurate	O
results	O
and	O
more	O
training	O
time	O
.	O
The	O
is	O
defined	O
by	O
:	O
where	O
and	O
are	O
the	O
input	O
and	O
output	O
vector	O
of	O
,	O
and	O
is	O
the	O
vocabulary	O
size	O
.	O
We	O
finally	O
get	O
the	O
fixed	O
length	O
embedding	O
of	O
each	O
tag	O
.	O
Although	O
the	O
number	O
of	O
tags	O
limits	O
the	O
effect	O
of	O
word	Task
analogy	Task
inference	Task
,	O
it	O
still	O
be	O
very	O
helpful	O
compared	O
to	O
simple	O
one	Method
hot	Method
embedding	Method
since	O
similar	O
tags	O
have	O
similar	O
surroundings	O
.	O
In	O
the	O
end	O
,	O
we	O
use	O
a	O
BiLSTM	Method
to	O
encode	O
both	O
the	O
context	O
and	O
query	O
embeddings	O
and	O
obtain	O
their	O
representations	O
and	O
and	O
the	O
last	O
hidden	O
state	O
of	O
both	O
directions	O
of	O
query	Method
representation	Method
.	O
where	O
,	O
,	O
represent	O
word	Method
-	Method
level	Method
embedding	Method
,	O
character	Method
-	Method
level	Method
embedding	Method
and	O
tags	Method
embedding	Method
respectively	O
.	O
is	O
the	O
concatenation	O
of	O
both	O
directions	O
’	O
last	O
hidden	O
state	O
.	O
subsection	O
:	O
Memory	Method
Network	Method
of	Method
Full	Method
-	Method
Orientation	Method
Matching	Method
Attention	Method
mechanism	Method
is	O
a	O
common	O
way	O
to	O
link	O
and	O
blend	O
the	O
content	O
between	O
the	O
context	O
and	O
query	O
.	O
Unlike	O
previous	O
methods	O
that	O
are	O
either	O
two	Method
dimensional	Method
matching	Method
or	O
one	Method
dimensional	Method
matching	Method
,	O
we	O
propose	O
a	O
full	Method
-	Method
orientation	Method
matching	Method
layer	Method
that	O
synthesizes	O
both	O
of	O
them	O
and	O
thus	O
combine	O
the	O
advantages	O
of	O
both	O
side	O
and	O
hedge	O
the	O
weakness	O
.	O
After	O
concatenating	O
all	O
the	O
attention	O
vectors	O
,	O
we	O
will	O
pass	O
them	O
into	O
a	O
bi	Method
-	Method
directional	Method
LSTM	Method
.	O
We	O
start	O
by	O
describing	O
our	O
model	O
in	O
the	O
single	Task
layer	Task
case	Task
,	O
which	O
implements	O
a	O
single	Method
memory	Method
hop	Method
operation	Method
.	O
We	O
then	O
show	O
it	O
can	O
be	O
stacked	O
to	O
give	O
multiple	O
hops	O
in	O
memory	O
.	O
subsubsection	O
:	O
Integral	Method
Query	Method
Matching	Method
The	O
input	O
of	O
this	O
step	O
is	O
the	O
representations	O
,	O
and	O
.	O
At	O
first	O
we	O
obtain	O
the	O
importance	O
of	O
each	O
word	O
in	O
passage	O
according	O
to	O
the	O
integral	O
query	O
by	O
means	O
of	O
computing	O
the	O
match	O
between	O
and	O
each	O
representation	O
by	O
taking	O
the	O
inner	Method
product	Method
followed	O
by	O
a	O
softmax	Method
:	O
Subsequently	O
the	O
first	O
matching	Method
module	Method
is	O
the	O
sum	O
of	O
the	O
inputs	O
weighted	O
by	O
attention	O
:	O
subsubsection	O
:	O
Query	Method
-	Method
Based	Method
Similarity	Method
Matching	Method
We	O
then	O
obtain	O
an	O
alignment	O
matrix	O
between	O
the	O
query	O
and	O
context	O
by	O
,	O
is	O
the	O
weight	O
parameter	O
,	O
is	O
elementwise	Method
multiplication	Method
.	O
Like	O
Seo	O
et	O
al	O
.	O
(	O
2017	O
)	O
,	O
we	O
use	O
this	O
alignment	O
matrix	O
to	O
compute	O
whether	O
the	O
query	O
words	O
are	O
relevant	O
to	O
each	O
context	O
word	O
.	O
For	O
each	O
context	O
word	O
,	O
there	O
is	O
an	O
attention	O
weight	O
that	O
represents	O
how	O
much	O
it	O
is	O
relevant	O
to	O
every	O
query	O
word	O
:	O
means	O
the	O
softmax	Method
function	Method
is	O
performed	O
across	O
the	O
row	O
vector	O
,	O
and	O
each	O
attention	O
vector	O
is	O
,	O
which	O
is	O
based	O
on	O
the	O
query	Method
embedding	Method
.	O
Hence	O
the	O
second	O
matching	Method
module	Method
is	O
,	O
where	O
each	O
is	O
the	O
column	O
of	O
.	O
subsubsection	O
:	O
Context	Method
-	Method
Based	Method
Similarity	Method
Matching	Method
When	O
we	O
consider	O
the	O
relevance	O
between	O
context	O
and	O
query	O
,	O
the	O
most	O
representative	O
word	O
in	O
the	O
query	O
sentence	O
can	O
be	O
chosen	O
by	O
,	O
and	O
the	O
attention	O
is	O
.	O
Then	O
we	O
obtain	O
the	O
last	O
matching	Method
module	Method
which	O
is	O
based	O
on	O
the	O
context	Method
embedding	Method
.	O
We	O
put	O
all	O
of	O
the	O
memories	O
in	O
a	O
linear	O
function	O
to	O
get	O
the	O
integrated	O
hierarchical	Method
matching	Method
module	Method
:	O
where	O
is	O
an	O
simple	O
linear	Method
function	Method
,	O
and	O
are	O
matrixes	O
that	O
are	O
tiled	O
n	O
times	O
by	O
and	O
.	O
Moreover	O
,	O
add	O
an	O
additional	O
gate	O
to	O
the	O
input	O
of	O
RNN	Method
:	O
The	O
gate	O
is	O
based	O
on	O
the	O
integration	O
of	O
hierarchical	O
attention	O
vectors	O
,	O
and	O
it	O
effectively	O
filtrates	O
the	O
part	O
of	O
tokens	O
that	O
are	O
helpful	O
in	O
understanding	O
the	O
relation	O
between	O
passage	O
and	O
query	O
.	O
Additionally	O
,	O
we	O
add	O
a	O
bias	O
to	O
improve	O
the	O
estimation	Task
:	O
Experiments	O
prove	O
that	O
this	O
gate	O
can	O
also	O
accelerate	O
the	O
speed	Metric
of	Metric
convergence	Metric
.	O
Finally	O
,	O
the	O
integrated	O
memory	O
is	O
passed	O
into	O
a	O
bi	Method
-	Method
directional	Method
LSTM	Method
,	O
and	O
the	O
output	O
will	O
captures	O
the	O
interaction	O
among	O
the	O
context	O
words	O
and	O
the	O
query	O
words	O
:	O
In	O
multiple	O
layers	O
,	O
the	O
integrated	O
hierarchical	Method
matching	Method
module	Method
can	O
be	O
regarded	O
as	O
the	O
input	O
of	O
next	O
layer	O
after	O
a	O
dimensionality	Method
reduction	Method
processing	Method
.	O
We	O
call	O
this	O
memory	Method
networks	Method
of	Method
full	Method
-	Method
orientation	Method
matching	Method
.	O
subsection	O
:	O
Output	O
layer	O
In	O
this	O
layer	O
,	O
we	O
follow	O
to	O
use	O
the	O
boundary	Method
model	Method
of	Method
pointer	Method
networks	Method
to	O
locate	O
the	O
answer	O
in	O
the	O
passage	O
.	O
Moreover	O
,	O
we	O
follow	O
to	O
initialize	O
the	O
hidden	O
state	O
of	O
the	O
pointer	Method
network	Method
by	O
a	O
query	Method
-	Method
aware	Method
representation	Method
:	O
where	O
,	O
and	O
are	O
parameters	O
,	O
is	O
the	O
initial	O
hidden	O
state	O
of	O
the	O
pointer	Method
network	Method
.	O
Then	O
we	O
use	O
the	O
passage	Method
representation	Method
along	O
with	O
the	O
initialized	O
hidden	O
state	O
to	O
predict	O
the	O
indices	O
that	O
represent	O
the	O
answer	O
’s	O
location	O
in	O
the	O
passage	O
:	O
where	O
is	O
parameter	O
,	O
that	O
respectively	O
represent	O
the	O
start	O
point	O
and	O
the	O
end	O
point	O
of	O
the	O
answer	O
,	O
is	O
the	O
vector	O
that	O
represents	O
-	O
th	O
word	O
in	O
the	O
passage	O
of	O
the	O
final	O
output	O
of	O
the	O
memory	Method
networks	Method
.	O
To	O
get	O
the	O
next	O
layer	O
of	O
hidden	O
state	O
,	O
we	O
need	O
to	O
pass	O
weighted	O
by	O
current	O
predicted	O
probability	O
to	O
the	O
Gated	Method
Recurrent	Method
Unit	Method
(	O
GRU	Method
)(	O
Chung	O
et	O
al	O
.	O
,	O
2014	O
)	O
:	O
For	O
the	O
loss	O
function	O
,	O
we	O
minimize	O
the	O
sum	O
of	O
the	O
negative	O
probabilities	O
of	O
the	O
true	O
start	O
and	O
end	O
indices	O
by	O
the	O
predicted	O
distributions	O
.	O
section	O
:	O
Experiment	O
subsection	O
:	O
Implementation	O
Settings	O
The	O
tokenizers	O
we	O
use	O
in	O
the	O
step	O
of	O
preprocessing	Task
data	Task
are	O
from	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O
We	O
also	O
use	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagger	O
and	O
named	Method
-	Method
entity	Method
recognition	Method
tagger	Method
in	O
Stanford	O
CoreNLP	O
utilities	O
to	O
transform	O
the	O
passage	O
and	O
question	O
.	O
For	O
the	O
skip	Method
-	Method
gram	Method
model	Method
,	O
our	O
model	O
refers	O
to	O
the	O
word2vec	Method
module	Method
in	O
open	O
source	O
software	O
library	O
,	O
Tensorflow	Method
,	O
the	O
skip	O
window	O
is	O
set	O
as	O
2	O
.	O
The	O
dataset	O
we	O
use	O
to	O
train	O
the	O
embedding	O
of	O
POS	Material
tags	O
and	O
NER	Task
tags	O
are	O
the	O
training	O
set	O
given	O
by	O
SQuAD	Material
,	O
in	O
which	O
all	O
the	O
sentences	O
are	O
tokenized	O
and	O
regrouped	O
as	O
a	O
list	O
.	O
To	O
improve	O
the	O
reliability	O
and	O
stabllity	O
,	O
we	O
screen	O
out	O
the	O
sentences	O
whose	O
length	O
are	O
shorter	O
than	O
9	O
.	O
We	O
use	O
100	O
one	Method
dimensional	Method
filters	Method
for	O
CNN	Method
in	O
the	O
character	Task
level	Task
embedding	Task
,	O
with	O
width	O
of	O
5	O
for	O
each	O
one	O
.	O
We	O
set	O
the	O
hidden	O
size	O
as	O
100	O
for	O
all	O
the	O
LSTM	O
and	O
GRU	Method
layers	O
and	O
apply	O
dropout	Method
between	Method
layers	Method
with	O
a	O
dropout	O
ratio	O
as	O
0.2	O
.	O
We	O
use	O
the	O
AdaDelta	Method
optimizer	Method
with	O
a	O
initial	O
learning	Metric
rate	Metric
as	O
0.001	O
.	O
For	O
the	O
memory	Method
networks	Method
,	O
we	O
set	O
the	O
number	O
of	O
layer	O
as	O
3	O
.	O
subsection	O
:	O
TriviaQA	Material
Results	O
We	O
first	O
evaluate	O
our	O
model	O
on	O
a	O
large	O
scale	O
reading	O
comprehension	O
dataset	O
TriviaQA	Material
version1.0	O
.	O
TriviaQA	Material
contains	O
over	O
650	O
K	O
question	O
-	O
answer	O
-	O
evidence	O
triples	O
,	O
that	O
are	O
derived	O
from	O
Web	O
search	O
results	O
and	O
Wikipedia	O
pages	O
â	O
with	O
highly	O
differing	O
levels	O
of	O
information	O
redundancy	O
.	O
TriviaQA	Material
is	O
the	O
first	O
dataset	O
where	O
questions	O
are	O
authored	O
by	O
trivia	O
enthusiasts	O
,	O
independently	O
of	O
the	O
evidence	O
documents	O
.	O
There	O
are	O
two	O
different	O
metrics	O
to	O
evaluate	O
model	Metric
accuracy	Metric
:	O
Exact	Metric
Match	Metric
(	O
EM	Metric
)	O
and	O
F1	Metric
Score	Metric
,	O
which	O
measures	O
the	O
weighted	Metric
average	Metric
of	O
the	O
precision	Metric
and	O
recall	Metric
rate	Metric
at	O
character	O
level	O
.	O
Because	O
the	O
evidence	O
is	O
gathered	O
by	O
an	O
automated	Method
process	Method
,	O
the	O
documents	O
are	O
not	O
guaranteed	O
to	O
contain	O
all	O
facts	O
needed	O
to	O
answer	O
the	O
question	O
.	O
In	O
addition	O
to	O
distant	Task
supervision	Task
evaluation	Task
,	O
we	O
also	O
evaluate	O
models	O
on	O
a	O
verified	O
subsets	O
.	O
Because	O
the	O
test	O
set	O
is	O
not	O
released	O
,	O
we	O
train	O
our	O
model	O
on	O
training	O
set	O
and	O
evaluate	O
our	O
model	O
on	O
dev	O
set	O
.	O
As	O
we	O
can	O
see	O
in	O
Figure	O
4	O
,	O
our	O
model	O
outperforms	O
all	O
other	O
baselines	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
all	O
subsets	O
on	O
TriviaQA	Material
.	O
subsection	O
:	O
SQuAD	Material
Results	O
We	O
also	O
use	O
the	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
v1.1	Material
to	O
conduct	O
our	O
experiments	O
.	O
Passages	O
in	O
the	O
dataset	O
are	O
retrieved	O
from	O
English	O
Wikipedia	O
by	O
means	O
of	O
Project	O
Nayuki	O
’s	O
Wikipedia	O
’s	O
internal	O
PageRanks	O
.	O
They	O
sampled	O
536	O
articles	O
uniformly	O
at	O
random	O
with	O
a	O
wide	O
range	O
of	O
topics	O
,	O
from	O
musical	O
celebrities	O
to	O
abstract	O
concepts	O
.	O
The	O
dataset	O
is	O
partitioned	O
randomly	O
into	O
a	O
training	O
set	O
(	O
80	O
%	O
)	O
,	O
a	O
development	O
set	O
(	O
10	O
%	O
)	O
,	O
and	O
a	O
hidden	O
test	O
set	O
(	O
10	O
%	O
)	O
.	O
The	O
host	O
of	O
SQuAD	Material
did	O
n’t	O
release	O
the	O
test	O
set	O
to	O
the	O
public	O
,	O
so	O
everybody	O
has	O
to	O
submit	O
their	O
model	O
and	O
the	O
host	O
will	O
run	O
it	O
on	O
the	O
test	O
set	O
for	O
them	O
.	O
Figure	O
5	O
shows	O
the	O
performance	O
of	O
our	O
model	O
and	O
competing	O
approaches	O
on	O
the	O
SQuAD	Material
.	O
The	O
results	O
of	O
this	O
dataset	O
are	O
all	O
exhibited	O
on	O
a	O
leaderboard	Method
,	O
and	O
top	O
methods	O
are	O
almost	O
all	O
ensemble	Method
models	Method
,	O
our	O
model	O
achieves	O
an	O
exact	Metric
match	Metric
score	O
of	O
75.37	O
%	O
and	O
an	O
F1	Metric
score	Metric
of	O
82.66	O
%	O
,	O
which	O
is	O
competitive	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
.	O
subsection	O
:	O
Ensemble	O
Details	O
The	O
main	O
current	O
ensemble	Method
methods	Method
in	O
the	O
machine	Task
comprehension	Task
is	O
simply	O
choosing	O
the	O
answer	O
with	O
the	O
highest	O
sum	O
of	O
confidence	O
scores	O
among	O
several	O
single	O
models	O
which	O
are	O
exactly	O
identical	O
except	O
the	O
random	O
initial	O
seed	O
.	O
However	O
,	O
the	O
performance	O
of	O
ensemble	Method
model	Method
can	O
obviously	O
be	O
better	O
if	O
there	O
is	O
some	O
diversity	O
among	O
single	O
models	O
.	O
In	O
our	O
SQuAD	Material
experiment	O
,	O
we	O
get	O
the	O
value	O
of	O
learning	Metric
rate	Metric
and	O
dropout	Metric
ratio	Metric
of	O
each	O
model	O
by	O
a	O
gaussian	Method
distribution	Method
,	O
in	O
which	O
the	O
mean	O
value	O
are	O
0.001	O
and	O
0.2	O
respectively	O
.	O
To	O
keep	O
the	O
diversity	O
in	O
a	O
reasonable	O
scope	O
,	O
we	O
set	O
the	O
variance	O
of	O
gaussian	Method
distribution	Method
as	O
and	O
respectively	O
.	O
Finally	O
,	O
we	O
build	O
an	O
ensemble	Method
model	Method
which	O
consists	O
of	O
14	O
single	O
models	O
with	O
different	O
parameters	O
.	O
subsection	O
:	O
Speed	O
and	O
Efficiency	O
Compared	O
to	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
the	O
SQuAD	Material
test	O
set	O
,	O
our	O
model	O
does	O
n’t	O
contain	O
the	O
self	Method
-	Method
matching	Method
attention	Method
layer	Method
which	O
is	O
stuck	O
with	O
high	O
computational	Metric
complexity	Metric
.	O
Our	O
MEMEN	Method
was	O
trained	O
with	O
NVIDIA	O
Titan	O
X	O
GPU	O
,	O
and	O
the	O
training	O
process	O
of	O
the	O
3	Method
-	Method
hops	Method
model	Method
took	O
roughly	O
5	O
hours	O
on	O
a	O
single	O
GPU	O
.	O
However	O
,	O
an	O
one	Method
-	Method
hop	Method
model	Method
took	O
22	O
hours	O
when	O
we	O
added	O
self	Method
-	Method
matching	Method
layer	Method
in	O
attention	O
memory	O
.	O
Although	O
the	O
accuracy	Metric
is	O
improved	O
a	O
little	O
compared	O
to	O
one	Method
-	Method
hop	Method
MEMEN	Method
model	Method
,	O
it	O
declined	O
sharply	O
as	O
the	O
number	O
of	O
hops	O
increased	O
,	O
not	O
to	O
speak	O
of	O
the	O
disadvantage	O
of	O
running	Metric
time	Metric
.	O
The	O
reason	O
might	O
be	O
that	O
multi	Method
-	Method
hops	Method
model	Method
with	O
self	Method
-	Method
matching	Method
layer	Method
is	O
too	O
complex	O
to	O
efficiently	O
learn	O
the	O
features	O
for	O
this	O
dataset	O
.	O
As	O
a	O
result	O
,	O
our	O
model	O
is	O
competitive	O
both	O
in	O
accuracy	Metric
and	O
efficiency	Metric
.	O
subsection	O
:	O
Hops	O
and	O
Ablations	O
Figure	O
6	O
shows	O
the	O
performance	O
of	O
our	O
single	O
model	O
on	O
SQuAD	Material
dev	O
set	O
with	O
different	O
number	O
of	O
hops	O
in	O
the	O
memory	O
network	O
.	O
As	O
we	O
can	O
see	O
,	O
both	O
the	O
EM	Metric
and	O
F1	Metric
score	Metric
increase	O
as	O
the	O
number	O
of	O
hops	O
enlarges	O
until	O
it	O
arrives	O
3	O
.	O
After	O
the	O
model	O
achieves	O
the	O
best	O
performance	O
with	O
3	O
hops	O
,	O
the	O
performance	O
gets	O
worse	O
as	O
the	O
number	O
of	O
hops	O
gets	O
large	O
,	O
which	O
might	O
result	O
in	O
overfitting	O
.	O
We	O
also	O
run	O
the	O
ablations	Method
of	O
our	O
single	O
model	O
on	O
SQuAD	Material
dev	O
set	O
to	O
evaluate	O
the	O
individual	O
contribution	O
.	O
As	O
Figure	O
7	O
shows	O
,	O
both	O
syntactic	O
embeddings	O
and	O
semantic	O
embeddings	O
contribute	O
towards	O
the	O
model	O
’s	O
performance	O
and	O
the	O
POS	Material
tags	O
seem	O
to	O
be	O
more	O
important	O
.	O
The	O
reason	O
may	O
be	O
that	O
the	O
number	O
of	O
POS	Material
tags	O
is	O
larger	O
than	O
that	O
of	O
NER	Task
tags	O
so	O
the	O
embedding	Method
is	O
easier	O
to	O
train	O
.	O
For	O
the	O
full	Task
-	Task
orientation	Task
matching	Task
,	O
we	O
remove	O
each	O
kind	O
of	O
attention	O
vector	O
respectively	O
and	O
the	O
linear	Method
function	Method
can	O
handle	O
any	O
two	O
of	O
the	O
rest	O
hierarchical	O
attention	O
vectors	O
.	O
For	O
ablating	Task
integral	Task
query	Task
matching	Task
,	O
the	O
result	O
drops	O
about	O
2	O
%	O
on	O
both	O
metrics	O
and	O
it	O
shows	O
that	O
the	O
integral	O
information	O
of	O
query	O
for	O
each	O
word	O
in	O
passage	O
is	O
crucial	O
.	O
The	O
query	Method
-	Method
based	Method
similarity	Method
matching	Method
accounts	O
for	O
about	O
10	O
%	O
performance	O
degradation	O
,	O
which	O
proves	O
the	O
effectiveness	O
of	O
alignment	O
context	O
words	O
against	O
query	O
.	O
For	O
context	Task
-	Task
based	Task
similarity	Task
matching	Task
,	O
we	O
simply	O
took	O
out	O
the	O
from	O
the	O
linear	O
function	O
and	O
it	O
is	O
proved	O
to	O
be	O
contributory	O
to	O
the	O
performance	O
of	O
full	Task
-	Task
orientation	Task
matching	Task
.	O
section	O
:	O
Related	O
Work	O
subsection	O
:	O
Machine	O
Reading	O
Comprehension	O
Dataset	O
.	O
Several	O
benchmark	O
datasets	O
play	O
an	O
important	O
role	O
in	O
progress	O
of	O
machine	Task
comprehension	Task
task	Task
and	Task
question	Task
answering	Task
research	Task
in	O
recent	O
years	O
.	O
MCTest	Method
is	O
one	O
of	O
the	O
famous	O
and	O
high	O
quality	O
datasets	O
.	O
There	O
are	O
660	O
fictional	O
stories	O
and	O
4	O
multiple	O
choice	O
questions	O
per	O
story	O
contained	O
in	O
it	O
,	O
and	O
the	O
labels	O
are	O
all	O
made	O
by	O
humans	O
.	O
Researchers	O
also	O
released	O
cloze	O
-	O
style	O
datasets	O
.	O
However	O
,	O
these	O
datasets	O
are	O
either	O
not	O
large	O
enough	O
to	O
support	O
deep	Method
neural	Method
network	Method
models	Method
or	O
too	O
easy	O
to	O
challenge	O
natural	O
language	O
.	O
Recently	O
,	O
released	O
the	O
Stanford	Material
Question	Material
Answering	Material
dataset	Material
(	O
SQuAD	Material
)	O
,	O
which	O
is	O
almost	O
two	O
orders	O
of	O
magnitude	O
larger	O
than	O
all	O
previous	O
hand	O
-	O
annotated	O
datasets	O
.	O
Moreover	O
,	O
this	O
dataset	O
consists	O
100	O
,	O
000	O
+	O
questions	O
posed	O
by	O
crowdworkers	O
on	O
a	O
set	O
of	O
Wikipedia	O
articles	O
,	O
where	O
the	O
answer	O
to	O
each	O
question	O
is	O
a	O
segment	O
of	O
text	O
from	O
the	O
corresponding	O
passage	O
,	O
rather	O
than	O
a	O
limited	O
set	O
of	O
multiple	O
choices	O
or	O
entities	O
.	O
TriviaQA	Material
is	O
also	O
a	O
large	O
and	O
high	O
quality	O
dataset	O
,	O
and	O
the	O
crucial	O
difference	O
between	O
TriviaQA	Material
and	O
SQuAD	Material
is	O
that	O
TriviaQA	Material
questions	O
have	O
not	O
been	O
crowdsourced	O
from	O
pre	O
-	O
selected	O
passages	O
.	O
subsection	O
:	O
Attention	Method
Based	Method
Models	Method
for	O
Machine	Task
Reading	Task
Many	O
works	O
are	O
based	O
on	O
the	O
task	O
of	O
machine	Task
reading	Task
comprehension	Task
,	O
and	O
attention	Method
mechanism	Method
have	O
been	O
particularly	O
successful	O
.	O
present	O
a	O
coattention	Method
encoder	Method
and	O
dynamic	Method
decoder	Method
to	O
locate	O
the	O
answer	O
.	O
propose	O
a	O
two	O
side	Method
attention	Method
mechanism	Method
to	O
compute	O
the	O
matching	Task
between	O
the	O
passage	O
and	O
query	O
.	O
match	O
the	O
passage	O
and	O
query	O
from	O
several	O
perspectives	O
and	O
predict	O
the	O
answer	O
by	O
globally	O
normalizing	Method
probability	Method
distributions	Method
.	O
propose	O
a	O
bi	Method
-	Method
directional	Method
attention	Method
flow	Method
to	O
achieve	O
a	O
query	Task
-	Task
aware	Task
context	Task
representation	Task
.	O
propose	O
self	Method
-	Method
aware	Method
representation	Method
and	O
multi	Method
-	Method
hop	Method
query	Method
-	Method
sensitive	Method
pointer	Method
to	O
predict	O
the	O
answer	O
span	O
.	O
propose	O
iterarively	O
inferring	O
the	O
answer	O
with	O
a	O
dynamic	O
number	O
of	O
steps	O
trained	O
with	O
reinforcement	Method
learning	Method
.	O
employ	O
gated	Method
self	Method
-	Method
matching	Method
attention	Method
to	O
obtain	O
the	O
relation	O
between	O
the	O
question	O
and	O
passage	O
.	O
Our	O
MEMEN	Method
construct	O
a	O
hierarchical	Method
orientation	Method
attention	Method
mechanism	Method
to	O
get	O
a	O
wider	O
match	O
while	O
applying	O
memory	Method
network	Method
for	O
deeper	Task
understand	Task
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
MEMEN	Method
for	O
Machine	Task
comprehension	Task
style	Task
question	Task
answering	Task
.	O
We	O
propose	O
the	O
multi	Method
-	Method
layer	Method
embedding	Method
to	O
encode	O
the	O
document	O
and	O
the	O
memory	Method
network	Method
of	Method
full	Method
-	Method
orientation	Method
matching	Method
to	O
obtain	O
the	O
interaction	O
of	O
the	O
context	O
and	O
query	O
.	O
The	O
experimental	O
evaluation	O
shows	O
that	O
our	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
TriviaQA	Material
dataset	Material
and	O
competitive	O
result	O
in	O
SQuAD	Material
.	O
Moreover	O
,	O
the	O
ablations	Method
and	Method
hops	Method
analysis	Method
demonstrate	O
the	O
importance	O
of	O
every	O
part	O
of	O
the	O
hierarchical	O
attention	O
vectors	O
and	O
the	O
benefit	O
of	O
multi	O
-	O
hops	O
in	O
memory	Task
network	Task
.	O
For	O
future	O
work	O
,	O
we	O
will	O
focus	O
on	O
question	Method
generative	Method
method	Method
and	O
sentence	Task
ranking	Task
in	O
machine	Task
reading	Task
tasks	Task
.	O
bibliography	O
:	O
References	O
