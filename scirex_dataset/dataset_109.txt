Model Method
- Method
Agnostic Method
Meta Method
- Method
Learning Method
for O
Fast Task
Adaptation Task
of Task
Deep Task
Networks Task
section O
: O
Abstract O
We O
propose O
an O
algorithm O
for O
meta Task
- Task
learning Task
that O
is O
model Task
- Task
agnostic Task
, O
in O
the O
sense O
that O
it O
is O
compatible O
with O
any O
model O
trained O
with O
gradient Method
descent Method
and O
applicable O
to O
a O
variety O
of O
different O
learning Task
problems Task
, O
including O
classification Task
, O
regression Task
, O
and O
reinforcement Task
learning Task
. O
The O
goal O
of O
meta Method
- Method
learning Method
is O
to O
train O
a O
model O
on O
a O
variety O
of O
learning Task
tasks Task
, O
such O
that O
it O
can O
solve O
new O
learning Task
tasks Task
using O
only O
a O
small O
number O
of O
training O
samples O
. O
In O
our O
approach O
, O
the O
parameters O
of O
the O
model O
are O
explicitly O
trained O
such O
that O
a O
small O
number O
of O
gradient O
steps O
with O
a O
small O
amount O
of O
training O
data O
from O
a O
new O
task O
will O
produce O
good O
generalization Task
performance O
on O
that O
task O
. O
In O
effect O
, O
our O
method O
trains O
the O
model O
to O
be O
easy O
to O
fine O
- O
tune O
. O
We O
demonstrate O
that O
this O
approach O
leads O
to O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
two O
fewshot Task
image Task
classification Task
benchmarks Task
, O
produces O
good O
results O
on O
few Task
- Task
shot Task
regression Task
, O
and O
accelerates O
fine Method
- Method
tuning Method
for O
policy Method
gradient Method
reinforcement Method
learning Method
with O
neural Method
network Method
policies Method
. O
section O
: O
Introduction O
Learning Task
quickly Task
is O
a O
hallmark O
of O
human Task
intelligence Task
, O
whether O
it O
involves O
recognizing O
objects O
from O
a O
few O
examples O
or O
quickly O
learning O
new O
skills O
after O
just O
minutes O
of O
experience O
. O
Our O
artificial Method
agents Method
should O
be O
able O
to O
do O
the O
same O
, O
learning O
and O
adapting O
quickly O
from O
only O
a O
few O
examples O
, O
and O
continuing O
to O
adapt O
as O
more O
data O
becomes O
available O
. O
This O
kind O
of O
fast Task
and Task
flexible Task
learning Task
is O
challenging O
, O
since O
the O
agent O
must O
integrate O
its O
prior O
experience O
with O
a O
small O
amount O
of O
new O
information O
, O
while O
avoiding O
overfitting O
to O
the O
new O
data O
. O
Furthermore O
, O
the O
form O
of O
prior O
experience O
and O
new O
data O
will O
depend O
on O
the O
task O
. O
As O
such O
, O
for O
the O
greatest O
applicability O
, O
the O
mechanism O
for O
learning Task
to O
learn O
( O
or O
meta Task
- Task
learning Task
) O
should O
be O
general O
to O
the O
task O
and O
1 O
University O
of O
California O
, O
Berkeley O
2 O
OpenAI O
. O
Correspondence O
to O
: O
Chelsea O
Finn O
< O
cbfinn@eecs.berkeley.edu>. O
section O
: O
Proceedings O
of O
the O
34 O
th O
International O
Conference O
on O
Machine Task
Learning Task
, O
Sydney O
, O
[ O
reference O
][ O
reference O
] O
by O
the O
author O
( O
s O
) O
. O
the O
form O
of O
computation O
required O
to O
complete O
the O
task O
. O
In O
this O
work O
, O
we O
propose O
a O
meta Method
- Method
learning Method
algorithm Method
that O
is O
general O
and O
model O
- O
agnostic O
, O
in O
the O
sense O
that O
it O
can O
be O
directly O
applied O
to O
any O
learning Task
problem Task
and O
model O
that O
is O
trained O
with O
a O
gradient Method
descent Method
procedure Method
. O
Our O
focus O
is O
on O
deep Method
neural Method
network Method
models Method
, O
but O
we O
illustrate O
how O
our O
approach O
can O
easily O
handle O
different O
architectures O
and O
different O
problem O
settings O
, O
including O
classification Task
, O
regression Task
, O
and O
policy Method
gradient Method
reinforcement Method
learning Method
, O
with O
minimal O
modification O
. O
In O
meta Method
- Method
learning Method
, O
the O
goal O
of O
the O
trained O
model O
is O
to O
quickly O
learn O
a O
new O
task O
from O
a O
small O
amount O
of O
new O
data O
, O
and O
the O
model O
is O
trained O
by O
the O
meta Method
- Method
learner Method
to O
be O
able O
to O
learn O
on O
a O
large O
number O
of O
different O
tasks O
. O
The O
key O
idea O
underlying O
our O
method O
is O
to O
train O
the O
model O
's O
initial O
parameters O
such O
that O
the O
model O
has O
maximal O
performance O
on O
a O
new O
task O
after O
the O
parameters O
have O
been O
updated O
through O
one O
or O
more O
gradient O
steps O
computed O
with O
a O
small O
amount O
of O
data O
from O
that O
new O
task O
. O
Unlike O
prior O
meta Method
- Method
learning Method
methods Method
that O
learn O
an O
update O
function O
or O
learning Method
rule Method
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
, O
our O
algorithm O
does O
not O
expand O
the O
number O
of O
learned O
parameters O
nor O
place O
constraints O
on O
the O
model O
architecture O
( O
e.g. O
by O
requiring O
a O
recurrent Method
model Method
[ O
reference O
] O
or O
a O
Siamese Method
network Method
[ O
reference O
] O
) O
, O
and O
it O
can O
be O
readily O
combined O
with O
fully Method
connected Method
, Method
convolutional Method
, Method
or Method
recurrent Method
neural Method
networks Method
. O
It O
can O
also O
be O
used O
with O
a O
variety O
of O
loss O
functions O
, O
including O
differentiable O
supervised O
losses O
and O
nondifferentiable O
reinforcement O
learning O
objectives O
. O
The O
process O
of O
training O
a O
model O
's O
parameters O
such O
that O
a O
few O
gradient O
steps O
, O
or O
even O
a O
single O
gradient O
step O
, O
can O
produce O
good O
results O
on O
a O
new O
task O
can O
be O
viewed O
from O
a O
feature Method
learning Method
standpoint Method
as O
building O
an O
internal Method
representation Method
that O
is O
broadly O
suitable O
for O
many O
tasks O
. O
If O
the O
internal Method
representation Method
is O
suitable O
to O
many O
tasks O
, O
simply O
fine O
- O
tuning O
the O
parameters O
slightly O
( O
e.g. O
by O
primarily O
modifying O
the O
top O
layer O
weights O
in O
a O
feedforward Method
model Method
) O
can O
produce O
good O
results O
. O
In O
effect O
, O
our O
procedure O
optimizes O
for O
models O
that O
are O
easy O
and O
fast O
to O
fine O
- O
tune O
, O
allowing O
the O
adaptation O
to O
happen O
in O
the O
right O
space O
for O
fast Task
learning Task
. O
From O
a O
dynamical Method
systems Method
standpoint Method
, O
our O
learning Method
process Method
can O
be O
viewed O
as O
maximizing O
the O
sensitivity O
of O
the O
loss O
functions O
of O
new O
tasks O
with O
respect O
to O
the O
parameters O
: O
when O
the O
sensitivity O
is O
high O
, O
small O
local O
changes O
to O
the O
parameters O
can O
lead O
to O
large O
improvements O
in O
the O
task Metric
loss Metric
. O
The O
primary O
contribution O
of O
this O
work O
is O
a O
simple O
modeland Method
task Method
- Method
agnostic Method
algorithm Method
for O
meta Task
- Task
learning Task
that O
trains O
a O
model O
's O
parameters O
such O
that O
a O
small O
number O
of O
gradient O
updates O
will O
lead O
to O
fast Task
learning Task
on O
a O
new O
task O
. O
We O
demonstrate O
the O
algorithm O
on O
different O
model O
types O
, O
including O
fully Method
connected Method
and Method
convolutional Method
networks Method
, O
and O
in O
several O
distinct O
domains O
, O
including O
few Task
- Task
shot Task
regression Task
, O
image Task
classification Task
, O
and O
reinforcement Task
learning Task
. O
Our O
evaluation O
shows O
that O
our O
meta Method
- Method
learning Method
algorithm Method
compares O
favorably O
to O
state O
- O
of O
- O
the O
- O
art O
one Method
- Method
shot Method
learning Method
methods Method
designed O
specifically O
for O
supervised Task
classification Task
, O
while O
using O
fewer O
parameters O
, O
but O
that O
it O
can O
also O
be O
readily O
applied O
to O
regression Task
and O
can O
accelerate O
reinforcement Task
learning Task
in O
the O
presence O
of O
task O
variability O
, O
substantially O
outperforming O
direct Method
pretraining Method
as O
initialization Task
. O
section O
: O
Model Method
- Method
Agnostic Method
Meta Method
- Method
Learning Method
We O
aim O
to O
train O
models O
that O
can O
achieve O
rapid Task
adaptation Task
, O
a O
problem O
setting O
that O
is O
often O
formalized O
as O
few Method
- Method
shot Method
learning Method
. O
In O
this O
section O
, O
we O
will O
define O
the O
problem O
setup O
and O
present O
the O
general O
form O
of O
our O
algorithm O
. O
section O
: O
Meta Task
- Task
Learning Task
Problem Task
Set O
- O
Up O
The O
goal O
of O
few Method
- Method
shot Method
meta Method
- Method
learning Method
is O
to O
train O
a O
model O
that O
can O
quickly O
adapt O
to O
a O
new O
task O
using O
only O
a O
few O
datapoints O
and O
training O
iterations O
. O
To O
accomplish O
this O
, O
the O
model O
or O
learner Method
is O
trained O
during O
a O
meta Method
- Method
learning Method
phase Method
on O
a O
set O
of O
tasks O
, O
such O
that O
the O
trained O
model O
can O
quickly O
adapt O
to O
new O
tasks O
using O
only O
a O
small O
number O
of O
examples O
or O
trials O
. O
In O
effect O
, O
the O
meta Task
- Task
learning Task
problem Task
treats O
entire O
tasks O
as O
training O
examples O
. O
In O
this O
section O
, O
we O
formalize O
this O
metalearning Task
problem Task
setting Task
in O
a O
general O
manner O
, O
including O
brief O
examples O
of O
different O
learning Task
domains Task
. O
We O
will O
discuss O
two O
different O
learning Task
domains Task
in O
detail O
in O
Section O
3 O
. O
We O
consider O
a O
model O
, O
denoted O
f Method
, O
that O
maps O
observations O
x O
to O
outputs O
a. O
During O
meta Method
- Method
learning Method
, O
the O
model O
is O
trained O
to O
be O
able O
to O
adapt O
to O
a O
large O
or O
infinite O
number O
of O
tasks O
. O
Since O
we O
would O
like O
to O
apply O
our O
framework O
to O
a O
variety O
of O
learning Task
problems Task
, O
from O
classification Task
to O
reinforcement Task
learning Task
, O
we O
introduce O
a O
generic O
notion O
of O
a O
learning Task
task Task
below O
. O
Formally O
, O
each O
task O
T O
= O
{ O
L O
( O
x O
1 O
, O
a O
1 O
, O
. O
. O
. O
, O
x O
H O
, O
a O
H O
) O
, O
q O
( O
x O
1 O
) O
, O
q O
( O
x O
t O
+ O
1 O
|x O
t O
, O
a O
t O
) O
, O
H O
} O
consists O
of O
a O
loss Method
function Method
L Method
, O
a O
distribution O
over O
initial O
observations O
q O
( O
x O
1 O
) O
, O
a O
transition Method
distribution Method
q O
( O
x O
t O
+ O
1 O
|x O
t O
, O
a O
t O
) O
, O
and O
an O
episode O
length O
H. O
In O
i.i.d O
. O
supervised Task
learning Task
problems Task
, O
the O
length O
H O
= O
1 O
. O
The O
model O
may O
generate O
samples O
of O
length O
H O
by O
choosing O
an O
output O
a O
t O
at O
each O
time O
t. O
The O
loss O
L O
( O
x O
1 O
, O
a O
1 O
, O
. O
. O
. O
, O
x O
H O
, O
a O
H O
) O
→ O
R Method
, O
provides O
task O
- O
specific O
feedback O
, O
which O
might O
be O
in O
the O
form O
of O
a O
misclassification O
loss O
or O
a O
cost O
function O
in O
a O
Markov Method
decision Method
process Method
. O
Figure O
1 O
. O
Diagram O
of O
our O
model Method
- Method
agnostic Method
meta Method
- Method
learning Method
algorithm Method
( O
MAML Method
) O
, O
which O
optimizes O
for O
a O
representation Method
θ Method
that O
can O
quickly O
adapt O
to O
new O
tasks O
. O
In O
our O
meta Task
- Task
learning Task
scenario Task
, O
we O
consider O
a O
distribution O
over O
tasks O
p O
( O
T O
) O
that O
we O
want O
our O
model O
to O
be O
able O
to O
adapt O
to O
. O
In O
the O
K Task
- Task
shot Task
learning Task
setting Task
, O
the O
model O
is O
trained O
to O
learn O
a O
new O
task O
T O
i O
drawn O
from O
p O
( O
T O
) O
from O
only O
K O
samples O
drawn O
from O
q O
i O
and O
feedback O
L O
Ti O
generated O
by O
T O
i O
. O
During O
meta Task
- Task
training Task
, O
a O
task O
T O
i O
is O
sampled O
from O
p O
( O
T O
) O
, O
the O
model O
is O
trained O
with O
K O
samples O
and O
feedback O
from O
the O
corresponding O
loss O
L O
Ti O
from O
T O
i O
, O
and O
then O
tested O
on O
new O
samples O
from O
T O
i O
. O
The O
model O
f O
is O
then O
improved O
by O
considering O
how O
the O
test O
error O
on O
new O
data O
from O
q O
i O
changes O
with O
respect O
to O
the O
parameters O
. O
In O
effect O
, O
the O
test Metric
error Metric
on O
sampled Task
tasks Task
T O
i O
serves O
as O
the O
training Metric
error Metric
of O
the O
meta Method
- Method
learning Method
process Method
. O
At O
the O
end O
of O
meta Task
- Task
training Task
, O
new O
tasks O
are O
sampled O
from O
p O
( O
T O
) O
, O
and O
meta Metric
- Metric
performance Metric
is O
measured O
by O
the O
model O
's O
performance O
after O
learning O
from O
K O
samples O
. O
Generally O
, O
tasks O
used O
for O
meta Task
- Task
testing Task
are O
held O
out O
during O
meta Task
- Task
training Task
. O
section O
: O
A O
Model Method
- Method
Agnostic Method
Meta Method
- Method
Learning Method
Algorithm O
In O
contrast O
to O
prior O
work O
, O
which O
has O
sought O
to O
train O
recurrent Method
neural Method
networks Method
that O
ingest O
entire O
datasets O
[ O
reference O
][ O
reference O
] O
or O
feature O
embeddings O
that O
can O
be O
combined O
with O
nonparametric Method
methods Method
at O
test O
time O
[ O
reference O
][ O
reference O
] O
, O
we O
propose O
a O
method O
that O
can O
learn O
the O
parameters O
of O
any O
standard O
model O
via O
meta Method
- Method
learning Method
in O
such O
a O
way O
as O
to O
prepare O
that O
model O
for O
fast Task
adaptation Task
. O
The O
intuition O
behind O
this O
approach O
is O
that O
some O
internal Method
representations Method
are O
more O
transferrable O
than O
others O
. O
For O
example O
, O
a O
neural Method
network Method
might O
learn O
internal O
features O
that O
are O
broadly O
applicable O
to O
all O
tasks O
in O
p O
( O
T O
) O
, O
rather O
than O
a O
single O
individual O
task O
. O
How O
can O
we O
encourage O
the O
emergence O
of O
such O
general Method
- Method
purpose Method
representations Method
? O
We O
take O
an O
explicit O
approach O
to O
this O
problem O
: O
since O
the O
model O
will O
be O
fine O
- O
tuned O
using O
a O
gradient Method
- Method
based Method
learning Method
rule Method
on O
a O
new O
task O
, O
we O
will O
aim O
to O
learn O
a O
model O
in O
such O
a O
way O
that O
this O
gradient Method
- Method
based Method
learning Method
rule Method
can O
make O
rapid O
progress O
on O
new O
tasks O
drawn O
from O
p O
( O
T O
) O
, O
without O
overfitting O
. O
In O
effect O
, O
we O
will O
aim O
to O
find O
model O
parameters O
that O
are O
sensitive O
to O
changes O
in O
the O
task O
, O
such O
that O
small O
changes O
in O
the O
parameters O
will O
produce O
large O
improvements O
on O
the O
loss Metric
function Metric
of O
any O
task O
drawn O
from O
p O
( O
T O
) O
, O
when O
altered O
in O
the O
direction O
of O
the O
gradient O
of O
that O
loss O
( O
see O
Figure O
1 O
) O
. O
We O
Compute O
adapted O
parameters O
with O
gradient Method
descent Method
: O
end O
for O
8 O
: O
end O
while O
make O
no O
assumption O
on O
the O
form O
of O
the O
model O
, O
other O
than O
to O
assume O
that O
it O
is O
parametrized O
by O
some O
parameter O
vector O
θ O
, O
and O
that O
the O
loss O
function O
is O
smooth O
enough O
in O
θ O
that O
we O
can O
use O
gradient Method
- Method
based Method
learning Method
techniques Method
. O
Formally O
, O
we O
consider O
a O
model O
represented O
by O
a O
parametrized Method
function Method
f Method
θ Method
with O
parameters O
θ O
. O
When O
adapting O
to O
a O
new O
task O
T O
i O
, O
the O
model O
's O
parameters O
θ O
become O
θ O
i O
. O
In O
our O
method O
, O
the O
updated O
parameter O
vector O
θ O
i O
is O
computed O
using O
one O
or O
more O
gradient Method
descent Method
updates Method
on O
task O
T O
i O
. O
For O
example O
, O
when O
using O
one O
gradient Method
update Method
, O
. O
The O
step O
size O
α O
may O
be O
fixed O
as O
a O
hyperparameter O
or O
metalearned O
. O
For O
simplicity O
of O
notation O
, O
we O
will O
consider O
one O
gradient Method
update Method
for O
the O
rest O
of O
this O
section O
, O
but O
using O
multiple Method
gradient Method
updates Method
is O
a O
straightforward O
extension O
. O
The O
model O
parameters O
are O
trained O
by O
optimizing O
for O
the O
performance O
of O
f O
θ O
i O
with O
respect O
to O
θ O
across O
tasks O
sampled O
from O
p O
( O
T O
) O
. O
More O
concretely O
, O
the O
meta Metric
- Metric
objective Metric
is O
as O
follows O
: O
Note O
that O
the O
meta Method
- Method
optimization Method
is O
performed O
over O
the O
model O
parameters O
θ O
, O
whereas O
the O
objective O
is O
computed O
using O
the O
updated O
model O
parameters O
θ O
. O
In O
effect O
, O
our O
proposed O
method O
aims O
to O
optimize O
the O
model O
parameters O
such O
that O
one O
or O
a O
small O
number O
of O
gradient O
steps O
on O
a O
new O
task O
will O
produce O
maximally O
effective O
behavior O
on O
that O
task O
. O
The O
meta Task
- Task
optimization Task
across O
tasks O
is O
performed O
via O
stochastic Method
gradient Method
descent Method
( O
SGD Method
) Method
, O
such O
that O
the O
model O
parameters O
θ O
are O
updated O
as O
follows O
: O
where O
β O
is O
the O
meta O
step O
size O
. O
The O
full O
algorithm O
, O
in O
the O
general O
case O
, O
is O
outlined O
in O
Algorithm O
1 O
. O
The O
MAML Method
meta O
- O
gradient O
update O
involves O
a O
gradient O
through O
a O
gradient O
. O
Computationally O
, O
this O
requires O
an O
additional O
backward O
pass O
through O
f O
to O
compute O
Hessian O
- O
vector O
products O
, O
which O
is O
supported O
by O
standard O
deep Method
learning Method
libraries Method
such O
as O
TensorFlow Method
[ O
reference O
] O
. O
In O
our O
experiments O
, O
we O
also O
include O
a O
comparison O
to O
dropping O
this O
backward Method
pass Method
and O
using O
a O
first Method
- Method
order Method
approximation Method
, O
which O
we O
discuss O
in O
Section O
5.2 O
. O
section O
: O
Species O
of O
MAML Method
In O
this O
section O
, O
we O
discuss O
specific O
instantiations O
of O
our O
meta Method
- Method
learning Method
algorithm Method
for O
supervised Task
learning Task
and O
reinforcement Task
learning Task
. O
The O
domains O
differ O
in O
the O
form O
of O
loss O
function O
and O
in O
how O
data O
is O
generated O
by O
the O
task O
and O
presented O
to O
the O
model O
, O
but O
the O
same O
basic O
adaptation Method
mechanism Method
can O
be O
applied O
in O
both O
cases O
. O
section O
: O
Supervised Task
Regression Task
and O
Classification Task
Few Method
- Method
shot Method
learning Method
is O
well O
- O
studied O
in O
the O
domain O
of O
supervised Task
tasks Task
, O
where O
the O
goal O
is O
to O
learn O
a O
new O
function O
from O
only O
a O
few O
input O
/ O
output O
pairs O
for O
that O
task O
, O
using O
prior O
data O
from O
similar O
tasks O
for O
meta Task
- Task
learning Task
. O
For O
example O
, O
the O
goal O
might O
be O
to O
classify O
images O
of O
a O
Segway O
after O
seeing O
only O
one O
or O
a O
few O
examples O
of O
a O
Segway O
, O
with O
a O
model O
that O
has O
previously O
seen O
many O
other O
types O
of O
objects O
. O
Likewise O
, O
in O
few Task
- Task
shot Task
regression Task
, O
the O
goal O
is O
to O
predict O
the O
outputs O
of O
a O
continuous O
- O
valued O
function O
from O
only O
a O
few O
datapoints O
sampled O
from O
that O
function O
, O
after O
training O
on O
many O
functions O
with O
similar O
statistical O
properties O
. O
To O
formalize O
the O
supervised Task
regression Task
and Task
classification Task
problems Task
in O
the O
context O
of O
the O
meta Task
- Task
learning Task
definitions Task
in O
Section O
2.1 O
, O
we O
can O
define O
the O
horizon O
H O
= O
1 O
and O
drop O
the O
timestep O
subscript O
on O
x O
t O
, O
since O
the O
model O
accepts O
a O
single O
input O
and O
produces O
a O
single O
output O
, O
rather O
than O
a O
sequence O
of O
inputs O
and O
outputs O
. O
The O
task O
T O
i O
generates O
K O
i.i.d O
. O
observations O
x O
from O
q O
i O
, O
and O
the O
task Metric
loss Metric
is O
represented O
by O
the O
error O
between O
the O
model O
's O
output O
for O
x O
and O
the O
corresponding O
target O
values O
y O
for O
that O
observation O
and O
task O
. O
Two O
common O
loss Method
functions Method
used O
for O
supervised Task
classification Task
and Task
regression Task
are O
cross Metric
- Metric
entropy Metric
and O
mean Metric
- Metric
squared Metric
error Metric
( O
MSE Metric
) O
, O
which O
we O
will O
describe O
below O
; O
though O
, O
other O
supervised Method
loss Method
functions Method
may O
be O
used O
as O
well O
. O
For O
regression Task
tasks Task
using O
mean Metric
- Metric
squared Metric
error Metric
, O
the O
loss O
takes O
the O
form O
: O
where O
x O
( O
j O
) O
, O
y O
( O
j O
) O
are O
an O
input O
/ O
output O
pair O
sampled O
from O
task O
T O
i O
. O
In O
K Task
- Task
shot Task
regression Task
tasks Task
, O
K O
input O
/ O
output O
pairs O
are O
provided O
for O
learning O
for O
each O
task O
. O
Similarly O
, O
for O
discrete Task
classification Task
tasks Task
with O
a O
crossentropy O
loss O
, O
the O
loss O
takes O
the O
form O
: O
section O
: O
Algorithm O
2 O
MAML Method
for O
Few Task
- Task
Shot Task
Supervised Task
Learning Task
Require O
: O
p O
( O
T O
) O
: O
distribution O
over O
tasks O
Require O
: O
α O
, O
β O
: O
step O
size O
hyperparameters O
1 O
: O
randomly O
initialize O
θ O
2 O
: O
while O
not O
done O
do O
3 O
: O
Sample O
batch O
of O
tasks O
Ti O
∼ O
p O
( O
T O
) O
4 O
: O
for O
all O
Ti O
do O
5 O
: O
Evaluate O
∇ O
θ O
LT O
i O
( O
f O
θ O
) O
using O
D O
and O
LT O
i O
in O
Equation O
( O
2 O
) O
or O
( O
3 O
) O
7 O
: O
Compute O
adapted O
parameters O
with O
gradient Method
descent Method
: O
Sample O
datapoints O
D O
i O
= O
{ O
x O
( O
j O
) O
, O
y O
( O
j O
) O
} O
from O
Ti O
for O
the O
meta Task
- Task
update Task
9 O
: O
end O
for O
10 O
: O
Update O
and O
LT O
i O
in O
Equation O
2 O
or O
3 O
11 O
: O
end O
while O
According O
to O
the O
conventional O
terminology O
, O
K Task
- Task
shot Task
classification Task
tasks Task
use O
K O
input O
/ O
output O
pairs O
from O
each O
class O
, O
for O
a O
total O
of O
N O
K O
data O
points O
for O
N Task
- Task
way Task
classification Task
. O
Given O
a O
distribution O
over O
tasks O
p O
( O
T O
i O
) O
, O
these O
loss Method
functions Method
can O
be O
directly O
inserted O
into O
the O
equations O
in O
Section O
2.2 O
to O
perform O
meta Task
- Task
learning Task
, O
as O
detailed O
in O
Algorithm O
2 O
. O
section O
: O
Reinforcement Method
Learning Method
In O
reinforcement Task
learning Task
( O
RL Task
) O
, O
the O
goal O
of O
few Task
- Task
shot Task
metalearning Task
is O
to O
enable O
an O
agent O
to O
quickly O
acquire O
a O
policy O
for O
a O
new O
test O
task O
using O
only O
a O
small O
amount O
of O
experience O
in O
the O
test O
setting O
. O
A O
new O
task O
might O
involve O
achieving O
a O
new O
goal O
or O
succeeding O
on O
a O
previously O
trained O
goal O
in O
a O
new O
environment O
. O
For O
example O
, O
an O
agent O
might O
learn O
to O
quickly O
figure O
out O
how O
to O
navigate O
mazes O
so O
that O
, O
when O
faced O
with O
a O
new O
maze O
, O
it O
can O
determine O
how O
to O
reliably O
reach O
the O
exit O
with O
only O
a O
few O
samples O
. O
In O
this O
section O
, O
we O
will O
discuss O
how O
MAML Method
can O
be O
applied O
to O
meta Method
- Method
learning Method
for O
RL Task
. O
Each O
RL Task
task Task
T O
i O
contains O
an O
initial O
state O
distribution O
q O
i O
( O
x O
1 O
) O
and O
a O
transition Method
distribution Method
q O
i O
( O
x O
t O
+ O
1 O
|x O
t O
, O
a O
t O
) O
, O
and O
the O
loss O
L O
Ti O
corresponds O
to O
the O
( O
negative O
) O
reward O
function O
R. O
The O
entire O
task O
is O
therefore O
a O
Markov Method
decision Method
process Method
( Method
MDP Method
) Method
with O
horizon Method
H Method
, O
where O
the O
learner O
is O
allowed O
to O
query O
a O
limited O
number O
of O
sample O
trajectories O
for O
few Method
- Method
shot Method
learning Method
. O
Any O
aspect O
of O
the O
MDP Method
may O
change O
across O
tasks O
in O
p O
( O
T O
) O
. O
The O
model O
being O
learned O
, O
f Method
θ Method
, O
is O
a O
policy Method
that O
maps O
from O
states O
x O
t O
to O
a O
distribution O
over O
actions O
a O
t O
at O
each O
timestep O
t O
∈ O
{ O
1 O
, O
... O
, O
H}. O
The O
loss O
for O
task O
T O
i O
and O
model Method
f Method
φ Method
takes O
the O
form O
In O
K Task
- Task
shot Task
reinforcement Task
learning Task
, O
K O
rollouts O
from O
f O
θ O
and O
task O
T O
i O
, O
( O
x O
1 O
, O
a O
1 O
, O
... O
x O
H O
) O
, O
and O
the O
corresponding O
rewards O
R O
( O
x O
t O
, O
a O
t O
) O
, O
may O
be O
used O
for O
adaptation Task
on O
a O
new O
task O
T O
i O
. O
section O
: O
Algorithm O
3 O
MAML Method
for O
Reinforcement Task
Learning Task
Require O
: O
p O
( O
T O
) O
: O
distribution O
over O
tasks O
Require O
: O
α O
, O
β O
: O
step O
size O
hyperparameters O
1 O
: O
randomly O
initialize O
θ O
2 O
: O
while O
not O
done O
do O
3 O
: O
Sample O
batch O
of O
tasks O
Ti O
∼ O
p O
( O
T O
) O
4 O
: O
for O
all O
Ti O
do O
5 O
: O
Compute O
adapted O
parameters O
with O
gradient Method
descent Method
: O
Sample O
trajectories O
end O
for O
10 O
: O
Update O
Since O
the O
expected O
reward O
is O
generally O
not O
differentiable O
due O
to O
unknown O
dynamics O
, O
we O
use O
policy Method
gradient Method
methods Method
to O
estimate O
the O
gradient O
both O
for O
the O
model Method
gradient Method
update Method
( Method
s Method
) O
and O
the O
meta Task
- Task
optimization Task
. O
Since O
policy Method
gradients Method
are O
an O
on Method
- Method
policy Method
algorithm Method
, O
each O
additional O
gradient Method
step Method
during O
the O
adaptation Task
of Task
f Task
θ Task
requires O
new O
samples O
from O
the O
current O
policy O
f O
θ O
i O
. O
We O
detail O
the O
algorithm O
in O
Algorithm O
3 O
. O
This O
algorithm O
has O
the O
same O
structure O
as O
Algorithm O
2 O
, O
with O
the O
principal O
difference O
being O
that O
steps O
5 O
and O
8 O
require O
sampling O
trajectories O
from O
the O
environment O
corresponding O
to O
task O
T O
i O
. O
Practical O
implementations O
of O
this O
method O
may O
also O
use O
a O
variety O
of O
improvements O
recently O
proposed O
for O
policy Method
gradient Method
algorithms Method
, O
including O
state Method
or Method
action Method
- Method
dependent Method
baselines Method
and O
trust O
regions O
[ O
reference O
] O
. O
section O
: O
Related O
Work O
The O
method O
that O
we O
propose O
in O
this O
paper O
addresses O
the O
general O
problem O
of O
meta Task
- Task
learning Task
[ O
reference O
][ O
reference O
][ O
reference O
] O
, O
which O
includes O
few Method
- Method
shot Method
learning Method
. O
A O
popular O
approach O
for O
metalearning Task
is O
to O
train O
a O
meta Method
- Method
learner Method
that O
learns O
how O
to O
update O
the O
parameters O
of O
the O
learner Method
's Method
model Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
This O
approach O
has O
been O
applied O
to O
learning Task
to O
optimize O
deep Method
networks Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
, O
as O
well O
as O
for O
learning Task
dynamically Task
changing Task
recurrent Task
networks Task
[ O
reference O
] O
. O
One O
recent O
approach O
learns O
both O
the O
weight Method
initialization Method
and O
the O
optimizer Method
, O
for O
few Task
- Task
shot Task
image Task
recognition Task
[ O
reference O
] O
. O
Unlike O
these O
methods O
, O
the O
MAML Method
learner O
's O
weights O
are O
updated O
using O
the O
gradient O
, O
rather O
than O
a O
learned Method
update Method
; O
our O
method O
does O
not O
introduce O
additional O
parameters O
for O
meta Method
- Method
learning Method
nor O
require O
a O
particular O
learner Method
architecture Method
. O
Few Method
- Method
shot Method
learning Method
methods Method
have O
also O
been O
developed O
for O
specific O
tasks O
such O
as O
generative Method
modeling Method
[ O
reference O
][ O
reference O
] O
and O
image Task
recognition Task
[ O
reference O
] O
. O
One O
successful O
approach O
for O
few Task
- Task
shot Task
classification Task
is O
to O
learn O
to O
compare O
new O
examples O
in O
a O
learned O
metric O
space O
using O
e.g. O
Siamese Method
networks Method
[ O
reference O
] O
or O
recurrence Method
with O
attention Method
mechanisms Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
These O
approaches O
have O
generated O
some O
of O
the O
most O
successful O
results O
, O
but O
are O
difficult O
to O
directly O
extend O
to O
other O
problems O
, O
such O
as O
reinforcement Method
learning Method
. O
Our O
method O
, O
in O
contrast O
, O
is O
agnostic O
to O
the O
form O
of O
the O
model O
and O
to O
the O
particular O
learning Task
task Task
. O
Another O
approach O
to O
meta Task
- Task
learning Task
is O
to O
train O
memoryaugmented Method
models Method
on O
many O
tasks O
, O
where O
the O
recurrent Method
learner Method
is O
trained O
to O
adapt O
to O
new O
tasks O
as O
it O
is O
rolled O
out O
. O
Such O
networks O
have O
been O
applied O
to O
few Task
- Task
shot Task
image Task
recognition Task
[ O
reference O
][ O
reference O
] O
and O
learning O
" O
fast O
" O
reinforcement Method
learning Method
agents Method
[ O
reference O
][ O
reference O
] O
. O
Our O
experiments O
show O
that O
our O
method O
outperforms O
the O
recurrent Method
approach Method
on O
fewshot Task
classification Task
. O
Furthermore O
, O
unlike O
these O
methods O
, O
our O
approach O
simply O
provides O
a O
good O
weight Method
initialization Method
and O
uses O
the O
same O
gradient Method
descent Method
update Method
for O
both O
the O
learner Method
and O
meta Task
- Task
update Task
. O
As O
a O
result O
, O
it O
is O
straightforward O
to O
finetune O
the O
learner Method
for O
additional O
gradient O
steps O
. O
Our O
approach O
is O
also O
related O
to O
methods O
for O
initialization Task
of Task
deep Task
networks Task
. O
In O
computer Task
vision Task
, O
models O
pretrained O
on O
large Task
- Task
scale Task
image Task
classification Task
have O
been O
shown O
to O
learn O
effective O
features O
for O
a O
range O
of O
problems O
[ O
reference O
] O
. O
In O
contrast O
, O
our O
method O
explicitly O
optimizes O
the O
model O
for O
fast O
adaptability Task
, O
allowing O
it O
to O
adapt O
to O
new O
tasks O
with O
only O
a O
few O
examples O
. O
Our O
method O
can O
also O
be O
viewed O
as O
explicitly O
maximizing O
sensitivity O
of O
new O
task O
losses O
to O
the O
model O
parameters O
. O
A O
number O
of O
prior O
works O
have O
explored O
sensitivity Task
in Task
deep Task
networks Task
, O
often O
in O
the O
context O
of O
initialization Task
[ O
reference O
][ O
reference O
] O
. O
Most O
of O
these O
works O
have O
considered O
good O
random Method
initializations Method
, O
though O
a O
number O
of O
papers O
have O
addressed O
datadependent Method
initializers Method
[ O
reference O
][ O
reference O
] O
, O
including O
learned Method
initializations Method
[ O
reference O
][ O
reference O
] O
. O
In O
contrast O
, O
our O
method O
explicitly O
trains O
the O
parameters O
for O
sensitivity O
on O
a O
given O
task O
distribution O
, O
allowing O
for O
extremely O
efficient O
adaptation Task
for O
problems O
such O
as O
K Task
- Task
shot Task
learning Task
and O
rapid Task
reinforcement Task
learning Task
in O
only O
one O
or O
a O
few O
gradient O
steps O
. O
section O
: O
Experimental O
Evaluation O
The O
goal O
of O
our O
experimental O
evaluation O
is O
to O
answer O
the O
following O
questions O
: O
( O
1 O
) O
Can O
MAML Method
enable O
fast Task
learning Task
of Task
new Task
tasks Task
? O
( O
2 O
) O
Can O
MAML Method
be O
used O
for O
meta Task
- Task
learning Task
in O
multiple O
different O
domains O
, O
including O
supervised Task
regression Task
, O
classification Task
, O
and O
reinforcement Task
learning Task
? O
( O
3 O
) O
Can O
a O
model O
learned O
with O
MAML Method
continue O
to O
improve O
with O
additional O
gradient O
updates O
and O
/ O
or O
examples O
? O
All O
of O
the O
meta Task
- Task
learning Task
problems Task
that O
we O
consider O
require O
some O
amount O
of O
adaptation O
to O
new O
tasks O
at O
test O
- O
time O
. O
When O
possible O
, O
we O
compare O
our O
results O
to O
an O
oracle O
that O
receives O
the O
identity O
of O
the O
task O
( O
which O
is O
a O
problem O
- O
dependent O
representation O
) O
as O
an O
additional O
input O
, O
as O
an O
upper O
bound O
on O
the O
performance O
of O
the O
model O
. O
All O
of O
the O
experiments O
were O
performed O
using O
TensorFlow Method
[ O
reference O
] O
, O
which O
allows O
for O
automatic Task
differentiation Task
through O
the O
gradient O
update O
( O
s O
) O
during O
meta Method
- Method
learning Method
. O
The O
code O
is O
available O
online O
1 O
. O
section O
: O
Regression Task
We O
start O
with O
a O
simple O
regression Task
problem Task
that O
illustrates O
the O
basic O
principles O
of O
MAML Method
. O
[ O
reference O
] O
. O
The O
baselines O
are O
likewise O
trained O
with O
Adam Method
. O
To O
evaluate O
performance O
, O
we O
finetune O
a O
single O
meta Method
- Method
learned Method
model Method
on O
varying O
numbers O
of O
K O
examples O
, O
and O
compare O
performance O
to O
two O
baselines O
: O
( O
a O
) O
pretraining O
on O
all O
of O
the O
tasks O
, O
which O
entails O
training O
a O
network O
to O
regress O
to O
random O
sinusoid O
functions O
and O
then O
, O
at O
test O
- O
time O
, O
fine Method
- Method
tuning Method
with O
gradient Method
descent Method
on O
the O
K O
provided O
points O
, O
using O
an O
automatically O
tuned O
step O
size O
, O
and O
( O
b O
) O
an O
oracle O
which O
receives O
the O
true O
amplitude O
and O
phase O
as O
input O
. O
In O
Appendix O
C O
, O
we O
show O
comparisons O
to O
additional O
multi Method
- Method
task Method
and Method
adaptation Method
methods Method
. O
We O
evaluate O
performance O
by O
fine O
- O
tuning O
the O
model O
learned O
by O
MAML Method
and O
the O
pretrained Method
model Method
on O
K O
= O
{ O
5 O
, O
10 O
, O
20 O
} O
datapoints O
. O
During O
fine Task
- Task
tuning Task
, O
each O
gradient O
step O
is O
computed O
using O
the O
same O
K O
datapoints O
. O
The O
qualitative O
results O
, O
shown O
in O
Figure O
2 O
and O
further O
expanded O
on O
in O
Appendix O
B O
show O
that O
the O
learned O
model O
is O
able O
to O
quickly O
adapt O
with O
only O
5 O
datapoints O
, O
shown O
as O
purple O
triangles O
, O
whereas O
the O
model O
that O
is O
pretrained O
using O
standard O
supervised Method
learning Method
on O
all O
tasks O
is O
unable O
to O
adequately O
adapt O
with O
so O
few O
datapoints O
without O
catastrophic O
overfitting O
. O
Crucially O
, O
when O
the O
K O
datapoints O
are O
all O
in O
one O
half O
of O
the O
input O
range O
, O
the O
model O
trained O
with O
MAML Method
can O
still O
infer O
the O
amplitude O
and O
phase O
in O
the O
other O
half O
of O
the O
range O
, O
demonstrating O
that O
the O
MAML Method
trained O
model O
f O
has O
learned O
to O
model O
the O
periodic O
nature O
of O
the O
sine O
wave O
. O
Furthermore O
, O
we O
observe O
both O
in O
the O
qualitative O
and O
quantitative O
results O
( O
Figure O
3 O
and O
Appendix O
B O
) O
that O
the O
model O
learned O
with O
MAML Method
continues O
to O
improve O
with O
additional O
gradient O
steps O
, O
despite O
being O
trained O
for O
maximal O
performance O
after O
one O
gradient O
step O
. O
This O
improvement O
suggests O
that O
MAML Method
optimizes O
the O
parameters O
such O
that O
they O
lie O
in O
a O
region O
that O
is O
amenable O
to O
fast O
adaptation Task
and O
is O
sensitive O
to O
loss O
functions O
from O
p O
( O
T O
) O
, O
as O
discussed O
in O
Section O
2.2 O
, O
rather O
than O
overfitting O
to O
parameters O
θ O
that O
only O
improve O
after O
one O
step O
. O
section O
: O
Classification Task
To O
evaluate O
MAML Method
in O
comparison O
to O
prior O
meta Method
- Method
learning Method
and Method
few Method
- Method
shot Method
learning Method
algorithms Method
, O
we O
applied O
our O
method O
to O
few Task
- Task
shot Task
image Task
recognition Task
on O
the O
Omniglot Material
[ O
reference O
] O
and O
MiniImagenet Material
datasets O
. O
The O
Omniglot Material
dataset Material
consists O
of O
20 O
instances O
of O
1623 O
characters O
from O
50 O
different O
alphabets O
. O
Each O
instance O
was O
drawn O
by O
a O
different O
person O
. O
The O
MiniImagenet Material
dataset O
was O
proposed O
by O
[ O
reference O
] O
, O
and O
involves O
64 O
training O
classes O
, O
12 O
validation O
classes O
, O
and O
24 O
test O
classes O
. O
The O
Omniglot Material
and O
MiniImagenet Task
image Task
recognition Task
tasks Task
are O
the O
most O
common O
recently O
used O
few O
- O
shot O
learning O
benchmarks O
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
We O
follow O
the O
experimental O
protocol O
proposed O
by O
[ O
reference O
] O
, O
which O
involves O
fast O
learning O
of O
N Task
- Task
way Task
classification Task
with O
1 O
or O
5 O
shots O
. O
The O
problem O
of O
N Task
- Task
way Task
classification Task
is O
set O
up O
as O
follows O
: O
select O
N O
unseen O
classes O
, O
provide O
the O
model O
with O
K O
different O
instances O
of O
each O
of O
the O
N O
classes O
, O
and O
evaluate O
the O
model O
's O
ability O
to O
classify O
new O
instances O
within O
the O
N O
classes O
. O
For O
Omniglot Material
, O
we O
randomly O
select O
1200 O
characters O
for O
training O
, O
irrespective O
of O
alphabet O
, O
and O
use O
the O
remaining O
for O
testing O
. O
The O
Omniglot Material
dataset Material
is O
augmented O
with O
rotations O
by O
multiples O
of O
90 O
degrees O
, O
as O
proposed O
by O
[ O
reference O
] O
. O
Our O
model O
follows O
the O
same O
architecture O
as O
the O
embedding O
function O
used O
by O
[ O
reference O
] O
, O
which O
has O
4 O
modules O
with O
a O
3 O
× O
3 O
convolutions O
and O
64 O
filters O
, O
followed O
by O
batch Method
normalization Method
[ O
reference O
] O
, O
a O
ReLU Method
nonlinearity Method
, O
and O
2 Method
× Method
2 Method
max Method
- Method
pooling Method
. O
The O
Omniglot Material
images O
are O
downsampled O
to O
28 O
× O
28 O
, O
so O
the O
dimensionality O
of O
the O
last O
hidden O
layer O
is O
64 O
. O
As O
in O
the O
baseline Method
classifier Method
used O
by O
[ O
reference O
] O
, O
the O
last O
layer O
is O
fed O
into O
a O
softmax Method
. O
For O
Omniglot Material
, O
we O
used O
strided Method
convolutions Method
instead O
of O
max Method
- Method
pooling Method
. O
For O
MiniImagenet Material
, O
we O
used O
32 O
filters O
per O
layer O
to O
reduce O
overfitting O
, O
as O
done O
by O
[ O
reference O
] O
. O
In O
order O
to O
also O
provide O
a O
fair O
comparison O
against O
memory Method
- Method
augmented Method
neural Method
networks Method
[ O
reference O
] O
and O
to O
test O
the O
flexibility O
of O
MAML Method
, O
we O
also O
provide O
results O
for O
a O
non Method
- Method
convolutional Method
network Method
. O
For O
this O
, O
we O
use O
a O
network O
with O
4 O
hidden O
layers O
with O
sizes O
256 O
, O
128 O
, O
64 O
, O
64 O
, O
each O
including O
batch Method
normalization Method
and O
ReLU O
nonlinearities O
, O
followed O
by O
a O
linear Method
layer Method
and O
softmax Method
. O
For O
all O
models O
, O
the O
loss Metric
function Metric
is O
the O
cross Metric
- Metric
entropy Metric
error Metric
between O
the O
predicted O
and O
true O
class O
. O
Additional O
hyperparameter O
details O
are O
included O
in O
Appendix O
A.1 O
. O
We O
present O
the O
results O
in O
Table O
1 O
. O
The O
convolutional Method
model Method
learned O
by O
MAML Method
compares O
well O
to O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
this O
task O
, O
narrowly O
outperforming O
the O
prior O
methods O
. O
Some O
of O
these O
existing O
methods O
, O
such O
as O
matching Method
networks Method
, O
Siamese Method
networks Method
, O
and O
memory Method
models Method
are O
designed O
with O
few Task
- Task
shot Task
classification Task
in O
mind O
, O
and O
are O
not O
readily O
applicable O
to O
domains O
such O
as O
reinforcement Task
learning Task
. O
Additionally O
, O
the O
model O
learned O
with O
MAML Method
uses O
[ O
reference O
] O
. O
5 Metric
- Metric
way Metric
Accuracy Metric
20 O
- O
way O
Accuracy O
Omniglot Material
[ O
reference O
] O
1 Method
- Method
shot Method
5 Method
- Method
shot Method
1 Method
- Method
shot Method
5 Method
- Method
shot Method
MANN Method
, O
no O
conv O
[ O
reference O
] O
82.8 O
% O
94.9 O
% O
-- O
MAML Method
, O
no O
conv O
( O
ours O
) O
89.7 O
± O
1.1 O
% O
97.5 O
± O
0.6 O
% O
-- Method
Siamese Method
nets Method
[ O
reference O
] O
97.3 O
% O
98.4 O
% O
88.2 O
% O
97.0 O
% O
matching Method
nets Method
[ O
reference O
] O
98.1 O
% O
98.9 O
% O
93.8 O
% O
98.5 O
% O
neural Metric
statistician Metric
[ O
reference O
] O
98.1 O
% O
99.5 O
% O
93.2 O
% O
98.1 O
% O
memory Metric
mod Metric
. O
[ O
reference O
] O
98.4 O
% O
99.6 O
% O
95.0 O
% O
98.6 O
% O
MAML Method
( O
ours O
) O
98.7 O
± O
0.4 O
% O
99.9 O
± O
0.1 O
% O
95.8 O
± O
0.3 O
% O
98.9 O
± O
0.2 O
% O
5 Metric
- Metric
way Metric
Accuracy Metric
MiniImagenet Material
[ O
reference O
] O
1 O
- O
shot O
5 O
- O
shot O
fine O
- O
tuning O
baseline O
28.86 O
± O
0.54 O
% O
49.79 O
± O
0.79 O
% O
nearest Method
neighbor Method
baseline Method
41.08 O
± O
0.70 O
% O
51.04 O
± O
0.65 O
% O
matching Method
nets Method
[ O
reference O
] O
43.56 O
± O
0.84 O
% O
55.31 O
± O
0.73 O
% O
meta Method
- Method
learner Method
LSTM Method
[ O
reference O
] O
43.44 O
± O
0.77 O
% O
60.60 O
± O
0.71 O
% O
MAML Method
, O
first Method
order Method
approx Method
. O
( O
ours O
) O
48.07 O
± O
1.75 O
% O
63.15 O
± O
0.91 O
% O
MAML Method
( O
ours O
) O
48.70 O
± O
1.84 O
% O
63.11 O
± O
0.92 O
% O
fewer O
overall O
parameters O
compared O
to O
matching Method
networks Method
and O
the O
meta Method
- Method
learner Method
LSTM Method
, O
since O
the O
algorithm O
does O
not O
introduce O
any O
additional O
parameters O
beyond O
the O
weights O
of O
the O
classifier Method
itself O
. O
Compared O
to O
these O
prior O
methods O
, O
memory Method
- Method
augmented Method
neural Method
networks Method
[ O
reference O
] O
specifically O
, O
and O
recurrent Method
meta Method
- Method
learning Method
models Method
in O
general O
, O
represent O
a O
more O
broadly O
applicable O
class O
of O
methods O
that O
, O
like O
MAML Method
, O
can O
be O
used O
for O
other O
tasks O
such O
as O
reinforcement Task
learning Task
[ O
reference O
][ O
reference O
] O
. O
However O
, O
as O
shown O
in O
the O
comparison O
, O
MAML Method
significantly O
outperforms O
memory Method
- Method
augmented Method
networks Method
and O
the O
meta Method
- Method
learner Method
LSTM Method
on O
5 O
- O
way O
Omniglot Material
and O
MiniImagenet Material
classification O
, O
both O
in O
the O
1 Task
- Task
shot Task
and Task
5 Task
- Task
shot Task
case Task
. O
A O
significant O
computational Metric
expense Metric
in O
MAML Method
comes O
from O
the O
use O
of O
second O
derivatives O
when O
backpropagating O
the O
meta O
- O
gradient O
through O
the O
gradient Method
operator Method
in O
the O
meta O
- O
objective O
( O
see O
Equation O
( O
1 O
) O
) O
. O
On O
MiniImagenet Material
, O
we O
show O
a O
comparison O
to O
a O
first O
- O
order O
approximation O
of O
MAML Method
, O
where O
these O
second O
derivatives O
are O
omitted O
. O
Note O
that O
the O
resulting O
method O
still O
computes O
the O
meta O
- O
gradient O
at O
the O
post O
- O
update O
parameter O
values O
θ O
i O
, O
which O
provides O
for O
effective O
meta Task
- Task
learning Task
. O
Surprisingly O
however O
, O
the O
performance O
of O
this O
method O
is O
nearly O
the O
same O
as O
that O
obtained O
with O
full O
second O
derivatives O
, O
suggesting O
that O
most O
of O
the O
improvement O
in O
MAML Method
comes O
from O
the O
gradients O
of O
the O
objective O
at O
the O
post O
- O
update O
parameter O
values O
, O
rather O
than O
the O
second Method
order Method
updates Method
from O
differentiating O
through O
the O
gradient Method
update Method
. O
Past O
work O
has O
observed O
that O
ReLU Method
neural Method
networks Method
are O
locally O
almost O
linear O
[ O
reference O
] O
, O
which O
suggests O
that O
second O
derivatives O
may O
be O
close O
to O
zero O
in O
most O
cases O
, O
partially O
explaining O
the O
good O
perfor O
- O
mance O
of O
the O
first Method
- Method
order Method
approximation Method
. O
This O
approximation O
removes O
the O
need O
for O
computing O
Hessian O
- O
vector O
products O
in O
an O
additional O
backward O
pass O
, O
which O
we O
found O
led O
to O
roughly O
33 O
% O
speed O
- O
up O
in O
network Task
computation Task
. O
section O
: O
Reinforcement Method
Learning Method
To O
evaluate O
MAML Method
on O
reinforcement Task
learning Task
problems Task
, O
we O
constructed O
several O
sets O
of O
tasks O
based O
off O
of O
the O
simulated O
continuous O
control O
environments O
in O
the O
rllab O
benchmark O
suite O
[ O
reference O
] O
. O
We O
discuss O
the O
individual O
domains O
below O
. O
In O
all O
of O
the O
domains O
, O
the O
model O
trained O
by O
MAML Method
is O
a O
neural Method
network Method
policy Method
with O
two O
hidden O
layers O
of O
size O
100 O
, O
with O
ReLU O
nonlinearities O
. O
The O
gradient Method
updates Method
are O
computed O
using O
vanilla Method
policy Method
gradient Method
( O
RE Method
- Method
INFORCE Method
) O
[ O
reference O
] O
, O
and O
we O
use O
trust Method
- Method
region Method
policy Method
optimization Method
( O
TRPO Method
) O
as O
the O
meta Method
- Method
optimizer Method
[ O
reference O
] O
. O
In O
order O
to O
avoid O
computing O
third O
derivatives O
, O
Figure O
5 O
. O
Reinforcement Method
learning Method
results O
for O
the O
half Task
- Task
cheetah Task
and Task
ant Task
locomotion Task
tasks Task
, O
with O
the O
tasks O
shown O
on O
the O
far O
right O
. O
Each O
gradient Method
step Method
requires O
additional O
samples O
from O
the O
environment O
, O
unlike O
the O
supervised Task
learning Task
tasks Task
. O
The O
results O
show O
that O
MAML Method
can O
adapt O
to O
new O
goal O
velocities O
and O
directions O
substantially O
faster O
than O
conventional O
pretraining Method
or O
random Method
initialization Method
, O
achieving O
good O
performs O
in O
just O
two O
or O
three O
gradient O
steps O
. O
We O
exclude O
the O
goal O
velocity O
, O
random O
baseline O
curves O
, O
since O
the O
returns O
are O
much O
worse O
( O
< O
−200 O
for O
cheetah O
and O
< O
−25 O
for O
ant O
) O
. O
we O
use O
finite Method
differences Method
to O
compute O
the O
Hessian Method
- Method
vector Method
products Method
for O
TRPO Method
. O
For O
both O
learning Task
and Task
meta Task
- Task
learning Task
updates Task
, O
we O
use O
the O
standard O
linear Method
feature Method
baseline Method
proposed O
by O
[ O
reference O
] O
, O
which O
is O
fitted O
separately O
at O
each O
iteration O
for O
each O
sampled O
task O
in O
the O
batch O
. O
We O
compare O
to O
three O
baseline O
models O
: O
( O
a O
) O
pretraining Method
one Method
policy Method
on O
all O
of O
the O
tasks O
and O
then O
fine Task
- Task
tuning Task
, O
( O
b O
) O
training O
a O
policy O
from O
randomly O
initialized O
weights O
, O
and O
( O
c O
) O
an O
oracle Method
policy Method
which O
receives O
the O
parameters O
of O
the O
task O
as O
input O
, O
which O
for O
the O
tasks O
below O
corresponds O
to O
a O
goal O
position O
, O
goal O
direction O
, O
or O
goal O
velocity O
for O
the O
agent O
. O
The O
baseline O
models O
of O
( O
a O
) O
and O
( O
b O
) O
are O
fine O
- O
tuned O
with O
gradient Method
descent Method
with O
a O
manually O
tuned O
step O
size O
. O
Videos O
of O
the O
learned O
policies O
can O
be O
viewed O
at O
sites.google.com O
/ O
view O
/ O
maml O
2D O
Navigation O
. O
In O
our O
first O
meta Task
- Task
RL Task
experiment O
, O
we O
study O
a O
set O
of O
tasks O
where O
a O
point O
agent O
must O
move O
to O
different O
goal O
positions O
in O
2D O
, O
randomly O
chosen O
for O
each O
task O
within O
a O
unit O
square O
. O
The O
observation O
is O
the O
current O
2D O
position O
, O
and O
actions O
correspond O
to O
velocity O
commands O
clipped O
to O
be O
in O
the O
range O
[ O
−0.1 O
, O
0.1 O
] O
. O
The O
reward O
is O
the O
negative O
squared O
distance O
to O
the O
goal O
, O
and O
episodes O
terminate O
when O
the O
agent O
is O
within O
0.01 O
of O
the O
goal O
or O
at O
the O
horizon O
of O
H O
= O
100 O
. O
The O
policy O
was O
trained O
with O
MAML Method
to O
maximize O
performance O
after O
1 O
policy Method
gradient Method
update Method
using O
20 O
trajectories O
. O
Additional O
hyperparameter O
settings O
for O
this O
problem O
and O
the O
following O
RL Task
problems Task
are O
in O
Appendix O
A.2 O
. O
In O
our O
evaluation O
, O
we O
compare O
adaptation Task
to O
a O
new O
task O
with O
up O
to O
4 O
gradient O
updates O
, O
each O
with O
40 O
samples O
. O
The O
results O
in O
Figure O
4 O
show O
the O
adaptation Task
performance O
of O
models O
that O
are O
initialized O
with O
MAML Method
, O
conventional O
pretraining Method
on O
the O
same O
set O
of O
tasks O
, O
random Method
initialization Method
, O
and O
an O
oracle Method
policy Method
that O
receives O
the O
goal O
position O
as O
input O
. O
The O
results O
show O
that O
MAML Method
can O
learn O
a O
model O
that O
adapts O
much O
more O
quickly O
in O
a O
single O
gradient O
update O
, O
and O
furthermore O
continues O
to O
improve O
with O
additional O
updates O
. O
Locomotion Task
. O
To O
study O
how O
well O
MAML Method
can O
scale O
to O
more O
complex O
deep Task
RL Task
problems Task
, O
we O
also O
study O
adaptation Task
on O
high Task
- Task
dimensional Task
locomotion Task
tasks Task
with O
the O
MuJoCo Method
simulator Method
[ O
reference O
] O
. O
The O
tasks O
require O
two O
simulated O
robots O
- O
a O
planar O
cheetah O
and O
a O
3D O
quadruped O
( O
the O
" O
ant O
" O
) O
- O
to O
run O
in O
a O
particular O
direction O
or O
at O
a O
particular O
velocity O
. O
In O
the O
goal O
velocity O
experiments O
, O
the O
reward O
is O
the O
negative O
absolute O
value O
between O
the O
current O
velocity O
of O
the O
agent O
and O
a O
goal O
, O
which O
is O
chosen O
uniformly O
at O
random O
between O
0.0 O
and O
2.0 O
for O
the O
cheetah O
and O
between O
0.0 O
and O
3.0 O
for O
the O
ant O
. O
In O
the O
goal O
direction O
experiments O
, O
the O
reward O
is O
the O
magnitude O
of O
the O
velocity O
in O
either O
the O
forward O
or O
backward O
direction O
, O
chosen O
at O
random O
for O
each O
task O
in O
p O
( O
T O
) O
. O
The O
horizon O
is O
H O
= O
200 O
, O
with O
20 O
rollouts O
per O
gradient O
step O
for O
all O
problems O
except O
the O
ant Task
forward Task
/ Task
backward Task
task Task
, O
which O
used O
40 O
rollouts O
per O
step O
. O
The O
results O
in O
[ O
reference O
] O
show O
that O
MAML Method
learns O
a O
model O
that O
can O
quickly O
adapt O
its O
velocity O
and O
direction O
with O
even O
just O
a O
single O
gradient O
update O
, O
and O
continues O
to O
improve O
with O
more O
gradient O
steps O
. O
The O
results O
also O
show O
that O
, O
on O
these O
challenging O
tasks O
, O
the O
MAML Method
initialization O
substantially O
outperforms O
random Method
initialization Method
and O
pretraining Method
. O
In O
fact O
, O
pretraining Method
is O
in O
some O
cases O
worse O
than O
random Method
initialization Method
, O
a O
fact O
observed O
in O
prior O
RL O
work O
[ O
reference O
] O
. O
section O
: O
Discussion O
and O
Future O
Work O
We O
introduced O
a O
meta Method
- Method
learning Method
method Method
based O
on O
learning O
easily O
adaptable O
model O
parameters O
through O
gradient Method
descent Method
. O
Our O
approach O
has O
a O
number O
of O
benefits O
. O
It O
is O
simple O
and O
does O
not O
introduce O
any O
learned O
parameters O
for O
metalearning Task
. O
It O
can O
be O
combined O
with O
any O
model Method
representation Method
that O
is O
amenable O
to O
gradient Method
- Method
based Method
training Method
, O
and O
any O
differentiable Task
objective Task
, O
including O
classification Task
, O
regression Task
, O
and O
reinforcement Task
learning Task
. O
Lastly O
, O
since O
our O
method O
merely O
produces O
a O
weight Method
initialization Method
, O
adaptation Task
can O
be O
performed O
with O
any O
amount O
of O
data O
and O
any O
number O
of O
gradient O
steps O
, O
though O
we O
demonstrate O
state O
- O
of O
- O
the O
- O
art O
results O
on O
classification Task
with O
only O
one O
or O
five O
examples O
per O
class O
. O
We O
also O
show O
that O
our O
method O
can O
adapt O
an O
RL Method
agent Method
using O
policy O
gradients O
and O
a O
very O
modest O
amount O
of O
experience O
. O
Reusing O
knowledge O
from O
past O
tasks O
may O
be O
a O
crucial O
ingredient O
in O
making O
high O
- O
capacity Method
scalable Method
models Method
, O
such O
as O
deep Method
neural Method
networks Method
, O
amenable O
to O
fast Task
training Task
with O
small O
datasets O
. O
We O
believe O
that O
this O
work O
is O
one O
step O
toward O
a O
simple O
and O
general O
- O
purpose O
meta Method
- Method
learning Method
technique Method
that O
can O
be O
applied O
to O
any O
problem O
and O
any O
model O
. O
Further O
research O
in O
this O
area O
can O
make O
multitask Task
initialization Task
a O
standard O
ingredient O
in O
deep Task
learning Task
and O
reinforcement Task
learning Task
. O
section O
: O
A. O
Additional O
Experiment O
Details O
In O
this O
section O
, O
we O
provide O
additional O
details O
of O
the O
experimental O
set O
- O
up O
and O
hyperparameters O
. O
section O
: O
A.1 O
. O
Classification Task
For O
N Task
- Task
way Task
, Task
K Task
- Task
shot Task
classification Task
, O
each O
gradient O
is O
computed O
using O
a O
batch O
size O
of O
N O
K O
examples O
. O
For O
Omniglot Material
, O
the O
5 O
- O
way O
convolutional O
and O
non O
- O
convolutional O
MAML Method
models O
were O
each O
trained O
with O
1 O
gradient O
step O
with O
step O
size O
α O
= O
0.4 O
and O
a O
meta O
batch O
- O
size O
of O
32 O
tasks O
. O
The O
network O
was O
evaluated O
using O
3 O
gradient O
steps O
with O
the O
same O
step O
size O
α O
= O
0.4 O
. O
The O
20 O
- O
way O
convolutional O
MAML Method
model Method
was O
trained O
and O
evaluated O
with O
5 O
gradient O
steps O
with O
step O
size O
α O
= O
0.1 O
. O
During O
training O
, O
the O
meta Metric
batch Metric
- Metric
size Metric
was O
set O
to O
16 O
tasks O
. O
For O
MiniImagenet Material
, O
both O
models O
were O
trained O
using O
5 O
gradient O
steps O
of O
size O
α O
= O
0.01 O
, O
and O
evaluated O
using O
10 O
gradient O
steps O
at O
test O
time O
. O
Following O
[ O
reference O
] O
, O
15 O
examples O
per O
class O
were O
used O
for O
evaluating O
the O
post O
- O
update O
meta O
- O
gradient O
. O
We O
used O
a O
meta O
batch O
- O
size O
of O
4 O
and O
2 O
tasks O
for O
1 Task
- Task
shot Task
and Task
5 Task
- Task
shot Task
training Task
respectively O
. O
All O
models O
were O
trained O
for O
60000 O
iterations O
on O
a O
single O
NVIDIA O
Pascal O
Titan O
X O
GPU O
. O
section O
: O
A.2 O
. O
Reinforcement Method
Learning Method
In O
all O
reinforcement Task
learning Task
experiments O
, O
the O
MAML Method
policy O
was O
trained O
using O
a O
single O
gradient Method
step Method
with O
α O
= O
0.1 O
. O
During O
evaluation O
, O
we O
found O
that O
halving O
the O
learning Metric
rate Metric
after O
the O
first O
gradient O
step O
produced O
superior O
performance O
. O
Thus O
, O
the O
step O
size O
during O
adaptation Task
was O
set O
to O
α O
= O
0.1 O
for O
the O
first O
step O
, O
and O
α O
= O
0.05 O
for O
all O
future O
steps O
. O
The O
step O
sizes O
for O
the O
baseline O
methods O
were O
manually O
tuned O
for O
each O
domain O
. O
In O
the O
2D Task
navigation Task
, O
we O
used O
a O
meta O
batch O
size O
of O
20 O
; O
in O
the O
locomotion Task
problems Task
, O
we O
used O
a O
meta O
batch O
size O
of O
40 O
tasks O
. O
The O
MAML Method
models O
were O
trained O
for O
up O
to O
500 O
meta O
- O
iterations O
, O
and O
the O
model O
with O
the O
best O
average O
return O
during O
training O
was O
used O
for O
evaluation O
. O
For O
the O
ant Task
goal Task
velocity Task
task Task
, O
we O
added O
a O
positive O
reward O
bonus O
at O
each O
timestep O
to O
prevent O
the O
ant O
from O
ending O
the O
episode O
. O
section O
: O
B. O
Additional O
Sinusoid O
Results O
In O
Figure O
6 O
, O
we O
show O
the O
full O
quantitative O
results O
of O
the O
MAML Method
model Method
trained O
on O
10 Material
- Material
shot Material
learning Material
and O
evaluated O
on O
5 Material
- Material
shot Material
, O
10 Material
- Material
shot Material
, O
and O
20 Material
- Material
shot Material
. O
In O
Figure O
7 O
, O
we O
show O
the O
qualitative O
performance O
of O
MAML Method
and O
the O
pretrained Method
baseline Method
on O
randomly O
sampled O
sinusoids O
. O
section O
: O
C. O
Additional O
Comparisons O
In O
this O
section O
, O
we O
include O
more O
thorough O
evaluations O
of O
our O
approach O
, O
including O
additional O
multi Task
- Task
task Task
baselines Task
and O
a O
comparison O
representative O
of O
the O
approach O
of O
[ O
reference O
] O
. O
section O
: O
C.1 O
. O
Multi Task
- Task
task Task
baselines Task
The O
pretraining Method
baseline Method
in O
the O
main O
text O
trained O
a O
single O
network O
on O
all O
tasks O
, O
which O
we O
referred O
to O
as O
" O
pretraining O
on O
all O
tasks O
" O
. O
To O
evaluate O
the O
model O
, O
as O
with O
MAML Method
, O
we O
fine O
- O
tuned O
this O
model O
on O
each O
test O
task O
using O
K O
examples O
. O
In O
the O
domains O
that O
we O
study O
, O
different O
tasks O
involve O
different O
output O
values O
for O
the O
same O
input O
. O
As O
a O
result O
, O
by O
pre O
- O
training O
on O
all O
tasks O
, O
the O
model O
would O
learn O
to O
output O
the O
average O
output O
for O
a O
particular O
input O
value O
. O
In O
some O
instances O
, O
this O
model O
may O
learn O
very O
little O
about O
the O
actual O
domain O
, O
and O
instead O
learn O
about O
the O
range O
of O
the O
output O
space O
. O
We O
experimented O
with O
a O
multi Method
- Method
task Method
method Method
to O
provide O
a O
point O
of O
comparison O
, O
where O
instead O
of O
averaging O
in O
the O
output O
space O
, O
we O
averaged O
in O
the O
parameter O
space O
. O
To O
achieve O
averaging O
in O
parameter O
space O
, O
we O
sequentially O
trained O
500 O
separate O
models O
on O
500 O
tasks O
drawn O
from O
p O
( O
T O
) O
. O
Each O
model O
was O
initialized O
randomly O
and O
trained O
on O
a O
large O
amount O
of O
data O
from O
its O
assigned O
task O
. O
We O
then O
took O
the O
average O
parameter O
vector O
across O
models O
and O
fine O
- O
tuned O
on O
5 O
datapoints O
with O
a O
tuned O
step O
size O
. O
All O
of O
our O
experiments O
for O
this O
method O
were O
on O
the O
sinusoid Task
task Task
because O
of O
computational O
requirements O
. O
The O
error Metric
of O
the O
individual O
regressors O
was O
low O
: O
less O
than O
0.02 O
on O
their O
respective O
sine O
waves O
. O
We O
tried O
three O
variants O
of O
this O
set O
- O
up O
. O
During O
training O
of O
the O
individual O
regressors O
, O
we O
tried O
using O
one O
of O
the O
following O
: O
no Method
regularization Method
, O
standard O
2 Method
weight Method
decay Method
, O
and O
2 O
weight Method
regularization Method
to O
the O
mean O
parameter O
vector O
thus O
far O
of O
the O
trained O
regressors O
. O
The O
latter O
two O
variants O
encourage O
the O
individual O
models O
to O
find O
parsimonious O
solutions O
. O
When O
using O
regularization Method
, O
we O
set O
the O
magnitude O
of O
the O
regularization O
to O
be O
as O
high O
as O
possible O
without O
significantly O
deterring O
performance O
. O
In O
our O
results O
, O
we O
refer O
to O
this O
approach O
as O
" O
multi Task
- Task
task Task
" O
. O
As O
seen O
in O
the O
results O
in O
Table O
2 O
, O
we O
find O
averaging Method
in O
the O
parameter O
space O
( O
multi Task
- Task
task Task
) O
performed O
worse O
than O
averaging O
in O
the O
output O
space O
( O
pretraining O
on O
all O
tasks O
) O
. O
This O
suggests O
that O
it O
is O
difficult O
to O
find O
parsimonious O
solutions O
to O
multiple O
tasks O
when O
training O
on O
tasks O
separately O
, O
and O
that O
MAML Method
is O
learning O
a O
solution O
that O
is O
more O
sophisticated O
than O
the O
mean O
optimal O
parameter O
vector O
. O
section O
: O
C.2 O
. O
Context Method
vector Method
adaptation Method
Rei O
( O
2015 O
) O
developed O
a O
method O
which O
learns O
a O
context O
vector O
that O
can O
be O
adapted O
online O
, O
with O
an O
application O
to O
recurrent Method
language Method
models Method
. O
The O
parameters O
in O
this O
context O
vector O
are O
learned O
and O
adapted O
in O
the O
same O
way O
as O
the O
parameters O
in O
the O
MAML Method
model Method
. O
To O
provide O
a O
comparison O
to O
using O
such O
a O
context O
vector O
for O
meta Task
- Task
learning Task
problems Task
, O
we O
concatenated O
a O
set O
of O
free O
parameters O
z O
to O
the O
input O
x O
, O
and O
only O
allowed O
the O
gradient O
steps O
to O
modify O
z O
, O
rather O
than O
modifying O
the O
model O
parameters O
θ O
, O
as O
in O
MAML Method
. O
For O
im O
- O
Figure O
6 O
. O
Quantitative Method
sinusoid Method
regression Method
results O
showing O
test O
- O
time O
learning O
curves O
with O
varying O
numbers O
of O
K O
test O
- O
time O
samples O
. O
Each O
gradient O
step O
is O
computed O
using O
the O
same O
K O
examples O
. O
Note O
that O
MAML Method
continues O
to O
improve O
with O
additional O
gradient O
steps O
without O
overfitting O
to O
the O
extremely O
small O
dataset O
during O
meta Task
- Task
testing Task
, O
and O
achieves O
a O
loss O
that O
is O
substantially O
lower O
than O
the O
baseline Method
fine Method
- Method
tuning Method
approach Method
. O
Table O
3 O
. O
5 O
- O
way O
Omniglot Material
Classification Material
1 O
- O
shot O
5 O
- O
shot O
context O
vector O
94.9 O
± O
0.9 O
% O
97.7 O
± O
0.3 O
% O
MAML Method
98.7 O
± O
0.4 O
% O
99.9 O
± O
0.1 O
% O
age O
inputs O
, O
z O
was O
concatenated O
channel O
- O
wise O
with O
the O
input O
image O
. O
We O
ran O
this O
method O
on O
Omniglot Material
and O
two O
RL O
domains O
following O
the O
same O
experimental O
protocol O
. O
We O
report O
the O
results O
in O
Tables O
3 O
, O
4 O
, O
and O
5 O
. O
Learning O
an O
adaptable O
context O
vector O
performed O
well O
on O
the O
toy Task
pointmass Task
problem Task
, O
but O
sub O
- O
par O
on O
more O
difficult O
problems O
, O
likely O
due O
to O
a O
less O
flexible O
meta Method
- Method
optimization Method
. O
section O
: O
section O
: O
Acknowledgements O
The O
authors O
would O
like O
to O
thank O
Xi O
Chen O
and O
Trevor O
Darrell O
for O
helpful O
discussions O
, O
Yan O
Duan O
and O
Alex O
Lee O
for O
technical O
advice O
, O
Nikhil O
Mishra O
, O
Haoran O
Tang O
, O
and O
Greg O
Kahn O
for O
feedback O
on O
an O
early O
draft O
of O
the O
paper O
, O
and O
the O
anonymous O
reviewers O
for O
their O
comments O
. O
This O
work O
was O
supported O
in O
part O
by O
an O
ONR O
PECASE O
award O
and O
an O
NSF O
GRFP O
award O
. O
section O
: O
