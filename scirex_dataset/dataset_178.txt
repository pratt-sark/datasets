document O
: O
Attention Method
- Method
Based Method
Models Method
for O
Speech Task
Recognition Task
Recurrent Method
sequence Method
generators Method
conditioned O
on O
input O
data O
through O
an O
attention Method
mechanism Method
have O
recently O
shown O
very O
good O
performance O
on O
a O
range O
of O
tasks O
including O
machine Task
translation Task
, O
handwriting Task
synthesis Task
and O
image Task
caption Task
generation Task
. O
We O
extend O
the O
attention Method
- Method
mechanism Method
with O
features O
needed O
for O
speech Task
recognition Task
. O
We O
show O
that O
while O
an O
adaptation O
of O
the O
model O
used O
for O
machine Task
translation Task
in O
reaches O
a O
competitive O
18.7 O
% O
phoneme Metric
error Metric
rate Metric
( O
PER Metric
) O
on O
the O
TIMIT O
phoneme Task
recognition Task
task Task
, O
it O
can O
only O
be O
applied O
to O
utterances O
which O
are O
roughly O
as O
long O
as O
the O
ones O
it O
was O
trained O
on O
. O
We O
offer O
a O
qualitative O
explanation O
of O
this O
failure O
and O
propose O
a O
novel O
and O
generic O
method O
of O
adding O
location Task
- Task
awareness Task
to O
the O
attention Method
mechanism Method
to O
alleviate O
this O
issue O
. O
The O
new O
method O
yields O
a O
model O
that O
is O
robust O
to O
long O
inputs O
and O
achieves O
18 O
% O
PER Metric
in O
single O
utterances O
and O
20 O
% O
in O
10 O
- O
times O
longer O
( O
repeated O
) O
utterances O
. O
Finally O
, O
we O
propose O
a O
change O
to O
the O
attention Method
mechanism Method
that O
prevents O
it O
from O
concentrating O
too O
much O
on O
single O
frames O
, O
which O
further O
reduces O
PER Metric
to O
17.6 O
% O
level O
. O
section O
: O
Introduction O
Recently O
, O
attention Method
- Method
based Method
recurrent Method
networks Method
have O
been O
successfully O
applied O
to O
a O
wide O
variety O
of O
tasks O
, O
such O
as O
handwriting Task
synthesis Task
, O
machine Task
translation Task
, O
image Task
caption Task
generation Task
and O
visual Task
object Task
classification Task
. O
Such O
models O
iteratively O
process O
their O
input O
by O
selecting O
relevant O
content O
at O
every O
step O
. O
This O
basic O
idea O
significantly O
extends O
the O
applicability O
range O
of O
end Method
- Method
to Method
- Method
end Method
training Method
methods Method
, O
for O
instance O
, O
making O
it O
possible O
to O
construct O
networks O
with O
external O
memory O
. O
We O
introduce O
extensions O
to O
attention Method
- Method
based Method
recurrent Method
networks Method
that O
make O
them O
applicable O
to O
speech Task
recognition Task
. O
Learning Task
to O
recognize O
speech O
can O
be O
viewed O
as O
learning O
to O
generate O
a O
sequence O
( O
transcription O
) O
given O
another O
sequence O
( O
speech O
) O
. O
From O
this O
perspective O
it O
is O
similar O
to O
machine Task
translation Task
and Task
handwriting Task
synthesis Task
tasks Task
, O
for O
which O
attention Method
- Method
based Method
methods Method
have O
been O
found O
suitable O
. O
However O
, O
compared O
to O
machine Task
translation Task
, O
speech Task
recognition Task
principally O
differs O
by O
requesting O
much O
longer O
input O
sequences O
( O
thousands O
of O
frames O
instead O
of O
dozens O
of O
words O
) O
, O
which O
introduces O
a O
challenge O
of O
distinguishing O
similar O
speech O
fragments O
in O
a O
single O
utterance O
. O
It O
is O
also O
different O
from O
handwriting Task
synthesis Task
, O
since O
the O
input O
sequence O
is O
much O
noisier O
and O
does O
not O
have O
as O
clear O
structure O
. O
For O
these O
reasons O
speech Task
recognition Task
is O
an O
interesting O
testbed O
for O
developing O
new O
attention Method
- Method
based Method
architectures Method
capable O
of O
processing O
long O
and O
noisy O
inputs O
. O
Application O
of O
attention Method
- Method
based Method
models Method
to O
speech Task
recognition Task
is O
also O
an O
important O
step O
toward O
building O
fully O
end Task
- Task
to Task
- Task
end Task
trainable Task
speech Task
recognition Task
systems Task
, O
which O
is O
an O
active O
area O
of O
research O
. O
The O
dominant O
approach O
is O
still O
based O
on O
hybrid Method
systems Method
consisting O
of O
a O
deep Method
neural Method
acoustic Method
model Method
, O
a O
triphone Method
HMM Method
model Method
and O
an O
n Method
- Method
gram Method
language Method
model Method
. O
This O
requires O
dictionaries Method
of Method
hand Method
- Method
crafted Method
pronunciation Method
and Method
phoneme Method
lexicons Method
, O
and O
a O
multi Method
- Method
stage Method
training Method
procedure Method
to O
make O
the O
components O
work O
together O
. O
Excellent O
results O
by O
an O
HMM Method
- Method
less Method
recognizer Method
have O
recently O
been O
reported O
, O
with O
the O
system O
consisting O
of O
a O
CTC Method
- O
trained O
neural O
network O
and O
a O
language Method
model Method
. O
Still O
, O
the O
language Method
model Method
was O
added O
only O
at O
the O
last O
stage O
in O
that O
work O
, O
thus O
leaving O
open O
a O
question O
of O
how O
much O
an O
acoustic Method
model Method
can O
benefit O
from O
being O
aware O
of O
a O
language Method
model Method
during O
training O
. O
In O
this O
paper O
, O
we O
evaluate O
attention Method
- Method
based Method
models Method
on O
a O
phoneme Task
recognition Task
task Task
using O
the O
widely O
- O
used O
TIMIT Material
dataset Material
. O
At O
each O
time O
step O
in O
generating O
an O
output O
sequence O
( O
phonemes O
) O
, O
an O
attention Method
mechanism Method
selects O
or O
weighs O
the O
signals O
produced O
by O
a O
trained O
feature Method
extraction Method
mechanism Method
at O
potentially O
all O
of O
the O
time O
steps O
in O
the O
input O
sequence O
( O
speech O
frames O
) O
. O
The O
weighted O
feature O
vector O
then O
helps O
to O
condition O
the O
generation O
of O
the O
next O
element O
of O
the O
output O
sequence O
. O
Since O
the O
utterances O
in O
this O
dataset O
are O
rather O
short O
( O
mostly O
under O
5 O
seconds O
) O
, O
we O
measure O
the O
ability O
of O
the O
considered O
models O
in O
recognizing O
much O
longer O
utterances O
which O
were O
created O
by O
artificially O
concatenating O
the O
existing O
utterances O
. O
We O
start O
with O
a O
model O
proposed O
in O
for O
the O
machine Task
translation Task
task Task
as O
the O
baseline O
. O
This O
model O
seems O
entirely O
vulnerable O
to O
the O
issue O
of O
similar O
speech O
fragments O
but O
despite O
our O
expectations O
it O
was O
competitive O
on O
the O
original O
test O
set O
, O
reaching O
18.7 O
% O
phoneme Metric
error Metric
rate Metric
( O
PER Metric
) O
. O
However O
, O
its O
performance O
degraded O
quickly O
with O
longer O
, O
concatenated O
utterances O
. O
We O
provide O
evidence O
that O
this O
model O
adapted O
to O
track O
the O
absolute O
location O
in O
the O
input O
sequence O
of O
the O
content O
it O
is O
recognizing O
, O
a O
strategy O
feasible O
for O
short O
utterances O
from O
the O
original O
test O
set O
but O
inherently O
unscalable O
. O
In O
order O
to O
circumvent O
this O
undesired O
behavior O
, O
in O
this O
paper O
, O
we O
propose O
to O
modify O
the O
attention Method
mechanism Method
such O
that O
it O
explicitly O
takes O
into O
account O
both O
( O
a O
) O
the O
location O
of O
the O
focus O
from O
the O
previous O
step O
, O
as O
in O
and O
( O
b O
) O
the O
features O
of O
the O
input O
sequence O
, O
as O
in O
. O
This O
is O
achieved O
by O
adding O
as O
inputs O
to O
the O
attention Method
mechanism Method
auxiliary O
convolutional O
features O
which O
are O
extracted O
by O
convolving O
the O
attention O
weights O
from O
the O
previous O
step O
with O
trainable Method
filters Method
. O
We O
show O
that O
a O
model O
with O
such O
convolutional Method
features Method
performs O
significantly O
better O
on O
the O
considered O
task O
( O
18.0 O
% O
PER Metric
) O
. O
More O
importantly O
, O
the O
model O
with O
convolutional Method
features Method
robustly O
recognized O
utterances O
many O
times O
longer O
than O
the O
ones O
from O
the O
training O
set O
, O
always O
staying O
below O
20 O
% O
PER Metric
. O
Therefore O
, O
the O
contribution O
of O
this O
work O
is O
three O
- O
fold O
. O
For O
one O
, O
we O
present O
a O
novel O
purely Method
neural Method
speech Method
recognition Method
architecture Method
based O
on O
an O
attention Method
mechanism Method
, O
whose O
performance O
is O
comparable O
to O
that O
of O
the O
conventional O
approaches O
on O
the O
TIMIT Material
dataset Material
. O
Moreover O
, O
we O
propose O
a O
generic O
method O
of O
adding O
location Task
awareness Task
to O
the O
attention Method
mechanism Method
. O
Finally O
, O
we O
introduce O
a O
modification O
of O
the O
attention Method
mechanism Method
to O
avoid O
concentrating O
the O
attention O
on O
a O
single O
frame O
, O
and O
thus O
avoid O
obtaining O
less O
“ O
effective O
training O
examples O
” O
, O
bringing O
the O
PER Metric
down O
to O
17.6 O
% O
. O
section O
: O
Attention Method
- Method
Based Method
Model Method
for O
Speech Task
Recognition Task
subsection O
: O
General O
Framework O
An O
attention Method
- Method
based Method
recurrent Method
sequence Method
generator Method
( O
ARSG Method
) O
is O
a O
recurrent Method
neural Method
network Method
that O
stochastically O
generates O
an O
output O
sequence O
from O
an O
input O
. O
In O
practice O
, O
is O
often O
processed O
by O
an O
encoder Method
which O
outputs O
a O
sequential Method
input Method
representation Method
more O
suitable O
for O
the O
attention Method
mechanism Method
to O
work O
with O
. O
In O
the O
context O
of O
this O
work O
, O
the O
output O
is O
a O
sequence O
of O
phonemes O
, O
and O
the O
input O
is O
a O
sequence O
of O
feature O
vectors O
. O
Each O
feature O
vector O
is O
extracted O
from O
a O
small O
overlapping O
window O
of O
audio O
frames O
. O
The O
encoder Method
is O
implemented O
as O
a O
deep Method
bidirectional Method
recurrent Method
network Method
( O
BiRNN Method
) O
, O
to O
form O
a O
sequential Method
representation Method
of Method
length Method
. O
At O
the O
- O
th O
step O
an O
ARSG Method
generates O
an O
output O
by O
focusing O
on O
the O
relevant O
elements O
of O
: O
where O
is O
the O
- O
th O
state O
of O
the O
recurrent Method
neural Method
network Method
to O
which O
we O
refer O
as O
the O
generator O
, O
is O
a O
vector O
of O
the O
attention O
weights O
, O
also O
often O
called O
the O
alignment O
. O
Using O
the O
terminology O
from O
, O
we O
call O
a O
glimpse O
. O
The O
step O
is O
completed O
by O
computing O
a O
new O
generator O
state O
: O
Long Method
short Method
- Method
term Method
memory Method
units Method
( O
LSTM Method
, O
) O
and O
gated Method
recurrent Method
units Method
( O
GRU Method
, O
) O
are O
typically O
used O
as O
a O
recurrent O
activation O
, O
to O
which O
we O
refer O
as O
a O
recurrency Method
. O
The O
process O
is O
graphically O
illustrated O
in O
Fig O
. O
[ O
reference O
] O
. O
Inspired O
by O
we O
distinguish O
between O
location Method
- Method
based Method
, Method
content Method
- Method
based Method
and O
hybrid Method
attention Method
mechanisms Method
. O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
describes O
the O
most O
generic O
, O
hybrid Method
attention Method
. O
If O
the O
term O
is O
dropped O
from O
arguments O
, O
i.e. O
, O
, O
we O
call O
it O
content O
- O
based O
( O
see O
, O
e.g. O
, O
or O
) O
. O
In O
this O
case O
, O
is O
often O
implemented O
by O
scoring O
each O
element O
in O
separately O
and O
normalizing O
the O
scores O
: O
The O
main O
limitation O
of O
such O
scheme O
is O
that O
identical O
or O
very O
similar O
elements O
of O
are O
scored O
equally O
regardless O
of O
their O
position O
in O
the O
sequence O
. O
This O
is O
the O
issue O
of O
“ O
similar O
speech O
fragments O
” O
raised O
above O
. O
Often O
this O
issue O
is O
partially O
alleviated O
by O
an O
encoder Method
such O
as O
e.g. O
a O
BiRNN Method
or O
a O
deep Method
convolutional Method
network Method
that O
encode O
contextual O
information O
into O
every O
element O
of O
. O
However O
, O
capacity O
of O
elements O
is O
always O
limited O
, O
and O
thus O
disambiguation O
by O
context O
is O
only O
possible O
to O
a O
limited O
extent O
. O
Alternatively O
, O
a O
location Method
- Method
based Method
attention Method
mechanism Method
computes O
the O
alignment O
from O
the O
generator O
state O
and O
the O
previous O
alignment O
only O
such O
that O
. O
For O
instance O
, O
Graves O
used O
the O
location Method
- Method
based Method
attention Method
mechanism Method
using O
a O
Gaussian Method
mixture Method
model Method
in O
his O
handwriting Method
synthesis Method
model Method
. O
In O
the O
case O
of O
speech Task
recognition Task
, O
this O
type O
of O
location Method
- Method
based Method
attention Method
mechanism Method
would O
have O
to O
predict O
the O
distance O
between O
consequent O
phonemes O
using O
only O
, O
which O
we O
expect O
to O
be O
hard O
due O
to O
large O
variance O
of O
this O
quantity O
. O
For O
these O
limitations O
associated O
with O
both O
content Method
- Method
based Method
and Method
location Method
- Method
based Method
mechanisms Method
, O
we O
argue O
that O
a O
hybrid Method
attention Method
mechanism Method
is O
a O
natural O
candidate O
for O
speech Task
recognition Task
. O
Informally O
, O
we O
would O
like O
an O
attention Method
model Method
that O
uses O
the O
previous O
alignment O
to O
select O
a O
short O
list O
of O
elements O
from O
, O
from O
which O
the O
content Method
- Method
based Method
attention Method
, O
in O
Eqs O
. O
( O
[ O
reference O
] O
) O
– O
( O
[ O
reference O
] O
) O
, O
will O
select O
the O
relevant O
ones O
without O
confusion O
. O
subsection O
: O
Proposed O
Model O
: O
ARSG Method
with O
Convolutional O
Features O
We O
start O
from O
the O
ARSG Method
- O
based O
model O
with O
the O
content Method
- Method
based Method
attention Method
mechanism Method
proposed O
in O
. O
This O
model O
can O
be O
described O
by O
Eqs O
. O
( O
[ O
reference O
] O
) O
– O
( O
[ O
reference O
] O
) O
, O
where O
and O
are O
vectors O
, O
and O
are O
matrices O
. O
We O
extend O
this O
content Method
- Method
based Method
attention Method
mechanism Method
of O
the O
original O
model O
to O
be O
location Task
- Task
aware Task
by O
making O
it O
take O
into O
account O
the O
alignment O
produced O
at O
the O
previous O
step O
. O
First O
, O
we O
extract O
vectors O
for O
every O
position O
of O
the O
previous O
alignment O
by O
convolving O
it O
with O
a O
matrix O
: O
These O
additional O
vectors O
are O
then O
used O
by O
the O
scoring Method
mechanism Method
: O
subsection O
: O
Score Method
Normalization Method
: O
Sharpening Task
and O
Smoothing Task
There O
are O
three O
potential O
issues O
with O
the O
normalization O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
. O
First O
, O
when O
the O
input O
sequence O
is O
long O
, O
the O
glimpse O
is O
likely O
to O
contain O
noisy O
information O
from O
many O
irrelevant O
feature O
vectors O
, O
as O
the O
normalized O
scores O
are O
all O
positive O
and O
sum O
to O
. O
This O
makes O
it O
difficult O
for O
the O
proposed O
ARSG Method
to O
focus O
clearly O
on O
a O
few O
relevant O
frames O
at O
each O
time O
. O
Second O
, O
the O
attention Method
mechanism Method
is O
required O
to O
consider O
all O
the O
frames O
each O
time O
it O
decodes O
a O
single O
output O
while O
decoding O
the O
output O
of O
length O
, O
leading O
to O
a O
computational Metric
complexity Metric
of O
. O
This O
may O
easily O
become O
prohibitively O
expensive O
, O
when O
input O
utterances O
are O
long O
( O
and O
issue O
that O
is O
less O
serious O
for O
machine Task
translation Task
, O
because O
in O
that O
case O
the O
input O
sequence O
is O
made O
of O
words O
, O
not O
of O
20ms O
acoustic O
frames O
) O
. O
The O
other O
side O
of O
the O
coin O
is O
that O
the O
use O
of O
softmax O
normalization O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
prefers O
to O
mostly O
focus O
on O
only O
a O
single O
feature O
vector O
. O
This O
prevents O
the O
model O
from O
aggregating O
multiple O
top O
- O
scored O
frames O
to O
form O
a O
glimpse O
. O
paragraph O
: O
Sharpening O
There O
is O
a O
straightforward O
way O
to O
address O
the O
first O
issue O
of O
a O
noisy O
glimpse O
by O
“ O
sharpening O
” O
the O
scores O
. O
One O
way O
to O
sharpen O
the O
weights O
is O
to O
introduce O
an O
inverse O
temperature O
to O
the O
softmax O
function O
such O
that O
or O
to O
keep O
only O
the O
top O
- O
frames O
according O
to O
the O
scores O
and O
re O
- O
normalize O
them O
. O
These O
sharpening Method
methods Method
, O
however O
, O
still O
requires O
us O
to O
compute O
the O
score O
of O
every O
frame O
each O
time O
( O
) O
, O
and O
they O
worsen O
the O
second O
issue O
, O
of O
overly O
narrow O
focus O
. O
We O
also O
propose O
and O
investigate O
a O
windowing Method
technique Method
. O
At O
each O
time O
, O
the O
attention Method
mechanism Method
considers O
only O
a O
subsequence O
of O
the O
whole O
sequence O
, O
where O
is O
the O
predefined O
window O
width O
and O
is O
the O
median O
of O
the O
alignment O
. O
The O
scores O
for O
are O
not O
computed O
, O
resulting O
in O
a O
lower O
complexity Metric
of O
. O
This O
windowing Method
technique Method
is O
similar O
to O
taking O
the O
top O
- O
frames O
, O
and O
similarly O
, O
has O
the O
effect O
of O
sharpening O
. O
The O
proposed O
sharpening Method
based O
on O
windowing Method
can O
be O
used O
both O
during O
training Task
and O
evaluation Task
. O
Later O
, O
in O
the O
experiments O
, O
we O
only O
consider O
the O
case O
where O
it O
is O
used O
during O
evaluation Task
. O
paragraph O
: O
Smoothing Task
We O
observed O
that O
the O
proposed O
sharpening Method
methods Method
indeed O
helped O
with O
long O
utterances O
. O
However O
, O
all O
of O
them O
, O
and O
especially O
selecting O
the O
frame O
with O
the O
highest O
score O
, O
negatively O
affected O
the O
model O
’s O
performance O
on O
the O
standard O
development O
set O
which O
mostly O
consists O
of O
short O
utterances O
. O
This O
observations O
let O
us O
hypothesize O
that O
it O
is O
helpful O
for O
the O
model O
to O
aggregate O
selections O
from O
multiple O
top O
- O
scored O
frames O
. O
In O
a O
sense O
this O
brings O
more O
diversity O
, O
i.e. O
, O
more O
effective O
training O
examples O
, O
to O
the O
output O
part O
of O
the O
model O
, O
as O
more O
input O
locations O
are O
considered O
. O
To O
facilitate O
this O
effect O
, O
we O
replace O
the O
unbounded O
exponential O
function O
of O
the O
softmax O
function O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
with O
the O
bounded Method
logistic Method
sigmoid Method
such O
that O
This O
has O
the O
effect O
of O
smoothing O
the O
focus O
found O
by O
the O
attention Method
mechanism Method
. O
section O
: O
Related O
Work O
Speech Method
recognizers Method
based O
on O
the O
connectionist Method
temporal Method
classification Method
( O
CTC Method
, O
) O
and O
its O
extension O
, O
RNN Method
Transducer Method
, O
are O
the O
closest O
to O
the O
ARSG Method
model O
considered O
in O
this O
paper O
. O
They O
follow O
earlier O
work O
on O
end Task
- Task
to Task
- Task
end Task
trainable Task
deep Task
learning Task
over O
sequences O
with O
gradient O
signals O
flowing O
through O
the O
alignment Method
process Method
. O
They O
have O
been O
shown O
to O
perform O
well O
on O
the O
phoneme Task
recognition Task
task Task
. O
Furthermore O
, O
the O
CTC Method
was O
recently O
found O
to O
be O
able O
to O
directly O
transcribe O
text O
from O
speech O
without O
any O
intermediate Method
phonetic Method
representation Method
. O
The O
considered O
ARSG Method
is O
different O
from O
both O
the O
CTC Method
and O
RNN Method
Transducer Method
in O
two O
ways O
. O
First O
, O
whereas O
the O
attention Method
mechanism Method
deterministically O
aligns O
the O
input O
and O
the O
output O
sequences O
, O
the O
CTC Method
and O
RNN Method
Transducer Method
treat O
the O
alignment O
as O
a O
latent O
random O
variable O
over O
which O
MAP Method
( Method
maximum Method
a Method
posteriori Method
) Method
inference Method
is O
performed O
. O
This O
deterministic O
nature O
of O
the O
ARSG Method
’s O
alignment O
mechanism O
allows O
beam Method
search Method
procedure Method
to O
be O
simpler O
. O
Furthermore O
, O
we O
empirically O
observe O
that O
a O
much O
smaller O
beam O
width O
can O
be O
used O
with O
the O
deterministic Method
mechanism Method
, O
which O
allows O
faster O
decoding Task
( O
see O
Sec O
. O
[ O
reference O
] O
and O
Fig O
. O
[ O
reference O
] O
) O
. O
Second O
, O
the O
alignment Method
mechanism Method
of O
both O
the O
CTC Method
and O
RNN Method
Transducer Method
is O
constrained O
to O
be O
“ O
monotonic O
” O
to O
keep O
marginalization O
of O
the O
alignment Task
tractable O
. O
On O
the O
other O
hand O
, O
the O
proposed O
attention Method
mechanism Method
can O
result O
in O
non Task
- Task
monotonic Task
alignment Task
, O
which O
makes O
it O
suitable O
for O
a O
larger O
variety O
of O
tasks O
other O
than O
speech Task
recognition Task
. O
A O
hybrid Method
attention Method
model Method
using O
a O
convolution Method
operation Method
was O
also O
proposed O
in O
for O
neural Task
Turing Task
machines Task
( O
NTM Task
) O
. O
At O
each O
time O
step O
, O
the O
NTM Method
computes O
content Method
- Method
based Method
attention Method
weights Method
which O
are O
then O
convolved O
with O
a O
predicted O
shifting O
distribution O
. O
Unlike O
the O
NTM Method
’s Method
approach Method
, O
the O
hybrid Method
mechanism Method
proposed O
here O
lets O
learning O
figure O
out O
how O
the O
content O
- O
based O
and O
location O
- O
based O
addressing O
be O
combined O
by O
a O
deep O
, O
parametric O
function O
( O
see O
Eq O
. O
( O
[ O
reference O
] O
) O
. O
) O
Sukhbaatar O
et O
al O
. O
describes O
a O
similar O
hybrid Method
attention Method
mechanism Method
, O
where O
location O
embeddings O
are O
used O
as O
input O
to O
the O
attention Method
model Method
. O
This O
approach O
has O
an O
important O
disadvantage O
that O
the O
model O
can O
not O
work O
with O
an O
input O
sequence O
longer O
than O
those O
seen O
during O
training O
. O
Our O
approach O
, O
on O
the O
other O
hand O
, O
works O
well O
on O
sequences O
many O
times O
longer O
than O
those O
seen O
during O
training O
( O
see O
Sec O
. O
[ O
reference O
] O
. O
) O
section O
: O
Experimental O
Setup O
We O
closely O
followed O
the O
procedure O
in O
. O
All O
experiments O
were O
performed O
on O
the O
TIMIT Material
corpus Material
. O
We O
used O
the O
train O
- O
dev O
- O
test O
split O
from O
the O
Kaldi Method
TIMIT Method
s5 Method
recipe Method
. O
We O
trained O
on O
the O
standard O
462 O
speaker O
set O
with O
all O
SA O
utterances O
removed O
and O
used O
the O
50 O
speaker O
dev O
set O
for O
early Task
stopping Task
. O
We O
tested O
on O
the O
24 O
speaker O
core O
test O
set O
. O
All O
networks O
were O
trained O
on O
40 O
mel O
- O
scale O
filter O
- O
bank O
features O
together O
with O
the O
energy O
in O
each O
frame O
, O
and O
first O
and O
second O
temporal O
differences O
, O
yielding O
in O
total O
123 O
features O
per O
frame O
. O
Each O
feature O
was O
rescaled O
to O
have O
zero O
mean O
and O
unit O
variance O
over O
the O
training O
set O
. O
Networks O
were O
trained O
on O
the O
full O
61 O
- O
phone O
set O
extended O
with O
an O
extra O
“ O
end O
- O
of O
- O
sequence O
” O
token O
that O
was O
appended O
to O
each O
target O
sequence O
. O
Similarly O
, O
we O
appended O
an O
all O
- O
zero O
frame O
at O
the O
end O
of O
each O
input O
sequence O
to O
indicate O
the O
end O
of O
the O
utterance O
. O
Decoding Task
was O
performed O
using O
the O
61 O
+ O
1 O
phoneme O
set O
, O
while O
scoring Task
was O
done O
on O
the O
39 O
phoneme O
set O
. O
subsection O
: O
Training O
Procedure O
One O
property O
of O
ARSG Method
models O
is O
that O
different O
subsets O
of O
parameters O
are O
reused O
different O
number O
of O
times O
; O
times O
for O
those O
of O
the O
encoder Method
, O
for O
the O
attention O
weights O
and O
times O
for O
all O
the O
other O
parameters O
of O
the O
ARSG Method
. O
This O
makes O
the O
scales O
of O
derivatives O
w.r.t O
. O
parameters O
vary O
significantly O
, O
and O
we O
handle O
it O
by O
using O
an O
adaptive Method
learning Method
rate Method
algorithm Method
, O
AdaDelta Method
which O
has O
two O
hyperparameters O
and O
. O
All O
the O
weight O
matrices O
were O
initialized O
from O
a O
normal Method
Gaussian Method
distribution Method
with O
its O
standard O
deviation O
set O
to O
. O
Recurrent O
weights O
were O
furthermore O
orthogonalized O
. O
As O
TIMIT Task
is O
a O
relatively O
small O
dataset O
, O
proper O
regularization Method
is O
crucial O
. O
We O
used O
the O
adaptive O
weight O
noise O
as O
a O
main O
regularizer Method
. O
We O
first O
trained O
our O
models O
with O
a O
column O
norm O
constraint O
with O
the O
maximum O
norm O
until O
the O
lowest O
development Metric
negative Metric
log Metric
- Metric
likelihood Metric
is O
achieved O
. O
During O
this O
time O
, O
and O
are O
set O
to O
and O
, O
respectively O
. O
At O
this O
point O
, O
we O
began O
using O
the O
adaptive O
weight O
noise O
, O
and O
scaled O
down O
the O
model Metric
complexity Metric
cost Metric
by O
a O
factor O
of O
10 O
, O
while O
disabling O
the O
column O
norm O
constraints O
. O
Once O
the O
new O
lowest O
development O
log O
- O
likelihood O
was O
reached O
, O
we O
fine O
- O
tuned O
the O
model O
with O
a O
smaller O
, O
until O
we O
did O
not O
observe O
the O
improvement O
in O
the O
development O
phoneme Metric
error Metric
rate Metric
( O
PER Metric
) O
for O
100 O
K O
weight Method
updates Method
. O
Batch O
size O
1 O
was O
used O
throughout O
the O
training O
. O
subsection O
: O
Details O
of O
Evaluated O
Models O
We O
evaluated O
the O
ARSGs Method
with O
different O
attention Method
mechanisms Method
. O
The O
encoder Method
was O
a O
3 O
- O
layer O
BiRNN Method
with O
256 O
GRU Method
units Method
in O
each O
direction O
, O
and O
the O
activations O
of O
the O
512 O
top O
- O
layer O
units O
were O
used O
as O
the O
representation O
. O
The O
generator O
had O
a O
single O
recurrent Method
layer Method
of Method
256 Method
GRU Method
units Method
. O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
had O
a O
hidden Method
layer Method
of O
64 O
maxout Method
units Method
. O
The O
initial O
states O
of O
both O
the O
encoder Method
and O
generator Method
were O
treated O
as O
additional O
parameters O
. O
Our O
baseline O
model O
is O
the O
one O
with O
a O
purely O
content Method
- Method
based Method
attention Method
mechanism Method
( O
See O
Eqs O
. O
( O
[ O
reference O
] O
) O
– O
( O
[ O
reference O
] O
) O
. O
) O
The O
scoring O
network O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
had O
512 O
hidden O
units O
. O
The O
other O
two O
models O
use O
the O
convolutional O
features O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
with O
and O
. O
One O
of O
them O
uses O
the O
smoothing Method
from O
Sec O
. O
[ O
reference O
] O
. O
paragraph O
: O
Decoding Method
Procedure Method
A O
left O
- O
to O
- O
right O
beam O
search O
over O
phoneme O
sequences O
was O
used O
during O
decoding Task
. O
Beam Task
search Task
was O
stopped O
when O
the O
“ O
end O
- O
of O
- O
sequence O
” O
token O
was O
emitted O
. O
We O
started O
with O
a O
beam O
width O
of O
10 O
, O
increasing O
it O
up O
to O
40 O
when O
the O
network O
failed O
to O
produce O
with O
the O
narrower O
beam O
. O
As O
shown O
in O
Fig O
. O
[ O
reference O
] O
, O
decoding O
with O
a O
wider O
beam O
gives O
little O
- O
to O
- O
none O
benefit O
. O
section O
: O
Results O
All O
the O
models O
achieved O
competitive O
PERs O
( O
see O
Table O
[ O
reference O
] O
) O
. O
With O
the O
convolutional O
features O
, O
we O
see O
3.7 O
% O
relative O
improvement O
over O
the O
baseline O
and O
further O
5.9 O
% O
with O
the O
smoothing Method
. O
To O
our O
surprise O
( O
see O
Sec O
. O
[ O
reference O
] O
. O
) O
, O
the O
baseline Method
model Method
learned O
to O
align O
properly O
. O
An O
alignment O
produced O
by O
the O
baseline Method
model Method
on O
a O
sequence O
with O
repeated O
phonemes O
( O
utterance O
FDHC0_SX209 O
) O
is O
presented O
in O
Fig O
. O
[ O
reference O
] O
which O
demonstrates O
that O
the O
baseline Method
model Method
is O
not O
confused O
by O
short O
- O
range O
repetitions O
. O
We O
can O
also O
see O
from O
the O
figure O
that O
it O
prefers O
to O
select O
frames O
that O
are O
near O
the O
beginning O
or O
even O
slightly O
before O
the O
phoneme O
location O
provided O
as O
a O
part O
of O
the O
dataset O
. O
The O
alignments O
produced O
by O
the O
other O
models O
were O
very O
similar O
visually O
. O
subsection O
: O
Forced Task
Alignment Task
of Task
Long Task
Utterances Task
The O
good O
performance O
of O
the O
baseline O
model O
led O
us O
to O
the O
question O
of O
how O
it O
distinguishes O
between O
repetitions O
of O
similar O
phoneme O
sequences O
and O
how O
reliably O
it O
decodes O
longer O
sequences O
with O
more O
repetitions O
. O
We O
created O
two O
datasets O
of O
long O
utterances O
; O
one O
by O
repeating O
each O
test O
utterance O
, O
and O
the O
other O
by O
concatenating O
randomly O
chosen O
utterances O
. O
In O
both O
cases O
, O
the O
waveforms O
were O
cross O
- O
faded O
with O
a O
0.05s O
silence O
inserted O
as O
the O
“ O
pau O
” O
phone O
. O
We O
concatenated O
up O
to O
utterances O
. O
First O
, O
we O
checked O
the O
forced Task
alignment Task
with O
these O
longer O
utterances O
by O
forcing O
the O
generator Method
to O
emit O
the O
correct O
phonemes O
. O
Each O
alignment O
was O
considered O
correct O
if O
90 O
% O
of O
the O
alignment O
weight O
lies O
inside O
the O
ground O
- O
truth O
phoneme O
window O
extended O
by O
20 O
frames O
on O
each O
side O
. O
Under O
this O
definition O
, O
all O
phones O
but O
the O
shown O
in O
Fig O
. O
[ O
reference O
] O
are O
properly O
aligned O
. O
The O
first O
column O
of O
Fig O
. O
[ O
reference O
] O
shows O
the O
number O
of O
correctly O
aligned O
frames O
w.r.t O
. O
the O
utterance O
length O
( O
in O
frames O
) O
for O
some O
of O
the O
considered O
models O
. O
One O
can O
see O
that O
the O
baseline O
model O
was O
able O
to O
decode O
sequences O
up O
to O
about O
120 O
phones O
when O
a O
single O
utterance O
was O
repeated O
, O
and O
up O
to O
about O
150 O
phones O
when O
different O
utterances O
were O
concatenated O
. O
Even O
when O
it O
failed O
, O
it O
correctly O
aligned O
about O
50 O
phones O
. O
On O
the O
other O
hand O
, O
the O
model O
with O
the O
hybrid Method
attention Method
mechanism Method
with O
convolutional Method
features Method
was O
able O
to O
align O
sequences O
up O
to O
200 O
phones O
long O
. O
However O
, O
once O
it O
began O
to O
fail O
, O
the O
model O
was O
not O
able O
to O
align O
almost O
all O
phones O
. O
The O
model O
with O
the O
smoothing Method
behaved O
similarly O
to O
the O
one O
with O
convolutional O
features O
only O
. O
We O
examined O
failed O
alignments O
to O
understand O
these O
two O
different O
modes O
of O
failure O
. O
Some O
of O
the O
examples O
are O
shown O
in O
the O
Supplementary O
Materials O
. O
We O
found O
that O
the O
baseline O
model O
properly O
aligns O
about O
40 O
first O
phones O
, O
then O
makes O
a O
jump O
to O
the O
end O
of O
the O
recording O
and O
cycles O
over O
the O
last O
10 O
phones O
. O
This O
behavior O
suggests O
that O
it O
learned O
to O
track O
its O
approximate O
location O
in O
the O
source O
sequence O
. O
However O
, O
the O
tracking Task
capability Task
is O
limited O
to O
the O
lengths O
observed O
during O
training O
. O
Once O
the O
tracker O
saturates O
, O
it O
jumps O
to O
the O
end O
of O
the O
recording O
. O
In O
contrast O
, O
when O
the O
location Method
- Method
aware Method
network Method
failed O
it O
just O
stopped O
aligning O
– O
no O
particular O
frames O
were O
selected O
for O
each O
phone O
. O
We O
attribute O
this O
behavior O
to O
the O
issue O
of O
noisy O
glimpse O
discussed O
in O
Sec O
. O
[ O
reference O
] O
. O
With O
a O
long O
utterance O
there O
are O
many O
irrelevant O
frames O
negatively O
affecting O
the O
weight O
assigned O
to O
the O
correct O
frames O
. O
In O
line O
with O
this O
conjecture O
, O
the O
location Method
- Method
aware Method
network Method
works O
slightly O
better O
on O
the O
repetition O
of O
the O
same O
utterance O
, O
where O
all O
frames O
are O
somehow O
relevant O
, O
than O
on O
the O
concatenation O
of O
different O
utterances O
, O
where O
each O
misaligned O
frame O
is O
irrelevant O
. O
To O
gain O
more O
insight O
we O
applied O
the O
alignment Method
sharpening Method
schemes Method
described O
in O
Sec O
. O
[ O
reference O
] O
. O
In O
the O
remaining O
columns O
of O
Fig O
. O
[ O
reference O
] O
, O
we O
see O
that O
the O
sharpening Method
methods Method
help O
the O
location Method
- Method
aware Method
network Method
to O
find O
proper O
alignments O
, O
while O
they O
show O
little O
effect O
on O
the O
baseline O
network O
. O
The O
windowing Method
technique Method
helps O
both O
the O
baseline Method
and Method
location Method
- Method
aware Method
networks Method
, O
with O
the O
location Method
- Method
aware Method
network Method
properly O
aligning O
nearly O
all O
sequences O
. O
During O
visual Task
inspection Task
, O
we O
noticed O
that O
in O
the O
middle O
of O
very O
long O
utterances O
the O
baseline Method
model Method
was O
confused O
by O
repetitions O
of O
similar O
content O
within O
the O
window O
, O
and O
that O
such O
confusions O
did O
not O
happen O
in O
the O
beginning O
. O
This O
supports O
our O
conjecture O
above O
. O
subsection O
: O
Decoding Task
Long Task
Utterances Task
We O
evaluated O
the O
models O
on O
long O
sequences O
. O
Each O
model O
was O
decoded O
using O
the O
alignment Method
sharpening Method
techniques Method
that O
helped O
to O
obtain O
proper O
forced O
alignments O
. O
The O
results O
are O
presented O
in O
Fig O
. O
[ O
reference O
] O
. O
The O
baseline Method
model Method
fails O
to O
decode O
long O
utterances O
, O
even O
when O
a O
narrow O
window O
is O
used O
to O
constrain O
the O
alignments O
it O
produces O
. O
The O
two O
other O
location Method
- Method
aware Method
networks Method
are O
able O
to O
decode O
utterances O
formed O
by O
concatenating O
up O
to O
11 O
test O
utterances O
. O
Better O
results O
were O
obtained O
with O
a O
wider O
window O
, O
presumably O
because O
it O
resembles O
more O
the O
training O
conditions O
when O
at O
each O
step O
the O
attention Method
mechanism Method
was O
seeing O
the O
whole O
input O
sequence O
. O
With O
the O
wide O
window O
, O
both O
of O
the O
networks O
scored O
about O
20 O
% O
PER Metric
on O
the O
long O
utterances O
, O
indicating O
that O
the O
proposed O
location Method
- Method
aware Method
attention Method
mechanism Method
can O
scale O
to O
sequences O
much O
longer O
than O
those O
in O
the O
training O
set O
with O
only O
minor O
modifications O
required O
at O
the O
decoding Method
stage Method
. O
section O
: O
Conclusions O
We O
proposed O
and O
evaluated O
a O
novel O
end Task
- Task
to Task
- Task
end Task
trainable Task
speech Task
recognition Task
architecture Task
based O
on O
a O
hybrid Method
attention Method
mechanism Method
which O
combines O
both O
content O
and O
location O
information O
in O
order O
to O
select O
the O
next O
position O
in O
the O
input O
sequence O
for O
decoding Task
. O
One O
desirable O
property O
of O
the O
proposed O
model O
is O
that O
it O
can O
recognize O
utterances O
much O
longer O
than O
the O
ones O
it O
was O
trained O
on O
. O
In O
the O
future O
, O
we O
expect O
this O
model O
to O
be O
used O
to O
directly O
recognize Task
text Task
from O
speech O
, O
in O
which O
case O
it O
may O
become O
important O
to O
incorporate O
a O
monolingual Method
language Method
model Method
to O
the O
ARSG Method
architecture O
. O
This O
work O
has O
contributed O
two O
novel O
ideas O
for O
attention Method
mechanisms Method
: O
a O
better O
normalization Method
approach Method
yielding O
smoother O
alignments O
and O
a O
generic O
principle O
for O
extracting O
and O
using O
features O
from O
the O
previous O
alignments O
. O
Both O
of O
these O
can O
potentially O
be O
applied O
beyond O
speech Task
recognition Task
. O
For O
instance O
, O
the O
proposed O
attention Method
can O
be O
used O
without O
modification O
in O
neural Method
Turing Method
machines Method
, O
or O
by O
using O
2–D Method
convolution Method
instead O
of O
1–D Method
, O
for O
improving O
image Task
caption Task
generation Task
. O
subsubsection O
: O
Acknowledgments O
All O
experiments O
were O
conducted O
using O
Theano O
, O
PyLearn2 Method
, Method
and Method
Blocks Method
libraries Method
. O
The O
authors O
would O
like O
to O
acknowledge O
the O
support O
of O
the O
following O
agencies O
for O
research O
funding O
and O
computing O
support O
: O
National O
Science O
Center O
( O
Poland O
) O
, O
NSERC O
, O
Calcul O
Québec O
, O
Compute O
Canada O
, O
the O
Canada O
Research O
Chairs O
and O
CIFAR O
. O
Bahdanau O
also O
thanks O
Planet Task
Intelligent Task
Systems Task
GmbH O
and O
Yandex O
. O
bibliography O
: O
References O
appendix O
: O
Additional O
Figures O
Baseline Method
Convolutional Method
Features Method
Smooth Method
Focus Method
Baseline Method
Convolutional Method
Features Method
Smooth Method
Focus Method
appendix O
: O
Detailed O
results O
of O
experiments O
