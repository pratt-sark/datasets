document	O
:	O
10	O
,	O
000	O
+	O
Times	O
Accelerated	Method
Robust	Method
Subset	Method
Selection	Method
(	O
ARSS	Method
)	O
Subset	Task
selection	Task
from	O
massive	O
data	O
with	O
noised	O
information	O
is	O
increasingly	O
popular	O
for	O
various	O
applications	O
.	O
This	O
problem	O
is	O
still	O
highly	O
challenging	O
as	O
current	O
methods	O
are	O
generally	O
slow	O
in	O
speed	O
and	O
sensitive	O
to	O
outliers	O
.	O
To	O
address	O
the	O
above	O
two	O
issues	O
,	O
we	O
propose	O
an	O
accelerated	Method
robust	Method
subset	Method
selection	Method
(	O
ARSS	Method
)	O
method	O
.	O
Specifically	O
in	O
the	O
subset	Task
selection	Task
area	Task
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
employ	O
the	O
-	Metric
norm	Metric
based	Metric
measure	Metric
for	O
the	O
representation	Task
loss	Task
,	O
preventing	O
large	O
errors	O
from	O
dominating	O
our	O
objective	O
.	O
As	O
a	O
result	O
,	O
the	O
robustness	Metric
against	O
outlier	O
elements	O
is	O
greatly	O
enhanced	O
.	O
Actually	O
,	O
data	O
size	O
is	O
generally	O
much	O
larger	O
than	O
feature	O
length	O
,	O
i.e.	O
.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
propose	O
a	O
speedup	Method
solver	Method
(	O
via	O
ALM	Method
and	Method
equivalent	Method
derivations	Method
)	O
to	O
highly	O
reduce	O
the	O
computational	Metric
cost	Metric
,	O
theoretically	O
from	O
to	O
.	O
Extensive	O
experiments	O
on	O
ten	O
benchmark	O
datasets	O
verify	O
that	O
our	O
method	O
not	O
only	O
outperforms	O
state	O
of	O
the	O
art	O
methods	O
,	O
but	O
also	O
runs	O
10	O
,	O
000	O
+	O
times	O
faster	O
than	O
the	O
most	O
related	O
method	O
.	O
section	O
:	O
Introduction	O
Due	O
to	O
the	O
explosive	O
growth	O
of	O
data	O
,	O
subset	Method
selection	Method
methods	Method
are	O
increasingly	O
popular	O
for	O
a	O
wide	O
range	O
of	O
machine	Task
learning	Task
and	Task
computer	Task
vision	Task
applications	Task
.	O
This	O
kind	O
of	O
methods	O
offer	O
the	O
potential	O
to	O
select	O
a	O
few	O
highly	O
representative	O
samples	O
or	O
exemplars	O
to	O
describe	O
the	O
entire	O
dataset	O
.	O
By	O
analyzing	O
a	O
few	O
,	O
we	O
can	O
roughly	O
know	O
all	O
.	O
Such	O
case	O
is	O
very	O
important	O
to	O
summarize	O
and	O
visualize	O
huge	O
datasets	O
of	O
texts	O
,	O
images	O
and	O
videos	O
etc	O
.	O
.	O
Besides	O
,	O
by	O
only	O
using	O
the	O
selected	O
exemplars	O
for	O
succeeding	O
tasks	O
,	O
the	O
cost	O
of	O
memories	Metric
and	O
computational	Metric
time	Metric
will	O
be	O
greatly	O
reduced	O
.	O
Additionally	O
,	O
as	O
outliers	O
are	O
generally	O
less	O
representative	O
,	O
the	O
side	O
effect	O
of	O
outliers	O
will	O
be	O
reduced	O
,	O
thus	O
boosting	O
the	O
performance	O
of	O
subsequent	O
applications	O
.	O
There	O
have	O
been	O
several	O
subset	Method
selection	Method
methods	Method
.	O
The	O
most	O
intuitional	O
method	O
is	O
to	O
randomly	O
select	O
a	O
fixed	O
number	O
of	O
samples	O
.	O
Although	O
highly	O
efficient	O
,	O
there	O
is	O
no	O
guarantee	O
for	O
an	O
effective	O
selection	Task
.	O
For	O
the	O
other	O
methods	O
,	O
depending	O
on	O
the	O
mechanism	O
of	O
representative	O
exemplars	O
,	O
there	O
are	O
mainly	O
three	O
categories	O
of	O
selection	Method
methods	Method
.	O
One	O
category	O
relies	O
on	O
the	O
assumption	O
that	O
the	O
data	O
points	O
lie	O
in	O
one	O
or	O
multiple	O
low	O
-	O
dimensional	O
subspaces	O
.	O
Specifically	O
,	O
the	O
Rank	Method
Revealing	Method
QR	Method
(	O
RRQR	Method
)	O
selects	O
the	O
subsets	O
that	O
give	O
the	O
best	O
conditional	O
sub	O
-	O
matrix	O
.	O
Unfortunately	O
,	O
this	O
method	O
has	O
suboptimal	O
properties	O
,	O
as	O
it	O
is	O
not	O
assured	O
to	O
find	O
the	O
globally	O
optimum	O
in	O
polynomial	O
time	O
.	O
Another	O
category	O
assumes	O
that	O
the	O
samples	O
are	O
distributed	O
around	O
centers	O
.	O
The	O
center	O
or	O
its	O
nearest	O
neighbour	O
are	O
selected	O
as	O
exemplars	O
.	O
Perhaps	O
,	O
Kmeans	Method
and	O
Kmedoids	Method
are	O
the	O
most	O
typical	O
methods	O
(	O
Kmedoids	Method
is	O
a	O
variant	O
of	O
Kmeans	Method
)	O
.	O
Both	O
methods	O
employ	O
an	O
EM	Method
-	Method
like	Method
algorithm	Method
.	O
Thus	O
,	O
the	O
results	O
depend	O
tightly	O
on	O
the	O
initialization	O
,	O
and	O
they	O
are	O
highly	O
unstable	O
for	O
large	O
(	O
i.e.	O
the	O
number	O
of	O
centers	O
or	O
selected	O
samples	O
)	O
.	O
Recently	O
,	O
there	O
are	O
a	O
few	O
methods	O
that	O
assume	O
exemplars	O
are	O
the	O
samples	O
that	O
can	O
best	O
represent	O
the	O
whole	O
dataset	O
.	O
However	O
,	O
for	O
,	O
the	O
optimization	Task
is	O
a	O
combinatorial	Task
problem	Task
(	O
NP	Task
-	Task
hard	Task
)	O
,	O
which	O
is	O
computationally	O
intractable	O
to	O
solve	O
.	O
Besides	O
,	O
the	O
representation	O
loss	O
is	O
measured	O
by	O
the	O
least	Method
square	Method
measure	Method
,	O
which	O
is	O
sensitive	O
to	O
outliers	O
in	O
data	O
.	O
Then	O
improves	O
by	O
employing	O
a	O
robust	Method
loss	Method
via	O
the	O
-	Method
norm	Method
;	O
the	O
-	Method
norm	Method
is	O
applied	O
to	O
samples	O
,	O
and	O
the	O
-	O
norm	O
is	O
used	O
for	O
features	O
.	O
In	O
this	O
way	O
,	O
the	O
side	O
effect	O
of	O
outlier	O
samples	O
is	O
relieved	O
.	O
The	O
solver	O
of	O
is	O
theoretically	O
perfect	O
due	O
to	O
its	O
ability	O
of	O
convergence	O
to	O
global	O
optima	O
.	O
Unfortunately	O
,	O
in	O
terms	O
of	O
computational	Metric
costs	Metric
,	O
the	O
solver	O
is	O
highly	O
complex	O
.	O
It	O
takes	O
for	O
one	O
iteration	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
This	O
is	O
infeasible	O
for	O
the	O
case	O
of	O
large	O
(	O
e.g.	O
it	O
takes	O
2000	O
+	O
hours	O
for	O
a	O
case	O
of	O
)	O
.	O
Moreover	O
,	O
the	O
representation	Method
loss	Method
is	O
only	O
robust	O
against	O
outlier	O
samples	O
.	O
Such	O
case	O
is	O
worth	O
improvement	O
,	O
as	O
there	O
may	O
exist	O
outlier	O
elements	O
in	O
real	O
data	O
.	O
paragraph	O
:	O
Contributions	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
accelerated	Method
robust	Method
subset	Method
selection	Method
method	Method
to	O
highly	O
raise	O
the	O
speed	O
on	O
the	O
one	O
hand	O
,	O
and	O
to	O
boost	O
the	O
robustness	Metric
on	O
the	O
other	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
the	O
-	Metric
norm	Metric
based	Metric
robust	Metric
measure	Metric
for	O
the	O
representation	Task
loss	Task
,	O
preventing	O
large	O
errors	O
from	O
dominating	O
our	O
objective	O
.	O
As	O
a	O
result	O
,	O
the	O
robustness	Metric
against	O
outliers	O
is	O
greatly	O
boosted	O
.	O
Then	O
,	O
based	O
on	O
the	O
observation	O
that	O
data	O
size	O
is	O
generally	O
much	O
larger	O
than	O
feature	O
length	O
,	O
i.e.	O
,	O
we	O
propose	O
a	O
speedup	Method
solver	Method
.	O
The	O
main	O
acceleration	O
is	O
owing	O
to	O
the	O
Augmented	Method
Lagrange	Method
Multiplier	Method
(	O
ALM	Method
)	O
and	O
an	O
equivalent	O
derivation	O
.	O
Via	O
them	O
,	O
we	O
reduce	O
the	O
computational	Metric
complexity	Metric
from	O
to	O
.	O
Extensive	O
results	O
on	O
ten	O
benchmark	O
datasets	O
demonstrate	O
that	O
in	O
average	O
,	O
our	O
method	O
is	O
10	O
,	O
000	O
+	O
times	O
faster	O
than	O
Nie	Method
’s	Method
method	Method
.	O
The	O
selection	Metric
quality	Metric
is	O
highly	O
encouraging	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
Additionally	O
,	O
via	O
another	O
equivalent	O
derivation	O
,	O
we	O
give	O
an	O
accelerated	Method
solver	Method
for	O
Nie	Method
’s	Method
method	Method
,	O
theoretically	O
reducing	O
the	O
computational	Metric
complexity	Metric
from	O
to	O
as	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
,	O
empirically	O
obtaining	O
a	O
500	O
+	O
times	O
speedup	O
compared	O
with	O
the	O
authorial	Method
solver	Method
.	O
paragraph	O
:	O
Notations	O
.	O
We	O
use	O
boldface	O
uppercase	O
letters	O
to	O
denote	O
matrices	O
and	O
boldface	O
lowercase	O
letters	O
to	O
represent	O
vectors	O
.	O
For	O
a	O
matrix	O
,	O
we	O
denote	O
its	O
row	O
and	O
column	O
as	O
and	O
respectively	O
.	O
The	O
-	O
norm	O
of	O
a	O
matrix	O
is	O
defined	O
as	O
.	O
The	O
-	O
norm	O
of	O
a	O
matrix	O
is	O
defined	O
as	O
;	O
thus	O
,	O
we	O
have	O
.	O
section	O
:	O
Subset	Task
Selection	Task
via	O
Self	Method
-	Method
Representation	Method
In	O
the	O
problem	O
of	O
subset	Task
selection	Task
,	O
we	O
are	O
often	O
given	O
a	O
set	O
of	O
unlabelled	O
points	O
,	O
where	O
is	O
the	O
feature	O
length	O
.	O
The	O
goal	O
is	O
to	O
select	O
the	O
top	O
most	O
representative	O
and	O
informative	O
samples	O
(	O
i.e.	O
exemplars	O
)	O
to	O
effectively	O
describe	O
the	O
entire	O
dataset	O
.	O
By	O
solely	O
using	O
these	O
exemplars	O
for	O
subsequent	O
tasks	O
,	O
we	O
could	O
greatly	O
reduce	O
the	O
computational	Metric
costs	Metric
and	O
largely	O
alleviate	O
the	O
side	O
effects	O
of	O
outlier	O
elements	O
in	O
data	O
.	O
Such	O
a	O
motivation	O
could	O
be	O
formulated	O
as	O
the	O
Transductive	Method
Experimental	Method
Design	Method
(	Method
TED	Method
)	Method
model	Method
:	O
where	O
is	O
the	O
selected	O
subset	O
matrix	O
,	O
whose	O
column	O
vectors	O
all	O
come	O
from	O
,	O
i.e.	O
;	O
is	O
the	O
corresponding	O
linear	O
combination	O
coefficients	O
.	O
By	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
,	O
TED	O
could	O
select	O
the	O
highly	O
informative	O
and	O
representative	O
samples	O
,	O
as	O
they	O
have	O
to	O
well	O
represent	O
all	O
the	O
samples	O
in	O
.	O
Although	O
TED	Method
(	O
[	O
reference	O
]	O
)	O
is	O
well	O
modeled	O
—	O
very	O
accurate	O
and	O
intuitive	O
,	O
there	O
are	O
two	O
bottlenecks	O
.	O
First	O
,	O
the	O
objective	O
is	O
a	O
combinatorial	Task
optimization	Task
problem	Task
.	O
It	O
is	O
NP	O
-	O
hard	O
to	O
exhaustively	O
search	O
the	O
optimal	O
subset	O
from	O
.	O
For	O
this	O
reason	O
,	O
the	O
author	O
approximate	O
(	O
[	O
reference	O
]	O
)	O
via	O
a	O
sequential	Task
optimization	Task
problem	Task
,	O
which	O
is	O
solved	O
by	O
an	O
inefficient	O
greedy	Method
optimization	Method
algorithm	Method
.	O
Second	O
,	O
similar	O
to	O
the	O
existing	O
least	Method
square	Method
loss	Method
based	Method
models	Method
in	O
machine	Task
learning	Task
and	Task
statistics	Task
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
sensitive	O
to	O
the	O
presence	O
of	O
outliers	O
.	O
Accordingly	O
,	O
Nie	O
et	O
al	O
.	O
propose	O
a	O
new	O
model	O
(	O
RRSS	Method
)	O
:	O
where	O
is	O
a	O
nonnegative	O
parameter	O
;	O
is	O
constrained	O
to	O
be	O
row	O
-	O
sparse	O
,	O
and	O
thus	O
to	O
select	O
the	O
most	O
representative	O
and	O
informative	O
samples	O
.	O
As	O
the	O
representation	O
loss	O
is	O
accumulated	O
via	O
the	O
-	O
norm	O
among	O
samples	O
,	O
compared	O
with	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
robustness	Metric
against	O
outlier	O
samples	O
is	O
enhanced	O
.	O
Equivalently	O
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
rewritten	O
in	O
the	O
matrix	O
format	O
:	O
Since	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
is	O
convex	O
in	O
,	O
the	O
global	O
minimum	O
may	O
be	O
found	O
by	O
differentiating	O
(	O
[	O
reference	O
]	O
)	O
and	O
setting	O
the	O
derivative	O
to	O
zero	O
,	O
resulting	O
in	O
a	O
linear	Method
system	Method
where	O
is	O
a	O
diagonal	O
matrix	O
with	O
the	O
diagonal	O
entry	O
as	O
and	O
.	O
It	O
seems	O
perfect	O
to	O
use	O
(	O
[	O
reference	O
]	O
)	O
to	O
solve	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
,	O
because	O
(	O
[	O
reference	O
]	O
)	O
looks	O
simple	O
and	O
the	O
global	O
optimum	O
is	O
theoretically	O
guaranteed	O
.	O
Unfortunately	O
,	O
in	O
terms	O
of	O
speed	Metric
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
usually	O
infeasible	O
due	O
to	O
the	O
incredible	O
computational	Metric
demand	Metric
in	O
the	O
case	O
of	O
large	O
(	O
the	O
number	O
of	O
samples	O
)	O
.	O
At	O
each	O
iteration	O
,	O
the	O
computational	Metric
complexity	Metric
of	O
(	O
[	O
reference	O
]	O
)	O
is	O
up	O
to	O
,	O
as	O
analyzed	O
in	O
Remark	O
[	O
reference	O
]	O
.	O
According	O
to	O
our	O
experiments	O
,	O
the	O
time	Metric
cost	Metric
is	O
up	O
to	O
2088	O
hours	O
(	O
i.e.	O
87	O
days	O
)	O
for	O
a	O
subset	Task
selection	Task
problem	Task
of	O
13000	O
samples	O
.	O
theorem	O
:	O
.	O
Since	O
∈	O
+	O
⁢U⁢nnXTX⁢γVR×NN	O
,	O
the	O
major	O
computational	Metric
cost	Metric
of	O
(	Method
)	Method
focuses	O
on	O
a	O
×NN	Method
linear	Method
system	Method
.	O
If	O
solved	O
by	O
the	O
Cholesky	Method
factorization	Method
method	Method
,	O
it	O
costs	O
⁢13N3	O
for	O
factorization	Task
as	O
well	O
as	O
⁢2N2	O
for	O
forward	Task
and	Task
backward	Task
substitution	Task
.	O
This	O
amounts	O
to	O
⁢O	O
(	O
N3	O
)	O
in	O
total	O
.	O
By	O
now	O
,	O
we	O
only	O
solve	O
an	O
.	O
Once	O
solving	O
all	O
the	O
set	O
of	O
{	O
an}=n1N	O
,	O
the	O
total	O
complexity	Metric
amounts	O
to	O
⁢O	O
(	O
N4	O
)	O
for	O
one	O
iteration	O
step	O
.	O
section	O
:	O
Accelerated	Method
Robust	Method
Subset	Method
Selection	Method
(	O
ARSS	Method
)	O
Due	O
to	O
the	O
huge	O
computational	Metric
costs	Metric
,	O
Nie	Method
’s	Method
method	Method
is	O
infeasible	O
for	O
the	O
case	O
of	O
large	O
—the	O
computational	Metric
time	Metric
is	O
up	O
to	O
2088	O
hours	O
for	O
a	O
case	O
of	O
13000	O
samples	O
.	O
Besides	O
,	O
Nie	Method
’s	Method
model	Method
(	O
[	O
reference	O
]	O
)	O
imposes	O
the	O
-	O
norm	O
among	O
features	O
,	O
which	O
is	O
prone	O
to	O
outliers	O
in	O
features	O
.	O
To	O
tackle	O
the	O
above	O
two	O
issues	O
,	O
we	O
propose	O
a	O
more	O
robust	O
model	O
in	O
the	O
-	O
norm	O
.	O
Although	O
the	O
resulted	O
objective	O
is	O
challenging	O
to	O
solve	O
,	O
a	O
speedup	Method
algorithm	Method
is	O
proposed	O
to	O
dramatically	O
save	O
the	O
computational	Metric
costs	Metric
.	O
For	O
the	O
same	O
task	O
of	O
,	O
it	O
costs	O
our	O
method	O
1.8	O
minutes	O
,	O
achieving	O
a	O
68429	O
times	O
acceleration	O
compared	O
with	O
the	O
speed	O
of	O
Nie	Method
’s	Method
method	Method
.	O
subsubsection	Method
:	O
Modeling	O
.	O
To	O
boost	O
the	O
robustness	Metric
against	O
outliers	O
in	O
both	O
samples	O
and	O
features	O
,	O
we	O
formulate	O
the	O
discrepancy	O
between	O
and	O
via	O
the	O
-	O
norm	O
.	O
There	O
are	O
theoretical	O
and	O
empirical	O
evidences	O
to	O
verify	O
that	O
compared	O
with	O
or	O
norms	O
,	O
the	O
-	O
norm	O
is	O
more	O
able	O
to	O
prevent	O
outlier	O
elements	O
from	O
dominating	O
the	O
objective	O
,	O
enhancing	O
the	O
robustness	Metric
.	O
Thus	O
,	O
we	O
have	O
the	O
following	O
objective	O
where	O
is	O
a	O
balancing	O
parameter	O
;	O
is	O
a	O
row	O
sparse	O
matrix	O
,	O
used	O
to	O
select	O
the	O
most	O
informative	O
and	O
representative	O
samples	O
.	O
By	O
minimizing	O
the	O
energy	O
of	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
could	O
capture	O
the	O
most	O
essential	O
properties	O
of	O
the	O
dataset	O
.	O
After	O
obtaining	O
the	O
optimal	O
,	O
the	O
row	O
indexes	O
are	O
sorted	O
by	O
the	O
row	O
-	O
sum	O
value	O
of	O
the	O
absolute	O
in	O
decreasing	O
order	O
.	O
The	O
samples	O
specified	O
by	O
the	O
top	O
indexes	O
are	O
selected	O
as	O
exemplars	O
.	O
Note	O
that	O
the	O
model	O
(	O
[	O
reference	O
]	O
)	O
could	O
be	O
applied	O
to	O
the	O
unsupervised	Task
feature	Task
selection	Task
problem	Task
by	O
only	O
transposing	O
the	O
data	O
matrix	O
.	O
In	O
this	O
case	O
,	O
is	O
a	O
row	O
sparse	O
matrix	O
,	O
used	O
to	O
select	O
the	O
most	O
representative	O
features	O
.	O
subsection	O
:	O
Accelerated	Method
Solver	Method
for	O
the	O
ARSS	Method
Objective	O
in	O
(	O
[	O
reference	O
]	O
)	O
Although	O
objective	O
(	O
[	O
reference	O
]	O
)	O
is	O
challenging	O
to	O
solve	O
,	O
we	O
propose	O
an	O
effective	O
and	O
highly	O
efficient	O
solver	O
.	O
The	O
acceleration	O
owes	O
to	O
the	O
ALM	Method
and	O
an	O
equivalent	O
derivation	O
.	O
paragraph	O
:	O
ALM	Method
The	O
most	O
intractable	O
challenge	O
of	O
(	O
[	O
reference	O
]	O
)	O
is	O
that	O
,	O
the	O
-	O
norm	O
is	O
non	O
-	O
convex	O
,	O
non	O
-	O
smooth	O
and	O
not	O
-	O
differentiable	O
at	O
the	O
zero	O
point	O
.	O
Therefore	O
,	O
it	O
is	O
beneficial	O
to	O
use	O
the	O
Augmented	Method
Lagrangian	Method
Method	Method
(	O
ALM	Method
)	O
to	O
solve	O
(	O
[	O
reference	O
]	O
)	O
,	O
resulting	O
in	O
several	O
easily	O
tackled	O
unconstrained	Task
subproblems	Task
.	O
By	O
solving	O
them	O
iteratively	O
,	O
the	O
solutions	O
of	O
subproblems	O
could	O
eventually	O
converge	O
to	O
a	O
minimum	O
.	O
Specifically	O
,	O
we	O
introduce	O
an	O
auxiliary	O
variable	O
.	O
Thus	O
,	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
becomes	O
:	O
To	O
deal	O
with	O
the	O
equality	O
constraint	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
most	O
convenient	O
method	O
is	O
to	O
add	O
a	O
penalty	O
,	O
resulting	O
in	O
where	O
is	O
a	O
penalty	O
parameter	O
.	O
To	O
guarantee	O
the	O
equality	O
constraint	O
,	O
it	O
requires	O
approaching	O
infinity	O
,	O
which	O
may	O
cause	O
bad	O
numerical	O
conditions	O
.	O
Instead	O
,	O
once	O
introducing	O
a	O
Lagrangian	O
multiplier	O
,	O
it	O
is	O
no	O
longer	O
requiring	O
.	O
Thus	O
,	O
we	O
rewrite	O
(	O
[	O
reference	O
]	O
)	O
into	O
the	O
standard	O
ALM	Method
formulation	Method
as	O
:	O
where	O
consists	O
of	O
Lagrangian	O
multipliers	O
.	O
In	O
the	O
following	O
,	O
a	O
highly	O
efficient	O
solver	O
will	O
be	O
given	O
.	O
paragraph	O
:	O
The	O
updating	Method
rule	Method
for	O
Similar	O
to	O
the	O
iterative	Method
thresholding	Method
(	O
IT	Method
)	O
in	O
,	O
the	O
degree	O
of	O
violations	O
of	O
the	O
equality	O
constraints	O
are	O
used	O
to	O
update	O
the	O
Lagrangian	O
multiplier	O
:	O
where	O
is	O
a	O
monotonically	O
increasing	O
parameter	O
over	O
iteration	O
steps	O
.	O
For	O
example	O
,	O
,	O
where	O
is	O
a	O
predefined	O
parameter	O
.	O
paragraph	Method
:	O
Efficient	O
solver	O
for	O
Removing	O
irrelevant	O
terms	O
with	O
from	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
where	O
.	O
According	O
to	O
the	O
definition	O
of	O
the	O
-	O
norm	O
and	O
the	O
Frobenius	O
-	O
norm	O
,	O
(	O
[	O
reference	O
]	O
)	O
could	O
be	O
decoupled	O
into	O
independent	O
and	O
unconstrained	O
subproblems	O
.	O
The	O
standard	O
form	O
of	O
these	O
subproblems	O
is	O
where	O
is	O
a	O
given	O
positive	O
parameter	O
,	O
is	O
the	O
scalar	O
variable	O
need	O
to	O
deal	O
with	O
,	O
is	O
a	O
known	O
scalar	O
constant	O
.	O
Zuo	O
et	O
al	O
.	O
has	O
recently	O
proposed	O
a	O
generalized	Method
iterative	Method
shrinkage	Method
algorithm	Method
to	O
solve	O
(	O
[	O
reference	O
]	O
)	O
.	O
This	O
algorithm	O
is	O
easy	O
to	O
implement	O
and	O
able	O
to	O
achieve	O
more	O
accurate	O
solutions	O
than	O
current	O
methods	O
.	O
Thus	O
,	O
we	O
use	O
it	O
for	O
our	O
problem	O
as	O
:	O
where	O
;	O
is	O
obtained	O
by	O
solving	O
the	O
following	O
equation	O
:	O
which	O
could	O
be	O
solved	O
efficiently	O
via	O
an	O
iterative	Method
algorithm	Method
.	O
In	O
this	O
manner	O
,	O
(	O
[	O
reference	O
]	O
)	O
could	O
be	O
sovled	O
extremely	O
fast	O
.	O
paragraph	O
:	O
Accelerated	Method
solver	Method
for	O
The	O
main	O
acceleration	O
focuses	O
on	O
the	O
solver	O
of	O
.	O
Removing	O
irrelevant	O
terms	O
with	O
from	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
where	O
is	O
a	O
nonnegative	O
parameter	O
,	O
.	O
Since	O
(	O
[	O
reference	O
]	O
)	O
is	O
convex	O
in	O
,	O
the	O
optimum	O
could	O
be	O
found	O
by	O
differentiating	O
(	O
[	O
reference	O
]	O
)	O
and	O
setting	O
the	O
derivative	O
to	O
zero	O
.	O
This	O
amounts	O
to	O
tackling	O
the	O
following	O
linear	Method
system	Method
:	O
As	O
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
mainly	O
a	O
linear	Method
system	Method
.	O
Once	O
solved	O
by	O
the	O
Cholesky	Method
factorization	Method
,	O
the	O
computational	Metric
complexity	Metric
is	O
highly	O
up	O
to	O
.	O
This	O
is	O
by	O
no	O
means	O
a	O
good	O
choice	O
for	O
real	O
applications	O
with	O
large	O
.	O
In	O
the	O
following	O
,	O
an	O
equivalent	O
derivation	O
of	O
(	O
[	O
reference	O
]	O
)	O
will	O
be	O
proposed	O
to	O
significantly	O
save	O
the	O
computational	Metric
complexity	Metric
.	O
theorem	O
:	O
.	O
The	O
×NN	Method
linear	Method
system	Method
(	O
)	O
is	O
equivalent	O
to	O
the	O
following	O
×LL	Method
linear	Method
system	Method
:	O
where	O
IL	Method
is	O
a	O
×LL	O
identity	O
matrix	O
.	O
proof	O
:	O
Proof	O
.	O
Note	O
that	O
is	O
a	O
diagonal	O
and	O
positive	O
-	O
definite	O
matrix	O
,	O
the	O
exponent	O
of	O
is	O
efficient	O
to	O
achieve	O
,	O
i.e.	O
.	O
We	O
have	O
the	O
following	O
equations	O
where	O
,	O
is	O
a	O
identity	O
matrix	O
.	O
The	O
following	O
equation	O
holds	O
for	O
any	O
conditions	O
Multiplying	O
(	O
[	O
reference	O
]	O
)	O
with	O
on	O
the	O
left	O
and	O
on	O
the	O
right	O
of	O
both	O
sides	O
of	O
the	O
equal	O
-	O
sign	O
,	O
we	O
have	O
the	O
equation	O
as	O
:	O
Therefore	O
,	O
substituting	O
(	O
[	O
reference	O
]	O
)	O
and	O
into	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
the	O
simplified	O
updating	O
rule	O
as	O
:	O
When	O
,	O
the	O
most	O
complex	O
operation	O
is	O
the	O
matrix	O
multiplications	O
,	O
not	O
the	O
linear	O
system.∎	O
theorem	O
:	O
.	O
We	O
have	O
two	O
equivalent	O
updating	Method
rules	Method
(	O
)	O
and	O
(	O
)	O
for	O
the	O
objective	O
(	O
)	O
.	O
If	O
using	O
(	O
)	O
when	O
≤NL	O
,	O
and	O
otherwise	O
using	O
(	O
)	O
as	O
shown	O
in	O
Algorithm	O
,	O
the	O
computational	Metric
complexity	Metric
of	O
solvers	O
for	O
(	O
)	O
is	O
⁢O	O
(	O
⁢NL2	O
)	O
.	O
Due	O
to	O
≫NL	O
,	O
we	O
have	O
highly	O
reduced	O
the	O
complexity	Metric
from	O
O	O
(	O
N	O
)	O
4	O
to	O
⁢O	O
(	O
⁢NL2	O
)	O
compared	O
with	O
Nie	Method
’s	Method
method	Method
.	O
Input	O
:	O
[	O
1	O
]	O
update	O
via	O
the	O
updating	Method
rule	Method
(	O
[	O
reference	O
]	O
)	O
,	O
that	O
is	O
.	O
update	O
via	O
the	O
updating	Method
rule	Method
(	O
[	O
reference	O
]	O
)	O
,	O
that	O
is	O
,	O
where	O
.	O
Output	O
:	O
A	O
Input	O
:	O
[	O
1	O
]	O
Initialize	O
.	O
update	O
by	O
the	O
updating	Method
rule	Method
(	O
[	O
reference	O
]	O
)	O
.	O
update	O
.	O
;	O
is	O
a	O
identity	O
matrix	O
.	O
via	O
Algorithm	O
.	O
update	O
by	O
the	O
updating	Method
rule	Method
(	O
[	O
reference	O
]	O
)	O
,	O
.	O
convergence	O
Output	O
:	O
The	O
solver	O
to	O
update	O
is	O
given	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
The	O
overall	O
solver	O
for	O
our	O
model	O
(	O
[	O
reference	O
]	O
)	O
is	O
summarized	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
According	O
to	O
Theorem	O
[	O
reference	O
]	O
and	O
Corollary	O
[	O
reference	O
]	O
,	O
the	O
solver	O
for	O
our	O
model	O
(	O
[	O
reference	O
]	O
)	O
is	O
highly	O
simplified	O
,	O
as	O
feature	O
length	O
is	O
generally	O
much	O
smaller	O
than	O
data	O
size	O
,	O
i.e	O
.	O
Similarly	O
,	O
Nie	Method
’s	Method
method	Method
could	O
be	O
highly	O
accelerated	O
by	O
Theorem	O
[	O
reference	O
]	O
,	O
obtaining	O
500	O
+	O
times	O
speedup	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O
theorem	O
:	O
.	O
Nie	O
’s	O
×NN	Method
solver	Method
(	O
)	O
[	O
]	O
is	O
equivalent	O
to	O
the	O
following	O
×LL	Method
linear	Method
system	Method
(	Method
)	Method
,	O
where	O
IL	Method
is	O
a	O
×LL	O
identity	O
matrix	O
.	O
proof	O
:	O
Proof	O
.	O
Based	O
on	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
the	O
following	O
equalities	O
:	O
The	O
derivations	O
are	O
equivalent	O
;	O
their	O
results	O
are	O
equal	O
.	O
∎	O
theorem	O
:	O
.	O
Since	O
feature	O
length	O
is	O
generally	O
much	O
smaller	O
than	O
data	O
size	O
,	O
i.e.	O
≪LN	O
,	O
our	O
accelerated	Method
solver	Method
(	O
)	O
for	O
Nie	Method
’s	Method
model	Method
(	O
)	O
is	O
highly	O
faster	O
than	O
the	O
authorial	Method
solver	Method
(	O
)	O
.	O
Theoretically	O
,	O
we	O
reduce	O
the	O
computational	Metric
complexity	Metric
from	O
O	O
(	O
N	O
)	O
4	O
to	O
O	O
(	O
NL2	O
+	O
NL	O
)	O
3	O
,	O
while	O
maintaining	O
the	O
same	O
solution	O
.	O
That	O
is	O
,	O
like	O
Nie	Method
’s	Method
solver	Method
(	O
)	O
,	O
our	O
speedup	Method
solver	Method
(	O
)	O
can	O
reach	O
the	O
global	O
optimum	O
.	O
Extensive	O
empirical	O
results	O
will	O
verify	O
the	O
huge	O
acceleration	O
section	O
:	O
Experiments	O
subsection	O
:	O
Experimental	O
Settings	O
In	O
this	O
part	O
,	O
the	O
experimental	O
settings	O
are	O
introduced	O
.	O
All	O
experiments	O
are	O
conducted	O
on	O
a	O
server	O
with	O
64	O
-	O
core	O
Intel	O
Xeon	O
E7	O
-	O
4820	O
@	O
2.00	O
GHz	O
,	O
18	O
Mb	O
Cache	O
and	O
0.986	O
TB	O
RAM	O
,	O
using	O
Matlab	O
2012	O
.	O
Brief	O
descriptions	O
of	O
ten	O
benchmark	O
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
,	O
where	O
‘	O
Total	O
’	O
denotes	O
the	O
total	O
set	O
of	O
samples	O
in	O
each	O
data	O
.	O
Due	O
to	O
the	O
high	O
computational	Metric
complexity	Metric
,	O
other	O
methods	O
can	O
only	O
handle	O
small	O
datasets	O
(	O
while	O
our	O
method	O
can	O
handle	O
the	O
total	O
set	O
)	O
.	O
Thus	O
,	O
we	O
randomly	O
choose	O
the	O
candidate	O
set	O
from	O
the	O
total	O
set	O
to	O
reduce	O
the	O
sample	O
size	O
,	O
i.e.	O
(	O
cf	O
.	O
‘	O
Total	O
’	O
and	O
‘	O
candid	O
.	O
’	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O
The	O
remainder	O
(	O
except	O
candidate	O
set	O
)	O
are	O
used	O
for	O
test	O
.	O
Specifically	O
,	O
to	O
simulate	O
the	O
varying	O
quality	O
of	O
samples	O
,	O
ten	O
percentage	O
of	O
candidate	O
samples	O
from	O
each	O
class	O
are	O
randomly	O
selected	O
and	O
arbitrarily	O
added	O
one	O
of	O
the	O
following	O
three	O
kinds	O
of	O
noise	O
:	O
“	O
Gaussian	Method
”	Method
,	O
“	O
Laplace	Method
”	O
and	O
“	O
Salt	Method
&	Method
pepper	Method
”	O
respectively	O
.	O
In	O
a	O
word	O
,	O
all	O
experiment	O
settings	O
are	O
same	O
and	O
fair	O
for	O
all	O
the	O
methods	O
.	O
subsection	O
:	O
Speed	Metric
Comparisons	Metric
There	O
are	O
two	O
parts	O
of	O
speed	Task
comparisons	Task
.	O
First	O
,	O
how	O
speed	O
varies	O
with	O
increasing	O
is	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
Then	O
the	O
comparison	O
of	O
specific	O
speed	O
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Note	O
that	O
TED	Method
and	O
RRSS	Method
denote	O
the	O
authorial	Method
solver	Method
(	O
via	O
authorial	Method
codes	Method
)	O
;	O
RRSS	Method
is	O
our	O
accelerated	Method
solver	Method
for	O
Nie	Method
’s	Method
model	Method
via	O
Theorem	O
[	O
reference	O
]	O
;	O
ARSS	Method
is	O
the	O
proposed	O
method	O
.	O
paragraph	O
:	O
Speed	O
vs.	O
increasing	O
To	O
verify	O
the	O
great	O
superiority	O
of	O
our	O
method	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
speed	O
,	O
three	O
experiments	O
are	O
conducted	O
.	O
The	O
results	O
are	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
where	O
there	O
are	O
three	O
sub	O
-	O
figures	O
showing	O
the	O
speed	O
of	O
four	O
methods	O
on	O
the	O
benchmark	O
datasets	O
of	O
Letter	Material
,	O
MNIST	Material
and	O
Waveform	Material
respectively	O
.	O
As	O
we	O
shall	O
see	O
,	O
both	O
selection	Metric
time	Metric
of	O
TED	Method
and	O
RRSS	Method
increases	O
dramatically	O
as	O
increases	O
.	O
No	O
surprisingly	O
,	O
RRSS	Method
is	O
incredibly	O
time	O
-	O
consuming	O
as	O
grows	O
—	O
the	O
order	O
of	O
curves	O
looks	O
higher	O
than	O
quadratic	O
.	O
Actually	O
,	O
the	O
theoretical	Metric
complexity	Metric
of	O
RRSS	Method
is	O
highly	O
up	O
to	O
as	O
analyzed	O
in	O
Remark	O
[	O
reference	O
]	O
.	O
Compared	O
with	O
TED	Method
and	O
RRSS	Method
,	O
the	O
curve	O
of	O
ARSS	Method
is	O
surprisingly	O
lower	O
and	O
highly	O
stable	O
against	O
increasing	O
;	O
there	O
is	O
almost	O
no	O
rise	O
of	O
selection	Metric
time	Metric
over	O
growing	O
.	O
This	O
is	O
owing	O
to	O
the	O
speedup	Method
techniques	Method
of	O
ALM	Method
and	O
equivalent	Method
derivations	Method
.	O
Via	O
them	O
,	O
we	O
reduce	O
the	O
computational	Metric
cost	Metric
from	O
to	O
,	O
as	O
analyzed	O
in	O
Theorem	O
[	O
reference	O
]	O
and	O
Corollary	O
[	O
reference	O
]	O
.	O
Moreover	O
,	O
with	O
the	O
help	O
of	O
Theorem	O
[	O
reference	O
]	O
,	O
RRSS	Method
is	O
the	O
second	O
faster	O
algorithm	O
that	O
is	O
significantly	O
accelerated	O
compared	O
with	O
the	O
authorial	Method
algorithm	Method
RRSS	Method
.	O
subsubsection	Method
:	O
Speed	O
with	O
fixed	O
The	O
speed	O
of	O
four	O
algorithms	O
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
a	O
,	O
where	O
each	O
row	O
shows	O
the	O
results	O
on	O
one	O
dataset	O
and	O
the	O
last	O
row	O
displays	O
the	O
average	O
results	O
.	O
Four	O
conclusions	O
can	O
be	O
drawn	O
from	O
Table	O
[	O
reference	O
]	O
a.	O
First	O
,	O
ARSS	Method
is	O
the	O
fastest	O
algorithm	O
,	O
and	O
RRSS	Method
is	O
the	O
second	O
fastest	O
algorithm	O
.	O
Second	O
,	O
with	O
the	O
help	O
of	O
Theorem	O
[	O
reference	O
]	O
,	O
RRSS	Method
is	O
highly	O
faster	O
than	O
RRSS	Method
,	O
averagely	O
obtaining	O
a	O
559	O
times	O
acceleration	Metric
.	O
Third	O
,	O
ARSS	Method
is	O
dramatically	O
faster	O
than	O
RRSS	Method
and	O
TED	Method
;	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
a	O
verify	O
an	O
average	O
acceleration	Metric
of	O
23275	O
times	O
faster	O
than	O
RRSS	Method
and	O
281	O
times	O
faster	O
than	O
TED	Method
.	O
This	O
means	O
that	O
for	O
example	O
if	O
it	O
takes	O
RRSS	O
100	O
years	O
to	O
do	O
a	O
subset	Task
selection	Task
task	Task
,	O
it	O
only	O
takes	O
our	O
method	O
1.6	O
days	O
to	O
address	O
the	O
same	O
problem	O
.	O
Finally	O
,	O
we	O
apply	O
ARSS	Method
to	O
the	O
whole	O
sample	O
set	O
of	O
each	O
data	O
.	O
The	O
results	O
are	O
displayed	O
in	O
the	O
column	O
in	O
Table	O
[	O
reference	O
]	O
,	O
showing	O
its	O
capability	O
to	O
process	O
very	O
large	O
datasets	O
.	O
‘	O
ARSS	Method
(	O
N	O
*	O
)	O
’	O
means	O
the	O
task	O
of	O
selecting	O
samples	O
from	O
the	O
whole	O
dataset	O
(	O
with	O
N	O
*	O
samples	O
as	O
shown	O
in	O
the	O
2ndcolumn	O
in	O
Table	O
)	O
,	O
while	O
‘	O
TED	O
’	O
to	O
‘	O
ARSS	Method
’	O
indicate	O
the	O
problem	O
of	O
dealing	O
with	O
the	O
candidate	O
sample	O
sets	O
(	O
with	O
N	O
samples	O
as	O
shown	O
in	O
the	O
3rd	O
column	O
in	O
Table	O
)	O
.	O
subsection	O
:	O
Prediction	Metric
Accuracy	Metric
paragraph	O
:	O
Accuracy	Metric
comparison	Metric
We	O
conduct	O
experiments	O
on	O
ten	O
benchmark	O
datasets	O
.	O
For	O
each	O
dataset	O
,	O
the	O
top	O
200	O
representative	O
samples	O
are	O
selected	O
for	O
training	O
.	O
The	O
prediction	Metric
accuracies	Metric
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
b	O
,	O
including	O
the	O
results	O
of	O
two	O
popular	O
classifiers	Method
.	O
Three	O
observations	O
can	O
be	O
drawn	O
from	O
this	O
table	O
.	O
First	O
,	O
Linear	Method
SVM	Method
generally	O
outperforms	O
KNN	Method
.	O
Second	O
,	O
in	O
general	O
,	O
our	O
method	O
performs	O
the	O
best	O
;	O
for	O
a	O
few	O
cases	O
,	O
our	O
method	O
achieves	O
comparable	O
results	O
with	O
the	O
best	O
performances	O
.	O
Third	O
,	O
compared	O
with	O
TED	Method
,	O
both	O
RRSS	Method
and	O
ARSS	Method
achieve	O
an	O
appreciable	O
advantage	O
.	O
The	O
above	O
analyses	O
are	O
better	O
illustrated	O
in	O
the	O
last	O
row	O
of	O
Table	O
[	O
reference	O
]	O
b.	O
These	O
results	O
demonstrate	O
that	O
the	O
loss	Method
in	O
our	O
model	O
is	O
well	O
suited	O
to	O
select	O
exemplars	O
from	O
the	O
sample	O
sets	O
of	O
various	O
quality	O
.	O
paragraph	O
:	O
Prediction	Metric
accuracies	Metric
vs.	O
increasing	O
To	O
give	O
a	O
more	O
detailed	O
comparison	O
,	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
prediction	Metric
accuracies	Metric
versus	O
growing	O
(	O
the	O
number	O
of	O
selected	O
samples	O
)	O
.	O
There	O
are	O
two	O
rows	O
and	O
four	O
columns	O
of	O
sub	O
-	O
figures	O
.	O
The	O
top	O
row	O
shows	O
the	O
results	O
of	O
KNN	Method
,	O
and	O
the	O
bottom	O
one	O
shows	O
results	O
of	O
SVM	Method
.	O
Each	O
column	O
gives	O
the	O
result	O
on	O
one	O
dataset	O
.	O
As	O
we	O
shall	O
see	O
,	O
the	O
prediction	Metric
accuracies	Metric
generally	O
increase	O
as	O
increases	O
.	O
Such	O
case	O
is	O
consistent	O
with	O
the	O
common	O
view	O
that	O
more	O
training	O
data	O
will	O
boost	O
the	O
prediction	Metric
accuracy	Metric
.	O
For	O
each	O
sub	O
-	O
figure	O
,	O
ARSS	Method
is	O
generally	O
among	O
the	O
best	O
.	O
This	O
case	O
implies	O
that	O
our	O
robust	Metric
objective	Metric
(	O
[	O
reference	O
]	O
)	O
via	O
the	O
-	O
norm	O
is	O
feasible	O
to	O
select	O
subsets	O
from	O
the	O
data	O
of	O
varying	O
qualities	O
.	O
section	O
:	O
Conclusion	O
To	O
deal	O
with	O
tremendous	O
data	O
of	O
varying	O
quality	O
,	O
we	O
propose	O
an	O
accelerated	Method
robust	Method
subset	Method
selection	Method
(	O
ARSS	Method
)	O
method	O
.	O
The	O
-	O
norm	O
is	O
exploited	O
to	O
enhance	O
the	O
robustness	Metric
against	O
both	O
outlier	O
samples	O
and	O
outlier	O
features	O
.	O
Although	O
the	O
resulted	O
objective	O
is	O
complex	O
to	O
solve	O
,	O
we	O
propose	O
a	O
highly	O
efficient	O
solver	O
via	O
two	O
techniques	O
:	O
ALM	Method
and	O
equivalent	Method
derivations	Method
.	O
Via	O
them	O
,	O
we	O
greatly	O
reduce	O
the	O
computational	Metric
complexity	Metric
from	O
to	O
.	O
Here	O
feature	O
length	O
is	O
much	O
smaller	O
than	O
data	O
size	O
,	O
i.e.	O
.	O
Extensive	O
results	O
on	O
ten	O
benchmark	O
datasets	O
verify	O
that	O
our	O
method	O
not	O
only	O
runs	O
10	O
,	O
000	O
+	O
times	O
faster	O
than	O
the	O
most	O
related	O
method	O
,	O
but	O
also	O
outperforms	O
state	O
of	O
the	O
art	O
methods	O
.	O
Moreover	O
,	O
we	O
propose	O
an	O
accelerated	Method
solver	Method
to	O
highly	O
speed	O
up	O
Nie	Method
’s	Method
method	Method
,	O
theoretically	O
reducing	O
the	O
computational	Metric
complexity	Metric
from	O
to	O
.	O
Empirically	O
,	O
our	O
accelerated	Method
solver	Method
could	O
achieve	O
equal	O
results	O
and	O
500	O
+	O
times	O
acceleration	Metric
compared	O
with	O
the	O
authorial	Method
solver	Method
.	O
paragraph	O
:	O
Limitation	O
.	O
Our	O
efficient	O
algorithm	O
build	O
on	O
the	O
observation	O
that	O
the	O
number	O
of	O
samples	O
is	O
generally	O
larger	O
than	O
feature	O
length	O
,	O
i.e.	O
.	O
For	O
the	O
case	O
of	O
,	O
the	O
acceleration	O
will	O
be	O
inapparent	O
.	O
section	O
:	O
Acknowledgements	O
The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
editor	O
and	O
the	O
reviewers	O
for	O
their	O
valuable	O
suggestions	O
.	O
Besides	O
,	O
this	O
work	O
is	O
supported	O
by	O
the	O
projects	O
(	O
Grant	O
No	O
.	O
61272331	O
,	O
91338202	O
,	O
61305049	O
and	O
61203277	O
)	O
of	O
the	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
.	O
bibliography	O
:	O
References	O
