document	O
:	O
The	O
Reactor	Method
:	O
A	O
fast	Method
and	Method
sample	Method
-	Method
efficient	Method
Actor	Method
-	Method
Critic	Method
agent	Method
for	O
Reinforcement	Task
Learning	Task
In	O
this	O
work	O
,	O
we	O
present	O
a	O
new	O
agent	Method
architecture	Method
,	O
called	O
Reactor	Method
,	O
which	O
combines	O
multiple	O
algorithmic	Method
and	Method
architectural	Method
contributions	Method
to	O
produce	O
an	O
agent	O
with	O
higher	O
sample	Metric
-	Metric
efficiency	Metric
than	O
Prioritized	Method
Dueling	Method
DQN	Method
wang2017sample	O
and	O
Categorical	Method
DQN	Method
bellemare2017distributional	O
,	O
while	O
giving	O
better	O
run	Metric
-	Metric
time	Metric
performance	O
than	O
A3C	O
mnih2016asynchronous	O
.	O
Our	O
first	O
contribution	O
is	O
a	O
new	O
policy	Method
evaluation	Method
algorithm	Method
called	O
Distributional	Method
Retrace	Method
,	O
which	O
brings	O
multi	Task
-	Task
step	Task
off	Task
-	Task
policy	Task
updates	Task
to	O
the	O
distributional	Task
reinforcement	Task
learning	Task
setting	Task
.	O
The	O
same	O
approach	O
can	O
be	O
used	O
to	O
convert	O
several	O
classes	O
of	O
multi	Method
-	Method
step	Method
policy	Method
evaluation	Method
algorithms	Method
,	O
designed	O
for	O
expected	Task
value	Task
evaluation	Task
,	O
into	O
distributional	Method
algorithms	Method
.	O
Next	O
,	O
we	O
introduce	O
the	O
-	Method
leave	Method
-	Method
one	Method
-	Method
out	Method
policy	Method
gradient	Method
algorithm	Method
,	O
which	O
improves	O
the	O
trade	O
-	O
off	O
between	O
variance	Metric
and	O
bias	O
by	O
using	O
action	O
values	O
as	O
a	O
baseline	O
.	O
Our	O
final	O
algorithmic	O
contribution	O
is	O
a	O
new	O
prioritized	Method
replay	Method
algorithm	Method
for	O
sequences	O
,	O
which	O
exploits	O
the	O
temporal	O
locality	O
of	O
neighboring	O
observations	O
for	O
more	O
efficient	O
replay	Task
prioritization	Task
.	O
Using	O
the	O
Atari	Material
2600	Material
benchmarks	Material
,	O
we	O
show	O
that	O
each	O
of	O
these	O
innovations	O
contribute	O
to	O
both	O
sample	Metric
efficiency	Metric
and	O
final	Metric
agent	Metric
performance	Metric
.	O
Finally	O
,	O
we	O
demonstrate	O
that	O
Reactor	Method
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
after	O
200	O
million	O
frames	O
and	O
less	O
than	O
a	O
day	O
of	O
training	O
.	O
section	O
:	O
Introduction	O
Model	Method
-	Method
free	Method
deep	Method
reinforcement	Method
learning	Method
has	O
achieved	O
several	O
remarkable	O
successes	O
in	O
domains	O
ranging	O
from	O
super	Task
-	Task
human	Task
-	Task
level	Task
control	Task
in	O
video	O
games	O
mnih15human	O
and	O
the	O
game	O
of	O
Go	O
silver2016mastering	O
,	O
agzero	O
,	O
to	O
continuous	Task
motor	Task
control	Task
tasks	Task
lillicrap2015continuous	O
,	O
schulman2015trust	O
.	O
Much	O
of	O
the	O
recent	O
work	O
can	O
be	O
divided	O
into	O
two	O
categories	O
.	O
First	O
,	O
those	O
of	O
which	O
that	O
,	O
often	O
building	O
on	O
the	O
DQN	Method
framework	Method
,	O
act	O
-	O
greedily	O
according	O
to	O
an	O
action	Method
-	Method
value	Method
function	Method
and	O
train	O
using	O
mini	O
-	O
batches	O
of	O
transitions	O
sampled	O
from	O
an	O
experience	O
replay	O
buffer	O
van2016deep	O
,	O
wang2015dueling	O
,	O
he2016learning	O
,	O
anschel2017averaged	O
.	O
These	O
value	Method
-	Method
function	Method
agents	Method
benefit	O
from	O
improved	O
sample	Metric
complexity	Metric
,	O
but	O
tend	O
to	O
suffer	O
from	O
long	O
runtimes	Metric
(	O
e.g.	O
DQN	Method
requires	O
approximately	O
a	O
week	O
to	O
train	O
on	O
Atari	Material
)	O
.	O
The	O
second	O
category	O
are	O
the	O
actor	Method
-	Method
critic	Method
agents	Method
,	O
which	O
includes	O
the	O
asynchronous	Method
advantage	Method
actor	Method
-	Method
critic	Method
(	Method
A3C	Method
)	Method
algorithm	Method
,	O
introduced	O
by	O
mnih2016asynchronous	O
.	O
These	O
agents	O
train	O
on	O
transitions	O
collected	O
by	O
multiple	O
actors	O
running	O
,	O
and	O
often	O
training	O
,	O
in	O
parallel	O
schulman2017proximal	O
,	O
vezhnevets2017feudal	O
.	O
The	O
deep	Method
actor	Method
-	Method
critic	Method
agents	Method
train	O
on	O
each	O
trajectory	O
only	O
once	O
,	O
and	O
thus	O
tend	O
to	O
have	O
worse	O
sample	Metric
complexity	Metric
.	O
However	O
,	O
their	O
distributed	O
nature	O
allows	O
significantly	O
faster	O
training	Task
in	O
terms	O
of	O
wall	Metric
-	Metric
clock	Metric
time	Metric
.	O
Still	O
,	O
not	O
all	O
existing	O
algorithms	O
can	O
be	O
put	O
in	O
the	O
above	O
two	O
categories	O
and	O
various	O
hybrid	O
approaches	O
do	O
exist	O
zhao2016deep	O
,	O
o2016combining	O
,	O
gu2016q	O
,	O
wang2017sample	O
.	O
Data	Task
-	Task
efficiency	Task
and	O
off	Task
-	Task
policy	Task
learning	Task
are	O
essential	O
for	O
many	O
real	Task
-	Task
world	Task
domains	Task
where	O
interactions	O
with	O
the	O
environment	O
are	O
expensive	O
.	O
Similarly	O
,	O
wall	O
-	O
clock	O
time	O
(	O
time	Metric
-	Metric
efficiency	Metric
)	O
directly	O
impacts	O
an	O
algorithm	O
â€™s	O
applicability	O
through	O
resource	Metric
costs	Metric
.	O
The	O
focus	O
of	O
this	O
work	O
is	O
to	O
produce	O
an	O
agent	O
that	O
is	O
sample	O
-	O
and	O
time	O
-	O
efficient	O
.	O
To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
new	O
reinforcement	Method
learning	Method
agent	Method
,	O
called	O
Reactor	Method
(	O
Retrace	Method
-	Method
Actor	Method
)	O
,	O
which	O
takes	O
a	O
principled	O
approach	O
to	O
combining	O
the	O
sample	Metric
-	Metric
efficiency	Metric
of	O
off	Method
-	Method
policy	Method
experience	Method
replay	Method
with	O
the	O
time	O
-	O
efficiency	O
of	O
asynchronous	Method
algorithms	Method
.	O
We	O
combine	O
recent	O
advances	O
in	O
both	O
categories	O
of	O
agents	O
with	O
novel	O
contributions	O
to	O
produce	O
an	O
agent	O
that	O
inherits	O
the	O
benefits	O
of	O
both	O
and	O
reaches	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
over	O
57	O
Atari	Material
2600	O
games	O
.	O
Our	O
primary	O
contributions	O
are	O
(	O
1	O
)	O
a	O
novel	O
policy	Method
gradient	Method
algorithm	Method
,	O
-	O
LOO	O
,	O
which	O
makes	O
better	O
use	O
of	O
action	O
-	O
value	O
estimates	O
to	O
improve	O
the	O
policy	O
gradient	O
;	O
(	O
2	O
)	O
the	O
first	O
multi	Method
-	Method
step	Method
off	Method
-	Method
policy	Method
distributional	Method
reinforcement	Method
learning	Method
algorithm	Method
,	O
distributional	Method
Retrace	Method
(	O
)	O
;	O
(	O
3	O
)	O
a	O
novel	O
prioritized	Method
replay	Method
for	O
off	Task
-	Task
policy	Task
sequences	Task
of	Task
transitions	Task
;	O
and	O
(	O
4	O
)	O
an	O
optimized	Method
network	Method
and	Method
parallel	Method
training	Method
architecture	Method
.	O
We	O
begin	O
by	O
reviewing	O
background	O
material	O
,	O
including	O
relevant	O
improvements	O
to	O
both	O
value	Method
-	Method
function	Method
agents	Method
and	O
actor	Method
-	Method
critic	Method
agents	Method
.	O
In	O
Section	O
[	O
reference	O
]	O
we	O
introduce	O
each	O
of	O
our	O
primary	O
contributions	O
and	O
present	O
the	O
Reactor	Method
agent	O
.	O
Finally	O
,	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
present	O
experimental	O
results	O
on	O
the	O
57	O
Atari	Material
2600	O
games	O
from	O
the	O
Arcade	O
Learning	O
Environment	O
(	O
ALE	O
)	O
bellemare2013arcade	O
,	O
as	O
well	O
as	O
a	O
series	O
of	O
ablation	Task
studies	Task
for	O
the	O
various	O
components	O
of	O
Reactor	Method
.	O
section	O
:	O
Background	O
We	O
consider	O
a	O
Markov	Method
decision	Method
process	Method
(	Method
MDP	Method
)	Method
with	O
state	O
space	O
and	O
finite	O
action	O
space	O
.	O
A	O
(	O
stochastic	Method
)	Method
policy	Method
is	O
a	O
mapping	O
from	O
states	O
to	O
a	O
probability	O
distribution	O
over	O
actions	O
.	O
We	O
consider	O
a	O
-	O
discounted	O
infinite	O
-	O
horizon	O
criterion	O
,	O
with	O
the	O
discount	O
factor	O
,	O
and	O
define	O
for	O
policy	O
the	O
action	O
-	O
value	O
of	O
a	O
state	O
-	O
action	O
pair	O
as	O
where	O
is	O
a	O
trajectory	O
generated	O
by	O
choosing	O
in	O
and	O
following	O
thereafter	O
,	O
i.e.	O
,	O
(	O
for	O
)	O
,	O
and	O
is	O
the	O
reward	O
signal	O
.	O
The	O
objective	O
in	O
reinforcement	Task
learning	Task
is	O
to	O
find	O
an	O
optimal	Method
policy	Method
,	O
which	O
maximises	O
.	O
The	O
optimal	O
action	O
-	O
values	O
are	O
given	O
by	O
.	O
subsection	O
:	O
Value	Method
-	Method
based	Method
algorithms	Method
The	O
Deep	Method
Q	Method
-	Method
Network	Method
(	Method
DQN	Method
)	Method
framework	Method
,	O
introduced	O
by	O
mnih15human	O
,	O
popularised	O
the	O
current	O
line	O
of	O
research	O
into	O
deep	Method
reinforcement	Method
learning	Method
by	O
reaching	O
human	O
-	O
level	O
,	O
and	O
beyond	O
,	O
performance	O
across	O
57	O
Atari	Material
2600	O
games	O
in	O
the	O
ALE	O
.	O
While	O
DQN	Method
includes	O
many	O
specific	O
components	O
,	O
the	O
essence	O
of	O
the	O
framework	O
,	O
much	O
of	O
which	O
is	O
shared	O
by	O
Neural	Method
Fitted	Method
Q	Method
-	O
Learning	O
riedmiller2005neural	O
,	O
is	O
to	O
use	O
of	O
a	O
deep	Method
convolutional	Method
neural	Method
network	Method
to	O
approximate	O
an	O
action	Method
-	Method
value	Method
function	Method
,	O
training	O
this	O
approximate	Method
action	Method
-	Method
value	Method
function	Method
using	O
the	O
Q	Method
-	Method
Learning	Method
algorithm	Method
watkins1992	O
and	O
mini	O
-	O
batches	O
of	O
one	O
-	O
step	O
transitions	O
(	O
)	O
drawn	O
randomly	O
from	O
an	O
experience	O
replay	O
buffer	O
lin1992self	O
.	O
Additionally	O
,	O
the	O
next	O
-	O
state	O
action	O
-	O
values	O
are	O
taken	O
from	O
a	O
target	O
network	O
,	O
which	O
is	O
updated	O
to	O
match	O
the	O
current	O
network	O
periodically	O
.	O
Thus	O
,	O
the	O
temporal	Metric
difference	Metric
(	Metric
TD	Metric
)	Metric
error	Metric
for	O
transition	O
used	O
by	O
these	O
algorithms	O
is	O
given	O
by	O
where	O
denotes	O
the	O
parameters	O
of	O
the	O
network	O
and	O
are	O
the	O
parameters	O
of	O
the	O
target	O
network	O
.	O
Since	O
this	O
seminal	O
work	O
,	O
we	O
have	O
seen	O
numerous	O
extensions	O
and	O
improvements	O
that	O
all	O
share	O
the	O
same	O
underlying	O
framework	O
.	O
Double	Method
DQN	Method
van2016deep	O
,	O
attempts	O
to	O
correct	O
for	O
the	O
over	Task
-	Task
estimation	Task
bias	Task
inherent	O
in	O
Q	Method
-	Method
Learning	Method
by	O
changing	O
the	O
second	O
term	O
of	O
eq	O
:	O
tderr	O
to	O
.	O
The	O
dueling	Method
architecture	Method
wang2015dueling	O
,	O
changes	O
the	O
network	O
to	O
estimate	O
action	O
-	O
values	O
using	O
separate	O
network	Method
heads	Method
and	O
with	O
Recently	O
,	O
rainbow	O
introduced	O
Rainbow	Method
,	O
a	O
value	Method
-	Method
based	Method
reinforcement	Method
learning	Method
agent	Method
combining	O
many	O
of	O
these	O
improvements	O
into	O
a	O
single	O
agent	O
and	O
demonstrating	O
that	O
they	O
are	O
largely	O
complementary	O
.	O
Rainbow	Method
significantly	O
out	O
performs	O
previous	O
methods	O
,	O
but	O
also	O
inherits	O
the	O
poorer	O
time	Metric
-	Metric
efficiency	Metric
of	O
the	O
DQN	Method
framework	Method
.	O
We	O
include	O
a	O
detailed	O
comparison	O
between	O
Reactor	Method
and	O
Rainbow	O
in	O
the	O
Appendix	O
.	O
In	O
the	O
remainder	O
of	O
the	O
section	O
we	O
will	O
describe	O
in	O
more	O
depth	O
other	O
recent	O
improvements	O
to	O
DQN	Method
.	O
subsubsection	O
:	O
Prioritized	Task
experience	Task
replay	Task
The	O
experience	Method
replay	Method
buffer	Method
was	O
first	O
introduced	O
by	O
lin1992self	O
and	O
later	O
used	O
in	O
DQN	O
mnih15human	O
.	O
Typically	O
,	O
the	O
replay	Method
buffer	Method
is	O
essentially	O
a	O
first	Method
-	Method
in	Method
-	Method
first	Method
-	Method
out	Method
queue	Method
with	O
new	O
transitions	O
gradually	O
replacing	O
older	O
transitions	O
.	O
The	O
agent	O
would	O
then	O
sample	O
a	O
mini	O
-	O
batch	O
uniformly	O
at	O
random	O
from	O
the	O
replay	O
buffer	O
.	O
Drawing	O
inspiration	O
from	O
prioritized	Method
sweeping	Method
moore1993prioritized	Method
,	O
prioritized	Method
experience	Method
replay	Method
replaces	O
the	O
uniform	Method
sampling	Method
with	O
prioritized	Method
sampling	Method
proportional	O
to	O
the	O
absolute	Metric
TD	Metric
error	Metric
schaul16prioritized	O
.	O
Specifically	O
,	O
for	O
a	O
replay	O
buffer	O
of	O
size	O
,	O
prioritized	O
experience	O
replay	O
samples	O
transition	O
with	O
probability	O
,	O
and	O
applies	O
weighted	Method
importance	Method
-	Method
sampling	Method
with	O
to	O
correct	O
for	O
the	O
prioritization	O
bias	O
,	O
where	O
Prioritized	Method
DQN	Method
significantly	O
increases	O
both	O
the	O
sample	Metric
-	Metric
efficiency	Metric
and	O
final	O
performance	O
over	O
DQN	Method
on	O
the	O
Atari	Material
2600	Material
benchmarks	Material
schaul2015prioritized	O
.	O
subsubsection	O
:	O
Retrace	Method
(	O
)	O
Retrace	Method
(	Method
)	Method
is	O
a	O
convergent	Method
off	Method
-	Method
policy	Method
multi	Method
-	Method
step	Method
algorithm	Method
extending	O
the	O
DQN	Method
agent	Method
munos2016safe	O
.	O
Assume	O
that	O
some	O
trajectory	O
has	O
been	O
generated	O
according	O
to	O
behaviour	O
policy	O
,	O
i.e.	O
,	O
.	O
Now	O
,	O
we	O
aim	O
to	O
evaluate	O
the	O
value	O
of	O
a	O
different	O
target	O
policy	O
,	O
i.e.	O
we	O
want	O
to	O
estimate	O
.	O
The	O
Retrace	Method
algorithm	Method
will	O
update	O
our	O
current	O
estimate	O
of	O
in	O
the	O
direction	O
of	O
where	O
is	O
the	O
temporal	O
difference	O
at	O
time	O
under	O
,	O
and	O
The	O
Retrace	Method
algorithm	Method
comes	O
with	O
the	O
theoretical	O
guarantee	O
that	O
in	O
finite	O
state	O
and	O
action	O
spaces	O
,	O
repeatedly	O
updating	O
our	O
current	O
estimate	O
according	O
to	O
(	O
[	O
reference	O
]	O
)	O
produces	O
a	O
sequence	O
of	O
Q	Method
functions	Method
which	O
converges	O
to	O
for	O
a	O
fixed	O
or	O
to	O
if	O
we	O
consider	O
a	O
sequence	O
of	O
policies	O
which	O
become	O
increasingly	O
greedy	O
w.r.t	O
.	O
the	O
estimates	O
munos2016safe	O
.	O
subsubsection	O
:	O
Distributional	Method
RL	Method
Distributional	Method
reinforcement	Method
learning	Method
refers	O
to	O
a	O
class	O
of	O
algorithms	O
that	O
directly	O
estimate	O
the	O
distribution	O
over	O
returns	O
,	O
whose	O
expectation	O
gives	O
the	O
traditional	O
value	O
function	O
bellemare2017distributional	O
.	O
Such	O
approaches	O
can	O
be	O
made	O
tractable	O
with	O
a	O
distributional	Method
Bellman	Method
equation	Method
,	O
and	O
the	O
recently	O
proposed	O
algorithm	O
showed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
the	O
Atari	Material
2600	Material
benchmarks	Material
.	O
parameterizes	O
the	O
distribution	O
over	O
returns	O
with	O
a	O
mixture	Method
over	Method
Diracs	Method
centered	O
on	O
a	O
uniform	O
grid	O
,	O
with	O
hyperparameters	O
that	O
bound	O
the	O
distribution	O
support	O
of	O
size	O
.	O
subsection	O
:	O
Actor	Method
-	Method
critic	Method
algorithms	Method
In	O
this	O
section	O
we	O
review	O
the	O
actor	Method
-	Method
critic	Method
framework	Method
for	O
reinforcement	Method
learning	Method
algorithms	Method
and	O
then	O
discuss	O
recent	O
advances	O
in	O
actor	Method
-	Method
critic	Method
algorithms	Method
along	O
with	O
their	O
various	O
trade	O
-	O
offs	O
.	O
The	O
asynchronous	Method
advantage	Method
actor	Method
-	Method
critic	Method
(	Method
A3C	Method
)	Method
algorithm	Method
mnih2016asynchronous	Method
,	O
maintains	O
a	O
parameterized	Method
policy	Method
and	Method
value	Method
function	Method
,	O
which	O
are	O
updated	O
with	O
A3C	O
uses	O
parallel	Method
CPU	Method
workers	Method
,	O
each	O
acting	O
independently	O
in	O
the	O
environment	O
and	O
applying	O
the	O
above	O
updates	O
asynchronously	O
to	O
a	O
shared	O
set	O
of	O
parameters	O
.	O
In	O
contrast	O
to	O
the	O
previously	O
discussed	O
value	Method
-	Method
based	Method
methods	Method
,	O
A3C	Method
is	O
an	O
on	Method
-	Method
policy	Method
algorithm	Method
,	O
and	O
does	O
not	O
use	O
a	O
GPU	Method
nor	O
a	O
replay	Method
buffer	Method
.	O
Proximal	Method
Policy	Method
Optimization	Method
(	O
PPO	Method
)	O
is	O
a	O
closely	O
related	O
actor	Method
-	Method
critic	Method
algorithm	Method
schulman2017proximal	Method
,	O
which	O
replaces	O
the	O
advantage	O
pgadv	Method
with	O
,	O
where	O
is	O
as	O
defined	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Although	O
both	O
PPO	Method
and	O
A3C	O
run	O
parallel	Method
workers	Method
collecting	O
trajectories	O
independently	O
in	O
the	O
environment	O
,	O
PPO	Method
collects	O
these	O
experiences	O
to	O
perform	O
a	O
single	O
,	O
synchronous	O
,	O
update	O
in	O
contrast	O
with	O
the	O
asynchronous	Method
updates	Method
of	O
A3C.	O
Actor	Method
-	Method
Critic	Method
Experience	Method
Replay	Method
(	O
ACER	Method
)	Method
extends	O
the	O
A3C	Method
framework	Method
with	O
an	O
experience	Method
replay	Method
buffer	Method
,	O
Retrace	Method
algorithm	Method
for	O
off	Task
-	Task
policy	Task
corrections	Task
,	O
and	O
the	O
Truncated	Method
Importance	Method
Sampling	Method
Likelihood	Method
Ratio	Method
(	Method
TISLR	Method
)	Method
algorithm	Method
used	O
for	O
off	Task
-	Task
policy	Task
policy	Task
optimization	Task
wang2017sample	O
.	O
section	O
:	O
The	O
Reactor	Method
The	O
Reactor	Method
is	O
a	O
combination	O
of	O
four	O
novel	O
contributions	O
on	O
top	O
of	O
recent	O
improvements	O
to	O
both	O
deep	Method
value	Method
-	Method
based	Method
RL	Method
and	O
policy	Method
-	Method
gradient	Method
algorithms	Method
.	O
Each	O
contribution	O
moves	O
Reactor	Method
towards	O
our	O
goal	O
of	O
achieving	O
both	O
sample	Metric
and	Metric
time	Metric
efficiency	Metric
.	O
subsection	O
:	O
-	O
LOO	O
The	O
Reactor	Method
architecture	O
represents	O
both	O
a	O
policy	Method
and	Method
action	Method
-	Method
value	Method
function	Method
.	O
We	O
use	O
a	O
policy	Method
gradient	Method
algorithm	Method
to	O
train	O
the	O
actor	Method
which	O
makes	O
use	O
of	O
our	O
current	O
estimate	O
of	O
.	O
Let	O
be	O
the	O
value	O
function	O
at	O
some	O
initial	O
state	O
,	O
the	O
policy	Method
gradient	Method
theorem	Method
says	O
that	O
,	O
where	O
refers	O
to	O
the	O
gradient	O
w.r.t	O
.	O
policy	O
parameters	O
Sutton00policygradient	O
.	O
We	O
now	O
consider	O
several	O
possible	O
ways	O
to	O
estimate	O
this	O
gradient	O
.	O
To	O
simplify	O
notation	O
,	O
we	O
drop	O
the	O
dependence	O
on	O
the	O
state	O
for	O
now	O
and	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
quantity	O
In	O
the	O
off	Task
-	Task
policy	Task
case	Task
,	O
we	O
consider	O
estimating	O
using	O
a	O
single	O
action	O
drawn	O
from	O
a	O
(	O
possibly	O
different	O
from	O
)	O
behaviour	O
distribution	O
.	O
Let	O
us	O
assume	O
that	O
for	O
the	O
chosen	O
action	O
we	O
have	O
access	O
to	O
an	O
unbiased	O
estimate	O
of	O
.	O
Then	O
,	O
we	O
can	O
use	O
likelihood	Method
ratio	Method
(	Method
LR	Method
)	Method
method	Method
combined	O
with	O
an	O
importance	Method
sampling	Method
(	Method
IS	Method
)	Method
ratio	Method
(	O
which	O
we	O
call	O
ISLR	Method
)	O
to	O
build	O
an	O
unbiased	O
estimate	O
of	O
:	O
where	O
is	O
a	O
baseline	O
that	O
depends	O
on	O
the	O
state	O
but	O
not	O
on	O
the	O
chosen	O
action	O
.	O
However	O
this	O
estimate	O
suffers	O
from	O
high	O
variance	O
.	O
A	O
possible	O
way	O
for	O
reducing	O
variance	Metric
is	O
to	O
estimate	O
directly	O
from	O
(	O
[	O
reference	O
]	O
)	O
by	O
using	O
the	O
return	O
for	O
the	O
chosen	O
action	O
and	O
our	O
current	O
estimate	O
of	O
for	O
the	O
other	O
actions	O
,	O
which	O
leads	O
to	O
the	O
so	O
-	O
called	O
leave	Method
-	Method
one	Method
-	Method
out	Method
(	Method
LOO	Method
)	Method
policy	Method
-	Method
gradient	Method
estimate	Method
:	O
This	O
estimate	O
has	O
low	O
variance	O
but	O
may	O
be	O
biased	O
if	O
the	O
estimated	O
values	O
differ	O
from	O
.	O
A	O
better	O
bias	Metric
-	Metric
variance	Metric
tradeoff	Metric
may	O
be	O
obtained	O
by	O
the	O
more	O
general	Method
-	Method
LOO	Method
policy	Method
-	Method
gradient	Method
estimate	Method
:	O
where	O
can	O
be	O
a	O
function	O
of	O
both	O
policies	O
,	O
and	O
,	O
and	O
the	O
selected	O
action	O
.	O
Notice	O
that	O
when	O
,	O
(	O
[	O
reference	O
]	O
)	O
reduces	O
to	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
when	O
,	O
then	O
(	O
[	O
reference	O
]	O
)	O
is	O
This	O
estimate	O
is	O
unbiased	O
and	O
can	O
be	O
seen	O
as	O
a	O
generalization	O
of	O
where	O
instead	O
of	O
using	O
a	O
state	O
-	O
only	O
dependent	O
baseline	O
,	O
we	O
use	O
a	O
state	O
-	O
and	O
-	O
action	O
-	O
dependent	O
baseline	O
(	O
our	O
current	O
estimate	O
)	O
and	O
add	O
the	O
correction	Method
term	Method
to	O
cancel	O
the	O
bias	O
.	O
Proposition	O
[	O
reference	O
]	O
gives	O
our	O
analysis	O
of	O
the	O
bias	O
of	O
,	O
with	O
a	O
proof	O
left	O
to	O
the	O
Appendix	O
.	O
propositionpropbias	O
Assume	O
and	O
that	O
.	O
Then	O
,	O
the	O
bias	O
of	O
is	O
.	O
Thus	O
the	O
bias	O
is	O
small	O
when	O
is	O
close	O
to	O
,	O
or	O
when	O
the	O
-	O
estimates	O
are	O
close	O
to	O
the	O
true	O
values	O
,	O
and	O
unbiased	O
regardless	O
of	O
the	O
estimates	O
if	O
.	O
The	O
variance	O
is	O
low	O
when	O
is	O
small	O
,	O
therefore	O
,	O
in	O
order	O
to	O
improve	O
the	O
bias	Metric
-	Metric
variance	Metric
tradeoff	Metric
we	O
recommend	O
using	O
the	O
-	Method
LOO	Method
estimate	Method
with	O
defined	O
as	O
:	O
for	O
some	O
constant	O
.	O
This	O
truncated	O
coefficient	O
shares	O
similarities	O
with	O
the	O
truncated	Method
IS	Method
gradient	Method
estimate	Method
introduced	O
in	O
wang2017sample	O
(	O
which	O
we	O
call	O
TISLR	Method
for	O
truncated	Method
-	Method
ISLR	Method
)	O
:	O
The	O
differences	O
are	O
:	O
(	O
i	O
)	O
we	O
truncate	O
instead	O
of	O
truncating	Method
,	O
which	O
provides	O
an	O
additional	O
variance	Metric
reduction	Metric
due	O
to	O
the	O
variance	O
of	O
the	O
LR	Method
(	O
since	O
this	O
LR	Method
may	O
be	O
large	O
when	O
a	O
low	O
probability	O
action	O
is	O
chosen	O
)	O
,	O
and	O
(	O
ii	O
)	O
we	O
use	O
our	O
-	O
baseline	O
instead	O
of	O
a	O
baseline	O
,	O
reducing	O
further	O
the	O
variance	O
of	O
the	O
LR	Method
estimate	Method
.	O
subsection	O
:	O
Distributional	Task
Retrace	Task
In	O
off	Task
-	Task
policy	Task
learning	Task
it	O
is	O
very	O
difficult	O
to	O
produce	O
an	O
unbiased	O
sample	O
of	O
when	O
following	O
another	O
policy	O
.	O
This	O
would	O
require	O
using	O
full	O
importance	Method
sampling	Method
correction	Method
along	O
the	O
trajectory	O
.	O
Instead	O
,	O
we	O
use	O
the	O
off	O
-	O
policy	O
corrected	O
return	O
computed	O
by	O
the	O
Retrace	Method
algorithm	Method
,	O
which	O
produces	O
a	O
(	O
biased	O
)	O
estimate	O
of	O
but	O
whose	O
bias	O
vanishes	O
asymptotically	O
munos2016safe	O
.	O
In	O
Reactor	Method
,	O
we	O
consider	O
predicting	O
an	O
approximation	O
of	O
the	O
return	Method
distribution	Method
function	Method
from	O
any	O
state	O
-	O
action	O
pair	O
in	O
a	O
similar	O
way	O
as	O
in	O
bellemare2017distributional	O
.	O
The	O
original	O
algorithm	O
C51	O
described	O
in	O
that	O
paper	O
considered	O
single	O
-	O
step	O
Bellman	Method
updates	Method
only	O
.	O
Here	O
we	O
need	O
to	O
extend	O
this	O
idea	O
to	O
multi	Task
-	Task
step	Task
updates	Task
and	O
handle	O
the	O
off	Task
-	Task
policy	Task
correction	Task
performed	O
by	O
the	O
Retrace	Method
algorithm	Method
,	O
as	O
defined	O
in	O
eq	O
:	O
retrace	Method
.	O
Next	O
,	O
we	O
describe	O
these	O
two	O
extensions	O
.	O
paragraph	O
:	O
Multi	Method
-	Method
step	Method
distributional	Method
Bellman	Method
operator	Method
:	O
First	O
,	O
we	O
extend	O
C51	Method
to	O
multi	O
-	O
step	O
Bellman	Task
backups	Task
.	O
We	O
consider	O
return	O
-	O
distributions	O
from	O
of	O
the	O
form	O
(	O
where	O
denotes	O
a	O
Dirac	O
in	O
)	O
which	O
are	O
supported	O
on	O
a	O
finite	O
uniform	O
grid	O
,	O
,	O
,	O
.	O
The	O
coefficients	O
(	O
discrete	O
distribution	O
)	O
corresponds	O
to	O
the	O
probabilities	O
assigned	O
to	O
each	O
atom	O
of	O
the	O
grid	O
.	O
From	O
an	O
observed	O
-	O
step	O
sequence	O
,	O
generated	O
by	O
behavior	Method
policy	Method
(	O
i.e	O
,	O
for	O
)	O
,	O
we	O
build	O
the	O
-	Method
step	Method
backed	Method
-	Method
up	Method
return	Method
-	Method
distribution	Method
from	O
.	O
The	O
-	O
step	O
distributional	O
Bellman	O
target	O
,	O
whose	O
expectation	O
is	O
,	O
is	O
given	O
by	O
:	O
Since	O
this	O
distribution	O
is	O
supported	O
on	O
the	O
set	O
of	O
atoms	O
,	O
which	O
is	O
not	O
necessarily	O
aligned	O
with	O
the	O
grid	O
,	O
we	O
do	O
a	O
projection	Method
step	Method
and	O
minimize	O
the	O
KL	Metric
-	Metric
loss	Metric
between	O
the	O
projected	O
target	O
and	O
the	O
current	O
estimate	O
,	O
just	O
as	O
with	O
C51	O
except	O
with	O
a	O
different	O
target	O
distribution	O
bellemare2017distributional	O
.	O
paragraph	O
:	O
Distributional	O
Retrace	O
:	O
Now	O
,	O
the	O
Retrace	Method
algorithm	Method
defined	O
in	O
eq	O
:	O
retrace	Method
involves	O
an	O
off	Method
-	Method
policy	Method
correction	Method
which	O
is	O
not	O
handled	O
by	O
the	O
previous	O
-	O
step	O
distributional	Method
Bellman	Method
backup	Method
.	O
The	O
key	O
to	O
extending	O
this	O
distributional	Task
back	Task
-	Task
up	Task
to	O
off	Task
-	Task
policy	Task
learning	Task
is	O
to	O
rewrite	O
the	O
Retrace	Method
algorithm	Method
as	O
a	O
linear	Method
combination	Method
of	Method
-	Method
step	Method
Bellman	Method
backups	Method
,	O
weighted	O
by	O
some	O
coefficients	O
.	O
Indeed	O
,	O
notice	O
that	O
eq	O
:	O
retrace	O
rewrites	O
as	O
where	O
.	O
These	O
coefficients	O
depend	O
on	O
the	O
degree	O
of	O
off	O
-	O
policy	O
-	O
ness	O
(	O
between	O
and	O
)	O
along	O
the	O
trajectory	O
.	O
We	O
have	O
that	O
,	O
but	O
notice	O
some	O
coefficients	O
may	O
be	O
negative	O
.	O
However	O
,	O
in	O
expectation	O
(	O
over	O
the	O
behavior	Method
policy	Method
)	O
they	O
are	O
non	O
-	O
negative	O
.	O
Indeed	O
,	O
by	O
definition	O
of	O
the	O
coefficients	O
eq	O
:	O
trace.cut	O
.	O
Thus	O
in	O
expectation	O
(	O
over	O
the	O
behavior	O
policy	O
)	O
,	O
the	O
Retrace	Method
update	Method
can	O
be	O
seen	O
as	O
a	O
convex	Method
combination	Method
of	Method
-	Method
step	Method
Bellman	Method
updates	Method
.	O
Then	O
,	O
the	O
distributional	Method
Retrace	Method
algorithm	Method
can	O
be	O
defined	O
as	O
backing	O
up	O
a	O
mixture	Method
of	Method
-	Method
step	Method
distributions	Method
.	O
More	O
precisely	O
,	O
we	O
define	O
the	O
Retrace	O
target	O
distribution	O
as	O
:	O
where	O
is	O
a	O
linear	Method
interpolation	Method
kernel	Method
,	O
projecting	O
onto	O
the	O
support	O
:	O
We	O
update	O
the	O
current	O
probabilities	O
by	O
performing	O
a	O
gradient	Method
step	Method
on	O
the	O
KL	Method
-	Method
loss	Method
Again	O
,	O
notice	O
that	O
some	O
target	O
â€˜	O
â€˜	O
probabilities	O
â€™	O
â€™	O
may	O
be	O
negative	O
for	O
some	O
sample	O
trajectory	O
,	O
but	O
in	O
expectation	O
they	O
will	O
be	O
non	O
-	O
negative	O
.	O
Since	O
the	O
gradient	O
of	O
a	O
KL	Method
-	Method
loss	Method
is	O
linear	O
w.r.t	O
.	O
its	O
first	O
argument	O
,	O
our	O
update	Method
rule	Method
eq	Method
:	O
kl.gradient	Method
provides	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
of	O
the	O
KL	O
between	O
the	O
expected	O
(	O
over	O
the	O
behavior	Method
policy	Method
)	O
Retrace	O
target	O
distribution	O
and	O
the	O
current	O
predicted	O
distribution	O
.	O
paragraph	O
:	O
Remark	O
:	O
The	O
same	O
method	O
can	O
be	O
applied	O
to	O
other	O
algorithms	O
(	O
such	O
as	O
TB	Method
(	Method
)	Method
precup2000eligibility	O
and	O
importance	Method
sampling	Method
precup01offpolicy	Method
)	O
in	O
order	O
to	O
derive	O
distributional	Method
versions	Method
of	O
other	O
off	Method
-	Method
policy	Method
multi	Method
-	Method
step	Method
RL	Method
algorithms	Method
.	O
subsection	O
:	O
Prioritized	Task
sequence	Task
replay	Task
Prioritized	Task
experience	Task
replay	Task
has	O
been	O
shown	O
to	O
boost	O
both	O
statistical	Metric
efficiency	Metric
and	O
final	O
performance	O
of	O
deep	Method
RL	Method
agents	Method
schaul16prioritized	O
.	O
However	O
,	O
as	O
originally	O
defined	O
prioritized	Method
replay	Method
does	O
not	O
handle	O
sequences	O
of	O
transitions	O
and	O
weights	O
all	O
unsampled	O
transitions	O
identically	O
.	O
In	O
this	O
section	O
we	O
present	O
an	O
alternative	O
initialization	Method
strategy	Method
,	O
called	O
lazy	Method
initialization	Method
,	O
and	O
argue	O
that	O
it	O
better	O
encodes	O
prior	O
information	O
about	O
temporal	O
difference	O
errors	O
.	O
We	O
then	O
briefly	O
describe	O
our	O
computationally	O
efficient	O
prioritized	Method
sequence	Method
sampling	Method
algorithm	Method
,	O
with	O
full	O
details	O
left	O
to	O
the	O
appendix	O
.	O
It	O
is	O
widely	O
recognized	O
that	O
TD	O
errors	O
tend	O
to	O
be	O
temporally	O
correlated	O
,	O
indeed	O
the	O
need	O
to	O
break	O
this	O
temporal	O
correlation	O
has	O
been	O
one	O
of	O
the	O
primary	O
justifications	O
for	O
the	O
use	O
of	O
experience	Task
replay	Task
mnih15human	O
.	O
Our	O
proposed	O
algorithm	O
begins	O
with	O
this	O
fundamental	O
assumption	O
.	O
theorem	O
:	O
.	O
Temporal	O
differences	O
are	O
temporally	O
correlated	O
,	O
with	O
correlation	O
decaying	O
on	O
average	O
with	O
the	O
time	O
-	O
difference	O
between	O
two	O
transitions	O
.	O
Prioritized	Method
experience	Method
replay	Method
adds	O
new	O
transitions	O
to	O
the	O
replay	O
buffer	O
with	O
a	O
constant	O
priority	O
,	O
but	O
given	O
the	O
above	O
assumption	O
we	O
can	O
devise	O
a	O
better	O
method	O
.	O
Specifically	O
,	O
we	O
propose	O
to	O
add	O
experience	O
to	O
the	O
buffer	O
with	O
no	O
priority	O
,	O
inserting	O
a	O
priority	O
only	O
after	O
the	O
transition	O
has	O
been	O
sampled	O
and	O
used	O
for	O
training	O
.	O
Also	O
,	O
instead	O
of	O
sampling	O
transitions	O
,	O
we	O
assign	O
priorities	O
to	O
all	O
(	O
overlapping	O
)	O
sequences	O
of	O
length	O
.	O
When	O
sampling	O
,	O
sequences	O
with	O
an	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
that	O
priority	O
.	O
Sequences	O
with	O
no	O
assigned	O
priority	O
are	O
sampled	O
proportionally	O
to	O
the	O
average	O
priority	O
of	O
assigned	O
priority	O
sequences	O
within	O
some	O
local	O
neighbourhood	O
.	O
Averages	Method
are	O
weighted	O
to	O
compensate	O
for	O
sampling	O
biases	O
(	O
i.e.	O
more	O
samples	O
are	O
made	O
in	O
areas	O
of	O
high	O
estimated	O
priorities	O
,	O
and	O
in	O
the	O
absence	O
of	O
weighting	O
this	O
would	O
lead	O
to	O
overestimation	O
of	O
unassigned	O
priorities	O
)	O
.	O
The	O
lazy	Method
initialization	Method
scheme	Method
starts	O
with	O
priorities	O
corresponding	O
to	O
the	O
sequences	O
for	O
which	O
a	O
priority	O
was	O
already	O
assigned	O
.	O
Then	O
it	O
extrapolates	O
a	O
priority	O
of	O
all	O
other	O
sequences	O
in	O
the	O
following	O
way	O
.	O
Let	O
us	O
define	O
a	O
partition	O
of	O
the	O
states	O
ordered	O
by	O
increasing	O
time	O
such	O
that	O
each	O
cell	O
contains	O
exactly	O
one	O
state	O
with	O
already	O
assigned	O
priority	O
.	O
We	O
define	O
the	O
estimated	O
priority	O
to	O
all	O
other	O
sequences	O
as	O
,	O
where	O
is	O
a	O
collection	O
of	O
contiguous	O
cells	O
containing	O
time	O
,	O
and	O
is	O
the	O
length	O
of	O
the	O
cell	O
containing	O
.	O
For	O
already	O
defined	O
priorities	O
denote	O
.	O
Cell	O
sizes	O
work	O
as	O
estimates	O
of	O
inverse	O
local	O
density	O
and	O
are	O
used	O
as	O
importance	O
weights	O
for	O
priority	Task
estimation	Task
.	O
For	O
the	O
algorithm	O
to	O
be	O
unbiased	O
,	O
partition	O
must	O
not	O
be	O
a	O
function	O
of	O
the	O
assigned	O
priorities	O
.	O
So	O
far	O
we	O
have	O
defined	O
a	O
class	O
of	O
algorithms	O
all	O
free	O
to	O
choose	O
the	O
partition	O
and	O
the	O
collection	O
of	O
cells	O
,	O
as	O
long	O
that	O
they	O
satisfy	O
the	O
above	O
constraints	O
.	O
Figure	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
illustrates	O
the	O
above	O
description	O
.	O
Now	O
,	O
with	O
probability	O
we	O
sample	O
uniformly	O
at	O
random	O
,	O
and	O
with	O
probability	O
we	O
sample	O
proportionally	O
to	O
.	O
We	O
implemented	O
an	O
algorithm	O
satisfying	O
the	O
above	O
constraints	O
and	O
called	O
it	O
Contextual	Method
Priority	Method
Tree	Method
(	O
CPT	Method
)	O
.	O
It	O
is	O
based	O
on	O
AVL	Method
trees	Method
velskii1976avl	O
and	O
can	O
execute	O
sampling	Method
,	O
insertion	Method
,	O
deletion	Method
and	O
density	Method
evaluation	Method
in	O
time	O
.	O
We	O
describe	O
CPT	O
in	O
detail	O
in	O
the	O
Appendix	O
in	O
Section	O
[	O
reference	O
]	O
.	O
We	O
treated	O
prioritization	Method
as	O
purely	O
a	O
variance	Method
reduction	Method
technique	Method
.	O
Importance	O
-	O
sampling	O
weights	O
were	O
evaluated	O
as	O
in	O
prioritized	Task
experience	Task
replay	Task
,	O
with	O
fixed	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O
We	O
used	O
simple	O
gradient	Method
magnitude	Method
estimates	Method
as	O
priorities	O
,	O
corresponding	O
to	O
a	O
mean	Metric
absolute	Metric
TD	Metric
error	Metric
along	O
a	O
sequence	O
for	O
Retrace	Task
,	O
as	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
for	O
the	O
classical	Task
RL	Task
case	Task
,	O
and	O
total	O
variation	O
in	O
the	O
distributional	Task
Retrace	Task
case	Task
.	O
subsection	O
:	O
Agent	Method
architecture	Method
In	O
order	O
to	O
improve	O
CPU	O
utilization	O
we	O
decoupled	O
acting	O
from	O
learning	Task
.	O
This	O
is	O
an	O
important	O
aspect	O
of	O
our	O
architecture	O
:	O
an	O
acting	Method
thread	Method
receives	O
observations	O
,	O
submits	O
actions	O
to	O
the	O
environment	O
,	O
and	O
stores	O
transitions	O
in	O
memory	O
,	O
while	O
a	O
learning	Method
thread	Method
re	O
-	O
samples	O
sequences	O
of	O
experiences	O
from	O
memory	O
and	O
trains	O
on	O
them	O
(	O
Figure	O
[	O
reference	O
]	O
,	O
left	O
)	O
.	O
We	O
typically	O
execute	O
4	O
-	O
6	O
acting	O
steps	O
per	O
each	O
learning	O
step	O
.	O
We	O
sample	O
sequences	O
of	O
length	O
in	O
batches	O
of	O
4	O
.	O
A	O
moving	Method
network	Method
is	O
unrolled	O
over	O
frames	O
1	O
-	O
32	O
while	O
the	O
target	O
network	O
is	O
unrolled	O
over	O
frames	O
2	O
-	O
33	O
.	O
We	O
allow	O
the	O
agent	O
to	O
be	O
distributed	O
over	O
multiple	O
machines	O
each	O
containing	O
action	O
-	O
learner	O
pairs	O
.	O
Each	O
worker	O
downloads	O
the	O
newest	O
network	O
parameters	O
before	O
each	O
learning	O
step	O
and	O
sends	O
delta	O
-	O
updates	O
at	O
the	O
end	O
of	O
it	O
.	O
Both	O
the	O
network	O
and	O
target	O
network	O
are	O
stored	O
on	O
a	O
shared	O
parameter	O
server	O
while	O
each	O
machine	O
contains	O
its	O
own	O
local	O
replay	O
memory	O
.	O
Training	Task
is	O
done	O
by	O
downloading	O
a	O
shared	Method
network	Method
,	O
evaluating	O
local	O
gradients	O
and	O
sending	O
them	O
to	O
be	O
applied	O
on	O
the	O
shared	Method
network	Method
.	O
While	O
the	O
agent	O
can	O
also	O
be	O
trained	O
on	O
a	O
single	O
machine	O
,	O
in	O
this	O
work	O
we	O
present	O
results	O
of	O
training	O
obtained	O
with	O
either	O
10	O
or	O
20	O
actor	Method
-	Method
learner	Method
workers	Method
and	O
one	O
parameter	Method
server	Method
.	O
In	O
Figure	O
[	O
reference	O
]	O
(	O
right	O
)	O
we	O
compare	O
resources	O
and	O
runtimes	O
of	O
Reactor	Method
with	O
related	O
algorithms	O
.	O
subsubsection	O
:	O
Network	Method
architecture	Method
In	O
some	O
domains	O
,	O
such	O
as	O
Atari	Material
,	O
it	O
is	O
useful	O
to	O
base	O
decisions	O
on	O
a	O
short	O
history	O
of	O
past	O
observations	O
.	O
The	O
two	O
techniques	O
generally	O
used	O
to	O
achieve	O
this	O
are	O
frame	Method
stacking	Method
and	O
recurrent	Method
network	Method
architectures	Method
.	O
We	O
chose	O
the	O
latter	O
over	O
the	O
former	O
for	O
reasons	O
of	O
implementation	Metric
simplicity	Metric
and	O
computational	Metric
efficiency	Metric
.	O
As	O
the	O
Retrace	Method
algorithm	Method
requires	O
evaluating	O
action	O
-	O
values	O
over	O
contiguous	O
sequences	O
of	O
trajectories	O
,	O
using	O
a	O
recurrent	Method
architecture	Method
allowed	O
each	O
frame	O
to	O
be	O
processed	O
by	O
the	O
convolutional	Method
network	Method
only	O
once	O
,	O
as	O
opposed	O
to	O
times	O
times	O
if	O
frame	O
concatenations	O
were	O
used	O
.	O
The	O
Reactor	Method
architecture	O
uses	O
a	O
recurrent	Method
neural	Method
network	Method
which	O
takes	O
an	O
observation	O
as	O
input	O
and	O
produces	O
two	O
outputs	O
:	O
categorical	O
action	O
-	O
value	O
distributions	O
(	O
here	O
is	O
a	O
bin	O
identifier	O
)	O
,	O
and	O
policy	O
probabilities	O
.	O
We	O
use	O
an	O
architecture	O
inspired	O
by	O
the	O
duelling	Method
network	Method
architecture	Method
wang2015dueling	O
.	O
We	O
split	O
action	O
-	O
value	O
-	O
distribution	O
logits	O
into	O
state	O
-	O
value	O
logits	O
and	O
advantage	O
logits	O
,	O
which	O
in	O
turn	O
are	O
connected	O
to	O
the	O
same	O
LSTM	Method
network	Method
hochreiter1997long	O
.	O
Final	O
action	O
-	O
value	O
logits	O
are	O
produced	O
by	O
summing	O
state	O
-	O
and	O
action	O
-	O
specific	O
logits	O
,	O
as	O
in	O
wang2015dueling	O
.	O
Finally	O
,	O
a	O
softmax	Method
layer	Method
on	O
top	O
for	O
each	O
action	O
produces	O
the	O
distributions	O
over	O
discounted	O
future	O
returns	O
.	O
The	O
policy	Method
head	Method
uses	O
a	O
softmax	Method
layer	Method
mixed	O
with	O
a	O
fixed	O
uniform	O
distribution	O
over	O
actions	O
,	O
where	O
this	O
mixing	O
ratio	O
is	O
a	O
hyperparameter	O
[	O
Section	O
5.1.3	O
]	O
wiering1999explorations	O
.	O
Policy	Method
and	Method
Q	Method
-	Method
networks	Method
have	O
separate	O
LSTMs	Method
.	O
Both	O
LSTMs	Method
are	O
connected	O
to	O
a	O
shared	Method
linear	Method
layer	Method
which	O
is	O
connected	O
to	O
a	O
shared	Method
convolutional	Method
neural	Method
network	Method
krizhevsky2012imagenet	Method
.	O
The	O
precise	O
network	O
specification	O
is	O
given	O
in	O
Table	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
.	O
Gradients	O
coming	O
from	O
the	O
policy	Method
LSTM	Method
are	O
blocked	O
and	O
only	O
gradients	O
originating	O
from	O
the	O
Q	Method
-	Method
network	Method
LSTM	Method
are	O
allowed	O
to	O
back	O
-	O
propagate	O
into	O
the	O
convolutional	Method
neural	Method
network	Method
.	O
We	O
block	O
gradients	O
from	O
the	O
policy	O
head	O
for	O
increased	O
stability	O
,	O
as	O
this	O
avoids	O
positive	O
feedback	O
loops	O
between	O
and	O
caused	O
by	O
shared	O
representations	O
.	O
We	O
used	O
the	O
Adam	Method
optimiser	Method
kingma2014adam	O
,	O
with	O
a	O
learning	Metric
rate	Metric
of	O
and	O
zero	O
momentum	O
because	O
asynchronous	O
updates	O
induce	O
implicit	O
momentum	O
mitliagkas2016asynchrony	O
.	O
Further	O
discussion	O
of	O
hyperparameters	O
and	O
their	O
optimization	Task
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
section	O
:	O
Experimental	O
Results	O
We	O
trained	O
and	O
evaluated	O
Reactor	Method
on	O
57	O
Atari	Material
games	O
bellemare2013arcade	O
.	O
Figure	O
[	O
reference	O
]	O
compares	O
the	O
performance	O
of	O
Reactor	Method
with	O
different	O
versions	O
of	O
Reactor	Method
each	O
time	O
leaving	O
one	O
of	O
the	O
algorithmic	O
improvements	O
out	O
.	O
We	O
can	O
see	O
that	O
each	O
of	O
the	O
algorithmic	O
improvements	O
(	O
Distributional	Method
retrace	Method
,	O
beta	Method
-	Method
LOO	Method
and	O
prioritized	Method
replay	Method
)	O
contributed	O
to	O
the	O
final	O
results	O
.	O
While	O
prioritization	Method
was	O
arguably	O
the	O
most	O
important	O
component	O
,	O
Beta	Method
-	Method
LOO	Method
clearly	O
outperformed	O
TISLR	Method
algorithm	Method
.	O
Although	O
distributional	Method
and	Method
non	Method
-	Method
distributional	Method
versions	Method
performed	O
similarly	O
in	O
terms	O
of	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
,	O
distributional	Method
version	Method
of	O
the	O
algorithm	O
generalized	O
better	O
when	O
tested	O
with	O
random	O
human	O
starts	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Comparing	O
to	O
prior	O
work	O
We	O
evaluated	O
Reactor	Method
with	O
target	O
update	O
frequency	O
,	O
and	O
-	O
LOO	O
with	O
on	O
57	O
Atari	Material
games	O
trained	O
on	O
10	O
machines	O
in	O
parallel	O
.	O
We	O
averaged	O
scores	O
over	O
200	O
episodes	O
using	O
30	O
random	O
human	O
starts	O
and	O
noop	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
)	O
.	O
We	O
calculated	O
mean	O
and	O
median	O
human	O
normalised	O
scores	O
across	O
all	O
games	O
.	O
We	O
also	O
ranked	O
all	O
algorithms	O
(	O
including	O
random	O
and	O
human	O
scores	O
)	O
for	O
each	O
game	O
and	O
evaluated	O
mean	Metric
rank	Metric
of	O
each	O
algorithm	O
across	O
all	O
57	O
Atari	Material
games	O
.	O
We	O
also	O
evaluated	O
mean	O
Rank	Metric
and	O
Elo	Metric
scores	Metric
for	O
each	O
algorithm	O
for	O
both	O
human	O
and	O
noop	O
start	O
settings	O
.	O
Please	O
refer	O
to	O
Section	O
[	O
reference	O
]	O
in	O
the	O
Appendix	O
for	O
more	O
details	O
.	O
Tables	O
[	O
reference	O
]	O
&	O
[	O
reference	O
]	O
compare	O
versions	O
of	O
our	O
algorithm	O
,	O
with	O
several	O
other	O
state	O
-	O
of	O
-	O
art	O
algorithms	O
across	O
57	O
Atari	Material
games	O
for	O
a	O
fixed	O
random	O
seed	O
across	O
all	O
games	O
bellemare2013arcade	O
.	O
We	O
compare	O
Reactor	Method
against	O
are	O
:	O
DQN	Method
mnih15human	O
,	O
Double	Method
DQN	Method
van2016deep	O
,	O
DQN	Method
with	Method
prioritised	Method
experience	Method
replay	Method
schaul2015prioritized	O
,	O
dueling	Method
architecture	Method
and	Method
prioritised	Method
dueling	Method
wang2015dueling	O
,	O
ACER	Method
wang2017sample	O
,	O
A3C	Method
mnih2016asynchronous	O
,	O
and	O
Rainbow	Method
rainbow	Method
.	O
Each	O
algorithm	O
was	O
exposed	O
to	O
200	O
million	O
frames	O
of	O
experience	O
,	O
or	O
500	O
million	O
frames	O
when	O
followed	O
by	O
,	O
and	O
the	O
same	O
pre	Method
-	Method
processing	Method
pipeline	Method
including	O
4	O
action	O
repeats	O
was	O
used	O
as	O
in	O
the	O
original	O
DQN	O
paper	O
mnih15human	O
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
Reactor	Method
exceeds	O
the	O
performance	O
of	O
all	O
algorithms	O
across	O
all	O
metrics	O
,	O
despite	O
requiring	O
under	O
two	O
days	O
of	O
training	O
.	O
With	O
500	O
million	O
frames	O
and	O
four	O
days	O
training	O
we	O
see	O
Reactor	Method
â€™s	O
performance	O
continue	O
to	O
improve	O
significantly	O
.	O
The	O
difference	O
in	O
time	Metric
-	Metric
efficiency	Metric
is	O
especially	O
apparent	O
when	O
comparing	O
Reactor	Method
and	O
Rainbow	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
,	O
right	O
)	O
.	O
Additionally	O
,	O
unlike	O
Rainbow	Method
,	O
Reactor	Method
does	O
not	O
use	O
Noisy	Method
Networks	Method
fortunato2017noisy	O
,	O
which	O
was	O
reported	O
to	O
have	O
contributed	O
to	O
the	O
performance	O
gains	O
.	O
When	O
evaluating	O
under	O
the	O
no	O
-	O
op	O
starts	O
regime	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
Reactor	Method
out	O
performs	O
all	O
methods	O
except	O
for	O
Rainbow	Method
.	O
This	O
suggests	O
that	O
Rainbow	Method
is	O
more	O
sample	O
-	O
efficient	O
when	O
training	O
and	O
evaluation	O
regimes	O
match	O
exactly	O
,	O
but	O
may	O
be	O
overfitting	O
to	O
particular	O
trajectories	O
due	O
to	O
the	O
significant	O
drop	O
in	O
performance	O
when	O
evaluated	O
on	O
the	O
random	O
human	O
starts	O
.	O
Regarding	O
ACER	Method
,	O
another	O
Retrace	Method
-	Method
based	Method
actor	Method
-	Method
critic	Method
architecture	Method
,	O
both	O
classical	O
and	O
distributional	Method
versions	Method
of	O
Reactor	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
exceeded	O
the	O
best	O
reported	O
median	O
human	Metric
normalized	Metric
score	Metric
of	O
1.9	O
with	O
noop	O
starts	O
achieved	O
in	O
500	O
million	O
steps	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
presented	O
a	O
new	O
off	Method
-	Method
policy	Method
agent	Method
based	O
on	O
Retrace	Method
actor	Method
-	Method
critic	Method
architecture	Method
and	O
show	O
that	O
it	O
achieves	O
similar	O
performance	O
as	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
while	O
giving	O
significant	O
real	O
-	O
time	O
performance	O
gains	O
.	O
We	O
demonstrate	O
the	O
benefits	O
of	O
each	O
of	O
the	O
suggested	O
algorithmic	Method
improvements	Method
,	O
including	O
Distributional	Method
Retrace	Method
,	O
beta	Method
-	Method
LOO	Method
policy	Method
gradient	Method
and	O
contextual	Method
priority	Method
tree	Method
.	O
bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
*	O
proof	O
:	O
Proof	O
.	O
The	O
bias	O
of	O
is	O
âˆŽ	O
subsection	O
:	O
Hyperparameter	Method
optimization	Method
As	O
we	O
believe	O
that	O
algorithms	O
should	O
be	O
robust	O
with	O
respect	O
to	O
the	O
choice	O
of	O
hyperparameters	O
,	O
we	O
spent	O
little	O
effort	O
on	O
parameter	Task
optimization	Task
.	O
In	O
total	O
,	O
we	O
explored	O
three	O
distinct	O
values	O
of	O
learning	Metric
rates	Metric
and	O
two	O
values	O
of	O
ADAM	O
momentum	O
(	O
the	O
default	O
and	O
zero	O
)	O
and	O
two	O
values	O
of	O
on	O
a	O
subset	O
of	O
7	O
Atari	Material
games	O
without	O
prioritization	Method
using	O
non	Method
-	Method
distributional	Method
version	Method
of	Method
Reactor	Method
.	O
We	O
later	O
used	O
those	O
values	O
for	O
all	O
experiments	O
.	O
We	O
did	O
not	O
optimize	O
for	O
batch	O
sizes	O
and	O
sequence	O
length	O
or	O
any	O
prioritization	Method
hyperparamters	Method
.	O
subsection	O
:	O
Rank	Metric
and	O
Elo	Metric
evaluation	Metric
Commonly	O
used	O
mean	O
and	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
have	O
several	O
disadvantages	O
.	O
A	O
mean	O
human	O
normalized	O
score	O
implicitly	O
puts	O
more	O
weight	O
on	O
games	O
that	O
computers	O
are	O
good	O
and	O
humans	O
are	O
bad	O
at	O
.	O
Comparing	O
algorithm	O
by	O
a	O
mean	O
human	Metric
normalized	Metric
score	Metric
across	O
57	O
Atari	Material
games	O
is	O
almost	O
equivalent	O
to	O
comparing	O
algorithms	O
on	O
a	O
small	O
subset	O
of	O
games	O
close	O
to	O
the	O
median	O
and	O
thus	O
dominating	O
the	O
signal	O
.	O
Typically	O
a	O
set	O
of	O
ten	O
most	O
score	O
-	O
generous	O
games	O
,	O
namely	O
Assault	O
,	O
Asterix	O
,	O
Breakout	O
,	O
Demon	O
Attack	O
,	O
Double	O
Dunk	O
,	O
Gopher	O
,	O
Pheonix	O
,	O
Stargunner	O
,	O
Upâ€™n	O
Down	O
and	O
Video	O
Pinball	O
can	O
explain	O
more	O
than	O
half	O
of	O
inter	Metric
-	Metric
algorithm	Metric
variance	Metric
.	O
A	O
median	Metric
human	Metric
normalized	Metric
score	Metric
has	O
the	O
opposite	O
disadvantage	O
by	O
effectively	O
discarding	O
very	O
easy	O
and	O
very	O
hard	O
games	O
from	O
the	O
comparison	O
.	O
As	O
typical	O
median	Metric
human	Metric
normalized	Metric
scores	Metric
are	O
within	O
the	O
range	O
of	O
1	O
-	O
2.5	O
,	O
an	O
algorithm	O
which	O
scores	O
zero	O
points	O
on	O
Montezuma	O
â€™s	O
Revenge	O
is	O
evaluated	O
equal	O
to	O
the	O
one	O
which	O
scores	O
2500	O
points	O
,	O
as	O
both	O
performance	O
levels	O
are	O
still	O
below	O
human	O
performance	O
making	O
incremental	O
improvements	O
on	O
hard	O
games	O
not	O
being	O
reflected	O
in	O
the	O
overall	O
evaluation	O
.	O
In	O
order	O
to	O
address	O
both	O
problem	O
,	O
we	O
also	O
evaluated	O
mean	Metric
rank	Metric
and	O
Elo	Metric
metrics	Metric
for	O
inter	Metric
-	Metric
algorithm	Metric
comparison	Metric
.	O
Those	O
metrics	O
implicitly	O
assign	O
the	O
same	O
weight	O
to	O
each	O
game	O
,	O
and	O
as	O
a	O
result	O
is	O
more	O
sensitive	O
of	O
relative	O
performance	O
on	O
very	O
hard	O
and	O
easy	O
games	O
:	O
swapping	O
scores	O
of	O
two	O
algorithms	O
on	O
any	O
game	O
would	O
result	O
in	O
the	O
change	O
of	O
both	O
mean	Metric
rank	Metric
and	O
Elo	Metric
metrics	Metric
.	O
We	O
calculated	O
separate	O
mean	Metric
rank	Metric
and	O
Elo	Metric
scores	Metric
for	O
each	O
algorithm	O
using	O
results	O
of	O
test	O
evaluations	O
with	O
30	O
random	O
noop	O
-	O
starts	O
and	O
30	O
random	O
human	O
starts	O
(	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O
All	O
algorithms	O
were	O
ranked	O
across	O
each	O
game	O
separately	O
,	O
and	O
a	O
mean	Metric
rank	Metric
was	O
evaluated	O
across	O
57	O
Atari	Material
games	O
.	O
For	O
Elo	Method
score	Method
evaluation	Method
algorithm	Method
,	O
was	O
considered	O
to	O
win	O
over	O
algorithm	O
if	O
it	O
obtained	O
more	O
scores	O
on	O
a	O
given	O
Atari	Material
.	O
We	O
produced	O
an	O
empirical	Metric
win	Metric
-	Metric
probability	Metric
matrix	Metric
by	O
summing	O
wins	O
across	O
all	O
games	O
and	O
used	O
this	O
matrix	O
to	O
evaluate	O
Elo	Metric
scores	Metric
.	O
A	O
ranking	O
difference	O
of	O
400	O
corresponds	O
to	O
the	O
odds	O
of	O
winning	O
of	O
10:1	O
under	O
the	O
Gaussian	Method
assumption	Method
.	O
subsection	O
:	O
Contextual	Method
priority	Method
tree	Method
Contextual	Method
priority	Method
tree	Method
is	O
one	O
possible	O
implementation	O
of	O
lazy	Task
prioritization	Task
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
All	O
sequence	O
keys	O
are	O
put	O
into	O
a	O
balanced	Method
binary	Method
search	Method
tree	Method
which	O
maintains	O
a	O
temporal	O
order	O
.	O
An	O
AVL	Method
tree	Method
(	O
)	O
was	O
chosen	O
due	O
to	O
the	O
ease	O
of	O
implementation	O
and	O
because	O
it	O
is	O
on	O
average	O
more	O
evenly	O
balanced	O
than	O
a	O
Red	O
-	O
Black	O
Tree	O
.	O
Each	O
tree	O
node	O
has	O
up	O
to	O
two	O
children	O
(	O
left	O
and	O
right	O
)	O
and	O
contains	O
currently	O
stored	O
key	O
and	O
a	O
priority	O
of	O
the	O
key	O
which	O
is	O
either	O
set	O
or	O
is	O
unknown	O
.	O
Some	O
trees	O
may	O
only	O
have	O
a	O
single	O
child	O
subtree	O
while	O
some	O
may	O
have	O
none	O
.	O
In	O
addition	O
to	O
this	O
information	O
,	O
we	O
were	O
tracking	O
other	O
summary	O
statistics	O
at	O
each	O
node	O
which	O
was	O
re	O
-	O
evaluated	O
after	O
each	O
tree	Method
rotation	Method
.	O
The	O
summary	Metric
statistics	Metric
was	O
evaluated	O
by	O
consuming	O
previously	O
evaluated	O
summary	Metric
statistics	Metric
of	O
both	O
children	O
and	O
a	O
priority	O
of	O
the	O
key	O
stored	O
within	O
the	O
current	O
node	O
.	O
In	O
particular	O
,	O
we	O
were	O
tracking	O
a	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
and	O
mean	O
-	O
priority	O
estimates	O
updated	O
according	O
to	O
rules	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
total	O
number	O
of	O
nodes	O
within	O
each	O
subtree	O
was	O
always	O
known	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
while	O
mean	O
priority	O
estimates	O
per	O
key	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
could	O
either	O
be	O
known	O
or	O
unknown	O
.	O
If	O
a	O
mean	O
priority	O
of	O
either	O
one	O
child	O
subtree	O
or	O
a	O
key	O
stored	O
within	O
the	O
current	O
node	O
is	O
unknown	O
then	O
it	O
can	O
be	O
estimated	O
to	O
by	O
exploiting	O
information	O
coming	O
from	O
another	O
sibling	O
subtree	O
or	O
a	O
priority	O
stored	O
within	O
the	O
parent	O
node	O
.	O
Sampling	O
was	O
done	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
up	O
while	O
sampling	O
either	O
one	O
of	O
the	O
children	O
subtrees	O
or	O
the	O
currently	O
held	O
key	O
proportionally	O
to	O
the	O
total	O
estimated	O
priority	O
masses	O
contained	O
within	O
.	O
The	O
rules	O
used	O
to	O
evaluate	O
proportions	O
are	O
shown	O
in	O
orange	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Similarly	O
,	O
probabilities	O
of	O
arbitrary	O
keys	O
can	O
be	O
queried	O
by	O
traversing	O
the	O
tree	O
from	O
the	O
root	O
node	O
towards	O
the	O
child	O
node	O
of	O
an	O
interest	O
while	O
maintaining	O
a	O
product	O
of	O
probabilities	O
at	O
each	O
branching	O
point	O
.	O
Insertion	Method
,	O
deletion	Method
,	O
sampling	Method
and	O
probability	Method
query	Method
operations	Method
can	O
be	O
done	O
in	O
O	O
(	O
ln	O
(	O
n	O
)	O
)	O
time	O
.	O
The	O
suggested	O
algorithm	O
has	O
the	O
desired	O
property	O
that	O
it	O
becomes	O
a	O
simple	O
proportional	Method
sampling	Method
algorithm	Method
once	O
all	O
the	O
priorities	O
are	O
known	O
.	O
While	O
some	O
key	O
priorities	O
are	O
unknown	O
,	O
they	O
are	O
estimated	O
by	O
using	O
nearby	O
known	O
key	O
priorities	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
Each	O
time	O
when	O
a	O
new	O
sequence	O
key	O
is	O
added	O
to	O
the	O
tree	O
,	O
it	O
was	O
set	O
to	O
have	O
an	O
unknown	O
priority	O
.	O
Any	O
priority	O
was	O
assigned	O
only	O
after	O
the	O
key	O
got	O
first	O
sampled	O
and	O
the	O
corresponding	O
sequence	O
got	O
passed	O
through	O
the	O
learner	O
.	O
When	O
a	O
priority	O
of	O
a	O
key	O
is	O
set	O
or	O
updated	O
,	O
the	O
key	O
node	O
is	O
deliberately	O
removed	O
from	O
and	O
placed	O
back	O
to	O
the	O
tree	O
in	O
order	O
to	O
become	O
a	O
leaf	O
-	O
node	O
.	O
This	O
helped	O
to	O
set	O
priorities	O
of	O
nodes	O
in	O
the	O
immediate	O
vicinity	O
more	O
accurately	O
by	O
using	O
the	O
freshest	O
information	O
available	O
.	O
subsection	O
:	O
Network	Method
architecture	Method
The	O
value	O
of	O
is	O
the	O
minimum	O
probability	O
of	O
choosing	O
a	O
random	O
action	O
and	O
it	O
is	O
hard	O
-	O
coded	O
into	O
the	O
policy	Method
network	Method
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
overall	O
network	O
topology	O
while	O
Table	O
[	O
reference	O
]	O
specifies	O
network	O
layer	O
sizes	O
.	O
subsection	O
:	O
Comparisons	O
with	O
Rainbow	O
In	O
this	O
section	O
we	O
compare	O
Reactor	Method
with	O
the	O
recently	O
published	O
Rainbow	Method
agent	Method
rainbow	Method
.	O
While	O
ACER	Method
is	O
the	O
most	O
closely	O
related	O
algorithmically	O
,	O
Rainbow	Method
is	O
most	O
closely	O
related	O
in	O
terms	O
of	O
performance	O
and	O
thus	O
a	O
deeper	O
understanding	O
of	O
the	O
trade	O
-	O
offs	O
between	O
Rainbow	Method
and	O
Reactor	Method
may	O
benefit	O
interested	O
readers	O
.	O
There	O
are	O
many	O
architectural	O
and	O
algorithmic	O
differences	O
between	O
Rainbow	Method
and	O
Reactor	Method
.	O
We	O
will	O
therefore	O
begin	O
by	O
highlighting	O
where	O
they	O
agree	O
.	O
Both	O
use	O
a	O
categorical	O
action	O
-	O
value	O
distribution	O
critic	O
bellemare2017distributional	O
,	O
factored	O
into	O
state	O
and	O
state	O
-	O
action	O
logits	O
wang2015dueling	O
,	O
Both	O
use	O
prioritized	Method
replay	Method
,	O
and	O
finally	O
,	O
both	O
perform	O
-	O
step	O
Bellman	Method
updates	Method
.	O
Despite	O
these	O
similarities	O
,	O
Reactor	Method
and	O
Rainbow	Method
are	O
fundamentally	O
different	O
algorithms	O
and	O
are	O
based	O
upon	O
different	O
lines	O
of	O
research	O
.	O
While	O
Rainbow	Method
uses	O
Q	Method
-	Method
Learning	Method
and	O
is	O
based	O
upon	O
DQN	Method
mnih15human	O
,	O
Reactor	Method
is	O
an	O
actor	Method
-	Method
critic	Method
algorithm	Method
most	O
closely	O
based	O
upon	O
A3C	Method
mnih2016asynchronous	O
.	O
Each	O
inherits	O
some	O
design	O
choices	O
from	O
their	O
predecessors	O
,	O
and	O
we	O
have	O
not	O
performed	O
an	O
extensive	O
ablation	O
comparing	O
these	O
various	O
differences	O
.	O
Instead	O
,	O
we	O
will	O
discuss	O
four	O
of	O
the	O
differences	O
we	O
believe	O
are	O
important	O
but	O
less	O
obvious	O
.	O
First	O
,	O
the	O
network	O
structures	O
are	O
substantially	O
different	O
.	O
Rainbow	Method
uses	O
noisy	Method
linear	Method
layers	Method
and	O
ReLU	O
activations	O
throughout	O
the	O
network	O
,	O
whereas	O
Reactor	Method
uses	O
standard	O
linear	Method
layers	Method
and	O
concatenated	O
ReLU	O
activations	O
throughout	O
.	O
To	O
overcome	O
partial	O
observability	O
,	O
Rainbow	Method
,	O
inheriting	O
this	O
choice	O
from	O
DQN	Method
,	O
uses	O
frame	Method
stacking	Method
.	O
On	O
the	O
other	O
hand	O
,	O
Reactor	Method
,	O
inheriting	O
its	O
choice	O
from	O
A3C	Method
,	O
uses	O
LSTMs	Method
after	O
the	O
convolutional	Method
layers	Method
of	O
the	O
network	O
.	O
It	O
is	O
also	O
difficult	O
to	O
directly	O
compare	O
the	O
number	O
of	O
parameters	O
in	O
each	O
network	O
because	O
the	O
use	O
of	O
noisy	Method
linear	Method
layers	Method
doubles	O
the	O
number	O
of	O
parameters	O
,	O
although	O
half	O
of	O
these	O
are	O
used	O
to	O
control	O
noise	O
,	O
while	O
the	O
LSTM	Method
units	Method
in	O
Reactor	Method
require	O
more	O
parameters	O
than	O
a	O
corresponding	O
linear	Method
layer	Method
would	O
.	O
Second	O
,	O
both	O
algorithms	O
perform	O
-	O
step	O
updates	O
,	O
however	O
,	O
the	O
Rainbow	Method
-	Method
step	Method
update	Method
does	O
not	O
use	O
any	O
form	O
of	O
off	Method
-	Method
policy	Method
correction	Method
.	O
Because	O
of	O
this	O
,	O
Rainbow	Method
is	O
restricted	O
to	O
using	O
only	O
small	O
values	O
of	O
(	O
e.g.	O
)	O
because	O
larger	O
values	O
would	O
make	O
sequences	O
more	O
off	O
-	O
policy	O
and	O
hurt	O
performance	O
.	O
By	O
comparison	O
,	O
Reactor	Method
uses	O
our	O
proposed	O
distributional	Method
Retrace	Method
algorithm	Method
for	O
off	Task
-	Task
policy	Task
correction	Task
of	Task
-	Task
step	Task
updates	Task
.	O
This	O
allows	O
the	O
use	O
of	O
larger	O
values	O
of	O
(	O
e.g.	O
)	O
without	O
loss	O
of	O
performance	O
.	O
Third	O
,	O
while	O
both	O
agents	O
use	O
prioritized	O
replay	O
buffers	O
schaul16prioritized	O
,	O
they	O
each	O
store	O
different	O
information	O
and	O
prioritize	O
using	O
different	O
algorithms	O
.	O
Rainbow	Method
stores	O
a	O
tuple	O
containing	O
the	O
state	O
,	O
action	O
,	O
sum	O
of	O
discounted	O
rewards	O
,	O
product	O
of	O
discount	O
factors	O
,	O
and	O
next	O
-	O
state	O
steps	O
away	O
.	O
Tuples	O
are	O
prioritized	O
based	O
upon	O
the	O
last	O
observed	O
TD	O
error	O
,	O
and	O
inserted	O
into	O
replay	O
with	O
a	O
maximum	O
priority	O
.	O
Reactor	Method
stores	O
length	O
sequences	O
of	O
tuples	O
and	O
also	O
prioritizes	O
based	O
upon	O
the	O
observed	O
TD	Metric
error	Metric
.	O
However	O
,	O
when	O
inserted	O
into	O
the	O
buffer	O
the	O
priority	O
is	O
instead	O
inferred	O
based	O
upon	O
the	O
known	O
priorities	O
of	O
neighboring	O
sequences	O
.	O
This	O
priority	Method
inference	Method
was	O
made	O
efficient	O
using	O
the	O
previously	O
introduced	O
contextual	Method
priority	Method
tree	Method
,	O
and	O
anecdotally	O
we	O
have	O
seen	O
it	O
improve	O
performance	O
over	O
a	O
simple	O
maximum	Method
priority	Method
approach	Method
.	O
Finally	O
,	O
the	O
two	O
algorithms	O
have	O
different	O
approaches	O
to	O
exploration	Task
.	O
Rainbow	Method
,	O
unlike	O
DQN	Method
,	O
does	O
not	O
use	O
-	Method
greedy	Method
exploration	Method
,	O
but	O
instead	O
replaces	O
all	O
linear	O
layers	O
with	O
noisy	O
linear	O
layers	O
which	O
induce	O
randomness	O
throughout	O
the	O
network	O
.	O
This	O
method	O
,	O
called	O
Noisy	Method
Networks	Method
fortunato2017noisy	O
,	O
creates	O
an	O
adaptive	Method
exploration	Method
integrated	O
into	O
the	O
agent	Method
â€™s	Method
network	Method
.	O
Reactor	Method
does	O
not	O
use	O
noisy	Method
networks	Method
,	O
but	O
instead	O
uses	O
the	O
same	O
entropy	Method
cost	Method
method	Method
used	O
by	O
A3C	O
and	O
many	O
others	O
mnih2016asynchronous	O
,	O
which	O
penalizes	O
deterministic	Method
policies	Method
thus	O
encouraging	O
indifference	O
between	O
similarly	O
valued	O
actions	O
.	O
Because	O
Rainbow	O
can	O
essentially	O
learn	O
not	O
to	O
explore	O
,	O
it	O
may	O
learn	O
to	O
become	O
entirely	O
greedy	O
in	O
the	O
early	O
parts	O
of	O
the	O
episode	O
,	O
while	O
still	O
exploring	O
in	O
states	O
not	O
as	O
frequently	O
seen	O
.	O
In	O
some	O
sense	O
,	O
this	O
is	O
precisely	O
what	O
we	O
want	O
from	O
an	O
exploration	Method
technique	Method
,	O
but	O
it	O
may	O
also	O
lead	O
to	O
highly	O
deterministic	O
trajectories	O
in	O
the	O
early	O
part	O
of	O
the	O
episode	O
and	O
an	O
increase	O
in	O
overfitting	O
to	O
those	O
trajectories	O
.	O
We	O
hypothesize	O
that	O
this	O
may	O
be	O
the	O
explanation	O
for	O
the	O
significant	O
difference	O
in	O
Rainbow	O
â€™s	O
performance	O
between	O
evaluation	Task
under	O
no	O
-	O
op	O
and	O
random	O
human	O
starts	O
,	O
and	O
why	O
Reactor	Method
does	O
not	O
show	O
such	O
a	O
large	O
difference	O
.	O
subsection	O
:	O
Atari	Material
results	O
