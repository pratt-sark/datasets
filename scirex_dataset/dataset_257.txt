document O
: O
DialogueRNN Method
: O
An O
Attentive O
RNN Method
for O
Emotion Task
Detection Task
in O
Conversations Task
Emotion Task
detection Task
in O
conversations O
is O
a O
necessary O
step O
for O
a O
number O
of O
applications O
, O
including O
opinion Task
mining Task
over O
chat O
history O
, O
social O
media O
threads O
, O
debates O
, O
argumentation Task
mining Task
, O
understanding Task
consumer Task
feedback Task
in O
live O
conversations O
, O
etc O
. O
Currently O
systems O
do O
not O
treat O
the O
parties O
in O
the O
conversation O
individually O
by O
adapting O
to O
the O
speaker O
of O
each O
utterance O
. O
In O
this O
paper O
, O
we O
describe O
a O
new O
method O
based O
on O
recurrent Method
neural Method
networks Method
that O
keeps O
track O
of O
the O
individual O
party O
states O
throughout O
the O
conversation O
and O
uses O
this O
information O
for O
emotion Task
classification Task
. O
Our O
model O
outperforms O
the O
state O
of O
the O
art O
by O
a O
significant O
margin O
on O
two O
different O
datasets O
. O
section O
: O
Introduction O
Emotion Task
detection Task
in O
conversation O
attracts O
increasing O
attention O
of O
the O
community O
due O
to O
its O
applications O
in O
many O
important O
tasks O
such O
as O
opinion Task
mining Task
over O
chat O
history O
and O
social O
media O
threads O
in O
YouTube O
, O
Facebook O
, O
Twitter O
, O
etc O
. O
In O
this O
paper O
, O
we O
present O
a O
method O
based O
on O
recurrent Method
neural Method
network Method
( O
RNN Method
) O
that O
can O
cater O
to O
these O
needs O
by O
processing O
the O
huge O
amount O
of O
available O
conversational O
data O
. O
Current O
systems O
, O
including O
the O
state O
of O
the O
art O
, O
do O
not O
distinguish O
different O
parties O
in O
a O
conversation O
in O
a O
meaningful O
way O
. O
They O
are O
not O
aware O
of O
the O
speaker O
of O
a O
given O
utterance O
. O
In O
contrast O
, O
we O
model O
individual O
party O
with O
party O
states O
, O
as O
the O
conversation O
flows O
, O
basing O
on O
the O
utterance O
, O
the O
context O
, O
and O
current O
party O
state O
. O
Our O
model O
is O
based O
on O
the O
assumption O
that O
there O
are O
three O
major O
aspects O
relevant O
to O
the O
emotion O
in O
a O
conversation O
: O
the O
speaker O
, O
the O
context O
from O
the O
preceding O
utterances O
, O
and O
the O
emotion O
of O
the O
preceding O
utterances O
. O
These O
three O
aspects O
are O
not O
necessarily O
independent O
, O
but O
their O
separate O
modeling O
significantly O
outperforms O
the O
state O
of O
the O
art O
( O
tab O
: O
results O
- O
text O
) O
. O
In O
dyadic O
conversations O
, O
the O
parties O
have O
distinct O
roles O
. O
Hence O
, O
to O
extract O
the O
context O
, O
it O
is O
crucial O
to O
consider O
the O
preceding O
turns O
of O
both O
speaker O
and O
listener O
at O
a O
given O
moment O
( O
fig O
: O
example O
) O
. O
Our O
DialogueRNN Method
employs O
three O
gated Method
recurrent Method
units Method
( O
GRU Method
) O
to O
model O
these O
aspects O
. O
The O
incoming O
utterance O
is O
fed O
into O
two O
GRUs Method
called O
global Method
GRU Method
and O
party Method
GRU Method
to O
update O
the O
context O
and O
party O
state O
, O
respectively O
. O
The O
global Method
GRU Method
encodes O
corresponding O
party O
information O
while O
encoding O
an O
utterance O
. O
Attending O
over O
this O
GRU Method
gives O
contextual Method
representation Method
that O
has O
information O
of O
all O
preceding O
utterances O
by O
different O
parties O
in O
the O
conversation O
. O
The O
speaker O
state O
depends O
on O
this O
context O
through O
attention O
and O
the O
speaker O
’s O
previous O
state O
. O
This O
ensures O
that O
at O
time O
, O
the O
speaker O
state O
directly O
gets O
information O
from O
the O
speaker O
’s O
previous O
state O
and O
global O
GRU Method
which O
has O
information O
on O
the O
preceding O
parties O
. O
Finally O
, O
the O
updated O
speaker O
state O
is O
fed O
into O
the O
emotion O
GRU Method
to O
decode O
the O
emotion Method
representation Method
of O
the O
given O
utterance O
, O
which O
is O
used O
for O
emotion Task
classification Task
. O
At O
time O
, O
emotion Method
GRU Method
cell Method
gets O
the O
emotion O
representation O
of O
and O
speaker O
state O
of O
. O
The O
emotion O
GRU Method
, O
along O
with O
the O
global Method
GRU Method
, O
plays O
a O
pivotal O
role O
in O
inter Task
- Task
party Task
relation Task
modeling Task
. O
On O
the O
other O
hand O
, O
party Method
GRU Method
models O
relation O
between O
two O
sequential O
states O
of O
the O
same O
party O
. O
In O
DialogueRNN Method
, O
all O
these O
three O
different O
types O
of O
GRUs Method
are O
connected O
in O
a O
recurrent O
manner O
. O
We O
believe O
that O
DialogueRNN Method
outperforms O
state O
- O
of O
- O
the O
- O
art O
contextual Method
emotion Method
classifiers Method
such O
as O
because O
of O
better O
context Method
representation Method
. O
The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
: O
sec O
: O
related O
- O
works O
discusses O
related O
work O
; O
sec O
: O
method O
provides O
detailed O
description O
of O
our O
model O
; O
sec O
: O
experiments O
, O
sec O
: O
results O
- O
discussion O
present O
the O
experimental O
results O
; O
finally O
, O
sec O
: O
conclusion O
concludes O
the O
paper O
. O
section O
: O
Related O
Work O
Emotion Task
recognition Task
has O
attracted O
attention O
in O
various O
fields O
such O
as O
natural Task
language Task
processing Task
, O
psychology Task
, O
cognitive Task
science Task
, O
and O
so O
on O
. O
ekman1993facial O
ekman1993facial O
found O
correlation O
between O
emotion O
and O
facial O
cues O
. O
datcu2008semantic O
datcu2008semantic O
fused O
acoustic O
information O
with O
visual O
cues O
for O
emotion Task
recognition Task
. O
alm2005emotions O
alm2005emotions O
introduced O
text Task
- Task
based Task
emotion Task
recognition Task
, O
developed O
in O
the O
work O
of O
strapparava2010annotating O
strapparava2010annotating O
. O
wollmer2010context O
wollmer2010context O
used O
contextual O
information O
for O
emotion Task
recognition Task
in O
multimodal Task
setting Task
. O
Recently O
, O
poria O
- O
EtAl:2017:Long O
poria O
- O
EtAl:2017:Long O
successfully O
used O
RNN Method
- O
based O
deep O
networks O
for O
multimodal Task
emotion Task
recognition Task
, O
which O
was O
followed O
by O
other O
works O
. O
Reproducing Task
human Task
interaction Task
requires O
deep Task
understanding Task
of Task
conversation Task
. O
ruusuvuori2013emotion O
ruusuvuori2013emotion O
states O
that O
emotion O
plays O
a O
pivotal O
role O
in O
conversations O
. O
It O
has O
been O
argued O
that O
emotional O
dynamics O
in O
a O
conversation O
is O
an O
inter O
- O
personal O
phenomenon O
. O
Hence O
, O
our O
model O
incorporates O
inter O
- O
personal O
interactions O
in O
an O
effective O
way O
. O
Further O
, O
since O
conversations O
have O
a O
natural O
temporal O
nature O
, O
we O
adopt O
the O
temporal O
nature O
through O
recurrent Method
network Method
. O
Memory Method
networks Method
has O
been O
successful O
in O
several O
NLP Task
areas Task
, O
including O
question Task
answering Task
, O
machine Task
translation Task
, O
speech Task
recognition Task
, O
and O
so O
on O
. O
Thus O
, O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
used O
memory Method
networks Method
for O
emotion Task
recognition Task
in O
dyadic O
conversations O
, O
where O
two O
distinct O
memory Method
networks Method
enabled O
inter Task
- Task
speaker Task
interaction Task
, O
yielding O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
section O
: O
Methodology O
subsection O
: O
Problem O
Definition O
Let O
there O
be O
parties O
/ O
participants O
( O
for O
the O
datasets O
we O
used O
) O
in O
a O
conversation O
. O
The O
task O
is O
to O
predict O
the O
emotion O
labels O
( O
happy O
, O
sad O
, O
neutral O
, O
angry O
, O
excited O
, O
and O
frustrated O
) O
of O
the O
constituent O
utterances O
, O
where O
utterance O
is O
uttered O
by O
party O
, O
while O
being O
the O
mapping O
between O
utterance O
and O
index O
of O
its O
corresponding O
party O
. O
Also O
, O
is O
the O
utterance Method
representation Method
, O
obtained O
using O
feature Method
extractors Method
described O
below O
. O
subsection O
: O
Unimodal Task
Feature Task
Extraction Task
For O
a O
fair O
comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
method O
, O
conversational Method
memory Method
networks Method
( O
CMN Method
) O
, O
we O
follow O
identical O
feature Method
extraction Method
procedures Method
. O
subsubsection O
: O
Textual Method
Feature Method
Extraction Method
We O
employ O
convolutional Method
neural Method
networks Method
( O
CNN Method
) O
for O
textual Task
feature O
extraction O
. O
Following O
kim2014convolutional O
kim2014convolutional O
, O
we O
obtain O
n O
- O
gram O
features O
from O
each O
utterance O
using O
three O
distinct O
convolution Method
filters Method
of O
sizes O
3 O
, O
4 O
, O
and O
5 O
respectively O
, O
each O
having O
50 O
feature O
- O
maps O
. O
Outputs O
are O
then O
subjected O
to O
max Method
- Method
pooling Method
followed O
by O
rectified Method
linear Method
unit Method
( O
ReLU Method
) O
activation O
. O
These O
activations O
are O
concatenated O
and O
fed O
to O
a O
dimensional Method
dense Method
layer Method
, O
which O
is O
regarded O
as O
the O
textual Task
utterance O
representation O
. O
This O
network O
is O
trained O
at O
utterance O
level O
with O
the O
emotion O
labels O
. O
subsubsection O
: O
Audio Task
and Task
Visual Task
Feature Task
Extraction Task
Identical O
to O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
, O
we O
use O
3D Method
- Method
CNN Method
and O
openSMILE Method
for O
visual Task
and Task
acoustic Task
feature Task
extraction Task
, O
respectively O
. O
subsection O
: O
Our O
Model O
We O
assume O
that O
the O
emotion O
of O
an O
utterance O
in O
a O
conversation O
depends O
on O
three O
major O
factors O
: O
the O
speaker O
. O
the O
context O
given O
by O
the O
preceding O
utterances O
. O
the O
emotion O
behind O
the O
preceding O
utterances O
. O
Our O
model O
DialogueRNN Method
, O
shown O
in O
fig O
: O
architecture O
, O
models O
these O
three O
factors O
as O
follows O
: O
each O
party O
is O
modeled O
using O
a O
party O
state O
which O
changes O
as O
and O
when O
that O
party O
utters O
an O
utterance O
. O
This O
enables O
the O
model O
to O
track O
the O
parties O
’ O
emotion O
dynamics O
through O
the O
conversations O
, O
which O
is O
related O
to O
the O
emotion O
behind O
the O
utterances O
. O
Furthermore O
, O
the O
context O
of O
an O
utterance O
is O
modeled O
using O
a O
global O
state O
( O
called O
global O
, O
because O
of O
being O
shared O
among O
the O
parties O
) O
, O
where O
the O
preceding O
utterances O
and O
the O
party O
states O
are O
jointly O
encoded O
for O
context Method
representation Method
, O
necessary O
for O
accurate O
party Method
state Method
representation Method
. O
Finally O
, O
the O
model O
infers O
emotion Method
representation Method
from O
the O
party O
state O
of O
the O
speaker O
along O
with O
the O
preceding O
speakers O
’ O
states O
as O
context O
. O
This O
emotion Method
representation Method
is O
used O
for O
the O
final O
emotion Task
classification Task
. O
We O
use O
GRU O
cells O
to O
update O
the O
states O
and O
representations O
. O
Each O
GRU Method
cell Method
computes O
a O
hidden O
state O
defined O
as O
, O
where O
is O
the O
current O
input O
and O
is O
the O
previous O
GRU O
state O
. O
also O
serves O
as O
the O
current O
GRU O
output O
. O
We O
provide O
the O
GRU O
computation O
details O
in O
the O
supplementary O
. O
GRUs Method
are O
efficient O
networks O
with O
trainable O
parameters O
: O
and O
. O
We O
model O
the O
emotion Method
representation Method
of O
the O
current O
utterance O
as O
a O
function O
of O
the O
emotion Method
representation Method
of O
the O
previous O
utterance O
and O
the O
state O
of O
the O
current O
speaker O
. O
Finally O
, O
this O
emotion Method
representation Method
is O
sent O
to O
a O
softmax Method
layer Method
for O
emotion Task
classification Task
. O
0.73 O
0.25 O
subsubsection O
: O
Global O
State O
( O
Global O
GRU O
) O
Global O
state O
aims O
to O
capture O
the O
context O
of O
a O
given O
utterance O
by O
jointly O
encoding O
utterance O
and O
speaker O
state O
. O
Each O
state O
also O
serves O
as O
speaker Method
- Method
specific Method
utterance Method
representation Method
. O
Attending O
on O
these O
states O
facilitates O
the O
inter O
- O
speaker O
and O
inter O
- O
utterance O
dependencies O
to O
produce O
improved O
context Method
representation Method
. O
The O
current O
utterance O
changes O
the O
speaker O
’s O
state O
from O
to O
. O
We O
capture O
this O
change O
with O
GRU O
cell O
with O
output O
size O
, O
using O
and O
: O
where O
is O
the O
size O
of O
global O
state O
vector O
, O
is O
the O
size O
of O
party O
state O
vector O
, O
, O
, O
, O
, O
, O
is O
party O
state O
size O
, O
and O
represents O
concatenation O
. O
subsubsection O
: O
Party O
State O
( O
Party O
GRU O
) O
DialogueRNN Method
keeps O
track O
of O
the O
state O
of O
individual O
speakers O
using O
fixed O
size O
vectors O
through O
out O
the O
conversation O
. O
These O
states O
are O
representative O
of O
the O
speakers O
’ O
state O
in O
the O
conversation O
, O
relevant O
to O
emotion Task
classification Task
. O
We O
update O
these O
states O
based O
on O
the O
current O
( O
at O
time O
) O
role O
of O
a O
participant O
in O
the O
conversation O
, O
which O
is O
either O
speaker O
or O
listener O
, O
and O
the O
incoming O
utterance O
. O
These O
state O
vectors O
are O
initialized O
with O
null O
vectors O
for O
all O
the O
participants O
. O
The O
main O
purpose O
of O
this O
module O
is O
to O
ensure O
that O
the O
model O
is O
aware O
of O
the O
speaker O
of O
each O
utterance O
and O
handle O
it O
accordingly O
. O
subsubsection O
: O
Speaker Method
Update Method
( O
Speaker Method
GRU Method
) O
: O
Speaker O
usually O
frames O
the O
response O
based O
on O
the O
context O
, O
which O
is O
the O
preceding O
utterances O
in O
the O
conversation O
. O
Hence O
, O
we O
capture O
context O
relevant O
to O
the O
utterance O
as O
follows O
: O
where O
are O
preceding O
global O
states O
( O
) O
, O
, O
, O
and O
. O
In O
eq:7 O
, O
we O
calculate O
attention O
scores O
over O
the O
previous O
global O
states O
representative O
of O
the O
previous O
utterances O
. O
This O
assigns O
higher O
attention O
scores O
to O
the O
utterances O
emotionally O
relevant O
to O
. O
Finally O
, O
in O
eq:6 O
the O
context O
vector O
is O
calculated O
by O
pooling O
the O
previous O
global O
states O
with O
. O
Now O
, O
we O
employ O
a O
GRU Method
cell Method
to O
update O
the O
current O
speaker O
state O
to O
the O
new O
state O
based O
on O
incoming O
utterance O
and O
context O
using O
GRU O
cell O
of O
output O
size O
where O
, O
, O
, O
and O
. O
This O
encodes O
the O
information O
on O
the O
current O
utterance O
along O
with O
its O
context O
from O
the O
global O
GRU O
into O
the O
speaker O
’s O
state O
, O
which O
helps O
in O
emotion Task
classification Task
down O
the O
line O
. O
subsubsection O
: O
Listener O
Update O
: O
Listener O
state O
models O
the O
listeners O
’ O
change O
of O
state O
due O
to O
the O
speaker O
’s O
utterance O
. O
We O
tried O
two O
listener Method
state Method
update Method
mechanisms Method
: O
Simply O
keep O
the O
state O
of O
the O
listener O
unchanged O
, O
that O
is O
Employ O
another O
GRU Method
cell Method
to O
update O
the O
listener O
state O
based O
on O
listener O
visual O
cues O
( O
facial O
expression O
) O
and O
its O
context O
, O
as O
where O
, O
, O
, O
and O
. O
Listener O
visual O
features O
of O
party O
at O
time O
are O
extracted O
using O
the O
model O
introduced O
by O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1710 O
- O
07557 O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1710 O
- O
07557 O
, O
pretrained O
on O
FER2013 Material
dataset Material
, O
where O
feature Metric
size Metric
. O
The O
simpler O
first O
approach O
turns O
out O
to O
be O
sufficient O
, O
since O
the O
second O
approach O
yields O
very O
similar O
result O
while O
increasing O
number O
of O
parameters O
. O
This O
is O
due O
to O
the O
fact O
that O
a O
listener O
becomes O
relevant O
to O
the O
conversation O
only O
when O
he O
/ O
she O
speaks O
. O
In O
other O
words O
, O
a O
silent O
party O
has O
no O
influence O
in O
a O
conversation O
. O
Now O
, O
when O
a O
party O
speaks O
, O
we O
update O
his O
/ O
her O
state O
with O
context O
which O
contains O
relevant O
information O
on O
all O
the O
preceding O
utterances O
, O
rendering O
explicit O
listener O
state O
update O
unnecessary O
. O
This O
is O
shown O
in O
tab O
: O
results O
- O
text O
. O
subsubsection O
: O
Emotion Method
Representation Method
( O
Emotion Method
GRU Method
) O
We O
infer O
the O
emotionally Method
relevant Method
representation Method
of O
utterance O
from O
the O
speaker O
’s O
state O
and O
the O
emotion Method
representation Method
of O
the O
previous O
utterance O
. O
Since O
context O
is O
important O
to O
the O
emotion O
of O
the O
incoming O
utterance O
, O
feeds O
fine O
- O
tuned O
emotionally O
relevant O
contextual O
information O
from O
other O
the O
party O
states O
into O
the O
emotion Method
representation Method
. O
This O
establishes O
a O
connection O
between O
the O
speaker O
state O
and O
the O
other O
party O
states O
. O
Hence O
, O
we O
model O
with O
a O
GRU Method
cell Method
( O
) O
with O
output O
size O
as O
where O
is O
the O
size O
of O
emotion O
representation O
vector O
, O
, O
, O
, O
and O
. O
Since O
speaker O
state O
gets O
information O
from O
global O
states O
, O
which O
serve O
as O
speaker Method
- Method
specific Method
utterance Method
representation Method
, O
one O
may O
claim O
that O
this O
way O
the O
model O
already O
has O
access O
to O
the O
information O
on O
other O
parties O
. O
However O
, O
as O
shown O
in O
the O
ablation O
study O
( O
sec O
: O
ablation O
- O
study O
) O
emotion Method
GRU Method
helps O
to O
improve O
the O
performance O
by O
directly O
linking O
states O
of O
preceding O
parties O
. O
Further O
, O
we O
believe O
that O
speaker O
and O
global O
GRUs O
( O
, O
) O
jointly O
act O
similar O
to O
an O
encoder Method
, O
whereas O
emotion Method
GRU Method
serves O
as O
a O
decoder Method
. O
subsubsection O
: O
Emotion Task
Classification Task
We O
use O
a O
two Method
- Method
layer Method
perceptron Method
with O
a O
final O
softmax Method
layer Method
to O
calculate O
emotion O
- O
class O
probabilities O
from O
emotion Method
representation Method
of O
utterance O
and O
then O
we O
pick O
the O
most O
likely O
emotion O
class O
: O
where O
, O
, O
, O
, O
, O
and O
is O
the O
predicted O
label O
for O
utterance O
. O
subsubsection O
: O
Training O
We O
use O
categorical Metric
cross Metric
- Metric
entropy Metric
along O
with O
L2 Method
- Method
regularization Method
as O
the O
measure Metric
of Metric
loss Metric
( Metric
) O
during O
training O
: O
where O
is O
the O
number O
of O
samples O
/ O
dialogues O
, O
is O
the O
number O
of O
utterances O
in O
sample O
, O
is O
the O
probability O
distribution O
of O
emotion O
labels O
for O
utterance O
of O
dialogue O
, O
is O
the O
expected O
class O
label O
of O
utterance O
of O
dialogue O
, O
is O
the O
L2 O
- O
regularizer O
weight O
, O
and O
is O
the O
set O
of O
trainable O
parameters O
where O
We O
used O
stochastic Method
gradient Method
descent Method
based Method
Adam Method
optimizer Method
to O
train O
our O
network O
. O
Hyperparameters Method
are O
optimized O
using O
grid Method
search Method
( O
values O
are O
added O
to O
the O
supplementary O
material O
) O
. O
subsection O
: O
DialogueRNN Method
Variants Method
We O
use O
DialogueRNN Method
( O
sec O
: O
model O
) O
as O
the O
basis O
for O
the O
following O
models O
: O
subsubsection O
: O
DialogueRNN Method
+ O
Listener O
State O
Update O
( O
DialogueRNN Method
) O
: O
This O
variant O
updates O
the O
listener O
state O
based O
on O
the O
the O
resulting O
speaker O
state O
, O
as O
described O
in O
eq:8 O
. O
subsubsection O
: O
Bidirectional Method
DialogueRNN Method
( O
BiDialogueRNN Method
) O
: O
Bidirectional Method
DialogueRNN Method
is O
analogous O
to O
bidirectional Method
RNNs Method
, O
where O
two O
different O
RNNs Method
are O
used O
for O
forward O
and O
backward O
passes O
of O
the O
input O
sequence O
. O
Outputs O
from O
the O
RNNs Method
are O
concatenated O
in O
sequence O
level O
. O
Similarly O
, O
in O
BiDialogueRNN Method
, O
the O
final O
emotion Method
representation Method
contains O
information O
from O
both O
past O
and O
future O
utterances O
in O
the O
dialogue O
through O
forward O
and O
backward O
DialogueRNNs O
respectively O
, O
which O
provides O
better O
context O
for O
emotion Task
classification Task
. O
subsubsection O
: O
DialogueRNN Method
+ O
attention O
( O
DialogueRNN Method
+ O
Att O
) O
: O
For O
each O
emotion Method
representation Method
, O
attention O
is O
applied O
over O
all O
surrounding O
emotion O
representations O
in O
the O
dialogue O
by O
matching O
them O
with O
( O
eq O
: O
beta O
, O
eq O
: O
beta O
- O
2 O
) O
. O
This O
provides O
context O
from O
the O
relevant O
( O
based O
on O
attention O
score O
) O
future O
and O
preceding O
utterances O
. O
subsubsection O
: O
Bidirectional O
DialogueRNN Method
+ O
Emotional O
attention O
( O
BiDialogueRNN Method
+ O
Att O
) O
: O
For O
each O
emotion Method
representation Method
of O
BiDialogueRNN Method
, O
attention O
is O
applied O
over O
all O
the O
emotion Method
representations Method
in O
the O
dialogue O
to O
capture O
context O
from O
the O
other O
utterances O
in O
dialogue O
: O
where O
, O
, O
, O
and O
. O
Further O
, O
are O
fed O
to O
a O
two Method
- Method
layer Method
perceptron Method
for O
emotion Task
classification Task
, O
as O
in O
eq:5 O
, O
eq O
: O
c O
- O
6 O
, O
eq O
: O
c O
- O
7 O
. O
section O
: O
Experimental O
Setting O
subsection O
: O
Datasets O
Used O
We O
use O
two O
emotion O
detection O
datasets O
IEMOCAP Material
and O
AVEC Material
to O
evaluate O
DialogueRNN Method
. O
We O
partition O
both O
datasets O
into O
train O
and O
test O
sets O
with O
roughly O
ratio O
such O
that O
the O
partitions O
do O
not O
share O
any O
speaker O
. O
table O
: O
dataset O
shows O
the O
distribution O
of O
train O
and O
test O
samples O
for O
both O
dataset O
. O
subsubsection O
: O
IEMOCAP Material
: O
IEMOCAP Material
dataset O
contains O
videos O
of O
two O
- O
way O
conversations O
of O
ten O
unique O
speakers O
, O
where O
only O
the O
first O
eight O
speakers O
from O
session O
one O
to O
four O
belong O
to O
the O
train O
- O
set O
. O
Each O
video O
contains O
a O
single O
dyadic O
dialogue O
, O
segmented O
into O
utterances O
. O
The O
utterances O
are O
annotated O
with O
one O
of O
six O
emotion O
labels O
, O
which O
are O
happy O
, O
sad O
, O
neutral O
, O
angry O
, O
excited O
, O
and O
frustrated O
. O
subsubsection O
: O
AVEC Material
: O
AVEC Material
dataset Material
is O
a O
modification O
of O
SEMAINE O
database O
containing O
interactions O
between O
humans O
and O
artificially O
intelligent O
agents O
. O
Each O
utterance O
of O
a O
dialogue O
is O
annotated O
with O
four O
real O
valued O
affective O
attributes O
: O
valence O
( O
) O
, O
arousal O
( O
) O
, O
expectancy O
( O
) O
, O
and O
power O
( O
) O
. O
The O
annotations O
are O
available O
every O
0.2 O
seconds O
in O
the O
original O
database O
. O
However O
, O
in O
order O
to O
adapt O
the O
annotations O
to O
our O
need O
of O
utterance Task
- Task
level Task
annotation Task
, O
we O
averaged O
the O
attributes O
over O
the O
span O
of O
an O
utterance O
. O
subsection O
: O
Baselines O
and O
State O
of O
the O
Art O
For O
a O
comprehensive O
evaluation O
of O
DialogueRNN Method
, O
we O
compare O
our O
model O
with O
the O
following O
baseline O
methods O
: O
subsubsection O
: O
c Method
- Method
LSTM Method
: O
Biredectional Method
LSTM Method
is O
used O
to O
capture O
the O
context O
from O
the O
surrounding O
utterances O
to O
generate O
context Method
- Method
aware Method
utterance Method
representation Method
. O
However O
, O
this O
model O
does O
not O
differentiate O
among O
the O
speakers O
. O
subsubsection O
: O
c Method
- Method
LSTM Method
+ Method
Att Method
: O
In O
this O
variant O
attention O
is O
applied O
applied O
to O
the O
c Method
- Method
LSTM Method
output Method
at O
each O
timestamp O
by O
following O
eq O
: O
beta O
, O
eq O
: O
beta O
- O
2 O
. O
This O
provides O
better O
context O
to O
the O
final O
utterance Method
representation Method
. O
subsubsection O
: O
TFN Method
: O
This O
is O
specific O
to O
multimodal Task
scenario Task
. O
Tensor Method
outer Method
product Method
is O
used O
to O
capture O
inter O
- O
modality O
and O
intra O
- O
modality O
interactions O
. O
This O
model O
does O
not O
capture O
context O
from O
surrounding O
utterances O
. O
subsubsection O
: O
MFN Method
: O
Specific O
to O
multimodal Task
scenario Task
, O
this O
model O
utilizes O
multi Method
- Method
view Method
learning Method
by O
modeling O
view O
- O
specific O
and O
cross O
- O
view O
interactions O
. O
Similar O
to O
TFN Method
, O
this O
model O
does O
not O
use O
contextual O
information O
. O
subsubsection O
: O
CNN Method
: O
This O
is O
identical O
to O
our O
textual Task
feature O
extractor O
network O
( O
sec O
: O
feature Method
- Method
extraction Method
) O
and O
it O
does O
not O
use O
contextual O
information O
from O
the O
surrounding O
utterances O
. O
subsubsection O
: O
Memnet Method
: O
As O
described O
in O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
, O
the O
current O
utterance O
is O
fed O
to O
a O
memory Method
network Method
, O
where O
the O
memories O
correspond O
to O
preceding O
utterances O
. O
The O
output O
from O
the O
memory Method
network Method
is O
used O
as O
the O
final O
utterance Method
representation Method
for O
emotion Task
classification Task
. O
subsubsection O
: O
CMN Method
: O
This O
state O
- O
of O
- O
the O
- O
art O
method O
models O
utterance O
context O
from O
dialogue O
history O
using O
two O
distinct O
GRUs Method
for O
two O
speakers O
. O
Finally O
, O
utterance Method
representation Method
is O
obtained O
by O
feeding O
the O
current O
utterance O
as O
query O
to O
two O
distinct O
memory Method
networks Method
for O
both O
speakers O
. O
subsection O
: O
Modalities O
We O
evaluated O
our O
model O
primarily O
on O
textual Task
modality O
. O
However O
, O
to O
substantiate O
efficacy O
of O
our O
model O
in O
multimodal Task
scenario Task
, O
we O
also O
experimented O
with O
multimodal O
features O
. O
section O
: O
Results O
and O
Discussion O
We O
compare O
DialogueRNN Method
and O
its O
variants O
with O
the O
baselines O
for O
textual Task
data O
in O
tab O
: O
results O
- O
text O
. O
As O
expected O
, O
on O
average O
DialogueRNN Method
outperforms O
all O
the O
baseline O
methods O
, O
including O
the O
state O
- O
of O
- O
the O
- O
art O
CMN Method
, O
on O
both O
of O
the O
datasets O
. O
subsection O
: O
Comparison O
with O
the O
State O
of O
the O
Art O
We O
compare O
the O
performance O
of O
DialogueRNN Method
against O
the O
performance O
of O
the O
state O
- O
of O
- O
the O
- O
art O
CMN Method
on O
IEMOCAP Material
and O
AVEC Method
datasets Method
for O
textual Task
modality O
. O
subsubsection O
: O
IEMOCAP Material
As O
evidenced O
by O
tab O
: O
results O
- O
text O
, O
for O
IEMOCAP Material
dataset O
, O
our O
model O
surpasses O
the O
state O
- O
of O
- O
the O
- O
art O
method O
CMN Method
by O
accuracy Metric
and O
f1 Metric
- Metric
score Metric
on O
average O
. O
We O
think O
that O
this O
enhancement O
is O
caused O
by O
the O
fundamental O
differences O
between O
CMN Method
and O
DialogueRNN Method
, O
which O
are O
party Method
state Method
modeling Method
with O
in O
eq:2 Method
, O
speaker O
specific O
utterance O
treatment O
in O
eq:2 O
, O
eq:3 O
, O
and O
global Method
state Method
capturing Method
with O
in O
eq:3 O
. O
Since O
we O
deal O
with O
six O
unbalanced O
emotion O
labels O
, O
we O
also O
explored O
the O
model O
performance O
for O
individual O
labels O
. O
DialogueRNN Method
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
method O
CMN Method
in O
five O
out O
of O
six O
emotion O
classes O
by O
significant O
margin O
. O
For O
frustrated O
class O
, O
DialogueRNN Method
lags O
behind O
CMN Method
by O
f1 Metric
- Metric
score Metric
. O
We O
think O
that O
DialogueRNN Method
may O
surpass O
CMN Method
using O
a O
standalone Method
classifier Method
for O
frustrated O
class O
. O
However O
, O
it O
can O
be O
observed O
in O
tab O
: O
results O
- O
text O
that O
some O
of O
the O
other O
variants O
of O
DialogueRNN Method
, O
like O
BiDialogueRNN Method
has O
already O
outperformed O
CMN Method
for O
frustrated O
class O
. O
subsubsection O
: O
AVEC Material
DialogueRNN Method
outperforms O
CMN Method
for O
valence O
, O
arousal O
, O
expectancy O
, O
and O
power O
attributes O
; O
see O
tab O
: O
results O
- O
text O
. O
It O
yields O
significantly O
lower O
mean Metric
absolute Metric
error Metric
( O
) O
and O
higher O
Pearson Metric
correlation Metric
coefficient Metric
( O
) O
for O
all O
four O
attributes O
. O
We O
believe O
this O
to O
be O
due O
to O
the O
incorporation O
of O
party O
state O
and O
emotion O
GRU O
, O
which O
are O
missing O
from O
CMN Method
. O
subsection O
: O
DialogueRNN Method
vs. O
DialogueRNN Method
Variants Method
We O
discuss O
the O
performance O
of O
different O
DialogueRNN Method
variants O
on O
IEMOCAP Material
and O
AVEC Method
datasets Method
for O
textual Task
modality O
. O
subsubsection O
: O
DialogueRNN Method
: O
Following O
tab O
: O
results O
- O
text O
, O
using O
explicit Method
listener Method
state Method
update Method
yields O
slightly O
worse O
performance O
than O
regular O
DialogueRNN Method
. O
This O
is O
true O
for O
both O
IEMOCAP Material
and O
AVEC Method
datasets Method
in O
general O
. O
However O
, O
the O
only O
exception O
to O
this O
trend O
is O
for O
happy O
emotion O
label O
for O
IEMOCAP Material
, O
where O
DialogueRNN Method
outperforms O
DialogueRNN Method
by O
f1 Metric
- Metric
score Metric
. O
We O
surmise O
that O
, O
this O
is O
due O
to O
the O
fact O
that O
a O
listener O
becomes O
relevant O
to O
the O
conversation O
only O
when O
he O
/ O
she O
speaks O
. O
Now O
, O
in O
DialogueRNN Method
, O
when O
a O
party O
speaks O
, O
we O
update O
his O
/ O
her O
state O
with O
context O
which O
contains O
relevant O
information O
on O
all O
the O
preceding O
utterances O
, O
rendering O
explicit O
listener O
state O
update O
of O
DialogueRNN Method
unnecessary O
. O
subsubsection O
: O
BiDialogueRNN Method
: O
Since O
BiDialogueRNN Method
captures O
context O
from O
the O
future O
utterances O
, O
we O
expect O
improved O
performance O
from O
it O
over O
DialogueRNN Method
. O
This O
is O
confirmed O
in O
tab O
: O
results O
- O
text O
, O
where O
BiDialogueRNN Method
outperforms O
DialogueRNN Method
on O
average O
on O
both O
datasets O
. O
subsubsection O
: O
DialogueRNN Method
+ Method
Attn Method
: O
DialogueRNN Method
+ Method
Attn Method
also O
uses O
information O
from O
the O
future O
utterances O
. O
However O
, O
here O
we O
take O
information O
from O
both O
past O
and O
future O
utterances O
by O
matching O
them O
with O
the O
current O
utterance O
and O
calculating O
attention O
score O
over O
them O
. O
This O
provides O
relevance O
to O
emotionally O
important O
context O
utterances O
, O
yielding O
better O
performance O
than O
BiDialogueRNN Method
. O
The O
improvement O
over O
BiDialogueRNN Method
is O
f1 Metric
- Metric
score Metric
for O
IEMOCAP Material
and O
consistently O
lower O
and O
higher O
in O
AVEC Material
. O
subsubsection O
: O
BiDialogueRNN Method
+ Method
Attn Method
: O
Since O
this O
setting O
generates O
the O
final O
emotion Method
representation Method
by O
attending O
over O
the O
emotion Method
representation Method
from O
BiDialogueRNN Method
, O
we O
expect O
better O
performance O
than O
both O
BiDialogueRNN Method
and O
DialogueRNN Method
+ Method
Attn Method
. O
This O
is O
confirmed O
in O
tab O
: O
results O
- O
text O
, O
where O
this O
setting O
performs O
the O
best O
in O
general O
than O
any O
other O
methods O
discussed O
, O
on O
both O
datasets O
. O
This O
setting O
yields O
higher O
f1 Metric
- Metric
score Metric
on O
average O
than O
the O
state O
- O
of O
- O
the O
- O
art O
CMN Method
and O
higher O
f1 Metric
- Metric
score Metric
than O
vanilla O
DialogueRNN Method
for O
IEMOCAP Material
dataset O
. O
For O
AVEC Material
dataset Material
also O
, O
this O
setting O
gives O
the O
best O
performance O
across O
all O
the O
four O
attributes O
. O
subsection O
: O
Multimodal Task
Setting Task
As O
both O
IEMOCAP Material
and O
AVEC Material
dataset Material
contain O
multimodal O
information O
, O
we O
have O
evaluated O
DialogueRNN Method
on O
multimodal O
features O
as O
used O
and O
provided O
by O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
. O
We O
use O
concatenation O
of O
the O
unimodal O
features O
as O
a O
fusion Method
method Method
by O
following O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
hazarika O
- O
EtAl:2018:N18 O
- O
1 O
, O
since O
fusion Method
mechanism Method
is O
not O
a O
focus O
of O
this O
paper O
. O
Now O
, O
as O
we O
can O
see O
in O
tab O
: O
results O
- O
multimodal O
, O
DialogueRNN Method
significantly O
outperforms O
the O
strong O
baselines O
and O
state O
- O
of O
- O
the O
- O
art O
method O
CMN Method
. O
subsection O
: O
Case O
Studies O
subsubsection O
: O
Dependency O
on O
preceding O
utterances O
( O
DialogueRNN Method
) O
One O
of O
the O
crucial O
components O
of O
DialogueRNN Method
is O
its O
attention Method
module Method
over O
the O
outputs O
of O
global Method
GRU Method
( Method
) O
. O
Figure O
[ O
reference O
] O
shows O
the O
attention O
vector O
( O
eq:7 O
) O
over O
the O
history O
of O
a O
given O
test O
utterance O
compared O
with O
the O
attention O
vector O
from O
the O
CMN Method
model Method
. O
The O
attention O
of O
our O
model O
is O
more O
focused O
compared O
with O
CMN Method
: O
the O
latter O
gives O
diluted O
attention Metric
scores Metric
leading O
to O
misclassifications O
. O
We O
observe O
this O
trend O
of O
focused O
- O
attention O
across O
cases O
and O
posit O
that O
it O
can O
be O
interpreted O
as O
a O
confidence Metric
indicator Metric
. O
Further O
in O
this O
example O
, O
the O
test O
utterance O
by O
( O
turn O
44 O
) O
comprises O
of O
a O
change O
in O
emotion O
from O
neutral O
to O
frustrated O
. O
DialogueRNN Method
anticipates O
this O
correctly O
by O
attending O
to O
turn O
41 O
and O
42 O
that O
are O
spoken O
by O
and O
, O
respectively O
. O
These O
two O
utterances O
provide O
self O
and O
inter O
- O
party O
influences O
that O
trigger O
the O
emotional O
shift O
. O
CMN Method
, O
however O
, O
fails O
to O
capture O
such O
dependencies O
and O
wrongly O
predicts O
neutral O
emotion O
. O
subsubsection O
: O
Dependency O
on O
future O
utterances O
( O
BiDialogueRNN Method
+ O
Att O
) O
fig O
: O
case O
- O
study O
visualizes O
the O
( O
eq O
: O
beta O
) O
attention O
over O
the O
emotion Method
representations Method
for O
a O
segment O
of O
a O
conversation O
between O
a O
couple O
. O
In O
the O
discussion O
, O
the O
woman O
( O
) O
is O
initially O
at O
a O
neutral O
state O
, O
whereas O
the O
man O
( O
) O
is O
angry O
throughout O
. O
The O
figure O
reveals O
that O
the O
emotional O
attention O
of O
the O
woman O
is O
localized O
to O
the O
duration O
of O
her O
neutral O
state O
( O
turns O
1 O
- O
16 O
approximately O
) O
. O
For O
example O
, O
in O
the O
dialogue O
, O
turns O
, O
and O
strongly O
attend O
to O
turn O
. O
Interestingly O
, O
turn O
attends O
to O
both O
past O
( O
turn O
) O
and O
future O
( O
turn O
) O
utterances O
. O
Similar O
trend O
across O
other O
utterances O
establish O
inter O
- O
dependence O
between O
emotional O
states O
of O
future O
and O
past O
utterances O
. O
0.49 O
0.49 O
0.49 O
0.49 O
The O
beneficial O
consideration O
of O
future O
utterances O
through O
is O
also O
apparent O
through O
turns O
. O
These O
utterances O
focus O
on O
the O
distant O
future O
( O
turn O
) O
where O
the O
man O
is O
at O
an O
enraged O
state O
, O
thus O
capturing O
emotional O
correlations O
across O
time O
. O
Although O
, O
turn O
is O
misclassified O
by O
our O
model O
, O
it O
still O
manages O
to O
infer O
a O
related O
emotional O
state O
( O
anger O
) O
against O
the O
correct O
state O
( O
frustrated O
) O
. O
We O
analyze O
more O
of O
this O
trend O
in O
section O
[ O
reference O
] O
. O
subsubsection O
: O
Dependency O
on O
distant O
context O
For O
all O
correct O
predictions O
in O
the O
IEMOCAP Material
test O
set O
in O
fig O
: O
DeltaTTrend O
we O
summarize O
the O
distribution O
over O
the O
relative O
distance O
between O
test O
utterance O
and O
( O
) O
highest O
attended O
utterance O
– O
either O
in O
the O
history O
or O
future O
– O
in O
the O
conversation O
. O
This O
reveals O
a O
decreasing O
trend O
with O
the O
highest O
dependence O
being O
within O
the O
local O
context O
. O
However O
, O
a O
significant O
portion O
of O
the O
test O
utterances O
( O
) O
, O
attend O
to O
utterances O
that O
are O
to O
turns O
away O
from O
themselves O
, O
which O
highlights O
the O
important O
role O
of O
long O
- O
term O
emotional O
dependencies O
. O
Such O
cases O
primarily O
occur O
in O
conversations O
that O
maintain O
a O
specific O
affective O
tone O
and O
do O
not O
incur O
frequent O
emotional O
shifts O
. O
fig O
: O
DistantAttention O
demonstrates O
a O
case O
of O
long Task
- Task
term Task
context Task
dependency Task
. O
The O
presented O
conversation O
maintains O
a O
happy O
mood O
throughout O
the O
dialogue O
. O
Although O
the O
turn O
comprising O
the O
sentence O
Horrible O
thing O
. O
I O
hated O
it O
. O
seems O
to O
be O
a O
negative O
expression O
, O
when O
seen O
with O
the O
global O
context O
, O
it O
reveals O
the O
excitement O
present O
in O
the O
speaker O
. O
To O
disambiguate O
such O
cases O
, O
our O
model O
attends O
to O
distant O
utterances O
in O
the O
past O
( O
turn O
, O
) O
which O
serve O
as O
prototypes O
of O
the O
emotional O
tonality O
of O
the O
overall O
conversation O
. O
subsection O
: O
Error Method
Analysis Method
A O
noticeable O
trend O
in O
the O
predictions O
is O
the O
high O
level O
of O
cross O
- O
predictions O
amongst O
related O
emotions O
. O
Most O
of O
the O
misclassifications O
by O
the O
model O
for O
happy O
emotion O
are O
for O
excited O
class O
. O
Also O
, O
anger O
and O
frustrated O
share O
misclassifications O
amongst O
each O
other O
. O
We O
suspect O
this O
is O
due O
to O
subtle O
difference O
between O
those O
emotion O
pairs O
, O
resulting O
in O
harder O
disambiguation Task
. O
Another O
class O
with O
high O
rate O
of O
false Metric
- Metric
positives Metric
is O
the O
neutral O
class O
. O
Primary O
reason O
for O
this O
could O
be O
its O
majority O
in O
the O
class O
distribution O
over O
the O
considered O
emotions O
. O
At O
the O
dialogue O
level O
, O
we O
observe O
that O
a O
significant O
amount O
of O
errors O
occur O
at O
turns O
having O
a O
change O
of O
emotion O
from O
the O
previous O
turn O
of O
the O
same O
party O
. O
Across O
all O
the O
occurrences O
of O
these O
emotional O
- O
shifts O
in O
the O
testing O
set O
, O
our O
model O
correctly O
predicts O
instances O
. O
This O
stands O
less O
as O
compared O
to O
the O
success O
that O
it O
achieves O
at O
regions O
of O
no O
emotional O
- O
shift O
. O
Changes O
in O
emotions O
in O
a O
dialogue O
is O
a O
complex O
phenomenon O
governed O
by O
latent O
dynamics O
. O
Further O
improvement O
of O
these O
cases O
remain O
as O
an O
open O
area O
of O
research O
. O
subsection O
: O
Ablation Task
Study Task
The O
main O
novelty O
of O
our O
method O
is O
the O
introduction O
of O
party O
state O
and O
emotion Method
GRU Method
( Method
) O
. O
To O
comprehensively O
study O
the O
impact O
of O
these O
two O
components O
, O
we O
remove O
them O
one O
at O
a O
time O
and O
evaluate O
their O
impact O
on O
IEMOCAP Material
. O
As O
expected O
, O
following O
tab O
: O
ablation Task
, O
party O
state O
stands O
very O
important O
, O
as O
without O
its O
presence O
the O
performance O
falls O
by O
. O
We O
suspect O
that O
party O
state O
helps O
in O
extracting O
useful O
contextual O
information O
relevant O
to O
parties O
’ O
emotion O
. O
Emotion Method
GRU Method
is O
also O
impactful O
, O
but O
less O
than O
party O
state O
, O
as O
its O
absence O
causes O
performance O
to O
fall O
by O
only O
. O
We O
believe O
the O
reason O
to O
be O
the O
lack O
of O
context O
flow O
from O
the O
other O
parties O
’ O
states O
through O
the O
emotion Method
representation Method
of O
the O
preceding O
utterances O
. O
section O
: O
Conclusion O
We O
have O
presented O
an O
RNN Method
- O
based O
neural O
architecture O
for O
emotion Task
detection Task
in O
a O
conversation O
. O
In O
contrast O
to O
the O
state O
- O
of O
- O
the O
- O
art O
method O
, O
CMN Method
, O
our O
method O
treats O
each O
incoming O
utterance O
taking O
into O
account O
characteristics O
of O
the O
speaker O
, O
which O
gives O
finer O
context O
to O
the O
utterance O
. O
Our O
model O
outperforms O
the O
current O
state O
of O
the O
art O
on O
two O
distinct O
datasets O
in O
both O
textual Task
and O
multimodal Task
setting Task
. O
Our O
method O
is O
designed O
to O
be O
scalable O
to O
multi Task
- Task
party Task
setting Task
with O
more O
than O
two O
speakers O
, O
though O
we O
could O
not O
test O
it O
due O
to O
unavailability O
of O
a O
multi O
- O
party O
conversation O
dataset O
with O
emotion O
labels O
. O
This O
is O
left O
to O
our O
future O
work O
. O
bibliography O
: O
References O
section O
: O
Supplementary O
Material O
subsection O
: O
GRU Method
Details O
We O
use O
GRU O
cells O
are O
defined O
as O
, O
where O
: O
is O
the O
current O
input O
, O
is O
the O
previous O
GRU O
output O
, O
and O
is O
the O
current O
GRU O
output O
. O
and O
are O
GRU O
parameters O
. O
, O
are O
refresh O
gate O
and O
update O
gate O
respectively O
. O
is O
the O
candidate O
output O
. O
stands O
for O
hadamard O
product O
. O
[ O
b O
] O
DialogueRNN Method
algorithm O
[ O
1 O
] O
DialogueRNN Method
, O
= O
utterances O
in O
the O
conversation O
, O
S O
= O
speakers O
Initialize O
the O
participant O
states O
with O
null O
vector O
: O
i: O
[ O
1 O
, O
M O
] O
Set O
the O
initial O
global O
and O
emotional O
state O
as O
null O
vector O
: O
Pass O
the O
dialogue O
through O
RNN Method
: O
t: O
[ O
1 O
, O
N O
] O
return O
DialogueCell O
, O
, O
, O
, O
Update O
global O
state O
: O
Get O
context O
from O
preceding O
global O
states O
: O
Update O
participant O
states O
: O
i: O
[ O
1 O
, O
M O
] O
Update O
speaker O
state O
: O
Update O
listener O
state O
: O
Update O
emotion Method
representation Method
: O
return O
