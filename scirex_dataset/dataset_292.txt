document O
: O
DeXpression Method
: O
Deep Method
Convolutional Method
Neural Method
Network Method
for O
Expression Task
Recognition Task
We O
propose O
a O
convolutional Method
neural Method
network Method
( O
CNN Method
) O
architecture O
for O
facial Task
expression Task
recognition Task
. O
The O
proposed O
architecture O
is O
independent O
of O
any O
hand Method
- Method
crafted Method
feature Method
extraction Method
and O
performs O
better O
than O
the O
earlier O
proposed O
convolutional Method
neural Method
network Method
based O
approaches O
. O
We O
visualize O
the O
automatically O
extracted O
features O
which O
have O
been O
learned O
by O
the O
network O
in O
order O
to O
provide O
a O
better O
understanding O
. O
The O
standard O
datasets O
, O
i.e. O
Extended Method
Cohn Method
- Method
Kanade Method
( O
CKP Method
) O
and O
MMI Material
Facial Material
Expression Material
Databse Material
are O
used O
for O
the O
quantitative Task
evaluation Task
. O
On O
the O
CKP Method
set O
the O
current O
state O
of O
the O
art O
approach O
, O
using O
CNNs Method
, O
achieves O
an O
accuracy Metric
of O
99.2 O
% O
. O
For O
the O
MMI Material
dataset O
, O
currently O
the O
best O
accuracy Metric
for O
emotion Task
recognition Task
is O
93.33 O
% O
. O
The O
proposed O
architecture O
achieves O
% O
for O
CKP Method
and O
% O
for O
MMI Material
, O
therefore O
performing O
better O
than O
the O
state O
of O
the O
art O
using O
CNNs Method
. O
Automatic Task
facial Task
expression Task
recognition Task
has O
a O
broad O
spectrum O
of O
applications O
such O
as O
human Task
- Task
computer Task
interaction Task
and O
safety Task
systems Task
. O
This O
is O
due O
to O
the O
fact O
that O
non O
- O
verbal O
cues O
are O
important O
forms O
of O
communication O
and O
play O
a O
pivotal O
role O
in O
interpersonal Task
communication Task
. O
The O
performance O
of O
the O
proposed O
architecture O
endorses O
the O
efficacy O
and O
reliable O
usage O
of O
the O
proposed O
work O
for O
real Task
world Task
applications Task
. O
section O
: O
Introduction O
Humans O
use O
different O
forms O
of O
communications O
such O
as O
speech O
, O
hand O
gestures O
and O
emotions O
. O
Being O
able O
to O
understand O
one O
’s O
emotions O
and O
the O
encoded O
feelings O
is O
an O
important O
factor O
for O
an O
appropriate O
and O
correct O
understanding Task
. O
With O
the O
ongoing O
research O
in O
the O
field O
of O
robotics Task
, O
especially O
in O
the O
field O
of O
humanoid Task
robots Task
, O
it O
becomes O
interesting O
to O
integrate O
these O
capabilities O
into O
machines O
allowing O
for O
a O
more O
diverse O
and O
natural O
way O
of O
communication Task
. O
One O
example O
is O
the O
Software O
called O
EmotiChat Method
. O
This O
is O
a O
chat Task
application Task
with O
emotion Task
recognition Task
. O
The O
user O
is O
monitored O
and O
whenever O
an O
emotion O
is O
detected O
( O
smile O
, O
etc O
. O
) O
, O
an O
emoticon O
is O
inserted O
into O
the O
chat O
window O
. O
Besides O
Human Task
Computer Task
Interaction Task
other O
fields O
like O
surveillance Task
or O
driver Task
safety Task
could O
also O
profit O
from O
it O
. O
Being O
able O
to O
detect O
the O
mood O
of O
the O
driver O
could O
help O
to O
detect O
the O
level O
of O
attention O
, O
so O
that O
automatic Method
systems Method
can O
adapt O
. O
Many O
methods O
rely O
on O
extraction Task
of Task
the Task
facial Task
region Task
. O
This O
can O
be O
realized O
through O
manual Method
inference Method
or O
an O
automatic Method
detection Method
approach Method
. O
Methods O
often O
involve O
the O
Facial Method
Action Method
Coding Method
System Method
( O
FACS Method
) O
which O
describes O
the O
facial O
expression O
using O
Action O
Units O
( O
AU O
) O
. O
An O
Action O
Unit O
is O
a O
facial O
action O
like O
” O
raising O
the O
Inner O
Brow O
” O
. O
Multiple O
activations O
of O
AUs O
describe O
the O
facial O
expression O
. O
Being O
able O
to O
correctly O
detect O
AUs Task
is O
a O
helpful O
step O
, O
since O
it O
allows O
making O
a O
statement O
about O
the O
activation O
level O
of O
the O
corresponding O
emotion O
. O
Handcrafted O
facial O
landmarks O
can O
be O
used O
such O
as O
done O
by O
Kotsia O
et O
al O
. O
. O
Detecting O
such O
landmarks O
can O
be O
hard O
, O
as O
the O
distance O
between O
them O
differs O
depending O
on O
the O
person O
. O
Not O
only O
AUs Method
can O
be O
used O
to O
detect O
emotions O
, O
but O
also O
texture O
. O
When O
a O
face O
shows O
an O
emotion O
the O
structure O
changes O
and O
different O
filters O
can O
be O
applied O
to O
detect O
this O
. O
The O
presented O
approach O
uses O
Artificial Method
Neural Method
Networks Method
( O
ANN Method
) O
. O
ANNs Method
differ O
, O
as O
they O
are O
trained O
on O
the O
data O
with O
less O
need O
for O
manual O
interference O
. O
Convolutional Method
Neural Method
Networks Method
are O
a O
special O
kind O
of O
ANN Method
and O
have O
been O
shown O
to O
work O
well O
as O
feature Method
extractor Method
when O
using O
images O
as O
input O
and O
are O
real O
- O
time O
capable O
. O
This O
allows O
for O
the O
usage O
of O
the O
raw O
input O
images O
without O
any O
pre O
- O
or O
postprocessing O
. O
GoogleNet Method
is O
a O
deep Method
neural Method
network Method
architecture Method
that O
relies O
on O
CNNs Method
. O
It O
has O
been O
introduced O
during O
the O
Image Task
Net Task
Large Task
Scale Task
Visual Task
Recognition Task
Challenge Task
( Task
ILSVRC Task
) Task
2014 Task
. O
This O
challenge O
analyses O
the O
quality O
of O
different O
image Method
classification Method
approaches Method
submitted O
by O
different O
groups O
. O
The O
images O
are O
separated O
into O
1000 O
different O
classes O
organized O
by O
the O
WordNet O
hierarchy O
. O
In O
the O
challenge O
” O
object Task
detection Task
with O
additional O
training O
data O
” O
GoogleNet Method
has O
achieved O
about O
44 O
% O
precision Metric
. O
These O
results O
have O
demonstrated O
the O
potential O
which O
lies O
in O
this O
kind O
of O
architecture O
. O
Therefore O
it O
has O
been O
used O
as O
inspiration O
for O
the O
proposed O
architecture O
. O
The O
proposed O
network O
has O
been O
evaluated O
on O
the O
Extended Method
Cohn Method
- Method
Kanade Method
Dataset O
( O
Section O
[ O
reference O
] O
) O
and O
on O
the O
MMI Material
Dataset Material
( O
Section O
[ O
reference O
] O
) O
. O
Typical O
pictures O
of O
persons O
showing O
emotions O
can O
be O
seen O
in O
Fig O
. O
[ O
reference O
] O
. O
The O
emotion O
Contempt O
of O
the O
CKP Method
set O
is O
not O
shown O
as O
no O
subject O
with O
consent O
for O
publication O
and O
an O
annotated O
emotion O
is O
part O
of O
the O
dataset O
. O
Results O
of O
experiments O
on O
these O
datasets O
demonstrate O
the O
success O
of O
using O
a O
deep Method
layered Method
neural Method
network Method
structure Method
. O
With O
a O
10 Metric
- Metric
fold Metric
cross Metric
- Metric
validation Metric
a O
recognition O
accuracy Metric
of O
99.6 O
% O
has O
been O
achieved O
. O
The O
paper O
is O
arranged O
as O
follows O
: O
After O
this O
introduction O
, O
Related O
Work O
( O
Section O
[ O
reference O
] O
) O
is O
presented O
which O
focuses O
on O
Emotion Task
/ Task
Expression Task
recognition Task
and O
the O
various O
approaches O
scientists O
have O
taken O
. O
Next O
is O
Section O
[ O
reference O
] O
, O
Background O
, O
which O
focuses O
on O
the O
main O
components O
of O
the O
architecture O
proposed O
in O
this O
article O
. O
Section O
[ O
reference O
] O
contains O
a O
summary O
of O
the O
used O
Datasets O
. O
In O
Section O
[ O
reference O
] O
the O
architecture O
is O
presented O
. O
This O
is O
followed O
by O
the O
experiments O
and O
its O
results O
( O
Section O
[ O
reference O
] O
) O
. O
Finally O
, O
Section O
[ O
reference O
] O
summarizes O
the O
article O
and O
concludes O
the O
article O
. O
section O
: O
Related O
Work O
A O
detailed O
overview O
for O
expression Task
recognition Task
was O
given O
by O
Căleanu O
and O
Bettadapura O
. O
In O
this O
Section O
mainly O
work O
which O
similar O
to O
the O
proposed O
method O
is O
presented O
as O
well O
as O
few O
selected O
articles O
which O
give O
a O
broad O
overview O
over O
the O
different O
methodologies O
. O
Recently O
Szegedy O
et O
al O
. O
have O
proposed O
an O
architecture O
called O
GoogLeNet Method
. O
This O
is O
a O
27 Method
layer Method
deep Method
network Method
, O
mostly O
composed O
of O
CNNs Method
. O
The O
network O
is O
trained O
using O
stochastic Method
gradient Method
descent Method
. O
In O
the O
ILSVRC Task
2014 Task
Classification Task
Challenge Task
this O
network O
achieved O
a O
top Metric
- Metric
5 Metric
error Metric
rate Metric
of O
6.67 O
% O
winning O
the O
first O
place O
. O
Using O
the O
the O
Extended Method
Cohn Method
- Method
Kanade Method
Dataset O
( O
Section O
[ O
reference O
] O
) O
, O
Happy O
and O
Routray O
classify O
between O
six O
basic O
emotions O
. O
Given O
an O
input O
image O
, O
their O
solution O
localizes O
the O
face O
region O
. O
From O
this O
region O
, O
facial O
patches O
e.g. O
the O
eyes O
or O
lips O
are O
detected O
and O
points O
of O
interest O
are O
marked O
. O
From O
the O
patches O
which O
have O
the O
most O
variance O
between O
two O
images O
, O
features O
are O
extracted O
. O
The O
dimensionality O
of O
the O
features O
is O
reduced O
and O
then O
given O
to O
a O
Support Method
Vector Method
Machine Method
( O
SVM Method
) O
. O
To O
evaluate O
the O
method O
, O
a O
10 Method
- Method
fold Method
cross Method
- Method
validation Method
is O
applied O
. O
The O
average O
accuracy Metric
is O
94.09 O
% O
. O
Video Task
based Task
emotion Task
recognition Task
has O
been O
proposed O
by O
Byeon O
and O
Kwak O
. O
They O
have O
developed O
a O
three O
dimensional O
CNN Method
which O
uses O
groups O
of O
5 O
consecutive O
frames O
as O
input O
. O
A O
database O
containing O
10 O
persons O
has O
been O
used O
to O
achieve O
an O
accuracy Metric
of O
95 O
% O
. O
Song O
et O
al O
. O
have O
used O
a O
deep O
convolutional Method
neural Method
network Method
for O
learning Task
facial Task
expressions Task
. O
The O
created O
network O
consists O
of O
five O
layers O
with O
a O
total O
of O
65k O
neurons O
. O
Convolutional Method
, O
pooling Method
, O
local Method
filter Method
layers Method
and O
one O
fully Method
connected Method
layer Method
are O
used O
to O
achieve O
an O
accuracy Metric
of O
99.2 O
% O
on O
the O
CKP Method
set O
. O
To O
avoid O
overfitting O
the O
dropout Method
method Method
was O
used O
. O
Luecy O
et O
al O
. O
have O
created O
the O
Extended Method
Cohn Method
- Method
Kanade Method
dataset O
. O
This O
dataset O
contains O
emotion O
annotations O
as O
well O
as O
Action O
Unit O
annotations O
. O
In O
regards O
to O
classification Task
, O
they O
also O
have O
evaluated O
the O
datasets O
using O
Active Method
Appearance Method
Models Method
( O
AAMs Method
) O
in O
combination O
with O
SVMs Method
. O
To O
find O
the O
position O
and O
track O
the O
face O
over O
different O
images O
, O
they O
have O
employed O
AAM Method
which O
generates O
a O
Mesh O
out O
of O
the O
face O
. O
From O
this O
mesh O
they O
have O
extracted O
two O
feature O
vectors O
. O
First O
, O
the O
normalized O
vertices O
with O
respect O
to O
rotation O
, O
translation O
, O
and O
scale O
. O
Second O
a O
gray O
- O
scale O
image O
from O
the O
mesh O
data O
, O
and O
the O
input O
images O
has O
been O
extracted O
. O
They O
have O
chosen O
a O
cross Method
- Method
validation Method
strategy Method
, O
where O
one O
subject O
is O
left O
out O
in O
the O
training O
process O
, O
achieving O
an O
accuracy Metric
of O
over O
80 O
% O
. O
Anderson O
et O
al O
. O
have O
developed O
a O
face Method
expression Method
system Method
, O
which O
is O
capable O
of O
recognizing O
the O
six O
basic O
emotions O
. O
Their O
system O
is O
built O
upon O
three O
components O
. O
The O
first O
one O
is O
a O
face Method
tracker Method
( O
derivative Method
of Method
ratio Method
template Method
) O
to O
detect O
the O
location O
of O
the O
face O
. O
The O
second O
component O
is O
an O
optical Method
flow Method
algorithm Method
to O
track O
the O
motion O
within O
the O
face O
. O
The O
last O
component O
is O
the O
recognition Method
engine Method
itself O
. O
It O
is O
based O
upon O
Support Method
Vector Method
Machines Method
and O
Multilayer Method
Perceptrons Method
. O
This O
approach O
has O
been O
implemented O
in O
EmotiChat Method
. O
They O
achieve O
a O
recognition O
accuracy Metric
of O
81.82 O
% O
. O
Kotsia O
and O
Pitas O
detect O
emotions O
by O
mapping O
a O
Candide O
grid O
, O
a O
face O
mask O
with O
a O
low O
number O
of O
polygons O
, O
onto O
a O
person O
’s O
face O
. O
The O
grid O
is O
initially O
placed O
randomly O
on O
the O
image O
, O
then O
it O
has O
to O
be O
manually O
placed O
on O
the O
persons O
face O
. O
Throughout O
the O
emotion O
, O
the O
grid O
is O
tracked O
using O
a O
KanadeâLucasâTomasi Method
tracker Method
. O
The O
geometric O
displacement O
information O
provided O
by O
the O
grid O
is O
used O
as O
feature O
vector O
for O
multiclass Method
SVMs Method
. O
The O
emotions O
are O
anger O
, O
disgust O
, O
fear O
, O
happiness O
, O
sadness O
, O
and O
surprise O
. O
They O
evaluate O
the O
model O
on O
the O
Cohn Material
- Material
Kanade Material
dataset Material
and O
an O
accuracy Metric
of O
99.7 O
% O
has O
been O
achieved O
. O
Shan O
et O
al O
. O
have O
created O
an O
emotion Method
recognition Method
system Method
based O
on O
Local Method
Binary Method
Patterns Method
( O
LBP Method
) O
. O
The O
LBPs Method
are O
calculated O
over O
the O
facial O
region O
. O
From O
the O
extracted O
LBPs O
a O
feature O
vector O
is O
derived O
. O
The O
features O
depend O
on O
the O
position O
and O
size O
of O
the O
sub O
- O
regions O
over O
witch O
the O
LBP Method
is O
calculated O
. O
AdaBoost Method
is O
used O
to O
find O
the O
sub O
- O
regions O
of O
the O
images O
which O
contain O
the O
most O
discriminative O
information O
. O
Different O
classification Method
algorithms Method
have O
been O
evaluated O
of O
which O
an O
SVM Method
with Method
Boosted Method
- Method
LBP Method
features Method
performs O
the O
best O
with O
a O
recognition O
accuracy Metric
of O
95.1 O
% O
on O
the O
CKP Method
set O
. O
In O
2013 O
Zafar O
et O
al O
. O
proposed O
an O
emotion Method
recognition Method
system Method
using O
Robust O
Normalized Method
Cross Method
Correlation Method
( O
NCC Method
) O
. O
The O
used O
NCC Method
is O
the O
” O
Correlation Metric
as O
a O
Rescaled O
Variance O
of O
the O
Difference O
between O
Standardized O
Scores O
” O
. O
Outlier O
pixels O
which O
influence O
the O
template Method
matching Method
too O
strong O
or O
too O
weak O
are O
excluded O
and O
not O
considered O
. O
This O
approach O
has O
been O
evaluated O
on O
different O
databases O
including O
AR Material
FaceDB Material
( O
85 O
% O
Recognition Metric
Accuracy Metric
) O
and O
the O
Extended Material
Cohn Material
Kanade Material
Database Material
( O
100 O
% O
Recognition Metric
Accuracy Metric
) O
. O
section O
: O
Convolutional Method
Neural Method
Networks Method
paragraph O
: O
Convolutional Method
Layer Method
Convolutional Method
Layers Method
perform O
a O
convolution Method
over O
the O
input O
. O
Let O
be O
the O
filter Method
with O
a O
kernel O
size O
applied O
to O
the O
input O
. O
is O
the O
number O
of O
input O
connections O
each O
CNN Method
neuron O
has O
. O
The O
resulting O
output O
of O
the O
layer O
calculates O
as O
follows O
: O
To O
calculate O
a O
more O
rich O
and O
diverse O
representation O
of O
the O
input O
, O
multiple O
filters Method
with O
can O
be O
applied O
on O
the O
input O
. O
The O
filters O
are O
realized O
by O
sharing O
weights O
of O
neighboring O
neurons O
. O
This O
has O
the O
positive O
effect O
that O
lesser O
weights O
have O
to O
be O
trained O
in O
contrast O
to O
standard O
Multilayer Method
Perceptrons Method
, O
since O
multiple O
weights O
are O
bound O
together O
. O
paragraph O
: O
Max Method
Pooling Method
Max Method
Pooling Method
reduces O
the O
input O
by O
applying O
the O
maximum O
function O
over O
the O
input O
. O
Let O
be O
the O
size O
of O
the O
filter O
, O
then O
the O
output O
calculates O
as O
follows O
: O
This O
layer O
features O
translational O
invariance O
with O
respect O
to O
the O
filter O
size O
. O
paragraph O
: O
Rectified Method
Linear Method
Unit Method
A O
Rectified Method
Linear Method
Unit Method
( O
ReLU Method
) O
is O
a O
cell O
of O
a O
neural Method
network Method
which O
uses O
the O
following O
activation O
function O
to O
calculate O
its O
output O
given O
: O
Using O
these O
cells O
is O
more O
efficient O
than O
sigmoid Method
and O
still O
forwards O
more O
information O
compared O
to O
binary Method
units Method
. O
When O
initializing O
the O
weights O
uniformly O
, O
half O
of O
the O
weights O
are O
negative O
. O
This O
helps O
creating O
a O
sparse Method
feature Method
representation Method
. O
Another O
positive O
aspect O
is O
the O
relatively O
cheap O
computation O
. O
No O
exponential Method
function Method
has O
to O
be O
calculated O
. O
This O
function O
also O
prevents O
the O
vanishing O
gradient O
error O
, O
since O
the O
gradients O
are O
linear O
functions O
or O
zero O
but O
in O
no O
case O
non O
- O
linear O
functions O
. O
paragraph O
: O
Fully Method
Connected Method
Layer Method
The O
fully Method
connected Method
layer Method
also O
known O
as O
Multilayer Method
Perceptron Method
connects O
all O
neurons O
of O
the O
prior O
layer O
to O
every O
neuron O
of O
its O
own O
layer O
. O
Let O
the O
input O
be O
with O
size O
and O
be O
the O
number O
of O
neurons O
in O
the O
fully Method
connected Method
layer Method
. O
This O
results O
in O
a O
Matrix O
. O
is O
the O
so O
called O
activation O
function O
. O
In O
our O
network O
is O
the O
identity O
function O
. O
paragraph O
: O
Output O
Layer O
The O
output O
layer O
is O
a O
one Method
hot Method
vector Method
representing O
the O
class O
of O
the O
given O
input O
image O
. O
It O
therefore O
has O
the O
dimensionality O
of O
the O
number O
of O
classes O
. O
The O
resulting O
class O
for O
the O
output O
vector O
is O
: O
paragraph O
: O
Softmax Method
Layer Method
The O
error O
is O
propagated O
back O
over O
a O
Softmax Method
layer Method
. O
Let O
N O
be O
the O
dimension O
of O
the O
input O
vector O
, O
then O
Softmax Method
calculates O
a O
mapping Method
such O
that O
: O
For O
each O
component O
, O
the O
output O
is O
calculated O
as O
follows O
: O
section O
: O
Datasets O
subsection O
: O
MMI Material
Dataset Material
The O
MMI Material
dataset O
has O
been O
introduced O
by O
Pantic O
et O
al O
. O
contains O
over O
2900 O
videos O
and O
images O
of O
75 O
persons O
. O
The O
annotations O
contain O
action O
units O
and O
emotions O
. O
The O
database O
contains O
a O
web O
- O
interface O
with O
an O
integrated O
search O
to O
scan O
the O
database O
. O
The O
videos O
/ O
images O
are O
colored O
. O
The O
people O
are O
of O
mixed O
age O
, O
different O
gender O
and O
have O
different O
ethnical O
background O
. O
The O
emotions O
investigated O
are O
the O
six O
basic O
emotions O
: O
Anger O
, O
Disgust O
, O
Fear O
, O
Happiness O
, O
Sadness O
, O
Surprise O
. O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
Anger O
[ O
b O
] O
0.15 O
Disgust O
[ O
b O
] O
0.15 O
Fear O
[ O
b O
] O
0.15 O
Happy O
[ O
b O
] O
0.15 O
Sadness Task
[ O
b O
] O
0.15 O
Surprise O
subsection O
: O
CKP Method
Dataset O
This O
dataset O
has O
been O
introduced O
by O
Lucey O
et O
al O
. O
. O
210 O
persons O
, O
aged O
18 O
to O
50 O
, O
have O
been O
recorded O
depicting O
emotions O
. O
This O
dataset O
presented O
by O
contains O
recordings O
of O
emotions O
of O
210 O
persons O
at O
the O
ages O
of O
18 O
to O
50 O
years O
. O
Both O
female O
and O
male O
persons O
are O
present O
and O
from O
different O
background O
. O
81 O
% O
are O
Euro O
- O
Americans O
and O
13 O
% O
are O
Afro O
- O
Americans O
. O
The O
images O
are O
of O
size O
640 O
490 O
px O
as O
well O
640 O
480 O
px O
. O
They O
are O
both O
grayscale O
and O
colored O
. O
In O
total O
this O
set O
has O
593 O
emotion Material
- Material
labeled Material
sequences Material
. O
The O
emotions O
consist O
of O
Anger O
, O
Disgust O
, O
Fear O
, O
Happiness O
, O
Sadness O
, O
Surprise O
, O
and O
Contempt O
. O
subsection O
: O
Comparison O
In O
the O
MMI Material
Dataset Material
( O
Fig O
. O
[ O
reference O
] O
) O
the O
emotion O
Anger O
is O
displayed O
in O
different O
ways O
, O
as O
can O
be O
seen O
by O
the O
eyebrows O
, O
forehead O
and O
mouth O
. O
The O
mouth O
in O
the O
lower O
image O
is O
tightly O
closed O
while O
in O
the O
upper O
image O
the O
mouth O
is O
open O
. O
For O
Disgust O
the O
differences O
are O
also O
visible O
, O
as O
the O
woman O
in O
the O
upper O
picture O
has O
a O
much O
stronger O
reaction O
. O
The O
man O
depicting O
Fear O
has O
contracted O
eyebrows O
which O
slightly O
cover O
the O
eyes O
. O
On O
the O
other O
hand O
the O
eyes O
of O
the O
woman O
are O
wide O
open O
. O
As O
for O
Happy O
both O
persons O
are O
smiling O
strongly O
. O
In O
the O
lower O
image O
the O
woman O
depicting O
Sadness O
has O
a O
stronger O
lip O
and O
chin O
reaction O
. O
The O
last O
emotion O
Surprise O
also O
has O
differences O
like O
the O
openness O
of O
the O
mouth O
. O
Such O
differences O
also O
appear O
in O
the O
CKP Method
set O
( O
Fig O
. O
[ O
reference O
] O
) O
. O
For O
Anger O
the O
eyebrows O
and O
cheeks O
differ O
. O
For O
Disgust O
larger O
differences O
can O
be O
seen O
. O
In O
the O
upper O
picture O
not O
only O
the O
curvature O
of O
the O
mouth O
is O
stronger O
, O
but O
the O
nose O
is O
also O
more O
involved O
. O
While O
both O
women O
displaying O
Fear O
show O
the O
same O
reaction O
around O
the O
eyes O
the O
mouth O
differs O
. O
In O
the O
lower O
image O
the O
mouth O
is O
nearly O
closed O
while O
teeth O
are O
visible O
in O
the O
upper O
one O
. O
Happiness O
is O
displayed O
similar O
. O
For O
the O
emotion O
Sadness O
the O
curvature O
of O
the O
mouth O
is O
visible O
in O
both O
images O
, O
but O
it O
is O
stronger O
in O
the O
upper O
one O
. O
The O
regions O
around O
the O
eyes O
differ O
as O
the O
eyebrows O
of O
the O
woman O
are O
straight O
. O
The O
last O
emotion O
Surprise O
has O
strong O
similarities O
like O
the O
open O
mouth O
an O
wide O
open O
eyes O
. O
Teeth O
are O
only O
displayed O
by O
the O
woman O
in O
the O
upper O
image O
. O
Thus O
for O
a O
better O
evaluation O
it O
is O
helpful O
to O
investigate O
multiple O
datasets O
. O
This O
aims O
at O
investigating O
whether O
the O
proposed O
approach O
works O
on O
different O
ways O
emotions O
are O
shown O
and O
whether O
it O
works O
on O
different O
emotions O
. O
For O
example O
Contempt O
which O
is O
only O
included O
in O
the O
CKP Method
set O
. O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
[ O
b O
] O
0.15 O
Anger O
[ O
b O
] O
0.15 O
Disgust O
[ O
b O
] O
0.15 O
Fear O
[ O
b O
] O
0.15 O
Happy O
[ O
b O
] O
0.15 O
Sadness Task
[ O
b O
] O
0.15 O
Surprise O
section O
: O
Proposed O
Architecture O
The O
proposed O
deep Method
Convolutional Method
Neural Method
Network Method
architecture Method
( O
depicted O
in O
Figure O
[ O
reference O
] O
) O
consists O
of O
four O
parts O
. O
The O
first O
part O
automatically O
preprocesses O
the O
data O
. O
This O
begins O
with O
Convolution Method
1 Method
, O
which O
applies O
different O
filters Method
. O
The O
next O
layer O
is O
Pooling O
1 O
, O
which O
down O
- O
samples O
the O
images O
and O
then O
they O
are O
normalized O
by O
LRN Method
1 O
. O
The O
next O
steps O
are O
the O
two O
FeatEx Method
( Method
Parallel Method
Feature Method
Extraction Method
Block Method
) Method
blocks Method
, O
highlighted O
in O
Figure O
[ O
reference O
] O
. O
They O
are O
the O
core O
of O
the O
proposed O
architecture O
and O
described O
later O
in O
this O
section O
. O
The O
features O
extracted O
by O
theses O
blocks O
are O
forwarded O
to O
a O
fully Method
connected Method
layer Method
, O
which O
uses O
them O
to O
classify O
the O
input O
into O
the O
different O
emotions O
. O
The O
described O
architecture O
is O
compact O
, O
which O
makes O
it O
not O
only O
fast O
to O
train O
, O
but O
also O
suitable O
for O
real Task
- Task
time Task
applications Task
. O
This O
is O
also O
important O
as O
the O
network O
was O
built O
with O
resource O
usage O
in O
mind O
. O
paragraph O
: O
FeatEx O
The O
key O
structure O
in O
our O
architecture O
is O
the O
Parallel Method
Feature Method
Extraction Method
Block Method
( O
FeatEx Method
) O
. O
It O
is O
inspired O
by O
the O
success O
of O
GoogleNet Method
. O
The O
block O
consists O
of O
Convolutional Method
, Method
Pooling Method
, O
and O
ReLU Method
Layers O
. O
The O
first O
Convolutional Method
layer Method
in O
FeatEx Method
reduces O
the O
dimension O
since O
it O
convolves O
with O
a O
filter Method
of Method
size Method
. O
It O
is O
enhanced O
by O
a O
ReLU Method
layer O
, O
which O
creates O
the O
desired O
sparseness O
. O
The O
output O
is O
then O
convolved O
with O
a O
filter Method
of Method
size Method
. O
In O
the O
parallel O
path O
a O
Max Method
Pooling Method
layer Method
is O
used O
to O
reduce O
information O
before O
applying O
a O
CNN Method
of O
size O
. O
This O
application O
of O
differently O
sized O
filters O
reflects O
the O
various O
scales O
at O
which O
faces O
can O
appear O
. O
The O
paths O
are O
concatenated O
for O
a O
more O
diverse O
representation O
of O
the O
input O
. O
Using O
this O
block O
twice O
yields O
good O
results O
. O
paragraph O
: O
Visualization Task
The O
different O
layers O
of O
the O
architecture O
produce O
feature O
vectors O
as O
can O
be O
seen O
in O
Fig O
[ O
reference O
] O
. O
The O
first O
part O
until O
LRN Method
1 O
preprocesses O
the O
data O
and O
creates O
multiple O
modified O
instances O
of O
the O
input O
. O
These O
show O
mostly O
edges O
with O
a O
low O
level O
of O
abstraction O
. O
The O
first O
FeatEx Method
block Method
creates O
two O
parallel O
paths O
of O
features O
with O
different O
scales O
, O
which O
are O
combined O
in O
Concat O
2 O
. O
The O
second O
FeatEx Method
block Method
refines O
the O
representation O
of O
the O
features O
. O
It O
also O
decreases O
the O
dimensionality O
. O
This O
visualization O
shows O
that O
the O
concatenation Method
of Method
FeatEx Method
blocks Method
is O
a O
valid O
approach O
to O
create O
an O
abstract Method
feature Method
representation Method
. O
The O
output O
dimensionality O
of O
each O
layer O
can O
be O
seen O
in O
Table O
[ O
reference O
] O
. O
section O
: O
Experiments O
and O
Results O
As O
implementation O
Caffe Method
was O
used O
. O
This O
is O
a O
deep Method
learning Method
framework Method
, O
maintained O
by O
the O
Berkeley Method
Vision Method
and Method
Learning Method
Center Method
( O
BVLC Method
) O
. O
paragraph O
: O
CKP Method
The O
CKP Method
database O
has O
been O
analyzed O
often O
and O
many O
different O
approaches O
have O
been O
evaluated O
in O
order O
to O
” O
solve O
” O
this O
set O
. O
To O
determine O
whether O
the O
architecture O
is O
competitive O
, O
it O
has O
been O
evaluated O
on O
the O
CKP Method
dataset O
. O
For O
the O
experiments O
all O
5870 O
annotated O
images O
have O
been O
used O
to O
do O
a O
10 Metric
- Metric
fold Metric
cross Metric
- Metric
validation Metric
. O
The O
proposed O
architecture O
has O
proven O
to O
be O
very O
effective O
on O
this O
dataset O
with O
an O
average O
accuracy Metric
of O
99.6 O
% O
. O
In O
Table O
[ O
reference O
] O
different O
results O
from O
state O
of O
the O
art O
approaches O
are O
listed O
as O
comparison O
. O
The O
100 O
% O
accuracy Metric
reported O
by O
Zafar O
is O
based O
on O
hand O
picked O
images O
. O
The O
results O
are O
not O
validated O
using O
cross Method
- Method
validation Method
. O
The O
confusion O
matrix O
in O
Fig O
. O
[ O
reference O
] O
depicts O
the O
results O
and O
shows O
that O
some O
emotions O
are O
perfectly O
recognized O
. O
[ O
b O
] O
0.46 O
[ O
b O
] O
0.46 O
paragraph O
: O
MMI Material
The O
MMI Material
Database O
contains O
videos O
of O
people O
showing O
emotions O
. O
From O
each O
video O
the O
20 O
frames O
, O
which O
represent O
the O
content O
of O
the O
video O
the O
most O
, O
have O
been O
extracted O
fully O
automatically O
. O
The O
first O
two O
of O
these O
frames O
have O
been O
discarded O
since O
they O
provide O
neutral O
expressions O
. O
To O
determine O
the O
frames O
, O
the O
difference O
between O
grayscale O
consecutive O
frames O
was O
calculated O
. O
To O
compensate O
noise O
the O
images O
have O
been O
smoothed O
using O
a O
Gaussian Method
filter Method
before O
calculation O
. O
To O
find O
the O
20 O
most O
representative O
images O
, O
changes O
which O
occur O
in O
a O
small O
timeframe O
, O
should O
only O
be O
represented O
by O
a O
single O
image O
. O
This O
was O
achieved O
by O
iterating O
over O
the O
differences O
using O
a O
maximum Method
filter Method
with O
decreasing O
filter O
size O
until O
20 O
frames O
have O
been O
found O
. O
In O
total O
3740 O
images O
have O
been O
extracted O
. O
The O
original O
images O
were O
then O
used O
for O
training O
and O
testing Task
. O
A O
10 Method
- Method
fold Method
cross Method
- Method
validation Method
has O
been O
applied O
. O
The O
average O
accuracy Metric
is O
98.63 O
% O
. O
This O
is O
better O
than O
the O
accuracies Metric
achieved O
by O
Wang O
and O
Yin O
( O
Table O
[ O
reference O
] O
) O
. O
To O
our O
knowledge O
they O
have O
been O
the O
only O
ones O
to O
evaluate O
the O
MMI Material
database O
on O
Emotions O
instead O
of O
Action O
Units O
. O
The O
results O
of O
the O
proposed O
approach O
are O
depicted O
in O
the O
Confusion Metric
Matrix Metric
in O
Fig O
. O
[ O
reference O
] O
. O
In O
the O
figure O
it O
is O
shown O
that O
the O
accuracy Metric
for O
Fear Task
is O
the O
lowest O
with O
93.75 O
% O
while O
Happiness Metric
is O
almost O
perfectly O
recognized O
with O
98.21 O
% O
. O
Fear O
and O
Surprise O
are O
the O
emotions O
confused O
the O
most O
. O
section O
: O
Discussion O
The O
accuracy Metric
on O
the O
CKP Method
set O
shows O
that O
the O
chosen O
approach O
is O
robust O
, O
misclassification Task
usually O
occurs O
on O
pictures O
which O
are O
the O
first O
few O
instances O
of O
an O
emotion O
sequence O
. O
Often O
a O
neutral O
facial O
expression O
is O
depicted O
in O
those O
frames O
. O
Thus O
those O
misclassifications O
are O
not O
necessarily O
an O
error O
in O
the O
approach O
, O
but O
in O
the O
data Task
selection Task
. O
Other O
than O
that O
no O
major O
problem O
could O
be O
detected O
. O
The O
emotion O
Surprise O
is O
often O
confused O
with O
Disgust O
with O
a O
rate O
of O
0.045 O
% O
which O
is O
the O
highest O
. O
Of O
those O
images O
, O
where O
an O
emotion O
is O
present O
, O
only O
few O
are O
wrongly O
classified O
. O
As O
there O
is O
no O
consent O
for O
the O
misclassified O
images O
, O
they O
can O
not O
be O
depicted O
here O
. O
However O
some O
unique O
names O
are O
provided O
. O
Image O
S119_001_00000010 O
is O
classified O
as O
Fear O
while O
the O
annotated O
emotion O
corresponds O
to O
Surprise O
. O
The O
image O
depicts O
a O
person O
with O
a O
wide O
open O
mouth O
and O
open O
eyes O
. O
Pictures O
representing O
Surprise O
are O
often O
very O
similar O
, O
since O
the O
persons O
also O
have O
wide O
open O
mouths O
and O
eyes O
. O
In O
image O
S032_004_00000014 O
the O
targeted O
label O
Fear O
is O
confused O
with O
Anger O
. O
While O
the O
mouth O
region O
in O
pictures O
with O
Anger O
differ O
, O
the O
eye O
regions O
are O
alike O
, O
since O
in O
both O
situations O
the O
eyes O
and O
eyebrows O
are O
contracted O
. O
Similar O
effects O
are O
experienced O
when O
dealing O
with O
the O
MMI Material
Dataset Material
. O
Since O
the O
first O
two O
frames O
are O
discarded O
most O
pictures O
with O
neutral O
positions O
are O
excluded O
. O
In O
few O
images O
a O
neutral O
position O
can O
still O
be O
found O
which O
gives O
rise O
to O
errors O
. O
For O
the O
same O
reason O
as O
the O
CKP Method
set O
images O
will O
not O
be O
displayed O
. O
Due O
to O
the O
approach O
to O
extract O
images O
of O
the O
videos O
, O
a O
unique O
identifier O
for O
the O
misclassified O
image O
can O
not O
be O
provided O
. O
The O
top O
confusions O
are O
observed O
for O
Fear O
and O
Surprise Task
with O
a O
rate O
of O
0.0159 O
% O
where O
Fear O
is O
wrongly O
misclassified O
as O
Surprise O
. O
Session O
1937 O
shows O
a O
woman O
displaying O
Fear O
but O
it O
is O
classified O
as O
Surprise O
. O
Both O
share O
common O
features O
like O
similar O
eye O
and O
mouth O
movement O
. O
In O
both O
emotions O
, O
participants O
move O
the O
head O
slightly O
backwards O
. O
This O
can O
be O
identified O
by O
wrinkled O
skin O
. O
The O
second O
most O
confusion Metric
rate Metric
, O
Surprise O
being O
mistaken O
as O
Sadness O
, O
is O
mostly O
based O
on O
neutral O
position O
images O
. O
Although O
the O
first O
two O
images O
are O
not O
used O
, O
some O
selected O
frames O
still O
do O
not O
contain O
an O
emotion O
. O
In O
Session O
1985 O
Surprise O
is O
being O
mistaken O
as O
Sadness O
. O
The O
image O
depicts O
a O
man O
with O
his O
mouth O
being O
slightly O
curved O
, O
making O
him O
look O
sad O
. O
DeXpression Method
extracts O
features O
and O
uses O
them O
to O
classify O
images O
, O
but O
in O
very O
few O
cases O
the O
emotions O
are O
confused O
. O
This O
happens O
, O
as O
discussed O
, O
usually O
in O
pictures O
depicting O
no O
emotion O
. O
DeXpression Method
performs O
very O
well O
on O
both O
tested O
sets O
, O
if O
an O
emotion O
is O
present O
. O
section O
: O
Conclusion O
and O
Future O
Work O
In O
this O
article O
DeXpression Method
is O
presented O
which O
works O
fully O
automatically O
. O
It O
is O
a O
neural Method
network Method
which O
has O
little O
computational Metric
effort Metric
compared O
to O
current O
state O
of O
the O
art O
CNN Method
architectures O
. O
In O
order O
to O
create O
it O
the O
new O
composed Method
structure Method
FeatEx Method
has O
been O
introduced O
. O
It O
consists O
of O
several O
Convolutional Method
layers Method
of O
different O
sizes O
, O
as O
well O
as O
Max O
Pooling O
and O
ReLU Method
layers O
. O
FeatEx Method
creates O
a O
rich O
feature Method
representation Method
of O
the O
input O
. O
The O
results O
of O
the O
10 Metric
- Metric
fold Metric
cross Metric
- Metric
validation Metric
yield O
, O
in O
average O
, O
a O
recognition O
accuracy Metric
of O
99.6 O
% O
on O
the O
CKP Method
dataset O
and O
98.36 O
% O
on O
the O
MMI Material
dataset O
. O
This O
shows O
that O
the O
proposed O
architecture O
is O
capable O
of O
competing O
with O
current O
state O
of O
the O
art O
approaches O
in O
the O
field O
of O
emotion Task
recognition Task
. O
In O
Section O
[ O
reference O
] O
the O
analysis O
has O
shown O
, O
that O
DeXpression Method
works O
without O
major O
mistakes O
. O
Most O
misclassifications O
have O
occurred O
during O
the O
first O
few O
images O
of O
an O
emotion O
sequence O
. O
Often O
in O
these O
images O
emotions O
are O
not O
yet O
displayed O
. O
paragraph O
: O
Future O
Work O
An O
application O
built O
on O
DeXpression Method
which O
is O
used O
in O
a O
real O
environment O
could O
benefit O
from O
distinguishing O
between O
more O
emotions O
such O
as O
Nervousness O
and O
Panic O
. O
Such O
a O
scenario O
could O
be O
large O
events O
where O
an O
early Task
detection Task
of Task
Panic Task
could O
help O
to O
prevent O
mass O
panics O
. O
Other O
approaches O
to O
enhance O
emotion Task
recognition Task
could O
be O
to O
allow O
for O
composed O
emotions O
. O
For O
example O
frustration O
can O
be O
accompanied O
by O
anger O
, O
therefore O
not O
only O
showing O
one O
emotion O
, O
but O
also O
the O
reason O
. O
Thus O
complex O
emotions O
could O
be O
more O
valuable O
than O
basic O
ones O
. O
Besides O
distinguishing O
between O
different O
emotions O
, O
also O
the O
strength O
of O
an O
emotion O
could O
be O
considered O
. O
Being O
able O
to O
distinguish O
between O
different O
levels O
could O
improve O
applications O
, O
like O
evaluating O
reactions O
to O
new O
products O
. O
In O
this O
example O
it O
could O
predict O
the O
amount O
of O
orders O
that O
will O
be O
made O
, O
therefore O
enabling O
producing O
the O
right O
amount O
of O
products O
. O
section O
: O
Acknowledgments O
We O
would O
like O
to O
thank O
the O
Affect Method
Analysis Method
Group O
of O
the O
University O
of O
Pittsburgh O
for O
providing O
the O
Extended Material
CohnâKanade Material
database Material
, O
and O
Prof O
. O
Pantic O
and O
Dr. O
Valstar O
for O
the O
MMI Material
data O
- O
base O
. O
bibliography O
: O
References O
