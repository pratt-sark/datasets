document O
: O
Multilingual Task
Part Task
- Task
of Task
- Task
Speech Task
Tagging Task
with O
Bidirectional Method
Long Method
Short Method
- Method
Term Method
Memory Method
Models Method
and O
Auxiliary Method
Loss Method
Bidirectional Method
long Method
short Method
- Method
term Method
memory Method
( O
bi Method
- Method
LSTM Method
) O
networks O
have O
recently O
proven O
successful O
for O
various O
NLP Task
sequence Task
modeling Task
tasks Task
, O
but O
little O
is O
known O
about O
their O
reliance O
to O
input O
representations O
, O
target O
languages O
, O
data O
set O
size O
, O
and O
label O
noise O
. O
We O
address O
these O
issues O
and O
evaluate O
bi O
- O
LSTMs Method
with O
word Method
, Method
character Method
, Method
and Method
unicode Method
byte Method
embeddings Method
for O
POS Task
tagging Task
. O
We O
compare O
bi O
- O
LSTMs Method
to O
traditional O
POS Method
taggers Method
across O
languages O
and O
data O
sizes O
. O
We O
also O
present O
a O
novel O
bi Method
- Method
LSTM Method
model O
, O
which O
combines O
the O
POS Task
tagging Task
loss O
function O
with O
an O
auxiliary Method
loss Method
function O
that O
accounts O
for O
rare O
words O
. O
The O
model O
obtains O
state O
- O
of O
- O
the O
- O
art O
performance O
across O
22 O
languages O
, O
and O
works O
especially O
well O
for O
morphologically O
complex O
languages O
. O
Our O
analysis O
suggests O
that O
bi O
- O
LSTMs Method
are O
less O
sensitive O
to O
training O
data O
size O
and O
label O
corruptions O
( O
at O
small O
noise O
levels O
) O
than O
previously O
assumed O
. O
section O
: O
Introduction O
Recently O
, O
bidirectional Method
long Method
short Method
- Method
term Method
memory Method
networks Method
( O
bi Method
- Method
LSTM Method
) O
have O
been O
used O
for O
language Task
modelling Task
, O
POS Task
tagging Task
, O
transition Task
- Task
based Task
dependency Task
parsing Task
, O
fine Task
- Task
grained Task
sentiment Task
analysis Task
, O
syntactic Task
chunking Task
, O
and O
semantic Task
role Task
labeling Task
. O
LSTMs Method
are O
recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
in O
which O
layers O
are O
designed O
to O
prevent O
vanishing O
gradients O
. O
Bidirectional O
LSTMs Method
make O
a O
backward O
and O
forward O
pass O
through O
the O
sequence O
before O
passing O
on O
to O
the O
next O
layer O
. O
For O
further O
details O
, O
see O
. O
We O
consider O
using O
bi O
- O
LSTMs Method
for O
POS Task
tagging Task
. O
Previous O
work O
on O
using O
deep Method
learning Method
- Method
based Method
methods Method
for O
POS Task
tagging Task
has O
focused O
either O
on O
a O
single O
language O
or O
a O
small O
set O
of O
languages O
. O
Instead O
we O
evaluate O
our O
models O
across O
22 O
languages O
. O
In O
addition O
, O
we O
compare O
performance O
with O
representations O
at O
different O
levels O
of O
granularity O
( O
words O
, O
characters O
, O
and O
bytes O
) O
. O
These O
levels O
of O
representation O
were O
previously O
introduced O
in O
different O
efforts O
, O
but O
a O
comparative O
evaluation O
was O
missing O
. O
Moreover O
, O
deep Method
networks Method
are O
often O
said O
to O
require O
large O
volumes O
of O
training O
data O
. O
We O
investigate O
to O
what O
extent O
bi O
- O
LSTMs Method
are O
more O
sensitive O
to O
the O
amount O
of O
training O
data O
and O
label O
noise O
than O
standard O
POS Method
taggers Method
. O
Finally O
, O
we O
introduce O
a O
novel O
model O
, O
a O
bi Method
- Method
LSTM Method
trained O
with O
auxiliary Method
loss Method
. O
The O
model O
jointly O
predicts O
the O
POS O
and O
the O
log O
frequency O
of O
the O
word O
. O
The O
intuition O
behind O
this O
model O
is O
that O
the O
auxiliary Method
loss Method
, O
being O
predictive O
of O
word O
frequency O
, O
helps O
to O
differentiate O
the O
representations O
of O
rare O
and O
common O
words O
. O
We O
indeed O
observe O
performance O
gains O
on O
rare O
and O
out O
- O
of O
- O
vocabulary O
words O
. O
These O
performance O
gains O
transfer O
into O
general O
improvements O
for O
morphologically O
rich O
languages O
. O
paragraph O
: O
Contributions O
In O
this O
paper O
, O
we O
a O
) O
evaluate O
the O
effectiveness O
of O
different O
representations O
in O
bi O
- O
LSTMs Method
, O
b O
) O
compare O
these O
models O
across O
a O
large O
set O
of O
languages O
and O
under O
varying O
conditions O
( O
data O
size O
, O
label O
noise O
) O
and O
c O
) O
propose O
a O
novel O
bi Method
- Method
LSTM Method
model O
with O
auxiliary Method
loss Method
( O
Logfreq Method
) Method
. O
section O
: O
Tagging Task
with O
bi O
- O
LSTMs Method
Recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
allow O
the O
computation O
of O
fixed Task
- Task
size Task
vector Task
representations Task
for O
word O
sequences O
of O
arbitrary O
length O
. O
An O
RNN Method
is O
a O
function O
that O
reads O
in O
vectors O
and O
produces O
an O
output O
vector O
, O
that O
depends O
on O
the O
entire O
sequence O
. O
The O
vector O
is O
then O
fed O
as O
an O
input O
to O
some O
classifier Method
, O
or O
higher Method
- Method
level Method
RNNs Method
in O
stacked Method
/ Method
hierarchical Method
models Method
. O
The O
entire O
network O
is O
trained O
jointly O
such O
that O
the O
hidden Method
representation Method
captures O
the O
important O
information O
from O
the O
sequence O
for O
the O
prediction Task
task Task
. O
A O
bidirectional Method
recurrent Method
neural Method
network Method
( O
bi Method
- Method
RNN Method
) O
is O
an O
extension O
of O
an O
RNN Method
that O
reads O
the O
input O
sequence O
twice O
, O
from O
left O
to O
right O
and O
right O
to O
left O
, O
and O
the O
encodings O
are O
concatenated O
. O
The O
literature O
uses O
the O
term O
bi Method
- Method
RNN Method
to O
refer O
to O
two O
related O
architectures O
, O
which O
we O
refer O
to O
here O
as O
“ O
context O
bi Method
- Method
RNN Method
” O
and O
“ O
sequence Method
bi Method
- Method
RNN Method
” O
. O
In O
a O
sequence Method
bi Method
- Method
RNN Method
( O
bi Method
- Method
RNN Method
) O
, O
the O
input O
is O
a O
sequence O
of O
vectors O
and O
the O
output O
is O
a O
concatenation O
( O
) O
of O
a O
forward O
( O
) O
and O
reverse O
( O
) O
RNN Method
each O
reading O
the O
sequence O
in O
a O
different O
directions O
: O
In O
a O
context O
bi Method
- Method
RNN Method
( O
bi Method
- Method
RNN Method
) O
, O
we O
get O
an O
additional O
input O
indicating O
a O
sequence O
position O
, O
and O
the O
resulting O
vectors O
result O
from O
concatenating O
the O
RNN Method
encodings Method
up O
to O
: O
Thus O
, O
the O
state O
vector O
in O
this O
bi Method
- Method
RNN Method
encodes O
information O
at O
position O
and O
its O
entire O
sequential O
context O
. O
Another O
view O
of O
the O
context O
bi Method
- Method
RNN Method
is O
of O
taking O
a O
sequence O
and O
returning O
the O
corresponding O
sequence O
of O
state O
vectors O
. O
LSTMs Method
are O
a O
variant O
of O
RNNs Method
that O
replace O
the O
cells O
of O
RNNs Method
with O
LSTM Method
cells Method
that O
were O
designed O
to O
prevent O
vanishing O
gradients O
. O
Bidirectional O
LSTMs Method
are O
the O
bi Method
- Method
RNN Method
counterpart O
based O
on O
LSTMs Method
. O
Our O
basic O
bi Method
- Method
LSTM Method
tagging O
model O
is O
a O
context O
bi Method
- Method
LSTM Method
taking O
as O
input O
word O
embeddings O
. O
We O
incorporate O
subtoken O
information O
using O
an O
hierarchical O
bi Method
- Method
LSTM Method
architecture O
. O
We O
compute O
subtoken O
- O
level O
( O
either O
characters O
or O
unicode O
byte O
) O
embeddings O
of O
words O
using O
a O
sequence O
bi Method
- Method
LSTM Method
at O
the O
lower O
level O
. O
This O
representation O
is O
then O
concatenated O
with O
the O
( O
learned O
) O
word O
embeddings O
vector O
which O
forms O
the O
input O
to O
the O
context O
bi Method
- Method
LSTM Method
at O
the O
next O
layer O
. O
This O
model O
, O
illustrated O
in O
Figure O
[ O
reference O
] O
( O
lower O
part O
in O
left O
figure O
) O
, O
is O
inspired O
by O
ballesteros O
: O
ea:2015 O
. O
We O
also O
test O
models O
in O
which O
we O
only O
keep O
sub O
- O
token O
information O
, O
e.g. O
, O
either O
both O
byte O
and O
character O
embeddings O
( O
Figure O
[ O
reference O
] O
, O
right O
) O
or O
a O
single O
( O
sub Method
-) Method
token Method
representation Method
alone O
. O
In O
our O
novel O
model O
, O
cf O
. O
Figure O
[ O
reference O
] O
left O
, O
we O
train O
the O
bi Method
- Method
LSTM Method
tagger O
to O
predict O
both O
the O
tags O
of O
the O
sequence O
, O
as O
well O
as O
a O
label O
that O
represents O
the O
log O
frequency O
of O
the O
token O
as O
estimated O
from O
the O
training O
data O
. O
Our O
combined O
cross Metric
- Metric
entropy Metric
loss Metric
is O
now O
: O
, O
where O
stands O
for O
a O
POS O
tag O
and O
is O
the O
log O
frequency O
label O
, O
i.e. O
, O
. O
Combining O
this O
log Method
frequency Method
objective Method
with O
the O
tagging Task
task Task
can O
be O
seen O
as O
an O
instance O
of O
multi Task
- Task
task Task
learning Task
in O
which O
the O
labels O
are O
predicted O
jointly O
. O
The O
idea O
behind O
this O
model O
is O
to O
make O
the O
representation O
predictive O
for O
frequency O
, O
which O
encourages O
the O
model O
to O
not O
share O
representations O
between O
common O
and O
rare O
words O
, O
thus O
benefiting O
the O
handling O
of O
rare O
tokens O
. O
section O
: O
Experiments O
All O
bi Method
- Method
LSTM Method
models O
were O
implemented O
in O
CNN Method
/ O
pycnn Method
, O
a O
flexible Method
neural Method
network Method
library Method
. O
For O
all O
models O
we O
use O
the O
same O
hyperparameters O
, O
which O
were O
set O
on O
English Material
dev Material
, O
i.e. O
, O
SGD Method
training Method
with O
cross Method
- Method
entropy Method
loss Method
, O
no O
mini O
- O
batches O
, O
20 O
epochs O
, O
default O
learning Metric
rate Metric
( O
0.1 O
) O
, O
128 O
dimensions O
for O
word O
embeddings O
, O
100 O
for O
character O
and O
byte O
embeddings O
, O
100 O
hidden O
states O
and O
Gaussian O
noise O
with O
= O
0.2 O
. O
As O
training Task
is O
stochastic O
in O
nature O
, O
we O
use O
a O
fixed O
seed O
throughout O
. O
Embeddings Method
are O
not O
initialized O
with O
pre O
- O
trained O
embeddings O
, O
except O
when O
reported O
otherwise O
. O
In O
that O
case O
we O
use O
off O
- O
the O
- O
shelf O
polyglot Method
embeddings Method
. O
No O
further O
unlabeled O
data O
is O
considered O
in O
this O
paper O
. O
The O
code O
is O
released O
at O
: O
paragraph O
: O
Taggers O
We O
want O
to O
compare O
POS O
taggers O
under O
varying O
conditions O
. O
We O
hence O
use O
three O
different O
types O
of O
taggers Method
: O
our O
implementation O
of O
a O
bi Method
- Method
LSTM Method
; O
Tnt Method
—a O
second Method
order Method
HMM Method
with O
suffix Method
trie Method
handling Method
for O
OOVs Method
. O
We O
use O
Tnt Method
as O
it O
was O
among O
the O
best O
performing O
taggers Method
evaluated O
in O
horsmann O
: O
ea:2015 O
. O
We O
complement O
the O
NN Method
- Method
based Method
and O
HMM Method
- Method
based Method
tagger Method
with O
a O
CRF Method
tagger Method
, O
using O
a O
freely O
available O
implementation O
based O
on O
crfsuite Method
. O
subsection O
: O
Datasets O
For O
the O
multilingual Task
experiments Task
, O
we O
use O
the O
data O
from O
the O
Universal O
Dependencies O
project O
v1.2 O
( O
17 O
POS O
) O
with O
the O
canonical O
data O
splits O
. O
For O
languages O
with O
token Task
segmentation Task
ambiguity Task
we O
use O
the O
provided O
gold Method
segmentation Method
. O
If O
there O
is O
more O
than O
one O
treebank O
per O
language O
, O
we O
use O
the O
treebank O
that O
has O
the O
canonical O
language O
name O
( O
e.g. O
, O
Finnish Material
instead O
of O
Finnish Material
- O
FTB Material
) O
. O
We O
consider O
all O
languages O
that O
have O
at O
least O
60k O
tokens O
and O
are O
distributed O
with O
word O
forms O
, O
resulting O
in O
22 O
languages O
. O
We O
also O
report O
accuracies Metric
on O
WSJ Material
( O
45 O
POS Material
) O
using O
the O
standard O
splits O
. O
The O
overview O
of O
languages O
is O
provided O
in O
Table O
[ O
reference O
] O
. O
subsection O
: O
Results O
Our O
results O
are O
given O
in O
Table O
[ O
reference O
] O
. O
First O
of O
all O
, O
notice O
that O
TnT Method
performs O
remarkably O
well O
across O
the O
22 O
languages O
, O
closely O
followed O
by O
CRF Method
. O
The O
bi Method
- Method
LSTM Method
tagger O
( O
) O
without O
lower O
- O
level O
bi Method
- Method
LSTM Method
for O
subtokens O
falls O
short O
, O
outperforms O
the O
traditional O
taggers Method
only O
on O
3 O
languages O
. O
The O
bi Method
- Method
LSTM Method
model O
clearly O
benefits O
from O
character Method
representations Method
. O
The O
model O
using O
characters O
alone O
( O
) O
works O
remarkably O
well O
, O
it O
improves O
over O
TnT Method
on O
9 O
languages O
( O
incl O
. O
Slavic Material
and O
Nordic Material
languages Material
) O
. O
The O
combined O
word Method
+ Method
character Method
representation Method
model Method
is O
the O
best O
representation O
, O
outperforming O
the O
baseline O
on O
all O
except O
one O
language O
( O
Indonesian Material
) O
, O
providing O
strong O
results O
already O
without O
pre O
- O
trained O
embeddings O
. O
This O
model O
( O
) O
reaches O
the O
biggest O
improvement O
( O
more O
than O
+ O
2 O
% O
accuracy Metric
) O
on O
Hebrew Material
and O
Slovene Material
. O
Initializing O
the O
word Method
embeddings Method
( O
+ O
Polyglot Method
) O
with O
off O
- O
the O
- O
shelf O
language Method
- Method
specific Method
embeddings Method
further O
improves O
accuracy Metric
. O
The O
only O
system O
we O
are O
aware O
of O
that O
evaluates O
on O
UD Material
is O
gillick O
: O
ea:2016 O
( O
last O
column O
) O
. O
However O
, O
note O
that O
these O
results O
are O
not O
strictly O
comparable O
as O
they O
use O
the O
earlier O
UD Material
v1.1 Material
version Material
. O
The O
overall O
best O
system O
is O
the O
multi O
- O
task O
bi Method
- Method
LSTM Method
freqbin O
( O
it O
uses O
and O
Polyglot O
initialization O
for O
) O
. O
While O
on O
macro O
average O
it O
is O
on O
par O
with O
bi Method
- Method
LSTM Method
, O
it O
obtains O
the O
best O
results O
on O
12 O
/ O
22 O
languages O
, O
and O
it O
is O
successful O
in O
predicting Task
POS Task
for O
OOV Task
tokens Task
( O
cf O
. O
Table O
[ O
reference O
] O
OOV O
Acc O
columns O
) O
, O
especially O
for O
languages O
like O
Arabic Material
, O
Farsi Material
, O
Hebrew O
, O
Finnish Material
. O
We O
examined O
simple O
RNNs Method
and O
confirm O
the O
finding O
of O
ling O
: O
ea:2015 Method
that O
they O
performed O
worse O
than O
their O
LSTM Method
counterparts Method
. O
Finally O
, O
the O
bi Method
- Method
LSTM Method
tagger O
is O
competitive O
on O
WSJ Material
, O
cf O
. O
Table O
[ O
reference O
] O
. O
paragraph O
: O
Rare O
words O
In O
order O
to O
evaluate O
the O
effect O
of O
modeling O
sub O
- O
token O
information O
, O
we O
examine O
accuracy Metric
rates O
at O
different O
frequency Metric
rates Metric
. O
Figure O
[ O
reference O
] O
shows O
absolute O
improvements O
in O
accuracy Metric
of O
bi Method
- Method
LSTM Method
over O
mean Method
log Method
frequency Method
, O
for O
different O
language O
families O
. O
We O
see O
that O
especially O
for O
Slavic Material
and O
non Material
- Material
Indoeuropean Material
languages Material
, O
having O
high O
morphologic Metric
complexity Metric
, O
most O
of O
the O
improvement O
is O
obtained O
in O
the O
Zipfian O
tail O
. O
Rare O
tokens O
benefit O
from O
the O
sub Method
- Method
token Method
representations Method
. O
paragraph O
: O
Data O
set O
size O
Prior O
work O
mostly O
used O
large O
data O
sets O
when O
applying O
neural Method
network Method
based Method
approaches Method
. O
We O
evaluate O
how O
brittle O
such O
models O
are O
with O
respect O
to O
their O
more O
traditional O
counterparts O
by O
training O
bi Method
- Method
LSTM Method
( O
without O
Polyglot Method
embeddings Method
) O
for O
increasing O
amounts O
of O
training O
instances O
( O
number O
of O
sentences O
) O
. O
The O
learning O
curves O
in O
Figure O
[ O
reference O
] O
show O
similar O
trends O
across O
language O
families O
. O
TnT Method
is O
better O
with O
little O
data O
, O
bi Method
- Method
LSTM Method
is O
better O
with O
more O
data O
, O
and O
bi Method
- Method
LSTM Method
always O
wins O
over O
CRF Method
. O
The O
bi Method
- Method
LSTM Method
model O
performs O
already O
surprisingly O
well O
after O
only O
500 O
training O
sentences O
. O
For O
non Material
- Material
Indoeuropean Material
languages Material
it O
is O
on O
par O
and O
above O
the O
other O
taggers Method
with O
even O
less O
data O
( O
100 O
sentences O
) O
. O
This O
shows O
that O
the O
bi O
- O
LSTMs Method
often O
needs O
more O
data O
than O
the O
generative Method
markovian Method
model Method
, O
but O
this O
is O
definitely O
less O
than O
what O
we O
expected O
. O
paragraph O
: O
Label O
Noise O
We O
investigated O
the O
susceptibility O
of O
the O
models O
to O
noise O
, O
by O
artificially O
corrupting O
training O
labels O
. O
Our O
initial O
results O
show O
that O
at O
low Metric
noise Metric
rates Metric
, O
bi O
- O
LSTMs Method
and O
TnT Method
are O
affected O
similarly O
, O
their O
accuracies Metric
drop O
to O
a O
similar O
degree O
. O
Only O
at O
higher O
noise O
levels O
( O
more O
than O
30 O
% O
corrupted O
labels O
) O
, O
bi O
- O
LSTMs Method
are O
less O
robust O
, O
showing O
higher O
drops O
in O
accuracy Metric
compared O
to O
TnT Method
. O
This O
is O
the O
case O
for O
all O
investigated O
language O
families O
. O
section O
: O
Related O
Work O
Character Method
embeddings Method
were O
first O
introduced O
by O
sutskever O
: O
ea:2011 O
for O
language Task
modeling Task
. O
Early O
applications O
include O
text Task
classification Task
. O
Recently O
, O
these O
representations O
were O
successfully O
applied O
to O
a O
range O
of O
structured Task
prediction Task
tasks Task
. O
For O
POS Task
tagging Task
, O
santos O
: O
zadrozny:2014 O
were O
the O
first O
to O
propose O
character Method
- Method
based Method
models Method
. O
They O
use O
a O
convolutional Method
neural Method
network Method
( O
CNN Method
; O
or O
convnet Method
) O
and O
evaluated O
their O
model O
on O
English Material
( O
PTB Material
) O
and O
Portuguese Material
, O
showing O
that O
the O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
close O
to O
taggers Method
using O
carefully O
designed O
feature O
templates O
. O
ling O
: O
ea:2015 O
extend O
this O
line O
and O
compare O
a O
novel O
bi Method
- Method
LSTM Method
model O
, O
learning O
word Method
representations Method
through O
character Method
embeddings Method
. O
They O
evaluate O
their O
model O
on O
a O
language Task
modeling Task
and O
POS Task
tagging Task
setup Task
, O
and O
show O
that O
bi O
- O
LSTMs Method
outperform O
the O
CNN Method
approach O
of O
santos O
: O
zadrozny:2014 O
. O
Similarly O
, O
labeau O
: O
ea:2015 O
evaluate O
character Task
embeddings Task
for O
German Material
. O
Bi O
- O
LSTMs Method
for O
POS Task
tagging Task
are O
also O
reported O
in O
wang O
: O
ea:2015:arxiv O
, O
however O
, O
they O
only O
explore O
word O
embeddings O
, O
orthographic O
information O
and O
evaluate O
on O
WSJ Material
only O
. O
A O
related O
study O
is O
cheng O
: O
fang O
: O
ostendorf:2015 O
who O
propose O
a O
multi Method
- Method
task Method
RNN Method
for O
named Task
entity Task
recognition Task
by O
jointly O
predicting O
the O
next O
token O
and O
current O
token O
’s O
name O
label O
. O
Our O
model O
is O
simpler O
, O
it O
uses O
a O
very O
coarse O
set O
of O
labels O
rather O
then O
integrating O
an O
entire O
language Task
modeling Task
task O
which O
is O
computationally O
more O
expensive O
. O
An O
interesting O
recent O
study O
is O
gillick O
: O
ea:2016 O
, O
they O
build O
a O
single O
byte Method
- Method
to Method
- Method
span Method
model Method
for O
multiple O
languages O
based O
on O
a O
sequence Method
- Method
to Method
- Method
sequence Method
RNN Method
achieving O
impressive O
results O
. O
We O
would O
like O
to O
extend O
this O
work O
in O
their O
direction O
. O
section O
: O
Conclusions O
We O
evaluated O
token Method
and Method
subtoken Method
- Method
level Method
representations Method
for O
neural Task
network Task
- Task
based Task
part Task
- Task
of Task
- Task
speech Task
tagging Task
across O
22 O
languages O
and O
proposed O
a O
novel O
multi Method
- Method
task Method
bi Method
- Method
LSTM Method
with O
auxiliary Method
loss Method
. O
The O
auxiliary Method
loss Method
is O
effective O
at O
improving O
the O
accuracy Metric
of O
rare O
words O
. O
Subtoken Method
representations Method
are O
necessary O
to O
obtain O
a O
state O
- O
of O
- O
the O
- O
art O
POS Method
tagger Method
, O
and O
character Method
embeddings Method
are O
particularly O
helpful O
for O
non Material
- Material
Indoeuropean Material
and O
Slavic Material
languages Material
. O
Combining O
them O
with O
word Method
embeddings Method
in O
a O
hierarchical Method
network Method
provides O
the O
best O
representation O
. O
The O
bi Method
- Method
LSTM Method
tagger O
is O
as O
effective O
as O
the O
CRF Method
and Method
HMM Method
taggers Method
with O
already O
as O
little O
as O
500 O
training O
sentences O
, O
but O
is O
less O
robust O
to O
label O
noise O
( O
at O
higher O
noise Metric
rates Metric
) O
. O
section O
: O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
feedback O
. O
AS O
is O
funded O
by O
the O
ERC O
Starting O
Grant O
LOWLANDS O
No O
. O
313695 O
. O
YG O
is O
supported O
by O
The O
Israeli O
Science O
Foundation O
( O
grant O
number O
1555 O
/ O
15 O
) O
and O
a O
Google O
Research O
Award O
. O
bibliography O
: O
References O
