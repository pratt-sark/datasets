document O
: O
Light Method
Gated Method
Recurrent Method
Units Method
for O
Speech Task
Recognition Task
A O
field O
that O
has O
directly O
benefited O
from O
the O
recent O
advances O
in O
deep Method
learning Method
is O
Automatic O
Speech Task
Recognition Task
( O
ASR Task
) O
. O
Despite O
the O
great O
achievements O
of O
the O
past O
decades O
, O
however O
, O
a O
natural O
and O
robust O
human Task
- Task
machine Task
speech Task
interaction Task
still O
appears O
to O
be O
out O
of O
reach O
, O
especially O
in O
challenging O
environments O
characterized O
by O
significant O
noise O
and O
reverberation O
. O
To O
improve O
robustness Metric
, O
modern O
speech Method
recognizers Method
often O
employ O
acoustic Method
models Method
based O
on O
Recurrent Method
Neural Method
Networks Method
( O
RNNs Method
) O
, O
that O
are O
naturally O
able O
to O
exploit O
large O
time O
contexts O
and O
long O
- O
term O
speech O
modulations O
. O
It O
is O
thus O
of O
great O
interest O
to O
continue O
the O
study O
of O
proper O
techniques O
for O
improving O
the O
effectiveness O
of O
RNNs Method
in O
processing O
speech O
signals O
. O
In O
this O
paper O
, O
we O
revise O
one O
of O
the O
most O
popular O
RNN Method
models O
, O
namely O
Gated Method
Recurrent Method
Units Method
( O
GRUs Method
) O
, O
and O
propose O
a O
simplified O
architecture O
that O
turned O
out O
to O
be O
very O
effective O
for O
ASR Task
. O
The O
contribution O
of O
this O
work O
is O
two O
- O
fold O
: O
First O
, O
we O
analyze O
the O
role O
played O
by O
the O
reset O
gate O
, O
showing O
that O
a O
significant O
redundancy O
with O
the O
update O
gate O
occurs O
. O
As O
a O
result O
, O
we O
propose O
to O
remove O
the O
former O
from O
the O
GRU Method
design O
, O
leading O
to O
a O
more O
efficient O
and O
compact O
single Method
- Method
gate Method
model Method
. O
Second O
, O
we O
propose O
to O
replace O
hyperbolic Method
tangent Method
with O
ReLU Method
activations O
. O
This O
variation O
couples O
well O
with O
batch Method
normalization Method
and O
could O
help O
the O
model O
learn O
long O
- O
term O
dependencies O
without O
numerical O
issues O
. O
Results O
show O
that O
the O
proposed O
architecture O
, O
called O
Light Method
GRU Method
( O
Li O
- O
GRU Method
) O
, O
not O
only O
reduces O
the O
per Metric
- Metric
epoch Metric
training Metric
time Metric
by O
more O
than O
30 O
% O
over O
a O
standard O
GRU Method
, O
but O
also O
consistently O
improves O
the O
recognition Metric
accuracy Metric
across O
different O
tasks O
, O
input O
features O
, O
noisy O
conditions O
, O
as O
well O
as O
across O
different O
ASR Task
paradigms O
, O
ranging O
from O
standard O
DNN Method
- O
HMM O
speech O
recognizers O
to O
end Method
- Method
to Method
- Method
end Method
CTC Method
models Method
. O
speech Task
recognition Task
, O
deep Task
learning Task
, O
recurrent Method
neural Method
networks Method
, O
LSTM Method
, O
GRU Method
section O
: O
Introduction O
Deep Method
learning Method
is O
an O
emerging O
technology O
that O
is O
considered O
one O
of O
the O
most O
promising O
directions O
for O
reaching O
higher O
levels O
of O
artificial Task
intelligence Task
. O
This O
paradigm O
is O
rapidly O
evolving O
and O
some O
noteworthy O
achievements O
of O
the O
last O
years O
include O
, O
among O
the O
others O
, O
the O
development O
of O
effective O
regularization Method
methods Method
, O
improved O
optimization Method
algorithms Method
, O
and O
better O
architectures O
. O
The O
exploration O
of O
generative Method
models Method
, O
deep Method
reinforcement Method
learning Method
as O
well O
as O
the O
evolution O
of O
sequence Task
to Task
sequence Task
paradigms Task
also O
represent O
important O
milestones O
in O
the O
field O
. O
Deep Method
learning Method
is O
now O
being O
deployed O
in O
a O
wide O
range O
of O
domains O
, O
including O
bio Task
- Task
informatics Task
, O
computer Task
vision Task
, O
machine Task
translation Task
, O
dialogue Task
systems Task
and O
natural Task
language Task
processing Task
, O
just O
to O
name O
a O
few O
. O
Another O
field O
that O
has O
been O
transformed O
by O
this O
technology O
is O
Automatic O
Speech Task
Recognition Task
( O
ASR Task
) O
. O
Thanks O
to O
modern O
Deep Method
Neural Method
Networks Method
( O
DNNs Method
) O
, O
current O
speech Method
recognizers Method
are O
now O
able O
to O
significantly O
outperform O
previous O
GMM Method
- Method
HMM Method
systems Method
, O
allowing O
ASR Task
to O
be O
applied O
in O
several O
contexts O
, O
such O
as O
web Task
- Task
search Task
, O
intelligent Task
personal Task
assistants Task
, O
car Task
control Task
and O
radiological Task
reporting Task
. O
Despite O
the O
progress O
of O
the O
last O
decade O
, O
state O
- O
of O
- O
the O
- O
art O
speech Method
recognizers Method
are O
still O
far O
away O
from O
reaching O
satisfactory O
robustness Metric
and O
flexibility Metric
. O
This O
lack O
of O
robustness Metric
typically O
happens O
when O
facing O
challenging O
acoustic O
conditions O
, O
characterized O
by O
considerable O
levels O
of O
non O
- O
stationary O
noise O
and O
acoustic O
reverberation O
. O
The O
development O
of O
robust O
ASR Task
has O
been O
recently O
fostered O
by O
the O
great O
success O
of O
some O
international O
challenges O
such O
as O
CHiME Material
, O
REVERB Method
and O
ASpIRE Method
, O
which O
were O
also O
extremely O
useful O
to O
establish O
common O
evaluation Method
frameworks Method
among O
researchers O
. O
Currently O
, O
the O
dominant O
approach O
to O
automatic Task
speech Task
recognition Task
relies O
on O
a O
combination O
of O
a O
discriminative O
DNN Method
and O
a O
generative Method
Hidden Method
Markov Method
Model Method
( O
HMM Method
) O
. O
The O
DNN Method
is O
normally O
employed O
for O
acoustic Task
modeling Task
purposes Task
to O
predict O
context Task
- Task
dependent Task
phone Task
targets Task
. O
The O
acoustic O
- O
level O
predictions O
are O
later O
embedded O
in O
an O
HMM Method
- Method
based Method
framework Method
, O
that O
also O
integrates O
phone O
- O
transitions O
, O
lexicon O
, O
and O
language O
model O
information O
to O
retrieve O
the O
final O
sequence O
of O
words O
. O
An O
emerging O
alternative O
is O
end Task
- Task
to Task
- Task
end Task
speech Task
recognition Task
, O
that O
aims O
to O
drastically O
simplify O
the O
current O
ASR Task
pipeline O
by O
using O
fully Method
discriminative Method
systems Method
that O
learn O
everything O
from O
data O
without O
( O
ideally O
) O
any O
additional O
human O
effort O
. O
Popular O
end Method
- Method
to Method
- Method
end Method
techniques Method
are O
attention Method
models Method
and O
Connectionist Method
Temporal Method
Classification Method
( O
CTC Method
) O
. O
Attention Method
models Method
are O
based O
on O
an O
encoder Method
- Method
decoder Method
architecture Method
coupled O
with O
an O
attention Method
mechanism Method
that O
decides O
which O
input O
information O
to O
analyze O
at O
each O
decoding O
step O
. O
CTC Method
is O
based O
on O
a O
DNN Method
predicting O
symbols O
from O
a O
predefined O
alphabet O
( O
characters O
, O
phones O
, O
words O
) O
to O
which O
an O
extra O
unit O
( O
blank O
) O
that O
emits O
no O
labels O
is O
added O
. O
Similarly O
to O
HMMs Method
, O
the O
likelihood O
( O
and O
its O
gradient O
with O
respect O
to O
the O
DNN Method
parameters O
) O
are O
computed O
with O
dynamic Method
programming Method
by O
summing O
over O
all O
the O
paths O
that O
are O
possible O
realizations O
of O
the O
ground O
- O
truth O
label O
sequence O
. O
This O
way O
, O
CTC Method
allows O
one O
to O
optimize O
the O
likelihood O
of O
the O
desired O
output O
sequence O
directly O
, O
without O
the O
need O
for O
an O
explicit O
label O
alignment O
. O
For O
both O
the O
aforementioned O
frameworks O
, O
Recurrent Method
Neural Method
Networks Method
( O
RNNs Method
) O
represent O
a O
valid O
alternative O
to O
standard O
feed O
- O
forward O
DNNs Method
. O
RNNs Method
, O
in O
fact O
, O
are O
more O
and O
more O
often O
employed O
in O
speech Task
recognition Task
, O
due O
to O
their O
capabilities O
to O
properly O
manage O
time O
contexts O
and O
capture O
long O
- O
term O
speech O
modulations O
. O
In O
the O
machine Task
learning Task
community Task
, O
the O
research O
of O
novel O
and O
powerful O
RNN Method
models O
is O
a O
very O
active O
research O
topic O
. O
General O
- O
purpose O
RNNs Method
such O
as O
Long Method
Short Method
Term Method
Memories Method
( O
LSTMs Method
) O
have O
been O
the O
subject O
of O
several O
studies O
and O
modifications O
over O
the O
past O
years O
. O
This O
evolution O
has O
recently O
led O
to O
a O
novel O
architecture O
called O
Gated Method
Recurrent Method
Unit Method
( O
GRU Method
) O
, O
that O
simplifies O
the O
complex O
LSTM Method
cell Method
design Method
. O
Our O
work O
continues O
these O
efforts O
by O
further O
revising O
GRUs Method
. O
Differently O
from O
previous O
efforts O
, O
our O
primary O
goal O
is O
not O
to O
derive O
a O
general O
- O
purpose O
RNN Method
, O
but O
to O
modify O
the O
standard O
GRU Method
design O
in O
order O
to O
better O
address O
speech Task
recognition Task
. O
In O
particular O
, O
the O
major O
contribution O
of O
this O
paper O
is O
twofold O
: O
First O
, O
we O
propose O
to O
remove O
the O
reset O
gate O
from O
the O
network Method
design Method
. O
Similarly O
to O
, O
we O
found O
that O
removing O
it O
does O
not O
significantly O
affect O
the O
system O
performance O
, O
also O
due O
to O
a O
certain O
redundancy O
observed O
between O
update O
and O
reset O
gates O
. O
Second O
, O
we O
propose O
to O
replace O
hyperbolic Method
tangent Method
( O
tanh Method
) O
with O
Rectified Method
Linear Method
Unit Method
( O
ReLU Method
) O
activations O
in O
the O
state Method
update Method
equation Method
. O
ReLU Method
units O
have O
been O
shown O
to O
be O
more O
effective O
than O
sigmoid Method
non Method
- Method
linearities Method
for O
feed O
- O
forward O
DNNs Method
. O
Despite O
its O
recent O
success O
, O
this O
non O
- O
linearity O
has O
largely O
been O
avoided O
for O
RNNs Method
, O
due O
to O
the O
numerical O
instabilities O
caused O
by O
the O
unboundedness O
of O
ReLU Method
activations O
: O
composing O
many O
times O
GRU Method
layers O
( O
i.e. O
, O
GRU Method
units Method
following O
an O
affine Method
transformation Method
) O
with O
sufficiently O
large O
weights O
can O
lead O
to O
arbitrarily O
large O
state O
values O
. O
However O
, O
when O
coupling O
our O
ReLU Method
- O
based O
GRU Method
with O
batch Method
normalization Method
, O
we O
did O
not O
experience O
such O
numerical O
issues O
. O
This O
allows O
us O
to O
take O
advantage O
of O
both O
techniques O
, O
that O
have O
been O
proven O
effective O
to O
mitigate O
the O
vanishing Task
gradient Task
problem Task
as O
well O
as O
to O
speed O
up O
network Task
training Task
. O
We O
evaluated O
our O
proposed O
architecture O
on O
different O
tasks O
, O
datasets O
, O
input O
features O
, O
noisy O
conditions O
as O
well O
as O
on O
different O
ASR Task
frameworks O
( O
i.e. O
, O
DNN Method
- O
HMM O
and O
CTC Method
) O
. O
Results O
show O
that O
the O
revised O
architecture O
reduces O
the O
per Metric
- Metric
epoch Metric
training Metric
wall Metric
- Metric
clock Metric
time Metric
by O
more O
than O
30 O
% O
, O
while O
improving O
the O
recognition Metric
accuracy Metric
. O
Moreover O
, O
the O
proposed O
solution O
leads O
to O
a O
compact Method
model Method
, O
that O
is O
arguably O
easier O
to O
interpret O
, O
understand O
and O
implement O
, O
due O
to O
a O
simplified O
design O
based O
on O
a O
single O
gate O
. O
The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
Sec O
. O
[ O
reference O
] O
recalls O
the O
standard O
GRU Method
architecture O
, O
while O
Sec O
. O
[ O
reference O
] O
illustrates O
in O
detail O
the O
proposed O
model O
and O
the O
related O
work O
. O
In O
Sec O
. O
[ O
reference O
] O
, O
a O
description O
of O
the O
adopted O
corpora O
and O
experimental O
setup O
is O
provided O
. O
The O
results O
are O
then O
reported O
in O
Sec O
. O
[ O
reference O
] O
. O
Finally O
, O
our O
conclusions O
are O
drawn O
in O
Sec O
. O
[ O
reference O
] O
. O
section O
: O
Gated Method
Recurrent Method
Units Method
The O
most O
suitable O
architecture O
able O
to O
learn O
short O
and O
long O
- O
term O
speech O
dependencies O
is O
represented O
by O
RNNs Method
. O
RNNs Method
, O
indeed O
, O
can O
potentially O
capture O
temporal O
information O
in O
a O
very O
dynamic O
fashion O
, O
allowing O
the O
network O
to O
freely O
decide O
the O
amount O
of O
contextual O
information O
to O
use O
for O
each O
time O
step O
. O
Several O
works O
have O
already O
highlighted O
the O
effectiveness O
of O
RNNs Method
in O
various O
speech Task
processing Task
tasks Task
, O
such O
as O
speech Task
recognition Task
, O
speech Task
enhancement Task
, O
speech Task
separation Task
as O
well O
as O
speech Task
activity Task
detection Task
. O
Training O
RNNs Method
, O
however O
, O
can O
be O
complicated O
by O
vanishing O
and O
exploding O
gradients O
, O
that O
might O
impair O
learning O
long O
- O
term O
dependencies O
. O
Although O
exploding O
gradients O
can O
be O
tackled O
with O
simple O
clipping Method
strategies Method
, O
the O
vanishing Task
gradient Task
problem Task
requires O
special O
architectures O
to O
be O
properly O
addressed O
. O
A O
common O
approach O
relies O
on O
the O
so O
- O
called O
gated Method
RNNs Method
, O
whose O
core O
idea O
is O
to O
introduce O
a O
gating Method
mechanism Method
for O
better O
controlling O
the O
flow O
of O
the O
information O
through O
the O
various O
time O
- O
steps O
. O
Within O
this O
family O
of O
architectures O
, O
vanishing O
gradient O
issues O
are O
mitigated O
by O
creating O
effective O
“ O
shortcuts O
” O
, O
in O
which O
the O
gradients O
can O
bypass O
multiple O
temporal O
steps O
. O
The O
most O
popular O
gated Method
RNNs Method
are O
LSTMs Method
, O
that O
often O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
several O
machine Task
learning Task
tasks Task
, O
including O
speech Task
recognition Task
. O
LSTMs Method
rely O
on O
memory Method
cells Method
that O
are O
controlled O
by O
forget O
, O
input O
, O
and O
output O
gates O
. O
Despite O
their O
effectiveness O
, O
such O
a O
sophisticated O
gating Method
mechanism Method
might O
result O
in O
an O
overly O
complex O
model O
. O
On O
the O
other O
hand O
, O
computational Metric
efficiency Metric
is O
a O
crucial O
issue O
for O
RNNs Method
and O
considerable O
research O
efforts O
have O
recently O
been O
devoted O
to O
the O
development O
of O
alternative O
architectures O
. O
A O
noteworthy O
attempt O
to O
simplify O
LSTMs Method
has O
recently O
led O
to O
a O
novel O
model O
called O
Gated Method
Recurrent Method
Unit Method
( O
GRU Method
) O
, O
that O
is O
based O
on O
just O
two O
multiplicative O
gates O
. O
In O
particular O
, O
the O
standard O
GRU Method
architecture O
is O
defined O
by O
the O
following O
equations O
: O
where O
and O
are O
vectors O
corresponding O
to O
the O
update O
and O
reset O
gates O
, O
respectively O
, O
while O
represents O
the O
state O
vector O
for O
the O
current O
time O
frame O
. O
Element O
- O
wise O
multiplications O
are O
denoted O
with O
. O
The O
activations O
of O
both O
gates O
are O
logistic Method
sigmoid Method
functions Method
, O
that O
constrain O
and O
to O
take O
values O
ranging O
from O
0 O
and O
1 O
. O
The O
candidate O
state O
is O
processed O
with O
a O
hyperbolic Method
tangent Method
. O
The O
network O
is O
fed O
by O
the O
current O
input O
vector O
( O
e.g. O
, O
a O
vector O
of O
speech O
features O
) O
, O
while O
the O
parameters O
of O
the O
model O
are O
the O
matrices O
, O
, O
( O
the O
feed O
- O
forward O
connections O
) O
and O
, O
, O
( O
the O
recurrent O
weights O
) O
. O
The O
architecture O
finally O
includes O
trainable O
bias O
vectors O
, O
and O
, O
that O
are O
added O
before O
the O
non O
- O
linearities O
are O
applied O
. O
As O
shown O
in O
Eq O
. O
[ O
reference O
] O
, O
the O
current O
state O
vector O
is O
a O
linear Method
interpolation Method
between O
the O
previous O
activation O
and O
the O
current O
candidate O
state O
. O
The O
weighting O
factors O
are O
set O
by O
the O
update Method
gate Method
, O
that O
decides O
how O
much O
the O
units O
will O
update O
their O
activations O
. O
This O
linear Method
interpolation Method
is O
the O
key O
component O
for O
learning Task
long Task
- Task
term Task
dependencies Task
. O
If O
is O
close O
to O
one O
, O
in O
fact O
, O
the O
previous O
state O
is O
kept O
unaltered O
and O
can O
remain O
unchanged O
for O
an O
arbitrary O
number O
of O
time O
steps O
. O
On O
the O
other O
hand O
, O
if O
is O
close O
to O
zero O
, O
the O
network O
tends O
to O
favor O
the O
candidate O
state O
, O
that O
depends O
more O
heavily O
on O
the O
current O
input O
and O
on O
the O
closer O
hidden O
states O
. O
The O
candidate O
state O
also O
depends O
on O
the O
reset O
gate O
, O
that O
allows O
the O
model O
to O
possibly O
delete O
the O
past O
memory O
by O
forgetting O
the O
previously O
computed O
states O
. O
section O
: O
A O
novel O
GRU Method
framework O
The O
main O
changes O
to O
the O
standard O
GRU Method
model O
concern O
the O
reset O
gate O
, O
ReLU Method
activations O
, O
and O
batch Method
normalization Method
, O
as O
outlined O
in O
the O
next O
sub O
- O
sections O
. O
subsection O
: O
Removing O
the O
reset O
gate O
From O
the O
previous O
introduction O
to O
GRUs Method
, O
it O
follows O
that O
the O
reset O
gate O
can O
be O
useful O
when O
significant O
discontinuities O
occur O
in O
the O
sequence O
. O
For O
language Task
modeling Task
, O
this O
may O
happen O
when O
moving O
from O
one O
text O
to O
another O
that O
is O
not O
semantically O
related O
. O
In O
such O
situation O
, O
it O
is O
convenient O
to O
reset O
the O
stored O
memory O
to O
avoid O
taking O
a O
decision O
biased O
by O
an O
unrelated O
history O
. O
Nevertheless O
, O
we O
believe O
that O
for O
some O
specific O
tasks O
like O
speech Task
recognition Task
this O
functionality O
might O
not O
be O
useful O
. O
In O
fact O
, O
a O
speech O
signal O
is O
a O
sequence O
that O
evolves O
rather O
slowly O
( O
the O
features O
are O
typically O
computed O
every O
10 O
ms O
) O
, O
in O
which O
the O
past O
history O
can O
virtually O
always O
be O
helpful O
. O
Even O
in O
the O
presence O
of O
strong O
discontinuities O
, O
for O
instance O
observable O
at O
the O
boundary O
between O
a O
vowel O
and O
a O
fricative O
, O
completely O
resetting O
the O
past O
memory O
can O
be O
harmful O
. O
On O
the O
other O
hand O
, O
it O
is O
helpful O
to O
memorize O
phonotactic O
features O
, O
since O
some O
phone O
transitions O
are O
more O
likely O
than O
others O
. O
We O
also O
argue O
that O
a O
certain O
redundancy O
in O
the O
activations O
of O
reset O
and O
update O
gates O
might O
occur O
when O
processing O
speech O
sequences O
. O
For O
instance O
, O
when O
it O
is O
necessary O
to O
give O
more O
importance O
to O
the O
current O
information O
, O
the O
GRU Method
model O
can O
set O
small O
values O
of O
. O
A O
similar O
effect O
can O
be O
achieved O
with O
the O
update O
gate O
only O
, O
if O
small O
values O
are O
assigned O
to O
. O
The O
latter O
solution O
tends O
to O
weight O
more O
the O
candidate O
state O
, O
that O
depends O
heavily O
on O
the O
current O
input O
. O
Similarly O
, O
a O
high O
value O
can O
be O
assigned O
either O
to O
or O
to O
, O
in O
order O
to O
place O
more O
importance O
on O
past O
states O
. O
This O
redundancy O
is O
also O
highlighted O
in O
Fig O
. O
[ O
reference O
] O
, O
where O
a O
temporal O
correlation O
in O
the O
average O
activations O
of O
update O
and O
reset O
gates O
can O
be O
readily O
appreciated O
for O
a O
GRU Method
trained O
on O
TIMIT Material
. O
This O
degree O
of O
redundancy O
will O
be O
analyzed O
in O
a O
quantitative O
way O
in O
Sec O
. O
[ O
reference O
] O
using O
the O
cross Metric
- Metric
correlation Metric
metric Metric
: O
where O
and O
are O
the O
average O
activations O
( O
over O
the O
neurons O
) O
of O
update O
and O
reset O
gates O
, O
respectively O
, O
and O
is O
the O
cross O
- O
correlation O
operator O
. O
Based O
on O
these O
reasons O
, O
the O
first O
variation O
to O
standard O
GRUs Method
thus O
concerns O
the O
removal O
of O
the O
reset O
gate O
. O
This O
change O
leads O
to O
the O
following O
modification O
of O
Eq O
. O
[ O
reference O
] O
: O
The O
main O
benefits O
of O
this O
intervention O
are O
related O
to O
the O
improved O
computational Metric
efficiency Metric
, O
that O
is O
achieved O
thanks O
to O
a O
more O
compact O
single Method
- Method
gate Method
model Method
. O
subsection O
: O
ReLU Method
activations O
The O
second O
modification O
consists O
in O
replacing O
the O
standard O
hyperbolic Method
tangent Method
with O
ReLU Method
activation O
. O
In O
particular O
, O
we O
modify O
the O
computation O
of O
candidate O
state O
( O
Eq O
. O
[ O
reference O
] O
) O
, O
as O
follows O
: O
Standard O
tanh Method
activations O
are O
less O
used O
in O
feedforward Method
networks Method
because O
they O
do O
not O
work O
as O
well O
as O
piecewise Method
- Method
linear Method
activations Method
when O
training O
deeper Method
networks Method
. O
The O
adoption O
of O
ReLU Method
- O
based O
neurons O
, O
that O
have O
shown O
to O
be O
effective O
in O
improving O
such O
limitations O
, O
was O
not O
so O
common O
in O
the O
past O
for O
RNNs Method
. O
This O
was O
due O
to O
numerical O
instabilities O
originating O
from O
the O
unbounded O
ReLU Method
functions O
applied O
over O
long O
time O
series O
. O
However O
, O
coupling O
this O
activation Method
function Method
with O
batch Method
normalization Method
turned O
out O
to O
be O
helpful O
for O
taking O
advantage O
of O
ReLU Method
neurons O
without O
numerical O
issues O
, O
as O
will O
be O
discussed O
in O
the O
next O
sub O
- O
section O
. O
subsection O
: O
Batch Method
Normalization Method
Batch Method
normalization Method
has O
been O
recently O
proposed O
in O
the O
machine Task
learning Task
community Task
and O
addresses O
the O
so O
- O
called O
internal Task
covariate Task
shift Task
problem Task
by O
normalizing O
the O
mean O
and O
the O
variance O
of O
each O
layer O
’s O
pre O
- O
activations O
for O
each O
training O
mini O
- O
batch O
. O
Several O
works O
have O
already O
shown O
that O
this O
technique O
is O
effective O
both O
to O
improve O
the O
system O
performance O
and O
to O
speed O
- O
up O
the O
training Task
procedure Task
. O
Batch Method
normalization Method
can O
be O
applied O
to O
RNNs Method
in O
different O
ways O
. O
In O
, O
the O
authors O
suggest O
to O
apply O
it O
to O
feed O
- O
forward O
connections O
only O
, O
while O
in O
the O
normalization Method
step Method
is O
extended O
to O
recurrent O
connections O
, O
using O
separate O
statistics O
for O
each O
time O
- O
step O
. O
In O
our O
work O
, O
we O
tried O
both O
approaches O
, O
but O
we O
did O
not O
observe O
substantial O
benefits O
when O
extending O
batch Method
normalization Method
to O
recurrent O
parameters O
( O
i.e. O
, O
and O
) O
. O
For O
this O
reason O
, O
we O
applied O
this O
technique O
to O
feed O
- O
forward O
connections O
only O
( O
i.e. O
, O
and O
) O
, O
obtaining O
a O
more O
compact O
model O
that O
is O
almost O
equally O
performing O
but O
significantly O
less O
computationally O
expensive O
. O
When O
batch Method
normalization Method
is O
limited O
to O
feed O
- O
forward O
connections O
, O
indeed O
, O
all O
the O
related O
computations O
become O
independent O
at O
each O
time O
step O
and O
they O
can O
be O
performed O
in O
parallel O
. O
This O
offers O
the O
possibility O
to O
apply O
it O
with O
reduced O
computational Metric
efforts Metric
. O
As O
outlined O
in O
the O
previous O
sub O
- O
section O
, O
coupling O
the O
proposed O
model O
with O
batch Method
- Method
normalization Method
could O
also O
help O
in O
limiting O
the O
numerical O
issues O
of O
ReLU Method
RNNs Method
. O
Batch Method
normalization Method
, O
in O
fact O
, O
rescales O
the O
neuron O
pre O
- O
activations O
, O
inherently O
bounding O
the O
values O
of O
the O
ReLU Method
neurons O
. O
In O
this O
way O
, O
our O
model O
concurrently O
takes O
advantage O
of O
the O
well O
- O
known O
benefits O
of O
both O
ReLU Method
activation O
and O
batch Method
normalization Method
. O
In O
our O
experiments O
, O
we O
found O
that O
the O
latter O
technique O
helps O
against O
numerical O
issues O
also O
when O
it O
is O
limited O
to O
feed O
- O
forward O
connections O
only O
. O
Formally O
, O
removing O
the O
reset O
gate O
, O
replacing O
the O
hyperbolic Method
tangent Method
function O
with O
the O
ReLU Method
activation O
, O
and O
applying O
batch Method
normalization Method
, O
now O
leads O
to O
the O
following O
model O
: O
The O
batch Task
normalization Task
works O
as O
described O
in O
, O
and O
is O
defined O
as O
follows O
: O
where O
and O
are O
the O
minibatch O
mean O
and O
variance O
, O
respectively O
. O
A O
small O
constant O
is O
added O
for O
numerical O
stability O
. O
The O
variables O
and O
are O
trainable O
scaling O
and O
shifting O
parameters O
, O
introduced O
to O
restore O
the O
network O
capacity O
. O
Note O
that O
the O
presence O
of O
makes O
the O
biases O
and O
redundant O
. O
Therefore O
, O
they O
are O
omitted O
in O
Eq O
. O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
We O
called O
this O
architecture O
Light Method
GRU Method
( O
Li Method
- Method
GRU Method
) O
, O
to O
emphasize O
the O
simplification Task
process Task
conducted O
on O
a O
standard O
GRU Method
. O
subsection O
: O
Related O
work O
A O
first O
attempt O
to O
remove O
from O
GRUs Method
has O
recently O
led O
to O
a O
single Method
- Method
gate Method
architecture Method
called O
Minimal Method
Gated Method
Recurrent Method
Unit Method
( O
M Method
- Method
GRU Method
) O
, O
that O
achieves O
a O
performance O
comparable O
to O
that O
obtained O
by O
standard O
GRUs Method
in O
handwritten Task
digit Task
recognition Task
as O
well O
as O
in O
a O
sentiment Task
classification Task
task Task
. O
To O
the O
best O
of O
our O
knowledge O
, O
our O
contribution O
is O
the O
first O
attempt O
that O
explores O
this O
architectural Method
variation Method
in O
speech Task
recognition Task
. O
Recently O
, O
some O
attempts O
have O
also O
been O
done O
for O
embedding O
ReLU Method
units O
in O
the O
RNN Method
framework O
. O
For O
instance O
, O
in O
authors O
replaced O
tanh Method
activations O
with O
ReLU Method
neurons O
in O
a O
vanilla O
RNN Method
, O
showing O
the O
capability O
of O
this O
model O
to O
learn O
long Task
- Task
term Task
dependencies Task
when O
a O
proper O
orthogonal Method
initialization Method
is O
adopted O
. O
In O
this O
work O
, O
we O
extend O
the O
use O
of O
ReLU Method
to O
a O
GRU Method
architecture O
. O
In O
summary O
, O
the O
novelty O
of O
our O
approach O
consists O
in O
the O
integration O
of O
three O
key O
design O
aspects O
( O
i.e O
, O
the O
removal O
of O
the O
reset O
gate O
, O
ReLU Method
activations O
and O
batch Method
normalization Method
) O
in O
a O
single O
model O
, O
that O
turned O
out O
to O
be O
particularly O
suitable O
for O
speech Task
recognition Task
. O
The O
potential O
benefits O
Li Method
- Method
GRUs Method
have O
been O
preliminarily O
observed O
as O
part O
of O
a O
work O
on O
speech Task
recognition Task
described O
in O
. O
This O
study O
extends O
our O
previous O
effort O
in O
several O
ways O
. O
First O
of O
all O
, O
we O
better O
analyze O
the O
correlation O
arising O
between O
reset O
and O
update O
gates O
. O
We O
then O
analyze O
some O
gradient O
statistics O
, O
and O
we O
better O
study O
the O
impact O
of O
batch Method
normalization Method
. O
Moreover O
, O
we O
assess O
our O
approach O
on O
a O
larger O
variety O
of O
speech Task
recognition Task
tasks Task
, O
considering O
several O
different O
datasets O
as O
well O
as O
noisy O
and O
reverberant O
conditions O
. O
Finally O
, O
we O
extend O
our O
experimental O
validation O
to O
a O
end O
- O
to O
- O
end O
CTC Method
model Method
. O
section O
: O
Experimental O
setup O
In O
the O
following O
sub O
- O
sections O
, O
the O
considered O
corpora O
, O
the O
RNN Method
setting O
as O
well O
as O
the O
HMM O
- O
DNN Method
and O
CTC O
setups O
are O
described O
. O
subsection O
: O
Corpora O
and O
tasks O
The O
main O
features O
of O
the O
corpora O
considered O
in O
this O
work O
are O
summarized O
in O
Tab O
. O
[ O
reference O
] O
. O
A O
first O
set O
of O
experiments O
with O
the O
TIMIT Material
corpus O
was O
performed O
to O
test O
the O
proposed O
model O
in O
a O
close Task
- Task
talking Task
scenario Task
. O
These O
experiments O
are O
based O
on O
the O
standard O
phoneme Task
recognition Task
task Task
, O
which O
is O
aligned O
with O
that O
proposed O
in O
the O
Kaldi Method
s5 Method
recipe Method
. O
To O
validate O
our O
model O
in O
a O
more O
realistic O
scenario O
, O
a O
set O
of O
experiments O
was O
also O
conducted O
in O
distant O
- O
talking O
conditions O
with O
the O
DIRHA Material
- Material
English Material
corpus O
. O
The O
reference O
context O
was O
a O
domestic O
environment O
characterized O
by O
the O
presence O
of O
non O
- O
stationary O
noise O
( O
with O
an O
average O
SNR Metric
of O
about O
10dB O
) O
and O
acoustic O
reverberation O
( O
with O
an O
average O
reverberation O
time O
of O
about O
0.7 O
seconds O
) O
. O
Training Method
was O
based O
on O
the O
original O
WSJ Material
- Material
5k Material
corpus Material
( O
consisting O
of O
7138 O
sentences O
uttered O
by O
83 O
speakers O
) O
that O
was O
contaminated O
with O
a O
set O
of O
impulse O
responses O
measured O
in O
a O
real O
apartment O
. O
The O
test O
phase O
was O
carried O
out O
with O
both O
real O
and O
simulated O
datasets O
, O
each O
consisting O
of O
409 O
WSJ Material
sentences Material
uttered O
by O
six O
native O
American O
speakers O
. O
A O
development O
set O
of O
310 O
WSJ Material
sentences Material
uttered O
by O
six O
different O
speakers O
was O
also O
used O
for O
hyperparameter Method
tuning Method
. O
To O
test O
our O
approach O
in O
different O
reverberation O
conditions O
, O
other O
contaminated O
versions O
of O
the O
latter O
training O
and O
test O
data O
are O
generated O
with O
different O
impulse O
responses O
. O
These O
simulations O
are O
based O
on O
the O
image Method
method Method
and O
correspond O
to O
four O
different O
reverberation O
times O
( O
ranging O
from O
250 O
to O
1000 O
ms O
) O
. O
More O
details O
on O
the O
realistic O
impulse O
responses O
adopted O
in O
this O
corpus O
can O
be O
found O
in O
. O
Additional O
experiments O
were O
conducted O
with O
the O
CHiME Material
4 O
dataset O
, O
that O
is O
based O
on O
both O
real O
and O
simulated O
data O
recorded O
in O
four O
noisy O
environments O
( O
on O
a O
bus O
, O
cafe O
, O
pedestrian O
area O
, O
and O
street O
junction O
) O
. O
The O
training O
set O
is O
composed O
of O
43690 O
noisy Material
WSJ Material
sentences Material
recored O
by O
five O
microphones O
( O
arranged O
on O
a O
tablet O
) O
and O
uttered O
by O
a O
total O
of O
87 O
speakers O
. O
The O
development O
set O
( O
DT Material
) O
is O
based O
on O
3280 O
WSJ Material
sentences Material
uttered O
by O
four O
speakers O
( O
1640 O
are O
real O
utterances O
referred O
to O
as O
DT Material
- O
real O
, O
and O
1640 O
are O
simulated O
denoted O
as O
DT Material
- O
sim O
) O
. O
The O
test O
set O
( O
ET Material
) O
is O
based O
on O
1320 O
real O
utterances O
( O
ET Material
- Material
real Material
) O
and O
1320 O
simulated O
sentences O
( O
DT Material
- O
real O
) O
from O
other O
four O
speakers O
. O
The O
experiments O
reported O
in O
this O
paper O
are O
based O
on O
the O
single Task
channel Task
setting Task
, O
in O
which O
the O
test O
phase O
is O
carried O
out O
with O
a O
single O
microphone O
( O
randomly O
selected O
from O
the O
considered O
microphone O
setup O
) O
. O
More O
information O
on O
CHiME Material
data O
can O
be O
found O
in O
. O
To O
evaluate O
the O
proposed O
model O
on O
a O
larger O
scale O
ASR Task
task O
, O
some O
additional O
experiments O
were O
performed O
with O
the O
TED Material
- Material
talk Material
dataset Material
, O
that O
was O
released O
in O
the O
context O
of O
the O
IWSLT Material
evaluation Material
campaigns Material
. O
The O
training O
set O
is O
composed O
of O
820 O
talks O
with O
a O
total O
of O
about O
166 O
hours O
of O
speech O
. O
The O
development O
test O
is O
composed O
of O
81 O
talks O
( O
16 O
hours O
) O
, O
while O
the O
test O
sets O
( O
TST Material
2011 Material
and O
TST Material
2012 Material
) O
are O
based O
on O
8 O
talks O
( O
1.5 O
hours O
) O
and O
32 O
talks O
( O
6.5 O
hours O
) O
, O
respectively O
. O
subsection O
: O
RNN Method
setting O
The O
architecture O
adopted O
for O
the O
experiments O
consisted O
of O
multiple O
recurrent Method
layers Method
, O
that O
were O
stacked O
together O
prior O
to O
the O
final O
softmax Method
classifier Method
. O
These O
recurrent Method
layers Method
were O
bidirectional O
RNNs Method
, O
which O
were O
obtained O
by O
concatenating O
the O
forward O
hidden O
states O
( O
collected O
by O
processing O
the O
sequence O
from O
the O
beginning O
to O
the O
end O
) O
with O
backward O
hidden O
states O
( O
gathered O
by O
scanning O
the O
speech O
in O
the O
reverse O
time O
order O
) O
. O
Recurrent Method
dropout Method
was O
used O
as O
regularization Method
technique Method
. O
Since O
extending O
standard O
dropout Method
to O
recurrent O
connections O
hinders O
learning Task
long Task
- Task
term Task
dependencies Task
, O
we O
followed O
the O
approach O
introduced O
in O
, O
that O
tackles O
this O
issue O
by O
sharing O
the O
same O
dropout O
mask O
across O
all O
the O
time O
steps O
. O
Moreover O
, O
batch Method
normalization Method
was O
adopted O
exploiting O
the O
method O
suggested O
in O
, O
as O
discussed O
in O
Sec O
. O
[ O
reference O
] O
. O
The O
feed O
- O
forward O
connections O
of O
the O
architecture O
were O
initialized O
according O
to O
the O
Glorot Method
’s Method
scheme Method
, O
while O
recurrent O
weights O
were O
initialized O
with O
orthogonal O
matrices O
. O
Similarly O
to O
, O
the O
gain O
factor O
of O
batch Method
normalization Method
was O
initialized O
to O
0.1 O
and O
the O
shift O
parameter O
was O
initialized O
to O
0 O
. O
Before O
training O
, O
the O
sentences O
were O
sorted O
in O
ascending O
order O
according O
to O
their O
lengths O
and O
, O
starting O
from O
the O
shortest O
utterance O
, O
minibatches O
of O
8 O
sentences O
were O
progressively O
processed O
by O
the O
training Method
algorithm Method
. O
This O
sorting Method
approach Method
minimizes O
the O
need O
of O
zero O
- O
paddings O
when O
forming O
mini O
- O
batches O
, O
resulting O
helpful O
to O
avoid O
possible O
biases O
on O
batch O
normalization O
statistics O
. O
Moreover O
, O
the O
sorting Method
approach Method
exploits O
a O
curriculum Method
learning Method
strategy Method
that O
has O
been O
shown O
to O
slightly O
improve O
the O
performance O
and O
to O
ensure O
numerical O
stability O
of O
gradients O
. O
The O
optimization Task
was O
done O
using O
the O
Adaptive Method
Moment Method
Estimation Method
( O
Adam Method
) O
algorithm O
running O
for O
22 O
epochs O
( O
35 O
for O
the O
TED Material
- Material
talk Material
corpus Material
) O
with O
, O
, O
. O
The O
performance O
on O
the O
development O
set O
was O
monitored O
after O
each O
epoch O
, O
while O
the O
learning Metric
rate Metric
was O
halved O
when O
the O
performance O
improvement O
went O
below O
a O
certain O
threshold O
( O
) O
. O
Gradient Method
truncation Method
was O
not O
applied O
, O
allowing O
the O
system O
to O
learn O
arbitrarily O
long O
time O
dependencies O
. O
The O
main O
hyperparameters O
of O
the O
model O
( O
i.e. O
, O
learning Metric
rate Metric
, O
number O
of O
hidden O
layers O
, O
hidden O
neurons O
per O
layer O
, O
dropout O
factor O
) O
were O
optimized O
on O
the O
development O
data O
. O
In O
particular O
, O
we O
guessed O
some O
initial O
values O
according O
to O
our O
experience O
, O
and O
starting O
from O
them O
we O
performed O
a O
grid Method
search Method
to O
progressively O
explore O
better O
configurations O
. O
A O
total O
of O
20 O
- O
25 O
experiments O
were O
conducted O
for O
all O
the O
various O
RNN Method
models O
. O
subsection O
: O
DNN Method
- O
HMM O
setup O
In O
the O
DNN Method
- O
HMM O
experiments O
, O
the O
DNN Method
is O
trained O
to O
predict O
context O
- O
dependent O
phone O
targets O
. O
The O
feature Method
extraction Method
is O
based O
on O
blocking O
the O
signal O
into O
frames O
of O
25 O
ms O
with O
an O
overlap O
of O
10 O
ms O
. O
The O
experimental O
activity O
is O
conducted O
considering O
different O
acoustic O
features O
, O
i.e. O
, O
39 O
MFCCs Method
( O
13 O
static O
+ O
+ O
) O
, O
40 O
log Method
- Method
mel Method
filter Method
- Method
bank Method
features Method
( O
FBANKS Method
) O
, O
as O
well O
as O
40 O
fMLLR Method
features Method
( O
extracted O
as O
reported O
in O
the O
s5 O
recipe O
of O
Kaldi Material
) O
. O
The O
labels O
were O
derived O
by O
performing O
a O
forced Method
alignment Method
procedure Method
on O
the O
original O
training O
datasets O
. O
See O
the O
standard O
s5 Method
recipe Method
of O
Kaldi Method
for O
more O
details O
. O
During O
test O
, O
the O
posterior O
probabilities O
generated O
for O
each O
frame O
by O
the O
RNN Method
are O
normalized O
by O
their O
prior O
probabilities O
. O
The O
obtained O
likelihoods O
are O
processed O
by O
an O
HMM Method
- Method
based Method
decoder Method
, O
that O
, O
after O
integrating O
the O
acoustic O
, O
lexicon O
and O
language O
model O
information O
in O
a O
single O
search Method
graph Method
, O
finally O
estimates O
the O
sequence O
of O
words O
uttered O
by O
the O
speaker O
. O
The O
RNN Method
part O
of O
the O
ASR Task
system O
was O
implemented O
with O
Theano O
, O
that O
was O
coupled O
with O
the O
Kaldi Method
decoder Method
to O
form O
a O
context O
- O
dependent O
RNN Method
- O
HMM O
speech O
recognizer O
. O
subsection O
: O
CTC Method
setup Method
The O
models O
used O
for O
the O
CTC Task
experiments Task
consisted O
of O
5 O
layers O
of O
bidirectional O
RNNs Method
of O
either O
250 O
or O
465 O
units O
. O
Unlike O
in O
the O
other O
experiments O
, O
weight O
noise O
was O
used O
for O
regularization Task
. O
The O
application O
of O
weight O
noise O
is O
a O
simplification O
of O
adaptive Method
weight Method
noise Method
and O
has O
been O
successfully O
used O
before O
to O
regularize Method
CTC Method
- Method
LSTM Method
models Method
. O
The O
weight O
noise O
was O
applied O
to O
all O
the O
weight O
matrices O
and O
sampled O
from O
a O
zero Method
mean Method
normal Method
distribution Method
with O
a O
standard O
deviation O
of O
. O
Batch Method
normalization Method
was O
used O
with O
the O
same O
initialization O
settings O
as O
in O
the O
other O
experiments O
. O
Glorot Method
’s Method
scheme Method
was O
used O
to O
initialize O
all O
the O
weights O
( O
also O
the O
recurrent O
ones O
) O
. O
The O
input O
features O
for O
these O
experiments O
were O
123 O
dimensional Method
FBANK Method
features Method
( O
40 O
+ O
energy O
+ O
+ O
) O
. O
These O
features O
were O
also O
used O
in O
the O
original O
work O
on O
CTC Method
- Method
LSTM Method
models Method
for O
speech Task
recognition Task
. O
The O
CTC Method
layer Method
itself O
was O
trained O
on O
the O
61 O
label O
set O
. O
Decoding Task
was O
done O
using O
the O
best Method
- Method
path Method
method Method
, O
without O
adding O
any O
external Method
phone Method
- Method
based Method
language Method
model Method
. O
After O
decoding O
, O
the O
labels O
were O
mapped O
to O
the O
final O
39 O
label O
set O
. O
The O
models O
were O
trained O
for O
50 O
epochs O
using O
batches O
of O
8 O
utterances O
. O
section O
: O
Results O
In O
the O
following O
sub O
- O
sections O
, O
we O
describe O
the O
experimental O
activity O
conducted O
to O
assess O
the O
proposed O
model O
. O
Most O
of O
the O
experiments O
reported O
in O
the O
following O
are O
based O
on O
hybrid O
DNN Method
- O
HMM O
speech O
recognizers O
, O
since O
the O
latter O
ASR Task
paradigm O
typically O
reaches O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
However O
, O
for O
the O
sake O
of O
comparison O
, O
we O
also O
extended O
the O
experimental O
validation O
to O
an O
end O
- O
to O
- O
end O
CTC Method
model Method
. O
More O
precisely O
, O
in O
sub O
- O
section O
[ O
reference O
] O
, O
we O
first O
quantitatively O
analyze O
the O
correlations O
between O
the O
update O
and O
reset O
gates O
in O
a O
standard O
GRU Method
. O
In O
sub O
- O
section O
[ O
reference O
] O
, O
we O
extend O
our O
study O
with O
some O
analysis O
of O
gradient Method
statistics Method
. O
The O
role O
of O
batch Method
normalization Method
and O
the O
CTC Method
experiments Method
are O
described O
in O
sub O
- O
sections O
[ O
reference O
] O
and O
[ O
reference O
] O
, O
respectively O
. O
The O
speech Task
recognition Task
performance O
will O
then O
be O
reported O
for O
TIMIT Material
, O
DIRHA Material
- Material
English Material
, O
CHiME Material
as O
well O
as O
for O
the O
TED Material
- Material
talk Material
corpus Material
in O
sub O
- O
sections O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
, O
respectively O
. O
The O
computational Metric
benefits O
of O
Li Method
- Method
GRU Method
are O
finally O
discussed O
in O
sub O
- O
section O
[ O
reference O
] O
. O
subsection O
: O
Correlation Method
analysis Method
The O
correlation O
between O
update O
and O
reset O
gates O
have O
been O
preliminarily O
discussed O
in O
Sec O
. O
[ O
reference O
] O
. O
In O
this O
sub O
- O
section O
, O
we O
take O
a O
step O
further O
by O
analyzing O
it O
in O
a O
quantitative O
way O
using O
the O
cross O
- O
correlation O
function O
defined O
in O
Eq O
. O
[ O
reference O
] O
. O
In O
particular O
, O
the O
cross Metric
- Metric
correlation Metric
between O
the O
average O
activations O
of O
update O
and O
reset O
gates O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
The O
gate O
activations O
are O
computed O
for O
all O
the O
input O
frames O
and O
, O
at O
each O
time O
step O
, O
an O
average O
over O
the O
hidden O
neurons O
is O
considered O
. O
The O
cross Metric
- Metric
correlation Metric
is O
displayed O
along O
with O
the O
auto Metric
- Metric
correlation Metric
, O
that O
represents O
the O
upper O
- O
bound O
limit O
of O
the O
former O
function O
. O
Fig O
. O
[ O
reference O
] O
clearly O
shows O
a O
high O
peak O
of O
, O
revealing O
that O
update O
and O
reset O
gates O
end O
up O
being O
redundant O
. O
This O
peak O
is O
about O
66 O
% O
of O
the O
maximum O
of O
and O
it O
is O
centered O
at O
, O
indicating O
that O
almost O
no O
- O
delay O
occurs O
between O
gate O
activations O
. O
This O
result O
is O
obtained O
with O
a O
single O
- O
layer O
GRU Method
of O
200 O
bidirectional O
neurons O
fed O
with O
MFCC O
features O
and O
trained O
with O
TIMIT Material
. O
After O
the O
training O
- O
step O
, O
the O
cross Metric
- Metric
correlation Metric
is O
averaged O
over O
all O
the O
development O
sentences O
. O
It O
would O
be O
of O
interest O
to O
examine O
the O
evolution O
of O
this O
correlation O
over O
the O
epochs O
. O
With O
this O
regard O
, O
Fig O
. O
[ O
reference O
] O
reports O
the O
peak O
of O
for O
some O
training O
epochs O
, O
showing O
that O
the O
GRU Method
attributes O
rather O
quickly O
a O
similar O
role O
to O
update O
and O
reset O
gates O
. O
In O
fact O
, O
after O
3 O
- O
4 O
epochs O
, O
the O
correlation O
peak O
reaches O
its O
maximum O
value O
, O
that O
is O
almost O
maintained O
for O
all O
the O
subsequent O
training O
iterations O
. O
subsection O
: O
Gradient Method
analysis Method
The O
analysis O
of O
the O
main O
gradient Metric
statistics Metric
can O
give O
some O
preliminary O
indications O
on O
the O
role O
played O
by O
the O
various O
parameters O
. O
With O
this O
goal O
, O
Table O
[ O
reference O
] O
reports O
the O
L2 Metric
norm Metric
of O
the O
gradient O
for O
the O
main O
parameters O
of O
the O
considered O
GRU Method
models O
. O
Results O
reveal O
that O
the O
reset O
gate O
weight O
matrices O
( O
i.e. O
, O
and O
) O
have O
a O
smaller O
gradient Metric
norm Metric
when O
compared O
to O
the O
other O
parameters O
. O
This O
result O
is O
somewhat O
expected O
, O
since O
the O
reset O
gate O
parameters O
are O
processed O
by O
two O
different O
non O
- O
linearities O
( O
i.e. O
, O
the O
sigmoid O
of O
Eq O
. O
[ O
reference O
] O
and O
the O
tanh Method
of O
[ O
reference O
] O
) O
, O
that O
can O
attenuate O
their O
gradients O
. O
This O
would O
anyway O
indicate O
that O
, O
on O
average O
, O
the O
reset O
gate O
has O
less O
impact O
on O
the O
final O
cost Metric
function Metric
, O
further O
supporting O
its O
removal O
from O
the O
GRU Method
design O
. O
When O
avoiding O
it O
, O
the O
norm O
of O
the O
gradient O
tends O
to O
increase O
( O
see O
for O
instance O
the O
recurrent Method
weights Method
of Method
M Method
- Method
GRU Method
model Method
) O
. O
This O
suggests O
that O
the O
functionalities O
of O
the O
reset O
gate O
, O
can O
be O
performed O
by O
other O
model Method
parameters Method
. O
The O
norm O
further O
increases O
in O
the O
case O
of O
Li O
- O
GRUs Method
, O
due O
to O
the O
adoption O
of O
ReLu O
units O
. O
This O
non O
- O
linearity O
, O
indeed O
, O
improves O
the O
back Method
- Method
propagation Method
of O
the O
gradient O
over O
both O
time O
- O
steps O
and O
hidden O
layers O
, O
making O
long O
- O
term O
dependencies O
easier O
to O
learn O
. O
Results O
are O
obtained O
with O
the O
same O
GRU Method
used O
in O
subsection O
[ O
reference O
] O
, O
considering O
M Method
- Method
GRU Method
and O
Li Method
- Method
GRU Method
models Method
with O
the O
same O
number O
of O
hidden O
units O
. O
subsection O
: O
Role O
of O
batch Method
normalization Method
After O
the O
preliminary O
analysis O
on O
correlation Metric
and O
gradients O
done O
in O
previous O
sub O
- O
sections O
, O
we O
now O
compare O
RNN Method
models O
in O
terms O
of O
their O
final O
speech Task
recognition Task
performance O
. O
To O
highlight O
the O
importance O
of O
batch Method
normalization Method
, O
Table O
[ O
reference O
] O
compares O
the O
Phone Metric
Error Metric
Rate Metric
( O
PER% Metric
) O
achieved O
with O
and O
without O
this O
technique O
. O
Results O
show O
that O
batch Method
normalization Method
is O
helpful O
to O
improve O
the O
ASR Task
performance O
, O
leading O
to O
a O
relative O
improvement O
of O
about O
7 O
% O
for O
GRU Method
and O
M O
- O
GRU Method
and O
18 O
% O
for O
the O
proposed O
Li Method
- Method
GRU Method
. O
The O
latter O
improvement O
confirms O
that O
our O
model O
couples O
particularly O
well O
with O
this O
technique O
, O
due O
to O
the O
adopted O
ReLU Method
activations O
. O
Without O
batch Method
normalization Method
, O
the O
ReLU Method
activations O
of O
the O
Li Method
- Method
GRU Method
are O
unbounded O
and O
tend O
to O
cause O
numerical O
instabilities O
. O
According O
to O
our O
experience O
, O
the O
convergence O
of O
Li Method
- Method
GRU Method
without O
batch Method
normalization Method
, O
can O
be O
achieved O
only O
by O
setting O
rather O
small O
learning O
rate O
values O
. O
The O
latter O
setting O
, O
however O
, O
can O
lead O
to O
a O
poor O
performance O
and O
, O
as O
clearly O
emerged O
from O
this O
experiment O
, O
coupling O
Li Method
- Method
GRU Method
with O
this O
technique O
is O
strongly O
recommended O
. O
subsection O
: O
CTC O
results O
Table O
[ O
reference O
] O
summarizes O
the O
results O
of O
CTC Method
on O
the O
TIMIT Material
data O
set O
. O
In O
these O
experiments O
, O
the O
Li Method
- Method
GRU Method
clearly O
outperforms O
the O
standard O
GRU Method
, O
showing O
the O
effectiveness O
of O
the O
proposed O
model O
even O
in O
a O
end O
- O
to O
- O
end O
ASR Task
framework O
. O
The O
improvement O
is O
obtained O
both O
with O
and O
without O
batch Method
normalization Method
and O
, O
similarly O
to O
what O
observed O
for O
hybrid Method
systems Method
, O
the O
latter O
technique O
leads O
to O
better O
performance O
when O
coupled O
with O
Li Method
- Method
GRU Method
. O
However O
, O
a O
smaller O
performance O
gain O
is O
observed O
when O
batch Method
normalization Method
is O
applied O
to O
the O
CTC Method
. O
This O
result O
could O
also O
be O
related O
to O
the O
different O
choice O
of O
the O
regularizer Method
, O
as O
weight O
noise O
was O
used O
instead O
of O
recurrent Method
dropout Method
. O
In O
general O
, O
PERs O
are O
higher O
than O
those O
of O
the O
hybrid Method
systems Method
. O
End Method
- Method
to Method
- Method
end Method
methods Method
, O
in O
fact O
, O
are O
relatively O
young O
models O
, O
that O
are O
currently O
still O
less O
competitive O
than O
more O
complex O
( O
and O
mature O
) O
DNN Method
- O
HMM O
approaches O
. O
We O
believe O
that O
the O
gap O
between O
CTC Method
and Method
hybrid Method
speech Method
recognizers Method
could O
be O
partially O
reduced O
in O
our O
experiments O
with O
a O
more O
careful O
setting O
of O
the O
hyperparameters O
and O
with O
the O
introduction O
of O
an O
external O
phone Method
- Method
based Method
language Method
model Method
. O
The O
main O
focus O
of O
the O
paper O
, O
however O
, O
is O
to O
show O
the O
effectiveness O
of O
the O
proposed O
Li O
- O
GRU Method
model O
, O
and O
a O
fair O
comparison O
between O
CTC Method
and O
hybrid Method
systems Method
is O
out O
of O
the O
scope O
of O
this O
work O
. O
subsection O
: O
Other O
results O
on O
TIMIT Material
The O
results O
of O
Table O
[ O
reference O
] O
and O
[ O
reference O
] O
highlighted O
that O
the O
proposed O
Li O
- O
GRU Method
model O
outperforms O
other O
GRU Method
architectures O
. O
In O
this O
sub O
- O
section O
, O
we O
extend O
this O
study O
by O
performing O
a O
more O
detailed O
comparison O
with O
the O
most O
popular O
RNN Method
architectures O
. O
To O
provide O
a O
fair O
comparison O
, O
batch Method
normalization Method
is O
hereinafter O
applied O
to O
all O
the O
considered O
RNN Method
models O
. O
Moreover O
, O
at O
least O
five O
experiments O
varying O
the O
initialization O
seeds O
were O
conducted O
for O
each O
RNN Method
architecture O
. O
The O
results O
are O
thus O
reported O
as O
the O
average O
PER Metric
with O
their O
corresponding O
standard O
deviation O
. O
Table O
[ O
reference O
] O
presents O
the O
ASR Task
performance O
obtained O
with O
the O
TIMIT Material
dataset O
. O
The O
first O
row O
reports O
the O
results O
achieved O
with O
a O
simple O
RNN Method
with O
ReLU Method
activations O
( O
no O
gating Method
mechanisms Method
are O
used O
here O
) O
. O
Although O
this O
architecture O
has O
recently O
shown O
promising O
results O
in O
some O
machine Task
learning Task
tasks Task
, O
our O
results O
confirm O
that O
gated Method
recurrent Method
networks Method
( O
rows O
2 O
- O
5 O
) O
outperform O
traditional O
RNNs Method
. O
We O
also O
observe O
that O
GRUs Method
tend O
to O
slightly O
outperform O
the O
LSTM Method
model Method
. O
As O
expected O
, O
M Method
- Method
GRU Method
( O
i.e. O
, O
the O
architecture O
without O
reset O
gate O
) O
achieves O
a O
performance O
very O
similar O
to O
that O
obtained O
with O
standard O
GRUs Method
, O
further O
supporting O
our O
speculation O
on O
the O
redundant O
role O
played O
by O
the O
reset O
gate O
in O
a O
speech Task
recognition Task
application Task
. O
The O
last O
row O
reports O
the O
performance O
achieved O
with O
the O
proposed O
model O
, O
in O
which O
, O
besides O
removing O
the O
reset O
gate O
, O
ReLU Method
activations O
are O
used O
. O
The O
Li Method
- Method
GRU Method
performance O
indicates O
that O
our O
architecture O
consistently O
outperforms O
the O
other O
RNNs Method
over O
all O
the O
considered O
input O
features O
. O
A O
remarkable O
achievement O
is O
the O
average O
PER Metric
( O
% O
) O
of O
% O
obtained O
with O
fMLLR Method
features Method
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
result O
yields O
the O
best O
published O
performance O
on O
the O
TIMIT Material
test O
- O
set O
. O
In O
Table O
[ O
reference O
] O
the O
PER Metric
( O
% O
) O
performance O
is O
split O
into O
five O
different O
phonetic O
categories O
( O
vowels O
, O
liquids O
, O
nasals O
, O
fricatives O
and O
stops O
) O
, O
showing O
that O
Li Method
- Method
GRU Method
exhibits O
the O
best O
results O
for O
all O
the O
considered O
classes O
. O
Previous O
results O
are O
obtained O
after O
optimizing O
the O
main O
hyperparameters Method
of O
the O
model O
on O
the O
development O
set O
. O
Table O
[ O
reference O
] O
reports O
the O
outcome O
of O
this O
optimization Method
process Method
, O
with O
the O
corresponding O
best O
architectures O
obtained O
for O
each O
RNN Method
architecture O
. O
For O
GRU Method
models O
, O
the O
best O
performance O
is O
achieved O
with O
5 O
hidden O
layers O
of O
465 O
neurons O
. O
It O
is O
also O
worth O
noting O
that O
M Method
- Method
GRU Method
and O
Li Method
- Method
GRU Method
have O
about O
30 O
% O
fewer O
parameters O
compared O
to O
the O
standard O
GRU Method
. O
subsection O
: O
Recognition Task
performance O
on O
DIRHA Material
English Material
WSJ Material
After O
a O
first O
set O
of O
experiments O
on O
TIMIT Material
, O
in O
this O
sub O
- O
section O
we O
assess O
our O
model O
on O
a O
more O
challenging O
and O
realistic O
distant Task
- Task
talking Task
task Task
, O
using O
the O
DIRHA Material
English Material
WSJ Material
corpus Material
. O
A O
challenging O
aspect O
of O
this O
dataset O
is O
the O
acoustic O
mismatch O
between O
training O
and O
testing O
conditions O
. O
Training Task
, O
in O
fact O
, O
is O
performed O
with O
a O
reverberated Material
version Material
of Material
WSJ Material
, O
while O
test O
is O
characterized O
by O
both O
non O
- O
stationary O
noises O
and O
reverberation O
. O
Tables O
[ O
reference O
] O
and O
[ O
reference O
] O
summarize O
the O
results O
obtained O
with O
the O
simulated O
and O
real O
parts O
of O
this O
dataset O
. O
These O
results O
exhibit O
a O
trend O
comparable O
to O
that O
observed O
for O
TIMIT Material
, O
confirming O
that O
Li Method
- Method
GRU Method
still O
outperform O
GRU Method
even O
in O
a O
more O
challenging O
scenario O
. O
The O
results O
are O
consistent O
over O
both O
real O
and O
simulated O
data O
as O
well O
as O
across O
the O
different O
features O
considered O
in O
this O
study O
. O
The O
reset Method
gate Method
removal Method
seems O
to O
play O
a O
more O
crucial O
role O
in O
the O
addressed O
distant Task
- Task
talking Task
scenario Task
. O
If O
the O
close O
- O
talking O
performance O
reported O
in O
Table O
[ O
reference O
] O
highlights O
comparable O
error Metric
rates Metric
between O
standard O
GRU Method
and O
M Method
- Method
GRU Method
, O
in O
the O
distant Task
- Task
talking Task
case Task
we O
even O
observe O
a O
small O
performance O
gain O
when O
removing O
the O
reset O
gate O
. O
We O
suppose O
that O
this O
behaviour O
is O
due O
to O
reverberation O
, O
that O
implicitly O
introduces O
redundancy O
in O
the O
signal O
, O
due O
to O
the O
multiple O
delayed O
replicas O
of O
each O
sample O
. O
This O
results O
in O
a O
forward O
memory O
effect O
, O
that O
can O
make O
reset O
gate O
ineffective O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
extend O
our O
previous O
experiments O
by O
generating O
simulated O
data O
with O
different O
reverberation O
times O
ranging O
from O
250 O
to O
1000 O
ms O
, O
as O
outlined O
in O
Sec O
. O
[ O
reference O
] O
. O
In O
order O
to O
simulate O
a O
more O
realistic O
situation O
, O
different O
impulse O
responses O
have O
been O
used O
for O
training O
and O
testing O
purposes O
. O
No O
additive O
noise O
is O
considered O
for O
these O
experiments O
. O
As O
expected O
, O
the O
performance O
degrades O
as O
the O
reverberation O
time O
increases O
. O
Similarly O
to O
previous O
achievements O
, O
we O
still O
observe O
that O
Li Method
- Method
GRU Method
outperform O
GRU Method
under O
all O
the O
considered O
reverberation O
conditions O
. O
subsection O
: O
Recognition Task
performance O
on O
CHiME Material
In O
this O
sub O
- O
section O
we O
extend O
the O
results O
to O
the O
CHiME Material
corpus O
, O
that O
is O
an O
important O
benchmark O
in O
the O
ASR Task
field Task
, O
thanks O
to O
the O
success O
of O
CHiME Material
challenges O
. O
In O
Tab O
. O
[ O
reference O
] O
a O
comparison O
across O
the O
various O
GRU Method
architectures O
is O
presented O
. O
For O
the O
sake O
of O
comparison O
, O
the O
results O
obtained O
with O
the O
official O
CHiME Material
4 O
are O
also O
reported O
in O
the O
first O
two O
rows O
. O
Results O
confirm O
the O
trend O
previously O
observed O
, O
highlighting O
a O
significant O
relative O
improvement O
of O
about O
14 O
% O
achieved O
when O
passing O
from O
GRU Method
to O
the O
proposed O
Li Method
- Method
GRU Method
. O
Similarly O
to O
our O
findings O
of O
the O
previous O
section O
, O
some O
small O
benefits O
can O
be O
observed O
when O
removing O
the O
reset O
gate O
. O
The O
largest O
performance O
gap O
, O
however O
, O
is O
reached O
when O
adopting O
ReLU Method
units O
( O
see O
M O
- O
GRU Method
and O
Li Method
- Method
GRU Method
columns O
) O
, O
confirming O
the O
effectiveness O
of O
this O
architectural Method
variation Method
. O
Note O
also O
that O
the O
GRU Method
systems O
significantly O
outperform O
the O
DNN Method
baseline O
, O
even O
when O
the O
latter O
is O
based O
on O
sequence Method
discriminative Method
training Method
( O
DNN Method
+ O
sMBR O
) O
. O
Tab O
. O
[ O
reference O
] O
splits O
the O
ASR Task
performance O
of O
the O
real O
test O
set O
into O
the O
four O
noisy O
categories O
. O
Li Method
- Method
GRU Method
outperforms O
GRU Method
in O
all O
the O
considered O
environments O
, O
with O
a O
performance O
gain O
that O
is O
higher O
when O
more O
challenging O
acoustic O
conditions O
are O
met O
. O
For O
instance O
, O
we O
obtain O
a O
relative O
improvement O
of O
16 O
% O
in O
the O
bus O
( O
BUS O
) O
environment O
( O
the O
noisiest O
) O
, O
against O
the O
relative O
improvement O
of O
9.5 O
% O
observed O
in O
the O
street O
( O
STR O
) O
recordings O
. O
subsection O
: O
Recognition Task
performance O
on O
TED Material
- Material
talks Material
Tab O
. O
[ O
reference O
] O
reports O
a O
comparison O
between O
GRU Method
and O
Li Method
- Method
GRU Method
on O
the O
TED Material
- Material
talks Material
corpus Material
. O
The O
experiments O
are O
performed O
with O
standard O
MFCC O
features O
, O
and O
a O
four Method
- Method
gram Method
language Method
model Method
is O
considered O
in O
the O
decoding Task
step Task
( O
see O
for O
more O
details O
) O
. O
Results O
on O
both O
test O
sets O
consistently O
shows O
the O
performance O
gain O
achieved O
with O
the O
proposed O
architecture O
. O
This O
further O
confirms O
the O
effectiveness O
of O
Li Method
- Method
GRU Method
, O
even O
for O
a O
larger O
scale O
ASR Task
task O
. O
In O
particular O
, O
a O
relative O
improvement O
of O
about O
14 O
- O
17 O
% O
is O
achieved O
. O
This O
improvement O
is O
statistically O
significant O
according O
to O
the O
Matched Method
Pairs Method
Sentence Method
Segment Method
Word Method
Error Method
Test Method
( O
MPSSW Method
) O
, O
that O
is O
conducted O
with O
NIST Method
sclite Method
t Method
tool Method
with O
a O
p Metric
- Metric
value Metric
of O
. O
subsection O
: O
Training Metric
time Metric
comparison Metric
In O
the O
previous O
subsections O
, O
we O
reported O
several O
speech Task
recognition Task
results O
, O
showing O
that O
Li Method
- Method
GRU Method
outperforms O
other O
RNNs Method
. O
In O
this O
sub O
- O
section O
, O
we O
finally O
focus O
on O
another O
key O
aspect O
of O
the O
proposed O
architecture O
, O
namely O
its O
improved O
computational Metric
efficiency Metric
. O
In O
Table O
[ O
reference O
] O
, O
we O
compare O
the O
per Metric
- Metric
epoch Metric
wall Metric
- Metric
clock Metric
training Metric
time Metric
of O
GRU Method
and O
Li Method
- Method
GRU Method
models Method
. O
The O
training Metric
time Metric
reduction Metric
achieved O
with O
the O
proposed O
architecture O
is O
about O
30 O
% O
for O
all O
the O
datasets O
. O
This O
reduction O
reflects O
the O
amount O
of O
parameters O
saved O
by O
Li Method
- Method
GRU Method
, O
that O
is O
also O
around O
30 O
% O
. O
The O
reduction O
of O
the O
computational Metric
complexity Metric
, O
originated O
by O
a O
more O
compact O
model O
, O
also O
arises O
for O
testing O
purposes O
, O
making O
our O
model O
potentially O
suitable O
for O
small O
- O
footprint O
ASR Task
, O
, O
which O
studies O
DNNs Method
designed O
for O
portable Task
devices Task
with O
small O
computational O
capabilities O
. O
section O
: O
Conclusions O
In O
this O
paper O
, O
we O
revised O
standard O
GRUs Method
for O
speech Task
recognition Task
purposes Task
. O
The O
proposed O
Li O
- O
GRU Method
architecture O
is O
a O
simplified O
version O
of O
a O
standard O
GRU Method
, O
in O
which O
the O
reset O
gate O
is O
removed O
and O
ReLU Method
activations O
are O
considered O
. O
Batch Method
normalization Method
is O
also O
used O
to O
further O
improve O
the O
system O
performance O
as O
well O
as O
to O
limit O
the O
numerical O
instabilities O
originated O
from O
ReLU Method
non O
- O
linearities O
. O
The O
experiments O
, O
conducted O
on O
different O
ASR Task
paradigms O
, O
tasks O
, O
features O
and O
environmental O
conditions O
, O
have O
confirmed O
the O
effectiveness O
of O
the O
proposed O
model O
. O
The O
Li Method
- Method
GRU Method
, O
in O
fact O
, O
not O
only O
yields O
a O
better O
recognition Task
performance O
, O
but O
also O
reduces O
the O
computational Metric
complexity Metric
, O
with O
a O
reduction O
of O
more O
than O
30 O
% O
of O
the O
training Metric
time Metric
over O
a O
standard O
GRU Method
. O
Future O
efforts O
will O
be O
focused O
on O
extending O
this O
work O
to O
other O
speech Task
- Task
based Task
tasks Task
, O
such O
as O
speech Task
enhancement Task
and O
speech Task
separation Task
as O
well O
as O
to O
explore O
the O
use O
of O
Li Method
- Method
GRU Method
in O
other O
possible O
fields O
. O
section O
: O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
Piergiorgio O
Svaizer O
for O
his O
insightful O
suggestions O
on O
an O
earlier O
version O
of O
this O
paper O
. O
We O
would O
also O
thank O
the O
anonymous O
reviewers O
for O
their O
careful O
reading O
of O
our O
manuscript O
and O
their O
helpful O
comments O
. O
We O
gratefully O
acknowledge O
the O
support O
of O
NVIDIA O
Corporation O
with O
the O
donation O
of O
a O
Tesla O
K40 O
GPU O
used O
for O
this O
research O
. O
Computations O
were O
also O
made O
on O
the O
Helios O
supercomputer O
from O
the O
University O
of O
Montreal O
, O
managed O
by O
Calcul O
QuÃ O
© O
bec O
and O
Compute O
Canada O
. O
bibliography O
: O
References O
