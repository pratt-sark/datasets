Parameter O
Re O
- O
Initialization Task
through O
Cyclical Method
Batch Method
Size Method
Schedules Method
section O
: O
Abstract O
Optimal Task
parameter Task
initialization Task
remains O
a O
crucial O
problem O
for O
neural Task
network Task
training Task
. O
A O
poor O
weight Method
initialization Method
may O
take O
longer O
to O
train O
and O
/ O
or O
converge O
to O
sub O
- O
optimal O
solutions O
. O
Here O
, O
we O
propose O
a O
method O
of O
weight Task
re Task
- Task
initialization Task
by O
repeated Method
annealing Method
and O
injection Task
of Task
noise Task
in O
the O
training Task
process Task
. O
We O
implement O
this O
through O
a O
cyclical Method
batch Method
size Method
schedule Method
motivated O
by O
a O
Bayesian Method
perspective Method
of Method
neural Method
network Method
training Method
. O
We O
evaluate O
our O
methods O
through O
extensive O
experiments O
on O
tasks O
in O
language Task
modeling Task
, O
natural Task
language Task
inference Task
, O
and O
image Task
classification Task
. O
We O
demonstrate O
the O
ability O
of O
our O
method O
to O
improve O
language Task
modeling Task
performance O
by O
up O
to O
7.91 O
perplexity Metric
and O
reduce O
training Metric
iterations Metric
by O
up O
to O
61 O
% O
, O
in O
addition O
to O
its O
flexibility O
in O
enabling O
snapshot O
ensembling Method
and O
use O
with O
adversarial Method
training Method
. O
section O
: O
Introduction O
Despite O
many O
promising O
empirical O
results O
at O
using O
stochastic Method
optimization Method
methods Method
to O
train O
highly O
non Method
- Method
convex Method
modern Method
deep Method
neural Method
networks Method
, O
we O
still O
lack O
theoretically O
robust O
practical O
methods O
which O
are O
able O
to O
escape O
saddle O
points O
and O
/ O
or O
sub O
- O
optimal O
local O
minima O
and O
converge O
to O
parameters O
that O
retain O
high O
testing O
performance O
. O
This O
lack O
of O
understanding O
leads O
to O
practical O
training O
challenges O
. O
Stochastic Method
Gradient Method
Descent Method
( O
SGD Method
) Method
is O
currently O
the O
de Method
- Method
facto Method
optimization Method
method Method
for O
training O
deep Method
neural Method
networks Method
( O
DNNs Method
) O
. O
Through O
extensive O
hyper Method
- Method
parameter Method
tuning Method
, O
SGD Method
can O
avoid O
poor O
local O
optima O
and O
achieve O
good O
generalization Metric
ability Metric
. O
One O
important O
hyper O
- O
parameter O
that O
can O
significantly O
affect O
SGD Task
performance O
is O
the O
weight Method
initialization Method
. O
For O
instance O
, O
initializing O
the O
weights O
to O
all O
zeros O
or O
all O
ones O
leads O
to O
extremely O
poor O
performance O
[ O
reference O
] O
. O
Different O
approaches O
have O
been O
proposed O
for O
weight Task
initialization Task
such O
as O
Xavier O
, O
MSRA Method
, O
Ortho O
, O
LSUV Method
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
These O
are O
mostly O
agnostic O
to O
the O
model Method
architecture Method
and O
the O
specific O
learning Task
task Task
. O
Our O
work O
explores O
the O
idea O
of O
adapting O
the O
weight Method
initialization Method
to O
the O
optimization Task
dynamics Task
of Task
the Task
specific Task
learning Task
task Task
at O
hand O
. O
From O
the O
Bayesian Method
perspective Method
, O
improved O
weight Method
initialization Method
can O
be O
viewed O
as O
starting O
with O
a O
better O
prior O
, O
which O
leads O
to O
a O
more O
accurate O
posterior O
and O
thus O
better O
generalization Metric
ability Metric
. O
This O
problem O
has O
been O
explored O
extensively O
in O
Bayesian Task
optimization Task
. O
For O
example O
, O
in O
the O
seminal O
works O
[ O
reference O
][ O
reference O
] O
, O
an O
adaptive Method
prior Method
is O
implemented O
via O
Markov Method
Chain Method
Monte Method
Carlo Method
( O
MCMC Method
) O
methods O
. O
Motivated O
by O
these O
ideas O
, O
we O
incorporate O
an O
" O
adaptive Method
initialization Method
" O
for O
neural Task
network Task
training Task
( O
see O
section O
2 O
for O
details O
) O
, O
where O
we O
use O
cyclical Method
batch Method
size Method
schedules Method
to O
control O
the O
noise O
( O
or O
temperature O
) O
of O
SGD Method
. O
As O
argued O
in O
[ O
reference O
] O
, O
both O
learning Metric
rate Metric
and O
batch Metric
size Metric
can O
be O
used O
to O
control O
the O
noise O
of O
SGD Method
but O
the O
latter O
has O
an O
advantage O
in O
that O
it O
allows O
more O
parallelization O
opportunity O
[ O
reference O
] O
. O
The O
idea O
of O
using O
batch O
size O
to O
control O
the O
noise O
in O
a O
simple O
cyclical Method
schedule Method
was O
recently O
proposed O
in O
[ O
reference O
] O
. O
Here O
, O
we O
build O
upon O
this O
work O
by O
studying O
different O
cyclical Method
annealing Method
strategies Method
for O
a O
wide O
range O
of O
problems O
. O
Additionally O
, O
we O
discuss O
how O
this O
can O
be O
combined O
with O
a O
new O
adversarial Method
regularization Method
scheme Method
recently O
proposed O
in O
[ O
reference O
] O
, O
as O
well O
as O
prior O
work O
[ O
reference O
] O
in O
order O
to O
obtain O
ensembles O
of O
models O
at O
no O
additional O
cost O
. O
In O
summary O
, O
our O
contributions O
are O
as O
follows O
: O
• O
We O
explore O
different O
cyclical O
batch O
size O
( O
CBS Method
) O
schedules O
for O
training O
neural Method
networks Method
inspired O
by O
Bayesian Method
statistics Method
, O
particularly O
adaptive O
MCMC Method
methods O
. O
The O
CBS Method
schedule O
leads O
to O
multiple O
perplexity Metric
improvement Metric
( O
up O
to O
7.91 O
) O
in O
language Task
modeling Task
and O
minor O
improvements O
in O
natural Task
language Task
inference Task
and O
image Task
classification Task
. O
Furthermore O
, O
we O
show O
that O
CBS Method
schedule O
can O
alleviate O
problems O
with O
overfitting O
and O
sub Task
- Task
optimal Task
parameter Task
initialization Task
. O
• O
Additionally O
, O
CBS Method
schedules O
require O
up O
to O
3× O
fewer O
SGD O
iterations O
due O
to O
larger O
batch O
sizes O
, O
which O
allows O
for O
more O
parallelization O
opportunity O
. O
This O
reflects O
the O
benefit O
of O
cycling O
the O
batch O
size O
instead O
of O
the O
learning O
rate O
as O
in O
prior O
work O
[ O
reference O
][ O
reference O
] O
• O
We O
showcase O
the O
flexibility O
of O
CBS Method
schedules O
for O
use O
with O
additional O
techniques O
. O
We O
propose O
a O
simple O
but O
effective O
ensembling Method
method O
that O
combines O
models O
saved O
during O
different O
cycles O
at O
no O
additional O
training Metric
cost Metric
. O
In O
addition O
, O
we O
show O
that O
CBS Method
schedule O
can O
be O
combined O
with O
other O
approaches O
such O
as O
the O
recently O
proposed O
adversarial Method
regularization Method
[ O
reference O
] O
to O
yield O
further O
classification Metric
accuracy Metric
improvement Metric
of O
0.26 O
% O
. O
section O
: O
Related O
Work O
[ O
5 O
] O
introduced O
Xavier Method
initialization Method
, O
which O
keeps O
the O
variance O
of O
input O
and O
output O
of O
all O
layers O
within O
a O
similar O
range O
in O
order O
to O
prevent O
vanishing O
or O
exploding O
values O
in O
both O
the O
forward O
and O
backward O
passes O
. O
Building O
off O
this O
idea O
, O
[ O
reference O
] O
explored O
a O
new O
strategy O
known O
as O
MSRA Method
to O
keep O
the O
variance O
constant O
for O
all O
convolutional Method
layers Method
. O
[ O
reference O
] O
proposed O
an O
orthogonal Method
initialization Method
( O
Ortho Method
) O
to O
achieve O
faster O
convergence Metric
, O
and O
more O
recently O
, O
[ O
reference O
] O
combined O
ideas O
from O
previous O
work O
and O
showed O
that O
a O
unit Method
variance Method
orthogonal Method
initialization Method
is O
beneficial O
for O
deep Method
models Method
. O
[ O
reference O
][ O
reference O
][ O
reference O
] O
show O
that O
the O
noise O
of O
SGD Method
is O
controlled O
by O
the O
ratio Metric
of Metric
learning Metric
rate Metric
to O
batch O
size O
. O
The O
authors O
argued O
that O
the O
SGD Method
algorithm Method
can O
be O
derived O
through O
Euler O
- O
Maruyama O
discretization O
of O
a O
Stochastic Method
Differential Method
Equation Method
( O
SDE Method
) O
. O
The O
SDE Method
dynamics O
are O
governed O
by O
a O
" O
noise O
scale O
" O
g O
≈ O
N O
/ O
B O
for O
the O
learning Metric
rate Metric
, O
N O
the O
training O
dataset O
size O
, O
and O
B O
the O
batch O
size O
. O
They O
conclude O
that O
a O
higher O
noise O
scale O
prevents O
SGD Method
from O
settling O
into O
sharper O
minima O
. O
This O
result O
supports O
a O
prior O
empirical O
observation O
[ O
reference O
] O
that O
under O
certain O
mild O
assumptions O
such O
as O
N O
B O
, O
the O
effect O
of O
dividing O
the O
learning Metric
rate Metric
by O
a O
constant O
factor O
is O
equivalent O
to O
that O
of O
multiplying O
the O
batch O
size O
by O
the O
same O
constant O
factor O
. O
In O
related O
work O
, O
[ O
reference O
] O
applied O
this O
understanding O
and O
used O
batch O
size O
as O
a O
knob O
to O
control O
the O
noise O
, O
and O
empirically O
showed O
that O
the O
baseline O
performance O
could O
be O
matched O
. O
[ O
reference O
] O
further O
explored O
how O
to O
use O
second O
- O
order O
information O
and O
adversarial Method
training Method
to O
control O
the O
noise O
for O
training Task
large Task
batch Task
size Task
. O
[ O
reference O
][ O
reference O
] O
showed O
using O
a O
statistical Method
mechanics Method
argument Method
that O
many O
other O
hyper Method
- Method
parameters Method
in O
neural Method
network Method
training Method
, O
e.g. O
data Metric
quality Metric
, O
can O
also O
act O
as O
temperature O
knobs O
. O
section O
: O
Methods O
The O
goal O
of O
neural Method
network Method
optimization Method
is O
to O
solve O
an O
empirical Task
risk Task
minimization Task
, O
with O
a O
loss O
function O
of O
the O
form O
: O
where O
θ O
is O
the O
model O
parameters O
, O
X O
is O
the O
training O
dataset O
and O
l O
( O
x O
, O
θ O
) O
is O
the O
loss O
function O
. O
Here O
N O
= O
|X| O
is O
the O
cardinality O
of O
the O
training O
set O
. O
In O
SGD Method
, O
a O
mini O
- O
batch O
, O
B O
⊂ O
{ O
1 O
, O
2 O
, O
... O
, O
N O
} O
is O
used O
to O
compute O
an O
( O
unbiased O
) O
gradient O
, O
i.e. O
, O
g O
t O
= O
1 O
|B| O
x∈B O
∇ O
θ O
l O
( O
x O
, O
θ O
t O
) O
, O
and O
this O
is O
typically O
used O
to O
optimize O
( O
1 O
) O
in O
the O
form O
: O
where O
η O
t O
is O
the O
learning Metric
rate Metric
( O
step O
size O
) O
at O
iteration O
t O
, O
and O
commonly O
annealed O
during O
training O
. O
By O
Bayes Method
' Method
Theorem Method
, O
given O
the O
input O
data O
, O
X O
, O
a O
prior O
distribution O
on O
the O
model O
parameters O
, O
P O
( O
θ O
) O
, O
and O
a O
likelihood O
function O
, O
P O
( O
X|θ O
) O
, O
the O
posterior O
distribution O
, O
P O
( O
θ|X O
) O
, O
is O
: O
From O
this O
Bayesian Method
perspective Method
, O
the O
goal O
of O
the O
neural Method
network Method
training Method
is O
to O
find O
the O
Maximum Task
- Task
APosteriori Task
( Task
MAP Task
) Task
point Task
for O
a O
given O
prior O
distribution O
. O
Note O
that O
in O
this O
context O
weight Task
initialization Task
and O
prior O
distribution O
are O
similar O
, O
that O
is O
a O
better O
prior O
distribution O
would O
lead O
to O
more O
informative O
posterior O
. O
In O
general O
, O
it O
may O
be O
difficult O
to O
design O
a O
better O
prior O
given O
only O
data O
and O
a O
model Method
architecture Method
. O
Additionally O
, O
the O
high O
dimensionality O
of O
the O
NN O
's O
parameter O
space O
renders O
various O
approaches O
such O
as O
adaptive O
priors O
intractable O
( O
e.g. O
adaptive O
MCMC Method
algorithms O
[ O
reference O
][ O
reference O
] O
) O
. O
Hence O
, O
we O
look O
into O
an O
adaptive Method
weight Method
" Method
re Method
- Method
initialization Method
" Method
strategy Method
. O
We O
start O
with O
an O
input O
prior O
( O
weight Method
initialization Method
) O
and O
compute O
an O
approximate O
MAP O
point O
by O
annealing O
the O
noise O
in O
SGD Method
. O
Once O
we O
compute O
the O
MAP O
point O
, O
we O
use O
it O
as O
a O
new O
initialization O
of O
the O
neural Method
network Method
weights Method
, O
and O
restart O
the O
noise Method
annealing Method
schedule Method
. O
We O
then O
iteratively O
repeat O
this O
process O
through O
the O
training Method
process Method
. O
One O
approach O
to O
controlling O
the O
level O
of O
noise O
in O
SGD Method
is O
via O
the O
learning Metric
rate Metric
, O
which O
is O
the O
approach O
used O
in O
[ O
reference O
][ O
reference O
] O
. O
However O
, O
as O
discussed O
in O
[ O
reference O
][ O
reference O
][ O
reference O
] O
, O
the O
batch O
size O
can O
also O
be O
used O
to O
control O
SGD O
noise O
. O
The O
motivation O
for O
this O
is O
that O
larger O
batch O
sizes O
allow O
for O
parallel Task
execution Task
which O
can O
accelerate O
training Task
. O
We O
implement O
weight Method
re Method
- Method
initialization Method
through O
cyclical Method
batch Method
size Method
schedules Method
. O
The O
SGD Method
training Method
process Method
is O
divided O
into O
one O
or O
more O
cycles O
, O
and O
in O
single O
cycle O
we O
gradually O
increase O
the O
batch O
size O
to O
decrease O
noise O
. O
As O
the O
noise O
level O
of O
SGD Method
is O
annealed O
, O
θ O
will O
approaches O
a O
local O
minima O
i.e. O
, O
an O
approximate O
MAP O
point O
of O
P O
( O
θ|X O
) O
. O
Then O
at O
the O
beginning O
of O
the O
subsequent O
cycle O
we O
drop O
the O
batch O
size O
back O
down O
to O
the O
initial O
value O
, O
which O
increases O
the O
noise O
in O
SGD Method
and O
" O
re O
- O
initializes O
" O
the O
neural Method
network Method
parameters Method
using O
the O
previous O
estimate O
. O
Several O
CBS Method
schedules O
are O
shown O
in O
Fig O
. O
1 O
. O
section O
: O
Results O
We O
perform O
a O
variety O
of O
experiments O
across O
different O
tasks O
and O
neural Method
network Method
architectures Method
in O
natural Task
language Task
processing Task
as O
well O
as O
image Task
classification Task
. O
We O
report O
our O
experimental O
findings O
on O
language Task
tasks Task
in O
section O
3.1 O
, O
and O
image Task
classification Task
in O
section O
3.2 O
. O
We O
illustrate O
that O
CBS Method
schedules O
can O
alleviate O
sub Task
- Task
optimal Task
initialization Task
in O
section O
3.3 O
. O
We O
follow O
the O
baseline O
training O
method O
for O
each O
task O
( O
for O
details O
please O
see O
Appendix O
A O
) O
. O
Alongside O
testing Metric
/ Metric
validation Metric
performance Metric
, O
we O
also O
report O
the O
number O
of O
training O
iterations O
( O
lower O
values O
are O
preferred O
) O
. O
section O
: O
Language O
Results O
Language Task
modeling Task
is O
a O
challenging O
problem O
due O
to O
the O
complex O
and O
long O
- O
range O
interactions O
between O
distant O
words O
[ O
reference O
] O
. O
One O
hope O
is O
that O
large O
/ Method
deep Method
models Method
might O
be O
able O
to O
capture O
these O
complex O
interactions O
, O
but O
large O
models O
easily O
overfit O
on O
these O
tasks O
and O
exhibit O
large O
gaps O
between O
training O
set O
and O
testing O
set O
performance O
. O
CBS Method
schedules O
effectively O
help O
us O
avoid O
overfitting O
, O
and O
in O
addition O
snapshot O
ensembling Method
enables O
even O
greater O
performance O
. O
We O
evaluate O
a O
large O
variety O
of O
CBS Method
schedules O
to O
positive O
results O
as O
shown O
in O
Table O
1 O
. O
Results O
are O
measured O
in O
perplexity Metric
, O
a O
standard O
figure Metric
of Metric
merit Metric
for O
evaluating O
the O
quality Metric
of O
language Method
models Method
by O
measuring O
its O
prediction O
of O
the O
empirical O
distribution O
of O
words O
( O
lower O
perplexity Metric
value Metric
is O
better O
) O
. O
As O
we O
can O
see O
, O
the O
best O
performing O
CBS Method
schedules O
result O
in O
significant O
improvements O
in O
perplexity Metric
( O
up O
to O
7.91 O
) O
over O
the O
baseline O
schedules O
and O
also O
offer O
reductions O
in O
the O
number O
of O
SGD Metric
training Metric
iterations Metric
( O
up O
to O
33 O
% O
) O
. O
For O
example O
, O
CBS Method
schedules O
achieve O
improvement O
of O
7.91 Metric
perplexity Metric
improvement Metric
on O
WikiText Material
2 Material
via O
CBS Method
- Method
1 Method
- Method
T Method
and O
reduce O
the O
SGD O
iterations O
from O
164k O
to O
111k O
via O
the O
CBS Method
- Method
1 Method
- Method
A Method
schedule Method
. O
Notice O
that O
almost O
all O
CBS Method
schedules O
outperform O
the O
baseline O
schedule O
. O
Fig O
. O
2 O
shows O
the O
training Metric
and Metric
testing Metric
perplexity Metric
of O
the O
L2 Method
model Method
on O
PTB Material
and O
WikiTest Material
2 Material
as O
trained O
via O
the O
baseline O
schedule O
along O
with O
our O
best O
CBS Method
schedule O
( O
from O
Table O
1 O
) O
. O
Notice O
the O
cyclical O
spikes O
in O
training O
and O
testing O
perplexity O
. O
The O
peaks O
occur O
during O
decreases O
in O
batch O
size O
, O
i.e. O
, O
increases O
in O
noise O
scale O
, O
which O
could O
help O
to O
escape O
sub O
- O
optimal O
local O
minima O
, O
and O
the O
troughs O
occur O
during O
increases O
in O
batch O
size O
, O
i.e. O
, O
decreases O
with O
noise O
scale O
. O
In O
order O
to O
support O
our O
claim O
that O
CBS Method
schedules O
are O
especially O
useful O
for O
counteracting O
overfitting Task
, O
we O
conducted O
additional O
language Method
modeling Method
experiments O
on O
models O
L1 O
' O
, O
L2 O
' O
with O
PTB Material
and O
WT2 Material
which O
use O
significantly O
lower O
dropout O
( O
0.2 O
and O
0.3 O
) O
than O
the O
original O
L1 Method
, Method
L2 Method
models Method
( O
0.5 O
and O
0.65 O
) O
. O
Because O
these O
models O
heavily O
overfit O
the O
training O
data O
, O
we O
report O
both O
the O
final O
testing Metric
perplexity Metric
as O
well O
as O
the O
best O
testing Metric
perplexity Metric
achieve O
during O
training O
. O
As O
seen O
in O
Table O
5 O
( O
in O
Appendix O
B O
) O
, O
with O
L2 Method
' Method
CBS Method
yields O
improvements O
of O
a O
staggering O
60.3 O
on O
final Metric
testing Metric
perplexity Metric
and O
36.2 O
on O
best O
testing Metric
perplexity Metric
. O
CBS Method
yields O
smaller O
improvements O
on O
L1 O
' O
of O
26.0 O
and O
25.3 O
, O
which O
are O
still O
much O
larger O
than O
the O
improvement O
achieved O
by O
CBS Method
on O
L1 O
and O
L2 O
. O
As O
mentioned O
above O
the O
goal O
of O
every O
cycle O
is O
to O
get O
an O
approximate O
MAP O
point O
. O
A O
very O
interesting O
idea O
proposed O
in O
[ O
reference O
] O
is O
to O
ensemble O
these O
MAP O
points O
by O
saving O
snapshots O
of O
the O
model O
at O
the O
end O
of O
every O
cycle O
. O
We O
follow O
that O
strategy O
with O
the O
only O
difference O
that O
we O
use O
a O
batch O
size O
cycle O
instead O
of O
cyclical O
learning O
rate O
proposed O
in O
[ O
reference O
] O
due O
to O
higher O
parallelization O
opportunities O
for O
the O
former O
. O
We O
perform O
experiments O
on O
snapshot O
ensembling Method
with O
the O
L2 Method
model Method
with O
the O
respective O
best O
performing O
CBS Method
schedules O
on O
PTB Material
and O
WikiText Material
2 Material
( O
CBS Method
- Method
1 Method
- Method
T Method
and O
CBS Method
- Method
1 Method
) O
, O
as O
well O
as O
the O
fixed Method
batch Method
size Method
baseline Method
. O
The O
CBS Method
ensembles O
on O
PTB Material
and O
WikiText Material
2 Material
result O
in O
test Metric
set Metric
perplexity Metric
of O
76.14 O
and O
88.47 O
, O
outperforming O
baseline O
ensembles O
on O
both O
datasets O
( O
76.52 O
, O
89.99 O
respectively O
) O
and O
CBS Method
single O
models O
( O
77.39 O
, O
91.78 O
respectively O
) O
. O
To O
further O
explore O
the O
properties O
of O
cyclical Method
batch Method
size Method
schedules Method
, O
we O
also O
evaluate O
these O
schedules O
on O
natural Task
language Task
inference Task
tasks O
, O
as O
shown O
in O
Table O
2 O
. O
In O
our O
experiments O
, O
CBS Method
schedules O
do O
not O
yield O
large O
performance O
improvements O
on O
models O
like O
E1 O
which O
exhibit O
smaller O
disparities O
between O
training O
and O
testing O
performance O
. O
This O
is O
in O
line O
with O
our O
limitation O
in O
that O
CBS Method
is O
more O
effective O
for O
models O
which O
tend O
to O
overfit O
. O
On O
the O
other O
hand O
, O
we O
see O
a O
large O
reduction O
in O
training Metric
iterations Metric
by O
up O
to O
62 O
% O
which O
is O
due O
to O
higher O
effective O
batch O
size O
used O
in O
CBS Method
than O
baseline O
. O
We O
also O
test O
our O
CBS Method
schedules O
on O
Cifar Material
- Material
10 Material
and O
ImageNet Material
. O
Table O
. O
3 O
reports O
the O
testing Metric
accuracy Metric
and O
the O
number O
of O
training Metric
iterations Metric
for O
different O
models O
on O
Cifar Material
- Material
10 Material
. O
We O
see O
that O
the O
CBS Method
schedules O
match O
baseline O
performance O
, O
but O
the O
number O
of O
training O
iterations O
used O
in O
CBS Method
schedules O
is O
up O
to O
2× O
fewer O
. O
section O
: O
Image Task
Classification Task
Results O
As O
seen O
in O
Fig O
. O
3 O
, O
the O
training O
curves O
of O
CBS Method
schedules O
also O
exhibit O
the O
aforementioned O
cyclical O
spikes O
both O
in O
training Metric
loss Metric
and O
testing Metric
accuracy Metric
. O
Similarly O
in O
the O
previously O
discussed O
language O
experiments O
, O
these O
spikes O
correspond O
to O
cycles O
in O
the O
CBS Method
schedules O
and O
can O
be O
thought O
of O
as O
re O
- O
initializations O
of O
the O
neural Method
network Method
weights Method
. O
We O
observe O
that O
CBS Method
achieves O
similar O
performance O
to O
the O
baseline O
. O
We O
offer O
further O
support O
for O
the O
hypothesis O
that O
CBS Method
schedules O
are O
more O
effective O
for O
overfitting Method
neural Method
networks Method
with O
experiments O
on O
model O
C4 O
, O
which O
achieves O
94.35 O
% O
training Metric
accuracy Metric
and O
55.55 O
% O
testing Metric
accuracy Metric
on O
Cifar Material
- Material
10 Material
. O
With O
CBS Method
- Method
15 Method
, O
we O
see O
90.71 O
% O
training Metric
accuracy Metric
and O
56.44 O
% O
testing Metric
accuracy Metric
, O
which O
is O
a O
larger O
improvement O
than O
that O
offered O
by O
CBS Method
on O
convolutional Method
models Method
on O
Cifar Material
- Material
10 Material
. O
We O
also O
explore O
combining O
CBS Method
with O
the O
recent O
adversarial Method
regularization Method
proposed O
by O
[ O
reference O
] O
. O
Combining O
CBS Method
- Method
15 Method
on O
C2 Method
with O
this O
strategy O
improves O
accuracy Metric
to O
94.82 O
% O
. O
This O
outperforms O
other O
schedules O
shown O
in O
Table O
3 O
. O
Applying O
snapshot O
ensembling Method
on O
C3 Method
trained O
with O
CBS Method
- Method
15 Method
- Method
2 Method
leads O
to O
improved O
accuracy Metric
of O
93.56 O
% O
as O
compared O
to O
92.58 O
% O
. O
After O
ensembling Method
ResNet50 Method
on O
Imagenet Material
with O
snapshots O
from O
the O
last O
two O
cycles O
, O
the O
performance O
increases O
to O
76.401 O
% O
from O
75.336 O
% O
. O
section O
: O
Sub Task
- Task
optimal Task
Initialization Task
Various O
effective O
initialization Method
methods Method
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
have O
been O
proposed O
previously O
; O
however O
, O
when O
presented O
with O
new O
architectures O
and O
new O
tasks O
, O
initialization Task
still O
needs O
to O
be O
explored O
empirically O
and O
often O
the O
final O
performance O
varies O
greatly O
with O
different O
initializations O
. O
In O
this O
section O
, O
we O
test O
if O
CBS Method
schedules O
can O
alleviate O
the O
problem O
of O
sub Task
- Task
optimal Task
initialization Task
. O
We O
test O
a O
Gaussian Method
initialization Method
with O
mean O
0 O
and O
standard O
deviation O
0.1 O
on O
an O
AlexNet Method
- Method
like Method
model Method
( O
C1 O
) O
. O
The O
baseline O
( O
BL Method
) Method
training Method
follows O
the O
same O
setting O
as O
described O
in O
Appendix O
A O
and O
achieves O
final O
accuracy Metric
84.27 O
% O
. O
For O
CBS Method
, O
we O
use O
cycle O
width O
of O
10 O
with O
3 O
steps O
. O
In O
particular O
, O
CBS Method
1 O
denotes O
a O
constant Metric
learning Metric
rate Metric
, O
and O
achieves O
final O
accuracy Metric
85.41 O
% O
. O
CBS Method
2 O
decays O
the O
learning Metric
rate Metric
by O
a O
factor O
of O
5 O
at O
epoch O
75 O
and O
achieves O
final O
accuracy Metric
84.95 O
% O
. O
We O
keep O
learning Metric
rate Metric
high O
during O
training O
because O
a O
high O
noise O
level O
helps O
θ O
escape O
sub O
- O
optimal O
local O
minima O
. O
Notice O
that O
all O
CBS Method
methods O
achieve O
better O
generalization Task
performance O
than O
the O
baseline O
. O
section O
: O
Conclusions O
In O
this O
work O
we O
explored O
different O
cyclical O
batch O
size O
( O
CBS Method
) O
schedules O
for O
training O
neural Method
networks Method
. O
We O
framed O
the O
motivation O
behind O
CBS Method
schedules O
through O
the O
lens O
of O
Bayesian Method
statistical Method
methods Method
, O
in O
particular O
adaptive O
MCMC Method
algorithms O
, O
which O
seek O
out O
better O
estimates O
of O
the O
posterior O
starting O
with O
a O
( O
poor O
) O
prior O
distribution O
. O
In O
the O
context O
of O
neural Task
network Task
training Task
, O
this O
translates O
to O
re O
- O
initialization O
of O
the O
weights O
via O
cycling O
between O
large O
and O
small O
batch O
sizes O
which O
control O
the O
noise O
in O
SGD Method
. O
We O
show O
empirical O
results O
which O
find O
this O
cyclical Method
batch Method
size Method
schedule Method
can O
significantly O
outperform O
fixed Method
batch Method
size Method
baselines Method
, O
especially O
in O
networks O
prone O
to O
overfitting O
or O
initialized O
poorly O
, O
on O
the O
tasks O
of O
language Task
modeling Task
, O
natural Task
language Task
inference Task
, O
and O
image Task
classification Task
with O
LSTMs Method
, O
CNNs Method
, O
and O
ResNets Method
. O
In O
our O
language Task
modeling Task
experiments O
, O
we O
see O
that O
a O
wide O
variety O
of O
CBS Method
schedules O
outperform O
the O
baseline O
by O
up O
to O
7.91 O
perplexity Metric
and O
up O
to O
33 O
% O
fewer O
training O
iterations O
. O
For O
natural Task
language Task
inference Task
and O
image Task
classification Task
tasks Task
, O
we O
observe O
a O
reduction O
in O
the O
number O
of O
training Metric
iterations Metric
of O
up O
to O
61 O
% O
, O
which O
translates O
directly O
into O
reduced O
runtime Metric
. O
Finally O
, O
we O
demonstrate O
the O
flexibility O
of O
CBS Method
as O
a O
building O
block O
for O
ensembling Method
and O
adversarial Method
training Method
methods Method
. O
Ensembling Method
on O
language Method
modeling Method
yields O
improvements O
of O
up O
to O
11.22 O
perplexity Metric
over O
the O
baseline O
and O
on O
image Task
classification Task
, O
an O
improvement O
of O
up O
to O
1.07 O
% O
accuracy Metric
. O
Adversarial Method
training Method
in O
conjunction O
with O
CBS Method
gives O
a O
bump O
in O
image Metric
classification Metric
accuracy Metric
of O
0.26 O
% O
. O
section O
: O
Limitations O
We O
believe O
that O
it O
is O
very O
important O
for O
every O
work O
to O
state O
its O
limitations O
( O
in O
general O
, O
but O
in O
particular O
in O
this O
area O
) O
. O
We O
performed O
an O
extensive O
variety O
of O
experiments O
on O
different O
tasks O
in O
order O
to O
comprehensively O
test O
the O
algorithm O
. O
The O
primary O
limitation O
of O
our O
work O
is O
that O
cyclical Method
batch Method
size Method
schedules Method
introduce O
another O
hyper O
- O
parameter O
that O
requires O
manual O
tuning O
. O
We O
note O
that O
this O
is O
also O
true O
for O
cyclical O
learning O
rate O
schedules O
, O
and O
hope O
to O
address O
this O
using O
second Method
order Method
methods Method
[ O
reference O
] O
as O
part O
of O
future O
work O
. O
Furthermore O
, O
for O
well Method
initialized Method
models Method
which O
are O
not O
prone O
to O
overfitting O
, O
single O
snapshot O
CBS Method
achieves O
similar O
performance O
to O
the O
baseline O
, O
although O
the O
cyclical O
ensembling Method
provides O
a O
modicum O
of O
improvement O
. O
section O
: O
A O
Training O
Details O
Here O
we O
catalogue O
details O
regarding O
all O
tasks O
, O
datasets O
, O
models O
, O
batch O
schedules O
, O
and O
other O
hyperparameters O
used O
in O
our O
experiments O
. O
In O
all O
experiments O
, O
we O
try O
to O
copy O
as O
many O
hyper O
- O
parameters O
from O
the O
original O
papers O
as O
possible O
. O
Tasks O
: O
We O
train O
networks O
to O
perform O
the O
following O
supervised Task
learning Task
tasks Task
: O
• O
Image Task
classification Task
. O
The O
network O
is O
trained O
to O
classify O
the O
content O
of O
images O
within O
a O
fixed O
set O
of O
object O
classes O
. O
• O
Language Method
modeling Method
. O
The O
network O
is O
trained O
to O
predict O
the O
last O
token O
in O
a O
sequence O
of O
English O
words O
. O
• O
Natural Task
Language Task
Inference Task
. O
The O
network O
is O
trained O
to O
classify O
the O
relationship O
between O
pairs O
of O
English O
sentences O
such O
as O
that O
of O
entailment O
, O
contradiction O
, O
or O
neutral O
. O
Datasets O
: O
We O
train O
networks O
on O
the O
following O
datasets O
. O
• O
Cifar Task
( Task
image Task
classification Task
) O
. O
The O
two O
Cifar Material
( O
i.e. O
, O
Cifar Material
- Material
10 Material
/ O
Cifar Material
- Material
100 Material
) O
datasets O
[ O
reference O
] O
contain O
50k O
training O
images O
and O
10k O
testing O
images O
, O
and O
10 O
/ O
100 O
label O
classes O
. O
• O
ImageNet Material
( O
image O
classification O
) O
. O
The O
ILSVRC Material
2012 Material
classification Material
dataset Material
consists O
of O
1000 O
label O
classes O
, O
with O
a O
total O
of O
1.2 O
million O
training O
images O
and O
50 O
, O
000 O
validation O
images O
. O
During O
training O
, O
we O
crop O
the O
image O
to O
224 O
× O
224 O
. O
• O
PTB Material
( O
language O
modeling O
) O
. O
The O
Penn Material
Tree Material
Bank Material
dataset Material
consists O
of O
preprocessed O
and O
tokenized O
sentences O
from O
the O
Wall Material
Street Material
Journal Material
. O
The O
training O
set O
is O
929k O
words O
, O
the O
validation O
set O
73k O
words O
, O
and O
test O
set O
82k O
words O
. O
The O
total O
vocabulary O
size O
is O
10k O
, O
and O
all O
words O
outside O
the O
vocabulary O
are O
replaced O
by O
a O
placeholder O
token O
. O
• O
WikiText Material
2 Material
( O
language Task
modeling Task
) O
. O
The O
Wikitext Material
2 Material
dataset Material
is O
modeled O
after O
the O
Penn Material
Tree Material
Bank Material
dataset Material
and O
consists O
of O
preprocessed O
and O
tokenized O
sentences O
from O
Wikipedia Material
. O
The O
training O
set O
is O
2089k O
words O
, O
the O
validation O
set O
218k O
words O
, O
and O
the O
test O
set O
246k O
words O
. O
The O
total O
vocabulary O
size O
is O
33k O
, O
and O
all O
words O
outside O
the O
vocabulary O
are O
replaced O
by O
a O
placeholder O
token O
. O
• O
SNLI Material
( O
natural Task
language Task
inference Task
) O
. O
The O
SNLI Material
dataset O
[ O
reference O
] O
consists O
of O
pairs O
of O
sentences O
annotated O
with O
one O
of O
three O
labels O
regarding O
textual O
entailment O
information O
: O
contradiction O
, O
neutral O
, O
or O
entailment O
. O
The O
training O
set O
contains O
550k O
pairs O
, O
and O
the O
validation O
set O
contains O
10k O
pairs O
. O
• O
MultiNLI Material
( O
natural Task
language Task
inference Task
. O
The O
MultiNLI Material
dataset O
[ O
reference O
] O
is O
modeled O
after O
the O
SNLI Material
dataset O
and O
contains O
a O
training O
set O
of O
393k O
pairs O
and O
a O
validation O
set O
of O
20k O
pairs O
. O
Model Method
Architecture Method
. O
We O
implement O
the O
following O
neural Method
network Method
architectures Method
. O
• O
C1 O
. O
AlexNet Method
- O
like O
on O
Cifar Material
- Material
10 Material
dataset O
as O
in O
[ O
reference O
][ O
C1 O
] O
, O
trained O
on O
the O
task O
of O
image Task
classification Task
. O
We O
train O
for O
200 O
epochs O
with O
an O
initial O
learning Metric
rate Metric
0.02 O
which O
we O
decay O
by O
a O
factor O
of O
5 O
at O
epoch O
30 O
, O
60 O
. O
In O
particular O
, O
we O
use O
initial O
learning O
rate O
0.05 O
for O
cyclic Task
scheduling Task
. O
• O
C2 O
. O
WResNet Method
16 O
- O
4 O
on O
Cifar Material
- Material
10 Material
dataset O
[ O
reference O
] O
, O
trained O
on O
the O
task O
of O
image Task
classification Task
. O
We O
train O
for O
200 O
epochs O
with O
an O
initial O
learning Metric
rate Metric
0.1 O
which O
we O
decay O
by O
a O
factor O
of O
5 O
at O
epoch O
60 O
, O
120 O
, O
and O
180 O
. O
• O
C3 O
. O
ResNet20 Method
on O
Cifar Material
- Material
10 Material
dataset O
[ O
reference O
] O
. O
We O
train O
it O
for O
160 O
epochs O
with O
initial O
learning Metric
rate Metric
0.1 O
, O
and O
decay O
a O
factor O
of O
5 O
at O
epoch O
80 O
, O
120 O
. O
In O
particular O
, O
we O
use O
initial O
learning O
rate O
0.05 O
for O
cyclic Task
scheduling Task
. O
• O
C4 O
. O
MLP3 Method
network Method
from O
[ O
reference O
] O
. O
The O
network O
consists O
of O
3 O
fully Method
connected Method
layers Method
with O
512 O
units O
each O
and O
ReLU O
activations O
. O
As O
a O
baseline O
, O
we O
train O
this O
network O
with O
vanilla Method
SGD Method
for O
240 O
epochs O
with O
a O
batch O
size O
of O
100 O
and O
an O
initial O
learning Metric
rate Metric
of O
0.1 O
, O
which O
is O
decayed O
by O
a O
factor O
of O
10 O
at O
150 O
and O
225 O
epochs O
. O
• O
I1 O
. O
ResNet50 Method
on O
ImageNet Material
dataset O
[ O
reference O
] O
, O
trained O
on O
the O
task O
of O
image Task
classification Task
for O
90 O
epochs O
with O
initial O
learning Metric
rate Metric
0.1 O
which O
we O
decay O
by O
a O
factor O
of O
10 O
at O
epoch O
30 O
, O
60 O
and O
80 O
. O
• O
L1 O
. O
Medium Method
Regularized Method
LSTM Method
[ O
reference O
] O
, O
trained O
on O
the O
task O
of O
language Task
modeling Task
. O
We O
use O
50 O
% O
dropout Method
on O
non O
- O
recurrent O
connections O
and O
train O
for O
39 O
epochs O
with O
initial O
learning Metric
rate Metric
of O
20 O
, O
decaying O
by O
a O
factor O
of O
1.2 O
every O
epoch O
after O
epoch O
6 O
. O
We O
set O
a O
backpropagation O
- O
through O
- O
time O
limit O
of O
35 O
steps O
and O
clip O
the O
max O
gradient O
norm O
at O
0.25 O
. O
• O
L2 O
. O
Large Method
Regularized Method
LSTM Method
[ O
reference O
] O
, O
trained O
on O
the O
task O
of O
language Task
modeling Task
. O
We O
use O
65 O
% O
dropout Method
on O
non O
- O
recurrent O
connections O
and O
train O
for O
55 O
epochs O
with O
initial O
learning Metric
rate Metric
of O
20 O
, O
decaying O
by O
a O
factor O
of O
1.15 O
every O
epoch O
after O
epoch O
14 O
. O
We O
set O
a O
backpropagation O
- O
through O
- O
time O
limit O
of O
35 O
steps O
and O
clip O
the O
max O
gradient O
norm O
at O
0.5 O
. O
• O
L1 O
' O
, O
L2 O
' O
Identical O
to O
L1 O
, O
L2 O
except O
for O
lower O
dropout O
: O
0.2 O
, O
0.3 O
respectively O
. O
Leads O
to O
significant O
overfitting O
, O
evidenced O
by O
test Metric
perplexity Metric
curve Metric
in O
Fig O
. O
5 O
. O
• O
E1 O
. O
ESIM Method
[ O
reference O
] O
. O
We O
train O
the O
base O
ESIM Method
model Method
without O
the O
tree Method
- Method
LSTM Method
, O
as O
in O
[ O
reference O
] O
, O
on O
the O
task O
of O
natural Task
language Task
inference Task
with O
ADAM Method
for O
10 O
epochs O
on O
MultiNLI Material
and O
also O
SNLI Material
. O
Training Task
Schedules Task
: O
We O
use O
the O
following O
batch Method
size Method
schedules Method
• O
BL O
. O
Use O
a O
fixed O
small O
batch O
size O
as O
specified O
in O
the O
original O
paper O
introducing O
the O
model O
or O
as O
is O
standard O
. O
• O
CBS Method
- O
k O
(- O
n O
) O
. O
Use O
a O
Cyclical Method
Batch Method
Size Method
schedule Method
, O
where O
k O
is O
the O
width O
of O
each O
step O
measured O
in O
epochs O
and O
n O
is O
the O
integer O
number O
of O
steps O
per O
cycle O
. O
When O
n O
is O
not O
specified O
it O
refers O
to O
the O
default O
value O
of O
4 O
. O
At O
the O
beginning O
of O
each O
cycle O
the O
batch O
size O
is O
initialized O
to O
the O
base O
batch O
size O
, O
and O
after O
each O
step O
it O
is O
then O
doubled O
. O
• O
CBS Method
- O
k O
(- O
n O
)- O
A. O
Use O
an O
aggressive Method
Cyclical Method
Batch Method
Size Method
schedule Method
, O
which O
is O
equivalent O
to O
the O
original O
CBS Method
schedule O
except O
after O
every O
step O
the O
batch O
size O
is O
quadrupled O
. O
• O
CBS Method
- O
k O
(- O
n O
)- O
T. O
Use O
a O
triangular Method
Cyclical Method
Batch Method
Size Method
schedule Method
, O
which O
is O
modeled O
after O
the O
triangular Method
schedule Method
. O
Each O
cycle O
consists O
of O
n O
steps O
doubling O
the O
batch O
size O
after O
each O
step O
, O
then O
n O
− O
2 O
symmetrical O
steps O
halving O
the O
batch O
size O
after O
each O
step O
. O
In O
all O
language O
modeling O
CBS Method
experiments O
, O
we O
use O
an O
initial O
batch O
size O
of O
10 O
, O
that O
is O
, O
half O
the O
baseline O
batch O
size O
as O
reported O
in O
the O
respective O
papers O
of O
each O
baseline O
model O
tested O
. O
The O
intuition O
behind O
starting O
with O
a O
smaller O
batch O
size O
is O
to O
introduce O
additional O
noise O
to O
help O
models O
escape O
sub O
- O
optimal O
local O
minima O
. O
For O
adversarial Task
training Task
used O
in O
image Task
classification Task
, O
we O
use O
FGSM Method
method Method
[ O
reference O
] O
to O
generate O
adversarial O
examples O
. O
Adversarial Method
training Method
is O
implemented O
for O
the O
first O
half O
training O
epochs O
. O
section O
: O
B O
Additional O
Results O
This O
section O
shows O
additional O
experiment O
results O
. O
section O
: O
