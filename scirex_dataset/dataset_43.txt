document O
: O
Generating Task
Images Task
from Task
Captions Task
with O
Attention Method
Motivated O
by O
the O
recent O
progress O
in O
generative Method
models Method
, O
we O
introduce O
a O
model O
that O
generates Task
images Task
from Task
natural Task
language Task
descriptions Task
. O
The O
proposed O
model O
iteratively O
draws O
patches O
on O
a O
canvas O
, O
while O
attending O
to O
the O
relevant O
words O
in O
the O
description O
. O
After O
training O
on O
Microsoft Material
COCO Material
, O
we O
compare O
our O
model O
with O
several O
baseline Method
generative Method
models Method
on O
image Task
generation Task
and O
retrieval Task
tasks Task
. O
We O
demonstrate O
that O
our O
model O
produces O
higher O
quality O
samples O
than O
other O
approaches O
and O
generates O
images O
with O
novel O
scene O
compositions O
corresponding O
to O
previously O
unseen O
captions O
in O
the O
dataset O
. O
section O
: O
Introduction O
Statistical Method
natural Method
image Method
modelling Method
remains O
a O
fundamental O
problem O
in O
computer Task
vision Task
and O
image Task
understanding Task
. O
The O
challenging O
nature O
of O
this O
task O
has O
motivated O
recent O
approaches O
to O
exploit O
the O
inference Method
and Method
generative Method
capabilities Method
of O
deep Method
neural Method
networks Method
. O
Previously O
studied O
deep Method
generative Method
models Method
of O
images O
often O
defined O
distributions O
that O
were O
restricted O
to O
being O
either O
unconditioned O
or O
conditioned O
on O
classification O
labels O
. O
In O
real Task
world Task
applications Task
, O
however O
, O
images O
rarely O
appear O
in O
isolation O
as O
they O
are O
often O
accompanied O
by O
unstructured O
textual O
descriptions O
, O
such O
as O
on O
web O
pages O
and O
in O
books O
. O
The O
additional O
information O
from O
these O
descriptions O
could O
be O
used O
to O
simplify O
the O
image Task
modelling Task
task Task
. O
Moreover O
, O
learning O
generative Method
models Method
conditioned O
on O
text O
also O
allows O
a O
better O
understanding O
of O
the O
generalization Metric
performance O
of O
the O
model O
, O
as O
we O
can O
create O
textual O
descriptions O
of O
completely O
new O
scenes O
not O
seen O
at O
training O
time O
. O
There O
are O
numerous O
ways O
to O
learn O
a O
generative Method
model Method
over O
both O
image O
and O
text O
modalities O
. O
One O
approach O
is O
to O
learn O
a O
generative Method
model Method
of Method
text Method
conditioned O
on O
images O
, O
known O
as O
caption Task
generation Task
kiros_icml14 O
, O
karpathy_captions O
, O
vinyals_captions O
, O
xu_captions O
. O
These O
models O
take O
an O
image Method
descriptor Method
and O
generate O
unstructured O
texts O
using O
a O
recurrent Method
decoder Method
. O
In O
contrast O
, O
in O
this O
paper O
we O
explore O
models O
that O
condition O
in O
the O
opposite O
direction O
, O
i.e. O
taking O
textual O
descriptions O
as O
input O
and O
using O
them O
to O
generate O
relevant O
images O
. O
Generating Task
high Task
dimensional Task
realistic Task
images Task
from O
their O
descriptions O
combines O
the O
two O
challenging O
components O
of O
language Task
modelling Task
and O
image Task
generation Task
, O
and O
can O
be O
considered O
to O
be O
more O
difficult O
than O
caption Task
generation Task
. O
In O
this O
paper O
, O
we O
illustrate O
how O
sequential Method
deep Method
learning Method
techniques Method
can O
be O
used O
to O
build O
a O
conditional Method
probabilistic Method
model Method
over O
natural O
image O
space O
effectively O
. O
By O
extending O
the O
Deep Method
Recurrent Method
Attention Method
Writer Method
( O
DRAW Method
) O
gregor_draw O
, O
our O
model O
iteratively O
draws O
patches O
on O
a O
canvas O
, O
while O
attending O
to O
the O
relevant O
words O
in O
the O
description O
. O
Overall O
, O
the O
main O
contributions O
of O
this O
work O
are O
the O
following O
: O
we O
introduce O
a O
conditional Method
alignDRAW Method
model Method
, O
a O
generative Method
model Method
of Method
images Method
from O
captions Method
using O
a O
soft Method
attention Method
mechanism Method
. O
The O
images O
generated O
by O
our O
alignDRAW Method
model Method
are O
refined O
in O
a O
post Method
- Method
processing Method
step Method
by O
a O
deterministic Method
Laplacian Method
pyramid Method
adversarial Method
network Method
denton_lapgan O
. O
We O
further O
illustrate O
how O
our O
method O
, O
learned O
on O
Microsoft Material
COCO Material
mscoco Material
, O
generalizes O
to O
captions O
describing O
novel O
scenes O
that O
are O
not O
seen O
in O
the O
dataset O
, O
such O
as O
“ O
A O
stop O
sign O
is O
flying O
in O
blue O
skies O
” O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
. O
[ O
A O
stop O
sign O
is O
flying O
in O
blue O
skies O
. O
] O
[ O
A O
herd O
of O
elephants O
flying O
in O
the O
blue O
skies O
. O
] O
[ O
A O
toilet O
seat O
sits O
open O
in O
the O
grass O
field O
. O
] O
[ O
A O
person O
skiing O
on O
sand O
clad O
vast O
desert O
. O
] O
section O
: O
Related O
Work O
Deep Method
Neural Method
Networks Method
have O
achieved O
significant O
success O
in O
various O
tasks O
such O
as O
image Task
recognition Task
krizhevsky_imagenet O
, O
speech Task
transcription Task
graves_speech O
, O
and O
machine Task
translation Task
bahdanau_mt O
. O
While O
most O
of O
the O
recent O
success O
has O
been O
achieved O
by O
discriminative Method
models Method
, O
generative Method
models Method
have O
not O
yet O
enjoyed O
the O
same O
level O
of O
success O
. O
Most O
of O
the O
previous O
work O
in O
generative Method
models Method
has O
focused O
on O
variants O
of O
Boltzmann Method
Machines Method
smolensky_rbm O
, O
russ_dbm O
and O
Deep Method
Belief Method
Networks Method
hinton_dbn O
. O
While O
these O
models O
are O
very O
powerful O
, O
each O
iteration O
of O
training Task
requires O
a O
computationally O
costly O
step O
of O
MCMC Method
to O
approximate O
derivatives O
of O
an O
intractable O
partition O
function O
( O
normalization O
constant O
) O
, O
making O
it O
difficult O
to O
scale O
them O
to O
large O
datasets O
. O
, O
have O
introduced O
the O
Variational Method
Auto Method
- Method
Encoder Method
( O
VAE Method
) O
which O
can O
be O
seen O
as O
a O
neural Method
network Method
with O
continuous O
latent O
variables O
. O
The O
encoder Method
is O
used O
to O
approximate O
a O
posterior O
distribution O
and O
the O
decoder Method
is O
used O
to O
stochastically O
reconstruct O
the O
data O
from O
latent O
variables O
. O
further O
introduced O
the O
Deep Method
Recurrent Method
Attention Method
Writer Method
( Method
DRAW Method
) Method
, O
extending O
the O
VAE Method
approach O
by O
incorporating O
a O
novel O
differentiable Method
attention Method
mechanism Method
. O
Generative Method
Adversarial Method
Networks Method
( O
GANs Method
) O
goodfellow_gan O
are O
another O
type O
of O
generative Method
models Method
that O
use O
noise Method
- Method
contrastive Method
estimation Method
gutmann_nce O
to O
avoid O
calculating O
an O
intractable O
partition O
function O
. O
The O
model O
consists O
of O
a O
generator Method
that O
generates O
samples O
using O
a O
uniform Method
distribution Method
and O
a O
discriminator Method
that O
discriminates O
between O
real O
and O
generated Task
images Task
. O
Recently O
, O
have O
scaled O
those O
models O
by O
training O
conditional Method
GANs Method
at O
each O
level O
of O
a O
Laplacian O
pyramid O
of O
images O
. O
While O
many O
of O
the O
previous O
approaches O
have O
focused O
on O
unconditional Method
models Method
or O
models O
conditioned O
on O
labels O
, O
in O
this O
paper O
we O
develop O
a O
generative Method
model Method
of Method
images Method
conditioned O
on O
captions O
. O
section O
: O
Model O
Our O
proposed O
model O
defines O
a O
generative Method
process Method
of Method
images Method
conditioned O
on O
captions O
. O
In O
particular O
, O
captions O
are O
represented O
as O
a O
sequence O
of O
consecutive O
words O
and O
images O
are O
represented O
as O
a O
sequence O
of O
patches O
drawn O
on O
a O
canvas O
over O
time O
. O
The O
model O
can O
be O
viewed O
as O
a O
part O
of O
the O
sequence Method
- Method
to Method
- Method
sequence Method
framework Method
ilya_mt O
, O
cho_mt O
, O
nitish_video O
. O
subsection O
: O
Language Method
Model Method
: O
the O
Bidirectional Method
Attention Method
RNN Method
Let O
be O
the O
input O
caption O
, O
represented O
as O
a O
sequence O
of O
1 O
- O
of O
- O
K O
encoded O
words O
, O
where O
is O
the O
size O
of O
the O
vocabulary O
and O
is O
the O
length O
of O
the O
sequence O
. O
We O
obtain O
the O
caption Method
sentence Method
representation Method
by O
first O
transforming O
each O
word O
to O
an O
- Method
dimensional Method
vector Method
representation Method
, O
using O
the O
Bidirectional Method
RNN Method
. O
In O
a O
Bidirectional Task
RNN Task
, O
the O
two O
LSTMs Method
hochreiter_lstm O
with O
forget O
gates O
gers_forget O
process O
the O
input O
sequence O
from O
both O
forward O
and O
backward O
directions O
. O
The O
Forward Method
LSTM Method
computes O
the O
sequence O
of O
forward O
hidden O
states O
, O
whereas O
the O
Backward Method
LSTM Method
computes O
the O
sequence O
of O
backward O
hidden O
states O
. O
These O
hidden O
states O
are O
then O
concatenated O
together O
into O
the O
sequence O
, O
with O
. O
subsection O
: O
Image Method
Model Method
: O
the O
Conditional Method
DRAW Method
Network Method
To O
generate O
an O
image O
conditioned O
on O
the O
caption O
information O
, O
we O
extended O
the O
DRAW Method
network Method
gregor_draw O
to O
include O
caption Method
representation Method
at O
each O
step O
, O
as O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
The O
conditional Method
DRAW Method
network Method
is O
a O
stochastic Method
recurrent Method
neural Method
network Method
that O
consists O
of O
a O
sequence O
of O
latent O
variables O
, O
, O
where O
the O
output O
is O
accumulated O
over O
all O
time O
- O
steps O
. O
For O
simplicity O
in O
notation O
, O
the O
images O
are O
assumed O
to O
have O
size O
- O
by O
- O
and O
only O
one O
color O
channel O
. O
Unlike O
the O
original O
DRAW Method
network Method
where O
latent O
variables O
are O
independent O
spherical Method
Gaussians Method
, O
the O
latent O
variables O
in O
the O
proposed O
alignDRAW Method
model Method
have O
their O
mean O
and O
variance O
depend O
on O
the O
previous O
hidden O
states O
of O
the O
generative Method
LSTM Method
, O
except O
for O
. O
Namely O
, O
the O
mean O
and O
variance O
of O
the O
prior O
distribution O
over O
are O
parameterized O
by O
: O
where O
, O
are O
the O
learned O
model O
parameters O
, O
and O
is O
the O
dimensionality O
of O
, O
the O
hidden O
state O
of O
the O
generative Method
LSTM Method
. O
Similar O
to O
bachman_sdm O
, O
we O
have O
observed O
that O
the O
model O
performance O
is O
improved O
by O
including O
dependencies O
between O
latent O
variables O
. O
Formally O
, O
an O
image O
is O
generated O
by O
iteratively O
computing O
the O
following O
set O
of O
equations O
for O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
, O
with O
and O
initialized O
to O
learned O
biases O
: O
The O
function O
is O
used O
to O
compute O
the O
alignment O
between O
the O
input O
caption O
and O
intermediate Method
image Method
generative Method
steps Method
bahdanau_mt O
. O
Given O
the O
caption Method
representation Method
from O
the O
language Method
model Method
, O
, O
the O
operator O
outputs O
a O
dynamic Method
sentence Method
representation Method
at O
each O
step O
through O
a O
weighted O
sum O
using O
alignment O
probabilities O
: O
The O
corresponding O
alignment O
probability O
for O
the O
word O
in O
the O
caption O
is O
obtained O
using O
the O
caption Method
representation Method
and O
the O
current O
hidden O
state O
of O
the O
generative Method
model Method
: O
where O
, O
, O
and O
are O
the O
learned O
model O
parameters O
of O
the O
alignment Method
model Method
. O
The O
function O
of O
Eq O
. O
[ O
reference O
] O
is O
defined O
by O
the O
LSTM Method
network Method
with O
forget O
gates O
gers_forget O
at O
a O
single O
time O
- O
step O
. O
To O
generate O
the O
next O
hidden O
state O
, O
the O
takes O
the O
previous O
hidden O
state O
and O
combines O
it O
with O
the O
input O
from O
both O
the O
latent O
sample O
and O
the O
sentence Method
representation Method
. O
The O
output O
of O
the O
function O
is O
then O
passed O
through O
the O
operator Method
which O
is O
added O
to O
a O
cumulative O
canvas O
matrix O
( O
Eq O
. O
[ O
reference O
] O
) O
. O
The O
operator O
produces O
two O
arrays O
of O
1D Method
Gaussian Method
filter Method
banks Method
and O
whose O
filter O
locations O
and O
scales O
are O
computed O
from O
the O
generative O
LSTM O
hidden O
state O
( O
same O
as O
defined O
in O
) O
. O
The O
Gaussian Method
filter Method
banks Method
are O
then O
applied O
to O
the O
generated O
- O
by O
- O
image O
patch O
, O
placing O
it O
onto O
the O
canvas O
: O
Finally O
, O
each O
entry O
from O
the O
final O
canvas O
matrix O
is O
transformed O
using O
a O
sigmoid Method
function Method
to O
produce O
a O
conditional Method
Bernoulli Method
distribution Method
with O
mean O
vector O
over O
the O
image O
pixels O
given O
the O
latent O
variables O
and O
the O
input O
caption O
. O
In O
practice O
, O
when O
generating O
an O
image O
, O
instead O
of O
sampling O
from O
the O
conditional O
Bernoulli O
distribution O
, O
we O
simply O
use O
the O
conditional O
mean O
. O
subsection O
: O
Learning O
The O
model O
is O
trained O
to O
maximize O
a O
variational O
lower O
bound O
on O
the O
marginal O
likelihood O
of O
the O
correct O
image O
given O
the O
input O
caption O
: O
Similar O
to O
the O
DRAW Method
model Method
, O
the O
inference Method
recurrent Method
network Method
produces O
an O
approximate O
posterior O
via O
a O
operator Method
, O
which O
reads O
a O
patch O
from O
an O
input O
image O
using O
two O
arrays O
of O
1D Method
Gaussian Method
filters Method
( O
inverse O
of O
from O
section O
[ O
reference O
] O
) O
at O
each O
time O
- O
step O
. O
Specifically O
, O
where O
is O
the O
error O
image O
and O
is O
initialized O
to O
the O
learned O
bias O
. O
Note O
that O
the O
inference Task
takes O
as O
its O
input O
both O
the O
output O
of O
the O
operator O
, O
which O
depends O
on O
the O
original O
input O
image O
, O
and O
the O
previous O
state O
of O
the O
generative Method
decoder Method
, O
which O
depends O
on O
the O
latent O
sample O
history O
and O
dynamic Method
sentence Method
representation Method
( O
see O
Eq O
. O
[ O
reference O
] O
) O
. O
Hence O
, O
the O
approximate O
posterior O
will O
depend O
on O
the O
input O
image O
, O
the O
corresponding O
caption O
, O
and O
the O
latent O
history O
, O
except O
for O
the O
first O
step O
, O
which O
depends O
only O
on O
. O
The O
terms O
in O
the O
variational O
lower O
bound O
Eq O
. O
[ O
reference O
] O
can O
be O
rearranged O
using O
the O
law Method
of Method
total Method
expectation Method
. O
Therefore O
, O
the O
variational O
bound O
is O
calculated O
as O
follows O
: O
The O
expectation O
can O
be O
approximated O
by O
Monte O
Carlo O
samples O
from O
: O
The O
model O
can O
be O
trained O
using O
stochastic Method
gradient Method
descent Method
. O
In O
all O
of O
our O
experiments O
, O
we O
used O
only O
a O
single O
sample O
from O
for O
parameter Task
learning Task
. O
Training O
details O
, O
hyperparameter O
settings O
, O
and O
the O
overall O
model O
architecture O
are O
specified O
in O
Appendix O
B. O
The O
code O
is O
available O
at O
. O
subsection O
: O
Generating Task
Images Task
from Task
Captions Task
During O
the O
image Task
generation Task
step O
, O
we O
discard O
the O
inference Method
network Method
and O
instead O
sample O
from O
the O
prior O
distribution O
. O
Due O
to O
the O
blurriness O
of O
samples O
generated O
by O
the O
DRAW Method
model Method
, O
we O
perform O
an O
additional O
post Task
processing Task
step Task
where O
we O
use O
an O
adversarial Method
network Method
trained O
on O
residuals O
of O
a O
Laplacian Method
pyramid Method
conditioned O
on O
the O
skipthought Method
representation Method
kiros_skipthought O
of O
the O
captions O
to O
sharpen O
the O
generated Task
images Task
, O
similar O
to O
denton_lapgan O
. O
By O
fixing O
the O
prior O
of O
the O
adversarial Method
generator Method
to O
its O
mean O
, O
it O
gets O
treated O
as O
a O
deterministic Method
neural Method
network Method
that O
allows O
us O
to O
define O
the O
conditional O
data O
term O
in O
Eq O
. O
[ O
reference O
] O
on O
the O
sharpened O
images O
and O
estimate O
the O
variational O
lower O
bound O
accordingly O
. O
[ O
A O
yellow O
school O
bus O
parked O
in O
a O
parking O
lot O
. O
] O
[ O
A O
red O
school O
bus O
parked O
in O
a O
parking O
lot O
. O
] O
[ O
A O
green O
school O
bus O
parked O
in O
a O
parking O
lot O
. O
] O
[ O
A O
blue O
school O
bus O
parked O
in O
a O
parking O
lot O
. O
] O
[ O
The O
decadent O
chocolate O
desert O
is O
on O
the O
table O
. O
] O
[ O
A O
bowl O
of O
bananas O
is O
on O
the O
table O
. O
] O
[ O
A O
vintage O
photo O
of O
a O
cat O
. O
] O
[ O
A O
vintage O
photo O
of O
a O
dog O
. O
] O
section O
: O
Experiments O
subsection O
: O
Microsoft Material
COCO Material
Microsoft Material
COCO Material
mscoco Material
is O
a O
large O
dataset O
containing O
82 O
, O
783 O
images O
, O
each O
annotated O
with O
at O
least O
5 O
captions O
. O
The O
rich O
collection O
of O
images O
with O
a O
wide O
variety O
of O
styles O
, O
backgrounds O
and O
objects O
makes O
the O
task O
of O
learning O
a O
good O
generative Method
model Method
very O
challenging O
. O
For O
consistency O
with O
related O
work O
on O
caption Task
generation Task
, O
we O
used O
only O
the O
first O
five O
captions O
when O
training O
and O
evaluating O
our O
model O
. O
The O
images O
were O
resized O
to O
pixels O
for O
consistency O
with O
other O
tiny O
image O
datasets O
krizhevsky_cifar O
. O
In O
the O
following O
subsections O
, O
we O
analyzed O
both O
the O
qualitative O
and O
quantitative O
aspects O
of O
our O
model O
as O
well O
as O
compared O
its O
performance O
with O
that O
of O
other O
, O
related O
generative Method
models Method
. O
Appendix O
A O
further O
reports O
some O
additional O
experiments O
using O
the O
MNIST O
dataset O
. O
subsubsection O
: O
Analysis Task
of Task
Generated Task
Images Task
The O
main O
goal O
of O
this O
work O
is O
to O
learn O
a O
model O
that O
can O
understand O
the O
semantic O
meaning O
expressed O
in O
the O
textual O
descriptions O
of O
images O
, O
such O
as O
the O
properties O
of O
objects O
, O
the O
relationships O
between O
them O
, O
and O
then O
use O
that O
knowledge O
to O
generate O
relevant O
images O
. O
To O
examine O
the O
understanding O
of O
our O
model O
, O
we O
wrote O
a O
set O
of O
captions O
inspired O
by O
the O
COCO Material
dataset Material
and O
changed O
some O
words O
in O
the O
captions O
to O
see O
whether O
the O
model O
made O
the O
relevant O
changes O
in O
the O
generated O
samples O
. O
First O
, O
we O
explored O
whether O
the O
model O
understood O
one O
of O
the O
most O
basic O
properties O
of O
any O
object O
, O
the O
color O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
generated Task
images Task
of O
school O
buses O
with O
four O
different O
colors O
: O
yellow O
, O
red O
, O
green O
and O
blue O
. O
Although O
, O
there O
are O
images O
of O
buses O
with O
different O
colors O
in O
the O
training O
set O
, O
all O
mentioned O
school O
buses O
are O
specifically O
colored O
yellow O
. O
Despite O
that O
, O
the O
model O
managed O
to O
generate O
images O
of O
an O
object O
that O
is O
visually O
reminiscent O
of O
a O
school O
bus O
that O
is O
painted O
with O
the O
specified O
color O
. O
Apart O
from O
changing O
the O
colors O
of O
objects O
, O
we O
next O
examined O
whether O
changing O
the O
background O
of O
the O
scene O
described O
in O
a O
caption O
would O
result O
in O
the O
appropriate O
changes O
in O
the O
generated O
samples O
. O
The O
task O
of O
changing O
the O
background O
of O
an O
image O
is O
somewhat O
harder O
than O
just O
changing O
the O
color O
of O
an O
object O
because O
the O
model O
will O
have O
to O
make O
alterations O
over O
a O
wider O
visual O
area O
. O
Nevertheless O
, O
as O
shown O
in O
Fig O
. O
[ O
reference O
] O
changing O
the O
skies O
from O
blue O
to O
rainy O
in O
a O
caption O
as O
well O
as O
changing O
the O
grass O
type O
from O
dry O
to O
green O
in O
another O
caption O
resulted O
in O
the O
appropriate O
changes O
in O
the O
generated O
image O
. O
Despite O
a O
large O
number O
of O
ways O
of O
changing O
colors O
and O
backgrounds O
in O
descriptions O
, O
in O
general O
we O
found O
that O
the O
model O
made O
appropriate O
changes O
as O
long O
as O
some O
similar O
pattern O
was O
present O
in O
the O
training O
set O
. O
However O
, O
the O
model O
struggled O
when O
the O
visual O
difference O
between O
objects O
was O
very O
small O
, O
such O
as O
when O
the O
objects O
have O
the O
same O
general O
shape O
and O
color O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
demonstrate O
that O
when O
we O
swap O
two O
objects O
that O
are O
both O
visually O
similar O
, O
for O
example O
cats O
and O
dogs O
, O
it O
is O
difficult O
to O
discriminate O
solely O
from O
the O
generated O
samples O
whether O
it O
is O
an O
image O
of O
a O
cat O
or O
dog O
, O
even O
though O
we O
might O
notice O
an O
animal O
- O
like O
shape O
. O
This O
highlights O
a O
limitation O
of O
the O
model O
in O
that O
it O
has O
difficulty O
modelling O
the O
fine O
- O
grained O
details O
of O
objects O
. O
As O
a O
test O
of O
model Task
generalization Task
, O
we O
tried O
generating Task
images Task
corresponding Task
to Task
captions Task
that O
describe O
scenarios O
that O
are O
highly O
unlikely O
to O
occur O
in O
real O
life O
. O
These O
captions O
describe O
a O
common O
object O
doing O
unusual O
things O
or O
set O
in O
a O
strange O
location O
, O
for O
example O
“ O
A O
toilet O
seat O
sits O
open O
in O
the O
grass O
field O
” O
. O
Even O
though O
some O
of O
these O
scenarios O
may O
never O
occur O
in O
real O
life O
, O
it O
is O
very O
easy O
for O
humans O
to O
imagine O
the O
corresponding O
scene O
. O
Nevertheless O
, O
as O
you O
can O
see O
in O
Fig O
. O
[ O
reference O
] O
, O
the O
model O
managed O
to O
generate O
reasonable O
images O
. O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
A O
very O
large O
commercial O
plane O
flying O
in O
blue O
skies O
. O
] O
[ O
A O
very O
large O
commercial O
plane O
flying O
in O
rainy O
skies O
. O
] O
[ O
A O
herd O
of O
elephants O
walking O
across O
a O
dry O
grass O
field O
. O
] O
[ O
A O
herd O
of O
elephants O
walking O
across O
a O
green O
grass O
field O
. O
] O
subsubsection O
: O
Analysis Task
of Task
Attention Task
After O
flipping O
sets O
of O
words O
in O
the O
captions O
, O
we O
further O
explored O
which O
words O
the O
model O
attended O
to O
when O
generating O
images O
. O
It O
turned O
out O
that O
during O
the O
generation Task
step Task
, O
the O
model O
mostly O
focused O
on O
the O
specific O
words O
( O
or O
nearby O
words O
) O
that O
carried O
the O
main O
semantic O
meaning O
expressed O
in O
the O
sentences O
. O
The O
attention O
values O
of O
words O
in O
sentences O
helped O
us O
interpret O
the O
reasons O
why O
the O
model O
made O
the O
changes O
it O
did O
when O
we O
flipped O
certain O
words O
. O
For O
example O
, O
in O
Fig O
. O
[ O
reference O
] O
, O
top O
row O
, O
we O
can O
see O
that O
when O
we O
flipped O
the O
word O
“ O
desert O
” O
to O
“ O
forest O
” O
, O
the O
attention O
over O
words O
in O
the O
sentence O
did O
not O
change O
drastically O
. O
This O
suggests O
that O
, O
in O
their O
respective O
sentences O
, O
the O
model O
looked O
at O
“ O
desert O
” O
and O
“ O
forest O
” O
with O
relatively O
equal O
probability O
, O
and O
thus O
made O
the O
correct O
changes O
. O
In O
contrast O
, O
when O
we O
swap O
words O
“ O
beach O
” O
and O
“ O
sun O
” O
, O
we O
can O
see O
a O
drastic O
change O
between O
sentences O
in O
the O
probability O
distribution O
over O
words O
. O
By O
noting O
that O
the O
model O
completely O
ignores O
the O
word O
“ O
sun O
” O
in O
the O
second O
sentence O
, O
we O
can O
therefore O
gain O
a O
more O
thorough O
understanding O
of O
why O
we O
see O
no O
visual O
differences O
between O
the O
images O
generated O
by O
each O
caption O
. O
[ O
3 O
A O
rider O
1 O
on O
a O
blue O
1 O
motorcycle O
in O
the O
. O
] O
[ O
3 O
A O
rider O
1 O
on O
a O
blue O
1 O
motorcycle O
in O
the O
. O
] O
[ O
2 O
A O
1 O
surfer O
, O
a O
woman O
, O
and O
a O
child O
walk O
on O
the O
. O
] O
[ O
3 O
A O
1 O
surfer O
, O
a O
woman O
, O
and O
a O
child O
walk O
on O
the O
. O
] O
[ O
alignDRAW O
] O
[ O
LAPGAN O
] O
[ O
Conv Method
- Method
Deconv Method
VAE Method
] O
[ O
Fully Method
- Method
Conn Method
VAE Method
] O
We O
also O
tried O
to O
analyze O
the O
way O
the O
model O
generated Task
images Task
. O
Unfortunately O
, O
we O
found O
that O
there O
was O
no O
significant O
connection O
between O
the O
patches O
drawn O
on O
canvas O
and O
the O
most O
attended O
words O
at O
particular O
time O
- O
steps O
. O
subsubsection O
: O
Comparison O
With O
Other O
Models O
Quantitatively O
evaluating O
generative Method
models Method
remains O
a O
challenging O
task O
in O
itself O
as O
each O
method O
of O
evaluation Task
suffers O
from O
its O
own O
specific O
drawbacks O
. O
Compared O
to O
reporting O
classification Metric
accuracies Metric
in O
discriminative Method
models Method
, O
the O
measures O
defining O
generative Method
models Method
are O
intractable O
most O
of O
the O
times O
and O
might O
not O
correctly O
define O
the O
real O
quality O
of O
the O
model O
. O
To O
get O
a O
better O
comparison O
between O
performances O
of O
different O
generative Method
models Method
, O
we O
report O
results O
on O
two O
different O
metrics O
as O
well O
as O
a O
qualitative O
comparison O
of O
different O
generative Method
models Method
. O
We O
compared O
the O
performance O
of O
the O
proposed O
model O
to O
the O
DRAW Method
model Method
conditioned O
on O
captions O
without O
the O
function O
( O
noalignDRAW O
) O
as O
well O
as O
the O
DRAW Method
model Method
conditioned O
on O
the O
skipthought O
vectors O
of O
kiros_skipthought O
( O
skipthoughtDRAW O
) O
. O
All O
of O
the O
conditional Method
DRAW Method
models Method
were O
trained O
with O
a O
binary Metric
cross Metric
- Metric
entropy Metric
cost Metric
function Metric
, O
i.e. O
they O
had O
Bernoulli O
conditional O
likelihoods O
. O
We O
also O
compared O
our O
model O
with O
Fully Method
- Method
Connected Method
( O
Fully Method
- Method
Conn Method
) O
and O
Convolutional Method
- Method
Deconvolutional Method
( O
Conv Method
- Method
Deconv Method
) O
Variational Method
Autoencoders Method
which O
were O
trained O
with O
the O
least Method
squares Method
cost Method
function Method
. O
The O
LAPGAN Method
model Method
of O
denton_lapgan O
was O
trained O
on O
a O
two Method
level Method
Laplacian Method
Pyramid Method
with O
a O
GAN Method
as O
a O
top Method
layer Method
generator Method
and O
all O
stages O
were O
conditioned O
on O
the O
same O
skipthought O
vector O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
bottom O
row O
, O
we O
generated O
several O
samples O
from O
the O
prior O
of O
each O
of O
the O
current O
state O
- O
of O
- O
the O
- O
art O
generative Method
models Method
, O
conditioned O
on O
the O
caption O
“ O
A O
group O
of O
people O
walk O
on O
a O
beach O
with O
surf O
boards O
” O
. O
While O
all O
of O
the O
samples O
look O
sharp O
, O
the O
images O
generated O
by O
LAPGAN Method
look O
more O
noisy O
and O
it O
is O
harder O
to O
make O
out O
definite O
objects O
, O
whereas O
the O
images O
generated O
by O
variational Method
models Method
trained O
with O
least Method
squares Method
cost Method
function Method
have O
a O
watercolor O
effect O
on O
the O
images O
. O
We O
found O
that O
the O
quality O
of O
generated O
samples O
was O
similar O
among O
different O
variants O
of O
conditional Method
DRAW Method
models Method
. O
As O
for O
the O
quantitative O
comparison O
of O
different O
models O
, O
we O
first O
compare O
the O
performances O
of O
the O
model O
trained O
with O
variational Method
methods Method
. O
We O
rank O
the O
images O
in O
the O
test O
set O
conditioned O
on O
the O
captions O
based O
on O
the O
variational Metric
lower Metric
bound Metric
of O
the O
log O
- O
probabilities O
and O
then O
report O
the O
Precision Metric
- Metric
Recall Metric
metric Metric
as O
an O
evaluation O
of O
the O
quality O
of O
the O
generative Method
model Method
( O
see O
Table O
[ O
reference O
] O
. O
) O
. O
Perhaps O
unsurprisingly O
, O
generative Method
models Method
did O
not O
perform O
well O
on O
the O
image Task
retrieval Task
task Task
. O
To O
deal O
with O
the O
large O
computational Metric
complexity Metric
involved O
in O
looping O
through O
each O
test O
image O
, O
we O
create O
a O
shortlist O
of O
one O
hundred O
images O
including O
the O
correct O
one O
, O
based O
on O
the O
images O
having O
the O
closest O
Euclidean O
distance O
in O
the O
last O
fully O
- O
connected O
feature O
space O
of O
a O
VGG Method
- Method
like Method
model Method
simonyan_convnet Method
trained O
on O
the O
CIFAR O
dataset O
krizhevsky_cifar O
. O
Since O
there O
are O
“ O
easy O
” O
images O
for O
which O
the O
model O
assigns O
high O
log O
- O
probabilities O
independent O
of O
the O
query O
caption O
, O
we O
instead O
look O
at O
the O
ratio O
of O
the O
likelihood O
of O
the O
image O
conditioned O
on O
the O
sentence O
to O
the O
likelihood O
of O
the O
image O
conditioned O
on O
the O
mean O
sentence Method
representation Method
in O
the O
training O
set O
, O
following O
the O
retrieval Method
protocol Method
of O
kiros_captions O
. O
We O
also O
found O
that O
the O
lower O
bound O
on O
the O
test O
log O
- O
probabilities O
decreased O
for O
sharpened O
images O
, O
and O
that O
sharpening O
considerably O
hurt O
the O
retrieval Task
results O
. O
Since O
sharpening O
changes O
the O
statistics O
of O
images O
, O
the O
estimated O
log O
- O
probabilities O
of O
image O
pixels O
is O
not O
necessarily O
a O
good O
metric O
. O
Some O
examples O
of O
generated Task
images Task
before O
and O
after O
sharpening O
are O
shown O
in O
Appendix O
C. O
Instead O
of O
calculating O
error O
per O
pixel O
, O
we O
turn O
to O
a O
smarter O
metric O
, O
the O
Structural Metric
Similarity Metric
Index Metric
( O
SSI Metric
) O
wang_ssi O
, O
which O
incorporates O
luminance O
and O
contrast O
masking O
into O
the O
error Task
calculation Task
. O
Strong O
inter O
- O
dependencies O
of O
closer O
pixels O
are O
also O
taken O
into O
account O
and O
the O
metric O
is O
calculated O
on O
small O
windows O
of O
the O
images O
. O
Due O
to O
independence O
property O
of O
test O
captions O
, O
we O
sampled O
fifty O
images O
from O
the O
prior O
of O
each O
generative Method
model Method
for O
every O
caption O
in O
the O
test O
set O
in O
order O
to O
calculate O
SSI Metric
. O
As O
you O
can O
see O
on O
Table O
[ O
reference O
] O
, O
SSI Metric
scores Metric
achieved O
by O
variational Method
models Method
were O
higher O
compared O
to O
SSI Metric
score Metric
achieved O
by O
LAPGAN Method
. O
c O
—— O
c O
c O
c O
c O
c O
—— O
c O
c O
Microsoft Material
COCO Material
( O
prior O
to O
sharpening O
) O
Image Method
Retrieval Method
Image Method
Similarity Method
Model Method
R@1 O
R@5 O
R@10 O
R@50 O
Med O
r O
SSI Metric
LAPGAN Metric
- O
- O
- O
- O
- O
0.08 O
0.07 O
Fully Method
- Method
Conn Method
VAE Method
1.0 O
6.6 O
12.0 O
53.4 O
47 O
0.156 O
0.10 O
Conv Method
- Method
Deconv Method
VAE Method
1.0 O
6.5 O
12.0 O
52.9 O
48 O
0.164 O
0.10 O
skipthoughtDRAW O
2.0 O
11.2 O
18.9 O
63.3 O
36 O
0.157 O
0.11 O
noalignDRAW O
2.8 O
14.1 O
23.1 O
68.0 O
31 O
0.155 O
0.11 O
alignDRAW O
3.0 O
14.0 O
22.9 O
68.5 O
31 O
0.156 O
0.11 O
section O
: O
Discussion O
In O
this O
paper O
, O
we O
demonstrated O
that O
the O
alignDRAW Method
model Method
, O
a O
combination O
of O
a O
recurrent Method
variational Method
autoencoder Method
with O
an O
alignment Method
model Method
over O
words O
, O
succeeded O
in O
generating O
images O
that O
correspond O
to O
a O
given O
input O
caption O
. O
By O
extensively O
using O
attentional Method
mechanisms Method
, O
our O
model O
gained O
several O
advantages O
. O
Namely O
, O
the O
use O
of O
the O
visual Method
attention Method
mechanism Method
allowed O
us O
to O
decompose O
the O
problem O
of O
image Task
generation Task
into O
a O
series O
of O
steps O
instead O
of O
a O
single O
forward O
pass O
, O
while O
the O
attention O
over O
words O
provided O
us O
an O
insight O
whenever O
our O
model O
failed O
to O
generate O
a O
relevant O
image O
. O
Additionally O
, O
our O
model O
generated Task
images Task
corresponding O
to O
captions O
which O
generalized O
beyond O
the O
training O
set O
, O
such O
as O
sentences O
describing O
novel O
scenarios O
which O
are O
highly O
unlikely O
to O
occur O
in O
real O
life O
. O
Because O
the O
alignDRAW Method
model Method
tends O
to O
output O
slightly O
blurry O
samples O
, O
we O
augmented O
the O
model O
with O
a O
sharpening Method
post Method
- Method
processing Method
step Method
in O
which O
GAN Method
generated O
edges O
which O
were O
added O
to O
the O
alignDRAW O
samples O
. O
Unfortunately O
, O
this O
is O
not O
an O
ideal O
solution O
due O
to O
the O
fact O
that O
the O
whole O
model O
was O
not O
trained O
in O
an O
end O
- O
to O
- O
end O
fashion O
. O
Therefore O
a O
direction O
of O
future O
work O
would O
be O
to O
find O
methods O
that O
can O
bypass O
the O
separate O
post Method
- Method
processing Method
step Method
and O
output O
sharp O
images O
directly O
in O
an O
end O
- O
to O
- O
end O
manner O
. O
Acknowledgments O
: O
This O
work O
was O
supported O
by O
Samsung O
and O
IARPA O
, O
Raytheon O
BBN O
Contract O
No O
. O
D11PC20071 O
. O
We O
would O
like O
to O
thank O
developers O
of O
Theano O
the O
authors O
of O
for O
open O
sourcing O
their O
code O
, O
and O
Ryan O
Kiros O
and O
Nitish O
Srivastava O
for O
helpful O
discussions O
. O
bibliography O
: O
References O
appendix O
: O
Appendix O
A O
: O
MNIST O
With O
Captions O
As O
an O
additional O
experiment O
, O
we O
trained O
our O
model O
on O
the O
MNIST O
dataset O
with O
artificial O
captions O
. O
Either O
one O
or O
two O
digits O
from O
the O
MNIST O
training O
dataset O
were O
placed O
on O
a O
blank O
image O
. O
One O
digit O
was O
placed O
in O
one O
of O
the O
four O
( O
top O
- O
left O
, O
top O
- O
right O
, O
bottom O
- O
left O
or O
bottom O
- O
right O
) O
corners O
of O
the O
image O
. O
Two O
digits O
were O
either O
placed O
horizontally O
or O
vertically O
in O
non O
- O
overlapping O
fashion O
. O
The O
corresponding O
artificial O
captions O
specified O
the O
identity O
of O
each O
digit O
along O
with O
their O
relative O
positions O
, O
e.g. O
“ O
The O
digit O
three O
is O
at O
the O
top O
of O
the O
digit O
one O
” O
, O
or O
“ O
The O
digit O
seven O
is O
at O
the O
bottom O
left O
of O
the O
image O
” O
. O
The O
generated Task
images Task
together O
with O
the O
attention O
alignments O
are O
displayed O
in O
Figure O
[ O
reference O
] O
. O
The O
model O
correctly O
displayed O
the O
specified O
digits O
at O
the O
described O
positions O
and O
even O
managed O
to O
generalize O
reasonably O
well O
to O
the O
configurations O
that O
were O
never O
present O
during O
training O
. O
In O
the O
case O
of O
generating O
two O
digits O
, O
the O
model O
would O
dynamically O
attend O
to O
the O
digit O
in O
the O
caption O
it O
was O
drawing O
at O
that O
particular O
time O
- O
step O
. O
Similarly O
, O
in O
the O
setting O
where O
the O
caption O
specified O
only O
a O
single O
digit O
, O
the O
model O
would O
correctly O
attend O
to O
the O
digit O
in O
the O
caption O
during O
the O
whole O
generation Task
process Task
. O
In O
both O
cases O
, O
the O
model O
placed O
small O
attention O
values O
on O
the O
words O
describing O
the O
position O
of O
digits O
in O
the O
images O
. O
appendix O
: O
Appendix O
B O
: O
Training O
Details O
subsection O
: O
Hyperparameters O
Each O
parameter O
in O
alignDRAW O
was O
initialized O
by O
sampling O
from O
a O
Gaussian Method
distribution Method
with O
mean O
and O
standard O
deviation O
. O
The O
model O
was O
trained O
using O
RMSprop Method
with O
an O
initial O
learning Metric
rate Metric
of O
. O
For O
the O
Microsoft Material
COCO Material
task O
, O
we O
trained O
our O
model O
for O
epochs O
. O
The O
learning Metric
rate Metric
was O
reduced O
to O
after O
epochs O
. O
For O
the O
MNIST Task
with Task
Captions Task
task Task
, O
the O
model O
was O
trained O
for O
150 O
epochs O
and O
the O
learning Metric
rate Metric
was O
reduced O
to O
after O
110 O
epochs O
. O
During O
each O
epoch O
, O
randomly O
created O
training O
samples O
were O
used O
for O
learning Task
. O
The O
norm O
of O
the O
gradients O
was O
clipped O
at O
during O
training O
to O
avoid O
the O
exploding Task
gradients Task
problem Task
. O
We O
used O
a O
vocabulary O
size O
of O
and O
for O
the O
Microsoft Material
COCO Material
and O
MNIST O
with O
Captions O
datasets O
respectively O
. O
All O
capital O
letters O
in O
the O
words O
were O
converted O
to O
small O
letters O
as O
a O
preprocessing O
step O
. O
For O
all O
tasks O
, O
the O
hidden O
states O
and O
in O
the O
language Method
model Method
had O
units O
. O
Hence O
the O
dimensionality O
of O
the O
concatenated O
state O
of O
the O
Bidirectional Method
LSTM Method
was O
256 O
. O
The O
parameters O
in O
the O
align O
operator O
( O
Eq O
. O
[ O
reference O
] O
) O
had O
a O
dimensionality O
of O
, O
so O
that O
, O
, O
and O
. O
The O
architectural O
configurations O
of O
the O
alignDRAW Method
models Method
are O
shown O
in O
Table O
[ O
reference O
] O
. O
c O
— O
c O
c O
c O
c O
c O
c O
alignDRAW O
Model O
Task O
# O
glimpses O
Inference O
Generative O
Latent O
Read O
Size O
Write O
Size O
T O
Dim O
of O
Dim O
of O
Dim O
of O
MS Material
COCO Material
32 O
550 O
550 O
275 O
9 O
9 O
MNIST O
32 O
300 O
300 O
150 O
8 O
8 O
The O
GAN Method
model O
used O
for O
sharpening Task
had O
the O
same O
configuration O
as O
the O
model O
trained O
by O
on O
the O
edge O
residuals O
of O
the O
CIFAR O
dataset O
. O
The O
configuration O
can O
be O
found O
at O
. O
The O
model O
was O
trained O
for O
epochs O
. O
subsection O
: O
Evaluation O
Table O
[ O
reference O
] O
shows O
the O
estimated O
variational Metric
lower Metric
bounds Metric
on O
the O
average Metric
train Metric
/ Metric
validation Metric
/ Metric
test Metric
log Metric
- Metric
probabilities Metric
. O
Note O
that O
the O
alignDRAW Method
model Method
does O
not O
suffer O
much O
from O
overfitting O
. O
The O
results O
substantially O
worsen O
after O
sharpening O
test O
images O
. O
c O
— O
c O
c O
c O
c O
Model O
Train O
Validation O
Test O
Test O
( O
after O
sharpening O
) O
skipthoughtDRAW O
- O
1794.29 O
- O
1797.41 O
- O
1791.37 O
- O
2045.84 O
noalignDRAW O
- O
1792.14 O
- O
1796.94 O
- O
1791.15 O
- O
2051.07 O
alignDRAW O
- O
1792.15 O
- O
1797.24 O
- O
1791.53 O
- O
2042.31 O
appendix O
: O
Appendix O
C O
: O
Effect O
of O
Sharpening O
Images O
. O
Some O
examples O
of O
generated Task
images Task
before O
( O
top O
row O
) O
and O
after O
( O
bottom O
row O
) O
sharpening O
images O
using O
an O
adversarial Method
network Method
trained O
on O
residuals O
of O
a O
Laplacian Method
pyramid Method
conditioned O
on O
the O
skipthought O
vectors O
of O
the O
captions O
. O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
[ O
] O
