document O
: O
Positional Method
Encoding Method
to O
Control O
Output O
Sequence O
Length O
Neural Method
encoder Method
- Method
decoder Method
models Method
have O
been O
successful O
in O
natural Task
language Task
generation Task
tasks Task
. O
However O
, O
real O
applications O
of O
abstractive Task
summarization Task
must O
consider O
additional O
constraint O
that O
a O
generated O
summary O
should O
not O
exceed O
a O
desired O
length O
. O
In O
this O
paper O
, O
we O
propose O
a O
simple O
but O
effective O
extension O
of O
a O
sinusoidal Method
positional Method
encoding Method
to O
enable O
neural Method
encoder Method
- Method
decoder Method
model Method
to O
preserves O
the O
length O
constraint O
. O
Unlike O
in O
previous O
studies O
where O
that O
learn O
embeddings O
representing O
each O
length O
, O
the O
proposed O
method O
can O
generate O
a O
text O
of O
any O
length O
even O
if O
the O
target O
length O
is O
not O
present O
in O
training O
data O
. O
The O
experimental O
results O
show O
that O
the O
proposed O
method O
can O
not O
only O
control O
the O
generation O
length O
but O
also O
improve O
the O
ROUGE Metric
scores Metric
. O
section O
: O
Introduction O
Neural Method
encoder Method
- Method
decoder Method
models Method
have O
been O
successfully O
applied O
to O
various O
natural Task
language Task
generation Task
tasks Task
including O
machine Task
translation Task
, O
summarization Task
, O
and O
caption Task
generation Task
. O
Still O
, O
it O
is O
necessary O
to O
control O
the O
output O
length O
for O
abstractive Task
summarization Task
, O
which O
generates O
a O
summary O
for O
a O
given O
text O
while O
satisfying O
a O
space O
constraint O
. O
In O
fact O
, O
Figure O
[ O
reference O
] O
shows O
a O
large O
variance O
in O
output O
sequences O
produced O
by O
a O
widely O
used O
encoder Method
- Method
decoder Method
model Method
, O
which O
has O
no O
mechanism O
for O
controlling O
the O
length O
of O
the O
output O
sequences O
. O
W18 O
- O
2706 O
trained O
embeddings O
that O
correspond O
to O
each O
output O
length O
to O
control O
the O
output O
sequence O
length O
. O
Since O
the O
embeddings O
for O
different O
lengths O
are O
independent O
, O
it O
is O
hard O
to O
generate O
a O
sequence O
of O
the O
length O
that O
is O
infrequent O
in O
training O
data O
. O
Thus O
, O
a O
method O
that O
can O
model O
any O
lengths O
continuously O
is O
required O
. O
kikuchi O
- O
EtAl:2016:EMNLP2016 O
proposed O
two O
learning Method
based Method
methods Method
for O
an O
LSTM Method
encoder Method
- Method
decoder Method
: O
LenEmb Method
and O
LenInit Method
. O
LenEmb Method
inputs O
an O
embedding O
representing O
the O
remaining O
length O
in O
each O
decoding O
step O
. O
Since O
this O
approach O
also O
prepares O
embeddings O
for O
each O
length O
independently O
, O
it O
suffers O
from O
the O
same O
problem O
as O
that O
in O
W18 O
- O
2706 O
. O
On O
the O
other O
hand O
, O
LenInit Method
can O
handle O
arbitrary O
lengths O
because O
it O
combines O
the O
scalar O
value O
of O
a O
desired O
length O
with O
a O
trainable Method
embedding Method
. O
LenInit Method
initializes O
the O
LSTM Method
cell Method
of O
the O
decoder Method
with O
the O
embedding O
depending O
on O
the O
scalar O
value O
of O
the O
desired O
length O
. O
D18 O
- O
1444 O
incorporated O
such O
scalar O
values O
into O
the O
initial O
state O
of O
the O
decoder Method
in O
a O
CNN Method
encoder Method
- Method
decoder Method
. O
These O
approaches O
deal O
with O
any O
length O
but O
it O
is O
reasonable O
to O
incorporate O
the O
distance O
to O
the O
desired O
terminal O
position O
into O
each O
decoding O
step O
such O
as O
in O
LenEmb O
. O
In O
this O
study O
, O
we O
focused O
on O
Transformer Method
, O
which O
recently O
achieved O
the O
state O
- O
of O
- O
the O
- O
art O
score O
on O
the O
machine Task
translation Task
task Task
. O
We O
extend O
the O
sinusoidal Method
positional Method
encoding Method
, O
which O
represents O
a O
position O
of O
each O
token O
in O
Transformer O
, O
to O
represent O
a O
distance O
from O
a O
terminal O
position O
on O
the O
decoder O
side O
. O
In O
this O
way O
, O
the O
proposed O
method O
considers O
the O
remaining O
length O
explicitly O
at O
each O
decoding O
step O
. O
Moreover O
, O
the O
proposed O
method O
can O
handle O
any O
desired O
length O
regardless O
of O
its O
appearance O
in O
a O
training O
corpus O
because O
it O
uses O
the O
same O
continuous O
space O
for O
any O
length O
. O
We O
conduct O
experiments O
on O
the O
headline Task
generation Task
task Task
. O
The O
experimental O
results O
show O
that O
our O
proposed O
method O
is O
able O
to O
not O
only O
control O
the O
output O
length O
but O
also O
improve O
the O
ROUGE Metric
scores Metric
from O
the O
baselines O
. O
Our O
code O
and O
constructed O
test O
data O
are O
publicly O
available O
at O
: O
https: O
// O
github.com O
/ O
takase O
/ O
control O
- O
lengthhttps: O
// O
github.com O
/ O
takase O
/ O
control O
- O
length O
. O
section O
: O
Positional Method
Encoding Method
Transformer Method
uses O
a O
sinusoidal Method
positional Method
encoding Method
to O
represent O
the O
position O
of O
an O
input O
. O
Transformer Method
feeds O
the O
sum O
of O
the O
positional Method
encoding Method
and O
token Method
embedding Method
to O
the O
input O
layer O
of O
its O
encoder Method
and Method
decoder Method
. O
Let O
be O
the O
position O
and O
be O
the O
embedding O
size O
. O
Then O
, O
the O
- O
th O
dimension O
of O
the O
sinusoidal Method
positional Method
encoding Method
is O
as O
follows O
: O
In O
short O
, O
each O
dimension O
of O
the O
positional Method
encoding Method
corresponds O
to O
a O
sinusoid O
whose O
period O
is O
. O
Since O
this O
function O
returns O
an O
identical O
value O
at O
the O
same O
position O
, O
the O
above O
positional Method
encoding Method
can O
be O
interpreted O
as O
representing O
the O
absolute O
position O
of O
each O
input O
token O
. O
In O
this O
paper O
, O
we O
extend O
Equations O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
to O
depend O
on O
the O
given O
output O
length O
and O
the O
distance O
from O
the O
terminal O
position O
. O
We O
propose O
two O
extensions O
: O
length Method
- Method
difference Method
positional Method
encoding Method
( O
) O
and O
length Method
- Method
ratio Method
positional Method
encoding Method
( O
) O
. O
Then O
we O
replace O
Equations O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
with O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
( O
or O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
) O
on O
the O
decoder O
side O
to O
control O
the O
output O
sequence O
length O
. O
We O
define O
and O
as O
follows O
: O
where O
presents O
the O
given O
length O
constraint O
. O
returns O
an O
identical O
value O
at O
the O
position O
where O
the O
remaining O
length O
to O
the O
terminal O
position O
is O
the O
same O
. O
returns O
a O
similar O
value O
at O
the O
positions O
where O
the O
ratio O
of O
the O
remaining O
length O
to O
the O
terminal O
position O
is O
similar O
. O
Let O
us O
consider O
the O
- O
th O
dimension O
as O
the O
simplest O
example O
. O
Since O
we O
obtain O
( O
or O
) O
at O
this O
dimension O
, O
the O
equations O
yield O
the O
same O
value O
when O
the O
remaining O
length O
ratio O
is O
the O
same O
, O
e.g. O
, O
, O
and O
, O
. O
We O
add O
( O
or O
) O
to O
the O
input O
layer O
of O
Transformer Method
in O
the O
same O
manner O
as O
in O
NIPS2017_7181 O
. O
In O
the O
training O
step O
, O
we O
assign O
the O
length O
of O
the O
correct O
output O
to O
. O
In O
the O
test O
phase O
, O
we O
control O
the O
output O
length O
by O
assigning O
the O
desired O
length O
to O
. O
section O
: O
Experiments O
subsection O
: O
Datasets O
We O
conduct O
experiments O
on O
the O
headline Task
generation Task
task Task
on O
Japanese O
and O
English O
datasets O
. O
The O
purpose O
of O
the O
experiments O
is O
to O
evaluate O
the O
ability O
of O
the O
proposed O
method O
to O
generate O
a O
summary O
of O
good O
quality O
within O
a O
specified O
length O
. O
We O
used O
JAMUL O
corpus O
as O
the O
Japanese O
test O
set O
. O
This O
test O
set O
contains O
three O
kinds O
of O
headlines O
for O
1 O
, O
181 O
news O
articles O
written O
by O
professional O
editors O
under O
the O
different O
upper O
bounds O
of O
headline O
lengths O
. O
The O
upper O
bounds O
are O
10 O
, O
13 O
, O
and O
26 O
characters O
( O
) O
. O
This O
test O
set O
is O
suitable O
for O
simulating O
the O
real Task
process Task
of Task
news Task
production Task
because O
it O
is O
constructed O
by O
a O
Japanese O
media O
company O
. O
In O
contrast O
, O
we O
have O
no O
English O
test O
sets O
that O
contain O
headlines O
of O
multiple O
lengths O
. O
Thus O
, O
we O
randomly O
extracted O
3 O
, O
000 O
sentence O
- O
headline O
pairs O
that O
satisfy O
a O
length O
constraint O
from O
the O
test O
set O
constructed O
from O
annotated O
English O
Gigaword O
by O
pre O
- O
processing O
scripts O
of O
rush O
- O
chopra O
- O
weston:2015:EMNLP O
. O
We O
set O
three O
configurations O
for O
the O
number O
of O
characters O
as O
the O
length O
constraint O
: O
0 O
to O
30 O
characters O
( O
) O
, O
30 O
to O
50 O
characters O
( O
) O
, O
and O
50 O
to O
75 O
characters O
( O
) O
. O
Moreover O
, O
we O
also O
evaluate O
the O
proposed O
method O
on O
the O
DUC Material
- Material
2004 Material
task Material
1 Material
for O
comparison O
with O
published O
scores O
in O
previous O
studies O
. O
Unfortunately O
, O
we O
have O
no O
large O
supervision O
data O
with O
multiple O
headlines O
of O
different O
lengths O
associated O
with O
each O
news O
article O
in O
both O
languages O
. O
Thus O
, O
we O
trained O
the O
proposed O
method O
on O
pairs O
with O
a O
one O
- O
to O
- O
one O
correspondences O
between O
the O
source O
articles O
and O
headlines O
. O
In O
the O
training O
step O
, O
we O
regarded O
the O
length O
of O
the O
target O
headline O
as O
the O
desired O
length O
. O
For O
Japanese O
, O
we O
used O
the O
JNC O
corpus O
, O
which O
contains O
a O
pair O
of O
the O
lead O
three O
sentences O
of O
a O
news O
article O
and O
its O
headline O
. O
The O
training O
set O
contains O
about O
1.6 O
M O
pairs O
. O
For O
English O
, O
we O
used O
sentence O
- O
headline O
pairs O
extracted O
from O
the O
annotated O
English O
Gigaword O
with O
the O
same O
pre O
- O
processing O
script O
used O
in O
the O
construction O
of O
the O
test O
set O
. O
The O
training O
set O
contains O
about O
3.8 O
M O
pairs O
. O
In O
this O
paper O
, O
we O
used O
a O
character Method
- Method
level Method
decoder Method
to O
control O
the O
number O
of O
characters O
. O
On O
the O
encoder O
side O
, O
we O
used O
subword O
units O
to O
construct O
the O
vocabulary O
. O
We O
set O
the O
hyper O
- O
parameter O
to O
fit O
the O
vocabulary O
size O
to O
about O
8k O
for O
Japanese O
and O
16k O
for O
English O
. O
subsection O
: O
Baselines O
We O
implemented O
two O
methods O
proposed O
by O
previous O
studies O
to O
control O
the O
output O
length O
and O
handle O
arbitrary O
lengths O
. O
We O
employed O
them O
and O
Transformer Method
as O
baselines O
. O
paragraph O
: O
LenInit O
kikuchi O
- O
EtAl:2016:EMNLP2016 O
proposed O
LenInit Method
, O
which O
controls O
the O
output O
length O
by O
initializing O
the O
LSTM Method
cell Method
of O
the O
decoder Method
as O
follows O
: O
where O
is O
a O
trainable O
vector O
. O
We O
incorporated O
this O
method O
with O
a O
widely O
used O
LSTM Method
encoder Method
- Method
decoder Method
model Method
. O
For O
a O
fair O
comparison O
, O
we O
set O
the O
same O
hyper O
- O
parameters O
as O
in O
D18 O
- O
1489 O
because O
they O
indicated O
that O
the O
LSTM Method
encoder Method
- Method
decoder Method
model Method
trained O
with O
the O
hyper O
- O
parameters O
achieved O
a O
similar O
performance O
to O
the O
state O
- O
of O
- O
the O
- O
art O
on O
the O
headline Task
generation Task
. O
paragraph O
: O
Length O
Control O
( O
LC O
) O
D18 O
- O
1444 O
proposed O
a O
length Method
control Method
method Method
that O
multiplies O
the O
desired O
length O
by O
input O
token O
embeddings O
. O
We O
trained O
the O
model O
with O
their O
hyper O
- O
parameters O
. O
paragraph O
: O
Transformer Method
Our O
proposed O
method O
is O
based O
on O
Transformer Method
. O
We O
trained O
Transformer Method
with O
the O
equal O
hyper O
- O
parameters O
as O
in O
the O
base O
model O
in O
NIPS2017_7181 O
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
shows O
the O
recall O
- O
oriented O
ROUGE Metric
- Metric
1 Metric
( O
R Metric
- Metric
1 Metric
) O
, O
2 O
( O
R Metric
- Metric
2 Metric
) O
, O
and O
L O
( O
R Metric
- Metric
L Metric
) O
scores O
of O
each O
method O
on O
the O
Japanese O
test O
set O
. O
This O
table O
indicates O
that O
Transformer Method
with O
the O
proposed O
method O
( O
Transformer Method
+ Method
and O
Transformer Method
+ Method
) O
outperformed O
the O
baselines O
for O
all O
given O
constraints O
( O
) O
. O
Transformer Method
+ Method
performed O
slightly O
better O
than O
Transformer Method
+ Method
. O
Moreover O
, O
we O
improved O
the O
performance O
by O
incorporating O
the O
standard O
sinusoidal Method
positional Method
encoding Method
( O
+ O
) O
on O
and O
. O
The O
results O
imply O
that O
the O
absolute O
position O
also O
helps O
to O
generate O
better O
headlines O
while O
controlling O
the O
output O
length O
. O
Table O
[ O
reference O
] O
shows O
the O
recall O
- O
oriented O
ROUGE Metric
scores Metric
on O
the O
English O
Gigaword O
test O
set O
. O
This O
table O
indicates O
that O
and O
significantly O
improved O
the O
performance O
on O
. O
Moreover O
, O
the O
absolute O
position O
( O
) O
also O
improved O
the O
performance O
in O
this O
test O
set O
. O
In O
particular O
, O
was O
very O
effective O
in O
the O
setting O
of O
very O
short O
headlines O
( O
) O
. O
However O
, O
the O
proposed O
method O
slightly O
lowered O
ROUGE Metric
- Metric
2 Metric
scores Metric
from O
the O
bare O
Transformer O
on O
. O
We O
infer O
that O
the O
bare Method
Transformer Method
can O
generate O
headlines O
whose O
lengths O
are O
close O
to O
30 O
and O
50 O
because O
the O
majority O
of O
the O
training O
set O
consists O
of O
headlines O
whose O
lengths O
are O
less O
than O
or O
equal O
to O
50 O
. O
However O
, O
most O
of O
the O
generated O
headlines O
breached O
the O
length O
constraints O
, O
as O
explained O
in O
Section O
[ O
reference O
] O
. O
To O
investigate O
whether O
the O
proposed O
method O
can O
generate O
good O
headlines O
for O
unseen O
lengths O
, O
we O
excluded O
headlines O
whose O
lengths O
are O
equal O
to O
the O
desired O
length O
( O
) O
from O
the O
training O
data O
. O
The O
lower O
parts O
of O
Table O
[ O
reference O
] O
and O
[ O
reference O
] O
show O
ROUGE Metric
scores Metric
of O
the O
proposed O
method O
trained O
on O
the O
modified O
training O
data O
. O
These O
parts O
show O
that O
the O
proposed O
method O
achieved O
comparable O
scores O
to O
ones O
trained O
on O
whole O
training O
dataset O
. O
These O
results O
indicate O
that O
the O
proposed O
method O
can O
generate O
high O
- O
quality O
headlines O
even O
if O
the O
length O
does O
not O
appear O
in O
the O
training O
data O
. O
Table O
[ O
reference O
] O
shows O
the O
recall O
- O
oriented O
ROUGE Metric
scores Metric
on O
the O
DUC Material
- Material
2004 Material
test Material
set Material
. O
Following O
the O
evaluation O
protocol O
, O
we O
truncated O
characters O
over O
75 O
bytes O
. O
The O
table O
indicates O
that O
and O
significantly O
improved O
the O
performance O
compared O
to O
the O
bare O
Transformer O
, O
and O
achieved O
better O
performance O
than O
the O
baselines O
except O
for O
R Metric
- Metric
2 Metric
of O
LenInit Method
. O
This O
table O
also O
shows O
the O
scores O
reported O
in O
the O
previous O
studies O
. O
The O
proposed O
method O
outperformed O
the O
previous O
methods O
that O
control O
the O
output O
length O
and O
achieved O
the O
competitive O
score O
to O
the O
state O
- O
of O
- O
the O
- O
art O
scores O
. O
Since O
the O
proposed O
method O
consists O
of O
a O
character Method
- Method
based Method
decoder Method
, O
it O
sometimes O
generated O
words O
unrelated O
to O
a O
source O
sentence O
. O
Thus O
, O
we O
applied O
a O
simple O
re Method
- Method
ranking Method
to O
each O
- O
best O
headlines O
generated O
by O
the O
proposed O
method O
( O
in O
this O
experiment O
) O
based O
on O
the O
contained O
words O
. O
Our O
re Method
- Method
ranking Method
strategy Method
selects O
a O
headline O
that O
contains O
source O
- O
side O
words O
the O
most O
. O
Table O
[ O
reference O
] O
shows O
that O
Transformer Method
+ Method
+ Method
with O
this O
re Method
- Method
ranking Method
( O
+ O
Re Metric
- Metric
ranking Metric
) O
achieved O
better O
scores O
than O
the O
state O
- O
of O
- O
the O
- O
art O
. O
subsection O
: O
Analysis O
of O
Output Metric
Length Metric
Following O
D18 O
- O
1444 O
, O
we O
used O
the O
variance O
of O
the O
generated O
summary O
lengths O
against O
the O
desired O
lengths O
as O
an O
indicator O
of O
the O
preciseness O
of O
the O
output O
lengths O
. O
We O
calculated O
variance O
( O
) O
for O
generated O
summaries O
as O
follows O
: O
where O
is O
the O
desired O
length O
and O
is O
the O
length O
of O
the O
generated O
summary O
. O
Table O
[ O
reference O
] O
shows O
the O
values O
of O
Equation O
( O
[ O
reference O
] O
) O
computed O
for O
each O
method O
and O
the O
desired O
lengths O
. O
This O
table O
indicates O
that O
could O
control O
the O
length O
of O
headlines O
precisely O
. O
In O
particular O
, O
could O
generate O
headlines O
with O
the O
identical O
length O
to O
the O
desired O
one O
in O
comparison O
with O
LenInit Method
and O
LC Method
. O
also O
generated O
headlines O
with O
a O
precise O
length O
but O
its O
variance O
is O
larger O
than O
those O
of O
previous O
studies O
in O
very O
short O
lengths O
, O
i.e. O
, O
and O
in O
Japanese O
. O
However O
, O
we O
consider O
is O
enough O
for O
real O
applications O
because O
the O
averaged O
difference O
between O
its O
output O
and O
the O
desired O
length O
is O
small O
, O
e.g. O
, O
for O
. O
The O
lower O
part O
of O
Table O
[ O
reference O
] O
shows O
the O
variances O
of O
the O
proposed O
method O
trained O
on O
the O
modified O
training O
data O
that O
does O
not O
contain O
headlines O
whose O
lengths O
are O
equal O
to O
the O
desired O
length O
, O
similar O
to O
the O
lower O
parts O
of O
Table O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
The O
variances O
for O
this O
part O
are O
comparable O
to O
the O
ones O
obtained O
when O
we O
trained O
the O
proposed O
method O
with O
whole O
training O
dataset O
. O
This O
fact O
indicates O
that O
the O
proposed O
method O
can O
generate O
an O
output O
that O
satisfies O
the O
constraint O
of O
the O
desired O
length O
even O
if O
the O
training O
data O
does O
not O
contain O
instances O
of O
such O
a O
length O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
length Method
- Method
dependent Method
positional Method
encodings Method
, O
and O
, O
that O
can O
control O
the O
output O
sequence O
length O
in O
Transformer O
. O
The O
experimental O
results O
demonstrate O
that O
the O
proposed O
method O
can O
generate O
a O
headline O
with O
the O
desired O
length O
even O
if O
the O
desired O
length O
is O
not O
present O
in O
the O
training O
data O
. O
Moreover O
, O
the O
proposed O
method O
significantly O
improved O
the O
quality Metric
of O
headlines Task
on O
the O
Japanese Task
headline Task
generation Task
task Task
while O
preserving O
the O
given O
length O
constraint O
. O
For O
English O
, O
the O
proposed O
method O
also O
generated O
headlines O
with O
the O
desired O
length O
precisely O
and O
achieved O
the O
top O
ROUGE Metric
scores Metric
on O
the O
DUC Material
- Material
2004 Material
test Material
set Material
. O
section O
: O
Acknowledgments O
The O
research O
results O
have O
been O
achieved O
by O
“ O
Research O
and O
Development O
of O
Deep Method
Learning Method
Technology Method
for O
Advanced Task
Multilingual Task
Speech Task
Translation Task
” O
, O
the O
Commissioned O
Research O
of O
National O
Institute O
of O
Information O
and O
Communications O
Technology O
( O
NICT O
) O
, O
Japan O
. O
bibliography O
: O
References O
