document O
: O
Transformer Method
- Method
XL Method
: O
Attentive Method
Language Method
Models Method
Beyond O
a O
Fixed O
- O
Length O
Context O
Transformer Method
networks Method
have O
a O
potential O
of O
learning Task
longer Task
- Task
term Task
dependency Task
, O
but O
are O
limited O
by O
a O
fixed O
- O
length O
context O
in O
the O
setting O
of O
language Task
modeling Task
. O
As O
a O
solution O
, O
we O
propose O
a O
novel O
neural Method
architecture Method
, O
Transformer Method
- Method
XL Method
, O
that O
enables O
Transformer Method
to O
learn O
dependency O
beyond O
a O
fixed O
length O
without O
disrupting O
temporal O
coherence O
. O
Concretely O
, O
it O
consists O
of O
a O
segment Method
- Method
level Method
recurrence Method
mechanism Method
and O
a O
novel O
positional Method
encoding Method
scheme Method
. O
Our O
method O
not O
only O
enables O
capturing Task
longer Task
- Task
term Task
dependency Task
, O
but O
also O
resolves O
the O
problem O
of O
context O
fragmentation O
. O
As O
a O
result O
, O
Transformer Method
- Method
XL Method
learns O
dependency O
that O
is O
about O
80 O
% O
longer O
than O
RNNs Method
and O
450 O
% O
longer O
than O
vanilla Method
Transformers Method
, O
achieves O
better O
performance O
on O
both O
short O
and O
long O
sequences O
, O
and O
is O
up O
to O
1 O
, O
800 O
+ O
times O
faster O
than O
vanilla Method
Transformer Method
during O
evaluation O
. O
Additionally O
, O
we O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
( O
SoTA O
) O
results O
of O
bpc Metric
/ O
perplexity Metric
from O
1.06 O
to O
0.99 O
on O
enwiki8 Material
, O
from O
1.13 O
to O
1.08 O
on O
text8 Material
, O
from O
20.5 O
to O
18.3 O
on O
WikiText Material
- Material
103 Material
, O
from O
23.7 O
to O
21.8 O
on O
One Material
Billion Material
Word Material
, O
and O
from O
55.3 O
to O
54.5 O
on O
Penn Material
Treebank Material
( O
without O
finetuning O
) O
. O
Our O
code O
, O
pretrained Method
models Method
, O
and O
hyperparameters Method
are O
available O
in O
both O
Tensorflow Method
and O
PyTorch Method
. O
section O
: O
Introduction O
Language Task
modeling Task
is O
among O
the O
important O
problems O
that O
require O
modeling Task
long Task
- Task
term Task
dependency Task
, O
with O
successful O
applications O
such O
as O
unsupervised Task
pretraining Task
dai2015semi O
, O
peters2018deep O
, O
radford2018improving O
, O
devlin2018bert O
. O
However O
, O
it O
has O
been O
a O
challenge O
to O
equip O
neural Method
networks Method
with O
the O
capability O
to O
model O
long Task
- Task
term Task
dependency Task
in O
sequential O
data O
. O
Recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
, O
in O
particular O
Long Method
Short Method
- Method
Term Method
Memory Method
( O
LSTM Method
) O
networks O
hochreiter1997long O
, O
have O
been O
a O
standard O
solution O
to O
language Task
modeling Task
and O
obtained O
strong O
results O
on O
multiple O
benchmarks O
. O
Despite O
the O
wide O
adaption O
, O
RNNs Method
are O
difficult O
to O
optimize O
due O
to O
gradient O
vanishing O
and O
explosion O
hochreiter2001gradient O
, O
and O
the O
introduction O
of O
gating Method
in O
LSTMs Method
and O
the O
gradient Method
clipping Method
technique Method
graves2013generating O
, O
pascanu2012understanding O
might O
not O
be O
sufficient O
to O
fully O
address O
this O
issue O
. O
Empirically O
, O
previous O
work O
has O
found O
that O
LSTM Method
language O
models O
use O
200 O
context O
words O
on O
average O
khandelwal2018sharp O
, O
indicating O
room O
for O
further O
improvement O
. O
On O
the O
other O
hand O
, O
the O
direct O
connections O
between O
long O
- O
distance O
word O
pairs O
baked O
in O
attention Method
mechanisms Method
might O
ease O
optimization Task
and O
enable O
the O
learning Task
of Task
long Task
- Task
term Task
dependency Task
bahdanau2014neural Task
, O
vaswani2017attention O
. O
Recently O
, O
designed O
a O
set O
of O
auxiliary O
losses O
to O
train O
deep Method
Transformer Method
networks Method
for O
character Method
- Method
level Method
language Method
modeling Method
, O
which O
outperform O
LSTMs Method
by O
a O
large O
margin O
. O
Despite O
the O
success O
, O
the O
LM Method
training Method
in O
is O
performed O
on O
separated O
fixed O
- O
length O
segments O
of O
a O
few O
hundred O
characters O
, O
without O
any O
information O
flow O
across O
segments O
. O
As O
a O
consequence O
of O
the O
fixed O
context O
length O
, O
the O
model O
can O
not O
capture O
any O
longer O
- O
term O
dependency O
beyond O
the O
predefined O
context O
length O
. O
In O
addition O
, O
the O
fixed O
- O
length O
segments O
are O
created O
by O
selecting O
a O
consecutive O
chunk O
of O
symbols O
without O
respecting O
the O
sentence O
or O
any O
other O
semantic O
boundary O
. O
Hence O
, O
the O
model O
lacks O
necessary O
contextual O
information O
needed O
to O
well O
predict O
the O
first O
few O
symbols O
, O
leading O
to O
inefficient O
optimization Task
and O
inferior O
performance O
. O
We O
refer O
to O
this O
problem O
as O
context Task
fragmentation Task
. O
To O
address O
the O
aforementioned O
limitations O
of O
fixed O
- O
length O
contexts O
, O
we O
propose O
a O
new O
architecture O
called O
Transformer Method
- Method
XL Method
( O
meaning O
extra O
long O
) O
. O
We O
introduce O
the O
notion O
of O
recurrence O
into O
our O
deep Method
self Method
- Method
attention Method
network Method
. O
In O
particular O
, O
instead O
of O
computing O
the O
hidden O
states O
from O
scratch O
for O
each O
new O
segment O
, O
we O
reuse O
the O
hidden O
states O
obtained O
in O
previous O
segments O
. O
The O
reused O
hidden O
states O
serve O
as O
memory O
for O
the O
current O
segment O
, O
which O
builds O
up O
a O
recurrent O
connection O
between O
the O
segments O
. O
As O
a O
result O
, O
modeling Task
very Task
long Task
- Task
term Task
dependency Task
becomes O
possible O
because O
information O
can O
be O
propagated O
through O
the O
recurrent O
connections O
. O
Meanwhile O
, O
passing O
information O
from O
the O
previous O
segment O
can O
also O
resolve O
the O
problem O
of O
context Task
fragmentation Task
. O
More O
importantly O
, O
we O
show O
the O
necessity O
of O
using O
relative Method
positional Method
encodings Method
rather O
than O
absolute O
ones O
, O
in O
order O
to O
enable O
state Task
reuse Task
without O
causing O
temporal O
confusion O
. O
Hence O
, O
as O
an O
additional O
technical O
contribution O
, O
we O
introduce O
a O
simple O
but O
more O
effective O
relative Method
positional Method
encoding Method
formulation Method
that O
generalizes O
to O
attention O
lengths O
longer O
than O
the O
one O
observed O
during O
training O
. O
Transformer Method
- Method
XL Method
obtained O
strong O
results O
on O
five O
datasets O
, O
varying O
from O
word O
- O
level O
to O
character Method
- Method
level Method
language Method
modeling Method
. O
Transformer Method
- Method
XL Method
improves O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
( O
SoTA Method
) O
results O
from O
1.06 O
to O
0.99 O
in O
bpc Metric
on O
enwiki8 Material
, O
from O
1.13 O
to O
1.08 O
in O
bpc Metric
on O
text8 Material
, O
from O
20.5 O
to O
18.3 O
in O
perplexity Metric
on O
WikiText Material
- Material
103 Material
, O
and O
from O
23.7 O
to O
21.8 O
in O
perplexity Metric
on O
One Material
Billion Material
Word Material
. O
On O
small O
data O
, O
Transformer Method
- Method
XL Method
also O
achieves O
a O
perplexity Metric
of O
54.5 O
on O
Penn Material
Treebank Material
without O
finetuning Task
, O
which O
is O
SoTA O
when O
comparable O
settings O
are O
considered O
. O
We O
use O
two O
methods O
to O
quantitatively O
study O
the O
effective O
lengths O
of O
Transformer Method
- Method
XL Method
and O
the O
baselines O
. O
Similar O
to O
, O
we O
gradually O
increase O
the O
attention O
length O
at O
test O
time O
until O
no O
further O
noticeable O
improvement O
( O
0.1 O
% O
relative O
gains O
) O
can O
be O
observed O
. O
Our O
best O
model O
in O
this O
settings O
use O
attention O
lengths O
of O
1 O
, O
600 O
and O
3 O
, O
800 O
on O
WikiText Material
- Material
103 Material
and O
enwiki8 Material
respectively O
. O
In O
addition O
, O
we O
devise O
a O
metric O
called O
Relative Metric
Effective Metric
Context Metric
Length Metric
( O
RECL Method
) O
that O
aims O
to O
perform O
a O
fair O
comparison O
of O
the O
gains O
brought O
by O
increasing O
the O
context O
lengths O
for O
different O
models O
. O
In O
this O
setting O
, O
Transformer Method
- Method
XL Method
learns O
a O
RECL O
of O
900 O
words O
on O
WikiText Material
- Material
103 Material
, O
while O
the O
numbers O
for O
recurrent Method
networks Method
and O
Transformer Method
are O
only O
500 O
and O
128 O
. O
section O
: O
Related O
Work O
In O
the O
last O
few O
years O
, O
the O
field O
of O
language Task
modeling Task
has O
witnessed O
many O
significant O
advances O
, O
including O
but O
not O
limited O
to O
devising O
novel O
architectures O
to O
better O
encode O
the O
context O
bengio2003neural O
, O
mikolov2010recurrent O
, O
zilly2016recurrent O
, O
krause2016multiplicative O
, O
grave2016improving O
, O
dauphin2016language O
, O
chung2016hierarchical O
, O
merity2016pointer O
, O
kalchbrenner2016neural O
, O
al2018character O
, O
improving O
regularization Method
and O
optimization Method
algorithms Method
, O
speeding O
up O
the O
Softmax Method
computation Method
morin2005hierarchical O
, O
kuchaiev2017factorization O
, O
grave2016efficient O
, O
jozefowicz2016exploring O
, O
and O
enriching O
the O
output O
distribution O
family O
yang2017breaking O
, O
kanai2018sigsoftmax O
. O
To O
capture O
the O
long O
- O
range O
context O
in O
language Task
modeling Task
, O
a O
line O
of O
work O
directly O
feeds O
a O
representation O
of O
the O
wider O
context O
into O
the O
network O
as O
an O
additional O
input O
. O
Existing O
works O
range O
from O
ones O
where O
context Method
representations Method
are O
manually O
defined O
mikolov2012context O
, O
ji2015document O
, O
wang2015larger O
to O
others O
that O
rely O
on O
document O
- O
level O
topics O
learned O
from O
data O
dieng2016topicrnn O
, O
wang2017topic O
. O
More O
broadly O
, O
in O
generic Task
sequence Task
modeling Task
, O
how O
to O
capture O
long Task
- Task
term Task
dependency Task
has O
been O
a O
long O
- O
standing O
research O
problem O
. O
From O
this O
perspective O
, O
since O
the O
ubiquitous O
adaption O
of O
LSTM Method
, O
many O
efforts O
have O
been O
spent O
on O
relieving O
the O
vanishing Task
gradient Task
problem Task
, O
including O
better O
initialization O
le2015simple O
, O
additional O
loss Method
signal Method
trinh2018learning O
, O
augmented Method
memory Method
structure Method
ke2018sparse O
and O
others O
that O
modify O
the O
internal Method
architecture Method
of O
RNNs Method
to O
ease O
the O
optimization Task
. O
Different O
from O
them O
, O
our O
work O
is O
based O
on O
the O
Transformer Method
architecture Method
and O
shows O
that O
language Task
modeling Task
as O
a O
real O
- O
world Task
task Task
benefits O
from O
the O
ability O
to O
learn O
longer O
- O
term O
dependency O
. O
section O
: O
Model O
Given O
a O
corpus O
of O
tokens O
, O
the O
task O
of O
language Task
modeling Task
is O
to O
estimate O
the O
joint O
probability O
, O
which O
is O
often O
auto Method
- Method
regressively Method
factorized Method
as O
. O
With O
the O
factorization Method
, O
the O
problem O
reduces O
to O
estimating O
each O
conditional O
factor O
. O
In O
this O
work O
, O
we O
stick O
to O
the O
standard O
neural Method
approach Method
to O
modeling O
the O
conditional O
probability O
. O
Specifically O
, O
a O
trainable Method
neural Method
network Method
is O
used O
to O
encode O
the O
context O
into O
a O
fixed O
size O
hidden O
state O
, O
which O
is O
multiplied O
with O
the O
word O
embeddings O
to O
obtain O
the O
logits O
. O
The O
logits O
are O
then O
fed O
into O
the O
Softmax Method
function Method
, O
yielding O
a O
categorical O
probability O
distribution O
over O
the O
next O
token O
. O
subsection O
: O
Vanilla Method
Transformer Method
Language Method
Models Method
In O
order O
to O
apply O
Transformer Task
or Task
self Task
- Task
attention Task
to O
language Task
modeling Task
, O
the O
central O
problem O
is O
how O
to O
train O
a O
Transformer Method
to O
effectively O
encode O
an O
arbitrarily O
long O
context O
into O
a O
fixed Method
size Method
representation Method
. O
Given O
infinite O
memory O
and O
computation O
, O
a O
simple O
solution O
would O
be O
to O
process O
the O
entire O
context O
sequence O
using O
an O
unconditional Method
Transformer Method
decoder Method
, O
similar O
to O
a O
feed Method
- Method
forward Method
neural Method
network Method
. O
However O
, O
this O
is O
usually O
infeasible O
with O
the O
limited O
resource O
in O
practice O
. O
[ O
b O
] O
0.292 O
[ O
b O
] O
0.69 O
One O
feasible O
but O
crude O
approximation O
is O
to O
split O
the O
entire O
corpus O
into O
shorter O
segments O
of O
manageable O
sizes O
, O
and O
only O
train O
the O
model O
within O
each O
segment O
, O
ignoring O
all O
contextual O
information O
from O
previous O
segments O
. O
This O
is O
the O
idea O
adopted O
by O
al2018character O
. O
We O
call O
it O
the O
vanilla Method
model Method
and O
visualize O
it O
in O
Fig O
. O
[ O
reference O
] O
. O
Under O
this O
training O
paradigm O
, O
information O
never O
flows O
across O
segments O
in O
either O
the O
forward O
or O
backward O
pass O
. O
There O
are O
two O
critical O
limitations O
of O
using O
a O
fixed O
- O
length O
context O
. O
First O
, O
the O
largest O
possible O
dependency O
length O
is O
upper O
bounded O
by O
the O
segment O
length O
, O
which O
is O
a O
few O
hundred O
on O
character Method
- Method
level Method
language Method
modeling Method
al2018character O
. O
Therefore O
, O
although O
the O
self Method
- Method
attention Method
mechanism Method
is O
less O
affected O
by O
the O
vanishing Task
gradient Task
problem Task
compared O
to O
RNNs Method
, O
the O
vanilla Method
model Method
is O
not O
able O
to O
fully O
exploit O
this O
optimization O
advantage O
. O
Second O
, O
though O
it O
is O
possible O
to O
use O
padding O
to O
respect O
the O
sentence O
or O
other O
semantic O
boundaries O
, O
in O
practice O
it O
has O
been O
standard O
practice O
to O
simply O
chunk O
long O
text O
into O
fixed O
- O
length O
segments O
due O
to O
improved O
efficiency O
peters2018deep O
, O
devlin2018bert O
, O
al2018character O
. O
However O
, O
simply O
chunking O
a O
sequence O
into O
fixed O
- O
length O
segments O
will O
lead O
to O
the O
context Task
fragmentation Task
problem Task
as O
discussed O
in O
Section O
[ O
reference O
] O
. O
During O
evaluation Task
, O
at O
each O
step O
, O
the O
vanilla Method
model Method
also O
consumes O
a O
segment O
of O
the O
same O
length O
as O
in O
training O
, O
but O
only O
makes O
one O
prediction O
at O
the O
last O
position O
. O
Then O
, O
at O
the O
next O
step O
, O
the O
segment O
is O
shifted O
to O
the O
right O
by O
only O
one O
position O
, O
and O
the O
new O
segment O
has O
to O
be O
processed O
all O
from O
scratch O
. O
As O
shown O
in O
Fig O
. O
[ O
reference O
] O
, O
this O
procedure O
ensures O
that O
each O
prediction O
utilizes O
the O
longest O
possible O
context O
exposed O
during O
training O
, O
and O
also O
relieves O
context O
fragmentation O
issue O
encountered O
in O
training O
. O
However O
, O
this O
evaluation O
procedure O
is O
extremely O
expensive O
. O
We O
will O
show O
that O
our O
proposed O
architecture O
is O
able O
to O
substantially O
improve O
the O
evaluation Metric
speed Metric
. O
subsection O
: O
Segment Method
- Method
Level Method
Recurrence Method
with O
State Method
Reuse Method
[ O
b O
] O
0.63 O
[ O
b O
] O
0.35 O
To O
address O
the O
limitations O
of O
using O
a O
fixed O
- O
length O
context O
, O
we O
propose O
to O
introduce O
a O
recurrence Method
mechanism Method
to O
the O
Transformer Method
architecture Method
. O
During O
training Task
, O
the O
hidden O
state O
sequence O
computed O
for O
the O
previous O
segment O
is O
fixed O
and O
cached O
to O
be O
reused O
as O
an O
extended O
context O
when O
the O
model O
processes O
the O
next O
new O
segment O
, O
as O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
Although O
the O
gradient O
still O
remains O
within O
a O
segment O
, O
this O
additional O
input O
allows O
the O
network O
to O
exploit O
information O
in O
the O
history O
, O
leading O
to O
an O
ability O
of O
modeling O
longer Task
- Task
term Task
dependency Task
and O
avoiding O
context O
fragmentation O
. O
Formally O
, O
let O
the O
two O
consecutive O
segments O
of O
length O
be O
and O
respectively O
. O
Denoting O
the O
- O
th O
layer O
hidden O
state O
sequence O
produced O
for O
the O
- O
th O
segment O
by O
, O
where O
is O
the O
hidden O
dimension O
. O
Then O
, O
the O
- O
th O
layer O
hidden O
state O
for O
segment O
is O
produced O
( O
schematically O
) O
as O
follows O
, O
where O
the O
function O
stands O
for O
stop O
- O
gradient O
, O
the O
notation O
indicates O
the O
concatenation O
of O
two O
hidden O
sequences O
along O
the O
length O
dimension O
, O
and O
denotes O
model O
parameters O
. O
Compared O
to O
the O
standard O
Transformer Method
, O
the O
critical O
difference O
lies O
in O
that O
the O
key O
and O
value O
are O
conditioned O
on O
the O
extended O
context O
and O
hence O
cached O
from O
the O
previous O
segment O
. O
We O
emphasize O
this O
particular O
design O
by O
the O
green O
paths O
in O
Fig O
. O
[ O
reference O
] O
. O
With O
this O
recurrence Method
mechanism Method
applied O
to O
every O
two O
consecutive O
segments O
of O
a O
corpus O
, O
it O
essentially O
creates O
a O
segment O
- O
level O
recurrence O
in O
the O
hidden O
states O
. O
As O
a O
result O
, O
the O
effective O
context O
being O
utilized O
can O
go O
way O
beyond O
just O
two O
segments O
. O
However O
, O
notice O
that O
the O
recurrent O
dependency O
between O
and O
shifts O
one O
layer O
downwards O
per O
- O
segment O
, O
which O
differs O
from O
the O
same O
- O
layer O
recurrence O
in O
conventional O
RNN Method
- Method
LMs Method
. O
Consequently O
, O
the O
largest O
possible O
dependency O
length O
grows O
linearly O
w.r.t O
. O
the O
number O
of O
layers O
as O
well O
as O
the O
segment O
length O
, O
i.e. O
, O
, O
as O
visualized O
by O
the O
shaded O
area O
in O
Fig O
. O
[ O
reference O
] O
. O
This O
is O
analogous O
to O
truncated Method
BPTT Method
mikolov2010recurrent O
, O
a O
technique O
developed O
for O
training O
RNN Method
- Method
LMs Method
. O
However O
, O
different O
from O
truncated Method
BPTT Method
, O
our O
method O
caches O
a O
sequence O
of O
hidden O
states O
instead O
of O
the O
last O
one O
, O
and O
should O
be O
applied O
together O
with O
the O
relative Method
positional Method
encoding Method
technique Method
described O
in O
Section O
[ O
reference O
] O
. O
Besides O
achieving O
extra O
long O
context O
and O
resolving O
fragmentation O
, O
another O
benefit O
that O
comes O
with O
the O
recurrence Method
scheme Method
is O
significantly O
faster O
evaluation Task
. O
Specifically O
, O
during O
evaluation Task
, O
the O
representations O
from O
the O
previous O
segments O
can O
be O
reused O
instead O
of O
being O
computed O
from O
scratch O
as O
in O
the O
case O
of O
the O
vanilla Method
model Method
. O
In O
our O
experiments O
on O
enwiki8 Material
, O
Transformer Method
- Method
XL Method
is O
up O
to O
1 O
, O
800 O
+ O
times O
faster O
than O
the O
vanilla Method
model Method
during O
evaluation O
( O
see O
Section O
[ O
reference O
] O
) O
. O
Finally O
, O
notice O
that O
the O
recurrence Method
scheme Method
does O
not O
need O
to O
be O
restricted O
to O
only O
the O
previous O
segment O
. O
In O
theory O
, O
we O
can O
cache O
as O
many O
previous O
segments O
as O
the O
GPU O
memory O
allows O
, O
and O
reuse O
all O
of O
them O
as O
the O
extra O
context O
when O
processing O
the O
current O
segment O
. O
Thus O
, O
we O
can O
cache O
a O
predefined O
length O
- O
old O
hidden O
states O
spanning O
( O
possibly O
) O
multiple O
segments O
, O
and O
refer O
to O
them O
as O
the O
memory O
, O
due O
to O
a O
clear O
connection O
to O
the O
memory Method
augmented Method
neural Method
networks Method
graves2014neural O
, O
weston2014memory O
. O
In O
our O
experiments O
, O
we O
set O
equal O
to O
the O
segment O
length O
during O
training O
, O
and O
increase O
it O
by O
multiple O
times O
during O
evaluation O
. O
subsection O
: O
Relative Method
Positional Method
Encodings Method
While O
we O
found O
the O
idea O
presented O
in O
the O
previous O
subsection O
very O
appealing O
, O
there O
is O
a O
crucial O
technical O
challenge O
we O
have O
n’t O
solved O
in O
order O
to O
reuse O
the O
hidden O
states O
. O
That O
is O
, O
how O
can O
we O
keep O
the O
positional O
information O
coherent O
when O
we O
reuse O
the O
states O
? O
Recall O
that O
, O
in O
the O
standard O
Transformer Method
, O
the O
information O
of O
sequence O
order O
is O
provided O
by O
a O
set O
of O
positional Method
encodings Method
, O
denoted O
as O
, O
where O
the O
- O
th O
row O
corresponds O
to O
the O
- O
th O
absolute O
position O
within O
a O
segment O
and O
prescribes O
the O
maximum O
possible O
length O
to O
be O
modeled O
. O
Then O
, O
the O
actual O
input O
to O
the O
Transformer Method
is O
the O
element Method
- Method
wise Method
addition Method
of O
the O
word Method
embeddings Method
and O
the O
positional Method
encodings Method
. O
If O
we O
simply O
adapt O
this O
positional Method
encoding Method
to O
our O
recurrence Method
mechanism Method
introduced O
above O
, O
the O
hidden O
state O
sequence O
would O
be O
computed O
schematically O
by O
where O
is O
the O
word O
embedding O
sequence O
of O
, O
and O
represents O
a O
transformation O
function O
. O
Notice O
that O
, O
both O
and O
are O
associated O
with O
the O
same O
positional Method
encoding Method
. O
As O
a O
result O
, O
the O
model O
has O
no O
information O
to O
distinguish O
the O
positional O
difference O
between O
and O
for O
any O
, O
resulting O
in O
a O
sheer O
performance O
loss O
. O
In O
order O
to O
avoid O
this O
failure O
mode O
, O
the O
fundamental O
idea O
is O
to O
only O
encode O
the O
relative O
positional O
information O
in O
the O
hidden O
states O
. O
Conceptually O
, O
the O
positional Method
encoding Method
gives O
the O
model O
a O
temporal O
clue O
or O
“ O
bias O
” O
about O
how O
information O
should O
be O
gathered O
, O
i.e. O
, O
where O
to O
attend O
. O
For O
the O
same O
purpose O
, O
instead O
of O
incorporating O
bias O
statically O
into O
the O
initial O
embedding O
, O
one O
can O
inject O
the O
same O
information O
into O
the O
attention O
score O
of O
each O
layer O
. O
More O
importantly O
, O
it O
is O
more O
intuitive O
and O
generalizable O
to O
define O
the O
temporal O
bias O
in O
a O
relative O
manner O
. O
For O
instance O
, O
when O
a O
query O
vector O
attends O
on O
the O
key O
vectors O
, O
it O
does O
not O
need O
to O
know O
the O
absolute O
position O
of O
each O
key O
vector O
to O
identify O
the O
temporal O
order O
of O
the O
segment O
. O
Instead O
, O
it O
suffices O
to O
know O
the O
relative O
distance O
between O
each O
key O
vector O
and O
itself O
, O
i.e. O
. O
Practically O
, O
one O
can O
create O
a O
set O
of O
relative Method
positional Method
encodings Method
, O
where O
the O
- O
th O
row O
indicates O
a O
relative O
distance O
of O
between O
two O
positions O
. O
By O
injecting O
the O
relative O
distance O
dynamically O
into O
the O
attention O
score O
, O
the O
query O
vector O
can O
easily O
distinguish O
the O
representations O
of O
and O
from O
their O
different O
distances O
, O
making O
the O
state Method
reuse Method
mechanism Method
feasible O
. O
Meanwhile O
, O
we O
wo O
n’t O
lose O
any O
temporal O
information O
, O
as O
the O
absolute O
position O
can O
be O
recovered O
recursively O
from O
relative O
distances O
. O
Previously O
, O
the O
idea O
of O
relative Method
positional Method
encodings Method
has O
been O
explored O
in O
the O
context O
of O
machine Task
translation Task
shaw2018self O
and O
music Task
generation Task
huang2018improved O
. O
Here O
, O
we O
offer O
a O
different O
derivation O
, O
arriving O
at O
a O
new O
form O
of O
relative Method
positional Method
encodings Method
, O
which O
not O
only O
has O
a O
one O
- O
to O
- O
one O
correspondence O
to O
its O
absolute O
counterpart O
but O
also O
enjoys O
much O
better O
generalization O
empirically O
( O
see O
Section O
[ O
reference O
] O
) O
. O
Firstly O
, O
in O
the O
standard O
Transformer O
vaswani2017attention O
, O
the O
attention O
score O
between O
query O
and O
key O
vector O
within O
the O
same O
segment O
can O
be O
decomposed O
as O
Following O
the O
idea O
of O
only O
relying O
on O
relative O
positional O
information O
, O
we O
propose O
to O
re O
- O
parameterize O
the O
four O
terms O
as O
follows O
The O
first O
change O
we O
make O
is O
to O
replace O
all O
appearances O
of O
the O
absolute O
positional O
embedding O
for O
computing O
key O
vectors O
in O
term O
and O
with O
its O
relative O
counterpart O
. O
This O
essentially O
reflects O
the O
prior O
that O
only O
the O
relative O
distance O
matters O
for O
where O
to O
attend O
. O
Note O
that O
is O
a O
sinusoid Method
encoding Method
matrix Method
vaswani2017attention Method
without O
learnable O
parameters O
. O
Secondly O
, O
we O
introduce O
a O
trainable O
parameter O
to O
replace O
the O
query O
in O
term O
. O
In O
this O
case O
, O
since O
the O
query O
vector O
is O
the O
same O
for O
all O
query O
positions O
, O
it O
suggests O
that O
the O
attentive O
bias O
towards O
different O
words O
should O
remain O
the O
same O
regardless O
of O
the O
query O
position O
. O
With O
a O
similar O
reasoning O
, O
a O
trainable O
parameter O
is O
added O
to O
substitute O
in O
term O
. O
Finally O
, O
we O
deliberately O
separate O
the O
two O
weight O
matrices O
and O
for O
producing O
the O
content O
- O
based O
key O
vectors O
and O
location O
- O
based O
key O
vectors O
respectively O
. O
Under O
the O
new O
parameterization O
, O
each O
term O
has O
an O
intuitive O
meaning O
: O
term O
represents O
content O
- O
based O
addressing O
, O
term O
captures O
a O
content O
- O
dependent O
positional O
bias O
, O
term O
governs O
a O
global O
content O
bias O
, O
and O
encodes O
a O
global O
positional O
bias O
. O
In O
comparison O
, O
the O
formulation O
in O
only O
has O
terms O
and O
, O
dropping O
the O
two O
bias O
terms O
and O
. O
Moreover O
, O
shaw2018self O
merge O
the O
multiplication Method
into O
a O
single O
trainable O
matrix O
, O
which O
abandons O
the O
inductive O
bias O
built O
into O
the O
original O
sinusoid Method
positional Method
encoding Method
vaswani2017attention Method
. O
In O
contrast O
, O
our O
relative Method
positional Method
embedding Method
adapts O
the O
sinusoid Method
formulation Method
. O
As O
a O
benefit O
of O
the O
inductive Method
bias Method
, O
a O
model O
trained O
on O
a O
memory O
of O
some O
certain O
length O
can O
automatically O
generalize O
to O
a O
memory O
several O
times O
longer O
during O
evaluation O
. O
Equipping O
the O
recurrence Method
mechanism Method
with O
our O
proposed O
relative Method
positional Method
embedding Method
, O
we O
finally O
arrive O
at O
the O
Transformer Method
- Method
XL Method
architecture O
. O
For O
completeness O
, O
we O
summarize O
the O
computational Method
procedure Method
for O
a O
- Method
layer Method
Transformer Method
- Method
XL Method
with O
a O
single O
attention O
head O
below O
: O
with O
defined O
as O
the O
word O
embedding O
sequence O
. O
In O
addition O
, O
it O
is O
worth O
mentioning O
that O
a O
naive O
way O
to O
compute O
requires O
computing O
for O
all O
pairs O
, O
whose O
cost O
is O
quadratic O
w.r.t O
. O
the O
sequence O
length O
. O
However O
, O
noticing O
that O
the O
value O
of O
only O
ranges O
from O
zero O
to O
the O
sequence O
length O
, O
we O
show O
a O
simple O
computation O
procedure O
in O
Appendix O
[ O
reference O
] O
, O
which O
reduces O
the O
cost O
to O
be O
linear O
w.r.t O
. O
the O
sequence O
length O
. O
section O
: O
Experiments O
subsection O
: O
Main O
Results O
We O
apply O
Transformer Method
- Method
XL Method
to O
a O
variety O
of O
datasets O
on O
both O
word O
- O
level O
and O
character Task
- Task
level Task
language O
modeling O
to O
have O
a O
comparison O
with O
state O
- O
of O
- O
the O
- O
art O
systems O
, O
including O
WikiText Material
- Material
103 Material
merity2016pointer O
, O
enwiki8 Material
mahoney2011large O
, O
text8 Material
mahoney2011large O
, O
One Material
Billion Material
Word Material
chelba2013one O
, O
and O
Penn Material
Treebank Material
mikolov2012context O
. O
WikiText Material
- Material
103 Material
is O
the O
largest O
available O
word Task
- Task
level Task
language Task
modeling Task
benchmark O
with O
long O
- O
term O
dependency O
. O
It O
contains O
103 O
M O
training O
tokens O
from O
28 O
K O
articles O
, O
with O
an O
average O
length O
of O
3.6 O
K O
tokens O
per O
article O
, O
which O
allows O
testing O
the O
ability O
of O
long Method
- Method
term Method
dependency Method
modeling Method
. O
We O
set O
the O
attention O
length O
to O
384 O
during O
training O
and O
1600 O
during O
evaluation O
. O
We O
adopted O
adaptive O
softmax O
and O
input O
representations O
baevski2018adaptive O
, O
grave2016efficient O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
Transformer Method
- Method
XL Method
reduces O
the O
previous O
SoTA O
perplexity Metric
from O
20.5 O
to O
18.3 O
, O
which O
demonstrates O
the O
superiority O
of O
the O
Transformer Method
- Method
XL Method
architecture O
. O
The O
dataset O
enwiki8 Material
contains O
100 O
M O
bytes O
of O
unprocessed O
Wikipedia O
text O
. O
We O
compare O
our O
architecture O
with O
the O
previous O
results O
in O
Table O
[ O
reference O
] O
. O
Under O
the O
model O
size O
constraint O
, O
the O
12 O
- O
layer O
Transformer Method
- Method
XL Method
achieves O
a O
new O
SoTA O
result O
, O
outperforming O
the O
12 Method
- Method
layer Method
vanilla Method
Transformer Method
from O
by O
0.05 O
, O
while O
both O
Transformer Method
variants Method
have O
a O
large O
margin O
over O
conventional O
RNN Method
- Method
based Method
models Method
. O
Notably O
, O
our O
12 Method
- Method
layer Method
architecture Method
achieves O
the O
same O
result O
as O
the O
64 Method
- Method
layer Method
network Method
from O
, O
using O
only O
17 O
% O
of O
the O
parameter O
budget O
. O
In O
order O
to O
see O
whether O
better O
performances O
can O
be O
obtained O
by O
increasing O
the O
model O
size O
, O
we O
train O
18 Method
- Method
layer Method
and Method
24 Method
- Method
layer Method
Transformer Method
- Method
XLs Method
with O
increased O
model O
sizes O
. O
With O
the O
attention O
length O
784 O
during O
training O
and O
3 O
, O
800 O
during O
evaluation O
, O
we O
obtained O
a O
new O
SoTA O
result O
and O
our O
method O
is O
the O
first O
to O
break O
through O
1.0 O
on O
widely O
- O
studied O
character Task
- Task
level Task
benchmarks O
. O
Different O
from O
, O
Transformer Method
- Method
XL Method
does O
not O
need O
any O
auxiliary O
losses O
, O
and O
thus O
all O
benefits O
are O
credited O
to O
a O
better O
architecture O
. O
Similar O
to O
but O
different O
from O
enwiki8 Material
, O
text8 Material
contains O
100 O
M O
processed O
Wikipedia O
characters O
created O
by O
lowering O
case O
the O
text O
and O
removing O
any O
character O
other O
than O
the O
26 O
letters O
a O
through O
z O
, O
and O
space O
. O
Due O
to O
the O
similarity O
, O
we O
simply O
adapt O
the O
best O
model O
and O
the O
same O
hyper O
- O
parameters O
on O
enwiki8 Material
to O
text8 Material
without O
further O
tuning O
. O
The O
comparison O
with O
previous O
methods O
is O
summarized O
in O
Table O
[ O
reference O
] O
. O
Again O
, O
Transformer Method
- Method
XL Method
achieves O
the O
new O
SoTA O
result O
with O
a O
clear O
margin O
. O
One Material
Billion Material
Word Material
does O
not O
preserve O
any O
long O
- O
term O
dependency O
because O
sentences O
have O
been O
shuffled O
. O
Consequently O
, O
this O
dataset O
mainly O
tests O
the O
ability O
of O
modeling O
only O
short Task
- Task
term Task
dependency Task
. O
The O
comparison O
between O
Transformer Method
- Method
XL Method
and O
the O
other O
methods O
is O
shown O
in O
Table O
[ O
reference O
] O
. O
Although O
Transformer Method
- Method
XL Method
is O
mainly O
designed O
to O
better O
capture O
longer O
- O
term O
dependency O
, O
it O
dramatically O
improves O
the O
single O
- O
model O
SoTA Method
from O
23.7 O
to O
21.8 O
. O
Specifically O
, O
Transformer Method
- Method
XL Method
significantly O
outperforms O
a O
contemporary O
method O
using O
vanilla Method
Transformers Method
, O
suggesting O
the O
advantage O
of O
Transformer Method
- Method
XL Method
is O
generalizable O
to O
modeling O
short O
sequences O
. O
We O
also O
report O
the O
results O
on O
word O
- O
level O
Penn Material
Treebank Material
in O
Table O
[ O
reference O
] O
. O
Similar O
to O
AWD Method
- Method
LSTM Method
merity2017regularizing O
, O
we O
apply O
variational Method
dropout Method
and O
weight Method
average Method
to O
Transformer Method
- Method
XL Method
. O
With O
proper O
regularization Method
, O
Transformer Method
- Method
XL Method
achieves O
a O
new O
SoTA O
result O
among O
models O
without O
two O
- O
step O
finetuning Method
. O
Penn Material
Treebank Material
has O
only O
1 O
M O
training O
tokens O
, O
which O
implies O
that O
Transformer Method
- Method
XL Method
also O
generalizes O
well O
even O
on O
small O
datasets O
. O
subsection O
: O
Ablation Task
Study Task
We O
conduct O
two O
sets O
of O
ablation Task
studies Task
to O
examine O
the O
effects O
of O
two O
proposed O
techniques O
used O
in O
Transformer Method
- Method
XL Method
: O
the O
recurrence Method
mechanism Method
and O
the O
new O
positional Method
encoding Method
scheme Method
. O
The O
first O
study O
is O
performed O
on O
WikiText Material
- Material
103 Material
, O
which O
requires O
modeling O
long O
- O
term O
dependency O
. O
The O
results O
are O
reported O
in O
Table O
[ O
reference O
] O
. O
Among O
the O
compared O
encoding Method
schemes Method
, O
is O
relative O
, O
while O
and O
are O
absolute O
. O
“ O
Full O
” O
and O
“ O
half O
” O
losses O
refer O
to O
applying O
a O
cross O
entropy O
loss O
to O
all O
or O
the O
recent O
half O
positions O
in O
the O
segment O
. O
We O
found O
that O
absolute Method
encodings Method
only O
work O
well O
with O
half O
losses O
because O
half O
losses O
exclude O
positions O
with O
very O
short O
attention O
lengths O
during O
training O
for O
better O
generalization Task
. O
Table O
[ O
reference O
] O
shows O
that O
both O
the O
recurrence Method
mechanism Method
and O
our O
encoding Method
scheme Method
are O
necessary O
to O
achieve O
the O
best O
performance O
, O
as O
well O
as O
generalizing O
to O
longer O
attention O
sequences O
during O
evaluation O
time O
. O
Although O
the O
backpropagation O
length O
during O
training O
is O
only O
128 O
, O
with O
the O
two O
techniques O
the O
attention O
length O
can O
be O
increased O
to O
640 O
at O
test O
time O
. O
In O
the O
standard O
setting O
with O
151 O
M O
parameters O
, O
the O
perplexity Metric
decreases O
as O
the O
attention O
length O
increases O
. O
Since O
the O
recurrence Method
mechanism Method
costs O
additional O
memory O
, O
we O
also O
compare O
Transformer Method
- Method
XL Method
with O
baselines O
under O
the O
same O
GPU O
memory O
constraints O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
in O
Appendix O
[ O
reference O
] O
, O
despite O
using O
a O
shorter O
backpropagation O
length O
, O
Transformer Method
- Method
XL Method
remains O
superior O
to O
the O
baselines O
. O
The O
second O
study O
targets O
at O
isolating O
the O
effects O
of O
resolving O
the O
context Task
fragmentation Task
problem Task
from O
the O
benefit O
of O
capturing O
longer O
context O
length O
. O
In O
order O
to O
achieve O
this O
goal O
, O
we O
deliberately O
choose O
a O
dataset O
that O
does O
not O
require O
long O
- O
term O
dependency O
, O
so O
that O
any O
improvement O
from O
establishing O
the O
recurrence O
can O
be O
attributed O
to O
solving O
the O
context O
fragmentation O
. O
Specifically O
, O
we O
perform O
this O
controlled O
experiment O
on O
the O
One Material
Billion Material
Word Material
dataset O
, O
which O
can O
only O
benefit O
from O
removing O
the O
context O
fragmentation O
. O
We O
train O
a O
20 O
- O
layer O
Transformer Method
- Method
XL Method
with O
0.3B Method
parameters Method
for O
400 O
K O
steps O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
using O
segment Method
- Method
level Method
recurrence Method
substantially O
improves O
performance O
even O
when O
long O
- O
term O
dependency O
is O
not O
needed O
, O
which O
is O
consistent O
with O
our O
previous O
discussion O
that O
the O
recurrence Method
mechanism Method
resolves O
the O
context Task
fragmentation Task
problem Task
. O
Moreover O
, O
our O
relative Method
positional Method
encodings Method
is O
also O
superior O
to O
shaw2018self O
on O
short O
sequences O
. O
subsection O
: O
Relative Metric
Effective Metric
Context Metric
Length Metric
proposed O
a O
method O
to O
evaluate O
the O
Effective Metric
Context Metric
Length Metric
( O
ECL Metric
) O
of O
a O
sequence Method
model Method
. O
ECL Metric
is O
the O
longest O
length O
to O
which O
increasing O
the O
context O
span O
would O
lead O
to O
a O
gain O
more O
than O
a O
threshold O
. O
However O
, O
ECL Metric
ignores O
the O
fact O
that O
it O
is O
harder O
to O
get O
improvement O
when O
a O
model O
already O
achieves O
a O
lower O
perplexity Metric
using O
only O
a O
shorter O
context O
, O
and O
thus O
it O
is O
not O
suitable O
for O
fair O
comparison O
among O
multiple O
models O
. O
We O
instead O
propose O
a O
new O
metric O
called O
Relative Metric
Effective Metric
Context Metric
Length Metric
( O
RECL Method
) O
. O
RECL Method
is O
defined O
on O
a O
model O
group O
instead O
of O
a O
single O
model O
, O
and O
the O
gain O
of O
a O
long O
context O
is O
measure O
by O
the O
relative O
improvement O
over O
the O
best O
short Method
context Method
model Method
. O
As O
such O
, O
the O
model O
group O
shares O
the O
same O
baseline O
to O
enable O
fair O
comparison O
. O
RECL Method
also O
has O
a O
parameter O
, O
which O
means O
constraining O
the O
comparison O
on O
top O
- O
hard O
examples O
. O
See O
Appedix O
[ O
reference O
] O
for O
more O
details O
about O
RECL O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
Transformer Method
- Method
XL Method
manages O
to O
model O
dependency O
of O
900 O
words O
long O
on O
average O
with O
. O
The O
RECL O
of O
Transformer Method
- Method
XL Method
is O
80 O
% O
and O
450 O
% O
longer O
than O
recurrent Method
networks Method
and O
Transformer Method
respectively O
. O
Both O
the O
recurrence Method
mechanism Method
and O
our O
positional Method
encodings Method
contribute O
to O
a O
longer O
RECL Metric
. O
This O
further O
substantiates O
our O
argument O
that O
Transformer Method
- Method
XL Method
is O
able O
to O
model O
longer O
- O
term O
dependency O
. O
subsection O
: O
Evaluation Metric
Speed Metric
Finally O
, O
we O
compare O
the O
evaluation Metric
speed Metric
of O
the O
proposed O
model O
with O
the O
vanilla Method
Transformer Method
model Method
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
due O
to O
the O
state Method
reuse Method
scheme Method
, O
Transformer Method
- Method
XL Method
achieves O
an O
up O
to O
1 O
, O
874 O
times O
speedup O
during O
evaluation Task
compared O
to O
the O
architecture O
in O
. O
section O
: O
Conclusions O
We O
propose O
a O
novel O
architecture O
, O
Transformer Method
- Method
XL Method
, O
for O
language Task
modeling Task
with O
self Method
- Method
attention Method
architectures Method
beyond O
a O
fixed O
- O
length O
context O
. O
Our O
main O
technical O
contributions O
include O
introducing O
the O
notion O
of O
recurrence O
in O
a O
purely O
self Method
- Method
attentive Method
model Method
and O
deriving O
a O
novel O
positional Method
encoding Method
scheme Method
. O
These O
two O
techniques O
form O
a O
complete O
set O
of O
solutions O
, O
as O
any O
one O
of O
them O
alone O
does O
not O
address O
the O
issue O
of O
fixed O
- O
length O
contexts O
. O
Transformer Method
- Method
XL Method
is O
the O
first O
self Method
- Method
attention Method
model Method
that O
achieves O
substantially O
better O
results O
than O
RNNs Method
on O
both O
character Task
- Task
level Task
and O
word Task
- Task
level Task
language Task
modeling Task
. O
Transformer Method
- Method
XL Method
is O
also O
able O
to O
model O
longer O
- O
term O
dependency O
than O
RNNs Method
and O
Transformer Method
, O
and O
achieves O
substantial O
speedup O
during O
evaluation Task
compared O
to O
vanilla Method
Transformers Method
. O
subsubsection O
: O
Acknowledgments O
This O
work O
was O
supported O
in O
part O
by O
the O
Office O
of O
Naval O
Research O
, O
NSF O
grant O
IIS1763562 O
, O
Google O
focused O
award O
, O
and O
the O
Nvidia O
fellowship O
. O
bibliography O
: O
References O
appendix O
: O
Ablation Task
Study Task
with O
Memory O
Constraints O
Table O
[ O
reference O
] O
compares O
Transformer Method
- Method
XL Method
with O
baseline O
under O
the O
same O
memory O
budget O
. O
Transformer Method
- Method
XL Method
still O
outperforms O
the O
baseline O
even O
with O
a O
shorter O
backprop O
length O
. O
appendix O
: O
Efficient Task
Computation Task
of Task
the Task
Attention Task
with O
Relative Method
Positional Method
Embedding Method
As O
we O
discussed O
in O
section O
[ O
reference O
] O
, O
the O
naive O
way O
of O
computing O
the O
for O
all O
pairs O
is O
subject O
to O
a O
quadratic Metric
cost Metric
. O
Here O
, O
we O
present O
a O
simple O
method O
with O
only O
a O
linear Metric
cost Metric
. O
Firstly O
, O
notice O
that O
the O
relative O
distance O
can O
only O
be O
integer O
from O
0 O
to O
, O
where O
and O
are O
the O
memory O
length O
and O
segment O
length O
respectively O
. O
Hence O
, O
the O
rows O
of O
the O
matrix O
consist O
of O
all O
possible O
vector O
outputs O
of O
for O
any O
. O
Note O
that O
we O
have O
defined O
in O
a O
reversed O
order O
, O
i.e. O
, O
, O
to O
make O
further O
discussion O
easier O
. O
Next O
, O
we O
collect O
the O
term O
for O
all O
possible O
into O
the O
following O
matrix O
, O
Then O
, O
we O
further O
define O
Now O
, O
it O
is O
easy O
to O
see O
an O
immediate O
relationship O
between O
and O
, O
where O
the O
- O
th O
row O
of O
is O
simply O
a O
left O
- O
shifted O
version O
of O
- O
th O
row O
of O
. O
Hence O
, O
the O
computation O
of O
only O
requires O
a O
matrix Method
multiplication Method
to O
compute O
and O
then O
a O
set O
of O
left O
- O
shifts O
. O
Similarly O
, O
we O
can O
collect O
all O
term O
for O
all O
possible O
into O
another O
matrix O
, O
Then O
, O
we O
can O
follow O
the O
same O
procedure O
to O
define O
Again O
, O
each O
row O
of O
is O
simply O
a O
left O
- O
shift O
version O
of O
. O
Hence O
, O
the O
main O
computation Metric
cost Metric
comes O
from O
the O
matrix Method
- Method
vector Method
multiplication Method
, O
which O
is O
not O
expensive O
any O
more O
. O
appendix O
: O
Details O
About O
RECL Task
[ O
b O
] O
0.5 O
[ O
b O
] O
0.5 O
[ O
b O
] O
0.5 O
[ O
b O
] O
0.5 O
In O
this O
section O
, O
we O
describe O
the O
details O
of O
the O
metric Metric
RECL Metric
. O
Let O
be O
a O
model O
group O
consisting O
of O
models O
. O
Let O
denote O
the O
loss O
of O
model O
on O
the O
- O
th O
token O
in O
the O
corpus O
with O
a O
context O
length O
. O
Concretely O
, O
the O
loss O
can O
be O
written O
as O
where O
is O
the O
probability O
distribution O
given O
by O
model O
, O
and O
is O
the O
- O
th O
token O
in O
the O
corpus O
. O
Given O
a O
short O
context O
length O
and O
a O
long O
context O
length O
such O
that O
, O
we O
can O
further O
define O
a O
baseline O
for O
each O
position O
, O
The O
relative Metric
loss Metric
of Metric
w.r.t Metric
. O
the O
model O
group O
is O
written O
as O
The O
above O
equation O
uses O
the O
minimum O
loss O
of O
all O
models O
on O
the O
short O
length O
as O
a O
baseline O
, O
and O
only O
losses O
smaller O
than O
the O
baseline O
will O
be O
effectively O
counted O
towards O
the O
relative O
loss O
. O
This O
enables O
fair O
comparison O
between O
multiple O
models O
because O
all O
models O
with O
a O
long O
context O
length O
need O
to O
improve O
over O
the O
same O
baseline O
. O
Sometimes O
we O
only O
care O
about O
those O
positions O
where O
the O
baseline O
performs O
poorly O
( O
which O
means O
short O
- O
term O
dependency O
with O
context O
length O
is O
not O
sufficient O
) O
, O
so O
given O
a O
ratio O
parameter O
, O
we O
define O
the O
set O
is O
the O
above O
equation O
as O
The O
relative Metric
gain Metric
is O
subsequently O
defined O
as O
the O
relative O
perplexity Metric
reduction O
: O
Given O
a O
step O
size O
, O
we O
then O
use O
an O
algorithm O
to O
find O
the O
RECL O
by O
thresholding O
the O
relative Metric
gain Metric
: O
Set O
initial O
short O
context O
length O
, O
and O
long O
context O
length O
Compute O
. O
If O
, O
return O
. O
If O
, O
set O
and O
go O
to O
step O
1 O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
visualize O
the O
unnormalized O
relative O
perplexity Metric
gains O
with O
various O
pairs O
of O
when O
. O
It O
is O
clear O
that O
Transformer Method
- Method
XL Method
has O
a O
longer O
RECL Metric
compared O
to O
RNNs Method
and O
other O
baselines O
because O
the O
relative O
gains O
are O
substantially O
larger O
. O
For O
reference O
, O
we O
plot O
the O
perplexities O
with O
varying O
context O
lengths O
in O
Figure O
[ O
reference O
] O
. O
The O
y O
- O
axis O
denotes O
the O
“ O
normal O
” O
perplexity Metric
( O
not O
calibrated O
by O
baselines O
) O
. O
appendix O
: O
Attention Method
Visualization Method
In O
this O
section O
, O
we O
provide O
some O
visualization O
of O
the O
attention O
learned O
by O
the O
SoTA Method
model Method
on O
the O
WikiText Material
- Material
103 Material
validation O
set O
. O
Recall O
that O
, O
this O
model O
has O
16 O
10 O
- O
head O
transformer O
layers O
and O
relies O
on O
a O
memory O
of O
length O
640 O
. O
The O
first O
visualization O
aims O
at O
revealing O
the O
overall O
trend O
of O
where O
the O
model O
is O
attending O
. O
Specifically O
, O
for O
each O
attention O
head O
of O
each O
layer O
, O
we O
average O
the O
attention O
distributions O
of O
all O
tokens O
in O
the O
validation O
set O
. O
This O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
As O
we O
can O
see O
, O
the O
overall O
trend O
is O
to O
focus O
more O
on O
the O
nearby O
tokens O
than O
the O
faraway O
ones O
. O
However O
, O
it O
is O
also O
very O
clear O
that O
some O
attention O
heads O
have O
a O
wider O
attention O
distribution O
over O
the O
entire O
memory O
span O
, O
notably O
the O
head O
8 O
from O
layer O
1 O
, O
head O
78 O
from O
layer O
8 O
, O
and O
the O
head O
158 O
from O
layer O
16 O
. O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
Since O
we O
are O
focused O
on O
learning Task
long Task
- Task
range Task
dependency Task
, O
we O
are O
especially O
interested O
in O
these O
heads O
with O
a O
wider O
attention O
span O
. O
Thus O
, O
in O
the O
second O
set O
of O
visualization Task
, O
we O
pick O
the O
three O
notable O
heads O
mentioned O
above O
, O
and O
visualize O
their O
attention O
behavior O
for O
a O
randomly O
chosen O
position O
, O
as O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
Here O
, O
we O
see O
three O
different O
patterns O
of O
wider O
attention O
: O
For O
the O
head O
8 O
in O
the O
1st O
layer O
, O
we O
see O
an O
almost O
uniform O
attention O
over O
the O
entire O
memory O
span O
. O
This O
is O
quite O
intuitive O
, O
as O
lower O
- O
level O
layers O
needs O
to O
screen O
the O
entire O
memory O
span O
to O
decide O
where O
to O
focus O
for O
higher O
- O
level O
layers O
For O
the O
head O
78 O
in O
the O
8th O
layer O
( O
a O
middle O
- O
level O
layer O
) O
, O
we O
see O
a O
very O
sparse O
attention O
pattern O
scattered O
in O
all O
ranges O
of O
the O
memory O
. O
Again O
, O
this O
well O
fits O
our O
intuition O
that O
as O
information O
accumulates O
, O
the O
network O
may O
focus O
on O
some O
particular O
position O
with O
special O
interests O
. O
For O
the O
head O
158 O
in O
the O
16th O
layer O
( O
i.e. O
the O
last O
layer O
) O
, O
each O
target O
location O
( O
corresponding O
to O
each O
row O
) O
has O
its O
own O
distinct O
sparse O
focus O
, O
differing O
from O
head O
78 O
where O
target O
locations O
largely O
share O
the O
same O
attentive O
location O
in O
memory O
. O
Meanwhile O
, O
the O
pattern O
is O
also O
different O
from O
the O
case O
of O
head O
8 O
, O
where O
a O
few O
locations O
are O
clearly O
attended O
more O
than O
others O
. O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
Finally O
, O
as O
we O
have O
discussed O
in O
section O
[ O
reference O
] O
, O
the O
attention Metric
score Metric
can O
be O
decomposed O
into O
four O
intuitive O
terms O
. O
Here O
, O
we O
want O
to O
further O
investigate O
how O
these O
four O
terms O
contribute O
to O
the O
overall O
attention O
trend O
in O
Fig O
. O
[ O
reference O
] O
. O
Since O
the O
term O
represents O
the O
global O
content O
bias O
, O
i.e. O
, O
the O
prior O
importance O
of O
each O
word O
regardless O
of O
the O
context O
, O
we O
will O
leave O
it O
out O
and O
focus O
on O
the O
terms O
, O
and O
. O
So O
, O
for O
each O
term O
, O
we O
take O
the O
Softmax O
w.r.t O
. O
the O
memory O
span O
and O
average O
the O
resulted O
distribution O
of O
all O
tokens O
in O
the O
validation O
set O
. O
The O
results O
are O
visualized O
in O
Fig O
. O
[ O
reference O
] O
: O
Since O
term Method
is O
fully O
content Method
- Method
based Method
addressing Method
, O
when O
averaging O
over O
all O
target O
words O
, O
the O
result O
is O
essentially O
uniform O
over O
the O
entire O
context O
, O
except O
for O
a O
few O
very O
close O
words O
, O
which O
are O
likely O
to O
be O
semantically O
similar O
to O
the O
target O
word O
. O
The O
overall O
trend O
of O
term O
highly O
resembles O
that O
of O
the O
entire O
attention O
distribution O
in O
Fig O
. O
[ O
reference O
] O
. O
It O
suggests O
that O
the O
global O
trend O
of O
focusing O
on O
the O
nearby O
context O
is O
largely O
contributed O
by O
this O
content O
- O
dependent O
positional O
bias O
. O
The O
overall O
trend O
of O
term O
is O
also O
focusing O
more O
on O
nearby O
words O
. O
However O
, O
compared O
to O
the O
trend O
of O
term O
, O
it O
is O
clearly O
flatter O
and O
biases O
towards O
a O
longer O
context O
. O
