document O
: O
Edinburgh Task
Neural Task
Machine Task
Translation Task
Systems Task
for O
WMT Material
16 Material
We O
participated O
in O
the O
WMT Material
2016 O
shared O
news O
translation O
task O
by O
building O
neural Method
translation Method
systems Method
for O
four O
language O
pairs O
, O
each O
trained O
in O
both O
directions O
: O
English Material
Czech Material
, O
English Material
German Material
, O
English Material
Romanian Material
and O
English Material
Russian Material
. O
Our O
systems O
are O
based O
on O
an O
attentional Method
encoder Method
- Method
decoder Method
, O
using O
BPE Method
subword Method
segmentation Method
for O
open Task
- Task
vocabulary Task
translation Task
with O
a O
fixed O
vocabulary O
. O
We O
experimented O
with O
using O
automatic O
back O
- O
translations O
of O
the O
monolingual Material
News Material
corpus Material
as O
additional O
training O
data O
, O
pervasive Method
dropout Method
, O
and O
target Method
- Method
bidirectional Method
models Method
. O
All O
reported O
methods O
give O
substantial O
improvements O
, O
and O
we O
see O
improvements O
of O
4.3–11.2 O
Bleu Metric
over O
our O
baseline O
systems O
. O
In O
the O
human O
evaluation O
, O
our O
systems O
were O
the O
( O
tied O
) O
best O
constrained O
system O
for O
7 O
out O
of O
8 O
translation O
directions O
in O
which O
we O
participated O
. O
obeyall O
= O
true O
section O
: O
Introduction O
We O
participated O
in O
the O
WMT Material
2016 O
shared O
news O
translation O
task O
by O
building O
neural Method
translation Method
systems Method
for O
four O
language O
pairs O
: O
English Material
Czech Material
, O
English Material
German Material
, O
English Material
Romanian Material
and O
English Material
Russian Material
. O
Our O
systems O
are O
based O
on O
an O
attentional Method
encoder Method
- Method
decoder Method
, O
using O
BPE Method
subword Method
segmentation Method
for O
open Task
- Task
vocabulary Task
translation Task
with O
a O
fixed O
vocabulary O
. O
We O
experimented O
with O
using O
automatic O
back O
- O
translations O
of O
the O
monolingual Material
News Material
corpus Material
as O
additional O
training O
data O
, O
pervasive Method
dropout Method
, O
and O
target Method
- Method
bidirectional Method
models Method
. O
section O
: O
Baseline O
System O
Our O
systems O
are O
attentional Method
encoder Method
- Method
decoder Method
networks O
. O
We O
base O
our O
implementation O
on O
the O
dl4mt Method
- Method
tutorial Method
, O
which O
we O
enhanced O
with O
new O
features O
such O
as O
ensemble Method
decoding Method
and O
pervasive Method
dropout Method
. O
We O
use O
minibatches O
of O
size O
80 O
, O
a O
maximum O
sentence O
length O
of O
50 O
, O
word O
embeddings O
of O
size O
500 O
, O
and O
hidden O
layers O
of O
size O
1024 O
. O
We O
clip O
the O
gradient O
norm O
to O
1.0 O
. O
We O
train O
the O
models O
with O
Adadelta Method
, O
reshuffling O
the O
training O
corpus O
between O
epochs O
. O
We O
validate O
the O
model O
every O
10000 O
minibatches O
via O
Bleu Metric
on O
a O
validation O
set O
( O
newstest2013 Material
, O
newstest2014 Material
, O
or O
half O
of O
newsdev2016 Material
for O
EN Material
RO Material
) O
. O
We O
perform O
early Method
stopping Method
for O
single O
models O
, O
and O
use O
the O
4 O
last O
saved O
models O
( O
with O
models O
saved O
every O
30000 O
minibatches O
) O
for O
the O
ensemble O
results O
. O
Note O
that O
ensemble Metric
scores Metric
are O
the O
result O
of O
a O
single O
training O
run O
. O
Due O
to O
resource O
limitations O
, O
we O
did O
not O
train O
ensemble Method
components Method
independently O
, O
which O
could O
result O
in O
more O
diverse O
models O
and O
better O
ensembles O
. O
Decoding Task
is O
performed O
with O
beam Method
search Method
with O
a O
beam O
size O
of O
12 O
. O
For O
some O
language O
pairs O
, O
we O
used O
the O
AmuNMT Method
C Method
++ Method
decoder Method
as O
a O
more O
efficient O
alternative O
to O
the O
theano O
implementation O
of O
the O
dl4mt Method
tutorial Method
. O
subsection O
: O
Byte Method
- Method
pair Method
encoding Method
( O
BPE Method
) O
To O
enable O
open Task
- Task
vocabulary Task
translation Task
, O
we O
segment O
words O
via O
byte Method
- Method
pair Method
encoding Method
( O
BPE Method
) O
. O
BPE Method
, O
originally O
devised O
as O
a O
compression Method
algorithm Method
, O
is O
adapted O
to O
word Task
segmentation Task
as O
follows O
: O
First O
, O
each O
word O
in O
the O
training O
vocabulary O
is O
represented O
as O
a O
sequence O
of O
characters O
, O
plus O
an O
end O
- O
of O
- O
word O
symbol O
. O
All O
characters O
are O
added O
to O
the O
symbol O
vocabulary O
. O
Then O
, O
the O
most O
frequent O
symbol O
pair O
is O
identified O
, O
and O
all O
its O
occurrences O
are O
merged O
, O
producing O
a O
new O
symbol O
that O
is O
added O
to O
the O
vocabulary O
. O
The O
previous O
step O
is O
repeated O
until O
a O
set O
number O
of O
merge O
operations O
have O
been O
learned O
. O
BPE Method
starts O
from O
a O
character Method
- Method
level Method
segmentation Method
, O
but O
as O
we O
increase O
the O
number O
of O
merge O
operations O
, O
it O
becomes O
more O
and O
more O
different O
from O
a O
pure O
character Method
- Method
level Method
model Method
in O
that O
frequent O
character O
sequences O
, O
and O
even O
full O
words O
, O
are O
encoded O
as O
a O
single O
symbol O
. O
This O
allows O
for O
a O
trade O
- O
off O
between O
the O
size O
of O
the O
model O
vocabulary O
and O
the O
length O
of O
training O
sequences O
. O
The O
ordered O
list O
of O
merge Method
operations Method
, O
learned O
on O
the O
training O
set O
, O
can O
be O
applied O
to O
any O
text O
to O
segment O
words O
into O
subword O
units O
that O
are O
in O
- O
vocabulary O
in O
respect O
to O
the O
training O
set O
( O
except O
for O
unseen O
characters O
) O
. O
To O
increase O
consistency O
in O
the O
segmentation Task
of O
the O
source O
and O
target O
text O
, O
we O
combine O
the O
source O
and O
target O
side O
of O
the O
training O
set O
for O
learning O
BPE Method
. O
For O
each O
language O
pair O
, O
we O
learn O
89500 O
merge O
operations O
. O
section O
: O
Experimental O
Features O
subsection O
: O
Synthetic O
Training O
Data O
WMT Material
provides O
task O
participants O
with O
large O
amounts O
of O
monolingual O
data O
, O
both O
in O
- O
domain O
and O
out O
- O
of O
- O
domain O
. O
We O
exploit O
this O
monolingual O
data O
for O
training O
as O
described O
in O
. O
Specifically O
, O
we O
sample O
a O
subset O
of O
the O
available O
target O
- O
side O
monolingual O
corpora O
, O
translate O
it O
automatically O
into O
the O
source O
side O
of O
the O
respective O
language O
pair O
, O
and O
then O
use O
this O
synthetic O
parallel O
data O
for O
training O
. O
For O
example O
, O
for O
EN Material
RO Material
, O
the O
back Task
- Task
translation Task
is O
performed O
with O
a O
RO Material
EN Material
system O
, O
and O
vice O
- O
versa O
. O
2015arXiv151106709S O
motivate O
the O
use O
of O
monolingual O
data O
with O
domain Method
adaptation Method
, O
reducing O
overfitting Task
, O
and O
better O
modelling Task
of Task
fluency Task
. O
We O
sample O
monolingual O
data O
from O
the O
News Material
Crawl Material
corpora Material
, O
which O
is O
in O
- O
domain O
with O
respect O
to O
the O
test O
set O
. O
The O
amount O
of O
monolingual O
data O
back O
- O
translated O
for O
each O
translation O
direction O
ranges O
from O
2 O
million O
to O
10 O
million O
sentences O
. O
Statistics O
about O
the O
amount O
of O
parallel O
and O
synthetic O
training O
data O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
With O
dl4mt Method
, O
we O
observed O
a O
translation Metric
speed Metric
of O
about O
200000 O
sentences O
per O
day O
( O
on O
a O
single O
Titan O
X O
GPU O
) O
. O
subsection O
: O
Pervasive Task
Dropout Task
For O
English Material
Romanian Material
, O
we O
observed O
poor O
performance O
because O
of O
overfitting O
. O
To O
mitigate O
this O
, O
we O
apply O
dropout Method
to O
all O
layers O
in O
the O
network O
, O
including O
recurrent Method
ones Method
. O
Previous O
work O
dropped O
out O
different O
units O
at O
each O
time O
step O
. O
When O
applied O
to O
recurrent O
connections O
, O
this O
has O
the O
downside O
that O
it O
impedes O
the O
information O
flow O
over O
long O
distances O
, O
and O
DBLP O
: O
conf O
/ O
icfhr O
/ O
PhamBKL14 O
propose O
to O
only O
apply O
dropout Method
to O
non O
- O
recurrent O
connections O
. O
Instead O
, O
we O
follow O
the O
approach O
suggested O
by O
2015arXiv151205287 O
G O
, O
and O
use O
the O
same O
dropout O
mask O
at O
each O
time O
step O
. O
Our O
implementation O
differs O
from O
the O
recommendations O
by O
2015arXiv151205287 O
G O
in O
one O
respect O
: O
we O
also O
drop O
words O
at O
random O
, O
but O
we O
do O
so O
on O
a O
token O
level O
, O
not O
on O
a O
type O
level O
. O
In O
other O
words O
, O
if O
a O
word O
occurs O
multiple O
times O
in O
a O
sentence O
, O
we O
may O
drop O
out O
any O
number O
of O
its O
occurrences O
, O
and O
not O
just O
none O
or O
all O
. O
In O
our O
English Material
Romanian Material
experiments O
, O
we O
drop O
out O
full O
words O
( O
both O
on O
the O
source O
and O
target O
side O
) O
with O
a O
probability O
of O
0.1 O
. O
For O
all O
other O
layers O
, O
the O
dropout O
probability O
is O
set O
to O
0.2 O
. O
subsection O
: O
Target O
- O
bidirectional Task
Translation Task
We O
found O
that O
during O
decoding Task
, O
the O
model O
would O
occasionally O
assign O
a O
high O
probability O
to O
words O
based O
on O
the O
target O
context O
alone O
, O
ignoring O
the O
source O
sentence O
. O
We O
speculate O
that O
this O
is O
an O
instance O
of O
the O
label Task
bias Task
problem Task
. O
To O
mitigate O
this O
problem O
, O
we O
experiment O
with O
training O
separate O
models O
that O
produce O
the O
target O
text O
from O
right O
- O
to O
- O
left O
( O
r2l O
) O
, O
and O
re O
- O
scoring O
the O
n O
- O
best O
lists O
that O
are O
produced O
by O
the O
main O
( O
left O
- O
to O
- O
right O
) O
models O
with O
these O
r2l Method
models Method
. O
Since O
the O
right O
- O
to O
- O
left O
model O
will O
see O
a O
complementary O
target O
context O
at O
each O
time O
step O
, O
we O
expect O
that O
the O
averaged O
probabilities O
will O
be O
more O
robust O
. O
In O
parallel O
to O
our O
experiments O
, O
this O
idea O
was O
published O
by O
liu2016 O
. O
We O
increase O
the O
size O
of O
the O
n O
- O
best O
- O
list O
to O
50 O
for O
the O
reranking Task
experiments Task
. O
A O
possible O
criticism O
of O
the O
l O
- O
r O
/ O
r Method
- Method
l Method
reranking Method
approach Method
is O
that O
the O
gains O
actually O
come O
from O
adding O
diversity O
to O
the O
ensemble O
, O
since O
we O
are O
now O
using O
two O
independent O
runs O
. O
However O
experiments O
in O
show O
that O
a O
l O
- O
r O
/ O
r Method
- Method
l Method
reranking Method
systems Method
is O
stronger O
than O
an O
ensemble O
created O
from O
two O
independent O
l O
- O
r O
runs O
. O
section O
: O
Results O
subsection O
: O
English Material
German Material
Table O
[ O
reference O
] O
shows O
results O
for O
English Material
German Material
. O
We O
observe O
improvements O
of O
3.4–5.7 O
Bleu Metric
from O
training O
with O
a O
mix O
of O
parallel O
and O
synthetic O
data O
, O
compared O
to O
the O
baseline O
that O
is O
only O
trained O
on O
parallel O
data O
. O
Using O
an O
ensemble O
of O
the O
last O
4 O
checkpoints O
gives O
further O
improvements O
( O
1.3–1.7 O
Bleu Metric
) O
. O
Our O
submitted O
system O
includes O
reranking O
of O
the O
50 O
- O
best O
output O
of O
the O
left Method
- Method
to Method
- Method
right Method
model Method
with O
a O
right O
- O
to O
- O
left O
model O
– O
again O
an O
ensemble O
of O
the O
last O
4 O
checkpoints O
– O
with O
uniform O
weights O
. O
This O
yields O
an O
improvements O
of O
0.6–1.1 O
Bleu Metric
. O
subsection O
: O
English O
Czech O
For O
English Material
Czech Material
, O
we O
trained O
our O
baseline O
model O
on O
the O
complete O
WMT16 Material
parallel Material
training Material
set Material
( O
including O
CzEng Material
1.6pre Material
) O
, O
until O
we O
observed O
convergence O
on O
our O
heldout O
set O
( O
newstest2014 Material
) O
. O
This O
took O
approximately O
1 O
M O
minibatches O
, O
or O
3 O
weeks O
. O
Then O
we O
continued O
training O
the O
model O
on O
a O
new O
parallel O
corpus O
, O
comprising O
8.2 O
M O
sentences O
back O
- O
translated O
from O
the O
Czech Material
monolingual Material
news2015 Material
, O
5 O
copies O
of O
news Material
- Material
commentary Material
v11 Material
, O
and O
9 O
M O
sentences O
sampled O
from O
Czeng Material
1.6pre Material
. O
The O
model O
used O
for O
back Task
- Task
translation Task
was O
a O
neural Method
MT Method
model Method
from O
earlier O
experiments O
, O
trained O
on O
WMT15 Material
data Material
. O
The O
training O
on O
this O
synthetic O
mix O
continued O
for O
a O
further O
400 O
, O
000 O
minibatches O
. O
The O
right Method
- Method
left Method
model Method
was O
trained O
using O
a O
similar O
process O
, O
but O
with O
the O
target O
side O
of O
the O
parallel O
corpus O
reversed O
prior O
to O
training O
. O
The O
resulting O
model O
had O
a O
slightly O
lower O
Bleu Metric
score O
on O
the O
dev O
data O
than O
the O
standard O
left Method
- Method
right Method
model Method
. O
We O
can O
see O
in O
Table O
[ O
reference O
] O
that O
back Task
- Task
translation Task
improves O
performance O
by O
2.2–2.8 O
Bleu Metric
, O
and O
that O
the O
final O
system O
( O
+ O
r2l Method
reranking Method
) O
improves O
by O
0.7–1.0 O
Bleu Metric
on O
the O
ensemble O
of O
4 O
, O
and O
4.3–4.9 O
on O
the O
baseline O
. O
For O
Czech Material
English Material
the O
training O
process O
was O
similar O
to O
the O
above O
, O
except O
that O
we O
created O
the O
synthetic O
training O
data O
( O
back O
- O
translated O
from O
samples O
of O
news2015 Material
monolingual Material
English Material
) O
in O
batches O
of O
2.5 O
M O
, O
and O
so O
were O
able O
to O
observe O
the O
effect O
of O
increasing O
the O
amount O
of O
synthetic O
data O
. O
After O
training O
a O
baseline Method
model Method
on O
all O
the O
WMT16 Material
parallel Material
set Material
, O
we O
continued O
training O
with O
a O
parallel O
corpus O
consisting O
of O
2 O
copies O
of O
the O
2.5 O
M O
sentences O
of O
back O
- O
translated O
data O
, O
5 O
copies O
of O
news Material
- Material
commentary Material
v11 Material
, O
and O
a O
matching O
quantity O
of O
data O
sampled O
from O
Czeng Material
1.6pre Material
. O
After O
training O
this O
to O
convergence O
, O
we O
restarted O
training O
from O
the O
baseline O
model O
using O
5 O
M O
sentences O
of O
back O
- O
translated O
data O
, O
5 O
copies O
of O
news Material
- Material
commentary Material
v11 Material
, O
and O
a O
matching O
quantity O
of O
data O
sampled O
from O
Czeng Material
1.6pre Material
. O
We O
repeated O
this O
with O
7.5 O
M O
sentences O
from O
news2015 Material
monolingual Material
, O
and O
then O
with O
10 O
M O
sentences O
of O
news2015 Material
. O
The O
back O
- O
translations O
were O
, O
as O
for O
English Material
Czech Material
, O
created O
with O
an O
earlier O
NMT Method
model Method
trained O
on O
WMT15 Material
data Material
. O
Our O
final O
Czech Material
English Material
was O
an O
ensemble O
of O
8 O
systems O
– O
the O
last O
4 O
save O
- O
points O
of O
the O
10 O
M O
synthetic O
data O
run O
, O
and O
the O
last O
4 O
save O
- O
points O
of O
the O
7.5 O
M O
run O
. O
We O
show O
this O
as O
ensemble8 O
in O
Table O
[ O
reference O
] O
, O
and O
the O
+ O
synthetic O
results O
are O
on O
the O
last O
( O
i.e. O
10 O
M O
) O
synthetic O
data O
run O
. O
We O
also O
show O
in O
Table O
[ O
reference O
] O
how O
increasing O
the O
amount O
of O
back O
- O
translated O
data O
affects O
the O
results O
. O
We O
see O
that O
most O
of O
the O
gain O
from O
back Task
- Task
translation Task
comes O
with O
the O
first O
batch O
, O
but O
increasing O
the O
amount O
of O
back O
- O
translated O
data O
does O
gradually O
improve O
performance O
. O
subsection O
: O
English Material
Romanian Material
The O
results O
of O
our O
English O
Romanian O
experiments O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
This O
language O
pair O
has O
the O
smallest O
amount O
of O
parallel O
training O
data O
, O
and O
we O
found O
dropout Method
to O
be O
very O
effective O
, O
yielding O
improvements O
of O
4–5 O
Bleu Metric
. O
We O
found O
that O
the O
use O
of O
diacritics O
was O
inconsistent O
in O
the O
Romanian Material
training Material
( O
and O
development O
) O
data O
, O
so O
for O
Romanian Material
English Material
we O
removed O
diacritics O
from O
the O
Romanian O
source O
side O
, O
obtaining O
improvements O
of O
1.3–1.4 O
Bleu Metric
. O
Synthetic O
training O
data O
gives O
improvements O
of O
4.1–5.1 O
Bleu Metric
. O
for O
English Material
Romanian Material
, O
we O
found O
that O
the O
best O
single O
system O
outperformed O
the O
ensemble O
of O
the O
last O
4 O
checkpoints O
on O
dev O
, O
and O
we O
thus O
submitted O
the O
best O
single O
system O
as O
primary O
system O
. O
subsection O
: O
English Material
Russian Material
For O
English Material
Russian Material
, O
we O
can O
not O
effectively O
learn O
BPE Method
on O
the O
joint O
vocabulary O
because O
alphabets O
differ O
. O
We O
thus O
follow O
the O
approach O
described O
in O
, O
first O
mapping O
the O
Russian Material
text Material
into O
Latin O
characters O
via O
ISO O
- O
9 O
transliteration O
, O
then O
learning O
the O
BPE Method
operations Method
on O
the O
concatenation O
of O
the O
English Material
and Material
latinized Material
Russian Material
training Material
data Material
, O
then O
mapping O
the O
BPE Method
operations Method
back O
into O
Cyrillic Material
alphabet O
. O
We O
apply O
the O
Latin Material
BPE Material
operations O
to O
the O
English Material
data Material
( O
training O
data O
and O
input O
) O
, O
and O
both O
the O
Cyrillic Material
and O
Latin Material
BPE Material
operations O
to O
the O
Russian Material
data Material
. O
Translation Task
results O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
As O
for O
the O
other O
language O
pairs O
, O
we O
observe O
strong O
improvements O
from O
synthetic O
training O
data O
( O
4–4.4 O
Bleu Metric
) O
. O
Ensembles Method
yield O
another O
1.1–1.7 O
Bleu Metric
. O
section O
: O
Shared O
Task O
Results O
Table O
[ O
reference O
] O
shows O
the O
ranking O
of O
our O
submitted O
systems O
at O
the O
WMT16 Task
shared Task
news Task
translation Task
task Task
. O
Our O
submissions O
are O
ranked O
( O
tied O
) O
first O
for O
5 O
out O
of O
8 O
translation O
directions O
in O
which O
we O
participated O
: O
EN Material
CS Material
, O
EN Material
DE Material
, O
and O
EN Material
RO Material
. O
They O
are O
also O
the O
( O
tied O
) O
best O
constrained Method
system Method
for O
EN Material
RU Material
and O
RO Material
EN Material
, O
or O
7 O
out O
of O
8 O
translation O
directions O
in O
total O
. O
Our O
models O
are O
also O
used O
in O
QT21 Task
- Task
HimL Task
- Task
SysComb Task
, O
ranked O
1–2 O
for O
EN Material
RO Material
, O
and O
in O
AMU Method
- Method
UEDIN Method
, O
ranked O
2–3 O
for O
EN Material
RU Material
, O
and O
1–2 O
for O
RU Material
EN Material
. O
section O
: O
Conclusion O
We O
describe O
Edinburgh Method
’s Method
neural Method
machine Method
translation Method
systems Method
for O
the O
WMT16 Task
shared Task
news Task
translation Task
task Task
. O
For O
all O
translation O
directions O
, O
we O
observe O
large O
improvements O
in O
translation Metric
quality Metric
from O
using O
synthetic O
parallel O
training O
data O
, O
obtained O
by O
back O
- O
translating O
in O
- O
domain O
monolingual O
target O
- O
side O
data O
. O
Pervasive Method
dropout Method
on O
all O
layers O
was O
used O
for O
English Material
Romanian Material
, O
and O
gave O
substantial O
improvements O
. O
For O
English Material
German Material
and O
English Material
Czech Material
, O
we O
trained O
a O
right Method
- Method
to Method
- Method
left Method
model Method
with O
reversed O
target O
side O
, O
and O
we O
found O
reranking O
the O
system O
output O
with O
these O
reversed O
models O
helpful O
. O
section O
: O
Acknowledgments O
This O
project O
has O
received O
funding O
from O
the O
European O
Union O
’s O
Horizon O
2020 O
research O
and O
innovation O
programme O
under O
grant O
agreements O
645452 O
( O
QT21 O
) O
, O
644333 O
( O
TraMOOC O
) O
and O
644402 O
( O
HimL O
) O
. O
bibliography O
: O
References O
