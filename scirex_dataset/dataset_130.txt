document	O
:	O
Appendix	O
We	O
present	O
the	O
first	O
massively	Method
distributed	Method
architecture	Method
for	O
deep	Task
reinforcement	Task
learning	Task
.	O
This	O
architecture	O
uses	O
four	O
main	O
components	O
:	O
parallel	Method
actors	Method
that	O
generate	O
new	O
behaviour	O
;	O
parallel	Method
learners	Method
that	O
are	O
trained	O
from	O
stored	O
experience	O
;	O
a	O
distributed	Method
neural	Method
network	Method
to	O
represent	O
the	O
value	O
function	O
or	O
behaviour	Method
policy	Method
;	O
and	O
a	O
distributed	O
store	O
of	O
experience	O
.	O
We	O
used	O
our	O
architecture	O
to	O
implement	O
the	O
Deep	Method
Q	Method
-	Method
Network	Method
algorithm	Method
(	O
DQN	Method
)	Method
.	O
Our	O
distributed	Method
algorithm	Method
was	O
applied	O
to	O
49	O
games	O
from	O
Atari	Material
2600	Material
games	Material
from	O
the	O
Arcade	Material
Learning	Material
Environment	Material
,	O
using	O
identical	O
hyperparameters	Method
.	O
Our	O
performance	O
surpassed	O
non	O
-	O
distributed	Method
DQN	Method
in	O
41	O
of	O
the	O
49	O
games	O
and	O
also	O
reduced	O
the	O
wall	Metric
-	Metric
time	Metric
required	O
to	O
achieve	O
these	O
results	O
by	O
an	O
order	O
of	O
magnitude	O
on	O
most	O
games	O
.	O
MassivelyParallelMethodsforDeepReinforcementLearning	O
section	O
:	O
Introduction	O
Deep	Method
learning	Method
methods	Method
have	O
recently	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
vision	Task
and	Task
speech	Task
domains	Task
,	O
mainly	O
due	O
to	O
their	O
ability	O
to	O
automatically	O
learn	O
high	O
-	O
level	O
features	O
from	O
a	O
supervised	Material
signal	Material
.	O
Recent	O
advances	O
in	O
reinforcement	Method
learning	Method
(	O
RL	Task
)	O
have	O
successfully	O
combined	O
deep	Method
learning	Method
with	O
value	Method
function	Method
approximation	Method
,	O
by	O
using	O
a	O
deep	Method
convolutional	Method
neural	Method
network	Method
to	O
represent	O
the	O
action	O
-	O
value	O
(	O
Q	O
)	O
function	O
.	O
Specifically	O
,	O
a	O
new	O
method	O
for	O
training	O
such	O
deep	Method
Q	Method
-	Method
networks	Method
,	O
known	O
as	O
DQN	Method
,	O
has	O
enabled	O
RL	Method
to	O
learn	O
control	Task
policies	Task
in	O
complex	O
environments	O
with	O
high	Material
dimensional	Material
images	Material
as	O
inputs	O
.	O
This	O
method	O
outperformed	O
a	O
human	O
professional	O
in	O
many	O
games	O
on	O
the	O
Atari	Material
2600	Material
platform	Material
,	O
using	O
the	O
same	O
network	Method
architecture	Method
and	O
hyper	O
-	O
parameters	O
.	O
However	O
,	O
DQN	Method
has	O
only	O
previously	O
been	O
applied	O
to	O
single	Method
-	Method
machine	Method
architectures	Method
,	O
in	O
practice	O
leading	O
to	O
long	O
training	Metric
times	Metric
.	O
For	O
example	O
,	O
it	O
took	O
12	O
-	O
14	O
days	O
on	O
a	O
GPU	Method
to	O
train	O
the	O
DQN	Method
algorithm	Method
on	O
a	O
single	O
Atari	Material
game	Material
.	O
In	O
this	O
work	O
,	O
our	O
goal	O
is	O
to	O
build	O
a	O
distributed	Method
architecture	Method
that	O
enables	O
us	O
to	O
scale	O
up	O
deep	Method
reinforcement	Method
learning	Method
algorithms	Method
such	O
as	O
DQN	Method
by	O
exploiting	O
massive	Material
computational	Material
resources	Material
.	O
One	O
of	O
the	O
main	O
advantages	O
of	O
deep	Method
learning	Method
is	O
that	O
computation	Task
can	O
be	O
easily	O
parallelized	O
.	O
In	O
order	O
to	O
exploit	O
this	O
scalability	O
,	O
deep	Method
learning	Method
algorithms	Method
have	O
made	O
extensive	O
use	O
of	O
hardware	O
advances	O
such	O
as	O
GPUs	Method
.	O
However	O
,	O
recent	O
approaches	O
have	O
focused	O
on	O
massively	Method
distributed	Method
architectures	Method
that	O
can	O
learn	O
from	O
more	O
data	O
in	O
parallel	O
and	O
therefore	O
outperform	O
training	O
on	O
a	O
single	O
machine	O
.	O
For	O
example	O
,	O
the	O
DistBelief	Method
framework	Method
distributes	O
the	O
neural	Method
network	Method
parameters	Method
across	O
many	O
machines	O
,	O
and	O
parallelizes	O
the	O
training	Task
by	O
using	O
asynchronous	Method
stochastic	Method
gradient	Method
descent	Method
(	O
ASGD	Method
)	O
.	O
DistBelief	Method
has	O
been	O
used	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
several	O
domains	O
and	O
has	O
been	O
shown	O
to	O
be	O
much	O
faster	O
than	O
single	Method
GPU	Method
training	Method
.	O
Existing	O
work	O
on	O
distributed	Task
deep	Task
learning	Task
has	O
focused	O
exclusively	O
on	O
supervised	Task
and	Task
unsupervised	Task
learning	Task
.	O
In	O
this	O
paper	O
we	O
develop	O
a	O
new	O
architecture	O
for	O
the	O
reinforcement	Method
learning	Method
paradigm	Method
.	O
This	O
architecture	O
consists	O
of	O
four	O
main	O
components	O
:	O
parallel	Method
actors	Method
that	O
generate	O
new	O
behaviour	O
;	O
parallel	Method
learners	Method
that	O
are	O
trained	O
from	O
stored	O
experience	O
;	O
a	O
distributed	Method
neural	Method
network	Method
to	O
represent	O
the	O
value	O
function	O
or	O
behaviour	Method
policy	Method
;	O
and	O
a	O
distributed	Method
experience	Method
replay	Method
memory	Method
.	O
A	O
unique	O
property	O
of	O
RL	Method
is	O
that	O
an	O
agent	O
influences	O
the	O
training	Task
data	Task
distribution	Task
by	O
interacting	O
with	O
its	O
environment	O
.	O
In	O
order	O
to	O
generate	O
more	O
data	O
,	O
we	O
deploy	O
multiple	O
agents	O
running	O
in	O
parallel	O
that	O
interact	O
with	O
multiple	O
instances	O
of	O
the	O
same	O
environment	O
.	O
Each	O
such	O
actor	O
can	O
store	O
its	O
own	O
record	O
of	O
past	O
experience	O
,	O
effectively	O
providing	O
a	O
distributed	Method
experience	Method
replay	Method
memory	Method
with	O
vastly	O
increased	O
capacity	O
compared	O
to	O
a	O
single	O
machine	Method
implementation	Method
.	O
Alternatively	O
this	O
experience	O
can	O
be	O
explicitly	O
aggregated	O
into	O
a	O
distributed	Material
database	Material
.	O
In	O
addition	O
to	O
generating	O
more	O
data	O
,	O
distributed	O
actors	O
can	O
explore	O
the	O
state	O
space	O
more	O
effectively	O
,	O
as	O
each	O
actor	O
behaves	O
according	O
to	O
a	O
slightly	O
different	O
policy	O
.	O
A	O
conceptually	O
distinct	O
set	O
of	O
distributed	Method
learners	Method
reads	O
samples	O
of	O
stored	O
experience	O
from	O
the	O
experience	Material
replay	Material
memory	Material
,	O
and	O
updates	O
the	O
value	Method
function	Method
or	O
policy	Method
according	O
to	O
a	O
given	O
RL	Method
algorithm	Method
.	O
Specifically	O
,	O
we	O
focus	O
in	O
this	O
paper	O
on	O
a	O
variant	O
of	O
the	O
DQN	Method
algorithm	Method
,	O
which	O
applies	O
ASGD	Method
updates	Method
to	O
the	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
.	O
As	O
in	O
DistBelief	Method
,	O
the	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
may	O
also	O
be	O
distributed	O
over	O
many	O
machines	O
.	O
We	O
applied	O
our	O
distributed	Method
framework	Method
for	O
RL	Method
,	O
known	O
as	O
Gorila	Method
(	Method
General	Method
Reinforcement	Method
Learning	Method
Architecture	Method
)	O
,	O
to	O
create	O
a	O
massively	Method
distributed	Method
version	Method
of	Method
the	Method
DQN	Method
algorithm	Method
.	O
We	O
applied	O
Gorila	Method
DQN	Method
to	O
49	O
games	Task
on	O
the	O
Atari	Method
2600	Method
platform	Method
.	O
We	O
outperformed	O
single	O
GPU	Method
DQN	Method
on	O
41	O
games	O
and	O
outperformed	O
human	O
professional	O
on	O
25	O
games	O
.	O
Gorila	Method
DQN	Method
also	O
trained	O
much	O
faster	O
than	O
the	O
non	O
-	O
distributed	Method
version	Method
in	O
terms	O
of	O
wall	Metric
-	Metric
time	Metric
,	O
reaching	O
the	O
performance	O
of	O
single	Method
GPU	Method
DQN	Method
roughly	O
ten	O
times	O
faster	O
for	O
most	O
games	O
.	O
section	O
:	O
Related	O
Work	O
There	O
have	O
been	O
several	O
previous	O
approaches	O
to	O
parallel	Task
or	Task
distributed	Task
RL	Task
.	O
A	O
significant	O
part	O
of	O
this	O
work	O
has	O
focused	O
on	O
distributed	Task
multi	Task
-	Task
agent	Task
systems	Task
.	O
In	O
this	O
approach	O
,	O
there	O
are	O
many	O
agents	O
taking	O
actions	O
within	O
a	O
single	O
shared	O
environment	O
,	O
working	O
cooperatively	O
to	O
achieve	O
a	O
common	O
objective	O
.	O
While	O
computation	Task
is	O
distributed	O
in	O
the	O
sense	O
of	O
decentralized	Task
control	Task
,	O
these	O
algorithms	O
focus	O
on	O
effective	O
teamwork	O
and	O
emergent	O
group	O
behaviors	O
.	O
Another	O
paradigm	O
which	O
has	O
been	O
explored	O
is	O
concurrent	Task
reinforcement	Task
learning	Task
,	O
in	O
which	O
an	O
agent	O
can	O
interact	O
in	O
parallel	O
with	O
an	O
inherently	O
distributed	O
environment	O
,	O
e.g.	O
to	O
optimize	O
interactions	O
with	O
multiple	O
users	O
on	O
the	O
internet	O
.	O
Our	O
goal	O
is	O
quite	O
different	O
to	O
both	O
these	O
distributed	Method
and	Method
concurrent	Method
RL	Method
paradigms	Method
:	O
we	O
simply	O
seek	O
to	O
solve	O
a	O
single	O
-	O
agent	Task
problem	Task
more	O
efficiently	O
by	O
exploiting	O
parallel	Method
computation	Method
.	O
The	O
MapReduce	Method
framework	Method
has	O
been	O
applied	O
to	O
standard	O
MDP	Method
solution	Method
methods	Method
such	O
as	O
policy	Task
evaluation	Task
,	O
policy	Task
iteration	Task
and	O
value	Task
iteration	Task
,	O
by	O
distributing	O
the	O
computation	O
involved	O
in	O
large	O
matrix	Method
multiplications	Method
.	O
However	O
,	O
this	O
work	O
is	O
narrowly	O
focused	O
on	O
batch	Method
methods	Method
for	O
linear	Task
function	Task
approximation	Task
,	O
and	O
is	O
not	O
immediately	O
applicable	O
to	O
non	Method
-	Method
linear	Method
representations	Method
using	O
online	Method
reinforcement	Method
learning	Method
in	O
environments	O
with	O
unknown	O
dynamics	O
.	O
Perhaps	O
the	O
closest	O
prior	O
work	O
to	O
our	O
own	O
is	O
a	O
parallelization	O
of	O
the	O
canonical	Method
Sarsa	Method
algorithm	Method
over	O
multiple	O
machines	O
.	O
Each	O
machine	O
has	O
its	O
own	O
instance	O
of	O
the	O
agent	O
and	O
environment	O
,	O
running	O
a	O
simple	O
reinforcement	Method
learning	Method
algorithm	Method
(	O
linear	Method
Sarsa	Method
,	O
in	O
this	O
case	O
)	O
.	O
The	O
changes	O
to	O
the	O
parameters	O
of	O
the	O
linear	Method
function	Method
approximator	Method
are	O
periodically	O
communicated	O
using	O
a	O
peer	Method
-	Method
to	Method
-	Method
peer	Method
mechanism	Method
,	O
focusing	O
especially	O
on	O
those	O
parameters	O
that	O
have	O
changed	O
most	O
.	O
In	O
contrast	O
,	O
our	O
architecture	O
allows	O
for	O
client	Task
-	Task
server	Task
communication	Task
and	O
a	O
separation	O
between	O
acting	Task
,	O
learning	Task
and	O
parameter	Task
updates	Task
;	O
furthermore	O
we	O
exploit	O
much	O
richer	O
function	Method
approximators	Method
using	O
a	O
distributed	Method
framework	Method
for	O
deep	Task
learning	Task
.	O
section	O
:	O
Background	O
subsection	O
:	O
DistBelief	O
DistBelief	Method
is	O
a	O
distributed	Method
system	Method
for	O
training	O
large	Task
neural	Task
networks	Task
on	O
massive	O
amounts	O
of	O
data	O
efficiently	O
by	O
using	O
two	O
types	O
of	O
parallelism	O
.	O
Model	Method
parallelism	Method
,	O
where	O
different	O
machines	O
are	O
responsible	O
for	O
storing	O
and	O
training	O
different	O
parts	O
of	O
the	O
model	O
,	O
is	O
used	O
to	O
allow	O
efficient	O
training	Task
of	Task
models	Task
much	O
larger	O
than	O
what	O
is	O
feasible	O
on	O
a	O
single	O
machine	O
or	O
GPU	O
.	O
Data	O
parallelism	O
,	O
where	O
multiple	O
copies	O
or	O
replicas	O
of	O
each	O
model	O
are	O
trained	O
on	O
different	O
parts	O
of	O
the	O
data	O
in	O
parallel	O
,	O
allows	O
for	O
more	O
efficient	O
training	Task
on	O
massive	Material
datasets	Material
than	O
a	O
single	O
process	O
.	O
We	O
briefly	O
discuss	O
the	O
two	O
main	O
components	O
of	O
the	O
DistBelief	Method
architecture	Method
–	O
the	O
central	Method
parameter	Method
server	Method
and	O
the	O
model	Method
replicas	Method
.	O
The	O
central	Method
parameter	Method
server	Method
holds	O
the	O
master	O
copy	O
of	O
the	O
model	O
.	O
The	O
job	O
of	O
the	O
parameter	Method
server	Method
is	O
to	O
apply	O
the	O
incoming	O
gradients	O
from	O
the	O
replicas	O
to	O
the	O
model	O
and	O
,	O
when	O
requested	O
,	O
to	O
send	O
its	O
latest	O
copy	O
of	O
the	O
model	O
to	O
the	O
replicas	O
.	O
The	O
parameter	Method
server	Method
can	O
be	O
sharded	O
across	O
many	O
machines	O
and	O
different	O
shards	O
apply	O
gradients	O
independently	O
of	O
other	O
shards	O
.	O
Each	O
replica	O
maintains	O
a	O
copy	O
of	O
the	O
model	O
being	O
trained	O
.	O
This	O
copy	O
could	O
be	O
sharded	O
across	O
multiple	O
machines	O
if	O
,	O
for	O
example	O
,	O
the	O
model	O
is	O
too	O
big	O
to	O
fit	O
on	O
a	O
single	O
machine	O
.	O
The	O
job	O
of	O
the	O
replicas	O
is	O
to	O
calculate	O
the	O
gradients	O
given	O
a	O
mini	O
-	O
batch	O
,	O
send	O
them	O
to	O
the	O
parameter	Method
server	Method
,	O
and	O
to	O
periodically	O
query	O
the	O
parameter	Method
server	Method
for	O
an	O
updated	O
version	O
of	O
the	O
model	O
.	O
The	O
replicas	O
send	O
gradients	O
and	O
request	O
updated	O
parameters	O
independently	O
of	O
each	O
other	O
and	O
hence	O
may	O
not	O
be	O
synced	O
to	O
the	O
same	O
parameters	O
at	O
any	O
given	O
time	O
.	O
subsection	O
:	O
Reinforcement	Method
Learning	Method
In	O
the	O
reinforcement	Method
learning	Method
(	Method
RL	Method
)	Method
paradigm	Method
,	O
the	O
agent	O
interacts	O
sequentially	O
with	O
an	O
environment	O
,	O
with	O
the	O
goal	O
of	O
maximising	O
cumulative	O
rewards	O
.	O
At	O
each	O
step	O
the	O
agent	O
observes	O
state	O
,	O
selects	O
an	O
action	O
,	O
and	O
receives	O
a	O
reward	O
.	O
The	O
agent	O
’s	O
policy	O
maps	O
states	O
to	O
actions	O
and	O
defines	O
its	O
behavior	O
.	O
The	O
goal	O
of	O
an	O
RL	Method
agent	Method
is	O
to	O
maximize	O
its	O
expected	O
total	O
reward	O
,	O
where	O
the	O
rewards	O
are	O
discounted	O
by	O
a	O
factor	O
per	O
time	O
-	O
step	O
.	O
Specifically	O
,	O
the	O
return	O
at	O
time	O
is	O
where	O
is	O
the	O
step	O
when	O
the	O
episode	O
terminates	O
.	O
The	O
action	Method
-	Method
value	Method
function	Method
is	O
the	O
expected	O
return	O
after	O
observing	O
state	O
and	O
taking	O
an	O
action	O
under	O
policy	O
,	O
,	O
and	O
the	O
optimal	O
action	Method
-	Method
value	Method
function	Method
is	O
the	O
maximum	O
possible	O
value	O
that	O
can	O
be	O
achieved	O
by	O
any	O
policy	Method
,	O
.	O
The	O
action	O
-	O
value	O
function	O
obeys	O
a	O
fundamental	O
recursion	Method
known	O
as	O
the	O
Bellman	Method
equation	Method
,	O
.	O
One	O
of	O
the	O
core	O
ideas	O
behind	O
reinforcement	Method
learning	Method
is	O
to	O
represent	O
the	O
action	Method
-	Method
value	Method
function	Method
using	O
a	O
function	Method
approximator	Method
such	O
as	O
a	O
neural	Method
network	Method
,	O
.	O
The	O
parameters	O
of	O
the	O
so	O
-	O
called	O
Q	Method
-	Method
network	Method
are	O
optimized	O
so	O
as	O
to	O
approximately	O
solve	O
the	O
Bellman	O
equation	O
.	O
For	O
example	O
,	O
the	O
Q	Method
-	Method
learning	Method
algorithm	Method
iteratively	O
updates	O
the	O
action	Method
-	Method
value	Method
function	Method
towards	O
a	O
sample	O
of	O
the	O
Bellman	O
target	O
,	O
.	O
However	O
,	O
it	O
is	O
well	O
-	O
known	O
that	O
the	O
Q	Method
-	Method
learning	Method
algorithm	Method
is	O
highly	O
unstable	O
when	O
combined	O
with	O
non	Method
-	Method
linear	Method
function	Method
approximators	Method
such	O
as	O
deep	Method
neural	Method
networks	Method
.	O
subsection	O
:	O
Deep	Method
Q	Method
-	Method
Networks	Method
Recently	O
,	O
a	O
new	O
RL	Method
algorithm	Method
has	O
been	O
developed	O
which	O
is	O
in	O
practice	O
much	O
more	O
stable	O
when	O
combined	O
with	O
deep	Method
Q	Method
-	Method
networks	Method
.	O
Like	O
Q	Method
-	Method
learning	Method
,	O
it	O
iteratively	O
solves	O
the	O
Bellman	Method
equation	Method
by	O
adjusting	O
the	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
towards	O
the	O
Bellman	O
target	O
.	O
However	O
,	O
DQN	Method
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
differs	O
from	O
Q	Method
-	Method
learning	Method
in	O
two	O
ways	O
.	O
First	O
,	O
DQN	Method
uses	O
experience	Method
replay	Method
.	O
At	O
each	O
time	O
-	O
step	O
during	O
an	O
agent	O
’s	O
interaction	O
with	O
the	O
environment	O
it	O
stores	O
the	O
experience	O
tuple	O
into	O
a	O
replay	O
memory	O
.	O
Second	O
,	O
DQN	Method
maintains	O
two	O
separate	O
Q	Method
-	Method
networks	Method
and	O
with	O
current	O
parameters	O
and	O
old	O
parameters	O
respectively	O
.	O
The	O
current	O
parameters	O
may	O
be	O
updated	O
many	O
times	O
per	O
time	O
-	O
step	O
,	O
and	O
are	O
copied	O
into	O
the	O
old	O
parameters	O
after	O
iterations	O
.	O
At	O
every	O
update	O
iteration	O
the	O
current	O
parameters	O
are	O
updated	O
so	O
as	O
to	O
minimise	O
the	O
mean	Metric
-	Metric
squared	Metric
Bellman	Metric
error	Metric
with	O
respect	O
to	O
old	O
parameters	O
,	O
by	O
optimizing	O
the	O
following	O
loss	O
function	O
(	O
DQN	Method
Loss	Method
)	O
,	O
For	O
each	O
update	O
,	O
a	O
tuple	O
of	O
experience	O
(	O
or	O
a	O
minibatch	O
of	O
such	O
samples	O
)	O
is	O
sampled	O
uniformly	O
from	O
the	O
replay	O
memory	O
.	O
For	O
each	O
sample	O
(	O
or	O
minibatch	O
)	O
,	O
the	O
current	O
parameters	O
are	O
updated	O
by	O
a	O
stochastic	Method
gradient	Method
descent	Method
algorithm	Method
.	O
Specifically	O
,	O
is	O
adjusted	O
in	O
the	O
direction	O
of	O
the	O
sample	O
gradient	O
of	O
the	O
loss	O
with	O
respect	O
to	O
,	O
Finally	O
,	O
actions	O
are	O
selected	O
at	O
each	O
time	O
-	O
step	O
by	O
an	O
-	O
greedy	O
behavior	O
with	O
respect	O
to	O
the	O
current	O
Q	Method
-	Method
network	Method
.	O
section	O
:	O
Distributed	Method
Architecture	Method
[	O
t	O
]	O
Distributed	Method
DQN	Method
Algorithm	Method
Initialise	O
replay	O
memory	O
to	O
size	O
.	O
Initialise	O
the	O
training	Method
network	Method
for	O
the	O
action	O
-	O
value	O
function	O
with	O
weights	O
and	O
target	Method
network	Method
with	O
weights	O
.	O
to	O
Initialise	O
the	O
start	O
state	O
to	O
.	O
Update	O
from	O
parameters	O
of	O
the	O
parameter	Method
server	Method
.	O
to	O
With	O
probability	O
take	O
a	O
random	O
action	O
or	O
else	O
=	O
.	O
Execute	O
the	O
action	O
in	O
the	O
environment	O
and	O
observe	O
the	O
reward	O
and	O
the	O
next	O
state	O
.	O
Store	O
in	O
.	O
Update	O
from	O
parameters	O
of	O
the	O
parameter	Method
server	Method
.	O
Sample	O
random	O
mini	O
-	O
batch	O
from	O
.	O
And	O
for	O
each	O
tuple	O
set	O
target	O
as	O
is	O
Calculate	O
the	O
loss	O
.	O
Compute	O
gradients	O
with	O
respect	O
to	O
the	O
network	O
parameters	O
using	O
equation	O
[	O
reference	O
]	O
.	O
Send	O
gradients	O
to	O
the	O
parameter	Method
server	Method
.	O
Every	O
global	O
steps	O
sync	O
with	O
parameters	O
from	O
the	O
parameter	Method
server	Method
.	O
We	O
now	O
introduce	O
Gorila	Method
(	Method
General	Method
Reinforcement	Method
Learning	Method
Architecture	Method
)	O
,	O
a	O
framework	O
for	O
massively	Task
distributed	Task
reinforcement	Task
learning	Task
.	O
The	O
Gorila	Method
architecture	Method
,	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
contains	O
the	O
following	O
components	O
:	O
Actors	O
.	O
Any	O
reinforcement	Method
learning	Method
agent	Method
must	O
ultimately	O
select	O
actions	O
to	O
apply	O
in	O
its	O
environment	O
.	O
We	O
refer	O
to	O
this	O
process	O
as	O
acting	O
.	O
The	O
Gorila	Method
architecture	Method
contains	O
different	O
actor	Method
processes	Method
,	O
applied	O
to	O
corresponding	O
instantiations	O
of	O
the	O
same	O
environment	O
.	O
Each	O
actor	O
generates	O
its	O
own	O
trajectories	O
of	O
experience	O
within	O
the	O
environment	O
,	O
and	O
as	O
a	O
result	O
each	O
actor	O
may	O
visit	O
different	O
parts	O
of	O
the	O
state	O
space	O
.	O
The	O
quantity	O
of	O
experience	O
that	O
is	O
generated	O
by	O
the	O
actors	O
after	O
time	O
-	O
steps	O
is	O
approximately	O
.	O
Each	O
actor	O
contains	O
a	O
replica	O
of	O
the	O
Q	Method
-	Method
network	Method
,	O
which	O
is	O
used	O
to	O
determine	O
behavior	O
,	O
for	O
example	O
using	O
an	O
-	Method
greedy	Method
policy	Method
.	O
The	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
are	O
synchronized	O
periodically	O
from	O
the	O
parameter	Method
server	Method
.	O
Experience	Task
replay	Task
memory	Task
.	O
The	O
experience	O
tuples	O
generated	O
by	O
the	O
actors	O
are	O
stored	O
in	O
a	O
replay	O
memory	O
.	O
We	O
consider	O
two	O
forms	O
of	O
experience	Task
replay	Task
memory	Task
.	O
First	O
,	O
a	O
local	O
replay	O
memory	O
stores	O
each	O
actor	O
’s	O
experience	O
locally	O
on	O
that	O
actor	O
’s	O
machine	O
.	O
If	O
a	O
single	O
machine	O
has	O
sufficient	O
memory	O
to	O
store	O
experience	O
tuples	O
,	O
then	O
the	O
overall	O
memory	Metric
capacity	Metric
becomes	O
.	O
Second	O
,	O
a	O
global	Method
replay	Method
memory	Method
aggregates	O
the	O
experience	O
into	O
a	O
distributed	Material
database	Material
.	O
In	O
this	O
approach	O
the	O
overall	O
memory	Metric
capacity	Metric
is	O
independent	O
of	O
and	O
may	O
be	O
scaled	O
as	O
desired	O
,	O
at	O
the	O
cost	O
of	O
additional	O
communication	Metric
overhead	Metric
.	O
Learners	O
.	O
Gorila	Method
contains	O
learner	Method
processes	Method
.	O
Each	O
learner	O
contains	O
a	O
replica	O
of	O
the	O
Q	Method
-	Method
network	Method
and	O
its	O
job	O
is	O
to	O
compute	O
desired	O
changes	O
to	O
the	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
.	O
For	O
each	O
learner	Task
update	Task
,	O
a	O
minibatch	O
of	O
experience	O
tuples	O
is	O
sampled	O
from	O
either	O
a	O
local	O
or	O
global	O
experience	O
replay	O
memory	O
(	O
see	O
above	O
)	O
.	O
The	O
learner	Method
applies	O
an	O
off	Method
-	Method
policy	Method
RL	Method
algorithm	Method
such	O
as	O
DQN	Method
to	O
this	O
minibatch	O
of	O
experience	O
,	O
in	O
order	O
to	O
generate	O
a	O
gradient	O
vector	O
.	O
The	O
gradients	O
are	O
communicated	O
to	O
the	O
parameter	Method
server	Method
;	O
and	O
the	O
parameters	O
of	O
the	O
Q	Method
-	Method
network	Method
are	O
updated	O
periodically	O
from	O
the	O
parameter	Method
server	Method
.	O
Parameter	Method
server	Method
.	O
Like	O
DistBelief	Method
,	O
the	O
Gorila	Method
architecture	Method
uses	O
a	O
central	Method
parameter	Method
server	Method
to	O
maintain	O
a	O
distributed	Method
representation	Method
of	Method
the	Method
Q	Method
-	Method
network	Method
.	O
The	O
parameter	O
vector	O
is	O
split	O
disjointly	O
across	O
different	O
machines	O
.	O
Each	O
machine	O
is	O
responsible	O
for	O
applying	O
gradient	Method
updates	Method
to	O
a	O
subset	O
of	O
the	O
parameters	O
.	O
The	O
parameter	Method
server	Method
receives	O
gradients	O
from	O
the	O
learners	Method
,	O
and	O
applies	O
these	O
gradients	O
to	O
modify	O
the	O
parameter	O
vector	O
,	O
using	O
an	O
asynchronous	Method
stochastic	Method
gradient	Method
descent	Method
algorithm	Method
.	O
The	O
Gorila	Method
architecture	Method
provides	O
considerable	O
flexibility	O
in	O
the	O
number	O
of	O
ways	O
an	O
RL	Method
agent	Method
may	O
be	O
parallelized	O
.	O
It	O
is	O
possible	O
to	O
have	O
parallel	Method
acting	Method
to	O
generate	O
large	O
quantities	O
of	O
data	O
into	O
a	O
global	Material
replay	Material
database	Material
,	O
and	O
then	O
process	O
that	O
data	O
with	O
a	O
single	O
serial	Method
learner	Method
.	O
In	O
contrast	O
,	O
it	O
is	O
possible	O
to	O
have	O
a	O
single	O
actor	O
generating	O
data	O
into	O
a	O
local	O
replay	O
memory	O
,	O
and	O
then	O
have	O
multiple	O
learners	O
process	O
this	O
data	O
in	O
parallel	O
to	O
learn	O
as	O
effectively	O
as	O
possible	O
from	O
this	O
experience	O
.	O
However	O
,	O
to	O
avoid	O
any	O
individual	O
component	O
from	O
becoming	O
a	O
bottleneck	O
,	O
the	O
Gorila	Method
architecture	Method
in	O
general	O
allows	O
for	O
arbitrary	O
numbers	O
of	O
actors	O
,	O
learners	O
,	O
and	O
parameter	O
servers	O
to	O
both	O
generate	O
data	O
,	O
learn	O
from	O
that	O
data	O
,	O
and	O
update	O
the	O
model	O
in	O
a	O
scalable	O
and	O
fully	O
distributed	O
fashion	O
.	O
The	O
simplest	O
overall	O
instantiation	O
of	O
Gorila	Method
,	O
which	O
we	O
consider	O
in	O
our	O
subsequent	O
experiments	O
,	O
is	O
the	O
bundled	Method
mode	Method
in	O
which	O
there	O
is	O
a	O
one	O
-	O
to	O
-	O
one	O
correspondence	O
between	O
actors	O
,	O
replay	O
memory	O
,	O
and	O
learners	O
(	O
)	O
.	O
Each	O
bundle	O
has	O
an	O
actor	O
generating	O
experience	O
,	O
a	O
local	O
replay	O
memory	O
to	O
store	O
that	O
experience	O
,	O
and	O
a	O
learner	O
that	O
updates	O
parameters	O
based	O
on	O
samples	O
of	O
experience	O
from	O
the	O
local	O
replay	O
memory	O
.	O
The	O
only	O
communication	O
between	O
bundles	O
is	O
via	O
parameters	O
:	O
the	O
learners	O
communicate	O
their	O
gradients	O
to	O
the	O
parameter	Method
server	Method
;	O
and	O
the	O
Q	Method
-	Method
networks	Method
in	O
the	O
actors	O
and	O
learners	O
are	O
periodically	O
synchronized	O
to	O
the	O
parameter	Method
server	Method
.	O
subsection	O
:	O
Gorila	O
DQN	O
We	O
now	O
consider	O
a	O
specific	O
instantiation	O
of	O
the	O
Gorila	Method
architecture	Method
implementing	O
the	O
DQN	Method
algorithm	Method
.	O
As	O
described	O
in	O
the	O
previous	O
section	O
,	O
the	O
DQN	Method
algorithm	Method
utilizes	O
two	O
copies	O
of	O
the	O
Q	Method
-	Method
network	Method
:	O
a	O
current	O
Q	Method
-	Method
network	Method
with	O
parameters	O
and	O
a	O
target	O
Q	Method
-	Method
network	Method
with	O
parameters	O
.	O
The	O
DQN	Method
algorithm	Method
is	O
extended	O
to	O
the	O
distributed	Task
implementation	Task
in	O
Gorila	Task
as	O
follows	O
.	O
The	O
parameter	Method
server	Method
maintains	O
the	O
current	O
parameters	O
and	O
the	O
actors	O
and	O
learners	O
contain	O
replicas	O
of	O
the	O
current	O
Q	Method
-	Method
network	Method
that	O
are	O
synchronized	O
from	O
the	O
parameter	Method
server	Method
before	O
every	O
acting	O
step	O
.	O
The	O
learner	O
additionally	O
maintains	O
the	O
target	O
Q	Method
-	Method
network	Method
.	O
The	O
learner	O
’s	O
target	O
network	O
is	O
updated	O
from	O
the	O
parameter	Method
server	Method
after	O
every	O
gradient	O
updates	O
in	O
the	O
central	O
parameter	O
server	O
.	O
Note	O
that	O
is	O
a	O
global	O
parameter	O
that	O
counts	O
the	O
total	O
number	O
of	O
updates	O
to	O
the	O
central	O
parameter	O
server	O
rather	O
than	O
counting	O
the	O
updates	O
from	O
the	O
local	Method
learner	Method
.	O
The	O
learners	O
generate	O
gradients	O
using	O
the	O
DQN	Method
gradient	Method
given	O
in	O
Equation	O
[	O
reference	O
]	O
.	O
However	O
,	O
the	O
gradients	O
are	O
not	O
applied	O
directly	O
,	O
but	O
instead	O
communicated	O
to	O
the	O
parameter	Method
server	Method
.	O
The	O
parameter	Method
server	Method
then	O
applies	O
the	O
updates	O
that	O
are	O
accumulated	O
from	O
many	O
learners	O
.	O
subsection	O
:	O
Stability	O
While	O
the	O
DQN	Method
training	Method
algorithm	Method
was	O
designed	O
to	O
ensure	O
stability	O
of	O
training	Method
neural	Method
networks	Method
with	O
reinforcement	Method
learning	Method
,	O
training	O
using	O
a	O
large	O
cluster	O
of	O
machines	O
running	O
multiple	O
other	O
tasks	O
poses	O
additional	O
challenges	O
.	O
The	O
Gorila	Method
DQN	Method
implementation	Method
uses	O
additional	O
safeguards	O
to	O
ensure	O
stability	O
in	O
the	O
presence	O
of	O
disappearing	O
nodes	O
,	O
slowdowns	O
in	O
network	O
traffic	O
,	O
and	O
slowdowns	O
of	O
individual	O
machines	O
.	O
One	O
such	O
safeguard	O
is	O
a	O
parameter	O
that	O
determines	O
the	O
maximum	O
time	O
delay	O
between	O
the	O
local	O
parameters	O
(	O
the	O
gradients	O
are	O
computed	O
using	O
)	O
and	O
the	O
parameters	O
in	O
the	O
parameter	Method
server	Method
.	O
All	O
gradients	O
older	O
than	O
the	O
threshold	O
are	O
discarded	O
by	O
the	O
parameter	Method
server	Method
.	O
Additionally	O
,	O
each	O
actor	Method
/	Method
learner	Method
keeps	O
a	O
running	Metric
average	Metric
and	O
standard	Metric
deviation	Metric
of	O
the	O
absolute	Metric
DQN	Metric
loss	Metric
for	O
the	O
data	O
it	O
sees	O
and	O
discards	O
gradients	O
with	O
absolute	O
loss	O
higher	O
than	O
the	O
mean	O
plus	O
several	O
standard	O
deviations	O
.	O
Finally	O
,	O
we	O
used	O
the	O
AdaGrad	Method
update	Method
rule	Method
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Experimental	O
Set	O
Up	O
We	O
evaluated	O
Gorila	Method
by	O
conducting	O
experiments	O
on	O
49	O
Atari	Material
2600	Material
games	Material
using	O
the	O
Arcade	Method
Learning	Method
Environment	Method
.	O
Atari	Task
games	Task
provide	O
a	O
challenging	O
and	O
diverse	O
set	O
of	O
reinforcement	Task
learning	Task
problems	Task
where	O
an	O
agent	O
must	O
learn	O
to	O
play	O
the	O
games	O
directly	O
from	O
RGB	Material
video	Material
input	Material
with	O
only	O
the	O
changes	O
in	O
the	O
score	O
provided	O
as	O
rewards	O
.	O
We	O
closely	O
followed	O
the	O
experimental	O
setup	O
of	O
DQN	Method
using	O
the	O
same	O
preprocessing	Method
and	Method
network	Method
architecture	Method
.	O
We	O
preprocessed	O
the	O
RGB	Material
images	Material
by	O
downsampling	O
them	O
to	O
and	O
extracting	O
the	O
luminance	O
channel	O
.	O
The	O
Q	Method
-	Method
network	Method
had	O
3	O
convolutional	Method
layers	Method
followed	O
by	O
a	O
fully	Method
-	Method
connected	Method
hidden	Method
layer	Method
.	O
The	O
input	O
to	O
the	O
network	O
is	O
obtained	O
by	O
concatenating	O
the	O
images	O
from	O
four	O
previous	O
preprocessed	Material
frames	Material
.	O
The	O
first	O
convolutional	Method
layer	Method
had	O
filters	O
of	O
size	O
and	O
stride	O
.	O
The	O
second	O
convolutional	Method
layer	Method
had	O
filters	O
of	O
size	O
with	O
stride	O
,	O
while	O
the	O
third	O
had	O
filters	O
with	O
size	O
and	O
stride	O
.	O
The	O
next	O
layer	O
had	O
fully	O
-	O
connected	O
output	O
units	O
,	O
which	O
is	O
followed	O
by	O
a	O
linear	Method
fully	Method
-	Method
connected	Method
output	Method
layer	Method
with	O
a	O
single	O
output	O
unit	O
for	O
each	O
valid	O
action	O
.	O
Each	O
hidden	O
layer	O
was	O
followed	O
by	O
a	O
rectifier	Method
nonlinearity	Method
.	O
We	O
have	O
used	O
the	O
same	O
frame	O
skipping	O
step	O
implemented	O
in	O
by	O
repeating	O
every	O
action	O
over	O
the	O
next	O
frames	O
.	O
In	O
all	O
experiments	O
,	O
Gorila	Method
DQN	Method
used	O
:	O
and	O
100	O
.	O
We	O
use	O
the	O
bundled	Method
mode	Method
.	O
Replay	O
memory	O
size	O
1	O
million	O
frames	O
and	O
used	O
-	O
greedy	O
as	O
the	O
behaviour	Method
policy	Method
with	O
annealed	O
from	O
1	O
to	O
0.1	O
over	O
the	O
first	O
one	O
million	O
global	O
updates	O
.	O
Each	O
learner	O
syncs	O
the	O
parameters	O
of	O
its	O
target	O
network	O
after	O
every	O
60	O
K	O
parameter	O
updates	O
performed	O
in	O
the	O
parameter	Method
server	Method
.	O
subsection	O
:	O
Evaluation	O
We	O
used	O
two	O
types	O
of	O
evaluations	O
.	O
The	O
first	O
follows	O
the	O
protocol	O
established	O
by	O
DQN	Method
.	O
Each	O
trained	O
agent	O
was	O
evaluated	O
on	O
30	O
episodes	O
of	O
the	O
game	O
it	O
was	O
trained	O
on	O
.	O
A	O
random	O
number	O
of	O
frames	O
were	O
skipped	O
by	O
repeatedly	O
taking	O
the	O
null	O
or	O
do	O
nothing	O
action	O
before	O
giving	O
control	O
to	O
the	O
agent	O
in	O
order	O
to	O
ensure	O
variation	O
in	O
the	O
initial	O
conditions	O
.	O
The	O
agents	O
were	O
allowed	O
to	O
play	O
until	O
the	O
end	O
of	O
the	O
game	O
or	O
up	O
to	O
18000	O
frames	O
(	O
5	O
minutes	O
)	O
,	O
whichever	O
came	O
first	O
,	O
and	O
the	O
scores	O
were	O
averaged	O
over	O
all	O
30	O
episodes	O
.	O
We	O
refer	O
to	O
this	O
evaluation	O
procedure	O
as	O
null	Task
op	Task
starts	Task
.	O
Testing	O
how	O
well	O
an	O
agent	O
generalizes	O
is	O
especially	O
important	O
in	O
the	O
Atari	Material
domain	Material
because	O
the	O
emulator	Method
is	O
completely	O
deterministic	O
.	O
Our	O
second	O
evaluation	O
method	O
,	O
which	O
we	O
call	O
human	O
starts	O
,	O
aims	O
to	O
measure	O
how	O
well	O
the	O
agent	O
generalizes	O
to	O
states	O
it	O
may	O
not	O
have	O
trained	O
on	O
.	O
To	O
that	O
end	O
,	O
we	O
have	O
introduced	O
100	O
random	O
starting	O
points	O
that	O
were	O
sampled	O
from	O
a	O
human	O
professional	O
’s	O
gameplay	O
for	O
each	O
game	O
.	O
To	O
evaluate	O
an	O
agent	O
,	O
we	O
ran	O
it	O
from	O
each	O
of	O
the	O
100	O
starting	O
points	O
until	O
the	O
end	O
of	O
the	O
game	O
or	O
until	O
a	O
total	O
of	O
108000	O
frames	O
(	O
equivalent	O
to	O
30	O
minutes	O
)	O
were	O
played	O
counting	O
the	O
frames	O
the	O
human	O
played	O
to	O
reach	O
the	O
starting	O
point	O
.	O
The	O
total	O
score	O
accumulated	O
only	O
by	O
the	O
agent	O
(	O
not	O
considering	O
any	O
points	O
won	O
by	O
the	O
human	O
player	O
)	O
were	O
averaged	O
to	O
obtain	O
the	O
evaluation	Metric
score	Metric
.	O
In	O
order	O
to	O
make	O
it	O
easier	O
to	O
compare	O
results	O
on	O
49	O
games	O
with	O
a	O
greatly	O
varying	O
range	O
of	O
scores	O
we	O
present	O
the	O
results	O
on	O
a	O
scale	O
where	O
0	O
is	O
the	O
score	O
obtained	O
by	O
a	O
random	O
agent	O
and	O
100	O
is	O
the	O
score	O
obtained	O
by	O
a	O
professional	O
human	Method
game	Method
player	Method
.	O
The	O
random	Method
agent	Method
selected	O
actions	O
uniformly	O
at	O
random	O
at	O
10Hz	O
and	O
it	O
was	O
evaluated	O
using	O
the	O
same	O
starting	O
states	O
as	O
the	O
agents	O
for	O
both	O
kinds	O
of	O
evaluations	O
(	O
null	O
op	O
starts	O
and	O
human	O
starts	O
)	O
.	O
We	O
selected	O
hyperparameter	O
values	O
by	O
performing	O
an	O
informal	O
search	O
on	O
the	O
games	O
of	O
Breakout	Material
,	O
Pong	Material
and	O
Seaquest	Material
which	O
were	O
then	O
fixed	O
for	O
all	O
the	O
games	O
.	O
We	O
have	O
trained	O
Gorila	Method
DQN	Method
5	O
times	O
on	O
each	O
game	O
using	O
the	O
same	O
fixed	O
hyperparameter	O
settings	O
and	O
random	Method
network	Method
initializations	Method
.	O
Following	O
DQN	Method
,	O
we	O
periodically	O
evaluated	O
each	O
model	O
during	O
training	O
and	O
kept	O
the	O
best	O
performing	O
network	O
parameters	O
for	O
the	O
final	O
evaluation	O
.	O
We	O
average	O
these	O
final	O
evaluations	O
over	O
the	O
5	O
runs	O
,	O
and	O
compare	O
the	O
mean	O
evaluations	O
with	O
DQN	O
and	O
human	Metric
expert	Metric
scores	Metric
.	O
section	O
:	O
Results	O
We	O
first	O
compared	O
Gorila	Method
DQN	Method
agents	Method
trained	O
for	O
up	O
to	O
6	O
days	O
to	O
single	O
GPU	Method
DQN	Method
agents	Method
trained	O
for	O
12	O
-	O
14	O
days	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
normalized	O
scores	O
under	O
the	O
human	O
starts	O
evaluation	O
.	O
Using	O
human	Method
starts	Method
Gorila	Method
DQN	Method
outperformed	O
single	Method
GPU	Method
DQN	Method
on	O
41	O
out	O
of	O
49	O
games	O
given	O
roughly	O
one	O
half	O
of	O
the	O
training	Metric
time	Metric
of	O
single	Method
GPU	Method
DQN	Method
.	O
On	O
22	O
of	O
the	O
games	O
Gorila	O
DQN	Method
obtained	O
double	O
the	O
score	O
of	O
single	Method
GPU	Method
DQN	Method
,	O
and	O
on	O
11	O
games	O
Gorila	O
DQN	Method
’s	O
score	O
was	O
5	O
times	O
higher	O
.	O
Similarly	O
,	O
using	O
the	O
original	O
null	Method
op	Method
starts	O
evaluation	O
Gorila	Method
DQN	Method
outperformed	O
the	O
single	Method
GPU	Method
DQN	Method
on	O
31	O
out	O
of	O
49	O
games	O
.	O
These	O
results	O
show	O
that	O
parallel	Method
training	Method
significantly	O
improved	O
performance	O
in	O
less	O
training	Metric
time	Metric
.	O
Also	O
,	O
better	O
results	O
on	O
human	O
starts	O
compared	O
to	O
null	O
op	O
starts	O
suggest	O
that	O
Gorila	Method
DQN	Method
is	O
especially	O
good	O
at	O
generalizing	O
to	O
potentially	O
unseen	O
states	O
compared	O
to	O
single	Method
GPU	Method
DQN	Method
.	O
Figure	O
[	O
reference	O
]	O
further	O
illustrates	O
these	O
improvements	O
in	O
generalization	Task
by	O
showing	O
Gorila	Metric
DQN	Metric
scores	Metric
with	O
human	O
starts	O
normalized	O
with	O
respect	O
to	O
GPU	O
DQN	O
scores	O
with	O
human	O
starts	O
(	O
blue	O
bars	O
)	O
and	O
Gorila	Metric
DQN	Metric
scores	Metric
from	O
null	O
op	O
starts	O
normalized	O
by	O
GPU	Method
DQN	Method
scores	Method
from	O
null	O
op	O
starts	O
(	O
gray	O
bars	O
)	O
.	O
In	O
fact	O
,	O
Gorila	Method
DQN	Method
performs	O
at	O
a	O
level	O
similar	O
or	O
superior	O
to	O
a	O
human	O
professional	O
(	O
75	O
%	O
of	O
the	O
human	O
score	O
or	O
above	O
)	O
in	O
25	O
games	O
despite	O
starting	O
from	O
states	O
sampled	O
from	O
human	O
play	O
.	O
One	O
possible	O
reason	O
for	O
the	O
improved	O
generalization	Task
is	O
the	O
significant	O
increase	O
in	O
the	O
number	O
of	O
states	O
Gorila	O
DQN	O
sees	O
by	O
using	O
100	O
parallel	O
actors	O
.	O
We	O
next	O
look	O
at	O
how	O
the	O
performance	O
of	O
Gorila	Method
DQN	Method
improved	O
during	O
training	Task
.	O
Figure	O
[	O
reference	O
]	O
shows	O
how	O
quickly	O
Gorila	Method
DQN	Method
reached	O
the	O
performance	O
of	O
single	Method
GPU	Method
DQN	Method
and	O
how	O
quickly	O
Gorila	Method
DQN	Method
reached	O
its	O
own	O
best	O
score	O
under	O
the	O
human	Metric
starts	Metric
evaluation	Metric
.	O
Gorila	Method
DQN	Method
surpassed	O
the	O
best	O
single	O
GPU	Metric
DQN	Metric
scores	Metric
on	O
19	O
games	O
in	O
6	O
hours	O
,	O
23	O
games	O
in	O
12	O
hours	O
,	O
30	O
in	O
24	O
hours	O
and	O
38	O
games	O
in	O
36	O
hours	O
(	O
red	O
curve	O
)	O
.	O
This	O
is	O
a	O
roughly	O
an	O
order	O
of	O
magnitude	O
reduction	O
in	O
training	Metric
time	Metric
required	O
to	O
reach	O
the	O
single	O
process	O
DQN	Metric
score	Metric
.	O
On	O
some	O
games	O
Gorila	O
DQN	Method
achieved	O
its	O
best	O
score	O
in	O
under	O
two	O
days	O
but	O
for	O
most	O
of	O
the	O
games	O
the	O
performance	O
keeps	O
improving	O
with	O
longer	O
training	O
time	O
(	O
blue	O
curve	O
)	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
we	O
have	O
introduced	O
the	O
first	O
massively	Method
distributed	Method
architecture	Method
for	O
deep	Task
reinforcement	Task
learning	Task
.	O
The	O
Gorila	Method
architecture	Method
acts	O
and	O
learns	O
in	O
parallel	O
,	O
using	O
a	O
distributed	Method
replay	Method
memory	Method
and	O
distributed	Method
neural	Method
network	Method
.	O
We	O
applied	O
Gorila	Method
to	O
an	O
asynchronous	Method
variant	Method
of	O
the	O
state	O
-	O
of	O
-	Method
the	Method
-	Method
art	Method
DQN	Method
algorithm	Method
.	O
A	O
single	O
machine	O
had	O
previously	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
the	O
challenging	O
suite	O
of	O
Atari	Material
2600	Material
games	Material
,	O
but	O
it	O
was	O
not	O
previously	O
known	O
whether	O
the	O
good	O
performance	O
of	O
DQN	Method
would	O
continue	O
to	O
scale	O
with	O
additional	O
computation	O
.	O
By	O
leveraging	O
massive	O
parallelism	O
,	O
Gorila	Method
DQN	Method
significantly	O
outperformed	O
single	Method
-	Method
GPU	Method
DQN	Method
on	O
41	O
out	O
of	O
49	O
games	O
;	O
achieving	O
by	O
far	O
the	O
best	O
results	O
in	O
this	O
domain	O
to	O
date	O
.	O
Gorila	Method
takes	O
a	O
further	O
step	O
towards	O
fulfilling	O
the	O
promise	O
of	O
deep	Method
learning	Method
in	O
RL	Method
:	O
a	O
scalable	Method
architecture	Method
that	O
performs	O
better	O
and	O
better	O
with	O
increased	O
computation	O
and	O
memory	O
.	O
bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
Data	O
We	O
present	O
all	O
the	O
data	O
that	O
has	O
been	O
used	O
in	O
the	O
paper	O
.	O
Table	O
1	O
shows	O
the	O
various	O
normalized	Metric
scores	Metric
for	O
null	Task
op	Task
evaluation	Task
.	O
Table	O
2	O
shows	O
the	O
various	O
normalized	Metric
scores	Metric
for	O
human	Task
start	Task
evaluation	Task
.	O
Table	O
3	O
shows	O
the	O
various	O
raw	O
scores	O
for	O
human	Task
start	Task
evaluation	Task
.	O
Table	O
4	O
shows	O
the	O
various	O
raw	O
scores	O
for	O
null	Task
op	Task
evaluation	Task
.	O
