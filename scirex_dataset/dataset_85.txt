document O
: O
Count Task
- Task
Based Task
Exploration Task
in Task
Feature Task
Space Task
for O
Reinforcement Task
Learning Task
We O
introduce O
a O
new O
count Method
- Method
based Method
optimistic Method
exploration Method
algorithm Method
for O
reinforcement Task
learning Task
( Task
RL Task
) O
that O
is O
feasible O
in O
environments O
with O
high O
- O
dimensional O
state O
- O
action O
spaces O
. O
The O
success O
of O
RL Method
algorithms Method
in O
these O
domains O
depends O
crucially O
on O
generalisation O
from O
limited O
training O
experience O
. O
Function Method
approximation Method
techniques Method
enable O
RL Method
agents Method
to O
generalise O
in O
order O
to O
estimate O
the O
value O
of O
unvisited O
states O
, O
but O
at O
present O
few O
methods O
enable O
generalisation O
regarding O
uncertainty O
. O
This O
has O
prevented O
the O
combination O
of O
scalable Method
RL Method
algorithms Method
with O
efficient O
exploration Method
strategies Method
that O
drive O
the O
agent O
to O
reduce O
its O
uncertainty O
. O
We O
present O
a O
new O
method O
for O
computing O
a O
generalised O
state O
visit O
- O
count O
, O
which O
allows O
the O
agent O
to O
estimate O
the O
uncertainty O
associated O
with O
any O
state O
. O
Our O
ϕ O
- O
pseudocount O
achieves O
generalisation Task
by O
exploiting O
the O
same O
feature Method
representation Method
of Method
the Method
state Method
space Method
that O
is O
used O
for O
value Method
function Method
approximation Method
. O
States O
that O
have O
less O
frequently O
observed O
features O
are O
deemed O
more O
uncertain O
. O
The O
ϕ Method
- Method
Exploration Method
- Method
Bonus Method
algorithm Method
rewards O
the O
agent O
for O
exploring O
in O
feature O
space O
rather O
than O
in O
the O
untransformed O
state O
space O
. O
The O
method O
is O
simpler O
and O
less O
computationally O
expensive O
than O
some O
previous O
proposals O
, O
and O
achieves O
near O
state O
- O
of O
- O
the O
- O
art O
results O
on O
high Task
- Task
dimensional Task
RL Task
benchmarks Task
. O
section O
: O
Introduction O
Reinforcement Method
learning Method
( Method
RL Method
) Method
methods Method
have O
recently O
enjoyed O
widely O
publicised O
success O
in O
domains O
that O
once O
seemed O
far O
beyond O
their O
reach O
. O
Much O
of O
this O
progress O
is O
due O
to O
the O
application O
of O
modern O
function Method
approximation Method
techniques Method
to O
the O
problem O
of O
policy Task
evaluation Task
for O
Markov Task
Decision Task
Processes Task
( O
MDPs Task
) O
. O
These O
techniques O
address O
a O
key O
shortcoming O
of O
tabular Method
MDP Method
solution Method
methods Method
: O
their O
inability O
to O
generalise O
what O
is O
learnt O
from O
one O
context O
to O
another O
. O
This O
sort O
of O
generalisation O
is O
crucial O
if O
the O
state O
- O
action O
space O
of O
the O
MDP Method
is O
large O
, O
because O
the O
agent O
typically O
only O
visits O
a O
small O
subset O
of O
that O
space O
during O
training O
. O
Comparatively O
little O
progress O
has O
been O
made O
on O
the O
problem O
of O
efficient Task
exploration Task
in Task
large Task
domains Task
. O
Even O
algorithms O
that O
use O
sophisticated O
nonlinear Method
methods Method
for O
policy Task
evaluation Task
tend O
to O
use O
very O
old O
, O
inefficient O
exploration Method
techniques Method
, O
such O
as O
the O
- Method
greedy Method
strategy Method
. O
There O
are O
more O
efficient O
tabular Method
count Method
- Method
based Method
exploration Method
algorithms Method
for O
finite Task
MDPs Task
, O
which O
drive O
the O
agent O
to O
reduce O
its O
uncertainty O
by O
visiting O
states O
that O
have O
low O
visit O
- O
counts O
. O
However O
, O
these O
algorithms O
are O
often O
ineffective O
in O
MDPs Task
with Task
high Task
- Task
dimensional Task
state Task
- Task
action Task
spaces Task
, O
because O
most O
states O
are O
never O
visited O
during O
training O
, O
and O
the O
visit O
- O
count O
remains O
at O
zero O
nearly O
everywhere O
. O
Count Method
- Method
based Method
exploration Method
algorithms Method
have O
only O
very O
recently O
been O
successfully O
adapted O
for O
these O
large O
problems O
. O
Just O
as O
function Method
approximation Method
techniques Method
achieve O
generalisation O
across O
the O
state O
space O
regarding O
value O
, O
these O
algorithms O
achieve O
generalisation O
regarding O
uncertainty O
. O
The O
breakthrough O
has O
been O
the O
development O
of O
generalised O
state O
visit O
- O
counts O
, O
which O
are O
larger O
for O
states O
that O
are O
more O
similar O
to O
visited O
states O
, O
and O
which O
can O
be O
nonzero O
for O
unvisited O
states O
. O
The O
key O
challenge O
is O
to O
compute O
an O
appropriate O
similarity Metric
measure Metric
in O
an O
efficient O
way O
, O
such O
that O
these O
exploration Method
methods Method
can O
be O
combined O
with O
scalable Method
RL Method
algorithms Method
. O
It O
soon O
becomes O
infeasible O
, O
for O
example O
, O
to O
do O
so O
by O
storing O
the O
entire O
history O
of O
visited O
states O
and O
comparing O
each O
new O
state O
to O
those O
in O
the O
history O
. O
The O
most O
promising O
proposals O
instead O
compute O
generalised O
counts O
from O
a O
compressed Method
representation Method
of Method
the Method
history Method
of Method
visited Method
states Method
– O
for O
example O
, O
by O
constructing O
a O
visit Method
- Method
density Method
model Method
over O
the O
state O
space O
and O
deriving O
a O
“ O
pseudocount Method
” Method
, O
or O
by O
using O
locality Method
- Method
sensitive Method
hashing Method
to O
cluster O
states O
and O
counting O
the O
occurrences O
in O
each O
cluster O
. O
This O
paper O
presents O
a O
new O
count Method
- Method
based Method
exploration Method
algorithm Method
that O
is O
feasible O
in O
environments O
with O
large O
state O
- O
action O
spaces O
. O
It O
can O
be O
combined O
with O
any O
value Method
- Method
based Method
RL Method
algorithm Method
that O
uses O
linear Method
function Method
approximation Method
( O
LFA Method
) O
. O
Our O
principal O
contribution O
is O
a O
new O
method O
for O
computing Task
generalised Task
visit Task
- Task
counts Task
. O
Following O
, O
we O
construct O
a O
visit Method
- Method
density Method
model Method
in O
order O
to O
measure O
the O
similarity O
between O
states O
. O
Our O
approach O
departs O
from O
theirs O
in O
that O
we O
do O
not O
construct O
our O
density Method
model Method
over O
the O
raw O
state O
space O
. O
Instead O
, O
we O
exploit O
the O
feature Method
map Method
that O
is O
used O
for O
value Method
function Method
approximation Method
, O
and O
construct O
a O
density Method
model Method
over O
the O
transformed O
feature O
space O
. O
This O
model O
assigns O
higher O
probability O
to O
state O
feature O
vectors O
that O
share O
features O
with O
visited O
states O
. O
Generalised O
visit O
- O
counts O
are O
then O
computed O
from O
these O
probabilities O
; O
states O
with O
frequently O
observed O
features O
are O
assigned O
higher O
counts O
. O
These O
counts O
serve O
as O
a O
measure O
of O
the O
uncertainty O
associated O
with O
a O
state O
. O
Exploration O
bonuses O
are O
then O
computed O
from O
these O
counts O
in O
order O
to O
encourage O
the O
agent O
to O
visit O
regions O
of O
the O
state O
- O
space O
with O
less O
familiar O
features O
. O
Our O
density Method
model Method
can O
be O
trivially O
derived O
from O
any O
feature Method
map Method
used O
for O
LFA Task
, O
regardless O
of O
the O
application O
domain O
, O
and O
requires O
little O
or O
no O
additional O
design O
. O
In O
contrast O
to O
existing O
algorithms O
, O
there O
is O
no O
need O
to O
perform O
a O
special O
dimensionality Method
reduction Method
of Method
the Method
state Method
space Method
in O
order O
to O
compute O
our O
generalised O
visit O
- O
counts O
. O
Our O
method O
uses O
the O
same O
lower Method
- Method
dimensional Method
feature Method
representation Method
to O
estimate O
value O
and O
to O
estimate O
uncertainty O
. O
This O
makes O
it O
simpler O
to O
implement O
and O
less O
computationally O
expensive O
than O
some O
existing O
proposals O
. O
Our O
evaluation O
demonstrates O
that O
this O
simple O
approach O
achieves O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
high Task
- Task
dimensional Task
RL Task
benchmarks Task
. O
section O
: O
Background O
and O
Related O
Work O
subsection O
: O
Reinforcement Method
Learning Method
The O
reinforcement Method
learning Method
( O
RL Task
) Task
problem Task
formalises O
the O
task O
of O
learning O
from O
interaction Task
to O
achieve O
a O
goal O
. O
It O
is O
usually O
formulated O
as O
an O
MDP Method
, O
where O
is O
the O
set O
of O
states O
of O
the O
environment O
, O
is O
the O
set O
of O
available O
actions O
, O
is O
the O
state O
transition O
distribution O
, O
is O
the O
reward O
function O
, O
and O
is O
the O
discount O
factor O
. O
The O
agent O
is O
formally O
a O
policy Method
that O
maps O
a O
state O
to O
an O
action O
. O
At O
timestep O
, O
the O
agent O
is O
in O
a O
state O
, O
receives O
a O
reward O
, O
and O
takes O
an O
action O
. O
We O
seek O
a O
policy O
that O
maximises O
the O
expected O
sum O
of O
future O
rewards O
, O
or O
value O
. O
The O
action O
- O
value O
of O
a O
state O
- O
action O
pair O
under O
a O
policy Method
π Method
is O
the O
expected O
discounted O
sum O
of O
future O
rewards O
, O
given O
that O
the O
agent O
takes O
action O
from O
state O
, O
and O
follows O
thereafter O
: O
. O
RL Method
methods Method
that O
compute O
a O
value O
function O
are O
called O
value Method
- Method
based Method
methods Method
. O
Tabular Method
methods Method
store O
the O
value O
function O
as O
a O
table O
having O
one O
entry O
for O
each O
state O
(- O
action O
) O
. O
This O
representation O
of O
the O
state O
space O
does O
not O
have O
sufficient O
structure O
to O
permit O
generalisation O
based O
on O
the O
similarity O
between O
states O
. O
Function Method
approximation Method
methods Method
achieve O
generalisation O
by O
approximating O
the O
value Method
function Method
by O
a O
parameterised Method
functional Method
form Method
. O
In O
LFA Method
the O
approximate Method
action Method
- Method
value Method
function Method
is O
a O
linear Method
combination Method
of Method
state Method
- Method
action Method
features Method
, O
where O
is O
an O
- Method
dimensional Method
feature Method
map Method
and O
is O
a O
parameter O
vector O
. O
subsection O
: O
Count Method
- Method
Based Method
Exploration Method
and O
Optimism Task
Since O
the O
true O
transition O
and O
reward O
distributions O
and O
are O
unknown O
to O
the O
agent O
, O
it O
must O
explore O
the O
environment O
to O
gather O
more O
information O
and O
reduce O
its O
uncertainty O
. O
At O
the O
same O
time O
, O
it O
must O
exploit O
its O
current O
information O
to O
maximise O
expected O
cumulative O
reward O
. O
This O
tradeoff O
between O
exploration O
and O
exploitation Task
is O
a O
fundamental O
problem O
in O
RL Task
. O
Many O
of O
the O
exploration Method
algorithms Method
that O
enjoy O
strong O
theoretical O
guarantees O
implement O
the O
‘ O
optimism O
in O
the O
face O
of O
uncertainty O
’ O
( O
OFU Method
) Method
heuristic Method
. O
Most O
are O
tabular O
and O
count O
- O
based O
in O
that O
they O
compute O
exploration O
bonuses O
from O
a O
table O
of O
state O
(- O
action O
) O
visit O
counts O
. O
These O
bonuses O
are O
added O
to O
the O
estimated O
state O
/ O
action O
value O
. O
Lower O
counts O
entail O
higher O
bonuses O
, O
so O
the O
agent O
is O
effectively O
optimistic O
about O
the O
value O
of O
less O
frequently O
visited O
regions O
of O
the O
environment O
. O
OFU Method
algorithms Method
are O
more O
efficient O
than O
random Method
strategies Method
like O
- O
greedy O
because O
the O
agent O
avoids O
actions O
that O
yield O
neither O
large O
rewards O
nor O
large O
reductions O
in O
uncertainty O
. O
One O
of O
the O
best O
known O
is O
the O
UCB1 Method
bandit Method
algorithm Method
, O
which O
selects O
an O
action O
that O
maximises O
an O
upper O
confidence O
bound O
+ O
⁢^Qt O
( O
a O
) O
⁢2logt⁢N O
( O
a O
) O
, O
where O
is O
the O
estimated O
mean O
reward O
and O
is O
the O
visit O
- O
count O
. O
The O
dependence O
of O
the O
bonus O
term O
on O
the O
inverse O
square O
- O
root O
of O
the O
visit Metric
- Metric
count Metric
is O
justified O
using O
Chernoff Method
bounds Method
. O
In O
the O
MDP Task
setting Task
, O
the O
tabular Method
OFU Method
algorithm Method
most O
closely O
resembling O
our O
method O
is O
Model Method
- Method
Based Method
Interval Method
Estimation Method
with O
Exploration Method
Bonuses Method
( O
MBIE Method
- Method
EB Method
) O
. O
Empirical O
estimates O
and O
of O
the O
transition O
and O
reward O
functions O
are O
maintained O
, O
and O
is O
augmented O
with O
a O
bonus O
term O
, O
where O
is O
the O
state O
- O
action O
visit O
- O
count O
, O
and O
is O
a O
theoretically O
derived O
constant O
. O
The O
Bellman O
optimality O
equation O
for O
the O
augmented Method
action Method
- Method
value Method
function Method
is O
. O
Here O
the O
dependence O
of O
the O
bonus O
on O
the O
inverse O
square O
- O
root O
of O
the O
visit Metric
- Metric
count Metric
is O
provably O
optimal O
. O
This O
equation O
can O
be O
solved O
using O
any O
MDP Method
solution Method
method Method
. O
subsection O
: O
Exploration Task
in O
Large Task
MDPs Task
While O
tabular Method
OFU Method
algorithms Method
perform O
well O
in O
practice O
on O
small O
MDPs Method
, O
their O
sample Metric
complexity Metric
becomes O
prohibitive O
for O
larger O
problems O
. O
MBIE Method
- Method
EB Method
, O
for O
example O
, O
has O
a O
sample Metric
complexity Metric
bound Metric
of O
. O
In O
the O
high Task
- Task
dimensional Task
setting Task
– O
where O
the O
agent O
can O
not O
hope O
to O
visit O
every O
state O
during O
training O
– O
this O
bound O
offers O
no O
guarantee O
that O
the O
trained O
agent O
will O
perform O
well O
. O
Several O
very O
recent O
extensions O
of O
count Method
- Method
based Method
exploration Method
methods Method
have O
produced O
impressive O
results O
on O
high Material
- Material
dimensional Material
RL Material
benchmarks Material
. O
These O
algorithms O
closely O
resemble O
MBIE Method
- Method
EB Method
, O
but O
they O
substitute O
the O
state O
- O
action O
visit O
- O
count O
for O
a O
generalised O
count O
which O
quantifies O
the O
similarity O
of O
a O
state O
to O
previously O
visited O
states O
. O
Bellemare O
et O
. O
al O
. O
construct O
a O
Context Method
Tree Method
Switching Method
( Method
CTS Method
) Method
density Method
model Method
over O
the O
state O
space O
such O
that O
higher O
probability O
is O
assigned O
to O
states O
that O
are O
more O
similar O
to O
visited O
states O
. O
A O
state O
pseudocount O
is O
then O
derived O
from O
this O
density O
. O
A O
subsequent O
extension O
of O
this O
work O
replaces O
the O
CTS Method
density Method
model Method
with O
a O
neural Method
network Method
. O
Another O
recent O
proposal O
uses O
locality Method
sensitive Method
hashing Method
( O
LSH Method
) O
to O
cluster O
similar O
states O
, O
and O
the O
number O
of O
visited O
states O
in O
a O
cluster O
serves O
as O
a O
generalised O
visit O
- O
count O
. O
As O
in O
the O
MBIE Method
- Method
EB Method
algorithm Method
, O
these O
counts O
are O
used O
to O
compute O
exploration O
bonuses O
. O
These O
three O
algorithms O
outperform O
random Method
strategies Method
, O
and O
are O
currently O
the O
leading O
exploration Method
methods Method
in O
large Task
discrete Task
domains Task
where O
exploration Task
is O
hard O
. O
section O
: O
Method O
Here O
we O
introduce O
the O
ϕ Method
- Method
Exploration Method
Bonus Method
( Method
- Method
EB Method
) Method
algorithm Method
, O
which O
drives O
the O
agent O
to O
visit O
states O
about O
which O
it O
is O
uncertain O
. O
Following O
other O
optimistic Method
count Method
- Method
based Method
exploration Method
algorithms Method
, O
we O
use O
a O
( O
generalised O
) O
state O
visit O
- O
count O
in O
order O
to O
estimate O
the O
uncertainty O
associated O
with O
a O
state O
. O
A O
generalised O
count O
is O
a O
novelty Metric
measure Metric
that O
quantifies O
how O
dissimilar O
a O
state O
is O
from O
those O
already O
visited O
. O
Measuring Task
novelty Task
therefore O
involves O
choosing O
a O
similarity Metric
measure Metric
for O
states O
. O
Of O
course O
, O
states O
can O
be O
similar O
in O
myriad O
ways O
, O
but O
not O
all O
of O
these O
are O
relevant O
to O
solving O
the O
MDP Method
. O
If O
the O
solution Method
method Method
used O
is O
value O
- O
based O
, O
then O
states O
should O
only O
be O
considered O
similar O
if O
they O
share O
the O
features O
that O
are O
determinative O
of O
value O
. O
This O
motivates O
us O
to O
construct O
a O
similarity Method
measure Method
that O
exploits O
the O
feature Method
representation Method
that O
is O
used O
for O
value Method
function Method
approximation Method
. O
These O
features O
are O
explicitly O
designed O
to O
be O
relevant O
for O
estimating Task
value Task
. O
If O
they O
were O
not O
, O
they O
would O
not O
permit O
a O
good O
approximation O
to O
the O
true O
value O
function O
. O
This O
sets O
our O
method O
apart O
from O
the O
approaches O
described O
in O
section O
[ O
reference O
] O
. O
They O
measure O
novelty O
with O
respect O
to O
a O
separate O
, O
exploration Method
- Method
specific Method
representation Method
of Method
the Method
state Method
space Method
, O
one O
that O
bears O
no O
relation O
to O
the O
value O
function O
or O
the O
reward O
structure O
of O
the O
MDP Method
. O
We O
argue O
that O
measuring O
novelty O
in O
feature O
space O
is O
a O
simpler O
and O
more O
principled O
approach O
, O
and O
hypothesise O
that O
more O
efficient O
exploration Task
will O
result O
. O
subsection O
: O
A O
Visit Method
- Method
Density Method
over O
Feature Method
Space Method
Our O
exploration Method
method Method
is O
designed O
for O
use O
with O
LFA Method
, O
and O
measures O
novelty Metric
with O
respect O
to O
a O
fixed O
feature Method
representation Method
of Method
the Method
state Method
space Method
. O
The O
challenge O
is O
to O
measure O
novelty O
without O
computing O
the O
distance O
between O
each O
new O
feature O
vector O
and O
those O
in O
the O
history O
. O
That O
approach O
becomes O
infeasible O
because O
the O
cost O
of O
computing O
these O
distances O
grows O
with O
the O
size O
of O
the O
history O
. O
Our O
method O
constructs O
a O
density Method
model Method
over O
feature O
space O
that O
assigns O
higher O
probability O
to O
states O
that O
share O
more O
features O
with O
more O
frequently O
observed O
states O
. O
Let O
be O
the O
feature O
mapping O
from O
the O
state O
space O
into O
an O
- O
dimensional O
feature O
space O
. O
Let O
denote O
the O
state O
feature O
vector O
observed O
at O
time O
. O
We O
denote O
the O
sequence O
of O
observed O
feature O
vectors O
after O
timesteps O
by O
, O
and O
denote O
the O
set O
of O
all O
finite O
sequences O
of O
feature O
vectors O
by O
. O
Let O
denote O
the O
sequence O
where O
is O
followed O
by O
. O
The O
- O
th O
element O
of O
is O
denoted O
by O
, O
and O
the O
- O
th O
element O
of O
is O
. O
theorem O
: O
( O
Feature O
Visit O
- O
Density O
) O
. O
Let O
be O
a O
density Method
model Method
that O
maps O
a O
finite O
sequence O
of O
feature O
vectors O
to O
a O
probability O
distribution O
over O
. O
The O
feature O
visit O
- O
density O
at O
time O
is O
the O
distribution O
over O
that O
is O
returned O
by O
after O
observing O
. O
We O
construct O
our O
feature Method
visit Method
- Method
density Method
as O
a O
product O
of O
independent Method
factor Method
distributions Method
over O
individual O
features O
: O
If O
is O
countable O
we O
can O
use O
a O
count Method
- Method
based Method
estimator Method
for O
the O
factor Method
models Method
, O
such O
as O
the O
empirical Method
estimator Method
, O
where O
is O
the O
number O
of O
times O
has O
occurred O
. O
In O
our O
implementation O
we O
use O
the O
Krichevsky Method
- Method
Trofimov Method
( Method
KT Method
) Method
estimator Method
. O
This O
density Method
model Method
induces O
a O
similarity Metric
measure Metric
on O
the O
feature O
space O
. O
Loosely O
speaking O
, O
feature O
vectors O
that O
share O
component O
features O
are O
deemed O
similar O
. O
This O
enables O
us O
to O
use O
as O
a O
novelty Metric
measure Metric
for O
states O
, O
by O
comparing O
the O
features O
of O
newly O
observed O
states O
to O
those O
in O
the O
history O
. O
If O
has O
more O
novel O
component O
features O
, O
will O
be O
lower O
. O
By O
modelling O
the O
features O
as O
independent O
, O
and O
using O
count Method
- Method
based Method
estimators Method
as O
factor Method
models Method
, O
our O
method O
learns O
reasonable O
novelty Method
estimates Method
from O
very O
little O
data O
. O
theorem O
: O
. O
Suppose O
we O
use O
a O
3 Method
- Method
D Method
binary Method
feature Method
map Method
and O
that O
after O
3 O
timesteps O
the O
history O
of O
observed O
feature O
vectors O
is O
. O
Let O
us O
estimate O
the O
feature O
visit O
densities O
of O
two O
unobserved O
feature O
vectors O
, O
and O
. O
Using O
the O
KT Method
estimator Method
for O
the O
factor Method
models Method
, O
we O
have O
, O
and O
. O
Note O
that O
because O
the O
component O
features O
of O
are O
more O
similar O
to O
those O
in O
the O
history O
. O
As O
desired O
, O
our O
novelty Metric
measure Metric
generalises O
across O
the O
state O
space O
. O
subsection O
: O
The O
- O
pseudocount O
Here O
we O
adopt O
a O
recently O
proposed O
method O
for O
computing O
generalised Task
visit Task
- Task
counts Task
from O
density Method
models Method
. O
By O
analogy O
with O
these O
pseudocounts O
, O
we O
derive O
two O
ϕ O
- O
pseudocounts O
from O
our O
feature Method
visit Method
- Method
density Method
. O
theorem O
: O
( O
- O
pseudocount O
and O
Naive Method
- Method
pseudocount Method
) O
. O
Let O
be O
the O
feature O
visit O
- O
density O
after O
observing O
. O
Let O
denote O
the O
same O
density Method
model Method
after O
has O
been O
observed O
. O
The O
naive O
ϕ O
- O
pseudocount O
⁢~Ntϕ O
( O
s O
) O
for O
a O
state O
at O
time O
is O
The O
ϕ O
- O
pseudocount O
for O
a O
state O
at O
time O
is O
Empirically O
, O
is O
usually O
larger O
than O
⁢~Ntϕ O
( O
s O
) O
and O
leads O
to O
better O
performance O
. O
subsection O
: O
Reinforcement Method
Learning Method
with O
- O
EB O
, O
Observe O
, O
Compute O
i O
in O
{ O
1 O
, O
… O
, O
M O
} O
Update O
with O
observed O
Compute O
Compute O
Compute O
Set O
Pass O
, O
to O
RL Method
algorithm Method
to O
update O
Following O
traditional O
count Method
- Method
based Method
exploration Method
algorithms Method
, O
we O
drive O
optimistic Task
exploration Task
by O
computing O
a O
bonus O
from O
the O
- O
pseudocount O
. O
theorem O
: O
( O
- O
Exploration O
Bonus O
) O
. O
Let O
be O
a O
free O
parameter O
. O
The O
ϕ O
- O
exploration O
bonus O
for O
a O
state O
- O
action O
pair O
at O
time O
is O
As O
in O
the O
MBIE Method
- Method
EB Method
algorithm Method
, O
this O
bonus O
is O
added O
to O
the O
reward O
. O
The O
agent O
is O
trained O
on O
the O
augmented O
reward O
using O
any O
value Method
- Method
based Method
RL Method
algorithm Method
with O
LFA Method
. O
At O
each O
timestep O
our O
algorithm O
performs O
updates O
for O
at O
most O
estimators O
, O
one O
for O
each O
feature O
. O
The O
cost O
of O
our O
method O
is O
therefore O
independent O
of O
the O
size O
of O
the O
state O
- O
action O
space O
, O
and O
scales O
only O
in O
the O
number O
of O
features O
. O
If O
the O
feature O
vectors O
are O
sparse O
, O
we O
can O
maintain O
a O
single O
prototype Method
estimator Method
for O
all O
the O
features O
that O
have O
not O
yet O
been O
observed O
. O
Under O
these O
conditions O
our O
method O
scales O
only O
in O
the O
number O
of O
observed O
features O
. O
section O
: O
Theoretical O
Results O
Here O
we O
formalise O
the O
comments O
made O
in O
section O
[ O
reference O
] O
by O
proving O
a O
bound O
that O
relates O
our O
pseudocount Method
to O
an O
appropriate O
similarity Metric
measure Metric
. O
To O
simplify O
the O
analysis O
, O
we O
prove O
results O
for O
the O
naive Task
- Task
exploration Task
bonus Task
, O
though O
we O
expect O
analogous O
results O
to O
hold O
for O
as O
well O
. O
We O
use O
the O
empirical Method
estimator Method
for O
the O
factor Method
models Method
in O
the O
visit Method
- Method
density Method
. O
Since O
the O
feature O
set O
we O
use O
in O
our O
implementation O
is O
binary O
, O
our O
analysis O
assumes O
. O
We O
begin O
by O
defining O
a O
similarity Metric
measure Metric
for O
binary O
feature O
vectors O
, O
and O
prove O
two O
lemmas O
. O
theorem O
: O
( O
Hamming O
Similarity O
for O
Binary O
Vectors O
) O
. O
Let O
be O
- O
length O
binary O
vectors O
. O
The O
Hamming O
similarity O
between O
and O
is O
. O
Note O
that O
for O
all O
. O
The O
Hamming O
similarity O
is O
large O
if O
and O
share O
features O
( O
i.e. O
if O
the O
- O
distance O
between O
them O
is O
small O
) O
. O
We O
now O
prove O
a O
lemma O
relating O
the O
joint O
probability O
of O
a O
feature O
vector O
to O
the O
sum O
of O
the O
probabilities O
of O
its O
factors O
. O
theorem O
: O
( O
AM Method
- Method
GM Method
Inequality Method
and O
Factorised Method
) O
. O
Let O
∈ϕ{0 O
, O
1}M O
, O
and O
let O
= O
⁢ρt O
( O
ϕ O
) O
∏=i1M⁢ρti O
( O
ϕi O
) O
. O
Then O
≤⁢ρ O
( O
ϕ O
) O
⁢1M∑=i1M⁢ρti O
( O
ϕi O
) O
. O
proof O
: O
Proof O
. O
By O
the O
inequality O
of O
arithmetic O
and O
geometric O
means O
∎ O
The O
following O
lemma O
relates O
the O
probability O
of O
an O
individual O
feature O
to O
its O
- O
distance O
from O
previously O
observed O
values O
. O
theorem O
: O
( O
Feature O
Visit O
- O
Density O
and O
- O
distance O
) O
. O
Let O
= O
⁢ρti O
( O
ϕi O
) O
⁢1tNt O
( O
ϕi O
) O
. O
Then O
for O
all O
∈ϕi O
, O
ϕk O
, O
i{0 O
, O
1 O
} O
, O
= O
⁢ρti O
( O
ϕi O
)- O
⁢1t∑=k1t1| O
- O
ϕiϕk O
, O
i| O
. O
proof O
: O
Proof O
. O
Suppose O
: O
The O
case O
follows O
by O
an O
almost O
identical O
argument O
. O
∎ O
The O
following O
theorem O
and O
its O
corollary O
are O
the O
major O
results O
of O
this O
section O
. O
These O
connect O
the O
Hamming O
similarity O
( O
to O
previously O
observed O
feature O
vectors O
) O
with O
both O
the O
feature O
visit O
- O
density O
and O
the O
- O
pseudocount O
. O
We O
show O
that O
a O
state O
which O
shares O
few O
features O
with O
those O
in O
the O
history O
will O
be O
assigned O
low O
probability O
by O
our O
density Method
model Method
, O
and O
will O
therefore O
have O
a O
low O
- O
pseudocount O
. O
theorem O
: O
( O
Feature O
Visit O
- O
Density O
and O
Average Metric
Similarity Metric
) O
. O
Let O
∈sS O
be O
a O
state O
with O
binary O
feature O
representation O
ϕ=⁢ϕ O
( O
s O
) O
∈{0 O
, O
1}M O
, O
and O
let O
= O
⁢ρt O
( O
ϕ O
) O
∏=i1M⁢ρti O
( O
ϕi O
) O
be O
its O
feature O
visit O
- O
density O
at O
time O
t. O
Then O
proof O
: O
Proof O
. O
where O
( O
a O
) O
follows O
from O
Lemma O
[ O
reference O
] O
, O
( O
b O
) O
from O
Lemma O
[ O
reference O
] O
, O
and O
( O
c O
) O
from O
Definition O
[ O
reference O
] O
. O
∎ O
We O
immediately O
get O
a O
similar O
bound O
for O
the O
naive O
- O
pseudocount O
. O
theorem O
: O
( O
- O
pseudocount O
and O
Total Metric
Similarity Metric
) O
. O
≤⁢~Ntϕ O
( O
s O
) O
∑=k1t⁢Sim O
( O
ϕ O
, O
ϕk O
) O
⁢~Ntϕ O
( O
s O
) O
≤∑=k1t⁢Sim O
( O
ϕ O
, O
ϕk O
) O
proof O
: O
Proof O
. O
Immediate O
from O
Theorem O
[ O
reference O
] O
and O
Definition O
[ O
reference O
] O
. O
∎ O
therefore O
captures O
an O
intuitive O
relation O
between O
novelty O
and O
similarity O
to O
visited O
states O
. O
By O
visiting O
a O
state O
that O
minimises O
the O
- O
pseudocount O
, O
an O
agent O
also O
minimises O
a O
lower O
bound O
on O
its O
Hamming O
similarity O
to O
previously O
visited O
states O
. O
As O
desired O
, O
we O
have O
a O
novelty Method
measure Method
that O
is O
closely O
related O
to O
the O
distances O
between O
states O
in O
feature O
space O
, O
but O
which O
obviates O
the O
cost O
of O
computing O
those O
distances O
directly O
. O
section O
: O
Empirical O
Evaluation O
Our O
evaluation O
is O
designed O
to O
answer O
the O
following O
research O
questions O
: O
Is O
a O
novelty Metric
measure Metric
derived O
from O
the O
features O
used O
for O
LFA Method
a O
good O
way O
to O
generalise O
state O
visit O
- O
counts O
? O
Does O
- O
EB O
produce O
improvement O
across O
a O
range O
of O
environments O
, O
or O
only O
if O
rewards O
are O
sparse O
? O
Can O
- O
EB O
with O
LFA Method
compete O
with O
the O
state O
- O
of O
- O
the O
- O
art O
in O
exploration Task
and O
deep Task
RL Task
? O
subsection O
: O
Setup O
We O
evaluate O
our O
algorithm O
on O
five O
games O
from O
the O
Arcade Material
Learning Material
Environment Material
( Material
ALE Material
) Material
, O
which O
has O
recently O
become O
a O
standard O
high O
- O
dimensional O
benchmark O
for O
RL Task
. O
The O
reward O
signal O
is O
computed O
from O
the O
game O
score O
. O
The O
raw O
state O
is O
a O
frame O
of O
video O
( O
a O
160 O
210 O
array O
of O
7 O
- O
bit O
pixels O
) O
. O
There O
are O
18 O
available O
actions O
. O
The O
ALE Method
is O
a O
particularly O
interesting O
testbed O
in O
our O
context O
, O
because O
the O
difficulty O
of O
exploration Task
varies O
greatly O
between O
games O
. O
Random Method
strategies Method
often O
work O
well O
, O
and O
it O
is O
in O
these O
games O
that O
Deep Method
Q Method
- Method
Networks Method
( O
DQN Method
) Method
with O
- Method
greedy Method
is O
able O
to O
achieve O
so O
- O
called O
human O
- O
level O
performance O
. O
In O
others O
, O
however O
, O
DQN Method
with O
- O
greedy O
does O
not O
improve O
upon O
a O
random Method
policy Method
, O
and O
its O
inability O
to O
explore O
efficiently O
is O
one O
of O
the O
key O
determinants O
of O
this O
failure O
. O
We O
chose O
five O
of O
these O
games O
where O
exploration Task
is O
hard O
. O
Three O
of O
the O
chosen O
games O
have O
sparse O
rewards O
( O
Montezuma O
’s O
Revenge O
, O
Venture O
, O
Freeway O
) O
and O
two O
have O
dense O
rewards O
( O
Frostbite O
, O
Q*bert O
) O
. O
Evaluating Task
agents Task
in O
the O
ALE Method
is O
computationally O
demanding O
. O
We O
chose O
to O
focus O
more O
resources O
on O
Montezuma O
’s O
Revenge O
and O
Venture O
, O
for O
two O
reasons O
: O
( O
1 O
) O
we O
hypothesise O
that O
- O
EB O
will O
produce O
more O
improvement O
in O
sparse Task
reward Task
games Task
, O
and O
( O
2 O
) O
leading O
algorithms O
with O
which O
we O
seek O
to O
compare O
- O
EB O
have O
also O
focused O
on O
these O
games O
. O
We O
conducted O
five O
independent O
learning O
trials O
for O
Montezuma O
and O
Venture Material
, O
and O
two O
trials O
for O
the O
remaining O
three O
games O
. O
All O
agents O
were O
trained O
for O
100 O
million O
frames O
on O
the O
no Metric
- Metric
op Metric
metric Metric
. O
Trained O
agents O
were O
then O
evaluated O
for O
500 O
episodes O
; O
Table O
[ O
reference O
] O
reports O
the O
average O
evaluation Metric
score Metric
. O
We O
implement O
Algorithm O
[ O
reference O
] O
using O
Sarsa Method
( Method
) Method
with O
replacing Method
traces Method
and O
LFA Method
as O
our O
RL Method
method Method
, O
because O
it O
is O
less O
likely O
to O
diverge O
than O
- O
learning O
. O
To O
implement O
LFA Method
in O
the O
ALE Method
we O
use O
the O
Blob O
- O
PROST O
feature O
set O
presented O
in O
. O
To O
date O
this O
is O
the O
best O
performing O
feature O
set O
for O
LFA Task
in O
the O
ALE Task
. O
The O
parameters O
for O
the O
Sarsa Method
( Method
algorithm Method
are O
set O
to O
the O
same O
values O
as O
in O
. O
Hereafter O
we O
refer O
to O
our O
algorithm O
as O
Sarsa Method
- Method
- Method
EB Method
. O
To O
conduct O
a O
controlled O
investigation O
of O
the O
effectiveness O
of O
- O
EB O
, O
we O
also O
evaluate O
a O
baseline O
implementation O
of O
Sarsa Method
( Method
) Method
with O
the O
same O
features O
but O
with O
- Method
greedy Method
exploration Method
( O
which O
we O
denote O
Sarsa Method
- Method
) O
. O
The O
same O
training Metric
and Metric
evaluation Metric
regime Metric
is O
used O
for O
both O
; O
learning O
curves O
are O
reported O
in O
Figure O
[ O
reference O
] O
. O
The O
coefficient O
in O
the O
- O
exploration O
bonus O
was O
set O
to O
0.05 O
for O
all O
games O
, O
after O
a O
coarse O
parameter O
search O
. O
This O
search O
was O
performed O
once O
, O
across O
a O
range O
of O
ALE O
games O
, O
and O
a O
value O
was O
chosen O
for O
which O
the O
agent O
achieved O
good O
scores O
in O
most O
games O
. O
subsection O
: O
Results O
subsubsection Method
: O
Comparison O
with O
- O
greedy O
Baseline O
In O
Montezuma O
’s O
Revenge O
, O
Sarsa O
- O
rarely O
leaves O
the O
first O
room O
. O
Its O
policy O
converges O
after O
an O
average O
of O
20 O
million O
frames O
. O
Sarsa O
- O
- O
EB O
continues O
to O
improve O
throughout O
training O
, O
visiting O
up O
to O
14 O
rooms O
. O
The O
largest O
improvement O
over O
the O
baseline O
occurs O
in O
Venture O
. O
Sarsa Method
- Method
fails O
to O
score O
, O
while O
Sarsa Method
- Method
- O
EB O
continues O
to O
improve O
throughout O
training O
. O
In O
Q*bert O
and O
Frostbite O
, O
the O
difference O
is O
less O
dramatic O
. O
These O
games O
have O
dense O
, O
well O
- O
shaped O
rewards O
that O
guide O
the O
agent O
’s O
path O
through O
state O
space O
and O
elide O
- O
greedy O
’s O
inefficiency O
. O
Nonetheless O
, O
Sarsa Method
- Method
- Method
EB Method
consistently O
outperforms O
Sarsa Method
- Method
throughout O
training O
so O
its O
cumulative Metric
reward Metric
is O
much O
higher O
. O
In O
Freeway Material
, O
Sarsa Method
- Method
- Method
EB Method
with O
fails O
to O
match O
the O
performance O
of O
the O
baseline O
algorithm O
, O
but O
with O
it O
performs O
better O
( O
Figure O
[ O
reference O
] O
shows O
the O
learning O
curve O
for O
the O
latter O
) O
. O
This O
sensitivity O
to O
the O
parameter O
likely O
results O
from O
the O
large O
number O
of O
unique O
Blob O
- O
PROST O
features O
that O
are O
active O
in O
Freeway O
, O
many O
of O
which O
are O
not O
relevant O
for O
finding O
the O
optimal Method
policy Method
. O
If O
is O
too O
high O
the O
agent O
is O
content O
to O
stand O
still O
and O
receive O
exploration O
bonuses O
for O
observing O
new O
configurations O
of O
traffic O
. O
This O
accords O
with O
our O
hypothesis O
that O
efficient O
optimistic Task
exploration Task
should O
involve O
measuring O
novelty O
with O
respect O
to O
task O
- O
relevant O
features O
. O
In O
summary O
, O
Sarsa Method
- Method
- Method
EB Method
with O
outperforms O
Sarsa Method
- Method
on O
all O
tested O
games O
except O
Freeway Material
. O
Since O
both O
use O
the O
same O
feature O
set O
and O
RL Method
algorithm Method
, O
and O
differ O
only O
in O
their O
exploration Method
policies Method
, O
this O
is O
strong O
evidence O
that O
- O
EB O
produces O
improvement O
over O
random Method
exploration Method
across O
a O
range O
of O
environments O
. O
This O
also O
supports O
our O
conjecture O
that O
using O
the O
same O
features O
for O
value Method
function Method
approximation Method
and O
novelty Method
estimation Method
is O
an O
appropriate O
way O
to O
generalise O
visit Task
- Task
counts Task
to O
the O
high Task
- Task
dimensional Task
setting Task
. O
subsubsection Method
: O
Comparison O
with O
Leading Method
Algorithms Method
Table O
[ O
reference O
] O
compares O
our O
evaluation Metric
scores Metric
to O
Double Method
DQN Method
( Method
DDQN Method
) O
, O
Double Method
DQN Method
with Method
pseudocount Method
( Method
DDQN Method
- Method
PC Method
) O
, O
A3C O
+ O
, O
DQN Method
Pop Method
- Method
Art Method
( O
DQN Method
- Method
PA Method
) O
, O
Dueling Method
Network Method
( O
Dueling Method
) O
, O
Gorila Method
, O
DQN Method
with O
Model Method
Prediction Method
Exploration Method
Bonuses Method
( O
MP Method
- Method
EB Method
) O
, O
Trust Method
Region Method
Policy Method
Optimisation Method
( O
TRPO Method
) O
, O
and O
TRPO Method
- Method
AE Method
- Method
SimHash Method
( Method
TRPO Method
- Method
Hash Method
) O
. O
The O
most O
interesting O
comparisons O
for O
our O
purposes O
are O
with O
TRPO Method
- Method
Hash Method
, O
DDQN Method
- Method
PC Method
, O
A3C Method
+ Method
, O
and O
MP Method
- Method
EB Method
, O
because O
these O
algorithms O
all O
use O
exploration Method
strategies Method
that O
drive O
the O
agent O
to O
reduce O
its O
uncertainty O
. O
TRPO Method
- Method
Hash Method
, O
DDQN Method
- Method
PC Method
, O
and O
A3C O
+ O
are O
count Method
- Method
based Method
methods Method
, O
MP Method
- Method
EB Method
seeks O
high O
model Metric
prediction Metric
error Metric
. O
Our O
Sarsa Method
- Method
- Method
EB Method
algorithm Method
achieves O
an O
average Metric
score Metric
of O
2745.4 O
on O
Montezuma Method
: O
the O
second O
highest O
reported O
score O
. O
On O
this O
game O
it O
far O
outperforms O
every O
algorithm O
apart O
from O
DDQN Method
- Method
PC Method
, O
despite O
only O
having O
trained O
for O
half O
the O
number O
of O
frames O
. O
Note O
that O
neither O
A3C Method
+ Method
nor O
TRPO Method
- Method
Hash Method
achieves O
more O
than O
200 O
points O
, O
despite O
their O
exploration Method
strategies Method
. O
On O
Venture O
Sarsa O
- O
- O
EB O
also O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
It O
achieves O
the O
third O
highest O
reported O
score O
despite O
its O
short O
training O
regime O
, O
and O
far O
outperforms O
A3C Method
+ Method
and O
TRPO Method
- Method
Hash Method
. O
DDQN Metric
- Metric
PC Metric
evaluation Metric
scores Metric
are O
not O
given O
for O
Venture O
, O
but O
reported O
learning Metric
curves Metric
suggest O
Sarsa Method
- Method
- O
EB O
performs O
much O
better O
here O
. O
The O
performance O
of O
Sarsa Method
- Method
- Method
EB Method
in O
Frostbite Method
also O
seems O
competitive O
given O
the O
shorter O
training O
regime O
. O
Nonlinear Method
algorithms Method
perform O
better O
in O
Q*bert O
. O
In O
Freeway O
Sarsa O
- O
- O
EB O
fails O
to O
score O
any O
points O
, O
for O
reasons O
already O
discussed O
. O
section O
: O
Conclusion O
We O
have O
introduced O
the O
- Method
Exploration Method
Bonus Method
method Method
, O
a O
count Method
- Method
based Method
optimistic Method
exploration Method
strategy Method
that O
scales O
to O
high O
- O
dimensional O
environments O
. O
It O
is O
simpler O
to O
implement O
and O
less O
computationally O
demanding O
than O
some O
other O
proposals O
. O
Our O
evaluation O
shows O
that O
it O
improves O
upon O
- Method
greedy Method
exploration Method
on O
a O
variety O
of O
games O
, O
and O
that O
it O
is O
even O
competitive O
with O
leading O
exploration Method
techniques Method
developed O
for O
deep Method
RL Method
. O
Unlike O
other O
methods O
, O
it O
does O
not O
require O
the O
design O
of O
an O
exploration Method
- Method
specific Method
state Method
representation Method
, O
but O
rather O
exploits O
the O
features O
used O
in O
the O
approximate O
value O
function O
. O
We O
have O
argued O
that O
computing O
novelty O
with O
respect O
to O
these O
task O
- O
relevant O
features O
is O
an O
efficient O
and O
principled O
way O
to O
generalise O
visit O
- O
counts O
for O
exploration Task
. O
We O
conclude O
by O
noting O
that O
this O
reliance O
on O
the O
feature Method
representation Method
used O
for O
LFA Method
is O
also O
a O
limitation O
. O
It O
is O
not O
obvious O
how O
a O
method O
like O
ours O
could O
be O
combined O
with O
the O
nonlinear Method
function Method
approximation Method
techniques Method
that O
have O
driven O
recent O
progress O
in O
RL Task
. O
We O
hope O
the O
success O
of O
our O
simple O
method O
will O
inspire O
future O
work O
in O
this O
direction O
. O
bibliography O
: O
References O
