document	O
:	O
Attention	O
Is	O
All	O
You	O
Need	O
The	O
dominant	Method
sequence	Method
transduction	Method
models	Method
are	O
based	O
on	O
complex	O
recurrent	Method
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	Method
and	O
a	O
decoder	Method
.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	Method
and	O
decoder	Method
through	O
an	O
attention	Method
mechanism	Method
.	O
We	O
propose	O
a	O
new	O
simple	O
network	Method
architecture	Method
,	O
the	O
Transformer	Method
,	O
based	O
solely	O
on	O
attention	Method
mechanisms	Method
,	O
dispensing	O
with	O
recurrence	Method
and	Method
convolutions	Method
entirely	O
.	O
Experiments	O
on	O
two	O
machine	Task
translation	Task
tasks	Task
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train	O
.	O
Our	O
model	O
achieves	O
28.4	O
BLEU	Metric
on	O
the	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
German	Task
translation	Task
task	Task
,	O
improving	O
over	O
the	O
existing	O
best	O
results	O
,	O
including	O
ensembles	Method
,	O
by	O
over	O
2	O
BLEU	Metric
.	O
On	O
the	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
task	Task
,	O
our	O
model	O
establishes	O
a	O
new	O
single	O
-	O
model	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
BLEU	Metric
score	Metric
of	O
41.8	O
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs	Method
,	O
a	O
small	O
fraction	O
of	O
the	O
training	Metric
costs	Metric
of	O
the	O
best	O
models	O
from	O
the	O
literature	O
.	O
We	O
show	O
that	O
the	O
Transformer	Method
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	Task
constituency	Task
parsing	Task
both	O
with	O
large	O
and	O
limited	O
training	O
data	O
.	O
section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
,	O
long	Method
short	Method
-	Method
term	Method
memory	Method
hochreiter1997	O
and	O
gated	O
recurrent	Method
gruEval14	O
neural	O
networks	O
in	O
particular	O
,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	Task
modeling	Task
and	O
transduction	Task
problems	Task
such	O
as	O
language	Task
modeling	Task
and	O
machine	Task
translation	Task
sutskever14	O
,	O
bahdanau2014neural	O
,	O
cho2014learning	O
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	Method
language	O
models	O
and	O
encoder	Method
-	Method
decoder	Method
architectures	Method
wu2016google	O
,	O
luong2015effective	O
,	O
jozefowicz2016exploring	O
.	O
Recurrent	Method
models	Method
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences	O
.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time	O
,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
and	O
the	O
input	O
for	O
position	O
.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples	O
,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths	O
,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples	O
.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	Metric
efficiency	Metric
through	O
factorization	Method
tricks	Method
Kuchaiev2017Factorization	O
and	O
conditional	Method
computation	Method
shazeer2017outrageously	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter	O
.	O
The	O
fundamental	O
constraint	O
of	O
sequential	Task
computation	Task
,	O
however	O
,	O
remains	O
.	O
Attention	Method
mechanisms	Method
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	Method
modeling	Method
and	O
transduction	Method
models	Method
in	O
various	O
tasks	O
,	O
allowing	O
modeling	Task
of	Task
dependencies	Task
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
bahdanau2014neural	O
,	O
structuredAttentionNetworks	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
decomposableAttnModel	O
,	O
however	O
,	O
such	O
attention	Method
mechanisms	Method
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	Method
network	O
.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer	Method
,	O
a	O
model	Method
architecture	Method
eschewing	O
recurrence	Method
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	Method
mechanism	Method
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output	O
.	O
The	O
Transformer	Method
allows	O
for	O
significantly	O
more	O
parallelization	Task
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	Metric
quality	Metric
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	Method
GPUs	Method
.	O
section	O
:	O
Background	O
The	O
goal	O
of	O
reducing	O
sequential	Task
computation	Task
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	Method
Neural	Method
GPU	Method
extendedngpu	O
,	O
ByteNet	Method
NalBytenet2017	O
and	O
ConvS2S	Method
JonasFaceNet2017	O
,	O
all	O
of	O
which	O
use	O
convolutional	Method
neural	Method
networks	Method
as	O
basic	O
building	O
block	O
,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions	O
.	O
In	O
these	O
models	O
,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions	O
,	O
linearly	O
for	O
ConvS2S	Method
and	O
logarithmically	O
for	O
ByteNet	Method
.	O
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
hochreiter2001gradient	O
.	O
In	O
the	O
Transformer	Method
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations	O
,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	Metric
resolution	Metric
due	O
to	O
averaging	O
attention	O
-	O
weighted	O
positions	O
,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi	O
-	O
Head	O
Attention	O
as	O
described	O
in	O
section	O
[	O
reference	O
]	O
.	O
Self	Method
-	Method
attention	Method
,	O
sometimes	O
called	O
intra	Method
-	Method
attention	Method
is	O
an	O
attention	Method
mechanism	Method
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence	O
.	O
Self	Method
-	Method
attention	Method
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	Task
comprehension	Task
,	O
abstractive	Task
summarization	Task
,	O
textual	Task
entailment	Task
and	O
learning	Task
task	Task
-	Task
independent	Task
sentence	Task
representations	Task
cheng2016long	O
,	O
decomposableAttnModel	Method
,	O
paulus2017deep	O
,	O
lin2017structured	O
.	O
End	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
networks	Method
are	O
based	O
on	O
a	O
recurrent	Method
attention	O
mechanism	O
instead	O
of	O
sequence	Method
-	Method
aligned	Method
recurrence	Method
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple	Task
-	Task
language	Task
question	Task
answering	Task
and	Task
language	Task
modeling	Task
tasks	Task
sukhbaatar2015	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
however	O
,	O
the	O
Transformer	Method
is	O
the	O
first	O
transduction	Method
model	Method
relying	O
entirely	O
on	O
self	O
-	O
attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequence	Method
-	Method
aligned	Method
RNNs	Method
or	O
convolution	Method
.	O
In	O
the	O
following	O
sections	O
,	O
we	O
will	O
describe	O
the	O
Transformer	Method
,	O
motivate	O
self	Task
-	Task
attention	Task
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
neural_gpu	Method
,	O
NalBytenet2017	Method
and	O
JonasFaceNet2017	Method
.	O
section	O
:	O
Model	O
Architecture	O
Most	O
competitive	O
neural	Method
sequence	Method
transduction	Method
models	Method
have	O
an	O
encoder	Method
-	Method
decoder	Method
structure	Method
cho2014learning	O
,	O
bahdanau2014neural	O
,	O
sutskever14	O
.	O
Here	O
,	O
the	O
encoder	Method
maps	O
an	O
input	O
sequence	O
of	O
symbol	Method
representations	Method
to	O
a	O
sequence	O
of	O
continuous	Method
representations	Method
.	O
Given	O
,	O
the	O
decoder	Method
then	O
generates	O
an	O
output	O
sequence	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time	O
.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto	Method
-	Method
regressive	Method
graves2013generating	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next	O
.	O
The	O
Transformer	Method
follows	O
this	O
overall	O
architecture	O
using	O
stacked	Method
self	Method
-	Method
attention	Method
and	O
point	Method
-	Method
wise	Method
,	Method
fully	Method
connected	Method
layers	Method
for	O
both	O
the	O
encoder	O
and	O
decoder	Method
,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
[	O
reference	O
]	O
,	O
respectively	O
.	O
subsection	O
:	O
Encoder	Method
and	O
Decoder	Method
Stacks	Method
paragraph	O
:	O
Encoder	Method
:	O
The	O
encoder	Method
is	O
composed	O
of	O
a	O
stack	Method
of	Method
identical	Method
layers	Method
.	O
Each	O
layer	O
has	O
two	O
sub	O
-	O
layers	O
.	O
The	O
first	O
is	O
a	O
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
mechanism	Method
,	O
and	O
the	O
second	O
is	O
a	O
simple	O
,	O
position	Method
-	Method
wise	Method
fully	Method
connected	Method
feed	Method
-	Method
forward	Method
network	Method
.	O
We	O
employ	O
a	O
residual	Method
connection	Method
he2016deep	O
around	O
each	O
of	O
the	O
two	O
sub	O
-	O
layers	O
,	O
followed	O
by	O
layer	Method
normalization	Method
.	O
That	O
is	O
,	O
the	O
output	O
of	O
each	O
sub	O
-	O
layer	O
is	O
,	O
where	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub	O
-	O
layer	O
itself	O
.	O
To	O
facilitate	O
these	O
residual	O
connections	O
,	O
all	O
sub	O
-	O
layers	O
in	O
the	O
model	O
,	O
as	O
well	O
as	O
the	O
embedding	O
layers	O
,	O
produce	O
outputs	O
of	O
dimension	O
.	O
paragraph	O
:	O
Decoder	O
:	O
The	O
decoder	Method
is	O
also	O
composed	O
of	O
a	O
stack	Method
of	Method
identical	Method
layers	Method
.	O
In	O
addition	O
to	O
the	O
two	O
sub	O
-	O
layers	O
in	O
each	O
encoder	Method
layer	Method
,	O
the	O
decoder	Method
inserts	O
a	O
third	O
sub	O
-	O
layer	O
,	O
which	O
performs	O
multi	O
-	O
head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack	O
.	O
Similar	O
to	O
the	O
encoder	Method
,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub	O
-	O
layers	O
,	O
followed	O
by	O
layer	Method
normalization	Method
.	O
We	O
also	O
modify	O
the	O
self	Method
-	Method
attention	Method
sub	Method
-	Method
layer	Method
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions	O
.	O
This	O
masking	O
,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position	O
,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
.	O
subsection	O
:	O
Attention	O
An	O
attention	Method
function	Method
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key	O
-	O
value	O
pairs	O
to	O
an	O
output	O
,	O
where	O
the	O
query	O
,	O
keys	O
,	O
values	O
,	O
and	O
output	O
are	O
all	O
vectors	O
.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values	O
,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key	O
.	O
subsubsection	Method
:	O
Scaled	Method
Dot	Method
-	Method
Product	Method
Attention	Method
We	O
call	O
our	O
particular	O
attention	O
"	O
Scaled	Method
Dot	Method
-	Method
Product	Method
Attention	Method
"	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
,	O
and	O
values	O
of	O
dimension	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys	O
,	O
divide	O
each	O
by	O
,	O
and	O
apply	O
a	O
softmax	Method
function	Method
to	O
obtain	O
the	O
weights	O
on	O
the	O
values	O
.	O
In	O
practice	O
,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously	O
,	O
packed	O
together	O
into	O
a	O
matrix	O
.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
and	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as	O
:	O
The	O
two	O
most	O
commonly	O
used	O
attention	Method
functions	Method
are	O
additive	Method
attention	Method
bahdanau2014neural	O
,	O
and	O
dot	Method
-	Method
product	Method
(	Method
multiplicative	Method
)	Method
attention	Method
.	O
Dot	Method
-	Method
product	Method
attention	Method
is	O
identical	O
to	O
our	O
algorithm	O
,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
.	O
Additive	Method
attention	Method
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed	Method
-	Method
forward	Method
network	Method
with	O
a	O
single	O
hidden	Method
layer	Method
.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	Metric
complexity	Metric
,	O
dot	Method
-	Method
product	Method
attention	Method
is	O
much	O
faster	O
and	O
more	O
space	O
-	O
efficient	O
in	O
practice	O
,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	Method
multiplication	Method
code	Method
.	O
While	O
for	O
small	O
values	O
of	O
the	O
two	O
mechanisms	O
perform	O
similarly	O
,	O
additive	Method
attention	Method
outperforms	O
dot	Method
product	Method
attention	Method
without	O
scaling	Method
for	O
larger	O
values	O
of	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
BritzGLL17	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude	O
,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
.	O
To	O
counteract	O
this	O
effect	O
,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
.	O
subsubsection	O
:	O
Multi	Task
-	Task
Head	Task
Attention	Task
Scaled	Task
Dot	Task
-	Task
Product	Task
Attention	Task
Multi	Task
-	Task
Head	Task
Attention	Task
Instead	O
of	O
performing	O
a	O
single	O
attention	Method
function	Method
with	O
-	O
dimensional	O
keys	O
,	O
values	O
and	O
queries	O
,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries	O
,	O
keys	O
and	O
values	O
times	O
with	O
different	O
,	O
learned	O
linear	O
projections	O
to	O
,	O
and	O
dimensions	O
,	O
respectively	O
.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries	O
,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	Method
function	Method
in	O
parallel	O
,	O
yielding	O
-	O
dimensional	O
output	O
values	O
.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected	O
,	O
resulting	O
in	O
the	O
final	O
values	O
,	O
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Multi	Task
-	Task
head	Task
attention	Task
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions	O
.	O
With	O
a	O
single	O
attention	O
head	O
,	O
averaging	O
inhibits	O
this	O
.	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
,	O
,	O
and	O
.	O
In	O
this	O
work	O
we	O
employ	O
parallel	O
attention	O
layers	O
,	O
or	O
heads	O
.	O
For	O
each	O
of	O
these	O
we	O
use	O
.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head	O
,	O
the	O
total	O
computational	Metric
cost	Metric
is	O
similar	O
to	O
that	O
of	O
single	Method
-	Method
head	Method
attention	Method
with	O
full	O
dimensionality	O
.	O
subsubsection	O
:	O
Applications	O
of	O
Attention	O
in	O
our	O
Model	O
The	O
Transformer	Method
uses	O
multi	O
-	O
head	O
attention	O
in	O
three	O
different	O
ways	O
:	O
In	O
"	O
encoder	Method
-	Method
decoder	Method
attention	Method
"	Method
layers	Method
,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	Method
layer	Method
,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder	Method
.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence	O
.	O
This	O
mimics	O
the	O
typical	O
encoder	Method
-	Method
decoder	Method
attention	Method
mechanisms	Method
in	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
such	O
as	O
wu2016google	Method
,	O
bahdanau2014neural	Method
,	O
JonasFaceNet2017	Method
.	O
The	O
encoder	Method
contains	O
self	Method
-	Method
attention	Method
layers	Method
.	O
In	O
a	O
self	Method
-	Method
attention	Method
layer	Method
all	O
of	O
the	O
keys	O
,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place	O
,	O
in	O
this	O
case	O
,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder	Method
.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder	O
.	O
Similarly	O
,	O
self	O
-	O
attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position	O
.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto	O
-	O
regressive	O
property	O
.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	Method
dot	Method
-	Method
product	Method
attention	Method
by	O
masking	O
out	O
(	O
setting	O
to	O
)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections	O
.	O
See	O
Figure	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
Position	Method
-	Method
wise	Method
Feed	Method
-	Method
Forward	Method
Networks	Method
In	O
addition	O
to	O
attention	O
sub	O
-	O
layers	O
,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	Method
and	Method
decoder	Method
contains	O
a	O
fully	Method
connected	Method
feed	Method
-	Method
forward	Method
network	Method
,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically	O
.	O
This	O
consists	O
of	O
two	O
linear	Method
transformations	Method
with	O
a	O
ReLU	O
activation	O
in	O
between	O
.	O
While	O
the	O
linear	Method
transformations	Method
are	O
the	O
same	O
across	O
different	O
positions	O
,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer	O
.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	Method
with	O
kernel	O
size	O
1	O
.	O
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
,	O
and	O
the	O
inner	Method
-	Method
layer	Method
has	O
dimensionality	O
.	O
subsection	O
:	O
Embeddings	Task
and	O
Softmax	Task
Similarly	O
to	O
other	O
sequence	Method
transduction	Method
models	Method
,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next	O
-	O
token	O
probabilities	O
.	O
In	O
our	O
model	O
,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre	Method
-	Method
softmax	Method
linear	Method
transformation	Method
,	O
similar	O
to	O
press2016using	O
.	O
In	O
the	O
embedding	O
layers	O
,	O
we	O
multiply	O
those	O
weights	O
by	O
.	O
subsection	O
:	O
Positional	Method
Encoding	Method
Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution	Method
,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence	O
,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
tokens	O
in	O
the	O
sequence	O
.	O
To	O
this	O
end	O
,	O
we	O
add	O
"	O
positional	O
encodings	O
"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks	O
.	O
The	O
positional	Method
encodings	Method
have	O
the	O
same	O
dimension	O
as	O
the	O
embeddings	O
,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed	O
.	O
There	O
are	O
many	O
choices	O
of	O
positional	Method
encodings	Method
,	O
learned	O
and	O
fixed	O
JonasFaceNet2017	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies	O
:	O
where	O
is	O
the	O
position	O
and	O
is	O
the	O
dimension	O
.	O
That	O
is	O
,	O
each	O
dimension	O
of	O
the	O
positional	Method
encoding	Method
corresponds	O
to	O
a	O
sinusoid	O
.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
to	O
.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions	O
,	O
since	O
for	O
any	O
fixed	O
offset	O
,	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
JonasFaceNet2017	O
instead	O
,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(	O
see	O
Table	O
[	O
reference	O
]	O
row	O
(	O
E	O
)	O
)	O
.	O
We	O
chose	O
the	O
sinusoidal	Method
version	Method
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training	O
.	O
section	O
:	O
Why	O
Self	O
-	O
Attention	O
In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self	Method
-	Method
attention	Method
layers	Method
to	O
the	O
recurrent	Method
and	O
convolutional	Method
layers	Method
commonly	O
used	O
for	O
mapping	O
one	O
variable	O
-	O
length	O
sequence	O
of	O
symbol	O
representations	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
,	O
with	O
,	O
such	O
as	O
a	O
hidden	Method
layer	Method
in	O
a	O
typical	O
sequence	Method
transduction	Method
encoder	Method
or	Method
decoder	Method
.	O
Motivating	O
our	O
use	O
of	O
self	Method
-	Method
attention	Method
we	O
consider	O
three	O
desiderata	O
.	O
One	O
is	O
the	O
total	O
computational	Metric
complexity	Metric
per	O
layer	O
.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized	O
,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required	O
.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long	O
-	O
range	O
dependencies	O
in	O
the	O
network	O
.	O
Learning	Task
long	Task
-	Task
range	Task
dependencies	Task
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	Task
transduction	Task
tasks	Task
.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network	O
.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences	O
,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long	O
-	O
range	O
dependencies	O
hochreiter2001gradient	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types	O
.	O
As	O
noted	O
in	O
Table	O
[	O
reference	O
]	O
,	O
a	O
self	Method
-	Method
attention	Method
layer	Method
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations	O
,	O
whereas	O
a	O
recurrent	Method
layer	O
requires	O
sequential	O
operations	O
.	O
In	O
terms	O
of	O
computational	Metric
complexity	Metric
,	O
self	Method
-	Method
attention	Method
layers	Method
are	O
faster	O
than	O
recurrent	Method
layers	O
when	O
the	O
sequence	O
length	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	Method
representations	Method
used	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
machine	Task
translations	Task
,	O
such	O
as	O
word	Method
-	Method
piece	Method
wu2016google	O
and	O
byte	Method
-	Method
pair	Method
sennrich2015neural	O
representations	O
.	O
To	O
improve	O
computational	Task
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences	O
,	O
self	Task
-	Task
attention	Task
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position	O
.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
.	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work	O
.	O
A	O
single	O
convolutional	Method
layer	Method
with	O
kernel	O
width	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions	O
.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
convolutional	Method
layers	Method
in	O
the	O
case	O
of	O
contiguous	O
kernels	O
,	O
or	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
NalBytenet2017	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network	O
.	O
Convolutional	Method
layers	Method
are	O
generally	O
more	O
expensive	O
than	O
recurrent	Method
layers	O
,	O
by	O
a	O
factor	O
of	O
.	O
Separable	Method
convolutions	Method
xception2016	O
,	O
however	O
,	O
decrease	O
the	O
complexity	Metric
considerably	O
,	O
to	O
.	O
Even	O
with	O
,	O
however	O
,	O
the	O
complexity	Metric
of	O
a	O
separable	Method
convolution	Method
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self	Method
-	Method
attention	Method
layer	Method
and	O
a	O
point	Method
-	Method
wise	Method
feed	Method
-	Method
forward	Method
layer	Method
,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model	O
.	O
As	O
side	O
benefit	O
,	O
self	O
-	O
attention	O
could	O
yield	O
more	O
interpretable	O
models	O
.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix	O
.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks	O
,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences	O
.	O
section	O
:	O
Training	O
This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models	O
.	O
subsection	O
:	O
Training	O
Data	O
and	O
Batching	O
We	O
trained	O
on	O
the	O
standard	O
WMT	Material
2014	Material
English	Material
-	Material
German	Material
dataset	Material
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs	O
.	O
Sentences	O
were	O
encoded	O
using	O
byte	Method
-	Method
pair	Method
encoding	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
BritzGLL17	O
,	O
which	O
has	O
a	O
shared	O
source	O
-	O
target	O
vocabulary	O
of	O
about	O
37000	O
tokens	O
.	O
For	O
English	Material
-	Material
French	Material
,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	Material
2014	Material
English	Material
-	Material
French	Material
dataset	Material
consisting	O
of	O
36	O
M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word	O
-	O
piece	O
vocabulary	O
wu2016google	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length	O
.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens	O
.	O
subsection	O
:	O
Hardware	O
and	O
Schedule	O
We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	Method
P100	Method
GPUs	Method
.	O
For	O
our	O
base	Method
models	Method
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper	O
,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds	O
.	O
We	O
trained	O
the	O
base	Method
models	Method
for	O
a	O
total	O
of	O
100	O
,	O
000	O
steps	O
or	O
12	O
hours	O
.	O
For	O
our	O
big	Method
models	Method
,(	O
described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
[	O
reference	O
]	O
)	O
,	O
step	O
time	O
was	O
1.0	O
seconds	O
.	O
The	O
big	Method
models	Method
were	O
trained	O
for	O
300	O
,	O
000	O
steps	O
(	O
3.5	O
days	O
)	O
.	O
subsection	O
:	O
Optimizer	O
We	O
used	O
the	O
Adam	Method
optimizer	Method
kingma2014adam	O
with	O
,	O
and	O
.	O
We	O
varied	O
the	O
learning	Metric
rate	Metric
over	O
the	O
course	O
of	O
training	O
,	O
according	O
to	O
the	O
formula	O
:	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	Metric
rate	Metric
linearly	O
for	O
the	O
first	O
training	O
steps	O
,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number	O
.	O
We	O
used	O
.	O
subsection	O
:	O
Regularization	Task
We	O
employ	O
three	O
types	O
of	O
regularization	Method
during	O
training	Task
:	O
paragraph	O
:	O
Residual	O
Dropout	O
We	O
apply	O
dropout	Method
srivastava2014dropout	O
to	O
the	O
output	O
of	O
each	O
sub	O
-	O
layer	O
,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub	O
-	O
layer	O
input	O
and	O
normalized	O
.	O
In	O
addition	O
,	O
we	O
apply	O
dropout	Method
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	Method
encodings	Method
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks	O
.	O
For	O
the	O
base	Method
model	Method
,	O
we	O
use	O
a	O
rate	O
of	O
.	O
paragraph	O
:	O
Label	Method
Smoothing	Method
During	O
training	Task
,	O
we	O
employed	O
label	Method
smoothing	Method
of	Method
value	Method
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
SzegedyVISW15	O
.	O
This	O
hurts	O
perplexity	O
,	O
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure	O
,	O
but	O
improves	O
accuracy	Metric
and	O
BLEU	Metric
score	Metric
.	O
section	O
:	O
Results	O
subsection	O
:	O
Machine	Task
Translation	Task
On	O
the	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
German	Task
translation	Task
task	Task
,	O
the	O
big	Method
transformer	Method
model	Method
(	O
Transformer	Method
(	O
big	O
)	O
in	O
Table	O
[	O
reference	O
]	O
)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(	O
including	O
ensembles	Method
)	O
by	O
more	O
than	O
BLEU	Metric
,	O
establishing	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
BLEU	Metric
score	Metric
of	O
.	O
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
[	O
reference	O
]	O
.	O
Training	O
took	O
days	O
on	O
P100	Method
GPUs	Method
.	O
Even	O
our	O
base	Method
model	Method
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles	O
,	O
at	O
a	O
fraction	O
of	O
the	O
training	Metric
cost	Metric
of	O
any	O
of	O
the	O
competitive	Method
models	Method
.	O
On	O
the	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
task	Task
,	O
our	O
big	Method
model	Method
achieves	O
a	O
BLEU	Metric
score	Metric
of	O
,	O
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models	O
,	O
at	O
less	O
than	O
the	O
training	Metric
cost	Metric
of	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
.	O
The	O
Transformer	Method
(	O
big	O
)	O
model	O
trained	O
for	O
English	Material
-	Material
to	Material
-	Material
French	Material
used	O
dropout	Method
rate	O
,	O
instead	O
of	O
.	O
For	O
the	O
base	Method
models	Method
,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints	O
,	O
which	O
were	O
written	O
at	O
10	O
-	O
minute	O
intervals	O
.	O
For	O
the	O
big	Method
models	Method
,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints	O
.	O
We	O
used	O
beam	Method
search	Method
with	O
a	O
beam	O
size	O
of	O
and	O
length	O
penalty	O
wu2016google	O
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set	O
.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	Task
to	O
input	O
length	O
+	O
,	O
but	O
terminate	O
early	O
when	O
possible	O
wu2016google	O
.	O
Table	O
[	O
reference	O
]	O
summarizes	O
our	O
results	O
and	O
compares	O
our	O
translation	Metric
quality	Metric
and	O
training	Metric
costs	Metric
to	O
other	O
model	O
architectures	O
from	O
the	O
literature	O
.	O
We	O
estimate	O
the	O
number	O
of	O
floating	O
point	O
operations	O
used	O
to	O
train	O
a	O
model	O
by	O
multiplying	O
the	O
training	Metric
time	Metric
,	O
the	O
number	O
of	O
GPUs	O
used	O
,	O
and	O
an	O
estimate	O
of	O
the	O
sustained	O
single	O
-	O
precision	O
floating	O
-	O
point	O
capacity	O
of	O
each	O
GPU	O
.	O
subsection	O
:	O
Model	O
Variations	O
To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer	Method
,	O
we	O
varied	O
our	O
base	Method
model	Method
in	O
different	O
ways	O
,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English	Task
-	Task
to	Task
-	Task
German	Task
translation	Task
on	O
the	O
development	O
set	O
,	O
newstest2013	O
.	O
We	O
used	O
beam	Method
search	Method
as	O
described	O
in	O
the	O
previous	O
section	O
,	O
but	O
no	O
checkpoint	Method
averaging	Method
.	O
We	O
present	O
these	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
In	O
Table	O
[	O
reference	O
]	O
rows	O
(	O
A	O
)	O
,	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions	O
,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant	O
,	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
While	O
single	Method
-	Method
head	Method
attention	Method
is	O
0.9	O
BLEU	Metric
worse	O
than	O
the	O
best	O
setting	O
,	O
quality	Metric
also	O
drops	O
off	O
with	O
too	O
many	O
heads	O
.	O
In	O
Table	O
[	O
reference	O
]	O
rows	O
(	O
B	O
)	O
,	O
we	O
observe	O
that	O
reducing	O
the	O
attention	Metric
key	Metric
size	Metric
hurts	O
model	Metric
quality	Metric
.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	Method
product	Method
may	O
be	O
beneficial	O
.	O
We	O
further	O
observe	O
in	O
rows	O
(	O
C	O
)	O
and	O
(	O
D	O
)	O
that	O
,	O
as	O
expected	O
,	O
bigger	O
models	O
are	O
better	O
,	O
and	O
dropout	Method
is	O
very	O
helpful	O
in	O
avoiding	O
over	Task
-	Task
fitting	Task
.	O
In	O
row	O
(	O
E	O
)	O
we	O
replace	O
our	O
sinusoidal	Method
positional	Method
encoding	Method
with	O
learned	O
positional	Method
embeddings	Method
JonasFaceNet2017	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	Method
model	Method
.	O
subsection	O
:	O
English	Task
Constituency	Task
Parsing	Task
To	O
evaluate	O
if	O
the	O
Transformer	Method
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	Task
constituency	Task
parsing	Task
.	O
This	O
task	O
presents	O
specific	O
challenges	O
:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input	O
.	O
Furthermore	O
,	O
RNN	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
have	O
not	O
been	O
able	O
to	O
attain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
small	Task
-	Task
data	Task
regimes	Task
.	O
We	O
trained	O
a	O
4	Method
-	Method
layer	Method
transformer	Method
with	O
on	O
the	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
portion	O
of	O
the	O
Penn	Material
Treebank	Material
marcus1993building	O
,	O
about	O
40	O
K	O
training	O
sentences	O
.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi	Task
-	Task
supervised	Task
setting	Task
,	O
using	O
the	O
larger	O
high	O
-	O
confidence	O
and	O
BerkleyParser	O
corpora	O
from	O
with	O
approximately	O
17	O
M	O
sentences	O
KVparse15	O
.	O
We	O
used	O
a	O
vocabulary	O
of	O
16	O
K	O
tokens	O
for	O
the	O
WSJ	Task
only	Task
setting	Task
and	O
a	O
vocabulary	O
of	O
32	O
K	O
tokens	O
for	O
the	O
semi	Task
-	Task
supervised	Task
setting	Task
.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout	Method
,	O
both	O
attention	O
and	O
residual	O
(	O
section	O
[	O
reference	O
]	O
)	O
,	O
learning	Metric
rates	Metric
and	O
beam	Metric
size	Metric
on	O
the	O
Section	O
22	O
development	O
set	O
,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English	Method
-	Method
to	Method
-	Method
German	Method
base	Method
translation	Method
model	Method
.	O
During	O
inference	Task
,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
.	O
We	O
used	O
a	O
beam	O
size	O
of	O
and	O
for	O
both	O
WSJ	Task
only	Task
and	O
the	O
semi	Task
-	Task
supervised	Task
setting	Task
.	O
Our	O
results	O
in	O
Table	O
[	O
reference	O
]	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task	O
-	O
specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well	O
,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	Method
Neural	Method
Network	Method
Grammar	Method
.	O
In	O
contrast	O
to	O
RNN	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
KVparse15	O
,	O
the	O
Transformer	Method
outperforms	O
the	O
BerkeleyParser	Method
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	O
training	O
set	O
of	O
40	O
K	O
sentences	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
presented	O
the	O
Transformer	Method
,	O
the	O
first	O
sequence	Method
transduction	Method
model	Method
based	O
entirely	O
on	O
attention	Method
,	O
replacing	O
the	O
recurrent	Method
layers	O
most	O
commonly	O
used	O
in	O
encoder	Method
-	Method
decoder	Method
architectures	Method
with	O
multi	O
-	O
headed	O
self	O
-	O
attention	O
.	O
For	O
translation	Task
tasks	Task
,	O
the	O
Transformer	Method
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	Method
or	O
convolutional	Method
layers	Method
.	O
On	O
both	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
German	Task
and	O
WMT	Task
2014	Task
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
tasks	Task
,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles	O
.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention	Method
-	Method
based	Method
models	Method
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks	O
.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	Method
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local	Method
,	Method
restricted	Method
attention	Method
mechanisms	Method
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images	O
,	O
audio	O
and	O
video	O
.	O
Making	O
generation	Task
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours	O
.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
.	O
paragraph	O
:	O
Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments	O
,	O
corrections	O
and	O
inspiration	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Attention	Method
Visualizations	Method
