document	O
:	O
Improving	O
MMD	Method
-	O
GAN	Method
Training	O
with	O
Repulsive	Method
Loss	Method
Function	Method
Generative	Method
adversarial	Method
nets	Method
(	O
GANs	Method
)	O
are	O
widely	O
used	O
to	O
learn	O
the	O
data	Task
sampling	Task
process	Task
and	O
their	O
performance	O
may	O
heavily	O
depend	O
on	O
the	O
loss	O
functions	O
,	O
given	O
a	O
limited	O
computational	O
budget	O
.	O

This	O
study	O
revisits	O
MMD	Method
-	Method
GAN	Method
that	O
uses	O
the	O
maximum	Method
mean	Method
discrepancy	Method
(	O
MMD	Method
)	O
as	O
the	O
loss	O
function	O
for	O
GAN	Method
and	O
makes	O
two	O
contributions	O
.	O

First	O
,	O
we	O
argue	O
that	O
the	O
existing	O
MMD	Method
loss	O
function	O
may	O
discourage	O
the	O
learning	Task
of	Task
fine	Task
details	Task
in	O
data	O
as	O
it	O
attempts	O
to	O
contract	O
the	O
discriminator	O
outputs	O
of	O
real	O
data	O
.	O

To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
a	O
repulsive	Metric
loss	Metric
function	O
to	O
actively	O
learn	O
the	O
difference	O
among	O
the	O
real	O
data	O
by	O
simply	O
rearranging	O
the	O
terms	O
in	O
MMD	Method
.	O

Second	O
,	O
inspired	O
by	O
the	O
hinge	Method
loss	O
,	O
we	O
propose	O
a	O
bounded	Method
Gaussian	Method
kernel	Method
to	O
stabilize	O
the	O
training	O
of	O
MMD	Method
-	Method
GAN	Method
with	O
the	O
repulsive	Metric
loss	Metric
function	O
.	O

The	O
proposed	O
methods	O
are	O
applied	O
to	O
the	O
unsupervised	O
image	Task
generation	Task
tasks	Task
on	O
CIFAR	Material
-	Material
10	Material
,	O
STL	O
-	O
10	O
,	O
CelebA	O
,	O
and	O
LSUN	O
bedroom	O
datasets	O
.	O

Results	O
show	O
that	O
the	O
repulsive	Metric
loss	Metric
function	O
significantly	O
improves	O
over	O
the	O
MMD	Method
loss	O
at	O
no	O
additional	O
computational	Metric
cost	Metric
and	O
outperforms	O
other	O
representative	Method
loss	Method
functions	Method
.	O

The	O
proposed	O
methods	O
achieve	O
an	O
FID	Metric
score	Metric
of	O
16.21	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
using	O
a	O
single	O
DCGAN	Method
network	O
and	O
spectral	Method
normalization	Method
.	O

section	O
:	O
Introduction	O
Generative	Method
adversarial	Method
nets	Method
(	O
GANs	Method
)	O
(	O
)	O
are	O
a	O
branch	O
of	O
generative	Method
models	Method
that	O
learns	O
to	O
mimic	O
the	O
real	Task
data	Task
generating	Task
process	Task
.	O

GANs	Method
have	O
been	O
intensively	O
studied	O
in	O
recent	O
years	O
,	O
with	O
a	O
variety	O
of	O
successful	O
applications	O
(	O
)	O
.	O

The	O
idea	O
of	O
GANs	Method
is	O
to	O
jointly	O
train	O
a	O
generator	Method
network	Method
that	O
attempts	O
to	O
produce	O
artificial	O
samples	O
,	O
and	O
a	O
discriminator	Method
network	Method
or	O
critic	Method
that	O
distinguishes	O
the	O
generated	O
samples	O
from	O
the	O
real	O
ones	O
.	O

Compared	O
to	O
maximum	Method
likelihood	Method
based	Method
methods	Method
,	O
GANs	Method
tend	O
to	O
produce	O
samples	O
with	O
sharper	O
and	O
more	O
vivid	O
details	O
but	O
require	O
more	O
efforts	O
to	O
train	O
.	O

Recent	O
studies	O
on	O
improving	O
GAN	Method
training	O
have	O
mainly	O
focused	O
on	O
designing	O
loss	O
functions	O
,	O
network	Method
architectures	Method
and	O
training	Method
procedures	Method
.	O

The	O
loss	O
function	O
,	O
or	O
simply	O
loss	O
,	O
defines	O
quantitatively	O
the	O
difference	O
of	O
discriminator	O
outputs	O
between	O
real	O
and	O
generated	O
samples	O
.	O

The	O
gradients	O
of	O
loss	O
functions	O
are	O
used	O
to	O
train	O
the	O
generator	Method
and	Method
discriminator	Method
.	O

This	O
study	O
focuses	O
on	O
a	O
loss	Method
function	Method
called	O
maximum	Method
mean	Method
discrepancy	Method
(	O
MMD	Method
)	O
,	O
which	O
is	O
well	O
known	O
as	O
the	O
distance	Metric
metric	Metric
between	O
two	O
probability	O
distributions	O
and	O
widely	O
applied	O
in	O
kernel	Task
two	Task
-	Task
sample	Task
test	Task
(	O
)	O
.	O

Theoretically	O
,	O
MMD	Method
reaches	O
its	O
global	O
minimum	O
zero	O
if	O
and	O
only	O
if	O
the	O
two	O
distributions	O
are	O
equal	O
.	O

Thus	O
,	O
MMD	Method
has	O
been	O
applied	O
to	O
compare	O
the	O
generated	O
samples	O
to	O
real	O
ones	O
directly	O
(	O
)	O
and	O
extended	O
as	O
the	O
loss	O
function	O
to	O
the	O
GAN	Method
framework	O
recently	O
(	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
interpret	O
the	O
optimization	O
of	O
MMD	Method
loss	O
by	O
the	O
discriminator	Method
as	O
a	O
combination	O
of	O
attraction	Method
and	Method
repulsion	Method
processes	Method
,	O
similar	O
to	O
that	O
of	O
linear	Method
discriminant	Method
analysis	Method
.	O

We	O
argue	O
that	O
the	O
existing	O
MMD	Method
loss	O
may	O
discourage	O
the	O
learning	Task
of	Task
fine	Task
details	Task
in	Task
data	Task
,	O
as	O
the	O
discriminator	Method
attempts	O
to	O
minimize	O
the	O
within	O
-	O
group	O
variance	O
of	O
its	O
outputs	O
for	O
the	O
real	O
data	O
.	O

To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
a	O
repulsive	Metric
loss	Metric
for	O
the	O
discriminator	Method
that	O
explicitly	O
explores	O
the	O
differences	O
among	O
real	O
data	O
.	O

The	O
proposed	O
loss	O
achieved	O
significant	O
improvements	O
over	O
the	O
MMD	Method
loss	O
on	O
image	Task
generation	Task
tasks	Task
of	O
four	O
benchmark	O
datasets	O
,	O
without	O
incurring	O
any	O
additional	O
computational	Metric
cost	Metric
.	O

Furthermore	O
,	O
a	O
bounded	Method
Gaussian	Method
kernel	Method
is	O
proposed	O
to	O
stabilize	O
the	O
training	Method
of	Method
discriminator	Method
.	O

As	O
such	O
,	O
using	O
a	O
single	O
kernel	Method
in	O
MMD	Method
-	Method
GAN	Method
is	O
sufficient	O
,	O
in	O
contrast	O
to	O
a	O
linear	Method
combination	Method
of	Method
kernels	Method
used	O
in	O
and	O
.	O

By	O
using	O
a	O
single	O
kernel	O
,	O
the	O
computational	Metric
cost	Metric
of	O
the	O
MMD	Method
loss	O
can	O
potentially	O
be	O
reduced	O
in	O
a	O
variety	O
of	O
applications	O
.	O

The	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
2	O
reviews	O
the	O
GANs	Method
trained	Method
using	O
the	O
MMD	Method
loss	O
(	O
MMD	Method
-	Method
GAN	Method
)	O
.	O

We	O
propose	O
the	O
repulsive	Metric
loss	Metric
for	O
discriminator	Method
in	O
Section	O
3	O
,	O
introduce	O
two	O
practical	O
techniques	O
to	O
stabilize	O
the	O
training	Task
process	Task
in	O
Section	O
4	O
,	O
and	O
present	O
the	O
results	O
of	O
extensive	O
experiments	O
in	O
Section	O
5	O
.	O

In	O
the	O
last	O
section	O
,	O
we	O
discuss	O
the	O
connections	O
between	O
our	O
model	O
and	O
existing	O
work	O
.	O

section	O
:	O
MMD	Method
-	Method
GAN	Method
In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
GAN	Method
model	O
and	O
MMD	Method
loss	O
.	O

Consider	O
a	O
random	O
variable	O
with	O
an	O
empirical	O
data	O
distribution	O
to	O
be	O
learned	O
.	O

A	O
typical	O
GAN	Method
model	O
consists	O
of	O
two	O
neural	Method
networks	Method
:	O
a	O
generator	Method
and	O
a	O
discriminator	Method
.	O

The	O
generator	Method
maps	O
a	O
latent	Method
code	Method
with	O
a	O
fixed	O
distribution	O
(	O
e.g.	O
,	O
Gaussian	O
)	O
to	O
the	O
data	O
space	O
:	O
,	O
where	O
represents	O
the	O
generated	O
samples	O
with	O
distribution	O
.	O

The	O
discriminator	Method
evaluates	O
the	O
scores	O
of	O
a	O
real	O
or	O
generated	O
sample	O
.	O

This	O
study	O
focuses	O
on	O
image	Task
generation	Task
tasks	Task
using	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
for	O
both	O
and	O
.	O

Several	O
loss	Method
functions	Method
have	O
been	O
proposed	O
to	O
quantify	O
the	O
difference	O
of	O
the	O
scores	O
between	O
real	O
and	O
generated	O
samples	O
:	O
and	O
,	O
including	O
the	O
minimax	Method
loss	Method
and	O
non	Method
-	Method
saturating	Method
loss	Method
(	O
)	O
,	O
hinge	Method
loss	O
(	O
)	O
,	O
Wasserstein	Method
loss	Method
(	O
)	O
and	O
maximum	Method
mean	Method
discrepancy	Method
(	O
MMD	Method
)	O
(	O
)	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
more	O
details	O
)	O
.	O

Among	O
them	O
,	O
MMD	Method
uses	O
kernel	Method
embedding	Method
associated	O
with	O
a	O
characteristic	O
kernel	O
such	O
that	O
is	O
infinite	O
-	O
dimensional	O
and	O
.	O

The	O
squared	O
MMD	Method
distance	O
between	O
two	O
distributions	O
and	O
is	O
The	O
kernel	Method
measures	O
the	O
similarity	O
between	O
two	O
samples	O
and	O
.	O

proved	O
that	O
,	O
using	O
a	O
characteristic	Method
kernel	Method
,	O
with	O
equality	O
applies	O
if	O
and	O
only	O
if	O
.	O

In	O
MMD	Method
-	Method
GAN	Method
,	O
the	O
discriminator	Method
can	O
be	O
interpreted	O
as	O
forming	O
a	O
new	O
kernel	Method
with	O
:	O
.	O

If	O
is	O
injective	Method
,	O
is	O
characteristic	O
and	O
reaches	O
its	O
minimum	O
if	O
and	O
only	O
if	O
(	O
)	O
.	O

Thus	O
,	O
the	O
objective	O
functions	O
for	O
and	O
could	O
be	O
(	O
)	O
:	O
MMD	Method
-	Method
GAN	Method
has	O
been	O
shown	O
to	O
be	O
more	O
effective	O
than	O
the	O
model	O
that	O
directly	O
uses	O
MMD	Method
as	O
the	O
loss	O
function	O
for	O
the	O
generator	O
(	O
)	O
.	O

showed	O
that	O
MMD	Method
and	O
Wasserstein	Method
metric	Method
are	O
weaker	O
objective	Metric
functions	Metric
for	O
GAN	Method
than	O
the	O
JensenâShannon	Metric
(	O
JS	Metric
)	O
divergence	O
(	O
related	O
to	O
minimax	O
loss	O
)	O
and	O
total	Metric
variation	Metric
(	O
TV	Metric
)	O
distance	O
(	O
related	O
to	O
hinge	Method
loss	O
)	O
.	O

The	O
reason	O
is	O
that	O
convergence	O
of	O
to	O
in	O
JS	Metric
-	O
divergence	O
and	O
TV	Metric
distance	O
also	O
implies	O
convergence	Metric
in	O
MMD	Method
and	O
Wasserstein	O
metric	O
.	O

Weak	Metric
metrics	Metric
are	O
desirable	O
as	O
they	O
provide	O
more	O
information	O
on	O
adjusting	O
the	O
model	O
to	O
fit	O
the	O
data	O
distribution	O
(	O
)	O
.	O

proved	O
that	O
the	O
GAN	Method
trained	O
using	O
the	O
minimax	O
loss	O
and	O
gradient	Method
updates	Method
on	O
model	O
parameters	O
is	O
locally	O
exponentially	O
stable	O
near	O
equilibrium	O
,	O
while	O
the	O
GAN	Method
using	O
Wasserstein	Method
loss	Method
is	O
not	O
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
demonstrate	O
that	O
the	O
MMD	Method
-	Method
GAN	Method
trained	O
by	O
gradient	Method
descent	Method
is	O
locally	O
exponentially	O
stable	O
near	O
equilibrium	O
.	O

section	O
:	O
Repulsive	Method
Loss	Method
Function	Method
[	O
t	O
]	O
0.26	O
[	O
t	O
]	O
0.26	O
ﬁ	O
i	O
[	O
t	O
]	O
0.26	O
ﬁ	O
i	O
In	O
this	O
section	O
,	O
we	O
interpret	O
the	O
training	O
of	O
MMD	Method
-	Method
GAN	Method
(	O
using	O
and	Method
)	O
as	O
a	O
combination	O
of	O
attraction	Method
and	Method
repulsion	Method
processes	Method
,	O
and	O
propose	O
a	O
novel	O
repulsive	Metric
loss	Metric
function	O
for	O
the	O
discriminator	Method
by	O
rearranging	O
the	O
components	O
in	O
.	O

First	O
,	O
consider	O
a	O
linear	Method
discriminant	Method
analysis	Method
(	O
LDA	Method
)	O
model	O
as	O
the	O
discriminator	Method
.	O

The	O
task	O
is	O
to	O
find	O
a	O
projection	O
to	O
maximize	O
the	O
between	O
-	O
group	O
variance	O
and	O
minimize	O
the	O
within	O
-	O
group	O
variance	O
,	O
where	O
and	O
are	O
group	O
mean	O
and	O
covariance	O
.	O

In	O
MMD	Method
-	Method
GAN	Method
,	O
the	O
neural	Method
-	Method
network	Method
discriminator	Method
works	O
in	O
a	O
similar	O
way	O
as	O
LDA	Method
.	O

By	O
minimizing	O
,	O
the	O
discriminator	Method
tackles	O
two	O
tasks	O
:	O
[	O
label=0	O
)	O
]	O
reduces	O
,	O
i.e.	O
,	O
causes	O
the	O
two	O
groups	O
and	O
to	O
repel	O
each	O
other	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
orange	O
arrows	O
)	O
,	O
or	O
maximize	O
between	O
-	O
group	O
variance	O
;	O
and	O
increases	O
and	O
,	O
i.e.	O
contracts	O
and	O
within	O
each	O
group	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
blue	O
arrows	O
)	O
,	O
or	O
minimize	O
the	O
within	O
-	O
group	O
variance	O
.	O

We	O
refer	O
to	O
loss	Method
functions	Method
that	O
contract	O
real	O
data	O
scores	O
as	O
attractive	O
losses	O
.	O

We	O
argue	O
that	O
the	O
attractive	O
loss	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
has	O
two	O
issues	O
that	O
may	O
slow	O
down	O
the	O
GAN	Method
training	O
:	O
The	O
discriminator	Method
may	O
focus	O
more	O
on	O
the	O
similarities	O
among	O
real	O
samples	O
(	O
in	O
order	O
to	O
contract	O
)	O
than	O
the	O
fine	O
details	O
that	O
separate	O
them	O
.	O

Initially	O
,	O
produces	O
low	O
-	O
quality	O
samples	O
and	O
it	O
may	O
be	O
adequate	O
for	O
to	O
learn	O
the	O
common	O
features	O
of	O
in	O
order	O
to	O
distinguish	O
between	O
and	O
.	O

Only	O
when	O
is	O
sufficiently	O
close	O
to	O
will	O
learn	O
the	O
fine	O
details	O
of	O
to	O
be	O
able	O
to	O
separate	O
from	O
.	O

Consequently	O
,	O
may	O
leave	O
out	O
some	O
fine	O
details	O
in	O
real	O
samples	O
,	O
thus	O
has	O
no	O
access	O
to	O
them	O
during	O
training	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
the	O
gradients	O
on	O
from	O
the	O
attraction	O
(	O
blue	O
arrows	O
)	O
and	O
repulsion	O
(	O
orange	O
arrows	O
)	O
terms	O
in	O
(	O
and	O
thus	O
)	O
may	O
have	O
opposite	O
directions	O
during	O
training	O
.	O

Their	O
summation	O
may	O
be	O
small	O
in	O
magnitude	O
even	O
when	O
is	O
far	O
away	O
from	O
,	O
which	O
may	O
cause	O
to	O
stagnate	O
locally	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
repulsive	Metric
loss	Metric
for	O
to	O
encourage	O
repulsion	O
of	O
the	O
real	O
data	O
scores	O
:	O
The	O
generator	O
uses	O
the	O
same	O
MMD	Method
loss	O
as	O
before	O
(	O
see	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Thus	O
,	O
the	O
adversary	O
lies	O
in	O
the	O
fact	O
that	O
contracts	O
via	O
maximizing	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
while	O
expands	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Additionally	O
,	O
also	O
learns	O
to	O
separate	O
the	O
real	O
data	O
by	O
minimizing	O
,	O
which	O
actively	O
explores	O
the	O
fine	O
details	O
in	O
real	O
samples	O
and	O
may	O
result	O
in	O
more	O
meaningful	O
gradients	O
for	O
.	O

Note	O
that	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
does	O
not	O
explicitly	O
push	O
the	O
average	O
score	O
of	O
away	O
from	O
that	O
of	O
because	O
it	O
may	O
have	O
no	O
effect	O
on	O
the	O
pair	O
-	O
wise	Metric
sample	Metric
distances	Metric
.	O

But	O
aims	O
to	O
match	O
the	O
average	O
scores	O
of	O
both	O
groups	O
.	O

Thus	O
,	O
we	O
believe	O
,	O
compared	O
to	O
the	O
model	O
using	O
and	O
,	O
our	O
model	O
of	O
and	O
is	O
less	O
likely	O
to	O
yield	O
opposite	O
gradients	O
when	O
and	O
are	O
distinct	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
demonstrate	O
that	O
GAN	Method
trained	O
using	O
gradient	Method
descent	Method
and	O
the	O
repulsive	O
MMD	Method
loss	O
is	O
locally	O
exponentially	O
stable	O
near	O
equilibrium	O
.	O

At	O
last	O
,	O
we	O
identify	O
a	O
general	O
form	O
of	O
loss	Method
function	Method
for	O
the	O
discriminator	Method
:	O
where	O
is	O
a	O
hyper	O
-	O
parameter	O
.	O

When	O
,	O
the	O
discriminator	O
loss	O
is	O
attractive	O
,	O
with	O
corresponding	O
to	O
the	O
original	O
MMD	Method
loss	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
;	O
when	O
,	O
is	O
repulsive	O
and	O
corresponds	O
to	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
.	O

It	O
is	O
interesting	O
that	O
when	O
,	O
the	O
discriminator	Method
explicitly	O
contracts	O
and	O
via	O
maximizing	Task
,	O
which	O
may	O
work	O
as	O
a	O
penalty	O
that	O
prevents	O
the	O
pairwise	O
distances	O
of	O
from	O
increasing	O
too	O
fast	O
.	O

Note	O
that	O
has	O
the	O
same	O
computational	Metric
cost	Metric
as	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
as	O
we	O
only	O
rearranged	O
the	O
terms	O
in	O
.	O

section	O
:	O
Regularization	Method
on	O
MMD	Method
and	O
Discriminator	Method
In	O
this	O
section	O
,	O
we	O
propose	O
two	O
approaches	O
to	O
stabilize	O
the	O
training	Task
of	O
MMD	Method
-	Method
GAN	Method
:	O
[	O
label=0	O
)	O
]	O
a	O
bounded	Method
kernel	Method
to	O
avoid	O
the	O
saturation	Task
issue	Task
caused	O
by	O
an	O
over	Method
-	Method
confident	Method
discriminator	Method
;	O
and	O
a	O
generalized	Method
power	Method
iteration	Method
method	Method
to	O
estimate	O
the	O
spectral	O
norm	O
of	O
a	O
convolutional	Method
kernel	Method
,	O
which	O
was	O
used	O
in	O
spectral	Method
normalization	Method
on	O
the	O
discriminator	O
in	O
all	O
experiments	O
in	O
this	O
study	O
unless	O
specified	O
otherwise	O
.	O

subsection	O
:	O
Kernel	O
in	O
MMD	Method
For	O
MMD	Method
-	Method
GAN	Method
,	O
the	O
following	O
two	O
kernels	O
have	O
been	O
used	O
:	O
Gaussian	Method
radial	Method
basis	Method
function	Method
(	O
RBF	Method
)	O
,	O
or	O
Gaussian	Method
kernel	Method
(	O
)	O
,	O
where	O
is	O
the	O
kernel	O
scale	O
or	O
bandwidth	O
.	O

Rational	O
quadratic	O
kernel	O
(	O
)	O
,	O
,	O
where	O
the	O
kernel	O
scale	O
corresponds	O
to	O
a	O
mixture	Method
of	Method
Gaussian	Method
kernels	Method
with	O
a	O
prior	O
on	O
the	O
inverse	O
kernel	O
scales	O
.	O

It	O
is	O
interesting	O
that	O
both	O
studies	O
used	O
a	O
linear	Method
combination	Method
of	Method
kernels	Method
with	O
five	O
different	O
kernel	O
scales	O
,	O
i.e.	O
,	O
and	O
,	O
where	O
,	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
illustration	O
)	O
.	O

We	O
suspect	O
the	O
reason	O
is	O
that	O
a	O
single	O
kernel	Method
is	O
saturated	O
when	O
the	O
distance	O
is	O
either	O
too	O
large	O
or	O
too	O
small	O
compared	O
to	O
the	O
kernel	O
scale	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
,	O
which	O
may	O
cause	O
diminishing	O
gradients	O
during	O
training	O
.	O

Both	O
and	O
applied	O
penalties	O
on	O
the	O
discriminator	O
parameters	O
but	O
not	O
to	O
the	O
MMD	Method
loss	O
itself	O
.	O

Thus	O
the	O
saturation	Task
issue	Task
may	O
still	O
exist	O
.	O

Using	O
a	O
linear	Method
combination	Method
of	Method
kernels	Method
with	O
different	O
kernel	O
scales	O
may	O
alleviate	O
this	O
issue	O
but	O
not	O
eradicate	O
it	O
.	O

Inspired	O
by	O
the	O
hinge	Method
loss	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
,	O
we	O
propose	O
a	O
bounded	Method
RBF	Method
(	O
RBF	Method
-	Method
B	Method
)	O
kernel	O
for	O
the	O
discriminator	Method
.	O

The	O
idea	O
is	O
to	O
prevent	O
from	O
pushing	O
too	O
far	O
away	O
from	O
and	O
causing	O
saturation	O
.	O

For	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
is	O
:	O
For	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
is	O
:	O
where	O
and	O
are	O
the	O
lower	O
and	O
upper	O
bounds	O
.	O

As	O
such	O
,	O
a	O
single	O
kernel	O
is	O
sufficient	O
and	O
we	O
set	O
,	O
and	O
in	O
all	O
experiments	O
for	O
simplicity	O
and	O
leave	O
their	O
tuning	O
for	O
future	O
work	O
.	O

It	O
should	O
be	O
noted	O
that	O
,	O
like	O
the	O
case	O
of	O
hinge	Method
loss	O
,	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
is	O
used	O
only	O
for	O
the	O
discriminator	Method
to	O
prevent	O
it	O
from	O
being	O
over	O
-	O
confident	O
.	O

The	O
generator	O
is	O
always	O
trained	O
using	O
the	O
original	O
RBF	Method
kernel	O
,	O
thus	O
we	O
retain	O
the	O
interpretation	O
of	O
MMD	Method
loss	O
as	O
a	O
metric	Metric
.	O

RBF	Method
-	Method
B	Method
kernel	Method
is	O
among	O
many	O
methods	O
to	O
address	O
the	O
saturation	Task
issue	Task
and	O
stabilize	O
MMD	Method
-	O
GAN	Method
training	O
.	O

We	O
found	O
random	Method
sampling	Method
kernel	Method
scale	Method
,	O
instance	O
noise	O
(	O
)	O
and	O
label	Method
smoothing	Method
(	O
)	O
may	O
also	O
improve	O
the	O
model	O
performance	O
and	O
stability	Metric
.	O

However	O
,	O
the	O
computational	Metric
cost	Metric
of	O
RBF	Method
-	Method
B	Method
kernel	Method
is	O
relatively	O
low	O
.	O

subsection	O
:	O
Spectral	Method
Normalization	Method
in	O
Discriminator	Method
Without	O
any	O
Lipschitz	O
constraints	O
,	O
the	O
discriminator	Method
may	O
simply	O
increase	O
the	O
magnitude	O
of	O
its	O
outputs	O
to	O
minimize	O
the	O
discriminator	O
loss	O
,	O
causing	O
unstable	Task
training	Task
.	O

Spectral	Method
normalization	Method
divides	O
the	O
weight	O
matrix	O
of	O
each	O
layer	O
by	O
its	O
spectral	O
norm	O
,	O
which	O
imposes	O
an	O
upper	O
bound	O
on	O
the	O
magnitudes	O
of	O
outputs	O
and	O
gradients	O
at	O
each	O
layer	O
of	O
(	O
)	O
.	O

However	O
,	O
to	O
estimate	O
the	O
spectral	O
norm	O
of	O
a	O
convolution	Method
kernel	Method
,	O
reshaped	O
the	O
kernel	O
into	O
a	O
matrix	O
.	O

We	O
propose	O
a	O
generalized	Method
power	Method
iteration	Method
method	Method
to	O
directly	O
estimate	O
the	O
spectral	O
norm	O
of	O
a	O
convolution	Method
kernel	Method
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
details	O
)	O
and	O
applied	O
spectral	Method
normalization	Method
to	O
the	O
discriminator	O
in	O
all	O
experiments	O
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
explore	O
using	O
gradient	O
penalty	O
to	O
impose	O
the	O
Lipschitz	O
constraint	O
(	O
)	O
for	O
the	O
proposed	O
repulsive	Metric
loss	Metric
.	O

section	O
:	O
Experiments	O
In	O
this	O
section	O
,	O
we	O
empirically	O
evaluate	O
the	O
proposed	O
[	O
label=0	O
)	O
]	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
on	O
unsupervised	O
training	O
of	O
GAN	Method
for	O
image	Task
generation	Task
tasks	Task
;	O
and	O
RBF	Method
-	Method
B	Method
kernel	Method
to	O
stabilize	O
MMD	Method
-	O
GAN	Method
training	O
.	O

The	O
generalized	Method
power	Method
iteration	Method
method	Method
is	O
evaluated	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

To	O
show	O
the	O
efficacy	O
of	O
,	O
we	O
compared	O
the	O
loss	Method
functions	Method
using	O
Gaussian	Method
kernel	Method
(	O
MMD	Method
-	Method
rep	Method
)	O
with	O
using	O
Gaussian	Method
kernel	Method
(	O
MMD	Method
-	Method
rbf	Method
)	O
(	O
)	O
and	O
rational	Method
quadratic	Method
kernel	Method
(	O
MMD	Method
-	Method
rq	Method
)	O
(	O
)	O
,	O
as	O
well	O
as	O
non	Method
-	Method
saturating	Method
loss	Method
(	O
)	O
and	O
hinge	Method
loss	O
(	O
)	O
.	O

To	O
show	O
the	O
efficacy	O
of	O
RBF	Method
-	Method
B	Method
kernel	Method
,	O
we	O
applied	O
it	O
to	O
both	O
and	O
,	O
resulting	O
in	O
two	O
methods	O
MMD	Method
-	Method
rbf	Method
-	Method
b	Method
and	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
.	O

The	O
Wasserstein	O
loss	O
was	O
excluded	O
for	O
comparison	O
because	O
it	O
can	O
not	O
be	O
directly	O
used	O
with	O
spectral	Method
normalization	Method
(	O
)	O
.	O

subsection	O
:	O
Experiment	O
Setup	O
Dataset	O
:	O
The	O
loss	O
functions	O
were	O
evaluated	O
on	O
four	O
datasets	O
:	O
[	O
label=0	O
)	O
]	O
CIFAR	Material
-	Material
10	Material
(	O
images	O
,	O
pixels	O
)	O
(	O
)	O
;	O
STL	O
-	O
10	O
(	O
images	O
,	O
pixels	O
)	O
(	O
)	O
;	O
CelebA	O
(	O
about	O
images	O
,	O
pixels	O
)	O
(	O
)	O
;	O
and	O
LSUN	O
bedrooms	O
(	O
around	O
million	O
images	O
,	O
pixels	O
)	O
(	O
)	O
.	O

The	O
images	O
were	O
scaled	O
to	O
range	O
to	O
avoid	O
numeric	O
issues	O
.	O

Network	Method
architecture	Method
:	O
The	O
DCGAN	Method
(	O
)	O
architecture	O
was	O
used	O
with	O
hyper	O
-	O
parameters	O
from	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
details	O
)	O
.	O

In	O
all	O
experiments	O
,	O
batch	Method
normalization	Method
(	O
BN	Method
)	O
(	O
)	O
was	O
used	O
in	O
the	O
generator	Method
,	O
and	O
spectral	Method
normalization	Method
with	O
the	O
generalized	Method
power	Method
iteration	Method
(	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
in	O
the	O
discriminator	Method
.	O

For	O
MMD	Method
related	O
losses	O
,	O
the	O
dimension	O
of	O
discriminator	O
output	O
layer	O
was	O
set	O
to	O
;	O
for	O
non	O
-	O
saturating	O
loss	O
and	O
hinge	Method
loss	O
,	O
it	O
was	O
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
investigate	O
the	O
impact	O
of	O
discriminator	O
output	O
dimension	O
on	O
the	O
performance	O
of	O
repulsive	Metric
loss	Metric
.	O

Hyper	O
-	O
parameters	O
:	O
We	O
used	O
Adam	Method
optimizer	Method
(	O
)	O
with	O
momentum	O
parameters	O
,	O
;	O
two	O
-	O
timescale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
(	O
)	O
with	O
two	O
learning	O
rates	O
chosen	O
from	O
(	O
16	O
combinations	O
in	O
total	O
)	O
;	O
and	O
batch	O
size	O
.	O

Fine	O
-	O
tuning	O
on	O
learning	Metric
rates	Metric
may	O
improve	O
the	O
model	O
performance	O
,	O
but	O
constant	O
learning	Metric
rates	Metric
were	O
used	O
for	O
simplicity	O
.	O

All	O
models	O
were	O
trained	O
for	O
iterations	O
on	O
CIFAR	Material
-	Material
10	Material
,	O
STL	O
-	O
10	O
,	O
CelebA	O
and	O
LSUN	O
bedroom	O
datasets	O
,	O
with	O
,	O
i.e.	O
,	O
one	O
discriminator	Method
update	Method
per	O
generator	Method
update	Method
.	O

For	O
MMD	Method
-	Method
rbf	Method
,	O
the	O
kernel	O
scales	O
were	O
used	O
due	O
to	O
a	O
better	O
performance	O
than	O
the	O
original	O
values	O
used	O
in	O
.	O

For	O
MMD	Method
-	Method
rq	Method
,	O
.	O

For	O
MMD	Method
-	O
rbf	O
-	O
b	O
,	O
MMD	Method
-	Method
rep	Method
,	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
,	O
a	O
single	O
Gaussian	Method
kernel	Method
with	O
was	O
used	O
.	O

Evaluation	Metric
metrics	Metric
:	O
Inception	Metric
score	Metric
(	O
IS	Metric
)	O
(	O
)	O
,	O
Fréchet	Metric
Inception	Metric
distance	Metric
(	O
FID	Metric
)	O
(	O
)	O
and	O
multi	Metric
-	Metric
scale	Metric
structural	Metric
similarity	Metric
(	O
MS	Metric
-	Metric
SSIM	Metric
)	O
(	O
)	O
were	O
used	O
for	O
quantitative	Task
evaluation	Task
.	O

Both	O
IS	Metric
and	O
FID	Metric
are	O
calculated	O
using	O
a	O
pre	Method
-	Method
trained	Method
Inception	Method
model	Method
(	O
)	O
.	O

Higher	O
IS	Metric
and	O
lower	O
FID	Metric
scores	Metric
indicate	O
better	O
image	Metric
quality	Metric
.	O

MS	Metric
-	Metric
SSIM	Metric
calculates	O
the	O
pair	Metric
-	Metric
wise	Metric
image	Metric
similarity	Metric
and	O
is	O
used	O
to	O
detect	O
mode	O
collapses	O
among	O
images	O
of	O
the	O
same	O
class	O
(	O
)	O
.	O

Lower	O
MS	Metric
-	Metric
SSIM	Metric
values	O
indicate	O
perceptually	O
more	O
diverse	O
images	O
.	O

For	O
each	O
model	O
,	O
randomly	O
generated	O
samples	O
and	O
real	O
samples	O
were	O
used	O
to	O
calculate	O
IS	Metric
,	O
FID	Metric
and	O
MS	Metric
-	Metric
SSIM	Metric
.	O

subsection	O
:	O
Quantitative	Task
Analysis	Task
The	O
models	O
here	O
differ	O
only	O
by	O
the	O
loss	O
functions	O
and	O
dimension	O
of	O
discriminator	O
outputs	O
.	O

See	O
Section	O
[	O
reference	O
]	O
.	O

For	O
CelebA	O
and	O
LSUN	O
-	O
bedroom	O
,	O
IS	Metric
is	O
not	O
meaningful	O
(	O
)	O
and	O
thus	O
omitted	O
.	O

On	O
LSUN	Method
-	Method
bedroom	Method
,	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rq	Method
did	O
not	O
achieve	O
reasonable	O
results	O
and	O
thus	O
are	O
omitted	O
.	O

[	O
t	O
]	O
ﬁ	O
i	O
[	O
t	O
]	O
ﬁ	O
i	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
Inception	Metric
score	Metric
,	O
FID	Metric
and	O
MS	Metric
-	Metric
SSIM	Metric
of	O
applying	O
different	O
loss	Method
functions	Method
on	O
the	O
benchmark	O
datasets	O
with	O
the	O
optimal	O
learning	Metric
rate	Metric
combinations	Metric
tested	O
experimentally	O
.	O

Note	O
that	O
the	O
same	O
training	O
setup	O
(	O
i.e.	O
,	O
DCGAN	Method
+	O
BN	Method
+	O
SN	Method
+	O
TTUR	Method
)	O
was	O
applied	O
for	O
each	O
loss	O
function	O
.	O

We	O
observed	O
that	O
:	O
[	O
label=0	O
)	O
]	O
MMD	Method
-	Method
rep	Method
and	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
performed	O
significantly	O
better	O
than	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rbf	Method
-	Method
b	Method
respectively	O
,	O
showing	O
the	O
proposed	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
greatly	O
improved	O
over	O
the	O
attractive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
;	O
Using	O
a	O
single	O
kernel	Method
,	O
MMD	Method
-	Method
rbf	Method
-	Method
b	Method
performed	O
better	O
than	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rq	Method
which	O
used	O
a	O
linear	Method
combination	Method
of	Method
five	Method
kernels	Method
,	O
indicating	O
that	O
the	O
kernel	Method
saturation	Method
may	O
be	O
an	O
issue	O
that	O
slows	O
down	O
MMD	Method
-	O
GAN	Method
training	O
;	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
performed	O
comparable	O
or	O
better	O
than	O
MMD	Method
-	Method
rep	Method
on	O
benchmark	O
datasets	O
where	O
we	O
found	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
managed	O
to	O
stabilize	O
MMD	Method
-	O
GAN	Method
training	O
using	O
repulsive	Metric
loss	Metric
.	O

MMD	Method
-	Method
rep	Method
and	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
performed	O
significantly	O
better	O
than	O
the	O
non	O
-	O
saturating	O
and	O
hinge	Method
losses	O
,	O
showing	O
the	O
efficacy	O
of	O
the	O
proposed	O
repulsive	Metric
loss	Metric
.	O

Additionally	O
,	O
we	O
trained	O
MMD	Method
-	Method
GAN	Method
using	O
the	O
general	O
loss	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
for	O
discriminator	Method
and	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
for	O
generator	Task
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
influence	O
of	O
on	O
the	O
performance	O
of	O
MMD	Method
-	Method
GAN	Method
with	O
RBF	Method
and	O
RBF	Method
-	Method
B	Method
kernel	Method
.	O

Note	O
that	O
when	O
,	O
the	O
models	O
are	O
essentially	O
MMD	Method
-	Method
rbf	Method
(	O
with	O
a	O
single	Method
Gaussian	Method
kernel	Method
)	O
and	O
MMD	Method
-	Method
rbf	Method
-	Method
b	Method
when	O
RBF	Method
and	O
RBF	Method
-	Method
B	Method
kernel	Method
are	O
used	O
respectively	O
.	O

We	O
observed	O
that	O
:	O
[	O
label=0	O
)	O
]	O
the	O
model	O
performed	O
well	O
using	O
repulsive	Metric
loss	Metric
(	O
i.e.	O
,	O
)	O
,	O
with	O
slightly	O
better	O
than	O
;	O
the	O
MMD	Method
-	O
rbf	O
model	O
can	O
be	O
significantly	O
improved	O
by	O
simply	O
increasing	O
from	O
to	O
,	O
which	O
reduces	O
the	O
attraction	O
of	O
discriminator	O
on	O
real	O
sample	O
scores	O
;	O
larger	O
may	O
lead	O
to	O
more	O
diverged	O
models	O
,	O
possibly	O
because	O
the	O
discriminator	Method
focuses	O
more	O
on	O
expanding	O
the	O
real	O
sample	O
scores	O
over	O
adversarial	Method
learning	Method
;	O
note	O
when	O
,	O
the	O
model	O
would	O
simply	O
learn	O
to	O
expand	O
all	O
real	O
sample	O
scores	O
and	O
pull	O
the	O
generated	O
sample	O
scores	O
to	O
real	O
samples	O
’	O
,	O
which	O
is	O
a	O
divergent	O
process	O
;	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
managed	O
to	O
stabilize	O
MMD	Method
-	Method
rep	Method
for	O
most	O
diverged	O
cases	O
but	O
may	O
occasionally	O
cause	O
the	O
FID	Metric
score	Metric
to	O
rise	O
up	O
.	O

The	O
proposed	O
methods	O
were	O
further	O
evaluated	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
used	O
a	O
simulation	O
study	O
to	O
show	O
the	O
local	Metric
stability	Metric
of	O
MMD	Method
-	Method
rep	Method
trained	O
by	O
gradient	Method
descent	Method
,	O
while	O
its	O
global	Metric
stability	Metric
is	O
not	O
guaranteed	O
as	O
bad	O
initialization	O
may	O
lead	O
to	O
trivial	O
solutions	O
.	O

The	O
problem	O
may	O
be	O
alleviated	O
by	O
adjusting	O
the	O
learning	Metric
rate	Metric
for	O
generator	Task
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
showed	O
the	O
proposed	O
generalized	Method
power	Method
iteration	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
imposes	O
a	O
stronger	O
Lipschitz	O
constraint	O
than	O
the	O
method	O
in	O
,	O
and	O
benefited	O
MMD	Method
-	O
GAN	Method
training	O
using	O
the	O
repulsive	Metric
loss	Metric
.	O

Moreover	O
,	O
the	O
RBF	Method
-	Method
B	Method
kernel	Method
managed	O
to	O
stabilize	O
the	O
MMD	Method
-	O
GAN	Method
training	O
for	O
various	O
configurations	O
of	O
the	O
spectral	Method
normalization	Method
method	Method
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
showed	O
the	O
gradient	O
penalty	O
can	O
also	O
be	O
used	O
with	O
the	O
repulsive	Metric
loss	Metric
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
showed	O
that	O
it	O
was	O
better	O
to	O
use	O
more	O
than	O
one	O
neuron	O
at	O
the	O
discriminator	O
output	O
layer	O
for	O
the	O
repulsive	Metric
loss	Metric
.	O

subsection	O
:	O
Qualitative	Method
Analysis	Method
[	O
t	O
]	O
1	O
ﬁ	O
i	O
[	O
t	O
]	O
1	O
ﬁ	O
i	O
The	O
discriminator	O
outputs	O
may	O
be	O
interpreted	O
as	O
a	O
learned	Method
representation	Method
of	O
the	O
input	O
samples	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
visualizes	O
the	O
discriminator	O
outputs	O
learned	O
by	O
the	O
MMD	Method
-	Method
rbf	Method
and	O
proposed	O
MMD	Method
-	O
rep	O
methods	O
on	O
CIFAR	Material
-	Material
10	Material
dataset	Material
using	O
t	Method
-	Method
SNE	Method
(	Method
)	O
.	O

MMD	Method
-	Method
rbf	Method
ignored	O
the	O
class	O
structure	O
in	O
data	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
while	O
MMD	Method
-	Method
rep	Method
learned	O
to	O
concentrate	O
the	O
data	O
from	O
the	O
same	O
class	O
and	O
separate	O
different	O
classes	O
to	O
some	O
extent	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
is	O
because	O
the	O
discriminator	Method
has	O
to	O
actively	O
learn	O
the	O
data	O
structure	O
in	O
order	O
to	O
expands	O
the	O
real	O
sample	O
scores	O
.	O

Thus	O
,	O
we	O
speculate	O
that	O
techniques	O
reinforcing	O
the	O
learning	Task
of	Task
cluster	Task
structures	Task
in	O
data	O
may	O
further	O
improve	O
the	O
training	O
of	O
MMD	Method
-	Method
GAN	Method
.	O

In	O
addition	O
,	O
the	O
performance	O
gain	O
of	O
proposed	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
over	O
the	O
attractive	O
loss	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
comes	O
at	O
no	O
additional	O
computational	Metric
cost	Metric
.	O

In	O
fact	O
,	O
by	O
using	O
a	O
single	O
kernel	Method
rather	O
than	O
a	O
linear	Method
combination	Method
of	Method
kernels	Method
,	O
MMD	Method
-	Method
rep	Method
and	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
are	O
simpler	O
than	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rq	Method
.	O

Besides	O
,	O
given	O
a	O
typically	O
small	O
batch	O
size	O
and	O
a	O
small	O
number	O
of	O
discriminator	O
output	O
neurons	O
(	O
and	O
in	O
our	O
experiments	O
)	O
,	O
the	O
cost	O
of	O
MMD	Method
over	O
the	O
non	O
-	O
saturating	O
and	O
hinge	Method
loss	O
is	O
marginal	O
compared	O
to	O
the	O
convolution	Method
operations	Method
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
provide	O
some	O
random	O
samples	O
generated	O
by	O
the	O
methods	O
in	O
our	O
study	O
.	O

section	O
:	O
Discussion	O
This	O
study	O
extends	O
the	O
previous	O
work	O
on	O
MMD	Method
-	O
GAN	Method
(	O
)	O
with	O
two	O
contributions	O
.	O

First	O
,	O
we	O
interpreted	O
the	O
optimization	O
of	O
MMD	Method
loss	O
as	O
a	O
combination	O
of	O
attraction	Method
and	Method
repulsion	Method
processes	Method
,	O
and	O
proposed	O
a	O
repulsive	Metric
loss	Metric
for	O
the	O
discriminator	Method
that	O
actively	O
learns	O
the	O
difference	O
among	O
real	O
data	O
.	O

Second	O
,	O
we	O
proposed	O
a	O
bounded	Method
Gaussian	Method
RBF	Method
(	O
RBF	Method
-	Method
B	Method
)	O
kernel	O
to	O
address	O
the	O
saturation	Task
issue	Task
.	O

Empirically	O
,	O
we	O
observed	O
that	O
the	O
repulsive	Metric
loss	Metric
may	O
result	O
in	O
unstable	Task
training	Task
,	O
due	O
to	O
factors	O
including	O
initialization	O
(	O
Appendix	O
[	O
reference	O
]	O
)	O
,	O
learning	Metric
rate	Metric
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
and	O
Lipschitz	O
constraints	O
on	O
the	O
discriminator	Method
(	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

The	O
RBF	Method
-	Method
B	Method
kernel	Method
managed	O
to	O
stabilize	O
the	O
MMD	Method
-	O
GAN	Method
training	O
in	O
many	O
cases	O
.	O

Tuning	O
the	O
hyper	O
-	O
parameters	O
in	O
RBF	Method
-	Method
B	Method
kernel	Method
or	O
using	O
other	O
regularization	Method
methods	Method
may	O
further	O
improve	O
our	O
results	O
.	O

The	O
theoretical	O
advantages	O
of	O
MMD	Method
-	Method
GAN	Method
require	O
the	O
discriminator	Method
to	O
be	O
injective	Method
.	O

The	O
proposed	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
attempts	O
to	O
realize	O
this	O
by	O
explicitly	O
maximizing	O
the	O
pair	O
-	O
wise	O
distances	O
among	O
the	O
real	O
samples	O
.	O

achieved	O
the	O
injection	O
property	O
by	O
using	O
the	O
discriminator	Method
as	O
the	O
encoder	Method
and	O
an	O
auxiliary	Method
network	Method
as	O
the	O
decoder	Method
to	O
reconstruct	O
the	O
real	O
and	O
generated	O
samples	O
,	O
which	O
is	O
more	O
computationally	O
extensive	O
than	O
our	O
proposed	O
approach	O
.	O

On	O
the	O
other	O
hand	O
,	O
imposed	O
a	O
Lipschitz	O
constraint	O
on	O
the	O
discriminator	Method
in	O
MMD	Method
-	Method
GAN	Method
via	O
gradient	Method
penalty	Method
,	O
which	O
may	O
not	O
necessarily	O
promote	O
an	O
injective	Method
discriminator	O
.	O

The	O
idea	O
of	O
repulsion	O
on	O
real	O
sample	O
scores	O
is	O
in	O
line	O
with	O
existing	O
studies	O
.	O

It	O
has	O
been	O
widely	O
accepted	O
that	O
the	O
quality	O
of	O
generated	O
samples	O
can	O
be	O
significantly	O
improved	O
by	O
integrating	O
labels	O
(	O
)	O
or	O
even	O
pseudo	O
-	O
labels	O
generated	O
by	O
k	Method
-	Method
means	Method
method	Method
(	O
)	O
in	O
the	O
training	O
of	O
discriminator	Method
.	O

The	O
reason	O
may	O
be	O
that	O
the	O
labels	O
help	O
concentrate	O
the	O
data	O
from	O
the	O
same	O
class	O
and	O
separate	O
those	O
from	O
different	O
classes	O
.	O

Using	O
a	O
pre	Method
-	Method
trained	Method
classifier	Method
may	O
also	O
help	O
produce	O
vivid	O
image	O
samples	O
(	O
)	O
as	O
the	O
learned	O
representations	O
of	O
the	O
real	O
samples	O
in	O
the	O
hidden	O
layers	O
of	O
the	O
classifier	Method
tend	O
to	O
be	O
well	O
separated	O
/	O
organized	O
and	O
may	O
produce	O
more	O
meaningful	O
gradients	O
to	O
the	O
generator	O
.	O

At	O
last	O
,	O
we	O
note	O
that	O
the	O
proposed	O
repulsive	Metric
loss	Metric
is	O
orthogonal	O
to	O
the	O
GAN	Method
studies	O
on	O
designing	O
network	Method
structures	Method
and	O
training	Task
procedures	Task
,	O
and	O
thus	O
may	O
be	O
combined	O
with	O
a	O
variety	O
of	O
novel	O
techniques	O
.	O

For	O
example	O
,	O
the	O
ResNet	Method
architecture	Method
(	O
)	O
has	O
been	O
reported	O
to	O
outperform	O
the	O
plain	O
DCGAN	Method
used	O
in	O
our	O
experiments	O
on	O
image	Task
generation	Task
tasks	Task
(	O
)	O
and	O
self	Method
-	Method
attention	Method
module	Method
may	O
further	O
improve	O
the	O
results	O
(	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
proposed	O
to	O
progressively	O
grows	O
the	O
size	O
of	O
both	O
discriminator	Method
and	O
generator	Method
and	O
achieved	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
unsupervised	Task
training	Task
of	Task
GANs	Task
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

Future	O
work	O
may	O
explore	O
these	O
directions	O
.	O

subsubsection	O
:	O
Acknowledgments	O
Wei	O
Wang	O
is	O
fully	O
supported	O
by	O
the	O
Ph.D.	O
scholarships	O
of	O
The	O
University	O
of	O
Melbourne	O
.	O

This	O
work	O
is	O
partially	O
funded	O
by	O
Australian	O
Research	O
Council	O
grant	O
DP150103512	O
and	O
undertaken	O
using	O
the	O
LIEF	Method
HPC	Method
-	Method
GPGPU	Method
Facility	Method
hosted	O
at	O
the	O
University	O
of	O
Melbourne	O
.	O

The	O
Facility	O
was	O
established	O
with	O
the	O
assistance	O
of	O
LIEF	O
Grant	O
LE170100200	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Stability	Task
analysis	Task
of	O
MMD	Method
-	Method
GAN	Method
This	O
section	O
demonstrates	O
that	O
,	O
under	O
mild	O
assumptions	O
,	O
MMD	Method
-	Method
GAN	Method
trained	O
by	O
gradient	Method
descent	Method
is	O
locally	O
exponentially	O
stable	O
at	O
equilibrium	O
.	O

It	O
is	O
organized	O
as	O
follows	O
.	O

The	O
main	O
assumption	O
and	O
proposition	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
followed	O
by	O
simulation	O
study	O
in	O
Section	O
[	O
reference	O
]	O
and	O
proof	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
discuss	O
the	O
indications	O
of	O
assumptions	O
on	O
the	O
discriminator	O
of	O
GAN	Method
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Main	O
Proposition	O
We	O
consider	O
GAN	Method
trained	O
using	O
the	O
MMD	Method
loss	O
for	O
generator	Method
and	O
either	O
the	O
attractive	O
loss	O
or	O
repulsive	Metric
loss	Metric
for	O
discriminator	Method
,	O
listed	O
below	O
:	O
where	O
.	O

Let	O
be	O
the	O
support	O
of	O
distribution	O
;	O
let	O
,	O
be	O
the	O
parameters	O
of	O
the	O
generator	Method
and	Method
discriminator	Method
respectively	O
.	O

To	O
prove	O
that	O
GANs	Method
trained	Method
using	O
the	O
minimax	Method
loss	Method
and	Method
gradient	Method
updates	Method
is	O
locally	O
stable	O
at	O
the	O
equilibrium	O
point	O
,	O
made	O
the	O
following	O
assumption	O
:	O
theorem	O
:	O
(	O
)	O
.	O

and	O
∈∀x⁢S	O
(	O
PX	O
),	O
=⁢DθD*	O
(	O
x	O
)	O
0	O
.	O

For	O
loss	O
functions	O
like	O
minimax	Method
and	O
Wasserstein	Method
,	O
may	O
be	O
interpreted	O
as	O
how	O
plausible	O
a	O
sample	O
is	O
real	O
.	O

Thus	O
at	O
equilibrium	O
,	O
it	O
may	O
be	O
reasonable	O
to	O
assume	O
all	O
real	O
and	O
generated	O
samples	O
are	O
equally	O
plausible	O
.	O

However	O
,	O
also	O
indicates	O
that	O
may	O
have	O
no	O
discrimination	O
power	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
discussion	O
)	O
.	O

For	O
MMD	Method
loss	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
may	O
be	O
interpreted	O
as	O
a	O
learned	Method
representation	Method
of	Method
the	Method
distribution	Method
.	O

As	O
long	O
as	O
two	O
distributions	O
and	O
match	O
,	O
.	O

On	O
the	O
other	O
hand	O
,	O
is	O
a	O
minima	Method
solution	Method
for	O
but	O
is	O
trained	O
to	O
find	O
local	O
maxima	O
.	O

Thus	O
in	O
contrast	O
to	O
Assumption	O
[	O
reference	O
]	O
,	O
we	O
assume	O
theorem	O
:	O
.	O

For	O
GANs	Method
using	O
MMD	Method
loss	O
in	O
Eq	O
.	O

,	O
and	O
random	O
initialization	O
on	O
parameters	O
,	O
at	O
equilibrium	O
,	O
⁢DθD*	O
(	O
x	O
)	O
is	O
injective	Method
on	O
⁢S	O
(	O
PX	O
)	O
⋃⁢S	O
(	O
PθG	O
*	O
)	O
.	O

Assumption	O
[	O
reference	O
]	O
indicates	O
that	O
is	O
not	O
constant	O
almost	O
everywhere	O
.	O

We	O
use	O
a	O
simulation	O
study	O
in	O
Section	O
[	O
reference	O
]	O
to	O
show	O
that	O
does	O
not	O
hold	O
in	O
general	O
for	O
MMD	Method
loss	O
.	O

Based	O
on	O
Assumption	O
[	O
reference	O
]	O
,	O
we	O
propose	O
the	O
following	O
proposition	O
and	O
prove	O
it	O
in	O
Appendix	O
[	O
reference	O
]	O
:	O
theorem	O
:	O
.	O

If	O
there	O
exists	O
∈θG*ΘG	O
such	O
that	O
=	O
PθG*PX	O
,	O
then	O
GANs	Method
with	O
MMD	Method
loss	O
in	O
Eq	O
.	O

has	O
equilibria	O
(	O
θG*	O
,	O
θD	O
)	O
for	O
any	O
∈θDΘD	O
.	O

Moreover	O
,	O
the	O
model	O
trained	O
using	O
gradient	Method
descent	Method
methods	Method
is	O
locally	O
exponentially	O
stable	O
at	O
(	O
θG*	O
,	O
θD	O
)	O
for	O
any	O
∈θDΘD	O
.	O

There	O
may	O
exist	O
non	O
-	O
realizable	O
cases	O
where	O
the	O
mapping	O
between	O
and	O
can	O
not	O
be	O
represented	O
by	O
any	O
generator	Method
with	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
use	O
a	O
simulation	O
study	O
to	O
show	O
that	O
both	O
the	O
attractive	O
MMD	Method
loss	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
proposed	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
may	O
be	O
locally	O
stable	O
and	O
leave	O
the	O
proof	O
for	O
future	O
work	O
.	O

subsection	O
:	O
Simulation	O
Study	O
[	O
t	O
]	O
0.4	O
[	O
t	O
]	O
0.4	O
[	O
t	O
]	O
0.4	O
[	O
t	O
]	O
0.4	O
In	O
this	O
section	O
,	O
we	O
reused	O
the	O
example	O
from	O
to	O
show	O
that	O
GAN	Method
trained	O
using	O
the	O
MMD	Method
loss	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
is	O
locally	O
stable	O
.	O

Consider	O
a	O
two	O
-	O
parameter	O
MMD	Method
-	O
GAN	Method
with	O
uniform	O
latent	O
distribution	O
over	O
,	O
generator	Method
,	O
discriminator	Method
,	O
and	O
Gaussian	Method
kernel	Method
.	O

The	O
MMD	Method
-	O
rbf	O
model	O
(	O
and	O
from	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
MMD	Method
-	Method
rep	Method
model	Method
(	O
and	O
from	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
were	O
tested	O
.	O

Each	O
model	O
was	O
applied	O
to	O
two	O
cases	O
:	O
the	O
data	O
distribution	O
is	O
the	O
same	O
as	O
,	O
i.e.	O
,	O
uniform	O
over	O
,	O
thus	O
is	O
realizable	O
;	O
is	O
standard	O
Gaussian	Method
,	O
thus	O
non	O
-	O
realizable	O
for	O
any	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
MMD	Method
-	Method
GAN	Method
are	O
locally	O
stable	O
in	O
both	O
cases	O
and	O
does	O
not	O
hold	O
in	O
general	O
for	O
MMD	Method
loss	O
.	O

However	O
,	O
MMD	Method
-	Method
rep	Method
may	O
not	O
be	O
globally	O
stable	O
for	O
the	O
tested	O
cases	O
:	O
initialization	O
of	O
in	O
some	O
regions	O
may	O
lead	O
to	O
the	O
trivial	O
solution	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

We	O
note	O
that	O
by	O
decreasing	O
the	O
learning	Metric
rate	Metric
for	O
,	O
the	O
area	O
of	O
such	O
regions	O
decreased	O
.	O

At	O
last	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
both	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rep	Method
had	O
the	O
same	O
nontrivial	O
solution	O
for	O
generator	Task
in	O
the	O
non	O
-	O
realizable	O
cases	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Proof	O
of	O
Proposition	O
[	O
reference	O
]	O
This	O
section	O
divides	O
the	O
proof	O
for	O
Proposition	O
[	O
reference	O
]	O
into	O
two	O
parts	O
.	O

First	O
,	O
we	O
show	O
that	O
GAN	Method
with	O
the	O
MMD	Method
loss	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
has	O
equilibria	O
for	O
any	O
parameter	O
configuration	O
of	O
discriminator	Method
;	O
second	O
,	O
we	O
prove	O
the	O
model	O
is	O
locally	O
exponentially	O
stable	O
.	O

For	O
convenience	O
,	O
we	O
consider	O
the	O
general	O
form	O
of	O
discriminator	O
loss	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
:	O
which	O
has	O
and	O
as	O
the	O
special	O
cases	O
when	O
equals	O
and	O
respectively	O
.	O

Consider	O
real	O
data	O
,	O
latent	O
variable	O
and	O
generated	O
variable	O
.	O

Let	O
,	O
,	O
be	O
their	O
samples	O
.	O

Denote	O
;	O
,	O
;	O
,	O
where	O
and	O
are	O
the	O
losses	O
for	O
and	O
respectively	O
.	O

Assume	O
an	O
isotropic	Method
stationary	Method
kernel	Method
(	O
)	O
is	O
used	O
in	O
MMD	Method
.	O

We	O
first	O
show	O
:	O
theorem	O
:	O
(	O
Part	O
1	O
)	O
.	O

If	O
there	O
exists	O
∈θG*ΘG	O
such	O
that	O
=	O
PθG*PX	O
,	O
the	O
GAN	Method
with	O
the	O
MMD	Method
loss	O
in	O
Eq	O
.	O

and	O
Eq	O
.	O

has	O
equilibria	O
(	O
θG*	O
,	O
θD	O
)	O
for	O
any	O
∈θDΘD	O
.	O

proof	O
:	O
Proof	O
.	O

Denote	O
and	O
where	O
is	O
the	O
kernel	O
of	O
MMD	Method
.	O

The	O
gradients	O
of	O
MMD	Method
loss	O
are	O
Note	O
that	O
,	O
given	O
i.i.d	O
.	O

drawn	O
samples	O
and	O
,	O
an	O
unbiased	Method
estimator	Method
of	O
the	O
squared	O
MMD	Method
is	O
(	O
)	O
At	O
equilibrium	O
,	O
consider	O
a	O
sequence	O
of	O
samples	O
with	O
,	O
we	O
have	O
where	O
for	O
we	O
have	O
used	O
the	O
fact	O
that	O
for	O
each	O
term	O
in	O
the	O
summation	O
,	O
there	O
exists	O
an	O
term	O
with	O
reversed	O
and	O
thus	O
the	O
summation	O
is	O
zero	O
.	O

Since	O
we	O
have	O
not	O
assumed	O
the	O
status	O
of	O
,	O
for	O
any	O
.	O

∎	O
We	O
proceed	O
to	O
prove	O
the	O
model	Task
stability	Task
.	O

First	O
,	O
following	O
Theorem	O
5	O
in	O
and	O
Theorem	O
4	O
in	O
,	O
it	O
is	O
straightforward	O
to	O
see	O
:	O
theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
≥⁢M∘kDθD2	O
(	O
PX	O
,	O
PθG	O
)	O
0	O
with	O
the	O
equality	O
if	O
and	O
only	O
if	O
=	O
PXPθG.	O
Lemma	O
[	O
reference	O
]	O
and	O
Proposition	O
1	O
(	O
Part	O
1	O
)	O
state	O
that	O
at	O
equilibrium	O
,	O
every	O
discriminator	O
and	O
kernel	O
will	O
give	O
,	O
thus	O
no	O
discriminator	Method
can	O
distinguish	O
the	O
two	O
distributions	O
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
cite	O
Theorem	O
A.4	O
from	O
:	O
theorem	O
:	O
(	O
)	O
.	O

Consider	O
a	O
non	O
-	O
linear	O
system	O
of	O
parameters	O
(	O
θ	O
,	O
γ	O
)	O
:	O
=	O
˙θ⁢h1	O
(	O
θ	O
,	O
γ	O
)	O
,	O
=	O
˙γ⁢h2	O
(	O
θ	O
,	O
γ	O
)	O
with	O
an	O
equilibrium	O
point	O
at	O
(	O
0	O
,	O
0	O
)	O
.	O

Let	O
there	O
exist	O
ϵ	O
such	O
that	O
∈∀γ⁢Bϵ	O
(	O
0	O
)	O
,	O
(	O
0	O
,	O
γ	O
)	O
is	O
an	O
equilibrium	O
.	O

If	O
J=⁢∂h1	O
(	O
θ	O
,	O
γ	O
)	O
∂θ|	O
(	O
0	O
,	O
0	O
)	O
is	O
a	O
Hurwitz	O
matrix	O
,	O
the	O
non	Method
-	Method
linear	Method
system	Method
is	O
exponentially	O
stable	O
.	O

Now	O
we	O
can	O
prove	O
:	O
theorem	O
:	O
(	O
Part	O
2	O
)	O
.	O

At	O
equilibrium	O
=	O
PθG*PX	O
,	O
the	O
GAN	Method
trained	O
using	O
MMD	Method
loss	O
and	O
gradient	O
descent	O
methods	O
is	O
locally	O
exponentially	O
stable	O
at	O
(	O
θG*	O
,	O
θD	O
)	O
for	O
any	O
∈θDΘD	O
.	O

proof	O
:	O
Proof	O
.	O

Inspired	O
by	O
,	O
we	O
first	O
derive	O
the	O
Jacobian	O
of	O
the	O
system	O
Denote	O
and	O
.	O

Based	O
on	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
we	O
have	O
where	O
is	O
the	O
kronecker	O
product	O
.	O

At	O
equilibrium	O
,	O
consider	O
a	O
sequence	O
of	O
samples	O
with	O
,	O
we	O
have	O
,	O
and	O
Given	O
Lemma	O
[	O
reference	O
]	O
and	O
fact	O
that	O
is	O
the	O
Hessian	O
matrix	O
of	O
,	O
is	O
negative	O
semidefinite	O
.	O

The	O
eigenvectors	O
of	O
corresponding	O
to	O
zero	O
eigenvalues	O
form	O
.	O

There	O
may	O
exist	O
small	O
distortion	O
such	O
that	O
.	O

That	O
is	O
,	O
is	O
locally	O
constant	O
along	O
some	O
directions	O
in	O
the	O
parameter	O
space	O
of	O
.	O

As	O
a	O
result	O
,	O
because	O
varying	O
along	O
these	O
directions	O
has	O
no	O
effect	O
on	O
.	O

Following	O
Lemma	O
C.3	O
of	O
,	O
we	O
consider	O
eigenvalue	Method
decomposition	Method
and	O
.	O

Let	O
,	O
such	O
that	O
,	O
.	O

Thus	O
,	O
the	O
projections	O
are	O
orthogonal	O
to	O
.	O

Then	O
,	O
the	O
Jacobian	O
corresponding	O
to	O
the	O
projected	Method
system	Method
has	O
the	O
form	O
with	O
block	O
and	O
,	O
where	O
is	O
negative	O
definite	O
.	O

Moreover	O
,	O
on	O
all	O
directions	O
exclude	O
those	O
described	O
by	O
,	O
the	O
system	O
is	O
surrounded	O
by	O
a	O
neighborhood	O
of	O
equilibia	O
at	O
least	O
locally	O
.	O

According	O
to	O
Lemma	O
[	O
reference	O
]	O
,	O
the	O
system	O
is	O
exponentially	O
stable	O
.	O

∎	O
subsection	O
:	O
Discussion	O
on	O
Assumption	O
[	O
reference	O
]	O
This	O
section	O
shows	O
that	O
constant	O
discriminator	O
output	O
for	O
indicates	O
that	O
may	O
have	O
no	O
discrimination	O
power	O
.	O

First	O
,	O
we	O
make	O
the	O
following	O
assumptions	O
:	O
theorem	O
:	O
.	O

is	O
a	O
multilayer	Method
perceptron	Method
where	O
each	O
layer	O
can	O
be	O
factorized	O
into	O
an	O
affine	Method
transform	Method
and	O
an	O
element	O
-	O
wise	O
activation	O
function	O
.	O

Each	O
activation	Method
function	Method
;	O
furthermore	O
,	O
has	O
a	O
finite	O
number	O
of	O
discontinuities	O
and	O
.	O

Input	O
data	O
to	O
is	O
continuous	O
and	O
its	O
support	O
is	O
compact	O
in	O
with	O
non	O
-	O
zero	O
measure	O
in	O
each	O
dimension	O
and	O
.	O

Based	O
on	O
Assumption	O
[	O
reference	O
]	O
,	O
we	O
have	O
the	O
following	O
proposition	O
:	O
theorem	O
:	O
.	O

If	O
∈∀xS	O
,	O
=	O
⁢D	O
(	O
x	O
)	O
c	O
,	O
where	O
c	O
is	O
constant	O
,	O
then	O
there	O
always	O
exists	O
distortion	O
⁢δx	O
such	O
that	O
∉	O
+	O
x⁢δxS	O
and	O
=	O
⁢D	O
(+	O
x⁢δx	O
)	O
c	O
.	O

proof	O
:	O
Proof	O
.	O

Without	O
loss	O
of	O
generality	O
,	O
we	O
consider	O
and	O
,	O
where	O
,	O
,	O
,	O
are	O
model	O
weights	O
and	O
biases	O
,	O
is	O
an	O
activation	O
function	O
satisfying	O
Assumption	O
[	O
reference	O
]	O
.	O

For	O
,	O
since	O
,	O
we	O
have	O
.	O

Furthermore	O
:	O
If	O
,	O
for	O
any	O
,	O
.	O

If	O
,	O
the	O
problem	O
has	O
unique	O
solution	O
for	O
any	O
as	O
long	O
as	O
is	O
within	O
the	O
output	O
range	O
of	O
.	O

If	O
,	O
let	O
and	O
be	O
two	O
basis	O
matrices	O
of	O
such	O
that	O
and	O
any	O
vector	O
in	O
can	O
be	O
represented	O
as	O
,	O
where	O
,	O
and	O
is	O
the	O
nullity	O
of	O
.	O

Let	O
the	O
projected	O
support	O
be	O
.	O

Thus	O
,	O
,	O
there	O
exists	O
such	O
that	O
with	O
.	O

Consider	O
the	O
Jacobian	O
:	O
where	O
and	O
is	O
the	O
input	O
to	O
activation	O
,	O
or	O
pre	O
-	O
activations	O
.	O

Since	O
is	O
continuous	O
and	O
compact	O
,	O
it	O
has	O
infinite	O
number	O
of	O
boundary	O
points	O
for	O
.	O

Consider	O
one	O
boundary	O
point	O
and	O
its	O
normal	O
line	O
.	O

Let	O
be	O
a	O
small	O
scalar	O
such	O
that	O
and	O
.	O

For	O
linear	O
activation	O
,	O
and	O
is	O
constant	O
.	O

Then	O
remains	O
for	O
,	O
i.e.	O
,	O
there	O
exists	O
such	O
that	O
.	O

For	O
nonlinear	Task
activations	Task
,	O
assume	O
has	O
discontinuities	O
.	O

Since	O
has	O
unique	O
solution	O
for	O
any	O
vector	O
,	O
the	O
boundary	O
points	O
can	O
not	O
yield	O
pre	O
-	O
activations	O
that	O
all	O
lie	O
on	O
the	O
discontinuities	O
in	O
any	O
of	O
the	O
directions	O
.	O

Though	O
we	O
might	O
need	O
to	O
sample	O
points	O
in	O
the	O
worst	O
case	O
to	O
find	O
an	O
exception	O
,	O
there	O
are	O
infinite	O
number	O
of	O
exceptions	O
.	O

Let	O
be	O
a	O
sample	O
where	O
does	O
not	O
lie	O
on	O
the	O
discontinuities	O
in	O
any	O
direction	O
.	O

Because	O
is	O
continuous	O
,	O
remains	O
for	O
,	O
i.e.	O
,	O
there	O
exists	O
such	O
that	O
.	O

In	O
conclusion	O
,	O
we	O
can	O
always	O
find	O
such	O
that	O
and	O
.	O

∎	O
Proposition	O
[	O
reference	O
]	O
indicates	O
that	O
if	O
for	O
,	O
can	O
not	O
discriminate	O
against	O
fake	O
samples	O
with	O
distortions	O
to	O
the	O
original	O
data	O
.	O

In	O
contrast	O
,	O
Assumption	O
[	O
reference	O
]	O
and	O
Lemma	O
[	O
reference	O
]	O
guarantee	O
that	O
,	O
at	O
equilibrium	O
,	O
the	O
discriminator	Method
trained	O
using	O
MMD	Method
loss	O
function	O
is	O
effective	O
against	O
such	O
fake	O
samples	O
given	O
a	O
large	O
number	O
of	O
i.i.d	O
.	O

test	O
samples	O
(	O
)	O
.	O

section	O
:	O
Supplementary	O
Methodology	O
subsection	O
:	O
Representative	O
loss	O
functions	O
in	O
literature	O
Several	O
loss	Method
functions	Method
have	O
been	O
proposed	O
to	O
quantify	O
the	O
difference	O
between	O
real	O
and	O
generated	O
sample	O
scores	O
,	O
including	O
:	O
(	O
assume	O
linear	O
activation	O
is	O
used	O
at	O
the	O
last	O
layer	O
of	O
)	O
The	O
Minimax	Metric
loss	Metric
(	O
)	O
:	O
and	O
,	O
which	O
can	O
be	O
derived	O
from	O
the	O
JensenâShannon	Metric
(	O
JS	Metric
)	O
divergence	O
between	O
and	O
the	O
model	O
distribution	O
.	O

The	O
non	O
-	O
saturating	O
loss	O
(	O
)	O
,	O
which	O
is	O
a	O
variant	O
of	O
the	O
minimax	O
loss	O
with	O
the	O
same	O
and	O
.	O

The	O
Hinge	O
loss	O
(	O
)	O
:	O
,	O
,	O
which	O
is	O
notably	O
known	O
for	O
usage	O
in	O
support	Task
vector	Task
machines	Task
and	O
is	O
related	O
to	O
the	O
total	Metric
variation	Metric
(	O
TV	Metric
)	O
distance	O
(	O
)	O
.	O

The	O
Wasserstein	O
loss	O
(	O
)	O
,	O
which	O
is	O
derived	O
from	O
the	O
Wasserstein	O
distance	O
between	O
and	O
:	O
,	O
,	O
where	O
is	O
subject	O
to	O
some	O
Lipschitz	O
constraint	O
.	O

The	O
maximum	Method
mean	Method
discrepancy	Method
(	O
MMD	Method
)	O
(	O
)	O
,	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Network	Method
Architecture	Method
For	O
unsupervised	O
image	Task
generation	Task
tasks	Task
on	O
CIFAR	Material
-	Material
10	Material
and	O
STL	O
-	O
10	O
datasets	O
,	O
the	O
DCGAN	Method
architecture	O
from	O
was	O
used	O
.	O

For	O
CelebA	O
and	O
LSUN	O
bedroom	O
datasets	O
,	O
we	O
added	O
more	O
layers	O
to	O
the	O
generator	Method
and	Method
discriminator	Method
accordingly	O
.	O

See	O
Table	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
details	O
.	O

0.45	O
0.45	O
0.45	O
0.45	O
section	O
:	O
Power	Method
Iteration	Method
for	O
Convolution	Method
Operation	Method
This	O
section	O
introduces	O
the	O
power	Method
iteration	Method
for	O
convolution	Method
operation	Method
(	O
PICO	Method
)	O
method	O
to	O
estimate	O
the	O
spectral	O
norm	O
of	O
a	O
convolution	Method
kernel	Method
,	O
and	O
compare	O
PICO	Method
with	O
the	O
power	Method
iteration	Method
for	Method
matrix	Method
(	O
PIM	Method
)	O
method	O
used	O
in	O
.	O

subsection	O
:	O
Method	O
Formation	O
For	O
a	O
weight	O
matrix	O
,	O
the	O
spectral	O
norm	O
is	O
defined	O
as	O
.	O

The	O
PIM	Method
is	O
used	O
to	O
estimate	O
(	O
)	O
,	O
which	O
iterates	O
between	O
two	O
steps	O
:	O
Update	O
;	O
Update	O
.	O

The	O
convolutional	Method
kernel	Method
is	O
a	O
tensor	O
of	O
shape	O
with	O
the	O
receptive	O
field	O
size	O
and	O
the	O
number	O
of	O
input	O
/	O
output	O
channels	O
.	O

To	O
estimate	O
,	O
reshaped	O
it	O
into	O
a	O
matrix	O
of	O
shape	O
and	O
estimated	O
.	O

We	O
propose	O
a	O
simple	O
method	O
to	O
calculate	O
directly	O
based	O
on	O
the	O
fact	O
that	O
convolution	Method
operation	Method
is	O
linear	O
.	O

For	O
any	O
linear	Method
map	Method
,	O
there	O
exists	O
matrix	O
such	O
that	O
can	O
be	O
represented	O
as	O
.	O

Thus	O
,	O
we	O
may	O
simply	O
substitute	O
in	O
the	O
PIM	Method
method	O
to	O
estimate	O
the	O
spectral	O
norm	O
of	O
any	O
linear	O
operation	O
.	O

In	O
the	O
case	O
of	O
convolution	Method
operation	Method
,	O
there	O
exist	O
doubly	Method
block	Method
circulant	Method
matrix	Method
such	O
that	O
.	O

Consider	O
which	O
is	O
essentially	O
the	O
transpose	Method
convolution	Method
of	Method
on	Method
(	Method
)	Method
.	O

Thus	O
,	O
similar	O
to	O
PIM	Method
,	O
PICO	Method
iterates	O
between	O
the	O
following	O
two	O
steps	O
:	O
Update	O
;	O
Do	O
transpose	Method
convolution	Method
of	O
on	O
to	O
get	O
;	O
update	O
.	O

Similar	O
approaches	O
have	O
been	O
proposed	O
in	O
and	O
from	O
different	O
angles	O
,	O
which	O
we	O
were	O
not	O
aware	O
during	O
this	O
study	O
.	O

In	O
addition	O
,	O
proposes	O
to	O
compute	O
the	O
exact	O
singular	O
values	O
of	O
convolution	O
kernels	O
using	O
FFT	Method
and	O
SVD	Method
.	O

In	O
spectral	Task
normalization	Task
,	O
only	O
the	O
first	O
singular	O
value	O
is	O
concerned	O
,	O
making	O
the	O
power	O
iteration	O
methods	O
PIM	Method
and	O
PICO	Method
more	O
efficient	O
than	O
FFT	Method
and	O
thus	O
preferred	O
in	O
our	O
study	O
.	O

However	O
,	O
we	O
believe	O
the	O
exact	O
method	O
FFT	Method
+	O
SVD	Method
(	O
)	O
may	O
eventually	O
inspire	O
more	O
rigorous	O
regularization	Method
methods	Method
for	O
GAN	Method
.	O

The	O
proposed	O
PICO	Method
method	O
estimates	O
the	O
real	O
spectral	O
norm	O
of	O
a	O
convolution	Method
kernel	Method
at	O
each	O
layer	O
,	O
thus	O
enforces	O
an	O
upper	O
bound	O
on	O
the	O
Lipschitz	O
constant	O
of	O
the	O
discriminator	Method
.	O

Denote	O
the	O
upper	O
bound	O
as	O
.	O

In	O
this	O
study	O
,	O
Leaky	Method
ReLU	Method
(	O
LReLU	Method
)	Method
was	O
used	O
at	O
each	O
layer	O
of	O
,	O
thus	O
(	O
)	O
.	O

In	O
practice	O
,	O
however	O
,	O
PICO	Method
would	O
often	O
cause	O
the	O
norm	O
of	O
the	O
signal	O
passing	O
through	O
to	O
decrease	O
to	O
zero	O
,	O
because	O
at	O
each	O
layer	O
,	O
the	O
signal	O
hardly	O
coincides	O
with	O
the	O
first	O
singular	O
-	O
vector	O
of	O
the	O
convolution	O
kernel	O
;	O
and	O
the	O
activation	Method
function	Method
LReLU	Method
often	O
reduces	O
the	O
norm	O
of	O
the	O
signal	O
.	O

Consequently	O
,	O
the	O
discriminator	O
outputs	O
tend	O
to	O
be	O
similar	O
for	O
all	O
the	O
inputs	O
.	O

To	O
compensate	O
the	O
loss	O
of	O
norm	O
at	O
each	O
layer	O
,	O
the	O
signal	O
is	O
multiplied	O
by	O
a	O
constant	O
after	O
each	O
spectral	Method
normalization	Method
.	O

This	O
essentially	O
enlarges	O
by	O
where	O
is	O
the	O
number	O
of	O
layers	O
in	O
the	O
DCGAN	Method
discriminator	O
.	O

For	O
all	O
experiments	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
fixed	O
as	O
all	O
loss	Method
functions	Method
performed	O
relatively	O
well	O
empirically	O
.	O

In	O
Appendix	O
Section	O
[	O
reference	O
]	O
,	O
we	O
tested	O
the	O
effects	O
of	O
coefficient	O
on	O
the	O
performance	O
of	O
several	O
loss	Method
functions	Method
.	O

subsection	O
:	O
Comparison	O
to	O
PIM	Method
PIM	Method
(	O
)	O
also	O
enforces	O
an	O
upper	O
bound	O
on	O
the	O
Lipschitz	O
constant	O
of	O
the	O
discriminator	Method
.	O

Consider	O
a	O
convolution	Method
kernel	Method
with	O
receptive	O
field	O
size	O
and	O
stride	O
.	O

Let	O
and	O
be	O
the	O
spectral	O
norm	O
estimated	O
by	O
PICO	Method
and	O
PIM	Method
respectively	O
.	O

We	O
empirically	O
found	O
that	O
varies	O
in	O
the	O
range	O
,	O
depending	O
on	O
the	O
kernel	O
.	O

For	O
a	O
typical	O
kernel	O
of	O
size	O
and	O
stride	O
,	O
may	O
vary	O
from	O
to	O
.	O

Thus	O
,	O
is	O
indefinite	O
and	O
may	O
vary	O
during	O
training	O
.	O

In	O
deep	Method
convolutional	Method
networks	Method
,	O
PIM	Method
could	O
potentially	O
result	O
in	O
a	O
very	O
loose	O
constraint	O
on	O
the	O
Lipschitz	O
constant	O
of	O
the	O
network	O
.	O

In	O
Appendix	O
Section	O
[	O
reference	O
]	O
,	O
we	O
experimentally	O
compare	O
the	O
performance	O
of	O
PICO	Method
and	O
PIM	Method
with	O
several	O
loss	Method
functions	Method
.	O

subsection	O
:	O
Experiments	O
[	O
t	O
]	O
ﬁ	O
i	O
[	O
t	O
]	O
ﬁ	O
i	O
[	O
t	O
]	O
ﬁ	O
i	O
[	O
t	O
]	O
ﬁ	O
i	O
In	O
this	O
section	O
,	O
we	O
empirically	O
evaluate	O
the	O
effects	O
of	O
coefficient	O
on	O
the	O
performance	O
of	O
PICO	Method
and	O
compare	O
PICO	Method
against	O
PIM	Method
using	O
several	O
loss	Method
functions	Method
.	O

Experiment	O
setup	O
:	O
We	O
used	O
a	O
similar	O
setup	O
as	O
Section	O
[	O
reference	O
]	O
with	O
the	O
following	O
adjustments	O
.	O

Four	O
loss	O
functions	O
were	O
tested	O
:	O
hinge	Method
,	O
MMD	Method
-	Method
rbf	Method
,	O
MMD	Method
-	Method
rep	Method
and	O
MMD	Method
-	Method
rep	Method
-	O
b	O
.	O

Either	O
PICO	Method
or	O
PIM	Method
was	O
used	O
at	O
each	O
layer	O
of	O
the	O
discriminator	Method
.	O

For	O
PICO	Method
,	O
five	O
coefficients	O
were	O
tested	O
:	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
and	O
256	O
(	O
note	O
this	O
is	O
the	O
overall	O
coefficient	O
for	O
layers	O
;	O
for	O
CIFAR	Material
-	Material
10	Material
and	O
STL	O
-	O
10	O
;	O
for	O
CelebA	O
and	O
LSUN	O
-	O
bedroom	O
;	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

FID	Metric
was	O
used	O
to	O
evaluate	O
the	O
performance	O
of	O
each	O
combination	O
of	O
loss	Method
function	Method
and	O
power	Method
iteration	Method
method	Method
,	O
e.g.	O
,	O
hinge	Method
+	O
PICO	Method
with	O
.	O

Results	O
:	O
For	O
each	O
combination	O
of	O
loss	O
function	O
and	O
power	Method
iteration	Method
method	Method
,	O
the	O
distribution	O
of	O
FID	Metric
scores	Metric
over	O
16	O
learning	O
rate	O
combinations	O
is	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

We	O
separated	O
well	O
-	O
performed	O
learning	O
rate	O
combinations	O
from	O
diverged	O
or	O
poorly	O
-	O
performed	O
ones	O
using	O
a	O
threshold	O
as	O
the	O
diverged	O
cases	O
often	O
had	O
non	O
-	O
meaningful	O
FID	Metric
scores	Metric
.	O

The	O
boxplot	O
shows	O
the	O
distribution	O
of	O
FID	Metric
scores	Metric
for	O
good	O
-	O
performed	O
cases	O
while	O
the	O
number	O
of	O
diverged	O
or	O
poorly	O
-	O
performed	O
cases	O
was	O
shown	O
above	O
each	O
box	O
if	O
it	O
is	O
non	O
-	O
zero	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
that	O
:	O
When	O
PICO	Method
was	O
used	O
,	O
the	O
hinge	Method
,	O
MMD	Method
-	Method
rbf	Method
and	O
MMD	Method
-	Method
rep	Method
methods	O
were	O
sensitive	O
to	O
the	O
choices	O
of	O
while	O
MMD	Method
-	Method
rep	Method
-	Method
b	Method
was	O
robust	O
.	O

For	O
hinge	Method
and	O
MMD	Method
-	Method
rbf	Method
,	O
higher	O
may	O
result	O
in	O
better	O
FID	Metric
scores	Metric
and	O
less	O
diverged	O
cases	O
over	O
16	O
learning	O
rate	O
combinations	O
.	O

For	O
MMD	Method
-	O
rep	O
,	O
higher	O
may	O
cause	O
more	O
diverged	O
cases	O
;	O
however	O
,	O
the	O
best	O
FID	Metric
scores	Metric
were	O
often	O
achieved	O
with	O
or	O
.	O

For	O
CIFAR	Material
-	Material
10	Material
,	O
STL	O
-	O
10	O
and	O
CelebA	O
datasets	O
,	O
PIM	Method
performed	O
comparable	O
to	O
PICO	Method
with	O
or	O
on	O
four	O
loss	O
functions	O
.	O

For	O
LSUN	O
bedroom	O
dataset	O
,	O
it	O
is	O
likely	O
that	O
the	O
performance	O
of	O
PIM	Method
corresponded	O
to	O
that	O
of	O
PICO	Method
with	O
.	O

This	O
implies	O
that	O
PIM	Method
may	O
result	O
in	O
a	O
relatively	O
loose	O
Lipschitz	O
constraint	O
on	O
deep	Method
convolutional	Method
networks	Method
.	O

MMD	Method
-	O
rep	O
-	O
b	O
performed	O
generally	O
better	O
than	O
hinge	Method
and	O
MMD	Method
-	O
rbf	O
with	O
tested	O
power	Method
iteration	Method
methods	Method
and	O
hyper	O
-	O
parameter	O
configurations	O
.	O

Using	O
PICO	Method
,	O
MMD	Method
-	Method
rep	Method
also	O
achieved	O
generally	O
better	O
FID	Metric
scores	Metric
than	O
hinge	Method
and	O
MMD	Method
-	Method
rbf	Method
.	O

This	O
implies	O
that	O
,	O
given	O
a	O
limited	O
computational	O
budget	O
,	O
the	O
proposed	O
repulsive	Metric
loss	Metric
may	O
be	O
a	O
better	O
choice	O
than	O
the	O
hinge	Method
and	O
MMD	Method
loss	O
for	O
the	O
discriminator	Method
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
best	O
FID	Metric
scores	Metric
obtained	O
by	O
PICO	Method
and	O
PIM	Method
where	O
was	O
fixed	O
at	O
for	O
hinge	Method
and	O
MMD	Method
-	Method
rbf	Method
,	O
and	O
for	O
MMD	Method
-	O
rep	O
and	O
MMD	Method
-	O
rep	O
-	O
b	O
.	O

For	O
hinge	Method
and	O
MMD	Method
-	O
rbf	O
,	O
PICO	Method
performed	O
significantly	O
better	O
than	O
PIM	Method
on	O
the	O
LSUN	O
-	O
bedroom	O
dataset	O
and	O
comparably	O
on	O
the	O
rest	O
datasets	O
.	O

For	O
MMD	Method
-	O
rep	O
and	O
MMD	Method
-	O
rep	O
-	O
b	O
,	O
PICO	Method
achieved	O
consistently	O
better	O
FID	Metric
scores	Metric
than	O
PIM	Method
.	O

However	O
,	O
compared	O
to	O
PIM	Method
,	O
PICO	Method
has	O
a	O
higher	O
computational	Metric
cost	Metric
which	O
roughly	O
equals	O
the	O
additional	O
cost	O
incurred	O
by	O
increasing	O
the	O
batch	O
size	O
by	O
two	O
(	O
)	O
.	O

This	O
may	O
be	O
problematic	O
when	O
a	O
small	O
batch	O
has	O
to	O
be	O
used	O
due	O
to	O
memory	O
constraints	O
,	O
e.g.	O
,	O
when	O
handling	O
high	Method
resolution	Method
images	Method
on	O
a	O
single	O
GPU	O
.	O

Thus	O
,	O
we	O
recommend	O
using	O
PICO	Method
when	O
the	O
computational	Metric
cost	Metric
is	O
less	O
of	O
a	O
concern	O
.	O

section	O
:	O
Supplementary	O
Experiments	O
subsection	O
:	O
Lipschitz	O
Constraint	O
via	O
Gradient	Method
Penalty	Method
All	O
methods	O
used	O
the	O
same	O
DCGAN	Method
architecture	O
.	O

Results	O
from	O
Table	O
1	O
.	O

Gradient	Method
penalty	Method
has	O
been	O
widely	O
used	O
to	O
impose	O
the	O
Lipschitz	O
constraint	O
on	O
the	O
discriminator	O
arguably	O
since	O
Wasserstein	O
GAN	Method
(	O
)	O
.	O

This	O
section	O
explores	O
whether	O
the	O
proposed	O
repulsive	Metric
loss	Metric
can	O
be	O
applied	O
with	O
gradient	Method
penalty	Method
.	O

Several	O
gradient	Method
penalty	Method
methods	Method
have	O
been	O
proposed	O
for	O
MMD	Method
-	Method
GAN	Method
.	O

penalized	O
the	O
gradient	O
norm	O
of	O
witness	O
function	O
w.r.t	O
.	O

the	O
interpolated	O
sample	O
to	O
one	O
,	O
where	O
.	O

More	O
recently	O
,	O
proposed	O
to	O
impose	O
the	O
Lipschitz	O
constraint	O
on	O
the	O
mapping	O
directly	O
and	O
derived	O
the	O
Scaled	Method
MMD	Method
(	O
SMMD	Method
)	O
as	O
,	O
where	O
the	O
scale	O
incorporates	O
gradient	O
and	O
smooth	O
penalties	O
.	O

Using	O
the	O
Gaussian	Method
kernel	Method
and	O
measure	O
leads	O
to	O
the	O
discriminator	O
loss	O
:	O
We	O
apply	O
the	O
same	O
formation	O
of	O
gradient	O
penalty	O
to	O
the	O
repulsive	Metric
loss	Metric
:	O
where	O
the	O
numerator	O
so	O
that	O
the	O
discriminator	O
will	O
always	O
attempt	O
to	O
minimize	O
both	O
and	O
the	O
Frobenius	O
norm	O
of	O
gradients	O
w.r.t	O
.	O

real	O
samples	O
.	O

Meanwhile	O
,	O
the	O
generator	Method
is	O
trained	O
using	O
the	O
MMD	Method
loss	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

Experiment	O
setup	O
:	O
The	O
gradient	O
-	O
penalized	O
repulsive	Metric
loss	Metric
(	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
referred	O
to	O
as	O
MMD	Method
-	Method
rep	Method
-	Method
gp	Method
)	O
was	O
evaluated	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

We	O
found	O
in	O
too	O
restrictive	O
and	O
used	O
instead	O
.	O

Same	O
as	O
,	O
the	O
output	O
dimension	O
of	O
discriminator	Method
was	O
set	O
to	O
one	O
.	O

Since	O
we	O
entrusted	O
the	O
Lipschitz	O
constraint	O
to	O
the	O
gradient	O
penalty	O
,	O
spectral	Method
normalization	Method
was	O
not	O
used	O
.	O

The	O
rest	O
experiment	O
setup	O
can	O
be	O
found	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Results	O
:	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
proposed	O
repulsive	Metric
loss	Metric
can	O
be	O
used	O
with	O
gradient	O
penalty	O
to	O
achieve	O
reasonable	O
results	O
on	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

For	O
comparison	O
,	O
we	O
cited	O
the	O
Inception	Metric
score	Metric
and	O
FID	Metric
for	O
Scaled	O
MMD	Method
-	O
GAN	Method
(	O
SMMDGAN	Method
)	O
and	O
Scaled	O
MMD	Method
-	O
GAN	Method
with	O
spectral	Method
normalization	Method
(	O
SN	Method
-	Method
SMMDGAN	Method
)	O
from	O
.	O

Note	O
that	O
SMMDGAN	Method
and	O
SN	Method
-	Method
SMMDGAN	Method
used	O
the	O
same	O
DCGAN	Method
architecture	O
as	O
MMD	Method
-	Method
rep	Method
-	Method
gp	Method
,	O
but	O
were	O
trained	O
for	O
150k	O
generator	O
updates	O
and	O
750k	O
discriminator	Method
updates	Method
,	O
much	O
more	O
than	O
that	O
of	O
MMD	Method
-	Method
rep	Method
-	Method
gp	Method
(	O
100k	O
for	O
both	O
and	O
)	O
.	O

Thus	O
,	O
the	O
repulsive	Metric
loss	Metric
significantly	O
improved	O
over	O
the	O
attractive	O
MMD	Method
loss	O
for	O
discriminator	Method
.	O

subsection	O
:	O
Output	O
Dimension	O
of	O
Discriminator	Method
In	O
this	O
section	O
,	O
we	O
investigate	O
the	O
impact	O
of	O
the	O
output	O
dimension	O
of	O
discriminator	Method
on	O
the	O
performance	O
of	O
repulsive	Metric
loss	Metric
.	O

Experiment	O
setup	O
:	O
We	O
used	O
a	O
similar	O
setup	O
as	O
Section	O
[	O
reference	O
]	O
with	O
the	O
following	O
adjustments	O
.	O

The	O
repulsive	Metric
loss	Metric
was	O
tested	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
with	O
a	O
variety	O
of	O
discriminator	O
output	O
dimensions	O
:	O
.	O

Spectral	Method
normalization	Method
was	O
applied	O
to	O
discriminator	Method
with	O
the	O
proposed	O
PICO	Method
method	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
and	O
the	O
coefficients	O
selected	O
from	O
.	O

Results	O
:	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
using	O
more	O
than	O
one	O
output	O
neuron	O
in	O
the	O
discriminator	Method
significantly	O
improved	O
the	O
performance	O
of	O
repulsive	Metric
loss	Metric
over	O
the	O
one	Method
-	Method
neuron	Method
case	Method
on	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

The	O
reason	O
may	O
be	O
that	O
using	O
insufficient	O
output	O
neurons	O
makes	O
it	O
harder	O
for	O
the	O
discriminator	Method
to	O
learn	O
an	O
injective	Method
and	O
discriminative	Method
representation	Method
of	O
the	O
data	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

However	O
,	O
the	O
performance	O
gain	O
diminished	O
when	O
more	O
neurons	O
were	O
used	O
,	O
perhaps	O
because	O
it	O
becomes	O
easier	O
for	O
to	O
surpass	O
the	O
generator	Method
and	O
trap	O
it	O
around	O
saddle	O
solutions	O
.	O

The	O
computation	Metric
cost	Metric
also	O
slightly	O
increased	O
due	O
to	O
more	O
output	O
neurons	O
.	O

subsection	O
:	O
Samples	O
of	O
Unsupervised	Task
Image	Task
Generation	Task
Generated	O
samples	O
on	O
CelebA	O
dataset	O
are	O
given	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
and	O
LSUN	O
bedrooms	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
[	O
t	O
]	O
0.45	O
