document	O
:	O
SINGLE	Task
IMAGE	Task
SUPER	Task
-	Task
RESOLUTION	Task
WITH	O
DILATED	Method
CONVOLUTION	Method
BASED	Method
MULTI	Method
-	Method
SCALE	Method
INFORMATION	Method
LEARNING	Method
INCEPTION	Method
MODULE	Method
Traditional	O
works	O
have	O
shown	O
that	O
patches	O
in	O
a	O
natural	O
image	O
tend	O
to	O
redundantly	O
recur	O
many	O
times	O
inside	O
the	O
image	O
,	O
both	O
within	O
the	O
same	O
scale	O
,	O
as	O
well	O
as	O
across	O
different	O
scales	O
.	O

Make	O
full	O
use	O
of	O
these	O
multi	O
-	O
scale	O
information	O
can	O
improve	O
the	O
image	Task
restoration	Task
performance	O
.	O

However	O
,	O
the	O
current	O
proposed	O
deep	Method
learning	Method
based	Method
restoration	Method
methods	Method
do	O
not	O
take	O
the	O
multi	O
-	O
scale	O
information	O
into	O
account	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
to	O
learn	O
multi	O
-	O
scale	O
information	O
and	O
design	O
a	O
deep	Method
network	Method
for	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
.	O

Different	O
dilated	Method
convolution	Method
learns	O
different	O
scale	O
feature	O
,	O
then	O
the	O
inception	Method
module	Method
concatenates	O
all	O
these	O
features	O
to	O
fuse	O
multi	O
-	O
scale	O
information	O
.	O

In	O
order	O
to	O
increase	O
the	O
reception	O
field	O
of	O
our	O
network	O
to	O
catch	O
more	O
contextual	O
information	O
,	O
we	O
cascade	O
multiple	O
inception	Method
modules	Method
to	O
constitute	O
a	O
deep	Method
network	Method
to	O
conduct	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
.	O

With	O
the	O
novel	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
,	O
the	O
proposed	O
end	O
-	O
to	O
-	O
end	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
network	O
can	O
take	O
advantage	O
of	O
multi	O
-	O
scale	O
information	O
to	O
improve	O
image	Task
super	Task
-	Task
resolution	Task
performance	O
.	O

Experimental	O
results	O
show	O
that	O
our	O
proposed	O
method	O
outperforms	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
methods	O
.	O

WuzhenShi	O
,	O
FengJiang	O
,	O
DebinZhao	O
SchoolofComputerScienceandTechnology	O
,	O
HarbinInstituteofTechnology	O
,	O
Harbin150001	O
,	O
China	O
Image	Task
super	Task
-	Task
resolution	Task
,	O
convolutional	Method
neural	Method
network	Method
,	O
multi	Task
-	Task
scale	Task
information	Task
,	O
dilated	Method
convolution	Method
,	O
inception	Method
module	Method
section	O
:	O
Introduction	O
(	O
a	O
)	O
Dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
(	O
b	O
)	O
Original	O
GoogLeNet	Method
inception	Method
module	Method
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
(	O
SR	Task
)	O
,	O
which	O
aims	O
at	O
recovering	O
a	O
high	Task
-	Task
resolution	Task
(	O
HR	Task
)	O
image	O
from	O
a	O
given	O
low	O
-	O
resolution	O
(	O
LR	O
)	O
one	O
.	O

It	O
is	O
always	O
the	O
research	O
emphasis	O
because	O
of	O
the	O
requirement	O
of	O
higher	Task
definition	Task
video	Task
displaying	Task
,	O
such	O
as	O
the	O
new	O
generation	O
of	O
Ultra	Method
High	Method
Definition	Method
(	O
UHD	O
)	O
TVs	O
.	O

However	O
,	O
most	O
video	O
content	O
would	O
not	O
at	O
sufficiently	O
high	O
resolution	O
for	O
UHD	Method
TVs	Method
.	O

As	O
a	O
result	O
,	O
we	O
have	O
to	O
develop	O
efficient	O
SR	Task
algorithms	O
to	O
generate	O
UHD	O
content	O
from	O
lower	O
resolutions	O
.	O

Traditional	O
single	O
image	O
SR	Task
methods	O
always	O
try	O
to	O
find	O
a	O
new	O
image	O
prior	O
or	O
propose	O
a	O
new	O
way	O
to	O
use	O
the	O
existing	O
image	O
priors	O
.	O

A	O
lot	O
of	O
image	O
prior	O
information	O
have	O
been	O
explored	O
in	O
the	O
image	Task
restoration	Task
literature	Task
,	O
such	O
as	O
local	O
smooth	O
,	O
non	O
-	O
local	O
self	O
-	O
similarity	O
and	O
image	O
sparsity	O
.	O

Based	O
on	O
the	O
assumption	O
that	O
low	O
and	O
high	O
resolution	O
images	O
have	O
the	O
same	O
sparse	Method
representation	Method
,	O
Yang	O
et	O
al	O
.	O

use	O
two	O
coupled	Method
dictionaries	Method
to	O
learn	O
a	O
nonlinear	Method
mapping	Method
between	O
the	O
LR	O
and	O
the	O
HR	Task
images	O
.	O

In	O
,	O
Glasner	O
et	O
al	O
.	O

begin	O
to	O
use	O
the	O
image	O
multi	O
-	O
scale	O
information	O
for	O
single	Task
image	Task
SR	Task
and	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

Recently	O
,	O
due	O
to	O
the	O
availability	O
of	O
much	O
larger	O
training	O
dataset	O
and	O
the	O
powerful	O
GPU	Method
implementation	Method
,	O
deep	Method
learning	Method
based	Method
methods	Method
achieve	O
great	O
success	O
in	O
many	O
fields	O
,	O
including	O
both	O
high	Task
level	Task
and	Task
low	Task
level	Task
computer	Task
vision	Task
problems	Task
.	O

Look	O
through	O
the	O
literature	O
,	O
most	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
image	O
SR	Task
methods	O
are	O
based	O
on	O
deep	Method
learning	Method
.	O

The	O
pioneering	O
SR	Task
method	O
is	O
SRCNN	Method
proposed	O
by	O
Dong	O
et	O
al	O
.	O

.	O

They	O
establish	O
the	O
relationship	O
between	O
the	O
traditional	O
sparse	O
-	O
coding	O
based	O
SR	Task
methods	O
and	O
their	O
network	Method
structure	Method
and	O
demonstrate	O
that	O
a	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
can	O
learn	O
a	O
mapping	O
from	O
low	O
resolution	O
image	O
to	O
high	O
resolution	O
one	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O

Dong	O
et	O
al	O
.	O

successfully	O
expand	O
SRCNN	Method
for	O
compression	Task
artifacts	Task
reduction	Task
by	O
introducing	O
a	O
feature	Method
enhancement	Method
layer	Method
.	O

Soon	O
after	O
,	O
they	O
proposed	O
a	O
fast	O
SRCNN	Method
method	Method
,	O
which	O
directly	O
maps	O
the	O
original	O
low	O
-	O
resolution	O
image	O
to	O
the	O
high	Task
-	Task
resolution	Task
one	O
.	O

Different	O
from	O
,	O
some	O
works	O
try	O
to	O
learn	O
image	O
high	O
frequency	O
details	O
instead	O
of	O
the	O
undegraded	O
image	O
.	O

In	O
,	O
Kim	O
et	O
al	O
.	O

cascade	O
many	O
convolutional	Method
layers	Method
to	O
form	O
a	O
very	O
deep	Method
network	Method
to	O
learn	O
image	Task
residual	Task
.	O

Investigating	O
an	O
effective	O
way	O
to	O
use	O
multi	O
-	O
scale	O
information	O
is	O
also	O
important	O
.	O

The	O
degraded	O
image	O
can	O
be	O
successful	O
recovered	O
is	O
mainly	O
based	O
on	O
the	O
assumption	O
that	O
patches	O
in	O
a	O
natural	O
image	O
tend	O
to	O
redundantly	O
recur	O
many	O
times	O
inside	O
the	O
image	O
.	O

However	O
,	O
it	O
is	O
not	O
only	O
exist	O
in	O
the	O
same	O
scale	O
but	O
also	O
across	O
different	O
scales	O
.	O

It	O
has	O
been	O
demonstrated	O
that	O
make	O
full	O
use	O
of	O
multi	O
-	O
scale	O
information	O
can	O
improve	O
the	O
restoration	Task
result	O
in	O
traditional	O
methods	O
.	O

However	O
,	O
the	O
multi	O
-	O
scale	O
information	O
has	O
been	O
little	O
investigated	O
in	O
deep	Method
learning	Method
methods	Method
.	O

In	O
,	O
Kim	O
et	O
al	O
.	O

try	O
to	O
train	O
a	O
multi	Method
-	Method
scale	Method
model	Method
for	O
different	O
magnification	O
SR	Task
.	O

It	O
is	O
a	O
very	O
rough	O
tactics	O
to	O
exploit	O
the	O
scale	O
information	O
since	O
they	O
just	O
put	O
different	O
scale	O
image	O
as	O
input	O
for	O
training	O
.	O

Its	O
success	O
can	O
be	O
attribute	O
to	O
the	O
powerful	O
learning	Metric
ability	Metric
of	O
CNN	Method
instead	O
of	O
the	O
multi	O
-	O
scale	O
information	O
being	O
considered	O
in	O
the	O
network	O
structure	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
to	O
learn	O
multi	O
-	O
scale	O
information	O
and	O
design	O
a	O
deep	Method
network	Method
for	O
single	Task
image	Task
SR	Task
.	O

Fig	O
.	O

[	O
reference	O
]	O
makes	O
a	O
comparison	O
between	O
the	O
proposed	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
and	O
the	O
original	O
GoogLeNet	Method
inception	Method
module	Method
.	O

Our	O
proposed	O
new	O
inception	Method
module	Method
contains	O
multiple	O
different	O
scale	Method
dilated	Method
convolution	Method
that	O
makes	O
it	O
can	O
learn	O
multi	O
-	O
scale	O
image	O
information	O
.	O

Furthermore	O
,	O
we	O
cascade	O
multiple	O
dilated	Method
convolution	Method
based	Method
inception	Method
modules	Method
to	O
constitute	O
a	O
deep	Method
network	Method
for	O
single	Task
image	Task
SR	Task
.	O

In	O
short	O
,	O
the	O
contributions	O
of	O
this	O
work	O
are	O
mainly	O
in	O
three	O
aspects	O
:	O
1	O
)	O
we	O
proposed	O
a	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
,	O
which	O
can	O
learn	O
multi	O
-	O
scale	O
information	O
with	O
only	O
single	O
scale	O
image	O
input	O
;	O
2	O
)	O
we	O
design	O
a	O
novel	O
deep	Method
network	Method
with	O
the	O
proposed	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
for	O
single	Task
image	Task
SR	Task
.	O

3	O
)	O
experimental	O
results	O
show	O
that	O
our	O
proposed	O
new	O
method	O
outperforms	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

section	O
:	O
DILATED	Method
CONVOLUTION	Method
BASED	Method
INCEPTION	Method
MODULE	Method
Dilated	Method
Convolution	Method
:	O
It	O
has	O
been	O
referred	O
to	O
in	O
the	O
past	O
as	O
âconvolution	O
with	O
a	O
dilated	O
filterâ	O
.	O

In	O
,	O
Yu	O
et	O
al	O
.	O

call	O
it	O
âdilated	O
convolutionâ	O
instead	O
of	O
âconvolution	O
with	O
a	O
dilated	O
filterâ	O
to	O
clarify	O
that	O
no	O
âdilated	O
filterâ	O
is	O
constructed	O
or	O
represented	O
.	O

It	O
can	O
be	O
formulated	O
as	O
where	O
is	O
called	O
as	O
a	O
dilated	Method
convolution	Method
or	O
an	O
-	Method
dilated	Method
convolution	Method
,	O
and	O
is	O
a	O
discrete	O
function	O
and	O
a	O
discrete	Method
filter	Method
of	Method
size	Method
,	O
respectively	O
.	O

Three	O
dilated	Method
convolutions	Method
have	O
been	O
shown	O
in	O
the	O
middle	O
part	O
of	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
where	O
is	O
a	O
filter	Method
and	O
the	O
kernel	O
dilation	O
factors	O
are	O
1	O
,	O
2	O
and	O
3	O
,	O
respectively	O
.	O

It	O
shows	O
that	O
filters	O
are	O
dilated	O
by	O
inserting	O
zeros	O
between	O
filter	O
elements	O
for	O
a	O
given	O
dilation	O
factor	O
.	O

We	O
refer	O
the	O
reader	O
to	O
for	O
more	O
information	O
about	O
the	O
dilated	Method
convolution	Method
.	O

Dilated	Method
Convolution	Method
based	Method
Inception	Method
Module	Method
:	O
To	O
make	O
full	O
use	O
of	O
the	O
image	O
multi	O
-	O
scale	O
information	O
,	O
Szegedy	O
et	O
al	O
.	O

proposed	O
an	O
inception	Method
module	Method
for	O
image	Task
classification	Task
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
b	O
,	O
the	O
GoogLeNet	Method
inception	Method
module	Method
contains	O
multiple	O
convolution	Method
with	O
different	O
kernel	O
size	O
.	O

It	O
concatenates	O
the	O
outputs	O
of	O
these	O
different	O
size	Method
filter	Method
to	O
fuse	O
different	O
scale	O
information	O
.	O

With	O
this	O
multi	Method
-	Method
scale	Method
information	Method
learning	Method
structure	Method
,	O
GoogLeNet	Method
achieves	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
classification	Task
and	Task
detection	Task
in	O
the	O
ImageNet	Task
Large	Task
-	Task
Scale	Task
Visual	Task
Recognition	Task
Challenge	Task
2014	O
(	O
ILSVRC14	Task
)	O
.	O

Inspired	O
by	O
this	O
successful	O
work	O
,	O
we	O
propose	O
a	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
to	O
learn	O
multi	O
-	O
scale	O
information	O
for	O
improving	O
single	O
image	O
SR	Task
performance	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
a	O
,	O
our	O
new	O
inception	Method
module	Method
first	O
use	O
three	O
dilated	Method
convolution	Method
with	O
kernel	O
size	O
and	O
dilated	O
factor	O
1	O
,	O
2	O
and	O
3	O
,	O
respectively	O
,	O
to	O
operate	O
on	O
the	O
previous	O
layers	O
.	O

Then	O
,	O
we	O
concatenate	O
all	O
these	O
convolution	O
outputs	O
to	O
fuse	O
different	O
scale	O
information	O
.	O

Insights	O
:	O
there	O
are	O
a	O
lot	O
of	O
ways	O
to	O
learn	O
the	O
multi	O
-	O
scale	O
information	O
by	O
deep	Method
network	Method
.	O

For	O
example	O
,	O
the	O
GoogLeNet	Method
inception	Method
module	Method
uses	O
different	O
kernel	O
size	O
of	O
convolution	Method
with	O
different	O
receptive	O
field	O
to	O
operate	O
on	O
the	O
previous	O
layer	O
.	O

However	O
,	O
it	O
is	O
still	O
operated	O
on	O
the	O
same	O
image	O
scale	O
space	O
.	O

The	O
other	O
choice	O
is	O
to	O
operate	O
on	O
different	O
scale	O
image	O
input	O
.	O

Farabet	O
et	O
al	O
.	O

use	O
this	O
way	O
to	O
learn	O
multi	Task
-	Task
scale	Task
feature	Task
for	O
scene	Task
parsing	Task
.	O

For	O
single	Task
image	Task
SR	Task
,	O
the	O
lower	O
solution	O
image	O
always	O
has	O
much	O
more	O
sharp	O
detail	O
information	O
than	O
the	O
interpolation	O
result	O
.	O

Therefore	O
,	O
it	O
can	O
improve	O
the	O
SR	Task
result	O
by	O
taking	O
these	O
small	O
scale	O
images	O
into	O
account	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
process	O
of	O
our	O
proposed	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
how	O
to	O
learn	O
multi	O
-	O
scale	O
information	O
.	O

The	O
blue	O
solid	O
lines	O
indicate	O
that	O
all	O
these	O
dilated	Method
convolutions	Method
actually	O
operate	O
on	O
the	O
same	O
scale	O
image	O
,	O
while	O
the	O
red	O
dash	O
lines	O
mean	O
these	O
convolutions	O
with	O
different	O
dilated	O
factors	O
learn	O
the	O
corresponding	O
scale	O
image	O
information	O
.	O

Furthermore	O
,	O
the	O
output	O
of	O
the	O
dilated	Method
convolution	Method
can	O
keep	O
the	O
same	O
size	O
with	O
its	O
input	O
,	O
so	O
we	O
can	O
fuse	O
these	O
different	O
scale	O
information	O
through	O
concatenation	Method
operator	Method
easily	O
.	O

section	O
:	O
PROPOSED	O
MULTI	Method
-	Method
SCALE	Method
INFORMATION	Method
LEARNING	Method
NETWORK	Method
STRUCTURE	Method
The	O
configuration	O
of	O
our	O
proposed	O
single	O
image	O
SR	Task
network	O
structure	O
is	O
outlined	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
which	O
cascades	O
dilated	Method
convolution	Method
based	Method
inception	Method
modules	Method
and	O
common	O
discrete	Method
convolutions	Method
.	O

It	O
can	O
be	O
explained	O
as	O
three	O
phases	O
,	O
i.e.	O
feature	Task
extraction	Task
,	O
feature	Task
enhancement	Task
and	O
image	Task
reconstruction	Task
.	O

Since	O
residual	Method
learning	Method
have	O
been	O
proven	O
to	O
be	O
an	O
effective	O
way	O
to	O
accelerate	O
the	O
network	Task
convergence	Task
in	O
recent	O
works	O
,	O
we	O
follow	O
them	O
to	O
make	O
our	O
network	O
predict	O
image	O
high	O
frequency	O
details	O
instead	O
of	O
the	O
HR	Task
image	O
itself	O
.	O

The	O
predicted	O
image	O
frequency	O
details	O
will	O
be	O
added	O
to	O
the	O
input	O
LR	O
image	O
to	O
get	O
the	O
final	O
desired	O
HR	Task
one	O
.	O

As	O
most	O
single	O
image	O
SR	Task
works	O
done	O
,	O
we	O
first	O
up	O
-	O
sample	O
the	O
LR	O
image	O
to	O
the	O
desired	O
size	O
using	O
bicubic	Method
interpolation	Method
.	O

For	O
the	O
ease	O
of	O
presentation	O
,	O
we	O
still	O
call	O
the	O
interpolation	O
result	O
as	O
a	O
”	O
low	O
-	O
resolution	O
”	O
image	O
,	O
although	O
it	O
has	O
the	O
same	O
size	O
as	O
the	O
HR	Task
image	O
.	O

For	O
the	O
feature	Task
extraction	Task
phase	Task
,	O
dilated	Method
convolution	Method
based	Method
inception	Method
modules	Method
operate	O
on	O
the	O
LR	O
input	O
.	O

The	O
filter	Method
kernel	Method
size	Method
is	O
,	O
where	O
is	O
the	O
number	O
of	O
image	O
channel	O
,	O
for	O
the	O
first	O
inception	Method
module	Method
layer	Method
.	O

The	O
inception	Method
module	Method
combines	O
different	O
scale	O
feature	O
information	O
through	O
concatenation	Method
operator	Method
.	O

To	O
further	O
fuse	O
these	O
multi	O
-	O
scale	O
information	O
,	O
in	O
the	O
next	O
we	O
add	O
a	O
nonlinear	Method
transformation	Method
layer	Method
after	O
the	O
inception	Method
module	Method
layer	Method
,	O
which	O
contains	O
common	Method
convolution	Method
with	O
filter	O
kernel	O
size	O
.	O

Both	O
the	O
high	O
level	O
and	O
low	Task
level	Task
vision	Task
works	O
have	O
proven	O
the	O
deeper	O
the	O
network	O
the	O
better	O
the	O
performance	O
.	O

Furthermore	O
,	O
the	O
larger	O
reception	O
field	O
of	O
the	O
network	O
can	O
catch	O
more	O
contextual	O
information	O
that	O
gives	O
more	O
clues	O
for	O
predicting	O
high	O
frequency	O
details	O
.	O

Therefore	O
,	O
we	O
iterate	O
times	O
the	O
process	O
of	O
dilated	Method
convolution	Method
based	Method
inception	Method
modules	Method
following	O
with	O
a	O
common	O
convolution	Method
operator	Method
for	O
the	O
feature	Task
enhancement	Task
phase	Task
.	O

In	O
this	O
phase	O
,	O
the	O
filter	O
kernel	O
size	O
is	O
as	O
show	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

In	O
the	O
image	Task
reconstruction	Task
phase	Task
,	O
we	O
use	O
a	O
single	O
common	Method
convolution	Method
of	Method
size	Method
to	O
predict	O
the	O
high	O
frequency	O
details	O
.	O

Finally	O
,	O
the	O
predicted	O
high	O
frequency	O
details	O
will	O
add	O
to	O
the	O
interpolation	O
output	O
to	O
get	O
the	O
desired	O
HR	Task
image	O
.	O

subsection	O
:	O
Training	O
There	O
are	O
a	O
lot	O
of	O
perceptually	Metric
relevant	Metric
characteristics	Metric
based	Metric
loss	Metric
functions	Metric
have	O
been	O
proposed	O
in	O
the	O
literature	O
.	O

But	O
for	O
a	O
fair	O
comparison	O
with	O
SRCNN	Method
and	O
FSRCNN	Method
,	O
we	O
adopt	O
the	O
mean	Metric
square	Metric
error	Metric
(	O
MSE	Metric
)	O
as	O
the	O
cost	Metric
function	Metric
of	O
our	O
network	O
.	O

Our	O
goal	O
is	O
to	O
train	O
an	O
end	Task
-	Task
to	Task
-	Task
end	Task
mapping	Task
to	O
predict	Task
high	Task
frequency	Task
detail	Task
,	O
where	O
is	O
an	O
interpolation	O
result	O
of	O
the	O
LR	O
image	O
and	O
is	O
the	O
estimated	O
high	O
frequency	O
detail	O
image	O
.	O

Given	O
a	O
training	O
dataset	O
,	O
the	O
optimization	Metric
objective	Metric
is	O
represented	O
as	O
where	O
is	O
the	O
network	O
parameters	O
needed	O
to	O
be	O
trained	O
,	O
is	O
the	O
estimated	O
high	O
frequency	O
image	O
with	O
respect	O
to	O
the	O
interpolation	O
result	O
of	O
a	O
LR	O
image	O
.	O

In	O
the	O
literature	O
,	O
people	O
suggest	O
to	O
use	O
to	O
recently	O
proposed	O
Parametric	Method
Rectified	Method
Linear	Method
Unit	Method
(	O
PReLU	Method
)	O
as	O
the	O
activation	Method
function	Method
instead	O
of	O
the	O
commonly	O
-	O
used	O
Rectified	Method
Linear	Method
Unit	Method
(	O
ReLU	Method
)	O
.	O

But	O
,	O
in	O
order	O
to	O
reduce	O
parameters	O
,	O
ReLU	Method
is	O
used	O
after	O
each	O
convolution	Method
layer	Method
in	O
our	O
very	O
deep	Method
network	Method
that	O
has	O
got	O
a	O
satisfactory	O
result	O
for	O
comparison	O
.	O

We	O
use	O
the	O
adaptive	Method
moment	Method
estimation	Method
(	O
Adam	Method
)	O
to	O
optimize	O
all	O
network	O
parameters	O
instead	O
of	O
the	O
commonly	O
used	O
stochastic	Method
gradient	Method
descent	Method
one	Method
.	O

section	O
:	O
EXPERIMENTAL	O
RESULTS	O
A	O
lot	O
of	O
experiments	O
have	O
been	O
done	O
to	O
show	O
dramatic	O
improvement	O
in	O
performance	O
can	O
be	O
obtained	O
by	O
our	O
proposed	O
method	O
.	O

We	O
give	O
the	O
experimental	O
details	O
and	O
report	O
the	O
quantitative	O
results	O
on	O
three	O
popular	O
dataset	O
in	O
this	O
section	O
.	O

We	O
name	O
the	O
proposed	O
method	O
as	O
Multiple	Method
Scale	Method
Super	Method
-	Method
Resolution	Method
Network	Method
(	O
MSSRNet	Method
)	O
.	O

subsection	O
:	O
Datasets	O
for	O
Training	O
and	O
Testing	O
It	O
is	O
well	O
known	O
that	O
training	O
dataset	O
is	O
very	O
important	O
for	O
the	O
performance	O
of	O
learning	Method
based	Method
image	Method
restoration	Method
methods	Method
.	O

A	O
lot	O
of	O
training	O
dataset	O
can	O
be	O
found	O
in	O
the	O
literature	O
.	O

For	O
example	O
,	O
SRCNN	Method
uses	O
a	O
91	O
images	O
dataset	O
and	O
VDSR	Method
uses	O
291	O
images	O
dataset	O
.	O

However	O
,	O
the	O
91	O
images	O
dataset	O
is	O
too	O
small	O
to	O
push	O
a	O
deep	Method
model	Method
to	O
the	O
best	O
performance	O
.	O

Some	O
image	O
in	O
the	O
291	O
image	O
dataset	O
are	O
JPEG	O
format	O
,	O
which	O
are	O
not	O
optimal	O
for	O
the	O
SR	Task
task	O
.	O

We	O
follow	O
FSRCNN	Method
to	O
use	O
the	O
General	Material
-	Material
100	Material
dataset	Material
,	O
which	O
contains	O
100	O
bmp	O
-	O
format	O
images	O
(	O
with	O
no	O
compression	O
)	O
.	O

We	O
set	O
the	O
patch	O
size	O
as	O
,	O
and	O
use	O
data	Method
augmentation	Method
(	O
rotation	O
or	O
flip	O
)	O
to	O
prepare	O
training	O
data	O
.	O

Following	O
FSRCNN	Method
,	O
SRCNN	Method
and	O
SCN	Method
,	O
we	O
use	O
three	O
dataset	O
,	O
i.e.	O
Set5	Material
(	O
5	O
images	O
)	O
,	O
Set14	Material
(	O
14	O
images	O
)	O
and	O
BSD200	Material
(	O
200	O
images	O
)	O
for	O
testing	O
,	O
which	O
are	O
widely	O
used	O
for	O
benchmark	O
in	O
other	O
works	O
.	O

subsection	O
:	O
Implementing	O
Details	O
and	O
Parameters	O
For	O
each	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
layer	Method
,	O
we	O
set	O
.	O

That	O
is	O
,	O
there	O
are	O
8	O
inception	O
module	O
per	O
layer	O
.	O

For	O
feature	Task
enhancement	Task
process	Task
,	O
we	O
iterate	O
5	O
inception	Method
modules	Method
to	O
make	O
the	O
network	O
deeper	O
,	O
i.e.	O
.	O

Filters	Method
are	O
initialized	O
using	O
the	O
initializer	Method
proposed	O
by	O
He	O
et	O
al	O
.	O

with	O
values	O
sampled	O
from	O
the	O
Uniform	O
distribution	O
.	O

For	O
the	O
other	O
hyper	O
-	O
parameters	O
of	O
Adam	Method
,	O
we	O
follow	O
to	O
set	O
the	O
exponential	O
decay	O
rates	O
for	O
the	O
first	O
and	O
second	Method
moment	Method
estimate	Method
to	O
0.9	O
and	O
0.999	O
,	O
respectively	O
.	O

Each	O
model	O
was	O
trained	O
only	O
100	O
epochs	O
and	O
each	O
epoch	O
iterates	O
2000	O
times	O
with	O
batch	O
size	O
of	O
64	O
.	O

We	O
set	O
a	O
lager	Metric
learning	Metric
rate	Metric
in	O
the	O
initial	O
training	O
phase	O
to	O
accelerate	O
convergence	O
,	O
then	O
decrease	O
it	O
gradually	O
to	O
make	O
the	O
model	O
more	O
stable	O
.	O

Therefore	O
,	O
the	O
learning	Metric
rates	Metric
are	O
0.001	O
,	O
0.0001	O
and	O
0.00001	O
for	O
the	O
first	O
50	O
epochs	O
,	O
the	O
51	O
to	O
80	O
epochs	O
and	O
the	O
last	O
20	O
epochs	O
,	O
respectively	O
.	O

Our	O
model	O
is	O
implemented	O
by	O
the	O
MatConvNet	Method
package	Method
[	O
17	O
]	O
.	O

subsection	O
:	O
Comparisons	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Methods	O
We	O
compare	O
our	O
method	O
with	O
five	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
learning	O
based	O
SR	Task
algorithms	O
that	O
rely	O
on	O
external	O
databases	O
,	O
namely	O
the	O
A	Method
+	Method
,	O
SRF	Method
,	O
SRCNN	Method
,	O
FSRCNN	Method
and	O
SCN	Method
.	O

A	O
+	O
and	O
SRF	Method
are	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
traditional	O
methods	O
,	O
while	O
SRCNN	Method
,	O
FSRCNN	Method
and	O
SCN	Method
are	O
three	O
newest	O
popular	O
deep	O
learning	O
based	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
image	O
methods	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
provide	O
a	O
summary	O
of	O
quantitative	O
evaluation	O
on	O
several	O
datasets	O
.	O

The	O
results	O
of	O
other	O
five	O
methods	O
are	O
the	O
same	O
as	O
reported	O
at	O
FSRCNN	Method
.	O

Our	O
method	O
outperforms	O
all	O
previous	O
methods	O
in	O
these	O
datasets	O
.	O

Compare	O
with	O
the	O
newest	O
FSRCNN	Method
,	O
our	O
method	O
can	O
improve	O
roughly	O
0.33	O
dB	O
,	O
0.22Db	O
and	O
0.37	O
dB	O
on	O
average	O
with	O
respect	O
to	O
up	O
-	O
sample	O
factor	O
2	O
,	O
3	O
and	O
4	O
on	O
Set5	Material
dataset	Material
,	O
respectively	O
.	O

Over	O
the	O
three	O
dataset	O
and	O
three	O
up	O
-	O
sample	O
factor	O
,	O
our	O
MSSRNet	Method
can	O
improve	O
roughly	O
2.33	O
dB	O
,	O
0.6	O
dB	O
,	O
0.46	O
dB	O
,	O
0.74	O
dB	O
,	O
0.32	O
dB	O
and	O
0.25	O
dB	O
on	O
average	O
,	O
in	O
comparison	O
with	O
Bicubic	Method
,	O
A	O
+	O
,	O
SRF	Method
,	O
SRCNN	Method
,	O
SCN	Method
and	O
FSRCNN	Method
,	O
respectively	O
.	O

To	O
get	O
better	O
performance	O
,	O
we	O
can	O
increase	O
the	O
network	O
depth	O
(	O
larger	O
)	O
,	O
which	O
is	O
called	O
deeper	O
is	O
better	O
in	O
the	O
literature	O
,	O
and	O
the	O
network	O
width	O
with	O
larger	O
.	O

In	O
our	O
experiments	O
,	O
we	O
have	O
implemented	O
the	O
fatter	Method
network	Method
with	O
and	O
,	O
and	O
the	O
deeper	Method
network	Method
with	O
and	O
.	O

Both	O
the	O
deeper	O
and	O
the	O
fatter	Method
networks	Method
show	O
PSNR	Metric
and	O
SSIM	Metric
gain	O
.	O

The	O
reader	O
can	O
download	O
our	O
test	O
code	O
to	O
get	O
more	O
quantitative	O
and	O
qualitative	O
results	O
.	O

section	O
:	O
CONCLUSION	O
In	O
this	O
paper	O
,	O
we	O
use	O
deep	Method
learning	Method
technology	Method
to	O
solve	O
the	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
problem	O
.	O

We	O
first	O
propose	O
a	O
dilated	Method
convolution	Method
based	Method
inception	Method
module	Method
,	O
which	O
can	O
learn	O
multi	O
-	O
scale	O
information	O
from	O
the	O
single	O
scale	O
input	O
image	O
.	O

We	O
design	O
a	O
deep	Method
network	Method
,	O
which	O
cascades	O
multiple	O
dilated	Method
convolution	Method
based	Method
inception	Method
modules	Method
,	O
for	O
single	Task
image	Task
super	Task
-	Task
resolution	Task
.	O

Experimental	O
results	O
show	O
that	O
the	O
proposed	O
method	O
outperforms	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
ones	O
.	O

As	O
future	O
work	O
we	O
plan	O
to	O
explore	O
MSSRNet	Method
for	O
video	Task
processing	Task
.	O

section	O
:	O
ACKNOWLEDGEMENTS	O
This	O
work	O
has	O
been	O
supported	O
in	O
part	O
by	O
the	O
Major	O
State	O
Basic	O
Research	O
Development	O
Program	O
of	O
China	O
(	O
973	O
Program	O
2015CB351804	O
)	O
,	O
the	O
National	O
Science	O
Foundation	O
of	O
China	O
under	O
Grant	O
No	O
.	O

61572155	O
.	O

bibliography	O
:	O
References	O
