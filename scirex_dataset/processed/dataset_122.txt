document	O
:	O
Achieving	O
Human	Task
Parity	Task
in	O
Conversational	Task
Speech	Task
Recognition	Task
Conversational	Task
speech	Task
recognition	Task
has	O
served	O
as	O
a	O
flagship	Task
speech	Task
recognition	Task
task	Task
since	O
the	O
release	O
of	O
the	O
Switchboard	Material
corpus	Material
in	O
the	O
1990s	O
.	O

In	O
this	O
paper	O
,	O
we	O
measure	O
the	O
human	Metric
error	Metric
rate	Metric
on	O
the	O
widely	O
used	O
NIST	O
2000	O
test	O
set	O
,	O
and	O
find	O
that	O
our	O
latest	O
automated	O
system	O
has	O
reached	O
human	O
parity	O
.	O

The	O
error	Metric
rate	Metric
of	O
professional	Method
transcribers	Method
is	O
5.9	O
%	O
for	O
the	O
Switchboard	Material
portion	Material
of	O
the	O
data	O
,	O
in	O
which	O
newly	O
acquainted	O
pairs	O
of	O
people	O
discuss	O
an	O
assigned	O
topic	O
,	O
and	O
11.3	O
%	O
for	O
the	O
CallHome	O
portion	O
where	O
friends	O
and	O
family	O
members	O
have	O
open	O
-	O
ended	O
conversations	O
.	O

In	O
both	O
cases	O
,	O
our	O
automated	O
system	O
establishes	O
a	O
new	O
state	O
of	O
the	O
art	O
,	O
and	O
edges	O
past	O
the	O
human	Metric
benchmark	Metric
,	O
achieving	O
error	Metric
rates	Metric
of	O
5.8	O
%	O
and	O
11.0	O
%	O
,	O
respectively	O
.	O

The	O
key	O
to	O
our	O
system	O
’s	O
performance	O
is	O
the	O
use	O
of	O
various	O
convolutional	Method
and	Method
LSTM	Method
acoustic	Method
model	Method
architectures	Method
,	O
combined	O
with	O
a	O
novel	O
spatial	Method
smoothing	Method
method	Method
and	O
lattice	Method
-	Method
free	Method
MMI	Method
acoustic	Method
training	Method
,	O
multiple	O
recurrent	Method
neural	Method
network	Method
language	Method
modeling	Method
approaches	Method
,	O
and	O
a	O
systematic	O
use	O
of	O
system	Method
combination	Method
.	O

W.Xiong	O
,	O
J.Droppo	O
,	O
X.Huang	O
,	O
F.Seide	O
,	O
M.Seltzer	O
,	O
A.Stolcke	O
,	O
D.YuandG.Zweig	O
MicrosoftResearch	O
TechnicalReportMSR	O
-	O
TR	O
-	O
2016	O
-	O
71	O
RevisedFebruary2017	O
Conversational	Task
speech	Task
recognition	Task
,	O
convolutional	Method
neural	Method
networks	Method
,	O
recurrent	Method
neural	Method
networks	Method
,	O
VGG	Method
,	O
ResNet	Method
,	O
LACE	Method
,	O
BLSTM	Method
,	O
spatial	Method
smoothing	Method
.	O

section	O
:	O
Introduction	O
Recent	O
years	O
have	O
seen	O
human	O
performance	O
levels	O
reached	O
or	O
surpassed	O
in	O
tasks	O
ranging	O
from	O
the	O
games	Task
of	Task
chess	Task
and	O
Go	O
to	O
simple	O
speech	Task
recognition	Task
tasks	Task
like	O
carefully	O
read	O
newspaper	O
speech	O
and	O
rigidly	O
constrained	Material
small	Material
-	Material
vocabulary	Material
tasks	Material
in	O
noise	O
.	O

In	O
the	O
area	O
of	O
speech	Task
recognition	Task
,	O
much	O
of	O
the	O
pioneering	O
early	O
work	O
was	O
driven	O
by	O
a	O
series	O
of	O
carefully	O
designed	O
tasks	O
with	O
DARPA	O
-	O
funded	O
datasets	O
publicly	O
released	O
by	O
the	O
LDC	Material
and	O
NIST	O
:	O
first	O
simple	O
ones	O
like	O
the	O
“	O
resource	Task
management	Task
”	O
task	O
with	O
a	O
small	O
vocabulary	O
and	O
carefully	O
controlled	O
grammar	O
;	O
then	O
read	Task
speech	Task
recognition	Task
in	O
the	O
Wall	Material
Street	Material
Journal	Material
task	Material
;	O
then	O
Broadcast	Material
News	Material
;	O
each	O
progressively	O
more	O
difficult	O
for	O
automatic	Task
systems	Task
.	O

One	O
of	O
last	O
big	O
initiatives	O
in	O
this	O
area	O
was	O
in	O
conversational	Material
telephone	Material
speech	Material
(	O
CTS	Material
)	O
,	O
which	O
is	O
especially	O
difficult	O
due	O
to	O
the	O
spontaneous	O
(	O
neither	O
read	O
nor	O
planned	O
)	O
nature	O
of	O
the	O
speech	O
,	O
its	O
informality	O
,	O
and	O
the	O
self	O
-	O
corrections	O
,	O
hesitations	O
and	O
other	O
disfluencies	O
that	O
are	O
pervasive	O
.	O

The	O
Switchboard	O
and	O
later	O
Fisher	Material
data	Material
collections	Material
of	O
the	O
1990s	O
and	O
early	O
2000s	O
provide	O
what	O
is	O
to	O
date	O
the	O
largest	O
and	O
best	O
studied	O
of	O
the	O
conversational	Material
corpora	Material
.	O

The	O
history	O
of	O
work	O
in	O
this	O
area	O
includes	O
key	O
contributions	O
by	O
institutions	O
such	O
as	O
IBM	O
,	O
BBN	O
,	O
SRI	O
,	O
AT	O
&	O
T	O
,	O
LIMSI	O
,	O
Cambridge	O
University	O
,	O
Microsoft	O
and	O
numerous	O
others	O
.	O

In	O
the	O
past	O
,	O
human	O
performance	O
on	O
this	O
task	O
has	O
been	O
widely	O
cited	O
as	O
being	O
4	O
%	O
.	O

However	O
,	O
the	O
error	Metric
rate	Metric
estimate	Metric
in	O
is	O
attributed	O
to	O
a	O
“	O
personal	O
communication	O
,	O
”	O
and	O
the	O
actual	O
source	O
of	O
this	O
number	O
is	O
ephemeral	O
.	O

To	O
better	O
understand	O
human	O
performance	O
,	O
we	O
have	O
used	O
professional	O
transcribers	O
to	O
transcribe	O
the	O
actual	O
test	O
sets	O
that	O
we	O
are	O
working	O
with	O
,	O
specifically	O
the	O
CallHome	Material
and	Material
Switchboard	Material
portions	Material
of	O
the	O
NIST	Material
eval	Material
2000	Material
test	Material
set	Material
.	O

We	O
find	O
that	O
the	O
human	Metric
error	Metric
rates	Metric
on	O
these	O
two	O
parts	O
are	O
different	O
almost	O
by	O
a	O
factor	O
of	O
two	O
,	O
so	O
a	O
single	O
number	O
is	O
inappropriate	O
to	O
cite	O
.	O

The	O
error	Metric
rate	Metric
on	O
Switchboard	O
is	O
about	O
5.9	O
%	O
,	O
and	O
for	O
CallHome	Task
11.3	O
%	O
.	O

We	O
improve	O
on	O
our	O
recently	O
reported	O
conversational	Method
speech	Method
recognition	Method
system	Method
by	O
about	O
0.4	O
%	O
,	O
and	O
now	O
exceed	O
human	O
performance	O
by	O
a	O
small	O
margin	O
.	O

Our	O
progress	O
is	O
a	O
result	O
of	O
the	O
careful	O
engineering	O
and	O
optimization	Method
of	Method
convolutional	Method
and	Method
recurrent	Method
neural	Method
networks	Method
.	O

While	O
the	O
basic	O
structures	O
have	O
been	O
well	O
known	O
for	O
a	O
long	O
period	O
,	O
it	O
is	O
only	O
recently	O
that	O
they	O
have	O
dominated	O
the	O
field	O
as	O
the	O
best	O
models	O
for	O
speech	Task
recognition	Task
.	O

Surprisingly	O
,	O
this	O
is	O
the	O
case	O
for	O
both	O
acoustic	Task
modeling	Task
and	O
language	Task
modeling	Task
.	O

In	O
comparison	O
to	O
the	O
standard	O
feed	Method
-	Method
forward	Method
MLPs	Method
or	O
DNNs	Method
that	O
first	O
demonstrated	O
breakthrough	O
performance	O
on	O
conversational	Task
speech	Task
recognition	Task
,	O
these	O
acoustic	Method
models	Method
have	O
the	O
ability	O
to	O
model	O
a	O
large	O
amount	O
of	O
acoustic	O
context	O
with	O
temporal	O
invariance	O
,	O
and	O
in	O
the	O
case	O
of	O
convolutional	Method
models	Method
,	O
with	O
frequency	O
invariance	O
as	O
well	O
.	O

In	O
language	Task
modeling	Task
,	O
recurrent	Method
models	Method
appear	O
to	O
improve	O
over	O
classical	O
N	Method
-	Method
gram	Method
models	Method
through	O
the	O
use	O
of	O
an	O
unbounded	O
word	O
history	O
,	O
as	O
well	O
as	O
the	O
generalization	Method
ability	Method
of	O
continuous	Method
word	Method
representations	Method
.	O

In	O
the	O
meantime	O
,	O
ensemble	Method
learning	Method
has	O
become	O
commonly	O
used	O
in	O
several	O
neural	Method
models	Method
,	O
to	O
improve	O
robustness	Metric
by	O
reducing	O
bias	Metric
and	Metric
variance	Metric
.	O

This	O
paper	O
is	O
an	O
expanded	O
version	O
of	O
,	O
with	O
the	O
following	O
additional	O
material	O
:	O
A	O
comprehensive	O
assessment	O
of	O
human	Metric
performance	O
on	O
the	O
NIST	Material
eval	Material
2000	Material
test	Material
set	Material
The	O
description	O
of	O
a	O
novel	O
spatial	Method
regularization	Method
method	Method
which	O
significantly	O
boosts	O
our	O
LSTM	Method
acoustic	Method
model	Method
performance	O
The	O
use	O
of	O
LSTM	Method
rather	O
than	O
RNN	Method
-	Method
LMs	Method
,	O
and	O
the	O
use	O
of	O
a	O
letter	Method
-	Method
trigram	Method
input	Method
representation	Method
A	O
two	O
-	O
level	Method
system	Method
combination	Method
,	O
based	O
on	O
a	O
subsystem	O
of	O
BLSTM	Method
-	Method
system	Method
variants	Method
that	O
,	O
by	O
itself	O
,	O
surpasses	O
the	O
best	O
previously	O
reported	O
results	O
Expanded	O
coverage	O
of	O
the	O
Microsoft	Method
Cognitive	Method
Toolkit	Method
(	O
CNTK	Method
)	O
used	O
to	O
build	O
our	O
models	O
An	O
analysis	O
of	O
human	Task
versus	Task
machine	Task
errors	Task
,	O
which	O
indicates	O
substantial	O
equivalence	O
,	O
with	O
the	O
exception	O
of	O
the	O
word	O
classes	O
of	O
backchannel	O
acknowledgments	O
(	O
e.g.	O
“	O
uh	O
-	O
huh	O
”	O
)	O
and	O
hesitations	O
(	O
e.g.	O
“	O
um	O
”	O
)	O
.	O

The	O
remainder	O
of	O
this	O
paper	O
describes	O
our	O
system	O
in	O
detail	O
.	O

Section	O
[	O
reference	O
]	O
describes	O
our	O
measurement	O
of	O
human	Metric
performance	Metric
.	O

Section	O
[	O
reference	O
]	O
describes	O
the	O
convolutional	Method
neural	Method
net	Method
(	Method
CNN	Method
)	Method
and	Method
long	Method
-	Method
short	Method
-	Method
term	Method
memory	Method
(	Method
LSTM	Method
)	Method
models	Method
.	O

Section	O
[	O
reference	O
]	O
describes	O
our	O
implementation	O
of	O
i	Method
-	Method
vector	Method
adaptation	Method
.	O

Section	O
[	O
reference	O
]	O
presents	O
out	O
lattice	Method
-	Method
free	Method
MMI	Method
training	Method
process	Method
.	O

Language	Method
model	Method
rescoring	Method
is	O
a	O
significant	O
part	O
of	O
our	O
system	O
,	O
and	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
describe	O
the	O
CNTK	Method
toolkit	Method
that	O
forms	O
the	O
basis	O
of	O
our	O
neural	Method
network	Method
models	Method
in	O
Section	O
[	O
reference	O
]	O
.	O

Experimental	O
results	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
followed	O
by	O
an	O
error	Method
analysis	Method
in	O
section	O
[	O
reference	O
]	O
,	O
a	O
review	O
of	O
relevant	O
prior	O
work	O
in	O
[	O
reference	O
]	O
and	O
concluding	O
remarks	O
.	O

section	O
:	O
Human	O
Performance	O
To	O
measure	O
human	O
performance	O
,	O
we	O
leveraged	O
an	O
existing	O
pipeline	O
in	O
which	O
Microsoft	Material
data	Material
is	O
transcribed	O
on	O
a	O
weekly	O
basis	O
.	O

This	O
pipeline	O
uses	O
a	O
large	O
commercial	O
vendor	O
to	O
perform	O
two	O
-	O
pass	Task
transcription	Task
.	O

In	O
the	O
first	O
pass	O
,	O
a	O
transcriber	O
works	O
from	O
scratch	O
to	O
transcribe	O
the	O
data	O
.	O

In	O
the	O
second	O
pass	O
,	O
a	O
second	O
listener	O
monitors	O
the	O
data	O
to	O
do	O
error	Task
correction	Task
.	O

Dozens	O
of	O
hours	O
of	O
test	O
data	O
are	O
processed	O
in	O
each	O
batch	O
.	O

One	O
week	O
,	O
we	O
added	O
the	O
NIST	Material
2000	Material
CTS	Material
evaluation	Material
data	Material
to	O
the	O
work	O
-	O
list	O
,	O
without	O
further	O
comment	O
.	O

The	O
intention	O
was	O
to	O
measure	O
the	O
error	Metric
rate	Metric
of	O
professional	O
transcribers	O
going	O
about	O
their	O
normal	O
everyday	O
business	O
.	O

Aside	O
from	O
the	O
standard	O
two	O
-	O
pass	O
checking	O
in	O
place	O
,	O
we	O
did	O
not	O
do	O
a	O
complex	O
multi	Task
-	Task
party	Task
transcription	Task
and	Task
adjudication	Task
process	Task
.	O

The	O
transcribers	O
were	O
given	O
the	O
same	O
audio	Material
segments	Material
as	O
were	O
provided	O
to	O
the	O
speech	Method
recognition	Method
system	Method
,	O
which	O
results	O
in	O
short	O
sentences	O
or	O
sentence	O
fragments	O
from	O
a	O
single	O
channel	O
.	O

This	O
makes	O
the	O
task	O
easier	O
since	O
the	O
speakers	O
are	O
more	O
clearly	O
separated	O
,	O
and	O
more	O
difficult	O
since	O
the	O
two	O
sides	O
of	O
the	O
conversation	O
are	O
not	O
interleaved	O
.	O

Thus	O
,	O
it	O
is	O
the	O
same	O
condition	O
as	O
reported	O
for	O
our	O
automated	O
systems	O
.	O

The	O
resulting	O
numbers	O
are	O
5.9	O
%	O
for	O
the	O
Switchboard	O
portion	O
,	O
and	O
11.3	O
%	O
for	O
the	O
CallHome	O
portion	O
of	O
the	O
NIST	Material
2000	Material
test	Material
set	Material
,	O
using	O
the	O
NIST	Method
scoring	Method
protocol	Method
.	O

These	O
numbers	O
should	O
be	O
taken	O
as	O
an	O
indication	O
of	O
the	O
“	O
error	Metric
rate	Metric
”	O
of	O
a	O
trained	O
professional	O
working	O
in	O
industry	O
-	O
standard	O
speech	Task
transcript	Task
production	Task
.	O

(	O
We	O
have	O
submitted	O
the	O
human	O
transcripts	O
thus	O
produced	O
to	O
the	O
Linguistic	Material
Data	Material
Consortium	Material
for	O
publication	O
,	O
so	O
as	O
to	O
facilitate	O
research	O
by	O
other	O
groups	O
.	O

)	O
Past	O
work	O
reports	O
inter	Metric
-	Metric
transcriber	Metric
error	Metric
rates	Metric
for	O
data	O
taken	O
from	O
the	O
later	O
RT03	Material
test	Material
set	Material
(	O
which	O
contains	O
Switchboard	Material
and	Material
Fisher	Material
,	O
but	O
no	O
CallHome	Material
data	Material
)	O
.	O

Error	Metric
rates	Metric
of	O
4.1	O
to	O
4.5	O
%	O
are	O
reported	O
for	O
extremely	O
careful	O
multiple	Task
transcriptions	Task
,	O
and	O
9.6	O
%	O
for	O
“	O
quick	Task
transcriptions	Task
.	O

”	O
While	O
this	O
is	O
a	O
different	O
test	O
set	O
,	O
the	O
numbers	O
are	O
in	O
line	O
with	O
our	O
findings	O
.	O

We	O
note	O
that	O
the	O
bulk	O
of	O
the	O
Fisher	Material
training	Material
data	Material
,	O
and	O
the	O
bulk	O
of	O
the	O
data	O
overall	O
,	O
was	O
transcribed	O
with	O
the	O
“	O
quick	O
transcription	O
”	O
guidelines	O
.	O

Thus	O
,	O
the	O
current	O
state	O
of	O
the	O
art	O
is	O
actually	O
far	O
exceeding	O
the	O
noise	O
level	O
in	O
its	O
own	O
training	O
data	O
.	O

Perhaps	O
the	O
most	O
important	O
point	O
is	O
the	O
extreme	O
variability	O
between	O
the	O
two	O
test	O
subsets	O
.	O

The	O
more	O
informal	O
CallHome	Material
data	Material
has	O
almost	O
double	O
the	O
human	Metric
error	Metric
rate	Metric
of	O
the	O
Switchboard	Material
data	Material
.	O

Interestingly	O
,	O
the	O
same	O
informality	O
,	O
multiple	O
speakers	O
per	O
channel	O
,	O
and	O
recording	O
conditions	O
that	O
make	O
CallHome	O
hard	O
for	O
computers	O
make	O
it	O
difficult	O
for	O
people	O
as	O
well	O
.	O

Notably	O
,	O
the	O
performance	O
of	O
our	O
artificial	O
system	O
aligns	O
almost	O
exactly	O
with	O
the	O
performance	O
of	O
people	O
on	O
both	O
sets	O
.	O

section	O
:	O
Convolutional	Method
and	Method
LSTM	Method
Neural	Method
Networks	Method
subsection	O
:	O
CNNs	Method
We	O
use	O
three	O
CNN	Method
variants	Method
.	O

The	O
first	O
is	O
the	O
VGG	Method
architecture	Method
of	O
.	O

Compared	O
to	O
the	O
networks	O
used	O
previously	O
in	O
image	Task
recognition	Task
,	O
this	O
network	O
uses	O
small	O
(	O
3x3	O
)	O
filters	O
,	O
is	O
deeper	O
,	O
and	O
applies	O
up	O
to	O
five	O
convolutional	Method
layers	Method
before	O
pooling	Method
.	O

The	O
second	O
network	O
is	O
modeled	O
on	O
the	O
ResNet	Method
architecture	Method
,	O
which	O
adds	O
highway	O
connections	O
,	O
i.e.	O
a	O
linear	O
transform	O
of	O
each	O
layer	O
’s	O
input	O
to	O
the	O
layer	O
’s	O
output	O
.	O

The	O
only	O
difference	O
is	O
that	O
we	O
apply	O
Batch	Method
Normalization	Method
before	O
computing	O
ReLU	O
activations	O
.	O

The	O
last	O
CNN	Method
variant	Method
is	O
the	O
LACE	Method
(	Method
layer	Method
-	Method
wise	Method
context	Method
expansion	Method
with	Method
attention	Method
)	Method
model	Method
.	O

LACE	Method
is	O
a	O
TDNN	Method
variant	Method
in	O
which	O
each	O
higher	O
layer	O
is	O
a	O
weighted	Method
sum	Method
of	Method
nonlinear	Method
transformations	Method
of	O
a	O
window	O
of	O
lower	O
layer	O
frames	O
.	O

In	O
other	O
words	O
,	O
each	O
higher	O
layer	O
exploits	O
broader	O
context	O
than	O
lower	O
layers	O
.	O

Lower	Method
layers	Method
focus	O
on	O
extracting	O
simple	O
local	O
patterns	O
while	O
higher	O
layers	O
extract	O
complex	O
patterns	O
that	O
cover	O
broader	O
contexts	O
.	O

Since	O
not	O
all	O
frames	O
in	O
a	O
window	O
carry	O
the	O
same	O
importance	O
,	O
an	O
attention	O
mask	O
is	O
applied	O
.	O

The	O
LACE	Method
model	Method
differs	O
from	O
the	O
earlier	O
TDNN	Method
models	Method
e.g.	O
in	O
the	O
use	O
of	O
a	O
learned	O
attention	O
mask	O
and	O
ResNet	O
like	O
linear	O
pass	O
-	O
through	O
.	O

As	O
illustrated	O
in	O
detail	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
model	O
is	O
composed	O
of	O
four	O
blocks	O
,	O
each	O
with	O
the	O
same	O
architecture	O
.	O

Each	O
block	O
starts	O
with	O
a	O
convolution	Method
layer	Method
with	O
stride	O
2	O
which	O
sub	O
-	O
samples	O
the	O
input	O
and	O
increases	O
the	O
number	O
of	O
channels	O
.	O

This	O
layer	O
is	O
followed	O
by	O
four	O
RELU	Method
-	Method
convolution	Method
layers	Method
with	O
jump	O
links	O
similar	O
to	O
those	O
used	O
in	O
ResNet	Method
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
layer	O
structure	O
and	O
parameters	O
of	O
the	O
three	O
CNN	Method
architectures	Method
.	O

We	O
also	O
trained	O
a	O
fused	Method
model	Method
by	O
combining	O
a	O
ResNet	Method
model	Method
and	O
a	O
VGG	Method
model	Method
at	O
the	O
senone	O
posterior	O
level	O
.	O

Both	O
base	O
models	O
are	O
independently	O
trained	O
,	O
and	O
then	O
the	O
score	Metric
fusion	Metric
weight	Metric
is	O
optimized	O
on	O
development	Material
data	Material
.	O

The	O
fused	O
system	O
is	O
our	O
best	O
single	O
system	O
.	O

subsection	O
:	O
LSTMs	Method
While	O
our	O
best	O
performing	O
models	O
are	O
convolutional	Method
,	O
the	O
use	O
of	O
long	Method
short	Method
-	Method
term	Method
memory	Method
networks	Method
(	O
LSTMs	Method
)	O
is	O
a	O
close	O
second	O
.	O

We	O
use	O
a	O
bidirectional	Method
architecture	Method
without	O
frame	Method
-	Method
skipping	Method
.	O

The	O
core	Method
model	Method
structure	Method
is	O
the	O
LSTM	Method
defined	O
in	O
.	O

We	O
found	O
that	O
using	O
networks	Method
with	O
more	O
than	O
six	O
layers	O
did	O
not	O
improve	O
the	O
word	Metric
error	Metric
rate	Metric
on	O
the	O
development	O
set	O
,	O
and	O
chose	O
512	O
hidden	O
units	O
,	O
per	O
direction	O
,	O
per	O
layer	O
,	O
as	O
that	O
provided	O
a	O
reasonable	O
trade	O
-	O
off	O
between	O
training	Metric
time	Metric
and	O
final	O
model	Metric
accuracy	Metric
.	O

subsection	O
:	O
Spatial	Method
Smoothing	Method
Inspired	O
by	O
the	O
human	O
auditory	O
cortex	O
,	O
where	O
neighboring	O
neurons	O
tend	O
to	O
simultaneously	O
activate	O
,	O
we	O
employ	O
a	O
spatial	Method
smoothing	Method
technique	Method
to	O
improve	O
the	O
accuracy	Metric
of	O
our	O
LSTM	Method
models	Method
.	O

The	O
smoothing	Method
is	O
implemented	O
as	O
a	O
regularization	Method
term	Method
on	O
the	O
activations	O
between	O
layers	O
of	O
the	O
acoustic	Method
model	Method
.	O

First	O
,	O
each	O
vector	O
of	O
activations	O
is	O
re	O
-	O
interpreted	O
as	O
a	O
2	Material
-	Material
dimensional	Material
image	Material
.	O

For	O
example	O
,	O
if	O
there	O
are	O
512	O
neurons	O
,	O
they	O
are	O
interpreted	O
as	O
the	O
pixels	O
of	O
a	O
16	O
by	O
32	O
image	O
.	O

Second	O
,	O
this	O
image	O
is	O
high	O
-	O
pass	O
filtered	O
.	O

The	O
filter	O
is	O
implemented	O
as	O
a	O
circular	Method
convolution	Method
with	O
a	O
3	Method
by	Method
3	Method
kernel	Method
.	O

The	O
center	O
tap	O
of	O
the	O
kernel	O
has	O
a	O
value	O
of	O
,	O
and	O
the	O
remaining	O
eight	O
having	O
a	O
value	O
of	O
.	O

Third	O
,	O
the	O
energy	O
of	O
this	O
high	O
-	O
pass	O
filtered	O
image	O
is	O
computed	O
and	O
added	O
to	O
the	O
training	Metric
objective	Metric
function	Metric
.	O

We	O
have	O
found	O
empirically	O
that	O
a	O
suitable	O
scale	O
for	O
this	O
energy	O
is	O
with	O
respect	O
to	O
the	O
existing	O
cross	Metric
entropy	Metric
objective	Metric
function	Metric
.	O

The	O
overall	O
effect	O
of	O
this	O
process	O
is	O
to	O
make	O
the	O
training	Method
algorithm	Method
prefer	O
models	O
that	O
have	O
correlated	O
neurons	O
,	O
and	O
to	O
improve	O
the	O
word	Metric
error	Metric
rate	Metric
of	O
the	O
acoustic	Method
model	Method
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
benefit	O
in	O
error	Metric
rate	Metric
for	O
some	O
of	O
our	O
early	O
systems	O
.	O

We	O
observed	O
error	Metric
reductions	Metric
of	O
between	O
5	O
and	O
10	O
%	O
relative	O
from	O
spatial	Method
smoothing	Method
.	O

section	O
:	O
Speaker	Method
Adaptive	Method
Modeling	Method
Speaker	Method
adaptive	Method
modeling	Method
in	O
our	O
system	O
is	O
based	O
on	O
conditioning	O
the	O
network	O
on	O
an	O
i	Method
-	Method
vector	Method
characterization	Method
of	O
each	O
speaker	O
.	O

A	O
100	O
-	O
dimensional	O
i	O
-	O
vector	O
is	O
generated	O
for	O
each	O
conversation	O
side	O
.	O

For	O
the	O
LSTM	Method
system	Method
,	O
the	O
conversation	O
-	O
side	O
i	O
-	O
vector	O
is	O
appended	O
to	O
each	O
frame	O
of	O
input	O
.	O

For	O
convolutional	Method
networks	Method
,	O
this	O
approach	O
is	O
inappropriate	O
because	O
we	O
do	O
not	O
expect	O
to	O
see	O
spatially	O
contiguous	O
patterns	O
in	O
the	O
input	O
.	O

Instead	O
,	O
for	O
the	O
CNNs	Method
,	O
we	O
add	O
a	O
learnable	O
weight	O
matrix	O
to	O
each	O
layer	O
,	O
and	O
add	O
to	O
the	O
activation	O
of	O
the	O
layer	O
before	O
the	O
nonlinearity	O
.	O

Thus	O
,	O
in	O
the	O
CNN	Method
,	O
the	O
i	O
-	O
vector	O
essentially	O
serves	O
as	O
an	O
speaker	O
-	O
dependent	O
bias	O
to	O
each	O
layer	O
.	O

Note	O
that	O
the	O
i	O
-	O
vectors	O
are	O
estimated	O
using	O
MFCC	O
features	O
;	O
by	O
using	O
them	O
subsequently	O
in	O
systems	O
based	O
on	O
log	O
-	O
filterbank	O
features	O
,	O
we	O
may	O
benefit	O
from	O
a	O
form	O
of	O
feature	Method
combination	Method
.	O

Performance	O
improvements	O
from	O
i	O
-	O
vectors	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
full	O
experimental	O
setup	O
is	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Lattice	Method
-	Method
Free	Method
Sequence	Method
Training	Method
After	O
standard	O
cross	Method
-	Method
entropy	Method
training	Method
,	O
we	O
optimize	O
the	O
model	O
parameters	O
using	O
the	O
maximum	Metric
mutual	Metric
information	Metric
(	Metric
MMI	Metric
)	Metric
objective	Metric
function	Metric
.	O

Denoting	O
a	O
word	O
sequence	O
by	O
and	O
its	O
corresponding	O
acoustic	O
realization	O
by	O
,	O
the	O
training	Metric
criterion	Metric
is	O
As	O
noted	O
in	O
,	O
the	O
necessary	O
gradient	O
for	O
use	O
in	O
backpropagation	Method
is	O
a	O
simple	O
function	O
of	O
the	O
posterior	O
probability	O
of	O
a	O
particular	O
acoustic	O
model	O
state	O
at	O
a	O
given	O
time	O
,	O
as	O
computed	O
by	O
summing	O
over	O
all	O
possible	O
word	O
sequences	O
in	O
an	O
unconstrained	O
manner	O
.	O

As	O
first	O
done	O
in	O
,	O
and	O
more	O
recently	O
in	O
,	O
this	O
can	O
be	O
accomplished	O
with	O
a	O
straightforward	O
alpha	Method
-	Method
beta	Method
computation	Method
over	O
the	O
finite	Method
state	Method
acceptor	Method
representing	O
the	O
decoding	O
search	O
space	O
.	O

In	O
,	O
the	O
search	O
space	O
is	O
taken	O
to	O
be	O
an	O
acceptor	O
representing	O
the	O
composition	O
for	O
a	O
unigram	Method
language	Method
model	Method
on	O
words	O
.	O

In	O
,	O
a	O
language	Method
model	Method
on	O
phonemes	Method
is	O
used	O
instead	O
.	O

In	O
our	O
implementation	O
,	O
we	O
use	O
a	O
mixed	Method
-	Method
history	Method
acoustic	Method
unit	Method
language	Method
model	Method
.	O

In	O
this	O
model	O
,	O
the	O
probability	O
of	O
transitioning	O
into	O
a	O
new	O
context	O
-	O
dependent	O
phonetic	O
state	O
(	O
senone	O
)	O
is	O
conditioned	O
on	O
both	O
the	O
senone	O
and	O
phone	O
history	O
.	O

We	O
found	O
this	O
model	O
to	O
perform	O
better	O
than	O
either	O
purely	O
word	Method
-	Method
based	Method
or	Method
phone	Method
-	Method
based	Method
models	Method
.	O

Based	O
on	O
a	O
set	O
of	O
initial	O
experiments	O
,	O
we	O
developed	O
the	O
following	O
procedure	O
:	O
Perform	O
a	O
forced	Task
alignment	Task
of	O
the	O
training	Material
data	Material
to	O
select	O
lexical	O
variants	O
and	O
determine	O
frame	Material
-	Material
aligned	Material
senone	Material
sequences	Material
.	O

Compress	O
consecutive	O
framewise	O
occurrences	O
of	O
a	O
single	O
senone	O
into	O
a	O
single	O
occurrence	O
.	O

Estimate	O
an	O
unsmoothed	Method
,	Method
variable	Method
-	Method
length	Method
N	Method
-	Method
gram	Method
language	Method
model	Method
from	O
this	O
data	O
,	O
where	O
the	O
history	O
state	O
consists	O
of	O
the	O
previous	O
phone	O
and	O
previous	O
senones	O
within	O
the	O
current	O
phone	O
.	O

To	O
illustrate	O
this	O
,	O
consider	O
the	O
sample	Material
senone	Material
sequence	Material
{	O
s	O
_	O
s2.1288	O
,	O
s	O
_	O
s3.1061	O
,	O
s	O
_	O
s4.1096	O
}	O
,	O
{	O
eh	O
_	O
s2.527	O
,	O
eh	O
_	O
s3.128	O
,	O
eh	O
_	O
s4.66	O
}	O
,	O
{	O
t	O
_	O
s2.729	O
,	O
t	O
_	O
s3.572	O
,	O
t	O
_	O
s4.748}.	O
When	O
predicting	O
the	O
state	O
following	O
eh	O
_	O
s4.66	O
the	O
history	O
consists	O
of	O
(	O
s	O
,	O
eh	O
_	O
s2.527	O
,	O
eh	O
_	O
s3.128	O
,	O
eh	O
_	O
s4.66	O
)	O
,	O
and	O
following	O
t	O
_	O
s2.729	O
,	O
the	O
history	O
is	O
(	O
eh	O
,	O
t	O
_	O
s2.729	O
)	O
.	O

We	O
construct	O
the	O
denominator	O
graph	O
from	O
this	O
language	Method
model	Method
,	O
and	O
HMM	O
transition	O
probabilities	O
as	O
determined	O
by	O
transition	Method
-	Method
counting	Method
in	O
the	O
senone	Material
sequences	Material
found	O
in	O
the	O
training	O
data	O
.	O

Our	O
approach	O
not	O
only	O
largely	O
reduces	O
the	O
complexity	Metric
of	O
building	O
up	O
the	O
language	Method
model	Method
but	O
also	O
provides	O
very	O
reliable	O
training	O
performance	O
.	O

We	O
have	O
found	O
it	O
convenient	O
to	O
do	O
the	O
full	O
computation	O
,	O
without	O
pruning	O
,	O
in	O
a	O
series	O
of	O
matrix	Method
-	Method
vector	Method
operations	Method
on	O
the	O
GPU	O
.	O

The	O
underlying	O
acceptor	O
is	O
represented	O
with	O
a	O
sparse	O
matrix	O
,	O
and	O
we	O
maintain	O
a	O
dense	O
likelihood	O
vector	O
for	O
each	O
time	O
frame	O
.	O

The	O
alpha	Method
and	Method
beta	Method
recursions	Method
are	O
implemented	O
with	O
CUSPARSE	Method
level	Method
-	Method
2	Method
routines	Method
:	O
sparse	Method
-	Method
matrix	Method
,	O
dense	Method
vector	Method
multiplies	Method
.	O

Run	Metric
time	Metric
is	O
about	O
100	O
times	O
faster	O
than	O
real	O
time	O
.	O

As	O
in	O
,	O
we	O
use	O
cross	Method
-	Method
entropy	Method
regularization	Method
.	O

In	O
all	O
the	O
lattice	Method
-	Method
free	Method
MMI	Method
(	Method
LFMMI	Method
)	O
experiments	O
mentioned	O
below	O
we	O
use	O
a	O
trigram	Method
language	Method
model	Method
.	O

Most	O
of	O
the	O
gain	O
is	O
usually	O
obtained	O
after	O
processing	O
24	O
to	O
48	O
hours	O
of	O
data	O
.	O

section	O
:	O
LM	Task
Rescoring	Task
and	O
System	Task
Combination	Task
An	O
initial	O
decoding	Task
is	O
done	O
with	O
a	O
WFST	Method
decoder	Method
,	O
using	O
the	O
architecture	O
described	O
in	O
.	O

We	O
use	O
an	O
N	Method
-	Method
gram	Method
language	Method
model	Method
trained	O
and	O
pruned	O
with	O
the	O
SRILM	Method
toolkit	Method
.	O

The	O
first	Method
-	Method
pass	Method
LM	Method
has	O
approximately	O
15.9	O
million	O
bigrams	O
,	O
trigrams	O
,	O
and	O
4grams	O
,	O
and	O
a	O
vocabulary	O
of	O
30	O
,	O
500	O
words	O
.	O

It	O
gives	O
a	O
perplexity	O
of	O
69	O
on	O
the	O
1997	O
CTS	Material
evaluation	Material
transcripts	Material
.	O

The	O
initial	O
decoding	Method
produces	O
a	O
lattice	O
with	O
the	O
pronunciation	O
variants	O
marked	O
,	O
from	O
which	O
500	O
-	O
best	O
lists	O
are	O
generated	O
for	O
rescoring	Task
purposes	Task
.	O

Subsequent	O
N	Method
-	Method
best	Method
rescoring	Method
uses	O
an	O
unpruned	Method
LM	Method
comprising	O
145	O
million	O
N	O
-	O
grams	O
.	O

All	O
N	Method
-	Method
gram	Method
LMs	Method
were	O
estimated	O
by	O
a	O
maximum	Metric
entropy	Metric
criterion	Metric
as	O
described	O
in	O
.	O

The	O
N	O
-	O
best	O
hypotheses	O
are	O
then	O
rescored	O
using	O
a	O
combination	O
of	O
the	O
large	Method
N	Method
-	Method
gram	Method
LM	Method
and	O
several	O
neural	Method
net	Method
LMs	Method
.	O

We	O
have	O
experimented	O
with	O
both	O
RNN	Method
LMs	Method
and	O
LSTM	Method
LMs	Method
,	O
and	O
describe	O
the	O
details	O
in	O
the	O
following	O
two	O
sections	O
.	O

subsection	O
:	O
RNN	Method
-	Method
LM	Method
setup	Method
Our	O
RNN	Method
-	Method
LMs	Method
are	O
trained	O
and	O
evaluated	O
using	O
the	O
CUED	Method
-	Method
RNNLM	Method
toolkit	Method
.	O

Our	O
RNN	Method
-	Method
LM	Method
configuration	Method
has	O
several	O
distinctive	O
features	O
,	O
as	O
described	O
below	O
.	O

We	O
trained	O
both	O
standard	O
,	O
forward	Method
-	Method
predicting	Method
RNN	Method
-	Method
LMs	Method
and	O
backward	Method
RNN	Method
-	Method
LMs	Method
that	O
predict	O
words	O
in	O
reverse	O
temporal	O
order	O
.	O

The	O
log	O
probabilities	O
from	O
both	O
models	O
are	O
added	O
.	O

As	O
is	O
customary	O
,	O
the	O
RNN	Method
-	Method
LM	Method
probability	Method
estimates	Method
are	O
interpolated	O
at	O
the	O
word	O
-	O
level	O
with	O
corresponding	O
N	O
-	O
gram	O
LM	O
probabilities	O
(	O
separately	O
for	O
the	O
forward	Method
and	Method
backward	Method
models	Method
)	O
.	O

In	O
addition	O
,	O
we	O
trained	O
a	O
second	O
RNN	Method
-	Method
LM	Method
for	O
each	O
direction	O
,	O
obtained	O
by	O
starting	O
with	O
different	O
random	O
initial	O
weights	O
.	O

The	O
two	O
RNN	Method
-	Method
LMs	Method
and	O
the	O
N	Method
-	Method
gram	Method
LM	Method
for	O
each	O
direction	O
are	O
interpolated	O
with	O
weights	O
of	O
(	O
0.375	O
,	O
0.375	O
,	O
0.25	O
)	O
.	O

In	O
order	O
to	O
make	O
use	O
of	O
LM	Material
training	Material
data	Material
that	O
is	O
not	O
fully	O
matched	O
to	O
the	O
target	O
conversational	Material
speech	Material
domain	Material
,	O
we	O
start	O
RNN	Method
-	Method
LM	Method
training	Method
with	O
the	O
union	O
of	O
in	Material
-	Material
domain	Material
(	O
here	O
,	O
CTS	Material
)	O
and	O
out	O
-	O
of	O
-	O
domain	O
(	O
e.g.	O
,	O
Web	Material
)	Material
data	Material
.	O

Upon	O
convergence	O
,	O
the	O
network	O
undergoes	O
a	O
second	O
training	Method
phase	Method
using	O
the	O
in	Material
-	Material
domain	Material
data	Material
only	O
.	O

Both	O
training	O
phases	O
use	O
in	Material
-	Material
domain	Material
validation	Material
data	Material
to	O
regulate	O
the	O
learning	Metric
rate	Metric
schedule	Metric
and	O
termination	O
.	O

Because	O
the	O
size	O
of	O
the	O
out	Material
-	Material
of	Material
-	Material
domain	Material
data	Material
is	O
a	O
multiple	O
of	O
the	O
in	Material
-	Material
domain	Material
data	Material
,	O
a	O
standard	O
training	O
on	O
a	O
simple	O
union	O
of	O
the	O
data	O
would	O
not	O
yield	O
a	O
well	O
-	O
matched	Method
model	Method
,	O
and	O
have	O
poor	O
perplexity	O
in	O
the	O
target	O
domain	O
.	O

We	O
found	O
best	O
results	O
with	O
an	O
RNN	Method
-	Method
LM	Method
configuration	Method
that	O
had	O
a	O
second	O
,	O
non	O
-	O
recurrent	O
hidden	O
layer	O
.	O

This	O
produced	O
lower	O
perplexity	Metric
and	O
word	Metric
error	Metric
than	O
the	O
standard	O
,	O
single	Method
-	Method
hidden	Method
-	Method
layer	Method
RNN	Method
-	Method
LM	Method
architecture	Method
.	O

The	O
overall	O
network	Method
architecture	Method
thus	O
had	O
two	O
hidden	O
layers	O
with	O
1000	O
units	O
each	O
,	O
using	O
ReLU	Method
nonlinearities	Method
.	O

Training	Method
used	O
noise	Method
-	Method
contrastive	Method
estimation	Method
(	O
NCE	Method
)	O
.	O

The	O
RNN	O
-	O
LM	O
output	O
vocabulary	O
consists	O
of	O
all	O
words	O
occurring	O
more	O
than	O
once	O
in	O
the	O
in	Material
-	Material
domain	Material
training	Material
set	Material
.	O

While	O
the	O
RNN	Method
-	Method
LM	Method
estimates	O
a	O
probability	O
for	O
unknown	O
words	O
,	O
we	O
take	O
a	O
different	O
approach	O
in	O
rescoring	Task
:	O
The	O
number	O
of	O
out	O
-	O
of	O
-	O
set	O
words	O
is	O
recorded	O
for	O
each	O
hypothesis	O
and	O
a	O
penalty	O
for	O
them	O
is	O
estimated	O
for	O
them	O
when	O
optimizing	O
the	O
relative	O
weights	O
for	O
all	O
model	O
scores	O
(	O
acoustic	O
,	O
LM	O
,	O
pronunciation	O
)	O
,	O
using	O
the	O
SRILM	Method
nbest	Method
-	Method
optimize	Method
tool	Method
.	O

subsection	O
:	O
LSTM	Method
-	Method
LM	Method
setup	Method
After	O
obtaining	O
good	O
results	O
with	O
RNN	Method
-	Method
LMs	Method
we	O
also	O
explored	O
the	O
LSTM	Method
recurrent	Method
network	Method
architecture	Method
for	O
language	Task
modeling	Task
,	O
inspired	O
by	O
recent	O
work	O
showing	O
gains	O
over	O
RNN	Method
-	Method
LMs	Method
for	O
conversational	Task
speech	Task
recognition	Task
.	O

In	O
addition	O
to	O
applying	O
the	O
lessons	O
learned	O
from	O
our	O
RNN	Method
-	Method
LM	Method
experiments	Method
,	O
we	O
explored	O
additional	O
alternatives	O
,	O
as	O
described	O
below	O
.	O

There	O
are	O
two	O
types	O
of	O
input	O
vectors	O
our	O
LSTM	Method
LMs	Method
take	O
,	O
word	Method
based	Method
one	Method
-	Method
hot	Method
vector	Method
input	Method
and	O
letter	O
trigram	O
vector	O
input	O
.	O

Including	O
both	O
forward	Method
and	Method
backward	Method
models	Method
,	O
we	O
trained	O
four	O
different	O
LSTM	Method
LMs	Method
in	O
total	O
.	O

For	O
the	O
word	Task
based	Task
input	Task
,	O
we	O
leveraged	O
the	O
approach	O
from	O
to	O
tie	O
the	O
input	O
embedding	O
and	O
output	Method
embedding	Method
together	O
.	O

Here	O
we	O
also	O
used	O
a	O
two	O
-	O
phase	Method
training	Method
schedule	Method
to	O
train	O
the	O
LSTM	Method
LMs	Method
.	O

First	O
we	O
train	O
the	O
model	O
on	O
the	O
combination	O
of	O
in	Material
-	Material
domain	Material
and	Material
out	Material
-	Material
domain	Material
data	Material
for	O
four	O
data	O
passes	O
without	O
any	O
learning	Method
rate	Method
adjustment	Method
.	O

We	O
then	O
start	O
from	O
the	O
resulting	O
model	O
and	O
train	O
on	O
in	Material
-	Material
domain	Material
data	Material
until	O
convergence	O
.	O

Overall	O
the	O
letter	Method
trigram	Method
based	Method
models	Method
perform	O
a	O
little	O
better	O
than	O
the	O
word	Method
based	Method
language	Method
model	Method
.	O

We	O
tried	O
applying	O
dropout	Method
on	O
both	O
types	O
of	O
language	Method
models	Method
but	O
did	O
n’t	O
see	O
an	O
improvement	O
.	O

Convergence	Task
was	O
improved	O
through	O
a	O
variation	O
of	O
self	Method
-	Method
stabilization	Method
,	O
in	O
which	O
each	O
output	O
vector	O
of	O
non	O
-	O
linearities	O
are	O
scaled	O
by	O
,	O
where	O
a	O
is	O
a	O
scalar	O
that	O
is	O
learned	O
for	O
each	O
output	O
.	O

This	O
has	O
a	O
similar	O
effect	O
as	O
the	O
scale	O
of	O
the	O
well	O
-	O
known	O
batch	Method
normalization	Method
technique	Method
,	O
but	O
can	O
be	O
used	O
in	O
recurrent	O
loops	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
impact	O
of	O
number	O
of	O
layers	O
on	O
the	O
final	O
perplexities	Metric
.	O

Based	O
on	O
this	O
,	O
we	O
proceeded	O
with	O
three	O
hidden	Method
layers	Method
,	O
with	O
1000	O
hidden	O
units	O
each	O
.	O

The	O
perplexities	O
of	O
each	O
LSTM	Method
-	Method
LM	Method
we	O
used	O
in	O
the	O
final	O
combination	O
(	O
before	O
interpolating	O
with	O
the	O
N	Method
-	Method
gram	Method
model	Method
)	O
can	O
be	O
found	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
the	O
final	O
system	O
,	O
we	O
interpolated	O
two	O
LSTM	Method
-	Method
LMs	Method
with	O
an	O
N	Method
-	Method
gram	Method
LM	Method
for	O
the	O
forward	Task
-	Task
direction	Task
LM	Task
,	O
and	O
similarly	O
for	O
the	O
backward	Task
-	Task
direction	Task
LM	Task
.	O

All	O
LSTMs	Method
use	O
three	O
hidden	Method
layers	Method
and	O
are	O
trained	O
on	O
in	Material
-	Material
domain	Material
and	Material
web	Material
data	Material
.	O

Unlike	O
for	O
the	O
RNN	Method
-	Method
LMs	Method
,	O
the	O
two	O
models	O
being	O
interpolated	O
differ	O
not	O
just	O
in	O
their	O
random	Method
initialization	Method
,	O
but	O
also	O
in	O
the	O
input	O
encoding	O
(	O
one	O
uses	O
a	O
triletter	Method
encoding	Method
,	O
the	O
other	O
a	O
one	Method
-	Method
hot	Method
word	Method
encoding	Method
)	O
.	O

The	O
forward	O
and	O
backward	O
LM	O
log	O
probability	O
scores	O
are	O
combined	O
additively	O
.	O

subsection	O
:	O
Training	O
data	O
The	O
4	Method
-	Method
gram	Method
language	Method
model	Method
for	O
decoding	Task
was	O
trained	O
on	O
the	O
available	O
CTS	Material
transcripts	Material
from	O
the	O
DARPA	Material
EARS	Material
program	Material
:	O
Switchboard	Material
(	O
3	O
M	O
words	O
)	O
,	O
BBN	Material
Switchboard	Material
-	Material
2	Material
transcripts	Material
(	O
850k	O
)	O
,	O
Fisher	O
(	O
21	O
M	O
)	O
,	O
English	Material
CallHome	Material
(	O
200k	O
)	O
,	O
and	O
the	O
University	Material
of	Material
Washington	Material
conversational	Material
Web	Material
corpus	Material
(	O
191	O
M	O
)	O
.	O

A	O
separate	O
N	Method
-	Method
gram	Method
model	Method
was	O
trained	O
from	O
each	O
source	O
and	O
interpolated	O
with	O
weights	O
optimized	O
on	O
RT	Material
-	Material
03	Material
transcripts	Material
.	O

For	O
the	O
unpruned	Task
large	Task
rescoring	Task
4	Task
-	Task
gram	Task
,	O
an	O
additional	O
LM	Method
component	Method
was	O
added	O
,	O
trained	O
on	O
133	O
M	O
word	O
of	O
LDC	Material
Broadcast	Material
News	Material
texts	Material
.	O

The	O
N	Method
-	Method
gram	Method
LM	Method
configuration	Method
is	O
modeled	O
after	O
that	O
described	O
in	O
,	O
except	O
that	O
maxent	Method
smoothing	Method
was	O
used	O
.	O

The	O
RNN	Method
and	Method
LSTM	Method
LMs	Method
were	O
trained	O
on	O
Switchboard	Material
and	Material
Fisher	Material
transcripts	Material
as	O
in	Material
-	Material
domain	Material
data	Material
(	O
20	O
M	O
words	O
for	O
gradient	Task
computation	Task
,	O
3	O
M	O
for	O
validation	Task
)	O
.	O

To	O
this	O
we	O
added	O
62	O
M	O
words	O
of	O
UW	Material
Web	Material
data	Material
as	O
out	O
-	O
of	O
-	O
domain	Material
data	Material
,	O
for	O
use	O
in	O
the	O
two	O
-	O
phase	O
training	Method
procedure	Method
described	O
above	O
.	O

subsection	O
:	O
RNN	Method
-	Method
LM	Method
and	Method
LSTM	Method
-	Method
LM	Method
performance	O
Table	O
[	O
reference	O
]	O
gives	O
perplexity	Metric
and	O
word	Metric
error	Metric
performance	Metric
for	O
various	O
recurrent	Method
neural	Method
net	Method
LM	Method
setups	Method
,	O
from	O
simple	O
to	O
more	O
complex	O
.	O

The	O
acoustic	Method
model	Method
used	O
was	O
the	O
ResNet	Method
CNN	Method
.	O

Note	O
that	O
,	O
unlike	O
the	O
results	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
the	O
neural	Method
net	Method
LMs	Method
in	O
Table	O
[	O
reference	O
]	O
are	O
interpolated	O
with	O
the	O
N	Method
-	Method
gram	Method
LM	Method
.	O

As	O
can	O
be	O
seen	O
,	O
each	O
of	O
the	O
measures	O
described	O
earlier	O
adds	O
incremental	O
gains	O
,	O
which	O
,	O
however	O
,	O
add	O
up	O
to	O
a	O
substantial	O
improvement	O
overall	O
.	O

The	O
total	O
gain	O
relative	O
to	O
a	O
purely	O
N	Method
-	Method
gram	Method
based	Method
system	Method
is	O
a	O
20	O
%	O
relative	Metric
error	Metric
reduction	Metric
with	O
RNN	Method
-	Method
LMs	Method
,	O
and	O
23	O
%	O
with	O
LSTM	Method
-	Method
LMs	Method
.	O

As	O
shown	O
later	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
the	O
gains	O
with	O
different	O
acoustic	Method
models	Method
are	O
similar	O
.	O

subsection	O
:	O
System	Task
Combination	Task
The	O
LM	Method
rescoring	Method
is	O
carried	O
out	O
separately	O
for	O
each	O
acoustic	Method
model	Method
.	O

The	O
rescored	O
N	O
-	O
best	O
lists	O
from	O
each	O
subsystem	O
are	O
then	O
aligned	O
into	O
a	O
single	O
confusion	Method
network	Method
using	O
the	O
SRILM	Method
nbest	Method
-	Method
rover	Method
tool	Method
.	O

However	O
,	O
the	O
number	O
of	O
potential	O
candidate	O
systems	O
is	O
too	O
large	O
to	O
allow	O
an	O
all	O
-	O
out	O
combination	O
,	O
both	O
for	O
practical	O
reasons	O
and	O
due	O
to	O
overfitting	Task
issues	Task
.	O

Instead	O
,	O
we	O
perform	O
a	O
greedy	Method
search	Method
,	O
starting	O
with	O
the	O
single	O
best	O
system	O
,	O
and	O
successively	O
adding	O
additional	O
systems	O
,	O
to	O
find	O
a	O
small	O
set	O
of	O
systems	O
that	O
are	O
maximally	O
complementary	O
.	O

The	O
RT	O
-	O
02	O
Switchboard	O
set	O
was	O
used	O
for	O
this	O
search	Method
procedure	Method
.	O

We	O
experimented	O
with	O
two	O
search	Method
algorithms	Method
to	O
find	O
good	O
subsets	O
of	O
systems	O
.	O

We	O
always	O
start	O
with	O
the	O
system	O
giving	O
the	O
best	O
individual	O
accuracy	Metric
on	O
the	O
development	O
set	O
.	O

In	O
one	O
approach	O
,	O
a	O
greedy	Method
forward	Method
search	Method
then	O
adds	O
systems	O
incrementally	O
to	O
the	O
combination	O
,	O
giving	O
each	O
equal	O
weight	O
.	O

If	O
no	O
improvement	O
is	O
found	O
with	O
any	O
of	O
the	O
unused	O
systems	O
,	O
we	O
try	O
adding	O
each	O
with	O
successively	O
lower	O
relative	O
weights	O
of	O
0.5	O
,	O
0.2	O
,	O
and	O
0.1	O
,	O
and	O
stop	O
if	O
none	O
of	O
these	O
give	O
an	O
improvement	O
.	O

A	O
second	O
variant	O
of	O
the	O
search	Method
procedure	Method
that	O
can	O
give	O
lower	O
error	Metric
(	O
as	O
measured	O
on	O
the	O
devset	Method
)	O
estimates	O
the	O
best	O
system	O
weights	O
for	O
each	O
incremental	O
combination	O
candidate	O
.	O

The	O
weight	Method
estimation	Method
is	O
done	O
using	O
an	O
expectation	Method
-	Method
maximization	Method
algorithm	Method
based	O
on	O
aligning	O
the	O
reference	O
words	O
to	O
the	O
confusion	O
networks	O
,	O
and	O
maximizing	O
the	O
weighted	O
probability	O
of	O
the	O
correct	O
word	O
at	O
each	O
alignment	O
position	O
.	O

To	O
avoid	O
overfitting	O
,	O
the	O
weights	O
for	O
an	O
-	O
way	O
combination	O
are	O
smoothed	O
hierarchically	O
,	O
i.e.	O
,	O
interpolated	O
with	O
the	O
weights	O
from	O
the	O
-	Method
way	Method
system	Method
that	O
preceded	O
it	O
.	O

This	O
tends	O
to	O
give	O
robust	O
weights	O
that	O
are	O
biased	O
toward	O
the	O
early	O
(	O
i.e.	O
,	O
better	O
)	O
subsystems	O
.	O

The	O
final	O
system	O
incorporated	O
a	O
variety	O
of	O
BLSTM	Method
models	Method
with	O
roughly	O
similar	O
performance	O
,	O
but	O
differing	O
in	O
various	O
metaparameters	O
(	O
number	O
of	O
senones	O
,	O
use	O
of	O
spatial	Method
smoothing	Method
,	O
and	O
choice	O
of	O
pronunciation	O
dictionaries	O
)	O
.	O

To	O
further	O
limit	O
the	O
number	O
of	O
free	O
parameters	O
to	O
be	O
estimated	O
in	O
system	Task
combination	Task
,	O
we	O
performed	O
system	Method
selection	Method
in	O
two	O
stages	O
.	O

First	O
,	O
we	O
selected	O
the	O
four	O
best	O
BLSTM	Method
systems	Method
.	O

We	O
then	O
combined	O
these	O
with	O
equal	O
weights	O
and	O
treated	O
them	O
as	O
a	O
single	O
subsystem	O
in	O
searching	O
for	O
a	O
larger	O
combination	O
including	O
other	O
acoustic	Method
models	Method
.	O

This	O
yielded	O
our	O
best	O
overall	O
combined	O
system	O
,	O
as	O
reported	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Microsoft	Method
Cognitive	Method
Toolkit	Method
(	O
CNTK	Method
)	O
All	O
neural	Method
networks	Method
in	O
the	O
final	O
system	O
were	O
trained	O
with	O
the	O
Microsoft	Method
Cognitive	Method
Toolkit	Method
,	O
or	O
CNTK	Method
.	O

on	O
a	O
Linux	O
-	O
based	O
multi	Method
-	Method
GPU	Method
server	Method
farm	Method
.	O

CNTK	Method
allows	O
for	O
flexible	O
model	Task
definition	Task
,	O
while	O
at	O
the	O
same	O
time	O
scaling	O
very	O
efficiently	O
across	O
multiple	O
GPUs	O
and	O
multiple	O
servers	O
.	O

The	O
resulting	O
fast	O
experimental	O
turnaround	O
using	O
the	O
full	O
2000	Material
-	Material
hour	Material
corpus	Material
was	O
critical	O
for	O
our	O
work	O
.	O

subsection	O
:	O
Flexible	O
,	O
Terse	Method
Model	Method
Definition	O
In	O
CNTK	Method
,	O
a	O
neural	Method
network	Method
(	O
and	O
the	O
training	O
criteria	O
)	O
are	O
specified	O
by	O
its	O
formula	O
,	O
using	O
a	O
custom	O
functional	O
language	O
(	O
BrainScript	Method
)	O
,	O
or	O
Python	Method
.	O

A	O
graph	Method
-	Method
based	Method
execution	Method
engine	Method
,	O
which	O
provides	O
automatic	Task
differentiation	Task
,	O
then	O
trains	O
the	O
model	O
’s	O
parameters	O
through	O
SGD	Method
.	O

Leveraging	O
a	O
stock	O
library	O
of	O
common	O
layer	O
types	O
,	O
networks	O
can	O
be	O
specified	O
very	O
tersely	O
.	O

Samples	O
can	O
be	O
found	O
in	O
.	O

subsection	O
:	O
Multi	Task
-	Task
Server	Task
Training	Task
using	O
1	Method
-	Method
bit	Method
SGD	Method
Training	O
the	O
acoustic	Method
models	Method
in	O
this	O
paper	O
on	O
a	O
single	O
GPU	Method
would	O
take	O
many	O
weeks	O
or	O
even	O
months	O
.	O

CNTK	Method
made	O
training	O
times	O
feasible	O
by	O
parallelizing	O
the	O
SGD	Method
training	Method
with	O
our	O
1	Method
-	Method
bit	Method
SGD	Method
parallelization	Method
technique	Method
.	O

This	O
data	Method
-	Method
parallel	Method
method	Method
distributes	O
minibatches	Method
over	O
multiple	O
worker	O
nodes	O
,	O
and	O
then	O
aggregates	O
the	O
sub	O
-	O
gradients	O
.	O

While	O
the	O
necessary	O
communication	Metric
time	Metric
would	O
otherwise	O
be	O
prohibitive	O
,	O
the	O
1	Method
-	Method
bit	Method
SGD	Method
method	Method
eliminates	O
the	O
bottleneck	O
by	O
two	O
techniques	O
:	O
1	Method
-	Method
bit	Method
quantization	Method
of	Method
gradients	Method
and	O
automatic	Method
minibatch	Method
-	Method
size	Method
scaling	Method
.	O

In	O
,	O
we	O
showed	O
that	O
gradient	O
values	O
can	O
be	O
quantized	O
to	O
just	O
a	O
single	O
bit	O
,	O
if	O
one	O
carries	O
over	O
the	O
quantization	O
error	O
from	O
one	O
minibatch	O
to	O
the	O
next	O
.	O

Each	O
time	O
a	O
sub	O
-	O
gradient	O
is	O
quantized	O
,	O
the	O
quantization	O
error	O
is	O
computed	O
and	O
remembered	O
,	O
and	O
then	O
added	O
to	O
the	O
next	O
minibatch	O
’s	O
sub	O
-	O
gradient	O
.	O

This	O
reduces	O
the	O
required	O
bandwidth	O
32	O
-	O
fold	O
with	O
minimal	O
loss	O
in	O
accuracy	Metric
.	O

Secondly	O
,	O
automatic	Method
minibatch	Method
-	Method
size	Method
scaling	Method
progressively	O
decreases	O
the	O
frequency	O
of	O
model	Method
updates	Method
.	O

At	O
regular	O
intervals	O
(	O
e.g.	O
every	O
72h	O
of	O
training	O
data	O
)	O
,	O
the	O
trainer	O
tries	O
larger	O
minibatch	O
sizes	O
on	O
a	O
small	O
subset	O
of	O
data	O
and	O
picks	O
the	O
largest	O
that	O
maintains	O
training	O
loss	O
.	O

These	O
two	O
techniques	O
allow	O
for	O
excellent	O
multi	O
-	O
GPU	O
/	O
server	O
scalability	O
,	O
and	O
reduced	O
the	O
acoustic	Method
-	Method
model	Method
training	Method
times	Method
on	O
2000h	O
from	O
months	O
to	O
between	O
1	O
and	O
3	O
weeks	O
,	O
making	O
this	O
work	O
feasible	O
.	O

subsection	O
:	O
Computational	Metric
performance	Metric
Table	O
[	O
reference	O
]	O
compares	O
the	O
runtimes	Metric
,	O
as	O
multiples	O
of	O
speech	O
duration	O
,	O
of	O
various	O
processing	Method
steps	Method
associated	O
with	O
the	O
different	O
acoustic	Method
model	Method
architectures	Method
(	O
figures	O
for	O
DNNs	Method
are	O
given	O
only	O
as	O
a	O
reference	O
point	O
,	O
since	O
they	O
are	O
not	O
used	O
in	O
our	O
system	O
)	O
.	O

Acoustic	Method
model	Method
(	O
AM	Method
)	Method
training	Method
comprises	O
the	O
forward	Method
and	Method
backward	Method
dynamic	Method
programming	Method
passes	Method
,	O
as	O
well	O
as	O
parameter	Method
updates	Method
.	O

AM	Task
evaluation	Task
refers	O
to	O
the	O
forward	Task
computation	Task
only	O
.	O

Decoding	Method
includes	O
AM	Task
evaluation	Task
along	O
with	O
hypothesis	Task
search	Task
(	O
only	O
the	O
former	O
makes	O
use	O
of	O
the	O
GPU	O
)	O
.	O

Runtimes	Metric
were	O
measured	O
on	O
a	O
12	O
-	O
core	O
Intel	O
Xeon	O
E5	O
-	O
2620v3	O
CPU	O
clocked	O
at	O
2.4GHz	O
,	O
with	O
an	O
Nvidia	O
Titan	O
X	O
GPU	O
.	O

We	O
observe	O
that	O
the	O
GPU	Method
gives	O
a	O
10	O
to	O
100	O
-	O
fold	O
speedup	O
for	O
AM	Task
evaluation	Task
over	O
the	O
CPU	Method
implementation	Method
.	O

AM	Task
evaluation	Task
is	O
thus	O
reduced	O
to	O
a	O
small	O
faction	O
of	O
overall	O
decoding	Metric
time	Metric
,	O
making	O
near	Task
-	Task
realtime	Task
operation	Task
possible	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Speech	Material
corpora	Material
We	O
train	O
with	O
the	O
commonly	O
used	O
English	Material
CTS	Material
(	O
Switchboard	Material
and	Material
Fisher	Material
)	Material
corpora	Material
.	O

Evaluation	O
is	O
carried	O
out	O
on	O
the	O
NIST	Material
2000	Material
CTS	Material
test	Material
set	Material
,	O
which	O
comprises	O
both	O
Switchboard	Material
(	Material
SWB	Material
)	O
and	O
CallHome	Material
(	Material
CH	Material
)	O
subsets	O
.	O

The	O
waveforms	O
were	O
segmented	O
according	O
to	O
the	O
NIST	Material
partitioned	Material
evaluation	Material
map	Material
(	Material
PEM	Material
)	Material
file	Material
,	O
with	O
150ms	O
of	O
dithered	O
silence	O
padding	O
added	O
in	O
the	O
case	O
of	O
the	O
CallHome	Material
conversations	Material
.	O

The	O
Switchboard	O
-	O
1	O
portion	O
of	O
the	O
NIST	Material
2002	Material
CTS	Material
test	Material
set	Material
was	O
used	O
for	O
tuning	Task
and	O
development	Task
.	O

The	O
acoustic	Material
training	Material
data	Material
is	O
comprised	O
by	O
LDC	Material
corpora	Material
97S62	O
,	O
2004S13	O
,	O
2005S13	O
,	O
2004S11	O
and	O
2004S09	O
;	O
see	O
for	O
a	O
full	O
description	O
.	O

subsection	O
:	O
Acoustic	Method
Model	Method
Details	O
Forty	O
-	O
dimensional	O
log	O
-	O
filterbank	O
features	O
were	O
extracted	O
every	O
10	O
milliseconds	O
,	O
using	O
a	O
25	O
-	O
millisecond	O
analysis	O
window	O
.	O

The	O
CNN	Method
models	Method
used	O
window	O
sizes	O
as	O
indicated	O
in	O
Table	O
[	O
reference	O
]	O
,	O
and	O
the	O
LSTMs	Method
processed	O
one	O
frame	O
of	O
input	O
at	O
a	O
time	O
.	O

The	O
bulk	O
of	O
our	O
models	O
use	O
three	O
state	Method
left	Method
-	Method
to	Method
-	Method
right	Method
triphone	Method
models	Method
with	O
9000	O
tied	O
states	O
.	O

Additionally	O
,	O
we	O
have	O
trained	O
several	O
models	O
with	O
27k	O
tied	O
states	O
.	O

The	O
phonetic	Material
inventory	Material
includes	O
special	O
models	O
for	O
noise	Material
,	O
vocalized	Material
-	Material
noise	Material
,	O
laughter	Material
and	O
silence	Material
.	O

We	O
use	O
a	O
30k	O
-	O
vocabulary	O
derived	O
from	O
the	O
most	O
common	O
words	O
in	O
the	O
Switchboard	Material
and	Material
Fisher	Material
corpora	Material
.	O

The	O
decoder	Method
uses	O
a	O
statically	O
compiled	O
unigram	O
graph	O
,	O
and	O
dynamically	O
applies	O
the	O
language	Method
model	Method
score	Method
.	O

The	O
unigram	Method
graph	Method
has	O
about	O
300k	O
states	O
and	O
500k	O
arcs	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
result	O
of	O
i	Method
-	Method
vector	Method
adaptation	Method
and	O
LFMMI	Method
training	Method
on	O
several	O
of	O
our	O
early	O
systems	O
.	O

We	O
achieve	O
a	O
5–8	O
%	O
relative	O
improvement	O
from	O
i	Metric
-	Metric
vectors	Metric
,	O
including	O
on	O
CNN	Method
systems	Method
.	O

The	O
last	O
row	O
of	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
effect	O
of	O
LFMMI	Method
training	Method
on	O
the	O
different	O
models	O
.	O

We	O
see	O
a	O
consistent	O
7–10	O
%	O
further	O
relative	O
reduction	O
in	O
error	Metric
rate	Metric
for	O
all	O
models	O
.	O

Considering	O
the	O
great	O
increase	O
in	O
procedural	Metric
simplicity	Metric
of	O
LFMMI	Method
over	O
the	O
previous	O
practice	O
of	O
writing	Task
lattices	Task
and	O
post	O
-	O
processing	O
them	O
,	O
we	O
consider	O
LFMMI	Method
to	O
be	O
a	O
significant	O
advance	O
in	O
technology	O
.	O

subsection	O
:	O
Overall	O
Results	O
and	O
Discussion	O
The	O
performance	O
of	O
all	O
our	O
component	Method
models	Method
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
along	O
with	O
the	O
BLSTM	Method
combination	Method
and	O
full	Method
system	Method
combination	Method
results	O
.	O

(	O
Recall	O
that	O
the	O
four	O
best	O
BLSTM	Method
systems	Method
are	O
combined	O
with	O
equal	O
weights	O
first	O
,	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

)	O
Key	O
benchmarks	O
from	O
the	O
literature	O
,	O
our	O
own	O
best	O
results	O
,	O
and	O
the	O
measured	O
human	Metric
error	Metric
rates	Metric
are	O
compared	O
in	O
Table	O
[	O
reference	O
]	O
.	O

All	O
models	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
are	O
selected	O
for	O
the	O
combined	O
systems	O
for	O
one	O
or	O
more	O
of	O
the	O
three	O
rescoring	Method
LMs	Method
.	O

The	O
only	O
exception	O
is	O
the	O
VGG	Method
+	Method
ResNet	Method
system	Method
,	O
which	O
combines	O
acoustic	O
senone	O
posteriors	O
from	O
the	O
VGG	Method
and	Method
ResNet	Method
networks	Method
.	O

While	O
this	O
yields	O
our	O
single	O
best	O
acoustic	Method
model	Method
,	O
only	O
the	O
individual	O
VGG	Method
and	Method
ResNet	Method
models	Method
are	O
used	O
in	O
the	O
overall	O
system	O
combination	O
.	O

We	O
also	O
observe	O
that	O
the	O
four	O
model	O
variants	O
chosen	O
for	O
the	O
combined	O
BLSTM	Method
subsystem	Method
differ	O
incrementally	O
by	O
one	O
hyperparameter	O
(	O
smoothing	Method
,	O
number	O
of	O
senones	O
,	O
dictionary	O
)	O
,	O
and	O
that	O
the	O
BLSTMs	Method
alone	O
achieve	O
an	O
error	Metric
that	O
is	O
within	O
3	O
%	O
relative	O
of	O
the	O
full	O
system	O
combination	O
.	O

This	O
validates	O
the	O
rationale	O
that	O
choosing	O
different	O
hyperparameters	O
is	O
an	O
effective	O
way	O
to	O
obtain	O
complementary	O
systems	O
for	O
combination	Task
purposes	Task
.	O

We	O
also	O
assessed	O
the	O
lower	Metric
bound	Metric
of	O
performance	O
for	O
our	O
lattice	Method
/	Method
N	Method
-	Method
best	Method
rescoring	Method
paradigm	Method
.	O

The	O
500	O
-	O
best	O
lists	O
from	O
the	O
lattices	O
generated	O
with	O
the	O
ResNet	Method
CNN	Method
system	Method
had	O
an	O
oracle	O
(	O
lowest	O
achievable	O
)	O
WER	Metric
of	O
2.7	O
%	O
on	O
the	O
Switchboard	O
portion	O
of	O
the	O
NIST	Metric
2000	Metric
evaluation	Metric
set	Metric
,	O
and	O
an	O
oracle	Metric
WER	Metric
of	O
4.9	O
%	O
on	O
the	O
CallHome	O
portion	O
.	O

The	O
oracle	Metric
error	Metric
of	O
the	O
combined	O
system	O
is	O
even	O
lower	O
(	O
though	O
harder	O
to	O
quantify	O
)	O
since	O
(	O
1	O
)	O
N	O
-	O
best	O
output	O
from	O
all	O
systems	O
are	O
combined	O
and	O
(	O
2	O
)	O
confusion	Method
network	Method
construction	Method
generates	O
new	O
possible	O
hypotheses	O
not	O
contained	O
in	O
the	O
original	O
N	O
-	O
best	O
lists	O
.	O

With	O
oracle	Metric
error	Metric
rates	Metric
less	O
than	O
half	O
the	O
currently	O
achieved	O
actual	O
error	Metric
rates	Metric
,	O
we	O
conclude	O
that	O
search	O
errors	O
are	O
not	O
a	O
major	O
limiting	O
factor	O
to	O
even	O
better	O
accuracy	Metric
.	O

section	O
:	O
Error	Task
Analysis	Task
In	O
this	O
section	O
,	O
we	O
compare	O
the	O
errors	O
made	O
by	O
our	O
artificial	Method
recognizer	Method
with	O
those	O
made	O
by	O
human	O
transcribers	O
.	O

We	O
find	O
that	O
the	O
machine	Metric
errors	Metric
are	O
substantially	O
the	O
same	O
as	O
human	O
ones	O
,	O
with	O
one	O
large	O
exception	O
:	O
confusions	O
between	O
backchannel	O
words	O
and	O
hesitations	O
.	O

The	O
distinction	O
is	O
that	O
backchannel	O
words	O
like	O
“	O
uh	O
-	O
huh	O
”	O
are	O
an	O
acknowledgment	O
of	O
the	O
speaker	O
,	O
also	O
signaling	O
that	O
the	O
speaker	O
should	O
keep	O
talking	O
,	O
while	O
hesitations	O
like	O
“	O
uh	O
”	O
are	O
used	O
to	O
indicate	O
that	O
the	O
current	O
speaker	O
has	O
more	O
to	O
say	O
and	O
wants	O
to	O
keep	O
his	O
or	O
her	O
turn	O
.	O

As	O
turn	Task
-	Task
management	Task
devices	Task
,	O
these	O
two	O
classes	O
of	O
words	O
therefore	O
have	O
exactly	O
opposite	O
functions	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
ten	O
most	O
common	O
substitutions	O
for	O
both	O
humans	O
and	O
the	O
artificial	Method
system	Method
.	O

Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
do	O
the	O
same	O
for	O
deletions	O
and	O
insertions	O
.	O

Focusing	O
on	O
the	O
substitutions	O
,	O
we	O
see	O
that	O
by	O
far	O
the	O
most	O
common	O
error	O
in	O
the	O
ASR	Method
system	Method
is	O
the	O
confusion	O
of	O
a	O
hesitation	O
in	O
the	O
reference	O
for	O
a	O
backchannel	O
in	O
the	O
hypothesis	O
.	O

People	O
do	O
not	O
seem	O
to	O
have	O
this	O
problem	O
.	O

We	O
speculate	O
that	O
this	O
is	O
due	O
to	O
the	O
nature	O
of	O
the	O
Fisher	Material
training	Material
corpus	Material
,	O
where	O
the	O
“	O
quick	O
transcription	O
”	O
guidelines	O
were	O
predominately	O
used	O
.	O

We	O
find	O
that	O
there	O
is	O
inconsistent	O
treatment	O
of	O
backchannel	O
and	O
hesitation	O
in	O
the	O
resulting	O
data	O
;	O
the	O
relatively	O
poor	O
performance	O
of	O
the	O
automatic	Method
system	Method
here	O
might	O
simply	O
be	O
due	O
to	O
confusions	O
in	O
the	O
training	O
data	O
annotations	O
.	O

For	O
perspective	O
,	O
there	O
are	O
over	O
twenty	O
-	O
one	O
thousand	O
words	O
in	O
each	O
test	O
set	O
.	O

Thus	O
the	O
errors	O
due	O
to	O
hesitation	O
/	O
backchannel	O
substitutions	O
account	O
for	O
an	O
error	Metric
rate	Metric
of	O
only	O
about	O
0.2	O
%	O
absolute	O
.	O

The	O
most	O
frequent	O
substitution	O
for	O
people	O
on	O
the	O
Switchboard	Material
corpus	Material
was	O
mistaking	O
a	O
hesitation	O
in	O
the	O
reference	O
for	O
the	O
word	O
“	O
hmm	O
.	O

”	O
The	O
scoring	O
guidelines	O
treat	O
“	O
hmm	O
”	O
as	O
a	O
word	O
distinct	O
from	O
backchannels	O
and	O
hesitations	O
,	O
so	O
this	O
is	O
not	O
a	O
scoring	O
mistake	O
.	O

Examination	O
of	O
the	O
contexts	O
in	O
which	O
the	O
error	O
is	O
made	O
show	O
that	O
it	O
is	O
most	O
often	O
intended	O
to	O
acknowledge	O
the	O
other	O
speaker	O
,	O
i.e.	O
as	O
a	O
backchannel	O
.	O

For	O
both	O
people	O
and	O
our	O
automated	O
system	O
,	O
the	O
insertion	O
and	O
deletion	O
patterns	O
are	O
similar	O
:	O
short	O
function	O
words	O
are	O
by	O
far	O
the	O
most	O
frequent	O
errors	O
.	O

In	O
particular	O
,	O
the	O
single	O
most	O
common	O
error	O
made	O
by	O
the	O
transcribers	O
was	O
to	O
omit	O
the	O
word	O
“	O
I.	O
”	O
While	O
we	O
believe	O
further	O
improvement	O
in	O
function	O
and	O
content	O
words	O
is	O
possible	O
,	O
the	O
significance	O
of	O
the	O
remaining	O
backchannel	Task
/	Task
hesitation	Task
confusions	Task
is	O
unclear	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
overall	O
error	Metric
rates	Metric
broken	O
down	O
by	O
substitutions	O
,	O
insertions	O
and	O
deletions	O
.	O

We	O
see	O
that	O
the	O
human	O
transcribers	O
have	O
a	O
somewhat	O
lower	O
substitution	Metric
rate	Metric
,	O
and	O
a	O
higher	O
deletion	Metric
rate	Metric
.	O

The	O
relatively	O
higher	O
deletion	Metric
rate	Metric
might	O
reflect	O
a	O
human	O
bias	O
to	O
avoid	O
outputting	O
uncertain	O
information	O
,	O
or	O
the	O
productivity	O
demands	O
on	O
a	O
professional	O
transcriber	O
.	O

In	O
all	O
cases	O
,	O
the	O
number	O
of	O
insertions	O
is	O
relatively	O
small	O
.	O

section	O
:	O
Relation	O
to	O
Prior	O
Work	O
Compared	O
to	O
earlier	O
applications	O
of	O
CNNs	Method
to	O
speech	Task
recognition	Task
,	O
our	O
networks	O
are	O
much	O
deeper	O
,	O
and	O
use	O
linear	O
bypass	O
connections	O
across	O
convolutional	Method
layers	Method
.	O

They	O
are	O
similar	O
in	O
spirit	O
to	O
those	O
studied	O
more	O
recently	O
by	O
.	O

We	O
improve	O
on	O
these	O
architectures	O
with	O
the	O
LACE	Method
model	Method
,	O
which	O
iteratively	O
expands	O
the	O
effective	O
window	O
size	O
,	O
layer	O
-	O
by	O
-	O
layer	O
,	O
and	O
adds	O
an	O
attention	O
mask	O
to	O
differentially	O
weight	O
distant	O
context	O
.	O

Our	O
spatial	Method
regularization	Method
technique	Method
is	O
similar	O
in	O
spirit	O
to	O
stimulated	O
deep	Method
neural	Method
networks	Method
.	O

Whereas	O
stimulated	Method
networks	Method
use	O
a	O
supervision	O
signal	O
to	O
encourage	O
locality	O
of	O
activations	O
in	O
the	O
model	O
,	O
our	O
technique	O
is	O
automatic	O
.	O

Our	O
use	O
of	O
lattice	Method
-	Method
free	Method
MMI	Method
is	O
distinctive	O
,	O
and	O
extends	O
previous	O
work	O
by	O
proposing	O
the	O
use	O
of	O
a	O
mixed	O
triphone	O
/	O
phoneme	O
history	O
in	O
the	O
language	Method
model	Method
.	O

On	O
the	O
language	Task
modeling	Task
side	Task
,	O
we	O
achieve	O
a	O
performance	O
boost	O
by	O
combining	O
multiple	O
LSTM	Method
-	Method
LMs	Method
in	O
both	O
forward	O
and	O
backward	O
directions	O
,	O
and	O
by	O
using	O
a	O
two	O
-	O
phase	Method
training	Method
regimen	Method
to	O
get	O
best	O
results	O
from	O
out	Material
-	Material
of	Material
-	Material
domain	Material
data	Material
.	O

For	O
our	O
best	O
CNN	Method
system	Method
,	O
LSTM	Method
-	Method
LM	Method
rescoring	Method
yields	O
a	O
relative	Metric
word	Metric
error	Metric
reduction	Metric
of	O
23	O
%	O
,	O
and	O
a	O
20	O
%	O
relative	O
gain	O
for	O
the	O
combined	O
recognition	Method
system	Method
,	O
considerably	O
larger	O
than	O
previously	O
reported	O
for	O
conversational	Task
speech	Task
recognition	Task
.	O

section	O
:	O
Conclusions	O
We	O
have	O
measured	O
the	O
human	Metric
error	Metric
rate	Metric
on	O
NIST	Task
’s	Task
2000	Task
conversational	Task
telephone	Task
speech	Task
recognition	Task
task	Task
.	O

We	O
find	O
that	O
there	O
is	O
a	O
great	O
deal	O
of	O
variability	O
between	O
the	O
Switchboard	Material
and	Material
CallHome	Material
subsets	Material
,	O
with	O
5.8	O
%	O
and	O
11.0	O
%	O
error	Metric
rates	Metric
respectively	O
.	O

For	O
the	O
first	O
time	O
,	O
we	O
report	O
automatic	Task
recognition	Task
performance	O
on	O
par	O
with	O
human	O
performance	O
on	O
this	O
task	O
.	O

Our	O
system	O
’s	O
performance	O
can	O
be	O
attributed	O
to	O
the	O
systematic	O
use	O
of	O
LSTMs	Method
for	O
both	O
acoustic	Task
and	Task
language	Task
modeling	Task
,	O
as	O
well	O
as	O
CNNs	Method
in	O
the	O
acoustic	Method
model	Method
,	O
and	O
extensive	O
combination	O
of	O
complementary	Method
system	Method
for	O
both	O
acoustic	Task
and	Task
language	Task
modeling	Task
.	O

section	O
:	O
Acknowledgments	O
We	O
thank	O
Arul	O
Menezes	O
for	O
access	O
to	O
the	O
Microsoft	Method
transcription	Method
pipeline	Method
;	O
Chris	O
Basoglu	O
,	O
Amit	O
Agarwal	O
and	O
Marko	O
Radmilac	O
for	O
their	O
invaluable	O
assistance	O
with	O
CNTK	Method
;	O
Jinyu	O
Li	O
and	O
Partha	O
Parthasarathy	O
for	O
many	O
helpful	O
conversations	O
.	O

We	O
also	O
thank	O
X.	O
Chen	O
from	O
Cambridge	O
University	O
for	O
valuable	O
assistance	O
with	O
the	O
CUED	Method
-	Method
RNNLM	Method
toolkit	Method
,	O
and	O
the	O
International	O
Computer	O
Science	O
Institute	O
for	O
compute	Material
and	Material
data	Material
resources	Material
.	O

bibliography	O
:	O
References	O
