In	O
statistical	Task
relational	Task
learning	Task
,	O
the	O
link	Task
prediction	Task
problem	Task
is	O
key	O
to	O
automatically	O
understand	O
the	O
structure	O
of	O
large	Method
knowledge	Method
bases	Method
.	O

As	O
in	O
previous	O
studies	O
,	O
we	O
propose	O
to	O
solve	O
this	O
problem	O
through	O
latent	Method
factorization	Method
.	O

However	O
,	O
here	O
we	O
make	O
use	O
of	O
complex	O
valued	O
embeddings	O
.	O

The	O
composition	Method
of	Method
complex	Method
embeddings	Method
can	O
handle	O
a	O
large	O
variety	O
of	O
binary	O
relations	O
,	O
among	O
them	O
symmetric	O
and	O
antisymmetric	O
relations	O
.	O

Compared	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
such	O
as	O
Neural	Method
Tensor	Method
Network	Method
and	O
Holographic	Method
Embeddings	Method
,	O
our	O
approach	O
based	O
on	O
complex	Method
embeddings	Method
is	O
arguably	O
simpler	O
,	O
as	O
it	O
only	O
uses	O
the	O
Hermitian	O
dot	O
product	O
,	O
the	O
complex	O
counterpart	O
of	O
the	O
standard	O
dot	O
product	O
between	O
real	O
vectors	O
.	O

Our	O
approach	O
is	O
scalable	O
to	O
large	O
datasets	O
as	O
it	O
remains	O
linear	O
in	O
both	O
space	O
and	O
time	O
,	O
while	O
consistently	O
outperforming	O
alternative	O
approaches	O
on	O
standard	O
link	Task
prediction	Task
benchmarks	Task
.	O

ruled	O
algothplop	O
algoAlgorithm	O
ComplexEmbeddingsforSimpleLinkPrediction	Method
section	O
:	O
Introduction	O
Web	O
-	O
scale	O
knowledge	O
bases	O
(	O
KBs	O
)	O
provide	O
a	O
structured	Method
representation	Method
of	Method
world	Method
knowledge	Method
,	O
with	O
projects	O
such	O
as	O
DBPedia	O
,	O
Freebase	O
or	O
the	O
Google	O
Knowledge	O
Vault	O
.	O

They	O
enable	O
a	O
wide	O
range	O
of	O
applications	O
such	O
as	O
recommender	Task
systems	Task
,	O
question	Task
answering	Task
or	O
automated	Task
personal	Task
agents	Task
.	O

The	O
incompleteness	O
of	O
these	O
KBs	O
has	O
stimulated	O
research	O
into	O
predicting	Task
missing	Task
entries	Task
,	O
a	O
task	O
known	O
as	O
link	Task
prediction	Task
that	O
is	O
one	O
of	O
the	O
main	O
problems	O
in	O
Statistical	Task
Relational	Task
Learning	Task
.	O

KBs	Method
express	O
data	O
as	O
a	O
directed	O
graph	O
with	O
labeled	O
edges	O
(	O
relations	O
)	O
between	O
nodes	O
(	O
entities	O
)	O
.	O

Natural	O
redundancies	O
among	O
the	O
recorded	O
relations	O
often	O
make	O
it	O
possible	O
to	O
fill	O
in	O
the	O
missing	O
entries	O
of	O
a	O
KB	O
.	O

As	O
an	O
example	O
,	O
the	O
relation	O
CountryOfBirth	O
is	O
not	O
recorded	O
for	O
all	O
entities	O
,	O
but	O
it	O
can	O
easily	O
be	O
inferred	O
if	O
the	O
relation	O
CityOfBirth	O
is	O
known	O
.	O

The	O
goal	O
of	O
link	Task
prediction	Task
is	O
the	O
automatic	Task
discovery	Task
of	Task
such	O
regularities	O
.	O

However	O
,	O
many	O
relations	O
are	O
non	O
-	O
deterministic	O
:	O
the	O
combination	O
of	O
the	O
two	O
facts	O
IsBornIn	O
(	O
John	O
,	O
Athens	O
)	O
and	O
IsLocatedIn	O
(	O
Athens	O
,	O
Greece	O
)	O
does	O
not	O
always	O
imply	O
the	O
fact	O
HasNationality	O
(	O
John	O
,	O
Greece	O
)	O
.	O

Hence	O
,	O
it	O
is	O
required	O
to	O
handle	O
other	O
facts	O
involving	O
these	O
relations	O
or	O
entities	O
in	O
a	O
probabilistic	O
fashion	O
.	O

To	O
do	O
so	O
,	O
an	O
increasingly	O
popular	O
method	O
is	O
to	O
state	O
the	O
link	Task
prediction	Task
task	Task
as	O
a	O
3D	Task
binary	Task
tensor	Task
completion	Task
problem	Task
,	O
where	O
each	O
slice	O
is	O
the	O
adjacency	O
matrix	O
of	O
one	O
relation	O
type	O
in	O
the	O
knowledge	O
graph	O
.	O

Completion	Task
based	O
on	O
low	Method
-	Method
rank	Method
factorization	Method
or	O
embeddings	Method
has	O
been	O
popularized	O
with	O
the	O
Netflix	O
challenge	O
.	O

A	O
partially	O
observed	O
matrix	O
or	O
tensor	O
is	O
decomposed	O
into	O
a	O
product	Method
of	Method
embedding	Method
matrices	Method
with	O
much	O
smaller	O
rank	O
,	O
resulting	O
in	O
fixed	O
-	O
dimensional	Method
vector	Method
representations	Method
for	O
each	O
entity	O
and	O
relation	O
in	O
the	O
database	O
.	O

For	O
a	O
given	O
fact	O
r	O
(	O
s	O
,	O
o	O
)	O
in	O
which	O
subject	O
is	O
linked	O
to	O
object	O
through	O
relation	O
,	O
the	O
score	O
can	O
then	O
be	O
recovered	O
as	O
a	O
multi	O
-	O
linear	O
product	O
between	O
the	O
embedding	O
vectors	O
of	O
,	O
and	O
.	O

Binary	O
relations	O
in	O
KBs	O
exhibit	O
various	O
types	O
of	O
patterns	O
:	O
hierarchies	O
and	O
compositions	O
like	O
FatherOf	O
,	O
OlderThan	O
or	O
IsPartOf	O
—with	O
partial	O
/	O
total	O
,	O
strict	O
/	O
non	O
-	O
strict	O
orders	O
—	O
and	O
equivalence	O
relations	O
like	O
IsSimilarTo	O
.	O

As	O
described	O
in	O
,	O
a	O
relational	Method
model	Method
should	O
(	O
a	O
)	O
be	O
able	O
to	O
learn	O
all	O
combinations	O
of	O
these	O
properties	O
,	O
namely	O
reflexivity	O
/	O
irreflexivity	O
,	O
symmetry	O
/	O
antisymmetry	O
and	O
transitivity	O
,	O
and	O
(	O
b	O
)	O
be	O
linear	O
in	O
both	O
time	O
and	O
memory	O
in	O
order	O
to	O
scale	O
to	O
the	O
size	O
of	O
present	O
day	O
KBs	O
,	O
and	O
keep	O
up	O
with	O
their	O
growth	O
.	O

Dot	Method
products	Method
of	Method
embeddings	Method
scale	O
well	O
and	O
can	O
naturally	O
handle	O
both	O
symmetry	O
and	O
(	O
ir	O
-)	O
reflexivity	O
of	O
relations	O
;	O
using	O
an	O
appropriate	O
loss	O
function	O
even	O
enables	O
transitivity	O
.	O

However	O
,	O
dealing	O
with	O
antisymmetric	O
relations	O
has	O
so	O
far	O
almost	O
always	O
implied	O
an	O
explosion	O
of	O
the	O
number	O
of	O
parameters	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
,	O
making	O
models	O
prone	O
to	O
overfitting	O
.	O

Finding	O
the	O
best	O
ratio	O
between	O
expressiveness	Metric
and	O
parameter	Metric
space	Metric
size	Metric
is	O
the	O
keystone	O
of	O
embedding	Method
models	Method
.	O

In	O
this	O
work	O
we	O
argue	O
that	O
the	O
standard	O
dot	Method
product	Method
between	Method
embeddings	Method
can	O
be	O
a	O
very	O
effective	O
composition	Method
function	Method
,	O
provided	O
that	O
one	O
uses	O
the	O
right	O
representation	O
.	O

Instead	O
of	O
using	O
embeddings	O
containing	O
real	O
numbers	O
we	O
discuss	O
and	O
demonstrate	O
the	O
capabilities	O
of	O
complex	Method
embeddings	Method
.	O

When	O
using	O
complex	O
vectors	O
,	O
i.e.	O
vectors	O
with	O
entries	O
in	O
,	O
the	O
dot	O
product	O
is	O
often	O
called	O
the	O
Hermitian	O
(	O
or	O
sesquilinear	O
)	O
dot	O
product	O
,	O
as	O
it	O
involves	O
the	O
conjugate	O
-	O
transpose	O
of	O
one	O
of	O
the	O
two	O
vectors	O
.	O

As	O
a	O
consequence	O
,	O
the	O
dot	O
product	O
is	O
not	O
symmetric	O
any	O
more	O
,	O
and	O
facts	O
about	O
antisymmetric	O
relations	O
can	O
receive	O
different	O
scores	O
depending	O
on	O
the	O
ordering	O
of	O
the	O
entities	O
involved	O
.	O

Thus	O
complex	O
vectors	O
can	O
effectively	O
capture	O
antisymmetric	O
relations	O
while	O
retaining	O
the	O
efficiency	O
benefits	O
of	O
the	O
dot	O
product	O
,	O
that	O
is	O
linearity	O
in	O
both	O
space	Metric
and	Metric
time	Metric
complexity	Metric
.	O

The	O
remainder	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

We	O
first	O
justify	O
the	O
intuition	O
of	O
using	O
complex	O
embeddings	O
in	O
the	O
square	Task
matrix	Task
case	Task
in	O
which	O
there	O
is	O
only	O
a	O
single	O
relation	O
between	O
entities	O
.	O

The	O
formulation	O
is	O
then	O
extended	O
to	O
a	O
stacked	O
set	O
of	O
square	O
matrices	O
in	O
a	O
third	O
-	O
order	O
tensor	O
to	O
represent	O
multiple	O
relations	O
.	O

We	O
then	O
describe	O
experiments	O
on	O
large	O
scale	O
public	O
benchmark	O
KBs	O
in	O
which	O
we	O
empirically	O
show	O
that	O
this	O
representation	O
leads	O
not	O
only	O
to	O
simpler	O
and	O
faster	O
algorithms	O
,	O
but	O
also	O
gives	O
a	O
systematic	O
accuracy	Metric
improvement	O
over	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
alternatives	O
.	O

To	O
give	O
a	O
clear	O
comparison	O
with	O
respect	O
to	O
existing	O
approaches	O
using	O
only	O
real	O
numbers	O
,	O
we	O
also	O
present	O
an	O
equivalent	O
reformulation	O
of	O
our	O
model	O
that	O
involves	O
only	O
real	O
embeddings	O
.	O

This	O
should	O
help	O
practitioners	O
when	O
implementing	O
our	O
method	O
,	O
without	O
requiring	O
the	O
use	O
of	O
complex	O
numbers	O
in	O
their	O
software	O
implementation	O
.	O

section	O
:	O
Relations	O
as	O
Real	O
Part	O
of	O
Low	O
-	O
Rank	O
Normal	O
Matrices	O
In	O
this	O
section	O
we	O
discuss	O
the	O
use	O
of	O
complex	Method
embeddings	Method
for	O
low	Task
-	Task
rank	Task
matrix	Task
factorization	Task
and	O
illustrate	O
this	O
by	O
considering	O
a	O
simplified	O
link	Task
prediction	Task
task	Task
with	O
merely	O
a	O
single	O
relation	O
type	O
.	O

Understanding	O
the	O
factorization	Task
in	Task
complex	Task
space	Task
leads	O
to	O
a	O
better	O
theoretical	O
understanding	O
of	O
the	O
class	O
of	O
matrices	O
that	O
can	O
actually	O
be	O
approximated	O
by	O
dot	Method
products	Method
of	Method
embeddings	Method
.	O

These	O
are	O
the	O
so	O
-	O
called	O
normal	O
matrices	O
for	O
which	O
the	O
left	O
and	O
right	O
embeddings	O
share	O
the	O
same	O
unitary	O
basis	O
.	O

subsection	O
:	O
Modelling	Task
Relations	Task
Let	O
be	O
a	O
set	O
of	O
entities	O
with	O
.	O

A	O
relation	O
between	O
two	O
entities	O
is	O
represented	O
as	O
a	O
binary	O
value	O
,	O
where	O
is	O
the	O
subject	O
of	O
the	O
relation	O
and	O
its	O
object	O
.	O

Its	O
probability	O
is	O
given	O
by	O
the	O
logistic	O
inverse	O
link	O
function	O
:	O
where	O
is	O
a	O
latent	O
matrix	O
of	O
scores	O
,	O
and	O
the	O
partially	O
observed	O
sign	O
matrix	O
.	O

Our	O
goal	O
is	O
to	O
find	O
a	O
generic	O
structure	O
for	O
that	O
leads	O
to	O
a	O
flexible	O
approximation	O
of	O
common	O
relations	O
in	O
real	O
world	O
KBs	O
.	O

Standard	O
matrix	Method
factorization	Method
approximates	O
by	O
a	O
matrix	Method
product	Method
,	O
where	O
and	O
are	O
two	O
functionally	O
independent	O
matrices	O
,	O
being	O
the	O
rank	O
of	O
the	O
matrix	O
.	O

Within	O
this	O
formulation	O
it	O
is	O
assumed	O
that	O
entities	O
appearing	O
as	O
subjects	O
are	O
different	O
from	O
entities	O
appearing	O
as	O
objects	O
.	O

This	O
means	O
that	O
the	O
same	O
entity	O
will	O
have	O
two	O
different	O
embedding	O
vectors	O
,	O
depending	O
on	O
whether	O
it	O
appears	O
as	O
the	O
subject	O
or	O
the	O
object	O
of	O
a	O
relation	O
.	O

This	O
extensively	O
studied	O
type	O
of	O
model	O
is	O
closely	O
related	O
to	O
the	O
singular	Method
value	Method
decomposition	Method
(	O
SVD	Method
)	O
and	O
fits	O
well	O
to	O
the	O
case	O
where	O
the	O
matrix	O
is	O
rectangular	O
.	O

However	O
,	O
in	O
many	O
link	Task
prediction	Task
problems	Task
,	O
the	O
same	O
entity	O
can	O
appear	O
as	O
both	O
subject	O
and	O
object	O
.	O

It	O
then	O
seems	O
natural	O
to	O
learn	O
joint	O
embeddings	O
of	O
the	O
entities	O
,	O
which	O
entails	O
sharing	O
the	O
embeddings	O
of	O
the	O
left	O
and	O
right	O
factors	O
,	O
as	O
proposed	O
by	O
several	O
authors	O
to	O
solve	O
the	O
link	Task
prediction	Task
problem	Task
.	O

In	O
order	O
to	O
use	O
the	O
same	O
embedding	O
for	O
subjects	O
and	O
objects	O
,	O
researchers	O
have	O
generalised	O
the	O
notion	O
of	O
dot	O
products	O
to	O
scoring	Method
functions	Method
,	O
also	O
known	O
as	O
composition	O
functions	O
,	O
that	O
combine	O
embeddings	O
in	O
specific	O
ways	O
.	O

We	O
briefly	O
recall	O
several	O
examples	O
of	O
scoring	Metric
functions	Metric
in	O
Table	O
[	O
reference	O
]	O
,	O
as	O
well	O
as	O
the	O
extension	O
proposed	O
in	O
this	O
paper	O
.	O

Using	O
the	O
same	O
embeddings	O
for	O
right	O
and	O
left	O
factors	O
boils	O
down	O
to	O
Eigenvalue	Method
decomposition	Method
:	O
It	O
is	O
often	O
used	O
to	O
approximate	O
real	O
symmetric	O
matrices	O
such	O
as	O
covariance	O
matrices	O
,	O
kernel	O
functions	O
and	O
distance	O
or	O
similarity	O
matrices	O
.	O

In	O
these	O
cases	O
all	O
eigenvalues	O
and	O
eigenvectors	O
live	O
in	O
the	O
real	O
space	O
and	O
is	O
orthogonal	O
:	O
.	O

We	O
are	O
in	O
this	O
work	O
however	O
explicitly	O
interested	O
in	O
problems	O
where	O
matrices	O
—	O
and	O
thus	O
the	O
relations	O
they	O
represent	O
—	O
can	O
also	O
be	O
antisymmetric	O
.	O

In	O
that	O
case	O
eigenvalue	Method
decomposition	Method
is	O
not	O
possible	O
in	O
the	O
real	O
space	O
;	O
there	O
only	O
exists	O
a	O
decomposition	O
in	O
the	O
complex	O
space	O
where	O
embeddings	O
are	O
composed	O
of	O
a	O
real	O
vector	O
component	O
and	O
an	O
imaginary	O
vector	O
component	O
.	O

With	O
complex	O
numbers	O
,	O
the	O
dot	O
product	O
,	O
also	O
called	O
the	O
Hermitian	O
product	O
,	O
or	O
sesquilinear	O
form	O
,	O
is	O
defined	O
as	O
:	O
where	O
and	O
are	O
complex	O
-	O
valued	O
vectors	O
,	O
i.e.	O
with	O
and	O
corresponding	O
to	O
the	O
real	O
and	O
imaginary	O
parts	O
of	O
the	O
vector	O
,	O
and	O
denoting	O
the	O
square	O
root	O
of	O
.	O

We	O
see	O
here	O
that	O
one	O
crucial	O
operation	O
is	O
to	O
take	O
the	O
conjugate	O
of	O
the	O
first	O
vector	O
:	O
.	O

A	O
simple	O
way	O
to	O
justify	O
the	O
Hermitian	O
product	O
for	O
composing	O
complex	O
vectors	O
is	O
that	O
it	O
provides	O
a	O
valid	O
topological	O
norm	O
in	O
the	O
induced	O
vectorial	O
space	O
.	O

For	O
example	O
,	O
implies	O
while	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
bilinear	O
form	O
as	O
there	O
are	O
many	O
complex	O
vectors	O
for	O
which	O
.	O

Even	O
with	O
complex	O
eigenvectors	O
,	O
the	O
inversion	O
of	O
in	O
the	O
eigendecomposition	Method
of	Method
Equation	Method
(	O
[	O
reference	O
]	O
)	O
leads	O
to	O
computational	O
issues	O
.	O

Fortunately	O
,	O
mathematicians	O
defined	O
an	O
appropriate	O
class	O
of	O
matrices	O
that	O
prevents	O
us	O
from	O
inverting	O
the	O
eigenvector	O
matrix	O
:	O
we	O
consider	O
the	O
space	O
of	O
normal	O
matrices	O
,	O
i.e.	O
the	O
complex	O
matrices	O
,	O
such	O
that	O
=	O
.	O

The	O
spectral	Method
theorem	Method
for	O
normal	O
matrices	O
states	O
that	O
a	O
matrix	O
is	O
normal	O
if	O
and	O
only	O
if	O
it	O
is	O
unitarily	O
diagonalizable	O
:	O
where	O
is	O
the	O
diagonal	O
matrix	O
of	O
eigenvalues	O
(	O
with	O
decreasing	O
modulus	O
)	O
and	O
is	O
a	O
unitary	O
matrix	O
of	O
eigenvectors	O
,	O
with	O
representing	O
its	O
complex	O
conjugate	O
.	O

The	O
set	O
of	O
purely	O
real	O
normal	O
matrices	O
includes	O
all	O
symmetric	O
and	O
antisymmetric	O
sign	O
matrices	O
(	O
useful	O
to	O
model	O
hierarchical	O
relations	O
such	O
as	O
IsOlder	O
)	O
,	O
as	O
well	O
as	O
all	O
orthogonal	O
matrices	O
(	O
including	O
permutation	O
matrices	O
)	O
,	O
and	O
many	O
other	O
matrices	O
that	O
are	O
useful	O
to	O
represent	O
binary	O
relations	O
,	O
such	O
as	O
assignment	Method
matrices	Method
which	O
represent	O
bipartite	O
graphs	O
.	O

However	O
,	O
far	O
from	O
all	O
matrices	O
expressed	O
as	O
are	O
purely	O
real	O
,	O
and	O
equation	O
[	O
reference	O
]	O
requires	O
the	O
scores	O
to	O
be	O
purely	O
real	O
.	O

So	O
we	O
simply	O
keep	O
only	O
the	O
real	O
part	O
of	O
the	O
decomposition	O
:	O
In	O
fact	O
,	O
performing	O
this	O
projection	O
on	O
the	O
real	O
subspace	O
allows	O
the	O
exact	O
decomposition	O
of	O
any	O
real	O
square	O
matrix	O
and	O
not	O
only	O
normal	O
ones	O
,	O
as	O
shown	O
by	O
.	O

Compared	O
to	O
the	O
singular	Method
value	Method
decomposition	Method
,	O
the	O
eigenvalue	Method
decomposition	Method
has	O
two	O
key	O
differences	O
:	O
The	O
eigenvalues	O
are	O
not	O
necessarily	O
positive	O
or	O
real	O
;	O
The	O
factorization	Method
(	O
[	O
reference	O
]	O
)	O
is	O
useful	O
as	O
the	O
rows	O
of	O
can	O
be	O
used	O
as	O
vectorial	Method
representations	Method
of	O
the	O
entities	O
corresponding	O
to	O
rows	O
and	O
columns	O
of	O
the	O
relation	O
matrix	O
.	O

Indeed	O
,	O
for	O
a	O
given	O
entity	O
,	O
its	O
subject	O
embedding	O
vector	O
is	O
the	O
complex	O
conjugate	O
of	O
its	O
object	O
embedding	O
vector	O
.	O

subsection	O
:	O
Low	Method
-	Method
Rank	Method
Decomposition	Method
In	O
a	O
link	Task
prediction	Task
problem	Task
,	O
the	O
relation	O
matrix	O
is	O
unknown	O
and	O
the	O
goal	O
is	O
to	O
recover	O
it	O
entirely	O
from	O
noisy	O
observations	O
.	O

To	O
enable	O
the	O
model	O
to	O
be	O
learnable	O
,	O
i.e.	O
to	O
generalize	O
to	O
unobserved	O
links	O
,	O
some	O
regularity	O
assumptions	O
are	O
needed	O
.	O

Since	O
we	O
deal	O
with	O
binary	O
relations	O
,	O
we	O
assume	O
that	O
they	O
have	O
low	O
sign	O
-	O
rank	O
.	O

The	O
sign	O
-	O
rank	O
of	O
a	O
sign	O
matrix	O
is	O
the	O
smallest	O
rank	O
of	O
a	O
real	O
matrix	O
that	O
has	O
the	O
same	O
sign	O
-	O
pattern	O
as	O
:	O
This	O
is	O
theoretically	O
justified	O
by	O
the	O
fact	O
that	O
the	O
sign	Metric
-	Metric
rank	Metric
is	O
a	O
natural	Metric
complexity	Metric
measure	Metric
of	O
sign	O
matrices	O
and	O
is	O
linked	O
to	O
learnability	Task
,	O
and	O
empirically	O
confirmed	O
by	O
the	O
wide	O
success	O
of	O
factorization	Method
models	Method
.	O

If	O
the	O
observation	O
matrix	O
is	O
low	O
-	O
sign	O
-	O
rank	O
,	O
then	O
our	O
model	O
can	O
decompose	O
it	O
with	O
a	O
rank	O
at	O
most	O
the	O
double	O
of	O
the	O
sign	O
-	O
rank	O
of	O
.	O

That	O
is	O
,	O
for	O
any	O
,	O
there	O
always	O
exists	O
a	O
matrix	O
with	O
the	O
same	O
sign	O
pattern	O
,	O
where	O
the	O
rank	O
of	O
is	O
at	O
most	O
twice	O
the	O
sign	O
-	O
rank	O
of	O
.	O

Although	O
twice	O
sounds	O
bad	O
,	O
this	O
is	O
actually	O
a	O
good	O
upper	O
bound	O
.	O

Indeed	O
,	O
the	O
sign	Metric
-	Metric
rank	Metric
is	O
often	O
much	O
lower	O
than	O
the	O
rank	O
of	O
.	O

For	O
example	O
,	O
the	O
rank	O
of	O
the	O
identity	O
matrix	O
is	O
,	O
but	O
.	O

By	O
permutation	O
of	O
the	O
columns	O
and	O
,	O
the	O
matrix	O
corresponds	O
to	O
the	O
relation	O
marriedTo	O
,	O
a	O
relation	O
known	O
to	O
be	O
hard	O
to	O
factorize	O
.	O

Yet	O
our	O
model	O
can	O
express	O
it	O
in	O
rank	O
6	O
,	O
for	O
any	O
.	O

By	O
imposing	O
a	O
low	O
-	O
rank	O
on	O
,	O
only	O
the	O
first	O
values	O
of	O
are	O
non	O
-	O
zero	O
.	O

So	O
we	O
can	O
directly	O
have	O
and	O
.	O

Individual	O
relation	O
scores	O
between	O
entities	O
and	O
can	O
be	O
predicted	O
through	O
the	O
following	O
product	O
of	O
their	O
embeddings	O
:	O
We	O
summarize	O
the	O
above	O
discussion	O
in	O
three	O
points	O
:	O
Our	O
factorization	O
encompasses	O
all	O
possible	O
binary	O
relations	O
.	O

By	O
construction	O
,	O
it	O
accurately	O
describes	O
both	O
symmetric	O
and	O
antisymmetric	O
relations	O
.	O

Learnable	O
relations	O
can	O
be	O
efficiently	O
approximated	O
by	O
a	O
simple	O
low	Method
-	Method
rank	Method
factorization	Method
,	O
using	O
complex	O
numbers	O
to	O
represent	O
the	O
latent	O
factors	O
.	O

section	O
:	O
Application	O
to	O
Binary	O
Multi	O
-	O
Relational	O
Data	O
The	O
previous	O
section	O
focused	O
on	O
modeling	O
a	O
single	O
type	O
of	O
relation	O
;	O
we	O
now	O
extend	O
this	O
model	O
to	O
multiple	O
types	O
of	O
relations	O
.	O

We	O
do	O
so	O
by	O
allocating	O
an	O
embedding	O
to	O
each	O
relation	O
,	O
and	O
by	O
sharing	O
the	O
entity	O
embeddings	O
across	O
all	O
relations	O
.	O

Let	O
and	O
be	O
the	O
set	O
of	O
relations	O
and	O
entities	O
present	O
in	O
the	O
KB	O
.	O

We	O
want	O
to	O
recover	O
the	O
matrices	O
of	O
scores	O
for	O
all	O
the	O
relations	O
.	O

Given	O
two	O
entities	O
and	O
,	O
the	O
log	O
-	O
odd	O
of	O
the	O
probability	O
that	O
the	O
fact	O
r	O
(	O
s	O
,	O
o	O
)	O
is	O
true	O
is	O
:	O
where	O
is	O
a	O
scoring	Method
function	Method
that	O
is	O
typically	O
based	O
on	O
a	O
factorization	O
of	O
the	O
observed	O
relations	O
and	O
denotes	O
the	O
parameters	O
of	O
the	O
corresponding	O
model	O
.	O

While	O
as	O
a	O
whole	O
is	O
unknown	O
,	O
we	O
assume	O
that	O
we	O
observe	O
a	O
set	O
of	O
true	O
and	O
false	O
facts	O
,	O
corresponding	O
to	O
the	O
partially	O
observed	O
adjacency	O
matrices	O
of	O
different	O
relations	O
,	O
where	O
is	O
the	O
set	O
of	O
observed	O
triples	O
.	O

The	O
goal	O
is	O
to	O
find	O
the	O
probabilities	O
of	O
entries	O
being	O
true	O
or	O
false	O
for	O
a	O
set	O
of	O
targeted	O
unobserved	O
triples	O
.	O

Depending	O
on	O
the	O
scoring	O
function	O
used	O
to	O
predict	O
the	O
entries	O
of	O
the	O
tensor	O
,	O
we	O
obtain	O
different	O
models	O
.	O

Examples	O
of	O
scoring	O
functions	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
model	Method
scoring	Method
function	Method
is	O
:	O
where	O
is	O
a	O
complex	O
vector	O
.	O

These	O
equations	O
provide	O
two	O
interesting	O
views	O
of	O
the	O
model	O
:	O
Changing	O
the	O
representation	O
:	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
would	O
correspond	O
to	O
DistMult	Method
with	O
real	O
embeddings	O
,	O
but	O
handles	O
asymmetry	O
thanks	O
to	O
the	O
complex	O
conjugate	O
of	O
one	O
of	O
the	O
embeddings	O
.	O

Changing	O
the	O
scoring	O
function	O
:	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
only	O
involves	O
real	O
vectors	O
corresponding	O
to	O
the	O
real	O
and	O
imaginary	O
parts	O
of	O
the	O
embeddings	O
and	O
relations	O
.	O

One	O
can	O
easily	O
check	O
that	O
this	O
function	O
is	O
antisymmetric	O
when	O
is	O
purely	O
imaginary	O
(	O
i.e.	O
its	O
real	O
part	O
is	O
zero	O
)	O
,	O
and	O
symmetric	O
when	O
is	O
real	O
.	O

Interestingly	O
,	O
by	O
separating	O
the	O
real	O
and	O
imaginary	O
part	O
of	O
the	O
relation	O
embedding	O
,	O
we	O
obtain	O
a	O
decomposition	O
of	O
the	O
relation	O
matrix	O
as	O
the	O
sum	O
of	O
a	O
symmetric	O
matrix	O
and	O
a	O
antisymmetric	O
matrix	O
.	O

Relation	O
embeddings	O
naturally	O
act	O
as	O
weights	O
on	O
each	O
latent	O
dimension	O
:	O
over	O
the	O
symmetric	O
,	O
real	O
part	O
of	O
,	O
and	O
over	O
the	O
antisymmetric	O
,	O
imaginary	O
part	O
of	O
.	O

Indeed	O
,	O
one	O
has	O
,	O
meaning	O
that	O
is	O
symmetric	O
,	O
while	O
is	O
antisymmetric	O
.	O

This	O
enables	O
us	O
to	O
accurately	O
describe	O
both	O
symmetric	O
and	O
antisymmetric	O
relations	O
between	O
pairs	O
of	O
entities	O
,	O
while	O
still	O
using	O
joint	Method
representations	Method
of	O
entities	O
,	O
whether	O
they	O
appear	O
as	O
subject	O
or	O
object	O
of	O
relations	O
.	O

Geometrically	O
,	O
each	O
relation	Method
embedding	Method
is	O
an	O
anisotropic	O
scaling	O
of	O
the	O
basis	O
defined	O
by	O
the	O
entity	Method
embeddings	Method
,	O
followed	O
by	O
a	O
projection	O
onto	O
the	O
real	O
subspace	O
.	O

section	O
:	O
Experiments	O
In	O
order	O
to	O
evaluate	O
our	O
proposal	O
,	O
we	O
conducted	O
experiments	O
on	O
both	O
synthetic	O
and	O
real	O
datasets	O
.	O

The	O
synthetic	O
dataset	O
is	O
based	O
on	O
relations	O
that	O
are	O
either	O
symmetric	O
or	O
antisymmetric	O
,	O
whereas	O
the	O
real	O
datasets	O
comprise	O
different	O
types	O
of	O
relations	O
found	O
in	O
different	O
,	O
standard	O
KBs	O
.	O

We	O
refer	O
to	O
our	O
model	O
as	O
ComplEx	Method
,	O
for	O
Complex	Method
Embeddings	Method
.	O

subsection	O
:	O
Synthetic	Task
Task	Task
To	O
assess	O
the	O
ability	O
of	O
our	O
proposal	O
to	O
accurately	O
model	O
symmetry	O
and	O
antisymmetry	O
,	O
we	O
randomly	O
generated	O
a	O
KB	O
of	O
two	O
relations	O
and	O
30	O
entities	O
.	O

One	O
relation	O
is	O
entirely	O
symmetric	O
,	O
while	O
the	O
other	O
is	O
completely	O
antisymmetric	O
.	O

This	O
dataset	O
corresponds	O
to	O
a	O
tensor	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
part	O
of	O
this	O
randomly	O
generated	O
tensor	O
,	O
with	O
a	O
symmetric	O
slice	O
and	O
an	O
antisymmetric	O
slice	O
,	O
decomposed	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
.	O

The	O
diagonal	O
is	O
unobserved	O
as	O
it	O
is	O
not	O
relevant	O
in	O
this	O
experiment	O
.	O

The	O
train	O
set	O
contains	O
1392	O
observed	O
triples	O
,	O
whereas	O
the	O
validation	O
and	O
test	O
sets	O
contain	O
174	O
triples	O
each	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
best	O
cross	Metric
-	Metric
validated	Metric
Average	Metric
Precision	Metric
(	O
area	Metric
under	Metric
Precision	Metric
-	Metric
Recall	Metric
curve	Metric
)	O
for	O
different	O
factorization	Method
models	Method
of	O
ranks	O
ranging	O
up	O
to	O
50	O
.	O

Models	O
were	O
trained	O
using	O
Stochastic	Method
Gradient	Method
Descent	Method
with	O
mini	Method
-	Method
batches	Method
and	O
AdaGrad	Method
for	O
tuning	O
the	O
learning	Metric
rate	Metric
,	O
by	O
minimizing	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
the	O
logistic	Method
model	Method
with	O
regularization	O
on	O
the	O
parameters	O
of	O
the	O
considered	O
model	O
:	O
In	O
our	O
model	O
,	O
corresponds	O
to	O
the	O
embeddings	O
.	O

We	O
describe	O
the	O
full	O
algorithm	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

is	O
validated	O
in	O
.	O

As	O
expected	O
,	O
DistMult	Method
is	O
not	O
able	O
to	O
model	O
antisymmetry	O
and	O
only	O
predicts	O
the	O
symmetric	O
relations	O
correctly	O
.	O

Although	O
TransE	Method
is	O
not	O
a	O
symmetric	Method
model	Method
,	O
it	O
performs	O
poorly	O
in	O
practice	O
,	O
particularly	O
on	O
the	O
antisymmetric	O
relation	O
.	O

RESCAL	Method
,	O
with	O
its	O
large	O
number	O
of	O
parameters	O
,	O
quickly	O
overfits	O
as	O
the	O
rank	O
grows	O
.	O

Canonical	Method
Polyadic	Method
(	O
CP	Method
)	O
decomposition	O
fails	O
on	O
both	O
relations	O
as	O
it	O
has	O
to	O
push	O
symmetric	O
and	O
antisymmetric	O
patterns	O
through	O
the	O
entity	O
embeddings	O
.	O

Surprisingly	O
,	O
only	O
our	O
model	O
succeeds	O
on	O
such	O
simple	O
data	O
.	O

subsection	O
:	O
Datasets	O
:	O
FB15	O
K	O
and	O
WN18	Material
We	O
next	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
FB15	O
K	O
and	O
WN18	Material
datasets	Material
.	O

FB15	O
K	O
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
curated	O
KB	O
of	O
general	O
facts	O
,	O
whereas	O
WN18	Material
is	O
a	O
subset	O
of	O
Wordnet	Material
,	O
a	O
database	O
featuring	O
lexical	O
relations	O
between	O
words	O
.	O

We	O
use	O
original	O
training	O
,	O
validation	O
and	O
test	O
set	O
splits	O
as	O
provided	O
by	O
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
the	O
metadata	O
of	O
the	O
two	O
datasets	O
.	O

Both	O
datasets	O
contain	O
only	O
positive	O
triples	O
.	O

As	O
in	O
,	O
we	O
generated	O
negatives	O
using	O
the	O
local	O
closed	O
world	O
assumption	O
.	O

That	O
is	O
,	O
for	O
a	O
triple	O
,	O
we	O
randomly	O
change	O
either	O
the	O
subject	O
or	O
the	O
object	O
at	O
random	O
,	O
to	O
form	O
a	O
negative	O
example	O
.	O

This	O
negative	Method
sampling	Method
is	O
performed	O
at	O
runtime	O
for	O
each	O
batch	O
of	O
training	O
positive	O
examples	O
.	O

For	O
evaluation	O
,	O
we	O
measure	O
the	O
quality	O
of	O
the	O
ranking	Metric
of	O
each	O
test	O
triple	O
among	O
all	O
possible	O
subject	O
and	O
object	O
substitutions	O
:	O
and	O
,	O
.	O

Mean	Metric
Reciprocal	Metric
Rank	Metric
(	O
MRR	Metric
)	O
and	O
Hits	Metric
at	O
are	O
the	O
standard	O
evaluation	Metric
measures	Metric
for	O
these	O
datasets	O
and	O
come	O
in	O
two	O
flavours	O
:	O
raw	O
and	O
filtered	O
.	O

The	O
filtered	Metric
metrics	Metric
are	O
computed	O
after	O
removing	O
all	O
the	O
other	O
positive	O
observed	O
triples	O
that	O
appear	O
in	O
either	O
training	O
,	O
validation	O
or	O
test	O
set	O
from	O
the	O
ranking	Task
,	O
whereas	O
the	O
raw	O
metrics	O
do	O
not	O
remove	O
these	O
.	O

Since	O
ranking	Metric
measures	Metric
are	O
used	O
,	O
previous	O
studies	O
generally	O
preferred	O
a	O
pairwise	Metric
ranking	Metric
loss	Metric
for	O
the	O
task	O
.	O

We	O
chose	O
to	O
use	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
the	O
logistic	Method
model	Method
,	O
as	O
it	O
is	O
a	O
continuous	O
surrogate	O
of	O
the	O
sign	O
-	O
rank	O
,	O
and	O
has	O
been	O
shown	O
to	O
learn	O
compact	Method
representations	Method
for	O
several	O
important	O
relations	O
,	O
especially	O
for	O
transitive	O
relations	O
.	O

In	O
preliminary	O
work	O
,	O
we	O
tried	O
both	O
losses	O
,	O
and	O
indeed	O
the	O
log	Method
-	Method
likelihood	Method
yielded	O
better	O
results	O
than	O
the	O
ranking	Metric
loss	Metric
(	O
except	O
with	O
TransE	Method
)	O
,	O
especially	O
on	O
FB15K.	O
We	O
report	O
both	O
filtered	Metric
and	O
raw	Metric
MRR	Metric
,	O
and	O
filtered	O
Hits	Metric
at	Metric
1	Metric
,	O
3	Metric
and	O
10	Metric
in	O
Table	O
[	O
reference	O
]	O
for	O
the	O
evaluated	O
models	O
.	O

Furthermore	O
,	O
we	O
chose	O
TransE	O
,	O
DistMult	Method
and	O
HolE	Method
as	O
baselines	O
since	O
they	O
are	O
the	O
best	O
performing	O
models	O
on	O
those	O
datasets	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

We	O
also	O
compare	O
with	O
the	O
CP	Method
model	O
to	O
emphasize	O
empirically	O
the	O
importance	O
of	O
learning	O
unique	O
embeddings	O
for	O
entities	O
.	O

For	O
experimental	O
fairness	O
,	O
we	O
reimplemented	O
these	O
methods	O
within	O
the	O
same	O
framework	O
as	O
the	O
ComplEx	Method
model	Method
,	O
using	O
theano	O
.	O

However	O
,	O
due	O
to	O
time	O
constraints	O
and	O
the	O
complexity	O
of	O
an	O
efficient	O
implementation	O
of	O
HolE	Method
,	O
we	O
record	O
the	O
original	O
results	O
for	O
HolE	Method
as	O
reported	O
in	O
.	O

subsection	O
:	O
Results	O
WN18	Material
describes	O
lexical	O
and	O
semantic	O
hierarchies	O
between	O
concepts	O
and	O
contains	O
many	O
antisymmetric	O
relations	O
such	O
as	O
hypernymy	O
,	O
hyponymy	O
,	O
or	O
being	O
”	O
part	O
of	O
”	O
.	O

Indeed	O
,	O
the	O
DistMult	Method
and	O
TransE	Method
models	Method
are	O
outperformed	O
here	O
by	O
ComplEx	Method
and	O
HolE	Method
,	O
which	O
are	O
on	O
par	O
with	O
respective	O
filtered	Metric
MRR	Metric
scores	Metric
of	O
0.941	O
and	O
0.938	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
filtered	O
test	O
set	O
MRR	Metric
for	O
the	O
models	O
considered	O
and	O
each	O
relation	O
of	O
WN18	Material
,	O
confirming	O
the	O
advantage	O
of	O
our	O
model	O
on	O
antisymmetric	O
relations	O
while	O
losing	O
nothing	O
on	O
the	O
others	O
.	O

2D	O
projections	O
of	O
the	O
relation	O
embeddings	O
provided	O
in	O
Appendix	O
[	O
reference	O
]	O
visually	O
corroborate	O
the	O
results	O
.	O

On	O
FB15	O
K	O
,	O
the	O
gap	O
is	O
much	O
more	O
pronounced	O
and	O
the	O
ComplEx	Method
model	Method
largely	O
outperforms	O
HolE	Method
,	O
with	O
a	O
filtered	Metric
MRR	Metric
of	O
0.692	O
and	O
59.9	O
%	O
of	O
Hits	Metric
at	Metric
1	Metric
,	O
compared	O
to	O
0.524	O
and	O
40.2	O
%	O
for	O
HolE.	Method
We	O
attribute	O
this	O
to	O
the	O
simplicity	O
of	O
our	O
model	O
and	O
the	O
different	O
loss	O
function	O
.	O

This	O
is	O
supported	O
by	O
the	O
relatively	O
small	O
gap	O
in	O
MRR	Metric
compared	O
to	O
DistMult	Method
(	O
0.654	O
)	O
;	O
our	O
model	O
can	O
in	O
fact	O
be	O
interpreted	O
as	O
a	O
complex	O
number	O
version	O
of	O
DistMult	Method
.	O

On	O
both	O
datasets	O
,	O
TransE	Method
and	O
CP	Method
are	O
largely	O
left	O
behind	O
.	O

This	O
illustrates	O
the	O
power	O
of	O
the	O
simple	O
dot	Method
product	Method
in	O
the	O
first	O
case	O
,	O
and	O
the	O
importance	O
of	O
learning	O
unique	O
entity	O
embeddings	O
in	O
the	O
second	O
.	O

CP	Method
performs	O
poorly	O
on	O
WN18	Material
due	O
to	O
the	O
small	O
number	O
of	O
relations	O
,	O
which	O
magnifies	O
this	O
subject	O
/	O
object	O
difference	O
.	O

Reported	O
results	O
are	O
given	O
for	O
the	O
best	O
set	O
of	O
hyper	O
-	O
parameters	O
evaluated	O
on	O
the	O
validation	O
set	O
for	O
each	O
model	O
,	O
after	O
grid	Method
search	Method
on	O
the	O
following	O
values	O
:	O
,	O
,	O
,	O
with	O
the	O
regularization	O
parameter	O
,	O
the	O
initial	O
learning	Metric
rate	Metric
(	O
then	O
tuned	O
at	O
runtime	O
with	O
AdaGrad	Method
)	O
,	O
and	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
triple	O
.	O

We	O
also	O
tried	O
varying	O
the	O
batch	O
size	O
but	O
this	O
had	O
no	O
impact	O
and	O
we	O
settled	O
with	O
100	O
batches	O
per	O
epoch	O
.	O

Best	O
ranks	O
were	O
generally	O
150	O
or	O
200	O
,	O
in	O
both	O
cases	O
scores	O
were	O
always	O
very	O
close	O
for	O
all	O
models	O
.	O

The	O
number	O
of	O
negative	O
samples	O
per	O
positive	O
sample	O
also	O
had	O
a	O
large	O
influence	O
on	O
the	O
filtered	O
MRR	Metric
on	O
FB15	O
K	O
(	O
up	O
to	O
+	O
0.08	O
improvement	O
from	O
1	O
to	O
10	Metric
negatives	O
)	O
,	O
but	O
not	O
much	O
on	O
WN18	Material
.	O

On	O
both	O
datasets	O
regularization	Task
was	O
important	O
(	O
up	O
to	O
+	O
0.05	O
on	O
filtered	Metric
MRR	Metric
between	O
and	O
optimal	O
one	O
)	O
.	O

We	O
found	O
the	O
initial	O
learning	Metric
rate	Metric
to	O
be	O
very	O
important	O
on	O
FB15	O
K	O
,	O
while	O
not	O
so	O
much	O
on	O
WN18	Material
.	O

We	O
think	O
this	O
may	O
also	O
explain	O
the	O
large	O
gap	O
of	O
improvement	O
our	O
model	O
provides	O
on	O
this	O
dataset	O
compared	O
to	O
previously	O
published	O
results	O
–	O
as	O
DistMult	Method
results	O
are	O
also	O
better	O
than	O
those	O
previously	O
reported	O
–	O
along	O
with	O
the	O
use	O
of	O
the	O
log	Method
-	Method
likelihood	Method
objective	Method
.	O

It	O
seems	O
that	O
in	O
general	O
AdaGrad	Method
is	O
relatively	O
insensitive	O
to	O
the	O
initial	O
learning	Metric
rate	Metric
,	O
perhaps	O
causing	O
some	O
overconfidence	O
in	O
its	O
ability	O
to	O
tune	O
the	O
step	O
size	O
online	O
and	O
consequently	O
leading	O
to	O
less	O
efforts	O
when	O
selecting	O
the	O
initial	O
step	O
size	O
.	O

Training	Task
was	O
stopped	O
using	O
early	O
stopping	O
on	O
the	O
validation	O
set	O
filtered	O
MRR	Metric
,	O
computed	O
every	O
50	O
epochs	O
with	O
a	O
maximum	O
of	O
1000	O
epochs	O
.	O

subsection	O
:	O
Influence	O
of	O
Negative	O
Samples	O
We	O
further	O
investigated	O
the	O
influence	O
of	O
the	O
number	O
of	O
negatives	O
generated	O
per	O
positive	O
training	O
sample	O
.	O

In	O
the	O
previous	O
experiment	O
,	O
due	O
to	O
computational	O
limitations	O
,	O
the	O
number	O
of	O
negatives	O
per	O
training	O
sample	O
,	O
,	O
was	O
validated	O
among	O
the	O
possible	O
numbers	O
.	O

We	O
want	O
to	O
explore	O
here	O
whether	O
increasing	O
these	O
numbers	O
could	O
lead	O
to	O
better	O
results	O
.	O

To	O
do	O
so	O
,	O
we	O
focused	O
on	O
FB15	O
K	O
,	O
with	O
the	O
best	O
validated	O
,	O
obtained	O
from	O
the	O
previous	O
experiment	O
.	O

We	O
then	O
let	O
vary	O
in	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
influence	O
of	O
the	O
number	O
of	O
generated	O
negatives	O
per	O
positive	O
training	O
triple	O
on	O
the	O
performance	O
of	O
our	O
model	O
on	O
FB15K.	O
Generating	O
more	O
negatives	O
clearly	O
improves	O
the	O
results	O
,	O
with	O
a	O
filtered	Metric
MRR	Metric
of	O
0.737	O
with	O
100	O
negative	O
triples	O
(	O
and	O
64.8	O
%	O
of	O
Hits@1	Metric
)	O
,	O
before	O
decreasing	O
again	O
with	O
200	O
negatives	O
.	O

The	O
model	O
also	O
converges	O
with	O
fewer	O
epochs	O
,	O
which	O
compensates	O
partially	O
for	O
the	O
additional	O
training	O
time	O
per	O
epoch	O
,	O
up	O
to	O
50	O
negatives	O
.	O

It	O
then	O
grows	O
linearly	O
as	O
the	O
number	O
of	O
negatives	O
increases	O
,	O
making	O
50	O
a	O
good	O
trade	O
-	O
off	O
between	O
accuracy	Metric
and	O
training	Metric
time	Metric
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
early	O
age	O
of	O
spectral	Method
theory	Method
in	O
linear	Task
algebra	Task
,	O
complex	O
numbers	O
were	O
not	O
used	O
for	O
matrix	Task
factorization	Task
and	O
mathematicians	O
mostly	O
focused	O
on	O
bi	O
-	O
linear	O
forms	O
.	O

The	O
eigen	Method
-	Method
decomposition	Method
in	O
the	O
complex	O
domain	O
as	O
taught	O
today	O
in	O
linear	Task
algebra	Task
courses	Task
came	O
40	O
years	O
later	O
.	O

Similarly	O
,	O
most	O
of	O
the	O
existing	O
approaches	O
for	O
tensor	Task
factorization	Task
were	O
based	O
on	O
decompositions	O
in	O
the	O
real	O
domain	O
,	O
such	O
as	O
the	O
Canonical	Method
Polyadic	Method
(	O
CP	Method
)	O
decomposition	O
.	O

These	O
methods	O
are	O
very	O
effective	O
in	O
many	O
applications	O
that	O
use	O
different	O
modes	O
of	O
the	O
tensor	O
for	O
different	O
types	O
of	O
entities	O
.	O

But	O
in	O
the	O
link	Task
prediction	Task
problem	Task
,	O
antisymmetry	Task
of	Task
relations	Task
was	O
quickly	O
seen	O
as	O
a	O
problem	O
and	O
asymmetric	O
extensions	O
of	O
tensors	O
were	O
studied	O
,	O
mostly	O
by	O
either	O
considering	O
independent	Method
embeddings	Method
or	O
considering	O
relations	O
as	O
matrices	O
instead	O
of	O
vectors	O
in	O
the	O
RESCAL	Method
model	Method
.	O

Direct	O
extensions	O
were	O
based	O
on	O
uni	O
-,	O
bi	O
-	O
and	O
trigram	O
latent	O
factors	O
for	O
triple	O
data	O
,	O
as	O
well	O
as	O
a	O
low	O
-	O
rank	O
relation	O
matrix	O
.	O

Pairwise	Method
interaction	Method
models	Method
were	O
also	O
considered	O
to	O
improve	O
prediction	Task
performances	O
.	O

For	O
example	O
,	O
the	O
Universal	Method
Schema	Method
approach	Method
factorizes	O
a	O
2D	Method
unfolding	Method
of	Method
the	Method
tensor	Method
(	O
a	O
matrix	O
of	O
entity	O
pairs	O
vs.	O
relations	O
)	O
while	O
extend	O
this	O
also	O
to	O
other	O
pairs	O
.	O

In	O
the	O
Neural	Method
Tensor	Method
Network	Method
(	O
NTN	Method
)	O
model	O
,	O
combine	O
linear	O
transformations	O
and	O
multiple	O
bilinear	O
forms	O
of	O
subject	O
and	O
object	O
embeddings	O
to	O
jointly	O
feed	O
them	O
into	O
a	O
nonlinear	Method
neural	Method
layer	Method
.	O

Its	O
non	O
-	O
linearity	O
and	O
multiple	O
ways	O
of	O
including	O
interactions	O
between	O
embeddings	O
gives	O
it	O
an	O
advantage	O
in	O
expressiveness	O
over	O
models	O
with	O
simpler	O
scoring	O
function	O
like	O
DistMult	Method
or	O
RESCAL	Method
.	O

As	O
a	O
downside	O
,	O
its	O
very	O
large	O
number	O
of	O
parameters	O
can	O
make	O
the	O
NTN	Method
model	O
harder	O
to	O
train	O
and	O
overfit	O
more	O
easily	O
.	O

The	O
original	O
multi	O
-	O
linear	O
DistMult	Method
model	O
is	O
symmetric	O
in	O
subject	O
and	O
object	O
for	O
every	O
relation	O
and	O
achieves	O
good	O
performance	O
,	O
presumably	O
due	O
to	O
its	O
simplicity	O
.	O

The	O
TransE	Method
model	Method
from	O
also	O
embeds	O
entities	O
and	O
relations	O
in	O
the	O
same	O
space	O
and	O
imposes	O
a	O
geometrical	O
structural	O
bias	O
into	O
the	O
model	O
:	O
the	O
subject	O
entity	O
vector	O
should	O
be	O
close	O
to	O
the	O
object	O
entity	O
vector	O
once	O
translated	O
by	O
the	O
relation	O
vector	O
.	O

A	O
recent	O
novel	O
way	O
to	O
handle	O
antisymmetry	O
is	O
via	O
the	O
Holographic	Method
Embeddings	Method
(	O
HolE	Method
)	O
model	O
by	O
.	O

In	O
HolE	Method
the	O
circular	O
correlation	O
is	O
used	O
for	O
combining	O
entity	O
embeddings	O
,	O
measuring	O
the	O
covariance	O
between	O
embeddings	O
at	O
different	O
dimension	O
shifts	O
.	O

This	O
generally	O
suggests	O
that	O
other	O
composition	O
functions	O
than	O
the	O
classical	O
tensor	Method
product	Method
can	O
be	O
helpful	O
as	O
they	O
allow	O
for	O
a	O
richer	O
interaction	O
of	O
embeddings	O
.	O

However	O
,	O
the	O
asymmetry	O
in	O
the	O
composition	O
function	O
in	O
HolE	Method
stems	O
from	O
the	O
asymmetry	O
of	O
circular	O
correlation	O
,	O
an	O
operation	O
,	O
whereas	O
ours	O
is	O
inherited	O
from	O
the	O
complex	O
inner	O
product	O
,	O
in	O
.	O

section	O
:	O
Conclusion	O
We	O
described	O
a	O
simple	O
approach	O
to	O
matrix	Task
and	Task
tensor	Task
factorization	Task
for	O
link	Task
prediction	Task
data	Task
that	O
uses	O
vectors	O
with	O
complex	O
values	O
and	O
retains	O
the	O
mathematical	O
definition	O
of	O
the	O
dot	O
product	O
.	O

The	O
class	Method
of	Method
normal	Method
matrices	Method
is	O
a	O
natural	O
fit	O
for	O
binary	O
relations	O
,	O
and	O
using	O
the	O
real	O
part	O
allows	O
for	O
efficient	O
approximation	O
of	O
any	O
learnable	O
relation	O
.	O

Results	O
on	O
standard	O
benchmarks	O
show	O
that	O
no	O
more	O
modifications	O
are	O
needed	O
to	O
improve	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

There	O
are	O
several	O
directions	O
in	O
which	O
this	O
work	O
can	O
be	O
extended	O
.	O

An	O
obvious	O
one	O
is	O
to	O
merge	O
our	O
approach	O
with	O
known	O
extensions	O
to	O
tensor	Method
factorization	Method
in	O
order	O
to	O
further	O
improve	O
predictive	Task
performance	O
.	O

For	O
example	O
,	O
the	O
use	O
of	O
pairwise	O
embeddings	O
together	O
with	O
complex	O
numbers	O
might	O
lead	O
to	O
improved	O
results	O
in	O
many	O
situations	O
that	O
involve	O
non	O
-	O
compositionality	O
.	O

Another	O
direction	O
would	O
be	O
to	O
develop	O
a	O
more	O
intelligent	O
negative	Method
sampling	Method
procedure	Method
,	O
to	O
generate	O
more	O
informative	O
negatives	O
with	O
respect	O
to	O
the	O
positive	O
sample	O
from	O
which	O
they	O
have	O
been	O
sampled	O
.	O

It	O
would	O
reduce	O
the	O
number	O
of	O
negatives	O
required	O
to	O
reach	O
good	O
performance	O
,	O
thus	O
accelerating	O
training	Metric
time	Metric
.	O

Also	O
,	O
if	O
we	O
were	O
to	O
use	O
complex	O
embeddings	O
every	O
time	O
a	O
model	O
includes	O
a	O
dot	O
product	O
,	O
e.g.	O
in	O
deep	Method
neural	Method
networks	Method
,	O
would	O
it	O
lead	O
to	O
a	O
similar	O
systematic	O
improvement	O
?	O
section	O
:	O
Acknowledgements	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
Paul	O
Allen	O
Foundation	O
through	O
an	O
Allen	O
Distinguished	O
Investigator	O
grant	O
and	O
in	O
part	O
by	O
a	O
Google	O
Focused	O
Research	O
Award	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
SGD	Method
algorithm	Method
We	O
describe	O
the	O
algorithm	O
to	O
learn	O
the	O
ComplEx	Method
model	Method
with	O
Stochastic	Method
Gradient	Method
Descent	Method
using	O
only	O
real	O
-	O
valued	O
vectors	O
.	O

Let	O
us	O
rewrite	O
equation	O
[	O
reference	O
]	O
,	O
by	O
denoting	O
the	O
real	O
part	O
of	O
embeddings	O
with	O
primes	O
and	O
the	O
imaginary	O
part	O
with	O
double	O
primes	O
:	O
,	O
,	O
,	O
.	O

The	O
set	O
of	O
parameters	O
is	O
,	O
and	O
the	O
scoring	Method
function	Method
involves	O
only	O
real	O
vectors	O
:	O
where	O
each	O
entity	O
and	O
each	O
relation	O
has	O
two	O
real	O
embeddings	O
.	O

Gradients	O
are	O
now	O
easy	O
to	O
write	O
:	O
where	O
is	O
the	O
element	O
-	O
wise	O
(	O
Hadamard	O
)	O
product	O
.	O

As	O
stated	O
in	O
equation	O
[	O
reference	O
]	O
we	O
use	O
the	O
sigmoid	O
link	O
function	O
,	O
and	O
minimize	O
the	O
-	O
regularized	O
negative	O
log	O
-	O
likelihood	O
:	O
To	O
handle	O
regularization	Task
,	O
note	O
that	O
the	O
squared	O
-	O
norm	O
of	O
a	O
complex	O
vector	O
is	O
the	O
sum	O
of	O
the	O
squared	O
modulus	O
of	O
each	O
entry	O
:	O
which	O
is	O
actually	O
the	O
sum	O
of	O
the	O
-	O
norms	O
of	O
the	O
vectors	O
of	O
the	O
real	O
and	O
imaginary	O
parts	O
.	O

We	O
can	O
finally	O
write	O
the	O
gradient	O
of	O
with	O
respect	O
to	O
a	O
real	O
embedding	O
for	O
one	O
triple	O
:	O
where	O
is	O
the	O
sigmoid	O
function	O
.	O

Algorithm	O
[	O
reference	O
]	O
describes	O
SGD	Method
for	O
this	O
formulation	O
of	O
the	O
scoring	O
function	O
.	O

When	O
contains	O
only	O
positive	O
triples	O
,	O
we	O
generate	O
negatives	O
per	O
positive	O
train	O
triple	O
,	O
by	O
corrupting	O
either	O
the	O
subject	O
or	O
the	O
object	O
of	O
the	O
positive	O
triple	O
,	O
as	O
described	O
in	O
.	O

[	O
t	O
]	O
SGD	Method
for	O
the	O
ComplEx	Method
model	Method
Training	O
set	O
,	O
Validation	O
set	O
,	O
learning	Metric
rate	Metric
,	O
embedding	O
dim	O
.	O

,	O
regularization	O
factor	O
,	O
negative	O
ratio	O
,	O
batch	O
size	O
,	O
max	O
iter	O
,	O
early	O
stopping	O
.	O

,	O
for	O
each	O
,	O
for	O
each	O
sample	O
Update	O
embeddings	O
w.r.t	O
.	O

:	O
Update	O
learning	Metric
rate	Metric
using	O
Adagrad	Method
break	O
if	O
filteredMRR	O
or	O
AP	O
on	O
decreased	O
appendix	O
:	O
WN18	Task
embeddings	Task
visualization	Task
We	O
used	O
principal	Method
component	Method
analysis	Method
(	O
PCA	Method
)	O
to	O
visualize	O
embeddings	O
of	O
the	O
relations	O
of	O
the	O
wordnet	Material
dataset	Material
(	O
WN18	Material
)	O
.	O

We	O
plotted	O
the	O
four	O
first	O
components	O
of	O
the	O
best	O
DistMult	Method
and	O
ComplEx	Method
model	Method
’s	O
embeddings	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

For	O
the	O
ComplEx	Method
model	Method
,	O
we	O
simply	O
concatenated	O
the	O
real	O
and	O
imaginary	O
parts	O
of	O
each	O
embedding	O
.	O

Most	O
of	O
WN18	Material
relations	O
describe	O
hierarchies	O
,	O
and	O
are	O
thus	O
antisymmetric	O
.	O

Each	O
of	O
these	O
hierarchic	O
relations	O
has	O
its	O
inverse	O
relation	O
in	O
the	O
dataset	O
.	O

For	O
example	O
:	O
hypernym	O
/	O
hyponym	O
,	O
part_of	O
/	O
has_part	O
,	O
synset_domain_topic_of	O
/	O
member_of_domain_topic	O
.	O

Since	O
DistMult	Method
is	O
unable	O
to	O
model	O
antisymmetry	O
,	O
it	O
will	O
correctly	O
represent	O
the	O
nature	O
of	O
each	O
pair	O
of	O
opposite	O
relations	O
,	O
but	O
not	O
the	O
direction	O
of	O
the	O
relations	O
.	O

Loosely	O
speaking	O
,	O
in	O
the	O
hypernym	O
/	O
hyponym	O
pair	O
the	O
nature	O
is	O
sharing	O
semantics	O
,	O
and	O
the	O
direction	O
is	O
that	O
one	O
entity	O
generalizes	O
the	O
semantics	O
of	O
the	O
other	O
.	O

This	O
makes	O
DistMult	Method
reprensenting	O
the	O
opposite	O
relations	O
with	O
very	O
close	O
embeddings	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
.	O

It	O
is	O
especially	O
striking	O
for	O
the	O
third	O
and	O
fourth	O
principal	O
component	O
(	O
bottom	O
-	O
left	O
)	O
.	O

Conversely	O
,	O
ComplEx	Method
manages	O
to	O
oppose	O
spatially	O
the	O
opposite	O
relations	O
.	O

