document	O
:	O
CollaboNet	Method
:	O
collaboration	O
of	O
deep	Method
neural	Method
networks	Method
for	O
biomedical	O
named	Task
entity	Task
recognition	Task
Background	O
Finding	Task
biomedical	Task
named	Task
entities	Task
is	O
one	O
of	O
the	O
most	O
essential	O
tasks	O
in	O
biomedical	Task
text	Task
mining	Task
.	O

Recently	O
,	O
deep	Method
learning	Method
-	Method
based	Method
approaches	Method
have	O
been	O
applied	O
to	O
biomedical	O
named	Task
entity	Task
recognition	Task
(	O
BioNER	Task
)	O
and	O
showed	O
promising	O
results	O
.	O

However	O
,	O
as	O
deep	Method
learning	Method
approaches	Method
need	O
an	O
abundant	O
amount	O
of	O
training	O
data	O
,	O
a	O
lack	O
of	O
data	O
can	O
hinder	O
performance	O
.	O

BioNER	O
datasets	O
are	O
scarce	O
resources	O
and	O
each	O
dataset	O
covers	O
only	O
a	O
small	O
subset	O
of	O
entity	O
types	O
.	O

Furthermore	O
,	O
many	O
bio	O
entities	O
are	O
polysemous	O
,	O
which	O
is	O
one	O
of	O
the	O
major	O
obstacles	O
in	O
named	Task
entity	Task
recognition	Task
.	O

Results	O
To	O
address	O
the	O
lack	O
of	O
data	O
and	O
the	O
entity	Task
type	Task
misclassification	Task
problem	Task
,	O
we	O
propose	O
CollaboNet	Method
which	O
utilizes	O
a	O
combination	O
of	O
multiple	O
NER	Task
models	O
.	O

In	O
CollaboNet	Method
,	O
models	O
trained	O
on	O
a	O
different	O
dataset	O
are	O
connected	O
to	O
each	O
other	O
so	O
that	O
a	O
target	O
model	O
obtains	O
information	O
from	O
other	O
collaborator	Method
models	Method
to	O
reduce	O
false	O
positives	O
.	O

Every	O
model	O
is	O
an	O
expert	O
on	O
their	O
target	O
entity	O
type	O
and	O
takes	O
turns	O
serving	O
as	O
a	O
target	O
and	O
a	O
collaborator	Method
model	Method
during	O
training	O
time	O
.	O

The	O
experimental	O
results	O
show	O
that	O
CollaboNet	Method
can	O
be	O
used	O
to	O
greatly	O
reduce	O
the	O
number	O
of	O
false	Metric
positives	Metric
and	O
misclassified	O
entities	O
including	O
polysemous	O
words	O
.	O

CollaboNet	Method
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
terms	O
of	O
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
score	Metric
.	O

Conclusions	O
We	O
demonstrated	O
the	O
benefits	O
of	O
combining	O
multiple	Method
models	Method
for	O
BioNER	Task
.	O

Our	O
model	O
has	O
successfully	O
reduced	O
the	O
number	O
of	O
misclassified	O
entities	O
and	O
improved	O
the	O
performance	O
by	O
leveraging	O
multiple	O
datasets	O
annotated	O
for	O
different	O
entity	O
types	O
.	O

Given	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
of	O
our	O
model	O
,	O
we	O
believe	O
that	O
CollaboNet	Method
can	O
improve	O
the	O
accuracy	Metric
of	O
downstream	Task
biomedical	Task
text	Task
mining	Task
applications	Task
such	O
as	O
bio	Task
-	Task
entity	Task
relation	Task
extraction	Task
.	O

Research	O
addressref	O
=	O
aff1	O
,	O
noteref	O
=	O
n1	O
,	O
email=wonjin.info@gmail.com	O
]	O
WY	O
Wonjin	O
Yoon	O
addressref	O
=	O
aff2	O
,	O
noteref	O
=	O
n1	O
,	O
email=chanhoso@korea.ac.kr	O
]	O
CHS	O
Chan	O
Ho	O
So	O
addressref	O
=	O
aff1	O
,	O
email=jinhyuk_lee@korea.ac.kr	O
]	O
JL	O
Jinhyuk	O
Lee	O
addressref	O
=	O
aff1	O
,	O
corref	O
=	O
aff1	O
,	O
email=kangj@korea.ac.kr	O
]	O
JK	O
Jaewoo	O
Kang	O
[	O
i	O
d	O
=	O
n1	O
]	O
Equal	O
contributor	O
NER	Task
Deep	Task
Learning	Task
Named	Task
Entity	Task
Recognition	Task
Text	Task
Mining	Task
section	O
:	O
Background	O
The	O
amount	O
of	O
biomedical	O
text	O
continues	O
to	O
increase	O
rapidly	O
.	O

There	O
were	O
4.7	O
million	O
full	O
-	O
text	O
online	O
accessible	O
articles	O
in	O
PubMed	O
Central	O
in	O
2017	O
.	O

One	O
of	O
the	O
obstacles	O
in	O
utilizing	O
biomedical	O
text	O
data	O
is	O
that	O
it	O
is	O
too	O
large	O
for	O
a	O
human	O
to	O
read	O
or	O
even	O
search	O
for	O
needed	O
information	O
.	O

This	O
has	O
led	O
to	O
the	O
demand	O
for	O
automated	Task
extraction	Task
of	Task
valuable	Task
information	Task
.	O

Text	Task
mining	Task
can	O
be	O
used	O
to	O
turn	O
the	O
time	Task
-	Task
consuming	Task
task	Task
into	O
a	O
fully	O
automated	Task
job	Task
.	O

Named	Task
Entity	Task
Recognition	Task
(	O
NER	Task
)	O
is	O
the	O
computerized	Task
procedure	Task
of	O
recognizing	Task
and	Task
labeling	Task
entities	Task
in	O
given	O
texts	O
.	O

In	O
the	O
biomedical	O
domain	O
,	O
typical	O
entity	O
types	O
include	O
disease	O
,	O
chemical	O
,	O
gene	O
and	O
protein	O
.	O

Biomedical	O
named	Task
entity	Task
recognition	Task
(	O
BioNER	Task
)	Task
is	O
an	O
essential	O
building	O
block	O
of	O
many	O
downstream	Task
text	Task
mining	Task
applications	Task
such	O
as	O
extracting	Task
drug	Task
-	Task
drug	Task
interactions	Task
and	O
disease	Task
-	Task
treatment	Task
relations	Task
.	O

BioNER	Method
is	O
also	O
used	O
when	O
building	O
a	O
sophisticated	O
biomedical	Method
entity	Method
search	Method
tool	Method
that	O
enables	O
users	O
to	O
pose	O
complex	O
queries	O
to	O
search	O
for	O
bio	O
-	O
entities	O
.	O

NER	Task
in	O
biomedical	Task
text	Task
mining	Task
is	O
focused	O
mainly	O
on	O
dictionary	Method
-	Method
,	O
rule	Method
-	Method
,	O
and	O
machined	Method
learning	Method
-	Method
based	Method
approaches	Method
.	O

Dictionary	Method
based	Method
systems	Method
have	O
a	O
simple	O
and	O
intuitive	O
structure	O
but	O
they	O
can	O
not	O
handle	O
unseen	O
entities	O
or	O
polysemous	O
words	O
,	O
resulting	O
in	O
low	Metric
recall	Metric
.	O

Moreover	O
,	O
building	O
and	O
maintaining	O
a	O
comprehensive	O
and	O
up	O
-	O
to	O
-	O
date	O
dictionary	O
involves	O
a	O
considerable	O
amount	O
of	O
manual	O
work	O
.	O

The	O
rule	Method
based	Method
approach	Method
is	O
more	O
scalable	O
,	O
but	O
it	O
needs	O
hand	O
crafted	O
feature	O
sets	O
to	O
fit	O
a	O
model	O
to	O
a	O
dataset	O
.	O

These	O
rule	Method
and	Method
dictionary	Method
-	Method
based	Method
approaches	Method
can	O
achieve	O
high	O
precision	Metric
but	O
can	O
produce	O
incorrect	O
predictions	O
when	O
a	O
new	O
word	O
,	O
which	O
is	O
not	O
in	O
the	O
training	O
data	O
,	O
appears	O
in	O
a	O
sentence	O
(	O
out	Task
-	Task
of	Task
-	Task
vocabulary	Task
problem	Task
)	O
.	O

This	O
out	Task
-	Task
of	Task
-	Task
vocabulary	Task
problem	Task
occurs	O
frequently	O
especially	O
in	O
the	O
biomedical	O
domain	O
,	O
as	O
it	O
is	O
common	O
for	O
a	O
new	O
biomedical	O
term	O
,	O
such	O
as	O
a	O
new	O
drug	O
name	O
,	O
to	O
be	O
registered	O
in	O
this	O
domain	O
.	O

Recently	O
,	O
studies	O
have	O
demonstrated	O
the	O
effectiveness	O
of	O
deep	Method
learning	Method
based	Method
methods	Method
.	O

Sahu	O
and	O
Anand	O
demonstrated	O
the	O
efficiency	O
of	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
for	O
NER	Task
in	O
biomedical	O
text	O
.	O

The	O
model	O
by	O
Sahu	O
and	O
Anand	O
is	O
composed	O
of	O
a	O
bidirectional	Method
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
Network	Method
(	O
BiLSTM	Method
)	O
and	O
Conditional	Method
Random	Method
Field	Method
(	O
CRF	Method
)	O
.	O

Sahu	O
and	O
Anand	O
also	O
used	O
character	Method
level	Method
word	Method
embeddings	Method
but	O
could	O
not	O
demonstrate	O
their	O
benefits	O
.	O

Habibi	O
et	O
al	O
.	O

combined	O
the	O
BiLSTM	O
-	O
CRF	Method
model	O
implementation	O
of	O
Lample	O
et	O
al	O
.	O

and	O
the	O
word	O
embeddings	O
of	O
Pyysalo	O
et	O
al	O
.	O

.	O

Habibi	O
et	O
al	O
.	O

utilized	O
character	O
level	O
word	O
embeddings	O
to	O
capture	O
characteristics	O
,	O
such	O
as	O
orthographic	O
features	O
,	O
of	O
bio	O
-	O
medical	O
entities	O
and	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
character	Method
level	Method
word	Method
embeddings	Method
in	O
BioNER	Method
.	O

Although	O
these	O
models	O
showed	O
some	O
promising	O
results	O
,	O
NER	Task
is	O
still	O
a	O
very	O
challenging	O
task	O
in	O
the	O
biomedical	Task
domain	Task
for	O
the	O
following	O
reasons	O
.	O

First	O
,	O
a	O
limited	O
amount	O
of	O
training	O
data	O
is	O
available	O
for	O
BioNER	Task
tasks	Task
.	O

Gold	O
-	O
standard	O
datasets	O
contain	O
annotations	O
of	O
one	O
or	O
two	O
entity	O
types	O
.	O

For	O
example	O
,	O
the	O
NCBI	O
corpus	O
includes	O
annotations	O
of	O
diseases	O
but	O
not	O
of	O
other	O
types	O
of	O
entities	O
such	O
as	O
genes	O
and	O
proteins	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
JNLPBA	Material
corpus	Material
contains	O
annotations	O
of	O
only	O
genes	O
and	O
proteins	O
.	O

Therefore	O
,	O
the	O
data	O
for	O
each	O
entity	O
type	O
comprises	O
only	O
a	O
small	O
portion	O
of	O
the	O
total	O
amount	O
of	O
annotated	O
data	O
.	O

Multi	Task
-	Task
task	Task
learning	Task
(	O
MTL	Task
)	O
is	O
a	O
method	O
for	O
training	O
a	O
single	O
model	O
for	O
multiple	O
tasks	O
at	O
the	O
same	O
time	O
.	O

MTL	Method
can	O
leverage	O
different	O
datasets	O
that	O
are	O
collected	O
for	O
different	O
but	O
related	O
tasks	O
.	O

Although	O
extracting	O
genes	O
is	O
different	O
from	O
extracting	O
chemicals	O
,	O
both	O
tasks	O
require	O
learning	O
some	O
common	O
features	O
that	O
can	O
help	O
understand	O
the	O
linguistic	O
expressions	O
of	O
biomedical	O
texts	O
.	O

Crichton	O
et	O
al	O
.	O

developed	O
an	O
MTL	Method
model	Method
that	O
was	O
trained	O
on	O
various	O
source	O
datasets	O
containing	O
annotations	O
of	O
different	O
subsets	O
of	O
entity	O
types	O
.	O

An	O
MTL	Method
model	Method
by	O
Wang	O
et	O
al	O
.	O

achieved	O
performance	O
comparable	O
to	O
that	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
task	O
NER	Task
models	O
.	O

Inspired	O
by	O
the	O
previous	O
studies	O
,	O
we	O
propose	O
CollaboNet	Method
which	O
uses	O
the	O
collaboration	O
of	O
multiple	O
models	O
.	O

Unlike	O
the	O
conventional	O
MTL	Method
methods	Method
which	O
use	O
only	O
a	O
single	O
static	Method
model	Method
,	O
CollaboNet	Method
is	O
composed	O
of	O
multiple	O
models	O
trained	O
on	O
different	O
datasets	O
for	O
different	O
tasks	O
.	O

Each	O
model	O
in	O
CollaboNet	Method
is	O
trained	O
on	O
dataset	O
annotated	O
on	O
a	O
specific	O
type	O
of	O
entity	O
and	O
becomes	O
an	O
expert	O
on	O
their	O
own	O
entity	O
type	O
.	O

Despite	O
the	O
high	O
recall	Metric
obtained	O
by	O
the	O
MTL	Method
based	Method
models	Method
,	O
the	O
precision	Metric
of	O
these	O
models	O
is	O
relatively	O
low	O
.	O

Since	O
MTL	Method
based	Method
models	Method
are	O
trained	O
on	O
multiple	O
types	O
of	O
entities	O
and	O
larger	O
training	O
data	O
,	O
they	O
have	O
a	O
broader	O
coverage	O
of	O
various	O
biomedical	O
entities	O
,	O
which	O
naturally	O
results	O
in	O
high	O
recall	Metric
.	O

On	O
the	O
other	O
hand	O
,	O
as	O
the	O
MTL	Method
models	Method
are	O
trained	O
on	O
combinations	O
of	O
different	O
entity	O
types	O
,	O
they	O
tend	O
to	O
have	O
difficulty	O
in	O
differentiating	O
among	O
entity	O
types	O
,	O
resulting	O
in	O
lower	O
precision	Metric
.	O

Another	O
reason	O
NER	Task
is	O
difficult	O
in	O
the	O
biomedical	Task
domain	Task
is	O
that	O
an	O
entity	O
could	O
be	O
labeled	O
as	O
different	O
entity	O
types	O
depending	O
on	O
its	O
textual	O
context	O
.	O

In	O
our	O
experiments	O
,	O
we	O
observed	O
that	O
many	O
incorrect	O
predictions	O
were	O
a	O
result	O
of	O
the	O
polysemy	Task
problem	Task
,	O
in	O
which	O
a	O
word	O
,	O
for	O
example	O
,	O
can	O
be	O
used	O
as	O
both	O
a	O
gene	O
and	O
disease	O
name	O
.	O

Models	O
designed	O
to	O
predict	O
disease	O
entities	O
misidentify	O
some	O
genes	O
as	O
diseases	O
.	O

This	O
misidentification	O
of	O
entity	O
types	O
increases	O
the	O
false	Metric
positive	Metric
rate	Metric
.	O

For	O
instance	O
,	O
BiLSTM	O
-	O
CRF	Method
based	O
models	O
for	O
disease	O
entities	O
mistakenly	O
label	O
the	O
gene	O
name	O
“	O
BRCA1	O
”	O
as	O
a	O
disease	O
entity	O
because	O
there	O
are	O
disease	O
names	O
such	O
as	O
“	O
BRCA1	O
abnormalities	O
”	O
or	O
“	O
Brca1	O
-	O
deficient	O
”	O
in	O
the	O
training	O
set	O
.	O

Besides	O
,	O
the	O
training	O
set	O
that	O
annotates	O
“	O
VHL	O
”	O
(	O
Von	O
Hippel	O
-	O
Lindau	O
disease	O
)	O
as	O
a	O
disease	O
entity	O
confuses	O
the	O
models	O
because	O
VHL	Method
is	O
also	O
used	O
as	O
a	O
gene	O
name	O
,	O
since	O
the	O
mutation	O
of	O
this	O
gene	O
causes	O
VHL	O
disease	O
.	O

To	O
solve	O
the	O
false	Task
positive	Task
problems	Task
due	O
to	O
polysemous	O
words	O
,	O
CollaboNet	Method
aggregates	O
the	O
results	O
of	O
collaborator	Method
models	Method
,	O
and	O
uses	O
them	O
as	O
an	O
additional	O
input	O
to	O
the	O
target	O
model	O
.	O

Consider	O
the	O
case	O
of	O
predicting	O
the	O
disease	Task
entity	Task
VHL	Task
utilizing	O
the	O
outputs	O
of	O
gene	Method
and	Method
chemical	Method
models	Method
.	O

Once	O
a	O
gene	Method
model	Method
predicts	O
VHL	Method
as	O
a	O
gene	O
,	O
the	O
gene	Method
model	Method
informs	O
a	O
disease	Method
model	Method
that	O
VHL	O
is	O
a	O
gene	O
entity	O
so	O
that	O
the	O
disease	Method
model	Method
will	O
not	O
predict	O
VHL	O
as	O
a	O
disease	O
.	O

In	O
CollaboNet	Method
,	O
each	O
model	O
is	O
individually	O
trained	O
on	O
an	O
entity	O
type	O
and	O
then	O
further	O
trained	O
on	O
the	O
outputs	O
of	O
other	O
models	O
that	O
are	O
trained	O
on	O
the	O
other	O
entity	O
types	O
.	O

The	O
models	O
in	O
CollaboNet	Method
take	O
turns	O
in	O
being	O
the	O
target	O
and	O
collaborator	O
models	O
during	O
training	O
.	O

Consequently	O
,	O
each	O
model	O
is	O
an	O
expert	O
in	O
its	O
own	O
domain	O
and	O
helps	O
improve	O
the	O
accuracy	Metric
by	O
leveraging	O
the	O
multi	O
-	O
domain	O
information	O
from	O
the	O
other	O
models	O
.	O

section	O
:	O
Methods	O
In	O
the	O
following	O
section	O
,	O
we	O
first	O
discuss	O
a	O
BiLSTM	O
-	O
CRF	Method
model	O
for	O
biomedical	O
named	Task
entity	Task
recognition	Task
.	O

The	O
overall	O
structure	O
of	O
the	O
BiLSTM	O
-	O
CRF	Method
model	O
is	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Next	O
,	O
we	O
introduce	O
the	O
structure	O
of	O
CollaboNet	Method
,	O
which	O
is	O
comprised	O
of	O
a	O
set	O
of	O
BiLSTM	O
-	O
CRF	Method
models	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Problem	O
Definition	O
Named	Task
entity	Task
recognition	Task
involves	O
annotating	O
words	O
in	O
a	O
sentence	O
as	O
named	O
entities	O
.	O

More	O
formally	O
,	O
given	O
an	O
input	O
sequence	O
,	O
we	O
predict	O
corresponding	O
labels	O
.	O

We	O
use	O
the	O
BIOES	Method
scheme	Method
for	O
representing	O
,	O
where	O
B	O
stands	O
for	O
Beginning	O
,	O
I	O
for	O
Inside	O
,	O
O	O
for	O
Out	O
,	O
E	O
for	O
End	O
,	O
and	O
S	O
for	O
Single	O
.	O

subsection	O
:	O
Embedding	Method
Layer	Method
subsubsection	O
:	O
Word	Method
Embedding	Method
(	O
WE	O
)	O
Word	Method
embedding	Method
is	O
an	O
effective	O
way	O
of	O
representing	Task
words	Task
.	O

As	O
word	O
embeddings	O
capture	O
semantic	O
and	O
syntactic	O
meanings	O
of	O
words	O
,	O
they	O
have	O
been	O
widely	O
used	O
in	O
various	O
natural	Task
language	Task
processing	Task
tasks	Task
including	O
named	Task
entity	Task
recognition	Task
.	O

The	O
experiment	O
of	O
Habibi	O
et	O
al	O
.	O

showed	O
that	O
word	Method
embeddings	Method
trained	O
on	O
biomedical	O
corpora	O
notably	O
improved	O
the	O
performance	O
of	O
BioNER	Method
models	Method
.	O

Pyysalo	O
et	O
al	O
.	O

were	O
the	O
first	O
to	O
suggest	O
training	O
word	O
embeddings	O
on	O
biomedical	O
corpora	O
from	O
PubMed	O
,	O
PubMed	O
Central	O
(	O
PMC	O
)	O
,	O
and	O
Wikipedia	O
.	O

The	O
results	O
of	O
Pyysalo	O
et	O
al	O
.	O

and	O
Habibi	O
et	O
al	O
.	O

suggest	O
that	O
using	O
word	Method
embeddings	Method
trained	O
on	O
biomedical	O
corpora	O
is	O
essential	O
for	O
BioNER	O
.	O

We	O
also	O
use	O
the	O
trained	O
word	O
embeddings	O
provided	O
by	O
Pyysalo	O
et	O
al	O
.	O

.	O

For	O
each	O
word	O
in	O
a	O
sequence	O
,	O
we	O
denote	O
a	O
word	O
represented	O
by	O
a	O
word	Method
embedding	Method
as	O
where	O
is	O
a	O
dimension	O
of	O
the	O
word	Method
embedding	Method
.	O

subsubsection	O
:	O
Character	Method
Level	Method
Word	Method
Embedding	Method
(	O
CLWE	Method
)	O
To	O
give	O
our	O
model	O
character	O
level	O
morphological	O
information	O
(	O
e.g.	O
,	O
‘	O
-	O
ase	O
’	O
is	O
common	O
in	O
protein	O
entities	O
)	O
,	O
we	O
also	O
leverage	O
the	O
character	O
level	O
information	O
of	O
each	O
word	O
.	O

We	O
build	O
character	Method
level	Method
word	Method
embeddings	Method
(	O
CLWEs	Method
)	O
using	O
a	O
convolution	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
,	O
similar	O
to	O
the	O
work	O
of	O
Santos	O
and	O
Zadrozny	O
.	O

Given	O
a	O
word	O
,	O
composed	O
of	O
number	O
of	O
characters	O
,	O
we	O
represent	O
where	O
is	O
a	O
randomly	Method
initialized	Method
character	Method
embedding	Method
for	O
each	O
unique	O
character	O
.	O

Note	O
that	O
unlike	O
the	O
word	Method
embeddings	Method
trained	O
on	O
separate	O
biomedical	O
corpora	O
,	O
character	O
embeddings	O
are	O
learned	O
from	O
only	O
the	O
BioNER	Task
task	Task
.	O

For	O
the	O
CNN	Method
,	O
padding	O
of	O
the	O
proper	O
size	O
(	O
)	O
according	O
to	O
window	O
size	O
should	O
be	O
attached	O
before	O
and	O
after	O
each	O
word	O
.	O

We	O
obtain	O
a	O
window	O
vector	O
by	O
simply	O
concatenating	O
the	O
character	O
embeddings	O
of	O
with	O
the	O
character	O
embeddings	O
of	O
characters	O
on	O
both	O
sides	O
:	O
From	O
the	O
window	O
vector	O
,	O
we	O
perform	O
a	O
convolution	Method
operation	Method
as	O
follows	O
:	O
where	O
and	O
denote	O
a	O
trainable	Method
filter	Method
and	O
bias	Method
,	O
respectively	O
.	O

We	O
obtain	O
the	O
element	O
-	O
wise	O
maximum	O
values	O
,	O
and	O
the	O
output	O
is	O
a	O
character	O
level	O
word	O
embedding	O
denoted	O
as	O
.	O

We	O
concatenate	O
the	O
character	Method
level	Method
word	Method
embedding	Method
with	O
the	O
word	Method
embedding	Method
trained	O
on	O
biomedical	O
corpora	O
as	O
to	O
utilize	O
both	O
representations	O
in	O
our	O
model	O
.	O

subsection	O
:	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	Method
LSTM	Method
)	O
A	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
is	O
a	O
neural	Method
network	Method
that	O
effectively	O
handles	O
variable	O
-	O
length	O
inputs	O
.	O

RNNs	Method
have	O
proven	O
to	O
be	O
useful	O
in	O
various	O
natural	Task
language	Task
processing	Task
tasks	Task
including	O
language	Task
modeling	Task
,	O
speech	Task
recognition	Task
and	O
machine	Task
translation	Task
.	O

Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	Method
LSTM	Method
)	Method
is	O
one	O
of	O
the	O
most	O
frequently	O
used	O
variants	O
of	O
recurrent	Method
neural	Method
networks	Method
.	O

Our	O
model	O
uses	O
the	O
LSTM	Method
architecture	Method
from	O
Graves	O
et	O
al	O
.	O

.	O

Given	O
the	O
outputs	O
of	O
an	O
embedding	Method
layer	Method
,	O
the	O
hidden	O
states	O
of	O
LSTM	Method
are	O
calculated	O
as	O
follows	O
:	O
where	O
and	O
denote	O
a	O
logistic	O
sigmoid	O
function	O
and	O
a	O
hyperbolic	O
tangent	O
function	O
,	O
respectively	O
,	O
and	O
is	O
an	O
element	O
-	O
wise	O
product	O
.	O

We	O
use	O
a	O
forward	Method
LSTM	Method
that	O
extracts	O
the	O
representations	O
of	O
inputs	O
in	O
the	O
forward	O
direction	O
,	O
and	O
we	O
use	O
a	O
backward	Method
LSTM	Method
that	O
represents	O
the	O
inputs	O
in	O
the	O
backward	O
direction	O
.	O

We	O
concatenate	O
the	O
two	O
states	O
coming	O
from	O
the	O
forward	Method
LSTM	Method
and	O
the	O
backward	Method
LSTM	Method
to	O
form	O
the	O
hidden	O
states	O
of	O
the	O
bi	Method
-	Method
directional	Method
LSTM	Method
(	Method
BiLSTM	Method
)	Method
.	O

BiLSTM	Method
,	O
proposed	O
by	O
Schuster	O
and	O
Paliwal	Method
,	O
was	O
extensively	O
used	O
in	O
various	O
sequence	Task
encoding	Task
tasks	Task
.	O

We	O
obtain	O
a	O
set	O
of	O
hidden	O
states	O
where	O
and	O
are	O
hidden	O
states	O
of	O
forward	Method
and	Method
backward	Method
LSTMs	Method
,	O
respectively	O
,	O
at	O
a	O
time	O
step	O
.	O

subsection	O
:	O
Bidirectional	O
LSTM	O
with	O
Conditional	Method
Random	Method
Field	Method
(	O
BiLSTM	O
-	O
CRF	Method
)	O
While	O
BiLSTM	Method
handles	O
long	Task
term	Task
dependency	Task
problems	Task
as	O
well	O
as	O
backward	O
dependency	O
issues	O
,	O
modeling	O
dependencies	O
among	O
adjacent	O
output	O
tags	O
helps	O
improve	O
the	O
performance	O
of	O
the	O
sequence	Method
labeling	Method
models	Method
.	O

We	O
applied	O
a	O
Conditional	Method
Random	Method
Field	Method
(	O
CRF	Method
)	O
to	O
the	O
output	O
layer	O
of	O
the	O
BiLSTM	Method
to	O
capture	O
these	O
dependencies	O
.	O

First	O
,	O
we	O
compute	O
the	O
probability	O
of	O
each	O
label	O
given	O
the	O
sequence	O
as	O
follows	O
:	O
where	O
and	O
are	O
parameters	O
of	O
the	O
fully	Method
connected	Method
layer	Method
for	O
BIOES	O
tags	O
,	O
and	O
the	O
function	O
computes	O
the	O
probability	O
of	O
each	O
tag	O
.	O

Based	O
on	O
the	O
probability	O
and	O
the	O
CRF	Method
layer	O
,	O
our	O
training	Metric
objective	Metric
to	O
minimize	O
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
cross	Metric
entropy	Metric
loss	Metric
for	O
the	O
label	O
,	O
and	O
is	O
the	O
negative	O
sentence	O
-	O
level	O
log	O
likelihood	O
.	O

The	O
score	O
of	O
a	O
tag	O
is	O
the	O
summation	O
of	O
the	O
transition	O
score	O
and	O
the	O
emission	O
score	O
from	O
our	O
LSTM	Method
at	O
time	O
step	O
.	O

At	O
test	O
time	O
,	O
we	O
use	O
Viterbi	Method
decoding	Method
to	O
find	O
the	O
most	O
probable	O
sequence	O
given	O
the	O
outputs	O
of	O
the	O
BiLSTM	O
-	O
CRF	Method
model	O
.	O

subsection	O
:	O
CollaboNet	Method
CollaboNet	Method
,	O
our	O
novel	O
NER	Task
model	O
,	O
is	O
composed	O
of	O
multiple	O
BiLSTM	O
-	O
CRF	Method
models	O
,	O
and	O
following	O
the	O
terminology	O
of	O
,	O
we	O
call	O
each	O
BiLSTM	O
-	O
CRF	Method
model	O
a	O
single	Method
-	Method
task	Method
model	Method
(	O
STM	Method
)	O
.	O

In	O
CollaboNet	Method
,	O
each	O
STM	Method
is	O
trained	O
on	O
a	O
specific	O
dataset	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
and	O
each	O
STM	Method
is	O
regarded	O
as	O
an	O
expert	O
on	O
a	O
particular	O
entity	O
type	O
.	O

These	O
experts	O
help	O
each	O
other	O
since	O
the	O
knowledge	O
of	O
each	O
expert	O
is	O
transferred	O
to	O
all	O
the	O
other	O
experts	O
.	O

Training	O
CollaboNet	Method
consists	O
of	O
phases	O
and	O
in	O
each	O
phase	O
,	O
except	O
for	O
the	O
first	O
preparation	O
phase	O
,	O
every	O
STM	Method
is	O
trained	O
on	O
a	O
single	O
dataset	O
for	O
one	O
epoch	O
.	O

More	O
formally	O
,	O
let	O
us	O
denote	O
a	O
set	O
of	O
datasets	O
as	O
,	O
and	O
a	O
single	Method
-	Method
task	Method
model	Method
as	O
,	O
which	O
is	O
trained	O
on	O
the	O
-	O
th	O
dataset	O
in	O
phase	O
.	O

In	O
the	O
preparation	O
phase	O
(	O
)	O
of	O
CollaboNet	Method
,	O
each	O
STM	Method
is	O
trained	O
independently	O
on	O
a	O
corresponding	O
dataset	O
until	O
the	O
performance	O
of	O
each	O
model	O
converges	O
.	O

Note	O
that	O
an	O
STM	O
in	O
the	O
preparation	O
phase	O
(	O
)	O
is	O
the	O
same	O
as	O
a	O
single	O
BiLSTM	O
-	O
CRF	Method
model	O
.	O

In	O
the	O
preparation	Task
phase	Task
,	O
we	O
assume	O
that	O
each	O
model	O
has	O
obtained	O
the	O
maximum	O
amount	O
of	O
knowledge	O
about	O
the	O
-	O
th	O
dataset	O
.	O

In	O
the	O
subsequent	O
phases	O
,	O
where	O
,	O
we	O
select	O
an	O
STM	Method
which	O
is	O
an	O
expert	O
on	O
the	O
dataset	O
.	O

We	O
refer	O
to	O
the	O
target	O
STM	O
as	O
the	O
target	O
model	O
,	O
and	O
the	O
remaining	O
STMs	O
as	O
the	O
collaborator	Method
models	Method
.	O

To	O
train	O
the	O
target	O
model	O
,	O
we	O
use	O
inputs	O
from	O
the	O
target	O
dataset	O
and	O
outputs	O
from	O
collaborator	Method
models	Method
.	O

We	O
train	O
each	O
STM	Method
on	O
its	O
dataset	O
for	O
one	O
epoch	O
,	O
and	O
change	O
the	O
target	O
STM	O
as	O
follows	O
:	O
where	O
denotes	O
concatenation	O
and	O
denotes	O
an	O
aggregation	Method
operation	Method
such	O
as	O
max	Method
pooling	Method
or	O
concatenation	Method
.	O

We	O
used	O
weighted	Method
max	Method
pooling	Method
for	O
the	O
aggregation	Task
operation	Task
.	O

is	O
the	O
input	O
sequences	O
of	O
-	O
th	O
dataset	O
,	O
and	O
is	O
output	O
,	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
.	O

When	O
aggregating	O
the	O
results	O
of	O
collaborator	Method
models	Method
,	O
we	O
multiply	O
each	O
of	O
the	O
results	O
by	O
a	O
weighting	O
factor	O
.	O

The	O
results	O
are	O
used	O
to	O
train	O
the	O
model	O
.	O

Using	O
the	O
outputs	O
obtained	O
by	O
Equation	O
[	O
reference	O
]	O
,	O
we	O
train	O
for	O
one	O
epoch	O
,	O
and	O
it	O
becomes	O
in	O
the	O
next	O
phase	O
.	O

The	O
CRF	Method
layer	O
is	O
attached	O
to	O
the	O
final	O
output	O
of	O
.	O

Once	O
we	O
iterate	O
all	O
the	O
target	O
datasets	O
,	O
the	O
next	O
phase	O
begins	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Datasets	O
We	O
used	O
5	O
datasets	O
(	O
BC2GM	O
,	O
BC4CHEMD	O
,	O
BC5CDR	Material
,	O
JNLPBA	Material
,	O
NCBI	O
)	O
,	O
all	O
of	O
which	O
were	O
collected	O
by	O
Crichton	O
et	O
al	O
.	O

.	O

Each	O
of	O
the	O
5	O
datasets	O
were	O
constructed	O
from	O
MEDLINE	O
abstracts	O
,	O
and	O
we	O
used	O
the	O
BIOES	O
notation	O
format	O
for	O
named	O
entity	O
labels	O
.	O

Each	O
dataset	O
focuses	O
on	O
one	O
of	O
the	O
three	O
biomedical	O
entity	O
types	O
:	O
disease	O
,	O
chemical	O
,	O
and	O
gene	O
/	O
protein	O
.	O

We	O
did	O
not	O
use	O
cell	O
-	O
type	O
entity	O
tags	O
from	O
JNLPBA	Material
for	O
the	O
entity	O
types	O
.	O

All	O
the	O
datasets	O
are	O
comprised	O
of	O
pairs	O
of	O
input	O
sentences	O
and	O
biomedical	O
entity	O
labels	O
for	O
the	O
sentences	O
.	O

While	O
the	O
JNLPBA	Material
dataset	O
has	O
only	O
training	O
and	O
test	O
sets	O
,	O
the	O
other	O
four	O
datasets	O
contain	O
training	O
,	O
development	O
and	O
test	O
sets	O
.	O

For	O
JNLPBA	Material
,	O
we	O
used	O
part	O
of	O
its	O
training	O
set	O
as	O
its	O
development	O
set	O
which	O
is	O
the	O
same	O
size	O
as	O
its	O
test	O
set	O
.	O

Also	O
,	O
we	O
found	O
that	O
the	O
JNLPBA	Material
dataset	O
from	O
Crichton	O
et	O
al	O
.	O

contained	O
sentences	O
that	O
were	O
incorrectly	O
split	O
.	O

So	O
we	O
preprocessed	O
the	O
original	O
dataset	O
by	O
Kim	O
et	O
al	O
.	O

with	O
a	O
more	O
accurate	O
sentence	Task
separation	Task
.	O

The	O
BC5CDR	Material
dataset	O
has	O
the	O
sub	O
-	O
datasets	O
BC5CDR	Material
-	O
chem	O
,	O
BC5CDR	Material
-	Material
disease	Material
and	O
BC5CDR	Material
-	O
both	O
,	O
and	O
they	O
contain	O
chemical	O
entity	O
types	O
,	O
disease	O
entity	O
types	O
,	O
and	O
both	O
entity	O
types	O
,	O
respectively	O
.	O

We	O
reported	O
the	O
performance	O
on	O
BC5CDR	Material
-	Material
chem	Material
and	O
BC5CDR	Material
-	Material
disease	Material
.	O

We	O
have	O
a	O
total	O
of	O
six	O
datasets	O
:	O
BC2GM	O
,	O
BC4CHEMD	O
,	O
BC5CDR	Material
-	Material
chem	Material
,	O
BC5CDR	Material
-	Material
disease	Material
,	O
JNLPBA	Material
,	O
and	O
NCBI	O
.	O

subsection	O
:	O
Metric	O
For	O
the	O
evaluation	O
of	O
the	O
named	Task
entity	Task
recognition	Task
task	O
,	O
true	O
positives	O
are	O
counted	O
from	O
exact	O
matches	O
between	O
predicted	O
entity	O
spans	O
and	O
ground	O
truth	O
spans	O
based	O
on	O
the	O
BIOES	Method
notation	Method
.	O

We	O
also	O
designed	O
and	O
applied	O
a	O
simple	O
post	Method
-	Method
processing	Method
step	Method
that	O
corrects	O
invalid	O
BIOES	O
sequences	O
.	O

This	O
simple	O
step	O
improved	O
precision	Metric
by	O
about	O
0.1	O
%	O
to	O
0.5	O
%	O
,	O
and	O
thus	O
boosted	O
the	O
F1	Metric
score	Metric
by	O
about	O
0.04	O
%	O
to	O
0.3	O
%	O
.	O

Precision	Metric
,	O
recall	Metric
and	O
F1	Metric
scores	Metric
were	O
used	O
to	O
evaluate	O
the	O
models	O
.	O

M	O
=	O
total	O
number	O
of	O
predicted	O
entities	O
in	O
the	O
sequence	O
.	O

N	O
=	O
total	O
number	O
of	O
ground	O
truth	O
entities	O
in	O
the	O
sequence	O
.	O

C	O
=	O
total	O
number	O
of	O
correct	O
entities	O
.	O

subsection	O
:	O
Settings	O
and	O
hyperparameters	O
We	O
used	O
the	O
200	Method
dimensional	Method
word	Method
embedding	Method
(	O
WE	Method
)	O
by	O
Pyysalo	O
et	O
al	O
.	O

which	O
was	O
trained	O
on	O
PubMed	O
,	O
PubMed	O
Central	O
(	O
PMC	O
)	O
and	O
Wikipedia	O
text	O
,	O
and	O
it	O
contains	O
about	O
5	O
million	O
words	O
.	O

Word2vec	O
was	O
used	O
to	O
train	O
the	O
word	Method
embedding	Method
.	O

For	O
character	Task
level	Task
word	Task
embedding	Task
(	O
CLWE	Task
)	O
,	O
we	O
used	O
window	O
sizes	O
of	O
3	O
,	O
5	O
,	O
and	O
7	O
.	O

We	O
used	O
AdaGrad	Method
optimizer	Method
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
which	O
was	O
exponentially	O
decayed	O
for	O
each	O
epoch	O
by	O
0.95	O
.	O

The	O
dimension	O
of	O
the	O
character	O
embedding	O
(	O
)	O
was	O
30	O
and	O
dimension	O
of	O
the	O
character	Method
level	Method
word	Method
embedding	Method
(	O
)	O
was	O
200	O
*	O
3	O
.	O

We	O
used	O
300	O
hidden	O
units	O
for	O
both	O
forward	Method
and	Method
backward	Method
LSTMs	Method
.	O

We	O
applied	O
dropout	Method
to	O
two	O
parts	O
of	O
CollaboNet	Method
:	O
outputs	O
of	O
CLWE	Method
(	O
0.5	O
)	O
and	O
BiLSTM	Method
(	O
0.3	O
)	O
.	O

The	O
mini	O
-	O
batch	O
size	O
for	O
our	O
experiment	O
was	O
10	O
.	O

Most	O
of	O
our	O
hyperparameter	O
settings	O
are	O
similar	O
to	O
those	O
of	O
Wang	O
et	O
al	O
.	O

.	O

Only	O
a	O
few	O
settings	O
such	O
as	O
the	O
dropout	Metric
rates	Metric
were	O
different	O
from	O
the	O
hyperparameters	Method
of	Method
Wang	Method
.	O

We	O
tuned	O
these	O
hyperparameters	O
using	O
validation	O
sets	O
.	O

The	O
preparation	Task
phase	Task
for	O
6	O
datasets	O
takes	O
approximately	O
900	O
minutes	O
,	O
which	O
is	O
the	O
same	O
amount	O
of	O
time	O
it	O
takes	O
to	O
train	O
6	O
single	Method
-	Method
task	Method
models	Method
.	O

The	O
rest	O
of	O
the	O
phases	O
require	O
3000	O
minutes	O
for	O
complete	O
training	O
.	O

If	O
we	O
exclude	O
BC4GM	Method
,	O
the	O
largest	O
dataset	O
,	O
then	O
the	O
training	Metric
time	Metric
for	O
is	O
reduced	O
to	O
1500	O
minutes	O
,	O
which	O
is	O
half	O
the	O
time	O
required	O
for	O
the	O
remainder	O
phases	O
.	O

section	O
:	O
Results	O
The	O
experimental	O
results	O
of	O
the	O
baseline	O
models	O
and	O
CollaboNet	Method
are	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
,	O
respectively	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
single	Method
-	Method
task	Method
models	Method
(	O
STMs	Method
)	O
where	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
between	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
multi	Method
-	Method
task	Method
learning	Method
model	Method
(	O
MTM	Method
)	O
and	O
our	O
CollaboNet	Method
.	O

Since	O
Wang	O
et	O
al	O
.	O

used	O
BC5CDR	Material
-	O
both	O
for	O
their	O
experiments	O
,	O
we	O
reran	O
their	O
models	O
on	O
BC5CDR	Material
-	Material
chem	Material
and	O
BC5CDR	Material
-	Material
disease	Material
for	O
a	O
fair	O
comparison	O
with	O
other	O
models	O
.	O

The	O
rerun	Metric
scores	Metric
are	O
denoted	O
with	O
asterisks	O
.	O

We	O
conducted	O
10	O
experiments	O
with	O
10	O
different	O
random	O
initializations	O
on	O
our	O
STM	Method
.	O

We	O
take	O
arithmetic	O
mean	O
over	O
the	O
6	O
datasets	O
to	O
compare	O
the	O
overall	O
performance	O
of	O
each	O
model	O
.	O

subsection	O
:	O
Performance	O
of	O
Single	Method
-	Method
Task	Method
Models	Method
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
STMs	Method
of	O
Habibi	O
et	O
al	O
.	O

and	O
Wang	O
et	O
al	O
.	O

(	O
baseline	Method
STMs	Method
)	O
,	O
and	O
our	O
STM	Method
on	O
the	O
6	O
datasets	O
.	O

While	O
the	O
baseline	O
STMs	Method
applied	O
BiLSTM	Method
for	O
the	O
Character	Method
Level	Method
Word	Method
Embedding	Method
(	Method
CLWE	Method
)	Method
layer	Method
,	O
our	O
STM	Method
used	O
Convolution	Method
Neural	Method
Network	Method
(	O
CNN	Method
)	O
for	O
the	O
CLWE	Method
layer	Method
.	O

On	O
average	O
,	O
our	O
STM	Method
significantly	O
outperforms	O
the	O
baseline	O
STMs	Method
in	O
terms	O
of	O
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
score	Metric
.	O

Although	O
,	O
Sahu	O
and	O
Anand	O
tried	O
to	O
improve	O
the	O
performance	O
of	O
NER	Task
models	O
with	O
CNN	Method
based	Method
CLWE	Method
layer	Method
,	O
they	O
have	O
failed	O
to	O
do	O
so	O
.	O

In	O
our	O
experiments	O
,	O
however	O
,	O
our	O
STM	Method
outperforms	O
other	O
baseline	O
STMs	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
STM	Method
with	O
CNN	Method
based	Method
CLWE	Method
layer	Method
.	O

subsection	O
:	O
Performance	O
of	O
CollaboNet	Method
Comparing	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
,	O
CollaboNet	Method
achieves	O
higher	O
precision	Metric
and	O
F1	Metric
score	Metric
than	O
most	O
STM	Method
models	Method
on	O
all	O
datasets	O
.	O

On	O
average	O
,	O
CollaboNet	Method
has	O
improved	O
both	O
precision	Metric
and	O
recall	Metric
.	O

CollaboNet	Method
also	O
outperforms	O
the	O
multi	Method
-	Method
task	Method
model	Method
(	O
MTM	Method
)	O
from	O
Wang	O
et	O
al	O
.	O

on	O
4	O
out	O
of	O
6	O
datasets	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

While	O
multi	Method
-	Method
task	Method
learning	Method
has	O
improved	O
performance	O
in	O
previous	O
studies	O
,	O
using	O
CollaboNet	Method
,	O
which	O
consists	O
of	O
expert	Method
models	Method
trained	O
for	O
each	O
entity	O
type	O
,	O
could	O
further	O
improve	O
biomedical	O
named	Task
entity	Task
recognition	Task
performance	O
.	O

section	O
:	O
Discussion	O
Compared	O
to	O
baseline	O
models	O
,	O
CollaboNet	Method
achieves	O
higher	O
precision	Metric
on	O
average	O
.	O

Even	O
though	O
we	O
observe	O
a	O
slight	O
increase	O
in	O
recall	Metric
,	O
the	O
increase	O
in	O
precision	Metric
is	O
more	O
valuable	O
than	O
that	O
in	O
recall	Metric
when	O
considering	O
the	O
practical	O
use	O
of	O
the	O
bioNER	Method
systems	Method
.	O

Important	O
information	O
tends	O
to	O
be	O
repeated	O
in	O
a	O
large	O
size	O
text	O
corpus	O
.	O

Therefore	O
,	O
missing	O
a	O
few	O
entities	O
may	O
not	O
hinder	O
the	O
performance	O
of	O
an	O
entire	O
system	O
,	O
as	O
this	O
can	O
be	O
compensated	O
elsewhere	O
.	O

However	O
,	O
incorrect	O
information	O
and	O
the	O
propagation	O
of	O
errors	O
can	O
effect	O
the	O
entire	O
system	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
report	O
the	O
error	O
types	O
of	O
our	O
STM	Method
and	O
CollaboNet	Method
.	O

We	O
define	O
bio	Metric
-	Metric
entity	Metric
error	Metric
as	O
recognizing	O
different	O
types	O
of	O
biomedical	O
entities	O
as	O
target	O
entity	O
types	O
.	O

For	O
instance	O
,	O
recognizing	O
‘	O
VHL	O
’	O
as	O
a	O
gene	O
when	O
it	O
was	O
used	O
as	O
a	O
disease	O
in	O
a	O
sentence	O
is	O
a	O
bio	O
-	O
entity	O
error	O
.	O

Note	O
that	O
a	O
bio	O
-	O
entity	O
error	O
could	O
occur	O
when	O
an	O
entity	O
is	O
a	O
polysemous	O
word	O
(	O
e.g.	O
VHL	O
)	O
,	O
or	O
comprised	O
of	O
multiple	O
words	O
(	O
e.g.	O
BRCA1	O
deficient	O
)	O
,	O
and	O
thus	O
correcting	O
bio	O
-	O
entity	O
errors	O
requires	O
contextual	O
information	O
or	O
supervision	O
of	O
other	O
entity	Method
type	Method
models	Method
.	O

The	O
error	Task
analysis	Task
was	O
conducted	O
on	O
4334	O
errors	O
of	O
our	O
STM	Method
and	O
3966	O
errors	O
of	O
CollaboNet	Method
on	O
5	O
datasets	O
(	O
BC2GM	O
,	O
BC5CDR	Material
-	Material
chem	Material
,	O
BC5CDR	Material
-	Material
disease	Material
,	O
JNLPBA	Material
,	O
NCBI	O
)	O
.	O

Error	Method
analysis	Method
was	O
conducted	O
on	O
models	O
which	O
showed	O
best	O
performance	O
in	O
our	O
experiments	O
.	O

The	O
error	Method
analysis	Method
of	O
our	O
STM	Method
,	O
which	O
is	O
a	O
single	O
BiLSTM	O
-	O
CRF	Method
model	O
,	O
shows	O
that	O
the	O
majority	O
of	O
errors	O
are	O
classified	O
as	O
bio	O
-	O
entity	O
errors	O
which	O
comprise	O
up	O
to	O
49.3	O
%	O
of	O
the	O
total	O
errors	O
in	O
JNLPBA	Material
.	O

According	O
to	O
the	O
error	Method
analysis	Method
of	O
our	O
STM	Method
model	Method
,	O
bio	O
-	O
entity	O
errors	O
constitute	O
1333	O
errors	O
out	O
of	O
4334	O
errors	O
,	O
comprising	O
30.8	O
%	O
of	O
all	O
the	O
errors	O
.	O

Although	O
bio	O
-	O
entity	O
error	O
was	O
not	O
the	O
most	O
common	O
error	O
type	O
,	O
the	O
importance	O
of	O
bio	O
-	O
entity	O
error	O
is	O
much	O
greater	O
that	O
of	O
other	O
errors	O
such	O
as	O
span	O
error	O
which	O
was	O
the	O
most	O
common	O
error	O
type	O
,	O
constituting	O
38	O
%	O
of	O
incorrect	O
errors	O
.	O

While	O
most	O
span	O
errors	O
tend	O
to	O
come	O
from	O
subjective	O
annotations	O
,	O
or	O
can	O
be	O
easily	O
fixed	O
by	O
non	O
-	O
experts	O
,	O
bio	O
-	O
entity	O
errors	O
are	O
difficult	O
to	O
detect	O
,	O
even	O
for	O
biomedical	Task
researchers	Task
.	O

Also	O
,	O
for	O
biomedical	Task
text	Task
mining	Task
methods	Task
,	O
such	O
as	O
drug	Task
-	Task
drug	Task
interaction	Task
extraction	Task
,	O
span	O
errors	O
can	O
cause	O
minor	O
errors	O
but	O
bio	O
-	O
entity	O
errors	O
could	O
lead	O
to	O
completely	O
different	O
results	O
.	O

In	O
CollaboNet	Method
,	O
each	O
expert	Method
model	Method
is	O
trained	O
on	O
a	O
single	O
entity	O
type	O
dataset	O
,	O
and	O
their	O
training	O
inputs	O
are	O
a	O
concatenation	O
of	O
word	O
embeddings	O
and	O
outputs	O
of	O
the	O
other	O
expert	Method
models	Method
.	O

We	O
expect	O
that	O
the	O
other	O
expert	Method
models	Method
will	O
transfer	O
knowledge	O
on	O
their	O
respective	O
entity	O
to	O
the	O
target	O
model	O
,	O
and	O
thus	O
improve	O
the	O
bio	Task
-	Task
entity	Task
type	Task
error	Task
problem	Task
by	O
collaboration	O
.	O

As	O
Table	O
[	O
reference	O
]	O
shows	O
,	O
CollaboNet	Method
performs	O
better	O
than	O
our	O
STM	Method
in	O
detecting	Task
polysemy	Task
and	Task
other	Task
entity	Task
types	Task
.	O

Among	O
3966	O
errors	O
from	O
CollaboNet	Method
,	O
736	O
errors	O
are	O
bio	O
-	O
entity	O
errors	O
,	O
comprising	O
18.6	O
%	O
of	O
all	O
the	O
errors	O
.	O

subsection	O
:	O
Case	O
study	O
We	O
sampled	O
the	O
predictions	O
of	O
CollaboNet	Method
and	O
those	O
of	O
our	O
STM	Method
(	Method
single	Method
-	Method
task	Method
model	Method
)	O
to	O
further	O
understand	O
the	O
strengths	O
of	O
CollaboNet	Method
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
first	O
example	O
from	O
chemical	O
dataset	O
in	O
Table	O
[	O
reference	O
]	O
shows	O
our	O
expected	O
result	O
from	O
CollaboNet	Method
.	O

Our	O
STM	Method
annotates	O
antilymphocyte	Method
globulin	Method
as	O
a	O
chemical	O
entity	O
.	O

However	O
,	O
it	O
is	O
clear	O
that	O
the	O
entity	O
is	O
not	O
a	O
chemical	O
but	O
a	O
type	O
of	O
globulin	Method
which	O
is	O
a	O
protein	O
.	O

The	O
second	O
example	O
sentence	O
from	O
the	O
chemical	O
dataset	O
is	O
about	O
an	O
ACE	O
/	O
ARB	O
entity	O
.	O

Again	O
,	O
our	O
STM	Method
misidentifies	O
the	O
entity	O
as	O
a	O
chemical	O
entity	O
.	O

On	O
the	O
other	O
hand	O
,	O
in	O
CollaboNet	Method
,	O
the	O
target	O
model	O
(	O
chemical	Method
model	Method
)	O
obtains	O
knowledge	O
from	O
one	O
of	O
the	O
collaborator	Method
models	Method
(	O
the	O
gene	Method
/	Method
protein	Method
model	Method
)	O
to	O
avoid	O
mistakenly	O
recognizing	O
the	O
entity	O
as	O
a	O
chemical	O
entity	O
.	O

As	O
globulin	O
or	O
ACE	O
entities	O
appear	O
in	O
the	O
gene	O
/	O
protein	O
dataset	O
,	O
the	O
chemical	Method
model	Method
obtains	O
information	O
from	O
the	O
gene	Method
/	Method
protein	Method
model	Method
.	O

In	O
the	O
disease	O
dataset	O
,	O
the	O
first	O
example	O
shows	O
a	O
multi	O
-	O
word	O
entity	O
in	O
parentheses	O
.	O

As	O
a	O
gene	Method
model	Method
can	O
pass	O
syntactic	O
and	O
semantic	O
information	O
about	O
a	O
word	O
e.g.	O
,	O
mutated	O
and	O
its	O
surrounding	O
words	O
to	O
a	O
disease	Method
model	Method
,	O
CollaboNet	Method
can	O
abstain	O
from	O
predicting	O
A	O
-	O
T	O
,	O
mutated	O
as	O
the	O
disease	O
entity	O
,	O
which	O
our	O
STM	Method
model	Method
failed	O
to	O
do	O
.	O

The	O
second	O
example	O
in	O
the	O
disease	O
dataset	O
is	O
on	O
cardiac	O
troponin	O
T	O
.	O

Since	O
cardiac	O
+	O
noun	O
in	O
biomedical	O
text	O
can	O
be	O
easily	O
considered	O
as	O
a	O
disease	O
name	O
,	O
our	O
STM	O
misidentified	O
this	O
word	O
as	O
a	O
disease	O
entity	O
.	O

However	O
,	O
with	O
the	O
help	O
of	O
a	O
gene	Method
model	Method
,	O
CollaboNet	Method
did	O
not	O
mark	O
it	O
as	O
a	O
disease	O
entity	O
.	O

The	O
gene	O
/	O
protein	O
entity	O
type	O
further	O
demonstrates	O
the	O
effectiveness	O
of	O
CollaboNet	Method
in	O
reducing	O
bio	Task
-	Task
entity	Task
type	Task
errors	Task
.	O

Two	O
example	O
sentences	O
contain	O
abbreviations	O
,	O
which	O
are	O
one	O
of	O
the	O
distinct	O
characteristics	O
of	O
gene	O
entities	O
.	O

LMB	Method
and	O
cHD	Method
are	O
incorrectly	O
predicted	O
as	O
gene	O
/	O
protein	O
entities	O
by	O
our	O
STM	Method
,	O
since	O
lots	O
of	O
gene	O
/	O
protein	O
entities	O
are	O
abbreviations	O
.	O

However	O
,	O
the	O
target	O
model	O
(	O
gene	Method
/	Method
protein	Method
model	Method
)	O
in	O
CollaboNet	Method
can	O
obtain	O
information	O
on	O
leptomycin	O
and	O
disease	O
from	O
the	O
chemical	Method
and	Method
disease	Method
models	Method
,	O
respectively	O
.	O

With	O
the	O
help	O
of	O
information	O
from	O
collaborator	Method
models	Method
,	O
CollaboNet	Method
can	O
effectively	O
increase	O
the	O
precision	Metric
of	O
other	O
entity	Method
type	Method
models	Method
.	O

In	O
addition	O
,	O
we	O
found	O
some	O
labels	O
in	O
the	O
ground	O
truth	O
set	O
,	O
which	O
we	O
believe	O
are	O
incorrect	O
.	O

Tsai	O
et	O
al	O
.	O

also	O
reported	O
that	O
the	O
inconsistent	O
annotations	O
in	O
the	O
JNLPBA	Material
corpus	Material
limit	O
the	O
NER	Task
system	O
.	O

We	O
report	O
our	O
findings	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
the	O
first	O
row	O
of	O
Table	O
[	O
reference	O
]	O
,	O
the	O
gene	O
/	O
protein	O
entity	O
osteopontin	O
was	O
not	O
marked	O
in	O
the	O
ground	O
truth	O
labels	O
,	O
whereas	O
our	O
network	O
correctly	O
predicted	O
it	O
as	O
a	O
gene	O
entity	O
.	O

The	O
second	O
row	O
also	O
displays	O
questionable	O
results	O
of	O
the	O
ground	O
truth	O
labels	O
.	O

Although	O
lg	O
and	O
bcl	O
-	O
6	O
,	O
which	O
are	O
abbreviations	O
of	O
Immunoglobulin	O
and	O
B	O
-	O
cell	O
lymphoma	O
6	O
,	O
where	O
not	O
labeled	O
in	O
the	O
ground	O
truth	O
labels	O
,	O
our	O
model	O
detected	O
them	O
as	O
a	O
gene	O
/	O
protein	O
entity	O
.	O

The	O
example	O
sentences	O
of	O
gene	O
/	O
protein	O
annotations	O
in	O
Table	O
[	O
reference	O
]	O
were	O
reviewed	O
by	O
several	O
domain	O
experts	O
and	O
medical	O
doctors	O
.	O

As	O
shown	O
in	O
the	O
third	O
row	O
,	O
beta	O
-	O
muricholate	O
is	O
a	O
chemical	O
entity	O
but	O
it	O
was	O
not	O
annotated	O
in	O
the	O
ground	O
truth	O
labels	O
.	O

However	O
,	O
the	O
last	O
row	O
shows	O
another	O
type	O
of	O
annotation	Metric
error	Metric
.	O

Contrast	O
media	O
is	O
a	O
general	O
term	O
for	O
a	O
medium	O
used	O
in	O
medical	Task
imaging	Task
and	O
since	O
is	O
not	O
a	O
proper	O
noun	O
,	O
it	O
is	O
not	O
a	O
named	O
entity	O
.	O

These	O
examples	O
shows	O
the	O
presence	O
of	O
incorrect	O
ground	O
truth	O
labels	O
,	O
which	O
can	O
harm	O
the	O
performance	O
of	O
bioNER	Method
models	Method
.	O

However	O
,	O
we	O
believe	O
that	O
these	O
missed	O
or	O
misidentified	O
ground	O
truth	O
labels	O
can	O
be	O
corrected	O
by	O
our	O
system	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
introduced	O
CollaboNet	Method
,	O
which	O
consists	O
of	O
multiple	O
BiLSTM	O
-	O
CRF	Method
models	O
,	O
for	O
biomedical	O
named	Task
entity	Task
recognition	Task
.	O

While	O
existing	O
models	O
were	O
only	O
able	O
to	O
handle	O
datasets	O
with	O
a	O
single	O
entity	O
type	O
,	O
CollaboNet	Method
leverages	O
multiple	O
datasets	O
and	O
achieves	O
the	O
highest	O
F1	Metric
scores	Metric
.	O

Unlike	O
recently	O
proposed	O
multi	Method
-	Method
task	Method
models	Method
,	O
CollaboNet	Method
is	O
built	O
upon	O
multiple	O
single	O
-	O
task	O
NER	Task
models	O
(	O
STMs	Method
)	O
that	O
send	O
information	O
to	O
each	O
other	O
for	O
more	O
accurate	O
predictions	O
.	O

In	O
addition	O
to	O
the	O
performance	O
improvement	O
over	O
multi	Method
-	Method
task	Method
models	Method
,	O
CollaboNet	Method
differentiates	O
between	O
biomedical	O
entities	O
that	O
are	O
polysemous	O
or	O
have	O
similar	O
orthographic	O
features	O
.	O

As	O
a	O
result	O
,	O
our	O
model	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
four	O
bioNER	O
datasets	O
in	O
terms	O
of	O
F1	Metric
score	Metric
,	O
precision	Metric
and	O
recall	Metric
.	O

Although	O
our	O
model	O
requires	O
a	O
large	O
amount	O
of	O
memory	O
and	O
time	O
,	O
which	O
existing	O
multi	Method
-	Method
task	Method
models	Method
require	O
as	O
well	O
,	O
the	O
simple	O
structure	O
of	O
CollaboNet	Method
allows	O
researchers	O
to	O
build	O
another	O
expert	Method
model	Method
for	O
different	O
entity	O
types	O
in	O
CollaboNet	Method
.	O

As	O
CollaboNet	Method
obtains	O
higher	O
precision	Metric
than	O
other	O
models	O
,	O
we	O
plan	O
to	O
apply	O
CollaboNet	Method
in	O
a	O
biomedical	Task
text	Task
mining	Task
system	Task
.	O

section	O
:	O
Availability	O
of	O
data	O
and	O
materials	O
The	O
source	O
code	O
of	O
CollaboNet	Method
and	O
the	O
datasets	O
are	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
wonjininfo	O
/	O
CollaboNet	Method
.	O

section	O
:	O
Abbreviations	O
BiLSTM	Method
:	O
Bidirectional	Task
long	Task
short	Task
-	Task
term	Task
memory	Task
;	O
BioNER	O
:	O
Biomedical	O
named	Task
entity	Task
recognition	Task
;	O
CE	Method
:	O
Character	Method
embedding	Method
;	O
CLWE	Method
:	O
Character	Method
level	Method
word	Method
embedding	Method
;	O
CNN	Method
:	Method
convolution	Method
neural	Method
network	Method
;	O
CRF	Method
:	O
Conditional	Method
random	Method
field	Method
;	O
LSTM	Method
:	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
;	O
MTL	Method
:	O
Multi	Method
-	Method
task	Method
learning	Method
;	O
MTM	Method
:	O
Multi	Method
-	Method
task	Method
model	Method
;	O
NER	Task
:	O
Named	Task
entity	Task
recognition	Task
;	O
NLP	Task
:	Task
Natural	Task
language	Task
processing	Task
;	O
PMC	O
:	O
PubMed	O
Central	O
;	O
STM	O
:	O
Single	Method
-	Method
task	Method
model	Method
;	O
RNN	Method
:	O
Recurrent	Method
neural	Method
network	Method
;	O
WE	O
:	O
Word	Method
embedding	Method
section	O
:	O
Funding	O
This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
of	O
Korea	O
(	O
NRF	O
-	O
2016M3A9A7916996	O
,	O
NRF	O
-	O
2017M3C4A7065887	O
)	O
and	O
National	O
IT	O
Industry	O
Promotion	O
Agency	O
grant	O
funded	O
by	O
the	O
Ministry	O
of	O
Science	O
and	O
ICT	O
and	O
Ministry	O
of	O
Health	O
and	O
Welfare	O
(	O
NO	O
.	O

C1202	O
-	O
18	O
-	O
1001	O
,	O
Development	O
Project	O
of	O
The	O
Precision	Task
Medicine	Task
Hospital	Task
Information	Task
System	Task
(	O
P	O
-	O
HIS	O
)	O
)	O
section	O
:	O
Competing	O
interests	O
The	O
authors	O
declare	O
that	O
they	O
have	O
no	O
competing	O
interests	O
.	O

section	O
:	O
Author	O
’s	O
contributions	O
WY	O
,	O
CHS	O
,	O
JL	O
and	O
JK	O
conceived	O
the	O
idea	O
.	O

WY	O
and	O
JL	O
designed	O
the	O
model	O
.	O

WY	O
and	O
CHS	Method
developed	O
CollaboNet	Method
.	O

CHS	Method
experimented	O
and	O
collected	O
analysis	O
examples	O
and	O
results	O
.	O

WY	O
,	O
JL	O
and	O
JK	O
wrote	O
the	O
manuscript	O
.	O

JK	Method
,	O
as	O
the	O
supervisor	O
of	O
WY	O
,	O
CHS	Method
and	O
JL	O
,	O
provided	O
guidance	O
on	O
the	O
experiment	O
.	O

All	O
authors	O
read	O
and	O
approved	O
the	O
final	O
manuscript	O
.	O

section	O
:	O
Acknowledgements	O
We	O
are	O
sincerely	O
grateful	O
to	O
Inah	O
Chang	O
for	O
conducting	O
manual	Task
error	Task
counting	Task
.	O

We	O
appreciate	O
Susan	O
Kim	O
for	O
editing	O
the	O
manuscript	O
.	O

bibliography	O
:	O
References	O
Scores	O
in	O
the	O
asterisked	O
(	O
*	O
)	O
cells	O
are	O
obtained	O
in	O
the	O
experiments	O
that	O
we	O
conducted	O
;	O
these	O
scores	O
are	O
not	O
reported	O
in	O
the	O
original	O
papers	O
.	O

The	O
best	O
scores	O
from	O
these	O
experiments	O
are	O
in	O
bold	O
.	O

Scores	O
in	O
the	O
asterisked	O
(	O
*	O
)	O
cells	O
are	O
obtained	O
in	O
the	O
experiments	O
that	O
we	O
conducted	O
;	O
these	O
scores	O
are	O
not	O
reported	O
in	O
the	O
original	O
papers	O
.	O

The	O
best	O
scores	O
from	O
these	O
experiments	O
are	O
in	O
bold	O
.	O

