V2V	Method
-	Method
PoseNet	Method
:	O
Voxel	Method
-	Method
to	Method
-	Method
Voxel	Method
Prediction	Method
Network	Method
for	O
Accurate	O
3D	Task
Hand	Task
and	O
Human	Task
Pose	Task
Estimation	Task
from	O
a	O
Single	O
Depth	O
Map	O
section	O
:	O
Abstract	O
Most	O
of	O
the	O
existing	O
deep	Method
learning	Method
-	Method
based	Method
methods	Method
for	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
are	O
based	O
on	O
a	O
common	O
framework	O
that	O
takes	O
a	O
2D	O
depth	O
map	O
and	O
directly	O
regresses	O
the	O
3D	O
coordinates	O
of	O
keypoints	O
,	O
such	O
as	O
hand	O
or	O
human	O
body	O
joints	O
,	O
via	O
2D	Method
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
.	O

The	O
first	O
weakness	O
of	O
this	O
approach	O
is	O
the	O
presence	O
of	O
perspective	O
distortion	O
in	O
the	O
2D	Task
depth	Task
map	Task
.	O

While	O
the	O
depth	O
map	O
is	O
intrinsically	O
3D	O
data	O
,	O
many	O
previous	O
methods	O
treat	O
depth	O
maps	O
as	O
2D	O
images	O
that	O
can	O
distort	O
the	O
shape	O
of	O
the	O
actual	O
object	O
through	O
projection	O
from	O
3D	O
to	O
2D	O
space	O
.	O

This	O
compels	O
the	O
network	O
to	O
perform	O
perspective	Task
distortion	Task
-	Task
invariant	Task
estimation	Task
.	O

The	O
second	O
weakness	O
of	O
the	O
conventional	O
approach	O
is	O
that	O
directly	O
regressing	O
3D	O
coordinates	O
from	O
a	O
2D	O
image	O
is	O
a	O
highly	O
nonlinear	Task
mapping	Task
,	O
which	O
causes	O
difficulty	O
in	O
the	O
learning	Task
procedure	Task
.	O

To	O
overcome	O
these	O
weaknesses	O
,	O
we	O
firstly	O
cast	O
the	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
problem	O
from	O
a	O
single	O
depth	O
map	O
into	O
a	O
voxel	Method
-	Method
to	Method
-	Method
voxel	Method
prediction	Method
that	O
uses	O
a	O
3D	O
voxelized	O
grid	O
and	O
estimates	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
.	O

We	O
design	O
our	O
model	O
as	O
a	O
3D	Method
CNN	Method
that	O
provides	O
accurate	O
estimates	O
while	O
running	O
in	O
real	O
-	O
time	O
.	O

Our	O
system	O
outperforms	O
previous	O
methods	O
in	O
almost	O
all	O
publicly	O
available	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
datasets	O
and	O
placed	O
first	O
in	O
the	O
HANDS	O
2017	O
frame	O
-	O
based	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
.	O

The	O
code	O
is	O
available	O
in	O
1	O
.	O

section	O
:	O
Introduction	O
Accurate	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
is	O
an	O
important	O
requirement	O
for	O
activity	Task
recognition	Task
with	O
diverse	O
applications	O
,	O
such	O
as	O
human	Task
-	Task
computer	Task
interaction	Task
or	O
augmented	Task
reality	Task
[	O
reference	O
]	O
.	O

It	O
has	O
been	O
studied	O
for	O
decades	O
in	O
computer	Task
vision	Task
community	Task
and	O
has	O
attracted	O
considerable	O
research	O
interest	O
again	O
due	O
to	O
the	O
introduction	O
of	O
low	Task
-	Task
cost	Task
depth	Task
cameras	Task
.	O

section	O
:	O
ΔZ	O
Figure	O
1	O
:	O
Visualization	Task
of	Task
perspective	Task
distortion	Task
in	O
2D	O
depth	O
image	O
.	O

The	O
3D	O
point	O
cloud	O
has	O
one	O
-	O
to	O
-	O
one	O
relation	O
with	O
a	O
3D	O
pose	O
,	O
but	O
the	O
2D	O
depth	O
image	O
has	O
many	O
-	O
to	O
-	O
one	O
relation	O
because	O
of	O
perspective	O
distortion	O
.	O

Thus	O
,	O
the	O
network	O
is	O
compelled	O
to	O
perform	O
perspective	Task
distortion	Task
-	Task
invariant	Task
estimation	Task
.	O

The	O
2D	Task
depth	Task
maps	Task
are	O
generated	O
by	O
translating	O
the	O
3D	O
point	O
cloud	O
by	O
∆X	O
=	O
-	O
300	O
,	O
0	O
,	O
300	O
mm	O
(	O
from	O
left	O
to	O
right	O
)	O
and	O
∆Y	O
=	O
-	O
300	O
,	O
0	O
,	O
300	O
mm	O
(	O
from	O
bottom	O
to	O
top	O
)	O
.	O

In	O
all	O
cases	O
,	O
∆Z	O
is	O
set	O
to	O
0	O
mm	O
.	O

Similar	O
values	O
to	O
the	O
real	O
human	O
hand	O
size	O
and	O
camera	O
projection	O
parameters	O
in	O
the	O
MSRA	Material
dataset	Material
were	O
used	O
for	O
our	O
visualization	Task
.	O

Recently	O
,	O
powerful	O
discriminative	Method
approaches	Method
based	O
on	O
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
are	O
outperforming	O
existing	O
methods	O
in	O
various	O
computer	Task
vision	Task
tasks	Task
including	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Although	O
these	O
approaches	O
achieved	O
significant	O
advancement	O
in	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
,	O
they	O
still	O
suffer	O
from	O
inaccurate	Task
estimation	Task
because	O
of	O
severe	O
self	O
-	O
occlusions	O
,	O
highly	O
articulated	O
shapes	O
of	O
target	O
objects	O
,	O
and	O
low	O
quality	O
of	O
depth	O
images	O
.	O

Analyzing	O
previ	Method
-	Method
ous	Method
deep	Method
learning	Method
-	Method
based	Method
methods	Method
for	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
image	O
,	O
most	O
of	O
these	O
methods	O
[	O
reference	O
]	O
are	O
based	O
on	O
a	O
common	O
framework	O
that	O
takes	O
a	O
2D	O
depth	O
image	O
and	O
directly	O
regresses	O
the	O
3D	O
coordinates	O
of	O
keypoints	O
,	O
such	O
as	O
hand	O
or	O
human	O
body	O
joints	O
.	O

However	O
,	O
we	O
argue	O
that	O
this	O
approach	O
has	O
two	O
serious	O
drawbacks	O
.	O

The	O
first	O
one	O
is	O
perspective	O
distortion	O
in	O
2D	Task
depth	Task
image	Task
.	O

As	O
the	O
pixel	O
values	O
of	O
a	O
2D	O
depth	O
map	O
represent	O
the	O
physical	O
distances	O
of	O
object	O
points	O
from	O
the	O
depth	O
camera	O
,	O
the	O
depth	O
map	O
is	O
intrinsically	O
3D	O
data	O
.	O

However	O
,	O
most	O
previous	O
methods	O
simply	O
take	O
depth	O
maps	O
as	O
a	O
2D	O
image	O
form	O
,	O
which	O
can	O
distort	O
the	O
shape	O
of	O
an	O
actual	O
object	O
in	O
the	O
3D	O
space	O
by	O
projecting	O
it	O
to	O
the	O
2D	O
image	O
space	O
.	O

Hence	O
,	O
the	O
network	O
see	O
a	O
distorted	O
object	O
and	O
is	O
burdened	O
to	O
perform	O
distortion	Task
-	Task
invariant	Task
estimation	Task
.	O

We	O
visualize	O
the	O
perspective	O
distortions	O
of	O
the	O
2D	O
depth	O
image	O
in	O
Figure	O
1	O
.	O

The	O
second	O
weakness	O
is	O
the	O
highly	O
non	O
-	O
linear	O
mapping	O
between	O
the	O
depth	O
map	O
and	O
3D	O
coordinates	O
.	O

This	O
highly	O
non	Method
-	Method
linear	Method
mapping	Method
hampers	O
the	O
learning	Method
procedure	Method
and	O
prevents	O
the	O
network	O
from	O
precisely	O
estimating	O
the	O
coordinates	O
of	O
keypoints	O
as	O
argued	O
by	O
Tompson	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

This	O
high	O
nonlinearity	O
is	O
attributed	O
to	O
the	O
fact	O
that	O
only	O
one	O
3D	O
coordinate	O
for	O
each	O
keypoint	O
has	O
to	O
be	O
regressed	O
from	O
the	O
input	O
.	O

To	O
cope	O
with	O
these	O
limitations	O
,	O
we	O
propose	O
the	O
voxelto	Method
-	Method
voxel	Method
prediction	Method
network	Method
for	O
pose	Task
estimation	Task
(	O
V2V	Method
-	Method
PoseNet	Method
)	O
.	O

In	O
contrast	O
to	O
most	O
of	O
the	O
previous	O
methods	O
,	O
the	O
V2V	Method
-	Method
PoseNet	Method
takes	O
a	O
voxelized	O
grid	O
as	O
input	O
and	O
estimates	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

By	O
converting	O
the	O
2D	O
depth	O
image	O
into	O
a	O
3D	O
voxelized	O
form	O
as	O
input	O
,	O
our	O
network	O
can	O
sees	O
the	O
actual	O
appearance	O
of	O
objects	O
without	O
perspective	O
distortion	O
.	O

Also	O
,	O
estimating	O
the	O
per	O
-	O
voxel	O
likelihood	O
of	O
each	O
keypoint	O
enables	O
the	O
network	O
to	O
learn	O
the	O
desired	O
task	O
more	O
easily	O
than	O
the	O
highly	O
nonlinear	Method
mapping	Method
that	O
estimates	O
3D	O
coordinates	O
directly	O
from	O
the	O
input	O
.	O

We	O
perform	O
a	O
thorough	O
experiment	O
to	O
demonstrate	O
the	O
usefulness	O
of	O
the	O
proposed	O
volumetric	Method
representation	Method
of	O
input	O
and	O
output	O
in	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
.	O

The	O
performance	O
of	O
the	O
four	O
combinations	O
of	O
input	O
(	O
i.e.	O
,	O
2D	O
depth	O
map	O
and	O
voxelized	O
grid	O
)	O
and	O
output	O
(	O
i.e.	O
,	O
3D	O
coordinates	O
and	O
per	O
-	O
voxel	O
likelihood	O
)	O
types	O
are	O
compared	O
.	O

The	O
experimental	O
results	O
show	O
that	O
the	O
proposed	O
voxelto	Method
-	Method
voxel	Method
prediction	Method
allows	O
our	O
method	O
to	O
achieve	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
in	O
almost	O
all	O
of	O
the	O
publicly	O
available	O
datasets	O
(	O
i.e.	O
,	O
three	O
3D	Task
hand	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
and	O
one	O
3D	O
human	O
[	O
reference	O
]	O
pose	O
estimation	O
datasets	O
)	O
while	O
it	O
runs	O
in	O
realtime	O
.	O

We	O
also	O
placed	O
first	O
in	O
the	O
HANDS	O
2017	O
frame	O
-	O
based	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
[	O
reference	O
]	O
.	O

We	O
hope	O
that	O
the	O
proposed	O
system	O
to	O
become	O
a	O
milestone	O
of	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
problems	O
from	O
a	O
single	O
depth	O
map	O
.	O

Now	O
,	O
we	O
assume	O
that	O
the	O
term	O
"	O
3D	Task
pose	Task
estimation	Task
"	O
refers	O
to	O
the	O
localization	Task
of	Task
the	Task
hand	Task
or	Task
human	Task
body	Task
keypoints	Task
in	O
3D	O
space	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
.	O

•	O
We	O
firstly	O
cast	O
the	O
problem	O
of	O
estimating	Task
3D	Task
pose	Task
from	O
a	O
single	O
depth	O
map	O
into	O
a	O
voxel	Task
-	Task
to	Task
-	Task
voxel	Task
prediction	Task
.	O

Unlike	O
most	O
of	O
the	O
previous	O
methods	O
that	O
regress	O
3D	O
coordinates	O
directly	O
from	O
the	O
2D	O
depth	O
image	O
,	O
our	O
proposed	O
V2V	Method
-	Method
PoseNet	Method
estimates	O
the	O
per	O
-	O
voxel	O
likelihood	O
from	O
a	O
voxelized	O
grid	O
input	O
.	O

•	O
We	O
empirically	O
validate	O
the	O
usefulness	O
of	O
the	O
volumetric	Method
input	Method
and	Method
output	Method
representations	Method
by	O
comparing	O
the	O
performance	O
of	O
each	O
input	O
type	O
(	O
i.e.	O
,	O
2D	O
depth	O
map	O
and	O
voxelized	O
grid	O
)	O
and	O
output	O
type	O
(	O
i.e.	O
,	O
3D	O
coordinates	O
and	O
per	O
-	O
voxel	O
likelihood	O
)	O
.	O

•	O
We	O
conduct	O
extensive	O
experiments	O
using	O
almost	O
all	O
of	O
the	O
existing	O
3D	O
pose	O
estimation	O
datasets	O
including	O
three	O
3D	Task
hand	Task
and	O
one	O
3D	O
human	Task
pose	Task
estimation	Task
datasets	O
.	O

We	O
show	O
that	O
the	O
proposed	O
method	O
produces	O
significantly	O
more	O
accurate	O
results	O
than	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
methods	O
.	O

The	O
proposed	O
method	O
also	O
placed	O
first	O
in	O
the	O
HANDS	O
2017	O
frame	O
-	O
based	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
.	O

section	O
:	O
Related	O
works	O
Depth	O
-	O
based	O
3D	Task
hand	Task
pose	O
estimation	O
.	O

Hand	Method
pose	Method
estimation	Method
methods	Method
can	O
be	O
categorized	O
into	O
generative	Method
,	Method
discriminative	Method
,	Method
and	Method
hybrid	Method
methods	Method
.	O

Generative	Method
methods	Method
assume	O
a	O
pre	O
-	O
defined	O
hand	Method
model	Method
and	O
fit	O
it	O
to	O
the	O
input	O
depth	O
image	O
by	O
minimizing	O
hand	O
-	O
crafted	O
cost	O
functions	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Particle	Method
swam	Method
optimization	Method
(	O
PSO	Method
)	O
[	O
reference	O
]	O
,	O
iterative	Method
closest	Method
point	Method
(	O
ICP	Method
)	O
[	O
reference	O
]	O
,	O
and	O
their	O
combination	O
[	O
reference	O
]	O
are	O
the	O
common	O
algorithms	O
used	O
to	O
obtain	O
optimal	O
hand	O
pose	O
results	O
.	O

Discriminative	Method
methods	Method
directly	O
localize	O
hand	O
joints	O
from	O
an	O
input	O
depth	O
map	O
.	O

Random	Method
forest	Method
-	Method
based	Method
methods	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
provide	O
fast	O
and	O
accurate	O
performance	O
.	O

However	O
,	O
they	O
utilize	O
hand	O
-	O
crafted	O
features	O
and	O
are	O
overcome	O
by	O
recent	O
CNN	Method
-	Method
based	Method
approaches	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
that	O
can	O
learn	O
useful	O
features	O
by	O
themselves	O
.	O

Tompson	O
et	O
al	O
.	O

[	O
reference	O
]	O
firstly	O
utilized	O
CNN	Method
to	O
localize	O
hand	O
keypoints	O
by	O
estimating	O
2D	O
heatmaps	O
for	O
each	O
hand	O
joint	O
.	O

Ge	O
et	O
al	O
.	O

[	O
reference	O
]	O
extended	O
this	O
method	O
by	O
exploiting	O
multi	Method
-	Method
view	Method
CNN	Method
to	O
estimate	O
2D	O
heatmaps	O
for	O
each	O
view	O
.	O

Ge	O
et	O
al	O
.	O

[	O
reference	O
]	O
transformed	O
the	O
2D	O
input	O
depth	O
map	O
to	O
the	O
3D	O
form	O
and	O
estimated	O
3D	O
coordinates	O
directly	O
via	O
3D	Method
CNN	Method
.	O

Guo	O
et	O
al	O
.	O

[	O
reference	O
][	O
reference	O
]	O
proposed	O
a	O
region	Method
ensemble	Method
network	Method
to	O
accurately	O
estimate	O
the	O
3D	O
coordinates	O
of	O
hand	O
keypoints	O
and	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
improved	O
this	O
network	O
by	O
iteratively	O
refining	O
the	O
estimated	O
pose	O
.	O

Oberweger	O
et	O
al	O
.	O

[	O
reference	O
]	O
improved	O
their	O
preceding	O
work	O
[	O
reference	O
]	O
by	O
utilizing	O
recent	O
network	Method
architecture	Method
,	O
data	Method
augmentation	Method
,	O
and	O
better	O
initial	Task
hand	Task
localization	Task
.	O

Hybrid	Method
methods	Method
are	O
proposed	O
to	O
combine	O
the	O
generative	Method
and	Method
discriminative	Method
approach	Method
.	O

Oberweger	O
et	O
al	O
.	O

[	O
reference	O
]	O
trained	O
discriminative	Method
and	Method
generative	Method
CNNs	Method
by	O
a	O
feedback	Method
loop	Method
.	O

Zhou	O
et	O
al	O
.	O

[	O
reference	O
]	O
pre	O
-	O
defined	O
a	O
hand	Method
model	Method
and	O
estimated	O
the	O
parameter	O
of	O
the	O
model	O
instead	O
of	O
regressing	O
3D	O
coordinates	O
directly	O
.	O

Ye	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
spatial	Method
attention	Method
mechanism	Method
and	O
hierarchical	O
PSO	Method
.	O

Wan	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
two	O
deep	Method
generative	Method
models	Method
with	O
a	O
shared	O
latent	O
space	O
and	O
trained	O
discriminator	Method
to	O
estimate	O
the	O
posterior	O
of	O
the	O
latent	O
pose	O
.	O

Depth	O
-	O
based	O
3D	O
human	Task
pose	Task
estimation	Task
.	O

Depth	O
-	O
based	O
3D	O
human	Task
pose	Task
estimation	Task
methods	O
also	O
rely	O
on	O
generative	Method
and	Method
discriminative	Method
models	Method
.	O

The	O
generative	Method
models	Method
estimate	O
the	O
pose	O
by	O
finding	O
the	O
correspondences	O
between	O
the	O
pre	O
-	O
defined	O
body	Method
model	Method
and	O
the	O
input	O
3D	O
point	O
cloud	O
.	O

The	O
ICP	Method
algorithm	O
is	O
commonly	O
used	O
for	O
3D	Task
body	Task
tracking	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

Another	O
method	O
such	O
as	O
template	Method
fitting	Method
with	O
Gaussian	Method
mixture	Method
models	Method
[	O
reference	O
]	O
was	O
also	O
proposed	O
.	O

By	O
contrast	O
,	O
the	O
discriminative	Method
models	Method
do	O
not	O
require	O
body	O
templates	O
and	O
they	O
directly	O
estimate	O
the	O
positions	O
of	O
body	O
joints	O
.	O

Conventional	O
discriminative	Method
methods	Method
are	O
mostly	O
based	O
on	O
random	Method
forests	Method
.	O

Shotton	O
et	O
al	O
.	O

[	O
reference	O
]	O
classified	O
each	O
pixel	O
into	O
one	O
of	O
the	O
body	O
parts	O
,	O
while	O
Girchick	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Jung	O
et	O
al	O
.	O

[	O
reference	O
]	O
directly	O
regressed	O
the	O
coordinates	O
of	O
body	O
joints	O
.	O

Jung	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
a	O
random	Method
tree	Method
walk	Method
algorithm	Method
(	O
RTW	Method
)	Method
,	O
which	O
reduced	O
the	O
running	Metric
time	Metric
significantly	O
.	O

Recently	O
,	O
Haque	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
the	O
viewpointinvariant	Method
pose	Method
estimation	Method
method	Method
using	O
CNN	Method
and	O
multiple	O
rounds	O
of	O
a	O
recurrent	Method
neural	Method
network	Method
.	O

Their	O
model	O
learns	O
viewpoint	O
-	O
invariant	O
features	O
,	O
which	O
makes	O
the	O
model	O
robust	O
to	O
viewpoint	O
variations	O
.	O

Volumetric	Method
representation	Method
using	O
depth	O
information	O
.	O

Wu	O
et	O
al	O
.	O

[	O
reference	O
]	O
introduced	O
the	O
volumetric	Method
representation	Method
of	Method
a	Method
depth	Method
image	Method
and	O
surpassed	O
the	O
existing	O
hand	Method
-	Method
crafted	Method
descriptor	Method
-	Method
based	Method
methods	Method
in	O
3D	Task
shape	Task
classification	Task
and	Task
retrieval	Task
.	O

They	O
represented	O
each	O
voxel	O
as	O
a	O
binary	O
random	O
variable	O
and	O
used	O
a	O
convolutional	Method
deep	Method
belief	Method
network	Method
to	O
learn	O
the	O
probability	O
distribution	O
for	O
each	O
voxel	O
.	O

Several	O
recent	O
works	O
[	O
reference	O
][	O
reference	O
]	O
also	O
represented	O
3D	O
input	O
data	O
as	O
a	O
volumetric	O
form	O
for	O
3D	Task
object	Task
classification	Task
and	Task
detection	Task
.	O

Our	O
work	O
follows	O
the	O
strategy	O
from	O
[	O
reference	O
]	O
,	O
wherein	O
several	O
types	O
of	O
volumetric	Method
representation	Method
(	O
i.e.	O
,	O
occupancy	Method
grid	Method
models	Method
)	O
were	O
proposed	O
to	O
fully	O
utilize	O
the	O
rich	O
source	O
of	O
3D	O
information	O
and	O
efficiently	O
deal	O
with	O
large	O
amounts	O
of	O
point	O
cloud	O
data	O
.	O

Their	O
proposed	O
CNN	Method
architecture	Method
and	O
occupancy	Method
grids	Method
outperform	O
those	O
of	O
Wu	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

Input	Task
and	Task
output	Task
representation	Task
in	O
3D	Task
pose	Task
estimation	Task
.	O

Most	O
of	O
the	O
existing	O
methods	O
for	O
3D	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
[	O
reference	O
]	O
are	O
based	O
on	O
the	O
model	O
in	O
Figure	O
2	O
(	O
a	O
)	O
that	O
takes	O
a	O
2D	O
depth	O
image	O
and	O
directly	O
regresses	O
3D	O
coordinates	O
.	O

Recently	O
,	O
Ge	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Deng	O
et	O
al	O
.	O

[	O
reference	O
]	O
To	O
estimate	O
the	O
per	Task
-	Task
voxel	Task
likelihood	Task
from	O
an	O
RGB	Material
image	Material
,	O
they	O
treated	O
the	O
discretized	O
depth	O
value	O
as	O
a	O
channel	O
of	O
the	O
feature	O
map	O
,	O
which	O
resulted	O
in	O
different	O
kernels	O
for	O
each	O
depth	O
value	O
.	O

In	O
contrast	O
to	O
all	O
the	O
above	O
approaches	O
,	O
our	O
proposed	O
system	O
estimates	O
the	O
per	O
-	O
voxel	O
likelihood	O
of	O
each	O
keypoint	O
via	O
the	O
3D	Method
fully	Method
convolutional	Method
network	Method
from	O
the	O
voxelized	O
input	O
as	O
in	O
Figure	O
2	O
(	O
d	O
)	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
network	O
is	O
the	O
first	O
model	O
to	O
generate	O
voxelized	O
output	O
from	O
To	O
simplify	O
the	O
figure	O
,	O
we	O
plotted	O
each	O
feature	O
map	O
without	O
Z	O
-	O
axis	O
and	O
combined	O
the	O
3D	O
heatmaps	O
of	O
all	O
keypoints	O
in	O
a	O
single	O
volume	O
.	O

Each	O
color	O
in	O
the	O
3D	O
heatmap	O
indicates	O
keypoints	O
in	O
the	O
same	O
finger	O
.	O

Offset	O
to	O
the	O
correct	O
ref	O
point	O
Figure	O
4	O
:	O
Reference	Method
point	Method
refining	Method
network	Method
.	O

This	O
network	O
takes	O
cropped	O
depth	O
image	O
and	O
outputs	O
the	O
3D	O
offset	O
from	O
the	O
current	O
reference	O
point	O
to	O
the	O
center	O
of	O
ground	O
-	O
truth	O
joint	O
locations	O
.	O

voxelized	O
input	O
using	O
3D	Method
CNN	Method
for	O
3D	Task
pose	Task
estimation	Task
.	O

section	O
:	O
Overview	O
of	O
the	O
proposed	O
model	O
The	O
goal	O
of	O
our	O
model	O
is	O
to	O
estimate	O
the	O
3D	O
coordinates	O
of	O
all	O
keypoints	O
.	O

First	O
,	O
we	O
convert	O
2D	O
depth	O
images	O
to	O
3D	O
volumetric	O
forms	O
by	O
reprojecting	O
the	O
points	O
in	O
the	O
3D	O
space	O
and	O
discretizing	O
the	O
continuous	O
space	O
.	O

After	O
voxelizing	O
the	O
2D	O
depth	O
image	O
,	O
the	O
V2V	Method
-	Method
PoseNet	Method
takes	O
the	O
3D	O
voxelized	O
data	O
as	O
an	O
input	O
and	O
estimates	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
.	O

The	O
position	O
of	O
the	O
highest	O
likelihood	O
response	O
for	O
each	O
keypoint	O
is	O
identified	O
and	O
warped	O
to	O
the	O
real	O
world	O
coordinate	O
,	O
which	O
becomes	O
the	O
final	O
result	O
of	O
our	O
model	O
.	O

Figure	O
3	O
shows	O
the	O
overall	O
architecture	O
of	O
the	O
proposed	O
V2V	Method
-	Method
PoseNet	Method
.	O

We	O
now	O
describe	O
the	O
target	Method
object	Method
localization	Method
refinement	Method
strategy	Method
,	O
the	O
process	O
of	O
generating	O
the	O
input	O
of	O
the	O
proposed	O
model	O
,	O
V2V	Method
-	Method
PoseNet	Method
,	O
and	O
some	O
related	O
issues	O
of	O
the	O
proposed	O
approach	O
in	O
the	O
following	O
sections	O
.	O

section	O
:	O
Refining	Task
target	Task
object	Task
localization	Task
To	O
localize	Task
keypoints	Task
,	O
such	O
as	O
hand	O
or	O
human	O
body	O
joints	O
,	O
a	O
cubic	O
box	O
that	O
contains	O
the	O
hand	O
or	O
human	O
body	O
in	O
3D	O
space	O
is	O
a	O
prerequisite	O
.	O

This	O
cubic	O
box	O
is	O
usually	O
placed	O
around	O
the	O
reference	O
point	O
,	O
which	O
is	O
obtained	O
using	O
groundtruth	O
joint	O
position	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
or	O
the	O
center	O
-	O
of	O
-	O
mass	O
after	O
simple	O
depth	Method
thresholding	Method
around	O
the	O
hand	O
region	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

However	O
,	O
utilizing	O
the	O
ground	O
-	O
truth	O
joint	O
position	O
is	O
infeasible	O
in	O
real	Task
-	Task
world	Task
applications	Task
.	O

Also	O
,	O
in	O
general	O
,	O
using	O
the	O
center	O
-	O
of	O
-	O
mass	O
calculated	O
by	O
simple	O
depth	Method
thresholding	Method
does	O
not	O
guarantee	O
that	O
the	O
object	O
is	O
correctly	O
contained	O
in	O
the	O
acquired	O
cubic	O
box	O
due	O
to	O
the	O
error	O
in	O
the	O
center	Method
-	Method
of	Method
-	Method
mass	Method
calculations	Method
in	O
cluttered	O
scenes	O
.	O

For	O
example	O
,	O
if	O
other	O
objects	O
are	O
near	O
the	O
target	O
object	O
,	O
then	O
the	O
simple	O
depth	Method
thresholding	Method
method	Method
can	O
not	O
properly	O
filter	O
the	O
other	O
objects	O
because	O
it	O
applies	O
the	O
same	O
threshold	O
value	O
to	O
all	O
input	O
data	O
.	O

Hence	O
,	O
the	O
computed	O
center	O
-	O
of	O
-	O
mass	O
becomes	O
erroneous	O
,	O
which	O
results	O
in	O
a	O
cubic	O
box	O
that	O
contains	O
only	O
some	O
part	O
of	O
the	O
target	O
object	O
.	O

To	O
overcome	O
these	O
limitations	O
,	O
we	O
train	O
a	O
simple	O
2D	Method
CNN	Method
following	O
Oberweger	O
et	O
al	O
.	O

[	O
reference	O
]	O
to	O
obtain	O
an	O
accurate	O
reference	O
point	O
as	O
shown	O
in	O
Figure	O
4	O
.	O

This	O
network	O
takes	O
a	O
depth	O
image	O
,	O
whose	O
reference	O
point	O
is	O
calculated	O
by	O
the	O
simple	O
depth	Method
thresholding	Method
around	O
the	O
hand	O
region	O
,	O
and	O
outputs	O
3D	O
offset	O
from	O
the	O
calculated	O
reference	O
point	O
to	O
the	O
center	O
of	O
ground	O
-	O
truth	O
joint	O
locations	O
.	O

The	O
refined	O
reference	O
point	O
can	O
be	O
obtained	O
by	O
adding	O
the	O
output	O
offset	O
value	O
of	O
the	O
network	O
to	O
the	O
calculated	O
reference	O
point	O
.	O

section	O
:	O
Generating	O
input	O
of	O
the	O
proposed	O
system	O
To	O
create	O
the	O
input	O
of	O
the	O
proposed	O
system	O
,	O
the	O
2D	Task
depth	Task
map	Task
should	O
be	O
converted	O
to	O
voxelized	O
form	O
.	O

To	O
voxelize	O
the	O
2D	Task
depth	Task
map	Task
,	O
we	O
first	O
reproject	O
each	O
pixel	O
of	O
the	O
depth	O
map	O
to	O
the	O
3D	O
space	O
.	O

After	O
reprojecting	O
all	O
depth	O
pixels	O
,	O
the	O
3D	O
space	O
is	O
discretized	O
based	O
on	O
the	O
pre	O
-	O
defined	O
voxel	O
size	O
.	O

Then	O
,	O
the	O
target	O
object	O
is	O
extracted	O
by	O
drawing	O
the	O
cubic	O
box	O
around	O
the	O
reference	O
point	O
obtained	O
in	O
Section	O
4	O
.	O

We	O
set	O
the	O
voxel	O
value	O
of	O
the	O
network	O
's	O
input	O
V	O
(	O
i	O
,	O
j	O
,	O
k	O
)	O
as	O
1	O
if	O
the	O
voxel	O
is	O
occupied	O
by	O
any	O
depth	O
point	O
and	O
0	O
otherwise	O
.	O

6	O
.	O

V2V	Method
-	Method
PoseNet	Method
section	O
:	O
Building	Method
block	Method
design	Method
We	O
use	O
four	O
kinds	O
of	O
building	O
blocks	O
in	O
designing	O
the	O
V2V	Method
-	Method
PoseNet	Method
.	O

The	O
first	O
one	O
is	O
the	O
volumetric	Method
basic	Method
block	Method
that	O
consists	O
of	O
a	O
volumetric	Method
convolution	Method
,	O
volumetric	Method
batch	Method
normalization	Method
[	O
reference	O
]	O
,	O
and	O
the	O
activation	Method
function	Method
(	O
i.e.	O
,	O
ReLU	Method
)	O
.	O

This	O
block	O
is	O
located	O
in	O
the	O
first	O
and	O
last	O
parts	O
of	O
the	O
network	O
.	O

The	O
second	O
one	O
is	O
the	O
volumetric	Method
residual	Method
block	Method
extended	O
from	O
the	O
2D	O
residual	O
block	O
of	O
option	O
B	O
in	O
[	O
reference	O
]	O
.	O

The	O
third	O
one	O
is	O
the	O
volumetric	Method
downsampling	Method
block	Method
that	O
is	O
identical	O
to	O
a	O
volumetric	Method
max	Method
pooling	Method
layer	Method
.	O

The	O
last	O
one	O
is	O
the	O
volumetric	Method
upsampling	Method
block	Method
,	O
which	O
consists	O
of	O
a	O
volumetric	Method
deconvolution	Method
layer	Method
,	O
volumetric	Method
batch	Method
normalization	Method
layer	Method
,	O
and	O
the	O
activation	Method
function	Method
(	O
i.e.	O
,	O
ReLU	Method
)	O
.	O

Adding	O
the	O
batch	Method
normalization	Method
layer	Method
and	O
the	O
activation	O
function	O
to	O
the	O
deconvolution	Method
layer	Method
helps	O
to	O
ease	O
the	O
learning	Task
procedure	Task
.	O

The	O
kernel	O
size	O
of	O
the	O
residual	O
blocks	O
is	O
3×3×3	O
and	O
that	O
of	O
the	O
downsampling	Method
and	Method
upsampling	Method
layers	Method
is	O
2×2×2	O
with	O
stride	O
2	O
.	O

section	O
:	O
Network	Task
design	Task
The	O
V2V	Method
-	Method
PoseNet	Method
performs	O
voxel	Task
-	Task
to	Task
-	Task
voxel	Task
prediction	Task
.	O

Thus	O
,	O
it	O
is	O
based	O
on	O
the	O
3D	Method
CNN	Method
architecture	Method
that	O
treats	O
the	O
Z	O
-	O
axis	O
as	O
an	O
additional	O
spatial	O
axis	O
so	O
that	O
the	O
kernel	O
shape	O
is	O
w×h×d	O
.	O

Our	O
network	Method
architecture	Method
is	O
based	O
on	O
the	O
hourglass	Method
model	Method
[	O
reference	O
]	O
,	O
which	O
was	O
slightly	O
modified	O
for	O
more	O
accurate	O
estimation	Task
.	O

As	O
the	O
Figure	O
3	O
shows	O
,	O
the	O
network	O
starts	O
from	O
the	O
7×7×7	O
volumetric	O
basic	O
block	O
and	O
the	O
volumetric	O
downsampling	O
block	O
.	O

After	O
downsampling	O
the	O
feature	O
map	O
,	O
three	O
consecutive	O
residual	O
blocks	O
extract	O
useful	O
local	O
features	O
.	O

The	O
output	O
of	O
the	O
residual	O
blocks	O
goes	O
through	O
the	O
encoder	Method
and	Method
decoder	Method
described	O
in	O
Figures	O
5	O
and	O
6	O
,	O
respectively	O
.	O

In	O
the	O
encoder	O
,	O
the	O
volumetric	Method
downsampling	Method
block	Method
reduces	O
the	O
spatial	O
size	O
of	O
the	O
feature	O
map	O
while	O
the	O
volu	Method
-	Method
Figure	O
6	O
:	O
Decoder	Method
of	O
the	O
V2V	Method
-	Method
PoseNet	Method
.	O

The	O
numbers	O
below	O
each	O
block	O
indicate	O
the	O
spatial	O
size	O
and	O
number	O
of	O
channels	O
of	O
each	O
feature	Method
map	Method
.	O

We	O
plotted	O
feature	O
map	O
without	O
Z	O
-	O
axis	O
to	O
simplify	O
the	O
figure	O
.	O

metric	Method
residual	Method
bock	Method
increases	O
the	O
number	O
of	O
channels	O
.	O

It	O
is	O
empirically	O
confirmed	O
that	O
this	O
increase	O
in	O
the	O
number	O
of	O
channels	O
helps	O
improve	O
the	O
performance	O
in	O
our	O
experiments	O
.	O

On	O
the	O
other	O
hand	O
,	O
in	O
the	O
decoder	Method
,	O
the	O
volumetric	Method
upsampling	Method
block	Method
enlarges	O
the	O
spatial	O
size	O
of	O
the	O
feature	O
map	O
.	O

When	O
upsampling	O
,	O
the	O
network	O
decreases	O
the	O
number	O
of	O
channels	O
to	O
compress	O
the	O
extracted	O
features	O
.	O

The	O
enlargement	O
of	O
the	O
volumetric	O
size	O
in	O
the	O
decoder	Method
helps	O
the	O
network	O
to	O
densely	O
localize	O
keypoints	O
because	O
it	O
reduces	O
the	O
stride	O
between	O
voxels	O
in	O
the	O
feature	O
map	O
.	O

The	O
encoder	Method
and	O
decoder	Method
are	O
connected	O
with	O
the	O
voxel	O
-	O
wise	O
addition	O
for	O
each	O
scale	O
so	O
that	O
the	O
decoder	O
can	O
upsample	O
the	O
feature	O
map	O
more	O
stably	O
.	O

After	O
passing	O
the	O
input	O
through	O
the	O
encoder	Method
and	Method
decoder	Method
,	O
the	O
network	O
predicts	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
through	O
two	O
1×1×1	O
volumetric	Method
basic	Method
blocks	Method
and	O
one	O
1×1×1	O
volumetric	Method
convolutional	Method
layer	Method
.	O

section	O
:	O
Network	Method
training	Method
To	O
supervise	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
,	O
we	O
generate	O
3D	O
heatmap	O
,	O
wherein	O
the	O
mean	O
of	O
Gaussian	O
peak	O
is	O
positioned	O
at	O
the	O
ground	O
-	O
truth	O
joint	O
location	O
as	O
follows	O
:	O
where	O
H	O
*	O
n	O
is	O
the	O
ground	O
-	O
truth	O
3D	O
heatmap	O
of	O
nth	O
keypoint	O
,	O
(	O
i	O
n	O
,	O
j	O
n	O
,	O
k	O
n	O
)	O
is	O
the	O
ground	O
-	O
truth	O
voxel	O
coordinate	O
of	O
nth	O
keypoint	O
,	O
and	O
σ	O
=	O
1.7	O
is	O
the	O
standard	O
deviation	O
of	O
the	O
Gaussian	O
peak	O
.	O

Also	O
,	O
we	O
adopt	O
the	O
mean	Metric
square	Metric
error	Metric
as	O
a	O
loss	O
function	O
L	O
as	O
follows	O
:	O
where	O
H	O
*	O
n	O
and	O
H	O
n	O
are	O
the	O
ground	O
-	O
truth	O
and	O
estimated	O
heatmaps	O
for	O
nth	O
keypoint	O
,	O
respectively	O
,	O
and	O
N	O
denotes	O
the	O
number	O
of	O
keypoints	O
.	O

section	O
:	O
Implementation	O
details	O
The	O
proposed	O
V2V	Method
-	Method
PoseNet	Method
is	O
trained	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
from	O
scratch	O
.	O

All	O
weights	O
are	O
initialized	O
from	O
the	O
zero	Method
-	Method
mean	Method
Gaussian	Method
distribution	Method
with	O
σ	O
=	O
0.001	O
.	O

Gradient	O
vectors	O
are	O
calculated	O
from	O
the	O
loss	O
function	O
and	O
the	O
weights	O
are	O
updated	O
by	O
the	O
RMSProp	Method
[	O
reference	O
]	O
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
8	O
.	O

The	O
learning	Metric
rate	Metric
is	O
set	O
to	O
2.5×10	O
−4	O
.	O

The	O
size	O
of	O
the	O
input	O
to	O
the	O
proposed	O
system	O
is	O
88×88×88	O
.	O

We	O
perform	O
data	Task
augmentation	Task
including	O
rotation	O
(	O
[	O
reference	O
]	O
[	O
reference	O
]	O
and	O
the	O
NVIDIA	Method
Titan	Method
X	Method
GPU	Method
is	O
used	O
for	O
training	O
and	O
testing	Task
.	O

We	O
trained	O
our	O
model	O
for	O
10	O
epochs	O
.	O

section	O
:	O
Experiment	O
section	O
:	O
Datasets	O
ICVL	Material
Hand	Material
Posture	Material
Dataset	Material
.	O

The	O
ICVL	Material
dataset	O
[	O
reference	O
]	O
consists	O
of	O
330	O
K	O
training	O
and	O
1.6	O
K	O
testing	O
depth	O
images	O
.	O

The	O
frames	O
are	O
collected	O
from	O
10	O
different	O
subjects	O
using	O
Intel	Method
's	Method
Creative	Method
Interactive	Method
Gesture	Method
Camera	Method
[	O
reference	O
]	O
.	O

The	O
annotation	O
of	O
hand	O
pose	O
contains	O
16	O
joints	O
,	O
which	O
include	O
three	O
joints	O
for	O
each	O
finger	O
and	O
one	O
joint	O
for	O
the	O
palm	O
.	O

NYU	Material
Hand	Material
Pose	Material
Dataset	Material
.	O

The	O
NYU	Material
dataset	Material
[	O
reference	O
]	O
consists	O
of	O
72	O
K	O
training	O
and	O
8.2	O
K	O
testing	O
depth	O
images	O
.	O

The	O
training	O
set	O
is	O
collected	O
from	O
subject	O
A	O
,	O
whereas	O
the	O
testing	O
set	O
is	O
collected	O
from	O
subjects	O
A	O
and	O
B	O
by	O
three	O
Kinects	O
from	O
different	O
views	O
.	O

The	O
annotations	O
of	O
hand	O
pose	O
contain	O
36	O
joints	O
.	O

Most	O
of	O
the	O
previous	O
works	O
only	O
used	O
frames	O
from	O
the	O
frontal	O
view	O
and	O
14	O
out	O
of	O
36	O
joints	O
in	O
the	O
evaluation	O
,	O
and	O
we	O
also	O
followed	O
them	O
.	O

MSRA	Material
Hand	O
Pose	O
Dataset	O
.	O

The	O
MSRA	Material
dataset	Material
[	O
39	O
]	O
contains	O
9	O
subjects	O
with	O
17	O
gestures	O
for	O
each	O
subject	O
.	O

Intel	Method
's	Method
Creative	Method
Interactive	Method
Gesture	Method
Camera	Method
[	O
reference	O
]	O
captured	O
76	O
K	O
depth	O
images	O
with	O
21	O
annotated	O
joints	O
.	O

For	O
evaluation	O
,	O
the	O
leave	Method
-	Method
one	Method
-	Method
subject	Method
-	Method
out	Method
cross	Method
-	Method
validation	Method
strategy	Method
is	O
utilized	O
.	O

HANDS	O
2017	O
Frame	O
-	O
based	O
3D	Task
Hand	Task
Pose	O
Estimation	O
Challenge	O
Dataset	O
.	O

The	O
HANDS	O
2017	O
frame	O
-	O
based	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
dataset	O
[	O
reference	O
]	O
consists	O
of	O
957	O
K	O
training	O
and	O
295	O
K	O
testing	O
depth	O
images	O
that	O
are	O
sampled	O
from	O
BigHand2.2	Material
M	Material
[	O
reference	O
]	O
and	O
First	Material
-	Material
Person	Material
Hand	Material
Action	Material
[	O
reference	O
]	O
datasets	O
.	O

There	O
are	O
five	O
subjects	O
in	O
the	O
training	O
set	O
and	O
ten	O
subjects	O
in	O
the	O
testing	O
stage	O
,	O
including	O
five	O
unseen	O
subjects	O
.	O

The	O
ground	O
-	O
truth	O
of	O
this	O
dataset	O
is	O
the	O
3D	O
coordinates	O
of	O
[	O
reference	O
]	O
section	O
:	O
Evaluation	O
metrics	O
We	O
used	O
3D	Metric
distance	Metric
error	Metric
and	O
percentage	Metric
of	Metric
success	Metric
frame	Metric
metrics	Metric
for	O
3D	Task
hand	Task
pose	O
estimation	O
following	O
[	O
reference	O
][	O
reference	O
]	O
.	O

For	O
3D	O
human	Task
pose	Task
estimation	Task
,	O
we	O
used	O
mean	Metric
average	Metric
precision	Metric
(	O
mAP	Metric
)	O
that	O
is	O
defined	O
as	O
the	O
detected	O
ratio	O
of	O
all	O
human	O
body	O
joints	O
based	O
on	O
10	O
cm	O
rule	O
following	O
[	O
reference	O
][	O
reference	O
]	O
.	O

section	O
:	O
Ablation	Task
study	Task
We	O
used	O
NYU	Material
hand	O
pose	O
dataset	O
[	O
reference	O
]	O
to	O
analyze	O
each	O
component	O
of	O
our	O
model	O
because	O
this	O
dataset	O
is	O
challenging	O
and	O
far	O
from	O
saturated	O
.	O

3D	Method
representation	Method
and	O
per	Task
-	Task
voxel	Task
likelihood	Task
estimation	Task
.	O

To	O
demonstrate	O
the	O
validity	O
of	O
the	O
3D	Method
representation	Method
of	O
the	O
input	O
and	O
per	Task
-	Task
voxel	Task
likelihood	Task
estimation	Task
,	O
we	O
compared	O
the	O
performances	O
of	O
the	O
four	O
different	O
combinations	O
of	O
the	O
input	O
and	O
output	O
forms	O
in	O
Table	O
1	O
.	O

As	O
the	O
table	O
shows	O
,	O
converting	O
the	O
input	O
representation	O
type	O
from	O
the	O
2D	O
depth	O
map	O
to	O
3D	O
voxelized	O
form	O
(	O
also	O
converting	O
the	O
model	O
from	O
2D	Method
CNN	Method
to	O
3D	Method
CNN	Method
)	O
substantially	O
improves	O
performance	O
,	O
regardless	O
of	O
output	O
representation	O
.	O

This	O
justifies	O
the	O
effectiveness	O
of	O
the	O
proposed	O
3D	Method
input	Method
representation	Method
that	O
is	O
free	O
from	O
perspective	O
distortion	O
.	O

The	O
results	O
also	O
show	O
that	O
converting	O
the	O
output	Method
representation	Method
from	O
the	O
3D	O
coordinates	O
to	O
the	O
per	O
-	O
voxel	O
likelihood	O
increases	O
the	O
performance	O
significantly	O
,	O
regardless	O
of	O
the	O
input	O
type	O
.	O

Among	O
the	O
four	O
combinations	O
,	O
voxel	O
-	O
to	O
-	O
voxel	O
gives	O
the	O
best	O
performance	O
even	O
with	O
the	O
smallest	O
number	O
of	O
parameters	O
.	O

Hence	O
,	O
the	O
superiority	O
of	O
the	O
voxel	Method
-	Method
to	Method
-	Method
voxel	Method
prediction	Method
scheme	Method
compared	O
with	O
other	O
input	O
and	O
output	O
combinations	O
is	O
clearly	O
justified	O
.	O

Table	O
3	O
:	O
Comparison	O
of	O
the	O
proposed	O
method	O
(	O
V2V	Method
-	Method
PoseNet	Method
)	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
the	O
three	O
3D	Task
hand	Task
pose	O
datasets	O
.	O

Mean	Metric
error	Metric
indicates	O
the	O
average	Metric
3D	Metric
distance	Metric
error	Metric
.	O

To	O
fairly	O
compare	O
four	O
combinations	O
,	O
we	O
used	O
the	O
same	O
network	Method
building	Method
blocks	Method
and	O
design	O
,	O
which	O
were	O
introduced	O
in	O
Section	O
6	O
.	O

The	O
only	O
difference	O
is	O
that	O
the	O
model	O
for	O
the	O
per	Task
-	Task
voxel	Task
likelihood	Task
estimation	Task
is	O
fully	Method
convolutional	Method
,	O
whereas	O
for	O
the	O
coordinate	Task
regression	Task
,	O
we	O
used	O
fully	O
connected	O
layers	O
at	O
the	O
end	O
of	O
the	O
network	O
.	O

Simply	O
converting	O
voxel	O
-	O
to	O
-	O
voxel	O
to	O
pixel	O
-	O
to	O
-	O
voxel	O
decreases	O
the	O
number	O
of	O
parameters	O
because	O
the	O
model	O
is	O
changed	O
from	O
the	O
3D	Method
CNN	Method
to	O
the	O
2D	Method
CNN	Method
.	O

To	O
compensate	O
for	O
this	O
change	O
,	O
we	O
doubled	O
the	O
number	O
of	O
channels	O
of	O
each	O
feature	Method
map	Method
in	O
the	O
pixelto	Method
-	Method
voxel	Method
model	Method
.	O

If	O
the	O
number	O
of	O
channels	O
is	O
not	O
doubled	O
,	O
then	O
the	O
performance	O
was	O
degraded	O
.	O

For	O
all	O
four	O
models	O
,	O
we	O
used	O
48×48	O
depth	O
map	O
or	O
48×48×48	O
voxelized	O
grid	O
as	O
input	O
because	O
the	O
original	O
size	O
(	O
88×88×88	O
)	O
does	O
not	O
fit	O
into	O
GPU	O
memory	O
in	O
the	O
case	O
of	O
voxel	O
-	O
to	O
-	O
coordinates	O
.	O

Refining	Task
localization	Task
of	Task
the	Task
target	Task
object	Task
.	O

To	O
demonstrate	O
the	O
importance	O
of	O
the	O
localization	Method
refining	Method
procedure	Method
in	O
Section	O
4	O
,	O
we	O
compared	O
the	O
performance	O
of	O
two	O
with	O
and	O
without	O
the	O
localization	Method
refinement	Method
step	Method
.	O

As	O
shown	O
in	O
Table	O
2	O
,	O
the	O
refined	O
reference	O
points	O
significantly	O
boost	O
the	O
accuracy	Metric
of	O
our	O
model	O
,	O
which	O
shows	O
that	O
the	O
reference	Method
point	Method
refining	Method
procedure	Method
has	O
a	O
crucial	O
influence	O
on	O
the	O
performance	O
.	O

Epoch	Method
ensemble	Method
.	O

To	O
obtain	O
more	O
accurate	O
and	O
robust	O
estimation	Task
,	O
we	O
applied	O
a	O
simple	O
ensemble	Method
technique	Method
that	O
we	O
call	O
epoch	Method
ensemble	Method
.	O

The	O
epoch	Method
ensemble	Method
averages	O
the	O
estimations	O
from	O
several	O
epochs	O
.	O

Specifically	O
,	O
we	O
save	O
the	O
trained	O
model	O
for	O
each	O
epoch	O
in	O
the	O
training	O
stage	O
and	O
then	O
in	O
the	O
testing	O
stage	O
,	O
we	O
average	O
all	O
the	O
estimated	O
3D	O
coordinates	O
from	O
the	O
trained	O
models	O
.	O

As	O
we	O
trained	O
our	O
model	O
by	O
10	O
epochs	O
,	O
we	O
used	O
10	O
models	O
to	O
obtain	O
the	O
final	O
estimation	Task
.	O

Epoch	Method
ensemble	Method
has	O
no	O
influence	O
in	O
running	Metric
time	Metric
when	O
each	O
model	O
is	O
running	O
in	O
different	O
GPUs	O
.	O

However	O
,	O
in	O
a	O
single	O
-	O
GPU	O
environment	O
,	O
epoch	Method
ensemble	Method
linearly	O
increases	O
running	Metric
time	Metric
.	O

The	O
effect	O
of	O
epoch	O
ensemble	O
is	O
shown	O
in	O
Table	O
2	O
.	O

section	O
:	O
Comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
We	O
compared	O
the	O
performance	O
of	O
the	O
V2V	Method
-	Method
PoseNet	Method
on	O
the	O
three	O
3D	Task
hand	Task
pose	O
estimation	O
datasets	O
(	O
ICVL	Material
[	O
reference	O
]	O
NYU	Material
[	O
reference	O
]	O
,	O
and	O
MSRA	Material
[	O
reference	O
]	O
)	O
with	O
most	O
of	O
the	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
,	O
which	O
include	O
latent	Method
random	Method
forest	Method
(	O
LRF	Method
)	O
[	O
reference	O
]	O
,	O
cascaded	Method
hand	Method
pose	Method
regression	Method
(	O
Cascade	Method
)	O
[	O
reference	O
]	O
,	O
DeepPrior	Method
with	Method
refinement	Method
(	O
DeepPrior	Method
)	O
[	O
reference	O
]	O
,	O
feedback	Method
loop	Method
training	Method
method	Method
(	O
Feedback	Method
)	O
[	O
reference	O
]	O
,	O
hand	Method
model	Method
based	Method
method	Method
(	O
DeepModel	Method
)	O
[	O
reference	O
]	O
,	O
hierarchical	Method
sampling	Method
optimization	Method
(	O
HSO	Method
)	O
[	O
reference	O
]	O
,	O
local	O
surface	O
normals	O
(	O
LSN	Method
)	O
[	O
reference	O
]	O
,	O
multiview	Method
CNN	Method
(	O
MultiView	O
)	O
[	O
reference	O
]	O
,	O
DISCO	Method
[	O
reference	O
]	O
,	O
Hand3D	Method
[	O
reference	O
]	O
,	O
DeepHand	O
[	O
reference	O
]	O
,	O
lie	Method
-	Method
x	Method
group	Method
based	Method
method	Method
(	O
Lie	O
-	O
X	O
)	O
[	O
reference	O
]	O
,	O
improved	O
DeepPrior	Method
(	O
DeepPrior	Method
++	Method
)	O
[	O
reference	O
]	O
,	O
region	Method
ensemble	Method
network	Method
(	O
REN	Method
-	Method
4×6×6	Method
[	O
reference	O
]	O
,	O
REN	O
-	O
9×6×6	O
[	O
reference	O
]	O
)	O
,	O
CrossingNets	O
[	O
reference	O
]	O
,	O
pose	Method
-	Method
guided	Method
REN	Method
(	O
Pose	Method
-	Method
REN	Method
)	O
[	O
reference	O
]	O
,	O
global	O
-	O
to	O
-	O
local	Method
prediction	Method
method	Method
(	O
Global	O
-	O
to	O
-	O
Local	O
)	O
[	O
reference	O
]	O
,	O
classification	Method
-	Method
guided	Method
approach	Method
(	O
Cls	Method
-	Method
Guide	Method
)	O
[	O
reference	O
]	O
,	O
3DCNN	Method
based	Method
method	Method
(	O
3DCNN	Method
)	O
[	O
reference	O
]	O
,	O
occlusion	Method
aware	Method
based	Method
method	Method
(	O
Occlusion	Method
)	O
[	O
reference	O
]	O
,	O
and	O
hallucinating	Method
heat	Method
distribution	Method
method	Method
(	O
HeatDist	Method
)	O
[	O
reference	O
]	O
.	O

Some	O
reported	O
results	O
of	O
previous	O
works	O
[	O
reference	O
]	O
are	O
calculated	O
by	O
prediction	O
labels	O
available	O
online	O
.	O

Other	O
results	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
are	O
calculated	O
from	O
the	O
figures	O
and	O
tables	O
of	O
their	O
papers	O
.	O

As	O
shown	O
in	O
Figure	O
7	O
and	O
Table	O
3	O
,	O
our	O
method	O
outperforms	O
all	O
existing	O
methods	O
on	O
the	O
three	O
3D	Task
hand	Task
pose	Task
estimation	Task
datasets	Task
in	O
standard	O
evaluation	Metric
metrics	Metric
.	O

This	O
shows	O
the	O
superiority	O
of	O
voxel	Method
-	Method
to	Method
-	Method
voxel	Method
prediction	Method
,	O
which	O
is	O
firstly	O
used	O
in	O
3D	Task
hand	Task
pose	O
estimation	O
.	O

The	O
performance	O
gap	O
between	O
ours	O
and	O
the	O
previous	O
works	O
is	O
largest	O
on	O
the	O
NYU	Material
dataset	Material
that	O
is	O
very	O
challenging	O
and	O
far	O
from	O
saturated	O
.	O

We	O
additionally	O
measured	O
the	O
average	Metric
3D	Metric
distance	Metric
error	Metric
distribution	Metric
over	O
various	O
yaw	O
and	O
pitch	O
angles	O
on	O
the	O
MSRA	Material
dataset	Material
following	O
the	O
protocol	O
of	O
previous	O
works	O
[	O
reference	O
]	O
as	O
in	O
Figure	O
8	O
.	O

As	O
it	O
demonstrates	O
,	O
our	O
method	O
provides	O
superior	O
results	O
in	O
almost	O
all	O
of	O
yaw	O
and	O
pitch	O
angles	O
.	O

Our	O
method	O
also	O
placed	O
first	O
in	O
the	O
HANDS	O
2017	O
framebased	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
[	O
reference	O
]	O
.	O

The	O
top	O
-	O
5	O
results	O
comparisons	O
are	O
shown	O
in	O
Table	O
4	O
.	O

As	O
shown	O
in	O
the	O
table	O
,	O
the	O
proposed	O
V2V	Method
-	Method
PoseNet	Method
outperforms	O
other	O
participants	O
.	O

A	O
more	O
detailed	O
analysis	O
of	O
the	O
challenge	O
results	O
is	O
covered	O
in	O
[	O
reference	O
]	O
.	O

We	O
also	O
evaluated	O
the	O
performance	O
of	O
the	O
proposed	O
system	O
on	O
the	O
ITOP	Material
3D	Material
human	Material
pose	Material
estimation	Material
dataset	Material
[	O
reference	O
]	O
.	O

We	O
compared	O
the	O
system	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
works	O
,	O
which	O
include	O
random	Method
forest	Method
-	Method
based	Method
method	Method
(	O
RF	Method
)	O
[	O
reference	O
]	O
,	O
RTW	Method
[	O
reference	O
]	O
method	O
(	O
VI	Method
)	O
[	O
reference	O
]	O
,	O
and	O
REN	Method
-	Method
9x6x6	Method
[	O
reference	O
]	O
.	O

The	O
score	O
of	O
each	O
method	O
is	O
obtained	O
from	O
[	O
reference	O
][	O
reference	O
]	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
the	O
proposed	O
system	O
outperforms	O
all	O
the	O
existing	O
methods	O
by	O
a	O
large	O
margin	O
in	O
both	O
of	O
views	O
,	O
which	O
indicates	O
that	O
our	O
model	O
can	O
be	O
applied	O
to	O
not	O
only	O
3D	Task
hand	Task
pose	O
estimation	O
,	O
but	O
also	O
other	O
challenging	O
problems	O
such	O
as	O
3D	O
human	Task
pose	Task
estimation	Task
from	O
the	O
front	O
-	O
and	O
top	O
-	O
views	O
.	O

The	O
qualitative	O
results	O
of	O
the	O
V2V	Method
-	Method
PoseNet	Method
on	O
the	O
ICVL	Material
,	O
NYU	Material
,	O
MSRA	Material
,	O
HANDS	Material
2017	Material
,	O
ITOP	Material
front	Material
-	Material
view	Material
,	O
and	O
ITOP	Material
top	Material
-	Material
view	Material
datasets	Material
are	O
shown	O
in	O
Figure	O
9	O
,	O
10	O
,	O
11	O
,	O
12	O
,	O
13	O
,	O
and	O
14	O
,	O
respectively	O
.	O

section	O
:	O
Computational	Metric
complexity	Metric
We	O
investigated	O
the	O
computational	Metric
complexity	Metric
of	O
the	O
proposed	O
method	O
.	O

The	O
training	Metric
time	Metric
of	O
the	O
V2V	Method
-	Method
PoseNet	Method
is	O
two	O
days	O
for	O
ICVL	Material
dataset	O
,	O
12	O
hours	O
for	O
NYU	Material
and	O
MSRA	Material
datasets	Material
,	O
six	O
days	O
for	O
HANDS	Material
2017	Material
challenge	Material
dataset	Material
,	O
and	O
three	O
hours	O
for	O
ITOP	Material
dataset	Material
.	O

The	O
testing	Metric
time	Metric
is	O
3.5	O
fps	Metric
when	O
using	O
10	O
models	O
for	O
epoch	Task
ensemble	Task
,	O
but	O
can	O
accelerate	O
to	O
35	O
fps	O
in	O
a	O
multi	O
-	O
GPU	O
environment	O
,	O
which	O
shows	O
the	O
applicability	O
of	O
the	O
proposed	O
method	O
to	O
real	Task
-	Task
time	Task
applications	Task
.	O

The	O
most	O
time	O
-	O
consuming	O
step	O
is	O
the	O
input	Task
generation	Task
that	O
includes	O
reference	Task
point	Task
refinement	Task
and	O
voxelizing	O
the	O
depth	O
map	O
.	O

This	O
step	O
takes	O
23	O
ms	O
and	O
most	O
of	O
the	O
time	O
is	O
spent	O
on	O
voxelizing	O
.	O

The	O
next	O
step	O
is	O
network	Task
forwarding	Task
,	O
which	O
takes	O
5	O
ms	O
and	O
takes	O
0.5	O
ms	O
to	O
extract	O
3D	O
coordinates	O
from	O
the	O
3D	O
heatmap	O
.	O

Note	O
that	O
our	O
model	O
outperforms	O
previous	O
works	O
by	O
a	O
large	O
margin	O
without	O
epoch	Method
ensemble	Method
on	O
the	O
ICVL	Material
,	O
NYU	Material
,	O
MSRA	Material
,	O
and	O
ITOP	Material
datasets	Material
while	O
running	O
in	O
real	O
-	O
time	O
using	O
a	O
single	O
GPU	Method
.	O

section	O
:	O
Conclusion	O
We	O
proposed	O
a	O
novel	O
and	O
powerful	O
network	O
,	O
V2V	Method
-	Method
PoseNet	Method
,	O
for	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
.	O

To	O
overcome	O
the	O
drawbacks	O
of	O
previous	O
works	O
,	O
we	O
converted	O
2D	O
depth	O
map	O
into	O
the	O
3D	Method
voxel	Method
representation	Method
and	O
processed	O
it	O
using	O
our	O
3D	Method
CNN	Method
model	Method
.	O

Also	O
,	O
instead	O
of	O
directly	O
regressing	O
3D	O
coordinates	O
of	O
keypoints	O
,	O
we	O
estimated	O
the	O
per	O
-	O
voxel	O
likelihood	O
for	O
each	O
keypoint	O
.	O

Those	O
two	O
conversions	O
boost	O
the	O
performance	O
significantly	O
and	O
make	O
the	O
proposed	O
V2V	Method
-	Method
PoseNet	Method
outperform	O
previous	O
works	O
on	O
the	O
three	O
3D	Task
hand	Task
and	O
one	O
3D	O
human	Task
pose	Task
estimation	Task
datasets	O
by	O
a	O
large	O
margin	O
.	O

It	O
also	O
allows	O
us	O
to	O
win	O
the	O
3D	Task
hand	Task
pose	O
estimation	O
challenge	O
.	O

As	O
voxel	Method
-	Method
tovoxel	Method
prediction	Method
is	O
firstly	O
tried	O
in	O
3D	Task
hand	Task
and	O
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
map	O
,	O
we	O
hope	O
this	O
work	O
to	O
provide	O
a	O
new	O
way	O
of	O
accurate	Task
3D	Task
pose	Task
estimation	Task
.	O

section	O
:	O
