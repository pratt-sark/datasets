Practical	O
Bayesian	Method
Optimization	Method
of	Method
Machine	Method
Learning	Method
Algorithms	Method
The	O
use	O
of	O
machine	Method
learning	Method
algorithms	Method
frequently	O
involves	O
careful	O
tuning	O
of	O
learning	O
parameters	O
and	O
model	Method
hyperparameters	Method
.	O

Unfortunately	O
,	O
this	O
tuning	O
is	O
often	O
a	O
“	O
black	O
art	O
”	O
requiring	O
expert	O
experience	O
,	O
rules	O
of	O
thumb	O
,	O
or	O
sometimes	O
bruteforce	Method
search	Method
.	O

There	O
is	O
therefore	O
great	O
appeal	O
for	O
automatic	Method
approaches	Method
that	O
can	O
optimize	O
the	O
performance	O
of	O
any	O
given	O
learning	Method
algorithm	Method
to	O
the	O
problem	O
at	O
hand	O
.	O

In	O
this	O
work	O
,	O
we	O
consider	O
this	O
problem	O
through	O
the	O
framework	O
of	O
Bayesian	Task
optimization	Task
,	O
in	O
which	O
a	O
learning	Method
algorithm	Method
’s	O
generalization	Task
performance	O
is	O
modeled	O
as	O
a	O
sample	O
from	O
a	O
Gaussian	Method
process	Method
(	O
GP	Method
)	O
.	O

We	O
show	O
that	O
certain	O
choices	O
for	O
the	O
nature	O
of	O
the	O
GP	Method
,	O
such	O
as	O
the	O
type	O
of	O
kernel	O
and	O
the	O
treatment	O
of	O
its	O
hyperparameters	O
,	O
can	O
play	O
a	O
crucial	O
role	O
in	O
obtaining	O
a	O
good	O
optimizer	Method
that	O
can	O
achieve	O
expertlevel	O
performance	O
.	O

We	O
describe	O
new	O
algorithms	O
that	O
take	O
into	O
account	O
the	O
variable	O
cost	O
(	O
duration	O
)	O
of	O
learning	Method
algorithm	Method
experiments	O
and	O
that	O
can	O
leverage	O
the	O
presence	O
of	O
multiple	O
cores	O
for	O
parallel	Task
experimentation	Task
.	O

We	O
show	O
that	O
these	O
proposed	O
algorithms	O
improve	O
on	O
previous	O
automatic	Method
procedures	Method
and	O
can	O
reach	O
or	O
surpass	O
human	Method
expert	Method
-	Method
level	Method
optimization	Method
for	O
many	O
algorithms	O
including	O
latent	Method
Dirichlet	Method
allocation	Method
,	O
structured	Method
SVMs	Method
and	O
convolutional	Method
neural	Method
networks	Method
.	O

Practical	O
Bayesian	Method
Optimization	Method
of	Method
Machine	Method
Learning	Method
Algorithms	Method
Jasper	O
Snoek	O
,	O
Ryan	O
Adams	O
,	O
Hugo	O
LaRochelle	O
–	O
NIPS	O
2012	O
“	O
...	O
(	O
Gaussian	Method
Processes	Method
)	O
are	O
inadequate	O
for	O
doing	O
speech	Task
and	O
vision	Task
.	O

I	O
still	O
think	O
they	O
're	O
inadequate	O
for	O
doing	O
speech	Task
and	O
vision	Task
.	O

But	O
when	O
you	O
're	O
in	O
a	O
domain	O
where	O
you	O
have	O
no	O
prior	O
knowledge	O
and	O
the	O
only	O
thing	O
that	O
you	O
can	O
expect	O
is	O
that	O
similar	O
inputs	O
should	O
have	O
similar	O
outputs	O
,	O
then	O
Gaussian	Method
Processes	Method
are	O
ideal	O
”	O
.	O

“	O
...	O
(	O
Gaussian	Method
Processes	Method
)	O
are	O
inadequate	O
for	O
doing	O
speech	Task
and	O
vision	Task
.	O

I	O
still	O
think	O
they	O
're	O
inadequate	O
for	O
doing	O
speech	Task
and	O
vision	Task
.	O

But	O
when	O
you	O
're	O
in	O
a	O
domain	O
where	O
you	O
have	O
no	O
prior	O
knowledge	O
and	O
the	O
only	O
thing	O
that	O
you	O
can	O
expect	O
is	O
that	O
similar	O
inputs	O
should	O
have	O
similar	O
outputs	O
,	O
then	O
Gaussian	Method
Processes	Method
are	O
ideal	O
”	O
.	O

“	O
...	O
Gaussian	Method
processes	Method
are	O
a	O
way	O
of	O
using	O
Machine	Method
Learning	Method
to	O
simulate	O
the	O
graduate	O
student	O
”	O
-	O
Geoff	O
Hinton	O
Motivation	O
N	O
…	O
.	O

1	O
2	O
3	O
...	O
...	O
...	O
...	O
Deep	Method
Neural	Method
Networks	Method
Require	O
Skill	O
to	O
Set	O
Hyperparameters	O
Common	O
Strategies	O
Grid	Method
Search	Method
Random	Method
Search	Method
Common	O
Strategies	O
Grid	Method
Search	Method
Random	Method
Search	Method
-	O
Sometimes	O
better	O
because	O
some	O
parameters	O
have	O
no	O
effect	O
Can	O
we	O
use	O
Machine	Method
Learning	Method
instead	O
?	O
-	O
To	O
predict	O
regions	O
of	O
the	O
hyperparameter	O
Space	O
that	O
might	O
give	O
better	O
results	O
.	O

-	O
to	O
predict	O
how	O
well	O
a	O
new	O
combination	O
of	O
hyperparameters	O
will	O
do	O
and	O
also	O
model	O
the	O
uncertainty	O
of	O
that	O
prediction	O
Bayesian	Method
Optimization	Method
-	O
Frame	Method
Hyperparameter	Method
Search	Method
as	O
an	O
Optimization	Method
Problem	Method
Bayesian	Method
Optimization	Method
-	O
Frame	Method
Hyperparameter	Method
Search	Method
as	O
an	O
Optimization	Task
Problem	Task
-	O
Model	O
the	O
estimation	O
of	O
the	O
function	O
from	O
high	O
level	O
parameters	O
(	O
hyperparameters	O
)	O
to	O
the	O
error	Metric
metric	Metric
as	O
a	O
regression	Task
problem	Task
Bayesian	Task
Optimization	Task
-	O
Frame	O
Hyperparameter	Method
Search	Method
as	O
an	O
Optimization	Task
Problem	Task
-	O
Model	O
the	O
estimation	O
of	O
the	O
function	O
from	O
high	O
level	O
parameters	O
(	O
hyperparameters	O
)	O
to	O
the	O
error	Metric
metric	Metric
as	O
a	O
regression	Task
problem	Task
-	O
Use	O
G.P	Method
Prior	O
:	O
“	O
Similar	O
inputs	O
have	O
similar	O
outputs	O
”	O
to	O
build	O
a	O
statistical	Method
model	Method
of	O
the	O
function	O
.	O

Prior	O
is	O
weak	O
but	O
general	O
and	O
effective	O
.	O

Bayesian	Task
Optimization	Task
-	O
Frame	O
Hyperparameter	Task
Search	Task
as	O
an	O
Optimization	Task
Problem	Task
-	O
Model	O
the	O
estimation	O
of	O
the	O
function	O
from	O
high	O
level	O
parameters	O
(	O
hyperparameters	O
)	O
to	O
the	O
error	Metric
metric	Metric
as	O
a	O
regression	Task
problem	Task
-	O
Use	O
G.P	O
Prior	O
:	O
“	O
Similar	O
inputs	O
have	O
similar	O
outputs	O
”	O
to	O
build	O
a	O
statistical	Method
model	Method
of	O
the	O
function	O
.	O

Prior	Method
is	O
weak	O
but	O
general	O
and	O
effective	O
.	O

-	O
Use	O
statistics	O
to	O
tell	O
us	O
:	O
•	O
Location	O
of	O
expected	O
minimum	O
of	O
the	O
function	O
•	O
Expected	O
Improvement	O
of	O
trying	O
other	O
parameters	O
Bayesian	Method
Optimization	Method
(	O
Mockus	O
'	O
78	O
)	O
-	O
Method	O
for	O
the	O
global	Task
optimization	Task
of	Task
multi	Task
-	Task
modal	Task
,	Task
computationally	Task
expensive	Task
black	Task
box	Task
functions	Task
-	O
Assumes	O
that	O
the	O
unknown	O
function	O
was	O
sampled	O
from	O
a	O
Gaussian	Method
Process	Method
(	O
prior	O
)	O
and	O
uses	O
the	O
observations	O
(	O
likelihood	O
)	O
to	O
maintain	O
a	O
posterior	O
-	O
Observations	O
are	O
the	O
measure	O
of	O
generalization	Task
performance	O
under	O
different	O
settings	O
of	O
the	O
hyperparameters	O
we	O
wish	O
to	O
optimize	O
.	O

-	O
The	O
next	O
set	O
of	O
hyperparameters	O
are	O
selected	O
using	O
the	O
maintained	O
posterior	O
–	O
using	O
a	O
strategy	O
determined	O
by	O
the	O
acquisition	Method
function	Method
Gaussian	Method
Processes	Method
Specifies	O
a	O
distribution	O
over	O
functions	O
such	O
that	O
any	O
finite	O
subset	O
of	O
N	O
points	O
follows	O
a	O
Multivariate	Method
Gaussian	Method
Distribution	Method
.	O

Gaussian	Method
Processes	Method
Specifies	O
a	O
distribution	O
over	O
functions	O
such	O
that	O
any	O
finite	O
subset	O
of	O
N	O
points	O
follows	O
a	O
Multivariate	Method
Gaussian	Method
Distribution	Method
.	O

The	O
properties	O
of	O
the	O
resulting	O
distribution	O
on	O
functions	O
is	O
specified	O
by	O
a	O
mean	O
and	O
a	O
positive	O
definite	O
covariance	O
function	O
The	O
predictive	Metric
mean	Metric
and	Metric
covariance	Metric
given	O
the	O
observations	O
Is	O
given	O
by	O
:	O
Intuition	O
•	O
GP	Method
's	Method
are	O
a	O
prior	O
for	O
smooth	O
functions	O
•	O
Similar	O
inputs	O
(	O
high	O
covariance	O
)	O
should	O
have	O
similar	O
outputs	O
Intuition	O
Exploration	O
:	O
Seek	O
Places	O
with	O
High	O
Variance	O
Exploitation	O
:	O
Seek	O
Places	O
in	O
the	O
locality	O
of	O
places	O
you	O
're	O
already	O
doing	O
well	O
at	O
.	O

Intuition	Task
Exploration	Task
:	O
Seek	O
Places	O
with	O
High	O
Variance	O
Exploitation	O
:	O
Seek	O
Places	O
in	O
the	O
locality	O
of	O
places	O
you	O
're	O
already	O
doing	O
well	O
at	O
.	O

The	O
acquisition	Method
function	Method
balances	O
these	O
to	O
determine	O
point	O
of	O
next	O
evaluation	Task
Acquisition	Task
Functions	Task
The	O
Acquisition	O
function	O
tells	O
us	O
which	O
experiment	O
to	O
run	O
next	O
and	O
what	O
it	O
's	O
goodness	O
will	O
be	O
1	O
.	O

GP	Method
Upper	Method
Confidence	Method
Bound	Method
Idea	O
:	O
Minimize	Task
regret	Task
over	O
course	O
of	O
optimization	Task
.	O

Balance	Task
exploration	Task
and	Task
exploitation	Task
2	O
.	O

Expected	O
Improvement	O
Idea	O
:	O
How	O
much	O
can	O
I	O
expect	O
to	O
improve	O
over	O
the	O
best	O
I	O
'	O
ve	O
seen	O
so	O
far	O
by	O
running	O
an	O
experiment	O
with	O
these	O
parameters	O
?	O
Intuition	O
Intuition	O
Intuition	O
Intuition	O
Intuition	O
Intuition	O
Intuition	O
Intuition	O
An	O
Eggsperiment	O
Parameters	O
:	O
Boiling	O
Time	O
(	O
1	O
-	O
12	O
m	O
)	O
Cooling	O
Time	O
(	O
1	O
-	O
12	O
m	O
)	O
Salt	O
(	O
0	O
-	O
10	O
pinches	O
)	O
Pepper	O
(	O
0	O
-	O
10	O
pinches	O
)	O
Optimal	O
'	O
Soft	O
Boiled	O
Egg	O
'	O
After	O
5	O
Iterations	O
....	O
After	O
5	O
Iterations	O
....	O
After	O
10	O
Iterations	O
....	O
After	O
10	O
Iterations	O
....	O
After	O
12	O
Iterations	O
....	O
After	O
14	O
Iterations	O
....	O
After	O
16	O
Iterations	O
....	O
After	O
20	O
Iterations	O
....	O
After	O
25	O
Iterations	O
....	O
After	O
25	O
Iterations	O
....	O
Practical	Task
Bayesian	Task
Optimization	Task
•	O
Integrate	O
out	O
all	O
parameters	O
in	O
Bayesian	Task
Optimization	Task
•	O
Choose	O
appropriate	O
covariance	O
•	O
Choice	O
of	O
acquisition	O
function	O
is	O
important	O
Accounting	O
for	O
additional	O
cost	O
–	O
Expected	Metric
Improvement	Metric
per	O
Second	O
Incorporate	O
a	O
preference	O
towards	O
choosing	O
points	O
that	O
are	O
not	O
only	O
good	O
,	O
but	O
likely	O
to	O
be	O
evaluated	O
quickly	O
Parallelizing	O
Bayesian	Method
Optimization	Method
'	O
N	O
'	O
completed	O
evaluations	O
'	O
J	O
'	O
pending	O
evaluations	O
Parallelizing	O
Bayesian	Method
Optimization	Method
'	O
N	O
'	O
completed	O
evaluations	O
'	O
J	O
'	O
pending	O
evaluations	O
Posterior	O
samples	O
after	O
3	O
Observations	O
Expected	O
improvement	O
under	O
individual	O
samples	O
Integrated	O
expected	O
improvement	O
Implications	O
Implications	O
Impossible	O
to	O
find	O
by	O
hand	O
!	O

!	O

CIFAR	Material
-	Material
10	Material
,	O
9	O
Hyperparameters	O
Benefits	O
For	O
each	O
input	O
dimension	O
,	O
an	O
appropriate	O
scale	O
for	O
measuring	Task
similarity	Task
is	O
learned	O
.	O

-	O
are	O
200	O
and	O
300	O
as	O
similar	O
as	O
2.0	O
and	O
3.0	O
?	O
Benefits	O
For	O
each	O
input	O
dimension	O
,	O
an	O
appropriate	O
scale	O
for	O
measuring	Task
similarity	Task
is	O
learned	O
.	O

-	O
are	O
200	O
and	O
300	O
as	O
similar	O
as	O
2.0	O
and	O
3.0	O
?	O
What	O
is	O
the	O
sensitivity	O
to	O
each	O
dimension	O
?	O
Which	O
dimensions	O
do	O
n't	O
matter	O
?	O
Benefits	O
For	O
each	O
input	O
dimension	O
,	O
an	O
appropriate	O
scale	O
for	O
measuring	Task
similarity	Task
is	O
learned	O
.	O

-	O
are	O
200	O
and	O
300	O
as	O
similar	O
as	O
2.0	O
and	O
3.0	O
?	O
What	O
is	O
the	O
sensitivity	O
to	O
each	O
dimension	O
?	O
Which	O
dimensions	O
do	O
n't	O
matter	O
?	O
Reproducible	O
Research	O
–	O
level	O
the	O
playing	O
field	O
.	O

Its	O
a	O
lot	O
more	O
honest	O
than	O
human	O
beings	O
Benefits	O
For	O
each	O
input	O
dimension	O
,	O
an	O
appropriate	O
scale	O
for	O
measuring	Task
similarity	Task
is	O
learned	O
.	O

-	O
are	O
200	O
and	O
300	O
as	O
similar	O
as	O
2.0	O
and	O
3.0	O
?	O
What	O
is	O
the	O
sensitivity	O
to	O
each	O
dimension	O
?	O
Which	O
dimensions	O
do	O
n't	O
matter	O
?	O
Reproducible	O
Research	O
–	O
level	O
the	O
playing	O
field	O
.	O

Its	O
a	O
lot	O
more	O
honest	O
than	O
human	O
beings	O
If	O
you	O
have	O
the	O
resources	O
to	O
run	O
a	O
fairly	O
large	O
number	O
of	O
experiments	O
,	O
bayesian	Method
optimization	Method
is	O
better	O
than	O
a	O
person	O
at	O
finding	O
good	O
combinations	O
of	O
hyperparameters	O
References	O
:	O
[	O
Paper	O
]	O
Practical	O
Bayesian	Method
Optimization	Method
of	Method
Machine	Method
Learning	Method
Algorithms	Method
Jasper	O
Snoek	O
,	O
Hugo	O
Larochelle	O
and	O
Ryan	O
P.	O
Adams	O
Advances	O
in	O
Neural	Task
Information	Task
Processing	Task
Systems	Task
,	O
2012	O
[	O
Talk	O
/	O
Slides	O
]	O
Jasper	O
Snoek	O
:	O
"	O
Bayesian	Method
Optimization	Method
for	O
Machine	Task
Learning	Task
and	Task
Science	Task
"	O
https:	O
//	O
www.youtube.com	O
/	O
watch?v=a79klpzaPgY	O
[	O
Book	O
]	O
Machine	Task
Learning	Task
:	O
a	O
Probabilistic	O
Perspective	O
Kevin	O
Murphy	O
http:	O
//	O
www.cs.ubc.ca	O
/	O
~murphyk	O
/	O
MLbook	O
/	O
index.html	O
