document	O
:	O
Image	Task
Restoration	Task
Using	O
Convolutional	Method
Auto	Method
-	Method
encoders	Method
with	O
Symmetric	Method
Skip	Method
Connections	Method
Image	Task
restoration	Task
,	O
including	O
image	Task
denoising	Task
,	O
super	Task
resolution	Task
,	O
inpainting	Task
,	O
and	O
so	O
on	O
,	O
is	O
a	O
well	O
-	O
studied	O
problem	O
in	O
computer	Task
vision	Task
and	O
image	Task
processing	Task
,	O
as	O
well	O
as	O
a	O
test	O
bed	O
for	O
low	Method
-	Method
level	Method
image	Method
modeling	Method
algorithms	Method
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
very	O
deep	Method
fully	Method
convolutional	Method
auto	Method
-	Method
encoder	Method
network	Method
for	O
image	Task
restoration	Task
,	O
which	O
is	O
a	O
encoding	Method
-	Method
decoding	Method
framework	Method
with	O
symmetric	Method
convolutional	Method
-	Method
deconvolutional	Method
layers	Method
.	O

In	O
other	O
words	O
,	O
the	O
network	O
is	O
composed	O
of	O
multiple	Method
layers	Method
of	Method
convolution	Method
and	Method
de	Method
-	Method
convolution	Method
operators	Method
,	O
learning	O
end	O
-	O
to	O
-	O
end	O
mappings	O
from	O
corrupted	O
images	O
to	O
the	O
original	O
ones	O
.	O

The	O
convolutional	Method
layers	Method
capture	O
the	O
abstraction	O
of	O
image	O
contents	O
while	O
eliminating	O
corruptions	O
.	O

Deconvolutional	Method
layers	Method
have	O
the	O
capability	O
to	O
upsample	O
the	O
feature	O
maps	O
and	O
recover	O
the	O
image	O
details	O
.	O

To	O
deal	O
with	O
the	O
problem	O
that	O
deeper	Method
networks	Method
tend	O
to	O
be	O
more	O
difficult	O
to	O
train	O
,	O
we	O
propose	O
to	O
symmetrically	O
link	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
with	O
skip	O
-	O
layer	O
connections	O
,	O
with	O
which	O
the	O
training	O
converges	O
much	O
faster	O
and	O
attains	O
better	O
results	O
.	O

The	O
skip	O
connections	O
from	O
convolutional	Method
layers	Method
to	O
their	O
mirrored	O
corresponding	O
deconvolutional	Method
layers	Method
exhibit	O
two	O
main	O
advantages	O
.	O

First	O
,	O
they	O
allow	O
the	O
signal	O
to	O
be	O
back	O
-	O
propagated	O
to	O
bottom	O
layers	O
directly	O
,	O
and	O
thus	O
tackles	O
the	O
problem	O
of	O
gradient	Task
vanishing	Task
,	O
making	O
training	O
deep	Method
networks	Method
easier	O
and	O
achieving	O
restoration	Task
performance	O
gains	O
consequently	O
.	O

Second	O
,	O
these	O
skip	O
connections	O
pass	O
image	O
details	O
from	O
convolutional	Method
layers	Method
to	O
deconvolutional	Method
layers	Method
,	O
which	O
is	O
beneficial	O
in	O
recovering	O
the	O
clean	O
image	O
.	O

Significantly	O
,	O
with	O
the	O
large	O
capacity	O
,	O
we	O
show	O
it	O
is	O
possible	O
to	O
cope	O
with	O
different	O
levels	O
of	O
corruptions	O
using	O
a	O
single	O
model	O
.	O

Using	O
the	O
same	O
framework	O
,	O
we	O
train	O
models	O
on	O
tasks	O
of	O
image	Task
denoising	Task
,	O
super	Task
resolution	Task
removing	Task
JPEG	Task
compression	Task
artifacts	Task
,	O
non	Task
-	Task
blind	Task
image	Task
deblurring	Task
and	O
image	Task
inpainting	Task
.	O

Our	O
experiment	O
results	O
on	O
benchmark	O
datasets	O
show	O
that	O
our	O
network	O
can	O
achieve	O
best	O
reported	O
performance	O
on	O
all	O
of	O
the	O
four	O
tasks	O
,	O
and	O
set	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

.	O

/	O
figs	O
/	O
Image	Task
restoration	Task
,	O
auto	Method
-	Method
encoder	Method
,	O
convolutional	Method
/	Method
de	Method
-	Method
convolutional	Method
networks	Method
,	O
skip	Method
connection	Method
,	O
image	Task
denoising	Task
,	O
super	Task
resolution	Task
,	O
image	Task
inpainting	Task
.	O

section	O
:	O
Introduction	O
Image	Task
restoration	Task
is	O
a	O
classical	O
problem	O
in	O
low	Task
-	Task
level	Task
vision	Task
,	O
which	O
has	O
been	O
widely	O
studies	O
in	O
the	O
literature	O
.	O

Yet	O
,	O
it	O
remains	O
an	O
active	O
research	O
topic	O
and	O
provides	O
a	O
test	O
bed	O
for	O
many	O
image	Method
modeling	Method
techniques	Method
.	O

Generally	O
speaking	O
,	O
image	Task
restoration	Task
is	O
the	O
operation	O
of	O
taking	O
a	O
corrupted	O
image	O
and	O
estimating	O
the	O
original	O
image	O
,	O
which	O
is	O
known	O
to	O
be	O
an	O
ill	Task
-	Task
posed	Task
inverse	Task
problem	Task
.	O

A	O
corrupted	O
image	O
can	O
be	O
represented	O
as	O
where	O
is	O
the	O
clean	O
version	O
of	O
;	O
is	O
the	O
degradation	O
function	O
and	O
is	O
the	O
additive	O
noise	O
.	O

By	O
accommodating	O
different	O
types	O
of	O
degradation	O
operators	O
and	O
noise	O
distributions	O
,	O
the	O
same	O
mathematical	Method
model	Method
applies	O
to	O
most	O
low	Task
-	Task
level	Task
imaging	Task
problems	Task
such	O
as	O
image	Task
denoising	Task
,	O
super	Task
-	Task
resolution	Task
,	O
inpainting	Task
and	O
recovering	Task
raw	Task
images	Task
from	O
compressed	O
images	O
.	O

In	O
the	O
past	O
decades	O
,	O
extensive	O
studies	O
have	O
been	O
carried	O
out	O
to	O
develop	O
various	O
of	O
image	Method
restoration	Method
methods	Method
.	O

Recently	O
,	O
deep	Method
neural	Method
networks	Method
(	O
DNNs	Method
)	O
have	O
shown	O
their	O
superior	O
performance	O
in	O
image	Task
processing	Task
and	O
computer	Task
vision	Task
tasks	Task
,	O
ranging	O
from	O
high	Task
-	Task
level	Task
recognition	Task
,	O
semantic	Task
segmentation	Task
to	O
low	Task
-	Task
level	Task
denoising	Task
and	O
super	Task
-	Task
resolution	Task
.	O

One	O
of	O
the	O
early	O
deep	Method
learning	Method
models	Method
which	O
has	O
been	O
used	O
for	O
image	Task
denoising	Task
is	O
the	O
Stacked	Method
Denoising	Method
Auto	Method
-	Method
encoders	Method
(	O
SdA	Method
)	O
.	O

It	O
is	O
an	O
extension	O
of	O
the	O
stacked	Method
auto	Method
-	Method
encoder	Method
and	O
was	O
originally	O
designed	O
for	O
unsupervised	Task
feature	Task
learning	Task
.	O

Denoising	Method
auto	Method
-	Method
encoders	Method
can	O
be	O
stacked	O
to	O
form	O
a	O
deep	Method
network	Method
by	O
feeding	O
output	O
of	O
the	O
previous	O
layer	O
to	O
the	O
current	O
layer	O
as	O
input	O
.	O

Jain	O
and	O
Seung	O
proposed	O
to	O
use	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNN	Method
)	O
to	O
denoise	O
natural	O
images	O
.	O

Their	O
framework	O
is	O
the	O
same	O
as	O
the	O
recent	O
Fully	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
FCN	Method
)	O
for	O
semantic	Task
image	Task
segmentation	Task
and	O
other	O
tasks	O
such	O
as	O
super	Task
-	Task
resolution	Task
,	O
although	O
their	O
network	O
is	O
not	O
as	O
deep	O
as	O
today	O
’s	O
models	O
.	O

Their	O
network	O
accepts	O
an	O
image	O
as	O
the	O
input	O
and	O
produces	O
an	O
entire	O
image	O
as	O
the	O
output	O
through	O
four	O
hidden	Method
layers	Method
of	Method
convolutional	Method
filters	Method
.	O

The	O
weights	O
are	O
learned	O
by	O
minimizing	O
the	O
difference	O
between	O
the	O
output	O
and	O
the	O
clean	O
image	O
.	O

By	O
observing	O
recent	O
superior	O
performance	O
of	O
CNN	Method
on	O
image	Task
processing	Task
tasks	Task
,	O
here	O
we	O
propose	O
a	O
very	O
deep	Method
fully	Method
convolutional	Method
CNN	Method
-	Method
based	Method
framework	Method
for	O
image	Task
restoration	Task
.	O

The	O
input	O
of	O
our	O
framework	O
is	O
a	O
corrupted	O
image	O
,	O
and	O
the	O
output	O
is	O
its	O
clean	O
version	O
.	O

We	O
observe	O
that	O
it	O
is	O
beneficial	O
to	O
train	O
a	O
very	O
deep	Method
model	Method
for	O
low	Task
-	Task
level	Task
tasks	Task
like	O
denoising	Task
,	O
super	Task
-	Task
resolution	Task
and	O
JPEG	Task
deblocking	Task
.	O

Our	O
network	O
is	O
much	O
deeper	O
compared	O
to	O
that	O
in	O
and	O
recent	O
low	Method
-	Method
level	Method
image	Method
processing	Method
models	Method
such	O
as	O
.	O

Instead	O
of	O
using	O
image	O
priors	O
,	O
the	O
proposed	O
framework	O
learns	O
fully	Method
convolutional	Method
and	Method
deconvolutional	Method
mappings	Method
from	O
corrupted	O
images	O
to	O
the	O
clean	O
ones	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
.	O

The	O
network	O
is	O
composed	O
of	O
multiple	Method
layers	Method
of	Method
convolution	Method
and	Method
deconvolution	Method
operators	Method
.	O

As	O
deeper	Method
networks	Method
tend	O
to	O
be	O
more	O
difficult	O
to	O
train	O
,	O
we	O
further	O
propose	O
to	O
symmetrically	O
link	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
with	O
multiple	O
skip	O
-	O
layer	O
connections	O
,	O
with	O
which	O
the	O
training	O
converges	O
much	O
faster	O
and	O
better	O
performance	O
is	O
achieved	O
.	O

Our	O
main	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
.	O

We	O
propose	O
a	O
very	O
deep	Method
network	Method
architecture	Method
for	O
image	Task
restoration	Task
.	O

The	O
network	O
consists	O
of	O
a	O
chain	Method
of	Method
symmetric	Method
convolutional	Method
layers	Method
and	O
deconvolutional	Method
layers	Method
.	O

The	O
convolutional	Method
layers	Method
act	O
as	O
the	O
feature	Method
extractor	Method
which	O
encode	O
the	O
primary	O
components	O
of	O
image	O
contents	O
while	O
eliminating	O
the	O
corruptions	O
.	O

The	O
deconvolutional	Method
layers	Method
then	O
decode	O
the	O
image	Task
abstraction	Task
to	O
recover	O
the	O
image	O
content	O
details	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
proposed	O
framework	O
is	O
the	O
first	O
attempt	O
to	O
used	O
both	O
convolution	Method
and	Method
deconvolution	Method
for	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
.	O

To	O
better	O
train	O
the	O
deep	Method
network	Method
,	O
we	O
propose	O
to	O
add	O
skip	O
connections	O
between	O
corresponding	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
.	O

These	O
shortcuts	O
divide	O
the	O
network	O
into	O
several	O
blocks	O
.	O

These	O
skip	O
connections	O
help	O
to	O
back	O
-	O
propagate	O
the	O
gradients	O
to	O
bottom	O
layers	O
and	O
pass	O
image	O
details	O
to	O
the	O
top	O
layers	O
.	O

These	O
two	O
characteristics	O
make	O
training	O
of	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
mapping	Task
from	O
corrupted	O
image	O
to	O
the	O
clean	O
one	O
easier	O
and	O
more	O
effective	O
,	O
and	O
thus	O
achieve	O
performance	O
improvement	O
while	O
the	O
network	O
going	O
deeper	O
.	O

We	O
apply	O
the	O
same	O
network	O
for	O
tasks	O
such	O
as	O
image	Task
denoising	Task
,	O
image	Task
super	Task
-	Task
resolution	Task
,	O
JPEG	Task
deblocking	Task
,	O
non	Task
-	Task
blind	Task
image	Task
deblurring	Task
and	O
image	Task
inpainting	Task
.	O

Experiments	O
on	O
a	O
few	O
widely	O
-	O
used	O
benchmark	O
datasets	O
demonstrate	O
the	O
advantages	O
of	O
our	O
network	O
over	O
other	O
recent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Moreover	O
,	O
relying	O
on	O
the	O
large	O
capacity	O
and	O
fitting	Metric
ability	Metric
,	O
our	O
network	O
can	O
be	O
trained	O
to	O
obtain	O
good	O
restoration	Task
performance	O
on	O
different	O
levels	O
of	O
corruption	O
even	O
using	O
a	O
single	O
model	O
.	O

The	O
remaining	O
content	O
is	O
organized	O
as	O
follows	O
.	O

We	O
provide	O
a	O
brief	O
review	O
of	O
related	O
work	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
present	O
the	O
architecture	O
of	O
the	O
proposed	O
network	O
,	O
as	O
well	O
as	O
training	O
,	O
testing	O
details	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
discuss	O
some	O
relevant	O
issues	O
.	O

Experimental	O
results	O
and	O
analysis	O
are	O
provided	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
work	O
In	O
the	O
past	O
decades	O
,	O
extensive	O
studies	O
have	O
been	O
conducted	O
to	O
develop	O
various	O
image	Method
restoration	Method
methods	Method
.	O

See	O
detailed	O
reviews	O
in	O
a	O
survey	O
.	O

Traditional	O
methods	O
such	O
as	O
the	O
BM3D	Method
algorithm	Method
and	O
dictionary	Method
learning	Method
based	Method
methods	Method
have	O
shown	O
very	O
promising	O
performance	O
on	O
image	Task
restoration	Task
topics	Task
such	O
as	O
image	Task
denoising	Task
and	O
super	Task
-	Task
resolution	Task
.	O

Due	O
to	O
the	O
ill	Task
-	Task
posed	Task
nature	Task
of	O
image	Task
restoration	Task
,	O
image	O
prior	O
knowledge	O
formulated	O
as	O
regularization	Method
techniques	Method
are	O
widely	O
used	O
.	O

Classic	O
regularization	Method
models	Method
,	O
such	O
as	O
total	Method
variation	Method
,	O
are	O
effective	O
in	O
removing	Task
noise	Task
artifacts	Task
,	O
but	O
also	O
tend	O
to	O
over	O
-	O
smooth	O
the	O
images	O
.	O

As	O
an	O
alternative	O
,	O
sparse	Method
representation	Method
based	Method
prior	Method
modeling	Method
is	O
popular	O
,	O
too	O
.	O

Mathematically	O
,	O
the	O
sparse	Method
representation	Method
model	Method
assumes	O
that	O
a	O
data	O
point	O
can	O
be	O
linearly	O
reconstructed	O
by	O
an	O
over	O
-	O
completed	O
dictionary	O
,	O
and	O
most	O
of	O
the	O
coefficients	O
are	O
zero	O
or	O
close	O
to	O
zero	O
.	O

An	O
active	O
(	O
and	O
probably	O
more	O
promising	O
)	O
category	O
for	O
image	Task
restoration	Task
is	O
the	O
neural	Method
network	Method
based	Method
methods	Method
.	O

The	O
most	O
significant	O
difference	O
between	O
neural	Method
network	Method
methods	Method
and	O
other	O
methods	O
is	O
that	O
they	O
typically	O
learn	O
parameters	O
for	O
image	Task
restoration	Task
directly	O
from	O
training	O
data	O
(	O
e.g.	O
,	O
pairs	O
of	O
clean	O
and	O
corrupted	O
images	O
)	O
rather	O
than	O
relying	O
on	O
pre	O
-	O
defined	O
image	O
priors	O
.	O

Stacked	Method
denoising	Method
auto	Method
-	Method
encoder	Method
is	O
one	O
of	O
the	O
most	O
well	O
-	O
known	O
deep	Method
neural	Method
network	Method
models	Method
which	O
can	O
be	O
used	O
for	O
image	Task
denoising	Task
.	O

Unsupervised	Task
pre	Task
-	Task
training	Task
,	O
which	O
minimizes	O
the	O
reconstruction	Metric
error	Metric
with	O
respect	O
to	O
inputs	O
,	O
is	O
done	O
for	O
one	O
layer	O
at	O
a	O
time	O
.	O

Once	O
all	O
layers	O
are	O
pre	O
-	O
trained	O
,	O
the	O
network	O
goes	O
through	O
a	O
fine	Method
-	Method
tuning	Method
stage	Method
.	O

Xie	O
et	O
al	O
.	O

combined	O
sparse	Method
coding	Method
and	O
deep	Method
networks	Method
pre	O
-	O
trained	O
with	O
denoising	Method
auto	Method
-	Method
encoder	Method
for	O
low	Task
-	Task
level	Task
vision	Task
tasks	Task
such	O
as	O
image	Task
denoising	Task
and	O
inpainting	Task
.	O

The	O
main	O
idea	O
is	O
that	O
the	O
sparsity	O
-	O
inducing	O
term	O
for	O
regularization	Task
is	O
proposed	O
for	O
improved	O
performance	O
.	O

Deep	Method
network	Method
cascade	Method
(	Method
DNC	Method
)	Method
is	O
a	O
cascade	Method
of	Method
multiple	Method
stacked	Method
collaborative	Method
local	Method
auto	Method
-	Method
encoders	Method
for	O
image	Task
super	Task
-	Task
resolution	Task
.	O

High	O
frequency	O
texture	O
enhanced	O
image	O
patches	O
are	O
fed	O
into	O
the	O
network	O
to	O
suppress	O
the	O
noises	O
and	O
collaborate	O
the	O
compatibility	O
of	O
the	O
overlapping	O
patches	O
.	O

Other	O
neural	Method
network	Method
based	Method
image	Method
restoration	Method
methods	Method
using	O
networks	Method
such	O
as	O
multi	Method
-	Method
layer	Method
perceptron	Method
.	O

Early	O
works	O
,	O
such	O
as	O
a	O
multi	Method
-	Method
layer	Method
perceptron	Method
with	O
a	O
multilevel	Method
sigmoidal	Method
function	Method
,	O
have	O
been	O
proposed	O
and	O
proved	O
to	O
be	O
effective	O
in	O
image	Task
restoration	Task
tasks	Task
.	O

Burger	O
et	O
al	O
.	O

presented	O
a	O
patch	Method
-	Method
based	Method
algorithm	Method
learned	O
on	O
a	O
large	O
dataset	O
with	O
a	O
plain	O
multi	Method
-	Method
layer	Method
perceptron	Method
and	O
is	O
able	O
to	O
compete	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
traditional	O
image	Task
denoising	Task
methods	O
such	O
as	O
BM3D.	Method
They	O
also	O
concluded	O
that	O
with	O
large	Method
networks	Method
,	O
large	O
training	O
data	O
,	O
neural	Method
networks	Method
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
image	Task
denoising	Task
performance	O
,	O
which	O
is	O
confirmed	O
in	O
the	O
work	O
here	O
.	O

Compared	O
to	O
auto	Method
-	Method
encodes	Method
and	O
multilayer	Method
perceptron	Method
,	O
it	O
seems	O
that	O
convolutional	Method
neural	Method
networks	Method
have	O
achieved	O
even	O
more	O
significant	O
success	O
in	O
the	O
field	O
of	O
image	Task
restoration	Task
.	O

Jain	O
and	O
Seung	O
proposed	O
fully	Method
convolutional	Method
CNN	Method
for	O
denoising	Task
.	O

The	O
network	O
is	O
trained	O
by	O
minimizing	O
the	O
loss	O
between	O
a	O
clean	O
image	O
and	O
its	O
corrupted	O
version	O
by	O
adding	O
noises	O
on	O
it	O
.	O

They	O
found	O
that	O
CNN	Method
works	O
well	O
on	O
both	O
blind	O
and	O
non	Task
-	Task
blind	Task
image	Task
denoising	Task
,	O
providing	O
comparable	O
or	O
even	O
superior	O
performance	O
to	O
wavelet	Method
and	Method
Markov	Method
Random	Method
Field	Method
(	Method
MRF	Method
)	Method
methods	Method
.	O

Recently	O
,	O
Dong	O
et	O
al	O
.	O

proposed	O
to	O
directly	O
learn	O
an	O
end	Task
-	Task
to	Task
-	Task
end	Task
mapping	Task
between	O
the	O
low	O
/	O
high	O
-	O
resolution	O
images	O
for	O
image	Task
super	Task
-	Task
resolution	Task
.	O

They	O
observed	O
that	O
convolutional	Method
neural	Method
networks	Method
are	O
enseentially	O
related	O
to	O
sparse	Method
coding	Method
based	Method
methods	Method
,	O
i.e.	O
,	O
the	O
three	O
layers	O
in	O
their	O
network	O
can	O
be	O
viewed	O
as	O
patch	Method
representation	Method
extractor	Method
,	O
non	Method
-	Method
linear	Method
mapping	Method
and	O
image	Method
reconstructor	Method
.	O

They	O
also	O
proposed	O
variant	Method
networks	Method
for	O
other	O
image	Task
restoration	Task
tasks	Task
such	O
as	O
JPEG	Task
debloking	Task
.	O

Wang	O
et	O
al	O
.	O

argued	O
that	O
domain	O
expertise	O
represented	O
by	O
the	O
conventional	O
sparse	Method
coding	Method
is	O
still	O
valuable	O
and	O
can	O
be	O
combined	O
to	O
achieve	O
further	O
improved	O
results	O
in	O
image	Task
super	Task
-	Task
resolution	Task
.	O

Instead	O
of	O
training	O
with	O
different	O
levels	O
of	O
scaling	O
factors	O
,	O
they	O
proposed	O
to	O
use	O
a	O
cascade	Method
structure	Method
to	O
repeatedly	O
enlarge	O
the	O
low	O
-	O
resolution	O
image	O
by	O
a	O
fixed	O
scale	O
until	O
reaching	O
a	O
desired	O
size	O
.	O

In	O
general	O
,	O
DNN	Method
-	Method
based	Method
methods	Method
learn	O
restoration	O
parameters	O
directly	O
from	O
data	O
,	O
which	O
tends	O
to	O
been	O
more	O
effective	O
in	O
real	O
-	O
world	Task
image	Task
restoration	Task
applications	Task
.	O

section	O
:	O
Very	O
deep	Method
convolutional	Method
auto	Method
-	Method
encoder	Method
for	O
image	Task
restoration	Task
The	O
proposed	O
framework	O
mainly	O
contains	O
a	O
chain	Method
of	Method
convolutional	Method
layers	Method
and	O
symmetric	Method
deconvolutional	Method
layers	Method
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Skip	O
connections	O
are	O
connected	O
symmetrically	O
from	O
convolutional	Method
layers	Method
to	O
deconvolutional	Method
layers	Method
.	O

We	O
term	O
our	O
method	O
“	O
RED	Method
-	Method
Net”—very	Method
deep	Method
Residual	Method
Encoder	Method
-	Method
Decoder	Method
Networks	Method
.	O

subsection	O
:	O
Architecture	O
The	O
framework	O
is	O
fully	O
convolutional	Method
(	O
and	O
deconvolutional	Method
.	O

Deconvolution	Method
is	O
essentially	O
unsampling	Method
convolution	Method
)	O
.	O

Rectification	Method
layers	Method
are	O
added	O
after	O
each	O
convolution	Method
and	Method
deconvolution	Method
.	O

For	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
problems	Task
,	O
we	O
use	O
neither	O
pooling	Method
nor	O
unpooling	Method
in	O
the	O
network	O
as	O
usually	O
pooling	Method
discards	O
useful	O
image	O
details	O
that	O
are	O
essential	O
for	O
these	O
tasks	O
.	O

It	O
is	O
worth	O
mentioning	O
that	O
since	O
the	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
are	O
symmetric	O
,	O
the	O
network	O
is	O
essentially	O
pixel	Method
-	Method
wise	Method
prediction	Method
,	O
thus	O
the	O
size	O
of	O
input	O
image	O
can	O
be	O
arbitrary	O
.	O

The	O
input	O
and	O
output	O
of	O
the	O
network	O
are	O
images	O
of	O
the	O
same	O
size	O
,	O
where	O
,	O
and	O
are	O
width	O
,	O
height	O
and	O
number	O
of	O
channels	O
.	O

Our	O
main	O
idea	O
is	O
that	O
the	O
convolutional	Method
layers	Method
act	O
as	O
a	O
feature	Method
extractor	Method
,	O
which	O
preserve	O
the	O
primary	O
components	O
of	O
objects	O
in	O
the	O
image	O
and	O
meanwhile	O
eliminating	O
the	O
corruptions	O
.	O

After	O
forwarding	O
through	O
the	O
convolutional	Method
layers	Method
,	O
the	O
corrupted	O
input	O
image	O
is	O
converted	O
into	O
a	O
“	O
clean	O
”	O
one	O
.	O

The	O
subtle	O
details	O
of	O
the	O
image	O
contents	O
may	O
be	O
lost	O
during	O
this	O
process	O
.	O

The	O
deconvolutional	Method
layers	Method
are	O
then	O
combined	O
to	O
recover	O
the	O
details	O
of	O
image	O
contents	O
.	O

The	O
output	O
of	O
the	O
deconvolutional	Method
layers	Method
is	O
the	O
recovered	O
clean	O
version	O
of	O
the	O
input	O
image	O
.	O

Moreover	O
,	O
we	O
add	O
skip	O
connections	O
from	O
a	O
convolutional	Method
layer	Method
to	O
its	O
corresponding	O
mirrored	Method
deconvolutional	Method
layer	Method
.	O

The	O
passed	O
convolutional	O
feature	O
maps	O
are	O
summed	O
to	O
the	O
deconvolutional	O
feature	O
maps	O
element	O
-	O
wise	O
,	O
and	O
passed	O
to	O
the	O
next	O
layer	O
after	O
rectification	Task
.	O

Deriving	O
from	O
the	O
above	O
architecture	O
,	O
we	O
have	O
used	O
two	O
networksvin	O
our	O
experiments	O
,	O
which	O
are	O
of	O
20	O
layers	O
and	O
30	O
layers	O
respectively	O
,	O
for	O
image	Task
denoising	Task
,	O
image	Task
super	Task
-	Task
resolution	Task
,	O
JPEG	Task
deblocking	Task
and	O
image	Task
inpainting	Task
.	O

subsection	O
:	O
Deconvolution	Method
decoder	Method
Architectures	Method
combining	O
layers	Method
of	Method
convolution	Method
and	Method
deconvolution	Method
have	O
been	O
proposed	O
for	O
semantic	Task
segmentation	Task
recently	O
.	O

In	O
contrast	O
to	O
convolutional	Method
layers	Method
,	O
in	O
which	O
multiple	O
input	O
activations	O
within	O
a	O
filter	O
window	O
are	O
fused	O
to	O
output	O
a	O
single	O
activation	O
,	O
deconvolutional	Method
layers	Method
associate	O
a	O
single	O
input	O
activation	O
with	O
multiple	O
outputs	O
.	O

Deconvolution	Method
is	O
usually	O
used	O
as	O
learnable	Method
up	Method
-	Method
sampling	Method
layers	Method
.	O

In	O
our	O
network	O
,	O
the	O
convolutional	Method
layers	Method
successively	O
down	O
-	O
sample	O
the	O
input	O
image	O
content	O
into	O
a	O
small	O
size	O
abstraction	O
.	O

Deconvolutional	Method
layers	Method
then	O
up	O
-	O
sample	O
the	O
abstraction	O
back	O
into	O
its	O
original	O
resolution	O
.	O

Besides	O
the	O
use	O
of	O
skip	O
connections	O
,	O
a	O
main	O
difference	O
between	O
our	O
model	O
and	O
is	O
that	O
our	O
network	O
is	O
fully	Method
convolutional	Method
and	Method
deconvolutional	Method
,	O
i.e.	O
,	O
without	O
pooling	Method
and	Method
un	Method
-	Method
pooling	Method
.	O

The	O
reason	O
is	O
that	O
for	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
,	O
the	O
aim	O
is	O
to	O
eliminate	O
low	O
level	O
corruption	O
while	O
preserving	O
image	O
details	O
instead	O
of	O
learning	O
image	O
abstractions	O
.	O

Different	O
from	O
high	Task
-	Task
level	Task
applications	Task
such	O
as	O
segmentation	Task
or	O
recognition	Task
,	O
pooling	Task
typically	O
eliminates	O
the	O
abundant	O
image	O
details	O
and	O
can	O
deteriorate	O
restoration	Task
performance	O
.	O

One	O
can	O
simply	O
replace	O
deconvolution	Method
with	O
convolution	Method
,	O
which	O
results	O
in	O
an	O
architecture	O
that	O
is	O
very	O
similar	O
to	O
recently	O
proposed	O
very	O
deep	Method
fully	Method
convolutional	Method
neural	Method
networks	Method
.	O

However	O
,	O
there	O
exist	O
essential	O
differences	O
between	O
a	O
fully	Method
convolution	Method
model	Method
and	O
our	O
model	O
.	O

Take	O
image	Task
denoising	Task
as	O
an	O
example	O
.	O

We	O
compare	O
the	O
5	Method
-	Method
layer	Method
and	Method
10	Method
-	Method
layer	Method
fully	Method
convolutional	Method
network	Method
with	O
our	O
network	O
(	O
combining	O
convolution	Method
and	Method
deconvolution	Method
,	O
but	O
without	O
skip	O
connection	O
)	O
.	O

For	O
fully	Method
convolutional	Method
networks	Method
,	O
we	O
use	O
padding	O
or	O
up	O
-	O
sampling	O
the	O
input	O
to	O
make	O
the	O
input	O
and	O
output	O
be	O
of	O
the	O
same	O
size	O
.	O

For	O
our	O
network	O
,	O
the	O
first	O
5	O
layers	O
are	O
convolutional	Method
and	O
the	O
second	O
5	O
layers	O
are	O
deconvolutional	Method
.	O

All	O
the	O
other	O
parameters	O
for	O
training	O
are	O
identical	O
,	O
i.e.	O
,	O
trained	O
with	O
SGD	Method
and	O
learning	Metric
rate	Metric
of	O
,	O
noise	Metric
level	Metric
.	O

The	O
Peak	Metric
Signal	Metric
-	Metric
to	Metric
-	Metric
Noise	Metric
Ratio	Metric
(	O
PSNR	Metric
)	O
on	O
the	O
validation	O
set	O
is	O
reported	O
,	O
which	O
shows	O
that	O
using	O
deconvolution	Method
works	O
better	O
than	O
the	O
fully	Method
convolutional	Method
counterpart	Method
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Furthermore	O
,	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
visualize	O
some	O
results	O
that	O
are	O
outputs	O
of	O
layer	O
2	O
,	O
5	O
,	O
8	O
and	O
10	O
from	O
the	O
10	Method
-	Method
layer	Method
fully	Method
convolutional	Method
network	Method
and	O
ours	O
.	O

In	O
the	O
fully	Task
convolution	Task
case	Task
,	O
the	O
noise	O
is	O
eliminated	O
step	O
by	O
step	O
,	O
i.e.	O
,	O
the	O
noise	O
level	O
is	O
reduced	O
after	O
each	O
layer	O
.	O

During	O
this	O
process	O
,	O
the	O
details	O
of	O
the	O
image	O
content	O
may	O
be	O
lost	O
.	O

Nevertheless	O
,	O
in	O
our	O
network	O
,	O
convolution	Method
preserves	O
the	O
primary	O
image	O
content	O
.	O

Then	O
deconvolution	Method
is	O
used	O
to	O
compensate	O
the	O
details	O
.	O

subsection	O
:	O
Skip	O
connections	O
An	O
intuitive	O
question	O
is	O
that	O
,	O
is	O
a	O
network	Method
with	Method
deconvolution	Method
able	O
to	O
recover	O
image	O
details	O
from	O
the	O
image	O
abstraction	O
only	O
?	O
We	O
find	O
that	O
in	O
shallow	Method
networks	Method
with	O
only	O
a	O
few	O
layers	O
of	O
convolution	Method
layers	Method
,	O
deconvolution	Method
is	O
able	O
to	O
recover	O
the	O
details	O
.	O

However	O
,	O
when	O
the	O
network	O
goes	O
deeper	O
or	O
using	O
operations	O
such	O
as	O
max	Method
pooling	Method
,	O
even	O
with	O
deconvolution	Method
layers	Method
,	O
it	O
does	O
not	O
work	O
that	O
well	O
,	O
possibly	O
because	O
too	O
much	O
details	O
are	O
already	O
lost	O
in	O
the	O
convolution	Method
and	Method
pooling	Method
.	O

The	O
second	O
question	O
is	O
that	O
,	O
when	O
our	O
network	O
goes	O
deeper	O
,	O
does	O
it	O
achieve	O
performance	O
gain	O
?	O
We	O
observe	O
that	O
deeper	Method
networks	Method
in	O
image	Task
restoration	Task
tasks	Task
tend	O
to	O
easily	O
suffer	O
from	O
performance	O
degradation	O
.	O

The	O
reason	O
may	O
be	O
two	O
folds	O
.	O

First	O
of	O
all	O
,	O
with	O
more	O
layers	O
of	O
convolution	Method
,	O
a	O
significant	O
amount	O
of	O
image	O
details	O
could	O
be	O
lost	O
or	O
corrupted	O
.	O

Given	O
only	O
the	O
image	Task
abstraction	Task
,	O
recovering	O
its	O
details	O
is	O
an	O
under	O
-	O
determined	O
problem	O
.	O

Secondly	O
,	O
in	O
terms	O
of	O
optimization	Task
,	O
deep	Method
networks	Method
often	O
suffer	O
from	O
gradients	O
vanishing	O
and	O
become	O
much	O
harder	O
to	O
train	O
—	O
a	O
problem	O
that	O
is	O
well	O
addressed	O
in	O
the	O
literature	O
of	O
neural	Method
networks	Method
.	O

To	O
address	O
the	O
above	O
two	O
problems	O
,	O
inspired	O
by	O
highway	Method
networks	Method
and	O
deep	Method
residual	Method
networks	Method
,	O
we	O
add	O
skip	O
connections	O
between	O
two	O
corresponding	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
building	O
block	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

There	O
are	O
two	O
reasons	O
for	O
using	O
such	O
connections	O
.	O

First	O
,	O
when	O
the	O
network	O
goes	O
deeper	O
,	O
as	O
mentioned	O
above	O
,	O
image	O
details	O
can	O
be	O
lost	O
,	O
making	O
deconvolution	Method
weaker	O
in	O
recovering	O
them	O
.	O

However	O
,	O
the	O
feature	O
maps	O
passed	O
by	O
skip	Method
connections	Method
carry	O
much	O
image	O
detail	O
,	O
which	O
helps	O
deconvolution	Task
to	O
recover	O
an	O
improved	O
clean	O
version	O
of	O
the	O
image	O
.	O

Second	O
,	O
the	O
skip	O
connections	O
also	O
achieve	O
benefits	O
on	O
back	O
-	O
propagating	O
the	O
gradient	O
to	O
bottom	O
layers	O
,	O
which	O
makes	O
training	O
deeper	Method
network	Method
much	O
easier	O
as	O
observed	O
in	O
and	O
.	O

Note	O
that	O
our	O
skip	Method
layer	Method
connections	Method
are	O
very	O
different	O
from	O
the	O
ones	O
proposed	O
in	O
and	O
,	O
where	O
the	O
only	O
concern	O
is	O
on	O
the	O
optimization	Task
side	Task
.	O

In	O
our	O
case	O
,	O
we	O
want	O
to	O
pass	O
information	O
of	O
the	O
convolutional	O
feature	O
maps	O
to	O
the	O
corresponding	O
deconvolutional	Method
layers	Method
.	O

The	O
very	O
deep	Method
highway	Method
networks	Method
are	O
essentially	O
feedforward	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTMs	Method
)	O
with	O
forget	O
gates	O
,	O
and	O
the	O
CNN	Method
layers	Method
of	O
deep	Method
residual	Method
network	Method
are	O
feedforward	O
LSTMs	Method
without	O
gates	O
.	O

Note	O
that	O
our	O
networks	O
are	O
in	O
general	O
not	O
in	O
the	O
format	O
of	O
standard	O
feedforward	O
LSTMs	Method
.	O

Instead	O
of	O
directly	O
learning	O
the	O
mappings	O
from	O
the	O
input	O
to	O
the	O
output	O
,	O
we	O
would	O
like	O
the	O
network	O
to	O
fit	O
the	O
residual	O
of	O
the	O
problem	O
,	O
which	O
is	O
denoted	O
as	O
.	O

Such	O
a	O
learning	Method
strategy	Method
is	O
applied	O
to	O
inner	O
blocks	O
of	O
the	O
encoding	Method
-	Method
decoding	Method
network	Method
to	O
make	O
training	Task
more	O
effective	O
.	O

Skip	O
connections	O
are	O
passed	O
every	O
two	O
convolutional	Method
layers	Method
to	O
their	O
mirrored	Method
deconvolutional	Method
layers	Method
.	O

Other	O
configurations	O
are	O
possible	O
and	O
our	O
experiments	O
show	O
that	O
this	O
configuration	O
already	O
works	O
very	O
well	O
.	O

Using	O
such	O
shortcuts	O
makes	O
the	O
network	O
easier	O
to	O
be	O
trained	O
and	O
gains	O
restoration	Task
performance	O
by	O
increasing	O
the	O
network	O
depth	O
.	O

subsection	O
:	O
Training	O
In	O
general	O
,	O
there	O
are	O
three	O
types	O
of	O
layers	O
in	O
our	O
network	O
:	O
convolution	Method
,	O
deconvolution	Method
and	O
element	Method
-	Method
wise	Method
sum	Method
.	O

Each	O
layer	O
is	O
followed	O
by	O
a	O
Rectified	Method
Linear	Method
Unit	Method
(	O
ReLU	Method
)	O
.	O

Let	O
be	O
the	O
input	O
,	O
the	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
are	O
expressed	O
as	O
:	O
where	O
and	O
represent	O
the	O
filters	O
and	O
biases	O
,	O
and	O
denotes	O
either	O
convolution	Method
or	O
deconvolution	Method
operation	Method
for	O
the	O
convenience	O
of	O
formulation	O
.	O

For	O
element	Method
-	Method
wise	Method
sum	Method
layer	Method
,	O
the	O
output	O
is	O
the	O
element	O
-	O
wise	O
sum	O
of	O
two	O
inputs	O
of	O
the	O
same	O
size	O
,	O
followed	O
by	O
the	O
ReLU	Method
activation	Method
:	O
Learning	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
mapping	Task
from	O
corrupted	O
images	O
to	O
clean	O
images	O
needs	O
to	O
estimate	O
the	O
weights	O
represented	O
by	O
the	O
convolutional	Method
and	Method
deconvolutional	Method
kernels	Method
.	O

Specifically	O
,	O
given	O
a	O
collection	O
of	O
training	O
sample	O
pairs	O
,	O
where	O
is	O
a	O
noisy	O
image	O
and	O
is	O
the	O
clean	O
version	O
as	O
the	O
groundtruth	O
.	O

We	O
minimize	O
the	O
following	O
Mean	Metric
Squared	Metric
Error	Metric
(	O
MSE	Metric
)	O
:	O
Traditionally	O
,	O
a	O
network	O
can	O
learn	O
the	O
mapping	O
from	O
the	O
corrupted	O
image	O
to	O
the	O
clean	O
version	O
directly	O
.	O

However	O
,	O
our	O
network	O
learns	O
for	O
the	O
additive	Task
corruption	Task
from	O
the	O
input	O
since	O
there	O
is	O
a	O
skip	O
connection	O
between	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
network	O
.	O

We	O
found	O
that	O
optimizing	O
for	O
the	O
corruption	Task
converges	O
better	O
than	O
optimizing	O
for	O
the	O
clean	O
image	O
.	O

In	O
the	O
extreme	O
case	O
,	O
if	O
the	O
input	O
is	O
a	O
clean	O
image	O
,	O
it	O
would	O
be	O
easier	O
to	O
push	O
the	O
network	O
to	O
be	O
zero	Method
mapping	Method
(	O
learning	O
the	O
corruption	O
)	O
than	O
to	O
fit	O
an	O
identity	Method
mapping	Method
(	O
learning	O
the	O
clean	O
image	O
)	O
with	O
a	O
stack	Method
of	Method
nonlinear	Method
layers	Method
.	O

We	O
implement	O
and	O
train	O
our	O
network	O
using	O
Caffe	Method
.	O

Empirically	O
,	O
we	O
find	O
that	O
using	O
Adam	Method
with	O
base	Metric
learning	Metric
rate	Metric
of	O
for	O
training	Task
converges	O
faster	O
than	O
traditional	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
.	O

The	O
base	O
learning	Metric
rate	Metric
for	O
all	O
layers	O
are	O
the	O
same	O
,	O
different	O
from	O
,	O
in	O
which	O
a	O
smaller	O
learning	Metric
rate	Metric
is	O
set	O
for	O
the	O
last	O
layer	O
.	O

This	O
is	O
not	O
necessary	O
in	O
our	O
network	O
.	O

Specifically	O
,	O
gradients	O
with	O
respect	O
to	O
the	O
parameters	O
of	O
th	O
layer	O
is	O
firstly	O
computed	O
as	O
:	O
Then	O
,	O
the	O
two	O
momentum	O
vectors	O
are	O
computed	O
as	O
:	O
The	O
update	Method
rule	Method
is	O
:	O
,	O
and	O
are	O
set	O
as	O
the	O
recommended	O
values	O
in	O
.	O

300	O
images	O
from	O
the	O
Berkeley	Material
Segmentation	Material
Dataset	Material
(	O
BSD	Material
)	O
are	O
used	O
to	O
generate	O
image	O
patches	O
as	O
the	O
training	O
set	O
for	O
each	O
image	Task
restoration	Task
task	Task
.	O

subsection	O
:	O
Testing	O
Although	O
trained	O
on	O
local	O
patches	O
,	O
our	O
network	O
can	O
perform	O
restoration	Task
on	O
images	O
of	O
arbitrary	O
sizes	O
.	O

Given	O
a	O
testing	O
image	O
,	O
one	O
can	O
simply	O
go	O
forward	O
through	O
the	O
network	O
,	O
which	O
is	O
already	O
able	O
to	O
outperform	O
existing	O
methods	O
.	O

To	O
achieve	O
even	O
better	O
results	O
,	O
we	O
propose	O
to	O
process	O
a	O
corrupted	O
image	O
on	O
multiple	O
orientations	O
.	O

Different	O
from	O
segmentation	Task
,	O
the	O
filter	Method
kernels	Method
in	O
our	O
network	O
only	O
eliminate	O
the	O
corruptions	O
,	O
which	O
is	O
usually	O
not	O
sensitive	O
to	O
the	O
orientation	O
of	O
image	O
contents	O
in	O
low	Task
level	Task
restoration	Task
tasks	Task
.	O

Therefore	O
,	O
we	O
can	O
rotate	O
and	O
mirror	O
flip	O
the	O
kernels	O
and	O
perform	O
forward	O
multiple	O
times	O
,	O
and	O
then	O
average	O
the	O
output	O
to	O
achieve	O
an	O
ensemble	O
of	O
multiple	O
tests	O
.	O

We	O
see	O
that	O
this	O
can	O
lead	O
to	O
slightly	O
better	O
performance	O
.	O

section	O
:	O
Discussions	O
subsection	O
:	O
Analysis	O
on	O
the	O
architecture	O
Assume	O
that	O
we	O
have	O
a	O
network	O
with	O
layers	O
,	O
and	O
skip	O
connections	O
are	O
passed	O
every	O
layer	O
in	O
the	O
first	O
half	O
of	O
the	O
network	O
.	O

For	O
the	O
convenience	O
of	O
presentation	O
,	O
we	O
denote	O
and	O
the	O
convolution	Method
and	Method
deconvolution	Method
operation	Method
in	O
each	O
layer	O
and	O
do	O
not	O
use	O
ReLU	Method
.	O

According	O
to	O
the	O
architecture	O
described	O
in	O
the	O
last	O
section	O
,	O
we	O
can	O
obtain	O
the	O
output	O
of	O
the	O
-	O
th	O
layer	O
as	O
follows	O
:	O
It	O
is	O
easy	O
to	O
observe	O
that	O
our	O
skip	O
connections	O
indicate	O
identity	Method
mapping	Method
.	O

The	O
output	O
of	O
the	O
network	O
is	O
:	O
Recursively	O
,	O
we	O
can	O
compute	O
more	O
specifically	O
as	O
follows	O
according	O
to	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
:	O
Since	O
can	O
be	O
expressed	O
as	O
,	O
we	O
convert	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
as	O
:	O
In	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
term	O
is	O
actually	O
the	O
output	O
of	O
the	O
given	O
network	O
without	O
skip	O
connections	O
.	O

The	O
difference	O
here	O
is	O
that	O
by	O
adopting	O
the	O
skip	O
connection	O
,	O
we	O
decode	O
each	O
feature	O
maps	O
in	O
the	O
first	O
half	Method
network	Method
and	O
integrate	O
them	O
to	O
the	O
output	O
.	O

The	O
most	O
significant	O
benefit	O
is	O
that	O
they	O
carry	O
important	O
image	O
details	O
,	O
which	O
helps	O
to	O
reconstruct	O
clean	O
image	O
.	O

Moreover	O
,	O
the	O
term	O
indicates	O
that	O
these	O
details	O
are	O
represented	O
at	O
different	O
levels	O
.	O

It	O
is	O
intuitive	O
to	O
see	O
the	O
following	O
fact	O
.	O

It	O
may	O
not	O
be	O
easy	O
to	O
tell	O
what	O
information	O
is	O
needed	O
for	O
reconstructing	O
clean	O
images	O
using	O
only	O
one	O
feature	Method
maps	Method
encoding	O
the	O
image	O
abstraction	O
;	O
but	O
much	O
easier	O
if	O
there	O
are	O
multiple	O
feature	Method
maps	Method
encoding	O
different	O
levels	O
of	O
image	O
abstraction	O
.	O

subsection	O
:	O
Gradient	Method
back	Method
-	Method
propagation	Method
For	O
back	Task
-	Task
propagation	Task
,	O
a	O
layer	O
receives	O
gradients	O
from	O
the	O
layers	O
that	O
it	O
is	O
connected	O
to	O
.	O

As	O
an	O
example	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
is	O
the	O
input	O
of	O
the	O
first	O
layer	O
,	O
after	O
two	O
convolutional	Method
layers	Method
and	O
,	O
the	O
output	O
is	O
.	O

To	O
update	O
the	O
parameters	O
represented	O
as	O
of	O
,	O
we	O
compute	O
the	O
derivative	O
of	O
with	O
respect	O
to	O
as	O
follows	O
:	O
where	O
using	O
and	O
is	O
only	O
for	O
the	O
clarity	O
of	O
presentation	O
,	O
they	O
are	O
essentially	O
the	O
same	O
.	O

We	O
can	O
further	O
formulate	O
(	O
[	O
reference	O
]	O
)	O
as	O
:	O
Only	O
is	O
computed	O
if	O
we	O
do	O
not	O
use	O
skip	O
connections	O
,	O
and	O
its	O
magnitide	O
may	O
become	O
very	O
small	O
after	O
back	O
-	O
propagating	O
through	O
many	O
layers	O
from	O
the	O
top	O
in	O
very	O
deep	Method
networks	Method
.	O

However	O
,	O
carries	O
larger	O
gradients	O
since	O
it	O
does	O
not	O
have	O
to	O
go	O
through	O
layers	O
of	O
,	O
,	O
and	O
in	O
this	O
example	O
.	O

Thus	O
with	O
the	O
first	O
term	O
only	O
,	O
it	O
is	O
more	O
unlikely	O
to	O
approach	O
zero	O
grdients	O
.	O

As	O
we	O
can	O
see	O
,	O
the	O
skip	O
connection	O
helps	O
to	O
update	O
the	O
filters	O
in	O
bottoms	O
layers	O
,	O
and	O
thus	O
makes	O
training	Task
easier	O
.	O

subsection	O
:	O
Training	O
with	O
symmetric	O
skip	O
connections	O
The	O
aim	O
of	O
restoration	Task
is	O
to	O
eliminate	O
corruption	O
while	O
preserving	O
the	O
image	O
details	O
as	O
mush	O
as	O
possible	O
.	O

Previous	O
works	O
typically	O
use	O
shallow	Method
networks	Method
for	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
tasks	Task
.	O

The	O
reason	O
may	O
be	O
that	O
deeper	Method
networks	Method
can	O
destroy	O
the	O
image	O
details	O
,	O
which	O
is	O
undesired	O
for	O
pixel	Task
-	Task
wise	Task
dense	Task
regression	Task
.	O

Even	O
worse	O
,	O
using	O
very	O
deep	Method
networks	Method
may	O
easily	O
suffer	O
from	O
training	O
issues	O
such	O
as	O
gradient	O
vanishing	O
.	O

Using	O
skip	O
connections	O
in	O
a	O
very	O
deep	Method
network	Method
can	O
address	O
both	O
of	O
the	O
above	O
two	O
problems	O
.	O

Firstly	O
,	O
we	O
design	O
experiments	O
to	O
show	O
that	O
using	O
skip	O
connections	O
is	O
beneficial	O
for	O
image	Task
detail	Task
presering	Task
.	O

Specifically	O
,	O
two	O
networks	O
are	O
trained	O
for	O
image	Task
denoising	Task
with	O
a	O
noise	O
level	O
of	O
.	O

(	O
a	O
)	O
In	O
the	O
first	O
network	O
,	O
we	O
use	O
5	O
layers	O
of	O
convolution	Method
with	O
stride	O
3	O
.	O

The	O
input	O
size	O
of	O
training	O
data	O
is	O
,	O
which	O
results	O
in	O
a	O
vector	O
after	O
5	O
layers	O
of	O
convolution	Method
,	O
encoding	O
the	O
very	O
high	O
level	O
abstraction	O
of	O
the	O
image	O
.	O

Then	O
deconvolution	Method
is	O
used	O
to	O
recover	O
the	O
input	O
from	O
the	O
feature	O
vector	O
.	O

The	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
can	O
observe	O
that	O
it	O
is	O
challenging	O
for	O
deconvolution	Task
to	O
recover	O
details	O
from	O
only	O
a	O
vector	O
encoding	O
the	O
abstraction	O
of	O
the	O
input	O
.	O

This	O
phenomenon	O
implies	O
that	O
simply	O
using	O
deep	Method
networks	Method
for	O
image	Task
restoration	Task
may	O
not	O
lead	O
to	O
satisfactory	O
results	O
.	O

(	O
b	O
)	O
The	O
second	O
network	O
uses	O
the	O
same	O
settings	O
as	O
the	O
first	O
one	O
,	O
but	O
adding	O
skip	O
connections	O
.	O

The	O
results	O
are	O
show	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Compared	O
to	O
the	O
first	O
network	O
,	O
the	O
one	O
with	O
skip	O
connections	O
can	O
recover	O
the	O
input	O
and	O
achieves	O
much	O
better	O
PSNR	Metric
values	O
.	O

This	O
is	O
easy	O
to	O
understand	O
since	O
the	O
feature	O
maps	O
with	O
abundant	O
details	O
at	O
bottom	O
layers	O
are	O
directly	O
passed	O
to	O
the	O
top	O
layers	O
.	O

Secondly	O
,	O
we	O
train	O
and	O
compare	O
five	O
different	O
networks	O
to	O
show	O
that	O
using	O
skip	O
connections	O
help	O
to	O
back	O
-	O
propagate	O
gradient	O
in	O
training	O
to	O
better	O
fit	O
the	O
end	O
-	O
to	O
-	O
end	Task
mapping	Task
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
five	O
networks	O
are	O
:	O
10	O
,	O
20	O
and	O
30	O
layer	Method
networks	Method
without	O
skip	O
connections	O
;	O
and	O
20	O
,	O
30	Method
layer	Method
networks	Method
with	O
skip	O
connections	O
.	O

As	O
can	O
be	O
seen	O
,	O
the	O
training	Metric
loss	Metric
increases	O
when	O
the	O
network	O
going	O
deeper	O
without	O
shortcuts	O
(	O
similar	O
phenomenon	O
is	O
also	O
observed	O
in	O
)	O
.	O

On	O
the	O
validation	O
set	O
,	O
deeper	Method
networks	Method
without	O
shortcuts	Method
achieve	O
lower	O
PSNR	Metric
and	O
we	O
even	O
observe	O
over	O
-	O
fitting	O
for	O
the	O
30	Method
-	Method
layer	Method
network	Method
.	O

These	O
results	O
may	O
be	O
due	O
to	O
the	O
gradient	Task
vanishing	Task
problem	Task
.	O

However	O
,	O
we	O
obtain	O
smaller	O
training	Metric
errors	Metric
on	O
the	O
training	O
set	O
and	O
higher	O
PSNR	Metric
and	O
better	O
generalization	Metric
capability	Metric
on	O
the	O
testing	O
set	O
when	O
using	O
skip	O
connections	O
.	O

subsection	O
:	O
Comparison	O
with	O
the	O
deep	Method
residual	Method
network	Method
One	O
may	O
use	O
different	O
types	O
of	O
skip	O
connections	O
in	O
our	O
network	O
.	O

A	O
straightforward	O
alternate	O
is	O
that	O
in	O
.	O

In	O
,	O
skip	O
connections	O
are	O
added	O
to	O
divide	O
the	O
network	O
into	O
sequential	O
blocks	O
.	O

A	O
benefit	O
of	O
our	O
model	O
is	O
that	O
our	O
skip	O
connections	O
have	O
element	O
-	O
wise	O
correspondence	O
,	O
which	O
can	O
be	O
very	O
important	O
in	O
pixel	Task
-	Task
wise	Task
prediction	Task
problems	Task
such	O
image	Task
denoising	Task
.	O

We	O
carry	O
out	O
experiments	O
to	O
compare	O
these	O
two	O
types	O
of	O
skip	O
connections	O
.	O

Here	O
the	O
block	O
size	O
indicates	O
the	O
span	O
of	O
the	O
connections	O
.	O

The	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
can	O
observe	O
that	O
our	O
connections	O
often	O
converge	O
to	O
a	O
better	O
optimum	O
,	O
demonstrating	O
that	O
element	O
-	O
wise	O
correspondence	O
can	O
be	O
important	O
.	O

Meanwhile	O
,	O
our	O
long	O
range	O
skip	O
connections	O
pass	O
the	O
image	O
detail	O
directly	O
from	O
bottom	O
layers	O
to	O
top	O
layers	O
.	O

If	O
we	O
use	O
the	O
skip	O
connection	O
type	O
in	O
,	O
the	O
network	O
may	O
still	O
lose	O
some	O
image	O
details	O
.	O

subsection	O
:	O
Testing	Metric
efficiency	Metric
To	O
apply	O
deep	Method
learning	Method
models	Method
on	O
devices	O
with	O
limited	O
computing	O
power	O
such	O
as	O
mobile	O
phones	O
,	O
one	O
has	O
to	O
speed	O
-	O
up	O
the	O
testing	O
phase	O
.	O

For	O
our	O
network	O
,	O
we	O
propose	O
to	O
use	O
down	Method
-	Method
sampling	Method
in	Method
convolutional	Method
layers	Method
to	O
reduce	O
the	O
size	O
of	O
the	O
feature	O
maps	O
.	O

In	O
order	O
to	O
obtain	O
an	O
output	O
of	O
the	O
same	O
size	O
as	O
the	O
input	O
,	O
deconvolution	Method
is	O
used	O
to	O
up	O
-	O
sample	O
the	O
feature	O
maps	O
in	O
the	O
symmetric	Method
deconvolutional	Method
layers	Method
.	O

Thus	O
,	O
the	O
testing	Metric
efficiency	Metric
can	O
be	O
well	O
improved	O
with	O
almost	O
negligible	O
performance	O
degradation	O
.	O

In	O
specific	O
,	O
we	O
use	O
stride	O
2	O
in	O
convolutional	Method
layers	Method
to	O
down	O
-	O
sample	O
the	O
feature	O
maps	O
.	O

Down	Method
-	Method
sampling	Method
at	O
different	O
convolutional	Method
layers	Method
are	O
tested	O
on	O
image	Task
denoising	Task
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
test	O
an	O
image	O
of	O
size	O
160	O
240	O
on	O
an	O
i7	O
-	O
2600	O
CPU	O
,	O
the	O
testing	Metric
time	Metric
for	O
“	O
no	O
down	O
-	O
sample	O
”	O
,	O
“	O
down	O
-	O
sample	O
at	O
conv1	O
”	O
,	O
“	O
down	O
-	O
sample	O
at	O
conv5	O
”	O
,	O
“	O
down	O
-	O
sample	O
at	O
conv9	O
”	O
,	O
”	O
down	O
-	O
sample	O
at	O
conv5	O
,	O
9	O
”	O
are	O
3.17s	O
,	O
0.84s	O
,	O
1.43s	O
,	O
2.00s	O
and	O
1.17s	O
respectively	O
.	O

The	O
main	O
observation	O
is	O
that	O
the	O
testing	O
PSNRs	Metric
may	O
slightly	O
degrade	O
according	O
to	O
the	O
scale	O
reduction	O
of	O
the	O
feature	O
map	O
in	O
the	O
entire	O
network	O
.	O

The	O
down	Method
-	Method
sampling	Method
in	O
the	O
first	O
convolutional	Method
layer	Method
reduces	O
the	O
size	O
of	O
the	O
feature	O
maps	O
to	O
1	O
/	O
4	O
,	O
which	O
leads	O
to	O
alomst	O
4x	O
faster	O
in	O
testing	Task
,	O
but	O
the	O
PSNR	Metric
only	O
degrades	O
less	O
than	O
0.1	O
compared	O
to	O
the	O
network	O
without	O
down	Method
-	Method
sampling	Method
.	O

The	O
down	Method
-	Method
sampling	Method
in	O
”	O
conv9	Method
”	O
reduces	O
1	O
/	O
3	O
of	O
the	O
testing	Metric
time	Metric
,	O
but	O
the	O
performance	O
is	O
almost	O
as	O
well	O
as	O
that	O
without	O
down	Method
-	Method
sampling	Method
.	O

As	O
a	O
result	O
,	O
an	O
”	O
earlier	O
”	O
down	Method
-	Method
sampling	Method
may	O
lead	O
to	O
slightly	O
worse	O
performance	O
,	O
but	O
it	O
achieves	O
much	O
faster	O
testing	Metric
efficiency	Metric
.	O

It	O
should	O
be	O
a	O
trade	O
-	O
off	O
in	O
different	O
application	O
situations	O
.	O

section	O
:	O
Experiments	O
In	O
this	O
section	O
,	O
we	O
first	O
provide	O
some	O
experimental	O
results	O
and	O
analysis	O
on	O
different	O
parameters	O
,	O
including	O
filter	O
number	O
,	O
filter	O
size	O
,	O
training	O
patch	O
size	O
and	O
skip	O
connection	O
step	O
size	O
,	O
of	O
the	O
network	O
.	O

Then	O
,	O
evaluation	O
of	O
image	Task
restoration	Task
tasks	Task
including	O
image	Task
denoising	Task
,	O
image	Task
super	Task
-	Task
resolution	Task
,	O
JPEG	Task
image	Task
deblocking	Task
,	O
non	Task
-	Task
blind	Task
image	Task
debluring	Task
and	O
image	Task
inpainting	Task
are	O
conducted	O
and	O
compared	O
against	O
a	O
few	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
each	O
topic	O
.	O

Peak	Metric
Signal	Metric
-	Metric
to	Metric
-	Metric
Noise	Metric
Ratio	Metric
(	O
PSNR	Metric
)	O
and	O
Structural	Metric
SIMilarity	Metric
(	O
SSIM	Metric
)	O
index	O
are	O
calculated	O
for	O
evaluation	O
.	O

For	O
our	O
method	O
,	O
which	O
is	O
denoted	O
as	O
RED	Method
-	Method
Net	Method
,	O
we	O
implement	O
three	O
versions	O
:	O
RED10	Method
contains	O
5	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
without	O
shortcuts	Method
,	O
RED20	Method
contains	O
10	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
with	O
shortcuts	O
of	O
step	O
size	O
2	O
,	O
and	O
RED30	Method
contains	O
15	O
convolutional	Method
and	Method
deconvolutional	Method
layers	Method
with	O
shortcuts	O
of	O
step	O
size	O
2	O
.	O

subsection	O
:	O
Network	O
parameters	O
Although	O
we	O
have	O
observed	O
that	O
deeper	Method
networks	Method
tend	O
to	O
achieve	O
better	O
image	Task
restoration	Task
performance	O
,	O
there	O
exist	O
more	O
problems	O
related	O
to	O
different	O
parameters	O
to	O
be	O
investigated	O
.	O

We	O
carried	O
out	O
image	Task
denoising	Task
experiments	O
on	O
three	O
folds	O
:	O
(	O
a	O
)	O
filter	O
number	O
,	O
(	O
b	O
)	O
filter	O
size	O
,	O
(	O
c	O
)	O
training	O
patch	O
size	O
and	O
(	O
d	O
)	O
step	O
size	O
of	O
skip	O
connections	O
,	O
to	O
show	O
the	O
effects	O
of	O
different	O
parameters	O
.	O

For	O
different	O
filter	O
numbers	O
,	O
we	O
fix	O
the	O
filter	O
size	O
as	O
,	O
training	O
patch	O
size	O
as	O
50	O
50	O
and	O
skip	O
connection	O
step	O
size	O
as	O
2	O
.	O

Different	O
filter	O
numbers	O
of	O
32	O
,	O
64	O
and	O
128	O
are	O
tested	O
,	O
and	O
the	O
PSNR	Metric
values	O
recorded	O
on	O
the	O
validation	O
set	O
during	O
training	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

To	O
converge	O
,	O
the	O
training	O
iterations	O
for	O
different	O
number	O
of	O
filters	O
are	O
similar	O
,	O
but	O
better	O
optimum	O
can	O
be	O
obtained	O
with	O
more	O
filters	O
.	O

However	O
,	O
a	O
smaller	O
number	O
of	O
filters	O
is	O
preferred	O
if	O
a	O
fast	O
testing	Metric
speed	Metric
is	O
desired	O
.	O

For	O
the	O
experiments	O
on	O
filter	Metric
size	Metric
,	O
we	O
set	O
the	O
filter	O
number	O
to	O
be	O
64	O
,	O
training	O
patch	O
size	O
as	O
50	O
50	O
,	O
skip	O
connection	O
step	O
size	O
as	O
2	O
.	O

Filter	O
size	O
of	O
3	O
3	O
,	O
5	O
5	O
,	O
7	O
7	O
,	O
9	O
9	O
are	O
tested	O
.	O

Figure	O
[	O
reference	O
]	O
show	O
the	O
PSNR	Metric
values	O
on	O
the	O
validation	O
set	O
while	O
training	O
.	O

It	O
is	O
clear	O
that	O
larger	O
filter	O
size	O
leads	O
to	O
better	O
performance	O
.	O

Different	O
from	O
high	O
-	O
level	O
tasks	O
which	O
favor	O
smaller	O
filter	O
sizes	O
,	O
larger	O
filter	O
size	O
tends	O
to	O
obtain	O
better	O
performance	O
in	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
applications	Task
.	O

However	O
,	O
there	O
may	O
exist	O
a	O
bottle	O
neck	O
as	O
the	O
performance	O
of	O
9	O
9	O
is	O
almost	O
as	O
the	O
same	O
as	O
7	O
7	O
in	O
our	O
experiments	O
.	O

The	O
reason	O
may	O
be	O
that	O
for	O
high	Task
-	Task
level	Task
tasks	Task
,	O
the	O
networks	O
have	O
to	O
learn	O
image	Task
abstraction	Task
for	O
classification	Task
,	O
which	O
is	O
usually	O
very	O
different	O
from	O
the	O
input	O
pixels	O
.	O

Larger	O
filter	O
size	O
may	O
result	O
in	O
larger	O
respective	O
fields	O
,	O
but	O
also	O
made	O
the	O
networks	O
more	O
difficult	O
to	O
train	O
and	O
converge	O
to	O
a	O
poor	O
optimum	O
.	O

Using	O
smaller	O
filter	O
size	O
is	O
mainly	O
beneficial	O
for	O
convergence	Task
in	O
such	O
complex	O
mappings	O
.	O

In	O
contrast	O
,	O
for	O
low	Task
-	Task
level	Task
image	Task
restoration	Task
,	O
the	O
training	Task
is	O
not	O
as	O
difficult	O
as	O
that	O
in	O
high	Task
-	Task
level	Task
applications	Task
since	O
only	O
a	O
bias	O
is	O
needed	O
to	O
be	O
learned	O
to	O
revise	O
the	O
corrupted	O
pixel	O
.	O

In	O
this	O
situation	O
,	O
utilizing	O
neighborhood	O
information	O
in	O
the	O
mapping	Task
stage	Task
is	O
more	O
important	O
,	O
since	O
the	O
desired	O
value	O
for	O
a	O
pixel	O
should	O
be	O
predicted	O
from	O
its	O
neighbor	O
pixels	O
.	O

However	O
,	O
using	O
larger	O
filter	O
size	O
inevitably	O
increases	O
the	O
complexity	Metric
(	O
e.g.	O
,	O
filter	O
size	O
of	O
9	O
9	O
is	O
9	O
times	O
more	O
complex	O
as	O
3	O
3	O
)	O
and	O
training	Metric
time	Metric
.	O

For	O
the	O
training	O
patch	O
size	O
,	O
we	O
set	O
the	O
filter	O
number	O
to	O
be	O
64	O
,	O
filter	O
size	O
as	O
3	O
3	O
,	O
skip	O
connection	O
step	O
size	O
as	O
2	O
.	O

Then	O
we	O
test	O
different	O
training	O
patch	O
sizes	O
of	O
25	O
25	O
,	O
50	O
50	O
,	O
75	O
75	O
,	O
100	O
100	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Better	O
performance	O
is	O
achieved	O
with	O
larger	O
training	O
patch	O
size	O
.	O

The	O
reason	O
can	O
be	O
two	O
folds	O
.	O

First	O
of	O
all	O
,	O
since	O
the	O
network	O
essentially	O
performs	O
pixel	Task
-	Task
wise	Task
prediction	Task
,	O
if	O
the	O
number	O
of	O
training	O
patches	O
are	O
the	O
same	O
,	O
larger	O
size	O
of	O
training	O
patch	O
results	O
in	O
more	O
pixels	O
to	O
be	O
used	O
,	O
which	O
is	O
equivalent	O
to	O
using	O
more	O
training	O
data	O
.	O

Secondly	O
,	O
the	O
corruptions	O
in	O
image	Task
restoration	Task
tasks	Task
can	O
be	O
described	O
as	O
some	O
types	O
of	O
latent	O
distributions	O
.	O

Larger	O
size	O
of	O
training	O
patch	O
contains	O
more	O
pixels	O
that	O
better	O
capture	O
the	O
latent	O
distributions	O
to	O
be	O
learned	O
,	O
which	O
consequently	O
helps	O
the	O
network	O
to	O
fit	O
the	O
corruptions	O
better	O
.	O

As	O
we	O
can	O
see	O
,	O
the	O
“	O
width	O
”	O
of	O
the	O
network	O
is	O
as	O
crucial	O
as	O
the	O
“	O
depth	O
”	O
in	O
training	O
a	O
network	O
with	O
satisfactory	O
image	Task
restoration	Task
performance	O
.	O

However	O
,	O
one	O
should	O
always	O
make	O
a	O
trade	O
-	O
off	O
between	O
the	O
performance	O
and	O
speed	Metric
.	O

We	O
also	O
provide	O
the	O
experiments	O
of	O
different	O
step	O
sizes	O
of	O
shortcuts	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
smaller	O
step	O
size	O
of	O
shortcuts	Method
achieves	O
better	O
performance	O
than	O
a	O
larger	O
one	O
.	O

We	O
believe	O
that	O
a	O
smaller	O
step	O
size	O
of	O
shortcuts	O
makes	O
it	O
easier	O
to	O
back	O
-	O
propagate	O
the	O
gradient	O
to	O
bottom	O
layers	O
,	O
thus	O
tackle	O
the	O
gradient	Task
vanishing	Task
issue	Task
better	O
.	O

Meanwhile	O
,	O
a	O
small	O
step	O
size	O
of	O
shortcuts	O
essentially	O
passes	O
more	O
direct	O
information	O
.	O

subsection	O
:	O
Image	Task
denoising	Task
Image	Task
denoising	Task
experiments	O
are	O
performed	O
on	O
two	O
datasets	O
:	O
14	O
common	O
benchmark	O
images	O
,	O
as	O
show	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
the	O
BSD	Material
dataset	O
.	O

As	O
a	O
common	O
experimental	O
setting	O
in	O
the	O
literature	O
,	O
additive	O
Gaussian	O
noises	O
with	O
zero	O
mean	O
and	O
standard	O
deviation	O
are	O
added	O
to	O
the	O
image	O
to	O
test	O
the	O
performance	O
of	O
denoising	Method
methods	Method
.	O

In	O
this	O
paper	O
we	O
test	O
noise	O
level	O
of	O
10	O
,	O
30	O
,	O
50	O
and	O
70	O
.	O

BM3D	Method
,	O
NCSR	Method
,	O
EPLL	Method
,	O
PCLR	Method
,	O
PGPD	Method
and	O
WMMN	Method
are	O
compared	O
with	O
our	O
method	O
.	O

For	O
these	O
methods	O
,	O
we	O
use	O
the	O
source	O
code	O
released	O
by	O
their	O
authors	O
and	O
test	O
on	O
the	O
images	O
with	O
their	O
default	O
parameters	O
.	O

Evaluation	O
on	O
the	O
14	O
images	O
Table	O
[	O
reference	O
]	O
presents	O
the	O
PSNR	Metric
and	O
SSIM	Metric
results	O
of	O
10	O
,	O
30	O
,	O
50	O
,	O
and	O
70	O
.	O

We	O
can	O
make	O
some	O
observations	O
from	O
the	O
results	O
.	O

First	O
of	O
all	O
,	O
the	O
10	Method
layer	Method
convolutional	Method
and	Method
deconvolutional	Method
network	Method
has	O
already	O
achieved	O
better	O
results	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
which	O
demonstrates	O
that	O
combining	O
convolution	Method
and	Method
deconvolution	Method
for	O
denoising	Task
works	O
well	O
,	O
even	O
without	O
any	O
skip	O
connections	O
.	O

Moreover	O
,	O
when	O
the	O
network	O
goes	O
deeper	O
,	O
the	O
skip	O
connections	O
proposed	O
in	O
this	O
paper	O
help	O
to	O
achieve	O
even	O
better	O
denoising	Metric
performance	Metric
,	O
which	O
exceeds	O
the	O
existing	O
best	O
method	O
WNNM	Method
by	O
0.32dB	O
,	O
0.43dB	O
,	O
0.49dB	O
and	O
0.51dB	O
on	O
noise	Metric
levels	Metric
of	O
being	O
10	O
,	O
30	O
,	O
50	O
and	O
70	O
respectively	O
.	O

While	O
WNNM	Method
is	O
only	O
slightly	O
better	O
than	O
the	O
second	O
best	O
existing	O
method	O
PCLR	Method
by	O
0.01dB	O
,	O
0.06dB	O
,	O
0.03dB	O
and	O
0.01dB	O
respectively	O
,	O
which	O
shows	O
the	O
large	O
improvement	O
of	O
our	O
model	O
.	O

Last	O
,	O
we	O
can	O
observe	O
that	O
the	O
more	O
complex	O
the	O
noise	O
is	O
,	O
the	O
more	O
improvement	O
our	O
model	O
achieves	O
than	O
other	O
methods	O
.	O

Similar	O
observations	O
can	O
be	O
made	O
on	O
the	O
evaluation	O
of	O
SSIM	Metric
.	O

Evaluation	O
on	O
BSD200	Material
For	O
the	O
BSD	Material
dataset	O
,	O
300	O
images	O
are	O
used	O
for	O
training	O
and	O
the	O
remaining	O
200	O
images	O
are	O
used	O
for	O
denoising	Task
to	O
show	O
more	O
experimental	O
results	O
.	O

For	O
efficiency	O
,	O
we	O
convert	O
the	O
images	O
to	O
gray	O
-	O
scale	O
and	O
resize	O
them	O
to	O
smaller	O
images	O
.	O

Then	O
all	O
the	O
methods	O
are	O
run	O
on	O
the	O
dataset	O
to	O
get	O
average	O
PSNR	Metric
and	O
SSIM	Metric
results	Metric
of	O
10	O
,	O
30	O
,	O
50	O
,	O
and	O
70	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
existing	O
methods	O
,	O
their	O
denoising	Metric
performance	Metric
does	O
not	O
differ	O
much	O
,	O
while	O
our	O
model	O
achieves	O
0.38dB	O
,	O
0.47dB	O
,	O
0.49dB	O
and	O
0.42dB	O
higher	O
of	O
PSNR	Metric
over	O
WNNM	Method
.	O

Blind	Task
denoising	Task
We	O
also	O
perform	O
blind	Task
denoising	Task
to	O
show	O
the	O
superior	O
performance	O
of	O
our	O
network	O
.	O

In	O
blind	Task
denoising	Task
,	O
the	O
training	O
set	O
consists	O
of	O
image	O
patches	O
of	O
different	O
levels	O
of	O
noises	O
,	O
and	O
a	O
30	Method
-	Method
layer	Method
network	Method
is	O
trained	O
on	O
this	O
training	O
set	O
.	O

In	O
the	O
testing	O
phase	O
,	O
we	O
test	O
noisy	O
images	O
with	O
of	O
10	O
,	O
30	O
,	O
50	O
and	O
70	O
using	O
this	O
model	O
.	O

The	O
evaluation	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Although	O
training	O
with	O
different	O
levels	O
of	O
corruption	O
,	O
we	O
can	O
observe	O
that	O
the	O
performance	O
of	O
our	O
network	O
degrades	O
comparing	O
to	O
the	O
case	O
in	O
which	O
using	O
separate	O
models	O
for	O
denoising	Task
.	O

This	O
is	O
reasonable	O
because	O
the	O
network	O
has	O
to	O
fit	O
much	O
more	O
complex	O
mappings	O
.	O

However	O
,	O
it	O
still	O
beats	O
the	O
existing	O
methods	O
.	O

For	O
PSNR	Metric
evaluation	O
,	O
our	O
blind	Method
denoising	Method
model	Method
achieves	O
the	O
same	O
performance	O
as	O
WNNM	Method
on	O
,	O
and	O
outperforms	O
WNNM	Method
by	O
0.35dB	O
,	O
0.43dB	O
and	O
0.40dB	O
on	O
=	O
30	O
,	O
50	O
and	O
70	O
respectively	O
,	O
which	O
is	O
still	O
marginal	O
improvements	O
.	O

For	O
SSIM	Metric
evaluation	O
,	O
our	O
network	O
is	O
0.0005	O
,	O
0.0141	O
,	O
0.0199	O
and	O
0.0182	O
higher	O
than	O
WNNM	Method
.	O

The	O
performance	O
improvement	O
is	O
more	O
obvious	O
on	O
BSD	Material
dataset	O
.	O

The	O
30	Method
-	Method
layer	Method
network	Method
outperforms	O
the	O
second	O
best	O
method	O
WNNM	Method
by	O
0.13dB	O
,	O
0.4dB	O
,	O
0.43dB	O
,	O
0.41dB	O
for	O
PSNR	Metric
and	O
0.0036	O
,	O
0.0173	O
,	O
0.0191	O
,	O
0.0198	O
for	O
SSIM	Metric
.	O

Visual	O
results	O
Some	O
visual	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
highlight	O
some	O
details	O
of	O
the	O
clean	O
image	O
and	O
the	O
recovered	O
ones	O
by	O
different	O
methods	O
.	O

The	O
first	O
observation	O
is	O
that	O
our	O
method	O
better	O
recovers	O
the	O
image	O
details	O
,	O
as	O
we	O
can	O
see	O
from	O
the	O
third	O
and	O
fourth	O
rows	O
,	O
which	O
is	O
due	O
to	O
the	O
high	O
PSNR	Metric
we	O
achieve	O
by	O
minimizing	O
the	O
pixel	Metric
-	Metric
wise	Metric
Euclidean	Metric
loss	Metric
.	O

Moreover	O
,	O
we	O
can	O
observe	O
from	O
the	O
first	O
and	O
second	O
rows	O
that	O
our	O
network	O
obtains	O
more	O
visually	O
smooth	O
results	O
than	O
other	O
methods	O
.	O

This	O
may	O
due	O
to	O
the	O
testing	Method
strategy	Method
which	O
average	O
the	O
output	O
of	O
different	O
orientations	O
.	O

subsection	O
:	O
Image	Task
super	Task
-	Task
resolution	Task
For	O
super	Task
-	Task
resolution	Task
,	O
The	O
high	O
-	O
resolution	O
image	O
is	O
first	O
down	O
-	O
sampled	O
with	O
scaling	O
factor	O
parameters	O
of	O
2	O
,	O
3	O
and	O
4	O
respectively	O
.	O

Since	O
the	O
size	O
of	O
the	O
input	O
and	O
output	O
of	O
our	O
network	O
are	O
the	O
same	O
,	O
we	O
up	O
-	O
sample	O
the	O
low	O
-	O
resolution	O
image	O
to	O
its	O
original	O
size	O
as	O
the	O
input	O
of	O
our	O
network	O
.	O

We	O
compare	O
our	O
network	O
with	O
SRCNN	Method
,	O
NBSRF	Method
,	O
CSCN	Method
,	O
CSC	Method
,	O
TSE	Method
and	O
ARFL	Method
+	Method
on	O
three	O
dataset	O
:	O
Set5	Material
,	O
Set14	O
and	O
BSD100	Material
.	O

The	O
results	O
of	O
the	O
compared	O
methods	O
are	O
either	O
cited	O
from	O
their	O
original	O
papers	O
or	O
obtained	O
using	O
the	O
released	O
source	O
code	O
by	O
the	O
authors	O
.	O

Evaluation	O
on	O
Set	O
5	O
The	O
evaluation	O
on	O
Set5	Material
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
general	O
,	O
our	O
10	Method
-	Method
layer	Method
network	Method
already	O
outperforms	O
the	O
compared	O
methods	O
,	O
and	O
we	O
achieve	O
better	O
performance	O
with	O
deeper	Method
networks	Method
.	O

The	O
second	O
best	O
method	O
is	O
CSCN	Method
,	O
which	O
is	O
also	O
a	O
recently	O
proposed	O
neural	Method
network	Method
based	Method
method	Method
.	O

Compared	O
to	O
CSCN	Method
,	O
our	O
30	Method
-	Method
layer	Method
network	Method
exceeds	O
it	O
by	O
0.52dB	O
,	O
0.56dB	O
,	O
0.47dB	O
on	O
PSNR	Metric
and	O
0.0032	Metric
,	O
0.0063	O
,	O
0.0094	O
on	O
SSIM	Metric
respectively	O
.	O

The	O
larger	O
scaling	O
parameter	O
is	O
,	O
the	O
better	O
improvement	O
our	O
method	O
can	O
make	O
,	O
which	O
demonstrates	O
that	O
our	O
network	O
is	O
better	O
at	O
fitting	O
complex	O
corruptions	O
than	O
other	O
methods	O
.	O

Evaluation	O
on	O
Set	O
14	O
The	O
evaluation	O
on	O
Set14	O
is	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
improvement	O
on	O
Set14	O
in	O
not	O
as	O
significant	O
as	O
that	O
on	O
Set5	Material
,	O
but	O
we	O
can	O
still	O
observe	O
that	O
the	O
30	Method
-	Method
layer	Method
network	Method
achieves	O
higher	O
PSNR	Metric
and	O
SSIM	Metric
than	O
the	O
second	O
best	O
CSCN	Method
for	O
0.23dB	O
,	O
0.06dB	O
,	O
0.1dB	O
and	O
0.0049	O
,	O
0.0070	O
,	O
0.0098	O
.	O

The	O
performance	O
on	O
10	Metric
-	Metric
layer	Metric
,	O
20	Method
-	Method
layer	Method
and	Method
30	Method
-	Method
layer	Method
RED	Method
-	Method
Net	Method
also	O
does	O
not	O
improve	O
that	O
much	O
as	O
on	O
Set5	Material
,	O
which	O
may	O
imply	O
that	O
Set14	O
is	O
more	O
difficult	O
to	O
perform	O
image	Task
super	Task
-	Task
resolution	Task
.	O

Evaluation	O
on	O
BSD	Material
100	O
We	O
also	O
evaluate	O
super	Task
-	Task
resolution	Task
results	O
on	O
BSD100	Material
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
overall	O
results	O
are	O
very	O
similar	O
than	O
those	O
on	O
Set5	Material
.	O

CSCN	Method
is	O
still	O
the	O
second	O
best	O
method	O
and	O
outperforms	O
other	O
compared	O
methods	O
by	O
large	O
margin	O
,	O
but	O
its	O
performance	O
is	O
not	O
as	O
good	O
as	O
our	O
10	Method
-	Method
layer	Method
network	Method
.	O

Our	O
deeper	Method
networks	Method
obtain	O
performance	O
gains	O
.	O

Compared	O
to	O
CSCN	Method
,	O
the	O
30	Method
-	Method
layer	Method
network	Method
achieves	O
higher	O
PSNR	Metric
for	O
0.45dB	O
,	O
0.38dB	O
,	O
0.29dB	O
and	O
higher	O
SSIM	Metric
for	O
0.0066	O
,	O
0.0084	O
,	O
0.0099	O
.	O

Comparisons	O
with	O
VDSR	Method
[	O
]	O
and	O
DRCN	Method
[	O
]	O
Concurrent	O
to	O
our	O
work	O
,	O
networks	O
which	O
incorporate	O
residual	Method
learning	Method
for	O
image	Task
super	Task
-	Task
resolution	Task
are	O
proposed	O
.	O

In	O
,	O
a	O
fully	Method
convolutional	Method
network	Method
termed	O
VDSR	Method
is	O
proposed	O
to	O
learn	O
the	O
residual	O
image	O
for	O
image	Task
super	Task
-	Task
resolution	Task
.	O

The	O
loss	Method
layer	Method
takes	O
three	O
inputs	O
:	O
residual	Task
estimate	Task
,	O
low	O
-	O
resolution	O
input	O
and	O
ground	O
truth	O
high	O
-	O
resolution	O
image	O
,	O
and	O
Euclidean	O
loss	O
is	O
computed	O
between	O
the	O
reconstructed	O
image	O
(	O
the	O
sum	O
of	O
network	O
input	O
and	O
output	O
)	O
and	O
ground	O
truth	O
.	O

DRCN	Method
proposed	O
to	O
use	O
a	O
recursive	Method
convolutional	Method
block	Method
,	O
which	O
does	O
not	O
increase	O
the	O
number	O
of	O
parameters	O
while	O
increasing	O
the	O
depth	O
of	O
the	O
network	O
.	O

To	O
ease	O
the	O
training	O
,	O
firstly	O
each	O
recursive	Method
layer	Method
is	O
supervised	O
to	O
reconstruct	O
the	O
target	O
high	O
-	O
resolution	O
image	O
(	O
HR	O
)	O
.	O

The	O
second	O
proposal	O
is	O
to	O
use	O
a	O
skip	O
-	O
connection	O
from	O
input	O
to	O
the	O
output	O
.	O

During	O
training	O
,	O
the	O
network	O
has	O
outputs	O
,	O
in	O
which	O
the	O
th	O
output	O
.	O

is	O
the	O
input	O
low	O
-	O
resolution	O
image	O
,	O
denotes	O
the	O
reconstruction	O
layer	O
and	O
is	O
the	O
output	O
of	O
th	O
recursive	Method
layer	Method
.	O

The	O
final	O
loss	Metric
includes	O
three	O
parts	O
:	O
(	O
a	O
)	O
the	O
Euclidean	Metric
loss	Metric
between	O
the	O
ground	Metric
truth	Metric
and	O
each	O
;	O
(	O
b	O
)	O
the	O
Euclidean	Metric
loss	Metric
between	O
the	O
ground	O
truth	O
and	O
the	O
weighted	O
sum	O
of	O
all	O
;	O
and	O
(	O
c	O
)	O
the	O
L2	Method
regularization	Method
on	O
the	O
network	O
weights	O
.	O

Although	O
skip	O
connections	O
are	O
used	O
in	O
our	O
network	O
,	O
VDSR	Method
and	O
DCRN	Method
to	O
perform	O
identity	Task
mapping	Task
,	O
their	O
differences	O
are	O
significant	O
.	O

Firstly	O
,	O
both	O
VDSR	Method
and	O
DRCN	Method
use	O
one	O
path	O
of	O
connections	O
between	O
the	O
input	O
and	O
output	O
,	O
which	O
actually	O
models	O
the	O
corruptions	O
.	O

In	O
VDSR	Method
,	O
the	O
network	O
itself	O
is	O
standard	O
fully	Method
convolutional	Method
.	O

DRCN	Method
uses	O
recursive	Method
convolutional	Method
layers	Method
that	O
lead	O
to	O
multiple	O
losses	O
,	O
which	O
is	O
different	O
from	O
VDSR	Method
.	O

The	O
skip	Method
connections	Method
in	O
VDSR	Method
and	O
DRCN	Method
model	Method
the	O
super	Task
-	Task
resolution	Task
problem	Task
as	O
learning	O
the	O
residual	O
image	O
,	O
which	O
actually	O
learns	O
the	O
corruption	O
as	O
in	O
image	Task
restoration	Task
.	O

In	O
other	O
words	O
,	O
the	O
residual	Method
learning	Method
is	O
only	O
conducted	O
in	O
the	O
input	O
-	O
output	O
level	O
(	O
low	O
-	O
resolution	O
and	O
high	O
-	O
resolution	O
images	O
)	O
in	O
VDSR	Method
and	O
DRCN	Method
.	O

In	O
contrast	O
,	O
our	O
network	O
uses	O
multiple	O
skip	O
connections	O
that	O
divide	O
the	O
network	O
into	O
multiple	O
blocks	O
for	O
residual	Task
learning	Task
.	O

Secondly	O
,	O
our	O
skip	O
connections	O
pass	O
image	Method
abstraction	Method
of	O
different	O
levels	O
from	O
multiple	O
convolutional	Method
layers	Method
forwardly	O
.	O

No	O
such	O
information	O
is	O
used	O
in	O
VDSR	Method
and	O
DRCN	Method
.	O

In	O
VDSR	Method
and	O
DRCN	Method
,	O
the	O
skip	Method
connection	Method
only	O
pass	O
the	O
input	O
image	O
.	O

However	O
,	O
in	O
our	O
network	O
,	O
different	O
levels	O
of	O
image	O
abstraction	O
are	O
obtained	O
after	O
the	O
convolutional	Method
layers	Method
,	O
and	O
they	O
are	O
passed	O
to	O
the	O
deconvolutional	Method
layers	Method
for	O
reconstruction	Task
.	O

At	O
last	O
,	O
our	O
skip	O
connections	O
help	O
to	O
back	O
-	O
propagate	O
gradients	O
in	O
different	O
layers	O
.	O

In	O
VDSR	Method
and	O
DCRN	Method
,	O
the	O
skip	Method
connections	Method
do	O
not	O
involve	O
in	O
back	O
-	O
propagating	O
gradients	O
since	O
they	O
connect	O
the	O
input	O
and	O
output	O
,	O
and	O
there	O
are	O
no	O
weights	O
to	O
be	O
updated	O
for	O
the	O
input	O
low	O
-	O
resolution	O
image	O
.	O

The	O
image	Metric
super	Metric
-	Metric
resolution	Metric
comparisons	Metric
of	O
VDSR	Method
,	O
DRCN	Method
and	O
our	O
network	O
on	O
Set5	Material
,	O
Set14	Material
and	O
BSD100	Material
are	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Blind	Task
super	Task
-	Task
resolution	Task
The	O
results	O
of	O
blind	Task
super	Task
-	Task
resolution	Task
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Among	O
the	O
compared	O
methods	O
,	O
CSCN	Method
can	O
also	O
deal	O
with	O
different	O
scaling	O
parameters	O
by	O
repeatedly	O
enlarging	O
the	O
image	O
by	O
a	O
smaller	O
scaling	O
factor	O
.	O

Our	O
method	O
is	O
different	O
from	O
CSCN	Method
.	O

Given	O
a	O
low	O
-	O
resolution	O
image	O
as	O
input	O
and	O
the	O
output	O
size	O
,	O
we	O
first	O
up	O
-	O
sample	O
the	O
input	O
image	O
to	O
the	O
desired	O
size	O
,	O
resulting	O
in	O
an	O
image	O
with	O
poor	O
details	O
.	O

Then	O
the	O
image	O
is	O
fed	O
into	O
our	O
network	O
.	O

The	O
output	O
is	O
an	O
image	O
of	O
the	O
same	O
size	O
with	O
fine	O
details	O
.	O

The	O
training	O
set	O
consists	O
of	O
image	O
patches	O
of	O
different	O
scaling	O
parameters	O
and	O
a	O
single	O
model	O
is	O
trained	O
.	O

Except	O
that	O
CSCN	Method
works	O
slightly	O
better	O
on	O
Set	O
14	O
with	O
scaling	O
factors	O
3	O
and	O
4	O
,	O
our	O
network	O
outperforms	O
the	O
existing	O
methods	O
,	O
showing	O
that	O
our	O
network	O
works	O
much	O
better	O
in	O
image	Task
super	Task
-	Task
resolution	Task
even	O
using	O
only	O
one	O
single	O
model	O
to	O
deal	O
with	O
complex	O
corruptions	O
.	O

Visual	O
results	O
Some	O
visual	O
results	O
in	O
grey	O
-	O
scale	O
images	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Note	O
that	O
it	O
is	O
straightforward	O
to	O
perform	O
super	Task
-	Task
resolution	Task
on	O
color	O
images	O
.	O

We	O
can	O
observe	O
from	O
the	O
second	O
and	O
third	O
rows	O
that	O
our	O
network	O
is	O
better	O
at	O
obtaining	O
high	O
resolution	O
edges	O
and	O
text	O
.	O

Meanwhile	O
,	O
our	O
results	O
seem	O
much	O
more	O
smooth	O
than	O
others	O
.	O

For	O
faces	O
such	O
as	O
the	O
fourth	O
row	O
,	O
out	Method
network	Method
still	O
obtains	O
better	O
visually	O
results	O
.	O

subsection	O
:	O
JPEG	Task
deblocking	Task
Lossy	Task
compression	Task
,	O
such	O
as	O
JPEG	Method
,	O
introduces	O
complex	O
compression	O
artifacts	O
,	O
particularly	O
the	O
blocking	O
artifacts	O
,	O
ringing	O
effects	O
and	O
blurring	O
.	O

In	O
this	O
section	O
,	O
we	O
carry	O
out	O
deblocking	Task
experiments	Task
to	O
recover	O
high	O
quality	O
images	O
from	O
their	O
JPEG	Task
compression	Task
.	O

As	O
in	O
other	O
compression	Method
artifacts	Method
reduction	Method
methods	Method
,	O
standard	O
JPEG	Method
compression	Method
schemes	Method
of	O
JPEG	Method
quality	Method
settings	Method
and	O
in	O
MATLAB	Method
JPEG	Method
encoder	Method
are	O
used	O
.	O

The	O
LIVE1	Material
dataset	Material
is	O
used	O
for	O
evaluation	O
,	O
and	O
we	O
have	O
compared	O
our	O
method	O
with	O
AR	Method
-	Method
CNN	Method
,	O
SA	Method
-	Method
DCT	Method
and	O
deeper	Method
SRCNN	Method
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
can	O
observe	O
that	O
since	O
the	O
Euclidean	O
loss	O
favors	O
a	O
high	O
PSNR	Metric
,	O
our	O
network	O
outperforms	O
other	O
methods	O
.	O

Compared	O
to	O
AR	Method
-	Method
CNN	Method
,	O
the	O
30	Method
-	Method
layer	Method
network	Method
exceeds	O
it	O
by	O
0.37dB	O
and	O
0.44dB	O
on	O
compression	Metric
quality	Metric
of	O
10	O
and	O
20	O
.	O

Meanwhile	O
,	O
we	O
can	O
see	O
that	O
compared	O
to	O
shallow	Method
networks	Method
,	O
using	O
significantly	O
deeper	Method
networks	Method
does	O
improve	O
the	O
deblocking	Metric
performance	Metric
.	O

subsection	O
:	O
Non	Task
-	Task
blind	Task
deblurring	Task
We	O
mainly	O
follow	O
the	O
experimental	O
protocols	O
as	O
in	O
for	O
evaluation	O
of	O
non	Task
-	Task
blind	Task
deblurring	Task
.	O

The	O
performance	O
on	O
deblurring	O
“	O
disk	O
”	O
,	O
“	O
motion	O
”	O
and	O
“	O
gaussian	O
”	O
kernels	O
are	O
compared	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
generate	O
blurred	O
image	O
patches	O
with	O
the	O
corresponding	O
kernels	O
,	O
and	O
train	O
end	Task
-	Task
to	Task
-	Task
end	Task
mapping	Task
with	O
pairs	O
of	O
blurred	O
and	O
non	O
-	O
blurred	O
image	O
patches	O
.	O

As	O
we	O
can	O
see	O
from	O
the	O
results	O
,	O
our	O
network	O
outperforms	O
those	O
compared	O
methods	O
with	O
significant	O
improvements	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
some	O
visual	O
comparisons	O
.	O

We	O
can	O
observe	O
from	O
the	O
visual	O
examples	O
that	O
our	O
network	O
works	O
better	O
than	O
the	O
compared	O
methods	O
on	O
recovering	O
the	O
image	O
details	O
,	O
as	O
well	O
as	O
achieving	O
visually	O
more	O
appealing	O
results	O
on	O
low	O
frequency	O
image	O
contents	O
.	O

subsection	O
:	O
Image	Task
inpainting	Task
In	O
this	O
section	O
,	O
we	O
conduct	O
text	Task
removal	Task
for	O
experiments	O
of	O
image	Task
inpainting	Task
.	O

Text	Material
is	O
added	O
to	O
the	O
original	O
image	O
from	O
the	O
LIVE1	Material
dataset	Material
with	O
font	O
size	O
of	O
10	O
and	O
20	O
.	O

We	O
have	O
compared	O
our	O
method	O
with	O
FoE	Method
.	O

For	O
our	O
model	O
,	O
we	O
extract	O
image	O
patches	O
with	O
text	O
on	O
them	O
and	O
learn	O
a	O
mapping	O
from	O
them	O
to	O
the	O
original	O
patches	O
.	O

For	O
FoE	Task
,	O
we	O
provide	O
both	O
images	O
with	O
text	O
and	O
masks	O
indicating	O
which	O
pixel	O
is	O
corrupted	O
.	O

The	O
average	O
PSNR	Metric
and	O
SSIM	Metric
for	O
font	O
size	O
10	O
and	O
20	O
on	O
LIVE	Material
are	O
:	O
38.24dB	O
,	O
0.9869	O
and	O
34.99dB	O
,	O
0.9828	O
using	O
30	Method
-	Method
layer	Method
RED	Method
-	Method
Net	Method
,	O
and	O
they	O
are	O
much	O
better	O
than	O
those	O
of	O
FoE	Method
,	O
which	O
are	O
34.59dB	O
,	O
0.9762	O
and	O
31.10dB	O
,	O
0.9510	O
.	O

For	O
scratch	Task
removal	Task
,	O
we	O
randomly	O
draw	O
scratch	O
on	O
the	O
clean	O
image	O
and	O
test	O
with	O
our	O
network	O
and	O
FoE.	Method
The	O
PSNR	Metric
and	O
SSIM	Metric
for	O
our	O
network	O
are	O
39.41dB	O
and	O
0.9923	O
,	O
which	O
is	O
much	O
better	O
than	O
32.92dB	O
and	O
0.9686	O
of	O
FoE.	Method
Figure	O
[	O
reference	O
]	O
shows	O
some	O
visual	O
comparisons	O
of	O
our	O
method	O
between	O
FoE.	Method
We	O
can	O
observe	O
from	O
the	O
examples	O
that	O
our	O
network	O
is	O
better	O
at	O
recovering	O
text	O
,	O
logos	O
,	O
faces	O
and	O
edges	O
in	O
the	O
natural	O
images	O
.	O

Looking	O
on	O
the	O
first	O
example	O
,	O
one	O
may	O
wonder	O
why	O
the	O
text	O
in	O
the	O
original	O
image	O
is	O
not	O
eliminated	O
.	O

For	O
traditional	O
methods	O
such	O
as	O
FoE	Method
,	O
this	O
problem	O
is	O
addressed	O
by	O
providing	O
a	O
mask	O
,	O
which	O
indicates	O
the	O
location	O
of	O
corrupted	O
pixels	O
.	O

While	O
our	O
network	O
is	O
trained	O
on	O
specific	O
distributions	O
of	O
corruptions	O
,	O
i.e.	O
,	O
the	O
text	O
of	O
font	O
sizes	O
10	O
and	O
20	O
that	O
are	O
added	O
.	O

It	O
is	O
equivalent	O
to	O
distinguishing	O
corrupted	O
and	O
non	O
-	O
corrupted	O
pixels	O
of	O
different	O
distributions	O
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
we	O
have	O
proposed	O
a	O
deep	Method
encoding	Method
and	Method
decoding	Method
framework	Method
for	O
image	Task
restoration	Task
.	O

Convolution	Method
and	O
deconvolution	Method
are	O
combined	O
,	O
modeling	O
the	O
restoration	Task
problem	Task
by	O
extracting	O
primary	O
image	O
content	O
and	O
recovering	O
details	O
.	O

More	O
importantly	O
,	O
we	O
propose	O
to	O
use	O
skip	O
connections	O
,	O
which	O
helps	O
on	O
recovering	O
clean	O
images	O
and	O
tackles	O
the	O
optimization	Task
difficulty	Task
caused	O
by	O
gradient	O
vanishing	O
,	O
and	O
thus	O
obtains	O
performance	O
gains	O
when	O
the	O
network	O
goes	O
deeper	O
.	O

Experimental	O
results	O
and	O
our	O
analysis	O
show	O
that	O
our	O
network	O
achieves	O
better	O
performance	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
image	Task
denoising	Task
,	O
image	Task
super	Task
-	Task
resolution	Task
,	O
JPEG	Task
deblocking	Task
and	O
image	Task
inpainting	Task
.	O

bibliography	O
:	O
References	O
