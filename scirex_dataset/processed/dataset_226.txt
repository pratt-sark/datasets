document	O
:	O
Recurrent	Method
Relational	Method
Networks	Method
This	O
paper	O
is	O
concerned	O
with	O
learning	Task
to	O
solve	O
tasks	O
that	O
require	O
a	O
chain	O
of	O
interdependent	O
steps	O
of	O
relational	Method
inference	Method
,	O
like	O
answering	O
complex	O
questions	O
about	O
the	O
relationships	O
between	O
objects	O
,	O
or	O
solving	O
puzzles	O
where	O
the	O
smaller	O
elements	O
of	O
a	O
solution	O
mutually	O
constrain	O
each	O
other	O
.	O

We	O
introduce	O
the	O
recurrent	Method
relational	Method
network	Method
,	O
a	O
general	Method
purpose	Method
module	Method
that	O
operates	O
on	O
a	O
graph	Method
representation	Method
of	Method
objects	Method
.	O

As	O
a	O
generalization	Method
of	Method
’s	Method
relational	Method
network	Method
,	O
it	O
can	O
augment	O
any	O
neural	Method
network	Method
model	Method
with	O
the	O
capacity	O
to	O
do	O
many	O
-	O
step	O
relational	Method
reasoning	Method
.	O

We	O
achieve	O
state	O
of	O
the	O
art	O
results	O
on	O
the	O
bAbI	Material
textual	Material
question	Material
-	Material
answering	Material
dataset	Material
with	O
the	O
recurrent	Method
relational	Method
network	Method
,	O
consistently	O
solving	O
20	O
/	O
20	O
tasks	O
.	O

As	O
bAbI	Material
is	O
not	O
particularly	O
challenging	O
from	O
a	O
relational	Method
reasoning	Method
point	O
of	O
view	O
,	O
we	O
introduce	O
Pretty	Material
-	Material
CLEVR	Material
,	O
a	O
new	O
diagnostic	O
dataset	O
for	O
relational	Method
reasoning	Method
.	O

In	O
the	O
Pretty	Material
-	Material
CLEVR	Material
set	O
-	O
up	O
,	O
we	O
can	O
vary	O
the	O
question	O
to	O
control	O
for	O
the	O
number	O
of	O
relational	Method
reasoning	Method
steps	O
that	O
are	O
required	O
to	O
obtain	O
the	O
answer	O
.	O

Using	O
Pretty	Material
-	Material
CLEVR	Material
,	O
we	O
probe	O
the	O
limitations	O
of	O
multi	Method
-	Method
layer	Method
perceptrons	Method
,	O
relational	Method
and	Method
recurrent	Method
relational	Method
networks	Method
.	O

Finally	O
,	O
we	O
show	O
how	O
recurrent	Method
relational	Method
networks	Method
can	O
learn	O
to	O
solve	O
Sudoku	Material
puzzles	Material
from	O
supervised	O
training	O
data	O
,	O
a	O
challenging	O
task	O
requiring	O
upwards	O
of	O
64	O
steps	O
of	O
relational	Method
reasoning	Method
.	O

We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
amongst	O
comparable	O
methods	O
by	O
solving	O
96.6	O
%	O
of	O
the	O
hardest	O
Sudoku	Material
puzzles	Material
.	O

section	O
:	O
Introduction	O
A	O
central	O
component	O
of	O
human	Task
intelligence	Task
is	O
the	O
ability	O
to	O
abstractly	O
reason	O
about	O
objects	O
and	O
their	O
interactions	O
spelke1995development	O
,	O
spelke2007core	O
.	O

As	O
an	O
illustrative	O
example	O
,	O
consider	O
solving	O
a	O
Sudoku	Material
.	O

A	O
Sudoku	Material
consists	O
of	O
81	O
cells	O
that	O
are	O
arranged	O
in	O
a	O
9	O
-	O
by	O
-	O
9	O
grid	O
,	O
which	O
must	O
be	O
filled	O
with	O
digits	O
1	O
to	O
9	O
so	O
that	O
each	O
digit	O
appears	O
exactly	O
once	O
in	O
each	O
row	O
,	O
column	O
and	O
3	O
-	O
by	O
-	O
3	O
non	O
-	O
overlapping	O
box	O
,	O
with	O
a	O
number	O
of	O
digits	O
given	O
.	O

To	O
solve	O
a	O
Sudoku	O
,	O
one	O
methodically	O
reasons	O
about	O
the	O
puzzle	O
in	O
terms	O
of	O
its	O
cells	O
and	O
their	O
interactions	O
over	O
many	O
steps	O
.	O

One	O
tries	O
placing	O
digits	O
in	O
cells	O
and	O
see	O
how	O
that	O
affects	O
other	O
cells	O
,	O
iteratively	O
working	O
toward	O
a	O
solution	O
.	O

Contrast	O
this	O
with	O
the	O
canonical	O
deep	Method
learning	Method
approach	Method
to	O
solving	Task
problems	Task
,	O
the	O
multilayer	Method
perceptron	Method
(	O
MLP	Method
)	O
,	O
or	O
multilayer	O
convolutional	Method
neural	Method
net	Method
(	O
CNN	Method
)	O
.	O

These	O
architectures	O
take	O
the	O
entire	O
Sudoku	O
as	O
an	O
input	O
and	O
output	O
the	O
entire	O
solution	O
in	O
a	O
single	O
forward	O
pass	O
,	O
ignoring	O
the	O
inductive	O
bias	O
that	O
objects	O
exists	O
in	O
the	O
world	O
,	O
and	O
that	O
they	O
affect	O
each	O
other	O
in	O
a	O
consistent	O
manner	O
.	O

Not	O
surprisingly	O
these	O
models	O
fall	O
short	O
when	O
faced	O
with	O
problems	O
that	O
require	O
even	O
basic	O
relational	Method
reasoning	Method
lake2016building	O
,	O
santoro2017simple	O
.	O

The	O
relational	Method
network	Method
of	O
santoro2017simple	O
is	O
an	O
important	O
first	O
step	O
towards	O
a	O
simple	O
module	O
for	O
reasoning	Task
about	Task
objects	Task
and	O
their	O
interactions	O
but	O
it	O
is	O
limited	O
to	O
performing	O
a	O
single	O
relational	Method
operation	Method
,	O
and	O
was	O
evaluated	O
on	O
datasets	O
that	O
require	O
a	O
maximum	O
of	O
three	O
steps	O
of	O
reasoning	O
(	O
which	O
,	O
surprisingly	O
,	O
can	O
be	O
solved	O
by	O
a	O
single	O
relational	Method
reasoning	Method
step	O
as	O
we	O
show	O
)	O
.	O

Looking	O
beyond	O
relational	Method
networks	Method
,	O
there	O
is	O
a	O
rich	O
literature	O
on	O
logic	Task
and	Task
reasoning	Task
in	O
artificial	Task
intelligence	Task
and	O
machine	Task
learning	Task
,	O
which	O
we	O
discuss	O
in	O
section	O
[	O
reference	O
]	O
.	O

Toward	O
generally	O
realizing	O
the	O
ability	O
to	O
methodically	O
reason	O
about	O
objects	O
and	O
their	O
interactions	O
over	O
many	O
steps	O
,	O
this	O
paper	O
introduces	O
a	O
composite	Method
function	Method
,	O
the	O
recurrent	Method
relational	Method
network	Method
.	O

It	O
serves	O
as	O
a	O
modular	Method
component	Method
for	O
many	O
-	O
step	O
relational	Method
reasoning	Method
in	O
end	Task
-	Task
to	Task
-	Task
end	Task
differentiable	Task
learning	Task
systems	Task
.	O

It	O
encodes	O
the	O
inductive	O
biases	O
that	O
1	O
)	O
objects	O
exists	O
in	O
the	O
world	O
2	O
)	O
they	O
can	O
be	O
sufficiently	O
described	O
by	O
properties	O
3	O
)	O
properties	O
can	O
change	O
over	O
time	O
4	O
)	O
objects	O
can	O
affect	O
each	O
other	O
and	O
5	O
)	O
given	O
the	O
properties	O
,	O
the	O
effects	O
object	O
have	O
on	O
each	O
other	O
is	O
invariant	O
to	O
time	O
.	O

An	O
important	O
insight	O
from	O
the	O
work	O
of	O
santoro2017simple	O
is	O
to	O
decompose	O
a	O
function	O
for	O
relational	Method
reasoning	Method
into	O
two	O
components	O
or	O
‘	O
‘	O
modules	O
’	O
’	O
:	O
a	O
perceptual	Method
front	Method
-	Method
end	Method
,	O
which	O
is	O
tasked	O
to	O
recognize	O
objects	O
in	O
the	O
raw	O
input	O
and	O
represent	O
them	O
as	O
vectors	O
,	O
and	O
a	O
relational	Method
reasoning	Method
module	Method
,	O
which	O
uses	O
the	O
representation	O
to	O
reason	O
about	O
the	O
objects	O
and	O
their	O
interactions	O
.	O

Both	O
modules	O
are	O
trained	O
jointly	O
end	O
-	O
to	O
-	O
end	O
.	O

In	O
computer	Task
science	Task
parlance	Task
,	O
the	O
relational	Method
reasoning	Method
module	Method
implements	O
an	O
interface	O
:	O
it	O
operates	O
on	O
a	O
graph	O
of	O
nodes	O
and	O
directed	O
edges	O
,	O
where	O
the	O
nodes	O
are	O
represented	O
by	O
real	O
valued	O
vectors	O
,	O
and	O
is	O
differentiable	O
.	O

This	O
paper	O
chiefly	O
develops	O
the	O
relational	Method
reasoning	Method
side	O
of	O
that	O
interface	O
.	O

Some	O
of	O
the	O
tasks	O
we	O
evaluate	O
on	O
can	O
be	O
efficiently	O
and	O
perfectly	O
solved	O
by	O
hand	Method
-	Method
crafted	Method
algorithms	Method
that	O
operate	O
on	O
the	O
symbolic	O
level	O
.	O

For	O
example	O
,	O
9	Material
-	Material
by	Material
-	Material
9	Material
Sudokus	Material
can	O
be	O
solved	O
in	O
a	O
fraction	O
of	O
a	O
second	O
with	O
constraint	Method
propagation	Method
and	O
search	Method
norvig2006solving	O
or	O
with	O
dancing	Method
links	Method
knuth2000dancing	O
.	O

These	O
symbolic	Method
algorithms	Method
are	O
superior	O
in	O
every	O
respect	O
but	O
one	O
:	O
they	O
do	O
n’t	O
comply	O
with	O
the	O
interface	O
,	O
as	O
they	O
are	O
not	O
differentiable	O
and	O
do	O
n’t	O
work	O
with	O
real	O
-	O
valued	O
vector	O
descriptions	O
.	O

They	O
therefore	O
can	O
not	O
be	O
used	O
in	O
a	O
combined	Method
model	Method
with	O
a	O
deep	Method
learning	Method
perceptual	Method
front	Method
-	Method
end	Method
and	O
learned	O
end	O
-	O
to	O
-	O
end	O
.	O

Following	O
,	O
we	O
use	O
the	O
term	O
‘	O
‘	O
relational	Method
reasoning	Method
’	O
’	O
liberally	O
for	O
an	O
object	Method
-	Method
and	Method
interaction	Method
-	Method
centric	Method
approach	Method
to	O
problem	Task
solving	Task
.	O

Although	O
the	O
term	O
‘	O
‘	O
relational	Method
reasoning	Method
’	O
’	O
is	O
similar	O
to	O
terms	O
in	O
other	O
branches	O
of	O
science	Task
,	O
like	O
relational	O
logic	O
or	O
first	O
order	O
logic	O
,	O
no	O
direct	O
parallel	Method
is	O
intended	O
.	O

This	O
paper	O
considers	O
many	O
-	O
step	O
relational	Method
reasoning	Method
,	O
a	O
challenging	O
task	O
for	O
deep	Method
learning	Method
architectures	Method
.	O

We	O
develop	O
a	O
recurrent	Method
relational	Method
reasoning	Method
module	Method
,	O
which	O
constitutes	O
our	O
main	O
contribution	O
.	O

We	O
show	O
that	O
it	O
is	O
a	O
powerful	O
architecture	O
for	O
many	O
-	O
step	O
relational	Method
reasoning	Method
on	O
three	O
varied	O
datasets	O
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
bAbI	Material
and	O
Sudoku	Material
.	O

section	O
:	O
Recurrent	Method
Relational	Method
Networks	Method
We	O
ground	O
the	O
discussion	O
of	O
a	O
recurrent	Method
relational	Method
network	Method
in	O
something	O
familiar	O
,	O
solving	O
a	O
Sudoku	Material
puzzle	Material
.	O

A	O
simple	O
strategy	O
works	O
by	O
noting	O
that	O
if	O
a	O
certain	O
Sudoku	O
cell	O
is	O
given	O
as	O
a	O
‘	O
‘	O
7	O
’	O
’	O
,	O
one	O
can	O
safely	O
remove	O
‘	O
‘	O
7	O
’	O
’	O
as	O
an	O
option	O
from	O
other	O
cells	O
in	O
the	O
same	O
row	O
,	O
column	O
and	O
box	O
.	O

In	O
a	O
message	Method
passing	Method
framework	Method
,	O
that	O
cell	O
needs	O
to	O
send	O
a	O
message	O
to	O
each	O
other	O
cell	O
in	O
the	O
same	O
row	O
,	O
column	O
,	O
and	O
box	O
,	O
broadcasting	O
it	O
’s	O
value	O
as	O
‘	O
‘	O
7	O
’	O
’	O
,	O
and	O
informing	O
those	O
cells	O
not	O
to	O
take	O
the	O
value	O
‘	O
‘	O
7	O
’	O
’	O
.	O

In	O
an	O
iteration	O
,	O
these	O
messages	O
are	O
sent	O
simultaneously	O
,	O
in	O
parallel	Method
,	O
between	O
all	O
cells	O
.	O

Each	O
cell	O
should	O
then	O
consider	O
all	O
incoming	O
messages	O
,	O
and	O
update	O
its	O
internal	O
state	O
to	O
.	O

With	O
the	O
updated	O
state	O
each	O
cell	O
should	O
send	O
out	O
new	O
messages	O
,	O
and	O
the	O
process	O
repeats	O
.	O

paragraph	O
:	O
Message	Task
passing	Task
on	O
a	O
graph	O
.	O

The	O
recurrent	Method
relational	Method
network	Method
will	O
learn	O
to	O
pass	O
messages	O
on	O
a	O
graph	O
.	O

For	O
Sudoku	O
,	O
the	O
graph	O
has	O
nodes	O
,	O
one	O
for	O
each	O
cell	O
in	O
the	O
Sudoku	O
.	O

Each	O
node	O
has	O
an	O
input	O
feature	O
vector	O
,	O
and	O
edges	O
to	O
and	O
from	O
all	O
nodes	O
that	O
are	O
in	O
the	O
same	O
row	O
,	O
column	O
and	O
box	O
in	O
the	O
Sudoku	O
.	O

The	O
graph	O
is	O
the	O
input	O
to	O
the	O
relational	Method
reasoning	Method
module	Method
,	O
and	O
vectors	O
would	O
generally	O
be	O
the	O
output	O
of	O
a	O
perceptual	Method
front	Method
-	Method
end	Method
,	O
for	O
instance	O
a	O
convolutional	Method
neural	Method
network	Method
.	O

Keeping	O
with	O
our	O
Sudoku	O
example	O
,	O
each	O
encodes	O
the	O
initial	O
cell	O
content	O
(	O
empty	O
or	O
given	O
)	O
and	O
the	O
row	O
and	O
column	O
position	O
of	O
the	O
cell	O
.	O

At	O
each	O
step	O
each	O
node	O
has	O
a	O
hidden	O
state	O
vector	O
,	O
which	O
is	O
initialized	O
to	O
the	O
features	O
,	O
such	O
that	O
.	O

At	O
each	O
step	O
,	O
each	O
node	O
sends	O
a	O
message	O
to	O
each	O
of	O
its	O
neighboring	O
nodes	O
.	O

We	O
define	O
the	O
message	O
from	O
node	O
to	O
node	O
at	O
step	O
by	O
where	O
,	O
the	O
message	Method
function	Method
,	O
is	O
a	O
multi	Method
-	Method
layer	Method
perceptron	Method
.	O

This	O
allows	O
the	O
network	O
to	O
learn	O
what	O
kind	O
of	O
messages	O
to	O
send	O
.	O

In	O
our	O
experiments	O
,	O
MLPs	Method
with	O
linear	O
outputs	O
were	O
used	O
.	O

Since	O
a	O
node	O
needs	O
to	O
consider	O
all	O
the	O
incoming	O
messages	O
we	O
sum	O
them	O
with	O
where	O
are	O
all	O
the	O
nodes	O
that	O
have	O
an	O
edge	O
into	O
node	O
.	O

For	O
Sudoku	O
,	O
contains	O
the	O
nodes	O
in	O
the	O
same	O
row	O
,	O
column	O
and	O
box	O
as	O
.	O

In	O
our	O
experiments	O
,	O
since	O
the	O
messages	O
in	O
(	O
[	O
reference	O
]	O
)	O
are	O
linear	O
,	O
this	O
is	O
similar	O
to	O
how	O
log	O
-	O
probabilities	O
are	O
summed	O
in	O
belief	Method
propagation	Method
murphy1999loopy	O
.	O

paragraph	O
:	O
Recurrent	O
node	O
updates	O
.	O

Finally	O
we	O
update	O
the	O
node	O
hidden	O
state	O
via	O
where	O
,	O
the	O
node	O
function	O
,	O
is	O
another	O
learned	Method
neural	Method
network	Method
.	O

The	O
dependence	O
on	O
the	O
previous	O
node	O
hidden	O
state	O
allows	O
the	O
network	O
to	O
iteratively	O
work	O
towards	O
a	O
solution	O
instead	O
of	O
starting	O
with	O
a	O
blank	O
slate	O
at	O
every	O
step	O
.	O

Injecting	O
the	O
feature	O
vector	O
at	O
each	O
step	O
like	O
this	O
allows	O
the	O
node	O
function	O
to	O
focus	O
on	O
the	O
messages	O
from	O
the	O
other	O
nodes	O
instead	O
of	O
trying	O
to	O
remember	O
the	O
input	O
.	O

paragraph	O
:	O
Supervised	Method
training	Method
.	O

The	O
above	O
equations	O
for	O
sending	Task
messages	Task
and	O
updating	Task
node	Task
states	O
define	O
a	O
recurrent	Method
relational	Method
network	Method
’s	O
core	O
.	O

To	O
train	O
a	O
recurrent	Method
relational	Method
network	Method
in	O
a	O
supervised	Task
manner	Task
to	O
solve	O
a	O
Sudoku	O
we	O
introduce	O
an	O
output	O
probability	O
distribution	O
over	O
the	O
digits	O
1	O
-	O
9	O
for	O
each	O
of	O
the	O
nodes	O
in	O
the	O
graph	O
.	O

The	O
output	O
distribution	O
for	O
node	O
at	O
step	O
is	O
given	O
by	O
where	O
is	O
a	O
MLP	Method
that	O
maps	O
the	O
node	O
hidden	O
state	O
to	O
the	O
output	O
probabilities	O
,	O
e.g.	O
using	O
a	O
softmax	O
nonlinearity	O
.	O

Given	O
the	O
target	O
digits	O
the	O
loss	O
at	O
step	O
,	O
is	O
then	O
the	O
sum	O
of	O
cross	O
-	O
entropy	O
terms	O
,	O
one	O
for	O
each	O
node	O
:	O
,	O
where	O
is	O
the	O
’	O
th	O
component	O
of	O
.	O

Equations	O
(	O
[	O
reference	O
]	O
)	O
to	O
(	O
[	O
reference	O
]	O
)	O
are	O
illustrated	O
in	O
figure	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Convergent	Task
message	Task
passing	Task
.	O

A	O
distinctive	O
feature	O
of	O
our	O
proposed	O
model	O
is	O
that	O
we	O
minimize	O
the	O
cross	O
entropy	O
between	O
the	O
output	O
and	O
target	O
distributions	O
at	O
every	O
step	O
.	O

At	O
test	O
time	O
we	O
only	O
consider	O
the	O
output	O
probabilities	O
at	O
the	O
last	O
step	O
,	O
but	O
having	O
a	O
loss	O
at	O
every	O
step	O
during	O
training	O
is	O
beneficial	O
.	O

Since	O
the	O
target	O
digits	O
are	O
constant	O
over	O
the	O
steps	O
,	O
it	O
encourages	O
the	O
network	O
to	O
learn	O
a	O
convergent	Method
message	Method
passing	Method
algorithm	Method
.	O

Secondly	O
,	O
it	O
helps	O
with	O
the	O
vanishing	Task
gradient	Task
problem	Task
.	O

paragraph	O
:	O
Variations	O
.	O

If	O
the	O
edges	O
are	O
unknown	O
,	O
the	O
graph	O
can	O
be	O
assumed	O
to	O
be	O
fully	O
connected	O
.	O

In	O
this	O
case	O
the	O
network	O
will	O
need	O
to	O
learn	O
which	O
objects	O
interact	O
with	O
each	O
other	O
.	O

If	O
the	O
edges	O
have	O
attributes	O
,	O
,	O
the	O
message	O
function	O
in	O
equation	O
[	O
reference	O
]	O
can	O
be	O
modified	O
such	O
that	O
.	O

If	O
the	O
output	O
of	O
interest	O
is	O
for	O
the	O
whole	O
graph	O
instead	O
of	O
for	O
each	O
node	O
the	O
output	O
in	O
equation	O
[	O
reference	O
]	O
can	O
be	O
modified	O
such	O
that	O
there	O
’s	O
a	O
single	O
output	O
.	O

The	O
loss	O
can	O
be	O
modified	O
accordingly	O
.	O

section	O
:	O
Experiments	O
Code	O
to	O
reproduce	O
all	O
experiments	O
can	O
be	O
found	O
at	O
.	O

subsection	O
:	O
bAbI	Material
question	O
-	O
answering	O
tasks	O
bAbI	Material
is	O
a	O
text	O
based	O
QA	O
dataset	O
from	O
Facebook	O
weston2015towards	O
designed	O
as	O
a	O
set	O
of	O
prerequisite	Task
tasks	Task
for	O
reasoning	Task
.	O

It	O
consists	O
of	O
20	O
types	O
of	O
tasks	O
,	O
with	O
10	O
,	O
000	O
questions	O
each	O
,	O
including	O
deduction	Task
,	O
induction	Task
,	O
spatial	Task
and	Task
temporal	Task
reasoning	Task
.	O

Each	O
question	O
,	O
e.g.	O
‘	O
‘	O
Where	O
is	O
the	O
milk	O
?	O
’	O
’	O
is	O
preceded	O
by	O
a	O
number	O
of	O
facts	O
in	O
the	O
form	O
of	O
short	O
sentences	O
,	O
e.g.	O
‘	O
‘	O
Daniel	O
journeyed	O
to	O
the	O
garden	O
.	O

Daniel	O
put	O
down	O
the	O
milk	O
.	O

’	O
’	O
The	O
target	O
is	O
a	O
single	O
word	O
,	O
in	O
this	O
case	O
‘	O
‘	O
garden	O
’	O
’	O
,	O
one	O
-	O
hot	O
encoded	O
over	O
the	O
full	O
bAbI	Material
vocabulary	O
of	O
177	O
words	O
.	O

A	O
task	O
is	O
considered	O
solved	O
if	O
a	O
model	O
achieves	O
greater	O
than	O
95	O
%	O
accuracy	Metric
.	O

The	O
most	O
difficult	O
tasks	O
require	O
reasoning	O
about	O
three	O
facts	O
.	O

To	O
map	O
the	O
questions	O
into	O
a	O
graph	O
we	O
treat	O
the	O
facts	O
related	O
to	O
a	O
question	O
as	O
the	O
nodes	O
in	O
a	O
fully	O
connected	O
graph	O
up	O
to	O
a	O
maximum	O
of	O
the	O
last	O
20	O
facts	O
.	O

The	O
fact	O
and	O
question	O
sentences	O
are	O
both	O
encoded	O
by	O
Long	Method
Short	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
hochreiter1997long	O
layers	Method
with	O
32	O
hidden	O
units	O
each	O
.	O

We	O
concatenate	O
the	O
last	O
hidden	O
state	O
of	O
each	O
LSTM	Method
and	O
pass	O
that	O
through	O
a	O
MLP	Method
.	O

The	O
output	O
is	O
considered	O
the	O
node	O
features	O
.	O

Following	O
santoro2017simple	O
all	O
edge	O
features	O
are	O
set	O
to	O
the	O
question	Method
encoding	Method
.	O

We	O
train	O
the	O
network	O
for	O
three	O
steps	O
.	O

At	O
each	O
step	O
,	O
we	O
sum	O
the	O
node	O
hidden	O
states	O
and	O
pass	O
that	O
through	O
a	O
MLP	Method
to	O
get	O
a	O
single	O
output	O
for	O
the	O
whole	O
graph	O
.	O

For	O
details	O
see	O
the	O
supplementary	O
material	O
.	O

Our	O
trained	O
network	O
solves	O
20	O
of	O
20	O
tasks	O
in	O
13	O
out	O
of	O
15	O
runs	O
.	O

This	O
is	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
and	O
markedly	O
more	O
stable	O
than	O
competing	O
methods	O
.	O

See	O
table	O
[	O
reference	O
]	O
.	O

We	O
perform	O
ablation	O
experiment	O
to	O
see	O
which	O
parts	O
of	O
the	O
model	O
are	O
important	O
,	O
including	O
varying	O
the	O
number	O
of	O
steps	O
.	O

We	O
find	O
that	O
using	O
dropout	Method
and	O
appending	O
the	O
question	Method
encoding	Method
to	O
the	O
fact	Method
encodings	Method
is	O
important	O
for	O
the	O
performance	O
.	O

See	O
the	O
supplementary	O
material	O
for	O
details	O
.	O

Surprisingly	O
,	O
we	O
find	O
that	O
we	O
only	O
need	O
a	O
single	O
step	O
of	O
relational	Method
reasoning	Method
to	O
solve	O
all	O
the	O
bAbI	Material
tasks	O
.	O

This	O
is	O
surprising	O
since	O
the	O
hardest	O
tasks	O
requires	O
reasoning	O
about	O
three	O
facts	O
.	O

It	O
’s	O
possible	O
that	O
there	O
are	O
superficial	O
correlations	O
in	O
the	O
tasks	O
that	O
the	O
model	O
learns	O
to	O
exploit	O
.	O

Alternatively	O
the	O
model	O
learns	O
to	O
compress	O
all	O
the	O
relevant	O
fact	O
-	O
relations	O
into	O
the	O
128	O
floats	O
resulting	O
from	O
the	O
sum	O
over	O
the	O
node	O
hidden	O
states	O
,	O
and	O
perform	O
the	O
remaining	O
reasoning	O
steps	O
in	O
the	O
output	O
MLP	Method
.	O

Regardless	O
,	O
it	O
appears	O
multiple	O
steps	O
of	O
relational	Method
reasoning	Method
are	O
not	O
important	O
for	O
the	O
bAbI	Material
dataset	O
.	O

subsection	O
:	O
Pretty	Material
-	Material
CLEVR	Material
Given	O
that	O
bAbI	Material
did	O
not	O
require	O
multiple	O
steps	O
of	O
relational	Method
reasoning	Method
and	O
in	O
order	O
to	O
test	O
our	O
hypothesis	O
that	O
our	O
proposed	O
model	O
is	O
better	O
suited	O
for	O
tasks	O
requiring	O
more	O
steps	O
of	O
relational	Method
reasoning	Method
we	O
create	O
a	O
diagnostic	O
dataset	O
‘	O
‘	O
Pretty	O
-	O
CLEVER	O
’	O
’	O
.	O

It	O
can	O
be	O
seen	O
as	O
an	O
extension	O
of	O
the	O
‘	O
‘	O
Sort	Material
-	Material
of	Material
-	Material
CLEVR	Material
’	O
’	O
data	O
set	O
by	O
santoro2017simple	O
which	O
has	O
questions	O
of	O
a	O
non	O
-	O
relational	O
and	O
relational	O
nature	O
.	O

‘	O
‘	O
Pretty	Material
-	Material
CLEVR	Material
’	O
’	O
takes	O
this	O
a	O
step	O
further	O
and	O
has	O
non	O
-	O
relational	O
questions	O
as	O
well	O
as	O
questions	O
requiring	O
varying	O
degrees	O
of	O
relational	Method
reasoning	Method
.	O

position	O
=	O
bottom	O
Samples.	O
[	O
.24	O
]	O
Results	O
.	O

Pretty	Material
-	Material
CLEVR	Material
consists	O
of	O
scenes	O
with	O
eight	O
colored	O
shapes	O
and	O
associated	O
questions	O
.	O

Questions	O
are	O
of	O
the	O
form	O
:	O
‘	O
‘	O
Starting	O
at	O
object	O
X	O
which	O
object	O
is	O
N	O
jumps	O
away	O
?	O
’	O
’	O
.	O

Objects	O
are	O
uniquely	O
defined	O
by	O
their	O
color	O
or	O
shape	O
.	O

If	O
the	O
start	O
object	O
is	O
defined	O
by	O
color	O
,	O
the	O
answer	O
is	O
a	O
shape	O
,	O
and	O
vice	O
versa	O
.	O

Jumps	O
are	O
defined	O
as	O
moving	O
to	O
the	O
closest	O
object	O
,	O
without	O
going	O
to	O
an	O
object	O
already	O
visited	O
.	O

See	O
figure	O
[	O
reference	O
]	O
.	O

Questions	O
with	O
zero	O
jumps	O
are	O
non	O
-	O
relational	O
and	O
correspond	O
to	O
:	O
‘	O
‘	O
What	O
color	O
is	O
shape	O
X	O
?	O
’	O
’	O
or	O
‘	O
‘	O
What	O
shape	O
is	O
color	O
X	O
?	O
’	O
’	O
.	O

We	O
create	O
100	O
,	O
000	O
random	O
scenes	O
,	O
and	O
128	O
questions	O
for	O
each	O
(	O
8	O
start	O
objects	O
,	O
0	O
-	O
7	O
jumps	O
,	O
output	O
is	O
color	O
or	O
shape	O
)	O
,	O
resulting	O
in	O
12.8	O
M	O
questions	O
.	O

We	O
also	O
render	O
the	O
scenes	O
as	O
images	O
.	O

The	O
‘	O
‘	O
jump	O
to	O
nearest	O
’	O
’	O
type	O
question	O
is	O
chosen	O
in	O
an	O
effort	O
to	O
eliminate	O
simple	O
correlations	O
between	O
the	O
scene	O
state	O
and	O
the	O
answer	O
.	O

It	O
is	O
highly	O
non	O
-	O
linear	O
in	O
the	O
sense	O
that	O
slight	O
differences	O
in	O
the	O
distance	O
between	O
objects	O
can	O
cause	O
the	O
answer	O
to	O
change	O
drastically	O
.	O

It	O
is	O
also	O
asymmetrical	O
,	O
i.e.	O
if	O
the	O
question	O
‘	O
‘	O
x	O
,	O
n	O
jumps	O
’	O
’	O
equals	O
‘	O
‘	O
y	O
’	O
’	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
‘	O
‘	O
y	O
,	O
n	O
jumps	O
’	O
’	O
equals	O
‘	O
‘	O
x	O
’	O
’	O
.	O

We	O
find	O
it	O
is	O
a	O
surprisingly	O
difficult	O
task	O
to	O
solve	O
,	O
even	O
with	O
a	O
powerful	O
model	O
such	O
as	O
the	O
RRN	Method
.	O

We	O
hope	O
others	O
will	O
use	O
it	O
to	O
evaluate	O
their	O
relational	Method
models	Method
.	O

Since	O
we	O
are	O
solely	O
interested	O
in	O
examining	O
the	O
effect	O
of	O
multiple	O
steps	O
of	O
relational	Method
reasoning	Method
we	O
train	O
on	O
the	O
state	O
descriptions	O
of	O
the	O
scene	O
.	O

We	O
consider	O
each	O
scene	O
as	O
a	O
fully	O
connected	O
undirected	O
graph	O
with	O
8	O
nodes	O
.	O

The	O
feature	O
vector	O
for	O
each	O
object	O
consists	O
of	O
the	O
position	O
,	O
shape	O
and	O
color	O
.	O

We	O
encode	O
the	O
question	O
as	O
the	O
start	O
object	O
shape	O
or	O
color	O
and	O
the	O
number	O
of	O
jumps	O
.	O

As	O
we	O
did	O
for	O
bAbI	Material
we	O
concatenate	O
the	O
question	O
and	O
object	O
features	O
and	O
pass	O
it	O
through	O
a	O
MLP	Method
to	O
get	O
the	O
node	O
features	O
.	O

To	O
make	O
the	O
task	O
easier	O
we	O
set	O
the	O
edge	O
features	O
to	O
the	O
euclidean	O
distance	O
between	O
the	O
objects	O
.	O

We	O
train	O
our	O
network	O
for	O
four	O
steps	O
and	O
compare	O
to	O
a	O
single	Method
step	Method
relational	Method
network	Method
and	O
a	O
baseline	O
MLP	Method
that	O
considers	O
the	O
entire	O
scene	O
state	O
,	O
all	O
pairwise	O
distances	O
,	O
and	O
the	O
question	O
as	O
a	O
single	O
vector	O
.	O

For	O
details	O
see	O
the	O
supplementary	O
material	O
.	O

Mirroring	O
the	O
results	O
from	O
the	O
‘	O
‘	O
Sort	Material
-	Material
of	Material
-	Material
CLEVR	Material
’	O
’	O
dataset	O
the	O
MLP	Method
perfectly	O
solves	O
the	O
non	O
-	O
relational	O
questions	O
,	O
but	O
struggle	O
with	O
even	O
single	O
jump	O
questions	O
and	O
seem	O
to	O
lower	O
bound	O
the	O
performance	O
of	O
the	O
relational	Method
networks	Method
.	O

The	O
relational	Method
network	Method
solves	O
the	O
non	Task
-	Task
relational	Task
questions	Task
as	O
well	O
as	O
the	O
ones	O
requiring	O
a	O
single	O
jump	O
,	O
but	O
the	O
accuracy	Metric
sharply	O
drops	O
off	O
with	O
more	O
jumps	O
.	O

This	O
matches	O
the	O
performance	O
of	O
the	O
recurrent	Method
relational	Method
network	Method
which	O
generally	O
performs	O
well	O
as	O
long	O
as	O
the	O
number	O
of	O
steps	O
is	O
greater	O
than	O
or	O
equal	O
to	O
the	O
number	O
of	O
jumps	O
.	O

See	O
fig	O
[	O
reference	O
]	O
.	O

It	O
seems	O
that	O
,	O
despite	O
our	O
best	O
efforts	O
,	O
there	O
are	O
spurious	O
correlations	O
in	O
the	O
data	O
such	O
that	O
questions	O
with	O
six	O
to	O
seven	O
jumps	O
are	O
easier	O
to	O
solve	O
than	O
those	O
with	O
four	O
to	O
five	O
jumps	O
.	O

subsection	O
:	O
Sudoku	O
We	O
create	O
training	O
,	O
validation	O
and	O
testing	O
sets	O
totaling	O
216	O
,	O
000	O
Sudoku	Material
puzzles	Material
with	O
a	O
uniform	O
distribution	O
of	O
givens	O
between	O
17	O
and	O
34	O
.	O

We	O
consider	O
each	O
of	O
the	O
81	O
cells	O
in	O
the	O
9x9	O
Sudoku	O
grid	O
a	O
node	O
in	O
a	O
graph	O
,	O
with	O
edges	O
to	O
and	O
from	O
each	O
other	O
cell	O
in	O
the	O
same	O
row	O
,	O
column	O
and	O
box	O
.	O

The	O
node	O
features	O
are	O
the	O
output	O
of	O
a	O
MLP	Method
which	O
takes	O
as	O
input	O
the	O
digit	O
for	O
the	O
cell	O
(	O
0	O
-	O
9	O
,	O
0	O
if	O
not	O
given	O
)	O
,	O
and	O
the	O
row	O
and	O
column	O
position	O
(	O
1	O
-	O
9	O
)	O
.	O

Edge	O
features	O
are	O
not	O
used	O
.	O

We	O
run	O
the	O
network	O
for	O
32	O
steps	O
and	O
at	O
every	O
step	O
the	O
output	O
function	O
maps	O
each	O
node	O
hidden	O
state	O
to	O
nine	O
output	O
logits	O
corresponding	O
to	O
the	O
nine	O
possible	O
digits	O
.	O

For	O
details	O
see	O
the	O
supplementary	O
material	O
.	O

Our	O
network	O
learns	O
to	O
solve	O
94.1	O
%	O
of	O
even	O
the	O
hardest	O
17	O
-	O
givens	O
Sudokus	O
after	O
32	O
steps	O
.	O

We	O
only	O
consider	O
a	O
puzzled	O
solved	O
if	O
all	O
the	O
digits	O
are	O
correct	O
,	O
i.e.	O
no	O
partial	O
credit	O
is	O
given	O
for	O
getting	O
individual	O
digits	O
correct	O
.	O

For	O
more	O
givens	O
the	O
accuracy	Metric
(	O
fraction	O
of	O
test	O
puzzles	O
solved	O
)	O
quickly	O
approaches	O
100	O
%	O
.	O

Since	O
the	O
network	O
outputs	O
a	O
probability	O
distribution	O
for	O
each	O
step	O
,	O
we	O
can	O
visualize	O
how	O
the	O
network	O
arrives	O
at	O
the	O
solution	O
step	O
by	O
step	O
.	O

For	O
an	O
example	O
of	O
this	O
see	O
figure	O
[	O
reference	O
]	O
.	O

To	O
examine	O
our	O
hypothesis	O
that	O
multiple	O
steps	O
are	O
required	O
we	O
plot	O
the	O
accuracy	Metric
as	O
a	O
function	O
of	O
the	O
number	O
of	O
steps	O
.	O

See	O
figure	O
[	O
reference	O
]	O
.	O

We	O
can	O
see	O
that	O
even	O
simple	O
Sudokus	Method
with	O
33	O
givens	O
require	O
upwards	O
of	O
10	O
steps	O
of	O
relational	Method
reasoning	Method
,	O
whereas	O
the	O
harder	O
17	O
givens	O
continue	O
to	O
improve	O
even	O
after	O
32	O
steps	O
.	O

Figure	O
[	O
reference	O
]	O
also	O
shows	O
that	O
the	O
model	O
has	O
learned	O
a	O
convergent	Method
algorithm	Method
.	O

The	O
model	O
was	O
trained	O
for	O
32	O
steps	O
,	O
but	O
seeing	O
that	O
the	O
accuracy	Metric
increased	O
with	O
more	O
steps	O
,	O
we	O
ran	O
the	O
model	O
for	O
64	O
steps	O
during	O
testing	O
.	O

At	O
64	O
steps	O
the	O
accuracy	Metric
for	O
the	O
17	O
givens	O
puzzles	O
increases	O
to	O
96.6	O
%	O
.	O

We	O
also	O
examined	O
the	O
importance	O
of	O
the	O
row	O
and	O
column	O
features	O
by	O
multiplying	O
the	O
row	O
and	O
column	O
embeddings	O
by	O
zero	O
and	O
re	O
-	O
tested	O
our	O
trained	O
network	O
.	O

At	O
64	O
steps	O
with	O
17	O
givens	O
,	O
the	O
accuracy	Metric
changed	O
to	O
96.7	O
%	O
.	O

It	O
thus	O
seems	O
the	O
network	O
does	O
not	O
use	O
the	O
row	O
and	O
column	O
position	O
information	O
to	O
solve	O
the	O
task	O
.	O

We	O
compare	O
our	O
network	O
to	O
several	O
other	O
differentiable	Method
methods	Method
.	O

See	O
table	O
[	O
reference	O
]	O
.	O

We	O
train	O
two	O
relational	Method
networks	Method
:	O
a	O
node	Method
and	O
a	O
graph	Method
centric	Method
.	O

For	O
details	O
see	O
the	O
supplementary	O
material	O
.	O

Of	O
the	O
two	O
,	O
the	O
node	O
centric	O
was	O
considerably	O
better	O
.	O

The	O
node	O
centric	O
correspond	O
exactly	O
to	O
our	O
proposed	O
network	O
with	O
a	O
single	O
step	O
,	O
yet	O
fails	O
to	O
solve	O
any	O
Sudoku	O
.	O

This	O
shows	O
that	O
multiple	O
steps	O
are	O
crucial	O
for	O
complex	O
relational	Method
reasoning	Method
.	O

Our	O
network	O
outperforms	O
loopy	Method
belief	Method
propagation	Method
,	O
with	O
parallel	Method
and	O
random	Method
messages	Method
passing	O
updates	O
bauke2008passing	O
.	O

It	O
also	O
outperforms	O
a	O
version	O
of	O
loopy	Method
belief	Method
propagation	Method
modified	O
specifically	O
for	O
solving	O
Sudokus	Task
that	O
uses	O
250	O
steps	O
,	O
Sinkhorn	O
balancing	O
every	O
two	O
steps	O
and	O
iteratively	O
picks	O
the	O
most	O
probable	O
digit	O
khan2014solving	O
.	O

We	O
also	O
compare	O
to	O
learning	O
the	O
messages	O
in	O
parallel	Method
loopy	O
BP	O
as	O
presented	O
in	O
lin2015deeply	O
.	O

We	O
tried	O
a	O
few	O
variants	O
including	O
a	O
single	O
step	O
as	O
presented	O
and	O
32	O
steps	O
with	O
and	O
without	O
a	O
loss	O
on	O
every	O
step	O
,	O
but	O
could	O
not	O
get	O
it	O
to	O
solve	O
any	O
17	O
given	O
Sudokus	O
.	O

Finally	O
we	O
outperform	O
park2016can	O
which	O
treats	O
the	O
Sudoku	Material
as	O
a	O
9x9	O
image	O
,	O
uses	O
10	O
convolutional	O
layers	Method
,	O
iteratively	O
picks	O
the	O
most	O
probable	O
digit	O
,	O
and	O
evaluate	O
on	O
easier	O
Sudokus	Material
with	O
24	O
-	O
36	O
givens	O
.	O

We	O
also	O
tried	O
to	O
train	O
a	O
version	O
of	O
our	O
network	O
that	O
only	O
had	O
a	O
loss	O
at	O
the	O
last	O
step	O
.	O

It	O
was	O
harder	O
to	O
train	O
,	O
performed	O
worse	O
and	O
did	O
n’t	O
learn	O
a	O
convergent	Method
algorithm	Method
.	O

subsection	O
:	O
Age	O
arithmetic	O
Anonymous	O
reviewer	O
2	O
suggested	O
the	O
following	O
task	O
which	O
we	O
include	O
here	O
.	O

The	O
task	O
is	O
to	O
infer	O
the	O
age	O
of	O
a	O
person	O
given	O
a	O
single	O
absolute	O
age	O
and	O
a	O
set	O
of	O
age	O
differences	O
,	O
e.g.	O
‘	O
‘	O
Alice	O
is	O
20	O
years	O
old	O
.	O

Alice	O
is	O
4	O
years	O
older	O
than	O
Bob	O
.	O

Charlie	O
is	O
6	O
years	O
younger	O
than	O
Bob	O
.	O

How	O
old	O
is	O
Charlie	O
?	O
’	O
’	O
.	O

Please	O
see	O
the	O
supplementary	O
material	O
for	O
details	O
on	O
the	O
task	O
and	O
results	O
.	O

section	O
:	O
Discussion	O
We	O
have	O
proposed	O
a	O
general	O
relational	Method
reasoning	Method
model	Method
for	O
solving	O
tasks	O
requiring	O
an	O
order	O
of	O
magnitude	O
more	O
complex	O
relational	Method
reasoning	Method
than	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
art	O
.	O

BaBi	Material
and	O
Sort	Material
-	Material
of	Material
-	Material
CLEVR	Material
require	O
a	O
few	O
steps	O
,	O
Pretty	Material
-	Material
CLEVR	Material
requires	O
up	O
to	O
eight	O
steps	O
and	O
Sudoku	O
requires	O
more	O
than	O
ten	O
steps	O
.	O

Our	O
relational	Method
reasoning	Method
module	Method
can	O
be	O
added	O
to	O
any	O
deep	Method
learning	Method
model	Method
to	O
add	O
a	O
powerful	O
relational	Method
reasoning	Method
capacity	Method
.	O

We	O
get	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
Sudokus	Task
solving	O
96.6	O
%	O
of	O
the	O
hardest	O
Sudokus	Material
with	O
17	O
givens	O
.	O

We	O
also	O
markedly	O
improve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
BaBi	Material
dataset	O
solving	O
20	O
/	O
20	O
tasks	O
in	O
13	O
out	O
of	O
15	O
runs	O
with	O
a	O
single	O
model	O
trained	O
jointly	O
on	O
all	O
tasks	O
.	O

One	O
potential	O
issue	O
with	O
having	O
a	O
loss	O
at	O
every	O
step	O
is	O
that	O
it	O
might	O
encourage	O
the	O
network	O
to	O
learn	O
a	O
greedy	Method
algorithm	Method
that	O
gets	O
stuck	O
in	O
a	O
local	O
minima	O
.	O

However	O
,	O
the	O
output	O
function	O
separates	O
the	O
node	O
hidden	O
states	O
and	O
messages	O
from	O
the	O
output	O
probability	O
distributions	O
.	O

The	O
network	O
therefore	O
has	O
the	O
capacity	O
to	O
use	O
a	O
small	O
part	O
of	O
the	O
hidden	O
state	O
for	O
retaining	O
a	O
current	O
best	O
guess	O
,	O
which	O
can	O
remain	O
constant	O
over	O
several	O
steps	O
,	O
and	O
other	O
parts	O
of	O
the	O
hidden	O
state	O
for	O
running	O
a	O
non	Method
-	Method
greedy	Method
multi	Method
-	Method
step	Method
algorithm	Method
.	O

Sending	O
messages	O
for	O
all	O
nodes	O
in	O
parallel	Method
and	O
summing	O
all	O
the	O
incoming	O
messages	O
might	O
seem	O
like	O
an	O
unsophisticated	O
approach	O
that	O
risk	O
resulting	O
in	O
oscillatory	O
behavior	O
and	O
drowning	O
out	O
the	O
important	O
messages	O
.	O

However	O
,	O
since	O
the	O
receiving	O
node	O
hidden	O
state	O
is	O
an	O
input	O
to	O
the	O
message	O
function	O
,	O
the	O
receiving	O
node	O
can	O
in	O
a	O
sense	O
determine	O
which	O
messages	O
it	O
wishes	O
to	O
receive	O
.	O

As	O
such	O
,	O
the	O
sum	Method
can	O
be	O
seen	O
as	O
an	O
implicit	Method
attention	Method
mechanism	Method
over	O
the	O
incoming	O
messages	O
.	O

Similarly	O
the	O
network	O
can	O
learn	O
an	O
optimal	O
message	Method
passing	Method
schedule	Method
,	O
by	O
ignoring	O
messages	O
based	O
on	O
the	O
history	O
and	O
current	O
state	O
of	O
the	O
receiving	O
and	O
sending	O
node	O
.	O

section	O
:	O
Related	O
work	O
Relational	Method
networks	Method
santoro2017simple	O
and	O
interaction	Method
networks	Method
battaglia2016interaction	O
are	O
the	O
most	O
directly	O
comparable	O
to	O
ours	O
.	O

These	O
models	O
correspond	O
to	O
using	O
a	O
single	O
step	O
of	O
equation	O
[	O
reference	O
]	O
.	O

Since	O
it	O
only	O
does	O
one	O
step	O
it	O
can	O
not	O
naturally	O
do	O
complex	O
multi	O
-	O
step	O
relational	Method
reasoning	Method
.	O

In	O
order	O
to	O
solve	O
the	O
tasks	O
that	O
require	O
more	O
than	O
a	O
single	O
step	O
it	O
must	O
compress	O
all	O
the	O
relevant	O
relations	O
into	O
a	O
fixed	O
size	O
vector	O
,	O
then	O
perform	O
the	O
remaining	O
relational	Method
reasoning	Method
in	O
the	O
last	O
forward	O
layers	Method
.	O

Relational	Method
networks	Method
,	O
interaction	Method
networks	Method
and	O
our	O
proposed	O
model	O
can	O
all	O
be	O
seen	O
as	O
an	O
instance	O
of	O
Graph	Method
Neural	Method
Networks	Method
scarselli2009graph	O
,	O
gilmer2017neural	O
.	O

Graph	Method
neural	Method
networks	Method
with	O
message	Method
passing	Method
computations	Method
go	O
back	O
to	O
.	O

However	O
,	O
there	O
are	O
key	O
differences	O
that	O
we	O
found	O
important	O
for	O
implementing	O
stable	O
multi	O
-	O
step	O
relational	Method
reasoning	Method
.	O

Including	O
the	O
node	O
features	O
at	O
every	O
step	O
in	O
eq	O
.	O

[	O
reference	O
]	O
is	O
important	O
to	O
the	O
stability	O
of	O
the	O
network	O
.	O

,	O
eq	O
.	O

3	O
has	O
the	O
node	O
features	O
,	O
,	O
inside	O
the	O
message	O
function	O
.	O

battaglia2016interaction	O
use	O
an	O
in	O
the	O
node	O
update	O
function	O
,	O
but	O
this	O
is	O
an	O
external	O
driving	O
force	O
.	O

sukhbaatar2016learning	O
also	O
proposed	O
to	O
include	O
the	O
node	O
features	O
at	O
every	O
step	O
.	O

Optimizing	O
the	O
loss	O
at	O
every	O
step	O
in	O
order	O
to	O
learn	O
a	O
convergent	Method
message	Method
passing	Method
algorithm	Method
is	O
novel	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

introduces	O
an	O
explicit	O
loss	O
term	O
to	O
ensure	O
convergence	Task
.	O

ross2011learning	O
trains	O
the	O
inference	Method
machine	Method
predictors	Method
on	O
every	O
step	O
,	O
but	O
there	O
are	O
no	O
hidden	O
states	O
;	O
the	O
node	O
states	O
are	O
the	O
output	O
marginals	O
directly	O
,	O
similar	O
to	O
how	O
belief	Method
propagation	Method
works	O
.	O

Our	O
model	O
can	O
also	O
be	O
seen	O
as	O
a	O
completely	O
learned	Method
message	Method
passing	Method
algorithm	Method
.	O

Belief	Method
propagation	Method
is	O
a	O
hand	O
-	O
crafted	O
message	Method
passing	Method
algorithm	Method
for	O
performing	O
exact	Task
inference	Task
in	O
directed	Task
acyclic	Task
graphical	Task
models	Task
.	O

If	O
the	O
graph	O
has	O
cycles	O
,	O
one	O
can	O
use	O
a	O
variant	O
,	O
loopy	Method
belief	Method
propagation	Method
,	O
but	O
it	O
is	O
not	O
guaranteed	O
to	O
be	O
exact	O
,	O
unbiased	O
or	O
converge	O
.	O

Empirically	O
it	O
works	O
well	O
though	O
and	O
it	O
is	O
widely	O
used	O
murphy1999loopy	O
.	O

Several	O
works	O
have	O
proposed	O
replacing	O
parts	O
of	O
belief	Method
propagation	Method
with	O
learned	Method
modules	Method
heess2013learning	O
,	O
lin2015deeply	O
.	O

Our	O
work	O
differs	O
by	O
not	O
being	O
rooted	O
in	O
loopy	Method
BP	Method
,	O
and	O
instead	O
learning	O
all	O
parts	O
of	O
a	O
general	O
message	Method
passing	Method
algorithm	Method
.	O

ross2011learning	O
proposes	O
Inference	Method
Machines	Method
which	O
ditch	O
the	O
belief	Method
propagation	Method
algorithm	Method
altogether	O
and	O
instead	O
train	O
a	O
series	O
of	O
regressors	Method
to	O
output	O
the	O
correct	O
marginals	O
by	O
passing	O
messages	O
on	O
a	O
graph	O
.	O

wei2016convolutional	O
applies	O
this	O
idea	O
to	O
pose	Task
estimation	Task
using	O
a	O
series	O
of	O
convolutional	O
layers	Method
and	O
deng2016structure	O
introduces	O
a	O
recurrent	Method
node	Method
update	Method
for	O
the	O
same	O
domain	O
.	O

There	O
is	O
rich	O
literature	O
on	O
combining	O
symbolic	Task
reasoning	Task
and	O
logic	Method
with	O
sub	Method
-	Method
symbolic	Method
distributed	Method
representations	Method
which	O
goes	O
all	O
the	O
way	O
back	O
to	O
the	O
birth	O
of	O
the	O
idea	O
of	O
parallel	Method
distributed	Method
processing	Method
mcculloch1943logical	O
.	O

See	O
raedt2016statistical	O
,	O
besold2017neural	O
for	O
two	O
recent	O
surveys	O
.	O

Here	O
we	O
describe	O
only	O
a	O
few	O
recent	O
methods	O
.	O

serafini2016learning	O
introduces	O
the	O
Logic	Method
Tensor	Method
Network	Method
(	O
LTN	Method
)	O
which	O
describes	O
a	O
first	O
order	O
logic	O
in	O
which	O
symbols	O
are	O
grounded	O
as	O
vector	O
embeddings	O
,	O
and	O
predicates	O
and	O
functions	O
are	O
grounded	O
as	O
tensor	Method
networks	Method
.	O

The	O
embeddings	Method
and	O
tensor	Method
networks	Method
are	O
then	O
optimized	O
jointly	O
to	O
maximize	O
a	O
fuzzy	Metric
satisfiability	Metric
measure	Metric
over	O
a	O
set	O
of	O
known	O
facts	O
and	O
fuzzy	O
constraints	O
.	O

vsourek2015lifted	O
introduces	O
the	O
Lifted	Method
Relational	Method
Network	Method
which	O
combines	O
relational	Method
logic	Method
with	O
neural	Method
networks	Method
by	O
creating	O
neural	Method
networks	Method
from	O
lifted	O
rules	O
and	O
training	O
examples	O
,	O
such	O
that	O
the	O
connections	O
between	O
neurons	O
created	O
from	O
the	O
same	O
lifted	O
rules	O
shares	O
weights	O
.	O

Our	O
approach	O
differs	O
fundamentally	O
in	O
that	O
we	O
do	O
not	O
aim	O
to	O
bridge	O
symbolic	Method
and	Method
sub	Method
-	Method
symbolic	Method
methods	Method
.	O

Instead	O
we	O
stay	O
completely	O
in	O
the	O
sub	Task
-	Task
symbolic	Task
realm	Task
.	O

We	O
do	O
not	O
introduce	O
or	O
consider	O
any	O
explicit	O
logic	O
,	O
aim	O
to	O
discover	O
(	O
fuzzy	O
)	O
logic	O
rules	O
,	O
or	O
attempt	O
to	O
include	O
prior	O
knowledge	O
in	O
the	O
form	O
of	O
logical	O
constraints	O
.	O

amos2017optnet	O
Introduces	O
OptNet	Method
,	O
a	O
neural	Method
network	Method
layer	Method
that	O
solve	O
quadratic	Task
programs	Task
using	O
an	O
efficient	O
differentiable	Method
solver	Method
.	O

OptNet	Method
is	O
trained	O
to	O
solve	O
4x4	O
Sudokus	O
amongst	O
other	O
problems	O
and	O
beats	O
the	O
deep	Method
convolutional	Method
network	Method
baseline	Method
as	O
described	O
in	O
park2016can	O
.	O

Unfortunately	O
we	O
can	O
not	O
compare	O
to	O
OptNet	Method
directly	O
as	O
it	O
has	O
computational	O
issues	O
scaling	O
to	O
9x9	O
Sudokus	O
(	O
Brandon	O
Amos	O
,	O
2018	O
,	O
personal	O
communication	O
)	O
.	O

sukhbaatar2016learning	O
proposes	O
the	O
Communication	Method
Network	Method
(	O
CommNet	Method
)	O
for	O
learning	Task
multi	Task
-	Task
agent	Task
cooperation	Task
and	Task
communication	Task
using	O
back	Method
-	Method
propagation	Method
.	O

It	O
is	O
similar	O
to	O
our	O
recurrent	Method
relational	Method
network	Method
,	O
but	O
differs	O
in	O
key	O
aspects	O
.	O

The	O
messages	O
passed	O
between	O
all	O
nodes	O
at	O
a	O
given	O
step	O
are	O
the	O
same	O
,	O
corresponding	O
to	O
the	O
average	O
of	O
all	O
the	O
node	O
hidden	O
states	O
.	O

Also	O
,	O
it	O
is	O
not	O
trained	O
to	O
minimize	O
the	O
loss	O
on	O
every	O
step	O
of	O
the	O
algorithm	O
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
’d	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
the	O
valuable	O
comments	O
and	O
suggestions	O
,	O
especially	O
reviewer	O
2	O
who	O
suggested	O
the	O
age	Task
arithmetic	Task
task	Task
.	O

This	O
research	O
was	O
supported	O
by	O
the	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
TITAN	Material
X	Material
GPUs	Material
.	O

bibliography	O
:	O
References	O
section	O
:	O
Supplementary	O
Material	O
subsection	O
:	O
bAbI	Material
experimental	O
details	O
Unless	O
otherwise	O
specified	O
we	O
use	O
128	O
hidden	O
units	O
for	O
all	O
layers	Method
and	O
all	O
MLPs	Method
are	O
3	O
ReLU	Method
layers	Method
followed	O
by	O
a	O
linear	Method
layer	Method
.	O

We	O
compute	O
each	O
node	O
feature	O
vector	O
as	O
where	O
is	O
fact	O
,	O
is	O
the	O
question	O
,	O
is	O
the	O
sentence	O
position	O
(	O
1	O
-	O
20	O
)	O
of	O
fact	O
and	O
is	O
a	O
random	O
offset	O
per	O
question	O
(	O
1	O
-	O
20	O
)	O
,	O
such	O
that	O
the	O
onehot	O
output	O
is	O
40	O
dimensional	O
.	O

The	O
offset	O
is	O
constant	O
for	O
all	O
facts	O
related	O
to	O
a	O
single	O
question	O
to	O
avoid	O
changing	O
the	O
relative	O
order	O
of	O
the	O
facts	O
.	O

The	O
random	O
offset	O
prevents	O
the	O
network	O
from	O
memorizing	O
the	O
position	O
of	O
the	O
facts	O
and	O
rather	O
reason	O
about	O
their	O
ordering	O
.	O

Our	O
message	Method
function	Method
is	O
a	O
MLP	Method
.	O

Our	O
node	Method
function	Method
uses	O
an	O
LSTM	Method
over	Method
reasoning	Method
steps	Method
where	O
is	O
the	O
cell	O
state	O
of	O
the	O
LSTM	Method
for	O
unit	O
at	O
time	O
.	O

is	O
initialized	O
to	O
zero	O
.	O

We	O
run	O
our	O
network	O
for	O
three	O
steps	O
.	O

To	O
get	O
a	O
graph	O
level	O
output	O
,	O
we	O
use	O
a	O
MLP	Method
over	O
the	O
sum	O
of	O
the	O
node	O
hidden	O
states	O
,	O
with	O
3	O
layers	Method
,	O
the	O
final	O
being	O
a	O
linear	Method
layer	Method
that	O
maps	O
to	O
the	O
output	O
logits	O
.	O

The	O
last	O
two	O
layers	Method
uses	O
dropout	Method
of	O
50	O
%	O
.	O

We	O
train	O
and	O
validate	O
on	O
all	O
20	O
tasks	O
jointly	O
using	O
the	O
9	O
,	O
000	O
training	O
and	O
1	O
,	O
000	O
validation	O
samples	O
defined	O
in	O
the	O
en_valid_10k	O
split	O
.	O

We	O
use	O
the	O
Adam	Method
optimizer	Method
with	O
a	O
batch	O
size	O
of	O
512	O
,	O
a	O
learning	Metric
rate	Metric
of	O
2e	O
-	O
4	O
and	O
L2	Method
regularization	Method
with	O
a	O
rate	O
of	O
1e	O
-	O
5	O
.	O

We	O
train	O
for	O
5	O
M	O
gradient	O
steps	O
.	O

subsection	O
:	O
bAbI	Material
ablation	O
experiments	O
To	O
test	O
which	O
parts	O
of	O
the	O
proposed	O
model	O
is	O
important	O
to	O
solving	O
the	O
bAbI	Material
tasks	O
we	O
perform	O
ablation	Task
experiments	O
.	O

One	O
of	O
the	O
main	O
differences	O
between	O
the	O
relational	Method
network	Method
and	O
our	O
proposed	O
model	O
,	O
aside	O
from	O
the	O
recurrent	Method
steps	Method
,	O
is	O
that	O
we	O
encode	O
the	O
sentences	O
and	O
question	O
together	O
.	O

We	O
ablate	O
the	O
model	O
in	O
two	O
ways	O
to	O
test	O
how	O
important	O
this	O
is	O
.	O

1	O
)	O
Using	O
a	O
single	O
linear	Method
layer	Method
instead	O
of	O
the	O
4	O
-	O
layer	O
MLP	Method
baseline	O
,	O
and	O
2	O
)	O
Not	O
encoding	O
them	O
together	O
.	O

In	O
this	O
case	O
the	O
node	O
hidden	O
states	O
are	O
initialized	O
to	O
the	O
fact	Method
encodings	Method
.	O

We	O
found	O
dropout	O
to	O
be	O
important	O
,	O
so	O
we	O
also	O
perform	O
an	O
ablation	Task
experiment	O
without	O
dropout	Method
.	O

We	O
run	O
each	O
ablation	O
experiment	O
eight	O
times	O
.	O

We	O
also	O
do	O
pseudo	Task
-	Task
ablation	Task
experiments	O
with	O
fewer	O
steps	O
by	O
measuring	O
at	O
each	O
step	O
of	O
the	O
RRN	Method
.	O

See	O
table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Pretty	Material
-	Material
CLEVR	Material
experimental	O
details	O
Our	O
setup	O
for	O
Pretty	Material
-	Material
CLEVR	Material
is	O
a	O
bit	O
simpler	O
than	O
for	O
bAbI.	Material
Unless	O
otherwise	O
specified	O
we	O
use	O
128	O
hidden	O
units	O
for	O
all	O
hidden	O
layers	Method
and	O
all	O
MLPs	Method
are	O
1	O
ReLU	Method
layer	Method
followed	O
by	O
a	O
linear	Method
layer	Method
.	O

We	O
compute	O
each	O
node	O
feature	O
vector	O
as	O
where	O
is	O
the	O
position	O
,	O
,	O
is	O
the	O
color	O
,	O
is	O
the	O
marker	O
,	O
is	O
the	O
marker	O
or	O
color	O
of	O
the	O
start	O
object	O
,	O
and	O
is	O
the	O
number	O
of	O
jumps	O
.	O

Our	O
message	Method
function	Method
is	O
a	O
MLP	Method
.	O

Our	O
node	Method
function	Method
is	O
,	O
Our	O
output	Method
function	Method
is	O
a	O
MLP	Method
with	O
a	O
dropout	O
fraction	O
of	O
0.5	O
in	O
the	O
penultimate	O
layer	O
.	O

The	O
last	O
layer	O
has	O
16	O
hidden	O
linear	O
units	O
.	O

We	O
run	O
our	O
recurrent	Method
relational	Method
network	Method
for	O
4	O
steps	O
.	O

We	O
train	O
on	O
the	O
12.8	O
M	O
training	O
questions	O
,	O
and	O
augment	O
the	O
data	O
by	O
scaling	O
and	O
rotating	O
the	O
scenes	O
randomly	O
.	O

We	O
use	O
separate	O
validation	O
and	O
test	O
sets	O
of	O
128.000	O
questions	O
each	O
.	O

We	O
use	O
the	O
Adam	Method
optimizer	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
1e	O
-	O
4	O
and	O
train	O
for	O
10	O
M	Method
gradient	Method
updates	Method
with	O
a	O
batch	O
size	O
of	O
128	O
.	O

The	O
baseline	O
RN	O
is	O
identical	O
to	O
the	O
described	O
RRN	Method
,	O
except	O
it	O
only	O
does	O
a	O
single	O
step	O
of	O
relational	Method
reasoning	Method
.	O

The	O
baseline	O
MLP	Method
takes	O
the	O
entire	O
scene	O
state	O
,	O
,	O
as	O
an	O
input	O
,	O
such	O
that	O
where	O
is	O
the	O
euclidean	O
distance	O
from	O
object	O
to	O
.	O

The	O
baseline	O
MLP	Method
has	O
4	O
ReLu	Method
layers	Method
with	O
256	O
hidden	O
units	O
,	O
with	O
dropout	O
of	O
0.5	O
on	O
the	O
last	O
layer	O
,	O
followed	O
by	O
a	O
linear	Method
layer	Method
with	O
16	O
hidden	O
units	O
.	O

The	O
baseline	O
MLP	Method
has	O
87	O
%	O
more	O
parameters	O
than	O
the	O
RRN	Method
and	O
RN	Method
(	O
261	O
,	O
136	O
vs	O
139	O
,	O
536	O
)	O
.	O

subsection	O
:	O
Sudoku	Material
dataset	Material
To	O
generate	O
our	O
dataset	O
the	O
starting	O
point	O
is	O
the	O
collection	O
of	O
49	O
,	O
151	O
unique	O
17	O
-	O
givens	O
puzzles	O
gathered	O
by	O
royle2016minimum	O
which	O
we	O
solve	O
using	O
the	O
solver	O
from	O
norvig2006solving	O
.	O

Then	O
we	O
split	O
the	O
puzzles	O
into	O
a	O
test	O
,	O
validation	O
and	O
training	O
pool	O
,	O
with	O
10	O
,	O
000	O
,	O
1	O
,	O
000	O
and	O
38	O
,	O
151	O
samples	O
respectively	O
.	O

To	O
generate	O
the	O
sets	O
we	O
train	O
,	O
validate	O
and	O
test	O
on	O
we	O
do	O
the	O
following	O
:	O
for	O
each	O
we	O
sample	O
puzzles	O
from	O
the	O
respective	O
pool	O
,	O
with	O
replacement	O
.	O

For	O
each	O
sampled	O
puzzle	O
we	O
add	O
random	O
digits	O
from	O
the	O
solution	O
.	O

We	O
then	O
swap	O
the	O
digits	O
according	O
to	O
a	O
random	O
permutation	O
,	O
e.g.	O
,	O
,	O
etc	O
.	O

The	O
resulting	O
puzzle	O
is	O
added	O
to	O
the	O
respective	O
set	O
.	O

For	O
the	O
test	O
,	O
validation	O
and	O
training	O
sets	O
we	O
sample	O
,	O
and	O
puzzles	O
in	O
this	O
way	O
.	O

subsection	O
:	O
Sudoku	O
experimental	O
details	O
Unless	O
otherwise	O
specified	O
we	O
use	O
96	O
hidden	O
units	O
for	O
all	O
hidden	O
layers	Method
and	O
all	O
MLPs	Method
are	O
3	O
ReLU	Method
layers	Method
followed	O
by	O
a	O
linear	Method
layer	Method
.	O

Denote	O
the	O
digit	O
for	O
cell	O
(	O
0	O
-	O
9	O
,	O
0	O
if	O
not	O
given	O
)	O
,	O
and	O
the	O
row	O
and	O
column	O
position	O
(	O
1	O
-	O
9	O
)	O
and	O
(	O
1	O
-	O
9	O
)	O
respectively	O
..	O
The	O
node	O
features	O
are	O
then	O
where	O
each	O
embed	O
is	O
a	O
16	Method
dimensional	Method
learned	Method
embedding	Method
.	O

We	O
could	O
probably	O
have	O
used	O
one	Method
-	Method
hot	Method
encoding	Method
instead	O
of	O
the	O
embeddings	O
,	O
embedding	Task
was	O
just	O
the	O
first	O
thing	O
we	O
tried	O
.	O

Edge	O
features	O
were	O
not	O
used	O
.	O

The	O
message	Method
function	Method
is	O
an	O
MLP	Method
.	O

The	O
node	O
function	O
,	O
is	O
identical	O
to	O
the	O
setup	O
for	O
bAbI	Material
,	O
i.e.	O
The	O
LSTM	Method
cell	O
state	O
is	O
initialized	O
to	O
zeros	O
.	O

The	O
output	O
function	O
is	O
a	O
linear	Method
layer	Method
with	O
nine	O
outputs	O
to	O
produce	O
the	O
output	O
logits	O
.	O

We	O
run	O
the	O
network	O
for	O
32	O
steps	O
with	O
a	O
loss	O
on	O
every	O
step	O
.	O

We	O
train	O
the	O
model	O
for	O
300.000	O
gradient	O
updates	O
with	O
a	O
batch	O
size	O
of	O
256	O
using	O
Adam	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
2e	O
-	O
4	O
and	O
L2	Method
regularization	Method
of	O
1e	O
-	O
4	O
on	O
all	O
weight	O
matrices	O
.	O

subsection	O
:	O
Sudoku	Method
relational	Method
network	Method
baseline	O
details	O
The	O
node	O
centric	O
corresponds	O
exactly	O
to	O
a	O
single	O
step	O
of	O
our	O
network	O
.	O

The	O
graph	Method
centric	Method
approach	Method
is	O
closer	O
to	O
the	O
original	O
relational	Method
network	Method
.	O

It	O
does	O
one	O
step	O
of	O
relational	Method
reasoning	Method
as	O
our	O
network	O
,	O
then	O
sums	O
all	O
the	O
node	O
hidden	O
states	O
.	O

The	O
sum	O
is	O
then	O
passed	O
through	O
a	O
4	O
layer	O
MLP	Method
with	O
outputs	O
,	O
one	O
for	O
each	O
cell	O
and	O
digit	O
.	O

The	O
graph	Method
centric	Method
model	Method
has	O
larger	O
hidden	O
states	O
of	O
256	O
in	O
all	O
layers	Method
to	O
compensate	O
somewhat	O
for	O
the	O
sum	O
squashing	O
the	O
entire	O
graph	O
into	O
a	O
fixed	O
size	O
vector	O
.	O

Otherwise	O
both	O
networks	O
are	O
identical	O
to	O
our	O
network	O
.	O

The	O
graph	Method
centric	Method
has	O
over	O
4	O
times	O
as	O
many	O
parameters	O
as	O
our	O
model	O
(	O
944	O
,	O
874	O
vs.	O
201	O
,	O
194	O
)	O
but	O
performs	O
worse	O
than	O
the	O
node	Method
centric	Method
.	O

subsection	O
:	O
Age	Task
arithmetic	Task
task	Task
details	O
We	O
generated	O
all	O
262	O
,	O
144	O
unique	O
trees	O
with	O
8	O
nodes	O
and	O
split	O
them	O
90%	O
/	O
10	O
%	O
into	O
training	O
and	O
test	O
graphs	O
.	O

The	O
nodes	O
represent	O
the	O
persons	O
,	O
and	O
the	O
edges	O
which	O
age	O
differences	O
will	O
be	O
given	O
to	O
the	O
network	O
.	O

During	O
training	O
and	O
testing	O
we	O
sample	O
a	O
batch	O
of	O
graphs	O
from	O
the	O
respective	O
set	O
and	O
sample	O
8	O
random	O
ages	O
(	O
0	O
-	O
99	O
)	O
for	O
each	O
.	O

We	O
compute	O
the	O
absolute	O
difference	O
as	O
well	O
as	O
the	O
sign	O
for	O
each	O
edge	O
in	O
the	O
graphs	O
.	O

This	O
gives	O
us	O
7	O
relative	O
facts	O
on	O
the	O
form	O
“	O
person	O
A	O
(	O
0	O
-	O
7	O
)	O
,	O
person	O
B	O
(	O
0	O
-	O
7	O
)	O
,	O
younger	O
/	O
older	O
(	O
-	O
1	O
,	O
1	O
)	O
,	O
absolute	O
age	O
difference	O
(	O
0	O
-	O
99	O
)	O
”	O
.	O

Then	O
we	O
add	O
the	O
final	O
fact	O
which	O
is	O
the	O
age	O
of	O
one	O
of	O
the	O
nodes	O
at	O
random	O
,	O
e.g.	O
“	O
3	O
,	O
3	O
,	O
0	O
,	O
47	O
”	O
,	O
using	O
the	O
zero	O
sign	O
to	O
indicate	O
this	O
fact	O
is	O
absolute	O
and	O
not	O
relative	O
.	O

The	O
question	O
is	O
the	O
age	O
of	O
one	O
of	O
the	O
persons	O
at	O
random	O
(	O
0	O
-	O
7	O
)	O
.	O

For	O
each	O
graph	O
we	O
compute	O
the	O
shortest	O
path	O
from	O
the	O
anchor	O
person	O
to	O
the	O
person	O
in	O
question	O
.	O

This	O
is	O
the	O
minimum	O
number	O
of	O
arithmetic	O
computations	O
that	O
must	O
be	O
performed	O
to	O
infer	O
the	O
persons	O
age	O
from	O
the	O
given	O
facts	O
.	O

The	O
8	O
facts	O
(	O
1	O
anchor	O
,	O
7	O
relative	O
)	O
are	O
given	O
to	O
the	O
network	O
as	O
a	O
fully	O
connected	O
graph	O
of	O
8	O
nodes	O
.	O

Note	O
,	O
this	O
graph	O
is	O
different	O
from	O
the	O
tree	O
used	O
to	O
generate	O
the	O
facts	O
.	O

The	O
network	O
never	O
sees	O
the	O
tree	O
.	O

The	O
input	O
vector	O
for	O
each	O
fact	O
are	O
the	O
four	O
fact	O
integers	O
and	O
the	O
question	O
integer	O
one	O
-	O
hot	O
encoded	O
and	O
concatenated	O
.	O

We	O
use	O
the	O
same	O
architecture	O
as	O
for	O
the	O
bAbI	Material
experiments	O
except	O
all	O
MLPs	Method
are	O
3	O
dense	O
layers	Method
with	O
128	O
ReLu	O
units	O
followed	O
by	O
one	O
linear	Method
layer	Method
.	O

We	O
train	O
the	O
network	O
for	O
8	O
steps	O
,	O
and	O
test	O
it	O
for	O
each	O
step	O
.	O

See	O
figure	O
[	O
reference	O
]	O
for	O
results	O
.	O

subsection	O
:	O
Unrolled	Method
recurrent	Method
relational	Method
network	Method
Recurrent	Method
relational	Method
network	Method
on	O
a	O
fully	O
connected	O
graph	O
with	O
3	O
nodes	O
.	O

Subscripts	O
denote	O
node	O
indices	O
and	O
superscripts	O
denote	O
steps	O
.	O

The	O
dashed	O
lines	O
indicate	O
the	O
recurrent	O
connections	O
.	O

subsection	O
:	O
Full	O
Sudoku	Method
solution	Method
An	O
example	O
Sudoku	O
.	O

Each	O
of	O
the	O
81	O
cells	O
contain	O
each	O
digit	O
1	O
-	O
9	O
,	O
which	O
is	O
useful	O
if	O
the	O
reader	O
wishes	O
to	O
try	O
to	O
solve	O
the	O
Sudoku	O
as	O
they	O
can	O
be	O
crossed	O
out	O
or	O
highlighted	O
,	O
etc	O
.	O

The	O
digit	O
font	O
size	O
corresponds	O
to	O
the	O
probability	O
our	O
model	O
assigns	O
to	O
each	O
digit	O
at	O
step	O
0	O
,	O
i.e.	O
before	O
any	O
steps	O
are	O
taken	O
.	O

Subsequent	O
pages	O
contains	O
the	O
Sudoku	O
as	O
it	O
evolves	O
with	O
more	O
steps	O
of	O
our	O
model	O
.	O

Step	O
1	O
Step	O
4	O
Step	O
8	O
Step	O
12	O
Step	O
16	O
Step	O
20	O
