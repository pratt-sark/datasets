document	O
:	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Networks	Method
for	O
Natural	Task
Language	Task
Understanding	Task
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Network	Method
(	O
MT	Method
-	Method
DNN	Method
)	O
for	O
learning	Task
representations	Task
across	O
multiple	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
tasks	O
.	O

MT	Method
-	Method
DNN	Method
not	O
only	O
leverages	O
large	O
amounts	O
of	O
cross	O
-	O
task	O
data	O
,	O
but	O
also	O
benefits	O
from	O
a	O
regularization	O
effect	O
that	O
leads	O
to	O
more	O
general	O
representations	O
to	O
help	O
adapt	O
to	O
new	O
tasks	O
and	O
domains	O
.	O

MT	Method
-	Method
DNN	Method
extends	O
the	O
model	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
a	O
pre	Method
-	Method
trained	Method
bidirectional	Method
transformer	Method
language	Method
model	Method
,	O
known	O
as	O
BERT	Method
bert2018	O
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	Task
tasks	O
,	O
including	O
SNLI	Material
,	O
SciTail	Material
,	O
and	O
eight	O
out	O
of	O
nine	O
GLUE	Material
tasks	O
,	O
pushing	O
the	O
GLUE	Material
benchmark	O
to	O
82.2	O
%	O
(	O
1.8	O
%	O
absolute	O
improvement	O
)	O
.	O

We	O
also	O
demonstrate	O
using	O
the	O
SNLI	Material
and	O
SciTail	Material
datasets	Material
that	O
the	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
allow	O
domain	Task
adaptation	Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	Method
representations	O
.	O

Our	O
code	O
and	O
pre	O
-	O
trained	O
models	O
will	O
be	O
made	O
publicly	O
available	O
.	O

section	O
:	O
Introduction	O
Learning	O
vector	Method
-	Method
space	Method
representations	Method
of	Method
text	Method
,	O
e.g.	O
,	O
words	O
and	O
sentences	O
,	O
is	O
fundamental	O
to	O
many	O
natural	Task
language	Task
understanding	Task
(	O
NLU	Task
)	O
tasks	O
.	O

Two	O
popular	O
approaches	O
are	O
multi	Task
-	Task
task	Task
learning	Task
and	O
language	Method
model	Method
pre	Method
-	Method
training	Method
.	O

In	O
this	O
paper	O
we	O
strive	O
to	O
combine	O
the	O
strengths	O
of	O
both	O
approaches	O
by	O
proposing	O
a	O
new	O
Multi	Method
-	Method
Task	Method
Deep	Method
Neural	Method
Network	Method
(	O
MT	Method
-	Method
DNN	Method
)	O
.	O

Multi	Task
-	Task
Task	Task
Learning	Task
(	O
MTL	Task
)	Task
is	O
inspired	O
by	O
human	Task
learning	Task
activities	Task
where	O
people	O
often	O
apply	O
the	O
knowledge	O
learned	O
from	O
previous	O
tasks	O
to	O
help	O
learn	O
a	O
new	O
task	O
caruana1997multitask	O
,	O
zhang2017survey	O
.	O

For	O
example	O
,	O
it	O
is	O
easier	O
for	O
a	O
person	O
who	O
knows	O
how	O
to	O
ski	O
to	O
learn	O
skating	O
than	O
the	O
one	O
who	O
does	O
not	O
.	O

Similarly	O
,	O
it	O
is	O
useful	O
for	O
multiple	O
(	O
related	O
)	O
tasks	O
to	O
be	O
learned	O
jointly	O
so	O
that	O
the	O
knowledge	O
learned	O
in	O
one	O
task	O
can	O
benefit	O
other	O
tasks	O
.	O

Recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
applying	O
MTL	Task
to	O
representation	Task
learning	Task
using	O
deep	Method
neural	Method
networks	Method
(	O
DNNs	Method
)	O
collobert2011natural	O
,	O
liu2015mtl	O
,	O
luong2015multi	O
,	O
mt	O
-	O
mrc2018	O
for	O
two	O
reasons	O
.	O

First	O
,	O
supervised	O
learning	O
of	O
DNNs	Method
requires	O
large	O
amounts	O
of	O
task	O
-	O
specific	O
labeled	O
data	O
,	O
which	O
is	O
not	O
always	O
available	O
.	O

MTL	Task
provides	O
an	O
effective	O
way	O
of	O
leveraging	O
supervised	O
data	O
from	O
many	O
related	O
tasks	O
.	O

Second	O
,	O
the	O
use	O
of	O
multi	Method
-	Method
task	Method
learning	Method
profits	O
from	O
a	O
regularization	O
effect	O
via	O
alleviating	O
overfitting	O
to	O
a	O
specific	O
task	O
,	O
thus	O
making	O
the	O
learned	O
representations	O
universal	O
across	O
tasks	O
.	O

In	O
contrast	O
to	O
MTL	Task
,	O
language	Method
model	Method
pre	Method
-	Method
training	Method
has	O
shown	O
to	O
be	O
effective	O
for	O
learning	O
universal	Task
language	Task
representations	Task
by	O
leveraging	O
large	O
amounts	O
of	O
unlabeled	O
data	O
.	O

A	O
recent	O
survey	O
is	O
included	O
in	O
gao2018neural	O
.	O

Some	O
of	O
the	O
most	O
prominent	O
examples	O
are	O
ELMo	Method
elmo2018	O
,	O
GPT	Method
gpt2018	O
and	O
BERT	Method
bert2018	O
.	O

These	O
are	O
neural	Method
network	Method
language	Method
models	Method
trained	O
on	O
text	O
data	O
using	O
unsupervised	O
objectives	O
.	O

For	O
example	O
,	O
BERT	Method
is	O
based	O
on	O
a	O
multi	Method
-	Method
layer	Method
bidirectional	Method
Transformer	Method
,	O
and	O
is	O
trained	O
on	O
plain	O
text	O
for	O
masked	Task
word	Task
prediction	Task
and	O
next	Task
sentence	Task
prediction	Task
tasks	Task
.	O

To	O
apply	O
a	O
pre	Method
-	Method
trained	Method
model	Method
to	O
specific	O
NLU	Task
tasks	O
,	O
we	O
often	O
need	O
to	O
fine	O
-	O
tune	O
,	O
for	O
each	O
task	O
,	O
the	O
model	O
with	O
additional	O
task	O
-	O
specific	O
layers	O
using	O
task	O
-	O
specific	O
training	O
data	O
.	O

For	O
example	O
,	O
bert2018	O
shows	O
that	O
BERT	Method
can	O
be	O
fine	O
-	O
tuned	O
this	O
way	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
range	O
of	O
NLU	Task
tasks	O
,	O
such	O
as	O
question	Task
answering	Task
and	O
natural	Task
language	Task
inference	Task
.	O

We	O
argue	O
that	O
MTL	Task
and	O
language	O
model	O
pre	O
-	O
training	O
are	O
complementary	O
technologies	O
,	O
and	O
can	O
be	O
combined	O
to	O
improve	O
the	O
learning	Task
of	Task
text	Task
representations	Task
to	O
boost	O
the	O
performance	O
of	O
various	O
NLU	Task
tasks	O
.	O

To	O
this	O
end	O
,	O
we	O
extend	O
the	O
MT	Method
-	Method
DNN	Method
model	O
originally	O
proposed	O
in	O
liu2015mtl	O
by	O
incorporating	O
BERT	Method
as	O
its	O
shared	Method
text	Method
encoding	Method
layers	Method
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
lower	O
layers	O
(	O
i.e.	O
,	O
text	O
encoding	O
layers	O
)	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
are	O
task	O
-	O
specific	O
,	O
combining	O
different	O
types	O
of	O
NLU	Task
tasks	O
such	O
as	O
single	Task
-	Task
sentence	Task
classification	Task
,	O
pairwise	Task
text	Task
classification	Task
,	O
text	Task
similarity	Task
,	O
and	O
relevance	Task
ranking	Task
.	O

Similar	O
to	O
the	O
BERT	Method
model	Method
,	O
MT	Method
-	Method
DNN	Method
is	O
trained	O
in	O
two	O
stages	O
:	O
pre	Method
-	Method
training	Method
and	O
fine	Task
-	Task
tuning	Task
.	O

Unlike	O
BERT	Method
,	O
MT	Method
-	Method
DNN	Method
uses	O
MTL	Task
in	O
the	O
fine	Method
-	Method
tuning	Method
stage	Method
with	O
multiple	O
task	O
-	O
specific	O
layers	O
in	O
its	O
model	Method
architecture	Method
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
out	O
of	O
nine	O
NLU	Task
tasks	O
used	O
in	O
the	O
General	Task
Language	Task
Understanding	Task
Evaluation	Task
(	O
GLUE	Material
)	O
benchmark	O
wang2018glue	O
,	O
pushing	O
the	O
GLUE	Material
benchmark	O
score	O
to	O
82.2	O
%	O
,	O
amounting	O
to	O
1.8	O
%	O
absolute	O
improvement	O
over	O
BERT	Method
.	O

We	O
further	O
extend	O
the	O
superiority	O
of	O
MT	Method
-	Method
DNN	Method
to	O
the	O
SNLI	Material
and	O
SciTail	Material
tasks	O
.	O

The	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
allow	O
domain	Task
adaptation	Task
with	O
substantially	O
fewer	O
in	O
-	O
domain	O
labels	O
than	O
the	O
pre	O
-	O
trained	O
BERT	Method
representations	O
.	O

For	O
example	O
,	O
our	O
adapted	O
models	O
achieve	O
the	O
accuracy	Metric
of	O
91.1	O
%	O
on	O
SNLI	Material
and	O
94.1	O
%	O
on	O
SciTail	Material
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
by	O
1.0	O
%	O
and	O
5.8	O
%	O
,	O
respectively	O
.	O

Even	O
with	O
only	O
0.1	O
%	O
or	O
1.0	O
%	O
of	O
the	O
original	O
training	O
data	O
,	O
the	O
performance	O
of	O
MT	Method
-	Method
DNN	Method
on	O
both	O
SNLI	Material
and	O
SciTail	Material
datasets	O
is	O
fairly	O
good	O
and	O
much	O
better	O
than	O
many	O
existing	O
models	O
.	O

All	O
of	O
these	O
clearly	O
demonstrate	O
MT	Method
-	Method
DNN	Method
’s	O
exceptional	O
generalization	Method
capability	Method
via	O
multi	Method
-	Method
task	Method
learning	Method
.	O

section	O
:	O
Tasks	O
The	O
MT	Method
-	Method
DNN	Method
model	O
combines	O
four	O
types	O
of	O
NLU	Task
tasks	O
:	O
single	Task
-	Task
sentence	Task
classification	Task
,	O
pairwise	Task
text	Task
classification	Task
,	O
text	Task
similarity	Task
scoring	Task
,	O
and	O
relevance	Task
ranking	Task
.	O

For	O
concreteness	O
,	O
we	O
describe	O
them	O
using	O
the	O
NLU	Task
tasks	O
defined	O
in	O
the	O
GLUE	Material
benchmark	O
as	O
examples	O
.	O

paragraph	O
:	O
Single	O
-	O
Sentence	Task
Classification	Task
:	O
Given	O
a	O
sentence	O
,	O
the	O
model	O
labels	O
it	O
using	O
one	O
of	O
the	O
pre	O
-	O
defined	O
class	O
labels	O
.	O

For	O
example	O
,	O
the	O
CoLA	Task
task	Task
is	O
to	O
predict	O
whether	O
an	O
English	Material
sentence	Material
is	O
grammatically	O
plausible	O
.	O

The	O
SST	Material
-	Material
2	Material
task	O
is	O
to	O
determine	O
whether	O
the	O
sentiment	O
of	O
a	O
sentence	O
extracted	O
from	O
movie	O
reviews	O
is	O
positive	O
or	O
negative	O
.	O

paragraph	O
:	O
Text	Metric
Similarity	Metric
:	O
This	O
is	O
a	O
regression	Task
task	Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
predicts	O
a	O
real	O
-	O
value	O
score	O
indicating	O
the	O
semantic	O
similarity	O
of	O
the	O
two	O
sentences	O
.	O

STS	Method
-	Method
B	Method
is	O
the	O
only	O
example	O
of	O
the	O
task	O
in	O
GLUE	Material
.	O

paragraph	O
:	O
Pairwise	Task
Text	Task
Classification	Task
:	O
Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
model	O
determines	O
the	O
relationship	O
of	O
the	O
two	O
sentences	O
based	O
on	O
a	O
set	O
of	O
pre	O
-	O
defined	O
labels	O
.	O

For	O
example	O
,	O
both	O
RTE	Method
and	O
MNLI	Material
are	O
language	Task
inference	Task
tasks	Task
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
a	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
other	O
.	O

QQP	Material
and	O
MRPC	Material
are	O
paragraph	O
datasets	O
that	O
consist	O
of	O
sentence	O
pairs	O
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
.	O

paragraph	O
:	O
Relevance	Task
Ranking	Task
:	O
Given	O
a	O
query	O
and	O
a	O
list	O
of	O
candidate	O
answers	O
,	O
the	O
model	O
ranks	O
all	O
the	O
candidates	O
in	O
the	O
order	O
of	O
relevance	O
to	O
the	O
query	O
.	O

QNLI	Material
is	O
a	O
version	O
of	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
rajpurkar2016squad	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
sentence	O
contains	O
the	O
correct	O
answer	O
to	O
a	O
given	O
query	O
.	O

Although	O
QNLI	Material
is	O
defined	O
as	O
a	O
binary	Task
classification	Task
task	Task
in	O
GLUE	Material
,	O
in	O
this	O
study	O
we	O
formulate	O
it	O
as	O
a	O
pairwise	Task
ranking	Task
task	Task
,	O
where	O
the	O
model	O
is	O
expected	O
to	O
rank	O
the	O
candidate	O
that	O
contains	O
the	O
correct	O
answer	O
higher	O
than	O
the	O
candidate	O
that	O
does	O
not	O
.	O

We	O
will	O
show	O
that	O
this	O
formulation	O
leads	O
to	O
a	O
significant	O
improvement	O
in	O
accuracy	Metric
over	O
binary	Task
classification	Task
.	O

section	O
:	O
The	O
Proposed	O
MT	Method
-	Method
DNN	Method
Model	O
The	O
architecture	O
of	O
the	O
MT	Method
-	Method
DNN	Method
model	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
lower	O
layers	O
are	O
shared	O
across	O
all	O
tasks	O
,	O
while	O
the	O
top	O
layers	O
represent	O
task	O
-	O
specific	O
outputs	O
.	O

The	O
input	O
,	O
which	O
is	O
a	O
word	O
sequence	O
(	O
either	O
a	O
sentence	O
or	O
a	O
pair	O
of	O
sentences	O
packed	O
together	O
)	O
is	O
first	O
represented	O
as	O
a	O
sequence	O
of	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
word	O
,	O
in	O
.	O

Then	O
the	O
transformer	Method
encoder	Method
captures	O
the	O
contextual	O
information	O
for	O
each	O
word	O
via	O
self	O
-	O
attention	O
,	O
and	O
generates	O
a	O
sequence	O
of	O
contextual	O
embeddings	O
in	O
.	O

This	O
is	O
the	O
shared	Method
semantic	Method
representation	Method
that	O
is	O
trained	O
by	O
our	O
multi	Task
-	Task
task	Task
objectives	Task
.	O

In	O
what	O
follows	O
,	O
we	O
elaborate	O
on	O
the	O
model	O
in	O
detail	O
.	O

paragraph	O
:	O
Lexicon	Method
Encoder	Method
(	O
)	O
:	O
The	O
input	O
is	O
a	O
sequence	O
of	O
tokens	O
of	O
length	O
.	O

Following	O
bert2018	O
,	O
the	O
first	O
token	O
is	O
always	O
the	O
[	O
CLS	O
]	O
token	O
.	O

If	O
is	O
packed	O
by	O
a	O
sentence	O
pair	O
,	O
we	O
separate	O
the	O
two	O
sentences	O
with	O
a	O
special	O
token	O
[	O
SEP	O
]	O
.	O

The	O
lexicon	Method
encoder	Method
maps	O
into	O
a	O
sequence	O
of	O
input	O
embedding	O
vectors	O
,	O
one	O
for	O
each	O
token	O
,	O
constructed	O
by	O
summing	O
the	O
corresponding	O
word	O
,	O
segment	O
,	O
and	O
positional	O
embeddings	O
.	O

paragraph	O
:	O
Transformer	Method
Encoder	Method
(	O
)	O
:	O
We	O
use	O
a	O
multi	Method
-	Method
layer	Method
bidirectional	Method
Transformer	Method
encoder	Method
vaswani2017attention	O
to	O
map	O
the	O
input	O
representation	O
vectors	O
(	O
)	O
into	O
a	O
sequence	O
of	O
contextual	O
embedding	O
vectors	O
.	O

This	O
is	O
the	O
shared	O
representation	O
across	O
different	O
tasks	O
.	O

Unlike	O
the	O
BERT	Method
model	Method
bert2018	O
that	O
learns	O
the	O
representation	O
via	O
pre	Method
-	Method
training	Method
and	O
adapts	O
it	O
to	O
each	O
individual	O
task	O
via	O
fine	Method
-	Method
tuning	Method
,	O
MT	Method
-	Method
DNN	Method
learns	O
the	O
representation	O
using	O
multi	O
-	O
task	O
objectives	O
.	O

paragraph	O
:	O
Single	Task
-	Task
Sentence	Task
Classification	Task
Output	O
:	O
Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
the	O
token	O
[	O
CLS	O
]	O
,	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	Method
representation	Method
of	O
input	O
sentence	O
.	O

Take	O
the	O
SST	Material
-	Material
2	Material
task	O
as	O
an	O
example	O
.	O

The	O
probability	O
that	O
is	O
labeled	O
as	O
class	O
(	O
i.e.	O
,	O
the	O
sentiment	O
)	O
is	O
predicted	O
by	O
a	O
logistic	Method
regression	Method
with	O
softmax	Method
:	O
where	O
is	O
the	O
task	O
-	O
specific	O
parameter	O
matrix	O
.	O

paragraph	O
:	O
Text	Metric
Similarity	Metric
Output	O
:	O
Take	O
the	O
STS	Task
-	Task
B	Task
task	Task
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
(	O
)	O
of	O
[	O
CLS	O
]	O
which	O
can	O
be	O
viewed	O
as	O
the	O
semantic	Method
representation	Method
of	O
the	O
input	O
sentence	O
pair	O
.	O

We	O
introduce	O
a	O
task	O
-	O
specific	O
parameter	O
vector	O
to	O
compute	O
the	O
similarity	Metric
score	Metric
as	O
:	O
where	O
is	O
a	O
sigmoid	Method
function	Method
that	O
maps	O
the	O
score	O
to	O
a	O
real	O
value	O
of	O
the	O
range	O
.	O

paragraph	O
:	O
Pairwise	Metric
Text	Metric
Classification	Metric
Output	O
:	O
Take	O
natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
as	O
an	O
example	O
.	O

The	O
NLI	Task
task	Task
defined	O
here	O
involves	O
a	O
premise	O
of	O
words	O
and	O
a	O
hypothesis	O
of	O
words	O
,	O
and	O
aims	O
to	O
find	O
a	O
logical	O
relationship	O
between	O
and	O
.	O

The	O
design	O
of	O
the	O
output	Method
module	Method
follows	O
the	O
answer	Method
module	Method
of	O
the	O
stochastic	Method
answer	Method
network	Method
(	O
SAN	Method
)	O
liu2018san4nli	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
NLI	Method
model	Method
.	O

SAN	Method
’s	O
answer	O
module	O
uses	O
multi	Method
-	Method
step	Method
reasoning	Method
.	O

Rather	O
than	O
directly	O
predicting	O
the	O
entailment	O
given	O
the	O
input	O
,	O
it	O
maintains	O
a	O
state	O
and	O
iteratively	O
refines	O
its	O
predictions	O
.	O

The	O
SAN	Method
answer	O
module	O
works	O
as	O
follows	O
.	O

We	O
first	O
construct	O
the	O
working	O
memory	O
of	O
premise	O
by	O
concatenating	O
the	O
contextual	O
embeddings	O
of	O
the	O
words	O
in	O
,	O
which	O
are	O
the	O
output	O
of	O
the	O
transformer	Method
encoder	Method
,	O
denoted	O
as	O
,	O
and	O
similarly	O
the	O
working	O
memory	O
of	O
hypothesis	Method
,	O
denoted	O
as	O
.	O

Then	O
,	O
we	O
perform	O
-	Method
step	Method
reasoning	Method
on	O
the	O
memory	O
to	O
output	O
the	O
relation	O
label	O
,	O
where	O
is	O
a	O
hyperparameter	O
.	O

At	O
the	O
beginning	O
,	O
the	O
initial	O
state	O
is	O
the	O
summary	O
of	O
:	O
,	O
where	O
.	O

At	O
time	O
step	O
in	O
the	O
range	O
of	O
,	O
the	O
state	O
is	O
defined	O
by	O
.	O

Here	O
,	O
is	O
computed	O
from	O
the	O
previous	O
state	O
and	O
memory	O
:	O
and	O
.	O

A	O
one	Method
-	Method
layer	Method
classifier	Method
is	O
used	O
to	O
determine	O
the	O
relation	O
at	O
each	O
step	O
:	O
At	O
last	O
,	O
we	O
utilize	O
all	O
of	O
the	O
outputs	O
by	O
averaging	O
the	O
scores	O
:	O
Each	O
is	O
a	O
probability	O
distribution	O
over	O
all	O
the	O
relations	O
.	O

During	O
training	Task
,	O
we	O
apply	O
stochastic	Method
prediction	Method
dropout	Method
liu2018san	O
before	O
the	O
above	O
averaging	Method
operation	Method
.	O

During	O
decoding	Task
,	O
we	O
average	O
all	O
outputs	O
to	O
improve	O
robustness	Metric
.	O

paragraph	O
:	O
Relevance	Metric
Ranking	Metric
Output	O
:	O
Take	O
QNLI	Material
as	O
an	O
example	O
.	O

Suppose	O
that	O
is	O
the	O
contextual	O
embedding	O
vector	O
of	O
[	O
CLS	Method
]	Method
which	O
is	O
the	O
semantic	Method
representation	Method
of	O
a	O
pair	O
of	O
question	O
and	O
its	O
candidate	O
answer	O
.	O

We	O
compute	O
the	O
relevance	Metric
score	Metric
as	O
:	O
For	O
a	O
given	O
,	O
we	O
rank	O
all	O
of	O
its	O
candidate	O
answers	O
based	O
on	O
their	O
relevance	O
scores	O
computed	O
using	O
Equation	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
The	O
Training	O
Procedure	O
The	O
training	Method
procedure	Method
of	O
MT	Method
-	Method
DNN	Method
consists	O
of	O
two	O
stages	O
:	O
pretraining	Task
and	O
multi	Task
-	Task
task	Task
fine	Task
-	Task
tuning	Task
.	O

The	O
pretraining	O
stage	O
follows	O
that	O
of	O
the	O
BERT	Method
model	O
bert2018	O
.	O

The	O
parameters	O
of	O
the	O
lexicon	Method
encoder	Method
and	O
Transformer	Method
encoder	Method
are	O
learned	O
using	O
two	O
unsupervised	Task
prediction	Task
tasks	Task
:	O
masked	Task
language	Task
modeling	Task
and	O
next	Task
sentence	Task
prediction	Task
.	O

In	O
the	O
multi	Task
-	Task
task	Task
fine	Task
-	Task
tuning	Task
stage	Task
,	O
we	O
use	O
mini	Method
-	Method
batch	Method
based	Method
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
to	O
learn	O
the	O
parameters	O
of	O
our	O
model	O
(	O
i.e.	O
,	O
the	O
parameters	O
of	O
all	O
shared	O
layers	O
and	O
task	O
-	O
specific	O
layers	O
)	O
as	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

In	O
each	O
epoch	O
,	O
a	O
mini	O
-	O
batch	O
is	O
selected	O
(	O
e.g	O
.	O

,	O
among	O
all	O
9	O
GLUE	Material
tasks	O
)	O
,	O
and	O
the	O
model	O
is	O
updated	O
according	O
to	O
the	O
task	O
-	O
specific	O
objective	O
for	O
the	O
task	O
.	O

This	O
approximately	O
optimizes	O
the	O
sum	O
of	O
all	O
multi	O
-	O
task	O
objectives	O
.	O

[	O
ht	O
!	O

]	O
Initialize	O
model	O
parameters	O
randomly	O
.	O

Pre	O
-	O
train	O
the	O
shared	Method
layers	Method
(	O
i.e.	O
,	O
the	O
lexicon	Method
encoder	Method
and	O
the	O
transformer	Method
encoder	Method
)	O
.	O

Set	O
the	O
max	O
number	O
of	O
epoch	O
:	O
.	O

//	O
Prepare	O
the	O
data	O
for	O
T	O
tasks	O
.	O

in	O
Pack	O
the	O
dataset	O
into	O
mini	O
-	O
batch	O
:	O
.	O

in	O
1	O
.	O

Merge	O
all	O
the	O
datasets	O
:	O
2	O
.	O

Shuffle	Method
in	O
D	O
//	O
bt	O
is	O
a	O
mini	O
-	O
batch	O
of	O
task	O
t.	O
3	O
.	O

Compute	O
loss	Metric
:	O
Eq	O
.	O

[	O
reference	O
]	O
for	O
classification	Task
Eq	Task
.	O

[	O
reference	O
]	O
for	O
regression	Task
Eq	Task
.	O

[	O
reference	O
]	O
for	O
ranking	Task
4	O
.	O

Compute	O
gradient	O
:	O
5	O
.	O

Update	Method
model	Method
:	O
Training	O
a	O
MT	Method
-	Method
DNN	Method
model	O
.	O

For	O
the	O
classification	Task
tasks	Task
(	O
i.e.	O
,	O
single	Task
-	Task
sentence	Task
or	Task
pairwise	Task
text	Task
classification	Task
)	O
,	O
we	O
use	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
as	O
the	O
objective	O
:	O
where	O
is	O
the	O
binary	O
indicator	O
(	O
0	O
or	O
1	O
)	O
if	O
class	O
label	O
is	O
the	O
correct	O
classification	O
for	O
,	O
and	O
is	O
defined	O
by	O
e.g.	O
,	O
Equation	O
[	O
reference	O
]	O
or	O
[	O
reference	O
]	O
.	O

For	O
the	O
text	Task
similarity	Task
tasks	Task
,	O
such	O
as	O
STS	Task
-	Task
B	Task
,	O
where	O
each	O
sentence	O
pair	O
is	O
annotated	O
with	O
a	O
real	O
-	O
valued	O
score	O
,	O
we	O
use	O
the	O
mean	Metric
squared	Metric
error	Metric
as	O
the	O
objective	O
:	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
objective	O
for	O
the	O
relevance	Task
ranking	Task
tasks	Task
follows	O
the	O
pairwise	Method
learning	Method
-	Method
to	Method
-	Method
rank	Method
paradigm	Method
learning	Method
-	O
to	O
-	O
rank2005burges	O
,	O
huang2013dssm	O
.	O

Take	O
QNLI	Material
as	O
an	O
example	O
.	O

Given	O
a	O
query	O
,	O
we	O
obtain	O
a	O
list	O
of	O
candidate	O
answers	O
which	O
contains	O
a	O
positive	O
example	O
that	O
includes	O
the	O
correct	O
answer	O
,	O
and	O
negative	O
examples	O
.	O

We	O
then	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
positive	O
example	O
given	O
queries	O
across	O
the	O
training	O
data	O
where	O
is	O
defined	O
by	O
Equation	O
[	O
reference	O
]	O
and	O
is	O
a	O
tuning	O
factor	O
determined	O
on	O
held	O
-	O
out	O
data	O
.	O

In	O
our	O
experiment	O
,	O
we	O
simply	O
set	O
to	O
1	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
MT	Method
-	Method
DNN	Method
on	O
three	O
popular	O
NLU	Task
benchmarks	O
:	O
GLUE	Material
,	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	O
,	O
and	O
SciTail	Material
.	O

We	O
compare	O
MT	Method
-	Method
DNN	Method
with	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
including	O
BERT	Method
and	O
demonstrate	O
the	O
effectiveness	O
of	O
MTL	Task
for	O
model	Task
fine	Task
-	Task
tuning	Task
using	O
GLUE	Material
and	O
domain	Method
adaptation	Method
using	O
SNLI	Material
and	O
SciTail	Material
.	O

subsection	O
:	O
Datasets	O
This	O
section	O
briefly	O
describes	O
the	O
GLUE	Material
,	O
SNLI	Material
,	O
and	O
SciTail	Material
datasets	Material
,	O
as	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
GLUE	Material
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
NLU	Task
tasks	O
,	O
including	O
question	Task
answering	Task
,	O
sentiment	Task
analysis	Task
,	O
and	O
textual	Task
entailment	Task
;	O
it	O
is	O
considered	O
well	O
-	O
designed	O
for	O
evaluating	O
the	O
generalization	Metric
and	Metric
robustness	Metric
of	O
NLU	Task
models	O
.	O

Both	O
SNLI	Material
and	O
SciTail	Material
are	O
NLI	Task
tasks	Task
.	O

paragraph	O
:	O
CoLA	O
The	O
Corpus	Task
of	Task
Linguistic	Task
Acceptability	Task
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
âacceptableâ	O
or	O
not	O
.	O

It	O
uses	O
Matthews	Metric
correlation	Metric
coefficient	Metric
as	O
the	O
evaluation	Metric
metric	Metric
.	O

paragraph	O
:	O
SST	Material
-	Material
2	Material
The	O
Stanford	Material
Sentiment	Material
Treebank	Material
is	O
to	O
determine	O
the	O
sentiment	O
of	O
sentences	O
.	O

The	O
sentences	O
are	O
extracted	O
from	O
movie	O
reviews	O
with	O
human	O
annotations	O
of	O
their	O
sentiment	O
.	O

Accuracy	Metric
is	O
used	O
as	O
the	O
evaluation	Metric
metric	Metric
.	O

paragraph	O
:	O
STS	Material
-	Material
B	Material
The	O
Semantic	Task
Textual	Task
Similarity	Task
Benchmark	Task
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
collected	O
from	O
multiple	O
data	O
resources	O
including	O
news	O
headlines	O
,	O
video	O
,	O
and	O
image	O
captions	O
,	O
and	O
NLI	Material
data	Material
.	O

Each	O
pair	O
is	O
human	O
-	O
annotated	O
with	O
a	O
similarity	Metric
score	Metric
from	O
one	O
to	O
five	O
,	O
indicating	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
.	O

The	O
task	O
is	O
evaluated	O
using	O
two	O
metrics	O
:	O
the	O
Pearson	Metric
and	Metric
Spearman	Metric
correlation	Metric
coefficients	Metric
.	O

paragraph	O
:	O
QNLI	Material
This	O
is	O
derived	O
from	O
the	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
rajpurkar2016squad	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	Task
classification	Task
task	Task
in	O
GLUE	Material
.	O

A	O
query	O
-	O
candidate	O
-	O
answer	O
tuple	O
is	O
labeled	O
as	O
positive	O
if	O
the	O
candidate	O
contains	O
the	O
correct	O
answer	O
to	O
the	O
query	O
and	O
negative	O
otherwise	O
.	O

In	O
this	O
study	O
,	O
however	O
,	O
we	O
formulate	O
QNLI	Material
as	O
a	O
relevance	Task
ranking	Task
task	Task
,	O
where	O
for	O
a	O
given	O
query	O
,	O
its	O
positive	O
candidate	O
answers	O
are	O
considered	O
more	O
relevant	O
,	O
and	O
thus	O
should	O
be	O
ranked	O
higher	O
than	O
its	O
negative	O
candidates	O
.	O

paragraph	O
:	O
QQP	Material
The	O
Quora	Material
Question	Material
Pairs	Material
dataset	Material
is	O
a	O
collection	O
of	O
question	O
pairs	O
extracted	O
from	O
the	O
community	Material
question	Material
-	Material
answering	Material
website	Material
Quora	Material
.	O

The	O
task	O
is	O
to	O
predict	O
whether	O
two	O
questions	O
are	O
semantically	O
equivalent	O
.	O

As	O
the	O
distribution	O
of	O
positive	O
and	O
negative	O
labels	O
is	O
unbalanced	O
,	O
both	O
accuracy	Metric
and	O
F1	Metric
score	Metric
are	O
used	O
as	O
evaluation	Metric
metrics	Metric
.	O

paragraph	O
:	O
MRPC	Material
The	O
Microsoft	Material
Research	Material
Paraphrase	Material
Corpus	Material
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
with	O
human	O
annotations	O
denoting	O
whether	O
a	O
sentence	O
pair	O
is	O
semantically	O
equivalent	O
to	O
the	O
other	O
in	O
the	O
pair	O
.	O

Similar	O
to	O
QQP	Material
,	O
both	O
accuracy	Metric
and	O
F1	Metric
score	Metric
are	O
used	O
as	O
evaluation	Metric
metrics	Metric
.	O

paragraph	O
:	O
MNLI	Material
Multi	Task
-	Task
Genre	Task
Natural	Task
Language	Task
Inference	Task
is	O
a	O
large	Task
-	Task
scale	Task
,	Task
crowd	Task
-	Task
sourced	Task
entailment	Task
classification	Task
task	Task
.	O

Given	O
a	O
pair	O
of	O
sentences	O
(	O
i.e.	O
,	O
a	O
premise	O
-	O
hypothesis	O
pair	O
)	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
hypothesis	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
premise	O
.	O

The	O
test	O
and	O
development	O
sets	O
are	O
split	O
into	O
in	O
-	O
domain	O
(	O
matched	Metric
)	O
and	O
cross	O
-	O
domain	O
(	O
mismatched	Metric
)	O
sets	O
.	O

The	O
evaluation	Metric
metric	Metric
is	O
accuracy	Metric
.	O

paragraph	O
:	O
RTE	O
The	O
Recognizing	Material
Textual	Material
Entailment	Material
dataset	Material
is	O
collected	O
from	O
a	O
series	O
of	O
annual	O
challenges	O
on	O
textual	Task
entailment	Task
.	O

The	O
task	O
is	O
similar	O
to	O
MNLI	Material
,	O
but	O
uses	O
only	O
two	O
labels	O
:	O
entailment	O
and	O
not_entailment	O
.	O

paragraph	O
:	O
WNLI	Material
The	O
Winograd	Material
NLI	Material
(	O
WNLI	Material
)	O
is	O
a	O
natural	Task
language	Task
inference	Task
dataset	O
derived	O
from	O
the	O
Winograd	Material
Schema	Material
dataset	Material
.	O

This	O
is	O
a	O
reading	Task
comprehension	Task
task	Task
.	O

The	O
goal	O
is	O
to	O
select	O
the	O
referent	O
of	O
a	O
pronoun	O
from	O
a	O
list	O
of	O
choices	O
in	O
a	O
given	O
sentence	O
which	O
contains	O
the	O
pronoun	O
.	O

paragraph	O
:	O
SNLI	Material
The	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	O
dataset	O
contains	O
570k	O
human	O
annotated	O
sentence	O
pairs	O
,	O
in	O
which	O
the	O
premises	O
are	O
drawn	O
from	O
the	O
captions	O
of	O
the	O
Flickr30	Material
corpus	Material
and	O
hypotheses	O
are	O
manually	O
annotated	O
.	O

This	O
is	O
the	O
most	O
widely	O
used	O
entailment	O
dataset	O
for	O
NLI	Task
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	Task
adaptation	Task
in	O
this	O
study	O
.	O

paragraph	O
:	O
SciTail	Material
This	O
is	O
a	O
textual	O
entailment	O
dataset	O
derived	O
from	O
a	O
science	Material
question	Material
answering	Material
(	O
SciQ	Material
)	O
dataset	O
.	O

The	O
task	O
involves	O
assessing	O
whether	O
a	O
given	O
premise	O
entails	O
a	O
given	O
hypothesis	O
.	O

In	O
contrast	O
to	O
other	O
entailment	O
datasets	O
mentioned	O
previously	O
,	O
the	O
hypotheses	O
in	O
SciTail	Material
are	O
created	O
from	O
science	O
questions	O
while	O
the	O
corresponding	O
answer	O
candidates	O
and	O
premises	O
come	O
from	O
relevant	O
web	O
sentences	O
retrieved	O
from	O
a	O
large	O
corpus	O
.	O

As	O
a	O
result	O
,	O
these	O
sentences	O
are	O
linguistically	O
challenging	O
and	O
the	O
lexical	O
similarity	O
of	O
premise	O
and	O
hypothesis	O
is	O
often	O
high	O
,	O
thus	O
making	O
SciTail	Material
particularly	O
difficult	O
.	O

The	O
dataset	O
is	O
used	O
only	O
for	O
domain	Task
adaptation	Task
in	O
this	O
study	O
.	O

subsection	O
:	O
Implementation	O
details	O
Our	O
implementation	O
of	O
MT	Method
-	Method
DNN	Method
is	O
based	O
on	O
the	O
PyTorch	O
implementation	O
of	O
BERT	Method
.	O

We	O
used	O
Adamax	Method
as	O
our	O
optimizer	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
5e	O
-	O
5	O
and	O
a	O
batch	O
size	O
of	O
32	O
.	O

The	O
maximum	O
number	O
of	O
epochs	O
was	O
set	O
to	O
5	O
.	O

A	O
linear	Method
learning	Method
rate	Method
decay	Method
schedule	Method
with	O
warm	O
-	O
up	O
over	O
0.1	O
was	O
used	O
,	O
unless	O
stated	O
otherwise	O
.	O

Following	O
,	O
we	O
set	O
the	O
number	O
of	O
steps	O
to	O
5	O
with	O
a	O
dropout	Metric
rate	Metric
of	O
0.1	O
.	O

To	O
avoid	O
the	O
exploding	Task
gradient	Task
problem	Task
,	O
we	O
clipped	O
the	O
gradient	O
norm	O
within	O
1	O
.	O

All	O
the	O
texts	O
were	O
tokenized	O
using	O
wordpieces	O
,	O
and	O
were	O
chopped	O
to	O
spans	O
no	O
longer	O
than	O
512	O
tokens	O
.	O

subsection	O
:	O
GLUE	Material
Results	O
The	O
test	O
results	O
on	O
GLUE	Material
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

MT	Method
-	Method
DNN	Method
outperforms	O
all	O
existing	O
systems	O
on	O
all	O
tasks	O
,	O
except	O
WNLI	Material
,	O
creating	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eight	O
GLUE	Material
tasks	O
and	O
pushing	O
the	O
benchmark	O
to	O
82.2	O
%	O
,	O
which	O
amounts	O
to	O
1.8	O
%	O
absolution	O
improvement	O
over	O
BERT	Method
LARGE	O
.	O

Since	O
MT	Method
-	Method
DNN	Method
uses	O
BERT	Method
LARGE	O
for	O
its	O
shared	O
layers	O
,	O
the	O
gain	O
is	O
solely	O
attributed	O
to	O
the	O
use	O
of	O
MTL	Task
in	O
fine	Task
-	Task
tuning	Task
.	O

MTL	Task
is	O
particularly	O
useful	O
for	O
the	O
tasks	O
with	O
little	O
in	O
-	O
domain	O
training	O
data	O
.	O

As	O
we	O
observe	O
in	O
the	O
table	O
,	O
on	O
the	O
same	O
type	O
of	O
tasks	O
,	O
the	O
improvements	O
over	O
BERT	Method
are	O
much	O
more	O
substantial	O
for	O
the	O
tasks	O
with	O
less	O
in	O
-	O
domain	O
training	O
data	O
e.g.	O
,	O
the	O
two	O
NLI	Task
tasks	Task
:	O
RTE	Method
vs.	O
MNLI	Material
,	O
and	O
the	O
two	O
paraphrase	Method
tasks	Method
:	O
MRPC	Material
vs.	O
QQP	Material
.	O

The	O
gain	O
of	O
MT	Method
-	Method
DNN	Method
is	O
also	O
attributed	O
to	O
its	O
flexible	O
modeling	Method
framework	Method
which	O
allows	O
us	O
to	O
incorporate	O
the	O
task	Method
-	Method
specific	Method
model	Method
structures	Method
and	O
training	Method
methods	Method
which	O
have	O
been	O
developed	O
in	O
the	O
single	Task
-	Task
task	Task
setting	Task
,	O
effectively	O
leveraging	O
the	O
existing	O
body	O
of	O
research	O
.	O

Two	O
such	O
examples	O
use	O
the	O
SAN	Method
answer	O
module	O
for	O
the	O
pairwise	Method
text	Method
classification	Method
output	Method
module	Method
,	O
and	O
the	O
pairwise	Metric
ranking	Metric
loss	Metric
for	O
the	O
QNLI	Material
task	O
which	O
by	O
design	O
is	O
a	O
binary	Task
classification	Task
problem	Task
in	O
GLUE	Material
.	O

To	O
investigate	O
the	O
relative	O
contributions	O
of	O
the	O
above	O
two	O
modeling	O
design	O
choices	O
,	O
we	O
implement	O
different	O
versions	O
of	O
MT	O
-	O
DNNs	Method
and	O
compare	O
their	O
performance	O
on	O
the	O
development	O
sets	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

BERT	Method
is	O
the	O
base	O
BERT	Method
model	Method
released	O
by	O
the	O
authors	O
,	O
which	O
we	O
used	O
as	O
a	O
baseline	O
.	O

We	O
fine	O
-	O
tuned	O
the	O
model	O
for	O
each	O
single	O
task	O
.	O

MT	Method
-	Method
DNN	Method
is	O
the	O
proposed	O
model	O
described	O
in	O
Section	O
[	O
reference	O
]	O
using	O
the	O
pre	O
-	O
trained	O
BERT	Method
BASE	O
as	O
its	O
shared	O
layers	O
.	O

We	O
then	O
fine	O
-	O
tuned	O
the	O
model	O
using	O
MTL	Task
on	O
all	O
GLUE	Material
tasks	O
.	O

Comparing	O
MT	Method
-	Method
DNN	Method
vs.	O
BERT	Method
BASE	O
,	O
we	O
see	O
that	O
the	O
results	O
on	O
dev	O
sets	O
are	O
consistent	O
with	O
the	O
GLUE	Material
test	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

ST	Method
-	Method
DNN	Method
,	O
standing	O
for	O
Single	Task
-	Task
Task	Task
DNN	Task
,	O
uses	O
the	O
same	O
model	Method
architecture	Method
as	O
MT	Method
-	Method
DNN	Method
.	O

But	O
,	O
instead	O
of	O
fine	O
-	O
tuning	O
one	O
model	O
for	O
all	O
tasks	O
using	O
MTL	Task
,	O
we	O
create	O
multiple	O
ST	O
-	O
DNNs	Method
,	O
one	O
for	O
each	O
task	O
using	O
only	O
its	O
in	O
-	O
domain	O
data	O
for	O
fine	Task
-	Task
tuning	Task
.	O

Thus	O
,	O
for	O
pairwise	Task
text	Task
classification	Task
tasks	Task
,	O
the	O
only	O
difference	O
between	O
their	O
ST	O
-	O
DNNs	Method
and	O
BERT	Method
models	O
is	O
the	O
design	O
of	O
the	O
task	Method
-	Method
specific	Method
output	Method
module	Method
.	O

The	O
results	O
show	O
that	O
on	O
three	O
out	O
of	O
four	O
tasks	O
(	O
MNLI	Material
,	O
QQP	Material
and	O
MRPC	Material
)	O
ST	O
-	O
DNNs	Method
outperform	O
their	O
BERT	Method
counterparts	O
,	O
justifying	O
the	O
effectiveness	O
of	O
the	O
SAN	Method
answer	O
module	O
.	O

We	O
also	O
compare	O
the	O
results	O
of	O
ST	Method
-	Method
DNN	Method
and	O
BERT	Method
on	O
QNLI	Material
.	O

While	O
ST	Method
-	Method
DNN	Method
is	O
fine	O
-	O
tuned	O
using	O
the	O
pairwise	Metric
ranking	Metric
loss	Metric
,	O
BERT	Method
views	O
QNLI	Material
as	O
binary	Task
classification	Task
and	O
is	O
fine	O
-	O
tuned	O
using	O
the	O
cross	Metric
entropy	Metric
loss	Metric
.	O

That	O
ST	Method
-	Method
DNN	Method
significantly	O
outperforms	O
BERT	Method
demonstrates	O
clearly	O
the	O
importance	O
of	O
problem	Task
formulation	Task
.	O

subsection	O
:	O
SNLI	Material
and	O
SciTail	Material
Results	O
In	O
Table	O
4	O
,	O
we	O
compare	O
our	O
adapted	O
models	O
,	O
using	O
all	O
in	O
-	O
domain	O
training	O
samples	O
,	O
against	O
several	O
strong	O
baselines	O
including	O
the	O
best	O
results	O
reported	O
in	O
the	O
leaderboards	O
.	O

We	O
see	O
that	O
MT	Method
-	Method
DNN	Method
generates	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
both	O
datasets	O
,	O
pushing	O
the	O
benchmarks	O
to	O
91.1	O
%	O
on	O
SNLI	Material
(	O
1.0	O
%	O
absolute	O
improvement	O
)	O
and	O
94.1	O
%	O
on	O
SciTail	Material
(	O
5.8	O
%	O
absolute	O
improvement	O
)	O
,	O
respectively	O
.	O

subsection	O
:	O
Domain	Task
Adaptation	Task
Results	O
One	O
of	O
the	O
most	O
important	O
criteria	O
for	O
building	O
practical	Method
systems	Method
is	O
fast	Task
adaptation	Task
to	O
new	O
tasks	O
and	O
domains	O
.	O

This	O
is	O
because	O
it	O
is	O
prohibitively	O
expensive	O
to	O
collect	O
labeled	O
training	O
data	O
for	O
new	O
domains	O
or	O
tasks	O
.	O

Very	O
often	O
,	O
we	O
only	O
have	O
very	O
small	O
training	O
data	O
or	O
even	O
no	O
training	O
data	O
.	O

To	O
evaluate	O
the	O
models	O
using	O
the	O
above	O
criterion	O
,	O
we	O
perform	O
domain	Task
adaptation	Task
experiments	O
on	O
two	O
NLI	Task
tasks	Task
,	O
SNLI	Material
and	O
SciTail	Material
,	O
using	O
the	O
following	O
procedure	O
:	O
fine	O
-	O
tune	O
the	O
MT	Method
-	Method
DNN	Method
model	O
on	O
eight	O
GLUE	Material
tasks	O
,	O
excluding	O
WNLI	Material
;	O
create	O
for	O
each	O
new	O
task	O
(	O
SNLI	Material
or	O
SciTail	Material
)	O
a	O
task	Method
-	Method
specific	Method
model	Method
,	O
by	O
adapting	O
the	O
trained	O
MT	Method
-	Method
DNN	Method
using	O
task	O
-	O
specific	O
training	O
data	O
;	O
evaluate	O
the	O
models	O
using	O
task	O
-	O
specific	O
test	O
data	O
.	O

We	O
denote	O
the	O
two	O
task	Method
-	Method
specific	Method
models	Method
as	O
MT	Method
-	Method
DNN	Method
.	O

For	O
comparison	O
,	O
we	O
also	O
perform	O
the	O
same	O
adaptation	Method
procedure	Method
to	O
the	O
pre	O
-	O
trained	O
BERT	Method
model	Method
,	O
creating	O
two	O
task	O
-	O
specific	O
BERT	Method
models	O
for	O
SNLI	Material
and	O
SciTail	Material
,	O
respectively	O
,	O
denoted	O
as	O
BERT	Method
.	O

We	O
split	O
the	O
training	O
data	O
of	O
SNLI	Material
and	O
SciTail	Material
,	O
and	O
randomly	O
sample	O
0.1	O
%	O
,	O
1	O
%	O
,	O
10	O
%	O
and	O
100	O
%	O
of	O
its	O
training	O
data	O
.	O

As	O
a	O
result	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SciTail	Material
,	O
which	O
includes	O
23	O
,	O
235	O
,	O
2.3k	O
and	O
23.5k	O
training	O
samples	O
.	O

Similarly	O
,	O
we	O
obtain	O
four	O
sets	O
of	O
training	O
data	O
for	O
SNLI	Material
,	O
which	O
includes	O
549	O
,	O
5.5k	O
,	O
54.9k	O
and	O
549.3k	O
training	O
samples	O
.	O

Results	O
on	O
different	O
amounts	O
of	O
training	O
data	O
of	O
SNLI	Material
and	O
SciTail	Material
are	O
reported	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

We	O
observe	O
that	O
our	O
model	O
pre	O
-	O
trained	O
on	O
GLUE	Material
via	O
multi	Method
-	Method
task	Method
learning	Method
outplays	O
the	O
BERT	Method
baseline	O
consistently	O
.	O

The	O
fewer	O
the	O
training	O
data	O
used	O
,	O
the	O
larger	O
improvement	O
MT	Method
-	Method
DNN	Method
demonstrates	O
over	O
BERT	Method
.	O

For	O
example	O
,	O
with	O
only	O
0.1	O
%	O
(	O
23	O
samples	O
)	O
of	O
the	O
SNLI	Material
training	O
data	O
,	O
MT	Method
-	Method
DNN	Method
achieves	O
82.1	O
%	O
in	O
accuracy	Metric
while	O
BERT	Method
’s	O
accuracy	Metric
is	O
52.5	O
%	O
;	O
with	O
1	O
%	O
of	O
the	O
training	O
data	O
,	O
the	O
accuracy	Metric
of	O
our	O
model	O
is	O
85.2	O
%	O
and	O
BERT	Method
is	O
78.1	O
%	O
.	O

We	O
observe	O
similar	O
results	O
on	O
SciTail	Material
.	O

The	O
results	O
indicate	O
that	O
the	O
representations	O
learned	O
by	O
MT	Method
-	Method
DNN	Method
are	O
more	O
effective	O
for	O
domain	Task
adaptation	Task
than	O
that	O
of	O
BERT	Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
proposed	O
a	O
model	O
called	O
MT	Method
-	Method
DNN	Method
to	O
combine	O
multi	Task
-	Task
task	Task
learning	Task
and	O
language	Task
model	Task
pre	Task
-	Task
training	Task
for	O
language	Task
representation	Task
learning	Task
.	O

MT	Method
-	Method
DNN	Method
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
ten	O
NLU	Task
tasks	O
across	O
three	O
popular	O
benchmarks	O
:	O
SNLI	Material
,	O
SciTail	Material
,	O
and	O
GLUE	Material
.	O

MT	Method
-	Method
DNN	Method
also	O
demonstrates	O
an	O
exceptional	O
generalization	Metric
capability	Metric
in	O
domain	Task
adaptation	Task
experiments	O
.	O

There	O
are	O
many	O
future	O
areas	O
to	O
explore	O
to	O
improve	O
MT	Method
-	Method
DNN	Method
,	O
including	O
a	O
deeper	O
understanding	O
of	O
model	Task
structure	Task
sharing	Task
in	O
MTL	Task
,	O
a	O
more	O
effective	O
training	Method
method	Method
that	O
leverages	O
relatedness	O
among	O
multiple	O
tasks	O
,	O
and	O
ways	O
of	O
incorporating	O
the	O
linguistic	O
structure	O
of	O
text	O
in	O
a	O
more	O
explicit	O
and	O
controllable	O
manner	O
.	O

section	O
:	O
Acknowledgements	O
We	O
would	O
like	O
to	O
thanks	O
Jade	O
Huang	O
from	O
Microsoft	O
for	O
her	O
generous	O
help	O
on	O
this	O
work	O
.	O

bibliography	O
:	O
References	O
