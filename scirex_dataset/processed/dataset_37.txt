document	O
:	O
Neural	Task
Machine	Task
Translation	Task
by	O
Jointly	Task
Learning	Task
to	O
Align	Task
and	O
Translate	Task
Neural	O
machine	Task
translation	Task
is	O
a	O
recently	O
proposed	O
approach	O
to	O
machine	Task
translation	Task
.	O

Unlike	O
the	O
traditional	O
statistical	O
machine	Task
translation	Task
,	O
the	O
neural	Task
machine	Task
translation	Task
aims	O
at	O
building	O
a	O
single	O
neural	Method
network	Method
that	O
can	O
be	O
jointly	O
tuned	O
to	O
maximize	O
the	O
translation	Metric
performance	Metric
.	O

The	O
models	O
proposed	O
recently	O
for	O
neural	Task
machine	Task
translation	Task
often	O
belong	O
to	O
a	O
family	O
of	O
encoder	Method
–	Method
decoders	Method
and	O
encode	O
a	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
from	O
which	O
a	O
decoder	Method
generates	O
a	O
translation	O
.	O

In	O
this	O
paper	O
,	O
we	O
conjecture	O
that	O
the	O
use	O
of	O
a	O
fixed	O
-	O
length	O
vector	O
is	O
a	O
bottleneck	O
in	O
improving	O
the	O
performance	O
of	O
this	O
basic	O
encoder	Method
–	Method
decoder	Method
architecture	Method
,	O
and	O
propose	O
to	O
extend	O
this	O
by	O
allowing	O
a	O
model	O
to	O
automatically	O
(	O
soft	O
-)	O
search	O
for	O
parts	O
of	O
a	O
source	O
sentence	O
that	O
are	O
relevant	O
to	O
predicting	O
a	O
target	O
word	O
,	O
without	O
having	O
to	O
form	O
these	O
parts	O
as	O
a	O
hard	O
segment	O
explicitly	O
.	O

With	O
this	O
new	O
approach	O
,	O
we	O
achieve	O
a	O
translation	Task
performance	O
comparable	O
to	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
phrase	Method
-	Method
based	Method
system	Method
on	O
the	O
task	O
of	O
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
.	O

Furthermore	O
,	O
qualitative	O
analysis	O
reveals	O
that	O
the	O
(	O
soft	O
-)	O
alignments	O
found	O
by	O
the	O
model	O
agree	O
well	O
with	O
our	O
intuition	O
.	O

.	O

/	O
figures	O
/	O
section	O
:	O
Introduction	O
Neural	O
machine	Task
translation	Task
is	O
a	O
newly	O
emerging	O
approach	O
to	O
machine	Task
translation	Task
,	O
recently	O
proposed	O
by	O
,	O
and	O
.	O

Unlike	O
the	O
traditional	O
phrase	Method
-	Method
based	Method
translation	Method
system	Method
which	O
consists	O
of	O
many	O
small	O
sub	O
-	O
components	O
that	O
are	O
tuned	O
separately	O
,	O
neural	Task
machine	Task
translation	Task
attempts	O
to	O
build	O
and	O
train	O
a	O
single	O
,	O
large	O
neural	Method
network	Method
that	O
reads	O
a	O
sentence	O
and	O
outputs	O
a	O
correct	O
translation	O
.	O

Most	O
of	O
the	O
proposed	O
neural	Method
machine	Method
translation	Method
models	Method
belong	O
to	O
a	O
family	O
of	O
encoder	Method
–	Method
decoders	Method
,	O
with	O
an	O
encoder	Method
and	O
a	O
decoder	O
for	O
each	O
language	O
,	O
or	O
involve	O
a	O
language	Method
-	Method
specific	Method
encoder	Method
applied	O
to	O
each	O
sentence	O
whose	O
outputs	O
are	O
then	O
compared	O
.	O

An	O
encoder	Method
neural	Method
network	Method
reads	O
and	O
encodes	O
a	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
.	O

A	O
decoder	Method
then	O
outputs	O
a	O
translation	O
from	O
the	O
encoded	O
vector	O
.	O

The	O
whole	O
encoder	Method
–	Method
decoder	Method
system	Method
,	O
which	O
consists	O
of	O
the	O
encoder	Method
and	O
the	O
decoder	Method
for	O
a	O
language	O
pair	O
,	O
is	O
jointly	O
trained	O
to	O
maximize	O
the	O
probability	O
of	O
a	O
correct	O
translation	O
given	O
a	O
source	O
sentence	O
.	O

A	O
potential	O
issue	O
with	O
this	O
encoder	Method
–	Method
decoder	Method
approach	Method
is	O
that	O
a	O
neural	Method
network	Method
needs	O
to	O
be	O
able	O
to	O
compress	O
all	O
the	O
necessary	O
information	O
of	O
a	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
.	O

This	O
may	O
make	O
it	O
difficult	O
for	O
the	O
neural	Method
network	Method
to	O
cope	O
with	O
long	O
sentences	O
,	O
especially	O
those	O
that	O
are	O
longer	O
than	O
the	O
sentences	O
in	O
the	O
training	O
corpus	O
.	O

showed	O
that	O
indeed	O
the	O
performance	O
of	O
a	O
basic	O
encoder	Method
–	Method
decoder	Method
deteriorates	O
rapidly	O
as	O
the	O
length	O
of	O
an	O
input	O
sentence	O
increases	O
.	O

In	O
order	O
to	O
address	O
this	O
issue	O
,	O
we	O
introduce	O
an	O
extension	O
to	O
the	O
encoder	Method
–	Method
decoder	Method
model	Method
which	O
learns	O
to	O
align	O
and	O
translate	O
jointly	O
.	O

Each	O
time	O
the	O
proposed	O
model	O
generates	O
a	O
word	O
in	O
a	O
translation	O
,	O
it	O
(	O
soft	O
-)	O
searches	O
for	O
a	O
set	O
of	O
positions	O
in	O
a	O
source	O
sentence	O
where	O
the	O
most	O
relevant	O
information	O
is	O
concentrated	O
.	O

The	O
model	O
then	O
predicts	O
a	O
target	O
word	O
based	O
on	O
the	O
context	O
vectors	O
associated	O
with	O
these	O
source	O
positions	O
and	O
all	O
the	O
previous	O
generated	O
target	O
words	O
.	O

The	O
most	O
important	O
distinguishing	O
feature	O
of	O
this	O
approach	O
from	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
is	O
that	O
it	O
does	O
not	O
attempt	O
to	O
encode	O
a	O
whole	O
input	O
sentence	O
into	O
a	O
single	O
fixed	O
-	O
length	O
vector	O
.	O

Instead	O
,	O
it	O
encodes	O
the	O
input	O
sentence	O
into	O
a	O
sequence	O
of	O
vectors	O
and	O
chooses	O
a	O
subset	O
of	O
these	O
vectors	O
adaptively	O
while	O
decoding	O
the	O
translation	Task
.	O

This	O
frees	O
a	O
neural	O
translation	Method
model	Method
from	O
having	O
to	O
squash	O
all	O
the	O
information	O
of	O
a	O
source	O
sentence	O
,	O
regardless	O
of	O
its	O
length	O
,	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
.	O

We	O
show	O
this	O
allows	O
a	O
model	O
to	O
cope	O
better	O
with	O
long	O
sentences	O
.	O

In	O
this	O
paper	O
,	O
we	O
show	O
that	O
the	O
proposed	O
approach	O
of	O
jointly	Task
learning	Task
to	O
align	Task
and	O
translate	Task
achieves	O
significantly	O
improved	O
translation	Metric
performance	Metric
over	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
approach	Method
.	O

The	O
improvement	O
is	O
more	O
apparent	O
with	O
longer	O
sentences	O
,	O
but	O
can	O
be	O
observed	O
with	O
sentences	O
of	O
any	O
length	O
.	O

On	O
the	O
task	O
of	O
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
,	O
the	O
proposed	O
approach	O
achieves	O
,	O
with	O
a	O
single	O
model	O
,	O
a	O
translation	Task
performance	O
comparable	O
,	O
or	O
close	O
,	O
to	O
the	O
conventional	O
phrase	Method
-	Method
based	Method
system	Method
.	O

Furthermore	O
,	O
qualitative	O
analysis	O
reveals	O
that	O
the	O
proposed	O
model	O
finds	O
a	O
linguistically	O
plausible	O
(	O
soft	O
-)	O
alignment	O
between	O
a	O
source	O
sentence	O
and	O
the	O
corresponding	O
target	O
sentence	O
.	O

section	O
:	O
Background	O
:	O
Neural	Task
Machine	Task
Translation	Task
From	O
a	O
probabilistic	O
perspective	O
,	O
translation	Task
is	O
equivalent	O
to	O
finding	O
a	O
target	O
sentence	O
that	O
maximizes	O
the	O
conditional	O
probability	O
of	O
given	O
a	O
source	O
sentence	O
,	O
i.e.	O
,	O
.	O

In	O
neural	Task
machine	Task
translation	Task
,	O
we	O
fit	O
a	O
parameterized	Method
model	Method
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
sentence	O
pairs	O
using	O
a	O
parallel	O
training	O
corpus	O
.	O

Once	O
the	O
conditional	O
distribution	O
is	O
learned	O
by	O
a	O
translation	Method
model	Method
,	O
given	O
a	O
source	O
sentence	O
a	O
corresponding	O
translation	O
can	O
be	O
generated	O
by	O
searching	O
for	O
the	O
sentence	O
that	O
maximizes	O
the	O
conditional	O
probability	O
.	O

Recently	O
,	O
a	O
number	O
of	O
papers	O
have	O
proposed	O
the	O
use	O
of	O
neural	Method
networks	Method
to	O
directly	O
learn	O
this	O
conditional	O
distribution	O
.	O

This	O
neural	O
machine	Task
translation	Task
approach	O
typically	O
consists	O
of	O
two	O
components	O
,	O
the	O
first	O
of	O
which	O
encodes	O
a	O
source	O
sentence	O
and	O
the	O
second	O
decodes	O
to	O
a	O
target	O
sentence	O
.	O

For	O
instance	O
,	O
two	O
recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
were	O
used	O
by	O
and	O
to	O
encode	O
a	O
variable	O
-	O
length	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
and	O
to	O
decode	O
the	O
vector	O
into	O
a	O
variable	O
-	O
length	O
target	O
sentence	O
.	O

Despite	O
being	O
a	O
quite	O
new	O
approach	O
,	O
neural	Task
machine	Task
translation	Task
has	O
already	O
shown	O
promising	O
results	O
.	O

reported	O
that	O
the	O
neural	Task
machine	Task
translation	Task
based	O
on	O
RNNs	Method
with	Method
long	Method
short	Method
-	Method
term	Method
memory	Method
(	Method
LSTM	Method
)	Method
units	Method
achieves	O
close	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
of	O
the	O
conventional	O
phrase	Method
-	Method
based	Method
machine	Method
translation	Method
system	Method
on	O
an	O
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
task	Task
.	O

Adding	O
neural	Method
components	Method
to	O
existing	O
translation	Method
systems	Method
,	O
for	O
instance	O
,	O
to	O
score	O
the	O
phrase	O
pairs	O
in	O
the	O
phrase	O
table	O
or	O
to	O
re	O
-	O
rank	O
candidate	O
translations	O
,	O
has	O
allowed	O
to	O
surpass	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
level	O
.	O

subsection	O
:	O
RNN	Method
Encoder	O
–	O
Decoder	O
Here	O
,	O
we	O
describe	O
briefly	O
the	O
underlying	O
framework	O
,	O
called	O
RNN	Method
Encoder	O
–	O
Decoder	O
,	O
proposed	O
by	O
and	O
upon	O
which	O
we	O
build	O
a	O
novel	O
architecture	O
that	O
learns	O
to	O
align	O
and	O
translate	O
simultaneously	O
.	O

In	O
the	O
Encoder	Method
–	Method
Decoder	Method
framework	Method
,	O
an	O
encoder	Method
reads	O
the	O
input	O
sentence	O
,	O
a	O
sequence	O
of	O
vectors	O
,	O
into	O
a	O
vector	O
.	O

The	O
most	O
common	O
approach	O
is	O
to	O
use	O
an	O
RNN	Method
such	O
that	O
and	O
where	O
is	O
a	O
hidden	O
state	O
at	O
time	O
,	O
and	O
is	O
a	O
vector	O
generated	O
from	O
the	O
sequence	O
of	O
the	O
hidden	O
states	O
.	O

and	O
are	O
some	O
nonlinear	O
functions	O
.	O

used	O
an	O
LSTM	Method
as	O
and	O
,	O
for	O
instance	O
.	O

The	O
decoder	Method
is	O
often	O
trained	O
to	O
predict	O
the	O
next	O
word	O
given	O
the	O
context	O
vector	O
and	O
all	O
the	O
previously	O
predicted	O
words	O
.	O

In	O
other	O
words	O
,	O
the	O
decoder	Method
defines	O
a	O
probability	O
over	O
the	O
translation	O
by	O
decomposing	O
the	O
joint	O
probability	O
into	O
the	O
ordered	O
conditionals	O
:	O
where	O
.	O

With	O
an	O
RNN	Method
,	O
each	O
conditional	O
probability	O
is	O
modeled	O
as	O
where	O
is	O
a	O
nonlinear	O
,	O
potentially	O
multi	O
-	O
layered	O
,	O
function	O
that	O
outputs	O
the	O
probability	O
of	O
,	O
and	O
is	O
the	O
hidden	O
state	O
of	O
the	O
RNN	Method
.	O

It	O
should	O
be	O
noted	O
that	O
other	O
architectures	O
such	O
as	O
a	O
hybrid	O
of	O
an	O
RNN	Method
and	O
a	O
de	Method
-	Method
convolutional	Method
neural	Method
network	Method
can	O
be	O
used	O
.	O

section	O
:	O
Learning	O
to	O
Align	O
and	O
Translate	Task
In	O
this	O
section	O
,	O
we	O
propose	O
a	O
novel	O
architecture	O
for	O
neural	Task
machine	Task
translation	Task
.	O

The	O
new	O
architecture	O
consists	O
of	O
a	O
bidirectional	O
RNN	Method
as	O
an	O
encoder	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
and	O
a	O
decoder	Method
that	O
emulates	O
searching	O
through	O
a	O
source	O
sentence	O
during	O
decoding	O
a	O
translation	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Decoder	Method
:	O
General	O
Description	O
In	O
a	O
new	O
model	Method
architecture	Method
,	O
we	O
define	O
each	O
conditional	O
probability	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
as	O
:	O
where	O
is	O
an	O
RNN	Method
hidden	O
state	O
for	O
time	O
,	O
computed	O
by	O
It	O
should	O
be	O
noted	O
that	O
unlike	O
the	O
existing	O
encoder	Method
–	Method
decoder	Method
approach	Method
(	O
see	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
,	O
here	O
the	O
probability	O
is	O
conditioned	O
on	O
a	O
distinct	O
context	O
vector	O
for	O
each	O
target	O
word	O
.	O

The	O
context	O
vector	O
depends	O
on	O
a	O
sequence	O
of	O
annotations	O
to	O
which	O
an	O
encoder	Method
maps	O
the	O
input	O
sentence	O
.	O

Each	O
annotation	O
contains	O
information	O
about	O
the	O
whole	O
input	O
sequence	O
with	O
a	O
strong	O
focus	O
on	O
the	O
parts	O
surrounding	O
the	O
-	O
th	O
word	O
of	O
the	O
input	O
sequence	O
.	O

We	O
explain	O
in	O
detail	O
how	O
the	O
annotations	O
are	O
computed	O
in	O
the	O
next	O
section	O
.	O

The	O
context	O
vector	O
is	O
,	O
then	O
,	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
these	O
annotations	O
:	O
The	O
weight	O
of	O
each	O
annotation	O
is	O
computed	O
by	O
where	O
is	O
an	O
alignment	Method
model	Method
which	O
scores	O
how	O
well	O
the	O
inputs	O
around	O
position	O
and	O
the	O
output	O
at	O
position	O
match	O
.	O

The	O
score	O
is	O
based	O
on	O
the	O
RNN	Method
hidden	O
state	O
(	O
just	O
before	O
emitting	O
,	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
and	O
the	O
-	O
th	O
annotation	O
of	O
the	O
input	O
sentence	O
.	O

We	O
parametrize	O
the	O
alignment	Method
model	Method
as	O
a	O
feedforward	Method
neural	Method
network	Method
which	O
is	O
jointly	O
trained	O
with	O
all	O
the	O
other	O
components	O
of	O
the	O
proposed	O
system	O
.	O

Note	O
that	O
unlike	O
in	O
traditional	O
machine	Task
translation	Task
,	O
the	O
alignment	Task
is	O
not	O
considered	O
to	O
be	O
a	O
latent	O
variable	O
.	O

Instead	O
,	O
the	O
alignment	Method
model	Method
directly	O
computes	O
a	O
soft	O
alignment	O
,	O
which	O
allows	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
to	O
be	O
backpropagated	O
through	O
.	O

This	O
gradient	O
can	O
be	O
used	O
to	O
train	O
the	O
alignment	Method
model	Method
as	O
well	O
as	O
the	O
whole	O
translation	Method
model	Method
jointly	O
.	O

We	O
can	O
understand	O
the	O
approach	O
of	O
taking	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
annotations	O
as	O
computing	O
an	O
expected	O
annotation	O
,	O
where	O
the	O
expectation	O
is	O
over	O
possible	O
alignments	O
.	O

Let	O
be	O
a	O
probability	O
that	O
the	O
target	O
word	O
is	O
aligned	O
to	O
,	O
or	O
translated	O
from	O
,	O
a	O
source	O
word	O
.	O

Then	O
,	O
the	O
-	O
th	O
context	O
vector	O
is	O
the	O
expected	O
annotation	O
over	O
all	O
the	O
annotations	O
with	O
probabilities	O
.	O

The	O
probability	O
,	O
or	O
its	O
associated	O
energy	O
,	O
reflects	O
the	O
importance	O
of	O
the	O
annotation	O
with	O
respect	O
to	O
the	O
previous	O
hidden	O
state	O
in	O
deciding	O
the	O
next	O
state	O
and	O
generating	O
.	O

Intuitively	O
,	O
this	O
implements	O
a	O
mechanism	Method
of	Method
attention	Method
in	O
the	O
decoder	Method
.	O

The	O
decoder	O
decides	O
parts	O
of	O
the	O
source	O
sentence	O
to	O
pay	O
attention	O
to	O
.	O

By	O
letting	O
the	O
decoder	Method
have	O
an	O
attention	Method
mechanism	Method
,	O
we	O
relieve	O
the	O
encoder	O
from	O
the	O
burden	O
of	O
having	O
to	O
encode	O
all	O
information	O
in	O
the	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
.	O

With	O
this	O
new	O
approach	O
the	O
information	O
can	O
be	O
spread	O
throughout	O
the	O
sequence	O
of	O
annotations	O
,	O
which	O
can	O
be	O
selectively	O
retrieved	O
by	O
the	O
decoder	Method
accordingly	O
.	O

subsection	O
:	O
Encoder	Method
:	O
Bidirectional	O
RNN	Method
for	O
Annotating	Task
Sequences	Task
The	O
usual	O
RNN	Method
,	O
described	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
reads	O
an	O
input	O
sequence	O
in	O
order	O
starting	O
from	O
the	O
first	O
symbol	O
to	O
the	O
last	O
one	O
.	O

However	O
,	O
in	O
the	O
proposed	O
scheme	O
,	O
we	O
would	O
like	O
the	O
annotation	O
of	O
each	O
word	O
to	O
summarize	O
not	O
only	O
the	O
preceding	O
words	O
,	O
but	O
also	O
the	O
following	O
words	O
.	O

Hence	O
,	O
we	O
propose	O
to	O
use	O
a	O
bidirectional	O
RNN	Method
,	O
which	O
has	O
been	O
successfully	O
used	O
recently	O
in	O
speech	Task
recognition	Task
.	O

A	O
BiRNN	Method
consists	O
of	O
forward	O
and	O
backward	O
RNN	Method
’s	O
.	O

The	O
forward	O
RNN	Method
reads	O
the	O
input	O
sequence	O
as	O
it	O
is	O
ordered	O
(	O
from	O
to	O
)	O
and	O
calculates	O
a	O
sequence	O
of	O
forward	O
hidden	O
states	O
.	O

The	O
backward	O
RNN	Method
reads	O
the	O
sequence	O
in	O
the	O
reverse	O
order	O
(	O
from	O
to	O
)	O
,	O
resulting	O
in	O
a	O
sequence	O
of	O
backward	O
hidden	O
states	O
.	O

We	O
obtain	O
an	O
annotation	O
for	O
each	O
word	O
by	O
concatenating	O
the	O
forward	O
hidden	O
state	O
and	O
the	O
backward	O
one	O
,	O
i.e.	O
,	O
.	O

In	O
this	O
way	O
,	O
the	O
annotation	O
contains	O
the	O
summaries	O
of	O
both	O
the	O
preceding	O
words	O
and	O
the	O
following	O
words	O
.	O

Due	O
to	O
the	O
tendency	O
of	O
RNNs	Method
to	O
better	O
represent	O
recent	O
inputs	O
,	O
the	O
annotation	O
will	O
be	O
focused	O
on	O
the	O
words	O
around	O
.	O

This	O
sequence	O
of	O
annotations	O
is	O
used	O
by	O
the	O
decoder	Method
and	O
the	O
alignment	Method
model	Method
later	O
to	O
compute	O
the	O
context	O
vector	O
(	O
Eqs	O
.	O

(	O
[	O
reference	O
]	O
)	O
–	O
(	O
[	O
reference	O
]	O
)	O
)	O
.	O

See	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
the	O
graphical	O
illustration	O
of	O
the	O
proposed	O
model	O
.	O

section	O
:	O
Experiment	O
Settings	O
We	O
evaluate	O
the	O
proposed	O
approach	O
on	O
the	O
task	O
of	O
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
.	O

We	O
use	O
the	O
bilingual	O
,	O
parallel	O
corpora	O
provided	O
by	O
ACL	O
WMT	Material
’	Material
14	Material
.	O

As	O
a	O
comparison	O
,	O
we	O
also	O
report	O
the	O
performance	O
of	O
an	O
RNN	Method
Encoder	O
–	O
Decoder	O
which	O
was	O
proposed	O
recently	O
by	O
.	O

We	O
use	O
the	O
same	O
training	O
procedures	O
and	O
the	O
same	O
dataset	O
for	O
both	O
models	O
.	O

subsection	O
:	O
Dataset	O
WMT	Material
’	Material
14	Material
contains	O
the	O
following	O
English	O
-	O
French	O
parallel	O
corpora	O
:	O
Europarl	O
(	O
61	O
M	O
words	O
)	O
,	O
news	O
commentary	O
(	O
5.5	O
M	O
)	O
,	O
UN	O
(	O
421	O
M	O
)	O
and	O
two	O
crawled	O
corpora	O
of	O
90	O
M	O
and	O
272.5	O
M	O
words	O
respectively	O
,	O
totaling	O
850	O
M	O
words	O
.	O

Following	O
the	O
procedure	O
described	O
in	O
,	O
we	O
reduce	O
the	O
size	O
of	O
the	O
combined	O
corpus	O
to	O
have	O
348	O
M	O
words	O
using	O
the	O
data	Method
selection	Method
method	Method
by	O
.	O

We	O
do	O
not	O
use	O
any	O
monolingual	O
data	O
other	O
than	O
the	O
mentioned	O
parallel	O
corpora	O
,	O
although	O
it	O
may	O
be	O
possible	O
to	O
use	O
a	O
much	O
larger	O
monolingual	O
corpus	O
to	O
pretrain	O
an	O
encoder	Method
.	O

We	O
concatenate	O
news	O
-	O
test	O
-	O
2012	O
and	O
news	O
-	O
test	O
-	O
2013	O
to	O
make	O
a	O
development	O
(	O
validation	O
)	O
set	O
,	O
and	O
evaluate	O
the	O
models	O
on	O
the	O
test	O
set	O
(	O
news	O
-	O
test	O
-	O
2014	O
)	O
from	O
WMT	Material
’	Material
14	Material
,	O
which	O
consists	O
of	O
3003	O
sentences	O
not	O
present	O
in	O
the	O
training	O
data	O
.	O

After	O
a	O
usual	O
tokenization	Method
,	O
we	O
use	O
a	O
shortlist	O
of	O
30	O
,	O
000	O
most	O
frequent	O
words	O
in	O
each	O
language	O
to	O
train	O
our	O
models	O
.	O

Any	O
word	O
not	O
included	O
in	O
the	O
shortlist	O
is	O
mapped	O
to	O
a	O
special	O
token	O
(	O
)	O
.	O

We	O
do	O
not	O
apply	O
any	O
other	O
special	O
preprocessing	Method
,	O
such	O
as	O
lowercasing	Method
or	O
stemming	Method
,	O
to	O
the	O
data	O
.	O

(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
subsection	O
:	O
Models	O
We	O
train	O
two	O
types	O
of	O
models	O
.	O

The	O
first	O
one	O
is	O
an	O
RNN	Method
Encoder	O
–	O
Decoder	O
,	O
and	O
the	O
other	O
is	O
the	O
proposed	O
model	O
,	O
to	O
which	O
we	O
refer	O
as	O
RNNsearch	Method
.	O

We	O
train	O
each	O
model	O
twice	O
:	O
first	O
with	O
the	O
sentences	O
of	O
length	O
up	O
to	O
30	O
words	O
(	O
RNNencdec	Method
-	Method
30	Method
,	O
RNNsearch	Method
-	Method
30	Method
)	O
and	O
then	O
with	O
the	O
sentences	O
of	O
length	O
up	O
to	O
50	O
word	O
(	O
RNNencdec	Method
-	Method
50	Method
,	O
RNNsearch	Method
-	Method
50	Method
)	O
.	O

The	O
encoder	Method
and	Method
decoder	Method
of	O
the	O
RNNencdec	Method
have	O
1000	O
hidden	O
units	O
each	O
.	O

The	O
encoder	Method
of	O
the	O
RNNsearch	Method
consists	O
of	O
forward	Method
and	Method
backward	Method
recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
each	O
having	O
1000	O
hidden	O
units	O
.	O

Its	O
decoder	Method
has	O
1000	O
hidden	O
units	O
.	O

In	O
both	O
cases	O
,	O
we	O
use	O
a	O
multilayer	Method
network	Method
with	O
a	O
single	O
maxout	Method
hidden	Method
layer	Method
to	O
compute	O
the	O
conditional	O
probability	O
of	O
each	O
target	O
word	O
.	O

We	O
use	O
a	O
minibatch	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
algorithm	O
together	O
with	O
Adadelta	Method
to	O
train	O
each	O
model	O
.	O

Each	O
SGD	Method
update	O
direction	O
is	O
computed	O
using	O
a	O
minibatch	O
of	O
80	O
sentences	O
.	O

We	O
trained	O
each	O
model	O
for	O
approximately	O
5	O
days	O
.	O

Once	O
a	O
model	O
is	O
trained	O
,	O
we	O
use	O
a	O
beam	Method
search	Method
to	O
find	O
a	O
translation	O
that	O
approximately	O
maximizes	O
the	O
conditional	O
probability	O
.	O

used	O
this	O
approach	O
to	O
generate	O
translations	O
from	O
their	O
neural	Method
machine	Method
translation	Method
model	Method
.	O

For	O
more	O
details	O
on	O
the	O
architectures	O
of	O
the	O
models	O
and	O
training	O
procedure	O
used	O
in	O
the	O
experiments	O
,	O
see	O
Appendices	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

section	O
:	O
Results	O
subsection	O
:	O
Quantitative	O
Results	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
list	O
the	O
translation	Metric
performances	Metric
measured	O
in	O
BLEU	Metric
score	Metric
.	O

It	O
is	O
clear	O
from	O
the	O
table	O
that	O
in	O
all	O
the	O
cases	O
,	O
the	O
proposed	O
RNNsearch	Method
outperforms	O
the	O
conventional	O
RNNencdec	Method
.	O

More	O
importantly	O
,	O
the	O
performance	O
of	O
the	O
RNNsearch	Method
is	O
as	O
high	O
as	O
that	O
of	O
the	O
conventional	O
phrase	Method
-	Method
based	Method
translation	Method
system	Method
(	O
Moses	Method
)	O
,	O
when	O
only	O
the	O
sentences	O
consisting	O
of	O
known	O
words	O
are	O
considered	O
.	O

This	O
is	O
a	O
significant	O
achievement	O
,	O
considering	O
that	O
Moses	O
uses	O
a	O
separate	O
monolingual	O
corpus	O
(	O
418	O
M	O
words	O
)	O
in	O
addition	O
to	O
the	O
parallel	O
corpora	O
we	O
used	O
to	O
train	O
the	O
RNNsearch	Method
and	O
RNNencdec	Method
.	O

One	O
of	O
the	O
motivations	O
behind	O
the	O
proposed	O
approach	O
was	O
the	O
use	O
of	O
a	O
fixed	O
-	O
length	O
context	O
vector	O
in	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
approach	Method
.	O

We	O
conjectured	O
that	O
this	O
limitation	O
may	O
make	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
approach	Method
to	O
underperform	O
with	O
long	O
sentences	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
see	O
that	O
the	O
performance	O
of	O
RNNencdec	Method
dramatically	O
drops	O
as	O
the	O
length	O
of	O
the	O
sentences	O
increases	O
.	O

On	O
the	O
other	O
hand	O
,	O
both	O
RNNsearch	Method
-	Method
30	Method
and	O
RNNsearch	Method
-	Method
50	Method
are	O
more	O
robust	O
to	O
the	O
length	O
of	O
the	O
sentences	O
.	O

RNNsearch	Method
-	Method
50	Method
,	O
especially	O
,	O
shows	O
no	O
performance	O
deterioration	O
even	O
with	O
sentences	O
of	O
length	O
50	O
or	O
more	O
.	O

This	O
superiority	O
of	O
the	O
proposed	O
model	O
over	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
is	O
further	O
confirmed	O
by	O
the	O
fact	O
that	O
the	O
RNNsearch	Method
-	Method
30	Method
even	O
outperforms	O
RNNencdec	Method
-	Method
50	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Qualitative	Method
Analysis	Method
subsubsection	O
:	O
Alignment	Task
The	O
proposed	O
approach	O
provides	O
an	O
intuitive	O
way	O
to	O
inspect	O
the	O
(	O
soft	O
-)	O
alignment	O
between	O
the	O
words	O
in	O
a	O
generated	O
translation	O
and	O
those	O
in	O
a	O
source	O
sentence	O
.	O

This	O
is	O
done	O
by	O
visualizing	O
the	O
annotation	O
weights	O
from	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

Each	O
row	O
of	O
a	O
matrix	O
in	O
each	O
plot	O
indicates	O
the	O
weights	O
associated	O
with	O
the	O
annotations	O
.	O

From	O
this	O
we	O
see	O
which	O
positions	O
in	O
the	O
source	O
sentence	O
were	O
considered	O
more	O
important	O
when	O
generating	O
the	O
target	O
word	O
.	O

We	O
can	O
see	O
from	O
the	O
alignments	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
that	O
the	O
alignment	O
of	O
words	O
between	O
English	O
and	O
French	O
is	O
largely	O
monotonic	O
.	O

We	O
see	O
strong	O
weights	O
along	O
the	O
diagonal	O
of	O
each	O
matrix	O
.	O

However	O
,	O
we	O
also	O
observe	O
a	O
number	O
of	O
non	O
-	O
trivial	O
,	O
non	O
-	O
monotonic	O
alignments	O
.	O

Adjectives	O
and	O
nouns	O
are	O
typically	O
ordered	O
differently	O
between	O
French	O
and	O
English	O
,	O
and	O
we	O
see	O
an	O
example	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
.	O

From	O
this	O
figure	O
,	O
we	O
see	O
that	O
the	O
model	O
correctly	O
translates	O
a	O
phrase	O
[	O
European	O
Economic	O
Area	O
]	O
into	O
[	O
zone	O
économique	O
européen	O
]	O
.	O

The	O
RNNsearch	Method
was	O
able	O
to	O
correctly	O
align	O
[	O
zone	O
]	O
with	O
[	O
Area	O
]	O
,	O
jumping	O
over	O
the	O
two	O
words	O
(	O
[	O
European	O
]	O
and	O
[	O
Economic	O
]	O
)	O
,	O
and	O
then	O
looked	O
one	O
word	O
back	O
at	O
a	O
time	O
to	O
complete	O
the	O
whole	O
phrase	O
[	O
zone	O
économique	O
européenne	O
]	O
.	O

The	O
strength	O
of	O
the	O
soft	Method
-	Method
alignment	Method
,	O
opposed	O
to	O
a	O
hard	O
-	O
alignment	O
,	O
is	O
evident	O
,	O
for	O
instance	O
,	O
from	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
.	O

Consider	O
the	O
source	O
phrase	O
[	O
the	O
man	O
]	O
which	O
was	O
translated	O
into	O
[	O
l	O
’	O
homme	O
]	O
.	O

Any	O
hard	O
alignment	O
will	O
map	O
[	O
the	O
]	O
to	O
[	O
l	O
’	O
]	O
and	O
[	O
man	O
]	O
to	O
[	O
homme	O
]	O
.	O

This	O
is	O
not	O
helpful	O
for	O
translation	Task
,	O
as	O
one	O
must	O
consider	O
the	O
word	O
following	O
[	O
the	O
]	O
to	O
determine	O
whether	O
it	O
should	O
be	O
translated	O
into	O
[	O
le	O
]	O
,	O
[	O
la	O
]	O
,	O
[	O
les	O
]	O
or	O
[	O
l’	O
]	O
.	O

Our	O
soft	Method
-	Method
alignment	Method
solves	O
this	O
issue	O
naturally	O
by	O
letting	O
the	O
model	O
look	O
at	O
both	O
[	O
the	O
]	O
and	O
[	O
man	O
]	O
,	O
and	O
in	O
this	O
example	O
,	O
we	O
see	O
that	O
the	O
model	O
was	O
able	O
to	O
correctly	O
translate	O
[	O
the	O
]	O
into	O
[	O
l’	O
]	O
.	O

We	O
observe	O
similar	O
behaviors	O
in	O
all	O
the	O
presented	O
cases	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

An	O
additional	O
benefit	O
of	O
the	O
soft	Task
alignment	Task
is	O
that	O
it	O
naturally	O
deals	O
with	O
source	O
and	O
target	O
phrases	O
of	O
different	O
lengths	O
,	O
without	O
requiring	O
a	O
counter	O
-	O
intuitive	O
way	O
of	O
mapping	O
some	O
words	O
to	O
or	O
from	O
nowhere	O
(	O
[	O
NULL	O
]	O
)	O
.	O

subsubsection	O
:	O
Long	O
Sentences	O
As	O
clearly	O
visible	O
from	O
Fig	O
.	O

[	O
reference	O
]	O
the	O
proposed	O
model	O
(	O
RNNsearch	Method
)	O
is	O
much	O
better	O
than	O
the	O
conventional	O
model	O
(	O
RNNencdec	Method
)	O
at	O
translating	O
long	O
sentences	O
.	O

This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
the	O
RNNsearch	Method
does	O
not	O
require	O
encoding	O
a	O
long	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
perfectly	O
,	O
but	O
only	O
accurately	O
encoding	O
the	O
parts	O
of	O
the	O
input	O
sentence	O
that	O
surround	O
a	O
particular	O
word	O
.	O

As	O
an	O
example	O
,	O
consider	O
this	O
source	O
sentence	O
from	O
the	O
test	O
set	O
:	O
An	O
admitting	O
privilege	O
is	O
the	O
right	O
of	O
a	O
doctor	O
to	O
admit	O
a	O
patient	O
to	O
a	O
hospital	O
or	O
a	O
medical	O
centre	O
to	O
carry	O
out	O
a	O
diagnosis	O
or	O
a	O
procedure	O
,	O
based	O
on	O
his	O
status	O
as	O
a	O
health	O
care	O
worker	O
at	O
a	O
hospital	O
.	O

The	O
RNNencdec	O
-	O
50	O
translated	O
this	O
sentence	O
into	O
:	O
Un	O
privilège	O
d’admission	O
est	O
le	O
droit	O
d’un	O
médecin	O
de	O
reconnaître	O
un	O
patient	O
à	O
l’hôpital	O
ou	O
un	O
centre	O
médical	O
d’un	O
diagnostic	O
ou	O
de	O
prendre	O
un	O
diagnostic	O
en	O
fonction	O
de	O
son	O
état	O
de	O
santé	O
.	O

The	O
RNNencdec	Method
-	Method
50	Method
correctly	O
translated	O
the	O
source	O
sentence	O
until	O
[	O
a	O
medical	O
center	O
]	O
.	O

However	O
,	O
from	O
there	O
on	O
(	O
underlined	O
)	O
,	O
it	O
deviated	O
from	O
the	O
original	O
meaning	O
of	O
the	O
source	O
sentence	O
.	O

For	O
instance	O
,	O
it	O
replaced	O
[	O
based	O
on	O
his	O
status	O
as	O
a	O
health	O
care	O
worker	O
at	O
a	O
hospital	O
]	O
in	O
the	O
source	O
sentence	O
with	O
[	O
en	O
fonction	O
de	O
son	O
état	O
de	O
santé	O
]	O
(	O
“	O
based	O
on	O
his	O
state	O
of	O
health	O
”	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
RNNsearch	Method
-	Method
50	Method
generated	O
the	O
following	O
correct	O
translation	O
,	O
preserving	O
the	O
whole	O
meaning	O
of	O
the	O
input	O
sentence	O
without	O
omitting	O
any	O
details	O
:	O
Un	O
privilège	O
d’admission	O
est	O
le	O
droit	O
d’un	O
médecin	O
d’admettre	O
un	O
patient	O
à	O
un	O
hôpital	O
ou	O
un	O
centre	O
médical	O
pour	O
effectuer	O
un	O
diagnostic	O
ou	O
une	O
procédure	O
,	O
selon	O
son	O
statut	O
de	O
travailleur	O
des	O
soins	O
de	O
santé	O
à	O
l’hôpital	O
.	O

Let	O
us	O
consider	O
another	O
sentence	O
from	O
the	O
test	O
set	O
:	O
This	O
kind	O
of	O
experience	O
is	O
part	O
of	O
Disney	O
’s	O
efforts	O
to	O
”	O
extend	O
the	O
lifetime	O
of	O
its	O
series	O
and	O
build	O
new	O
relationships	O
with	O
audiences	O
via	O
digital	O
platforms	O
that	O
are	O
becoming	O
ever	O
more	O
important	O
,	O
”	O
he	O
added	O
.	O

The	O
translation	O
by	O
the	O
RNNencdec	Method
-	Method
50	Method
is	O
Ce	O
type	O
d’expérience	O
fait	O
partie	O
des	O
initiatives	O
du	O
Disney	O
pour	O
”	O
prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
nouvelles	O
et	O
de	O
développer	O
des	O
liens	O
avec	O
les	O
lecteurs	O
numériques	O
qui	O
deviennent	O
plus	O
complexes	O
.	O

As	O
with	O
the	O
previous	O
example	O
,	O
the	O
RNNencdec	Method
began	O
deviating	O
from	O
the	O
actual	O
meaning	O
of	O
the	O
source	O
sentence	O
after	O
generating	O
approximately	O
30	O
words	O
(	O
see	O
the	O
underlined	O
phrase	O
)	O
.	O

After	O
that	O
point	O
,	O
the	O
quality	O
of	O
the	O
translation	O
deteriorates	O
,	O
with	O
basic	O
mistakes	O
such	O
as	O
the	O
lack	O
of	O
a	O
closing	O
quotation	O
mark	O
.	O

Again	O
,	O
the	O
RNNsearch	Method
-	Method
50	Method
was	O
able	O
to	O
translate	O
this	O
long	O
sentence	O
correctly	O
:	O
Ce	O
genre	O
d’expérience	O
fait	O
partie	O
des	O
efforts	O
de	O
Disney	O
pour	O
”	O
prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
séries	O
et	O
créer	O
de	O
nouvelles	O
relations	O
avec	O
des	O
publics	O
via	O
des	O
plateformes	O
numériques	O
de	O
plus	O
en	O
plus	O
importantes	O
”	O
,	O
a	O
-	O
t	O
-	O
il	O
ajouté	O
.	O

In	O
conjunction	O
with	O
the	O
quantitative	O
results	O
presented	O
already	O
,	O
these	O
qualitative	O
observations	O
confirm	O
our	O
hypotheses	O
that	O
the	O
RNNsearch	Method
architecture	O
enables	O
far	O
more	O
reliable	O
translation	Task
of	O
long	O
sentences	O
than	O
the	O
standard	O
RNNencdec	Method
model	Method
.	O

In	O
Appendix	O
[	O
reference	O
]	O
,	O
we	O
provide	O
a	O
few	O
more	O
sample	O
translations	O
of	O
long	O
source	O
sentences	O
generated	O
by	O
the	O
RNNencdec	Method
-	Method
50	Method
,	O
RNNsearch	Method
-	Method
50	Method
and	O
Google	Method
Translate	Method
along	O
with	O
the	O
reference	O
translations	O
.	O

section	O
:	O
Related	O
Work	O
subsection	O
:	O
Learning	O
to	O
Align	O
A	O
similar	O
approach	O
of	O
aligning	O
an	O
output	O
symbol	O
with	O
an	O
input	O
symbol	O
was	O
proposed	O
recently	O
by	O
in	O
the	O
context	O
of	O
handwriting	Task
synthesis	Task
.	O

Handwriting	Task
synthesis	Task
is	O
a	O
task	O
where	O
the	O
model	O
is	O
asked	O
to	O
generate	O
handwriting	O
of	O
a	O
given	O
sequence	O
of	O
characters	O
.	O

In	O
his	O
work	O
,	O
he	O
used	O
a	O
mixture	Method
of	Method
Gaussian	Method
kernels	Method
to	O
compute	O
the	O
weights	O
of	O
the	O
annotations	O
,	O
where	O
the	O
location	O
,	O
width	O
and	O
mixture	O
coefficient	O
of	O
each	O
kernel	O
was	O
predicted	O
from	O
an	O
alignment	Method
model	Method
.	O

More	O
specifically	O
,	O
his	O
alignment	O
was	O
restricted	O
to	O
predict	O
the	O
location	O
such	O
that	O
the	O
location	O
increases	O
monotonically	O
.	O

The	O
main	O
difference	O
from	O
our	O
approach	O
is	O
that	O
,	O
in	O
,	O
the	O
modes	O
of	O
the	O
weights	O
of	O
the	O
annotations	O
only	O
move	O
in	O
one	O
direction	O
.	O

In	O
the	O
context	O
of	O
machine	Task
translation	Task
,	O
this	O
is	O
a	O
severe	O
limitation	O
,	O
as	O
(	O
long	O
-	O
distance	O
)	O
reordering	O
is	O
often	O
needed	O
to	O
generate	O
a	O
grammatically	O
correct	O
translation	O
(	O
for	O
instance	O
,	O
English	O
-	O
to	O
-	O
German	O
)	O
.	O

Our	O
approach	O
,	O
on	O
the	O
other	O
hand	O
,	O
requires	O
computing	O
the	O
annotation	O
weight	O
of	O
every	O
word	O
in	O
the	O
source	O
sentence	O
for	O
each	O
word	O
in	O
the	O
translation	O
.	O

This	O
drawback	O
is	O
not	O
severe	O
with	O
the	O
task	O
of	O
translation	Task
in	O
which	O
most	O
of	O
input	O
and	O
output	O
sentences	O
are	O
only	O
15–40	O
words	O
.	O

However	O
,	O
this	O
may	O
limit	O
the	O
applicability	O
of	O
the	O
proposed	O
scheme	O
to	O
other	O
tasks	O
.	O

subsection	O
:	O
Neural	Method
Networks	Method
for	O
Machine	Task
Translation	Task
Since	O
introduced	O
a	O
neural	Method
probabilistic	Method
language	Method
model	Method
which	O
uses	O
a	O
neural	Method
network	Method
to	O
model	O
the	O
conditional	O
probability	O
of	O
a	O
word	O
given	O
a	O
fixed	O
number	O
of	O
the	O
preceding	O
words	O
,	O
neural	Method
networks	Method
have	O
widely	O
been	O
used	O
in	O
machine	Task
translation	Task
.	O

However	O
,	O
the	O
role	O
of	O
neural	Method
networks	Method
has	O
been	O
largely	O
limited	O
to	O
simply	O
providing	O
a	O
single	O
feature	O
to	O
an	O
existing	O
statistical	Method
machine	Method
translation	Method
system	Method
or	O
to	O
re	O
-	O
rank	O
a	O
list	O
of	O
candidate	O
translations	O
provided	O
by	O
an	O
existing	O
system	O
.	O

For	O
instance	O
,	O
proposed	O
using	O
a	O
feedforward	Method
neural	Method
network	Method
to	O
compute	O
the	O
score	O
of	O
a	O
pair	O
of	O
source	O
and	O
target	O
phrases	O
and	O
to	O
use	O
the	O
score	O
as	O
an	O
additional	O
feature	O
in	O
the	O
phrase	O
-	O
based	O
statistical	O
machine	Task
translation	Task
system	O
.	O

More	O
recently	O
,	O
and	O
reported	O
the	O
successful	O
use	O
of	O
the	O
neural	Method
networks	Method
as	O
a	O
sub	O
-	O
component	O
of	O
the	O
existing	O
translation	Method
system	Method
.	O

Traditionally	O
,	O
a	O
neural	Method
network	Method
trained	O
as	O
a	O
target	Method
-	Method
side	Method
language	Method
model	Method
has	O
been	O
used	O
to	O
rescore	O
or	O
rerank	O
a	O
list	O
of	O
candidate	O
translations	O
.	O

Although	O
the	O
above	O
approaches	O
were	O
shown	O
to	O
improve	O
the	O
translation	Metric
performance	Metric
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
machine	Method
translation	Method
systems	Method
,	O
we	O
are	O
more	O
interested	O
in	O
a	O
more	O
ambitious	O
objective	O
of	O
designing	O
a	O
completely	O
new	O
translation	Method
system	Method
based	O
on	O
neural	Method
networks	Method
.	O

The	O
neural	O
machine	Task
translation	Task
approach	O
we	O
consider	O
in	O
this	O
paper	O
is	O
therefore	O
a	O
radical	O
departure	O
from	O
these	O
earlier	O
works	O
.	O

Rather	O
than	O
using	O
a	O
neural	Method
network	Method
as	O
a	O
part	O
of	O
the	O
existing	O
system	O
,	O
our	O
model	O
works	O
on	O
its	O
own	O
and	O
generates	O
a	O
translation	Task
from	O
a	O
source	O
sentence	O
directly	O
.	O

section	O
:	O
Conclusion	O
The	O
conventional	O
approach	O
to	O
neural	Task
machine	Task
translation	Task
,	O
called	O
an	O
encoder	Method
–	Method
decoder	Method
approach	Method
,	O
encodes	O
a	O
whole	O
input	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
from	O
which	O
a	O
translation	O
will	O
be	O
decoded	O
.	O

We	O
conjectured	O
that	O
the	O
use	O
of	O
a	O
fixed	O
-	O
length	O
context	O
vector	O
is	O
problematic	O
for	O
translating	O
long	O
sentences	O
,	O
based	O
on	O
a	O
recent	O
empirical	O
study	O
reported	O
by	O
and	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
novel	O
architecture	O
that	O
addresses	O
this	O
issue	O
.	O

We	O
extended	O
the	O
basic	O
encoder	Method
–	Method
decoder	Method
by	O
letting	O
a	O
model	O
(	O
soft	O
-)	O
search	O
for	O
a	O
set	O
of	O
input	O
words	O
,	O
or	O
their	O
annotations	O
computed	O
by	O
an	O
encoder	Method
,	O
when	O
generating	O
each	O
target	O
word	O
.	O

This	O
frees	O
the	O
model	O
from	O
having	O
to	O
encode	O
a	O
whole	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
,	O
and	O
also	O
lets	O
the	O
model	O
focus	O
only	O
on	O
information	O
relevant	O
to	O
the	O
generation	O
of	O
the	O
next	O
target	O
word	O
.	O

This	O
has	O
a	O
major	O
positive	O
impact	O
on	O
the	O
ability	O
of	O
the	O
neural	Method
machine	Method
translation	Method
system	Method
to	O
yield	O
good	O
results	O
on	O
longer	O
sentences	O
.	O

Unlike	O
with	O
the	O
traditional	O
machine	Method
translation	Method
systems	Method
,	O
all	O
of	O
the	O
pieces	O
of	O
the	O
translation	Method
system	Method
,	O
including	O
the	O
alignment	Method
mechanism	Method
,	O
are	O
jointly	O
trained	O
towards	O
a	O
better	O
log	Metric
-	Metric
probability	Metric
of	O
producing	O
correct	O
translations	O
.	O

We	O
tested	O
the	O
proposed	O
model	O
,	O
called	O
RNNsearch	Method
,	O
on	O
the	O
task	O
of	O
English	Task
-	Task
to	Task
-	Task
French	Task
translation	Task
.	O

The	O
experiment	O
revealed	O
that	O
the	O
proposed	O
RNNsearch	Method
outperforms	O
the	O
conventional	O
encoder	Method
–	Method
decoder	Method
model	Method
(	O
RNNencdec	Method
)	O
significantly	O
,	O
regardless	O
of	O
the	O
sentence	O
length	O
and	O
that	O
it	O
is	O
much	O
more	O
robust	O
to	O
the	O
length	O
of	O
a	O
source	O
sentence	O
.	O

From	O
the	O
qualitative	O
analysis	O
where	O
we	O
investigated	O
the	O
(	O
soft	O
-)	O
alignment	O
generated	O
by	O
the	O
RNNsearch	Method
,	O
we	O
were	O
able	O
to	O
conclude	O
that	O
the	O
model	O
can	O
correctly	O
align	O
each	O
target	O
word	O
with	O
the	O
relevant	O
words	O
,	O
or	O
their	O
annotations	O
,	O
in	O
the	O
source	O
sentence	O
as	O
it	O
generated	O
a	O
correct	O
translation	O
.	O

Perhaps	O
more	O
importantly	O
,	O
the	O
proposed	O
approach	O
achieved	O
a	O
translation	Task
performance	O
comparable	O
to	O
the	O
existing	O
phrase	Method
-	Method
based	Method
statistical	Method
machine	Method
translation	Method
.	O

It	O
is	O
a	O
striking	O
result	O
,	O
considering	O
that	O
the	O
proposed	O
architecture	O
,	O
or	O
the	O
whole	O
family	O
of	O
neural	Task
machine	Task
translation	Task
,	O
has	O
only	O
been	O
proposed	O
as	O
recently	O
as	O
this	O
year	O
.	O

We	O
believe	O
the	O
architecture	O
proposed	O
here	O
is	O
a	O
promising	O
step	O
toward	O
better	O
machine	Task
translation	Task
and	O
a	O
better	O
understanding	Task
of	Task
natural	Task
languages	Task
in	O
general	O
.	O

One	O
of	O
challenges	O
left	O
for	O
the	O
future	O
is	O
to	O
better	O
handle	O
unknown	O
,	O
or	O
rare	O
words	O
.	O

This	O
will	O
be	O
required	O
for	O
the	O
model	O
to	O
be	O
more	O
widely	O
used	O
and	O
to	O
match	O
the	O
performance	O
of	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
machine	Method
translation	Method
systems	Method
in	O
all	O
contexts	O
.	O

section	O
:	O
Acknowledgments	O
The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
developers	O
of	O
Theano	O
.	O

We	O
acknowledge	O
the	O
support	O
of	O
the	O
following	O
agencies	O
for	O
research	O
funding	O
and	O
computing	O
support	O
:	O
NSERC	O
,	O
Calcul	O
Québec	O
,	O
Compute	O
Canada	O
,	O
the	O
Canada	O
Research	O
Chairs	O
and	O
CIFAR	O
.	O

Bahdanau	O
thanks	O
the	O
support	O
from	O
Planet	O
Intelligent	O
Systems	O
GmbH.	O
We	O
also	O
thank	O
Felix	O
Hill	O
,	O
Bart	O
van	O
Merriénboer	O
,	O
Jean	O
Pouget	O
-	O
Abadie	O
,	O
Coline	O
Devin	O
and	O
Tae	O
-	O
Ho	O
Kim	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Model	O
Architecture	O
subsection	O
:	O
Architectural	O
Choices	O
The	O
proposed	O
scheme	O
in	O
Section	O
[	O
reference	O
]	O
is	O
a	O
general	O
framework	O
where	O
one	O
can	O
freely	O
define	O
,	O
for	O
instance	O
,	O
the	O
activation	Method
functions	Method
of	O
recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
and	O
the	O
alignment	Method
model	Method
.	O

Here	O
,	O
we	O
describe	O
the	O
choices	O
we	O
made	O
for	O
the	O
experiments	O
in	O
this	O
paper	O
.	O

subsubsection	O
:	O
Recurrent	Method
Neural	Method
Network	Method
For	O
the	O
activation	O
function	O
of	O
an	O
RNN	Method
,	O
we	O
use	O
the	O
gated	Method
hidden	Method
unit	Method
recently	O
proposed	O
by	O
.	O

The	O
gated	Method
hidden	Method
unit	Method
is	O
an	O
alternative	O
to	O
the	O
conventional	O
simple	Method
units	Method
such	O
as	O
an	O
element	Method
-	Method
wise	Method
.	O

This	O
gated	Method
unit	Method
is	O
similar	O
to	O
a	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	Method
LSTM	Method
)	Method
unit	Method
proposed	O
earlier	O
by	O
,	O
sharing	O
with	O
it	O
the	O
ability	O
to	O
better	O
model	O
and	O
learn	O
long	O
-	O
term	O
dependencies	O
.	O

This	O
is	O
made	O
possible	O
by	O
having	O
computation	O
paths	O
in	O
the	O
unfolded	O
RNN	Method
for	O
which	O
the	O
product	O
of	O
derivatives	O
is	O
close	O
to	O
1	O
.	O

These	O
paths	O
allow	O
gradients	O
to	O
flow	O
backward	O
easily	O
without	O
suffering	O
too	O
much	O
from	O
the	O
vanishing	O
effect	O
.	O

It	O
is	O
therefore	O
possible	O
to	O
use	O
LSTM	Method
units	Method
instead	O
of	O
the	O
gated	Method
hidden	Method
unit	Method
described	O
here	O
,	O
as	O
was	O
done	O
in	O
a	O
similar	O
context	O
by	O
.	O

The	O
new	O
state	O
of	O
the	O
RNN	Method
employing	O
gated	Method
hidden	Method
units	Method
is	O
computed	O
by	O
where	O
is	O
an	O
element	Method
-	Method
wise	Method
multiplication	Method
,	O
and	O
is	O
the	O
output	O
of	O
the	O
update	O
gates	O
(	O
see	O
below	O
)	O
.	O

The	O
proposed	O
updated	O
state	O
is	O
computed	O
by	O
where	O
is	O
an	O
-	O
dimensional	O
embedding	O
of	O
a	O
word	O
,	O
and	O
is	O
the	O
output	O
of	O
the	O
reset	O
gates	O
(	O
see	O
below	O
)	O
.	O

When	O
is	O
represented	O
as	O
a	O
-	O
of	O
-	O
vector	O
,	O
is	O
simply	O
a	O
column	O
of	O
an	O
embedding	Method
matrix	Method
.	O

Whenever	O
possible	O
,	O
we	O
omit	O
bias	O
terms	O
to	O
make	O
the	O
equations	O
less	O
cluttered	O
.	O

The	O
update	O
gates	O
allow	O
each	O
hidden	Method
unit	Method
to	O
maintain	O
its	O
previous	O
activation	O
,	O
and	O
the	O
reset	O
gates	O
control	O
how	O
much	O
and	O
what	O
information	O
from	O
the	O
previous	O
state	O
should	O
be	O
reset	O
.	O

We	O
compute	O
them	O
by	O
where	O
is	O
a	O
logistic	Method
sigmoid	Method
function	Method
.	O

At	O
each	O
step	O
of	O
the	O
decoder	Method
,	O
we	O
compute	O
the	O
output	O
probability	O
(	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
as	O
a	O
multi	O
-	O
layered	O
function	O
.	O

We	O
use	O
a	O
single	O
hidden	Method
layer	Method
of	Method
maxout	Method
units	Method
and	O
normalize	O
the	O
output	O
probabilities	O
(	O
one	O
for	O
each	O
word	O
)	O
with	O
a	O
softmax	O
function	O
(	O
see	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
.	O

subsubsection	O
:	O
Alignment	Method
Model	Method
The	O
alignment	Method
model	Method
should	O
be	O
designed	O
considering	O
that	O
the	O
model	O
needs	O
to	O
be	O
evaluated	O
times	O
for	O
each	O
sentence	O
pair	O
of	O
lengths	O
and	O
.	O

In	O
order	O
to	O
reduce	O
computation	O
,	O
we	O
use	O
a	O
single	Method
-	Method
layer	Method
multilayer	Method
perceptron	Method
such	O
that	O
where	O
and	O
are	O
the	O
weight	O
matrices	O
.	O

Since	O
does	O
not	O
depend	O
on	O
,	O
we	O
can	O
pre	O
-	O
compute	O
it	O
in	O
advance	O
to	O
minimize	O
the	O
computational	Metric
cost	Metric
.	O

subsection	O
:	O
Detailed	O
Description	O
of	O
the	O
Model	O
subsubsection	O
:	O
Encoder	Method
In	O
this	O
section	O
,	O
we	O
describe	O
in	O
detail	O
the	O
architecture	O
of	O
the	O
proposed	O
model	O
(	O
RNNsearch	Method
)	O
used	O
in	O
the	O
experiments	O
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
)	O
.	O

From	O
here	O
on	O
,	O
we	O
omit	O
all	O
bias	O
terms	O
in	O
order	O
to	O
increase	O
readability	O
.	O

The	O
model	O
takes	O
a	O
source	O
sentence	O
of	O
1	O
-	O
of	O
-	O
K	O
coded	O
word	O
vectors	O
as	O
input	O
and	O
outputs	O
a	O
translated	O
sentence	O
of	O
1	O
-	O
of	O
-	O
K	O
coded	O
word	O
vectors	O
where	O
and	O
are	O
the	O
vocabulary	O
sizes	O
of	O
source	O
and	O
target	O
languages	O
,	O
respectively	O
.	O

and	O
respectively	O
denote	O
the	O
lengths	O
of	O
source	O
and	O
target	O
sentences	O
.	O

First	O
,	O
the	O
forward	O
states	O
of	O
the	O
bidirectional	Method
recurrent	Method
neural	Method
network	Method
(	Method
BiRNN	Method
)	Method
are	O
computed	O
:	O
where	O
is	O
the	O
word	O
embedding	O
matrix	O
.	O

,	O
are	O
weight	O
matrices	O
.	O

and	O
are	O
the	O
word	O
embedding	O
dimensionality	O
and	O
the	O
number	O
of	O
hidden	O
units	O
,	O
respectively	O
.	O

is	O
as	O
usual	O
a	O
logistic	Method
sigmoid	Method
function	Method
.	O

The	O
backward	O
states	O
are	O
computed	O
similarly	O
.	O

We	O
share	O
the	O
word	O
embedding	O
matrix	O
between	O
the	O
forward	O
and	O
backward	O
RNNs	Method
,	O
unlike	O
the	O
weight	Method
matrices	Method
.	O

We	O
concatenate	O
the	O
forward	O
and	O
backward	O
states	O
to	O
to	O
obtain	O
the	O
annotations	O
,	O
where	O
subsubsection	O
:	O
Decoder	Method
The	O
hidden	O
state	O
of	O
the	O
decoder	O
given	O
the	O
annotations	O
from	O
the	O
encoder	O
is	O
computed	O
by	O
where	O
is	O
the	O
word	Method
embedding	Method
matrix	Method
for	O
the	O
target	O
language	O
.	O

,	O
,	O
and	O
are	O
weights	O
.	O

Again	O
,	O
and	O
are	O
the	O
word	O
embedding	O
dimensionality	O
and	O
the	O
number	O
of	O
hidden	O
units	O
,	O
respectively	O
.	O

The	O
initial	O
hidden	O
state	O
is	O
computed	O
by	O
where	O
.	O

The	O
context	O
vector	O
are	O
recomputed	O
at	O
each	O
step	O
by	O
the	O
alignment	Method
model	Method
:	O
where	O
and	O
is	O
the	O
-	O
th	O
annotation	O
in	O
the	O
source	O
sentence	O
(	O
see	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
)	O
.	O

and	O
are	O
weight	O
matrices	O
.	O

Note	O
that	O
the	O
model	O
becomes	O
RNN	Method
Encoder	O
–	O
Decoder	O
,	O
if	O
we	O
fix	O
to	O
.	O

With	O
the	O
decoder	O
state	O
,	O
the	O
context	O
and	O
the	O
last	O
generated	O
word	O
,	O
we	O
define	O
the	O
probability	O
of	O
a	O
target	O
word	O
as	O
where	O
and	O
is	O
the	O
-	O
th	O
element	O
of	O
a	O
vector	O
which	O
is	O
computed	O
by	O
,	O
,	O
and	O
are	O
weight	O
matrices	O
.	O

This	O
can	O
be	O
understood	O
as	O
having	O
a	O
deep	O
output	O
with	O
a	O
single	O
maxout	Method
hidden	Method
layer	Method
.	O

subsubsection	O
:	O
Model	Method
Size	Method
For	O
all	O
the	O
models	O
used	O
in	O
this	O
paper	O
,	O
the	O
size	O
of	O
a	O
hidden	O
layer	O
is	O
1000	O
,	O
the	O
word	O
embedding	O
dimensionality	O
is	O
620	O
and	O
the	O
size	O
of	O
the	O
maxout	Method
hidden	Method
layer	Method
in	O
the	O
deep	O
output	O
is	O
500	O
.	O

The	O
number	O
of	O
hidden	O
units	O
in	O
the	O
alignment	Method
model	Method
is	O
1000	O
.	O

appendix	O
:	O
Training	O
Procedure	O
subsection	O
:	O
Parameter	Method
Initialization	Method
We	O
initialized	O
the	O
recurrent	O
weight	O
matrices	O
and	O
as	O
random	O
orthogonal	O
matrices	O
.	O

For	O
and	O
,	O
we	O
initialized	O
them	O
by	O
sampling	O
each	O
element	O
from	O
the	O
Gaussian	Method
distribution	Method
of	Method
mean	Method
and	Method
variance	Method
.	O

All	O
the	O
elements	O
of	O
and	O
all	O
the	O
bias	O
vectors	O
were	O
initialized	O
to	O
zero	O
.	O

Any	O
other	O
weight	O
matrix	O
was	O
initialized	O
by	O
sampling	O
from	O
the	O
Gaussian	Method
distribution	Method
of	Method
mean	Method
and	Method
variance	Method
.	O

subsection	O
:	O
Training	O
We	O
used	O
the	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
algorithm	O
.	O

Adadelta	Method
was	O
used	O
to	O
automatically	O
adapt	O
the	O
learning	O
rate	O
of	O
each	O
parameter	O
(	O
and	O
)	O
.	O

We	O
explicitly	O
normalized	O
the	O
-	O
norm	O
of	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
each	O
time	O
to	O
be	O
at	O
most	O
a	O
predefined	O
threshold	O
of	O
,	O
when	O
the	O
norm	O
was	O
larger	O
than	O
the	O
threshold	O
.	O

Each	O
SGD	Method
update	O
direction	O
was	O
computed	O
with	O
a	O
minibatch	O
of	O
80	O
sentences	O
.	O

At	O
each	O
update	O
our	O
implementation	O
requires	O
time	O
proportional	O
to	O
the	O
length	O
of	O
the	O
longest	O
sentence	O
in	O
a	O
minibatch	O
.	O

Hence	O
,	O
to	O
minimize	O
the	O
waste	O
of	O
computation	O
,	O
before	O
every	O
20	O
-	O
th	O
update	O
,	O
we	O
retrieved	O
1600	O
sentence	O
pairs	O
,	O
sorted	O
them	O
according	O
to	O
the	O
lengths	O
and	O
split	O
them	O
into	O
20	O
minibatches	O
.	O

The	O
training	O
data	O
was	O
shuffled	O
once	O
before	O
training	O
and	O
was	O
traversed	O
sequentially	O
in	O
this	O
manner	O
.	O

In	O
Tables	O
[	O
reference	O
]	O
we	O
present	O
the	O
statistics	O
related	O
to	O
training	O
all	O
the	O
models	O
used	O
in	O
the	O
experiments	O
.	O

appendix	O
:	O
Translations	O
of	O
Long	O
Sentences	O
