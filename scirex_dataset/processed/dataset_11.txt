document	O
:	O
Incentivizing	Task
Exploration	Task
In	O
Reinforcement	Task
Learning	Task
With	O
Deep	Method
Predictive	Method
Models	Method
Achieving	O
efficient	Task
and	Task
scalable	Task
exploration	Task
in	Task
complex	Task
domains	Task
poses	O
a	O
major	O
challenge	O
in	O
reinforcement	Task
learning	Task
.	O

While	O
Bayesian	Method
and	Method
PAC	Method
-	Method
MDP	Method
approaches	Method
to	O
the	O
exploration	Task
problem	Task
offer	O
strong	O
formal	O
guarantees	O
,	O
they	O
are	O
often	O
impractical	O
in	O
higher	O
dimensions	O
due	O
to	O
their	O
reliance	O
on	O
enumerating	O
the	O
state	O
-	O
action	O
space	O
.	O

Hence	O
,	O
exploration	Task
in	Task
complex	Task
domains	Task
is	O
often	O
performed	O
with	O
simple	O
epsilon	Method
-	Method
greedy	Method
methods	Method
.	O

In	O
this	O
paper	O
,	O
we	O
consider	O
the	O
challenging	O
Atari	Material
games	Material
domain	Material
,	O
which	O
requires	O
processing	O
raw	O
pixel	O
inputs	O
and	O
delayed	O
rewards	O
.	O

We	O
evaluate	O
several	O
more	O
sophisticated	O
exploration	Method
strategies	Method
,	O
including	O
Thompson	Method
sampling	Method
and	O
Boltzman	Method
exploration	Method
,	O
and	O
propose	O
a	O
new	O
exploration	Method
method	Method
based	O
on	O
assigning	O
exploration	O
bonuses	O
from	O
a	O
concurrently	O
learned	O
model	Method
of	Method
the	Method
system	Method
dynamics	Method
.	O

By	O
parameterizing	O
our	O
learned	O
model	O
with	O
a	O
neural	Method
network	Method
,	O
we	O
are	O
able	O
to	O
develop	O
a	O
scalable	O
and	O
efficient	O
approach	O
to	O
exploration	Task
bonuses	Task
that	O
can	O
be	O
applied	O
to	O
tasks	O
with	O
complex	O
,	O
high	O
-	O
dimensional	O
state	O
spaces	O
.	O

In	O
the	O
Atari	Material
domain	Material
,	O
our	O
method	O
provides	O
the	O
most	O
consistent	O
improvement	O
across	O
a	O
range	O
of	O
games	O
that	O
pose	O
a	O
major	O
challenge	O
for	O
prior	O
methods	O
.	O

In	O
addition	O
to	O
raw	O
game	O
-	O
scores	O
,	O
we	O
also	O
develop	O
an	O
AUC	Metric
-	Metric
100	Metric
metric	Metric
for	O
the	O
Atari	Material
Learning	Material
domain	Material
to	O
evaluate	O
the	O
impact	O
of	O
exploration	Task
on	O
this	O
benchmark	O
.	O

section	O
:	O
Introduction	O
In	O
reinforcement	Task
learning	Task
(	O
RL	Task
)	O
,	O
agents	O
acting	O
in	O
unknown	O
environments	O
face	O
the	O
exploration	O
versus	O
exploitation	O
tradeoff	O
.	O

Without	O
adequate	O
exploration	O
,	O
the	O
agent	O
might	O
fail	O
to	O
discover	O
effective	O
control	Method
strategies	Method
,	O
particularly	O
in	O
complex	O
domains	O
.	O

Both	O
PAC	Method
-	Method
MDP	Method
algorithms	Method
,	O
such	O
as	O
MBIE	Method
-	Method
EB	Method
,	O
and	O
Bayesian	Method
algorithms	Method
such	O
as	O
Bayesian	Method
Exploration	Method
Bonuses	Method
(	O
BEB	Method
)	O
have	O
managed	O
this	O
tradeoff	O
by	O
assigning	O
exploration	O
bonuses	O
to	O
novel	O
states	O
.	O

In	O
these	O
methods	O
,	O
the	O
novelty	O
of	O
a	O
state	O
-	O
action	O
pair	O
is	O
derived	O
from	O
the	O
number	O
of	O
times	O
an	O
agent	O
has	O
visited	O
that	O
pair	O
.	O

While	O
these	O
approaches	O
offer	O
strong	O
formal	O
guarantees	O
,	O
their	O
requirement	O
of	O
an	O
enumerable	Method
representation	Method
of	O
the	O
agent	O
’s	O
environment	O
renders	O
them	O
impractical	O
for	O
large	Task
-	Task
scale	Task
tasks	Task
.	O

As	O
such	O
,	O
exploration	Task
in	Task
large	Task
RL	Task
tasks	Task
is	O
still	O
most	O
often	O
performed	O
using	O
simple	O
heuristics	Method
,	O
such	O
as	O
the	O
epsilon	Method
-	Method
greedy	Method
strategy	Method
,	O
which	O
can	O
be	O
inadequate	O
in	O
more	O
complex	O
settings	O
.	O

In	O
this	O
paper	O
,	O
we	O
evaluate	O
several	O
exploration	Method
strategies	Method
that	O
can	O
be	O
scaled	O
up	O
to	O
complex	O
tasks	O
with	O
high	O
-	O
dimensional	O
inputs	O
.	O

Our	O
results	O
show	O
that	O
Boltzman	Method
exploration	Method
and	O
Thompson	Method
sampling	Method
significantly	O
improve	O
on	O
the	O
naïve	Method
epsilon	Method
-	Method
greedy	Method
strategy	Method
.	O

However	O
,	O
we	O
show	O
that	O
the	O
biggest	O
and	O
most	O
consistent	O
improvement	O
can	O
be	O
achieved	O
by	O
assigning	O
exploration	O
bonuses	O
based	O
on	O
a	O
learned	O
model	O
of	O
the	O
system	Method
dynamics	Method
with	O
learned	Method
representations	Method
.	O

To	O
that	O
end	O
,	O
we	O
describe	O
a	O
method	O
that	O
learns	O
a	O
state	Method
representation	Method
from	O
observations	O
,	O
trains	O
a	O
dynamics	Method
model	Method
using	O
this	O
representation	O
concurrently	O
with	O
the	O
policy	Method
,	O
and	O
uses	O
the	O
misprediction	O
error	O
in	O
this	O
model	O
to	O
asses	O
the	O
novelty	O
of	O
each	O
state	O
.	O

Novel	O
states	O
are	O
expected	O
to	O
disagree	O
more	O
strongly	O
with	O
the	O
model	O
than	O
those	O
states	O
that	O
have	O
been	O
visited	O
frequently	O
in	O
the	O
past	O
,	O
and	O
assigning	O
exploration	O
bonuses	O
based	O
on	O
this	O
disagreement	O
can	O
produce	O
rapid	O
and	O
effective	O
exploration	Task
.	O

Using	O
learned	O
model	Method
dynamics	Method
to	O
assess	O
a	O
state	O
’s	O
novelty	O
presents	O
several	O
challenges	O
.	O

Capturing	O
an	O
adequate	O
representation	O
of	O
the	O
agent	O
’s	O
environment	O
for	O
use	O
in	O
dynamics	Task
predictions	Task
can	O
be	O
accomplished	O
by	O
training	O
a	O
model	O
to	O
predict	O
the	O
next	O
state	O
from	O
the	O
previous	O
ground	O
-	O
truth	O
state	O
-	O
action	O
pair	O
.	O

However	O
,	O
one	O
would	O
not	O
expect	O
pixel	O
intensity	O
values	O
to	O
adequately	O
capture	O
the	O
salient	O
features	O
of	O
a	O
given	O
state	O
-	O
space	O
.	O

To	O
provide	O
a	O
more	O
suitable	O
representation	O
of	O
the	O
system	O
’s	O
state	O
space	O
,	O
we	O
propose	O
a	O
method	O
for	O
encoding	O
the	O
state	O
space	O
into	O
lower	O
dimensional	O
domains	O
.	O

To	O
achieve	O
sufficient	O
generality	O
and	O
scalability	O
,	O
we	O
modeled	O
the	O
system	O
’s	O
dynamics	O
with	O
a	O
deep	Method
neural	Method
network	Method
.	O

This	O
allows	O
for	O
on	O
-	O
the	O
-	O
fly	O
learning	O
of	O
a	O
model	Method
representation	Method
that	O
can	O
easily	O
be	O
trained	O
in	O
parallel	O
to	O
learning	O
a	O
policy	Method
.	O

Our	O
main	O
contribution	O
is	O
a	O
scalable	O
and	O
efficient	O
method	O
for	O
assigning	Task
exploration	Task
bonuses	Task
in	O
large	Task
RL	Task
problems	Task
with	O
complex	O
observations	O
,	O
as	O
well	O
as	O
an	O
extensive	O
empirical	O
evaluation	O
of	O
this	O
approach	O
and	O
other	O
simple	O
alternative	O
strategies	O
,	O
such	O
as	O
Boltzman	Method
exploration	Method
and	O
Thompson	Method
sampling	Method
.	O

Our	O
approach	O
assigns	O
model	Method
-	Method
based	Method
exploration	Method
bonuses	Method
from	O
learned	O
representations	O
and	O
dynamics	O
,	O
using	O
only	O
the	O
observations	O
and	O
actions	O
.	O

It	O
can	O
scale	O
to	O
large	O
problems	O
where	O
Bayesian	Method
approaches	Method
to	O
exploration	Task
become	O
impractical	O
,	O
and	O
we	O
show	O
that	O
it	O
achieves	O
significant	O
improvement	O
in	O
learning	Metric
speed	Metric
on	O
the	O
task	O
of	O
learning	Task
to	O
play	O
Atari	Task
games	Task
from	O
raw	O
images	O
.	O

Our	O
approach	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
a	O
number	O
of	O
games	O
,	O
and	O
achieves	O
particularly	O
large	O
improvements	O
for	O
games	O
on	O
which	O
human	O
players	O
strongly	O
outperform	O
prior	O
methods	O
.	O

Aside	O
from	O
achieving	O
a	O
high	O
final	O
score	O
,	O
our	O
method	O
also	O
achieves	O
substantially	O
faster	O
learning	Task
.	O

To	O
evaluate	O
the	O
speed	O
of	O
the	O
learning	Method
process	Method
,	O
we	O
propose	O
the	O
AUC	Metric
-	Metric
100	Metric
benchmark	Metric
to	O
evaluate	O
learning	Task
progress	Task
on	O
the	O
Atari	Material
domain	Material
.	O

section	O
:	O
Preliminaries	O
We	O
consider	O
an	O
infinite	Task
-	Task
horizon	Task
discounted	Task
Markov	Task
decision	Task
process	Task
(	Task
MDP	Task
)	Task
,	O
defined	O
by	O
the	O
tuple	O
,	O
where	O
is	O
a	O
finite	O
set	O
of	O
states	O
,	O
a	O
finite	O
set	O
of	O
actions	O
,	O
the	O
transition	O
probability	O
distribution	O
,	O
the	O
reward	O
function	O
,	O
an	O
initial	O
state	O
distribution	O
,	O
and	O
the	O
discount	O
factor	O
.	O

We	O
are	O
interested	O
in	O
finding	O
a	O
policy	O
that	O
maximizes	O
the	O
expected	O
reward	O
over	O
all	O
time	O
.	O

This	O
maximization	O
can	O
be	O
accomplished	O
using	O
a	O
variety	O
of	O
reinforcement	Method
learning	Method
algorithms	Method
.	O

In	O
this	O
work	O
,	O
we	O
are	O
concerned	O
with	O
online	Task
reinforcement	Task
learning	Task
wherein	O
the	O
algorithm	O
receives	O
a	O
tuple	O
at	O
each	O
step	O
.	O

Here	O
,	O
is	O
the	O
previous	O
state	O
,	O
is	O
the	O
previous	O
action	O
,	O
is	O
the	O
new	O
state	O
,	O
and	O
is	O
the	O
reward	O
collected	O
as	O
a	O
result	O
of	O
this	O
transition	O
.	O

The	O
reinforcement	Method
learning	Method
algorithm	Method
must	O
use	O
this	O
tuple	O
to	O
update	O
its	O
policy	O
and	O
maximize	O
long	O
-	O
term	O
reward	O
and	O
then	O
choose	O
the	O
new	O
action	O
.	O

It	O
is	O
often	O
insufficient	O
to	O
simply	O
choose	O
the	O
best	O
action	O
based	O
on	O
previous	O
experience	O
,	O
since	O
this	O
strategy	O
can	O
quickly	O
fall	O
into	O
a	O
local	O
optimum	O
.	O

Instead	O
,	O
the	O
learning	Method
algorithm	Method
must	O
perform	O
exploration	Task
.	O

Prior	O
work	O
has	O
suggested	O
methods	O
that	O
address	O
the	O
exploration	Task
problem	Task
by	O
acting	O
with	O
“	O
optimism	O
under	O
uncertainty	O
.	O

”	O
If	O
one	O
assumes	O
that	O
the	O
reinforcement	Method
learning	Method
algorithm	Method
will	O
tend	O
to	O
choose	O
the	O
best	O
action	O
,	O
it	O
can	O
be	O
encouraged	O
to	O
visit	O
state	O
-	O
action	O
pairs	O
that	O
it	O
has	O
not	O
frequently	O
seen	O
by	O
augmenting	O
the	O
reward	O
function	O
to	O
deliver	O
a	O
bonus	O
for	O
visiting	O
novel	O
states	O
.	O

This	O
is	O
accomplished	O
with	O
the	O
augmented	O
reward	O
function	O
where	O
is	O
a	O
novelty	O
function	O
designed	O
to	O
capture	O
the	O
novelty	O
of	O
a	O
given	O
state	O
-	O
action	O
pair	O
.	O

Prior	O
work	O
has	O
suggested	O
a	O
variety	O
of	O
different	O
novelty	O
functions	O
e.g.	O
,	O
based	O
on	O
state	O
visitation	O
frequency	O
.	O

While	O
such	O
methods	O
offer	O
a	O
number	O
of	O
appealing	O
guarantees	O
,	O
such	O
as	O
near	Task
-	Task
Bayesian	Task
exploration	Task
in	O
polynomial	O
time	O
,	O
they	O
require	O
a	O
concise	O
,	O
often	O
discrete	Method
representation	Method
of	O
the	O
agent	O
’s	O
state	O
-	O
action	O
space	O
to	O
measure	O
state	O
visitation	O
frequencies	O
.	O

In	O
our	O
approach	O
,	O
we	O
will	O
employ	O
function	Method
approximation	Method
and	O
representation	Method
learning	Method
to	O
devise	O
an	O
alternative	O
to	O
these	O
requirements	O
.	O

section	O
:	O
Model	Method
Learning	Method
For	O
Exploration	Task
Bonuses	Task
We	O
would	O
like	O
to	O
encourage	O
agent	Task
exploration	Task
by	O
giving	O
the	O
agent	O
exploration	O
bonuses	O
for	O
visiting	O
novel	O
states	O
.	O

Identifying	O
states	O
as	O
novel	O
requires	O
we	O
supply	O
some	O
representation	O
of	O
the	O
agent	O
’s	O
state	O
space	O
,	O
as	O
well	O
as	O
a	O
mechanism	O
to	O
use	O
this	O
representation	O
to	O
assess	O
novelty	O
.	O

Unsupervised	Method
learning	Method
methods	Method
offer	O
one	O
promising	O
avenue	O
for	O
acquiring	O
a	O
concise	Task
representation	Task
of	Task
the	Task
state	Task
with	O
a	O
good	O
similarity	Metric
metric	Metric
.	O

This	O
can	O
be	O
accomplished	O
using	O
dimensionality	Method
reduction	Method
,	O
clustering	Method
,	O
or	O
graph	Method
-	Method
based	Method
techniques	Method
.	O

In	O
our	O
work	O
,	O
we	O
draw	O
on	O
recent	O
developments	O
in	O
representation	Task
learning	Task
with	O
neural	Method
networks	Method
,	O
as	O
discussed	O
in	O
the	O
following	O
section	O
.	O

However	O
,	O
even	O
with	O
a	O
good	O
learned	O
state	Method
representation	Method
,	O
maintaining	O
a	O
table	O
of	O
visitation	O
frequencies	O
becomes	O
impractical	O
for	O
complex	O
tasks	O
.	O

Instead	O
,	O
we	O
learn	O
a	O
model	O
of	O
the	O
task	O
dynamics	O
that	O
can	O
be	O
used	O
to	O
assess	O
the	O
novelty	O
of	O
a	O
new	O
state	O
.	O

Formally	O
,	O
let	O
denote	O
the	O
encoding	O
of	O
the	O
state	O
,	O
and	O
let	O
be	O
a	O
dynamics	Method
predictor	Method
parameterized	O
by	O
.	O

takes	O
an	O
encoded	O
version	O
of	O
a	O
state	O
at	O
time	O
and	O
the	O
agent	O
’s	O
action	O
at	O
time	O
and	O
attempts	O
to	O
predict	O
an	O
encoded	O
version	O
of	O
the	O
agent	O
’s	O
state	O
at	O
time	O
.	O

The	O
parameterization	O
of	O
is	O
discussed	O
further	O
in	O
the	O
next	O
section	O
.	O

For	O
each	O
state	O
transition	O
,	O
we	O
can	O
attempt	O
to	O
predict	O
from	O
using	O
our	O
predictive	Method
model	Method
.	O

This	O
prediction	O
will	O
have	O
some	O
error	O
Let	O
,	O
the	O
normalized	Metric
prediction	Metric
error	Metric
at	O
time	O
,	O
be	O
given	O
by	O
.	O

We	O
can	O
assign	O
a	O
novelty	O
function	O
to	O
via	O
where	O
is	O
a	O
decay	O
constant	O
.	O

We	O
can	O
now	O
realize	O
our	O
augmented	Method
reward	Method
function	Method
as	O
This	O
approach	O
is	O
motivated	O
by	O
the	O
idea	O
that	O
,	O
as	O
our	O
ability	O
to	O
model	O
the	O
dynamics	O
of	O
a	O
particular	O
state	O
-	O
action	O
pair	O
improves	O
,	O
we	O
have	O
come	O
to	O
understand	O
the	O
state	O
better	O
and	O
hence	O
its	O
novelty	O
is	O
lower	O
.	O

When	O
we	O
do	O
n’t	O
understand	O
the	O
state	O
-	O
action	O
pair	O
well	O
enough	O
to	O
make	O
accurate	O
predictions	O
,	O
we	O
assume	O
that	O
more	O
knowledge	O
about	O
that	O
particular	O
area	O
of	O
the	O
model	O
dynamics	O
is	O
needed	O
and	O
hence	O
a	O
higher	O
novelty	Metric
measure	Metric
is	O
assigned	O
.	O

Using	O
learned	O
model	Method
dynamics	Method
to	O
assign	O
novelty	O
functions	O
allows	O
us	O
to	O
address	O
the	O
exploration	Task
versus	Task
exploitation	Task
problem	Task
in	O
a	O
non	O
-	O
greedy	O
way	O
.	O

With	O
an	O
appropriate	O
representation	O
,	O
even	O
when	O
we	O
encounter	O
a	O
new	O
state	O
-	O
action	O
pair	O
,	O
we	O
expect	O
to	O
be	O
accurate	O
so	O
long	O
as	O
enough	O
similar	O
state	O
-	O
action	O
pairs	O
have	O
been	O
encountered	O
.	O

[	O
tb	O
]	O
Reinforcement	Method
learning	Method
with	O
model	Method
prediction	Method
exploration	Method
bonuses	O
{	O
algorithmic}	O
[	O
1	O
]	O
=	O
maxe1	O
,	O
EpochLength	O
,	O
β	O
,	O
C	O
t	O
in	O
T	O
(	O
st	O
,	O
at	O
,	O
s	O
+	O
t1	O
,	O
⁢R	O
(	O
st	O
,	O
at	O
)	O
)	O
the	O
observations	O
to	O
obtain	O
⁢σ	O
(	O
st	O
)	O
and	O
⁢σ	O
(	O
s	O
+	O
t1	O
)	O
=	O
⁢e	O
(	O
st	O
,	O
at	O
)	O
∥	O
-	O
⁢σ	O
(	O
s	O
+	O
t1	O
)	O
⁢Mϕ	O
(	O
⁢σ	O
(	O
st	O
),	O
at	O
)	O
∥22	O
and	O
=	O
⁢¯e	O
(	O
st	O
,	O
at	O
)	O
⁢e	O
(	O
st	O
,	O
at	O
)	O
maxe	O
.	O

=	O
⁢R⁢Bonus	O
(	O
st	O
,	O
at	O
)+	O
⁢R	O
(	O
s	O
,	O
a	O
)	O
⁢β	O
(	O
⁢¯et	O
(	O
st	O
,	O
at	O
)	O
∗tC	O
)	O
>	O
⁢e	O
(	O
st	O
,	O
at	O
)	O
maxe	O
(	O
st	O
,	O
at	O
,	O
R⁢bonus	O
)	O
in	O
a	O
memory	O
bank	O
Ω.	O
Ω	O
to	O
the	O
reinforcement	Method
learning	Method
algorithm	Method
to	O
update	O
π	O
.	O

tmodEpochLength==0	O
Ω	O
to	O
update	O
M.	O
update	O
σ	O
.	O

optimized	Method
policy	Method
π	O
Our	O
model	Method
-	Method
based	Method
exploration	Method
bonuses	Method
can	O
be	O
incorporated	O
into	O
any	O
online	Method
reinforcement	Method
learning	Method
algorithm	Method
that	O
updates	O
the	O
policy	O
based	O
on	O
state	O
,	O
action	O
,	O
reward	O
tuples	O
of	O
the	O
form	O
,	O
such	O
as	O
Q	Method
-	Method
learning	Method
or	O
actor	Method
-	Method
critic	Method
algorithms	Method
.	O

Our	O
method	O
is	O
summarized	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

At	O
each	O
step	O
,	O
we	O
receive	O
a	O
tuple	O
and	O
compute	O
the	O
Euclidean	O
distance	O
between	O
the	O
encoded	O
state	O
to	O
the	O
prediction	O
made	O
by	O
our	O
model	O
.	O

This	O
is	O
used	O
to	O
compute	O
the	O
exploration	O
-	O
augmented	O
reward	O
using	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
tuples	O
are	O
stored	O
in	O
a	O
memory	O
bank	O
at	O
the	O
end	O
of	O
every	O
step	O
.	O

Every	O
step	O
,	O
the	O
policy	O
is	O
updated	O
.	O

Once	O
per	O
epoch	O
,	O
corresponding	O
to	O
50000	O
observations	O
in	O
our	O
implementation	O
,	O
the	O
dynamics	Method
model	Method
is	O
updated	O
to	O
improve	O
its	O
accuracy	Metric
.	O

If	O
desired	O
,	O
the	O
representation	Method
encoder	Method
can	O
also	O
be	O
updated	O
at	O
this	O
time	O
.	O

We	O
found	O
that	O
retraining	O
once	O
every	O
5	O
epochs	O
to	O
be	O
sufficient	O
.	O

This	O
approach	O
is	O
modular	O
and	O
compatible	O
with	O
any	O
representation	Method
of	Method
and	Method
,	O
as	O
well	O
as	O
any	O
reinforcement	Method
learning	Method
method	Method
that	O
updates	O
its	O
policy	O
based	O
on	O
a	O
continuous	O
stream	O
of	O
observation	O
,	O
action	O
,	O
reward	O
tuples	O
.	O

Incorporating	O
exploration	O
bonuses	O
does	O
make	O
the	O
reinforcement	Task
learning	Task
task	Task
nonstationary	O
,	O
though	O
we	O
did	O
not	O
find	O
this	O
to	O
be	O
a	O
major	O
issue	O
in	O
practice	O
,	O
as	O
shown	O
in	O
our	O
experimental	O
evaluation	O
.	O

In	O
the	O
following	O
section	O
,	O
we	O
discuss	O
the	O
particular	O
choice	O
for	O
and	O
that	O
we	O
use	O
for	O
learning	Method
policies	Method
for	O
playing	O
Atari	Task
games	Task
from	O
raw	O
images	O
.	O

section	O
:	O
Deep	Method
Learning	Method
Architectures	Method
Though	O
the	O
dynamics	Method
model	Method
and	O
the	O
encoder	Method
from	O
the	O
previous	O
section	O
can	O
be	O
parametrized	O
by	O
any	O
appropriate	O
method	O
,	O
we	O
found	O
that	O
using	O
deep	Method
neural	Method
networks	Method
for	O
both	O
achieved	O
good	O
empirical	O
results	O
on	O
the	O
Atari	Material
games	Material
benchmark	Material
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
the	O
particular	O
networks	O
used	O
in	O
our	O
implementation	O
.	O

subsection	O
:	O
Autoencoders	Method
The	O
most	O
direct	O
way	O
of	O
learning	O
a	O
dynamics	Method
model	Method
is	O
to	O
directly	O
predict	O
the	O
state	O
at	O
the	O
next	O
time	O
step	O
,	O
which	O
in	O
the	O
Atari	Material
games	Material
benchmark	Material
corresponds	O
to	O
the	O
next	O
frame	O
’s	O
pixel	O
intensity	O
values	O
.	O

However	O
,	O
directly	O
predicting	O
these	O
pixel	O
intensity	O
values	O
is	O
unsatisfactory	O
,	O
since	O
we	O
do	O
not	O
expect	O
pixel	O
intensity	O
to	O
capture	O
the	O
salient	O
features	O
of	O
the	O
environment	O
in	O
a	O
robust	O
way	O
.	O

In	O
our	O
experiments	O
,	O
a	O
dynamics	Method
model	Method
trained	O
to	O
predict	O
raw	O
frames	O
exhibited	O
extremely	O
poor	O
behavior	O
,	O
assigning	O
exploration	O
bonuses	O
in	O
near	O
equality	O
at	O
most	O
time	O
steps	O
,	O
as	O
discussed	O
in	O
our	O
experimental	O
results	O
section	O
.	O

To	O
overcome	O
these	O
difficulties	O
,	O
we	O
seek	O
a	O
function	O
which	O
encodes	O
a	O
lower	Method
dimensional	Method
representation	Method
of	Method
the	Method
state	Method
.	O

For	O
the	O
task	O
of	O
representing	Task
Atari	Task
frames	Task
,	O
we	O
found	O
that	O
an	O
autoencoder	Method
could	O
be	O
used	O
to	O
successfully	O
obtain	O
an	O
encoding	Method
function	Method
and	O
achieve	O
dimensionality	Task
reduction	Task
and	O
feature	Task
extraction	Task
.	O

Our	O
autoencoder	Method
has	O
8	O
hidden	O
layers	O
,	O
followed	O
by	O
a	O
Euclidean	Method
loss	Method
layer	Method
,	O
which	O
computes	O
the	O
distance	O
between	O
the	O
output	O
features	O
and	O
the	O
original	O
input	O
image	O
.	O

The	O
hidden	O
layers	O
are	O
reduced	O
in	O
dimension	O
until	O
maximal	O
compression	O
occurs	O
with	O
128	O
units	O
.	O

After	O
this	O
,	O
the	O
activations	O
are	O
decoded	O
by	O
passing	O
through	O
hidden	Method
layers	Method
with	O
increasingly	O
large	O
size	O
.	O

We	O
train	O
the	O
network	O
on	O
a	O
set	O
of	O
250	O
,	O
000	O
images	O
and	O
test	O
on	O
a	O
further	O
set	O
of	O
25	O
,	O
000	O
images	O
.	O

We	O
compared	O
two	O
separate	O
methodologies	O
for	O
capturing	O
these	O
images	O
.	O

Static	O
AE	O
:	O
A	O
random	Method
agent	Method
plays	O
for	O
enough	O
time	O
to	O
collect	O
the	O
required	O
images	O
.	O

The	O
auto	Method
-	Method
encoder	Method
is	O
trained	O
offline	O
before	O
the	O
policy	Method
learning	Method
algorithm	Method
begins	O
.	O

Dynamic	Task
AE	Task
:	O
Initialize	O
with	O
an	O
epsilon	Method
-	Method
greedy	Method
strategy	Method
and	O
collect	O
images	O
and	O
actions	O
while	O
the	O
agent	O
acts	O
under	O
the	O
policy	Method
learning	Method
algorithm	Method
.	O

After	O
5	O
epochs	O
,	O
train	O
the	O
auto	Method
encoder	Method
from	O
this	O
data	O
.	O

Continue	O
to	O
collect	O
data	O
and	O
periodically	O
retrain	O
the	O
auto	Method
encoder	Method
in	O
parallel	O
with	O
the	O
policy	Method
training	Method
algorithm	Method
.	O

We	O
found	O
that	O
the	O
reconstructed	O
input	O
achieves	O
a	O
small	O
but	O
non	O
-	O
trivial	O
residual	O
on	O
the	O
test	O
set	O
regardless	O
of	O
which	O
auto	Method
encoder	Method
training	Method
technique	Method
is	O
utilized	O
,	O
suggesting	O
that	O
in	O
both	O
cases	O
it	O
learns	O
underlying	O
features	O
of	O
the	O
state	O
space	O
while	O
avoiding	O
overfitting	O
.	O

To	O
obtain	O
a	O
lower	Method
dimensional	Method
representation	Method
of	O
the	O
agent	O
’s	O
state	O
space	O
,	O
a	O
snapshot	O
of	O
the	O
network	O
’s	O
first	O
six	O
layers	O
is	O
saved	O
.	O

The	O
sixth	O
layer	O
’s	O
output	O
(	O
circled	O
in	O
figure	O
one	O
)	O
is	O
then	O
utilized	O
as	O
an	O
encoding	O
for	O
the	O
original	O
state	O
space	O
.	O

That	O
is	O
,	O
we	O
construct	O
an	O
encoding	O
by	O
running	O
through	O
the	O
first	O
six	O
hidden	O
layers	O
of	O
our	O
autoencoder	Method
and	O
then	O
taking	O
the	O
sixth	O
layers	O
output	O
to	O
be	O
.	O

In	O
practice	O
,	O
we	O
found	O
that	O
using	O
the	O
sixth	O
layer	O
’s	O
output	O
(	O
rather	O
than	O
the	O
bottleneck	O
at	O
the	O
fifth	O
layer	O
)	O
obtained	O
the	O
best	O
model	O
learning	O
results	O
.	O

See	O
the	O
appendix	O
for	O
further	O
discussion	O
on	O
this	O
result	O
.	O

subsection	O
:	O
Model	Method
Learning	Method
Architecture	Method
Equipped	O
with	O
an	O
encoding	O
,	O
we	O
can	O
now	O
consider	O
the	O
task	O
of	O
predicting	Task
model	Task
dynamics	Task
.	O

For	O
this	O
task	O
,	O
a	O
much	O
simpler	O
two	Method
layer	Method
neural	Method
network	Method
suffices	O
.	O

takes	O
as	O
input	O
the	O
encoded	O
version	O
of	O
a	O
state	O
at	O
time	O
along	O
with	O
the	O
agent	O
’s	O
action	O
and	O
seeks	O
to	O
predict	O
the	O
encoded	O
next	O
frame	O
.	O

Loss	Method
is	O
computed	O
via	O
a	O
Euclidean	Method
loss	Method
layer	Method
regressing	Method
on	O
the	O
ground	O
truth	O
.	O

We	O
find	O
that	O
this	O
model	O
initially	O
learns	O
a	O
representation	O
close	O
to	O
the	O
identity	O
function	O
and	O
consequently	O
the	O
loss	O
residual	O
is	O
similar	O
for	O
most	O
state	O
-	O
action	O
pairs	O
.	O

However	O
,	O
after	O
approximately	O
5	O
epochs	O
,	O
it	O
begins	O
to	O
learn	O
more	O
complex	O
dynamics	O
and	O
consequently	O
better	O
identify	O
novel	O
states	O
.	O

We	O
evaluate	O
the	O
quality	O
of	O
the	O
learned	O
model	O
in	O
the	O
appendix	O
.	O

section	O
:	O
Related	O
Work	O
Exploration	Task
is	O
an	O
intensely	O
studied	O
area	O
of	O
reinforcement	Task
learning	Task
.	O

Many	O
of	O
the	O
pioneering	O
algorithms	O
in	O
this	O
area	O
,	O
such	O
as	O
and	O
,	O
achieve	O
efficient	O
exploration	Task
that	O
scales	O
polynomially	O
with	O
the	O
number	O
of	O
parameters	O
in	O
the	O
agent	O
’s	O
state	O
space	O
(	O
see	O
also	O
)	O
.	O

However	O
,	O
as	O
the	O
size	O
of	O
state	O
spaces	O
increases	O
,	O
these	O
methods	O
quickly	O
become	O
intractable	O
.	O

A	O
number	O
of	O
prior	O
methods	O
also	O
examine	O
various	O
techniques	O
for	O
using	O
models	Method
and	O
prediction	Method
to	O
incentivize	Task
exploration	Task
.	O

However	O
,	O
such	O
methods	O
typically	O
operate	O
directly	O
on	O
the	O
transition	O
matrix	O
of	O
a	O
discrete	Method
MDP	Method
,	O
and	O
do	O
not	O
provide	O
for	O
a	O
straightforward	O
extension	O
to	O
very	O
large	Task
or	Task
continuous	Task
spaces	Task
,	O
where	O
function	Method
approximation	Method
is	O
required	O
.	O

A	O
number	O
of	O
prior	O
methods	O
have	O
also	O
been	O
proposed	O
to	O
incorporate	O
domain	O
-	O
specific	O
factors	O
to	O
improve	O
exploration	Task
.	O

Doshi	O
-	O
Velez	O
et	O
al	O
.	O

proposed	O
incorporating	O
priors	O
into	O
policy	Task
optimization	Task
,	O
while	O
Lang	O
et	O
al	O
.	O

developed	O
a	O
method	O
specific	O
to	O
relational	Task
domains	Task
.	O

Finally	O
,	O
Schmidhuber	O
et	O
al	O
.	O

have	O
developed	O
a	O
curiosity	Method
driven	Method
approach	Method
to	O
exploration	Task
which	O
uses	O
model	Method
predictors	Method
to	O
aid	O
in	O
control	Task
.	O

Several	O
exploration	Method
techniques	Method
have	O
been	O
proposed	O
that	O
can	O
extend	O
more	O
readily	O
to	O
large	O
state	O
spaces	O
.	O

Among	O
these	O
,	O
methods	O
such	O
as	O
C	Method
-	Method
PACE	Method
and	O
metric	Method
-	Method
require	O
a	O
good	O
metric	O
on	O
the	O
state	O
space	O
that	O
satisfies	O
the	O
assumptions	O
of	O
the	O
algorithm	O
.	O

The	O
corresponding	O
representation	Task
learning	Task
issue	Task
has	O
some	O
parallels	O
to	O
the	O
representation	Task
problem	Task
that	O
we	O
address	O
by	O
using	O
an	O
autoecoder	Method
,	O
but	O
it	O
is	O
unclear	O
how	O
the	O
appropriate	O
metric	O
for	O
the	O
prior	O
methods	O
can	O
be	O
acquired	O
automatically	O
on	O
tasks	O
with	O
raw	O
sensory	O
input	O
,	O
such	O
as	O
the	O
Atari	Task
games	Task
in	O
our	O
experimental	O
evaluation	O
.	O

Methods	O
based	O
on	O
Monte	Method
-	Method
Carlo	Method
tree	Method
search	Method
can	O
also	O
scale	O
gracefully	O
to	O
complex	O
domains	O
,	O
and	O
indeed	O
previous	O
work	O
has	O
applied	O
such	O
techniques	O
to	O
the	O
task	O
of	O
playing	O
Atari	Task
games	Task
from	O
screen	O
images	O
.	O

However	O
,	O
this	O
approach	O
is	O
computationally	O
very	O
intensive	O
,	O
and	O
requires	O
access	O
to	O
a	O
generative	Method
model	Method
of	O
the	O
system	O
in	O
order	O
to	O
perform	O
the	O
tree	Task
search	Task
,	O
which	O
is	O
not	O
always	O
available	O
in	O
online	Method
reinforcement	Method
learning	Method
.	O

On	O
the	O
other	O
hand	O
,	O
our	O
method	O
readily	O
integrates	O
into	O
any	O
online	Method
reinforcement	Method
learning	Method
algorithm	Method
.	O

Finally	O
,	O
several	O
recent	O
papers	O
have	O
focused	O
on	O
driving	O
the	O
Q	O
value	O
higher	O
.	O

In	O
,	O
the	O
authors	O
use	O
network	Method
dropout	Method
to	O
perform	O
Thompson	Method
sampling	Method
.	O

In	O
Boltzman	Task
exploration	Task
,	O
a	O
positive	O
probability	O
is	O
assigned	O
to	O
any	O
possible	O
action	O
according	O
to	O
its	O
expected	O
utility	O
and	O
according	O
to	O
a	O
temperature	O
parameter	O
.	O

Both	O
of	O
these	O
methods	O
focus	O
on	O
controlling	O
Q	O
values	O
rather	O
than	O
model	Method
-	Method
based	Method
exploration	Method
.	O

A	O
comparison	O
to	O
both	O
is	O
provided	O
in	O
the	O
next	O
section	O
.	O

section	O
:	O
Experimental	O
Results	O
We	O
evaluate	O
our	O
approach	O
on	O
14	O
games	O
from	O
the	O
Arcade	Task
Learning	Task
Environment	Task
.	O

The	O
task	O
consists	O
of	O
choosing	O
actions	O
in	O
an	O
Atari	Method
emulator	Method
based	O
on	O
raw	O
images	O
of	O
the	O
screen	O
.	O

Previous	O
work	O
has	O
tackled	O
this	O
task	O
using	O
Q	Method
-	Method
learning	Method
with	O
epsilon	Method
-	Method
greedy	Method
exploration	Method
,	O
as	O
well	O
as	O
Monte	Method
Carlo	Method
tree	Method
search	Method
and	O
policy	Method
gradient	Method
methods	Method
.	O

We	O
use	O
Deep	Method
Q	Method
Networks	Method
(	O
DQN	Method
)	Method
as	O
the	O
reinforcement	Method
learning	Method
algorithm	Method
within	O
our	O
method	O
,	O
and	O
compare	O
its	O
performance	O
to	O
the	O
same	O
DQN	Method
method	Method
using	O
only	O
epsilon	Method
-	Method
greedy	Method
exploration	Method
,	O
Boltzman	Method
exploration	Method
,	O
and	O
a	O
Thompson	Method
sampling	Method
approach	Method
.	O

The	O
results	O
for	O
14	O
games	O
in	O
the	O
Arcade	Task
Learning	Task
Environment	Task
are	O
presented	O
in	O
Table	O
1	O
.	O

We	O
chose	O
those	O
games	O
that	O
were	O
particularly	O
challenging	O
for	O
prior	O
methods	O
and	O
ones	O
where	O
human	O
experts	O
outperform	O
prior	Method
learning	Method
methods	Method
.	O

We	O
evaluated	O
two	O
versions	O
of	O
our	O
approach	O
;	O
using	O
either	O
an	O
autoencoder	Method
trained	O
in	O
advance	O
by	O
running	O
epsilon	Method
-	Method
greedy	Method
Q	Method
-	Method
learning	Method
to	O
collect	O
data	O
(	O
denoted	O
as	O
“	O
Static	O
AE	O
”	O
)	O
,	O
or	O
using	O
an	O
autoencoder	Method
trained	O
concurrently	O
with	O
the	O
model	O
and	O
policy	Method
on	O
the	O
same	O
image	O
data	O
(	O
denoted	O
as	O
“	O
Dynamic	O
AE	O
”	O
)	O
.	O

Table	O
1	O
also	O
shows	O
results	O
from	O
the	O
DQN	Method
implementation	Method
reported	O
in	O
previous	O
work	O
,	O
along	O
with	O
human	O
expert	O
performance	O
on	O
each	O
game	O
.	O

Note	O
that	O
our	O
DQN	Method
implementation	Method
did	O
not	O
attain	O
the	O
same	O
score	O
on	O
all	O
of	O
the	O
games	O
as	O
prior	O
work	O
due	O
to	O
a	O
shorter	O
running	Metric
time	Metric
.	O

Since	O
we	O
are	O
primarily	O
concerned	O
with	O
the	O
rate	O
of	O
learning	Task
and	O
not	O
the	O
final	O
results	O
,	O
we	O
do	O
not	O
consider	O
this	O
a	O
deficiency	O
.	O

To	O
directly	O
evaluate	O
the	O
benefit	O
of	O
including	O
exploration	O
bonuses	O
,	O
we	O
compare	O
the	O
performance	O
of	O
our	O
approach	O
primarily	O
to	O
our	O
own	O
DQN	Method
implementation	Method
,	O
with	O
the	O
prior	O
scores	O
provided	O
for	O
reference	O
.	O

In	O
addition	O
to	O
raw	O
-	O
game	O
scores	O
,	O
and	O
learning	O
curves	O
,	O
we	O
also	O
analyze	O
our	O
results	O
on	O
a	O
new	O
benchmark	O
we	O
have	O
named	O
Area	Metric
Under	Metric
Curve	Metric
100	Metric
(	O
AUC	Metric
-	Metric
100	Metric
)	O
.	O

For	O
each	O
game	O
,	O
this	O
benchmark	O
computes	O
the	O
area	O
under	O
the	O
game	O
-	O
score	O
learning	O
curve	O
(	O
using	O
the	O
trapezoid	Method
rule	Method
to	O
approximate	O
the	O
integral	O
)	O
.	O

This	O
area	O
is	O
then	O
normalized	O
by	O
100	O
times	O
the	O
score	O
maximum	O
game	O
score	O
achieved	O
in	O
,	O
which	O
represents	O
100	O
epochs	O
of	O
play	O
at	O
the	O
best	O
-	O
known	O
levels	O
.	O

This	O
metric	O
more	O
effectively	O
captures	O
improvements	O
to	O
the	O
game	O
’s	O
learning	Metric
rate	Metric
and	O
does	O
not	O
require	O
running	O
the	O
games	O
for	O
1000	O
epochs	O
as	O
in	O
.	O

For	O
this	O
reason	O
,	O
we	O
suggest	O
it	O
as	O
an	O
alternative	O
metric	O
to	O
raw	O
game	O
-	O
score	O
.	O

paragraph	O
:	O
Bowling	O
The	O
policy	O
without	O
exploration	O
tended	O
to	O
fixate	O
on	O
a	O
set	O
pattern	O
of	O
nocking	O
down	O
six	O
pins	O
per	O
frame	O
.	O

When	O
bonuses	O
were	O
added	O
,	O
the	O
dynamics	Method
learner	Method
quickly	O
became	O
adept	O
at	O
predicting	O
this	O
outcome	O
and	O
was	O
thus	O
encouraged	O
to	O
explore	O
other	O
release	O
points	O
.	O

paragraph	O
:	O
Frostbite	O
This	O
game	O
’s	O
dynamics	O
changed	O
substantially	O
via	O
the	O
addition	O
of	O
extra	O
platforms	O
as	O
the	O
player	O
progressed	O
.	O

As	O
the	O
dynamics	O
of	O
these	O
more	O
complex	O
systems	O
was	O
not	O
well	O
understood	O
,	O
the	O
system	O
was	O
encouraged	O
to	O
visit	O
them	O
often	O
(	O
which	O
required	O
making	O
further	O
progress	O
in	O
the	O
game	O
)	O
.	O

paragraph	O
:	O
Seaquest	O
A	O
submarine	O
must	O
surface	O
for	O
air	O
between	O
bouts	O
of	O
fighting	O
sharks	O
.	O

However	O
,	O
if	O
the	O
player	O
resurfaces	O
too	O
soon	O
they	O
will	O
suffer	O
a	O
penalty	O
with	O
effects	O
on	O
the	O
game	O
’s	O
dynamics	O
.	O

Since	O
these	O
effects	O
are	O
poorly	O
understood	O
by	O
the	O
model	Method
learning	Method
algorithm	Method
,	O
resurfacing	Method
receives	O
a	O
high	O
exploration	O
bonus	O
and	O
hence	O
the	O
agent	O
eventually	O
learns	O
to	O
successfully	O
resurface	O
at	O
the	O
correct	O
time	O
.	O

paragraph	O
:	O
bert	O
Exploration	O
bonuses	O
resulted	O
in	O
a	O
lower	O
score	O
.	O

In	O
bert	O
,	O
the	O
background	O
changes	O
color	O
after	O
level	O
one	O
.	O

The	O
dynamics	Method
predictor	Method
is	O
unable	O
to	O
quickly	O
adapt	O
to	O
such	O
a	O
dramatic	O
change	O
in	O
the	O
environment	O
and	O
consequently	O
,	O
exploration	O
bonuses	O
are	O
assigned	O
in	O
near	O
equality	O
to	O
almost	O
every	O
state	O
that	O
is	O
visited	O
.	O

This	O
negatively	O
impacts	O
the	O
final	O
policy	O
.	O

Learning	O
curves	O
for	O
each	O
of	O
the	O
games	O
are	O
shown	O
in	O
Figure	O
(	O
3	O
)	O
.	O

Note	O
that	O
both	O
of	O
the	O
exploration	Method
bonus	Method
algorithms	Method
learn	O
significantly	O
faster	O
than	O
epsilon	Method
-	Method
greedy	Method
Q	Method
-	Method
learning	Method
,	O
and	O
often	O
continue	O
learning	O
even	O
after	O
the	O
epsilon	Method
-	Method
greedy	Method
strategy	Method
converges	O
.	O

All	O
games	O
had	O
the	O
inputs	O
normalized	O
according	O
to	O
and	O
were	O
run	O
for	O
100	O
epochs	O
(	O
where	O
one	O
epoch	O
is	O
50	O
,	O
000	O
time	O
steps	O
)	O
.	O

Between	O
each	O
epoch	O
,	O
the	O
policy	O
was	O
updated	O
and	O
then	O
the	O
new	O
policy	O
underwent	O
10	O
,	O
000	O
time	O
steps	O
of	O
testing	O
.	O

The	O
results	O
represent	O
the	O
average	O
testing	O
score	O
across	O
three	O
trials	O
after	O
100	O
epoch	O
each	O
.	O

The	O
results	O
show	O
that	O
more	O
nuanced	Method
exploration	Method
strategies	Method
generally	O
improve	O
on	O
the	O
naive	O
epsilon	Method
greedy	Method
approach	Method
,	O
with	O
the	O
Boltzman	Method
and	Method
Thompson	Method
sampling	Method
methods	Method
achieving	O
the	O
best	O
results	O
on	O
three	O
of	O
the	O
games	O
.	O

However	O
,	O
exploration	Method
bonuses	Method
achieve	O
the	O
fastest	O
learning	Task
and	O
the	O
best	O
results	O
most	O
consistently	O
,	O
outperforming	O
the	O
other	O
three	O
methods	O
on	O
7	O
of	O
the	O
14	O
games	O
in	O
terms	O
of	O
AUC	Metric
-	Metric
100	Metric
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
evaluated	O
several	O
scalable	O
and	O
efficient	O
exploration	Method
algorithms	Method
for	O
reinforcement	Task
learning	Task
in	O
tasks	O
with	O
complex	O
,	O
high	O
-	O
dimensional	O
observations	O
.	O

Our	O
results	O
show	O
that	O
a	O
new	O
method	O
based	O
on	O
assigning	O
exploration	O
bonuses	O
most	O
consistently	O
achieves	O
the	O
largest	O
improvement	O
on	O
a	O
range	O
of	O
challenging	O
Atari	Task
games	Task
,	O
particularly	O
those	O
on	O
which	O
human	O
players	O
outperform	O
prior	Method
learning	Method
methods	Method
.	O

Our	O
exploration	Method
method	Method
learns	O
a	O
model	O
of	O
the	O
dynamics	O
concurrently	O
with	O
the	O
policy	O
.	O

This	O
model	O
predicts	O
a	O
learned	O
representation	O
of	O
the	O
state	O
,	O
and	O
a	O
function	O
of	O
this	O
prediction	O
error	O
is	O
added	O
to	O
the	O
reward	O
as	O
an	O
exploration	O
bonus	O
to	O
encourage	O
the	O
policy	O
to	O
visit	O
states	O
with	O
high	O
novelty	O
.	O

One	O
of	O
the	O
limitations	O
of	O
our	O
approach	O
is	O
that	O
the	O
misprediction	Metric
error	Metric
metric	Metric
assumes	O
that	O
any	O
misprediction	O
in	O
the	O
state	O
is	O
caused	O
by	O
inaccuracies	O
in	O
the	O
model	O
.	O

While	O
this	O
is	O
true	O
in	O
determinstic	O
environments	O
,	O
stochastic	O
dynamics	O
violate	O
this	O
assumption	O
.	O

An	O
extension	O
of	O
our	O
approach	O
to	O
stochastic	Task
systems	Task
requires	O
a	O
more	O
nuanced	O
treatment	O
of	O
the	O
distinction	O
between	O
stochastic	O
dynamics	O
and	O
uncertain	O
dynamics	O
,	O
which	O
we	O
hope	O
to	O
explore	O
in	O
future	O
work	O
.	O

Another	O
intriguing	O
direction	O
for	O
future	O
work	O
is	O
to	O
examine	O
how	O
the	O
learned	Method
dynamics	Method
model	Method
can	O
be	O
incorporated	O
into	O
the	O
policy	Method
learning	Method
process	Method
,	O
beyond	O
just	O
providing	O
exploration	O
bonuses	O
.	O

This	O
could	O
in	O
principle	O
enable	O
substantially	O
faster	O
learning	Task
than	O
purely	O
model	Method
-	Method
free	Method
approaches	Method
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Appendix	O
subsection	O
:	O
On	O
auto	Method
encoder	Method
layer	Method
selection	Method
Recall	O
that	O
we	O
trained	O
an	O
auto	Method
-	Method
encoder	Method
to	O
encode	O
the	O
game	O
’s	O
state	O
space	O
.	O

We	O
then	O
trained	O
a	O
predictive	Method
model	Method
on	O
the	O
next	O
auto	O
-	O
encoded	O
frame	O
rather	O
than	O
directly	O
training	O
on	O
the	O
pixel	O
intensity	O
values	O
of	O
the	O
next	O
frame	O
.	O

To	O
obtain	O
the	O
encoded	O
space	O
,	O
we	O
ran	O
each	O
state	O
through	O
an	O
eight	Method
layer	Method
auto	Method
-	Method
encoder	Method
for	O
training	O
and	O
then	O
utilized	O
the	O
auto	Method
-	Method
encoder	Method
’s	Method
sixth	Method
layer	Method
as	O
an	O
encoded	O
state	O
space	O
.	O

We	O
chose	O
to	O
use	O
the	O
sixth	Method
layer	Method
rather	O
than	O
the	O
bottleneck	Method
fourth	Method
layer	Method
because	O
we	O
found	O
that	O
,	O
over	O
20	O
iterations	O
of	O
Seaquest	O
at	O
100	O
epochs	O
per	O
iteration	O
,	O
using	O
this	O
layer	O
for	O
encoding	Task
delivered	O
measurably	O
better	O
performance	O
than	O
using	O
the	O
bottleneck	Method
layer	Method
.	O

The	O
results	O
of	O
that	O
experiment	O
are	O
presented	O
below	O
.	O

subsection	O
:	O
On	O
the	O
quality	O
of	O
the	O
learned	O
model	Method
dynamics	Method
Evaluating	O
the	O
quality	O
of	O
the	O
learned	O
dynamics	Method
model	Method
is	O
somewhat	O
difficult	O
because	O
the	O
system	O
is	O
rewarded	O
achieving	O
higher	O
error	Metric
rates	Metric
.	O

A	O
dynamics	Method
model	Method
that	O
converges	O
quickly	O
is	O
not	O
useful	O
for	O
exploration	Task
bonuses	Task
.	O

Nevertheless	O
,	O
when	O
we	O
plot	O
the	O
mean	O
of	O
the	O
normalized	O
residuals	O
across	O
all	O
games	O
and	O
all	O
trials	O
used	O
in	O
our	O
experiments	O
,	O
we	O
see	O
that	O
the	O
errors	O
of	O
the	O
learned	O
dynamics	Method
models	Method
continually	O
decrease	O
over	O
time	O
.	O

The	O
mean	O
normalized	O
residual	O
after	O
100	O
epochs	O
is	O
approximately	O
half	O
of	O
the	O
maximal	O
mean	O
achieved	O
.	O

This	O
suggests	O
that	O
each	O
dynamics	Method
model	Method
was	O
able	O
to	O
correctly	O
learn	O
properties	O
of	O
underlying	O
dynamics	O
for	O
its	O
given	O
game	O
.	O

subsection	O
:	O
Raw	Metric
AUC	Metric
-	Metric
100	Metric
scores	Metric
