document	O
:	O
A	O
large	O
annotated	O
corpus	O
for	O
learning	Task
natural	Task
language	Task
inference	Task
Understanding	Task
entailment	Task
and	Task
contradiction	Task
is	O
fundamental	O
to	O
understanding	Task
natural	Task
language	Task
,	O
and	O
inference	Task
about	Task
entailment	Task
and	Task
contradiction	Task
is	O
a	O
valuable	O
testing	O
ground	O
for	O
the	O
development	O
of	O
semantic	Task
representations	Task
.	O

However	O
,	O
machine	Task
learning	Task
research	O
in	O
this	O
area	O
has	O
been	O
dramatically	O
limited	O
by	O
the	O
lack	O
of	O
large	O
-	O
scale	O
resources	O
.	O

To	O
address	O
this	O
,	O
we	O
introduce	O
the	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
corpus	Material
,	O
a	O
new	O
,	O
freely	O
available	O
collection	O
of	O
labeled	O
sentence	O
pairs	O
,	O
written	O
by	O
humans	O
doing	O
a	O
novel	O
grounded	Task
task	Task
based	O
on	O
image	Task
captioning	Task
.	O

At	O
570	O
K	O
pairs	O
,	O
it	O
is	O
two	O
orders	O
of	O
magnitude	O
larger	O
than	O
all	O
other	O
resources	O
of	O
its	O
type	O
.	O

This	O
increase	O
in	O
scale	O
allows	O
lexicalized	Method
classifiers	O
to	O
outperform	O
some	O
sophisticated	O
existing	O
entailment	Method
models	Method
,	O
and	O
it	O
allows	O
a	O
neural	Method
network	Method
-	Method
based	Method
model	Method
to	O
perform	O
competitively	O
on	O
natural	Task
language	Task
inference	Task
benchmarks	Task
for	O
the	O
first	O
time	O
.	O

section	O
:	O
Introduction	O
The	O
semantic	O
concepts	O
of	O
entailment	O
and	O
contradiction	O
are	O
central	O
to	O
all	O
aspects	O
of	O
natural	Task
language	Task
meaning	Task
,	O
from	O
the	O
lexicon	O
to	O
the	O
content	O
of	O
entire	O
texts	O
.	O

Thus	O
,	O
natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
—	O
characterizing	O
and	O
using	O
these	O
relations	O
in	O
computational	Task
systems	Task
—	O
is	O
essential	O
in	O
tasks	O
ranging	O
from	O
information	Task
retrieval	Task
to	O
semantic	Task
parsing	Task
to	O
commonsense	Task
reasoning	Task
.	O

NLI	Task
has	O
been	O
addressed	O
using	O
a	O
variety	O
of	O
techniques	O
,	O
including	O
those	O
based	O
on	O
symbolic	Method
logic	Method
,	O
knowledge	Method
bases	Method
,	O
and	O
neural	Method
networks	Method
.	O

In	O
recent	O
years	O
,	O
it	O
has	O
become	O
an	O
important	O
testing	O
ground	O
for	O
approaches	O
employing	O
distributed	Task
word	Task
and	O
phrase	Task
representations	Task
.	O

Distributed	Method
representations	Method
excel	O
at	O
capturing	O
relations	O
based	O
in	O
similarity	O
,	O
and	O
have	O
proven	O
effective	O
at	O
modeling	O
simple	O
dimensions	O
of	O
meaning	O
like	O
evaluative	O
sentiment	O
(	O
e.g.	O
,	O
?	O
)	O
,	O
but	O
it	O
is	O
less	O
clear	O
that	O
they	O
can	O
be	O
trained	O
to	O
support	O
the	O
full	O
range	O
of	O
logical	O
and	O
commonsense	O
inferences	O
required	O
for	O
NLI	Task
.	O

In	O
a	O
SemEval	Task
2014	Task
task	Task
aimed	O
at	O
evaluating	O
distributed	Method
representations	Method
for	O
NLI	Task
,	O
the	O
best	O
-	O
performing	O
systems	O
relied	O
heavily	O
on	O
additional	O
features	O
and	O
reasoning	Method
capabilities	Method
.	O

Our	O
ultimate	O
objective	O
is	O
to	O
provide	O
an	O
empirical	O
evaluation	O
of	O
learning	Method
-	Method
centered	Method
approaches	Method
to	O
NLI	Task
,	O
advancing	O
the	O
case	O
for	O
NLI	Task
as	O
a	O
tool	O
for	O
the	O
evaluation	O
of	O
domain	Method
-	Method
general	Method
approaches	Method
to	O
semantic	Task
representation	Task
.	O

However	O
,	O
in	O
our	O
view	O
,	O
existing	O
NLI	Task
corpora	Task
do	O
not	O
permit	O
such	O
an	O
assessment	O
.	O

They	O
are	O
generally	O
too	O
small	O
for	O
training	O
modern	O
data	O
-	O
intensive	O
,	O
wide	Method
-	Method
coverage	Method
models	Method
,	O
many	O
contain	O
sentences	O
that	O
were	O
algorithmically	O
generated	O
,	O
and	O
they	O
are	O
often	O
beset	O
with	O
indeterminacies	O
of	O
event	Method
and	Method
entity	Method
coreference	Method
that	O
significantly	O
impact	O
annotation	Metric
quality	Metric
.	O

To	O
address	O
this	O
,	O
this	O
paper	O
introduces	O
the	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	Material
corpus	Material
,	O
a	O
collection	O
of	O
sentence	O
pairs	O
labeled	O
for	O
entailment	O
,	O
contradiction	O
,	O
and	O
semantic	O
independence	O
.	O

At	O
570	O
,	O
152	O
sentence	O
pairs	O
,	O
SNLI	Material
is	O
two	O
orders	O
of	O
magnitude	O
larger	O
than	O
all	O
other	O
resources	O
of	O
its	O
type	O
.	O

And	O
,	O
in	O
contrast	O
to	O
many	O
such	O
resources	O
,	O
all	O
of	O
its	O
sentences	O
and	O
labels	O
were	O
written	O
by	O
humans	O
in	O
a	O
grounded	O
,	O
naturalistic	O
context	O
.	O

In	O
a	O
separate	O
validation	O
phase	O
,	O
we	O
collected	O
four	O
additional	O
judgments	O
for	O
each	O
label	O
for	O
56	O
,	O
941	O
of	O
the	O
examples	O
.	O

Of	O
these	O
,	O
98	O
%	O
of	O
cases	O
emerge	O
with	O
a	O
three	O
-	O
annotator	O
consensus	O
,	O
and	O
58	O
%	O
see	O
a	O
unanimous	O
consensus	O
from	O
all	O
five	O
annotators	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
this	O
corpus	O
to	O
evaluate	O
a	O
variety	O
of	O
models	O
for	O
natural	Task
language	Task
inference	Task
,	O
including	O
rule	Method
-	Method
based	Method
systems	Method
,	O
simple	O
linear	Method
classifiers	Method
,	O
and	O
neural	Method
network	Method
-	Method
based	Method
models	Method
.	O

We	O
find	O
that	O
two	O
models	O
achieve	O
comparable	O
performance	O
:	O
a	O
feature	Method
-	Method
rich	Method
classifier	Method
model	Method
and	O
a	O
neural	Method
network	Method
model	Method
centered	O
around	O
a	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
network	Method
(	O
LSTM	Method
;	O
?	O
)	O
.	O

We	O
further	O
evaluate	O
the	O
LSTM	Method
model	Method
by	O
taking	O
advantage	O
of	O
its	O
ready	O
support	O
for	O
transfer	Task
learning	Task
,	O
and	O
show	O
that	O
it	O
can	O
be	O
adapted	O
to	O
an	O
existing	O
NLI	Task
challenge	Task
task	Task
,	O
yielding	O
the	O
best	O
reported	O
performance	O
by	O
a	O
neural	Method
network	Method
model	Method
and	O
approaching	O
the	O
overall	O
state	O
of	O
the	O
art	O
.	O

section	O
:	O
A	O
new	O
corpus	O
for	O
NLI	Task
To	O
date	O
,	O
the	O
primary	O
sources	O
of	O
annotated	O
NLI	Task
corpora	O
have	O
been	O
the	O
Recognizing	Task
Textual	Task
Entailment	Task
(	O
RTE	Task
)	Task
challenge	Task
tasks	Task
.	O

These	O
are	O
generally	O
high	O
-	O
quality	O
,	O
hand	O
-	O
labeled	O
data	O
sets	O
,	O
and	O
they	O
have	O
stimulated	O
innovative	O
logical	Method
and	Method
statistical	Method
models	Method
of	O
natural	Task
language	Task
reasoning	Task
,	O
but	O
their	O
small	O
size	O
(	O
fewer	O
than	O
a	O
thousand	O
examples	O
each	O
)	O
limits	O
their	O
utility	O
as	O
a	O
testbed	O
for	O
learned	O
distributed	Method
representations	Method
.	O

The	O
data	O
for	O
the	O
SemEval	Task
2014	Task
task	Task
called	O
Sentences	O
Involving	O
Compositional	O
Knowledge	O
(	O
SICK	O
)	O
is	O
a	O
step	O
up	O
in	O
terms	O
of	O
size	O
,	O
but	O
only	O
to	O
4	O
,	O
500	O
training	O
examples	O
,	O
and	O
its	O
partly	O
automatic	O
construction	O
introduced	O
some	O
spurious	O
patterns	O
into	O
the	O
data	O
(	O
?	O
,	O
6	O
)	O
.	O

The	O
Denotation	O
Graph	O
entailment	O
set	O
contains	O
millions	O
of	O
examples	O
of	O
entailments	O
between	O
sentences	O
and	O
artificially	O
constructed	O
short	O
phrases	O
,	O
but	O
it	O
was	O
labeled	O
using	O
fully	O
automatic	Method
methods	Method
,	O
and	O
is	O
noisy	O
enough	O
that	O
it	O
is	O
probably	O
suitable	O
only	O
as	O
a	O
source	O
of	O
supplementary	O
training	O
data	O
.	O

Outside	O
the	O
domain	O
of	O
sentence	Task
-	Task
level	Task
entailment	Task
,	O
levy2014focused	O
introduce	O
a	O
large	O
corpus	O
of	O
semi	O
-	O
automatically	O
annotated	O
entailment	O
examples	O
between	O
subject	O
–	O
verb	O
–	O
object	O
relation	O
triples	O
,	O
and	O
the	O
second	O
release	O
of	O
the	O
Paraphrase	O
Database	O
includes	O
automatically	O
generated	O
entailment	O
annotations	O
over	O
a	O
large	O
corpus	O
of	O
pairs	O
of	O
words	O
and	O
short	O
phrases	O
.	O

Existing	O
resources	O
suffer	O
from	O
a	O
subtler	O
issue	O
that	O
impacts	O
even	O
projects	O
using	O
only	O
human	O
-	O
provided	O
annotations	O
:	O
indeterminacies	O
of	O
event	O
and	O
entity	O
coreference	O
lead	O
to	O
insurmountable	O
indeterminacy	O
concerning	O
the	O
correct	O
semantic	O
label	O
(	O
?	O
;	O
?	O
)	O
.	O

For	O
an	O
example	O
of	O
the	O
pitfalls	O
surrounding	O
entity	Task
coreference	Task
,	O
consider	O
the	O
sentence	O
pair	O
A	O
boat	O
sank	O
in	O
the	O
Pacific	O
Ocean	O
and	O
A	O
boat	O
sank	O
in	O
the	O
Atlantic	O
Ocean	O
.	O

The	O
pair	O
could	O
be	O
labeled	O
as	O
a	O
contradiction	O
if	O
one	O
assumes	O
that	O
the	O
two	O
sentences	O
refer	O
to	O
the	O
same	O
single	O
event	O
,	O
but	O
could	O
also	O
be	O
reasonably	O
labeled	O
as	O
neutral	O
if	O
that	O
assumption	O
is	O
not	O
made	O
.	O

In	O
order	O
to	O
ensure	O
that	O
our	O
labeling	Method
scheme	Method
assigns	O
a	O
single	O
correct	O
label	O
to	O
every	O
pair	O
,	O
we	O
must	O
select	O
one	O
of	O
these	O
approaches	O
across	O
the	O
board	O
,	O
but	O
both	O
choices	O
present	O
problems	O
.	O

If	O
we	O
opt	O
not	O
to	O
assume	O
that	O
events	O
are	O
coreferent	O
,	O
then	O
we	O
will	O
only	O
ever	O
find	O
contradictions	O
between	O
sentences	O
that	O
make	O
broad	O
universal	O
assertions	O
,	O
but	O
if	O
we	O
opt	O
to	O
assume	O
coreference	O
,	O
new	O
counterintuitive	O
predictions	O
emerge	O
.	O

For	O
example	O
,	O
Ruth	O
Bader	O
Ginsburg	O
was	O
appointed	O
to	O
the	O
US	O
Supreme	O
Court	O
and	O
I	O
had	O
a	O
sandwich	O
for	O
lunch	O
today	O
would	O
unintuitively	O
be	O
labeled	O
as	O
a	O
contradiction	O
,	O
rather	O
than	O
neutral	O
,	O
under	O
this	O
assumption	O
.	O

Entity	Task
coreference	Task
presents	O
a	O
similar	O
kind	O
of	O
indeterminacy	O
,	O
as	O
in	O
the	O
pair	O
A	O
tourist	O
visited	O
New	O
York	O
and	O
A	O
tourist	O
visited	O
the	O
city	O
.	O

Assuming	O
coreference	O
between	O
New	O
York	O
and	O
the	O
city	O
justifies	O
labeling	O
the	O
pair	O
as	O
an	O
entailment	O
,	O
but	O
without	O
that	O
assumption	O
the	O
city	O
could	O
be	O
taken	O
to	O
refer	O
to	O
a	O
specific	O
unknown	O
city	O
,	O
leaving	O
the	O
pair	O
neutral	O
.	O

This	O
kind	O
of	O
indeterminacy	O
of	O
label	O
can	O
be	O
resolved	O
only	O
once	O
the	O
questions	O
of	O
coreference	O
are	O
resolved	O
.	O

With	O
SNLI	Material
,	O
we	O
sought	O
to	O
address	O
the	O
issues	O
of	O
size	Metric
,	O
quality	Metric
,	O
and	O
indeterminacy	O
.	O

To	O
do	O
this	O
,	O
we	O
employed	O
a	O
crowdsourcing	Method
framework	Method
with	O
the	O
following	O
crucial	O
innovations	O
.	O

First	O
,	O
the	O
examples	O
were	O
grounded	O
in	O
specific	O
scenarios	O
,	O
and	O
the	O
premise	O
and	O
hypothesis	O
sentences	O
in	O
each	O
example	O
were	O
constrained	O
to	O
describe	O
that	O
scenario	O
from	O
the	O
same	O
perspective	O
,	O
which	O
helps	O
greatly	O
in	O
controlling	O
event	Task
and	Task
entity	Task
coreference	Task
.	O

Second	O
,	O
the	O
prompt	O
gave	O
participants	O
the	O
freedom	O
to	O
produce	O
entirely	O
novel	O
sentences	O
within	O
the	O
task	O
setting	O
,	O
which	O
led	O
to	O
richer	O
examples	O
than	O
we	O
see	O
with	O
the	O
more	O
proscribed	O
string	Method
-	Method
editing	Method
techniques	Method
of	O
earlier	O
approaches	O
,	O
without	O
sacrificing	O
consistency	O
.	O

Third	O
,	O
a	O
subset	O
of	O
the	O
resulting	O
sentences	O
were	O
sent	O
to	O
a	O
validation	Task
task	Task
aimed	O
at	O
providing	O
a	O
highly	O
reliable	O
set	O
of	O
annotations	O
over	O
the	O
same	O
data	O
,	O
and	O
at	O
identifying	O
areas	O
of	O
inferential	O
uncertainty	O
.	O

subsection	O
:	O
Data	O
collection	O
We	O
will	O
show	O
you	O
the	O
caption	O
for	O
a	O
photo	O
.	O

We	O
will	O
not	O
show	O
you	O
the	O
photo	O
.	O

Using	O
only	O
the	O
caption	O
and	O
what	O
you	O
know	O
about	O
the	O
world	O
:	O
Write	O
one	O
alternate	O
caption	O
that	O
is	O
definitely	O
a	O
true	O
description	O
of	O
the	O
photo	O
.	O

Example	O
:	O
For	O
the	O
caption	O
“	O
Two	O
dogs	O
are	O
running	O
through	O
a	O
field	O
.	O

”	O
you	O
could	O
write	O
“	O
There	O
are	O
animals	O
outdoors	O
.	O

”	O
Write	O
one	O
alternate	O
caption	O
that	O
might	O
be	O
a	O
true	O
description	O
of	O
the	O
photo	O
.	O

Example	O
:	O
For	O
the	O
caption	O
“	O
Two	O
dogs	O
are	O
running	O
through	O
a	O
field	O
.	O

”	O
you	O
could	O
write	O
“	O
Some	O
puppies	O
are	O
running	O
to	O
catch	O
a	O
stick	O
.	O

”	O
Write	O
one	O
alternate	O
caption	O
that	O
is	O
definitely	O
a	O
false	O
description	O
of	O
the	O
photo	O
.	O

Example	O
:	O
For	O
the	O
caption	O
“	O
Two	O
dogs	O
are	O
running	O
through	O
a	O
field	O
.	O

”	O
you	O
could	O
write	O
“	O
The	O
pets	O
are	O
sitting	O
on	O
a	O
couch	O
.	O

”	O
This	O
is	O
different	O
from	O
the	O
maybe	O
correct	O
category	O
because	O
it	O
’s	O
impossible	O
for	O
the	O
dogs	O
to	O
be	O
both	O
running	O
and	O
sitting	O
.	O

We	O
used	O
Amazon	O
Mechanical	O
Turk	O
for	O
data	Task
collection	Task
.	O

In	O
each	O
individual	O
task	O
(	O
each	O
HIT	O
)	O
,	O
a	O
worker	O
was	O
presented	O
with	O
premise	O
scene	O
descriptions	O
from	O
a	O
pre	O
-	O
existing	O
corpus	O
,	O
and	O
asked	O
to	O
supply	O
hypotheses	O
for	O
each	O
of	O
our	O
three	O
labels—	O
entailment	O
,	O
neutral	O
,	O
and	O
contradiction	O
—forcing	O
the	O
data	O
to	O
be	O
balanced	O
among	O
these	O
classes	O
.	O

The	O
instructions	O
that	O
we	O
provided	O
to	O
the	O
workers	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Below	O
the	O
instructions	O
were	O
three	O
fields	O
for	O
each	O
of	O
three	O
requested	O
sentences	O
,	O
corresponding	O
to	O
our	O
entailment	O
,	O
neutral	O
,	O
and	O
contradiction	O
labels	O
,	O
a	O
fourth	O
field	O
(	O
marked	O
optional	O
)	O
for	O
reporting	O
problems	O
,	O
and	O
a	O
link	O
to	O
an	O
FAQ	O
page	O
.	O

That	O
FAQ	Method
grew	O
over	O
the	O
course	O
of	O
data	O
collection	O
.	O

It	O
warned	O
about	O
disallowed	Method
techniques	Method
(	O
e.g.	O
,	O
reusing	O
the	O
same	O
sentence	O
for	O
many	O
different	O
prompts	O
,	O
which	O
we	O
saw	O
in	O
a	O
few	O
cases	O
)	O
,	O
provided	O
guidance	O
concerning	O
sentence	O
length	O
and	O
complexity	O
(	O
we	O
did	O
not	O
enforce	O
a	O
minimum	O
length	O
,	O
and	O
we	O
allowed	O
bare	O
NPs	O
as	O
well	O
as	O
full	O
sentences	O
)	O
,	O
and	O
reviewed	O
logistical	O
issues	O
around	O
payment	O
timing	O
.	O

About	O
2	O
,	O
500	O
workers	O
contributed	O
.	O

For	O
the	O
premises	O
,	O
we	O
used	O
captions	O
from	O
the	O
Flickr30k	O
corpus	O
,	O
a	O
collection	O
of	O
approximately	O
160k	O
captions	O
(	O
corresponding	O
to	O
about	O
30k	O
images	O
)	O
collected	O
in	O
an	O
earlier	O
crowdsourced	O
effort	O
.	O

The	O
captions	O
were	O
not	O
authored	O
by	O
the	O
photographers	O
who	O
took	O
the	O
source	O
images	O
,	O
and	O
they	O
tend	O
to	O
contain	O
relatively	O
literal	O
scene	O
descriptions	O
that	O
are	O
suited	O
to	O
our	O
approach	O
,	O
rather	O
than	O
those	O
typically	O
associated	O
with	O
personal	O
photographs	O
(	O
as	O
in	O
their	O
example	O
:	O
Our	O
trip	O
to	O
the	O
Olympic	O
Peninsula	O
)	O
.	O

In	O
order	O
to	O
ensure	O
that	O
the	O
label	O
for	O
each	O
sentence	O
pair	O
can	O
be	O
recovered	O
solely	O
based	O
on	O
the	O
available	O
text	O
,	O
we	O
did	O
not	O
use	O
the	O
images	O
at	O
all	O
during	O
corpus	O
collection	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
some	O
key	O
statistics	O
about	O
the	O
collected	O
corpus	O
,	O
and	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
distributions	O
of	O
sentence	O
lengths	O
for	O
both	O
our	O
source	O
hypotheses	O
and	O
our	O
newly	O
collected	O
premises	O
.	O

We	O
observed	O
that	O
while	O
premise	O
sentences	O
varied	O
considerably	O
in	O
length	O
,	O
hypothesis	O
sentences	O
tended	O
to	O
be	O
as	O
short	O
as	O
possible	O
while	O
still	O
providing	O
enough	O
information	O
to	O
yield	O
a	O
clear	O
judgment	O
,	O
clustering	O
at	O
around	O
seven	O
words	O
.	O

We	O
also	O
observed	O
that	O
the	O
bulk	O
of	O
the	O
sentences	O
from	O
both	O
sources	O
were	O
syntactically	O
complete	O
rather	O
than	O
fragments	O
,	O
and	O
the	O
frequency	O
with	O
which	O
the	O
parser	Method
produces	O
a	O
parse	O
rooted	O
with	O
an	O
‘S	O
’	O
(	O
sentence	O
)	O
node	O
attests	O
to	O
this	O
.	O

subsection	O
:	O
Data	Task
validation	Task
In	O
order	O
to	O
measure	O
the	O
quality	O
of	O
our	O
corpus	O
,	O
and	O
in	O
order	O
to	O
construct	O
maximally	O
useful	O
testing	O
and	O
development	O
sets	O
,	O
we	O
performed	O
an	O
additional	O
round	O
of	O
validation	O
for	O
about	O
10	O
%	O
of	O
our	O
data	O
.	O

This	O
validation	O
phase	O
followed	O
the	O
same	O
basic	O
form	O
as	O
the	O
Mechanical	Task
Turk	Task
labeling	Task
task	Task
used	O
to	O
label	O
the	O
SICK	O
entailment	O
data	O
:	O
we	O
presented	O
workers	O
with	O
pairs	O
of	O
sentences	O
in	O
batches	O
of	O
five	O
,	O
and	O
asked	O
them	O
to	O
choose	O
a	O
single	O
label	O
for	O
each	O
pair	O
.	O

We	O
supplied	O
each	O
pair	O
to	O
four	O
annotators	O
,	O
yielding	O
five	O
labels	O
per	O
pair	O
including	O
the	O
label	O
used	O
by	O
the	O
original	O
author	O
.	O

The	O
instructions	O
were	O
similar	O
to	O
the	O
instructions	O
for	O
initial	O
data	O
collection	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
and	O
linked	O
to	O
a	O
similar	O
FAQ	Method
.	O

Though	O
we	O
initially	O
used	O
a	O
very	O
restrictive	O
qualification	O
(	O
based	O
on	O
past	Metric
approval	Metric
rate	Metric
)	O
to	O
select	O
workers	O
for	O
the	O
validation	Task
task	Task
,	O
we	O
nonetheless	O
discovered	O
(	O
and	O
deleted	O
)	O
some	O
instances	O
of	O
random	Method
guessing	Method
in	O
an	O
early	O
batch	O
of	O
work	O
,	O
and	O
subsequently	O
instituted	O
a	O
fully	O
closed	O
qualification	O
restricted	O
to	O
about	O
30	O
trusted	O
workers	O
.	O

For	O
each	O
pair	O
that	O
we	O
validated	O
,	O
we	O
assigned	O
a	O
gold	O
label	O
.	O

If	O
any	O
one	O
of	O
the	O
three	O
labels	O
was	O
chosen	O
by	O
at	O
least	O
three	O
of	O
the	O
five	O
annotators	O
,	O
it	O
was	O
chosen	O
as	O
the	O
gold	O
label	O
.	O

If	O
there	O
was	O
no	O
such	O
consensus	O
,	O
which	O
occurred	O
in	O
about	O
2	O
%	O
of	O
cases	O
,	O
we	O
assigned	O
the	O
placeholder	O
label	O
‘	O
-	O
’	O
.	O

While	O
these	O
unlabeled	O
examples	O
are	O
included	O
in	O
the	O
corpus	O
distribution	O
,	O
they	O
are	O
unlikely	O
to	O
be	O
helpful	O
for	O
the	O
standard	O
NLI	Task
classification	Task
task	Task
,	O
and	O
we	O
do	O
not	O
include	O
them	O
in	O
either	O
training	O
or	O
evaluation	Task
in	O
the	O
experiments	O
that	O
we	O
discuss	O
in	O
this	O
paper	O
.	O

The	O
results	O
of	O
this	O
validation	O
process	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Nearly	O
all	O
of	O
the	O
examples	O
received	O
a	O
majority	O
label	O
,	O
indicating	O
broad	O
consensus	O
about	O
the	O
nature	O
of	O
the	O
data	O
and	O
categories	O
.	O

The	O
gold	O
-	O
labeled	O
examples	O
are	O
very	O
nearly	O
evenly	O
distributed	O
across	O
the	O
three	O
labels	O
.	O

The	O
Fleiss	Metric
scores	Metric
(	O
computed	O
over	O
every	O
example	O
with	O
a	O
full	O
five	O
annotations	O
)	O
are	O
likely	O
to	O
be	O
conservative	O
given	O
our	O
large	O
and	O
unevenly	O
distributed	O
pool	O
of	O
annotators	O
,	O
but	O
they	O
still	O
provide	O
insights	O
about	O
the	O
levels	O
of	O
disagreement	O
across	O
the	O
three	O
semantic	O
classes	O
.	O

This	O
disagreement	O
likely	O
reflects	O
not	O
just	O
the	O
limitations	O
of	O
large	O
crowdsourcing	O
efforts	O
but	O
also	O
the	O
uncertainty	O
inherent	O
in	O
naturalistic	Task
NLI	Task
.	O

Regardless	O
,	O
the	O
overall	O
rate	O
of	O
agreement	Metric
is	O
extremely	O
high	O
,	O
suggesting	O
that	O
the	O
corpus	O
is	O
sufficiently	O
high	O
quality	O
to	O
pose	O
a	O
challenging	O
but	O
realistic	O
machine	Task
learning	Task
task	Task
.	O

subsection	O
:	O
The	O
distributed	O
corpus	O
Table	O
[	O
reference	O
]	O
shows	O
a	O
set	O
of	O
randomly	O
chosen	O
validated	O
examples	O
from	O
the	O
development	O
set	O
with	O
their	O
labels	O
.	O

Qualitatively	O
,	O
we	O
find	O
the	O
data	O
that	O
we	O
collected	O
draws	O
fairly	O
extensively	O
on	O
commonsense	O
knowledge	O
,	O
and	O
that	O
hypothesis	O
and	O
premise	O
sentences	O
often	O
differ	O
structurally	O
in	O
significant	O
ways	O
,	O
suggesting	O
that	O
there	O
is	O
room	O
for	O
improvement	O
beyond	O
superficial	Method
word	Method
alignment	Method
models	Method
.	O

We	O
also	O
find	O
the	O
sentences	O
that	O
we	O
collected	O
to	O
be	O
largely	O
fluent	O
,	O
correctly	O
spelled	O
English	O
,	O
with	O
a	O
mix	O
of	O
full	O
sentences	O
and	O
caption	O
-	O
style	O
noun	O
phrase	O
fragments	O
,	O
though	O
punctuation	O
and	O
capitalization	O
are	O
often	O
omitted	O
.	O

The	O
corpus	O
is	O
available	O
under	O
a	O
CreativeCommons	O
Attribution	O
-	O
ShareAlike	O
license	O
,	O
the	O
same	O
license	O
used	O
for	O
the	O
Flickr30k	O
source	O
captions	O
.	O

It	O
can	O
be	O
downloaded	O
at	O
:	O
paragraph	O
:	O
Partition	O
We	O
distribute	O
the	O
corpus	O
with	O
a	O
pre	O
-	O
specified	O
train	O
/	O
test	O
/	O
development	O
split	O
.	O

The	O
test	O
and	O
development	O
sets	O
contain	O
10k	O
examples	O
each	O
.	O

Each	O
original	O
ImageFlickr	O
caption	O
occurs	O
in	O
only	O
one	O
of	O
the	O
three	O
sets	O
,	O
and	O
all	O
of	O
the	O
examples	O
in	O
the	O
test	O
and	O
development	O
sets	O
have	O
been	O
validated	O
.	O

paragraph	O
:	O
Parses	O
The	O
distributed	O
corpus	O
includes	O
parses	O
produced	O
by	O
the	O
Stanford	Method
PCFG	Method
Parser	Method
3.5.2	Method
,	O
trained	O
on	O
the	O
standard	O
training	O
set	O
as	O
well	O
as	O
on	O
the	O
Brown	O
Corpus	O
(	O
?	O
)	O
,	O
which	O
we	O
found	O
to	O
improve	O
the	O
parse	Metric
quality	Metric
of	O
the	O
descriptive	O
sentences	O
and	O
noun	O
phrases	O
found	O
in	O
the	O
descriptions	O
.	O

section	O
:	O
Our	O
data	O
as	O
a	O
platform	O
for	O
evaluation	O
The	O
most	O
immediate	O
application	O
for	O
our	O
corpus	O
is	O
in	O
developing	O
models	O
for	O
the	O
task	O
of	O
NLI	Task
.	O

In	O
particular	O
,	O
since	O
it	O
is	O
dramatically	O
larger	O
than	O
any	O
existing	O
corpus	O
of	O
comparable	O
quality	O
,	O
we	O
expect	O
it	O
to	O
be	O
suitable	O
for	O
training	O
parameter	Method
-	Method
rich	Method
models	Method
like	O
neural	Method
networks	Method
,	O
which	O
have	O
not	O
previously	O
been	O
competitive	O
at	O
this	O
task	O
.	O

Our	O
ability	O
to	O
evaluate	O
standard	O
classifier	Method
-	Method
base	Method
NLI	Method
models	Method
,	O
however	O
,	O
was	O
limited	O
to	O
those	O
which	O
were	O
designed	O
to	O
scale	O
to	O
SNLI	Material
’s	O
size	O
without	O
modification	O
,	O
so	O
a	O
more	O
complete	O
comparison	O
of	O
approaches	O
will	O
have	O
to	O
wait	O
for	O
future	O
work	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
performance	O
of	O
three	O
classes	O
of	O
models	O
which	O
could	O
scale	O
readily	O
:	O
(	O
i	O
)	O
models	O
from	O
a	O
well	O
-	O
known	O
NLI	Task
system	Task
,	O
the	O
Excitement	Method
Open	Method
Platform	Method
;	O
(	O
ii	O
)	O
variants	O
of	O
a	O
strong	O
but	O
simple	O
feature	Method
-	Method
based	Method
classifier	Method
model	Method
,	O
which	O
makes	O
use	O
of	O
both	O
unlexicalized	O
and	O
lexicalized	Method
features	O
,	O
and	O
(	O
iii	O
)	O
distributed	Method
representation	Method
models	Method
,	O
including	O
a	O
baseline	Method
model	Method
and	O
neural	Method
network	Method
sequence	Method
models	Method
.	O

subsection	O
:	O
Excitement	Method
Open	Method
Platform	Method
models	Method
The	O
first	O
class	O
of	O
models	O
is	O
from	O
the	O
Excitement	O
Open	O
Platform	O
(	O
EOP	O
,	O
?	O
;	O
?	O
)	O
—an	O
open	O
source	O
platform	O
for	O
RTE	Task
research	Task
.	O

EOP	Method
is	O
a	O
tool	O
for	O
quickly	O
developing	O
NLI	Task
systems	Task
while	O
sharing	O
components	O
such	O
as	O
common	O
lexical	O
resources	O
and	O
evaluation	O
sets	O
.	O

We	O
evaluate	O
on	O
two	O
algorithms	O
included	O
in	O
the	O
distribution	O
:	O
a	O
simple	O
edit	Method
-	Method
distance	Method
based	Method
algorithm	Method
and	O
a	O
classifier	Method
-	Method
based	Method
algorithm	Method
,	O
the	O
latter	O
both	O
in	O
a	O
bare	O
form	O
and	O
augmented	O
with	O
EOP	Method
’s	O
full	O
suite	O
of	O
lexical	O
resources	O
.	O

Our	O
initial	O
goal	O
was	O
to	O
better	O
understand	O
the	O
difficulty	O
of	O
the	O
task	O
of	O
classifying	Task
SNLI	Task
corpus	Task
inferences	Task
,	O
rather	O
than	O
necessarily	O
the	O
performance	O
of	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
RTE	Method
system	Method
.	O

We	O
approached	O
this	O
by	O
running	O
the	O
same	O
system	O
on	O
several	O
data	O
sets	O
:	O
our	O
own	O
test	O
set	O
,	O
the	O
SICK	O
test	O
data	O
,	O
and	O
the	O
standard	O
RTE	O
-	O
3	O
test	O
set	O
.	O

We	O
report	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Each	O
of	O
the	O
models	O
was	O
separately	O
trained	O
on	O
the	O
training	O
set	O
of	O
each	O
corpus	O
.	O

All	O
models	O
are	O
evaluated	O
only	O
on	O
2	Task
-	Task
class	Task
entailment	Task
.	O

To	O
convert	O
3	Task
-	Task
class	Task
problems	Task
like	O
SICK	O
and	O
SNLI	Material
to	O
this	O
setting	O
,	O
all	O
instances	O
of	O
contradiction	O
and	O
unknown	O
are	O
converted	O
to	O
nonentailment	O
.	O

This	O
yields	O
a	O
most	O
-	O
frequent	O
-	O
class	Metric
baseline	Metric
accuracy	Metric
of	O
66	O
%	O
on	O
SNLI	Material
,	O
and	O
71	O
%	O
on	O
SICK	O
.	O

This	O
is	O
intended	O
primarily	O
to	O
demonstrate	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
rather	O
than	O
necessarily	O
the	O
performance	O
of	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
RTE	Method
system	Method
.	O

The	O
edit	Method
distance	Method
algorithm	Method
tunes	O
the	O
weight	O
of	O
the	O
three	O
case	Method
-	Method
insensitive	Method
edit	Method
distance	Method
operations	Method
on	O
the	O
training	O
set	O
,	O
after	O
removing	O
stop	O
words	O
.	O

In	O
addition	O
to	O
the	O
base	O
classifier	Method
-	Method
based	Method
system	Method
distributed	O
with	O
the	O
platform	O
,	O
we	O
train	O
a	O
variant	O
which	O
includes	O
information	O
from	O
WordNet	O
and	O
VerbOcean	O
,	O
and	O
makes	O
use	O
of	O
features	O
based	O
on	O
tree	O
patterns	O
and	O
dependency	O
tree	O
skeletons	O
.	O

subsection	O
:	O
Lexicalized	Method
Classifier	Method
Unlike	O
the	O
RTE	O
datasets	O
,	O
SNLI	Material
’s	O
size	O
supports	O
approaches	O
which	O
make	O
use	O
of	O
rich	O
lexicalized	Method
features	O
.	O

We	O
evaluate	O
a	O
simple	O
lexicalized	Method
classifier	Method
to	O
explore	O
the	O
ability	O
of	O
non	O
-	O
specialized	Method
models	Method
to	O
exploit	O
these	O
features	O
in	O
lieu	O
of	O
more	O
involved	O
language	Task
understanding	Task
.	O

Our	O
classifier	Method
implements	O
6	O
feature	O
types	O
;	O
3	O
unlexicalized	O
and	O
3	O
lexicalized	Method
:	O
The	O
BLEU	Metric
score	Metric
of	O
the	O
hypothesis	O
with	O
respect	O
to	O
the	O
premise	O
,	O
using	O
an	O
n	O
-	O
gram	O
length	O
between	O
1	O
and	O
4	O
.	O

The	O
length	O
difference	O
between	O
the	O
hypothesis	O
and	O
the	O
premise	O
,	O
as	O
a	O
real	O
-	O
valued	O
feature	O
.	O

The	O
overlap	O
between	O
words	O
in	O
the	O
premise	O
and	O
hypothesis	O
,	O
both	O
as	O
an	O
absolute	O
count	O
and	O
a	O
percentage	O
of	O
possible	O
overlap	O
,	O
and	O
both	O
over	O
all	O
words	O
and	O
over	O
just	O
nouns	O
,	O
verbs	O
,	O
adjectives	O
,	O
and	O
adverbs	O
.	O

An	O
indicator	O
for	O
every	O
unigram	Method
and	O
bigram	Method
in	O
the	O
hypothesis	O
.	O

Cross	Method
-	Method
unigrams	Method
:	O
for	O
every	O
pair	O
of	O
words	O
across	O
the	O
premise	O
and	O
hypothesis	O
which	O
share	O
a	O
POS	O
tag	O
,	O
an	O
indicator	O
feature	O
over	O
the	O
two	O
words	O
.	O

Cross	Method
-	Method
bigrams	Method
:	O
for	O
every	O
pair	O
of	O
bigrams	O
across	O
the	O
premise	O
and	O
hypothesis	O
which	O
share	O
a	O
POS	O
tag	O
on	O
the	O
second	O
word	O
,	O
an	O
indicator	O
feature	O
over	O
the	O
two	O
bigrams	O
.	O

We	O
report	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
along	O
with	O
ablation	O
studies	O
for	O
removing	O
the	O
cross	O
-	O
bigram	Method
features	O
(	O
leaving	O
only	O
the	O
cross	O
-	O
unigram	Method
feature	Method
)	O
and	O
for	O
removing	O
all	O
lexicalized	Method
features	O
.	O

On	O
our	O
large	O
corpus	O
in	O
particular	O
,	O
there	O
is	O
a	O
substantial	O
jump	O
in	O
accuracy	Metric
from	O
using	O
lexicalized	Method
features	O
,	O
and	O
another	O
from	O
using	O
the	O
very	O
sparse	O
cross	O
-	O
bigram	Method
features	O
.	O

The	O
latter	O
result	O
suggests	O
that	O
there	O
is	O
value	O
in	O
letting	O
the	O
classifier	Method
automatically	O
learn	O
to	O
recognize	O
structures	O
like	O
explicit	O
negations	O
and	O
adjective	O
modification	O
.	O

A	O
similar	O
result	O
was	O
shown	O
in	O
sidaw12simple	O
for	O
bigram	Method
features	O
in	O
sentiment	Task
analysis	Task
.	O

It	O
is	O
surprising	O
that	O
the	O
classifier	Method
performs	O
as	O
well	O
as	O
it	O
does	O
without	O
any	O
notion	O
of	O
alignment	O
or	O
tree	O
transformations	O
.	O

Although	O
we	O
expect	O
that	O
richer	O
models	O
would	O
perform	O
better	O
,	O
the	O
results	O
suggest	O
that	O
given	O
enough	O
data	O
,	O
cross	O
bigrams	O
with	O
the	O
noisy	O
part	O
-	O
of	O
-	O
speech	O
overlap	O
constraint	O
can	O
produce	O
an	O
effective	O
model	O
.	O

subsection	O
:	O
Sentence	Task
embeddings	Task
and	O
NLI	Task
SNLI	Material
is	O
suitably	O
large	O
and	O
diverse	O
to	O
make	O
it	O
possible	O
to	O
train	O
neural	Method
network	Method
models	Method
that	O
produce	O
distributed	Task
representations	Task
of	Task
sentence	Task
meaning	Task
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
the	O
performance	O
of	O
three	O
such	O
models	O
on	O
the	O
corpus	O
.	O

To	O
focus	O
specifically	O
on	O
the	O
strengths	O
of	O
these	O
models	O
at	O
producing	O
informative	Task
sentence	Task
representations	Task
,	O
we	O
use	O
sentence	Method
embedding	Method
as	O
an	O
intermediate	O
step	O
in	O
the	O
NLI	Task
classification	Task
task	Task
:	O
each	O
model	O
must	O
produce	O
a	O
vector	Method
representation	Method
of	O
each	O
of	O
the	O
two	O
sentences	O
without	O
using	O
any	O
context	O
from	O
the	O
other	O
sentence	O
,	O
and	O
the	O
two	O
resulting	O
vectors	O
are	O
then	O
passed	O
to	O
a	O
neural	Method
network	Method
classifier	Method
which	O
predicts	O
the	O
label	O
for	O
the	O
pair	O
.	O

This	O
choice	O
allows	O
us	O
to	O
focus	O
on	O
existing	O
models	O
for	O
sentence	Task
embedding	Task
,	O
and	O
it	O
allows	O
us	O
to	O
evaluate	O
the	O
ability	O
of	O
those	O
models	O
to	O
learn	O
useful	O
representations	O
of	O
meaning	O
(	O
which	O
may	O
be	O
independently	O
useful	O
for	O
subsequent	O
tasks	O
)	O
,	O
at	O
the	O
cost	O
of	O
excluding	O
from	O
consideration	O
possible	O
strong	O
neural	Method
models	Method
for	O
NLI	Task
that	O
directly	O
compare	O
the	O
two	O
inputs	O
at	O
the	O
word	O
or	O
phrase	O
level	O
.	O

label=	O
[	O
text	O
width=40	O
mm	O
,	O
align	O
=	O
center	O
]	O
softmax=	O
[	O
fill	O
=	O
red!40	O
,	O
text	O
width=40	O
mm	O
,	O
align	O
=	O
center	O
]	O
preclass=	O
[	O
fill	O
=	O
orange!40	O
,	O
text	O
width=50	O
mm	O
,	O
align	O
=	O
center	O
]	O
e=	O
[	O
fill	O
=	O
green!40	O
,	O
text	O
width=26	O
mm	O
,	O
align	O
=	O
center	O
]	O
m=	O
[	O
draw	O
=	O
black	O
,	O
text	O
width=38	O
mm	O
,	O
align	O
=	O
center	O
]	O
[	O
softmax	O
]	O
(	O
softmax	O
)	O
at	O
(	O
0	O
*	O
21pt	O
,	O
6	O
*	O
29pt	O
)	O
3	Method
-	Method
way	Method
softmax	Method
classifier	Method
;	O
[	O
preclass	O
]	O
(	O
pc3	O
)	O
at	O
(	O
0	O
*	O
21pt	O
,	O
5	O
*	O
29pt	O
)	O
200d	O
layer	O
;	O
[	O
preclass	O
]	O
(	O
pc2	O
)	O
at	O
(	O
0	O
*	O
21pt	O
,	O
4	O
*	O
29pt	O
)	O
200d	O
layer	O
;	O
[	O
preclass	O
]	O
(	O
pc1	O
)	O
at	O
(	O
0	O
*	O
21pt	O
,	O
3	O
*	O
29pt	O
)	O
200d	O
layer	O
;	O
[	O
e	O
]	O
(	O
pe	O
)	O
at	O
(	O
-	O
3	O
*	O
21pt	O
,	O
1.85	O
*	O
29pt	O
)	O
100d	O
premise	O
;	O
[	O
e	O
]	O
(	O
he	O
)	O
at	O
(	O
3	O
*	O
21pt	O
,	O
1.85	O
*	O
29pt	O
)	O
100d	O
hypothesis	O
;	O
[	O
m	O
]	O
(	O
pem	O
)	O
at	O
(	O
-	O
3	O
*	O
21pt	O
,	O
0.5	O
*	O
29pt	O
)	O
sentence	Method
model	Method
with	O
premise	O
input	O
;	O
[	O
m	O
]	O
(	O
hem	O
)	O
at	O
(	O
3	O
*	O
21pt	O
,	O
0.5	O
*	O
29pt	O
)	O
sentence	Method
model	Method
with	O
hypothesis	O
input	O
;	O
latex	O
fwd	O
=	O
[	O
draw	O
=	O
black	O
,	O
line	O
width=1pt	O
]	O
[	O
fwd	O
]	O
(	O
pc3	O
)	O
–	O
(	O
softmax	O
)	O
;	O
[	O
fwd	O
]	O
(	O
pc2	O
)	O
–	O
(	O
pc3	O
)	O
;	O
[	O
fwd	O
]	O
(	O
pc1	O
)	O
–	O
(	O
pc2	O
)	O
;	O
[	O
fwd	O
]	O
(	O
pe	O
)	O
–	O
(	O
pc1	O
)	O
;	O
[	O
fwd	O
]	O
(	O
he	O
)	O
–	O
(	O
pc1	O
)	O
;	O
[	O
fwd	O
]	O
(	O
hem	O
)	O
–	O
(	O
he	O
)	O
;	O
[	O
fwd	O
]	O
(	O
pem	O
)	O
–	O
(	O
pe	O
)	O
;	O
Our	O
neural	Method
network	Method
classifier	Method
,	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
and	O
based	O
on	O
a	O
one	O
-	O
layer	Method
model	Method
in	O
?	O
)	O
,	O
is	O
simply	O
a	O
stack	O
of	O
three	O
200d	Method
layers	Method
,	O
with	O
the	O
bottom	O
layer	O
taking	O
the	O
concatenated	O
sentence	O
representations	O
as	O
input	O
and	O
the	O
top	O
layer	O
feeding	O
a	O
softmax	Method
classifier	Method
,	O
all	O
trained	O
jointly	O
with	O
the	O
sentence	Method
embedding	Method
model	Method
itself	O
.	O

We	O
test	O
three	O
sentence	Method
embedding	Method
models	Method
,	O
each	O
set	O
to	O
use	O
100d	O
phrase	O
and	O
sentence	O
embeddings	O
.	O

Our	O
baseline	O
sentence	Method
embedding	Method
model	Method
simply	O
sums	O
the	O
embeddings	O
of	O
the	O
words	O
in	O
each	O
sentence	O
.	O

In	O
addition	O
,	O
we	O
experiment	O
with	O
two	O
simple	O
sequence	Method
embedding	Method
models	Method
:	O
a	O
plain	Method
RNN	Method
and	O
an	O
LSTM	Method
RNN	Method
.	O

The	O
word	O
embeddings	O
for	O
all	O
of	O
the	O
models	O
are	O
initialized	O
with	O
the	O
300d	O
reference	O
GloVe	O
vectors	O
(	O
840B	O
token	O
version	O
,	O
?	O
)	O
and	O
fine	O
-	O
tuned	O
as	O
part	O
of	O
training	O
.	O

In	O
addition	O
,	O
all	O
of	O
the	O
models	O
use	O
an	O
additional	O
neural	Method
network	Method
layer	Method
to	O
map	O
these	O
300d	O
embeddings	O
into	O
the	O
lower	O
-	O
dimensional	O
phrase	O
and	O
sentence	O
embedding	O
space	O
.	O

All	O
of	O
the	O
models	O
are	O
randomly	O
initialized	O
using	O
standard	O
techniques	O
and	O
trained	O
using	O
AdaDelta	Method
minibatch	Method
SGD	Method
until	O
performance	O
on	O
the	O
development	O
set	O
stops	O
improving	O
.	O

We	O
applied	O
L2	Method
regularization	Method
to	O
all	O
models	O
,	O
manually	O
tuning	O
the	O
strength	O
coefficient	O
for	O
each	O
,	O
and	O
additionally	O
applied	O
dropout	Method
to	O
the	O
inputs	O
and	O
outputs	O
of	O
the	O
sentence	Method
embedding	Method
models	Method
(	O
though	O
not	O
to	O
its	O
internal	O
connections	O
)	O
with	O
a	O
fixed	O
dropout	O
rate	O
.	O

All	O
models	O
were	O
implemented	O
in	O
a	O
common	O
framework	O
for	O
this	O
paper	O
,	O
and	O
the	O
implementations	O
will	O
be	O
made	O
available	O
at	O
publication	O
time	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
sum	Method
of	Method
words	Method
model	Method
performed	O
slightly	O
worse	O
than	O
the	O
fundamentally	O
similar	O
lexicalized	Method
classifier	Method
—	O
while	O
the	O
sum	Method
of	Method
words	Method
model	Method
can	O
use	O
pretrained	O
word	O
embeddings	O
to	O
better	O
handle	O
rare	O
words	O
,	O
it	O
lacks	O
even	O
the	O
rudimentary	O
sensitivity	O
to	O
word	O
order	O
that	O
the	O
lexicalized	Method
model	Method
’s	Method
bigram	Method
features	Method
provide	O
.	O

Of	O
the	O
two	O
RNN	Method
models	Method
,	O
the	O
LSTM	Method
’s	O
more	O
robust	O
ability	O
to	O
learn	O
long	O
-	O
term	O
dependencies	O
serves	O
it	O
well	O
,	O
giving	O
it	O
a	O
substantial	O
advantage	O
over	O
the	O
plain	O
RNN	Method
,	O
and	O
resulting	O
in	O
performance	O
that	O
is	O
essentially	O
equivalent	O
to	O
the	O
lexicalized	Method
classifier	Method
on	O
the	O
test	O
set	O
(	O
LSTM	Method
performance	O
near	O
the	O
stopping	Metric
iteration	Metric
varies	O
by	O
up	O
to	O
0.5	O
%	O
between	O
evaluation	O
steps	O
)	O
.	O

While	O
the	O
lexicalized	Method
model	Method
fits	O
the	O
training	O
set	O
almost	O
perfectly	O
,	O
the	O
gap	O
between	O
train	O
and	O
test	Metric
set	Metric
accuracy	Metric
is	O
relatively	O
small	O
for	O
all	O
three	O
neural	Method
network	Method
models	Method
,	O
suggesting	O
that	O
research	O
into	O
significantly	O
higher	O
capacity	O
versions	O
of	O
these	O
models	O
would	O
be	O
productive	O
.	O

subsection	O
:	O
Analysis	O
and	O
discussion	O
Figure	O
[	O
reference	O
]	O
shows	O
a	O
learning	O
curve	O
for	O
the	O
LSTM	Method
and	O
the	O
lexicalized	Method
and	O
unlexicalized	Method
feature	Method
-	Method
based	Method
models	Method
.	O

It	O
shows	O
that	O
the	O
large	O
size	O
of	O
the	O
corpus	O
is	O
crucial	O
to	O
both	O
the	O
LSTM	Method
and	O
the	O
lexicalized	Method
model	Method
,	O
and	O
suggests	O
that	O
additional	O
data	O
would	O
yield	O
still	O
better	O
performance	O
for	O
both	O
.	O

In	O
addition	O
,	O
though	O
the	O
LSTM	Method
and	O
the	O
lexicalized	Method
model	Method
show	O
similar	O
performance	O
when	O
trained	O
on	O
the	O
current	O
full	O
corpus	O
,	O
the	O
somewhat	O
steeper	O
slope	O
for	O
the	O
LSTM	Method
hints	O
that	O
its	O
ability	O
to	O
learn	O
arbitrarily	O
structured	O
representations	O
of	O
sentence	O
meaning	O
may	O
give	O
it	O
an	O
advantage	O
over	O
the	O
more	O
constrained	O
lexicalized	Method
model	Method
on	O
still	O
larger	O
datasets	O
.	O

We	O
were	O
struck	O
by	O
the	O
speed	O
with	O
which	O
the	O
lexicalized	Method
classifier	Method
outperforms	O
its	O
unlexicalized	Method
counterpart	Method
.	O

With	O
only	O
100	O
training	O
examples	O
,	O
the	O
cross	O
-	O
bigram	Method
classifier	O
is	O
already	O
performing	O
better	O
.	O

Empirically	O
,	O
we	O
find	O
that	O
the	O
top	O
weighted	O
features	O
for	O
the	O
classifier	Method
trained	O
on	O
100	O
examples	O
tend	O
to	O
be	O
high	O
precision	O
entailments	O
;	O
e.g.	O
,	O
playing	O
outside	O
(	O
most	O
scenes	O
are	O
outdoors	O
)	O
,	O
a	O
banana	O
person	O
eating	O
.	O

If	O
relatively	O
few	O
spurious	O
entailments	O
get	O
high	O
weight	O
—	O
as	O
it	O
appears	O
is	O
the	O
case	O
—	O
then	O
it	O
makes	O
sense	O
that	O
,	O
when	O
these	O
do	O
fire	O
,	O
they	O
boost	O
accuracy	Metric
in	O
identifying	Task
entailments	Task
.	O

There	O
are	O
revealing	O
patterns	O
in	O
the	O
errors	O
common	O
to	O
all	O
the	O
models	O
considered	O
here	O
.	O

Despite	O
the	O
large	O
size	O
of	O
the	O
training	O
corpus	O
and	O
the	O
distributional	O
information	O
captured	O
by	O
GloVe	Method
initialization	Method
,	O
many	O
lexical	O
relationships	O
are	O
still	O
misanalyzed	O
,	O
leading	O
to	O
incorrect	O
predictions	O
of	O
independent	O
,	O
even	O
for	O
pairs	O
that	O
are	O
common	O
in	O
the	O
training	O
corpus	O
like	O
beach	O
/	O
surf	O
and	O
sprinter	O
/	O
runner	O
.	O

Semantic	O
mistakes	O
at	O
the	O
phrasal	O
level	O
(	O
e.g.	O
,	O
predicting	O
contradiction	O
for	O
A	O
male	O
is	O
placing	O
an	O
order	O
in	O
a	O
deli	O
/	O
A	O
man	O
buying	O
a	O
sandwich	O
at	O
a	O
deli	O
)	O
indicate	O
that	O
additional	O
attention	O
to	O
compositional	O
semantics	O
would	O
pay	O
off	O
.	O

However	O
,	O
many	O
of	O
the	O
persistent	O
problems	O
run	O
deeper	O
,	O
to	O
inferences	O
that	O
depend	O
on	O
world	O
knowledge	O
and	O
context	O
-	O
specific	O
inferences	O
,	O
as	O
in	O
the	O
entailment	Task
pair	O
A	O
race	O
car	O
driver	O
leaps	O
from	O
a	O
burning	O
car	O
/	O
A	O
race	O
car	O
driver	O
escaping	O
danger	O
,	O
for	O
which	O
both	O
the	O
lexicalized	Method
classifier	Method
and	O
the	O
LSTM	Method
predict	Method
neutral	Method
.	O

In	O
other	O
cases	O
,	O
the	O
models	O
’	O
attempts	O
to	O
shortcut	O
this	O
kind	O
of	O
inference	O
through	O
lexical	O
cues	O
can	O
lead	O
them	O
astray	O
.	O

Some	O
of	O
these	O
examples	O
have	O
qualities	O
reminiscent	O
of	O
Winograd	Method
schemas	Method
.	O

For	O
example	O
,	O
all	O
the	O
models	O
wrongly	O
predict	O
entailment	O
for	O
A	O
young	O
girl	O
throws	O
sand	O
toward	O
the	O
ocean	O
/	O
A	O
girl	O
ca	O
n’t	O
stand	O
the	O
ocean	O
,	O
presumably	O
because	O
of	O
distributional	O
associations	O
between	O
throws	O
and	O
ca	O
n’t	O
stand	O
.	O

Analysis	O
of	O
the	O
models	O
’	O
predictions	O
also	O
yields	O
insights	O
into	O
the	O
extent	O
to	O
which	O
they	O
grapple	O
with	O
event	Task
and	Task
entity	Task
coreference	Task
.	O

For	O
the	O
most	O
part	O
,	O
the	O
original	O
image	O
prompts	O
contained	O
a	O
focal	O
element	O
that	O
the	O
caption	O
writer	O
identified	O
with	O
a	O
syntactic	O
subject	O
,	O
following	O
information	Method
structuring	Method
conventions	Method
associating	O
subjects	O
and	O
topics	O
in	O
English	O
.	O

Our	O
annotators	O
generally	O
followed	O
suit	O
,	O
writing	O
sentences	O
that	O
,	O
while	O
structurally	O
diverse	O
,	O
share	O
topic	O
/	O
focus	O
(	O
theme	O
/	O
rheme	O
)	O
structure	O
with	O
their	O
premises	O
.	O

This	O
promotes	O
a	O
coherent	O
,	O
situation	O
-	O
specific	O
construal	O
of	O
each	O
sentence	O
pair	O
.	O

This	O
is	O
information	O
that	O
our	O
models	O
can	O
easily	O
take	O
advantage	O
of	O
,	O
but	O
it	O
can	O
lead	O
them	O
astray	O
.	O

For	O
instance	O
,	O
all	O
of	O
them	O
stumble	O
with	O
the	O
amusingly	O
simple	O
case	O
A	O
woman	O
prepares	O
ingredients	O
for	O
a	O
bowl	O
of	O
soup	O
/	O
A	O
soup	O
bowl	O
prepares	O
a	O
woman	O
,	O
in	O
which	O
prior	O
expectations	O
about	O
parallelism	O
are	O
not	O
met	O
.	O

Another	O
headline	O
example	O
of	O
this	O
type	O
is	O
A	O
man	O
wearing	O
padded	O
arm	O
protection	O
is	O
being	O
bitten	O
by	O
a	O
German	O
shepherd	O
dog	O
/	O
A	O
man	O
bit	O
a	O
dog	O
,	O
which	O
all	O
the	O
models	O
wrongly	O
diagnose	O
as	O
entailment	O
,	O
though	O
the	O
sentences	O
report	O
two	O
very	O
different	O
stories	O
.	O

A	O
model	O
with	O
access	O
to	O
explicit	O
information	O
about	O
syntactic	O
or	O
semantic	O
structure	O
should	O
perform	O
better	O
on	O
cases	O
like	O
these	O
.	O

section	O
:	O
Transfer	Task
learning	Task
with	O
SICK	O
To	O
the	O
extent	O
that	O
successfully	O
training	O
a	O
neural	Method
network	Method
model	Method
like	O
our	O
LSTM	Method
on	O
SNLI	Material
forces	O
that	O
model	O
to	O
encode	O
broadly	O
accurate	O
representations	O
of	O
English	O
scene	O
descriptions	O
and	O
to	O
build	O
an	O
entailment	Method
classifier	Method
over	O
those	O
relations	O
,	O
we	O
should	O
expect	O
it	O
to	O
be	O
readily	O
possible	O
to	O
adapt	O
the	O
trained	O
model	O
for	O
use	O
on	O
other	O
NLI	Task
tasks	Task
.	O

In	O
this	O
section	O
,	O
we	O
evaluate	O
on	O
the	O
SICK	Task
entailment	Task
task	Task
using	O
a	O
simple	O
transfer	Method
learning	Method
method	Method
and	O
achieve	O
competitive	O
results	O
.	O

To	O
perform	O
transfer	Task
,	O
we	O
take	O
the	O
parameters	O
of	O
the	O
LSTM	Method
RNN	Method
model	Method
trained	O
on	O
SNLI	Material
and	O
use	O
them	O
to	O
initialize	O
a	O
new	O
model	O
,	O
which	O
is	O
trained	O
from	O
that	O
point	O
only	O
on	O
the	O
training	O
portion	O
of	O
SICK	O
.	O

The	O
only	O
newly	O
initialized	O
parameters	O
are	O
softmax	O
layer	O
parameters	O
and	O
the	O
embeddings	O
for	O
words	O
that	O
appear	O
in	O
SICK	O
,	O
but	O
not	O
in	O
SNLI	Material
(	O
which	O
are	O
populated	O
with	O
GloVe	O
embeddings	O
as	O
above	O
)	O
.	O

We	O
use	O
the	O
same	O
model	O
hyperparameters	O
that	O
were	O
used	O
to	O
train	O
the	O
original	O
model	O
,	O
with	O
the	O
exception	O
of	O
the	O
L2	O
regularization	O
strength	O
,	O
which	O
is	O
re	O
-	O
tuned	O
.	O

We	O
additionally	O
transfer	O
the	O
accumulators	Method
that	O
are	O
used	O
by	O
AdaDelta	Method
to	O
set	O
the	O
learning	Metric
rates	Metric
.	O

This	O
lowers	O
the	O
starting	Metric
learning	Metric
rates	Metric
,	O
and	O
is	O
intended	O
to	O
ensure	O
that	O
the	O
model	O
does	O
not	O
learn	O
too	O
quickly	O
in	O
its	O
first	O
few	O
epochs	O
after	O
transfer	O
and	O
destroy	O
the	O
knowledge	O
accumulated	O
in	O
the	O
pre	O
-	O
transfer	O
phase	O
of	O
training	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Training	O
on	O
SICK	O
alone	O
yields	O
poor	O
performance	O
,	O
and	O
the	O
model	O
trained	O
on	O
SNLI	Material
fails	O
when	O
tested	O
on	O
SICK	O
data	O
,	O
labeling	O
more	O
neutral	O
examples	O
as	O
contradiction	O
s	O
than	O
correctly	O
,	O
possibly	O
as	O
a	O
result	O
of	O
subtle	O
differences	O
in	O
how	O
the	O
labeling	Task
task	Task
was	O
presented	O
.	O

In	O
contrast	O
,	O
transferring	O
SNLI	Material
representations	Material
to	O
SICK	O
yields	O
the	O
best	O
performance	O
yet	O
reported	O
for	O
an	O
unaugmented	Method
neural	Method
network	Method
model	Method
,	O
surpasses	O
the	O
available	O
EOP	Method
models	Method
,	O
and	O
approaches	O
both	O
the	O
overall	O
state	O
of	O
the	O
art	O
at	O
84.6	O
%	O
and	O
the	O
84	O
%	O
level	O
of	O
interannotator	Metric
agreement	Metric
,	O
which	O
likely	O
represents	O
an	O
approximate	O
performance	O
ceiling	O
.	O

This	O
suggests	O
that	O
the	O
introduction	O
of	O
a	O
large	O
high	O
-	O
quality	O
corpus	O
makes	O
it	O
possible	O
to	O
train	O
representation	Method
-	Method
learning	Method
models	Method
for	O
sentence	Task
meaning	Task
that	O
are	O
competitive	O
with	O
the	O
best	O
hand	Method
-	Method
engineered	Method
models	Method
on	O
inference	Task
tasks	Task
.	O

We	O
attempted	O
to	O
apply	O
this	O
same	O
transfer	Method
evaluation	Method
technique	Method
to	O
the	O
RTE	Task
-	Task
3	Task
challenge	Task
,	O
but	O
found	O
that	O
the	O
small	O
training	O
set	O
(	O
800	O
examples	O
)	O
did	O
not	O
allow	O
the	O
model	O
to	O
adapt	O
to	O
the	O
unfamiliar	O
genre	O
of	O
text	O
used	O
in	O
that	O
corpus	O
,	O
such	O
that	O
no	O
training	O
configuration	O
yielded	O
competitive	O
performance	O
.	O

Further	O
research	O
on	O
effective	O
transfer	Method
learning	Method
on	O
small	O
data	O
sets	O
with	O
neural	Method
models	Method
might	O
facilitate	O
improvements	O
here	O
.	O

section	O
:	O
Conclusion	O
Natural	O
languages	O
are	O
powerful	O
vehicles	O
for	O
reasoning	Task
,	O
and	O
nearly	O
all	O
questions	O
about	O
meaningfulness	O
in	O
language	O
can	O
be	O
reduced	O
to	O
questions	O
of	O
entailment	O
and	O
contradiction	O
in	O
context	O
.	O

This	O
suggests	O
that	O
NLI	Task
is	O
an	O
ideal	O
testing	O
ground	O
for	O
theories	O
of	O
semantic	Task
representation	Task
,	O
and	O
that	O
training	O
for	O
NLI	Task
tasks	Task
can	O
provide	O
rich	O
domain	Method
-	Method
general	Method
semantic	Method
representations	Method
.	O

To	O
date	O
,	O
however	O
,	O
it	O
has	O
not	O
been	O
possible	O
to	O
fully	O
realize	O
this	O
potential	O
due	O
to	O
the	O
limited	O
nature	O
of	O
existing	O
NLI	Task
resources	Task
.	O

This	O
paper	O
sought	O
to	O
remedy	O
this	O
with	O
a	O
new	O
,	O
large	O
-	O
scale	O
,	O
naturalistic	O
corpus	O
of	O
sentence	O
pairs	O
labeled	O
for	O
entailment	O
,	O
contradiction	O
,	O
and	O
independence	O
.	O

We	O
used	O
this	O
corpus	O
to	O
evaluate	O
a	O
range	O
of	O
models	O
,	O
and	O
found	O
that	O
both	O
simple	O
lexicalized	Method
models	Method
and	O
neural	Method
network	Method
models	Method
perform	O
well	O
,	O
and	O
that	O
the	O
representations	O
learned	O
by	O
a	O
neural	Method
network	Method
model	Method
on	O
our	O
corpus	O
can	O
be	O
used	O
to	O
dramatically	O
improve	O
performance	O
on	O
a	O
standard	O
challenge	O
dataset	O
.	O

We	O
hope	O
that	O
SNLI	Material
presents	O
valuable	O
training	O
data	O
and	O
a	O
challenging	O
testbed	O
for	O
the	O
continued	O
application	O
of	O
machine	Task
learning	Task
to	O
semantic	Task
representation	Task
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
gratefully	O
acknowledge	O
support	O
from	O
a	O
Google	O
Faculty	O
Research	O
Award	O
,	O
a	O
gift	O
from	O
Bloomberg	O
L.P.	O
,	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Deep	Task
Exploration	Task
and	Task
Filtering	Task
of	Task
Text	Task
(	O
DEFT	Task
)	O
Program	O
under	O
Air	O
Force	O
Research	O
Laboratory	O
(	O
AFRL	O
)	O
contract	O
no	O
.	O

FA8750	O
-	O
13	O
-	O
2	O
-	O
0040	O
,	O
the	O
National	O
Science	O
Foundation	O
under	O
grant	O
no	O
.	O

IIS	O
1159679	O
,	O
and	O
the	O
Department	O
of	O
the	O
Navy	O
,	O
Office	O
of	O
Naval	O
Research	O
,	O
under	O
grant	O
no	O
.	O

N00014	O
-	O
10	O
-	O
1	O
-	O
0109	O
.	O

Any	O
opinions	O
,	O
findings	O
,	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
views	O
of	O
Google	O
,	O
Bloomberg	O
L.P.	O
,	O
DARPA	O
,	O
AFRL	O
NSF	O
,	O
ONR	O
,	O
or	O
the	O
US	O
government	O
.	O

We	O
also	O
thank	O
our	O
many	O
excellent	O
Mechanical	O
Turk	O
contributors	O
.	O

bibliography	O
:	O
References	O
