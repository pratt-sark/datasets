document	O
:	O
Deeply	Method
-	Method
Supervised	Method
Nets	Method
Our	O
proposed	O
deeply	Method
-	Method
supervised	Method
nets	Method
(	O
DSN	Method
)	O
method	O
simultaneously	O
minimizes	O
classification	Metric
error	Metric
while	O
making	O
the	O
learning	Method
process	Method
of	O
hidden	O
layers	O
direct	O
and	O
transparent	O
.	O

We	O
make	O
an	O
attempt	O
to	O
boost	O
the	O
classification	Task
performance	O
by	O
studying	O
a	O
new	O
formulation	O
in	O
deep	Method
networks	Method
.	O

Three	O
aspects	O
in	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	Method
style	Method
architectures	Method
are	O
being	O
looked	O
at	O
:	O
(	O
1	O
)	O
transparency	Method
of	O
the	O
intermediate	O
layers	O
to	O
the	O
overall	O
classification	Task
;	O
(	O
2	O
)	O
discriminativeness	Method
and	O
robustness	O
of	O
learned	O
features	O
,	O
especially	O
in	O
the	O
early	O
layers	O
;	O
(	O
3	O
)	O
effectiveness	Method
in	O
training	Task
due	O
to	O
the	O
presence	O
of	O
the	O
exploding	O
and	O
vanishing	O
gradients	O
.	O

We	O
introduce	O
“	O
companion	O
objective	O
”	O
to	O
the	O
individual	O
hidden	O
layers	O
,	O
in	O
addition	O
to	O
the	O
overall	O
objective	O
at	O
the	O
output	O
layer	O
(	O
a	O
different	O
strategy	O
to	O
layer	Method
-	Method
wise	Method
pre	Method
-	Method
training	Method
)	O
.	O

We	O
extend	O
techniques	O
from	O
stochastic	Method
gradient	Method
methods	Method
to	O
analyze	O
our	O
algorithm	O
.	O

The	O
advantage	O
of	O
our	O
method	O
is	O
evident	O
and	O
our	O
experimental	O
result	O
on	O
benchmark	O
datasets	O
shows	O
significant	O
performance	O
gain	O
over	O
existing	O
methods	O
(	O
e.g.	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
MNIST	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
SVHN	Material
)	O
.	O

section	O
:	O
Introduction	O
Much	O
attention	O
has	O
been	O
given	O
to	O
a	O
resurgence	O
of	O
neural	Method
networks	Method
,	O
deep	Method
learning	Method
(	Method
DL	Method
)	Method
in	O
particular	O
,	O
which	O
can	O
be	O
of	O
unsupervised	O
,	O
supervised	O
,	O
or	O
a	O
hybrid	Method
form	Method
.	O

Significant	O
performance	O
gain	O
has	O
been	O
observed	O
,	O
especially	O
in	O
the	O
presence	O
of	O
large	O
amount	O
of	O
training	O
data	O
,	O
when	O
deep	Method
learning	Method
techniques	Method
are	O
used	O
for	O
image	Task
classification	Task
and	O
speech	Task
recognition	Task
.	O

On	O
the	O
one	O
hand	O
,	O
hierarchical	Method
and	O
recursive	Method
networks	Method
have	O
demonstrated	O
great	O
promise	O
in	O
automatically	Task
learning	Task
thousands	Task
or	Task
even	Task
millions	Task
of	Task
features	Task
for	O
pattern	Task
recognition	Task
;	O
on	O
the	O
other	O
hand	O
concerns	O
about	O
deep	Task
learning	Task
have	O
been	O
raised	O
and	O
many	O
fundamental	O
questions	O
remain	O
open	O
.	O

Some	O
potential	O
problems	O
with	O
the	O
current	O
DL	Method
frameworks	Method
include	O
:	O
reduced	O
transparency	O
and	O
discriminativeness	O
of	O
the	O
features	O
learned	O
at	O
hidden	O
layers	O
;	O
training	O
difficulty	O
due	O
to	O
exploding	O
and	O
vanishing	O
gradients	O
;	O
lack	O
of	O
a	O
thorough	O
mathematical	O
understanding	O
about	O
the	O
algorithmic	O
behavior	O
,	O
despite	O
of	O
some	O
attempts	O
made	O
on	O
the	O
theoretical	O
side	O
;	O
dependence	O
on	O
the	O
availability	O
of	O
large	O
amount	O
of	O
training	O
data	O
;	O
complexity	O
of	O
manual	Method
tuning	Method
during	O
training	Task
.	O

Nevertheless	O
,	O
DL	Method
is	O
capable	O
of	O
automatically	O
learning	O
and	O
fusing	O
rich	O
hierarchical	Method
features	O
in	O
an	O
integrated	Method
framework	Method
.	O

Recent	O
activities	O
in	O
open	Task
-	Task
sourcing	Task
and	O
experience	Task
sharing	Task
have	O
also	O
greatly	O
helped	O
the	O
adopting	O
and	O
advancing	O
of	O
DL	Method
in	O
the	O
machine	Task
learning	Task
community	Task
and	O
beyond	O
.	O

Several	O
techniques	O
,	O
such	O
as	O
dropout	Method
,	O
dropconnect	Method
,	O
pre	Method
-	Method
training	Method
,	O
and	O
data	Method
augmentation	Method
,	O
have	O
been	O
proposed	O
to	O
enhance	O
the	O
performance	O
of	O
DL	Method
from	O
various	O
angles	O
,	O
in	O
addition	O
to	O
a	O
variety	O
of	O
engineering	Method
tricks	Method
used	O
to	O
fine	O
-	O
tune	O
feature	O
scale	O
,	O
step	O
size	O
,	O
and	O
convergence	Metric
rate	Metric
.	O

Features	O
learned	O
automatically	O
by	O
the	O
CNN	Method
algorithm	Method
are	O
intuitive	O
.	O

Some	O
portion	O
of	O
features	O
,	O
especially	O
for	O
those	O
in	O
the	O
early	O
layers	O
,	O
also	O
demonstrate	O
certain	O
degree	O
of	O
opacity	O
.	O

This	O
finding	O
is	O
also	O
consistent	O
with	O
an	O
observation	O
that	O
different	O
initializations	O
of	O
the	O
feature	Method
learning	Method
at	O
the	O
early	O
layers	O
make	O
negligible	O
difference	O
to	O
the	O
final	O
classification	Task
.	O

In	O
addition	O
,	O
the	O
presence	O
of	O
vanishing	O
gradients	O
also	O
makes	O
the	O
DL	Method
training	Method
slow	O
and	O
ineffective	O
.	O

In	O
this	O
paper	O
,	O
we	O
address	O
the	O
feature	Task
learning	Task
problem	Task
in	O
DL	Method
by	O
presenting	O
a	O
new	O
algorithm	O
,	O
deeply	Method
-	Method
supervised	Method
nets	Method
(	O
DSN	Method
)	O
,	O
which	O
enforces	O
direct	O
and	O
early	O
supervision	O
for	O
both	O
the	O
hidden	O
layers	O
and	O
the	O
output	O
layer	O
.	O

We	O
introduce	O
companion	O
objective	O
to	O
the	O
individual	O
hidden	O
layers	O
,	O
which	O
is	O
used	O
as	O
an	O
additional	O
constraint	O
(	O
or	O
a	O
new	O
regularization	O
)	O
to	O
the	O
learning	Method
process	Method
.	O

Our	O
new	O
formulation	O
significantly	O
enhances	O
the	O
performance	O
of	O
existing	O
supervised	Method
DL	Method
methods	Method
.	O

We	O
also	O
make	O
an	O
attempt	O
to	O
provide	O
justification	O
for	O
our	O
formulation	O
using	O
stochastic	Method
gradient	Method
techniques	Method
.	O

We	O
show	O
an	O
improvement	O
of	O
the	O
convergence	Metric
rate	Metric
of	O
the	O
proposed	O
method	O
over	O
standard	O
ones	O
,	O
assuming	O
local	O
strong	O
convexity	O
of	O
the	O
optimization	O
function	O
(	O
a	O
very	O
loose	O
assumption	O
but	O
pointing	O
to	O
a	O
promising	O
direction	O
)	O
.	O

Several	O
existing	O
approaches	O
are	O
particularly	O
worth	O
mentioning	O
and	O
comparing	O
with	O
.	O

In	O
,	O
layer	Method
-	Method
wise	Method
supervised	Method
pre	Method
-	Method
training	Method
is	O
performed	O
.	O

Our	O
proposed	O
method	O
does	O
not	O
perform	O
pre	Method
-	Method
training	Method
and	O
it	O
emphasizes	O
the	O
importance	O
of	O
minimizing	O
the	O
output	O
classification	Metric
error	Metric
while	O
reducing	O
the	O
prediction	Metric
error	Metric
of	O
each	O
individual	O
layer	O
.	O

This	O
is	O
important	O
as	O
the	O
backpropagation	Method
is	O
performed	O
altogether	O
in	O
an	O
integrated	Method
framework	Method
.	O

In	O
,	O
label	O
information	O
is	O
used	O
for	O
unsupervised	Task
learning	Task
.	O

Semi	Task
-	Task
supervised	Task
learning	Task
is	O
carried	O
in	O
deep	Method
learning	Method
.	O

In	O
,	O
an	O
SVM	Method
classifier	Method
is	O
used	O
for	O
the	O
output	O
layer	O
,	O
instead	O
of	O
the	O
standard	O
softmax	O
function	O
in	O
the	O
CNN	Method
.	O

Our	O
framework	O
(	O
DSN	Method
)	O
,	O
with	O
the	O
choice	O
of	O
using	O
SVM	Method
,	O
softmax	O
or	O
other	O
classifiers	Method
,	O
emphasizes	O
the	O
direct	O
supervision	O
of	O
each	O
intermediate	O
layer	O
.	O

In	O
the	O
experiments	O
,	O
we	O
show	O
consistent	O
improvement	O
of	O
DSN	Method
-	O
SVM	O
and	O
DSN	Method
-	O
Softmax	O
over	O
CNN	Method
-	Method
SVM	Method
and	O
CNN	Method
-	Method
Softmax	Method
respectively	O
.	O

We	O
observe	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
MNIST	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
SVHN	Material
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
our	O
formulation	O
is	O
inclusive	O
to	O
various	O
techniques	O
proposed	O
recently	O
such	O
as	O
averaging	Method
,	O
dropconnect	Method
,	O
and	O
Maxout	Method
.	O

We	O
expect	O
to	O
see	O
more	O
classification	Metric
error	Metric
reduction	O
with	O
careful	O
engineering	O
for	O
DSN	Method
.	O

section	O
:	O
Deeply	Method
-	Method
Supervised	Method
Nets	Method
In	O
this	O
section	O
,	O
we	O
give	O
the	O
main	O
formulation	O
of	O
the	O
proposed	O
deeply	Method
-	Method
supervised	Method
nets	Method
(	O
DSN	Method
)	O
.	O

We	O
focus	O
on	O
building	O
our	O
infrastructure	O
around	O
supervised	Method
CNN	Method
style	Method
frameworks	Method
by	O
introducing	O
classifier	Method
,	O
e.g.	O
SVM	Method
model	Method
,	O
to	O
each	O
layer	O
.	O

An	O
early	O
attempt	O
to	O
combine	O
SVM	Method
with	O
DL	Method
was	O
made	O
in	O
,	O
which	O
however	O
has	O
a	O
different	O
motivation	O
with	O
ours	O
and	O
only	O
studies	O
the	O
output	O
layer	O
with	O
some	O
preliminary	O
experimental	O
results	O
.	O

subsection	O
:	O
Motivation	O
We	O
are	O
motivated	O
by	O
the	O
following	O
simple	O
observation	O
:	O
in	O
general	O
,	O
a	O
discriminative	Method
classifier	Method
trained	O
on	O
highly	O
discriminative	O
features	O
will	O
display	O
better	O
performance	O
than	O
a	O
discriminative	Method
classifier	Method
trained	O
on	O
less	O
discriminative	O
features	O
.	O

If	O
the	O
features	O
in	O
question	O
are	O
the	O
hidden	O
layer	O
feature	O
maps	O
of	O
a	O
deep	Method
network	Method
,	O
this	O
observation	O
means	O
that	O
the	O
performance	O
of	O
a	O
discriminative	Method
classifier	Method
trained	O
using	O
these	O
hidden	O
layer	O
feature	O
maps	O
can	O
serve	O
as	O
a	O
proxy	O
for	O
the	O
quality	O
/	O
discriminativeness	O
of	O
those	O
hidden	O
layer	O
feature	O
maps	O
,	O
and	O
further	O
to	O
the	O
quality	O
of	O
the	O
upper	O
layer	O
feature	O
maps	O
.	O

By	O
making	O
appropriate	O
use	O
of	O
this	O
feature	O
quality	O
feedback	O
at	O
each	O
hidden	O
layer	O
of	O
the	O
network	O
,	O
we	O
are	O
able	O
to	O
directly	O
influence	O
the	O
hidden	Method
layer	Method
weight	Method
/	Method
filter	Method
update	Method
process	Method
to	O
favor	O
highly	O
discriminative	O
feature	O
maps	O
.	O

This	O
is	O
a	O
source	O
of	O
supervision	O
that	O
acts	O
deep	O
within	O
the	O
network	O
at	O
each	O
layer	O
;	O
when	O
our	O
proxy	O
for	O
feature	Metric
quality	Metric
is	O
good	O
,	O
we	O
expect	O
to	O
much	O
more	O
rapidly	O
approach	O
the	O
region	O
of	O
good	O
features	O
than	O
would	O
be	O
the	O
case	O
if	O
we	O
had	O
to	O
rely	O
on	O
the	O
gradual	Method
backpropagation	Method
from	O
the	O
output	Method
layer	Method
alone	O
.	O

We	O
also	O
expect	O
to	O
alleviate	O
the	O
common	O
problem	O
of	O
having	O
gradients	O
that	O
“	O
explode	O
”	O
or	O
“	O
vanish	O
”	O
.	O

One	O
concern	O
with	O
a	O
direct	O
pursuit	O
of	O
feature	O
discriminativeness	O
at	O
all	O
hidden	O
layers	O
is	O
that	O
this	O
might	O
interfere	O
with	O
the	O
overall	O
network	O
performance	O
,	O
since	O
it	O
is	O
ultimately	O
the	O
feature	O
maps	O
at	O
the	O
output	O
layer	O
which	O
are	O
used	O
for	O
the	O
final	O
classification	Task
;	O
our	O
experimental	O
results	O
indicate	O
that	O
this	O
is	O
not	O
the	O
case	O
.	O

Our	O
basic	O
network	Method
architecture	Method
will	O
be	O
similar	O
to	O
the	O
standard	O
one	O
used	O
in	O
the	O
CNN	Method
framework	Method
.	O

Our	O
additional	O
deep	O
feedback	O
is	O
brought	O
in	O
by	O
associating	O
a	O
companion	O
local	O
output	O
with	O
each	O
hidden	Method
layer	Method
.	O

We	O
may	O
think	O
of	O
this	O
companion	O
local	O
output	O
as	O
analogous	O
to	O
the	O
final	O
output	O
that	O
a	O
truncated	Method
network	Method
would	O
have	O
produced	O
.	O

Backpropagation	O
of	O
error	Metric
now	O
proceeds	O
as	O
usual	O
,	O
with	O
the	O
crucial	O
difference	O
that	O
we	O
now	O
backpropagate	O
not	O
only	O
from	O
the	O
final	O
layer	O
but	O
also	O
simultaneously	O
from	O
our	O
local	O
companion	O
output	O
.	O

The	O
empirical	O
result	O
suggests	O
the	O
following	O
main	O
properties	O
of	O
the	O
companion	Method
objective	Method
:	O
(	O
1	O
)	O
it	O
acts	O
as	O
a	O
kind	O
of	O
feature	Method
regularization	Method
(	O
although	O
an	O
unusual	O
one	O
)	O
,	O
which	O
leads	O
to	O
significant	O
reduction	O
to	O
the	O
testing	Metric
error	Metric
but	O
not	O
necessarily	O
to	O
the	O
train	Metric
error	Metric
;	O
(	O
2	O
)	O
it	O
results	O
in	O
faster	O
convergence	Metric
,	O
especially	O
in	O
presence	O
of	O
small	O
training	O
data	O
(	O
see	O
Figure	O
(	O
[	O
reference	O
]	O
)	O
for	O
an	O
illustration	O
on	O
a	O
running	O
example	O
)	O
.	O

subsection	O
:	O
Formulation	O
We	O
focus	O
on	O
the	O
supervised	Task
learning	Task
case	Task
and	O
let	O
be	O
our	O
set	O
of	O
input	O
training	O
data	O
where	O
sample	O
denotes	O
the	O
raw	O
input	O
data	O
and	O
is	O
the	O
corresponding	O
groundtruth	O
label	O
for	O
sample	O
.	O

We	O
drop	O
for	O
notational	O
simplicity	O
,	O
since	O
each	O
sample	O
is	O
considered	O
independently	O
.	O

The	O
goal	O
of	O
deep	Method
nets	Method
,	O
specifically	O
convolutional	Method
neural	Method
networks	Method
(	Method
CNN	Method
)	Method
,	O
is	O
to	O
learn	O
layers	O
of	O
filters	O
and	O
weights	O
for	O
the	O
minimization	O
of	O
classification	Metric
error	Metric
at	O
the	O
output	O
layer	O
.	O

Here	O
,	O
we	O
absorb	O
the	O
bias	O
term	O
into	O
the	O
weight	O
parameters	O
and	O
do	O
not	O
differentiate	O
weights	O
from	O
filters	O
and	O
denote	O
a	O
recursive	Method
function	Method
for	O
each	O
layer	O
as	O
:	O
denotes	O
the	O
total	O
number	O
of	O
layers	O
;	O
are	O
the	O
filters	O
/	O
weights	O
to	O
be	O
learned	O
;	O
is	O
the	O
feature	O
map	O
produced	O
at	O
layer	O
;	O
refers	O
to	O
the	O
convolved	O
/	O
filtered	O
responses	O
on	O
the	O
previous	O
feature	O
map	O
;	O
is	O
a	O
pooling	O
function	O
on	O
;	O
Combining	O
all	O
layers	O
of	O
weights	O
gives	O
Now	O
we	O
introduce	O
a	O
set	O
of	O
classifiers	Method
,	O
e.g.	O
SVM	Method
(	O
other	O
classifiers	O
like	O
Softmax	Method
can	O
be	O
applied	O
and	O
we	O
will	O
show	O
results	O
using	O
both	O
SVM	Method
and	O
Softmax	Method
in	O
the	O
experiments	O
)	O
,	O
one	O
for	O
each	O
hidden	O
layer	O
,	O
in	O
addition	O
to	O
the	O
in	O
the	O
standard	O
CNN	Method
framework	Method
.	O

We	O
denote	O
the	O
as	O
the	O
SVM	O
weights	O
for	O
the	O
output	O
layer	O
.	O

Thus	O
,	O
we	O
build	O
our	O
overall	O
combined	Metric
objective	Metric
function	Metric
as	O
:	O
where	O
and	O
We	O
name	O
as	O
the	O
overall	Metric
loss	Metric
(	O
output	O
layer	O
)	O
and	O
as	O
the	O
companion	O
loss	O
(	O
hidden	O
layers	O
)	O
,	O
which	O
are	O
both	O
squared	O
hinge	O
losses	O
of	O
the	O
prediction	O
errors	O
.	O

The	O
above	O
formulation	O
can	O
be	O
understood	O
intuitively	O
:	O
in	O
addition	O
to	O
learning	O
convolution	O
kernels	O
and	O
weights	O
,	O
,	O
as	O
in	O
the	O
standard	O
CNN	Method
model	Method
,	O
enforcing	O
a	O
constraint	O
at	O
each	O
hidden	O
layer	O
for	O
directly	O
making	O
a	O
good	O
label	Task
prediction	Task
gives	O
a	O
strong	O
push	O
for	O
having	O
discriminative	O
and	O
sensible	O
features	O
at	O
each	O
individual	O
layer	O
.	O

In	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
are	O
respectively	O
the	O
margin	O
and	O
squared	O
hinge	O
loss	O
of	O
the	O
SVM	Method
classifier	Method
(	O
L2SVM	Method
)	O
at	O
the	O
output	O
layer	O
(	O
we	O
omit	O
the	O
balance	O
term	O
in	O
front	O
of	O
the	O
hinge	O
for	O
notational	O
simplicity	O
)	O
;	O
in	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
are	O
respectively	O
the	O
margin	O
and	O
squared	O
hinge	O
loss	O
of	O
the	O
SVM	Method
classifier	Method
at	O
each	O
hidden	O
layer	O
.	O

Note	O
that	O
for	O
each	O
,	O
the	O
directly	O
depends	O
on	O
,	O
which	O
is	O
dependent	O
on	O
up	O
to	O
the	O
th	O
layer	O
.	O

depends	O
on	O
,	O
which	O
is	O
decided	O
by	O
the	O
entire	O
.	O

The	O
second	O
term	O
in	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
often	O
goes	O
to	O
zero	O
during	O
the	O
course	O
of	O
training	O
;	O
this	O
way	O
,	O
the	O
overall	O
goal	O
of	O
producing	O
good	O
classification	Metric
of	O
the	O
output	O
layer	O
is	O
not	O
altered	O
and	O
the	O
companion	O
objective	O
just	O
acts	O
as	O
a	O
proxy	O
or	O
regularization	O
.	O

This	O
is	O
achieved	O
by	O
having	O
as	O
a	O
threshold	O
(	O
a	O
hyper	O
parameter	O
)	O
in	O
the	O
second	O
term	O
of	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
with	O
a	O
hinge	O
loss	O
:	O
once	O
the	O
overall	O
value	O
of	O
the	O
hidden	O
layer	O
reaches	O
or	O
is	O
below	O
,	O
it	O
vanishes	O
and	O
no	O
longer	O
plays	O
role	O
in	O
the	O
learning	Task
process	Task
.	O

balances	O
the	O
importance	O
of	O
the	O
error	Metric
in	O
the	O
output	Metric
objective	Metric
and	O
the	O
companion	Metric
objective	Metric
.	O

In	O
addition	O
,	O
we	O
could	O
use	O
a	O
simple	O
decay	O
function	O
as	O
to	O
enforce	O
the	O
second	O
term	O
to	O
vanish	O
after	O
certain	O
number	O
of	O
iterations	O
,	O
where	O
is	O
the	O
epoch	O
step	O
and	O
is	O
the	O
total	O
number	O
of	O
epochs	O
(	O
wheather	O
or	O
not	O
to	O
have	O
the	O
decay	O
on	O
might	O
vary	O
in	O
different	O
experiments	O
although	O
the	O
differences	O
may	O
not	O
be	O
very	O
big	O
)	O
.	O

To	O
summarize	O
,	O
we	O
describe	O
this	O
optimization	Task
problem	Task
as	O
follows	O
:	O
we	O
want	O
to	O
learn	O
filters	O
/	O
weights	O
for	O
the	O
entire	O
network	O
such	O
that	O
an	O
SVM	Method
classifier	Method
trained	O
on	O
the	O
output	O
feature	O
maps	O
(	O
that	O
depend	O
on	O
those	O
filters	O
/	O
features	O
)	O
will	O
display	O
good	O
performance	O
.	O

We	O
seek	O
this	O
output	O
performance	O
while	O
also	O
requiring	O
some	O
“	O
satisfactory	O
”	O
level	O
of	O
performance	O
on	O
the	O
part	O
of	O
the	O
hidden	Method
layer	Method
classifiers	Method
.	O

We	O
are	O
saying	O
:	O
restrict	O
attention	O
to	O
the	O
parts	O
of	O
feature	O
space	O
that	O
,	O
when	O
considered	O
at	O
the	O
internal	O
layers	O
,	O
lead	O
to	O
highly	O
discriminative	O
hidden	O
layer	O
feature	O
maps	O
(	O
as	O
measured	O
via	O
our	O
proxy	O
of	O
hidden	Method
-	Method
layer	Method
classifier	Method
performance	O
)	O
.	O

The	O
main	O
difference	O
between	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
previous	O
attempts	O
in	O
layer	Task
-	Task
wise	Task
supervised	Task
training	Task
is	O
that	O
we	O
perform	O
the	O
optimization	Task
altogether	O
with	O
a	O
robust	Metric
measure	Metric
(	O
or	O
regularization	O
)	O
of	O
the	O
hidden	O
layer	O
.	O

For	O
example	O
,	O
greedy	Method
layer	Method
-	Method
wise	Method
pretraining	Method
was	O
performed	O
as	O
either	O
initialization	O
or	O
fine	Method
-	Method
tuning	Method
which	O
results	O
in	O
some	O
overfitting	O
.	O

The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
benchmark	O
results	O
demonstrate	O
the	O
particular	O
advantage	O
of	O
our	O
formulation	O
.	O

As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
,	O
indeed	O
both	O
CNN	Method
and	O
DSN	Method
reach	O
training	Metric
error	Metric
near	O
zero	O
but	O
DSN	Method
demonstrates	O
a	O
clear	O
advantage	O
of	O
having	O
a	O
better	O
generalization	Metric
capability	Metric
.	O

To	O
train	O
the	O
DSN	Method
model	O
using	O
SGD	Method
,	O
the	O
gradients	O
of	O
the	O
objective	O
function	O
w.r.t	O
the	O
parameters	O
in	O
the	O
model	O
are	O
:	O
The	O
gradient	O
w.r.t	O
just	O
follows	O
the	O
conventional	O
CNN	Method
based	Method
model	Method
plus	O
the	O
gradient	O
that	O
directly	O
comes	O
from	O
the	O
hidden	O
layer	O
supervision	O
.	O

Next	O
,	O
we	O
provide	O
more	O
discussions	O
to	O
and	O
try	O
to	O
understand	O
intuitively	O
about	O
our	O
formulation	O
,	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

For	O
ease	O
of	O
reference	O
,	O
we	O
write	O
this	O
objective	O
function	O
as	O
where	O
and	O
.	O

subsection	O
:	O
Stochastic	Method
Gradient	Method
Descent	Method
View	Method
We	O
focus	O
on	O
the	O
convergence	Metric
advantage	O
of	O
DSN	Method
,	O
instead	O
of	O
the	O
regularization	Method
to	O
the	O
generalization	Task
aspect	Task
.	O

In	O
addition	O
to	O
the	O
present	O
problem	O
in	O
CNN	Task
where	O
learned	O
features	O
are	O
not	O
always	O
intuitive	O
and	O
discriminative	O
,	O
the	O
difficulty	O
of	O
training	O
deep	Method
neural	Method
networks	Method
has	O
been	O
discussed	O
.	O

As	O
we	O
can	O
observe	O
from	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
change	O
of	O
the	O
bottom	O
layer	O
weights	O
get	O
propagated	O
through	O
layers	O
of	O
functions	O
,	O
leading	O
to	O
exploding	O
or	O
vanishing	O
gradients	O
.	O

Various	O
techniques	O
and	O
parameter	Method
tuning	Method
tricks	Method
have	O
been	O
proposed	O
to	O
better	O
train	O
deep	Method
neural	Method
networks	Method
,	O
such	O
as	O
pre	Method
-	Method
training	Method
and	O
dropout	Method
.	O

Here	O
we	O
provide	O
a	O
somewhat	O
loose	O
analysis	O
to	O
our	O
proposed	O
formulation	O
,	O
in	O
a	O
hope	O
to	O
understand	O
its	O
advantage	O
in	O
effectiveness	O
.	O

The	O
objective	O
function	O
in	O
deep	Method
neural	Method
networks	Method
is	O
highly	O
non	O
-	O
convex	O
.	O

Here	O
we	O
make	O
the	O
following	O
assumptions	O
/	O
observations	O
:	O
(	O
1	O
)	O
the	O
objective	Method
/	Method
energy	Method
function	Method
of	O
DL	Method
observes	O
a	O
large	O
“	O
flat	O
”	O
area	O
around	O
the	O
“	O
optimal	O
”	O
solution	O
where	O
any	O
result	O
has	O
a	O
similar	O
performance	O
;	O
locally	O
we	O
still	O
assume	O
a	O
convex	O
(	O
or	O
even	O
-	O
strongly	O
convex	O
)	O
function	O
whose	O
optimization	Task
is	O
often	O
performed	O
with	O
stochastic	Method
gradient	Method
descent	Method
algorithm	Method
.	O

The	O
definition	O
of	O
-	O
strongly	O
convex	O
is	O
standard	O
:	O
A	O
function	O
is	O
-	O
strongly	O
convex	O
if	O
and	O
any	O
subgradient	O
at	O
,	O
and	O
the	O
update	Method
rule	Method
in	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
at	O
step	O
is	O
,	O
where	O
refers	O
to	O
the	O
step	O
rate	O
and	O
helps	O
to	O
project	O
onto	O
the	O
space	O
of	O
.	O

Let	O
be	O
the	O
optimum	O
solution	O
,	O
upper	O
bounds	O
for	O
and	O
in	O
for	O
the	O
strongly	O
convex	O
function	O
,	O
and	O
for	O
convex	O
function	O
in	O
.	O

Here	O
we	O
make	O
an	O
attempt	O
to	O
understand	O
the	O
convergence	O
of	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
w.r.t	O
.	O

,	O
due	O
to	O
the	O
presence	O
of	O
large	O
area	O
of	O
flat	O
function	O
shown	O
in	O
Figure	O
(	O
[	O
reference	O
]	O
.b	O
)	O
.	O

In	O
,	O
a	O
convergence	Metric
rate	Metric
is	O
given	O
for	O
the	O
M	Method
-	Method
estimators	Method
with	O
locally	Method
convex	Method
function	Method
with	O
compositional	O
loss	O
and	O
regularization	O
terms	O
.	O

Both	O
terms	O
in	O
eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
here	O
refer	O
to	O
the	O
same	O
class	O
label	O
prediction	Metric
error	Metric
,	O
a	O
reason	O
for	O
calling	O
the	O
second	O
term	O
as	O
companion	O
objective	O
.	O

Our	O
motivation	O
is	O
two	O
-	O
fold	O
:	O
(	O
1	O
)	O
encourage	O
the	O
features	O
learned	O
at	O
each	O
layer	O
to	O
be	O
directly	O
discriminative	O
for	O
class	Task
label	Task
prediction	Task
,	O
while	O
keeping	O
the	O
ultimate	O
goal	O
of	O
minimizing	Task
class	Task
label	Task
prediction	Task
at	O
the	O
output	O
layer	O
;	O
(	O
2	O
)	O
alleviate	O
the	O
exploding	Task
and	Task
vanishing	Task
gradients	Task
problem	Task
as	O
each	O
layer	O
now	O
has	O
a	O
direct	O
supervision	O
from	O
the	O
ground	O
truth	O
labels	O
.	O

One	O
might	O
raise	O
a	O
concern	O
that	O
learning	O
highly	O
discriminative	O
intermediate	O
stage	O
filters	O
may	O
not	O
necessarily	O
lead	O
to	O
the	O
best	O
prediction	O
at	O
the	O
output	O
layer	O
.	O

An	O
illustration	O
can	O
been	O
seen	O
in	O
Figure	O
(	O
[	O
reference	O
]	O
.b	O
)	O
.	O

Next	O
,	O
we	O
give	O
a	O
loose	O
theoretical	O
analysis	O
to	O
our	O
framework	O
,	O
which	O
is	O
also	O
validated	O
by	O
comprehensive	O
experimental	O
studies	O
with	O
overwhelming	O
advantages	O
over	O
the	O
existing	O
methods	O
.	O

We	O
name	O
as	O
the	O
-	O
feasible	O
set	O
for	O
a	O
function	O
.	O

First	O
we	O
show	O
that	O
a	O
feasible	O
solution	O
for	O
leads	O
to	O
a	O
feasible	O
one	O
to	O
.	O

That	O
is	O
:	O
theorem	O
:	O
if	O
∥w	O
(	O
m	O
)	O
∥2	O
+	O
ℓ	O
((	O
^W	O
(	O
1	O
)	O
,	O
..	O
,	O
^W	O
(	O
m	O
)),	O
w	O
(	O
m	O
))	O
≤γ	O
then	O
there	O
exists	O
(	O
^W	O
(	O
1	O
)	O
,	O
..	O
,	O
^W	O
(	O
m	O
)	O
,	O
..	O
,	O
^W	O
(	O
m′	O
)	O
)	O
such	O
that	O
∥w	O
(	O
m′	O
)	O
∥2	O
+	O
ℓ	O
((	O
^W	O
(	O
1	O
)	O
,	O
..	O
,	O
^W	O
(	O
m	O
)	O
..	O
,	O
^W	O
(	O
m′	O
)),	O
w	O
(	O
m′	O
))	O
≤γ	O
.	O

Note	O
that	O
we	O
drop	O
the	O
>	O
W	O
(	O
j	O
),	O
jm	O
since	O
the	O
filters	O
above	O
layer	O
m	O
do	O
not	O
participate	O
in	O
the	O
computation	O
for	O
the	O
objective	O
function	O
of	O
this	O
layer	O
.	O

As	O
we	O
can	O
see	O
from	O
an	O
illustration	O
of	O
our	O
network	Method
architecture	Method
shown	O
in	O
fig	O
.	O

(	O
[	O
reference	O
]	O
.a	O
)	O
,	O
for	O
such	O
that	O
.	O

Then	O
there	O
is	O
a	O
trivial	O
solution	O
for	O
the	O
network	O
for	O
every	O
layer	O
up	O
to	O
,	O
we	O
let	O
and	O
,	O
meaning	O
that	O
the	O
filters	O
will	O
be	O
identity	O
matrices	O
.	O

This	O
results	O
in	O
.	O

Lemma	O
[	O
reference	O
]	O
shows	O
that	O
a	O
good	O
solution	O
for	O
is	O
also	O
a	O
good	O
one	O
for	O
,	O
but	O
it	O
may	O
not	O
be	O
the	O
case	O
the	O
other	O
way	O
around	O
.	O

That	O
is	O
:	O
a	O
that	O
makes	O
small	O
may	O
not	O
necessarily	O
produce	O
discriminative	O
features	O
for	O
the	O
hidden	O
layers	O
to	O
have	O
a	O
small	O
.	O

However	O
,	O
can	O
be	O
viewed	O
as	O
a	O
regularization	Method
term	Method
.	O

Since	O
observes	O
a	O
very	O
flat	O
area	O
near	O
even	O
zero	O
on	O
the	O
training	O
data	O
and	O
it	O
is	O
ultimately	O
the	O
test	Metric
error	Metric
that	O
we	O
really	O
care	O
about	O
,	O
we	O
thus	O
only	O
focus	O
on	O
the	O
,	O
,	O
which	O
makes	O
both	O
and	O
small	O
.	O

Therefore	O
,	O
it	O
is	O
not	O
unreasonable	O
to	O
assume	O
that	O
and	O
share	O
the	O
same	O
optimal	O
.	O

Let	O
and	O
be	O
strongly	O
convex	O
around	O
,	O
and	O
,	O
with	O
and	O
,	O
where	O
and	O
are	O
the	O
subgradients	O
for	O
and	O
at	O
respectively	O
.	O

It	O
can	O
be	O
directly	O
seen	O
that	O
is	O
also	O
strongly	O
convex	O
and	O
for	O
subgradient	O
of	O
at	O
,	O
.	O

theorem	O
:	O
Suppose	O
≤⁢E	O
[	O
∥^gpt∥2	O
]	O
G2	O
and	O
≤⁢E	O
[	O
∥^gqt∥2	O
]	O
G2	O
,	O
and	O
we	O
use	O
the	O
update	O
rule	O
of	O
=	O
W	O
+	O
t1⁢ΠW	O
(-	O
Wt⁢ηt	O
(+	O
^gpt^gqt	O
)	O
)	O
where	O
=	O
⁢E	O
[	O
^gpt	O
]	O
gpt	O
and	O
=	O
⁢E	O
[	O
^gqt	O
]	O
gqt	O
.	O

If	O
we	O
use	O
=	O
ηt⁢	O
/	O
1	O
(+	O
λ1λ2	O
)	O
t	O
,	O
then	O
at	O
time	O
stamp	O
T	O
Since	O
,	O
it	O
can	O
be	O
directly	O
seen	O
that	O
Based	O
on	O
lemma	O
1	O
in	O
,	O
this	O
upper	O
bound	O
directly	O
holds	O
.	O

theorem	O
:	O
Following	O
the	O
assumptions	O
in	O
lemma	O
,	O
but	O
now	O
we	O
assume	O
=	O
ηt	O
/	O
1	O
t	O
since	O
λ1	O
and	O
λ2	O
are	O
not	O
always	O
readily	O
available	O
,	O
then	O
started	O
from	O
≤∥	O
-	O
W1W⋆∥2D	O
the	O
convergence	Metric
rate	Metric
is	O
bounded	O
by	O
Let	O
,	O
we	O
have	O
Thus	O
,	O
Therefore	O
,	O
with	O
,	O
With	O
being	O
small	O
,	O
we	O
have	O
theorem	O
:	O
Let	O
⁢P	O
(	O
W	O
)	O
be	O
λ1	O
-	O
strongly	O
convex	O
and	O
⁢Q	O
(	O
W	O
)	O
be	O
λ2	O
-	O
strongly	O
convex	O
near	O
optimal	O
W⋆	O
and	O
denote	O
WT	Method
(	Method
F	Method
)	Method
and	O
WT	Method
(	Method
P	Method
)	Method
as	O
the	O
solution	O
after	O
T	O
iterations	O
when	O
applying	O
SGD	Method
on	O
⁢F	O
(	O
W	O
)	O
and	O
⁢P	O
(	O
W	O
)	O
respectively	O
.	O

Then	O
our	O
deeply	Method
supervised	Method
framework	Method
in	O
eqn	O
.	O

(	O
)	O
improves	O
the	O
the	O
speed	O
over	O
using	O
top	O
layer	O
only	O
by	O
=	O
⁢E	O
[	O
∥	O
-	O
WT	O
(	O
P	O
)	O
W⋆∥2	O
]	O
⁢E	O
[	O
∥	O
-	O
WT	O
(	O
F	O
)	O
W⋆∥2	O
]	O
⁢Θ	O
(+	O
1λ22λ12	O
),	O
=⁢whenηt⁢	O
/	O
1λt	O
,	O
⁢and	O
=	O
⁢E	O
[	O
∥	O
-	O
WT	O
(	O
P	O
)	O
W⋆∥2	O
]	O
⁢E	O
[	O
∥	O
-	O
WT	O
(	O
F	O
)	O
W⋆∥2	O
]	O
⁢Θ	O
(	O
e⁢ln	O
(	O
T	O
)	O
λ2	O
),	O
=⁢whenηt	O
/	O
1	O
t	O
Lemma	O
[	O
reference	O
]	O
shows	O
the	O
compatibility	O
of	O
the	O
companion	O
objective	O
of	O
w.r.t	O
the	O
output	Metric
objective	Metric
.	O

The	O
first	O
equation	O
can	O
be	O
directly	O
derived	O
from	O
lemma	O
[	O
reference	O
]	O
and	O
the	O
second	O
equation	O
can	O
be	O
seen	O
from	O
lemma	O
[	O
reference	O
]	O
.	O

In	O
general	O
which	O
leads	O
to	O
a	O
great	O
improvement	O
in	O
convergence	Metric
speed	Metric
and	O
the	O
constraints	O
in	O
each	O
hidden	O
layer	O
also	O
helps	O
to	O
learning	O
filters	O
which	O
are	O
directly	O
discriminative	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
DSN	Method
method	O
on	O
four	O
standard	O
benchmark	O
datasets	O
:	O
MNIST	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
and	O
SVHN	Material
.	O

We	O
follow	O
a	O
common	O
training	O
protocol	O
used	O
by	O
Krizhevsky	O
et	O
al	O
.	O

in	O
all	O
experiments	O
.	O

We	O
use	O
SGD	Method
solver	Method
with	O
mini	O
-	O
batch	O
size	O
of	O
at	O
a	O
fixed	O
constant	O
momentum	O
value	O
of	O
.	O

Initial	O
value	O
for	O
learning	Metric
rate	Metric
and	O
weight	Metric
decay	Metric
factor	Metric
is	O
determined	O
based	O
on	O
the	O
validation	O
set	O
.	O

For	O
a	O
fair	O
comparison	O
and	O
clear	O
illustration	O
of	O
the	O
effectiveness	O
of	O
DSN	Method
,	O
we	O
match	O
the	O
complexity	Metric
of	O
our	O
model	O
with	O
that	O
in	O
network	Method
architectures	Method
used	O
in	O
and	O
to	O
have	O
a	O
comparable	O
number	O
of	O
parameters	O
.	O

We	O
also	O
incorporate	O
two	O
dropout	Method
layers	Method
with	O
dropout	O
rate	O
at	O
.	O

Companion	O
objective	O
at	O
the	O
convolutional	Method
layers	Method
is	O
imposed	O
to	O
backpropagate	O
the	O
classification	Metric
error	Metric
guidance	O
to	O
the	O
underlying	O
convolutional	Method
layers	Method
.	O

Learning	Metric
rates	Metric
are	O
annealed	O
during	O
training	O
by	O
a	O
factor	O
of	O
according	O
to	O
an	O
epoch	O
schedule	O
determined	O
on	O
the	O
validation	O
set	O
.	O

The	O
proposed	O
DSN	Method
framework	O
is	O
not	O
difficult	O
to	O
train	O
and	O
there	O
are	O
no	O
particular	O
engineering	O
tricks	O
adopted	O
.	O

Our	O
system	O
is	O
built	O
on	O
top	O
of	O
widely	O
used	O
Caffe	Method
infrastructure	Method
.	O

For	O
the	O
network	Method
architecture	Method
setup	O
,	O
we	O
adopted	O
the	O
mlpconv	Method
layer	Method
and	O
global	Method
averaged	Method
pooling	Method
scheme	Method
introduced	O
in	O
.	O

DSN	Method
can	O
be	O
equipped	O
with	O
different	O
types	O
of	O
loss	O
functions	O
,	O
such	O
as	O
Softmax	Method
and	O
SVM	Method
.	O

We	O
show	O
performance	O
boost	O
of	O
DSN	Method
-	O
SVM	O
and	O
DSN	Method
-	O
Softmax	O
over	O
CNN	Method
-	Method
SVM	Method
and	O
CNN	Method
-	Method
Softmax	Method
respectively	O
(	O
see	O
Figure	O
(	O
[	O
reference	O
]	O
.a	O
)	O
)	O
.	O

The	O
performance	O
gain	O
is	O
more	O
evident	O
in	O
presence	O
of	O
small	O
training	O
data	O
(	O
see	O
Figure	O
(	O
[	O
reference	O
]	O
.b	O
)	O
)	O
;	O
this	O
might	O
partially	O
ease	O
the	O
burden	O
of	O
requiring	O
large	O
training	O
data	O
for	O
DL	Method
.	O

Overall	O
,	O
we	O
observe	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
classification	Metric
error	Metric
in	O
all	O
four	O
datasets	O
(	O
without	O
data	O
augmentation	O
)	O
,	O
for	O
MINIST	Material
,	O
for	O
CIFAR	Material
-	Material
10	Material
,	O
for	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
for	O
SVHN	Material
(	O
for	O
CIFAR	Material
-	Material
10	Material
with	O
data	Method
augmentation	Method
)	O
.	O

All	O
results	O
are	O
achieved	O
without	O
using	O
averaging	Method
,	O
which	O
is	O
not	O
exclusive	O
to	O
our	O
method	O
.	O

Figure	O
(	O
[	O
reference	O
]	O
)	O
gives	O
an	O
illustration	O
of	O
some	O
learned	O
features	O
.	O

subsection	O
:	O
MNIST	Material
We	O
first	O
validate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
DSN	Method
on	O
the	O
MNIST	Material
handwritten	O
digits	O
classification	O
task	O
,	O
a	O
widely	O
and	O
extensively	O
adopted	O
benchmark	O
in	O
machine	Method
learning	Method
.	O

MNIST	Material
dataset	O
consists	O
of	O
images	O
of	O
10	O
different	O
classes	O
(	O
0	O
to	O
9	O
)	O
of	O
size	O
with	O
60	O
,	O
000	O
training	O
samples	O
and	O
10	O
,	O
000	O
test	O
samples	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
results	O
from	O
four	O
methods	O
,	O
namely	O
:	O
(	O
1	O
)	O
conventional	O
CNN	Method
with	Method
softmax	Method
loss	Method
(	O
CNN	Method
-	Method
Softmax	Method
)	O
,	O
(	O
2	O
)	O
the	O
proposed	O
DSN	Method
with	O
softmax	O
loss	O
(	O
DSN	Method
-	O
Softmax	O
)	O
,	O
(	O
3	O
)	O
CNN	Method
with	Method
max	Method
-	Method
margin	Method
objective	Method
(	O
CNN	Method
-	Method
SVM	Method
)	O
,	O
and	O
(	O
4	O
)	O
the	O
proposed	O
DSN	Method
with	O
max	Method
-	Method
margin	Method
objective	Method
(	O
DSN	Method
-	O
SVM	O
)	O
.	O

DSN	Method
-	O
Softmax	O
and	O
DSN	Method
-	O
SVM	O
outperform	O
both	O
their	O
competing	O
CNN	Method
algorithms	Method
(	O
DSN	Method
-	O
SVM	O
shows	O
classification	Metric
error	Metric
of	O
under	O
a	O
single	O
model	O
without	O
data	Method
whitening	Method
and	O
augmentation	Method
)	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
classification	Metric
error	Metric
of	O
the	O
competing	O
methods	O
when	O
trained	O
w.r.t	O
.	O

varying	O
sizes	O
of	O
training	O
samples	O
(	O
gain	O
of	O
DSN	Method
-	O
SVM	O
over	O
CNN	Method
-	Method
Softmax	Method
at	O
samples	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
shows	O
a	O
comparison	O
of	O
generalization	Metric
error	Metric
between	O
CNN	Method
and	O
DSN	Method
.	O

subsection	O
:	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
CIFAR	Material
-	Material
10	Material
dataset	Material
consists	O
of	O
color	O
images	O
.	O

A	O
total	O
number	O
of	O
60	O
,	O
000	O
images	O
are	O
split	O
into	O
50	O
,	O
000	O
training	O
and	O
10	O
,	O
000	O
testing	O
images	O
.	O

The	O
dataset	O
is	O
preprocessed	O
by	O
global	Method
contrast	Method
normalization	Method
.	O

To	O
compare	O
our	O
results	O
with	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
in	O
this	O
case	O
,	O
we	O
also	O
augmented	O
the	O
dataset	O
by	O
zero	O
padding	O
4	O
pixels	O
on	O
each	O
side	O
,	O
then	O
do	O
corner	O
cropping	O
and	O
random	O
flipping	O
on	O
the	O
fly	O
during	O
training	O
.	O

No	O
model	Method
averaging	Method
is	O
done	O
at	O
the	O
test	O
phase	O
and	O
we	O
only	O
crop	O
the	O
center	O
of	O
a	O
test	O
sample	O
.	O

Table	O
(	O
[	O
reference	O
]	O
)	O
shows	O
our	O
result	O
.	O

Our	O
DSN	Method
model	O
achieved	O
an	O
error	Metric
rates	O
of	O
without	O
data	Method
augmentation	Method
and	O
with	O
data	Method
agumentation	Method
(	O
the	O
best	O
known	O
result	O
to	O
our	O
knowledge	O
)	O
.	O

DSN	Method
also	O
provides	O
added	O
robustness	O
to	O
hyperparameter	O
choice	O
,	O
in	O
that	O
the	O
early	O
layers	O
are	O
guided	O
with	O
direct	O
classification	O
loss	O
,	O
leading	O
to	O
a	O
faster	O
convergence	Metric
rate	Metric
and	O
relieved	O
burden	O
on	O
heavy	O
hyperparameter	Method
tuning	Method
.	O

We	O
also	O
compared	O
the	O
gradients	O
in	O
DSN	Method
and	O
those	O
in	O
CNN	Method
,	O
observing	O
times	O
greater	O
gradient	Metric
variance	Metric
of	O
DSN	Method
over	O
CNN	Method
in	O
the	O
first	O
convolutional	Method
layer	Method
.	O

This	O
is	O
consistent	O
with	O
an	O
observation	O
in	O
,	O
and	O
the	O
assumptions	O
and	O
motivations	O
we	O
make	O
in	O
this	O
work	O
.	O

To	O
see	O
what	O
the	O
features	O
have	O
been	O
learned	O
in	O
DSN	Method
vs.	O
CNN	O
,	O
we	O
select	O
one	O
example	O
image	O
from	O
each	O
of	O
the	O
ten	O
categories	O
of	O
CIFAR	Material
-	Material
10	Material
dataset	Material
,	O
run	O
one	O
forward	Method
pass	Method
,	O
and	O
show	O
the	O
feature	O
maps	O
learned	O
from	O
the	O
first	O
(	O
bottom	O
)	O
convolutional	Method
layer	Method
in	O
Figure	O
(	O
[	O
reference	O
]	O
)	O
.	O

Only	O
the	O
top	O
30	O
%	O
activations	O
are	O
shown	O
in	O
each	O
of	O
the	O
feature	Method
maps	Method
.	O

Feature	O
maps	O
learned	O
by	O
DSN	Method
show	O
to	O
be	O
more	O
intuitive	O
than	O
those	O
by	O
CNN	Method
.	O

CIFAR	Material
-	Material
100	Material
dataset	O
is	O
similar	O
to	O
CIFAR	Material
-	Material
10	Material
dataset	Material
,	O
except	O
that	O
it	O
has	O
100	O
classes	O
.	O

The	O
number	O
of	O
images	O
for	O
each	O
class	O
is	O
then	O
instead	O
of	O
as	O
in	O
CIFAR	Material
-	Material
10	Material
,	O
which	O
makes	O
the	O
classification	Task
task	Task
more	O
challenging	O
.	O

We	O
use	O
the	O
same	O
network	O
settings	O
as	O
in	O
CIFAR	Material
-	Material
10	Material
.	O

Table	O
(	O
[	O
reference	O
]	O
)	O
shows	O
previous	O
best	O
results	O
and	O
is	O
reported	O
by	O
DSN	Method
.	O

The	O
performance	O
boost	O
consistently	O
shown	O
on	O
both	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
again	O
demonstrates	O
the	O
advantage	O
of	O
the	O
DSN	Method
method	O
.	O

subsection	O
:	O
Street	O
View	O
House	O
Numbers	O
Street	O
View	O
House	O
Numbers	O
(	O
SVHN	Material
)	O
dataset	O
consists	O
of	O
digits	O
for	O
training	O
,	O
digits	O
for	O
testing	O
,	O
and	O
extra	O
training	O
samples	O
on	O
color	O
images	O
.	O

We	O
followed	O
the	O
previous	O
works	O
for	O
data	Task
preparation	Task
,	O
namely	O
:	O
we	O
select	O
400	O
samples	O
per	O
class	O
from	O
the	O
training	O
set	O
and	O
200	O
samples	O
per	O
class	O
from	O
the	O
extra	O
set	O
.	O

The	O
remaining	O
598	O
,	O
388	O
images	O
are	O
used	O
for	O
training	O
.	O

We	O
followed	O
to	O
preprocess	O
the	O
dataset	O
by	O
Local	Method
Contrast	Method
Normalization	Method
(	O
LCN	Method
)	O
.	O

We	O
do	O
not	O
do	O
data	Method
augmentation	Method
in	O
training	Task
and	O
use	O
only	O
a	O
single	O
model	O
in	O
testing	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
recent	O
comparable	O
results	O
.	O

Note	O
that	O
Dropconnect	Method
uses	O
data	Method
augmentation	Method
and	O
multiple	Method
model	Method
voting	Method
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
have	O
presented	O
a	O
new	O
formulation	O
,	O
deeply	Method
-	Method
supervised	Method
nets	Method
(	O
DSN	Method
)	O
,	O
attempting	O
to	O
make	O
a	O
more	O
transparent	O
learning	Method
process	Method
for	O
deep	Method
learning	Method
.	O

Evident	O
performance	O
enhancement	O
over	O
existing	O
approaches	O
has	O
been	O
obtained	O
.	O

A	O
stochastic	Method
gradient	Method
view	Method
also	O
sheds	O
light	O
to	O
the	O
understanding	O
of	O
our	O
formulation	O
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
is	O
supported	O
by	O
NSF	O
award	O
IIS	O
-	O
1216528	O
(	O
IIS	O
-	O
1360566	O
)	O
and	O
NSF	O
award	O
IIS	O
-	O
0844566	O
(	O
IIS	O
-	O
1360568	O
)	O
.	O

We	O
thank	O
Min	O
Lin	O
,	O
Naiyan	O
Wang	O
,	O
Baoyuan	O
Wang	O
,	O
Jingdong	O
Wang	O
,	O
Liwei	O
Wang	O
,	O
and	O
David	O
Wipf	O
for	O
help	O
discussions	O
.	O

We	O
are	O
greatful	O
for	O
the	O
generous	O
donation	O
of	O
the	O
GPUs	O
by	O
NVIDIA	O
.	O

bibliography	O
:	O
References	O
