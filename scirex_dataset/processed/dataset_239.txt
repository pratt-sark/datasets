document	O
:	O
Effective	O
Use	O
of	O
Word	O
Order	O
for	O
Text	Task
Categorization	Task
with	O
Convolutional	Method
Neural	Method
Networks	Method
Convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
is	O
a	O
neural	Method
network	Method
that	O
can	O
make	O
use	O
of	O
the	O
internal	O
structure	O
of	O
data	O
such	O
as	O
the	O
2D	O
structure	O
of	O
image	O
data	O
.	O

This	O
paper	O
studies	O
CNN	Method
on	O
text	Task
categorization	Task
to	O
exploit	O
the	O
1D	O
structure	O
(	O
namely	O
,	O
word	O
order	O
)	O
of	O
text	O
data	O
for	O
accurate	Task
prediction	Task
.	O

Instead	O
of	O
using	O
low	O
-	O
dimensional	O
word	O
vectors	O
as	O
input	O
as	O
is	O
often	O
done	O
,	O
we	O
directly	O
apply	O
CNN	Method
to	O
high	O
-	O
dimensional	O
text	O
data	O
,	O
which	O
leads	O
to	O
directly	O
learning	O
embedding	Task
of	Task
small	Task
text	Task
regions	Task
for	O
use	O
in	O
classification	Task
.	O

In	O
addition	O
to	O
a	O
straightforward	O
adaptation	O
of	O
CNN	Method
from	O
image	O
to	O
text	O
,	O
a	O
simple	O
but	O
new	O
variation	O
which	O
employs	O
bag	Method
-	Method
of	Method
-	Method
word	Method
conversion	Method
in	O
the	O
convolution	Method
layer	Method
is	O
proposed	O
.	O

An	O
extension	O
to	O
combine	O
multiple	O
convolution	Method
layers	Method
is	O
also	O
explored	O
for	O
higher	O
accuracy	Metric
.	O

The	O
experiments	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
in	O
comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

section	O
:	O
Introduction	O
Text	Task
categorization	Task
is	O
the	O
task	O
of	O
automatically	Task
assigning	Task
pre	Task
-	Task
defined	Task
categories	Task
to	O
documents	O
written	O
in	O
natural	O
languages	O
.	O

Several	O
types	O
of	O
text	Task
categorization	Task
have	O
been	O
studied	O
,	O
each	O
of	O
which	O
deals	O
with	O
different	O
types	O
of	O
documents	O
and	O
categories	O
,	O
such	O
as	O
topic	Task
categorization	Task
to	O
detect	O
discussed	O
topics	O
(	O
e.g.	O
,	O
sports	O
,	O
politics	O
)	O
,	O
spam	Task
detection	Task
,	O
and	O
sentiment	Task
classification	Task
to	O
determine	O
the	O
sentiment	O
typically	O
in	O
product	Task
or	Task
movie	Task
reviews	Task
.	O

A	O
standard	O
approach	O
to	O
text	Task
categorization	Task
is	O
to	O
represent	O
documents	O
by	O
bag	O
-	O
of	O
-	O
word	O
vectors	O
,	O
namely	O
,	O
vectors	O
that	O
indicate	O
which	O
words	O
appear	O
in	O
the	O
documents	O
but	O
do	O
not	O
preserve	O
word	O
order	O
,	O
and	O
use	O
classification	Method
models	Method
such	O
as	O
SVM	Method
.	O

It	O
has	O
been	O
noted	O
that	O
loss	O
of	O
word	O
order	O
caused	O
by	O
bag	O
-	O
of	O
-	O
word	O
vectors	O
(	O
bow	Method
vectors	Method
)	O
is	O
particularly	O
problematic	O
on	O
sentiment	Task
classification	Task
.	O

A	O
simple	O
remedy	O
is	O
to	O
use	O
word	Method
bi	Method
-	Method
grams	Method
in	O
addition	O
to	O
uni	O
-	O
grams	O
.	O

However	O
,	O
use	O
of	O
word	O
-	O
grams	O
with	O
on	O
text	Task
categorization	Task
in	O
general	O
is	O
not	O
always	O
effective	O
;	O
e.g.	O
,	O
on	O
topic	Task
categorization	Task
,	O
simply	O
adding	O
phrases	O
or	O
-	O
grams	O
is	O
not	O
effective	O
(	O
see	O
,	O
e.g.	O
,	O
references	O
in	O
)	O
.	O

To	O
benefit	O
from	O
word	O
order	O
on	O
text	Task
categorization	Task
,	O
we	O
take	O
a	O
different	O
approach	O
,	O
which	O
employs	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
.	O

CNN	Method
is	O
a	O
neural	Method
network	Method
that	O
can	O
make	O
use	O
of	O
the	O
internal	O
structure	O
of	O
data	O
such	O
as	O
the	O
2D	O
structure	O
of	O
image	O
data	O
through	O
convolution	Method
layers	Method
,	O
where	O
each	O
computation	Method
unit	Method
responds	O
to	O
a	O
small	O
region	O
of	O
input	O
data	O
(	O
e.g.	O
,	O
a	O
small	O
square	O
of	O
a	O
large	O
image	O
)	O
.	O

We	O
apply	O
CNN	Method
to	O
text	Task
categorization	Task
to	O
make	O
use	O
of	O
the	O
1D	O
structure	O
(	O
word	O
order	O
)	O
of	O
document	O
data	O
so	O
that	O
each	O
unit	O
in	O
the	O
convolution	Method
layer	Method
responds	O
to	O
a	O
small	O
region	O
of	O
a	O
document	O
(	O
a	O
sequence	O
of	O
words	O
)	O
.	O

CNN	Method
has	O
been	O
very	O
successful	O
on	O
image	Task
classification	Task
;	O
see	O
e.g.	O
,	O
the	O
winning	O
solutions	O
of	O
ImageNet	Task
Large	Task
Scale	Task
Visual	Task
Recognition	Task
Challenge	Task
.	O

On	O
text	O
,	O
since	O
the	O
work	O
on	O
token	Task
-	Task
level	Task
applications	Task
(	O
e.g.	O
,	O
POS	Task
tagging	Task
)	O
by	O
nnnlpJMLR11	O
,	O
CNN	Method
has	O
been	O
used	O
in	O
systems	O
for	O
entity	Task
search	Task
,	O
sentence	Task
modeling	Task
,	O
word	Task
embedding	Task
learning	Task
,	O
product	Task
feature	Task
mining	Task
,	O
and	O
so	O
on	O
.	O

Notably	O
,	O
in	O
many	O
of	O
these	O
CNN	Method
studies	O
on	O
text	O
,	O
the	O
first	O
layer	O
of	O
the	O
network	O
converts	O
words	O
in	O
sentences	O
to	O
word	O
vectors	O
by	O
table	Method
lookup	Method
.	O

The	O
word	O
vectors	O
are	O
either	O
trained	O
as	O
part	O
of	O
CNN	Method
training	O
,	O
or	O
fixed	O
to	O
those	O
learned	O
by	O
some	O
other	O
method	O
(	O
e.g.	O
,	O
word2vec	Method
)	O
from	O
an	O
additional	O
large	O
corpus	O
.	O

The	O
latter	O
is	O
a	O
form	O
of	O
semi	Method
-	Method
supervised	Method
learning	Method
,	O
which	O
we	O
study	O
elsewhere	O
.	O

We	O
are	O
interested	O
in	O
the	O
effectiveness	O
of	O
CNN	Method
itself	O
without	O
aid	O
of	O
additional	O
resources	O
;	O
therefore	O
,	O
word	O
vectors	O
should	O
be	O
trained	O
as	O
part	O
of	O
network	Task
training	Task
if	O
word	Task
vector	Task
lookup	Task
is	O
to	O
be	O
done	O
.	O

A	O
question	O
arises	O
,	O
however	O
,	O
whether	O
word	Method
vector	Method
lookup	Method
in	O
a	O
purely	O
supervised	Task
setting	Task
is	O
really	O
useful	O
for	O
text	Task
categorization	Task
.	O

The	O
essence	O
of	O
convolution	Method
layers	Method
is	O
to	O
convert	O
text	O
regions	O
of	O
a	O
fixed	O
size	O
(	O
e.g.	O
,	O
“	O
am	O
so	O
happy	O
”	O
with	O
size	O
3	O
)	O
to	O
feature	O
vectors	O
,	O
as	O
described	O
later	O
.	O

In	O
that	O
sense	O
,	O
a	O
word	Method
vector	Method
learning	Method
layer	Method
is	O
a	O
special	O
(	O
and	O
unusual	O
)	O
case	O
of	O
convolution	Method
layer	Method
with	O
region	O
size	O
one	O
.	O

Why	O
is	O
size	O
one	O
appropriate	O
if	O
bi	O
-	O
grams	O
are	O
more	O
discriminating	O
than	O
uni	O
-	O
grams	O
?	O
Hence	O
,	O
we	O
take	O
a	O
different	O
approach	O
.	O

We	O
directly	O
apply	O
CNN	Method
to	O
high	O
-	O
dimensional	O
one	O
-	O
hot	O
vectors	O
;	O
i.e.	O
,	O
we	O
directly	O
learn	O
embedding	Task
We	O
use	O
the	O
term	O
‘	O
embedding	O
’	O
loosely	O
to	O
mean	O
a	O
structure	Method
-	Method
preserving	Method
function	Method
,	O
in	O
particular	O
,	O
a	O
function	O
that	O
generates	O
low	O
-	O
dimensional	O
features	O
that	O
preserve	O
the	O
predictive	O
structure	O
.	O

of	O
text	O
regions	O
without	O
going	O
through	O
word	Method
embedding	Method
learning	Method
.	O

This	O
approach	O
is	O
made	O
possible	O
by	O
solving	O
the	O
computational	Task
issue	Task
through	O
efficient	O
handling	O
of	O
high	O
-	O
dimensional	O
sparse	O
data	O
on	O
GPU	O
,	O
and	O
it	O
turned	O
out	O
to	O
have	O
the	O
merits	O
of	O
improving	O
accuracy	Metric
with	O
fast	O
training	O
/	O
prediction	Task
and	O
simplifying	O
the	O
system	O
(	O
fewer	O
hyper	O
-	O
parameters	O
to	O
tune	O
)	O
.	O

Our	O
CNN	Method
code	O
for	O
text	O
is	O
publicly	O
available	O
on	O
the	O
internet	O
.	O

We	O
study	O
the	O
effectiveness	O
of	O
CNN	Method
on	O
text	Task
categorization	Task
and	O
explain	O
why	O
CNN	Method
is	O
suitable	O
for	O
the	O
task	O
.	O

Two	O
types	O
of	O
CNN	Method
are	O
tested	O
:	O
seq	Method
-	Method
CNN	Method
is	O
a	O
straightforward	O
adaptation	O
of	O
CNN	Method
from	O
image	O
to	O
text	O
,	O
and	O
bow	Method
-	Method
CNN	Method
is	O
a	O
simple	O
but	O
new	O
variation	O
of	O
CNN	Method
that	O
employs	O
bag	Method
-	Method
of	Method
-	Method
word	Method
conversion	Method
in	O
the	O
convolution	Method
layer	Method
.	O

The	O
experiments	O
show	O
that	O
seq	Method
-	Method
CNN	Method
outperforms	O
bow	Method
-	Method
CNN	Method
on	O
sentiment	Task
classification	Task
,	O
vice	O
versa	O
on	O
topic	Task
classification	Task
,	O
and	O
the	O
winner	O
generally	O
outperforms	O
the	O
conventional	O
bag	Method
-	Method
of	Method
-	Method
-	Method
gram	Method
vector	Method
-	Method
based	Method
methods	Method
,	O
as	O
well	O
as	O
previous	O
CNN	Method
models	O
for	O
text	O
which	O
are	O
more	O
complex	O
.	O

In	O
particular	O
,	O
to	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
that	O
has	O
successfully	O
used	O
word	O
order	O
to	O
improve	O
topic	Task
classification	Task
performance	O
.	O

A	O
simple	O
extension	O
that	O
combines	O
multiple	O
convolution	Method
layers	Method
(	O
thus	O
combining	O
multiple	O
types	O
of	O
text	Task
region	Task
embedding	Task
)	O
leads	O
to	O
further	O
improvement	O
.	O

Through	O
empirical	O
analysis	O
,	O
we	O
will	O
show	O
that	O
CNN	Method
can	O
make	O
effective	O
use	O
of	O
high	O
-	O
order	O
-	O
grams	O
when	O
conventional	O
methods	O
fail	O
.	O

section	O
:	O
CNN	Method
for	O
document	Task
classification	Task
We	O
first	O
review	O
CNN	Method
applied	O
to	O
image	O
data	O
and	O
then	O
discuss	O
the	O
application	O
of	O
CNN	Method
to	O
document	Task
classification	Task
tasks	Task
to	O
introduce	O
seq	Method
-	Method
CNN	Method
and	O
bow	Method
-	Method
CNN	Method
.	O

subsection	O
:	O
Preliminary	O
:	O
CNN	Method
for	O
image	Task
CNN	Method
is	O
a	O
feed	Method
-	Method
forward	Method
neural	Method
network	Method
with	O
convolution	Method
layers	Method
interleaved	O
with	O
pooling	Method
layers	Method
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
the	O
top	O
layer	O
performs	O
classification	Task
using	O
the	O
features	O
generated	O
by	O
the	O
layers	O
below	O
.	O

A	O
convolution	Method
layer	Method
consists	O
of	O
several	O
computation	Method
units	Method
,	O
each	O
of	O
which	O
takes	O
as	O
input	O
a	O
region	O
vector	O
that	O
represents	O
a	O
small	O
region	O
of	O
the	O
input	O
image	O
and	O
applies	O
a	O
non	O
-	O
linear	O
function	O
to	O
it	O
.	O

Typically	O
,	O
the	O
region	O
vector	O
is	O
a	O
concatenation	O
of	O
pixels	O
in	O
the	O
region	O
,	O
which	O
would	O
be	O
,	O
for	O
example	O
,	O
75	O
-	O
dimensional	O
if	O
the	O
region	O
is	O
and	O
the	O
number	O
of	O
channels	O
is	O
three	O
(	O
red	O
,	O
green	O
,	O
and	O
blue	O
)	O
.	O

Conceptually	O
,	O
computation	Method
units	Method
are	O
placed	O
over	O
the	O
input	O
image	O
so	O
that	O
the	O
entire	O
image	O
is	O
collectively	O
covered	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
region	O
stride	O
(	O
distance	O
between	O
the	O
region	O
centers	O
)	O
is	O
often	O
set	O
to	O
a	O
small	O
value	O
such	O
as	O
1	O
so	O
that	O
regions	O
overlap	O
with	O
each	O
other	O
,	O
though	O
the	O
stride	O
in	O
Figure	O
[	O
reference	O
]	O
is	O
set	O
larger	O
than	O
the	O
region	O
size	O
for	O
illustration	O
.	O

A	O
distinguishing	O
feature	O
of	O
convolution	Method
layers	Method
is	O
weight	Method
sharing	Method
.	O

Given	O
input	O
,	O
a	O
unit	O
associated	O
with	O
the	O
-	O
th	O
region	O
computes	O
where	O
is	O
a	O
region	O
vector	O
representing	O
the	O
region	O
of	O
at	O
location	O
,	O
and	O
is	O
a	O
pre	O
-	O
defined	O
component	Method
-	Method
wise	Method
non	Method
-	Method
linear	Method
activation	Method
function	Method
,	O
(	O
e.g.	O
,	O
applying	O
to	O
each	O
vector	Method
component	Method
)	O
.	O

The	O
matrix	O
of	O
weights	O
and	O
the	O
vector	O
of	O
biases	O
are	O
learned	O
through	O
training	O
,	O
and	O
they	O
are	O
shared	O
by	O
the	O
computation	O
units	O
in	O
the	O
same	O
layer	O
.	O

This	O
weight	Method
sharing	Method
enables	O
learning	O
useful	O
features	O
irrespective	O
of	O
their	O
location	O
,	O
while	O
preserving	O
the	O
location	O
where	O
the	O
useful	O
features	O
appeared	O
.	O

We	O
regard	O
the	O
output	O
of	O
a	O
convolution	Method
layer	Method
as	O
an	O
‘	O
image	O
’	O
so	O
that	O
the	O
output	O
of	O
each	O
computation	Method
unit	Method
is	O
considered	O
to	O
be	O
a	O
‘	O
pixel	O
’	O
of	O
channels	O
where	O
is	O
the	O
number	O
of	O
weight	O
vectors	O
(	O
i.e.	O
,	O
the	O
number	O
of	O
rows	O
of	O
)	O
or	O
the	O
number	O
of	O
neurons	O
.	O

In	O
other	O
words	O
,	O
a	O
convolution	Method
layer	Method
converts	O
image	O
regions	O
to	O
m	O
-	O
dim	O
vectors	O
,	O
and	O
the	O
locations	O
of	O
the	O
regions	O
are	O
inherited	O
through	O
this	O
conversion	O
.	O

The	O
output	O
image	O
of	O
the	O
convolution	Method
layer	Method
is	O
passed	O
to	O
a	O
pooling	Method
layer	Method
,	O
which	O
essentially	O
shrinks	O
the	O
image	O
by	O
merging	O
neighboring	O
pixels	O
,	O
so	O
that	O
higher	O
layers	O
can	O
deal	O
with	O
more	O
abstract	O
/	O
global	O
information	O
.	O

A	O
pooling	Method
layer	Method
consists	O
of	O
pooling	Method
units	Method
,	O
each	O
of	O
which	O
is	O
associated	O
with	O
a	O
small	O
region	O
of	O
the	O
image	O
.	O

Commonly	O
-	O
used	O
merging	Method
methods	Method
are	O
average	Method
-	Method
pooling	Method
and	O
max	Method
-	Method
pooling	Method
,	O
which	O
respectively	O
compute	O
the	O
channel	O
-	O
wise	O
average	O
/	O
maximum	O
of	O
each	O
region	O
.	O

subsection	O
:	O
CNN	Method
for	O
text	O
Now	O
we	O
consider	O
application	O
of	O
CNN	Method
to	O
text	O
data	O
.	O

Suppose	O
that	O
we	O
are	O
given	O
a	O
document	O
with	O
vocabulary	O
.	O

CNN	Method
requires	O
vector	Method
representation	Method
of	Method
data	Method
that	O
preserves	O
internal	O
locations	O
(	O
word	O
order	O
in	O
this	O
case	O
)	O
as	O
input	O
.	O

A	O
straightforward	O
representation	O
would	O
be	O
to	O
treat	O
each	O
word	O
as	O
a	O
pixel	O
,	O
treat	O
as	O
if	O
it	O
were	O
an	O
image	O
of	O
pixels	O
with	O
channels	O
,	O
and	O
to	O
represent	O
each	O
pixel	O
(	O
i.e.	O
,	O
each	O
word	O
)	O
as	O
a	O
-	O
dimensional	O
one	O
-	O
hot	O
vector	O
.	O

As	O
a	O
running	O
toy	O
example	O
,	O
suppose	O
that	O
vocabulary	O
“	O
do	O
n’t	O
”	O
,	O
“	O
hate	O
”	O
,	O
“	O
I	O
”	O
,	O
“	O
it	O
”	O
,	O
“	O
love	O
”	O
and	O
we	O
associate	O
the	O
words	O
with	O
dimensions	O
of	O
vector	O
in	O
alphabetical	O
order	O
(	O
as	O
shown	O
)	O
,	O
and	O
that	O
document	O
=	O
“	O
I	O
love	O
it	O
”	O
.	O

Then	O
,	O
we	O
have	O
a	O
document	O
vector	O
:	O
subsubsection	O
:	O
seq	Method
-	Method
CNN	Method
for	O
text	O
As	O
in	O
the	O
convolution	Method
layer	Method
for	O
image	O
,	O
we	O
represent	O
each	O
region	O
(	O
which	O
each	O
computation	O
unit	O
responds	O
to	O
)	O
by	O
a	O
concatenation	O
of	O
the	O
pixels	O
,	O
which	O
makes	O
-	O
dimensional	O
region	O
vectors	O
where	O
is	O
the	O
region	O
size	O
fixed	O
in	O
advance	O
.	O

For	O
example	O
,	O
on	O
the	O
example	O
document	O
vector	O
above	O
,	O
with	O
and	O
stride	O
1	O
,	O
we	O
would	O
have	O
two	O
regions	O
“	O
I	O
love	O
”	O
and	O
“	O
love	O
it	O
”	O
represented	O
by	O
the	O
following	O
vectors	O
:	O
The	O
rest	O
is	O
the	O
same	O
as	O
image	O
;	O
the	O
text	O
region	O
vectors	O
are	O
converted	O
to	O
feature	O
vectors	O
,	O
i.e.	O
,	O
the	O
convolution	Method
layer	Method
learns	O
to	O
embed	O
text	O
regions	O
into	O
low	O
-	O
dimensional	O
vector	O
space	O
.	O

We	O
call	O
a	O
neural	Method
net	Method
with	O
a	O
convolution	Method
layer	Method
with	O
this	O
region	O
representation	O
seq	Method
-	O
CNN	Method
(	O
‘	O
seq	Method
’	O
for	O
keeping	O
sequences	O
of	O
words	O
)	O
to	O
distinguish	O
it	O
from	O
bow	Method
-	Method
CNN	Method
,	O
described	O
next	O
.	O

subsubsection	O
:	O
bow	Method
-	Method
CNN	Method
for	O
text	Task
A	O
potential	O
problem	O
of	O
seq	Method
-	Method
CNN	Method
however	O
,	O
is	O
that	O
unlike	O
image	O
data	O
with	O
3	O
RGB	O
channels	O
,	O
the	O
number	O
of	O
‘	O
channels	O
’	O
(	O
size	O
of	O
vocabulary	O
)	O
may	O
be	O
very	O
large	O
(	O
e.g.	O
,	O
100	O
K	O
)	O
,	O
which	O
could	O
make	O
each	O
region	O
vector	O
very	O
high	O
-	O
dimensional	O
if	O
the	O
region	O
size	O
is	O
large	O
.	O

Since	O
the	O
dimensionality	O
of	O
region	O
vectors	O
determines	O
the	O
dimensionality	O
of	O
weight	O
vectors	O
,	O
having	O
high	O
-	O
dimensional	O
region	O
vectors	O
means	O
more	O
parameters	O
to	O
learn	O
.	O

If	O
is	O
too	O
large	O
,	O
the	O
model	O
becomes	O
too	O
complex	O
(	O
w.r.t	O
.	O

the	O
amount	O
of	O
training	O
data	O
available	O
)	O
and	O
/	O
or	O
training	Task
becomes	O
unaffordably	O
expensive	O
even	O
with	O
efficient	O
handling	O
of	O
sparse	O
data	O
;	O
therefore	O
,	O
one	O
has	O
to	O
lower	O
the	O
dimensionality	Metric
by	O
lowering	O
the	O
vocabulary	Metric
size	Metric
and	O
/	O
or	O
the	O
region	O
size	O
,	O
which	O
may	O
or	O
may	O
not	O
be	O
desirable	O
,	O
depending	O
on	O
the	O
nature	O
of	O
the	O
task	O
.	O

An	O
alternative	O
we	O
provide	O
is	O
to	O
perform	O
bag	Method
-	Method
of	Method
-	Method
word	Method
conversion	Method
to	O
make	O
region	O
vectors	O
-	O
dimensional	O
instead	O
of	O
-	O
dimensional	O
;	O
e.g.	O
,	O
the	O
example	O
region	O
vectors	O
above	O
would	O
be	O
converted	O
to	O
:	O
With	O
this	O
representation	O
,	O
we	O
have	O
fewer	O
parameters	O
to	O
learn	O
.	O

Essentially	O
,	O
the	O
expressiveness	O
of	O
bow	Method
-	Method
convolution	Method
(	O
which	O
loses	O
word	O
order	O
only	O
within	O
small	O
regions	O
)	O
is	O
somewhere	O
between	O
seq	Method
-	O
convolution	O
and	O
bow	Method
vectors	Method
.	O

subsubsection	Method
:	O
Pooling	Method
for	O
text	O
Whereas	O
the	O
size	O
of	O
images	O
is	O
fixed	O
in	O
image	Task
applications	Task
,	O
documents	O
are	O
naturally	O
variable	O
-	O
sized	O
,	O
and	O
therefore	O
,	O
with	O
a	O
fixed	O
stride	O
,	O
the	O
output	O
of	O
a	O
convolution	Method
layer	Method
is	O
also	O
variable	O
-	O
sized	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Given	O
the	O
variable	O
-	O
sized	O
output	O
of	O
the	O
convolution	Method
layer	Method
,	O
standard	O
pooling	Method
for	O
image	O
(	O
which	O
uses	O
a	O
fixed	O
pooling	O
region	O
size	O
and	O
a	O
fixed	O
stride	O
)	O
would	O
produce	O
variable	O
-	O
sized	O
output	O
,	O
which	O
can	O
be	O
passed	O
to	O
another	O
convolution	Method
layer	Method
.	O

To	O
produce	O
fixed	O
-	O
sized	O
output	O
,	O
which	O
is	O
required	O
by	O
the	O
fully	Method
-	Method
connected	Method
top	Method
layer	Method
,	O
we	O
fix	O
the	O
number	O
of	O
pooling	O
units	O
and	O
dynamically	O
determine	O
the	O
pooling	O
region	O
size	O
on	O
each	O
data	O
point	O
so	O
that	O
the	O
entire	O
data	O
is	O
covered	O
without	O
overlapping	O
.	O

In	O
the	O
previous	O
CNN	Method
work	O
on	O
text	O
,	O
pooling	Task
is	O
typically	O
max	Method
-	Method
pooling	Method
over	O
the	O
entire	O
data	O
(	O
i.e.	O
,	O
one	O
pooling	Method
unit	Method
associated	O
with	O
the	O
whole	O
text	O
)	O
.	O

The	O
dynamic	Method
k	Method
-	Method
max	Method
pooling	Method
of	O
for	O
sentence	Task
modeling	Task
extends	O
it	O
to	O
take	O
the	O
largest	O
values	O
where	O
is	O
a	O
function	O
of	O
the	O
sentence	O
length	O
,	O
but	O
it	O
is	O
again	O
over	O
the	O
entire	O
data	O
,	O
and	O
the	O
operation	O
is	O
limited	O
to	O
max	Method
-	Method
pooling	Method
.	O

Our	O
pooling	Method
differs	O
in	O
that	O
it	O
is	O
a	O
natural	O
extension	O
of	O
standard	O
pooling	Method
for	O
image	Task
,	O
in	O
which	O
not	O
only	O
max	Method
-	Method
pooling	Method
but	O
other	O
types	O
can	O
be	O
applied	O
.	O

With	O
multiple	O
pooling	O
units	O
associated	O
with	O
different	O
regions	O
,	O
the	O
top	Method
layer	Method
can	O
receive	O
locational	O
information	O
(	O
e.g.	O
,	O
if	O
there	O
are	O
two	O
pooling	O
units	O
,	O
the	O
features	O
from	O
the	O
first	O
half	O
and	O
last	O
half	O
of	O
a	O
document	O
are	O
distinguished	O
)	O
.	O

This	O
turned	O
out	O
to	O
be	O
useful	O
(	O
along	O
with	O
average	Method
-	Method
pooling	Method
)	O
on	O
topic	Task
classification	Task
,	O
as	O
shown	O
later	O
.	O

subsection	O
:	O
CNN	Method
vs.	O
bag	Method
-	Method
of	Method
-	Method
-	Method
grams	Method
Traditional	O
methods	O
represent	O
each	O
document	O
entirely	O
with	O
one	O
bag	Method
-	Method
of	Method
-	Method
-	Method
gram	Method
vector	Method
and	O
then	O
apply	O
a	O
classifier	Method
model	Method
such	O
as	O
SVM	Method
.	O

However	O
,	O
since	O
high	O
-	O
order	O
-	O
grams	O
are	O
susceptible	O
to	O
data	O
sparsity	O
,	O
use	O
of	O
a	O
large	O
such	O
as	O
20	O
is	O
not	O
only	O
infeasible	O
but	O
also	O
ineffective	O
.	O

Also	O
note	O
that	O
a	O
bag	O
-	O
of	O
-	O
-	O
gram	O
represents	O
each	O
-	O
gram	O
by	O
a	O
one	O
-	O
hot	O
vector	O
and	O
ignores	O
the	O
fact	O
that	O
some	O
-	O
grams	O
share	O
constituent	O
words	O
.	O

By	O
contrast	O
,	O
CNN	Method
internally	O
learns	O
embedding	O
of	O
text	O
regions	O
(	O
given	O
the	O
consituent	O
words	O
as	O
input	O
)	O
useful	O
for	O
the	O
intended	O
task	O
.	O

Consequently	O
,	O
a	O
large	O
such	O
as	O
20	O
can	O
be	O
used	O
especially	O
with	O
the	O
bow	Method
-	Method
convolution	Method
layer	Method
,	O
which	O
turned	O
out	O
to	O
be	O
useful	O
on	O
topic	Task
classification	Task
.	O

A	O
neuron	O
trained	O
to	O
assign	O
a	O
large	O
value	O
to	O
,	O
e.g.	O
,	O
“	O
I	O
love	O
”	O
(	O
and	O
a	O
small	O
value	O
to	O
“	O
I	O
hate	O
”	O
)	O
is	O
likely	O
to	O
assign	O
a	O
large	O
value	O
to	O
“	O
we	O
love	O
”	O
(	O
and	O
a	O
small	O
value	O
to	O
“	O
we	O
hate	O
”	O
)	O
as	O
well	O
,	O
even	O
though	O
“	O
we	O
love	O
”	O
was	O
never	O
seen	O
during	O
training	O
.	O

We	O
will	O
confirm	O
these	O
points	O
empirically	O
later	O
.	O

subsection	O
:	O
Extension	O
:	O
parallel	O
CNN	Method
We	O
have	O
described	O
CNN	Method
with	O
the	O
simplest	O
network	Method
architecture	Method
that	O
has	O
one	O
pair	O
of	O
convolution	Method
and	Method
pooling	Method
layers	Method
.	O

While	O
this	O
can	O
be	O
extended	O
in	O
several	O
ways	O
(	O
e.g.	O
,	O
with	O
deeper	O
layers	O
)	O
,	O
in	O
our	O
experiments	O
,	O
we	O
explored	O
parallel	O
CNN	Method
,	O
which	O
has	O
two	O
or	O
more	O
convolution	O
layers	O
in	O
parallel	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
idea	O
is	O
to	O
learn	O
multiple	O
types	O
of	O
embedding	O
of	O
small	O
text	O
regions	O
so	O
that	O
they	O
can	O
complement	O
each	O
other	O
to	O
improve	O
model	O
accuracy	Metric
.	O

In	O
this	O
architecture	O
,	O
multiple	O
convolution	Method
-	Method
pooling	Method
pairs	Method
with	O
different	O
region	O
sizes	O
(	O
and	O
possibly	O
different	O
region	Method
vector	Method
representations	Method
)	O
are	O
given	O
one	O
-	O
hot	O
vectors	O
as	O
input	O
and	O
produce	O
feature	O
vectors	O
for	O
each	O
region	O
;	O
the	O
top	O
layer	O
takes	O
the	O
concatenation	O
of	O
the	O
produced	O
feature	O
vectors	O
as	O
input	O
.	O

section	O
:	O
Experiments	O
We	O
experimented	O
with	O
CNN	Method
on	O
two	O
tasks	O
,	O
topic	Task
classification	Task
and	O
sentiment	Task
classification	Task
.	O

Detailed	O
information	O
for	O
reproducing	O
the	O
results	O
is	O
available	O
on	O
the	O
internet	O
along	O
with	O
our	O
code	O
.	O

subsection	O
:	O
CNN	Method
We	O
fixed	O
the	O
activation	O
function	O
to	O
rectifier	O
and	O
minimized	O
square	O
loss	O
with	O
regularization	Method
by	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
.	O

We	O
only	O
used	O
the	O
30	O
K	O
words	O
that	O
appeared	O
most	O
frequently	O
in	O
the	O
training	O
set	O
;	O
thus	O
,	O
for	O
example	O
,	O
in	O
seq	Method
-	Method
CNN	Method
with	O
region	O
size	O
3	O
,	O
a	O
region	O
vector	O
is	O
90	O
K	O
dimensional	O
.	O

Out	O
-	O
of	O
-	O
vocabulary	O
words	O
were	O
represented	O
by	O
a	O
zero	O
vector	O
.	O

On	O
bow	Method
-	Method
CNN	Method
,	O
to	O
speed	O
up	O
computation	Task
,	O
we	O
used	O
variable	O
region	O
stride	O
so	O
that	O
a	O
larger	O
stride	O
was	O
taken	O
where	O
repetition	O
of	O
the	O
same	O
region	O
vectors	O
can	O
be	O
avoided	O
by	O
doing	O
so	O
.	O

Padding	O
size	O
was	O
fixed	O
to	O
where	O
is	O
the	O
region	O
size	O
.	O

We	O
used	O
two	O
techniques	O
commonly	O
used	O
with	O
CNN	Method
on	O
image	O
,	O
which	O
typically	O
led	O
to	O
small	O
performance	O
improvements	O
.	O

One	O
is	O
dropout	O
optionally	O
applied	O
to	O
the	O
input	O
to	O
the	O
top	O
layer	O
.	O

The	O
other	O
is	O
response	Method
normalization	Method
as	O
in	O
,	O
which	O
in	O
our	O
case	O
scales	O
the	O
output	O
of	O
the	O
pooling	Method
layer	Method
at	O
each	O
location	O
by	O
multiplying	O
.	O

subsection	O
:	O
Baseline	O
methods	O
For	O
comparison	O
,	O
we	O
tested	O
SVM	Method
with	O
the	O
linear	Method
kernel	Method
and	O
fully	Method
-	Method
connected	Method
neural	Method
networks	Method
(	O
see	O
e.g.	O
,	O
Bishop95	O
)	O
with	O
bag	O
-	O
of	O
-	O
-	O
gram	O
vectors	O
as	O
input	O
.	O

To	O
experiment	O
with	O
fully	Method
-	Method
connected	Method
neural	Method
nets	Method
,	O
as	O
in	O
CNN	Method
,	O
we	O
minimized	O
square	O
loss	O
with	O
regularization	Method
and	O
optional	O
dropout	O
by	O
SGD	Method
,	O
and	O
activation	O
was	O
fixed	O
to	O
rectifier	O
.	O

To	O
generate	O
bag	Task
-	Task
of	Task
-	Task
-	Task
gram	Task
vectors	Task
,	O
on	O
topic	Task
classification	Task
,	O
we	O
first	O
set	O
each	O
component	O
to	O
where	O
is	O
the	O
word	O
frequency	O
in	O
the	O
document	O
and	O
then	O
scaled	O
them	O
to	O
unit	O
vectors	O
,	O
which	O
we	O
found	O
always	O
improved	O
performance	O
over	O
raw	O
frequency	O
.	O

On	O
sentiment	Task
classification	Task
,	O
as	O
is	O
often	O
done	O
,	O
we	O
generated	O
binary	O
vectors	O
and	O
scaled	O
them	O
to	O
unit	O
vectors	O
.	O

We	O
tested	O
three	O
types	O
of	O
bag	Method
-	Method
of	Method
-	Method
-	Method
gram	Method
:	O
bow1	Method
with	O
,	O
bow2	Method
with	O
,	O
and	O
bow3	Method
with	O
;	O
that	O
is	O
,	O
bow1	Method
is	O
the	O
traditional	O
bow	O
vectors	O
,	O
and	O
with	O
bow3	Method
,	O
each	O
component	O
of	O
the	O
vectors	O
corresponds	O
to	O
either	O
uni	O
-	O
gram	O
,	O
bi	O
-	O
gram	O
,	O
or	O
tri	O
-	O
gram	O
of	O
words	O
.	O

We	O
used	O
SVMlight	Method
for	O
the	O
SVM	Task
experiments	O
.	O

paragraph	O
:	O
NB	Method
-	Method
LM	Method
We	O
also	O
tested	O
NB	Method
-	Method
LM	Method
,	O
which	O
first	O
appeared	O
(	O
but	O
without	O
performance	O
report	O
)	O
as	O
NBSVM	Method
in	O
WM12	Method
and	O
later	O
with	O
a	O
small	O
modification	O
produced	O
performance	O
that	O
exceeds	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	Method
methods	Method
on	O
IMDB	Material
(	O
which	O
we	O
experimented	O
with	O
)	O
in	O
MMRB14	Method
.	O

We	O
experimented	O
with	O
the	O
MMRB14	Method
version	Method
,	O
which	O
generates	O
binary	O
bag	O
-	O
of	O
-	O
-	O
gram	O
vectors	O
,	O
multiplies	O
the	O
component	O
for	O
each	O
-	O
gram	O
with	O
(	O
NB	O
-	O
weight	O
)	O
where	O
the	O
probabilities	O
are	O
estimated	O
using	O
the	O
training	O
data	O
,	O
and	O
does	O
logistic	Method
regression	Method
training	Method
.	O

We	O
used	O
MMRB14	Method
’s	Method
software	Method
with	O
a	O
modification	O
so	O
that	O
the	O
regularization	O
parameter	O
can	O
be	O
tuned	O
on	O
development	O
data	O
.	O

subsection	O
:	O
Model	Method
selection	Method
For	O
all	O
the	O
methods	O
,	O
the	O
hyper	O
-	O
parameters	O
such	O
as	O
net	O
configurations	O
and	O
regularization	O
parameters	O
were	O
chosen	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
data	O
(	O
held	O
-	O
out	O
portion	O
of	O
the	O
training	O
data	O
)	O
,	O
and	O
using	O
the	O
chosen	O
hyper	O
-	O
parameters	O
,	O
the	O
models	O
were	O
re	O
-	O
trained	O
using	O
all	O
the	O
training	O
data	O
.	O

subsection	O
:	O
Data	O
,	O
tasks	O
,	O
and	O
data	Task
preprocessing	Task
paragraph	O
:	O
IMDB	Material
:	O
movie	O
reviews	O
The	O
IMDB	Material
dataset	O
is	O
a	O
benchmark	O
dataset	O
for	O
sentiment	Task
classification	Task
.	O

The	O
task	O
is	O
to	O
determine	O
if	O
the	O
movie	O
reviews	O
are	O
positive	O
or	O
negative	O
.	O

Both	O
the	O
training	O
and	O
test	O
sets	O
consist	O
of	O
25	O
K	O
reviews	O
.	O

For	O
preprocessing	Task
,	O
we	O
tokenized	O
the	O
text	O
so	O
that	O
emoticons	O
such	O
as	O
“	O
:	O
-)	O
”	O
are	O
treated	O
as	O
tokens	O
and	O
converted	O
all	O
the	O
characters	O
to	O
lower	O
case	O
.	O

paragraph	O
:	O
Elec	Material
:	O
electronics	O
product	O
reviews	O
Elec	Material
consists	O
of	O
electronic	O
product	O
reviews	O
.	O

It	O
is	O
part	O
of	O
a	O
large	O
Amazon	O
review	O
dataset	O
.	O

We	O
chose	O
electronics	O
as	O
it	O
seemed	O
to	O
be	O
very	O
different	O
from	O
movies	O
.	O

Following	O
the	O
generation	O
of	O
IMDB	Material
,	O
we	O
chose	O
the	O
training	O
set	O
and	O
the	O
test	O
set	O
so	O
that	O
one	O
half	O
of	O
each	O
set	O
consists	O
of	O
positive	O
reviews	O
and	O
the	O
other	O
half	O
is	O
negative	O
,	O
regarding	O
rating	O
1	O
and	O
2	O
as	O
negative	O
and	O
4	O
and	O
5	O
as	O
positive	O
,	O
and	O
that	O
the	O
reviewed	O
products	O
are	O
disjoint	O
between	O
the	O
training	O
set	O
and	O
test	O
set	O
.	O

Note	O
that	O
to	O
extract	O
text	O
from	O
the	O
original	O
data	O
,	O
we	O
only	O
used	O
the	O
text	O
section	O
,	O
and	O
we	O
did	O
not	O
use	O
the	O
summary	O
section	O
.	O

This	O
way	O
,	O
we	O
obtained	O
a	O
test	O
set	O
of	O
25	O
K	O
reviews	O
(	O
same	O
as	O
IMDB	Material
)	O
and	O
training	O
sets	O
of	O
various	O
sizes	O
.	O

The	O
training	O
and	O
test	O
sets	O
are	O
available	O
on	O
the	O
internet	O
.	O

Data	Task
preprocessing	Task
was	O
the	O
same	O
as	O
IMDB	Material
.	O

paragraph	O
:	O
RCV1	Method
:	O
topic	Task
categorization	Task
RCV1	Method
is	O
a	O
corpus	Material
of	Material
Reuters	Material
news	Material
articles	Material
as	O
described	O
in	O
LYRL04	Material
.	O

RCV1	Method
has	O
103	O
topic	O
categories	O
in	O
a	O
hierarchy	O
,	O
and	O
one	O
document	O
may	O
be	O
associated	O
with	O
more	O
than	O
one	O
topic	O
.	O

Performance	O
on	O
this	O
task	O
(	O
multi	Task
-	Task
label	Task
categorization	Task
)	O
is	O
known	O
to	O
be	O
sensitive	O
to	O
thresholding	Method
strategies	Method
,	O
which	O
are	O
algorithms	O
additional	O
to	O
the	O
models	O
we	O
would	O
like	O
to	O
test	O
.	O

Therefore	O
,	O
we	O
also	O
experimented	O
with	O
single	Task
-	Task
label	Task
categorization	Task
to	O
assign	O
one	O
of	O
55	O
second	O
-	O
level	O
topics	O
to	O
each	O
document	O
to	O
directly	O
evaluate	O
models	O
.	O

For	O
this	O
task	O
,	O
we	O
used	O
the	O
documents	O
from	O
a	O
one	O
-	O
month	O
period	O
as	O
the	O
test	O
set	O
and	O
generated	O
various	O
sizes	O
of	O
training	O
sets	O
from	O
the	O
documents	O
with	O
earlier	O
dates	O
.	O

Data	O
sizes	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
in	O
LYRL04	Material
,	O
we	O
used	O
the	O
concatenation	O
of	O
the	O
headline	O
and	O
text	O
elements	O
.	O

Data	Task
preprocessing	Task
was	O
the	O
same	O
as	O
IMDB	Material
except	O
that	O
we	O
used	O
the	O
stopword	O
list	O
provided	O
by	O
LYRL04	Material
and	O
regarded	O
numbers	O
as	O
stopwords	O
.	O

subsection	O
:	O
Performance	O
results	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
error	Metric
rates	Metric
of	O
CNN	Method
in	O
comparison	O
with	O
the	O
baseline	O
methods	O
.	O

The	O
first	O
thing	O
to	O
note	O
is	O
that	O
on	O
all	O
the	O
datasets	O
,	O
the	O
best	O
-	O
performing	O
CNN	Method
outperforms	O
the	O
baseline	O
methods	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
approach	O
.	O

To	O
look	O
into	O
the	O
details	O
,	O
let	O
us	O
first	O
focus	O
on	O
CNN	Method
with	O
one	O
convolution	Method
layer	Method
(	O
seq	Method
-	Method
and	O
bow	Method
-	Method
CNN	Method
in	O
the	O
table	O
)	O
.	O

On	O
sentiment	Task
classification	Task
(	O
IMDB	Material
and	O
Elec	Material
)	O
,	O
the	O
configuration	O
chosen	O
by	O
model	Method
selection	Method
was	O
:	O
region	O
size	O
3	O
,	O
stride	O
1	O
,	O
1000	O
weight	O
vectors	O
,	O
and	O
max	Method
-	Method
pooling	Method
with	O
one	O
pooling	Method
unit	Method
,	O
for	O
both	O
types	O
of	O
CNN	Method
;	O
seq	Method
-	Method
CNN	Method
outperforms	O
bow	Method
-	Method
CNN	Method
,	O
as	O
well	O
as	O
all	O
the	O
baseline	O
methods	O
except	O
for	O
one	O
.	O

Note	O
that	O
with	O
a	O
small	O
region	O
size	O
and	O
max	Method
-	Method
pooling	Method
,	O
if	O
a	O
review	O
contains	O
a	O
short	O
phrase	O
that	O
conveys	O
strong	O
sentiment	O
(	O
e.g.	O
,	O
“	O
A	O
great	O
movie	O
!	O

”	O
)	O
,	O
the	O
review	O
could	O
receive	O
a	O
high	O
score	O
irrespective	O
of	O
the	O
rest	O
of	O
the	O
review	O
.	O

It	O
is	O
sensible	O
that	O
this	O
type	O
of	O
configuration	O
is	O
effective	O
on	O
sentiment	Task
classification	Task
.	O

By	O
contrast	O
,	O
on	O
topic	Task
categorization	Task
(	O
RCV1	Task
)	O
,	O
the	O
configuration	O
chosen	O
for	O
bow	Method
-	Method
CNN	Method
by	O
model	Method
selection	Method
was	O
:	O
region	O
size	O
20	O
,	O
variable	O
-	O
stride	O
2	O
,	O
average	Method
-	Method
pooling	Method
with	O
10	O
pooling	O
units	O
,	O
and	O
1000	O
weight	O
vectors	O
,	O
which	O
is	O
very	O
different	O
from	O
sentiment	Task
classification	Task
.	O

This	O
is	O
presumably	O
because	O
on	O
topic	Task
classification	Task
,	O
a	O
larger	O
context	O
would	O
be	O
more	O
predictive	O
than	O
short	O
fragments	O
(	O
larger	O
region	O
size	O
)	O
,	O
the	O
entire	O
document	O
matters	O
(	O
the	O
effectiveness	O
of	O
average	Method
-	Method
pooling	Method
)	O
,	O
and	O
the	O
location	O
of	O
predictive	O
text	O
also	O
matters	O
(	O
multiple	O
pooling	O
units	O
)	O
.	O

The	O
last	O
point	O
may	O
be	O
because	O
news	O
documents	O
tend	O
to	O
have	O
crucial	O
sentences	O
(	O
as	O
well	O
as	O
the	O
headline	O
)	O
at	O
the	O
beginning	O
.	O

On	O
this	O
task	O
,	O
while	O
both	O
seq	Method
and	O
bow	Method
-	Method
CNN	Method
outperform	O
the	O
baseline	O
methods	O
,	O
bow	Method
-	Method
CNN	Method
outperforms	O
seq	Method
-	Method
CNN	Method
,	O
which	O
indicates	O
that	O
in	O
this	O
setting	O
the	O
merit	O
of	O
having	O
fewer	O
parameters	O
is	O
larger	O
than	O
the	O
benefit	O
of	O
keeping	O
word	O
order	O
in	O
each	O
region	O
.	O

Now	O
we	O
turn	O
to	O
parallel	O
CNN	Method
.	O

On	O
IMDB	Material
,	O
seq2	Method
-	Method
CNN	Method
,	O
which	O
has	O
two	O
seq	Method
-	O
convolution	O
layers	O
(	O
region	O
size	O
2	O
and	O
3	O
;	O
1000	O
neurons	O
each	O
;	O
followed	O
by	O
one	O
unit	O
of	O
max	Method
-	Method
pooling	Method
each	O
)	O
,	O
outperforms	O
seq	Method
-	Method
CNN	Method
.	O

With	O
more	O
neurons	O
(	O
3000	O
neurons	O
each	O
;	O
Table	O
[	O
reference	O
]	O
)	O
it	O
further	O
exceeds	O
the	O
best	O
-	O
performing	O
baseline	O
,	O
which	O
is	O
also	O
the	O
best	O
previous	O
supervised	O
result	O
.	O

We	O
presume	O
the	O
effectiveness	O
of	O
seq2	Method
-	Method
CNN	Method
indicates	O
that	O
the	O
length	O
of	O
predictive	O
text	O
regions	O
is	O
variable	O
.	O

The	O
best	O
performance	O
7.67	O
on	O
IMDB	Material
was	O
obtained	O
by	O
‘	O
seq2	Method
-	Method
bow	Method
-	Method
CNN	Method
’	Method
,	O
equipped	O
with	O
three	O
layers	O
in	O
parallel	O
:	O
two	O
seq	Method
-	O
convolution	O
layers	O
(	O
1000	O
neurons	O
each	O
)	O
as	O
in	O
seq2	Method
-	Method
CNN	Method
above	O
and	O
one	O
layer	O
(	O
20	O
neurons	O
)	O
that	O
regards	O
the	O
entire	O
document	O
as	O
one	O
region	O
and	O
represents	O
the	O
region	O
(	O
document	O
)	O
by	O
a	O
bag	Method
-	Method
of	Method
-	Method
-	O
gram	O
vector	O
(	O
bow3	Method
)	O
as	O
input	O
to	O
the	O
computation	Method
unit	Method
;	O
in	O
particular	O
,	O
we	O
generated	O
bow3	O
vectors	O
by	O
multiplying	O
the	O
NB	O
-	O
weights	O
with	O
binary	O
vectors	O
,	O
motivated	O
by	O
the	O
good	O
performance	O
of	O
NB	Method
-	Method
LM	Method
.	O

This	O
third	O
layer	O
is	O
a	O
bow	Method
-	Method
convolution	Method
layer	Method
with	O
one	O
region	O
of	O
variable	O
size	O
that	O
takes	O
one	O
-	O
hot	O
vectors	O
with	O
-	O
gram	O
vocabulary	O
as	O
input	O
to	O
learn	O
document	Task
embedding	Task
.	O

The	O
seq2	O
-	O
bow	O
-	O
CNN	Method
for	O
Elec	Material
in	O
the	O
table	O
is	O
the	O
same	O
except	O
that	O
the	O
regions	O
sizes	O
of	O
seq	Method
-	O
convolution	O
layers	O
are	O
3	O
and	O
4	O
.	O

On	O
both	O
datasets	O
,	O
performance	O
is	O
improved	O
over	O
seq2	Method
-	Method
CNN	Method
.	O

The	O
results	O
suggest	O
that	O
what	O
can	O
be	O
learned	O
through	O
these	O
three	O
layers	O
are	O
distinct	O
enough	O
to	O
complement	O
each	O
other	O
.	O

The	O
effectiveness	O
of	O
the	O
third	O
layer	O
indicates	O
that	O
not	O
only	O
short	O
word	O
sequences	O
but	O
also	O
global	O
context	O
in	O
a	O
large	O
window	O
may	O
be	O
useful	O
on	O
this	O
task	O
;	O
thus	O
,	O
inclusion	O
of	O
a	O
bow	Method
-	Method
convolution	Method
layer	Method
with	O
-	Method
gram	Method
vocabulary	Method
with	O
a	O
large	O
fixed	O
region	O
size	O
might	O
be	O
even	O
more	O
effective	O
,	O
providing	O
more	O
focused	O
context	O
,	O
but	O
we	O
did	O
not	O
pursue	O
it	O
in	O
this	O
work	O
.	O

paragraph	O
:	O
Baseline	O
methods	O
Comparing	O
the	O
baseline	O
methods	O
with	O
each	O
other	O
,	O
on	O
sentiment	Task
classification	Task
,	O
reducing	O
the	O
vocabulary	O
to	O
the	O
most	O
frequent	O
-	O
grams	O
notably	O
hurt	O
performance	O
(	O
also	O
observed	O
on	O
NB	Method
-	Method
LM	Method
and	O
NN	Method
)	O
even	O
though	O
some	O
reduction	O
is	O
a	O
common	O
practice	O
.	O

Error	Metric
rates	Metric
were	O
clearly	O
improved	O
by	O
addition	O
of	O
bi	Method
-	Method
and	Method
tri	Method
-	Method
grams	Method
.	O

By	O
contrast	O
,	O
on	O
topic	Task
categorization	Task
,	O
bi	Method
-	Method
grams	Method
only	O
slightly	O
improved	O
accuracy	Metric
,	O
and	O
reduction	O
of	O
vocabulary	O
did	O
not	O
hurt	O
performance	O
.	O

NB	Method
-	Method
LM	Method
is	O
very	O
strong	O
on	O
IMDB	Material
and	O
poor	O
on	O
RCV1	Method
;	O
its	O
effectiveness	O
appears	O
to	O
be	O
data	O
-	O
dependent	O
,	O
as	O
also	O
observed	O
by	O
WM12	Method
.	O

paragraph	O
:	O
Comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
previous	O
best	O
supervised	O
result	O
on	O
IMDB	Material
is	O
8.13	O
by	O
NB	Method
-	Method
LM	Method
with	O
bow3	Method
(	O
MMRB14	Method
)	O
,	O
and	O
our	O
best	O
error	Metric
rate	Metric
7.67	O
is	O
better	O
by	O
nearly	O
0.5	O
%	O
.	O

reports	O
7.46	O
with	O
the	O
semi	Method
-	Method
supervised	Method
method	Method
that	O
learns	O
low	Method
-	Method
dimensional	Method
vector	Method
representations	Method
of	Method
documents	Method
from	O
unlabeled	O
data	O
.	O

Their	O
result	O
is	O
not	O
directly	O
comparable	O
with	O
our	O
supervised	O
results	O
due	O
to	O
use	O
of	O
additional	O
resource	O
.	O

Nevertheless	O
,	O
our	O
best	O
result	O
rivals	O
their	O
result	O
.	O

We	O
tested	O
bow	Method
-	Method
CNN	Method
on	O
the	O
multi	Task
-	Task
label	Task
topic	Task
categorization	Task
task	Task
on	O
RCV1	Task
to	O
compare	O
with	O
LYRL04	Material
.	O

We	O
used	O
the	O
same	O
thresholding	Method
strategy	Method
as	O
LYRL04	Material
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
bow	Method
-	Method
CNN	Method
outperforms	O
LYRL04	Material
’s	O
best	O
results	O
even	O
though	O
our	O
data	Method
preprocessing	Method
is	O
much	O
simpler	O
(	O
no	O
stemming	O
and	O
no	O
tf	Method
-	Method
idf	Method
weighting	Method
)	O
.	O

paragraph	O
:	O
Previous	O
CNN	Method
We	O
focus	O
on	O
the	O
sentence	Task
classification	Task
studies	Task
due	O
to	O
its	O
relation	O
to	O
text	Task
categorization	Task
.	O

Kim14	O
studied	O
fine	Task
-	Task
tuning	Task
of	O
pre	O
-	O
trained	O
word	O
vectors	O
to	O
produce	O
input	O
to	O
parallel	O
CNN	Method
.	O

He	O
reported	O
that	O
performance	O
was	O
poor	O
when	O
word	O
vectors	O
were	O
trained	O
as	O
part	O
of	O
CNN	Method
training	O
(	O
i.e.	O
,	O
no	O
additional	O
method	O
/	O
corpus	O
)	O
.	O

On	O
our	O
tasks	O
,	O
we	O
were	O
also	O
unable	O
to	O
outperform	O
the	O
baselines	O
with	O
this	O
type	O
of	O
model	O
.	O

Also	O
,	O
with	O
our	O
approach	O
,	O
a	O
system	O
is	O
simpler	O
with	O
one	O
fewer	O
layer	O
–	O
no	O
need	O
to	O
tune	O
the	O
dimensionality	O
of	O
word	O
vectors	O
or	O
meta	O
-	O
parameters	O
for	O
word	Method
vector	Method
learning	Method
.	O

KGB14	Method
proposed	O
complex	O
modifications	O
of	O
CNN	Method
for	O
sentence	Task
modeling	Task
.	O

Notably	O
,	O
given	O
word	O
vectors	O
,	O
their	O
convolution	Method
with	Method
feature	Method
maps	Method
produces	O
for	O
each	O
region	O
a	O
matrix	O
(	O
instead	O
of	O
a	O
vector	O
as	O
in	O
standard	O
CNN	Method
)	O
.	O

Using	O
the	O
provided	O
code	O
,	O
we	O
found	O
that	O
their	O
model	O
is	O
too	O
resource	O
-	O
demanding	O
for	O
our	O
tasks	O
.	O

On	O
IMDB	Material
and	O
Elec	Material
the	O
best	O
error	Metric
rates	Metric
we	O
obtained	O
by	O
training	O
with	O
various	O
configurations	O
that	O
fit	O
in	O
memory	O
for	O
24	O
hours	O
each	O
on	O
GPU	O
(	O
cf	O
.	O

Fig	O
[	O
reference	O
]	O
)	O
were	O
10.13	O
and	O
9.37	O
,	O
respectively	O
,	O
which	O
is	O
no	O
better	O
than	O
SVM	Method
bow2	Method
.	O

Since	O
excellent	O
performances	O
were	O
reported	O
on	O
short	Task
sentence	Task
classification	Task
,	O
we	O
presume	O
that	O
their	O
model	O
is	O
optimized	O
for	O
short	O
sentences	O
,	O
but	O
not	O
for	O
text	Task
categorization	Task
in	O
general	O
.	O

paragraph	O
:	O
Performance	O
dependency	O
CNN	Method
training	O
is	O
known	O
to	O
be	O
expensive	O
,	O
compared	O
with	O
,	O
e.g.	O
,	O
linear	Method
models	Method
–	O
linear	Method
SVM	Method
with	O
bow3	Method
on	O
IMDB	Material
only	O
takes	O
9	O
minutes	O
using	O
SVMlight	Method
(	O
single	O
-	O
core	O
)	O
on	O
a	O
high	O
-	O
end	O
Intel	O
CPU	O
.	O

Nevertheless	O
,	O
with	O
our	O
code	O
on	O
GPU	Method
,	O
CNN	Method
training	O
only	O
takes	O
minutes	O
(	O
to	O
a	O
few	O
hours	O
)	O
on	O
these	O
datasets	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Finally	O
,	O
the	O
results	O
with	O
training	O
sets	O
of	O
various	O
sizes	O
on	O
Elec	Material
and	O
RCV1	Method
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Why	O
is	O
CNN	Method
effective	O
?	O
In	O
this	O
section	O
we	O
explain	O
the	O
effectiveness	O
of	O
CNN	Method
through	O
looking	O
into	O
what	O
it	O
learns	O
from	O
training	O
.	O

First	O
,	O
for	O
comparison	O
,	O
we	O
show	O
the	O
-	O
grams	O
that	O
SVM	Method
with	O
bow3	Method
found	O
to	O
be	O
the	O
most	O
predictive	O
;	O
i.e.	O
,	O
the	O
following	O
-	O
grams	O
were	O
assigned	O
the	O
10	O
largest	O
weights	O
by	O
SVM	Method
with	O
binary	O
features	O
on	O
Elec	Material
for	O
the	O
negative	O
and	O
positive	O
class	O
,	O
respectively	O
:	O
poor	O
,	O
useless	O
,	O
returned	O
,	O
not	O
worth	O
,	O
return	O
,	O
worse	O
,	O
disappointed	O
,	O
terrible	O
,	O
worst	O
,	O
horrible	O
great	O
,	O
excellent	O
,	O
perfect	O
,	O
love	O
,	O
easy	O
,	O
amazing	O
,	O
awesome	O
,	O
no	O
problems	O
,	O
perfectly	O
,	O
beat	O
Note	O
that	O
,	O
even	O
though	O
SVM	Method
was	O
also	O
given	O
bi	O
-	O
and	O
tri	O
-	O
grams	O
,	O
the	O
top	O
10	O
features	O
chosen	O
by	O
SVM	Method
with	O
binary	O
features	O
are	O
mostly	O
uni	O
-	O
grams	O
;	O
furthermore	O
,	O
the	O
top	O
100	O
features	O
(	O
50	O
for	O
each	O
class	O
)	O
include	O
28	O
bi	O
-	O
grams	O
but	O
only	O
four	O
tri	O
-	O
grams	O
.	O

This	O
means	O
that	O
,	O
with	O
the	O
given	O
size	O
of	O
training	O
data	O
,	O
SVM	Method
still	O
heavily	O
counts	O
on	O
uni	O
-	O
grams	O
,	O
which	O
could	O
be	O
ambiguous	O
,	O
and	O
can	O
not	O
fully	O
take	O
advantage	O
of	O
higher	O
-	O
order	O
-	O
grams	O
.	O

By	O
contrast	O
,	O
NB	O
-	O
weights	O
tend	O
to	O
promote	O
-	O
grams	O
with	O
a	O
larger	O
;	O
the	O
100	O
features	O
that	O
were	O
assigned	O
the	O
largest	O
NB	O
-	O
weights	O
are	O
7	O
uni	O
-	O
,	O
33	O
bi	O
-	O
,	O
and	O
60	O
tri	O
-	O
grams	O
.	O

However	O
,	O
as	O
seen	O
above	O
,	O
NB	Method
-	Method
weights	Method
do	O
not	O
always	O
lead	O
to	O
the	O
best	O
performance	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
some	O
of	O
text	O
regions	O
learned	O
by	O
seq	Method
-	Method
CNN	Method
to	O
be	O
predictive	O
on	O
Elec	Material
.	O

This	O
net	O
has	O
one	O
convolution	Method
layer	Method
with	O
region	O
size	O
3	O
and	O
1000	O
neurons	O
;	O
thus	O
,	O
embedding	O
by	O
the	O
convolution	Method
layer	Method
produces	O
a	O
1000	O
-	O
dim	O
vector	O
for	O
each	O
region	O
,	O
which	O
(	O
after	O
pooling	Method
)	O
serves	O
as	O
features	O
in	O
the	O
top	O
layer	O
where	O
weights	O
are	O
assigned	O
to	O
the	O
1000	O
vector	Method
components	Method
.	O

In	O
the	O
table	O
,	O
N	O
/	O
P	O
indicates	O
the	O
component	O
that	O
received	O
the	O
-	O
th	O
highest	O
weight	O
in	O
the	O
top	O
layer	O
for	O
the	O
negative	O
/	O
positive	O
class	O
,	O
respectively	O
.	O

The	O
table	O
shows	O
the	O
text	O
regions	O
(	O
in	O
the	O
training	O
set	O
)	O
whose	O
embedded	O
vectors	O
have	O
a	O
large	O
value	O
in	O
the	O
corresponding	O
component	O
,	O
i.e.	O
,	O
predictive	O
text	O
regions	O
.	O

Note	O
that	O
the	O
embedded	O
vectors	O
for	O
the	O
text	O
regions	O
listed	O
in	O
the	O
same	O
row	O
are	O
close	O
to	O
each	O
other	O
as	O
they	O
have	O
a	O
large	O
value	O
in	O
the	O
same	O
component	O
.	O

That	O
is	O
,	O
Table	O
[	O
reference	O
]	O
also	O
shows	O
that	O
the	O
proximity	O
of	O
the	O
embedded	O
vectors	O
tends	O
to	O
reflect	O
the	O
proximity	O
in	O
terms	O
of	O
the	O
relations	O
to	O
the	O
target	O
classes	O
(	O
positive	O
/	O
negative	O
sentiment	O
)	O
.	O

This	O
is	O
the	O
effect	O
of	O
embedding	O
,	O
which	O
helps	O
classification	Task
by	O
the	O
top	O
layer	O
.	O

With	O
the	O
bag	Method
-	Method
of	Method
-	Method
-	Method
gram	Method
representation	Method
,	O
only	O
the	O
-	O
grams	O
that	O
appear	O
in	O
the	O
training	O
data	O
can	O
participate	O
in	O
prediction	Task
.	O

By	O
contrast	O
,	O
one	O
strength	O
of	O
CNN	Method
is	O
that	O
-	O
grams	O
(	O
or	O
text	O
regions	O
of	O
size	O
)	O
can	O
contribute	O
to	O
accurate	O
prediction	Task
even	O
if	O
they	O
did	O
not	O
appear	O
in	O
the	O
training	O
data	O
,	O
as	O
long	O
as	O
(	O
some	O
of	O
)	O
their	O
constituent	O
words	O
did	O
,	O
because	O
input	O
of	O
embedding	O
is	O
the	O
constituent	O
words	O
of	O
the	O
region	O
.	O

To	O
see	O
this	O
point	O
,	O
in	O
Table	O
[	O
reference	O
]	O
we	O
show	O
the	O
text	O
regions	O
from	O
the	O
test	O
set	O
,	O
which	O
did	O
not	O
appear	O
in	O
the	O
training	O
data	O
,	O
either	O
entirely	O
or	O
partially	O
as	O
bi	O
-	O
grams	O
,	O
and	O
yet	O
whose	O
embedded	O
features	O
have	O
large	O
values	O
in	O
the	O
heavily	O
-	O
weighted	O
(	O
predictive	Method
)	Method
component	Method
thus	O
contributing	O
to	O
the	O
prediction	Task
.	O

There	O
are	O
many	O
more	O
of	O
these	O
,	O
and	O
we	O
only	O
show	O
a	O
small	O
part	O
of	O
them	O
that	O
fit	O
certain	O
patterns	O
.	O

One	O
noticeable	O
pattern	O
is	O
(	O
be	O
-	O
verb	O
,	O
adverb	O
,	O
sentiment	O
adjective	O
)	O
such	O
as	O
“	O
am	O
entirely	O
satisfied	O
”	O
and	O
“	O
’	O
m	O
overall	O
impressed	O
”	O
.	O

These	O
adjectives	O
alone	O
could	O
be	O
ambiguous	O
as	O
they	O
may	O
be	O
negated	O
.	O

To	O
know	O
that	O
the	O
writer	O
is	O
indeed	O
“	O
satisfied	O
”	O
,	O
we	O
need	O
to	O
see	O
the	O
sequence	O
“	O
am	O
satisfied	O
”	O
,	O
but	O
the	O
insertion	O
of	O
adverb	O
such	O
as	O
“	O
entirely	O
”	O
is	O
very	O
common	O
.	O

“	O
best	O
X	O
ever	O
’	O
is	O
another	O
pattern	O
that	O
a	O
discriminating	O
pair	O
of	O
words	O
are	O
not	O
adjacent	O
to	O
each	O
other	O
.	O

These	O
patterns	O
require	O
tri	O
-	O
grams	O
for	O
disambiguation	Task
,	O
and	O
seq	Method
-	Method
CNN	Method
successfully	O
makes	O
use	O
of	O
them	O
even	O
though	O
the	O
exact	O
tri	O
-	O
grams	O
were	O
not	O
seen	O
during	O
training	O
,	O
as	O
a	O
result	O
of	O
learning	Task
,	O
e.g.	O
,	O
“	O
am	O
X	O
satisfied	O
”	O
with	O
non	O
-	O
negative	O
X	O
(	O
e.g.	O
,	O
“	O
am	O
very	O
satisfied	O
”	O
,	O
“	O
am	O
so	O
satisfied	O
”	O
)	O
to	O
be	O
predictive	O
of	O
the	O
positive	O
class	O
through	O
training	O
.	O

That	O
is	O
,	O
CNN	Method
can	O
effectively	O
use	O
word	O
order	O
when	O
bag	O
-	O
of	O
-	O
-	Method
gram	Method
-	Method
based	Method
approaches	Method
fail	O
.	O

section	O
:	O
Conclusion	O
This	O
paper	O
showed	O
that	O
CNN	Method
provides	O
an	O
alternative	O
mechanism	O
for	O
effective	O
use	O
of	O
word	O
order	O
for	O
text	Task
categorization	Task
through	O
direct	Task
embedding	Task
of	Task
small	Task
text	Task
regions	Task
,	O
different	O
from	O
the	O
traditional	O
bag	Method
-	Method
of	Method
-	Method
-	Method
gram	Method
approach	Method
or	O
word	O
-	O
vector	O
CNN	Method
.	O

With	O
the	O
parallel	O
CNN	Method
framework	O
,	O
several	O
types	O
of	O
embedding	O
can	O
be	O
learned	O
and	O
combined	O
so	O
that	O
they	O
can	O
complement	O
each	O
other	O
for	O
higher	O
accuracy	Metric
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
on	O
sentiment	Task
classification	Task
and	O
topic	Task
classification	Task
were	O
achieved	O
using	O
this	O
approach	O
.	O

section	O
:	O
Acknowledgements	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
useful	O
suggestions	O
.	O

The	O
second	O
author	O
was	O
supported	O
by	O
NSF	O
IIS	O
-	O
1250985	O
and	O
NSF	O
IIS	O
-	O
1407939	O
.	O

bibliography	O
:	O
References	O
