document	O
:	O
Adversarial	Method
Autoencoders	Method
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
‘	O
‘	O
adversarial	Method
autoencoder	Method
’	O
’	O
(	O
AAE	Method
)	O
,	O
which	O
is	O
a	O
probabilistic	Method
autoencoder	Method
that	O
uses	O
the	O
recently	O
proposed	O
generative	Method
adversarial	Method
networks	Method
(	O
GAN	Method
)	O
to	O
perform	O
variational	Task
inference	Task
by	O
matching	O
the	O
aggregated	O
posterior	O
of	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	Method
with	O
an	O
arbitrary	O
prior	O
distribution	O
.	O

Matching	O
the	O
aggregated	O
posterior	O
to	O
the	O
prior	O
ensures	O
that	O
generating	O
from	O
any	O
part	O
of	O
prior	O
space	O
results	O
in	O
meaningful	O
samples	O
.	O

As	O
a	O
result	O
,	O
the	O
decoder	Method
of	O
the	O
adversarial	Method
autoencoder	Method
learns	O
a	O
deep	Method
generative	Method
model	Method
that	O
maps	O
the	O
imposed	O
prior	O
to	O
the	O
data	O
distribution	O
.	O

We	O
show	O
how	O
the	O
adversarial	Method
autoencoder	Method
can	O
be	O
used	O
in	O
applications	O
such	O
as	O
semi	Task
-	Task
supervised	Task
classification	Task
,	O
disentangling	Task
style	Task
and	Task
content	Task
of	Task
images	Task
,	O
unsupervised	Task
clustering	Task
,	O
dimensionality	Task
reduction	Task
and	O
data	Task
visualization	Task
.	O

We	O
performed	O
experiments	O
on	O
MNIST	Material
,	O
Street	Material
View	Material
House	Material
Numbers	Material
and	O
Toronto	Material
Face	Material
datasets	Material
and	O
show	O
that	O
adversarial	Method
autoencoders	Method
achieve	O
competitive	O
results	O
in	O
generative	Task
modeling	Task
and	O
semi	Task
-	Task
supervised	Task
classification	Task
tasks	Task
.	O

section	O
:	O
Introduction	O
Building	O
scalable	Method
generative	Method
models	Method
to	O
capture	O
rich	O
distributions	O
such	O
as	O
audio	O
,	O
images	O
or	O
video	O
is	O
one	O
of	O
the	O
central	O
challenges	O
of	O
machine	Task
learning	Task
.	O

Until	O
recently	O
,	O
deep	Method
generative	Method
models	Method
,	O
such	O
as	O
Restricted	Method
Boltzmann	Method
Machines	Method
(	O
RBM	Method
)	O
,	O
Deep	Method
Belief	Method
Networks	Method
(	O
DBNs	Method
)	O
and	O
Deep	Method
Boltzmann	Method
Machines	Method
(	O
DBMs	Method
)	O
were	O
trained	O
primarily	O
by	O
MCMC	Method
-	Method
based	Method
algorithms	Method
geoff	O
,	O
russ	Method
.	O

In	O
these	O
approaches	O
the	O
MCMC	Method
methods	Method
compute	O
the	O
gradient	Method
of	Method
log	Method
-	Method
likelihood	Method
which	O
becomes	O
more	O
imprecise	O
as	O
training	O
progresses	O
.	O

This	O
is	O
because	O
samples	O
from	O
the	O
Markov	Method
Chains	Method
are	O
unable	O
to	O
mix	O
between	O
modes	O
fast	O
enough	O
.	O

In	O
recent	O
years	O
,	O
generative	Method
models	Method
have	O
been	O
developed	O
that	O
may	O
be	O
trained	O
via	O
direct	Method
back	Method
-	Method
propagation	Method
and	O
avoid	O
the	O
difficulties	O
that	O
come	O
with	O
MCMC	Method
training	Method
.	O

For	O
example	O
,	O
variational	Method
autoencoders	Method
(	O
VAE	Method
)	O
vae	Method
,	O
rezende	O
or	O
importance	O
weighted	Method
autoencoders	Method
yuri	Method
use	O
a	O
recognition	Method
network	Method
to	O
predict	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
,	O
generative	Method
adversarial	Method
networks	Method
(	O
GAN	Method
)	O
gan	Method
use	O
an	O
adversarial	Method
training	Method
procedure	Method
to	O
directly	O
shape	O
the	O
output	O
distribution	O
of	O
the	O
network	O
via	O
back	Method
-	Method
propagation	Method
and	O
generative	Method
moment	Method
matching	Method
networks	Method
(	O
GMMN	Method
)	O
gmmn	Method
use	O
a	O
moment	Method
matching	Method
cost	Method
function	Method
to	O
learn	O
the	O
data	O
distribution	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
general	O
approach	O
,	O
called	O
an	O
adversarial	Method
autoencoder	Method
(	O
AAE	Method
)	O
that	O
can	O
turn	O
an	O
autoencoder	Method
into	O
a	O
generative	Method
model	Method
.	O

In	O
our	O
model	O
,	O
an	O
autoencoder	Method
is	O
trained	O
with	O
dual	O
objectives	O
–	O
a	O
traditional	O
reconstruction	Metric
error	Metric
criterion	Metric
,	O
and	O
an	O
adversarial	Method
training	Method
criterion	Method
gan	Method
that	O
matches	O
the	O
aggregated	O
posterior	O
distribution	O
of	O
the	O
latent	Method
representation	Method
of	O
the	O
autoencoder	Method
to	O
an	O
arbitrary	O
prior	O
distribution	O
.	O

We	O
show	O
that	O
this	O
training	O
criterion	O
has	O
a	O
strong	O
connection	O
to	O
VAE	Method
training	O
.	O

The	O
result	O
of	O
the	O
training	O
is	O
that	O
the	O
encoder	Method
learns	O
to	O
convert	O
the	O
data	O
distribution	O
to	O
the	O
prior	O
distribution	O
,	O
while	O
the	O
decoder	Method
learns	O
a	O
deep	Method
generative	Method
model	Method
that	O
maps	O
the	O
imposed	O
prior	O
to	O
the	O
data	O
distribution	O
.	O

subsection	O
:	O
Generative	Method
Adversarial	Method
Networks	Method
The	O
Generative	Method
Adversarial	Method
Networks	Method
(	O
GAN	Method
)	O
gan	Method
framework	O
establishes	O
a	O
min	Method
-	Method
max	Method
adversarial	Method
game	Method
between	O
two	O
neural	Method
networks	Method
–	O
a	O
generative	Method
model	Method
,	O
,	O
and	O
a	O
discriminative	Method
model	Method
,	O
.	O

The	O
discriminator	Method
model	Method
,	O
,	O
is	O
a	O
neural	Method
network	Method
that	O
computes	O
the	O
probability	O
that	O
a	O
point	O
in	O
data	O
space	O
is	O
a	O
sample	O
from	O
the	O
data	O
distribution	O
(	O
positive	O
samples	O
)	O
that	O
we	O
are	O
trying	O
to	O
model	O
,	O
rather	O
than	O
a	O
sample	O
from	O
our	O
generative	Method
model	Method
(	O
negative	O
samples	O
)	O
.	O

Concurrently	O
,	O
the	O
generator	O
uses	O
a	O
function	O
that	O
maps	O
samples	O
from	O
the	O
prior	O
to	O
the	O
data	O
space	O
.	O

is	O
trained	O
to	O
maximally	O
confuse	O
the	O
discriminator	O
into	O
believing	O
that	O
samples	O
it	O
generates	O
come	O
from	O
the	O
data	O
distribution	O
.	O

The	O
generator	O
is	O
trained	O
by	O
leveraging	O
the	O
gradient	Method
of	Method
w.r.t	Method
.	O

,	O
and	O
using	O
that	O
to	O
modify	O
its	O
parameters	O
.	O

The	O
solution	O
to	O
this	O
game	O
can	O
be	O
expressed	O
as	O
following	O
gan	Method
:	O
The	O
generator	Method
and	O
the	O
discriminator	Method
can	O
be	O
found	O
using	O
alternating	Method
SGD	Method
in	O
two	O
stages	O
:	O
(	O
a	O
)	O
Train	O
the	O
discriminator	Method
to	O
distinguish	O
the	O
true	O
samples	O
from	O
the	O
fake	O
samples	O
generated	O
by	O
the	O
generator	O
.	O

(	O
b	O
)	O
Train	O
the	O
generator	Method
so	O
as	O
to	O
fool	O
the	O
discriminator	Method
with	O
its	O
generated	O
samples	O
.	O

section	O
:	O
Adversarial	Method
Autoencoders	Method
Let	O
be	O
the	O
input	O
and	O
be	O
the	O
latent	O
code	O
vector	O
(	O
hidden	O
units	O
)	O
of	O
an	O
autoencoder	Method
with	O
a	O
deep	Method
encoder	Method
and	Method
decoder	Method
.	O

Let	O
be	O
the	O
prior	O
distribution	O
we	O
want	O
to	O
impose	O
on	O
the	O
codes	O
,	O
be	O
an	O
encoding	Method
distribution	Method
and	O
be	O
the	O
decoding	Method
distribution	Method
.	O

Also	O
let	O
be	O
the	O
data	O
distribution	O
,	O
and	O
be	O
the	O
model	Method
distribution	Method
.	O

The	O
encoding	Method
function	Method
of	O
the	O
autoencoder	Method
defines	O
an	O
aggregated	O
posterior	O
distribution	O
of	O
on	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	Method
as	O
follows	O
:	O
The	O
adversarial	Method
autoencoder	Method
is	O
an	O
autoencoder	Method
that	O
is	O
regularized	O
by	O
matching	O
the	O
aggregated	O
posterior	O
,	O
,	O
to	O
an	O
arbitrary	O
prior	O
,	O
.	O

In	O
order	O
to	O
do	O
so	O
,	O
an	O
adversarial	Method
network	Method
is	O
attached	O
on	O
top	O
of	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	Method
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

It	O
is	O
the	O
adversarial	Method
network	Method
that	O
guides	O
to	O
match	O
.	O

The	O
autoencoder	Method
,	O
meanwhile	O
,	O
attempts	O
to	O
minimize	O
the	O
reconstruction	Metric
error	Metric
.	O

The	O
generator	O
of	O
the	O
adversarial	Method
network	Method
is	O
also	O
the	O
encoder	O
of	O
the	O
autoencoder	Method
.	O

The	O
encoder	Method
ensures	O
the	O
aggregated	O
posterior	O
distribution	O
can	O
fool	O
the	O
discriminative	Method
adversarial	Method
network	Method
into	O
thinking	O
that	O
the	O
hidden	O
code	O
comes	O
from	O
the	O
true	O
prior	O
distribution	O
.	O

Both	O
,	O
the	O
adversarial	Method
network	Method
and	O
the	O
autoencoder	Method
are	O
trained	O
jointly	O
with	O
SGD	Method
in	O
two	O
phases	O
–	O
the	O
reconstruction	Method
phase	Method
and	O
the	O
regularization	Method
phase	Method
–	O
executed	O
on	O
each	O
mini	O
-	O
batch	O
.	O

In	O
the	O
reconstruction	Task
phase	Task
,	O
the	O
autoencoder	Method
updates	O
the	O
encoder	Method
and	O
the	O
decoder	Method
to	O
minimize	O
the	O
reconstruction	Metric
error	Metric
of	O
the	O
inputs	O
.	O

In	O
the	O
regularization	Task
phase	Task
,	O
the	O
adversarial	Method
network	Method
first	O
updates	O
its	O
discriminative	Method
network	Method
to	O
tell	O
apart	O
the	O
true	O
samples	O
(	O
generated	O
using	O
the	O
prior	O
)	O
from	O
the	O
generated	O
samples	O
(	O
the	O
hidden	O
codes	O
computed	O
by	O
the	O
autoencoder	Method
)	O
.	O

The	O
adversarial	Method
network	Method
then	O
updates	O
its	O
generator	Method
(	O
which	O
is	O
also	O
the	O
encoder	O
of	O
the	O
autoencoder	Method
)	O
to	O
confuse	O
the	O
discriminative	Method
network	Method
.	O

Once	O
the	O
training	O
procedure	O
is	O
done	O
,	O
the	O
decoder	Method
of	O
the	O
autoencoder	Method
will	O
define	O
a	O
generative	Method
model	Method
that	O
maps	O
the	O
imposed	O
prior	O
of	O
to	O
the	O
data	O
distribution	O
.	O

There	O
are	O
several	O
possible	O
choices	O
for	O
the	O
encoder	Method
,	O
,	O
of	O
adversarial	Method
autoencoders	Method
:	O
Deterministic	O
:	O
Here	O
we	O
assume	O
that	O
is	O
a	O
deterministic	O
function	O
of	O
.	O

In	O
this	O
case	O
,	O
the	O
encoder	O
is	O
similar	O
to	O
the	O
encoder	Method
of	O
a	O
standard	O
autoencoder	Method
and	O
the	O
only	O
source	O
of	O
stochasticity	O
in	O
is	O
the	O
data	O
distribution	O
,	O
.	O

Gaussian	Method
posterior	Method
:	O
Here	O
we	O
assume	O
that	O
is	O
a	O
Gaussian	O
distribution	O
whose	O
mean	O
and	O
variance	O
is	O
predicted	O
by	O
the	O
encoder	Method
network	Method
:	O
.	O

In	O
this	O
case	O
,	O
the	O
stochasticity	O
in	O
comes	O
from	O
both	O
the	O
data	O
-	O
distribution	O
and	O
the	O
randomness	O
of	O
the	O
Gaussian	Method
distribution	Method
at	O
the	O
output	O
of	O
the	O
encoder	Method
.	O

We	O
can	O
use	O
the	O
same	O
re	Method
-	Method
parametrization	Method
trick	Method
of	O
vae	Method
for	O
back	Task
-	Task
propagation	Task
through	O
the	O
encoder	Method
network	Method
.	O

Universal	Method
approximator	Method
posterior	Method
:	O
Adversarial	Method
autoencoders	Method
can	O
be	O
used	O
to	O
train	O
the	O
as	O
the	O
universal	Method
approximator	Method
of	Method
the	Method
posterior	Method
.	O

Suppose	O
the	O
encoder	Method
network	Method
of	O
the	O
adversarial	Method
autoencoder	Method
is	O
the	O
function	O
that	O
takes	O
the	O
input	O
and	O
a	O
random	O
noise	O
with	O
a	O
fixed	O
distribution	O
(	O
e.g.	O
,	O
Gaussian	O
)	O
.	O

We	O
can	O
sample	O
from	O
arbitrary	O
posterior	O
distribution	O
,	O
by	O
evaluating	O
at	O
different	O
samples	O
of	O
.	O

In	O
other	O
words	O
,	O
we	O
can	O
assume	O
and	O
the	O
posterior	O
and	O
the	O
aggregated	O
posterior	O
are	O
defined	O
as	O
follows	O
:	O
In	O
this	O
case	O
,	O
the	O
stochasticity	O
in	O
comes	O
from	O
both	O
the	O
data	O
-	O
distribution	O
and	O
the	O
random	O
noise	O
at	O
the	O
input	O
of	O
the	O
encoder	Method
.	O

Note	O
that	O
in	O
this	O
case	O
the	O
posterior	O
is	O
no	O
longer	O
constrained	O
to	O
be	O
Gaussian	O
and	O
the	O
encoder	Method
can	O
learn	O
any	O
arbitrary	O
posterior	O
distribution	O
for	O
a	O
given	O
input	O
.	O

Since	O
there	O
is	O
an	O
efficient	O
method	O
of	O
sampling	O
from	O
the	O
aggregated	O
posterior	O
,	O
the	O
adversarial	Method
training	Method
procedure	Method
can	O
match	O
to	O
by	O
direct	O
back	Method
-	Method
propagation	Method
through	O
the	O
encoder	Method
network	Method
.	O

Choosing	O
different	O
types	O
of	O
will	O
result	O
in	O
different	O
kinds	O
of	O
models	O
with	O
different	O
training	O
dynamics	O
.	O

For	O
example	O
,	O
in	O
the	O
deterministic	O
case	O
of	O
,	O
the	O
network	O
has	O
to	O
match	O
to	O
by	O
only	O
exploiting	O
the	O
stochasticity	O
of	O
the	O
data	O
distribution	O
,	O
but	O
since	O
the	O
empirical	O
distribution	O
of	O
the	O
data	O
is	O
fixed	O
by	O
the	O
training	O
set	O
,	O
and	O
the	O
mapping	Method
is	O
deterministic	O
,	O
this	O
might	O
produce	O
a	O
that	O
is	O
not	O
very	O
smooth	O
.	O

However	O
,	O
in	O
the	O
Gaussian	Task
or	Task
universal	Task
approximator	Task
case	Task
,	O
the	O
network	O
has	O
access	O
to	O
additional	O
sources	O
of	O
stochasticity	O
that	O
could	O
help	O
it	O
in	O
the	O
adversarial	Method
regularization	Method
stage	O
by	O
smoothing	O
out	O
.	O

Nevertheless	O
,	O
after	O
extensive	O
hyper	O
-	O
parameter	O
search	O
,	O
we	O
obtained	O
similar	O
test	O
-	O
likelihood	O
with	O
each	O
type	O
of	O
.	O

So	O
in	O
the	O
rest	O
of	O
the	O
paper	O
,	O
we	O
only	O
report	O
results	O
with	O
the	O
deterministic	O
version	O
of	O
.	O

subsection	O
:	O
Relationship	O
to	O
Variational	Method
Autoencoders	Method
Our	O
work	O
is	O
similar	O
in	O
spirit	O
to	O
variational	O
autoencoders	O
vae	Method
;	O
however	O
,	O
while	O
they	O
use	O
a	O
KL	O
divergence	O
penalty	O
to	O
impose	O
a	O
prior	O
distribution	O
on	O
the	O
hidden	O
code	O
vector	O
of	O
the	O
autoencoder	Method
,	O
we	O
use	O
an	O
adversarial	Method
training	Method
procedure	Method
to	O
do	O
so	O
by	O
matching	O
the	O
aggregated	O
posterior	O
of	O
the	O
hidden	O
code	O
vector	O
with	O
the	O
prior	O
distribution	O
.	O

VAE	Method
vae	Method
minimizes	O
the	O
following	O
upper	O
-	O
bound	O
on	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
:	O
where	O
the	O
aggregated	O
posterior	O
is	O
defined	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
we	O
have	O
assumed	O
is	O
Gaussian	O
and	O
is	O
an	O
arbitrary	O
distribution	O
.	O

The	O
variational	Method
bound	Method
contains	O
three	O
terms	O
.	O

The	O
first	O
term	O
can	O
be	O
viewed	O
as	O
the	O
reconstruction	O
term	O
of	O
an	O
autoencoder	Method
and	O
the	O
second	O
and	O
third	O
terms	O
can	O
be	O
viewed	O
as	O
regularization	O
terms	O
.	O

Without	O
the	O
regularization	O
terms	O
,	O
the	O
model	O
is	O
simply	O
a	O
standard	O
autoencoder	Method
that	O
reconstructs	O
the	O
input	O
.	O

However	O
,	O
in	O
the	O
presence	O
of	O
the	O
regularization	O
terms	O
,	O
the	O
VAE	Method
learns	O
a	O
latent	Method
representation	Method
that	O
is	O
compatible	O
with	O
.	O

The	O
second	O
term	O
of	O
the	O
cost	O
function	O
encourages	O
large	O
variances	O
for	O
the	O
posterior	O
distribution	O
while	O
the	O
third	O
term	O
minimizes	O
the	O
cross	Metric
-	Metric
entropy	Metric
between	O
the	O
aggregated	O
posterior	O
and	O
the	O
prior	O
.	O

KL	O
divergence	O
or	O
the	O
cross	O
-	O
entropy	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
encourages	O
to	O
pick	O
the	O
modes	O
of	O
.	O

In	O
adversarial	Method
autoencoders	Method
,	O
we	O
replace	O
the	O
second	O
two	O
terms	O
with	O
an	O
adversarial	Method
training	Method
procedure	Method
that	O
encourages	O
to	O
match	O
to	O
the	O
whole	O
distribution	O
of	O
.	O

In	O
this	O
section	O
,	O
we	O
compare	O
the	O
ability	O
of	O
the	O
adversarial	Method
autoencoder	Method
to	O
the	O
VAE	Method
to	O
impose	O
a	O
specified	O
prior	O
distribution	O
on	O
the	O
coding	O
distribution	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
shows	O
the	O
coding	O
space	O
of	O
the	O
test	O
data	O
resulting	O
from	O
an	O
adversarial	Method
autoencoder	Method
trained	O
on	O
MNIST	Material
digits	O
in	O
which	O
a	O
spherical	O
2	O
-	O
D	O
Gaussian	O
prior	O
distribution	O
is	O
imposed	O
on	O
the	O
hidden	O
codes	O
.	O

The	O
learned	O
manifold	O
in	O
Figure	O
[	O
reference	O
]	O
a	O
exhibits	O
sharp	O
transitions	O
indicating	O
that	O
the	O
coding	O
space	O
is	O
filled	O
and	O
exhibits	O
no	O
‘	O
‘	O
holes	O
’	O
’	O
.	O

In	O
practice	O
,	O
sharp	O
transitions	O
in	O
the	O
coding	O
space	O
indicate	O
that	O
images	O
generated	O
by	O
interpolating	O
within	O
lie	O
on	O
the	O
data	O
manifold	O
(	O
Figure	O
[	O
reference	O
]	O
e	O
)	O
.	O

By	O
contrast	O
,	O
Figure	O
[	O
reference	O
]	O
c	O
shows	O
the	O
coding	O
space	O
of	O
a	O
VAE	Method
with	O
the	O
same	O
architecture	O
used	O
in	O
the	O
adversarial	Method
autoencoder	Method
experiments	O
.	O

We	O
can	O
see	O
that	O
in	O
this	O
case	O
the	O
VAE	Method
roughly	O
matches	O
the	O
shape	O
of	O
a	O
2	Method
-	Method
D	Method
Gaussian	Method
distribution	Method
.	O

However	O
,	O
no	O
data	O
points	O
map	O
to	O
several	O
local	O
regions	O
of	O
the	O
coding	O
space	O
indicating	O
that	O
the	O
VAE	Method
may	O
not	O
have	O
captured	O
the	O
data	O
manifold	O
as	O
well	O
as	O
the	O
adversarial	Method
autoencoder	Method
.	O

Figures	O
[	O
reference	O
]	O
b	O
and	O
[	O
reference	O
]	O
d	O
show	O
the	O
code	O
space	O
of	O
an	O
adversarial	Method
autoencoder	Method
and	O
of	O
a	O
VAE	Method
where	O
the	O
imposed	O
distribution	O
is	O
a	O
mixture	Method
of	Method
10	Method
2	Method
-	Method
D	Method
Gaussians	Method
.	O

The	O
adversarial	Method
autoencoder	Method
successfully	O
matched	O
the	O
aggregated	O
posterior	O
with	O
the	O
prior	O
distribution	O
(	O
Figure	O
[	O
reference	O
]	O
b	O
)	O
.	O

In	O
contrast	O
,	O
the	O
VAE	Method
exhibit	O
systematic	O
differences	O
from	O
the	O
mixture	Method
10	Method
Gaussians	Method
indicating	O
that	O
the	O
VAE	Method
emphasizes	O
matching	O
the	O
modes	O
of	O
the	O
distribution	O
as	O
discussed	O
above	O
(	O
Figure	O
[	O
reference	O
]	O
d	O
)	O
.	O

An	O
important	O
difference	O
between	O
VAEs	Method
and	O
adversarial	Method
autoencoders	Method
is	O
that	O
in	O
VAEs	Method
,	O
in	O
order	O
to	O
back	O
-	O
propagate	O
through	O
the	O
KL	O
divergence	O
by	O
Monte	Method
-	Method
Carlo	Method
sampling	Method
,	O
we	O
need	O
to	O
have	O
access	O
to	O
the	O
exact	O
functional	O
form	O
of	O
the	O
prior	O
distribution	O
.	O

However	O
,	O
in	O
AAEs	Method
,	O
we	O
only	O
need	O
to	O
be	O
able	O
to	O
sample	O
from	O
the	O
prior	O
distribution	O
in	O
order	O
to	O
induce	O
to	O
match	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
demonstrate	O
that	O
the	O
adversarial	Method
autoencoder	Method
can	O
impose	O
complicated	O
distributions	O
(	O
e.g.	O
,	O
swiss	O
roll	O
distribution	O
)	O
without	O
having	O
access	O
to	O
the	O
explicit	O
functional	O
form	O
of	O
the	O
distribution	O
.	O

subsection	O
:	O
Relationship	O
to	O
GANs	Method
and	O
GMMNs	Method
In	O
the	O
original	O
generative	Method
adversarial	Method
networks	Method
(	O
GAN	Method
)	O
paper	O
gan	Method
,	O
GANs	Method
were	O
used	O
to	O
impose	O
the	O
data	O
distribution	O
at	O
the	O
pixel	O
level	O
on	O
the	O
output	O
layer	O
of	O
a	O
neural	Method
network	Method
.	O

Adversarial	Method
autoencoders	Method
,	O
however	O
,	O
rely	O
on	O
the	O
autoencoder	Method
training	Method
to	O
capture	O
the	O
data	O
distribution	O
.	O

In	O
adversarial	Method
training	Method
procedure	Method
of	O
our	O
method	O
,	O
a	O
much	O
simpler	O
distribution	O
(	O
e.g.	O
,	O
Gaussian	O
as	O
opposed	O
to	O
the	O
data	O
distribution	O
)	O
is	O
imposed	O
in	O
a	O
much	O
lower	O
dimensional	O
space	O
(	O
e.g.	O
,	O
as	O
opposed	O
to	O
)	O
which	O
results	O
in	O
a	O
better	O
test	Metric
-	Metric
likelihood	Metric
as	O
is	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Generative	Method
moment	Method
matching	Method
networks	Method
(	O
GMMN	Method
)	O
gmmn	Method
use	O
the	O
maximum	Method
mean	Method
discrepancy	Method
(	O
MMD	Method
)	O
objective	O
to	O
shape	O
the	O
distribution	O
of	O
the	O
output	O
layer	O
of	O
a	O
neural	Method
network	Method
.	O

The	O
MMD	Method
objective	O
can	O
be	O
interpreted	O
as	O
minimizing	O
the	O
distance	O
between	O
all	O
moments	O
of	O
the	O
model	Method
distribution	Method
and	O
the	O
data	O
distribution	O
.	O

It	O
has	O
been	O
shown	O
that	O
GMMNs	Method
can	O
be	O
combined	O
with	O
pre	O
-	O
trained	O
dropout	Method
autoencoders	Method
to	O
achieve	O
better	O
likelihood	O
results	O
(	O
GMMN	Method
+	Method
AE	Method
)	O
.	O

Our	O
adversarial	Method
autoencoder	Method
also	O
relies	O
on	O
the	O
autoencoder	Method
to	O
capture	O
the	O
data	O
distribution	O
.	O

However	O
,	O
the	O
main	O
difference	O
of	O
our	O
work	O
with	O
GMMN	Method
+	Method
AE	Method
is	O
that	O
the	O
adversarial	Method
training	Method
procedure	Method
of	O
our	O
method	O
acts	O
as	O
a	O
regularizer	Method
that	O
shapes	O
the	O
code	O
distribution	O
while	O
training	O
the	O
autoencoder	Method
from	O
scratch	O
;	O
whereas	O
,	O
the	O
GMMN	Method
+	O
AE	O
model	O
first	O
trains	O
a	O
standard	O
dropout	Method
autoencoder	Method
and	O
then	O
fits	O
a	O
distribution	O
in	O
the	O
code	O
space	O
of	O
the	O
pre	O
-	O
trained	O
network	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
show	O
that	O
the	O
test	Metric
-	Metric
likelihood	Metric
achieved	O
by	O
the	O
joint	Method
training	Method
scheme	Method
of	O
adversarial	Method
autoencoders	Method
outperforms	O
the	O
test	Metric
-	Metric
likelihood	Metric
of	O
GMMN	Method
and	O
GMMN	Method
+	Method
AE	Method
on	O
MNIST	Material
and	O
Toronto	Material
Face	Material
datasets	Material
.	O

subsection	O
:	O
Incorporating	O
Label	O
Information	O
in	O
the	O
Adversarial	Task
Regularization	Task
In	O
the	O
scenarios	O
where	O
data	O
is	O
labeled	O
,	O
we	O
can	O
incorporate	O
the	O
label	O
information	O
in	O
the	O
adversarial	Method
training	Method
stage	Method
to	O
better	O
shape	O
the	O
distribution	O
of	O
the	O
hidden	O
code	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
how	O
to	O
leverage	O
partial	O
or	O
complete	O
label	O
information	O
to	O
regularize	O
the	O
latent	Method
representation	Method
of	O
the	O
autoencoder	Method
more	O
heavily	O
.	O

To	O
demonstrate	O
this	O
architecture	O
we	O
return	O
to	O
Figure	O
[	O
reference	O
]	O
b	O
in	O
which	O
the	O
adversarial	Method
autoencoder	Method
is	O
fit	O
to	O
a	O
mixture	Method
of	Method
10	Method
2	Method
-	Method
D	Method
Gaussians	Method
.	O

We	O
now	O
aim	O
to	O
force	O
each	O
mode	O
of	O
the	O
mixture	Method
of	Method
Gaussian	Method
distribution	Method
to	O
represent	O
a	O
single	O
label	O
of	O
MNIST	Material
.	O

Figure	O
[	O
reference	O
]	O
demonstrates	O
the	O
training	O
procedure	O
for	O
this	O
semi	Method
-	Method
supervised	Method
approach	Method
.	O

We	O
add	O
a	O
one	O
-	O
hot	O
vector	O
to	O
the	O
input	O
of	O
the	O
discriminative	Method
network	Method
to	O
associate	O
the	O
label	O
with	O
a	O
mode	O
of	O
the	O
distribution	O
.	O

The	O
one	O
-	O
hot	O
vector	O
acts	O
as	O
switch	O
that	O
selects	O
the	O
corresponding	O
decision	O
boundary	O
of	O
the	O
discriminative	Method
network	Method
given	O
the	O
class	O
label	O
.	O

This	O
one	O
-	O
hot	O
vector	O
has	O
an	O
extra	O
class	O
for	O
unlabeled	O
examples	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
imposing	O
a	O
mixture	O
of	O
10	O
2	Method
-	Method
D	Method
Gaussians	Method
(	O
Figure	O
[	O
reference	O
]	O
b	O
and	O
[	O
reference	O
]	O
a	O
)	O
,	O
the	O
one	O
hot	O
vector	O
contains	O
11	O
classes	O
.	O

Each	O
of	O
the	O
first	O
10	O
class	O
selects	O
a	O
decision	O
boundary	O
for	O
the	O
corresponding	O
individual	O
mixture	Method
component	Method
.	O

The	O
extra	O
class	O
in	O
the	O
one	O
-	O
hot	O
vector	O
corresponds	O
to	O
unlabeled	O
training	O
points	O
.	O

When	O
an	O
unlabeled	O
point	O
is	O
presented	O
to	O
the	O
model	O
,	O
the	O
extra	O
class	O
is	O
turned	O
on	O
,	O
to	O
select	O
the	O
decision	O
boundary	O
for	O
the	O
full	Method
mixture	Method
of	Method
Gaussian	Method
distribution	Method
.	O

During	O
the	O
positive	Task
phase	Task
of	O
adversarial	Task
training	Task
,	O
we	O
provide	O
the	O
label	O
of	O
the	O
mixture	Method
component	Method
(	O
that	O
the	O
positive	O
sample	O
is	O
drawn	O
from	O
)	O
to	O
the	O
discriminator	O
through	O
the	O
one	Method
-	Method
hot	Method
vector	Method
.	O

The	O
positive	O
samples	O
fed	O
for	O
unlabeled	O
examples	O
come	O
from	O
the	O
full	O
mixture	Method
of	Method
Gaussian	Method
,	O
rather	O
than	O
from	O
a	O
particular	O
class	O
.	O

During	O
the	O
negative	Task
phase	Task
,	O
we	O
provide	O
the	O
label	O
of	O
the	O
training	O
point	O
image	O
to	O
the	O
discriminator	O
through	O
the	O
one	Method
-	Method
hot	Method
vector	Method
.	O

Figure	O
[	O
reference	O
]	O
a	O
shows	O
the	O
latent	Method
representation	Method
of	O
an	O
adversarial	Method
autoencoder	Method
trained	O
with	O
a	O
prior	O
that	O
is	O
a	O
mixture	O
of	O
10	O
2	Method
-	Method
D	Method
Gaussians	Method
trained	O
on	O
10	O
K	O
labeled	O
MNIST	Material
examples	O
and	O
40	O
K	O
unlabeled	O
MNIST	Material
examples	O
.	O

In	O
this	O
case	O
,	O
the	O
-	O
th	O
mixture	Method
component	Method
of	O
the	O
prior	O
has	O
been	O
assigned	O
to	O
the	O
-	O
th	O
class	O
in	O
a	O
semi	Method
-	Method
supervised	Method
fashion	Method
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
the	O
manifold	O
of	O
the	O
first	O
three	O
mixture	Method
components	Method
.	O

Note	O
that	O
the	O
style	Method
representation	Method
is	O
consistently	O
represented	O
within	O
each	O
mixture	Method
component	Method
,	O
independent	O
of	O
its	O
class	O
.	O

For	O
example	O
,	O
the	O
upper	O
-	O
left	O
region	O
of	O
all	O
panels	O
in	O
Figure	O
[	O
reference	O
]	O
b	O
correspond	O
to	O
the	O
upright	O
writing	O
style	O
and	O
lower	O
-	O
right	O
region	O
of	O
these	O
panels	O
correspond	O
to	O
the	O
tilted	O
writing	O
style	O
of	O
digits	O
.	O

This	O
method	O
may	O
be	O
extended	O
to	O
arbitrary	O
distributions	O
with	O
no	O
parametric	O
forms	O
–	O
as	O
demonstrated	O
by	O
mapping	O
the	O
MNIST	Material
data	O
set	O
onto	O
a	O
‘	O
‘	O
swiss	O
roll	O
’	O
’	O
(	O
a	O
conditional	Method
Gaussian	Method
distribution	Method
whose	O
mean	O
is	O
uniformly	O
distributed	O
along	O
the	O
length	O
of	O
a	O
swiss	O
roll	O
axis	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
depicts	O
the	O
coding	O
space	O
and	O
Figure	O
[	O
reference	O
]	O
d	O
highlights	O
the	O
images	O
generated	O
by	O
walking	O
along	O
the	O
swiss	O
roll	O
axis	O
in	O
the	O
latent	O
space	O
.	O

section	O
:	O
Likelihood	Method
Analysis	Method
of	O
Adversarial	Method
Autoencoders	Method
The	O
experiments	O
presented	O
in	O
the	O
previous	O
sections	O
have	O
only	O
demonstrated	O
qualitative	O
results	O
.	O

In	O
this	O
section	O
we	O
measure	O
the	O
ability	O
of	O
the	O
AAE	Method
as	O
a	O
generative	Method
model	Method
to	O
capture	O
the	O
data	O
distribution	O
by	O
comparing	O
the	O
likelihood	O
of	O
this	O
model	O
to	O
generate	O
hold	O
-	O
out	O
images	O
on	O
the	O
MNIST	Material
and	O
Toronto	Material
face	Material
dataset	Material
(	O
TFD	Material
)	O
using	O
the	O
evaluation	O
procedure	O
described	O
in	O
gan	Method
.	O

We	O
trained	O
an	O
adversarial	Method
autoencoder	Method
on	O
MNIST	Material
and	O
TFD	Material
in	O
which	O
the	O
model	O
imposed	O
a	O
high	O
-	O
dimensional	O
Gaussian	O
distribution	O
on	O
the	O
underlying	O
hidden	O
code	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
samples	O
drawn	O
from	O
the	O
adversarial	Method
autoencoder	Method
trained	O
on	O
these	O
datasets	O
.	O

A	O
video	O
showing	O
the	O
learnt	O
TFD	Material
manifold	O
can	O
be	O
found	O
at	O
.	O

To	O
determine	O
whether	O
the	O
model	O
is	O
over	O
-	O
fitting	O
by	O
copying	O
the	O
training	O
data	O
points	O
,	O
we	O
used	O
the	O
last	O
column	O
of	O
these	O
figures	O
to	O
show	O
the	O
nearest	O
neighbors	O
,	O
in	O
Euclidean	O
distance	O
,	O
to	O
the	O
generative	Method
model	Method
samples	O
in	O
the	O
second	O
-	O
to	O
-	O
last	O
column	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
the	O
adversarial	Method
autoencoder	Method
by	O
computing	O
its	O
log	Method
-	Method
likelihood	Method
on	O
the	O
hold	O
out	O
test	O
set	O
.	O

Evaluation	O
of	O
the	O
model	O
using	O
likelihood	O
is	O
not	O
straightforward	O
because	O
we	O
can	O
not	O
directly	O
compute	O
the	O
probability	O
of	O
an	O
image	O
.	O

Thus	O
,	O
we	O
calculate	O
a	O
lower	Metric
bound	Metric
of	O
the	O
true	Metric
log	Metric
-	Metric
likelihood	Metric
using	O
the	O
methods	O
described	O
in	O
prior	O
work	O
stacked_cae	O
,	O
gsn	Method
,	O
gan	Method
.	O

We	O
fit	O
a	O
Gaussian	Method
Parzen	Method
window	Method
(	O
kernel	Method
density	Method
estimator	Method
)	O
to	O
samples	O
generated	O
from	O
the	O
model	O
and	O
compute	O
the	O
likelihood	O
of	O
the	O
test	O
data	O
under	O
this	O
distribution	O
.	O

The	O
free	O
-	O
parameter	O
of	O
the	O
Parzen	Method
window	Method
is	O
selected	O
via	O
cross	Method
-	Method
validation	Method
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
log	Metric
-	Metric
likelihood	Metric
of	O
the	O
adversarial	Method
autoencoder	Method
for	O
real	Material
-	Material
valued	Material
MNIST	Material
and	O
TFD	Material
to	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
including	O
DBN	Method
geoff	Method
,	O
Stacked	Method
CAE	Method
stacked_cae	Method
,	O
Deep	Method
GSN	Method
gsn	Method
,	O
Generative	Method
Adversarial	Method
Networks	Method
gan	Method
and	O
GMMN	Method
+	O
AE	O
gmmn	Method
.	O

Note	O
that	O
the	O
Parzen	Method
window	Method
estimate	Method
is	O
a	O
lower	O
bound	O
on	O
the	O
true	Metric
log	Metric
-	Metric
likelihood	Metric
and	O
the	O
tightness	O
of	O
this	O
bound	O
depends	O
on	O
the	O
number	O
of	O
samples	O
drawn	O
.	O

To	O
obtain	O
a	O
comparison	O
with	O
a	O
tighter	O
lower	O
bound	O
,	O
we	O
additionally	O
report	O
Parzen	Method
window	Method
estimates	Method
evaluated	O
with	O
10	O
million	O
samples	O
for	O
both	O
the	O
adversarial	Method
autoencoders	Method
and	O
the	O
generative	Method
adversarial	Method
network	Method
gan	Method
.	O

In	O
all	O
comparisons	O
we	O
find	O
that	O
the	O
adversarial	Method
autoencoder	Method
achieves	O
superior	O
log	Metric
-	Metric
likelihoods	Metric
to	O
competing	O
methods	O
.	O

However	O
,	O
the	O
reader	O
must	O
be	O
aware	O
that	O
the	O
metrics	O
currently	O
available	O
for	O
evaluating	O
the	O
likelihood	O
of	O
generative	Method
models	Method
such	O
as	O
GANs	Method
are	O
deeply	O
flawed	O
.	O

Theis	O
et	O
al	O
.	O

theis	O
detail	O
the	O
problems	O
with	O
such	O
metrics	O
,	O
including	O
the	O
10	Method
K	Method
and	Method
10	Method
M	Method
sample	Method
Parzen	Method
window	Method
estimate	Method
.	O

section	O
:	O
Supervised	O
Adversarial	Method
Autoencoders	Method
Semi	Task
-	Task
supervised	Task
learning	Task
is	O
a	O
long	O
-	O
standing	O
conceptual	Task
problem	Task
in	O
machine	Task
learning	Task
.	O

Recently	O
,	O
generative	Method
models	Method
have	O
become	O
one	O
of	O
the	O
most	O
popular	O
approaches	O
for	O
semi	Task
-	Task
supervised	Task
learning	Task
as	O
they	O
can	O
disentangle	O
the	O
class	O
label	O
information	O
from	O
many	O
other	O
latent	O
factors	O
of	O
variation	O
in	O
a	O
principled	O
way	O
semi	O
-	O
vae	Method
,	O
adgm	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
focus	O
on	O
the	O
fully	Task
supervised	Task
scenarios	Task
and	O
discuss	O
an	O
architecture	O
of	O
adversarial	Method
autoencoders	Method
that	O
can	O
separate	O
the	O
class	O
label	O
information	O
from	O
the	O
image	O
style	O
information	O
.	O

We	O
then	O
extend	O
this	O
architecture	O
to	O
the	O
semi	Task
-	Task
supervised	Task
settings	Task
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
order	O
to	O
incorporate	O
the	O
label	O
information	O
,	O
we	O
alter	O
the	O
network	Method
architecture	Method
of	O
Figure	O
[	O
reference	O
]	O
to	O
provide	O
a	O
one	Method
-	Method
hot	Method
vector	Method
encoding	Method
of	O
the	O
label	O
to	O
the	O
decoder	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
decoder	Method
utilizes	O
both	O
the	O
one	O
-	O
hot	O
vector	O
identifying	O
the	O
label	O
and	O
the	O
hidden	O
code	O
to	O
reconstruct	O
the	O
image	O
.	O

This	O
architecture	O
forces	O
the	O
network	O
to	O
retain	O
all	O
information	O
independent	O
of	O
the	O
label	O
in	O
the	O
hidden	O
code	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
demonstrates	O
the	O
results	O
of	O
such	O
a	O
network	O
trained	O
on	O
MNIST	Material
digits	O
in	O
which	O
the	O
hidden	O
code	O
is	O
forced	O
into	O
a	O
15	Method
-	Method
D	Method
Gaussian	Method
.	O

Each	O
row	O
of	O
Figure	O
[	O
reference	O
]	O
a	O
presents	O
reconstructed	O
images	O
in	O
which	O
the	O
hidden	O
code	O
is	O
fixed	O
to	O
a	O
particular	O
value	O
but	O
the	O
label	O
is	O
systematically	O
explored	O
.	O

Note	O
that	O
the	O
style	O
of	O
the	O
reconstructed	O
images	O
is	O
consistent	O
across	O
a	O
given	O
row	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
demonstrates	O
the	O
same	O
experiment	O
applied	O
to	O
Street	Material
View	Material
House	Material
Numbers	Material
dataset	Material
svhn	Material
.	O

A	O
video	O
showing	O
the	O
learnt	O
SVHN	Material
style	O
manifold	O
can	O
be	O
found	O
at	O
.	O

In	O
this	O
experiment	O
,	O
the	O
one	O
-	O
hot	O
vector	O
represents	O
the	O
label	O
associated	O
with	O
the	O
central	O
digit	O
in	O
the	O
image	O
.	O

Note	O
that	O
the	O
style	O
information	O
in	O
each	O
row	O
contains	O
information	O
about	O
the	O
labels	O
of	O
the	O
left	O
-	O
most	O
and	O
right	O
-	O
most	O
digits	O
because	O
the	O
left	O
-	O
most	O
and	O
right	O
-	O
most	O
digits	O
are	O
not	O
provided	O
as	O
label	O
information	O
in	O
the	O
one	Method
-	Method
hot	Method
encoding	Method
.	O

section	O
:	O
Semi	O
-	O
Supervised	O
Adversarial	Method
Autoencoders	Method
Building	O
on	O
the	O
foundations	O
from	O
Section	O
[	O
reference	O
]	O
,	O
we	O
now	O
use	O
the	O
adversarial	Method
autoencoder	Method
to	O
develop	O
models	O
for	O
semi	Task
-	Task
supervised	Task
learning	Task
that	O
exploit	O
the	O
generative	Method
description	Method
of	O
the	O
unlabeled	O
data	O
to	O
improve	O
the	O
classification	Task
performance	O
that	O
would	O
be	O
obtained	O
by	O
using	O
only	O
the	O
labeled	O
data	O
.	O

Specifically	O
,	O
we	O
assume	O
the	O
data	O
is	O
generated	O
by	O
a	O
latent	O
class	O
variable	O
that	O
comes	O
from	O
a	O
Categorical	O
distribution	O
as	O
well	O
as	O
a	O
continuous	O
latent	O
variable	O
that	O
comes	O
from	O
a	O
Gaussian	Method
distribution	Method
:	O
We	O
alter	O
the	O
network	Method
architecture	Method
of	O
Figure	O
[	O
reference	O
]	O
so	O
that	O
the	O
inference	Method
network	Method
of	O
the	O
AAE	Method
predicts	O
both	O
the	O
discrete	O
class	O
variable	O
and	O
the	O
continuous	O
latent	O
variable	O
using	O
the	O
encoder	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
decoder	O
then	O
utilizes	O
both	O
the	O
class	O
label	O
as	O
a	O
one	O
-	O
hot	O
vector	O
and	O
the	O
continuous	Method
hidden	Method
code	Method
to	O
reconstruct	O
the	O
image	O
.	O

There	O
are	O
two	O
separate	O
adversarial	Method
networks	Method
that	O
regularize	O
the	O
hidden	Method
representation	Method
of	O
the	O
autoencoder	Method
.	O

The	O
first	O
adversarial	Method
network	Method
imposes	O
a	O
Categorical	O
distribution	O
on	O
the	O
label	Method
representation	Method
.	O

This	O
adversarial	Method
network	Method
ensures	O
that	O
the	O
latent	O
class	O
variable	O
does	O
not	O
carry	O
any	O
style	O
information	O
and	O
that	O
the	O
aggregated	O
posterior	O
distribution	O
of	O
matches	O
the	O
Categorical	O
distribution	O
.	O

The	O
second	O
adversarial	Method
network	Method
imposes	O
a	O
Gaussian	Method
distribution	Method
on	O
the	O
style	Method
representation	Method
which	O
ensures	O
the	O
latent	O
variable	O
is	O
a	O
continuous	O
Gaussian	O
variable	O
.	O

Both	O
of	O
the	O
adversarial	Method
networks	Method
as	O
well	O
as	O
the	O
autoencoder	Method
are	O
trained	O
jointly	O
with	O
SGD	Method
in	O
three	O
phases	O
–	O
the	O
reconstruction	Method
phase	Method
,	O
regularization	Method
phase	Method
and	O
the	O
semi	Task
-	Task
supervised	Task
classification	Task
phase	O
.	O

In	O
the	O
reconstruction	Task
phase	Task
,	O
the	O
autoencoder	Method
updates	O
the	O
encoder	Method
and	O
the	O
decoder	Method
to	O
minimize	O
the	O
reconstruction	Metric
error	Metric
of	O
the	O
inputs	O
on	O
an	O
unlabeled	O
mini	O
-	O
batch	O
.	O

In	O
the	O
regularization	Method
phase	Method
,	O
each	O
of	O
the	O
adversarial	Method
networks	Method
first	O
updates	O
their	O
discriminative	Method
network	Method
to	O
tell	O
apart	O
the	O
true	O
samples	O
(	O
generated	O
using	O
the	O
Categorical	O
and	O
Gaussian	O
priors	O
)	O
from	O
the	O
generated	O
samples	O
(	O
the	O
hidden	O
codes	O
computed	O
by	O
the	O
autoencoder	Method
)	O
.	O

The	O
adversarial	Method
networks	Method
then	O
update	O
their	O
generator	Method
to	O
confuse	O
their	O
discriminative	Method
networks	Method
.	O

In	O
the	O
semi	Task
-	Task
supervised	Task
classification	Task
phase	O
,	O
the	O
autoencoder	Method
updates	Method
to	O
minimize	O
the	O
cross	Metric
-	Metric
entropy	Metric
cost	Metric
on	O
a	O
labeled	O
mini	O
-	O
batch	O
.	O

The	O
results	O
of	O
semi	Task
-	Task
supervised	Task
classification	Task
experiments	O
on	O
MNIST	Material
and	O
SVHN	Material
datasets	O
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

On	O
the	O
MNIST	Material
dataset	O
with	O
100	O
and	O
1000	O
labels	O
,	O
the	O
performance	O
of	O
AAEs	Method
is	O
significantly	O
better	O
than	O
VAEs	Method
,	O
on	O
par	O
with	O
VAT	Method
vat	Method
and	O
CatGAN	Method
catgan	Method
,	O
but	O
is	O
outperformed	O
by	O
the	O
Ladder	Method
networks	Method
ladder	Method
and	O
the	O
ADGM	Method
adgm	Method
.	O

We	O
also	O
trained	O
a	O
supervised	Method
AAE	Method
model	Method
on	O
all	O
the	O
available	O
labels	O
,	O
and	O
obtained	O
the	O
error	Metric
rate	Metric
of	O
.	O

In	O
comparison	O
,	O
a	O
dropout	Method
supervised	Method
neural	Method
network	Method
with	O
the	O
same	O
architecture	O
achieves	O
the	O
error	Metric
rate	Metric
of	O
on	O
the	O
full	O
MNIST	Material
dataset	O
,	O
which	O
highlights	O
the	O
regularization	O
effect	O
of	O
the	O
adversarial	Method
training	Method
.	O

On	O
the	O
SVHN	Material
dataset	O
with	O
1000	O
labels	O
,	O
the	O
AAE	Method
almost	O
matches	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
classification	Metric
performance	Metric
achieved	O
by	O
the	O
ADGM	Method
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
all	O
the	O
AAE	Method
models	Method
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
whereas	O
the	O
semi	O
-	O
supervised	O
VAE	Method
models	O
have	O
to	O
be	O
trained	O
one	O
layer	O
at	O
a	O
time	O
semi	O
-	O
vae	Method
.	O

section	O
:	O
Unsupervised	Task
Clustering	Task
with	O
Adversarial	Method
Autoencoders	Method
In	O
the	O
previous	O
section	O
,	O
we	O
showed	O
that	O
with	O
a	O
limited	O
label	O
information	O
,	O
the	O
adversarial	Method
autoencoder	Method
is	O
able	O
to	O
learn	O
powerful	O
semi	Method
-	Method
supervised	Method
representations	Method
.	O

However	O
,	O
the	O
question	O
that	O
has	O
remained	O
unanswered	O
is	O
whether	O
it	O
is	O
possible	O
to	O
learn	O
as	O
‘	O
‘	O
powerful	O
’	O
’	O
representations	O
from	O
unlabeled	O
data	O
without	O
any	O
supervision	O
.	O

In	O
this	O
section	O
,	O
we	O
show	O
that	O
the	O
adversarial	Method
autoencoder	Method
can	O
disentangle	O
discrete	O
class	O
variables	O
from	O
the	O
continuous	O
latent	O
style	O
variables	O
in	O
a	O
purely	O
unsupervised	Method
fashion	Method
.	O

The	O
architecture	O
that	O
we	O
use	O
is	O
similar	O
to	O
Figure	O
[	O
reference	O
]	O
,	O
with	O
the	O
difference	O
that	O
we	O
remove	O
the	O
semi	Task
-	Task
supervised	Task
classification	Task
stage	O
and	O
thus	O
no	O
longer	O
train	O
the	O
network	O
on	O
any	O
labeled	O
mini	O
-	O
batch	O
.	O

Another	O
difference	O
is	O
that	O
the	O
inference	Method
network	Method
predicts	O
a	O
one	O
-	O
hot	O
vector	O
whose	O
dimension	O
is	O
the	O
number	O
of	O
categories	O
that	O
we	O
wish	O
the	O
data	O
to	O
be	O
clustered	O
into	O
.	O

Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
unsupervised	Task
clustering	Task
performance	O
of	O
the	O
AAE	Method
on	O
MNIST	Material
when	O
the	O
number	O
of	O
clusters	O
is	O
16	O
.	O

Each	O
row	O
corresponds	O
to	O
one	O
cluster	O
.	O

The	O
first	O
image	O
in	O
each	O
row	O
shows	O
the	O
cluster	O
heads	O
,	O
which	O
are	O
digits	O
generated	O
by	O
fixing	O
the	O
style	O
variable	O
to	O
zero	O
and	O
setting	O
the	O
label	O
variable	O
to	O
one	O
of	O
the	O
16	O
one	O
-	O
hot	O
vectors	O
.	O

The	O
rest	O
of	O
the	O
images	O
in	O
each	O
row	O
are	O
random	O
test	O
images	O
that	O
have	O
been	O
categorized	O
into	O
the	O
corresponding	O
category	O
based	O
on	O
.	O

We	O
can	O
see	O
that	O
the	O
AAE	Method
has	O
picked	O
up	O
some	O
discrete	O
styles	O
as	O
the	O
class	O
labels	O
.	O

For	O
example	O
,	O
the	O
digit	O
s	O
and	O
s	O
that	O
are	O
tilted	O
(	O
cluster	O
16	O
and	O
11	O
)	O
are	O
put	O
in	O
a	O
separate	O
cluster	O
than	O
the	O
straight	O
s	O
and	O
s	O
(	O
cluster	O
15	O
and	O
10	O
)	O
,	O
or	O
the	O
network	O
has	O
separated	O
digit	O
s	O
into	O
two	O
clusters	O
(	O
cluster	O
4	O
,	O
6	O
)	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
written	O
with	O
a	O
loop	O
.	O

We	O
performed	O
an	O
experiment	O
to	O
evaluate	O
the	O
unsupervised	Task
clustering	Task
performance	O
of	O
AAEs	Method
.	O

We	O
used	O
the	O
following	O
evaluation	O
protocol	O
:	O
Once	O
the	O
training	O
is	O
done	O
,	O
for	O
each	O
cluster	O
,	O
we	O
found	O
the	O
validation	O
example	O
that	O
maximizes	O
,	O
and	O
assigned	O
the	O
label	O
of	O
to	O
all	O
the	O
points	O
in	O
the	O
cluster	O
.	O

We	O
then	O
computed	O
the	O
test	Metric
error	Metric
based	O
on	O
the	O
assigned	O
class	O
labels	O
to	O
each	O
cluster	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
AAE	Method
achieves	O
the	O
classification	Metric
error	Metric
rate	Metric
of	O
9.55	O
%	O
and	O
4.10	O
%	O
with	O
16	O
and	O
30	O
total	O
labels	O
respectively	O
.	O

We	O
observed	O
that	O
as	O
the	O
number	O
of	O
clusters	O
grows	O
,	O
the	O
classification	Metric
rate	Metric
improves	O
.	O

section	O
:	O
Dimensionality	Task
Reduction	Task
with	O
Adversarial	Method
Autoencoders	Method
Visualization	Task
of	Task
high	Task
dimensional	Task
data	Task
is	O
a	O
very	O
important	O
problem	O
in	O
many	O
applications	O
as	O
it	O
facilitates	O
the	O
understanding	O
of	O
the	O
generative	Task
process	Task
of	O
the	O
data	O
and	O
allows	O
us	O
to	O
extract	O
useful	O
information	O
about	O
the	O
data	O
.	O

A	O
popular	O
approach	O
of	O
data	Task
visualization	Task
is	O
learning	O
a	O
low	Method
dimensional	Method
embedding	Method
in	O
which	O
nearby	O
points	O
correspond	O
to	O
similar	O
objects	O
.	O

Over	O
the	O
last	O
decade	O
,	O
a	O
large	O
number	O
of	O
new	O
non	Method
-	Method
parametric	Method
dimensionality	Method
reduction	Method
techniques	Method
such	O
as	O
t	Method
-	Method
SNE	Method
tsne	Method
have	O
been	O
proposed	O
.	O

The	O
main	O
drawback	O
of	O
these	O
methods	O
is	O
that	O
they	O
do	O
not	O
have	O
a	O
parametric	Method
encoder	Method
that	O
can	O
be	O
used	O
to	O
find	O
the	O
embedding	O
of	O
the	O
new	O
data	O
points	O
.	O

Different	O
methods	O
such	O
as	O
parametric	Method
t	Method
-	Method
SNE	Method
param_tsne	Method
have	O
been	O
proposed	O
to	O
address	O
this	O
issue	O
.	O

Autoencoders	Method
are	O
interesting	O
alternatives	O
as	O
they	O
provide	O
the	O
non	O
-	O
linear	O
mapping	O
required	O
for	O
such	O
embeddings	O
;	O
but	O
it	O
is	O
widely	O
known	O
that	O
non	Method
-	Method
regularized	Method
autoencoders	Method
‘	O
‘	O
fracture	O
’	O
’	O
the	O
manifold	O
into	O
many	O
different	O
domains	O
which	O
result	O
in	O
very	O
different	O
codes	O
for	O
similar	O
images	O
geoff_dim_reduce	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
an	O
adversarial	Method
autoencoder	Method
architecture	O
for	O
dimensionality	Task
reduction	Task
and	O
data	Task
visualization	Task
purposes	Task
.	O

We	O
will	O
show	O
that	O
in	O
these	O
autoencoders	Method
,	O
the	O
adversarial	Method
regularization	Method
attaches	O
the	O
hidden	O
code	O
of	O
similar	O
images	O
to	O
each	O
other	O
and	O
thus	O
prevents	O
the	O
manifold	Task
fracturing	Task
problem	Task
that	O
is	O
typically	O
encountered	O
in	O
the	O
embeddings	O
learnt	O
by	O
the	O
autoencoders	Method
.	O

Suppose	O
we	O
have	O
a	O
dataset	O
with	O
class	O
labels	O
and	O
we	O
would	O
like	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
dataset	O
to	O
,	O
where	O
is	O
typically	O
2	O
or	O
3	O
for	O
the	O
visualization	Task
purposes	Task
.	O

We	O
alter	O
the	O
architecture	O
of	O
Figure	O
[	O
reference	O
]	O
to	O
Figure	O
[	O
reference	O
]	O
in	O
which	O
the	O
final	O
representation	O
is	O
achieved	O
by	O
adding	O
the	O
dimensional	Method
distributed	Method
representation	Method
of	O
the	O
cluster	Method
head	Method
with	O
the	O
dimensional	Method
style	Method
representation	Method
.	O

The	O
cluster	Method
head	Method
representation	Method
is	O
obtained	O
by	O
multiplying	O
the	O
dimensional	O
one	O
-	O
hot	O
class	O
label	O
vector	O
by	O
an	O
matrix	O
,	O
where	O
the	O
rows	O
of	O
represent	O
the	O
cluster	Method
head	Method
representations	Method
that	O
are	O
learned	O
with	O
SGD	Method
.	O

We	O
introduce	O
an	O
additional	O
cost	O
function	O
that	O
penalizes	O
the	O
Euclidean	O
distance	O
between	O
every	O
two	O
cluster	O
heads	O
.	O

Specifically	O
,	O
if	O
the	O
Euclidean	O
distance	O
is	O
larger	O
than	O
a	O
threshold	O
,	O
the	O
cost	O
function	O
is	O
zero	O
,	O
and	O
if	O
it	O
is	O
smaller	O
than	O
,	O
the	O
cost	Method
function	Method
linearly	O
penalizes	O
the	O
distance	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
a	O
,	O
b	O
)	O
show	O
the	O
results	O
of	O
the	O
semi	Method
-	Method
supervised	Method
dimensionality	Method
reduction	Method
in	Method
dimensions	Method
on	O
the	O
MNIST	Material
dataset	O
(	O
)	O
with	O
1000	O
and	O
100	O
labels	O
.	O

We	O
can	O
see	O
that	O
the	O
network	O
can	O
achieve	O
a	O
clean	O
separation	O
of	O
the	O
digit	O
clusters	O
and	O
obtain	O
the	O
semi	Task
-	Task
supervised	Task
classification	Task
error	O
of	O
4.20	O
%	O
and	O
6.08	O
%	O
respectively	O
.	O

Note	O
that	O
because	O
of	O
the	O
2D	O
constraint	O
,	O
the	O
classification	Metric
error	Metric
is	O
not	O
as	O
good	O
as	O
the	O
high	O
-	O
dimensional	O
cases	O
;	O
and	O
that	O
the	O
style	O
distribution	O
of	O
each	O
cluster	O
is	O
not	O
quite	O
Gaussian	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
shows	O
the	O
result	O
of	O
unsupervised	Task
dimensionality	Task
reduction	Task
in	O
dimensions	O
where	O
the	O
number	O
of	O
clusters	O
have	O
chosen	O
to	O
be	O
.	O

We	O
can	O
see	O
that	O
the	O
network	O
can	O
achieve	O
a	O
rather	O
clean	O
separation	O
of	O
the	O
digit	O
clusters	O
and	O
sub	O
-	O
clusters	O
.	O

For	O
example	O
,	O
the	O
network	O
has	O
assigned	O
two	O
different	O
clusters	O
to	O
digit	O
1	O
(	O
green	O
clusters	O
)	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
straight	O
or	O
tilted	O
.	O

The	O
network	O
is	O
also	O
clustering	O
digit	O
6	O
into	O
three	O
clusters	O
(	O
black	O
clusters	O
)	O
depending	O
on	O
how	O
much	O
tilted	O
the	O
digit	O
is	O
.	O

Also	O
the	O
network	O
has	O
assigned	O
two	O
separate	O
clusters	O
for	O
digit	O
2	O
(	O
red	O
clusters	O
)	O
,	O
depending	O
on	O
whether	O
the	O
digit	O
is	O
written	O
with	O
a	O
loop	O
.	O

This	O
AAE	Method
architecture	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
can	O
also	O
be	O
used	O
to	O
embed	O
images	O
into	O
larger	O
dimensionalities	O
(	O
)	O
.	O

For	O
example	O
,	O
Figure	O
[	O
reference	O
]	O
d	O
shows	O
the	O
result	O
of	O
semi	Task
-	Task
supervised	Task
dimensionality	Task
reduction	Task
in	O
dimensions	O
with	O
100	O
labels	O
.	O

In	O
this	O
case	O
,	O
we	O
fixed	O
matrix	O
to	O
and	O
thus	O
the	O
cluster	O
heads	O
are	O
the	O
corners	O
of	O
a	O
dimensional	O
simplex	O
.	O

The	O
style	Method
representation	Method
is	O
learnt	O
to	O
be	O
a	O
10D	Method
Gaussian	Method
distribution	Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
and	O
is	O
directly	O
added	O
to	O
the	O
cluster	O
head	O
to	O
construct	O
the	O
final	O
representation	O
.	O

Once	O
the	O
network	O
is	O
trained	O
,	O
in	O
order	O
to	O
visualize	O
the	O
10D	Method
learnt	Method
representation	Method
,	O
we	O
use	O
a	O
linear	Method
transformation	Method
to	O
map	O
the	O
10D	Method
representation	Method
to	O
a	O
2D	O
space	O
such	O
that	O
the	O
cluster	O
heads	O
are	O
mapped	O
to	O
the	O
points	O
that	O
are	O
uniformly	O
placed	O
on	O
a	O
2D	O
circle	O
.	O

We	O
can	O
verify	O
from	O
this	O
figure	O
that	O
in	O
this	O
high	O
-	O
dimensional	O
case	O
,	O
the	O
style	Method
representation	Method
has	O
indeed	O
learnt	O
to	O
have	O
a	O
Gaussian	Method
distribution	Method
.	O

With	O
100	O
total	O
labels	O
,	O
this	O
model	O
achieves	O
the	O
classification	Metric
error	Metric
-	Metric
rate	Metric
of	O
3.90	O
%	O
which	O
is	O
worse	O
than	O
the	O
classification	Metric
error	Metric
-	Metric
rate	Metric
of	O
1.90	O
%	O
that	O
is	O
achieved	O
by	O
the	O
AAE	Method
architecture	O
with	O
the	O
concatenated	Method
style	Method
and	Method
label	Method
representation	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
proposed	O
to	O
use	O
the	O
GAN	Method
framework	O
as	O
a	O
variational	Method
inference	Method
algorithm	Method
for	O
both	O
discrete	O
and	O
continuous	O
latent	O
variables	O
in	O
probabilistic	Method
autoencoders	Method
.	O

Our	O
method	O
called	O
the	O
adversarial	Method
autoencoder	Method
(	O
AAE	Method
)	O
,	O
is	O
a	O
generative	Method
autoencoder	Method
that	O
achieves	O
competitive	Metric
test	Metric
likelihoods	Metric
on	O
real	Material
-	Material
valued	Material
MNIST	Material
and	O
Toronto	Material
Face	Material
datasets	Material
.	O

We	O
discussed	O
how	O
this	O
method	O
can	O
be	O
extended	O
to	O
semi	Task
-	Task
supervised	Task
scenarios	Task
and	O
showed	O
that	O
it	O
achieves	O
competitive	O
semi	Task
-	Task
supervised	Task
classification	Task
performance	O
on	O
MNIST	Material
and	O
SVHN	Material
datasets	O
.	O

Finally	O
,	O
we	O
demonstrated	O
the	O
applications	O
of	O
adversarial	Method
autoencoders	Method
in	O
disentangling	O
the	O
style	O
and	O
content	O
of	O
images	O
,	O
unsupervised	Task
clustering	Task
,	O
dimensionality	Task
reduction	Task
and	O
data	Task
visualization	Task
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
Ilya	O
Sutskever	O
,	O
Oriol	O
Vinyals	O
,	O
Jon	O
Gauthier	O
,	O
Sam	O
Bowman	O
and	O
other	O
members	O
of	O
the	O
Google	O
Brain	O
team	O
for	O
helpful	O
discussions	O
.	O

We	O
thank	O
the	O
developers	O
of	O
TensorFlow	Method
tensorflow2015	O
-	O
whitepaper	O
,	O
which	O
we	O
used	O
for	O
all	O
of	O
our	O
experiments	O
.	O

We	O
also	O
thank	O
NVIDIA	Material
for	O
GPU	O
donations	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Experiment	O
Details	O
subsection	O
:	O
Likelihood	O
Experiments	O
The	O
encoder	Method
,	O
decoder	Method
and	O
discriminator	Method
each	O
have	O
two	O
layers	O
of	O
1000	O
hidden	O
units	O
with	O
ReLU	O
activation	O
function	O
.	O

The	O
activation	O
of	O
the	O
last	O
layer	O
of	O
is	O
linear	O
.	O

The	O
weights	O
are	O
initialized	O
with	O
a	O
Gaussian	Method
distribution	Method
with	O
the	O
standard	O
deviation	O
of	O
0.01	O
.	O

The	O
mini	O
-	O
batch	O
size	O
is	O
100	O
.	O

The	O
autoencoder	Method
is	O
trained	O
with	O
a	O
Euclidean	Method
cost	Method
function	Method
for	O
reconstruction	Task
.	O

On	O
the	O
MNIST	Material
dataset	O
we	O
use	O
the	O
sigmoid	O
activation	O
function	O
in	O
the	O
last	O
layer	O
of	O
the	O
autoencoder	Method
and	O
on	O
the	O
TFD	Material
dataset	O
we	O
use	O
the	O
linear	Method
activation	Method
function	Method
.	O

The	O
dimensionality	O
of	O
the	O
hidden	O
code	O
is	O
and	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
Gaussian	O
prior	O
is	O
and	O
for	O
MNIST	Material
and	O
TFD	Material
,	O
respectively	O
.	O

On	O
the	O
Toronto	Material
Face	Material
dataset	Material
,	O
data	O
points	O
are	O
subtracted	O
by	O
the	O
mean	O
and	O
divided	O
by	O
the	O
standard	O
deviation	O
along	O
each	O
input	O
dimension	O
across	O
the	O
whole	O
training	O
set	O
to	O
normalize	O
the	O
contrast	O
.	O

However	O
,	O
after	O
obtaining	O
the	O
samples	O
,	O
we	O
rescaled	O
the	O
images	O
(	O
by	O
inverting	O
the	O
pre	Method
-	Method
processing	Method
stage	Method
)	O
to	O
have	O
pixel	O
intensities	O
between	O
0	O
and	O
1	O
so	O
that	O
we	O
can	O
have	O
a	O
fair	O
likelihood	O
comparison	O
with	O
other	O
methods	O
.	O

In	O
the	O
deterministic	O
case	O
of	O
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
code	O
should	O
be	O
consistent	O
with	O
the	O
intrinsic	O
dimensionality	O
of	O
the	O
data	O
,	O
since	O
the	O
only	O
source	O
of	O
stochasticity	O
in	O
is	O
the	O
data	O
distribution	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
MNIST	Material
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
code	O
can	O
be	O
between	O
5	O
to	O
8	O
,	O
and	O
for	O
TFD	Material
and	O
SVHN	Material
,	O
it	O
can	O
be	O
between	O
10	O
to	O
20	O
.	O

For	O
training	O
AAEs	Method
with	O
higher	O
dimensionalities	O
in	O
the	O
code	O
space	O
(	O
e.g.	O
,	O
1000	O
)	O
,	O
the	O
probabilistic	Method
along	O
with	O
the	O
re	Method
-	Method
parametrization	Method
trick	Method
can	O
be	O
used	O
.	O

subsection	O
:	O
Semi	Task
-	Task
Supervised	Task
Experiments	Task
subsubsection	O
:	O
MNIST	Material
The	O
encoder	Method
,	O
decoder	Method
and	O
discriminator	Method
each	O
have	O
two	O
layers	O
of	O
1000	O
hidden	O
units	O
with	O
ReLU	O
activation	O
function	O
.	O

The	O
last	O
layer	O
of	O
the	O
autoencoder	Method
can	O
have	O
a	O
linear	O
or	O
sigmoid	O
activation	O
(	O
sigmoid	O
is	O
better	O
for	O
sample	Task
visualization	Task
)	O
.	O

The	O
cost	Method
function	Method
is	O
half	O
the	O
Euclidean	Metric
error	Metric
.	O

The	O
last	O
layer	O
of	O
and	O
has	O
the	O
softmax	O
and	O
linear	O
activation	O
function	O
,	O
respectively	O
.	O

The	O
and	O
share	O
the	O
first	O
two	O
1000	O
-	O
unit	O
layers	O
of	O
the	O
encoder	Method
.	O

The	O
dimensionality	Metric
of	O
both	O
the	O
style	Method
and	Method
label	Method
representation	Method
is	O
10	O
.	O

On	O
the	O
style	Method
representation	Method
,	O
we	O
impose	O
a	O
Gaussian	Method
distribution	Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
.	O

On	O
the	O
label	Task
representation	Task
,	O
we	O
impose	O
a	O
Categorical	O
distribution	O
.	O

The	O
semi	Task
-	Task
supervised	Task
cost	Task
is	O
a	O
cross	Metric
-	Metric
entropy	Metric
cost	Metric
function	Metric
at	O
the	O
output	O
of	O
.	O

We	O
use	O
gradient	Method
descent	Method
with	O
momentum	Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	Metric
reconstruction	Metric
cost	Metric
and	O
the	O
semi	Task
-	Task
supervised	Task
cost	Task
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	Method
and	Method
discriminator	Method
of	O
both	O
of	O
the	O
adversarial	Method
networks	Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	Metric
cost	Metric
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.001	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.0001	O
.	O

For	O
the	O
semi	Task
-	Task
supervised	Task
cost	Task
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.1	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

For	O
both	O
the	O
discriminative	Metric
and	Metric
generative	Metric
costs	Metric
of	O
the	O
adversarial	Method
networks	Method
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.1	O
,	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
and	O
after	O
1000	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

We	O
train	O
the	O
network	O
for	O
5000	O
epochs	O
.	O

We	O
add	O
a	O
Gaussian	O
noise	O
with	O
standard	O
deviation	O
of	O
0.3	O
only	O
to	O
the	O
input	O
layer	O
and	O
only	O
at	O
the	O
training	O
time	O
.	O

No	O
dropout	Method
,	O
weight	Method
decay	Method
or	O
other	O
Gaussian	Method
noise	Method
regularization	Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

The	O
labeled	O
examples	O
were	O
chosen	O
at	O
random	O
,	O
but	O
we	O
made	O
sure	O
they	O
are	O
distributed	O
evenly	O
across	O
the	O
classes	O
.	O

In	O
the	O
case	O
of	O
MNIST	Material
with	O
100	O
labels	O
,	O
the	O
test	Metric
error	Metric
after	O
the	O
first	O
epochs	O
is	O
16.50	O
%	O
,	O
after	O
50	O
epochs	O
is	O
3.40	O
%	O
,	O
after	O
500	O
epochs	O
is	O
2.21	O
%	O
and	O
after	O
5000	O
epochs	O
is	O
1.80	O
%	O
.	O

Batch	Method
-	Method
normalization	Method
batch	Method
did	O
not	O
help	O
in	O
the	O
case	O
of	O
the	O
MNIST	Material
dataset	O
.	O

subsubsection	O
:	O
SVHN	Material
The	O
SVHN	Material
dataset	O
has	O
about	O
530	O
K	O
training	O
points	O
and	O
26	O
K	O
test	O
points	O
.	O

Data	O
points	O
are	O
subtracted	O
by	O
the	O
mean	O
and	O
divided	O
by	O
the	O
standard	O
deviation	O
along	O
each	O
input	O
dimension	O
across	O
the	O
whole	O
training	O
set	O
to	O
normalize	O
the	O
contrast	O
.	O

The	O
dimensionality	Metric
of	O
the	O
label	Method
representation	Method
is	O
10	O
and	O
for	O
the	O
style	Method
representation	Method
we	O
use	O
20	O
dimensions	O
.	O

We	O
use	O
gradient	Method
descent	Method
with	O
momentum	Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	Metric
reconstruction	Metric
cost	Metric
and	O
the	O
semi	Task
-	Task
supervised	Task
cost	Task
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	Method
and	Method
discriminator	Method
of	O
both	O
of	O
the	O
adversarial	Method
networks	Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	Metric
cost	Metric
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.0001	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.00001	O
.	O

For	O
the	O
semi	Task
-	Task
supervised	Task
cost	Task
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.1	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.01	O
.	O

For	O
both	O
the	O
discriminative	Metric
and	Metric
generative	Metric
costs	Metric
of	O
the	O
adversarial	Method
networks	Method
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
and	O
after	O
250	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

We	O
train	O
the	O
network	O
for	O
1000	O
epochs	O
.	O

We	O
use	O
dropout	Method
at	O
the	O
input	Method
layer	Method
with	O
the	O
dropout	Metric
rate	Metric
of	O
20	O
%	O
.	O

No	O
other	O
dropout	Method
,	O
weight	Method
decay	Method
or	O
Gaussian	Method
noise	Method
regularization	Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

The	O
labeled	O
examples	O
were	O
chosen	O
at	O
random	O
,	O
but	O
we	O
made	O
sure	O
they	O
are	O
distributed	O
evenly	O
across	O
the	O
classes	O
.	O

In	O
the	O
case	O
of	O
SVHN	Material
with	O
1000	O
labels	O
,	O
the	O
test	Metric
error	Metric
after	O
the	O
first	O
epochs	O
is	O
49.34	O
%	O
,	O
after	O
50	O
epochs	O
is	O
25.86	O
%	O
,	O
after	O
500	O
epochs	O
is	O
18.15	O
%	O
and	O
after	O
1000	O
epochs	O
is	O
17.66	O
%	O
.	O

Batch	Method
-	Method
normalization	Method
were	O
used	O
in	O
all	O
the	O
autoencoder	Method
layers	Method
including	O
the	O
softmax	O
layer	O
of	O
,	O
the	O
linear	Method
layer	Method
of	O
as	O
well	O
as	O
the	O
linear	O
output	O
layer	O
of	O
the	O
autoencoder	Method
.	O

We	O
found	O
batch	O
-	O
normalization	O
batch	O
to	O
be	O
crucial	O
in	O
training	O
the	O
AAE	Method
network	O
on	O
the	O
SVHN	Material
dataset	O
.	O

subsection	O
:	O
Unsupervised	Task
Clustering	Task
Experiments	O
The	O
encoder	Method
,	O
decoder	Method
and	O
discriminator	Method
each	O
have	O
two	O
layers	O
of	O
3000	O
hidden	O
units	O
with	O
ReLU	Method
activation	Method
function	Method
.	O

The	O
last	O
layer	O
of	O
the	O
autoencoder	Method
has	O
a	O
sigmoid	O
activation	O
function	O
.	O

The	O
cost	Method
function	Method
is	O
half	O
the	O
Euclidean	Metric
error	Metric
.	O

The	O
dimensionality	Metric
of	O
the	O
style	Method
and	Method
label	Method
representation	Method
is	O
5	O
and	O
30	O
(	O
number	O
of	O
clusters	O
)	O
,	O
respectively	O
.	O

On	O
the	O
style	Method
representation	Method
,	O
we	O
impose	O
a	O
Gaussian	Method
distribution	Method
with	O
the	O
standard	O
deviation	O
of	O
1	O
.	O

On	O
the	O
label	Task
representation	Task
,	O
we	O
impose	O
a	O
Categorical	O
distribution	O
.	O

We	O
use	O
gradient	Method
descent	Method
with	O
momentum	Method
for	O
optimizing	O
all	O
the	O
cost	O
functions	O
.	O

The	O
momentum	O
value	O
for	O
the	O
autoencoder	Metric
reconstruction	Metric
cost	Metric
is	O
fixed	O
to	O
0.9	O
.	O

The	O
momentum	O
value	O
for	O
the	O
generator	Method
and	Method
discriminator	Method
of	O
both	O
of	O
the	O
adversarial	Method
networks	Method
is	O
fixed	O
to	O
0.1	O
.	O

For	O
the	O
reconstruction	Metric
cost	Metric
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
and	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.001	O
.	O

For	O
both	O
the	O
discriminative	Metric
and	Metric
generative	Metric
costs	Metric
of	O
the	O
adversarial	Method
networks	Method
,	O
we	O
use	O
the	O
initial	O
learning	Metric
rate	Metric
of	O
0.1	O
and	O
after	O
50	O
epochs	O
reduce	O
it	O
to	O
0.01	O
.	O

We	O
train	O
the	O
network	O
for	O
1500	O
epochs	O
.	O

We	O
use	O
dropout	Method
at	O
the	O
input	Method
layer	Method
with	O
the	O
dropout	Metric
rate	Metric
of	O
20	O
%	O
.	O

No	O
other	O
dropout	Method
,	O
weight	Method
decay	Method
or	O
Gaussian	Method
noise	Method
regularization	Method
were	O
used	O
in	O
any	O
other	O
layer	O
.	O

Batch	Method
-	Method
normalization	Method
was	O
used	O
only	O
in	O
the	O
encoder	O
layers	O
of	O
the	O
autoencoder	Method
including	O
the	O
last	O
layer	O
of	O
and	O
.	O

We	O
found	O
batch	O
-	O
normalization	O
batch	O
to	O
be	O
crucial	O
in	O
training	O
the	O
AAE	Method
networks	O
for	O
unsupervised	Task
clustering	Task
.	O

