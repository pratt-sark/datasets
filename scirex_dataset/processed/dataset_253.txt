Conditional	Method
Random	Method
Fields	Method
as	O
Recurrent	Method
Neural	Method
Networks	Method
section	O
:	O
Abstract	O
Pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
,	O
such	O
as	O
semantic	Task
segmentation	Task
,	O
play	O
a	O
central	O
role	O
in	O
image	Task
understanding	Task
.	O

Recent	O
approaches	O
have	O
attempted	O
to	O
harness	O
the	O
capabilities	O
of	O
deep	Method
learning	Method
techniques	Method
for	O
image	Task
recognition	Task
to	O
tackle	O
pixellevel	Task
labelling	Task
tasks	Task
.	O

One	O
central	O
issue	O
in	O
this	O
methodology	O
is	O
the	O
limited	O
capacity	O
of	O
deep	Method
learning	Method
techniques	Method
to	O
delineate	O
visual	O
objects	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
introduce	O
a	O
new	O
form	O
of	O
convolutional	Method
neural	Method
network	Method
that	O
combines	O
the	O
strengths	O
of	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
and	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRFs	Method
)-	O
based	O
probabilistic	O
graphical	O
modelling	O
.	O

To	O
this	O
end	O
,	O
we	O
formulate	O
mean	Method
-	Method
field	Method
approximate	Method
inference	Method
for	O
the	O
Conditional	Method
Random	Method
Fields	Method
with	O
Gaussian	O
pairwise	O
potentials	O
as	O
Recurrent	Method
Neural	Method
Networks	Method
.	O

This	O
network	O
,	O
called	O
CRF	Method
-	Method
RNN	Method
,	O
is	O
then	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
CNN	Method
to	O
obtain	O
a	O
deep	Method
network	Method
that	O
has	O
desirable	O
properties	O
of	O
both	O
CNNs	Method
and	O
CRFs	Method
.	O

Importantly	O
,	O
our	O
system	O
fully	O
integrates	O
CRF	Method
modelling	O
with	O
CNNs	Method
,	O
making	O
it	O
possible	O
to	O
train	O
the	O
whole	O
deep	Method
network	Method
end	O
-	O
to	O
-	O
end	O
with	O
the	O
usual	O
back	Method
-	Method
propagation	Method
algorithm	Method
,	O
avoiding	O
offline	Method
post	Method
-	Method
processing	Method
methods	Method
for	O
object	Task
delineation	Task
.	O

We	O
apply	O
the	O
proposed	O
method	O
to	O
the	O
problem	O
of	O
semantic	Task
image	Task
segmentation	Task
,	O
obtaining	O
top	O
results	O
on	O
the	O
challenging	O
Pascal	Material
VOC	Material
2012	Material
segmentation	Material
benchmark	Material
.	O

section	O
:	O
Introduction	O
Low	Task
-	Task
level	Task
computer	Task
vision	Task
problems	Task
such	O
as	O
semantic	Task
image	Task
segmentation	Task
or	O
depth	Task
estimation	Task
often	O
involve	O
assigning	O
a	O
label	O
to	O
each	O
pixel	O
in	O
an	O
image	O
.	O

While	O
the	O
feature	Method
representation	Method
used	O
to	O
classify	O
individual	O
pixels	O
plays	O
an	O
important	O
role	O
in	O
this	O
task	O
,	O
it	O
is	O
similarly	O
important	O
to	O
consider	O
factors	O
such	O
as	O
image	O
edges	O
,	O
appearance	O
consistency	O
and	O
spatial	O
consistency	O
while	O
assigning	O
labels	O
in	O
order	O
to	O
obtain	O
accurate	O
and	O
precise	O
results	O
.	O

Designing	O
a	O
strong	Method
feature	Method
representation	Method
is	O
a	O
key	O
chal	O
-	O
*	O
Authors	O
contributed	O
equally	O
.	O

†	O
Work	O
conducted	O
while	O
authors	O
at	O
the	O
University	O
of	O
Oxford	O
.	O

lenge	O
in	O
pixel	Task
-	Task
level	Task
labelling	Task
problems	Task
.	O

Work	O
on	O
this	O
topic	O
includes	O
:	O
TextonBoost	Method
[	O
reference	O
]	O
,	O
TextonForest	Method
[	O
reference	O
]	O
,	O
and	O
Random	Method
Forest	Method
-	Method
based	Method
classifiers	Method
[	O
reference	O
]	O
.	O

Recently	O
,	O
supervised	Method
deep	Method
learning	Method
approaches	Method
such	O
as	O
large	Method
-	Method
scale	Method
deep	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
have	O
been	O
immensely	O
successful	O
in	O
many	O
high	O
-	O
level	Task
computer	Task
vision	Task
tasks	Task
such	O
as	O
image	Task
recognition	Task
[	O
reference	O
]	O
and	O
object	Task
detection	Task
[	O
reference	O
]	O
.	O

This	O
motivates	O
exploring	O
the	O
use	O
of	O
CNNs	Method
for	O
pixel	Task
-	Task
level	Task
labelling	Task
problems	Task
.	O

The	O
key	O
insight	O
is	O
to	O
learn	O
a	O
strong	O
feature	Method
representation	Method
end	O
-	O
to	O
-	O
end	O
for	O
the	O
pixel	Task
-	Task
level	Task
labelling	Task
task	Task
instead	O
of	O
hand	O
-	O
crafting	O
features	Method
with	O
heuristic	Method
parameter	Method
tuning	Method
.	O

In	O
fact	O
,	O
a	O
number	O
of	O
recent	O
approaches	O
including	O
the	O
particularly	O
interesting	O
works	O
FCN	Method
[	O
reference	O
]	O
and	O
DeepLab	Method
[	O
reference	O
]	O
have	O
shown	O
a	O
significant	O
accuracy	Metric
boost	O
by	O
adapting	O
stateof	O
-	O
the	O
-	O
art	O
CNN	Method
based	O
image	O
classifiers	O
to	O
the	O
semantic	Task
segmentation	Task
problem	O
.	O

However	O
,	O
there	O
are	O
significant	O
challenges	O
in	O
adapting	O
CNNs	Method
designed	O
for	O
high	Task
level	Task
computer	Task
vision	Task
tasks	Task
such	O
as	O
object	Task
recognition	Task
to	O
pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
.	O

Firstly	O
,	O
traditional	O
CNNs	Method
have	O
convolutional	Method
filters	Method
with	O
large	O
receptive	O
fields	O
and	O
hence	O
produce	O
coarse	O
outputs	O
when	O
restructured	O
to	O
produce	O
pixel	O
-	O
level	O
labels	O
[	O
reference	O
]	O
.	O

Presence	O
of	O
maxpooling	Method
layers	Method
in	O
CNNs	Method
further	O
reduces	O
the	O
chance	O
of	O
getting	O
a	O
fine	O
segmentation	O
output	O
[	O
reference	O
]	O
.	O

This	O
,	O
for	O
instance	O
,	O
can	O
result	O
in	O
non	O
-	O
sharp	O
boundaries	O
and	O
blob	O
-	O
like	O
shapes	O
in	O
semantic	Task
segmentation	Task
tasks	O
.	O

Secondly	O
,	O
CNNs	Method
lack	O
smoothness	O
constraints	O
that	O
encourage	O
label	O
agreement	O
between	O
similar	O
pixels	O
,	O
and	O
spatial	O
and	O
appearance	O
consistency	O
of	O
the	O
labelling	O
output	O
.	O

Lack	O
of	O
such	O
smoothness	O
constraints	O
can	O
result	O
in	O
poor	O
object	Task
delineation	Task
and	O
small	O
spurious	O
regions	O
in	O
the	O
segmentation	O
output	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

On	O
a	O
separate	O
track	O
to	O
the	O
progress	O
of	O
deep	Method
learning	Method
techniques	Method
,	O
probabilistic	Method
graphical	Method
models	Method
have	O
been	O
developed	O
as	O
effective	O
methods	O
to	O
enhance	O
the	O
accuracy	Metric
of	O
pixellevel	Task
labelling	Task
tasks	Task
.	O

In	O
particular	O
,	O
Markov	Method
Random	Method
Fields	Method
(	O
MRFs	Method
)	Method
and	O
its	O
variant	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRFs	Method
)	O
have	O
observed	O
widespread	O
success	O
in	O
this	O
area	O
[	O
reference	O
][	O
reference	O
]	O
and	O
have	O
become	O
one	O
of	O
the	O
most	O
successful	O
graphical	Method
models	Method
used	O
in	O
computer	Task
vision	Task
.	O

The	O
key	O
idea	O
of	O
CRF	Method
inference	O
for	O
semantic	Task
labelling	Task
is	O
to	O
formulate	O
the	O
label	Task
assignment	Task
problem	Task
as	O
a	O
probabilistic	Task
inference	Task
problem	Task
that	O
incorporates	O
assumptions	O
such	O
as	O
the	O
label	O
agreement	O
between	O
similar	O
pixels	O
.	O

CRF	Method
inference	O
is	O
able	O
to	O
refine	O
weak	O
and	O
coarse	O
pixel	O
-	O
level	O
label	O
predictions	O
to	O
produce	O
sharp	O
boundaries	O
and	O
fine	Task
-	Task
grained	Task
segmentations	Task
.	O

Therefore	O
,	O
intuitively	O
,	O
CRFs	Method
can	O
be	O
used	O
to	O
overcome	O
the	O
drawbacks	O
in	O
utilizing	O
CNNs	Method
for	O
pixel	Task
-	Task
level	Task
labelling	Task
tasks	Task
.	O

One	O
way	O
to	O
utilize	O
CRFs	Method
to	O
improve	O
the	O
semantic	Task
labelling	Task
results	Task
produced	O
by	O
a	O
CNN	Method
is	O
to	O
apply	O
CRF	Method
inference	O
as	O
a	O
post	Method
-	Method
processing	Method
step	Method
disconnected	O
from	O
the	O
training	O
of	O
the	O
CNN	Method
[	O
reference	O
]	O
.	O

Arguably	O
,	O
this	O
does	O
not	O
fully	O
harness	O
the	O
strength	O
of	O
CRFs	Method
since	O
it	O
is	O
not	O
integrated	O
with	O
the	O
deep	Method
network	Method
.	O

In	O
this	O
setup	O
,	O
the	O
deep	Method
network	Method
is	O
unaware	O
of	O
the	O
CRF	Method
during	O
the	O
training	O
phase	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
deep	Method
learning	Method
solution	Method
for	O
the	O
pixel	Task
-	Task
level	Task
semantic	Task
image	Task
segmentation	Task
problem	Task
.	O

Our	O
formulation	O
combines	O
the	O
strengths	O
of	O
both	O
CNNs	O
and	O
CRF	Method
based	O
graphical	O
models	O
in	O
one	O
unified	O
framework	O
.	O

More	O
specifically	O
,	O
we	O
formulate	O
mean	Method
-	Method
field	Method
approximate	Method
inference	Method
for	O
the	O
dense	O
CRF	Method
with	O
Gaussian	Method
pairwise	Method
potentials	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
which	O
can	O
refine	O
coarse	O
outputs	O
from	O
a	O
traditional	O
CNN	Method
in	O
the	O
forward	O
pass	O
,	O
while	O
passing	O
error	O
differentials	O
back	O
to	O
the	O
CNN	Method
during	O
training	O
.	O

Importantly	O
,	O
with	O
our	O
formulation	O
,	O
the	O
whole	O
deep	Method
network	Method
,	O
which	O
comprises	O
a	O
traditional	O
CNN	Method
and	O
an	O
RNN	Method
for	O
CRF	Method
inference	O
,	O
can	O
be	O
trained	O
endto	O
-	O
end	O
utilizing	O
the	O
usual	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

Arguably	O
,	O
when	O
properly	O
trained	O
,	O
the	O
proposed	O
network	O
should	O
outperform	O
a	O
system	O
where	O
CRF	Method
inference	O
is	O
applied	O
as	O
a	O
post	Method
-	Method
processing	Method
method	Method
on	O
independent	O
pixel	O
-	O
level	O
predictions	O
produced	O
by	O
a	O
pre	O
-	O
trained	O
CNN	Method
.	O

Our	O
experimental	O
evaluation	O
confirms	O
that	O
this	O
indeed	O
is	O
the	O
case	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
our	O
network	O
on	O
the	O
popular	O
Pascal	Material
VOC	Material
2012	Material
benchmark	Material
,	O
achieving	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
of	O
74.7	O
%	O
.	O

Our	O
source	O
code	O
and	O
models	O
are	O
publicly	O
available	O
1	O
.	O

section	O
:	O
Related	O
Work	O
In	O
this	O
section	O
we	O
review	O
approaches	O
that	O
make	O
use	O
of	O
deep	Method
learning	Method
and	O
CNNs	Method
for	O
low	Task
-	Task
level	Task
computer	Task
vision	Task
tasks	Task
,	O
with	O
a	O
focus	O
on	O
semantic	Task
image	Task
segmentation	Task
.	O

A	O
wide	O
variety	O
of	O
approaches	O
have	O
been	O
proposed	O
to	O
tackle	O
the	O
semantic	Task
image	Task
segmentation	Task
task	Task
using	O
deep	Method
learning	Method
.	O

These	O
approaches	O
can	O
be	O
categorized	O
into	O
two	O
main	O
strategies	O
.	O

The	O
first	O
strategy	O
is	O
based	O
on	O
utilizing	O
separate	O
mechanisms	O
for	O
feature	Task
extraction	Task
,	O
and	O
image	Task
segmentation	Task
exploiting	O
the	O
edges	O
of	O
the	O
image	O
[	O
reference	O
][	O
reference	O
]	O
.	O

One	O
representative	O
instance	O
of	O
this	O
scheme	O
is	O
the	O
application	O
of	O
a	O
CNN	Method
for	O
the	O
extraction	Task
of	Task
meaningful	Task
features	Task
,	O
and	O
using	O
superpixels	Method
to	O
account	O
for	O
the	O
structural	O
pattern	O
of	O
the	O
image	O
.	O

Two	O
representative	O
examples	O
are	O
[	O
reference	O
][	O
reference	O
]	O
,	O
where	O
the	O
authors	O
first	O
ob	O
-	O
tained	O
superpixels	O
from	O
the	O
image	O
and	O
then	O
used	O
a	O
feature	Method
extraction	Method
process	Method
on	O
each	O
of	O
them	O
.	O

The	O
main	O
disadvantage	O
of	O
this	O
strategy	O
is	O
that	O
errors	O
in	O
the	O
initial	O
proposals	O
(	O
e.g	O
:	O
super	O
-	O
pixels	O
)	O
may	O
lead	O
to	O
poor	O
predictions	O
,	O
no	O
matter	O
how	O
good	O
the	O
feature	Method
extraction	Method
process	Method
is	O
.	O

Pinheiro	O
and	O
Collobert	O
[	O
reference	O
]	O
employed	O
an	O
RNN	Method
to	O
model	O
the	O
spatial	O
dependencies	O
during	O
scene	Task
parsing	Task
.	O

In	O
contrast	O
to	O
their	O
approach	O
,	O
we	O
show	O
that	O
a	O
typical	O
graphical	Method
model	Method
such	O
as	O
a	O
CRF	Method
can	O
be	O
formulated	O
as	O
an	O
RNN	Method
to	O
form	O
a	O
part	O
of	O
a	O
deep	Method
network	Method
,	O
to	O
perform	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
combined	O
with	O
a	O
CNN	Method
.	O

The	O
second	O
strategy	O
is	O
to	O
directly	O
learn	O
a	O
nonlinear	Method
model	Method
from	O
the	O
images	O
to	O
the	O
label	O
map	O
.	O

This	O
,	O
for	O
example	O
,	O
was	O
shown	O
in	O
[	O
reference	O
]	O
,	O
where	O
the	O
authors	O
replaced	O
the	O
last	O
fully	O
connected	O
layers	O
of	O
a	O
CNN	Method
by	O
convolutional	Method
layers	Method
to	O
keep	O
spatial	O
information	O
.	O

An	O
important	O
contribution	O
in	O
this	O
direction	O
is	O
[	O
reference	O
]	O
,	O
where	O
Long	O
et	O
al	O
.	O

used	O
the	O
concept	O
of	O
fully	Method
convolutional	Method
networks	Method
,	O
and	O
the	O
notion	O
that	O
top	O
layers	O
obtain	O
meaningful	O
features	O
for	O
object	Task
recognition	Task
whereas	O
low	O
layers	O
keep	O
information	O
about	O
the	O
structure	O
of	O
the	O
image	O
,	O
such	O
as	O
edges	O
.	O

In	O
their	O
work	O
,	O
connections	O
from	O
early	O
layers	O
to	O
later	O
layers	O
were	O
used	O
to	O
combine	O
these	O
cues	O
.	O

Bell	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
][	O
reference	O
]	O
used	O
a	O
CRF	Method
to	O
refine	O
segmentation	Task
results	O
obtained	O
from	O
a	O
CNN	Method
.	O

Bell	O
et	O
al	O
.	O

focused	O
on	O
material	Task
recognition	Task
and	O
segmentation	Task
,	O
whereas	O
Chen	O
et	O
al	O
.	O

reported	O
very	O
significant	O
improvements	O
on	O
semantic	Task
image	Task
segmentation	Task
.	O

In	O
contrast	O
to	O
these	O
works	O
,	O
which	O
employed	O
CRF	Method
inference	O
as	O
a	O
standalone	Task
post	Task
-	Task
processing	Task
step	Task
disconnected	O
from	O
the	O
CNN	Method
training	O
,	O
our	O
approach	O
is	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
network	Method
that	O
jointly	O
learns	O
the	O
parameters	O
of	O
the	O
CNN	Method
and	O
the	O
CRF	Method
in	O
one	O
unified	Method
deep	Method
network	Method
.	O

Works	O
that	O
use	O
neural	Method
networks	Method
to	O
predict	O
structured	O
output	O
are	O
found	O
in	O
different	O
domains	O
.	O

For	O
example	O
,	O
Do	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
an	O
approach	O
to	O
combine	O
deep	Method
neural	Method
networks	Method
and	O
Markov	Method
networks	Method
for	O
sequence	Task
labeling	Task
tasks	Task
.	O

Jain	O
et	O
al	O
.	O

[	O
reference	O
]	O
has	O
shown	O
Convolutional	Method
Neural	Method
Networks	Method
can	O
perform	O
well	O
like	O
MRFs	O
/	O
CRFs	Method
approaches	O
in	O
image	Task
restoration	Task
application	Task
.	O

Another	O
domain	O
which	O
benefits	O
from	O
the	O
combination	O
of	O
CNNs	Method
and	O
structured	Method
loss	Method
is	O
handwriting	Task
recognition	Task
.	O

In	O
natural	Task
language	Task
processing	Task
,	O
Yao	O
et	O
al	O
.	O

[	O
reference	O
]	O
shows	O
that	O
the	O
performance	O
of	O
an	O
RNN	Method
-	O
based	O
words	O
tagger	O
can	O
be	O
significantly	O
improved	O
by	O
incorporating	O
elements	O
of	O
the	O
CRF	Method
model	O
.	O

In	O
[	O
reference	O
]	O
,	O
the	O
authors	O
combined	O
a	O
CNN	Method
with	O
Hidden	Method
Markov	Method
Models	Method
for	O
that	O
purpose	O
,	O
whereas	O
more	O
recently	O
,	O
Peng	O
et	O
al	O
.	O

[	O
reference	O
]	O
used	O
a	O
modified	O
version	O
of	O
CRFs	Method
.	O

Related	O
to	O
this	O
line	O
of	O
works	O
,	O
in	O
[	O
reference	O
]	O
a	O
joint	O
CNN	Method
and	O
CRF	Method
model	O
was	O
used	O
for	O
text	Task
recognition	Task
on	O
natural	O
images	O
.	O

Tompson	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
the	O
use	O
of	O
joint	Method
training	Method
of	O
a	O
CNN	Method
and	O
an	O
MRF	Method
for	O
human	Task
pose	Task
estimation	Task
,	O
while	O
Chen	O
et	O
al	O
.	O

[	O
reference	O
]	O
focused	O
on	O
the	O
image	Task
classification	Task
problem	Task
with	O
a	O
similar	O
approach	O
.	O

Another	O
prominent	O
work	O
is	O
[	O
reference	O
]	O
,	O
in	O
which	O
the	O
authors	O
express	O
deformable	Method
part	Method
models	Method
,	O
a	O
kind	O
of	O
MRF	Method
,	O
as	O
a	O
layer	O
in	O
a	O
neural	Method
network	Method
.	O

In	O
our	O
approach	O
,	O
we	O
cast	O
a	O
different	O
graphical	Method
model	Method
as	O
a	O
neural	Method
network	Method
layer	Method
.	O

A	O
number	O
of	O
approaches	O
have	O
been	O
proposed	O
for	O
automatic	Task
learning	Task
of	Task
graphical	Task
model	Task
parameters	Task
and	O
joint	Method
training	Method
of	Method
classifiers	Method
and	O
graphical	Method
models	Method
.	O

Barbu	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
a	O
joint	Method
training	Method
of	O
a	O
MRF	O
/	O
CRF	Method
model	O
together	O
with	O
an	O
inference	Method
algorithm	Method
in	O
their	O
Active	Method
Random	Method
Field	Method
approach	Method
.	O

Domke	O
[	O
reference	O
]	O
advocated	O
back	Method
-	Method
propagation	Method
based	Method
parameter	Method
optimization	Method
in	O
graphical	Method
models	Method
when	O
approximate	Method
inference	Method
methods	Method
such	O
as	O
mean	Method
-	Method
field	Method
and	O
belief	Method
propagation	Method
are	O
used	O
.	O

This	O
idea	O
was	O
utilized	O
in	O
[	O
reference	O
]	O
,	O
where	O
a	O
binary	O
dense	O
CRF	Method
was	O
used	O
for	O
human	Task
pose	Task
estimation	Task
.	O

Similarly	O
,	O
Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
Stoyanov	O
et	O
al	O
.	O

[	O
reference	O
]	O
showed	O
how	O
back	Method
-	Method
propagation	Method
through	O
belief	Method
propagation	Method
can	O
be	O
used	O
to	O
optimize	O
model	O
parameters	O
.	O

Ross	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
in	O
particular	O
proposes	O
an	O
approach	O
based	O
on	O
learning	Task
messages	Task
.	O

Many	O
of	O
these	O
ideas	O
can	O
be	O
traced	O
back	O
to	O
[	O
reference	O
]	O
,	O
which	O
proposes	O
unrolling	Method
message	Method
passing	Method
algorithms	Method
as	O
simpler	O
operations	O
that	O
could	O
be	O
performed	O
within	O
a	O
CNN	Method
.	O

In	O
a	O
different	O
setup	O
,	O
Krähenbühl	O
and	O
Koltun	O
[	O
reference	O
]	O
demonstrated	O
automatic	Method
parameter	Method
tuning	Method
of	O
dense	O
CRF	Method
when	O
a	O
modified	O
mean	Method
-	Method
field	Method
algorithm	Method
is	O
used	O
for	O
inference	Task
.	O

An	O
alternative	O
inference	Method
approach	Method
for	O
dense	O
CRF	Method
,	O
not	O
based	O
on	O
mean	Method
-	Method
field	Method
,	O
is	O
proposed	O
in	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
to	O
the	O
works	O
described	O
above	O
,	O
our	O
approach	O
shows	O
that	O
it	O
is	O
possible	O
to	O
formulate	O
dense	O
CRF	Method
as	O
an	O
RNN	Method
so	O
that	O
one	O
can	O
form	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
system	Method
for	O
semantic	Task
image	Task
segmentation	Task
which	O
combines	O
the	O
strengths	O
of	O
deep	Method
learning	Method
and	O
graphical	Method
modelling	Method
.	O

After	O
our	O
initial	O
publication	O
of	O
the	O
technical	O
report	O
of	O
this	O
work	O
on	O
arXiv.org	O
,	O
a	O
number	O
of	O
independent	O
works	O
[	O
reference	O
][	O
reference	O
]	O
appeared	O
on	O
arXiv.org	O
presenting	O
similar	O
joint	Method
training	Method
approaches	Method
for	O
semantic	Task
image	Task
segmentation	Task
.	O

section	O
:	O
Conditional	Method
Random	Method
Fields	Method
In	O
this	O
section	O
we	O
provide	O
a	O
brief	O
overview	O
of	O
Conditional	Method
Random	Method
Fields	Method
(	O
CRF	Method
)	O
for	O
pixel	Task
-	Task
wise	Task
labelling	Task
and	O
introduce	O
the	O
notation	O
used	O
in	O
the	O
paper	O
.	O

A	O
CRF	Method
,	O
used	O
in	O
the	O
context	O
of	O
pixel	Task
-	Task
wise	Task
label	Task
prediction	Task
,	O
models	O
pixel	O
labels	O
as	O
random	O
variables	O
that	O
form	O
a	O
Markov	Method
Random	Method
Field	Method
(	O
MRF	Method
)	O
when	O
conditioned	O
upon	O
a	O
global	O
observation	O
.	O

The	O
global	Task
observation	Task
is	O
usually	O
taken	O
to	O
be	O
the	O
image	O
.	O

Let	O
X	O
i	O
be	O
the	O
random	O
variable	O
associated	O
to	O
pixel	O
i	O
,	O
which	O
represents	O
the	O
label	O
assigned	O
to	O
the	O
pixel	O
i	O
and	O
can	O
take	O
any	O
value	O
from	O
a	O
pre	O
-	O
defined	O
set	O
of	O
labels	O
L	O
=	O
{	O
l	O
1	O
,	O
l	O
2	O
,	O
.	O

.	O

.	O

,	O
l	O
L	O
}	O
.	O

Let	O
X	O
be	O
the	O
vector	O
formed	O
by	O
the	O
random	O
variables	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O

.	O

,	O
X	O
N	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
in	O
the	O
image	O
.	O

Given	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
,	O
where	O
V	O
=	O
{	O
X	O
1	O
,	O
X	O
2	O
,	O
.	O

.	O

.	O

,	O
X	O
N	O
}	O
,	O
and	O
a	O
global	O
observation	O
(	O
image	O
)	O
I	O
,	O
the	O
pair	O
(	O
I	O
,	O
X	O
)	O
can	O
be	O
modelled	O
as	O
a	O
CRF	Method
characterized	O
by	O
a	O
Gibbs	Method
distribution	Method
of	O
the	O
form	O
P	O
(	O
X	O
=	O
x|I	O
)	O
=	O
tion	O
[	O
reference	O
]	O
.	O

From	O
now	O
on	O
,	O
we	O
drop	O
the	O
conditioning	O
on	O
I	O
in	O
the	O
notation	O
for	O
convenience	O
.	O

In	O
the	O
fully	O
connected	O
pairwise	O
CRF	Method
model	O
of	O
[	O
reference	O
]	O
,	O
the	O
energy	O
of	O
a	O
label	O
assignment	O
x	O
is	O
given	O
by	O
:	O
where	O
the	O
unary	O
energy	O
components	O
ψ	O
u	O
(	O
x	O
i	O
)	O
measure	O
the	O
inverse	O
likelihood	O
(	O
and	O
therefore	O
,	O
the	O
cost	O
)	O
of	O
the	O
pixel	O
i	O
taking	O
the	O
label	O
x	O
i	O
,	O
and	O
pairwise	O
energy	O
components	O
ψ	O
p	O
(	O
x	O
i	O
,	O
x	O
j	O
)	O
measure	O
the	O
cost	O
of	O
assigning	O
labels	O
x	O
i	O
,	O
x	O
j	O
to	O
pixels	O
i	O
,	O
j	O
simultaneously	O
.	O

In	O
our	O
model	O
,	O
unary	O
energies	O
are	O
obtained	O
from	O
a	O
CNN	Method
,	O
which	O
,	O
roughly	O
speaking	O
,	O
predicts	O
labels	O
for	O
pixels	O
without	O
considering	O
the	O
smoothness	O
and	O
the	O
consistency	O
of	O
the	O
label	O
assignments	O
.	O

The	O
pairwise	O
energies	O
provide	O
an	O
image	Method
data	Method
-	Method
dependent	Method
smoothing	Method
term	Method
that	O
encourages	O
assigning	O
similar	O
labels	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

As	O
was	O
done	O
in	O
[	O
reference	O
]	O
,	O
we	O
model	O
pairwise	O
potentials	O
as	O
weighted	Method
Gaussians	Method
:	O
where	O
each	O
k	O
,	O
is	O
a	O
Gaussian	Method
kernel	Method
applied	O
on	O
feature	O
vectors	O
.	O

The	O
feature	O
vector	O
of	O
pixel	O
i	O
,	O
denoted	O
by	O
f	O
i	O
,	O
is	O
derived	O
from	O
image	O
features	O
such	O
as	O
spatial	O
location	O
and	O
RGB	O
values	O
[	O
reference	O
]	O
.	O

We	O
use	O
the	O
same	O
features	O
as	O
in	O
[	O
reference	O
]	O
.	O

The	O
function	O
µ	O
(	O
.	O

,	O
.	O

)	O
,	O
called	O
the	O
label	O
compatibility	O
function	O
,	O
captures	O
the	O
compatibility	O
between	O
different	O
pairs	O
of	O
labels	O
as	O
the	O
name	O
implies	O
.	O

Minimizing	O
the	O
above	O
CRF	Method
energy	O
E	O
(	O
x	O
)	O
yields	O
the	O
most	O
probable	O
label	O
assignment	O
x	O
for	O
the	O
given	O
image	O
.	O

Since	O
this	O
exact	Task
minimization	Task
is	O
intractable	O
,	O
a	O
mean	Method
-	Method
field	Method
approximation	Method
to	O
the	O
CRF	Method
distribution	O
is	O
used	O
for	O
approximate	Task
maximum	Task
posterior	Task
marginal	Task
inference	Task
.	O

It	O
consists	O
in	O
approximating	O
the	O
CRF	Method
distribution	O
P	O
(	O
X	O
)	O
by	O
a	O
simpler	O
distribution	Method
Q	Method
(	Method
X	Method
)	Method
,	O
which	O
can	O
be	O
written	O
as	O
the	O
product	O
of	O
independent	O
marginal	O
distributions	O
,	O
i.e.	O
,	O
Q	O
(	O
X	O
)	O
=	O
i	O
Q	O
i	O
(	O
X	O
i	O
)	O
.	O

The	O
steps	O
of	O
the	O
iterative	Method
algorithm	Method
for	O
approximate	Task
mean	Task
-	Task
field	Task
inference	Task
and	O
its	O
reformulation	O
as	O
an	O
RNN	Method
are	O
discussed	O
next	O
.	O

[	O
reference	O
]	O
,	O
broken	O
down	O
to	O
common	O
CNN	Method
operations	O
.	O

section	O
:	O
Algorithm	O
1	O
Mean	Method
-	Method
field	Method
in	O
dense	O
CRFs	Method
Adding	O
Unary	O
Potentials	O
Normalizing	O
end	O
while	O
section	O
:	O
A	O
Mean	Method
-	Method
field	Method
Iteration	Method
as	O
a	O
Stack	O
of	O
CNN	Method
Layers	O
A	O
key	O
contribution	O
of	O
this	O
paper	O
is	O
to	O
show	O
that	O
the	O
meanfield	O
CRF	Method
inference	O
can	O
be	O
reformulated	O
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O

To	O
this	O
end	O
,	O
we	O
first	O
consider	O
individual	O
steps	O
of	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
summarized	O
in	O
Algorithm	O
1	O
[	O
reference	O
]	O
,	O
and	O
describe	O
them	O
as	O
CNN	Method
layers	O
.	O

Our	O
contribution	O
is	O
based	O
on	O
the	O
observation	O
that	O
filter	Method
-	Method
based	Method
approximate	Method
mean	Method
-	Method
field	Method
inference	Method
approach	Method
for	O
dense	O
CRFs	Method
relies	O
on	O
applying	O
Gaussian	Method
spatial	Method
and	Method
bilateral	Method
filters	Method
on	O
the	O
mean	Method
-	Method
field	Method
approximates	Method
in	O
each	O
iteration	O
.	O

Unlike	O
the	O
standard	O
convolutional	Method
layer	Method
in	O
a	O
CNN	Method
,	O
in	O
which	O
filters	O
are	O
fixed	O
after	O
the	O
training	O
stage	O
,	O
we	O
use	O
edge	Method
-	Method
preserving	Method
Gaussian	Method
filters	Method
[	O
reference	O
][	O
reference	O
]	O
,	O
coefficients	O
of	O
which	O
depend	O
on	O
the	O
original	O
spatial	O
and	O
appearance	O
information	O
of	O
the	O
image	O
.	O

These	O
filters	O
have	O
the	O
additional	O
advantages	O
of	O
requiring	O
a	O
smaller	O
set	O
of	O
parameters	O
,	O
despite	O
the	O
filter	O
size	O
being	O
potentially	O
as	O
big	O
as	O
the	O
image	O
.	O

While	O
reformulating	O
the	O
steps	O
of	O
the	O
inference	Method
algorithm	Method
as	O
CNN	Method
layers	O
,	O
it	O
is	O
essential	O
to	O
be	O
able	O
to	O
calculate	O
error	O
differentials	O
in	O
each	O
layer	O
w.r.t	O
.	O

its	O
inputs	O
in	O
order	O
to	O
be	O
able	O
to	O
back	O
-	O
propagate	O
the	O
error	O
differentials	O
to	O
previous	O
layers	O
during	O
training	O
.	O

We	O
also	O
discuss	O
how	O
to	O
calculate	O
error	O
differentials	O
with	O
respect	O
to	O
the	O
parameters	O
in	O
each	O
layer	O
,	O
enabling	O
their	O
optimization	Task
through	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

Therefore	O
,	O
in	O
our	O
formulation	O
,	O
CRF	Method
parameters	O
such	O
as	O
the	O
weights	O
of	O
the	O
Gaussian	O
kernels	O
and	O
the	O
label	O
compatibility	O
function	O
can	O
also	O
be	O
optimized	O
automatically	O
during	O
the	O
training	O
of	O
the	O
full	Method
network	Method
.	O

Once	O
the	O
individual	O
steps	O
of	O
the	O
algorithm	O
are	O
broken	O
down	O
as	O
CNN	Method
layers	O
,	O
the	O
full	Method
algorithm	Method
can	O
then	O
be	O
formulated	O
as	O
an	O
RNN	Method
.	O

We	O
explain	O
this	O
in	O
Section	O
5	O
after	O
discussing	O
the	O
steps	O
of	O
Algorithm	O
1	O
in	O
detail	O
below	O
.	O

In	O
Algorithm	O
1	O
and	O
the	O
remainder	O
of	O
this	O
paper	O
,	O
we	O
use	O
U	O
i	O
(	O
l	O
)	O
to	O
denote	O
the	O
negative	O
of	O
the	O
unary	O
energy	O
introduced	O
in	O
the	O
previous	O
section	O
,	O
i.e.	O
,	O
U	O
i	O
(	O
l	O
)	O
=	O
−ψ	O
u	O
(	O
X	O
i	O
=	O
l	O
)	O
.	O

In	O
the	O
conventional	O
CRF	Method
setting	O
,	O
this	O
input	O
U	O
i	O
(	O
l	O
)	O
to	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
is	O
obtained	O
from	O
an	O
independent	Method
classifier	Method
.	O

section	O
:	O
Initialization	O
In	O
the	O
initialization	O
step	O
of	O
the	O
algorithm	O
,	O
the	O
operation	O
,	O
is	O
performed	O
.	O

Note	O
that	O
this	O
is	O
equivalent	O
to	O
applying	O
a	O
softmax	O
function	O
over	O
the	O
unary	O
potentials	O
U	O
across	O
all	O
the	O
labels	O
at	O
each	O
pixel	O
.	O

The	O
softmax	Method
function	Method
has	O
been	O
extensively	O
used	O
in	O
CNN	Method
architectures	O
before	O
and	O
is	O
therefore	O
well	O
known	O
in	O
the	O
deep	Task
learning	Task
community	Task
.	O

This	O
operation	O
does	O
not	O
include	O
any	O
parameters	O
and	O
the	O
error	O
differentials	O
received	O
at	O
the	O
output	O
of	O
the	O
step	O
during	O
back	Method
-	Method
propagation	Method
could	O
be	O
passed	O
down	O
to	O
the	O
unary	O
potential	O
inputs	O
after	O
performing	O
usual	O
backward	Method
pass	Method
calculations	Method
of	O
the	O
softmax	Method
transformation	Method
.	O

section	O
:	O
Message	Task
Passing	Task
In	O
the	O
dense	O
CRF	Method
formulation	O
,	O
message	Task
passing	Task
is	O
implemented	O
by	O
applying	O
M	Method
Gaussian	Method
filters	Method
on	O
Q	O
values	O
.	O

Gaussian	Method
filter	Method
coefficients	Method
are	O
derived	O
based	O
on	O
image	O
features	O
such	O
as	O
the	O
pixel	O
locations	O
and	O
RGB	O
values	O
,	O
which	O
reflect	O
how	O
strongly	O
a	O
pixel	O
is	O
related	O
to	O
other	O
pixels	O
.	O

Since	O
the	O
CRF	Method
is	O
potentially	O
fully	O
connected	O
,	O
each	O
filter	O
's	O
receptive	O
field	O
spans	O
the	O
whole	O
image	O
,	O
making	O
it	O
infeasible	O
to	O
use	O
a	O
brute	O
-	O
force	O
implementation	O
of	O
the	O
filters	O
.	O

Fortunately	O
,	O
several	O
approximation	Method
techniques	Method
exist	O
to	O
make	O
computation	O
of	O
high	Task
dimensional	Task
Gaussian	Task
filtering	Task
significantly	O
faster	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
the	O
Permutohedral	Method
lattice	Method
implementation	Method
[	O
reference	O
]	O
,	O
which	O
can	O
compute	O
the	O
filter	O
response	O
in	O
O	O
(	O
N	O
)	O
time	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
pixels	O
of	O
the	O
image	O
[	O
reference	O
]	O
.	O

During	O
back	Method
-	Method
propagation	Method
,	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
inputs	O
are	O
calculated	O
by	O
sending	O
the	O
error	O
derivatives	O
w.r.t	O
.	O

the	O
filter	O
outputs	O
through	O
the	O
same	O
M	O
Gaussian	Method
filters	Method
in	O
reverse	O
direction	O
.	O

In	O
terms	O
of	O
permutohedral	Method
lattice	Method
operations	Method
,	O
this	O
can	O
be	O
accomplished	O
by	O
only	O
reversing	O
the	O
order	O
of	O
the	O
separable	Method
filters	Method
in	O
the	O
blur	Method
stage	Method
,	O
while	O
building	O
the	O
permutohedral	O
lattice	O
,	O
splatting	Method
,	O
and	O
slicing	O
in	O
the	O
same	O
way	O
as	O
in	O
the	O
forward	O
pass	O
.	O

Therefore	O
,	O
back	Method
-	Method
propagation	Method
through	O
this	O
filtering	Method
stage	Method
can	O
also	O
be	O
performed	O
in	O
O	O
(	O
N	O
)	O
time	O
.	O

Following	O
[	O
reference	O
]	O
,	O
we	O
use	O
two	O
Gaussian	Method
kernels	Method
,	O
a	O
spatial	Method
kernel	Method
and	O
a	O
bilateral	Method
kernel	Method
.	O

In	O
this	O
work	O
,	O
for	O
simplicity	O
,	O
we	O
keep	O
the	O
bandwidth	O
values	O
of	O
the	O
filters	O
fixed	O
.	O

It	O
is	O
also	O
possible	O
to	O
use	O
multiple	O
spatial	O
and	O
bilateral	O
kernels	O
with	O
different	O
bandwidth	O
values	O
and	O
learn	O
their	O
optimal	O
linear	Method
combination	Method
.	O

section	O
:	O
Weighting	Method
Filter	Method
Outputs	O
The	O
next	O
step	O
of	O
the	O
mean	Method
-	Method
field	Method
iteration	Method
is	O
taking	O
a	O
weighted	O
sum	O
of	O
the	O
M	O
filter	O
outputs	O
from	O
the	O
previous	O
step	O
,	O
for	O
each	O
class	O
label	O
l.	O
When	O
each	O
class	O
label	O
is	O
considered	O
individually	O
,	O
this	O
can	O
be	O
viewed	O
as	O
usual	O
convolution	Method
with	O
a	O
1	Method
×	Method
1	Method
filter	Method
with	O
M	O
input	O
channels	O
,	O
and	O
one	O
output	O
channel	O
.	O

Since	O
both	O
inputs	O
and	O
the	O
outputs	O
to	O
this	O
step	O
are	O
known	O
during	O
back	Method
-	Method
propagation	Method
,	O
the	O
error	O
derivative	O
w.r.t	O
.	O

the	O
filter	O
weights	O
can	O
be	O
computed	O
,	O
making	O
it	O
possible	O
to	O
automatically	O
learn	O
the	O
filter	O
weights	O
(	O
relative	O
contributions	O
from	O
each	O
Gaussian	Method
filter	Method
output	O
from	O
the	O
previous	O
stage	O
)	O
.	O

Error	O
derivative	O
w.r.t	O
.	O

the	O
inputs	O
can	O
also	O
be	O
computed	O
in	O
the	O
usual	O
manner	O
to	O
pass	O
the	O
error	O
derivatives	O
down	O
to	O
the	O
previous	O
stage	O
.	O

To	O
obtain	O
a	O
higher	O
number	O
of	O
tunable	O
parameters	O
,	O
in	O
contrast	O
to	O
[	O
reference	O
]	O
,	O
we	O
use	O
independent	O
kernel	O
weights	O
for	O
each	O
class	O
label	O
.	O

The	O
intuition	O
is	O
that	O
the	O
relative	O
importance	O
of	O
the	O
spatial	O
kernel	O
vs	O
the	O
bilateral	Method
kernel	Method
depends	O
on	O
the	O
visual	O
class	O
.	O

For	O
example	O
,	O
bilateral	O
kernels	O
may	O
have	O
on	O
the	O
one	O
hand	O
a	O
high	O
importance	O
in	O
bicycle	Task
detection	Task
,	O
because	O
similarity	O
of	O
colours	O
is	O
determinant	O
;	O
on	O
the	O
other	O
hand	O
they	O
may	O
have	O
low	O
importance	O
for	O
TV	Task
detection	Task
,	O
given	O
that	O
whatever	O
is	O
inside	O
the	O
TV	O
screen	O
may	O
have	O
many	O
different	O
colours	O
.	O

section	O
:	O
Compatibility	Method
Transform	Method
In	O
the	O
compatibility	Method
transform	Method
step	Method
,	O
outputs	O
from	O
the	O
previous	O
step	O
(	O
denoted	O
byQ	O
in	O
Algorithm	O
1	O
)	O
are	O
shared	O
between	O
the	O
labels	O
to	O
a	O
varied	O
extent	O
,	O
depending	O
on	O
the	O
compatibility	O
between	O
these	O
labels	O
.	O

Compatibility	O
between	O
the	O
two	O
labels	O
l	O
and	O
l	O
is	O
parameterized	O
by	O
the	O
label	O
compatibility	O
function	O
µ	O
(	O
l	O
,	O
l	O
)	O
.	O

The	O
Potts	Method
model	Method
,	O
given	O
by	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
[	O
l	O
=	O
l	O
]	O
,	O
where	O
[	O
.	O

]	O
is	O
the	O
Iverson	O
bracket	O
,	O
assigns	O
a	O
fixed	O
penalty	O
if	O
different	O
labels	O
are	O
assigned	O
to	O
pixels	O
with	O
similar	O
properties	O
.	O

A	O
limitation	O
of	O
this	O
model	O
is	O
that	O
it	O
assigns	O
the	O
same	O
penalty	O
for	O
all	O
different	O
pairs	O
of	O
labels	O
.	O

Intuitively	O
,	O
better	O
results	O
can	O
be	O
obtained	O
by	O
taking	O
the	O
compatibility	O
between	O
different	O
label	O
pairs	O
into	O
account	O
and	O
penalizing	O
the	O
assignments	O
accordingly	O
.	O

For	O
example	O
,	O
assigning	O
labels	O
"	O
person	O
"	O
and	O
"	O
bicycle	O
"	O
to	O
nearby	O
pixels	O
should	O
have	O
a	O
lesser	O
penalty	O
than	O
assigning	O
labels	O
"	O
sky	O
"	O
and	O
"	O
bicycle	O
"	O
.	O

Therefore	O
,	O
learning	O
the	O
function	O
µ	O
from	O
data	O
is	O
preferred	O
to	O
fixing	O
it	O
in	O
advance	O
with	O
Potts	Method
model	Method
.	O

We	O
also	O
relax	O
our	O
compatibility	Method
transform	Method
model	Method
by	O
assuming	O
that	O
µ	O
(	O
l	O
,	O
l	O
)	O
=	O
µ	O
(	O
l	O
,	O
l	O
)	O
in	O
general	O
.	O

Compatibility	Task
transform	Task
step	Task
can	O
be	O
viewed	O
as	O
another	O
convolution	Method
layer	Method
where	O
the	O
spatial	O
receptive	O
field	O
of	O
the	O
filter	O
is	O
1	O
×	O
1	O
,	O
and	O
the	O
number	O
of	O
input	O
and	O
output	O
channels	O
are	O
both	O
L.	O
Learning	O
the	O
weights	O
of	O
this	O
filter	O
is	O
equivalent	O
to	O
learning	O
the	O
label	O
compatibility	O
function	O
µ.	O
Transferring	O
error	O
differentials	O
from	O
the	O
output	O
of	O
this	O
step	O
to	O
the	O
input	O
can	O
be	O
done	O
since	O
this	O
step	O
is	O
a	O
usual	O
convolution	Method
operation	Method
.	O

section	O
:	O
Adding	O
Unary	O
Potentials	O
In	O
this	O
step	O
,	O
the	O
output	O
from	O
the	O
compatibility	Method
transform	Method
stage	Method
is	O
subtracted	O
element	O
-	O
wise	O
from	O
the	O
unary	O
inputs	O
U	O
.	O

While	O
no	O
parameters	O
are	O
involved	O
in	O
this	O
step	O
,	O
transferring	O
error	O
differentials	O
can	O
be	O
done	O
trivially	O
by	O
copying	O
the	O
differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
to	O
both	O
inputs	O
with	O
the	O
appropriate	O
sign	O
.	O

section	O
:	O
Normalization	Task
Finally	O
,	O
the	O
normalization	O
step	O
of	O
the	O
iteration	O
can	O
be	O
considered	O
as	O
another	O
softmax	Method
operation	Method
with	O
no	O
parameters	O
.	O

Differentials	O
at	O
the	O
output	O
of	O
this	O
step	O
can	O
be	O
passed	O
on	O
to	O
the	O
input	O
using	O
the	O
softmax	Method
operation	Method
's	Method
backward	Method
pass	Method
.	O

section	O
:	O
The	O
End	O
-	O
to	O
-	O
end	O
Trainable	Method
Network	Method
We	O
now	O
describe	O
our	O
end	O
-	O
to	O
-	O
end	Method
deep	Method
learning	Method
system	Method
for	O
semantic	Task
image	Task
segmentation	Task
.	O

To	O
pave	O
the	O
way	O
for	O
this	O
,	O
we	O
first	O
explain	O
how	O
repeated	O
mean	Method
-	Method
field	Method
iterations	Method
can	O
be	O
organized	O
as	O
an	O
RNN	Method
.	O

section	O
:	O
CRF	Method
as	O
RNN	Method
In	O
the	O
previous	O
section	O
,	O
it	O
was	O
shown	O
that	O
one	O
iteration	O
of	O
the	O
mean	Method
-	Method
field	Method
algorithm	Method
can	O
be	O
formulated	O
as	O
a	O
stack	O
of	O
common	O
CNN	Method
layers	O
(	O
see	O
Fig	O
.	O

1	O
)	O
.	O

We	O
use	O
the	O
function	O
f	Method
θ	Method
to	O
denote	O
the	O
transformation	O
done	O
by	O
one	O
mean	Method
-	Method
field	Method
iteration	Method
:	O
given	O
an	O
image	O
I	O
,	O
pixel	O
-	O
wise	O
unary	O
potential	O
values	O
U	O
and	O
an	O
estimation	O
of	O
marginal	O
probabilities	O
Q	O
in	O
from	O
the	O
previous	O
iteration	O
,	O
the	O
next	O
estimation	Task
of	Task
marginal	Task
distributions	Task
after	O
one	O
mean	Method
-	Method
field	Method
iteration	Method
is	O
given	O
by	O
f	Method
θ	Method
(	O
U	O
,	O
Q	O
in	O
,	O
I	O
)	O
.	O

..	O
,	O
l	O
L	O
}	O
represents	O
the	O
CRF	Method
parameters	O
described	O
in	O
Section	O
4	O
.	O

Multiple	O
mean	Method
-	Method
field	Method
iterations	Method
can	O
be	O
implemented	O
by	O
repeating	O
the	O
above	O
stack	O
of	O
layers	O
in	O
such	O
a	O
way	O
that	O
each	O
iteration	O
takes	O
Q	O
value	O
estimates	O
from	O
the	O
previous	O
iteration	O
and	O
the	O
unary	O
values	O
in	O
their	O
original	O
form	O
.	O

This	O
is	O
equivalent	O
to	O
treating	O
the	O
iterative	Method
mean	Method
-	Method
field	Method
inference	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
as	O
shown	O
in	O
Fig	O
.	O

2	O
.	O

Using	O
the	O
notation	O
in	O
the	O
figure	O
,	O
the	O
behaviour	O
of	O
the	O
network	O
is	O
given	O
by	O
the	O
following	O
equations	O
where	O
T	O
is	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
:	O
We	O
name	O
this	O
RNN	Method
structure	O
CRF	Method
-	Method
RNN	Method
.	O

Parameters	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
are	O
the	O
same	O
as	O
the	O
mean	O
-	O
field	O
parameters	O
described	O
in	O
Section	O
4	O
and	O
denoted	O
by	O
θ	O
here	O
.	O

Since	O
the	O
calculation	O
of	O
error	O
differentials	O
w.r.t	O
.	O

these	O
parameters	O
in	O
a	O
single	O
iteration	O
was	O
described	O
in	O
Section	O
4	O
,	O
they	O
can	O
be	O
learnt	O
in	O
the	O
RNN	Method
setting	O
using	O
the	O
standard	O
back	Method
-	Method
propagation	Method
through	Method
time	Method
algorithm	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

It	O
was	O
shown	O
in	O
[	O
reference	O
]	O
that	O
the	O
mean	Method
-	Method
field	Method
iterative	Method
algorithm	Method
for	O
dense	O
CRF	Method
converges	O
in	O
less	O
than	O
10	O
iterations	O
.	O

Furthermore	O
,	O
in	O
practice	O
,	O
after	O
Figure	O
2	O
.	O

The	O
CRF	Method
-	O
RNN	Method
Network	O
.	O

We	O
formulate	O
the	O
iterative	Method
mean	Method
-	Method
field	Method
algorithm	Method
as	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O

Gating	O
functions	O
G1	O
and	O
G2	O
are	O
fixed	O
as	O
described	O
in	O
the	O
text	O
.	O

about	O
5	O
iterations	O
,	O
increasing	O
the	O
number	O
of	O
iterations	O
usually	O
does	O
not	O
significantly	O
improve	O
results	O
[	O
reference	O
]	O
.	O

Therefore	O
,	O
it	O
does	O
not	O
suffer	O
from	O
the	O
vanishing	Task
and	Task
exploding	Task
gradient	Task
problem	Task
inherent	O
to	O
deep	O
RNNs	Method
[	O
reference	O
][	O
reference	O
]	O
.	O

This	O
allows	O
us	O
to	O
use	O
a	O
plain	O
RNN	Method
architecture	O
instead	O
of	O
more	O
sophisticated	O
architectures	O
such	O
as	O
LSTMs	Method
in	O
our	O
network	O
.	O

section	O
:	O
Completing	O
the	O
Picture	O
Our	O
approach	O
comprises	O
a	O
fully	Method
convolutional	Method
network	Method
stage	Method
,	O
which	O
predicts	O
pixel	O
-	O
level	O
labels	O
without	O
considering	O
structure	O
,	O
followed	O
by	O
a	O
CRF	Method
-	O
RNN	Method
stage	O
,	O
which	O
performs	O
CRF	Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
for	O
structured	Task
prediction	Task
.	O

The	O
complete	O
system	O
,	O
therefore	O
,	O
unifies	O
strengths	O
of	O
both	O
CNNs	Method
and	O
CRFs	Method
and	O
is	O
trainable	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
[	O
reference	O
]	O
and	O
the	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
procedure	O
.	O

During	O
training	Task
,	O
a	O
whole	O
image	O
(	O
or	O
many	O
of	O
them	O
)	O
can	O
be	O
used	O
as	O
the	O
mini	O
-	O
batch	O
and	O
the	O
error	O
at	O
each	O
pixel	O
output	O
of	O
the	O
network	O
can	O
be	O
computed	O
using	O
an	O
appropriate	O
loss	Method
function	Method
such	O
as	O
the	O
softmax	Method
loss	Method
with	O
respect	O
to	O
the	O
ground	Task
truth	Task
segmentation	Task
of	O
the	O
image	O
.	O

We	O
used	O
the	O
FCN	Method
-	Method
8s	Method
architecture	Method
of	O
[	O
reference	O
]	O
as	O
the	O
first	O
part	O
of	O
our	O
network	O
,	O
which	O
provides	O
unary	O
potentials	O
to	O
the	O
CRF	Method
.	O

This	O
network	O
is	O
based	O
on	O
the	O
VGG	Method
-	Method
16	Method
network	Method
[	O
reference	O
]	O
but	O
has	O
been	O
restructured	O
to	O
perform	O
pixel	Task
-	Task
wise	Task
prediction	Task
instead	O
of	O
image	Task
classification	Task
.	O

The	O
complete	O
architecture	O
of	O
our	O
network	O
,	O
including	O
the	O
FCN8s	Method
part	Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

In	O
the	O
forward	O
pass	O
through	O
the	O
network	O
,	O
once	O
the	O
computation	O
enters	O
the	O
CRF	Method
-	Method
RNN	Method
after	O
passing	O
through	O
the	O
CNN	Method
stage	O
,	O
it	O
takes	O
T	O
iterations	O
for	O
the	O
data	O
to	O
leave	O
the	O
loop	O
created	O
by	O
the	O
RNN	Method
.	O

Neither	O
the	O
CNN	Method
that	O
provides	O
unary	O
values	O
nor	O
the	O
layers	O
after	O
the	O
CRF	Method
-	Method
RNN	Method
(	O
i.e.	O
,	O
the	O
loss	O
layers	O
)	O
need	O
to	O
perform	O
any	O
computations	O
during	O
this	O
time	O
since	O
the	O
refinement	O
happens	O
only	O
inside	O
the	O
RNN	Method
's	O
loop	O
.	O

Once	O
the	O
output	O
Y	O
leaves	O
the	O
loop	O
,	O
next	O
stages	O
of	O
the	O
deep	Method
network	Method
after	O
the	O
CRF	Method
-	Method
RNN	Method
can	O
continue	O
the	O
forward	O
pass	O
.	O

In	O
our	O
setup	O
,	O
a	O
softmax	Method
loss	Method
layer	Method
directly	O
follows	O
the	O
CRF	Method
-	Method
RNN	Method
and	O
terminates	O
the	O
network	O
.	O

During	O
the	O
backward	O
pass	O
,	O
once	O
the	O
error	O
differentials	O
reach	O
the	O
CRF	Method
-	Method
RNN	Method
's	O
output	O
Y	O
,	O
they	O
similarly	O
spend	O
T	O
iterations	O
within	O
the	O
loop	O
before	O
reaching	O
the	O
RNN	Method
input	O
U	O
in	O
order	O
to	O
propagate	O
to	O
the	O
CNN	Method
which	O
provides	O
the	O
unary	O
input	O
.	O

In	O
each	O
iteration	O
inside	O
the	O
loop	O
,	O
error	O
differentials	O
are	O
computed	O
inside	O
each	O
component	O
of	O
the	O
mean	Method
-	Method
field	Method
iteration	Method
as	O
described	O
in	O
Section	O
4	O
.	O

We	O
note	O
that	O
unnecessarily	O
increasing	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
could	O
potentially	O
result	O
in	O
the	O
vanishing	Task
and	Task
exploding	Task
gradient	Task
problems	Task
in	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

We	O
,	O
however	O
,	O
did	O
not	O
experience	O
this	O
problem	O
during	O
our	O
experiments	O
.	O

section	O
:	O
Implementation	O
Details	O
In	O
the	O
present	O
section	O
we	O
describe	O
the	O
implementation	O
details	O
of	O
the	O
proposed	O
network	O
,	O
as	O
well	O
as	O
its	O
training	Method
process	Method
.	O

The	O
high	O
-	O
level	O
architecture	O
of	O
our	O
system	O
,	O
which	O
was	O
implemented	O
using	O
the	O
popular	O
Caffe	O
[	O
reference	O
]	O
deep	O
learning	O
library	O
,	O
is	O
shown	O
in	O
Fig	O
.	O

3	O
.	O

Complete	O
architecture	O
of	O
the	O
deep	Method
network	Method
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O

The	O
full	O
source	O
code	O
and	O
the	O
trained	O
models	O
of	O
our	O
approach	O
will	O
be	O
made	O
publicly	O
available	O
.	O

We	O
initialized	O
the	O
first	O
part	O
of	O
the	O
network	O
using	O
the	O
publicly	O
available	O
weights	O
of	O
the	O
FCN	Method
-	Method
8s	Method
network	Method
[	O
reference	O
]	O
.	O

The	O
compatibility	O
transform	O
parameters	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
were	O
initialized	O
using	O
the	O
Potts	Method
model	Method
,	O
and	O
kernel	O
width	O
and	O
weight	O
parameters	O
were	O
obtained	O
from	O
a	O
cross	Method
-	Method
validation	Method
process	Method
.	O

We	O
found	O
that	O
such	O
initialization	O
results	O
in	O
faster	O
convergence	Task
of	Task
training	Task
.	O

During	O
the	O
training	O
phase	O
,	O
parameters	O
of	O
the	O
whole	O
network	O
were	O
optimized	O
end	O
-	O
to	O
-	O
end	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
.	O

In	O
particular	O
we	O
used	O
full	Task
image	Task
training	Task
described	O
in	O
[	O
reference	O
]	O
,	O
with	O
learning	Metric
rate	Metric
fixed	O
at	O
10	O
and	O
momentum	O
set	O
to	O
0.99	O
.	O

These	O
extreme	O
values	O
of	O
the	O
parameters	O
were	O
used	O
since	O
we	O
employed	O
only	O
one	O
image	O
per	O
batch	O
to	O
avoid	O
reaching	O
memory	O
limits	O
of	O
the	O
GPU	O
.	O

In	O
all	O
our	O
experiments	O
,	O
during	O
training	Task
,	O
we	O
set	O
the	O
number	O
of	O
mean	O
-	O
field	O
iterations	O
T	O
in	O
the	O
CRF	Method
-	Method
RNN	Method
to	O
5	O
to	O
avoid	O
vanishing	Task
/	Task
exploding	Task
gradient	Task
problems	Task
and	O
to	O
reduce	O
the	O
training	Metric
time	Metric
.	O

During	O
the	O
test	O
time	O
,	O
iteration	O
count	O
was	O
increased	O
to	O
10	O
.	O

The	O
effect	O
of	O
this	O
parameter	O
value	O
on	O
the	O
accuracy	Metric
is	O
discussed	O
in	O
section	O
7.1	O
.	O

Loss	Method
function	Method
During	O
the	O
training	O
of	O
the	O
models	O
that	O
achieved	O
the	O
best	O
results	O
reported	O
in	O
this	O
paper	O
,	O
we	O
used	O
the	O
standard	O
softmax	Method
loss	Method
function	Method
,	O
that	O
is	O
,	O
the	O
log	Method
-	Method
likelihood	Method
error	Method
function	Method
described	O
in	O
[	O
reference	O
]	O
.	O

The	O
standard	O
metric	O
used	O
in	O
the	O
Pascal	Material
VOC	Material
challenge	Material
is	O
the	O
average	Metric
intersection	Metric
over	Metric
union	Metric
(	O
IU	Metric
)	O
,	O
which	O
we	O
also	O
use	O
here	O
to	O
report	O
the	O
results	O
.	O

In	O
our	O
experiments	O
we	O
found	O
that	O
high	O
values	O
of	O
IU	Metric
on	O
the	O
validation	O
set	O
were	O
associated	O
to	O
low	O
values	O
of	O
the	O
averaged	Metric
softmax	Metric
loss	Metric
,	O
to	O
a	O
large	O
extent	O
.	O

We	O
also	O
tried	O
the	O
robust	O
loglikelihood	O
in	O
[	O
reference	O
]	O
as	O
a	O
loss	O
function	O
for	O
CRF	Method
-	O
RNN	Method
training	O
.	O

However	O
,	O
this	O
did	O
not	O
result	O
in	O
increased	O
accuracy	Metric
nor	O
faster	O
convergence	Metric
.	O

Normalization	Method
techniques	Method
As	O
described	O
in	O
Section	O
4	O
,	O
we	O
use	O
the	O
exponential	O
function	O
followed	O
by	O
pixel	Method
-	Method
wise	Method
normalization	Method
across	O
channels	O
in	O
several	O
stages	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

Since	O
this	O
operation	O
has	O
a	O
tendency	O
to	O
result	O
in	O
small	O
gradients	O
with	O
respect	O
to	O
the	O
input	O
when	O
the	O
input	O
value	O
is	O
large	O
,	O
we	O
conducted	O
several	O
experiments	O
where	O
we	O
replaced	O
this	O
by	O
a	O
rectifier	Method
linear	Method
unit	Method
(	O
ReLU	Method
)	Method
operation	Method
followed	O
by	O
a	O
normalization	Method
across	O
the	O
channels	O
.	O

Our	O
hypothesis	O
was	O
that	O
this	O
approach	O
may	O
approximate	O
the	O
original	O
operation	O
adequately	O
while	O
speeding	O
up	O
the	O
training	Task
due	O
to	O
improved	O
gradients	O
.	O

Furthermore	O
,	O
ReLU	Method
would	O
induce	O
sparsity	O
on	O
the	O
probability	O
of	O
labels	O
assigned	O
to	O
pixels	O
,	O
implicitly	O
pruning	O
low	O
likelihood	O
configurations	O
,	O
which	O
could	O
have	O
a	O
positive	O
effect	O
.	O

However	O
,	O
this	O
approach	O
did	O
not	O
lead	O
to	O
better	O
results	O
,	O
obtaining	O
1	O
%	O
IU	Metric
lower	O
than	O
the	O
original	O
setting	O
performance	O
.	O

section	O
:	O
Experiments	O
We	O
present	O
experimental	O
results	O
with	O
the	O
proposed	O
CRF	Method
-	O
RNN	Method
framework	O
.	O

We	O
use	O
these	O
datasets	O
:	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
,	O
and	O
the	O
Pascal	Material
Context	Material
dataset	Material
.	O

We	O
use	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
as	O
it	O
has	O
become	O
the	O
golden	O
standard	O
to	O
comprehensively	O
evaluate	O
any	O
new	O
semantic	Task
segmentation	Task
approach	O
in	O
comparison	O
to	O
existing	O
methods	O
.	O

We	O
also	O
use	O
the	O
Pascal	Material
Context	Material
dataset	Material
to	O
assess	O
how	O
well	O
our	O
approach	O
performs	O
on	O
a	O
dataset	O
with	O
different	O
characteristics	O
.	O

section	O
:	O
Pascal	Material
VOC	Material
Datasets	Material
In	O
order	O
to	O
evaluate	O
our	O
approach	O
with	O
existing	O
methods	O
under	O
the	O
same	O
circumstances	O
,	O
we	O
conducted	O
two	O
main	O
experiments	O
with	O
the	O
Pascal	Material
VOC	Material
2012	Material
dataset	Material
,	O
followed	O
by	O
a	O
qualitative	O
experiment	O
.	O

In	O
the	O
first	O
experiment	O
,	O
following	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
we	O
used	O
a	O
training	O
set	O
consisted	O
of	O
VOC	Material
2012	Material
training	Material
data	Material
(	O
1464	O
images	O
)	O
,	O
and	O
training	O
and	O
validation	O
data	O
of	O
[	O
reference	O
]	O
,	O
which	O
amounts	O
to	O
a	O
total	O
of	O
11	O
,	O
685	O
images	O
.	O

After	O
removing	O
the	O
overlapping	O
images	O
between	O
VOC	Material
2012	Material
validation	Material
data	Material
and	O
this	O
training	O
dataset	O
,	O
we	O
were	O
left	O
with	O
346	O
images	O
from	O
the	O
original	O
VOC	Material
2012	Material
validation	Material
set	Material
to	O
validate	O
our	O
models	O
on	O
.	O

We	O
call	O
this	O
set	O
the	O
reduced	Metric
validation	Metric
set	Metric
in	O
the	O
sequel	O
.	O

Annotations	O
of	O
the	O
VOC	Material
2012	Material
test	Material
set	Material
,	O
which	O
consists	O
of	O
1456	O
images	O
,	O
are	O
not	O
publicly	O
available	O
and	O
hence	O
the	O
final	O
results	O
on	O
the	O
test	O
set	O
were	O
obtained	O
by	O
submitting	O
the	O
results	O
to	O
the	O
Pascal	Material
VOC	Material
challenge	Material
evaluation	Material
server	Material
[	O
reference	O
]	O
.	O

Regardless	O
of	O
the	O
smaller	O
number	O
of	O
images	O
,	O
we	O
found	O
that	O
the	O
relative	O
improvements	O
of	O
the	O
accuracy	Metric
on	O
our	O
validation	O
set	O
were	O
in	O
good	O
agreement	O
with	O
the	O
test	O
set	O
.	O

As	O
a	O
first	O
step	O
we	O
directly	O
compared	O
the	O
potential	O
advantage	O
of	O
learning	O
the	O
model	O
end	O
-	O
to	O
-	O
end	O
with	O
respect	O
to	O
alternative	O
learning	Method
strategies	Method
.	O

These	O
are	O
plain	O
FCN	Method
-	Method
8s	Method
without	O
applying	O
CRF	Method
,	O
and	O
with	O
CRF	Method
as	O
a	O
postprocessing	Method
method	Method
disconnected	O
from	O
the	O
training	O
of	O
FCN	Method
,	O
which	O
is	O
comparable	O
to	O
the	O
approach	O
described	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
reported	O
in	O
Table	O
1	O
and	O
show	O
a	O
clear	O
advantage	O
of	O
the	O
end	O
-	O
to	O
-	O
end	Method
strategy	Method
over	O
the	O
offline	Method
application	Method
of	O
CRF	Method
as	O
a	O
post	Method
-	Method
processing	Method
method	Method
.	O

This	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
during	O
the	O
SGD	Method
training	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
,	O
the	O
CNN	Method
component	O
and	O
the	O
CRF	Method
component	O
learn	O
how	O
to	O
co	O
-	O
operate	O
with	O
each	O
other	O
to	O
produce	O
the	O
optimum	O
output	O
of	O
the	O
whole	O
network	O
.	O

We	O
then	O
proceeded	O
to	O
compare	O
our	O
approach	O
with	O
all	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
that	O
used	O
training	O
data	O
from	O
the	O
standard	O
VOC	Material
2012	Material
training	Material
and	Material
validation	Material
sets	Material
,	O
and	O
from	O
the	O
dataset	O
published	O
with	O
[	O
reference	O
]	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
2	O
,	O
above	O
the	O
bar	O
,	O
and	O
we	O
can	O
see	O
that	O
our	O
approach	O
outperforms	O
all	O
competitors	O
.	O

In	O
the	O
second	O
experiment	O
,	O
in	O
addition	O
to	O
the	O
above	O
training	O
set	O
,	O
we	O
used	O
data	O
from	O
the	O
Microsoft	O
COCO	Material
dataset	O
[	O
reference	O
]	O
as	O
was	O
done	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

We	O
selected	O
images	O
from	O
MS	O
COCO	Material
2014	O
training	O
set	O
where	O
the	O
ground	Task
truth	Task
segmentation	Task
has	O
at	O
least	O
200	O
pixels	O
marked	O
with	O
classes	O
labels	O
present	O
in	O
the	O
VOC	Material
2012	Material
dataset	Material
.	O

With	O
this	O
selection	O
,	O
we	O
ended	O
up	O
using	O
66	O
,	O
099	O
images	O
from	O
the	O
COCO	Material
dataset	O
and	O
therefore	O
a	O
total	O
of	O
66	O
,	O
099	O
+	O
11	O
,	O
685	O
=	O
77	O
,	O
784	O
training	O
images	O
were	O
used	O
in	O
the	O
second	O
experiment	O
.	O

The	O
same	O
reduced	Metric
validation	Metric
set	Metric
was	O
used	O
in	O
this	O
second	O
experiment	O
as	O
well	O
.	O

In	O
this	O
case	O
,	O
we	O
first	O
fine	O
-	O
tuned	O
the	O
plain	Method
FCN	Method
-	Method
32s	Method
network	Method
(	O
without	O
the	O
CRF	Method
-	Method
RNN	Method
part	Method
)	O
on	O
COCO	Material
data	O
,	O
then	O
we	O
built	O
an	O
FCN	Method
-	Method
8s	Method
network	Method
with	O
the	O
learnt	O
weights	O
and	O
finally	O
train	O
the	O
CRF	Method
-	O
RNN	Method
network	O
end	O
-	O
to	O
-	O
end	O
using	O
VOC	Material
2012	Material
training	Material
data	Material
only	O
.	O

Since	O
the	O
MS	O
COCO	Material
ground	Task
truth	Task
segmentation	Task
data	O
contains	O
somewhat	O
coarse	O
segmentation	O
masks	O
where	O
objects	O
are	O
not	O
delineated	O
properly	O
,	O
we	O
found	O
that	O
fine	O
-	O
tuning	O
our	O
model	O
with	O
COCO	Material
did	O
not	O
yield	O
significant	O
improvements	O
.	O

This	O
can	O
be	O
understood	O
because	O
the	O
primary	O
advantage	O
of	O
our	O
model	O
comes	O
from	O
delineating	O
the	O
objects	O
and	O
improving	O
fine	O
segmentation	O
boundaries	O
.	O

The	O
VOC	Material
2012	Material
training	Material
dataset	Material
therefore	O
helps	O
our	O
model	O
learn	O
this	O
task	O
effectively	O
.	O

The	O
results	O
of	O
this	O
experiment	O
are	O
shown	O
in	O
Table	O
2	O
,	O
below	O
the	O
bar	O
,	O
and	O
we	O
see	O
that	O
our	O
approach	O
sets	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
VOC	Material
2012	Material
dataset	Material
.	O

Note	O
that	O
in	O
both	O
setups	O
,	O
our	O
approach	O
outperforms	O
competing	O
methods	O
due	O
to	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
of	O
the	O
CNN	Method
and	O
CRF	Method
in	O
the	O
unified	O
CRF	Method
-	O
RNN	Method
framework	O
.	O

We	O
also	O
evaluated	O
our	O
models	O
on	O
the	O
VOC	Material
2010	Material
,	O
and	O
VOC	Material
2011	Material
test	Material
set	Material
(	O
see	O
Table	O
2	O
)	O
.	O

In	O
all	O
cases	O
our	O
method	O
achieves	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
order	O
to	O
have	O
a	O
qualitative	O
evidence	O
about	O
how	O
CRF	Method
-	Method
RNN	Method
learns	O
,	O
we	O
visualize	O
the	O
compatibility	O
function	O
learned	O
after	O
the	O
training	O
stage	O
of	O
the	O
CRF	Method
-	Method
RNN	Method
as	O
a	O
matrix	Method
representation	Method
in	O
Fig	O
.	O

5	O
.	O

Element	O
(	O
i	O
,	O
j	O
)	O
of	O
this	O
matrix	O
corresponds	O
to	O
µ	O
(	O
i	O
,	O
j	O
)	O
defined	O
earlier	O
:	O
a	O
high	O
value	O
at	O
(	O
i	O
,	O
j	O
)	O
implies	O
high	O
penalty	O
for	O
assigning	O
label	O
i	O
to	O
a	O
pixel	O
when	O
a	O
similar	O
pixel	O
(	O
spatially	O
or	O
appearance	O
wise	O
)	O
is	O
assigned	O
label	O
j.	O
For	O
example	O
we	O
can	O
appreciate	O
that	O
the	O
learned	O
compatibility	O
matrix	O
assigns	O
a	O
low	O
penalty	O
to	O
pairs	O
of	O
labels	O
that	O
tend	O
to	O
appear	O
together	O
,	O
such	O
as	O
[	O
Motorbike	O
,	O
Person	O
]	O
,	O
and	O
[	O
Dining	O
[	O
reference	O
]	O
n	O
/	O
a	O
39.1	O
n	O
/	O
a	O
O2PCPMC	O
[	O
reference	O
]	O
49.6	O
48.8	O
47.8	O
Divmbest	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
48.1	O
NUS	O
-	O
UDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
50.0	O
SDS	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
51.6	O
MSRA	Method
-	Method
CFM	Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
61.8	O
FCN	Method
-	Method
8s	Method
[	O
reference	O
]	O
n	O
/	O
a	O
62.7	O
62.2	O
Hypercolumn	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
62.6	O
Zoomout	O
[	O
reference	O
]	O
64.4	O
64.1	O
64.4	O
Context	O
-	O
Deep	O
-	O
CNN	Method
-	O
CRF	Method
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
70.7	O
DeepLabMSc	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.6	O
Our	O
method	O
w	O
/	O
o	O
COCO	Material
73.6	O
72.4	O
72.0	O
BoxSup	O
[	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
71.0	O
DeepLab	O
[	O
reference	O
][	O
reference	O
]	O
n	O
/	O
a	O
n	O
/	O
a	O
72.7	O
Our	O
method	O
with	O
COCO	Material
75.7	O
75.0	O
74.7	O
Table	O
2	O
.	O

Mean	O
IU	Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	Method
-	Method
RNN	Method
,	O
compared	O
to	O
the	O
other	O
approaches	O
on	O
the	O
Pascal	O
VOC	Material
2010	Material
-	O
2012	O
test	O
datasets	O
.	O

Methods	O
from	O
the	O
first	O
group	O
do	O
not	O
use	O
MS	O
COCO	Material
data	O
for	O
training	O
.	O

The	O
methods	O
from	O
the	O
second	O
group	O
use	O
both	O
COCO	Material
and	O
VOC	O
datasets	O
for	O
training	O
.	O

section	O
:	O
Pascal	Material
Context	Material
Dataset	Material
We	O
conducted	O
an	O
experiment	O
on	O
the	O
Pascal	Material
Context	Material
dataset	Material
[	O
reference	O
]	O
,	O
which	O
differs	O
from	O
the	O
previous	O
one	O
in	O
the	O
larger	O
number	O
of	O
classes	O
considered	O
,	O
59	O
.	O

We	O
used	O
the	O
provided	O
partitions	O
of	O
training	O
and	O
validation	O
sets	O
,	O
and	O
the	O
obtained	O
results	O
are	O
reported	O
in	O
Table	O
3	O
.	O

Table	O
3	O
.	O

Mean	O
IU	Metric
accuracy	O
of	O
our	O
approach	O
,	O
CRF	Method
-	Method
RNN	Method
,	O
evaluated	O
on	O
the	O
Pascal	Material
Context	Material
validation	Material
set	Material
.	O

Figure	O
5	O
.	O

Visualization	O
of	O
the	O
learnt	O
label	O
compatibility	O
matrix	O
.	O

In	O
the	O
standard	O
Potts	Method
model	Method
,	O
diagonal	O
entries	O
are	O
equal	O
to	O
−1	O
,	O
while	O
off	O
-	O
diagonal	O
entries	O
are	O
zero	O
.	O

These	O
values	O
have	O
changed	O
after	O
the	O
end	O
-	O
to	O
-	O
end	O
training	O
of	O
our	O
network	O
.	O

Best	O
viewed	O
in	O
colour	O
.	O

section	O
:	O
Effect	O
of	O
Design	O
Choices	O
We	O
performed	O
a	O
number	O
of	O
additional	O
experiments	O
on	O
the	O
Pascal	Material
VOC	Material
2012	Material
validation	Material
set	Material
described	O
above	O
to	O
study	O
the	O
effect	O
of	O
some	O
design	O
choices	O
we	O
made	O
.	O

We	O
first	O
studied	O
the	O
performance	O
gains	O
attained	O
by	O
our	O
modifications	O
to	O
the	O
CRF	Method
over	O
the	O
CRF	Method
approach	O
proposed	O
by	O
[	O
reference	O
]	O
.	O

We	O
found	O
that	O
using	O
different	O
filter	O
weights	O
for	O
different	O
classes	O
improved	O
the	O
performance	O
by	O
1.8	O
percentage	O
points	O
,	O
and	O
that	O
introducing	O
the	O
asymmetric	Method
compatibility	Method
transform	Method
further	O
boosted	O
the	O
performance	O
by	O
0.9	O
percentage	O
points	O
.	O

Regarding	O
the	O
RNN	Method
parameter	O
iteration	O
count	O
T	O
,	O
incrementing	O
it	O
to	O
T	O
=	O
10	O
during	O
the	O
test	O
time	O
,	O
from	O
T	O
=	O
5	O
during	O
the	O
train	O
time	O
,	O
produced	O
an	O
accuracy	Metric
improvement	O
of	O
0.2	O
percentage	O
points	O
.	O

Setting	O
T	O
=	O
10	O
also	O
during	O
training	O
reduced	O
the	O
accuracy	Metric
by	O
0.7	O
percentage	O
points	O
.	O

We	O
believe	O
that	O
this	O
might	O
be	O
due	O
to	O
a	O
vanishing	O
gradient	O
effect	O
caused	O
by	O
using	O
too	O
many	O
iterations	O
.	O

In	O
practice	O
that	O
leads	O
to	O
the	O
first	O
part	O
of	O
the	O
network	O
(	O
the	O
one	O
producing	O
unary	O
potentials	O
)	O
receiving	O
a	O
very	O
weak	O
error	O
gradient	O
signal	O
during	O
training	O
,	O
thus	O
hampering	O
its	O
learning	Method
capacity	Method
.	O

End	Metric
-	Metric
to	Metric
-	Metric
end	Metric
training	Metric
after	O
the	O
initialization	O
of	O
CRF	Method
parameters	O
improved	O
performance	O
by	O
3.4	O
percentage	O
points	O
.	O

We	O
also	O
conducted	O
an	O
experiment	O
where	O
we	O
froze	O
the	O
FCN8s	Method
part	Method
and	O
fine	O
-	O
tuned	O
only	O
the	O
RNN	Method
part	O
(	O
i.e.	O
,	O
CRF	Method
parameters	O
)	O
.	O

It	O
improved	O
the	O
performance	O
over	O
initialization	Method
by	O
only	O
1	O
percentage	O
point	O
.	O

We	O
therefore	O
conclude	O
that	O
end	O
-	O
toend	O
training	O
significantly	O
contributed	O
to	O
boost	O
the	O
accuracy	Metric
of	O
the	O
system	O
.	O

Treating	O
each	O
iteration	O
of	O
mean	Method
-	Method
field	Method
inference	Method
as	O
an	O
independent	O
step	O
with	O
its	O
own	O
parameters	O
,	O
and	O
training	O
endto	O
-	O
end	O
with	O
5	O
such	O
iterations	O
yielded	O
a	O
final	O
mean	O
IU	Metric
score	O
of	O
only	O
70.9	O
,	O
supporting	O
the	O
hypothesis	O
that	O
the	O
recurrent	O
structure	O
of	O
our	O
approach	O
is	O
important	O
for	O
its	O
success	O
.	O

section	O
:	O
Conclusion	O
We	O
presented	O
CRF	Method
-	Method
RNN	Method
,	O
an	O
interpretation	O
of	O
dense	O
CRFs	Method
as	O
Recurrent	Method
Neural	Method
Networks	Method
.	O

Our	O
formulation	O
fully	O
integrates	O
CRF	Method
-	O
based	O
probabilistic	O
graphical	O
modelling	O
with	O
emerging	O
deep	Method
learning	Method
techniques	Method
.	O

In	O
particular	O
,	O
the	O
proposed	O
CRF	Method
-	Method
RNN	Method
can	O
be	O
plugged	O
in	O
as	O
a	O
part	O
of	O
a	O
traditional	O
deep	Method
neural	Method
network	Method
:	O
It	O
is	O
capable	O
of	O
passing	O
on	O
error	O
differentials	O
from	O
its	O
outputs	O
to	O
inputs	O
during	O
back	Method
-	Method
propagation	Method
based	Method
training	Method
of	O
the	O
deep	Method
network	Method
while	O
learning	O
CRF	Method
parameters	O
.	O

We	O
demonstrate	O
the	O
use	O
of	O
this	O
approach	O
by	O
utilizing	O
it	O
for	O
the	O
semantic	Task
segmentation	Task
task	O
:	O
we	O
form	O
an	O
end	O
-	O
to	O
-	O
end	Method
trainable	Method
deep	Method
network	Method
by	O
combining	O
a	O
fully	Method
convolutional	Method
neural	Method
network	Method
with	O
the	O
CRF	Method
-	Method
RNN	Method
.	O

Our	O
system	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
popular	O
Pascal	Material
VOC	Material
segmentation	Material
benchmark	Material
.	O

This	O
improvement	O
can	O
be	O
attributed	O
to	O
the	O
uniting	O
of	O
the	O
strengths	O
of	O
CNNs	Method
and	O
CRFs	Method
in	O
a	O
single	O
deep	Method
network	Method
.	O

In	O
the	O
future	O
,	O
we	O
plan	O
to	O
investigate	O
the	O
advantages	O
/	O
disadvantages	O
of	O
restricting	O
the	O
capabilities	O
of	O
the	O
RNN	Method
part	O
of	O
our	O
network	O
to	O
mean	O
-	O
field	O
inference	O
of	O
dense	O
CRF	Method
.	O

A	O
sensible	O
baseline	O
to	O
the	O
work	O
presented	O
here	O
would	O
be	O
to	O
use	O
more	O
standard	O
RNNs	Method
(	O
e.g.	O
LSTMs	Method
)	O
that	O
learn	O
to	O
iteratively	O
improve	O
the	O
input	O
unary	O
potentials	O
to	O
make	O
them	O
closer	O
to	O
the	O
ground	O
-	O
truth	O
.	O

section	O
:	O
section	O
:	O
Acknowledgement	O
This	O
work	O
was	O
supported	O
by	O
grants	O
Leverhulme	O
Trust	O
,	O
EPSRC	O
EP	O
/	O
I001107	O
/	O
2	O
and	O
ERC	O
321162	O
-	O
HELIOS	O
.	O

We	O
thank	O
the	O
Caffe	O
team	O
,	O
Baidu	O
IDL	O
,	O
and	O
the	O
Oxford	O
ARC	O
team	O
for	O
their	O
support	O
.	O

We	O
gratefully	O
acknowledge	O
GPU	O
donations	O
from	O
NVIDIA	O
.	O

section	O
:	O
