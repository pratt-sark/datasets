document	O
:	O
Evaluation	O
of	O
Output	Method
Embeddings	Method
for	O
Fine	Task
-	Task
Grained	Task
Image	Task
Classification	Task
Image	Task
classification	Task
has	O
advanced	O
significantly	O
in	O
recent	O
years	O
with	O
the	O
availability	O
of	O
large	O
-	O
scale	O
image	O
sets	O
.	O

However	O
,	O
fine	Task
-	Task
grained	Task
classification	Task
remains	O
a	O
major	O
challenge	O
due	O
to	O
the	O
annotation	Metric
cost	Metric
of	O
large	O
numbers	O
of	O
fine	O
-	O
grained	O
categories	O
.	O

This	O
project	O
shows	O
that	O
compelling	O
classification	Task
performance	O
can	O
be	O
achieved	O
on	O
such	O
categories	O
even	O
without	O
labeled	O
training	O
data	O
.	O

Given	O
image	O
and	O
class	O
embeddings	O
,	O
we	O
learn	O
a	O
compatibility	O
function	O
such	O
that	O
matching	O
embeddings	O
are	O
assigned	O
a	O
higher	O
score	O
than	O
mismatching	O
ones	O
;	O
zero	Task
-	Task
shot	Task
classification	Task
of	O
an	O
image	O
proceeds	O
by	O
finding	O
the	O
label	O
yielding	O
the	O
highest	O
joint	Metric
compatibility	Metric
score	Metric
.	O

We	O
use	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
image	O
features	O
and	O
focus	O
on	O
different	O
supervised	O
attributes	O
and	O
unsupervised	O
output	O
embeddings	O
either	O
derived	O
from	O
hierarchies	Method
or	O
learned	O
from	O
unlabeled	O
text	O
corpora	O
.	O

We	O
establish	O
a	O
substantially	O
improved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
Animals	O
with	O
Attributes	O
and	O
Caltech	O
-	O
UCSD	O
Birds	O
datasets	O
.	O

Most	O
encouragingly	O
,	O
we	O
demonstrate	O
that	O
purely	O
unsupervised	O
output	O
embeddings	O
(	O
learned	O
from	O
Wikipedia	O
and	O
improved	O
with	O
fine	O
-	O
grained	O
text	O
)	O
achieve	O
compelling	O
results	O
,	O
even	O
outperforming	O
the	O
previous	O
supervised	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

By	O
combining	O
different	O
output	O
embeddings	O
,	O
we	O
further	O
improve	O
results	O
.	O

section	O
:	O
Introduction	O
The	O
image	Task
classification	Task
problem	Task
has	O
been	O
redefined	O
by	O
the	O
emergence	O
of	O
large	O
scale	O
datasets	O
such	O
as	O
ImageNet	O
.	O

Since	O
deep	Method
learning	Method
methods	Method
dominated	O
recent	O
Large	Task
-	Task
Scale	Task
Visual	Task
Recognition	Task
Challenges	Task
(	O
ILSVRC12	Task
-	O
14	O
)	O
,	O
the	O
attention	O
of	O
the	O
computer	Task
vision	Task
community	Task
has	O
been	O
drawn	O
to	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNN	Method
)	O
.	O

Training	O
CNNs	Method
requires	O
massive	O
amounts	O
of	O
labeled	O
data	O
;	O
but	O
,	O
in	O
fine	O
-	O
grained	O
image	O
collections	O
,	O
where	O
the	O
categories	O
are	O
visually	O
very	O
similar	O
,	O
the	O
data	O
population	O
decreases	O
significantly	O
.	O

We	O
are	O
interested	O
in	O
the	O
most	O
extreme	O
case	O
of	O
learning	Task
with	O
a	O
limited	O
amount	O
of	O
labeled	O
data	O
,	O
zero	Task
-	Task
shot	Task
learning	Task
,	O
in	O
which	O
no	O
labeled	O
data	O
is	O
available	O
for	O
some	O
classes	O
.	O

Without	O
labels	O
,	O
we	O
need	O
alternative	O
sources	O
of	O
information	O
that	O
relate	O
object	O
classes	O
.	O

Attributes	O
,	O
which	O
describe	O
well	O
-	O
known	O
common	O
characteristics	O
of	O
objects	O
,	O
are	O
an	O
appealing	O
source	O
of	O
information	O
,	O
and	O
they	O
can	O
be	O
easily	O
obtained	O
through	O
crowd	Method
-	Method
sourcing	Method
techniques	Method
.	O

However	O
,	O
fine	O
-	O
grained	O
concepts	O
present	O
a	O
special	O
challenge	O
:	O
due	O
to	O
the	O
high	O
degree	O
of	O
similarity	O
among	O
categories	O
,	O
a	O
large	O
number	O
of	O
attributes	O
are	O
required	O
to	O
effectively	O
model	O
these	O
subtle	O
differences	O
.	O

This	O
increases	O
the	O
cost	O
of	O
attribute	Task
annotation	Task
.	O

One	O
aim	O
of	O
this	O
work	O
is	O
to	O
move	O
towards	O
eliminating	O
the	O
human	Method
labeling	Method
component	Method
from	O
zero	Method
-	Method
shot	Method
learning	Method
,	O
e.g.	O
by	O
using	O
alternative	O
sources	O
of	O
information	O
.	O

On	O
the	O
other	O
hand	O
,	O
large	Method
-	Method
margin	Method
support	Method
vector	Method
machines	Method
(	O
SVM	Method
)	Method
operate	O
with	O
labeled	O
training	O
images	O
,	O
so	O
a	O
lack	O
of	O
labels	O
limits	O
their	O
use	O
for	O
this	O
task	O
.	O

Inspired	O
by	O
previous	O
work	O
on	O
label	Task
embedding	Task
and	O
structured	Task
SVMs	Task
,	O
we	O
propose	O
to	O
use	O
a	O
Structured	Method
Joint	Method
Embedding	Method
(	O
SJE	Method
)	O
framework	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
that	O
relates	O
input	O
embeddings	O
(	O
i.e.	O
image	O
features	O
)	O
and	O
output	O
embeddings	O
(	O
i.e.	O
side	O
information	O
)	O
through	O
a	O
compatibility	O
function	O
,	O
therefore	O
taking	O
advantage	O
of	O
a	O
structure	O
in	O
the	O
output	O
space	O
.	O

The	O
SJE	Method
framework	O
separates	O
the	O
subspace	Task
learning	Task
problem	Task
from	O
the	O
specific	O
input	O
and	O
output	O
features	O
used	O
in	O
a	O
given	O
application	O
.	O

As	O
a	O
general	O
framework	O
,	O
it	O
can	O
be	O
applied	O
to	O
any	O
learning	Task
problem	Task
where	O
more	O
than	O
one	O
modality	O
is	O
provided	O
for	O
an	O
object	O
.	O

Our	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
demonstrate	O
that	O
unsupervised	Method
class	Method
embeddings	Method
trained	O
from	O
large	O
unlabeled	O
text	O
corpora	O
are	O
competitive	O
to	O
previously	O
published	O
results	O
that	O
use	O
human	O
supervision	O
.	O

(	O
2	O
)	O
Using	O
the	O
most	O
recent	O
deep	Method
architectures	Method
as	O
input	O
embeddings	O
,	O
we	O
significantly	O
improve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SoA	O
)	O
.	O

(	O
3	O
)	O
We	O
extensively	O
evaluate	O
several	O
unsupervised	Method
output	Method
embeddings	Method
for	O
fine	Task
-	Task
grained	Task
classification	Task
in	O
a	O
zero	Method
-	Method
shot	Method
setting	Method
on	O
three	O
challenging	O
datasets	O
.	O

(	O
4	O
)	O
By	O
combining	O
different	O
output	O
embeddings	O
we	O
obtain	O
best	O
results	O
,	O
surpassing	O
the	O
SoA	O
by	O
a	O
large	O
margin	O
.	O

(	O
5	O
)	O
We	O
propose	O
a	O
novel	O
weakly	Method
-	Method
supervised	Method
Word2Vec	Method
variant	Method
that	O
improves	O
the	O
accuracy	Metric
when	O
combined	O
with	O
other	O
output	O
embeddings	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
provides	O
a	O
review	O
of	O
the	O
relevant	O
literature	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
details	O
the	O
SJE	Method
method	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
explains	O
the	O
output	O
embeddings	O
that	O
we	O
analyze	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
our	O
experimental	O
evaluation	O
;	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
the	O
discussion	O
and	O
our	O
conclusions	O
.	O

section	O
:	O
Related	O
Work	O
Learning	Task
to	O
classify	Task
in	O
the	O
absence	O
of	O
labeled	O
data	O
(	O
zero	Task
-	Task
shot	Task
learning	Task
)	O
is	O
a	O
challenging	O
problem	O
,	O
and	O
achieving	O
better	O
-	O
than	O
-	O
chance	O
performance	O
requires	O
structure	O
in	O
the	O
output	O
space	O
.	O

Attributes	O
provide	O
one	O
such	O
space	O
;	O
they	O
relate	O
different	O
classes	O
through	O
well	O
-	O
known	O
and	O
shared	O
characteristics	O
of	O
objects	O
.	O

Attributes	O
,	O
which	O
are	O
often	O
collected	O
manually	O
,	O
have	O
shown	O
promising	O
results	O
in	O
various	O
applications	O
,	O
i.e.	O
caption	Task
generation	Task
,	O
face	Task
recognition	Task
,	O
image	Task
retrieval	Task
,	O
action	Task
recognition	Task
and	O
image	Task
classification	Task
.	O

The	O
main	O
challenge	O
of	O
attribute	Method
-	Method
based	Method
zero	Method
-	Method
shot	Method
learning	Method
arises	O
on	O
more	O
challenging	O
fine	O
-	O
grained	O
data	O
collections	O
,	O
in	O
which	O
categories	O
may	O
visually	O
differ	O
only	O
subtly	O
.	O

Therefore	O
,	O
generic	O
attributes	O
fail	O
at	O
modeling	O
small	O
intra	O
-	O
class	O
variance	O
between	O
objects	O
.	O

Improved	O
performance	O
requires	O
a	O
large	O
number	O
of	O
specific	O
attributes	O
which	O
increases	O
the	O
cost	O
of	O
data	Task
gathering	Task
.	O

As	O
an	O
alternative	O
to	O
manual	Method
annotation	Method
,	O
side	O
information	O
can	O
be	O
collected	O
automatically	O
from	O
text	O
corpora	O
.	O

Bag	Task
-	Task
of	Task
-	Task
words	Task
is	O
an	O
example	O
where	O
class	O
embeddings	O
correspond	O
to	O
histograms	O
of	O
vocabulary	O
words	O
extracted	O
automatically	O
from	O
unlabeled	O
text	O
.	O

Another	O
example	O
is	O
using	O
taxonomical	O
order	O
of	O
classes	O
as	O
structured	O
output	O
embeddings	O
.	O

Such	O
a	O
taxonomy	O
can	O
be	O
built	O
automatically	O
from	O
a	O
pre	O
-	O
defined	O
ontology	Method
such	O
as	O
WordNet	O
.	O

In	O
this	O
case	O
,	O
the	O
distance	O
between	O
nodes	O
is	O
measured	O
using	O
semantic	Metric
similarity	Metric
metrics	Metric
.	O

Finally	O
,	O
distributed	Method
text	Method
representations	Method
learned	O
from	O
large	O
unsupervised	O
text	O
corpora	O
can	O
be	O
employed	O
as	O
structured	O
embeddings	O
.	O

We	O
compare	O
several	O
representatives	O
of	O
these	O
methods	O
(	O
and	O
their	O
combinations	O
)	O
in	O
our	O
evaluation	O
.	O

Embedding	O
labels	O
in	O
an	O
Euclidean	O
space	O
is	O
an	O
effective	O
tool	O
to	O
model	O
latent	O
relationships	O
between	O
classes	O
.	O

These	O
relationships	O
can	O
be	O
collected	O
separately	O
from	O
the	O
data	O
,	O
learned	O
from	O
the	O
data	O
or	O
derived	O
from	O
side	O
information	O
.	O

In	O
order	O
to	O
collect	O
relationships	O
independently	O
of	O
data	O
,	O
compressed	Method
sensing	Method
uses	O
random	Method
projections	Method
whereas	O
Error	Method
Correcting	Method
Output	Method
Codes	Method
builds	O
embeddings	O
inspired	O
from	O
information	Method
theory	Method
.	O

WSABIE	Method
uses	O
images	O
with	O
their	O
corresponding	O
labels	O
to	O
learn	O
an	O
embedding	O
of	O
the	O
labels	O
,	O
and	O
CCA	Method
maximizes	O
the	O
correlation	Metric
between	O
two	O
different	O
data	O
modalities	O
.	O

DeViSE	O
employs	O
a	O
ranking	Method
formulation	Method
for	O
zero	Task
-	Task
shot	Task
learning	Task
using	O
images	O
and	O
distributed	Method
text	Method
representations	Method
.	O

The	O
ALE	Method
method	Method
employs	O
an	O
approximate	Method
ranking	Method
formulation	Method
for	O
the	O
same	O
using	O
images	O
and	O
attributes	O
.	O

ConSe	Method
uses	O
the	O
probabilities	O
of	O
a	O
softmax	Method
-	Method
output	Method
layer	Method
to	O
weigh	O
the	O
semantic	O
vectors	O
of	O
all	O
the	O
classes	O
.	O

In	O
this	O
work	O
,	O
we	O
use	O
the	O
multiclass	O
objective	O
to	O
learn	O
structured	O
output	O
embeddings	O
obtained	O
from	O
various	O
sources	O
.	O

Among	O
the	O
closest	O
related	O
work	O
,	O
ALE	Method
uses	O
Fisher	O
Vectors	O
(	O
FV	Method
)	O
as	O
input	O
and	O
binary	O
attributes	O
/	O
hierarchies	O
as	O
output	O
embeddings	O
.	O

Similarly	O
,	O
DeviSe	O
uses	O
CNN	O
features	O
as	O
input	O
and	O
Word2Vec	Method
representations	Method
as	O
output	O
embeddings	O
.	O

In	O
this	O
work	O
,	O
we	O
benefit	O
from	O
both	O
ideas	O
:	O
(	O
1	O
)	O
We	O
use	O
SoA	O
image	O
features	O
,	O
i.e.	O
FV	Method
and	O
CNN	Method
,	O
(	O
2	O
)	O
among	O
others	O
,	O
we	O
also	O
use	O
attributes	O
and	O
Word2Vec	O
as	O
output	O
embeddings	O
.	O

Our	O
work	O
differs	O
from	O
w.r.t	Method
.	O

two	O
aspects	O
:	O
(	O
1	O
)	O
We	O
propose	O
and	O
evaluate	O
several	O
output	Method
embedding	Method
methods	Method
specifically	O
built	O
for	O
fine	Task
-	Task
grained	Task
classification	Task
.	O

(	O
2	O
)	O
We	O
show	O
how	O
some	O
of	O
these	O
output	O
embeddings	O
complement	O
each	O
other	O
for	O
zero	Task
-	Task
shot	Task
learning	Task
on	O
general	O
and	O
fine	O
-	O
grained	O
datasets	O
.	O

The	O
reader	O
should	O
be	O
aware	O
of	O
.	O

section	O
:	O
Structured	Method
Joint	Method
Embedding	Method
s	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
leverage	O
input	O
and	O
output	O
embeddings	O
in	O
a	O
joint	Method
framework	Method
by	O
learning	O
a	O
compatibility	O
between	O
these	O
embeddings	O
.	O

We	O
are	O
interested	O
in	O
the	O
problem	O
of	O
zero	Task
-	Task
shot	Task
learning	Task
for	O
image	Task
classification	Task
where	O
training	O
and	O
test	O
images	O
belong	O
to	O
two	O
disjoint	O
sets	O
of	O
classes	O
.	O

Following	O
,	O
given	O
input	O
/	O
output	O
and	O
from	O
,	O
Structured	Method
Joint	Method
Embedding	Method
(	O
SJE	Method
)	O
learns	O
by	O
minimizing	O
the	O
empirical	Metric
risk	Metric
where	O
defines	O
the	O
cost	O
of	O
predicting	O
when	O
the	O
true	O
label	O
is	O
.	O

Here	O
,	O
we	O
use	O
the	O
loss	O
.	O

subsection	O
:	O
Model	O
We	O
define	O
a	O
compatibility	O
function	O
between	O
an	O
input	O
space	O
and	O
a	O
structured	O
output	O
space	O
.	O

Given	O
a	O
specific	O
input	O
embedding	O
,	O
we	O
derive	O
a	O
prediction	Task
by	O
maximizing	O
the	O
compatibility	O
over	O
SJE	Method
as	O
follows	O
:	O
The	O
parameter	O
vector	O
can	O
be	O
written	O
as	O
a	O
matrix	O
with	O
being	O
the	O
input	O
embedding	O
dimension	O
and	O
being	O
the	O
output	O
embedding	O
dimension	O
.	O

This	O
leads	O
to	O
the	O
bi	O
-	O
linear	O
form	O
of	O
the	O
compatibility	O
function	O
:	O
Here	O
,	O
the	O
input	O
embedding	O
is	O
denoted	O
by	O
and	O
the	O
output	O
embedding	O
by	O
.	O

The	O
matrix	O
is	O
learned	O
by	O
enforcing	O
the	O
correct	O
label	O
to	O
be	O
ranked	O
higher	O
than	O
any	O
of	O
the	O
other	O
labels	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
i.e.	O
multiclass	O
objective	O
.	O

This	O
formulation	O
is	O
closely	O
related	O
to	O
.	O

Within	O
the	O
label	Method
embedding	Method
framework	Method
,	O
ALE	Method
and	O
DeViSe	O
use	O
pairwise	Metric
ranking	Metric
objective	Metric
,	O
WSABIE	Method
learns	O
both	O
and	O
through	O
ranking	Task
,	O
whereas	O
we	O
use	O
multiclass	O
objective	O
.	O

Similarly	O
,	O
use	O
the	O
regression	O
objective	O
and	O
CCA	Method
maximizes	O
the	O
correlation	O
of	O
input	O
and	O
output	O
embeddings	O
.	O

subsection	O
:	O
Parameter	Method
Learning	Method
According	O
to	O
the	O
unregularized	Method
structured	Method
SVM	Method
formulation	Method
,	O
the	O
objective	O
is	O
:	O
where	O
the	O
misclassification	Metric
loss	Metric
takes	O
the	O
form	O
:	O
For	O
the	O
zero	Task
-	Task
shot	Task
learning	Task
scenario	Task
,	O
the	O
training	O
and	O
test	O
classes	O
are	O
disjoint	O
.	O

Therefore	O
,	O
we	O
fix	O
to	O
the	O
output	O
embeddings	O
of	O
training	O
classes	O
and	O
learn	O
.	O

For	O
prediction	Task
,	O
we	O
project	O
a	O
test	O
image	O
onto	O
the	O
and	O
search	O
for	O
the	O
nearest	O
output	O
embedding	O
vector	O
(	O
using	O
the	O
dot	O
product	O
similarity	O
)	O
that	O
corresponds	O
to	O
one	O
of	O
the	O
test	O
classes	O
.	O

We	O
use	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	Method
for	O
optimization	Task
which	O
consists	O
in	O
sampling	O
at	O
each	O
step	O
and	O
searching	O
for	O
the	O
highest	O
ranked	O
class	O
.	O

If	O
,	O
we	O
update	O
as	O
follows	O
:	O
where	O
is	O
the	O
learning	O
step	O
-	O
size	O
used	O
at	O
iteration	O
.	O

We	O
use	O
a	O
constant	O
step	O
size	O
chosen	O
by	O
cross	Method
-	Method
validation	Method
and	O
we	O
perform	O
regularization	Method
through	O
early	Method
stopping	Method
.	O

subsection	O
:	O
Learning	O
Combined	Task
Embeddings	Task
For	O
some	O
classification	Task
tasks	Task
,	O
there	O
may	O
be	O
multiple	O
output	O
embeddings	O
available	O
,	O
each	O
capturing	O
a	O
different	O
aspect	O
of	O
the	O
structure	O
of	O
the	O
output	O
space	O
.	O

Each	O
may	O
also	O
have	O
a	O
different	O
signal	Metric
-	Metric
to	Metric
-	Metric
noise	Metric
ratio	Metric
.	O

Since	O
each	O
output	Method
embedding	Method
possibly	O
offers	O
non	O
-	O
redundant	O
information	O
about	O
the	O
output	O
space	O
,	O
as	O
also	O
shown	O
in	O
,	O
we	O
can	O
learn	O
a	O
better	O
joint	Method
embedding	Method
by	O
combining	O
them	O
together	O
.	O

We	O
model	O
the	O
resulting	O
compatibility	Metric
score	Metric
as	O
where	O
are	O
the	O
joint	O
embedding	O
weight	O
matrices	O
corresponding	O
to	O
the	O
output	O
embeddings	O
(	O
)	O
.	O

In	O
our	O
experiments	O
,	O
we	O
first	O
train	O
each	O
independently	O
,	O
then	O
perform	O
a	O
grid	Method
search	Method
over	O
on	O
a	O
validation	O
set	O
.	O

Interestingly	O
,	O
we	O
found	O
that	O
the	O
optimal	O
for	O
previously	O
-	O
seen	O
classes	O
is	O
often	O
different	O
from	O
the	O
one	O
for	O
unseen	O
classes	O
.	O

Therefore	O
,	O
it	O
is	O
critical	O
to	O
cross	O
-	O
validate	O
on	O
the	O
zero	Method
-	Method
shot	Method
setting	Method
.	O

Note	O
that	O
if	O
we	O
take	O
,	O
Equation	O
[	O
reference	O
]	O
is	O
equivalent	O
to	O
simply	O
concatenating	O
the	O
.	O

This	O
corresponds	O
to	O
stacking	O
the	O
into	O
a	O
single	O
matrix	O
and	O
computing	O
the	O
standard	O
compatibility	O
as	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

However	O
,	O
such	O
a	O
stacking	Method
learns	O
a	O
large	O
where	O
a	O
high	O
dimensional	O
biases	O
the	O
final	O
prediction	O
.	O

In	O
contrast	O
,	O
eliminates	O
the	O
bias	O
,	O
leading	O
to	O
better	O
predictions	O
.	O

Thus	O
,	O
can	O
be	O
thought	O
of	O
as	O
the	O
confidence	O
associated	O
with	O
whose	O
contribution	O
we	O
can	O
control	O
.	O

We	O
show	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
that	O
finding	O
an	O
appropriate	O
can	O
yield	O
improved	O
accuracy	Metric
compared	O
to	O
any	O
single	O
.	O

section	O
:	O
Output	Method
Embeddings	Method
In	O
this	O
section	O
,	O
we	O
describe	O
three	O
types	O
of	O
output	O
embeddings	O
:	O
human	O
-	O
annotated	O
attributes	O
,	O
unsupervised	O
word	O
embeddings	O
learned	O
from	O
large	O
text	O
corpora	O
,	O
and	O
hierarchical	Method
embeddings	Method
derived	O
from	O
WordNet	O
.	O

subsection	O
:	O
Embedding	Task
by	O
Human	Task
Annotation	Task
:	O
Attributes	O
Annotating	Task
images	Task
with	O
class	O
labels	O
is	O
a	O
laborious	O
process	O
when	O
the	O
objects	O
represent	O
fine	O
-	O
grained	O
concepts	O
that	O
are	O
not	O
common	O
in	O
our	O
daily	O
lives	O
.	O

Attributes	O
provide	O
a	O
means	O
to	O
describe	O
such	O
fine	O
-	O
grained	O
concepts	O
.	O

They	O
model	O
shared	O
characteristics	O
of	O
objects	O
such	O
as	O
color	O
and	O
texture	O
which	O
are	O
easily	O
annotated	O
by	O
humans	O
and	O
converted	O
to	O
machine	O
-	O
readable	O
vector	O
format	O
.	O

The	O
set	O
of	O
descriptive	O
attributes	O
may	O
be	O
determined	O
by	O
language	O
experts	O
or	O
by	O
fine	O
-	O
grained	O
object	O
experts	O
.	O

The	O
association	O
between	O
an	O
attribute	O
and	O
a	O
category	O
can	O
be	O
a	O
binary	O
value	O
depicting	O
the	O
presence	O
/	O
absence	O
of	O
an	O
attribute	O
(	O
)	O
or	O
a	O
continuous	O
value	O
that	O
defines	O
the	O
confidence	O
level	O
of	O
an	O
attribute	O
(	O
)	O
for	O
each	O
class	O
.	O

We	O
write	O
per	O
-	O
class	O
attributes	O
as	O
:	O
where	O
can	O
be	O
or	O
a	O
real	O
number	O
that	O
associates	O
a	O
class	O
with	O
an	O
attribute	O
,	O
denotes	O
the	O
associated	O
class	O
and	O
is	O
the	O
number	O
of	O
attributes	O
.	O

Potentially	O
,	O
encodes	O
more	O
information	O
than	O
.	O

For	O
instance	O
,	O
for	O
classes	O
rat	O
,	O
monkey	O
,	O
whale	O
and	O
the	O
attribute	O
big	O
,	O
implies	O
that	O
in	O
terms	O
of	O
size	O
rat	O
monkey	O
whale	O
,	O
whereas	O
can	O
be	O
interpreted	O
as	O
rat	O
monkey	O
whale	O
which	O
is	O
more	O
accurate	O
.	O

We	O
empirically	O
show	O
the	O
benefit	O
of	O
over	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

In	O
practice	O
,	O
our	O
output	Method
embeddings	Method
use	O
a	O
per	O
-	O
class	O
vector	O
form	O
,	O
but	O
they	O
can	O
vary	O
in	O
dimensionality	O
(	O
)	O
.	O

For	O
the	O
rest	O
of	O
the	O
section	O
we	O
denote	O
the	O
output	O
embeddings	O
as	O
for	O
brevity	O
.	O

subsection	O
:	O
Learning	Task
Label	Task
Embeddings	Task
from	O
Text	O
In	O
this	O
section	O
,	O
we	O
describe	O
unsupervised	Task
and	Task
weakly	Task
-	Task
supervised	Task
label	Task
embeddings	Task
mined	O
from	O
text	O
.	O

With	O
these	O
label	Method
embeddings	Method
,	O
we	O
can	O
(	O
1	O
)	O
avoid	O
dependence	O
on	O
costly	O
manual	O
annotation	O
of	O
attributes	O
and	O
(	O
2	O
)	O
combine	O
the	O
embeddings	O
with	O
attributes	O
,	O
where	O
available	O
,	O
to	O
achieve	O
better	O
performance	O
.	O

Word2Vec	Method
(	O
φW	O
)	O
.	O

In	O
Word2Vec	Method
,	O
a	O
two	Method
-	Method
layer	Method
neural	Method
network	Method
is	O
trained	O
to	O
predict	O
a	O
set	O
of	O
target	O
words	O
from	O
a	O
set	O
of	O
context	O
words	O
.	O

Words	O
in	O
the	O
vocabulary	O
are	O
assigned	O
with	O
one	Method
-	Method
shot	Method
encoding	Method
so	O
that	O
the	O
first	O
layer	O
acts	O
as	O
a	O
look	O
-	O
up	O
table	O
to	O
retrieve	O
the	O
embedding	O
for	O
any	O
word	O
in	O
the	O
vocabulary	O
.	O

The	O
second	O
layer	O
predicts	O
the	O
target	O
word	O
(	O
s	O
)	O
via	O
hierarchical	Method
soft	Method
-	Method
max	Method
.	O

Word2Vec	Method
has	O
two	O
main	O
formulations	O
for	O
the	O
target	Task
prediction	Task
:	O
skip	Method
-	Method
gram	Method
(	Method
SG	Method
)	O
and	O
continuous	Method
bag	Method
-	Method
of	Method
-	Method
words	Method
(	O
CBOW	Method
)	O
.	O

In	O
SG	Method
,	O
words	O
within	O
a	O
local	O
context	O
window	O
are	O
predicted	O
from	O
the	O
centering	O
word	O
.	O

In	O
CBOW	Method
,	O
the	O
center	O
word	O
of	O
a	O
context	O
window	O
is	O
predicted	O
from	O
the	O
surrounding	O
words	O
.	O

Embeddings	Method
are	O
obtained	O
by	O
back	O
-	O
propagating	O
the	O
prediction	O
error	O
gradient	O
over	O
a	O
training	O
set	O
of	O
context	O
windows	O
sampled	O
from	O
the	O
text	O
corpus	O
.	O

GloVe	O
(	O
φG	O
)	O
.	O

GloVe	Method
incorporates	O
co	O
-	O
occurrence	O
statistics	O
of	O
words	O
that	O
frequently	O
appear	O
together	O
within	O
the	O
document	O
.	O

Intuitively	O
,	O
the	O
co	O
-	O
occurrence	O
statistics	O
encode	O
meaning	O
since	O
semantically	O
similar	O
words	O
such	O
as	O
“	O
ice	O
”	O
and	O
“	O
water	O
”	O
occur	O
together	O
more	O
frequently	O
than	O
semantically	O
dissimilar	O
words	O
such	O
as	O
“	O
ice	O
”	O
and	O
“	O
fashion	O
.	O

”	O
The	O
training	O
objective	O
is	O
to	O
learn	O
word	O
vectors	O
such	O
that	O
their	O
dot	O
product	O
equals	O
the	O
co	O
-	O
occurrence	O
probability	O
of	O
these	O
two	O
words	O
.	O

This	O
approach	O
has	O
recently	O
been	O
shown	O
to	O
outperform	O
Word2Vec	Method
on	O
the	O
word	Task
analogy	Task
prediction	Task
task	Task
.	O

Weakly	Method
-	Method
supervised	Method
Word2Vec	Method
(	Method
φW⁢ws	Method
)	O
.	O

The	O
standard	O
Word2Vec	Method
scans	O
the	O
entire	O
document	O
using	O
each	O
word	O
within	O
a	O
sample	O
window	O
as	O
the	O
target	O
for	O
prediction	Task
.	O

However	O
,	O
if	O
we	O
know	O
the	O
global	O
context	O
,	O
i.e.	O
the	O
topic	O
of	O
the	O
document	O
,	O
we	O
can	O
use	O
that	O
topic	O
as	O
our	O
target	O
.	O

For	O
instance	O
,	O
in	O
Wikipedia	O
,	O
the	O
entire	O
article	O
is	O
related	O
to	O
the	O
same	O
topic	O
.	O

Therefore	O
,	O
we	O
can	O
sample	O
our	O
context	O
windows	O
from	O
any	O
location	O
within	O
the	O
article	O
rather	O
than	O
searching	O
for	O
context	O
windows	O
where	O
the	O
topic	O
explicitly	O
appears	O
in	O
the	O
text	O
.	O

We	O
consider	O
this	O
method	O
as	O
a	O
weak	Method
form	Method
of	Method
supervision	Method
.	O

We	O
achieve	O
the	O
best	O
results	O
in	O
our	O
experiments	O
using	O
our	O
novel	O
variant	O
of	O
the	O
CBOW	Method
formulation	Method
.	O

Here	O
,	O
we	O
pre	O
-	O
train	O
the	O
first	O
layer	O
weights	O
using	O
standard	O
Word2Vec	Method
on	O
Wikipedia	O
,	O
and	O
fine	O
-	O
tune	O
the	O
second	Method
layer	Method
weights	Method
using	O
a	O
negative	Method
-	Method
sampling	Method
objective	Method
only	O
on	O
the	O
fine	O
-	O
grained	O
text	O
corpus	O
.	O

These	O
weights	O
correspond	O
to	O
the	O
final	O
output	O
embedding	O
.	O

The	O
negative	Task
sampling	Task
objective	Task
is	O
formulated	O
as	O
follows	O
:	O
where	O
and	O
are	O
the	O
label	O
embeddings	O
we	O
seek	O
to	O
learn	O
,	O
and	O
is	O
the	O
average	O
of	O
word	O
embeddings	O
within	O
a	O
context	O
window	O
around	O
word	O
.	O

consists	O
of	O
context	O
and	O
matching	O
targets	O
,	O
and	O
consists	O
of	O
the	O
same	O
and	O
mismatching	O
.	O

To	O
find	O
the	O
(	O
which	O
are	O
the	O
columns	O
of	O
the	O
first	O
-	O
layer	O
network	O
weights	O
)	O
,	O
we	O
take	O
them	O
from	O
a	O
standard	O
unsupervised	Method
Word2Vec	Method
model	Method
trained	O
on	O
Wikipedia	O
.	O

During	O
SGD	Method
,	O
the	O
are	O
fixed	O
and	O
we	O
update	O
each	O
sampled	O
and	O
at	O
each	O
iteration	O
.	O

Intuitively	O
,	O
we	O
seek	O
to	O
maximize	O
the	O
similarity	O
between	O
context	O
and	O
target	O
vectors	O
for	O
matching	O
pairs	O
,	O
and	O
minimize	O
it	O
for	O
mismatching	O
pairs	O
.	O

Bag	Method
-	Method
of	Method
-	Method
Words	Method
(	O
φB	O
)	O
.	O

BoW	Method
builds	O
a	O
“	O
bag	O
”	O
of	O
word	O
frequencies	O
by	O
counting	O
the	O
occurrence	O
of	O
each	O
vocabulary	O
word	O
that	O
appears	O
within	O
a	O
document	O
.	O

It	O
does	O
not	O
preserve	O
the	O
order	O
in	O
which	O
words	O
appear	O
in	O
a	O
document	O
,	O
so	O
it	O
disregards	O
the	O
grammar	O
.	O

We	O
collect	O
Wikipedia	O
articles	O
that	O
correspond	O
to	O
each	O
object	O
class	O
and	O
build	O
a	O
vocabulary	O
of	O
most	O
frequently	O
occurring	O
words	O
.	O

We	O
then	O
build	O
histograms	O
of	O
these	O
words	O
to	O
vectorize	O
our	O
classes	O
.	O

subsection	O
:	O
Hierarchical	Method
Embeddings	Method
Semantic	O
similarity	Method
measures	Method
how	O
closely	O
related	O
two	O
word	O
senses	O
are	O
according	O
to	O
their	O
meaning	O
.	O

Such	O
a	O
similarity	O
can	O
be	O
estimated	O
by	O
measuring	O
the	O
distance	O
between	O
terms	O
in	O
an	O
ontology	O
.	O

WordNet	Method
,	O
a	O
large	O
-	O
scale	O
hierarchical	O
database	O
of	O
over	O
100	O
,	O
000	O
words	O
for	O
English	O
,	O
provides	O
us	O
a	O
means	O
of	O
building	O
our	O
class	O
hierarchy	O
.	O

To	O
measure	O
similarity	Metric
,	O
we	O
use	O
Jiang	O
-	O
Conrath	O
(	O
)	O
,	O
Lin	O
(	O
)	O
and	O
path	O
(	O
)	O
similarities	O
formulated	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
denote	O
our	O
whole	O
family	O
of	O
hierarchical	O
embeddings	O
as	O
.	O

For	O
a	O
more	O
detailed	O
survey	O
,	O
the	O
reader	O
may	O
refer	O
to	O
.	O

section	O
:	O
Experiments	O
While	O
our	O
main	O
contribution	O
is	O
a	O
detailed	O
analysis	O
of	O
output	Task
embeddings	Task
,	O
good	O
image	Method
representations	Method
are	O
crucial	O
to	O
obtain	O
good	O
classification	Task
performance	O
.	O

In	O
Sec	O
.	O

[	O
reference	O
]	O
we	O
detail	O
datasets	O
,	O
input	O
and	O
output	O
embeddings	O
used	O
in	O
our	O
experiments	O
and	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
we	O
present	O
our	O
results	O
.	O

subsection	O
:	O
Experimental	O
Setting	O
We	O
evaluate	O
SJE	Method
on	O
three	O
datasets	O
:	O
Caltech	Material
UCSD	Material
Birds	Material
(	O
CUB	Material
)	O
and	O
Stanford	O
Dogs	O
(	O
Dogs	O
)	O
are	O
fine	O
-	O
grained	O
,	O
and	O
Animals	O
With	O
Attributes	O
(	O
AWA	O
)	O
is	O
a	O
standard	O
attribute	O
dataset	O
for	O
zero	Task
-	Task
shot	Task
classification	Task
.	O

CUB	Material
contains	O
11	O
,	O
788	O
images	O
of	O
200	O
bird	O
species	O
,	O
Dogs	O
contains	O
19	O
,	O
501	O
images	O
of	O
113	O
dog	O
breeds	O
and	O
AWA	O
contains	O
30	O
,	O
475	O
images	O
of	O
50	O
different	O
animals	O
.	O

We	O
use	O
a	O
truly	O
zero	Method
-	Method
shot	Method
setting	Method
where	O
the	O
train	O
,	O
val	O
,	O
and	O
test	O
sets	O
belong	O
to	O
mutually	O
exclusive	O
classes	O
.	O

We	O
employ	O
train	O
and	O
val	O
,	O
i.e.	O
disjoint	O
subsets	O
of	O
training	O
set	O
,	O
for	O
cross	Metric
-	Metric
validation	Metric
.	O

We	O
report	O
average	O
per	O
-	O
class	O
top	O
-	O
1	O
accuracy	Metric
on	O
the	O
test	O
set	O
.	O

For	O
CUB	Material
,	O
we	O
use	O
the	O
same	O
zero	O
-	O
shot	O
split	O
as	O
with	O
150	O
classes	O
for	O
the	O
train	O
+	O
val	O
set	O
and	O
50	O
disjoint	O
classes	O
for	O
the	O
test	O
set	O
.	O

AWA	O
has	O
a	O
predefined	O
split	O
for	O
40	O
train	O
+	O
val	O
and	O
10	O
test	O
classes	O
.	O

For	O
Dogs	O
,	O
we	O
use	O
approximately	O
the	O
same	O
ratio	O
of	O
classes	O
for	O
train	O
+	O
val	O
/	O
test	O
as	O
CUB	Material
,	O
i.e.	O
85	O
classes	O
for	O
train	O
+	O
val	O
and	O
28	O
classes	O
for	O
test	O
.	O

This	O
is	O
the	O
first	O
attempt	O
to	O
perform	O
zero	Method
-	Method
shot	Method
learning	Method
on	O
the	O
Dogs	O
dataset	O
.	O

Input	O
Embeddings	O
.	O

We	O
use	O
Fisher	O
Vectors	O
(	O
FV	Method
)	O
and	O
Deep	Method
CNN	Method
Features	Method
(	O
CNN	Method
)	O
.	O

FV	Method
aggregates	O
per	O
image	O
statistics	O
computed	O
from	O
local	O
image	O
patches	O
into	O
a	O
fixed	Method
-	Method
length	Method
local	Method
image	Method
descriptor	Method
.	O

We	O
extract	O
128	O
-	O
dim	O
SIFT	O
from	O
regular	O
grids	O
at	O
multiple	O
scales	O
,	O
reduce	O
them	O
to	O
64	O
-	O
dim	O
using	O
PCA	Method
,	O
build	O
a	O
visual	O
vocabulary	O
with	O
256	O
Gaussians	Method
and	O
finally	O
reduce	O
the	O
FVs	Method
to	O
4	O
,	O
096	O
.	O

As	O
an	O
alternative	O
,	O
we	O
extract	O
features	O
from	O
a	O
deep	Method
convolutional	Method
network	Method
.	O

Features	O
that	O
are	O
typically	O
obtained	O
from	O
the	O
activations	O
of	O
the	O
fully	Method
connected	Method
layers	Method
have	O
been	O
shown	O
to	O
induce	O
semantic	O
similarities	O
.	O

We	O
resize	O
each	O
image	O
to	O
224	O
224	O
and	O
feed	O
into	O
the	O
network	O
which	O
was	O
pre	O
-	O
trained	O
following	O
the	O
model	Method
architecture	Method
of	O
either	O
AlexNet	Method
or	O
GoogLeNet	Method
.	O

For	O
AlexNet	O
(	O
denoted	O
as	O
CNN	Method
)	O
we	O
use	O
the	O
4	O
,	O
096	O
-	O
dim	O
top	O
-	O
layer	O
hidden	O
unit	O
activations	O
(	O
âfc7â	O
)	O
as	O
features	O
,	O
and	O
for	O
GoogLeNet	O
(	O
denoted	O
as	O
GOOG	O
)	O
we	O
use	O
the	O
1	Method
,	Method
024	Method
-	Method
dim	Method
top	Method
-	Method
layer	Method
pooling	Method
units	Method
.	O

For	O
both	O
networks	O
,	O
we	O
used	O
the	O
publicly	O
-	O
available	O
BVLC	Method
implementations	Method
.	O

We	O
do	O
not	O
perform	O
any	O
task	Method
-	Method
specific	Method
pre	Method
-	Method
processing	Method
,	O
such	O
as	O
cropping	O
foreground	O
objects	O
or	O
detecting	O
parts	O
.	O

Output	Method
Embeddings	Method
.	O

AWA	O
classes	O
have	O
85	O
binary	O
and	O
continuous	O
attributes	O
.	O

CUB	Material
classes	O
have	O
312	O
continuous	O
attributes	O
and	O
the	O
continuous	O
values	O
are	O
thresholded	O
around	O
the	O
mean	O
to	O
obtain	O
binary	O
attributes	O
.	O

The	O
Dogs	O
dataset	O
does	O
not	O
have	O
human	O
-	O
annotated	O
attributes	O
available	O
.	O

We	O
train	O
Word2Vec	Method
(	Method
)	Method
and	O
GloVe	Method
(	Method
)	O
on	O
the	O
English	O
-	O
language	O
Wikipedia	O
from	O
13.02.2014	O
.	O

We	O
first	O
pre	O
-	O
process	O
it	O
by	O
replacing	O
the	O
class	O
-	O
names	O
,	O
i.e.	O
black	O
-	O
footed	O
albatross	O
,	O
with	O
alternative	O
unique	O
names	O
,	O
i.e.	O
scientific	O
name	O
,	O
phoebastrianigripes	O
.	O

We	O
cross	O
-	O
validate	O
the	O
skip	O
-	O
window	O
size	O
and	O
embedding	O
dimensions	O
.	O

For	O
our	O
proposed	O
weakly	Method
-	Method
supervised	Method
Word2Vec	Method
(	Method
)	Method
,	O
we	O
use	O
the	O
same	O
embedding	O
dimensions	O
as	O
the	O
plain	O
Word2Vec	O
(	O
)	O
.	O

For	O
BoW	Method
,	O
we	O
download	O
the	O
Wikipedia	O
articles	O
that	O
correspond	O
to	O
each	O
class	O
and	O
build	O
the	O
vocabulary	O
by	O
omitting	O
least	O
-	O
and	O
most	O
-	O
frequently	O
occurring	O
words	O
.	O

We	O
cross	O
-	O
validate	O
the	O
vocabulary	O
size	O
.	O

is	O
a	O
histogram	O
of	O
the	O
vocabulary	O
words	O
as	O
they	O
appear	O
in	O
the	O
respective	O
document	O
.	O

For	O
hierarchical	Task
embeddings	Task
(	O
)	O
,	O
we	O
use	O
the	O
WordNet	O
hierarchy	O
spanning	O
our	O
classes	O
and	O
their	O
ancestors	O
up	O
to	O
the	O
root	O
of	O
the	O
tree	O
.	O

We	O
employ	O
the	O
widely	O
used	O
NLTK	Method
library	Method
for	O
building	O
the	O
hierarchy	O
and	O
measuring	O
the	O
similarity	O
between	O
nodes	O
.	O

Therefore	O
,	O
each	O
vector	O
is	O
populated	O
with	O
similarity	Method
measures	Method
of	O
the	O
class	O
to	O
all	O
other	O
classes	O
.	O

Combination	Method
of	Method
output	Method
embeddings	Method
.	O

We	O
explore	O
combinations	O
of	O
five	O
types	O
of	O
output	O
embeddings	O
:	O
supervised	O
attributes	O
,	O
unsupervised	O
Word2Vec	O
,	O
GloVe	Method
,	O
BoW	Method
and	O
WordNet	Method
-	Method
derived	Method
similarity	Method
embeddings	Method
.	O

We	O
either	O
concatenate	O
(	O
cnc	Method
)	O
or	O
combine	O
(	O
cmb	Method
)	O
different	O
embeddings	Method
.	O

In	O
cnc	Method
,	O
for	O
instance	O
in	O
AWA	O
,	O
85	O
-	O
dim	O
and	O
400	O
-	O
dim	O
would	O
be	O
merged	O
to	O
485	O
-	O
dim	O
output	O
embeddings	O
.	O

In	O
this	O
case	O
,	O
if	O
we	O
use	O
1	O
,	O
024	O
-	O
dim	O
GOOG	O
as	O
input	O
embeddings	O
,	O
we	O
learn	O
a	O
single	O
1	O
,	O
024	O
485	O
-	O
dim	O
.	O

In	O
cmb	Method
,	O
we	O
first	O
learn	O
1	O
,	O
024	O
85	O
-	O
dim	O
and	O
1	O
,	O
024	O
400	O
-	O
dim	O
and	O
then	O
cross	O
-	O
validate	O
the	O
coefficients	O
to	O
determine	O
the	O
amount	O
each	O
embedding	O
contributes	O
to	O
the	O
final	O
score	O
.	O

subsection	O
:	O
Experimental	O
Results	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
several	O
output	Method
embeddings	Method
on	O
the	O
CUB	Material
,	O
AWA	O
and	O
Dogs	O
datasets	O
.	O

Discrete	O
vs	O
Continuous	O
Attributes	O
.	O

Attribute	Method
representations	Method
are	O
defined	O
as	O
a	O
vector	O
per	O
class	O
,	O
or	O
a	O
column	O
of	O
the	O
(	O
class	O
attribute	O
)	O
matrix	O
.	O

These	O
vectors	O
(	O
85	O
-	O
dim	O
for	O
AWA	O
,	O
312	O
-	O
dim	O
for	O
CUB	Material
)	O
can	O
either	O
model	O
the	O
presence	O
/	O
absence	O
(	O
)	O
or	O
the	O
confidence	O
level	O
(	O
)	O
of	O
each	O
attribute	O
.	O

We	O
show	O
that	O
continuous	O
attributes	O
indeed	O
encode	O
more	O
semantics	O
than	O
binary	O
attributes	O
by	O
observing	O
a	O
substantial	O
improvement	O
with	O
over	O
with	O
deep	O
features	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Overall	O
,	O
CNN	Method
outperforms	O
FV	Method
,	O
while	O
GOOG	Method
gives	O
the	O
best	O
performing	O
results	O
;	O
therefore	O
in	O
the	O
following	O
,	O
we	O
comment	O
only	O
on	O
our	O
results	O
obtained	O
using	O
GOOG	Method
.	O

On	O
CUB	Material
,	O
i.e.	O
a	O
fine	O
-	O
grained	O
dataset	O
,	O
obtains	O
37.8	O
%	O
accuracy	Metric
,	O
which	O
is	O
significantly	O
above	O
the	O
SoA	O
(	O
26.9	O
%	O
)	O
.	O

Moreover	O
,	O
achieves	O
an	O
impressive	O
50.1	O
%	O
accuracy	Metric
;	O
outperforming	O
the	O
SoA	O
by	O
a	O
large	O
margin	O
.	O

We	O
observe	O
the	O
same	O
trend	O
for	O
AWA	O
,	O
which	O
is	O
a	O
benchmark	O
dataset	O
for	O
zero	Task
-	Task
shot	Task
learning	Task
.	O

On	O
AWA	O
,	O
obtains	O
52.0	O
%	O
accuracy	Metric
and	O
improves	O
the	O
accuracy	Metric
substantially	O
to	O
66.7	O
%	O
,	O
significantly	O
outperforming	O
the	O
SoA	O
(	O
48.5	O
%	O
)	O
.	O

To	O
summarize	O
,	O
we	O
have	O
shown	O
that	O
improves	O
the	O
performance	O
of	O
using	O
deep	O
features	O
,	O
which	O
indicates	O
that	O
with	O
,	O
the	O
SJE	Method
method	O
learns	O
a	O
matrix	O
that	O
better	O
approximates	O
the	O
compatibility	O
of	O
images	O
and	O
side	O
information	O
than	O
.	O

Learned	O
Embeddings	O
from	O
Text	O
.	O

As	O
the	O
visual	O
similarity	O
between	O
objects	O
in	O
different	O
classes	O
increases	O
,	O
e.g.	O
in	O
fine	O
-	O
grained	O
datasets	O
,	O
the	O
cost	O
of	O
collecting	O
attributes	O
also	O
increases	O
.	O

Therefore	O
,	O
we	O
aim	O
to	O
extract	O
class	O
similarities	O
automatically	O
from	O
unlabeled	O
online	O
textual	O
resources	O
.	O

We	O
evaluate	O
three	O
methods	O
,	O
Word2Vec	Method
(	Method
)	O
,	O
GloVe	Method
(	Method
)	O
and	O
the	O
historically	O
most	O
commonly	O
-	O
used	O
method	O
BoW	Method
(	O
)	O
.	O

We	O
build	O
and	O
on	O
the	O
entire	O
English	O
Wikipedia	O
dump	O
.	O

Note	O
that	O
the	O
plain	O
Word2Vec	O
was	O
used	O
in	O
;	O
however	O
,	O
rather	O
than	O
using	O
Word2Vec	Method
in	O
an	O
averaging	Method
mechanism	Method
,	O
we	O
pre	O
-	O
process	O
the	O
Wikipedia	O
as	O
described	O
in	O
Sec	O
[	O
reference	O
]	O
so	O
that	O
our	O
class	O
names	O
are	O
directly	O
present	O
in	O
the	O
Word2Vec	O
vocabulary	O
.	O

This	O
leads	O
to	O
a	O
significant	O
accuracy	Metric
improvement	O
.	O

For	O
we	O
use	O
a	O
subset	O
of	O
Wikipedia	O
populated	O
only	O
with	O
articles	O
that	O
correspond	O
to	O
our	O
classes	O
.	O

On	O
CUB	Material
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	Metric
is	O
observed	O
with	O
(	O
28.4	O
%	O
)	O
improving	O
the	O
supervised	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
is	O
promising	O
and	O
impressive	O
since	O
does	O
not	O
use	O
any	O
human	O
supervision	O
.	O

On	O
AWA	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	Metric
is	O
observed	O
with	O
(	O
58.8	O
%	O
)	O
followed	O
by	O
(	O
51.2	O
%	O
)	O
,	O
improving	O
the	O
supervised	O
SoA	O
(	O
48.5	O
%	O
)	O
significantly	O
.	O

On	O
Dogs	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
the	O
best	O
accuracy	Metric
is	O
obtained	O
with	O
(	O
33.0	O
%	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
using	O
(	O
19.6	O
%	O
)	O
and	O
(	O
17.8	O
%	O
)	O
leads	O
to	O
significantly	O
lower	O
accuracies	Metric
.	O

Unlike	O
birds	O
,	O
different	O
dog	O
breeds	O
belong	O
to	O
the	O
same	O
species	O
and	O
thus	O
they	O
share	O
a	O
common	O
scientific	O
name	O
.	O

As	O
a	O
result	O
,	O
our	O
method	O
of	O
cleanly	Task
pre	Task
-	Task
processing	Task
Wikipedia	Task
by	O
replacing	O
the	O
occurrences	O
of	O
bird	O
names	O
with	O
a	O
unique	O
scientific	O
name	O
was	O
not	O
possible	O
for	O
Dogs	O
.	O

This	O
may	O
lead	O
to	O
vectors	O
obtained	O
from	O
Wikipedia	O
for	O
dogs	O
that	O
are	O
vulnerable	O
to	O
variation	O
in	O
nomenclature	O
.	O

In	O
summary	O
,	O
our	O
results	O
indicate	O
no	O
winner	O
among	O
,	O
and	O
.	O

These	O
embeddings	O
may	O
be	O
task	O
specific	O
and	O
complement	O
each	O
other	O
.	O

We	O
investigate	O
the	O
complementarity	O
of	O
embeddings	O
in	O
the	O
following	O
sections	O
.	O

Effect	O
of	O
Text	O
Corpus	O
.	O

For	O
and	O
,	O
we	O
analyze	O
the	O
effects	O
of	O
three	O
text	O
corpora	O
(	O
B	O
,	O
W	O
,	O
B	O
+	O
W	O
)	O
with	O
varying	O
size	O
and	O
specificity	O
.	O

We	O
build	O
our	O
specialized	O
bird	O
corpus	O
(	O
B	O
)	O
by	O
collecting	O
bird	O
-	O
related	O
information	O
from	O
various	O
online	O
resources	O
,	O
i.e.	O
audubon.org	O
,	O
birdweb.org	O
,	O
allaboutbirds.org	O
and	O
BNA	O
.	O

In	O
combination	O
,	O
this	O
corresponds	O
to	O
50	O
MB	O
of	O
bird	O
-	O
related	O
text	O
.	O

We	O
use	O
the	O
English	O
-	O
language	O
Wikipedia	O
from	O
13.02.2014	O
as	O
our	O
large	O
and	O
general	O
corpus	O
(	O
W	O
)	O
which	O
is	O
40	O
GB	O
of	O
text	O
.	O

Finally	O
,	O
we	O
combine	O
B	O
and	O
W	O
to	O
build	O
a	O
large	O
-	O
scale	O
text	O
corpus	O
enriched	O
with	O
bird	O
specific	O
text	O
(	O
B	O
+	O
W	O
)	O
.	O

On	O
W	O
and	O
B	O
+	O
W	O
,	O
a	O
small	O
window	O
size	O
(	O
10	O
for	O
and	O
20	O
for	O
)	O
;	O
on	O
B	O
,	O
a	O
large	O
window	O
size	O
(	O
35	O
for	O
and	O
50	O
for	O
)	O
is	O
required	O
.	O

We	O
choose	O
parameters	O
after	O
a	O
grid	Method
search	Method
.	O

Increased	O
specificity	O
of	O
the	O
text	O
corpus	O
implies	O
semantic	O
consistency	O
throughout	O
the	O
text	O
.	O

Therefore	O
,	O
large	O
context	O
windows	O
capture	O
semantics	O
well	O
in	O
our	O
bird	O
specific	O
(	O
B	O
)	O
corpus	O
.	O

On	O
the	O
other	O
hand	O
,	O
W	O
is	O
organized	O
alphabetically	O
w.r.t	O
.	O

the	O
document	O
title	O
;	O
hence	O
,	O
a	O
large	O
sampling	O
window	O
can	O
include	O
content	O
from	O
another	O
article	O
that	O
is	O
adjacent	O
to	O
the	O
target	O
word	O
alphabetically	O
.	O

Here	O
,	O
small	O
windows	O
capture	O
semantics	O
better	O
by	O
looking	O
at	O
the	O
text	O
locally	O
.	O

We	O
report	O
our	O
results	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

Using	O
,	O
B	O
+	O
W	O
(	O
26.1	O
%	O
)	O
gives	O
the	O
highest	O
accuracy	Metric
,	O
followed	O
by	O
W	O
(	O
24.2	O
%	O
)	O
.	O

One	O
possible	O
reason	O
is	O
that	O
when	O
the	O
semantic	O
similarity	O
is	O
modeled	O
with	O
cooccurrence	O
statistics	O
,	O
output	O
embeddings	O
become	O
more	O
informative	O
with	O
the	O
increasing	O
corpus	O
size	O
,	O
since	O
the	O
probability	O
of	O
cooccurrence	O
of	O
similar	O
concepts	O
increases	O
.	O

Using	O
,	O
the	O
accuracy	Metric
obtained	O
with	O
B	O
(	O
22.5	O
%	O
)	O
is	O
already	O
higher	O
than	O
the	O
-	O
based	O
SoA	O
(	O
22.3	O
%	O
)	O
,	O
illustrating	O
the	O
benefit	O
of	O
using	O
fine	O
-	O
grained	O
text	O
for	O
fine	Task
-	Task
grained	Task
tasks	Task
.	O

Another	O
advantage	O
of	O
using	O
B	O
is	O
that	O
,	O
since	O
it	O
is	O
short	O
,	O
building	O
is	O
efficient	O
.	O

Moreover	O
,	O
building	O
with	O
B	O
does	O
not	O
require	O
any	O
annotation	O
effort	O
.	O

Building	O
using	O
W	Method
(	O
28.4	O
%	O
)	O
gives	O
the	O
highest	O
accuracy	Metric
,	O
followed	O
by	O
W	Method
+	Method
B	Method
(	O
27.5	O
%	O
)	O
which	O
improves	O
the	O
supervised	O
SoA	O
(	O
26.9	O
%	O
)	O
.	O

We	O
speculate	O
that	O
since	O
Word2Vec	Method
is	O
a	O
variant	O
of	O
the	O
Feedforward	Method
Neural	Method
Network	Method
Language	Method
Model	Method
(	Method
FNNLM	Method
)	Method
,	O
a	O
deep	Method
architecture	Method
,	O
it	O
may	O
learn	O
more	O
from	O
negative	O
data	O
than	O
positives	O
.	O

This	O
was	O
also	O
observed	O
for	O
CNN	O
features	O
learned	O
with	O
a	O
large	O
number	O
of	O
unlabeled	O
surrogate	O
classes	O
.	O

Additionally	O
,	O
we	O
propose	O
a	O
weakly	Method
-	Method
supervised	Method
alternative	Method
to	O
Word2Vec	Method
framework	Method
(	O
,	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
weak	O
-	O
supervision	O
comes	O
from	O
using	O
the	O
specialized	O
B	O
corpus	O
to	O
fine	O
-	O
tune	O
the	O
weights	O
of	O
the	O
network	O
and	O
model	O
the	O
bird	O
-	O
related	O
information	O
.	O

With	O
alone	O
,	O
we	O
obtain	O
21.0	O
%	O
accuracy	Metric
.	O

However	O
,	O
when	O
it	O
is	O
combined	O
with	O
(	O
28.4	O
%	O
)	O
,	O
the	O
accuracy	Metric
improves	O
to	O
29.7	O
%	O
.	O

Compared	O
to	O
the	O
results	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
29.7	O
%	O
is	O
the	O
highest	O
accuracy	Metric
obtained	O
using	O
unsupervised	Method
embeddings	Method
.	O

We	O
regard	O
these	O
results	O
as	O
a	O
very	O
encouraging	O
evidence	O
that	O
Word2Vec	Method
representations	Method
can	O
indeed	O
be	O
made	O
more	O
discriminative	O
for	O
fine	Task
-	Task
grained	Task
zero	Task
-	Task
shot	Task
learning	Task
by	O
integrating	O
a	O
fine	O
-	O
grained	O
text	O
corpus	O
directly	O
to	O
the	O
output	Task
embedding	Task
learning	Task
problem	Task
.	O

Hierarchical	Method
Embeddings	Method
.	O

The	O
hierarchical	O
organization	O
of	O
concepts	O
typically	O
embodies	O
a	O
fair	O
amount	O
of	O
hidden	O
information	O
about	O
language	O
,	O
such	O
as	O
synonymy	O
,	O
semantic	O
relations	O
,	O
etc	O
.	O

Therefore	O
,	O
semantic	O
relatedness	O
defined	O
by	O
hierarchical	O
distance	O
between	O
classes	O
can	O
form	O
numerical	O
vectors	O
to	O
be	O
used	O
as	O
output	Task
embeddings	Task
for	O
zero	Task
-	Task
shot	Task
learning	Task
.	O

We	O
build	O
ontological	O
relationships	O
between	O
our	O
classes	O
using	O
the	O
WordNet	Method
taxonomy	Method
.	O

Due	O
to	O
its	O
large	O
size	O
,	O
WordNet	O
encapsulates	O
all	O
of	O
our	O
AWA	O
and	O
Dog	O
classes	O
.	O

For	O
CUB	Material
,	O
the	O
high	O
level	O
bird	O
species	O
,	O
i.e.	O
albatross	O
,	O
appear	O
as	O
synsets	O
in	O
WordNet	O
,	O
but	O
the	O
specific	O
bird	O
names	O
,	O
i.e.	O
black	O
-	O
footed	O
albatross	O
,	O
are	O
not	O
always	O
present	O
.	O

Therefore	O
we	O
take	O
the	O
hierarchy	O
up	O
to	O
high	O
level	O
bird	O
species	O
as	O
-	O
is	O
and	O
we	O
assume	O
the	O
specific	O
bird	O
classes	O
are	O
all	O
at	O
the	O
bottom	O
of	O
the	O
hierarchy	O
located	O
with	O
the	O
same	O
distance	O
to	O
their	O
immediate	O
ancestors	O
.	O

The	O
WordNet	O
hierarchy	O
contains	O
319	O
nodes	O
for	O
CUB	Material
(	O
200	O
classes	O
)	O
,	O
104	O
nodes	O
for	O
AWA	O
(	O
50	O
classes	O
)	O
and	O
163	O
nodes	O
for	O
Dogs	O
(	O
113	O
classes	O
)	O
.	O

We	O
measure	O
the	O
distance	O
between	O
classes	O
using	O
the	O
similarity	Method
measures	Method
from	O
Sec	O
[	O
reference	O
]	O
.	O

While	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
different	O
hierarchical	O
similarity	Method
measures	Method
have	O
very	O
different	O
behaviors	O
on	O
each	O
dataset	O
.	O

The	O
best	O
performing	O
obtains	O
51.2	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
accuracy	Metric
on	O
AWA	O
which	O
reaches	O
our	O
(	O
52.0	O
%	O
)	O
and	O
improves	O
(	O
44.9	O
%	O
)	O
significantly	O
.	O

On	O
CUB	Material
,	O
obtains	O
20.6	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
remain	O
below	O
our	O
(	O
37.8	O
%	O
)	O
and	O
approaches	O
(	O
22.1	O
%	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
on	O
Dogs	O
obtains	O
24.3	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
is	O
significantly	O
higher	O
than	O
the	O
unsupervised	Method
text	Method
embeddings	Method
(	O
19.6	O
%	O
)	O
and	O
(	O
17.8	O
%	O
)	O
.	O

Combining	O
Output	Method
Embeddings	Method
.	O

In	O
this	O
section	O
,	O
we	O
combine	O
output	O
embeddings	O
obtained	O
through	O
human	O
annotation	O
(	O
)	O
,	O
from	O
text	O
(	O
)	O
and	O
from	O
hierarchies	O
(	O
)	O
.	O

As	O
a	O
reference	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
summarizes	O
the	O
results	O
obtained	O
using	O
one	O
output	Method
embedding	Method
at	O
a	O
time	O
.	O

Our	O
intuition	O
is	O
that	O
because	O
the	O
different	O
embeddings	O
attempt	O
to	O
encapsulate	O
different	O
information	O
,	O
accuracy	Metric
should	O
improve	O
when	O
multiple	O
embeddings	O
are	O
combined	O
.	O

We	O
can	O
observe	O
this	O
complementarity	O
either	O
by	O
simple	O
concatenation	Method
(	O
cnc	Method
)	O
or	O
systematically	O
combining	O
(	O
cmb	Method
)	Method
output	Method
embeddings	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
also	O
known	O
as	O
early	O
/	O
late	O
fusion	O
.	O

For	O
cnc	Method
,	O
we	O
perform	O
full	O
SJE	Method
training	O
and	O
cross	Method
-	Method
validation	Method
on	O
the	O
concatenated	O
output	O
embeddings	O
.	O

For	O
cmb	Method
,	O
we	O
learn	O
joint	O
embeddings	O
for	O
each	O
output	O
separately	O
(	O
which	O
is	O
trivially	O
parallelized	O
)	O
,	O
and	O
find	O
ensemble	O
weights	O
via	O
cross	Method
-	Method
validation	Method
.	O

In	O
contrast	O
to	O
the	O
cnc	Method
method	Method
,	O
no	O
additional	O
joint	Method
training	Method
is	O
used	O
,	O
although	O
it	O
can	O
improve	O
performance	O
in	O
practice	O
.	O

We	O
observe	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
in	O
almost	O
all	O
cases	O
cmb	Method
outperforms	O
cnc	Method
.	O

We	O
analyze	O
the	O
combination	O
of	O
unsupervised	Method
embeddings	Method
(	O
)	O
.	O

On	O
AWA	O
,	O
(	O
58.8	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
combined	O
with	O
(	O
51.2	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
achieve	O
60.1	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
improves	O
the	O
SoA	O
(	O
48.5	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
by	O
a	O
large	O
margin	O
.	O

On	O
CUB	Material
,	O
combining	O
(	O
24.2	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
(	O
20.6	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
get	O
29.9	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
improve	O
the	O
supervised	O
-	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Supporting	O
our	O
initial	O
claim	O
,	O
unsupervised	O
output	O
embeddings	O
obtained	O
from	O
different	O
sources	O
,	O
i.e.	O
text	O
vs	O
hierarchy	O
,	O
seem	O
to	O
be	O
complementary	O
to	O
each	O
other	O
.	O

In	O
some	O
cases	O
,	O
cmb	Method
performs	O
worse	O
than	O
cnc	Method
;	O
e.g.	O
28.2	O
%	O
versus	O
35.1	O
%	O
when	O
using	O
with	O
on	O
Dogs	O
.	O

In	O
most	O
other	O
cases	O
cmb	Method
performs	O
equivalent	O
or	O
better	O
.	O

Combining	O
supervised	Method
(	Method
)	O
and	O
unsupervised	Method
embeddings	Method
(	O
)	O
shows	O
a	O
similar	O
trend	O
.	O

On	O
AWA	O
,	O
combining	O
(	O
66.7	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
and	O
leads	O
to	O
73.9	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
which	O
significantly	O
exceeds	O
the	O
SoA	O
(	O
48.5	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

On	O
CUB	Material
,	O
combining	O
with	O
and	O
leads	O
to	O
51.7	O
%	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
,	O
improving	O
both	O
the	O
results	O
we	O
obtained	O
with	O
(	O
50.1	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
the	O
supervised	O
-	O
SoA	O
(	O
26.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
have	O
shown	O
with	O
these	O
experiments	O
that	O
output	O
embeddings	O
obtained	O
through	O
human	O
annotation	O
can	O
also	O
be	O
complemented	O
with	O
unsupervised	Method
output	Method
embeddings	Method
using	O
the	O
SJE	Method
framework	O
.	O

Qualitative	O
Results	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
top	O
-	O
5	O
highest	O
ranked	O
images	O
for	O
classes	O
chimpanzee	O
,	O
leopard	O
and	O
seal	O
that	O
are	O
selected	O
from	O
10	O
test	O
classes	O
of	O
AWA	O
.	O

We	O
use	O
GOOG	O
as	O
input	O
embeddings	O
and	O
as	O
output	O
embeddings	O
we	O
use	O
supervised	Method
,	O
the	O
best	O
performing	O
unsupervised	Method
embedding	Method
on	O
AWA	O
(	O
)	O
,	O
and	O
the	O
combination	O
of	O
the	O
two	O
(	O
)	O
.	O

For	O
the	O
class	O
chimpanzee	O
,	O
emphasizes	O
that	O
chimpanzees	O
live	O
on	O
trees	O
,	O
which	O
is	O
among	O
the	O
list	O
of	O
attributes	O
.	O

On	O
the	O
other	O
hand	O
,	O
models	O
the	O
social	O
nature	O
of	O
the	O
animal	O
,	O
ranking	O
a	O
group	O
of	O
chimpanzees	O
interacting	O
with	O
each	O
other	O
at	O
the	O
highest	O
.	O

Indeed	O
this	O
information	O
can	O
easily	O
be	O
retrieved	O
from	O
Wikipedia	O
.	O

synthesizes	O
both	O
aspects	O
.	O

Similarly	O
,	O
for	O
leopard	O
puts	O
an	O
emphasis	O
on	O
the	O
head	O
where	O
we	O
can	O
observe	O
several	O
of	O
the	O
attributes	O
,	O
i.e.	O
color	O
,	O
spotted	O
,	O
whereas	O
seems	O
to	O
place	O
the	O
animal	O
in	O
the	O
wild	O
.	O

combines	O
both	O
aspects	O
.	O

In	O
case	O
of	O
class	Task
seal	Task
,	O
retrieves	O
images	O
related	O
to	O
water	O
and	O
ranks	O
whales	O
and	O
seals	O
highest	O
,	O
whereas	O
adds	O
more	O
context	O
by	O
placing	O
seals	O
in	O
the	O
icy	O
natural	O
environment	O
and	O
within	O
groups	O
.	O

Finally	O
,	O
ranks	O
seal	O
-	O
shaped	O
animals	O
on	O
ice	O
,	O
close	O
to	O
water	O
and	O
within	O
groups	O
the	O
highest	O
.	O

We	O
find	O
these	O
qualitative	O
results	O
interesting	O
as	O
they	O
depict	O
how	O
(	O
1	O
)	O
unsupervised	O
embeddings	O
capture	O
nameable	O
semantics	O
about	O
objects	O
and	O
(	O
2	O
)	O
different	O
output	O
embeddings	O
are	O
semantically	O
complementary	O
for	O
zero	Method
-	Method
shot	Method
learning	Method
.	O

section	O
:	O
Conclusion	O
We	O
evaluated	O
the	O
Structured	Method
Joint	Method
Embedding	Method
(	O
SJE	Method
)	O
framework	O
on	O
supervised	O
attributes	O
and	O
unsupervised	O
output	O
embeddings	O
obtained	O
from	O
hierarchies	O
and	O
unlabeled	O
text	O
corpora	O
.	O

We	O
proposed	O
a	O
novel	O
weakly	Method
-	Method
supervised	Method
label	Method
embedding	Method
technique	Method
.	O

By	O
combining	O
multiple	O
output	O
embeddings	O
(	O
cmb	Method
)	O
,	O
we	O
established	O
a	O
new	O
SoA	O
on	O
AWA	O
(	O
73.9	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
and	O
CUB	Material
(	O
51.7	O
%	O
,	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

Moreover	O
,	O
we	O
showed	O
that	O
unsupervised	Task
zero	Task
-	Task
shot	Task
learning	Task
with	O
SJE	Method
improves	O
the	O
SoA	O
,	O
to	O
60.1	O
%	O
on	O
AWA	O
and	O
29.9	O
%	O
on	O
CUB	Material
,	O
and	O
obtains	O
35.1	O
%	O
on	O
Dogs	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

We	O
emphasize	O
the	O
following	O
take	O
-	O
home	O
points	O
:	O
(	O
1	O
)	O
Unsupervised	Method
label	Method
embeddings	Method
learned	O
from	O
text	O
corpora	O
yield	O
compelling	O
zero	O
-	O
shot	O
results	O
,	O
outperforming	O
previous	O
supervised	O
SoA	O
on	O
AWA	O
and	O
CUB	Material
(	O
Tab	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O

(	O
2	O
)	O
Integrating	O
specialized	O
text	O
corpora	O
helps	O
due	O
to	O
incorporating	O
more	O
fine	O
-	O
grained	O
information	O
to	O
output	O
embeddings	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

(	O
3	O
)	O
Combining	O
unsupervised	Method
output	Method
embeddings	Method
improve	O
the	O
zero	Metric
-	Metric
shot	Metric
performance	Metric
,	O
suggesting	O
that	O
they	O
provide	O
complementary	O
information	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

(	O
4	O
)	O
There	O
is	O
still	O
a	O
large	O
gap	O
between	O
the	O
performance	O
of	O
unsupervised	O
output	O
embeddings	O
and	O
human	O
-	O
annotated	O
attributes	O
on	O
AWA	O
and	O
CUB	Material
,	O
suggesting	O
that	O
better	O
methods	O
are	O
needed	O
for	O
learning	O
discriminative	Task
output	Task
embeddings	Task
from	O
text	O
.	O

(	O
5	O
)	O
Finally	O
,	O
supporting	O
,	O
encoding	O
continuous	O
nature	O
of	O
attributes	O
significantly	O
improve	O
upon	O
binary	O
attributes	O
for	O
zero	Task
-	Task
shot	Task
classification	Task
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
.	O

As	O
future	O
work	O
,	O
we	O
plan	O
to	O
investigate	O
other	O
methods	O
to	O
combine	O
multiple	O
output	O
embeddings	O
and	O
to	O
improve	O
the	O
discriminative	Metric
power	Metric
of	O
unsupervised	Method
and	Method
weakly	Method
-	Method
supervised	Method
label	Method
embeddings	Method
for	O
fine	Task
-	Task
grained	Task
classification	Task
.	O

subsection	O
:	O
Acknowledgments	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
ONR	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0762	O
,	O
NSF	O
CMMI	O
-	O
1266184	O
,	O
Google	O
Faculty	O
Research	O
Award	O
,	O
and	O
NSF	O
Graduate	O
Fellowship	O
.	O

bibliography	O
:	O
References	O
