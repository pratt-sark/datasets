document	O
:	O
Deep	Method
contextualized	Method
word	Method
representations	Method
We	O
introduce	O
a	O
new	O
type	O
of	O
deep	Method
contextualized	Method
word	Method
representation	Method
that	O
models	O
both	O
(	O
1	O
)	O
complex	O
characteristics	O
of	O
word	O
use	O
(	O
e.g.	O
,	O
syntax	O
and	O
semantics	O
)	O
,	O
and	O
(	O
2	O
)	O
how	O
these	O
uses	O
vary	O
across	O
linguistic	O
contexts	O
(	O
i.e.	O
,	O
to	O
model	O
polysemy	O
)	O
.	O

Our	O
word	O
vectors	O
are	O
learned	O
functions	O
of	O
the	O
internal	O
states	O
of	O
a	O
deep	Method
bidirectional	Method
language	Method
model	Method
(	O
biLM	Method
)	Method
,	O
which	O
is	O
pre	O
-	O
trained	O
on	O
a	O
large	Material
text	Material
corpus	Material
.	O

We	O
show	O
that	O
these	O
representations	O
can	O
be	O
easily	O
added	O
to	O
existing	O
models	O
and	O
significantly	O
improve	O
the	O
state	O
of	O
the	O
art	O
across	O
six	O
challenging	O
NLP	Task
problems	Task
,	O
including	O
question	Task
answering	Task
,	O
textual	Task
entailment	Task
and	O
sentiment	Task
analysis	Task
.	O

We	O
also	O
present	O
an	O
analysis	O
showing	O
that	O
exposing	O
the	O
deep	O
internals	O
of	O
the	O
pre	Method
-	Method
trained	Method
network	Method
is	O
crucial	O
,	O
allowing	O
downstream	Method
models	Method
to	O
mix	O
different	O
types	O
of	O
semi	O
-	O
supervision	O
signals	O
.	O

section	O
:	O
Introduction	O
Pre	O
-	O
trained	O
word	Method
representations	Method
word2vec	Method
,	O
Pennington2014GloveGV	O
are	O
a	O
key	O
component	O
in	O
many	O
neural	Method
language	Method
understanding	Method
models	Method
.	O

However	O
,	O
learning	Task
high	Task
quality	Task
representations	Task
can	O
be	O
challenging	O
.	O

They	O
should	O
ideally	O
model	O
both	O
(	O
1	O
)	O
complex	O
characteristics	O
of	O
word	O
use	O
(	O
e.g.	O
,	O
syntax	O
and	O
semantics	O
)	O
,	O
and	O
(	O
2	O
)	O
how	O
these	O
uses	O
vary	O
across	O
linguistic	O
contexts	O
(	O
i.e.	O
,	O
to	O
model	O
polysemy	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
type	O
of	O
deep	Method
contextualized	Method
word	Method
representation	Method
that	O
directly	O
addresses	O
both	O
challenges	O
,	O
can	O
be	O
easily	O
integrated	O
into	O
existing	O
models	O
,	O
and	O
significantly	O
improves	O
the	O
state	O
of	O
the	O
art	O
in	O
every	O
considered	O
case	O
across	O
a	O
range	O
of	O
challenging	O
language	Task
understanding	Task
problems	Task
.	O

Our	O
representations	O
differ	O
from	O
traditional	O
word	Method
type	Method
embeddings	Method
in	O
that	O
each	O
token	O
is	O
assigned	O
a	O
representation	O
that	O
is	O
a	O
function	O
of	O
the	O
entire	O
input	O
sentence	O
.	O

We	O
use	O
vectors	O
derived	O
from	O
a	O
bidirectional	Method
LSTM	Method
that	O
is	O
trained	O
with	O
a	O
coupled	Method
language	Method
model	Method
(	Method
LM	Method
)	Method
objective	Method
on	O
a	O
large	Material
text	Material
corpus	Material
.	O

For	O
this	O
reason	O
,	O
we	O
call	O
them	O
ELMo	Method
(	Method
Embeddings	Method
from	Method
Language	Method
Models	Method
)	Method
representations	Method
.	O

Unlike	O
previous	O
approaches	O
for	O
learning	Task
contextualized	Task
word	Task
vectors	Task
Peters2017SemisupervisedST	Task
,	O
McCann2017LearnedIT	Method
,	O
ELMo	Method
representations	Method
are	O
deep	O
,	O
in	O
the	O
sense	O
that	O
they	O
are	O
a	O
function	O
of	O
all	O
of	O
the	O
internal	O
layers	O
of	O
the	O
biLM	Method
.	O

More	O
specifically	O
,	O
we	O
learn	O
a	O
linear	Method
combination	Method
of	O
the	O
vectors	O
stacked	O
above	O
each	O
input	O
word	O
for	O
each	O
end	O
task	O
,	O
which	O
markedly	O
improves	O
performance	O
over	O
just	O
using	O
the	O
top	Method
LSTM	Method
layer	Method
.	O

Combining	O
the	O
internal	O
states	O
in	O
this	O
manner	O
allows	O
for	O
very	O
rich	O
word	Method
representations	Method
.	O

Using	O
intrinsic	O
evaluations	O
,	O
we	O
show	O
that	O
the	O
higher	O
-	O
level	O
LSTM	Method
states	Method
capture	O
context	O
-	O
dependent	O
aspects	O
of	O
word	O
meaning	O
(	O
e.g.	O
,	O
they	O
can	O
be	O
used	O
without	O
modification	O
to	O
perform	O
well	O
on	O
supervised	Task
word	Task
sense	Task
disambiguation	Task
tasks	Task
)	O
while	O
lower	O
-	O
level	O
states	O
model	O
aspects	O
of	O
syntax	O
(	O
e.g.	O
,	O
they	O
can	O
be	O
used	O
to	O
do	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
)	O
.	O

Simultaneously	O
exposing	O
all	O
of	O
these	O
signals	O
is	O
highly	O
beneficial	O
,	O
allowing	O
the	O
learned	O
models	O
select	O
the	O
types	O
of	O
semi	Task
-	Task
supervision	Task
that	O
are	O
most	O
useful	O
for	O
each	O
end	O
task	O
.	O

Extensive	O
experiments	O
demonstrate	O
that	O
ELMo	Method
representations	Method
work	O
extremely	O
well	O
in	O
practice	O
.	O

We	O
first	O
show	O
that	O
they	O
can	O
be	O
easily	O
added	O
to	O
existing	O
models	O
for	O
six	O
diverse	Task
and	Task
challenging	Task
language	Task
understanding	Task
problems	Task
,	O
including	O
textual	Task
entailment	Task
,	O
question	Task
answering	Task
and	O
sentiment	Task
analysis	Task
.	O

The	O
addition	O
of	O
ELMo	Method
representations	Method
alone	O
significantly	O
improves	O
the	O
state	O
of	O
the	O
art	O
in	O
every	O
case	O
,	O
including	O
up	O
to	O
20	O
%	O
relative	Metric
error	Metric
reductions	Metric
.	O

For	O
tasks	O
where	O
direct	O
comparisons	O
are	O
possible	O
,	O
ELMo	Method
outperforms	O
CoVe	Method
McCann2017LearnedIT	Method
,	O
which	O
computes	O
contextualized	Method
representations	Method
using	O
a	O
neural	Method
machine	Method
translation	Method
encoder	Method
.	O

Finally	O
,	O
an	O
analysis	O
of	O
both	O
ELMo	Method
and	O
CoVe	Method
reveals	O
that	O
deep	Method
representations	Method
outperform	O
those	O
derived	O
from	O
just	O
the	O
top	Method
layer	Method
of	O
an	O
LSTM	Method
.	O

Our	O
trained	O
models	O
and	O
code	O
are	O
publicly	O
available	O
,	O
and	O
we	O
expect	O
that	O
ELMo	Method
will	O
provide	O
similar	O
gains	O
for	O
many	O
other	O
NLP	Task
problems	Task
.	O

section	O
:	O
Related	O
work	O
Due	O
to	O
their	O
ability	O
to	O
capture	O
syntactic	O
and	O
semantic	O
information	O
of	O
words	O
from	O
large	O
scale	O
unlabeled	Material
text	Material
,	O
pretrained	O
word	Method
vectors	Method
Turian2010WordRA	Method
,	O
word2vec	Material
,	O
Pennington2014GloveGV	Material
are	O
a	O
standard	O
component	O
of	O
most	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NLP	Method
architectures	Method
,	O
including	O
for	O
question	Task
answering	Task
liu2017stochastic	Task
,	O
textual	Task
entailment	Task
Chen2017EnhancedLF	Task
and	O
semantic	Task
role	Task
labeling	Task
He2017DeepSR	Task
.	O

However	O
,	O
these	O
approaches	O
for	O
learning	Task
word	Task
vectors	Task
only	O
allow	O
a	O
single	O
context	Method
-	Method
independent	Method
representation	Method
for	O
each	O
word	O
.	O

Previously	O
proposed	O
methods	O
overcome	O
some	O
of	O
the	O
shortcomings	O
of	O
traditional	O
word	Method
vectors	Method
by	O
either	O
enriching	O
them	O
with	O
subword	O
information	O
[	O
e.g.	O
,	O
]	O
[	O
]	O
Wieting2016CharagramEW	O
,	O
Bojanowski2017EnrichingWV	O
or	O
learning	O
separate	O
vectors	O
for	O
each	O
word	O
sense	O
[	O
e.g.	O
,	O
]	O
[	O
]	O
Neelakantan2014EfficientNE	O
.	O

Our	O
approach	O
also	O
benefits	O
from	O
subword	O
units	O
through	O
the	O
use	O
of	O
character	Method
convolutions	Method
,	O
and	O
we	O
seamlessly	O
incorporate	O
multi	O
-	O
sense	O
information	O
into	O
downstream	Task
tasks	Task
without	O
explicitly	O
training	O
to	O
predict	O
predefined	O
sense	O
classes	O
.	O

Other	O
recent	O
work	O
has	O
also	O
focused	O
on	O
learning	O
context	Task
-	Task
dependent	Task
representations	Task
.	O

context2vec	Method
Melamud2016context2vecLG	Method
uses	O
a	O
bidirectional	Method
Long	Method
Short	Method
Term	Method
Memory	Method
[	O
LSTM	Method
;	O
]	O
[	O
]	O
LSTM	Method
:	O
Hochreiter1997	Method
to	O
encode	O
the	O
context	O
around	O
a	O
pivot	O
word	O
.	O

Other	O
approaches	O
for	O
learning	Task
contextual	Task
embeddings	Task
include	O
the	O
pivot	O
word	O
itself	O
in	O
the	O
representation	O
and	O
are	O
computed	O
with	O
the	O
encoder	O
of	O
either	O
a	O
supervised	Method
neural	Method
machine	Method
translation	Method
(	O
MT	Method
)	Method
system	Method
[	O
CoVe	O
;	O
]	O
[	O
]	O
McCann2017LearnedIT	Method
or	O
an	O
unsupervised	Method
language	Method
model	Method
Peters2017SemisupervisedST	Method
.	O

Both	O
of	O
these	O
approaches	O
benefit	O
from	O
large	O
datasets	O
,	O
although	O
the	O
MT	Method
approach	Method
is	O
limited	O
by	O
the	O
size	O
of	O
parallel	Material
corpora	Material
.	O

In	O
this	O
paper	O
,	O
we	O
take	O
full	O
advantage	O
of	O
access	O
to	O
plentiful	O
monolingual	Material
data	Material
,	O
and	O
train	O
our	O
biLM	Method
on	O
a	O
corpus	O
with	O
approximately	O
30	O
million	O
sentences	O
Chelba2014OneBW	O
.	O

We	O
also	O
generalize	O
these	O
approaches	O
to	O
deep	Method
contextual	Method
representations	Method
,	O
which	O
we	O
show	O
work	O
well	O
across	O
a	O
broad	O
range	O
of	O
diverse	O
NLP	Task
tasks	Task
.	O

Previous	O
work	O
has	O
also	O
shown	O
that	O
different	O
layers	O
of	O
deep	Method
biRNNs	Method
encode	O
different	O
types	O
of	O
information	O
.	O

For	O
example	O
,	O
introducing	O
multi	Task
-	Task
task	Task
syntactic	Task
supervision	Task
(	O
e.g.	O
,	O
part	O
-	O
of	O
-	O
speech	O
tags	O
)	O
at	O
the	O
lower	O
levels	O
of	O
a	O
deep	Method
LSTM	Method
can	O
improve	O
overall	O
performance	O
of	O
higher	O
level	O
tasks	O
such	O
as	O
dependency	Task
parsing	Task
joint	O
-	O
many	O
-	O
iclr07	Method
or	O
CCG	Method
super	Method
tagging	Method
Sgaard2016DeepML	Method
.	O

In	O
an	O
RNN	Method
-	Method
based	Method
encoder	Method
-	Method
decoder	Method
machine	Method
translation	Method
system	Method
,	O
Belinkov2017WhatDN	O
showed	O
that	O
the	O
representations	O
learned	O
at	O
the	O
first	O
layer	O
in	O
a	O
2	Method
-	Method
layer	Method
LSTM	Method
encoder	Method
are	O
better	O
at	O
predicting	Task
POS	Task
tags	Task
then	O
second	O
layer	O
.	O

Finally	O
,	O
the	O
top	O
layer	O
of	O
an	O
LSTM	Method
for	O
encoding	O
word	O
context	O
Melamud2016context2vecLG	O
has	O
been	O
shown	O
to	O
learn	O
representations	Task
of	Task
word	Task
sense	Task
.	O

We	O
show	O
that	O
similar	O
signals	O
are	O
also	O
induced	O
by	O
the	O
modified	O
language	Metric
model	Metric
objective	Metric
of	O
our	O
ELMo	Method
representations	Method
,	O
and	O
it	O
can	O
be	O
very	O
beneficial	O
to	O
learn	O
models	O
for	O
downstream	Task
tasks	Task
that	O
mix	O
these	O
different	O
types	O
of	O
semi	Method
-	Method
supervision	Method
.	O

Dai2015SemisupervisedSL	Method
and	O
Ramachandran2017ImproveSeq2SeqLMGal2016ATG	Method
pretrain	Method
encoder	Method
-	Method
decoder	Method
pairs	Method
using	O
language	Method
models	Method
and	O
sequence	Method
autoencoders	Method
and	O
then	O
fine	O
tune	O
with	O
task	O
specific	O
supervision	O
.	O

In	O
contrast	O
,	O
after	O
pretraining	O
the	O
biLM	Method
with	O
unlabeled	Material
data	Material
,	O
we	O
fix	O
the	O
weights	O
and	O
add	O
additional	O
task	O
-	O
specific	O
model	O
capacity	O
,	O
allowing	O
us	O
to	O
leverage	O
large	O
,	O
rich	O
and	O
universal	Method
biLM	Method
representations	Method
for	O
cases	O
where	O
downstream	O
training	O
data	O
size	O
dictates	O
a	O
smaller	O
supervised	Method
model	Method
.	O

section	O
:	O
ELMo	Method
:	O
Embeddings	Task
from	O
Language	Method
Models	Method
Unlike	O
most	O
widely	O
used	O
word	Method
embeddings	Method
Pennington2014GloveGV	O
,	O
ELMo	Method
word	Method
representations	Method
are	O
functions	O
of	O
the	O
entire	O
input	O
sentence	O
,	O
as	O
described	O
in	O
this	O
section	O
.	O

They	O
are	O
computed	O
on	O
top	O
of	O
two	Method
-	Method
layer	Method
biLMs	Method
with	O
character	Method
convolutions	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
as	O
a	O
linear	O
function	O
of	O
the	O
internal	O
network	O
states	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
setup	O
allows	O
us	O
to	O
do	O
semi	Task
-	Task
supervised	Task
learning	Task
,	O
where	O
the	O
biLM	Method
is	O
pretrained	O
at	O
a	O
large	O
scale	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
and	O
easily	O
incorporated	O
into	O
a	O
wide	O
range	O
of	O
existing	O
neural	Method
NLP	Method
architectures	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Bidirectional	Method
language	Method
models	Method
Given	O
a	O
sequence	O
of	O
tokens	O
,	O
,	O
a	O
forward	Method
language	Method
model	Method
computes	O
the	O
probability	O
of	O
the	O
sequence	O
by	O
modeling	O
the	O
probability	O
of	O
token	O
given	O
the	O
history	O
:	O
Recent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
language	Method
models	Method
Jzefowicz2016ExploringTL	O
,	O
Melis2017OnTS	Method
,	O
Merity2017RegularizingAO	Method
compute	O
a	O
context	Method
-	Method
independent	Method
token	Method
representation	Method
(	O
via	O
token	Method
embeddings	Method
or	O
a	O
CNN	Method
over	Method
characters	Method
)	O
then	O
pass	O
it	O
through	O
layers	O
of	O
forward	Method
LSTMs	Method
.	O

At	O
each	O
position	O
,	O
each	O
LSTM	Method
layer	Method
outputs	O
a	O
context	Method
-	Method
dependent	Method
representation	Method
where	O
.	O

The	O
top	O
layer	O
LSTM	O
output	O
,	O
,	O
is	O
used	O
to	O
predict	O
the	O
next	O
token	O
with	O
a	O
Softmax	Method
layer	Method
.	O

A	O
backward	Method
LM	Method
is	O
similar	O
to	O
a	O
forward	Method
LM	Method
,	O
except	O
it	O
runs	O
over	O
the	O
sequence	O
in	O
reverse	O
,	O
predicting	O
the	O
previous	O
token	O
given	O
the	O
future	O
context	O
:	O
It	O
can	O
be	O
implemented	O
in	O
an	O
analogous	O
way	O
to	O
a	O
forward	Method
LM	Method
,	O
with	O
each	O
backward	Method
LSTM	Method
layer	Method
in	O
a	O
layer	Method
deep	Method
model	Method
producing	O
representations	O
of	O
given	O
.	O

A	O
biLM	Method
combines	O
both	O
a	O
forward	Method
and	Method
backward	Method
LM	Method
.	O

Our	O
formulation	O
jointly	O
maximizes	O
the	O
log	O
likelihood	O
of	O
the	O
forward	O
and	O
backward	O
directions	O
:	O
We	O
tie	O
the	O
parameters	O
for	O
both	O
the	O
token	Method
representation	Method
(	O
)	O
and	O
Softmax	Method
layer	Method
(	O
)	O
in	O
the	O
forward	O
and	O
backward	O
direction	O
while	O
maintaining	O
separate	O
parameters	O
for	O
the	O
LSTMs	Method
in	O
each	O
direction	O
.	O

Overall	O
,	O
this	O
formulation	O
is	O
similar	O
to	O
the	O
approach	O
of	O
Peters2017SemisupervisedST	Task
,	O
with	O
the	O
exception	O
that	O
we	O
share	O
some	O
weights	O
between	O
directions	O
instead	O
of	O
using	O
completely	O
independent	O
parameters	O
.	O

In	O
the	O
next	O
section	O
,	O
we	O
depart	O
from	O
previous	O
work	O
by	O
introducing	O
a	O
new	O
approach	O
for	O
learning	Task
word	Task
representations	Task
that	O
are	O
a	O
linear	Method
combination	Method
of	Method
the	Method
biLM	Method
layers	Method
.	O

subsection	O
:	O
ELMo	Method
ELMo	Method
is	O
a	O
task	O
specific	O
combination	O
of	O
the	O
intermediate	Method
layer	Method
representations	Method
in	O
the	O
biLM	Method
.	O

For	O
each	O
token	O
,	O
a	O
-	Method
layer	Method
biLM	Method
computes	O
a	O
set	O
of	O
representations	O
where	O
is	O
the	O
token	O
layer	O
and	O
,	O
for	O
each	O
biLSTM	Method
layer	Method
.	O

For	O
inclusion	O
in	O
a	O
downstream	Method
model	Method
,	O
ELMo	Method
collapses	O
all	O
layers	O
in	O
into	O
a	O
single	O
vector	O
,	O
.	O

In	O
the	O
simplest	O
case	O
,	O
ELMo	Method
just	O
selects	O
the	O
top	O
layer	O
,	O
,	O
as	O
in	O
TagLM	Method
Peters2017SemisupervisedST	Method
and	O
CoVe	Material
McCann2017LearnedIT	Material
.	O

More	O
generally	O
,	O
we	O
compute	O
a	O
task	O
specific	O
weighting	O
of	O
all	O
biLM	Method
layers	Method
:	O
In	O
(	O
[	O
reference	O
]	O
)	O
,	O
are	O
softmax	O
-	O
normalized	O
weights	O
and	O
the	O
scalar	O
parameter	O
allows	O
the	O
task	Method
model	Method
to	O
scale	O
the	O
entire	O
ELMo	O
vector	O
.	O

is	O
of	O
practical	O
importance	O
to	O
aid	O
the	O
optimization	Task
process	Task
(	O
see	O
supplemental	O
material	O
for	O
details	O
)	O
.	O

Considering	O
that	O
the	O
activations	O
of	O
each	O
biLM	Method
layer	Method
have	O
a	O
different	O
distribution	O
,	O
in	O
some	O
cases	O
it	O
also	O
helped	O
to	O
apply	O
layer	Method
normalization	Method
Ba2016LayerN	Method
to	O
each	O
biLM	Method
layer	Method
before	O
weighting	O
.	O

subsection	O
:	O
Using	O
biLMs	Method
for	O
supervised	Task
NLP	Task
tasks	Task
Given	O
a	O
pre	O
-	O
trained	O
biLM	Method
and	O
a	O
supervised	Method
architecture	Method
for	O
a	O
target	O
NLP	Task
task	Task
,	O
it	O
is	O
a	O
simple	O
process	O
to	O
use	O
the	O
biLM	Method
to	O
improve	O
the	O
task	Method
model	Method
.	O

We	O
simply	O
run	O
the	O
biLM	Method
and	O
record	O
all	O
of	O
the	O
layer	Method
representations	Method
for	O
each	O
word	O
.	O

Then	O
,	O
we	O
let	O
the	O
end	Method
task	Method
model	Method
learn	O
a	O
linear	Method
combination	Method
of	O
these	O
representations	O
,	O
as	O
described	O
below	O
.	O

First	O
consider	O
the	O
lowest	O
layers	O
of	O
the	O
supervised	Method
model	Method
without	O
the	O
biLM	Method
.	O

Most	O
supervised	Method
NLP	Method
models	Method
share	O
a	O
common	O
architecture	O
at	O
the	O
lowest	O
layers	O
,	O
allowing	O
us	O
to	O
add	O
ELMo	Method
in	O
a	O
consistent	O
,	O
unified	O
manner	O
.	O

Given	O
a	O
sequence	O
of	O
tokens	O
,	O
it	O
is	O
standard	O
to	O
form	O
a	O
context	Method
-	Method
independent	Method
token	Method
representation	Method
for	O
each	O
token	O
position	O
using	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
and	O
optionally	O
character	Method
-	Method
based	Method
representations	Method
.	O

Then	O
,	O
the	O
model	O
forms	O
a	O
context	Method
-	Method
sensitive	Method
representation	Method
,	O
typically	O
using	O
either	O
bidirectional	Method
RNNs	Method
,	O
CNNs	Method
,	O
or	O
feed	Method
forward	Method
networks	Method
.	O

To	O
add	O
ELMo	Method
to	O
the	O
supervised	Method
model	Method
,	O
we	O
first	O
freeze	O
the	O
weights	O
of	O
the	O
biLM	Method
and	O
then	O
concatenate	O
the	O
ELMo	O
vector	O
with	O
and	O
pass	O
the	O
ELMo	Method
enhanced	Method
representation	Method
into	O
the	O
task	Method
RNN	Method
.	O

For	O
some	O
tasks	O
(	O
e.g.	O
,	O
SNLI	O
,	O
SQuAD	O
)	O
,	O
we	O
observe	O
further	O
improvements	O
by	O
also	O
including	O
ELMo	Method
at	O
the	O
output	O
of	O
the	O
task	Method
RNN	Method
by	O
introducing	O
another	O
set	O
of	O
output	O
specific	O
linear	O
weights	O
and	O
replacing	O
with	O
.	O

As	O
the	O
remainder	O
of	O
the	O
supervised	Method
model	Method
remains	O
unchanged	O
,	O
these	O
additions	O
can	O
happen	O
within	O
the	O
context	O
of	O
more	O
complex	O
neural	Method
models	Method
.	O

For	O
example	O
,	O
see	O
the	O
SNLI	Material
experiments	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
where	O
a	O
bi	Method
-	Method
attention	Method
layer	Method
follows	O
the	O
biLSTMs	Method
,	O
or	O
the	O
coreference	Task
resolution	Task
experiments	O
where	O
a	O
clustering	Method
model	Method
is	O
layered	O
on	O
top	O
of	O
the	O
biLSTMs	Method
.	O

Finally	O
,	O
we	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
moderate	O
amount	O
of	O
dropout	O
to	O
ELMo	Method
Srivastava2014DropoutAS	Method
and	O
in	O
some	O
cases	O
to	O
regularize	O
the	O
ELMo	O
weights	O
by	O
adding	O
to	O
the	O
loss	O
.	O

This	O
imposes	O
an	O
inductive	O
bias	O
on	O
the	O
ELMo	O
weights	O
to	O
stay	O
close	O
to	O
an	O
average	O
of	O
all	O
biLM	Method
layers	Method
.	O

Our	O
baseline	O
ELMo	Method
+	O
baseline	O
Increase	O
(	O
absolute	O
/	O
relative	O
)	O
subsection	O
:	O
Pre	O
-	O
trained	O
bidirectional	Method
language	Method
model	Method
architecture	Method
The	O
pre	O
-	O
trained	O
biLMs	Method
in	O
this	O
paper	O
are	O
similar	O
to	O
the	O
architectures	O
in	O
Jzefowicz2016ExploringTL	Material
and	O
kim2015characterNeuralLM	O
,	O
but	O
modified	O
to	O
support	O
joint	O
training	O
of	O
both	O
directions	O
and	O
add	O
a	O
residual	O
connection	O
between	O
LSTM	Method
layers	Method
.	O

We	O
focus	O
on	O
large	Task
scale	Task
biLMs	Task
in	O
this	O
work	O
,	O
as	O
Peters2017SemisupervisedST	Task
highlighted	O
the	O
importance	O
of	O
using	O
biLMs	Method
over	O
forward	Method
-	Method
only	Method
LMs	Method
and	O
large	Task
scale	Task
training	Task
.	O

To	O
balance	O
overall	O
language	Metric
model	Metric
perplexity	Metric
with	O
model	Metric
size	Metric
and	O
computational	Metric
requirements	Metric
for	O
downstream	Task
tasks	Task
while	O
maintaining	O
a	O
purely	O
character	Method
-	Method
based	Method
input	Method
representation	Method
,	O
we	O
halved	O
all	O
embedding	O
and	O
hidden	O
dimensions	O
from	O
the	O
single	O
best	O
model	Method
CNN	Method
-	Method
BIG	Method
-	Method
LSTM	Method
in	O
Jzefowicz2016ExploringTL	Method
.	O

The	O
final	O
model	O
uses	O
biLSTM	Method
layers	Method
with	O
4096	O
units	O
and	O
512	O
dimension	O
projections	O
and	O
a	O
residual	O
connection	O
from	O
the	O
first	O
to	O
second	O
layer	O
.	O

The	O
context	Method
insensitive	Method
type	Method
representation	Method
uses	O
2048	O
character	Method
n	Method
-	Method
gram	Method
convolutional	Method
filters	Method
followed	O
by	O
two	O
highway	Method
layers	Method
Srivastava2015TrainingVD	O
and	O
a	O
linear	Method
projection	Method
down	O
to	O
a	O
512	Method
representation	Method
.	O

As	O
a	O
result	O
,	O
the	O
biLM	Method
provides	O
three	O
layers	O
of	O
representations	O
for	O
each	O
input	O
token	O
,	O
including	O
those	O
outside	O
the	O
training	O
set	O
due	O
to	O
the	O
purely	O
character	O
input	O
.	O

In	O
contrast	O
,	O
traditional	O
word	Method
embedding	Method
methods	Method
only	O
provide	O
one	O
layer	Method
of	Method
representation	Method
for	O
tokens	O
in	O
a	O
fixed	O
vocabulary	O
.	O

After	O
training	O
for	O
10	O
epochs	O
on	O
the	O
1B	Material
Word	Material
Benchmark	Material
Chelba2014OneBW	Material
,	O
the	O
average	Metric
forward	Metric
and	Metric
backward	Metric
perplexities	Metric
is	O
39.7	O
,	O
compared	O
to	O
30.0	O
for	O
the	O
forward	Method
CNN	Method
-	Method
BIG	Method
-	Method
LSTM	Method
.	O

Generally	O
,	O
we	O
found	O
the	O
forward	O
and	O
backward	O
perplexities	O
to	O
be	O
approximately	O
equal	O
,	O
with	O
the	O
backward	O
value	O
slightly	O
lower	O
.	O

Once	O
pretrained	O
,	O
the	O
biLM	Method
can	O
compute	O
representations	O
for	O
any	O
task	O
.	O

In	O
some	O
cases	O
,	O
fine	O
tuning	O
the	O
biLM	Method
on	O
domain	Material
specific	Material
data	Material
leads	O
to	O
significant	O
drops	O
in	O
perplexity	Metric
and	O
an	O
increase	O
in	O
downstream	Task
task	Task
performance	O
.	O

This	O
can	O
be	O
seen	O
as	O
a	O
type	O
of	O
domain	Task
transfer	Task
for	O
the	O
biLM	Method
.	O

As	O
a	O
result	O
,	O
in	O
most	O
cases	O
we	O
used	O
a	O
fine	O
-	O
tuned	O
biLM	Method
in	O
the	O
downstream	Task
task	Task
.	O

See	O
supplemental	O
material	O
for	O
details	O
.	O

section	O
:	O
Evaluation	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
ELMo	Method
across	O
a	O
diverse	O
set	O
of	O
six	O
benchmark	O
NLP	Task
tasks	Task
.	O

In	O
every	O
task	O
considered	O
,	O
simply	O
adding	O
ELMo	Method
establishes	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
,	O
with	O
relative	Metric
error	Metric
reductions	Metric
ranging	O
from	O
6	O
-	O
20	O
%	O
over	O
strong	O
base	Method
models	Method
.	O

This	O
is	O
a	O
very	O
general	O
result	O
across	O
a	O
diverse	O
set	O
model	O
architectures	O
and	O
language	Task
understanding	Task
tasks	Task
.	O

In	O
the	O
remainder	O
of	O
this	O
section	O
we	O
provide	O
high	O
-	O
level	O
sketches	O
of	O
the	O
individual	O
task	O
results	O
;	O
see	O
the	O
supplemental	O
material	O
for	O
full	O
experimental	O
details	O
.	O

Question	Task
answering	Task
The	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
Rajpurkar2016SQuAD10	O
contains	O
100K	O
+	O
crowd	Material
sourced	Material
question	Material
-	Material
answer	Material
pairs	Material
where	O
the	O
answer	O
is	O
a	O
span	O
in	O
a	O
given	O
Wikipedia	O
paragraph	O
.	O

Our	O
baseline	O
model	O
ClarkAdvancingRC	Method
is	O
an	O
improved	O
version	O
of	O
the	O
Bidirectional	Method
Attention	Method
Flow	Method
model	Method
in	O
[	O
BiDAF	O
;	O
]	O
[	O
]	O
Seo2016BidirectionalAF	O
.	O

It	O
adds	O
a	O
self	Method
-	Method
attention	Method
layer	Method
after	O
the	O
bidirectional	Method
attention	Method
component	Method
,	O
simplifies	O
some	O
of	O
the	O
pooling	Method
operations	Method
and	O
substitutes	O
the	O
LSTMs	Method
for	O
gated	Method
recurrent	Method
units	Method
[	O
GRUs	Method
;	O
]	O
[	O
]	O
GRU	O
:	O
Cho2014	O
.	O

After	O
adding	O
ELMo	Method
to	O
the	O
baseline	O
model	O
,	O
test	Metric
set	Metric
F	Metric
improved	O
by	O
4.7	O
%	O
from	O
81.1	O
%	O
to	O
85.8	O
%	O
,	O
a	O
24.9	O
%	O
relative	Metric
error	Metric
reduction	Metric
over	O
the	O
baseline	O
,	O
and	O
improving	O
the	O
overall	O
single	O
model	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
1.4	O
%	O
.	O

A	O
11	O
member	O
ensemble	O
pushes	O
F	Metric
to	O
87.4	O
,	O
the	O
overall	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
at	O
time	O
of	O
submission	O
to	O
the	O
leaderboard	O
.	O

The	O
increase	O
of	O
4.7	O
%	O
with	O
ELMo	Method
is	O
also	O
significantly	O
larger	O
then	O
the	O
1.8	O
%	O
improvement	O
from	O
adding	O
CoVe	Method
to	O
a	O
baseline	O
model	O
McCann2017LearnedIT	Method
.	O

Textual	Task
entailment	Task
Textual	Task
entailment	Task
is	O
the	O
task	O
of	O
determining	O
whether	O
a	O
“	O
hypothesis	O
”	O
is	O
true	O
,	O
given	O
a	O
“	O
premise	O
”	O
.	O

The	O
Stanford	Task
Natural	Task
Language	Task
Inference	Task
(	O
SNLI	Material
)	Material
corpus	Material
snliemnlp2015	Material
provides	O
approximately	O
550	O
K	O
hypothesis	O
/	O
premise	O
pairs	O
.	O

Our	O
baseline	O
,	O
the	O
ESIM	Method
sequence	Method
model	Method
from	O
Chen2017EnhancedLF	Method
,	O
uses	O
a	O
biLSTM	Method
to	O
encode	O
the	O
premise	O
and	O
hypothesis	O
,	O
followed	O
by	O
a	O
matrix	Method
attention	Method
layer	Method
,	O
a	O
local	Method
inference	Method
layer	Method
,	O
another	O
biLSTM	Method
inference	Method
composition	Method
layer	Method
,	O
and	O
finally	O
a	O
pooling	Method
operation	Method
before	O
the	O
output	O
layer	O
.	O

Overall	O
,	O
adding	O
ELMo	Method
to	O
the	O
ESIM	Method
model	Method
improves	O
accuracy	Metric
by	O
an	O
average	O
of	O
0.7	O
%	O
across	O
five	O
random	O
seeds	O
.	O

A	O
five	O
member	O
ensemble	O
pushes	O
the	O
overall	O
accuracy	Metric
to	O
89.3	O
%	O
,	O
exceeding	O
the	O
previous	O
ensemble	O
best	O
of	O
88.9	O
%	O
Gong2017NaturalLI	Method
.	O

Semantic	Task
role	Task
labeling	Task
A	O
semantic	Method
role	Method
labeling	Method
(	O
SRL	Method
)	Method
system	Method
models	O
the	O
predicate	O
-	O
argument	O
structure	O
of	O
a	O
sentence	O
,	O
and	O
is	O
often	O
described	O
as	O
answering	O
“	O
Who	O
did	O
what	O
to	O
whom	O
”	O
.	O

He2017DeepSR	O
modeled	O
SRL	Method
as	O
a	O
BIO	Task
tagging	Task
problem	Task
and	O
used	O
an	O
8	Method
-	Method
layer	Method
deep	Method
biLSTM	Method
with	O
forward	O
and	O
backward	O
directions	O
interleaved	O
,	O
following	O
Zhou2015EndtoendLO	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
when	O
adding	O
ELMo	Method
to	O
a	O
re	O
-	O
implementation	O
of	O
He2017DeepSR	Method
the	O
single	O
model	Metric
test	Metric
set	Metric
F	Metric
jumped	O
3.2	O
%	O
from	O
81.4	O
%	O
to	O
84.6	O
%	O
–	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
OntoNotes	Material
benchmark	Material
Pradhan2013TowardsRL	Material
,	O
even	O
improving	O
over	O
the	O
previous	O
best	O
ensemble	O
result	O
by	O
1.2	O
%	O
.	O

Coreference	Task
resolution	Task
Coreference	Task
resolution	Task
is	O
the	O
task	O
of	O
clustering	Task
mentions	Task
in	O
text	O
that	O
refer	O
to	O
the	O
same	O
underlying	O
real	O
world	O
entities	O
.	O

Our	O
baseline	O
model	O
is	O
the	O
end	Method
-	Method
to	Method
-	Method
end	Method
span	Method
-	Method
based	Method
neural	Method
model	Method
of	O
Lee2017EndtoendNC	Method
.	O

It	O
uses	O
a	O
biLSTM	Method
and	Method
attention	Method
mechanism	Method
to	O
first	O
compute	O
span	Method
representations	Method
and	O
then	O
applies	O
a	O
softmax	Method
mention	Method
ranking	Method
model	Method
to	O
find	O
coreference	O
chains	O
.	O

In	O
our	O
experiments	O
with	O
the	O
OntoNotes	Material
coreference	Material
annotations	Material
from	O
the	O
CoNLL	Material
2012	Material
shared	Material
task	Material
Pradhan2012CoNLL2012ST	Material
,	O
adding	O
ELMo	Method
improved	O
the	O
average	Metric
F	Metric
by	O
3.2	O
%	O
from	O
67.2	O
to	O
70.4	O
,	O
establishing	O
a	O
new	O
state	O
of	O
the	O
art	O
,	O
again	O
improving	O
over	O
the	O
previous	O
best	O
ensemble	O
result	O
by	O
1.6	O
%	O
F	Metric
.	O

Named	Task
entity	Task
extraction	Task
The	O
CoNLL	Task
2003	Task
NER	Task
task	Task
CoNLL2003NER	Material
consists	O
of	O
newswire	Material
from	O
the	O
Reuters	Material
RCV1	Material
corpus	Material
tagged	O
with	O
four	O
different	O
entity	O
types	O
(	O
PER	O
,	O
LOC	O
,	O
ORG	O
,	O
MISC	O
)	O
.	O

Following	O
recent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
lample	O
-	O
EtAl:2016:N16	Method
-	Method
1	Method
,	Method
Peters2017SemisupervisedST	Method
,	O
the	O
baseline	O
model	O
uses	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
,	O
a	O
character	Method
-	Method
based	Method
CNN	Method
representation	Method
,	O
two	O
biLSTM	Method
layers	Method
and	O
a	O
conditional	Method
random	Method
field	Method
(	O
CRF	Method
)	Method
loss	Method
CRF	Method
:	O
Lafferty2001	Method
,	O
similar	O
to	O
NLPfromScratch	Method
:	O
Collobert2011	Method
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
our	O
ELMo	Method
enhanced	Method
biLSTM	Method
-	Method
CRF	Method
achieves	O
92.22	O
%	O
F	Metric
averaged	O
over	O
five	O
runs	O
.	O

The	O
key	O
difference	O
between	O
our	O
system	O
and	O
the	O
previous	O
state	O
of	O
the	O
art	O
from	O
Peters2017SemisupervisedST	Method
is	O
that	O
we	O
allowed	O
the	O
task	Method
model	Method
to	O
learn	O
a	O
weighted	Method
average	Method
of	Method
all	Method
biLM	Method
layers	Method
,	O
whereas	O
Peters2017SemisupervisedST	Method
only	O
use	O
the	O
top	O
biLM	O
layer	O
.	O

As	O
shown	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
using	O
all	O
layers	O
instead	O
of	O
just	O
the	O
last	O
layer	O
improves	O
performance	O
across	O
multiple	O
tasks	O
.	O

Sentiment	Task
analysis	Task
The	O
fine	Task
-	Task
grained	Task
sentiment	Task
classification	Task
task	Task
in	O
the	O
Stanford	Material
Sentiment	Material
Treebank	Material
[	O
SST	Material
-	Material
5;	Material
][]	Material
socher2013recursive	Material
involves	O
selecting	O
one	O
of	O
five	O
labels	O
(	O
from	O
very	O
negative	O
to	O
very	O
positive	O
)	O
to	O
describe	O
a	O
sentence	O
from	O
a	O
movie	Material
review	Material
.	O

The	O
sentences	O
contain	O
diverse	O
linguistic	O
phenomena	O
such	O
as	O
idioms	O
and	O
complex	O
syntactic	O
constructions	O
such	O
as	O
negations	O
that	O
are	O
difficult	O
for	O
models	O
to	O
learn	O
.	O

Our	O
baseline	O
model	O
is	O
the	O
biattentive	Method
classification	Method
network	Method
(	O
BCN	Method
)	O
from	O
McCann2017LearnedIT	Method
,	O
which	O
also	O
held	O
the	O
prior	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
when	O
augmented	O
with	O
CoVe	O
embeddings	O
.	O

Replacing	O
CoVe	Method
with	O
ELMo	Method
in	O
the	O
BCN	Method
model	Method
results	O
in	O
a	O
1.0	O
%	O
absolute	Metric
accuracy	Metric
improvement	O
over	O
the	O
state	O
of	O
the	O
art	O
.	O

Input	O
Only	O
Input	O
&	O
Output	O
Output	O
Only	O
section	O
:	O
Analysis	O
This	O
section	O
provides	O
an	O
ablation	Task
analysis	Task
to	O
validate	O
our	O
chief	O
claims	O
and	O
to	O
elucidate	O
some	O
interesting	O
aspects	O
of	O
ELMo	Method
representations	Method
.	O

Sec	O
.	O

[	O
reference	O
]	O
shows	O
that	O
using	O
deep	Method
contextual	Method
representations	Method
in	O
downstream	Task
tasks	Task
improves	O
performance	O
over	O
previous	O
work	O
that	O
uses	O
just	O
the	O
top	O
layer	O
,	O
regardless	O
of	O
whether	O
they	O
are	O
produced	O
from	O
a	O
biLM	Method
or	Method
MT	Method
encoder	Method
,	O
and	O
that	O
ELMo	Method
representations	Method
provide	O
the	O
best	O
overall	O
performance	O
.	O

Sec	O
.	O

[	O
reference	O
]	O
explores	O
the	O
different	O
types	O
of	O
contextual	O
information	O
captured	O
in	O
biLMs	Method
and	O
uses	O
two	O
intrinsic	O
evaluations	O
to	O
show	O
that	O
syntactic	O
information	O
is	O
better	O
represented	O
at	O
lower	O
layers	O
while	O
semantic	O
information	O
is	O
captured	O
a	O
higher	O
layers	O
,	O
consistent	O
with	O
MT	Method
encoders	Method
.	O

It	O
also	O
shows	O
that	O
our	O
biLM	Method
consistently	O
provides	O
richer	O
representations	O
then	O
CoVe	O
.	O

Additionally	O
,	O
we	O
analyze	O
the	O
sensitivity	O
to	O
where	O
ELMo	Method
is	O
included	O
in	O
the	O
task	Method
model	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
training	O
set	O
size	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
,	O
and	O
visualize	O
the	O
ELMo	O
learned	O
weights	O
across	O
the	O
tasks	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Alternate	Method
layer	Method
weighting	Method
schemes	Method
There	O
are	O
many	O
alternatives	O
to	O
Equation	O
[	O
reference	O
]	O
for	O
combining	O
the	O
biLM	Method
layers	Method
.	O

Previous	O
work	O
on	O
contextual	Method
representations	Method
used	O
only	O
the	O
last	O
layer	O
,	O
whether	O
it	O
be	O
from	O
a	O
biLM	Method
Peters2017SemisupervisedST	Method
or	O
an	O
MT	Method
encoder	Method
[	O
CoVe;	O
][]	O
McCann2017LearnedIT	O
.	O

The	O
choice	O
of	O
the	O
regularization	O
parameter	O
is	O
also	O
important	O
,	O
as	O
large	O
values	O
such	O
as	O
effectively	O
reduce	O
the	O
weighting	O
function	O
to	O
a	O
simple	O
average	O
over	O
the	O
layers	O
,	O
while	O
smaller	O
values	O
(	O
e.g.	O
,	O
)	O
allow	O
the	O
layer	O
weights	O
to	O
vary	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
these	O
alternatives	O
for	O
SQuAD	Method
,	O
SNLI	Method
and	O
SRL	Method
.	O

Including	O
representations	O
from	O
all	O
layers	O
improves	O
overall	O
performance	O
over	O
just	O
using	O
the	O
last	O
layer	O
,	O
and	O
including	O
contextual	Method
representations	Method
from	O
the	O
last	O
layer	O
improves	O
performance	O
over	O
the	O
baseline	O
.	O

For	O
example	O
,	O
in	O
the	O
case	O
of	O
SQuAD	O
,	O
using	O
just	O
the	O
last	O
biLM	Method
layer	Method
improves	O
development	Metric
F	Metric
by	O
3.9	O
%	O
over	O
the	O
baseline	O
.	O

Averaging	O
all	O
biLM	Method
layers	Method
instead	O
of	O
using	O
just	O
the	O
last	O
layer	O
improves	O
F	O
another	O
0.3	O
%	O
(	O
comparing	O
“	O
Last	O
Only	O
”	O
to	O
=	O
1	O
columns	O
)	O
,	O
and	O
allowing	O
the	O
task	Method
model	Method
to	O
learn	O
individual	O
layer	O
weights	O
improves	O
F	O
another	O
0.2	O
%	O
(	O
=	O
1	O
vs.	O
=	O
0.001	O
)	O
.	O

A	O
small	O
is	O
preferred	O
in	O
most	O
cases	O
with	O
ELMo	Method
,	O
although	O
for	O
NER	Task
,	O
a	O
task	O
with	O
a	O
smaller	O
training	O
set	O
,	O
the	O
results	O
are	O
insensitive	O
to	O
(	O
not	O
shown	O
)	O
.	O

The	O
overall	O
trend	O
is	O
similar	O
with	O
CoVe	O
but	O
with	O
smaller	O
increases	O
over	O
the	O
baseline	O
.	O

For	O
SNLI	Method
,	O
averaging	O
all	O
layers	O
with	O
=	O
1	O
improves	O
development	Metric
accuracy	Metric
from	O
88.2	O
to	O
88.7	O
%	O
over	O
using	O
just	O
the	O
last	O
layer	O
.	O

SRL	Metric
F	Metric
increased	O
a	O
marginal	O
0.1	O
%	O
to	O
82.2	O
for	O
the	O
=	O
1	O
case	O
compared	O
to	O
using	O
the	O
last	Method
layer	Method
only	O
.	O

subsection	O
:	O
Where	O
to	O
include	O
ELMo	Method
?	O
All	O
of	O
the	O
task	O
architectures	O
in	O
this	O
paper	O
include	O
word	O
embeddings	O
only	O
as	O
input	O
to	O
the	O
lowest	Method
layer	Method
biRNN	Method
.	O

However	O
,	O
we	O
find	O
that	O
including	O
ELMo	Method
at	O
the	O
output	O
of	O
the	O
biRNN	Method
in	O
task	Method
-	Method
specific	Method
architectures	Method
improves	O
overall	O
results	O
for	O
some	O
tasks	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
including	O
ELMo	Method
at	O
both	O
the	O
input	O
and	O
output	O
layers	O
for	O
SNLI	Method
and	O
SQuAD	Method
improves	O
over	O
just	O
the	O
input	O
layer	O
,	O
but	O
for	O
SRL	Method
(	O
and	O
coreference	Metric
resolution	Metric
,	O
not	O
shown	O
)	O
performance	O
is	O
highest	O
when	O
it	O
is	O
included	O
at	O
just	O
the	O
input	O
layer	O
.	O

One	O
possible	O
explanation	O
for	O
this	O
result	O
is	O
that	O
both	O
the	O
SNLI	Method
and	Method
SQuAD	Method
architectures	Method
use	O
attention	O
layers	O
after	O
the	O
biRNN	Method
,	O
so	O
introducing	O
ELMo	Method
at	O
this	O
layer	O
allows	O
the	O
model	O
to	O
attend	O
directly	O
to	O
the	O
biLM	Method
’s	Method
internal	Method
representations	Method
.	O

In	O
the	O
SRL	Task
case	Task
,	O
the	O
task	O
-	O
specific	O
context	O
representations	O
are	O
likely	O
more	O
important	O
than	O
those	O
from	O
the	O
biLM	Method
.	O

subsection	O
:	O
What	O
information	O
is	O
captured	O
by	O
the	O
biLM	Method
’s	Method
representations	Method
?	O
Since	O
adding	O
ELMo	Method
improves	O
task	Task
performance	O
over	O
word	O
vectors	O
alone	O
,	O
the	O
biLM	Method
’s	Method
contextual	Method
representations	Method
must	O
encode	O
information	O
generally	O
useful	O
for	O
NLP	Task
tasks	Task
that	O
is	O
not	O
captured	O
in	O
word	O
vectors	O
.	O

Intuitively	O
,	O
the	O
biLM	Method
must	O
be	O
disambiguating	O
the	O
meaning	O
of	O
words	O
using	O
their	O
context	O
.	O

Consider	O
“	O
play	O
”	O
,	O
a	O
highly	O
polysemous	O
word	O
.	O

The	O
top	O
of	O
Table	O
[	O
reference	O
]	O
lists	O
nearest	O
neighbors	O
to	O
“	O
play	O
”	O
using	O
GloVe	O
vectors	O
.	O

They	O
are	O
spread	O
across	O
several	O
parts	O
of	O
speech	O
(	O
e.g.	O
,	O
“	O
played	O
”	O
,	O
“	O
playing	O
”	O
as	O
verbs	O
,	O
and	O
“	O
player	O
”	O
,	O
“	O
game	O
”	O
as	O
nouns	O
)	O
but	O
concentrated	O
in	O
the	O
sports	O
-	O
related	O
senses	O
of	O
“	O
play	O
”	O
.	O

In	O
contrast	O
,	O
the	O
bottom	O
two	O
rows	O
show	O
nearest	O
neighbor	O
sentences	O
from	O
the	O
SemCor	Material
dataset	Material
(	O
see	O
below	O
)	O
using	O
the	O
biLM	Method
’s	Method
context	Method
representation	Method
of	O
“	O
play	O
”	O
in	O
the	O
source	O
sentence	O
.	O

In	O
these	O
cases	O
,	O
the	O
biLM	Method
is	O
able	O
to	O
disambiguate	O
both	O
the	O
part	O
of	O
speech	O
and	O
word	O
sense	O
in	O
the	O
source	O
sentence	O
.	O

These	O
observations	O
can	O
be	O
quantified	O
using	O
an	O
intrinsic	Method
evaluation	Method
of	Method
the	Method
contextual	Method
representations	Method
similar	O
to	O
Belinkov2017WhatDN	Method
.	O

To	O
isolate	O
the	O
information	O
encoded	O
by	O
the	O
biLM	Method
,	O
the	O
representations	O
are	O
used	O
to	O
directly	O
make	O
predictions	O
for	O
a	O
fine	Task
grained	Task
word	Task
sense	Task
disambiguation	Task
(	O
WSD	Task
)	Task
task	Task
and	O
a	O
POS	Task
tagging	Task
task	Task
.	O

Using	O
this	O
approach	O
,	O
it	O
is	O
also	O
possible	O
to	O
compare	O
to	O
CoVe	O
,	O
and	O
across	O
each	O
of	O
the	O
individual	O
layers	O
.	O

Word	Task
sense	Task
disambiguation	Task
Given	O
a	O
sentence	O
,	O
we	O
can	O
use	O
the	O
biLM	Method
representations	Method
to	O
predict	O
the	O
sense	O
of	O
a	O
target	O
word	O
using	O
a	O
simple	O
1	Method
-	Method
nearest	Method
neighbor	Method
approach	Method
,	O
similar	O
to	O
Melamud2016context2vecLG	Method
.	O

To	O
do	O
so	O
,	O
we	O
first	O
use	O
the	O
biLM	Method
to	O
compute	O
representations	O
for	O
all	O
words	O
in	O
SemCor	Material
3.0	Material
,	O
our	O
training	O
corpus	O
Miller1994UsingAS	O
,	O
and	O
then	O
take	O
the	O
average	Method
representation	Method
for	O
each	O
sense	O
.	O

At	O
test	O
time	O
,	O
we	O
again	O
use	O
the	O
biLM	Method
to	O
compute	O
representations	O
for	O
a	O
given	O
target	O
word	O
and	O
take	O
the	O
nearest	O
neighbor	O
sense	O
from	O
the	O
training	O
set	O
,	O
falling	O
back	O
to	O
the	O
first	O
sense	O
from	O
WordNet	O
for	O
lemmas	O
not	O
observed	O
during	O
training	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
WSD	Method
results	O
using	O
the	O
evaluation	O
framework	O
from	O
Raganato2017WordSD	O
across	O
the	O
same	O
suite	O
of	O
four	O
test	O
sets	O
in	O
Raganato2017NeuralSL	O
.	O

Overall	O
,	O
the	O
biLM	Method
top	Method
layer	Method
representations	Method
have	O
F	O
of	O
69.0	O
and	O
are	O
better	O
at	O
WSD	Method
then	O
the	O
first	O
layer	O
.	O

This	O
is	O
competitive	O
with	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
WSD	Method
-	Method
specific	Method
supervised	Method
model	Method
using	O
hand	Method
crafted	Method
features	Method
Iacobacci2016EmbeddingsFW	Method
and	O
a	O
task	Method
specific	Method
biLSTM	Method
that	O
is	O
also	O
trained	O
with	O
auxiliary	O
coarse	O
-	O
grained	O
semantic	O
labels	O
and	O
POS	O
tags	O
Raganato2017NeuralSL	O
.	O

The	O
CoVe	Method
biLSTM	Method
layers	Method
follow	O
a	O
similar	O
pattern	O
to	O
those	O
from	O
the	O
biLM	Method
(	O
higher	O
overall	O
performance	O
at	O
the	O
second	O
layer	O
compared	O
to	O
the	O
first	O
)	O
;	O
however	O
,	O
our	O
biLM	Method
outperforms	O
the	O
CoVe	Method
biLSTM	Method
,	O
which	O
trails	O
the	O
WordNet	Method
first	Method
sense	Method
baseline	Method
.	O

POS	Task
tagging	Task
To	O
examine	O
whether	O
the	O
biLM	Method
captures	O
basic	O
syntax	O
,	O
we	O
used	O
the	O
context	Method
representations	Method
as	O
input	O
to	O
a	O
linear	Method
classifier	Method
that	O
predicts	O
POS	O
tags	O
with	O
the	O
Wall	Material
Street	Material
Journal	Material
portion	Material
of	O
the	O
Penn	Material
Treebank	Material
(	O
PTB	Material
)	Material
Marcus1993BuildingAL	Material
.	O

As	O
the	O
linear	Method
classifier	Method
adds	O
only	O
a	O
small	O
amount	O
of	O
model	O
capacity	O
,	O
this	O
is	O
direct	O
test	O
of	O
the	O
biLM	Method
’s	Method
representations	Method
.	O

Similar	O
to	O
WSD	Method
,	O
the	O
biLM	Method
representations	Method
are	O
competitive	O
with	O
carefully	O
tuned	O
,	O
task	Method
specific	Method
biLSTMs	Method
Ling2015FindingFI	Method
,	O
Ma2016EndtoendSL	Method
.	O

However	O
,	O
unlike	O
WSD	Method
,	O
accuracies	Metric
using	O
the	O
first	Method
biLM	Method
layer	Method
are	O
higher	O
than	O
the	O
top	O
layer	O
,	O
consistent	O
with	O
results	O
from	O
deep	Method
biLSTMs	Method
in	O
multi	Task
-	Task
task	Task
training	Task
Sgaard2016DeepML	Task
,	O
joint	Task
-	Task
many	Task
-	Task
iclr07	Task
and	O
MT	Task
Belinkov2017WhatDN	Task
.	O

CoVe	Metric
POS	Metric
tagging	Metric
accuracies	Metric
follow	O
the	O
same	O
pattern	O
as	O
those	O
from	O
the	O
biLM	Method
,	O
and	O
just	O
like	O
for	O
WSD	Method
,	O
the	O
biLM	Method
achieves	O
higher	O
accuracies	Metric
than	O
the	O
CoVe	Method
encoder	Method
.	O

Implications	O
for	O
supervised	Task
tasks	Task
Taken	O
together	O
,	O
these	O
experiments	O
confirm	O
different	O
layers	O
in	O
the	O
biLM	Method
represent	O
different	O
types	O
of	O
information	O
and	O
explain	O
why	O
including	O
all	O
biLM	Method
layers	Method
is	O
important	O
for	O
the	O
highest	O
performance	O
in	O
downstream	Task
tasks	Task
.	O

In	O
addition	O
,	O
the	O
biLM	Method
’s	Method
representations	Method
are	O
more	O
transferable	O
to	O
WSD	Task
and	O
POS	Task
tagging	Task
than	O
those	O
in	O
CoVe	Method
,	O
helping	O
to	O
illustrate	O
why	O
ELMo	Method
outperforms	O
CoVe	Method
in	O
downstream	Task
tasks	Task
.	O

subsection	O
:	O
Sample	Metric
efficiency	Metric
Adding	O
ELMo	Method
to	O
a	O
model	O
increases	O
the	O
sample	Metric
efficiency	Metric
considerably	O
,	O
both	O
in	O
terms	O
of	O
number	O
of	O
parameter	O
updates	O
to	O
reach	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
and	O
the	O
overall	O
training	Metric
set	Metric
size	Metric
.	O

For	O
example	O
,	O
the	O
SRL	Method
model	Method
reaches	O
a	O
maximum	O
development	Metric
F	Metric
after	O
486	O
epochs	O
of	O
training	O
without	O
ELMo	Method
.	O

After	O
adding	O
ELMo	Method
,	O
the	O
model	O
exceeds	O
the	O
baseline	O
maximum	O
at	O
epoch	O
10	O
,	O
a	O
98	O
%	O
relative	O
decrease	O
in	O
the	O
number	O
of	O
updates	O
needed	O
to	O
reach	O
the	O
same	O
level	O
of	O
performance	O
.	O

In	O
addition	O
,	O
ELMo	Method
-	Method
enhanced	Method
models	Method
use	O
smaller	O
training	O
sets	O
more	O
efficiently	O
than	O
models	O
without	O
ELMo	Method
.	O

Figure	O
1	O
compares	O
the	O
performance	O
of	O
baselines	O
models	O
with	O
and	O
without	O
ELMo	Method
as	O
the	O
percentage	O
of	O
the	O
full	O
training	O
set	O
is	O
varied	O
from	O
0.1	O
%	O
to	O
100	O
%	O
.	O

Improvements	O
with	O
ELMo	Method
are	O
largest	O
for	O
smaller	O
training	O
sets	O
and	O
significantly	O
reduce	O
the	O
amount	O
of	O
training	O
data	O
needed	O
to	O
reach	O
a	O
given	O
level	O
of	O
performance	O
.	O

In	O
the	O
SRL	Task
case	Task
,	O
the	O
ELMo	Method
model	Method
with	O
1	O
%	O
of	O
the	O
training	O
set	O
has	O
about	O
the	O
same	O
F	Metric
as	O
the	O
baseline	O
model	O
with	O
10	O
%	O
of	O
the	O
training	O
set	O
.	O

subsection	O
:	O
Visualization	O
of	O
learned	O
weights	O
Figure	O
2	O
visualizes	O
the	O
softmax	O
-	O
normalized	O
learned	O
layer	O
weights	O
.	O

At	O
the	O
input	O
layer	O
,	O
the	O
task	Method
model	Method
favors	O
the	O
first	O
biLSTM	Method
layer	Method
.	O

For	O
coreference	Task
and	O
SQuAD	Task
,	O
the	O
this	O
is	O
strongly	O
favored	O
,	O
but	O
the	O
distribution	O
is	O
less	O
peaked	O
for	O
the	O
other	O
tasks	O
.	O

The	O
output	O
layer	O
weights	O
are	O
relatively	O
balanced	O
,	O
with	O
a	O
slight	O
preference	O
for	O
the	O
lower	O
layers	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
a	O
general	O
approach	O
for	O
learning	O
high	O
-	O
quality	Task
deep	Task
context	Task
-	Task
dependent	Task
representations	Task
from	O
biLMs	Method
,	O
and	O
shown	O
large	O
improvements	O
when	O
applying	O
ELMo	Method
to	O
a	O
broad	O
range	O
of	O
NLP	Task
tasks	Task
.	O

Through	O
ablations	O
and	O
other	O
controlled	O
experiments	O
,	O
we	O
have	O
also	O
confirmed	O
that	O
the	O
biLM	Method
layers	Method
efficiently	O
encode	O
different	O
types	O
of	O
syntactic	O
and	O
semantic	O
information	O
about	O
words	O
-	O
in	O
-	O
context	O
,	O
and	O
that	O
using	O
all	O
layers	O
improves	O
overall	O
task	O
performance	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplemental	O
Material	O
to	O
accompany	O
Deep	Method
contextualized	Method
word	Method
representations	Method
This	O
supplement	O
contains	O
details	O
of	O
the	O
model	Method
architectures	Method
,	O
training	Method
routines	Method
and	O
hyper	O
-	O
parameter	O
choices	O
for	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
Section	O
[	O
reference	O
]	O
.	O

All	O
of	O
the	O
individual	O
models	O
share	O
a	O
common	O
architecture	O
in	O
the	O
lowest	O
layers	O
with	O
a	O
context	Method
independent	Method
token	Method
representation	Method
below	O
several	O
layers	O
of	O
stacked	Method
RNNs	Method
–	O
LSTMs	Method
in	O
every	O
case	O
except	O
the	O
SQuAD	Method
model	Method
that	O
uses	O
GRUs	Method
.	O

subsection	O
:	O
Fine	Task
tuning	Task
biLM	Task
As	O
noted	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
fine	O
tuning	O
the	O
biLM	Method
on	O
task	Material
specific	Material
data	Material
typically	O
resulted	O
in	O
significant	O
drops	O
in	O
perplexity	Metric
.	O

To	O
fine	O
tune	O
on	O
a	O
given	O
task	O
,	O
the	O
supervised	O
labels	O
were	O
temporarily	O
ignored	O
,	O
the	O
biLM	Method
fine	O
tuned	O
for	O
one	O
epoch	O
on	O
the	O
training	O
split	O
and	O
evaluated	O
on	O
the	O
development	O
split	O
.	O

Once	O
fine	O
tuned	O
,	O
the	O
biLM	O
weights	O
were	O
fixed	O
during	O
task	Task
training	Task
.	O

Table	O
[	O
reference	O
]	O
lists	O
the	O
development	O
set	O
perplexities	O
for	O
the	O
considered	O
tasks	O
.	O

In	O
every	O
case	O
except	O
CoNLL	Material
2012	Material
,	O
fine	Task
tuning	Task
results	O
in	O
a	O
large	O
improvement	O
in	O
perplexity	Metric
,	O
e.g.	O
,	O
from	O
72.1	O
to	O
16.8	O
for	O
SNLI	Method
.	O

The	O
impact	O
of	O
fine	Task
tuning	Task
on	O
supervised	Task
performance	Task
is	O
task	O
dependent	O
.	O

In	O
the	O
case	O
of	O
SNLI	Method
,	O
fine	O
tuning	O
the	O
biLM	Method
increased	O
development	Metric
accuracy	Metric
0.6	O
%	O
from	O
88.9	O
%	O
to	O
89.5	O
%	O
for	O
our	O
single	O
best	O
model	O
.	O

However	O
,	O
for	O
sentiment	Task
classification	Task
development	O
set	O
accuracy	Metric
is	O
approximately	O
the	O
same	O
regardless	O
whether	O
a	O
fine	Method
tuned	Method
biLM	Method
was	O
used	O
.	O

subsection	O
:	O
Importance	O
of	O
in	O
Eqn	O
.	O

(	O
1	O
)	O
The	O
parameter	O
in	O
Eqn	O
.	O

(	O
[	O
reference	O
]	O
)	O
was	O
of	O
practical	O
importance	O
to	O
aid	O
optimization	Task
,	O
due	O
to	O
the	O
different	O
distributions	O
between	O
the	O
biLM	Method
internal	Method
representations	Method
and	O
the	O
task	Method
specific	Method
representations	Method
.	O

It	O
is	O
especially	O
important	O
in	O
the	O
last	O
-	O
only	O
case	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

Without	O
this	O
parameter	O
,	O
the	O
last	O
-	O
only	O
case	O
performed	O
poorly	O
(	O
well	O
below	O
the	O
baseline	O
)	O
for	O
SNLI	Method
and	O
training	Task
failed	O
completely	O
for	O
SRL	Method
.	O

subsection	O
:	O
Textual	Task
Entailment	Task
Our	O
baseline	Method
SNLI	Method
model	Method
is	O
the	O
ESIM	Method
sequence	Method
model	Method
from	O
Chen2017EnhancedLF	Method
.	O

Following	O
the	O
original	O
implementation	O
,	O
we	O
used	O
300	O
dimensions	O
for	O
all	O
LSTM	Method
and	Method
feed	Method
forward	Method
layers	Method
and	O
pre	O
-	O
trained	O
300	Method
dimensional	Method
GloVe	Method
embeddings	Method
that	O
were	O
fixed	O
during	O
training	O
.	O

For	O
regularization	Task
,	O
we	O
added	O
50	O
%	O
variational	Method
dropout	Method
Gal2016ATG	Method
to	O
the	O
input	O
of	O
each	O
LSTM	Method
layer	Method
and	O
50	O
%	O
dropout	Method
Srivastava2014DropoutAS	O
at	O
the	O
input	O
to	O
the	O
final	O
two	O
fully	Method
connected	Method
layers	Method
.	O

All	O
feed	Method
forward	Method
layers	Method
use	O
ReLU	Method
activations	Method
.	O

Parameters	O
were	O
optimized	O
using	O
Adam	Method
Kingma2014AdamAM	Method
with	O
gradient	O
norms	O
clipped	O
at	O
5.0	O
and	O
initial	O
learning	Metric
rate	Metric
0.0004	O
,	O
decreasing	O
by	O
half	O
each	O
time	O
accuracy	Metric
on	O
the	O
development	O
set	O
did	O
not	O
increase	O
in	O
subsequent	O
epochs	O
.	O

The	O
batch	O
size	O
was	O
32	O
.	O

The	O
best	O
ELMo	Method
configuration	Method
added	O
ELMo	O
vectors	O
to	O
both	O
the	O
input	O
and	O
output	O
of	O
the	O
lowest	Method
layer	Method
LSTM	Method
,	O
using	O
(	O
[	O
reference	O
]	O
)	O
with	O
layer	O
normalization	O
and	O
.	O

Due	O
to	O
the	O
increased	O
number	O
of	O
parameters	O
in	O
the	O
ELMo	Method
model	Method
,	O
we	O
added	O
regularization	O
with	O
regularization	O
coefficient	O
0.0001	O
to	O
all	O
recurrent	O
and	O
feed	O
forward	O
weight	O
matrices	O
and	O
50	O
%	O
dropout	O
after	O
the	O
attention	Method
layer	Method
.	O

Table	O
[	O
reference	O
]	O
compares	O
test	O
set	O
accuracy	Metric
of	O
our	O
system	O
to	O
previously	O
published	O
systems	O
.	O

Overall	O
,	O
adding	O
ELMo	Method
to	O
the	O
ESIM	Method
model	Method
improved	O
accuracy	Metric
by	O
0.7	O
%	O
establishing	O
a	O
new	O
single	O
model	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
of	O
88.7	O
%	O
,	O
and	O
a	O
five	O
member	Method
ensemble	Method
pushes	O
the	O
overall	O
accuracy	Metric
to	O
89.3	O
%	O
.	O

subsection	O
:	O
Question	Task
Answering	Task
Our	O
QA	Method
model	Method
is	O
a	O
simplified	O
version	O
of	O
the	O
model	O
from	O
ClarkAdvancingRC	Method
.	O

It	O
embeds	O
tokens	O
by	O
concatenating	O
each	O
token	O
’s	O
case	O
-	O
sensitive	O
300	O
dimensional	O
GloVe	O
word	O
vector	O
Pennington2014GloveGV	O
with	O
a	O
character	Method
-	Method
derived	Method
embedding	Method
produced	O
using	O
a	O
convolutional	Method
neural	Method
network	Method
followed	O
by	O
max	Method
-	Method
pooling	Method
on	O
learned	O
character	Method
embeddings	Method
.	O

The	O
token	O
embeddings	O
are	O
passed	O
through	O
a	O
shared	Method
bi	Method
-	Method
directional	Method
GRU	Method
,	O
and	O
then	O
the	O
bi	Method
-	Method
directional	Method
attention	Method
mechanism	Method
from	O
BiDAF	Method
.	O

The	O
augmented	O
context	O
vectors	O
are	O
then	O
passed	O
through	O
a	O
linear	Method
layer	Method
with	O
ReLU	Method
activations	Method
,	O
a	O
residual	Method
self	Method
-	Method
attention	Method
layer	Method
that	O
uses	O
a	O
GRU	Method
followed	O
by	O
the	O
same	O
attention	Method
mechanism	Method
applied	O
context	O
-	O
to	O
-	O
context	O
,	O
and	O
another	O
linear	Method
layer	Method
with	O
ReLU	Method
activations	Method
.	O

Finally	O
,	O
the	O
results	O
are	O
fed	O
through	O
linear	Method
layers	Method
to	O
predict	O
the	O
start	O
and	O
end	O
token	O
of	O
the	O
answer	O
.	O

Variational	Method
dropout	Method
is	O
used	O
before	O
the	O
input	O
to	O
the	O
GRUs	O
and	O
the	O
linear	Method
layers	Method
at	O
a	O
rate	O
of	O
0.2	O
.	O

A	O
dimensionality	O
of	O
90	O
is	O
used	O
for	O
the	O
GRUs	Method
,	O
and	O
180	O
for	O
the	O
linear	Method
layers	Method
.	O

We	O
optimize	O
the	O
model	O
using	O
Adadelta	Method
with	O
a	O
batch	O
size	O
of	O
45	O
.	O

At	O
test	O
time	O
we	O
use	O
an	O
exponential	Method
moving	Method
average	Method
of	O
the	O
weights	O
and	O
limit	O
the	O
output	O
span	O
to	O
be	O
of	O
at	O
most	O
size	O
17	O
.	O

We	O
do	O
not	O
update	O
the	O
word	O
vectors	O
during	O
training	O
.	O

Performance	O
was	O
highest	O
when	O
adding	O
ELMo	Method
without	Method
layer	Method
normalization	Method
to	O
both	O
the	O
input	O
and	O
output	O
of	O
the	O
contextual	Method
GRU	Method
layer	Method
and	O
leaving	O
the	O
ELMo	O
weights	O
unregularized	O
(	O
)	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
test	O
set	O
results	O
from	O
the	O
SQuAD	Material
leaderboard	Material
as	O
of	O
November	O
17	O
,	O
2017	O
when	O
we	O
submitted	O
our	O
system	O
.	O

Overall	O
,	O
our	O
submission	O
had	O
the	O
highest	O
single	O
model	O
and	O
ensemble	Task
results	O
,	O
improving	O
the	O
previous	O
single	O
model	O
result	O
(	O
SAN	Method
)	O
by	O
1.4	O
%	O
F	Metric
and	O
our	O
baseline	O
by	O
4.2	O
%	O
.	O

A	O
11	O
member	O
ensemble	O
pushes	O
F	O
to	O
87.4	O
%	O
,	O
1.0	O
%	O
increase	O
over	O
the	O
previous	O
ensemble	O
best	O
.	O

subsection	O
:	O
Semantic	Task
Role	Task
Labeling	Task
Our	O
baseline	O
SRL	Method
model	Method
is	O
an	O
exact	O
reimplementation	O
of	O
He2017DeepSR	Method
.	O

Words	O
are	O
represented	O
using	O
a	O
concatenation	Method
of	Method
100	Method
dimensional	Method
vector	Method
representations	Method
,	O
initialized	O
using	O
GloVe	Method
Pennington2014GloveGV	Method
and	O
a	O
binary	O
,	O
per	O
-	O
word	O
predicate	O
feature	O
,	O
represented	O
using	O
an	O
100	Method
dimensional	Method
embedding	Method
.	O

This	O
200	O
dimensional	Method
token	Method
representation	Method
is	O
then	O
passed	O
through	O
an	O
8	O
layer	O
“	O
interleaved	O
”	O
biLSTM	Method
with	O
a	O
300	O
dimensional	O
hidden	O
size	O
,	O
in	O
which	O
the	O
directions	O
of	O
the	O
LSTM	Method
layers	Method
alternate	O
per	O
layer	O
.	O

This	O
deep	Method
LSTM	Method
uses	O
Highway	Method
connections	Method
Srivastava2015TrainingVD	O
between	O
layers	Method
and	O
variational	Method
recurrent	Method
dropout	Method
Gal2016ATG	Method
.	O

This	O
deep	Method
representation	Method
is	O
then	O
projected	O
using	O
a	O
final	O
dense	Method
layer	Method
followed	O
by	O
a	O
softmax	Method
activation	Method
to	O
form	O
a	O
distribution	O
over	O
all	O
possible	O
tags	O
.	O

Labels	O
consist	O
of	O
semantic	O
roles	O
from	O
PropBank	Method
Palmer2005propbank	Method
augmented	O
with	O
a	O
BIO	Method
labeling	Method
scheme	Method
to	O
represent	O
argument	O
spans	O
.	O

During	O
training	O
,	O
we	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
tag	O
sequence	O
using	O
Adadelta	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
1.0	O
and	O
Zeiler2012ADADELTAAA	Method
.	O

At	O
test	O
time	O
,	O
we	O
perform	O
Viterbi	Method
decoding	Method
to	O
enforce	O
valid	O
spans	O
using	O
BIO	O
constraints	O
.	O

Variational	Method
dropout	Method
of	O
10	O
%	O
is	O
added	O
to	O
all	O
LSTM	Method
hidden	Method
layers	Method
.	O

Gradients	O
are	O
clipped	O
if	O
their	O
value	O
exceeds	O
1.0	O
.	O

Models	O
are	O
trained	O
for	O
500	O
epochs	O
or	O
until	O
validation	Metric
F1	Metric
does	O
not	O
improve	O
for	O
200	O
epochs	O
,	O
whichever	O
is	O
sooner	O
.	O

The	O
pretrained	O
GloVe	O
vectors	O
are	O
fine	O
-	O
tuned	O
during	O
training	O
.	O

The	O
final	O
dense	O
layer	O
and	O
all	O
cells	O
of	O
all	O
LSTMs	Method
are	O
initialized	O
to	O
be	O
orthogonal	O
.	O

The	O
forget	O
gate	O
bias	O
is	O
initialized	O
to	O
1	O
for	O
all	O
LSTMs	Method
,	O
with	O
all	O
other	O
gates	O
initialized	O
to	O
0	O
,	O
as	O
per	O
Jzefowicz2015AnEE	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
test	O
set	O
F1	Metric
scores	Metric
of	O
our	O
ELMo	Method
augmented	Method
implementation	Method
of	O
He2017DeepSR	Method
with	O
previous	O
results	O
.	O

Our	O
single	Metric
model	Metric
score	Metric
of	O
84.6	O
F1	Metric
represents	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
the	O
CONLL	Task
2012	Task
Semantic	Task
Role	Task
Labeling	Task
task	Task
,	O
surpassing	O
the	O
previous	O
single	O
model	O
result	O
by	O
2.9	O
F1	Metric
and	O
a	O
5	Method
-	Method
model	Method
ensemble	Method
by	O
1.2	O
F1	Method
.	O

subsection	O
:	O
Coreference	Task
resolution	Task
Our	O
baseline	Method
coreference	Method
model	Method
is	O
the	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
model	Method
from	O
Lee2017EndtoendNC	Method
with	O
all	O
hyperparameters	O
exactly	O
following	O
the	O
original	O
implementation	O
.	O

The	O
best	O
configuration	O
added	O
ELMo	Method
to	O
the	O
input	O
of	O
the	O
lowest	Method
layer	Method
biLSTM	Method
and	O
weighted	O
the	O
biLM	Method
layers	Method
using	O
(	O
[	O
reference	O
]	O
)	O
without	O
any	O
regularization	Method
(	O
)	O
or	O
layer	Method
normalization	Method
.	O

50	O
%	O
dropout	O
was	O
added	O
to	O
the	O
ELMo	Method
representations	Method
.	O

Table	O
[	O
reference	O
]	O
compares	O
our	O
results	O
with	O
previously	O
published	O
results	O
.	O

Overall	O
,	O
we	O
improve	O
the	O
single	O
model	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
3.2	O
%	O
average	Metric
F	Metric
,	O
and	O
our	O
single	O
model	O
result	O
improves	O
the	O
previous	O
ensemble	O
best	O
by	O
1.6	O
%	O
F	Metric
.	O

Adding	O
ELMo	Method
to	O
the	O
output	O
from	O
the	O
biLSTM	Method
in	O
addition	O
to	O
the	O
biLSTM	O
input	O
reduced	O
F	O
by	O
approximately	O
0.7	O
%	O
(	O
not	O
shown	O
)	O
.	O

subsection	O
:	O
Named	Task
Entity	Task
Recognition	Task
Our	O
baseline	O
NER	Method
model	Method
concatenates	O
50	O
dimensional	O
pre	O
-	O
trained	O
Senna	O
vectors	O
NLPfromScratch	O
:	O
Collobert2011	Method
with	O
a	O
CNN	Method
character	Method
based	Method
representation	Method
.	O

The	O
character	Method
representation	Method
uses	O
16	O
dimensional	Method
character	Method
embeddings	Method
and	O
128	O
convolutional	Method
filters	Method
of	O
width	O
three	O
characters	O
,	O
a	O
ReLU	Method
activation	Method
and	O
by	O
max	Method
pooling	Method
.	O

The	O
token	Method
representation	Method
is	O
passed	O
through	O
two	O
biLSTM	Method
layers	Method
,	O
the	O
first	O
with	O
200	O
hidden	O
units	O
and	O
the	O
second	O
with	O
100	O
hidden	O
units	O
before	O
a	O
final	O
dense	Method
layer	Method
and	O
softmax	Method
layer	Method
.	O

During	O
training	O
,	O
we	O
use	O
a	O
CRF	Method
loss	Method
and	O
at	O
test	O
time	O
perform	O
decoding	Task
using	O
the	O
Viterbi	Method
algorithm	Method
while	O
ensuring	O
that	O
the	O
output	O
tag	O
sequence	O
is	O
valid	O
.	O

Variational	Method
dropout	Method
is	O
added	O
to	O
the	O
input	O
of	O
both	O
biLSTM	Method
layers	Method
.	O

During	O
training	O
the	O
gradients	O
are	O
rescaled	O
if	O
their	O
norm	O
exceeds	O
5.0	O
and	O
parameters	O
updated	O
using	O
Adam	Method
with	O
constant	O
learning	Metric
rate	Metric
of	O
0.001	O
.	O

The	O
pre	O
-	O
trained	O
Senna	Method
embeddings	Method
are	O
fine	O
tuned	O
during	O
training	O
.	O

We	O
employ	O
early	Method
stopping	Method
on	O
the	O
development	O
set	O
and	O
report	O
the	O
averaged	Metric
test	Metric
set	Metric
score	Metric
across	O
five	O
runs	O
with	O
different	O
random	O
seeds	O
.	O

ELMo	Method
was	O
added	O
to	O
the	O
input	O
of	O
the	O
lowest	Method
layer	Method
task	Method
biLSTM	Method
.	O

As	O
the	O
CoNLL	Material
2003	Material
NER	Material
data	Material
set	Material
is	O
relatively	O
small	O
,	O
we	O
found	O
the	O
best	O
performance	O
by	O
constraining	O
the	O
trainable	O
layer	O
weights	O
to	O
be	O
effectively	O
constant	O
by	O
setting	O
with	O
(	O
[	O
reference	O
]	O
)	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
test	O
set	O
F	Metric
scores	Metric
of	O
our	O
ELMo	Method
enhanced	Method
biLSTM	Method
-	Method
CRF	Method
tagger	Method
with	O
previous	O
results	O
.	O

Overall	O
,	O
the	O
92.22	O
%	O
F	Metric
from	O
our	O
system	O
establishes	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

When	O
compared	O
to	O
Peters2017SemisupervisedST	Method
,	O
using	O
representations	O
from	O
all	O
layers	O
of	O
the	O
biLM	Method
provides	O
a	O
modest	O
improvement	O
.	O

subsection	O
:	O
Sentiment	Task
classification	Task
We	O
use	O
almost	O
the	O
same	O
biattention	Method
classification	Method
network	Method
architecture	Method
described	O
in	O
McCann2017LearnedIT	Method
,	O
with	O
the	O
exception	O
of	O
replacing	O
the	O
final	O
maxout	Method
network	Method
with	O
a	O
simpler	O
feedforward	Method
network	Method
composed	O
of	O
two	O
ReLu	Method
layers	Method
with	O
dropout	Method
.	O

A	O
BCN	Method
model	Method
with	O
a	O
batch	Method
-	Method
normalized	Method
maxout	Method
network	Method
reached	O
significantly	O
lower	O
validation	Metric
accuracies	Metric
in	O
our	O
experiments	O
,	O
although	O
there	O
may	O
be	O
discrepancies	O
between	O
our	O
implementation	O
and	O
that	O
of	O
McCann2017LearnedIT	Method
.	O

To	O
match	O
the	O
CoVe	Task
training	Task
setup	Task
,	O
we	O
only	O
train	O
on	O
phrases	O
that	O
contain	O
four	O
or	O
more	O
tokens	O
.	O

We	O
use	O
300	O
-	O
d	O
hidden	O
states	O
for	O
the	O
biLSTM	Method
and	O
optimize	O
the	O
model	O
parameters	O
with	O
Adam	Method
using	O
a	O
learning	Metric
rate	Metric
of	O
0.0001	O
.	O

The	O
trainable	O
biLM	O
layer	O
weights	O
are	O
regularized	O
by	O
,	O
and	O
we	O
add	O
ELMo	Method
to	O
both	O
the	O
input	O
and	O
output	O
of	O
the	O
biLSTM	Method
;	O
the	O
output	O
ELMo	O
vectors	O
are	O
computed	O
with	O
a	O
second	O
biLSTM	Method
and	O
concatenated	O
to	O
the	O
input	O
.	O

