document	O
:	O
Bi	Method
-	Method
Directional	Method
Attention	Method
Flow	Method
for	O
Machine	Task
Comprehension	Task
Machine	Task
comprehension	Task
(	O
MC	Task
)	O
,	O
answering	O
a	O
query	O
about	O
a	O
given	O
context	O
paragraph	O
,	O
requires	O
modeling	O
complex	O
interactions	O
between	O
the	O
context	O
and	O
the	O
query	O
.	O

Recently	O
,	O
attention	Method
mechanisms	Method
have	O
been	O
successfully	O
extended	O
to	O
MC	Task
.	O

Typically	O
these	O
methods	O
use	O
attention	O
to	O
focus	O
on	O
a	O
small	O
portion	O
of	O
the	O
context	O
and	O
summarize	O
it	O
with	O
a	O
fixed	O
-	O
size	O
vector	O
,	O
couple	O
attentions	O
temporally	O
,	O
and	O
/	O
or	O
often	O
form	O
a	O
uni	O
-	O
directional	O
attention	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
the	O
Bi	Method
-	Method
Directional	Method
Attention	Method
Flow	Method
(	O
BiDAF	O
)	O
network	O
,	O
a	O
multi	Method
-	Method
stage	Method
hierarchical	Method
process	Method
that	O
represents	O
the	O
context	O
at	O
different	O
levels	O
of	O
granularity	O
and	O
uses	O
bi	Method
-	Method
directional	Method
attention	Method
flow	Method
mechanism	Method
to	O
obtain	O
a	O
query	Method
-	Method
aware	Method
context	Method
representation	Method
without	O
early	Task
summarization	Task
.	O

Our	O
experimental	O
evaluations	O
show	O
that	O
our	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
and	O
CNN	Material
/	Material
DailyMail	Material
cloze	Material
test	Material
.	O

section	O
:	O
Introduction	O
The	O
tasks	O
of	O
machine	Task
comprehension	Task
(	Task
MC	Task
)	Task
and	O
question	Task
answering	Task
(	Task
QA	Task
)	O
have	O
gained	O
significant	O
popularity	O
over	O
the	O
past	O
few	O
years	O
within	O
the	O
natural	Task
language	Task
processing	Task
and	O
computer	Task
vision	Task
communities	Task
.	O

Systems	O
trained	O
end	O
-	O
to	O
-	O
end	O
now	O
achieve	O
promising	O
results	O
on	O
a	O
variety	O
of	O
tasks	O
in	O
the	O
text	O
and	O
image	O
domains	O
.	O

One	O
of	O
the	O
key	O
factors	O
to	O
the	O
advancement	O
has	O
been	O
the	O
use	O
of	O
neural	Method
attention	Method
mechanism	Method
,	O
which	O
enables	O
the	O
system	O
to	O
focus	O
on	O
a	O
targeted	O
area	O
within	O
a	O
context	O
paragraph	O
(	O
for	O
MC	Task
)	O
or	O
within	O
an	O
image	O
(	O
for	O
Visual	Task
QA	Task
)	O
,	O
that	O
is	O
most	O
relevant	O
to	O
answer	O
the	O
question	O
memnn	O
,	O
antol2015vqa	O
,	O
xiong2016dynamic	O
.	O

Attention	Method
mechanisms	Method
in	O
previous	O
works	O
typically	O
have	O
one	O
or	O
more	O
of	O
the	O
following	O
characteristics	O
.	O

First	O
,	O
the	O
computed	O
attention	O
weights	O
are	O
often	O
used	O
to	O
extract	O
the	O
most	O
relevant	O
information	O
from	O
the	O
context	O
for	O
answering	O
the	O
question	O
by	O
summarizing	O
the	O
context	O
into	O
a	O
fixed	O
-	O
size	O
vector	O
.	O

Second	O
,	O
in	O
the	O
text	O
domain	O
,	O
they	O
are	O
often	O
temporally	O
dynamic	O
,	O
whereby	O
the	O
attention	O
weights	O
at	O
the	O
current	O
time	O
step	O
are	O
a	O
function	O
of	O
the	O
attended	O
vector	O
at	O
the	O
previous	O
time	O
step	O
.	O

Third	O
,	O
they	O
are	O
usually	O
uni	O
-	O
directional	O
,	O
wherein	O
the	O
query	O
attends	O
on	O
the	O
context	O
paragraph	O
or	O
the	O
image	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
the	O
Bi	Method
-	Method
Directional	Method
Attention	Method
Flow	Method
(	O
BiDAF	O
)	O
network	O
,	O
a	O
hierarchical	Method
multi	Method
-	Method
stage	Method
architecture	Method
for	O
modeling	O
the	O
representations	O
of	O
the	O
context	O
paragraph	O
at	O
different	O
levels	O
of	O
granularity	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

BiDAF	Method
includes	O
character	O
-	O
level	O
,	O
word	O
-	O
level	O
,	O
and	O
contextual	O
embeddings	O
,	O
and	O
uses	O
bi	Method
-	Method
directional	Method
attention	Method
flow	O
to	O
obtain	O
a	O
query	Method
-	Method
aware	Method
context	Method
representation	Method
.	O

Our	O
attention	Method
mechanism	Method
offers	O
following	O
improvements	O
to	O
the	O
previously	O
popular	O
attention	Method
paradigms	Method
.	O

First	O
,	O
our	O
attention	Method
layer	Method
is	O
not	O
used	O
to	O
summarize	O
the	O
context	O
paragraph	O
into	O
a	O
fixed	O
-	O
size	O
vector	O
.	O

Instead	O
,	O
the	O
attention	O
is	O
computed	O
for	O
every	O
time	O
step	O
,	O
and	O
the	O
attended	O
vector	O
at	O
each	O
time	O
step	O
,	O
along	O
with	O
the	O
representations	O
from	O
previous	O
layers	O
,	O
is	O
allowed	O
to	O
flow	O
through	O
to	O
the	O
subsequent	O
modeling	Method
layer	Method
.	O

This	O
reduces	O
the	O
information	O
loss	O
caused	O
by	O
early	Task
summarization	Task
.	O

Second	O
,	O
we	O
use	O
a	O
memory	Method
-	Method
less	Method
attention	Method
mechanism	Method
.	O

That	O
is	O
,	O
while	O
we	O
iteratively	O
compute	O
attention	O
through	O
time	O
as	O
in	O
,	O
the	O
attention	O
at	O
each	O
time	O
step	O
is	O
a	O
function	O
of	O
only	O
the	O
query	O
and	O
the	O
context	O
paragraph	O
at	O
the	O
current	O
time	O
step	O
and	O
does	O
not	O
directly	O
depend	O
on	O
the	O
attention	O
at	O
the	O
previous	O
time	O
step	O
.	O

We	O
hypothesize	O
that	O
this	O
simplification	O
leads	O
to	O
the	O
division	O
of	O
labor	O
between	O
the	O
attention	Method
layer	Method
and	O
the	O
modeling	Method
layer	Method
.	O

It	O
forces	O
the	O
attention	Method
layer	Method
to	O
focus	O
on	O
learning	O
the	O
attention	O
between	O
the	O
query	O
and	O
the	O
context	O
,	O
and	O
enables	O
the	O
modeling	Method
layer	Method
to	O
focus	O
on	O
learning	O
the	O
interaction	O
within	O
the	O
query	Method
-	Method
aware	Method
context	Method
representation	Method
(	O
the	O
output	O
of	O
the	O
attention	Method
layer	Method
)	O
.	O

It	O
also	O
allows	O
the	O
attention	O
at	O
each	O
time	O
step	O
to	O
be	O
unaffected	O
from	O
incorrect	O
attendances	O
at	O
previous	O
time	O
steps	O
.	O

Our	O
experiments	O
show	O
that	O
memory	Task
-	Task
less	Task
attention	Task
gives	O
a	O
clear	O
advantage	O
over	O
dynamic	O
attention	O
.	O

Third	O
,	O
we	O
use	O
attention	Method
mechanisms	Method
in	O
both	O
directions	O
,	O
query	O
-	O
to	O
-	O
context	O
and	O
context	O
-	O
to	O
-	O
query	O
,	O
which	O
provide	O
complimentary	O
information	O
to	O
each	O
other	O
.	O

Our	O
BiDAF	Method
model	Method
outperforms	O
all	O
previous	O
approaches	O
on	O
the	O
highly	O
-	O
competitive	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
test	O
set	O
leaderboard	O
at	O
the	O
time	O
of	O
submission	O
.	O

With	O
a	O
modification	O
to	O
only	O
the	O
output	O
layer	O
,	O
BiDAF	Method
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
CNN	Material
/	Material
DailyMail	Material
cloze	Material
test	Material
.	O

We	O
also	O
provide	O
an	O
in	O
-	O
depth	O
ablation	O
study	O
of	O
our	O
model	O
on	O
the	O
SQuAD	Material
development	O
set	O
,	O
visualize	O
the	O
intermediate	O
feature	O
spaces	O
in	O
our	O
model	O
,	O
and	O
analyse	O
its	O
performance	O
as	O
compared	O
to	O
a	O
more	O
traditional	O
language	Method
model	Method
for	O
machine	Task
comprehension	Task
rajpurkar2016squad	O
.	O

section	O
:	O
Model	O
Our	O
machine	Method
comprehension	Method
model	Method
is	O
a	O
hierarchical	Method
multi	Method
-	Method
stage	Method
process	Method
and	O
consists	O
of	O
six	O
layers	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
:	O
Character	Method
Embedding	Method
Layer	Method
maps	O
each	O
word	O
to	O
a	O
vector	O
space	O
using	O
character	Method
-	Method
level	Method
CNNs	Method
.	O

Word	Method
Embedding	Method
Layer	Method
maps	O
each	O
word	O
to	O
a	O
vector	O
space	O
using	O
a	O
pre	O
-	O
trained	O
word	Method
embedding	Method
model	Method
.	O

Contextual	Method
Embedding	Method
Layer	Method
utilizes	O
contextual	O
cues	O
from	O
surrounding	O
words	O
to	O
refine	O
the	O
embedding	O
of	O
the	O
words	O
.	O

These	O
first	O
three	O
layers	O
are	O
applied	O
to	O
both	O
the	O
query	O
and	O
context	O
.	O

Attention	Method
Flow	Method
Layer	Method
couples	O
the	O
query	O
and	O
context	O
vectors	O
and	O
produces	O
a	O
set	O
of	O
query	O
-	O
aware	O
feature	O
vectors	O
for	O
each	O
word	O
in	O
the	O
context	O
.	O

Modeling	Method
Layer	Method
employs	O
a	O
Recurrent	Method
Neural	Method
Network	Method
to	O
scan	O
the	O
context	O
.	O

Output	Method
Layer	Method
provides	O
an	O
answer	O
to	O
the	O
query	O
.	O

paragraph	O
:	O
1	O
.	O

Character	Method
Embedding	Method
Layer	Method
.	O

Character	Method
embedding	Method
layer	Method
is	O
responsible	O
for	O
mapping	O
each	O
word	O
to	O
a	O
high	O
-	O
dimensional	O
vector	O
space	O
.	O

Let	O
and	O
represent	O
the	O
words	O
in	O
the	O
input	O
context	O
paragraph	O
and	O
query	O
,	O
respectively	O
.	O

Following	O
,	O
we	O
obtain	O
the	O
character	Method
-	Method
level	Method
embedding	Method
of	O
each	O
word	O
using	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNN	Method
)	O
.	O

Characters	O
are	O
embedded	O
into	O
vectors	O
,	O
which	O
can	O
be	O
considered	O
as	O
1D	O
inputs	O
to	O
the	O
CNN	Method
,	O
and	O
whose	O
size	O
is	O
the	O
input	O
channel	O
size	O
of	O
the	O
CNN	Method
.	O

The	O
outputs	O
of	O
the	O
CNN	Method
are	O
max	O
-	O
pooled	O
over	O
the	O
entire	O
width	O
to	O
obtain	O
a	O
fixed	O
-	O
size	O
vector	O
for	O
each	O
word	O
.	O

paragraph	O
:	O
2	O
.	O

Word	Method
Embedding	Method
Layer	Method
.	O

Word	Method
embedding	Method
layer	Method
also	O
maps	O
each	O
word	O
to	O
a	O
high	O
-	O
dimensional	O
vector	O
space	O
.	O

We	O
use	O
pre	O
-	O
trained	O
word	O
vectors	O
,	O
GloVe	Method
glove	Method
,	O
to	O
obtain	O
the	O
fixed	O
word	O
embedding	O
of	O
each	O
word	O
.	O

The	O
concatenation	O
of	O
the	O
character	O
and	O
word	O
embedding	O
vectors	O
is	O
passed	O
to	O
a	O
two	O
-	O
layer	Method
Highway	Method
Network	Method
highway	Method
.	O

The	O
outputs	O
of	O
the	O
Highway	Method
Network	Method
are	O
two	O
sequences	O
of	O
-	O
dimensional	O
vectors	O
,	O
or	O
more	O
conveniently	O
,	O
two	O
matrices	O
:	O
for	O
the	O
context	O
and	O
for	O
the	O
query	O
.	O

paragraph	O
:	O
3	O
.	O

Contextual	Method
Embedding	Method
Layer	Method
.	O

We	O
use	O
a	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
Network	Method
(	Method
LSTM	Method
)	Method
lstm	Method
on	O
top	O
of	O
the	O
embeddings	O
provided	O
by	O
the	O
previous	O
layers	O
to	O
model	O
the	O
temporal	O
interactions	O
between	O
words	O
.	O

We	O
place	O
an	O
LSTM	Method
in	O
both	O
directions	O
,	O
and	O
concatenate	O
the	O
outputs	O
of	O
the	O
two	O
LSTMs	Method
.	O

Hence	O
we	O
obtain	O
from	O
the	O
context	O
word	O
vectors	O
,	O
and	O
from	O
query	O
word	O
vectors	O
.	O

Note	O
that	O
each	O
column	O
vector	O
of	O
and	O
is	O
-	O
dimensional	O
because	O
of	O
the	O
concatenation	O
of	O
the	O
outputs	O
of	O
the	O
forward	Method
and	Method
backward	Method
LSTMs	Method
,	O
each	O
with	O
-	O
dimensional	O
output	O
.	O

It	O
is	O
worth	O
noting	O
that	O
the	O
first	O
three	O
layers	O
of	O
the	O
model	O
are	O
computing	O
features	O
from	O
the	O
query	O
and	O
context	O
at	O
different	O
levels	O
of	O
granularity	O
,	O
akin	O
to	O
the	O
multi	Method
-	Method
stage	Method
feature	Method
computation	Method
of	O
convolutional	Method
neural	Method
networks	Method
in	O
the	O
computer	Task
vision	Task
field	Task
.	O

paragraph	O
:	O
4	O
.	O

Attention	Method
Flow	Method
Layer	Method
.	O

Attention	Method
flow	Method
layer	Method
is	O
responsible	O
for	O
linking	O
and	O
fusing	O
information	O
from	O
the	O
context	O
and	O
the	O
query	O
words	O
.	O

Unlike	O
previously	O
popular	O
attention	Method
mechanisms	Method
memnn	Method
,	O
hill2015goldilocks	O
,	O
iterative	Method
,	O
reasonet	Method
,	O
the	O
attention	Method
flow	Method
layer	Method
is	O
not	O
used	O
to	O
summarize	O
the	O
query	O
and	O
context	O
into	O
single	O
feature	O
vectors	O
.	O

Instead	O
,	O
the	O
attention	O
vector	O
at	O
each	O
time	O
step	O
,	O
along	O
with	O
the	O
embeddings	O
from	O
previous	O
layers	O
,	O
are	O
allowed	O
to	O
flow	O
through	O
to	O
the	O
subsequent	O
modeling	Method
layer	Method
.	O

This	O
reduces	O
the	O
information	O
loss	O
caused	O
by	O
early	Task
summarization	Task
.	O

The	O
inputs	O
to	O
the	O
layer	O
are	O
contextual	Method
vector	Method
representations	Method
of	O
the	O
context	O
and	O
the	O
query	O
.	O

The	O
outputs	O
of	O
the	O
layer	O
are	O
the	O
query	Method
-	Method
aware	Method
vector	Method
representations	Method
of	O
the	O
context	O
words	O
,	O
,	O
along	O
with	O
the	O
contextual	O
embeddings	O
from	O
the	O
previous	O
layer	O
.	O

In	O
this	O
layer	O
,	O
we	O
compute	O
attentions	O
in	O
two	O
directions	O
:	O
from	O
context	O
to	O
query	O
as	O
well	O
as	O
from	O
query	O
to	O
context	O
.	O

Both	O
of	O
these	O
attentions	O
,	O
which	O
will	O
be	O
discussed	O
below	O
,	O
are	O
derived	O
from	O
a	O
shared	O
similarity	O
matrix	O
,	O
,	O
between	O
the	O
contextual	O
embeddings	O
of	O
the	O
context	O
(	O
)	O
and	O
the	O
query	O
(	O
)	O
,	O
where	O
indicates	O
the	O
similarity	O
between	O
-	O
th	O
context	O
word	O
and	O
-	O
th	O
query	O
word	O
.	O

The	O
similarity	O
matrix	O
is	O
computed	O
by	O
where	O
is	O
a	O
trainable	O
scalar	O
function	O
that	O
encodes	O
the	O
similarity	O
between	O
its	O
two	O
input	O
vectors	O
,	O
is	O
-	O
th	O
column	O
vector	O
of	O
,	O
and	O
is	O
-	O
th	O
column	O
vector	O
of	O
,	O
We	O
choose	O
,	O
where	O
is	O
a	O
trainable	O
weight	O
vector	O
,	O
is	O
elementwise	Method
multiplication	Method
,	O
is	O
vector	O
concatenation	O
across	O
row	O
,	O
and	O
implicit	Method
multiplication	Method
is	O
matrix	Method
multiplication	Method
.	O

Now	O
we	O
use	O
to	O
obtain	O
the	O
attentions	O
and	O
the	O
attended	O
vectors	O
in	O
both	O
directions	O
.	O

Context	Task
-	Task
to	Task
-	Task
query	Task
Attention	Task
.	O

Context	Task
-	Task
to	Task
-	Task
query	Task
(	O
C2Q	O
)	O
attention	Task
signifies	O
which	O
query	O
words	O
are	O
most	O
relevant	O
to	O
each	O
context	O
word	O
.	O

Let	O
represent	O
the	O
attention	O
weights	O
on	O
the	O
query	O
words	O
by	O
-	O
th	O
context	O
word	O
,	O
for	O
all	O
.	O

The	O
attention	O
weight	O
is	O
computed	O
by	O
,	O
and	O
subsequently	O
each	O
attended	O
query	O
vector	O
is	O
.	O

Hence	O
is	O
a	O
-	O
by	O
-	O
matrix	O
containing	O
the	O
attended	O
query	O
vectors	O
for	O
the	O
entire	O
context	O
.	O

Query	Task
-	Task
to	Task
-	Task
context	Task
Attention	Task
.	O

Query	Task
-	Task
to	Task
-	Task
context	Task
(	Task
Q2C	Task
)	Task
attention	Task
signifies	O
which	O
context	O
words	O
have	O
the	O
closest	O
similarity	O
to	O
one	O
of	O
the	O
query	O
words	O
and	O
are	O
hence	O
critical	O
for	O
answering	O
the	O
query	O
.	O

We	O
obtain	O
the	O
attention	O
weights	O
on	O
the	O
context	O
words	O
by	O
,	O
where	O
the	O
maximum	O
function	O
(	O
)	O
is	O
performed	O
across	O
the	O
column	O
.	O

Then	O
the	O
attended	O
context	O
vector	O
is	O
.	O

This	O
vector	O
indicates	O
the	O
weighted	O
sum	O
of	O
the	O
most	O
important	O
words	O
in	O
the	O
context	O
with	O
respect	O
to	O
the	O
query	O
.	O

is	O
tiled	O
times	O
across	O
the	O
column	O
,	O
thus	O
giving	O
.	O

Finally	O
,	O
the	O
contextual	O
embeddings	O
and	O
the	O
attention	O
vectors	O
are	O
combined	O
together	O
to	O
yield	O
,	O
where	O
each	O
column	O
vector	O
can	O
be	O
considered	O
as	O
the	O
query	Method
-	Method
aware	Method
representation	Method
of	O
each	O
context	O
word	O
.	O

We	O
define	O
by	O
where	O
is	O
the	O
-	O
th	O
column	O
vector	O
(	O
corresponding	O
to	O
-	O
th	O
context	O
word	O
)	O
,	O
is	O
a	O
trainable	Method
vector	Method
function	Method
that	O
fuses	O
its	O
(	O
three	O
)	O
input	O
vectors	O
,	O
and	O
is	O
the	O
output	O
dimension	O
of	O
the	O
function	O
.	O

While	O
the	O
function	O
can	O
be	O
an	O
arbitrary	O
trainable	Method
neural	Method
network	Method
,	O
such	O
as	O
multi	Method
-	Method
layer	Method
perceptron	Method
,	O
a	O
simple	O
concatenation	Method
as	O
following	O
still	O
shows	O
good	O
performance	O
in	O
our	O
experiments	O
:	O
(	O
i.e.	O
,	O
)	O
.	O

paragraph	O
:	O
5	O
.	O

Modeling	Method
Layer	Method
.	O

The	O
input	O
to	O
the	O
modeling	Method
layer	Method
is	O
,	O
which	O
encodes	O
the	O
query	Method
-	Method
aware	Method
representations	Method
of	Method
context	Method
words	Method
.	O

The	O
output	O
of	O
the	O
modeling	Method
layer	Method
captures	O
the	O
interaction	O
among	O
the	O
context	O
words	O
conditioned	O
on	O
the	O
query	O
.	O

This	O
is	O
different	O
from	O
the	O
contextual	Method
embedding	Method
layer	Method
,	O
which	O
captures	O
the	O
interaction	O
among	O
context	O
words	O
independent	O
of	O
the	O
query	O
.	O

We	O
use	O
two	O
layers	O
of	O
bi	Method
-	Method
directional	Method
LSTM	Method
,	O
with	O
the	O
output	O
size	O
of	O
for	O
each	O
direction	O
.	O

Hence	O
we	O
obtain	O
a	O
matrix	O
,	O
which	O
is	O
passed	O
onto	O
the	O
output	Method
layer	Method
to	O
predict	O
the	O
answer	O
.	O

Each	O
column	O
vector	O
of	O
is	O
expected	O
to	O
contain	O
contextual	O
information	O
about	O
the	O
word	O
with	O
respect	O
to	O
the	O
entire	O
context	O
paragraph	O
and	O
the	O
query	O
.	O

paragraph	O
:	O
6	O
.	O

Output	O
Layer	O
.	O

The	O
output	Method
layer	Method
is	O
application	O
-	O
specific	O
.	O

The	O
modular	O
nature	O
of	O
BiDAF	Method
allows	O
us	O
to	O
easily	O
swap	O
out	O
the	O
output	O
layer	O
based	O
on	O
the	O
task	O
,	O
with	O
the	O
rest	O
of	O
the	O
architecture	O
remaining	O
exactly	O
the	O
same	O
.	O

Here	O
,	O
we	O
describe	O
the	O
output	Method
layer	Method
for	O
the	O
QA	Task
task	Task
.	O

In	O
section	O
[	O
reference	O
]	O
,	O
we	O
use	O
a	O
slight	O
modification	O
of	O
this	O
output	Method
layer	Method
for	O
cloze	Task
-	Task
style	Task
comprehension	Task
.	O

The	O
QA	Task
task	Task
requires	O
the	O
model	O
to	O
find	O
a	O
sub	O
-	O
phrase	O
of	O
the	O
paragraph	O
to	O
answer	O
the	O
query	O
.	O

The	O
phrase	O
is	O
derived	O
by	O
predicting	O
the	O
start	O
and	O
the	O
end	O
indices	O
of	O
the	O
phrase	O
in	O
the	O
paragraph	O
.	O

We	O
obtain	O
the	O
probability	O
distribution	O
of	O
the	O
start	O
index	O
over	O
the	O
entire	O
paragraph	O
by	O
where	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

For	O
the	O
end	O
index	O
of	O
the	O
answer	O
phrase	O
,	O
we	O
pass	O
to	O
another	O
bidirectional	Method
LSTM	Method
layer	Method
and	O
obtain	O
.	O

Then	O
we	O
use	O
to	O
obtain	O
the	O
probability	O
distribution	O
of	O
the	O
end	O
index	O
in	O
a	O
similar	O
manner	O
:	O
Training	Task
.	O

We	O
define	O
the	O
training	Metric
loss	Metric
(	O
to	O
be	O
minimized	O
)	O
as	O
the	O
sum	O
of	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
start	O
and	O
end	O
indices	O
by	O
the	O
predicted	O
distributions	O
,	O
averaged	O
over	O
all	O
examples	O
:	O
where	O
is	O
the	O
set	O
of	O
all	O
trainable	O
weights	O
in	O
the	O
model	O
(	O
the	O
weights	O
and	O
biases	O
of	O
CNN	Method
filters	Method
and	O
LSTM	Method
cells	Method
,	O
,	O
and	O
)	O
,	O
is	O
the	O
number	O
of	O
examples	O
in	O
the	O
dataset	O
,	O
and	O
are	O
the	O
true	O
start	O
and	O
end	O
indices	O
of	O
the	O
-	O
th	O
example	O
,	O
respectively	O
,	O
and	O
indicates	O
the	O
-	O
th	O
value	O
of	O
the	O
vector	O
.	O

Test	O
.	O

The	O
answer	O
span	O
where	O
with	O
the	O
maximum	O
value	O
of	O
is	O
chosen	O
,	O
which	O
can	O
be	O
computed	O
in	O
linear	O
time	O
with	O
dynamic	Method
programming	Method
.	O

section	O
:	O
Related	O
Work	O
paragraph	O
:	O
Machine	Task
comprehension	Task
.	O

A	O
significant	O
contributor	O
to	O
the	O
advancement	O
of	O
MC	Method
models	Method
has	O
been	O
the	O
availability	O
of	O
large	O
datasets	O
.	O

Early	O
datasets	O
such	O
as	O
MCTest	Material
richardson2013mctest	O
were	O
too	O
small	O
to	O
train	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
models	Method
.	O

Massive	O
cloze	O
test	O
datasets	O
(	O
CNN	O
/	O
DailyMail	Material
by	O
Hermann2015TeachingMT	O
and	O
Childrens	O
Book	O
Test	O
by	O
)	O
,	O
enabled	O
the	O
application	O
of	O
deep	Method
neural	Method
architectures	Method
to	O
this	O
task	O
.	O

More	O
recently	O
,	O
rajpurkar2016squad	O
released	O
the	O
Stanford	Material
Question	Material
Answering	Material
(	O
SQuAD	Material
)	O
dataset	O
with	O
over	O
100	O
,	O
000	O
questions	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
our	O
comprehension	Method
system	Method
on	O
both	O
SQuAD	Material
and	O
CNN	Material
/	Material
DailyMail	Material
datasets	Material
.	O

Previous	O
works	O
in	O
end	Task
-	Task
to	Task
-	Task
end	Task
machine	Task
comprehension	Task
use	O
attention	Method
mechanisms	Method
in	O
three	O
distinct	O
ways	O
.	O

The	O
first	O
group	O
(	O
largely	O
inspired	O
by	O
)	O
uses	O
a	O
dynamic	Method
attention	Method
mechanism	Method
,	O
in	O
which	O
the	O
attention	O
weights	O
are	O
updated	O
dynamically	O
given	O
the	O
query	O
and	O
the	O
context	O
as	O
well	O
as	O
the	O
previous	O
attention	O
.	O

argue	O
that	O
the	O
dynamic	Method
attention	Method
model	Method
performs	O
better	O
than	O
using	O
a	O
single	O
fixed	O
query	O
vector	O
to	O
attend	O
on	O
context	O
words	O
on	O
CNN	O
&	O
DailyMail	Material
datasets	O
.	O

show	O
that	O
simply	O
using	O
bilinear	O
term	O
for	O
computing	O
the	O
attention	O
weights	O
in	O
the	O
same	O
model	O
drastically	O
improves	O
the	O
accuracy	Metric
.	O

reverse	O
the	O
direction	O
of	O
the	O
attention	O
(	O
attending	O
on	O
query	O
words	O
as	O
the	O
context	O
RNN	O
progresses	O
)	O
for	O
SQuAD	Material
.	O

In	O
contrast	O
to	O
these	O
models	O
,	O
BiDAF	Method
uses	O
a	O
memory	Method
-	Method
less	Method
attention	Method
mechanism	Method
.	O

The	O
second	O
group	O
computes	O
the	O
attention	O
weights	O
once	O
,	O
which	O
are	O
then	O
fed	O
into	O
an	O
output	Method
layer	Method
for	O
final	O
prediction	Task
(	O
e.g.	O
,	O
)	O
.	O

Attention	Method
-	Method
over	Method
-	Method
attention	Method
model	Method
aoa	Method
uses	O
a	O
2D	O
similarity	O
matrix	O
between	O
the	O
query	O
and	O
context	O
words	O
(	O
similar	O
to	O
Equation	O
[	O
reference	O
]	O
)	O
to	O
compute	O
the	O
weighted	Task
average	Task
of	Task
query	Task
-	Task
to	Task
-	Task
context	Task
attention	Task
.	O

In	O
contrast	O
to	O
these	O
models	O
,	O
BiDAF	Method
does	O
not	O
summarize	O
the	O
two	O
modalities	O
in	O
the	O
attention	Method
layer	Method
and	O
instead	O
lets	O
the	O
attention	O
vectors	O
flow	O
into	O
the	O
modeling	Method
(	Method
RNN	Method
)	Method
layer	Method
.	O

The	O
third	O
group	O
(	O
considered	O
as	O
variants	O
of	O
Memory	Method
Network	Method
memnn	Method
)	O
repeats	O
computing	O
an	O
attention	O
vector	O
between	O
the	O
query	O
and	O
the	O
context	O
through	O
multiple	O
layers	O
,	O
typically	O
referred	O
to	O
as	O
multi	Method
-	Method
hop	Method
iterative	Method
,	O
ga	Method
.	O

combine	O
Memory	Method
Networks	Method
with	O
Reinforcement	Method
Learning	Method
in	O
order	O
to	O
dynamically	O
control	O
the	O
number	O
of	O
hops	O
.	O

One	O
can	O
also	O
extend	O
our	O
BiDAF	Method
model	Method
to	O
incorporate	O
multiple	O
hops	O
.	O

paragraph	O
:	O
Visual	Task
question	Task
answering	Task
.	O

The	O
task	O
of	O
question	Task
answering	Task
has	O
also	O
gained	O
a	O
lot	O
of	O
interest	O
in	O
the	O
computer	Task
vision	Task
community	Task
.	O

Early	O
works	O
on	O
visual	Task
question	Task
answering	Task
(	O
VQA	Task
)	O
involved	O
encoding	O
the	O
question	O
using	O
an	O
RNN	Method
,	O
encoding	O
the	O
image	O
using	O
a	O
CNN	Method
and	O
combining	O
them	O
to	O
answer	O
the	O
question	O
antol2015vqa	O
,	O
Malinowski2015AskYN	O
.	O

Attention	Method
mechanisms	Method
have	O
also	O
been	O
successfully	O
employed	O
for	O
the	O
VQA	Task
task	Task
and	O
can	O
be	O
broadly	O
clustered	O
based	O
on	O
the	O
granularity	O
of	O
their	O
attention	O
and	O
the	O
approach	O
to	O
construct	O
the	O
attention	O
matrix	O
.	O

At	O
the	O
coarse	O
level	O
of	O
granularity	O
,	O
the	O
question	O
attends	O
to	O
different	O
patches	O
in	O
the	O
image	O
Zhu2015Visual7WGQ	O
,	O
xiong2016dynamic	O
.	O

At	O
a	O
finer	O
level	O
,	O
each	O
question	O
word	O
attends	O
to	O
each	O
image	O
patch	O
and	O
the	O
highest	O
attention	O
value	O
for	O
each	O
spatial	O
location	O
Xu2016AskAA	O
is	O
adopted	O
.	O

A	O
hybrid	O
approach	O
is	O
to	O
combine	O
questions	Method
representations	Method
at	O
multiple	O
levels	O
of	O
granularity	O
(	O
unigrams	O
,	O
bigrams	O
,	O
trigrams	O
)	O
yang2015stacked	O
.	O

Several	O
approaches	O
to	O
constructing	O
the	O
attention	O
matrix	O
have	O
been	O
used	O
including	O
element	Method
-	Method
wise	Method
product	Method
,	O
element	Method
-	Method
wise	Method
sum	Method
,	O
concatenation	Method
and	O
Multimodal	Method
Compact	Method
Bilinear	Method
Pooling	Method
fukui2016multimodal	O
.	O

lu2016hierarchical	O
have	O
recently	O
shown	O
that	O
in	O
addition	O
to	O
attending	O
from	O
the	O
question	O
to	O
image	O
patches	O
,	O
attending	O
from	O
the	O
image	O
back	O
to	O
the	O
question	O
words	O
provides	O
an	O
improvement	O
on	O
the	O
VQA	Task
task	Task
.	O

This	O
finding	O
in	O
the	O
visual	Task
domain	Task
is	O
consistent	O
with	O
our	O
finding	O
in	O
the	O
language	O
domain	O
,	O
where	O
our	O
bi	Method
-	Method
directional	Method
attention	Method
between	O
the	O
query	O
and	O
context	O
provides	O
improved	O
results	O
.	O

Their	O
model	O
,	O
however	O
,	O
uses	O
the	O
attention	O
weights	O
directly	O
in	O
the	O
output	O
layer	O
and	O
does	O
not	O
take	O
advantage	O
of	O
the	O
attention	O
flow	O
to	O
the	O
modeling	Method
layer	Method
.	O

section	O
:	O
Question	Task
Answering	Task
Experiments	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
our	O
model	O
on	O
the	O
task	O
of	O
question	Task
answering	Task
using	O
the	O
recently	O
released	O
SQuAD	Material
rajpurkar2016squad	O
,	O
which	O
has	O
gained	O
a	O
huge	O
attention	O
over	O
a	O
few	O
months	O
.	O

In	O
the	O
next	O
section	O
,	O
we	O
evaluate	O
our	O
model	O
on	O
the	O
task	O
of	O
cloze	Task
-	Task
style	Task
reading	Task
comprehension	Task
.	O

paragraph	O
:	O
Dataset	O
.	O

SQuAD	Material
is	O
a	O
machine	O
comprehension	O
dataset	O
on	O
a	O
large	O
set	O
of	O
Wikipedia	O
articles	O
,	O
with	O
more	O
than	O
100	O
,	O
000	O
questions	O
.	O

The	O
answer	O
to	O
each	O
question	O
is	O
always	O
a	O
span	O
in	O
the	O
context	O
.	O

The	O
model	O
is	O
given	O
a	O
credit	O
if	O
its	O
answer	O
matches	O
one	O
of	O
the	O
human	O
written	O
answers	O
.	O

Two	O
metrics	O
are	O
used	O
to	O
evaluate	O
models	O
:	O
Exact	Metric
Match	Metric
(	Metric
EM	Metric
)	Metric
and	O
a	O
softer	O
metric	O
,	O
F1	Metric
score	Metric
,	O
which	O
measures	O
the	O
weighted	Metric
average	Metric
of	Metric
the	Metric
precision	Metric
and	O
recall	Metric
rate	Metric
at	O
character	O
level	O
.	O

The	O
dataset	O
consists	O
of	O
90k	O
/	O
10k	O
train	O
/	O
dev	O
question	O
-	O
context	O
tuples	O
with	O
a	O
large	O
hidden	O
test	O
set	O
.	O

It	O
is	O
one	O
of	O
the	O
largest	O
available	O
MC	O
datasets	O
with	O
human	O
-	O
written	O
questions	O
and	O
serves	O
as	O
a	O
great	O
test	O
bed	O
for	O
our	O
model	O
.	O

paragraph	O
:	O
Model	O
Details	O
.	O

The	O
model	O
architecture	O
used	O
for	O
this	O
task	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Each	O
paragraph	O
and	O
question	O
are	O
tokenized	O
by	O
a	O
regular	Method
-	Method
expression	Method
-	Method
based	Method
word	Method
tokenizer	Method
(	O
PTB	Method
Tokenizer	Method
)	O
and	O
fed	O
into	O
the	O
model	O
.	O

We	O
use	O
100	O
1D	Method
filters	Method
for	O
CNN	Task
char	Task
embedding	Task
,	O
each	O
with	O
a	O
width	O
of	O
5	O
.	O

The	O
hidden	O
state	O
size	O
(	O
)	O
of	O
the	O
model	O
is	O
100	O
.	O

The	O
model	O
has	O
about	O
2.6	O
million	O
parameters	O
.	O

We	O
use	O
the	O
AdaDelta	Method
adadelta	Method
optimizer	Method
,	O
with	O
a	O
minibatch	O
size	O
of	O
60	O
and	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
,	O
for	O
12	O
epochs	O
.	O

A	O
dropout	Metric
dropout	Metric
rate	Metric
of	Metric
is	O
used	O
for	O
the	O
CNN	Method
,	O
all	O
LSTM	Method
layers	Method
,	O
and	O
the	O
linear	Method
transformation	Method
before	O
the	O
softmax	O
for	O
the	O
answers	O
.	O

During	O
training	O
,	O
the	O
moving	O
averages	O
of	O
all	O
weights	O
of	O
the	O
model	O
are	O
maintained	O
with	O
the	O
exponential	O
decay	O
rate	O
of	O
.	O

At	O
test	O
time	O
,	O
the	O
moving	O
averages	O
instead	O
of	O
the	O
raw	O
weights	O
are	O
used	O
.	O

The	O
training	O
process	O
takes	O
roughly	O
20	O
hours	O
on	O
a	O
single	O
Titan	O
X	O
GPU	O
.	O

We	O
also	O
train	O
an	O
ensemble	Method
model	Method
consisting	O
of	O
12	O
training	O
runs	O
with	O
the	O
identical	O
architecture	O
and	O
hyper	O
-	O
parameters	O
.	O

At	O
test	O
time	O
,	O
we	O
choose	O
the	O
answer	O
with	O
the	O
highest	O
sum	O
of	O
confidence	O
scores	O
amongst	O
the	O
12	O
runs	O
for	O
each	O
question	O
.	O

paragraph	O
:	O
Results	O
.	O

The	O
results	O
of	O
our	O
model	O
and	O
competing	O
approaches	O
on	O
the	O
hidden	Task
test	Task
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

BiDAF	Method
(	Method
ensemble	Method
)	Method
achieves	O
an	O
EM	Metric
score	O
of	O
73.3	O
and	O
an	O
F1	Metric
score	Metric
of	O
81.1	O
,	O
outperforming	O
all	O
previous	O
approaches	O
.	O

[	O
htbp	O
]	O
0.6	O
[	O
htbp	O
]	O
0.4	O
paragraph	O
:	O
Ablations	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
our	O
model	O
and	O
its	O
ablations	O
on	O
the	O
SQuAD	Material
dev	O
set	O
.	O

Both	O
char	O
-	O
level	O
and	O
word	Method
-	Method
level	Method
embeddings	Method
contribute	O
towards	O
the	O
model	O
’s	O
performance	O
.	O

We	O
conjecture	O
that	O
word	Method
-	Method
level	Method
embedding	Method
is	O
better	O
at	O
representing	O
the	O
semantics	O
of	O
each	O
word	O
as	O
a	O
whole	O
,	O
while	O
char	Method
-	Method
level	Method
embedding	Method
can	O
better	O
handle	O
out	O
-	O
of	O
-	O
vocab	O
(	O
OOV	O
)	O
or	O
rare	O
words	O
.	O

To	O
evaluate	O
bi	Method
-	Method
directional	Method
attention	Method
,	O
we	O
remove	O
C2Q	O
and	O
Q2C	O
attentions	O
.	O

For	O
ablating	Task
C2Q	Task
attention	Task
,	O
we	O
replace	O
the	O
attended	O
question	O
vector	O
with	O
the	O
average	O
of	O
the	O
output	O
vectors	O
of	O
the	O
question	O
’s	O
contextual	Method
embedding	Method
layer	Method
(	O
LSTM	Method
)	Method
.	O

C2Q	O
attention	O
proves	O
to	O
be	O
critical	O
with	O
a	O
drop	O
of	O
more	O
than	O
10	O
points	O
on	O
both	O
metrics	O
.	O

For	O
ablating	Task
Q2C	Task
attention	Task
,	O
the	O
output	O
of	O
the	O
attention	Method
layer	Method
,	O
,	O
does	O
not	O
include	O
terms	O
that	O
have	O
the	O
attended	O
Q2C	O
vectors	O
,	O
.	O

To	O
evaluate	O
the	O
attention	O
flow	O
,	O
we	O
study	O
a	O
dynamic	Method
attention	Method
model	Method
,	O
where	O
the	O
attention	O
is	O
dynamically	O
computed	O
within	O
the	O
modeling	Method
layer	Method
’s	O
LSTM	Method
,	O
following	O
previous	O
work	O
Bahdanau2014NeuralMT	O
,	O
wang2016machine	O
.	O

This	O
is	O
in	O
contrast	O
with	O
our	O
approach	O
,	O
where	O
the	O
attention	O
is	O
pre	O
-	O
computed	O
before	O
flowing	O
to	O
the	O
modeling	Method
layer	Method
.	O

Despite	O
being	O
a	O
simpler	O
attention	Method
mechanism	Method
,	O
our	O
proposed	O
static	Method
attention	Method
outperforms	O
the	O
dynamically	O
computed	O
attention	O
by	O
more	O
than	O
3	O
points	O
.	O

We	O
conjecture	O
that	O
separating	O
out	O
the	O
attention	Method
layer	Method
results	O
in	O
a	O
richer	O
set	O
of	O
features	O
computed	O
in	O
the	O
first	O
4	O
layers	O
which	O
are	O
then	O
incorporated	O
by	O
the	O
modeling	Method
layer	Method
.	O

We	O
also	O
show	O
the	O
performance	O
of	O
BiDAF	Method
with	O
several	O
different	O
definitions	O
of	O
and	O
functions	O
(	O
Equation	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Visualizations	Task
.	O

We	O
now	O
provide	O
a	O
qualitative	O
analysis	O
of	O
our	O
model	O
on	O
the	O
SQuAD	Material
dev	O
set	O
.	O

First	O
,	O
we	O
visualize	O
the	O
feature	O
spaces	O
after	O
the	O
word	Method
and	Method
contextual	Method
embedding	Method
layers	Method
.	O

These	O
two	O
layers	O
are	O
responsible	O
for	O
aligning	O
the	O
embeddings	O
between	O
the	O
query	O
and	O
context	O
words	O
which	O
are	O
the	O
inputs	O
to	O
the	O
subsequent	O
attention	Method
layer	Method
.	O

To	O
visualize	O
the	O
embeddings	O
,	O
we	O
choose	O
a	O
few	O
frequent	O
query	O
words	O
in	O
the	O
dev	O
data	O
and	O
look	O
at	O
the	O
context	O
words	O
that	O
have	O
the	O
highest	O
cosine	O
similarity	O
to	O
the	O
query	O
words	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

At	O
the	O
word	Method
embedding	Method
layer	Method
,	O
query	O
words	O
such	O
as	O
When	O
,	O
Where	O
and	O
Who	O
are	O
not	O
well	O
aligned	O
to	O
possible	O
answers	O
in	O
the	O
context	O
,	O
but	O
this	O
dramatically	O
changes	O
in	O
the	O
contextual	Method
embedding	Method
layer	Method
which	O
has	O
access	O
to	O
context	O
from	O
surrounding	O
words	O
and	O
is	O
just	O
1	O
layer	O
below	O
the	O
attention	Method
layer	Method
.	O

When	O
begins	O
to	O
match	O
years	O
,	O
Where	O
matches	O
locations	O
,	O
and	O
Who	O
matches	O
names	O
.	O

We	O
also	O
visualize	O
these	O
two	O
feature	O
spaces	O
using	O
t	Method
-	Method
SNE	Method
in	O
Figure	O
[	O
reference	O
]	O
.	O

t	Method
-	Method
SNE	Method
is	O
performed	O
on	O
a	O
large	O
fraction	O
of	O
dev	O
data	O
but	O
we	O
only	O
plot	O
data	O
points	O
corresponding	O
to	O
the	O
months	O
of	O
the	O
year	O
.	O

An	O
interesting	O
pattern	O
emerges	O
in	O
the	O
Word	O
space	O
,	O
where	O
May	O
is	O
separated	O
from	O
the	O
rest	O
of	O
the	O
months	O
because	O
May	O
has	O
multiple	O
meanings	O
in	O
the	O
English	O
language	O
.	O

The	O
contextual	Method
embedding	Method
layer	Method
uses	O
contextual	O
cues	O
from	O
surrounding	O
words	O
and	O
is	O
able	O
to	O
separate	O
the	O
usages	O
of	O
the	O
word	O
May	O
.	O

Finally	O
we	O
visualize	O
the	O
attention	O
matrices	O
for	O
some	O
question	O
-	O
context	O
tuples	O
in	O
the	O
dev	O
data	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
the	O
first	O
example	O
,	O
Where	O
matches	O
locations	O
and	O
in	O
the	O
second	O
example	O
,	O
many	O
matches	O
quantities	O
and	O
numerical	O
symbols	O
.	O

Also	O
,	O
entities	O
in	O
the	O
question	O
typically	O
attend	O
to	O
the	O
same	O
entities	O
in	O
the	O
context	O
,	O
thus	O
providing	O
a	O
feature	O
for	O
the	O
model	O
to	O
localize	O
possible	O
answers	O
.	O

paragraph	O
:	O
Discussions	O
.	O

We	O
analyse	O
the	O
performance	O
of	O
our	O
our	O
model	O
with	O
a	O
traditional	O
language	Method
-	Method
feature	Method
-	Method
based	Method
baseline	Method
rajpurkar2016squad	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
a	O
Venn	O
diagram	O
of	O
the	O
dev	O
set	O
questions	O
correctly	O
answered	O
by	O
the	O
models	O
.	O

Our	O
model	O
is	O
able	O
to	O
answer	O
more	O
than	O
86	O
%	O
of	O
the	O
questions	O
correctly	O
answered	O
by	O
the	O
baseline	O
.	O

The	O
14	O
%	O
that	O
are	O
incorrectly	O
answered	O
does	O
not	O
have	O
a	O
clear	O
pattern	O
.	O

This	O
suggests	O
that	O
neural	Method
architectures	Method
are	O
able	O
to	O
exploit	O
much	O
of	O
the	O
information	O
captured	O
by	O
the	O
language	O
features	O
.	O

We	O
also	O
break	O
this	O
comparison	O
down	O
by	O
the	O
first	O
words	O
in	O
the	O
questions	O
(	O
Figure	O
[	O
reference	O
]	O
c	O
)	O
.	O

Our	O
model	O
outperforms	O
the	O
traditional	O
baseline	O
comfortably	O
in	O
every	O
category	O
.	O

paragraph	O
:	O
Error	Method
Analysis	Method
.	O

We	O
randomly	O
select	O
50	O
incorrect	O
questions	O
(	O
based	O
on	O
EM	Metric
)	O
and	O
categorize	O
them	O
into	O
6	O
classes	O
.	O

50	O
%	O
of	O
errors	O
are	O
due	O
to	O
the	O
imprecise	O
boundaries	O
of	O
the	O
answers	O
,	O
28	O
%	O
involve	O
syntactic	O
complications	O
and	O
ambiguities	O
,	O
14	O
%	O
are	O
paraphrase	Task
problems	Task
,	O
4	O
%	O
require	O
external	O
knowledge	O
,	O
2	O
%	O
need	O
multiple	O
sentences	O
to	O
answer	O
,	O
and	O
2	O
%	O
are	O
due	O
to	O
mistakes	O
during	O
tokenization	Task
.	O

See	O
Appendix	O
[	O
reference	O
]	O
for	O
the	O
examples	O
of	O
the	O
error	O
modes	O
.	O

section	O
:	O
Cloze	O
Test	O
Experiments	O
We	O
also	O
evaluate	O
our	O
model	O
on	O
the	O
task	O
of	O
cloze	Task
-	Task
style	Task
reading	Task
comprehension	Task
using	O
the	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
Hermann2015TeachingMT	O
.	O

paragraph	O
:	O
Dataset	O
.	O

In	O
a	O
cloze	Task
test	Task
,	O
the	O
reader	O
is	O
asked	O
to	O
fill	O
in	O
words	O
that	O
have	O
been	O
removed	O
from	O
a	O
passage	O
,	O
for	O
measuring	O
one	O
’s	O
ability	O
to	O
comprehend	O
text	O
.	O

Hermann2015TeachingMT	O
have	O
recently	O
compiled	O
a	O
massive	O
Cloze	O
-	O
style	O
comprehension	O
dataset	O
,	O
consisting	O
of	O
300k	O
/	O
4k	O
/	O
3k	O
and	O
879k	O
/	O
65k	O
/	O
53k	O
(	O
train	O
/	O
dev	O
/	O
test	O
)	O
examples	O
from	O
CNN	O
and	O
DailyMail	Material
news	O
articles	O
,	O
respectively	O
.	O

Each	O
example	O
has	O
a	O
news	O
article	O
and	O
an	O
incomplete	O
sentence	O
extracted	O
from	O
the	O
human	O
-	O
written	O
summary	O
of	O
the	O
article	O
.	O

To	O
distinguish	O
this	O
task	O
from	O
language	Task
modeling	Task
and	O
force	O
one	O
to	O
refer	O
to	O
the	O
article	O
to	O
predict	O
the	O
correct	O
missing	O
word	O
,	O
the	O
missing	O
word	O
is	O
always	O
a	O
named	O
entity	O
,	O
anonymized	O
with	O
a	O
random	O
ID	O
.	O

Also	O
,	O
the	O
IDs	O
must	O
be	O
shuffled	O
constantly	O
during	O
test	O
,	O
which	O
is	O
also	O
critical	O
for	O
full	Task
anonymization	Task
.	O

paragraph	O
:	O
Model	O
Details	O
.	O

The	O
model	O
architecture	O
used	O
for	O
this	O
task	O
is	O
very	O
similar	O
to	O
that	O
for	O
SQuAD	Material
(	O
Section	O
[	O
reference	O
]	O
)	O
with	O
only	O
a	O
few	O
small	O
changes	O
to	O
adapt	O
it	O
to	O
the	O
cloze	Task
test	Task
.	O

Since	O
each	O
answer	O
in	O
the	O
CNN	Material
/	Material
DailyMail	Material
datasets	Material
is	O
always	O
a	O
single	O
word	O
(	O
entity	O
)	O
,	O
we	O
only	O
need	O
to	O
predict	O
the	O
start	O
index	O
(	O
)	O
;	O
the	O
prediction	O
for	O
the	O
end	O
index	O
(	O
)	O
is	O
omitted	O
from	O
the	O
loss	O
function	O
.	O

Also	O
,	O
we	O
mask	O
out	O
all	O
non	O
-	O
entity	O
words	O
in	O
the	O
final	O
classification	Method
layer	Method
so	O
that	O
they	O
are	O
forced	O
to	O
be	O
excluded	O
from	O
possible	O
answers	O
.	O

Another	O
important	O
difference	O
from	O
SQuAD	Material
is	O
that	O
the	O
answer	O
entity	O
might	O
appear	O
more	O
than	O
once	O
in	O
the	O
context	O
paragraph	O
.	O

To	O
address	O
this	O
,	O
we	O
follow	O
a	O
similar	O
strategy	O
from	O
.	O

During	O
training	O
,	O
after	O
we	O
obtain	O
,	O
we	O
sum	O
all	O
probability	O
values	O
of	O
the	O
entity	O
instances	O
in	O
the	O
context	O
that	O
correspond	O
to	O
the	O
correct	O
answer	O
.	O

Then	O
the	O
loss	O
function	O
is	O
computed	O
from	O
the	O
summed	O
probability	O
.	O

We	O
use	O
a	O
minibatch	O
size	O
of	O
48	O
and	O
train	O
for	O
8	O
epochs	O
,	O
with	O
early	O
stop	O
when	O
the	O
accuracy	Metric
on	O
validation	O
data	O
starts	O
to	O
drop	O
.	O

Inspired	O
by	O
the	O
window	Method
-	Method
based	Method
method	Method
hill2015goldilocks	O
,	O
we	O
split	O
each	O
article	O
into	O
short	O
sentences	O
where	O
each	O
sentence	O
is	O
a	O
19	O
-	O
word	O
window	O
around	O
each	O
entity	O
(	O
hence	O
the	O
same	O
word	O
might	O
appear	O
in	O
multiple	O
sentences	O
)	O
.	O

The	O
RNNs	Method
in	O
BiDAF	Method
are	O
not	O
feed	O
-	O
forwarded	O
or	O
back	O
-	O
propagated	O
across	O
sentences	O
,	O
which	O
speed	O
up	O
the	O
training	Method
process	Method
by	O
parallelization	Task
.	O

The	O
entire	O
training	O
process	O
takes	O
roughly	O
60	O
hours	O
on	O
eight	O
Titan	O
X	O
GPUs	O
.	O

The	O
other	O
hyper	O
-	O
parameters	O
are	O
identical	O
to	O
the	O
model	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Results	O
.	O

The	O
results	O
of	O
our	O
single	Method
-	Method
run	Method
models	Method
and	O
competing	O
approaches	O
on	O
the	O
CNN	Material
/	Material
DailyMail	Material
datasets	Material
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

indicates	O
ensemble	Method
methods	Method
.	O

BiDAF	Method
outperforms	O
previous	O
single	Method
-	Method
run	Method
models	Method
on	O
both	O
datasets	O
for	O
both	O
val	O
and	O
test	O
data	O
.	O

On	O
the	O
DailyMail	Material
test	O
,	O
our	O
single	Method
-	Method
run	Method
model	Method
even	O
outperforms	O
the	O
best	O
ensemble	Method
method	Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
BiDAF	Method
,	O
a	O
multi	Method
-	Method
stage	Method
hierarchical	Method
process	Method
that	O
represents	O
the	O
context	O
at	O
different	O
levels	O
of	O
granularity	O
and	O
uses	O
a	O
bi	Method
-	Method
directional	Method
attention	Method
flow	Method
mechanism	Method
to	O
achieve	O
a	O
query	Task
-	Task
aware	Task
context	Task
representation	Task
without	O
early	Task
summarization	Task
.	O

The	O
experimental	O
evaluations	O
show	O
that	O
our	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
Stanford	Material
Question	Material
Answering	Material
Dataset	Material
(	O
SQuAD	Material
)	O
and	O
CNN	O
/	O
DailyMail	Material
cloze	O
test	O
.	O

The	O
ablation	O
analyses	O
demonstrate	O
the	O
importance	O
of	O
each	O
component	O
in	O
our	O
model	O
.	O

The	O
visualizations	O
and	O
discussions	O
show	O
that	O
our	O
model	O
is	O
learning	O
a	O
suitable	O
representation	O
for	O
MC	Task
and	O
is	O
capable	O
of	O
answering	O
complex	O
questions	O
by	O
attending	O
to	O
correct	O
locations	O
in	O
the	O
given	O
paragraph	O
.	O

Future	O
work	O
involves	O
extending	O
our	O
approach	O
to	O
incorporate	O
multiple	O
hops	O
of	O
the	O
attention	Method
layer	Method
.	O

subsubsection	O
:	O
Acknowledgments	O
This	O
research	O
was	O
supported	O
by	O
the	O
NSF	O
(	O
IIS	O
1616112	O
)	O
,	O
NSF	O
(	O
III	O
1703166	O
)	O
,	O
Allen	O
Institute	O
for	O
AI	O
(	O
66	O
-	O
9175	O
)	O
,	O
Allen	O
Distinguished	O
Investigator	O
Award	O
,	O
Google	O
Research	O
Faculty	O
Award	O
,	O
and	O
Samsung	O
GRO	O
Award	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
helpful	O
comments	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Error	Method
Analysis	Method
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
modes	O
of	O
errors	O
by	O
BiDAF	O
and	O
shows	O
examples	O
for	O
each	O
category	O
of	O
error	O
in	O
SQuAD	Material
.	O

Context	O
:	O
“	O
The	O
Free	O
Movement	O
of	O
Workers	O
Regulation	O
articles	O
1	O
to	O
7	O
set	O
out	O
the	O
main	O
provisions	O
on	O
equal	O
treatment	O
of	O
workers	O
.	O

”	O
Question	O
:	O
“	O
Which	O
articles	O
of	O
the	O
Free	O
Movement	O
of	O
Workers	O
Regulation	O
set	O
out	O
the	O
primary	O
provisions	O
on	O
equal	O
treatment	O
of	O
workers	O
?	O
”	O
Prediction	Task
:	O
“	O
1	O
to	O
7	O
”	O
,	O
Answer	O
:	O
“	O
articles	O
1	O
to	O
7	O
”	O
Context	O
:	O
“	O
A	O
piece	O
of	O
paper	O
was	O
later	O
found	O
on	O
which	O
Luther	O
had	O
written	O
his	O
last	O
statement	O
.	O

”	O
Question	O
:	O
“	O
What	O
was	O
later	O
discovered	O
written	O
by	O
Luther	O
?	O
”	O
Prediction	Task
:	O
“	O
A	O
piece	O
of	O
paper	O
”	O
,	O
Answer	O
:	O
“	O
his	O
last	O
statement	O
”	O
Context	O
:	O
“	O
Generally	O
,	O
education	O
in	O
Australia	O
follows	O
the	O
three	Method
-	Method
tier	Method
model	Method
which	O
includes	O
primary	O
education	O
(	O
primary	O
schools	O
)	O
,	O
followed	O
by	O
secondary	O
education	O
(	O
secondary	O
schools	O
/	O
high	O
schools	O
)	O
and	O
tertiary	O
education	O
(	O
universities	O
and	O
/	O
or	O
TAFE	O
colleges	O
)	O
.	O

”	O
Question	O
:	O
“	O
What	O
is	O
the	O
first	O
model	O
of	O
education	Task
,	O
in	O
the	O
Australian	Task
system	Task
?	O
”	O
Prediction	Task
:	O
“	O
three	O
-	O
tier	O
”	O
,	O
Answer	O
:	O
“	O
primary	O
education	O
”	O
Context	O
:	O
“	O
On	O
June	O
4	O
,	O
2014	O
,	O
the	O
NFL	O
announced	O
that	O
the	O
practice	O
of	O
branding	O
Super	O
Bowl	O
games	O
with	O
Roman	O
numerals	O
,	O
a	O
practice	O
established	O
at	O
Super	O
Bowl	O
V	O
,	O
would	O
be	O
temporarily	O
suspended	O
,	O
and	O
that	O
the	O
game	O
would	O
be	O
named	O
using	O
Arabic	O
numerals	O
as	O
Super	O
Bowl	O
50	O
as	O
opposed	O
to	O
Super	O
Bowl	O
L.	O
”	O
Question	O
:	O
“	O
If	O
Roman	O
numerals	O
were	O
used	O
in	O
the	O
naming	O
of	O
the	O
50th	O
Super	O
Bowl	O
,	O
which	O
one	O
would	O
have	O
been	O
used	O
?	O
’	O
Prediction	Task
:	O
“	O
Super	O
Bowl	O
50	O
”	O
,	O
Answer	O
:	O
“	O
L	O
”	O
Context	O
:	O
“	O
Over	O
the	O
next	O
several	O
years	O
in	O
addition	O
to	O
host	O
to	O
host	O
interactive	O
connections	O
the	O
network	O
was	O
enhanced	O
to	O
support	O
terminal	O
to	O
host	O
connections	O
,	O
host	O
to	O
host	O
batch	O
connections	O
(	O
remote	Task
job	Task
submission	Task
,	O
remote	Task
printing	Task
,	O
batch	Task
file	Task
transfer	Task
)	O
,	O
interactive	Task
file	Task
transfer	Task
,	O
gateways	O
to	O
the	O
Tymnet	O
and	O
Telenet	O
public	O
data	O
networks	O
,	O
X.25	O
host	O
attachments	O
,	O
gateways	O
to	O
X.25	O
data	O
networks	O
,	O
Ethernet	O
attached	O
hosts	O
,	O
and	O
eventually	O
TCP	O
/	O
IP	O
and	O
additional	O
public	O
universities	O
in	O
Michigan	O
join	O
the	O
network	O
.	O

All	O
of	O
this	O
set	O
the	O
stage	O
for	O
Merit	O
’s	O
role	O
in	O
the	O
NSFNET	Task
project	Task
starting	O
in	O
the	O
mid	O
-	O
1980s	O
.	O

”	O
Question	O
:	O
“	O
What	O
set	O
the	O
stage	O
for	O
Merits	O
role	O
in	O
NSFNET	O
”	O
Prediction	Task
:	O
“	O
All	O
of	O
this	O
set	O
the	O
stage	O
for	O
Merit	O
’s	O
role	O
in	O
the	O
NSFNET	Task
project	Task
starting	O
in	O
the	O
mid	O
-	O
1980s	O
”	O
,	O
Answer	O
:	O
“	O
Ethernet	O
attached	O
hosts	O
,	O
and	O
eventually	O
TCP	Method
/	Method
IP	Method
and	O
additional	O
public	O
universities	O
in	O
Michigan	O
join	O
the	O
network	O
”	O
Context	O
:	O
“	O
English	O
chemist	O
John	O
Mayow	O
(	O
1641	O
-	O
1679	O
)	O
refined	O
this	O
work	O
by	O
showing	O
that	O
fire	O
requires	O
only	O
a	O
part	O
of	O
air	O
that	O
he	O
called	O
spiritus	O
nitroaereus	O
or	O
just	O
nitroaereus	O
.	O

”	O
Question	O
:	O
“	O
John	O
Mayow	O
died	O
in	O
what	O
year	O
?	O
”	O
Prediction	Task
:	O
“	O
1641	O
-	O
1679	O
”	O
,	O
Answer	O
:	O
“	O
1679	O
”	O
appendix	O
:	O
Variations	O
of	O
Similarity	Method
and	Method
Fusion	Method
Functions	Method
In	O
this	O
appendix	O
section	O
,	O
we	O
experimentally	O
demonstrate	O
how	O
different	O
choices	O
of	O
the	O
similarity	O
function	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
and	O
the	O
fusion	O
function	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
impact	O
the	O
performance	O
of	O
our	O
model	O
.	O

Each	O
variation	O
is	O
defined	O
as	O
following	O
:	O
paragraph	O
:	O
Eqn	O
.	O

[	O
reference	O
]	O
:	O
dot	O
product	O
.	O

Dot	O
product	O
is	O
defined	O
as	O
where	O
indicates	O
matrix	O
transpose	O
.	O

Dot	Method
product	Method
has	O
been	O
used	O
for	O
the	O
measurement	O
of	O
similarity	O
between	O
two	O
vectors	O
by	O
.	O

paragraph	O
:	O
Eqn	O
.	O

[	O
reference	O
]	O
:	O
linear	O
.	O

Linear	O
is	O
defined	O
as	O
where	O
is	O
a	O
trainable	O
weight	O
matrix	O
.	O

This	O
can	O
be	O
considered	O
as	O
the	O
simplification	O
of	O
Equation	O
[	O
reference	O
]	O
by	O
dropping	O
the	O
term	O
in	O
the	O
concatenation	O
.	O

paragraph	O
:	O
Eqn	O
.	O

[	O
reference	O
]	O
:	O
bilinear	O
.	O

Bilinear	O
is	O
defined	O
as	O
where	O
is	O
a	O
trainable	O
weight	O
matrix	O
.	O

Bilinear	O
term	O
has	O
been	O
used	O
by	O
.	O

paragraph	O
:	O
Eqn	O
.	O

[	O
reference	O
]	O
:	O
linear	O
after	O
MLP	Method
.	O

We	O
can	O
also	O
perform	O
linear	Method
mapping	Method
after	O
single	Method
layer	Method
of	Method
perceptron	Method
:	O
where	O
and	O
are	O
trainable	O
weight	O
matrix	O
and	O
bias	O
,	O
respectively	O
.	O

Linear	Method
mapping	Method
after	O
perceptron	Method
layer	Method
has	O
been	O
used	O
by	O
.	O

paragraph	O
:	O
Eqn	O
.	O

[	O
reference	O
]	O
:	O
MLP	Method
after	O
concatenation	Method
.	O

We	O
can	O
define	O
as	O
where	O
and	O
are	O
trainable	O
weight	O
matrix	O
and	O
bias	O
.	O

This	O
is	O
equivalent	O
to	O
adding	O
ReLU	O
after	O
linearly	O
transforming	O
the	O
original	O
definition	O
of	O
.	O

Since	O
the	O
output	O
dimension	O
of	O
changes	O
,	O
the	O
input	O
dimension	O
of	O
the	O
first	O
LSTM	Method
of	O
the	O
modeling	Method
layer	Method
will	O
change	O
as	O
well	O
.	O

The	O
results	O
of	O
these	O
variations	O
on	O
the	O
dev	O
data	O
of	O
SQuAD	Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
there	O
are	O
non	O
-	O
trivial	O
gaps	O
between	O
our	O
definition	O
of	O
and	O
other	O
definitions	O
employed	O
by	O
previous	O
work	O
.	O

Adding	O
MLP	Method
in	Method
does	O
not	O
seem	O
to	O
help	O
,	O
yielding	O
slightly	O
worse	O
result	O
than	O
without	O
MLP	Method
.	O

