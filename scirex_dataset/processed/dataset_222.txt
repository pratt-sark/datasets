document	O
:	O
Sequence	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
Neural	Task
machine	Task
translation	Task
(	O
NMT	Task
)	O
offers	O
a	O
novel	O
alternative	O
formulation	O
of	O
translation	Task
that	O
is	O
potentially	O
simpler	O
than	O
statistical	Method
approaches	Method
.	O

However	O
to	O
reach	O
competitive	O
performance	O
,	O
NMT	Task
models	O
need	O
to	O
be	O
exceedingly	O
large	O
.	O

In	O
this	O
paper	O
we	O
consider	O
applying	O
knowledge	Method
distillation	Method
approaches	Method
that	O
have	O
proven	O
successful	O
for	O
reducing	O
the	O
size	O
of	O
neural	Method
models	Method
in	O
other	O
domains	O
to	O
the	O
problem	O
of	O
NMT	Task
.	O

We	O
demonstrate	O
that	O
standard	O
knowledge	Method
distillation	Method
applied	O
to	O
word	Task
-	Task
level	Task
prediction	Task
can	O
be	O
effective	O
for	O
NMT	Task
,	O
and	O
also	O
introduce	O
two	O
novel	O
sequence	Method
-	Method
level	Method
versions	Method
of	O
knowledge	Method
distillation	Method
that	O
further	O
improve	O
performance	O
,	O
and	O
somewhat	O
surprisingly	O
,	O
seem	O
to	O
eliminate	O
the	O
need	O
for	O
beam	O
search	O
(	O
even	O
when	O
applied	O
on	O
the	O
original	O
teacher	Method
model	Method
)	O
.	O

Our	O
best	O
student	Method
model	Method
runs	O
times	O
faster	O
than	O
its	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
teacher	O
with	O
little	O
loss	O
in	O
performance	O
.	O

It	O
is	O
also	O
significantly	O
better	O
than	O
a	O
baseline	O
model	O
trained	O
without	O
knowledge	Method
distillation	Method
:	O
by	O
BLEU	Metric
with	O
greedy	Method
decoding	Method
/	O
beam	Method
search	Method
.	O

Applying	O
weight	Method
pruning	Method
on	O
top	O
of	O
knowledge	Task
distillation	Task
results	O
in	O
a	O
student	Method
model	Method
that	O
has	O
fewer	O
parameters	O
than	O
the	O
original	O
teacher	Method
model	Method
,	O
with	O
a	O
decrease	O
of	O
BLEU	Metric
.	O

section	O
:	O
Introduction	O
Neural	Task
machine	Task
translation	Task
(	O
NMT	Task
)	Task
is	O
a	O
deep	Method
learning	Method
-	Method
based	Method
method	Method
for	O
translation	Task
that	O
has	O
recently	O
shown	O
promising	O
results	O
as	O
an	O
alternative	O
to	O
statistical	Method
approaches	Method
.	O

NMT	Task
systems	O
directly	O
model	O
the	O
probability	O
of	O
the	O
next	O
word	O
in	O
the	O
target	O
sentence	O
simply	O
by	O
conditioning	O
a	O
recurrent	Method
neural	Method
network	Method
on	O
the	O
source	O
sentence	O
and	O
previously	O
generated	O
target	O
words	O
.	O

While	O
both	O
simple	O
and	O
surprisingly	O
accurate	O
,	O
NMT	Task
systems	O
typically	O
need	O
to	O
have	O
very	O
high	O
capacity	O
in	O
order	O
to	O
perform	O
well	O
:	O
Sutskever2014	O
used	O
a	O
-	Method
layer	Method
LSTM	Method
with	O
hidden	Method
units	Method
per	O
layer	O
(	O
herein	O
)	O
and	O
Zhou2016	O
obtained	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
English	Material
French	Material
with	O
a	O
-	Method
layer	Method
LSTM	Method
with	O
units	O
per	O
layer	O
.	O

The	O
sheer	O
size	O
of	O
the	O
models	O
requires	O
cutting	Method
-	Method
edge	Method
hardware	Method
for	O
training	O
and	O
makes	O
using	O
the	O
models	O
on	O
standard	O
setups	O
very	O
challenging	O
.	O

This	O
issue	O
of	O
excessively	Task
large	Task
networks	Task
has	O
been	O
observed	O
in	O
several	O
other	O
domains	O
,	O
with	O
much	O
focus	O
on	O
fully	Method
-	Method
connected	Method
and	Method
convolutional	Method
networks	Method
for	O
multi	Task
-	Task
class	Task
classification	Task
.	O

Researchers	O
have	O
particularly	O
noted	O
that	O
large	O
networks	O
seem	O
to	O
be	O
necessary	O
for	O
training	O
,	O
but	O
learn	O
redundant	O
representations	O
in	O
the	O
process	O
.	O

Therefore	O
compressing	O
deep	Method
models	Method
into	O
smaller	Method
networks	Method
has	O
been	O
an	O
active	O
area	O
of	O
research	O
.	O

As	O
deep	Method
learning	Method
systems	Method
obtain	O
better	O
results	O
on	O
NLP	Task
tasks	Task
,	O
compression	Task
also	O
becomes	O
an	O
important	O
practical	O
issue	O
with	O
applications	O
such	O
as	O
running	O
deep	Method
learning	Method
models	Method
for	O
speech	Task
and	Task
translation	Task
locally	Task
on	O
cell	O
phones	O
.	O

Existing	O
compression	Method
methods	Method
generally	O
fall	O
into	O
two	O
categories	O
:	O
(	O
1	O
)	O
pruning	Task
and	O
(	O
2	O
)	O
knowledge	Task
distillation	Task
.	O

Pruning	Method
methods	Method
,	O
zero	O
-	O
out	O
weights	O
or	O
entire	O
neurons	O
based	O
on	O
an	O
importance	O
criterion	O
:	O
LeCun1990	Method
use	O
(	O
a	O
diagonal	Method
approximation	Method
to	O
)	O
the	O
Hessian	O
to	O
identify	O
weights	O
whose	O
removal	O
minimally	O
impacts	O
the	O
objective	O
function	O
,	O
while	O
Han2016	O
remove	O
weights	O
based	O
on	O
thresholding	O
their	O
absolute	O
values	O
.	O

Knowledge	Method
distillation	Method
approaches	Method
learn	O
a	O
smaller	O
student	Method
network	Method
to	O
mimic	O
the	O
original	O
teacher	Method
network	Method
by	O
minimizing	O
the	O
loss	Metric
(	O
typically	O
or	O
cross	Metric
-	Metric
entropy	Metric
)	O
between	O
the	O
student	O
and	O
teacher	O
output	O
.	O

In	O
this	O
work	O
,	O
we	O
investigate	O
knowledge	Task
distillation	Task
in	O
the	O
context	O
of	O
neural	Task
machine	Task
translation	Task
.	O

We	O
note	O
that	O
NMT	Task
differs	O
from	O
previous	O
work	O
which	O
has	O
mainly	O
explored	O
non	Method
-	Method
recurrent	Method
models	Method
in	O
the	O
multi	Task
-	Task
class	Task
prediction	Task
setting	Task
.	O

For	O
NMT	Task
,	O
while	O
the	O
model	O
is	O
trained	O
on	O
multi	Task
-	Task
class	Task
prediction	Task
at	O
the	O
word	O
-	O
level	O
,	O
it	O
is	O
tasked	O
with	O
predicting	O
complete	O
sequence	O
outputs	O
conditioned	O
on	O
previous	O
decisions	O
.	O

With	O
this	O
difference	O
in	O
mind	O
,	O
we	O
experiment	O
with	O
standard	O
knowledge	Method
distillation	Method
for	O
NMT	Task
and	O
also	O
propose	O
two	O
new	O
versions	O
of	O
the	O
approach	O
that	O
attempt	O
to	O
approximately	O
match	O
the	O
sequence	O
-	O
level	O
(	O
as	O
opposed	O
to	O
word	O
-	O
level	O
)	O
distribution	O
of	O
the	O
teacher	Method
network	Method
.	O

This	O
sequence	Method
-	Method
level	Method
approximation	Method
leads	O
to	O
a	O
simple	O
training	Method
procedure	Method
wherein	O
the	O
student	Method
network	Method
is	O
trained	O
on	O
a	O
newly	O
generated	O
dataset	O
that	O
is	O
the	O
result	O
of	O
running	O
beam	Method
search	Method
with	O
the	O
teacher	Method
network	Method
.	O

We	O
run	O
experiments	O
to	O
compress	O
a	O
large	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
LSTM	Method
model	Method
,	O
and	O
find	O
that	O
with	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
we	O
are	O
able	O
to	O
learn	O
a	O
LSTM	Method
that	O
roughly	O
matches	O
the	O
performance	O
of	O
the	O
full	O
system	O
.	O

We	O
see	O
similar	O
results	O
compressing	O
a	O
model	O
down	O
to	O
on	O
a	O
smaller	O
data	O
set	O
.	O

Furthermore	O
,	O
we	O
observe	O
that	O
our	O
proposed	O
approach	O
has	O
other	O
benefits	O
,	O
such	O
as	O
not	O
requiring	O
any	O
beam	O
search	O
at	O
test	O
-	O
time	O
.	O

As	O
a	O
result	O
we	O
are	O
able	O
to	O
perform	O
greedy	Method
decoding	Method
on	O
the	O
model	O
times	O
faster	O
than	O
beam	Method
search	Method
on	O
the	O
model	O
with	O
comparable	O
performance	O
.	O

Our	O
student	Method
models	Method
can	O
even	O
be	O
run	O
efficiently	O
on	O
a	O
standard	O
smartphone	O
.	O

Finally	O
,	O
we	O
apply	O
weight	Method
pruning	Method
on	O
top	O
of	O
the	O
student	Method
network	Method
to	O
obtain	O
a	O
model	O
that	O
has	O
fewer	O
parameters	O
than	O
the	O
original	O
teacher	Method
model	Method
.	O

We	O
have	O
released	O
all	O
the	O
code	O
for	O
the	O
models	O
described	O
in	O
this	O
paper	O
.	O

section	O
:	O
Background	O
subsection	O
:	O
Sequence	Task
-	Task
to	Task
-	Task
Sequence	Task
with	O
Attention	O
Let	O
and	O
be	O
(	O
random	O
variable	O
sequences	O
representing	O
)	O
the	O
source	O
/	O
target	O
sentence	O
,	O
with	O
and	O
respectively	O
being	O
the	O
source	O
/	O
target	O
lengths	O
.	O

Machine	Task
translation	Task
involves	O
finding	O
the	O
most	O
probable	O
target	O
sentence	O
given	O
the	O
source	O
:	O
where	O
is	O
the	O
set	O
of	O
all	O
possible	O
sequences	O
.	O

NMT	Task
models	O
parameterize	O
with	O
an	O
encoder	Method
neural	Method
network	Method
which	O
reads	O
the	O
source	O
sentence	O
and	O
a	O
decoder	Method
neural	Method
network	Method
which	O
produces	O
a	O
distribution	O
over	O
the	O
target	O
sentence	O
(	O
one	O
word	O
at	O
a	O
time	O
)	O
given	O
the	O
source	O
.	O

We	O
employ	O
the	O
attentional	Method
architecture	Method
from	O
Luong2015	O
,	O
which	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
English	Material
German	Material
translation	O
.	O

subsection	O
:	O
Knowledge	Task
Distillation	Task
Knowledge	Method
distillation	Method
describes	O
a	O
class	O
of	O
methods	O
for	O
training	O
a	O
smaller	O
student	Method
network	Method
to	O
perform	O
better	O
by	O
learning	O
from	O
a	O
larger	O
teacher	Method
network	Method
(	O
in	O
addition	O
to	O
learning	O
from	O
the	O
training	O
data	O
set	O
)	O
.	O

We	O
generally	O
assume	O
that	O
the	O
teacher	O
has	O
previously	O
been	O
trained	O
,	O
and	O
that	O
we	O
are	O
estimating	O
parameters	O
for	O
the	O
student	O
.	O

Knowledge	Task
distillation	Task
suggests	O
training	O
by	O
matching	O
the	O
student	O
’s	O
predictions	O
to	O
the	O
teacher	O
’s	O
predictions	O
.	O

For	O
classification	Task
this	O
usually	O
means	O
matching	O
the	O
probabilities	O
either	O
via	O
on	O
the	O
scale	O
or	O
by	O
cross	Method
-	Method
entropy	Method
.	O

Concretely	O
,	O
assume	O
we	O
are	O
learning	O
a	O
multi	Method
-	Method
class	Method
classifier	Method
over	O
a	O
data	O
set	O
of	O
examples	O
of	O
the	O
form	O
with	O
possible	O
classes	O
.	O

The	O
usual	O
training	Metric
criteria	Metric
is	O
to	O
minimize	O
NLL	Method
for	O
each	O
example	O
from	O
the	O
training	O
data	O
,	O
where	O
is	O
the	O
indicator	O
function	O
and	O
the	O
distribution	O
from	O
our	O
model	O
(	O
parameterized	O
by	O
)	O
.	O

This	O
objective	O
can	O
be	O
seen	O
as	O
minimizing	O
the	O
cross	Metric
-	Metric
entropy	Metric
between	O
the	O
degenerate	O
data	O
distribution	O
(	O
which	O
has	O
all	O
of	O
its	O
probability	O
mass	O
on	O
one	O
class	O
)	O
and	O
the	O
model	Method
distribution	Method
.	O

In	O
knowledge	Task
distillation	Task
,	O
we	O
assume	O
access	O
to	O
a	O
learned	O
teacher	O
distribution	O
,	O
possibly	O
trained	O
over	O
the	O
same	O
data	O
set	O
.	O

Instead	O
of	O
minimizing	O
cross	O
-	O
entropy	O
with	O
the	O
observed	O
data	O
,	O
we	O
instead	O
minimize	O
the	O
cross	Metric
-	Metric
entropy	Metric
with	O
the	O
teacher	O
’s	O
probability	O
distribution	O
,	O
where	O
parameterizes	O
the	O
teacher	O
distribution	O
and	O
remains	O
fixed	O
.	O

Note	O
the	O
cross	Method
-	Method
entropy	Method
setup	Method
is	O
identical	O
,	O
but	O
the	O
target	O
distribution	O
is	O
no	O
longer	O
a	O
sparse	O
distribution	O
.	O

Training	O
on	O
is	O
attractive	O
since	O
it	O
gives	O
more	O
information	O
about	O
other	O
classes	O
for	O
a	O
given	O
data	O
point	O
(	O
e.g.	O
similarity	O
between	O
classes	O
)	O
and	O
has	O
less	O
variance	O
in	O
gradients	O
.	O

Since	O
this	O
new	O
objective	O
has	O
no	O
direct	O
term	O
for	O
the	O
training	O
data	O
,	O
it	O
is	O
common	O
practice	O
to	O
interpolate	O
between	O
the	O
two	O
losses	O
,	O
where	O
is	O
mixture	O
parameter	O
combining	O
the	O
one	Method
-	Method
hot	Method
distribution	Method
and	O
the	O
teacher	Method
distribution	Method
.	O

section	O
:	O
Knowledge	Task
Distillation	Task
for	O
NMT	Task
The	O
large	O
sizes	O
of	O
neural	Method
machine	Method
translation	Method
systems	Method
make	O
them	O
an	O
ideal	O
candidate	O
for	O
knowledge	Method
distillation	Method
approaches	Method
.	O

In	O
this	O
section	O
we	O
explore	O
three	O
different	O
ways	O
this	O
technique	O
can	O
be	O
applied	O
to	O
NMT	Task
.	O

subsection	O
:	O
Word	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
NMT	Task
systems	O
are	O
trained	O
directly	O
to	O
minimize	O
word	O
NLL	O
,	O
,	O
at	O
each	O
position	O
.	O

Therefore	O
if	O
we	O
have	O
a	O
teacher	Method
model	Method
,	O
standard	O
knowledge	Method
distillation	Method
for	O
multi	Task
-	Task
class	Task
cross	Task
-	Task
entropy	Task
can	O
be	O
applied	O
.	O

We	O
define	O
this	O
distillation	Method
for	O
a	O
sentence	O
as	O
,	O
where	O
is	O
the	O
target	O
vocabulary	O
set	O
.	O

The	O
student	O
can	O
further	O
be	O
trained	O
to	O
optimize	O
the	O
mixture	O
of	O
and	O
.	O

In	O
the	O
context	O
of	O
NMT	Task
,	O
we	O
refer	O
to	O
this	O
approach	O
as	O
word	Task
-	Task
level	Task
knowledge	Task
distillation	Task
and	O
illustrate	O
this	O
in	O
Figure	O
1	O
(	O
left	O
)	O
.	O

subsection	O
:	O
Sequence	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
Word	Method
-	Method
level	Method
knowledge	Method
distillation	Method
allows	O
transfer	O
of	O
these	O
local	O
word	O
distributions	O
.	O

Ideally	O
however	O
,	O
we	O
would	O
like	O
the	O
student	Method
model	Method
to	O
mimic	O
the	O
teacher	O
’s	O
actions	O
at	O
the	O
sequence	O
-	O
level	O
.	O

The	O
sequence	O
distribution	O
is	O
particularly	O
important	O
for	O
NMT	Task
,	O
because	O
wrong	O
predictions	O
can	O
propagate	O
forward	O
at	O
test	O
-	O
time	O
.	O

First	O
,	O
consider	O
the	O
sequence	O
-	O
level	O
distribution	O
specified	O
by	O
the	O
model	O
over	O
all	O
possible	O
sequences	O
,	O
for	O
any	O
length	O
.	O

The	O
sequence	O
-	O
level	O
negative	O
log	O
-	O
likelihood	O
for	O
NMT	Task
then	O
involves	O
matching	O
the	O
one	Method
-	Method
hot	Method
distribution	Method
over	O
all	O
complete	O
sequences	O
,	O
where	O
is	O
the	O
observed	O
sequence	O
.	O

Of	O
course	O
,	O
this	O
just	O
shows	O
that	O
from	O
a	O
negative	Method
log	Method
likelihood	Method
perspective	Method
,	O
minimizing	O
word	Method
-	Method
level	Method
NLL	Method
and	O
sequence	Method
-	Method
level	Method
NLL	Method
are	O
equivalent	O
in	O
this	O
model	O
.	O

But	O
now	O
consider	O
the	O
case	O
of	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
.	O

As	O
before	O
,	O
we	O
can	O
simply	O
replace	O
the	O
distribution	O
from	O
the	O
data	O
with	O
a	O
probability	Method
distribution	Method
derived	O
from	O
our	O
teacher	Method
model	Method
.	O

However	O
,	O
instead	O
of	O
using	O
a	O
single	O
word	Method
prediction	Method
,	O
we	O
use	O
to	O
represent	O
the	O
teacher	O
’s	O
sequence	O
distribution	O
over	O
the	O
sample	O
space	O
of	O
all	O
possible	O
sequences	O
,	O
Note	O
that	O
is	O
inherently	O
different	O
from	O
,	O
as	O
the	O
sum	O
is	O
over	O
an	O
exponential	O
number	O
of	O
terms	O
.	O

Despite	O
its	O
intractability	O
,	O
we	O
posit	O
that	O
this	O
sequence	O
-	O
level	O
objective	O
is	O
worthwhile	O
.	O

It	O
gives	O
the	O
teacher	O
the	O
chance	O
to	O
assign	O
probabilities	O
to	O
complete	O
sequences	O
and	O
therefore	O
transfer	O
a	O
broader	O
range	O
of	O
knowledge	O
.	O

We	O
thus	O
consider	O
an	O
approximation	O
of	O
this	O
objective	O
.	O

Our	O
simplest	O
approximation	O
is	O
to	O
replace	O
the	O
teacher	Method
distribution	Method
with	O
its	O
mode	O
,	O
Observing	O
that	O
finding	O
the	O
mode	O
is	O
itself	O
intractable	O
,	O
we	O
use	O
beam	Method
search	Method
to	O
find	O
an	O
approximation	O
.	O

The	O
loss	O
is	O
then	O
where	O
is	O
now	O
the	O
output	O
from	O
running	O
beam	Method
search	Method
with	O
the	O
teacher	Method
model	Method
.	O

Using	O
the	O
mode	O
seems	O
like	O
a	O
poor	O
approximation	O
for	O
the	O
teacher	Task
distribution	Task
,	O
as	O
we	O
are	O
approximating	O
an	O
exponentially	Method
-	Method
sized	Method
distribution	Method
with	O
a	O
single	O
sample	O
.	O

However	O
,	O
previous	O
results	O
showing	O
the	O
effectiveness	O
of	O
beam	Method
search	Method
decoding	Method
for	O
NMT	Task
lead	O
us	O
to	O
belief	O
that	O
a	O
large	O
portion	O
of	O
’s	O
mass	O
lies	O
in	O
a	O
single	O
output	O
sequence	O
.	O

In	O
fact	O
,	O
in	O
experiments	O
we	O
find	O
that	O
with	O
beam	O
of	O
size	O
,	O
(	O
on	O
average	O
)	O
accounts	O
for	O
of	O
the	O
distribution	O
for	O
German	Material
English	Material
,	O
and	O
for	O
Thai	Material
English	Material
(	O
Table	O
1	O
:	O
)	O
.	O

To	O
summarize	O
,	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
suggests	O
to	O
:	O
(	O
1	O
)	O
train	O
a	O
teacher	Method
model	Method
,	O
(	O
2	O
)	O
run	O
beam	Method
search	Method
over	O
the	O
training	O
set	O
with	O
this	O
model	O
,	O
(	O
3	O
)	O
train	O
the	O
student	Method
network	Method
with	O
cross	Method
-	Method
entropy	Method
on	O
this	O
new	O
dataset	O
.	O

Step	O
(	O
3	O
)	O
is	O
identical	O
to	O
the	O
word	Method
-	Method
level	Method
NLL	Method
process	Method
except	O
now	O
on	O
the	O
newly	O
-	O
generated	O
data	O
set	O
.	O

This	O
is	O
shown	O
in	O
Figure	O
1	O
(	O
center	O
)	O
.	O

subsection	O
:	O
Sequence	Method
-	Method
Level	Method
Interpolation	Method
Next	O
we	O
consider	O
integrating	O
the	O
training	O
data	O
back	O
into	O
the	O
process	O
,	O
such	O
that	O
we	O
train	O
the	O
student	Method
model	Method
as	O
a	O
mixture	O
of	O
our	O
sequence	O
-	O
level	O
teacher	O
-	O
generated	O
data	O
(	O
)	O
with	O
the	O
original	O
training	O
data	O
(	O
)	O
,	O
where	O
is	O
the	O
gold	O
target	O
sequence	O
.	O

Since	O
the	O
second	O
term	O
is	O
intractable	O
,	O
we	O
could	O
again	O
apply	O
the	O
mode	Method
approximation	Method
from	O
the	O
previous	O
section	O
,	O
and	O
train	O
on	O
both	O
observed	O
(	O
)	O
and	O
teacher	O
-	O
generated	O
(	O
)	O
data	O
.	O

However	O
,	O
this	O
process	O
is	O
non	O
-	O
ideal	O
for	O
two	O
reasons	O
:	O
(	O
1	O
)	O
unlike	O
for	O
standard	O
knowledge	Method
distribution	Method
,	O
it	O
doubles	O
the	O
size	O
of	O
the	O
training	O
data	O
,	O
and	O
(	O
2	O
)	O
it	O
requires	O
training	O
on	O
both	O
the	O
teacher	O
-	O
generated	O
sequence	O
and	O
the	O
true	O
sequence	O
,	O
conditioned	O
on	O
the	O
same	O
source	O
input	O
.	O

The	O
latter	O
concern	O
is	O
particularly	O
problematic	O
since	O
we	O
observe	O
that	O
and	O
are	O
often	O
quite	O
different	O
.	O

As	O
an	O
alternative	O
,	O
we	O
propose	O
a	O
single	Method
-	Method
sequence	Method
approximation	Method
that	O
is	O
more	O
attractive	O
in	O
this	O
setting	O
.	O

This	O
approach	O
is	O
inspired	O
by	O
local	Method
updating	Method
,	O
a	O
method	O
for	O
discriminative	Task
training	Task
in	O
statistical	Task
machine	Task
translation	Task
(	O
although	O
to	O
our	O
knowledge	O
not	O
for	O
knowledge	Task
distillation	Task
)	O
.	O

Local	Method
updating	Method
suggests	O
selecting	O
a	O
training	O
sequence	O
which	O
is	O
close	O
to	O
and	O
has	O
high	O
probability	O
under	O
the	O
teacher	Method
model	Method
,	O
where	O
is	O
a	O
function	Metric
measuring	Metric
closeness	Metric
(	O
e.g.	O
Jaccard	Metric
similarity	Metric
or	O
BLEU	Metric
)	O
.	O

Following	O
local	Method
updating	Method
,	O
we	O
can	O
approximate	O
this	O
sequence	O
by	O
running	O
beam	Method
search	Method
and	O
choosing	O
where	O
is	O
the	O
-	O
best	O
list	O
from	O
beam	Method
search	Method
.	O

We	O
take	O
to	O
be	O
smoothed	O
sentence	O
-	O
level	O
BLEU	Metric
.	O

We	O
justify	O
training	O
on	O
from	O
a	O
knowledge	Task
distillation	Task
perspective	Task
with	O
the	O
following	O
generative	Method
process	Method
:	O
suppose	O
that	O
there	O
is	O
a	O
true	O
target	O
sequence	O
(	O
which	O
we	O
do	O
not	O
observe	O
)	O
that	O
is	O
first	O
generated	O
from	O
the	O
underlying	O
data	O
distribution	O
.	O

And	O
further	O
suppose	O
that	O
the	O
target	O
sequence	O
that	O
we	O
observe	O
(	O
)	O
is	O
a	O
noisy	O
version	O
of	O
the	O
unobserved	O
true	O
sequence	O
:	O
i.e.	O
(	O
i	O
)	O
,	O
(	O
ii	O
)	O
,	O
where	O
is	O
,	O
for	O
example	O
,	O
a	O
noise	O
function	O
that	O
independently	O
replaces	O
each	O
element	O
in	O
with	O
a	O
random	O
element	O
in	O
with	O
some	O
small	O
probability	O
.	O

In	O
such	O
a	O
case	O
,	O
ideally	O
the	O
student	Method
’s	Method
distribution	Method
should	O
match	O
the	O
mixture	Method
distribution	Method
,	O
In	O
this	O
setting	O
,	O
due	O
to	O
the	O
noise	O
assumption	O
,	O
now	O
has	O
significant	O
probability	O
mass	O
around	O
a	O
neighborhood	O
of	O
(	O
not	O
just	O
at	O
)	O
,	O
and	O
therefore	O
the	O
of	O
the	O
mixture	Method
distribution	Method
is	O
likely	O
something	O
other	O
than	O
(	O
the	O
observed	O
sequence	O
)	O
or	O
(	O
the	O
output	O
from	O
beam	Method
search	Method
)	O
.	O

We	O
can	O
see	O
that	O
is	O
a	O
natural	O
approximation	O
to	O
the	O
of	O
this	O
mixture	O
distribution	O
between	O
and	O
for	O
some	O
.	O

We	O
illustrate	O
this	O
framework	O
in	O
Figure	O
1	O
(	O
right	O
)	O
and	O
visualize	O
the	O
distribution	O
over	O
a	O
real	O
example	O
in	O
Figure	O
2	O
.	O

section	O
:	O
Experimental	O
Setup	O
To	O
test	O
out	O
these	O
approaches	O
,	O
we	O
conduct	O
two	O
sets	O
of	O
NMT	Task
experiments	O
:	O
high	O
resource	O
(	O
English	Material
German	Material
)	O
and	O
low	O
resource	O
(	O
Thai	Material
English	Material
)	O
.	O

The	O
English	Material
-	Material
German	Material
data	Material
comes	O
from	O
WMT	Material
2014	Material
.	O

The	O
training	O
set	O
has	O
m	O
sentences	O
and	O
we	O
take	O
newstest2012	O
/	O
newstest2013	O
as	O
the	O
dev	O
set	O
and	O
newstest2014	O
as	O
the	O
test	O
set	O
.	O

We	O
keep	O
the	O
top	O
k	O
most	O
frequent	O
words	O
,	O
and	O
replace	O
the	O
rest	O
with	O
UNK	O
.	O

The	O
teacher	Method
model	Method
is	O
a	O
LSTM	Method
(	O
as	O
in	O
Luong2015	O
)	O
and	O
we	O
train	O
two	O
student	Method
models	Method
:	O
and	O
.	O

The	O
Thai	Material
-	Material
English	Material
data	Material
comes	O
from	O
IWSLT	Material
2015	Material
.	O

There	O
are	O
k	O
sentences	O
in	O
the	O
training	O
set	O
and	O
we	O
take	O
2010	O
/	O
2011	O
/	O
2012	O
data	O
as	O
the	O
dev	O
set	O
and	O
2012	O
/	O
2013	O
as	O
the	O
test	O
set	O
,	O
with	O
a	O
vocabulary	O
size	O
is	O
k.	O
Size	O
of	O
the	O
teacher	Method
model	Method
is	O
(	O
which	O
performed	O
better	O
than	O
,	O
models	O
)	O
,	O
and	O
the	O
student	Method
model	Method
is	O
.	O

Other	O
training	O
details	O
mirror	O
Luong2015	O
.	O

We	O
evaluate	O
on	O
tokenized	Metric
BLEU	Metric
with	O
multi	O
-	O
bleu.perl	O
,	O
and	O
experiment	O
with	O
the	O
following	O
variations	O
:	O
paragraph	O
:	O
Word	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
(	O
Word	Method
-	Method
KD	Method
)	O
Student	Method
is	O
trained	O
on	O
the	O
original	O
data	O
and	O
additionally	O
trained	O
to	O
minimize	O
the	O
cross	O
-	O
entropy	O
of	O
the	O
teacher	O
distribution	O
at	O
the	O
word	O
-	O
level	O
.	O

We	O
tested	O
and	O
found	O
to	O
work	O
better	O
.	O

paragraph	O
:	O
Sequence	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
(	O
Seq	Method
-	Method
KD	Method
)	O
Student	Method
is	O
trained	O
on	O
the	O
teacher	O
-	O
generated	O
data	O
,	O
which	O
is	O
the	O
result	O
of	O
running	O
beam	Method
search	Method
and	O
taking	O
the	O
highest	O
-	O
scoring	O
sequence	O
with	O
the	O
teacher	Method
model	Method
.	O

We	O
use	O
beam	O
size	O
(	O
we	O
did	O
not	O
see	O
improvements	O
with	O
a	O
larger	O
beam	O
)	O
.	O

paragraph	O
:	O
Sequence	Method
-	Method
Level	Method
Interpolation	Method
(	O
Seq	Method
-	Method
Inter	Method
)	O
Student	O
is	O
trained	O
on	O
the	O
sequence	O
on	O
the	O
teacher	O
’s	O
beam	O
that	O
had	O
the	O
highest	O
BLEU	Metric
(	O
beam	O
size	O
)	O
.	O

We	O
adopt	O
a	O
fine	Method
-	Method
tuning	Method
approach	Method
where	O
we	O
begin	O
training	O
from	O
a	O
pretrained	Method
model	Method
(	O
either	O
on	O
original	O
data	O
or	O
Seq	Method
-	Method
KD	Method
data	O
)	O
and	O
train	O
with	O
a	O
smaller	O
learning	Metric
rate	Metric
(	O
)	O
.	O

For	O
English	Material
-	Material
German	Material
we	O
generate	O
Seq	Method
-	Method
Inter	Method
data	O
on	O
a	O
smaller	O
portion	O
of	O
the	O
training	O
set	O
(	O
)	O
for	O
efficiency	O
.	O

The	O
above	O
methods	O
are	O
complementary	O
and	O
can	O
be	O
combined	O
with	O
each	O
other	O
.	O

For	O
example	O
,	O
we	O
can	O
train	O
on	O
teacher	O
-	O
generated	O
data	O
but	O
still	O
include	O
a	O
word	O
-	O
level	O
cross	O
-	O
entropy	O
term	O
between	O
the	O
teacher	O
/	O
student	O
(	O
Seq	Method
-	Method
KD	Method
Word	Method
-	Method
KD	Method
in	O
Table	O
1	O
)	O
,	O
or	O
fine	O
-	O
tune	O
towards	O
Seq	Method
-	Method
Inter	Method
data	O
starting	O
from	O
the	O
baseline	Method
model	Method
trained	O
on	O
original	O
data	O
(	O
Baseline	O
Seq	Method
-	Method
Inter	Method
in	O
Table	O
1	O
)	O
.	O

section	O
:	O
Results	O
and	O
Discussion	O
Results	O
of	O
our	O
experiments	O
are	O
shown	O
in	O
Table	O
1	O
.	O

We	O
find	O
that	O
while	O
word	O
-	O
level	O
knowledge	O
distillation	O
(	O
Word	Method
-	Method
KD	Method
)	O
does	O
improve	O
upon	O
the	O
baseline	O
,	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
(	O
Seq	Method
-	Method
KD	Method
)	O
does	O
better	O
on	O
English	Material
German	Material
and	O
performs	O
similarly	O
on	O
Thai	Material
English	Material
.	O

Combining	O
them	O
(	O
Seq	Method
-	Method
KD	Method
Word	Method
-	Method
KD	Method
)	O
results	O
in	O
further	O
gains	O
for	O
the	O
and	Method
models	Method
(	O
although	O
not	O
for	O
the	O
model	O
)	O
,	O
indicating	O
that	O
these	O
methods	O
provide	O
orthogonal	O
means	O
of	O
transferring	O
knowledge	O
from	O
the	O
teacher	O
to	O
the	O
student	O
:	O
Word	Method
-	Method
KD	Method
is	O
transferring	O
knowledge	O
at	O
the	O
the	O
local	O
(	O
i.e.	O
word	O
)	O
level	O
while	O
Seq	Method
-	Method
KD	Method
is	O
transferring	O
knowledge	O
at	O
the	O
global	O
(	O
i.e.	O
sequence	O
)	O
level	O
.	O

Sequence	Method
-	Method
level	Method
interpolation	Method
(	O
Seq	Method
-	Method
Inter	Method
)	O
,	O
in	O
addition	O
to	O
improving	O
models	O
trained	O
via	O
Word	Method
-	Method
KD	Method
and	O
Seq	Method
-	Method
KD	Method
,	O
also	O
improves	O
upon	O
the	O
original	O
teacher	Method
model	Method
that	O
was	O
trained	O
on	O
the	O
actual	O
data	O
but	O
fine	O
-	O
tuned	O
towards	O
Seq	Method
-	Method
Inter	Method
data	O
(	O
Baseline	O
Seq	Method
-	Method
Inter	Method
)	O
.	O

In	O
fact	O
,	O
greedy	Method
decoding	Method
with	O
this	O
fine	Method
-	Method
tuned	Method
model	Method
has	O
similar	O
performance	O
(	O
)	O
as	O
beam	Method
search	Method
with	O
the	O
original	O
model	O
(	O
)	O
,	O
allowing	O
for	O
faster	O
decoding	Task
even	O
with	O
an	O
identically	Method
-	Method
sized	Method
model	Method
.	O

We	O
hypothesize	O
that	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
is	O
effective	O
because	O
it	O
allows	O
the	O
student	Method
network	Method
to	O
only	O
model	O
relevant	O
parts	O
of	O
the	O
teacher	O
distribution	O
(	O
i.e.	O
around	O
the	O
teacher	O
’s	O
mode	O
)	O
instead	O
of	O
‘	O
wasting	O
’	O
parameters	O
on	O
trying	O
to	O
model	O
the	O
entire	O
space	O
of	O
translations	O
.	O

Our	O
results	O
suggest	O
that	O
this	O
is	O
indeed	O
the	O
case	O
:	O
the	O
probability	O
mass	O
that	O
Seq	Method
-	Method
KD	Method
models	O
assign	O
to	O
the	O
approximate	O
mode	O
is	O
much	O
higher	O
than	O
is	O
the	O
case	O
for	O
baseline	Method
models	Method
trained	O
on	O
original	O
data	O
(	O
Table	O
1	O
:	O
)	O
.	O

For	O
example	O
,	O
on	O
English	Material
German	Material
the	O
(	O
approximate	O
)	O
for	O
the	O
Seq	Method
-	Method
KD	Method
model	O
(	O
on	O
average	O
)	O
accounts	O
for	O
of	O
the	O
total	O
probability	O
mass	O
,	O
while	O
the	O
corresponding	O
number	O
is	O
for	O
the	O
baseline	O
.	O

This	O
also	O
explains	O
the	O
success	O
of	O
greedy	Method
decoding	Method
for	O
Seq	Method
-	Method
KD	Method
models	O
—	O
since	O
we	O
are	O
only	O
modeling	O
around	O
the	O
teacher	O
’s	O
mode	O
,	O
the	O
student	Method
’s	Method
distribution	Method
is	O
more	O
peaked	O
and	O
therefore	O
the	O
is	O
much	O
easier	O
to	O
find	O
.	O

Seq	Method
-	Method
Inter	Method
offers	O
a	O
compromise	O
between	O
the	O
two	O
,	O
with	O
the	O
greedily	O
-	O
decoded	O
sequence	O
accounting	O
for	O
of	O
the	O
distribution	O
.	O

Finally	O
,	O
although	O
past	O
work	O
has	O
shown	O
that	O
models	O
with	O
lower	O
perplexity	O
generally	O
tend	O
to	O
have	O
higher	O
BLEU	Metric
,	O
our	O
results	O
indicate	O
that	O
this	O
is	O
not	O
necessarily	O
the	O
case	O
.	O

The	O
perplexity	Metric
of	O
the	O
baseline	O
English	Material
German	Material
model	O
is	O
while	O
the	O
perplexity	O
of	O
the	O
corresponding	O
Seq	Method
-	Method
KD	Method
model	O
is	O
,	O
despite	O
the	O
fact	O
that	O
Seq	Method
-	Method
KD	Method
model	O
does	O
significantly	O
better	O
for	O
both	O
greedy	O
(	O
BLEU	Metric
)	O
and	O
beam	O
search	O
(	O
BLEU	Metric
)	O
decoding	O
.	O

subsection	O
:	O
Decoding	Metric
Speed	Metric
Run	Metric
-	Metric
time	Metric
complexity	Metric
for	O
beam	Task
search	Task
grows	O
linearly	O
with	O
beam	O
size	O
.	O

Therefore	O
,	O
the	O
fact	O
that	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
allows	O
for	O
greedy	Task
decoding	Task
is	O
significant	O
,	O
with	O
practical	O
implications	O
for	O
running	O
NMT	Task
systems	O
across	O
various	O
devices	O
.	O

To	O
test	O
the	O
speed	O
gains	O
,	O
we	O
run	O
the	O
teacher	Method
/	Method
student	Method
models	Method
on	O
GPU	Method
,	O
CPU	O
,	O
and	O
smartphone	O
,	O
and	O
check	O
the	O
average	O
number	O
of	O
source	O
words	O
translated	O
per	O
second	O
(	O
Table	O
2	O
)	O
.	O

We	O
use	O
a	O
GeForce	Method
GTX	Method
Titan	Method
X	Method
for	O
GPU	Method
and	O
a	O
Samsung	O
Galaxy	O
6	O
smartphone	O
.	O

We	O
find	O
that	O
we	O
can	O
run	O
the	O
student	Method
model	Method
times	O
faster	O
with	O
greedy	Method
decoding	Method
than	O
the	O
teacher	Method
model	Method
with	O
beam	Method
search	Method
on	O
GPU	O
(	O
vs	O
words	O
/	O
sec	O
)	O
,	O
with	O
similar	O
performance	O
.	O

subsection	O
:	O
Weight	Method
Pruning	Method
Although	O
knowledge	Method
distillation	Method
enables	O
training	O
faster	O
models	O
,	O
the	O
number	O
of	O
parameters	O
for	O
the	O
student	Method
models	Method
is	O
still	O
somewhat	O
large	O
(	O
Table	O
1	O
:	O
Params	O
)	O
,	O
due	O
to	O
the	O
word	O
embeddings	O
which	O
dominate	O
most	O
of	O
the	O
parameters	O
.	O

For	O
example	O
,	O
on	O
the	O
English	Material
German	Material
model	O
the	O
word	Method
embeddings	Method
account	O
for	O
approximately	O
(	O
m	O
out	O
of	O
m	O
)	O
of	O
the	O
parameters	O
.	O

The	O
size	O
of	O
word	O
embeddings	O
have	O
little	O
impact	O
on	O
run	Metric
-	Metric
time	Metric
as	O
the	O
word	Method
embedding	Method
layer	Method
is	O
a	O
simple	O
lookup	O
table	O
that	O
only	O
affects	O
the	O
first	O
layer	O
of	O
the	O
model	O
.	O

We	O
therefore	O
focus	O
next	O
on	O
reducing	O
the	O
memory	Metric
footprint	Metric
of	O
the	O
student	Method
models	Method
further	O
through	O
weight	Method
pruning	Method
.	O

Weight	Method
pruning	Method
for	O
NMT	Task
was	O
recently	O
investigated	O
by	O
See2016	O
,	O
who	O
found	O
that	O
up	O
to	O
of	O
the	O
parameters	O
in	O
a	O
large	O
NMT	Task
model	O
can	O
be	O
pruned	O
with	O
little	O
loss	O
in	O
performance	O
.	O

We	O
take	O
our	O
best	O
English	Material
German	Material
student	O
model	O
(	O
Seq	Method
-	Method
KD	Method
Seq	Method
-	Method
Inter	Method
)	O
and	O
prune	O
of	O
the	O
parameters	O
by	O
removing	O
the	O
weights	O
with	O
the	O
lowest	O
absolute	O
values	O
.	O

We	O
then	O
retrain	O
the	O
pruned	Method
model	Method
on	O
Seq	Method
-	Method
KD	Method
data	O
with	O
a	O
learning	Metric
rate	Metric
of	O
and	O
fine	O
-	O
tune	O
towards	O
Seq	Method
-	Method
Inter	Method
data	O
with	O
a	O
learning	Metric
rate	Metric
of	O
.	O

As	O
observed	O
by	O
See2016	O
,	O
retraining	O
proved	O
to	O
be	O
crucial	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
3	O
.	O

Our	O
findings	O
suggest	O
that	O
compression	O
benefits	O
achieved	O
through	O
weight	Method
pruning	Method
and	O
knowledge	Method
distillation	Method
are	O
orthogonal	O
.	O

Pruning	O
of	O
the	O
weight	O
in	O
the	O
student	Method
model	Method
results	O
in	O
a	O
model	O
with	O
fewer	O
parameters	O
than	O
the	O
original	O
teacher	Method
model	Method
with	O
only	O
a	O
decrease	O
of	O
BLEU	Metric
.	O

While	O
pruning	O
of	O
the	O
weights	O
results	O
in	O
a	O
more	O
appreciable	O
decrease	O
of	O
BLEU	Metric
,	O
the	O
model	O
is	O
drastically	O
smaller	O
with	O
m	O
parameters	O
,	O
which	O
is	O
fewer	O
than	O
the	O
original	O
teacher	Method
model	Method
.	O

subsection	O
:	O
Further	O
Observations	O
For	O
models	O
trained	O
with	O
word	Task
-	Task
level	Task
knowledge	Task
distillation	Task
,	O
we	O
also	O
tried	O
regressing	O
the	O
student	Method
network	Method
’s	O
top	O
-	O
most	O
hidden	O
layer	O
at	O
each	O
time	O
step	O
to	O
the	O
teacher	Method
network	Method
’s	O
top	O
-	O
most	O
hidden	O
layer	O
as	O
a	O
pretraining	O
step	O
,	O
noting	O
that	O
Romero2015	O
obtained	O
improvements	O
with	O
a	O
similar	O
technique	O
on	O
feed	Method
-	Method
forward	Method
models	Method
.	O

We	O
found	O
this	O
to	O
give	O
comparable	O
results	O
to	O
standard	O
knowledge	Method
distillation	Method
and	O
hence	O
did	O
not	O
pursue	O
this	O
further	O
.	O

There	O
have	O
been	O
promising	O
recent	O
results	O
on	O
eliminating	O
word	O
embeddings	O
completely	O
and	O
obtaining	O
word	Method
representations	Method
directly	O
from	O
characters	O
with	O
character	Method
composition	Method
models	Method
,	O
which	O
have	O
many	O
fewer	O
parameters	O
than	O
word	O
embedding	O
lookup	O
tables	O
.	O

Combining	O
such	O
methods	O
with	O
knowledge	Method
distillation	Method
/	Method
pruning	Method
to	O
further	O
reduce	O
the	O
memory	Metric
footprint	Metric
of	O
NMT	Task
systems	O
remains	O
an	O
avenue	O
for	O
future	O
work	O
.	O

section	O
:	O
Related	O
Work	O
Compressing	Method
deep	Method
learning	Method
models	Method
is	O
an	O
active	O
area	O
of	O
current	O
research	O
.	O

Pruning	Method
methods	Method
involve	O
pruning	O
weights	O
or	O
entire	O
neurons	O
/	O
nodes	O
based	O
on	O
some	O
criterion	O
.	O

LeCun1990	O
prune	O
weights	O
based	O
on	O
an	O
approximation	Method
of	Method
the	Method
Hessian	Method
,	O
while	O
Han2016	O
show	O
that	O
a	O
simple	O
magnitude	Method
-	Method
based	Method
pruning	Method
works	O
well	O
.	O

Prior	O
work	O
on	O
removing	O
neurons	O
/	O
nodes	O
include	O
Srinivas2015	O
and	O
Mariet2016	O
.	O

See2016	O
were	O
the	O
first	O
to	O
apply	O
pruning	Method
to	O
Neural	Task
Machine	Task
Translation	Task
,	O
observing	O
that	O
that	O
different	O
parts	O
of	O
the	O
architecture	O
(	O
input	O
word	O
embeddings	O
,	O
LSTM	O
matrices	O
,	O
etc	O
.	O

)	O
admit	O
different	O
levels	O
of	O
pruning	O
.	O

Knowledge	Method
distillation	Method
approaches	Method
train	O
a	O
smaller	O
student	Method
model	Method
to	O
mimic	O
a	O
larger	O
teacher	Method
model	Method
,	O
by	O
minimizing	O
the	O
loss	O
between	O
the	O
teacher	Method
/	Method
student	Method
predictions	Method
.	O

Romero2015	O
additionally	O
regress	O
on	O
the	O
intermediate	O
hidden	O
layers	O
of	O
the	O
student	Method
/	Method
teacher	Method
network	Method
as	O
a	O
pretraining	O
step	O
,	O
while	O
Mou2015	O
obtain	O
smaller	O
word	O
embeddings	O
from	O
a	O
teacher	Method
model	Method
via	O
regression	Method
.	O

There	O
has	O
also	O
been	O
work	O
on	O
transferring	O
knowledge	O
across	O
different	O
network	Method
architectures	Method
:	O
Chan2015b	O
show	O
that	O
a	O
deep	Method
non	Method
-	Method
recurrent	Method
neural	Method
network	Method
can	O
learn	O
from	O
an	O
RNN	Method
;	O
Geras2016	O
train	O
a	O
CNN	Method
to	O
mimic	O
an	O
LSTM	Method
for	O
speech	Task
recognition	Task
.	O

Kuncoro2016	O
recently	O
investigated	O
knowledge	Task
distillation	Task
for	O
structured	Task
prediction	Task
by	O
having	O
a	O
single	O
parser	Method
learn	O
from	O
an	O
ensemble	Method
of	Method
parsers	Method
.	O

Other	O
approaches	O
for	O
compression	Task
involve	O
low	O
rank	O
factorizations	O
of	O
weight	O
matrices	O
,	O
sparsity	Method
-	Method
inducing	Method
regularizers	Method
,	O
binarization	Method
of	Method
weights	Method
,	O
and	O
weight	Method
sharing	Method
.	O

Finally	O
,	O
although	O
we	O
have	O
motivated	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
in	O
the	O
context	O
of	O
training	O
a	O
smaller	O
model	O
,	O
there	O
are	O
other	O
techniques	O
that	O
train	O
on	O
a	O
mixture	O
of	O
the	O
model	O
’s	O
predictions	O
and	O
the	O
data	O
,	O
such	O
as	O
local	Method
updating	Method
,	O
hope	Method
/	Method
fear	Method
training	Method
,	O
SEARN	Method
,	O
DAgger	Method
,	O
and	O
minimum	Method
risk	Method
training	Method
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
we	O
have	O
investigated	O
existing	O
knowledge	Method
distillation	Method
methods	Method
for	O
NMT	Task
(	O
which	O
work	O
at	O
the	O
word	O
-	O
level	O
)	O
and	O
introduced	O
two	O
sequence	Method
-	Method
level	Method
variants	Method
of	O
knowledge	Method
distillation	Method
,	O
which	O
provide	O
improvements	O
over	O
standard	O
word	Method
-	Method
level	Method
knowledge	Method
distillation	Method
.	O

We	O
have	O
chosen	O
to	O
focus	O
on	O
translation	Task
as	O
this	O
domain	O
has	O
generally	O
required	O
the	O
largest	O
capacity	O
deep	Method
learning	Method
models	Method
,	O
but	O
the	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
framework	Method
has	O
been	O
successfully	O
applied	O
to	O
a	O
wide	O
range	O
of	O
tasks	O
including	O
parsing	Task
,	O
summarization	Task
,	O
dialogue	Task
,	O
NER	Task
/	Task
POS	Task
-	Task
tagging	Task
,	O
image	Task
captioning	Task
,	O
video	Task
generation	Task
,	O
and	O
speech	Task
recognition	Task
.	O

We	O
anticipate	O
that	O
methods	O
described	O
in	O
this	O
paper	O
can	O
be	O
used	O
to	O
similarly	O
train	O
smaller	Method
models	Method
in	O
other	O
domains	O
.	O

bibliography	O
:	O
References	O
