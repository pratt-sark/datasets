document	O
:	O
Scaling	Method
Memory	Method
-	Method
Augmented	Method
Neural	Method
Networks	Method
with	O
Sparse	Task
Reads	Task
and	Task
Writes	Task
Neural	Method
networks	Method
augmented	O
with	O
external	O
memory	O
have	O
the	O
ability	O
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
complex	O
tasks	O
.	O

These	O
models	O
appear	O
promising	O
for	O
applications	O
such	O
as	O
language	Task
modeling	Task
and	O
machine	Task
translation	Task
.	O

However	O
,	O
they	O
scale	O
poorly	O
in	O
both	O
space	Metric
and	O
time	Metric
as	O
the	O
amount	O
of	O
memory	O
grows	O
—	O
limiting	O
their	O
applicability	O
to	O
real	Task
-	Task
world	Task
domains	Task
.	O

Here	O
,	O
we	O
present	O
an	O
end	O
-	O
to	O
-	O
end	Method
differentiable	Method
memory	Method
access	Method
scheme	Method
,	O
which	O
we	O
call	O
Sparse	Method
Access	Method
Memory	Method
(	O
SAM	Method
)	O
,	O
that	O
retains	O
the	O
representational	O
power	O
of	O
the	O
original	O
approaches	O
whilst	O
training	O
efficiently	O
with	O
very	O
large	O
memories	O
.	O

We	O
show	O
that	O
SAM	Method
achieves	O
asymptotic	Metric
lower	Metric
bounds	Metric
in	O
space	Metric
and	O
time	Metric
complexity	O
,	O
and	O
find	O
that	O
an	O
implementation	O
runs	O
faster	O
and	O
with	O
less	O
physical	O
memory	O
than	O
non	Method
-	Method
sparse	Method
models	Method
.	O

SAM	Method
learns	O
with	O
comparable	O
data	Metric
efficiency	Metric
to	O
existing	O
models	O
on	O
a	O
range	O
of	O
synthetic	Task
tasks	Task
and	O
one	Task
-	Task
shot	Task
Omniglot	Task
character	Task
recognition	Task
,	O
and	O
can	O
scale	O
to	O
tasks	O
requiring	O
s	O
of	O
time	Metric
steps	O
and	O
memories	O
.	O

As	O
well	O
,	O
we	O
show	O
how	O
our	O
approach	O
can	O
be	O
adapted	O
for	O
models	O
that	O
maintain	O
temporal	O
associations	O
between	O
memories	O
,	O
as	O
with	O
the	O
recently	O
introduced	O
Differentiable	Method
Neural	Method
Computer	Method
.	O

capbtabboxtable	O
[	O
]	O
[	O
]	O
section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
,	O
such	O
as	O
the	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
,	O
have	O
proven	O
to	O
be	O
powerful	O
sequence	Method
learning	Method
models	Method
.	O

However	O
,	O
one	O
limitation	O
of	O
the	O
LSTM	Method
architecture	Method
is	O
that	O
the	O
number	O
of	O
parameters	O
grows	O
proportionally	O
to	O
the	O
square	O
of	O
the	O
size	O
of	O
the	O
memory	O
,	O
making	O
them	O
unsuitable	O
for	O
problems	O
requiring	O
large	O
amounts	O
of	O
long	O
-	O
term	O
memory	O
.	O

Recent	O
approaches	O
,	O
such	O
as	O
Neural	Method
Turing	Method
Machines	Method
(	O
NTMs	Method
)	O
and	O
Memory	Method
Networks	Method
,	O
have	O
addressed	O
this	O
issue	O
by	O
decoupling	O
the	O
memory	O
capacity	O
from	O
the	O
number	O
of	O
model	O
parameters	O
.	O

We	O
refer	O
to	O
this	O
class	O
of	O
models	O
as	O
memory	Method
augmented	Method
neural	Method
networks	Method
(	O
MANNs	Method
)	O
.	O

External	O
memory	O
allows	O
MANNs	Method
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
problems	O
that	O
have	O
eluded	O
the	O
capabilities	O
of	O
traditional	O
LSTMs	Method
,	O
and	O
to	O
generalize	O
to	O
longer	O
sequence	O
lengths	O
.	O

Nonetheless	O
,	O
MANNs	Method
have	O
had	O
limited	O
success	O
in	O
real	Task
world	Task
application	Task
.	O

A	O
significant	O
difficulty	O
in	O
training	O
these	O
models	O
results	O
from	O
their	O
smooth	O
read	O
and	O
write	O
operations	O
,	O
which	O
incur	O
linear	Metric
computational	Metric
overhead	Metric
on	O
the	O
number	O
of	O
memories	O
stored	O
per	O
time	Metric
step	O
of	O
training	O
.	O

Even	O
worse	O
,	O
they	O
require	O
duplication	O
of	O
the	O
entire	O
memory	O
at	O
each	O
time	Metric
step	O
to	O
perform	O
backpropagation	O
through	O
time	Metric
(	O
BPTT	Method
)	O
.	O

To	O
deal	O
with	O
sufficiently	O
complex	O
problems	O
,	O
such	O
as	O
processing	O
a	O
book	O
,	O
or	O
Wikipedia	Material
,	O
this	O
overhead	O
becomes	O
prohibitive	O
.	O

For	O
example	O
,	O
to	O
store	O
memories	O
,	O
a	O
straightforward	O
implementation	O
of	O
the	O
NTM	Method
trained	O
over	O
a	O
sequence	O
of	O
length	O
consumes	O
physical	O
memory	O
;	O
to	O
store	O
memories	O
the	O
overhead	O
exceeds	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
MANN	Method
named	O
SAM	Method
(	O
sparse	O
access	O
memory	O
)	O
.	O

By	O
thresholding	O
memory	O
modifications	O
to	O
a	O
sparse	O
subset	O
,	O
and	O
using	O
efficient	O
data	Method
structures	Method
for	O
content	Method
-	Method
based	Method
read	Method
operations	Method
,	O
our	O
model	O
is	O
optimal	O
in	O
space	Metric
and	O
time	Metric
with	O
respect	O
to	O
memory	Metric
size	Metric
,	O
while	O
retaining	O
end	O
-	O
to	O
-	O
end	Method
gradient	Method
based	Method
optimization	Method
.	O

To	O
test	O
whether	O
the	O
model	O
is	O
able	O
to	O
learn	O
with	O
this	O
sparse	Method
approximation	Method
,	O
we	O
examined	O
its	O
performance	O
on	O
a	O
selection	O
of	O
synthetic	O
and	O
natural	O
tasks	O
:	O
algorithmic	Task
tasks	Task
from	O
the	O
NTM	O
work	O
,	O
Babi	Material
reasoning	O
tasks	O
used	O
with	O
Memory	Method
Networks	Method
and	O
Omniglot	Task
one	Task
-	Task
shot	Task
classification	Task
.	O

We	O
also	O
tested	O
several	O
of	O
these	O
tasks	O
scaled	O
to	O
longer	O
sequences	O
via	O
curriculum	Task
learning	Task
.	O

For	O
large	O
external	O
memories	O
we	O
observed	O
improvements	O
in	O
empirical	O
run	O
-	O
time	Metric
and	O
memory	O
overhead	O
by	O
up	O
to	O
three	O
orders	O
magnitude	O
over	O
vanilla	O
NTMs	Method
,	O
while	O
maintaining	O
near	O
-	O
identical	O
data	Metric
efficiency	Metric
and	O
performance	O
.	O

Further	O
,	O
in	O
Supplementary	O
[	O
reference	O
]	O
we	O
demonstrate	O
the	O
generality	O
of	O
our	O
approach	O
by	O
describing	O
how	O
to	O
construct	O
a	O
sparse	Method
version	Method
of	O
the	O
recently	O
published	O
Differentiable	Method
Neural	Method
Computer	Method
.	O

This	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
(	O
SDNC	Method
)	O
is	O
over	O
faster	O
than	O
the	O
canonical	Method
dense	Method
variant	Method
for	O
a	O
memory	O
size	O
of	O
slots	O
,	O
and	O
achieves	O
the	O
best	O
reported	O
result	O
in	O
the	O
Babi	Material
tasks	O
without	O
supervising	O
the	O
memory	O
access	O
.	O

section	O
:	O
Background	O
subsection	O
:	O
Attention	Task
and	O
content	Method
-	Method
based	Method
addressing	Method
An	O
external	O
memory	O
is	O
a	O
collection	O
of	O
real	O
-	O
valued	O
vectors	O
,	O
or	O
words	O
,	O
of	O
fixed	O
size	O
.	O

A	O
soft	O
read	O
operation	O
is	O
defined	O
to	O
be	O
a	O
weighted	Method
average	Method
over	O
memory	O
words	O
,	O
where	O
is	O
a	O
vector	O
of	O
weights	O
with	O
non	O
-	O
negative	O
entries	O
that	O
sum	O
to	O
one	O
.	O

Attending	O
to	O
memory	O
is	O
formalized	O
as	O
the	O
problem	O
of	O
computing	Task
.	O

A	O
content	Method
addressable	Method
memory	Method
,	O
proposed	O
in	O
,	O
is	O
an	O
external	Method
memory	Method
with	O
an	O
addressing	Method
scheme	Method
which	O
selects	O
based	O
upon	O
the	O
similarity	O
of	O
memory	O
words	O
to	O
a	O
given	O
query	O
.	O

Specifically	O
,	O
for	O
the	O
th	O
read	O
weight	O
we	O
define	O
,	O
where	O
is	O
a	O
similarity	Metric
measure	Metric
,	O
typically	O
Euclidean	O
distance	O
or	O
cosine	O
similarity	O
,	O
and	O
is	O
a	O
differentiable	Method
monotonic	Method
transformation	Method
,	O
typically	O
a	O
softmax	Method
.	O

We	O
can	O
think	O
of	O
this	O
as	O
an	O
instance	O
of	O
kernel	Method
smoothing	Method
where	O
the	O
network	O
learns	O
to	O
query	O
relevant	O
points	O
.	O

Because	O
the	O
read	O
operation	O
(	O
[	O
reference	O
]	O
)	O
and	O
content	Method
-	Method
based	Method
addressing	Method
scheme	Method
(	O
[	O
reference	O
]	O
)	O
are	O
smooth	O
,	O
we	O
can	O
place	O
them	O
within	O
a	O
neural	Method
network	Method
,	O
and	O
train	O
the	O
full	Method
model	Method
using	O
backpropagation	Method
.	O

subsection	O
:	O
Memory	Method
Networks	Method
One	O
recent	O
architecture	O
,	O
Memory	Method
Networks	Method
,	O
make	O
use	O
of	O
a	O
content	Method
addressable	Method
memory	Method
that	O
is	O
accessed	O
via	O
a	O
series	O
of	O
read	Method
operations	Method
and	O
has	O
been	O
successfully	O
applied	O
to	O
a	O
number	O
of	O
question	Task
answering	Task
tasks	Task
.	O

In	O
these	O
tasks	O
,	O
the	O
memory	O
is	O
pre	O
-	O
loaded	O
using	O
a	O
learned	O
embedding	O
of	O
the	O
provided	O
context	O
,	O
such	O
as	O
a	O
paragraph	O
of	O
text	O
,	O
and	O
then	O
the	O
controller	O
,	O
given	O
an	O
embedding	O
of	O
the	O
question	O
,	O
repeatedly	O
queries	O
the	O
memory	O
by	O
content	O
-	O
based	O
reads	O
to	O
determine	O
an	O
answer	O
.	O

subsection	O
:	O
Neural	Method
Turing	Method
Machine	Method
The	O
Neural	Method
Turing	Method
Machine	Method
is	O
a	O
recurrent	Method
neural	Method
network	Method
equipped	O
with	O
a	O
content	O
-	O
addressable	O
memory	O
,	O
similar	O
to	O
Memory	Method
Networks	Method
,	O
but	O
with	O
the	O
additional	O
capability	O
to	O
write	O
to	O
memory	O
over	O
time	Metric
.	O

The	O
memory	O
is	O
accessed	O
by	O
a	O
controller	Method
network	Method
,	O
typically	O
an	O
LSTM	Method
,	O
and	O
the	O
full	Method
model	Method
is	O
differentiable	O
—	O
allowing	O
it	O
to	O
be	O
trained	O
via	O
BPTT	Method
.	O

A	O
write	O
to	O
memory	O
,	O
consists	O
of	O
a	O
copy	O
of	O
the	O
memory	O
from	O
the	O
previous	O
time	Metric
step	O
decayed	O
by	O
the	O
erase	O
matrix	O
indicating	O
obsolete	O
or	O
inaccurate	O
content	O
,	O
and	O
an	O
addition	O
of	O
new	O
or	O
updated	O
information	O
.	O

The	O
erase	O
matrix	O
is	O
constructed	O
as	O
the	O
outer	O
product	O
between	O
a	O
set	O
of	O
write	O
weights	O
and	O
erase	O
vector	O
.	O

The	O
add	O
matrix	O
is	O
the	O
outer	O
product	O
between	O
the	O
write	O
weights	O
and	O
a	O
new	O
write	O
word	O
,	O
which	O
the	O
controller	O
outputs	O
.	O

section	O
:	O
Architecture	O
This	O
paper	O
introduces	O
Sparse	Method
Access	Method
Memory	Method
(	O
SAM	Method
)	O
,	O
a	O
new	O
neural	Method
memory	Method
architecture	Method
with	O
two	O
innovations	O
.	O

Most	O
importantly	O
,	O
all	O
writes	O
to	O
and	O
reads	O
from	O
external	O
memory	O
are	O
constrained	O
to	O
a	O
sparse	O
subset	O
of	O
the	O
memory	O
words	O
,	O
providing	O
similar	O
functionality	O
as	O
the	O
NTM	Method
,	O
while	O
allowing	O
computational	Task
and	Task
memory	Task
efficient	Task
operation	Task
.	O

Secondly	O
,	O
we	O
introduce	O
a	O
sparse	Method
memory	Method
management	Method
scheme	Method
that	O
tracks	O
memory	O
usage	O
and	O
finds	O
unused	O
blocks	O
of	O
memory	O
for	O
recording	O
new	O
information	O
.	O

For	O
a	O
memory	O
containing	O
words	O
,	O
SAM	Method
executes	O
a	O
forward	O
,	O
backward	O
step	O
in	O
time	Metric
,	O
initializes	O
in	O
space	Metric
,	O
and	O
consumes	O
space	Metric
per	O
time	Metric
step	O
.	O

Under	O
some	O
reasonable	O
assumptions	O
,	O
SAM	Method
is	O
asymptotically	O
optimal	O
in	O
time	Metric
and	O
space	Metric
complexity	O
(	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Read	O
The	O
sparse	Task
read	Task
operation	Task
is	O
defined	O
to	O
be	O
a	O
weighted	Method
average	Method
over	O
a	O
selection	O
of	O
words	O
in	O
memory	O
:	O
where	O
contains	O
number	O
of	O
non	O
-	O
zero	O
entries	O
with	O
indices	O
;	O
is	O
a	O
small	O
constant	O
,	O
independent	O
of	O
,	O
typically	O
or	O
.	O

We	O
will	O
refer	O
to	O
sparse	O
analogues	O
of	O
weight	O
vectors	O
as	O
,	O
and	O
when	O
discussing	O
operations	O
that	O
are	O
used	O
in	O
both	O
the	O
sparse	O
and	O
dense	O
versions	O
of	O
our	O
model	O
use	O
.	O

We	O
wish	O
to	O
construct	O
such	O
that	O
.	O

For	O
content	Task
-	Task
based	Task
reads	Task
where	O
is	O
defined	O
by	O
(	O
[	O
reference	O
]	O
)	O
,	O
an	O
effective	O
approach	O
is	O
to	O
keep	O
the	O
largest	O
non	O
-	O
zero	O
entries	O
and	O
set	O
the	O
remaining	O
entries	O
to	O
zero	O
.	O

We	O
can	O
compute	O
naively	O
in	O
time	Metric
by	O
calculating	O
and	O
keeping	O
the	O
largest	O
values	O
.	O

However	O
,	O
linear	O
-	O
time	Metric
operation	O
can	O
be	O
avoided	O
.	O

Since	O
the	O
largest	O
values	O
in	O
correspond	O
to	O
the	O
closest	O
points	O
to	O
our	O
query	O
,	O
we	O
can	O
use	O
an	O
approximate	Method
nearest	Method
neighbor	Method
data	Method
-	Method
structure	Method
,	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
to	O
calculate	O
in	O
time	Metric
.	O

Sparse	Task
read	Task
can	O
be	O
considered	O
a	O
special	O
case	O
of	O
the	O
matrix	Method
-	Method
vector	Method
product	Method
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
with	O
two	O
key	O
distinctions	O
.	O

The	O
first	O
is	O
that	O
we	O
pass	O
gradients	O
for	O
only	O
a	O
constant	O
number	O
of	O
rows	O
of	O
memory	O
per	O
time	Metric
step	O
,	O
versus	O
,	O
which	O
results	O
in	O
a	O
negligible	O
fraction	O
of	O
non	O
-	O
zero	O
error	O
gradient	O
per	O
timestep	O
when	O
the	O
memory	O
is	O
large	O
.	O

The	O
second	O
distinction	O
is	O
in	O
implementation	O
:	O
by	O
using	O
an	O
efficient	O
sparse	Method
matrix	Method
format	Method
such	O
as	O
Compressed	Method
Sparse	Method
Rows	Method
(	O
CSR	Method
)	O
,	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
and	O
its	O
gradients	O
in	O
constant	O
time	Metric
and	O
space	Metric
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Write	O
The	O
write	Method
operation	Method
is	O
SAM	Method
is	O
an	O
instance	O
of	O
(	O
[	O
reference	O
]	O
)	O
where	O
the	O
write	O
weights	O
are	O
constrained	O
to	O
contain	O
a	O
constant	O
number	O
of	O
non	O
-	O
zero	O
entries	O
.	O

This	O
is	O
done	O
by	O
a	O
simple	O
scheme	O
where	O
the	O
controller	O
writes	O
either	O
to	O
previously	O
read	O
locations	O
,	O
in	O
order	O
to	O
update	O
contextually	O
relevant	O
memories	O
,	O
or	O
the	O
least	O
recently	O
accessed	O
location	O
,	O
in	O
order	O
to	O
overwrite	O
stale	O
or	O
unused	O
memory	O
slots	O
with	O
fresh	O
content	O
.	O

The	O
introduction	O
of	O
sparsity	O
could	O
be	O
achieved	O
via	O
other	O
write	Method
schemes	Method
.	O

For	O
example	O
,	O
we	O
could	O
use	O
a	O
sparse	Method
content	Method
-	Method
based	Method
write	Method
scheme	Method
,	O
where	O
the	O
controller	O
chooses	O
a	O
query	O
vector	O
and	O
applies	O
writes	O
to	O
similar	O
words	O
in	O
memory	O
.	O

This	O
would	O
allow	O
for	O
direct	Task
memory	Task
updates	Task
,	O
but	O
would	O
create	O
problems	O
when	O
the	O
memory	O
is	O
empty	O
(	O
and	O
shift	O
further	O
complexity	O
to	O
the	O
controller	O
)	O
.	O

We	O
decided	O
upon	O
the	O
previously	O
read	O
/	O
least	Method
recently	Method
accessed	Method
addressing	Method
scheme	Method
for	O
simplicity	O
and	O
flexibility	O
.	O

The	O
write	O
weights	O
are	O
defined	O
as	O
where	O
the	O
controller	O
outputs	O
the	O
interpolation	O
gate	O
parameter	O
and	O
the	O
write	O
gate	O
parameter	O
.	O

The	O
write	O
to	O
the	O
previously	O
read	O
locations	O
is	O
purely	O
additive	O
,	O
while	O
the	O
least	O
recently	O
accessed	O
word	O
is	O
set	O
to	O
zero	O
before	O
being	O
written	O
to	O
.	O

When	O
the	O
read	O
operation	O
is	O
sparse	O
(	O
has	O
non	O
-	O
zero	O
entries	O
)	O
,	O
it	O
follows	O
the	O
write	O
operation	O
is	O
also	O
sparse	O
.	O

We	O
define	O
to	O
be	O
an	O
indicator	O
over	O
words	O
in	O
memory	O
,	O
with	O
a	O
value	O
of	O
when	O
the	O
word	O
minimizes	O
a	O
usage	Metric
measure	Metric
If	O
there	O
are	O
several	O
words	O
that	O
minimize	O
then	O
we	O
choose	O
arbitrarily	O
between	O
them	O
.	O

We	O
tried	O
two	O
definitions	O
of	O
.	O

The	O
first	O
definition	O
is	O
a	O
time	Metric
-	O
discounted	O
sum	O
of	O
write	O
weights	O
where	O
is	O
the	O
discount	O
factor	O
.	O

This	O
usage	O
definition	O
is	O
incorporated	O
within	O
Dense	Method
Access	Method
Memory	Method
(	O
DAM	Method
)	O
,	O
a	O
dense	Method
-	Method
approximation	Method
to	O
SAM	Method
that	O
is	O
used	O
for	O
experimental	O
comparison	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
second	O
usage	O
definition	O
,	O
used	O
by	O
SAM	Method
,	O
is	O
simply	O
the	O
number	O
of	O
time	Metric
-	O
steps	O
since	O
a	O
non	O
-	O
negligible	O
memory	O
access	O
:	O
.	O

Here	O
,	O
is	O
a	O
tuning	O
parameter	O
that	O
we	O
typically	O
choose	O
to	O
be	O
.	O

We	O
maintain	O
this	O
usage	O
statistic	O
in	O
constant	O
time	Metric
using	O
a	O
custom	O
data	O
-	O
structure	O
(	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
we	O
also	O
use	O
the	O
least	O
recently	O
accessed	O
word	O
to	O
calculate	O
the	O
erase	O
matrix	O
.	O

is	O
defined	O
to	O
be	O
the	O
expansion	O
of	O
this	O
usage	O
indicator	O
where	O
is	O
a	O
vector	O
of	O
ones	O
.	O

The	O
total	O
cost	O
of	O
the	O
write	O
is	O
constant	O
in	O
time	Metric
and	O
space	Metric
for	O
both	O
the	O
forwards	O
and	O
backwards	O
pass	O
,	O
which	O
improves	O
on	O
the	O
linear	O
space	Metric
and	O
time	Metric
dense	O
write	O
(	O
see	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Controller	O
We	O
use	O
a	O
one	O
layer	O
LSTM	Method
for	O
the	O
controller	Method
throughout	O
.	O

At	O
each	O
time	Metric
step	O
,	O
the	O
LSTM	Method
receives	O
a	O
concatenation	O
of	O
the	O
external	O
input	O
,	O
,	O
the	O
word	O
,	O
read	O
in	O
the	O
previous	O
time	Metric
step	O
.	O

The	O
LSTM	Method
then	O
produces	O
a	O
vector	O
,	O
,	O
of	O
read	O
and	O
write	O
parameters	O
for	O
memory	O
access	O
via	O
a	O
linear	Method
layer	Method
.	O

The	O
word	O
read	O
from	O
memory	O
for	O
the	O
current	O
time	Metric
step	O
,	O
,	O
is	O
then	O
concatenated	O
with	O
the	O
output	O
of	O
the	O
LSTM	Method
,	O
and	O
this	O
vector	O
is	O
fed	O
through	O
a	O
linear	Method
layer	Method
to	O
form	O
the	O
final	O
output	O
,	O
.	O

The	O
full	O
control	O
flow	O
is	O
illustrated	O
in	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Efficient	O
backpropagation	O
through	O
time	Metric
We	O
have	O
already	O
demonstrated	O
how	O
the	O
forward	Method
operations	Method
in	O
SAM	Method
can	O
be	O
efficiently	O
computed	O
in	O
time	Metric
.	O

However	O
,	O
when	O
considering	O
space	Metric
complexity	Metric
of	O
MANNs	O
,	O
there	O
remains	O
a	O
dependence	O
on	O
for	O
the	O
computation	O
of	O
the	O
derivatives	O
at	O
the	O
corresponding	O
time	Metric
step	O
.	O

A	O
naive	O
implementation	O
requires	O
the	O
state	O
of	O
the	O
memory	O
to	O
be	O
cached	O
at	O
each	O
time	Metric
step	O
,	O
incurring	O
a	O
space	Metric
overhead	O
of	O
,	O
which	O
severely	O
limits	O
memory	O
size	O
and	O
sequence	O
length	O
.	O

Fortunately	O
,	O
this	O
can	O
be	O
remedied	O
.	O

Since	O
there	O
are	O
only	O
words	O
that	O
are	O
written	O
at	O
each	O
time	Metric
step	O
,	O
we	O
instead	O
track	O
the	O
sparse	O
modifications	O
made	O
to	O
the	O
memory	O
at	O
each	O
timestep	O
,	O
apply	O
them	O
in	O
-	O
place	O
to	O
compute	O
in	O
time	Metric
and	O
space	Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
can	O
restore	O
the	O
state	O
of	O
from	O
in	O
time	Metric
by	O
reverting	O
the	O
sparse	Method
modifications	Method
applied	O
at	O
time	Metric
step	O
.	O

As	O
such	O
the	O
memory	O
is	O
actually	O
rolled	O
back	O
to	O
previous	O
states	O
during	O
backpropagation	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

At	O
the	O
end	O
of	O
the	O
backward	O
pass	O
,	O
the	O
memory	O
ends	O
rolled	O
back	O
to	O
the	O
start	O
state	O
.	O

If	O
required	O
,	O
such	O
as	O
when	O
using	O
truncating	Method
BPTT	Method
,	O
the	O
final	O
memory	O
state	O
can	O
be	O
restored	O
by	O
making	O
a	O
copy	O
of	O
prior	O
to	O
calling	O
backwards	O
in	O
time	Metric
,	O
or	O
by	O
re	O
-	O
applying	O
the	O
sparse	Method
updates	Method
in	O
time	Metric
.	O

subsection	O
:	O
Approximate	Method
nearest	Method
neighbors	Method
When	O
querying	O
the	O
memory	O
,	O
we	O
can	O
use	O
an	O
approximate	Method
nearest	Method
neighbor	Method
index	Method
(	O
ANN	Method
)	O
to	O
search	O
over	O
the	O
external	O
memory	O
for	O
the	O
nearest	O
words	O
.	O

Where	O
a	O
linear	Method
KNN	Method
search	Method
inspects	O
every	O
element	O
in	O
memory	O
(	O
taking	O
time	Metric
)	O
,	O
an	O
ANN	Method
index	Method
maintains	O
a	O
structure	O
over	O
the	O
dataset	O
to	O
allow	O
for	O
fast	O
inspection	O
of	O
nearby	O
points	O
in	O
time	Metric
.	O

In	O
our	O
case	O
,	O
the	O
memory	O
is	O
still	O
a	O
dense	O
tensor	O
that	O
the	O
network	O
directly	O
operates	O
on	O
;	O
however	O
the	O
ANN	Method
is	O
a	O
structured	O
view	O
of	O
its	O
contents	O
.	O

Both	O
the	O
memory	O
and	O
the	O
ANN	O
index	O
are	O
passed	O
through	O
the	O
network	O
and	O
kept	O
in	O
sync	O
during	O
writes	O
.	O

However	O
there	O
are	O
no	O
gradients	O
with	O
respect	O
to	O
the	O
ANN	Method
as	O
its	O
function	O
is	O
fixed	O
.	O

We	O
considered	O
two	O
types	O
of	O
ANN	Method
indexes	Method
:	O
FLANN	Method
’s	Method
randomized	Method
k	Method
-	Method
d	Method
tree	Method
implementation	Method
that	O
arranges	O
the	O
datapoints	O
in	O
an	O
ensemble	O
of	O
structured	O
(	O
randomized	Method
k	Method
-	Method
d	Method
)	Method
trees	Method
to	O
search	O
for	O
nearby	O
points	O
via	O
comparison	Method
-	Method
based	Method
search	Method
,	O
and	O
one	O
that	O
uses	O
locality	Method
sensitive	Method
hash	Method
(	O
LSH	Method
)	O
functions	O
that	O
map	O
points	O
into	O
buckets	O
with	O
distance	O
-	O
preserving	O
guarantees	O
.	O

We	O
used	O
randomized	Method
k	Method
-	Method
d	Method
trees	Method
for	O
small	O
word	O
sizes	O
and	O
LSHs	Method
for	O
large	O
word	O
sizes	O
.	O

For	O
both	O
ANN	Method
implementations	Method
,	O
there	O
is	O
an	O
cost	O
for	O
insertion	Task
,	O
deletion	Task
and	O
query	Task
.	O

We	O
also	O
rebuild	O
the	O
ANN	Method
from	O
scratch	O
every	O
insertions	O
to	O
ensure	O
it	O
does	O
not	O
become	O
imbalanced	O
.	O

section	O
:	O
Results	O
subsection	O
:	O
Speed	Metric
and	Metric
memory	Metric
benchmarks	Metric
0.47	O
0.47	O
We	O
measured	O
the	O
forward	O
and	O
backward	O
times	O
of	O
the	O
SAM	Method
architecture	O
versus	O
the	O
dense	O
DAM	Method
variant	O
and	O
the	O
original	O
NTM	Method
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

SAM	Method
is	O
over	O
times	O
faster	O
than	O
the	O
NTM	Method
when	O
the	O
memory	O
contains	O
one	O
million	O
words	O
and	O
an	O
exact	O
linear	O
-	O
index	O
is	O
used	O
,	O
and	O
times	O
faster	O
with	O
the	O
k	Method
-	Method
d	Method
tree	Method
(	O
Figure	O
[	O
reference	O
]	O
sf	O
:	O
speed	O
)	O
.	O

With	O
an	O
ANN	Method
the	O
model	O
runs	O
in	O
sublinear	O
time	Metric
with	O
respect	O
to	O
the	O
memory	O
size	O
.	O

SAM	Method
’s	O
memory	O
usage	O
per	O
time	Metric
step	O
is	O
independent	O
of	O
the	O
number	O
of	O
memory	O
words	O
(	O
Figure	O
[	O
reference	O
]	O
sf	Method
:	O
memory	O
)	O
,	O
which	O
empirically	O
verifies	O
the	O
space	Metric
claim	O
from	O
Supplementary	O
[	O
reference	O
]	O
.	O

For	O
memory	O
words	O
SAM	Method
uses	O
of	O
physical	O
memory	O
to	O
initialize	O
the	O
network	O
and	O
to	O
run	O
a	O
100	O
step	O
forward	O
and	O
backward	O
pass	O
,	O
compared	O
with	O
the	O
NTM	Method
which	O
consumes	O
.	O

subsection	O
:	O
Learning	Task
with	O
sparse	Task
memory	Task
access	Task
We	O
have	O
established	O
that	O
SAM	Method
reaps	O
a	O
huge	O
computational	Metric
and	Metric
memory	Metric
advantage	O
of	O
previous	O
models	O
,	O
but	O
can	O
we	O
really	O
learn	O
with	O
SAM	Method
’s	O
sparse	O
approximations	O
?	O
We	O
investigated	O
the	O
learning	Metric
cost	Metric
of	O
inducing	Task
sparsity	Task
,	O
and	O
the	O
effect	O
of	O
placing	O
an	O
approximate	O
nearest	O
neighbor	O
index	O
within	O
the	O
network	O
,	O
by	O
comparing	O
SAM	Method
with	O
its	O
dense	O
variant	O
DAM	Method
and	O
some	O
established	O
models	O
,	O
the	O
NTM	Method
and	O
the	O
LSTM	Method
.	O

We	O
trained	O
each	O
model	O
on	O
three	O
of	O
the	O
original	O
NTM	Task
tasks	Task
.	O

1	O
.	O

Copy	Method
:	O
copy	O
a	O
random	O
input	O
sequence	O
of	O
length	O
1–20	O
,	O
2	O
.	O

Associative	Task
Recall	Task
:	O
given	O
3	O
-	O
6	O
random	O
(	O
key	O
,	O
value	O
)	O
pairs	O
,	O
and	O
subsequently	O
a	O
cue	O
key	O
,	O
return	O
the	O
associated	O
value	O
.	O

3	O
.	O

Priority	Method
Sort	Method
:	O
Given	O
20	O
random	O
keys	O
and	O
priority	O
values	O
,	O
return	O
the	O
top	O
16	O
keys	O
in	O
descending	O
order	O
of	O
priority	O
.	O

We	O
chose	O
these	O
tasks	O
because	O
the	O
NTM	Method
is	O
known	O
to	O
perform	O
well	O
on	O
them	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
that	O
sparse	Method
models	Method
are	O
able	O
to	O
learn	O
with	O
comparable	O
efficiency	O
to	O
the	O
dense	Method
models	Method
and	O
,	O
surprisingly	O
,	O
learn	O
more	O
effectively	O
for	O
some	O
tasks	O
—	O
notably	O
priority	Task
sort	Task
and	O
associative	Task
recall	Task
.	O

This	O
shows	O
that	O
sparse	O
reads	O
and	O
writes	O
can	O
actually	O
benefit	O
early	O
-	O
stage	O
learning	Task
in	O
some	O
cases	O
.	O

Full	O
hyperparameter	O
details	O
are	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

0.32	O
0.32	O
0.32	O
subsection	O
:	O
Scaling	O
with	O
a	O
curriculum	O
The	O
computational	Metric
efficiency	Metric
of	O
SAM	Method
opens	O
up	O
the	O
possibility	O
of	O
training	O
on	O
tasks	O
that	O
require	O
storing	O
a	O
large	O
amount	O
of	O
information	O
over	O
long	O
sequences	O
.	O

Here	O
we	O
show	O
this	O
is	O
possible	O
in	O
practice	O
,	O
by	O
scaling	Task
tasks	Task
to	O
a	O
large	O
scale	O
via	O
an	O
exponentially	Method
increasing	Method
curriculum	Method
.	O

We	O
parametrized	O
three	O
of	O
the	O
tasks	O
described	O
in	O
Section	O
[	O
reference	O
]	O
:	O
associative	Task
recall	Task
,	O
copy	O
,	O
and	O
priority	Task
sort	Task
,	O
with	O
a	O
progressively	O
increasing	O
difficulty	O
level	O
which	O
characterises	O
the	O
length	O
of	O
the	O
sequence	O
and	O
number	O
of	O
entries	O
to	O
store	O
in	O
memory	O
.	O

For	O
example	O
,	O
level	O
specifies	O
the	O
input	O
sequence	O
length	O
for	O
the	O
copy	Task
task	Task
.	O

We	O
exponentially	O
increased	O
the	O
maximum	O
level	O
when	O
the	O
network	O
begins	O
to	O
learn	O
the	O
fundamental	Method
algorithm	Method
.	O

Since	O
the	O
time	Metric
taken	O
for	O
a	O
forward	O
and	O
backward	O
pass	O
scales	O
with	O
the	O
sequence	O
length	O
,	O
following	O
a	O
standard	O
linearly	Method
increasing	Method
curriculum	Method
could	O
potentially	O
take	O
,	O
if	O
the	O
same	O
amount	O
of	O
training	O
was	O
required	O
at	O
each	O
step	O
of	O
the	O
curriculum	O
.	O

Specifically	O
,	O
was	O
doubled	O
whenever	O
the	O
average	Metric
training	Metric
loss	Metric
dropped	O
below	O
a	O
threshold	O
for	O
a	O
number	O
of	O
episodes	O
.	O

The	O
level	O
was	O
sampled	O
for	O
each	O
minibatch	O
from	O
the	O
uniform	O
distribution	O
over	O
integers	O
.	O

We	O
compared	O
the	O
dense	Method
models	Method
,	O
NTM	O
and	O
DAM	Method
,	O
with	O
both	O
SAM	Method
with	O
an	O
exact	Method
nearest	Method
neighbor	Method
index	Method
(	O
SAM	Method
linear	O
)	O
and	O
with	O
locality	Method
sensitive	Method
hashing	Method
(	O
SAM	Method
ANN	O
)	O
.	O

The	O
dense	Method
models	Method
contained	O
64	O
memory	O
words	O
,	O
while	O
the	O
sparse	Method
models	Method
had	O
words	O
.	O

These	O
sizes	O
were	O
chosen	O
to	O
ensure	O
all	O
models	O
use	O
approximately	O
the	O
same	O
amount	O
of	O
physical	O
memory	O
when	O
trained	O
over	O
100	O
steps	O
.	O

For	O
all	O
tasks	O
,	O
SAM	Method
was	O
able	O
to	O
advance	O
further	O
than	O
the	O
other	O
models	O
,	O
and	O
in	O
the	O
associative	Task
recall	Task
task	Task
,	O
SAM	Method
was	O
able	O
to	O
advance	O
through	O
the	O
curriculum	O
to	O
sequences	O
greater	O
than	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

Note	O
that	O
we	O
did	O
not	O
use	O
truncated	Method
backpropagation	Method
,	O
so	O
this	O
involved	O
BPTT	Method
for	O
over	O
steps	O
with	O
a	O
memory	O
size	O
in	O
the	O
millions	O
of	O
words	O
.	O

To	O
investigate	O
whether	O
SAM	Method
was	O
able	O
to	O
learn	O
algorithmic	Task
solutions	Task
to	O
tasks	O
,	O
we	O
investigated	O
its	O
ability	O
to	O
generalize	O
to	O
sequences	O
that	O
far	O
exceeded	O
those	O
observed	O
during	O
training	O
.	O

Namely	O
we	O
trained	O
SAM	Method
on	O
the	O
associative	Task
recall	Task
task	Task
up	O
to	O
sequences	O
of	O
length	O
,	O
and	O
found	O
it	O
was	O
then	O
able	O
to	O
generalize	O
to	O
sequences	O
of	O
length	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Question	Task
answering	Task
on	O
the	O
Babi	Material
tasks	O
introduced	O
toy	Task
tasks	Task
they	O
considered	O
a	O
prerequisite	O
to	O
agents	O
which	O
can	O
reason	O
and	O
understand	O
natural	O
language	O
.	O

They	O
are	O
synthetically	Task
generated	Task
language	Task
tasks	Task
with	O
a	O
vocab	O
of	O
about	O
150	O
words	O
that	O
test	O
various	O
aspects	O
of	O
simple	O
reasoning	Task
such	O
as	O
deduction	Task
,	O
induction	Task
and	O
coreferencing	Task
.	O

We	O
tested	O
the	O
models	O
(	O
including	O
the	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
on	O
this	O
task	O
.	O

The	O
full	O
results	O
and	O
training	O
details	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
.	O

The	O
MANNs	Method
,	O
except	O
the	O
NTM	Method
,	O
are	O
able	O
to	O
learn	O
solutions	O
comparable	O
to	O
the	O
previous	O
best	O
results	O
,	O
failing	O
at	O
only	O
2	O
of	O
the	O
tasks	O
.	O

The	O
SDNC	Method
manages	O
to	O
solve	O
all	O
but	O
1	O
of	O
the	O
tasks	O
,	O
the	O
best	O
reported	O
result	O
on	O
Babi	Material
that	O
we	O
are	O
aware	O
of	O
.	O

Notably	O
the	O
best	O
prior	O
results	O
have	O
been	O
obtained	O
by	O
using	O
supervising	O
the	O
memory	Task
retrieval	Task
(	O
during	O
training	O
the	O
model	O
is	O
provided	O
annotations	O
which	O
indicate	O
which	O
memories	O
should	O
be	O
used	O
to	O
answer	O
a	O
query	O
)	O
.	O

More	O
directly	O
comparable	O
previous	O
work	O
with	O
end	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
networks	Method
,	O
which	O
did	O
not	O
use	O
supervision	O
,	O
fails	O
at	O
6	O
of	O
the	O
tasks	O
.	O

Both	O
the	O
sparse	Method
and	O
dense	Method
perform	O
comparably	O
at	O
this	O
task	O
,	O
again	O
indicating	O
the	O
sparse	Method
approximations	Method
do	O
not	O
impair	O
learning	Task
.	O

We	O
believe	O
the	O
NTM	Method
may	O
perform	O
poorly	O
since	O
it	O
lacks	O
a	O
mechanism	O
which	O
allows	O
it	O
to	O
allocate	O
memory	O
effectively	O
.	O

subsection	O
:	O
Learning	O
on	O
real	O
world	O
data	O
Finally	O
,	O
we	O
demonstrate	O
that	O
the	O
model	O
is	O
capable	O
of	O
learning	O
in	O
a	O
non	O
-	O
synthetic	O
dataset	O
.	O

Omniglot	Method
is	O
a	O
dataset	O
of	O
1623	O
characters	O
taken	O
from	O
50	O
different	O
alphabets	O
,	O
with	O
20	O
examples	O
of	O
each	O
character	O
.	O

This	O
dataset	O
is	O
used	O
to	O
test	O
rapid	Task
,	Task
or	Task
one	Task
-	Task
shot	Task
learning	Task
,	O
since	O
there	O
are	O
few	O
examples	O
of	O
each	O
character	O
but	O
many	O
different	O
character	O
classes	O
.	O

Following	O
,	O
we	O
generate	O
episodes	O
where	O
a	O
subset	O
of	O
characters	O
are	O
randomly	O
selected	O
from	O
the	O
dataset	O
,	O
rotated	O
and	O
stretched	O
,	O
and	O
assigned	O
a	O
randomly	O
chosen	O
label	O
.	O

At	O
each	O
time	Metric
step	O
an	O
example	O
of	O
one	O
of	O
the	O
characters	O
is	O
presented	O
,	O
along	O
with	O
the	O
correct	O
label	O
of	O
the	O
proceeding	O
character	O
.	O

Each	O
character	O
is	O
presented	O
10	O
times	O
in	O
an	O
episode	O
(	O
but	O
each	O
presentation	O
may	O
be	O
any	O
one	O
of	O
the	O
20	O
examples	O
of	O
the	O
character	O
)	O
.	O

In	O
order	O
to	O
succeed	O
at	O
the	O
task	O
the	O
model	O
must	O
learn	O
to	O
rapidly	O
associate	O
a	O
novel	O
character	O
with	O
the	O
correct	O
label	O
,	O
such	O
that	O
it	O
can	O
correctly	O
classify	O
subsequent	O
examples	O
of	O
the	O
same	O
character	O
class	O
.	O

Again	O
,	O
we	O
used	O
an	O
exponential	Method
curriculum	Method
,	O
doubling	O
the	O
number	O
of	O
additional	O
characters	O
provided	O
to	O
the	O
model	O
whenever	O
the	O
cost	O
was	O
reduced	O
under	O
a	O
threshold	O
.	O

After	O
training	O
all	O
MANNs	Method
for	O
the	O
same	O
length	O
of	O
time	Metric
,	O
a	O
validation	Task
task	Task
with	O
characters	O
was	O
used	O
to	O
select	O
the	O
best	O
run	O
,	O
and	O
this	O
was	O
then	O
tested	O
on	O
a	O
test	O
set	O
,	O
containing	O
all	O
novel	O
characters	O
for	O
different	O
sequence	O
lengths	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

All	O
of	O
the	O
MANNs	Method
were	O
able	O
to	O
perform	O
much	O
better	O
than	O
chance	O
,	O
even	O
on	O
sequences	O
longer	O
than	O
seen	O
during	O
training	O
.	O

SAM	Method
outperformed	O
other	O
models	O
,	O
presumably	O
due	O
to	O
its	O
much	O
larger	O
memory	O
capacity	O
.	O

Previous	O
results	O
on	O
the	O
Omniglot	Task
curriculum	Task
task	Task
are	O
not	O
identical	O
,	O
since	O
we	O
used	O
1	O
-	O
hot	O
labels	O
throughout	O
and	O
the	O
training	O
curriculum	O
scaled	O
to	O
longer	O
sequences	O
,	O
but	O
our	O
results	O
with	O
the	O
dense	Method
models	Method
are	O
comparable	O
(	O
errors	O
with	O
characters	O
)	O
,	O
while	O
the	O
SAM	Method
is	O
significantly	O
better	O
(	O
errors	O
with	O
characters	O
)	O
.	O

section	O
:	O
Discussion	O
Scaling	Task
memory	Task
systems	Task
is	O
a	O
pressing	O
research	O
direction	O
due	O
to	O
potential	O
for	O
compelling	O
applications	O
with	O
large	O
amounts	O
of	O
memory	O
.	O

We	O
have	O
demonstrated	O
that	O
you	O
can	O
train	O
neural	Method
networks	Method
with	O
large	O
memories	O
via	O
a	O
sparse	Method
read	Method
and	Method
write	Method
scheme	Method
that	O
makes	O
use	O
of	O
efficient	O
data	O
structures	O
within	O
the	O
network	O
,	O
and	O
obtain	O
significant	O
speedups	O
during	O
training	Task
.	O

Although	O
we	O
have	O
focused	O
on	O
a	O
specific	O
MANN	Method
(	O
SAM	Method
)	Method
,	O
which	O
is	O
closely	O
related	O
to	O
the	O
NTM	Method
,	O
the	O
approach	O
taken	O
here	O
is	O
general	O
and	O
can	O
be	O
applied	O
to	O
many	O
differentiable	O
memory	Method
architectures	Method
,	O
such	O
as	O
Memory	Method
Networks	Method
.	O

It	O
should	O
be	O
noted	O
that	O
there	O
are	O
multiple	O
possible	O
routes	O
toward	O
scalable	Task
memory	Task
architectures	Task
.	O

For	O
example	O
,	O
prior	O
work	O
aimed	O
at	O
scaling	O
Neural	Method
Turing	Method
Machines	Method
used	O
reinforcement	Method
learning	Method
to	O
train	O
a	O
discrete	Method
addressing	Method
policy	Method
.	O

This	O
approach	O
also	O
touches	O
only	O
a	O
sparse	O
set	O
of	O
memories	O
at	O
each	O
time	Metric
step	O
,	O
but	O
relies	O
on	O
higher	O
variance	O
estimates	O
of	O
the	O
gradient	O
during	O
optimization	Task
.	O

Though	O
we	O
can	O
only	O
guess	O
at	O
what	O
class	O
of	O
memory	Method
models	Method
will	O
become	O
staple	O
in	O
machine	Task
learning	Task
systems	Task
of	O
the	O
future	O
,	O
we	O
argue	O
in	O
Supplementary	O
[	O
reference	O
]	O
that	O
they	O
will	O
be	O
no	O
more	O
efficient	O
than	O
SAM	Method
in	O
space	Metric
and	O
time	Metric
complexity	Metric
if	O
they	O
address	O
memories	O
based	O
on	O
content	O
.	O

We	O
have	O
experimented	O
with	O
randomized	Method
k	Method
-	Method
d	Method
trees	Method
and	O
LSH	Method
within	O
the	O
network	O
to	O
reduce	O
the	O
forward	O
pass	O
of	O
training	Task
to	O
sublinear	O
time	Metric
,	O
but	O
there	O
may	O
be	O
room	O
for	O
improvement	O
here	O
.	O

K	Method
-	Method
d	Method
trees	Method
were	O
not	O
designed	O
specifically	O
for	O
fully	O
online	Task
scenarios	Task
,	O
and	O
can	O
become	O
imbalanced	O
during	O
training	O
.	O

Recent	O
work	O
in	O
tree	Method
ensemble	Method
models	Method
,	O
such	O
as	O
Mondrian	Method
forests	Method
,	O
show	O
promising	O
results	O
in	O
maintaining	O
balanced	Task
hierarchical	Task
set	Task
coverage	Task
in	O
the	O
online	Task
setting	Task
.	O

An	O
alternative	O
approach	O
which	O
may	O
be	O
well	O
-	O
suited	O
is	O
LSH	Method
forests	O
,	O
which	O
adaptively	O
modifies	O
the	O
number	O
of	O
hashes	O
used	O
.	O

It	O
would	O
be	O
an	O
interesting	O
empirical	O
investigation	O
to	O
more	O
fully	O
assess	O
different	O
ANN	Method
approaches	Method
in	O
the	O
challenging	O
context	O
of	O
training	O
a	O
neural	Method
network	Method
.	O

Humans	O
are	O
able	O
to	O
retain	O
a	O
large	O
,	O
task	O
-	O
dependent	O
set	O
of	O
memories	O
obtained	O
in	O
one	O
pass	O
with	O
a	O
surprising	O
amount	O
of	O
fidelity	Metric
.	O

Here	O
we	O
have	O
demonstrated	O
architectures	O
that	O
may	O
one	O
day	O
compete	O
with	O
humans	O
at	O
these	O
kinds	O
of	O
tasks	O
.	O

subsection	O
:	O
Acknowledgements	O
We	O
thank	O
Vyacheslav	O
Egorov	O
,	O
Edward	O
Grefenstette	O
,	O
Malcolm	O
Reynolds	O
,	O
Fumin	O
Wang	O
and	O
Yori	O
Zwols	O
for	O
their	O
assistance	O
,	O
and	O
the	O
Google	O
DeepMind	O
family	O
for	O
helpful	O
discussions	O
and	O
encouragement	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Supplementary	O
Information	O
appendix	O
:	O
Time	Metric
and	O
space	Metric
complexity	Metric
Under	O
a	O
reasonable	O
class	O
of	O
content	Method
addressable	Method
memory	Method
architectures	Method
,	O
SAM	Method
is	O
optimal	O
in	O
time	Metric
and	O
space	Metric
complexity	Metric
.	O

theorem	O
:	O
.	O

Let	O
M	O
be	O
a	O
collection	O
of	O
real	O
vectors	O
m1	O
,	O
m2	O
,	O
…	O
,	O
mN	O
of	O
fixed	O
dimension	O
d.	O
Let	O
A	O
be	O
the	O
set	O
of	O
all	O
content	Method
addressable	Method
memory	Method
data	Method
structures	Method
that	O
store	O
M	O
and	O
can	O
return	O
at	O
least	O
one	O
word	O
mj	O
such	O
that	O
≤⁢D	O
(	O
q	O
,	O
mj	O
)	O
⁢c	O
(+	O
1ϵ	O
)	O
for	O
a	O
given	O
Lp	O
norm	O
D	O
,	O
query	O
vector	O
q	O
,	O
and	O
>	O
ϵ0	O
;	O
provided	O
such	O
a	O
memory	O
mc	O
exists	O
with	O
=	O
⁢D	O
(	O
q	O
,	O
mc	O
)	O
c	O
.	O

Existing	O
lower	O
bounds	O
assert	O
that	O
for	O
any	O
data	O
structure	O
,	O
requires	O
time	Metric
and	O
space	Metric
to	O
perform	O
a	O
read	O
operation	O
.	O

The	O
SAM	Method
memory	O
architecture	O
proposed	O
in	O
this	O
paper	O
is	O
contained	O
within	O
as	O
it	O
computes	O
the	O
approximate	Task
nearest	Task
neighbors	Task
problem	Task
in	O
fixed	O
dimensions	O
.	O

As	O
we	O
will	O
show	O
,	O
SAM	Method
requires	O
time	Metric
to	O
query	O
and	O
maintain	O
the	O
ANN	Method
,	O
to	O
perform	O
all	O
subsequent	O
sparse	Task
read	Task
,	Task
write	Task
,	Task
and	Task
error	Task
gradient	Task
calculations	Task
.	O

It	O
requires	O
space	Metric
to	O
initialize	O
the	O
memory	O
and	O
to	O
store	O
intermediate	O
sparse	O
tensors	O
.	O

We	O
thus	O
conclude	O
it	O
is	O
optimal	O
in	O
asymptotic	Metric
time	Metric
and	O
space	Metric
complexity	Metric
.	O

subsection	O
:	O
Initialization	O
Upon	O
initialization	Task
,	O
SAM	Method
consumes	O
space	Metric
and	O
time	Metric
to	O
instantiate	O
the	O
memory	O
and	O
the	O
memory	O
Jacobian	O
.	O

Furthermore	O
,	O
it	O
requires	O
time	Metric
and	O
space	Metric
to	O
initialize	O
auxiliary	O
data	O
structures	O
which	O
index	O
the	O
memory	O
,	O
such	O
as	O
the	O
approximate	Method
nearest	Method
neighbor	Method
which	O
provides	O
a	O
content	O
-	O
structured	O
view	O
of	O
the	O
memory	O
,	O
and	O
the	O
least	O
accessed	O
ring	O
,	O
which	O
maintains	O
the	O
temporal	O
ordering	O
in	O
which	O
memory	O
words	O
are	O
accessed	O
.	O

These	O
initializations	O
represent	O
an	O
unavoidable	O
one	Metric
-	Metric
off	Metric
cost	Metric
that	O
does	O
not	O
recur	O
per	O
step	O
of	O
training	O
,	O
and	O
ultimately	O
has	O
little	O
effect	O
on	O
training	Metric
speed	Metric
.	O

For	O
the	O
remainder	O
of	O
the	O
analysis	O
we	O
will	O
concentrate	O
on	O
the	O
space	Metric
and	O
time	Metric
cost	Metric
per	O
training	O
step	O
.	O

subsection	O
:	O
Read	O
Recall	O
the	O
sparse	O
read	O
operation	O
,	O
As	O
is	O
chosen	O
to	O
be	O
a	O
fixed	O
constant	O
,	O
it	O
is	O
clear	O
we	O
can	O
compute	O
(	O
[	O
reference	O
]	O
)	O
in	O
time	Metric
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
see	O
the	O
gradients	O
are	O
sparse	O
with	O
only	O
non	O
-	O
zero	O
terms	O
,	O
and	O
where	O
is	O
a	O
vector	O
of	O
zeros	O
.	O

Thus	O
they	O
can	O
both	O
be	O
computed	O
in	O
constant	O
time	Metric
by	O
skipping	O
the	O
computation	O
of	O
zeros	O
.	O

Furthermore	O
by	O
using	O
an	O
efficient	O
sparse	Method
matrix	Method
format	Method
to	O
store	O
these	O
matrices	O
and	O
vectors	O
,	O
such	O
as	O
the	O
CSR	Method
,	O
we	O
can	O
represent	O
them	O
using	O
at	O
most	O
values	O
.	O

Since	O
the	O
read	O
word	O
and	O
its	O
respective	O
error	O
gradient	O
is	O
the	O
size	O
of	O
a	O
single	O
word	O
in	O
memory	O
(	O
elements	O
)	O
,	O
the	O
overall	O
space	Metric
complexity	Metric
is	O
per	O
time	Metric
step	O
for	O
the	O
read	O
.	O

subsection	O
:	O
Write	O
Recall	O
the	O
write	O
operation	O
,	O
where	O
is	O
the	O
add	O
matrix	O
,	O
is	O
the	O
erase	O
matrix	O
,	O
and	O
is	O
defined	O
to	O
be	O
the	O
erase	O
weight	O
matrix	O
.	O

We	O
chose	O
the	O
write	O
weights	O
to	O
be	O
an	O
interpolation	O
between	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
the	O
previously	O
read	O
locations	O
,	O
For	O
sparse	Task
reads	Task
where	O
is	O
a	O
sparse	O
vector	O
with	O
non	O
-	O
zeros	O
,	O
the	O
write	O
weights	O
is	O
also	O
sparse	O
with	O
non	O
-	O
zeros	O
:	O
for	O
the	O
least	O
recently	O
accessed	O
location	O
and	O
for	O
the	O
previously	O
read	O
locations	O
.	O

Thus	O
the	O
sparse	Method
-	Method
dense	Method
outer	Method
product	Method
can	O
be	O
performed	O
in	O
time	Metric
as	O
is	O
a	O
fixed	O
constant	O
.	O

Since	O
can	O
be	O
represented	O
as	O
a	O
sparse	O
matrix	O
with	O
one	O
single	O
non	O
-	O
zero	O
,	O
the	O
erase	O
matrix	O
can	O
also	O
.	O

As	O
and	O
are	O
sparse	O
matrices	O
we	O
can	O
then	O
add	O
them	O
component	O
-	O
wise	O
to	O
the	O
dense	O
in	O
time	Metric
.	O

By	O
analogous	O
arguments	O
the	O
backward	Method
pass	Method
can	O
be	O
computed	O
in	O
time	Metric
and	O
each	O
sparse	O
matrix	O
can	O
be	O
represented	O
in	O
space	Metric
.	O

We	O
avoid	O
caching	O
the	O
modified	O
memory	O
,	O
and	O
thus	O
duplicating	O
it	O
,	O
by	O
applying	O
the	O
write	O
directly	O
to	O
the	O
memory	O
.	O

To	O
restore	O
its	O
prior	O
state	O
during	O
the	O
backward	O
pass	O
,	O
which	O
is	O
crucial	O
to	O
gradient	Task
calculations	Task
at	O
earlier	O
time	Metric
steps	O
,	O
we	O
roll	O
the	O
memory	O
it	O
back	O
by	O
reverting	O
the	O
sparse	Method
modifications	Method
with	O
an	O
additional	O
time	Metric
overhead	O
(	O
Supplementary	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
location	O
of	O
the	O
least	O
recently	O
accessed	O
memory	O
can	O
be	O
maintained	O
in	O
time	Metric
by	O
constructing	O
a	O
circular	Method
linked	Method
list	Method
that	O
tracks	O
the	O
indices	O
of	O
words	O
in	O
memory	O
,	O
and	O
preserves	O
a	O
strict	O
ordering	O
of	O
relative	O
temporal	O
access	O
.	O

The	O
first	O
element	O
in	O
the	O
ring	O
is	O
the	O
least	O
recently	O
accessed	O
word	O
in	O
memory	O
,	O
and	O
the	O
last	O
element	O
in	O
the	O
ring	O
is	O
the	O
most	O
recently	O
modified	O
.	O

We	O
keep	O
a	O
‘	O
‘	O
head	O
’	O
’	O
pointer	O
to	O
the	O
first	O
element	O
in	O
the	O
ring	O
.	O

When	O
a	O
memory	O
word	O
is	O
randomly	O
accessed	O
,	O
we	O
can	O
push	O
its	O
respective	O
index	O
to	O
the	O
back	O
of	O
the	O
ring	O
in	O
time	Metric
by	O
redirecting	O
a	O
small	O
number	O
of	O
pointers	O
.	O

When	O
we	O
wish	O
to	O
pop	O
the	O
least	O
recently	O
accessed	O
memory	O
(	O
and	O
write	O
to	O
it	O
)	O
we	O
move	O
the	O
head	O
to	O
the	O
next	O
element	O
in	O
the	O
ring	O
in	O
time	Metric
.	O

subsection	O
:	O
Content	Method
-	Method
based	Method
addressing	Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
we	O
can	O
calculate	O
the	O
content	O
-	O
based	O
attention	O
,	O
or	O
read	O
weights	O
,	O
in	O
time	Metric
using	O
an	O
approximate	Method
nearest	Method
neighbor	Method
index	Method
that	O
views	O
the	O
memory	O
.	O

We	O
keep	O
the	O
ANN	O
index	O
synchronized	O
with	O
the	O
memory	O
by	O
passing	O
it	O
through	O
the	O
network	O
as	O
a	O
non	O
-	O
differentiable	O
member	O
of	O
the	O
network	O
’s	O
state	O
(	O
so	O
we	O
do	O
not	O
pass	O
gradients	O
for	O
it	O
)	O
,	O
and	O
we	O
update	O
the	O
index	O
upon	O
each	O
write	O
or	O
erase	O
to	O
memory	O
in	O
time	Metric
.	O

Maintaining	Task
and	Task
querying	Task
the	O
ANN	Task
index	Task
represents	O
the	O
most	O
expensive	O
part	O
of	O
the	O
network	O
,	O
which	O
is	O
reasonable	O
as	O
content	Task
-	Task
based	Task
addressing	Task
is	O
inherently	O
expensive	O
.	O

For	O
the	O
backward	Task
pass	Task
computation	Task
,	O
specifically	O
calculating	O
and	O
with	O
respect	O
to	O
,	O
we	O
can	O
once	O
again	O
compute	O
these	O
using	O
sparse	Method
matrix	Method
operations	Method
in	O
time	Metric
.	O

This	O
is	O
because	O
the	O
non	O
-	O
zero	O
locations	O
have	O
been	O
determined	O
during	O
the	O
forward	O
pass	O
.	O

Thus	O
to	O
conclude	O
,	O
SAM	Method
consumes	O
in	O
total	O
space	Metric
for	O
both	O
the	O
forward	O
and	O
backward	O
step	O
during	O
training	O
,	O
time	Metric
per	O
forward	O
step	O
,	O
and	O
per	O
backward	O
step	O
.	O

appendix	O
:	O
Control	Method
flow	Method
appendix	O
:	O
Training	O
details	O
Here	O
we	O
provide	O
additional	O
details	O
on	O
the	O
training	O
regime	O
used	O
for	O
our	O
experiments	O
used	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

To	O
avoid	O
bias	O
in	O
our	O
results	O
,	O
we	O
chose	O
the	O
learning	Metric
rate	Metric
that	O
worked	O
best	O
for	O
DAM	Method
(	O
and	O
not	O
SAM	Method
)	O
.	O

We	O
tried	O
learning	Metric
rates	Metric
and	O
found	O
that	O
DAM	Method
trained	O
best	O
with	O
.	O

We	O
also	O
tried	O
values	O
of	O
and	O
found	O
no	O
significant	O
difference	O
in	O
performance	O
across	O
the	O
values	O
.	O

We	O
used	O
100	O
hidden	O
units	O
for	O
the	O
LSTM	Method
(	O
including	O
the	O
controller	Method
LSTMs	Method
)	O
,	O
a	O
minibatch	Method
of	O
,	O
asynchronous	Method
workers	Method
to	O
speed	O
up	O
training	Task
,	O
and	O
RMSProp	Method
to	O
optimize	O
the	O
controller	O
.	O

We	O
used	O
memory	O
access	O
heads	O
and	O
configured	O
SAM	Method
to	O
read	O
from	O
only	O
locations	O
per	O
head	O
.	O

appendix	O
:	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
Recently	O
proposed	O
a	O
novel	O
MANN	Method
the	O
Differentiable	Method
Neural	Method
Computer	Method
(	O
DNC	Method
)	O
.	O

The	O
two	O
innovations	O
proposed	O
by	O
this	O
model	O
are	O
a	O
new	O
approach	O
to	O
tracking	Task
memory	Task
freeness	Task
(	O
dynamic	Task
memory	Task
allocation	Task
)	O
and	O
a	O
mechanism	O
for	O
associating	O
memories	O
together	O
(	O
temporal	Method
memory	Method
linkage	Method
)	O
.	O

We	O
demonstrate	O
here	O
that	O
the	O
approaches	O
enumerated	O
in	O
the	O
paper	O
can	O
be	O
adapted	O
to	O
new	O
models	O
by	O
outlining	O
a	O
sparse	Method
version	Method
of	O
this	O
model	O
,	O
the	O
Sparse	Method
Differentiable	Method
Neural	Method
Computer	Method
(	O
SDNC	Method
)	O
,	O
which	O
learns	O
with	O
similar	O
data	Metric
efficiency	Metric
while	O
retaining	O
the	O
computational	O
advantages	O
of	O
sparsity	O
.	O

subsection	O
:	O
Architecture	O
For	O
brevity	O
,	O
we	O
will	O
only	O
explain	O
the	O
sparse	O
implementations	O
of	O
these	O
two	O
items	O
,	O
for	O
the	O
full	O
model	O
details	O
refer	O
to	O
the	O
original	O
paper	O
.	O

The	O
mechanism	O
for	O
sparse	Task
memory	Task
reads	Task
and	Task
writes	Task
was	O
implemented	O
identically	O
to	O
SAM	Method
.	O

It	O
is	O
possible	O
to	O
implement	O
a	O
scalable	Method
version	Method
of	O
the	O
dynamic	Method
memory	Method
allocation	Method
system	Method
of	O
the	O
DNC	Method
avoiding	O
any	O
operations	O
by	O
using	O
a	O
heap	O
.	O

However	O
,	O
because	O
it	O
is	O
practical	O
to	O
run	O
the	O
SDNC	Method
with	O
many	O
more	O
memory	O
words	O
,	O
reusing	O
memory	O
is	O
less	O
crucial	O
so	O
we	O
did	O
not	O
implement	O
this	O
and	O
used	O
the	O
same	O
usage	O
tracking	O
as	O
in	O
SAM	Method
.	O

The	O
temporal	Method
memory	Method
linkage	Method
in	O
the	O
DNC	Method
is	O
a	O
system	O
for	O
associating	O
and	O
recalling	O
memory	O
locations	O
which	O
were	O
written	O
in	O
a	O
temporal	O
order	O
,	O
for	O
exampling	Task
storing	Task
and	Task
retrieving	Task
a	Task
list	Task
.	O

In	O
the	O
DNC	Method
this	O
is	O
done	O
by	O
maintaining	O
a	O
temporal	Method
linkage	Method
matrix	Method
.	O

represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
after	O
location	O
.	O

This	O
matrix	O
is	O
updated	O
by	O
tracking	O
the	O
precedence	O
weighting	O
,	O
where	O
represents	O
the	O
degree	O
to	O
which	O
location	O
was	O
written	O
to	O
.	O

The	O
memory	Method
linkage	Method
is	O
updated	O
according	O
to	O
the	O
following	O
recurrence	O
The	O
temporal	Method
linkage	Method
can	O
be	O
used	O
to	O
compute	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
either	O
forward	O
or	O
backward	O
The	O
read	O
head	O
then	O
uses	O
a	O
3	O
-	O
way	O
softmax	O
to	O
select	O
between	O
a	O
content	O
-	O
based	O
read	O
or	O
following	O
the	O
forward	O
or	O
backward	O
weighting	O
.	O

Naively	O
,	O
the	O
link	Method
matrix	Method
requires	O
memory	O
and	O
computation	O
although	O
proposes	O
a	O
method	O
to	O
reduce	O
the	O
computational	Metric
cost	Metric
to	O
and	O
memory	Metric
cost	Metric
.	O

In	O
order	O
to	O
maintain	O
the	O
scaling	O
properties	O
of	O
the	O
SAM	Method
,	O
we	O
wish	O
to	O
avoid	O
any	O
computational	O
dependence	O
on	O
.	O

We	O
do	O
this	O
by	O
maintaining	O
two	O
sparse	O
matrices	O
that	O
approximate	O
and	O
respectively	O
.	O

We	O
store	O
these	O
matrices	O
in	O
Compressed	O
Sparse	O
Row	O
format	O
.	O

They	O
are	O
defined	O
by	O
the	O
following	O
updates	O
:	O
Additionally	O
,	O
is	O
,	O
as	O
with	O
the	O
other	O
weight	O
vectors	O
maintained	O
as	O
a	O
sparse	O
vector	O
with	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

This	O
means	O
that	O
the	O
outer	O
product	O
of	O
has	O
at	O
most	O
non	O
-	O
zero	O
entries	O
.	O

In	O
addition	O
to	O
the	O
updates	O
specified	O
above	O
,	O
we	O
also	O
constrain	O
each	O
row	O
of	O
the	O
matrices	O
and	O
to	O
have	O
at	O
most	O
non	O
-	O
zero	O
entries	O
—	O
this	O
constraint	O
can	O
be	O
applied	O
in	O
because	O
at	O
most	O
rows	O
change	O
in	O
the	O
matrix	O
.	O

Once	O
these	O
matrices	O
are	O
applied	O
the	O
read	O
weights	O
following	O
the	O
temporal	O
links	O
can	O
be	O
computed	O
similar	O
to	O
before	O
:	O
Note	O
,	O
the	O
number	O
of	O
locations	O
we	O
read	O
from	O
,	O
,	O
does	O
not	O
have	O
to	O
equal	O
the	O
number	O
of	O
outward	O
and	O
inward	O
links	O
we	O
preserve	O
,	O
.	O

We	O
typically	O
choose	O
as	O
this	O
is	O
still	O
very	O
fast	O
to	O
compute	O
(	O
in	O
total	O
to	O
calculate	O
on	O
a	O
single	O
CPU	O
thread	O
)	O
and	O
we	O
see	O
no	O
learning	O
benefit	O
with	O
larger	O
.	O

In	O
order	O
to	O
compute	O
the	O
gradients	O
,	O
and	O
need	O
to	O
be	O
stored	O
.	O

This	O
could	O
be	O
done	O
by	O
maintaining	O
a	O
sparse	O
record	O
of	O
the	O
updates	O
applied	O
and	O
reversing	O
them	O
,	O
similar	O
to	O
that	O
performed	O
with	O
the	O
memory	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O

However	O
,	O
for	O
implementation	O
simplicity	O
we	O
did	O
not	O
pass	O
gradients	O
through	O
the	O
temporal	O
linkage	O
matrices	O
.	O

subsection	O
:	O
Results	O
We	O
benchmarked	O
the	O
speed	Metric
and	Metric
memory	Metric
performance	Metric
of	O
the	O
SDNC	Method
versus	O
a	O
naive	O
DNC	Method
implementation	O
(	O
details	O
of	O
setup	O
in	O
Supplementary	O
[	O
reference	O
]	O
)	O
.	O

The	O
results	O
are	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Here	O
,	O
the	O
computational	O
benefits	O
of	O
sparsity	O
are	O
more	O
pronounced	O
due	O
to	O
the	O
expensive	O
(	O
quadratic	O
time	Metric
and	O
space	Metric
)	O
temporal	O
transition	O
table	O
operations	O
in	O
the	O
DNC	Method
.	O

We	O
were	O
only	O
able	O
to	O
run	O
comparative	O
benchmarks	O
up	O
to	O
,	O
as	O
the	O
DNC	Method
quickly	O
exceeded	O
the	O
machine	O
’s	O
physical	O
memory	O
for	O
larger	O
values	O
;	O
however	O
even	O
at	O
this	O
modest	O
memory	O
size	O
we	O
see	O
a	O
speed	O
increase	O
of	O
and	O
physical	Metric
memory	Metric
reduction	Metric
of	O
.	O

Note	O
,	O
unlike	O
the	O
SAM	Method
memory	O
benchmark	O
in	O
Section	O
[	O
reference	O
]	O
we	O
plot	O
the	O
total	Metric
memory	Metric
consumption	Metric
,	O
i.e.	O
the	O
memory	Metric
overhead	Metric
of	O
the	O
initial	O
start	O
state	O
plus	O
the	O
memory	O
overhead	O
of	O
unrolling	O
the	O
core	O
over	O
a	O
sequence	O
.	O

This	O
is	O
because	O
the	O
SDNC	Method
and	O
DNC	Method
do	O
not	O
have	O
identical	O
start	O
states	O
.	O

The	O
sparse	Method
temporal	Method
transition	Method
matrices	Method
consume	O
much	O
less	O
memory	O
than	O
the	O
corresponding	O
in	O
the	O
DNC	Method
.	O

0.47	O
0.47	O
In	O
order	O
to	O
compare	O
the	O
models	O
on	O
an	O
interesting	O
task	O
we	O
ran	O
the	O
DNC	Method
and	O
SDNC	Method
on	O
the	O
Babi	Material
task	O
(	O
this	O
task	O
is	O
described	O
more	O
fully	O
in	O
the	O
main	O
text	O
)	O
.	O

The	O
results	O
are	O
described	O
in	O
Supplementary	O
[	O
reference	O
]	O
and	O
demonstrate	O
the	O
SDNC	Method
is	O
capable	O
of	O
learning	Task
competitively	O
.	O

In	O
particular	O
,	O
it	O
achieves	O
the	O
best	O
report	O
result	O
on	O
the	O
Babi	Material
task	O
.	O

appendix	O
:	O
Benchmarking	O
details	O
Each	O
model	O
contained	O
an	O
LSTM	Method
controller	O
with	O
100	O
hidden	O
units	O
,	O
an	O
external	O
memory	O
containing	O
slots	O
of	O
memory	O
,	O
with	O
word	O
size	O
and	O
access	O
heads	O
.	O

For	O
speed	Task
benchmarks	Task
,	O
a	O
minibatch	O
size	O
of	O
was	O
used	O
to	O
ensure	O
fair	O
comparison	O
-	O
as	O
many	O
dense	O
operations	O
(	O
e.g.	O
matrix	Task
multiplication	Task
)	O
can	O
be	O
batched	O
efficiently	O
.	O

For	O
memory	Task
benchmarks	Task
,	O
the	O
minibatch	O
size	O
was	O
set	O
to	O
.	O

We	O
used	O
Torch7	Method
to	O
implement	O
SAM	Method
,	O
DAM	Method
,	O
NTM	Method
,	O
DNC	Method
and	O
SDNC	Method
.	O

Eigen	Method
v3	Method
was	O
used	O
for	O
the	O
fast	O
sparse	Task
tensor	Task
operations	Task
,	O
using	O
the	O
provided	O
CSC	O
and	O
CSR	O
formats	O
.	O

All	O
benchmarks	O
were	O
run	O
on	O
a	O
Linux	O
desktop	O
running	O
Ubuntu	O
14.04.1	O
with	O
32GiB	O
of	O
RAM	O
and	O
an	O
Intel	O
Xeon	O
E5	O
-	O
1650	O
3.20GHz	O
processor	O
with	O
power	O
scaling	O
disabled	O
.	O

appendix	O
:	O
Generalization	O
on	O
associative	Task
recall	Task
appendix	O
:	O
Babi	Material
results	O
See	O
the	O
main	O
text	O
for	O
a	O
description	O
of	O
the	O
Babi	Material
task	O
and	O
its	O
relevance	O
.	O

Here	O
we	O
report	O
the	O
best	O
and	O
mean	O
results	O
for	O
all	O
of	O
the	O
models	O
on	O
this	O
task	O
.	O

The	O
task	O
was	O
encoded	O
using	O
straightforward	O
1	Method
-	Method
hot	Method
word	Method
encodings	Method
for	O
both	O
the	O
input	O
and	O
output	O
.	O

We	O
trained	O
a	O
single	O
model	O
on	O
all	O
of	O
the	O
tasks	O
,	O
and	O
used	O
the	O
10	O
,	O
000	O
examples	O
per	O
task	O
version	O
of	O
the	O
training	O
set	O
(	O
a	O
small	O
subset	O
of	O
which	O
we	O
used	O
as	O
a	O
validation	O
set	O
for	O
selecting	O
the	O
best	O
run	O
and	O
hyperparameters	O
)	O
.	O

Previous	O
work	O
has	O
reported	O
best	O
results	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
,	O
which	O
with	O
only	O
15	O
runs	O
is	O
a	O
noisy	O
comparison	O
,	O
so	O
we	O
additionally	O
report	O
the	O
mean	O
and	O
variance	O
for	O
all	O
runs	O
with	O
the	O
best	O
selected	O
hyperparameters	O
(	O
Supplementary	O
table	O
[	O
reference	O
]	O
)	O
.	O

