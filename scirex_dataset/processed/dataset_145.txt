document	O
:	O
PointFusion	Method
:	O
Deep	Task
Sensor	Task
Fusion	Task
for	O
3D	Task
Bounding	Task
Box	Task
Estimation	Task
We	O
present	O
PointFusion	Method
,	O
a	O
generic	Method
3D	Method
object	Method
detection	Method
method	Method
that	O
leverages	O
both	O
image	O
and	O
3D	O
point	O
cloud	O
information	O
.	O

Unlike	O
existing	O
methods	O
that	O
either	O
use	O
multi	Method
-	Method
stage	Method
pipelines	Method
or	O
hold	O
sensor	O
and	O
dataset	O
-	O
specific	O
assumptions	O
,	O
PointFusion	Method
is	O
conceptually	O
simple	O
and	O
application	O
-	O
agnostic	O
.	O

The	O
image	O
data	O
and	O
the	O
raw	O
point	O
cloud	O
data	O
are	O
independently	O
processed	O
by	O
a	O
CNN	Method
and	O
a	O
PointNet	Method
architecture	Method
,	O
respectively	O
.	O

The	O
resulting	O
outputs	O
are	O
then	O
combined	O
by	O
a	O
novel	O
fusion	Method
network	Method
,	O
which	O
predicts	O
multiple	O
3D	O
box	O
hypotheses	O
and	O
their	O
confidences	O
,	O
using	O
the	O
input	O
3D	O
points	O
as	O
spatial	O
anchors	O
.	O

We	O
evaluate	O
PointFusion	Method
on	O
two	O
distinctive	O
datasets	O
:	O
the	O
KITTI	Material
dataset	O
that	O
features	O
driving	O
scenes	O
captured	O
with	O
a	O
lidar	Method
-	Method
camera	Method
setup	Method
,	O
and	O
the	O
SUN	Material
-	Material
RGBD	Material
dataset	O
that	O
captures	O
indoor	O
environments	O
with	O
RGB	O
-	O
D	O
cameras	O
.	O

Our	O
model	O
is	O
the	O
first	O
one	O
that	O
is	O
able	O
to	O
perform	O
better	O
or	O
on	O
-	O
par	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
these	O
diverse	O
datasets	O
without	O
any	O
dataset	O
-	O
specific	O
model	Method
tuning	Method
.	O

section	O
:	O
Introduction	O
We	O
focus	O
on	O
3D	Task
object	Task
detection	Task
,	O
which	O
is	O
a	O
fundamental	O
computer	Task
vision	Task
problem	Task
impacting	O
most	O
autonomous	Task
robotics	Task
systems	Task
including	O
self	Task
-	Task
driving	Task
cars	Task
and	O
drones	Task
.	O

The	O
goal	O
of	O
3D	Task
object	Task
detection	Task
is	O
to	O
recover	O
the	O
6	O
DoF	O
pose	O
and	O
the	O
3D	O
bounding	O
box	O
dimensions	O
for	O
all	O
objects	O
of	O
interest	O
in	O
the	O
scene	O
.	O

While	O
recent	O
advances	O
in	O
convolutional	Method
neural	Method
networks	Method
have	O
enabled	O
accurate	O
2D	Task
detection	O
in	O
complex	Task
environments	Task
,	O
the	O
3D	Task
object	Task
detection	Task
problem	Task
still	O
remains	O
an	O
open	O
challenge	O
.	O

Methods	O
for	O
3D	Task
box	Task
regression	Task
from	O
a	O
single	O
image	O
,	O
even	O
including	O
recent	O
deep	Method
learning	Method
methods	Method
such	O
as	O
,	O
still	O
have	O
relatively	O
low	O
accuracy	Metric
especially	O
in	O
depth	Task
estimates	Task
at	O
longer	O
ranges	O
.	O

Hence	O
,	O
many	O
current	O
real	Method
-	Method
world	Method
systems	Method
either	O
use	O
stereo	Method
or	O
augment	O
their	O
sensor	Method
stack	Method
with	O
lidar	Method
and	O
radar	Method
.	O

The	O
lidar	Method
-	O
radar	Method
mixed	O
-	O
sensor	O
setup	O
is	O
particularly	O
popular	O
in	O
self	Task
-	Task
driving	Task
cars	Task
and	O
is	O
typically	O
handled	O
by	O
a	O
multi	Method
-	Method
stage	Method
pipeline	Method
,	O
which	O
preprocesses	O
each	O
sensor	O
modality	O
separately	O
and	O
then	O
performs	O
a	O
late	Method
fusion	Method
or	O
decision	Method
-	Method
level	Method
fusion	Method
step	Method
using	O
an	O
expert	Method
-	Method
designed	Method
tracking	Method
system	Method
such	O
as	O
a	O
Kalman	Method
filter	Method
.	O

Such	O
systems	O
make	O
simplifying	O
assumptions	O
and	O
make	O
decisions	O
in	O
the	O
absence	O
of	O
context	O
from	O
other	O
sensors	O
.	O

Inspired	O
by	O
the	O
successes	O
of	O
deep	Method
learning	Method
for	O
handling	O
diverse	O
raw	O
sensory	O
input	O
,	O
we	O
propose	O
an	O
early	O
fusion	Method
model	Method
for	O
3D	Task
box	Task
estimation	Task
,	O
which	O
directly	O
learns	O
to	O
combine	O
image	O
and	O
depth	O
information	O
optimally	O
.	O

Various	O
combinations	O
of	O
cameras	O
and	O
3D	O
sensors	O
are	O
widely	O
used	O
in	O
the	O
field	O
,	O
and	O
it	O
is	O
desirable	O
to	O
have	O
a	O
single	O
algorithm	O
that	O
generalizes	O
to	O
as	O
many	O
different	O
problem	O
settings	O
as	O
possible	O
.	O

Many	O
real	Task
-	Task
world	Task
robotic	Task
systems	Task
are	O
equipped	O
with	O
multiple	O
3D	O
sensors	O
:	O
for	O
example	O
,	O
autonomous	Task
cars	Task
often	O
have	O
multiple	O
lidars	O
and	O
potentially	O
also	O
radars	O
.	O

Yet	O
,	O
current	O
algorithms	O
often	O
assume	O
a	O
single	O
RGB	Method
-	Method
D	Method
camera	Method
,	O
which	O
provides	O
RGB	Material
-	Material
D	Material
images	Material
,	O
or	O
a	O
single	O
lidar	Method
sensor	O
,	O
which	O
allows	O
the	O
creation	O
of	O
a	O
local	O
front	O
view	O
image	O
of	O
the	O
lidar	Method
depth	O
and	O
intensity	O
readings	O
.	O

Many	O
existing	O
algorithms	O
also	O
make	O
strong	O
domain	O
-	O
specific	O
assumptions	O
.	O

For	O
example	O
,	O
MV3D	Method
assumes	O
that	O
all	O
objects	O
can	O
be	O
segmented	O
in	O
a	O
top	O
-	O
down	O
2D	Task
view	O
of	O
the	O
point	O
cloud	O
,	O
which	O
works	O
for	O
the	O
common	O
self	Task
-	Task
driving	Task
case	Task
but	O
does	O
not	O
generalize	O
to	O
indoor	O
scenes	O
where	O
objects	O
can	O
be	O
placed	O
on	O
top	O
of	O
each	O
other	O
.	O

Furthermore	O
,	O
the	O
top	Method
-	Method
down	Method
view	Method
approach	Method
tends	O
to	O
work	O
well	O
for	O
objects	O
such	O
as	O
cars	O
,	O
but	O
does	O
not	O
for	O
other	O
key	O
object	O
classes	O
such	O
as	O
pedestrians	O
or	O
cyclists	O
.	O

Unlike	O
the	O
above	O
approaches	O
,	O
our	O
architecture	O
is	O
designed	O
to	O
be	O
domain	O
-	O
agnostic	O
and	O
agnostic	O
to	O
the	O
placement	O
,	O
type	O
,	O
and	O
number	O
of	O
3D	O
sensors	O
.	O

As	O
such	O
,	O
it	O
is	O
generic	O
and	O
can	O
be	O
used	O
for	O
a	O
variety	O
of	O
robotics	Task
applications	Task
.	O

In	O
designing	O
such	O
a	O
generic	Method
model	Method
,	O
we	O
need	O
to	O
solve	O
the	O
challenge	O
of	O
combining	O
the	O
heterogeneous	O
image	O
and	O
3D	O
point	O
cloud	O
data	O
.	O

Previous	O
work	O
addresses	O
this	O
challenge	O
by	O
directly	O
transforming	O
the	O
point	O
cloud	O
to	O
a	O
convolution	O
-	O
friendly	O
form	O
.	O

This	O
includes	O
either	O
projecting	O
the	O
point	O
cloud	O
onto	O
the	O
image	O
or	O
voxelizing	O
the	O
point	O
cloud	O
.	O

Both	O
of	O
these	O
operations	O
involve	O
lossy	Method
data	Method
quantization	Method
and	O
require	O
special	O
models	O
to	O
handle	O
sparsity	O
in	O
the	O
lidar	Method
image	O
or	O
in	O
voxel	O
space	O
.	O

Instead	O
,	O
our	O
solution	O
retains	O
the	O
inputs	O
in	O
their	O
native	O
representation	O
and	O
processes	O
them	O
using	O
heterogeneous	Method
network	Method
architectures	Method
.	O

Specifically	O
for	O
the	O
point	Task
cloud	Task
,	O
we	O
use	O
a	O
variant	O
of	O
the	O
recently	O
proposed	O
PointNet	Method
architecture	Method
,	O
which	O
allows	O
us	O
to	O
process	O
the	O
raw	O
points	O
directly	O
.	O

Our	O
deep	Method
network	Method
for	O
3D	Task
object	Task
box	Task
regression	Task
from	O
images	O
and	O
sparse	O
point	O
clouds	O
has	O
three	O
main	O
components	O
:	O
an	O
off	Method
-	Method
the	Method
-	Method
shelf	Method
CNN	Method
that	O
extracts	O
appearance	O
and	O
geometry	O
features	O
from	O
input	O
RGB	Material
image	Material
crops	Material
,	O
a	O
variant	O
of	O
PointNet	Method
that	O
processes	O
the	O
raw	O
3D	O
point	O
cloud	O
,	O
and	O
a	O
fusion	Method
sub	Method
-	Method
network	Method
that	O
combines	O
the	O
two	O
outputs	O
to	O
predict	O
3D	O
bounding	O
boxes	O
.	O

This	O
heterogeneous	Method
network	Method
architecture	Method
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
takes	O
full	O
advantage	O
of	O
the	O
two	O
data	O
sources	O
without	O
introducing	O
any	O
data	O
processing	O
biases	O
.	O

Our	O
fusion	Method
sub	Method
-	Method
network	Method
features	O
a	O
novel	O
dense	Method
3D	Method
box	Method
prediction	Method
architecture	Method
,	O
in	O
which	O
for	O
each	O
input	O
3D	O
point	O
,	O
the	O
network	O
predicts	O
the	O
corner	O
locations	O
of	O
a	O
3D	O
box	O
relative	O
to	O
the	O
point	O
.	O

The	O
network	O
then	O
uses	O
a	O
learned	O
scoring	Method
function	Method
to	O
select	O
the	O
best	O
prediction	O
.	O

The	O
method	O
is	O
inspired	O
by	O
the	O
concept	O
of	O
spatial	Method
anchors	Method
and	O
dense	Task
prediction	Task
.	O

The	O
intuition	O
is	O
that	O
predicting	O
relative	O
spatial	O
locations	O
using	O
input	O
3D	O
points	O
as	O
anchors	O
reduces	O
the	O
variance	O
of	O
the	O
regression	Metric
objective	Metric
comparing	O
to	O
an	O
architecture	O
that	O
directly	O
regresses	O
the	O
3D	O
location	O
of	O
each	O
corner	O
.	O

We	O
demonstrate	O
that	O
the	O
dense	Method
prediction	Method
architecture	Method
outperforms	O
the	O
architecture	O
that	O
regresses	O
3D	O
corner	O
locations	O
directly	O
by	O
a	O
large	O
margin	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
distinctive	O
3D	Task
object	Task
detection	Task
datasets	Task
.	O

The	O
KITTI	Material
dataset	O
focuses	O
on	O
the	O
outdoor	Task
urban	Task
driving	Task
scenario	Task
in	O
which	O
pedestrians	O
,	O
cyclists	O
,	O
and	O
cars	O
are	O
annotated	O
in	O
data	O
acquired	O
with	O
a	O
camera	O
-	O
lidar	Method
system	O
.	O

The	O
SUN	Material
-	Material
RGBD	Material
dataset	O
is	O
recorded	O
via	O
RGB	O
-	O
D	O
cameras	O
in	O
indoor	O
environments	O
,	O
with	O
more	O
than	O
700	O
object	O
categories	O
.	O

We	O
show	O
that	O
by	O
combining	O
PointFusion	Method
with	O
an	O
off	O
-	O
the	O
-	O
shelf	O
2D	Task
object	O
detector	O
,	O
we	O
get	O
comparable	O
or	O
better	O
3D	Task
object	Task
detections	Task
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
designed	O
for	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
model	O
is	O
the	O
first	O
to	O
achieve	O
competitive	O
results	O
on	O
these	O
very	O
different	O
datasets	O
,	O
proving	O
its	O
general	O
applicability	O
.	O

section	O
:	O
Related	O
Work	O
We	O
give	O
an	O
overview	O
of	O
the	O
previous	O
work	O
on	O
6	Task
-	Task
DoF	Task
object	Task
pose	Task
estimation	Task
,	O
which	O
is	O
related	O
to	O
our	O
approach	O
.	O

Geometry	Method
-	Method
based	Method
methods	Method
A	O
number	O
of	O
methods	O
focus	O
on	O
estimating	O
the	O
6	Task
-	Task
DoF	Task
object	Task
pose	Task
from	O
a	O
single	O
image	O
or	O
an	O
image	O
sequence	O
.	O

These	O
include	O
keypoint	Task
matching	Task
between	O
2D	Task
images	O
and	O
their	O
corresponding	O
3D	Method
CAD	Method
models	Method
,	O
or	O
aligning	O
3D	Method
-	Method
reconstructed	Method
models	Method
with	O
ground	Method
-	Method
truth	Method
models	Method
to	O
recover	O
the	O
object	O
poses	O
.	O

Gupta	O
propose	O
to	O
predict	O
a	O
semantic	Task
segmentation	Task
map	Task
as	O
well	O
as	O
object	O
pose	O
hypotheses	O
using	O
a	O
CNN	Method
and	O
then	O
align	O
the	O
hypotheses	O
with	O
known	O
object	Method
CAD	Method
models	Method
using	O
ICP	Method
.	O

These	O
types	O
of	O
methods	O
rely	O
on	O
strong	O
category	O
shape	O
priors	O
or	O
ground	Method
-	Method
truth	Method
object	Method
CAD	Method
models	Method
,	O
which	O
makes	O
them	O
difficult	O
to	O
scale	O
to	O
larger	O
datasets	O
.	O

In	O
contrary	O
,	O
our	O
generic	O
method	O
estimates	O
both	O
the	O
6	O
-	O
DoF	O
pose	O
and	O
spatial	O
dimensions	O
of	O
an	O
object	O
without	O
object	O
category	O
knowledge	O
or	O
CAD	Method
models	Method
.	O

3D	Task
box	Task
regression	Task
from	O
images	O
The	O
recent	O
advances	O
in	O
deep	Method
models	Method
have	O
dramatically	O
improved	O
2D	Task
object	O
detection	O
,	O
and	O
some	O
methods	O
propose	O
to	O
extend	O
the	O
objectives	O
with	O
the	O
full	O
3D	O
object	O
poses	O
.	O

uses	O
R	Method
-	Method
CNN	Method
to	O
propose	O
2D	Task
RoI	O
and	O
another	O
network	O
to	O
regress	O
the	O
object	O
poses	O
.	O

combines	O
a	O
set	O
of	O
deep	O
-	O
learned	O
3D	O
object	O
parameters	O
and	O
geometric	O
constraints	O
from	O
2D	Task
RoIs	O
to	O
recover	O
the	O
full	O
3D	O
box	O
.	O

Xiang	O
jointly	O
learns	O
a	O
viewpoint	Method
-	Method
dependent	Method
detector	Method
and	O
a	O
pose	Method
estimator	Method
by	O
clustering	O
3D	O
voxel	O
patterns	O
learned	O
from	O
object	Method
models	Method
.	O

Although	O
these	O
methods	O
excel	O
at	O
estimating	Task
object	Task
orientations	Task
,	O
localizing	O
the	O
objects	O
in	O
3D	O
from	O
an	O
image	O
is	O
often	O
handled	O
by	O
imposing	O
geometric	O
constraints	O
and	O
remains	O
a	O
challenge	O
for	O
lack	O
of	O
direct	Task
depth	Task
measurements	Task
.	O

One	O
of	O
the	O
key	O
contributions	O
of	O
our	O
model	O
is	O
that	O
it	O
learns	O
to	O
effectively	O
combine	O
the	O
complementary	O
image	O
and	O
depth	O
sensor	O
information	O
.	O

3D	Method
box	Method
regression	Method
from	O
depth	O
data	O
Newer	O
studies	O
have	O
proposed	O
to	O
directly	O
tackle	O
the	O
3D	Task
object	Task
detection	Task
problem	Task
in	O
discretized	Task
3D	Task
spaces	Task
.	O

Song	O
learn	O
to	O
classify	O
3D	Task
bounding	Task
box	Task
proposals	Task
generated	O
by	O
a	O
3D	O
sliding	O
window	O
using	O
synthetically	O
-	O
generated	O
3D	O
features	O
.	O

A	O
follow	O
-	O
up	O
study	O
uses	O
a	O
3D	Method
variant	Method
of	O
the	O
Region	Method
Proposal	Method
Network	Method
to	O
generate	O
3D	Task
proposals	Task
and	O
uses	O
a	O
3D	Method
ConvNet	Method
to	O
process	O
the	O
voxelized	O
point	O
cloud	O
.	O

A	O
similar	O
approach	O
by	O
Li	O
focuses	O
on	O
detecting	Task
vehicles	Task
and	O
processes	O
the	O
voxelized	O
input	O
with	O
a	O
3D	Method
fully	Method
convolutional	Method
network	Method
.	O

However	O
,	O
these	O
methods	O
are	O
often	O
prohibitively	O
expensive	O
because	O
of	O
the	O
discretized	Method
volumetric	Method
representation	Method
.	O

As	O
an	O
example	O
,	O
takes	O
around	O
20	O
seconds	O
to	O
process	O
one	O
frame	O
.	O

Other	O
methods	O
,	O
such	O
as	O
VeloFCN	Method
,	O
focus	O
on	O
a	O
single	O
lidar	Method
setup	O
and	O
form	O
a	O
dense	O
depth	O
and	O
intensity	O
image	O
,	O
which	O
is	O
processed	O
with	O
a	O
single	O
2D	Task
CNN	O
.	O

Unlike	O
these	O
methods	O
,	O
we	O
adopt	O
the	O
recently	O
proposed	O
PointNet	Method
to	O
process	O
the	O
raw	Task
point	Task
cloud	Task
.	O

The	O
setup	O
can	O
accommodate	O
multiple	O
depth	O
sensors	O
,	O
and	O
the	O
time	Metric
complexity	Metric
scales	O
linearly	O
with	O
the	O
number	O
of	O
range	O
measurements	O
irrespective	O
of	O
the	O
spatial	O
extent	O
of	O
the	O
3D	O
scene	O
.	O

2D	Task
-	O
3D	O
fusion	O
Our	O
paper	O
is	O
most	O
related	O
to	O
recent	O
methods	O
that	O
fuse	O
image	O
and	O
lidar	Method
data	O
.	O

MV3D	Method
by	O
Chen	O
generates	O
object	Task
detection	Task
proposals	Task
in	O
the	O
top	O
-	O
down	O
lidar	Method
view	O
and	O
projects	O
them	O
to	O
the	O
front	O
-	O
lidar	Method
and	O
image	O
views	O
,	O
fusing	O
all	O
the	O
corresponding	O
features	O
to	O
do	O
oriented	Method
box	Method
regression	Method
.	O

This	O
approach	O
assumes	O
a	O
single	O
-	O
lidar	Method
setup	O
and	O
bakes	O
in	O
the	O
restrictive	O
assumption	O
that	O
all	O
objects	O
are	O
on	O
the	O
same	O
spatial	O
plane	O
and	O
can	O
be	O
localized	O
solely	O
from	O
a	O
top	O
-	O
down	O
view	O
of	O
the	O
point	O
cloud	O
,	O
which	O
works	O
for	O
cars	Task
but	O
not	O
pedestrians	O
and	O
bicyclists	O
.	O

In	O
contrast	O
,	O
our	O
method	O
has	O
no	O
scene	O
or	O
object	O
-	O
specific	O
limitations	O
,	O
as	O
well	O
as	O
no	O
limitations	O
on	O
the	O
kind	O
and	O
number	O
of	O
depth	O
sensors	O
used	O
.	O

section	O
:	O
PointFusion	Method
In	O
this	O
section	O
,	O
we	O
describe	O
our	O
PointFusion	Method
model	O
,	O
which	O
performs	O
3D	Method
bounding	Method
box	Method
regression	Method
from	O
a	O
2D	Task
image	O
crop	O
and	O
a	O
corresponding	O
3D	O
point	O
cloud	O
that	O
is	O
typically	O
produced	O
by	O
lidar	Method
sensors	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

When	O
our	O
model	O
is	O
combined	O
with	O
a	O
state	O
of	O
the	O
art	O
2D	Task
object	O
detector	O
supplying	O
the	O
2D	Task
object	O
crops	O
,	O
such	O
as	O
,	O
we	O
get	O
a	O
complete	O
3D	Method
object	Method
detection	Method
system	Method
.	O

We	O
leave	O
the	O
theoretically	O
straightforward	O
end	O
-	O
to	O
-	O
end	O
model	O
to	O
future	O
work	O
since	O
we	O
already	O
get	O
state	O
of	O
the	O
art	O
results	O
with	O
this	O
simpler	O
two	O
-	O
stage	O
setup	O
.	O

In	O
addition	O
,	O
the	O
current	O
setup	O
allows	O
us	O
to	O
plug	O
in	O
any	O
state	O
-	O
of	O
-	O
the	Method
-	Method
art	Method
detector	Method
without	O
modifying	O
the	O
fusion	Method
network	Method
.	O

PointFusion	Method
has	O
three	O
main	O
components	O
:	O
a	O
variant	O
of	O
the	O
PointNet	Method
network	Method
that	O
extracts	O
point	O
cloud	O
features	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
A	O
)	O
,	O
a	O
CNN	Method
that	O
extracts	O
image	O
appearance	O
features	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
B	O
)	O
,	O
and	O
a	O
fusion	Method
network	Method
that	O
combines	O
both	O
and	O
outputs	O
a	O
3D	O
bounding	O
box	O
for	O
the	O
object	O
in	O
the	O
crop	O
.	O

We	O
describe	O
two	O
variants	O
of	O
the	O
fusion	Method
network	Method
:	O
a	O
vanilla	Method
global	Method
architecture	Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
C	O
)	O
and	O
a	O
novel	O
dense	Method
fusion	Method
network	Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
D	O
)	O
,	O
in	O
which	O
we	O
use	O
a	O
dense	Method
spatial	Method
anchor	Method
mechanism	Method
to	O
improve	O
the	O
3D	O
box	O
predictions	O
and	O
two	O
scoring	Method
functions	Method
to	O
select	O
the	O
best	O
predictions	O
.	O

Below	O
,	O
we	O
go	O
into	O
the	O
details	O
of	O
our	O
point	Method
cloud	Method
and	O
fusion	Method
sub	Method
-	Method
components	Method
.	O

subsection	O
:	O
Point	Method
Cloud	Method
Network	Method
We	O
process	O
the	O
input	O
point	O
clouds	O
using	O
a	O
variant	O
of	O
the	O
PointNet	Method
architecture	Method
by	O
Qi	Method
.	O

PointNet	Method
pioneered	O
the	O
use	O
of	O
a	O
symmetric	Method
function	Method
(	O
max	Method
-	Method
pooling	Method
)	O
to	O
achieve	O
permutation	O
invariance	O
in	O
the	O
processing	Task
of	Task
unordered	Task
3D	Task
point	Task
cloud	Task
sets	Task
.	O

The	O
model	O
ingests	O
raw	O
point	O
clouds	O
and	O
learns	O
a	O
spatial	Method
encoding	Method
of	O
each	O
point	O
and	O
also	O
an	O
aggregated	O
global	O
point	O
cloud	O
feature	O
.	O

These	O
features	O
are	O
then	O
used	O
for	O
classification	Task
and	Task
semantic	Task
segmentation	Task
.	O

PointNet	Method
has	O
many	O
desirable	O
properties	O
:	O
it	O
processes	O
the	O
raw	O
points	O
directly	O
without	O
lossy	O
operations	O
like	O
voxelization	O
or	O
projection	O
,	O
and	O
it	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
input	O
points	O
.	O

However	O
,	O
the	O
original	O
PointNet	Method
formulation	Method
can	O
not	O
be	O
used	O
for	O
3D	Task
regression	Task
out	Task
of	Task
the	Task
box	Task
.	O

Here	O
we	O
describe	O
two	O
important	O
changes	O
we	O
made	O
to	O
PointNet	O
.	O

paragraph	O
:	O
No	O
BatchNorm	O
Batch	Method
normalization	Method
has	O
become	O
indispensable	O
in	O
modern	O
neural	Method
architecture	Method
design	Method
as	O
it	O
effectively	O
reduces	O
the	O
covariance	O
shift	O
in	O
the	O
input	O
features	O
.	O

In	O
the	O
original	O
PointNet	Method
implementation	Method
,	O
all	O
fully	Method
connected	Method
layers	Method
are	O
followed	O
by	O
a	O
batch	Method
normalization	Method
layer	Method
.	O

However	O
,	O
we	O
found	O
that	O
batch	Method
normalization	Method
hampers	O
the	O
3D	Task
bounding	Task
box	Task
estimation	Task
performance	O
.	O

Batch	Method
normalization	Method
aims	O
to	O
eliminate	O
the	O
scale	O
and	O
bias	O
in	O
its	O
input	O
data	O
,	O
but	O
for	O
the	O
task	O
of	O
3D	Task
regression	Task
,	O
the	O
absolute	O
numerical	O
values	O
of	O
the	O
point	O
locations	O
are	O
helpful	O
.	O

Therefore	O
,	O
our	O
PointNet	Method
variant	Method
has	O
all	O
batch	O
normalization	O
layers	O
removed	O
.	O

paragraph	O
:	O
Input	Method
normalization	Method
As	O
described	O
in	O
the	O
setup	O
,	O
the	O
corresponding	O
3D	Task
point	Task
cloud	Task
of	O
an	O
image	O
bounding	O
box	O
is	O
obtained	O
by	O
finding	O
all	O
points	O
in	O
the	O
scene	O
that	O
can	O
be	O
projected	O
onto	O
the	O
box	O
.	O

However	O
,	O
the	O
spatial	O
location	O
of	O
the	O
3D	O
points	O
is	O
highly	O
correlated	O
with	O
the	O
2D	Task
box	O
location	O
,	O
which	O
introduces	O
undesirable	O
biases	O
.	O

PointNet	Method
applies	O
a	O
Spatial	Method
Transformer	Method
Network	Method
(	O
STN	Method
)	O
to	O
canonicalize	O
the	O
input	O
space	O
.	O

However	O
,	O
we	O
found	O
that	O
the	O
STN	Method
is	O
not	O
able	O
to	O
fully	O
correct	O
these	O
biases	O
.	O

We	O
instead	O
use	O
the	O
known	O
camera	O
geometry	O
to	O
compute	O
the	O
canonical	O
rotation	O
matrix	O
.	O

rotates	O
the	O
ray	O
passing	O
through	O
the	O
center	O
of	O
the	O
2D	Task
box	O
to	O
the	O
-	O
axis	O
of	O
the	O
camera	O
frame	O
.	O

This	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Fusion	Method
Network	Method
The	O
fusion	Method
network	Method
takes	O
as	O
input	O
an	O
image	O
feature	O
extracted	O
using	O
a	O
standard	O
CNN	Method
and	O
the	O
corresponding	O
point	O
cloud	O
feature	O
produced	O
by	O
the	O
PointNet	Method
sub	Method
-	Method
network	Method
.	O

Its	O
job	O
is	O
to	O
combine	O
these	O
features	O
and	O
to	O
output	O
a	O
3D	O
bounding	O
box	O
for	O
the	O
target	O
object	O
.	O

Below	O
we	O
propose	O
two	O
fusion	Method
network	Method
formulations	Method
,	O
a	O
vanilla	Method
global	Method
fusion	Method
network	Method
,	O
and	O
a	O
novel	O
dense	Method
fusion	Method
network	Method
.	O

paragraph	O
:	O
Global	Method
fusion	Method
network	Method
As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
C	O
,	O
the	O
global	Method
fusion	Method
network	Method
processes	O
the	O
image	O
and	O
point	O
cloud	O
features	O
and	O
directly	O
regresses	O
the	O
3D	O
locations	O
of	O
the	O
eight	O
corners	O
of	O
the	O
target	O
bounding	O
box	O
.	O

We	O
experimented	O
with	O
a	O
number	O
of	O
fusion	Method
functions	Method
and	O
found	O
that	O
a	O
concatenation	O
of	O
the	O
two	O
vectors	O
,	O
followed	O
by	O
applying	O
a	O
number	O
of	O
fully	Method
connected	Method
layers	Method
,	O
results	O
in	O
optimal	O
performance	O
.	O

The	O
loss	Method
function	Method
with	O
the	O
global	Method
fusion	Method
network	Method
is	O
then	O
:	O
where	O
are	O
the	O
ground	O
-	O
truth	O
box	O
corners	O
,	O
are	O
the	O
predicted	O
corner	O
locations	O
and	O
is	O
the	O
spatial	O
transformation	O
regularization	O
loss	O
introduced	O
in	O
to	O
enforce	O
the	O
orthogonality	O
of	O
the	O
learned	O
spatial	O
transform	O
matrix	O
.	O

A	O
major	O
drawback	O
of	O
the	O
global	Method
fusion	Method
network	Method
is	O
that	O
the	O
variance	O
of	O
the	O
regression	O
target	O
is	O
directly	O
dependent	O
on	O
the	O
particular	O
scenario	O
.	O

For	O
autonomous	Task
driving	Task
,	O
the	O
system	O
may	O
be	O
expected	O
to	O
detect	O
objects	O
from	O
1	O
m	O
to	O
over	O
100	O
m	O
.	O

This	O
variance	O
places	O
a	O
burden	O
on	O
the	O
network	O
and	O
results	O
in	O
sub	O
-	O
optimal	O
performance	O
.	O

To	O
address	O
this	O
,	O
we	O
turn	O
to	O
the	O
well	O
-	O
studied	O
problem	O
of	O
2D	Task
object	O
detection	O
for	O
inspiration	Task
.	O

Instead	O
of	O
directly	O
regressing	O
the	O
2D	Task
box	O
,	O
a	O
common	O
solution	O
is	O
to	O
generate	O
object	O
proposals	O
by	O
using	O
sliding	O
windows	O
or	O
by	O
predicting	O
the	O
box	O
displacement	O
relative	O
to	O
spatial	O
anchors	O
.	O

These	O
ideas	O
motivate	O
our	O
dense	Method
fusion	Method
network	Method
,	O
which	O
is	O
described	O
below	O
.	O

paragraph	O
:	O
Dense	Method
fusion	Method
network	Method
The	O
main	O
idea	O
behind	O
this	O
model	O
is	O
to	O
use	O
the	O
input	O
3D	O
points	O
as	O
dense	O
spatial	O
anchors	O
.	O

Instead	O
of	O
directly	O
regressing	O
the	O
absolute	O
locations	O
of	O
the	O
3D	O
box	O
corners	O
,	O
for	O
each	O
input	O
3D	O
point	O
we	O
predict	O
the	O
spatial	O
offsets	O
from	O
that	O
point	O
to	O
the	O
corner	O
locations	O
of	O
a	O
nearby	O
box	O
.	O

As	O
a	O
result	O
,	O
the	O
network	O
becomes	O
agnostic	O
to	O
the	O
spatial	O
extent	O
of	O
a	O
scene	O
.	O

The	O
model	O
architecture	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
C.	O
We	O
use	O
a	O
variant	O
of	O
PointNet	Method
that	O
outputs	O
point	O
-	O
wise	O
features	O
.	O

For	O
each	O
point	O
,	O
these	O
are	O
concatenated	O
with	O
the	O
global	O
PointNet	O
feature	O
and	O
the	O
image	O
feature	O
resulting	O
in	O
an	O
input	O
tensor	O
.	O

The	O
dense	Method
fusion	Method
network	Method
processes	O
this	O
input	O
using	O
several	O
layers	O
and	O
outputs	O
a	O
3D	Method
bounding	Method
box	Method
prediction	Method
along	O
with	O
a	O
score	O
for	O
each	O
point	O
.	O

At	O
test	O
time	O
,	O
the	O
prediction	O
that	O
has	O
the	O
highest	O
score	O
is	O
selected	O
to	O
be	O
the	O
final	O
prediction	O
.	O

Concretely	O
,	O
the	O
loss	Metric
function	Metric
of	O
the	O
dense	Method
fusion	Method
network	Method
is	O
:	O
where	O
is	O
the	O
number	O
of	O
the	O
input	O
points	O
,	O
is	O
the	O
offset	O
between	O
the	O
ground	O
truth	O
box	O
corner	O
locations	O
and	O
the	O
-	O
th	O
input	O
point	O
,	O
and	O
contains	O
the	O
predicted	O
offsets	O
.	O

is	O
the	O
score	O
function	O
loss	O
,	O
which	O
we	O
explain	O
in	O
depth	O
in	O
the	O
next	O
subsection	O
.	O

paragraph	O
:	O
3D	Method
box	Method
parameterization	Method
We	O
parameterize	O
a	O
3D	O
box	O
by	O
its	O
8	O
corners	O
since	O
:	O
(	O
1	O
)	O
The	O
representation	O
is	O
employed	O
in	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
which	O
facilitates	O
fair	O
comparison	O
.	O

(	O
2	O
)	O
It	O
generalizes	O
any	O
3D	O
shapes	O
with	O
N	O
reference	O
points	O
,	O
and	O
it	O
works	O
well	O
with	O
our	O
spatial	Method
anchor	Method
scheme	Method
:	O
we	O
can	O
predict	O
the	O
spatial	O
offsets	O
instead	O
of	O
the	O
absolute	O
locations	O
of	O
the	O
corners	O
.	O

subsection	O
:	O
Dense	Task
Fusion	Task
Prediction	Task
Scoring	Task
The	O
goal	O
of	O
the	O
function	O
is	O
to	O
focus	O
the	O
network	O
on	O
learning	O
spatial	O
offsets	O
from	O
points	O
that	O
are	O
close	O
to	O
the	O
target	O
box	O
.	O

We	O
propose	O
two	O
scoring	Method
functions	Method
:	O
a	O
supervised	Method
scoring	Method
function	Method
that	O
directly	O
trains	O
the	O
network	O
to	O
predict	O
if	O
a	O
point	O
is	O
inside	O
the	O
target	O
bounding	O
box	O
and	O
an	O
unsupervised	Method
scoring	Method
function	Method
that	O
lets	O
network	O
to	O
choose	O
the	O
point	O
that	O
would	O
result	O
in	O
the	O
optimal	O
prediction	O
.	O

paragraph	O
:	O
Supervised	Method
scoring	Method
The	O
supervised	Metric
scoring	Metric
loss	Metric
trains	O
the	O
network	O
to	O
predict	O
if	O
a	O
point	O
is	O
inside	O
the	O
target	O
box	O
.	O

Let	O
’s	O
denote	O
the	O
offset	O
regression	O
loss	O
for	O
point	O
as	O
,	O
and	O
the	O
binary	Metric
classification	Metric
loss	Metric
of	O
the	O
-	O
th	O
point	O
as	O
.	O

Then	O
we	O
have	O
:	O
where	O
indicates	O
whether	O
the	O
-	O
th	O
point	O
is	O
in	O
the	O
target	O
bounding	O
box	O
and	O
is	O
a	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
that	O
penalizes	O
incorrect	O
predictions	O
of	O
whether	O
a	O
given	O
point	O
is	O
inside	O
the	O
box	O
.	O

As	O
defined	O
,	O
this	O
supervised	Method
scoring	Method
function	Method
focuses	O
the	O
network	O
on	O
learning	O
to	O
predict	O
the	O
spatial	O
offsets	O
from	O
points	O
that	O
are	O
inside	O
the	O
target	O
bounding	O
box	O
.	O

However	O
,	O
this	O
formulation	O
might	O
not	O
give	O
the	O
optimal	O
result	O
,	O
as	O
the	O
point	O
most	O
confidently	O
inside	O
the	O
box	O
may	O
not	O
be	O
the	O
point	O
with	O
the	O
best	O
prediction	O
.	O

paragraph	O
:	O
Unsupervised	Method
scoring	Method
The	O
goal	O
of	O
unsupervised	Task
scoring	Task
is	O
to	O
let	O
the	O
network	O
learn	O
directly	O
which	O
points	O
are	O
likely	O
to	O
give	O
the	O
best	O
hypothesis	O
,	O
whether	O
they	O
are	O
most	O
confidently	O
inside	O
the	O
object	O
box	O
or	O
not	O
.	O

We	O
need	O
to	O
train	O
the	O
network	O
to	O
assign	O
high	O
confidence	O
to	O
the	O
point	O
that	O
is	O
likely	O
to	O
produce	O
a	O
good	O
prediction	O
.	O

The	O
formulation	O
includes	O
two	O
competing	O
loss	O
terms	O
:	O
we	O
prefer	O
high	O
confidences	O
for	O
all	O
points	O
,	O
however	O
,	O
corner	O
prediction	O
errors	O
are	O
scored	O
proportional	O
to	O
this	O
confidence	O
.	O

Let	O
’s	O
define	O
to	O
be	O
the	O
corner	O
offset	O
regression	O
loss	O
for	O
point	O
.	O

Then	O
the	O
loss	O
becomes	O
:	O
where	O
is	O
the	O
weight	O
factor	O
between	O
the	O
two	O
terms	O
.	O

Above	O
,	O
the	O
second	O
term	O
encodes	O
a	O
logarithmic	O
bonus	O
for	O
increasing	O
confidences	O
.	O

We	O
empirically	O
find	O
the	O
best	O
and	O
use	O
in	O
all	O
of	O
our	O
experiments	O
.	O

section	O
:	O
Experiments	O
We	O
focus	O
on	O
answering	O
two	O
questions	O
:	O
1	O
)	O
does	O
PointFusion	Method
perform	O
well	O
on	O
different	O
sensor	O
configurations	O
and	O
environments	O
compared	O
to	O
models	O
that	O
hold	O
dataset	O
or	O
sensor	O
-	O
specific	O
assumptions	O
,	O
and	O
2	O
)	O
do	O
the	O
dense	Method
prediction	Method
architectures	Method
perform	O
better	O
than	O
architectures	O
that	O
directly	O
regress	O
the	O
spatial	O
locations	O
.	O

To	O
answer	O
1	O
)	O
,	O
we	O
compare	O
our	O
model	O
against	O
the	O
state	O
of	O
the	O
art	O
on	O
two	O
distinctive	O
datasets	O
,	O
the	O
KITTI	Material
dataset	O
and	O
the	O
SUN	Material
-	Material
RGBD	Material
dataset	O
.	O

To	O
answer	O
2	O
)	O
,	O
we	O
conduct	O
ablation	Task
studies	Task
for	O
the	O
model	Method
variations	Method
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Datasets	O
paragraph	O
:	O
KITTI	Material
The	O
KITTI	Material
dataset	O
contains	O
both	O
2D	Task
and	O
3D	Task
annotations	Task
of	Task
cars	Task
,	O
pedestrians	O
,	O
and	O
cyclists	O
in	O
urban	Task
driving	Task
scenarios	Task
.	O

The	O
sensor	Method
configuration	Method
includes	O
a	O
wide	Method
-	Method
angle	Method
camera	Method
and	O
a	O
Velodyne	O
HDL	O
-	O
64E	O
LiDAR	Method
.	O

The	O
official	O
training	O
set	O
contains	O
7481	O
images	O
.	O

We	O
follow	O
and	O
split	O
the	O
dataset	O
into	O
training	O
and	O
validation	O
sets	O
,	O
each	O
containing	O
around	O
half	O
of	O
the	O
entire	O
set	O
.	O

We	O
report	O
model	O
performance	O
on	O
the	O
validation	O
set	O
for	O
all	O
three	O
object	O
categories	O
.	O

paragraph	O
:	O
SUN	Material
-	Material
RGBD	Material
The	O
SUN	Material
-	Material
RGBD	Material
dataset	O
focuses	O
on	O
indoor	O
environments	O
,	O
in	O
which	O
as	O
many	O
as	O
700	O
object	O
categories	O
are	O
labeled	O
.	O

The	O
dataset	O
is	O
collected	O
via	O
different	O
types	O
of	O
RGB	Method
-	Method
D	Method
cameras	Method
with	O
varying	O
resolutions	O
.	O

The	O
training	O
and	O
testing	O
sets	O
contain	O
5285	O
and	O
5050	O
images	O
,	O
respectively	O
.	O

We	O
report	O
model	O
performance	O
on	O
the	O
testing	O
set	O
.	O

We	O
follow	O
the	O
KITTI	Material
training	O
and	O
evaluation	O
setup	O
with	O
one	O
exception	O
.	O

Because	O
SUN	Material
-	Material
RGBD	Material
does	O
not	O
have	O
a	O
direct	O
mapping	O
between	O
the	O
2D	Task
and	O
3D	O
object	O
annotations	O
,	O
for	O
each	O
3D	O
object	O
annotation	O
,	O
we	O
project	O
the	O
8	O
corners	O
of	O
the	O
3D	O
box	O
to	O
the	O
image	O
plane	O
and	O
use	O
the	O
minimum	O
enclosing	O
2D	Task
bounding	O
box	O
as	O
training	O
data	O
for	O
the	O
2D	Task
object	O
detector	O
and	O
our	O
models	O
.	O

We	O
report	O
3D	Task
detection	Task
performance	O
of	O
our	O
models	O
on	O
the	O
same	O
10	O
object	O
categories	O
as	O
in	O
.	O

Because	O
these	O
10	O
object	O
categories	O
contain	O
relatively	O
large	O
objects	O
,	O
we	O
also	O
show	O
detection	Task
results	O
on	O
the	O
19	O
categories	O
from	O
to	O
show	O
our	O
model	O
’s	O
performance	O
on	O
objects	O
of	O
all	O
sizes	O
.	O

We	O
use	O
the	O
same	O
set	O
of	O
hyper	O
-	O
parameters	O
in	O
both	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
.	O

subsection	O
:	O
Metrics	Metric
We	O
use	O
the	O
3D	Metric
object	Metric
detection	Metric
average	Metric
precision	Metric
metric	Metric
(	O
)	O
in	O
our	O
evaluation	O
.	O

A	O
predicted	O
3D	O
box	O
is	O
a	O
true	O
positive	O
if	O
its	O
3D	Metric
intersection	Metric
-	Metric
over	Metric
-	Metric
union	Metric
ratio	Metric
(	O
3D	Metric
IoU	Metric
)	O
with	O
a	O
ground	O
truth	O
box	O
is	O
over	O
a	O
threshold	O
.	O

We	O
compute	O
a	O
per	Metric
-	Metric
class	Metric
precision	Metric
-	Metric
recall	Metric
curve	Metric
and	O
use	O
the	O
area	O
under	O
the	O
curve	O
as	O
the	O
AP	Metric
measure	Metric
.	O

We	O
use	O
the	O
official	Metric
evaluation	Metric
protocol	Metric
for	O
the	O
KITTI	Material
dataset	O
,	O
i.e.	O
,	O
the	O
3D	O
IoU	O
thresholds	O
are	O
0.7	O
,	O
0.5	O
,	O
0.5	O
for	O
Car	O
,	O
Cyclist	O
,	O
Pedestrian	O
respectively	O
.	O

Following	O
,	O
we	O
use	O
a	O
3D	O
IoU	O
threshold	O
0.25	O
for	O
all	O
classes	O
in	O
SUN	Material
-	Material
RGBD	Material
.	O

subsection	O
:	O
Implementation	O
Details	O
Architecture	O
We	O
use	O
a	O
ResNet	Method
-	Method
50	Method
pretrained	Method
on	O
ImageNet	Method
for	O
processing	O
the	O
input	O
image	Task
crop	Task
.	O

The	O
output	O
feature	O
vector	O
is	O
produced	O
by	O
the	O
final	O
residual	O
block	O
(	O
block	O
-	O
4	O
)	O
and	O
averaged	O
across	O
the	O
feature	O
map	O
locations	O
.	O

We	O
use	O
the	O
original	O
implementation	O
of	O
PointNet	Method
with	O
all	O
batch	Method
normalization	Method
layers	Method
removed	O
.	O

For	O
the	O
2D	Task
object	O
detector	O
,	O
we	O
use	O
an	O
off	O
-	O
the	O
-	O
shelf	O
Faster	Method
-	Method
RCNN	Method
implementation	Method
pretrained	O
on	O
MS	Method
-	Method
COCO	Method
and	O
fine	O
-	O
tuned	O
on	O
the	O
datasets	O
used	O
in	O
the	O
experiments	O
.	O

We	O
use	O
the	O
same	O
set	O
of	O
hyper	O
-	O
parameters	O
and	O
architectures	O
in	O
all	O
of	O
our	O
experiments	O
.	O

Training	O
and	O
evaluation	Task
During	O
training	Task
,	O
we	O
randomly	O
resize	O
and	O
shift	O
the	O
ground	O
truth	O
2D	Task
bounding	O
boxes	O
by	O
10	O
%	O
along	O
their	O
and	O
dimensions	O
.	O

These	O
boxes	O
are	O
used	O
as	O
the	O
input	O
crops	O
for	O
our	O
models	O
.	O

At	O
evaluation	O
time	O
,	O
we	O
use	O
the	O
output	O
of	O
the	O
trained	O
2D	Task
detector	O
.	O

For	O
each	O
input	O
2D	Task
box	O
,	O
we	O
crop	O
and	O
resize	O
the	O
image	O
to	O
and	O
randomly	O
sample	O
a	O
maximum	O
of	O
400	O
input	O
3D	O
points	O
in	O
both	O
training	O
and	O
evaluation	Task
.	O

At	O
evaluation	O
time	O
,	O
we	O
apply	O
PointFusion	Method
to	O
the	O
top	O
300	O
2D	Task
detector	O
boxes	O
for	O
each	O
image	O
.	O

The	O
3D	Metric
detection	Metric
score	Metric
is	O
computed	O
by	O
multiplying	O
the	O
2D	Task
detection	O
score	O
and	O
the	O
predicted	O
3D	O
bounding	O
box	O
scores	O
.	O

subsection	O
:	O
Architectures	O
We	O
compare	O
6	O
model	O
variants	O
to	O
showcase	O
the	O
effectiveness	O
of	O
our	O
design	O
choices	O
.	O

final	O
uses	O
our	O
dense	Method
prediction	Method
architecture	Method
and	O
the	O
unsupervised	Method
scoring	Method
function	Method
as	O
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

dense	Method
implements	O
the	O
dense	Method
prediction	Method
architecture	Method
with	O
a	O
supervised	O
scoring	O
function	O
as	O
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

dense	Method
-	Method
no	Method
-	Method
i	Method
m	Method
is	O
the	O
same	O
as	O
dense	Method
but	O
takes	O
only	O
the	O
point	O
cloud	O
as	O
input	O
.	O

global	Method
is	O
a	O
baseline	Method
model	Method
that	O
directly	O
regresses	O
the	O
8	O
corner	O
locations	O
of	O
a	O
3D	O
box	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
D.	O
global	Method
-	Method
no	Method
-	Method
i	Method
m	Method
is	O
the	O
same	O
as	O
the	O
global	O
but	O
takes	O
only	O
the	O
point	O
cloud	O
as	O
input	O
.	O

rgb	Method
-	O
d	O
replaces	O
the	O
PointNet	Method
component	Method
with	O
a	O
generic	Method
CNN	Method
,	O
which	O
takes	O
a	O
depth	O
image	O
as	O
input	O
.	O

We	O
use	O
it	O
as	O
an	O
example	O
of	O
a	O
homogeneous	Method
architecture	Method
baseline	Method
.	O

subsection	O
:	O
Evaluation	O
on	O
KITTI	Material
paragraph	O
:	O
Overview	O
Table	O
[	O
reference	O
]	O
shows	O
a	O
comprehensive	O
comparison	O
of	O
models	O
that	O
are	O
trained	O
and	O
evaluated	O
only	O
with	O
the	O
car	O
category	O
on	O
the	O
KITTI	Material
validation	O
set	O
,	O
including	O
all	O
baselines	O
and	O
the	O
state	O
of	O
the	O
art	O
methods	O
3DOP	O
(	O
stereo	Method
)	O
,	O
VeloFCN	Method
(	O
LiDAR	Method
)	O
,	O
and	O
MV3D	Method
(	O
LiDAR	Method
+	O
rgb	Method
)	O
.	O

Among	O
our	O
variants	O
,	O
final	O
achieves	O
the	O
best	O
performance	O
,	O
while	O
the	O
homogeneous	O
CNN	O
architecture	O
rgb	Method
-	O
d	O
has	O
the	O
worst	O
performance	O
,	O
which	O
underscores	O
the	O
effectiveness	O
of	O
our	O
heterogeneous	Method
model	Method
design	Method
.	O

Compare	O
with	O
MV3D	Method
[	O
]	O
The	O
final	O
model	O
also	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
method	O
MV3D	Method
on	O
the	O
easy	O
category	O
(	O
3	O
%	O
more	O
in	O
)	O
,	O
and	O
has	O
a	O
similar	O
performance	O
on	O
the	O
moderate	O
category	O
(	O
1.5	O
%	O
less	O
in	O
)	O
.	O

When	O
we	O
train	O
a	O
single	O
model	O
using	O
all	O
3	O
KITTI	Material
categories	O
final	O
(	O
all	O
-	O
class	O
)	O
,	O
we	O
roughly	O
get	O
a	O
3	O
%	O
further	O
increase	O
,	O
achieving	O
a	O
6	O
%	O
gain	O
over	O
MV3D	Method
on	O
the	O
easy	O
examples	O
and	O
a	O
0.5	O
%	O
gain	O
on	O
the	O
moderate	O
ones	O
.	O

This	O
shows	O
that	O
our	O
model	O
learns	O
a	O
generic	Method
3D	Method
representation	Method
that	O
can	O
be	O
shared	O
across	O
categories	O
.	O

Still	O
,	O
MV3D	Method
outperforms	O
our	O
models	O
on	O
the	O
hard	O
examples	O
,	O
which	O
are	O
objects	O
that	O
are	O
significantly	O
occluded	O
,	O
by	O
a	O
considerable	O
margin	O
(	O
6	O
%	O
and	O
3	O
%	O
for	O
the	O
two	O
models	O
mentioned	O
)	O
.	O

We	O
believe	O
that	O
the	O
gap	O
with	O
MV3D	O
for	O
occluded	O
objects	O
is	O
due	O
to	O
two	O
factors	O
:	O
1	O
)	O
MV3D	Method
uses	O
a	O
bird	Method
’s	Method
eye	Method
view	Method
detector	Method
for	O
cars	Task
,	O
which	O
is	O
less	O
susceptible	O
to	O
occlusion	O
than	O
our	O
front	O
-	O
view	O
setup	O
.	O

It	O
also	O
uses	O
custom	O
-	O
designed	O
features	O
for	O
car	Task
detection	Task
that	O
should	O
generalize	O
better	O
with	O
few	O
training	O
examples	O
2	O
)	O
MV3D	Method
is	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
system	Method
,	O
which	O
allows	O
one	O
component	O
to	O
potentially	O
correct	O
errors	O
in	O
another	O
.	O

Turning	O
our	O
approach	O
into	O
a	O
fully	O
end	Task
-	Task
to	Task
-	Task
end	Task
system	Task
may	O
help	O
close	O
this	O
gap	O
further	O
.	O

Unlike	O
MV3D	Method
,	O
our	O
general	O
and	O
simple	O
method	O
achieves	O
excellent	O
results	O
on	O
pedestrian	Task
and	Task
cyclist	Task
,	O
which	O
are	O
state	O
of	O
the	O
art	O
by	O
a	O
large	O
margin	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Global	O
vs.	O
dense	O
The	O
dense	Method
architecture	Method
has	O
a	O
clear	O
advantage	O
over	O
the	O
global	Method
architecture	Method
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
:	O
dense	Method
and	Method
dense	Method
-	Method
no	Method
-	Method
i	Method
m	Method
outperforms	O
global	Method
and	Method
global	Method
-	Method
no	Method
-	Method
i	Method
m	Method
,	O
respectively	O
,	O
by	O
large	O
margins	O
.	O

This	O
shows	O
the	O
effectiveness	O
of	O
using	O
input	O
points	O
as	O
spatial	O
anchors	O
.	O

Supervised	Metric
vs	Metric
unsupervised	Metric
scores	Metric
In	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
we	O
introduce	O
a	O
supervised	Method
and	Method
an	Method
unsupervised	Method
scoring	Method
function	Method
formulation	Method
.	O

Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
show	O
that	O
the	O
unsupervised	Method
scoring	Method
function	Method
performs	O
a	O
bit	O
better	O
for	O
our	O
car	Method
-	Method
only	Method
and	Method
all	Method
-	Method
category	Method
models	Method
.	O

These	O
results	O
support	O
our	O
hypothesis	O
that	O
a	O
point	O
confidently	O
inside	O
the	O
object	O
is	O
not	O
always	O
the	O
point	O
that	O
will	O
give	O
the	O
best	O
prediction	O
.	O

It	O
is	O
better	O
to	O
rely	O
on	O
a	O
self	Method
-	Method
learned	Method
scoring	Method
function	Method
for	O
the	O
specific	O
task	O
than	O
on	O
a	O
hand	O
-	O
picked	O
proxy	O
objective	O
.	O

Effect	O
of	O
fusion	Task
Both	O
car	Task
-	Task
only	Task
and	Task
all	Task
-	Task
category	Task
evaluation	Task
results	O
show	O
that	O
fusing	O
lidar	Method
and	O
image	O
information	O
always	O
yields	O
significant	O
gains	O
over	O
lidar	Method
-	O
only	O
architectures	O
,	O
but	O
the	O
gains	O
vary	O
across	O
classes	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
largest	O
gains	O
are	O
for	O
pedestrian	O
(	O
3	O
%	O
to	O
47	O
%	O
in	O
for	O
easy	O
examples	O
)	O
and	O
for	O
cyclist	O
(	O
5	O
%	O
to	O
32	O
%	O
)	O
.	O

Objects	O
in	O
these	O
categories	O
are	O
smaller	O
and	O
have	O
fewer	O
lidar	Method
points	O
,	O
so	O
they	O
benefit	O
the	O
most	O
from	O
high	O
-	O
resolution	O
camera	O
data	O
.	O

Although	O
sparse	O
lidar	Method
points	O
often	O
suffice	O
in	O
determining	O
the	O
spatial	Task
location	Task
of	Task
an	Task
object	Task
,	O
image	O
appearance	O
features	O
are	O
still	O
helpful	O
in	O
estimating	O
the	O
object	O
dimensions	O
and	O
orientation	O
.	O

This	O
effect	O
is	O
analyzed	O
qualitatively	O
below	O
.	O

Qualitative	O
Results	O
Fig	O
.	O

[	O
reference	O
]	O
showcases	O
some	O
sample	O
predictions	O
from	O
the	O
lidar	Method
-	O
only	O
architecture	O
dense	O
-	O
no	O
-	O
i	O
m	O
and	O
our	O
final	O
model	O
.	O

We	O
observe	O
that	O
the	O
fusion	Method
model	Method
is	O
better	O
at	O
estimating	O
the	O
dimension	Task
and	Task
orientation	Task
of	Task
objects	Task
than	O
the	O
lidar	Method
-	O
only	O
model	O
.	O

In	O
column	O
(	O
a	O
)	O
,	O
one	O
can	O
see	O
that	O
the	O
fusion	Method
model	Method
is	O
able	O
to	O
determine	O
the	O
correct	O
orientation	O
and	O
spatial	O
extents	O
of	O
the	O
cyclists	O
and	O
the	O
pedestrians	O
whereas	O
the	O
lidar	Method
-	O
only	O
model	O
often	O
outputs	O
inaccurate	O
boxes	O
.	O

Similar	O
trends	O
can	O
also	O
be	O
observed	O
in	O
(	O
b	O
)	O
.	O

In	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
,	O
we	O
note	O
that	O
although	O
the	O
lidar	Method
-	O
only	O
model	O
correctly	O
determines	O
the	O
dimensions	O
of	O
the	O
cars	O
,	O
it	O
fails	O
to	O
predict	O
the	O
correct	O
orientations	O
of	O
the	O
cars	O
that	O
are	O
occluded	O
or	O
distant	O
.	O

The	O
third	O
row	O
of	O
Fig	O
.	O

[	O
reference	O
]	O
shows	O
more	O
complex	O
scenarios	O
.	O

(	O
a	O
)	O
shows	O
that	O
our	O
model	O
correctly	O
detects	O
a	O
person	O
on	O
a	O
ladder	O
.	O

(	O
b	O
)	O
shows	O
a	O
complex	O
highway	O
driving	O
scene	O
.	O

(	O
c	O
)	O
and	O
(	O
d	O
)	O
show	O
that	O
our	O
model	O
may	O
occasionally	O
fail	O
in	O
extremely	O
cluttered	O
scenes	O
.	O

Number	O
of	O
input	O
points	O
Finally	O
,	O
we	O
conduct	O
a	O
study	O
on	O
the	O
effect	O
of	O
limiting	O
the	O
number	O
of	O
input	O
points	O
at	O
test	O
time	O
.	O

Given	O
a	O
final	O
model	O
trained	O
with	O
at	O
most	O
400	O
points	O
per	O
crop	O
,	O
we	O
vary	O
the	O
maximum	O
number	O
of	O
input	O
points	O
per	O
RoI	O
and	O
evaluate	O
how	O
the	O
3D	Task
detection	Task
performance	O
changes	O
.	O

As	O
shown	O
in	O
[	O
reference	O
]	O
,	O
the	O
performance	O
stays	O
constant	O
at	O
300	O
-	O
500	O
points	O
and	O
degrades	O
rapidly	O
below	O
200	O
points	O
.	O

This	O
shows	O
that	O
our	O
model	O
needs	O
a	O
certain	O
amount	O
of	O
points	O
to	O
perform	O
well	O
but	O
is	O
also	O
robust	O
against	O
variations	O
.	O

subsection	O
:	O
Evaluation	O
on	O
SUN	Material
-	Material
RGBD	Material
Comparison	O
with	O
our	O
baselines	O
As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
final	O
is	O
our	O
best	O
model	O
variant	O
and	O
outperforms	O
the	O
rgb	Method
-	O
d	O
baseline	O
by	O
6	O
%	O
mAP	Metric
.	O

This	O
is	O
a	O
much	O
smaller	O
gap	O
than	O
in	O
the	O
KITTI	Material
dataset	O
,	O
which	O
shows	O
that	O
the	O
CNN	Method
performs	O
well	O
when	O
it	O
is	O
given	O
dense	O
depth	O
information	O
(	O
rgb	Method
-	O
d	O
cameras	O
provide	O
a	O
depth	O
measurement	O
for	O
every	O
rgb	Method
image	O
pixel	O
)	O
.	O

Furthermore	O
,	O
rgb	Method
-	O
d	O
performs	O
roughly	O
on	O
-	O
par	O
with	O
our	O
lidar	Method
-	O
only	O
model	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
PointNet	Method
subcomponent	Method
and	O
the	O
dense	Method
architecture	Method
.	O

Comparison	O
with	O
other	O
methods	O
We	O
compare	O
our	O
model	O
with	O
three	O
approaches	O
from	O
the	O
current	O
state	O
of	O
the	O
art	O
.	O

Deep	Method
Sliding	Method
Shapes	Method
(	O
DSS	Method
)	O
generates	O
3D	O
regions	O
using	O
a	O
proposal	Method
network	Method
and	O
then	O
processes	O
them	O
using	O
a	O
3D	Method
convolutional	Method
network	Method
,	O
which	O
is	O
prohibitively	O
slow	O
.	O

Our	O
model	O
outperforms	O
DSS	Method
by	O
3	O
%	O
mAP	Metric
while	O
being	O
15	O
times	O
faster	O
.	O

Clouds	Method
of	Method
Oriented	Method
Gradients	Method
(	O
COG	Method
)	O
by	O
Ren	Method
exploits	O
the	O
scene	O
layout	O
information	O
and	O
performs	O
exhaustive	Method
3D	Method
bounding	Method
box	Method
search	Method
,	O
which	O
makes	O
it	O
run	O
in	O
the	O
tens	O
of	O
minutes	O
.	O

In	O
contrast	O
,	O
PointFusion	Method
only	O
uses	O
the	O
3D	O
points	O
that	O
project	O
to	O
a	O
2D	Task
detection	O
box	O
and	O
still	O
outperforms	O
COG	Method
on	O
6	O
out	O
of	O
10	O
categories	O
,	O
while	O
approaching	O
its	O
overall	O
mAP	Metric
performance	O
.	O

PointFusion	Method
also	O
compares	O
favorably	O
to	O
the	O
method	O
of	O
Lahoud	Method
,	O
which	O
uses	O
a	O
multi	Method
-	Method
stage	Method
pipeline	Method
to	O
perform	O
detection	Task
,	O
orientation	Task
regression	Task
and	O
object	Task
refinement	Task
using	O
object	O
relation	O
information	O
.	O

Our	O
method	O
is	O
simpler	O
and	O
does	O
not	O
make	O
environment	O
-	O
specific	O
assumptions	O
,	O
yet	O
it	O
obtains	O
a	O
marginally	O
better	O
mAP	Metric
while	O
being	O
3	O
times	O
faster	O
.	O

Note	O
that	O
for	O
simplicity	O
,	O
our	O
evaluation	O
protocol	O
passes	O
all	O
300	O
2D	Task
detector	O
proposals	O
for	O
each	O
image	O
to	O
PointFusion	Method
.	O

Since	O
our	O
2D	Task
detector	O
takes	O
only	O
0.2s	O
per	O
frame	O
,	O
we	O
can	O
easily	O
get	O
sub	O
-	O
second	Metric
evaluation	Metric
times	Metric
simply	O
by	O
discarding	O
detection	O
boxes	O
with	O
scores	O
below	O
a	O
threshold	O
,	O
with	O
minimal	O
performance	O
losses	O
.	O

Qualitative	O
results	O
Fig	O
.	O

[	O
reference	O
]	O
shows	O
some	O
sample	O
detection	Task
results	O
from	O
the	O
final	O
model	O
on	O
19	O
object	Task
categories	Task
.	O

Our	O
model	O
is	O
able	O
to	O
detect	O
objects	O
of	O
various	O
scales	O
,	O
orientations	O
,	O
and	O
positions	O
.	O

Note	O
that	O
because	O
our	O
model	O
does	O
not	O
use	O
a	O
top	Method
-	Method
down	Method
view	Method
representation	Method
,	O
it	O
is	O
able	O
to	O
detect	O
objects	O
that	O
are	O
on	O
top	O
of	O
other	O
objects	O
,	O
e.g.	O
,	O
pillows	O
on	O
top	O
of	O
a	O
bed	O
.	O

Failure	O
modes	O
include	O
errors	O
caused	O
by	O
objects	O
which	O
are	O
only	O
partially	O
visible	O
in	O
the	O
image	O
or	O
from	O
cascading	O
errors	O
from	O
the	O
2D	Task
detector	O
.	O

section	O
:	O
Conclusions	O
and	O
Future	O
Work	O
We	O
present	O
the	O
PointFusion	Method
network	O
,	O
which	O
accurately	O
estimates	O
3D	Task
object	Task
bounding	Task
boxes	Task
from	O
image	O
and	O
point	O
cloud	O
information	O
.	O

Our	O
model	O
makes	O
two	O
main	O
contributions	O
.	O

First	O
,	O
we	O
process	O
the	O
inputs	O
using	O
heterogeneous	Method
network	Method
architectures	Method
.	O

The	O
raw	O
point	O
cloud	O
data	O
is	O
directly	O
handled	O
using	O
a	O
PointNet	Method
model	Method
,	O
which	O
avoids	O
lossy	O
input	O
preprocessing	O
such	O
as	O
quantization	Method
or	O
projection	O
.	O

Second	O
,	O
we	O
introduce	O
a	O
novel	O
dense	Method
fusion	Method
network	Method
,	O
which	O
combines	O
the	O
image	Method
and	Method
point	Method
cloud	Method
representations	Method
.	O

It	O
predicts	O
multiple	O
3D	O
box	O
hypotheses	O
relative	O
to	O
the	O
input	O
3D	O
points	O
,	O
which	O
serve	O
as	O
spatial	O
anchors	O
,	O
and	O
automatically	O
learns	O
to	O
select	O
the	O
best	O
hypothesis	O
.	O

We	O
show	O
that	O
with	O
the	O
same	O
architecture	O
and	O
hyper	O
-	O
parameters	O
,	O
our	O
method	O
is	O
able	O
to	O
perform	O
on	O
par	O
or	O
better	O
than	O
methods	O
that	O
hold	O
dataset	O
and	O
sensor	O
-	O
specific	O
assumptions	O
on	O
two	O
drastically	O
different	O
datasets	O
.	O

Promising	O
directions	O
of	O
future	O
work	O
include	O
combining	O
the	O
2D	Task
detector	O
and	O
the	O
PointFusion	Method
network	O
into	O
a	O
single	O
end	Method
-	Method
to	Method
-	Method
end	Method
3D	Method
detector	Method
,	O
as	O
well	O
as	O
extending	O
our	O
model	O
with	O
a	O
temporal	Method
component	Method
to	O
perform	O
joint	Task
detection	Task
and	Task
tracking	Task
in	Task
video	Task
and	Task
point	Task
cloud	Task
streams	Task
.	O

bibliography	O
:	O
References	O
section	O
:	O
Supplementary	O
subsection	O
:	O
3D	Task
Localization	Task
AP	Task
in	O
KITTI	Material
In	O
addition	O
to	O
the	O
metrics	O
,	O
we	O
also	O
report	O
results	O
on	O
a	O
3D	Metric
localization	Metric
metrics	Metric
just	O
for	O
reference	O
.	O

A	O
predicted	O
3D	O
box	O
is	O
a	O
true	O
positive	O
if	O
its	O
2D	Task
top	O
-	O
down	O
view	O
box	O
has	O
an	O
IoU	O
with	O
a	O
ground	O
truth	O
box	O
is	O
greater	O
than	O
a	O
threshold	O
.	O

We	O
compute	O
a	O
per	Metric
-	Metric
class	Metric
precision	Metric
-	Metric
recall	Metric
curve	Metric
and	O
use	O
the	O
area	O
under	O
the	O
curve	O
as	O
the	O
AP	Metric
measure	Metric
.	O

We	O
use	O
the	O
official	Metric
evaluation	Metric
protocol	Metric
for	O
the	O
KITTI	Material
dataset	O
,	O
i.e.	O
,	O
the	O
3D	O
IoU	O
thresholds	O
are	O
0.7	O
,	O
0.5	O
,	O
0.5	O
for	O
Car	O
,	O
Cyclist	O
,	O
Pedestrian	O
respectively	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
on	O
models	O
that	O
are	O
trained	O
on	O
Car	Material
only	Material
,	O
with	O
the	O
exception	O
of	O
final	O
(	O
all	O
-	O
class	O
)	O
,	O
which	O
is	O
trained	O
on	O
all	O
categories	O
,	O
and	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
models	O
that	O
are	O
trained	O
on	O
all	O
categories	O
.	O

subsection	O
:	O
The	O
rgbd	Method
baseline	Method
In	O
the	O
experiment	O
section	O
,	O
we	O
show	O
that	O
the	O
rgbd	Method
baseline	Method
model	Method
performs	O
the	O
worst	O
on	O
the	O
KITTI	Material
dataset	O
.	O

We	O
observe	O
that	O
most	O
of	O
the	O
predicted	O
boxes	O
have	O
less	O
than	O
0.5	O
IoU	O
with	O
ground	O
truth	O
boxes	O
due	O
to	O
the	O
errors	O
in	O
the	O
predicted	O
depth	O
.	O

The	O
performance	O
gap	O
is	O
reduced	O
in	O
the	O
SUN	Material
-	Material
RGBD	Material
dataset	O
due	O
to	O
the	O
availability	O
of	O
denser	O
depth	O
map	O
.	O

However	O
,	O
it	O
is	O
non	O
-	O
trivial	O
to	O
achieve	O
such	O
performance	O
using	O
a	O
CNN	Method
-	Method
based	Method
architecture	Method
.	O

Here	O
we	O
describe	O
the	O
rgbd	Method
baseline	Method
in	O
detail	O
.	O

subsubsection	O
:	O
Input	O
representation	O
The	O
rgbd	Method
baseline	Method
is	O
a	O
CNN	Method
architecture	Method
that	O
takes	O
as	O
input	O
a	O
5	O
-	O
channel	O
tensor	O
.	O

The	O
first	O
three	O
channels	O
is	O
the	O
input	O
RGB	Material
image	Material
.	O

The	O
fourth	O
channel	O
is	O
the	O
depth	O
channel	O
.	O

For	O
KITTI	Material
,	O
we	O
obtain	O
the	O
depth	O
channel	O
by	O
projecting	O
the	O
lidar	Method
point	O
cloud	O
on	O
to	O
the	O
image	O
plane	O
,	O
and	O
assigning	O
zeros	O
to	O
the	O
pixels	O
that	O
have	O
no	O
depth	O
values	O
.	O

For	O
SUN	Material
-	Material
RGBD	Material
,	O
we	O
use	O
the	O
depth	O
image	O
.	O

We	O
normalize	O
the	O
depth	O
measurement	O
by	O
the	O
maximum	O
depth	O
range	O
value	O
.	O

The	O
fifth	O
channel	O
is	O
a	O
depth	O
measurement	O
binary	O
mask	O
:	O
1	O
indicates	O
that	O
the	O
corresponding	O
pixel	O
in	O
the	O
depth	O
channel	O
has	O
a	O
depth	O
value	O
.	O

This	O
is	O
to	O
add	O
extra	O
information	O
to	O
help	O
the	O
model	O
to	O
distinguish	O
between	O
no	O
measurements	O
and	O
small	O
measurements	O
.	O

Empirically	O
we	O
find	O
this	O
extra	O
channel	O
useful	O
.	O

subsubsection	O
:	O
Learning	Metric
Objective	Metric
We	O
found	O
that	O
training	O
the	O
model	O
to	O
predict	O
the	O
3D	Task
corner	Task
locations	Task
is	O
ineffective	O
due	O
to	O
the	O
highly	O
non	O
-	O
linear	O
mapping	O
and	O
lack	O
of	O
image	O
grounding	O
.	O

Hence	O
we	O
regress	O
the	O
box	O
corner	O
pixel	O
locations	O
and	O
the	O
depth	O
of	O
the	O
corners	O
and	O
then	O
use	O
the	O
camera	O
geometry	O
to	O
recover	O
the	O
full	O
3D	O
box	O
.	O

A	O
similar	O
approach	O
has	O
been	O
applied	O
in	O
.	O

The	O
pixel	O
regression	O
target	O
is	O
normalized	O
between	O
0	O
and	O
1	O
by	O
the	O
dimensions	O
of	O
the	O
input	O
2D	Task
box	O
.	O

For	O
the	O
depth	Task
objective	Task
,	O
we	O
found	O
that	O
directly	O
regressing	O
the	O
depth	O
value	O
is	O
difficult	O
especially	O
for	O
the	O
KITTI	Material
dataset	O
,	O
in	O
which	O
the	O
target	O
objects	O
have	O
large	O
location	O
variance	O
.	O

Instead	O
,	O
we	O
employed	O
a	O
multi	Method
-	Method
hypothesis	Method
method	Method
:	O
we	O
discretize	O
the	O
depth	O
objective	O
into	O
overlapping	O
bins	O
and	O
train	O
the	O
network	O
to	O
predict	O
which	O
bin	O
contains	O
the	O
center	O
of	O
the	O
target	O
box	O
.	O

The	O
network	O
is	O
also	O
trained	O
to	O
predict	O
the	O
residual	O
depth	O
of	O
each	O
corner	O
to	O
the	O
center	O
of	O
the	O
predicted	O
depth	O
bin	O
.	O

At	O
test	O
time	O
,	O
the	O
corner	O
depth	O
values	O
can	O
be	O
recovered	O
by	O
adding	O
up	O
the	O
center	O
depth	O
of	O
the	O
predicted	O
bin	O
and	O
the	O
predicted	O
residual	O
depth	O
of	O
each	O
corner	O
.	O

Intuitively	O
,	O
this	O
method	O
lets	O
the	O
network	O
to	O
have	O
a	O
coarse	O
-	O
to	O
-	O
fine	O
estimate	O
of	O
the	O
depth	O
,	O
alleviating	O
the	O
large	O
variance	O
in	O
the	O
depth	Metric
objective	Metric
.	O

