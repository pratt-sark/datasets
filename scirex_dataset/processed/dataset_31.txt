document	O
:	O
Loss	Method
-	Method
Sensitive	Method
Generative	Method
Adversarial	Method
Networks	Method
on	O
Lipschitz	Method
Densities	Method
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
Lipschitz	Method
regularization	Method
theory	Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	Method
-	Method
Sensitive	Method
Generative	Method
Adversarial	Method
Network	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
.	O

Specifically	O
,	O
it	O
trains	O
a	O
loss	O
function	O
to	O
distinguish	O
between	O
real	O
and	O
fake	O
samples	O
by	O
designated	O
margins	O
,	O
while	O
learning	O
a	O
generator	Method
alternately	O
to	O
produce	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

The	O
LS	Method
-	Method
GAN	Method
further	O
regularizes	O
its	O
loss	Method
function	Method
with	O
a	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
density	O
of	O
real	O
data	O
,	O
yielding	O
a	O
regularized	Method
model	Method
that	O
can	O
better	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
a	O
reasonable	O
number	O
of	O
training	O
examples	O
than	O
the	O
classic	O
GAN	Method
.	O

We	O
will	O
further	O
present	O
a	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	O
-	O
GAN	Method
)	O
and	O
show	O
it	O
contains	O
a	O
large	O
family	O
of	O
regularized	O
GAN	Method
models	O
,	O
including	O
both	O
LS	Method
-	Method
GAN	Method
and	O
Wasserstein	O
GAN	Method
,	O
as	O
its	O
special	O
cases	O
.	O

Compared	O
with	O
the	O
other	O
GAN	Method
models	O
,	O
we	O
will	O
conduct	O
experiments	O
to	O
show	O
both	O
LS	Method
-	Method
GAN	Method
and	O
GLS	Method
-	Method
GAN	Method
exhibit	O
competitive	O
ability	O
in	O
generating	O
new	O
images	O
in	O
terms	O
of	O
the	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
assessed	O
on	O
a	O
separate	O
test	O
set	O
.	O

We	O
further	O
extend	O
the	O
LS	Method
-	Method
GAN	Method
to	O
a	O
conditional	Method
form	Method
for	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
problems	Task
,	O
and	O
demonstrate	O
its	O
outstanding	O
performance	O
on	O
image	O
classification	Task
tasks	Task
.	O

Keywords	O
:	O
Generative	Method
Adversarial	Method
Nets	Method
(	O
GANs	Method
)	O
,	O
Lipschitz	O
regularity	O
,	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
section	O
:	O
Introduction	O
A	O
classic	O
Generative	Method
Adversarial	Method
Net	Method
(	O
GAN	Method
)	O
learns	O
a	O
discriminator	Method
and	O
a	O
generator	Method
by	O
playing	O
a	O
two	Method
-	Method
player	Method
minimax	Method
game	Method
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
discriminator	O
is	O
trained	O
to	O
distinguish	O
real	O
samples	O
from	O
those	O
generated	O
by	O
the	O
generator	O
,	O
and	O
it	O
in	O
turn	O
guides	O
the	O
generator	O
to	O
produce	O
realistic	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

However	O
,	O
from	O
both	O
theoretical	O
and	O
practical	O
perspectives	O
,	O
a	O
critical	O
question	O
is	O
whether	O
the	O
GAN	Method
can	O
generate	O
realistic	O
samples	O
from	O
arbitrary	O
data	O
distribution	O
without	O
any	O
prior	O
?	O
If	O
not	O
,	O
what	O
kind	O
of	O
prior	O
ought	O
to	O
be	O
imposed	O
on	O
the	O
data	O
distribution	O
to	O
regularize	O
the	O
GAN	Method
?	O
Indeed	O
,	O
the	O
classic	O
GAN	Method
imposes	O
no	O
prior	O
on	O
the	O
data	O
distribution	O
.	O

This	O
represents	O
an	O
ambitious	O
goal	O
to	O
generate	O
samples	O
from	O
any	O
distributions	O
.	O

However	O
,	O
it	O
in	O
turn	O
requires	O
a	O
non	Method
-	Method
parametric	Method
discriminator	Method
to	O
prove	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
samples	O
by	O
assuming	O
the	O
model	O
has	O
infinite	O
capacity	O
(	O
see	O
Section	O
4	O
of	O
)	O
.	O

This	O
is	O
a	O
too	O
strong	O
assumption	O
to	O
establish	O
the	O
theoretical	O
basis	O
for	O
the	O
GAN	Method
.	O

Moreover	O
,	O
with	O
such	O
an	O
assumption	O
,	O
its	O
generalizability	O
becomes	O
susceptible	O
.	O

Specifically	O
,	O
one	O
could	O
argue	O
the	O
learned	Method
generator	Method
may	O
be	O
overfit	O
by	O
an	O
unregularized	Method
discriminator	Method
in	O
an	O
non	O
-	O
parametric	O
fashion	O
by	O
merely	O
memorizing	O
or	O
interpolating	O
training	O
examples	O
.	O

In	O
other	O
words	O
,	O
it	O
could	O
lack	O
the	O
generalization	O
ability	O
to	O
generate	O
new	O
samples	O
out	O
of	O
existing	O
data	O
.	O

Indeed	O
,	O
Arora	O
et	O
al	O
.	O

have	O
shown	O
that	O
the	O
GAN	Method
minimizing	O
the	O
Jensen	Metric
-	Metric
Shannon	Metric
distance	Metric
between	O
the	O
distributions	O
of	O
generated	O
and	O
real	O
data	O
could	O
fail	O
to	O
generalize	O
to	O
produce	O
new	O
samples	O
with	O
a	O
reasonable	O
size	O
of	O
training	O
set	O
.	O

Thus	O
,	O
a	O
properly	O
regularized	O
GAN	Method
is	O
demanded	O
to	O
establish	O
provable	O
generalizability	O
by	O
focusing	O
on	O
a	O
restricted	O
yet	O
still	O
sufficiently	O
large	O
family	O
of	O
data	O
distributions	O
.	O

subsection	O
:	O
Objective	O
:	O
Towards	O
Regularized	O
GANs	Method
In	O
this	O
paper	O
,	O
we	O
attempt	O
to	O
develop	O
regularization	Method
theory	Method
and	O
algorithms	O
for	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
.	O

Specifically	O
,	O
we	O
introduce	O
a	O
loss	Method
function	Method
to	O
quantify	O
the	O
quality	Metric
of	Metric
generated	Metric
samples	Metric
.	O

A	O
constraint	O
is	O
imposed	O
so	O
that	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
counterpart	O
.	O

Specifically	O
,	O
in	O
the	O
learning	Method
algorithm	Method
,	O
we	O
will	O
define	O
margins	O
to	O
separate	O
the	O
losses	O
between	O
generated	O
and	O
real	O
samples	O
.	O

Then	O
,	O
an	O
optimal	Method
generator	Method
will	O
be	O
trained	O
to	O
produce	O
realistic	O
samples	O
with	O
minimum	O
losses	O
.	O

The	O
loss	Method
function	Method
and	O
the	O
generator	Method
will	O
be	O
trained	O
in	O
an	O
adversarial	Method
fashion	Method
until	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
real	O
ones	O
.	O

We	O
will	O
also	O
develop	O
new	O
theory	O
to	O
analyze	O
the	O
LS	Method
-	Method
GAN	Method
on	O
the	O
basis	O
of	O
Lipschitz	O
regularity	O
.	O

We	O
note	O
that	O
the	O
reason	O
of	O
making	O
non	Method
-	Method
parametric	Method
assumption	Method
of	Method
infinite	Method
capacity	Method
on	O
the	O
discriminator	Method
in	O
the	O
classic	O
GAN	Method
is	O
due	O
to	O
its	O
ambitious	O
goal	O
to	O
generate	O
data	O
from	O
any	O
arbitrary	O
distribution	O
.	O

However	O
,	O
no	O
free	O
lunch	O
principle	O
reminds	O
us	O
of	O
the	O
need	O
to	O
impose	O
a	O
suitable	O
prior	O
on	O
the	O
data	O
distribution	O
from	O
which	O
real	O
samples	O
are	O
generated	O
.	O

This	O
inspires	O
us	O
to	O
impose	O
a	O
Lipschitz	O
regularity	O
condition	O
by	O
assuming	O
the	O
data	O
density	O
does	O
not	O
change	O
abruptly	O
.	O

Based	O
on	O
this	O
mild	O
condition	O
,	O
we	O
will	O
show	O
that	O
the	O
density	O
of	O
generated	O
samples	O
by	O
LS	Method
-	Method
GAN	Method
can	O
exactly	O
match	O
that	O
of	O
real	O
data	O
.	O

More	O
importantly	O
,	O
the	O
Lipschitz	O
regularity	O
allows	O
us	O
to	O
prove	O
the	O
LS	Method
-	Method
GAN	Method
can	O
well	O
generalize	O
to	O
produce	O
new	O
data	O
from	O
training	O
examples	O
.	O

To	O
this	O
end	O
,	O
we	O
will	O
provide	O
a	O
Probably	Metric
Approximate	Metric
Correct	Metric
(	O
PAC	Metric
)-	O
style	O
theorem	O
by	O
showing	O
the	O
empirical	Method
LS	Method
-	Method
GAN	Method
model	Method
trained	O
with	O
a	O
reasonable	O
number	O
of	O
examples	O
can	O
be	O
sufficiently	O
close	O
to	O
the	O
oracle	Method
LS	Method
-	Method
GAN	Method
trained	O
with	O
hypothetically	O
known	O
data	O
distribution	O
,	O
thereby	O
proving	O
the	O
generalizability	O
of	O
LS	Method
-	Method
GAN	Method
in	O
generating	O
samples	O
from	O
any	O
Lipschitz	O
data	O
distribution	O
.	O

We	O
will	O
also	O
make	O
a	O
non	Method
-	Method
parametric	Method
analysis	Method
of	Method
the	Method
LS	Method
-	Method
GAN	Method
.	O

It	O
does	O
not	O
rely	O
on	O
any	O
parametric	O
form	O
of	O
the	O
loss	O
function	O
to	O
characterize	O
its	O
optimality	O
in	O
the	O
space	O
of	O
Lipschtiz	O
functions	O
.	O

It	O
gives	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	Metric
loss	Metric
,	O
which	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	Task
gradient	Task
.	O

This	O
suggests	O
that	O
the	O
LS	Method
-	Method
GAN	Method
can	O
provide	O
sufficient	O
gradient	O
to	O
update	O
its	O
LS	Method
-	Method
GAN	Method
generator	O
even	O
if	O
the	O
loss	O
function	O
has	O
been	O
fully	O
optimized	O
,	O
thus	O
avoiding	O
the	O
vanishing	Task
gradient	Task
problem	O
that	O
could	O
occur	O
in	O
training	O
the	O
GAN	Method
.	O

subsection	O
:	O
Extensions	O
:	O
Generalized	Method
and	O
Conditional	Method
LS	Method
-	Method
GANs	Method
We	O
further	O
present	O
a	O
generalized	O
form	O
of	O
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
and	O
conduct	O
experiment	O
to	O
demonstrate	O
it	O
has	O
the	O
best	O
generalization	Metric
ability	Metric
.	O

We	O
will	O
show	O
this	O
is	O
not	O
a	O
surprising	O
result	O
as	O
the	O
GLS	Method
-	Method
GAN	Method
contains	O
a	O
large	O
family	O
of	O
regularized	O
GANs	Method
with	O
both	O
LS	Method
-	Method
GAN	Method
and	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
as	O
its	O
special	O
cases	O
.	O

Moreover	O
,	O
we	O
will	O
extend	O
a	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
that	O
can	O
generate	O
samples	O
from	O
given	O
conditions	O
.	O

In	O
particular	O
,	O
with	O
class	O
labels	O
being	O
conditions	O
,	O
the	O
learned	O
loss	O
function	O
can	O
be	O
used	O
as	O
a	O
classifier	Method
for	O
both	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
.	O

The	O
advantage	O
of	O
such	O
a	O
classifier	Method
arises	O
from	O
its	O
ability	O
of	O
exploring	O
generated	O
examples	O
to	O
uncover	O
intrinsic	O
variations	O
for	O
different	O
classes	O
.	O

Experiment	O
results	O
demonstrate	O
competitive	O
performance	O
of	O
the	O
CLS	Method
-	Method
GAN	Method
classifier	Method
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

subsection	O
:	O
Paper	O
Structure	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

Section	O
[	O
reference	O
]	O
reviews	O
the	O
related	O
work	O
,	O
and	O
the	O
proposed	O
LS	Method
-	Method
GAN	Method
is	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
analyze	O
the	O
LS	Method
-	Method
GAN	Method
by	O
proving	O
the	O
distributional	O
consistency	O
between	O
generated	O
and	O
real	O
data	O
with	O
the	O
Lipschitz	O
regularity	O
condition	O
on	O
the	O
data	O
distribution	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
will	O
discuss	O
the	O
generalizability	Task
problem	Task
arising	O
from	O
using	O
sample	Method
means	Method
to	O
approximate	O
the	O
expectations	O
in	O
the	O
training	O
objectives	O
.	O

We	O
will	O
make	O
a	O
comparison	O
with	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
present	O
a	O
generalized	O
LS	Method
-	Method
GAN	Method
with	O
both	O
WGAN	Method
and	O
LS	Method
-	Method
GAN	Method
as	O
its	O
special	O
cases	O
in	O
Section	O
[	O
reference	O
]	O
.	O

A	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
algorithm	O
is	O
followed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Then	O
we	O
will	O
show	O
how	O
the	O
model	O
can	O
be	O
extended	O
to	O
a	O
conditional	Method
model	Method
for	O
both	O
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
learning	Task
in	O
Section	O
[	O
reference	O
]	O
.	O

Experiment	O
results	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
we	O
conclude	O
in	O
Section	O
[	O
reference	O
]	O
.	O

Source	O
codes	O
.	O

The	O
source	O
codes	O
for	O
both	O
LS	Method
-	Method
GAN	Method
and	O
GLS	Method
-	Method
GAN	Method
are	O
available	O
at	O
,	O
in	O
the	O
frameworks	O
of	O
torch	O
,	O
pytorch	O
and	O
tensorflow	Method
.	O

LS	Method
-	Method
GAN	Method
is	O
also	O
supported	O
by	O
Microsoft	O
CNTK	O
at	O
.	O

section	O
:	O
Related	O
Work	O
Deep	Method
generative	Method
models	Method
,	O
especially	O
the	O
Generative	O
Adversarial	O
Net	O
(	O
GAN	Method
)	O
,	O
have	O
attracted	O
many	O
attentions	O
recently	O
due	O
to	O
their	O
demonstrated	O
abilities	O
of	O
generating	O
real	O
samples	O
following	O
the	O
underlying	O
data	O
densities	O
.	O

In	O
particular	O
,	O
the	O
GAN	Method
attempts	O
to	O
learn	O
a	O
pair	O
of	O
discriminator	Method
and	Method
generator	Method
by	O
playing	O
a	O
maximin	Method
game	Method
to	O
seek	O
an	O
equilibrium	O
,	O
in	O
which	O
the	O
discriminator	O
is	O
trained	O
by	O
distinguishing	O
real	O
samples	O
from	O
generated	O
ones	O
and	O
the	O
generator	Method
is	O
optimized	O
to	O
produce	O
samples	O
that	O
can	O
fool	O
the	O
discriminator	O
.	O

A	O
family	O
of	O
GAN	Method
architectures	O
have	O
been	O
proposed	O
to	O
implement	O
this	O
idea	O
.	O

For	O
example	O
,	O
recent	O
progresses	O
have	O
shown	O
impressive	O
performances	O
on	O
synthesizing	O
photo	O
-	O
realistic	O
images	O
by	O
constructing	O
multiple	O
strided	Method
and	Method
factional	Method
-	Method
strided	Method
convolutional	Method
layers	Method
for	O
discriminators	Method
and	O
generators	Method
.	O

On	O
the	O
contrary	O
,	O
proposed	O
to	O
use	O
a	O
Laplacian	Method
pyramid	Method
to	O
produce	O
high	O
-	O
quality	O
images	O
by	O
iteratively	O
adding	O
multiple	O
layers	O
of	O
noises	O
at	O
different	O
resolutions	O
.	O

presented	O
to	O
train	O
a	O
recurrent	Method
generative	Method
model	Method
by	O
using	O
adversarial	Method
training	Method
to	O
unroll	O
gradient	Method
-	Method
based	Method
optimizations	Method
to	O
create	O
high	O
quality	O
images	O
.	O

In	O
addition	O
to	O
designing	O
different	O
GAN	Method
networks	O
,	O
research	O
efforts	O
have	O
been	O
made	O
to	O
train	O
the	O
GAN	Method
by	O
different	O
criteria	O
.	O

For	O
example	O
,	O
presented	O
an	O
energy	O
-	O
based	O
GAN	Method
by	O
minimizing	O
an	O
energy	Method
function	Method
to	O
learn	O
an	O
optimal	Method
discriminator	Method
,	O
and	O
an	O
auto	Method
-	Method
encoder	Method
structured	Method
discriminator	Method
is	O
presented	O
to	O
compute	O
the	O
energy	O
.	O

The	O
authors	O
also	O
present	O
a	O
theoretical	O
analysis	O
by	O
showing	O
this	O
variant	O
of	O
GAN	Method
can	O
generate	O
samples	O
whose	O
density	O
can	O
recover	O
the	O
underlying	O
true	O
data	O
density	O
.	O

However	O
,	O
it	O
still	O
needs	O
to	O
assume	O
the	O
discriminator	Method
has	O
infinite	O
modeling	O
capacity	O
to	O
prove	O
the	O
result	O
in	O
a	O
non	O
-	O
parametric	O
fashion	O
,	O
and	O
its	O
generalizability	O
of	O
producing	O
new	O
data	O
out	O
of	O
training	O
examples	O
is	O
unknown	O
without	O
theoretical	O
proof	O
or	O
empirical	O
evidence	O
.	O

In	O
addition	O
,	O
presented	O
to	O
analyze	O
the	O
GAN	Method
from	O
information	Task
theoretical	Task
perspective	Task
,	O
and	O
they	O
seek	O
to	O
minimize	O
the	O
variational	Method
estimate	Method
of	Method
f	Method
-	Method
divergence	Method
,	O
and	O
show	O
that	O
the	O
classic	O
GAN	Method
is	O
included	O
as	O
a	O
special	O
case	O
of	O
f	Method
-	Method
GAN	Method
.	O

In	O
contrast	O
,	O
InfoGAN	Method
proposed	O
another	O
information	O
-	O
theoretic	O
GAN	Method
to	O
learn	O
disentangled	Method
representations	Method
capturing	O
various	O
latent	O
concepts	O
and	O
factors	O
in	O
generating	O
samples	O
.	O

Most	O
recently	O
,	O
propose	O
to	O
minimize	O
the	O
Earth	O
-	O
Mover	O
distance	O
between	O
the	O
density	O
of	O
generated	O
samples	O
and	O
the	O
true	O
data	O
density	O
,	O
and	O
they	O
show	O
the	O
resultant	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	Method
can	O
address	O
the	O
vanishing	Task
gradient	Task
problem	O
that	O
the	O
classic	O
GAN	Method
suffers	O
.	O

Besides	O
the	O
class	O
of	O
GANs	Method
,	O
there	O
exist	O
other	O
models	O
that	O
also	O
attempt	O
to	O
generate	O
natural	O
images	O
.	O

For	O
example	O
,	O
rendered	O
images	O
by	O
matching	O
features	O
in	O
a	O
convolutional	Method
network	Method
with	O
respect	O
to	O
reference	O
images	O
.	O

used	O
deconvolutional	Method
network	Method
to	O
render	O
3D	Method
chair	Method
models	Method
in	O
various	O
styles	O
and	O
viewpoints	O
.	O

introduced	O
a	O
deep	Method
recurrent	Method
neutral	Method
network	Method
architecture	Method
for	O
image	Task
generation	Task
with	O
a	O
sequence	O
of	O
variational	Method
auto	Method
-	Method
encoders	Method
to	O
iteratively	O
construct	O
complex	O
images	O
.	O

Recent	O
efforts	O
have	O
also	O
been	O
made	O
on	O
leveraging	O
the	O
learned	O
representations	O
by	O
deep	Method
generative	Method
networks	Method
to	O
improve	O
the	O
classification	Metric
accuracy	Metric
when	O
it	O
is	O
too	O
difficult	O
or	O
expensive	O
to	O
label	O
sufficient	O
training	O
examples	O
.	O

For	O
example	O
,	O
presented	O
variational	Method
auto	Method
-	Method
encoders	Method
by	O
combining	O
deep	Method
generative	Method
models	Method
and	O
approximate	Method
variational	Method
inference	Method
to	O
explore	O
both	O
labeled	O
and	O
unlabeled	O
data	O
.	O

treated	O
the	O
samples	O
from	O
the	O
GAN	Method
generator	O
as	O
a	O
new	O
class	O
,	O
and	O
explore	O
unlabeled	O
examples	O
by	O
assigning	O
them	O
to	O
a	O
class	O
different	O
from	O
the	O
new	O
one	O
.	O

proposed	O
to	O
train	O
a	O
ladder	Method
network	Method
by	O
minimizing	O
the	O
sum	Metric
of	Metric
supervised	Metric
and	O
unsupervised	Metric
cost	Metric
functions	Metric
through	O
back	Method
-	Method
propagation	Method
,	O
which	O
avoids	O
the	O
conventional	O
layer	Method
-	Method
wise	Method
pre	Method
-	Method
training	Method
approach	Method
.	O

presented	O
an	O
approach	O
to	O
learning	O
a	O
discriminative	Method
classifier	Method
by	O
trading	O
-	O
off	O
mutual	O
information	O
between	O
observed	O
examples	O
and	O
their	O
predicted	O
classes	O
against	O
an	O
adversarial	Method
generative	Method
model	Method
.	O

sought	O
to	O
jointly	O
distinguish	O
between	O
not	O
only	O
real	O
and	O
generated	O
samples	O
but	O
also	O
their	O
latent	O
variables	O
in	O
an	O
adversarial	Method
process	Method
.	O

Recently	O
,	O
presented	O
a	O
novel	O
paradigm	O
of	O
localized	O
GANs	Method
to	O
explore	O
the	O
local	O
consistency	O
of	O
classifiers	O
in	O
local	O
coordinate	O
charts	O
,	O
as	O
well	O
as	O
showed	O
an	O
intrinsic	O
connection	O
with	O
Laplace	O
-	O
Beltrami	O
operator	O
along	O
the	O
manifold	O
.	O

These	O
methods	O
have	O
shown	O
promising	O
results	O
for	O
classification	Task
tasks	Task
by	O
leveraging	O
deep	Method
generative	Method
models	Method
.	O

section	O
:	O
Loss	O
-	O
Sensitive	O
GAN	Method
The	O
classic	O
GAN	Method
consists	O
of	O
two	O
players	O
–	O
a	O
generator	Method
producing	O
samples	O
from	O
random	O
noises	O
,	O
and	O
a	O
discriminator	Method
distinguishing	O
real	O
and	O
fake	O
samples	O
.	O

The	O
generator	Method
and	O
discriminator	Method
are	O
trained	O
in	O
an	O
adversarial	Method
fashion	Method
to	O
reach	O
an	O
equilibrium	O
in	O
which	O
generated	O
samples	O
become	O
indistinguishable	O
from	O
their	O
real	O
counterparts	O
.	O

On	O
the	O
contrary	O
,	O
in	O
the	O
LS	Method
-	Method
GAN	Method
we	O
seek	O
to	O
learn	O
a	O
loss	O
function	O
parameterized	O
with	O
by	O
assuming	O
that	O
a	O
real	O
example	O
ought	O
to	O
have	O
a	O
smaller	O
loss	O
than	O
a	O
generated	O
sample	O
by	O
a	O
desired	O
margin	O
.	O

Then	O
the	O
generator	O
can	O
be	O
trained	O
to	O
generate	O
realistic	O
samples	O
by	O
minimizing	O
their	O
losses	O
.	O

Formally	O
,	O
consider	O
a	O
generator	Method
function	Method
that	O
produces	O
a	O
sample	O
by	O
transforming	O
a	O
noise	O
input	O
drawn	O
from	O
a	O
simple	O
distribution	Method
such	O
as	O
uniform	Method
and	Method
Gaussian	Method
distributions	Method
.	O

Then	O
for	O
a	O
real	O
example	O
and	O
a	O
generated	O
sample	O
,	O
the	O
loss	Method
function	Method
can	O
be	O
trained	O
to	O
distinguish	O
them	O
with	O
the	O
following	O
constraint	O
:	O
where	O
is	O
the	O
margin	O
measuring	O
the	O
difference	O
between	O
and	O
.	O

This	O
constraint	O
requires	O
a	O
real	O
sample	O
be	O
separated	O
from	O
a	O
generated	O
counterpart	O
in	O
terms	O
of	O
their	O
losses	O
by	O
at	O
least	O
a	O
margin	O
of	O
.	O

The	O
above	O
hard	O
constraint	O
can	O
be	O
relaxed	O
by	O
introducing	O
a	O
nonnegative	O
slack	O
variable	O
that	O
quantifies	O
the	O
violation	O
of	O
the	O
above	O
constraint	O
.	O

This	O
results	O
in	O
the	O
following	O
minimization	Task
problem	Task
to	O
learn	O
the	O
loss	O
function	O
given	O
a	O
fixed	O
generator	O
,	O
where	O
is	O
a	O
positive	O
balancing	O
parameter	O
,	O
and	O
is	O
the	O
data	O
distribution	O
of	O
real	O
samples	O
.	O

The	O
first	O
term	O
minimizes	O
the	O
expected	O
loss	O
function	O
over	O
data	O
distribution	O
since	O
a	O
smaller	O
loss	O
is	O
preferred	O
on	O
real	O
samples	O
.	O

The	O
second	O
term	O
is	O
the	O
expected	Metric
error	Metric
caused	O
by	O
the	O
violation	O
of	O
the	O
constraint	O
.	O

Without	O
loss	O
of	O
generality	O
,	O
we	O
require	O
the	O
loss	O
function	O
should	O
be	O
nonnegative	O
.	O

Given	O
a	O
fixed	O
loss	O
function	O
,	O
on	O
the	O
other	O
hand	O
,	O
one	O
can	O
solve	O
the	O
following	O
minimization	Task
problem	Task
to	O
find	O
an	O
optimal	O
generator	O
.	O

We	O
can	O
use	O
and	O
to	O
denote	O
the	O
density	O
of	O
samples	O
generated	O
by	O
and	O
respectively	O
,	O
with	O
being	O
drawn	O
from	O
.	O

However	O
,	O
for	O
the	O
simplicity	O
of	O
notations	O
,	O
we	O
will	O
use	O
and	O
to	O
denote	O
and	O
without	O
explicitly	O
mentioning	O
and	O
that	O
should	O
be	O
clear	O
in	O
the	O
context	O
.	O

Finally	O
,	O
let	O
us	O
summarize	O
the	O
above	O
objectives	O
.	O

The	O
LS	Method
-	Method
GAN	Method
optimizes	O
and	O
alternately	O
by	O
seeking	O
an	O
equilibrium	O
such	O
that	O
minimizes	O
which	O
is	O
an	O
equivalent	O
form	O
of	O
(	O
[	O
reference	O
]	O
)	O
with	O
,	O
and	O
minimizes	O
In	O
the	O
next	O
section	O
,	O
we	O
will	O
show	O
the	O
consistency	O
between	O
and	O
for	O
LS	Method
-	Method
GAN	Method
.	O

section	O
:	O
Theoretical	Task
Analysis	Task
:	O
Distributional	Task
Consistency	Task
Suppose	O
is	O
a	O
Nash	Method
equilibrium	Method
that	O
jointly	O
solves	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

We	O
will	O
show	O
that	O
as	O
,	O
the	O
density	O
distribution	O
of	O
the	O
samples	O
generated	O
by	O
will	O
converge	O
to	O
the	O
real	O
data	O
density	O
.	O

First	O
,	O
we	O
have	O
the	O
following	O
definition	O
.	O

Definition	O
.	O

For	O
any	O
two	O
samples	O
x	O
and	O
z	O
,	O
the	O
loss	O
function	O
⁢F	O
(	O
x	O
)	O
is	O
Lipschitz	O
continuous	O
with	O
respect	O
to	O
a	O
distance	O
metric	O
Δ	O
if	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
κ	O
,	O
i.e	O
,	O
<	O
κ	O
+	O
∞.	O
To	O
prove	O
our	O
main	O
result	O
,	O
we	O
assume	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

The	O
data	Method
density	Method
P⁢data	Method
is	O
supported	O
in	O
a	O
compact	O
set	O
D	O
,	O
and	O
it	O
is	O
Lipschitz	O
continuous	O
wrt	O
Δ	O
with	O
a	O
bounded	O
constant	O
<	O
κ	O
+	O
∞.	O
The	O
set	O
of	O
Lipschitz	O
densities	O
with	O
a	O
compact	O
support	O
contain	O
a	O
large	O
family	O
of	O
distributions	O
that	O
are	O
dense	O
in	O
the	O
space	O
of	O
continuous	O
densities	O
.	O

For	O
example	O
,	O
the	O
density	O
of	O
natural	O
images	O
are	O
defined	O
over	O
a	O
compact	O
set	O
of	O
pixel	O
values	O
,	O
and	O
it	O
can	O
be	O
consider	O
as	O
Lipschitz	O
continuous	O
,	O
since	O
the	O
densities	O
of	O
two	O
similar	O
images	O
are	O
unlikely	O
to	O
change	O
abruptly	O
at	O
an	O
unbounded	O
rate	O
.	O

If	O
real	O
samples	O
are	O
distributed	O
on	O
a	O
manifold	O
(	O
or	O
is	O
supported	O
in	O
a	O
manifold	O
)	O
,	O
we	O
only	O
require	O
the	O
Lipschitz	O
condition	O
hold	O
on	O
this	O
manifold	O
.	O

This	O
makes	O
the	O
Lipschitz	O
regularity	O
applicable	O
to	O
the	O
data	O
densities	O
on	O
a	O
thin	O
manifold	O
embedded	O
in	O
the	O
ambient	O
space	O
.	O

Let	O
us	O
show	O
the	O
existence	O
of	O
Nash	O
equilibrium	O
such	O
that	O
both	O
the	O
loss	O
function	O
and	O
the	O
density	O
of	O
generated	O
samples	O
are	O
Lipschitz	O
.	O

Let	O
be	O
the	O
class	O
of	O
functions	O
over	O
with	O
a	O
bounded	O
yet	O
sufficiently	O
large	O
Lipschitz	O
constant	O
such	O
that	O
belongs	O
to	O
.	O

It	O
is	O
not	O
difficult	O
to	O
show	O
that	O
the	O
space	O
is	O
convex	O
and	O
compact	O
if	O
its	O
member	O
functions	O
are	O
supported	O
in	O
a	O
compact	O
set	O
.	O

In	O
addition	O
,	O
we	O
note	O
both	O
and	O
are	O
convex	O
in	O
and	O
in	O
.	O

Then	O
,	O
according	O
to	O
the	O
Sion	O
’s	O
theorem	O
,	O
with	O
and	O
being	O
optimized	O
over	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
there	O
exists	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
such	O
that	O
both	O
Lθ	Method
*	Method
and	O
PG	Method
*	Method
are	O
Lipschitz	O
.	O

Now	O
we	O
can	O
prove	O
the	O
main	O
lemma	O
of	O
this	O
paper	O
.	O

The	O
Lipschitz	O
regularity	O
relaxes	O
the	O
strong	O
non	O
-	O
parametric	O
assumption	O
on	O
the	O
GAN	Method
’s	O
discriminator	O
with	O
infinite	O
capacity	O
to	O
the	O
above	O
weaker	O
Lipschitz	O
assumption	O
for	O
the	O
LS	Method
-	Method
GAN	Method
.	O

This	O
allows	O
us	O
to	O
show	O
the	O
following	O
lemma	O
that	O
establishes	O
the	O
distributional	O
consistency	O
between	O
the	O
optimal	O
by	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
–	O
(	O
[	O
reference	O
]	O
)	O
and	O
the	O
data	O
density	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
for	O
a	O
Nash	O
equilibrium	O
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
in	O
Lemma	O
,	O
we	O
have	O
Thus	O
,	O
⁢PG*	O
(	O
x	O
)	O
converges	O
to	O
⁢P⁢data	O
(	O
x	O
)	O
as	O
→λ	O
+	O
∞.	O
The	O
proof	O
of	O
this	O
lemma	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

By	O
letting	O
λ	O
go	O
infinitely	O
large	O
,	O
the	O
density	O
⁢PG*	O
(	O
x	O
)	O
of	O
generated	O
samples	O
should	O
exactly	O
match	O
the	O
real	O
data	O
density	O
⁢P⁢data	O
(	O
x	O
)	O
.	O

Equivalently	O
,	O
we	O
can	O
simply	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
(	O
)	O
as	O
it	O
plays	O
no	O
role	O
as	O
→λ	O
+	O
∞.	O
Putting	O
the	O
above	O
two	O
lemmas	O
together	O
,	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
Lθ	O
*	O
and	O
PG	O
*	O
are	O
Lipschitz.	O
(	O
ii	O
)	O
∫x⁢|	O
-	O
⁢P⁢data	O
(	O
x	O
)	O
⁢PG*	O
(	O
x	O
)	O
|dx≤2λ→0	O
,	O
as	O
→λ	O
+	O
∞.	O
section	O
:	O
Learning	Task
and	O
Generalizability	O
The	O
minimization	Task
problems	Task
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
can	O
not	O
be	O
solved	O
directly	O
since	O
the	O
expectations	O
over	O
the	O
distributions	O
of	O
true	O
data	O
and	O
noises	O
are	O
unavailable	O
or	O
intractable	O
.	O

Instead	O
,	O
one	O
can	O
approximate	O
them	O
with	O
empirical	Method
means	Method
on	O
a	O
set	O
of	O
finite	O
real	O
examples	O
and	O
noise	O
vectors	O
drawn	O
from	O
and	O
respectively	O
.	O

This	O
results	O
in	O
the	O
following	O
two	O
alternative	O
problems	O
.	O

and	O
where	O
the	O
random	O
vectors	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
different	O
from	O
used	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

The	O
sample	O
mean	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
computed	O
over	O
pairs	O
randomly	O
drawn	O
from	O
real	O
and	O
generated	O
samples	O
,	O
which	O
is	O
an	O
approximation	O
to	O
the	O
second	O
expectation	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Generalizability	O
We	O
have	O
proved	O
the	O
density	O
of	O
generated	O
samples	O
by	O
the	O
LS	Method
-	Method
GAN	Method
is	O
consistent	O
with	O
the	O
real	O
data	O
density	O
in	O
Theorem	O
[	O
reference	O
]	O
.	O

This	O
consistency	O
is	O
established	O
based	O
on	O
the	O
two	O
oracle	Metric
objectives	Metric
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

However	O
,	O
in	O
practice	O
,	O
the	O
population	O
expectations	O
in	O
these	O
two	O
objectives	O
can	O
not	O
be	O
computed	O
directly	O
over	O
and	O
.	O

Instead	O
,	O
they	O
are	O
approximated	O
in	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
by	O
sample	Method
means	Method
on	O
a	O
finite	O
set	O
of	O
real	O
and	O
generated	O
examples	O
.	O

This	O
raises	O
the	O
question	O
about	O
the	O
generalizability	O
of	O
the	O
LS	Method
-	Method
GAN	Method
model	O
.	O

We	O
wonder	O
,	O
with	O
more	O
training	O
examples	O
,	O
if	O
the	O
empirical	Method
model	Method
trained	O
with	O
finitely	O
many	O
examples	O
can	O
generalize	O
to	O
the	O
oracle	Method
model	Method
.	O

In	O
particular	O
,	O
we	O
wish	O
to	O
estimate	O
the	O
sample	Metric
complexity	Metric
of	O
how	O
many	O
examples	O
are	O
required	O
to	O
sufficiently	O
bound	O
the	O
generalization	O
difference	O
between	O
the	O
empirical	O
and	O
oracle	O
objectives	O
.	O

Arora	O
et	O
al	O
.	O

has	O
proposed	O
a	O
neural	Method
network	Method
distance	Method
to	O
analyze	O
the	O
generalization	Metric
ability	Metric
for	O
the	O
GAN	Method
.	O

However	O
,	O
this	O
neural	Method
network	Method
distance	Method
can	O
not	O
be	O
directly	O
applied	O
here	O
,	O
as	O
it	O
is	O
not	O
related	O
with	O
the	O
objectives	O
that	O
are	O
used	O
to	O
train	O
the	O
LS	Method
-	Method
GAN	Method
.	O

So	O
the	O
generalization	Metric
ability	Metric
in	O
terms	O
of	O
the	O
neural	O
network	O
distance	O
does	O
not	O
imply	O
the	O
LS	Method
-	Method
GAN	Method
could	O
also	O
generalize	O
.	O

Thus	O
,	O
a	O
direct	O
generalization	Method
analysis	Method
of	O
the	O
LS	Method
-	Method
GAN	Method
is	O
required	O
based	O
on	O
its	O
own	O
objectives	O
.	O

First	O
,	O
let	O
us	O
consider	O
the	O
generalization	Task
in	O
terms	O
of	O
.	O

This	O
objective	O
is	O
used	O
to	O
train	O
the	O
loss	Method
function	Method
to	O
distinguish	O
between	O
real	O
and	O
generated	O
samples	O
.	O

Consider	O
the	O
oracle	Metric
objective	Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
population	O
expectations	O
and	O
the	O
empirical	Metric
objective	Metric
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
sample	O
means	O
We	O
need	O
to	O
show	O
if	O
and	O
how	O
fast	O
the	O
difference	O
would	O
eventually	O
vanish	O
as	O
the	O
number	O
of	O
training	O
examples	O
grows	O
.	O

To	O
this	O
end	O
,	O
we	O
need	O
to	O
define	O
the	O
following	O
notations	O
about	O
the	O
model	Metric
complexity	Metric
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
for	O
LS	Method
-	Method
GAN	Method
,	O
the	O
loss	Method
function	Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
the	O
distance	O
between	O
two	O
samples	O
is	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
generalization	O
theorem	O
in	O
a	O
Probably	O
Approximately	O
Correct	O
(	O
PAC	Metric
)	O
style	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
N	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
loss	O
function	O
such	O
that	O
∈θRN	O
.	O

The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

This	O
theorem	O
shows	O
the	O
sample	Metric
complexity	Metric
to	O
bound	O
the	O
difference	O
between	O
and	O
is	O
polynomial	O
in	O
the	O
model	Metric
size	Metric
,	O
as	O
well	O
as	O
both	O
Lipschitz	O
constants	O
and	O
.	O

Similarly	O
,	O
we	O
can	O
establish	O
the	O
generalizability	O
to	O
train	O
the	O
generator	Method
function	Method
by	O
considering	O
the	O
empirical	Metric
objective	Metric
and	O
the	O
oracle	O
objective	O
over	O
empirical	O
and	O
real	O
distributions	O
,	O
respectively	O
.	O

We	O
use	O
the	O
following	O
notions	O
to	O
characterize	O
the	O
complexity	Metric
of	O
the	O
generator	Method
.	O

theorem	O
:	O
.	O

We	O
assume	O
that	O
The	O
generator	Method
function	Method
is	O
-	O
Lipschitz	O
in	O
its	O
parameter	O
,	O
i.e.	O
,	O
for	O
any	O
;	O
Also	O
,	O
we	O
have	O
is	O
-	O
Lipschitz	O
in	O
,	O
i.e.	O
,	O
;	O
The	O
samples	O
’s	O
drawn	O
from	O
are	O
bounded	O
,	O
i.e.	O
,	O
.	O

Then	O
we	O
can	O
prove	O
the	O
following	O
theorem	O
to	O
establish	O
the	O
generalizability	O
of	O
the	O
generator	O
in	O
terms	O
of	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
with	O
at	O
least	O
probability	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
where	O
C′	O
is	O
a	O
sufficiently	O
large	O
constant	O
,	O
and	O
M	O
is	O
the	O
number	O
of	O
parameters	O
of	O
the	O
generator	Method
function	Method
such	O
that	O
∈ϕRM	O
.	O

subsection	O
:	O
Bounded	O
Lipschitz	O
Constants	O
for	O
Regularization	Task
Our	O
generalization	Method
theory	Method
in	O
Theorem	O
[	O
reference	O
]	O
conjectures	O
that	O
the	O
required	O
number	O
of	O
training	O
examples	O
is	O
lower	O
bounded	O
by	O
a	O
polynomial	O
of	O
Lipschitz	O
constants	O
and	O
of	O
the	O
loss	O
function	O
wrt	O
and	O
.	O

This	O
suggests	O
us	O
to	O
bound	O
both	O
constants	O
to	O
reduce	O
the	O
sample	Metric
complexity	Metric
of	O
the	O
LS	Method
-	Method
GAN	Method
to	O
improve	O
its	O
generalization	Task
performance	O
.	O

Specifically	O
,	O
bounding	O
the	O
Lipschitz	O
constants	O
and	O
can	O
be	O
implemented	O
by	O
adding	O
two	O
gradient	O
penalties	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
to	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
as	O
the	O
surrogate	O
of	O
the	O
Lipschitz	O
constants	O
.	O

For	O
simplicity	O
,	O
we	O
ignore	O
the	O
second	O
gradient	O
penalty	O
(	O
II	O
)	O
for	O
in	O
experiments	O
,	O
as	O
the	O
sample	Metric
complexity	Metric
is	O
only	O
log	O
-	O
linear	O
in	O
it	O
,	O
whose	O
impact	O
on	O
generalization	Metric
performance	Metric
is	O
negligible	O
compared	O
with	O
that	O
of	O
.	O

Otherwise	O
,	O
penalizing	O
(	O
II	O
)	O
needs	O
to	O
compute	O
its	O
gradient	O
wrt	O
,	O
which	O
is	O
with	O
a	O
Hessian	O
matrix	O
,	O
and	O
this	O
is	O
usually	O
computationally	O
demanding	O
.	O

Note	O
that	O
the	O
above	O
gradient	Method
penalty	Method
differs	O
from	O
that	O
used	O
in	O
that	O
aims	O
to	O
constrain	O
the	O
Lipschitz	O
constant	O
close	O
to	O
one	O
as	O
in	O
the	O
definition	O
of	O
the	O
Wasserstein	O
distance	O
.	O

However	O
,	O
we	O
are	O
motivated	O
to	O
have	O
lower	O
sample	Metric
complexity	Metric
by	O
directly	O
minimizing	O
the	O
Lipschitz	O
constant	O
rather	O
than	O
constraining	O
it	O
to	O
one	O
.	O

Two	O
gradient	Method
penalty	Method
approaches	Method
are	O
thus	O
derived	O
from	O
different	O
theoretical	O
perspectives	O
,	O
and	O
also	O
make	O
practical	O
differences	O
in	O
experiments	O
.	O

section	O
:	O
Wasserstein	O
GAN	Method
and	O
Generalized	Method
LS	Method
-	Method
GAN	Method
In	O
this	O
section	O
,	O
we	O
discuss	O
two	O
issues	O
about	O
LS	Method
-	Method
GAN	Method
.	O

First	O
,	O
we	O
discuss	O
its	O
connection	O
with	O
the	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
,	O
and	O
then	O
show	O
that	O
the	O
WGAN	Method
is	O
a	O
special	O
case	O
of	O
a	O
generalized	O
form	O
of	O
LS	Method
-	Method
GAN	Method
.	O

subsection	O
:	O
Comparison	O
with	O
Wasserstein	O
GAN	Method
We	O
notice	O
that	O
the	O
recently	O
proposed	O
Wasserstein	O
GAN	Method
(	O
WGAN	Method
)	O
uses	O
the	O
Earth	Method
-	Method
Mover	Method
(	Method
EM	Method
)	Method
distance	Method
to	O
address	O
the	O
vanishing	Task
gradient	Task
and	O
saturated	Task
JS	Task
distance	Task
problems	Task
in	O
the	O
classic	O
GAN	Method
by	O
showing	O
the	O
EM	Method
distance	Method
is	O
continuous	O
and	O
differentiable	O
almost	O
everywhere	O
.	O

While	O
both	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
address	O
these	O
problems	O
from	O
different	O
perspectives	O
that	O
are	O
independently	O
developed	O
almost	O
simultaneously	O
,	O
both	O
turn	O
out	O
to	O
use	O
the	O
Lipschitz	O
regularity	O
in	O
training	O
their	O
GAN	Method
models	O
.	O

This	O
constraint	O
plays	O
vital	O
but	O
different	O
roles	O
in	O
the	O
two	O
models	O
.	O

In	O
the	O
LS	Method
-	Method
GAN	Method
,	O
the	O
Lipschitz	O
regularity	O
naturally	O
arises	O
from	O
the	O
Lipschitz	O
assumption	O
on	O
the	O
data	O
density	O
and	O
the	O
generalization	O
bound	O
.	O

Under	O
this	O
regularity	O
condition	O
,	O
we	O
have	O
proved	O
in	O
Theorem	O
[	O
reference	O
]	O
that	O
the	O
density	O
of	O
generated	O
samples	O
matches	O
the	O
underlying	O
data	O
density	O
.	O

On	O
the	O
contrary	O
,	O
the	O
WGAN	Method
introduces	O
the	O
Lipschitz	O
constraint	O
from	O
the	O
Kantorovich	Method
-	Method
Rubinstein	Method
duality	Method
of	O
the	O
EM	Method
distance	Method
but	O
it	O
is	O
not	O
proved	O
in	O
if	O
the	O
density	O
of	O
samples	O
generated	O
by	O
WGAN	Method
is	O
consistent	O
with	O
that	O
of	O
real	O
data	O
.	O

Here	O
we	O
assert	O
that	O
the	O
WGAN	Method
also	O
models	O
an	O
underlying	O
Lipschitz	Method
density	Method
.	O

To	O
prove	O
this	O
,	O
we	O
restate	O
the	O
WGAN	Method
as	O
follows	O
.	O

The	O
WGAN	Method
seeks	O
to	O
find	O
a	O
critic	Method
and	O
a	O
generator	Method
such	O
that	O
and	O
Let	O
be	O
the	O
density	O
of	O
samples	O
generated	O
by	O
.	O

Then	O
,	O
we	O
prove	O
the	O
following	O
lemma	O
about	O
the	O
WGAN	Method
in	O
Appendix	O
[	O
reference	O
]	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
an	O
optimal	O
solution	O
(	O
fw*	O
,	O
gϕ	O
*	O
)	O
to	O
the	O
WGAN	Method
such	O
that	O
Pgϕ	O
*	O
is	O
Lipschitz	O
,	O
we	O
have	O
This	O
lemma	O
shows	O
both	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
are	O
based	O
on	O
the	O
same	O
Lipschitz	O
regularity	O
condition	O
.	O

Although	O
both	O
methods	O
are	O
derived	O
from	O
very	O
different	O
perspectives	O
,	O
it	O
is	O
interesting	O
to	O
make	O
a	O
comparison	O
between	O
their	O
respective	O
forms	O
.	O

Formally	O
,	O
the	O
WGAN	Method
seeks	O
to	O
maximize	O
the	O
difference	O
between	O
the	O
first	O
-	O
order	O
moments	O
of	O
under	O
the	O
densities	O
of	O
real	O
and	O
generated	O
examples	O
.	O

In	O
this	O
sense	O
,	O
the	O
WGAN	Method
can	O
be	O
considered	O
as	O
a	O
kind	O
of	O
first	Method
-	Method
order	Method
moment	Method
method	Method
.	O

Numerically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
tends	O
to	O
be	O
minimized	O
to	O
be	O
arbitrarily	O
small	O
over	O
generated	O
samples	O
,	O
which	O
could	O
make	O
be	O
unbounded	O
above	O
.	O

This	O
is	O
why	O
the	O
WGAN	Method
must	O
be	O
trained	O
by	O
clipping	O
the	O
network	O
weights	O
of	O
on	O
a	O
bounded	O
box	O
to	O
prevent	O
from	O
becoming	O
unbounded	O
above	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	Method
-	Method
GAN	Method
treats	O
real	O
and	O
generated	O
examples	O
in	O
pairs	O
,	O
and	O
maximizes	O
the	O
difference	O
of	O
their	O
losses	O
up	O
to	O
a	O
data	O
-	O
dependant	O
margin	O
.	O

Specifically	O
,	O
as	O
shown	O
in	O
the	O
second	O
term	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
when	O
the	O
loss	O
of	O
a	O
generated	O
sample	O
becomes	O
too	O
large	O
wrt	O
that	O
of	O
a	O
paired	O
real	O
example	O
,	O
the	O
maximization	O
of	O
will	O
stop	O
if	O
the	O
difference	O
exceeds	O
.	O

This	O
prevents	O
the	O
minimization	Task
problem	Task
(	O
[	O
reference	O
]	O
)	O
unbounded	O
below	O
,	O
making	O
it	O
better	O
posed	O
to	O
solve	O
.	O

More	O
importantly	O
,	O
paring	O
real	O
and	O
generated	O
samples	O
in	O
prevents	O
their	O
losses	O
from	O
being	O
decomposed	O
into	O
two	O
separate	O
first	O
-	O
order	O
moments	O
like	O
in	O
the	O
WGAN	Method
.	O

The	O
LS	Method
-	Method
GAN	Method
makes	O
pairwise	O
comparison	O
between	O
the	O
losses	O
of	O
real	O
and	O
generated	O
samples	O
,	O
thereby	O
enforcing	O
real	O
and	O
generated	O
samples	O
to	O
coordinate	O
with	O
each	O
other	O
to	O
learn	O
the	O
optimal	O
loss	O
function	O
.	O

Specifically	O
,	O
when	O
a	O
generated	O
sample	O
becomes	O
close	O
to	O
a	O
paired	O
real	O
example	O
,	O
the	O
LS	Method
-	Method
GAN	Method
will	O
stop	O
increasing	O
the	O
difference	O
between	O
their	O
losses	O
.	O

Below	O
we	O
discuss	O
a	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
model	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
show	O
that	O
both	O
WGAN	Method
and	O
LS	Method
-	Method
GAN	Method
are	O
simply	O
two	O
special	O
cases	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
.	O

subsection	O
:	O
GLS	Method
-	Method
GAN	Method
:	O
Generalized	Method
LS	Method
-	Method
GAN	Method
In	O
proving	O
Lemma	O
[	O
reference	O
]	O
,	O
it	O
is	O
noted	O
that	O
we	O
only	O
have	O
used	O
two	O
properties	O
of	O
in	O
the	O
objective	Metric
function	Metric
training	O
the	O
loss	O
function	O
:	O
1	O
)	O
for	O
any	O
;	O
2	O
)	O
for	O
.	O

This	O
inspires	O
us	O
to	O
generalize	O
the	O
LS	Method
-	Method
GAN	Method
with	O
any	O
alternative	O
cost	Method
function	Method
satisfying	O
these	O
two	O
properties	O
,	O
and	O
this	O
will	O
yield	O
the	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	Method
-	Method
GAN	Method
)	O
.	O

We	O
will	O
show	O
that	O
both	O
LS	Method
-	Method
GAN	Method
and	O
WGAN	Method
can	O
be	O
seen	O
as	O
two	O
extreme	O
cases	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
with	O
two	O
properly	O
defined	O
cost	O
functions	O
.	O

Formally	O
,	O
if	O
a	O
cost	O
function	O
satisfies	O
for	O
any	O
and	O
for	O
any	O
,	O
given	O
a	O
fixed	O
generator	O
,	O
we	O
use	O
the	O
following	O
objective	O
to	O
learn	O
,	O
with	O
highlighting	O
its	O
dependency	O
on	O
a	O
chosen	O
cost	O
function	O
.	O

For	O
simplicity	O
,	O
we	O
only	O
involve	O
the	O
second	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
define	O
the	O
generalized	O
objective	O
.	O

But	O
it	O
does	O
not	O
affect	O
the	O
conclusion	O
as	O
the	O
role	O
of	O
the	O
first	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
would	O
vanish	O
with	O
being	O
set	O
to	O
.	O

Following	O
the	O
proof	O
of	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
can	O
prove	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
given	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
jointly	O
minimizing	O
⁢SC	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
and	O
⁢T	O
(	O
θ*	O
,	O
ϕ	O
)	O
with	O
a	O
cost	O
function	O
C	O
satisfying	O
the	O
above	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
,	O
we	O
have	O
In	O
particular	O
,	O
we	O
can	O
choose	O
a	O
leaky	Method
rectified	Method
linear	Method
function	Method
for	O
this	O
cost	Method
function	Method
,	O
i.e.	O
,	O
with	O
a	O
slope	O
.	O

As	O
long	O
as	O
,	O
it	O
is	O
easy	O
to	O
verify	O
satisfies	O
these	O
two	O
conditions	O
.	O

Now	O
the	O
LS	Method
-	Method
GAN	Method
is	O
a	O
special	O
case	O
of	O
this	O
Generalized	Method
LS	Method
-	Method
GAN	Method
(	O
GLS	O
-	O
GAN	Method
)	O
when	O
,	O
as	O
.	O

We	O
denote	O
this	O
equivalence	O
as	O
LS	Method
-	Method
GAN	Method
=	O
GLS	O
-	O
GAN	Method
(	O
C0	O
)	O
What	O
is	O
more	O
interesting	O
is	O
the	O
WGAN	Method
,	O
an	O
independently	O
developed	O
GAN	Method
model	O
with	O
stable	O
training	O
performance	O
,	O
also	O
becomes	O
a	O
special	O
case	O
of	O
this	O
GLS	Method
-	Method
GAN	Method
with	O
.	O

Indeed	O
,	O
when	O
,	O
,	O
and	O
Since	O
the	O
last	O
term	O
is	O
a	O
const	O
,	O
irrespective	O
of	O
,	O
it	O
can	O
be	O
discarded	O
without	O
affecting	O
optimization	O
over	O
.	O

Thus	O
,	O
we	O
have	O
By	O
comparing	O
this	O
with	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
WGAN	Method
is	O
equivalent	O
to	O
the	O
GLS	Method
-	Method
GAN	Method
with	O
,	O
with	O
the	O
critic	Method
function	Method
being	O
equivalent	O
to	O
.	O

Thus	O
we	O
have	O
WGAN	Method
=	O
GLS	O
-	O
GAN	Method
(	O
C1	O
)	O
Therefore	O
,	O
by	O
varying	O
the	O
slope	O
in	O
,	O
we	O
will	O
obtain	O
a	O
family	O
of	O
the	O
GLS	O
-	O
GANs	Method
with	O
varied	O
beyond	O
the	O
LS	Method
-	Method
GAN	Method
and	O
the	O
WGAN	Method
.	O

Of	O
course	O
,	O
it	O
is	O
unnecessary	O
to	O
limit	O
to	O
a	O
leaky	Method
rectified	Method
linear	Method
function	Method
.	O

We	O
can	O
explore	O
more	O
cost	O
functions	O
as	O
long	O
as	O
they	O
satisfy	O
the	O
two	O
conditions	O
(	O
I	O
)	O
and	O
(	O
II	O
)	O
.	O

In	O
experiments	O
,	O
we	O
will	O
demonstrate	O
the	O
GLS	Method
-	Method
GAN	Method
has	O
competitive	O
generalization	Task
performance	O
on	O
generating	O
new	O
images	O
(	O
c.f	O
.	O

Section	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Non	Method
-	Method
Parametric	Method
Analysis	Method
Now	O
we	O
can	O
characterize	O
the	O
optimal	O
loss	O
functions	O
learned	O
from	O
the	O
objective	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
will	O
provide	O
us	O
an	O
insight	O
into	O
the	O
LS	Method
-	Method
GAN	Method
model	O
.	O

We	O
generalize	O
the	O
non	Method
-	Method
parametric	Method
maximum	Method
likelihood	Method
method	Method
in	O
and	O
consider	O
non	Method
-	Method
parametric	Method
solutions	Method
to	O
the	O
optimal	O
loss	O
function	O
by	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
over	O
the	O
whole	O
class	O
of	O
Lipschitz	O
loss	O
functions	O
.	O

Let	O
,	O
i.e.	O
,	O
the	O
first	O
data	O
points	O
are	O
real	O
examples	O
and	O
the	O
rest	O
are	O
generated	O
samples	O
.	O

Then	O
we	O
have	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
.	O

The	O
following	O
functions	O
^Lθ	O
*	O
and	O
~Lθ	O
*	O
both	O
minimize	O
⁢Sm	O
(	O
θ	O
,	O
ϕ	O
*	O
)	O
in	O
Fκ	Method
:	O
with	O
the	O
parameters	O
θ*=	O
[	O
l1*	O
,	O
⋯	O
,	O
l⁢2m*	O
]	O
∈R⁢2	O
m	O
.	O

They	O
are	O
supported	O
in	O
the	O
convex	O
hull	O
of	O
{	O
x	O
(	O
1	O
),	O
⋯	O
,	O
x	O
(	O
⁢2	O
m	O
)	O
}	O
,	O
and	O
we	O
have	O
for	O
=	O
i1	O
,	O
⋯	O
,	O
⁢2	O
m	O
,	O
i.e.	O
,	O
their	O
values	O
coincide	O
on	O
{	O
x	O
(	O
1	O
),	O
x	O
(	O
2	O
),	O
⋯	O
,	O
x	O
(	O
⁢2m	O
)	O
}.	O
The	O
proof	O
of	O
this	O
theorem	O
is	O
given	O
in	O
the	O
appendix	O
.	O

From	O
the	O
theorem	O
,	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
any	O
convex	O
combination	O
of	O
these	O
two	O
forms	O
attains	O
the	O
same	O
value	O
of	O
,	O
and	O
is	O
also	O
a	O
global	O
minimizer	O
.	O

Thus	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

All	O
the	O
functions	O
in	O
minimize	O
Sm	O
in	O
Fκ	O
.	O

This	O
shows	O
that	O
the	O
global	O
minimizer	O
is	O
not	O
unique	O
.	O

Moreover	O
,	O
through	O
the	O
proof	O
of	O
Theorem	O
[	O
reference	O
]	O
,	O
one	O
can	O
find	O
that	O
and	O
are	O
the	O
upper	O
and	O
lower	O
bound	O
of	O
any	O
optimal	Method
loss	Method
function	Method
solution	Method
to	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
.	O

In	O
particular	O
,	O
we	O
have	O
the	O
following	O
corollary	O
.	O

theorem	O
:	O
.	O

For	O
any	O
∈⁢Lθ*	O
(	O
x	O
)	O
Fκ	O
that	O
minimizes	O
Sm	O
,	O
the	O
corresponding	O
⁢^Lθ*	O
(	O
x	O
)	O
and	O
⁢~Lθ*	O
(	O
x	O
)	O
are	O
the	O
lower	O
and	O
upper	O
bounds	O
of	O
⁢Lθ*	O
(	O
x	O
)	O
,	O
i.e.	O
,	O
The	O
proof	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

The	O
parameters	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
sought	O
by	O
minimizing	O
where	O
is	O
short	O
for	O
,	O
and	O
the	O
constraints	O
are	O
imposed	O
to	O
ensure	O
the	O
learned	O
loss	O
functions	O
stay	O
in	O
.	O

With	O
a	O
greater	O
value	O
of	O
,	O
a	O
larger	O
class	O
of	O
loss	Metric
function	Metric
will	O
be	O
sought	O
.	O

Thus	O
,	O
one	O
can	O
control	O
the	O
modeling	O
ability	O
of	O
the	O
loss	O
function	O
by	O
setting	O
a	O
proper	O
value	O
to	O
.	O

Problem	O
(	O
[	O
reference	O
]	O
)	O
is	O
a	O
typical	O
linear	Task
programming	Task
problem	Task
.	O

In	O
principle	O
,	O
one	O
can	O
solve	O
this	O
problem	O
to	O
obtain	O
a	O
non	Method
-	Method
parametric	Method
loss	Method
function	Method
for	O
the	O
LS	Method
-	Method
GAN	Method
.	O

Unfortunately	O
,	O
it	O
consists	O
of	O
a	O
large	O
number	O
of	O
constraints	O
,	O
whose	O
scale	O
is	O
at	O
an	O
order	O
of	O
.	O

This	O
prevents	O
us	O
from	O
using	O
(	O
[	O
reference	O
]	O
)	O
directly	O
to	O
solve	O
an	O
optimal	O
non	Method
-	Method
parametric	Method
LS	Method
-	Method
GAN	Method
model	Method
with	O
a	O
very	O
large	O
number	O
of	O
training	O
examples	O
.	O

On	O
the	O
contrary	O
,	O
a	O
more	O
tractable	O
solution	O
is	O
to	O
use	O
a	O
parameterized	Method
network	Method
to	O
solve	O
the	O
optimization	Task
problem	Task
(	O
[	O
reference	O
]	O
)	O
constrained	O
in	O
,	O
and	O
iteratively	O
update	O
parameterized	Method
and	Method
with	O
the	O
gradient	Method
descent	Method
method	Method
.	O

Although	O
the	O
non	Method
-	Method
parametric	Method
solution	Method
can	O
not	O
be	O
solved	O
directly	O
,	O
it	O
is	O
valuable	O
in	O
shedding	O
some	O
light	O
on	O
what	O
kind	O
of	O
the	O
loss	O
function	O
would	O
be	O
learned	O
by	O
a	O
deep	Method
network	Method
.	O

It	O
is	O
well	O
known	O
that	O
the	O
training	Task
of	O
the	O
classic	O
GAN	Method
generator	O
suffers	O
from	O
vanishing	Task
gradient	Task
problem	O
as	O
the	O
discriminator	Method
can	O
be	O
optimized	O
very	O
quickly	O
.	O

Recent	O
study	O
has	O
revealed	O
that	O
this	O
is	O
caused	O
by	O
using	O
the	O
Jensen	Method
-	Method
Shannon	Method
(	Method
JS	Method
)	Method
distance	Method
that	O
becomes	O
locally	O
saturated	O
and	O
gets	O
vanishing	Task
gradient	Task
to	O
train	O
the	O
GAN	Method
generator	O
if	O
the	O
discriminator	Method
is	O
over	O
-	O
trained	O
.	O

Similar	O
problem	O
has	O
also	O
been	O
found	O
in	O
the	O
energy	O
-	O
based	O
GAN	Method
(	O
EBGAN	O
)	O
as	O
it	O
minimizes	O
the	O
total	O
variation	O
that	O
is	O
not	O
continuous	O
or	O
(	O
sub	O
-)	O
differentiable	O
if	O
the	O
corresponding	O
discriminator	Method
is	O
fully	O
optimized	O
.	O

On	O
the	O
contrary	O
,	O
as	O
revealed	O
in	O
Theorem	O
[	O
reference	O
]	O
and	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
both	O
the	O
upper	O
and	O
lower	O
bounds	O
of	O
the	O
optimal	O
loss	O
function	O
of	O
the	O
LS	Method
-	Method
GAN	Method
are	O
cone	O
-	O
shaped	O
(	O
in	O
terms	O
of	O
that	O
defines	O
the	O
Lipschitz	O
continuity	O
)	O
,	O
and	O
have	O
non	O
-	O
vanishing	Task
gradient	Task
almost	O
everywhere	O
.	O

Moreover	O
,	O
Problem	O
(	O
[	O
reference	O
]	O
)	O
only	O
contains	O
linear	O
objective	O
and	O
constraints	O
;	O
this	O
is	O
contrary	O
to	O
the	O
classic	O
GAN	Method
that	O
involves	O
logistic	O
loss	O
terms	O
that	O
are	O
prone	O
to	O
saturation	O
with	O
vanishing	Task
gradient	Task
.	O

Thus	O
,	O
an	O
optimal	Method
loss	Method
function	Method
that	O
is	O
properly	O
sought	O
in	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
is	O
unlikely	O
to	O
saturate	O
between	O
these	O
two	O
bounds	O
,	O
and	O
it	O
should	O
be	O
able	O
to	O
provide	O
sufficient	O
gradient	O
to	O
update	O
the	O
generator	O
by	O
descending	O
(	O
[	O
reference	O
]	O
)	O
even	O
if	O
it	O
has	O
been	O
trained	O
till	O
optimality	O
.	O

Our	O
experiment	O
also	O
shows	O
that	O
,	O
even	O
if	O
the	O
loss	O
function	O
is	O
quickly	O
trained	O
to	O
optimality	O
,	O
it	O
can	O
still	O
provide	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	Method
in	O
the	O
LS	Method
-	Method
GAN	Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Conditional	Method
LS	Method
-	Method
GAN	Method
The	O
LS	Method
-	Method
GAN	Method
can	O
easily	O
be	O
generalized	O
to	O
produce	O
a	O
sample	O
based	O
on	O
a	O
given	O
condition	O
,	O
yielding	O
a	O
new	O
paradigm	O
of	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
.	O

For	O
example	O
,	O
if	O
the	O
condition	O
is	O
an	O
image	O
class	O
,	O
the	O
CLS	Method
-	Method
GAN	Method
seeks	O
to	O
produce	O
images	O
of	O
the	O
given	O
class	O
;	O
otherwise	O
,	O
if	O
a	O
text	O
description	O
is	O
given	O
as	O
a	O
condition	O
,	O
the	O
model	O
attempts	O
to	O
generate	O
images	O
aligned	O
with	O
the	O
given	O
description	O
.	O

This	O
gives	O
us	O
more	O
flexibility	O
in	O
controlling	O
what	O
samples	O
to	O
be	O
generated	O
.	O

Formally	O
,	O
the	O
generator	O
of	O
CLS	O
-	O
GAN	Method
takes	O
a	O
condition	O
vector	O
as	O
input	O
along	O
with	O
a	O
noise	O
vector	O
to	O
produce	O
a	O
sample	O
.	O

To	O
train	O
the	O
model	O
,	O
we	O
define	O
a	O
loss	Metric
function	Metric
to	O
measure	O
the	O
degree	O
of	O
the	O
misalignment	O
between	O
a	O
data	O
sample	O
and	O
a	O
given	O
condition	O
.	O

For	O
a	O
real	O
example	O
aligned	O
with	O
the	O
condition	O
,	O
its	O
loss	Metric
function	Metric
should	O
be	O
smaller	O
than	O
that	O
of	O
a	O
generated	O
sample	O
by	O
a	O
margin	O
of	O
.	O

This	O
results	O
in	O
the	O
following	O
constraint	O
,	O
Like	O
the	O
LS	Method
-	Method
GAN	Method
,	O
this	O
type	O
of	O
constraint	O
yields	O
the	O
following	O
non	Method
-	Method
zero	Method
-	Method
sum	Method
game	Method
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
which	O
seeks	O
a	O
Nash	O
equilibrium	O
so	O
that	O
minimizes	O
and	O
minimizes	O
where	O
denotes	O
either	O
the	O
joint	O
data	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
or	O
its	O
marginal	O
distribution	O
over	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O

Playing	O
the	O
above	O
game	O
will	O
lead	O
to	O
a	O
trained	O
pair	O
of	O
loss	O
function	O
and	O
generator	Method
.	O

We	O
can	O
show	O
that	O
the	O
learned	O
generator	Method
can	O
produce	O
samples	O
whose	O
distribution	O
follows	O
the	O
true	O
data	O
density	O
for	O
a	O
given	O
condition	O
.	O

To	O
prove	O
this	O
,	O
we	O
say	O
a	O
loss	O
function	O
is	O
Lipschitz	O
if	O
it	O
is	O
Lipschitz	O
continuous	O
in	O
its	O
first	O
argument	O
.	O

We	O
also	O
impose	O
the	O
following	O
regularity	O
condition	O
on	O
the	O
conditional	O
density	O
.	O

theorem	O
:	O
.	O

For	O
each	O
y	O
,	O
the	O
conditional	O
density	O
P⁢data	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
,	O
and	O
is	O
supported	O
in	O
a	O
convex	O
compact	O
set	O
of	O
x.	O
Then	O
it	O
is	O
not	O
difficult	O
to	O
prove	O
the	O
following	O
theorem	O
,	O
which	O
shows	O
that	O
the	O
conditional	O
density	O
becomes	O
as	O
.	O

Here	O
denotes	O
the	O
density	O
of	O
samples	O
generated	O
by	O
with	O
sampled	O
random	O
noise	O
.	O

theorem	O
:	O
.	O

Under	O
Assumption	O
,	O
a	O
Nash	Method
equilibrium	Method
(	O
θ*	O
,	O
ϕ	O
*	O
)	O
exists	O
such	O
that	O
(	O
i	O
)	O
⁢Lθ*	O
(	O
x	O
,	O
y	O
)	O
is	O
Lipschitz	O
continuous	O
in	O
x	O
for	O
each	O
y;	O
(	O
ii	O
)	O
PG*	O
(	O
x|y	O
)	O
is	O
Lipschitz	O
continuous;	O
(	O
iii	O
)	O
∫x|P⁢data	O
(	O
x|y	O
)-	O
PG*	O
(	O
x|y	O
)	O
|dx≤2λ	O
.	O

In	O
addition	O
,	O
similar	O
upper	O
and	O
lower	O
bounds	O
can	O
be	O
derived	O
to	O
characterize	O
the	O
learned	O
conditional	O
loss	O
function	O
following	O
the	O
same	O
idea	O
for	O
LS	Method
-	Method
GAN	Method
.	O

A	O
useful	O
byproduct	O
of	O
the	O
CLS	Method
-	Method
GAN	Method
is	O
one	O
can	O
use	O
the	O
learned	O
loss	O
function	O
to	O
predict	O
the	O
label	O
of	O
an	O
example	O
by	O
The	O
advantage	O
of	O
such	O
a	O
CLS	Method
-	Method
GAN	Method
classifier	Method
is	O
it	O
is	O
trained	O
with	O
both	O
labeled	O
and	O
generated	O
examples	O
,	O
the	O
latter	O
of	O
which	O
can	O
improve	O
the	O
training	O
of	O
the	O
classifier	Method
by	O
revealing	O
more	O
potential	O
variations	O
within	O
different	O
classes	O
of	O
samples	O
.	O

It	O
also	O
provides	O
a	O
way	O
to	O
evaluate	O
the	O
model	O
based	O
on	O
its	O
classification	Metric
performance	Metric
.	O

This	O
is	O
an	O
objective	O
metric	O
we	O
can	O
use	O
to	O
assess	O
the	O
quality	O
of	O
feature	Method
representations	Method
learned	O
by	O
the	O
model	O
.	O

For	O
a	O
classification	Task
task	Task
,	O
a	O
suitable	O
value	O
should	O
be	O
set	O
to	O
.	O

Although	O
Theorem	O
[	O
reference	O
]	O
shows	O
would	O
converge	O
to	O
the	O
true	O
conditional	O
density	O
by	O
increasing	O
,	O
it	O
only	O
ensures	O
it	O
is	O
a	O
good	O
generative	O
rather	O
than	O
classification	Method
model	Method
.	O

However	O
,	O
a	O
too	O
large	O
value	O
of	O
tends	O
to	O
ignore	O
the	O
first	Method
loss	Method
minimization	Method
term	Method
of	O
(	O
[	O
reference	O
]	O
)	O
that	O
plays	O
an	O
important	O
role	O
in	O
minimizing	Task
classification	Task
error	Task
.	O

Thus	O
,	O
a	O
trade	O
-	O
off	O
should	O
be	O
made	O
to	O
balance	O
between	O
classification	Task
and	Task
generation	Task
objectives	Task
.	O

subsection	O
:	O
Semi	O
-	O
Supervised	O
LS	Method
-	Method
GAN	Method
The	O
above	O
CLS	Method
-	Method
GAN	Method
can	O
be	O
considered	O
as	O
a	O
fully	Method
supervised	Method
model	Method
to	O
classify	O
examples	O
into	O
different	O
classes	O
.	O

It	O
can	O
also	O
be	O
extended	O
to	O
a	O
Semi	Method
-	Method
Supervised	Method
model	Method
by	O
incorporating	O
unlabeled	O
examples	O
.	O

Suppose	O
we	O
have	O
classes	O
indexed	O
by	O
.	O

In	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
for	O
each	O
class	O
,	O
we	O
choose	O
a	O
loss	O
function	O
that	O
,	O
for	O
example	O
,	O
can	O
be	O
defined	O
as	O
the	O
negative	O
log	O
-	O
softmax	O
,	O
where	O
is	O
the	O
th	O
activation	O
output	O
from	O
a	O
network	Method
layer	Method
.	O

Suppose	O
we	O
also	O
have	O
unlabeled	O
examples	O
available	O
,	O
and	O
we	O
can	O
define	O
a	O
new	O
loss	O
function	O
for	O
these	O
unlabeled	O
examples	O
so	O
that	O
they	O
can	O
be	O
involved	O
in	O
training	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

Consider	O
an	O
unlabeled	O
example	O
,	O
its	O
groundtruth	O
label	O
is	O
unknown	O
.	O

However	O
,	O
the	O
best	O
guess	O
of	O
its	O
label	O
can	O
be	O
made	O
by	O
choosing	O
the	O
one	O
that	O
minimizes	O
over	O
,	O
and	O
this	O
inspires	O
us	O
to	O
define	O
the	O
following	O
loss	O
function	O
for	O
the	O
unlabeled	O
example	O
as	O
Here	O
we	O
modify	O
to	O
so	O
can	O
be	O
viewed	O
as	O
the	O
probability	O
that	O
does	O
not	O
belong	O
to	O
any	O
known	O
label	O
.	O

Then	O
we	O
have	O
the	O
following	O
loss	Task
-	Task
sensitive	Task
objective	Task
that	O
explores	O
unlabeled	O
examples	O
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
,	O
This	O
objective	O
is	O
combined	O
with	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
to	O
train	O
the	O
loss	Method
function	Method
network	Method
by	O
minimizing	O
where	O
is	O
a	O
positive	O
hyperparameter	O
balancing	O
the	O
contributions	O
from	O
labeled	O
and	O
labeled	O
examples	O
.	O

The	O
idea	O
of	O
extending	O
the	O
GAN	Method
for	O
semi	Task
-	Task
supervised	Task
learning	Task
has	O
been	O
proposed	O
by	O
Odena	O
and	O
Salimans	O
et	O
al	O
.	O

,	O
where	O
generated	O
samples	O
are	O
assigned	O
to	O
an	O
artificial	O
class	O
,	O
and	O
unlabeled	O
examples	O
are	O
treated	O
as	O
the	O
negative	O
examples	O
.	O

Our	O
proposed	O
semi	Method
-	Method
supervised	Method
learning	Method
differs	O
in	O
creating	O
a	O
new	O
loss	O
function	O
for	O
unlabeled	O
examples	O
from	O
the	O
losses	O
for	O
existing	O
classes	O
,	O
by	O
minimizing	O
which	O
we	O
make	O
the	O
best	O
guess	O
of	O
the	O
classes	O
of	O
unlabeled	O
examples	O
.	O

The	O
guessed	O
labeled	O
will	O
provide	O
additional	O
information	O
to	O
train	O
the	O
CLS	O
-	O
GAN	Method
model	O
,	O
and	O
the	O
updated	O
model	O
will	O
in	O
turn	O
improve	O
the	O
guess	O
over	O
the	O
training	O
course	O
.	O

The	O
experiments	O
in	O
the	O
following	O
section	O
will	O
show	O
that	O
this	O
approach	O
can	O
generate	O
very	O
competitive	O
performance	O
especially	O
when	O
the	O
labeled	O
data	O
is	O
very	O
limited	O
.	O

section	O
:	O
Experiments	O
Objective	Task
evaluation	Task
of	O
a	O
data	Method
generative	Method
model	Method
is	O
not	O
an	O
easy	O
task	O
as	O
there	O
is	O
no	O
consensus	O
criteria	O
to	O
quantify	O
the	O
quality	O
of	O
generated	O
samples	O
.	O

For	O
this	O
reason	O
,	O
we	O
will	O
make	O
a	O
qualitative	Task
analysis	Task
of	Task
generated	Task
images	Task
,	O
and	O
use	O
image	Task
classification	Task
to	O
quantitatively	O
evaluate	O
the	O
resultant	O
LS	Method
-	Method
GAN	Method
model	O
.	O

First	O
,	O
we	O
will	O
assess	O
the	O
quality	O
of	O
generated	Metric
images	Metric
by	O
the	O
LS	Method
-	Method
GAN	Method
in	O
comparison	O
with	O
the	O
classic	O
GAN	Method
model	O
.	O

Then	O
,	O
we	O
will	O
make	O
an	O
objective	O
evaluation	O
on	O
the	O
CLS	Method
-	Method
GAN	Method
to	O
classify	Task
images	Task
.	O

This	O
task	O
evaluates	O
the	O
quality	O
of	O
feature	Method
representations	Method
learned	O
by	O
the	O
CLS	Method
-	Method
GAN	Method
in	O
terms	O
of	O
its	O
classification	Metric
accuracy	Metric
directly	O
.	O

Finally	O
,	O
we	O
will	O
assess	O
the	O
generalizability	O
of	O
various	O
GAN	Method
models	O
in	O
generating	O
new	O
images	O
out	O
of	O
training	O
examples	O
by	O
proposing	O
the	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
on	O
a	O
separate	O
test	O
set	O
.	O

subsection	O
:	O
Architectures	O
We	O
adopted	O
the	O
ideas	O
behind	O
the	O
network	Method
architecture	Method
for	O
the	O
DCGAN	Method
to	O
build	O
the	O
generator	Method
and	Method
the	Method
loss	Method
function	Method
networks	Method
.	O

Compared	O
with	O
the	O
conventional	O
CNNs	Method
,	O
maxpooling	Method
layers	Method
were	O
replaced	O
with	O
strided	Method
convolutions	Method
in	O
both	O
networks	O
,	O
and	O
fractionally	Method
-	Method
strided	Method
convolutions	Method
were	O
used	O
in	O
the	O
generator	Method
network	Method
to	O
upsample	O
feature	O
maps	O
across	O
layers	O
to	O
finer	O
resolutions	O
.	O

Batch	Method
-	Method
normalization	Method
layers	Method
were	O
added	O
in	O
both	O
networks	O
between	O
convolutional	Method
layers	Method
,	O
and	O
fully	Method
connected	Method
layers	Method
were	O
removed	O
from	O
these	O
networks	O
.	O

However	O
,	O
unlike	O
the	O
DCGAN	Method
,	O
the	O
LS	Method
-	Method
GAN	Method
model	O
(	O
unconditional	O
version	O
in	O
Section	O
[	O
reference	O
]	O
)	O
did	O
not	O
use	O
a	O
sigmoid	O
layer	O
as	O
the	O
output	O
for	O
the	O
loss	Method
function	Method
network	Method
.	O

Instead	O
,	O
we	O
removed	O
it	O
and	O
directly	O
output	O
the	O
activation	O
before	O
the	O
removed	O
sigmoid	Method
layer	Method
.	O

On	O
the	O
other	O
hand	O
,	O
for	O
the	O
loss	Method
function	Method
network	Method
in	O
CLS	Method
-	Method
GAN	Method
,	O
a	O
global	Method
mean	Method
-	Method
pooling	Method
layer	Method
was	O
added	O
on	O
top	O
of	O
convolutional	Method
layers	Method
.	O

This	O
produced	O
a	O
feature	O
map	O
that	O
output	O
the	O
conditional	O
loss	O
on	O
different	O
classes	O
.	O

In	O
the	O
generator	Method
network	Method
,	O
Tanh	Method
was	O
used	O
to	O
produce	O
images	O
whose	O
pixel	O
values	O
are	O
scaled	O
to	O
.	O

Thus	O
,	O
all	O
image	O
examples	O
in	O
datasets	O
were	O
preprocessed	O
to	O
have	O
their	O
pixel	O
values	O
in	O
.	O

More	O
details	O
about	O
the	O
design	O
of	O
network	Method
architectures	Method
can	O
be	O
found	O
in	O
literature	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
network	Method
architecture	Method
for	O
the	O
CLS	O
-	O
GAN	Method
model	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
datasets	Material
in	O
the	O
experiments	O
.	O

In	O
particular	O
,	O
the	O
architecture	O
of	O
the	O
loss	Method
function	Method
network	Method
was	O
adapted	O
from	O
that	O
used	O
in	O
with	O
nine	O
hidden	O
layers	O
.	O

subsection	O
:	O
Training	O
Details	O
The	O
models	O
were	O
trained	O
in	O
a	O
mini	O
-	O
batch	O
of	O
images	O
,	O
and	O
their	O
weights	O
were	O
initialized	O
from	O
a	O
zero	Method
-	Method
mean	Method
Gaussian	Method
distribution	Method
with	O
a	O
standard	O
deviation	O
of	O
.	O

The	O
Adam	Method
optimizer	Method
was	O
used	O
to	O
train	O
the	O
network	O
with	O
initial	O
learning	Metric
rate	Metric
and	O
being	O
set	O
to	O
and	O
respectively	O
,	O
while	O
the	O
learning	Metric
rate	Metric
was	O
annealed	O
every	O
epochs	O
by	O
a	O
factor	O
of	O
.	O

The	O
other	O
hyperparameters	O
such	O
as	O
and	O
were	O
chosen	O
based	O
on	O
an	O
independent	O
validation	O
set	O
held	O
out	O
from	O
training	O
examples	O
.	O

We	O
also	O
tested	O
various	O
forms	O
of	O
loss	O
margins	O
between	O
real	O
and	O
fake	O
samples	O
.	O

For	O
example	O
,	O
we	O
tried	O
the	O
distance	O
between	O
image	O
representations	O
as	O
the	O
margin	O
,	O
and	O
found	O
the	O
best	O
result	O
can	O
be	O
achieved	O
when	O
.	O

The	O
distance	O
between	O
convolutional	O
features	O
was	O
supposed	O
to	O
capture	O
perceptual	O
dissimilarity	O
between	O
images	O
.	O

But	O
we	O
should	O
avoid	O
a	O
direct	O
use	O
of	O
the	O
convolutional	O
features	O
from	O
the	O
loss	Method
function	Method
network	Method
,	O
since	O
we	O
found	O
they	O
would	O
tend	O
to	O
collapse	O
to	O
a	O
trivial	O
point	O
as	O
the	O
loss	O
margin	O
vanishes	O
.	O

The	O
feature	O
maps	O
from	O
a	O
separate	O
pretrained	Method
deep	Method
network	Method
,	O
such	O
as	O
Inception	Method
and	Method
VGG	Method
-	Method
16	Method
networks	Method
,	O
could	O
be	O
a	O
better	O
choice	O
to	O
define	O
the	O
loss	O
margin	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
images	O
generated	O
by	O
LS	Method
-	Method
GAN	Method
on	O
CelebA	O
with	O
the	O
inception	O
and	O
VGG	O
-	O
16	O
margins	O
.	O

However	O
,	O
for	O
a	O
fair	O
comparison	O
,	O
we	O
did	O
not	O
use	O
these	O
external	O
deep	Method
networks	Method
in	O
other	O
experiments	O
on	O
image	Task
generation	Task
and	O
classification	Task
tasks	Task
.	O

We	O
simply	O
used	O
the	O
distance	O
between	O
raw	O
images	O
as	O
the	O
loss	O
margin	O
,	O
and	O
it	O
still	O
achieved	O
competitive	O
results	O
.	O

This	O
demonstrates	O
the	O
robustness	O
of	O
the	O
proposed	O
method	O
without	O
having	O
to	O
choose	O
a	O
sophisticated	O
loss	O
margin	O
.	O

This	O
is	O
also	O
consistent	O
with	O
our	O
theoretical	O
analysis	O
where	O
we	O
do	O
not	O
assume	O
any	O
particular	O
form	O
of	O
loss	O
margin	O
to	O
prove	O
the	O
results	O
.	O

For	O
the	O
generator	Method
network	Method
of	O
LS	Method
-	Method
GAN	Method
,	O
it	O
took	O
a	O
-	O
dimensional	O
random	O
vector	O
drawn	O
from	O
Unif	Method
as	O
input	O
.	O

For	O
the	O
CLS	O
-	O
GAN	Method
generator	O
,	O
an	O
one	Method
-	Method
hot	Method
vector	Method
encoding	O
the	O
image	O
class	O
condition	O
was	O
concatenated	O
with	O
the	O
sampled	O
random	O
vector	O
.	O

The	O
CLS	Method
-	Method
GAN	Method
was	O
trained	O
by	O
involving	O
both	O
unlabeled	O
and	O
labeled	O
examples	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

This	O
was	O
compared	O
against	O
the	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	Method
and	Method
semi	Method
-	Method
supervised	Method
models	Method
.	O

subsection	O
:	O
Generated	O
Images	O
by	O
LS	Method
-	Method
GAN	Method
First	O
we	O
made	O
a	O
qualitative	O
comparison	O
between	O
the	O
images	O
generated	O
by	O
the	O
DCGAN	Method
and	O
the	O
LS	Method
-	Method
GAN	Method
on	O
the	O
celebA	O
dataset	O
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
visual	Metric
quality	Metric
of	Metric
images	Metric
generated	O
by	O
LS	Method
-	Method
GAN	Method
and	O
DCGAN	Method
after	O
they	O
were	O
trained	O
for	O
epochs	O
,	O
and	O
there	O
was	O
no	O
perceptible	O
difference	O
between	O
the	O
qualities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
the	O
DCGAN	Method
architecture	Method
has	O
been	O
exhaustively	O
fine	O
-	O
tuned	O
in	O
terms	O
of	O
the	O
classic	O
GAN	Method
training	O
criterion	O
to	O
maximize	O
the	O
image	Task
generation	Task
performance	O
.	O

It	O
was	O
susceptible	O
that	O
its	O
architecture	O
could	O
be	O
fragile	O
if	O
we	O
make	O
some	O
change	O
to	O
it	O
.	O

Here	O
we	O
tested	O
if	O
the	O
LS	Method
-	Method
GAN	Method
can	O
be	O
more	O
robust	O
than	O
the	O
DCGAN	Method
when	O
a	O
structure	O
change	O
was	O
made	O
.	O

For	O
example	O
,	O
one	O
of	O
the	O
most	O
key	O
components	O
in	O
the	O
DCGAN	Method
is	O
the	O
batch	Method
normalization	Method
inserted	O
between	O
the	O
fractional	Method
convolution	Method
layers	Method
in	O
the	O
generator	Method
network	Method
.	O

It	O
has	O
been	O
reported	O
in	O
literature	O
that	O
the	O
batch	Method
normalization	Method
not	O
only	O
plays	O
a	O
key	O
role	O
in	O
training	O
the	O
DCGAN	Method
model	Method
,	O
but	O
also	O
prevents	O
the	O
mode	O
collapse	O
of	O
the	O
generator	O
into	O
few	O
data	O
points	O
.	O

The	O
results	O
were	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

If	O
one	O
removed	O
the	O
batch	Method
normalization	Method
layers	Method
from	O
the	O
generator	Method
,	O
the	O
DCGAN	Method
would	O
collapse	O
without	O
producing	O
any	O
face	O
images	O
.	O

On	O
the	O
contrary	O
,	O
the	O
LS	Method
-	Method
GAN	Method
still	O
performed	O
very	O
well	O
even	O
if	O
these	O
batch	Method
normalization	Method
layers	Method
were	O
removed	O
,	O
and	O
there	O
was	O
no	O
perceived	O
deterioration	O
or	O
mode	O
collapse	O
of	O
the	O
generated	O
images	O
.	O

This	O
shows	O
that	O
the	O
LS	Method
-	Method
GAN	Method
was	O
more	O
resilient	O
than	O
the	O
DCGAN	Method
.	O

We	O
also	O
analyzed	O
the	O
magnitude	O
(	O
norm	O
)	O
of	O
the	O
generator	O
’s	O
gradient	O
(	O
in	O
logarithmic	O
scale	O
)	O
in	O
Figure	O
[	O
reference	O
]	O
over	O
iterations	O
.	O

With	O
the	O
loss	O
function	O
being	O
updated	O
every	O
iteration	O
,	O
the	O
generator	Method
was	O
only	O
updated	O
every	O
,	O
,	O
and	O
iterations	O
.	O

From	O
the	O
figure	O
,	O
we	O
note	O
that	O
the	O
magnitude	O
of	O
the	O
generator	O
’s	O
gradient	O
,	O
no	O
matter	O
how	O
frequently	O
the	O
loss	O
function	O
was	O
updated	O
,	O
gradually	O
increased	O
until	O
it	O
stopped	O
at	O
the	O
same	O
level	O
.	O

This	O
implies	O
the	O
objective	O
function	O
to	O
update	O
the	O
generator	Method
tended	O
to	O
be	O
linear	O
rather	O
than	O
saturated	O
through	O
the	O
training	Method
process	Method
,	O
which	O
was	O
consistent	O
with	O
our	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
optimal	Method
loss	Method
function	Method
.	O

Thus	O
,	O
it	O
provided	O
sufficient	O
gradient	O
to	O
continuously	O
update	O
the	O
generator	Method
.	O

Furthermore	O
,	O
we	O
compared	O
the	O
images	O
generated	O
with	O
different	O
frequencies	O
of	O
updating	O
the	O
loss	O
function	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
there	O
was	O
no	O
noticeable	O
difference	O
in	O
the	O
visual	Metric
quality	Metric
.	O

This	O
shows	O
the	O
LS	Method
-	Method
GAN	Method
was	O
not	O
affected	O
by	O
over	O
-	O
trained	O
loss	O
function	O
in	O
experiments	O
.	O

subsection	O
:	O
Image	Task
Classification	Task
We	O
conducted	O
experiments	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
to	O
compare	O
the	O
classification	Metric
accuracy	Metric
of	O
LS	Method
-	Method
GAN	Method
with	O
the	O
other	O
approaches	O
.	O

subsubsection	O
:	O
CIFAR	Material
-	Material
10	Material
The	O
CIFAR	Material
dataset	Material
consists	O
of	O
50	O
,	O
000	O
training	O
images	O
and	O
test	O
images	O
on	O
ten	O
image	O
categories	O
.	O

We	O
tested	O
the	O
proposed	O
CLS	O
-	O
GAN	Method
model	O
with	O
class	O
labels	O
as	O
conditions	O
.	O

In	O
the	O
supervised	Task
training	Task
,	O
all	O
labeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

We	O
also	O
conducted	O
experiments	O
with	O
labeled	O
examples	O
per	O
class	O
,	O
which	O
was	O
a	O
more	O
challenging	O
task	O
as	O
much	O
fewer	O
labeled	O
examples	O
were	O
used	O
for	O
training	O
.	O

In	O
this	O
case	O
,	O
the	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
to	O
train	O
the	O
model	O
in	O
a	O
semi	Task
-	Task
supervised	Task
fashion	Task
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
each	O
mini	O
-	O
batch	O
,	O
the	O
same	O
number	O
of	O
labeled	O
and	O
unlabeled	O
examples	O
were	O
used	O
to	O
update	O
the	O
model	O
by	O
stochastic	Method
gradient	Method
descent	Method
.	O

The	O
experiment	O
results	O
on	O
this	O
task	O
were	O
reported	O
by	O
averaging	O
over	O
ten	O
subsets	O
of	O
labeled	O
examples	O
.	O

Both	O
hyperparameters	O
and	O
were	O
chosen	O
via	O
a	O
five	O
-	O
fold	Method
cross	Method
-	Method
validation	Method
on	O
the	O
labeled	O
examples	O
from	O
and	O
respectively	O
.	O

Once	O
they	O
were	O
chosen	O
,	O
the	O
model	O
was	O
trained	O
with	O
the	O
chosen	O
hyperparameters	O
on	O
the	O
whole	O
training	O
set	O
,	O
and	O
the	O
performance	O
was	O
reported	O
based	O
on	O
the	O
results	O
on	O
the	O
test	O
set	O
.	O

As	O
in	O
the	O
improved	O
GAN	Method
,	O
we	O
also	O
adopted	O
the	O
weight	Method
normalization	Method
and	O
feature	Method
matching	Method
mechanisms	Method
for	O
the	O
sake	O
of	O
the	O
fair	O
comparison	O
.	O

We	O
compared	O
the	O
proposed	O
model	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
in	O
literature	O
.	O

In	O
particular	O
,	O
we	O
compared	O
with	O
the	O
conditional	O
GAN	Method
as	O
well	O
as	O
the	O
DCGAN	Method
.	O

For	O
the	O
sake	O
of	O
fair	O
comparison	O
,	O
the	O
conditional	O
GAN	Method
shared	O
the	O
same	O
architecture	O
as	O
the	O
CLS	Method
-	Method
GAN	Method
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
DCGAN	Method
algorithm	Method
max	O
-	O
pooled	O
the	O
discriminator	O
’s	O
convolution	O
features	O
from	O
all	O
layers	O
to	O
grids	O
as	O
the	O
image	O
features	O
,	O
and	O
a	O
L2	Method
-	Method
SVM	Method
was	O
then	O
trained	O
to	O
classify	O
images	O
.	O

The	O
DCGAN	Method
was	O
an	O
unsupervised	Method
model	Method
which	O
had	O
shown	O
competitive	O
performance	O
on	O
generating	O
photo	O
-	O
realistic	O
images	O
.	O

Its	O
feature	Method
representations	Method
were	O
believed	O
to	O
reach	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
modeling	Task
images	Task
with	O
no	O
supervision	O
.	O

We	O
also	O
compared	O
with	O
the	O
other	O
recently	O
developed	O
supervised	Method
and	Method
semi	Method
-	Method
supervised	Method
models	Method
in	O
literature	O
,	O
including	O
the	O
baseline	Method
1	Method
Layer	Method
K	Method
-	Method
means	Method
feature	Method
extraction	Method
pipeline	Method
,	O
a	O
multi	Method
-	Method
layer	Method
extension	Method
of	O
the	O
baseline	Method
model	Method
(	O
3	O
Layer	Method
K	Method
-	Method
means	Method
Learned	Method
RF	Method
)	O
,	O
View	Method
Invariant	Method
K	Method
-	Method
means	Method
,	O
Examplar	Method
CNN	Method
,	O
Ladder	Method
Network	Method
,	O
as	O
well	O
as	O
CatGAN	Method
.	O

In	O
particular	O
,	O
among	O
the	O
compared	O
semi	Method
-	Method
supervised	Method
algorithms	Method
,	O
the	O
improved	O
GAN	Method
had	O
recorded	O
the	O
best	O
performance	O
in	O
literature	O
.	O

Furthermore	O
,	O
we	O
also	O
compared	O
with	O
the	O
ALI	Method
that	O
extended	O
the	O
classic	O
GAN	Method
by	O
jointly	O
generating	O
data	O
and	O
inferring	O
their	O
representations	O
,	O
which	O
achieved	O
comparable	O
performance	O
to	O
the	O
Improved	O
GAN	Method
.	O

This	O
pointed	O
out	O
an	O
interesting	O
direction	O
to	O
extend	O
the	O
CLS	Method
-	Method
GAN	Method
by	O
directly	O
inferring	O
the	O
data	Method
representation	Method
,	O
and	O
we	O
will	O
leave	O
it	O
in	O
the	O
future	O
work	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
experiment	O
results	O
,	O
showing	O
the	O
CSL	O
-	O
GAN	Method
successfully	O
outperformed	O
the	O
compared	O
algorithms	O
in	O
both	O
fully	Task
-	Task
supervised	Task
and	Task
semi	Task
-	Task
supervised	Task
settings	Task
.	O

subsubsection	O
:	O
SVHN	Material
The	O
SVHN	Material
(	O
i.e.	O
,	O
Street	Material
View	Material
House	Material
Number	Material
)	O
dataset	O
contains	O
color	O
images	O
of	O
house	O
numbers	O
collected	O
by	O
Google	O
Street	O
View	O
.	O

They	O
were	O
roughly	O
centered	O
on	O
a	O
digit	O
in	O
a	O
house	O
number	O
,	O
and	O
the	O
objective	O
is	O
to	O
recognize	O
the	O
digit	O
.	O

The	O
training	O
set	O
has	O
digits	O
while	O
the	O
test	O
set	O
consists	O
of	O
.	O

To	O
test	O
the	O
model	O
,	O
labeled	O
digits	O
were	O
used	O
to	O
train	O
the	O
model	O
,	O
which	O
are	O
uniformly	O
selected	O
from	O
ten	O
digit	O
classes	O
,	O
that	O
is	O
labeled	O
examples	O
per	O
digit	O
class	O
.	O

The	O
remaining	O
unlabeled	O
examples	O
were	O
used	O
as	O
additional	O
data	O
to	O
enhance	O
the	O
generative	Method
ability	Method
of	O
CLS	Method
-	Method
GAN	Method
in	O
semi	Task
-	Task
supervised	Task
fashion	Task
.	O

We	O
expect	O
a	O
good	O
generative	Method
model	Method
could	O
produce	O
additional	O
examples	O
to	O
augment	O
the	O
training	O
set	O
.	O

We	O
used	O
the	O
same	O
experiment	O
setup	O
and	O
network	Method
architecture	Method
for	O
CIFAR	Material
-	Material
10	Material
to	O
train	O
the	O
LS	Method
-	Method
GAN	Method
on	O
this	O
dataset	O
.	O

Table	O
[	O
reference	O
]	O
reports	O
the	O
result	O
on	O
the	O
SVHN	Material
,	O
and	O
it	O
shows	O
that	O
the	O
LS	Method
-	Method
GAN	Method
performed	O
the	O
best	O
among	O
the	O
compared	O
algorithms	O
.	O

subsubsection	O
:	O
Analysis	Task
of	Task
Generated	Task
Images	Task
by	O
CLS	Method
-	Method
GAN	Method
Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
generated	O
images	O
by	O
CLS	Method
-	Method
GAN	Method
for	O
MNIST	O
,	O
CIFAR	Material
-	Material
10	Material
and	O
SVHN	Material
datasets	O
.	O

On	O
each	O
dataset	O
,	O
images	O
in	O
a	O
column	O
were	O
generated	O
for	O
the	O
same	O
class	O
.	O

On	O
the	O
MNIST	O
and	O
the	O
SVHN	Material
,	O
both	O
handwritten	O
and	O
street	O
-	O
view	O
digits	O
are	O
quite	O
legible	O
.	O

Both	O
also	O
cover	O
many	O
variants	O
for	O
each	O
digit	O
class	O
.	O

For	O
example	O
,	O
the	O
synthesized	O
MNIST	O
digits	O
have	O
various	O
writing	O
styles	O
,	O
rotations	O
and	O
sizes	O
,	O
and	O
the	O
generated	O
SVHN	Material
digits	O
have	O
various	O
lighting	O
conditions	O
,	O
sizes	O
and	O
even	O
different	O
co	O
-	O
occurring	O
digits	O
in	O
the	O
cropped	O
bounding	O
boxes	O
.	O

On	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
,	O
image	O
classes	O
can	O
be	O
recognized	O
from	O
the	O
generated	O
images	O
although	O
some	O
visual	O
details	O
are	O
missing	O
.	O

This	O
is	O
because	O
the	O
images	O
in	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
have	O
very	O
low	O
resolution	O
(	O
pixels	O
)	O
,	O
and	O
most	O
details	O
are	O
even	O
missing	O
from	O
input	O
examples	O
.	O

We	O
also	O
observe	O
that	O
if	O
we	O
set	O
a	O
small	O
value	O
to	O
the	O
hyperparameter	O
,	O
the	O
generated	O
images	O
would	O
become	O
very	O
similar	O
to	O
each	O
other	O
within	O
each	O
class	O
.	O

As	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
images	O
were	O
generated	O
by	O
halving	O
used	O
for	O
generating	O
images	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

A	O
smaller	O
means	O
a	O
relatively	O
large	O
weight	O
was	O
placed	O
on	O
the	O
first	O
loss	Method
minimization	Method
term	Method
of	O
(	O
[	O
reference	O
]	O
)	O
,	O
which	O
tends	O
to	O
collapse	O
generated	O
images	O
to	O
a	O
single	O
mode	O
as	O
it	O
aggressively	O
minimizes	O
their	O
losses	O
to	O
train	O
the	O
generator	Method
.	O

This	O
is	O
also	O
consistent	O
with	O
Theorem	O
[	O
reference	O
]	O
where	O
the	O
density	O
of	O
generated	O
samples	O
with	O
a	O
smaller	O
could	O
have	O
a	O
larger	O
deviation	O
from	O
the	O
underlying	O
density	O
.	O

One	O
should	O
avoid	O
the	O
collapse	O
of	O
trained	O
generator	Method
since	O
diversifying	O
generated	O
images	O
can	O
improve	O
the	O
classification	Metric
performance	Metric
of	O
the	O
CLS	Method
-	Method
GAN	Method
by	O
revealing	O
more	O
intra	O
-	O
class	O
variations	O
.	O

This	O
will	O
help	O
improve	O
the	O
model	O
’s	O
generalization	Metric
ability	Metric
as	O
these	O
variations	O
could	O
appear	O
in	O
future	O
images	O
.	O

However	O
,	O
one	O
should	O
also	O
avoid	O
setting	O
too	O
large	O
value	O
to	O
.	O

Otherwise	O
,	O
the	O
role	O
of	O
the	O
first	Method
loss	Method
minimization	Method
term	Method
could	O
be	O
underestimated	O
,	O
which	O
can	O
also	O
adversely	O
affect	O
the	O
classification	Task
results	O
without	O
reducing	O
the	O
training	Metric
loss	Metric
to	O
a	O
satisfactory	O
level	O
.	O

Therefore	O
,	O
we	O
choose	O
a	O
proper	O
value	O
for	O
by	O
cross	Method
-	Method
validation	Method
on	O
the	O
training	O
set	O
in	O
the	O
experiments	O
.	O

In	O
brief	O
,	O
the	O
comparison	O
between	O
Figure	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
reveals	O
a	O
trade	O
-	O
off	O
between	O
image	Task
generation	Task
quality	O
and	O
classification	Metric
accuracy	Metric
through	O
the	O
hyperparameter	Method
.	O

Such	O
a	O
trade	O
-	O
off	O
is	O
intuitive	O
:	O
while	O
a	O
classification	Task
task	Task
usually	O
focuses	O
on	O
learning	O
class	Task
-	Task
invariant	Task
representations	Task
that	O
do	O
not	O
change	O
within	O
a	O
class	O
,	O
image	Task
generation	Task
should	O
be	O
able	O
to	O
capture	O
many	O
variant	O
factors	O
(	O
e.g.	O
,	O
lighting	O
conditions	O
,	O
viewing	O
angles	O
,	O
and	O
object	O
poses	O
)	O
so	O
that	O
it	O
could	O
diversify	O
generated	O
samples	O
for	O
each	O
class	O
.	O

Although	O
diversified	O
examples	O
can	O
augment	O
training	O
dataset	O
,	O
it	O
comes	O
at	O
a	O
cost	O
of	O
trading	O
class	O
-	O
invariance	O
for	O
modeling	O
variant	O
generation	O
factors	O
.	O

Perhaps	O
,	O
this	O
is	O
an	O
intrinsic	O
dilemma	O
between	O
supervised	Method
learning	Method
and	O
data	Task
generation	Task
that	O
is	O
worth	O
more	O
theoretical	O
and	O
empirical	O
studies	O
in	O
future	O
.	O

subsection	O
:	O
Evaluation	O
of	O
Generalization	Metric
Performances	Metric
Most	O
of	O
existing	O
metrics	O
like	O
Inception	Metric
Score	Metric
for	O
evaluating	O
GAN	Method
models	O
focus	O
on	O
comparing	O
the	O
qualities	O
and	O
diversities	O
of	O
their	O
generated	O
images	O
.	O

However	O
,	O
even	O
though	O
a	O
GAN	Method
model	O
can	O
produce	O
diverse	O
and	O
high	O
quality	O
images	O
with	O
no	O
collapsed	Method
generators	Method
,	O
it	O
is	O
still	O
unknown	O
if	O
the	O
model	O
can	O
generate	O
unseen	O
images	O
out	O
of	O
given	O
examples	O
,	O
or	O
simply	O
memorizing	O
existing	O
ones	O
.	O

While	O
one	O
of	O
our	O
main	O
pursuits	O
in	O
this	O
paper	O
is	O
a	O
generalizable	O
LS	Method
-	Method
GAN	Method
,	O
we	O
were	O
motivated	O
to	O
propose	O
the	O
following	O
Minimum	Metric
Reconstruction	Metric
Error	Metric
(	O
MRE	Metric
)	O
to	O
compare	O
its	O
generalizability	O
with	O
various	O
GANs	Method
.	O

Specifically	O
,	O
for	O
an	O
unseen	O
test	O
image	O
,	O
we	O
aim	O
to	O
find	O
an	O
input	O
noise	O
that	O
can	O
best	O
reconstruct	O
with	O
the	O
smallest	O
error	O
,	O
i.e.	O
,	O
where	O
is	O
the	O
GAN	Method
generator	O
under	O
evaluation	O
.	O

Obviously	O
,	O
if	O
is	O
adequate	O
to	O
produce	O
new	O
images	O
,	O
it	O
should	O
have	O
a	O
small	O
reconstruction	Metric
error	Metric
on	O
a	O
separate	O
test	O
set	O
that	O
has	O
not	O
been	O
used	O
in	O
training	O
the	O
model	O
.	O

We	O
assessed	O
the	O
GAN	Method
’s	O
generalizability	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
tiny	O
ImageNet	O
datasets	O
.	O

On	O
CIFAR	Material
-	Material
10	Material
,	O
we	O
split	O
the	O
dataset	O
into	O
50	O
%	O
training	O
examples	O
,	O
25	O
%	O
validation	O
examples	O
and	O
25	O
%	O
test	O
examples	O
;	O
the	O
tiny	O
ImageNet	O
was	O
split	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
in	O
a	O
ratio	O
of	O
10:1:1	O
.	O

For	O
a	O
fair	O
comparison	O
,	O
all	O
the	O
hyperparameters	O
,	O
including	O
the	O
number	O
of	O
epochs	O
,	O
were	O
chosen	O
based	O
on	O
the	O
average	Metric
MREs	Metric
on	O
the	O
validation	O
set	O
,	O
and	O
the	O
test	O
MREs	Metric
were	O
reported	O
for	O
comparison	O
.	O

The	O
optimal	O
’s	O
were	O
iteratively	O
updated	O
on	O
the	O
validation	O
and	O
test	O
sets	O
by	O
descending	O
the	O
gradient	O
of	O
the	O
reconstruction	O
errors	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
compare	O
the	O
test	O
MREs	Metric
over	O
epochs	O
by	O
LS	Method
-	Method
GAN	Method
,	O
GLS	Method
-	Method
GAN	Method
,	O
WGAN	Method
,	O
WGAN	Method
-	O
GP	O
and	O
DCGAN	Method
on	O
CIFAR	Material
-	Material
10	Material
respectively	O
.	O

For	O
the	O
sake	O
of	O
a	O
fair	O
comparison	O
,	O
all	O
models	O
were	O
trained	O
with	O
the	O
network	Method
architecture	Method
used	O
in	O
.	O

The	O
result	O
clearly	O
shows	O
the	O
regularized	Method
models	Method
,	O
including	O
GLS	Method
-	Method
GAN	Method
,	O
LS	Method
-	Method
GAN	Method
,	O
WGAN	Method
-	O
GP	O
and	O
WGAN	Method
,	O
have	O
apparently	O
better	O
generalization	Metric
performances	Metric
than	O
the	O
unregularized	Method
DCGAN	Method
based	O
on	O
the	O
classic	O
GAN	Method
model	O
.	O

On	O
CIFAR	Material
-	Material
10	Material
,	O
the	O
test	O
MRE	Metric
was	O
reduced	O
from	O
by	O
DCGAN	Method
to	O
as	O
small	O
as	O
and	O
by	O
WGAN	Method
and	O
GLS	O
-	O
GAN	Method
respectively	O
;	O
on	O
tiny	O
ImageNet	O
,	O
the	O
GLS	Method
-	Method
GAN	Method
reaches	O
the	O
smallest	O
test	O
MRE	Metric
of	O
among	O
all	O
compared	O
regularized	O
and	O
unregularized	O
GANs	Method
.	O

In	O
addition	O
,	O
the	O
DCGAN	Method
exhibited	O
fluctuating	O
MREs	Metric
on	O
the	O
CIFAR	Material
-	Material
10	Material
,	O
while	O
the	O
regularized	Method
models	Method
steadily	O
decreased	O
the	O
MREs	Metric
over	O
epochs	O
.	O

This	O
implies	O
regularized	O
GANs	Method
have	O
more	O
stable	O
training	O
than	O
the	O
classic	O
GAN	Method
.	O

We	O
illustrate	O
some	O
examples	O
of	O
reconstructed	O
images	O
by	O
different	O
GANs	Method
on	O
the	O
test	O
set	O
along	O
with	O
their	O
test	O
MREs	Metric
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
results	O
show	O
the	O
GLS	Method
-	Method
GAN	Method
achieved	O
the	O
smallest	O
test	Metric
MRE	Metric
of	O
0.1089	O
and	O
0.2085	O
with	O
a	O
LeakyReLU	O
cost	O
function	O
of	O
slope	O
and	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
tiny	O
ImageNet	O
,	O
followed	O
by	O
the	O
other	O
regularized	O
GAN	Method
models	O
.	O

This	O
is	O
not	O
a	O
surprising	O
result	O
since	O
it	O
has	O
been	O
shown	O
in	O
Section	O
[	O
reference	O
]	O
that	O
the	O
other	O
regularized	O
GANs	Method
such	O
as	O
LS	Method
-	Method
GAN	Method
and	O
WGAN	Method
are	O
only	O
special	O
cases	O
of	O
the	O
GLS	O
-	O
GAN	Method
model	O
that	O
covers	O
larger	O
family	O
of	O
models	O
.	O

Here	O
we	O
only	O
considered	O
LeakyReLU	O
as	O
the	O
cost	Method
function	Method
for	O
GLS	Method
-	Method
GAN	Method
.	O

Of	O
course	O
,	O
there	O
exist	O
many	O
more	O
cost	O
functions	O
satisfying	O
the	O
two	O
conditions	O
in	O
Section	O
[	O
reference	O
]	O
to	O
expand	O
the	O
family	O
of	O
regularized	O
GANs	Method
,	O
which	O
should	O
have	O
potentials	O
of	O
yielding	O
even	O
better	O
generalization	Metric
performances	Metric
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
Loss	O
-	O
Sensitive	O
GAN	Method
(	O
LS	Method
-	Method
GAN	Method
)	O
approach	O
to	O
generate	O
samples	O
from	O
a	O
data	O
distribution	O
.	O

The	O
LS	Method
-	Method
GAN	Method
learns	O
a	O
loss	Method
function	Method
to	O
distinguish	O
between	O
generated	O
and	O
real	O
samples	O
,	O
where	O
the	O
loss	O
of	O
a	O
real	O
sample	O
should	O
be	O
smaller	O
by	O
a	O
margin	O
than	O
that	O
of	O
a	O
generated	O
sample	O
.	O

Our	O
theoretical	O
analysis	O
shows	O
the	O
distributional	O
consistency	O
between	O
the	O
real	O
and	O
generated	O
samples	O
based	O
on	O
the	O
Lipschitz	O
regularity	O
.	O

This	O
no	O
longer	O
needs	O
a	O
non	Method
-	Method
parametric	Method
discriminator	Method
with	O
infinite	O
modeling	O
ability	O
in	O
the	O
classic	O
GAN	Method
,	O
allowing	O
us	O
to	O
search	O
for	O
the	O
optimal	O
loss	O
function	O
in	O
a	O
smaller	O
functional	O
space	O
with	O
a	O
bounded	O
Lipschitz	O
constant	O
.	O

Moreover	O
,	O
we	O
prove	O
the	O
generalizability	O
of	O
LS	Method
-	Method
GAN	Method
by	O
showing	O
its	O
required	O
number	O
of	O
training	O
examples	O
is	O
polynomial	O
in	O
its	O
complexity	Metric
.	O

This	O
suggests	O
the	O
generalization	Metric
performance	Metric
can	O
be	O
improved	O
by	O
penalizing	O
the	O
Lipschitz	O
constants	O
(	O
via	O
their	O
gradient	O
surrogates	O
)	O
of	O
the	O
loss	O
function	O
to	O
reduce	O
the	O
sample	Metric
complexity	Metric
.	O

Furthermore	O
,	O
our	O
non	Method
-	Method
parametric	Method
analysis	Method
of	O
the	O
optimal	Method
loss	Method
function	Method
shows	O
its	O
lower	O
and	O
upper	O
bounds	O
are	O
cone	O
-	O
shaped	O
with	O
non	O
-	O
vanishing	Task
gradient	Task
almost	O
everywhere	O
,	O
implying	O
the	O
generator	Method
can	O
be	O
continuously	O
updated	O
even	O
if	O
the	O
loss	O
function	O
is	O
over	O
-	O
trained	O
.	O

Finally	O
,	O
we	O
extend	O
the	O
LS	Method
-	Method
GAN	Method
to	O
a	O
Conditional	Method
LS	Method
-	Method
GAN	Method
(	O
CLS	Method
-	Method
GAN	Method
)	O
for	O
semi	Task
-	Task
supervised	Task
tasks	Task
,	O
and	O
demonstrate	O
it	O
reaches	O
competitive	O
performances	O
on	O
both	O
image	Task
generation	Task
and	O
classification	Task
tasks	Task
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
To	O
prove	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
two	O
probability	O
densities	O
⁢p	O
(	O
x	O
)	O
and	O
⁢q	O
(	O
x	O
)	O
,	O
if	O
≥⁢p	O
(	O
x	O
)	O
⁢ηq	O
(	O
x	O
)	O
almost	O
everywhere	O
,	O
we	O
have	O
for	O
∈η	O
(	O
0	O
,	O
1	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

We	O
have	O
the	O
following	O
equalities	O
and	O
inequalities	O
:	O
This	O
completes	O
the	O
proof	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Lemma	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

Suppose	O
is	O
a	O
Nash	O
equilibrium	O
for	O
the	O
problem	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
.	O

We	O
also	O
have	O
for	O
any	O
as	O
minimizes	O
.	O

In	O
particular	O
,	O
we	O
can	O
replace	O
in	O
with	O
,	O
which	O
yields	O
Applying	O
this	O
inequality	O
into	O
(	O
[	O
reference	O
]	O
)	O
leads	O
to	O
where	O
the	O
last	O
inequality	O
follows	O
as	O
is	O
nonnegative	O
.	O

On	O
the	O
other	O
hand	O
,	O
consider	O
a	O
particular	O
loss	Method
function	Method
When	O
is	O
a	O
sufficiently	O
small	O
positive	O
coefficient	O
,	O
is	O
a	O
nonexpansive	O
function	O
(	O
i.e.	O
,	O
a	O
function	O
with	O
Lipschitz	O
constant	O
no	O
larger	O
than	O
.	O

)	O
.	O

This	O
follows	O
from	O
the	O
assumption	O
that	O
and	O
are	O
Lipschitz	O
.	O

In	O
this	O
case	O
,	O
we	O
have	O
By	O
placing	O
this	O
into	O
,	O
one	O
can	O
show	O
that	O
where	O
the	O
first	O
equality	O
uses	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
second	O
equality	O
is	O
obtained	O
by	O
substituting	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
into	O
the	O
equation	O
.	O

Assuming	O
that	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
the	O
above	O
equation	O
would	O
be	O
strictly	O
upper	O
bounded	O
by	O
and	O
we	O
have	O
This	O
results	O
in	O
a	O
contradiction	O
with	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Therefore	O
,	O
we	O
must	O
have	O
for	O
almost	O
everywhere	O
.	O

By	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
have	O
Let	O
,	O
this	O
leads	O
to	O
This	O
proves	O
that	O
converges	O
to	O
as	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Lemma	O
[	O
reference	O
]	O
proof	O
:	O
Proof	O
.	O

Suppose	O
a	O
pair	O
of	O
jointly	O
solve	O
the	O
WGAN	Method
problem	O
.	O

Then	O
,	O
on	O
one	O
hand	O
,	O
we	O
have	O
where	O
the	O
inequality	O
follows	O
from	O
by	O
replacing	O
with	O
.	O

Consider	O
a	O
particular	O
.	O

Since	O
and	O
are	O
Lipschitz	O
by	O
assumption	O
,	O
when	O
is	O
sufficiently	O
small	O
,	O
it	O
can	O
be	O
shown	O
that	O
.	O

Substituting	O
this	O
into	O
,	O
we	O
get	O
Let	O
us	O
assume	O
on	O
a	O
set	O
of	O
nonzero	O
measure	O
,	O
we	O
would	O
have	O
This	O
leads	O
to	O
a	O
contradiction	O
with	O
(	O
[	O
reference	O
]	O
)	O
,	O
so	O
we	O
must	O
have	O
almost	O
everywhere	O
.	O

Hence	O
,	O
by	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
prove	O
the	O
conclusion	O
that	O
∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
For	O
simplicity	O
,	O
throughout	O
this	O
section	O
,	O
we	O
disregard	O
the	O
first	O
loss	O
minimization	O
term	O
in	O
and	O
,	O
since	O
the	O
role	O
of	O
the	O
first	O
term	O
would	O
vanish	O
as	O
goes	O
to	O
.	O

However	O
,	O
even	O
if	O
it	O
is	O
involved	O
,	O
the	O
following	O
proof	O
still	O
holds	O
with	O
only	O
some	O
minor	O
changes	O
.	O

To	O
prove	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
need	O
the	O
following	O
lemma	O
.	O

theorem	O
:	O
.	O

For	O
all	O
loss	O
functions	O
Lθ	O
,	O
with	O
at	O
least	O
the	O
probability	O
of	O
-	O
1η	O
,	O
we	O
have	O
when	O
the	O
number	O
of	O
samples	O
with	O
a	O
sufficiently	O
large	O
constant	O
C.	O
The	O
proof	O
of	O
this	O
lemma	O
needs	O
to	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
and	O
the	O
fact	O
that	O
is	O
an	O
1	O
-	O
Lipschitz	O
to	O
bound	O
the	O
difference	O
for	O
a	O
loss	O
function	O
.	O

Then	O
,	O
to	O
get	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
,	O
a	O
standard	O
-	O
net	O
will	O
be	O
constructed	O
to	O
yield	O
finite	O
points	O
that	O
are	O
dense	O
enough	O
to	O
cover	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
functions	O
.	O

The	O
proof	O
details	O
are	O
given	O
below	O
.	O

proof	O
:	O
Proof	O
.	O

For	O
a	O
loss	O
function	O
,	O
we	O
compute	O
over	O
a	O
set	O
of	O
samples	O
drawn	O
from	O
and	O
respectively	O
.	O

To	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
,	O
we	O
need	O
to	O
bound	O
the	O
change	O
of	O
this	O
function	O
when	O
a	O
sample	O
is	O
changed	O
.	O

Denote	O
by	O
when	O
the	O
th	O
sample	O
is	O
replaced	O
with	O
and	O
.	O

Then	O
we	O
have	O
where	O
the	O
first	O
inequality	O
uses	O
the	O
fact	O
that	O
is	O
-	O
Lipschitz	O
,	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
bounded	O
by	O
and	O
is	O
-	O
Lipschitz	O
in	O
.	O

Now	O
we	O
can	O
apply	O
the	O
McDiarmid	O
’s	O
inequality	O
.	O

Noting	O
that	O
we	O
have	O
The	O
above	O
bound	O
applies	O
to	O
a	O
single	O
loss	O
function	O
.	O

To	O
get	O
the	O
union	O
bound	O
,	O
we	O
consider	O
a	O
-	O
net	O
,	O
i.e.	O
,	O
for	O
any	O
,	O
there	O
is	O
a	O
in	O
this	O
net	O
so	O
that	O
.	O

This	O
standard	O
net	O
can	O
be	O
constructed	O
to	O
contain	O
finite	O
loss	O
functions	O
such	O
that	O
,	O
where	O
is	O
the	O
number	O
of	O
parameters	O
in	O
a	O
loss	O
function	O
.	O

Note	O
that	O
we	O
implicitly	O
assume	O
the	O
parameter	O
space	O
of	O
the	O
loss	O
function	O
is	O
bounded	O
so	O
we	O
can	O
construct	O
such	O
a	O
net	O
containing	O
finite	O
points	O
here	O
.	O

Therefore	O
,	O
we	O
have	O
the	O
following	O
union	O
bound	O
for	O
all	O
that	O
,	O
with	O
probability	O
,	O
when	O
.	O

The	O
last	O
step	O
is	O
to	O
obtain	O
the	O
union	O
bound	O
for	O
all	O
loss	O
functions	O
beyond	O
.	O

To	O
show	O
that	O
,	O
we	O
consider	O
the	O
following	O
inequality	O
where	O
the	O
first	O
inequality	O
uses	O
that	O
fact	O
that	O
is	O
-	O
Lipschitz	O
again	O
,	O
and	O
the	O
second	O
inequality	O
follows	O
from	O
that	O
is	O
-	O
Lipschitz	O
in	O
.	O

Similarly	O
,	O
we	O
can	O
also	O
show	O
that	O
Now	O
we	O
can	O
derive	O
the	O
union	O
bound	O
over	O
all	O
loss	O
functions	O
.	O

For	O
any	O
,	O
by	O
construction	O
we	O
can	O
find	O
a	O
such	O
that	O
.	O

Then	O
,	O
with	O
probability	O
,	O
we	O
have	O
This	O
proves	O
the	O
lemma	O
.	O

∎	O
Now	O
we	O
can	O
prove	O
Theorem	O
[	O
reference	O
]	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
let	O
us	O
bound	O
.	O

Consider	O
that	O
minimizes	O
.	O

Then	O
with	O
probability	O
,	O
when	O
,	O
we	O
have	O
where	O
the	O
first	O
inequality	O
follows	O
from	O
the	O
inequality	O
as	O
may	O
not	O
minimize	O
,	O
and	O
the	O
second	O
inequality	O
is	O
a	O
direct	O
application	O
of	O
the	O
above	O
lemma	O
.	O

Similarly	O
,	O
we	O
can	O
prove	O
the	O
other	O
direction	O
.	O

With	O
probability	O
,	O
we	O
have	O
Finally	O
,	O
a	O
more	O
rigourous	O
discussion	O
about	O
the	O
generalizability	O
should	O
consider	O
that	O
is	O
updated	O
iteratively	O
.	O

Therefore	O
we	O
have	O
a	O
sequence	O
of	O
generated	O
over	O
iterations	O
for	O
.	O

Thus	O
,	O
a	O
union	O
bound	O
over	O
all	O
generators	O
should	O
be	O
considered	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
this	O
makes	O
the	O
required	O
number	O
of	O
training	O
examples	O
become	O
However	O
,	O
the	O
iteration	O
number	O
is	O
usually	O
much	O
smaller	O
than	O
the	O
model	O
size	O
(	O
which	O
is	O
often	O
hundreds	O
of	O
thousands	O
)	O
,	O
and	O
thus	O
this	O
factor	O
will	O
not	O
affect	O
the	O
above	O
lower	O
bound	O
of	O
.	O

∎	O
appendix	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
and	O
Corollary	O
[	O
reference	O
]	O
We	O
prove	O
Theorem	O
[	O
reference	O
]	O
as	O
follows	O
.	O

proof	O
:	O
Proof	O
.	O

First	O
,	O
the	O
existence	O
of	O
a	O
minimizer	O
follows	O
from	O
the	O
fact	O
that	O
the	O
functions	O
in	O
form	O
a	O
compact	O
set	O
,	O
and	O
the	O
objective	O
function	O
is	O
convex	O
.	O

To	O
prove	O
the	O
minimizer	O
has	O
the	O
two	O
forms	O
in	O
(	O
[	O
reference	O
]	O
)	O
,	O
for	O
each	O
,	O
let	O
us	O
consider	O
It	O
is	O
not	O
hard	O
to	O
verify	O
that	O
and	O
for	O
.	O

Indeed	O
,	O
by	O
noting	O
that	O
has	O
its	O
Lipschitz	O
constant	O
bounded	O
by	O
,	O
we	O
have	O
,	O
and	O
thus	O
Because	O
by	O
the	O
assumption	O
(	O
i.e.	O
,	O
it	O
is	O
lower	O
bounded	O
by	O
zero	O
)	O
,	O
it	O
can	O
be	O
shown	O
that	O
for	O
all	O
Hence	O
,	O
by	O
the	O
definition	O
of	O
and	O
taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
because	O
for	O
any	O
,	O
and	O
it	O
is	O
true	O
in	O
particular	O
for	O
.	O

This	O
shows	O
.	O

Similarly	O
,	O
one	O
can	O
prove	O
.	O

To	O
show	O
this	O
,	O
we	O
have	O
by	O
the	O
Lipschitz	O
continuity	O
of	O
.	O

By	O
taking	O
the	O
minimum	O
over	O
,	O
we	O
have	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
by	O
the	O
definition	O
of	O
.	O

Combining	O
these	O
two	O
inequalities	O
shows	O
that	O
.	O

Now	O
we	O
can	O
prove	O
for	O
any	O
function	O
,	O
there	O
exist	O
and	O
both	O
of	O
which	O
attain	O
the	O
same	O
value	O
of	O
as	O
,	O
since	O
only	O
depends	O
on	O
the	O
values	O
of	O
on	O
the	O
data	O
points	O
.	O

In	O
particular	O
,	O
this	O
shows	O
that	O
any	O
global	O
minimum	O
in	O
of	O
can	O
also	O
be	O
attained	O
by	O
the	O
corresponding	O
functions	O
of	O
the	O
form	O
(	O
[	O
reference	O
]	O
)	O
.	O

By	O
setting	O
for	O
,	O
this	O
completes	O
the	O
proof	O
.	O

∎	O
Finally	O
,	O
we	O
prove	O
Corollary	O
[	O
reference	O
]	O
that	O
bounds	O
with	O
and	O
constructed	O
above	O
.	O

proof	O
:	O
Proof	O
.	O

By	O
the	O
Lipschitz	O
continuity	O
,	O
we	O
have	O
Since	O
,	O
it	O
follows	O
that	O
Taking	O
the	O
maximum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
we	O
obtain	O
This	O
proves	O
the	O
lower	O
bound	O
.	O

Similarly	O
,	O
we	O
have	O
by	O
Lipschitz	O
continuity	O
which	O
,	O
by	O
taking	O
the	O
minimum	O
over	O
on	O
the	O
left	O
hand	O
side	O
,	O
leads	O
to	O
This	O
shows	O
the	O
upper	O
bound	O
.	O

∎	O
