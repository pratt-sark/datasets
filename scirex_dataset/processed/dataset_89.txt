document	O
:	O
Globally	Method
Normalized	Method
Transition	Method
-	Method
Based	Method
Neural	Method
Networks	Method
We	O
introduce	O
a	O
globally	Method
normalized	Method
transition	Method
-	Method
based	Method
neural	Method
network	Method
model	Method
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
,	O
dependency	Task
parsing	Task
and	O
sentence	Task
compression	Task
results	O
.	O

Our	O
model	O
is	O
a	O
simple	O
feed	Method
-	Method
forward	Method
neural	Method
network	Method
that	O
operates	O
on	O
a	O
task	Method
-	Method
specific	Method
transition	Method
system	Method
,	O
yet	O
achieves	O
comparable	O
or	O
better	O
accuracies	Metric
than	O
recurrent	Method
models	Method
.	O

We	O
discuss	O
the	O
importance	O
of	O
global	O
as	O
opposed	O
to	O
local	Method
normalization	Method
:	O
a	O
key	O
insight	O
is	O
that	O
the	O
label	Task
bias	Task
problem	Task
implies	O
that	O
globally	Method
normalized	Method
models	Method
can	O
be	O
strictly	O
more	O
expressive	O
than	O
locally	Method
normalized	Method
models	Method
.	O

section	O
:	O
Introduction	O
Neural	Method
network	Method
approaches	Method
have	O
taken	O
the	O
field	O
of	O
natural	Task
language	Task
processing	Task
(	O
NLP	Task
)	O
by	O
storm	O
.	O

In	O
particular	O
,	O
variants	O
of	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
networks	O
have	O
produced	O
impressive	O
results	O
on	O
some	O
of	O
the	O
classic	O
NLP	Task
tasks	O
such	O
as	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
,	O
syntactic	Task
parsing	Task
and	O
semantic	Task
role	Task
labeling	Task
.	O

One	O
might	O
speculate	O
that	O
it	O
is	O
the	O
recurrent	O
nature	O
of	O
these	O
models	O
that	O
enables	O
these	O
results	O
.	O

In	O
this	O
work	O
we	O
demonstrate	O
that	O
simple	O
feed	Method
-	Method
forward	Method
networks	Method
without	O
any	O
recurrence	Method
can	O
achieve	O
comparable	O
or	O
better	O
accuracies	Metric
than	O
LSTMs	Method
,	O
as	O
long	O
as	O
they	O
are	O
globally	O
normalized	O
.	O

Our	O
model	O
,	O
described	O
in	O
detail	O
in	O
Section	O
[	O
reference	O
]	O
,	O
uses	O
a	O
transition	Method
system	Method
and	O
feature	Method
embeddings	Method
as	O
introduced	O
by	O
chen	O
-	O
manning:2014:EMNLP	O
.	O

We	O
do	O
not	O
use	O
any	O
recurrence	Method
,	O
but	O
perform	O
beam	Method
search	Method
for	O
maintaining	O
multiple	O
hypotheses	O
and	O
introduce	O
global	Method
normalization	Method
with	O
a	O
conditional	Method
random	Method
field	Method
(	O
CRF	Method
)	O
objective	O
to	O
overcome	O
the	O
label	Task
bias	Task
problem	Task
that	O
locally	Method
normalized	Method
models	Method
suffer	O
from	O
.	O

Since	O
we	O
use	O
beam	Method
inference	Method
,	O
we	O
approximate	O
the	O
partition	O
function	O
by	O
summing	O
over	O
the	O
elements	O
in	O
the	O
beam	O
,	O
and	O
use	O
early	Method
updates	Method
.	O

We	O
compute	O
gradients	O
based	O
on	O
this	O
approximate	Method
global	Method
normalization	Method
and	O
perform	O
full	Method
backpropagation	Method
training	Method
of	O
all	O
neural	Method
network	Method
parameters	Method
based	O
on	O
the	O
CRF	Method
loss	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
revisit	O
the	O
label	Task
bias	Task
problem	Task
and	O
the	O
implication	O
that	O
globally	Method
normalized	Method
models	Method
are	O
strictly	O
more	O
expressive	O
than	O
locally	Method
normalized	Method
models	Method
.	O

Lookahead	O
features	O
can	O
partially	O
mitigate	O
this	O
discrepancy	O
,	O
but	O
can	O
not	O
fully	O
compensate	O
for	O
it	O
—	O
a	O
point	O
to	O
which	O
we	O
return	O
later	O
.	O

To	O
empirically	O
demonstrate	O
the	O
effectiveness	O
of	O
global	Method
normalization	Method
,	O
we	O
evaluate	O
our	O
model	O
on	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
,	O
syntactic	O
dependency	Task
parsing	Task
and	O
sentence	Task
compression	Task
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Our	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
on	O
all	O
of	O
these	O
tasks	O
,	O
matching	O
or	O
outperforming	O
LSTMs	Method
while	O
being	O
significantly	O
faster	O
.	O

In	O
particular	O
for	O
dependency	Task
parsing	Task
on	O
the	O
Wall	O
Street	O
Journal	O
we	O
achieve	O
the	O
best	O
-	O
ever	O
published	O
unlabeled	Metric
attachment	Metric
score	Metric
of	O
94.61	O
%	O
.	O

As	O
discussed	O
in	O
more	O
detail	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
also	O
outperform	O
previous	O
structured	Method
training	Method
approaches	Method
used	O
for	O
neural	Method
network	Method
transition	Method
-	Method
based	Method
parsing	Method
.	O

Our	O
ablation	O
experiments	O
show	O
that	O
we	O
outperform	O
weiss	O
-	O
etAl:2015:ACL	O
and	O
alberti	O
-	O
EtAl:2015:EMNLP	O
because	O
we	O
do	O
global	Method
backpropagation	Method
training	Method
of	O
all	O
model	O
parameters	O
,	O
while	O
they	O
fix	O
the	O
neural	Method
network	Method
parameters	Method
when	O
training	O
the	O
global	O
part	O
of	O
their	O
model	O
.	O

We	O
also	O
outperform	O
zhou	O
-	O
etAl:2015:ACL	O
despite	O
using	O
a	O
smaller	O
beam	O
.	O

To	O
shed	O
additional	O
light	O
on	O
the	O
label	Task
bias	Task
problem	Task
in	O
practice	O
,	O
we	O
provide	O
a	O
sentence	O
compression	O
example	O
where	O
the	O
local	Method
model	Method
completely	O
fails	O
.	O

We	O
then	O
demonstrate	O
that	O
a	O
globally	Method
normalized	Method
parsing	Method
model	Method
without	O
any	O
lookahead	O
features	O
is	O
almost	O
as	O
accurate	O
as	O
our	O
best	O
model	O
,	O
while	O
a	O
locally	Method
normalized	Method
model	Method
loses	O
more	O
than	O
10	O
%	O
absolute	O
in	O
accuracy	Metric
because	O
it	O
can	O
not	O
effectively	O
incorporate	O
evidence	O
as	O
it	O
becomes	O
available	O
.	O

Finally	O
,	O
we	O
provide	O
an	O
open	O
-	O
source	O
implementation	O
of	O
our	O
method	O
,	O
called	O
SyntaxNet	Method
,	O
which	O
we	O
have	O
integrated	O
into	O
the	O
popular	O
TensorFlow	Method
framework	Method
.	O

We	O
also	O
provide	O
a	O
pre	O
-	O
trained	O
,	O
state	O
-	O
of	O
-	O
the	O
art	O
English	Method
dependency	Method
parser	Method
called	O
“	O
Parsey	Method
McParseface	Method
,	O
”	O
which	O
we	O
tuned	O
for	O
a	O
balance	O
of	O
speed	Metric
,	O
simplicity	Metric
,	O
and	O
accuracy	Metric
.	O

section	O
:	O
Model	O
At	O
its	O
core	O
,	O
our	O
model	O
is	O
an	O
incremental	Method
transition	Method
-	Method
based	Method
parser	Method
.	O

To	O
apply	O
it	O
to	O
different	O
tasks	O
we	O
only	O
need	O
to	O
adjust	O
the	O
transition	Method
system	Method
and	O
the	O
input	O
features	O
.	O

subsection	O
:	O
Transition	Method
System	Method
Given	O
an	O
input	O
,	O
most	O
often	O
a	O
sentence	O
,	O
we	O
define	O
:	O
A	O
set	O
of	O
states	O
.	O

A	O
special	O
start	O
state	O
.	O

A	O
set	O
of	O
allowed	O
decisions	O
for	O
all	O
.	O

A	O
transition	O
function	O
returning	O
a	O
new	O
state	O
for	O
any	O
decision	O
.	O

We	O
will	O
use	O
a	O
function	O
to	O
compute	O
the	O
score	O
of	O
decision	O
in	O
state	O
for	O
input	O
.	O

The	O
vector	O
contains	O
the	O
model	O
parameters	O
and	O
we	O
assume	O
that	O
is	O
differentiable	O
with	O
respect	O
to	O
.	O

In	O
this	O
section	O
,	O
for	O
brevity	O
,	O
we	O
will	O
drop	O
the	O
dependence	O
of	O
in	O
the	O
functions	O
given	O
above	O
,	O
simply	O
writing	O
,	O
,	O
,	O
and	O
.	O

Throughout	O
this	O
work	O
we	O
will	O
use	O
transition	Method
systems	Method
in	O
which	O
all	O
complete	O
structures	O
for	O
the	O
same	O
input	O
have	O
the	O
same	O
number	O
of	O
decisions	O
(	O
or	O
for	O
brevity	O
)	O
.	O

In	O
dependency	Task
parsing	Task
for	O
example	O
,	O
this	O
is	O
true	O
for	O
both	O
the	O
arc	Method
-	Method
standard	Method
and	O
arc	Method
-	Method
eager	Method
transition	Method
systems	Method
,	O
where	O
for	O
a	O
sentence	O
of	O
length	O
,	O
the	O
number	O
of	O
decisions	O
for	O
any	O
complete	O
parse	O
is	O
.	O

A	O
complete	O
structure	O
is	O
then	O
a	O
sequence	O
of	O
decision	O
/	O
state	O
pairs	O
such	O
that	O
,	O
for	O
,	O
and	O
.	O

We	O
use	O
the	O
notation	O
to	O
refer	O
to	O
a	O
decision	O
sequence	O
.	O

We	O
assume	O
that	O
there	O
is	O
a	O
one	O
-	O
to	O
-	O
one	O
mapping	O
between	O
decision	O
sequences	O
and	O
states	O
:	O
that	O
is	O
,	O
we	O
essentially	O
assume	O
that	O
a	O
state	O
encodes	O
the	O
entire	O
history	O
of	O
decisions	O
.	O

Thus	O
,	O
each	O
state	O
can	O
be	O
reached	O
by	O
a	O
unique	O
decision	O
sequence	O
from	O
.	O

We	O
will	O
use	O
decision	O
sequences	O
and	O
states	O
interchangeably	O
:	O
in	O
a	O
slight	O
abuse	O
of	O
notation	O
,	O
we	O
define	O
to	O
be	O
equal	O
to	O
where	O
is	O
the	O
state	O
reached	O
by	O
the	O
decision	O
sequence	O
.	O

The	O
scoring	Metric
function	Metric
can	O
be	O
defined	O
in	O
a	O
number	O
of	O
ways	O
.	O

In	O
this	O
work	O
,	O
following	O
chen	O
-	O
manning:2014:EMNLP	O
,	O
weiss	O
-	O
etAl:2015:ACL	O
,	O
and	O
zhou	O
-	O
etAl:2015:ACL	O
,	O
we	O
define	O
it	O
via	O
a	O
feed	Method
-	Method
forward	Method
neural	Method
network	Method
as	O
Here	O
are	O
the	O
parameters	O
of	O
the	O
neural	Method
network	Method
,	O
excluding	O
the	O
parameters	O
at	O
the	O
final	O
layer	O
.	O

are	O
the	O
final	O
layer	O
parameters	O
for	O
decision	Task
.	O

is	O
the	O
representation	O
for	O
state	O
computed	O
by	O
the	O
neural	Method
network	Method
under	O
parameters	O
.	O

Note	O
that	O
the	O
score	O
is	O
linear	O
in	O
the	O
parameters	O
.	O

We	O
next	O
describe	O
how	O
softmax	Method
-	Method
style	Method
normalization	Method
can	O
be	O
performed	O
at	O
the	O
local	O
or	O
global	O
level	O
.	O

subsection	O
:	O
Global	O
vs.	O
Local	Method
Normalization	Method
In	O
the	O
chen	O
-	O
manning:2014:EMNLP	O
style	O
of	O
greedy	Method
neural	Method
network	Method
parsing	Method
,	O
the	O
conditional	O
probability	O
distribution	O
over	O
decisions	O
given	O
context	O
is	O
defined	O
as	O
where	O
Each	O
is	O
a	O
local	O
normalization	O
term	O
.	O

The	O
probability	O
of	O
a	O
sequence	O
of	O
decisions	O
is	O
Beam	Method
search	Method
can	O
be	O
used	O
to	O
attempt	O
to	O
find	O
the	O
maximum	O
of	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
with	O
respect	O
to	O
.	O

The	O
additive	O
scores	O
used	O
in	O
beam	Method
search	Method
are	O
the	O
log	O
-	O
softmax	O
of	O
each	O
decision	O
,	O
,	O
not	O
the	O
raw	O
scores	O
.	O

In	O
contrast	O
,	O
a	O
Conditional	Method
Random	Method
Field	Method
(	O
CRF	Method
)	O
defines	O
a	O
distribution	O
as	O
follows	O
:	O
where	O
and	O
is	O
the	O
set	O
of	O
all	O
valid	O
sequences	O
of	O
decisions	O
of	O
length	O
.	O

is	O
a	O
global	O
normalization	O
term	O
.	O

The	O
inference	Task
problem	Task
is	O
now	O
to	O
find	O
Beam	Task
search	Task
can	O
again	O
be	O
used	O
to	O
approximately	O
find	O
the	O
.	O

subsection	O
:	O
Training	O
Training	O
data	O
consists	O
of	O
inputs	O
paired	O
with	O
gold	O
decision	O
sequences	O
.	O

We	O
use	O
stochastic	Method
gradient	Method
descent	Method
on	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
the	O
data	O
under	O
the	O
model	O
.	O

Under	O
a	O
locally	Method
normalized	Method
model	Method
,	O
the	O
negative	O
log	O
-	O
likelihood	O
is	O
whereas	O
under	O
a	O
globally	Method
normalized	Method
model	Method
it	O
is	O
A	O
significant	O
practical	O
advantange	O
of	O
the	O
locally	Method
normalized	Method
cost	Method
Eq	Method
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
that	O
the	O
local	O
partition	O
function	O
and	O
its	O
derivative	O
can	O
usually	O
be	O
computed	O
efficiently	O
.	O

In	O
contrast	O
,	O
the	O
term	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
contains	O
a	O
sum	O
over	O
that	O
is	O
in	O
many	O
cases	O
intractable	O
.	O

To	O
make	O
learning	Task
tractable	O
with	O
the	O
globally	Method
normalized	Method
model	Method
,	O
we	O
use	O
beam	Method
search	Method
and	O
early	Method
updates	Method
.	O

As	O
the	O
training	O
sequence	O
is	O
being	O
decoded	O
,	O
we	O
keep	O
track	O
of	O
the	O
location	O
of	O
the	O
gold	O
path	O
in	O
the	O
beam	O
.	O

If	O
the	O
gold	O
path	O
falls	O
out	O
of	O
the	O
beam	O
at	O
step	O
,	O
a	O
stochastic	Method
gradient	Method
step	Method
is	O
taken	O
on	O
the	O
following	O
objective	O
:	O
Here	O
the	O
set	O
contains	O
all	O
paths	O
in	O
the	O
beam	O
at	O
step	O
,	O
together	O
with	O
the	O
gold	O
path	O
prefix	O
.	O

It	O
is	O
straightforward	O
to	O
derive	O
gradients	O
of	O
the	O
loss	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
and	O
to	O
back	O
-	O
propagate	O
gradients	O
to	O
all	O
levels	O
of	O
a	O
neural	Method
network	Method
defining	O
the	O
score	O
.	O

If	O
the	O
gold	O
path	O
remains	O
in	O
the	O
beam	O
throughout	O
decoding	Task
,	O
a	O
gradient	Method
step	Method
is	O
performed	O
using	O
,	O
the	O
beam	O
at	O
the	O
end	O
of	O
decoding	Task
.	O

section	O
:	O
The	O
Label	Task
Bias	Task
Problem	Task
Intuitively	O
,	O
we	O
would	O
like	O
the	O
model	O
to	O
be	O
able	O
to	O
revise	O
an	O
earlier	O
decision	O
made	O
during	O
search	O
,	O
when	O
later	O
evidence	O
becomes	O
available	O
that	O
rules	O
out	O
the	O
earlier	O
decision	O
as	O
incorrect	O
.	O

At	O
first	O
glance	O
,	O
it	O
might	O
appear	O
that	O
a	O
locally	Method
normalized	Method
model	Method
used	O
in	O
conjunction	O
with	O
beam	Method
search	Method
or	O
exact	Method
search	Method
is	O
able	O
to	O
revise	O
earlier	O
decisions	O
.	O

However	O
the	O
label	Task
bias	Task
problem	Task
(	O
see	O
bottou	O
,	O
collins99	O
pages	O
222	O
-	O
226	O
,	O
crf	O
,	O
bottou	O
-	O
lecun	O
-	O
2005	O
,	O
smithJohnson07	O
)	O
means	O
that	O
locally	Method
normalized	Method
models	Method
often	O
have	O
a	O
very	O
weak	O
ability	O
to	O
revise	O
earlier	O
decisions	O
.	O

This	O
section	O
gives	O
a	O
formal	O
perspective	O
on	O
the	O
label	Task
bias	Task
problem	Task
,	O
through	O
a	O
proof	O
that	O
globally	Method
normalized	Method
models	Method
are	O
strictly	O
more	O
expressive	O
than	O
locally	Method
normalized	Method
models	Method
.	O

The	O
theorem	O
was	O
originally	O
proved	O
by	O
smithJohnson07	O
.	O

The	O
example	O
underlying	O
the	O
proof	O
gives	O
a	O
clear	O
illustration	O
of	O
the	O
label	Task
bias	Task
problem	Task
.	O

paragraph	O
:	O
Global	Method
Models	Method
can	O
be	O
Strictly	O
More	O
Expressive	O
than	O
Local	Method
Models	Method
Consider	O
a	O
tagging	Task
problem	Task
where	O
the	O
task	O
is	O
to	O
map	O
an	O
input	O
sequence	O
to	O
a	O
decision	O
sequence	O
.	O

First	O
,	O
consider	O
a	O
locally	Method
normalized	Method
model	Method
where	O
we	O
restrict	O
the	O
scoring	O
function	O
to	O
access	O
only	O
the	O
first	O
input	O
symbols	O
when	O
scoring	O
decision	O
.	O

We	O
will	O
return	O
to	O
this	O
restriction	O
soon	O
.	O

The	O
scoring	O
function	O
can	O
be	O
an	O
otherwise	O
arbitrary	O
function	O
of	O
the	O
tuple	O
:	O
Second	O
,	O
consider	O
a	O
globally	Method
normalized	Method
model	Method
This	O
model	O
again	O
makes	O
use	O
of	O
a	O
scoring	O
function	O
restricted	O
to	O
the	O
first	O
input	O
symbols	O
when	O
scoring	O
decision	O
.	O

Define	O
to	O
be	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
under	O
the	O
local	Method
model	Method
obtained	O
as	O
the	O
scores	O
vary	O
.	O

Similarly	O
,	O
define	O
to	O
be	O
the	O
set	O
of	O
all	O
possible	O
distributions	O
under	O
the	O
global	Method
model	Method
.	O

Here	O
a	O
“	O
distribution	O
”	O
is	O
a	O
function	O
from	O
a	O
pair	O
to	O
a	O
probability	O
.	O

Our	O
main	O
result	O
is	O
the	O
following	O
:	O
theorem	O
:	O
See	O
also	O
PL	O
is	O
a	O
strict	O
subset	O
of	O
PG	Method
,	O
that	O
is	O
⊊PLPG	O
.	O

To	O
prove	O
this	O
we	O
will	O
first	O
prove	O
that	O
.	O

This	O
step	O
is	O
straightforward	O
.	O

We	O
then	O
show	O
that	O
;	O
that	O
is	O
,	O
there	O
are	O
distributions	O
in	O
that	O
are	O
not	O
in	O
.	O

The	O
proof	O
that	O
gives	O
a	O
clear	O
illustration	O
of	O
the	O
label	Task
bias	Task
problem	Task
.	O

Proof	O
that	O
⊆PLPG	O
:	O
We	O
need	O
to	O
show	O
that	O
for	O
any	O
locally	O
normalized	O
distribution	O
,	O
we	O
can	O
construct	O
a	O
globally	Method
normalized	Method
model	Method
such	O
that	O
.	O

Consider	O
a	O
locally	Method
normalized	Method
model	Method
with	O
scores	O
.	O

Define	O
a	O
global	Method
model	Method
with	O
scores	O
Then	O
it	O
is	O
easily	O
verified	O
that	O
for	O
all	O
.	O

In	O
proving	O
we	O
will	O
use	O
a	O
simple	O
problem	O
where	O
every	O
example	O
seen	O
in	O
training	O
or	O
test	O
data	O
is	O
one	O
of	O
the	O
following	O
two	O
tagged	O
sentences	O
:	O
Note	O
that	O
the	O
input	O
is	O
ambiguous	O
:	O
it	O
can	O
take	O
tags	O
B	O
or	O
D	O
.	O

This	O
ambiguity	O
is	O
resolved	O
when	O
the	O
next	O
input	O
symbol	O
,	O
c	O
or	O
e	O
,	O
is	O
observed	O
.	O

Now	O
consider	O
a	O
globally	Method
normalized	Method
model	Method
,	O
where	O
the	O
scores	O
are	O
defined	O
as	O
follows	O
.	O

Define	O
as	O
the	O
set	O
of	O
bigram	O
tag	O
transitions	O
seen	O
in	O
the	O
data	O
.	O

Similarly	O
,	O
define	O
as	O
the	O
set	O
of	O
(	O
word	O
,	O
tag	O
)	O
pairs	O
seen	O
in	O
the	O
data	O
.	O

We	O
define	O
where	O
is	O
the	O
single	O
scalar	O
parameter	O
of	O
the	O
model	O
,	O
and	O
if	O
is	O
true	O
,	O
otherwise	O
.	O

Proof	O
that	O
⊈PGPL	O
:	O
We	O
will	O
construct	O
a	O
globally	Method
normalized	Method
model	Method
such	O
that	O
there	O
is	O
no	O
locally	Method
normalized	Method
model	Method
such	O
that	O
.	O

Under	O
the	O
definition	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
In	O
contrast	O
,	O
under	O
any	O
definition	O
for	O
,	O
we	O
must	O
have	O
This	O
follows	O
because	O
and	O
.	O

The	O
inequality	O
then	O
immediately	O
implies	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

It	O
follows	O
that	O
for	O
sufficiently	O
large	O
values	O
of	O
,	O
we	O
have	O
,	O
and	O
given	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
it	O
is	O
impossible	O
to	O
define	O
a	O
locally	Method
normalized	Method
model	Method
with	O
and	O
.	O

Under	O
the	O
restriction	O
that	O
scores	O
depend	O
only	O
on	O
the	O
first	O
input	O
symbols	O
,	O
the	O
globally	Method
normalized	Method
model	Method
is	O
still	O
able	O
to	O
model	O
the	O
data	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
while	O
the	O
locally	Method
normalized	Method
model	Method
fails	O
(	O
see	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
ambiguity	O
at	O
input	O
symbol	O
b	O
is	O
naturally	O
resolved	O
when	O
the	O
next	O
symbol	O
(	O
c	O
or	O
e	O
)	O
is	O
observed	O
,	O
but	O
the	O
locally	Method
normalized	Method
model	Method
is	O
not	O
able	O
to	O
revise	O
its	O
prediction	O
.	O

It	O
is	O
easy	O
to	O
fix	O
the	O
locally	Method
normalized	Method
model	Method
for	O
the	O
example	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
by	O
allowing	O
scores	O
that	O
take	O
into	O
account	O
the	O
input	O
symbol	O
.	O

More	O
generally	O
we	O
can	O
have	O
a	O
model	O
of	O
the	O
form	O
where	O
the	O
integer	O
specifies	O
the	O
amount	O
of	O
lookahead	O
in	O
the	O
model	O
.	O

Such	O
lookahead	Method
is	O
common	O
in	O
practice	O
,	O
but	O
insufficient	O
in	O
general	O
.	O

For	O
every	O
amount	O
of	O
lookahead	O
,	O
we	O
can	O
construct	O
examples	O
that	O
can	O
not	O
be	O
modeled	O
with	O
a	O
locally	Method
normalized	Method
model	Method
by	O
duplicating	O
the	O
middle	O
input	O
b	O
in	O
(	O
[	O
reference	O
]	O
)	O
times	O
.	O

Only	O
a	O
local	Method
model	Method
with	O
scores	O
that	O
considers	O
the	O
entire	O
input	O
can	O
capture	O
any	O
distribution	O
:	O
in	O
this	O
case	O
the	O
decomposition	O
makes	O
no	O
independence	O
assumptions	O
.	O

However	O
,	O
increasing	O
the	O
amount	O
of	O
context	O
used	O
as	O
input	O
comes	O
at	O
a	O
cost	O
,	O
requiring	O
more	O
powerful	O
learning	Method
algorithms	Method
,	O
and	O
potentially	O
more	O
training	O
data	O
.	O

For	O
a	O
detailed	O
analysis	O
of	O
the	O
trade	O
-	O
offs	O
between	O
structural	O
features	O
in	O
CRFs	Method
and	O
more	O
powerful	O
local	Method
classifiers	Method
without	O
structural	O
constraints	O
,	O
see	O
liang08structure	O
;	O
in	O
these	O
experiments	O
local	Method
classifiers	Method
are	O
unable	O
to	O
reach	O
the	O
performance	O
of	O
CRFs	Method
on	O
problems	O
such	O
as	O
parsing	Task
and	O
named	Task
entity	Task
recognition	Task
where	O
structural	O
constraints	O
are	O
important	O
.	O

Note	O
that	O
there	O
is	O
nothing	O
to	O
preclude	O
an	O
approach	O
that	O
makes	O
use	O
of	O
both	O
global	Method
normalization	Method
and	O
more	O
powerful	O
scoring	Method
functions	Method
,	O
obtaining	O
the	O
best	O
of	O
both	O
worlds	O
.	O

The	O
experiments	O
that	O
follow	O
make	O
use	O
of	O
both	O
.	O

section	O
:	O
Experiments	O
To	O
demonstrate	O
the	O
flexibility	O
and	O
modeling	O
power	O
of	O
our	O
approach	O
,	O
we	O
provide	O
experimental	O
results	O
on	O
a	O
diverse	O
set	O
of	O
structured	Task
prediction	Task
tasks	Task
.	O

We	O
apply	O
our	O
approach	O
to	O
POS	Method
tagging	Method
,	O
syntactic	O
dependency	Task
parsing	Task
,	O
and	O
sentence	Task
compression	Task
.	O

While	O
directly	O
optimizing	O
the	O
global	Method
model	Method
defined	O
by	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
works	O
well	O
,	O
we	O
found	O
that	O
training	O
the	O
model	O
in	O
two	O
steps	O
achieves	O
the	O
same	O
precision	Metric
much	O
faster	O
:	O
we	O
first	O
pretrain	O
the	O
network	O
using	O
the	O
local	O
objective	O
given	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
and	O
then	O
perform	O
additional	O
training	O
steps	O
using	O
the	O
global	O
objective	O
given	O
in	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

We	O
pretrain	O
all	O
layers	O
except	O
the	O
softmax	O
layer	O
in	O
this	O
way	O
.	O

We	O
purposefully	O
abstain	O
from	O
complicated	O
hand	O
engineering	O
of	O
input	O
features	O
,	O
which	O
might	O
improve	O
performance	O
further	O
.	O

We	O
use	O
the	O
training	O
recipe	O
from	O
weiss	O
-	O
etAl:2015:ACL	O
for	O
each	O
training	O
stage	O
of	O
our	O
model	O
.	O

Specifically	O
,	O
we	O
use	O
averaged	Method
stochastic	Method
gradient	Method
descent	Method
with	O
momentum	Method
,	O
and	O
we	O
tune	O
the	O
learning	Metric
rate	Metric
,	O
learning	Metric
rate	Metric
schedule	Metric
,	O
momentum	O
,	O
and	O
early	Metric
stopping	Metric
time	Metric
using	O
a	O
separate	O
held	O
-	O
out	O
corpus	O
for	O
each	O
task	O
.	O

We	O
tune	O
again	O
with	O
a	O
different	O
set	O
of	O
hyperparameters	O
for	O
training	Task
with	O
the	O
global	Metric
objective	Metric
.	O

subsection	O
:	O
Part	Task
of	Task
Speech	Task
Tagging	Task
Part	Task
of	Task
speech	Task
(	O
POS	Task
)	O
tagging	O
is	O
a	O
classic	O
NLP	Task
task	O
,	O
where	O
modeling	O
the	O
structure	O
of	O
the	O
output	O
is	O
important	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

paragraph	O
:	O
Data	O
&	O
Evaluation	O
.	O

We	O
conducted	O
experiments	O
on	O
a	O
number	O
of	O
different	O
datasets	O
:	O
(	O
1	O
)	O
the	O
English	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
part	O
of	O
the	O
Penn	Material
Treebank	Material
with	O
standard	O
POS	Task
tagging	O
splits	O
;	O
(	O
2	O
)	O
the	O
English	O
“	O
Treebank	O
Union	O
”	O
multi	O
-	O
domain	O
corpus	O
containing	O
data	O
from	O
the	O
OntoNotes	O
corpus	O
version	O
5	O
,	O
the	O
English	O
Web	O
Treebank	O
,	O
and	O
the	O
updated	O
and	O
corrected	O
Question	O
Treebank	O
with	O
identical	O
setup	O
to	O
weiss	O
-	O
etAl:2015:ACL	O
;	O
and	O
(	O
3	O
)	O
the	O
CoNLL	O
’	O
09	O
multi	O
-	O
lingual	O
shared	O
task	O
.	O

paragraph	O
:	O
Model	O
Configuration	O
.	O

Inspired	O
by	O
the	O
integrated	O
POS	Method
tagging	Method
and	O
parsing	Method
transition	Method
system	Method
of	O
bohnet	O
-	O
nivre:2012:EMNLP	O
-	O
CoNLL	O
,	O
we	O
employ	O
a	O
simple	O
transition	Method
system	Method
that	O
uses	O
only	O
a	O
Shift	O
action	O
and	O
predicts	O
the	O
POS	Task
tag	O
of	O
the	O
current	O
word	O
on	O
the	O
buffer	O
as	O
it	O
gets	O
shifted	O
to	O
the	O
stack	O
.	O

We	O
extract	O
the	O
following	O
features	O
on	O
a	O
window	O
tokens	O
centered	O
at	O
the	O
current	O
focus	O
token	O
:	O
word	O
,	O
cluster	O
,	O
character	O
n	O
-	O
gram	O
up	O
to	O
length	O
3	O
.	O

We	O
also	O
extract	O
the	O
tag	O
predicted	O
for	O
the	O
previous	O
4	O
tokens	O
.	O

The	O
network	O
in	O
these	O
experiments	O
has	O
a	O
single	O
hidden	Method
layer	Method
with	O
256	O
units	O
on	O
WSJ	O
and	O
Treebank	O
Union	O
and	O
64	O
on	O
CoNLL’09	O
.	O

paragraph	O
:	O
Results	O
.	O

In	O
Table	O
[	O
reference	O
]	O
we	O
compare	O
our	O
model	O
to	O
a	O
linear	O
CRF	Method
and	O
to	O
the	O
compositional	O
character	O
-	O
to	O
-	O
word	O
LSTM	Method
model	O
of	O
ling	O
-	O
EtAl:2015:EMNLP	O
.	O

The	O
CRF	Method
is	O
a	O
first	Method
-	Method
order	Method
linear	Method
model	Method
with	O
exact	Method
inference	Method
and	O
the	O
same	O
emission	O
features	O
as	O
our	O
model	O
.	O

It	O
additionally	O
also	O
has	O
transition	O
features	O
of	O
the	O
word	O
,	O
cluster	Method
and	Method
character	Method
n	Method
-	Method
gram	Method
up	O
to	O
length	O
3	O
on	O
both	O
endpoints	O
of	O
the	O
transition	O
.	O

The	O
results	O
for	O
ling	O
-	O
EtAl:2015:EMNLP	O
were	O
solicited	O
from	O
the	O
authors	O
.	O

Our	O
local	Method
model	Method
already	O
compares	O
favorably	O
against	O
these	O
methods	O
on	O
average	O
.	O

Using	O
beam	Method
search	Method
with	O
a	O
locally	Method
normalized	Method
model	Method
does	O
not	O
help	O
,	O
but	O
with	O
global	Method
normalization	Method
it	O
leads	O
to	O
a	O
7	O
%	O
reduction	O
in	O
relative	Metric
error	Metric
,	O
empirically	O
demonstrating	O
the	O
effect	O
of	O
label	Metric
bias	Metric
.	O

The	O
set	O
of	O
character	O
ngrams	O
feature	O
is	O
very	O
important	O
,	O
increasing	O
average	Metric
accuracy	Metric
on	O
the	O
CoNLL’09	O
datasets	O
by	O
about	O
0.5	O
%	O
absolute	O
.	O

This	O
shows	O
that	O
character	Method
-	Method
level	Method
modeling	Method
can	O
also	O
be	O
done	O
with	O
a	O
simple	O
feed	Method
-	Method
forward	Method
network	Method
without	O
recurrence	Method
.	O

subsection	O
:	O
Dependency	Task
Parsing	Task
In	O
dependency	Task
parsing	Task
the	O
goal	O
is	O
to	O
produce	O
a	O
directed	O
tree	O
representing	O
the	O
syntactic	O
structure	O
of	O
the	O
input	O
sentence	O
.	O

paragraph	O
:	O
Data	O
&	O
Evaluation	O
.	O

We	O
use	O
the	O
same	O
corpora	O
as	O
in	O
our	O
POS	Method
tagging	Method
experiments	O
,	O
except	O
that	O
we	O
use	O
the	O
standard	O
parsing	O
splits	O
of	O
the	O
WSJ	Method
.	O

To	O
avoid	O
over	O
-	O
fitting	O
to	O
the	O
development	O
set	O
(	O
Sec	O
.	O

22	O
)	O
,	O
we	O
use	O
Sec	O
.	O

24	O
for	O
tuning	O
the	O
hyperparameters	O
of	O
our	O
models	O
.	O

We	O
convert	O
the	O
English	O
constituency	O
trees	O
to	O
Stanford	O
style	O
dependencies	O
using	O
version	O
3.3.0	O
of	O
the	O
converter	O
.	O

For	O
English	O
,	O
we	O
use	O
predicted	O
POS	Task
tags	O
(	O
the	O
same	O
POS	Task
tags	O
are	O
used	O
for	O
all	O
models	O
)	O
and	O
exclude	O
punctuation	O
from	O
the	O
evaluation	O
,	O
as	O
is	O
standard	O
.	O

For	O
the	O
CoNLL	O
’	O
09	O
datasets	O
we	O
follow	O
standard	O
practice	O
and	O
include	O
all	O
punctuation	O
in	O
the	O
evaluation	O
.	O

We	O
follow	O
alberti	O
-	O
EtAl:2015:EMNLP	O
and	O
use	O
our	O
own	O
predicted	O
POS	Task
tags	O
so	O
that	O
we	O
can	O
include	O
a	O
k	O
-	O
best	O
tag	O
feature	O
(	O
see	O
below	O
)	O
but	O
use	O
the	O
supplied	O
predicted	O
morphological	O
features	O
.	O

We	O
report	O
unlabeled	Metric
and	O
labeled	Metric
attachment	Metric
scores	Metric
(	O
UAS	Metric
/	O
LAS	Metric
)	O
.	O

paragraph	O
:	O
Model	O
Configuration	O
.	O

Our	O
model	O
configuration	O
is	O
basically	O
the	O
same	O
as	O
the	O
one	O
originally	O
proposed	O
by	O
chen	O
-	O
manning:2014:EMNLP	O
and	O
then	O
refined	O
by	O
weiss	O
-	O
etAl:2015:ACL	O
.	O

In	O
particular	O
,	O
we	O
use	O
the	O
arc	Method
-	Method
standard	Method
transition	O
system	O
and	O
extract	O
the	O
same	O
set	O
of	O
features	O
as	O
prior	O
work	O
:	O
words	O
,	O
part	O
of	O
speech	O
tags	O
,	O
and	O
dependency	O
arcs	O
and	O
labels	O
in	O
the	O
surrounding	O
context	O
of	O
the	O
state	O
,	O
as	O
well	O
as	O
k	O
-	O
best	O
tags	O
as	O
proposed	O
by	O
alberti	O
-	O
EtAl:2015:EMNLP	O
.	O

We	O
use	O
two	O
hidden	Method
layers	Method
of	O
1	O
,	O
024	O
dimensions	O
each	O
.	O

paragraph	O
:	O
Results	O
.	O

Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
our	O
final	O
parsing	Task
results	O
and	O
a	O
comparison	O
to	O
the	O
best	O
systems	O
from	O
the	O
literature	O
.	O

We	O
obtain	O
the	O
best	O
ever	O
published	O
results	O
on	O
almost	O
all	O
datasets	O
,	O
including	O
the	O
WSJ	O
.	O

Our	O
main	O
results	O
use	O
the	O
same	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
as	O
weiss	O
-	O
etAl:2015:ACL	O
and	O
alberti	O
-	O
EtAl:2015:EMNLP	O
,	O
but	O
no	O
tri	Method
-	Method
training	Method
.	O

When	O
we	O
artificially	O
restrict	O
ourselves	O
to	O
not	O
use	O
pre	O
-	O
trained	O
word	O
embeddings	O
,	O
we	O
observe	O
only	O
a	O
modest	O
drop	O
of	O
0.5	O
%	O
UAS	Metric
;	O
for	O
example	O
,	O
training	O
only	O
on	O
the	O
WSJ	Method
yields	O
94.08	O
%	O
UAS	Metric
and	O
92.15	O
%	O
LAS	Metric
for	O
our	O
global	Method
model	Method
with	O
a	O
beam	O
of	O
size	O
32	O
.	O

Even	O
though	O
we	O
do	O
not	O
use	O
tri	Method
-	Method
training	Method
,	O
our	O
model	O
compares	O
favorably	O
to	O
the	O
94.26	O
%	O
LAS	Metric
and	O
92.41	O
%	O
UAS	Metric
reported	O
by	O
weiss	O
-	O
etAl:2015:ACL	O
with	O
tri	Method
-	Method
training	Method
.	O

As	O
we	O
show	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
these	O
gains	O
can	O
be	O
attributed	O
to	O
the	O
full	O
backpropagation	Method
training	Method
that	O
differentiates	O
our	O
approach	O
from	O
that	O
of	O
weiss	O
-	O
etAl:2015:ACL	O
and	O
alberti	O
-	O
EtAl:2015:EMNLP	O
.	O

Our	O
results	O
also	O
significantly	O
outperform	O
the	O
LSTM	Method
-	O
based	O
approaches	O
of	O
dyer	O
-	O
etAl:2015:ACL	O
and	O
BallesterosDS15	O
.	O

subsection	O
:	O
Sentence	Task
Compression	Task
Our	O
final	O
structured	Task
prediction	Task
task	Task
is	O
extractive	Task
sentence	Task
compression	Task
.	O

paragraph	O
:	O
Data	O
&	O
Evaluation	O
.	O

We	O
follow	O
filippova	O
-	O
emnlp15	O
,	O
where	O
a	O
large	O
news	O
collection	O
is	O
used	O
to	O
heuristically	O
generate	O
compression	Task
instances	Task
.	O

Our	O
final	O
corpus	O
contains	O
about	O
2.3	O
M	O
compression	O
instances	O
:	O
we	O
use	O
2	O
M	O
examples	O
for	O
training	O
,	O
130k	O
for	O
development	O
and	O
160k	O
for	O
the	O
final	O
test	O
.	O

We	O
report	O
per	Metric
-	Metric
token	Metric
F1	Metric
score	Metric
and	O
per	Metric
-	Metric
sentence	Metric
accuracy	Metric
(	O
A	O
)	O
,	O
i.e.	O
percentage	O
of	O
instances	O
that	O
fully	O
match	O
the	O
golden	Method
compressions	Method
.	O

Following	O
filippova	O
-	O
emnlp15	O
we	O
also	O
run	O
a	O
human	O
evaluation	O
on	O
200	O
sentences	O
where	O
we	O
ask	O
the	O
raters	O
to	O
score	O
compressions	Metric
for	O
readability	Metric
(	O
read	O
)	O
and	O
informativeness	Metric
(	O
info	O
)	O
on	O
a	O
scale	O
from	O
0	O
to	O
5	O
.	O

paragraph	O
:	O
Model	O
Configuration	O
.	O

The	O
transition	Method
system	Method
for	O
sentence	Task
compression	Task
is	O
similar	O
to	O
POS	Method
tagging	Method
:	O
we	O
scan	O
sentences	O
from	O
left	O
-	O
to	O
-	O
right	O
and	O
label	O
each	O
token	O
as	O
keep	O
or	O
drop	O
.	O

We	O
extract	O
features	O
from	O
words	O
,	O
POS	Task
tags	O
,	O
and	O
dependency	O
labels	O
from	O
a	O
window	O
of	O
tokens	O
centered	O
on	O
the	O
input	O
,	O
as	O
well	O
as	O
features	O
from	O
the	O
history	O
of	O
predictions	O
.	O

We	O
use	O
a	O
single	O
hidden	Method
layer	Method
of	O
size	O
400	O
.	O

paragraph	O
:	O
Results	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
our	O
sentence	Task
compression	Task
results	O
.	O

Our	O
globally	Method
normalized	Method
model	Method
again	O
significantly	O
outperforms	O
the	O
local	Method
model	Method
.	O

Beam	Method
search	Method
with	O
a	O
locally	Method
normalized	Method
model	Method
suffers	O
from	O
severe	O
label	O
bias	O
issues	O
that	O
we	O
discuss	O
on	O
a	O
concrete	O
example	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
also	O
compare	O
to	O
the	O
sentence	Method
compression	Method
system	Method
from	O
filippova	O
-	O
emnlp15	O
,	O
a	O
3	O
-	O
layer	O
stacked	O
LSTM	Method
which	O
uses	O
dependency	O
label	O
information	O
.	O

The	O
LSTM	Method
and	O
our	O
global	Method
model	Method
perform	O
on	O
par	O
on	O
both	O
the	O
automatic	Metric
evaluation	Metric
as	O
well	O
as	O
the	O
human	O
ratings	O
,	O
but	O
our	O
model	O
is	O
roughly	O
100	O
faster	O
.	O

All	O
compressions	O
kept	O
approximately	O
42	O
%	O
of	O
the	O
tokens	O
on	O
average	O
and	O
all	O
the	O
models	O
are	O
significantly	O
better	O
than	O
the	O
automatic	Method
extractions	Method
(	O
)	O
.	O

section	O
:	O
Discussion	O
We	O
derived	O
a	O
proof	O
for	O
the	O
label	Task
bias	Task
problem	Task
and	O
the	O
advantages	O
of	O
global	Method
models	Method
.	O

We	O
then	O
emprirically	O
verified	O
this	O
theoretical	O
superiority	O
by	O
demonstrating	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
three	O
different	O
tasks	O
.	O

In	O
this	O
section	O
we	O
situate	O
and	O
compare	O
our	O
model	O
to	O
previous	O
work	O
and	O
provide	O
two	O
examples	O
of	O
the	O
label	Task
bias	Task
problem	Task
in	O
practice	O
.	O

subsection	O
:	O
Related	O
Neural	O
CRF	Method
Work	O
Neural	Method
network	Method
models	Method
have	O
been	O
been	O
combined	O
with	O
conditional	Method
random	Method
fields	Method
and	O
globally	Method
normalized	Method
models	Method
before	O
.	O

bottou	O
-	O
97	O
and	O
lecun	O
-	O
98h	O
describe	O
global	Method
training	Method
of	Method
neural	Method
network	Method
models	Method
for	O
structured	Task
prediction	Task
problems	Task
.	O

conditional_neural_fields	Method
add	O
a	O
non	Method
-	Method
linear	Method
neural	Method
network	Method
layer	Method
to	O
a	O
linear	O
-	O
chain	O
CRF	Method
and	O
neural_crf	Method
apply	O
a	O
similar	O
approach	O
to	O
more	O
general	O
Markov	Method
network	Method
structures	Method
.	O

recurrentCRF	Method
and	O
Zheng_2015_ICCV	O
introduce	O
recurrence	O
into	O
the	O
model	O
and	O
huang2015bidirectional	O
finally	O
combine	O
CRFs	Method
and	O
LSTMs	Method
.	O

These	O
neural	O
CRF	Method
models	O
are	O
limited	O
to	O
sequence	Task
labeling	Task
tasks	Task
where	O
exact	Task
inference	Task
is	O
possible	O
,	O
while	O
our	O
model	O
works	O
well	O
when	O
exact	Task
inference	Task
is	O
intractable	O
.	O

subsection	O
:	O
Related	O
Transition	Method
-	Method
Based	Method
Parsing	Method
Work	O
For	O
early	O
work	O
on	O
neural	Method
-	Method
networks	Method
for	O
transition	Task
-	Task
based	Task
parsing	Task
,	O
see	O
Henderson	O
henderson:2003:NAACL	O
,	O
henderson:2004:ACL	O
.	O

Our	O
work	O
is	O
closest	O
to	O
the	O
work	O
of	O
weiss	O
-	O
etAl:2015:ACL	O
,	O
zhou	O
-	O
etAl:2015:ACL	O
and	O
watanabe	O
-	O
sumita:2015:ACL	O
;	O
in	O
these	O
approaches	O
global	Method
normalization	Method
is	O
added	O
to	O
the	O
local	Method
model	Method
of	O
chen	O
-	O
manning:2014:EMNLP	O
.	O

Empirically	O
,	O
weiss	O
-	O
etAl:2015:ACL	O
achieves	O
the	O
best	O
performance	O
,	O
even	O
though	O
their	O
model	O
keeps	O
the	O
parameters	O
of	O
the	O
locally	Method
normalized	Method
neural	Method
network	Method
fixed	O
and	O
only	O
trains	O
a	O
perceptron	Method
that	O
uses	O
the	O
activations	O
as	O
features	O
.	O

Their	O
model	O
is	O
therefore	O
limited	O
in	O
its	O
ability	O
to	O
revise	O
the	O
predictions	O
of	O
the	O
locally	Method
normalized	Method
model	Method
.	O

In	O
Table	O
[	O
reference	O
]	O
we	O
show	O
that	O
full	Method
backpropagation	Method
training	Method
all	O
the	O
way	O
to	O
the	O
word	O
embeddings	O
is	O
very	O
important	O
and	O
significantly	O
contributes	O
to	O
the	O
performance	O
of	O
our	O
model	O
.	O

We	O
also	O
compared	O
training	Task
under	O
the	O
CRF	Method
objective	O
with	O
a	O
Perceptron	O
-	O
like	O
hinge	O
loss	O
between	O
the	O
gold	O
and	O
best	O
elements	O
of	O
the	O
beam	O
.	O

When	O
we	O
limited	O
the	O
backpropagation	O
depth	O
to	O
training	O
only	O
the	O
top	O
layer	O
,	O
we	O
found	O
negligible	O
differences	O
in	O
accuracy	Metric
:	O
93.20	O
%	O
and	O
93.28	O
%	O
for	O
the	O
CRF	Method
objective	O
and	O
hinge	O
loss	O
respectively	O
.	O

However	O
,	O
when	O
training	O
with	O
full	Method
backpropagation	Method
the	O
CRF	Method
accuracy	O
is	O
0.2	O
%	O
higher	O
and	O
training	O
converged	O
more	O
than	O
4	O
faster	O
.	O

zhou	O
-	O
etAl:2015:ACL	O
perform	O
full	Method
backpropagation	Method
training	Method
like	O
us	O
,	O
but	O
even	O
with	O
a	O
much	O
larger	O
beam	O
,	O
their	O
performance	O
is	O
significantly	O
lower	O
than	O
ours	O
.	O

We	O
also	O
apply	O
our	O
model	O
to	O
two	O
additional	O
tasks	O
,	O
while	O
they	O
experiment	O
only	O
with	O
dependency	Task
parsing	Task
.	O

Finally	O
,	O
watanabe	O
-	O
sumita:2015:ACL	O
introduce	O
recurrent	Method
components	Method
and	O
additional	O
techniques	O
like	O
max	Method
-	Method
violation	Method
updates	Method
for	O
a	O
corresponding	O
constituency	Method
parsing	Method
model	Method
.	O

In	O
contrast	O
,	O
our	O
model	O
does	O
not	O
require	O
any	O
recurrence	Method
or	Method
specialized	Method
training	Method
.	O

subsection	O
:	O
Label	O
Bias	O
in	O
Practice	O
We	O
observed	O
several	O
instances	O
of	O
severe	O
label	O
bias	O
in	O
the	O
sentence	Task
compression	Task
task	Task
.	O

Although	O
using	O
beam	Method
search	Method
with	O
the	O
local	Method
model	Method
outperforms	O
greedy	Method
inference	Method
on	O
average	O
,	O
beam	Method
search	Method
leads	O
the	O
local	Method
model	Method
to	O
occasionally	O
produce	O
empty	O
compressions	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
these	O
are	O
not	O
search	O
errors	O
:	O
the	O
empty	Method
compression	Method
has	O
higher	O
probability	O
under	O
than	O
the	O
prediction	O
from	O
greedy	Method
inference	Method
.	O

However	O
,	O
the	O
more	O
expressive	O
globally	Method
normalized	Method
model	Method
does	O
not	O
suffer	O
from	O
this	O
limitation	O
,	O
and	O
correctly	O
gives	O
the	O
empty	O
compression	O
almost	O
zero	O
probability	O
.	O

We	O
also	O
present	O
some	O
empirical	O
evidence	O
that	O
the	O
label	Task
bias	Task
problem	Task
is	O
severe	O
in	O
parsing	Task
.	O

We	O
trained	O
models	O
where	O
the	O
scoring	O
functions	O
in	O
parsing	Task
at	O
position	O
in	O
the	O
sentence	O
are	O
limited	O
to	O
considering	O
only	O
tokens	O
;	O
hence	O
unlike	O
the	O
full	Method
parsing	Method
model	Method
,	O
there	O
is	O
no	O
ability	O
to	O
look	O
ahead	O
in	O
the	O
sentence	O
when	O
making	O
a	O
decision	O
.	O

The	O
result	O
for	O
a	O
greedy	Method
model	Method
under	O
this	O
constraint	O
is	O
76.96	O
%	O
UAS	Metric
;	O
for	O
a	O
locally	Method
normalized	Method
model	Method
with	O
beam	Method
search	Method
is	O
81.35	O
%	O
;	O
and	O
for	O
a	O
globally	Method
normalized	Method
model	Method
is	O
93.60	O
%	O
.	O

Thus	O
the	O
globally	Method
normalized	Method
model	Method
gets	O
very	O
close	O
to	O
the	O
performance	O
of	O
a	O
model	O
with	O
full	O
lookahead	O
,	O
while	O
the	O
locally	Method
normalized	Method
model	Method
with	O
a	O
beam	O
gives	O
dramatically	O
lower	O
performance	O
.	O

In	O
our	O
final	O
experiments	O
with	O
full	Task
lookahead	Task
,	O
the	O
globally	Method
normalized	Method
model	Method
achieves	O
94.01	O
%	O
accuracy	Metric
,	O
compared	O
to	O
93.07	O
%	O
accuracy	Metric
for	O
a	O
local	Method
model	Method
with	O
beam	Method
search	Method
.	O

Thus	O
adding	O
lookahead	Method
allows	O
the	O
local	Method
model	Method
to	O
close	O
the	O
gap	O
in	O
performance	O
to	O
the	O
global	Method
model	Method
;	O
however	O
there	O
is	O
still	O
a	O
significant	O
difference	O
in	O
accuracy	Metric
,	O
which	O
may	O
in	O
large	O
part	O
be	O
due	O
to	O
the	O
label	Task
bias	Task
problem	Task
.	O

A	O
number	O
of	O
authors	O
have	O
considered	O
modified	O
training	Method
procedures	Method
for	O
greedy	Method
models	Method
,	O
or	O
for	O
locally	Method
normalized	Method
models	Method
.	O

daume09searn	O
introduce	O
Searn	Method
,	O
an	O
algorithm	O
that	O
allows	O
a	O
classifier	Method
making	O
greedy	Task
decisions	Task
to	O
become	O
more	O
robust	O
to	O
errors	O
made	O
in	O
previous	O
decisions	O
.	O

goldberg2013training	O
describe	O
improvements	O
to	O
a	O
greedy	Method
parsing	Method
approach	Method
that	O
makes	O
use	O
of	O
methods	O
from	O
imitation	Method
learning	Method
to	O
augment	O
the	O
training	O
set	O
.	O

Note	O
that	O
these	O
methods	O
are	O
focused	O
on	O
greedy	Method
models	Method
:	O
they	O
are	O
unlikely	O
to	O
solve	O
the	O
label	Task
bias	Task
problem	Task
when	O
used	O
in	O
conjunction	O
with	O
beam	Method
search	Method
,	O
given	O
that	O
the	O
problem	O
is	O
one	O
of	O
expressivity	O
of	O
the	O
underlying	O
model	O
.	O

More	O
recent	O
work	O
has	O
augmented	O
locally	Method
normalized	Method
models	Method
with	O
correctness	O
probabilities	O
or	O
error	O
states	O
,	O
effectively	O
adding	O
a	O
step	O
after	O
every	O
decision	O
where	O
the	O
probability	O
of	O
correctness	O
of	O
the	O
resulting	O
structure	O
is	O
evaluated	O
.	O

This	O
gives	O
considerable	O
gains	O
over	O
a	O
locally	Method
normalized	Method
model	Method
,	O
although	O
performance	O
is	O
lower	O
than	O
our	O
full	Method
globally	Method
normalized	Method
approach	Method
.	O

section	O
:	O
Conclusions	O
We	O
presented	O
a	O
simple	O
and	O
yet	O
powerful	O
model	Method
architecture	Method
that	O
produces	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
POS	Method
tagging	Method
,	O
dependency	Task
parsing	Task
and	O
sentence	Task
compression	Task
.	O

Our	O
model	O
combines	O
the	O
flexibility	O
of	O
transition	Method
-	Method
based	Method
algorithms	Method
and	O
the	O
modeling	Method
power	Method
of	Method
neural	Method
networks	Method
.	O

Our	O
results	O
demonstrate	O
that	O
feed	Method
-	Method
forward	Method
network	Method
without	Method
recurrence	Method
can	O
outperform	O
recurrent	Method
models	Method
such	O
as	O
LSTMs	Method
when	O
they	O
are	O
trained	O
with	O
global	Method
normalization	Method
.	O

We	O
further	O
support	O
our	O
empirical	O
findings	O
with	O
a	O
proof	O
showing	O
that	O
global	Method
normalization	Method
helps	O
the	O
model	O
overcome	O
the	O
label	Task
bias	Task
problem	Task
from	O
which	O
locally	Method
normalized	Method
models	Method
suffer	O
.	O

section	O
:	O
Acknowledgements	O
We	O
would	O
like	O
to	O
thank	O
Ling	O
Wang	O
for	O
training	O
his	O
C2W	Method
part	Method
-	Method
of	Method
-	Method
speech	Method
tagger	Method
on	O
our	O
setup	O
,	O
and	O
Emily	O
Pitler	O
,	O
Ryan	O
McDonald	O
,	O
Greg	O
Coppola	O
and	O
Fernando	O
Pereira	O
for	O
tremendously	O
helpful	O
discussions	O
.	O

Finally	O
,	O
we	O
are	O
grateful	O
to	O
all	O
members	O
of	O
the	O
Google	O
Parsing	O
Team	O
.	O

bibliography	O
:	O
References	O
