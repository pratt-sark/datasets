Graph	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
GCNNs	Method
)	Method
are	O
the	O
most	O
recent	O
exciting	O
advancement	O
in	O
deep	Task
learning	Task
field	Task
and	O
their	O
applications	O
are	O
quickly	O
spreading	O
in	O
multi	O
-	O
cross	Task
-	Task
domains	Task
including	O
bioinformatics	Material
,	O
chemoinformatics	Material
,	O
social	Task
networks	Task
,	O
natural	Task
language	Task
processing	Task
and	O
computer	Task
vision	Task
.	O

In	O
this	O
paper	O
,	O
we	O
expose	O
and	O
tackle	O
some	O
of	O
the	O
basic	O
weaknesses	O
of	O
a	O
GCNN	Method
model	Method
with	O
a	O
capsule	Method
idea	Method
presented	O
in	O
and	O
propose	O
our	O
Graph	Method
Capsule	Method
Network	Method
(	O
GCAPS	Method
-	Method
CNN	Method
)	Method
model	Method
.	O

In	O
addition	O
,	O
we	O
design	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
to	O
solve	O
especially	O
graph	Task
classification	Task
problem	Task
which	O
current	O
GCNN	Method
models	Method
find	O
challenging	O
.	O

Through	O
extensive	O
experiments	O
,	O
we	O
show	O
that	O
our	O
proposed	O
Graph	Method
Capsule	Method
Network	Method
can	O
significantly	O
outperforms	O
both	O
the	O
existing	O
state	O
-	O
of	O
-	O
art	O
deep	Method
learning	Method
methods	Method
and	O
graph	Method
kernels	Method
on	O
graph	Material
classification	Material
benchmark	Material
datasets	Material
.	O

sectionÂ§Â§Â§	Method
sectionÂ§Â§Â§	Method
GraphCapsuleConvolutionalNeuralNetworks	Method
section	O
:	O
Introduction	O
Graphs	Method
are	O
one	O
of	O
the	O
most	O
fundamental	O
structures	O
that	O
have	O
been	O
widely	O
used	O
for	O
representing	O
many	O
types	O
of	O
data	O
.	O

Learning	Task
on	Task
graphs	Task
such	O
as	O
graph	Task
semi	Task
-	Task
supervised	Task
learning	Task
,	O
graph	Task
classification	Task
or	O
graph	Task
evolution	Task
have	O
found	O
wide	O
applications	O
in	O
domains	O
such	O
as	O
bioinformatics	Task
,	O
chemoinformatics	Task
,	O
social	Task
networks	Task
,	O
natural	Task
language	Task
processing	Task
and	O
computer	Task
vision	Task
.	O

With	O
remarkable	O
successes	O
of	O
deep	Method
learning	Method
approaches	Method
in	O
image	Task
classification	Task
and	O
object	Task
recognition	Task
that	O
attain	O
“	O
superhuman	O
”	O
performance	O
,	O
there	O
has	O
been	O
a	O
surge	O
of	O
research	O
interests	O
in	O
generalizing	O
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
to	O
structures	O
beyond	O
regular	O
grids	O
,	O
i.e.	O
,	O
from	O
2D	Material
/	Material
3D	Material
images	Material
to	O
arbitrary	O
structures	O
such	O
as	O
graphs	O
.	O

These	O
convolutional	Method
networks	Method
on	O
graphs	Material
are	O
now	O
commonly	O
known	O
as	O
Graph	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
GCNNs	Method
)	O
.	O

The	O
principal	O
idea	O
behind	O
graph	Method
convolution	Method
has	O
been	O
derived	O
from	O
the	O
graph	Method
signal	Method
processing	Method
domain	Method
,	O
which	O
has	O
since	O
been	O
extended	O
in	O
different	O
ways	O
for	O
a	O
variety	O
of	O
purposes	O
.	O

In	O
this	O
paper	O
,	O
we	O
expose	O
three	O
major	O
limitations	O
of	O
the	O
standard	O
GCNN	Method
model	Method
commonly	O
used	O
in	O
existing	O
deep	Method
learning	Method
approaches	Method
on	O
graphs	Task
,	O
especially	O
when	O
applied	O
to	O
the	O
graph	Task
classification	Task
problem	Task
,	O
and	O
explore	O
ways	O
to	O
overcome	O
these	O
limitations	O
.	O

In	O
particular	O
,	O
we	O
propose	O
a	O
new	O
model	O
,	O
referred	O
to	O
as	O
Graph	Method
Capsule	Method
Convolution	Method
Neural	Method
Networks	Method
(	Method
GCAPS	Method
-	Method
CNN	Method
)	O
.	O

It	O
is	O
inspired	O
by	O
the	O
notion	O
of	O
capsules	O
developed	O
in	O
:	O
capsules	Method
are	O
new	O
types	O
of	O
neurons	O
which	O
encapsulate	O
more	O
information	O
in	O
a	O
local	O
pool	O
operation	O
(	O
e.g.	O
,	O
a	O
convolution	Method
operation	Method
in	O
a	O
CNN	Method
)	O
by	O
computing	O
a	O
small	O
vector	O
of	O
highly	O
informative	O
outputs	O
rather	O
than	O
just	O
taking	O
a	O
scalar	O
output	O
.	O

Our	O
graph	Method
capsule	Method
idea	Method
is	O
quite	O
general	O
and	O
can	O
be	O
employed	O
in	O
any	O
version	O
of	O
GCNN	Method
model	Method
either	O
design	O
for	O
solving	O
graph	Task
semi	Task
-	Task
supervised	Task
problem	Task
or	O
doing	O
sequence	Task
learning	Task
on	Task
graphs	Task
via	O
Graph	Method
Convolution	Method
Recurrent	Method
Neural	Method
Network	Method
models	Method
(	O
GCRNNs	Method
)	O
.	O

The	O
first	O
limitation	O
of	O
the	O
standard	O
GCNN	Method
model	Method
is	O
due	O
to	O
the	O
basic	O
graph	Method
convolution	Method
operation	Method
which	O
is	O
defined	O
–	O
in	O
its	O
purest	O
form	O
–	O
as	O
the	O
aggregation	O
of	O
node	O
values	O
in	O
a	O
local	O
neighborhood	O
corresponding	O
to	O
each	O
feature	O
(	O
or	O
channel	O
)	O
.	O

As	O
such	O
,	O
there	O
is	O
a	O
potential	O
loss	O
of	O
information	O
associated	O
with	O
the	O
basic	O
graph	Method
convolution	Method
operation	Method
.	O

This	O
problem	O
has	O
been	O
noted	O
before	O
,	O
but	O
has	O
not	O
attracted	O
much	O
attention	O
until	O
recently	O
.	O

To	O
address	O
this	O
limitation	O
,	O
we	O
propose	O
to	O
improve	O
upon	O
the	O
basic	O
graph	Method
convolution	Method
operation	Method
by	O
introducing	O
the	O
notion	O
of	O
graph	O
capsules	O
which	O
encapsulate	O
more	O
information	O
about	O
nodes	O
in	O
a	O
local	O
neighborhood	O
,	O
where	O
the	O
local	O
neighborhood	O
is	O
defined	O
in	O
the	O
same	O
way	O
as	O
in	O
the	O
standard	O
GCCN	Method
model	Method
.	O

Similar	O
to	O
the	O
original	O
capsule	Method
idea	Method
proposed	O
in	O
,	O
this	O
is	O
achieved	O
by	O
replacing	O
the	O
scalar	O
output	O
of	O
a	O
graph	Method
convolution	Method
operation	Method
with	O
a	O
small	O
vector	O
output	O
containing	O
higher	O
order	O
statistical	O
information	O
per	O
feature	O
.	O

Another	O
source	O
of	O
inspiration	O
for	O
our	O
proposed	O
GCAPS	Method
-	Method
CNN	Method
model	Method
comes	O
from	O
one	O
of	O
the	O
most	O
successful	O
graph	Method
kernels	Method
–	O
the	O
Weisfeiler	Method
-	Method
Lehman	Method
(	O
WL	Method
)-	Method
subtree	Method
graph	Method
kernel	Method
designed	O
specifically	O
for	O
solving	O
the	O
graph	Task
classification	Task
problem	Task
.	O

In	O
WL	Method
-	Method
subtree	Method
graph	Method
kernel	Method
,	O
node	O
labels	O
(	O
features	O
)	O
are	O
collected	O
from	O
neighbors	O
of	O
each	O
node	O
in	O
a	O
local	O
neighborhood	O
and	O
compressed	O
injectively	O
to	O
form	O
a	O
new	O
node	O
label	O
in	O
each	O
iteration	O
.	O

The	O
histogram	O
of	O
these	O
new	O
node	O
labels	O
are	O
concatenated	O
in	O
each	O
iteration	O
to	O
serve	O
as	O
a	O
graph	O
invariant	O
feature	O
vector	O
.	O

The	O
important	O
point	O
to	O
notice	O
here	O
is	O
that	O
due	O
to	O
the	O
injection	Method
process	Method
,	O
one	O
can	O
recover	O
the	O
exact	O
node	O
labels	O
of	O
local	O
neighbors	O
in	O
each	O
iteration	O
without	O
losing	O
track	O
of	O
them	O
.	O

In	O
contrast	O
,	O
this	O
is	O
not	O
possible	O
in	O
the	O
standard	O
GCNN	Method
model	Method
as	O
the	O
input	O
feature	O
values	O
of	O
node	O
neighbors	O
are	O
lost	O
after	O
the	O
graph	Method
convolution	Method
operation	Method
.	O

The	O
second	O
major	O
limitation	O
of	O
the	O
standard	O
GCNN	Method
model	Method
is	O
specific	O
to	O
its	O
(	O
in	O
)	O
ability	O
in	O
tackling	O
the	O
graph	Task
classification	Task
problem	Task
.	O

GCNN	Method
models	Method
can	O
not	O
be	O
applied	O
directly	O
because	O
they	O
are	O
equivariant	O
(	O
not	O
invariant	O
)	O
with	O
respect	O
to	O
the	O
node	O
order	O
in	O
a	O
graph	O
.	O

To	O
be	O
precise	O
,	O
consider	O
a	O
graph	O
with	O
Laplacian	O
and	O
node	O
feature	O
matrix	O
.	O

Let	O
be	O
the	O
output	O
function	O
of	O
a	O
GCNN	Method
model	Method
where	O
are	O
the	O
number	O
of	O
nodes	O
,	O
input	O
dimension	O
and	O
hidden	O
dimension	O
of	O
node	O
features	O
,	O
respectively	O
.	O

Then	O
,	O
is	O
a	O
permutation	O
equivariant	O
function	O
,	O
i.e.	O
,	O
for	O
any	O
P	O
permutation	O
matrix	O
=	O
⁢f	O
(	O
PX	O
,	O
PLPT	O
)	O
⁢Pf	O
(	O
X	O
,	O
L	O
)	O
.	O

This	O
specific	O
permutation	O
equivariance	O
property	O
prevent	O
us	O
from	O
directly	O
applying	O
GCNN	Method
to	O
a	O
graph	Task
classification	Task
problem	Task
,	O
since	O
it	O
can	O
not	O
provide	O
any	O
guarantee	O
that	O
the	O
outputs	O
of	O
any	O
two	O
isomorphic	O
graphs	O
are	O
always	O
the	O
same	O
.	O

Consequently	O
,	O
a	O
GCNN	Method
architecture	Method
needs	O
an	O
additional	O
graph	Method
permutation	Method
invariant	Method
layer	Method
in	O
order	O
to	O
perform	O
the	O
graph	Task
classification	Task
task	Task
successfully	O
.	O

This	O
invariant	Method
layer	Method
also	O
needs	O
to	O
be	O
differentiable	O
for	O
end	Task
-	Task
to	Task
-	Task
end	Task
learning	Task
.	O

Very	O
limited	O
amount	O
of	O
efforts	O
has	O
been	O
devoted	O
to	O
carefully	O
designing	O
such	O
an	O
invariant	Method
GCNN	Method
model	Method
for	O
the	O
purpose	O
of	O
graph	Task
classification	Task
.	O

Currently	O
the	O
most	O
common	O
method	O
for	O
achieving	O
graph	Task
permutation	Task
invariance	Task
is	O
performing	O
aggregation	Method
(	O
i.e.	O
,	O
summing	O
)	O
over	O
all	O
graph	O
node	O
values	O
.	O

Though	O
simple	O
and	O
fast	O
,	O
it	O
can	O
again	O
incur	O
significant	O
loss	O
of	O
information	O
.	O

Likewise	O
,	O
using	O
a	O
max	Method
-	Method
pooling	Method
layer	Method
to	O
achieve	O
graph	Task
permutation	Task
invariance	Task
encounters	O
similar	O
issues	O
.	O

A	O
few	O
attempts	O
have	O
been	O
made	O
that	O
go	O
beyond	O
aggregation	Method
or	O
max	Method
-	Method
pooling	Method
in	O
designing	O
graph	Method
permutation	Method
invariant	Method
GCNNs	Method
.	O

In	O
the	O
authors	O
propose	O
a	O
global	O
ordering	O
of	O
nodes	O
by	O
sorting	O
them	O
according	O
to	O
their	O
values	O
in	O
the	O
last	O
hidden	O
layer	O
.	O

This	O
type	O
of	O
invariance	O
is	O
based	O
on	O
creating	O
an	O
order	O
among	O
nodes	O
and	O
has	O
also	O
been	O
explored	O
before	O
in	O
.	O

However	O
,	O
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
show	O
that	O
there	O
are	O
some	O
issues	O
with	O
this	O
type	O
of	O
approach	O
.	O

A	O
more	O
tangential	O
approach	O
has	O
been	O
adopted	O
in	O
based	O
on	O
group	Method
theory	Method
to	O
design	O
transformation	O
operations	O
and	O
tensor	Method
aggregation	Method
rules	Method
that	O
results	O
in	O
permutation	O
invariant	O
outputs	O
.	O

However	O
,	O
this	O
approach	O
relies	O
on	O
computing	O
high	O
order	O
tensors	O
which	O
are	O
computationally	O
expensive	O
in	O
many	O
cases	O
.	O

To	O
that	O
end	O
,	O
we	O
propose	O
a	O
novel	O
permutation	Method
invariant	Method
layer	Method
based	O
on	O
computing	O
the	O
covariance	O
of	O
the	O
data	O
whose	O
output	O
does	O
not	O
depend	O
upon	O
the	O
order	O
of	O
nodes	O
in	O
the	O
graph	O
.	O

It	O
is	O
also	O
fast	O
to	O
compute	O
since	O
it	O
requires	O
only	O
a	O
single	O
dense	Method
-	Method
matrix	Method
multiplication	Method
operation	Method
.	O

Our	O
last	O
concern	O
with	O
the	O
standard	O
GCNN	Method
model	Method
is	O
their	O
limited	O
ability	O
in	O
exploiting	O
global	O
information	O
for	O
the	O
purpose	O
of	O
graph	Task
classification	Task
.	O

The	O
filters	Method
employed	O
in	O
graph	Method
convolutions	Method
are	O
in	O
essence	O
local	O
in	O
nature	O
and	O
hence	O
can	O
only	O
provide	O
an	O
“	O
average	O
/	O
aggregate	O
view	O
”	O
of	O
the	O
local	Material
data	Material
.	O

This	O
shortcoming	O
poses	O
a	O
serious	O
difficulty	O
in	O
handling	O
graphs	O
where	O
node	O
labels	O
are	O
not	O
present	O
;	O
approaches	O
which	O
initialize	O
(	O
node	O
)	O
feature	O
values	O
using	O
,	O
e.g.	O
,	O
node	O
degree	O
,	O
are	O
not	O
much	O
helpful	O
in	O
this	O
respect	O
.	O

We	O
propose	O
to	O
utilize	O
global	O
features	O
(	O
features	O
that	O
account	O
for	O
the	O
full	O
graph	O
structure	O
)	O
using	O
a	O
family	O
of	O
graph	Method
spectral	Method
distances	Method
as	O
proposed	O
in	O
to	O
remedy	O
this	O
problem	O
.	O

In	O
summary	O
,	O
the	O
major	O
contributions	O
of	O
our	O
paper	O
are	O
:	O
We	O
propose	O
a	O
novel	O
Graph	Method
Capsule	Method
Convolution	Method
Neural	Method
Network	Method
model	Method
based	O
on	O
the	O
capsule	Method
idea	Method
to	O
capture	O
highly	O
informative	O
output	O
in	O
a	O
small	O
vector	O
in	O
place	O
of	O
a	O
scaler	O
output	O
currently	O
employed	O
in	O
GCNN	Method
models	Method
.	O

We	O
develop	O
a	O
novel	O
graph	Method
permutation	Method
invariant	Method
layer	Method
based	O
on	O
computing	O
the	O
covariance	O
of	O
data	O
to	O
solve	O
graph	Task
classification	Task
problem	Task
.	O

We	O
show	O
that	O
it	O
is	O
a	O
better	O
choice	O
than	O
performing	O
node	Method
aggregation	Method
or	O
doing	O
max	Method
pooling	Method
and	O
at	O
the	O
same	O
time	O
it	O
can	O
be	O
computed	O
efficiently	O
.	O

Lastly	O
,	O
we	O
advocate	O
explicitly	O
including	O
global	O
graph	O
structure	O
features	O
at	O
each	O
graph	O
node	O
to	O
enable	O
the	O
proposed	O
GCAPS	Method
-	Method
CNN	Method
model	Method
to	O
exploit	O
them	O
for	O
graph	Task
learning	Task
tasks	Task
.	O

We	O
organize	O
our	O
paper	O
into	O
five	O
sections	O
.	O

We	O
start	O
with	O
the	O
related	O
work	O
on	O
graph	Method
kernels	Method
and	O
GCNNs	Method
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
present	O
our	O
core	O
idea	O
behind	O
graph	Method
capsules	Method
in	O
Section	O
[	O
reference	O
]	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
building	O
a	O
graph	Method
permutation	Method
invariant	Method
layer	Method
especially	O
for	O
solving	O
the	O
graph	Task
classification	Task
problem	Task
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
propose	O
to	O
equip	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
with	O
enhanced	O
global	O
features	O
to	O
exploit	O
the	O
full	O
graph	O
structure	O
for	O
learning	Task
on	Task
graphs	Task
.	O

Lastly	O
in	O
Section	O
[	O
reference	O
]	O
we	O
conduct	O
experiments	O
and	O
show	O
the	O
superior	O
performance	O
of	O
our	O
proposed	O
GCAPS	Method
-	Method
CNN	Method
model	Method
.	O

section	O
:	O
Related	O
Work	O
There	O
are	O
three	O
main	O
approaches	O
for	O
solving	O
the	O
graph	Task
classification	Task
problem	Task
.	O

The	O
most	O
common	O
approach	O
is	O
concerned	O
with	O
building	O
graph	Method
kernels	Method
.	O

In	O
graph	Method
kernels	Method
,	O
a	O
graph	O
is	O
decomposed	O
into	O
(	O
possibly	O
different	O
)	O
sub	O
-	O
structures	O
.	O

The	O
graph	Method
kernel	Method
is	O
defined	O
based	O
on	O
the	O
frequency	O
of	O
each	O
sub	O
-	O
structure	O
appeared	O
in	O
and	O
,	O
respectively	O
.	O

Namely	O
,	O
,	O
where	O
is	O
the	O
vector	O
containing	O
frequencies	O
of	O
sub	O
-	O
structures	O
,	O
and	O
is	O
an	O
inner	O
product	O
in	O
an	O
appropriately	O
defined	O
normed	O
vector	O
space	O
.	O

Much	O
of	O
work	O
has	O
been	O
devoted	O
to	O
deciding	O
on	O
which	O
sub	O
-	O
structures	O
are	O
more	O
suitable	O
than	O
others	O
.	O

Among	O
the	O
existing	O
graph	Method
kernels	Method
,	O
popular	O
ones	O
are	O
graphlets	Method
,	O
random	Method
walk	Method
and	Method
shortest	Method
path	Method
kernels	Method
,	O
and	O
Weisfeiler	Method
-	Method
Lehman	Method
subtree	Method
kernel	Method
.	O

Furthermore	O
,	O
deep	Method
graph	Method
kernels	Method
,	O
graph	Method
invariant	Method
kernels	Method
,	O
optimal	Method
assignment	Method
graph	Method
kernels	Method
and	O
multiscale	Method
laplacian	Method
graph	Method
kernel	Method
have	O
been	O
proposed	O
with	O
the	O
goal	O
to	O
re	O
-	O
define	O
kernel	O
functions	O
to	O
appropriately	O
capture	O
sub	O
-	O
structural	O
similarity	O
at	O
different	O
levels	O
.	O

Another	O
line	O
of	O
research	O
in	O
this	O
area	O
focuses	O
on	O
efficiently	O
computing	O
these	O
kernels	O
either	O
through	O
exploiting	O
certain	O
structure	O
dependency	O
,	O
or	O
via	O
approximation	Method
or	O
randomization	Method
.	O

The	O
second	O
category	O
involves	O
constructing	O
explicit	O
graph	O
features	O
such	O
as	O
Fgsd	O
features	O
in	O
which	O
is	O
based	O
on	O
a	O
family	O
of	O
graph	Method
spectral	Method
distances	Method
.	O

It	O
comes	O
with	O
certain	O
theoretical	O
guarantees	O
.	O

The	O
Skew	O
Spectrum	O
of	O
Graphs	O
based	O
on	O
group	Method
-	Method
theoretic	Method
approaches	Method
is	O
another	O
example	O
in	O
this	O
category	O
.	O

Graphlet	Method
spectrum	Method
improves	O
upon	O
this	O
work	O
by	O
including	O
labeled	O
information	O
;	O
it	O
also	O
accounts	O
for	O
the	O
relative	O
position	O
of	O
subgraphs	O
within	O
a	O
graph	O
.	O

However	O
,	O
the	O
main	O
concern	O
with	O
graphlet	O
spectrum	O
or	O
skew	Method
spectrum	Method
is	O
its	O
computational	Metric
complexity	Metric
.	O

The	O
third	O
–	O
more	O
recent	O
and	O
perhaps	O
more	O
promising	O
–	O
approach	O
to	O
the	O
graph	Task
classification	Task
is	O
on	O
developing	O
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
for	O
graphs	Material
.	O

The	O
original	O
idea	O
of	O
defining	O
graph	Method
convolution	Method
operations	Method
comes	O
from	O
the	O
graph	Task
signal	Task
processing	Task
domain	Task
,	O
which	O
has	O
since	O
been	O
recognized	O
as	O
the	O
problem	O
of	O
learning	Task
filter	Task
parameters	Task
that	O
appear	O
in	O
the	O
graph	Method
fourier	Method
transform	Method
in	O
the	O
form	O
of	O
a	O
graph	O
Laplacian	O
.	O

Various	O
GCNN	Method
models	Method
such	O
a	O
have	O
been	O
proposed	O
,	O
where	O
traditional	O
graph	Method
filters	Method
are	O
replaced	O
by	O
a	O
self	O
-	O
loop	O
graph	O
adjacency	O
matrix	O
and	O
the	O
outputs	O
of	O
each	O
neural	O
network	O
layer	O
output	O
are	O
computed	O
using	O
a	O
propagation	Method
rule	Method
while	O
updating	O
the	O
network	O
weights	O
.	O

The	O
authors	O
in	O
extend	O
such	O
GCNN	Method
models	Method
by	O
utilizing	O
fast	Method
localized	Method
spectral	Method
filters	Method
and	O
efficient	O
pooling	Method
operations	Method
.	O

A	O
very	O
different	O
approach	O
is	O
proposed	O
in	O
where	O
a	O
set	O
of	O
local	O
nodes	O
are	O
converted	O
into	O
a	O
sequence	O
in	O
order	O
to	O
create	O
receptive	O
fields	O
which	O
are	O
then	O
fed	O
into	O
a	O
1D	Method
convolutional	Method
neural	Method
network	Method
.	O

Another	O
popular	O
name	O
for	O
GCNNs	Method
is	O
message	Method
passing	Method
neural	Method
networks	Method
(	O
MPNNs	Method
)	O
.	O

Though	O
the	O
authors	O
in	O
suggests	O
that	O
GCNNs	Method
are	O
a	O
special	O
case	O
of	O
MPNNs	Method
,	O
we	O
believe	O
that	O
both	O
are	O
equivalent	O
models	O
in	O
a	O
certain	O
sense	O
;	O
it	O
is	O
simply	O
a	O
matter	O
of	O
how	O
the	O
graph	Method
convolution	Method
operation	Method
is	O
defined	O
.	O

In	O
MPNNs	Method
the	O
hidden	O
states	O
of	O
each	O
node	O
is	O
updated	O
based	O
on	O
messages	O
received	O
from	O
its	O
neighbors	O
as	O
well	O
as	O
the	O
values	O
of	O
the	O
previous	O
hidden	O
states	O
in	O
each	O
iteration	O
.	O

This	O
is	O
made	O
possible	O
by	O
replacing	O
traditional	O
neural	Method
networks	Method
in	O
GCNN	Method
with	O
a	O
small	Method
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
with	O
the	O
same	O
weight	O
parameters	O
shared	O
across	O
all	O
nodes	O
in	O
the	O
graph	O
.	O

Note	O
that	O
here	O
the	O
number	O
of	O
iterations	O
in	O
MPNNs	O
can	O
be	O
related	O
to	O
the	O
depth	O
of	O
a	O
GCNN	Method
model	Method
.	O

In	O
the	O
authors	O
propose	O
to	O
condition	O
the	O
learning	O
parameters	O
of	O
filters	Method
based	O
on	O
edges	O
rather	O
than	O
on	O
traditional	O
nodes	O
.	O

This	O
approach	O
is	O
similar	O
to	O
some	O
instances	O
of	O
MPNNs	Method
such	O
as	O
in	O
where	O
learning	O
parameters	O
are	O
also	O
associated	O
with	O
edges	O
.	O

All	O
the	O
above	O
MPNNs	Method
models	Method
employ	O
aggregation	Method
as	O
the	O
graph	Method
permutation	Method
invariant	Method
layer	Method
for	O
solving	O
the	O
graph	Task
classification	Task
problem	Task
.	O

In	O
contrast	O
,	O
the	O
authors	O
in	O
employs	O
a	O
max	Method
-	Method
sort	Method
pooling	Method
layer	Method
and	O
group	Method
theory	Method
to	O
achieve	O
graph	O
permutation	O
invariance	O
.	O

[	O
]	O
[	O
]	O
[	O
]	O
[	O
]	O
[	O
]	O
[	O
]	O
[	O
]	O
Applying	O
Graph	O
Capsule	O
Function	O
at	O
node	O
A	O
Capsule	O
Vector	O
(	O
for	O
example	O
containing	O
moments	O
)	O
section	O
:	O
Graph	Method
Capsule	Method
CNN	Method
Model	Method
Basic	O
Setup	O
and	O
Notations	O
:	O
Consider	O
a	O
graph	O
of	O
size	O
,	O
where	O
is	O
the	O
vertex	O
set	O
,	O
the	O
edge	O
set	O
(	O
with	O
no	O
self	O
-	O
loops	O
)	O
and	O
the	O
weighted	O
adjacency	O
matrix	O
.	O

The	O
standard	O
graph	O
Laplacian	O
is	O
defined	O
as	O
,	O
where	O
is	O
the	O
degree	O
matrix	O
.	O

Let	O
be	O
the	O
node	O
feature	O
matrix	O
,	O
where	O
is	O
the	O
input	O
dimension	O
.	O

When	O
used	O
,	O
we	O
will	O
use	O
to	O
denote	O
the	O
dimension	O
of	O
hidden	O
(	O
latent	O
)	O
variables	O
/	O
feature	O
space	O
.	O

General	Method
GCNN	Method
Model	Method
:	O
We	O
start	O
by	O
describing	O
a	O
general	O
GCNN	Method
model	Method
before	O
presenting	O
our	O
Graph	Method
Capsule	Method
CNN	Method
model	Method
.	O

Let	O
be	O
a	O
graph	O
with	O
graph	O
Laplacian	O
and	O
be	O
a	O
node	O
feature	O
matrix	O
.	O

Then	O
the	O
most	O
general	O
form	O
of	O
a	O
GCNN	Method
layer	Method
output	Method
function	Method
equipped	O
with	O
polynomial	Method
filters	Method
is	O
given	O
by	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
,	O
In	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
,	O
is	O
defined	O
as	O
a	O
graph	Method
convolution	Method
filter	Method
of	O
polynomial	Method
form	Method
with	O
degree	O
.	O

While	O
are	O
learning	O
weight	O
parameters	O
where	O
each	O
.	O

Note	O
that	O
can	O
be	O
seen	O
as	O
a	O
new	O
node	O
feature	O
matrix	O
with	O
extended	O
dimension	O
.	O

Furthermore	O
,	O
can	O
be	O
replaced	O
by	O
any	O
other	O
suitable	O
filter	Method
matrix	Method
as	O
discussed	O
in	O
.	O

A	O
GCNN	Method
model	Method
with	O
a	O
depth	O
of	O
layers	O
can	O
be	O
expressed	O
recursively	O
as	O
,	O
where	O
is	O
the	O
weight	O
parameter	O
matrix	O
for	O
the	O
layer	O
,	O
.	O

One	O
can	O
notice	O
that	O
in	O
any	O
layer	O
the	O
basic	O
computation	O
expression	O
involve	O
is	O
.	O

This	O
expression	O
represents	O
that	O
the	O
new	O
feature	O
value	O
of	O
node	O
(	O
associated	O
with	O
the	O
row	O
)	O
is	O
yielded	O
out	O
as	O
a	O
single	O
(	O
scalar	O
)	O
aggregated	O
value	O
based	O
on	O
its	O
local	O
-	O
hood	O
neighbors	O
.	O

This	O
particular	O
operation	O
can	O
incur	O
significant	O
loss	O
of	O
information	O
.	O

We	O
aim	O
to	O
remedy	O
this	O
issue	O
by	O
introducing	O
our	O
novel	O
GCAPS	Method
-	Method
CNN	Method
model	Method
based	O
on	O
the	O
fundamental	O
capsule	Method
idea	Method
.	O

subsection	O
:	O
Graph	Method
Capsule	Method
Networks	Method
The	O
core	O
idea	O
behind	O
our	O
proposed	O
graph	Method
capsule	Method
convolutional	Method
neural	Method
network	Method
is	O
to	O
capture	O
more	O
information	O
in	O
a	O
local	O
node	O
pool	O
beyond	O
what	O
is	O
captured	O
by	O
aggregation	Method
,	O
the	O
graph	Method
convolution	Method
operation	Method
used	O
in	O
a	O
standard	O
GCCN	Method
model	Method
.	O

This	O
new	O
information	O
is	O
encapsulated	O
in	O
so	O
-	O
called	O
instantiation	O
parameters	O
described	O
in	O
which	O
forms	O
a	O
capsule	O
vector	O
of	O
highly	O
informative	O
outputs	O
.	O

The	O
quality	O
of	O
these	O
parameters	O
are	O
determined	O
by	O
their	O
ability	O
to	O
encode	O
the	O
node	O
feature	O
values	O
in	O
a	O
local	O
neighborhood	O
of	O
each	O
node	O
as	O
well	O
decode	O
(	O
i.e.	O
,	O
to	O
reconstruct	O
)	O
them	O
from	O
the	O
capsule	O
vector	O
.	O

For	O
instance	O
,	O
one	O
can	O
take	O
the	O
histogram	O
of	O
neighborhood	O
feature	O
values	O
as	O
the	O
capsule	O
vector	O
.	O

If	O
histogram	O
bandwidth	O
is	O
sufficiently	O
small	O
,	O
we	O
can	O
guarantee	O
to	O
recover	O
back	O
all	O
the	O
original	O
input	O
node	O
values	O
.	O

This	O
strategy	O
has	O
been	O
used	O
in	O
constructing	O
a	O
successful	O
graph	Method
kernel	Method
.	O

However	O
,	O
as	O
histogram	Method
is	O
not	O
a	O
continuous	O
differentiable	O
function	O
,	O
it	O
can	O
not	O
be	O
employed	O
in	O
backpropagation	Method
for	O
end	Task
-	Task
to	Task
-	Task
end	Task
deep	Task
learning	Task
.	O

Beside	O
seeking	O
representative	O
instantiation	O
parameters	O
,	O
we	O
further	O
impose	O
two	O
more	O
constraints	O
on	O
a	O
graph	O
capsule	O
function	O
.	O

First	O
,	O
we	O
want	O
our	O
graph	O
capsule	O
function	O
to	O
be	O
permutation	O
invariant	O
(	O
unlike	O
equivariant	O
as	O
discussed	O
in	O
)	O
with	O
respect	O
to	O
the	O
input	O
node	O
order	O
since	O
we	O
are	O
interested	O
in	O
a	O
model	O
that	O
can	O
produce	O
the	O
same	O
output	O
for	O
isomorphic	O
graphs	O
.	O

Second	O
,	O
we	O
would	O
like	O
to	O
be	O
able	O
to	O
compute	O
these	O
parameters	O
efficiently	O
.	O

Graph	Method
Capsule	Method
Function	Method
:	O
To	O
describe	O
a	O
general	O
graph	Method
capsule	Method
function	Method
,	O
consider	O
an	O
node	O
with	O
value	O
and	O
the	O
set	O
of	O
its	O
neighborhood	O
node	O
values	O
as	O
including	O
itself	O
.	O

In	O
the	O
standard	O
graph	Method
convolution	Method
operation	Method
,	O
the	O
output	O
is	O
a	O
scalar	O
function	O
which	O
takes	O
input	O
neighbors	O
at	O
the	O
node	O
and	O
yields	O
an	O
output	O
given	O
by	O
where	O
represents	O
edge	O
weights	O
between	O
nodes	O
and	O
.	O

In	O
our	O
graph	Method
capsule	Method
network	Method
,	O
we	O
replace	O
with	O
a	O
vector	Method
-	Method
valued	Method
capsule	Method
function	Method
.	O

For	O
example	O
,	O
consider	O
a	O
capsule	Method
function	Method
that	O
captures	O
higher	O
-	O
order	O
statistical	O
moments	O
as	O
follows	O
(	O
for	O
simplicity	O
,	O
we	O
omit	O
the	O
mean	O
and	O
standard	O
deviation	O
)	O
,	O
Figure	O
[	O
reference	O
]	O
shows	O
an	O
instance	O
of	O
applying	O
our	O
graph	Method
capsule	Method
function	Method
on	O
a	O
specific	O
node	O
.	O

Consequently	O
,	O
for	O
an	O
input	O
feature	O
matrix	O
,	O
our	O
graph	Method
capsule	Method
network	Method
will	O
produce	O
an	O
output	O
where	O
is	O
the	O
number	O
of	O
instantiation	O
parameters	O
.	O

Managing	O
Graph	Method
Capsule	Method
Vector	Method
Dimension	Method
:	O
In	O
the	O
first	O
layer	O
,	O
our	O
graph	Method
capsule	Method
network	Method
receives	O
an	O
input	O
and	O
produces	O
a	O
non	O
-	O
linear	O
output	O
.	O

Since	O
our	O
graph	Method
capsule	Method
function	Method
produces	O
a	O
vector	O
of	O
dimension	O
(	O
for	O
each	O
input	O
dimension	O
)	O
,	O
the	O
feature	O
dimension	O
of	O
the	O
output	O
in	O
subsequent	O
layers	O
can	O
quickly	O
blow	O
up	O
to	O
an	O
unmanageable	O
value	O
.	O

To	O
keep	O
it	O
in	O
check	O
,	O
we	O
restrict	O
the	O
feature	O
dimension	O
of	O
the	O
output	O
to	O
be	O
always	O
at	O
any	O
middle	O
layer	O
of	O
a	O
GCAP	Method
-	Method
CNN	Method
(	O
here	O
represents	O
the	O
hidden	O
dimension	O
of	O
that	O
layer	O
)	O
.	O

This	O
can	O
be	O
accomplished	O
in	O
two	O
ways	O
1	O
)	O
either	O
by	O
flattening	O
the	O
last	O
two	O
dimension	O
of	O
and	O
carrying	O
out	O
graph	Method
convolution	Method
in	O
usual	O
way	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
for	O
an	O
example	O
)	O
2	O
)	O
or	O
by	O
taking	O
the	O
weighted	O
combination	O
of	O
dimension	O
capsule	O
vectors	O
(	O
this	O
is	O
similar	O
to	O
performing	O
attention	Method
mechanism	Method
)	O
at	O
each	O
node	O
as	O
performed	O
in	O
.	O

We	O
leave	O
the	O
second	O
approach	O
for	O
our	O
future	O
work	O
.	O

Thus	O
in	O
a	O
nutshell	O
,	O
our	O
graph	Method
capsule	Method
network	Method
in	O
layer	O
(	O
)	O
receives	O
an	O
input	O
and	O
produces	O
an	O
output	O
.	O

Graph	Method
Capsule	Method
Function	Method
with	O
Statistical	Method
Moments	Method
:	O
In	O
this	O
paper	O
,	O
we	O
consider	O
higher	O
-	O
order	O
statistical	O
moments	O
as	O
instantiation	O
parameters	O
because	O
they	O
are	O
permutationally	O
invariant	O
and	O
can	O
nicely	O
be	O
computed	O
through	O
matrix	Method
-	Method
multiplication	Method
operations	Method
in	O
a	O
fast	O
manner	O
.	O

To	O
see	O
exactly	O
how	O
,	O
let	O
be	O
the	O
output	O
matrix	O
corresponding	O
to	O
dimension	O
.	O

Then	O
,	O
we	O
can	O
compute	O
containing	O
statistical	O
moments	O
as	O
instantiation	O
parameters	O
as	O
follows	O
,	O
where	O
is	O
a	O
hadamard	Method
product	Method
.	O

Here	O
to	O
keep	O
the	O
feature	O
dimensions	O
in	O
check	O
from	O
growing	O
,	O
we	O
flatten	O
the	O
last	O
two	O
dimension	O
of	O
the	O
input	O
as	O
and	O
performs	O
usual	O
graph	Method
convolution	Method
operation	Method
followed	O
by	O
a	O
linear	Method
transformation	Method
with	O
as	O
the	O
learning	O
weight	O
parameter	O
.	O

Note	O
that	O
here	O
is	O
used	O
to	O
denote	O
both	O
the	O
capsule	O
dimension	O
as	O
well	O
the	O
order	O
of	O
statistical	O
moments	O
.	O

Graph	Method
Capsule	Method
Function	Method
with	O
Polynomial	O
Coefficients	O
:	O
As	O
mentioned	O
earlier	O
,	O
the	O
quality	O
of	O
instantiation	O
parameters	O
depend	O
upon	O
their	O
capability	O
to	O
encode	O
and	O
decode	O
the	O
input	O
values	O
.	O

Therefore	O
,	O
we	O
seek	O
capsule	O
functions	O
which	O
are	O
bijective	O
in	O
nature	O
i.e.	O
,	O
guaranteed	O
to	O
preserve	O
everything	O
about	O
the	O
local	O
neighborhood	O
.	O

For	O
instance	O
,	O
one	O
consider	O
coefficients	O
of	O
polynomial	O
as	O
instantiation	O
parameters	O
by	O
taking	O
the	O
set	O
of	O
local	O
node	O
feature	O
values	O
as	O
roots	O
,	O
One	O
can	O
show	O
that	O
from	O
a	O
given	O
full	O
set	O
of	O
polynomial	O
coefficients	O
,	O
we	O
are	O
guaranteed	O
to	O
recover	O
back	O
all	O
the	O
original	O
node	O
values	O
(	O
upto	O
permutation	O
)	O
.	O

However	O
,	O
the	O
first	O
issue	O
with	O
this	O
approach	O
is	O
that	O
they	O
are	O
expensive	O
to	O
compute	O
at	O
each	O
node	O
.	O

Specifically	O
,	O
a	O
combinatorial	Method
algorithm	Method
without	O
fast	Method
fourier	Method
transform	Method
takes	O
complexity	O
to	O
compute	O
where	O
is	O
the	O
number	O
of	O
roots	O
.	O

Also	O
,	O
there	O
is	O
numerical	O
instability	O
issue	O
associated	O
with	O
computing	O
polynomial	O
coefficients	O
.	O

There	O
are	O
ways	O
to	O
deal	O
with	O
these	O
kind	O
issues	O
but	O
we	O
leave	O
pursuing	O
this	O
direction	O
for	O
our	O
future	O
work	O
.	O

In	O
short	O
,	O
our	O
graph	Method
capsule	Method
idea	Method
is	O
powerful	O
and	O
can	O
be	O
employed	O
in	O
any	O
type	O
of	O
GCNN	Method
model	Method
for	O
either	O
solving	O
graph	Task
semi	Task
-	Task
supervised	Task
learning	Task
problem	Task
or	O
performing	O
sequence	Task
learning	Task
on	Task
graphs	Task
using	O
Graph	Method
Recurrent	Method
Neural	Method
Network	Method
models	Method
(	O
GCRNNs	Method
)	O
or	O
doing	O
link	Task
prediction	Task
via	O
Graph	Method
Autoencoders	Method
(	O
GAEs	Method
)	O
or	O
/	O
and	O
for	O
generating	O
synthetic	Task
graphs	Task
through	O
Graph	Method
Generative	Method
Adversarial	Method
models	Method
(	O
GGANs	Method
)	O
.	O

section	O
:	O
Designing	O
Graph	Method
Permutation	Method
Invariant	Method
Layer	Method
In	O
this	O
section	O
,	O
we	O
focus	O
on	O
the	O
second	O
limitation	O
of	O
GCNN	Method
model	Method
regarding	O
achieving	O
permutation	Task
invariance	Task
for	O
graph	Task
classification	Task
purpose	Task
.	O

Before	O
presenting	O
our	O
novel	O
invariant	Method
layer	Method
in	O
GCAPS	Method
-	Method
CNN	Method
model	Method
,	O
we	O
first	O
discuss	O
the	O
shortcomings	O
of	O
Max	Method
-	Method
Sort	Method
Pooling	Method
Layer	Method
which	O
is	O
the	O
next	O
popular	O
choice	O
after	O
aggregation	Task
for	O
achieving	O
invariance	Task
.	O

subsection	O
:	O
Problems	O
with	O
Max	Method
-	Method
Sort	Method
Pooling	Method
Layer	Method
We	O
design	O
a	O
test	O
to	O
determine	O
whether	O
the	O
invariant	O
graph	O
feature	O
constructed	O
by	O
a	O
model	O
has	O
any	O
degree	O
of	O
certainty	O
to	O
produce	O
the	O
same	O
output	O
for	O
sub	O
-	O
graph	O
isomers	O
or	O
not	O
.	O

Sub	Task
-	Task
Graph	Task
Isomorphism	Task
Feature	Task
Test	Task
:	O
Consider	O
two	O
graphs	O
and	O
such	O
that	O
is	O
isomorphic	O
to	O
a	O
sub	O
-	O
graph	O
of	O
.	O

Let	O
be	O
the	O
invariant	O
feature	O
vector	O
(	O
w.r.t	O
.	O

to	O
graph	O
isomorphism	O
)	O
of	O
respectively	O
.	O

Then	O
,	O
we	O
define	O
sub	Metric
-	Metric
graph	Metric
isomorphism	Metric
feature	Metric
test	Metric
as	O
a	O
criteria	O
providing	O
guarantee	O
that	O
each	O
elements	O
of	O
and	O
are	O
comparable	O
under	O
certain	O
notion	O
i.e.	O
,	O
for	O
any	O
.	O

Here	O
represents	O
a	O
comparison	O
operator	O
defined	O
in	O
a	O
sensible	O
way	O
.	O

Satisfying	O
this	O
test	O
is	O
very	O
desirable	O
for	O
graph	Task
classification	Task
problem	Task
since	O
it	O
is	O
quite	O
likely	O
that	O
sub	O
-	O
graph	O
isomers	O
of	O
a	O
graph	O
belong	O
to	O
the	O
same	O
class	O
label	O
.	O

This	O
property	O
helps	O
the	O
model	O
to	O
learn	O
weight	O
parameter	O
appropriately	O
which	O
is	O
shared	O
across	O
the	O
same	O
input	O
place	O
i.e.	O
,	O
and	O
.	O

theorem	O
:	O
Let	O
∈f1	O
,	O
f2Rk	O
be	O
the	O
feature	O
vectors	O
containing	O
top	O
-	O
kmax	O
node	O
values	O
in	O
sorted	O
order	O
for	O
graphs	O
G1	O
,	O
G2	O
respectively	O
and	O
given	O
G1	O
is	O
sub	O
-	O
graph	O
isomorphic	O
to	O
G2	O
.	O

Then	O
the	O
Max	Method
-	Method
Sort	Method
Pooling	Method
Layer	Method
fails	O
the	O
Sub	Task
-	Task
graph	Task
Isomorphism	Task
Feature	Task
Test	Task
owing	O
to	O
the	O
comparison	O
done	O
with	O
respect	O
to	O
node	O
ordering	O
.	O

Remarks	O
:	O
Max	Method
-	Method
Sort	Method
Pooling	Method
layer	Method
fails	O
the	O
test	O
because	O
it	O
does	O
not	O
guarantee	O
that	O
for	O
any	O
.	O

Here	O
(	O
not	O
comparable	O
)	O
operator	O
represents	O
that	O
the	O
node	O
corresponding	O
to	O
values	O
and	O
may	O
not	O
be	O
the	O
same	O
in	O
sub	O
-	O
graph	O
isomers	O
.	O

Even	O
including	O
a	O
single	O
node	O
(	O
value	O
)	O
in	O
vector	O
which	O
is	O
not	O
present	O
in	O
can	O
mess	O
up	O
the	O
whole	O
comparision	O
order	O
of	O
and	O
elements	O
.	O

As	O
a	O
result	O
,	O
in	O
Max	Method
-	Method
Sort	Method
Pooling	Method
layer	Method
the	O
comparison	O
is	O
not	O
always	O
guaranteed	O
to	O
be	O
sensible	O
which	O
makes	O
the	O
problem	O
of	O
learning	O
weight	O
parameters	O
harder	O
.	O

In	O
general	O
,	O
any	O
invariant	Method
graph	Method
feature	Method
vector	Method
that	O
relies	O
on	O
node	O
ordering	O
will	O
fail	O
this	O
test	O
.	O

subsection	O
:	O
Covariance	Method
as	O
Permutation	Method
Invariant	Method
Layer	Method
Our	O
novel	O
idea	O
of	O
permutation	O
invariant	O
features	O
in	O
GCAPS	Method
-	Method
CNN	Method
model	Method
is	O
computing	O
the	O
covariance	O
of	O
layer	O
output	O
given	O
as	O
follows	O
,	O
Here	O
is	O
the	O
mean	O
of	O
output	O
and	O
is	O
a	O
covariance	O
function	O
.	O

Since	O
covariance	O
function	O
is	O
differentiable	O
and	O
does	O
not	O
depends	O
upon	O
the	O
order	O
of	O
row	O
elements	O
,	O
it	O
can	O
serve	O
as	O
a	O
permutation	Method
invariant	Method
layer	Method
in	O
GCAPS	Method
-	Method
CNN	Method
model	Method
.	O

Also	O
,	O
it	O
is	O
fast	O
in	O
computation	Task
due	O
to	O
a	O
single	O
matrix	Method
-	Method
multiplication	Method
operation	Method
.	O

Note	O
that	O
we	O
flatten	O
the	O
last	O
two	O
dimension	O
of	O
GCAPS	O
-	O
CNN	O
layer	O
output	O
in	O
order	O
to	O
compute	O
the	O
covariance	O
.	O

Moreover	O
,	O
covariance	O
provides	O
much	O
richer	O
information	O
about	O
the	O
data	O
by	O
including	O
shapes	O
,	O
norms	O
and	O
angles	O
(	O
between	O
node	O
hidden	O
features	O
)	O
information	O
rather	O
than	O
just	O
providing	O
the	O
mean	O
of	O
data	O
.	O

Infact	O
in	O
multivariate	Method
normal	Method
distribution	Method
,	O
it	O
is	O
used	O
as	O
a	O
statistical	O
parameter	O
to	O
approximate	O
the	O
normal	O
density	O
and	O
thus	O
also	O
reflects	O
information	O
about	O
the	O
data	O
distribution	O
.	O

This	O
particular	O
property	O
along	O
with	O
invariance	O
has	O
been	O
exploited	O
before	O
in	O
for	O
computing	O
similarity	Task
between	O
two	O
set	O
of	O
vectors	O
.	O

One	O
can	O
also	O
think	O
about	O
fitting	O
multivariate	O
normal	O
distribution	O
on	O
but	O
it	O
involves	O
computing	O
inverse	O
of	O
covariance	O
matrix	O
which	O
is	O
computationally	O
expensive	O
.	O

Since	O
each	O
element	O
of	O
covariance	O
matrix	O
is	O
invariant	O
to	O
node	O
orders	O
,	O
we	O
can	O
flatten	O
the	O
symmetric	O
covariance	O
matrix	O
to	O
construct	O
the	O
graph	O
invariant	O
feature	O
vector	O
.	O

On	O
an	O
another	O
positive	O
note	O
,	O
here	O
the	O
output	O
dimension	O
of	O
does	O
not	O
depend	O
upon	O
number	O
of	O
nodes	O
and	O
can	O
be	O
adjusted	O
according	O
to	O
computational	O
constraints	O
.	O

theorem	O
:	O
Let	O
∈f1	O
,	O
f2Rk	O
be	O
the	O
feature	O
vectors	O
containing	O
covariance	O
elements	O
of	O
node	O
feature	O
matrices	O
for	O
graphs	O
G1	O
,	O
G2	O
respectively	O
and	O
given	O
G1	O
is	O
sub	O
-	O
graph	O
isomorphic	O
to	O
G2	O
.	O

Then	O
the	O
covariance	Method
invariant	Method
layer	Method
pass	O
the	O
Sub	Task
-	Task
Graph	Task
Isomorphism	Task
Feature	Task
Test	Task
owing	O
to	O
the	O
comparison	O
done	O
with	O
respect	O
to	O
feature	O
dimensions	O
.	O

Remarks	O
:	O
It	O
is	O
quite	O
straightforward	O
to	O
see	O
that	O
the	O
feature	O
dimension	O
order	O
of	O
a	O
node	O
does	O
not	O
depend	O
upon	O
the	O
graph	O
node	O
ordering	O
and	O
hence	O
the	O
order	O
is	O
same	O
across	O
all	O
graphs	O
.	O

As	O
a	O
result	O
,	O
each	O
elements	O
of	O
and	O
are	O
always	O
comparable	O
.	O

To	O
be	O
more	O
specific	O
,	O
covariance	O
output	O
compares	O
both	O
the	O
norms	O
sand	O
angles	O
between	O
the	O
corresponding	O
pairs	O
of	O
feature	O
dimension	O
vectors	O
in	O
two	O
graphs	O
.	O

section	O
:	O
Designing	O
GCAP	Method
-	Method
CNN	Method
with	O
Global	Method
Features	Method
Besides	O
guaranteeing	O
permutation	O
invariance	O
in	O
GCAP	Method
-	Method
CNN	Method
model	Method
,	O
another	O
important	O
desired	O
characteristic	O
of	O
graph	Method
classification	Method
model	Method
is	O
to	O
capture	O
global	O
structure	O
(	O
or	O
features	O
)	O
of	O
a	O
graph	O
.	O

For	O
instance	O
,	O
considering	O
only	O
node	O
degree	O
(	O
as	O
a	O
node	O
feature	O
)	O
is	O
a	O
local	O
information	O
and	O
not	O
much	O
helpful	O
towards	O
solving	O
graph	Task
classification	Task
problem	Task
.	O

On	O
the	O
other	O
hand	O
,	O
considering	O
spectral	Method
embedding	Method
as	O
a	O
node	O
feature	O
takes	O
global	O
piece	O
of	O
information	O
into	O
account	O
and	O
have	O
been	O
proven	O
successful	O
in	O
serving	O
as	O
a	O
node	O
vector	O
for	O
problems	O
dealing	O
with	O
graph	Task
semi	Task
-	Task
supervised	Task
learning	Task
.	O

We	O
define	O
global	O
features	O
that	O
takes	O
full	O
graph	O
structure	O
into	O
account	O
during	O
their	O
computation	O
.	O

While	O
local	O
features	O
only	O
depend	O
upon	O
some	O
(	O
at	O
-	O
most	O
)	O
hop	O
node	O
neighbors	O
.	O

Unfortunately	O
,	O
the	O
basic	O
design	O
of	O
GCNN	Method
model	Method
can	O
only	O
capture	O
local	O
structure	O
information	O
of	O
the	O
graph	O
at	O
each	O
node	O
.	O

We	O
make	O
this	O
loose	O
statement	O
more	O
concrete	O
with	O
the	O
following	O
theorem	O
.	O

theorem	O
:	O
:	O
Let	O
G	O
be	O
a	O
graph	O
with	O
∈LR×NN	O
graph	O
Laplacian	O
and	O
∈XR×Nd	O
node	O
feature	O
matrix	O
.	O

Let	O
⁢f	O
(	O
ℓ	O
)(	O
X	O
,	O
L	O
)	O
be	O
the	O
output	O
function	O
of	O
a	O
ℓ⁢th	Method
GCNN	Method
layer	Method
equipped	O
with	O
polynomial	Method
filters	Method
of	O
degree	O
k.	O
Then	O
[	O
⁢f	O
(	O
ℓ	O
)(	O
X	O
,	O
L	O
)]	O
i	O
output	O
at	O
i⁢th	O
node	O
(	O
i.e.	O
,	O
i⁢th	O
row	O
in	O
⁢f	O
(	O
ℓ	O
)(	O
⋅	O
)	O
)	O
depends	O
upon	O
“	O
only	O
”	O
on	O
the	O
input	O
values	O
of	O
neighbors	O
distant	O
at	O
most	O
“	O
⁢kℓ	O
-	O
hops	O
”	O
away	O
.	O

Proof	O
:	O
We	O
can	O
proof	O
this	O
statement	O
by	O
mathematical	Task
induction	Task
.	O

It	O
is	O
easy	O
to	O
see	O
that	O
the	O
base	O
case	O
holds	O
true	O
.	O

Lets	O
assume	O
it	O
also	O
holds	O
true	O
for	O
i.e.	O
,	O
node	O
output	O
depends	O
upon	O
neighbors	O
distant	O
upto	O
hop	O
away	O
.	O

Then	O
in	O
we	O
focus	O
on	O
the	O
term	O
,	O
particularly	O
the	O
last	O
term	O
involving	O
.	O

Matrix	Method
multiplication	Method
of	O
with	O
will	O
result	O
in	O
node	O
to	O
include	O
all	O
node	O
information	O
which	O
are	O
at	O
-	O
most	O
hop	O
distance	O
away	O
.	O

But	O
since	O
a	O
node	O
in	O
at	O
a	O
distance	O
hops	O
(	O
from	O
node	O
)	O
can	O
contain	O
information	O
upto	O
hops	O
,	O
we	O
have	O
node	O
containing	O
information	O
at	O
-	O
most	O
hops	O
distance	O
away	O
.	O

Remarks	O
:	O
Above	O
theorem	O
[	O
reference	O
]	O
establishes	O
that	O
GCNN	Method
model	Method
with	O
layers	Method
can	O
capture	O
only	O
hop	O
local	O
-	O
hood	O
structure	O
information	O
at	O
each	O
node	O
.	O

Thus	O
,	O
employing	O
GCNN	Method
for	O
graph	Task
classification	Task
with	O
say	O
aggregation	Method
layer	Method
can	O
capture	O
only	O
average	O
variation	O
of	O
hop	O
local	O
-	O
hood	O
information	O
over	O
the	O
whole	O
graph	O
.	O

To	O
include	O
more	O
global	O
information	O
about	O
the	O
graph	O
one	O
can	O
either	O
increase	O
(	O
i.e	O
,	O
choose	O
higher	O
order	Method
graph	Method
convolution	Method
filters	Method
)	O
or	O
(	O
i.e	O
,	O
the	O
depth	Method
of	Method
GCNN	Method
model	Method
)	O
.	O

Both	O
these	O
choices	O
increases	O
model	Metric
complexity	Metric
and	O
thus	O
would	O
require	O
more	O
data	O
samples	O
to	O
reach	O
satisfying	O
results	O
.	O

However	O
among	O
the	O
two	O
,	O
we	O
prefer	O
increasing	O
the	O
depth	O
of	O
GCNN	Method
model	Method
because	O
the	O
first	O
choice	O
leads	O
to	O
increase	O
in	O
the	O
breadth	O
of	O
the	O
GCNN	Method
layer	Method
(	O
see	O
footnote	O
[	O
reference	O
]	O
about	O
in	O
Section	O
[	O
reference	O
]	O
)	O
and	O
based	O
on	O
the	O
current	O
understanding	O
of	O
deep	Method
learning	Method
theory	Method
,	O
increasing	O
the	O
depth	O
is	O
favored	O
more	O
over	O
the	O
breadth	O
.	O

For	O
cases	O
where	O
graph	O
node	O
features	O
are	O
missing	O
,	O
it	O
is	O
a	O
common	O
practice	O
to	O
take	O
node	O
degree	O
as	O
a	O
node	O
feature	O
.	O

Such	O
practices	O
can	O
work	O
for	O
problems	O
like	O
graph	Task
semi	Task
-	Task
supervised	Task
where	O
local	O
-	O
structure	O
information	O
drives	O
node	O
output	O
labels	O
(	O
or	O
classes	O
)	O
.	O

But	O
in	O
graph	Task
classification	Task
global	Task
features	Task
governs	O
the	O
output	O
labels	O
and	O
hence	O
taking	O
node	O
degree	O
is	O
not	O
sufficient	O
.	O

Of	O
course	O
,	O
we	O
can	O
go	O
for	O
a	O
very	O
deep	Method
GCNN	Method
model	Method
that	O
will	O
allows	O
us	O
to	O
exploit	O
more	O
global	O
information	O
but	O
requires	O
higher	O
sample	Metric
complexity	Metric
to	O
achieve	O
satisfying	O
results	O
.	O

To	O
balance	O
the	O
two	O
(	O
model	Metric
complexity	Metric
with	O
depth	O
vs.	O
required	O
sample	Metric
complexity	Metric
)	O
,	O
we	O
propose	O
to	O
incorporate	O
Fgsd	Method
features	Method
in	O
our	O
GCAP	Method
-	Method
CNN	Method
model	Method
computed	O
at	O
each	O
node	O
.	O

As	O
shown	O
in	O
Fgsd	O
features	O
capture	O
global	O
information	O
about	O
the	O
graph	O
and	O
can	O
also	O
be	O
computed	O
in	O
fast	O
manner	O
.	O

Specifically	O
,	O
at	O
each	O
node	O
Fgsd	O
features	O
are	O
computed	O
as	O
the	O
histogram	O
of	O
the	O
multi	O
-	O
set	O
formed	O
by	O
taking	O
the	O
harmonic	O
distance	O
between	O
all	O
nodes	O
and	O
the	O
node	O
.	O

It	O
is	O
given	O
by	O
,	O
where	O
is	O
the	O
harmonic	O
distance	O
,	O
are	O
any	O
graph	O
nodes	O
and	O
is	O
the	O
eigenvalue	O
and	O
eigenvector	O
respectively	O
.	O

In	O
our	O
experiments	O
,	O
we	O
employ	O
these	O
features	O
only	O
for	O
datasets	O
where	O
node	O
feature	O
are	O
missing	O
(	O
specifically	O
for	O
social	Material
network	Material
datasets	Material
in	O
our	O
case	O
)	O
.	O

Although	O
this	O
strategy	O
can	O
always	O
be	O
used	O
by	O
concatenating	O
Fgsd	O
features	O
with	O
original	O
node	O
feature	O
values	O
to	O
capture	O
more	O
global	O
information	O
.	O

Further	O
inspired	O
from	O
Weisfeiler	Method
-	Method
lehman	Method
graph	Method
kernel	Method
which	O
also	O
concatenate	O
features	O
in	O
each	O
labeling	O
iteration	O
,	O
we	O
also	O
propose	O
to	O
pass	O
concatenated	O
outputs	O
from	O
intermediate	O
layers	O
to	O
our	O
covariance	Method
and	Method
fully	Method
connected	Method
layers	Method
.	O

Finally	O
,	O
our	O
whole	O
end	Method
-	Method
to	Method
-	Method
end	Method
GCAP	Method
-	Method
CNN	Method
learning	Method
model	Method
is	O
guaranteed	O
to	O
produce	O
the	O
same	O
output	O
for	O
isomorphic	O
graphs	O
.	O

section	O
:	O
Experiment	O
and	O
Results	O
Deep	Method
Learning	Method
Methods	Method
Graph	Method
Kernels	Method
GCAPS	Method
-	Method
CNN	Method
Model	Method
Configuration	Method
:	O
We	O
build	O
layer	Method
GCAPS	Method
-	Method
CNN	Method
with	O
following	O
configuration	O
:	O
.	O

Here	O
represents	O
a	O
Graph	Method
Capsule	Method
CNN	Method
layer	Method
with	O
hidden	O
dimensions	O
and	O
instantiation	O
parameters	O
.	O

As	O
mentioned	O
earlier	O
,	O
we	O
take	O
the	O
intermediate	O
output	O
of	O
each	O
layers	O
and	O
form	O
a	O
concatenated	Method
tensor	Method
which	O
is	O
subsequently	O
pass	O
through	O
layer	O
which	O
computes	O
mean	O
and	O
covariance	O
of	O
the	O
input	O
.	O

Output	O
of	O
layer	O
is	O
then	O
passed	O
to	O
two	O
fully	Method
connected	Method
layers	Method
with	O
again	O
output	O
dimensions	O
and	O
finally	O
connects	O
to	O
a	O
softmax	Method
layer	Method
for	O
computing	O
class	O
probabilities	O
.	O

In	O
between	O
intermediate	O
layers	O
,	O
we	O
use	O
batch	Method
normalization	Method
and	O
dropout	Method
technique	Method
to	O
prevent	O
overfitting	O
along	O
with	O
norm	Method
regularization	Method
.	O

We	O
set	O
depending	O
upon	O
the	O
dataset	O
size	O
(	O
towards	O
higher	O
for	O
larger	O
dataset	O
)	O
and	O
for	O
setting	O
hidden	O
dimension	O
.	O

We	O
restrict	O
for	O
computing	O
higher	O
-	O
order	O
statistical	O
moments	O
due	O
to	O
computational	O
constraints	O
.	O

Further	O
,	O
we	O
employ	O
ADAM	Method
optimization	Method
technique	Method
with	O
initial	O
learning	Metric
rate	Metric
chosen	O
from	O
the	O
set	O
with	O
a	O
decaying	O
factor	O
of	O
after	O
every	O
few	O
epochs	O
.	O

Batch	O
size	O
is	O
set	O
according	O
to	O
the	O
given	O
dataset	O
size	O
and	O
memory	O
requirements	O
.	O

Number	O
of	O
epochs	O
are	O
chosen	O
from	O
the	O
set	O
.	O

All	O
the	O
above	O
mentioned	O
hyper	O
-	O
parameters	O
are	O
tuned	O
based	O
on	O
the	O
training	Metric
loss	Metric
.	O

Average	Metric
classification	Metric
accuracy	Metric
based	O
on	O
fold	Metric
cross	Metric
validation	Metric
error	Metric
is	O
reported	O
for	O
each	O
dataset	O
.	O

Our	O
GCAPS	Method
-	Method
CNN	Method
code	Method
and	O
data	O
will	O
be	O
made	O
available	O
at	O
Github	O
.	O

Datasets	O
:	O
To	O
evaluate	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
,	O
we	O
perform	O
graph	Task
classification	Task
tasks	Task
on	O
variety	O
of	O
benchmark	Material
datasets	Material
.	O

In	O
first	O
round	O
,	O
we	O
used	O
bioinformatics	Material
datasets	Material
namely	O
:	O
PTC	Material
,	O
PROTEINS	Material
,	O
NCI1	Material
,	O
NCI109	Material
,	O
D	O
&	O
D	O
,	O
and	O
ENZYMES	O
.	O

In	O
second	O
round	O
,	O
we	O
used	O
social	Material
network	Material
datasets	Material
namely	O
:	O
COLLAB	Material
,	O
IMDB	Material
-	Material
BINARY	Material
,	O
IMDB	Material
-	Material
MULTI	Material
,	O
REDDIT	Material
-	Material
BINARY	Material
and	Material
REDDIT	Material
-	Material
MULTI	Material
-	Material
5K.	Material
D	Material
&	Material
D	Material
dataset	Material
contains	O
enzymes	O
and	O
non	O
-	O
enzymes	O
proteins	O
structures	O
.	O

For	O
other	O
datasets	O
details	O
can	O
be	O
found	O
in	O
.	O

Also	O
for	O
each	O
dataset	O
number	O
of	O
graphs	O
,	O
maximum	O
and	O
average	O
number	O
of	O
nodes	O
is	O
shown	O
in	O
the	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O

Deep	Method
Learning	Method
Methods	Method
Graph	Method
Kernels	Method
Experimental	O
Set	O
-	O
up	O
:	O
All	O
experiments	O
were	O
performed	O
on	O
a	O
single	O
machine	O
loaded	O
with	O
recently	O
launched	O
NVIDIA	Material
TITAN	Material
VOLTA	Material
GPUs	Material
and	O
GB	Material
RAM	Material
.	O

We	O
compare	O
our	O
method	O
with	O
both	O
deep	Method
learning	Method
models	Method
and	O
graph	Method
kernels	Method
.	O

Deep	Method
Learning	Method
Baselines	Method
:	O
For	O
deep	Method
learning	Method
approaches	Method
,	O
we	O
adopted	O
recently	O
proposed	O
state	O
-	O
of	O
-	O
art	O
graph	Method
convolutional	Method
neural	Method
networks	Method
namely	O
:	O
PATCHY	Method
-	Method
SAN	Method
(	Method
PSCN	Method
)	O
,	O
Diffusion	Method
CNNs	Method
(	O
DCNN	Method
)	O
[	O
]	O
,	O
Dynamic	Method
Edge	Method
CNN	Method
(	O
ECC	Method
)	O
and	O
Deep	Method
Graph	Method
CNN	Method
(	Method
DGCNN	Method
)	Method
.	O

Graph	Method
Kernel	Method
Baselines	Method
:	O
We	O
adopted	O
state	O
-	O
of	O
-	O
art	O
graphs	Method
kernels	Method
for	O
comparison	O
namely	O
:	O
Random	Method
Walk	Method
(	Method
RW	Method
)	O
,	O
Shortest	Method
Path	Method
Kernel	Method
(	O
SP	Method
)	O
,	O
Graphlet	Method
Kernel	Method
(	O
GK	Method
)	O
,	O
Weisfeiler	Method
-	Method
Lehman	Method
Sub	Method
-	Method
tree	Method
Kernel	Method
(	O
WL	Method
)	O
,	O
Deep	Method
Graph	Method
Kernels	Method
(	O
DGK	Method
)	O
and	O
Multiscale	Method
Laplacian	Method
Graph	Method
Kernels	Method
(	O
MLK	Method
)	O
.	O

Baselines	O
Settings	O
:	O
We	O
adopted	O
the	O
same	O
procedure	O
from	O
previous	O
works	O
to	O
make	O
a	O
fair	O
comparison	O
and	O
used	O
-	O
fold	O
cross	Method
validation	Method
with	O
LIBSVM	Method
library	Method
to	O
report	O
the	O
classification	Metric
performance	Metric
for	O
graph	Method
kernels	Method
.	O

Parameters	O
of	O
SVM	Method
are	O
independently	O
tuned	O
using	O
training	O
folds	O
data	O
and	O
best	O
average	O
classification	Metric
accuracies	Metric
are	O
reported	O
for	O
each	O
method	O
.	O

For	O
Random	Method
-	Method
Walk	Method
(	Method
RW	Method
)	Method
kernel	Method
,	O
decay	O
factor	O
is	O
chosen	O
from	O
.	O

For	O
Weisfeiler	Method
-	Method
Lehman	Method
(	Method
WL	Method
)	Method
kernel	Method
,	O
we	O
chose	O
height	O
of	O
subtree	Method
kernel	Method
from	O
.	O

For	O
graphlet	Method
kernel	Method
(	O
GK	Method
)	O
,	O
we	O
chose	O
graphlets	O
size	O
and	O
for	O
deep	Method
graph	Method
kernels	Method
(	O
DGK	Method
)	O
,	O
we	O
report	O
the	O
best	O
classification	Metric
accuracy	Metric
obtained	O
among	O
:	O
deep	Method
graphlet	Method
kernel	Method
,	O
deep	Method
shortest	Method
path	Method
kernel	Method
and	O
deep	Method
Weisfeiler	Method
-	Method
Lehman	Method
kernel	Method
.	O

For	O
Multiscale	Method
Laplacian	Method
Graph	Method
(	Method
MLG	Method
)	Method
kernel	Method
,	O
we	O
chose	O
and	O
parameter	O
of	O
the	O
algorithm	O
from	O
,	O
radius	O
size	O
from	O
,	O
and	O
level	O
number	O
from	O
.	O

For	O
diffusion	Method
-	Method
convolutional	Method
neural	Method
networks	Method
(	O
DCNN	Method
)	O
,	O
we	O
chose	O
number	O
of	O
hops	O
from	O
.	O

For	O
the	O
rest	O
,	O
best	O
reported	O
results	O
were	O
borrowed	O
from	O
papers	O
PATCHY	O
-	O
SAN	O
(	O
)	O
,	O
ECC	Method
(	O
without	O
edge	O
labels	O
since	O
all	O
other	O
methods	O
also	O
relies	O
on	O
only	O
node	O
labels	O
)	O
and	O
DGCNN	Method
(	O
with	O
sorting	Method
layer	Method
)	O
,	O
since	O
the	O
experimental	O
setup	O
was	O
the	O
same	O
and	O
a	O
fair	O
comparison	O
can	O
be	O
made	O
.	O

In	O
short	O
,	O
we	O
follow	O
the	O
same	O
procedure	O
as	O
mentioned	O
in	O
previous	O
papers	O
.	O

Note	O
:	O
some	O
results	O
are	O
not	O
present	O
because	O
either	O
they	O
are	O
not	O
previously	O
reported	O
or	O
source	O
code	O
not	O
available	O
to	O
run	O
them	O
.	O

Graph	Task
Classification	Task
Results	O
:	O
From	O
Table	O
[	O
reference	O
]	O
,	O
it	O
is	O
clear	O
that	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
consistently	O
outperforms	O
most	O
of	O
the	O
considered	O
deep	Method
learning	Method
methods	Method
on	O
bioinformatics	Material
datasets	Material
(	O
except	O
on	O
D	Material
&	Material
D	Material
dataset	Material
)	O
with	O
a	O
significant	O
margin	O
of	O
classification	Metric
accuracy	Metric
gain	Metric
(	O
highest	O
being	O
on	O
NCI1	Material
dataset	Material
)	O
.	O

Again	O
,	O
this	O
trend	O
is	O
continued	O
to	O
be	O
the	O
same	O
on	O
social	Material
network	Material
datasets	Material
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Here	O
,	O
we	O
were	O
able	O
to	O
achieve	O
upto	O
accuracy	Metric
gain	O
on	O
COLLAB	Material
dataset	Material
and	O
rest	O
were	O
around	O
gain	O
with	O
consistency	O
when	O
compared	O
against	O
other	O
deep	Method
learning	Method
approaches	Method
.	O

Our	O
GCAPS	Method
-	Method
CNN	Method
is	O
also	O
very	O
competitive	O
with	O
state	O
-	O
of	O
-	O
art	O
graph	Method
kernel	Method
methods	Method
.	O

It	O
again	O
show	O
a	O
consistent	O
performance	O
gain	O
of	O
accuracy	Metric
(	O
highest	O
being	O
on	O
PTC	Material
dataset	Material
)	O
on	O
many	O
bioinformatic	Material
datasets	Material
when	O
compared	O
against	O
with	O
strong	Method
graph	Method
kernels	Method
.	O

While	O
other	O
considered	O
deep	Method
learning	Method
methods	Method
are	O
not	O
even	O
close	O
enough	O
to	O
beat	O
graph	Method
kernels	Method
on	O
many	O
of	O
these	O
datasets	O
.	O

It	O
is	O
worth	O
mentioning	O
that	O
the	O
most	O
deep	Method
learning	Method
models	Method
(	O
like	O
ours	O
)	O
are	O
also	O
scalable	O
while	O
graph	Method
kernels	Method
are	O
more	O
fine	O
tuned	O
towards	O
handling	O
small	O
graphs	O
.	O

For	O
social	Material
network	Material
datasets	Material
,	O
we	O
have	O
a	O
significant	O
gain	O
of	O
atleast	Metric
accuracy	Metric
(	O
highest	O
being	O
on	O
REDDIT	Material
-	Material
MULTI	Material
dataset	Material
)	O
against	O
graph	Method
kernels	Method
as	O
observed	O
in	O
Table	O
[	O
reference	O
]	O
.	O

But	O
this	O
is	O
expected	O
as	O
deep	Method
learning	Method
methods	Method
tend	O
to	O
do	O
better	O
with	O
the	O
large	O
amount	O
of	O
data	O
available	O
for	O
training	O
on	O
social	Material
networks	Material
datasets	Material
.	O

Altogether	O
,	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
shows	O
very	O
promising	O
results	O
against	O
both	O
the	O
current	O
state	O
-	O
of	O
-	O
art	O
deep	Method
learning	Method
methods	Method
and	O
graph	Method
kernels	Method
.	O

section	O
:	O
Conclusion	O
&	O
Future	O
Work	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
Graph	Method
Capsule	Method
Network	Method
(	Method
GCAPS	Method
-	Method
CNN	Method
)	Method
model	Method
based	O
on	O
the	O
fundamental	O
capsule	Method
idea	Method
to	O
address	O
some	O
of	O
the	O
basic	O
weaknesses	O
of	O
existing	O
GCNN	Method
models	Method
.	O

Our	O
graph	Method
capsule	Method
network	Method
model	Method
by	O
design	O
captures	O
more	O
local	O
structure	O
information	O
than	O
traditional	O
GCNN	Method
and	O
can	O
provide	O
much	O
richer	O
representation	O
of	O
individual	O
graph	O
nodes	O
or	O
for	O
the	O
whole	O
graph	O
.	O

For	O
our	O
purpose	O
,	O
we	O
employ	O
a	O
capsule	Method
function	Method
that	O
preserves	O
statistical	O
moments	O
formation	O
since	O
they	O
are	O
faster	O
to	O
compute	O
.	O

Furthermore	O
,	O
we	O
propose	O
a	O
novel	O
permutation	Method
invariant	Method
layer	Method
based	O
on	O
computing	O
covariance	O
in	O
our	O
GCAPS	Method
-	Method
CNN	Method
architecture	Method
to	O
deal	O
with	O
graph	Task
classification	Task
problem	Task
which	O
most	O
GCNN	Method
models	Method
find	O
challenging	O
.	O

This	O
covariance	O
can	O
again	O
be	O
computed	O
in	O
a	O
fast	O
manner	O
and	O
has	O
shown	O
to	O
be	O
better	O
than	O
adopting	O
aggregation	Method
or	Method
max	Method
-	Method
sort	Method
pooling	Method
layer	Method
.	O

On	O
the	O
top	O
,	O
we	O
also	O
propose	O
to	O
equip	O
our	O
GCAPS	Method
-	Method
CNN	Method
model	Method
with	O
Fgsd	O
features	O
explicitly	O
to	O
capture	O
more	O
global	O
information	O
in	O
absence	O
of	O
node	O
features	O
.	O

This	O
is	O
essential	O
to	O
consider	O
since	O
non	Method
-	Method
deep	Method
GCNN	Method
models	Method
are	O
not	O
capable	O
enough	O
to	O
exploit	O
global	O
information	O
implicitly	O
.	O

Finally	O
,	O
we	O
show	O
GCAPS	Method
-	Method
CNN	Method
superior	O
performance	O
on	O
many	O
bioinformatics	Material
and	Material
social	Material
network	Material
datasets	Material
in	O
comparison	O
with	O
existing	O
deep	Method
learning	Method
methods	Method
as	O
well	O
as	O
strong	O
graph	Method
kernels	Method
and	O
set	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Our	O
general	O
idea	O
of	O
graph	Method
capsule	Method
is	O
quite	O
rich	O
and	O
can	O
taken	O
to	O
another	O
level	O
by	O
designing	O
more	O
sophisticated	O
capsule	Method
functions	Method
that	O
are	O
capable	O
of	O
preserving	O
more	O
information	O
in	O
a	O
local	O
pool	O
.	O

In	O
our	O
future	O
work	O
,	O
we	O
will	O
investigate	O
various	O
other	O
capsule	Method
functions	Method
such	O
as	O
polynomial	O
coefficients	O
(	O
as	O
instantiation	O
parameters	O
)	O
which	O
comes	O
with	O
theoretical	O
guarantees	O
.	O

Another	O
choice	O
,	O
we	O
will	O
investigate	O
is	O
performing	O
kernel	Method
density	Method
estimation	Method
technique	Method
in	O
end	Task
-	Task
to	Task
-	Task
end	Task
deep	Task
learning	Task
framework	Task
and	O
understanding	O
their	O
theoretical	O
significance	O
.	O

Lastly	O
,	O
we	O
will	O
also	O
explore	O
the	O
other	O
approach	O
of	O
managing	O
the	O
graph	O
capsule	O
vector	O
dimension	O
as	O
discussed	O
in	O
.	O

section	O
:	O
Acknowledgement	O
The	O
research	O
was	O
supported	O
in	O
part	O
by	O
US	O
DoD	O
DTRA	O
grants	O
HDTRA1	O
-	O
09	O
-	O
1	O
-	O
0050	O
and	O
HDTRA1	O
-	O
14	O
-	O
1	O
-	O
0040	O
,	O
ARO	O
MURI	O
Award	O
W911NF	O
-	O
12	O
-	O
1	O
-	O
0385	O
and	O
NSF	O
grants	O
CNS	O
1618339	O
and	O
CNS	O
1617729	O
.	O

bibliography	O
:	O
References	O
