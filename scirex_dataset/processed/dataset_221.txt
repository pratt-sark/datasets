document	O
:	O
In	O
-	O
Place	O
Activated	O
BatchNorm	Method
for	O
Memory	Task
-	Task
Optimized	Task
Training	Task
of	Task
DNNs	Task
In	O
this	O
work	O
we	O
present	O
In	Task
-	Task
Place	Task
Activated	Task
Batch	Task
Normalization	Task
(	O
InPlace	Task
-	Task
ABN	Task
)	O
–	O
a	O
novel	O
approach	O
to	O
drastically	O
reduce	O
the	O
training	Metric
memory	Metric
footprint	Metric
of	O
modern	O
deep	Method
neural	Method
networks	Method
in	O
a	O
computationally	O
efficient	O
way	O
.	O

Our	O
solution	O
substitutes	O
the	O
conventionally	O
used	O
succession	Method
of	Method
BatchNorm	Method
+	Method
Activation	Method
layers	Method
with	O
a	O
single	O
plugin	Method
layer	Method
,	O
hence	O
avoiding	O
invasive	Method
framework	Method
surgery	Method
while	O
providing	O
straightforward	O
applicability	O
for	O
existing	O
deep	Method
learning	Method
frameworks	Method
.	O

We	O
obtain	O
memory	O
savings	O
of	O
up	O
to	O
50	O
%	O
by	O
dropping	O
intermediate	O
results	O
and	O
by	O
recovering	O
required	O
information	O
during	O
the	O
backward	O
pass	O
through	O
the	O
inversion	O
of	O
stored	O
forward	O
results	O
,	O
with	O
only	O
minor	O
increase	O
(	O
-	O
)	O
in	O
computation	Metric
time	Metric
.	O

Also	O
,	O
we	O
demonstrate	O
how	O
frequently	O
used	O
checkpointing	Method
approaches	Method
can	O
be	O
made	O
computationally	O
as	O
efficient	O
as	O
InPlace	O
-	O
ABN	Method
.	O

In	O
our	O
experiments	O
on	O
image	Task
classification	Task
,	O
we	O
demonstrate	O
on	O
-	O
par	O
results	O
on	O
ImageNet	Material
-	Material
1k	Material
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
.	O

On	O
the	O
memory	O
-	O
demanding	O
task	O
of	O
semantic	Task
segmentation	Task
,	O
we	O
report	O
results	O
for	O
COCO	Material
-	Material
Stuff	Material
,	O
Cityscapes	Material
and	O
Mapillary	Material
Vistas	Material
,	O
obtaining	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
latter	O
without	O
additional	O
training	O
data	O
but	O
in	O
a	O
single	O
-	O
scale	O
and	O
-	O
model	O
scenario	O
.	O

Code	O
can	O
be	O
found	O
at	O
.	O

leftmargin=*	O
,	O
itemsep=0pt	O
section	O
:	O
Introduction	O
High	O
-	O
performance	O
computer	Method
vision	Method
recognition	Method
models	Method
typically	O
take	O
advantage	O
of	O
deep	Method
network	Method
backbones	Method
,	O
generating	O
rich	O
feature	Method
representations	Method
for	O
target	O
applications	O
to	O
operate	O
on	O
.	O

For	O
example	O
,	O
top	O
-	O
ranked	O
architectures	O
used	O
in	O
the	O
2017	O
LSUN	Task
or	Task
MS	Task
COCO	Task
segmentation	Task
/	Task
detection	Task
challenges	Task
are	O
predominantly	O
based	O
on	O
ResNet	Method
/	Method
ResNeXt	Method
models	Method
comprising	O
>	O
100	O
layers	O
.	O

Obviously	O
,	O
depth	O
/	O
width	O
of	O
networks	O
strongly	O
correlate	O
with	O
GPU	O
memory	O
requirements	O
and	O
at	O
given	O
hardware	O
memory	O
limitations	O
,	O
trade	O
-	O
offs	O
have	O
to	O
be	O
made	O
to	O
balance	O
feature	Method
extractor	Method
performance	O
vs.	O
application	O
-	O
specific	O
parameters	O
like	O
network	Metric
output	Metric
resolution	Metric
or	O
training	Metric
data	Metric
size	Metric
.	O

A	O
particularly	O
memory	Task
-	Task
demanding	Task
task	Task
is	O
semantic	Task
segmentation	Task
,	O
where	O
one	O
has	O
to	O
compromise	O
significantly	O
on	O
the	O
number	O
of	O
training	O
crops	O
per	O
minibatch	O
and	O
their	O
spatial	O
resolution	O
.	O

In	O
fact	O
,	O
many	O
recent	O
works	O
based	O
on	O
modern	O
backbone	Method
networks	Method
have	O
to	O
set	O
the	O
training	O
batch	O
size	O
to	O
no	O
more	O
than	O
a	O
single	O
crop	O
per	O
GPU	O
,	O
which	O
is	O
partially	O
also	O
due	O
to	O
suboptimal	O
memory	O
management	O
in	O
some	O
deep	Method
learning	Method
frameworks	Method
.	O

In	O
this	O
work	O
,	O
we	O
focus	O
on	O
increasing	O
the	O
memory	Metric
efficiency	Metric
of	O
the	O
training	Method
process	Method
of	O
modern	O
network	Method
architectures	Method
in	O
order	O
to	O
further	O
leverage	O
performance	O
of	O
deep	Method
neural	Method
networks	Method
in	O
tasks	O
like	O
image	Task
classification	Task
and	O
semantic	Task
segmentation	Task
.	O

We	O
introduce	O
a	O
novel	O
and	O
unified	Method
layer	Method
that	O
replaces	O
the	O
commonly	O
used	O
succession	O
of	O
batch	Method
normalization	Method
(	O
BN	Method
)	O
and	O
nonlinear	Method
activation	Method
layers	Method
(	O
Act	Method
)	O
,	O
which	O
are	O
integral	O
with	O
modern	O
deep	Method
learning	Method
architectures	Method
like	O
ResNet	Method
,	O
ResNeXt	Method
,	O
Inception	Method
-	Method
ResNet	Method
,	O
WideResNet	Method
,	O
Squeeze	Method
-	Method
and	Method
-	Method
Excitation	Method
Networks	Method
,	O
DenseNet	Method
,	O
.	O

Our	O
solution	O
is	O
coined	O
InPlace	O
-	O
ABN	Method
and	O
proposes	O
to	O
merge	O
batch	Method
normalization	Method
and	O
activation	O
layers	O
in	O
order	O
to	O
enable	O
in	Task
-	Task
place	Task
computation	Task
,	O
using	O
only	O
a	O
single	O
memory	O
buffer	O
for	O
storing	O
the	O
results	O
(	O
see	O
illustration	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

During	O
the	O
backward	O
pass	O
,	O
we	O
can	O
efficiently	O
recover	O
all	O
required	O
quantities	O
from	O
this	O
buffer	O
by	O
inverting	O
the	O
forward	Method
pass	Method
computations	Method
.	O

Our	O
approach	O
yields	O
a	O
theoretical	O
memory	Metric
reduction	Metric
of	O
up	O
to	O
,	O
and	O
our	O
experiments	O
on	O
semantic	Task
segmentation	Task
show	O
additional	O
data	Metric
throughput	Metric
of	O
up	O
to	O
during	O
training	Task
,	O
when	O
compared	O
to	O
prevailing	O
sequential	O
execution	O
of	O
BN	Method
+	O
Act	O
.	O

Our	O
memory	O
gains	O
are	O
obtained	O
without	O
introducing	O
noticeable	O
computational	Metric
overhead	Metric
,	O
side	O
-	O
by	O
-	O
side	O
runtime	O
comparisons	O
show	O
only	O
between	O
+	O
-	O
increase	O
in	O
computation	Metric
time	Metric
.	O

As	O
additional	O
contribution	O
,	O
we	O
review	O
the	O
checkpointing	Method
memory	Method
management	Method
strategy	Method
and	O
propose	O
a	O
computationally	O
optimized	O
application	O
of	O
this	O
idea	O
in	O
the	O
context	O
of	O
BN	Method
layers	O
.	O

This	O
optimization	O
allows	O
us	O
to	O
drop	O
re	O
-	O
computation	O
of	O
certain	O
quantities	O
needed	O
during	O
the	O
backward	O
pass	O
,	O
eventually	O
leading	O
to	O
reduced	O
computation	Metric
times	Metric
as	O
per	O
our	O
InPlace	Method
-	Method
ABN	Method
.	O

However	O
,	O
independent	O
of	O
the	O
proposed	O
optimized	O
application	O
of	O
,	O
conventional	O
checkpointing	Method
in	O
general	O
suffers	O
from	O
higher	O
implementation	Metric
complexity	Metric
(	O
with	O
the	O
necessity	O
to	O
invasively	O
manipulate	O
the	O
computation	O
graph	O
)	O
,	O
while	O
our	O
main	O
InPlace	Method
-	Method
ABN	Method
contribution	Method
can	O
be	O
easily	O
implemented	O
as	O
self	O
-	O
contained	O
,	O
standard	O
plug	Method
-	Method
in	Method
layer	Method
and	O
therefore	O
simply	O
integrated	O
in	O
any	O
modern	O
deep	Method
learning	Method
framework	Method
.	O

Our	O
experimental	O
evaluations	O
demonstrate	O
on	O
-	O
par	O
performance	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
trained	O
for	O
image	Task
classification	Task
on	O
ImageNet	Material
(	O
in	O
directly	O
comparable	O
memory	O
settings	O
)	O
,	O
and	O
significantly	O
improved	O
results	O
for	O
the	O
memory	O
-	O
critical	O
application	O
of	O
semantic	Task
segmentation	Task
.	O

To	O
summarize	O
,	O
we	O
provide	O
the	O
following	O
contributions	O
:	O
Introduction	O
of	O
a	O
novel	O
,	O
self	Method
-	Method
contained	Method
InPlace	Method
-	Method
ABN	Method
layer	Method
that	O
enables	O
joint	O
,	O
in	O
-	O
place	O
computation	O
of	O
BN	Method
+	O
Act	O
,	O
approximately	O
halvening	O
the	O
memory	Metric
requirements	Metric
during	O
training	O
of	O
modern	O
deep	Method
learning	Method
models	Method
.	O

A	O
computationally	O
more	O
efficient	O
application	O
of	O
the	O
checkpointing	Method
memory	Method
management	Method
strategy	Method
in	O
the	O
context	O
of	O
BN	Method
layers	O
,	O
inspired	O
by	O
optimizations	Method
used	O
for	O
InPlace	Task
-	Task
ABN	Task
.	O

Experimental	O
evaluations	O
for	O
i	O
)	O
image	Task
classification	Task
on	O
ImageNet	Material
-	Material
1k	Material
showing	O
approximately	O
on	O
-	O
par	O
performance	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
and	O
ii	O
)	O
semantic	Task
segmentation	Task
on	O
COCO	Material
-	Material
Stuff	Material
,	O
Cityscapes	Material
and	O
Mapillary	Material
Vistas	Material
,	O
considerably	O
benefiting	O
from	O
the	O
additional	O
available	O
memory	O
and	O
generating	O
new	O
high	O
-	O
scores	O
on	O
the	O
challenging	O
Vistas	Material
dataset	Material
.	O

section	O
:	O
Related	O
Work	O
The	O
topic	O
of	O
optimizing	Task
memory	Task
management	Task
in	O
deep	Task
learning	Task
frameworks	Task
is	O
typically	O
addressed	O
at	O
different	O
levels	O
.	O

Efficient	O
deep	Method
learning	Method
frameworks	Method
like	O
TensorFlow	Method
,	O
MxNet	Method
or	O
PyTorch	Method
follow	O
distinct	O
memory	Method
allocation	Method
strategies	Method
.	O

Among	O
them	O
is	O
checkpointing	Method
,	O
which	O
provides	O
additional	O
memory	O
at	O
the	O
cost	O
of	O
runtime	O
via	O
storing	O
activation	O
buffers	O
as	O
so	O
-	O
called	O
checkpoints	O
,	O
from	O
where	O
required	O
quantities	O
can	O
be	O
re	O
-	O
computed	O
during	O
the	O
backward	O
pass	O
.	O

The	O
paper	O
in	O
describes	O
how	O
to	O
recursively	O
apply	O
such	O
a	O
variant	O
on	O
sub	Task
-	Task
graphs	Task
between	O
checkpoints	O
.	O

In	O
this	O
is	O
further	O
optimized	O
with	O
dynamic	Method
programming	Method
,	O
where	O
a	O
storage	Method
policy	Method
is	O
determined	O
that	O
minimizes	O
the	O
computational	Metric
costs	Metric
for	O
re	Task
-	Task
computation	Task
at	O
a	O
fixed	O
memory	O
budget	O
.	O

Virtually	O
all	O
deep	Method
learning	Method
frameworks	Method
based	O
on	O
NVIDIA	Method
hardware	Method
exploit	O
low	O
-	O
level	O
functionality	O
libraries	O
CUDA	Method
and	O
cuDNN	Method
,	O
providing	O
GPU	O
-	O
accelerated	O
and	O
performance	O
-	O
optimized	O
primitives	O
and	O
basic	O
functionalities	O
.	O

Another	O
line	O
of	O
research	O
has	O
focused	O
on	O
training	O
CNNs	Method
with	O
reduced	O
precision	Metric
and	O
therefore	O
smaller	O
memory	O
-	O
footprint	O
datatypes	O
.	O

Such	O
works	O
include	O
(	O
partially	O
)	O
binarized	O
weights	O
/	O
activations	O
/	O
gradients	O
,	O
which	O
however	O
typically	O
lead	O
to	O
degraded	O
overall	O
performance	O
.	O

With	O
mixed	Method
precision	Method
training	Method
,	O
this	O
issue	O
seems	O
to	O
be	O
overcome	O
and	O
we	O
plan	O
to	O
exploit	O
this	O
as	O
complementary	O
technique	O
in	O
future	O
work	O
,	O
freeing	O
up	O
even	O
more	O
memory	O
for	O
training	O
deep	Method
networks	Method
without	O
sacrificing	O
runtime	Metric
.	O

In	O
the	O
authors	O
modify	O
ResNet	Method
in	O
a	O
way	O
to	O
contain	O
reversible	O
residual	O
blocks	O
,	O
residual	O
blocks	O
whose	O
activations	O
can	O
be	O
reconstructed	O
backwards	O
.	O

Backpropagation	O
through	O
reversible	O
blocks	O
can	O
be	O
performed	O
without	O
having	O
stored	O
intermediate	O
activations	O
during	O
the	O
forward	O
pass	O
,	O
which	O
allows	O
to	O
save	O
memory	O
.	O

However	O
,	O
the	O
cost	O
to	O
pay	O
is	O
twofold	O
.	O

First	O
,	O
one	O
has	O
to	O
recompute	O
each	O
residual	O
function	O
during	O
the	O
backward	O
pass	O
,	O
thus	O
having	O
the	O
same	O
overhead	O
as	O
checkpointing	O
.	O

Second	O
,	O
the	O
network	Method
design	Method
is	O
limited	O
to	O
using	O
blocks	O
with	O
certain	O
restrictions	O
,	O
reversible	O
blocks	O
can	O
not	O
be	O
generated	O
for	O
bottlenecks	O
where	O
information	O
is	O
supposed	O
to	O
be	O
discarded	O
.	O

Finally	O
,	O
we	O
stress	O
that	O
only	O
training	Metric
time	Metric
memory	Metric
-	Metric
efficiency	Metric
is	O
targeted	O
here	O
while	O
test	Task
-	Task
time	Task
optimization	Task
as	O
done	O
in	O
NVIDIAs	O
TensorRT	Method
is	O
beyond	O
our	O
scope	O
.	O

section	O
:	O
In	O
-	O
Place	O
Activated	Task
Batch	Task
Normalization	Task
Here	O
,	O
we	O
describe	O
our	O
contribution	O
to	O
avoid	O
the	O
storage	O
of	O
a	O
buffer	O
that	O
is	O
typically	O
needed	O
for	O
the	O
gradient	Task
computation	Task
during	O
the	O
backward	O
pass	O
through	O
the	O
batch	Method
normalization	Method
layer	O
.	O

As	O
opposed	O
to	O
existing	O
approaches	O
we	O
also	O
show	O
that	O
our	O
solution	O
minimizes	O
the	O
computational	Metric
overhead	Metric
we	O
have	O
to	O
trade	O
for	O
saving	O
additional	O
memory	O
.	O

subsection	O
:	O
Batch	Method
Normalization	Method
Review	O
Batch	Method
Normalization	Method
has	O
been	O
introduced	O
in	O
as	O
an	O
effective	O
tool	O
to	O
reduce	O
internal	Task
covariate	Task
shift	Task
in	O
deep	Method
networks	Method
and	O
accelerate	O
the	O
training	Task
process	Task
.	O

Ever	O
since	O
,	O
BN	Method
plays	O
a	O
key	O
role	O
in	O
most	O
modern	O
deep	Method
learning	Method
architectures	Method
.	O

The	O
key	O
idea	O
consists	O
in	O
having	O
a	O
normalization	Method
layer	Method
that	O
applies	O
an	O
axis	O
-	O
aligned	O
whitening	O
of	O
the	O
input	O
distribution	O
,	O
followed	O
by	O
a	O
scale	Method
-	Method
and	Method
-	Method
shift	Method
operation	Method
aiming	O
at	O
preserving	O
the	O
network	Method
’s	Method
representation	Method
capacity	Method
.	O

The	O
whitening	Method
operation	Method
exploits	O
statistics	O
computed	O
on	O
a	O
minibatch	O
level	O
only	O
.	O

The	O
by	O
-	O
product	O
of	O
this	O
approximation	O
is	O
an	O
additional	O
regularizing	O
effect	O
for	O
the	O
training	Task
process	Task
.	O

In	O
details	O
,	O
we	O
can	O
fix	O
a	O
particular	O
unit	O
in	O
the	O
network	O
and	O
let	O
be	O
the	O
set	O
of	O
values	O
takes	O
from	O
a	O
minibatch	O
of	O
training	O
examples	O
.	O

The	O
batch	Method
normalization	Method
operation	O
applied	O
to	O
first	O
performs	O
a	O
whitening	Method
of	Method
the	Method
activation	Method
using	O
statistics	O
computed	O
from	O
the	O
minibatch	Method
:	O
Here	O
is	O
a	O
small	O
constant	O
that	O
is	O
introduced	O
to	O
prevent	O
numerical	O
issues	O
,	O
and	O
and	O
are	O
the	O
empirical	O
mean	O
and	O
variance	O
of	O
the	O
activation	O
unit	O
,	O
respectively	O
,	O
computed	O
with	O
respect	O
to	O
the	O
minibatch	O
,	O
The	O
whitened	O
activations	O
are	O
then	O
scaled	O
and	O
shifted	O
by	O
learnable	O
parameters	O
and	O
,	O
obtaining	O
The	O
BN	Method
transformation	O
described	O
above	O
can	O
in	O
principle	O
be	O
applied	O
to	O
any	O
activation	O
in	O
the	O
network	O
and	O
is	O
typically	O
adopted	O
with	O
channel	O
-	O
specific	O
-	O
parameters	O
.	O

Using	O
BN	Method
renders	O
training	O
resilient	O
to	O
the	O
scale	O
of	O
parameters	O
,	O
thus	O
enabling	O
the	O
use	O
of	O
higher	O
learning	Metric
rates	Metric
.	O

At	O
test	O
time	O
,	O
the	O
BN	Method
statistics	O
are	O
fixed	O
to	O
and	O
,	O
estimated	O
from	O
the	O
entire	O
training	O
set	O
.	O

These	O
statistics	O
are	O
typically	O
updated	O
at	O
training	O
time	O
with	O
a	O
running	O
mean	O
over	O
the	O
respective	O
minibatch	O
statistics	O
,	O
but	O
could	O
also	O
be	O
recomputed	O
before	O
starting	O
the	O
testing	O
phase	O
.	O

Also	O
,	O
the	O
computation	Task
of	Task
networks	Task
trained	O
with	O
batch	Method
normalization	Method
can	O
be	O
sped	O
up	O
by	O
absorbing	O
BN	Method
parameters	O
into	O
the	O
preceding	O
Conv	Method
layer	Method
,	O
by	O
performing	O
a	O
simple	O
update	O
of	O
the	O
convolution	O
weights	O
and	O
biases	O
.	O

This	O
is	O
possible	O
because	O
at	O
test	O
-	O
time	O
BN	Method
becomes	O
a	O
linear	Method
operation	Method
.	O

subsection	O
:	O
Memory	Method
Optimization	Method
Strategies	Method
Here	O
we	O
sketch	O
our	O
proposed	O
memory	Method
optimization	Method
strategies	Method
after	O
introducing	O
both	O
,	O
the	O
standard	O
(	O
memory	O
-	O
inefficient	O
)	O
use	O
of	O
batch	Method
normalization	Method
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
coined	Method
checkpointing	Method
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
provide	O
diagrams	O
showing	O
the	O
forward	O
and	O
backward	O
passes	O
of	O
a	O
typical	O
building	O
block	O
BN	Method
+	O
Act	O
+	O
ConvHaving	O
the	O
convolution	Method
at	O
the	O
end	O
of	O
the	O
block	O
is	O
not	O
strictly	O
necessary	O
,	O
but	O
supports	O
comprehension	Task
.	O

that	O
we	O
find	O
in	O
modern	O
deep	Method
architectures	Method
.	O

The	O
activation	O
function	O
(	O
ReLU	Method
)	O
is	O
denoted	O
by	O
.	O

Computations	O
occurring	O
during	O
the	O
forward	O
pass	O
are	O
shown	O
in	O
green	O
and	O
involve	O
the	O
entire	O
minibatch	O
(	O
we	O
omit	O
the	O
subscript	O
)	O
.	O

Computations	O
happening	O
during	O
the	O
backward	O
pass	O
are	O
shown	O
in	O
cyan	O
and	O
gray	O
.	O

The	O
gray	O
part	O
aims	O
at	O
better	O
highlighting	O
the	O
additional	O
computation	O
that	O
has	O
been	O
introduced	O
to	O
compensate	O
for	O
the	O
memory	O
savings	O
.	O

Rectangles	O
are	O
in	O
general	O
volatile	O
buffers	O
holding	O
intermediate	O
results	O
,	O
except	O
for	O
rectangles	O
surrounded	O
by	O
a	O
dashed	O
frame	O
,	O
which	O
represent	O
buffers	O
that	O
need	O
to	O
be	O
stored	O
for	O
the	O
backward	O
pass	O
and	O
thus	O
significantly	O
impact	O
the	O
training	Metric
memory	Metric
footprint	Metric
.	O

,	O
in	O
Figure	O
[	O
reference	O
]	O
only	O
and	O
will	O
be	O
stored	O
for	O
the	O
backward	O
pass	O
,	O
while	O
in	O
Figure	O
[	O
reference	O
]	O
only	O
is	O
stored	O
.	O

For	O
the	O
sake	O
of	O
presentation	O
clarity	O
,	O
we	O
have	O
omitted	O
two	O
additional	O
buffers	O
holding	O
and	O
for	O
the	O
BN	Method
backward	O
phase	O
.	O

Nevertheless	O
,	O
these	O
buffers	O
represent	O
in	O
general	O
a	O
small	O
fraction	O
of	O
the	O
total	O
allocated	O
memory	O
.	O

Moreover	O
,	O
we	O
have	O
also	O
omitted	O
the	O
gradients	O
with	O
respect	O
to	O
the	O
model	O
parameters	O
(	O
,	O
and	O
Conv	O
weights	O
)	O
.	O

Standard	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
present	O
the	O
standard	O
implementation	O
of	O
the	O
reference	O
building	Method
block	Method
,	O
as	O
used	O
in	O
all	O
deep	Method
learning	Method
frameworks	Method
.	O

During	O
the	O
forward	O
pass	O
both	O
,	O
the	O
input	O
to	O
BN	Method
and	O
the	O
output	O
of	O
the	O
activation	O
function	O
need	O
to	O
be	O
stored	O
for	O
the	O
backward	O
pass	O
.	O

Variable	O
is	O
used	O
during	O
the	O
backward	O
pass	O
through	O
to	O
compute	O
both	O
the	O
gradient	O
its	O
input	O
and	O
,	O
and	O
where	O
denotes	O
the	O
loss	O
,	O
while	O
is	O
required	O
for	O
the	O
backward	O
pass	O
through	O
the	O
activation	O
as	O
well	O
as	O
potential	O
subsequent	O
operations	O
like	O
the	O
convolution	Method
shown	O
in	O
the	O
figure	O
.	O

Checkpointing	Task
[	O
]	O
.	O

This	O
technique	O
allows	O
to	O
trade	O
computation	O
for	O
memory	O
when	O
training	O
neural	Method
networks	Method
,	O
applicable	O
in	O
a	O
very	O
broad	O
setting	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
limit	O
its	O
application	O
to	O
the	O
building	O
block	O
under	O
consideration	O
like	O
in	O
.	O

In	O
contrast	O
to	O
the	O
standard	O
implementation	O
,	O
which	O
occupies	O
two	O
buffers	O
for	O
the	O
backward	O
pass	O
of	O
the	O
shown	O
building	O
block	O
,	O
checkpointing	Method
requires	O
only	O
a	O
single	O
buffer	O
.	O

The	O
trick	O
consists	O
in	O
storing	O
only	O
and	O
recomputing	O
during	O
the	O
backward	O
pass	O
by	O
reiterating	O
the	O
forward	O
operations	O
starting	O
from	O
(	O
see	O
gray	O
-	O
colored	O
operations	O
)	O
.	O

Clearly	O
,	O
the	O
computational	Metric
overhead	Metric
to	O
be	O
paid	O
comprises	O
both	O
,	O
recomputation	O
of	O
the	O
BN	Method
and	O
activation	O
layers	O
.	O

It	O
is	O
worth	O
observing	O
that	O
recomputing	O
(	O
gray	O
)	O
during	O
the	O
backward	O
phase	O
can	O
reuse	O
values	O
for	O
and	O
available	O
from	O
the	O
forward	O
pass	O
and	O
fuse	O
together	O
the	O
normalization	Method
and	O
subsequent	O
affine	Method
transformation	Method
into	O
a	O
single	O
scale	Method
-	Method
and	Method
-	Method
shift	Method
operation	Method
.	O

Accordingly	O
,	O
the	O
cost	O
of	O
the	O
second	O
forward	O
pass	O
over	O
becomes	O
less	O
expensive	O
(	O
see	O
also	O
)	O
.	O

The	O
three	O
approaches	O
that	O
follow	O
are	O
all	O
contributions	O
of	O
this	O
work	O
.	O

The	O
first	O
represents	O
a	O
variation	O
of	O
checkpointing	Task
,	O
which	O
allows	O
us	O
to	O
save	O
additional	O
computations	O
in	O
the	O
context	O
of	O
BN	Method
.	O

The	O
second	O
and	O
third	O
are	O
our	O
main	O
contributions	O
,	O
providing	O
strategies	O
that	O
yield	O
the	O
same	O
memory	Metric
savings	Metric
and	O
even	O
lower	O
computational	Metric
costs	Metric
compared	O
to	O
the	O
proposed	O
,	O
optimized	O
checkpointing	Method
,	O
but	O
are	O
both	O
self	O
-	O
contained	O
and	O
thus	O
much	O
easier	O
to	O
integrate	O
in	O
existing	O
deep	Method
learning	Method
frameworks	Method
.	O

Checkpointing	Method
(	O
proposed	O
version	O
)	O
.	O

Direct	O
application	O
of	O
the	O
checkpointing	Method
technique	Method
in	O
the	O
sketched	Method
building	Method
block	Method
,	O
which	O
is	O
adopted	O
also	O
in	O
,	O
is	O
not	O
computationally	O
optimal	O
since	O
additional	O
operations	O
could	O
be	O
saved	O
by	O
storing	O
,	O
the	O
normalized	O
value	O
of	O
as	O
per	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
,	O
instead	O
of	O
.	O

Indeed	O
,	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
subsection	O
,	O
the	O
backward	O
pass	O
through	O
BN	Method
requires	O
recomputing	O
if	O
not	O
already	O
stored	O
.	O

For	O
this	O
reason	O
,	O
we	O
propose	O
in	O
Figure	O
[	O
reference	O
]	O
an	O
alternative	O
implementation	O
that	O
is	O
computationally	O
more	O
efficient	O
by	O
retaining	O
from	O
the	O
forward	O
pass	O
through	O
the	O
BN	Method
layer	O
.	O

From	O
we	O
can	O
recover	O
during	O
the	O
backward	O
pass	O
by	O
applying	O
the	O
scale	Method
-	Method
and	Method
-	Method
shift	Method
operation	Method
,	O
followed	O
by	O
the	O
activation	O
function	O
(	O
see	O
gray	O
-	O
colored	O
operations	O
)	O
.	O

In	O
this	O
way	O
,	O
the	O
computation	O
of	O
becomes	O
slightly	O
more	O
efficient	O
than	O
the	O
one	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
for	O
we	O
save	O
the	O
fusion	Task
operation	Task
.	O

Finally	O
,	O
an	O
additional	O
saving	O
of	O
the	O
normalization	Method
step	Method
derives	O
from	O
using	O
the	O
stored	O
in	O
the	O
backward	O
implementation	O
of	O
BN	Method
rather	O
than	O
recomputing	O
it	O
from	O
.	O

To	O
distinguish	O
the	O
efficient	O
backward	Method
implementation	Method
of	O
BN	Method
from	O
the	O
standard	O
one	O
we	O
write	O
in	O
place	O
of	O
(	O
cyan	O
-	O
colored	O
,	O
see	O
additionally	O
§	O
[	O
reference	O
]	O
)	O
.	O

In	O
-	O
Place	O
Activated	Method
Batch	Method
Normalization	Method
I.	O
A	O
limitation	O
of	O
the	O
memory	Method
-	Method
reduction	Method
strategy	Method
described	O
above	O
is	O
that	O
the	O
last	O
layer	O
,	O
namely	O
Conv	O
in	O
the	O
example	O
,	O
depends	O
on	O
non	O
-	O
local	O
quantities	O
like	O
(	O
or	O
)	O
for	O
the	O
computation	O
of	O
the	O
gradient	O
.	O

This	O
makes	O
the	O
implementation	O
of	O
the	O
approach	O
within	O
standard	O
frameworks	O
somewhat	O
cumbersome	O
,	O
because	O
the	O
backward	O
pass	O
of	O
any	O
layer	O
that	O
follows	O
,	O
which	O
relies	O
on	O
the	O
existence	O
of	O
,	O
has	O
to	O
somehow	O
trigger	O
its	O
recomputation	O
.	O

To	O
render	O
the	O
implementation	O
of	O
the	O
proposed	O
memory	O
savings	O
easier	O
and	O
self	O
-	O
contained	O
,	O
we	O
suggest	O
an	O
alternative	O
strategy	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
which	O
relies	O
on	O
having	O
only	O
as	O
the	O
saved	O
buffer	O
during	O
the	O
forward	O
pass	O
,	O
thus	O
operating	O
an	O
in	O
-	O
place	O
computation	O
through	O
the	O
BN	Method
layer	O
(	O
therefrom	O
the	O
paper	O
’s	O
title	O
)	O
.	O

By	O
doing	O
so	O
,	O
any	O
layer	O
that	O
follows	O
the	O
activation	O
would	O
have	O
the	O
information	O
for	O
the	O
gradient	Task
computation	Task
locally	O
available	O
.	O

Having	O
stored	O
,	O
we	O
need	O
to	O
recompute	O
backwards	O
,	O
for	O
it	O
will	O
be	O
needed	O
in	O
the	O
backward	O
pass	O
through	O
the	O
BN	Method
layer	O
.	O

However	O
,	O
this	O
operation	O
is	O
only	O
possible	O
if	O
the	O
activation	O
function	O
is	O
invertible	O
.	O

Even	O
though	O
this	O
requirement	O
does	O
not	O
hold	O
for	O
ReLU	Method
,	O
one	O
of	O
the	O
most	O
dominantly	O
used	O
activation	Method
functions	Method
,	O
we	O
show	O
in	O
§	O
[	O
reference	O
]	O
that	O
an	O
invertible	Method
function	Method
like	O
Leaky	Method
ReLU	Method
with	O
a	O
small	O
slope	O
works	O
well	O
as	O
a	O
surrogate	O
of	O
ReLU	Method
without	O
compromising	O
on	O
the	O
model	Metric
quality	Metric
.	O

We	O
also	O
need	O
to	O
invert	O
the	O
scale	O
-	O
and	O
-	O
shift	O
operation	O
,	O
which	O
is	O
in	O
general	O
possible	O
if	O
.	O

In	O
-	O
Place	O
Activated	Method
Batch	Method
Normalization	Method
II	O
.	O

The	O
complexity	O
of	O
the	O
computation	O
of	O
used	O
in	O
the	O
backward	Method
pass	Method
of	Method
InPlace	Method
-	Method
ABN	Method
I	O
can	O
be	O
further	O
reduced	O
by	O
rewriting	O
the	O
gradients	O
and	O
directly	O
as	O
functions	O
of	O
instead	O
of	O
.	O

The	O
explicit	O
inversion	O
of	O
to	O
recover	O
applies	O
scale	Method
-	Method
and	Method
-	Method
shift	Method
operations	Method
(	O
per	O
feature	O
channel	O
)	O
.	O

If	O
the	O
partial	O
derivatives	O
are	O
however	O
based	O
on	O
directly	O
,	O
the	O
resulting	O
modified	O
gradients	O
(	O
derivations	O
given	O
in	O
the	O
Appendix	O
)	O
show	O
that	O
the	O
same	O
computation	O
can	O
be	O
absorbed	O
into	O
the	O
gradient	O
at	O
cost	O
(	O
per	O
feature	O
channel	O
)	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
show	O
the	O
diagram	O
of	O
this	O
optimization	O
,	O
where	O
we	O
denote	O
as	O
the	O
implementation	O
of	O
the	O
backward	O
pass	O
as	O
a	O
function	O
of	O
.	O

subsection	O
:	O
Technical	O
Details	O
The	O
key	O
components	O
of	O
our	O
method	O
are	O
the	O
computation	O
of	O
the	O
inverse	O
of	O
both	O
the	O
activation	O
function	O
(	O
InPlace	Method
-	Method
ABN	Method
I	Method
&	O
II	O
)	O
and	O
(	O
InPlace	O
-	O
ABN	O
I	O
)	O
,	O
and	O
the	O
implementation	O
of	O
a	O
backward	O
pass	O
through	O
the	O
batch	Method
normalization	Method
layer	O
that	O
depends	O
on	O
,	O
the	O
output	O
of	O
the	O
forward	O
pass	O
through	O
the	O
same	O
layer	O
.	O

Invertible	Method
activation	Method
function	Method
.	O

Many	O
activation	Method
functions	Method
are	O
actually	O
invertible	O
and	O
can	O
be	O
computed	O
in	O
-	O
place	O
(	O
sigmoid	O
,	O
hyperbolic	O
tangent	O
,	O
Leaky	Method
ReLU	Method
,	O
and	O
others	O
)	O
,	O
but	O
the	O
probably	O
most	O
commonly	O
used	O
one	O
,	O
namely	O
ReLU	Method
,	O
is	O
not	O
invertible	O
.	O

However	O
,	O
we	O
can	O
replace	O
it	O
with	O
Leaky	Method
ReLU	Method
(	O
see	O
,	O
Figure	O
[	O
reference	O
]	O
)	O
with	O
slope	O
without	O
impacting	O
the	O
quality	O
of	O
the	O
trained	O
models	O
.	O

This	O
will	O
be	O
the	O
activation	Method
function	Method
that	O
we	O
use	O
in	O
our	O
experimental	O
evaluation	O
due	O
to	O
its	O
affinity	O
to	O
standard	O
ReLU	Method
,	O
even	O
though	O
other	O
activation	Method
functions	Method
could	O
be	O
used	O
.	O

The	O
corresponding	O
forward	O
pass	O
through	O
the	O
activation	O
function	O
with	O
slope	O
for	O
negative	O
inputs	O
and	O
its	O
inverse	O
used	O
in	O
our	O
backward	O
pass	O
are	O
given	O
as	O
follows	O
:	O
Leaky	Method
ReLU	Method
and	O
its	O
inverse	O
share	O
the	O
same	O
computational	Metric
cost	Metric
,	O
an	O
elementwise	Method
sign	Method
check	Method
and	O
scaling	Method
operation	Method
.	O

Hence	O
,	O
the	O
overhead	O
deriving	O
from	O
the	O
recomputation	O
of	O
in	O
the	O
backward	O
pass	O
of	O
the	O
previously	O
shown	O
,	O
checkpointing	Method
-	Method
based	Method
approaches	Method
and	O
its	O
inverse	O
employed	O
in	O
the	O
backward	O
pass	O
of	O
our	O
method	O
are	O
equivalent	O
.	O

To	O
give	O
further	O
evidence	O
of	O
the	O
interchangeability	O
of	O
ReLU	Method
and	O
Leaky	Method
ReLU	Method
with	O
slope	O
,	O
we	O
have	O
successfully	O
retrained	O
well	O
-	O
known	O
models	O
like	O
ResNeXt	Method
and	O
WideResNet	O
on	O
ImageNet	Material
using	O
Leaky	Method
ReLU	Method
(	O
see	O
§	O
[	O
reference	O
]	O
)	O
.	O

InPlace	O
-	O
ABN	O
I	O
:	O
Backward	O
pass	O
through	O
BN	Method
.	O

The	O
gradient	O
,	O
which	O
is	O
obtained	O
from	O
the	O
backward	O
pass	O
through	O
the	O
BN	Method
layer	O
,	O
can	O
be	O
written	O
as	O
a	O
function	O
of	O
and	O
as	O
where	O
the	O
gradients	O
of	O
the	O
BN	Method
parameters	O
are	O
given	O
by	O
The	O
expression	O
above	O
differs	O
from	O
what	O
is	O
found	O
in	O
the	O
original	O
BN	Method
paper	O
,	O
but	O
the	O
refactoring	O
was	O
already	O
used	O
in	O
the	O
Caffe	Method
framework	Method
.	O

It	O
is	O
implemented	O
by	O
in	O
the	O
proposed	O
solutions	O
in	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
and	O
does	O
not	O
depend	O
on	O
.	O

Hence	O
,	O
we	O
store	O
during	O
the	O
forward	O
pass	O
only	O
(	O
this	O
dependency	O
was	O
omitted	O
from	O
the	O
diagrams	O
)	O
.	O

Instead	O
,	O
in	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
which	O
depends	O
on	O
,	O
requires	O
the	O
additional	O
recomputation	O
of	O
from	O
via	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
)	O
.	O

Hence	O
,	O
it	O
also	O
requires	O
storing	O
.	O

Our	O
solution	O
is	O
hence	O
memory	O
-	O
wise	O
more	O
efficient	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
from	O
Figure	O
[	O
reference	O
]	O
.	O

Inversion	Task
of	Task
πγ	Task
,	O
β	O
.	O

In	O
the	O
configuration	O
of	O
InPlace	O
-	O
ABN	O
I	O
,	O
the	O
inversion	O
of	O
becomes	O
critical	O
if	O
since	O
.	O

While	O
we	O
never	O
encountered	O
such	O
a	O
case	O
in	O
practice	O
,	O
one	O
can	O
protect	O
against	O
it	O
by	O
preventing	O
from	O
getting	O
less	O
than	O
a	O
given	O
tolerance	O
.	O

We	O
can	O
even	O
avoid	O
this	O
problem	O
by	O
simply	O
not	O
considering	O
a	O
learnable	O
parameter	O
and	O
by	O
fixing	O
it	O
to	O
,	O
in	O
case	O
the	O
activation	O
function	O
is	O
scale	O
covariant	O
(	O
all	O
ReLU	O
-	O
like	O
activations	O
)	O
and	O
when	O
a	O
Conv	Method
layer	Method
follows	O
.	O

Indeed	O
,	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
network	O
retains	O
the	O
exact	O
same	O
capacity	O
in	O
that	O
case	O
,	O
for	O
can	O
be	O
absorbed	O
into	O
the	O
subsequent	O
Conv	O
layer	O
.	O

InPlace	O
-	O
ABN	O
II	O
:	O
Backward	O
pass	O
through	O
BN	Method
.	O

We	O
obtain	O
additional	O
memory	O
savings	O
for	O
our	O
solution	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
as	O
outlined	O
in	O
§	O
[	O
reference	O
]	O
.	O

The	O
gradient	O
when	O
written	O
as	O
a	O
function	O
of	O
instead	O
of	O
becomes	O
For	O
the	O
gradients	O
of	O
the	O
BN	Method
parameters	O
,	O
remains	O
as	O
above	O
but	O
we	O
get	O
and	O
we	O
write	O
for	O
the	O
actual	O
backward	O
implementation	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Detailed	O
derivations	O
are	O
provided	O
in	O
the	O
Appendix	O
of	O
this	O
paper	O
.	O

In	O
summary	O
,	O
both	O
of	O
our	O
optimized	O
main	O
contributions	O
are	O
memory	O
-	O
wise	O
more	O
efficient	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
solution	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
InPlace	O
-	O
ABN	Method
II	Method
is	O
computationally	O
even	O
more	O
efficient	O
than	O
the	O
proposed	O
,	O
optimized	O
checkpointing	Method
from	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Implementation	O
Details	O
We	O
have	O
implemented	O
the	O
proposed	O
InPlace	Method
-	Method
ABN	Method
I	Method
layer	Method
in	O
PyTorch	Method
,	O
by	O
simply	O
creating	O
a	O
new	O
layer	O
that	O
fuses	O
batch	Method
normalization	Method
with	O
an	O
(	O
invertible	O
)	O
activation	O
function	O
.	O

In	O
this	O
way	O
we	O
can	O
deal	O
with	O
the	O
computation	O
of	O
from	O
internally	O
in	O
the	O
layer	O
,	O
thus	O
keeping	O
the	O
implementation	O
self	O
-	O
contained	O
.	O

We	O
have	O
released	O
code	O
at	O
for	O
easy	O
plug	O
-	O
in	O
replacement	O
of	O
the	O
block	O
BN	Method
+	O
Act	O
in	O
modern	O
architectures	O
.	O

The	O
forward	Method
and	Method
backward	Method
implementations	Method
are	O
also	O
given	O
as	O
pseudocode	O
in	O
Algorithm	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

In	O
the	O
forward	O
pass	O
,	O
in	O
line	O
3	O
,	O
we	O
explicitly	O
indicate	O
the	O
buffers	O
that	O
are	O
stored	O
and	O
needed	O
for	O
the	O
backward	O
pass	O
.	O

Any	O
other	O
buffer	O
can	O
be	O
overwritten	O
with	O
in	O
-	O
place	O
computations	O
,	O
,	O
and	O
can	O
point	O
to	O
the	O
same	O
memory	O
location	O
.	O

In	O
the	O
backward	O
pass	O
,	O
we	O
recover	O
the	O
stored	O
buffers	O
in	O
line	O
1	O
and	O
,	O
again	O
,	O
every	O
computation	O
can	O
be	O
done	O
in	O
-	O
place	O
if	O
the	O
buffer	O
is	O
not	O
needed	O
anymore	O
(	O
,	O
,	O
can	O
share	O
the	O
same	O
memory	O
location	O
as	O
well	O
as	O
,	O
and	O
)	O
.	O

As	O
opposed	O
to	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
pseudocode	O
shows	O
also	O
the	O
dependencies	O
on	O
additional	O
,	O
small	O
,	O
buffers	O
like	O
and	O
reports	O
the	O
gradients	O
with	O
respect	O
to	O
the	O
BN	Method
layer	O
parameters	O
and	O
.	O

Please	O
note	O
the	O
difference	O
during	O
backward	Task
computation	Task
when	O
applying	O
InPlace	Method
-	Method
ABN	Method
I	Method
or	O
InPlace	Method
-	Method
ABN	Method
II	Method
,	O
respectively	O
.	O

[	O
t	O
]	O
[	O
1	O
]	O
,	O
,	O
save	O
for	O
backward	O
,	O
InPlace	O
-	O
ABN	O
Forward	O
[	O
t	O
]	O
[	O
1	O
]	O
,	O
,	O
saved	O
tensors	O
during	O
forward	O
InPlace	O
-	O
ABN	O
I	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
InPlace	O
-	O
ABN	O
II	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
,	O
InPlace	O
-	O
ABN	O
Backward	O
section	O
:	O
Experiments	O
We	O
assess	O
the	O
effectiveness	O
of	O
our	O
proposed	O
,	O
memory	Method
efficient	Method
InPlace	Method
-	Method
ABN	Method
layer	Method
for	O
the	O
tasks	O
of	O
image	Task
classification	Task
and	O
semantic	Task
segmentation	Task
in	O
§	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
respectively	O
.	O

Additionally	O
,	O
we	O
provide	O
timing	O
analyses	O
in	O
§	O
[	O
reference	O
]	O
.	O

Experiments	O
were	O
run	O
and	O
timed	O
on	O
machines	O
comprising	O
four	O
NVIDIA	O
Titan	Method
Xp	Method
cards	Method
(	O
with	O
12	O
GB	O
of	O
RAM	O
each	O
)	O
.	O

Where	O
not	O
otherwise	O
noted	O
,	O
the	O
activation	O
function	O
used	O
in	O
all	O
experiments	O
is	O
Leaky	O
ReLU	O
with	O
slope	O
.	O

subsection	O
:	O
Image	Task
Classification	Task
We	O
have	O
trained	O
several	O
residual	Method
-	Method
unit	Method
-	Method
based	Method
models	Method
on	O
ImageNet	Material
-	Material
1k	Material
to	O
demonstrate	O
the	O
effectiveness	O
of	O
InPlace	Method
-	Method
ABN	Method
for	O
the	O
task	O
of	O
image	Task
classification	Task
.	O

In	O
particular	O
,	O
we	O
focus	O
our	O
attention	O
on	O
two	O
main	O
questions	O
:	O
i	O
)	O
whether	O
using	O
an	O
invertible	O
activation	O
function	O
(	O
Leaky	Method
ReLU	Method
in	O
our	O
experiments	O
)	O
impacts	O
on	O
the	O
performance	O
of	O
the	O
models	O
,	O
and	O
ii	O
)	O
how	O
the	O
memory	O
savings	O
obtained	O
with	O
our	O
method	O
can	O
be	O
exploited	O
to	O
improve	O
classification	Metric
accuracy	Metric
.	O

Our	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
and	O
described	O
in	O
this	O
subsection	O
.	O

ResNeXt	Method
-	Method
101	Method
/	O
ResNeXt	Method
-	Method
152	Method
.	O

This	O
is	O
a	O
variant	O
of	O
the	O
original	O
ResNet	Method
architecture	Method
in	O
which	O
the	O
bottleneck	O
residual	O
units	O
are	O
replaced	O
with	O
a	O
multi	Method
-	Method
branch	Method
version	Method
.	O

In	O
practice	O
,	O
this	O
equates	O
to	O
‘	O
‘	O
grouping	O
’	O
’	O
the	O
convolutions	Method
of	O
each	O
residual	O
unit	O
.	O

The	O
number	O
of	O
groups	O
,	O
parallel	O
branches	O
,	O
is	O
known	O
as	O
cardinality	O
and	O
is	O
an	O
additional	O
hyperparameter	O
to	O
be	O
set	O
.	O

We	O
follow	O
the	O
best	O
performing	O
design	O
guidelines	O
described	O
in	O
and	O
train	O
models	O
with	O
cardinality	O
64	O
,	O
while	O
considering	O
the	O
101	O
-	O
and	O
152	O
-	O
layers	O
configurations	O
.	O

During	O
training	O
,	O
we	O
proportionally	O
scale	O
input	O
images	O
so	O
that	O
their	O
smallest	O
side	O
equals	O
pixels	O
,	O
before	O
randomly	O
taking	O
crops	O
.	O

Additionally	O
,	O
we	O
perform	O
per	Method
-	Method
channel	Method
mean	Method
and	Method
variance	Method
normalization	Method
and	O
color	Method
augmentation	Method
as	O
described	O
in	O
.	O

We	O
train	O
using	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
Nesterov	Method
updates	Method
,	O
initial	O
learning	Metric
rate	Metric
,	O
weight	O
decay	O
and	O
momentum	O
.	O

The	O
training	O
is	O
run	O
for	O
a	O
total	O
of	O
epochs	O
,	O
reducing	O
the	O
learning	Metric
rate	Metric
every	O
epochs	O
by	O
a	O
factor	O
.	O

WideResNet	O
-	O
38	O
.	O

This	O
is	O
another	O
modern	O
architecture	O
built	O
by	O
stacking	Method
residual	Method
units	Method
.	O

Compared	O
to	O
the	O
original	O
ResNet	Method
,	O
WideResNet	O
trades	O
depth	O
for	O
width	O
,	O
it	O
uses	O
units	O
with	O
an	O
increased	O
number	O
of	O
feature	O
channels	O
while	O
reducing	O
the	O
total	O
number	O
of	O
stacked	O
units	O
.	O

For	O
training	Task
,	O
we	O
use	O
the	O
same	O
setup	O
and	O
hyperparameters	O
as	O
for	O
ResNeXt	O
,	O
with	O
one	O
exception	O
:	O
following	O
we	O
train	O
for	O
epochs	O
,	O
linearly	O
decreasing	O
the	O
learning	Metric
rate	Metric
from	O
to	O
.	O

Discussion	O
of	O
results	O
.	O

In	O
our	O
experiments	O
we	O
also	O
compared	O
the	O
validation	Metric
accuracy	Metric
obtained	O
when	O
replacing	O
ReLU	Method
with	O
Leaky	Method
ReLU	Method
in	O
a	O
ResNeXt	Method
-	Method
101	Method
trained	O
with	O
ReLU	Method
.	O

We	O
also	O
considered	O
the	O
opposite	O
case	O
,	O
replacing	O
Leaky	O
ReLU	O
with	O
ReLU	O
in	O
a	O
Leaky	Method
ReLU	Method
-	Method
trained	Method
network	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Our	O
results	O
are	O
in	O
line	O
with	O
,	O
and	O
never	O
differ	O
by	O
more	O
than	O
a	O
single	O
point	O
per	O
training	O
except	O
for	O
the	O
center	Metric
crop	Metric
evaluation	Metric
top	Metric
-	Metric
1	Metric
results	O
,	O
probably	O
also	O
due	O
to	O
non	O
-	O
deterministic	O
training	O
behaviour	O
.	O

Our	O
results	O
may	O
slightly	O
differ	O
from	O
what	O
was	O
reported	O
in	O
the	O
original	O
papers	O
,	O
as	O
our	O
training	O
protocol	O
does	O
not	O
exactly	O
match	O
the	O
one	O
in	O
(	O
data	Task
augmentation	Task
regarding	O
scale	O
and	O
aspect	O
ratio	O
settings	O
,	O
learning	O
rate	O
schedule	O
,	O
)	O
or	O
due	O
to	O
changes	O
in	O
reference	O
implementations	O
.	O

Next	O
,	O
we	O
focus	O
on	O
how	O
to	O
better	O
exploit	O
the	O
memory	O
savings	O
due	O
to	O
our	O
proposed	O
InPlace	Method
-	Method
ABN	Method
for	O
improving	O
classification	Metric
accuracy	Metric
.	O

As	O
a	O
baseline	O
,	O
we	O
train	O
ResNeXt	Method
-	Method
101	Method
with	O
standard	O
Batch	Method
Normalization	Method
and	O
the	O
maximum	O
batch	O
size	O
that	O
fits	O
in	O
GPU	O
memory	O
,	O
256	O
images	O
per	O
batch	O
.	O

Then	O
,	O
we	O
consider	O
two	O
different	O
options	O
:	O
i	O
)	O
using	O
the	O
extra	O
memory	O
to	O
fit	O
more	O
images	O
per	O
training	O
batch	O
while	O
fixing	O
the	O
network	Method
architecture	Method
,	O
or	O
ii	O
)	O
fixing	O
the	O
batch	O
size	O
while	O
training	O
a	O
larger	O
network	O
.	O

For	O
option	O
i	O
)	O
we	O
double	O
the	O
batch	O
size	O
to	O
512	O
(	O
ResNeXt	Method
-	Method
101	Method
,	O
InPlace	O
-	O
ABN	O
,	O
512	O
in	O
Table	O
[	O
reference	O
]	O
)	O
,	O
while	O
for	O
option	O
ii	O
)	O
we	O
train	O
ResNeXt	Method
-	Method
152	Method
and	O
WideResNet	Method
-	Method
38	Method
.	O

Note	O
that	O
neither	O
ResNeXt	Method
-	Method
152	Method
nor	O
WideResNet	Method
-	Method
38	Method
would	O
fit	O
in	O
memory	O
when	O
using	O
256	O
images	O
per	O
training	O
batch	O
and	O
when	O
using	O
standard	O
BN	Method
.	O

As	O
it	O
is	O
clear	O
from	O
the	O
table	O
,	O
both	O
i	O
)	O
and	O
ii	O
)	O
result	O
in	O
a	O
noticeable	O
performance	O
increase	O
.	O

Interestingly	O
,	O
training	O
ResNeXt	Method
-	Method
101	Method
with	O
an	O
increased	O
batch	O
size	O
results	O
in	O
similar	O
accuracy	Metric
to	O
the	O
deeper	O
(	O
and	O
computationally	O
more	O
expensive	O
)	O
ResNeXt	Method
-	Method
152	Method
model	O
.	O

As	O
an	O
additional	O
reference	O
,	O
we	O
train	O
ResNeXt	Method
-	Method
101	Method
with	O
synchronized	Method
Batch	Method
Normalization	Method
(	O
InPlace	O
-	O
ABNsync	Method
)	O
,	O
which	O
can	O
be	O
seen	O
as	O
a	O
‘	O
‘	O
virtual	O
’	O
’	O
increase	O
of	O
batch	Metric
size	Metric
applied	O
to	O
the	O
computation	O
of	O
BN	Method
statistics	O
.	O

In	O
this	O
case	O
we	O
only	O
observe	O
small	O
accuracy	Metric
improvements	O
when	O
compared	O
to	O
the	O
baseline	O
model	O
.	O

For	O
the	O
future	O
,	O
we	O
plan	O
to	O
conduct	O
further	O
experiments	O
with	O
deeper	O
variants	O
of	O
DenseNet	Method
,	O
and	O
investigate	O
effects	O
of	O
InPlace	Method
-	Method
ABN	Method
on	O
Squeeze	Method
&	Method
Excitation	Method
networks	Method
or	O
deformable	Method
convolutional	Method
networks	Method
.	O

subsection	O
:	O
Semantic	Task
Segmentation	Task
The	O
goal	O
of	O
semantic	Task
segmentation	Task
is	O
to	O
assign	O
categorical	O
labels	O
to	O
each	O
pixel	O
in	O
an	O
image	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
segmentations	Task
are	O
typically	O
obtained	O
by	O
combining	O
classification	Method
models	Method
pretrained	O
on	O
ImageNet	Material
(	O
typically	O
referred	O
to	O
as	O
body	O
)	O
with	O
segmentation	Method
-	Method
specific	Method
head	Method
architectures	Method
and	O
jointly	O
fine	O
-	O
tuning	O
them	O
on	O
suitable	O
,	O
(	O
densely	O
)	O
annotated	O
training	O
data	O
like	O
Cityscapes	Material
,	O
COCO	Material
-	Material
Stuff	Material
,	O
ADE20	Material
K	Material
or	O
Mapillary	Material
Vistas	Material
.	O

Datasets	O
used	O
for	O
Evaluation	O
.	O

We	O
report	O
results	O
on	O
Cityscapes	Material
,	O
COCO	Material
-	Material
Stuff	Material
and	O
Mapillary	Material
Vistas	Material
,	O
since	O
these	O
datasets	O
have	O
complementary	O
properties	O
in	O
terms	O
of	O
image	Metric
content	Metric
,	O
size	Metric
,	O
number	O
of	O
class	Metric
labels	Metric
and	O
annotation	Metric
quality	Metric
.	O

Cityscapes	Material
shows	O
street	O
-	O
level	O
images	O
captured	O
in	O
central	O
Europe	O
and	O
comprises	O
a	O
total	O
of	O
5k	O
densely	O
annotated	O
images	O
(	O
19	O
object	O
categories	O
+	O
1	O
void	O
class	O
,	O
all	O
images	O
sized	O
2048	O
1024	O
)	O
,	O
split	O
into	O
2975	O
/	O
500	O
/	O
1525	O
images	O
for	O
training	O
,	O
validation	Task
and	O
test	O
,	O
respectively	O
.	O

While	O
there	O
exist	O
additional	O
20k	O
images	O
with	O
so	O
-	O
called	O
coarse	O
annotations	O
,	O
we	O
learn	O
only	O
from	O
the	O
high	O
-	O
quality	O
(	O
fine	O
)	O
annotations	O
in	O
the	O
training	O
set	O
and	O
test	O
on	O
the	O
corresponding	O
validation	O
set	O
(	O
for	O
which	O
ground	O
truth	O
is	O
publicly	O
available	O
)	O
.	O

We	O
also	O
show	O
results	O
on	O
COCO	Material
-	Material
Stuff	Material
,	O
which	O
provides	O
stuff	O
-	O
class	O
annotations	O
for	O
the	O
well	O
-	O
known	O
MS	Material
COCO	Material
dataset	Material
.	O

This	O
dataset	O
comprises	O
65k	O
COCO	Material
images	Material
(	O
with	O
40k	O
for	O
training	O
,	O
5k	O
for	O
validation	Metric
,	O
5k	O
for	O
test	O
-	O
dev	O
and	O
15k	O
as	O
challenge	O
test	O
set	O
)	O
with	O
annotations	O
for	O
91	O
stuff	O
classes	O
and	O
1	O
void	O
class	O
.	O

Images	Material
are	O
smaller	O
than	O
in	O
Cityscapes	Material
and	O
with	O
varying	O
sizes	O
,	O
and	O
the	O
provided	O
semantic	O
annotations	O
are	O
based	O
on	O
superpixel	Method
segmentations	Method
,	O
consequently	O
suffering	O
from	O
considerable	O
mislabelings	O
.	O

Finally	O
,	O
we	O
also	O
report	O
results	O
on	O
Mapillary	Material
Vistas	Material
(	O
research	O
edition	O
)	O
,	O
a	O
novel	O
and	O
large	O
-	O
scale	O
street	O
-	O
level	O
image	O
dataset	O
comprising	O
25k	O
densely	O
annotation	O
images	O
(	O
65	O
object	O
categories	O
+	O
1	O
void	O
class	O
,	O
images	O
have	O
varying	O
aspect	O
ratios	O
and	O
sizes	O
up	O
to	O
22	O
Megapixel	O
)	O
,	O
split	O
into	O
18k	O
/	O
2k	O
/	O
5k	O
images	O
for	O
training	O
,	O
validation	Metric
and	O
test	O
,	O
respectively	O
.	O

Similar	O
to	O
the	O
aforementioned	O
datasets	O
,	O
we	O
train	O
on	O
training	O
data	O
and	O
test	O
on	O
validation	O
data	O
.	O

Segmentation	Method
approach	Method
.	O

We	O
chose	O
to	O
adopt	O
the	O
recently	O
introduced	O
DeepLabV3	Method
segmentation	Method
approach	Method
as	O
head	O
,	O
and	O
evaluate	O
its	O
performance	O
with	O
body	Method
networks	Method
from	O
§	O
[	O
reference	O
]	O
.	O

DeepLabV3	Method
is	O
exploiting	O
atrous	Method
(	Method
dilated	Method
)	Method
convolutions	Method
in	O
a	O
cascaded	O
way	O
for	O
capturing	O
contextual	O
information	O
,	O
together	O
with	O
crop	O
-	O
level	O
features	O
encoding	O
global	O
context	O
(	O
close	O
in	O
spirit	O
to	O
PSPNet	O
’s	O
global	O
feature	O
)	O
.	O

We	O
follow	O
the	O
parameter	O
choices	O
suggested	O
in	O
,	O
assembling	O
the	O
head	O
as	O
4	O
parallel	O
Conv	O
blocks	O
with	O
output	O
channels	O
each	O
and	O
dilation	O
rates	O
(	O
with	O
x8	O
downsampled	O
crop	O
sizes	O
from	O
the	O
body	O
)	O
and	O
kernel	O
sizes	O
,	O
respectively	O
.	O

The	O
global	O
features	O
are	O
computed	O
in	O
a	O
channel	O
-	O
specific	O
way	O
and	O
Conv	O
ed	O
into	O
additional	O
channels	O
.	O

Each	O
output	O
block	O
is	O
followed	O
by	O
BatchNorm	O
before	O
all	O
features	O
are	O
stacked	O
and	O
reduced	O
by	O
another	O
Conv	O
+	O
BN	Method
+	O
Act	O
block	O
(	O
into	O
256	O
features	O
)	O
and	O
finally	O
Conv	O
ed	O
to	O
the	O
number	O
of	O
target	O
classes	O
.	O

We	O
exploit	O
our	O
proposed	O
InPlace	Method
-	Method
ABN	Method
strategy	Method
also	O
in	O
the	O
head	Method
architecture	Method
.	O

Finally	O
,	O
we	O
apply	O
bilinear	Method
upsampling	Method
to	O
the	O
logits	O
to	O
obtain	O
the	O
original	O
input	O
crop	O
resolution	O
before	O
computing	O
the	O
loss	O
using	O
an	O
online	Method
bootstrapping	Method
strategy	Method
as	O
described	O
in	O
(	O
setting	O
and	O
)	O
.	O

We	O
did	O
not	O
apply	O
hybrid	Method
dilated	Method
convolutions	Method
nor	O
added	O
an	O
auxiliary	O
loss	O
as	O
proposed	O
in	O
.	O

Training	Material
data	Material
is	O
sampled	O
in	O
a	O
uniform	O
way	O
(	O
by	O
shuffling	O
the	O
database	O
in	O
each	O
epoch	O
)	O
and	O
all	O
Cityscapes	Material
experiments	O
are	O
run	O
for	O
360	O
epochs	O
using	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
and	O
polynomial	Metric
learning	Metric
rate	Metric
decay	Metric
,	O
following	O
.	O

COCO	Material
-	Material
Stuff	Material
experiments	O
were	O
trained	O
only	O
for	O
30	O
epochs	O
,	O
which	O
however	O
approximately	O
matches	O
the	O
number	O
of	O
iterations	O
on	O
Cityscapes	Material
due	O
to	O
the	O
considerably	O
larger	O
dataset	O
size	O
.	O

For	O
optimization	Task
,	O
we	O
use	O
stochastic	Method
gradient	Method
descent	Method
with	O
momentum	Method
and	O
weight	Method
decay	Method
.	O

For	O
training	Task
data	Task
augmentation	Task
,	O
we	O
apply	O
random	Method
horizontal	Method
flipping	Method
(	O
with	O
prob	O
.	O

)	O
and	O
random	O
scaling	O
selected	O
from	O
-	O
before	O
cropping	O
the	O
actual	O
patches	O
.	O

Discussion	O
of	O
Results	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
provide	O
results	O
on	O
validation	O
data	O
for	O
Cityscapes	Material
and	O
COCO	Material
-	Material
Stuff	Material
under	O
different	O
BN	Method
layer	O
configurations	O
.	O

We	O
distinguish	O
between	O
standard	O
BN	Method
layers	O
(	O
coined	O
Std	O
-	O
BN	Method
)	O
and	O
our	O
proposed	O
variants	O
using	O
in	O
-	O
place	O
,	O
activated	O
BN	Method
(	O
InPlace	Method
-	Method
ABN	Method
)	O
as	O
well	O
as	O
its	O
gradient	Method
-	Method
synchronized	Method
version	Method
InPlace	Method
-	Method
ABNsync	Method
.	O

All	O
experiments	O
are	O
based	O
on	O
Leaky	Method
ReLU	Method
activations	Method
.	O

Trainings	O
were	O
conducted	O
in	O
a	O
way	O
to	O
maximize	O
GPU	O
memory	O
utilization	O
by	O
i	O
)	O
fixing	O
the	O
training	Metric
crop	Metric
size	Metric
and	O
therefore	O
pushing	O
the	O
amount	O
of	O
crops	O
per	O
minibatch	O
to	O
the	O
limit	O
(	O
denoted	O
as	O
fixed	O
crop	O
)	O
or	O
ii	O
)	O
fixing	O
the	O
number	O
of	O
crops	O
per	O
minibatch	O
and	O
maximizing	O
the	O
training	O
crop	O
resolutions	O
(	O
fixed	O
batch	O
)	O
.	O

Experiments	O
are	O
conducted	O
for	O
ResNeXt	Method
-	Method
101	Method
and	O
WideResNet	O
-	O
38	O
network	O
bodies	O
,	O
where	O
the	O
latter	O
seems	O
preferable	O
for	O
segmentation	Task
tasks	Task
.	O

Both	O
body	Method
networks	Method
were	O
solely	O
trained	O
on	O
ImageNet	Material
-	Material
1k	Material
.	O

All	O
results	O
derive	O
from	O
single	O
-	O
scale	Method
testing	Method
without	O
horizontal	Method
image	Method
flipping	Method
,	O
deliberately	O
avoiding	O
dilution	O
of	O
results	O
by	O
well	O
-	O
known	O
bells	O
and	O
whistles	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
positive	O
effects	O
of	O
applying	O
more	O
training	O
data	O
(	O
in	O
terms	O
of	O
both	O
,	O
#	O
training	O
crops	O
per	O
minibatch	O
and	O
input	O
crop	O
resolutions	O
)	O
on	O
the	O
validation	O
results	O
.	O

The	O
increase	O
of	O
data	O
(	O
pixels	O
/	O
minibatch	O
)	O
we	O
can	O
put	O
in	O
GPU	O
memory	O
,	O
relative	O
to	O
the	O
baseline	O
(	O
top	O
row	O
)	O
is	O
reported	O
in	O
square	O
brackets	O
.	O

We	O
observe	O
that	O
higher	O
input	O
resolution	O
is	O
in	O
general	O
even	O
more	O
beneficial	O
than	O
adding	O
more	O
crops	O
to	O
the	O
batch	O
.	O

For	O
the	O
sake	O
of	O
direct	O
comparability	O
we	O
left	O
the	O
learning	Metric
rates	Metric
unchanged	O
,	O
but	O
there	O
might	O
be	O
better	O
hyper	O
-	O
parameters	O
for	O
our	O
variants	O
of	O
InPlace	Method
-	Method
ABN	Method
and	Method
InPlace	Method
-	Method
ABNsync	Method
.	O

In	O
essence	O
,	O
our	O
results	O
closely	O
approach	O
reported	O
numbers	O
for	O
Cityscapes	Material
in	O
,	O
which	O
however	O
include	O
more	O
training	O
data	O
in	O
the	O
body	Method
model	Method
training	Method
or	O
are	O
based	O
on	O
already	O
trained	O
models	O
like	O
DeepLabV2	Method
.	O

For	O
COCO	Material
-	Material
Stuff	Material
,	O
results	O
are	O
not	O
directly	O
comparable	O
to	O
the	O
original	O
publication	O
since	O
they	O
used	O
a	O
lower	O
-	O
performing	O
VGG	Method
-	Method
16	Method
network	Method
for	O
the	O
body	O
.	O

However	O
,	O
all	O
experiments	O
show	O
significant	O
improvements	O
the	O
baseline	O
method	O
in	O
the	O
first	O
line	O
of	O
Table	O
[	O
reference	O
]	O
.	O

Optimizing	Task
Settings	Task
for	O
Cityscapes	Material
and	O
Vistas	Material
Datasets	Material
.	O

Here	O
we	O
show	O
further	O
experiments	O
when	O
tuning	O
settings	O
like	O
#	O
training	O
crops	O
and	O
crop	O
sizes	O
in	O
favor	O
of	O
our	O
method	O
(	O
as	O
opposed	O
to	O
maintaining	O
comparability	O
with	O
baselines	O
above	O
)	O
.	O

First	O
,	O
we	O
report	O
Cityscapes	Material
results	O
in	O
Table	O
[	O
reference	O
]	O
when	O
fine	O
-	O
tuning	O
our	O
InPlace	O
-	O
ABN	O
ResNeXt	Method
-	Method
152	Method
ImageNet	O
model	O
from	O
§	O
[	O
reference	O
]	O
,	O
using	O
12	O
crops	O
of	O
size	O
per	O
minibatch	O
(	O
using	O
the	O
gradient	Method
-	Method
synchronized	Method
variant	Method
InPlace	Method
-	Method
ABNsync	Method
)	O
.	O

Going	O
deeper	O
with	O
the	O
body	O
results	O
in	O
a	O
validation	Metric
set	Metric
score	Metric
of	O
78.49	O
%	O
,	O
improving	O
over	O
the	O
score	O
of	O
77.58	O
%	O
obtained	O
by	O
ResNeXt	Method
-	Method
101	Method
,	O
InPlace	O
-	O
ABNsync	Method
.	O

We	O
provide	O
additional	O
results	O
using	O
WideResNet	Task
-	Task
38	Task
InPlace	Task
-	Task
ABNsync	Task
-	Task
based	Task
settings	Task
,	O
where	O
we	O
trained	O
the	O
model	O
with	O
i	O
)	O
16	O
crops	O
at	O
yielding	O
79.02	O
%	O
and	O
ii	O
)	O
12	O
crops	O
at	O
,	O
resulting	O
in	O
79.16	O
%	O
.	O

As	O
can	O
be	O
seen	O
,	O
the	O
combination	O
of	O
InPlace	Method
-	Method
ABNsync	Method
with	O
larger	O
crop	O
sizes	O
improves	O
by	O
over	O
the	O
best	O
performing	O
setting	O
in	O
Table	O
[	O
reference	O
]	O
(	O
InPlace	O
-	O
ABN	O
with	O
20	O
crops	O
at	O
)	O
.	O

We	O
also	O
list	O
a	O
non	O
-	O
gradient	O
synchronized	O
experiment	O
(	O
as	O
this	O
gave	O
the	O
highest	O
score	O
on	O
Cityscapes	Material
before	O
)	O
,	O
where	O
an	O
increase	O
of	O
the	O
crop	Metric
size	Metric
yields	O
to	O
minor	O
improvements	O
,	O
climbing	O
from	O
78.31	O
%	O
to	O
78.45	O
%	O
.	O

Finally	O
,	O
we	O
have	O
run	O
another	O
experiment	O
with	O
12	O
crops	O
at	O
where	O
we	O
however	O
used	O
a	O
different	O
training	Method
data	Method
sampling	Method
strategy	Method
.	O

Instead	O
of	O
just	O
randomly	O
perturbing	O
the	O
dataset	O
and	O
taking	O
training	O
crops	O
from	O
random	O
positions	O
,	O
we	O
compiled	O
the	O
minibatches	O
per	O
epoch	O
in	O
a	O
way	O
to	O
show	O
all	O
classes	O
approximately	O
uniformly	O
(	O
thus	O
following	O
an	O
oversampling	Method
strategy	Method
for	O
underrepresented	O
categories	O
)	O
.	O

In	O
practice	O
,	O
we	O
tracked	O
object	O
class	O
presence	O
for	O
all	O
images	O
and	O
eventually	O
class	O
-	O
uniformly	O
sampled	O
from	O
eligible	O
image	O
candidates	O
,	O
making	O
sure	O
to	O
take	O
training	O
crops	O
from	O
areas	O
containing	O
the	O
class	O
of	O
interest	O
.	O

Applying	O
this	O
sampling	Method
strategy	Method
coined	O
Class	Method
-	Method
Uniform	Method
sampling	Method
yields	O
79.40	O
%	O
,	O
which	O
matches	O
the	O
highest	O
reported	O
score	O
on	O
Cityscapes	Material
validation	O
data	O
reported	O
in	O
,	O
without	O
however	O
using	O
additional	O
training	O
data	O
.	O

Next	O
,	O
we	O
provide	O
results	O
for	O
the	O
Mapillary	Material
Vistas	Material
dataset	O
,	O
using	O
hyperparameter	O
settings	O
inspired	O
by	O
our	O
highest	O
scoring	O
configuration	O
for	O
Cityscapes	Material
.	O

Vistas	Method
is	O
considerably	O
larger	O
than	O
Cityscapes	Material
(	O
in	O
terms	O
of	O
#	O
classes	O
,	O
#	O
images	O
and	O
image	O
resolution	O
)	O
,	O
so	O
running	O
an	O
exhaustive	O
amount	O
of	O
experiments	O
is	O
prohibitive	O
in	O
terms	O
of	O
training	Metric
time	Metric
.	O

Due	O
to	O
the	O
increase	O
of	O
object	O
classes	O
(	O
19	O
for	O
Cityscapes	Material
and	O
65	O
for	O
Vistas	Material
)	O
,	O
we	O
used	O
minibatches	O
of	O
12	O
crops	O
at	O
(	O
with	O
InPlace	O
-	O
ABNsync	O
)	O
,	O
increased	O
the	O
initial	O
learning	Metric
rate	Metric
to	O
and	O
trained	O
for	O
90	O
epochs	O
.	O

This	O
setting	O
leads	O
to	O
the	O
highest	O
reported	O
single	O
-	O
scale	Metric
score	Metric
of	O
53.12	O
%	O
on	O
validation	O
data	O
so	O
far	O
,	O
significantly	O
outperforming	O
the	O
LSUN	Method
2017	Method
segmentation	Method
winner	O
’s	O
single	Method
-	Method
scale	Method
approach	Method
of	O
51.59	O
%	O
.	O

As	O
also	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
,	O
their	O
approach	O
additionally	O
used	O
hybrid	Method
dilated	Method
convolutions	Method
,	O
applied	O
an	O
inverse	Method
frequency	Method
weighting	Method
for	O
correcting	O
training	Task
data	Task
class	Task
imbalance	Task
as	O
well	O
as	O
pretrained	O
on	O
Cityscapes	Material
.	O

subsection	O
:	O
Timing	Task
analyses	Task
Besides	O
the	O
discussed	O
memory	O
improvements	O
and	O
their	O
impact	O
on	O
computer	Task
vision	Task
applications	Task
,	O
we	O
also	O
provide	O
actual	O
runtime	Metric
comparisons	Metric
and	O
analyses	O
for	O
the	O
InPlace	Task
-	Task
ABN	Task
I	O
setting	O
shown	O
in	O
[	O
reference	O
]	O
,	O
as	O
this	O
is	O
the	O
implementation	O
we	O
made	O
publicly	O
available	O
.	O

Isolating	O
a	O
single	O
BN	Method
+	O
Act	O
+	O
Conv	O
block	O
,	O
we	O
evaluate	O
the	O
computational	Metric
times	Metric
required	O
for	O
a	O
forward	O
and	O
backward	O
pass	O
over	O
it	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

We	O
compare	O
the	O
conventional	O
approach	O
of	O
serially	Task
executing	Task
layers	Task
and	O
storing	O
intermediate	O
results	O
(	O
Standard	O
)	O
,	O
our	O
proposed	O
InPlace	Method
-	Method
ABN	Method
I	Method
and	O
the	O
Checkpointing	Method
approach	Method
.	O

In	O
order	O
to	O
obtain	O
fair	O
timing	O
comparisons	O
,	O
we	O
re	O
-	O
implemented	O
the	O
checkpointing	Method
idea	Method
in	O
PyTorch	Method
.	O

The	O
results	O
are	O
obtained	O
by	O
running	O
all	O
operations	O
over	O
a	O
batch	O
comprising	O
32	O
-	O
images	O
and	O
setting	O
the	O
meta	O
-	O
parameters	O
(	O
number	O
of	O
feature	O
channels	O
,	O
spatial	O
dimensions	O
)	O
to	O
those	O
encountered	O
in	O
the	O
four	O
modules	O
of	O
ResNeXt	Method
-	Method
101	Method
,	O
denoted	O
as	O
Conv1	Method
-	Method
Conv4	Method
.	O

The	O
actual	O
runtimes	O
were	O
averaged	O
over	O
200	O
iterations	O
.	O

We	O
observe	O
consistent	O
speed	O
advantages	O
in	O
favor	O
of	O
our	O
method	O
when	O
comparing	O
against	O
Checkpointing	Task
,	O
with	O
the	O
actual	O
percentage	O
difference	O
depending	O
on	O
block	O
’s	O
meta	O
-	O
parameters	O
.	O

As	O
we	O
can	O
see	O
,	O
InPlace	O
-	O
ABN	O
induces	O
computation	Metric
time	Metric
increase	O
between	O
over	O
Standard	O
while	O
Checkpointing	Method
is	O
almost	O
doubling	O
our	O
overheads	O
.	O

section	O
:	O
Conclusions	O
In	O
this	O
work	O
we	O
have	O
presented	O
InPlace	Method
-	Method
ABN	Method
,	O
which	O
is	O
a	O
novel	O
,	O
computationally	O
efficient	O
fusion	O
of	O
batch	Method
normalization	Method
and	O
activation	O
layers	O
,	O
targeting	O
memory	Task
-	Task
optimization	Task
for	O
modern	O
deep	Method
neural	Method
networks	Method
during	O
training	O
time	O
.	O

We	O
reconstruct	O
necessary	O
quantities	O
for	O
the	O
backward	O
pass	O
by	O
inverting	O
the	O
forward	Method
computation	Method
from	O
the	O
storage	O
buffer	O
,	O
and	O
manage	O
to	O
free	O
up	O
almost	O
50	O
%	O
of	O
the	O
memory	O
needed	O
in	O
conventional	O
BN	Method
+	O
Act	O
implementations	O
at	O
little	O
additional	O
computational	Metric
costs	Metric
.	O

In	O
contrast	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
checkpointing	Method
attempts	Method
,	O
our	O
method	O
is	O
reconstructing	O
discarded	O
buffers	O
backwards	O
during	O
the	O
backward	O
pass	O
,	O
thus	O
allowing	O
us	O
to	O
encapsulate	O
BN	Method
+	O
Act	O
as	O
self	Method
-	Method
contained	Method
layer	Method
,	O
which	O
is	O
easy	O
to	O
implement	O
and	O
deploy	O
in	O
virtually	O
all	O
modern	O
deep	Method
learning	Method
frameworks	Method
.	O

We	O
have	O
validated	O
our	O
approach	O
with	O
experiments	O
for	O
image	Task
classification	Task
on	O
ImageNet	Task
-	Task
1k	Task
and	O
semantic	Task
segmentation	Task
on	O
Cityscapes	Material
,	O
COCO	Material
-	Material
Stuff	Material
and	O
Mapillary	Material
Vistas	Material
.	O

Our	O
obtained	O
networks	O
have	O
performed	O
consistently	O
and	O
considerably	O
better	O
when	O
trained	O
with	O
larger	O
batch	O
sizes	O
(	O
or	O
training	O
crop	O
sizes	O
)	O
,	O
leading	O
to	O
a	O
new	O
high	O
-	O
score	O
on	O
the	O
challenging	O
Mapillary	Material
Vistas	Material
dataset	O
in	O
a	O
single	O
-	O
scale	O
,	O
single	O
-	O
model	Task
inference	Task
setting	Task
.	O

In	O
future	O
works	O
,	O
we	O
will	O
investigate	O
the	O
consequences	O
of	O
our	O
approach	O
for	O
problems	O
like	O
object	Task
detection	Task
,	O
instance	Task
-	Task
specific	Task
segmentation	Task
and	O
learning	Task
in	Task
3D.	Task
Derivations	Task
for	O
gradient	Task
computation	Task
are	O
provided	O
in	O
the	O
Appendix	O
.	O

Acknowledgements	O
.	O

We	O
acknowledge	O
financial	O
support	O
from	O
project	O
DIGIMAP	Method
,	O
funded	O
under	O
grant	O
#	O
860375	O
by	O
the	O
Austrian	O
Research	O
Promotion	O
Agency	O
(	O
FFG	O
)	O
.	O

section	O
:	O
Appendix	O
–	O
Derivation	O
of	O
Gradient	Task
We	O
follow	O
the	O
gradient	Method
derivations	Method
as	O
provided	O
in	O
the	O
original	O
batch	Method
normalization	Method
paper	O
and	O
rewrite	O
them	O
as	O
a	O
function	O
of	O
,	O
starting	O
with	O
generally	O
required	O
derivatives	O
for	O
InPlace	O
-	O
ABN	O
and	O
particular	O
ones	O
of	O
InPlace	O
-	O
ABN	O
I.	O
For	O
InPlace	Task
-	Task
ABN	Task
II	Task
,	O
we	O
write	O
gradients	O
and	O
as	O
functions	O
of	O
instead	O
of	O
in	O
the	O
following	O
way	O
:	O
.	O

/	O
bib	O
/	O
GeneralRefs	O
.	O

