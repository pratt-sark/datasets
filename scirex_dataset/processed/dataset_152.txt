document	O
:	O
Multiplicative	Method
LSTM	Method
for	O
sequence	Task
modelling	Task
We	O
introduce	O
multiplicative	Method
LSTM	Method
(	O
mLSTM	Method
)	O
,	O
a	O
recurrent	Method
neural	Method
network	Method
architecture	Method
for	O
sequence	Task
modelling	Task
that	O
combines	O
the	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
and	O
multiplicative	Method
recurrent	Method
neural	Method
network	Method
architectures	Method
.	O

mLSTM	Method
is	O
characterised	O
by	O
its	O
ability	O
to	O
have	O
different	O
recurrent	O
transition	O
functions	O
for	O
each	O
possible	O
input	O
,	O
which	O
we	O
argue	O
makes	O
it	O
more	O
expressive	O
for	O
autoregressive	Task
density	Task
estimation	Task
.	O

We	O
demonstrate	O
empirically	O
that	O
mLSTM	Method
outperforms	O
standard	O
LSTM	Method
and	O
its	O
deep	Method
variants	Method
for	O
a	O
range	O
of	O
character	O
level	O
language	Task
modelling	Task
tasks	O
.	O

In	O
this	O
version	O
of	O
the	O
paper	O
,	O
we	O
regularise	O
mLSTM	Method
to	O
achieve	O
1.27	O
bits	O
/	O
char	O
on	O
text8	Material
and	O
1.24	O
bits	O
/	O
char	O
on	O
Hutter	Material
Prize	Material
.	O

We	O
also	O
apply	O
a	O
purely	O
byte	O
-	O
level	O
mLSTM	Method
on	O
the	O
WikiText	Material
-	Material
2	Material
dataset	Material
to	O
achieve	O
a	O
character	Metric
level	Metric
entropy	Metric
of	O
1.26	O
bits	O
/	O
char	O
,	O
corresponding	O
to	O
a	O
word	Metric
level	Metric
perplexity	Metric
of	O
88.8	O
,	O
which	O
is	O
comparable	O
to	O
word	Method
level	Method
LSTMs	Method
regularised	Method
in	O
similar	O
ways	O
on	O
the	O
same	O
task	O
.	O

section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
are	O
powerful	O
sequence	Method
density	Method
estimators	Method
that	O
can	O
use	O
long	O
contexts	O
to	O
make	O
predictions	O
.	O

They	O
have	O
achieved	O
tremendous	O
success	O
in	O
(	O
conditional	Task
)	Task
sequence	Task
modelling	Task
tasks	Task
such	O
as	O
language	Task
modelling	Task
,	O
machine	Task
translation	Task
and	O
speech	Task
recognition	Task
.	O

Generative	Method
models	Method
of	Method
sequences	Method
can	O
apply	O
factorization	Method
via	O
the	O
product	Method
rule	Method
to	O
perform	O
density	Task
estimation	Task
of	Task
the	Task
sequence	Task
,	O
RNNs	Method
can	O
model	O
sequences	O
with	O
the	O
above	O
factorization	O
by	O
using	O
a	O
hidden	O
state	O
to	O
summarize	O
past	O
inputs	O
.	O

The	O
hidden	O
state	O
vector	O
is	O
updated	O
recursively	O
using	O
the	O
previous	O
hidden	O
state	O
vector	O
and	O
the	O
current	O
input	O
as	O
where	O
is	O
a	O
differentiable	O
function	O
with	O
learnable	O
parameters	O
.	O

In	O
a	O
vanilla	Method
RNN	Method
,	O
multiplies	O
its	O
inputs	O
by	O
a	O
matrix	O
and	O
squashes	O
the	O
result	O
with	O
a	O
non	O
-	O
linear	O
function	O
such	O
as	O
a	O
hyperbolic	O
tangent	O
(	O
)	O
.	O

The	O
updated	O
hidden	O
state	O
vector	O
is	O
then	O
used	O
to	O
predict	O
a	O
probability	O
distribution	O
over	O
the	O
next	O
sequence	O
element	O
,	O
using	O
function	O
.	O

In	O
the	O
case	O
where	O
consists	O
of	O
mutually	O
exclusive	O
discrete	O
outcomes	O
,	O
may	O
apply	O
a	O
matrix	Method
multiplication	Method
followed	O
by	O
a	O
softmax	Method
function	Method
:	O
Generative	Method
RNNs	Method
can	O
evaluate	O
log	O
-	O
likelihoods	O
of	O
sequences	O
exactly	O
,	O
and	O
are	O
differentiable	O
with	O
respect	O
to	O
these	O
log	O
-	O
likelihoods	O
.	O

RNNs	Method
can	O
be	O
difficult	O
to	O
train	O
due	O
to	O
the	O
vanishing	Task
gradient	Task
problem	Task
,	O
but	O
advances	O
such	O
as	O
the	O
long	Method
short	Method
-	Method
term	Method
memory	Method
architecture	O
(	O
LSTM	Method
)	O
have	O
allowed	O
RNNs	Method
to	O
be	O
successful	O
.	O

Despite	O
their	O
success	O
,	O
generative	Method
RNNs	Method
(	O
as	O
well	O
as	O
other	O
conditional	Method
generative	Method
models	Method
)	O
are	O
known	O
to	O
have	O
problems	O
with	O
recovering	O
from	O
mistakes	O
.	O

Each	O
time	O
the	O
recursive	Method
function	Method
of	O
the	O
RNN	Method
is	O
applied	O
and	O
the	O
hidden	O
state	O
is	O
updated	O
,	O
the	O
RNN	Method
must	O
decide	O
which	O
information	O
from	O
the	O
previous	O
hidden	O
state	O
to	O
store	O
,	O
due	O
to	O
its	O
limited	O
capacity	O
.	O

If	O
the	O
RNN	Method
â€™s	Method
hidden	Method
representation	Method
remembers	O
the	O
wrong	O
information	O
and	O
reaches	O
a	O
bad	O
numerical	O
state	O
for	O
predicting	O
future	O
sequence	O
elements	O
,	O
for	O
instance	O
as	O
a	O
result	O
of	O
an	O
unexpected	O
input	O
,	O
it	O
may	O
take	O
many	O
time	O
-	O
steps	O
to	O
recover	O
.	O

We	O
argue	O
that	O
RNN	Method
architectures	Method
with	O
hidden	O
-	O
to	O
-	O
hidden	O
transition	O
functions	O
that	O
are	O
input	O
-	O
dependent	O
are	O
better	O
suited	O
to	O
recover	O
from	O
surprising	O
inputs	O
.	O

Our	O
approach	O
to	O
generative	Method
RNNs	Method
combines	O
LSTM	Method
units	O
with	O
multiplicative	Method
RNN	Method
(	O
mRNN	Method
)	O
factorized	O
hidden	O
weights	O
,	O
allowing	O
flexible	O
input	O
-	O
dependent	O
transitions	O
that	O
are	O
easier	O
to	O
control	O
due	O
to	O
the	O
gating	Method
units	Method
of	O
LSTM	Method
.	O

We	O
compare	O
this	O
multiplicative	Method
LSTM	Method
hybrid	O
architecture	O
with	O
other	O
variants	O
of	O
LSTM	Method
on	O
a	O
range	O
of	O
character	O
level	O
language	Task
modelling	Task
tasks	O
.	O

Multiplicative	Method
LSTM	Method
is	O
most	O
appropriate	O
when	O
it	O
can	O
learn	O
parameters	O
specifically	O
for	O
each	O
possible	O
input	O
at	O
a	O
given	O
timestep	O
.	O

Therefore	O
,	O
its	O
main	O
application	O
is	O
to	O
sequences	O
of	O
discrete	O
mutually	O
exclusive	O
elements	O
,	O
such	O
as	O
language	Task
modelling	Task
and	O
related	O
problems	O
.	O

subsection	O
:	O
Input	O
-	O
dependent	O
transition	O
functions	O
RNNs	Method
learn	O
a	O
mapping	O
from	O
previous	O
hidden	O
state	O
and	O
input	O
to	O
hidden	O
state	O
.	O

Let	O
denote	O
the	O
input	O
to	O
the	O
next	O
hidden	O
state	O
before	O
any	O
non	O
-	O
linear	O
operation	O
:	O
where	O
is	O
the	O
hidden	O
-	O
to	O
-	O
hidden	O
weight	O
matrix	O
,	O
and	O
is	O
the	O
input	O
-	O
to	O
-	O
hidden	O
weight	O
matrix	O
.	O

For	O
problems	O
such	O
as	O
language	Task
modelling	Task
,	O
is	O
a	O
one	O
-	O
hot	O
vector	O
,	O
meaning	O
that	O
the	O
output	O
of	O
is	O
a	O
column	O
in	O
,	O
corresponding	O
to	O
the	O
unit	O
element	O
in	O
.	O

The	O
possible	O
future	O
hidden	O
states	O
in	O
an	O
RNN	Method
can	O
be	O
viewed	O
as	O
a	O
tree	O
structure	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

For	O
an	O
alphabet	O
of	O
inputs	O
and	O
a	O
fixed	O
,	O
there	O
will	O
be	O
possible	O
transition	O
functions	O
between	O
and	O
.	O

The	O
relative	O
magnitude	O
of	O
to	O
will	O
need	O
to	O
be	O
large	O
for	O
the	O
RNN	Method
to	O
be	O
able	O
to	O
use	O
long	O
range	O
dependencies	O
,	O
and	O
the	O
resulting	O
possible	O
hidden	O
state	O
vectors	O
will	O
therefore	O
be	O
highly	O
correlated	O
across	O
the	O
possible	O
inputs	O
,	O
limiting	O
the	O
width	O
of	O
the	O
tree	O
and	O
making	O
it	O
harder	O
for	O
the	O
RNN	Method
to	O
form	O
distinct	O
hidden	Method
representations	Method
for	O
different	O
sequences	O
of	O
inputs	O
.	O

However	O
,	O
if	O
the	O
RNN	Method
has	O
flexible	O
input	O
-	O
dependent	O
transition	O
functions	O
,	O
the	O
tree	O
will	O
be	O
able	O
to	O
grow	O
wider	O
more	O
quickly	O
,	O
giving	O
the	O
RNN	Method
the	O
flexibility	O
to	O
represent	O
more	O
probability	O
distributions	O
.	O

In	O
a	O
vanilla	Method
RNN	Method
,	O
it	O
is	O
difficult	O
to	O
allow	O
inputs	O
to	O
greatly	O
affect	O
the	O
hidden	O
state	O
vector	O
without	O
erasing	O
information	O
from	O
the	O
past	O
hidden	O
state	O
.	O

However	O
,	O
an	O
RNN	Method
with	O
a	O
transition	Method
function	Method
mapping	Method
dependent	O
on	O
the	O
input	O
would	O
allow	O
the	O
relative	O
values	O
of	O
to	O
vary	O
with	O
each	O
possible	O
input	O
,	O
without	O
overwriting	O
the	O
contribution	O
from	O
the	O
previous	O
hidden	O
state	O
,	O
allowing	O
for	O
more	O
long	O
term	O
information	O
to	O
be	O
stored	O
.	O

This	O
ability	O
to	O
adjust	O
to	O
new	O
inputs	O
quickly	O
while	O
limiting	O
the	O
overwriting	O
of	O
information	O
should	O
make	O
an	O
RNN	Method
more	O
robust	O
to	O
mistakes	O
when	O
it	O
encounters	O
surprising	O
inputs	O
,	O
as	O
the	O
hidden	O
vector	O
is	O
less	O
likely	O
to	O
get	O
trapped	O
in	O
a	O
bad	O
numerical	O
state	O
for	O
making	O
future	Task
predictions	Task
.	O

subsection	O
:	O
Multiplicative	Method
RNN	Method
The	O
multiplicative	Method
RNN	Method
(	O
mRNN	Method
)	O
is	O
an	O
architecture	O
designed	O
specifically	O
to	O
allow	O
flexible	O
input	O
-	O
dependent	O
transitions	O
.	O

Its	O
formulation	O
was	O
inspired	O
by	O
the	O
tensor	Method
RNN	Method
,	O
an	O
RNN	Method
architecture	Method
that	O
allows	O
for	O
a	O
different	O
transition	O
matrix	O
for	O
each	O
possible	O
input	O
.	O

The	O
tensor	Method
RNN	Method
features	O
a	O
3	O
-	O
way	O
tensor	O
,	O
which	O
contains	O
a	O
separately	O
learned	O
transition	O
matrix	O
for	O
each	O
input	O
dimension	O
.	O

The	O
3	O
-	O
way	O
tensor	O
can	O
be	O
stored	O
as	O
an	O
array	O
of	O
matrices	O
where	O
superscript	O
is	O
used	O
to	O
denote	O
the	O
index	O
in	O
the	O
array	O
,	O
and	O
is	O
the	O
dimensionality	O
of	O
.	O

The	O
specific	O
hidden	O
-	O
to	O
-	O
hidden	O
weight	O
matrix	O
used	O
for	O
a	O
given	O
input	O
is	O
then	O
For	O
language	Task
modelling	Task
problems	O
,	O
only	O
one	O
unit	O
of	O
will	O
be	O
on	O
,	O
and	O
will	O
be	O
the	O
matrix	O
in	O
corresponding	O
to	O
that	O
unit	O
.	O

Hidden	Task
-	Task
to	Task
-	Task
hidden	Task
propagation	Task
in	O
the	O
tensor	Method
RNN	Method
is	O
then	O
given	O
by	O
The	O
large	O
number	O
of	O
parameters	O
in	O
the	O
tensor	Method
RNN	Method
make	O
it	O
impractical	O
for	O
most	O
problems	O
.	O

mRNNs	Method
can	O
be	O
thought	O
of	O
as	O
a	O
shared	Method
-	Method
parameter	Method
approximation	Method
to	O
the	O
tensor	Method
RNN	Method
that	O
use	O
a	O
factorized	O
hidden	O
-	O
to	O
-	O
hidden	O
transition	O
matrix	O
in	O
place	O
of	O
the	O
normal	O
RNN	O
hidden	O
-	O
to	O
-	O
hidden	O
matrix	O
,	O
with	O
an	O
input	O
-	O
dependent	O
intermediate	O
diagonal	O
matrix	O
.	O

The	O
input	O
-	O
dependent	O
hidden	O
-	O
to	O
-	O
hidden	O
weight	O
matrix	O
,	O
is	O
then	O
An	O
mRNN	Method
is	O
thus	O
equivalent	O
to	O
a	O
tensor	Method
RNN	Method
using	O
the	O
above	O
form	O
for	O
.	O

For	O
readability	O
,	O
an	O
mRNN	Method
can	O
also	O
be	O
described	O
using	O
intermediate	O
state	O
as	O
follows	O
:	O
mRNNs	Method
have	O
improved	O
on	O
vanilla	Method
RNNs	Method
at	O
character	O
level	O
language	Task
modelling	Task
tasks	O
,	O
but	O
have	O
fallen	O
short	O
of	O
the	O
more	O
popular	O
LSTM	Method
architecture	O
,	O
for	O
instance	O
as	O
shown	O
with	O
LSTM	Method
baselines	O
from	O
.	O

The	O
standard	O
RNN	Method
units	Method
in	O
an	O
mRNN	Method
do	O
not	O
provide	O
an	O
easy	O
way	O
for	O
information	O
to	O
bypass	O
its	O
complex	O
transitions	O
,	O
resulting	O
in	O
the	O
potential	O
for	O
difficulty	O
in	O
retaining	Task
long	Task
term	Task
information	Task
.	O

subsection	O
:	O
Long	Task
short	Task
-	Task
term	Task
memory	Task
LSTM	Method
is	O
a	O
commonly	O
used	O
RNN	Method
architecture	Method
that	O
uses	O
a	O
series	O
of	O
multiplicative	O
gates	O
to	O
control	O
how	O
information	O
flows	O
in	O
and	O
out	O
of	O
internal	O
states	O
of	O
the	O
network	O
.	O

There	O
are	O
several	O
slightly	O
different	O
variants	O
of	O
LSTM	Method
,	O
and	O
we	O
present	O
the	O
variant	O
used	O
in	O
our	O
experiments	O
.	O

The	O
LSTM	Method
hidden	O
state	O
receives	O
inputs	O
from	O
the	O
input	O
layer	O
and	O
the	O
previous	O
hidden	O
state	O
:	O
The	O
LSTM	Method
network	O
also	O
has	O
3	O
gating	Method
units	Method
â€“	O
input	O
gate	O
,	O
output	O
gate	O
,	O
and	O
forget	O
gate	O
â€“	O
that	O
have	O
both	O
recurrent	O
and	O
feed	O
-	O
forward	O
connections	O
:	O
where	O
is	O
the	O
logistic	O
sigmoid	O
function	O
.	O

The	O
input	O
gate	O
controls	O
how	O
much	O
of	O
the	O
input	O
to	O
each	O
hidden	O
unit	O
is	O
written	O
to	O
the	O
internal	O
state	O
vector	O
,	O
and	O
the	O
forget	O
gate	O
determines	O
how	O
much	O
of	O
the	O
previous	O
internal	O
state	O
is	O
preserved	O
.	O

This	O
combination	O
of	O
write	O
and	O
forget	O
gates	O
allows	O
the	O
network	O
to	O
control	O
what	O
information	O
should	O
be	O
stored	O
and	O
overwritten	O
across	O
each	O
time	O
-	O
step	O
.	O

The	O
internal	O
state	O
is	O
updated	O
by	O
The	O
output	O
gate	O
controls	O
how	O
much	O
of	O
each	O
unit	O
â€™s	O
activation	O
is	O
preserved	O
.	O

It	O
allows	O
the	O
LSTM	Method
cell	O
to	O
keep	O
information	O
that	O
is	O
not	O
relevant	O
to	O
the	O
current	O
output	O
,	O
but	O
may	O
be	O
relevant	O
later	O
.	O

The	O
final	O
output	O
of	O
the	O
hidden	O
state	O
is	O
given	O
by	O
LSTM	Method
â€™s	O
ability	O
to	O
control	O
how	O
information	O
is	O
stored	O
in	O
each	O
unit	O
has	O
proven	O
generally	O
useful	O
.	O

subsection	O
:	O
Comparing	O
LSTM	Method
with	O
mRNN	Method
The	O
LSTM	Method
and	O
mRNN	Method
architectures	O
both	O
feature	Method
multiplicative	Method
units	Method
,	O
but	O
these	O
units	O
serve	O
different	O
purposes	O
.	O

LSTM	Method
â€™s	O
gates	O
are	O
designed	O
to	O
control	O
the	O
flow	O
of	O
information	O
through	O
the	O
network	O
,	O
whereas	O
mRNN	Method
â€™s	O
gates	O
are	O
designed	O
to	O
allow	O
transition	O
functions	O
to	O
vary	O
across	O
inputs	O
.	O

LSTM	Method
gates	O
receive	O
input	O
from	O
both	O
the	O
input	O
units	O
and	O
hidden	O
units	O
,	O
allowing	O
multiplicative	O
interactions	O
between	O
hidden	O
units	O
,	O
but	O
also	O
potentially	O
limiting	O
the	O
extent	O
of	O
input	O
-	O
hidden	O
multiplicative	O
interaction	O
.	O

LSTM	Method
gates	O
are	O
also	O
squashed	O
with	O
a	O
sigmoid	O
,	O
forcing	O
them	O
to	O
take	O
values	O
between	O
0	O
and	O
1	O
,	O
which	O
makes	O
them	O
easier	O
to	O
control	O
,	O
but	O
less	O
expressive	O
than	O
mRNN	Method
â€™s	O
linear	O
gates	O
.	O

For	O
language	Task
modelling	Task
problems	O
,	O
mRNN	Method
â€™s	O
linear	O
gates	O
do	O
not	O
need	O
to	O
be	O
controlled	O
by	O
the	O
network	O
because	O
they	O
are	O
explicitly	O
learned	O
for	O
each	O
input	O
.	O

They	O
are	O
also	O
placed	O
in	O
between	O
a	O
product	O
of	O
2	O
dense	O
matrices	O
,	O
giving	O
more	O
flexibility	O
to	O
the	O
possible	O
values	O
of	O
the	O
final	O
product	O
of	O
matrices	O
.	O

section	O
:	O
Multiplicative	Method
LSTM	Method
Since	O
the	O
LSTM	Method
and	O
mRNN	Method
architectures	Method
are	O
complimentary	O
,	O
we	O
propose	O
the	O
multiplicative	Method
LSTM	Method
(	O
mLSTM	Method
)	O
,	O
a	O
hybrid	Method
architecture	Method
that	O
combines	O
the	O
factorized	O
hidden	O
-	O
to	O
-	O
hidden	O
transition	O
of	O
mRNNs	Method
with	O
the	O
gating	Method
framework	Method
from	O
LSTMs	Method
.	O

The	O
mRNN	Method
and	O
LSTM	Method
architectures	Method
can	O
be	O
combined	O
by	O
adding	O
connections	O
from	O
the	O
mRNN	Method
â€™s	O
intermediate	O
state	O
(	O
which	O
is	O
redefined	O
below	O
for	O
convenience	O
)	O
to	O
each	O
gating	O
units	O
in	O
the	O
LSTM	Method
,	O
resulting	O
in	O
the	O
following	O
system	O
:	O
We	O
set	O
the	O
dimensionality	O
of	O
and	O
equal	O
for	O
all	O
our	O
experiments	O
.	O

We	O
also	O
chose	O
to	O
share	O
across	O
all	O
LSTM	Method
unit	O
types	O
,	O
resulting	O
in	O
a	O
model	O
with	O
1.25	O
times	O
the	O
number	O
of	O
recurrent	O
weights	O
as	O
LSTM	Method
for	O
the	O
same	O
number	O
of	O
hidden	O
units	O
.	O

The	O
goal	O
of	O
this	O
architecture	O
is	O
to	O
combine	O
the	O
flexible	O
input	O
-	O
dependent	O
transitions	O
of	O
mRNNs	Method
with	O
the	O
long	O
time	O
lag	O
and	O
information	Method
control	Method
of	Method
LSTMs	Method
.	O

The	O
gated	O
units	O
of	O
LSTMs	Method
could	O
make	O
it	O
easier	O
to	O
control	O
(	O
or	O
bypass	O
)	O
the	O
complex	O
transitions	O
in	O
that	O
result	O
from	O
the	O
factorized	O
hidden	O
weight	O
matrix	O
.	O

The	O
additional	O
sigmoid	O
input	O
and	O
forget	O
gates	O
featured	O
in	O
LSTM	Method
units	O
allow	O
even	O
more	O
flexible	O
input	O
-	O
dependent	O
transition	O
functions	O
than	O
in	O
regular	Method
mRNNs	Method
.	O

section	O
:	O
Related	O
approaches	O
Many	O
recently	O
proposed	O
RNN	Method
architectures	Method
use	O
recurrent	O
depth	O
,	O
which	O
is	O
depth	O
between	O
recurrent	O
steps	O
.	O

Recurrent	O
depth	O
allows	O
more	O
non	O
-	O
linearity	O
in	O
the	O
combination	O
of	O
inputs	O
and	O
previous	O
hidden	O
states	O
from	O
every	O
time	O
step	O
,	O
which	O
in	O
turn	O
allows	O
for	O
more	O
flexible	O
input	O
-	O
dependent	O
transitions	O
.	O

Recurrent	O
depth	O
has	O
been	O
found	O
to	O
perform	O
better	O
than	O
other	O
kinds	O
of	O
non	Method
-	Method
recurrent	Method
depth	Method
for	O
sequence	Task
modelling	Task
.	O

Recurrent	Method
highway	Method
networks	Method
(	O
RHNs	Method
)	O
use	O
a	O
more	O
sophisticated	O
recurrent	O
depth	O
that	O
carefully	O
controls	O
propagation	O
through	O
layers	O
using	O
gating	O
units	O
.	O

The	O
gating	Method
units	Method
also	O
allow	O
for	O
a	O
greater	O
deal	O
of	O
multiplicative	O
interaction	O
between	O
the	O
inputs	O
and	O
hidden	Method
units	Method
.	O

While	O
adding	O
recurrent	O
depth	O
could	O
improve	O
our	O
model	O
,	O
we	O
believe	O
that	O
maximizing	O
the	O
input	O
-	O
dependent	O
flexibility	O
of	O
the	O
transition	O
function	O
is	O
more	O
important	O
for	O
expressive	Task
sequence	Task
modelling	Task
.	O

Recurrent	O
depth	O
can	O
do	O
this	O
through	O
non	Method
-	Method
linear	Method
layers	Method
combining	O
hidden	O
and	O
input	O
contributions	O
,	O
but	O
our	O
method	O
can	O
do	O
this	O
independently	O
of	O
non	O
-	O
linear	O
depth	O
.	O

Another	O
approach	O
,	O
multiplicative	Method
integration	Method
RNNs	Method
(	O
MI	Method
-	Method
RNNs	Method
)	O
,	O
use	O
Hadamard	O
products	O
instead	O
of	O
addition	O
when	O
combining	O
contributions	O
from	O
input	O
and	O
hidden	O
units	O
.	O

When	O
applying	O
this	O
to	O
LSTM	Method
,	O
this	O
architecture	O
achieves	O
impressive	O
sequence	Task
modelling	Task
results	O
.	O

The	O
main	O
difference	O
between	O
multiplicative	Method
integration	Method
LSTM	Method
and	O
mLSTM	Method
is	O
that	O
mLSTM	Method
applies	O
the	O
Hadamard	O
product	O
between	O
the	O
multiplication	O
of	O
two	O
matrices	O
.	O

In	O
the	O
case	O
of	O
LSTM	Method
,	O
this	O
allows	O
for	O
the	O
potential	O
for	O
greater	O
expressiveness	O
,	O
without	O
significantly	O
increasing	O
the	O
size	O
of	O
the	O
model	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
System	O
Setup	O
Our	O
experiments	O
measure	O
the	O
performance	O
of	O
mLSTM	Method
for	O
character	O
-	O
level	O
language	Task
modelling	Task
tasks	O
of	O
varying	O
complexity	O
.	O

Our	O
initial	O
experiments	O
,	O
which	O
appeared	O
in	O
previous	O
versions	O
of	O
this	O
work	O
,	O
were	O
mainly	O
designed	O
to	O
compare	O
the	O
convergence	Metric
and	O
final	O
performance	O
of	O
mLSTM	Method
vs	O
LSTM	Method
and	O
its	O
deep	Method
variants	Method
.	O

Our	O
follow	O
up	O
experiments	O
explored	O
training	Task
and	O
regularisation	Method
of	O
mLSTM	Method
in	O
more	O
detail	O
,	O
with	O
goal	O
of	O
comparing	O
more	O
directly	O
with	O
the	O
most	O
competitive	O
architectures	O
in	O
the	O
literature	O
.	O

Our	O
initial	O
and	O
follow	O
up	O
experiments	O
used	O
slightly	O
different	O
set	O
ups	O
;	O
initial	O
experiments	O
used	O
a	O
variant	O
of	O
RMSprop	Method
,	O
,	O
with	O
normalized	O
updates	O
in	O
place	O
of	O
a	O
learning	Metric
rate	Metric
.	O

All	O
unnormalized	O
update	O
directions	O
,	O
computed	O
by	O
RMSprop	Method
,	O
were	O
normalized	O
to	O
have	O
length	O
,	O
where	O
was	O
decayed	O
exponentially	O
over	O
training	O
:	O
This	O
update	Method
rule	Method
is	O
similar	O
to	O
applying	O
gradient	Method
norm	Method
clipping	Method
,	O
with	O
a	O
very	O
high	O
learning	Metric
rate	Metric
balanced	O
out	O
by	O
a	O
very	O
low	O
gradient	O
norm	O
threshold	O
.	O

The	O
initial	O
experiments	O
also	O
used	O
a	O
slightly	O
non	O
-	O
standard	O
version	O
of	O
LSTM	Method
(	O
and	O
mLSTM	Method
)	O
with	O
the	O
output	O
gate	O
inside	O
of	O
the	O
final	O
tanh	O
of	O
the	O
LSTM	Method
cell	O
.	O

This	O
gave	O
us	O
slightly	O
better	O
results	O
in	O
preliminary	O
experiments	O
with	O
very	O
small	O
models	O
,	O
but	O
likely	O
does	O
not	O
make	O
much	O
difference	O
.	O

We	O
use	O
LSTM	Method
(	O
RMSprop	Method
)	O
and	O
mLSTM	Method
(	O
RMSprop	Method
)	O
in	O
tables	O
to	O
distinguish	O
results	O
obtained	O
by	O
these	O
initial	O
set	O
of	O
experiments	O
.	O

For	O
our	O
follow	O
up	O
experiments	O
,	O
we	O
use	O
more	O
standard	O
methodology	O
to	O
be	O
more	O
comparable	O
to	O
the	O
literature	O
.	O

We	O
used	O
ADAM	Method
,	O
always	O
starting	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
and	O
decaying	O
this	O
linearly	O
to	O
a	O
minimum	O
learning	Metric
rate	Metric
(	O
which	O
was	O
always	O
in	O
the	O
range	O
to	O
)	O
.	O

The	O
mLSTMs	Method
used	O
the	O
standard	O
LSTM	Method
cell	O
with	O
the	O
output	O
gate	O
outside	O
the	O
tanh	O
.	O

These	O
mLSTMs	Method
also	O
used	O
scaled	O
orthogonal	O
initialisations	O
for	O
the	O
hidden	O
weights	O
,	O
an	O
initial	O
forget	O
gate	O
bias	O
of	O
3	O
,	O
and	O
truncated	O
backpropogation	O
lengths	O
from	O
200	O
to	O
250	O
.	O

We	O
compared	O
mLSTM	Method
to	O
previously	O
reported	O
regular	Method
LSTM	Method
,	O
stacked	O
LSTM	Method
,	O
and	O
RNN	Method
character	Method
-	Method
level	Method
language	Method
models	Method
.	O

We	O
run	O
detailed	O
experiments	O
on	O
the	O
text8	Material
and	O
Hutter	Material
Prize	Material
datasets	Material
to	O
test	O
medium	Method
scale	Method
character	Method
-	Method
level	Method
language	Method
modelling	Method
.	O

We	O
test	O
our	O
best	O
model	O
from	O
these	O
experiments	O
on	O
the	O
WikiText	Material
-	Material
2	Material
dataset	Material
to	O
measure	O
performance	O
on	O
smaller	O
scale	O
character	O
level	O
language	Task
modelling	Task
,	O
and	O
to	O
compare	O
with	O
word	Method
level	Method
models	Method
.	O

Previous	O
versions	O
of	O
the	O
paper	O
also	O
report	O
a	O
character	O
level	O
result	O
on	O
Penn	Material
Treebank	Material
dataset	Material
of	O
1.35	O
bits	O
/	O
char	O
with	O
an	O
unregularised	Method
mLSTM	Method
,	O
however	O
we	O
do	O
not	O
include	O
this	O
experiment	O
in	O
this	O
version	O
as	O
we	O
have	O
no	O
results	O
with	O
our	O
updated	O
training	O
and	O
regularisation	Method
methodology	O
.	O

subsection	O
:	O
Hutter	Material
Prize	Material
dataset	O
We	O
performed	O
experiments	O
using	O
the	O
Hutter	Material
Prize	Material
dataset	O
,	O
originally	O
used	O
for	O
the	O
Hutter	Material
Prize	Material
compression	O
benchmark	O
.	O

This	O
dataset	O
consists	O
mostly	O
of	O
English	O
language	O
text	O
and	O
mark	O
-	O
up	O
language	O
text	O
,	O
but	O
also	O
contains	O
text	O
in	O
other	O
languages	O
,	O
including	O
non	O
-	O
Latin	O
languages	O
.	O

The	O
dataset	O
is	O
modelled	O
using	O
a	O
UTF	Method
-	Method
8	Method
encoding	Method
,	O
and	O
contains	O
205	O
unique	O
bytes	O
.	O

In	O
our	O
initial	O
experiments	O
,	O
we	O
compared	O
mLSTMs	Method
and	O
2	Method
-	Method
layer	Method
stacked	Method
LSTMs	Method
for	O
varying	O
network	O
sizes	O
,	O
ranging	O
from	O
about	O
3â€“20	O
million	O
parameters	O
.	O

These	O
results	O
all	O
used	O
RMS	Method
prop	Method
with	O
normalized	O
updates	O
,	O
stopping	O
after	O
4	O
epochs	O
on	O
the	O
first	O
95	O
million	O
characters	O
,	O
with	O
test	O
performance	O
measured	O
on	O
the	O
last	O
5	O
million	O
bytes	O
.	O

Hyperparameters	Method
for	O
each	O
mLSTM	Method
and	O
stacked	O
LSTM	Method
were	O
kept	O
constant	O
across	O
all	O
sizes	O
.	O

The	O
results	O
,	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
show	O
that	O
mLSTM	Method
gives	O
an	O
improvement	O
across	O
all	O
network	O
sizes	O
.	O

We	O
hypothesized	O
that	O
mLSTM	Method
â€™s	O
superior	O
performance	O
over	O
stacked	O
LSTM	Method
was	O
in	O
part	O
due	O
to	O
its	O
ability	O
to	O
recover	O
from	O
surprising	O
inputs	O
.	O

To	O
test	O
this	O
we	O
looked	O
at	O
each	O
network	O
â€™s	O
performance	O
after	O
viewing	O
surprising	O
inputs	O
that	O
occurred	O
naturally	O
in	O
the	O
test	O
set	O
by	O
creating	O
a	O
set	O
of	O
the	O
10	O
%	O
characters	O
with	O
the	O
largest	O
average	O
loss	O
taken	O
by	O
mLSTM	Method
and	O
stacked	O
LSTM	Method
.	O

Both	O
networks	O
perform	O
roughly	O
equally	O
on	O
this	O
set	O
of	O
surprising	O
characters	O
,	O
with	O
mLSTM	Method
and	O
stacked	O
LSTM	Method
taking	O
losses	O
of	O
6.27	O
bits	O
/	O
character	O
and	O
6.29	O
bits	O
/	O
character	O
respectively	O
.	O

However	O
,	O
stacked	O
LSTM	Method
tended	O
to	O
take	O
much	O
larger	O
losses	O
than	O
mLSTM	Method
in	O
the	O
timesteps	O
immediately	O
following	O
surprising	O
inputs	O
.	O

One	O
to	O
four	O
time	O
-	O
steps	O
after	O
a	O
surprising	O
input	O
occurred	O
,	O
mLSTM	Method
and	O
stacked	O
LSTM	Method
took	O
average	O
losses	O
of	O
(	O
2.26	O
,	O
2.04	O
,	O
1.61	O
,	O
1.51	O
)	O
and	O
(	O
2.48	O
,	O
2.25	O
,	O
1.79	O
,	O
1.67	O
)	O
bits	O
per	O
character	O
respectively	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

mLSTM	Method
â€™s	O
overall	O
advantage	O
over	O
stacked	O
LSTM	Method
was	O
1.42	O
bits	O
/	O
char	O
to	O
1.53	O
bits	O
/	O
char	O
;	O
mLSTM	Method
â€™s	O
advantage	O
over	O
stacked	O
LSTM	Method
was	O
greater	O
after	O
a	O
surprising	O
input	O
than	O
it	O
is	O
in	O
general	O
.	O

We	O
also	O
explore	O
more	O
standard	O
training	Method
methodology	Method
and	O
regularisation	Method
methods	O
on	O
this	O
dataset	O
.	O

These	O
experiments	O
all	O
used	O
ADAM	Method
,	O
and	O
the	O
standard	O
90	O
-	O
5	O
-	O
5	O
training	Metric
validation	Metric
test	Metric
split	Metric
on	O
this	O
dataset	O
.	O

We	O
firstly	O
consider	O
a	O
standard	O
unregularised	Method
mLSTM	Method
trained	O
with	O
this	O
methodology	O
.	O

We	O
then	O
experiment	O
with	O
an	O
mLSTM	Method
with	O
a	O
linear	O
embedding	Method
layer	Method
and	O
weight	Method
normalization	Method
on	O
recurrent	O
weights	O
(	O
mLSTM	Method
+	O
emb	Method
+	O
WN	Method
)	O
,	O
which	O
is	O
similar	O
to	O
the	O
mLSTM	Method
architecture	Method
used	O
in	O
,	O
which	O
was	O
built	O
off	O
our	O
initial	O
work	O
.	O

We	O
also	O
consider	O
regularisation	Method
of	O
the	O
later	Method
model	Method
with	O
variational	Method
dropout	Method
(	O
mLSTM	Method
+	O
emb	Method
+	O
WN	Method
+	O
VD	Method
)	O
.	O

Variational	Method
dropout	Method
is	O
a	O
form	O
of	O
dropout	Method
where	O
the	O
dropout	O
mask	O
is	O
shared	O
across	O
a	O
sequence	O
.	O

The	O
standard	O
unregularised	Method
LSTM	Method
used	O
1900	O
hidden	O
units	O
and	O
20	O
million	O
parameters	O
.	O

The	O
weight	Method
normalized	Method
mLSTM	Method
used	O
1900	O
hidden	O
units	O
,	O
and	O
a	O
linear	O
embedding	Method
layer	Method
of	O
400	O
,	O
giving	O
it	O
22	O
million	O
parameters	O
.	O

The	O
large	O
embedding	Method
layer	Method
was	O
used	O
because	O
it	O
was	O
found	O
to	O
work	O
well	O
with	O
dropout	Method
.	O

Since	O
this	O
embedding	Method
layer	Method
is	O
linear	O
,	O
it	O
could	O
potentially	O
be	O
removed	O
during	O
test	O
time	O
by	O
multiplying	O
its	O
incoming	O
and	O
outgoing	O
weight	O
matrices	O
to	O
reduce	O
the	O
number	O
of	O
parameters	O
(	O
however	O
we	O
report	O
parameter	O
numbers	O
with	O
the	O
embedding	Method
layer	Method
)	O
.	O

For	O
the	O
regularised	O
weight	O
normalized	O
mLSTM	Method
,	O
we	O
apply	O
a	O
variational	Method
dropout	Method
of	O
0.2	O
to	O
the	O
hidden	O
state	O
and	O
to	O
the	O
embedding	Method
layer	Method
(	O
dropout	O
masks	O
for	O
both	O
the	O
hidden	O
state	O
and	O
embedding	Method
layer	Method
were	O
shared	O
across	O
a	O
sequence	O
)	O
.	O

We	O
also	O
consider	O
a	O
larger	O
version	O
of	O
the	O
weight	Method
normalized	Method
mLSTM	Method
with	O
2800	O
hidden	O
units	O
and	O
46	O
million	O
parameters	O
.	O

We	O
increased	O
the	O
dropout	O
in	O
the	O
embedding	Method
layer	Method
to	O
0.5	O
on	O
this	O
model	O
.	O

All	O
results	O
without	O
variational	Method
dropout	Method
used	O
early	O
stopping	O
on	O
the	O
validation	Metric
error	Metric
to	O
reduce	O
overfitting	O
.	O

The	O
results	O
for	O
these	O
experiments	O
are	O
given	O
in	O
table	O
[	O
reference	O
]	O
.	O

Interestingly	O
,	O
adding	O
weight	Method
normalization	Method
and	O
an	O
embedding	Method
layer	Method
hurt	O
performance	O
in	O
the	O
absence	O
of	O
regularisation	Method
.	O

However	O
,	O
when	O
combined	O
with	O
variational	Method
dropout	Method
,	O
this	O
model	O
outperformed	O
all	O
previous	O
static	Method
single	Method
model	Method
neural	Method
network	Method
results	O
on	O
Hutter	Material
Prize	Material
.	O

We	O
did	O
not	O
explore	O
variational	Method
dropout	Method
applied	O
to	O
mLSTM	Method
without	O
weight	Method
normalization	Method
.	O

Earlier	O
versions	O
of	O
this	O
work	O
also	O
considered	O
dynamic	Task
evaluation	Task
of	Task
mLSTMs	Task
on	O
this	O
task	O
,	O
however	O
this	O
is	O
now	O
in	O
a	O
separate	O
paper	O
focused	O
on	O
dynamic	Task
evaluation	Task
.	O

We	O
also	O
tested	O
an	O
MI	Method
-	Method
LSTM	Method
,	O
mLSTM	Method
â€™s	O
nearest	O
neighbor	O
,	O
with	O
a	O
slightly	O
larger	O
size	O
(	O
22	O
M	O
parameters	O
)	O
and	O
a	O
very	O
similar	O
hyperparameter	Method
configuration	Method
and	O
initialisation	Method
scheme	Method
(	O
compared	O
with	O
unregularised	Method
mLSTM	Method
with	O
no	O
WN	Method
)	O
.	O

MI	Method
-	Method
LSTM	Method
achieved	O
a	O
relatively	O
poor	O
test	Metric
set	Metric
performance	O
of	O
1.53	O
bits	O
/	O
char	O
,	O
as	O
compared	O
with	O
1.40	O
bits	O
/	O
char	O
for	O
mLSTM	Method
under	O
the	O
same	O
settings	O
.	O

The	O
MI	Method
-	Method
LSTM	Method
also	O
converged	O
more	O
slowly	O
,	O
although	O
eventually	O
did	O
require	O
early	O
stopping	O
like	O
the	O
mLSTM	Method
.	O

While	O
this	O
particular	O
experiment	O
can	O
not	O
conclusively	O
prove	O
anything	O
about	O
the	O
relative	O
utility	O
of	O
mLSTM	Method
vs.	O
MI	Method
-	Method
LSTM	Method
on	O
this	O
task	O
,	O
it	O
does	O
show	O
that	O
the	O
two	O
architectures	O
are	O
sufficiently	O
different	O
to	O
obtain	O
very	O
different	O
results	O
under	O
the	O
same	O
hyper	O
-	O
parameter	O
settings	O
.	O

subsection	O
:	O
Text8	Material
dataset	Material
Text8	Material
contains	O
100	O
million	O
characters	O
of	O
English	O
text	O
taken	O
from	O
Wikipedia	O
in	O
2006	O
,	O
consisting	O
of	O
just	O
the	O
26	O
characters	O
of	O
the	O
English	O
alphabet	O
plus	O
spaces	O
.	O

This	O
dataset	O
can	O
be	O
found	O
at	O
.	O

This	O
corpus	O
has	O
been	O
widely	O
used	O
to	O
benchmark	O
RNN	Method
character	Method
level	Method
language	Method
models	Method
,	O
with	O
the	O
first	O
90	O
million	O
characters	O
used	O
for	O
training	O
,	O
the	O
next	O
5	O
million	O
used	O
for	O
validation	Task
,	O
and	O
the	O
final	O
5	O
million	O
used	O
for	O
testing	O
.	O

The	O
results	O
of	O
these	O
experiments	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
first	O
set	O
of	O
experiments	O
we	O
performed	O
were	O
designed	O
to	O
be	O
comparable	O
to	O
those	O
of	O
,	O
who	O
benchmarked	O
several	O
deep	Method
LSTMs	Method
against	O
shallow	Method
LSTMs	Method
on	O
this	O
dataset	O
.	O

The	O
shallow	O
LSTM	Method
had	O
a	O
hidden	O
state	O
dimensionality	O
of	O
512	O
,	O
and	O
the	O
deep	Method
versions	Method
had	O
reduced	O
dimensionality	O
to	O
give	O
them	O
roughly	O
the	O
same	O
number	O
of	O
parameters	O
.	O

Our	O
experiment	O
used	O
an	O
mLSTM	Method
with	O
a	O
hidden	O
dimensionality	O
of	O
450	O
,	O
giving	O
it	O
slightly	O
fewer	O
parameters	O
than	O
the	O
past	O
work	O
,	O
and	O
our	O
own	O
LSTM	Method
baseline	O
with	O
hidden	O
dimensionality	O
512	O
.	O

mLSTM	Method
showed	O
an	O
improvement	O
over	O
our	O
baseline	O
and	O
the	O
previously	O
reported	O
best	O
deep	O
LSTM	Method
variant	O
.	O

We	O
also	O
ran	O
experiments	O
to	O
compare	O
a	O
large	Method
mLSTM	Method
with	O
other	O
reported	O
experiments	O
.	O

We	O
trained	O
an	O
mLSTM	Method
with	O
hidden	O
dimensionality	O
of	O
1900	O
on	O
the	O
text8	Material
dataset	O
.	O

Unregularised	O
mLSTM	Method
was	O
able	O
to	O
fit	O
the	O
training	O
data	O
well	O
and	O
achieved	O
a	O
competitive	O
performance	O
;	O
however	O
it	O
was	O
outperformed	O
by	O
other	O
architectures	O
that	O
are	O
less	O
prone	O
to	O
over	O
-	O
fitting	O
.	O

We	O
later	O
considered	O
our	O
best	O
training	O
setup	O
from	O
the	O
Hutter	Material
Prize	Material
dataset	O
,	O
reusing	O
the	O
exact	O
same	O
architecture	O
and	O
hyper	O
-	O
parameters	O
from	O
this	O
task	O
,	O
with	O
the	O
only	O
difference	O
being	O
the	O
number	O
of	O
input	O
characters	O
(	O
27	O
for	O
text8	Material
)	O
,	O
which	O
reduces	O
the	O
number	O
of	O
parameters	O
to	O
around	O
45	O
million	O
.	O

This	O
well	O
regularised	O
mLSTM	Method
was	O
able	O
to	O
achieve	O
a	O
much	O
stronger	O
performance	O
on	O
text8	Material
,	O
tying	O
RHNs	Method
with	O
a	O
recurrent	O
depth	O
of	O
10	O
for	O
the	O
best	O
result	O
on	O
this	O
dataset	O
.	O

subsection	O
:	O
WikiText	Material
-	Material
2	Material
The	O
WikiText	Material
-	Material
2	Material
dataset	Material
has	O
been	O
a	O
common	O
benchmark	O
for	O
very	O
recent	O
advances	O
in	O
word	O
-	O
level	O
language	Task
modelling	Task
.	O

This	O
dataset	O
contains	O
2	O
million	O
training	O
tokens	O
and	O
a	O
vocab	O
size	O
of	O
33k	O
.	O

Documents	O
are	O
given	O
in	O
non	O
-	O
shuffled	O
order	O
,	O
causing	O
the	O
data	O
to	O
contain	O
more	O
long	O
-	O
range	O
dependencies	O
.	O

We	O
use	O
this	O
dataset	O
to	O
benchmark	O
how	O
our	O
advances	O
in	O
character	O
-	O
level	O
language	Task
modelling	Task
stack	O
up	O
against	O
word	Method
level	Method
language	Method
models	Method
.	O

Character	Method
language	Method
models	Method
generally	O
perform	O
worse	O
than	O
word	Method
-	Method
level	Method
language	Method
models	Method
on	O
standard	O
English	O
text	O
benchmarks	O
.	O

One	O
reason	O
for	O
this	O
is	O
word	Method
level	Method
language	Method
models	Method
know	O
the	O
test	O
set	O
vocabulary	O
in	O
advance	O
,	O
whereas	O
character	Method
level	Method
models	Method
model	O
a	O
distribution	O
over	O
all	O
possible	O
words	O
,	O
including	O
out	O
of	O
vocabulary	O
words	O
,	O
making	O
the	O
task	O
inherently	O
more	O
difficult	O
from	O
character	Method
level	Method
view	Method
.	O

Furthermore	O
,	O
very	O
rare	O
words	O
,	O
which	O
character	Method
level	Method
models	Method
are	O
more	O
equipped	O
to	O
handle	O
than	O
word	Method
level	Method
models	Method
,	O
are	O
mapped	O
to	O
an	O
unknown	O
token	O
.	O

From	O
the	O
perspective	O
of	O
training	Task
,	O
character	Method
level	Method
language	Method
models	Method
must	O
model	O
longer	O
range	O
dependencies	O
,	O
and	O
must	O
learn	O
a	O
more	O
complex	O
non	O
-	O
linear	O
fit	O
to	O
capture	O
joint	O
dependencies	O
between	O
characters	O
.	O

Character	Method
level	Method
models	Method
do	O
have	O
an	O
inherent	O
advantage	O
of	O
being	O
able	O
to	O
capture	O
subword	O
language	O
information	O
,	O
motivating	O
their	O
use	O
on	O
traditionally	O
word	Task
-	Task
level	Task
tasks	Task
.	O

Character	Method
level	Method
language	Method
models	Method
can	O
be	O
compared	O
with	O
word	Method
level	Method
language	Method
models	Method
by	O
converting	O
bits	O
per	O
character	O
to	O
perplexity	O
.	O

In	O
this	O
case	O
,	O
we	O
model	O
the	O
data	O
at	O
the	O
UTF	O
-	O
8	O
byte	O
level	O
.	O

The	O
bits	O
per	O
word	O
can	O
be	O
computed	O
as	O
where	O
in	O
this	O
case	O
,	O
symbols	O
are	O
UTF	O
-	O
8	O
bytes	O
.	O

2	O
raised	O
to	O
the	O
power	O
of	O
the	O
number	O
of	O
bits	O
/	O
word	O
is	O
then	O
the	O
perplexity	O
.	O

The	O
WikiText	Material
-	Material
2	Material
test	Material
set	Material
is	O
245	O
,	O
569	O
words	O
long	O
,	O
and	O
1	O
,	O
256	O
,	O
449	O
bytes	O
long	O
,	O
so	O
each	O
word	O
is	O
on	O
average	O
5.1165	O
UTF	O
-	O
8	O
bytes	O
long	O
.	O

A	O
character	Method
level	Method
model	Method
can	O
also	O
assign	O
word	O
level	O
probabilities	O
directly	O
by	O
taking	O
the	O
product	O
of	O
the	O
probabilities	O
of	O
the	O
characters	O
in	O
a	O
word	O
,	O
including	O
the	O
probability	O
of	O
the	O
character	O
ending	O
the	O
word	O
(	O
either	O
a	O
space	O
or	O
a	O
newline	O
)	O
.	O

A	O
byte	Method
level	Method
model	Method
is	O
likely	O
at	O
a	O
slight	O
disadvantage	O
compared	O
with	O
word	O
-	O
level	O
because	O
it	O
must	O
predict	O
some	O
information	O
that	O
gets	O
removed	O
during	O
tokenization	O
(	O
such	O
as	O
spaces	O
vs.	O
newlines	O
)	O
,	O
but	O
the	O
perplexity	O
given	O
by	O
the	O
conversion	O
above	O
could	O
atleast	O
be	O
seen	O
as	O
an	O
upper	O
bound	O
of	O
the	O
word	Method
level	Method
perplexity	Method
such	O
a	O
model	O
could	O
achieve	O
predicting	O
byte	O
by	O
byte	O
.	O

This	O
is	O
because	O
the	O
entropy	O
of	O
the	O
file	O
after	O
tokenization	O
(	O
which	O
word	Method
level	Method
models	Method
measure	Method
)	O
will	O
always	O
be	O
less	O
than	O
or	O
equal	O
to	O
the	O
entropy	O
of	O
the	O
file	O
before	O
tokenization	O
(	O
which	O
byte	Method
level	Method
models	Method
measure	O
)	O
.	O

We	O
trained	O
the	O
mLSTM	Method
configuration	Method
from	O
the	O
Hutter	Material
Prize	Material
dataset	O
,	O
using	O
an	O
embedding	Method
layer	Method
,	O
weight	Method
normalization	Method
,	O
and	O
a	O
variational	Method
dropout	Method
of	O
0.5	O
in	O
both	O
the	O
hidden	O
and	O
embedding	Method
layer	Method
,	O
to	O
model	O
WikiText	Material
-	Material
2	Material
at	O
the	O
byte	O
level	O
.	O

This	O
model	O
contained	O
46	O
million	O
parameters	O
,	O
which	O
is	O
larger	O
than	O
most	O
word	Method
level	Method
models	Method
that	O
use	O
tied	O
input	O
and	O
output	O
embeddings	O
to	O
share	O
parameters	O
,	O
but	O
similar	O
in	O
size	O
to	O
untied	O
word	Method
level	Method
models	Method
on	O
this	O
dataset	O
.	O

The	O
results	O
are	O
given	O
in	O
table	O
[	O
reference	O
]	O
.	O

Byte	O
mLSTM	Method
achieves	O
a	O
byte	Metric
-	Metric
level	Metric
test	Metric
set	Metric
cross	Metric
entropy	Metric
of	O
bits	O
/	O
char	O
,	O
corresponding	O
to	O
a	O
perplexity	O
of	O
.	O

Despite	O
all	O
the	O
disadvantages	O
faced	O
by	O
character	Method
level	Method
models	Method
,	O
byte	Method
level	Method
mLSTM	Method
achieves	O
similar	O
word	Metric
level	Metric
perplexity	Metric
to	O
previous	O
word	Method
-	Method
level	Method
LSTM	Method
baselines	Method
that	O
also	O
use	O
variational	Method
dropout	Method
for	O
regularisation	Method
.	O

Byte	O
mLSTM	Method
does	O
not	O
perform	O
as	O
well	O
as	O
word	Method
-	Method
level	Method
models	Method
that	O
use	O
adaptive	Method
add	Method
-	Method
on	Method
methods	Method
or	O
very	O
recent	O
advances	O
in	O
regularisation	Method
/	O
hyper	O
-	O
parameter	O
tuning	O
,	O
however	O
it	O
could	O
likely	O
benefit	O
from	O
these	O
advances	O
as	O
well	O
.	O

section	O
:	O
Discussion	O
This	O
work	O
combined	O
the	O
mRNN	Method
â€™s	O
factorized	O
hidden	O
weights	O
with	O
the	O
LSTM	Method
â€™s	O
hidden	O
units	O
for	O
generative	Task
modelling	Task
of	Task
discrete	Task
multinomial	Task
sequences	Task
.	O

This	O
mLSTM	Method
architecture	Method
was	O
motivated	O
by	O
its	O
ability	O
to	O
have	O
both	O
controlled	O
and	O
flexible	O
input	O
-	O
dependent	O
transitions	O
,	O
to	O
allow	O
for	O
fast	O
changes	O
to	O
the	O
distributed	Method
hidden	Method
representation	Method
without	O
erasing	O
information	O
.	O

In	O
a	O
series	O
of	O
character	O
-	O
level	O
language	Task
modelling	Task
experiments	O
,	O
mLSTM	Method
showed	O
improvements	O
over	O
LSTM	Method
and	O
its	O
deep	Method
variants	Method
.	O

mLSTM	Method
regularised	O
with	O
variational	Method
dropout	Method
performed	O
favorably	O
compared	O
with	O
baselines	O
in	O
the	O
literature	O
,	O
outperforming	O
all	O
previous	O
neural	Method
models	Method
on	O
Hutter	Material
Prize	Material
and	O
tying	O
the	O
best	O
previous	O
result	O
on	O
text8	Material
.	O

Byte	Method
-	Method
level	Method
mLSTM	Method
was	O
also	O
able	O
to	O
perform	O
competitively	O
with	O
word	Method
-	Method
level	Method
language	Method
models	Method
on	O
WikiText	Material
-	Material
2	Material
.	O

Unlike	O
many	O
previous	O
approaches	O
that	O
have	O
achieved	O
success	O
at	O
character	O
level	O
language	Task
modelling	Task
,	O
mLSTM	Method
does	O
not	O
use	O
non	O
-	O
linear	O
recurrent	O
depth	O
.	O

All	O
mLSTMs	Method
considered	O
in	O
this	O
work	O
only	O
had	O
2	O
linear	O
recurrent	O
transition	O
matrices	O
,	O
whereas	O
comparable	O
works	O
such	O
as	O
recurrent	Method
highway	Method
networks	Method
use	O
a	O
recurrent	O
depth	O
of	O
up	O
to	O
10	O
to	O
achieve	O
best	O
results	O
.	O

This	O
makes	O
mLSTM	Method
more	O
easily	O
parallelizable	O
than	O
these	O
approaches	O
.	O

Additionally	O
,	O
our	O
work	O
suggests	O
that	O
a	O
large	O
depth	O
is	O
not	O
necessary	O
to	O
achieve	O
competitive	O
results	O
on	O
character	O
level	O
language	Task
modelling	Task
.	O

We	O
hypothesize	O
that	O
mLSTM	Method
â€™s	O
ability	O
to	O
have	O
very	O
different	O
transition	O
functions	O
for	O
each	O
possible	O
input	O
is	O
what	O
makes	O
it	O
successful	O
at	O
this	O
task	O
.	O

While	O
recurrent	O
depth	O
can	O
accomplish	O
this	O
too	O
,	O
mLSTM	Method
can	O
achieve	O
this	O
more	O
efficiently	O
.	O

While	O
these	O
results	O
are	O
promising	O
,	O
it	O
remains	O
to	O
be	O
seen	O
how	O
mLSTM	Method
performs	O
at	O
word	O
-	O
level	O
language	Task
modelling	Task
and	O
other	O
discrete	Task
multinomial	Task
generative	Task
modelling	Task
tasks	Task
,	O
and	O
whether	O
mLSTM	Method
can	O
be	O
formulated	O
to	O
apply	O
more	O
broadly	O
to	O
tasks	O
with	O
continuous	O
or	O
non	O
-	O
sparse	O
input	O
units	O
.	O

We	O
also	O
hope	O
this	O
work	O
will	O
motivate	O
further	O
exploration	O
in	O
generative	Method
RNN	Method
architectures	Method
with	O
flexible	O
input	O
-	O
dependent	O
transition	O
functions	O
.	O

bibliography	O
:	O
References	O
