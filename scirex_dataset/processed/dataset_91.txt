Recent	O
advances	O
in	O
neural	O
variational	Method
inference	Method
have	O
spawned	O
a	O
renaissance	O
in	O
deep	Method
latent	Method
variable	Method
models	Method
.	O

In	O
this	O
paper	O
we	O
introduce	O
a	O
generic	O
variational	Method
inference	Method
framework	O
for	O
generative	Task
and	Task
conditional	Task
models	Task
of	Task
text	Task
.	O

While	O
traditional	O
variational	Method
methods	Method
derive	O
an	O
analytic	Method
approximation	Method
for	O
the	O
intractable	O
distributions	O
over	O
latent	O
variables	O
,	O
here	O
we	O
construct	O
an	O
inference	Method
network	Method
conditioned	O
on	O
the	O
discrete	O
text	O
input	O
to	O
provide	O
the	O
variational	O
distribution	O
.	O

We	O
validate	O
this	O
framework	O
on	O
two	O
very	O
different	O
text	Task
modelling	Task
applications	Task
,	O
generative	Task
document	Task
modelling	Task
and	O
supervised	Task
question	Task
answering	Task
.	O

Our	O
neural	Method
variational	Method
document	Method
model	Method
combines	O
a	O
continuous	Method
stochastic	Method
document	Method
representation	Method
with	O
a	O
bag	Method
-	Method
of	Method
-	Method
words	Method
generative	Method
model	Method
and	O
achieves	O
the	O
lowest	O
reported	O
perplexities	Metric
on	O
two	O
standard	O
test	O
corpora	O
.	O

The	O
neural	Method
answer	Method
selection	Method
model	Method
employs	O
a	O
stochastic	Method
representation	Method
layer	Method
within	O
an	O
attention	Method
mechanism	Method
to	O
extract	O
the	O
semantics	O
between	O
a	O
question	O
and	O
answer	O
pair	O
.	O

On	O
two	O
question	Metric
answering	Metric
benchmarks	Metric
this	O
model	O
exceeds	O
all	O
previous	O
published	O
benchmarks	O
.	O

NeuralVariationalInferenceforTextProcessing	Method
section	O
:	O
Introduction	O
Probabilistic	Method
generative	Method
models	Method
underpin	O
many	O
successful	O
applications	O
within	O
the	O
field	O
of	O
natural	Task
language	Task
processing	Task
(	O
NLP	Task
)	O
.	O

Their	O
popularity	O
stems	O
from	O
their	O
ability	O
to	O
use	O
unlabelled	O
data	O
effectively	O
,	O
to	O
incorporate	O
abundant	O
linguistic	O
features	O
,	O
and	O
to	O
learn	O
interpretable	O
dependencies	O
among	O
data	O
.	O

However	O
these	O
successes	O
are	O
tempered	O
by	O
the	O
fact	O
that	O
as	O
the	O
structure	O
of	O
such	O
generative	Method
models	Method
becomes	O
deeper	O
and	O
more	O
complex	O
,	O
true	Task
Bayesian	Task
inference	Task
becomes	O
intractable	O
due	O
to	O
the	O
high	O
dimensional	O
integrals	O
required	O
.	O

Markov	Method
chain	Method
Monte	Method
Carlo	Method
(	O
MCMC	Method
)	O
neal1993probabilistic	O
,	O
andrieu2003introduction	O
and	O
variational	Method
inference	Method
jordan1999introduction	O
,	O
attias2000variational	O
,	O
beal2003variational	O
are	O
the	O
standard	O
approaches	O
for	O
approximating	O
these	O
integrals	O
.	O

However	O
the	O
computational	Metric
cost	Metric
of	O
the	O
former	O
results	O
in	O
impractical	Task
training	Task
for	O
the	O
large	Method
and	Method
deep	Method
neural	Method
networks	Method
which	O
are	O
now	O
fashionable	O
,	O
and	O
the	O
latter	O
is	O
conventionally	O
confined	O
due	O
to	O
the	O
underestimation	O
of	O
posterior	O
variance	O
.	O

The	O
lack	O
of	O
effective	O
and	O
efficient	O
inference	Method
methods	Method
hinders	O
our	O
ability	O
to	O
create	O
highly	O
expressive	Method
models	Method
of	O
text	O
,	O
especially	O
in	O
the	O
situation	O
where	O
the	O
model	O
is	O
non	O
-	O
conjugate	O
.	O

This	O
paper	O
introduces	O
a	O
neural	Method
variational	Method
framework	Method
for	O
generative	Task
models	Task
of	Task
text	Task
,	O
inspired	O
by	O
the	O
variational	Method
auto	Method
-	Method
encoder	Method
rezende2014stochastic	O
,	O
kingma2013auto	O
.	O

The	O
principle	O
idea	O
is	O
to	O
build	O
an	O
inference	Method
network	Method
,	O
implemented	O
by	O
a	O
deep	Method
neural	Method
network	Method
conditioned	O
on	O
text	O
,	O
to	O
approximate	O
the	O
intractable	O
distributions	O
over	O
the	O
latent	O
variables	O
.	O

Instead	O
of	O
providing	O
an	O
analytic	Method
approximation	Method
,	O
as	O
in	O
traditional	O
variational	Method
Bayes	Method
,	O
neural	O
variational	Method
inference	Method
learns	O
to	O
model	O
the	O
posterior	O
probability	O
,	O
thus	O
endowing	O
the	O
model	O
with	O
strong	O
generalisation	O
abilities	O
.	O

Due	O
to	O
the	O
flexibility	O
of	O
deep	Method
neural	Method
networks	Method
,	O
the	O
inference	Method
network	Method
is	O
capable	O
of	O
learning	Task
complicated	Task
non	Task
-	Task
linear	Task
distributions	Task
and	O
processing	O
structured	O
inputs	O
such	O
as	O
word	O
sequences	O
.	O

Inference	Method
networks	Method
can	O
be	O
designed	O
as	O
,	O
but	O
not	O
restricted	O
to	O
,	O
multilayer	Method
perceptrons	Method
(	O
MLP	Method
)	O
,	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
,	O
and	O
recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
,	O
approaches	O
which	O
are	O
rarely	O
used	O
in	O
conventional	O
generative	Method
models	Method
.	O

By	O
using	O
the	O
reparameterisation	Method
method	Method
rezende2014stochastic	O
,	O
kingma2013auto	O
,	O
the	O
inference	Method
network	Method
is	O
trained	O
through	O
back	Method
-	Method
propagating	Method
unbiased	Method
and	Method
low	Method
variance	Method
gradients	Method
w.r.t	Method
.	O

the	O
latent	O
variables	O
.	O

Within	O
this	O
framework	O
,	O
we	O
propose	O
a	O
Neural	Method
Variational	Method
Document	Method
Model	Method
(	O
NVDM	Method
)	O
for	O
document	Method
modelling	Method
and	O
a	O
Neural	Method
Answer	Method
Selection	Method
Model	Method
(	O
NASM	Method
)	O
for	O
question	Task
answering	Task
,	O
a	O
task	O
that	O
selects	O
the	O
sentences	O
that	O
correctly	O
answer	O
a	O
factoid	O
question	O
from	O
a	O
set	O
of	O
candidate	O
sentences	O
.	O

The	O
NVDM	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
is	O
an	O
unsupervised	Method
generative	Method
model	Method
of	Method
text	Method
which	O
aims	O
to	O
extract	O
a	O
continuous	O
semantic	O
latent	O
variable	O
for	O
each	O
document	O
.	O

This	O
model	O
can	O
be	O
interpreted	O
as	O
a	O
variational	Method
auto	Method
-	Method
encoder	Method
:	O
an	O
MLP	Method
encoder	O
(	O
inference	Method
network	Method
)	O
compresses	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
document	Method
representation	Method
into	O
a	O
continuous	O
latent	O
distribution	O
,	O
and	O
a	O
softmax	Method
decoder	Method
(	O
generative	Method
model	Method
)	O
reconstructs	O
the	O
document	O
by	O
generating	O
the	O
words	O
independently	O
.	O

A	O
primary	O
feature	O
of	O
NVDM	Method
is	O
that	O
each	O
word	O
is	O
generated	O
directly	O
from	O
a	O
dense	Method
continuous	Method
document	Method
representation	Method
instead	O
of	O
the	O
more	O
common	O
binary	O
semantic	O
vector	O
hinton2009replicated	O
,	O
larochelle2012neural	O
,	O
Srivastava2013	O
,	O
mnih2014neural	O
.	O

Our	O
experiments	O
demonstrate	O
that	O
our	O
neural	Method
document	Method
model	Method
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
perplexities	Metric
on	O
the	O
20NewsGroups	O
and	O
RCV1	O
-	O
v2	O
.	O

The	O
NASM	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
is	O
a	O
supervised	Method
conditional	Method
model	Method
which	O
imbues	O
LSTMs	Method
hochreiter1997long	O
with	O
a	O
latent	Method
stochastic	Method
attention	Method
mechanism	Method
to	O
model	O
the	O
semantics	O
of	O
question	O
-	O
answer	O
pairs	O
and	O
predict	O
their	O
relatedness	O
.	O

The	O
attention	Method
model	Method
is	O
designed	O
to	O
focus	O
on	O
the	O
phrases	O
of	O
an	O
answer	O
that	O
are	O
strongly	O
connected	O
to	O
the	O
question	O
semantics	O
and	O
is	O
modelled	O
by	O
a	O
latent	Method
distribution	Method
.	O

This	O
mechanism	O
allows	O
the	O
model	O
to	O
deal	O
with	O
the	O
ambiguity	O
inherent	O
in	O
the	O
task	O
and	O
learns	O
pair	Method
-	Method
specific	Method
representations	Method
that	O
are	O
more	O
effective	O
at	O
predicting	Task
answer	Task
matches	Task
,	O
rather	O
than	O
independent	O
embeddings	O
of	O
question	O
and	O
answer	O
sentences	O
.	O

Bayesian	Method
inference	Method
provides	O
a	O
natural	O
safeguard	O
against	O
overfitting	O
,	O
especially	O
as	O
the	O
training	O
sets	O
available	O
for	O
this	O
task	O
are	O
small	O
.	O

The	O
experiments	O
show	O
that	O
the	O
LSTM	Method
with	O
a	O
latent	Method
stochastic	Method
attention	Method
mechanism	Method
learns	O
an	O
effective	O
attention	Method
model	Method
and	O
outperforms	O
both	O
previously	O
published	O
results	O
,	O
and	O
our	O
own	O
strong	O
non	Method
-	Method
stochastic	Method
attention	Method
baselines	Method
.	O

In	O
summary	O
,	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
neural	O
variational	Method
inference	Method
for	O
text	Task
processing	Task
on	O
two	O
diverse	O
tasks	O
.	O

These	O
models	O
are	O
simple	O
,	O
expressive	O
and	O
can	O
be	O
trained	O
efficiently	O
with	O
the	O
highly	O
scalable	O
stochastic	Method
gradient	Method
back	Method
-	Method
propagation	Method
.	O

Our	O
neural	Method
variational	Method
framework	Method
is	O
suitable	O
for	O
both	O
unsupervised	Task
and	Task
supervised	Task
learning	Task
tasks	Task
,	O
and	O
can	O
be	O
generalised	O
to	O
incorporate	O
any	O
type	O
of	O
neural	Method
networks	Method
.	O

section	O
:	O
Neural	Method
Variational	Method
Inference	Method
Framework	Method
Latent	Method
variable	Method
modelling	Method
is	O
popular	O
in	O
many	O
NLP	Task
problems	O
,	O
but	O
it	O
is	O
non	O
-	O
trivial	O
to	O
carry	O
out	O
effective	O
and	O
efficient	O
inference	Task
for	O
models	O
with	O
complex	O
and	O
deep	O
structure	O
.	O

In	O
this	O
section	O
we	O
introduce	O
a	O
generic	O
neural	O
variational	Method
inference	Method
framework	O
that	O
we	O
apply	O
to	O
both	O
the	O
unsupervised	O
NVDM	Method
and	O
supervised	O
NASM	Method
in	O
the	O
follow	O
sections	O
.	O

We	O
define	O
a	O
generative	Method
model	Method
with	O
a	O
latent	O
variable	O
,	O
which	O
can	O
be	O
considered	O
as	O
the	O
stochastic	Method
units	Method
in	O
deep	Method
neural	Method
networks	Method
.	O

We	O
designate	O
the	O
observed	O
parent	O
and	O
child	O
nodes	O
of	O
as	O
and	O
respectively	O
.	O

Hence	O
,	O
the	O
joint	O
distribution	O
of	O
the	O
generative	Method
model	Method
is	O
,	O
and	O
the	O
variational	Metric
lower	Metric
bound	Metric
is	O
derived	O
as	O
:	O
where	O
parameterises	O
the	O
generative	O
distributions	O
and	O
.	O

In	O
order	O
to	O
have	O
a	O
tight	O
lower	O
bound	O
,	O
the	O
variational	Method
distribution	Method
should	O
approach	O
the	O
true	O
posterior	O
.	O

Here	O
,	O
we	O
employ	O
a	O
parameterised	Method
diagonal	Method
Gaussian	Method
as	Method
.	O

The	O
three	O
steps	O
to	O
construct	O
the	O
inference	Method
network	Method
are	O
:	O
Construct	O
vector	Method
representations	Method
of	Method
the	Method
observed	Method
variables	Method
:	O
,	O
.	O

Assemble	O
a	O
joint	Method
representation	Method
:	O
.	O

Parameterise	O
the	O
variational	Method
distribution	Method
over	O
the	O
latent	O
variable	O
:	O
.	O

and	O
can	O
be	O
any	O
type	O
of	O
deep	Method
neural	Method
networks	Method
that	O
are	O
suitable	O
for	O
the	O
observed	O
data	O
;	O
is	O
an	O
MLP	Method
that	O
concatenates	O
the	O
vector	Method
representations	Method
of	Method
the	Method
conditioning	Method
variables	Method
;	O
is	O
a	O
linear	Method
transformation	Method
which	O
outputs	O
the	O
parameters	O
of	O
the	O
Gaussian	Method
distribution	Method
.	O

By	O
sampling	O
from	O
the	O
variational	O
distribution	O
,	O
,	O
we	O
are	O
able	O
to	O
carry	O
out	O
stochastic	Method
back	Method
-	Method
propagation	Method
to	O
optimise	O
the	O
lower	O
bound	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

During	O
training	O
,	O
the	O
model	O
parameters	O
together	O
with	O
the	O
inference	Method
network	Method
parameters	Method
are	O
updated	O
by	O
stochastic	Method
back	Method
-	Method
propagation	Method
based	O
on	O
the	O
samples	O
drawn	O
from	O
.	O

For	O
the	O
gradients	O
w.r.t	O
.	O

,	O
we	O
have	O
the	O
form	O
:	O
For	O
the	O
gradients	O
w.r.t	O
.	O

we	O
reparameterise	O
and	O
sample	O
to	O
reduce	O
the	O
variance	Metric
in	O
stochastic	Method
estimation	Method
rezende2014stochastic	O
,	O
kingma2013auto	O
.	O

The	O
update	O
of	O
can	O
be	O
carried	O
out	O
by	O
back	O
-	O
propagating	O
the	O
gradients	O
w.r.t	O
.	O

and	O
:	O
It	O
is	O
worth	O
mentioning	O
that	O
unsupervised	Method
learning	Method
is	O
a	O
special	O
case	O
of	O
the	O
neural	Method
variational	Method
framework	Method
where	O
has	O
no	O
parent	O
node	O
.	O

In	O
that	O
case	O
is	O
directly	O
drawn	O
from	O
the	O
prior	O
instead	O
of	O
the	O
conditional	O
distribution	O
,	O
and	O
.	O

Here	O
we	O
only	O
discuss	O
the	O
scenario	O
where	O
the	O
latent	O
variables	O
are	O
continuous	O
and	O
the	O
parameterised	Method
diagonal	Method
Gaussian	Method
is	O
employed	O
as	O
the	O
variational	Method
distribution	Method
.	O

However	O
the	O
framework	O
is	O
also	O
suitable	O
for	O
discrete	O
units	O
,	O
and	O
the	O
only	O
modification	O
needed	O
is	O
to	O
replace	O
the	O
Gaussian	Method
with	O
a	O
multinomial	Method
parameterised	Method
by	O
the	O
outputs	O
of	O
a	O
softmax	Method
function	Method
.	O

Though	O
the	O
reparameterisation	Method
trick	Method
for	O
continuous	O
variables	O
is	O
not	O
applicable	O
for	O
this	O
case	O
,	O
a	O
policy	Method
gradient	Method
approach	Method
mnih2014neural	O
can	O
help	O
to	O
alleviate	O
the	O
high	Task
variance	Task
problem	Task
during	O
stochastic	Method
estimation	Method
.	O

proposed	O
a	O
variational	Method
inference	Method
framework	O
for	O
semi	Task
-	Task
supervised	Task
learning	Task
,	O
but	O
the	O
prior	O
distribution	O
over	O
the	O
hidden	O
variable	O
remains	O
as	O
the	O
standard	O
Gaussian	O
prior	O
,	O
while	O
we	O
apply	O
a	O
conditional	Method
parameterised	Method
Gaussian	Method
distribution	Method
,	O
which	O
is	O
jointly	O
learned	O
with	O
the	O
variational	Method
distribution	Method
.	O

section	O
:	O
Neural	Method
Variational	Method
Document	Method
Model	Method
The	O
Neural	Method
Variational	Method
Document	Method
Model	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
is	O
a	O
simple	O
instance	O
of	O
unsupervised	Method
learning	Method
where	O
a	O
continuous	O
hidden	O
variable	O
,	O
which	O
generates	O
all	O
the	O
words	O
in	O
a	O
document	O
independently	O
,	O
is	O
introduced	O
to	O
represent	O
its	O
semantic	O
content	O
.	O

Let	O
be	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
representation	Method
of	O
a	O
document	O
and	O
be	O
the	O
one	Method
-	Method
hot	Method
representation	Method
of	O
the	O
word	O
at	O
position	O
.	O

As	O
an	O
unsupervised	Method
generative	Method
model	Method
,	O
we	O
could	O
interpret	O
NVDM	Method
as	O
a	O
variational	Method
autoencoder	Method
:	O
an	O
MLP	Method
encoder	O
compresses	O
document	Method
representations	Method
into	O
continuous	O
hidden	O
vectors	O
(	O
)	O
;	O
a	O
softmax	Method
decoder	Method
reconstructs	O
the	O
documents	O
by	O
independently	O
generating	O
the	O
words	O
(	O
)	O
.	O

To	O
maximise	O
the	O
log	O
-	O
likelihood	O
of	O
documents	O
,	O
we	O
derive	O
the	O
lower	Metric
bound	Metric
:	O
where	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
document	O
and	O
is	O
a	O
Gaussian	O
prior	O
for	O
.	O

Here	O
,	O
we	O
consider	O
is	O
observed	O
for	O
all	O
the	O
documents	O
.	O

The	O
conditional	O
probability	O
over	O
words	O
(	O
decoder	O
)	O
is	O
modelled	O
by	O
multinomial	Method
logistic	Method
regression	Method
and	O
shared	O
across	O
documents	O
:	O
where	O
learns	O
the	O
semantic	O
word	O
embeddings	O
and	O
represents	O
the	O
bias	O
term	O
.	O

As	O
there	O
is	O
no	O
supervision	O
information	O
for	O
the	O
latent	O
semantics	O
,	O
,	O
the	O
posterior	Method
approximation	Method
is	O
only	O
conditioned	O
on	O
the	O
current	O
document	O
.	O

The	O
inference	Method
network	Method
is	O
modelled	O
as	O
:	O
For	O
each	O
document	O
,	O
the	O
neural	Method
network	Method
generates	O
its	O
own	O
parameters	O
and	O
that	O
parameterise	O
the	O
latent	O
distribution	O
over	O
document	O
semantics	O
.	O

Based	O
on	O
the	O
samples	O
,	O
the	O
lower	O
bound	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
can	O
be	O
optimised	O
by	O
back	O
-	O
propagating	O
the	O
stochastic	Method
gradients	Method
w.r.t	Method
.	O

and	O
.	O

Since	O
is	O
a	O
standard	O
Gaussian	Method
prior	Method
,	O
the	O
Gaussian	Method
KL	Method
-	Method
Divergence	Method
can	O
be	O
computed	O
analytically	O
to	O
further	O
lower	O
the	O
variance	O
of	O
the	O
gradients	O
.	O

Moreover	O
,	O
it	O
also	O
acts	O
as	O
a	O
regulariser	Method
for	O
updating	O
the	O
parameters	O
of	O
the	O
inference	Method
network	Method
.	O

section	O
:	O
Neural	Method
Answer	Method
Selection	Method
Model	Method
Answer	Task
sentence	Task
selection	Task
is	O
a	O
question	Task
answering	Task
paradigm	Task
where	O
a	O
model	O
must	O
identify	O
the	O
correct	O
sentences	O
answering	O
a	O
factual	O
question	O
from	O
a	O
set	O
of	O
candidate	O
sentences	O
.	O

Assume	O
a	O
question	O
is	O
associated	O
with	O
a	O
set	O
of	O
answer	O
sentences	O
,	O
together	O
with	O
their	O
judgements	O
,	O
where	O
if	O
the	O
answer	O
is	O
correct	O
and	O
otherwise	O
.	O

This	O
is	O
a	O
classification	Task
task	Task
where	O
we	O
treat	O
each	O
training	O
data	O
point	O
as	O
a	O
triple	O
while	O
predicting	O
for	O
the	O
unlabelled	Task
question	Task
-	Task
answer	Task
pair	Task
.	O

The	O
Neural	Method
Answer	Method
Selection	Method
Model	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
is	O
a	O
supervised	Method
model	Method
that	O
learns	O
the	O
question	Method
and	Method
answer	Method
representations	Method
and	O
predicts	O
their	O
relatedness	O
.	O

It	O
employs	O
two	O
different	O
LSTMs	Method
to	O
embed	O
raw	O
question	O
inputs	O
and	O
answer	O
inputs	O
.	O

Let	O
and	O
be	O
the	O
state	O
outputs	O
of	O
the	O
two	O
LSTMs	Method
,	O
and	O
,	O
be	O
the	O
positions	O
of	O
the	O
states	O
.	O

Conventionally	O
,	O
the	O
last	O
state	O
outputs	O
and	O
,	O
as	O
the	O
independent	Method
question	Method
and	Method
answer	Method
representations	Method
,	O
can	O
be	O
used	O
for	O
relatedness	Task
prediction	Task
.	O

In	O
NASM	Method
,	O
however	O
,	O
we	O
aim	O
to	O
learn	O
pair	O
-	O
specific	O
representations	O
through	O
a	O
latent	Method
attention	Method
mechanism	Method
,	O
which	O
is	O
more	O
effective	O
for	O
pair	Task
relatedness	Task
prediction	Task
.	O

NASM	Method
applies	O
an	O
attention	Method
model	Method
to	O
focus	O
on	O
the	O
words	O
in	O
the	O
answer	O
sentence	O
that	O
are	O
prominent	O
for	O
predicting	O
the	O
answer	O
matched	O
to	O
the	O
current	O
question	O
.	O

Instead	O
of	O
using	O
a	O
deterministic	O
question	O
vector	O
,	O
such	O
as	O
,	O
NASM	Method
employs	O
a	O
latent	Method
distribution	Method
to	O
model	O
the	O
question	O
semantics	O
,	O
which	O
is	O
a	O
parameterised	Method
diagonal	Method
Gaussian	Method
.	O

Therefore	O
,	O
the	O
attention	Method
model	Method
extracts	O
a	O
context	O
vector	O
by	O
iteratively	O
attending	O
to	O
the	O
answer	O
tokens	O
based	O
on	O
the	O
stochastic	O
vector	O
.	O

In	O
doing	O
so	O
the	O
model	O
is	O
able	O
to	O
adapt	O
to	O
the	O
ambiguity	O
inherent	O
in	O
questions	O
and	O
obtain	O
salient	O
information	O
through	O
attention	O
.	O

Compared	O
to	O
its	O
deterministic	Method
counterpart	Method
(	O
applying	O
as	O
the	O
question	O
semantics	O
)	O
,	O
the	O
stochastic	Method
units	Method
incorporated	O
into	O
NASM	Method
allow	O
multi	O
-	O
modal	O
attention	O
distributions	O
.	O

Further	O
,	O
by	O
marginalising	O
over	O
the	O
latent	O
variables	O
,	O
NASM	Method
is	O
more	O
robust	O
against	O
overfitting	O
,	O
which	O
is	O
important	O
for	O
small	Task
question	Task
answering	Task
training	Task
sets	Task
.	O

In	O
this	O
model	O
,	O
the	O
conditional	O
distribution	O
is	O
:	O
For	O
each	O
question	O
,	O
the	O
neural	Method
network	Method
generates	O
the	O
corresponding	O
parameters	O
and	O
that	O
parameterise	O
the	O
latent	O
distribution	O
over	O
question	O
semantics	O
.	O

Following	O
bahdanau2014neural	Method
,	O
the	O
attention	Method
model	Method
is	O
defined	O
as	O
:	O
where	O
is	O
the	O
normalised	O
attention	O
score	O
at	O
answer	O
token	O
,	O
and	O
the	O
context	O
vector	O
is	O
the	O
weighted	O
sum	O
of	O
all	O
the	O
state	O
outputs	O
.	O

We	O
adopt	O
as	O
the	O
question	Method
and	Method
answer	Method
representations	Method
for	O
predicting	O
their	O
relatedness	O
.	O

is	O
a	O
deterministic	O
vector	O
that	O
is	O
equal	O
to	O
,	O
while	O
is	O
a	O
combination	O
of	O
the	O
sequence	O
output	O
and	O
the	O
context	O
vector	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

For	O
the	O
prediction	Task
of	Task
pair	Task
relatedness	Task
,	O
we	O
model	O
the	O
conditional	O
probability	O
distribution	O
by	O
sigmoid	Method
function	Method
:	O
To	O
maximise	O
the	O
log	O
-	O
likelihood	O
we	O
use	O
the	O
variational	O
lower	O
bound	O
:	O
Following	O
the	O
neural	O
variational	Method
inference	Method
framework	O
,	O
we	O
construct	O
a	O
deep	Method
neural	Method
network	Method
as	O
the	O
inference	Method
network	Method
:	O
where	O
and	O
are	O
also	O
modelled	O
by	O
LSTMs	Method
,	O
and	O
the	O
relatedness	O
label	O
is	O
modelled	O
by	O
a	O
simple	O
linear	Method
transformation	Method
into	O
the	O
vector	O
.	O

According	O
to	O
the	O
joint	Method
representation	Method
,	O
we	O
then	O
generate	O
the	O
parameters	O
and	O
,	O
which	O
parameterise	O
the	O
variational	O
distribution	O
over	O
the	O
question	O
semantics	O
.	O

To	O
emphasise	O
,	O
though	O
both	O
and	O
are	O
modelled	O
as	O
parameterised	Method
Gaussian	Method
distributions	Method
,	O
as	O
an	O
approximation	O
only	O
functions	O
during	O
inference	Task
by	O
producing	O
samples	O
to	O
compute	O
the	O
stochastic	O
gradients	O
,	O
while	O
is	O
the	O
generative	Method
distribution	Method
that	O
generates	O
the	O
samples	O
for	O
predicting	Task
the	Task
question	Task
-	Task
answer	Task
relatedness	Task
.	O

Based	O
on	O
the	O
samples	O
,	O
we	O
use	O
SGVB	Method
to	O
optimise	O
the	O
lower	O
bound	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

The	O
model	O
parameters	O
and	O
the	O
inference	O
network	O
parameters	O
are	O
updated	O
jointly	O
using	O
their	O
stochastic	O
gradients	O
.	O

In	O
this	O
case	O
,	O
similar	O
to	O
the	O
NVDM	Method
,	O
the	O
Gaussian	Method
KL	Method
divergence	Method
can	O
be	O
analytically	O
computed	O
during	O
training	Task
process	Task
.	O

[	O
Perplexity	O
on	O
test	O
dataset	O
.	O

]	O
ModelDim20NewsRCV1LDA5010911437LDA20010581142RSM50953988docNADE50896742SBN50909784fDARN50917724fDARN200—	O
-	O
598NVDM50836563NVDM200852550	O
[	O
The	O
five	O
nearest	O
words	O
in	O
the	O
semantic	O
space	O
.	O

]	O
section	O
:	O
Experiments	O
subsection	O
:	O
Dataset	O
&	O
Setup	O
for	O
Document	Task
Modelling	Task
We	O
experiment	O
with	O
NVDM	Method
on	O
two	O
standard	O
news	O
corpora	O
:	O
the	O
20NewsGroupshttp:	O
//	O
qwone.com	O
/	O
jason	O
/	O
20Newsgroups	O
and	O
the	O
Reuters	O
RCV1	O
-	O
v2http:	O
//	O
trec.nist.gov	O
/	O
data	O
/	O
reuters	O
/	O
reuters.html	O
.	O

The	O
former	O
is	O
a	O
collection	O
of	O
newsgroup	O
documents	O
,	O
consisting	O
of	O
11	O
,	O
314	O
training	O
and	O
7	O
,	O
531	O
test	O
articles	O
.	O

The	O
latter	O
is	O
a	O
large	O
collection	O
from	O
Reuters	O
newswire	O
stories	O
with	O
794	O
,	O
414	O
training	O
and	O
10	O
,	O
000	O
test	O
cases	O
.	O

The	O
vocabulary	Metric
size	Metric
of	O
these	O
two	O
datasets	O
are	O
set	O
as	O
2	O
,	O
000	O
and	O
10	O
,	O
000	O
.	O

To	O
make	O
a	O
direct	O
comparison	O
with	O
the	O
prior	O
work	O
we	O
follow	O
the	O
same	O
preprocessing	Method
procedure	Method
and	O
setup	O
as	O
hinton2009replicated	O
,	O
larochelle2012neural	O
,	O
Srivastava2013	O
,	O
and	O
mnih2014neural	O
.	O

We	O
train	O
NVDM	Method
models	O
with	O
50	O
and	O
200	O
dimensional	Method
document	Method
representations	Method
respectively	O
.	O

For	O
the	O
inference	Task
network	Task
,	O
we	O
use	O
an	O
MLP	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
with	O
2	O
layers	O
and	O
500	O
dimension	O
rectifier	O
linear	O
units	O
,	O
which	O
converts	O
document	Method
representations	Method
into	O
embeddings	O
.	O

During	O
training	Task
we	O
carry	O
out	O
stochastic	Method
estimation	Method
by	O
taking	O
one	O
sample	O
for	O
estimating	O
the	O
stochastic	O
gradients	O
,	O
while	O
in	O
prediction	Task
we	O
use	O
20	O
samples	O
for	O
predicting	Task
document	Task
perplexity	Task
.	O

The	O
model	O
is	O
trained	O
by	O
Adam	Method
DBLP	Method
:	Method
journals	Method
/	Method
corr	Method
/	O
KingmaB14	Method
and	O
tuned	O
by	O
hold	Method
-	Method
out	Method
validation	Method
perplexity	Method
.	O

We	O
alternately	O
optimise	O
the	O
generative	Method
model	Method
and	O
the	O
inference	Method
network	Method
by	O
fixing	O
the	O
parameters	O
of	O
one	O
while	O
updating	O
the	O
parameters	O
of	O
the	O
other	O
.	O

subsection	O
:	O
Experiments	O
on	O
Document	Task
Modelling	Task
Table	O
[	O
reference	O
]	O
presents	O
the	O
test	O
document	Metric
perplexity	Metric
.	O

The	O
first	O
column	O
lists	O
the	O
models	O
,	O
and	O
the	O
second	O
column	O
shows	O
the	O
dimension	O
of	O
latent	O
variables	O
used	O
in	O
the	O
experiments	O
.	O

The	O
final	O
two	O
columns	O
present	O
the	O
perplexity	O
achieved	O
by	O
each	O
topic	Method
model	Method
on	O
the	O
20NewsGroups	O
and	O
RCV1	O
-	O
v2	O
datasets	O
.	O

In	O
document	Task
modelling	Task
,	O
perplexity	Task
is	O
computed	O
by	O
,	O
where	O
is	O
the	O
number	O
of	O
documents	O
,	O
represents	O
the	O
length	O
of	O
the	O
th	O
document	O
and	O
is	O
the	O
log	O
probability	O
of	O
the	O
words	O
in	O
the	O
document	O
.	O

Since	O
is	O
intractable	O
in	O
the	O
NVDM	Method
,	O
we	O
use	O
the	O
variational	Method
lower	Method
bound	Method
(	O
which	O
is	O
an	O
upper	O
bound	O
on	O
perplexity	Task
)	O
to	O
compute	O
the	O
perplexity	Method
following	O
mnih2014neural	Method
.	O

While	O
all	O
the	O
baseline	O
models	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
apply	O
discrete	O
latent	O
variables	O
,	O
here	O
NVDM	Method
employs	O
a	O
continuous	Method
stochastic	Method
document	Method
representation	Method
.	O

The	O
experimental	O
results	O
indicate	O
that	O
NVDM	Method
achieves	O
the	O
best	O
performance	O
on	O
both	O
datasets	O
.	O

For	O
the	O
experiments	O
on	O
RCV1	O
-	O
v2	O
dataset	O
,	O
the	O
NVDM	Method
with	O
latent	O
variable	O
of	O
50	O
dimension	O
performs	O
even	O
better	O
than	O
the	O
fDARN	Method
with	O
200	O
dimension	O
.	O

It	O
demonstrates	O
that	O
our	O
document	Method
model	Method
with	O
continuous	O
latent	O
variables	O
has	O
higher	O
expressiveness	O
and	O
better	O
generalisation	Metric
ability	Metric
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
5	O
nearest	O
words	O
selected	O
according	O
to	O
the	O
semantic	O
vector	O
learned	O
from	O
NVDM	Method
and	O
docNADE	Method
.	O

In	O
addition	O
to	O
the	O
perplexities	O
,	O
we	O
also	O
qualitatively	O
evaluate	O
the	O
semantic	O
information	O
learned	O
by	O
NVDM	Method
on	O
the	O
20NewsGroups	O
dataset	O
with	O
latent	O
variables	O
of	O
50	O
dimension	O
.	O

We	O
assume	O
each	O
dimension	O
in	O
the	O
latent	O
space	O
represents	O
a	O
topic	O
that	O
corresponds	O
to	O
a	O
specific	O
semantic	O
meaning	O
.	O

Table	O
[	O
reference	O
]	O
presents	O
5	O
randomly	O
selected	O
topics	O
with	O
10	O
words	O
that	O
have	O
the	O
strongest	O
positive	O
connection	O
with	O
the	O
topic	O
.	O

Based	O
on	O
the	O
words	O
in	O
each	O
column	O
,	O
we	O
can	O
deduce	O
their	O
corresponding	O
topics	O
as	O
:	O
Space	O
,	O
Religion	O
,	O
Encryption	O
,	O
Sport	O
and	O
Policy	O
.	O

Although	O
the	O
model	O
does	O
not	O
impose	O
independent	O
interpretability	O
on	O
the	O
latent	O
representation	O
dimensions	O
,	O
we	O
still	O
see	O
that	O
the	O
NVDM	Method
learns	O
locally	O
interpretable	O
structure	O
.	O

figureThe	O
standard	Metric
deviations	Metric
of	O
MAP	Metric
scores	Metric
computed	O
by	O
running	O
10	O
NASM	Method
models	O
on	O
WikiQA	Material
with	O
different	O
numbers	O
of	O
samples	O
.	O

subsection	O
:	O
Dataset	O
&	O
Setup	O
for	O
Answer	Task
Sentence	Task
Selection	Task
We	O
experiment	O
on	O
two	O
answer	O
selection	O
datasets	O
,	O
the	O
QASent	Material
and	O
the	O
WikiQA	Material
datasets	Material
.	O

QASent	Material
wang2007jeopardy	O
is	O
created	O
from	O
the	O
TREC	O
QA	O
track	O
,	O
and	O
the	O
WikiQA	Material
yang	O
-	O
yih	O
-	O
meek:2015:EMNLP	O
is	O
constructed	O
from	O
Wikipedia	O
,	O
which	O
is	O
less	O
noisy	O
and	O
less	O
biased	O
towards	O
lexical	Method
overlap	Method
.	O

Table	O
[	O
reference	O
]	O
summarises	O
the	O
statistics	O
of	O
the	O
two	O
datasets	O
.	O

In	O
order	O
to	O
investigate	O
the	O
effectiveness	O
of	O
our	O
NASM	Method
model	O
we	O
also	O
implemented	O
two	O
strong	O
baseline	O
models	O
—	O
a	O
vanilla	O
LSTM	Method
model	O
(	O
LSTM	Method
)	O
and	O
an	O
LSTM	Method
model	O
with	O
a	O
deterministic	Method
attention	Method
mechanism	Method
(	O
LSTM	Method
+	Method
Att	Method
)	O
.	O

The	O
former	O
directly	O
applies	O
the	O
QA	O
matching	O
function	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
on	O
the	O
independent	Method
question	Method
and	Method
answer	Method
representations	Method
which	O
are	O
the	O
last	O
state	O
outputs	O
and	O
from	O
the	O
question	Method
and	Method
answer	Method
LSTM	Method
models	Method
.	O

The	O
latter	O
adds	O
an	O
attention	Method
model	Method
to	O
learn	O
pair	Method
-	Method
specific	Method
representation	Method
for	O
prediction	Task
on	O
the	O
basis	O
of	O
the	O
vanilla	O
LSTM	Method
.	O

Moreover	O
,	O
LSTM	Method
+	Method
Att	Method
is	O
the	O
deterministic	O
counterpart	O
of	O
NASM	Method
,	O
which	O
has	O
the	O
same	O
neural	Method
network	Method
architecture	Method
as	O
NASM	Method
.	O

The	O
only	O
difference	O
is	O
that	O
it	O
replaces	O
the	O
stochastic	Method
units	Method
with	O
deterministic	Method
ones	Method
,	O
and	O
no	O
inference	Method
network	Method
is	O
required	O
to	O
carry	O
out	O
stochastic	Method
estimation	Method
.	O

Following	O
previous	O
work	O
,	O
for	O
each	O
of	O
our	O
models	O
we	O
also	O
add	O
a	O
lexical	Method
overlap	Method
feature	O
by	O
combining	O
a	O
co	O
-	O
occurrence	O
word	O
count	O
feature	O
with	O
the	O
probability	O
generated	O
from	O
the	O
neural	Method
model	Method
.	O

MAP	Metric
and	O
MRR	Metric
are	O
adopted	O
as	O
the	O
evaluation	Metric
metrics	Metric
for	O
this	O
task	O
.	O

To	O
facilitate	O
direct	O
comparison	O
with	O
previous	O
work	O
we	O
follow	O
the	O
same	O
experimental	O
setup	O
as	O
Yu:2014	O
and	O
severyn2015disi	O
.	O

The	O
word	O
embeddings	O
are	O
obtained	O
by	O
running	O
the	O
word2vec	O
tool	O
DBLP	O
:	O
conf	O
/	O
nips	O
/	O
MikolovSCCD13	O
on	O
the	O
English	O
Wikipedia	O
dump	O
and	O
the	O
AQUAINThttps:	O
//	O
catalog.ldc.upenn.edu	O
/	O
LDC2002T31	O
corpus	O
.	O

We	O
use	O
LSTMs	Method
with	Method
layers	Method
and	O
hidden	O
units	O
,	O
and	O
apply	O
dropout	Method
after	O
the	O
embedding	Method
layer	Method
.	O

For	O
the	O
construction	O
of	O
the	O
inference	Task
network	Task
,	O
we	O
use	O
an	O
MLP	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
with	O
2	O
layers	O
and	O
tanh	O
units	O
of	O
50	O
dimension	O
,	O
and	O
an	O
MLP	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
with	O
2	O
layers	O
and	O
tanh	O
units	O
of	O
150	O
dimension	O
for	O
modelling	O
the	O
joint	Method
representation	Method
.	O

During	O
training	Task
we	O
carry	O
out	O
stochastic	Method
estimation	Method
by	O
taking	O
one	O
sample	O
for	O
computing	O
the	O
gradients	O
,	O
while	O
in	O
prediction	Task
we	O
use	O
20	O
samples	O
to	O
calculate	O
the	O
expectation	O
of	O
the	O
lower	O
bound	O
.	O

Figure	O
[	O
reference	O
]	O
presents	O
the	O
standard	O
deviation	O
of	O
NASM	Method
’s	O
MAP	Metric
scores	Metric
while	O
using	O
different	O
numbers	O
of	O
samples	O
.	O

Considering	O
the	O
trade	O
-	O
off	O
between	O
computational	Metric
cost	Metric
and	O
variance	Metric
,	O
we	O
chose	O
20	O
samples	O
for	O
prediction	Task
in	O
all	O
the	O
experiments	O
.	O

The	O
models	O
are	O
trained	O
using	O
Adam	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
KingmaB14	O
,	O
with	O
hyperparameters	O
selected	O
by	O
optimising	O
the	O
MAP	Metric
score	O
on	O
the	O
development	O
set	O
.	O

subsection	O
:	O
Experiments	O
on	O
Answer	Task
Sentence	Task
Selection	Task
Table	O
[	O
reference	O
]	O
compares	O
the	O
results	O
of	O
our	O
models	O
with	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
both	O
answer	O
selection	O
datasets	O
.	O

On	O
the	O
QASent	Material
dataset	O
,	O
our	O
vanilla	O
LSTM	Method
model	O
outperforms	O
the	O
deep	O
CNN	Method
model	O
by	O
approximately	O
on	O
MAP	Metric
and	O
on	O
MRR	Metric
.	O

The	O
LSTM	Method
+	Method
Att	Method
performs	O
slightly	O
better	O
than	O
the	O
vanilla	O
LSTM	Method
model	O
,	O
and	O
our	O
NASM	Method
improves	O
the	O
results	O
further	O
.	O

Since	O
the	O
QASent	Material
dataset	O
is	O
biased	O
towards	O
lexical	O
overlapping	O
features	O
,	O
after	O
combining	O
with	O
a	O
co	O
-	O
occurrence	O
word	O
count	O
feature	O
,	O
our	O
best	O
model	O
NASM	Method
outperforms	O
all	O
the	O
previous	O
models	O
,	O
including	O
both	O
neural	Method
network	Method
based	Method
models	Method
and	O
classifiers	Method
with	O
a	O
set	O
of	O
hand	O
-	O
crafted	O
features	O
(	O
e.g.	O
LCLR	Method
)	O
.	O

Similarly	O
,	O
on	O
the	O
WikiQA	Material
dataset	O
,	O
all	O
of	O
our	O
models	O
outperform	O
the	O
previous	O
distributional	Method
models	Method
by	O
a	O
large	O
margin	O
.	O

By	O
including	O
a	O
word	O
count	O
feature	O
,	O
our	O
models	O
improve	O
further	O
and	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

Notably	O
,	O
on	O
both	O
datasets	O
,	O
our	O
two	O
LSTM	Method
-	O
based	O
models	O
have	O
set	O
strong	O
baselines	O
and	O
NASM	Method
works	O
even	O
better	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
introducing	O
stochastic	Method
units	Method
to	O
model	O
question	O
semantics	O
in	O
this	O
answer	Task
sentence	Task
selection	Task
task	Task
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
compare	O
the	O
effectiveness	O
of	O
the	O
latent	Method
attention	Method
mechanism	Method
(	O
NASM	Method
)	O
and	O
its	O
deterministic	Method
counterpart	Method
(	O
LSTM	Method
+	Method
Att	Method
)	O
by	O
visualising	O
the	O
attention	O
scores	O
on	O
the	O
answer	O
sentences	O
.	O

For	O
most	O
of	O
the	O
negative	O
answer	O
sentences	O
,	O
neither	O
of	O
the	O
two	O
attention	Method
models	Method
can	O
attend	O
to	O
reasonable	O
words	O
that	O
are	O
beneficial	O
for	O
predicting	Task
relatedness	Task
.	O

But	O
for	O
the	O
correct	O
answer	O
sentences	O
,	O
such	O
as	O
the	O
ones	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
both	O
attention	Method
models	Method
are	O
able	O
to	O
capture	O
crucial	O
information	O
by	O
attending	O
to	O
different	O
parts	O
of	O
the	O
sentence	O
based	O
on	O
the	O
question	O
semantics	O
.	O

Interestingly	O
,	O
compared	O
to	O
the	O
deterministic	Method
counterpart	Method
LSTM	Method
+	Method
Att	Method
,	O
our	O
NASM	Method
assigns	O
higher	O
attention	O
scores	O
on	O
the	O
prominent	O
words	O
that	O
are	O
relevant	O
to	O
the	O
question	O
,	O
which	O
forms	O
a	O
more	O
peaked	O
distribution	O
and	O
in	O
turn	O
helps	O
the	O
model	O
achieve	O
better	O
performance	O
.	O

In	O
order	O
to	O
have	O
an	O
intuitive	O
observation	O
on	O
the	O
latent	O
distributions	O
,	O
we	O
present	O
Hinton	O
diagrams	O
of	O
their	O
log	Method
standard	Method
deviation	Method
parameters	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

In	O
a	O
Hinton	O
diagram	O
,	O
the	O
size	O
of	O
a	O
square	O
is	O
proportional	O
to	O
a	O
value	O
’s	O
magnitude	O
,	O
and	O
the	O
colour	O
(	O
black	O
/	O
white	O
)	O
indicates	O
its	O
sign	O
(	O
positive	O
/	O
negative	O
)	O
.	O

In	O
this	O
case	O
,	O
we	O
visualise	O
the	O
parameters	O
of	O
50	O
conditional	O
distributions	O
with	O
the	O
questions	O
selected	O
from	O
5	O
different	O
groups	O
,	O
which	O
start	O
with	O
‘	O
how	O
’	O
,	O
‘	O
what	O
’	O
,	O
‘	O
who	O
’	O
,	O
‘	O
when	O
’	O
and	O
‘	O
where	O
’	O
.	O

All	O
the	O
log	O
standard	O
deviations	O
are	O
initialised	O
as	O
zero	O
before	O
training	O
.	O

According	O
to	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
the	O
questions	O
starting	O
with	O
‘	O
how	O
’	O
have	O
more	O
white	O
areas	O
,	O
which	O
indicates	O
higher	O
variances	O
or	O
more	O
uncertainties	O
are	O
in	O
these	O
dimensions	O
.	O

By	O
contrast	O
,	O
the	O
questions	O
starting	O
with	O
‘	O
what	O
’	O
have	O
black	O
squares	O
in	O
almost	O
every	O
dimension	O
.	O

Intuitively	O
,	O
it	O
is	O
more	O
difficult	O
to	O
understand	O
and	O
answer	O
the	O
questions	O
starting	O
with	O
‘	O
how	O
’	O
than	O
the	O
others	O
,	O
while	O
the	O
‘	O
what	O
’	O
questions	O
commonly	O
have	O
explicit	O
words	O
indicating	O
the	O
possible	O
answers	O
.	O

To	O
validate	O
this	O
,	O
we	O
compute	O
the	O
stratified	O
MAP	Metric
scores	O
based	O
on	O
different	O
question	O
type	O
.	O

The	O
MAP	Metric
of	O
’	O
how	O
’	O
questions	O
is	O
0.524	O
which	O
is	O
the	O
lowest	O
among	O
the	O
five	O
groups	O
.	O

Hence	O
empirically	O
,	O
’	O
how	O
’	O
questions	O
are	O
harder	O
to	O
’	O
understand	O
and	O
answer	O
’	O
.	O

section	O
:	O
Discussion	O
As	O
shown	O
in	O
the	O
experiments	O
,	O
neural	O
variational	Method
inference	Method
brings	O
consistent	O
improvements	O
on	O
the	O
performance	O
of	O
both	O
NLP	Task
tasks	O
.	O

The	O
basic	O
intuition	O
is	O
that	O
the	O
latent	O
distributions	O
grant	O
the	O
ability	O
to	O
sum	O
over	O
all	O
the	O
possibilities	O
in	O
terms	O
of	O
semantics	O
.	O

From	O
the	O
perspective	O
of	O
optimisation	Task
,	O
one	O
of	O
the	O
most	O
important	O
reasons	O
is	O
that	O
Bayesian	Method
learning	Method
guards	O
against	O
overfitting	O
.	O

According	O
to	O
Eq	O
.	O

[	O
reference	O
]	O
in	O
NVDM	Method
,	O
since	O
we	O
adopt	O
as	O
a	O
standard	O
Gaussian	O
prior	O
,	O
the	O
KL	O
divergence	O
term	O
can	O
be	O
analytically	O
computed	O
as	O
.	O

It	O
is	O
not	O
difficult	O
to	O
find	O
that	O
it	O
actually	O
acts	O
as	O
L2	Method
regulariser	Method
when	O
we	O
update	O
the	O
.	O

Similarly	O
,	O
in	O
NASM	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
,	O
we	O
also	O
have	O
the	O
KL	O
divergence	O
term	O
.	O

Different	O
from	O
NVDM	Method
,	O
it	O
attempts	O
to	O
minimise	O
the	O
distance	O
between	O
and	O
that	O
are	O
both	O
conditional	O
distributions	O
.	O

Because	O
as	O
well	O
as	O
are	O
learned	O
during	O
training	O
,	O
the	O
two	O
distributions	O
are	O
mutually	O
restrained	O
while	O
being	O
updated	O
.	O

Therefore	O
,	O
NVDM	Method
simply	O
penalises	O
the	O
large	O
and	O
encourages	O
to	O
approach	O
the	O
prior	O
for	O
every	O
document	O
,	O
but	O
in	O
NASM	Method
,	O
acts	O
like	O
a	O
moving	Method
baseline	Method
distribution	Method
which	O
regularises	O
the	O
update	O
of	O
for	O
every	O
different	O
conditions	O
.	O

In	O
practice	O
,	O
we	O
carry	O
out	O
early	Task
stopping	Task
by	O
observing	O
the	O
prediction	Task
performance	O
on	O
development	O
dataset	O
for	O
the	O
question	Task
answer	Task
selection	Task
task	Task
.	O

Using	O
the	O
same	O
learning	O
rate	O
and	O
neural	Method
network	Method
structure	Method
,	O
LSTM	Method
+	Method
Att	Method
reaches	O
optimal	O
performance	O
and	O
starts	O
to	O
overfit	O
on	O
training	O
dataset	O
generally	O
at	O
the	O
th	O
iteration	O
,	O
while	O
NASM	Method
starts	O
to	O
overfit	O
around	O
the	O
th	O
iteration	O
.	O

More	O
interestingly	O
,	O
in	O
the	O
question	Task
answer	Task
selection	Task
experiments	O
,	O
NASM	Method
learns	O
more	O
peaked	O
attention	O
scores	O
than	O
its	O
deterministic	O
counterpart	O
LSTM	Method
+	Method
Att	Method
.	O

For	O
the	O
update	Method
process	Method
of	O
LSTM	Method
+	Method
Att	Method
,	O
we	O
find	O
there	O
exists	O
a	O
relatively	O
big	O
variance	O
in	O
the	O
gradients	O
w.r.t	O
.	O

question	O
semantics	O
(	O
LSTM	Method
+	Method
Att	Method
applies	O
deterministic	O
while	O
NASM	Method
applies	O
stochastic	O
)	O
.	O

This	O
is	O
because	O
the	O
training	O
dataset	O
is	O
small	O
and	O
contains	O
many	O
negative	O
answer	O
sentences	O
that	O
brings	O
no	O
benefit	O
but	O
noise	O
to	O
the	O
learning	O
of	O
the	O
attention	Method
model	Method
.	O

In	O
contrast	O
,	O
for	O
the	O
update	Method
process	Method
of	O
NASM	Method
,	O
we	O
observe	O
more	O
stable	O
gradients	O
w.r.t	O
.	O

the	O
parameters	O
of	O
latent	O
distributions	O
.	O

The	O
optimisation	O
of	O
the	O
lower	O
bound	O
on	O
one	O
hand	O
maximises	O
the	O
conditional	O
log	O
-	O
likelihood	O
(	O
that	O
the	O
deterministic	Method
counterpart	Method
cares	O
about	O
)	O
and	O
on	O
the	O
other	O
hand	O
minimises	O
the	O
KL	Method
-	Method
divergence	Method
(	O
that	O
regularises	O
the	O
gradients	O
)	O
.	O

Hence	O
,	O
each	O
update	O
of	O
the	O
lower	O
bound	O
actually	O
keeps	O
the	O
gradients	O
w.r.t	O
.	O

from	O
swinging	O
heavily	O
.	O

Besides	O
,	O
since	O
the	O
values	O
of	O
are	O
not	O
very	O
significant	O
in	O
this	O
case	O
,	O
the	O
distribution	O
of	O
attention	O
scores	O
mainly	O
depends	O
on	O
.	O

Therefore	O
,	O
the	O
learning	O
of	O
the	O
attention	Method
model	Method
benefits	O
from	O
the	O
regularisation	Method
as	O
well	O
,	O
and	O
it	O
explains	O
the	O
fact	O
that	O
NASM	Method
learns	O
more	O
peaked	O
attention	O
scores	O
which	O
in	O
turn	O
helps	O
achieve	O
a	O
better	O
prediction	Task
performance	O
.	O

Since	O
the	O
computations	O
of	O
NVDM	Method
and	O
NASM	Method
can	O
be	O
parallelised	O
on	O
GPU	O
and	O
only	O
one	O
sample	O
is	O
required	O
during	O
training	O
process	O
,	O
it	O
is	O
very	O
efficient	O
to	O
carry	O
out	O
the	O
neural	O
variational	Method
inference	Method
.	O

Moreover	O
,	O
for	O
both	O
NVDM	Method
and	O
NASM	Method
,	O
all	O
the	O
parameters	O
are	O
updated	O
by	O
back	Method
-	Method
propagation	Method
.	O

Thus	O
,	O
the	O
increased	O
computation	Metric
time	Metric
for	O
the	O
stochastic	O
units	O
only	O
comes	O
from	O
the	O
added	O
parameters	O
of	O
the	O
inference	Method
network	Method
.	O

section	O
:	O
Related	O
Work	O
Training	O
an	O
inference	Method
network	Method
to	O
approximate	O
the	O
variational	O
distribution	O
was	O
first	O
proposed	O
in	O
the	O
context	O
of	O
Helmholtz	O
machines	O
hinton1994autoencoders	O
,	O
hinton1995wake	O
,	O
dayan1996varieties	O
,	O
but	O
applications	O
of	O
these	O
directed	Method
generative	Method
models	Method
come	O
up	O
against	O
the	O
problem	O
of	O
establishing	O
low	Method
variance	Method
gradient	Method
estimators	Method
.	O

Recent	O
advances	O
in	O
neural	O
variational	Method
inference	Method
mitigate	O
this	O
problem	O
by	O
reparameterising	O
the	O
continuous	O
random	O
variables	O
rezende2014stochastic	O
,	O
kingma2013auto	O
,	O
using	O
control	Method
variates	Method
mnih2014neural	O
or	O
approximating	O
the	O
posterior	O
with	O
importance	Method
sampling	Method
bornschein2014reweighted	O
.	O

The	O
instantiations	O
of	O
these	O
ideas	O
gregor2015draw	O
,	O
kingma2014semi	O
,	O
ba2015learning	O
have	O
demonstrated	O
strong	O
performance	O
on	O
the	O
tasks	O
of	O
image	Task
processing	Task
.	O

The	O
recent	O
variants	O
of	O
generative	Method
auto	Method
-	Method
encoder	Method
louizos2015variational	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
MakhzaniSJG15	O
are	O
also	O
very	O
competitive	O
.	O

tang2013learning	O
applies	O
the	O
similar	O
idea	O
of	O
introducing	O
stochastic	Method
units	Method
for	O
expression	Task
classification	Task
,	O
but	O
its	O
inference	Task
is	O
carried	O
out	O
by	O
Monte	Method
Carlo	Method
EM	Method
algorithm	Method
with	O
the	O
reliance	O
on	O
importance	Method
sampling	Method
,	O
which	O
is	O
less	O
efficient	O
and	O
lack	O
of	O
scalability	O
.	O

Another	O
class	O
of	O
neural	Method
generative	Method
models	Method
make	O
use	O
of	O
the	O
autoregressive	Method
assumption	Method
larochelle2011neural	O
,	O
uria2014deep	O
,	O
germain2015made	O
,	O
gregor2013deep	O
.	O

Applications	O
of	O
these	O
models	O
on	O
document	Task
modelling	Task
achieve	O
significant	O
improvements	O
on	O
generating	Task
documents	Task
,	O
compared	O
to	O
conventional	O
probabilistic	Method
topic	Method
models	Method
hofmann1999probabilistic	O
,	O
blei2003latent	O
and	O
also	O
the	O
RBMs	Method
hinton2009replicated	O
,	O
Srivastava2013	O
.	O

While	O
these	O
models	O
that	O
use	O
binary	O
semantic	O
vectors	O
,	O
our	O
NVDM	Method
employs	O
dense	Method
continuous	Method
document	Method
representations	Method
which	O
are	O
both	O
expressive	O
and	O
easy	O
to	O
train	O
.	O

The	O
semantic	Method
word	Method
vector	Method
model	Method
maas2011learning	O
also	O
employs	O
a	O
continuous	O
semantic	O
vector	O
to	O
generate	O
words	O
,	O
but	O
the	O
model	O
is	O
trained	O
by	O
MAP	Metric
inference	O
which	O
does	O
not	O
permit	O
the	O
calculation	O
of	O
the	O
posterior	O
distribution	O
.	O

A	O
very	O
similar	O
idea	O
to	O
NVDM	Method
is	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
BowmanVVDJB15	O
,	O
which	O
employs	O
VAE	Method
to	O
generate	O
sentences	O
from	O
a	O
continuous	O
space	O
.	O

Apart	O
from	O
the	O
work	O
mentioned	O
above	O
,	O
there	O
is	O
other	O
interesting	O
work	O
on	O
question	Task
answering	Task
with	O
deep	Method
neural	Method
networks	Method
.	O

One	O
of	O
the	O
popular	O
streams	O
is	O
mapping	O
factoid	O
questions	O
with	O
answer	O
triples	O
in	O
the	O
knowledge	O
base	O
Bordes:2014:EMNLP	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
BordesWU14	O
,	O
DBLP	O
:	O
conf	O
/	O
acl	O
/	O
YihHM14	O
.	O

Moreover	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
WestonCB14	O
,	O
sukhbaatar2015end	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
KumarISBEPOGS15	O
further	O
exploit	O
memory	Method
networks	Method
,	O
where	O
long	O
-	O
term	O
memories	O
act	O
as	O
dynamic	O
knowledge	O
bases	O
.	O

Another	O
attention	Method
-	Method
based	Method
model	Method
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
HermannKGEKSB15	O
applies	O
the	O
attentive	Method
network	Method
to	O
help	O
read	O
and	O
comprehend	O
for	O
long	O
articles	O
.	O

section	O
:	O
Conclusion	O
This	O
paper	O
introduced	O
a	O
deep	O
neural	O
variational	Method
inference	Method
framework	O
for	O
generative	Method
models	Method
of	Method
text	Method
.	O

We	O
experimented	O
on	O
two	O
diverse	O
tasks	O
,	O
document	Task
modelling	Task
and	O
question	Task
answer	Task
selection	Task
tasks	Task
to	O
demonstrate	O
the	O
effectiveness	O
of	O
this	O
framework	O
,	O
where	O
in	O
both	O
cases	O
our	O
models	O
achieve	O
state	O
of	O
the	O
art	O
performance	O
.	O

Apart	O
from	O
the	O
promising	O
results	O
,	O
our	O
model	O
also	O
has	O
the	O
advantages	O
of	O
(	O
1	O
)	O
simple	O
,	O
expressive	O
,	O
and	O
efficient	O
when	O
training	O
with	O
the	O
SGVB	Method
algorithm	Method
;	O
(	O
2	O
)	O
suitable	O
for	O
both	O
unsupervised	Task
and	Task
supervised	Task
learning	Task
tasks	Task
;	O
and	O
(	O
3	O
)	O
capable	O
of	O
generalising	O
to	O
incorporate	O
any	O
type	O
of	O
neural	Method
network	Method
.	O

bibliography	O
:	O
References	O
section	O
:	O
t	Method
-	Method
SNE	Method
Visualisation	Method
of	Method
Document	Method
Representations	Method
[	O
Neural	Method
Variational	Method
Document	Method
Model	Method
]	O
[	O
Semantic	Task
Word	Task
Vector	Task
]	O
section	O
:	O
Details	O
of	O
the	O
Deep	Method
Neural	Method
Network	Method
Structures	Method
subsection	O
:	O
Neural	Method
Variational	Method
Document	Method
Model	Method
(	O
1	O
)	O
Inference	Method
Network	Method
:	O
(	O
2	O
)	O
Generative	Method
Model	Method
:	O
(	O
3	O
)	O
KL	Method
Divergence	Method
:	O
The	O
variational	O
lower	O
bound	O
to	O
be	O
optimised	O
:	O
subsection	O
:	O
Neural	Method
Answer	Method
Selection	Method
Model	Method
(	O
1	O
)	O
Inference	Method
Network	Method
:	O
(	O
2	O
)	O
Generative	Method
Model	Method
:	O
:	O
(	O
3	O
)	O
KL	O
Divergence	O
:	O
The	O
variational	O
lower	O
bound	O
to	O
be	O
optimised	O
:	O
section	O
:	O
Computational	Metric
Complexity	Metric
The	O
computational	Metric
complexity	Metric
of	O
NVDM	Method
for	O
a	O
training	O
document	O
is	O
.	O

Here	O
,	O
represents	O
the	O
cost	O
for	O
the	O
inference	Method
network	Method
to	O
generate	O
a	O
sample	O
,	O
where	O
is	O
the	O
number	O
of	O
the	O
layers	O
in	O
the	O
inference	Method
network	Method
and	O
is	O
the	O
average	O
dimension	O
of	O
these	O
layers	O
.	O

Besides	O
,	O
is	O
the	O
cost	O
of	O
reconstructing	O
the	O
document	O
from	O
a	O
sample	O
,	O
where	O
is	O
the	O
average	O
length	O
of	O
the	O
documents	O
and	O
represents	O
the	O
volume	O
of	O
words	O
applied	O
in	O
this	O
document	Method
model	Method
,	O
which	O
is	O
conventionally	O
much	O
lager	O
than	O
.	O

The	O
computational	Metric
complexity	Metric
of	O
NASM	Method
for	O
a	O
training	Task
question	Task
-	Task
answer	Task
pair	Task
is	O
.	O

The	O
inference	Method
network	Method
needs	O
.	O

It	O
takes	O
to	O
produce	O
the	O
joint	Method
representation	Method
for	O
a	O
question	O
-	O
answer	O
pair	O
and	O
its	O
label	O
,	O
where	O
is	O
the	O
total	O
number	O
of	O
parameters	O
of	O
an	O
LSTM	Method
and	O
is	O
the	O
average	O
length	O
of	O
the	O
sentences	O
.	O

Based	O
on	O
the	O
joint	Method
representation	Method
,	O
an	O
MLP	Method
spends	O
to	O
generate	O
a	O
sample	O
,	O
where	O
is	O
the	O
number	O
of	O
layers	O
and	O
represents	O
the	O
average	O
dimension	O
.	O

The	O
generative	Method
model	Method
requires	O
.	O

Similarly	O
,	O
it	O
costs	O
to	O
construct	O
the	O
generative	O
latent	O
distribution	O
,	O
where	O
can	O
be	O
saved	O
if	O
the	O
LSTMs	Method
are	O
shared	O
by	O
the	O
inference	Method
network	Method
and	O
the	O
generative	Method
model	Method
.	O

Besides	O
,	O
the	O
attention	Method
model	Method
takes	O
and	O
the	O
relatedness	Task
prediction	Task
takes	O
the	O
last	O
.	O

Since	O
the	O
computations	O
of	O
NVDM	Method
and	O
NASM	Method
can	O
be	O
parallelised	O
in	O
GPU	O
and	O
only	O
one	O
sample	O
is	O
required	O
during	O
training	O
process	O
,	O
it	O
is	O
very	O
efficient	O
to	O
carry	O
out	O
the	O
neural	O
variational	Method
inference	Method
.	O

As	O
NVDM	Method
is	O
an	O
instantiation	O
of	O
variational	Method
auto	Method
-	Method
encoder	Method
,	O
its	O
computational	Metric
complexity	Metric
is	O
the	O
same	O
as	O
the	O
deterministic	Method
auto	Method
-	Method
encoder	Method
.	O

In	O
addition	O
,	O
the	O
computational	Metric
complexity	Metric
of	O
LSTM	Method
+	Method
Att	Method
,	O
the	O
deterministic	O
counterpart	O
of	O
NASM	Method
,	O
is	O
also	O
.	O

There	O
is	O
only	O
time	O
increase	O
by	O
introducing	O
an	O
inference	Method
network	Method
for	O
NASM	Method
when	O
compared	O
to	O
LSTM	Method
+	Method
Att	Method
.	O

