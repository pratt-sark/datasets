document	O
:	O
Improving	Method
neural	Method
networks	Method
by	Method
preventing	Method
co	Method
-	Method
adaptation	Method
of	Method
feature	Method
detectors	Method
When	O
a	O
large	O
feedforward	O
neural	Method
network	Method
is	O
trained	O
on	O
a	O
small	O
training	O
set	O
,	O
it	O
typically	O
performs	O
poorly	O
on	O
held	O
-	O
out	O
test	O
data	O
.	O

This	O
“	O
overfitting	O
”	O
is	O
greatly	O
reduced	O
by	O
randomly	O
omitting	O
half	O
of	O
the	O
feature	Method
detectors	Method
on	O
each	O
training	O
case	O
.	O

This	O
prevents	O
complex	O
co	Task
-	Task
adaptations	Task
in	O
which	O
a	O
feature	Method
detector	Method
is	O
only	O
helpful	O
in	O
the	O
context	O
of	O
several	O
other	O
specific	O
feature	Method
detectors	Method
.	O

Instead	O
,	O
each	O
neuron	O
learns	O
to	O
detect	O
a	O
feature	O
that	O
is	O
generally	O
helpful	O
for	O
producing	O
the	O
correct	O
answer	O
given	O
the	O
combinatorially	O
large	O
variety	O
of	O
internal	O
contexts	O
in	O
which	O
it	O
must	O
operate	O
.	O

Random	Method
“	Method
dropout	Method
”	O
gives	O
big	O
improvements	O
on	O
many	O
benchmark	O
tasks	O
and	O
sets	O
new	O
records	O
for	O
speech	Task
and	O
object	Task
recognition	Task
.	O

A	O
feedforward	O
,	O
artificial	O
neural	Method
network	Method
uses	O
layers	O
of	O
non	O
-	O
linear	O
“	O
hidden	O
”	O
units	O
between	O
its	O
inputs	O
and	O
its	O
outputs	O
.	O

By	O
adapting	O
the	O
weights	O
on	O
the	O
incoming	O
connections	O
of	O
these	O
hidden	Method
units	Method
it	O
learns	O
feature	Method
detectors	Method
that	O
enable	O
it	O
to	O
predict	O
the	O
correct	O
output	O
when	O
given	O
an	O
input	O
vector	O
.	O

If	O
the	O
relationship	O
between	O
the	O
input	O
and	O
the	O
correct	O
output	O
is	O
complicated	O
and	O
the	O
network	O
has	O
enough	O
hidden	O
units	O
to	O
model	O
it	O
accurately	O
,	O
there	O
will	O
typically	O
be	O
many	O
different	O
settings	O
of	O
the	O
weights	O
that	O
can	O
model	O
the	O
training	O
set	O
almost	O
perfectly	O
,	O
especially	O
if	O
there	O
is	O
only	O
a	O
limited	O
amount	O
of	O
labeled	O
training	O
data	O
.	O

Each	O
of	O
these	O
weight	O
vectors	O
will	O
make	O
different	O
predictions	O
on	O
held	O
-	O
out	O
test	O
data	O
and	O
almost	O
all	O
of	O
them	O
will	O
do	O
worse	O
on	O
the	O
test	O
data	O
than	O
on	O
the	O
training	O
data	O
because	O
the	O
feature	Method
detectors	Method
have	O
been	O
tuned	O
to	O
work	O
well	O
together	O
on	O
the	O
training	O
data	O
but	O
not	O
on	O
the	O
test	O
data	O
.	O

Overfitting	Method
can	O
be	O
reduced	O
by	O
using	O
“	O
dropout	Method
”	Method
to	O
prevent	O
complex	O
co	O
-	O
adaptations	O
on	O
the	O
training	O
data	O
.	O

On	O
each	O
presentation	O
of	O
each	O
training	O
case	O
,	O
each	O
hidden	O
unit	O
is	O
randomly	O
omitted	O
from	O
the	O
network	O
with	O
a	O
probability	O
of	O
0.5	O
,	O
so	O
a	O
hidden	O
unit	O
can	O
not	O
rely	O
on	O
other	O
hidden	O
units	O
being	O
present	O
.	O

Another	O
way	O
to	O
view	O
the	O
dropout	Method
procedure	Method
is	O
as	O
a	O
very	O
efficient	O
way	O
of	O
performing	O
model	Method
averaging	Method
with	O
neural	Method
networks	Method
.	O

A	O
good	O
way	O
to	O
reduce	O
the	O
error	O
on	O
the	O
test	O
set	O
is	O
to	O
average	O
the	O
predictions	O
produced	O
by	O
a	O
very	O
large	O
number	O
of	O
different	O
networks	O
.	O

The	O
standard	O
way	O
to	O
do	O
this	O
is	O
to	O
train	O
many	O
separate	O
networks	O
and	O
then	O
to	O
apply	O
each	O
of	O
these	O
networks	O
to	O
the	O
test	O
data	O
,	O
but	O
this	O
is	O
computationally	O
expensive	O
during	O
both	O
training	Task
and	O
testing	Task
.	O

Random	Method
dropout	Method
makes	O
it	O
possible	O
to	O
train	O
a	O
huge	O
number	O
of	O
different	O
networks	O
in	O
a	O
reasonable	O
time	O
.	O

There	O
is	O
almost	O
certainly	O
a	O
different	O
network	O
for	O
each	O
presentation	O
of	O
each	O
training	O
case	O
but	O
all	O
of	O
these	O
networks	O
share	O
the	O
same	O
weights	O
for	O
the	O
hidden	O
units	O
that	O
are	O
present	O
.	O

We	O
use	O
the	O
standard	O
,	O
stochastic	Method
gradient	Method
descent	Method
procedure	Method
for	O
training	O
the	O
dropout	O
neural	Method
networks	Method
on	O
mini	O
-	O
batches	O
of	O
training	O
cases	O
,	O
but	O
we	O
modify	O
the	O
penalty	O
term	O
that	O
is	O
normally	O
used	O
to	O
prevent	O
the	O
weights	O
from	O
growing	O
too	O
large	O
.	O

Instead	O
of	O
penalizing	O
the	O
squared	O
length	O
(	O
L2	O
norm	O
)	O
of	O
the	O
whole	O
weight	O
vector	O
,	O
we	O
set	O
an	O
upper	O
bound	O
on	O
the	O
L2	O
norm	O
of	O
the	O
incoming	O
weight	O
vector	O
for	O
each	O
individual	O
hidden	O
unit	O
.	O

If	O
a	O
weight	Method
-	Method
update	Method
violates	O
this	O
constraint	O
,	O
we	O
renormalize	O
the	O
weights	O
of	O
the	O
hidden	O
unit	O
by	O
division	O
.	O

Using	O
a	O
constraint	O
rather	O
than	O
a	O
penalty	O
prevents	O
weights	O
from	O
growing	O
very	O
large	O
no	O
matter	O
how	O
large	O
the	O
proposed	O
weight	Method
-	Method
update	Method
is	O
.	O

This	O
makes	O
it	O
possible	O
to	O
start	O
with	O
a	O
very	O
large	O
learning	Metric
rate	Metric
which	O
decays	O
during	O
learning	Task
,	O
thus	O
allowing	O
a	O
far	O
more	O
thorough	O
search	O
of	O
the	O
weight	O
-	O
space	O
than	O
methods	O
that	O
start	O
with	O
small	O
weights	O
and	O
use	O
a	O
small	O
learning	Metric
rate	Metric
.	O

At	O
test	O
time	O
,	O
we	O
use	O
the	O
“	O
mean	Method
network	Method
”	O
that	O
contains	O
all	O
of	O
the	O
hidden	O
units	O
but	O
with	O
their	O
outgoing	O
weights	O
halved	O
to	O
compensate	O
for	O
the	O
fact	O
that	O
twice	O
as	O
many	O
of	O
them	O
are	O
active	O
.	O

In	O
practice	O
,	O
this	O
gives	O
very	O
similar	O
performance	O
to	O
averaging	O
over	O
a	O
large	O
number	O
of	O
dropout	Method
networks	Method
.	O

In	O
networks	O
with	O
a	O
single	O
hidden	O
layer	O
of	O
units	O
and	O
a	O
“	O
softmax	Method
”	Method
output	Method
layer	Method
for	O
computing	O
the	O
probabilities	O
of	O
the	O
class	O
labels	O
,	O
using	O
the	O
mean	Method
network	Method
is	O
exactly	O
equivalent	O
to	O
taking	O
the	O
geometric	O
mean	O
of	O
the	O
probability	O
distributions	O
over	O
labels	O
predicted	O
by	O
all	O
possible	O
networks	O
.	O

Assuming	O
the	O
dropout	Method
networks	Method
do	O
not	O
all	O
make	O
identical	O
predictions	O
,	O
the	O
prediction	O
of	O
the	O
mean	Method
network	Method
is	O
guaranteed	O
to	O
assign	O
a	O
higher	O
log	O
probability	O
to	O
the	O
correct	O
answer	O
than	O
the	O
mean	O
of	O
the	O
log	O
probabilities	O
assigned	O
by	O
the	O
individual	O
dropout	Method
networks	Method
.	O

Similarly	O
,	O
for	O
regression	Task
with	O
linear	O
output	O
units	O
,	O
the	O
squared	Metric
error	Metric
of	O
the	O
mean	Method
network	Method
is	O
always	O
better	O
than	O
the	O
average	O
of	O
the	O
squared	Metric
errors	Metric
of	O
the	O
dropout	Method
networks	Method
.	O

We	O
initially	O
explored	O
the	O
effectiveness	O
of	O
dropout	Method
using	O
MNIST	Material
,	O
a	O
widely	O
used	O
benchmark	O
for	O
machine	Method
learning	Method
algorithms	Method
.	O

It	O
contains	O
60	O
,	O
000	O
28x28	O
training	O
images	O
of	O
individual	O
hand	O
written	O
digits	O
and	O
10	O
,	O
000	O
test	O
images	O
.	O

Performance	O
on	O
the	O
test	O
set	O
can	O
be	O
greatly	O
improved	O
by	O
enhancing	O
the	O
training	O
data	O
with	O
transformed	O
images	O
or	O
by	O
wiring	O
knowledge	O
about	O
spatial	O
transformations	O
into	O
a	O
convolutional	O
neural	Method
network	Method
or	O
by	O
using	O
generative	Method
pre	Method
-	Method
training	Method
to	O
extract	O
useful	O
features	O
from	O
the	O
training	O
images	O
without	O
using	O
the	O
labels	O
.	O

Without	O
using	O
any	O
of	O
these	O
tricks	O
,	O
the	O
best	O
published	O
result	O
for	O
a	O
standard	O
feedforward	O
neural	Method
network	Method
is	O
160	O
errors	O
on	O
the	O
test	O
set	O
.	O

This	O
can	O
be	O
reduced	O
to	O
about	O
130	O
errors	O
by	O
using	O
50	O
%	O
dropout	Method
with	O
separate	O
L2	O
constraints	O
on	O
the	O
incoming	O
weights	O
of	O
each	O
hidden	O
unit	O
and	O
further	O
reduced	O
to	O
about	O
110	O
errors	O
by	O
also	O
dropping	O
out	O
a	O
random	O
20	O
%	O
of	O
the	O
pixels	O
(	O
see	O
figure	O
[	O
reference	O
]	O
)	O
.	O

Dropout	Method
can	O
also	O
be	O
combined	O
with	O
generative	Method
pre	Method
-	Method
training	Method
,	O
but	O
in	O
this	O
case	O
we	O
use	O
a	O
small	O
learning	Metric
rate	Metric
and	O
no	O
weight	O
constraints	O
to	O
avoid	O
losing	O
the	O
feature	Method
detectors	Method
discovered	O
by	O
the	O
pre	Method
-	Method
training	Method
.	O

The	O
publically	O
available	O
,	O
pre	O
-	O
trained	O
deep	Method
belief	Method
net	Method
described	O
in	O
got	O
118	O
errors	O
when	O
it	O
was	O
fine	O
-	O
tuned	O
using	O
standard	O
back	Method
-	Method
propagation	Method
and	O
92	O
errors	O
when	O
fine	O
-	O
tuned	O
using	O
50	O
%	O
dropout	O
of	O
the	O
hidden	O
units	O
.	O

When	O
the	O
publically	O
available	O
code	O
at	O
URL	O
was	O
used	O
to	O
pre	O
-	O
train	O
a	O
deep	Method
Boltzmann	Method
machine	Method
five	O
times	O
,	O
the	O
unrolled	Method
network	Method
got	O
103	O
,	O
97	O
,	O
94	O
,	O
93	O
and	O
88	O
errors	O
when	O
fine	O
-	O
tuned	O
using	O
standard	O
backpropagation	Method
and	O
83	O
,	O
79	O
,	O
78	O
,	O
78	O
and	O
77	O
errors	O
when	O
using	O
50	O
%	O
dropout	O
of	O
the	O
hidden	O
units	O
.	O

The	O
mean	O
of	O
79	O
errors	O
is	O
a	O
record	O
for	O
methods	O
that	O
do	O
not	O
use	O
prior	O
knowledge	O
or	O
enhanced	O
training	O
sets	O
(	O
For	O
details	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

We	O
then	O
applied	O
dropout	Method
to	O
TIMIT	Material
,	O
a	O
widely	O
used	O
benchmark	O
for	O
recognition	O
of	O
clean	O
speech	Task
with	O
a	O
small	O
vocabulary	O
.	O

Speech	Method
recognition	Method
systems	Method
use	O
hidden	Method
Markov	Method
models	Method
(	O
HMMs	Method
)	O
to	O
deal	O
with	O
temporal	O
variability	O
and	O
they	O
need	O
an	O
acoustic	Method
model	Method
that	O
determines	O
how	O
well	O
a	O
frame	O
of	O
coefficients	O
extracted	O
from	O
the	O
acoustic	O
input	O
fits	O
each	O
possible	O
state	O
of	O
each	O
hidden	Method
Markov	Method
model	Method
.	O

Recently	O
,	O
deep	O
,	O
pre	O
-	O
trained	O
,	O
feedforward	O
neural	Method
networks	Method
that	O
map	O
a	O
short	O
sequence	O
of	O
frames	O
into	O
a	O
probability	O
distribution	O
over	O
HMM	O
states	O
have	O
been	O
shown	O
to	O
outperform	O
tradional	Method
Gaussian	Method
mixture	Method
models	Method
on	O
both	O
TIMIT	Material
and	O
a	O
variety	O
of	O
more	O
realistic	O
large	Task
vocabulary	Task
tasks	Task
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
frame	Metric
classification	Metric
error	Metric
rate	Metric
on	O
the	O
core	O
test	O
set	O
of	O
the	O
TIMIT	Material
benchmark	O
when	O
the	O
central	O
frame	O
of	O
a	O
window	O
is	O
classified	O
as	O
belonging	O
to	O
the	O
HMM	O
state	O
that	O
is	O
given	O
the	O
highest	O
probability	O
by	O
the	O
neural	Method
net	Method
.	O

The	O
input	O
to	O
the	O
net	O
is	O
21	O
adjacent	O
frames	O
with	O
an	O
advance	O
of	O
10ms	O
per	O
frame	O
.	O

The	O
neural	Method
net	Method
has	O
4	O
fully	Method
-	Method
connected	Method
hidden	Method
layers	Method
of	O
4000	O
units	O
per	O
layer	O
and	O
185	O
“	O
softmax	O
”	O
output	O
units	O
that	O
are	O
subsequently	O
merged	O
into	O
the	O
39	O
distinct	O
classes	O
used	O
for	O
the	O
benchmark	O
.	O

Dropout	Method
of	O
50	O
%	O
of	O
the	O
hidden	O
units	O
significantly	O
improves	O
classification	Task
for	O
a	O
variety	O
of	O
different	O
network	Method
architectures	Method
(	O
see	O
figure	O
[	O
reference	O
]	O
)	O
.	O

To	O
get	O
the	O
frame	Metric
recognition	Metric
rate	Metric
,	O
the	O
class	O
probabilities	O
that	O
the	O
neural	Method
network	Method
outputs	O
for	O
each	O
frame	O
are	O
given	O
to	O
a	O
decoder	Method
which	O
knows	O
about	O
transition	O
probabilities	O
between	O
HMM	O
states	O
and	O
runs	O
the	O
Viterbi	Method
algorithm	Method
to	O
infer	O
the	O
single	O
best	O
sequence	O
of	O
HMM	O
states	O
.	O

Without	O
dropout	Method
,	O
the	O
recognition	Metric
rate	Metric
is	O
%	O
and	O
with	O
dropout	Method
this	O
improves	O
to	O
%	O
,	O
which	O
is	O
a	O
record	O
for	O
methods	O
that	O
do	O
not	O
use	O
any	O
information	O
about	O
speaker	O
identity	O
.	O

CIFAR	Material
-	Material
10	Material
is	O
a	O
benchmark	O
task	O
for	O
object	Task
recognition	Task
.	O

It	O
uses	O
32x32	O
downsampled	O
color	O
images	O
of	O
10	O
different	O
object	O
classes	O
that	O
were	O
found	O
by	O
searching	O
the	O
web	O
for	O
the	O
names	O
of	O
the	O
class	O
(	O
e.g.	O
dog	O
)	O
or	O
its	O
subclasses	O
(	O
e.g.	O
Golden	O
Retriever	O
)	O
.	O

These	O
images	O
were	O
labeled	O
by	O
hand	O
to	O
produce	O
50	O
,	O
000	O
training	O
images	O
and	O
10	O
,	O
000	O
test	O
images	O
in	O
which	O
there	O
is	O
a	O
single	O
dominant	O
object	O
that	O
could	O
plausibly	O
be	O
given	O
the	O
class	O
name	O
(	O
see	O
figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
best	O
published	O
error	Metric
rate	Metric
on	O
the	O
test	O
set	O
,	O
without	O
using	O
transformed	O
data	O
,	O
is	O
18.5	O
%	O
.	O

We	O
achieved	O
an	O
error	Metric
rate	Metric
of	O
16.6	O
%	O
by	O
using	O
a	O
neural	Method
network	Method
with	O
three	O
convolutional	Method
hidden	Method
layers	Method
interleaved	O
with	O
three	O
“	O
max	Method
-	Method
pooling	Method
”	Method
layers	Method
that	O
report	O
the	O
maximum	O
activity	O
in	O
local	O
pools	O
of	O
convolutional	O
units	O
.	O

These	O
six	O
layers	O
were	O
followed	O
by	O
one	O
locally	Method
-	Method
connected	Method
layer	Method
(	O
For	O
details	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

Using	O
dropout	Method
in	O
the	O
last	O
hidden	Method
layer	Method
gives	O
an	O
error	Metric
rate	Metric
of	O
15.6	O
%	O
.	O

ImageNet	Material
is	O
an	O
extremely	O
challenging	O
object	Task
recognition	Task
dataset	O
consisting	O
of	O
thousands	O
of	O
high	O
-	O
resolution	O
images	O
of	O
thousands	O
of	O
classes	O
of	O
object	O
.	O

In	O
2010	O
,	O
a	O
subset	O
of	O
1000	O
classes	O
with	O
roughly	O
1000	O
examples	O
per	O
class	O
was	O
the	O
basis	O
of	O
an	O
object	Task
recognition	Task
competition	O
in	O
which	O
the	O
winning	O
entry	O
,	O
which	O
was	O
actually	O
an	O
average	O
of	O
six	O
separate	O
models	O
,	O
achieved	O
an	O
error	Metric
rate	Metric
of	O
47.2	O
%	O
on	O
the	O
test	O
set	O
.	O

The	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
this	O
dataset	O
is	O
45.7	O
%	O
.	O

We	O
achieved	O
comparable	O
performance	O
of	O
48.6	O
%	O
error	Metric
using	O
a	O
single	O
neural	Method
network	Method
with	O
five	O
convolutional	Method
hidden	Method
layers	Method
interleaved	O
with	O
“	O
max	Method
-	Method
pooling	Method
”	Method
layer	Method
followed	O
by	O
two	O
globally	Method
connected	Method
layers	Method
and	O
a	O
final	O
1000	Method
-	Method
way	Method
softmax	Method
layer	Method
.	O

All	O
layers	O
had	O
L2	O
weight	O
constraints	O
on	O
the	O
incoming	O
weights	O
of	O
each	O
hidden	O
unit	O
.	O

Using	O
50	O
%	O
dropout	Method
in	O
the	O
sixth	O
hidden	Method
layer	Method
reduces	O
this	O
to	O
a	O
record	O
42.4	O
%	O
(	O
For	O
details	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

For	O
the	O
speech	Task
recognition	O
dataset	O
and	O
both	O
of	O
the	O
object	Task
recognition	Task
datasets	O
it	O
is	O
necessary	O
to	O
make	O
a	O
large	O
number	O
of	O
decisions	O
in	O
designing	O
the	O
architecture	O
of	O
the	O
net	O
.	O

We	O
made	O
these	O
decisions	O
by	O
holding	O
out	O
a	O
separate	O
validation	O
set	O
that	O
was	O
used	O
to	O
evaluate	O
the	O
performance	O
of	O
a	O
large	O
number	O
of	O
different	O
architectures	O
and	O
we	O
then	O
used	O
the	O
architecture	O
that	O
performed	O
best	O
with	O
dropout	Method
on	O
the	O
validation	O
set	O
to	O
assess	O
the	O
performance	O
of	O
dropout	Method
on	O
the	O
real	O
test	O
set	O
.	O

The	O
Reuters	Material
dataset	Material
contains	O
documents	O
that	O
have	O
been	O
labeled	O
with	O
a	O
hierarchy	O
of	O
classes	O
.	O

We	O
created	O
training	O
and	O
test	O
sets	O
each	O
containing	O
201	O
,	O
369	O
documents	O
from	O
50	O
mutually	O
exclusive	O
classes	O
.	O

Each	O
document	O
was	O
represented	O
by	O
a	O
vector	O
of	O
counts	O
for	O
2000	O
common	O
non	O
-	O
stop	O
words	O
,	O
with	O
each	O
count	O
being	O
transformed	O
to	O
.	O

A	O
feedforward	O
neural	Method
network	Method
with	O
2	O
fully	Method
connected	Method
layers	Method
of	O
2000	O
hidden	O
units	O
trained	O
with	O
backpropagation	Method
gets	O
31.05	O
%	O
error	Metric
on	O
the	O
test	O
set	O
.	O

This	O
is	O
reduced	O
to	O
29.62	O
%	O
by	O
using	O
50	O
%	O
dropout	Method
(	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

We	O
have	O
tried	O
various	O
dropout	O
probabilities	O
and	O
almost	O
all	O
of	O
them	O
improve	O
the	O
generalization	Metric
performance	Metric
of	O
the	O
network	O
.	O

For	O
fully	O
connected	O
layers	O
,	O
dropout	O
in	O
all	O
hidden	O
layers	O
works	O
better	O
than	O
dropout	Method
in	O
only	O
one	O
hidden	O
layer	O
and	O
more	O
extreme	O
probabilities	O
tend	O
to	O
be	O
worse	O
,	O
which	O
is	O
why	O
we	O
have	O
used	O
0.5	O
throughout	O
this	O
paper	O
.	O

For	O
the	O
inputs	O
,	O
dropout	O
can	O
also	O
help	O
,	O
though	O
it	O
it	O
often	O
better	O
to	O
retain	O
more	O
than	O
50	O
%	O
of	O
the	O
inputs	O
.	O

It	O
is	O
also	O
possible	O
to	O
adapt	O
the	O
individual	O
dropout	O
probability	O
of	O
each	O
hidden	O
or	O
input	O
unit	O
by	O
comparing	O
the	O
average	O
performance	O
on	O
a	O
validation	O
set	O
with	O
the	O
average	O
performance	O
when	O
the	O
unit	O
is	O
present	O
.	O

This	O
makes	O
the	O
method	O
work	O
slightly	O
better	O
.	O

For	O
datasets	O
in	O
which	O
the	O
required	O
input	O
-	O
output	O
mapping	O
has	O
a	O
number	O
of	O
fairly	O
different	O
regimes	O
,	O
performance	O
can	O
probably	O
be	O
further	O
improved	O
by	O
making	O
the	O
dropout	O
probabilities	O
be	O
a	O
learned	O
function	O
of	O
the	O
input	O
,	O
thus	O
creating	O
a	O
statistically	O
efficient	O
“	O
mixture	O
of	O
experts	O
”	O
in	O
which	O
there	O
are	O
combinatorially	O
many	O
experts	O
,	O
but	O
each	O
parameter	O
gets	O
adapted	O
on	O
a	O
large	O
fraction	O
of	O
the	O
training	O
data	O
.	O

Dropout	Method
is	O
considerably	O
simpler	O
to	O
implement	O
than	O
Bayesian	Method
model	Method
averaging	Method
which	O
weights	O
each	O
model	O
by	O
its	O
posterior	O
probability	O
given	O
the	O
training	O
data	O
.	O

For	O
complicated	O
model	Task
classes	Task
,	O
like	O
feedforward	O
neural	Method
networks	Method
,	O
Bayesian	Method
methods	Method
typically	O
use	O
a	O
Markov	Method
chain	Method
Monte	Method
Carlo	Method
method	Method
to	O
sample	O
models	O
from	O
the	O
posterior	O
distribution	O
.	O

By	O
contrast	O
,	O
dropout	Method
with	O
a	O
probability	O
of	O
assumes	O
that	O
all	O
the	O
models	O
will	O
eventually	O
be	O
given	O
equal	O
importance	O
in	O
the	O
combination	O
but	O
the	O
learning	O
of	O
the	O
shared	O
weights	O
takes	O
this	O
into	O
account	O
.	O

At	O
test	O
time	O
,	O
the	O
fact	O
that	O
the	O
dropout	O
decisions	O
are	O
independent	O
for	O
each	O
unit	O
makes	O
it	O
very	O
easy	O
to	O
approximate	O
the	O
combined	O
opinions	O
of	O
exponentially	O
many	O
dropout	Method
nets	Method
by	O
using	O
a	O
single	O
pass	O
through	O
the	O
mean	Method
net	Method
.	O

This	O
is	O
far	O
more	O
efficient	O
than	O
averaging	O
the	O
predictions	O
of	O
many	O
separate	O
models	O
.	O

A	O
popular	O
alternative	O
to	O
Bayesian	Method
model	Method
averaging	Method
is	O
“	O
bagging	Method
”	Method
in	O
which	O
different	O
models	O
are	O
trained	O
on	O
different	O
random	O
selections	O
of	O
cases	O
from	O
the	O
training	O
set	O
and	O
all	O
models	O
are	O
given	O
equal	O
weight	O
in	O
the	O
combination	O
.	O

Bagging	Method
is	O
most	O
often	O
used	O
with	O
models	O
such	O
as	O
decision	Method
trees	Method
because	O
these	O
are	O
very	O
quick	O
to	O
fit	O
to	O
data	O
and	O
very	O
quick	O
at	O
test	O
time	O
.	O

Dropout	Method
allows	O
a	O
similar	O
approach	O
to	O
be	O
applied	O
to	O
feedforward	O
neural	Method
networks	Method
which	O
are	O
much	O
more	O
powerful	O
models	O
.	O

Dropout	Method
can	O
be	O
seen	O
as	O
an	O
extreme	O
form	O
of	O
bagging	Method
in	O
which	O
each	O
model	O
is	O
trained	O
on	O
a	O
single	O
case	O
and	O
each	O
parameter	O
of	O
the	O
model	O
is	O
very	O
strongly	O
regularized	O
by	O
sharing	O
it	O
with	O
the	O
corresponding	O
parameter	O
in	O
all	O
the	O
other	O
models	O
.	O

This	O
is	O
a	O
much	O
better	O
regularizer	Method
than	O
the	O
standard	O
method	O
of	O
shrinking	O
parameters	O
towards	O
zero	O
.	O

A	O
familiar	O
and	O
extreme	O
case	O
of	O
dropout	Method
is	O
“	O
naive	Method
bayes	Method
”	Method
in	O
which	O
each	O
input	O
feature	O
is	O
trained	O
separately	O
to	O
predict	O
the	O
class	O
label	O
and	O
then	O
the	O
predictive	O
distributions	O
of	O
all	O
the	O
features	O
are	O
multiplied	O
together	O
at	O
test	O
time	O
.	O

When	O
there	O
is	O
very	O
little	O
training	O
data	O
,	O
this	O
often	O
works	O
much	O
better	O
than	O
logistic	Method
classification	Method
which	O
trains	O
each	O
input	O
feature	O
to	O
work	O
well	O
in	O
the	O
context	O
of	O
all	O
the	O
other	O
features	O
.	O

Finally	O
,	O
there	O
is	O
an	O
intriguing	O
similarity	O
between	O
dropout	O
and	O
a	O
recent	O
theory	O
of	O
the	O
role	O
of	O
sex	O
in	O
evolution	Task
.	O

One	O
possible	O
interpretation	O
of	O
the	O
theory	O
of	O
mixability	O
articulated	O
in	O
is	O
that	O
sex	O
breaks	O
up	O
sets	O
of	O
co	O
-	O
adapted	O
genes	O
and	O
this	O
means	O
that	O
achieving	O
a	O
function	O
by	O
using	O
a	O
large	O
set	O
of	O
co	O
-	O
adapted	O
genes	O
is	O
not	O
nearly	O
as	O
robust	O
as	O
achieving	O
the	O
same	O
function	O
,	O
perhaps	O
less	O
than	O
optimally	O
,	O
in	O
multiple	O
alternative	O
ways	O
,	O
each	O
of	O
which	O
only	O
uses	O
a	O
small	O
number	O
of	O
co	O
-	O
adapted	O
genes	O
.	O

This	O
allows	O
evolution	O
to	O
avoid	O
dead	O
-	O
ends	O
in	O
which	O
improvements	O
in	O
fitness	O
require	O
co	O
-	O
ordinated	O
changes	O
to	O
a	O
large	O
number	O
of	O
co	O
-	O
adapted	O
genes	O
.	O

It	O
also	O
reduces	O
the	O
probability	O
that	O
small	O
changes	O
in	O
the	O
environment	O
will	O
cause	O
large	O
decreases	O
in	O
fitness	O
â	O
a	O
phenomenon	O
which	O
is	O
known	O
as	O
“	O
overfitting	O
”	O
in	O
the	O
field	O
of	O
machine	Task
learning	Task
.	O

bibliography	O
:	O
References	O
and	O
Notes	O
We	O
thank	O
N.	O
Jaitly	O
for	O
help	O
with	O
TIMIT	Material
,	O
H.	O
Larochelle	O
,	O
R.	O
Neal	O
,	O
K.	O
Swersky	O
and	O
C.K.I.	O
Williams	O
for	O
helpful	O
discussions	O
,	O
and	O
NSERC	O
,	O
Google	O
and	O
Microsoft	O
Research	O
for	O
funding	O
.	O

GEH	Method
and	O
RRS	Method
are	O
members	O
of	O
the	O
Canadian	O
Institute	O
for	O
Advanced	O
Research	O
.	O

appendix	O
:	O
Experiments	O
on	O
MNIST	Material
subsection	O
:	O
Details	O
for	O
dropout	Method
training	Method
The	O
MNIST	Material
dataset	O
consists	O
of	O
28	O
28	O
digit	O
images	O
-	O
60	O
,	O
000	O
for	O
training	O
and	O
10	O
,	O
000	O
for	O
testing	O
.	O

The	O
objective	O
is	O
to	O
classify	O
the	O
digit	O
images	O
into	O
their	O
correct	O
digit	O
class	O
.	O

We	O
experimented	O
with	O
neural	Method
nets	Method
of	O
different	O
architectures	O
(	O
different	O
number	O
of	O
hidden	O
units	O
and	O
layers	O
)	O
to	O
evaluate	O
the	O
sensitivity	O
of	O
the	O
dropout	Method
method	Method
to	O
these	O
choices	O
.	O

We	O
show	O
results	O
for	O
4	O
nets	O
(	O
784	O
-	O
800	O
-	O
800	O
-	O
10	O
,	O
784	O
-	O
1200	O
-	O
1200	O
-	O
10	O
,	O
784	O
-	O
2000	O
-	O
2000	O
-	O
10	O
,	O
784	O
-	O
1200	O
-	O
1200	O
-	O
1200	O
-	O
10	O
)	O
.	O

For	O
each	O
of	O
these	O
architectures	O
we	O
use	O
the	O
same	O
dropout	Metric
rates	Metric
-	O
50	O
%	O
dropout	Method
for	O
all	O
hidden	O
units	O
and	O
20	O
%	O
dropout	Method
for	O
visible	O
units	O
.	O

We	O
use	O
stochastic	Method
gradient	Method
descent	Method
with	O
100	Method
-	Method
sized	Method
minibatches	Method
and	O
a	O
cross	Metric
-	Metric
entropy	Metric
objective	Metric
function	Metric
.	O

An	O
exponentially	O
decaying	O
learning	Metric
rate	Metric
is	O
used	O
that	O
starts	O
at	O
the	O
value	O
of	O
10.0	O
(	O
applied	O
to	O
the	O
average	O
gradient	O
in	O
each	O
minibatch	O
)	O
.	O

The	O
learning	Metric
rate	Metric
is	O
multiplied	O
by	O
0.998	O
after	O
each	O
epoch	O
of	O
training	O
.	O

The	O
incoming	O
weight	O
vector	O
corresponding	O
to	O
each	O
hidden	O
unit	O
is	O
constrained	O
to	O
have	O
a	O
maximum	O
squared	O
length	O
of	O
.	O

If	O
,	O
as	O
a	O
result	O
of	O
an	O
update	O
,	O
the	O
squared	O
length	O
exceeds	O
,	O
the	O
vector	O
is	O
scaled	O
down	O
so	O
as	O
to	O
make	O
it	O
have	O
a	O
squared	O
length	O
of	O
.	O

Using	O
cross	Method
validation	Method
we	O
found	O
that	O
gave	O
best	O
results	O
.	O

Weights	O
are	O
initialzed	O
to	O
small	O
random	O
values	O
drawn	O
from	O
a	O
zero	Method
-	Method
mean	Method
normal	Method
distribution	Method
with	O
standard	O
deviation	O
0.01	O
.	O

Momentum	Method
is	O
used	O
to	O
speed	O
up	O
learning	Task
.	O

The	O
momentum	O
starts	O
off	O
at	O
a	O
value	O
of	O
0.5	O
and	O
is	O
increased	O
linearly	O
to	O
0.99	O
over	O
the	O
first	O
500	O
epochs	O
,	O
after	O
which	O
it	O
stays	O
at	O
0.99	O
.	O

Also	O
,	O
the	O
learning	Metric
rate	Metric
is	O
multiplied	O
by	O
a	O
factor	O
of	O
(	O
1	O
-	O
momentum	O
)	O
.	O

No	O
weight	Method
decay	Method
is	O
used	O
.	O

Weights	O
were	O
updated	O
at	O
the	O
end	O
of	O
each	O
minibatch	O
.	O

Training	O
was	O
done	O
for	O
3000	O
epochs	O
.	O

The	O
weight	Method
update	Method
takes	O
the	O
following	O
form	O
:	O
where	O
,	O
with	O
,	O
,	O
,	O
,	O
.	O

While	O
using	O
a	O
constant	Metric
learning	Metric
rate	Metric
also	O
gives	O
improvements	O
over	O
standard	O
backpropagation	Method
,	O
starting	O
with	O
a	O
high	O
learning	Metric
rate	Metric
and	O
decaying	O
it	O
provided	O
a	O
significant	O
boost	O
in	O
performance	O
.	O

Constraining	O
input	O
vectors	O
to	O
have	O
a	O
fixed	O
length	O
prevents	O
weights	O
from	O
increasing	O
arbitrarily	O
in	O
magnitude	O
irrespective	O
of	O
the	O
learning	Metric
rate	Metric
.	O

This	O
gives	O
the	O
network	O
a	O
lot	O
of	O
opportunity	O
to	O
search	O
for	O
a	O
good	O
configuration	O
in	O
the	O
weight	O
space	O
.	O

As	O
the	O
learning	Metric
rate	Metric
decays	O
,	O
the	O
algorithm	O
is	O
able	O
to	O
take	O
smaller	O
steps	O
and	O
finds	O
the	O
right	O
step	O
size	O
at	O
which	O
it	O
can	O
make	O
learning	O
progress	O
.	O

Using	O
a	O
high	O
final	O
momentum	O
distributes	O
gradient	O
information	O
over	O
a	O
large	O
number	O
of	O
updates	O
making	O
learning	Task
stable	O
in	O
this	O
scenario	O
where	O
each	O
gradient	Task
computation	Task
is	O
for	O
a	O
different	O
stochastic	Method
network	Method
.	O

subsection	O
:	O
Details	O
for	O
dropout	Task
finetuning	Task
Apart	O
from	O
training	O
a	O
neural	Method
network	Method
starting	O
from	O
random	O
weights	O
,	O
dropout	Method
can	O
also	O
be	O
used	O
to	O
finetune	Method
pretrained	Method
models	Method
.	O

We	O
found	O
that	O
finetuning	O
a	O
model	O
using	O
dropout	Method
with	O
a	O
small	O
learning	Metric
rate	Metric
can	O
give	O
much	O
better	O
performace	O
than	O
standard	O
backpropagation	Method
finetuning	Method
.	O

Deep	Method
Belief	Method
Nets	Method
-	O
We	O
took	O
a	O
neural	Method
network	Method
pretrained	O
using	O
a	O
Deep	Method
Belief	Method
Network	Method
.	O

It	O
had	O
a	O
784	O
-	O
500	O
-	O
500	O
-	O
2000	Method
architecture	Method
and	O
was	O
trained	O
using	O
greedy	Method
layer	Method
-	Method
wise	Method
Contrastive	Method
Divergence	Method
learning	Method
.	O

Instead	O
of	O
fine	O
-	O
tuning	O
it	O
with	O
the	O
usual	O
backpropagation	Method
algorithm	Method
,	O
we	O
used	O
the	O
dropout	Method
version	Method
of	O
it	O
.	O

Dropout	Metric
rate	Metric
was	O
same	O
as	O
before	O
:	O
50	O
%	O
for	O
hidden	O
units	O
and	O
20	O
%	O
for	O
visible	O
units	O
.	O

A	O
constant	O
small	O
learning	Metric
rate	Metric
of	O
1.0	O
was	O
used	O
.	O

No	O
constraint	O
was	O
imposed	O
on	O
the	O
length	O
of	O
incoming	O
weight	O
vectors	O
.	O

No	O
weight	O
decay	O
was	O
used	O
.	O

All	O
other	O
hyper	O
-	O
parameters	O
were	O
set	O
to	O
be	O
the	O
same	O
as	O
before	O
.	O

The	O
model	O
was	O
trained	O
for	O
1000	O
epochs	O
with	O
stochstic	Method
gradient	Method
descent	Method
using	O
minibatches	O
of	O
size	O
100	O
.	O

While	O
standard	O
back	Method
propagation	Method
gave	O
about	O
118	O
errors	O
,	O
dropout	Method
decreased	O
the	O
errors	O
to	O
about	O
92	O
.	O

Deep	Method
Boltzmann	Method
Machines	Method
-	O
We	O
also	O
took	O
a	O
pretrained	Method
Deep	Method
Boltzmann	Method
Machine	Method
(	O
784	O
-	O
500	O
-	O
1000	O
-	O
10	O
)	O
and	O
finetuned	O
it	O
using	O
dropout	Method
-	Method
backpropagation	Method
.	O

The	O
model	O
uses	O
a	O
1784	Method
-	Method
500	Method
-	Method
1000	Method
-	Method
10	Method
architecture	Method
(	O
The	O
extra	O
1000	O
input	O
units	O
come	O
from	O
the	O
mean	O
-	O
field	O
activations	O
of	O
the	O
second	O
layer	O
of	O
hidden	O
units	O
in	O
the	O
DBM	Method
,	O
See	O
for	O
details	O
)	O
.	O

All	O
finetuning	O
hyper	O
-	O
parameters	O
were	O
set	O
to	O
be	O
the	O
same	O
as	O
the	O
ones	O
used	O
for	O
a	O
Deep	Method
Belief	Method
Network	Method
.	O

We	O
were	O
able	O
to	O
get	O
a	O
mean	O
of	O
about	O
79	O
errors	O
with	O
dropout	Method
whereas	O
usual	O
finetuning	Method
gives	O
about	O
94	O
errors	O
.	O

[	O
]	O
[	O
]	O
subsection	O
:	O
Effect	O
on	O
features	O
One	O
reason	O
why	O
dropout	Method
gives	O
major	O
improvements	O
over	O
backpropagation	Method
is	O
that	O
it	O
encourages	O
each	O
individual	O
hidden	O
unit	O
to	O
learn	O
a	O
useful	O
feature	O
without	O
relying	O
on	O
specific	O
other	O
hidden	O
units	O
to	O
correct	O
its	O
mistakes	O
.	O

In	O
order	O
to	O
verify	O
this	O
and	O
better	O
understand	O
the	O
effect	O
of	O
dropout	O
on	O
feature	Task
learning	Task
,	O
we	O
look	O
at	O
the	O
first	O
level	O
of	O
features	O
learned	O
by	O
a	O
784	O
-	O
500	O
-	O
500	O
neural	Method
network	Method
without	O
any	O
generative	Method
pre	Method
-	Method
training	Method
.	O

The	O
features	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Each	O
panel	O
shows	O
100	O
random	O
features	O
learned	O
by	O
each	O
network	O
.	O

The	O
features	O
that	O
dropout	Method
learns	O
are	O
simpler	O
and	O
look	O
like	O
strokes	O
,	O
whereas	O
the	O
ones	O
learned	O
by	O
standard	O
backpropagation	Method
are	O
difficult	O
to	O
interpret	O
.	O

This	O
confirms	O
that	O
dropout	Method
indeed	O
forces	O
the	O
discriminative	Method
model	Method
to	O
learn	O
good	O
features	O
which	O
are	O
less	O
co	O
-	O
adapted	O
and	O
leads	O
to	O
better	O
generalization	Task
.	O

appendix	O
:	O
Experiments	O
on	O
TIMIT	Material
The	O
TIMIT	Material
Acoustic	O
-	O
Phonetic	O
Continuous	O
Speech	O
Corpus	O
is	O
a	O
standard	O
dataset	O
used	O
for	O
evaluation	O
of	O
automatic	O
speech	Task
recognition	O
systems	O
.	O

It	O
consists	O
of	O
recordings	O
of	O
630	O
speakers	O
of	O
8	O
dialects	O
of	O
American	Material
English	Material
each	O
reading	O
10	O
phonetically	O
-	O
rich	O
sentences	O
.	O

It	O
also	O
comes	O
with	O
the	O
word	O
and	O
phone	O
-	O
level	O
transcriptions	O
of	O
the	O
speech	Task
.	O

The	O
objective	O
is	O
to	O
convert	O
a	O
given	O
speech	Task
signal	O
into	O
a	O
transcription	O
sequence	O
of	O
phones	O
.	O

This	O
data	O
needs	O
to	O
be	O
pre	O
-	O
processed	O
to	O
extract	O
input	O
features	O
and	O
output	O
targets	O
.	O

We	O
used	O
Kaldi	Method
,	O
an	O
open	Method
source	Method
code	Method
library	Method
for	O
speech	Task
,	O
to	O
pre	O
-	O
process	O
the	O
dataset	O
so	O
that	O
our	O
results	O
can	O
be	O
reproduced	O
exactly	O
.	O

The	O
inputs	O
to	O
our	O
networks	O
are	O
log	Method
filter	Method
bank	Method
responses	Method
.	O

They	O
are	O
extracted	O
for	O
25	O
ms	O
speech	Task
windows	O
with	O
strides	O
of	O
10	O
ms	O
.	O

Each	O
dimension	O
of	O
the	O
input	O
representation	O
was	O
normalized	O
to	O
have	O
mean	O
0	O
and	O
variance	O
1	O
.	O

Minibatches	O
of	O
size	O
100	O
were	O
used	O
for	O
both	O
pretraining	Task
and	Task
dropout	Task
finetuning	Task
.	O

We	O
tried	O
several	O
network	Method
architectures	Method
by	O
varying	O
the	O
number	O
of	O
input	O
frames	O
(	O
15	O
and	O
31	O
)	O
,	O
number	O
of	O
layers	O
in	O
the	O
neural	Method
network	Method
(	O
3	O
,	O
4	O
and	O
5	O
)	O
and	O
the	O
number	O
of	O
hidden	O
units	O
in	O
each	O
layer	O
(	O
2000	O
and	O
4000	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
validation	Metric
error	Metric
curves	Metric
for	O
a	O
number	O
of	O
these	O
combinations	O
.	O

Using	O
dropout	Method
consistently	O
leads	O
to	O
lower	O
error	Metric
rates	Metric
.	O

[	O
]	O
[	O
]	O
subsection	O
:	O
Pretraining	O
For	O
all	O
our	O
experiments	O
on	O
TIMIT	Material
,	O
we	O
pretrain	O
the	O
neural	Method
network	Method
with	O
a	O
Deep	Method
Belief	Method
Network	Method
.	O

Since	O
the	O
inputs	O
are	O
real	O
-	O
valued	O
,	O
the	O
first	O
layer	O
was	O
pre	O
-	O
trained	O
as	O
a	O
Gaussian	Method
RBM	Method
.	O

Visible	O
biases	O
were	O
initialized	O
to	O
zero	O
and	O
weights	O
to	O
random	O
numbers	O
sampled	O
from	O
a	O
zero	O
-	O
mean	O
normal	O
distribution	O
with	O
standard	O
deviation	O
0.01	O
.	O

The	O
variance	O
of	O
each	O
visible	O
unit	O
was	O
set	O
to	O
1.0	O
and	O
not	O
learned	O
.	O

Learning	Task
was	O
done	O
by	O
minimizing	O
Contrastive	O
Divergence	O
.	O

Momentum	Method
was	O
used	O
to	O
speed	O
up	O
learning	Task
.	O

Momentum	O
started	O
at	O
0.5	O
and	O
was	O
increased	O
linearly	O
to	O
0.9	O
over	O
20	O
epochs	O
.	O

A	O
learning	Metric
rate	Metric
of	O
0.001	O
on	O
the	O
average	O
gradient	O
was	O
used	O
(	O
which	O
was	O
then	O
multiplied	O
by	O
1	O
-	O
momentum	O
)	O
.	O

An	O
L2	O
weight	O
decay	O
of	O
0.001	O
was	O
used	O
.	O

The	O
model	O
was	O
trained	O
for	O
100	O
epochs	O
.	O

All	O
subsequent	O
layers	O
were	O
trained	O
as	O
binary	Method
RBMs	Method
.	O

A	O
learning	Metric
rate	Metric
of	O
0.01	O
was	O
used	O
.	O

The	O
visible	O
bias	O
of	O
each	O
unit	O
was	O
initialized	O
to	O
where	O
was	O
the	O
mean	O
activation	O
of	O
that	O
unit	O
in	O
the	O
dataset	O
.	O

All	O
other	O
hyper	O
-	O
parameters	O
were	O
set	O
to	O
be	O
the	O
same	O
as	O
those	O
we	O
used	O
for	O
the	O
Gaussian	Method
RBM	Method
.	O

Each	O
layer	O
was	O
trained	O
for	O
50	O
epochs	O
.	O

subsection	O
:	O
Dropout	Task
Finetuning	Task
The	O
pretrained	O
RBMs	Method
were	O
used	O
to	O
initialize	O
the	O
weights	O
in	O
a	O
neural	Method
network	Method
.	O

The	O
network	O
was	O
then	O
finetuned	O
with	O
dropout	Method
-	Method
backpropagation	Method
.	O

Momentum	O
was	O
increased	O
from	O
0.5	O
to	O
0.9	O
linearly	O
over	O
10	O
epochs	O
.	O

A	O
small	O
constant	O
learning	O
rate	O
of	O
1.0	O
was	O
used	O
(	O
applied	O
to	O
the	O
average	O
gradient	O
on	O
a	O
minibatch	O
)	O
.	O

All	O
other	O
hyperparameters	O
are	O
the	O
same	O
as	O
for	O
MNIST	Material
dropout	O
finetuning	O
.	O

The	O
model	O
needs	O
to	O
be	O
run	O
for	O
about	O
200	O
epochs	O
to	O
converge	O
.	O

The	O
same	O
network	O
was	O
also	O
finetuned	O
with	O
standard	O
backpropagation	Method
using	O
a	O
smaller	O
learning	Metric
rate	Metric
of	O
0.1	O
,	O
keeping	O
all	O
other	O
hyperparameters	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
frame	Metric
classification	Metric
error	Metric
and	O
cross	Metric
-	Metric
entropy	Metric
objective	Metric
value	Metric
on	O
the	O
training	Metric
and	Metric
validation	Metric
sets	Metric
.	O

We	O
compare	O
the	O
performance	O
of	O
dropout	Method
and	O
standard	O
backpropagation	Method
on	O
several	O
network	Method
architectures	Method
and	O
input	Method
representations	Method
.	O

Dropout	Method
consistently	O
achieves	O
lower	O
error	Metric
and	O
cross	Metric
-	Metric
entropy	Metric
.	O

It	O
significantly	O
controls	O
overfitting	O
,	O
making	O
the	O
method	O
robust	O
to	O
choices	O
of	O
network	O
architecture	O
.	O

It	O
allows	O
much	O
larger	O
nets	O
to	O
be	O
trained	O
and	O
removes	O
the	O
need	O
for	O
early	Task
stopping	Task
.	O

We	O
also	O
observed	O
that	O
the	O
final	O
error	Metric
obtained	O
by	O
the	O
model	O
is	O
not	O
very	O
sensitive	O
to	O
the	O
choice	O
of	O
learning	Metric
rate	Metric
and	O
momentum	O
.	O

appendix	O
:	O
Experiments	O
on	O
Reuters	O
Reuters	Material
Corpus	Material
Volume	Material
I	Material
(	O
RCV1	Material
-	Material
v2	Material
)	O
is	O
an	O
archive	O
of	O
804	O
,	O
414	O
newswire	O
stories	O
that	O
have	O
been	O
manually	O
categorized	O
into	O
103	O
topics	O
.	O

The	O
corpus	O
covers	O
four	O
major	O
groups	O
:	O
corporate	O
/	O
industrial	O
,	O
economics	O
,	O
government	O
/	O
social	O
,	O
and	O
markets	O
.	O

Sample	O
topics	O
include	O
Energy	O
Markets	O
,	O
Accounts	O
/	O
Earnings	O
,	O
Government	O
Borrowings	O
,	O
Disasters	O
and	O
Accidents	O
,	O
Interbank	O
Markets	O
,	O
Legal	O
/	O
Judicial	O
,	O
Production	O
/	O
Services	O
,	O
etc	O
.	O

The	O
topic	O
classes	O
form	O
a	O
tree	O
which	O
is	O
typically	O
of	O
depth	O
three	O
.	O

We	O
took	O
the	O
dataset	O
and	O
split	O
it	O
into	O
63	O
classes	O
based	O
on	O
the	O
the	O
63	O
categories	O
at	O
the	O
second	O
-	O
level	O
of	O
the	O
category	O
tree	O
.	O

We	O
removed	O
11	O
categories	O
that	O
did	O
not	O
have	O
any	O
data	O
and	O
one	O
category	O
that	O
had	O
only	O
4	O
training	O
examples	O
.	O

We	O
also	O
removed	O
one	O
category	O
that	O
covered	O
a	O
huge	O
chunk	O
(	O
25	O
%	O
)	O
of	O
the	O
examples	O
.	O

This	O
left	O
us	O
with	O
50	O
classes	O
and	O
402	O
,	O
738	O
documents	O
.	O

We	O
divided	O
the	O
documents	O
into	O
equal	O
-	O
sized	O
training	O
and	O
test	O
sets	O
randomly	O
.	O

Each	O
document	O
was	O
represented	O
using	O
the	O
2000	O
most	O
frequent	O
non	O
-	O
stopwords	O
in	O
the	O
dataset	O
.	O

[	O
]	O
[	O
]	O
We	O
trained	O
a	O
neural	Method
network	Method
using	O
dropout	Method
-	Method
backpropagation	Method
and	O
compared	O
it	O
with	O
standard	O
backpropagation	Method
.	O

We	O
used	O
a	O
2000	O
-	O
2000	O
-	O
1000	O
-	O
50	O
architecture	O
.	O

The	O
training	Method
hyperparameters	Method
are	O
same	O
as	O
that	O
in	O
MNIST	Material
dropout	O
training	O
(	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O

Training	O
was	O
done	O
for	O
500	O
epochs	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
training	O
and	O
test	O
set	O
errors	O
as	O
learning	O
progresses	O
.	O

We	O
show	O
two	O
nets	O
-	O
one	O
with	O
a	O
2000	O
-	O
2000	O
-	O
1000	O
-	O
50	O
and	O
another	O
with	O
a	O
2000	Method
-	Method
1000	Method
-	Method
1000	Method
-	Method
50	Method
architecture	Method
trained	O
with	O
and	O
without	O
dropout	Method
.	O

As	O
in	O
all	O
previous	O
datasets	O
discussed	O
so	O
far	O
,	O
we	O
obtain	O
significant	O
improvements	O
here	O
too	O
.	O

The	O
learning	Method
not	O
only	O
results	O
in	O
better	O
generalization	Task
,	O
but	O
also	O
proceeds	O
smoothly	O
,	O
without	O
the	O
need	O
for	O
early	O
stopping	O
.	O

appendix	O
:	O
Tiny	Material
Images	Material
and	O
CIFAR	Material
-	Material
10	Material
The	O
Tiny	Material
Images	Material
dataset	Material
contains	O
80	O
million	O
color	O
images	O
collected	O
from	O
the	O
web	O
.	O

The	O
images	O
were	O
found	O
by	O
searching	O
various	O
image	Method
search	Method
engines	Method
for	O
English	Material
nouns	Material
,	O
so	O
each	O
image	O
comes	O
with	O
a	O
very	O
unreliable	O
label	O
,	O
which	O
is	O
the	O
noun	O
that	O
was	O
used	O
to	O
find	O
it	O
.	O

The	O
CIFAR	Material
-	Material
10	Material
dataset	O
is	O
a	O
subset	O
of	O
the	O
Tiny	Material
Images	Material
dataset	Material
which	O
contains	O
60000	O
images	O
divided	O
among	O
ten	O
classes	O
.	O

Each	O
class	O
contains	O
5000	O
training	O
images	O
and	O
1000	O
testing	O
images	O
.	O

The	O
classes	O
are	O
airplane	O
,	O
automobile	O
,	O
bird	O
,	O
cat	O
,	O
deer	O
,	O
dog	O
,	O
frog	O
,	O
horse	O
,	O
ship	O
,	O
and	O
truck	O
.	O

The	O
CIFAR	Material
-	Material
10	Material
dataset	O
was	O
obtained	O
by	O
filtering	O
the	O
Tiny	Material
Images	Material
dataset	Material
to	O
remove	O
images	O
with	O
incorrect	O
labels	O
.	O

The	O
CIFAR	Material
-	Material
10	Material
images	O
are	O
highly	O
varied	O
,	O
and	O
there	O
is	O
no	O
canonical	O
viewpoint	O
or	O
scale	O
at	O
which	O
the	O
objects	O
appear	O
.	O

The	O
only	O
criteria	O
for	O
including	O
an	O
image	O
were	O
that	O
the	O
image	O
contain	O
one	O
dominant	O
instance	O
of	O
a	O
CIFAR	Material
-	Material
10	Material
class	O
,	O
and	O
that	O
the	O
object	O
in	O
the	O
image	O
be	O
easily	O
identifiable	O
as	O
belonging	O
to	O
the	O
class	O
indicated	O
by	O
the	O
image	O
label	O
.	O

appendix	O
:	O
ImageNet	Material
ImageNet	Material
is	O
a	O
dataset	O
of	O
millions	O
of	O
labeled	O
images	O
in	O
thousands	O
of	O
categories	O
.	O

The	O
images	O
were	O
collected	O
from	O
the	O
web	O
and	O
labelled	O
by	O
human	Method
labellers	Method
using	O
Amazon	Method
’s	Method
Mechanical	Method
Turk	Method
crowd	Method
-	Method
sourcing	Method
tool	Method
.	O

In	O
2010	O
,	O
a	O
subset	O
of	O
roughly	O
1000	O
images	O
in	O
each	O
of	O
1000	O
classes	O
was	O
the	O
basis	O
of	O
an	O
object	Task
recognition	Task
competition	O
,	O
a	O
part	O
of	O
the	O
Pascal	Material
Visual	Material
Object	Material
Challenge	Material
.	O

This	O
is	O
the	O
version	O
of	O
ImageNet	Material
on	O
which	O
we	O
performed	O
our	O
experiments	O
.	O

In	O
all	O
,	O
there	O
are	O
roughly	O
1.3	O
million	O
training	O
images	O
,	O
50000	O
validation	O
images	O
,	O
and	O
150000	O
testing	O
images	O
.	O

This	O
dataset	O
is	O
similar	O
in	O
spirit	O
to	O
the	O
CIFAR	Material
-	Material
10	Material
,	O
but	O
on	O
a	O
much	O
bigger	O
scale	O
.	O

The	O
images	O
are	O
full	O
-	O
resolution	O
,	O
and	O
there	O
are	O
1000	O
categories	O
instead	O
of	O
ten	O
.	O

Another	O
difference	O
is	O
that	O
the	O
ImageNet	Material
images	O
often	O
contain	O
multiple	O
instances	O
of	O
ImageNet	Material
objects	O
,	O
simply	O
due	O
to	O
the	O
sheer	O
number	O
of	O
object	O
classes	O
.	O

For	O
this	O
reason	O
,	O
even	O
a	O
human	O
would	O
have	O
difficulty	O
approaching	O
perfect	O
accuracy	Metric
on	O
this	O
dataset	O
.	O

For	O
our	O
experiments	O
we	O
resized	O
all	O
images	O
to	O
pixels	O
.	O

appendix	O
:	O
Convolutional	Method
Neural	Method
Networks	Method
Our	O
models	O
for	O
CIFAR	Material
-	Material
10	Material
and	O
ImageNet	Material
are	O
deep	O
,	O
feed	O
-	O
forward	O
convolutional	O
neural	Method
networks	Method
(	O
CNNs	Method
)	O
.	O

Feed	O
-	O
forward	O
neural	Method
networks	Method
are	O
models	O
which	O
consist	O
of	O
several	O
layers	O
of	O
“	O
neurons	O
”	O
,	O
where	O
each	O
neuron	O
in	O
a	O
given	O
layer	O
applies	O
a	O
linear	Method
filter	Method
to	O
the	O
outputs	O
of	O
the	O
neurons	O
in	O
the	O
previous	O
layer	O
.	O

Typically	O
,	O
a	O
scalar	O
bias	O
is	O
added	O
to	O
the	O
filter	O
output	O
and	O
a	O
nonlinear	O
activation	O
function	O
is	O
applied	O
to	O
the	O
result	O
before	O
the	O
neuron	O
’s	O
output	O
is	O
passed	O
to	O
the	O
next	O
layer	O
.	O

The	O
linear	Method
filters	Method
and	O
biases	O
are	O
referred	O
to	O
as	O
weights	O
,	O
and	O
these	O
are	O
the	O
parameters	O
of	O
the	O
network	O
that	O
are	O
learned	O
from	O
the	O
training	O
data	O
.	O

CNNs	Method
differ	O
from	O
ordinary	O
neural	Method
networks	Method
in	O
several	O
ways	O
.	O

First	O
,	O
neurons	O
in	O
a	O
CNN	Method
are	O
organized	O
topographically	O
into	O
a	O
bank	O
that	O
reflects	O
the	O
organization	O
of	O
dimensions	O
in	O
the	O
input	O
data	O
.	O

So	O
for	O
images	O
,	O
the	O
neurons	O
are	O
laid	O
out	O
on	O
a	O
2D	O
grid	O
.	O

Second	O
,	O
neurons	O
in	O
a	O
CNN	Method
apply	O
filters	Method
which	O
are	O
local	O
in	O
extent	O
and	O
which	O
are	O
centered	O
at	O
the	O
neuron	O
’s	O
location	O
in	O
the	O
topographic	O
organization	O
.	O

This	O
is	O
reasonable	O
for	O
datasets	O
where	O
we	O
expect	O
the	O
dependence	O
of	O
input	O
dimensions	O
to	O
be	O
a	O
decreasing	O
function	O
of	O
distance	O
,	O
which	O
is	O
the	O
case	O
for	O
pixels	O
in	O
natural	O
images	O
.	O

In	O
particular	O
,	O
we	O
expect	O
that	O
useful	O
clues	O
to	O
the	O
identity	O
of	O
the	O
object	O
in	O
an	O
input	O
image	O
can	O
be	O
found	O
by	O
examining	O
small	O
local	O
neighborhoods	O
of	O
the	O
image	O
.	O

Third	O
,	O
all	O
neurons	O
in	O
a	O
bank	O
apply	O
the	O
same	O
filter	Method
,	O
but	O
as	O
just	O
mentioned	O
,	O
they	O
apply	O
it	O
at	O
different	O
locations	O
in	O
the	O
input	O
image	O
.	O

This	O
is	O
reasonable	O
for	O
datasets	O
with	O
roughly	O
stationary	O
statistics	O
,	O
such	O
as	O
natural	O
images	O
.	O

We	O
expect	O
that	O
the	O
same	O
kinds	O
of	O
structures	O
can	O
appear	O
at	O
all	O
positions	O
in	O
an	O
input	O
image	O
,	O
so	O
it	O
is	O
reasonable	O
to	O
treat	O
all	O
positions	O
equally	O
by	O
filtering	O
them	O
in	O
the	O
same	O
way	O
.	O

In	O
this	O
way	O
,	O
a	O
bank	Method
of	Method
neurons	Method
in	O
a	O
CNN	Method
applies	O
a	O
convolution	Method
operation	Method
to	O
its	O
input	O
.	O

A	O
single	O
layer	O
in	O
a	O
CNN	Method
typically	O
has	O
multiple	O
banks	Method
of	Method
neurons	Method
,	O
each	O
performing	O
a	O
convolution	Method
with	O
a	O
different	O
filter	O
.	O

These	O
banks	O
of	O
neurons	O
become	O
distinct	O
input	O
channels	O
into	O
the	O
next	O
layer	O
.	O

The	O
distance	O
,	O
in	O
pixels	O
,	O
between	O
the	O
boundaries	O
of	O
the	O
receptive	O
fields	O
of	O
neighboring	O
neurons	O
in	O
a	O
convolutional	Method
bank	Method
determines	O
the	O
stride	O
with	O
which	O
the	O
convolution	Method
operation	Method
is	O
applied	O
.	O

Larger	O
strides	O
imply	O
fewer	O
neurons	O
per	O
bank	O
.	O

Our	O
models	O
use	O
a	O
stride	O
of	O
one	O
pixel	O
unless	O
otherwise	O
noted	O
.	O

One	O
important	O
consequence	O
of	O
this	O
convolutional	Method
shared	Method
-	Method
filter	Method
architecture	Method
is	O
a	O
drastic	O
reduction	O
in	O
the	O
number	O
of	O
parameters	O
relative	O
to	O
a	O
neural	Method
net	Method
in	O
which	O
all	O
neurons	O
apply	O
different	O
filters	O
.	O

This	O
reduces	O
the	O
net	O
’s	O
representational	O
capacity	O
,	O
but	O
it	O
also	O
reduces	O
its	O
capacity	O
to	O
overfit	O
,	O
so	O
dropout	Method
is	O
far	O
less	O
advantageous	O
in	O
convolutional	Method
layers	Method
.	O

subsection	O
:	O
Pooling	O
CNNs	Method
typically	O
also	O
feature	O
“	O
pooling	Method
”	Method
layers	Method
which	O
summarize	O
the	O
activities	O
of	O
local	O
patches	O
of	O
neurons	O
in	O
convolutional	Method
layers	Method
.	O

Essentially	O
,	O
a	O
pooling	Method
layer	Method
takes	O
as	O
input	O
the	O
output	O
of	O
a	O
convolutional	Method
layer	Method
and	O
subsamples	O
it	O
.	O

A	O
pooling	Method
layer	Method
consists	O
of	O
pooling	Method
units	Method
which	O
are	O
laid	O
out	O
topographically	O
and	O
connected	O
to	O
a	O
local	O
neighborhood	O
of	O
convolutional	O
unit	O
outputs	O
from	O
the	O
same	O
bank	O
.	O

Each	O
pooling	Method
unit	Method
then	O
computes	O
some	O
function	O
of	O
the	O
bank	O
’s	O
output	O
in	O
that	O
neighborhood	O
.	O

Typical	O
functions	O
are	O
maximum	O
and	O
average	O
.	O

Pooling	Method
layers	Method
with	O
such	O
units	O
are	O
called	O
max	Method
-	Method
pooling	Method
and	Method
average	Method
-	Method
pooling	Method
layers	Method
,	O
respectively	O
.	O

The	O
pooling	Method
units	Method
are	O
usually	O
spaced	O
at	O
least	O
several	O
pixels	O
apart	O
,	O
so	O
that	O
there	O
are	O
fewer	O
total	O
pooling	O
units	O
than	O
there	O
are	O
convolutional	O
unit	O
outputs	O
in	O
the	O
previous	O
layer	O
.	O

Making	O
this	O
spacing	O
smaller	O
than	O
the	O
size	O
of	O
the	O
neighborhood	O
that	O
the	O
pooling	O
units	O
summarize	O
produces	O
overlapping	O
pooling	O
.	O

This	O
variant	O
makes	O
the	O
pooling	Method
layer	Method
produce	O
a	O
coarse	Method
coding	Method
of	O
the	O
convolutional	O
unit	O
outputs	O
,	O
which	O
we	O
have	O
found	O
to	O
aid	O
generalization	Task
in	O
our	O
experiments	O
.	O

We	O
refer	O
to	O
this	O
spacing	O
as	O
the	O
stride	O
between	O
pooling	O
units	O
,	O
analogously	O
to	O
the	O
stride	O
between	O
convolutional	O
units	O
.	O

Pooling	Method
layers	Method
introduce	O
a	O
level	O
of	O
local	O
translation	O
invariance	O
to	O
the	O
network	O
,	O
which	O
improves	O
generalization	Task
.	O

They	O
are	O
the	O
analogues	O
of	O
complex	O
cells	O
in	O
the	O
mammalian	O
visual	O
cortex	O
,	O
which	O
pool	O
activities	O
of	O
multiple	O
simple	O
cells	O
.	O

These	O
cells	O
are	O
known	O
to	O
exhibit	O
similar	O
phase	O
-	O
invariance	O
properties	O
.	O

subsection	O
:	O
Local	Method
response	Method
normalization	Method
Our	O
networks	O
also	O
include	O
response	Method
normalization	Method
layers	Method
.	O

This	O
type	O
of	O
layer	O
encourages	O
competition	O
for	O
large	O
activations	O
among	O
neurons	O
belonging	O
to	O
different	O
banks	O
.	O

In	O
particular	O
,	O
the	O
activity	O
of	O
a	O
neuron	O
in	O
bank	O
at	O
position	O
in	O
the	O
topographic	O
organization	O
is	O
divided	O
by	O
where	O
the	O
sum	O
runs	O
over	O
“	O
adjacent	O
”	O
banks	O
of	O
neurons	O
at	O
the	O
same	O
position	O
in	O
the	O
topographic	O
organization	O
.	O

The	O
ordering	O
of	O
the	O
banks	O
is	O
of	O
course	O
arbitrary	O
and	O
determined	O
before	O
training	O
begins	O
.	O

Response	Method
normalization	Method
layers	Method
implement	O
a	O
form	O
of	O
lateral	O
inhibition	O
found	O
in	O
real	O
neurons	O
.	O

The	O
constants	O
,	O
and	O
are	O
hyper	O
-	O
parameters	O
whose	O
values	O
are	O
determined	O
using	O
a	O
validation	O
set	O
.	O

subsection	O
:	O
Neuron	Method
nonlinearities	Method
All	O
of	O
the	O
neurons	O
in	O
our	O
networks	O
utilize	O
the	O
max	Method
-	Method
with	Method
-	Method
zero	Method
nonlinearity	Method
.	O

That	O
is	O
,	O
their	O
output	O
is	O
computed	O
as	O
where	O
is	O
the	O
total	O
input	O
to	O
the	O
neuron	O
(	O
equivalently	O
,	O
the	O
output	O
of	O
the	O
neuron	Method
’s	Method
linear	Method
filter	Method
added	O
to	O
the	O
bias	O
)	O
.	O

This	O
nonlinearity	O
has	O
several	O
advantages	O
over	O
traditional	O
saturating	Method
neuron	Method
models	Method
,	O
including	O
a	O
significant	O
reduction	O
in	O
the	O
training	Metric
time	Metric
required	O
to	O
reach	O
a	O
given	O
error	Metric
rate	Metric
.	O

This	O
nonlinearity	O
also	O
reduces	O
the	O
need	O
for	O
contrast	Task
-	Task
normalization	Task
and	O
similar	O
data	O
pre	Method
-	Method
processing	Method
schemes	Method
,	O
because	O
neurons	O
with	O
this	O
nonlinearity	O
do	O
not	O
saturate	O
–	O
their	O
activities	O
simply	O
scale	O
up	O
when	O
presented	O
with	O
unusually	O
large	O
input	O
values	O
.	O

Consequently	O
,	O
the	O
only	O
data	O
pre	O
-	O
processing	O
step	O
which	O
we	O
take	O
is	O
to	O
subtract	O
the	O
mean	O
activity	O
from	O
each	O
pixel	O
,	O
so	O
that	O
the	O
data	O
is	O
centered	O
.	O

So	O
we	O
train	O
our	O
networks	O
on	O
the	O
(	O
centered	O
)	O
raw	O
RGB	O
values	O
of	O
the	O
pixels	O
.	O

subsection	O
:	O
Objective	Metric
function	Metric
Our	O
networks	O
maximize	O
the	O
multinomial	Task
logistic	Task
regression	Task
objective	Task
,	O
which	O
is	O
equivalent	O
to	O
minimizing	O
the	O
average	O
across	O
training	O
cases	O
of	O
the	O
cross	Metric
-	Metric
entropy	Metric
between	O
the	O
true	O
label	O
distribution	O
and	O
the	O
model	O
’s	O
predicted	O
label	O
distribution	O
.	O

subsection	O
:	O
Weight	Method
initialization	Method
We	O
initialize	O
the	O
weights	O
in	O
our	O
model	O
from	O
a	O
zero	Method
-	Method
mean	Method
normal	Method
distribution	Method
with	O
a	O
variance	O
set	O
high	O
enough	O
to	O
produce	O
positive	O
inputs	O
into	O
the	O
neurons	O
in	O
each	O
layer	O
.	O

This	O
is	O
a	O
slightly	O
tricky	O
point	O
when	O
using	O
the	O
max	Method
-	Method
with	Method
-	Method
zero	Method
nonlinearity	Method
.	O

If	O
the	O
input	O
to	O
a	O
neuron	O
is	O
always	O
negative	O
,	O
no	O
learning	O
will	O
take	O
place	O
because	O
its	O
output	O
will	O
be	O
uniformly	O
zero	O
,	O
as	O
will	O
the	O
derivative	O
of	O
its	O
output	O
with	O
respect	O
to	O
its	O
input	O
.	O

Therefore	O
it	O
’s	O
important	O
to	O
initialize	O
the	O
weights	O
from	O
a	O
distribution	O
with	O
a	O
sufficiently	O
large	O
variance	O
such	O
that	O
all	O
neurons	O
are	O
likely	O
to	O
get	O
positive	O
inputs	O
at	O
least	O
occasionally	O
.	O

In	O
practice	O
,	O
we	O
simply	O
try	O
different	O
variances	O
until	O
we	O
find	O
an	O
initialization	O
that	O
works	O
.	O

It	O
usually	O
only	O
takes	O
a	O
few	O
attempts	O
.	O

We	O
also	O
find	O
that	O
initializing	O
the	O
biases	O
of	O
the	O
neurons	O
in	O
the	O
hidden	O
layers	O
with	O
some	O
positive	O
constant	O
(	O
1	O
in	O
our	O
case	O
)	O
helps	O
get	O
learning	O
off	O
the	O
ground	O
,	O
for	O
the	O
same	O
reason	O
.	O

subsection	O
:	O
Training	O
We	O
train	O
our	O
models	O
using	O
stochastic	Method
gradient	Method
descent	Method
with	O
a	O
batch	O
size	O
of	O
128	O
examples	O
and	O
momentum	O
of	O
0.9	O
.	O

Therefore	O
the	O
update	Method
rule	Method
for	O
weight	O
is	O
where	O
is	O
the	O
iteration	O
index	O
,	O
is	O
a	O
momentum	O
variable	O
,	O
is	O
the	O
learning	Metric
rate	Metric
,	O
and	O
is	O
the	O
average	O
over	O
the	O
batch	O
of	O
the	O
derivative	O
of	O
the	O
objective	O
with	O
respect	O
to	O
.	O

We	O
use	O
the	O
publicly	O
available	O
cuda	Method
-	Method
convnet	Method
package	Method
to	O
train	O
all	O
of	O
our	O
models	O
on	O
a	O
single	O
NVIDIA	O
GTX	O
580	O
GPU	O
.	O

Training	O
on	O
CIFAR	Material
-	Material
10	Material
takes	O
roughly	O
90	O
minutes	O
.	O

Training	O
on	O
ImageNet	Material
takes	O
roughly	O
four	O
days	O
with	O
dropout	Method
and	O
two	O
days	O
without	O
.	O

subsection	O
:	O
Learning	Metric
rates	Metric
We	O
use	O
an	O
equal	O
learning	Metric
rate	Metric
for	O
each	O
layer	O
,	O
whose	O
value	O
we	O
determine	O
heuristically	O
as	O
the	O
largest	O
power	O
of	O
ten	O
that	O
produces	O
reductions	O
in	O
the	O
objective	Metric
function	Metric
.	O

In	O
practice	O
it	O
is	O
typically	O
of	O
the	O
order	O
or	O
.	O

We	O
reduce	O
the	O
learning	Metric
rate	Metric
twice	O
by	O
a	O
factor	O
of	O
ten	O
shortly	O
before	O
terminating	O
training	O
.	O

appendix	O
:	O
Models	O
for	O
CIFAR	Material
-	Material
10	Material
Our	O
model	O
for	O
CIFAR	Material
-	Material
10	Material
without	Material
dropout	Material
is	O
a	O
CNN	Method
with	O
three	O
convolutional	Method
layers	Method
.	O

Pooling	Method
layers	Method
follow	O
all	O
three	O
.	O

All	O
of	O
the	O
pooling	Method
layers	Method
summarize	O
a	O
neighborhood	O
and	O
use	O
a	O
stride	O
of	O
2	O
.	O

The	O
pooling	Method
layer	Method
which	O
follows	O
the	O
first	O
convolutional	Method
layer	Method
performs	O
max	Method
-	Method
pooling	Method
,	O
while	O
the	O
remaining	O
pooling	Method
layers	Method
perform	O
average	Method
-	Method
pooling	Method
.	O

Response	Method
normalization	Method
layers	Method
follow	O
the	O
first	O
two	O
pooling	Method
layers	Method
,	O
with	O
,	O
,	O
and	O
.	O

The	O
upper	O
-	O
most	O
pooling	Method
layer	Method
is	O
connected	O
to	O
a	O
ten	O
-	O
unit	Method
softmax	Method
layer	Method
which	O
outputs	O
a	O
probability	O
distribution	O
over	O
class	O
labels	O
.	O

All	O
convolutional	Method
layers	Method
have	O
64	O
filter	Method
banks	Method
and	O
use	O
a	O
filter	O
size	O
of	O
(	O
times	O
the	O
number	O
of	O
channels	O
in	O
the	O
preceding	O
layer	O
)	O
.	O

Our	O
model	O
for	O
CIFAR	Material
-	Material
10	Material
with	Material
dropout	Material
is	O
similar	O
,	O
but	O
because	O
dropout	Method
imposes	O
a	O
strong	O
regularization	O
on	O
the	O
network	O
,	O
we	O
are	O
able	O
to	O
use	O
more	O
parameters	O
.	O

Therefore	O
we	O
add	O
a	O
fourth	Method
weight	Method
layer	Method
,	O
which	O
takes	O
its	O
input	O
from	O
the	O
third	O
pooling	Method
layer	Method
.	O

This	O
weight	Method
layer	Method
is	O
locally	O
-	O
connected	O
but	O
not	O
convolutional	Method
.	O

It	O
is	O
like	O
a	O
convolutional	Method
layer	Method
in	O
which	O
filters	O
in	O
the	O
same	O
bank	O
do	O
not	O
share	O
weights	O
.	O

This	O
layer	O
contains	O
16	O
banks	O
of	O
filters	O
of	O
size	O
.	O

This	O
is	O
the	O
layer	O
in	O
which	O
we	O
use	O
50	O
%	O
dropout	Method
.	O

The	O
softmax	Method
layer	Method
takes	O
its	O
input	O
from	O
this	O
fourth	Method
weight	Method
layer	Method
.	O

appendix	O
:	O
Models	O
for	O
ImageNet	Material
Our	O
model	O
for	O
ImageNet	Material
with	Material
dropout	Material
is	O
a	O
CNN	Method
which	O
is	O
trained	O
on	O
patches	O
randomly	O
extracted	O
from	O
the	O
images	O
,	O
as	O
well	O
as	O
their	O
horizontal	O
reflections	O
.	O

This	O
is	O
a	O
form	O
of	O
data	Task
augmentation	Task
that	O
reduces	O
the	O
network	O
’s	O
capacity	O
to	O
overfit	O
the	O
training	O
data	O
and	O
helps	O
generalization	Task
.	O

The	O
network	O
contains	O
seven	O
weight	O
layers	O
.	O

The	O
first	O
five	O
are	O
convolutional	Method
,	O
while	O
the	O
last	O
two	O
are	O
globally	O
-	O
connected	O
.	O

Max	Method
-	Method
pooling	Method
layers	Method
follow	O
the	O
first	O
,	O
second	O
,	O
and	O
fifth	O
convolutional	Method
layers	Method
.	O

All	O
of	O
the	O
pooling	Method
layers	Method
summarize	O
a	O
neighborhood	O
and	O
use	O
a	O
stride	O
of	O
2	O
.	O

Response	Method
-	Method
normalization	Method
layers	Method
follow	O
the	O
first	O
and	O
second	O
pooling	Method
layers	Method
.	O

The	O
first	O
convolutional	Method
layer	Method
has	O
64	O
filter	Method
banks	Method
with	O
filters	O
which	O
it	O
applies	O
with	O
a	O
stride	O
of	O
4	O
pixels	O
(	O
this	O
is	O
the	O
distance	O
between	O
neighboring	O
neurons	O
in	O
a	O
bank	O
)	O
.	O

The	O
second	O
convolutional	Method
layer	Method
has	O
256	O
filter	Method
banks	Method
with	O
filters	Method
.	O

This	O
layer	O
takes	O
two	O
inputs	O
.	O

The	O
first	O
input	O
to	O
this	O
layer	O
is	O
the	O
(	O
pooled	O
and	O
response	O
-	O
normalized	O
)	O
output	O
of	O
the	O
first	Method
convolutional	Method
layer	Method
.	O

The	O
256	O
banks	O
in	O
this	O
layer	O
are	O
divided	O
arbitrarily	O
into	O
groups	O
of	O
64	O
,	O
and	O
each	O
group	O
connects	O
to	O
a	O
unique	O
random	O
16	O
channels	O
from	O
the	O
first	O
convolutional	Method
layer	Method
.	O

The	O
second	O
input	O
to	O
this	O
layer	O
is	O
a	O
subsampled	Method
version	Method
of	O
the	O
original	O
image	O
(	O
)	O
,	O
which	O
is	O
filtered	O
by	O
this	O
layer	O
with	O
a	O
stride	O
of	O
2	O
pixels	O
.	O

The	O
two	O
maps	O
resulting	O
from	O
filtering	O
the	O
two	O
inputs	O
are	O
summed	O
element	O
-	O
wise	O
(	O
they	O
have	O
exactly	O
the	O
same	O
dimensions	O
)	O
and	O
a	O
max	Method
-	Method
with	Method
-	Method
zero	Method
nonlinearity	Method
is	O
applied	O
to	O
the	O
sum	O
in	O
the	O
usual	O
way	O
.	O

The	O
third	O
,	O
fourth	O
,	O
and	O
fifth	O
convolutional	Method
layers	Method
are	O
connected	O
to	O
one	O
another	O
without	O
any	O
intervening	O
pooling	O
or	O
normalization	O
layers	O
,	O
but	O
the	O
max	O
-	O
with	O
-	O
zero	O
nonlinearity	O
is	O
applied	O
at	O
each	O
layer	O
after	O
linear	Method
filtering	Method
.	O

The	O
third	O
convolutional	Method
layer	Method
has	O
512	O
filter	Method
banks	Method
divided	O
into	O
groups	O
of	O
32	O
,	O
each	O
group	O
connecting	O
to	O
a	O
unique	O
random	O
subset	O
of	O
16	O
channels	O
produced	O
by	O
the	O
(	O
pooled	O
,	O
normalized	O
)	O
outputs	O
of	O
the	O
second	O
convolutional	Method
layer	Method
.	O

The	O
fourth	O
and	O
fifth	O
convolutional	Method
layers	Method
similarly	O
have	O
512	O
filter	Method
banks	Method
divided	O
into	O
groups	O
of	O
32	O
,	O
each	O
group	O
connecting	O
to	O
a	O
unique	O
random	O
subset	O
of	O
32	O
channels	O
produced	O
by	O
the	O
layer	O
below	O
.	O

The	O
next	O
two	O
weight	Method
layers	Method
are	O
globally	O
-	O
connected	O
,	O
with	O
4096	O
neurons	O
each	O
.	O

In	O
these	O
last	O
two	O
layers	O
we	O
use	O
50	O
%	O
dropout	Method
.	O

Finally	O
,	O
the	O
output	O
of	O
the	O
last	O
globally	Method
-	Method
connected	Method
layer	Method
is	O
fed	O
to	O
a	O
1000	Method
-	Method
way	Method
softmax	Method
which	O
produces	O
a	O
distribution	O
over	O
the	O
1000	O
class	O
labels	O
.	O

We	O
test	O
our	O
model	O
by	O
averaging	O
the	O
prediction	O
of	O
the	O
net	O
on	O
ten	O
patches	O
of	O
the	O
input	O
image	O
:	O
the	O
center	O
patch	O
,	O
the	O
four	O
corner	O
patches	O
,	O
and	O
their	O
horizontal	O
reflections	O
.	O

Even	O
though	O
we	O
make	O
ten	O
passes	O
of	O
each	O
image	O
at	O
test	O
time	O
,	O
we	O
are	O
able	O
to	O
run	O
our	O
system	O
in	O
real	O
-	O
time	O
.	O

Our	O
model	O
for	O
ImageNet	Material
without	O
dropout	O
is	O
similar	O
,	O
but	O
without	O
the	O
two	O
globally	O
-	O
connected	O
layers	O
which	O
create	O
serious	O
overfitting	O
when	O
used	O
without	O
dropout	Method
.	O

In	O
order	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
validation	O
set	O
,	O
we	O
found	O
it	O
necessary	O
to	O
use	O
the	O
very	O
complicated	O
network	Method
architecture	Method
described	O
above	O
.	O

Fortunately	O
,	O
the	O
complexity	O
of	O
this	O
architecture	O
is	O
not	O
the	O
main	O
point	O
of	O
our	O
paper	O
.	O

What	O
we	O
wanted	O
to	O
demonstrate	O
is	O
that	O
dropout	Method
is	O
a	O
significant	O
help	O
even	O
for	O
the	O
very	O
complex	O
neural	Method
nets	Method
that	O
have	O
been	O
developed	O
by	O
the	O
joint	O
efforts	O
of	O
many	O
groups	O
over	O
many	O
years	O
to	O
be	O
really	O
good	O
at	O
object	Task
recognition	Task
.	O

This	O
is	O
clearly	O
demonstrated	O
by	O
the	O
fact	O
that	O
using	O
non	Method
-	Method
convolutional	Method
higher	Method
layers	Method
with	O
a	O
lot	O
of	O
parameters	O
leads	O
to	O
a	O
big	O
improvement	O
with	O
dropout	Method
but	O
makes	O
things	O
worse	O
without	O
dropout	Method
.	O

