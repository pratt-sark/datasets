document	O
:	O
Convolutional	Method
Neural	Method
Network	Method
Architectures	Method
for	O
Matching	Task
Natural	Task
Language	Task
Sentences	Task
Semantic	Task
matching	Task
is	O
of	O
central	O
importance	O
to	O
many	O
natural	Task
language	Task
tasks	Task
.	O

A	O
successful	O
matching	Method
algorithm	Method
needs	O
to	O
adequately	O
model	O
the	O
internal	O
structures	O
of	O
language	O
objects	O
and	O
the	O
interaction	O
between	O
them	O
.	O

As	O
a	O
step	O
toward	O
this	O
goal	O
,	O
we	O
propose	O
convolutional	Method
neural	Method
network	Method
models	Method
for	O
matching	O
two	O
sentences	O
,	O
by	O
adapting	O
the	O
convolutional	Method
strategy	Method
in	O
vision	Task
and	O
speech	Task
.	O

The	O
proposed	O
models	O
not	O
only	O
nicely	O
represent	O
the	O
hierarchical	O
structures	O
of	O
sentences	O
with	O
their	O
layer	Method
-	Method
by	Method
-	Method
layer	Method
composition	Method
and	O
pooling	Method
,	O
but	O
also	O
capture	O
the	O
rich	O
matching	O
patterns	O
at	O
different	O
levels	O
.	O

Our	O
models	O
are	O
rather	O
generic	O
,	O
requiring	O
no	O
prior	O
knowledge	O
on	O
language	O
,	O
and	O
can	O
hence	O
be	O
applied	O
to	O
matching	Task
tasks	Task
of	O
different	O
nature	O
and	O
in	O
different	O
languages	O
.	O

The	O
empirical	O
study	O
on	O
a	O
variety	O
of	O
matching	Task
tasks	Task
demonstrates	O
the	O
efficacy	O
of	O
the	O
proposed	O
model	O
on	O
a	O
variety	O
of	O
matching	Task
tasks	Task
and	O
its	O
superiority	O
to	O
competitor	O
models	O
.	O

section	O
:	O
Introduction	O
Matching	Task
two	O
potentially	O
heterogenous	O
language	O
objects	O
is	O
central	O
to	O
many	O
natural	Task
language	Task
applications	Task
.	O

It	O
generalizes	O
the	O
conventional	O
notion	O
of	O
similarity	O
(	O
e.g.	O
,	O
in	O
paraphrase	Task
identification	Task
)	O
or	O
relevance	Task
(	O
e.g.	O
,	O
in	O
information	Task
retrieval	Task
)	O
,	O
since	O
it	O
aims	O
to	O
model	O
the	O
correspondence	O
between	O
“	O
linguistic	O
objects	O
”	O
of	O
different	O
nature	O
at	O
different	O
levels	O
of	O
abstractions	O
.	O

Examples	O
include	O
top	Task
-	Task
re	Task
-	Task
ranking	Task
in	O
machine	Task
translation	Task
(	O
e.g.	O
,	O
comparing	O
the	O
meanings	O
of	O
a	O
French	O
sentence	O
and	O
an	O
English	O
sentence	O
)	O
and	O
dialogue	O
(	O
e.g.	O
,	O
evaluating	O
the	O
appropriateness	O
of	O
a	O
response	O
to	O
a	O
given	O
utterance	O
)	O
.	O

Natural	O
language	O
sentences	O
have	O
complicated	O
structures	O
,	O
both	O
sequential	O
and	O
hierarchical	O
,	O
that	O
are	O
essential	O
for	O
understanding	O
them	O
.	O

A	O
successful	O
sentence	Method
-	Method
matching	Method
algorithm	Method
therefore	O
needs	O
to	O
capture	O
not	O
only	O
the	O
internal	O
structures	O
of	O
sentences	O
but	O
also	O
the	O
rich	O
patterns	O
in	O
their	O
interactions	O
.	O

Towards	O
this	O
end	O
,	O
we	O
propose	O
deep	Method
neural	Method
network	Method
models	Method
,	O
which	O
adapt	O
the	O
convolutional	Method
strategy	Method
(	O
proven	O
successful	O
on	O
image	Task
and	O
speech	Task
)	O
to	O
natural	Task
language	Task
.	O

To	O
further	O
explore	O
the	O
relation	O
between	O
representing	O
sentences	O
and	O
matching	O
them	O
,	O
we	O
devise	O
a	O
novel	O
model	O
that	O
can	O
naturally	O
host	O
both	O
the	O
hierarchical	Method
composition	Method
for	O
sentences	O
and	O
the	O
simple	O
-	O
to	O
-	O
comprehensive	Method
fusion	Method
of	Method
matching	Method
patterns	Method
with	O
the	O
same	O
convolutional	Method
architecture	Method
.	O

Our	O
model	O
is	O
generic	O
,	O
requiring	O
no	O
prior	O
knowledge	O
of	O
natural	O
language	O
(	O
e.g.	O
,	O
parse	O
tree	O
)	O
and	O
putting	O
essentially	O
no	O
constraints	O
on	O
the	O
matching	Task
tasks	Task
.	O

This	O
is	O
part	O
of	O
our	O
continuing	O
effort	O
in	O
understanding	O
natural	Task
language	Task
objects	Task
and	O
the	O
matching	Task
between	O
them	O
.	O

Our	O
main	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
.	O

First	O
,	O
we	O
devise	O
novel	O
deep	Method
convolutional	Method
network	Method
architectures	Method
that	O
can	O
naturally	O
combine	O
1	O
)	O
the	O
hierarchical	Method
sentence	Method
modeling	Method
through	O
layer	Method
-	Method
by	Method
-	Method
layer	Method
composition	Method
and	O
pooling	Method
,	O
and	O
2	O
)	O
the	O
capturing	O
of	O
the	O
rich	O
matching	O
patterns	O
at	O
different	O
levels	O
of	O
abstraction	O
;	O
Second	O
,	O
we	O
perform	O
extensive	O
empirical	O
study	O
on	O
tasks	O
with	O
different	O
scales	O
and	O
characteristics	O
,	O
and	O
demonstrate	O
the	O
superior	O
power	O
of	O
the	O
proposed	O
architectures	O
over	O
competitor	O
methods	O
.	O

paragraph	O
:	O
Roadmap	O
We	O
start	O
by	O
introducing	O
a	O
convolution	Method
network	Method
in	O
Section	O
[	O
reference	O
]	O
as	O
the	O
basic	O
architecture	O
for	O
sentence	Task
modeling	Task
,	O
and	O
how	O
it	O
is	O
related	O
to	O
existing	O
sentence	Method
models	Method
.	O

Based	O
on	O
that	O
,	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
propose	O
two	O
architectures	O
for	O
sentence	Task
matching	Task
,	O
with	O
a	O
detailed	O
discussion	O
of	O
their	O
relation	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
briefly	O
discuss	O
the	O
learning	O
of	O
the	O
proposed	O
architectures	O
.	O

Then	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
report	O
our	O
empirical	O
study	O
,	O
followed	O
by	O
a	O
brief	O
discussion	O
of	O
related	O
work	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Convolutional	Method
Sentence	Method
Model	Method
We	O
start	O
with	O
proposing	O
a	O
new	O
convolutional	Method
architecture	Method
for	O
modeling	Task
sentences	Task
.	O

As	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
it	O
takes	O
as	O
input	O
the	O
embedding	O
of	O
words	O
(	O
often	O
trained	O
beforehand	O
with	O
unsupervised	Method
methods	Method
)	O
in	O
the	O
sentence	O
aligned	O
sequentially	O
,	O
and	O
summarize	O
the	O
meaning	O
of	O
a	O
sentence	O
through	O
layers	O
of	O
convolution	Method
and	O
pooling	Method
,	O
until	O
reaching	O
a	O
fixed	Method
length	Method
vectorial	Method
representation	Method
in	O
the	O
final	O
layer	O
.	O

As	O
in	O
most	O
convolutional	Method
models	Method
,	O
we	O
use	O
convolution	Method
units	Method
with	O
a	O
local	O
“	O
receptive	O
field	O
”	O
and	O
shared	O
weights	O
,	O
but	O
we	O
design	O
a	O
large	O
feature	O
map	O
to	O
adequately	O
model	O
the	O
rich	O
structures	O
in	O
the	O
composition	O
of	O
words	O
.	O

paragraph	O
:	O
Convolution	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
convolution	Method
in	Method
Layer	Method
-	Method
1	Method
operates	O
on	O
sliding	O
windows	O
of	O
words	O
(	O
width	O
)	O
,	O
and	O
the	O
convolutions	O
in	O
deeper	O
layers	O
are	O
defined	O
in	O
a	O
similar	O
way	O
.	O

Generally	O
,	O
with	O
sentence	Method
input	Method
,	O
the	O
convolution	Method
unit	Method
for	O
feature	Method
map	Method
of	O
type	O
-	O
(	O
among	O
of	O
them	O
)	O
on	O
Layer	O
-	O
is	O
and	O
its	O
matrix	O
form	O
is	O
,	O
where	O
gives	O
the	O
output	O
of	O
feature	O
map	O
of	O
type	O
-	O
for	O
location	O
in	O
Layer	O
-	O
;	O
is	O
the	O
parameters	O
for	O
on	O
Layer	O
-	O
,	O
with	O
matrix	O
form	O
;	O
is	O
the	O
activation	O
function	O
(	O
e.g.	O
,	O
Sigmoid	Method
or	O
Relu	Method
)	O
denotes	O
the	O
segment	O
of	O
Layer	O
-	O
for	O
the	O
convolution	O
at	O
location	O
,	O
while	O
concatenates	O
the	O
vectors	O
for	O
(	O
width	O
of	O
sliding	O
window	O
)	O
words	O
from	O
sentence	O
input	O
.	O

paragraph	O
:	O
Max	Method
-	Method
Pooling	Method
We	O
take	O
a	O
max	Method
-	Method
pooling	Method
in	O
every	O
two	O
-	O
unit	O
window	O
for	O
every	O
,	O
after	O
each	O
convolution	O
The	O
effects	O
of	O
pooling	Method
are	O
two	O
-	O
fold	O
:	O
1	O
)	O
it	O
shrinks	O
the	O
size	O
of	O
the	O
representation	O
by	O
half	O
,	O
thus	O
quickly	O
absorbs	O
the	O
differences	O
in	O
length	O
for	O
sentence	Task
representation	Task
,	O
and	O
2	O
)	O
it	O
filters	O
out	O
undesirable	O
composition	O
of	O
words	O
(	O
see	O
Section	O
[	O
reference	O
]	O
for	O
some	O
analysis	O
)	O
.	O

paragraph	O
:	O
Length	O
Variability	O
The	O
variable	O
length	O
of	O
sentences	O
in	O
a	O
fairly	O
broad	O
range	O
can	O
be	O
readily	O
handled	O
with	O
the	O
convolution	Method
and	Method
pooling	Method
strategy	Method
.	O

More	O
specifically	O
,	O
we	O
put	O
all	O
-	O
zero	O
padding	O
vectors	O
after	O
the	O
last	O
word	O
of	O
the	O
sentence	O
until	O
the	O
maximum	O
length	O
.	O

To	O
eliminate	O
the	O
boundary	O
effect	O
caused	O
by	O
the	O
great	O
variability	O
of	O
sentence	O
lengths	O
,	O
we	O
add	O
to	O
the	O
convolutional	Method
unit	Method
a	O
gate	O
which	O
sets	O
the	O
output	O
vectors	O
to	O
all	O
-	O
zeros	O
if	O
the	O
input	O
is	O
all	O
zeros	O
.	O

For	O
any	O
given	O
sentence	O
input	O
,	O
the	O
output	O
of	O
type	Method
-	Method
filter	Method
for	O
location	O
in	O
the	O
layer	O
is	O
given	O
by	O
where	O
if	O
all	O
the	O
elements	O
in	O
vector	O
equals	O
0	O
,	O
otherwise	O
.	O

This	O
gate	O
,	O
working	O
with	O
max	Method
-	Method
pooling	Method
and	O
positive	O
activation	O
function	O
(	O
e.g.	O
,	O
Sigmoid	O
)	O
,	O
keeps	O
away	O
the	O
artifacts	O
from	O
padding	O
in	O
all	O
layers	O
.	O

Actually	O
it	O
creates	O
a	O
natural	O
hierarchy	O
of	O
all	O
-	O
zero	O
padding	O
(	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
consisting	O
of	O
nodes	O
in	O
the	O
neural	Method
net	Method
that	O
would	O
not	O
contribute	O
in	O
the	O
forward	O
process	O
(	O
as	O
in	O
prediction	Task
)	O
and	O
backward	Task
propagation	Task
(	O
as	O
in	O
learning	Task
)	O
.	O

subsection	O
:	O
Some	O
Analysis	O
on	O
the	O
Convolutional	Method
Architecture	Method
The	O
convolutional	Method
unit	Method
,	O
when	O
combined	O
with	O
max	Method
-	Method
pooling	Method
,	O
can	O
act	O
as	O
the	O
compositional	O
operator	O
with	O
local	Method
selection	Method
mechanism	Method
as	O
in	O
the	O
recursive	Method
autoencoder	Method
.	O

Figure	O
[	O
reference	O
]	O
gives	O
an	O
example	O
on	O
what	O
could	O
happen	O
on	O
the	O
first	O
two	O
layers	O
with	O
input	O
sentence	O
“	O
The	O
cat	O
sat	O
on	O
the	O
mat	O
”	O
.	O

Just	O
for	O
illustration	O
purpose	O
,	O
we	O
present	O
a	O
dramatic	O
choice	O
of	O
parameters	O
(	O
by	O
turning	O
off	O
some	O
elements	O
in	O
)	O
to	O
make	O
the	O
convolution	Method
units	Method
focus	O
on	O
different	O
segments	O
within	O
a	O
3	O
-	O
word	O
window	O
.	O

For	O
example	O
,	O
some	O
feature	O
maps	O
(	O
group	O
2	O
)	O
give	O
compositions	O
for	O
“	O
the	O
cat	O
”	O
and	O
“	O
cat	O
sat	O
”	O
,	O
each	O
being	O
a	O
vector	O
.	O

Different	O
feature	Method
maps	Method
offer	O
a	O
variety	O
of	O
compositions	O
,	O
with	O
confidence	O
encoded	O
in	O
the	O
values	O
(	O
color	O
coded	O
in	O
output	O
of	O
convolution	Method
layer	Method
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
pooling	O
then	O
chooses	O
,	O
for	O
each	O
composition	O
type	O
,	O
between	O
two	O
adjacent	O
sliding	O
windows	O
,	O
e.g.	O
,	O
between	O
“	O
on	O
the	O
”	O
and	O
“	O
the	O
mat	O
”	O
for	O
feature	O
maps	O
group	O
2	O
from	O
the	O
rightmost	O
two	O
sliding	O
windows	O
.	O

paragraph	O
:	O
Relation	O
to	O
Recursive	Method
Models	Method
Our	O
convolutional	Method
model	Method
differs	O
from	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
,	O
)	O
and	O
Recursive	Method
Auto	Method
-	Method
Encoder	Method
(	O
RAE	Method
,	O
)	O
in	O
several	O
important	O
ways	O
.	O

First	O
,	O
unlike	O
RAE	Method
,	O
it	O
does	O
not	O
take	O
a	O
single	O
path	O
of	O
word	O
/	O
phrase	O
composition	O
determined	O
either	O
by	O
a	O
separate	O
gating	Method
function	Method
,	O
an	O
external	Method
parser	Method
,	O
or	O
just	O
natural	O
sequential	O
order	O
.	O

Instead	O
,	O
it	O
takes	O
multiple	O
choices	O
of	O
composition	O
via	O
a	O
large	O
feature	O
map	O
(	O
encoded	O
in	O
for	O
different	O
)	O
,	O
and	O
leaves	O
the	O
choices	O
to	O
the	O
pooling	O
afterwards	O
to	O
pick	O
the	O
more	O
appropriate	O
segments	O
(	O
in	O
every	O
adjacent	O
two	O
)	O
for	O
each	O
composition	O
.	O

With	O
any	O
window	O
width	O
,	O
the	O
type	O
of	O
composition	O
would	O
be	O
much	O
richer	O
than	O
that	O
of	O
RAE	Method
.	O

Second	O
,	O
our	O
convolutional	Method
model	Method
can	O
take	O
supervised	Task
training	Task
and	O
tune	O
the	O
parameters	O
for	O
a	O
specific	O
task	O
,	O
a	O
property	O
vital	O
to	O
our	O
supervised	Method
learning	Method
-	Method
to	Method
-	Method
match	Method
framework	Method
.	O

However	O
,	O
unlike	O
recursive	Method
models	Method
,	O
the	O
convolutional	Method
architecture	Method
has	O
a	O
fixed	O
depth	O
,	O
which	O
bounds	O
the	O
level	O
of	O
composition	O
it	O
could	O
do	O
.	O

For	O
tasks	O
like	O
matching	Task
,	O
this	O
limitation	O
can	O
be	O
largely	O
compensated	O
with	O
a	O
network	Method
afterwards	Method
that	O
can	O
take	O
a	O
“	O
global	O
”	O
synthesis	O
on	O
the	O
learned	O
sentence	Method
representation	Method
.	O

paragraph	O
:	O
Relation	O
to	O
“	O
Shallow	Method
”	Method
Convolutional	Method
Models	Method
The	O
proposed	O
convolutional	Method
sentence	Method
model	Method
takes	O
simple	O
architectures	O
such	O
as	O
(	O
essentially	O
the	O
same	O
convolutional	Method
architecture	Method
as	O
SENNA	Method
)	O
,	O
which	O
consists	O
of	O
a	O
convolution	Method
layer	Method
and	O
a	O
max	Method
-	Method
pooling	Method
over	O
the	O
entire	O
sentence	O
for	O
each	O
feature	O
map	O
.	O

This	O
type	O
of	O
models	O
,	O
with	O
local	Method
convolutions	Method
and	O
a	O
global	Method
pooling	Method
,	O
essentially	O
do	O
a	O
“	O
soft	Method
”	Method
local	Method
template	Method
matching	Method
and	O
is	O
able	O
to	O
detect	O
local	O
features	O
useful	O
for	O
a	O
certain	O
task	O
.	O

Since	O
the	O
sentence	O
-	O
level	O
sequential	O
order	O
is	O
inevitably	O
lost	O
in	O
the	O
global	Method
pooling	Method
,	O
the	O
model	O
is	O
incapable	O
of	O
modeling	O
more	O
complicated	O
structures	O
.	O

It	O
is	O
not	O
hard	O
to	O
see	O
that	O
our	O
convolutional	Method
model	Method
degenerates	O
to	O
the	O
SENNA	Method
-	Method
type	Method
architecture	Method
if	O
we	O
limit	O
the	O
number	O
of	O
layers	O
to	O
be	O
two	O
and	O
set	O
the	O
pooling	O
window	O
infinitely	O
large	O
.	O

section	O
:	O
Convolutional	Method
Matching	Method
Models	Method
Based	O
on	O
the	O
discussion	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
propose	O
two	O
related	O
convolutional	Method
architectures	Method
,	O
namely	O
Arc	Method
-	Method
I	Method
and	O
Arc	Method
-	Method
II	Method
)	O
,	O
for	O
matching	O
two	O
sentences	O
.	O

subsection	O
:	O
Architecture	Method
-	Method
I	Method
(	O
Arc	Method
-	Method
I	Method
)	O
Architecture	Method
-	Method
I	Method
(	O
Arc	Method
-	Method
I	Method
)	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
takes	O
a	O
conventional	O
approach	O
:	O
It	O
first	O
finds	O
the	O
representation	O
of	O
each	O
sentence	O
,	O
and	O
then	O
compares	O
the	O
representation	O
for	O
the	O
two	O
sentences	O
with	O
a	O
multi	Method
-	Method
layer	Method
perceptron	Method
(	O
MLP	Method
)	O
.	O

It	O
is	O
essentially	O
the	O
Siamese	Method
architecture	Method
introduced	O
in	O
,	O
which	O
has	O
been	O
applied	O
to	O
different	O
tasks	O
as	O
a	O
nonlinear	O
similarity	O
function	O
.	O

Although	O
Arc	Method
-	Method
I	Method
enjoys	O
the	O
flexibility	O
brought	O
by	O
the	O
convolutional	Method
sentence	Method
model	Method
,	O
it	O
suffers	O
from	O
a	O
drawback	O
inherited	O
from	O
the	O
Siamese	Method
architecture	Method
:	O
it	O
defers	O
the	O
interaction	O
between	O
two	O
sentences	O
(	O
in	O
the	O
final	O
MLP	Method
)	O
to	O
until	O
their	O
individual	O
representation	O
matures	O
(	O
in	O
the	O
convolution	Method
model	Method
)	O
,	O
therefore	O
runs	O
at	O
the	O
risk	O
of	O
losing	O
details	O
(	O
e.g.	O
,	O
a	O
city	O
name	O
)	O
important	O
for	O
the	O
matching	Task
task	Task
in	O
representing	O
the	O
sentences	O
.	O

In	O
other	O
words	O
,	O
in	O
the	O
forward	Method
phase	Method
(	O
prediction	Task
)	O
,	O
the	O
representation	O
of	O
each	O
sentence	O
is	O
formed	O
without	O
knowledge	O
of	O
each	O
other	O
.	O

This	O
can	O
not	O
be	O
adequately	O
circumvented	O
in	O
backward	Task
phase	Task
(	Task
learning	Task
)	O
,	O
when	O
the	O
convolutional	Method
model	Method
learns	O
to	O
extract	O
structures	O
informative	O
for	O
matching	Task
on	O
a	O
population	O
level	O
.	O

subsection	O
:	O
Architecture	Method
-	Method
II	Method
(	O
Arc	Method
-	Method
II	Method
)	O
In	O
view	O
of	O
the	O
drawback	O
of	O
Architecture	Method
-	Method
I	Method
,	O
we	O
propose	O
Architecture	Method
-	Method
II	Method
(	O
Arc	Method
-	Method
II	Method
)	O
that	O
is	O
built	O
directly	O
on	O
the	O
interaction	O
space	O
between	O
two	O
sentences	O
.	O

It	O
has	O
the	O
desirable	O
property	O
of	O
letting	O
two	O
sentences	O
meet	O
before	O
their	O
own	O
high	O
-	O
level	O
representations	O
mature	O
,	O
while	O
still	O
retaining	O
the	O
space	O
for	O
the	O
individual	O
development	O
of	O
abstraction	O
of	O
each	O
sentence	O
.	O

Basically	O
,	O
in	O
Layer	Method
-	Method
1	Method
,	O
we	O
take	O
sliding	O
windows	O
on	O
both	O
sentences	O
,	O
and	O
model	O
all	O
the	O
possible	O
combinations	O
of	O
them	O
through	O
“	O
one	Method
-	Method
dimensional	Method
”	Method
(	Method
1D	Method
)	Method
convolutions	Method
.	O

For	O
segment	O
on	O
and	O
segment	O
on	O
,	O
we	O
have	O
the	O
feature	O
map	O
where	O
simply	O
concatenates	O
the	O
vectors	O
for	O
sentence	O
segments	O
for	O
and	O
:	O
Clearly	O
the	O
1D	Method
convolution	Method
preserves	O
the	O
location	O
information	O
about	O
both	O
segments	O
.	O

After	O
that	O
in	O
Layer	O
-	O
2	O
,	O
it	O
performs	O
a	O
2D	Method
max	Method
-	Method
pooling	Method
in	O
non	O
-	O
overlapping	O
windows	O
(	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
In	O
Layer	O
-	O
3	O
,	O
we	O
perform	O
a	O
2D	Method
convolution	Method
on	O
windows	O
of	O
output	O
from	O
Layer	O
-	O
2	O
:	O
This	O
could	O
go	O
on	O
for	O
more	O
layers	O
of	O
2D	Method
convolution	Method
and	O
2D	Method
max	Method
-	Method
pooling	Method
,	O
analogous	O
to	O
that	O
of	O
convolutional	Method
architecture	Method
for	O
image	Task
input	Task
.	O

paragraph	O
:	O
The	O
2D	Method
-	Method
Convolution	Method
After	O
the	O
first	O
convolution	O
,	O
we	O
obtain	O
a	O
low	Method
level	Method
representation	Method
of	O
the	O
interaction	O
between	O
the	O
two	O
sentences	O
,	O
and	O
from	O
then	O
we	O
obtain	O
a	O
high	Method
level	Method
representation	Method
which	O
encodes	O
the	O
information	O
from	O
both	O
sentences	O
.	O

The	O
general	O
two	Method
-	Method
dimensional	Method
convolution	Method
is	O
formulated	O
as	O
where	O
concatenates	O
the	O
corresponding	O
vectors	O
from	O
its	O
2D	O
receptive	O
field	O
in	O
Layer	O
-	O
.	O

This	O
pooling	O
has	O
different	O
mechanism	O
as	O
in	O
the	O
1D	Task
case	Task
,	O
for	O
it	O
selects	O
not	O
only	O
among	O
compositions	O
on	O
different	O
segments	O
but	O
also	O
among	O
different	O
local	O
matchings	O
.	O

This	O
pooling	Method
strategy	Method
resembles	O
the	O
dynamic	Method
pooling	Method
in	O
in	O
a	O
similarity	Task
learning	Task
context	Task
,	O
but	O
with	O
two	O
distinctions	O
:	O
1	O
)	O
it	O
happens	O
on	O
a	O
fixed	O
architecture	O
and	O
2	O
)	O
it	O
has	O
much	O
richer	O
structure	O
than	O
just	O
similarity	O
.	O

subsection	O
:	O
Some	O
Analysis	O
on	O
Arc	Method
-	Method
II	Method
paragraph	O
:	O
Order	Method
Preservation	Method
Both	O
the	O
convolution	Method
and	Method
pooling	Method
operation	Method
in	O
Architecture	Method
-	Method
II	Method
have	O
this	O
order	O
preserving	O
property	O
.	O

Generally	O
,	O
contains	O
information	O
about	O
the	O
words	O
in	O
before	O
those	O
in	O
,	O
although	O
they	O
may	O
be	O
generated	O
with	O
slightly	O
different	O
segments	O
in	O
,	O
due	O
to	O
the	O
2D	Method
pooling	Method
(	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
orders	O
is	O
however	O
retained	O
in	O
a	O
“	O
conditional	O
”	O
sense	O
.	O

Our	O
experiments	O
show	O
that	O
when	O
Arc	Method
-	Method
II	Method
is	O
trained	O
on	O
the	O
triples	O
where	O
randomly	O
shuffles	O
the	O
words	O
in	O
,	O
it	O
consistently	O
gains	O
some	O
ability	O
of	O
finding	O
the	O
correct	O
in	O
the	O
usual	O
contrastive	Task
negative	Task
sampling	Task
setting	Task
,	O
which	O
however	O
does	O
not	O
happen	O
with	O
Arc	Method
-	Method
I	Method
.	O

paragraph	O
:	O
Model	O
Generality	O
It	O
is	O
not	O
hard	O
to	O
show	O
that	O
Arc	Method
-	Method
II	Method
actually	O
subsumes	O
Arc	Method
-	Method
I	Method
as	O
a	O
special	O
case	O
.	O

Indeed	O
,	O
in	O
Arc	Method
-	Method
II	Method
if	O
we	O
choose	O
(	O
by	O
turning	O
off	O
some	O
parameters	O
in	O
)	O
to	O
keep	O
the	O
representations	O
of	O
the	O
two	O
sentences	O
separated	O
until	O
the	O
final	O
MLP	Method
,	O
Arc	Method
-	Method
II	Method
can	O
actually	O
act	O
fully	O
like	O
Arc	Method
-	Method
I	Method
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

More	O
specifically	O
,	O
if	O
we	O
let	O
the	O
feature	O
maps	O
in	O
the	O
first	O
convolution	Method
layer	Method
to	O
be	O
either	O
devoted	O
to	O
or	O
devoted	O
to	O
(	O
instead	O
of	O
taking	O
both	O
as	O
in	O
general	O
case	O
)	O
,	O
the	O
output	O
of	O
each	O
segment	O
-	O
pair	O
is	O
naturally	O
divided	O
into	O
two	O
corresponding	O
groups	O
.	O

As	O
a	O
result	O
,	O
the	O
output	O
for	O
each	O
filter	O
,	O
denoted	O
(	O
is	O
the	O
number	O
of	O
sliding	O
windows	O
)	O
,	O
will	O
be	O
of	O
rank	O
-	O
one	O
,	O
possessing	O
essentially	O
the	O
same	O
information	O
as	O
the	O
result	O
of	O
the	O
first	O
convolution	Method
layer	Method
in	O
Arc	Method
-	Method
I	Method
.	O

Clearly	O
the	O
2D	Task
pooling	Task
that	O
follows	O
will	O
reduce	O
to	O
1D	Method
pooling	Method
,	O
with	O
this	O
separateness	O
preserved	O
.	O

If	O
we	O
further	O
limit	O
the	O
parameters	O
in	O
the	O
second	O
convolution	Method
units	Method
(	O
more	O
specifically	O
)	O
to	O
those	O
for	O
and	O
,	O
we	O
can	O
ensure	O
the	O
individual	O
development	O
of	O
different	O
levels	O
of	O
abstraction	O
on	O
each	O
side	O
,	O
and	O
fully	O
recover	O
the	O
functionality	O
of	O
Arc	Method
-	Method
I	Method
.	O

As	O
suggested	O
by	O
the	O
order	O
-	O
preserving	O
property	O
and	O
the	O
generality	O
of	O
Arc	Method
-	Method
II	Method
,	O
this	O
architecture	O
offers	O
not	O
only	O
the	O
capability	O
but	O
also	O
the	O
inductive	O
bias	O
for	O
the	O
individual	O
development	O
of	O
internal	Task
abstraction	Task
on	O
each	O
sentence	O
,	O
despite	O
the	O
fact	O
that	O
it	O
is	O
built	O
on	O
the	O
interaction	O
between	O
two	O
sentences	O
.	O

As	O
a	O
result	O
,	O
Arc	Method
-	Method
II	Method
can	O
naturally	O
blend	O
two	O
seemingly	O
diverging	Method
processes	Method
:	O
1	O
)	O
the	O
successive	Task
composition	Task
within	O
each	O
sentence	O
,	O
and	O
2	O
)	O
the	O
extraction	O
and	O
fusion	Task
of	Task
matching	Task
patterns	Task
between	O
them	O
,	O
hence	O
is	O
powerful	O
for	O
matching	Task
linguistic	Task
objects	Task
with	O
rich	O
structures	O
.	O

This	O
intuition	O
is	O
verified	O
by	O
the	O
superior	O
performance	O
of	O
Arc	Method
-	Method
II	Method
in	O
experiments	O
(	O
Section	O
[	O
reference	O
]	O
)	O
on	O
different	O
matching	Task
tasks	Task
.	O

section	O
:	O
Training	O
We	O
employ	O
a	O
discriminative	Method
training	Method
strategy	Method
with	O
a	O
large	O
margin	O
objective	O
.	O

Suppose	O
that	O
we	O
are	O
given	O
the	O
following	O
triples	O
from	O
the	O
oracle	O
,	O
with	O
matched	O
with	O
better	O
than	O
with	O
.	O

We	O
have	O
the	O
following	O
ranking	Metric
-	Metric
based	Metric
loss	Metric
as	O
objective	O
:	O
where	O
is	O
predicted	Metric
matching	Metric
score	Metric
for	O
,	O
and	O
includes	O
the	O
parameters	O
for	O
convolution	Method
layers	Method
and	O
those	O
for	O
the	O
MLP	Method
.	O

The	O
optimization	Task
is	O
relatively	O
straightforward	O
for	O
both	O
architectures	O
with	O
the	O
standard	O
back	Method
-	Method
propagation	Method
.	O

The	O
gating	Method
function	Method
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
can	O
be	O
easily	O
adopted	O
into	O
the	O
gradient	O
by	O
discounting	O
the	O
contribution	O
from	O
convolution	Method
units	Method
that	O
have	O
been	O
turned	O
off	O
by	O
the	O
gating	O
function	O
.	O

In	O
other	O
words	O
,	O
We	O
use	O
stochastic	Method
gradient	Method
descent	Method
for	O
the	O
optimization	Task
of	Task
models	Task
.	O

All	O
the	O
proposed	O
models	O
perform	O
better	O
with	O
mini	O
-	O
batch	O
(	O
in	O
sizes	O
)	O
which	O
can	O
be	O
easily	O
parallelized	O
on	O
single	O
machine	O
with	O
multi	O
-	O
cores	O
.	O

For	O
regularization	Task
,	O
we	O
find	O
that	O
for	O
both	O
architectures	O
,	O
early	Task
stopping	Task
is	O
enough	O
for	O
models	O
with	O
medium	O
size	O
and	O
large	O
training	O
sets	O
(	O
with	O
over	O
500	O
K	O
instances	O
)	O
.	O

For	O
small	O
datasets	O
(	O
less	O
than	O
10k	O
training	O
instances	O
)	O
however	O
,	O
we	O
have	O
to	O
combine	O
early	Method
stopping	Method
and	O
dropout	Method
to	O
deal	O
with	O
the	O
serious	O
overfitting	Task
problem	Task
.	O

We	O
use	O
50	Method
-	Method
dimensional	Method
word	Method
embedding	Method
trained	O
with	O
the	O
Word2Vec	Method
:	O
the	O
embedding	O
for	O
English	O
words	O
(	O
Section	O
[	O
reference	O
]	O
&	O
[	O
reference	O
]	O
)	O
is	O
learnt	O
on	O
Wikipedia	O
(	O
1B	O
words	O
)	O
,	O
while	O
that	O
for	O
Chinese	O
words	O
(	O
Section	O
[	O
reference	O
]	O
)	O
is	O
learnt	O
on	O
Weibo	O
data	O
(	O
300	O
M	O
words	O
)	O
.	O

Our	O
other	O
experiments	O
(	O
results	O
omitted	O
here	O
)	O
suggest	O
that	O
fine	O
-	O
tuning	O
the	O
word	Method
embedding	Method
can	O
further	O
improve	O
the	O
performances	O
of	O
all	O
models	O
,	O
at	O
the	O
cost	O
of	O
longer	O
training	Task
.	O

We	O
vary	O
the	O
maximum	O
length	O
of	O
words	O
for	O
different	O
tasks	O
to	O
cope	O
with	O
its	O
longest	O
sentence	O
.	O

We	O
use	O
3	O
-	O
word	O
window	O
throughout	O
all	O
experiments	O
,	O
but	O
test	O
various	O
numbers	O
of	O
feature	O
maps	O
(	O
typically	O
from	O
200	O
to	O
500	O
)	O
,	O
for	O
optimal	O
performance	O
.	O

Arc	Method
-	Method
II	Method
models	O
for	O
all	O
tasks	O
have	O
eight	O
layers	O
(	O
three	O
for	O
convolution	Method
,	O
three	O
for	O
pooling	Method
,	O
and	O
two	O
for	O
MLP	Method
)	O
,	O
while	O
Arc	Method
-	Method
I	Method
performs	O
better	O
with	O
less	O
layers	O
(	O
two	O
for	O
convolution	Method
,	O
two	O
for	O
pooling	Method
,	O
and	O
two	O
for	O
MLP	Method
)	O
and	O
more	O
hidden	O
nodes	O
.	O

We	O
use	O
ReLu	Method
as	O
the	O
activation	Method
function	Method
for	O
all	O
of	O
models	O
(	O
convolution	Method
and	Method
MLP	Method
)	O
,	O
which	O
yields	O
comparable	O
or	O
better	O
results	O
to	O
sigmoid	O
-	O
like	O
functions	O
,	O
but	O
converges	O
faster	O
.	O

section	O
:	O
Experiments	O
We	O
report	O
the	O
performance	O
of	O
the	O
proposed	O
models	O
on	O
three	O
matching	Task
tasks	Task
of	O
different	O
nature	O
,	O
and	O
compare	O
it	O
with	O
that	O
of	O
other	O
competitor	Method
models	Method
.	O

Among	O
them	O
,	O
the	O
first	O
two	O
tasks	O
(	O
namely	O
,	O
Sentence	Task
Completion	Task
and	O
Tweet	Task
-	Task
Response	Task
Matching	Task
)	O
are	O
about	O
matching	Task
of	Task
language	Task
objects	Task
of	O
heterogenous	O
natures	O
,	O
while	O
the	O
third	O
one	O
(	O
paraphrase	Task
identification	Task
)	O
is	O
a	O
natural	O
example	O
of	O
matching	Task
homogeneous	Task
objects	Task
.	O

Moreover	O
,	O
the	O
three	O
tasks	O
involve	O
two	O
languages	O
,	O
different	O
types	O
of	O
matching	Task
,	O
and	O
distinctive	O
writing	O
styles	O
,	O
proving	O
the	O
broad	O
applicability	O
of	O
the	O
proposed	O
models	O
.	O

subsection	O
:	O
Competitor	Method
Methods	Method
WordEmbed	O
:	O
We	O
first	O
represent	O
each	O
short	O
-	O
text	O
as	O
the	O
sum	O
of	O
the	O
embedding	O
of	O
the	O
words	O
it	O
contains	O
.	O

The	O
matching	Metric
score	Metric
of	O
two	O
short	O
-	O
texts	O
are	O
calculated	O
with	O
an	O
MLP	Method
with	O
the	O
embedding	O
of	O
the	O
two	O
documents	O
as	O
input	O
;	O
DeepMatch	Method
:	O
We	O
take	O
the	O
matching	Method
model	Method
in	O
and	O
train	O
it	O
on	O
our	O
datasets	O
with	O
3	O
hidden	O
layers	O
and	O
1	O
,	O
000	O
hidden	O
nodes	O
in	O
the	O
first	O
hidden	O
layer	O
;	O
uRAE	Method
+	Method
MLP	Method
:	O
We	O
use	O
the	O
Unfolding	Method
Recursive	Method
Autoencoder	Method
to	O
get	O
a	O
100	Method
-	Method
dimensional	Method
vector	Method
representation	Method
of	O
each	O
sentence	O
,	O
and	O
put	O
an	O
MLP	Method
on	O
the	O
top	O
as	O
in	O
WordEmbed	O
;	O
SENNA	O
+	O
MLP	O
/	O
sim	O
:	O
We	O
use	O
the	O
SENNA	Method
-	Method
type	Method
sentence	Method
model	Method
for	O
sentence	Task
representation	Task
;	O
SenMLP	Method
:	O
We	O
take	O
the	O
whole	O
sentence	O
as	O
input	O
(	O
with	O
word	O
embedding	O
aligned	O
sequentially	O
)	O
,	O
and	O
use	O
an	O
MLP	Method
to	O
obtain	O
the	O
score	Metric
of	Metric
coherence	Metric
.	O

All	O
the	O
competitor	O
models	O
are	O
trained	O
on	O
the	O
same	O
training	O
set	O
as	O
the	O
proposed	O
models	O
,	O
and	O
we	O
report	O
the	O
best	O
test	O
performance	O
over	O
different	O
choices	O
of	O
models	O
(	O
e.g.	O
,	O
the	O
number	O
and	O
size	O
of	O
hidden	O
layers	O
in	O
MLP	Method
)	O
.	O

subsection	O
:	O
Experiment	O
I	O
:	O
Sentence	Task
Completion	Task
This	O
is	O
an	O
artificial	Task
task	Task
designed	O
to	O
elucidate	O
how	O
different	O
matching	Method
models	Method
can	O
capture	O
the	O
correspondence	O
between	O
two	O
clauses	O
within	O
a	O
sentence	O
.	O

Basically	O
,	O
we	O
take	O
a	O
sentence	O
from	O
Reuters	O
with	O
two	O
“	O
balanced	O
”	O
clauses	O
(	O
with	O
8	O
28	O
words	O
)	O
divided	O
by	O
one	O
comma	O
,	O
and	O
use	O
the	O
first	O
clause	O
as	O
and	O
the	O
second	O
as	O
.	O

The	O
task	O
is	O
then	O
to	O
recover	O
the	O
original	O
second	O
clause	O
for	O
any	O
given	O
first	O
clause	O
.	O

The	O
matching	Task
here	O
is	O
considered	O
heterogeneous	O
since	O
the	O
relation	O
between	O
the	O
two	O
is	O
nonsymmetrical	O
on	O
both	O
lexical	O
and	O
semantic	O
levels	O
.	O

We	O
deliberately	O
make	O
the	O
task	O
harder	O
by	O
using	O
negative	O
second	O
clauses	O
similar	O
to	O
the	O
original	O
ones	O
,	O
both	O
in	O
training	O
and	O
testing	O
.	O

One	O
representative	O
example	O
is	O
given	O
as	O
follows	O
:	O
:	O
Although	O
the	O
state	O
has	O
only	O
four	O
votes	O
in	O
the	O
Electoral	O
College	O
,	O
:	O
its	O
loss	O
would	O
be	O
a	O
symbolic	O
blow	O
to	O
republican	O
presidential	O
candi	O
date	O
Bob	O
Dole	O
.	O

:	O
but	O
it	O
failed	O
to	O
garner	O
enough	O
votes	O
to	O
override	O
an	O
expected	O
veto	O
by	O
president	O
Clinton	O
.	O

All	O
models	O
are	O
trained	O
on	O
3	O
million	O
triples	O
(	O
from	O
600	O
K	O
positive	O
pairs	O
)	O
,	O
and	O
tested	O
on	O
50	O
K	O
positive	O
pairs	O
,	O
each	O
accompanied	O
by	O
four	O
negatives	O
,	O
with	O
results	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
two	O
proposed	O
models	O
get	O
nearly	O
half	O
of	O
the	O
cases	O
right	O
,	O
with	O
large	O
margin	O
over	O
other	O
sentence	Method
models	Method
and	O
models	O
without	O
explicit	Method
sequence	Method
modeling	Method
.	O

Arc	Method
-	Method
II	Method
outperforms	O
Arc	Method
-	Method
I	Method
significantly	O
,	O
showing	O
the	O
power	O
of	O
joint	Method
modeling	Method
of	Method
matching	Method
and	O
sentence	Task
meaning	Task
.	O

As	O
another	O
convolutional	Method
model	Method
,	O
SENNA	Method
+	Method
MLP	Method
performs	O
fairly	O
well	O
on	O
this	O
task	O
,	O
although	O
still	O
running	O
behind	O
the	O
proposed	O
convolutional	Method
architectures	Method
since	O
it	O
is	O
too	O
shallow	O
to	O
adequately	O
model	O
the	O
sentence	O
.	O

It	O
is	O
a	O
bit	O
surprising	O
that	O
uRAE	O
comes	O
last	O
on	O
this	O
task	O
,	O
which	O
might	O
be	O
caused	O
by	O
the	O
facts	O
that	O
1	O
)	O
the	O
representation	Method
model	Method
(	O
including	O
word	Method
-	Method
embedding	Method
)	O
is	O
not	O
trained	O
on	O
Reuters	O
,	O
and	O
2	O
)	O
the	O
split	O
-	O
sentence	O
setting	O
hurts	O
the	O
parsing	Task
,	O
which	O
is	O
vital	O
to	O
the	O
quality	O
of	O
learned	O
sentence	Method
representation	Method
.	O

subsection	O
:	O
Experiment	O
II	O
:	O
Matching	O
A	O
Response	O
to	O
A	O
Tweet	O
We	O
trained	O
our	O
model	O
with	O
4.5	O
million	O
original	O
(	O
tweet	O
,	O
response	O
)	O
pairs	O
collected	O
from	O
Weibo	O
,	O
a	O
major	O
Chinese	O
microblog	O
service	O
.	O

Compared	O
to	O
Experiment	O
I	O
,	O
the	O
writing	O
style	O
is	O
obviously	O
more	O
free	O
and	O
informal	O
.	O

For	O
each	O
positive	O
pair	O
,	O
we	O
find	O
ten	O
random	O
responses	O
as	O
negative	O
examples	O
,	O
rendering	O
45	O
million	O
triples	O
for	O
training	O
.	O

One	O
example	O
(	O
translated	O
to	O
English	O
)	O
is	O
given	O
below	O
,	O
with	O
standing	O
for	O
the	O
tweet	O
,	O
the	O
original	O
response	O
,	O
and	O
the	O
randomly	O
selected	O
response	O
:	O
:	O
Damn	O
,	O
I	O
have	O
to	O
work	O
overtime	O
this	O
weekend	O
!	O

:	O
Try	O
to	O
have	O
some	O
rest	O
buddy	O
.	O

:	O
It	O
is	O
hard	O
to	O
find	O
a	O
job	O
,	O
better	O
start	O
polishing	O
your	O
resume	O
.	O

We	O
hold	O
out	O
300	O
K	O
original	O
(	O
tweet	O
,	O
response	O
)	O
pairs	O
and	O
test	O
the	O
matching	Method
model	Method
on	O
their	O
ability	O
to	O
pick	O
the	O
original	O
response	O
from	O
four	O
random	O
negatives	O
,	O
with	O
results	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

This	O
task	O
is	O
slightly	O
easier	O
than	O
Experiment	O
I	O
,	O
with	O
more	O
training	O
instances	O
and	O
purely	O
random	O
negatives	O
.	O

It	O
requires	O
less	O
about	O
the	O
grammatical	O
rigor	O
but	O
more	O
on	O
detailed	O
modeling	O
of	O
loose	O
and	O
local	O
matching	O
patterns	O
(	O
e.g.	O
,	O
work	O
-	O
overtime⇔	O
rest	O
)	O
.	O

Again	O
Arc	Method
-	Method
II	Method
beats	O
other	O
models	O
with	O
large	O
margins	O
,	O
while	O
two	O
convolutional	Method
sentence	Method
models	Method
Arc	Method
-	Method
I	Method
and	O
SENNA	Method
+	Method
MLP	Method
come	O
next	O
.	O

subsection	O
:	O
Experiment	O
III	O
:	O
Paraphrase	Task
Identification	Task
Paraphrase	Task
identification	Task
aims	O
to	O
determine	O
whether	O
two	O
sentences	O
have	O
the	O
same	O
meaning	O
,	O
a	O
problem	O
considered	O
a	O
touchstone	O
of	O
natural	Task
language	Task
understanding	Task
.	O

This	O
experiment	O
is	O
included	O
to	O
test	O
our	O
methods	O
on	O
matching	Task
homogenous	Task
objects	Task
.	O

Here	O
we	O
use	O
the	O
benchmark	O
MSRP	O
dataset	O
,	O
which	O
contains	O
4	O
,	O
076	O
instances	O
for	O
training	O
and	O
1	O
,	O
725	O
for	O
test	O
.	O

We	O
use	O
all	O
the	O
training	O
instances	O
and	O
report	O
the	O
test	O
performance	O
from	O
early	O
stopping	O
.	O

As	O
stated	O
earlier	O
,	O
our	O
model	O
is	O
not	O
specially	O
tailored	O
for	O
modeling	O
synonymy	Task
,	O
and	O
generally	O
requires	O
instances	O
to	O
work	O
favorably	O
.	O

Nevertheless	O
,	O
our	O
generic	Method
matching	Method
models	Method
still	O
manage	O
to	O
perform	O
reasonably	O
well	O
,	O
achieving	O
an	O
accuracy	Metric
and	O
F1	Metric
score	Metric
close	O
to	O
the	O
best	O
performer	O
in	O
2008	O
based	O
on	O
hand	O
-	O
crafted	O
features	O
,	O
but	O
still	O
significantly	O
lower	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
76.8%	O
/	O
83.6	O
%	O
)	O
,	O
achieved	O
with	O
unfolding	Method
-	Method
RAE	Method
and	O
other	O
features	O
designed	O
for	O
this	O
task	O
.	O

subsection	O
:	O
Discussions	O
Arc	Method
-	Method
II	Method
outperforms	O
others	O
significantly	O
when	O
the	O
training	O
instances	O
are	O
relatively	O
abundant	O
(	O
as	O
in	O
Experiment	O
I	O
&	O
II	O
)	O
.	O

Its	O
superiority	O
over	O
Arc	Method
-	Method
I	Method
,	O
however	O
,	O
is	O
less	O
salient	O
when	O
the	O
sentences	O
have	O
deep	O
grammatical	O
structures	O
and	O
the	O
matching	Task
relies	O
less	O
on	O
the	O
local	O
matching	O
patterns	O
,	O
as	O
in	O
Experiment	O
-	O
I.	O
This	O
therefore	O
raises	O
the	O
interesting	O
question	O
about	O
how	O
to	O
balance	O
the	O
representation	O
of	O
matching	O
and	O
the	O
representations	O
of	O
objects	O
,	O
and	O
whether	O
we	O
can	O
guide	O
the	O
learning	Task
process	Task
through	O
something	O
like	O
curriculum	Task
learning	Task
.	O

As	O
another	O
important	O
observation	O
,	O
convolutional	Method
models	Method
(	O
Arc	Method
-	Method
I	Method
&	O
II	Method
,	O
SENNA	Method
+	Method
MLP	Method
)	O
perform	O
favorably	O
over	O
bag	Method
-	Method
of	Method
-	Method
words	Method
models	Method
,	O
indicating	O
the	O
importance	O
of	O
utilizing	O
sequential	O
structures	O
in	O
understanding	O
and	O
matching	Task
sentences	Task
.	O

Quite	O
interestingly	O
,	O
as	O
shown	O
by	O
our	O
other	O
experiments	O
,	O
Arc	Method
-	Method
I	Method
and	O
Arc	Method
-	Method
II	Method
trained	O
purely	O
with	O
random	O
negatives	O
automatically	O
gain	O
some	O
ability	O
in	O
telling	O
whether	O
the	O
words	O
in	O
a	O
given	O
sentence	O
are	O
in	O
right	O
sequential	O
order	O
(	O
with	O
around	O
60	O
%	O
accuracy	Metric
for	O
both	O
)	O
.	O

It	O
is	O
therefore	O
a	O
bit	O
surprising	O
that	O
an	O
auxiliary	Task
task	Task
on	O
identifying	O
the	O
correctness	O
of	O
word	O
order	O
in	O
the	O
response	O
does	O
not	O
enhance	O
the	O
ability	O
of	O
the	O
model	O
on	O
the	O
original	O
matching	Task
tasks	Task
.	O

We	O
noticed	O
that	O
simple	O
sum	Method
of	Method
embedding	Method
learned	O
via	O
Word2Vec	Method
yields	O
reasonably	O
good	O
results	O
on	O
all	O
three	O
tasks	O
.	O

We	O
hypothesize	O
that	O
the	O
Word2Vec	Method
embedding	Method
is	O
trained	O
in	O
such	O
a	O
way	O
that	O
the	O
vector	Method
summation	Method
can	O
act	O
as	O
a	O
simple	O
composition	O
,	O
and	O
hence	O
retains	O
a	O
fair	O
amount	O
of	O
meaning	O
in	O
the	O
short	O
text	O
segment	O
.	O

This	O
is	O
in	O
contrast	O
with	O
other	O
bag	Method
-	Method
of	Method
-	Method
words	Method
models	Method
like	O
DeepMatch	Method
.	O

section	O
:	O
Related	O
Work	O
Matching	Task
structured	Task
objects	Task
rarely	O
goes	O
beyond	O
estimating	O
the	O
similarity	O
of	O
objects	O
in	O
the	O
same	O
domain	O
,	O
with	O
few	O
exceptions	O
like	O
.	O

When	O
dealing	O
with	O
language	Task
objects	Task
,	O
most	O
methods	O
still	O
focus	O
on	O
seeking	O
vectorial	Method
representations	Method
in	O
a	O
common	O
latent	O
space	O
,	O
and	O
calculating	O
the	O
matching	Metric
score	Metric
with	O
inner	Method
product	Method
.	O

Few	O
work	O
has	O
been	O
done	O
on	O
building	O
a	O
deep	Method
architecture	Method
on	O
the	O
interaction	O
space	O
for	O
texts	O
-	O
pairs	O
,	O
but	O
it	O
is	O
largely	O
based	O
on	O
a	O
bag	Method
-	Method
of	Method
-	Method
words	Method
representation	Method
of	Method
text	Method
.	O

Our	O
models	O
are	O
related	O
to	O
the	O
long	O
thread	O
of	O
work	O
on	O
sentence	Task
representation	Task
.	O

Aside	O
from	O
the	O
models	O
with	O
recursive	O
nature	O
(	O
as	O
discussed	O
in	O
Section	O
2.1	O
)	O
,	O
it	O
is	O
fairly	O
common	O
practice	O
to	O
use	O
the	O
sum	Method
of	Method
word	Method
-	Method
embedding	Method
to	O
represent	O
a	O
short	O
-	O
text	O
,	O
mostly	O
for	O
classification	Task
.	O

There	O
is	O
very	O
little	O
work	O
on	O
convolutional	Task
modeling	Task
of	Task
language	Task
.	O

In	O
addition	O
to	O
,	O
there	O
is	O
a	O
very	O
recent	O
model	O
on	O
sentence	Task
representation	Task
with	O
dynamic	Method
convolutional	Method
neural	Method
network	Method
.	O

This	O
work	O
relies	O
heavily	O
on	O
a	O
carefully	O
designed	O
pooling	Method
strategy	Method
to	O
handle	O
the	O
variable	O
length	O
of	O
sentence	O
with	O
a	O
relatively	O
small	O
feature	O
map	O
,	O
tailored	O
for	O
classification	Task
problems	Task
with	O
modest	O
sizes	O
.	O

section	O
:	O
Conclusion	O
We	O
propose	O
deep	Method
convolutional	Method
architectures	Method
for	O
matching	Task
natural	Task
language	Task
sentences	Task
,	O
which	O
can	O
nicely	O
combine	O
the	O
hierarchical	Method
modeling	Method
of	O
individual	O
sentences	O
and	O
the	O
patterns	O
of	O
their	O
matching	O
.	O

Empirical	O
study	O
shows	O
our	O
models	O
can	O
outperform	O
competitors	O
on	O
a	O
variety	O
of	O
matching	Task
tasks	Task
.	O

paragraph	O
:	O
Acknowledgments	O
:	O
B.	O
Hu	O
and	O
Q.	O
Chen	O
are	O
supported	O
in	O
part	O
by	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
61173075	O
.	O

Z.	O
Lu	O
and	O
H.	O
Li	O
are	O
supported	O
in	O
part	O
by	O
China	O
National	O
973	O
project	O
2014CB340301	O
.	O

bibliography	O
:	O
References	O
