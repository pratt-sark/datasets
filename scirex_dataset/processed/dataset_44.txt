document	O
:	O
Neural	Method
Architectures	Method
for	O
Named	Task
Entity	Task
Recognition	Task
State	O
-	O
of	O
-	O
the	O
-	O
art	O
named	Method
entity	Method
recognition	Method
systems	Method
rely	O
heavily	O
on	O
hand	O
-	O
crafted	O
features	O
and	O
domain	O
-	O
specific	O
knowledge	O
in	O
order	O
to	O
learn	O
effectively	O
from	O
the	O
small	O
,	O
supervised	O
training	O
corpora	O
that	O
are	O
available	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
two	O
new	O
neural	Method
architectures	Method
—	O
one	O
based	O
on	O
bidirectional	O
LSTMs	Method
and	O
conditional	Method
random	Method
fields	Method
,	O
and	O
the	O
other	O
that	O
constructs	O
and	O
labels	O
segments	O
using	O
a	O
transition	Method
-	Method
based	Method
approach	Method
inspired	O
by	O
shift	Method
-	Method
reduce	Method
parsers	Method
.	O

Our	O
models	O
rely	O
on	O
two	O
sources	O
of	O
information	O
about	O
words	O
:	O
character	Method
-	Method
based	Method
word	Method
representations	Method
learned	O
from	O
the	O
supervised	O
corpus	O
and	O
unsupervised	Method
word	Method
representations	Method
learned	O
from	O
unannotated	O
corpora	O
.	O

Our	O
models	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
NER	Task
in	O
four	O
languages	O
without	O
resorting	O
to	O
any	O
language	O
-	O
specific	O
knowledge	O
or	O
resources	O
such	O
as	O
gazetteers	O
.	O

section	O
:	O
Introduction	O
Named	Task
entity	Task
recognition	Task
(	O
NER	Task
)	O
is	O
a	O
challenging	O
learning	Task
problem	Task
.	O

One	O
the	O
one	O
hand	O
,	O
in	O
most	O
languages	O
and	O
domains	O
,	O
there	O
is	O
only	O
a	O
very	O
small	O
amount	O
of	O
supervised	O
training	O
data	O
available	O
.	O

On	O
the	O
other	O
,	O
there	O
are	O
few	O
constraints	O
on	O
the	O
kinds	O
of	O
words	O
that	O
can	O
be	O
names	O
,	O
so	O
generalizing	O
from	O
this	O
small	O
sample	O
of	O
data	O
is	O
difficult	O
.	O

As	O
a	O
result	O
,	O
carefully	O
constructed	O
orthographic	O
features	O
and	O
language	O
-	O
specific	O
knowledge	O
resources	O
,	O
such	O
as	O
gazetteers	Method
,	O
are	O
widely	O
used	O
for	O
solving	O
this	O
task	O
.	O

Unfortunately	O
,	O
language	O
-	O
specific	O
resources	O
and	O
features	O
are	O
costly	O
to	O
develop	O
in	O
new	O
languages	O
and	O
new	O
domains	O
,	O
making	O
NER	Task
a	O
challenge	O
to	O
adapt	O
.	O

Unsupervised	Method
learning	Method
from	O
unannotated	O
corpora	O
offers	O
an	O
alternative	O
strategy	O
for	O
obtaining	O
better	O
generalization	Task
from	O
small	O
amounts	O
of	O
supervision	O
.	O

However	O
,	O
even	O
systems	O
that	O
have	O
relied	O
extensively	O
on	O
unsupervised	O
features	O
have	O
used	O
these	O
to	O
augment	O
,	O
rather	O
than	O
replace	O
,	O
hand	O
-	O
engineered	O
features	O
(	O
e.g.	O
,	O
knowledge	O
about	O
capitalization	O
patterns	O
and	O
character	O
classes	O
in	O
a	O
particular	O
language	O
)	O
and	O
specialized	O
knowledge	O
resources	O
(	O
e.g.	O
,	O
gazetteers	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
neural	Method
architectures	Method
for	O
NER	Task
that	O
use	O
no	O
language	O
-	O
specific	O
resources	O
or	O
features	O
beyond	O
a	O
small	O
amount	O
of	O
supervised	O
training	O
data	O
and	O
unlabeled	O
corpora	O
.	O

Our	O
models	O
are	O
designed	O
to	O
capture	O
two	O
intuitions	O
.	O

First	O
,	O
since	O
names	O
often	O
consist	O
of	O
multiple	O
tokens	O
,	O
reasoning	O
jointly	O
over	O
tagging	O
decisions	O
for	O
each	O
token	O
is	O
important	O
.	O

We	O
compare	O
two	O
models	O
here	O
,	O
(	O
i	O
)	O
a	O
bidirectional	O
LSTM	Method
with	O
a	O
sequential	Method
conditional	Method
random	Method
layer	Method
above	O
it	O
(	O
LSTM	Method
-	Method
CRF	Method
;	O
§	O
[	O
reference	O
]	O
)	O
,	O
and	O
(	O
ii	O
)	O
a	O
new	O
model	O
that	O
constructs	O
and	O
labels	O
chunks	O
of	O
input	O
sentences	O
using	O
an	O
algorithm	O
inspired	O
by	O
transition	Method
-	Method
based	Method
parsing	Method
with	O
states	O
represented	O
by	O
stack	O
LSTMs	Method
(	O
S	Method
-	Method
LSTM	Method
;	O
§	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
token	O
-	O
level	O
evidence	O
for	O
“	O
being	O
a	O
name	O
”	O
includes	O
both	O
orthographic	O
evidence	O
(	O
what	O
does	O
the	O
word	O
being	O
tagged	O
as	O
a	O
name	O
look	O
like	O
?	O
)	O
and	O
distributional	O
evidence	O
(	O
where	O
does	O
the	O
word	O
being	O
tagged	O
tend	O
to	O
occur	O
in	O
a	O
corpus	O
?	O
)	O
.	O

To	O
capture	O
orthographic	O
sensitivity	O
,	O
we	O
use	O
character	Method
-	Method
based	Method
word	Method
representation	Method
model	Method
to	O
capture	O
distributional	O
sensitivity	O
,	O
we	O
combine	O
these	O
representations	O
with	O
distributional	Method
representations	Method
.	O

Our	O
word	Method
representations	Method
combine	O
both	O
of	O
these	O
,	O
and	O
dropout	Method
training	Method
is	O
used	O
to	O
encourage	O
the	O
model	O
to	O
learn	O
to	O
trust	O
both	O
sources	O
of	O
evidence	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

Experiments	O
in	O
English	Material
,	O
Dutch	O
,	O
German	O
,	O
and	O
Spanish	O
show	O
that	O
we	O
are	O
able	O
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NER	Metric
performance	Metric
with	O
the	O
LSTM	Method
-	O
CRF	Method
model	O
in	O
Dutch	O
,	O
German	O
,	O
and	O
Spanish	O
,	O
and	O
very	O
near	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
English	Material
without	O
any	O
hand	O
-	O
engineered	O
features	O
or	O
gazetteers	O
(	O
§	O
[	O
reference	O
]	O
)	O
.	O

The	O
transition	Method
-	Method
based	Method
algorithm	Method
likewise	O
surpasses	O
the	O
best	O
previously	O
published	O
results	O
in	O
several	O
languages	O
,	O
although	O
it	O
performs	O
less	O
well	O
than	O
the	O
LSTM	Method
-	O
CRF	Method
model	O
.	O

section	O
:	O
LSTM	Method
-	O
CRF	Method
Model	O
We	O
provide	O
a	O
brief	O
description	O
of	O
LSTMs	Method
and	O
CRFs	Method
,	O
and	O
present	O
a	O
hybrid	Method
tagging	Method
architecture	Method
.	O

This	O
architecture	O
is	O
similar	O
to	O
the	O
ones	O
presented	O
by	O
collobert2011natural	O
and	O
huang:2015	O
.	O

subsection	O
:	O
LSTM	Method
Recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
are	O
a	O
family	O
of	O
neural	Method
networks	Method
that	O
operate	O
on	O
sequential	O
data	O
.	O

They	O
take	O
as	O
input	O
a	O
sequence	O
of	O
vectors	O
and	O
return	O
another	O
sequence	O
that	O
represents	O
some	O
information	O
about	O
the	O
sequence	O
at	O
every	O
step	O
in	O
the	O
input	O
.	O

Although	O
RNNs	Method
can	O
,	O
in	O
theory	O
,	O
learn	O
long	O
dependencies	O
,	O
in	O
practice	O
they	O
fail	O
to	O
do	O
so	O
and	O
tend	O
to	O
be	O
biased	O
towards	O
their	O
most	O
recent	O
inputs	O
in	O
the	O
sequence	O
.	O

Long	Method
Short	Method
-	Method
term	Method
Memory	Method
Networks	Method
(	O
LSTMs	Method
)	O
have	O
been	O
designed	O
to	O
combat	O
this	O
issue	O
by	O
incorporating	O
a	O
memory	O
-	O
cell	O
and	O
have	O
been	O
shown	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
.	O

They	O
do	O
so	O
using	O
several	O
gates	O
that	O
control	O
the	O
proportion	O
of	O
the	O
input	O
to	O
give	O
to	O
the	O
memory	O
cell	O
,	O
and	O
the	O
proportion	O
from	O
the	O
previous	O
state	O
to	O
forget	O
.	O

We	O
use	O
the	O
following	O
implementation	O
:	O
where	O
is	O
the	O
element	Method
-	Method
wise	Method
sigmoid	Method
function	Method
,	O
and	O
is	O
the	O
element	O
-	O
wise	O
product	O
.	O

For	O
a	O
given	O
sentence	O
containing	O
words	O
,	O
each	O
represented	O
as	O
a	O
-	O
dimensional	O
vector	O
,	O
an	O
LSTM	Method
computes	O
a	O
representation	O
of	O
the	O
left	O
context	O
of	O
the	O
sentence	O
at	O
every	O
word	O
.	O

Naturally	O
,	O
generating	O
a	O
representation	O
of	O
the	O
right	O
context	O
as	O
well	O
should	O
add	O
useful	O
information	O
.	O

This	O
can	O
be	O
achieved	O
using	O
a	O
second	O
LSTM	Method
that	O
reads	O
the	O
same	O
sequence	O
in	O
reverse	O
.	O

We	O
will	O
refer	O
to	O
the	O
former	O
as	O
the	O
forward	O
LSTM	Method
and	O
the	O
latter	O
as	O
the	O
backward	O
LSTM	Method
.	O

These	O
are	O
two	O
distinct	O
networks	O
with	O
different	O
parameters	O
.	O

This	O
forward	O
and	O
backward	O
LSTM	Method
pair	O
is	O
referred	O
to	O
as	O
a	O
bidirectional	O
LSTM	Method
.	O

The	O
representation	O
of	O
a	O
word	O
using	O
this	O
model	O
is	O
obtained	O
by	O
concatenating	O
its	O
left	Method
and	Method
right	Method
context	Method
representations	Method
,	O
.	O

These	O
representations	O
effectively	O
include	O
a	O
representation	O
of	O
a	O
word	O
in	O
context	O
,	O
which	O
is	O
useful	O
for	O
numerous	O
tagging	Task
applications	Task
.	O

subsection	O
:	O
CRF	Method
Tagging	O
Models	O
A	O
very	O
simple	O
—	O
but	O
surprisingly	O
effective	O
—	O
tagging	Method
model	Method
is	O
to	O
use	O
the	O
’s	O
as	O
features	O
to	O
make	O
independent	O
tagging	O
decisions	O
for	O
each	O
output	O
.	O

Despite	O
this	O
model	O
’s	O
success	O
in	O
simple	O
problems	O
like	O
POS	Task
tagging	Task
,	O
its	O
independent	O
classification	Task
decisions	Task
are	O
limiting	O
when	O
there	O
are	O
strong	O
dependencies	O
across	O
output	O
labels	O
.	O

NER	Task
is	O
one	O
such	O
task	O
,	O
since	O
the	O
“	O
grammar	Method
”	O
that	O
characterizes	O
interpretable	O
sequences	O
of	O
tags	O
imposes	O
several	O
hard	O
constraints	O
(	O
e.g.	O
,	O
I	O
-	O
PER	O
can	O
not	O
follow	O
B	O
-	O
LOC	O
;	O
see	O
§	O
[	O
reference	O
]	O
for	O
details	O
)	O
that	O
would	O
be	O
impossible	O
to	O
model	O
with	O
independence	O
assumptions	O
.	O

Therefore	O
,	O
instead	O
of	O
modeling	O
tagging	Task
decisions	Task
independently	O
,	O
we	O
model	O
them	O
jointly	O
using	O
a	O
conditional	Method
random	Method
field	Method
.	O

For	O
an	O
input	O
sentence	O
we	O
consider	O
to	O
be	O
the	O
matrix	O
of	O
scores	O
output	O
by	O
the	O
bidirectional	O
LSTM	Method
network	O
.	O

is	O
of	O
size	O
,	O
where	O
is	O
the	O
number	O
of	O
distinct	O
tags	O
,	O
and	O
corresponds	O
to	O
the	O
score	O
of	O
the	O
tag	O
of	O
the	O
word	O
in	O
a	O
sentence	O
.	O

For	O
a	O
sequence	O
of	O
predictions	O
we	O
define	O
its	O
score	O
to	O
be	O
where	O
is	O
a	O
matrix	O
of	O
transition	O
scores	O
such	O
that	O
represents	O
the	O
score	O
of	O
a	O
transition	O
from	O
the	O
tag	O
to	O
tag	O
.	O

and	O
are	O
the	O
start	O
and	O
end	O
tags	O
of	O
a	O
sentence	O
,	O
that	O
we	O
add	O
to	O
the	O
set	O
of	O
possible	O
tags	O
.	O

is	O
therefore	O
a	O
square	O
matrix	O
of	O
size	O
.	O

A	O
softmax	O
over	O
all	O
possible	O
tag	O
sequences	O
yields	O
a	O
probability	O
for	O
the	O
sequence	O
:	O
During	O
training	Task
,	O
we	O
maximize	O
the	O
log	O
-	O
probability	O
of	O
the	O
correct	O
tag	O
sequence	O
:	O
where	O
represents	O
all	O
possible	O
tag	O
sequences	O
(	O
even	O
those	O
that	O
do	O
not	O
verify	O
the	O
IOB	O
format	O
)	O
for	O
a	O
sentence	O
.	O

From	O
the	O
formulation	O
above	O
,	O
it	O
is	O
evident	O
that	O
we	O
encourage	O
our	O
network	O
to	O
produce	O
a	O
valid	O
sequence	O
of	O
output	O
labels	O
.	O

While	O
decoding	Task
,	O
we	O
predict	O
the	O
output	O
sequence	O
that	O
obtains	O
the	O
maximum	O
score	O
given	O
by	O
:	O
Since	O
we	O
are	O
only	O
modeling	O
bigram	O
interactions	O
between	O
outputs	O
,	O
both	O
the	O
summation	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
and	O
the	O
maximum	O
a	O
posteriori	O
sequence	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
can	O
be	O
computed	O
using	O
dynamic	Method
programming	Method
.	O

subsection	O
:	O
Parameterization	O
and	O
Training	O
The	O
scores	O
associated	O
with	O
each	O
tagging	O
decision	O
for	O
each	O
token	O
(	O
i.e.	O
,	O
the	O
’s	O
)	O
are	O
defined	O
to	O
be	O
the	O
dot	O
product	O
between	O
the	O
embedding	O
of	O
a	O
word	O
-	O
in	O
-	O
context	O
computed	O
with	O
a	O
bidirectional	O
LSTM	Method
—	O
exactly	O
the	O
same	O
as	O
the	O
POS	Method
tagging	Method
model	Method
of	O
ling:2015	Method
and	O
these	O
are	O
combined	O
with	O
bigram	O
compatibility	O
scores	O
(	O
i.e.	O
,	O
the	O
’	O
s	O
)	O
.	O

This	O
architecture	O
is	O
shown	O
in	O
figure	O
[	O
reference	O
]	O
.	O

Circles	O
represent	O
observed	O
variables	O
,	O
diamonds	O
are	O
deterministic	O
functions	O
of	O
their	O
parents	O
,	O
and	O
double	O
circles	O
are	O
random	O
variables	O
.	O

The	O
parameters	O
of	O
this	O
model	O
are	O
thus	O
the	O
matrix	O
of	O
bigram	O
compatibility	O
scores	O
,	O
and	O
the	O
parameters	O
that	O
give	O
rise	O
to	O
the	O
matrix	O
,	O
namely	O
the	O
parameters	O
of	O
the	O
bidirectional	O
LSTM	Method
,	O
the	O
linear	O
feature	O
weights	O
,	O
and	O
the	O
word	O
embeddings	O
.	O

As	O
in	O
part	O
[	O
reference	O
]	O
,	O
let	O
denote	O
the	O
sequence	O
of	O
word	O
embeddings	O
for	O
every	O
word	O
in	O
a	O
sentence	O
,	O
and	O
be	O
their	O
associated	O
tags	O
.	O

We	O
return	O
to	O
a	O
discussion	O
of	O
how	O
the	O
embeddings	O
are	O
modeled	O
in	O
Section	O
[	O
reference	O
]	O
.	O

The	O
sequence	O
of	O
word	O
embeddings	O
is	O
given	O
as	O
input	O
to	O
a	O
bidirectional	O
LSTM	Method
,	O
which	O
returns	O
a	O
representation	O
of	O
the	O
left	O
and	O
right	O
context	O
for	O
each	O
word	O
as	O
explained	O
in	O
[	O
reference	O
]	O
.	O

These	O
representations	O
are	O
concatenated	O
(	O
)	O
and	O
linearly	O
projected	O
onto	O
a	O
layer	O
whose	O
size	O
is	O
equal	O
to	O
the	O
number	O
of	O
distinct	O
tags	O
.	O

Instead	O
of	O
using	O
the	O
softmax	O
output	O
from	O
this	O
layer	O
,	O
we	O
use	O
a	O
CRF	Method
as	O
previously	O
described	O
to	O
take	O
into	O
account	O
neighboring	O
tags	O
,	O
yielding	O
the	O
final	O
predictions	O
for	O
every	O
word	O
.	O

Additionally	O
,	O
we	O
observed	O
that	O
adding	O
a	O
hidden	O
layer	O
between	O
and	O
the	O
CRF	Method
layer	O
marginally	O
improved	O
our	O
results	O
.	O

All	O
results	O
reported	O
with	O
this	O
model	O
incorporate	O
this	O
extra	O
-	O
layer	O
.	O

The	O
parameters	O
are	O
trained	O
to	O
maximize	O
Eq	O
.	O

[	O
reference	O
]	O
of	O
observed	O
sequences	O
of	O
NER	Task
tags	O
in	O
an	O
annotated	O
corpus	O
,	O
given	O
the	O
observed	O
words	O
.	O

subsection	O
:	O
Tagging	Method
Schemes	Method
The	O
task	O
of	O
named	Task
entity	Task
recognition	Task
is	O
to	O
assign	O
a	O
named	O
entity	O
label	O
to	O
every	O
word	O
in	O
a	O
sentence	O
.	O

A	O
single	O
named	O
entity	O
could	O
span	O
several	O
tokens	O
within	O
a	O
sentence	O
.	O

Sentences	O
are	O
usually	O
represented	O
in	O
the	O
IOB	O
format	O
(	O
Inside	O
,	O
Outside	O
,	O
Beginning	O
)	O
where	O
every	O
token	O
is	O
labeled	O
as	O
B	O
-	O
label	O
if	O
the	O
token	O
is	O
the	O
beginning	O
of	O
a	O
named	O
entity	O
,	O
I	O
-	O
label	O
if	O
it	O
is	O
inside	O
a	O
named	O
entity	O
but	O
not	O
the	O
first	O
token	O
within	O
the	O
named	O
entity	O
,	O
or	O
O	O
otherwise	O
.	O

However	O
,	O
we	O
decided	O
to	O
use	O
the	O
IOBES	Method
tagging	Method
scheme	Method
,	O
a	O
variant	O
of	O
IOB	Method
commonly	O
used	O
for	O
named	Task
entity	Task
recognition	Task
,	O
which	O
encodes	O
information	O
about	O
singleton	O
entities	O
(	O
S	O
)	O
and	O
explicitly	O
marks	O
the	O
end	O
of	O
named	O
entities	O
(	O
E	O
)	O
.	O

Using	O
this	O
scheme	O
,	O
tagging	O
a	O
word	O
as	O
I	O
-	O
label	O
with	O
high	O
-	O
confidence	O
narrows	O
down	O
the	O
choices	O
for	O
the	O
subsequent	O
word	O
to	O
I	O
-	O
label	O
or	O
E	O
-	O
label	O
,	O
however	O
,	O
the	O
IOB	Method
scheme	Method
is	O
only	O
capable	O
of	O
determining	O
that	O
the	O
subsequent	O
word	O
can	O
not	O
be	O
the	O
interior	O
of	O
another	O
label	O
.	O

ratinov2009design	O
and	O
dai2015enhancing	O
showed	O
that	O
using	O
a	O
more	O
expressive	O
tagging	Method
scheme	Method
like	O
IOBES	Method
improves	O
model	O
performance	O
marginally	O
.	O

However	O
,	O
we	O
did	O
not	O
observe	O
a	O
significant	O
improvement	O
over	O
the	O
IOB	Method
tagging	Method
scheme	Method
.	O

section	O
:	O
Transition	Method
-	Method
Based	Method
Chunking	Method
Model	Method
As	O
an	O
alternative	O
to	O
the	O
LSTM	Method
-	Method
CRF	Method
discussed	O
in	O
the	O
previous	O
section	O
,	O
we	O
explore	O
a	O
new	O
architecture	O
that	O
chunks	O
and	O
labels	O
a	O
sequence	O
of	O
inputs	O
using	O
an	O
algorithm	O
similar	O
to	O
transition	Method
-	Method
based	Method
dependency	Method
parsing	Method
.	O

This	O
model	O
directly	O
constructs	O
representations	O
of	O
the	O
multi	O
-	O
token	O
names	O
(	O
e.g.	O
,	O
the	O
name	O
Mark	O
Watney	O
is	O
composed	O
into	O
a	O
single	O
representation	O
)	O
.	O

This	O
model	O
relies	O
on	O
a	O
stack	Method
data	Method
structure	Method
to	O
incrementally	O
construct	O
chunks	O
of	O
the	O
input	O
.	O

To	O
obtain	O
representations	O
of	O
this	O
stack	O
used	O
for	O
predicting	Task
subsequent	Task
actions	Task
,	O
we	O
use	O
the	O
Stack	Method
-	Method
LSTM	Method
presented	O
by	O
dyer:2015	O
,	O
in	O
which	O
the	O
LSTM	Method
is	O
augmented	O
with	O
a	O
“	O
stack	O
pointer	O
.	O

”	O
While	O
sequential	O
LSTMs	Method
model	O
sequences	O
from	O
left	O
to	O
right	O
,	O
stack	O
LSTMs	Method
permit	O
embedding	O
of	O
a	O
stack	O
of	O
objects	O
that	O
are	O
both	O
added	O
to	O
(	O
using	O
a	O
push	O
operation	O
)	O
and	O
removed	O
from	O
(	O
using	O
a	O
pop	O
operation	O
)	O
.	O

This	O
allows	O
the	O
Stack	Method
-	Method
LSTM	Method
to	O
work	O
like	O
a	O
stack	Method
that	O
maintains	O
a	O
“	O
summary	O
embedding	O
”	O
of	O
its	O
contents	O
.	O

We	O
refer	O
to	O
this	O
model	O
as	O
Stack	Method
-	Method
LSTM	Method
or	O
S	Method
-	Method
LSTM	Method
model	O
for	O
simplicity	O
.	O

Finally	O
,	O
we	O
refer	O
interested	O
readers	O
to	O
the	O
original	O
paper	O
for	O
details	O
about	O
the	O
Stack	O
-	O
LSTM	Method
model	O
since	O
in	O
this	O
paper	O
we	O
merely	O
use	O
the	O
same	O
architecture	O
through	O
a	O
new	O
transition	Method
-	Method
based	Method
algorithm	Method
presented	O
in	O
the	O
following	O
Section	O
.	O

subsection	O
:	O
Chunking	Method
Algorithm	Method
We	O
designed	O
a	O
transition	Method
inventory	Method
which	O
is	O
given	O
in	O
Figure	O
[	O
reference	O
]	O
that	O
is	O
inspired	O
by	O
transition	Method
-	Method
based	Method
parsers	Method
,	O
in	O
particular	O
the	O
arc	Method
-	Method
standard	Method
parser	Method
of	O
nivre2004	O
.	O

In	O
this	O
algorithm	O
,	O
we	O
make	O
use	O
of	O
two	O
stacks	O
(	O
designated	O
output	O
and	O
stack	O
representing	O
,	O
respectively	O
,	O
completed	O
chunks	O
and	O
scratch	O
space	O
)	O
and	O
a	O
buffer	O
that	O
contains	O
the	O
words	O
that	O
have	O
yet	O
to	O
be	O
processed	O
.	O

The	O
transition	O
inventory	O
contains	O
the	O
following	O
transitions	O
:	O
The	O
shift	Method
transition	Method
moves	O
a	O
word	O
from	O
the	O
buffer	O
to	O
the	O
stack	O
,	O
the	O
out	O
transition	O
moves	O
a	O
word	O
from	O
the	O
buffer	O
directly	O
into	O
the	O
output	O
stack	O
while	O
the	O
reduce	O
(	O
y	O
)	O
transition	O
pops	O
all	O
items	O
from	O
the	O
top	O
of	O
the	O
stack	O
creating	O
a	O
“	O
chunk	O
,	O
”	O
labels	O
this	O
with	O
label	O
,	O
and	O
pushes	O
a	O
representation	O
of	O
this	O
chunk	O
onto	O
the	O
output	O
stack	O
.	O

The	O
algorithm	O
completes	O
when	O
the	O
stack	O
and	O
buffer	O
are	O
both	O
empty	O
.	O

The	O
algorithm	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
which	O
shows	O
the	O
sequence	O
of	O
operations	O
required	O
to	O
process	O
the	O
sentence	O
Mark	O
Watney	O
visited	O
Mars	O
.	O

The	O
model	O
is	O
parameterized	O
by	O
defining	O
a	O
probability	O
distribution	O
over	O
actions	O
at	O
each	O
time	O
step	O
,	O
given	O
the	O
current	O
contents	O
of	O
the	O
stack	O
,	O
buffer	O
,	O
and	O
output	O
,	O
as	O
well	O
as	O
the	O
history	O
of	O
actions	O
taken	O
.	O

Following	O
dyer:2015	O
,	O
we	O
use	O
stack	O
LSTMs	Method
to	O
compute	O
a	O
fixed	O
dimensional	O
embedding	O
of	O
each	O
of	O
these	O
,	O
and	O
take	O
a	O
concatenation	O
of	O
these	O
to	O
obtain	O
the	O
full	O
algorithm	O
state	O
.	O

This	O
representation	O
is	O
used	O
to	O
define	O
a	O
distribution	O
over	O
the	O
possible	O
actions	O
that	O
can	O
be	O
taken	O
at	O
each	O
time	O
step	O
.	O

The	O
model	O
is	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
sequences	O
of	O
reference	O
actions	O
(	O
extracted	O
from	O
a	O
labeled	O
training	O
corpus	O
)	O
given	O
the	O
input	O
sentences	O
.	O

To	O
label	O
a	O
new	O
input	O
sequence	O
at	O
test	O
time	O
,	O
the	O
maximum	O
probability	O
action	O
is	O
chosen	O
greedily	O
until	O
the	O
algorithm	O
reaches	O
a	O
termination	O
state	O
.	O

Although	O
this	O
is	O
not	O
guaranteed	O
to	O
find	O
a	O
global	O
optimum	O
,	O
it	O
is	O
effective	O
in	O
practice	O
.	O

Since	O
each	O
token	O
is	O
either	O
moved	O
directly	O
to	O
the	O
output	O
(	O
1	O
action	O
)	O
or	O
first	O
to	O
the	O
stack	O
and	O
then	O
the	O
output	O
(	O
2	O
actions	O
)	O
,	O
the	O
total	O
number	O
of	O
actions	O
for	O
a	O
sequence	O
of	O
length	O
is	O
maximally	O
.	O

It	O
is	O
worth	O
noting	O
that	O
the	O
nature	O
of	O
this	O
algorithm	Method
model	Method
makes	O
it	O
agnostic	O
to	O
the	O
tagging	Method
scheme	Method
used	O
since	O
it	O
directly	O
predicts	O
labeled	O
chunks	O
.	O

subsection	O
:	O
Representing	Task
Labeled	Task
Chunks	Task
When	O
the	O
operation	O
is	O
executed	O
,	O
the	O
algorithm	O
shifts	O
a	O
sequence	O
of	O
tokens	O
(	O
together	O
with	O
their	O
vector	O
embeddings	O
)	O
from	O
the	O
stack	O
to	O
the	O
output	O
buffer	O
as	O
a	O
single	O
completed	O
chunk	O
.	O

To	O
compute	O
an	O
embedding	O
of	O
this	O
sequence	O
,	O
we	O
run	O
a	O
bidirectional	O
LSTM	Method
over	O
the	O
embeddings	O
of	O
its	O
constituent	O
tokens	O
together	O
with	O
a	O
token	O
representing	O
the	O
type	O
of	O
the	O
chunk	O
being	O
identified	O
(	O
i.e.	O
,	O
)	O
.	O

This	O
function	O
is	O
given	O
as	O
,	O
where	O
is	O
a	O
learned	O
embedding	O
of	O
a	O
label	O
type	O
.	O

Thus	O
,	O
the	O
output	O
buffer	O
contains	O
a	O
single	O
vector	Method
representation	Method
for	O
each	O
labeled	O
chunk	O
that	O
is	O
generated	O
,	O
regardless	O
of	O
its	O
length	O
.	O

section	O
:	O
Input	O
Word	Method
Embeddings	Method
The	O
input	O
layers	O
to	O
both	O
of	O
our	O
models	O
are	O
vector	Method
representations	Method
of	Method
individual	Method
words	Method
.	O

Learning	O
independent	Method
representations	Method
for	O
word	O
types	O
from	O
the	O
limited	O
NER	Task
training	O
data	O
is	O
a	O
difficult	O
problem	O
:	O
there	O
are	O
simply	O
too	O
many	O
parameters	O
to	O
reliably	O
estimate	O
.	O

Since	O
many	O
languages	O
have	O
orthographic	O
or	O
morphological	O
evidence	O
that	O
something	O
is	O
a	O
name	O
(	O
or	O
not	O
a	O
name	O
)	O
,	O
we	O
want	O
representations	O
that	O
are	O
sensitive	O
to	O
the	O
spelling	O
of	O
words	O
.	O

We	O
therefore	O
use	O
a	O
model	O
that	O
constructs	O
representations	O
of	O
words	O
from	O
representations	O
of	O
the	O
characters	O
they	O
are	O
composed	O
of	O
(	O
[	O
reference	O
]	O
)	O
.	O

Our	O
second	O
intuition	O
is	O
that	O
names	O
,	O
which	O
may	O
individually	O
be	O
quite	O
varied	O
,	O
appear	O
in	O
regular	O
contexts	O
in	O
large	O
corpora	O
.	O

Therefore	O
we	O
use	O
embeddings	O
learned	O
from	O
a	O
large	O
corpus	O
that	O
are	O
sensitive	O
to	O
word	O
order	O
(	O
[	O
reference	O
]	O
)	O
.	O

Finally	O
,	O
to	O
prevent	O
the	O
models	O
from	O
depending	O
on	O
one	O
representation	O
or	O
the	O
other	O
too	O
strongly	O
,	O
we	O
use	O
dropout	Method
training	Method
and	O
find	O
this	O
is	O
crucial	O
for	O
good	O
generalization	Task
performance	O
(	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Character	Method
-	Method
based	Method
models	Method
of	Method
words	Method
An	O
important	O
distinction	O
of	O
our	O
work	O
from	O
most	O
previous	O
approaches	O
is	O
that	O
we	O
learn	O
character	O
-	O
level	O
features	O
while	O
training	O
instead	O
of	O
hand	O
-	O
engineering	O
prefix	O
and	O
suffix	O
information	O
about	O
words	O
.	O

Learning	Task
character	Task
-	Task
level	Task
embeddings	Task
has	O
the	O
advantage	O
of	O
learning	O
representations	O
specific	O
to	O
the	O
task	O
and	O
domain	O
at	O
hand	O
.	O

They	O
have	O
been	O
found	O
useful	O
for	O
morphologically	O
rich	O
languages	O
and	O
to	O
handle	O
the	O
out	Task
-	Task
of	Task
-	Task
vocabulary	Task
problem	Task
for	O
tasks	O
like	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
and	O
language	Task
modeling	Task
or	O
dependency	Task
parsing	Task
.	O

Figure	O
[	O
reference	O
]	O
describes	O
our	O
architecture	O
to	O
generate	O
a	O
word	Method
embedding	Method
for	O
a	O
word	O
from	O
its	O
characters	O
.	O

A	O
character	O
lookup	O
table	O
initialized	O
at	O
random	O
contains	O
an	O
embedding	O
for	O
every	O
character	O
.	O

The	O
character	O
embeddings	O
corresponding	O
to	O
every	O
character	O
in	O
a	O
word	O
are	O
given	O
in	O
direct	O
and	O
reverse	O
order	O
to	O
a	O
forward	O
and	O
a	O
backward	O
LSTM	Method
.	O

The	O
embedding	O
for	O
a	O
word	O
derived	O
from	O
its	O
characters	O
is	O
the	O
concatenation	O
of	O
its	O
forward	Method
and	Method
backward	Method
representations	Method
from	O
the	O
bidirectional	O
LSTM	Method
.	O

This	O
character	Method
-	Method
level	Method
representation	Method
is	O
then	O
concatenated	O
with	O
a	O
word	Method
-	Method
level	Method
representation	Method
from	O
a	O
word	Method
lookup	Method
-	Method
table	Method
.	O

During	O
testing	O
,	O
words	O
that	O
do	O
not	O
have	O
an	O
embedding	O
in	O
the	O
lookup	O
table	O
are	O
mapped	O
to	O
a	O
UNK	Method
embedding	Method
.	O

To	O
train	O
the	O
UNK	Method
embedding	Method
,	O
we	O
replace	O
singletons	O
with	O
the	O
UNK	Method
embedding	Method
with	O
a	O
probability	O
.	O

In	O
all	O
our	O
experiments	O
,	O
the	O
hidden	O
dimension	O
of	O
the	O
forward	O
and	O
backward	O
character	O
LSTMs	Method
are	O
each	O
,	O
which	O
results	O
in	O
our	O
character	Method
-	Method
based	Method
representation	Method
of	Method
words	Method
being	O
of	O
dimension	O
.	O

Recurrent	Method
models	Method
like	O
RNNs	Method
and	O
LSTMs	Method
are	O
capable	O
of	O
encoding	O
very	O
long	O
sequences	O
,	O
however	O
,	O
they	O
have	O
a	O
representation	O
biased	O
towards	O
their	O
most	O
recent	O
inputs	O
.	O

As	O
a	O
result	O
,	O
we	O
expect	O
the	O
final	O
representation	O
of	O
the	O
forward	O
LSTM	Method
to	O
be	O
an	O
accurate	O
representation	O
of	O
the	O
suffix	O
of	O
the	O
word	O
,	O
and	O
the	O
final	O
state	O
of	O
the	O
backward	O
LSTM	Method
to	O
be	O
a	O
better	O
representation	O
of	O
its	O
prefix	O
.	O

Alternative	O
approaches	O
—	O
most	O
notably	O
like	O
convolutional	Method
networks	Method
—	O
have	O
been	O
proposed	O
to	O
learn	O
representations	Task
of	Task
words	Task
from	O
their	O
characters	O
.	O

However	O
,	O
convnets	Method
are	O
designed	O
to	O
discover	O
position	O
-	O
invariant	O
features	O
of	O
their	O
inputs	O
.	O

While	O
this	O
is	O
appropriate	O
for	O
many	O
problems	O
,	O
e.g.	O
,	O
image	Task
recognition	Task
(	O
a	O
cat	O
can	O
appear	O
anywhere	O
in	O
a	O
picture	O
)	O
,	O
we	O
argue	O
that	O
important	O
information	O
is	O
position	O
dependent	O
(	O
e.g.	O
,	O
prefixes	O
and	O
suffixes	O
encode	O
different	O
information	O
than	O
stems	O
)	O
,	O
making	O
LSTMs	Method
an	O
a	O
priori	O
better	O
function	O
class	O
for	O
modeling	O
the	O
relationship	O
between	O
words	O
and	O
their	O
characters	O
.	O

subsection	O
:	O
Pretrained	Method
embeddings	Method
As	O
in	O
collobert2011natural	O
,	O
we	O
use	O
pretrained	O
word	O
embeddings	O
to	O
initialize	O
our	O
lookup	O
table	O
.	O

We	O
observe	O
significant	O
improvements	O
using	O
pretrained	Method
word	Method
embeddings	Method
over	O
randomly	O
initialized	O
ones	O
.	O

Embeddings	Method
are	O
pretrained	O
using	O
skip	Method
-	Method
n	Method
-	Method
gram	Method
,	O
a	O
variation	O
of	O
word2vec	Method
that	O
accounts	O
for	O
word	O
order	O
.	O

These	O
embeddings	O
are	O
fine	O
-	O
tuned	O
during	O
training	O
.	O

Word	O
embeddings	O
for	O
Spanish	O
,	O
Dutch	O
,	O
German	O
and	O
English	Material
are	O
trained	O
using	O
the	O
Spanish	O
Gigaword	O
version	O
3	O
,	O
the	O
Leipzig	O
corpora	O
collection	O
,	O
the	O
German	O
monolingual	O
training	O
data	O
from	O
the	O
2010	O
Machine	Task
Translation	Task
Workshop	Task
and	O
the	O
English	O
Gigaword	O
version	O
4	O
(	O
with	O
the	O
LA	O
Times	O
and	O
NY	O
Times	O
portions	O
removed	O
)	O
respectively	O
.	O

We	O
use	O
an	O
embedding	O
dimension	O
of	O
for	O
English	Material
,	O
for	O
other	O
languages	O
,	O
a	O
minimum	O
word	O
frequency	O
cutoff	O
of	O
,	O
and	O
a	O
window	O
size	O
of	O
.	O

subsection	O
:	O
Dropout	Method
training	Method
Initial	O
experiments	O
showed	O
that	O
character	O
-	O
level	O
embeddings	O
did	O
not	O
improve	O
our	O
overall	O
performance	O
when	O
used	O
in	O
conjunction	O
with	O
pretrained	Method
word	Method
representations	Method
.	O

To	O
encourage	O
the	O
model	O
to	O
depend	O
on	O
both	O
representations	O
,	O
we	O
use	O
dropout	Method
training	Method
,	O
applying	O
a	O
dropout	O
mask	O
to	O
the	O
final	O
embedding	Method
layer	Method
just	O
before	O
the	O
input	O
to	O
the	O
bidirectional	O
LSTM	Method
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
observe	O
a	O
significant	O
improvement	O
in	O
our	O
model	O
’s	O
performance	O
after	O
using	O
dropout	Method
(	O
see	O
table	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Experiments	O
This	O
section	O
presents	O
the	O
methods	O
we	O
use	O
to	O
train	O
our	O
models	O
,	O
the	O
results	O
we	O
obtained	O
on	O
various	O
tasks	O
and	O
the	O
impact	O
of	O
our	O
networks	O
’	O
configuration	O
on	O
model	O
performance	O
.	O

subsection	O
:	O
Training	O
For	O
both	O
models	O
presented	O
,	O
we	O
train	O
our	O
networks	O
using	O
the	O
back	Method
-	Method
propagation	Method
algorithm	Method
updating	O
our	O
parameters	O
on	O
every	O
training	O
example	O
,	O
one	O
at	O
a	O
time	O
,	O
using	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
a	O
learning	O
rate	O
of	O
and	O
a	O
gradient	Method
clipping	Method
of	O
.	O

Several	O
methods	O
have	O
been	O
proposed	O
to	O
enhance	O
the	O
performance	O
of	O
SGD	Method
,	O
such	O
as	O
Adadelta	Method
or	O
Adam	Method
.	O

Although	O
we	O
observe	O
faster	O
convergence	Metric
using	O
these	O
methods	O
,	O
none	O
of	O
them	O
perform	O
as	O
well	O
as	O
SGD	Method
with	O
gradient	Method
clipping	Method
.	O

Our	O
LSTM	Method
-	O
CRF	Method
model	O
uses	O
a	O
single	Method
layer	Method
for	O
the	O
forward	O
and	O
backward	O
LSTMs	Method
whose	O
dimensions	O
are	O
set	O
to	O
.	O

Tuning	O
this	O
dimension	O
did	O
not	O
significantly	O
impact	O
model	O
performance	O
.	O

We	O
set	O
the	O
dropout	Metric
rate	Metric
to	O
.	O

Using	O
higher	O
rates	O
negatively	O
impacted	O
our	O
results	O
,	O
while	O
smaller	O
rates	O
led	O
to	O
longer	O
training	Metric
time	Metric
.	O

The	O
stack	O
-	O
LSTM	Method
model	O
uses	O
two	O
layers	O
each	O
of	O
dimension	O
for	O
each	O
stack	O
.	O

The	O
embeddings	O
of	O
the	O
actions	O
used	O
in	O
the	O
composition	Method
functions	Method
have	O
dimensions	O
each	O
,	O
and	O
the	O
output	O
embedding	O
is	O
of	O
dimension	O
.	O

We	O
experimented	O
with	O
different	O
dropout	Metric
rates	Metric
and	O
reported	O
the	O
scores	O
using	O
the	O
best	O
dropout	Metric
rate	Metric
for	O
each	O
language	O
.	O

It	O
is	O
a	O
greedy	Method
model	Method
that	O
apply	O
locally	O
optimal	O
actions	O
until	O
the	O
entire	O
sentence	O
is	O
processed	O
,	O
further	O
improvements	O
might	O
be	O
obtained	O
with	O
beam	Method
search	Method
or	O
training	O
with	O
exploration	Task
.	O

subsection	O
:	O
Data	O
Sets	O
We	O
test	O
our	O
model	O
on	O
different	O
datasets	O
for	O
named	Task
entity	Task
recognition	Task
.	O

To	O
demonstrate	O
our	O
model	O
’s	O
ability	O
to	O
generalize	O
to	O
different	O
languages	O
,	O
we	O
present	O
results	O
on	O
the	O
CoNLL	O
-	O
2002	O
and	O
CoNLL	Material
-	Material
2003	Material
datasets	Material
that	O
contain	O
independent	O
named	O
entity	O
labels	O
for	O
English	Material
,	O
Spanish	O
,	O
German	O
and	O
Dutch	O
.	O

All	O
datasets	O
contain	O
four	O
different	O
types	O
of	O
named	O
entities	O
:	O
locations	O
,	O
persons	O
,	O
organizations	O
,	O
and	O
miscellaneous	O
entities	O
that	O
do	O
not	O
belong	O
in	O
any	O
of	O
the	O
three	O
previous	O
categories	O
.	O

Although	O
POS	O
tags	O
were	O
made	O
available	O
for	O
all	O
datasets	O
,	O
we	O
did	O
not	O
include	O
them	O
in	O
our	O
models	O
.	O

We	O
did	O
not	O
perform	O
any	O
dataset	Task
preprocessing	Task
,	O
apart	O
from	O
replacing	O
every	O
digit	O
with	O
a	O
zero	O
in	O
the	O
English	O
NER	O
dataset	O
.	O

subsection	O
:	O
Results	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
comparisons	O
with	O
other	O
models	O
for	O
named	Task
entity	Task
recognition	Task
in	O
English	Material
.	O

To	O
make	O
the	O
comparison	O
between	O
our	O
model	O
and	O
others	O
fair	O
,	O
we	O
report	O
the	O
scores	O
of	O
other	O
models	O
with	O
and	O
without	O
the	O
use	O
of	O
external	O
labeled	O
data	O
such	O
as	O
gazetteers	O
and	O
knowledge	O
bases	O
.	O

Our	O
models	O
do	O
not	O
use	O
gazetteers	O
or	O
any	O
external	O
labeled	O
resources	O
.	O

The	O
best	O
score	O
reported	O
on	O
this	O
task	O
is	O
by	O
luojoint	O
.	O

They	O
obtained	O
a	O
F	Metric
of	O
91.2	O
by	O
jointly	O
modeling	O
the	O
NER	Task
and	O
entity	Task
linking	Task
tasks	Task
.	O

Their	O
model	O
uses	O
a	O
lot	O
of	O
hand	O
-	O
engineered	O
features	O
including	O
spelling	O
features	O
,	O
WordNet	O
clusters	O
,	O
Brown	O
clusters	O
,	O
POS	O
tags	O
,	O
chunks	O
tags	O
,	O
as	O
well	O
as	O
stemming	O
and	O
external	O
knowledge	O
bases	O
like	O
Freebase	O
and	O
Wikipedia	O
.	O

Our	O
LSTM	Method
-	O
CRF	Method
model	O
outperforms	O
all	O
other	O
systems	O
,	O
including	O
the	O
ones	O
using	O
external	O
labeled	O
data	O
like	O
gazetteers	O
.	O

Our	O
Stack	O
-	O
LSTM	Method
model	O
also	O
outperforms	O
all	O
previous	O
models	O
that	O
do	O
not	O
incorporate	O
external	O
features	O
,	O
apart	O
from	O
the	O
one	O
presented	O
by	O
chiu2015named	O
.	O

Tables	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
present	O
our	O
results	O
on	O
NER	Task
for	O
German	O
,	O
Dutch	O
and	O
Spanish	O
respectively	O
in	O
comparison	O
to	O
other	O
models	O
.	O

On	O
these	O
three	O
languages	O
,	O
the	O
LSTM	Method
-	O
CRF	Method
model	O
significantly	O
outperforms	O
all	O
previous	O
methods	O
,	O
including	O
the	O
ones	O
using	O
external	O
labeled	O
data	O
.	O

The	O
only	O
exception	O
is	O
Dutch	O
,	O
where	O
the	O
model	O
of	O
gillick2015multilingual	O
can	O
perform	O
better	O
by	O
leveraging	O
the	O
information	O
from	O
other	O
NER	Task
datasets	O
.	O

The	O
Stack	Method
-	Method
LSTM	Method
also	O
consistently	O
presents	O
state	O
-	O
the	O
-	O
art	O
(	O
or	O
close	O
to	O
)	O
results	O
compared	O
to	O
systems	O
that	O
do	O
not	O
use	O
external	O
data	O
.	O

As	O
we	O
can	O
see	O
in	O
the	O
tables	O
,	O
the	O
Stack	O
-	O
LSTM	Method
model	O
is	O
more	O
dependent	O
on	O
character	Method
-	Method
based	Method
representations	Method
to	O
achieve	O
competitive	O
performance	O
;	O
we	O
hypothesize	O
that	O
the	O
LSTM	Method
-	O
CRF	Method
model	O
requires	O
less	O
orthographic	O
information	O
since	O
it	O
gets	O
more	O
contextual	O
information	O
out	O
of	O
the	O
bidirectional	O
LSTMs	Method
;	O
however	O
,	O
the	O
Stack	O
-	O
LSTM	Method
model	O
consumes	O
the	O
words	O
one	O
by	O
one	O
and	O
it	O
just	O
relies	O
on	O
the	O
word	Method
representations	Method
when	O
it	O
chunks	O
words	O
.	O

subsection	O
:	O
Network	Method
architectures	Method
Our	O
models	O
had	O
several	O
components	O
that	O
we	O
could	O
tweak	O
to	O
understand	O
their	O
impact	O
on	O
the	O
overall	O
performance	O
.	O

We	O
explored	O
the	O
impact	O
that	O
the	O
CRF	Method
,	O
the	O
character	Method
-	Method
level	Method
representations	Method
,	O
pretraining	O
of	O
our	O
word	Method
embeddings	Method
and	O
dropout	Method
had	O
on	O
our	O
LSTM	Method
-	O
CRF	Method
model	O
.	O

We	O
observed	O
that	O
pretraining	O
our	O
word	Method
embeddings	Method
gave	O
us	O
the	O
biggest	O
improvement	O
in	O
overall	O
performance	O
of	O
in	O
F	Metric
.	O

The	O
CRF	Method
layer	O
gave	O
us	O
an	O
increase	O
of	O
,	O
while	O
using	O
dropout	O
resulted	O
in	O
a	O
difference	O
of	O
and	O
finally	O
learning	O
character	O
-	O
level	O
word	O
embeddings	O
resulted	O
in	O
an	O
increase	O
of	O
about	O
.	O

For	O
the	O
Stack	Method
-	Method
LSTM	Method
we	O
performed	O
a	O
similar	O
set	O
of	O
experiments	O
.	O

Results	O
with	O
different	O
architectures	O
are	O
given	O
in	O
table	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
CoNLL	Task
-	Task
2002	Task
shared	Task
task	Task
,	O
carreras2002named	O
obtained	O
among	O
the	O
best	O
results	O
on	O
both	O
Dutch	O
and	O
Spanish	O
by	O
combining	O
several	O
small	O
fixed	Method
-	Method
depth	Method
decision	Method
trees	Method
.	O

Next	O
year	O
,	O
in	O
the	O
CoNLL	Task
-	Task
2003	Task
Shared	Task
Task	Task
,	O
florian2003named	O
obtained	O
the	O
best	O
score	O
on	O
German	O
by	O
combining	O
the	O
output	O
of	O
four	O
diverse	Method
classifiers	Method
.	O

qi2009combining	O
later	O
improved	O
on	O
this	O
with	O
a	O
neural	Method
network	Method
by	O
doing	O
unsupervised	Method
learning	Method
on	O
a	O
massive	O
unlabeled	O
corpus	O
.	O

Several	O
other	O
neural	Method
architectures	Method
have	O
previously	O
been	O
proposed	O
for	O
NER	Task
.	O

For	O
instance	O
,	O
collobert2011natural	O
uses	O
a	O
CNN	Method
over	O
a	O
sequence	O
of	O
word	Method
embeddings	Method
with	O
a	O
CRF	Method
layer	O
on	O
top	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
our	O
first	O
model	O
without	O
character	O
-	O
level	O
embeddings	O
and	O
with	O
the	O
bidirectional	O
LSTM	Method
being	O
replaced	O
by	O
a	O
CNN	Method
.	O

More	O
recently	O
,	O
huang:2015	O
presented	O
a	O
model	O
similar	O
to	O
our	O
LSTM	Method
-	Method
CRF	Method
,	O
but	O
using	O
hand	O
-	O
crafted	O
spelling	O
features	O
.	O

zhou2015end	O
also	O
used	O
a	O
similar	O
model	O
and	O
adapted	O
it	O
to	O
the	O
semantic	Task
role	Task
labeling	Task
task	Task
.	O

lin2009phrase	O
used	O
a	O
linear	O
chain	O
CRF	Method
with	O
regularization	Method
,	O
they	O
added	O
phrase	O
cluster	O
features	O
extracted	O
from	O
the	O
web	O
data	O
and	O
spelling	O
features	O
.	O

passos2014lexicon	O
also	O
used	O
a	O
linear	O
chain	O
CRF	Method
with	O
spelling	O
features	O
and	O
gazetteers	O
.	O

Language	Method
independent	Method
NER	Method
models	Method
like	O
ours	O
have	O
also	O
been	O
proposed	O
in	O
the	O
past	O
.	O

Cucerzan	O
and	O
Yarowsky	O
cucerzan1999language	O
,	O
cucerzan2002language	O
present	O
semi	Method
-	Method
supervised	Method
bootstrapping	Method
algorithms	Method
for	O
named	Task
entity	Task
recognition	Task
by	O
co	O
-	O
training	O
character	O
-	O
level	O
(	O
word	O
-	O
internal	O
)	O
and	O
token	O
-	O
level	O
(	O
context	O
)	O
features	O
.	O

eisenstein2011structured	O
use	O
Bayesian	Method
nonparametrics	Method
to	O
construct	O
a	O
database	O
of	O
named	O
entities	O
in	O
an	O
almost	O
unsupervised	Task
setting	Task
.	O

ratinov2009design	O
quantitatively	O
compare	O
several	O
approaches	O
for	O
NER	Task
and	O
build	O
their	O
own	O
supervised	Method
model	Method
using	O
a	O
regularized	Method
average	Method
perceptron	Method
and	O
aggregating	Method
context	Method
information	Method
.	O

Finally	O
,	O
there	O
is	O
currently	O
a	O
lot	O
of	O
interest	O
in	O
models	O
for	O
NER	Task
that	O
use	O
letter	Method
-	Method
based	Method
representations	Method
.	O

gillick2015multilingual	O
model	O
the	O
task	O
of	O
sequence	Task
-	Task
labeling	Task
as	O
a	O
sequence	Task
to	Task
sequence	Task
learning	Task
problem	Task
and	O
incorporate	O
character	Method
-	Method
based	Method
representations	Method
into	O
their	O
encoder	Method
model	Method
.	O

chiu2015named	O
employ	O
an	O
architecture	O
similar	O
to	O
ours	O
,	O
but	O
instead	O
use	O
CNNs	Method
to	O
learn	O
character	O
-	O
level	O
features	O
,	O
in	O
a	O
way	O
similar	O
to	O
the	O
work	O
by	O
santos2015boosting	O
.	O

section	O
:	O
Conclusion	O
This	O
paper	O
presents	O
two	O
neural	Method
architectures	Method
for	O
sequence	Task
labeling	Task
that	O
provide	O
the	O
best	O
NER	Task
results	O
ever	O
reported	O
in	O
standard	O
evaluation	O
settings	O
,	O
even	O
compared	O
with	O
models	O
that	O
use	O
external	O
resources	O
,	O
such	O
as	O
gazetteers	O
.	O

A	O
key	O
aspect	O
of	O
our	O
models	O
are	O
that	O
they	O
model	O
output	O
label	O
dependencies	O
,	O
either	O
via	O
a	O
simple	O
CRF	Method
architecture	O
,	O
or	O
using	O
a	O
transition	Method
-	Method
based	Method
algorithm	Method
to	O
explicitly	O
construct	O
and	O
label	O
chunks	O
of	O
the	O
input	O
.	O

Word	Method
representations	Method
are	O
also	O
crucially	O
important	O
for	O
success	O
:	O
we	O
use	O
both	O
pre	O
-	O
trained	O
word	Method
representations	Method
and	O
“	O
character	Method
-	Method
based	Method
”	Method
representations	Method
that	O
capture	O
morphological	O
and	O
orthographic	O
information	O
.	O

To	O
prevent	O
the	O
learner	O
from	O
depending	O
too	O
heavily	O
on	O
one	O
representation	O
class	O
,	O
dropout	Method
is	O
used	O
.	O

section	O
:	O
Acknowledgments	O
This	O
work	O
was	O
sponsored	O
in	O
part	O
by	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Information	O
Innovation	O
Office	O
(	O
I2O	O
)	O
under	O
the	O
Low	O
Resource	O
Languages	O
for	O
Emergent	O
Incidents	O
(	O
LORELEI	O
)	O
program	O
issued	O
by	O
DARPA	O
/	O
I2O	O
under	O
Contract	O
No	O
.	O

HR0011	O
-	O
15	O
-	O
C	O
-	O
0114	O
.	O

Miguel	O
Ballesteros	O
is	O
supported	O
by	O
the	O
European	O
Commission	O
under	O
the	O
contract	O
numbers	O
FP7	O
-	O
ICT	O
-	O
610411	O
(	O
project	O
MULTISENSOR	O
)	O
and	O
H2020	O
-	O
RIA	O
-	O
645012	O
(	O
project	O
KRISTINA	O
)	O
.	O

bibliography	O
:	O
References	O
