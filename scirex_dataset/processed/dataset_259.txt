document	O
:	O
Image	Task
Captioning	Task
and	O
Visual	Task
Question	Task
Answering	Task
Based	O
on	O
Attributes	O
and	O
External	O
Knowledge	O
Much	O
of	O
the	O
recent	O
progress	O
in	O
Vision	Task
-	Task
to	Task
-	Task
Language	Task
problems	Task
has	O
been	O
achieved	O
through	O
a	O
combination	O
of	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
and	O
Recurrent	Method
Neural	Method
Networks	Method
(	O
RNNs	Method
)	O
.	O

This	O
approach	O
does	O
not	O
explicitly	O
represent	O
high	O
-	O
level	O
semantic	O
concepts	O
,	O
but	O
rather	O
seeks	O
to	O
progress	O
directly	O
from	O
image	O
features	O
to	O
text	O
.	O

In	O
this	O
paper	O
we	O
first	O
propose	O
a	O
method	O
of	O
incorporating	O
high	O
-	O
level	O
concepts	O
into	O
the	O
successful	O
CNN	Method
-	Method
RNN	Method
approach	Method
,	O
and	O
show	O
that	O
it	O
achieves	O
a	O
significant	O
improvement	O
on	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
both	O
image	Task
captioning	Task
and	O
visual	Task
question	Task
answering	Task
.	O

We	O
further	O
show	O
that	O
the	O
same	O
mechanism	O
can	O
be	O
used	O
to	O
incorporate	O
external	O
knowledge	O
,	O
which	O
is	O
critically	O
important	O
for	O
answering	Task
high	Task
level	Task
visual	Task
questions	Task
.	O

Specifically	O
,	O
we	O
design	O
a	O
visual	Method
question	Method
answering	Method
model	Method
that	O
combines	O
an	O
internal	Method
representation	Method
of	O
the	O
content	O
of	O
an	O
image	O
with	O
information	O
extracted	O
from	O
a	O
general	O
knowledge	O
base	O
to	O
answer	O
a	O
broad	O
range	O
of	O
image	Task
-	Task
based	Task
questions	Task
.	O

It	O
particularly	O
allows	O
questions	O
to	O
be	O
asked	O
where	O
the	O
image	O
alone	O
does	O
not	O
contain	O
the	O
the	O
information	O
required	O
to	O
select	O
the	O
appropriate	O
answer	O
.	O

Our	O
final	O
model	O
achieves	O
the	O
best	O
reported	O
results	O
for	O
both	O
image	Task
captioning	Task
and	O
visual	Task
question	Task
answering	Task
on	O
several	O
of	O
the	O
major	O
benchmark	O
datasets	O
.	O

Image	Task
Captioning	Task
,	O
Visual	Task
Question	Task
Answering	Task
,	O
Concepts	Task
Learning	Task
,	O
Recurrent	Method
Neural	Method
Networks	Method
,	O
LSTM	Method
.	O

section	O
:	O
Introduction	O
Vision	Task
-	Task
to	Task
-	Task
Language	Task
problems	Task
present	O
a	O
particular	O
challenge	O
in	O
Computer	Task
Vision	Task
because	O
they	O
require	O
translation	Task
between	O
two	O
different	O
forms	O
of	O
information	O
.	O

In	O
this	O
sense	O
the	O
problem	O
is	O
similar	O
to	O
that	O
of	O
machine	Task
translation	Task
between	Task
languages	Task
.	O

In	O
machine	Task
language	Task
translation	Task
there	O
have	O
been	O
a	O
series	O
of	O
results	O
showing	O
that	O
good	O
performance	O
can	O
be	O
achieved	O
without	O
developing	O
a	O
higher	O
-	O
level	O
model	O
of	O
the	O
state	O
of	O
the	O
world	O
.	O

In	O
,	O
for	O
instance	O
,	O
a	O
source	O
sentence	O
is	O
transformed	O
into	O
a	O
fixed	Method
-	Method
length	Method
vector	Method
representation	Method
by	O
an	O
‘	O
encoder	Method
’	Method
RNN	Method
,	O
which	O
in	O
turn	O
is	O
used	O
as	O
the	O
initial	O
hidden	O
state	O
of	O
a	O
‘	O
decoder	Method
’	Method
RNN	Method
that	O
generates	O
the	O
target	O
sentence	O
.	O

Despite	O
the	O
supposed	O
equivalence	O
between	O
an	O
image	O
and	O
a	O
thousand	O
words	O
,	O
the	O
manner	O
in	O
which	O
information	O
is	O
represented	O
in	O
each	O
data	O
form	O
could	O
hardly	O
be	O
more	O
different	O
.	O

Human	O
language	O
is	O
designed	O
specifically	O
so	O
as	O
to	O
communicate	O
information	O
between	O
humans	O
,	O
whereas	O
even	O
the	O
most	O
carefully	O
composed	O
image	O
is	O
the	O
culmination	O
of	O
a	O
complex	O
set	O
of	O
physical	O
processes	O
over	O
which	O
humans	O
have	O
little	O
control	O
.	O

Given	O
the	O
differences	O
between	O
these	O
two	O
forms	O
of	O
information	O
,	O
it	O
seems	O
surprising	O
that	O
methods	O
inspired	O
by	O
machine	Task
language	Task
translation	Task
have	O
been	O
so	O
successful	O
.	O

These	O
RNN	Method
-	Method
based	Method
methods	Method
which	O
translate	O
directly	O
from	O
image	O
features	O
to	O
text	O
,	O
without	O
developing	O
a	O
high	O
-	O
level	O
model	O
of	O
the	O
state	O
of	O
the	O
world	O
,	O
represent	O
the	O
current	O
state	O
of	O
the	O
art	O
for	O
key	Task
Vision	Task
-	Task
to	Task
-	Task
Language	Task
(	Task
V2L	Task
)	Task
problems	Task
,	O
such	O
as	O
image	Task
captioning	Task
and	O
visual	Task
question	Task
answering	Task
.	O

This	O
approach	O
is	O
reflected	O
in	O
many	O
recent	O
successful	O
works	O
on	O
image	Task
captioning	Task
,	O
such	O
as	O
.	O

Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
captioning	Method
methods	Method
use	O
a	O
CNN	Method
as	O
an	O
image	O
‘	O
encoder	Method
’	Method
to	O
produce	O
a	O
fixed	Method
-	Method
length	Method
vector	Method
representation	Method
,	O
which	O
is	O
then	O
fed	O
into	O
the	O
‘	O
decoder	Method
’	Method
RNN	Method
to	O
generate	O
a	O
caption	Task
.	O

Visual	Task
Question	Task
Answering	Task
(	O
VQA	Task
)	O
is	O
a	O
more	O
recent	O
challenge	O
than	O
image	Task
captioning	Task
.	O

It	O
is	O
distinct	O
from	O
many	O
problems	O
in	O
Computer	Task
Vision	Task
because	O
the	O
question	O
to	O
be	O
answered	O
is	O
not	O
determined	O
until	O
run	O
time	O
.	O

In	O
this	O
V2L	Task
problem	Task
an	O
image	O
and	O
a	O
free	O
-	O
form	O
,	O
open	O
-	O
ended	O
question	O
about	O
the	O
image	O
are	O
presented	O
to	O
the	O
method	O
which	O
is	O
required	O
to	O
produce	O
a	O
suitable	O
answer	O
.	O

As	O
in	O
image	Task
captioning	Task
,	O
the	O
current	O
state	O
of	O
the	O
art	O
in	O
VQA	Task
relies	O
on	O
passing	O
CNN	O
features	O
to	O
an	O
RNN	Method
language	Method
model	Method
.	O

However	O
,	O
visual	Task
question	Task
answering	Task
is	O
a	O
significantly	O
more	O
complex	O
problem	O
than	O
image	Task
captioning	Task
,	O
not	O
least	O
because	O
it	O
requires	O
accessing	O
information	O
not	O
present	O
in	O
the	O
image	O
.	O

This	O
may	O
be	O
common	O
sense	O
,	O
or	O
specific	O
knowledge	O
about	O
the	O
image	O
subject	O
.	O

For	O
example	O
,	O
given	O
an	O
image	O
,	O
such	O
as	O
Figure	O
[	O
reference	O
]	O
,	O
showing	O
‘	O
a	O
group	O
of	O
people	O
enjoying	O
a	O
sunny	O
day	O
at	O
the	O
beach	O
with	O
umbrellas	O
’	O
,	O
if	O
one	O
asks	O
a	O
question	O
‘	O
why	O
do	O
they	O
have	O
umbrellas	O
?	O
’	O
,	O
to	O
answer	O
this	O
question	O
,	O
the	O
machine	O
must	O
not	O
only	O
detect	O
the	O
scene	O
‘	O
beach	O
’	O
,	O
but	O
must	O
know	O
that	O
‘	O
umbrellas	O
are	O
often	O
used	O
as	O
points	O
of	O
shade	O
on	O
a	O
sunny	O
beach	O
’	O
.	O

Recently	O
,	O
Antol	O
et	O
al	O
.	O

also	O
have	O
suggested	O
that	O
VQA	Method
is	O
a	O
more	O
“	O
AI	O
-	O
complete	O
”	O
task	O
since	O
it	O
requires	O
multimodal	O
knowledge	O
beyond	O
a	O
single	O
sub	O
-	O
domain	O
.	O

The	O
contributions	O
of	O
this	O
paper	O
are	O
two	O
-	O
fold	O
.	O

First	O
,	O
we	O
propose	O
a	O
fully	Method
trainable	Method
attribute	Method
-	Method
based	Method
neural	Method
network	Method
founded	O
upon	O
the	O
CNN	Method
+	Method
RNN	Method
architecture	Method
,	O
that	O
can	O
be	O
applied	O
to	O
multiple	O
V2L	Task
problems	Task
.	O

We	O
do	O
this	O
by	O
inserting	O
an	O
explicit	O
representation	O
of	O
attributes	O
of	O
the	O
scene	O
which	O
are	O
meaningful	O
to	O
humans	O
.	O

Each	O
semantic	O
attribute	O
corresponds	O
to	O
a	O
word	O
mined	O
from	O
the	O
training	O
image	O
descriptions	O
,	O
and	O
represents	O
higher	O
-	O
level	O
knowledge	O
about	O
the	O
content	O
of	O
the	O
image	O
.	O

A	O
CNN	Method
-	Method
based	Method
classifier	Method
is	O
trained	O
for	O
each	O
attribute	O
,	O
and	O
the	O
set	O
of	O
attribute	O
likelihoods	O
for	O
an	O
image	O
form	O
a	O
high	O
-	O
level	O
representation	Task
of	Task
image	Task
content	Task
.	O

An	O
RNN	Method
is	O
then	O
trained	O
to	O
generate	O
captions	O
,	O
or	O
question	O
answers	O
,	O
on	O
the	O
basis	O
of	O
the	O
likelihoods	O
.	O

Our	O
attribute	Method
-	Method
based	Method
model	Method
yields	O
significantly	O
better	O
performance	O
than	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
in	O
the	O
task	O
of	O
image	Task
captioning	Task
.	O

Based	O
on	O
the	O
proposed	O
attribute	Method
-	Method
based	Method
V2L	Method
model	Method
,	O
our	O
second	O
contribution	O
is	O
to	O
introduce	O
a	O
method	O
of	O
incorporating	O
knowledge	O
external	O
to	O
the	O
image	O
,	O
including	O
common	O
sense	O
,	O
into	O
the	O
VQA	Method
process	Method
.	O

In	O
this	O
work	O
,	O
we	O
fuse	O
the	O
automatically	O
generated	O
description	O
of	O
an	O
image	O
with	O
information	O
extracted	O
from	O
an	O
external	O
knowledge	O
base	O
(	O
KB	O
)	O
to	O
provide	O
an	O
answer	O
to	O
a	O
general	O
question	O
about	O
the	O
image	O
(	O
See	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
image	O
description	O
takes	O
the	O
form	O
of	O
a	O
set	O
of	O
captions	Method
,	O
and	O
the	O
external	Material
knowledge	Material
is	O
text	O
-	O
based	O
information	O
mined	O
from	O
a	O
Knowledge	O
Base	O
.	O

Specifically	O
,	O
for	O
each	O
of	O
the	O
top	O
-	O
attributes	O
detected	O
in	O
the	O
image	O
we	O
generate	O
a	O
query	O
which	O
may	O
be	O
applied	O
to	O
a	O
Resource	Method
Description	Method
Framework	Method
(	O
RDF	Material
)	Material
KB	Material
,	O
such	O
as	O
DBpedia	Material
.	O

RDF	Method
is	O
the	O
standard	O
format	O
for	O
large	Material
KBs	Material
,	O
of	O
which	O
there	O
are	O
many	O
.	O

The	O
queries	O
are	O
specified	O
using	O
Semantic	Method
Protocol	Method
And	O
RDF	Method
Query	Method
Language	Method
(	O
SPARQL	Method
)	O
.	O

We	O
encode	O
the	O
paragraphs	O
extracted	O
from	O
the	O
KB	O
using	O
Doc2Vec	Method
,	O
which	O
maps	O
paragraphs	O
into	O
a	O
fixed	Method
-	Method
length	Method
feature	Method
representation	Method
.	O

The	O
encoded	O
attributes	O
,	O
captions	O
,	O
and	O
KB	O
information	O
are	O
then	O
input	O
to	O
an	O
LSTM	Method
which	O
is	O
trained	O
so	O
as	O
to	O
maximise	O
the	O
likelihood	O
of	O
the	O
ground	O
truth	O
answers	O
in	O
a	O
training	O
set	O
.	O

We	O
further	O
propose	O
a	O
question	Method
-	Method
guided	Method
knowledge	Method
selection	Method
scheme	Method
to	O
improve	O
the	O
quality	O
of	O
the	O
extracted	O
KB	O
information	O
.	O

The	O
knowledge	O
that	O
is	O
not	O
related	O
to	O
the	O
question	O
is	O
filtered	O
out	O
.	O

The	O
approach	O
that	O
we	O
propose	O
here	O
combines	O
the	O
generality	O
of	O
information	O
that	O
using	O
a	O
KB	Method
allows	O
with	O
the	O
generality	O
of	O
questions	O
that	O
the	O
LSTM	Method
allows	O
.	O

In	O
addition	O
,	O
it	O
achieves	O
an	O
accuracy	Metric
of	O
70.98	O
%	O
on	O
the	O
Toronto	Material
COCO	Material
-	Material
QA	Material
,	O
while	O
the	O
latest	O
state	O
of	O
the	O
art	O
is	O
61.60	O
%	O
.	O

On	O
the	O
VQA	Task
evaluation	Task
server	Task
(	O
which	O
does	O
not	O
publish	O
ground	O
truth	O
answers	O
for	O
its	O
test	O
set	O
)	O
,	O
we	O
also	O
produce	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
,	O
which	O
is	O
59.50	O
%	O
.	O

A	O
preliminary	O
version	O
of	O
this	O
work	O
was	O
published	O
at	O
CVPR	O
2016	O
.	O

The	O
new	O
material	O
in	O
this	O
paper	O
comprises	O
further	O
experiments	O
on	O
two	O
additional	O
VQA	Material
datasets	Material
.	O

More	O
ablation	Method
models	Method
of	O
the	O
original	O
model	O
are	O
implemented	O
and	O
studied	O
.	O

More	O
importantly	O
,	O
a	O
new	O
model	O
(	O
A	O
+	O
C	O
+	O
Selected	O
-	Method
K	Method
-	Method
LSTM	Method
)	O
is	O
introduced	O
for	O
the	O
visual	Task
question	Task
answering	Task
task	Task
,	O
leading	O
to	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O

section	O
:	O
Related	O
work	O
subsection	O
:	O
Attribute	Method
-	Method
based	Method
Representation	Method
Using	O
attribute	Method
-	Method
based	Method
models	Method
as	O
a	O
high	O
-	O
level	Method
representation	Method
has	O
shown	O
potential	O
in	O
many	O
computer	Task
vision	Task
tasks	Task
such	O
as	O
object	Task
recognition	Task
,	O
image	Task
annotation	Task
and	O
image	Task
retrieval	Task
.	O

Farhadi	O
et	O
al	O
.	O

were	O
among	O
the	O
first	O
to	O
propose	O
to	O
use	O
a	O
set	O
of	O
visual	O
semantic	O
attributes	O
to	O
identify	O
familiar	O
objects	O
,	O
and	O
to	O
describe	O
unfamiliar	O
objects	O
.	O

Vogel	O
and	O
Schiele	O
used	O
visual	O
attributes	O
describing	O
scenes	O
to	O
characterize	O
image	O
regions	O
and	O
combined	O
these	O
local	O
semantics	O
into	O
a	O
global	Task
image	Task
description	Task
.	O

Su	O
et	O
al	O
.	O

defined	O
six	O
groups	O
of	O
attributes	O
to	O
build	O
intermediate	O
-	O
level	O
features	O
for	O
image	Task
classification	Task
.	O

Li	O
et	O
al	O
.	O

introduced	O
the	O
concept	O
of	O
an	O
‘	O
object	O
bank	O
’	O
which	O
enables	O
objects	O
to	O
be	O
used	O
as	O
attributes	O
for	O
scene	Task
representation	Task
.	O

subsection	O
:	O
Image	Task
Captioning	Task
The	O
problem	O
of	O
annotating	Task
images	Task
with	O
natural	O
language	O
at	O
the	O
scene	O
level	O
has	O
long	O
been	O
studied	O
in	O
both	O
computer	Task
vision	Task
and	O
natural	Task
language	Task
processing	Task
.	O

Hodosh	O
et	O
al	O
.	O

proposed	O
to	O
frame	O
sentence	Task
-	Task
based	Task
image	Task
annotation	Task
as	O
the	O
task	O
of	O
ranking	Task
a	O
given	O
pool	O
of	O
captions	O
.	O

Similarly	O
,	O
posed	O
the	O
task	O
as	O
a	O
retrieval	Task
problem	Task
,	O
but	O
based	O
on	O
co	Task
-	Task
embedding	Task
of	Task
images	Task
and	O
text	O
in	O
the	O
same	O
space	O
.	O

Recently	O
,	O
Socher	O
et	O
al	O
.	O

used	O
neural	Method
networks	Method
to	O
co	O
-	O
embed	O
image	Material
and	Material
sentences	Material
together	O
and	O
Karpathy	O
et	O
al	O
.	O

co	O
-	O
embedded	O
image	Material
crops	Material
and	O
sub	O
-	O
sentences	O
.	O

Attributes	Method
have	O
been	O
used	O
in	O
many	O
image	Method
captioning	Method
methods	Method
to	O
fill	O
the	O
gaps	O
in	O
predetermined	O
caption	O
templates	O
.	O

Farhadi	O
et	O
al	O
.	O

,	O
for	O
instance	O
,	O
used	O
detections	O
to	O
infer	O
a	O
triplet	O
of	O
scene	O
elements	O
which	O
is	O
converted	O
to	O
text	O
using	O
a	O
template	O
.	O

Li	O
et	O
al	O
.	O

composed	O
image	Material
descriptions	Material
given	O
computer	O
vision	O
based	O
inputs	O
such	O
as	O
detected	O
objects	O
,	O
modifiers	O
and	O
locations	O
using	O
web	O
-	O
scale	O
-	O
grams	O
.	O

Zhu	O
et	O
al	O
.	O

converted	O
image	Task
parsing	Task
results	O
into	O
a	O
semantic	Method
representation	Method
in	O
the	O
form	O
of	O
Web	O
Ontology	O
Language	O
,	O
which	O
is	O
converted	O
to	O
human	Material
readable	Material
text	Material
.	O

A	O
more	O
sophisticated	O
CRF	Method
-	Method
based	Method
method	Method
use	O
of	O
attribute	O
detections	O
beyond	O
triplets	O
was	O
proposed	O
by	O
Kulkarni	O
et	O
al	O
.	O

The	O
advantage	O
of	O
template	Method
-	Method
based	Method
methods	Method
is	O
that	O
the	O
resulting	O
captions	O
are	O
more	O
likely	O
to	O
be	O
grammatically	O
correct	O
.	O

The	O
drawback	O
is	O
that	O
they	O
still	O
rely	O
on	O
hard	O
-	O
coded	O
visual	O
concepts	O
and	O
suffer	O
the	O
implied	O
limits	O
on	O
the	O
variety	O
of	O
the	O
output	O
.	O

Fang	O
et	O
al	O
.	O

won	O
the	O
2015	O
COCO	Task
Captioning	Task
Challenge	Task
with	O
an	O
approach	O
that	O
is	O
similar	O
to	O
ours	O
in	O
as	O
much	O
as	O
it	O
applies	O
a	O
visual	O
concept	O
(	O
i.e.	O
,	O
attribute	Method
)	Method
detection	Method
process	Method
before	O
generating	O
sentences	O
.	O

They	O
first	O
learned	O
independent	Method
detectors	Method
for	O
visual	O
words	O
based	O
on	O
a	O
multi	Method
-	Method
instance	Method
learning	Method
framework	Method
and	O
then	O
used	O
a	O
maximum	Method
entropy	Method
language	Method
model	Method
conditioned	O
on	O
the	O
set	O
of	O
visually	O
detected	O
words	O
directly	O
to	O
generate	O
captions	O
.	O

In	O
contrast	O
to	O
the	O
aforementioned	O
two	O
-	O
stage	Method
methods	Method
,	O
the	O
recent	O
dominant	O
trend	O
in	O
V2L	Task
is	O
to	O
use	O
an	O
architecture	O
which	O
connects	O
a	O
CNN	Method
to	O
an	O
RNN	Method
to	O
learn	O
the	O
mapping	O
from	O
images	O
to	O
sentences	O
directly	O
.	O

Mao	O
et	O
al	O
.	O

,	O
for	O
instance	O
,	O
proposed	O
a	O
multimodal	Method
RNN	Method
(	O
m	Method
-	Method
RNN	Method
)	O
to	O
estimate	O
the	O
probability	O
distribution	O
of	O
the	O
next	O
word	O
given	O
previous	O
words	O
and	O
the	O
deep	O
CNN	O
feature	O
of	O
an	O
image	O
at	O
each	O
time	O
step	O
.	O

Similarly	O
,	O
Kiros	O
et	O
al	O
.	O

constructed	O
a	O
joint	Method
multimodal	Method
embedding	Method
space	Method
using	O
a	O
powerful	O
deep	Method
CNN	Method
model	Method
and	O
an	O
LSTM	Method
that	O
encodes	O
text	Material
.	O

Karpathy	O
and	O
Li	O
also	O
proposed	O
a	O
multimodal	Method
RNN	Method
generative	Method
model	Method
,	O
but	O
in	O
contrast	O
to	O
,	O
their	O
RNN	Method
is	O
conditioned	O
on	O
the	O
image	O
information	O
only	O
at	O
the	O
first	O
time	O
step	O
.	O

Vinyals	O
et	O
al	O
.	O

combined	O
deep	Method
CNNs	Method
for	O
image	Task
classification	Task
with	O
an	O
LSTM	Method
for	O
sequence	Task
modeling	Task
,	O
to	O
create	O
a	O
single	O
network	O
that	O
generates	O
descriptions	Task
of	Task
images	Task
.	O

Chen	O
et	O
al	O
.	O

learn	O
a	O
bi	Method
-	Method
directional	Method
mapping	Method
between	O
images	O
and	O
their	O
sentence	O
-	O
based	O
descriptions	O
,	O
which	O
allows	O
to	O
reconstruct	O
visual	O
features	O
given	O
an	O
image	O
description	O
.	O

Xu	O
et	O
al	O
.	O

proposed	O
a	O
model	O
based	O
on	O
visual	O
attention	O
.	O

Jia	O
et	O
al	O
.	O

applied	O
additional	O
retrieved	O
sentences	O
to	O
guide	O
the	O
LSTM	Method
in	O
generating	Task
captions	Task
.	O

Interestingly	O
,	O
this	O
end	Method
-	Method
to	Method
-	Method
end	Method
CNN	Method
-	Method
RNN	Method
approach	Method
ignores	O
the	O
image	Task
-	Task
to	Task
-	Task
word	Task
mapping	Task
which	O
was	O
an	O
essential	O
step	O
in	O
many	O
of	O
the	O
previous	O
image	Method
captioning	Method
systems	Method
detailed	O
above	O
.	O

The	O
CNN	Method
-	Method
RNN	Method
approach	Method
has	O
the	O
advantage	O
that	O
it	O
is	O
able	O
to	O
generate	O
a	O
wider	O
variety	O
of	O
captions	O
,	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
and	O
outperforms	O
the	O
previous	O
approach	O
on	O
the	O
benchmarks	O
.	O

It	O
is	O
not	O
clear	O
,	O
however	O
,	O
what	O
the	O
impact	O
of	O
bypassing	O
the	O
intermediate	O
high	Method
-	Method
level	Method
representation	Method
is	O
,	O
and	O
particularly	O
to	O
what	O
extent	O
the	O
RNN	Method
language	Method
model	Method
might	O
be	O
compensating	O
.	O

Donahue	O
et	O
al	O
.	O

described	O
an	O
experiment	O
,	O
for	O
example	O
,	O
using	O
tags	Method
and	Method
CRF	Method
models	Method
as	O
a	O
mid	Method
-	Method
layer	Method
representation	Method
for	O
video	Material
to	O
generate	O
descriptions	O
,	O
but	O
it	O
was	O
designed	O
to	O
prove	O
that	O
LSTM	Method
outperforms	O
an	O
SMT	Method
-	Method
based	Method
approach	Method
.	O

It	O
remains	O
unclear	O
whether	O
the	O
mid	Method
-	Method
layer	Method
representation	Method
or	O
the	O
LSTM	Method
leads	O
to	O
the	O
success	O
.	O

Our	O
paper	O
provides	O
several	O
well	O
-	O
designed	O
experiments	O
to	O
answer	O
this	O
question	O
.	O

We	O
thus	O
here	O
show	O
not	O
only	O
a	O
method	O
for	O
introducing	O
a	O
high	Method
-	Method
level	Method
representation	Method
into	O
the	O
CNN	Method
-	Method
RNN	Method
framework	Method
,	O
and	O
that	O
doing	O
so	O
improves	O
performance	O
,	O
but	O
we	O
also	O
investigate	O
the	O
value	O
of	O
high	O
-	O
level	O
information	O
more	O
broadly	O
in	O
V2L	Task
tasks	Task
.	O

This	O
is	O
of	O
critical	O
importance	O
at	O
this	O
time	O
because	O
V2L	Task
has	O
a	O
long	O
way	O
to	O
go	O
,	O
particularly	O
in	O
the	O
generality	O
of	O
the	O
images	O
and	O
text	O
it	O
is	O
applicable	O
to	O
.	O

subsection	O
:	O
Visual	Task
Question	Task
Answering	Task
Malinowski	O
et	O
al	O
.	O

may	O
be	O
the	O
first	O
to	O
study	O
the	O
VQA	Task
problem	Task
.	O

They	O
proposed	O
a	O
method	O
that	O
combines	O
semantic	Task
parsing	Task
and	O
image	Task
segmentation	Task
with	O
a	O
Bayesian	Method
approach	Method
to	O
sampling	O
from	O
nearest	O
neighbors	O
in	O
the	O
training	O
set	O
.	O

Tu	O
et	O
al	O
.	O

built	O
a	O
query	Method
answering	Method
system	Method
based	O
on	O
a	O
joint	Method
parse	Method
graph	Method
from	O
text	Material
and	Material
videos	Material
.	O

Geman	O
et	O
al	O
.	O

proposed	O
an	O
automatic	Method
‘	Method
query	Method
generator	Method
’	Method
that	O
is	O
trained	O
on	O
annotated	Material
images	Material
and	O
produces	O
a	O
sequence	O
of	O
binary	Material
questions	Material
from	O
any	O
given	O
test	O
image	O
.	O

Each	O
of	O
these	O
approaches	O
places	O
significant	O
limitations	O
on	O
the	O
form	O
of	O
question	O
that	O
can	O
be	O
answered	O
.	O

Most	O
recently	O
,	O
inspired	O
by	O
the	O
significant	O
progress	O
achieved	O
using	O
deep	Method
neural	Method
network	Method
models	Method
in	O
both	O
computer	Task
vision	Task
and	O
natural	Task
language	Task
processing	Task
,	O
an	O
architecture	O
which	O
combines	O
a	O
CNN	Method
and	O
RNN	Method
to	O
learn	O
the	O
mapping	O
from	O
images	Material
to	O
sentences	O
has	O
become	O
the	O
dominant	O
trend	O
.	O

Both	O
Gao	O
et	O
al	O
.	O

and	O
Malinowski	O
et	O
al	O
.	O

used	O
RNNs	Method
to	O
encode	O
the	O
question	O
and	O
output	O
the	O
answer	O
.	O

Whereas	O
Gao	O
et	O
al	O
.	O

used	O
two	O
networks	O
,	O
a	O
separate	O
encoder	Method
and	Method
decoder	Method
,	O
Malinowski	O
et	O
al	O
.	O

used	O
a	O
single	O
network	O
for	O
both	O
encoding	Task
and	Task
decoding	Task
.	O

Ren	O
et	O
al	O
.	O

focused	O
on	O
questions	O
with	O
a	O
single	O
-	O
word	O
answer	O
and	O
formulated	O
the	O
task	O
as	O
a	O
classification	Task
problem	Task
using	O
an	O
LSTM	Method
.	O

Antol	O
et	O
al	O
.	O

proposed	O
a	O
large	O
-	O
scale	Material
open	Material
-	Material
ended	Material
VQA	Material
dataset	Material
based	O
on	O
COCO	Method
,	O
which	O
is	O
called	O
VQA	Method
.	O

Inspired	O
by	O
Xu	O
et	O
al	O
.	O

who	O
encode	O
visual	O
attention	O
in	O
the	O
Image	Task
Captioning	Task
,	O
propose	O
to	O
use	O
the	O
spatial	O
attention	O
to	O
help	O
answering	Task
visual	Task
questions	Task
.	O

formulate	O
the	O
VQA	Method
as	O
a	O
classification	Task
problem	Task
and	O
restrict	O
the	O
answer	O
only	O
can	O
be	O
drawn	O
from	O
a	O
fixed	O
answer	O
space	O
.	O

Our	O
framework	O
also	O
exploits	O
both	O
CNN	Method
and	Method
RNNs	Method
,	O
but	O
in	O
contrast	O
to	O
preceding	O
approaches	O
which	O
use	O
only	O
image	O
features	O
extracted	O
from	O
a	O
CNN	Method
in	O
answering	O
a	O
question	O
,	O
we	O
employ	O
multiple	O
sources	O
,	O
including	O
image	Material
content	Material
,	O
generated	O
image	Material
captions	Material
and	O
mined	O
external	O
knowledge	O
,	O
to	O
feed	O
to	O
an	O
RNN	Method
to	O
answer	O
questions	O
.	O

Large	Material
-	Material
scale	Material
Knowledge	Material
Bases	Material
(	O
KBs	Material
)	O
,	O
such	O
as	O
Freebase	Material
and	O
DBpedia	Material
,	O
have	O
been	O
used	O
successfully	O
in	O
several	O
natural	Task
language	Task
Question	Task
Answering	Task
(	Task
QA	Task
)	Task
systems	Task
.	O

However	O
,	O
VQA	Method
systems	Method
exploiting	O
KBs	Method
are	O
still	O
relatively	O
rare	O
.	O

Zhu	O
et	O
al	O
.	O

used	O
a	O
hand	O
-	O
crafted	O
KB	O
primarily	O
containing	O
image	O
-	O
related	O
information	O
such	O
as	O
category	O
labels	O
,	O
attribute	O
labels	O
and	O
affordance	O
labels	O
,	O
but	O
also	O
some	O
quantities	O
relating	O
to	O
their	O
specific	O
question	O
format	O
such	O
as	O
GPS	O
coordinates	O
and	O
similar	O
.	O

Instead	O
of	O
building	O
a	O
problem	Task
-	Task
specific	Task
KB	Task
,	O
we	O
use	O
a	O
pre	O
-	O
built	O
large	Material
-	Material
scale	Material
KB	Material
(	O
DBpedia	Material
)	O
from	O
which	O
we	O
extract	O
information	O
using	O
a	O
standard	O
RDF	Method
query	Method
language	Method
.	O

DBpedia	Material
has	O
been	O
created	O
by	O
extracting	O
structured	Material
information	Material
from	O
Wikipedia	Material
,	O
and	O
is	O
thus	O
significantly	O
larger	O
and	O
more	O
general	O
than	O
a	O
hand	O
-	O
crafted	O
KB	O
.	O

Rather	O
than	O
having	O
a	O
user	O
pose	O
their	O
question	O
in	O
a	O
formal	O
query	O
language	O
,	O
our	O
VQA	Method
system	Method
is	O
able	O
to	O
encode	O
questions	O
written	O
in	O
natural	Material
language	Material
automatically	O
.	O

This	O
is	O
achieved	O
without	O
manually	O
specified	O
formalization	O
,	O
but	O
rather	O
depends	O
on	O
processing	O
a	O
suitable	O
training	O
set	O
.	O

The	O
result	O
is	O
a	O
model	O
which	O
is	O
very	O
general	O
in	O
the	O
forms	O
of	O
question	O
that	O
it	O
will	O
accept	O
.	O

The	O
quality	O
of	O
the	O
information	O
in	O
the	O
KB	O
is	O
one	O
of	O
the	O
primary	O
issues	O
in	O
this	O
approach	O
to	O
VQA	Task
.	O

The	O
problem	O
is	O
that	O
KBs	O
constructed	O
by	O
analysing	O
Wikipedia	Material
and	O
similar	Material
are	O
patchy	O
and	O
inconsistent	O
at	O
best	O
,	O
and	O
hand	Material
-	Material
curated	Material
KBs	Material
are	O
inevitably	O
very	O
topic	O
specific	O
.	O

Using	O
visually	O
-	O
sourced	O
information	O
is	O
a	O
promising	O
approach	O
to	O
solve	O
this	O
problem	O
,	O
but	O
has	O
a	O
way	O
to	O
go	O
before	O
it	O
might	O
be	O
usefully	O
applied	O
within	O
our	O
approach	O
.	O

After	O
inspecting	O
the	O
database	O
shows	O
that	O
the	O
âcommentâ	O
field	O
is	O
the	O
most	O
generally	O
informative	O
about	O
an	O
attribute	O
,	O
as	O
it	O
contains	O
a	O
general	O
text	O
description	O
of	O
it	O
.	O

We	O
therefore	O
find	O
this	O
is	O
still	O
a	O
feasible	O
solution	O
.	O

section	O
:	O
Image	Task
Captioning	Task
using	O
Attributes	O
Our	O
image	Method
captioning	Method
model	Method
is	O
summarized	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
model	O
includes	O
an	O
image	Method
analysis	Method
part	Method
and	O
a	O
caption	Method
generation	Method
part	Method
.	O

In	O
the	O
image	Task
analysis	Task
part	Task
,	O
we	O
first	O
use	O
supervised	Method
learning	Method
to	O
predict	O
a	O
set	O
of	O
attributes	O
,	O
based	O
on	O
words	O
commonly	O
found	O
in	O
image	Material
captions	Material
We	O
solve	O
this	O
as	O
a	O
multi	Task
-	Task
label	Task
classification	Task
problem	Task
and	O
train	O
a	O
corresponding	O
deep	Method
CNN	Method
by	O
minimizing	O
an	O
element	Method
-	Method
wise	Method
logistic	Method
loss	Method
function	Method
.	O

Secondly	O
,	O
a	O
fixed	O
length	O
vector	O
is	O
created	O
for	O
each	O
image	O
,	O
whose	O
length	O
is	O
the	O
size	O
of	O
the	O
attribute	O
set	O
.	O

Each	O
dimension	O
of	O
the	O
vector	O
contains	O
the	O
prediction	O
probability	O
for	O
a	O
particular	O
attribute	O
.	O

In	O
the	O
captioning	Task
generation	Task
part	Task
,	O
we	O
apply	O
an	O
LSTM	Method
-	Method
based	Method
sentence	Method
generator	Method
.	O

In	O
the	O
baseline	O
model	O
,	O
as	O
in	O
we	O
use	O
a	O
pre	Method
-	Method
trained	Method
CNN	Method
to	O
extract	O
image	O
features	O
which	O
are	O
fed	O
into	O
the	O
LSTM	Method
directly	O
.	O

For	O
the	O
sake	O
of	O
completeness	O
a	O
fine	O
-	O
tuned	O
version	O
of	O
this	O
approach	O
is	O
also	O
implemented	O
.	O

subsection	O
:	O
Attribute	Method
-	Method
based	Method
Image	Method
Representation	Method
Our	O
first	O
task	O
is	O
to	O
describe	O
the	O
image	O
content	O
in	O
terms	O
of	O
a	O
set	O
of	O
attributes	O
.	O

An	O
attribute	O
vocabulary	O
is	O
first	O
constructed	O
.	O

Unlike	O
,	O
that	O
use	O
a	O
vocabulary	O
from	O
separate	O
hand	Material
-	Material
labeled	Material
training	Material
data	Material
,	O
our	O
semantic	O
attributes	O
are	O
extracted	O
from	O
training	O
captions	O
and	O
can	O
be	O
any	O
part	O
of	O
speech	O
,	O
including	O
object	O
names	O
(	O
nouns	O
)	O
,	O
motions	O
(	O
verbs	O
)	O
or	O
properties	O
(	O
adjectives	O
)	O
.	O

The	O
direct	O
use	O
of	O
captions	O
guarantees	O
that	O
the	O
most	O
salient	O
attributes	O
for	O
an	O
image	O
set	O
are	O
extracted	O
.	O

We	O
use	O
the	O
(	O
)	O
most	O
common	O
words	O
in	O
the	O
training	O
captions	O
to	O
determine	O
the	O
attribute	O
vocabulary	O
.	O

Similar	O
to	O
,	O
the	O
top	O
most	O
frequent	O
closed	O
-	O
class	O
words	O
such	O
as	O
`	O
a'	O
,	O
`on'	O
,	O
`of	O
'	O
are	O
removed	O
since	O
they	O
are	O
in	O
nearly	O
every	O
caption	O
.	O

In	O
contrast	O
to	O
,	O
our	O
vocabulary	O
is	O
not	O
tense	O
or	O
plurality	O
sensitive	O
,	O
for	O
instance	O
,	O
`	O
ride	O
'	O
and	O
`	O
riding	O
'	O
are	O
classified	O
as	O
the	O
same	O
semantic	O
attribute	O
,	O
similarly	O
`	O
bag	O
'	O
and	O
`	O
bags	O
'	O
.	O

This	O
significantly	O
decreases	O
the	O
size	O
of	O
our	O
attribute	O
vocabulary	O
.	O

The	O
full	O
list	O
of	O
attributes	O
can	O
be	O
found	O
in	O
the	O
supplementary	O
material	O
.	O

Our	O
attributes	O
represent	O
a	O
set	O
of	O
high	O
-	O
level	O
semantic	O
constructs	O
,	O
the	O
totality	O
of	O
which	O
the	O
LSTM	Method
then	O
attempts	O
to	O
represent	O
in	O
sentence	O
form	O
.	O

Generating	O
a	O
sentence	O
from	O
a	O
vector	O
of	O
attribute	O
likelihoods	O
exploits	O
a	O
much	O
larger	O
set	O
of	O
candidate	O
words	O
which	O
are	O
learned	O
separately	O
,	O
allowing	O
for	O
greater	O
flexibility	O
in	O
the	O
generated	O
text	O
.	O

Given	O
this	O
attribute	O
vocabulary	O
,	O
we	O
can	O
associate	O
each	O
image	O
with	O
a	O
set	O
of	O
attributes	O
according	O
to	O
its	O
captions	O
.	O

We	O
then	O
wish	O
to	O
predict	O
the	O
attributes	O
given	O
a	O
test	O
image	O
.	O

Because	O
we	O
do	O
not	O
have	O
ground	O
truth	O
bounding	O
boxes	O
for	O
attributes	O
,	O
we	O
can	O
not	O
train	O
a	O
detector	Method
for	O
each	O
using	O
the	O
standard	O
approach	O
.	O

Fang	O
et	O
al	O
.	O

solved	O
a	O
similar	O
problem	O
using	O
a	O
Multiple	Method
Instance	Method
Learning	Method
framework	Method
to	O
detect	Task
visual	Task
words	Task
from	O
images	Material
.	O

Motivated	O
by	O
the	O
relatively	O
small	O
number	O
of	O
times	O
that	O
each	O
word	O
appears	O
in	O
a	O
caption	O
,	O
we	O
instead	O
treat	O
this	O
as	O
a	O
multi	Task
-	Task
label	Task
classification	Task
problem	Task
.	O

To	O
address	O
the	O
concern	O
that	O
some	O
attributes	O
may	O
only	O
apply	O
to	O
image	O
sub	O
-	O
regions	O
,	O
we	O
follow	O
Wei	O
et	O
al	O
.	O

in	O
designing	O
a	O
region	Method
-	Method
based	Method
multi	Method
-	Method
label	Method
classification	Method
framework	Method
that	O
takes	O
an	O
arbitrary	O
number	O
of	O
sub	O
-	O
region	O
proposals	O
as	O
input	O
,	O
then	O
a	O
shared	Method
CNN	Method
is	O
associated	O
with	O
each	O
proposal	O
,	O
and	O
the	O
CNN	O
output	O
results	O
from	O
different	O
proposals	O
are	O
aggregated	O
with	O
max	Method
pooling	Method
to	O
produce	O
the	O
final	O
prediction	Task
.	O

Figure	O
[	O
reference	O
]	O
summarizes	O
the	O
attribute	Method
prediction	Method
network	Method
.	O

The	O
model	O
is	O
a	O
VggNet	Method
structure	Method
followed	O
by	O
a	O
max	Method
-	Method
pooling	Method
operation	Method
on	O
the	O
regions	O
with	O
a	O
multi	O
-	O
label	O
loss	O
.	O

The	O
CNN	Method
model	Method
is	O
first	O
initialized	O
from	O
the	O
VggNet	Method
pre	Method
-	Method
trained	O
on	O
ImageNet	Material
.	O

The	O
shared	Method
CNN	Method
is	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
multi	Material
-	Material
label	Material
dataset	Material
(	O
our	O
image	Material
-	Material
attribute	Material
training	Material
data	Material
)	O
.	O

In	O
this	O
step	O
,	O
the	O
input	O
is	O
the	O
global	Material
image	Material
and	O
the	O
output	O
of	O
the	O
last	O
fully	Method
-	Method
connected	Method
layer	Method
is	O
fed	O
into	O
a	O
-	O
way	O
softmax	O
over	O
the	O
class	O
labels	O
.	O

The	O
here	O
represents	O
the	O
attributes	O
vocabulary	O
size	O
.	O

In	O
contrast	O
to	O
who	O
employs	O
the	O
squared	Method
loss	Method
,	O
we	O
find	O
that	O
element	Method
-	Method
wise	Method
logistic	Method
loss	Method
function	Method
performs	O
better	O
.	O

Suppose	O
that	O
there	O
are	O
training	O
examples	O
and	O
is	O
the	O
label	O
vector	O
of	O
the	O
image	O
,	O
where	O
if	O
the	O
image	O
is	O
annotated	O
with	O
attribute	O
,	O
and	O
otherwise	O
.	O

If	O
the	O
predictive	O
probability	O
vector	O
is	O
,	O
the	O
cost	Metric
function	Metric
to	O
be	O
minimized	O
is	O
During	O
the	O
fine	Task
-	Task
tuning	Task
process	Task
,	O
the	O
parameters	O
of	O
the	O
last	O
fully	Method
connected	Method
layer	Method
(	O
i.e.	O
the	O
attribute	Method
prediction	Method
layer	Method
)	O
are	O
initialized	O
with	O
a	O
Xavier	Method
initialization	Method
.	O

The	O
learning	Metric
rates	Metric
of	O
‘	O
fc6	Method
’	O
and	O
‘	O
fc7	Method
’	O
of	O
the	O
VggNet	Method
are	O
initialized	O
as	O
0.001	O
and	O
the	O
last	O
fully	Method
connected	Method
layer	Method
is	O
initialized	O
as	O
0.01	O
.	O

All	O
the	O
other	O
layers	O
are	O
fixed	O
during	O
training	O
.	O

We	O
executed	O
40	O
epochs	O
in	O
total	O
and	O
decreased	O
the	O
learning	Metric
rate	Metric
to	O
one	O
tenth	O
of	O
the	O
current	O
rate	O
for	O
each	O
layer	O
after	O
10	O
epochs	O
.	O

The	O
momentum	O
is	O
set	O
to	O
0.9	O
.	O

The	O
dropout	Metric
rate	Metric
is	O
set	O
to	O
0.5	O
.	O

To	O
predict	O
attributes	O
based	O
on	O
regions	O
,	O
we	O
first	O
extract	O
hundreds	O
of	O
proposal	O
windows	O
from	O
an	O
image	O
.	O

However	O
,	O
considering	O
the	O
computational	Metric
inefficiency	Metric
of	O
deep	Method
CNNs	Method
,	O
the	O
number	O
of	O
proposals	O
processed	O
needs	O
to	O
be	O
small	O
.	O

Similar	O
to	O
,	O
we	O
first	O
apply	O
the	O
normalized	Method
cut	Method
algorithm	Method
to	O
group	O
the	O
proposal	O
bounding	O
boxes	O
into	O
clusters	O
based	O
on	O
the	O
IoU	O
scores	O
matrix	O
.	O

The	O
top	O
hypotheses	O
in	O
terms	O
of	O
the	O
predictive	Metric
scores	Metric
reported	O
by	O
the	O
proposal	Method
generation	Method
algorithm	Method
are	O
kept	O
and	O
fed	O
into	O
the	O
shared	Method
CNN	Method
.	O

We	O
also	O
include	O
the	O
whole	O
image	O
in	O
the	O
hypothesis	O
group	O
.	O

As	O
a	O
result	O
,	O
there	O
are	O
hypotheses	O
for	O
each	O
image	O
.	O

We	O
set	O
in	O
all	O
experiments	O
.	O

We	O
use	O
Multiscale	Method
Combinatorial	Method
Grouping	Method
(	O
MCG	Method
)	O
for	O
the	O
proposal	Task
generation	Task
.	O

Finally	O
,	O
a	O
cross	Method
hypothesis	Method
max	Method
-	Method
pooling	Method
is	O
applied	O
to	O
integrate	O
the	O
outputs	O
into	O
a	O
single	O
prediction	O
vector	O
.	O

Since	O
we	O
formulate	O
the	O
attribute	Task
prediction	Task
as	O
a	O
multi	Task
-	Task
label	Task
problem	Task
,	O
our	O
attributes	Method
prediction	Method
network	Method
can	O
be	O
replaced	O
by	O
any	O
other	O
multi	Method
-	Method
label	Method
classification	Method
framework	Method
and	O
it	O
also	O
can	O
be	O
benefit	O
from	O
the	O
development	O
of	O
the	O
multi	Task
-	Task
label	Task
classification	Task
researches	Task
.	O

For	O
example	O
,	O
to	O
address	O
the	O
computational	Metric
inefficiency	Metric
of	O
using	O
a	O
large	O
numbers	O
of	O
proposed	O
regions	O
,	O
we	O
can	O
apply	O
an	O
‘	O
R	Method
-	Method
CNN	Method
’	Method
architecture	Method
so	O
that	O
we	O
do	O
not	O
need	O
to	O
compute	O
the	O
convolutional	O
feature	O
map	O
multiple	O
times	O
.	O

The	O
Regional	Method
Proposal	Method
Network	Method
can	O
predict	O
region	O
proposal	O
and	O
attributes	O
together	O
so	O
that	O
we	O
do	O
not	O
need	O
the	O
external	O
region	Method
proposal	Method
tools	Method
.	O

We	O
even	O
can	O
consider	O
the	O
attributes	O
dependencies	O
by	O
using	O
the	O
recently	O
proposed	O
CNN	Method
-	Method
RNN	Method
model	Method
.	O

However	O
,	O
we	O
leave	O
them	O
as	O
the	O
further	O
work	O
.	O

subsection	O
:	O
Caption	Method
Generation	Method
Model	Method
Similar	O
to	O
,	O
we	O
propose	O
to	O
train	O
a	O
caption	Method
generation	Method
model	Method
by	O
maximizing	O
the	O
probability	O
of	O
the	O
correct	O
description	O
given	O
the	O
image	O
.	O

However	O
,	O
rather	O
than	O
using	O
image	O
features	O
directly	O
as	O
in	O
typically	O
the	O
case	O
,	O
we	O
use	O
the	O
semantic	O
attribute	O
prediction	O
value	O
from	O
the	O
previous	O
section	O
as	O
the	O
input	O
.	O

Suppose	O
that	O
is	O
a	O
sequence	O
of	O
words	O
.	O

The	O
log	O
-	O
likelihood	O
of	O
the	O
words	O
given	O
their	O
context	O
words	O
and	O
the	O
corresponding	O
image	O
can	O
be	O
written	O
as	O
:	O
where	O
is	O
the	O
probability	O
of	O
generating	O
the	O
word	O
given	O
attribute	O
vector	O
and	O
previous	O
words	O
.	O

We	O
employ	O
the	O
LSTM	Method
,	O
a	O
particular	O
form	O
of	O
RNN	Method
,	O
to	O
model	O
this	O
.	O

The	O
LSTM	Method
is	O
a	O
memory	Method
cell	Method
encoding	O
knowledge	O
at	O
every	O
time	O
step	O
for	O
what	O
inputs	O
have	O
been	O
observed	O
up	O
to	O
this	O
step	O
.	O

We	O
follow	O
the	O
model	O
used	O
in	O
.	O

Letting	O
be	O
the	O
sigmoid	O
nonlinearity	O
,	O
the	O
LSTM	Method
updates	Method
for	O
time	O
step	O
given	O
inputs	O
,	O
,	O
are	O
:	O
Here	O
,	O
are	O
the	O
input	O
,	O
forget	O
,	O
memory	O
,	O
output	O
state	O
of	O
the	O
LSTM	Method
.	O

The	O
various	O
matrices	O
are	O
trained	O
parameters	O
and	O
represents	O
the	O
product	O
with	O
a	O
gate	O
value	O
.	O

is	O
the	O
hidden	O
state	O
at	O
time	O
step	O
and	O
is	O
fed	O
to	O
a	O
Softmax	Method
,	O
which	O
will	O
produce	O
a	O
probability	O
distribution	O
over	O
all	O
words	O
and	O
indicate	O
the	O
word	O
at	O
time	O
step	O
.	O

Training	O
details	O
:	O
The	O
LSTM	Method
model	Method
for	O
image	Task
captioning	Task
is	O
trained	O
in	O
an	O
unrolled	O
form	O
.	O

More	O
formally	O
,	O
the	O
LSTM	Method
takes	O
the	O
attributes	O
vector	O
and	O
a	O
sequence	O
of	O
words	O
,	O
where	O
is	O
a	O
special	O
start	O
word	O
and	O
is	O
a	O
special	O
END	O
token	O
.	O

Each	O
word	O
has	O
been	O
represented	O
as	O
a	O
one	O
-	O
hot	O
vector	O
of	O
dimension	O
equal	O
to	O
the	O
size	O
of	O
words	O
dictionary	O
.	O

The	O
words	Method
dictionaries	Method
are	O
built	O
based	O
on	O
words	O
that	O
occur	O
at	O
least	O
5	O
times	O
in	O
the	O
training	O
set	O
,	O
which	O
lead	O
to	O
2538	O
,	O
7414	O
,	O
and	O
8791	O
words	O
on	O
Flickr8k	Material
,	O
Flickr30k	Material
and	Material
MS	Material
COCO	Material
datasets	Material
separately	O
.	O

Note	O
it	O
is	O
different	O
from	O
the	O
semantic	O
attributes	O
vocabulary	O
.	O

The	O
training	O
procedure	O
is	O
as	O
following	O
:	O
At	O
time	O
step	O
,	O
we	O
set	O
,	O
and	O
,	O
where	O
is	O
the	O
learnable	O
attributes	O
embedding	O
weights	O
.	O

This	O
gives	O
us	O
an	O
initial	O
LSTM	O
hidden	O
state	O
which	O
can	O
be	O
used	O
in	O
the	O
next	O
time	O
step	O
.	O

From	O
to	O
,	O
we	O
set	O
and	O
the	O
hidden	O
state	O
is	O
given	O
by	O
the	O
previous	O
step	O
,	O
where	O
is	O
the	O
learnable	O
word	O
embedding	O
weights	O
.	O

The	O
probability	O
distribution	O
over	O
all	O
words	O
is	O
then	O
computed	O
by	O
the	O
LSTM	Method
feed	Method
-	Method
forward	Method
process	Method
.	O

Finally	O
,	O
on	O
the	O
last	O
step	O
when	O
represents	O
the	O
last	O
word	O
,	O
the	O
target	O
label	O
is	O
set	O
to	O
the	O
END	O
token	O
.	O

Our	O
training	O
objective	O
is	O
to	O
learn	O
parameters	O
,	O
and	O
all	O
parameters	O
in	O
LSTM	Method
by	O
minimizing	O
the	O
following	O
cost	Metric
function	Metric
:	O
where	O
is	O
the	O
number	O
of	O
training	O
examples	O
and	O
is	O
the	O
length	O
of	O
the	O
sentence	O
for	O
the	O
-	O
th	O
training	O
example	O
.	O

corresponds	O
to	O
the	O
activation	O
of	O
the	O
Softmax	Method
layer	Method
in	O
the	O
LSTM	Method
model	Method
for	O
the	O
-	O
th	O
input	O
and	O
represents	O
model	O
parameters	O
,	O
is	O
a	O
regularization	O
term	O
.	O

We	O
use	O
SGD	Method
with	O
mini	O
-	O
batches	O
of	O
100	O
image	Material
-	Material
sentence	Material
pairs	Material
.	O

The	O
attributes	O
embedding	O
size	O
,	O
word	O
embedding	O
size	O
and	O
hidden	O
state	O
size	O
are	O
all	O
set	O
to	O
256	O
in	O
all	O
the	O
experiments	O
.	O

The	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.001	O
and	O
clip	O
gradients	O
is	O
5	O
.	O

The	O
dropout	Metric
rate	Metric
is	O
set	O
to	O
0.5	O
.	O

To	O
infer	O
the	O
sentence	O
given	O
an	O
input	O
image	O
,	O
we	O
use	O
Beam	Method
Search	Method
,	O
i.e.	O
,	O
we	O
iteratively	O
consider	O
the	O
set	O
of	O
best	O
sentences	O
up	O
to	O
time	O
as	O
candidates	O
to	O
generate	O
sentences	O
at	O
time	O
,	O
and	O
only	O
keep	O
the	O
best	O
results	O
.	O

We	O
set	O
the	O
to	O
5	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
some	O
examples	O
of	O
the	O
predicted	O
attributes	O
and	O
generated	O
captions	O
.	O

More	O
results	O
can	O
be	O
found	O
in	O
the	O
supplementary	O
material	O
.	O

section	O
:	O
A	O
VQA	Method
Model	Method
with	O
External	O
Knowledge	O
The	O
key	O
differentiator	O
of	O
our	O
VQA	Method
model	Method
is	O
that	O
it	O
is	O
able	O
to	O
usefully	O
combine	O
image	O
information	O
with	O
that	O
extracted	O
from	O
a	O
Knowledge	Material
Base	Material
,	O
within	O
the	O
LSTM	Method
framework	Method
.	O

The	O
novelty	O
lies	O
in	O
the	O
fact	O
that	O
this	O
is	O
achieved	O
by	O
representing	O
both	O
of	O
these	O
disparate	O
forms	O
of	O
information	O
as	O
text	O
before	O
combining	O
them	O
.	O

Figure	O
[	O
reference	O
]	O
summarises	O
how	O
this	O
is	O
achieved	O
:	O
given	O
an	O
image	Material
,	O
an	O
attribute	Method
-	Method
based	Method
representation	Method
(	O
in	O
Section	O
[	O
reference	O
]	O
)	O
is	O
first	O
generated	O
and	O
it	O
will	O
used	O
as	O
one	O
of	O
input	O
sources	O
of	O
our	O
VQA	Method
-	Method
LSTM	Method
model	Method
.	O

The	O
second	O
input	O
source	O
are	O
those	O
captions	O
generated	O
in	O
section	O
[	O
reference	O
]	O
.	O

Rather	O
than	O
inputing	O
the	O
generated	O
words	O
directly	O
,	O
the	O
hidden	O
state	O
vector	O
of	O
the	O
caption	Method
-	Method
LSTM	Method
after	O
it	O
has	O
generated	O
the	O
last	O
word	O
in	O
each	O
caption	O
is	O
used	O
to	O
represent	O
its	O
content	O
.	O

Average	Method
-	Method
pooling	Method
is	O
applied	O
over	O
the	O
5	O
hidden	O
-	O
state	O
vectors	O
,	O
to	O
obtain	O
a	O
vector	Method
representation	Method
for	O
the	O
image	Material
.	O

The	O
third	O
input	O
source	O
is	O
the	O
textual	Material
knowledge	Material
which	O
is	O
mined	O
from	O
a	O
large	O
-	O
scale	Material
knowledge	Material
base	Material
,	O
the	O
DBpedia	Material
.	O

More	O
details	O
are	O
shown	O
in	O
the	O
following	O
section	O
.	O

subsection	O
:	O
Relating	O
to	O
the	O
Knowledge	Material
Base	Material
The	O
external	O
data	O
source	O
that	O
we	O
use	O
here	O
is	O
DBpeida	Material
as	O
a	O
source	O
of	O
general	Material
background	Material
information	Material
,	O
although	O
any	O
such	O
KB	O
could	O
equally	O
be	O
applied	O
.	O

DBpeida	Method
is	O
a	O
structured	Material
database	Material
of	Material
information	Material
extracted	O
from	O
Wikipedia	Material
.	O

The	O
whole	O
DBpedia	Material
dataset	Material
describes	O
million	O
entities	O
,	O
of	O
which	O
million	O
are	O
classified	O
in	O
a	O
consistent	O
ontology	O
.	O

The	O
data	O
can	O
be	O
accessed	O
using	O
an	O
SQL	Method
-	Method
like	Method
query	Method
language	Method
for	O
RDF	Method
called	O
SPARQL	Method
.	O

Given	O
an	O
image	O
and	O
its	O
predicted	O
attributes	O
,	O
we	O
use	O
the	O
top	O
-	O
five	O
most	O
strongly	O
predicted	O
attributes	O
to	O
generate	O
DBpedia	O
queries	O
.	O

There	O
are	O
a	O
range	O
of	O
problems	O
with	O
DBpedia	Task
and	Task
similar	Task
,	O
however	O
,	O
including	O
the	O
sparsity	O
of	O
the	O
information	O
,	O
and	O
the	O
inconsistency	O
of	O
its	O
representation	O
.	O

Inspecting	O
the	O
database	O
shows	O
that	O
the	O
‘	O
comment	O
’	O
field	O
is	O
the	O
most	O
generally	O
informative	O
about	O
an	O
attribute	O
,	O
as	O
it	O
contains	O
a	O
general	O
text	O
description	O
of	O
it	O
.	O

We	O
therefore	O
retrieve	O
the	O
comment	Material
text	Material
for	O
each	O
query	O
term	O
.	O

The	O
KB	Method
+	Method
SPARQL	Method
combination	Method
is	O
very	O
general	O
,	O
however	O
,	O
and	O
could	O
be	O
applied	O
problem	O
specific	O
KBs	Material
,	O
or	O
a	O
database	Material
of	Material
common	Material
sense	Material
information	Material
,	O
and	O
can	O
even	O
perform	O
basic	O
inference	O
over	O
RDF	Material
.	O

Figure	O
[	O
reference	O
]	O
shows	O
an	O
example	O
of	O
the	O
query	Material
language	Material
and	O
returned	Material
text	Material
.	O

Since	O
the	O
text	O
returned	O
by	O
the	O
SPARQL	Material
query	Material
is	O
typically	O
much	O
longer	O
than	O
the	O
captions	O
generated	O
in	O
the	O
section	O
[	O
reference	O
]	O
,	O
we	O
turn	O
to	O
Doc2Vec	Method
to	O
extract	O
the	O
semantic	O
meanings	O
.	O

Doc2Vec	Method
,	O
also	O
known	O
as	O
Paragraph	Method
Vector	Method
,	O
is	O
an	O
unsupervised	Method
algorithm	Method
that	O
learns	O
fixed	Method
-	Method
length	Method
feature	Method
representations	Method
from	O
variable	O
-	O
length	O
pieces	O
of	O
texts	O
,	O
such	O
as	O
sentences	O
,	O
paragraphs	Material
,	O
and	O
documents	O
.	O

Le	O
et	O
al	O
.	O

proved	O
that	O
it	O
can	O
capture	O
the	O
semantics	O
of	O
paragraphs	O
.	O

A	O
Doc2Vec	Method
model	Method
is	O
trained	O
to	O
predict	O
words	O
in	O
the	O
document	O
given	O
the	O
context	O
words	O
.	O

We	O
collect	O
100	O
,	O
000	O
documents	O
from	O
DBpedia	Material
to	O
train	O
a	O
model	O
with	O
vector	O
size	O
500	O
.	O

To	O
obtain	O
the	O
knowledge	O
vector	O
for	O
image	O
,	O
we	O
combine	O
the	O
5	O
returned	O
paragraphs	O
in	O
to	O
a	O
single	O
large	O
paragraph	O
,	O
before	O
semantic	O
features	O
using	O
our	O
pre	O
-	O
trained	O
Doc2Vec	Method
model	Method
.	O

subsection	O
:	O
Question	Task
-	Task
guided	Task
Knowledge	Task
Selection	Task
We	O
incrementally	O
implemented	O
a	O
question	Method
-	Method
guided	Method
knowledge	Method
selection	Method
scheme	Method
to	O
rule	O
out	O
the	O
noise	O
information	O
,	O
since	O
we	O
observed	O
that	O
some	O
mined	O
knowledge	O
are	O
not	O
necessary	O
for	O
answering	O
the	O
given	O
question	O
.	O

For	O
example	O
,	O
if	O
the	O
question	O
is	O
asking	O
about	O
the	O
‘	O
dog	O
’	O
in	O
the	O
image	O
,	O
it	O
does	O
not	O
make	O
sense	O
to	O
input	O
a	O
piece	O
of	O
‘	O
bird	O
’	O
knowledge	O
into	O
the	O
model	O
,	O
although	O
the	O
image	O
does	O
have	O
a	O
‘	O
bird	O
’	O
inside	O
.	O

Given	O
a	O
question	O
and	O
mined	O
knowledge	O
paragraphs	O
using	O
above	O
KB	Method
+	Method
SPARQL	Method
combination	Method
,	O
we	O
first	O
use	O
our	O
pre	O
-	O
trained	O
Doc2Vec	Method
model	Method
to	O
extract	O
the	O
semantic	O
feature	O
of	O
the	O
question	O
and	O
the	O
feature	O
for	O
each	O
single	O
knowledge	O
paragraph	O
,	O
where	O
.	O

Then	O
,	O
we	O
find	O
the	O
closest	O
knowledge	O
paragraphs	O
to	O
the	O
question	O
based	O
on	O
the	O
cosine	O
similarity	O
between	O
the	O
and	O
.	O

Finally	O
,	O
we	O
combine	O
the	O
selected	O
knowledge	O
paragraphs	O
in	O
to	O
a	O
single	O
one	O
and	O
use	O
the	O
Doc2Vec	Method
model	Method
to	O
extract	O
its	O
semantic	O
feature	O
.	O

In	O
our	O
experiments	O
,	O
we	O
set	O
.	O

subsection	O
:	O
An	O
Answer	Method
Generation	Method
Model	Method
with	O
Multiple	O
Inputs	O
We	O
propose	O
to	O
train	O
a	O
VQA	Method
model	Method
by	O
maximizing	O
the	O
probability	O
of	O
the	O
correct	O
answer	O
given	O
the	O
image	O
and	O
question	O
.	O

We	O
want	O
our	O
VQA	Method
model	Method
to	O
be	O
able	O
to	O
generate	O
multiple	O
word	O
answers	O
,	O
so	O
we	O
formulate	O
the	O
answering	Task
process	Task
as	O
a	O
word	Method
sequence	Method
generation	Method
procedure	Method
.	O

Let	O
represent	O
the	O
sequence	O
of	O
words	O
in	O
a	O
question	O
,	O
and	O
the	O
answer	O
sequence	O
,	O
where	O
and	O
are	O
the	O
length	O
of	O
question	O
and	O
answer	O
,	O
respectively	O
.	O

The	O
log	O
-	O
likelihood	O
of	O
the	O
generated	O
answer	O
can	O
be	O
written	O
as	O
:	O
where	O
is	O
the	O
probability	O
of	O
generating	O
given	O
image	O
information	O
,	O
question	O
and	O
previous	O
words	O
.	O

We	O
employ	O
an	O
encoder	Method
LSTM	Method
to	O
take	O
the	O
semantic	O
information	O
from	O
image	Material
and	O
the	O
question	O
,	O
while	O
using	O
a	O
decoder	Method
LSTM	Method
to	O
generate	O
the	O
answer	O
.	O

Weights	O
are	O
shared	O
between	O
the	O
encoder	Method
and	Method
decoder	Method
LSTM	Method
.	O

In	O
the	O
training	O
phase	O
,	O
the	O
question	O
and	O
answer	O
are	O
concatenated	O
as	O
,	O
where	O
is	O
a	O
special	O
END	O
token	O
.	O

Each	O
word	O
is	O
represented	O
as	O
a	O
one	O
-	O
hot	O
vector	O
of	O
dimension	O
equal	O
to	O
the	O
size	O
of	O
the	O
word	O
dictionary	O
.	O

The	O
training	O
procedure	O
is	O
as	O
follows	O
:	O
at	O
time	O
step	O
,	O
we	O
set	O
the	O
LSTM	O
input	O
:	O
where	O
,	O
,	O
are	O
learnable	O
embedding	O
weights	O
for	O
the	O
vector	Method
representation	Method
of	Method
attributes	Method
,	O
captions	O
and	O
external	O
knowledge	O
,	O
respectively	O
.	O

Given	O
the	O
randomly	O
initialized	O
hidden	O
state	O
,	O
the	O
encoder	Method
LSTM	Method
feeds	O
forward	O
to	O
produce	O
hidden	O
state	O
which	O
encodes	O
all	O
of	O
the	O
input	O
information	O
.	O

From	O
to	O
,	O
we	O
set	O
and	O
the	O
hidden	O
state	O
is	O
given	O
by	O
the	O
previous	O
step	O
,	O
where	O
is	O
the	O
learnable	O
word	O
embedding	O
weights	O
.	O

The	O
decoder	Method
LSTM	Method
runs	O
from	O
time	O
step	O
to	O
.	O

Specifically	O
,	O
at	O
time	O
step	O
,	O
the	O
LSTM	Method
layer	Method
takes	O
the	O
input	O
and	O
the	O
hidden	O
state	O
corresponding	O
to	O
the	O
last	O
word	O
of	O
the	O
question	O
,	O
where	O
is	O
the	O
start	O
word	O
of	O
the	O
answer	O
.	O

The	O
hidden	O
state	O
thus	O
encodes	O
all	O
available	O
information	O
about	O
the	O
image	O
and	O
the	O
question	O
.	O

The	O
probability	O
distribution	O
over	O
all	O
answer	O
words	O
in	O
the	O
vocabulary	O
is	O
then	O
computed	O
by	O
the	O
LSTM	Method
feed	Method
-	Method
forward	Method
process	Method
.	O

Finally	O
,	O
for	O
the	O
final	O
step	O
,	O
when	O
represents	O
the	O
last	O
word	O
of	O
the	O
answer	O
,	O
the	O
target	O
label	O
is	O
set	O
to	O
the	O
END	O
token	O
.	O

Our	O
training	O
objective	O
is	O
to	O
learn	O
parameters	O
,	O
,	O
,	O
and	O
all	O
the	O
parameters	O
in	O
the	O
LSTM	Method
by	O
minimizing	O
the	O
following	O
cost	Metric
function	Metric
:	O
where	O
is	O
the	O
number	O
of	O
training	O
examples	O
,	O
and	O
and	O
are	O
the	O
length	O
of	O
question	O
and	O
answer	O
respectively	O
for	O
the	O
-	O
th	O
training	O
example	O
.	O

Let	O
correspond	O
to	O
the	O
activation	O
of	O
the	O
Softmax	Method
layer	Method
in	O
the	O
LSTM	Method
model	Method
for	O
the	O
-	O
th	O
input	O
and	O
represent	O
the	O
model	O
parameters	O
.	O

Note	O
that	O
is	O
a	O
regularization	O
term	O
,	O
where	O
.	O

We	O
use	O
Stochastic	Method
gradient	Method
Descent	Method
(	O
SGD	Method
)	O
with	O
mini	O
-	O
batches	O
of	O
100	O
image	Material
-	Material
QA	Material
pairs	Material
.	O

The	O
attributes	O
,	O
internal	O
textual	O
representation	O
,	O
external	O
knowledge	O
embedding	O
size	O
,	O
word	O
embedding	O
size	O
and	O
hidden	O
state	O
size	O
are	O
all	O
256	O
in	O
all	O
experiments	O
.	O

The	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.001	O
and	O
clip	O
gradients	O
is	O
5	O
.	O

The	O
dropout	Metric
rate	Metric
is	O
set	O
to	O
0.5	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Evaluation	O
on	O
Image	Task
Captioning	Task
subsubsection	O
:	O
Dataset	O
We	O
report	O
image	Task
captioning	Task
results	O
on	O
the	O
popular	Material
Flickr8k	Material
,	Material
Flickr30k	Material
and	Material
Microsoft	Material
COCO	Material
dataset	Material
.	O

These	O
datasets	O
contain	O
8	O
,	O
000	O
,	O
31	O
,	O
000	O
and	O
123	O
,	O
287	O
images	O
respectively	O
,	O
and	O
each	O
image	O
is	O
annotated	O
with	O
5	O
sentences	O
.	O

In	O
our	O
reported	O
results	O
,	O
we	O
use	O
pre	O
-	O
defined	O
splits	O
for	O
Flickr8k	Material
.	O

Because	O
most	O
of	O
previous	O
works	O
in	O
image	Task
captioning	Task
are	O
not	O
evaluated	O
on	O
the	O
official	O
split	O
for	O
Flickr30k	Material
and	O
MS	Material
COCO	Material
,	O
for	O
fair	O
comparison	O
,	O
we	O
report	O
results	O
with	O
the	O
widely	O
used	O
publicly	O
available	O
splits	O
in	O
the	O
work	O
of	O
.We	O
further	O
tested	O
on	O
the	O
actually	O
MS	Material
COCO	Material
test	Material
set	Material
consisting	O
of	O
40775	O
images	O
(	O
human	Material
captions	Material
for	O
this	O
split	O
are	O
not	O
available	O
publicly	O
)	O
,	O
and	O
evaluated	O
them	O
on	O
the	O
COCO	Metric
evaluation	Metric
server	Metric
.	O

subsubsection	Method
:	O
Evaluation	O
Metrics	Metric
:	O
We	O
report	O
results	O
with	O
the	O
frequently	O
used	O
BLEU	Metric
metric	Metric
and	O
sentence	Metric
perplexity	Metric
(	O
)	O
.	O

For	O
MS	Material
COCO	Material
dataset	Material
,	O
we	O
additionally	O
evaluate	O
our	O
model	O
based	O
on	O
the	O
metrics	Metric
of	Metric
METEOR	Metric
and	Metric
CIDEr	Metric
.	O

Baselines	O
:	O
To	O
verify	O
the	O
effectiveness	O
of	O
our	O
high	Method
-	Method
level	Method
attributes	Method
representation	Method
,	O
we	O
provide	O
a	O
baseline	O
method	O
.	O

The	O
baseline	O
framework	O
is	O
same	O
as	O
the	O
one	O
proposed	O
in	O
section	O
[	O
reference	O
]	O
,	O
except	O
that	O
the	O
attributes	O
vector	O
is	O
replaced	O
by	O
the	O
last	O
hidden	O
layer	O
of	O
CNN	Method
directly	O
.	O

For	O
the	O
VggNet	Method
+	Method
LSTM	Method
,	O
we	O
use	O
the	O
second	O
fully	Method
connected	Method
layer	Method
(	O
fc7	Method
)	O
as	O
the	O
image	O
features	O
,	O
which	O
has	O
4096	O
dimensions	O
.	O

In	O
VggNet	Method
-	Method
PCA	Method
+	Method
LSTM	Method
,	O
PCA	Method
is	O
applied	O
to	O
decrease	O
the	O
feature	O
dimension	O
from	O
4096	O
to	O
1000	O
.	O

VggNet	Method
+	Method
ft	Method
+	Method
LSTM	Method
applies	O
a	O
VggNet	Method
that	O
has	O
been	O
fine	O
-	O
tuned	O
on	O
the	O
target	O
dataset	O
,	O
based	O
on	O
the	O
task	O
of	O
image	Task
-	Task
attributes	Task
classification	Task
.	O

Our	O
Approaches	O
:	O
We	O
evaluate	O
several	O
variants	O
of	O
our	O
approach	O
:	O
Att	Method
-	Method
GT	Method
+	Method
LSTM	Method
models	Method
use	O
ground	O
-	O
truth	O
attributes	O
as	O
the	O
input	O
while	O
Att	O
-	O
RegionCNN	O
+	O
LSTM	O
uses	O
the	O
attributes	O
vector	O
predicted	O
by	O
the	O
region	Method
based	Method
attributes	Method
prediction	Method
network	Method
in	O
section	O
[	O
reference	O
]	O
.	O

We	O
also	O
evaluate	O
an	O
approach	O
Att	O
-	O
SVM	Method
+	Method
LSTM	Method
with	O
linear	Method
SVM	Method
predicted	Method
attributes	Method
vector	Method
.	O

We	O
use	O
the	O
second	O
fully	Method
connected	Method
layer	Method
of	O
the	O
fine	Method
-	Method
tuned	Method
VggNet	Method
to	O
feed	O
the	O
SVM	Method
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
the	O
region	Method
based	Method
attributes	Method
prediction	Method
in	O
the	O
captioning	Task
task	Task
,	O
the	O
Att	Method
-	Method
GlobalCNN	Method
+	Method
LSTM	Method
is	O
implemented	O
by	O
using	O
the	O
global	Material
image	Material
for	O
attributes	Task
prediction	Task
.	O

Results	O
:	O
Table	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
report	O
image	Material
captioning	Material
results	O
on	O
Flickr8k	Material
,	O
Flickr30k	Method
and	O
Microsoft	Material
COCO	Material
dataset	Material
.	O

It	O
is	O
not	O
surprising	O
that	O
Att	Method
-	Method
GT	Method
+	Method
LSTM	Method
model	Method
performs	O
best	O
,	O
since	O
ground	O
truth	O
attributes	O
labels	O
are	O
used	O
.	O

We	O
report	O
these	O
results	O
here	O
just	O
to	O
show	O
the	O
advances	O
of	O
adding	O
an	O
intermediate	O
image	Method
-	Method
to	Method
-	Method
word	Method
mapping	Method
stage	Method
.	O

Ideally	O
,	O
if	O
we	O
could	O
train	O
a	O
perfectly	O
accurate	O
attribute	Method
predictor	Method
,	O
we	O
could	O
obtain	O
an	O
outstanding	O
improvement	O
compared	O
to	O
both	O
baseline	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Indeed	O
,	O
apart	O
from	O
using	O
ground	O
truth	O
attributes	O
,	O
our	O
Att	Method
-	Method
RegionCNN	Method
+	Method
LSTM	Method
models	Method
generate	O
the	O
best	O
results	O
on	O
all	O
the	O
three	O
datasets	O
over	O
all	O
evaluation	Metric
metrics	Metric
.	O

Especially	O
comparing	O
with	O
baselines	O
,	O
which	O
do	O
not	O
contain	O
an	O
attributes	Method
prediction	Method
layer	Method
,	O
our	O
final	O
models	O
bring	O
significant	O
improvements	O
,	O
nearly	O
15	O
%	O
for	O
B	Metric
-	Metric
1	Metric
and	O
30	O
%	O
for	O
CIDEr	Task
on	O
average	O
.	O

VggNet	Method
+	Method
ft	Method
+	Method
LSTM	Method
models	Method
perform	O
better	O
than	O
other	O
baselines	O
because	O
of	O
the	O
fine	Task
-	Task
tuning	Task
on	O
the	O
target	O
dataset	O
.	O

However	O
,	O
they	O
do	O
not	O
perform	O
as	O
well	O
as	O
our	O
attributes	Method
-	Method
based	Method
models	Method
.	O

Att	Method
-	Method
SVM	Method
+	Method
LSTM	Method
and	O
Att	Method
-	Method
GlobalCNN	Method
+	Method
LSTM	Method
under	O
-	O
perform	O
Att	O
-	O
RegionCNN	O
+	O
LSTM	O
,	O
indicating	O
that	O
region	Method
-	Method
based	Method
attributes	Method
prediction	Method
provides	O
useful	O
detail	O
beyond	O
whole	Task
image	Task
classification	Task
.	O

Our	O
final	O
model	O
also	O
outperforms	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
listed	O
in	O
the	O
tables	O
.	O

We	O
also	O
evaluated	O
an	O
approach	O
(	O
not	O
shown	O
in	O
table	O
)	O
that	O
combines	O
CNN	O
features	O
and	O
attributes	O
vector	O
together	O
as	O
the	O
input	O
of	O
the	O
LSTM	Method
,	O
but	O
we	O
found	O
this	O
approach	O
is	O
not	O
as	O
good	O
as	O
using	O
attributes	O
vector	O
only	O
in	O
the	O
same	O
setting	O
.	O

In	O
any	O
case	O
,	O
above	O
experiments	O
show	O
that	O
an	O
intermediate	O
image	Task
-	Task
to	Task
-	Task
words	Task
stage	Task
(	O
i.e.	O
attributes	Method
prediction	Method
layer	Method
)	O
bring	O
us	O
significant	O
improvements	O
.	O

We	O
further	O
generated	O
captions	O
for	O
the	O
images	O
in	O
the	O
COCO	Material
test	Material
set	Material
containing	O
40	O
,	O
775	O
images	O
and	O
evaluated	O
them	O
on	O
the	O
COCO	Metric
evaluation	Metric
server	Metric
.	O

These	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
achieve	O
0.73	O
on	O
B	Metric
-	Metric
1	Metric
,	O
and	O
surpass	O
human	O
performances	O
on	O
13	O
of	O
the	O
14	O
metrics	Metric
reported	O
.	O

Other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
are	O
also	O
shown	O
for	O
comparison	O
.	O

Human	Metric
Evaluation	Metric
:	O
We	O
additionally	O
perform	O
a	O
human	O
evaluation	O
on	O
our	O
proposed	O
model	O
,	O
to	O
evaluate	O
the	O
caption	Task
generation	Task
ability	Task
.	O

We	O
randomly	O
sample	O
1000	O
results	O
from	O
the	O
COCO	Material
validation	Material
dataset	Material
,	O
generated	O
by	O
our	O
proposed	O
model	Method
Att	Method
-	Method
RegionCNN	Method
+	Method
LSTM	Method
and	O
the	O
baseline	Method
model	Method
VggNet	Method
+	Method
LSTM	Method
.	O

Following	O
the	O
human	Metric
evaluation	Metric
protocol	Metric
of	O
the	O
MS	Material
COCO	Material
Captioning	Material
Challenge	Material
2015	Material
,	O
two	O
evaluation	Metric
metrics	Metric
are	O
applied	O
.	O

M1	Method
is	O
the	O
percentage	O
of	O
captions	O
that	O
are	O
evaluated	O
as	O
better	O
or	O
equal	O
to	O
human	O
caption	O
and	O
M2	Metric
is	O
the	O
percentage	O
of	O
captions	O
that	O
pass	O
the	O
Turing	Metric
Test	Metric
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
the	O
human	O
evaluation	O
results	O
.	O

We	O
can	O
see	O
our	O
model	O
outperforms	O
the	O
baseline	O
model	O
on	O
both	O
metrics	O
.	O

We	O
did	O
not	O
evaluate	O
on	O
the	O
test	O
split	O
because	O
the	O
human	Material
ground	Material
truth	Material
is	O
not	O
publicly	O
available	O
.	O

Table	O
[	O
reference	O
]	O
summarizes	O
some	O
properties	O
of	O
recurrent	Method
layers	Method
employed	O
in	O
some	O
recent	O
RNN	Method
-	Method
based	Method
methods	Method
.	O

We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
using	O
a	O
relatively	O
low	O
dimensional	O
visual	O
input	O
feature	O
and	O
recurrent	Method
layer	Method
.	O

Lower	O
dimension	O
of	O
visual	O
input	O
and	O
RNN	O
normally	O
means	O
less	O
parameters	O
in	O
the	O
RNN	Method
training	Method
stage	Method
,	O
as	O
well	O
as	O
lower	O
computation	Metric
cost	Metric
.	O

subsection	O
:	O
Evaluation	O
on	O
Visual	Task
Question	Task
Answering	Task
We	O
evaluate	O
our	O
model	O
on	O
four	O
recent	O
publicly	O
available	O
visual	Material
question	Material
answering	Material
datasets	Material
.	O

DAQURA	Method
-	Method
ALL	Method
is	O
proposed	O
in	O
.	O

There	O
are	O
7	O
,	O
795	O
training	O
questions	O
and	O
5	O
,	O
673	O
test	O
questions	O
.	O

DAQURA	Method
-	Method
REDUCED	Method
is	O
a	O
reduced	Method
version	Method
of	Method
DAQURA	Method
-	Method
ALL	Method
.	O

There	O
are	O
3	O
,	O
876	O
training	O
questions	O
and	O
only	O
297	O
test	O
questions	O
.	O

This	O
dataset	O
is	O
constrained	O
to	O
37	O
object	O
categories	O
and	O
uses	O
only	O
25	O
test	O
images	O
.	O

Two	O
large	Material
-	Material
scale	Material
VQA	Material
data	Material
are	O
constructed	O
both	O
based	O
on	O
MS	Material
COCO	Material
images	Material
.	O

The	O
Toronto	Material
COCO	Material
-	Material
QA	Material
Dataset	O
contains	O
78	O
,	O
736	O
training	O
and	O
38	O
,	O
948	O
testing	O
examples	O
,	O
which	O
are	O
generated	O
from	O
117	O
,	O
684	O
images	O
.	O

All	O
of	O
the	O
question	O
-	O
answer	O
pairs	O
in	O
this	O
dataset	O
are	O
automatically	O
converted	O
from	O
human	Material
-	Material
sourced	Material
image	Material
descriptions	Material
.	O

Another	O
benchmarked	O
dataset	O
is	O
VQA	Method
,	O
which	O
is	O
a	O
much	O
larger	O
dataset	O
and	O
contains	O
614	O
,	O
163	O
questions	O
and	O
6	O
,	O
141	O
,	O
630	O
answers	O
based	O
on	O
204	O
,	O
721	O
MS	Material
COCO	Material
images	Material
.	O

We	O
randomly	O
choose	O
5000	O
images	O
from	O
the	O
validation	O
set	O
as	O
our	O
val	O
set	O
,	O
with	O
the	O
remainder	O
testing	O
.	O

The	O
human	O
ground	O
truth	O
answers	O
for	O
the	O
actual	O
VQA	Material
test	Material
split	Material
are	O
not	O
available	O
publicly	O
and	O
only	O
can	O
be	O
evaluated	O
via	O
the	O
VQA	Method
evaluation	Method
server	Method
.	O

Hence	O
,	O
we	O
also	O
apply	O
our	O
final	O
model	O
on	O
a	O
test	O
split	O
and	O
report	O
the	O
overall	O
accuracy	Metric
.	O

Table	O
[	O
reference	O
]	O
displays	O
some	O
dataset	O
statistics	O
.	O

subsubsection	O
:	O
Results	O
on	O
DAQURA	O
Metrics	Metric
:	O
Following	O
,	O
the	O
accuracy	Metric
value	Metric
(	O
the	O
proportion	O
of	O
correctly	O
answered	O
test	O
questions	O
)	O
,	O
and	O
the	O
Wu	Metric
-	Metric
Palmer	Metric
similarity	Metric
(	O
WUPS	Method
)	O
are	O
used	O
to	O
measure	O
performance	O
.	O

The	O
WUPS	Method
calculates	O
the	O
similarity	O
between	O
two	O
words	O
based	O
on	O
the	O
similarity	O
between	O
their	O
common	O
subsequence	O
in	O
the	O
taxonomy	O
tree	O
.	O

If	O
the	O
similarity	O
between	O
two	O
words	O
is	O
greater	O
than	O
a	O
threshold	O
then	O
the	O
candidate	O
answer	O
is	O
considered	O
to	O
be	O
right	O
.	O

We	O
report	O
on	O
thresholds	O
0.9	O
and	O
0.0	O
,	O
following	O
.	O

Evaluations	O
:	O
To	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
model	O
,	O
we	O
provide	O
two	O
baseline	O
models	O
and	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
table	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
Baseline	O
method	O
is	O
implemented	O
simply	O
by	O
connecting	O
a	O
CNN	Method
to	O
an	O
LSTM	Method
.	O

The	O
CNN	Method
is	O
a	O
pre	O
-	O
trained	O
(	O
on	O
ImageNet	Method
)	Method
VggNet	Method
model	Method
from	O
which	O
we	O
extract	O
the	O
coefficients	O
of	O
the	O
last	O
fully	Method
connected	Method
layer	Method
.	O

We	O
also	O
implement	O
a	O
baseline	Method
model	Method
VggNet	Method
+	Method
ft	Method
-	Method
LSTM	Method
,	O
which	O
applies	O
a	O
vggNet	Method
that	O
has	O
been	O
fine	O
-	O
tuned	O
on	O
the	O
COCO	Material
dataset	Material
,	O
based	O
on	O
the	O
task	O
of	O
image	Task
-	Task
attributes	Task
classification	Task
.	O

We	O
also	O
present	O
results	O
from	O
a	O
series	O
of	O
cut	Method
down	Method
versions	Method
of	O
our	O
approach	O
for	O
comparison	O
.	O

Att	Method
-	Method
LSTM	Method
uses	O
only	O
the	O
semantic	Method
level	Method
attribute	Method
representation	Method
as	O
the	O
LSTM	O
input	O
.	O

To	O
evaluate	O
the	O
contribution	O
of	O
the	O
internal	Method
textual	Method
representation	Method
and	O
external	O
knowledge	O
for	O
the	O
question	Task
answering	Task
,	O
we	O
feed	O
the	O
image	Method
caption	Method
representation	Method
and	O
knowledge	Method
representation	Method
with	O
the	O
separately	O
,	O
producing	O
two	O
models	O
,	O
Att	Method
+	Method
Cap	Method
-	Method
LSTM	Method
and	O
Att	Method
+	Method
Know	Method
-	Method
LSTM	Method
.	O

We	O
also	O
tested	O
the	O
Cap	Method
+	Method
Know	Method
-	Method
LSTM	Method
,	O
for	O
the	O
experiment	O
completeness	O
.	O

Att	Method
+	Method
Cap	Method
+	Method
Know	Method
-	Method
LSTM	Method
combines	O
all	O
the	O
available	O
information	O
.	O

Our	O
final	O
model	O
is	O
the	O
A	Method
+	Method
C	Method
+	Method
Selected	Method
-	Method
K	Method
-	Method
LSTM	Method
,	O
which	O
uses	O
the	O
selected	O
knowledge	O
information	O
(	O
see	O
section	O
[	O
reference	O
]	O
)	O
as	O
the	O
input	O
.	O

GUESS	Method
simply	O
selects	O
the	O
modal	O
answer	O
from	O
the	O
training	O
set	O
for	O
each	O
of	O
4	O
question	O
types	O
.	O

VIS	Method
+	Method
BOW	Method
performs	O
multinomial	Method
logistic	Method
regression	Method
based	O
on	O
image	O
features	O
and	O
a	O
BOW	Method
vector	Method
obtained	O
by	O
summing	O
all	O
the	O
word	O
vectors	O
of	O
the	O
question	O
.	O

VIS	Method
+	Method
LSTM	Method
has	O
one	O
LSTM	Method
to	O
encode	O
the	O
image	O
and	O
question	O
,	O
while	O
2	O
-	O
VIS	O
+	O
BLSTM	O
has	O
two	O
image	O
feature	O
inputs	O
,	O
at	O
the	O
start	O
and	O
the	O
end	O
.	O

Malinowski	O
et	O
al	O
.	O

propose	O
a	O
neural	Method
-	Method
based	Method
approach	Method
and	O
Ma	O
et	O
al	O
.	O

encodes	O
both	O
images	Material
and	O
questions	Material
with	O
a	O
CNN	Method
.	O

Yang	O
et	O
al	O
.	O

use	O
a	O
stacked	Method
attention	Method
networks	Method
to	O
infer	O
the	O
answer	O
progressively	O
.	O

All	O
of	O
our	O
proposed	O
models	O
outperform	O
the	O
Baseline	O
method	O
.	O

And	O
our	O
final	O
model	O
A	O
+	O
C	O
+	O
Selected	O
-	Method
K	Method
-	Method
LSTM	Method
achieves	O
the	O
best	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
DAQURA	Material
-	Material
Reduced	Material
set	Material
.	O

Att	Method
+	Method
Cap	Method
+	Method
Know	Method
-	Method
LSTM	Method
performs	O
not	O
as	O
good	O
as	O
A	O
+	O
C	O
+	O
Selected	O
-	O
K	Method
-	Method
LSTM	Method
,	O
which	O
shows	O
the	O
effectiveness	O
of	O
our	O
question	Method
-	Method
based	Method
knowledge	Method
selection	Method
scheme	Method
.	O

subsubsection	O
:	O
Results	O
on	O
Toronto	Material
COCO	Material
-	Material
QA	Material
Evaluations	O
:	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
on	O
Toronto	Material
COCO	Material
-	Material
QA	Material
.	O

All	O
of	O
our	O
proposed	O
models	O
outperform	O
the	O
Baseline	O
and	O
all	O
of	O
the	O
comparator	Method
state	Method
-	Method
of	Method
-	Method
the	Method
-	Method
art	Method
methods	Method
.	O

Our	O
final	O
model	O
A	O
+	O
C	O
+	O
Selected	O
-	Method
K	Method
-	Method
LSTM	Method
achieves	O
the	O
best	O
results	O
.	O

It	O
surpasses	O
the	O
baseline	O
by	O
nearly	O
20	O
%	O
and	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
around	O
10	O
%	O
.	O

Att	Method
+	Method
Cap	Method
-	Method
LSTM	Method
clearly	O
improves	O
the	O
results	O
over	O
the	O
Att	Method
-	Method
LSTM	Method
model	Method
.	O

This	O
proves	O
that	O
internal	Method
textual	Method
representation	Method
plays	O
a	O
significant	O
role	O
in	O
the	O
VQA	Task
task	Task
.	O

The	O
Att	Method
+	Method
Know	Method
-	Method
LSTM	Method
model	Method
does	O
not	O
perform	O
as	O
well	O
as	O
Att	Method
+	Method
Cap	Method
-	Method
LSTM	Method
,	O
which	O
suggests	O
that	O
the	O
information	O
extracted	O
from	O
captions	O
is	O
more	O
valuable	O
than	O
that	O
extracted	O
from	O
the	O
KB	Material
.	O

Cap	Method
+	Method
Know	Method
-	Method
LSTM	Method
also	O
performs	O
better	O
than	O
Att	Method
+	Method
Know	Method
-	Method
LSTM	Method
.	O

This	O
is	O
not	O
surprising	O
because	O
the	O
Toronto	Material
COCO	Material
-	Material
QA	Material
questions	O
were	O
generated	O
automatically	O
from	O
the	O
MS	Material
COCO	Material
captions	Material
,	O
and	O
thus	O
the	O
fact	O
that	O
they	O
can	O
be	O
answered	O
by	O
training	O
on	O
the	O
captions	O
is	O
to	O
be	O
expected	O
.	O

This	O
generation	Method
process	Method
also	O
leads	O
to	O
questions	O
which	O
require	O
little	O
external	O
information	O
to	O
answer	O
.	O

The	O
comparison	O
on	O
the	O
Toronto	Material
COCO	Material
-	Material
QA	Material
thus	O
provides	O
an	O
important	O
benchmark	O
against	O
related	O
methods	O
,	O
but	O
does	O
not	O
really	O
test	O
the	O
ability	O
of	O
our	O
method	O
to	O
incorporate	O
extra	O
information	O
.	O

It	O
is	O
thus	O
interesting	O
that	O
the	O
additional	O
external	O
information	O
provides	O
any	O
benefit	O
at	O
all	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
per	O
-	O
category	Metric
accuracy	Metric
for	O
different	O
models	O
.	O

Surprisingly	O
,	O
the	O
counting	Metric
ability	Metric
(	O
see	O
question	O
type	O
‘	O
Number	O
’	O
)	O
increases	O
when	O
both	O
captions	O
and	O
external	O
knowledge	O
are	O
included	O
.	O

This	O
may	O
be	O
because	O
some	O
‘	O
counting	O
’	O
questions	O
are	O
not	O
framed	O
in	O
terms	O
of	O
the	O
labels	O
used	O
in	O
the	O
MS	Material
COCO	Material
captions	Material
.	O

Ren	O
et	O
al	O
.	O

also	O
observed	O
similar	O
cases	O
.	O

In	O
they	O
mentioned	O
that	O
“	O
there	O
was	O
some	O
observable	O
counting	Metric
ability	Metric
in	O
very	O
clean	O
images	O
with	O
a	O
single	O
object	O
type	O
but	O
the	O
ability	O
was	O
fairly	O
weak	O
when	O
different	O
object	O
types	O
are	O
present	O
”	O
.	O

We	O
also	O
find	O
there	O
is	O
a	O
slight	O
increase	O
for	O
the	O
‘	O
color	O
’	O
questions	O
when	O
the	O
KB	Method
is	O
used	O
.	O

Indeed	O
,	O
some	O
questions	O
like	O
‘	O
What	O
is	O
the	O
color	O
of	O
the	O
stop	O
sign	O
?	O
’	O
can	O
be	O
answered	O
directly	O
from	O
the	O
KB	Material
,	O
without	O
the	O
visual	O
cue	O
.	O

subsubsection	O
:	O
Results	O
on	O
VQA	O
Antol	O
et	O
al	O
.	O

provide	O
the	O
VQA	Material
dataset	Material
which	O
is	O
intended	O
to	O
support	O
“	O
free	Task
-	Task
form	Task
and	Task
open	Task
-	Task
ended	Task
Visual	Task
Question	Task
Answering	Task
”	O
.	O

They	O
also	O
provide	O
a	O
metric	O
for	O
measuring	O
performance	O
:	O
thus	O
means	O
that	O
at	O
least	O
3	O
of	O
the	O
10	O
humans	O
who	O
answered	O
the	O
question	O
gave	O
the	O
same	O
answer	O
.	O

Evaluation	Task
:	O
There	O
are	O
several	O
splits	O
for	O
VQA	Material
dataset	Material
,	O
such	O
as	O
the	O
validation	Material
set	Material
,	O
test	O
-	O
develop	O
and	O
test	O
-	O
standard	O
set	O
.	O

We	O
first	O
tested	O
several	O
aspects	O
of	O
our	O
models	O
on	O
the	O
validation	O
set	O
(	O
we	O
randomly	O
choose	O
5000	O
images	O
from	O
the	O
validation	O
set	O
as	O
our	O
val	O
set	O
,	O
with	O
the	O
remainder	O
testing	O
)	O
.	O

Inspecting	O
Table	O
[	O
reference	O
]	O
,	O
results	O
the	O
on	O
VQA	Material
validation	Material
set	Material
,	O
we	O
see	O
that	O
the	O
attribute	Method
-	Method
based	Method
Att	Method
-	Method
LSTM	Method
is	O
a	O
significant	O
improvement	O
over	O
our	O
VggNet	Method
+	Method
LSTM	Method
baseline	Method
.	O

We	O
also	O
evaluate	O
another	O
baseline	O
,	O
the	O
VggNet	Method
+	Method
ft	Method
+	Method
LSTM	Method
,	O
which	O
uses	O
the	O
penultimate	O
layer	O
of	O
the	O
attributes	Method
prediction	Method
CNN	Method
(	O
in	O
Section	O
[	O
reference	O
]	O
)	O
as	O
the	O
input	O
to	O
the	O
LSTM	Method
.	O

Its	O
overall	O
accuracy	Metric
on	O
the	O
VQA	Task
is	O
50.01	O
,	O
which	O
is	O
still	O
lower	O
than	O
our	O
proposed	O
models	O
(	O
detailed	O
results	O
of	O
different	O
question	O
types	O
are	O
not	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
due	O
to	O
the	O
limited	O
space	O
.	O

)	O
Adding	O
either	O
image	O
captions	O
or	O
external	O
knowledge	O
further	O
improves	O
the	O
result	O
.	O

Our	O
final	O
model	O
A	O
+	O
C	O
+	O
S	O
-	Method
K	Method
-	Method
LSTM	Method
produces	O
the	O
best	O
results	O
,	O
outperforming	O
the	O
baseline	Method
VggNet	Method
-	Method
LSTM	Method
by	O
11	O
%	O
.	O

Figure	O
[	O
reference	O
]	O
relates	O
the	O
performance	O
of	O
the	O
various	O
models	O
on	O
five	O
categories	O
of	O
questions	O
.	O

The	O
‘	O
object	O
’	O
category	O
is	O
the	O
average	O
of	O
the	O
accuracy	Metric
of	O
question	O
types	O
starting	O
with	O
‘	O
what	O
kind	O
/	O
type	O
/	O
sport	O
/	O
animal	O
/	O
brand	O
…	O
’	O
,	O
while	O
the	O
‘	O
number	O
’	O
and	O
‘	O
color	O
’	O
category	O
corresponds	O
to	O
the	O
question	O
type	O
‘	O
how	O
many	O
’	O
and	O
‘	O
what	O
color	O
’	O
.	O

The	O
performance	O
comparison	O
across	O
categories	O
is	O
of	O
particular	O
interest	O
here	O
because	O
answering	O
different	O
classes	O
of	O
questions	O
requires	O
different	O
amounts	O
of	O
external	O
knowledge	O
.	O

The	O
’	O
Where	O
’	O
questions	O
,	O
for	O
instance	O
,	O
require	O
knowledge	O
of	O
potential	O
locations	O
,	O
and	O
’	O
Why	O
’	O
questions	O
typically	O
require	O
general	O
knowledge	O
about	O
people	O
’s	O
motivation	O
.	O

’	O
Number	O
’	O
and	O
’	O
Color	O
’	O
questions	O
,	O
in	O
contrast	O
,	O
can	O
be	O
answered	O
directly	O
.	O

The	O
results	O
show	O
that	O
for	O
’	O
Why	O
’	O
questions	O
,	O
adding	O
the	O
KB	O
improves	O
performance	O
by	O
more	O
than	O
(	O
Att	Method
-	Method
LSTM	Method
achieves	O
while	O
Att	Method
+	Method
Know	Method
-	Method
LSTM	Method
achieves	O
)	O
,	O
and	O
that	O
the	O
combined	O
A	Method
+	Method
C	Method
+	Method
K	Method
-	Method
LSTM	Method
achieves	O
.	O

We	O
further	O
improve	O
it	O
to	O
by	O
using	O
the	O
question	Method
-	Method
guided	Method
knowledge	Method
selected	Method
model	Method
A	O
+	O
C	O
+	O
S	O
-	O
K	Method
-	Method
LSTM	Method
.	O

Compared	O
with	O
the	O
Att	Method
-	Method
LSTM	Method
model	Method
,	O
the	O
performance	O
gain	O
of	O
the	O
Cap	Method
+	Method
Know	Method
-	Method
LSTM	Method
model	Method
mainly	O
come	O
from	O
the	O
‘	O
why	O
’	O
and	O
‘	O
where	O
’	O
started	O
questions	O
.	O

This	O
means	O
that	O
the	O
external	O
knowledge	O
we	O
employed	O
in	O
the	O
model	O
provide	O
useful	O
information	O
to	O
answer	O
such	O
questions	O
.	O

The	O
figure	O
[	O
reference	O
]	O
shows	O
an	O
real	O
example	O
produced	O
by	O
our	O
model	O
.	O

More	O
questions	O
that	O
require	O
common	O
-	O
sense	O
knowledge	O
to	O
answer	O
can	O
be	O
found	O
in	O
the	O
supplementary	O
materials	O
.	O

We	O
have	O
also	O
tested	O
on	O
the	O
VQA	Material
test	Material
-	Material
dev	Material
and	Material
test	Material
-	Material
standard	Material
consisting	O
of	O
60	O
,	O
864	O
and	O
244	O
,	O
302	O
questions	O
(	O
for	O
which	O
ground	O
truth	O
answers	O
are	O
not	O
published	O
)	O
using	O
our	O
final	O
A	Method
+	Method
C	Method
+	Method
S	Method
-	Method
K	Method
-	Method
LSTM	Method
model	Method
,	O
and	O
evaluated	O
them	O
on	O
the	O
VQA	Metric
evaluation	Metric
server	Metric
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
server	O
reported	O
results	O
.	O

The	O
results	O
on	O
the	O
Test	O
-	O
dev	O
can	O
be	O
found	O
in	O
the	O
supplementary	O
material	O
.	O

Antol	O
et	O
al	O
.	O

provide	O
several	O
results	O
for	O
this	O
dataset	O
.	O

In	O
each	O
case	O
they	O
encode	O
the	O
image	O
with	O
the	O
final	O
hidden	Method
layer	Method
from	O
VggNet	Method
,	O
and	O
questions	Material
and	O
captions	Material
are	O
encoded	O
using	O
a	O
BOW	Method
representation	Method
.	O

A	O
softmax	Method
neural	Method
network	Method
classifier	Method
with	O
2	O
hidden	O
layers	O
and	O
1000	O
hidden	O
units	O
(	O
dropout	O
0.5	O
)	O
in	O
each	O
layer	O
with	O
tanh	O
non	O
-	O
linearity	O
is	O
then	O
trained	O
,	O
the	O
output	O
space	O
of	O
which	O
is	O
the	O
1000	O
most	O
frequent	O
answers	O
in	O
the	O
training	O
set	O
.	O

They	O
also	O
provide	O
an	O
LSTM	Method
model	Method
followed	O
by	O
a	O
softmax	Method
layer	Method
to	O
generate	O
the	O
answer	O
.	O

Two	O
version	O
of	O
this	O
approach	O
are	O
used	O
,	O
one	O
which	O
is	O
given	O
only	O
the	O
question	O
and	O
the	O
image	O
,	O
and	O
one	O
which	O
is	O
given	O
only	O
the	O
question	O
(	O
see	O
for	O
details	O
)	O
.	O

Our	O
final	O
model	O
outperforms	O
all	O
the	O
listed	O
approaches	O
according	O
to	O
the	O
overall	O
accuracy	Metric
.	O

Figure	O
[	O
reference	O
]	O
provides	O
some	O
indicative	O
results	O
.	O

More	O
results	O
can	O
be	O
found	O
in	O
the	O
supplementary	O
material	O
.	O

section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
first	O
examined	O
the	O
importance	O
of	O
introducing	O
an	O
intermediate	Method
attribute	Method
prediction	Method
layer	Method
into	O
the	O
predominant	Method
CNN	Method
-	Method
LSTM	Method
framework	Method
,	O
which	O
was	O
neglected	O
by	O
almost	O
all	O
previous	O
work	O
.	O

We	O
implemented	O
an	O
attribute	Method
-	Method
based	Method
model	Method
which	O
can	O
be	O
applied	O
to	O
the	O
task	O
of	O
image	Task
captioning	Task
.	O

We	O
have	O
shown	O
that	O
an	O
explicit	O
representation	Method
of	Method
image	Method
content	Method
improves	O
V2L	Task
performance	O
,	O
in	O
all	O
cases	O
.	O

Indeed	O
,	O
at	O
the	O
time	O
of	O
submitting	O
this	O
paper	O
,	O
our	O
image	Method
captioning	Method
model	Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
several	O
captioning	Material
datasets	Material
.	O

Secondly	O
,	O
in	O
this	O
paper	O
we	O
have	O
shown	O
that	O
it	O
is	O
possible	O
to	O
extend	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
RNN	Method
-	Method
based	Method
VQA	Method
approach	Method
so	O
as	O
to	O
incorporate	O
the	O
large	O
volumes	O
of	O
information	O
required	O
to	O
answer	O
general	O
,	O
open	O
-	O
ended	O
,	O
questions	O
about	O
images	Material
.	O

The	O
knowledge	Material
bases	Material
which	O
are	O
currently	O
available	O
do	O
not	O
contain	O
much	O
of	O
the	O
information	O
which	O
would	O
be	O
beneficial	O
to	O
this	O
process	O
,	O
but	O
nonetheless	O
can	O
still	O
be	O
used	O
to	O
significantly	O
improve	O
performance	O
on	O
questions	O
requiring	O
external	O
knowledge	O
(	O
such	O
as	O
’	O
Why	O
’	O
questions	O
)	O
.	O

The	O
approach	O
that	O
we	O
propose	O
is	O
very	O
general	O
,	O
however	O
,	O
and	O
will	O
be	O
applicable	O
to	O
more	O
informative	O
knowledge	O
bases	O
should	O
they	O
become	O
available	O
.	O

We	O
further	O
implement	O
a	O
knowledge	Method
selection	Method
scheme	Method
which	O
reflects	O
both	O
of	O
the	O
content	O
of	O
the	O
question	O
and	O
the	O
image	O
,	O
in	O
order	O
to	O
extract	O
more	O
specifically	O
related	O
information	O
.	O

Currently	O
our	O
system	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
three	O
VQA	Material
datasets	Material
and	O
produces	O
the	O
best	O
results	O
on	O
the	O
VQA	Metric
evaluation	Metric
server	Metric
.	O

Further	O
work	O
includes	O
generating	O
knowledge	O
-	O
base	O
queries	O
which	O
reflect	O
the	O
content	O
of	O
the	O
question	O
and	O
the	O
image	O
,	O
in	O
order	O
to	O
extract	O
more	O
specifically	O
related	O
information	O
.	O

The	O
Knowledge	O
Base	O
itself	O
also	O
can	O
be	O
improved	O
.	O

For	O
instance	O
,	O
Open	Method
-	Method
IE	Method
provides	O
more	O
general	O
common	O
-	O
sense	O
knowledge	O
such	O
as	O
‘	O
cats	O
eat	O
fish	O
’	O
.	O

Such	O
knowledge	O
will	O
help	O
answer	O
high	O
-	O
level	O
questions	O
.	O

section	O
:	O
Acknowledgements	O
This	O
research	O
was	O
in	O
part	O
supported	O
by	O
the	O
Data	O
to	O
Decisions	O
Cooperative	O
Research	O
Centre	O
.	O

Correspondence	O
should	O
be	O
addressed	O
to	O
C.	O
Shen	O
.	O

bibliography	O
:	O
References	O
Qi	O
Wu	O
is	O
a	O
postdoctoral	O
researcher	O
at	O
the	O
Australian	O
Centre	O
for	O
Visual	Task
Technologies	Task
(	O
ACVT	Task
)	O
of	O
the	O
University	O
of	O
Adelaide	O
.	O

His	O
research	O
interests	O
include	O
cross	Task
-	Task
depiction	Task
object	Task
detection	Task
and	Task
classification	Task
,	O
attributes	Task
learning	Task
,	O
neural	Method
networks	Method
,	O
and	O
image	Task
captioning	Task
.	O

He	O
received	O
a	O
Bachelor	O
in	O
mathematical	Task
sciences	Task
from	O
China	Material
Jiliang	Material
University	Material
,	O
a	O
Masters	O
in	O
Computer	Task
Science	Task
,	O
and	O
a	O
PhD	O
in	O
computer	Task
vision	Task
from	O
the	O
University	O
of	O
Bath	O
(	O
UK	O
)	O
in	O
2012	O
and	O
2015	O
,	O
respectively	O
.	O

Chunhua	O
Shen	O
is	O
a	O
Professor	O
of	O
computer	Task
science	Task
at	O
the	O
University	O
of	O
Adelaide	O
.	O

He	O
was	O
with	O
the	O
computer	Method
vision	Method
program	Method
at	O
NICTA	O
(	O
National	Material
ICT	Material
Australia	Material
)	O
in	O
Canberra	O
for	O
six	O
years	O
before	O
moving	O
back	O
to	O
Adelaide	O
.	O

He	O
studied	O
at	O
Nanjing	O
University	O
(	O
China	O
)	O
,	O
at	O
the	O
Australian	O
National	O
University	O
,	O
and	O
received	O
his	O
PhD	O
degree	O
from	O
the	O
University	O
of	O
Adelaide	O
.	O

In	O
2012	O
,	O
he	O
was	O
awarded	O
the	O
Australian	O
Research	O
Council	O
Future	O
Fellowship	O
.	O

Peng	O
Wang	O
is	O
a	O
postdoctoral	O
researcher	O
at	O
the	O
Australian	O
Centre	O
for	O
Visual	Task
Technologies	Task
(	O
ACVT	Task
)	O
of	O
the	O
University	O
of	O
Adelaide	O
.	O

He	O
received	O
a	O
Bachelor	O
in	O
electrical	Task
engineering	Task
and	O
automation	Task
,	O
and	O
a	O
PhD	O
in	O
control	Task
science	Task
and	Task
engineering	Task
from	O
Beihang	O
University	O
(	O
China	O
)	O
in	O
2004	O
and	O
2011	O
,	O
respectively	O
.	O

Anthony	O
Dick	O
is	O
an	O
Associate	O
Professor	O
at	O
the	O
University	O
of	O
Adelaide	O
.	O

He	O
received	O
a	O
PhD	O
degree	O
from	O
the	O
University	O
of	O
Cambridge	O
in	O
2002	O
,	O
where	O
he	O
worked	O
on	O
3D	Task
reconstruction	Task
of	Task
architecture	Task
from	O
images	Material
.	O

His	O
research	O
interests	O
include	O
image	Task
-	Task
based	Task
modeling	Task
,	O
automated	Task
video	Task
surveillance	Task
,	O
and	O
image	Task
search	Task
.	O

Anton	O
van	O
den	O
Hengel	O
is	O
a	O
Professor	O
at	O
the	O
University	O
of	O
Adelaide	O
and	O
the	O
founding	O
Director	O
of	O
The	O
Australian	O
Centre	O
for	O
Visual	Task
Technologies	Task
(	O
ACVT	Task
)	O
.	O

He	O
received	O
a	O
PhD	O
in	O
Computer	Task
Vision	Task
in	O
2000	O
,	O
a	O
Master	O
Degree	O
in	O
Computer	Task
Science	Task
in	O
1994	O
,	O
a	O
Bachelor	O
of	O
Laws	O
in	O
1993	O
,	O
and	O
a	O
Bachelor	O
of	O
Mathematical	Task
Science	Task
in	O
1991	O
,	O
all	O
from	O
The	O
University	O
of	O
Adelaide	O
.	O

