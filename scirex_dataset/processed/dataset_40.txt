document	O
:	O
Depthwise	Method
Separable	Method
Convolutions	Method
for	O
Neural	Task
Machine	Task
Translation	Task
Depthwise	Method
separable	Method
convolutions	Method
reduce	O
the	O
number	O
of	O
parameters	O
and	O
computation	O
used	O
in	O
convolutional	Method
operations	Method
while	O
increasing	O
representational	Metric
efficiency	Metric
.	O

They	O
have	O
been	O
shown	O
to	O
be	O
successful	O
in	O
image	Task
classification	Task
models	Task
,	O
both	O
in	O
obtaining	O
better	O
models	O
than	O
previously	O
possible	O
for	O
a	O
given	O
parameter	O
count	O
(	O
the	O
Xception	Method
architecture	Method
)	O
and	O
considerably	O
reducing	O
the	O
number	O
of	O
parameters	O
required	O
to	O
perform	O
at	O
a	O
given	O
level	O
(	O
the	O
MobileNets	Method
family	Method
of	Method
architectures	Method
)	O
.	O

Recently	O
,	O
convolutional	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
networks	Method
have	O
been	O
applied	O
to	O
machine	Task
translation	Task
tasks	O
with	O
good	O
results	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
how	O
depthwise	Method
separable	Method
convolutions	Method
can	O
be	O
applied	O
to	O
neural	O
machine	Task
translation	Task
.	O

We	O
introduce	O
a	O
new	O
architecture	O
inspired	O
by	O
Xception	Method
and	O
ByteNet	Method
,	O
called	O
SliceNet	Method
,	O
which	O
enables	O
a	O
significant	O
reduction	O
of	O
the	O
parameter	Metric
count	Metric
and	O
amount	O
of	O
computation	O
needed	O
to	O
obtain	O
results	O
like	O
ByteNet	Method
,	O
and	O
,	O
with	O
a	O
similar	O
parameter	O
count	O
,	O
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

In	O
addition	O
to	O
showing	O
that	O
depthwise	Method
separable	Method
convolutions	Method
perform	O
well	O
for	O
machine	Task
translation	Task
,	O
we	O
investigate	O
the	O
architectural	O
changes	O
that	O
they	O
enable	O
:	O
we	O
observe	O
that	O
thanks	O
to	O
depthwise	Method
separability	Method
,	O
we	O
can	O
increase	O
the	O
length	O
of	O
convolution	O
windows	O
,	O
removing	O
the	O
need	O
for	O
filter	Method
dilation	Method
.	O

We	O
also	O
introduce	O
a	O
new	O
"	O
super	Method
-	Method
separable	Method
"	Method
convolution	Method
operation	Method
that	O
further	O
reduces	O
the	O
number	O
of	O
parameters	O
and	O
computational	Metric
cost	Metric
for	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

section	O
:	O
Introduction	O
In	O
recent	O
years	O
,	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
recurrent	Method
neural	Method
networks	Method
(	Method
RNNs	Method
)	O
with	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	Method
LSTM	Method
)	Method
cells	Method
hochreiter1997	Method
have	O
proven	O
successful	O
at	O
many	O
natural	Task
language	Task
processing	Task
(	O
NLP	Task
)	O
tasks	O
,	O
including	O
machine	Task
translation	Task
sutskever14	O
,	O
bahdanau2014neural	O
,	O
cho2014learning	O
.	O

In	O
fact	O
,	O
the	O
results	O
they	O
yielded	O
have	O
been	O
so	O
good	O
that	O
the	O
gap	O
between	O
human	O
translations	O
and	O
machine	O
translations	O
has	O
narrowed	O
significantly	O
and	O
LSTM	Method
-	Method
based	Method
recurrent	Method
neural	Method
networks	Method
have	O
become	O
standard	O
in	O
natural	Task
language	Task
processing	Task
.	O

Even	O
more	O
recently	O
,	O
auto	Method
-	Method
regressive	Method
convolutional	Method
models	Method
have	O
proven	O
highly	O
effective	O
when	O
applied	O
to	O
audio	O
wavenet2016	O
,	O
image	O
pixelcnn2016	O
and	O
text	Task
generation	Task
bytenet2016	O
.	O

Their	O
success	O
on	O
sequence	O
data	O
in	O
particular	O
rivals	O
or	O
surpasses	O
that	O
of	O
previous	O
recurrent	Method
models	Method
bytenet2016	O
,	O
fbpaper	O
.	O

Convolutions	Method
provide	O
the	O
means	O
for	O
efficient	O
non	Task
-	Task
local	Task
referencing	Task
across	O
time	O
without	O
the	O
need	O
for	O
the	O
fully	Method
sequential	Method
processing	Method
of	Method
RNNs	Method
.	O

However	O
,	O
a	O
major	O
critique	O
of	O
such	O
models	O
is	O
their	O
computational	Metric
complexity	Metric
and	O
large	O
parameter	O
count	O
.	O

These	O
are	O
the	O
principal	O
concerns	O
addressed	O
within	O
this	O
work	O
:	O
inspired	O
by	O
the	O
efficiency	O
of	O
depthwise	Method
separable	Method
convolutions	Method
demonstrated	O
in	O
the	O
domain	O
of	O
vision	Task
,	O
in	O
particular	O
the	O
Xception	Method
architecture	Method
xception2016	O
and	O
MobileNets	Method
mobilenets2017	O
,	O
we	O
generalize	O
these	O
techniques	O
and	O
apply	O
them	O
to	O
the	O
language	Task
domain	Task
,	O
with	O
great	O
success	O
.	O

section	O
:	O
Our	O
contribution	O
We	O
present	O
a	O
new	O
convolutional	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
architecture	Method
,	O
dubbed	O
SliceNet	Method
,	O
and	O
apply	O
it	O
to	O
machine	Task
translation	Task
tasks	O
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

Our	O
architecture	O
features	O
two	O
key	O
ideas	O
:	O
Inspired	O
by	O
the	O
Xception	Method
network	Method
,	O
our	O
model	O
is	O
a	O
stack	Method
of	Method
depthwise	Method
separable	Method
convolution	Method
layers	Method
with	O
residual	O
connections	O
.	O

Such	O
an	O
architecture	O
has	O
been	O
previously	O
shown	O
to	O
perform	O
well	O
for	O
image	Task
classification	Task
.	O

We	O
also	O
experimented	O
with	O
using	O
grouped	Method
convolutions	Method
(	O
or	O
"	O
sub	Method
-	Method
separable	Method
convolutions	Method
"	O
)	O
and	O
add	O
even	O
more	O
separation	O
with	O
our	O
new	O
super	Method
-	Method
separable	Method
convolutions	Method
.	O

We	O
do	O
away	O
with	O
filter	Method
dilation	Method
in	O
our	O
architecture	O
,	O
after	O
exploring	O
the	O
trade	O
-	O
off	O
between	O
filter	Method
dilation	Method
and	O
larger	O
convolution	O
windows	O
.	O

Filter	Method
dilation	Method
was	O
previously	O
a	O
key	O
component	O
of	O
successful	O
1D	Method
convolutional	Method
architectures	Method
for	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
tasks	Task
,	O
such	O
as	O
ByteNet	Method
bytenet2016	O
and	O
WaveNet	Method
wavenet2016	O
,	O
but	O
we	O
obtain	O
better	O
results	O
without	O
dilation	Method
thanks	O
to	O
separability	Method
.	O

subsection	O
:	O
Separable	Method
convolutions	Method
and	O
grouped	Method
convolutions	Method
The	O
depthwise	Method
separable	Method
convolution	Method
operation	Method
can	O
be	O
understood	O
as	O
related	O
to	O
both	O
grouped	Method
convolutions	Method
and	O
the	O
"	O
inception	Method
modules	Method
"	O
used	O
by	O
the	O
Inception	Method
family	Method
of	Method
convolutional	Method
network	Method
architectures	Method
,	O
a	O
connection	O
explored	O
in	O
Xception	Method
xception2016	O
.	O

It	O
consists	O
of	O
a	O
depthwise	Method
convolution	Method
,	O
i.e.	O
a	O
spatial	Method
convolution	Method
performed	O
independently	O
over	O
every	O
channel	O
of	O
an	O
input	O
,	O
followed	O
by	O
a	O
pointwise	Method
convolution	Method
,	O
i.e.	O
a	O
regular	Method
convolution	Method
with	O
1x1	O
windows	O
,	O
projecting	O
the	O
channels	O
computed	O
by	O
the	O
depthwise	Method
convolution	Method
onto	O
a	O
new	O
channel	O
space	O
.	O

The	O
depthwise	Method
separable	Method
convolution	Method
operation	Method
should	O
not	O
be	O
confused	O
with	O
spatially	Method
separable	Method
convolutions	Method
,	O
which	O
are	O
also	O
often	O
called	O
“	O
separable	Method
convolutions	Method
”	O
in	O
the	O
image	Task
processing	Task
community	Task
.	O

Their	O
mathematical	O
formulation	O
is	O
as	O
follow	O
(	O
we	O
use	O
to	O
denote	O
the	O
element	O
-	O
wise	O
product	O
)	O
:	O
Thus	O
,	O
the	O
fundamental	O
idea	O
behind	O
depthwise	Method
separable	Method
convolutions	Method
is	O
to	O
replace	O
the	O
feature	Method
learning	Method
operated	O
by	O
regular	Method
convolutions	Method
over	O
a	O
joint	O
"	O
space	O
-	O
cross	O
-	O
channels	O
realm	O
"	O
into	O
two	O
simpler	O
steps	O
,	O
a	O
spatial	Method
feature	Method
learning	Method
step	Method
,	O
and	O
a	O
channel	Method
combination	Method
step	Method
.	O

This	O
is	O
a	O
powerful	O
simplification	O
under	O
the	O
oft	O
-	O
verified	O
assumption	O
that	O
the	O
2D	O
or	O
3D	O
inputs	O
that	O
convolutions	Method
operate	O
on	O
will	O
feature	O
both	O
fairly	O
independent	O
channels	O
and	O
highly	O
correlated	O
spatial	O
locations	O
.	O

A	O
deep	Method
neural	Method
network	Method
forms	O
a	O
chain	O
of	O
differentiable	Method
feature	Method
learning	Method
modules	Method
,	O
structured	O
as	O
a	O
discrete	O
set	O
of	O
units	O
,	O
each	O
trained	O
to	O
learn	O
a	O
particular	O
feature	O
.	O

These	O
units	O
are	O
subsequently	O
composed	O
and	O
combined	O
,	O
gradually	O
learning	O
higher	O
and	O
higher	O
levels	O
of	O
feature	O
abstraction	O
with	O
increasing	O
depth	O
.	O

Of	O
significance	O
is	O
the	O
availability	O
of	O
dedicated	O
feature	O
pathways	O
that	O
are	O
merged	O
together	O
later	O
in	O
the	O
network	O
;	O
this	O
is	O
one	O
property	O
enabled	O
by	O
depthwise	Method
separable	Method
convolutions	Method
,	O
which	O
define	O
independent	O
feature	O
pathways	O
that	O
are	O
later	O
merged	O
.	O

In	O
contrast	O
,	O
regular	Method
convolutional	Method
layers	Method
break	O
this	O
creed	O
by	O
learning	Method
filters	Method
that	O
must	O
simultaneously	O
perform	O
the	O
extraction	O
of	O
spatial	O
features	O
and	O
their	O
merger	O
into	O
channel	O
dimensions	O
;	O
an	O
inefficient	O
and	O
ineffective	O
use	O
of	O
parameters	O
.	O

Grouped	Method
convolutions	Method
(	O
or	O
"	O
sub	Method
-	Method
separable	Method
convolutions	Method
"	Method
)	O
are	O
an	O
intermediary	O
step	O
between	O
regular	Method
convolutions	Method
and	O
depthwise	Method
separable	Method
convolutions	Method
.	O

They	O
consist	O
in	O
splitting	O
the	O
channels	O
of	O
an	O
input	O
into	O
several	O
non	O
-	O
overlapping	O
segments	O
(	O
or	O
"	O
groups	O
"	O
)	O
,	O
performing	O
a	O
regular	Method
spatial	Method
convolution	Method
over	O
each	O
segment	O
independently	O
,	O
then	O
concatenating	O
the	O
resulting	O
feature	O
maps	O
along	O
the	O
channel	O
axis	O
.	O

Depthwise	Method
separable	Method
convolutions	Method
have	O
been	O
previously	O
shown	O
in	O
Xception	Method
xception2016	O
to	O
allow	O
for	O
image	Method
classification	Method
models	Method
that	O
outperform	O
similar	O
architectures	O
with	O
the	O
same	O
number	O
of	O
parameters	O
,	O
by	O
making	O
more	O
efficient	O
use	O
of	O
the	O
parameters	O
available	O
for	O
representation	Method
learning	Method
.	O

In	O
MobileNets	Method
mobilenets2017	O
,	O
depthwise	Method
separable	Method
convolutions	Method
allowed	O
to	O
create	O
very	O
small	O
image	Method
classification	Method
models	Method
(	O
e.g.	O
4.2	O
M	O
parameters	O
for	O
1.0	O
MobileNet	O
-	O
224	O
)	O
that	O
retained	O
much	O
of	O
the	O
capabilities	O
of	O
architectures	O
that	O
are	O
far	O
larger	O
(	O
e.g.	O
138	O
M	O
parameters	O
for	O
VGG16	O
)	O
,	O
again	O
,	O
by	O
making	O
more	O
efficient	O
use	O
of	O
parameters	O
.	O

The	O
theoretical	O
justifications	O
for	O
replacing	O
regular	Method
convolution	Method
with	O
depthwise	Method
separable	Method
convolution	Method
,	O
as	O
well	O
as	O
the	O
strong	O
gains	O
achieved	O
in	O
practice	O
by	O
such	O
architectures	O
,	O
are	O
a	O
significant	O
motivation	O
for	O
applying	O
them	O
to	O
1D	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
.	O

The	O
key	O
gains	O
from	O
separability	O
can	O
be	O
seen	O
when	O
comparing	O
the	O
number	O
of	O
parameters	O
(	O
which	O
in	O
this	O
case	O
corresponds	O
to	O
the	O
computational	Metric
cost	Metric
too	O
)	O
of	O
separable	Method
convolutions	Method
,	O
group	Method
convolutions	Method
,	O
and	O
regular	Method
convolutions	Method
.	O

Assume	O
we	O
have	O
channels	O
and	O
filters	O
(	O
often	O
or	O
more	O
)	O
and	O
a	O
receptive	O
field	O
of	O
size	O
(	O
often	O
but	O
we	O
will	O
use	O
upto	O
)	O
.	O

The	O
number	O
of	O
parameters	O
for	O
a	O
regular	Method
convolution	Method
,	O
separable	Method
convolution	Method
,	O
and	O
group	Method
convolution	Method
with	O
groups	O
is	O
:	O
subsection	O
:	O
Super	Method
-	Method
separable	Method
convolutions	Method
As	O
can	O
be	O
seen	O
above	O
,	O
the	O
size	O
(	O
and	O
cost	Metric
)	O
of	O
a	O
separable	Method
convolution	Method
with	Method
channels	Method
and	O
a	O
receptive	O
field	O
of	O
size	O
is	O
.	O

When	O
is	O
small	O
compared	O
to	O
(	O
as	O
is	O
usuallty	O
the	O
case	O
)	O
the	O
term	O
dominates	O
,	O
which	O
raises	O
the	O
question	O
how	O
it	O
could	O
be	O
reduced	O
.	O

We	O
use	O
the	O
idea	O
from	O
group	Method
convolutions	Method
and	O
the	O
recent	O
separable	Method
-	Method
LSTM	Method
paper	Method
to	O
further	O
reduce	O
this	O
size	O
by	O
factoring	O
the	O
final	O
convolution	O
,	O
and	O
we	O
call	O
the	O
result	O
a	O
super	Method
-	Method
separable	Method
convolution	Method
.	O

We	O
define	O
a	O
super	Method
-	Method
separable	Method
convolution	Method
(	O
noted	O
)	O
with	O
groups	O
as	O
follows	O
.	O

Applied	O
to	O
a	O
tensor	O
,	O
we	O
first	O
split	O
on	O
the	O
depth	O
dimension	O
into	O
groups	O
,	O
then	O
apply	O
a	O
separable	Method
convolution	Method
to	O
each	O
group	O
separately	O
,	O
and	O
then	O
concatenate	O
the	O
results	O
on	O
the	O
depth	O
dimension	O
.	O

where	O
is	O
split	O
on	O
the	O
depth	O
axis	O
and	O
for	O
are	O
the	O
parameters	O
of	O
each	O
separable	Method
convolution	Method
.	O

Since	O
each	O
is	O
of	O
size	O
and	O
each	O
is	O
of	O
size	O
,	O
the	O
final	O
size	O
of	O
a	O
super	Method
-	Method
separable	Method
convolution	Method
is	O
.	O

Parameter	O
counts	O
(	O
or	O
computational	O
budget	O
per	O
position	O
)	O
for	O
all	O
convolution	Method
types	Method
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Note	O
that	O
a	O
super	Method
-	Method
separable	Method
convolution	Method
does	O
n’t	O
allow	O
channels	O
in	O
separate	O
groups	O
to	O
exchange	O
information	O
.	O

To	O
avoid	O
making	O
a	O
bottleneck	O
of	O
this	O
kind	O
,	O
we	O
use	O
stack	Method
super	Method
-	Method
separable	Method
convolutions	Method
in	O
layer	Method
with	O
co	Method
-	Method
prime	Method
.	O

In	O
particular	O
,	O
in	O
our	O
experiments	O
we	O
always	O
alternate	O
and	O
.	O

subsection	O
:	O
Filter	Method
dilation	Method
and	O
convolution	Method
window	Method
size	Method
Filter	Method
dilation	Method
,	O
as	O
introduced	O
in	O
,	O
is	O
a	O
technique	O
for	O
aggregating	O
multiscale	O
information	O
across	O
considerably	O
larger	O
receptive	O
fields	O
in	O
convolution	O
operations	O
,	O
while	O
avoiding	O
an	O
explosion	O
in	O
parameter	O
count	O
for	O
the	O
convolution	Method
kernels	Method
.	O

It	O
has	O
been	O
presented	O
in	O
and	O
as	O
a	O
key	O
component	O
of	O
convolutional	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
autoregressive	Method
architectures	Method
.	O

When	O
dilated	O
convolution	O
layers	O
are	O
stacked	O
such	O
that	O
consecutive	O
layers	O
’	O
dilation	O
values	O
have	O
common	O
divisors	O
,	O
an	O
issue	O
similar	O
to	O
the	O
checkerboard	O
artifacts	O
in	O
deconvolutions	Method
odena2016deconvolution	O
appears	O
.	O

Uneven	O
filter	O
coverage	O
results	O
in	O
dead	O
zones	O
where	O
filter	O
coverage	O
is	O
reduced	O
(	O
as	O
displayed	O
in	O
the	O
plaid	O
-	O
like	O
appearance	O
of	O
Figure	O
1	O
in	O
dilatedconv2015	O
)	O
.	O

Choosing	O
dilation	O
factors	O
that	O
are	O
co	O
-	O
prime	O
can	O
indeed	O
offer	O
some	O
relief	O
from	O
these	O
artifacts	O
,	O
however	O
,	O
it	O
would	O
be	O
preferable	O
to	O
do	O
away	O
with	O
the	O
necessity	O
for	O
dilation	O
entirely	O
.	O

The	O
purpose	O
of	O
filter	Method
dilation	Method
is	O
to	O
increase	O
the	O
receptive	O
field	O
of	O
the	O
convolution	Method
operation	Method
,	O
i.e.	O
the	O
spatial	O
extent	O
from	O
which	O
feature	O
information	O
can	O
be	O
gathered	O
,	O
at	O
a	O
reasonable	O
computational	Metric
cost	Metric
.	O

A	O
similar	O
effect	O
would	O
be	O
achieved	O
by	O
simply	O
using	O
larger	O
convolution	O
windows	O
.	O

Besides	O
,	O
the	O
use	O
of	O
larger	O
windows	O
would	O
avoid	O
an	O
important	O
shortcoming	O
of	O
filter	Method
dilation	Method
,	O
unequal	O
convolutional	O
coverage	O
of	O
the	O
input	O
space	O
.	O

Notably	O
,	O
the	O
use	O
of	O
depthwise	Method
separable	Method
convolutions	Method
in	O
our	O
network	O
in	O
place	O
of	O
regular	Method
convolutions	Method
makes	O
each	O
convolution	Method
operation	Method
significantly	O
cheaper	O
(	O
we	O
are	O
able	O
to	O
cut	O
the	O
number	O
of	O
non	O
-	O
embedding	O
model	O
parameters	O
by	O
half	O
)	O
,	O
thus	O
lifting	O
the	O
computational	O
and	O
memory	O
limitations	O
that	O
guided	O
the	O
development	O
of	O
filter	Method
dilation	Method
in	O
the	O
first	O
place	O
.	O

In	O
our	O
experiments	O
,	O
we	O
explore	O
the	O
trade	O
-	O
off	O
between	O
using	O
lower	O
dilation	Metric
rates	Metric
and	O
increasing	O
the	O
size	O
of	O
the	O
convolution	O
windows	O
for	O
our	O
depthwise	Method
separable	Method
convolution	Method
layers	Method
.	O

In	O
contrast	O
to	O
the	O
conclusions	O
drawn	O
in	O
WaveNet	Method
and	O
ByteNet	Method
,	O
we	O
find	O
that	O
the	O
computational	Metric
savings	Metric
brought	O
on	O
by	O
depthwise	Method
separable	Method
convolutions	Method
allow	O
us	O
to	O
do	O
away	O
with	O
dilation	O
entirely	O
.	O

In	O
fact	O
,	O
we	O
observe	O
no	O
benefits	O
of	O
dilations	Method
:	O
our	O
best	O
models	O
feature	O
larger	O
filters	Method
and	O
no	O
dilation	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

A	O
comparison	O
of	O
the	O
parameter	O
count	O
for	O
different	O
convolution	Method
operations	Method
is	O
found	O
in	O
Table	O
[	O
reference	O
]	O
.	O

section	O
:	O
SliceNet	Method
architecture	Method
Here	O
we	O
present	O
the	O
model	O
we	O
use	O
for	O
our	O
experiments	O
,	O
called	O
SliceNet	Method
in	O
reference	O
to	O
the	O
way	O
separable	Method
convolutions	Method
operate	O
on	O
channel	O
-	O
wise	O
slices	O
of	O
their	O
inputs	O
.	O

Our	O
model	O
follows	O
the	O
convolutional	Method
autoregressive	Method
structure	Method
introduced	O
by	O
ByteNet	Method
bytenet2016	O
,	O
WaveNet	Method
wavenet2016	O
and	O
PixelCNN	Method
pixelcnn2016	O
.	O

Inputs	O
and	O
outputs	O
are	O
embedded	O
into	O
the	O
same	O
feature	O
depth	O
,	O
encoded	O
by	O
two	O
separate	O
sub	Method
-	Method
networks	Method
and	O
concatenated	O
before	O
being	O
fed	O
into	O
a	O
decoder	Method
that	O
autoregressively	O
generates	O
each	O
element	O
of	O
the	O
output	O
.	O

At	O
each	O
step	O
,	O
the	O
autoregressive	Method
decoder	Method
produces	O
a	O
new	O
output	Method
prediction	Method
given	O
the	O
encoded	O
inputs	O
and	O
the	O
encoding	O
of	O
the	O
existing	O
predicted	O
outputs	O
.	O

The	O
encoders	Method
and	O
the	O
decoder	Method
(	O
described	O
in	O
Section	O
[	O
reference	O
]	O
)	O
are	O
constructed	O
from	O
stacks	O
of	O
convolutional	Method
modules	Method
(	O
described	O
in	O
Section	O
[	O
reference	O
]	O
)	O
and	O
attention	O
(	O
described	O
in	O
Section	O
[	O
reference	O
]	O
)	O
is	O
used	O
to	O
allow	O
the	O
decoder	O
to	O
get	O
information	O
from	O
the	O
encoder	O
.	O

subsection	O
:	O
Convolutional	Method
modules	Method
To	O
perform	O
local	Task
computation	Task
,	O
we	O
use	O
modules	Method
of	Method
convolutions	Method
with	O
ReLU	Method
non	Method
-	Method
linearities	Method
and	O
layer	Method
normalization	Method
.	O

A	O
module	Method
of	Method
convolutions	Method
gets	O
as	O
input	O
a	O
tensor	O
of	O
shape	O
[	O
sequence	O
length	O
,	O
feature	O
channels	O
]	O
and	O
returns	O
a	O
tensor	O
of	O
the	O
same	O
shape	O
.	O

Each	O
step	O
in	O
our	O
module	O
consist	O
of	O
three	O
components	O
:	O
a	O
activation	O
of	O
the	O
inputs	O
,	O
followed	O
by	O
a	O
depthwise	Method
separable	Method
convolution	Method
,	O
followed	O
by	O
layer	Method
normalization	Method
.	O

Layer	Method
normalization	Method
layernorm2016	O
acts	O
over	O
the	O
hidden	O
units	O
of	O
the	O
layer	O
below	O
,	O
computing	O
layer	O
-	O
wise	O
statistics	O
and	O
normalizing	O
accordingly	O
.	O

These	O
normalized	O
units	O
are	O
then	O
scaled	O
and	O
shifted	O
by	O
scalar	O
learned	O
parameters	O
and	O
respectively	O
,	O
producing	O
the	O
final	O
units	O
to	O
be	O
activated	O
by	O
a	O
non	Method
-	Method
linearity	Method
:	O
where	O
the	O
sum	O
are	O
taken	O
only	O
over	O
the	O
last	O
(	O
depth	O
)	O
dimension	O
of	O
,	O
and	O
and	O
are	O
learned	O
scalars	O
.	O

A	O
complete	O
convolution	Method
step	Method
is	O
therefore	O
defined	O
as	O
:	O
The	O
convolutional	Method
steps	Method
are	O
composed	O
into	O
modules	O
by	O
stacking	O
them	O
and	O
adding	O
residual	O
connections	O
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

We	O
use	O
stacks	O
of	O
four	O
convolutional	Method
steps	Method
with	O
two	O
skip	O
-	O
connections	O
between	O
the	O
stack	O
input	O
and	O
the	O
outputs	O
of	O
the	O
second	O
and	O
fourth	O
convolutional	O
steps	O
:	O
ConvModules	Method
are	O
used	O
in	O
stacks	O
in	O
our	O
module	O
,	O
the	O
output	O
of	O
the	O
last	O
feeding	O
into	O
the	O
next	O
.	O

We	O
denote	O
a	O
stack	O
with	O
modules	O
by	O
subsection	O
:	O
Attention	Method
modules	Method
For	O
attention	Task
,	O
we	O
use	O
a	O
simple	O
inner	Method
-	Method
product	Method
attention	Method
that	O
takes	O
as	O
input	O
two	O
tensors	O
:	O
of	O
shape	O
and	O
of	O
shape	O
.	O

The	O
attention	Method
mechanism	Method
computes	O
the	O
feature	O
vector	O
similarities	O
at	O
each	O
position	O
and	O
re	O
-	O
scales	O
according	O
to	O
the	O
depth	O
:	O
To	O
allow	O
the	O
attention	O
to	O
access	O
positional	O
information	O
,	O
we	O
add	O
a	O
signal	O
that	O
carries	O
it	O
.	O

We	O
call	O
this	O
signal	O
the	O
timing	O
,	O
it	O
is	O
a	O
tensor	O
of	O
any	O
shape	O
defined	O
by	O
concatenating	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies	O
calculated	O
upto	O
:	O
Our	O
full	O
attention	Method
mechanism	Method
consists	O
of	O
adding	O
the	O
timing	O
signal	O
to	O
the	O
targets	O
,	O
performing	O
two	O
convolutional	Method
steps	Method
,	O
and	O
then	O
attending	O
to	O
the	O
source	O
:	O
subsection	O
:	O
Autoregressive	Method
structure	Method
As	O
previously	O
discussed	O
,	O
the	O
outputs	O
of	O
our	O
model	O
are	O
generated	O
in	O
an	O
autoregressive	Method
manner	Method
.	O

Unlike	O
RNNs	Method
,	O
autoregressive	Method
sequence	Method
generation	Method
depends	O
not	O
only	O
on	O
the	O
previously	O
generated	O
output	O
,	O
but	O
potentially	O
all	O
previously	O
generated	O
outputs	O
.	O

This	O
notion	O
of	O
long	O
term	O
dependencies	O
has	O
proven	O
highly	O
effect	O
in	O
NMT	Task
before	O
.	O

By	O
using	O
attention	O
,	O
establishing	O
long	O
term	O
dependencies	O
has	O
been	O
shown	O
to	O
significantly	O
boost	O
task	O
performance	O
of	O
RNNs	Method
for	O
NMT	Task
.	O

Similarly	O
,	O
a	O
convolutional	Method
autoregressive	Method
generation	Method
scheme	Method
offer	O
large	O
receptive	O
fields	O
over	O
the	O
inputs	O
and	O
past	O
outputs	O
,	O
capable	O
of	O
establishing	O
these	O
long	O
term	O
dependencies	O
.	O

Below	O
we	O
detail	O
the	O
structure	O
of	O
the	O
InputEncoder	Method
,	O
IOMixer	Method
and	Method
Decoder	Method
.	O

The	O
OutputEmbedding	Method
simply	O
performs	O
a	O
learning	Method
-	Method
embedding	Method
look	Method
-	Method
up	Method
.	O

We	O
denote	O
the	O
concatenation	O
of	O
tensors	O
and	O
along	O
the	O
dimension	O
as	O
.	O

section	O
:	O
Related	O
Work	O
Machine	Task
translation	Task
using	O
deep	Method
neural	Method
networks	Method
achieved	O
great	O
success	O
with	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
that	O
used	O
recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
with	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	Method
LSTM	Method
,	Method
)	Method
cells	Method
.	O

The	O
basic	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
architecture	Method
is	O
composed	O
of	O
an	O
RNN	Method
encoder	Method
which	O
reads	O
the	O
source	O
sentence	O
one	O
token	O
at	O
a	O
time	O
and	O
transforms	O
it	O
into	O
a	O
fixed	O
-	O
sized	O
state	O
vector	O
.	O

This	O
is	O
followed	O
by	O
an	O
RNN	Method
decoder	Method
,	O
which	O
generates	O
the	O
target	O
sentence	O
,	O
one	O
token	O
at	O
a	O
time	O
,	O
from	O
the	O
state	O
vector	O
.	O

While	O
a	O
pure	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
recurrent	Method
neural	Method
network	Method
can	O
already	O
obtain	O
good	O
translation	Task
results	O
,	O
it	O
suffers	O
from	O
the	O
fact	O
that	O
the	O
whole	O
input	O
sentence	O
needs	O
to	O
be	O
encoded	O
into	O
a	O
single	O
fixed	O
-	O
size	O
vector	O
.	O

This	O
clearly	O
manifests	O
itself	O
in	O
the	O
degradation	O
of	O
translation	Metric
quality	Metric
on	O
longer	O
sentences	O
and	O
was	O
overcome	O
in	O
by	O
using	O
a	O
neural	Method
model	Method
of	Method
attention	Method
.	O

We	O
use	O
a	O
simplified	O
version	O
of	O
this	O
neural	Method
attention	Method
mechanism	Method
in	O
SliceNet	Method
,	O
as	O
introduced	O
above	O
.	O

Convolutional	Method
architectures	Method
have	O
been	O
used	O
to	O
obtain	O
good	O
results	O
in	O
word	O
-	O
level	O
neural	O
machine	Task
translation	Task
starting	O
from	O
and	O
later	O
in	O
.	O

These	O
early	O
models	O
used	O
a	O
standard	O
RNN	Method
on	O
top	O
of	O
the	O
convolution	Method
to	O
generate	O
the	O
output	O
.	O

The	O
state	O
of	O
this	O
RNN	Method
has	O
a	O
fixed	O
size	O
,	O
and	O
in	O
the	O
first	O
one	O
the	O
sentence	Method
representation	Method
generated	O
by	O
the	O
convolutional	Method
network	Method
is	O
also	O
a	O
fixed	O
-	O
size	O
vector	O
,	O
which	O
creates	O
a	O
bottleneck	O
and	O
hurts	O
performance	O
,	O
especially	O
on	O
longer	O
sentences	O
,	O
similarly	O
to	O
the	O
limitations	O
of	O
RNN	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
without	O
attention	Method
discussed	O
above	O
.	O

Fully	O
convolutional	O
neural	O
machine	Task
translation	Task
without	O
this	O
bottleneck	O
was	O
first	O
achieved	O
in	O
and	O
.	O

The	O
model	O
in	O
(	O
Extended	Method
Neural	Method
GPU	Method
)	O
used	O
a	O
recurrent	Method
stack	Method
of	Method
gated	Method
convolutional	Method
layers	Method
,	O
while	O
the	O
model	O
in	O
(	O
ByteNet	Method
)	O
did	O
away	O
with	O
recursion	Method
and	O
used	O
left	Method
-	Method
padded	Method
convolutions	Method
in	O
the	O
decoder	Method
.	O

This	O
idea	O
,	O
introduced	O
in	O
WaveNet	Method
,	O
significantly	O
improves	O
efficiency	O
of	O
the	O
model	O
.	O

The	O
same	O
technique	O
is	O
used	O
in	O
SliceNet	Method
as	O
well	O
,	O
and	O
it	O
has	O
been	O
used	O
in	O
a	O
number	O
of	O
neural	Method
translation	Method
models	Method
recently	O
,	O
most	O
notably	O
in	O
where	O
it	O
is	O
combined	O
with	O
an	O
attention	Method
mechanism	Method
in	O
a	O
way	O
similar	O
to	O
SliceNet	Method
.	O

Depthwise	Method
separable	Method
convolutions	Method
were	O
first	O
studied	O
by	O
Sifre	O
during	O
a	O
2013	O
internship	O
at	O
Google	O
Brain	O
,	O
and	O
were	O
first	O
introduced	O
in	O
an	O
ICLR	O
2014	O
presentation	O
.	O

In	O
2016	O
,	O
they	O
were	O
demonstrated	O
to	O
yield	O
strong	O
results	O
on	O
large	Task
-	Task
scale	Task
image	Task
classification	Task
in	O
Xception	Method
,	O
and	O
in	O
2017	O
they	O
were	O
shown	O
to	O
lead	O
to	O
small	O
and	O
parameter	O
-	O
efficient	O
image	Method
classification	Method
models	Method
in	O
MobileNets	Method
.	O

section	O
:	O
Experiments	O
We	O
design	O
our	O
experiments	O
with	O
the	O
goal	O
to	O
answer	O
two	O
key	O
questions	O
:	O
What	O
is	O
the	O
performance	O
impact	O
of	O
replacing	Method
convolutions	Method
in	O
a	O
ByteNet	Method
-	Method
like	Method
model	Method
with	O
depthwise	Method
separable	Method
convolutions	Method
?	O
What	O
is	O
the	O
performance	O
trade	O
-	O
off	O
of	O
reducing	O
dilation	Task
while	O
correspondingly	O
increasing	O
convolution	O
window	O
size	O
?	O
In	O
addition	O
,	O
we	O
make	O
two	O
auxiliary	O
experiments	O
:	O
One	O
experiment	O
to	O
test	O
the	O
performance	O
of	O
an	O
intermediate	Method
separability	Method
point	O
in	O
-	O
between	O
regular	Method
convolutions	Method
and	O
full	Method
depthwise	Method
separability	Method
:	O
we	O
replace	O
depthwise	Method
separable	Method
convolutions	Method
with	O
grouped	Method
convolutions	Method
(	O
sub	Method
-	Method
separable	Method
convolutions	Method
)	O
with	O
groups	O
of	O
size	O
16	O
.	O

One	O
experiment	O
to	O
test	O
the	O
performance	O
impact	O
of	O
our	O
newly	O
-	O
introduced	O
super	Method
-	Method
separable	Method
convolutions	Method
compared	O
to	O
depthwise	Method
separable	Method
convolutions	Method
.	O

We	O
evaluate	O
all	O
models	O
on	O
the	O
WMT	Task
English	Task
to	Task
German	Task
translation	Task
task	Task
and	O
use	O
newstest2013	O
evaluation	O
set	O
for	O
this	O
purpose	O
.	O

For	O
two	O
best	O
large	O
models	O
,	O
we	O
also	O
provide	O
results	O
on	O
the	O
standard	O
test	O
set	O
,	O
newstest2014	O
,	O
to	O
compare	O
with	O
other	O
works	O
.	O

For	O
tokenization	Task
,	O
we	O
use	O
subword	O
units	O
,	O
and	O
follow	O
the	O
same	O
tokenization	Method
process	Method
as	O
Sennrich	Method
.	O

All	O
of	O
our	O
experiments	O
are	O
implemented	O
using	O
the	O
TensorFlow	Method
framework	Method
.	O

A	O
comparison	O
of	O
our	O
different	O
models	O
in	O
terms	O
of	O
parameter	Metric
count	Metric
and	O
Negative	Metric
Log	Metric
Perplexity	Metric
as	O
well	O
as	O
per	Metric
-	Metric
token	Metric
Accuracy	Metric
on	O
our	O
task	O
are	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
parameter	O
count	O
(	O
and	O
computation	Metric
cost	Metric
)	O
of	O
the	O
different	O
types	O
of	O
convolution	Method
operations	Method
used	O
was	O
already	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Our	O
experimental	O
results	O
allow	O
us	O
to	O
draw	O
the	O
following	O
conclusions	O
:	O
Depthwise	Method
separable	Method
convolutions	Method
are	O
strictly	O
superior	O
to	O
regular	Method
convolutions	Method
in	O
a	O
ByteNet	Method
-	Method
like	Method
architecture	Method
,	O
resulting	O
in	O
models	O
that	O
are	O
more	O
accurate	O
while	O
requiring	O
fewer	O
parameters	O
and	O
being	O
computationally	O
cheaper	O
to	O
train	O
and	O
run	O
.	O

Using	O
sub	Method
-	Method
separable	Method
convolutions	Method
with	O
groups	O
of	O
size	O
16	O
instead	O
of	O
full	Method
depthwise	Method
separable	Method
convolutions	Method
results	O
in	O
a	O
performance	O
dip	O
,	O
which	O
may	O
indicate	O
that	O
higher	O
separability	O
(	O
i.e.	O
groups	O
as	O
small	O
as	O
possible	O
,	O
tending	O
to	O
full	Method
depthwise	Method
separable	Method
convolutions	Method
)	O
is	O
preferable	O
in	O
this	O
setup	O
,	O
this	O
further	O
confirming	O
the	O
advantages	O
of	O
depthwise	Method
separable	Method
convolutions	Method
.	O

The	O
need	O
for	O
dilation	Task
can	O
be	O
completely	O
removed	O
by	O
using	O
correspondingly	O
larger	O
convolution	O
windows	O
,	O
which	O
is	O
made	O
computationally	O
tractable	O
by	O
the	O
use	O
of	O
depthwise	Method
separable	Method
convolutions	Method
.	O

The	O
newly	O
-	O
introduced	O
super	Method
-	Method
separable	Method
convolution	Method
operation	Method
seems	O
to	O
offer	O
an	O
incremental	O
performance	O
improvement	O
.	O

Finally	O
,	O
we	O
run	O
two	O
larger	O
models	O
with	O
a	O
design	O
based	O
on	O
the	O
conclusions	O
drawn	O
from	O
our	O
first	O
round	O
of	O
experiments	O
:	O
a	O
SliceNet	Method
model	Method
which	O
uses	O
depthwise	Method
separable	Method
convolutions	Method
and	O
a	O
SliceNet	Method
model	Method
which	O
uses	O
super	Method
-	Method
separable	Method
convolutions	Method
,	O
with	O
significantly	O
higher	O
feature	O
depth	O
in	O
both	O
cases	O
.	O

We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
where	O
we	O
also	O
include	O
previously	O
reported	O
results	O
for	O
comparison	O
.	O

For	O
getting	O
the	O
BLEU	Metric
,	O
we	O
used	O
a	O
beam	Method
-	Method
search	Method
decoder	Method
with	O
a	O
beam	O
size	O
of	O
and	O
a	O
length	O
penalty	O
tuned	O
on	O
the	O
evaluation	O
set	O
(	O
newstest2013	O
)	O
.	O

subsection	O
:	O
Conclusions	O
In	O
this	O
work	O
,	O
we	O
introduced	O
a	O
new	O
convolutional	Method
architecture	Method
for	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
tasks	Task
,	O
called	O
SliceNet	Method
,	O
based	O
on	O
the	O
use	O
of	O
depthwise	Method
separable	Method
convolutions	Method
.	O

We	O
showed	O
how	O
this	O
architecture	O
achieves	O
results	O
beating	O
not	O
only	O
ByteNet	Method
but	O
also	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
while	O
using	O
over	O
two	O
times	O
less	O
(	O
non	O
-	O
embedding	O
)	O
parameters	O
and	O
floating	O
point	O
operations	O
than	O
the	O
ByteNet	Method
architecture	Method
.	O

Additionally	O
,	O
we	O
have	O
shown	O
that	O
filter	Method
dilation	Method
,	O
previously	O
thought	O
to	O
be	O
a	O
key	O
component	O
of	O
successful	O
convolutional	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
architectures	Method
,	O
was	O
not	O
a	O
requirement	O
.	O

The	O
use	O
of	O
depthwise	Method
separable	Method
convolutions	Method
makes	O
much	O
larger	O
convolution	O
window	O
sizes	O
possible	O
,	O
and	O
we	O
found	O
that	O
we	O
could	O
achieve	O
the	O
best	O
results	O
by	O
using	O
larger	O
windows	O
instead	O
of	O
dilated	Method
filters	Method
.	O

We	O
have	O
also	O
introduced	O
a	O
new	O
type	O
of	O
depthwise	Method
separable	Method
convolution	Method
,	O
the	O
super	Method
-	Method
separable	Method
convolution	Method
,	O
which	O
shows	O
incremental	O
performance	O
improvements	O
over	O
depthwise	Method
separable	Method
convolutions	Method
.	O

Our	O
work	O
is	O
one	O
more	O
point	O
on	O
a	O
significant	O
trendline	O
started	O
with	O
Xception	Method
and	O
MobileNets	Method
,	O
that	O
indicates	O
that	O
in	O
any	O
convolutional	Method
model	Method
,	O
whether	O
for	O
1D	O
or	O
2D	O
data	O
,	O
it	O
is	O
possible	O
to	O
replace	O
convolutions	Method
with	O
depthwise	Method
separable	Method
convolutions	Method
and	O
obtain	O
a	O
model	O
that	O
is	O
simultaneously	O
cheaper	O
to	O
run	O
,	O
smaller	O
,	O
and	O
performs	O
a	O
few	O
percentage	O
points	O
better	O
.	O

This	O
trend	O
is	O
backed	O
by	O
both	O
solid	O
theoretical	O
foundations	O
and	O
strong	O
experimental	O
results	O
.	O

We	O
expect	O
our	O
current	O
work	O
to	O
play	O
a	O
significant	O
role	O
in	O
affirming	O
and	O
accelerating	O
this	O
trend	O
,	O
and	O
we	O
hope	O
to	O
see	O
depthwise	Method
separable	Method
convolutions	Method
replace	O
regular	Method
convolutions	Method
for	O
an	O
increasing	O
number	O
of	O
use	O
cases	O
in	O
the	O
future	O
.	O

bibliography	O
:	O
References	O
