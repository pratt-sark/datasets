document	O
:	O
Modelling	Task
Interaction	Task
of	Task
Sentence	Task
Pair	Task
with	O
Coupled	O
-	O
LSTMs	Method
Recently	O
,	O
there	O
is	O
rising	O
interest	O
in	O
modelling	O
the	O
interactions	Task
of	Task
two	Task
sentences	Task
with	O
deep	Method
neural	Method
networks	Method
.	O

However	O
,	O
most	O
of	O
the	O
existing	O
methods	O
encode	O
two	O
sequences	O
with	O
separate	O
encoders	Method
,	O
in	O
which	O
a	O
sentence	O
is	O
encoded	O
with	O
little	O
or	O
no	O
information	O
from	O
the	O
other	O
sentence	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
deep	Method
architecture	Method
to	O
model	O
the	O
strong	Task
interaction	Task
of	Task
sentence	Task
pair	Task
with	O
two	O
coupled	O
-	O
LSTMs	Method
.	O

Specifically	O
,	O
we	O
introduce	O
two	O
coupled	O
ways	O
to	O
model	O
the	O
interdependences	O
of	O
two	O
LSTMs	Method
,	O
coupling	O
the	O
local	O
contextualized	O
interactions	Task
of	Task
two	Task
sentences	Task
.	O

We	O
then	O
aggregate	O
these	O
interactions	O
and	O
use	O
a	O
dynamic	Method
pooling	Method
to	O
select	O
the	O
most	O
informative	O
features	O
.	O

Experiments	O
on	O
two	O
very	O
large	O
datasets	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
proposed	O
architecture	O
and	O
its	O
superiority	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

section	O
:	O
Introduction	O
Distributed	Method
representations	Method
of	Method
words	Method
or	O
sentences	O
have	O
been	O
widely	O
used	O
in	O
many	O
natural	Task
language	Task
processing	Task
(	Task
NLP	Task
)	Task
tasks	Task
,	O
such	O
as	O
text	Task
classification	Task
,	O
question	Task
answering	Task
and	O
machine	Task
translation	Task
and	O
so	O
on	O
.	O

Among	O
these	O
tasks	O
,	O
a	O
common	O
problem	O
is	O
modelling	O
the	O
relevance	O
/	O
similarity	O
of	O
the	O
sentence	O
pair	O
,	O
which	O
is	O
also	O
called	O
text	Task
semantic	Task
matching	Task
.	O

Recently	O
,	O
deep	Method
learning	Method
based	Method
models	Method
is	O
rising	O
a	O
substantial	O
interest	O
in	O
text	Task
semantic	Task
matching	Task
and	O
have	O
achieved	O
some	O
great	O
progresses	O
.	O

According	O
to	O
the	O
phases	O
of	O
interaction	O
between	O
two	O
sentences	O
,	O
previous	O
models	O
can	O
be	O
classified	O
into	O
three	O
categories	O
.	O

paragraph	O
:	O
Weak	Method
interaction	Method
Models	Method
Some	O
early	O
works	O
focus	O
on	O
sentence	O
level	O
interactions	O
,	O
such	O
as	O
ARC	Method
-	Method
I	Method
,	O
CNTN	Method
and	O
so	O
on	O
.	O

These	O
models	O
first	O
encode	O
two	O
sequences	O
with	O
some	O
basic	O
(	O
Neural	Method
Bag	Method
-	Method
of	Method
-	Method
words	Method
,	Method
BOW	Method
)	O
or	O
advanced	Method
(	Method
RNN	Method
,	Method
CNN	Method
)	Method
components	Method
of	O
neural	Method
networks	Method
separately	O
,	O
and	O
then	O
compute	O
the	O
matching	Metric
score	Metric
based	O
on	O
the	O
distributed	O
vectors	O
of	O
two	O
sentences	O
.	O

In	O
this	O
paradigm	O
,	O
two	O
sentences	O
have	O
no	O
interaction	O
until	O
arriving	O
final	O
phase	O
.	O

paragraph	O
:	O
Semi	Method
-	Method
interaction	Method
Models	Method
Some	O
improved	O
methods	O
focus	O
on	O
utilizing	O
multi	Method
-	Method
granularity	Method
representation	Method
(	O
word	O
,	O
phrase	O
and	O
sentence	O
level	O
)	O
,	O
such	O
as	O
MultiGranCNN	Method
and	O
Multi	Method
-	Method
Perspective	Method
CNN	Method
.	O

Another	O
kind	O
of	O
models	O
use	O
soft	Method
attention	Method
mechanism	Method
to	O
obtain	O
the	O
representation	O
of	O
one	O
sentence	O
by	O
depending	O
on	O
representation	O
of	O
another	O
sentence	O
,	O
such	O
as	O
ABCNN	Method
,	O
Attention	O
LSTM	Method
.	O

These	O
models	O
can	O
alleviate	O
the	O
weak	Task
interaction	Task
problem	Task
,	O
but	O
are	O
still	O
insufficient	O
to	O
model	O
the	O
contextualized	O
interaction	O
on	O
the	O
word	O
as	O
well	O
as	O
phrase	O
level	O
.	O

paragraph	O
:	O
Strong	Method
Interaction	Method
Models	Method
These	O
models	O
directly	O
build	O
an	O
interaction	O
space	O
between	O
two	O
sentences	O
and	O
model	O
the	O
interaction	O
at	O
different	O
positions	O
.	O

ARC	Method
-	Method
II	Method
and	O
MV	O
-	O
LSTM	Method
.	O

These	O
models	O
enable	O
the	O
model	O
to	O
easily	O
capture	O
the	O
difference	O
between	O
semantic	O
capacity	O
of	O
two	O
sentences	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
deep	Method
neural	Method
network	Method
architecture	Method
to	O
model	O
the	O
strong	O
interactions	Task
of	Task
two	Task
sentences	Task
.	O

Different	O
with	O
modelling	O
two	O
sentences	O
with	O
separated	O
LSTMs	Method
,	O
we	O
utilize	O
two	O
interdependent	O
LSTMs	Method
,	O
called	O
coupled	O
-	O
LSTMs	Method
,	O
to	O
fully	O
affect	O
each	O
other	O
at	O
different	O
time	O
steps	O
.	O

The	O
output	O
of	O
coupled	O
-	O
LSTMs	Method
at	O
each	O
step	O
depends	O
on	O
both	O
sentences	O
.	O

Specifically	O
,	O
we	O
propose	O
two	O
interdependent	O
ways	O
for	O
the	O
coupled	O
-	O
LSTMs	Method
:	O
loosely	Method
coupled	Method
model	Method
(	O
LC	Method
-	Method
LSTMs	Method
)	O
and	O
tightly	Method
coupled	Method
model	Method
(	O
TC	Method
-	Method
LSTMs	Method
)	O
.	O

Similar	O
to	O
bidirectional	O
LSTM	Method
for	O
single	O
sentence	O
,	O
there	O
are	O
four	O
directions	O
can	O
be	O
used	O
in	O
coupled	O
-	O
LSTMs	Method
.	O

To	O
utilize	O
all	O
the	O
information	O
of	O
four	O
directions	O
of	O
coupled	O
-	O
LSTMs	Method
,	O
we	O
aggregate	O
them	O
and	O
adopt	O
a	O
dynamic	Method
pooling	Method
strategy	Method
to	O
automatically	O
select	O
the	O
most	O
informative	O
interaction	O
signals	O
.	O

Finally	O
,	O
we	O
feed	O
them	O
into	O
a	O
fully	Method
connected	Method
layer	Method
,	O
followed	O
by	O
an	O
output	Method
layer	Method
to	O
compute	O
the	O
matching	Metric
score	Metric
.	O

The	O
contributions	O
of	O
this	O
paper	O
can	O
be	O
summarized	O
as	O
follows	O
.	O

Different	O
with	O
the	O
architectures	O
of	O
using	O
similarity	Method
matrix	Method
,	O
our	O
proposed	O
architecture	O
directly	O
model	O
the	O
strong	O
interactions	Task
of	Task
two	Task
sentences	Task
with	O
coupled	O
-	O
LSTMs	Method
,	O
which	O
can	O
capture	O
the	O
useful	O
local	O
semantic	O
relevances	O
of	O
two	O
sentences	O
.	O

Our	O
architecture	O
can	O
also	O
capture	O
the	O
multiple	O
granular	O
interactions	O
by	O
several	O
stacked	O
coupled	O
-	O
LSTMs	Method
layers	O
.	O

Compared	O
to	O
the	O
previous	O
works	O
on	O
text	Task
matching	Task
,	O
we	O
perform	O
extensive	O
empirical	O
studies	O
on	O
two	O
very	O
large	O
datasets	O
.	O

The	O
massive	O
scale	O
of	O
the	O
datasets	O
allows	O
us	O
to	O
train	O
a	O
very	O
deep	Method
neural	Method
networks	Method
.	O

Experiment	O
results	O
demonstrate	O
that	O
our	O
proposed	O
architecture	O
is	O
more	O
effective	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

section	O
:	O
Sentence	Method
Modelling	Method
with	O
LSTM	Method
Long	O
short	O
-	O
term	O
memory	O
network	O
(	O
LSTM	Method
)	O
is	O
a	O
type	O
of	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
,	O
and	O
specifically	O
addresses	O
the	O
issue	O
of	O
learning	Task
long	Task
-	Task
term	Task
dependencies	Task
.	O

LSTM	Method
maintains	O
a	O
memory	O
cell	O
that	O
updates	O
and	O
exposes	O
its	O
content	O
only	O
when	O
deemed	O
necessary	O
.	O

While	O
there	O
are	O
numerous	O
LSTM	Method
variants	O
,	O
here	O
we	O
use	O
the	O
LSTM	Method
architecture	O
used	O
by	O
,	O
which	O
is	O
similar	O
to	O
the	O
architecture	O
of	O
but	O
without	O
peep	O
-	O
hole	O
connections	O
.	O

We	O
define	O
the	O
LSTM	Method
units	O
at	O
each	O
time	O
step	O
to	O
be	O
a	O
collection	O
of	O
vectors	O
in	O
:	O
an	O
input	O
gate	O
,	O
a	O
forget	O
gate	O
,	O
an	O
output	O
gate	O
,	O
a	O
memory	O
cell	O
and	O
a	O
hidden	O
state	O
.	O

is	O
the	O
number	O
of	O
the	O
LSTM	Method
units	O
.	O

The	O
elements	O
of	O
the	O
gating	O
vectors	O
,	O
and	O
are	O
in	O
.	O

The	O
LSTM	Method
is	O
precisely	O
specified	O
as	O
follows	O
.	O

where	O
is	O
the	O
input	O
at	O
the	O
current	O
time	O
step	O
;	O
is	O
an	O
affine	Method
transformation	Method
which	O
depends	O
on	O
parameters	O
of	O
the	O
network	O
and	O
.	O

denotes	O
the	O
logistic	Method
sigmoid	Method
function	Method
and	O
denotes	O
elementwise	Method
multiplication	Method
.	O

Intuitively	O
,	O
the	O
forget	O
gate	O
controls	O
the	O
amount	O
of	O
which	O
each	O
unit	O
of	O
the	O
memory	O
cell	O
is	O
erased	O
,	O
the	O
input	O
gate	O
controls	O
how	O
much	O
each	O
unit	O
is	O
updated	O
,	O
and	O
the	O
output	O
gate	O
controls	O
the	O
exposure	O
of	O
the	O
internal	O
memory	O
state	O
.	O

The	O
update	O
of	O
each	O
LSTM	Method
unit	O
can	O
be	O
written	O
precisely	O
as	O
follows	O
Here	O
,	O
the	O
function	O
is	O
a	O
shorthand	O
for	O
Eq	O
.	O

(	O
[	O
reference	O
]	O
-	O
[	O
reference	O
]	O
)	O
.	O

section	O
:	O
Coupled	O
-	O
LSTMs	Method
for	O
Strong	Task
Sentence	Task
Interaction	Task
To	O
deal	O
with	O
two	O
sentences	O
,	O
one	O
straightforward	O
method	O
is	O
to	O
model	O
them	O
with	O
two	O
separate	O
LSTMs	Method
.	O

However	O
,	O
this	O
method	O
is	O
difficult	O
to	O
model	O
local	O
interactions	Task
of	Task
two	Task
sentences	Task
.	O

An	O
improved	O
way	O
is	O
to	O
introduce	O
attention	Method
mechanism	Method
,	O
which	O
has	O
been	O
used	O
in	O
many	O
tasks	O
,	O
such	O
as	O
machine	Task
translation	Task
and	O
question	Task
answering	Task
.	O

Inspired	O
by	O
the	O
multi	Method
-	Method
dimensional	Method
recurrent	Method
neural	Method
network	Method
and	O
grid	O
LSTM	Method
in	O
computer	Task
vision	Task
community	Task
,	O
we	O
propose	O
two	O
models	O
to	O
capture	O
the	O
interdependences	O
between	O
two	O
parallel	O
LSTMs	Method
,	O
called	O
coupled	O
-	O
LSTMs	Method
(	O
C	Method
-	Method
LSTMs	Method
)	O
.	O

[	O
Parallel	O
LSTMs	Method
]	O
[	O
Attention	O
LSTMs	Method
]	O
[	O
Loosely	O
coupled	O
-	O
LSTMs	Method
]	O
[	O
Tightly	Method
coupled	Method
-	Method
LSTMs	Method
]	O
To	O
facilitate	O
our	O
models	O
,	O
we	O
firstly	O
give	O
some	O
definitions	O
.	O

Given	O
two	O
sequences	O
and	O
,	O
we	O
let	O
denote	O
the	O
embedded	Method
representation	Method
of	O
the	O
word	O
.	O

The	O
standard	O
LSTM	Method
have	O
one	O
temporal	O
dimension	O
.	O

When	O
dealing	O
with	O
a	O
sentence	O
,	O
LSTM	Method
regards	O
the	O
position	O
as	O
time	O
step	O
.	O

At	O
position	O
of	O
sentence	O
,	O
the	O
output	O
reflects	O
the	O
meaning	O
of	O
subsequence	O
.	O

To	O
model	O
the	O
interaction	O
of	O
two	O
sentences	O
as	O
early	O
as	O
possible	O
,	O
we	O
define	O
to	O
represent	O
the	O
interaction	O
of	O
the	O
subsequences	O
and	O
.	O

Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
and	O
[	O
reference	O
]	O
(	O
d	O
)	O
illustrate	O
our	O
two	O
propose	O
models	O
.	O

For	O
intuitive	O
comparison	O
of	O
weak	Method
interaction	Method
parallel	Method
LSTMs	Method
,	O
we	O
also	O
give	O
parallel	O
LSTMs	Method
and	O
attention	O
LSTMs	Method
in	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
and	O
[	O
reference	O
]	O
(	O
b	O
)	O
.	O

We	O
describe	O
our	O
two	O
proposed	O
models	O
as	O
follows	O
.	O

subsection	O
:	O
Loosely	O
Coupled	O
-	O
LSTMs	Method
(	O
LC	O
-	O
LSTMs	Method
)	O
To	O
model	O
the	O
local	O
contextual	O
interactions	Task
of	Task
two	Task
sentences	Task
,	O
we	O
enable	O
two	O
LSTMs	Method
to	O
be	O
interdependent	O
at	O
different	O
positions	O
.	O

Inspired	O
by	O
Grid	O
LSTM	Method
and	O
word	O
-	O
by	O
-	O
word	O
attention	O
LSTMs	Method
,	O
we	O
propose	O
a	O
loosely	Method
coupling	Method
model	Method
for	O
two	O
interdependent	O
LSTMs	Method
.	O

More	O
concretely	O
,	O
we	O
refer	O
to	O
as	O
the	O
encoding	O
of	O
subsequence	O
in	O
the	O
first	O
LSTM	Method
influenced	O
by	O
the	O
output	O
of	O
the	O
second	O
LSTM	Method
on	O
subsequence	O
.	O

Meanwhile	O
,	O
is	O
the	O
encoding	O
of	O
subsequence	O
in	O
the	O
second	O
LSTM	Method
influenced	O
by	O
the	O
output	O
of	O
the	O
first	O
LSTM	Method
on	O
subsequence	O
and	O
are	O
computed	O
as	O
where	O
subsection	O
:	O
Tightly	Method
Coupled	Method
-	Method
LSTMs	Method
(	O
TC	Method
-	Method
LSTMs	Method
)	O
The	O
hidden	O
states	O
of	O
LC	Method
-	Method
LSTMs	Method
are	O
the	O
combination	O
of	O
the	O
hidden	O
states	O
of	O
two	O
interdependent	O
LSTMs	Method
,	O
whose	O
memory	O
cells	O
are	O
separated	O
.	O

Inspired	O
by	O
the	O
configuration	O
of	O
the	O
multi	O
-	O
dimensional	O
LSTM	Method
,	O
we	O
further	O
conflate	O
both	O
the	O
hidden	O
states	O
and	O
the	O
memory	O
cells	O
of	O
two	O
LSTMs	Method
.	O

We	O
assume	O
that	O
directly	O
model	O
the	O
interaction	O
of	O
the	O
subsequences	O
and	O
,	O
which	O
depends	O
on	O
two	O
previous	O
interaction	O
and	O
,	O
where	O
are	O
the	O
positions	O
in	O
sentence	O
and	O
.	O

We	O
define	O
a	O
tightly	Method
coupled	Method
-	Method
LSTMs	Method
units	Method
as	O
follows	O
.	O

where	O
the	O
gating	Method
units	Method
and	O
determine	O
which	O
memory	O
units	O
are	O
affected	O
by	O
the	O
inputs	O
through	O
,	O
and	O
which	O
memory	O
cells	O
are	O
written	O
to	O
the	O
hidden	O
units	O
.	O

is	O
an	O
affine	Method
transformation	Method
which	O
depends	O
on	O
parameters	O
of	O
the	O
network	O
and	O
.	O

In	O
contrast	O
to	O
the	O
standard	O
LSTM	Method
defined	O
over	O
time	O
,	O
each	O
memory	Method
unit	Method
of	O
a	O
tightly	O
coupled	O
-	O
LSTMs	Method
has	O
two	O
preceding	O
states	O
and	O
and	O
two	O
corresponding	O
forget	O
gates	O
and	O
.	O

subsection	O
:	O
Analysis	O
of	O
Two	O
Proposed	O
Models	O
Our	O
two	O
proposed	O
coupled	O
-	O
LSTMs	Method
can	O
be	O
formulated	O
as	O
where	O
C	Method
-	Method
LSTMs	Method
can	O
be	O
either	O
TC	Method
-	Method
LSTMs	Method
or	O
LC	Method
-	Method
LSTMs	Method
.	O

The	O
input	O
consisted	O
of	O
two	O
type	O
of	O
information	O
at	O
step	O
(	O
i	O
,	O
j	O
)	O
in	O
coupled	O
-	O
LSTMs	Method
:	O
temporal	O
dimension	O
h	O
-	O
i1	O
,	O
j	O
,	O
hi	O
,-	O
j1	O
,	O
c	O
-	O
i1	O
,	O
j	O
,	O
ci	O
,-	O
j1	O
and	O
depth	O
dimension	O
xi	O
,	O
yj	O
.	O

The	O
difference	O
between	O
TC	Method
-	Method
LSTMs	Method
and	O
LC	Method
-	Method
LSTMs	Method
is	O
the	O
dependence	O
of	O
information	O
from	O
temporal	O
and	O
depth	O
dimension	O
.	O

paragraph	O
:	O
Interaction	O
Between	O
Temporal	O
Dimensions	O
The	O
TC	Method
-	Method
LSTMs	Method
model	Method
the	O
interactions	O
at	O
position	O
by	O
merging	O
the	O
internal	O
memory	O
and	O
hidden	O
state	O
along	O
row	O
and	O
column	O
dimensions	O
.	O

In	O
contrast	O
with	O
TC	Method
-	Method
LSTMs	Method
,	O
LC	Method
-	Method
LSTMs	Method
firstly	O
use	O
two	O
standard	O
LSTMs	Method
in	O
parallel	O
,	O
producing	O
hidden	O
states	O
and	O
along	O
row	O
and	O
column	O
dimensions	O
respectively	O
,	O
which	O
are	O
then	O
merged	O
together	O
flowing	O
next	O
step	O
.	O

paragraph	O
:	O
Interaction	O
Between	O
Depth	O
Dimension	O
In	O
TC	Method
-	Method
LSTMs	Method
,	O
each	O
hidden	O
state	O
at	O
higher	O
layer	O
receives	O
a	O
fusion	O
of	O
information	O
and	O
,	O
flowed	O
from	O
lower	O
layer	O
.	O

However	O
,	O
in	O
LC	Method
-	Method
LSTMs	Method
,	O
the	O
information	O
and	O
are	O
accepted	O
by	O
two	O
corresponding	O
LSTMs	Method
at	O
the	O
higher	O
layer	O
separately	O
.	O

The	O
two	O
architectures	O
have	O
their	O
own	O
characteristics	O
,	O
TC	Method
-	Method
LSTMs	Method
give	O
more	O
strong	O
interactions	O
among	O
different	O
dimensions	O
while	O
LC	Method
-	Method
LSTMs	Method
ensures	O
the	O
two	O
sequences	O
interact	O
closely	O
without	O
being	O
conflated	O
using	O
two	O
separated	O
LSTMs	Method
.	O

subsubsection	O
:	O
Comparison	O
of	O
LC	Method
-	Method
LSTMs	Method
and	O
word	O
-	O
by	O
-	O
word	O
Attention	O
LSTMs	Method
The	O
main	O
idea	O
of	O
attention	O
LSTMs	Method
is	O
that	O
the	O
representation	O
of	O
sentence	O
X	O
is	O
obtained	O
dynamically	O
based	O
on	O
the	O
alignment	O
degree	O
between	O
the	O
words	O
in	O
sentence	O
X	O
and	O
Y	O
,	O
which	O
is	O
asymmetric	O
unidirectional	Method
encoding	Method
.	O

Nevertheless	O
,	O
in	O
LC	O
-	O
LSTM	Method
,	O
each	O
hidden	O
state	O
of	O
each	O
step	O
is	O
obtained	O
with	O
the	O
consideration	O
of	O
interaction	O
between	O
two	O
sequences	O
with	O
symmetrical	O
encoding	O
fashion	O
.	O

section	O
:	O
End	O
-	O
to	O
-	O
End	Method
Architecture	Method
for	O
Sentence	Task
Matching	Task
In	O
this	O
section	O
,	O
we	O
present	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
deep	Method
architecture	Method
for	O
matching	O
two	O
sentences	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Embedding	Method
Layer	Method
To	O
model	O
the	O
sentences	O
with	O
neural	Method
model	Method
,	O
we	O
firstly	O
need	O
transform	O
the	O
one	Method
-	Method
hot	Method
representation	Method
of	Method
word	Method
into	O
the	O
distributed	Method
representation	Method
.	O

All	O
words	O
of	O
two	O
sequences	O
and	O
will	O
be	O
mapped	O
into	O
low	Method
dimensional	Method
vector	Method
representations	Method
,	O
which	O
are	O
taken	O
as	O
input	O
of	O
the	O
network	O
.	O

subsection	O
:	O
Stacked	O
Coupled	O
-	O
LSTMs	Method
Layers	O
After	O
the	O
embedding	Method
layer	Method
,	O
we	O
use	O
our	O
proposed	O
coupled	O
-	O
LSTMs	Method
to	O
capture	O
the	O
strong	O
interactions	O
between	O
two	O
sentences	O
.	O

A	O
basic	O
block	O
consists	O
of	O
five	O
layers	O
.	O

We	O
firstly	O
use	O
four	O
directional	O
coupled	O
-	O
LSTMs	Method
to	O
model	O
the	O
local	O
interactions	O
with	O
different	O
information	O
flows	O
.	O

And	O
then	O
we	O
sum	O
the	O
outputs	O
of	O
these	O
LSTMs	Method
by	O
aggregation	Method
layer	Method
.	O

To	O
increase	O
the	O
learning	Task
capabilities	O
of	O
the	O
coupled	O
-	O
LSTMs	Method
,	O
we	O
stack	O
the	O
basic	O
block	O
on	O
top	O
of	O
each	O
other	O
.	O

subsubsection	O
:	O
Four	O
Directional	O
Coupled	O
-	O
LSTMs	Method
Layers	O
The	O
C	Method
-	Method
LSTMs	Method
is	O
defined	O
along	O
a	O
certain	O
pre	O
-	O
defined	O
direction	O
,	O
we	O
can	O
extend	O
them	O
to	O
access	O
to	O
the	O
surrounding	O
context	O
in	O
all	O
directions	O
.	O

Similar	O
to	O
bi	O
-	O
directional	O
LSTM	Method
,	O
there	O
are	O
four	O
directions	O
in	O
coupled	O
-	O
LSTMs	Method
.	O

subsubsection	O
:	O
Aggregation	Method
Layer	Method
The	O
aggregation	Method
layer	Method
sums	O
the	O
outputs	O
of	O
four	O
directional	O
coupled	O
-	O
LSTMs	Method
into	O
a	O
vector	O
.	O

where	O
the	O
superscript	O
of	O
denotes	O
the	O
different	O
directions	O
.	O

subsubsection	O
:	O
Stacking	Method
C	Method
-	Method
LSTMs	Method
Blocks	Method
To	O
increase	O
the	O
capabilities	O
of	O
network	O
of	O
learning	Task
multiple	Task
granularities	Task
of	Task
interactions	Task
,	O
we	O
stack	O
several	O
blocks	O
(	O
four	O
C	O
-	O
LSTMs	Method
layers	O
and	O
one	O
aggregation	Method
layer	Method
)	O
to	O
form	O
deep	Method
architectures	Method
.	O

subsection	O
:	O
Pooling	Method
Layer	Method
The	O
output	O
of	O
stacked	O
coupled	O
-	O
LSTMs	Method
layers	O
is	O
a	O
tensor	O
,	O
where	O
and	O
are	O
the	O
lengths	O
of	O
sentences	O
,	O
and	O
is	O
the	O
number	O
of	O
hidden	O
neurons	O
.	O

We	O
apply	O
dynamic	Method
pooling	Method
to	O
automatically	O
extract	O
subsampling	O
matrix	O
in	O
each	O
slice	O
,	O
similar	O
to	O
.	O

More	O
formally	O
,	O
for	O
each	O
slice	O
matrix	O
,	O
we	O
partition	O
the	O
rows	O
and	O
columns	O
of	O
into	O
roughly	O
equal	O
grids	O
.	O

These	O
grid	O
are	O
non	O
-	O
overlapping	O
.	O

Then	O
we	O
select	O
the	O
maximum	O
value	O
within	O
each	O
grid	O
.	O

Since	O
each	O
slice	O
consists	O
of	O
the	O
hidden	O
states	O
of	O
one	O
neuron	O
at	O
different	O
positions	O
,	O
the	O
pooling	Method
operation	Method
can	O
be	O
regarded	O
as	O
the	O
most	O
informative	O
interactions	O
captured	O
by	O
the	O
neuron	O
.	O

Thus	O
,	O
we	O
get	O
a	O
tensor	O
,	O
which	O
is	O
further	O
reshaped	O
into	O
a	O
vector	O
.	O

subsection	O
:	O
Fully	Method
-	Method
Connected	Method
Layer	Method
The	O
vector	O
obtained	O
by	O
pooling	Method
layer	Method
is	O
fed	O
into	O
a	O
full	Method
connection	Method
layer	Method
to	O
obtain	O
a	O
final	O
more	O
abstractive	Method
representation	Method
.	O

subsection	O
:	O
Output	O
Layer	O
The	O
output	O
layer	O
depends	O
on	O
the	O
types	O
of	O
the	O
tasks	O
,	O
we	O
choose	O
the	O
corresponding	O
form	O
of	O
output	O
layer	O
.	O

There	O
are	O
two	O
popular	O
types	O
of	O
text	Task
matching	Task
tasks	Task
in	O
NLP	Task
.	O

One	O
is	O
ranking	Task
task	Task
,	O
such	O
as	O
community	Task
question	Task
answering	Task
.	O

Another	O
is	O
classification	Task
task	Task
,	O
such	O
as	O
textual	Task
entailment	Task
.	O

For	O
ranking	Task
task	Task
,	O
the	O
output	O
is	O
a	O
scalar	Metric
matching	Metric
score	Metric
,	O
which	O
is	O
obtained	O
by	O
a	O
linear	Method
transformation	Method
after	O
the	O
last	O
fully	Method
-	Method
connected	Method
layer	Method
.	O

For	O
classification	Task
task	Task
,	O
the	O
outputs	O
are	O
the	O
probabilities	O
of	O
the	O
different	O
classes	O
,	O
which	O
is	O
computed	O
by	O
a	O
softmax	Method
function	Method
after	O
the	O
last	O
fully	Method
-	Method
connected	Method
layer	Method
.	O

section	O
:	O
Training	O
Our	O
proposed	O
architecture	O
can	O
deal	O
with	O
different	O
sentence	Task
matching	Task
tasks	Task
.	O

The	O
loss	Method
functions	Method
varies	O
with	O
different	O
tasks	O
.	O

paragraph	O
:	O
Max	Metric
-	Metric
Margin	Metric
Loss	Metric
for	O
Ranking	Task
Task	Task
Given	O
a	O
positive	O
sentence	O
pair	O
and	O
its	O
corresponding	O
negative	O
pair	O
.	O

The	O
matching	Metric
score	Metric
should	O
be	O
larger	O
than	O
.	O

For	O
this	O
task	O
,	O
we	O
use	O
the	O
contrastive	Method
max	Method
-	Method
margin	Method
criterion	Method
to	O
train	O
our	O
models	O
on	O
matching	Task
task	Task
.	O

The	O
ranking	Metric
-	Metric
based	Metric
loss	Metric
is	O
defined	O
as	O
where	O
is	O
predicted	Metric
matching	Metric
score	Metric
for	O
.	O

paragraph	O
:	O
Cross	Metric
-	Metric
entropy	Metric
Loss	Metric
for	O
Classification	Task
Task	Task
Given	O
a	O
sentence	O
pair	O
and	O
its	O
label	O
.	O

The	O
output	O
of	O
neural	Method
network	Method
is	O
the	O
probabilities	O
of	O
the	O
different	O
classes	O
.	O

The	O
parameters	O
of	O
the	O
network	O
are	O
trained	O
to	O
minimise	O
the	O
cross	O
-	O
entropy	O
of	O
the	O
predicted	O
and	O
true	O
label	O
distributions	O
.	O

where	O
l	O
is	O
one	O
-	O
hot	Method
representation	Method
of	O
the	O
ground	O
-	O
truth	O
label	O
;	O
is	O
predicted	O
probabilities	O
of	O
labels	O
;	O
is	O
the	O
class	O
number	O
.	O

To	O
minimize	O
the	O
objective	O
,	O
we	O
use	O
stochastic	Method
gradient	Method
descent	Method
with	O
the	O
diagonal	Method
variant	Method
of	Method
AdaGrad	Method
.	O

To	O
prevent	O
exploding	O
gradients	O
,	O
we	O
perform	O
gradient	Method
clipping	Method
by	O
scaling	O
the	O
gradient	O
when	O
the	O
norm	O
exceeds	O
a	O
threshold	O
.	O

section	O
:	O
Experiment	O
In	O
this	O
section	O
,	O
we	O
investigate	O
the	O
empirical	O
performances	O
of	O
our	O
proposed	O
model	O
on	O
two	O
different	O
text	Task
matching	Task
tasks	Task
:	O
classification	Task
task	Task
(	O
recognizing	Task
textual	Task
entailment	Task
)	O
and	O
ranking	Task
task	Task
(	O
matching	Task
of	Task
question	Task
and	Task
answer	Task
)	O
.	O

subsection	O
:	O
Hyperparameters	Method
and	O
Training	O
The	O
word	O
embeddings	O
for	O
all	O
of	O
the	O
models	O
are	O
initialized	O
with	O
the	O
100d	O
GloVe	O
vectors	O
(	O
840B	O
token	O
version	O
,	O
)	O
and	O
fine	O
-	O
tuned	O
during	O
training	O
to	O
improve	O
the	O
performance	O
.	O

The	O
other	O
parameters	O
are	O
initialized	O
by	O
randomly	O
sampling	O
from	O
uniform	O
distribution	O
in	O
.	O

For	O
each	O
task	O
,	O
we	O
take	O
the	O
hyperparameters	O
which	O
achieve	O
the	O
best	O
performance	O
on	O
the	O
development	O
set	O
via	O
an	O
small	O
grid	O
search	O
over	O
combinations	O
of	O
the	O
initial	O
learning	Metric
rate	Metric
,	O
regularization	O
and	O
the	O
threshold	O
value	O
of	O
gradient	O
norm	O
[	O
5	O
,	O
10	O
,	O
100	O
]	O
.	O

The	O
final	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
Table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Competitor	Method
Methods	Method
Neural	Task
bag	Task
-	Task
of	Task
-	Task
words	Task
(	O
NBOW	Method
)	O
:	O
Each	O
sequence	O
as	O
the	O
sum	O
of	O
the	O
embeddings	O
of	O
the	O
words	O
it	O
contains	O
,	O
then	O
they	O
are	O
concatenated	O
and	O
fed	O
to	O
a	O
MLP	Method
.	O

Single	O
LSTM	Method
:	O
A	O
single	O
LSTM	Method
to	O
encode	O
the	O
two	O
sequences	O
,	O
which	O
is	O
used	O
in	O
.	O

Parallel	O
LSTMs	Method
:	O
Two	O
sequences	O
are	O
encoded	O
by	O
two	O
LSTMs	Method
separately	O
,	O
then	O
they	O
are	O
concatenated	O
and	O
fed	O
to	O
a	O
MLP	Method
.	O

Attention	O
LSTMs	Method
:	O
An	O
attentive	O
LSTM	Method
to	O
encode	O
two	O
sentences	O
into	O
a	O
semantic	O
space	O
,	O
which	O
used	O
in	O
.	O

Word	O
-	O
by	O
-	O
word	O
Attention	O
LSTMs	Method
:	O
An	O
improvement	O
of	O
attention	O
LSTM	Method
by	O
introducing	O
word	Method
-	Method
by	Method
-	Method
word	Method
attention	Method
mechanism	Method
,	O
which	O
used	O
in	O
.	O

subsection	O
:	O
Experiment	O
-	O
I	O
:	O
Recognizing	Task
Textual	Task
Entailment	Task
Recognizing	Task
textual	Task
entailment	Task
(	O
RTE	Task
)	O
is	O
a	O
task	O
to	O
determine	O
the	O
semantic	O
relationship	O
between	O
two	O
sentences	O
.	O

We	O
use	O
the	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
Corpus	Material
(	O
SNLI	Material
)	O
.	O

This	O
corpus	O
contains	O
570	O
K	O
sentence	O
pairs	O
,	O
and	O
all	O
of	O
the	O
sentences	O
and	O
labels	O
stem	O
from	O
human	O
annotators	O
.	O

SNLI	Material
is	O
two	O
orders	O
of	O
magnitude	O
larger	O
than	O
all	O
other	O
existing	O
RTE	Task
corpora	O
.	O

Therefore	O
,	O
the	O
massive	O
scale	O
of	O
SNLI	Material
allows	O
us	O
to	O
train	O
powerful	O
neural	Method
networks	Method
such	O
as	O
our	O
proposed	O
architecture	O
in	O
this	O
paper	O
.	O

subsubsection	O
:	O
Results	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
evaluation	O
results	O
on	O
SNLI	Material
.	O

The	O
rd	O
column	O
of	O
the	O
table	O
gives	O
the	O
number	O
of	O
parameters	O
of	O
different	O
models	O
without	O
the	O
word	O
embeddings	O
.	O

Our	O
proposed	O
two	O
C	O
-	O
LSTMs	Method
models	O
with	O
four	O
stacked	O
blocks	O
outperform	O
all	O
the	O
competitor	O
models	O
,	O
which	O
indicates	O
that	O
our	O
thinner	O
and	O
deeper	Method
network	Method
does	O
work	O
effectively	O
.	O

Besides	O
,	O
we	O
can	O
see	O
both	O
LC	Method
-	Method
LSTMs	Method
and	O
TC	Method
-	Method
LSTMs	Method
benefit	O
from	O
multi	O
-	O
directional	O
layer	O
,	O
while	O
the	O
latter	O
obtains	O
more	O
gains	O
than	O
the	O
former	O
.	O

We	O
attribute	O
this	O
discrepancy	O
between	O
two	O
models	O
to	O
their	O
different	O
mechanisms	O
of	O
controlling	O
the	O
information	O
flow	O
from	O
depth	O
dimension	O
.	O

Compared	O
with	O
attention	O
LSTMs	Method
,	O
our	O
two	O
models	O
achieve	O
comparable	O
results	O
to	O
them	O
using	O
much	O
fewer	O
parameters	O
(	O
nearly	O
)	O
.	O

By	O
stacking	Method
C	Method
-	Method
LSTMs	Method
,	O
the	O
performance	O
of	O
them	O
are	O
improved	O
significantly	O
,	O
and	O
the	O
four	O
stacked	O
TC	O
-	O
LSTMs	Method
achieve	O
accuracy	Metric
on	O
this	O
dataset	O
.	O

Moreover	O
,	O
we	O
can	O
see	O
TC	Method
-	Method
LSTMs	Method
achieve	O
better	O
performance	O
than	O
LC	Method
-	Method
LSTMs	Method
on	O
this	O
task	O
,	O
which	O
need	O
fine	O
-	O
grained	O
reasoning	O
over	O
pairs	O
of	O
words	O
as	O
well	O
as	O
phrases	O
.	O

[	O
3rd	O
neuron	O
]	O
[	O
17th	O
neuron	O
]	O
subsubsection	O
:	O
Understanding	Task
Behaviors	Task
of	Task
Neurons	Task
in	O
C	Method
-	Method
LSTMs	Method
To	O
get	O
an	O
intuitive	O
understanding	O
of	O
how	O
the	O
C	Method
-	Method
LSTMs	Method
work	O
on	O
this	O
problem	O
,	O
we	O
examined	O
the	O
neuron	O
activations	O
in	O
the	O
last	O
aggregation	Method
layer	Method
while	O
evaluating	O
the	O
test	O
set	O
using	O
TC	Method
-	Method
LSTMs	Method
.	O

We	O
find	O
that	O
some	O
cells	O
are	O
bound	O
to	O
certain	O
roles	O
.	O

Let	O
denotes	O
the	O
activation	O
of	O
the	O
-	O
th	O
neuron	O
at	O
the	O
position	O
of	O
,	O
where	O
and	O
.	O

By	O
visualizing	O
the	O
hidden	O
state	O
and	O
analyzing	O
the	O
maximum	O
activation	O
,	O
we	O
can	O
find	O
that	O
there	O
exist	O
multiple	O
interpretable	O
neurons	O
.	O

For	O
example	O
,	O
when	O
some	O
contextualized	O
local	O
perspectives	O
are	O
semantically	O
related	O
at	O
point	O
of	O
the	O
sentence	O
pair	O
,	O
the	O
activation	O
value	O
of	O
hidden	O
neuron	O
tend	O
to	O
be	O
maximum	O
,	O
meaning	O
that	O
the	O
model	O
could	O
capture	O
some	O
reasoning	O
patterns	O
.	O

Figure	O
[	O
reference	O
]	O
illustrates	O
this	O
phenomenon	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
,	O
a	O
neuron	O
shows	O
its	O
ability	O
to	O
monitor	O
the	O
local	O
contextual	O
interactions	O
about	O
color	O
.	O

The	O
activation	O
in	O
the	O
patch	O
,	O
including	O
the	O
word	O
pair	O
“	O
(	O
red	O
,	O
green	O
)	O
”	O
,	O
is	O
much	O
higher	O
than	O
others	O
.	O

This	O
is	O
informative	O
pattern	O
for	O
the	O
relation	Task
prediction	Task
of	O
these	O
two	O
sentences	O
,	O
whose	O
ground	O
truth	O
is	O
contradiction	O
.	O

An	O
interesting	O
thing	O
is	O
there	O
are	O
two	O
words	O
describing	O
color	O
in	O
the	O
sentence	O
“	O
A	O
person	O
in	O
a	O
red	O
shirt	O
and	O
black	O
pants	O
hunched	O
over	O
.	O

”	O
.	O

Our	O
model	O
ignores	O
the	O
useless	O
word	O
“	O
black	O
”	O
,	O
which	O
indicates	O
that	O
this	O
neuron	O
selectively	O
captures	O
pattern	O
by	O
contextual	O
understanding	O
,	O
not	O
just	O
word	O
level	O
interaction	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
,	O
another	O
neuron	O
shows	O
that	O
it	O
can	O
capture	O
the	O
local	O
contextual	O
interactions	O
,	O
such	O
as	O
“	O
(	O
walking	O
down	O
the	O
street	O
,	O
outside	O
)	O
”	O
.	O

These	O
patterns	O
can	O
be	O
easily	O
captured	O
by	O
pooling	Method
layer	Method
and	O
provide	O
a	O
strong	O
support	O
for	O
the	O
final	O
prediction	Task
.	O

Table	O
[	O
reference	O
]	O
illustrates	O
multiple	O
interpretable	O
neurons	O
and	O
some	O
representative	O
word	O
or	O
phrase	O
pairs	O
which	O
can	O
activate	O
these	O
neurons	O
.	O

These	O
cases	O
show	O
that	O
our	O
models	O
can	O
capture	O
contextual	O
interactions	O
beyond	O
word	O
level	O
.	O

subsubsection	O
:	O
Error	Method
Analysis	Method
Although	O
our	O
models	O
C	O
-	O
LSTMs	Method
are	O
more	O
sensitive	O
to	O
the	O
discrepancy	O
of	O
the	O
semantic	O
capacity	O
between	O
two	O
sentences	O
,	O
some	O
semantic	O
mistakes	O
at	O
the	O
phrasal	O
level	O
still	O
exist	O
.	O

For	O
example	O
,	O
our	O
models	O
failed	O
to	O
capture	O
the	O
key	O
informative	O
pattern	O
when	O
predicting	O
the	O
entailment	O
sentence	O
pair	O
“	O
A	O
girl	O
takes	O
off	O
her	O
shoes	O
and	O
eats	O
blue	O
cotton	O
candy	O
/	O
The	O
girl	O
is	O
eating	O
while	O
barefoot	O
.	O

”	O
Besides	O
,	O
despite	O
the	O
large	O
size	O
of	O
the	O
training	O
corpus	O
,	O
it	O
’s	O
still	O
very	O
different	O
to	O
solve	O
some	O
cases	O
,	O
which	O
depend	O
on	O
the	O
combination	O
of	O
the	O
world	O
knowledge	O
and	O
context	O
-	O
sensitive	O
inferences	O
.	O

For	O
example	O
,	O
given	O
an	O
entailment	O
pair	O
“	O
a	O
man	O
grabs	O
his	O
crotch	O
during	O
a	O
political	O
demonstration	O
/	O
The	O
man	O
is	O
making	O
a	O
crude	O
gesture	O
”	O
,	O
all	O
models	O
predict	O
“	O
neutral	O
”	O
.	O

This	O
analysis	O
suggests	O
that	O
some	O
architectural	O
improvements	O
or	O
external	O
world	O
knowledge	O
are	O
necessary	O
to	O
eliminate	O
all	O
errors	O
instead	O
of	O
simply	O
scaling	O
up	O
the	O
basic	Method
model	Method
.	O

subsection	O
:	O
Experiment	O
-	O
II	O
:	O
Matching	Task
Question	Task
and	O
Answer	O
Matching	Task
question	Task
answering	Task
(	O
MQA	Task
)	O
is	O
a	O
typical	O
task	O
for	O
semantic	Task
matching	Task
.	O

Given	O
a	O
question	O
,	O
we	O
need	O
select	O
a	O
correct	O
answer	O
from	O
some	O
candidate	O
answers	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
the	O
dataset	O
collected	O
from	O
Yahoo	O
!	O

Answers	O
with	O
the	O
getByCategory	O
function	O
provided	O
in	O
Yahoo	O
!	O

Answers	Task
API	Task
,	O
which	O
produces	O
questions	O
and	O
corresponding	O
best	O
answers	O
.	O

We	O
then	O
select	O
the	O
pairs	O
in	O
which	O
the	O
length	O
of	O
questions	O
and	O
answers	O
are	O
both	O
in	O
the	O
interval	O
,	O
thus	O
obtaining	O
question	O
answer	O
pairs	O
to	O
form	O
the	O
positive	O
pairs	O
.	O

For	O
negative	O
pairs	O
,	O
we	O
first	O
use	O
each	O
question	O
’s	O
best	O
answer	O
as	O
a	O
query	O
to	O
retrieval	O
top	O
results	O
from	O
the	O
whole	O
answer	O
set	O
with	O
Lucene	O
,	O
where	O
or	O
answers	O
will	O
be	O
selected	O
randomly	O
to	O
construct	O
the	O
negative	O
pairs	O
.	O

The	O
whole	O
dataset	O
is	O
divided	O
into	O
training	O
,	O
validation	O
and	O
testing	O
data	O
with	O
proportion	O
.	O

Moreover	O
,	O
we	O
give	O
two	O
test	O
settings	O
:	O
selecting	O
the	O
best	O
answer	O
from	O
5	O
and	O
10	O
candidates	O
respectively	O
.	O

subsubsection	O
:	O
Results	O
Results	O
of	O
MQA	Method
are	O
shown	O
in	O
the	O
Table	O
[	O
reference	O
]	O
.	O

For	O
our	O
models	O
,	O
due	O
to	O
stacking	O
block	O
more	O
than	O
three	O
layers	O
can	O
not	O
make	O
significant	O
improvements	O
on	O
this	O
task	O
,	O
we	O
just	O
use	O
three	O
stacked	Method
C	Method
-	Method
LSTMs	Method
.	O

By	O
analyzing	O
the	O
evaluation	O
results	O
of	O
question	Task
-	Task
answer	Task
matching	Task
in	O
table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
strong	O
interaction	Method
models	Method
(	O
attention	O
LSTMs	Method
,	O
our	O
C	Method
-	Method
LSTMs	Method
)	O
consistently	O
outperform	O
the	O
weak	Method
interaction	Method
models	Method
(	O
NBOW	Method
,	O
parallel	O
LSTMs	Method
)	O
with	O
a	O
large	O
margin	O
,	O
which	O
suggests	O
the	O
importance	O
of	O
modelling	O
strong	O
interaction	O
of	O
two	O
sentences	O
.	O

Our	O
proposed	O
two	O
C	Method
-	Method
LSTMs	Method
surpass	O
the	O
competitor	Method
methods	Method
and	O
C	Method
-	Method
LSTMs	Method
augmented	O
with	O
multi	O
-	O
directions	O
layers	O
and	O
multiple	O
stacked	O
blocks	O
fully	O
utilize	O
multiple	O
levels	O
of	O
abstraction	O
to	O
directly	O
boost	O
the	O
performance	O
.	O

Additionally	O
,	O
LC	Method
-	Method
LSTMs	Method
is	O
superior	O
to	O
TC	Method
-	Method
LSTMs	Method
.	O

The	O
reason	O
may	O
be	O
that	O
MQA	Method
is	O
a	O
relative	Task
simple	Task
task	Task
,	O
which	O
requires	O
less	O
reasoning	O
abilities	O
,	O
compared	O
with	O
RTE	Task
task	O
.	O

Moreover	O
,	O
the	O
parameters	O
of	O
LC	Method
-	Method
LSTMs	Method
are	O
less	O
than	O
TC	Method
-	Method
LSTMs	Method
,	O
which	O
ensures	O
the	O
former	O
can	O
avoid	O
suffering	O
from	O
overfitting	O
on	O
a	O
relatively	O
smaller	O
corpus	O
.	O

section	O
:	O
Related	O
Work	O
Our	O
architecture	O
for	O
sentence	Task
pair	Task
encoding	Task
can	O
be	O
regarded	O
as	O
strong	O
interaction	Method
models	Method
,	O
which	O
have	O
been	O
explored	O
in	O
previous	O
models	O
.	O

An	O
intuitive	O
paradigm	O
is	O
to	O
compute	O
similarities	O
between	O
all	O
the	O
words	O
or	O
phrases	O
of	O
the	O
two	O
sentences	O
.	O

socher2011dynamic	O
[	O
socher2011dynamic	O
]	O
firstly	O
used	O
this	O
paradigm	O
for	O
paraphrase	Task
detection	Task
.	O

The	O
representations	O
of	O
words	O
or	O
phrases	O
are	O
learned	O
based	O
on	O
recursive	Method
autoencoders	Method
.	O

wan2015deep	O
[	O
wan2015deep	O
]	O
used	O
LSTM	Method
to	O
enhance	O
the	O
positional	O
contextual	O
interactions	O
of	O
the	O
words	O
or	O
phrases	O
between	O
two	O
sentences	O
.	O

The	O
input	O
of	O
LSTM	Method
for	O
one	O
sentence	O
does	O
not	O
involve	O
another	O
sentence	O
.	O

A	O
major	O
limitation	O
of	O
this	O
paradigm	O
is	O
the	O
interaction	O
of	O
two	O
sentence	O
is	O
captured	O
by	O
a	O
pre	O
-	O
defined	O
similarity	Metric
measure	Metric
.	O

Thus	O
,	O
it	O
is	O
not	O
easy	O
to	O
increase	O
the	O
depth	O
of	O
the	O
network	O
.	O

Compared	O
with	O
this	O
paradigm	O
,	O
we	O
can	O
stack	O
our	O
C	Method
-	Method
LSTMs	Method
to	O
model	O
multiple	O
-	O
granularity	O
interactions	Task
of	Task
two	Task
sentences	Task
.	O

rocktaschel2015reasoning	O
[	O
rocktaschel2015reasoning	O
]	O
used	O
two	O
LSTMs	Method
equipped	O
with	O
attention	Method
mechanism	Method
to	O
capture	O
the	O
iteration	O
between	O
two	O
sentences	O
.	O

This	O
architecture	O
is	O
asymmetrical	O
for	O
two	O
sentences	O
,	O
where	O
the	O
obtained	O
final	Method
representation	Method
is	O
sensitive	O
to	O
the	O
two	O
sentences	O
’	O
order	O
.	O

Compared	O
with	O
the	O
attentive	O
LSTM	Method
,	O
our	O
proposed	O
C	Method
-	Method
LSTMs	Method
are	O
symmetrical	O
and	O
model	O
the	O
local	O
contextual	O
interaction	O
of	O
two	O
sequences	O
directly	O
.	O

section	O
:	O
Conclusion	O
and	O
Future	O
Work	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
deep	Method
architecture	Method
to	O
capture	O
the	O
strong	O
interaction	O
information	O
of	O
sentence	O
pair	O
.	O

Experiments	O
on	O
two	O
large	Task
scale	Task
text	Task
matching	Task
tasks	Task
demonstrate	O
the	O
efficacy	O
of	O
our	O
proposed	O
model	O
and	O
its	O
superiority	O
to	O
competitor	O
models	O
.	O

Besides	O
,	O
our	O
visualization	Task
analysis	Task
revealed	O
that	O
multiple	O
interpretable	O
neurons	O
in	O
our	O
proposed	O
models	O
can	O
capture	O
the	O
contextual	O
interactions	O
of	O
the	O
words	O
or	O
phrases	O
.	O

In	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
incorporate	O
some	O
gating	Method
strategies	Method
into	O
the	O
depth	O
dimension	O
of	O
our	O
proposed	O
models	O
,	O
like	O
highway	Method
or	Method
residual	Method
network	Method
,	O
to	O
enhance	O
the	O
interactions	O
between	O
depth	O
and	O
other	O
dimensions	O
thus	O
training	O
more	O
deep	Method
and	Method
powerful	Method
neural	Method
networks	Method
.	O

.	O

/	O
nlp	O
,	O
..	O
/	O
ours	O
.	O

