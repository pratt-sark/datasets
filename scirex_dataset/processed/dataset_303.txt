document	O
:	O
Quaternion	Method
Convolutional	Method
Neural	Method
Networks	Method
for	O
End	Task
-	Task
to	Task
-	Task
End	Task
Automatic	Task
Speech	Task
Recognition	Task
Recently	O
,	O
the	O
connectionist	Method
temporal	Method
classification	Method
(	O
CTC	Method
)	O
model	O
coupled	O
with	O
recurrent	Method
(	O
RNN	Method
)	O
or	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
,	O
made	O
it	O
easier	O
to	O
train	O
speech	Task
recognition	Task
systems	Task
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
.	O

However	O
in	O
real	Method
-	Method
valued	Method
models	Method
,	O
time	Method
frame	Method
components	Method
such	O
as	O
mel	O
-	O
filter	O
-	O
bank	O
energies	O
and	O
the	O
cepstral	O
coefficients	O
obtained	O
from	O
them	O
,	O
together	O
with	O
their	O
first	O
and	O
second	O
order	O
derivatives	O
,	O
are	O
processed	O
as	O
individual	O
elements	O
,	O
while	O
a	O
natural	O
alternative	O
is	O
to	O
process	O
such	O
components	O
as	O
composed	O
entities	O
.	O

We	O
propose	O
to	O
group	O
such	O
elements	O
in	O
the	O
form	O
of	O
quaternions	O
and	O
to	O
process	O
these	O
quaternions	O
using	O
the	O
established	O
quaternion	Method
algebra	Method
.	O

Quaternion	O
numbers	O
and	O
quaternion	Method
neural	Method
networks	Method
have	O
shown	O
their	O
efficiency	O
to	O
process	O
multidimensional	O
inputs	O
as	O
entities	O
,	O
to	O
encode	O
internal	O
dependencies	O
,	O
and	O
to	O
solve	O
many	O
tasks	O
with	O
less	O
learning	O
parameters	O
than	O
real	Method
-	Method
valued	Method
models	Method
.	O

This	O
paper	O
proposes	O
to	O
integrate	O
multiple	O
feature	O
views	O
in	O
quaternion	Method
-	Method
valued	Method
convolutional	Method
neural	Method
network	Method
(	O
QCNN	Method
)	O
,	O
to	O
be	O
used	O
for	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
mapping	Task
with	O
the	O
CTC	Method
model	O
.	O

Promising	O
results	O
are	O
reported	O
using	O
simple	O
QCNNs	Method
in	O
phoneme	Task
recognition	Task
experiments	O
with	O
the	O
TIMIT	Material
corpus	Material
.	O

More	O
precisely	O
,	O
QCNNs	Method
obtain	O
a	O
lower	O
phoneme	Metric
error	Metric
rate	Metric
(	O
PER	Metric
)	O
with	O
less	O
learning	O
parameters	O
than	O
a	O
competing	Method
model	Method
based	O
on	O
real	Method
-	Method
valued	Method
CNNs	Method
.	O

TitouanParcollet	O
,	O
YingZhang	O
,	O
MohamedMorchid	O
,	O
ChihebTrabelsi	O
,	O
GeorgesLinarès	O
,	O
RenatoDeMori	O
andYoshuaBengio	O
Universitéd’Avignon	O
,	O
LIA	O
,	O
France	O
UniversitédeMontréal	O
,	O
MILA	O
,	O
Canada	O
McGillUniversity	O
,	O
Montréal	O
,	O
Canada	O
Orkis	O
,	O
Aixenprovence	O
,	O
France	O
ElementAI	O
,	O
Montréal	O
,	O
Canada	O
titouan.parcollet@alumni.univ	O
-	O
avignon.fr	O
,	O
ying.zhlisa@gmail.com	O
,	O
mohamed.morchid@univ	O
-	O
avignon.fr	O
,	O
chiheb.trabelsi@polymtl.ca	O
,	O
georges.linares@univ	O
-	O
avignon.fr	O
,	O
rdemori@cs.mcgill.ca	O
Index	O
Terms	O
:	O
quaternion	Method
convolutional	Method
neural	Method
networks	Method
,	O
automatic	Task
speech	Task
recognition	Task
,	O
deep	Task
learning	Task
section	O
:	O
Introduction	O
Recurrent	Method
(	O
RNN	Method
)	O
and	O
convolutional	Method
(	O
CNN	Method
)	O
neural	O
networks	O
have	O
improved	O
the	O
performance	O
over	O
hidden	Method
Markov	Method
models	Method
(	O
HMM	Method
)	O
combined	O
with	O
gaussian	Method
mixtures	Method
models	Method
(	O
GMMs	Method
)	O
in	O
automatic	Task
speech	Task
recognition	Task
(	O
ASR	Task
)	O
systems	O
during	O
the	O
last	O
decade	O
.	O

More	O
recently	O
,	O
end	O
-	O
to	O
-	O
end	Method
approaches	Method
received	O
a	O
growing	O
interest	O
due	O
to	O
the	O
promising	O
results	O
obtained	O
with	O
connectionist	Method
temporal	Method
classification	Method
(	O
CTC	Method
)	O
combined	O
with	O
RNNs	Method
or	O
CNNs	Method
.	O

However	O
,	O
despite	O
such	O
evolution	O
of	O
models	O
and	O
paradigms	O
,	O
the	O
acoustic	O
features	O
remain	O
almost	O
the	O
same	O
.	O

The	O
main	O
motivation	O
is	O
that	O
filters	O
spaced	O
linearly	O
at	O
low	O
frequencies	O
and	O
logarithmically	O
at	O
high	O
frequencies	O
make	O
it	O
possible	O
to	O
capture	O
phonetically	O
important	O
acoustic	O
correlates	O
.	O

Early	O
evidence	O
was	O
provided	O
in	O
showing	O
that	O
mel	Method
frequency	Method
scaled	Method
cepstral	Method
coefficients	Method
(	O
MFCCs	Method
)	O
are	O
effective	O
in	O
capturing	O
the	O
acoustic	O
information	O
required	O
to	O
recognize	O
syllables	O
in	O
continuous	O
speech	O
.	O

Motivated	O
by	O
these	O
analysis	O
,	O
a	O
small	O
number	O
of	O
MFCCs	Method
(	O
usually	O
)	O
with	O
their	O
first	O
and	O
second	O
time	O
-	O
derivatives	O
,	O
as	O
proposed	O
in	O
,	O
have	O
been	O
found	O
suited	O
for	O
statistical	O
and	O
neural	O
ASR	Task
systems	O
.	O

In	O
most	O
systems	O
,	O
a	O
time	O
frame	O
of	O
the	O
speech	O
signal	O
is	O
represented	O
by	O
a	O
vector	O
with	O
real	O
-	O
valued	O
elements	O
that	O
express	O
sequences	O
of	O
MFCCs	O
,	O
or	O
filter	O
energies	O
,	O
and	O
their	O
temporal	O
context	O
features	O
.	O

A	O
concern	O
addressed	O
in	O
this	O
paper	O
,	O
is	O
the	O
fact	O
that	O
the	O
relations	O
between	O
different	O
views	O
of	O
the	O
features	O
associated	O
with	O
a	O
frequency	O
are	O
not	O
explicitly	O
represented	O
in	O
the	O
feature	O
vectors	O
used	O
so	O
far	O
.	O

Therefore	O
,	O
this	O
paper	O
proposes	O
to	O
:	O
Introduce	O
a	O
new	O
quaternion	Method
representation	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
to	O
encode	O
multiple	O
views	O
of	O
a	O
time	O
-	O
frame	O
frequency	O
in	O
which	O
different	O
views	O
are	O
encoded	O
as	O
values	O
of	O
imaginary	O
parts	O
of	O
a	O
hyper	O
-	O
complex	O
number	O
.	O

Thus	O
,	O
vectors	O
of	O
quaternions	O
are	O
embedded	O
using	O
operations	O
defined	O
by	O
a	O
specific	O
quaternion	Method
algebra	Method
to	O
preserve	O
a	O
distinction	O
between	O
features	O
of	O
each	O
frequency	Method
representation	Method
.	O

Merge	O
a	O
quaternion	Method
convolutional	Method
neural	Method
network	Method
(	O
QCNN	Method
,	O
Section	O
[	O
reference	O
]	O
)	O
with	O
the	O
CTC	Method
in	O
a	O
unified	O
and	O
easily	O
reusable	O
framework	O
.	O

Compare	O
and	O
evaluate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
QCNN	Method
to	O
an	O
equivalent	O
real	Method
-	Method
valued	Method
model	Method
on	O
the	O
TIMIT	Task
phonemes	Task
recognition	Task
task	Task
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

There	O
are	O
advantages	O
which	O
could	O
derive	O
from	O
bundling	O
groups	O
of	O
numbers	O
into	O
a	O
quaternion	O
.	O

Like	O
capsule	Method
networks	Method
,	O
quaternion	Method
networks	Method
create	O
a	O
tighter	O
association	O
between	O
small	O
groups	O
of	O
numbers	O
rather	O
than	O
having	O
one	O
homogeneous	Method
representation	Method
.	O

In	O
addition	O
,	O
this	O
kind	O
of	O
structure	O
reduces	O
the	O
number	O
of	O
required	O
parameters	O
considerably	O
,	O
because	O
only	O
one	O
weight	O
is	O
necessary	O
between	O
two	O
quaternion	O
units	O
,	O
instead	O
of	O
4	O
.	O

The	O
hypothesis	O
tested	O
here	O
is	O
whether	O
these	O
advantages	O
lead	O
to	O
better	O
generalization	Task
.	O

The	O
conducted	O
experiments	O
on	O
the	O
TIMIT	Material
dataset	Material
yielded	O
a	O
phoneme	Metric
error	Metric
rate	Metric
(	O
PER	Metric
)	O
of	O
%	O
for	O
QCNNs	Method
which	O
is	O
significantly	O
lower	O
than	O
the	O
PER	Metric
obtained	O
with	O
real	Method
-	Method
valued	Method
CNNs	Method
(	O
%	O
)	O
,	O
with	O
the	O
same	O
input	O
features	O
.	O

Moreover	O
,	O
from	O
a	O
practical	O
point	O
of	O
view	O
,	O
the	O
resulting	O
networks	O
have	O
a	O
considerably	O
smaller	O
memory	Metric
footprint	Metric
due	O
to	O
a	O
smaller	O
set	O
of	O
parameters	O
.	O

section	O
:	O
Quaternion	Method
algebra	Method
The	O
quaternions	Method
algebra	Method
defines	O
operations	O
between	O
quaternion	O
numbers	O
.	O

A	O
quaternion	O
Q	O
is	O
an	O
extension	O
of	O
a	O
complex	O
number	O
defined	O
in	O
a	O
four	O
dimensional	O
space	O
.	O

,	O
with	O
,	O
r	O
,	O
x	O
,	O
y	O
,	O
and	O
z	O
four	O
real	O
numbers	O
,	O
and	O
,	O
i	O
,	O
j	O
,	O
and	O
k	O
are	O
the	O
quaternion	O
unit	O
basis	O
.	O

Such	O
a	O
definition	O
can	O
be	O
used	O
for	O
describing	O
spatial	Task
rotations	Task
that	O
can	O
also	O
be	O
represented	O
by	O
the	O
following	O
matrix	O
of	O
real	O
numbers	O
:	O
In	O
a	O
quaternion	O
,	O
is	O
the	O
real	O
part	O
while	O
is	O
the	O
imaginary	O
part	O
(	O
)	O
or	O
the	O
vector	O
part	O
.	O

Basic	O
quaternion	O
definitions	O
are	O
all	O
products	O
of	O
,	O
k	O
are	O
:	O
,	O
conjugate	O
of	O
is	O
:	O
,	O
unit	O
quaternion	O
,	O
the	O
Hamilton	O
product	O
between	O
and	O
is	O
defined	O
as	O
follows	O
:	O
The	O
Hamilton	O
product	O
is	O
used	O
in	O
QCNNs	Method
to	O
perform	O
transformations	O
of	O
vectors	O
representing	O
quaternions	O
,	O
as	O
well	O
as	O
scaling	O
and	O
interpolation	O
between	O
two	O
rotations	O
following	O
a	O
geodesic	O
over	O
a	O
sphere	O
in	O
the	O
space	O
as	O
shown	O
in	O
.	O

section	O
:	O
Quaternion	Method
convolutional	Method
neural	Method
networks	Method
This	O
section	O
defines	O
the	O
internal	Method
quaternion	Method
representation	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
the	O
quaternion	Method
convolution	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
a	O
proper	O
parameter	Method
initialization	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
connectionist	Method
temporal	Method
classification	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Quaternion	Method
internal	Method
representation	Method
The	O
QCNN	Method
is	O
a	O
quaternion	Method
extension	Method
of	O
well	O
-	O
known	O
real	O
-	O
valued	O
and	O
complex	O
-	O
valued	O
deep	O
convolutional	Method
networks	O
(	O
CNN	Method
)	O
.	O

The	O
quaternion	O
algebra	O
is	O
ensured	O
by	O
manipulating	O
matrices	O
of	O
real	O
numbers	O
.	O

Consequently	O
,	O
a	O
traditional	O
convolutional	Method
layer	O
,	O
with	O
a	O
kernel	Method
that	O
contains	O
feature	O
maps	O
,	O
is	O
split	O
into	O
4	O
parts	O
:	O
the	O
first	O
part	O
equal	O
to	O
,	O
the	O
second	O
one	O
to	O
,	O
the	O
third	O
one	O
to	O
and	O
the	O
last	O
one	O
to	O
of	O
a	O
quaternion	O
.	O

Nonetheless	O
,	O
an	O
important	O
condition	O
to	O
perform	O
backpropagation	Method
in	O
either	O
real	Method
,	Method
complex	Method
or	Method
quaternion	Method
neural	Method
networks	Method
is	O
to	O
have	O
cost	O
and	O
activation	O
functions	O
that	O
are	O
differentiable	O
with	O
respect	O
to	O
each	O
part	O
of	O
the	O
real	O
,	O
complex	O
or	O
quaternion	O
number	O
.	O

Many	O
activation	Method
functions	Method
for	O
quaternion	O
have	O
been	O
investigated	O
and	O
a	O
quaternion	Method
backpropagation	Method
algorithm	Method
have	O
been	O
proposed	O
in	O
.	O

Consequently	O
,	O
the	O
split	O
activation	O
function	O
is	O
applied	O
to	O
every	O
layer	O
and	O
is	O
defined	O
as	O
follows	O
:	O
with	O
corresponding	O
to	O
any	O
standard	O
activation	O
function	O
.	O

subsection	O
:	O
Quaternion	Method
-	Method
valued	Method
convolution	Method
Following	O
a	O
recent	O
proposition	O
for	O
convolution	Task
of	Task
complex	Task
numbers	Task
and	O
quaternions	O
,	O
this	O
paper	O
presents	O
basic	O
neural	Method
networks	Method
convolution	Method
operations	Method
using	O
quaternion	Method
algebra	Method
.	O

The	O
convolution	Method
process	Method
is	O
defined	O
in	O
the	O
real	O
-	O
valued	O
space	O
by	O
convolving	O
a	O
filter	Method
matrix	Method
with	O
a	O
vector	O
.	O

In	O
a	O
QCNN	Method
,	O
the	O
convolution	Method
of	Method
a	Method
quaternion	Method
filter	Method
matrix	Method
with	O
a	O
quaternion	O
vector	O
is	O
performed	O
.	O

For	O
this	O
computation	O
,	O
the	O
Hamilton	O
product	O
is	O
computed	O
using	O
the	O
real	Method
-	Method
valued	Method
matrices	Method
representation	Method
of	Method
quaternions	Method
.	O

Let	O
be	O
a	O
quaternion	Method
weight	Method
filter	Method
matrix	Method
,	O
and	O
the	O
quaternion	O
input	O
vector	O
.	O

The	O
quaternion	O
convolution	O
w.r.t	O
the	O
Hamilton	Method
product	Method
is	O
defined	O
as	O
follows	O
:	O
and	O
can	O
thus	O
be	O
expressed	O
in	O
a	O
matrix	Method
form	Method
:	O
An	O
illustration	O
of	O
such	O
operation	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Weight	Method
initialization	Method
Weight	Method
initialization	Method
is	O
crucial	O
to	O
efficiently	O
train	O
neural	Method
networks	Method
.	O

An	O
appropriate	O
initialization	O
improves	O
training	Metric
speed	Metric
and	O
reduces	O
the	O
risk	O
of	O
exploding	O
or	O
vanishing	O
gradient	O
.	O

A	O
quaternion	Method
initialization	Method
is	O
composed	O
of	O
two	O
steps	O
.	O

First	O
,	O
for	O
each	O
weight	O
to	O
be	O
initialized	O
,	O
a	O
purely	O
imaginary	O
quaternion	O
is	O
generated	O
following	O
an	O
uniform	O
distribution	O
in	O
the	O
interval	O
.	O

The	O
imaginary	O
unit	O
is	O
then	O
normalized	O
to	O
obtain	O
following	O
the	O
quaternion	Method
normalization	Method
equation	Method
.	O

The	O
later	O
is	O
used	O
alongside	O
to	O
other	O
well	O
known	O
initializing	Metric
criterion	Metric
such	O
as	O
or	O
to	O
complete	O
the	O
initialization	Method
process	Method
of	O
a	O
given	O
quaternion	O
weight	O
named	O
.	O

Moreover	O
,	O
the	O
generated	O
weight	O
has	O
a	O
polar	O
form	O
defined	O
by	O
:	O
with	O
Therefore	O
,	O
is	O
generated	O
as	O
follows	O
:	O
,	O
,	O
,	O
.	O

However	O
,	O
represents	O
a	O
randomly	O
generated	O
variable	O
with	O
respect	O
to	O
the	O
variance	O
of	O
the	O
quaternion	O
weight	O
and	O
the	O
selected	O
initialization	O
criterion	O
.	O

The	O
initialization	Method
process	Method
follows	O
and	O
to	O
derive	O
the	O
variance	O
of	O
the	O
quaternion	O
-	O
valued	O
weight	O
parameters	O
.	O

Therefore	O
,	O
the	O
variance	O
of	O
W	O
has	O
to	O
be	O
investigated	O
:	O
is	O
equals	O
to	O
since	O
the	O
weight	O
distribution	O
is	O
symmetric	O
around	O
.	O

Nonetheless	O
,	O
the	O
value	O
of	O
is	O
not	O
trivial	O
in	O
the	O
case	O
of	O
quaternion	O
-	O
valued	O
matrices	O
.	O

Indeed	O
,	O
follows	O
a	O
Chi	O
-	O
distributed	O
with	O
four	O
degrees	O
of	O
freedom	O
(	O
DOFs	O
)	O
and	O
is	O
expressed	O
and	O
computed	O
as	O
follows	O
:	O
Therefore	O
,	O
in	O
order	O
to	O
respect	O
the	O
He	O
Criterion	O
,	O
the	O
variance	O
would	O
be	O
equal	O
to	O
:	O
subsection	O
:	O
Connectionist	Method
Temporal	Method
Classification	Method
In	O
the	O
acoustic	Task
modeling	Task
part	Task
of	O
ASR	Task
systems	O
,	O
the	O
task	O
of	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
mapping	Task
from	O
an	O
input	O
acoustic	O
signal	O
to	O
a	O
sequence	O
of	O
symbols	O
is	O
complex	O
due	O
to	O
:	O
and	O
could	O
be	O
in	O
arbitrary	O
length	O
.	O

The	O
alignment	O
between	O
and	O
is	O
unknown	O
in	O
most	O
cases	O
.	O

Specially	O
,	O
is	O
usually	O
shorter	O
than	O
in	O
terms	O
of	O
phoneme	O
symbols	O
.	O

To	O
alleviate	O
these	O
problems	O
,	O
connectionist	Method
temporal	Method
classification	Method
(	O
CTC	Method
)	O
has	O
been	O
proposed	O
.	O

First	O
,	O
a	O
softmax	Method
is	O
applied	O
at	O
each	O
timestep	O
,	O
or	O
frame	O
,	O
providing	O
a	O
probability	O
of	O
emitting	O
each	O
symbol	O
at	O
that	O
timestep	O
.	O

This	O
probability	O
results	O
in	O
a	O
symbol	Method
sequences	Method
representation	Method
,	O
with	O
in	O
the	O
latent	O
space	O
.	O

A	O
blank	O
symbol	O
is	O
introduced	O
as	O
an	O
extra	O
label	O
to	O
allow	O
the	O
classifier	Method
to	O
deal	O
with	O
the	O
unknown	O
alignment	O
.	O

Then	O
,	O
is	O
transformed	O
to	O
the	O
final	O
output	O
sequence	O
with	O
a	O
many	O
-	O
to	O
-	O
one	O
function	O
defined	O
as	O
follows	O
:	O
Consequently	O
,	O
the	O
output	O
sequence	O
is	O
a	O
summation	O
over	O
the	O
probability	O
of	O
all	O
possible	O
alignments	O
between	O
and	O
after	O
applying	O
the	O
function	O
.	O

Accordingly	O
to	O
the	O
parameters	O
of	O
the	O
models	O
are	O
learned	O
based	O
on	O
the	O
cross	Metric
entropy	Metric
loss	Metric
function	Metric
:	O
During	O
the	O
inference	Task
,	O
a	O
best	Method
path	Method
decoding	Method
algorithm	Method
is	O
performed	O
.	O

Therefore	O
,	O
the	O
latent	O
sequence	O
with	O
the	O
highest	O
probability	O
is	O
obtained	O
by	O
performing	O
argmax	Method
of	O
the	O
softmax	O
output	O
at	O
each	O
timestep	O
.	O

The	O
final	O
sequence	O
is	O
obtained	O
by	O
applying	O
the	O
function	O
to	O
the	O
latent	O
sequence	O
.	O

section	O
:	O
Experiments	O
The	O
performance	O
and	O
efficiency	O
of	O
the	O
proposed	O
QCNNs	Method
is	O
evaluated	O
on	O
a	O
phoneme	Task
recognition	Task
task	Task
.	O

This	O
section	O
provides	O
details	O
on	O
the	O
dataset	O
and	O
the	O
quaternion	Method
features	Method
representation	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
the	O
models	O
configurations	O
(	O
Section	O
[	O
reference	O
]	O
)	O
,	O
and	O
finally	O
a	O
discussion	O
of	O
the	O
observed	O
results	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
TIMIT	Material
dataset	Material
and	O
acoustic	O
features	O
of	O
quaternions	O
The	O
TIMIT	Material
dataset	Material
is	O
composed	O
of	O
a	O
standard	O
462	O
-	O
speaker	O
training	O
dataset	O
,	O
a	O
50	O
-	O
speakers	O
development	O
dataset	O
and	O
a	O
core	O
test	O
dataset	O
of	O
sentences	O
.	O

During	O
the	O
experiments	O
,	O
the	O
SA	O
records	O
of	O
the	O
training	O
set	O
are	O
removed	O
and	O
the	O
development	O
set	O
is	O
used	O
for	O
early	Task
stopping	Task
.	O

The	O
raw	O
audio	O
is	O
transformed	O
into	O
-	O
dimensional	O
log	O
mel	O
-	O
filter	O
-	O
bank	O
coefficients	O
with	O
deltas	O
,	O
delta	O
-	O
deltas	O
,	O
and	O
energy	O
terms	O
,	O
resulting	O
in	O
a	O
one	O
dimensional	O
vector	O
of	O
length	O
.	O

An	O
acoustic	O
quaternion	O
associated	O
with	O
a	O
frequency	O
and	O
a	O
time	O
frame	O
is	O
defined	O
as	O
follows	O
:	O
It	O
represents	O
multiple	O
views	O
of	O
a	O
frequency	O
at	O
time	O
frame	O
,	O
consisting	O
of	O
the	O
energy	O
in	O
the	O
filter	O
band	O
corresponding	O
to	O
,	O
its	O
first	O
time	O
derivative	O
describing	O
a	O
slope	O
view	O
,	O
and	O
its	O
second	O
time	O
derivative	O
describing	O
a	O
concavity	O
view	O
.	O

Finally	O
,	O
a	O
unique	O
quaternion	O
is	O
composed	O
with	O
the	O
three	O
corresponding	O
energy	O
terms	O
.	O

Thus	O
,	O
the	O
quaternion	O
input	O
vector	O
length	O
is	O
(	O
)	O
.	O

subsection	O
:	O
Models	O
architectures	O
The	O
architectures	O
of	O
both	O
CNN	Method
and	O
QCNN	Method
models	Method
are	O
inspired	O
by	O
.	O

A	O
first	O
D	O
convolutional	Method
layer	O
is	O
followed	O
by	O
a	O
maxpooling	Method
layer	Method
along	O
the	O
frequency	O
axis	O
.	O

Then	O
,	O
D	O
convolutional	Method
layers	O
are	O
included	O
,	O
together	O
with	O
dense	O
layers	O
of	O
sizes	O
and	O
respectively	O
for	O
real	Method
-	Method
and	Method
quaternion	Method
-	Method
valued	Method
models	Method
(	O
with	O
)	O
.	O

Indeed	O
,	O
the	O
output	O
of	O
a	O
dense	Method
quaternion	Method
-	Method
valued	Method
layer	Method
has	O
nodes	O
and	O
is	O
times	O
larger	O
than	O
the	O
number	O
of	O
units	O
.	O

The	O
filter	Method
size	Method
is	O
rectangular	O
,	O
and	O
a	O
padding	Method
is	O
applied	O
to	O
keep	O
the	O
sequence	O
and	O
signal	O
sizes	O
unaltered	O
.	O

The	O
number	O
of	O
feature	O
maps	O
varies	O
from	O
to	O
for	O
the	O
real	Method
-	Method
valued	Method
models	Method
and	O
from	O
to	O
for	O
quaternion	Method
-	Method
valued	Method
models	Method
.	O

Indeed	O
,	O
the	O
number	O
of	O
output	O
feature	O
maps	O
is	O
times	O
larger	O
in	O
the	O
QCNN	Method
due	O
to	O
the	O
quaternion	Method
convolution	Method
,	O
meaning	O
quaternion	O
-	O
valued	O
feature	O
maps	O
correspond	O
to	O
real	O
-	O
valued	O
ones	O
.	O

The	O
PReLU	Method
activation	Method
function	Method
is	O
employed	O
for	O
both	O
models	O
.	O

A	O
dropout	O
of	O
and	O
a	O
regularization	O
of	O
are	O
used	O
across	O
all	O
the	O
layers	O
,	O
except	O
the	O
input	O
and	O
output	O
ones	O
.	O

CNNs	Method
and	O
QCNNs	Method
are	O
trained	O
with	O
the	O
Adam	Method
learning	Method
rate	Method
optimizer	Method
and	O
vanilla	Method
hyperparameters	Method
during	O
epochs	O
.	O

Then	O
,	O
a	O
fine	Method
-	Method
tuning	Method
process	Method
of	O
epochs	O
is	O
performed	O
with	O
a	O
standard	O
and	O
a	O
learning	Metric
rate	Metric
of	O
.	O

Finally	O
,	O
the	O
standard	O
CTC	Method
loss	O
function	O
defined	O
in	O
and	O
implemented	O
in	O
is	O
applied	O
.	O

Experiments	O
are	O
performed	O
on	O
Tesla	Method
P100	Method
and	O
Geforce	Method
Titan	Method
X	Method
GPUs	Method
.	O

subsection	O
:	O
Results	O
and	O
discussion	O
Results	O
on	O
the	O
phoneme	Task
recognition	Task
task	Task
of	O
the	O
TIMIT	Material
dataset	Material
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
is	O
worth	O
noticing	O
the	O
important	O
difference	O
in	O
terms	O
of	O
the	O
number	O
of	O
learning	O
parameters	O
between	O
real	Method
and	Method
quaternion	Method
valued	Method
CNNs	Method
.	O

It	O
is	O
easily	O
explained	O
by	O
the	O
quaternion	Method
algebra	Method
.	O

In	O
the	O
case	O
of	O
a	O
dense	Method
layer	Method
with	O
input	O
values	O
and	O
hidden	O
units	O
,	O
a	O
real	Method
-	Method
valued	Method
model	Method
will	O
have	O
M	O
parameters	O
,	O
while	O
to	O
maintain	O
equal	O
input	O
and	O
output	O
nodes	O
(	O
)	O
the	O
quaternion	O
equivalent	O
has	O
quaternions	O
inputs	O
and	O
quaternion	O
-	O
valued	O
hidden	O
units	O
.	O

Therefore	O
the	O
number	O
of	O
parameters	O
for	O
the	O
quaternion	Method
model	Method
is	O
M.	O
Such	O
a	O
complexity	Method
reduction	Method
turns	O
out	O
to	O
produce	O
better	O
results	O
and	O
may	O
have	O
other	O
advantages	O
such	O
as	O
a	O
smallest	O
memory	Metric
footprint	Metric
while	O
saving	O
NN	Method
models	Method
.	O

Moreover	O
,	O
the	O
reduction	O
of	O
the	O
number	O
of	O
parameters	O
does	O
not	O
result	O
in	O
poor	O
performance	O
in	O
the	O
QCNN	Method
.	O

Indeed	O
,	O
the	O
best	O
PER	Metric
reported	O
is	O
%	O
from	O
a	O
QCNN	Method
with	O
feature	O
maps	O
and	O
layers	Method
,	O
compared	O
to	O
a	O
PER	Metric
of	O
%	O
for	O
a	O
real	O
-	O
valued	O
CNN	Method
with	O
feature	Method
maps	Method
and	O
layers	O
.	O

It	O
is	O
worth	O
underlying	O
that	O
both	O
model	O
accuracies	Metric
are	O
increasing	O
with	O
the	O
size	O
and	O
the	O
depth	O
of	O
the	O
neural	Method
network	Method
.	O

However	O
,	O
bigger	O
real	O
-	O
valued	O
feature	O
maps	O
leads	O
to	O
overfitting	O
.	O

In	O
fact	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
best	O
PER	Metric
for	O
a	O
real	Method
-	Method
valued	Method
model	Method
is	O
reached	O
with	O
(	O
)	O
feature	O
maps	O
and	O
decreasing	O
at	O
(	O
%	O
)	O
and	O
(	O
)	O
.	O

The	O
QCNN	Method
does	O
not	O
suffer	O
from	O
such	O
weaknesses	O
due	O
to	O
the	O
smaller	O
density	O
of	O
the	O
neural	Method
network	Method
and	O
achieved	O
a	O
constant	O
PER	Metric
improvement	O
alongside	O
with	O
the	O
increasing	O
number	O
of	O
feature	O
maps	O
.	O

Furthermore	O
,	O
QCNNs	Method
always	O
performed	O
better	O
than	O
CNNs	Method
independently	O
of	O
the	O
model	Method
topologies	Method
.	O

With	O
much	O
fewer	O
learning	O
parameters	O
for	O
a	O
given	O
architecture	O
,	O
the	O
QCNN	Method
performs	O
always	O
better	O
than	O
the	O
real	O
-	O
valued	O
one	O
on	O
the	O
reported	O
task	O
.	O

In	O
terms	O
of	O
PER	Metric
,	O
an	O
average	O
relative	Metric
gain	Metric
of	O
%	O
(	O
w.r.t	Metric
CNNs	Metric
result	Metric
)	O
is	O
obtained	O
on	O
the	O
testing	O
set	O
.	O

It	O
is	O
also	O
worth	O
recalling	O
that	O
the	O
best	O
PER	Metric
of	Metric
%	Metric
is	O
obtained	O
with	O
just	O
a	O
QCNN	Method
without	O
HMMs	O
,	O
RNNs	Method
,	O
attention	Method
mechanisms	Method
,	O
batch	Method
normalization	Method
,	O
phoneme	Method
language	Method
model	Method
,	O
acoustic	Method
data	Method
normalization	Method
or	O
adaptation	Task
.	O

Further	O
improvements	O
can	O
be	O
obtained	O
with	O
exactly	O
the	O
same	O
QCNN	Method
by	O
just	O
introducing	O
a	O
new	O
acoustic	O
feature	O
in	O
the	O
real	O
part	O
of	O
the	O
quaternions	O
.	O

section	O
:	O
Related	O
work	O
Early	O
attempts	O
to	O
perform	O
phoneme	Task
and	Task
phonetic	Task
feature	Task
recognition	Task
with	O
multilayer	Method
perceptrons	Method
(	O
MLP	Method
)	O
were	O
proposed	O
in	O
.	O

A	O
PER	Metric
of	O
%	O
is	O
reported	O
in	O
using	O
RNNs	Method
.	O

More	O
recently	O
,	O
in	O
a	O
Mean	Method
-	Method
Covariance	Method
Restricted	Method
Boltzmann	Method
Machine	Method
(	Method
RBM	Method
)	Method
is	O
used	O
for	O
recognizing	Task
phonemes	Task
in	O
the	O
TIMIT	Material
corpus	Material
using	O
RBM	Method
for	O
feature	Task
extraction	Task
.	O

Along	O
this	O
line	O
of	O
research	O
,	O
in	O
an	O
approach	O
called	O
the	O
Connectionist	O
Temporal	O
Classification	O
(	O
CTC	Method
)	O
has	O
been	O
developed	O
and	O
can	O
be	O
used	O
without	O
an	O
explicit	O
input	O
-	O
output	O
alignment	O
.	O

Bidirectional	Method
RNNs	Method
(	O
BRNNs	Method
)	O
are	O
used	O
in	O
for	O
processing	O
input	O
data	O
in	O
both	O
directions	O
with	O
two	O
separate	O
hidden	O
layers	O
,	O
which	O
are	O
then	O
composed	O
in	O
an	O
output	O
layer	O
.	O

With	O
standard	O
mel	O
frequency	O
energies	O
,	O
first	O
and	O
second	O
time	O
derivatives	O
a	O
PER	Metric
of	O
%	O
was	O
obtained	O
.	O

Other	O
recent	O
results	O
with	O
real	O
-	O
valued	O
vectors	O
of	O
similar	O
features	O
are	O
reported	O
in	O
.	O

Other	O
types	O
of	O
quaternion	Method
valued	Method
neural	Method
networks	Method
(	O
QNNs	Method
)	O
were	O
introduced	O
for	O
encoding	O
RGB	O
color	O
relations	O
in	O
image	O
pixels	O
,	O
and	O
for	O
classifying	Task
human	Task
/	Task
human	Task
conversation	Task
topics	Task
.	O

A	O
quaternion	O
deep	O
convolutional	Method
and	O
residual	O
neural	O
network	O
proposed	O
in	O
have	O
shown	O
impressive	O
results	O
on	O
the	O
CIFAR	Task
images	Task
classification	Task
task	Task
.	O

However	O
,	O
a	O
specific	O
quaternion	O
is	O
used	O
for	O
each	O
RGB	O
color	O
value	O
as	O
in	O
rather	O
than	O
integrating	O
pixel	O
multiple	O
views	O
as	O
in	O
,	O
and	O
suggested	O
in	O
this	O
paper	O
for	O
an	O
ASR	Task
task	O
.	O

section	O
:	O
Conclusions	O
Summary	O
.	O

This	O
paper	O
proposes	O
to	O
integrate	O
multiple	O
acoustic	O
feature	O
views	O
with	O
quaternion	O
hyper	O
complex	O
numbers	O
,	O
and	O
to	O
process	O
these	O
features	O
with	O
a	O
convolutional	Method
neural	O
network	O
of	O
quaternions	O
.	O

The	O
phoneme	Task
recognition	Task
experiments	O
have	O
shown	O
that	O
:	O
1	O
)	O
Given	O
an	O
equivalent	O
architecture	O
,	O
QCNNs	Method
always	O
outperform	O
CNNs	Method
with	O
significantly	O
less	O
parameters	O
;	O
2	O
)	O
QCNNs	Method
obtain	O
better	O
results	O
than	O
CNNs	Method
with	O
a	O
similar	O
number	O
of	O
learning	O
parameters	O
;	O
3	O
)	O
The	O
best	O
result	O
obtained	O
with	O
QCNNs	Method
is	O
better	O
than	O
the	O
one	O
observed	O
with	O
the	O
real	Method
-	Method
valued	Method
counterpart	Method
.	O

This	O
demonstrates	O
the	O
initial	O
intuition	O
that	O
the	O
capability	O
of	O
the	O
Hamilton	Method
product	Method
to	O
learn	O
internal	O
latent	O
relations	O
helps	O
quaternions	Method
-	Method
valued	Method
neural	Method
networks	Method
to	O
achieve	O
better	O
results	O
.	O

Limitations	O
and	O
Future	O
Work	O
.	O

So	O
far	O
,	O
traditional	O
acoustic	O
features	O
,	O
such	O
as	O
mel	Method
filter	Method
bank	Method
energies	Method
,	O
first	Method
and	Method
second	Method
derivatives	Method
have	O
shown	O
that	O
significantly	O
good	O
results	O
can	O
be	O
obtained	O
with	O
a	O
relative	O
small	O
set	O
of	O
input	O
features	O
for	O
a	O
speech	O
time	O
frame	O
.	O

Nevertheless	O
,	O
speech	O
science	O
has	O
shown	O
that	O
other	O
multi	O
-	O
view	O
context	O
-	O
dependent	O
acoustic	O
relations	O
characterize	O
signals	O
of	O
phonemes	O
in	O
context	O
.	O

Future	O
work	O
will	O
attempt	O
to	O
characterize	O
those	O
multi	O
-	O
view	O
features	O
that	O
mostly	O
contribute	O
to	O
reduce	O
ambiguities	O
in	O
representing	Task
phoneme	Task
events	Task
.	O

Furthermore	O
,	O
quaternions	Method
-	Method
valued	Method
RNNs	Method
will	O
also	O
be	O
investigated	O
to	O
see	O
if	O
they	O
can	O
contribute	O
to	O
the	O
improvement	O
of	O
recently	O
achieved	O
top	O
of	O
the	O
line	O
results	O
with	O
real	Method
number	Method
RNNs	Method
.	O

section	O
:	O
Acknowledgements	O
The	O
experiments	O
were	O
conducted	O
using	O
Keras	Method
.	O

The	O
authors	O
would	O
like	O
to	O
acknowledge	O
the	O
computing	O
support	O
of	O
Compute	O
Canada	O
and	O
the	O
founding	O
support	O
of	O
Orkis	O
,	O
NSERC	O
,	O
Samsung	O
,	O
IBM	O
and	O
CHIST	O
-	O
ERA	O
/	O
FRQ	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Kyle	O
Kastner	O
and	O
Mirco	O
Ravanelli	O
for	O
their	O
helpful	O
comments	O
.	O

bibliography	O
:	O
References	O
