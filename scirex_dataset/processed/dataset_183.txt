document	O
:	O
Abstractive	Task
Text	Task
Summarization	Task
using	O
Sequence	Method
-	Method
to	Method
-	Method
sequence	Method
RNNs	Method
and	O
Beyond	O
In	O
this	O
work	O
,	O
we	O
model	O
abstractive	Task
text	Task
summarization	Task
using	O
Attentional	Method
Encoder	Method
-	Method
Decoder	Method
Recurrent	Method
Neural	Method
Networks	Method
,	O
and	O
show	O
that	O
they	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
two	O
different	O
corpora	O
.	O

We	O
propose	O
several	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	Task
that	O
are	O
not	O
adequately	O
modeled	O
by	O
the	O
basic	O
architecture	O
,	O
such	O
as	O
modeling	O
key	O
-	O
words	O
,	O
capturing	O
the	O
hierarchy	O
of	O
sentence	O
-	O
to	O
-	O
word	O
structure	O
,	O
and	O
emitting	O
words	O
that	O
are	O
rare	O
or	O
unseen	O
at	O
training	O
time	O
.	O

Our	O
work	O
shows	O
that	O
many	O
of	O
our	O
proposed	O
models	O
contribute	O
to	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
consisting	O
of	O
multi	O
-	O
sentence	O
summaries	O
,	O
and	O
establish	O
performance	O
benchmarks	O
for	O
further	O
research	O
.	O

section	O
:	O
Introduction	O
Abstractive	Task
text	Task
summarization	Task
is	O
the	O
task	O
of	O
generating	O
a	O
headline	Task
or	O
a	O
short	O
summary	O
consisting	O
of	O
a	O
few	O
sentences	O
that	O
captures	O
the	O
salient	O
ideas	O
of	O
an	O
article	O
or	O
a	O
passage	O
.	O

We	O
use	O
the	O
adjective	O
‘	O
abstractive	O
’	O
to	O
denote	O
a	O
summary	O
that	O
is	O
not	O
a	O
mere	O
selection	O
of	O
a	O
few	O
existing	O
passages	O
or	O
sentences	O
extracted	O
from	O
the	O
source	O
,	O
but	O
a	O
compressed	O
paraphrasing	O
of	O
the	O
main	O
contents	O
of	O
the	O
document	O
,	O
potentially	O
using	O
vocabulary	O
unseen	O
in	O
the	O
source	O
document	O
.	O

This	O
task	O
can	O
also	O
be	O
naturally	O
cast	O
as	O
mapping	O
an	O
input	O
sequence	O
of	O
words	O
in	O
a	O
source	O
document	O
to	O
a	O
target	O
sequence	O
of	O
words	O
called	O
summary	O
.	O

In	O
the	O
recent	O
past	O
,	O
deep	Method
-	Method
learning	Method
based	Method
models	Method
that	O
map	O
an	O
input	O
sequence	O
into	O
another	O
output	O
sequence	O
,	O
called	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
,	O
have	O
been	O
successful	O
in	O
many	O
problems	O
such	O
as	O
machine	Task
translation	Task
,	O
speech	Task
recognition	Task
and	O
video	Task
captioning	Task
.	O

In	O
the	O
framework	O
of	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
models	Method
,	O
a	O
very	O
relevant	O
model	O
to	O
our	O
task	O
is	O
the	O
attentional	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
encoder	Method
-	Method
decoder	Method
model	Method
proposed	O
in	O
nmt	Method
,	O
which	O
has	O
produced	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
machine	Task
translation	Task
(	O
MT	Task
)	O
,	O
which	O
is	O
also	O
a	O
natural	Task
language	Task
task	Task
.	O

Despite	O
the	O
similarities	O
,	O
abstractive	Task
summarization	Task
is	O
a	O
very	O
different	O
problem	O
from	O
MT	Task
.	O

Unlike	O
in	O
MT	Task
,	O
the	O
target	O
(	O
summary	O
)	O
is	O
typically	O
very	O
short	O
and	O
does	O
not	O
depend	O
very	O
much	O
on	O
the	O
length	O
of	O
the	O
source	O
(	O
document	O
)	O
in	O
summarization	Task
.	O

Additionally	O
,	O
a	O
key	O
challenge	O
in	O
summarization	Task
is	O
to	O
optimally	O
compress	O
the	O
original	O
document	O
in	O
a	O
lossy	O
manner	O
such	O
that	O
the	O
key	O
concepts	O
in	O
the	O
original	O
document	O
are	O
preserved	O
,	O
whereas	O
in	O
MT	Task
,	O
the	O
translation	Task
is	O
expected	O
to	O
be	O
loss	O
-	O
less	O
.	O

In	O
translation	Task
,	O
there	O
is	O
a	O
strong	O
notion	O
of	O
almost	O
one	O
-	O
to	O
-	O
one	O
word	O
-	O
level	O
alignment	O
between	O
source	O
and	O
target	O
,	O
but	O
in	O
summarization	Task
,	O
it	O
is	O
less	O
obvious	O
.	O

We	O
make	O
the	O
following	O
main	O
contributions	O
in	O
this	O
work	O
:	O
(	O
i	O
)	O
We	O
apply	O
the	O
off	O
-	O
the	O
-	O
shelf	O
attentional	O
encoder	O
-	O
decoder	O
RNN	Method
that	O
was	O
originally	O
developed	O
for	O
machine	Task
translation	Task
to	O
summarization	Task
,	O
and	O
show	O
that	O
it	O
already	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
on	O
two	O
different	O
English	Material
corpora	Material
.	O

(	O
ii	O
)	O
Motivated	O
by	O
concrete	O
problems	O
in	O
summarization	Task
that	O
are	O
not	O
sufficiently	O
addressed	O
by	O
the	O
machine	Task
translation	Task
based	O
model	O
,	O
we	O
propose	O
novel	O
models	O
and	O
show	O
that	O
they	O
provide	O
additional	O
improvement	O
in	O
performance	O
.	O

(	O
iii	O
)	O
We	O
propose	O
a	O
new	O
dataset	O
for	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
of	O
a	O
document	O
into	O
multiple	O
sentences	O
and	O
establish	O
benchmarks	O
.	O

The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
.	O

In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
describe	O
each	O
specific	O
problem	O
in	O
abstractive	Task
summarization	Task
that	O
we	O
aim	O
to	O
solve	O
,	O
and	O
present	O
a	O
novel	O
model	O
that	O
addresses	O
it	O
.	O

Section	O
[	O
reference	O
]	O
contextualizes	O
our	O
models	O
with	O
respect	O
to	O
closely	O
related	O
work	O
on	O
the	O
topic	O
of	O
abstractive	Task
text	Task
summarization	Task
.	O

We	O
present	O
the	O
results	O
of	O
our	O
experiments	O
on	O
three	O
different	O
data	O
sets	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
also	O
present	O
some	O
qualitative	O
analysis	O
of	O
the	O
output	O
from	O
our	O
models	O
in	O
Section	O
[	O
reference	O
]	O
before	O
concluding	O
the	O
paper	O
with	O
remarks	O
on	O
our	O
future	O
direction	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Models	O
In	O
this	O
section	O
,	O
we	O
first	O
describe	O
the	O
basic	O
encoder	O
-	O
decoder	O
RNN	Method
that	O
serves	O
as	O
our	O
baseline	O
and	O
then	O
propose	O
several	O
novel	O
models	O
for	O
summarization	Task
,	O
each	O
addressing	O
a	O
specific	O
weakness	O
in	O
the	O
baseline	O
.	O

subsection	O
:	O
Encoder	O
-	O
Decoder	O
RNN	Method
with	O
Attention	Method
and	O
Large	Task
Vocabulary	Task
Trick	Task
Our	O
baseline	O
model	O
corresponds	O
to	O
the	O
neural	O
machine	Task
translation	Task
model	O
used	O
in	O
nmt	Method
.	O

The	O
encoder	Method
consists	O
of	O
a	O
bidirectional	O
GRU	O
-	O
RNN	Method
,	O
while	O
the	O
decoder	Method
consists	O
of	O
a	O
uni	O
-	O
directional	O
GRU	O
-	O
RNN	Method
with	O
the	O
same	O
hidden	O
-	O
state	O
size	O
as	O
that	O
of	O
the	O
encoder	O
,	O
and	O
an	O
attention	Method
mechanism	Method
over	O
the	O
source	O
-	O
hidden	O
states	O
and	O
a	O
soft	Method
-	Method
max	Method
layer	Method
over	O
target	O
vocabulary	O
to	O
generate	O
words	O
.	O

In	O
the	O
interest	O
of	O
space	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
original	O
paper	O
for	O
a	O
detailed	O
treatment	O
of	O
this	O
model	O
.	O

In	O
addition	O
to	O
the	O
basic	O
model	O
,	O
we	O
also	O
adapted	O
to	O
the	O
summarization	Task
problem	Task
,	O
the	O
large	Task
vocabulary	Task
‘	O
trick	O
’	O
(	O
LVT	Method
)	O
described	O
in	O
lvt	Method
.	O

In	O
our	O
approach	O
,	O
the	O
decoder	O
-	O
vocabulary	O
of	O
each	O
mini	O
-	O
batch	O
is	O
restricted	O
to	O
words	O
in	O
the	O
source	O
documents	O
of	O
that	O
batch	O
.	O

In	O
addition	O
,	O
the	O
most	O
frequent	O
words	O
in	O
the	O
target	O
dictionary	O
are	O
added	O
until	O
the	O
vocabulary	O
reaches	O
a	O
fixed	O
size	O
.	O

The	O
aim	O
of	O
this	O
technique	O
is	O
to	O
reduce	O
the	O
size	O
of	O
the	O
soft	Method
-	Method
max	Method
layer	Method
of	O
the	O
decoder	Method
which	O
is	O
the	O
main	O
computational	Metric
bottleneck	Metric
.	O

In	O
addition	O
,	O
this	O
technique	O
also	O
speeds	O
up	O
convergence	Task
by	O
focusing	O
the	O
modeling	O
effort	O
only	O
on	O
the	O
words	O
that	O
are	O
essential	O
to	O
a	O
given	O
example	O
.	O

This	O
technique	O
is	O
particularly	O
well	O
suited	O
to	O
summarization	Task
since	O
a	O
large	O
proportion	O
of	O
the	O
words	O
in	O
the	O
summary	O
come	O
from	O
the	O
source	O
document	O
in	O
any	O
case	O
.	O

subsection	O
:	O
Capturing	Task
Keywords	Task
using	O
Feature	Method
-	Method
rich	Method
Encoder	Method
In	O
summarization	Task
,	O
one	O
of	O
the	O
key	O
challenges	O
is	O
to	O
identify	O
the	O
key	O
concepts	O
and	O
key	O
entities	O
in	O
the	O
document	O
,	O
around	O
which	O
the	O
story	O
revolves	O
.	O

In	O
order	O
to	O
accomplish	O
this	O
goal	O
,	O
we	O
may	O
need	O
to	O
go	O
beyond	O
the	O
word	Method
-	Method
embeddings	Method
-	Method
based	Method
representation	Method
of	O
the	O
input	O
document	O
and	O
capture	O
additional	O
linguistic	O
features	O
such	O
as	O
parts	O
-	O
of	O
-	O
speech	O
tags	O
,	O
named	O
-	O
entity	O
tags	O
,	O
and	O
TF	O
and	O
IDF	O
statistics	O
of	O
the	O
words	O
.	O

We	O
therefore	O
create	O
additional	O
look	Method
-	Method
up	Method
based	Method
embedding	Method
matrices	Method
for	O
the	O
vocabulary	O
of	O
each	O
tag	O
-	O
type	O
,	O
similar	O
to	O
the	O
embeddings	O
for	O
words	O
.	O

For	O
continuous	O
features	O
such	O
as	O
TF	O
and	O
IDF	Method
,	O
we	O
convert	O
them	O
into	O
categorical	O
values	O
by	O
discretizing	O
them	O
into	O
a	O
fixed	O
number	O
of	O
bins	O
,	O
and	O
use	O
one	Method
-	Method
hot	Method
representations	Method
to	O
indicate	O
the	O
bin	O
number	O
they	O
fall	O
into	O
.	O

This	O
allows	O
us	O
to	O
map	O
them	O
into	O
an	O
embeddings	O
matrix	O
like	O
any	O
other	O
tag	O
-	O
type	O
.	O

Finally	O
,	O
for	O
each	O
word	O
in	O
the	O
source	O
document	O
,	O
we	O
simply	O
look	O
-	O
up	O
its	O
embeddings	O
from	O
all	O
of	O
its	O
associated	O
tags	O
and	O
concatenate	O
them	O
into	O
a	O
single	O
long	O
vector	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

On	O
the	O
target	O
side	O
,	O
we	O
continue	O
to	O
use	O
only	O
word	Method
-	Method
based	Method
embeddings	Method
as	O
the	O
representation	O
.	O

subsection	O
:	O
Modeling	Task
Rare	Task
/	Task
Unseen	Task
Words	Task
using	O
Switching	Method
Generator	Method
-	Method
Pointer	Method
Often	O
-	O
times	O
in	O
summarization	Task
,	O
the	O
keywords	O
or	O
named	O
-	O
entities	O
in	O
a	O
test	O
document	O
that	O
are	O
central	O
to	O
the	O
summary	O
may	O
actually	O
be	O
unseen	O
or	O
rare	O
with	O
respect	O
to	O
training	O
data	O
.	O

Since	O
the	O
vocabulary	O
of	O
the	O
decoder	O
is	O
fixed	O
at	O
training	O
time	O
,	O
it	O
can	O
not	O
emit	O
these	O
unseen	O
words	O
.	O

Instead	O
,	O
a	O
most	O
common	O
way	O
of	O
handling	O
these	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
is	O
to	O
emit	O
an	O
‘	O
UNK	O
’	O
token	O
as	O
a	O
placeholder	O
.	O

However	O
this	O
does	O
not	O
result	O
in	O
legible	O
summaries	O
.	O

In	O
summarization	Task
,	O
an	O
intuitive	O
way	O
to	O
handle	O
such	O
OOV	Material
words	Material
is	O
to	O
simply	O
point	O
to	O
their	O
location	O
in	O
the	O
source	O
document	O
instead	O
.	O

We	O
model	O
this	O
notion	O
using	O
our	O
novel	O
switching	Method
decoder	Method
/	O
pointer	Method
architecture	Method
which	O
is	O
graphically	O
represented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

In	O
this	O
model	O
,	O
the	O
decoder	Method
is	O
equipped	O
with	O
a	O
‘	O
switch	O
’	O
that	O
decides	O
between	O
using	O
the	O
generator	O
or	O
a	O
pointer	Method
at	O
every	O
time	O
-	O
step	O
.	O

If	O
the	O
switch	O
is	O
turned	O
on	O
,	O
the	O
decoder	O
produces	O
a	O
word	O
from	O
its	O
target	O
vocabulary	O
in	O
the	O
normal	O
fashion	O
.	O

However	O
,	O
if	O
the	O
switch	O
is	O
turned	O
off	O
,	O
the	O
decoder	Method
instead	O
generates	O
a	O
pointer	Method
to	O
one	O
of	O
the	O
word	O
-	O
positions	O
in	O
the	O
source	O
.	O

The	O
word	O
at	O
the	O
pointer	Method
-	O
location	O
is	O
then	O
copied	O
into	O
the	O
summary	O
.	O

The	O
switch	Method
is	O
modeled	O
as	O
a	O
sigmoid	Method
activation	Method
function	Method
over	O
a	O
linear	Method
layer	Method
based	O
on	O
the	O
entire	O
available	O
context	O
at	O
each	O
time	O
-	O
step	O
as	O
shown	O
below	O
.	O

where	O
is	O
the	O
probability	O
of	O
the	O
switch	O
turning	O
on	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	O
,	O
is	O
the	O
hidden	O
state	O
,	O
is	O
the	O
embedding	O
vector	O
of	O
the	O
emission	O
from	O
the	O
previous	O
time	O
step	O
,	O
is	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
,	O
and	O
and	O
are	O
the	O
switch	O
parameters	O
.	O

We	O
use	O
attention	O
distribution	O
over	O
word	O
positions	O
in	O
the	O
document	O
as	O
the	O
distribution	O
to	O
sample	O
the	O
pointer	Method
from	O
.	O

In	O
the	O
above	O
equation	O
,	O
is	O
the	O
pointer	Method
value	O
at	O
word	O
-	O
position	O
in	O
the	O
summary	O
,	O
sampled	O
from	O
the	O
attention	Method
distribution	Method
over	O
the	O
document	O
word	O
-	O
positions	O
,	O
where	O
is	O
the	O
probability	O
of	O
the	O
time	O
-	O
step	O
in	O
the	O
decoder	O
pointing	O
to	O
the	O
position	O
in	O
the	O
document	O
,	O
and	O
is	O
the	O
encoder	O
’s	O
hidden	O
state	O
at	O
position	O
.	O

At	O
training	O
time	O
,	O
we	O
provide	O
the	O
model	O
with	O
explicit	O
pointer	Method
information	O
whenever	O
the	O
summary	O
word	O
does	O
not	O
exist	O
in	O
the	O
target	O
vocabulary	O
.	O

When	O
the	O
OOV	O
word	O
in	O
summary	O
occurs	O
in	O
multiple	O
document	O
positions	O
,	O
we	O
break	O
the	O
tie	O
in	O
favor	O
of	O
its	O
first	O
occurrence	O
.	O

At	O
training	O
time	O
,	O
we	O
optimize	O
the	O
conditional	O
log	O
-	O
likelihood	O
shown	O
below	O
,	O
with	O
additional	O
regularization	O
penalties	O
.	O

where	O
and	O
are	O
the	O
summary	O
and	O
document	O
words	O
respectively	O
,	O
is	O
an	O
indicator	O
function	O
that	O
is	O
set	O
to	O
0	O
whenever	O
the	O
word	O
at	O
position	O
in	O
the	O
summary	O
is	O
OOV	O
with	O
respect	O
to	O
the	O
decoder	O
vocabulary	O
.	O

At	O
test	O
time	O
,	O
the	O
model	O
decides	O
automatically	O
at	O
each	O
time	O
-	O
step	O
whether	O
to	O
generate	O
or	O
to	O
point	O
,	O
based	O
on	O
the	O
estimated	O
switch	O
probability	O
.	O

We	O
simply	O
use	O
the	O
of	O
the	O
posterior	O
probability	O
of	O
generation	O
or	O
pointing	O
to	O
generate	O
the	O
best	O
output	O
at	O
each	O
time	O
step	O
.	O

The	O
pointer	Method
mechanism	O
may	O
be	O
more	O
robust	O
in	O
handling	O
rare	Task
words	Task
because	O
it	O
uses	O
the	O
encoder	Method
’s	Method
hidden	Method
-	Method
state	Method
representation	Method
of	O
rare	O
words	O
to	O
decide	O
which	O
word	O
from	O
the	O
document	O
to	O
point	O
to	O
.	O

Since	O
the	O
hidden	O
state	O
depends	O
on	O
the	O
entire	O
context	O
of	O
the	O
word	O
,	O
the	O
model	O
is	O
able	O
to	O
accurately	O
point	O
to	O
unseen	O
words	O
although	O
they	O
do	O
not	O
appear	O
in	O
the	O
target	O
vocabulary	O
.	O

subsection	O
:	O
Capturing	Task
Hierarchical	Task
Document	Task
Structure	Task
with	O
Hierarchical	O
Attention	O
In	O
datasets	O
where	O
the	O
source	O
document	O
is	O
very	O
long	O
,	O
in	O
addition	O
to	O
identifying	O
the	O
keywords	O
in	O
the	O
document	O
,	O
it	O
is	O
also	O
important	O
to	O
identify	O
the	O
key	O
sentences	O
from	O
which	O
the	O
summary	O
can	O
be	O
drawn	O
.	O

This	O
model	O
aims	O
to	O
capture	O
this	O
notion	O
of	O
two	O
levels	O
of	O
importance	O
using	O
two	O
bi	Method
-	Method
directional	Method
RNNs	Method
on	O
the	O
source	O
side	O
,	O
one	O
at	O
the	O
word	O
level	O
and	O
the	O
other	O
at	O
the	O
sentence	O
level	O
.	O

The	O
attention	Method
mechanism	Method
operates	O
at	O
both	O
levels	O
simultaneously	O
.	O

The	O
word	O
-	O
level	O
attention	O
is	O
further	O
re	O
-	O
weighted	O
by	O
the	O
corresponding	O
sentence	O
-	O
level	O
attention	O
and	O
re	O
-	O
normalized	O
as	O
shown	O
below	O
:	O
where	O
is	O
the	O
word	O
-	O
level	O
attention	O
weight	O
at	O
position	O
of	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
ID	O
of	O
the	O
sentence	O
at	O
word	O
position	O
,	O
is	O
the	O
sentence	O
-	O
level	O
attention	O
weight	O
for	O
the	O
sentence	O
in	O
the	O
source	O
,	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
source	O
document	O
,	O
and	O
is	O
the	O
re	O
-	O
scaled	O
attention	O
at	O
the	O
word	O
position	O
.	O

The	O
re	O
-	O
scaled	O
attention	O
is	O
then	O
used	O
to	O
compute	O
the	O
attention	O
-	O
weighted	O
context	O
vector	O
that	O
goes	O
as	O
input	O
to	O
the	O
hidden	O
state	O
of	O
the	O
decoder	O
.	O

Further	O
,	O
we	O
also	O
concatenate	O
additional	O
positional	O
embeddings	O
to	O
the	O
hidden	O
state	O
of	O
the	O
sentence	O
-	O
level	O
RNN	Method
to	O
model	O
positional	O
importance	O
of	O
sentences	O
in	O
the	O
document	O
.	O

This	O
architecture	O
therefore	O
models	O
key	O
sentences	O
as	O
well	O
as	O
keywords	O
within	O
those	O
sentences	O
jointly	O
.	O

A	O
graphical	Method
representation	Method
of	O
this	O
model	O
is	O
displayed	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
A	O
vast	O
majority	O
of	O
past	O
work	O
in	O
summarization	Task
has	O
been	O
extractive	Task
,	O
which	O
consists	O
of	O
identifying	O
key	O
sentences	O
or	O
passages	O
in	O
the	O
source	O
document	O
and	O
reproducing	O
them	O
as	O
summary	O
.	O

Humans	O
on	O
the	O
other	O
hand	O
,	O
tend	O
to	O
paraphrase	O
the	O
original	O
story	O
in	O
their	O
own	O
words	O
.	O

As	O
such	O
,	O
human	O
summaries	O
are	O
abstractive	O
in	O
nature	O
and	O
seldom	O
consist	O
of	O
reproduction	O
of	O
original	O
sentences	O
from	O
the	O
document	O
.	O

The	O
task	O
of	O
abstractive	Task
summarization	Task
has	O
been	O
standardized	O
using	O
the	O
DUC	Material
-	Material
2003	Material
and	O
DUC	Material
-	Material
2004	Material
competitions	O
.	O

The	O
data	O
for	O
these	O
tasks	O
consists	O
of	O
news	O
stories	O
from	O
various	O
topics	O
with	O
multiple	O
reference	O
summaries	O
per	O
story	O
generated	O
by	O
humans	O
.	O

The	O
best	O
performing	O
system	O
on	O
the	O
DUC	Material
-	Material
2004	Material
task	O
,	O
called	O
TOPIARY	Method
,	O
used	O
a	O
combination	O
of	O
linguistically	Method
motivated	Method
compression	Method
techniques	Method
,	O
and	O
an	O
unsupervised	Method
topic	Method
detection	Method
algorithm	Method
that	O
appends	O
keywords	O
extracted	O
from	O
the	O
article	O
onto	O
the	O
compressed	O
output	O
.	O

Some	O
of	O
the	O
other	O
notable	O
work	O
in	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
includes	O
using	O
traditional	O
phrase	O
-	O
table	O
based	O
machine	Task
translation	Task
approaches	O
,	O
compression	Method
using	O
weighted	Method
tree	Method
-	Method
transformation	Method
rules	Method
and	O
quasi	Method
-	Method
synchronous	Method
grammar	Method
approaches	Method
.	O

With	O
the	O
emergence	O
of	O
deep	Method
learning	Method
as	O
a	O
viable	O
alternative	O
for	O
many	O
NLP	Task
tasks	Task
,	O
researchers	O
have	O
started	O
considering	O
this	O
framework	O
as	O
an	O
attractive	O
,	O
fully	O
data	O
-	O
driven	O
alternative	O
to	O
abstractive	Task
summarization	Task
.	O

In	O
namas	Method
,	O
the	O
authors	O
use	O
convolutional	Method
models	Method
to	O
encode	O
the	O
source	O
,	O
and	O
a	O
context	Method
-	Method
sensitive	Method
attentional	Method
feed	Method
-	Method
forward	Method
neural	Method
network	Method
to	O
generate	O
the	O
summary	O
,	O
producing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
Gigaword	Material
and	O
DUC	Material
datasets	Material
.	O

In	O
an	O
extension	O
to	O
this	O
work	O
,	O
chopra	O
used	O
a	O
similar	O
convolutional	Method
model	Method
for	O
the	O
encoder	Method
,	O
but	O
replaced	O
the	O
decoder	Method
with	O
an	O
RNN	Method
,	O
producing	O
further	O
improvement	O
in	O
performance	O
on	O
both	O
datasets	O
.	O

In	O
another	O
paper	O
that	O
is	O
closely	O
related	O
to	O
our	O
work	O
,	O
hu:2015:EMNLP	Method
introduce	O
a	O
large	O
dataset	O
for	O
Chinese	Task
short	Task
text	Task
summarization	Task
.	O

They	O
show	O
promising	O
results	O
on	O
their	O
Chinese	Material
dataset	Material
using	O
an	O
encoder	O
-	O
decoder	O
RNN	Method
,	O
but	O
do	O
not	O
report	O
experiments	O
on	O
English	Material
corpora	Material
.	O

In	O
another	O
very	O
recent	O
work	O
,	O
jianpeng	O
used	O
RNN	Method
based	O
encoder	O
-	O
decoder	O
for	O
extractive	Task
summarization	Task
of	Task
documents	Task
.	O

This	O
model	O
is	O
not	O
directly	O
comparable	O
to	O
ours	O
since	O
their	O
framework	O
is	O
extractive	O
while	O
ours	O
and	O
that	O
of	O
,	O
and	O
is	O
abstractive	O
.	O

Our	O
work	O
starts	O
with	O
the	O
same	O
framework	O
as	O
,	O
where	O
we	O
use	O
RNNs	Method
for	O
both	O
source	O
and	O
target	O
,	O
but	O
we	O
go	O
beyond	O
the	O
standard	O
architecture	O
and	O
propose	O
novel	O
models	O
that	O
address	O
critical	O
problems	O
in	O
summarization	Task
.	O

We	O
also	O
note	O
that	O
this	O
work	O
is	O
an	O
extended	O
version	O
of	O
nallapati	O
.	O

In	O
addition	O
to	O
performing	O
more	O
extensive	O
experiments	O
compared	O
to	O
that	O
work	O
,	O
we	O
also	O
propose	O
a	O
novel	O
dataset	O
for	O
document	Task
summarization	Task
on	O
which	O
we	O
establish	O
benchmark	O
numbers	O
too	O
.	O

Below	O
,	O
we	O
analyze	O
the	O
similarities	O
and	O
differences	O
of	O
our	O
proposed	O
models	O
with	O
related	O
work	O
on	O
summarization	Task
.	O

Feature	Method
-	Method
rich	Method
encoder	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Linguistic	O
features	O
such	O
as	O
POS	O
tags	O
,	O
and	O
named	O
-	O
entities	O
as	O
well	O
as	O
TF	O
and	O
IDF	O
information	O
were	O
used	O
in	O
many	O
extractive	Method
approaches	Method
to	O
summarization	Task
,	O
but	O
they	O
are	O
novel	O
in	O
the	O
context	O
of	O
deep	Method
learning	Method
approaches	Method
for	O
abstractive	Task
summarization	Task
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
.	O

Switching	O
generator	O
-	O
pointer	Method
model	O
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
This	O
model	O
combines	O
extractive	Method
and	Method
abstractive	Method
approaches	Method
to	O
summarization	Task
in	O
a	O
single	O
end	Method
-	Method
to	Method
-	Method
end	Method
framework	Method
.	O

namas	Method
also	O
used	O
a	O
combination	O
of	O
extractive	Method
and	Method
abstractive	Method
approaches	Method
,	O
but	O
their	O
extractive	Method
model	Method
is	O
a	O
separate	O
log	Method
-	Method
linear	Method
classifier	Method
with	O
handcrafted	O
features	O
.	O

Pointer	Method
networks	Method
have	O
also	O
been	O
used	O
earlier	O
for	O
the	O
problem	O
of	O
rare	Task
words	Task
in	O
the	O
context	O
of	O
machine	Task
translation	Task
,	O
but	O
the	O
novel	O
addition	O
of	O
switch	O
in	O
our	O
model	O
allows	O
it	O
to	O
strike	O
a	O
balance	O
between	O
when	O
to	O
be	O
faithful	O
to	O
the	O
original	O
source	O
(	O
e.g.	O
,	O
for	O
named	O
entities	O
and	O
OOV	O
)	O
and	O
when	O
it	O
is	O
allowed	O
to	O
be	O
creative	O
.	O

We	O
believe	O
such	O
a	O
process	O
arguably	O
mimics	O
how	O
human	O
produces	O
summaries	O
.	O

For	O
a	O
more	O
detailed	O
treatment	O
of	O
this	O
model	O
,	O
and	O
experiments	O
on	O
multiple	O
tasks	O
,	O
please	O
refer	O
to	O
the	O
parallel	O
work	O
published	O
by	O
some	O
of	O
the	O
authors	O
of	O
this	O
work	O
.	O

Hierarchical	Method
attention	Method
model	Method
(	O
Sec	O
.	O

[	O
reference	O
]	O
)	O
:	O
Previously	O
proposed	O
hierarchical	Method
encoder	Method
-	Method
decoder	Method
models	Method
use	O
attention	O
only	O
at	O
sentence	O
-	O
level	O
.	O

The	O
novelty	O
of	O
our	O
approach	O
lies	O
in	O
joint	Task
modeling	Task
of	Task
attention	Task
at	O
both	O
sentence	O
and	O
word	O
levels	O
,	O
where	O
the	O
word	O
-	O
level	O
attention	O
is	O
further	O
influenced	O
by	O
sentence	O
-	O
level	O
attention	O
,	O
thus	O
capturing	O
the	O
notion	O
of	O
important	O
sentences	O
and	O
important	O
words	O
within	O
those	O
sentences	O
.	O

Concatenation	Method
of	Method
positional	Method
embeddings	Method
with	O
the	O
hidden	O
state	O
at	O
sentence	O
-	O
level	O
is	O
also	O
new	O
.	O

section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Gigaword	Material
Corpus	O
In	O
this	O
series	O
of	O
experiments	O
,	O
we	O
used	O
the	O
annotated	O
Gigaword	Material
corpus	O
as	O
described	O
in	O
namas	O
.	O

We	O
used	O
the	O
scripts	O
made	O
available	O
by	O
the	O
authors	O
of	O
this	O
work	O
to	O
preprocess	O
the	O
data	O
,	O
which	O
resulted	O
in	O
about	O
3.8	O
M	O
training	O
examples	O
.	O

The	O
script	O
also	O
produces	O
about	O
400	O
K	O
validation	O
and	O
test	O
examples	O
,	O
but	O
we	O
created	O
a	O
randomly	O
sampled	O
subset	O
of	O
2000	O
examples	O
each	O
for	O
validation	Task
and	Task
testing	Task
purposes	Task
,	O
on	O
which	O
we	O
report	O
our	O
performance	O
.	O

Further	O
,	O
we	O
also	O
acquired	O
the	O
exact	O
test	O
sample	O
used	O
in	O
namas	O
to	O
make	O
precise	O
comparison	O
of	O
our	O
models	O
with	O
theirs	O
.	O

We	O
also	O
made	O
small	O
modifications	O
to	O
the	O
script	O
to	O
extract	O
not	O
only	O
the	O
tokenized	O
words	O
,	O
but	O
also	O
system	O
-	O
generated	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
.	O

Training	O
:	O
For	O
all	O
the	O
models	O
we	O
discuss	O
below	O
,	O
we	O
used	O
200	O
dimensional	O
word2vec	O
vectors	O
trained	O
on	O
the	O
same	O
corpus	O
to	O
initialize	O
the	O
model	O
embeddings	O
,	O
but	O
we	O
allowed	O
them	O
to	O
be	O
updated	O
during	O
training	O
.	O

The	O
hidden	O
state	O
dimension	O
of	O
the	O
encoder	Method
and	Method
decoder	Method
was	O
fixed	O
at	O
400	O
in	O
all	O
our	O
experiments	O
.	O

When	O
we	O
used	O
only	O
the	O
first	O
sentence	O
of	O
the	O
document	O
as	O
the	O
source	O
,	O
as	O
done	O
in	O
namas	O
,	O
the	O
encoder	Metric
vocabulary	Metric
size	Metric
was	O
119	O
,	O
505	O
and	O
that	O
of	O
the	O
decoder	Method
stood	O
at	O
68	O
,	O
885	O
.	O

We	O
used	O
Adadelta	Method
for	O
training	Task
,	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.001	O
.	O

We	O
used	O
a	O
batch	O
-	O
size	O
of	O
50	O
and	O
randomly	O
shuffled	O
the	O
training	O
data	O
at	O
every	O
epoch	O
,	O
while	O
sorting	O
every	O
10	O
batches	O
according	O
to	O
their	O
lengths	O
to	O
speed	O
up	O
training	Task
.	O

We	O
did	O
not	O
use	O
any	O
dropout	Method
or	Method
regularization	Method
,	O
but	O
applied	O
gradient	Method
clipping	Method
.	O

We	O
used	O
early	Method
stopping	Method
based	O
on	O
the	O
validation	O
set	O
and	O
used	O
the	O
best	O
model	O
on	O
the	O
validation	O
set	O
to	O
report	O
all	O
test	O
performance	O
numbers	O
.	O

For	O
all	O
our	O
models	O
,	O
we	O
employ	O
the	O
large	Method
-	Method
vocabulary	Method
trick	Method
,	O
where	O
we	O
restrict	O
the	O
decoder	O
vocabulary	O
size	O
to	O
2	O
,	O
000	O
,	O
because	O
it	O
cuts	O
down	O
the	O
training	Metric
time	Metric
per	O
epoch	O
by	O
nearly	O
three	O
times	O
,	O
and	O
helps	O
this	O
and	O
all	O
subsequent	O
models	O
converge	O
in	O
only	O
50%	O
-	O
75	O
%	O
of	O
the	O
epochs	O
needed	O
for	O
the	O
model	O
based	O
on	O
full	O
vocabulary	O
.	O

Decoding	Task
:	O
At	O
decode	O
-	O
time	O
,	O
we	O
used	O
beam	Method
search	Method
of	O
size	O
5	O
to	O
generate	O
the	O
summary	O
,	O
and	O
limited	O
the	O
size	O
of	O
summary	O
to	O
a	O
maximum	O
of	O
30	O
words	O
,	O
since	O
this	O
is	O
the	O
maximum	O
size	O
we	O
noticed	O
in	O
the	O
sampled	O
validation	O
set	O
.	O

We	O
found	O
that	O
the	O
average	O
system	Metric
summary	Metric
length	Metric
from	O
all	O
our	O
models	O
(	O
7.8	O
to	O
8.3	O
)	O
agrees	O
very	O
closely	O
with	O
that	O
of	O
the	O
ground	O
truth	O
on	O
the	O
validation	O
set	O
(	O
about	O
8.7	O
words	O
)	O
,	O
without	O
any	O
specific	O
tuning	O
.	O

Computational	Metric
costs	Metric
:	O
We	O
trained	O
all	O
our	O
models	O
on	O
a	O
single	O
Tesla	Method
K40	Method
GPU	Method
.	O

Most	O
models	O
took	O
about	O
10	O
hours	O
per	O
epoch	O
on	O
an	O
average	O
except	O
the	O
hierarchical	Method
attention	Method
model	Method
,	O
which	O
took	O
12	O
hours	O
per	O
epoch	O
.	O

All	O
models	O
typically	O
converged	O
within	O
15	O
epochs	O
using	O
our	O
early	Metric
stopping	Metric
criterion	Metric
based	O
on	O
the	O
validation	Metric
cost	Metric
.	O

The	O
wall	Metric
-	Metric
clock	Metric
training	Metric
time	Metric
until	O
convergence	Metric
therefore	O
varies	O
between	O
6	O
-	O
8	O
days	O
depending	O
on	O
the	O
model	O
.	O

Generating	Task
summaries	Task
at	O
test	O
time	O
is	O
reasonably	O
fast	O
with	O
a	O
throughput	O
of	O
about	O
20	O
summaries	O
per	O
second	O
on	O
a	O
single	O
GPU	O
,	O
using	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	O
metrics	O
:	O
Similar	O
to	O
and	O
,	O
we	O
use	O
the	O
full	Metric
length	Metric
F1	Metric
variant	Metric
of	O
Rouge	Metric
to	O
evaluate	O
our	O
system	O
.	O

Although	O
limited	Metric
length	Metric
recall	Metric
was	O
the	O
preferred	O
metric	O
for	O
most	O
previous	O
work	O
,	O
one	O
of	O
its	O
disadvantages	O
is	O
choosing	O
the	O
length	O
limit	O
which	O
varies	O
from	O
corpus	O
to	O
corpus	O
,	O
making	O
it	O
difficult	O
for	O
researchers	O
to	O
compare	O
performances	O
.	O

Full	Task
-	Task
length	Task
recall	Task
,	O
on	O
the	O
other	O
hand	O
,	O
does	O
not	O
impose	O
a	O
length	O
restriction	O
but	O
unfairly	O
favors	O
longer	O
summaries	O
.	O

Full	Method
-	Method
length	Method
F1	Method
solves	O
this	O
problem	O
since	O
it	O
can	O
penalize	O
longer	O
summaries	O
,	O
while	O
not	O
imposing	O
a	O
specific	O
length	O
restriction	O
.	O

In	O
addition	O
,	O
we	O
also	O
report	O
the	O
percentage	O
of	O
tokens	O
in	O
the	O
system	O
summary	O
that	O
occur	O
in	O
the	O
source	O
(	O
which	O
we	O
call	O
‘	O
src	O
.	O

copy	Metric
rate	Metric
’	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

We	O
describe	O
all	O
our	O
experiments	O
and	O
results	O
on	O
the	O
Gigaword	Material
corpus	O
below	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
:	O
This	O
is	O
the	O
baseline	O
attentional	O
encoder	Method
-	Method
decoder	Method
model	Method
with	O
the	O
large	Method
vocabulary	Method
trick	Method
.	O

This	O
model	O
is	O
trained	O
only	O
on	O
the	O
first	O
sentence	O
from	O
the	O
source	O
document	O
,	O
as	O
done	O
in	O
namas	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
:	O
This	O
model	O
is	O
identical	O
to	O
the	O
model	O
above	O
except	O
for	O
the	O
fact	O
that	O
it	O
is	O
trained	O
on	O
the	O
first	O
two	O
sentences	O
from	O
the	O
source	O
.	O

On	O
this	O
corpus	O
,	O
adding	O
the	O
additional	O
sentence	O
in	O
the	O
source	O
does	O
seem	O
to	O
aid	O
performance	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
tried	O
adding	O
more	O
sentences	O
,	O
but	O
the	O
performance	O
dropped	O
,	O
which	O
is	O
probably	O
because	O
the	O
latter	O
sentences	O
in	O
this	O
corpus	O
are	O
not	O
pertinent	O
to	O
the	O
summary	O
.	O

words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
-	O
hieratt	O
:	O
Since	O
we	O
used	O
two	O
sentences	O
from	O
source	O
document	O
,	O
we	O
trained	O
the	O
hierarchical	Method
attention	Method
model	Method
proposed	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
this	O
model	O
improves	O
performance	O
compared	O
to	O
its	O
flatter	O
counterpart	O
by	O
learning	O
the	O
relative	O
importance	O
of	O
the	O
first	O
two	O
sentences	O
automatically	O
.	O

feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
:	O
Here	O
,	O
we	O
still	O
train	O
on	O
the	O
first	O
two	O
sentences	O
,	O
but	O
we	O
exploit	O
the	O
parts	O
-	O
of	O
-	O
speech	O
and	O
named	O
-	O
entity	O
tags	O
in	O
the	O
annotated	Material
gigaword	Material
corpus	Material
as	O
well	O
as	O
TF	O
,	O
IDF	O
values	O
,	O
to	O
augment	O
the	O
input	O
embeddings	O
on	O
the	O
source	O
side	O
as	O
described	O
in	O
Sec	O
[	O
reference	O
]	O
.	O

In	O
total	O
,	O
our	O
embedding	Method
vector	Method
grew	O
from	O
the	O
original	O
100	O
to	O
155	O
,	O
and	O
produced	O
incremental	O
gains	O
compared	O
to	O
its	O
counterpart	O
words	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
demonstrating	O
the	O
utility	O
of	O
syntax	Method
based	Method
features	Method
in	O
this	O
task	O
.	O

feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
-	O
ptr	O
:	O
This	O
is	O
the	O
switching	Method
generator	Method
/	O
pointer	Method
model	O
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
but	O
in	O
addition	O
,	O
we	O
also	O
use	O
feature	O
-	O
rich	O
embeddings	O
on	O
the	O
document	O
side	O
as	O
in	O
the	O
above	O
model	O
.	O

Our	O
experiments	O
indicate	O
that	O
the	O
new	O
model	O
is	O
able	O
to	O
achieve	O
the	O
best	O
performance	O
on	O
our	O
test	O
set	O
by	O
all	O
three	O
Rouge	Metric
variants	Metric
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Comparison	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
:	O
We	O
compared	O
the	O
performance	O
of	O
our	O
model	Method
words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
sample	O
created	O
by	O
namas	O
,	O
as	O
displayed	O
in	O
the	O
bottom	O
part	O
of	O
Table	O
[	O
reference	O
]	O
.	O

We	O
also	O
trained	O
another	O
system	O
which	O
we	O
call	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
which	O
has	O
a	O
larger	O
LVT	Method
vocabulary	O
size	O
of	O
5k	O
,	O
but	O
also	O
has	O
much	O
larger	O
source	O
and	O
target	O
vocabularies	O
of	O
400	O
K	O
and	O
200	O
K	O
respectively	O
.	O

The	O
reason	O
we	O
did	O
not	O
evaluate	O
our	O
best	O
validation	O
models	O
here	O
is	O
that	O
this	O
test	O
set	O
consisted	O
of	O
only	O
1	O
sentence	O
from	O
the	O
source	O
document	O
,	O
and	O
did	O
not	O
include	O
NLP	O
annotations	O
,	O
which	O
are	O
needed	O
in	O
our	O
best	O
models	O
.	O

The	O
table	O
shows	O
that	O
,	O
despite	O
this	O
fact	O
,	O
our	O
model	O
outperforms	O
the	O
ABS	Method
+	Method
model	Method
of	O
namas	Method
with	O
statistical	Metric
significance	Metric
.	O

In	O
addition	O
,	O
our	O
models	O
exhibit	O
better	O
abstractive	Metric
ability	Metric
as	O
shown	O
by	O
the	O
src	Method
.	O

copy	Metric
rate	Metric
metric	Metric
in	O
the	O
last	O
column	O
of	O
the	O
table	O
.	O

Further	O
,	O
our	O
larger	O
model	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
of	O
with	O
statistically	O
significant	O
improvement	O
on	O
Rouge	Metric
-	Metric
1	Metric
.	O

We	O
believe	O
the	O
bidirectional	O
RNN	Method
we	O
used	O
to	O
model	O
the	O
source	O
captures	O
richer	O
contextual	O
information	O
of	O
every	O
word	O
than	O
the	O
bag	Method
-	Method
of	Method
-	Method
embeddings	Method
representation	Method
used	O
by	O
namas	O
and	O
chopra	O
in	O
their	O
convolutional	Method
attentional	Method
encoders	Method
,	O
which	O
might	O
explain	O
our	O
superior	O
performance	O
.	O

Further	O
,	O
explicit	O
modeling	O
of	O
important	O
information	O
such	O
as	O
multiple	O
source	O
sentences	O
,	O
word	O
-	O
level	O
linguistic	O
features	O
,	O
using	O
the	O
switch	Method
mechanism	Method
to	O
point	O
to	O
source	O
words	O
when	O
needed	O
,	O
and	O
hierarchical	O
attention	O
,	O
solve	O
specific	O
problems	O
in	O
summarization	Task
,	O
each	O
boosting	O
performance	O
incrementally	O
.	O

subsection	O
:	O
DUC	Material
Corpus	Material
The	O
DUC	Material
corpus	Material
comes	O
in	O
two	O
parts	O
:	O
the	O
2003	Material
corpus	Material
consisting	O
of	O
624	O
document	O
,	O
summary	O
pairs	O
and	O
the	O
2004	Material
corpus	Material
consisting	O
of	O
500	O
pairs	O
.	O

Since	O
these	O
corpora	O
are	O
too	O
small	O
to	O
train	O
large	O
neural	Method
networks	Method
on	O
,	O
namas	Method
trained	O
their	O
models	O
on	O
the	O
Gigaword	Material
corpus	O
,	O
but	O
combined	O
it	O
with	O
an	O
additional	O
log	Method
-	Method
linear	Method
extractive	Method
summarization	Method
model	Method
with	O
handcrafted	O
features	O
,	O
that	O
is	O
trained	O
on	O
the	O
DUC	Material
2003	Material
corpus	Material
.	O

They	O
call	O
the	O
original	O
neural	Method
attention	Method
model	Method
the	O
ABS	Method
model	Method
,	O
and	O
the	O
combined	Method
model	Method
ABS	Method
+	Method
.	O

chopra	O
also	O
report	O
the	O
performance	O
of	O
their	O
RAS	Method
-	Method
Elman	Method
model	Method
on	O
this	O
corpus	O
and	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
since	O
it	O
outperforms	O
all	O
previously	O
published	O
baselines	O
including	O
non	Method
-	Method
neural	Method
network	Method
based	Method
extractive	Method
and	Method
abstractive	Method
systems	Method
,	O
as	O
measured	O
by	O
the	O
official	Metric
DUC	Metric
metric	Metric
of	Metric
recall	Metric
at	O
75	O
bytes	O
.	O

In	O
these	O
experiments	O
,	O
we	O
use	O
the	O
same	O
metric	O
to	O
evaluate	O
our	O
models	O
too	O
,	O
but	O
we	O
omit	O
reporting	O
numbers	O
from	O
other	O
systems	O
in	O
the	O
interest	O
of	O
space	O
.	O

In	O
our	O
work	O
,	O
we	O
simply	O
run	O
the	O
models	O
trained	O
on	O
Gigaword	Material
corpus	O
as	O
they	O
are	O
,	O
without	O
tuning	O
them	O
on	O
the	O
DUC	Material
validation	Material
set	Material
.	O

The	O
only	O
change	O
we	O
made	O
to	O
the	O
decoder	Method
is	O
to	O
suppress	O
the	O
model	O
from	O
emitting	O
the	O
end	O
-	O
of	O
-	O
summary	O
tag	O
,	O
and	O
force	O
it	O
to	O
emit	O
exactly	O
30	O
words	O
for	O
every	O
summary	O
,	O
since	O
the	O
official	O
evaluation	O
on	O
this	O
corpus	O
is	O
based	O
on	O
limited	O
-	O
length	O
Rouge	Metric
recall	O
.	O

On	O
this	O
corpus	O
too	O
,	O
since	O
we	O
have	O
only	O
a	O
single	O
sentence	O
from	O
source	O
and	O
no	O
NLP	Material
annotations	Material
,	O
we	O
ran	O
just	O
the	O
models	Method
words	Method
-	Method
lvt2k	Method
-	Method
1sent	Method
and	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
.	O

The	O
performance	O
of	O
this	O
model	O
on	O
the	O
test	O
set	O
is	O
compared	O
with	O
ABS	Method
and	Method
ABS	Method
+	Method
models	Method
,	O
RAS	Method
-	O
Elman	Method
from	O
,	O
as	O
well	O
as	O
TOPIARY	Method
,	O
the	O
top	O
performing	O
system	O
on	O
DUC	Material
-	Material
2004	Material
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
note	O
our	O
best	O
model	O
words	Method
-	Method
lvt5k	Method
-	Method
1sent	Method
outperforms	O
RAS	Method
-	Method
Elman	Method
on	O
two	O
of	O
the	O
three	O
variants	O
of	O
Rouge	Metric
,	O
while	O
being	O
competitive	O
on	O
Rouge	Metric
-	Metric
1	Metric
.	O

subsection	O
:	O
CNN	Material
/	Material
Daily	Material
Mail	Material
Corpus	Material
The	O
existing	O
abstractive	O
text	O
summarization	O
corpora	O
including	O
Gigaword	Material
and	O
DUC	Material
consist	O
of	O
only	O
one	O
sentence	O
in	O
each	O
summary	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
a	O
new	O
corpus	O
that	O
comprises	O
multi	Material
-	Material
sentence	Material
summaries	Material
.	O

To	O
produce	O
this	O
corpus	O
,	O
we	O
modify	O
an	O
existing	O
corpus	O
that	O
has	O
been	O
used	O
for	O
the	O
task	O
of	O
passage	Task
-	Task
based	Task
question	Task
answering	Task
.	O

In	O
this	O
work	O
,	O
the	O
authors	O
used	O
the	O
human	O
generated	O
abstractive	O
summary	O
bullets	O
from	O
new	O
-	O
stories	O
in	O
CNN	Material
and	O
Daily	Material
Mail	Material
websites	Material
as	O
questions	O
(	O
with	O
one	O
of	O
the	O
entities	O
hidden	O
)	O
,	O
and	O
stories	O
as	O
the	O
corresponding	O
passages	O
from	O
which	O
the	O
system	O
is	O
expected	O
to	O
answer	O
the	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
question	O
.	O

The	O
authors	O
released	O
the	O
scripts	O
that	O
crawl	O
,	O
extract	O
and	O
generate	O
pairs	O
of	O
passages	O
and	O
questions	O
from	O
these	O
websites	O
.	O

With	O
a	O
simple	O
modification	O
of	O
the	O
script	O
,	O
we	O
restored	O
all	O
the	O
summary	O
bullets	O
of	O
each	O
story	O
in	O
the	O
original	O
order	O
to	O
obtain	O
a	O
multi	O
-	O
sentence	O
summary	O
,	O
where	O
each	O
bullet	O
is	O
treated	O
as	O
a	O
sentence	O
.	O

In	O
all	O
,	O
this	O
corpus	O
has	O
286	O
,	O
817	O
training	O
pairs	O
,	O
13	O
,	O
368	O
validation	O
pairs	O
and	O
11	O
,	O
487	O
test	O
pairs	O
,	O
as	O
defined	O
by	O
their	O
scripts	O
.	O

The	O
source	O
documents	O
in	O
the	O
training	O
set	O
have	O
766	O
words	O
spanning	O
29.74	O
sentences	O
on	O
an	O
average	O
while	O
the	O
summaries	O
consist	O
of	O
53	O
words	O
and	O
3.72	O
sentences	O
.	O

The	O
unique	O
characteristics	O
of	O
this	O
dataset	O
such	O
as	O
long	O
documents	O
,	O
and	O
ordered	O
multi	O
-	O
sentence	O
summaries	O
present	O
interesting	O
challenges	O
,	O
and	O
we	O
hope	O
will	O
attract	O
future	O
researchers	O
to	O
build	O
and	O
test	O
novel	O
models	O
on	O
it	O
.	O

The	O
dataset	O
is	O
released	O
in	O
two	O
versions	O
:	O
one	O
consisting	O
of	O
actual	O
entity	O
names	O
,	O
and	O
the	O
other	O
,	O
in	O
which	O
entity	O
occurrences	O
are	O
replaced	O
with	O
document	O
-	O
specific	O
integer	O
-	O
ids	O
beginning	O
from	O
0	O
.	O

Since	O
the	O
vocabulary	Metric
size	Metric
is	O
smaller	O
in	O
the	O
anonymized	Method
version	Method
,	O
we	O
used	O
it	O
in	O
all	O
our	O
experiments	O
below	O
.	O

We	O
limited	O
the	O
source	O
vocabulary	O
size	O
to	O
150	O
K	O
,	O
and	O
the	O
target	O
vocabulary	O
to	O
60	O
K	O
,	O
the	O
source	O
and	O
target	O
lengths	O
to	O
at	O
most	O
800	O
and	O
100	O
words	O
respectively	O
.	O

We	O
used	O
100	Method
-	Method
dimensional	Method
word2vec	Method
embeddings	Method
trained	O
on	O
this	O
dataset	O
as	O
input	O
,	O
and	O
we	O
fixed	O
the	O
model	O
hidden	O
state	O
size	O
at	O
200	O
.	O

We	O
also	O
created	O
explicit	O
pointers	O
in	O
the	O
training	O
data	O
by	O
matching	O
only	O
the	O
anonymized	O
entity	O
-	O
ids	O
between	O
source	O
and	O
target	O
on	O
similar	O
lines	O
as	O
we	O
did	O
for	O
the	O
OOV	Material
words	Material
in	O
Gigaword	Material
corpus	O
.	O

Computational	Metric
costs	Metric
:	O
We	O
used	O
a	O
single	O
Tesla	Method
K	Method
-	Method
40	Method
GPU	Method
to	O
train	O
our	O
models	O
on	O
this	O
dataset	O
as	O
well	O
.	O

While	O
the	O
flat	Method
models	Method
(	O
words	Method
-	Method
lvt2k	Method
and	Method
words	Method
-	Method
lvt2k	Method
-	Method
ptr	Method
)	O
took	O
under	O
5	O
hours	O
per	O
epoch	O
,	O
the	O
hierarchical	Method
attention	Method
model	Method
was	O
very	O
expensive	O
,	O
consuming	O
nearly	O
12.5	O
hours	O
per	O
epoch	O
.	O

Convergence	O
of	O
all	O
models	O
is	O
also	O
slower	O
on	O
this	O
dataset	O
compared	O
to	O
Gigaword	Material
,	O
taking	O
nearly	O
35	O
epochs	O
for	O
all	O
models	O
.	O

Thus	O
,	O
the	O
wall	Metric
-	Metric
clock	Metric
time	Metric
for	O
training	O
until	O
convergence	Task
is	O
about	O
7	O
days	O
for	O
the	O
flat	Method
models	Method
,	O
but	O
nearly	O
18	O
days	O
for	O
the	O
hierarchical	Method
attention	Method
model	Method
.	O

Decoding	Method
is	O
also	O
slower	O
as	O
well	O
,	O
with	O
a	O
throughput	O
of	O
2	O
examples	O
per	O
second	O
for	O
flat	Method
models	Method
and	O
1.5	O
examples	O
per	O
second	O
for	O
the	O
hierarchical	Method
attention	Method
model	Method
,	O
when	O
run	O
on	O
a	O
single	O
GPU	Method
with	O
a	O
batch	O
size	O
of	O
1	O
.	O

Evaluation	Task
:	O
We	O
evaluated	O
our	O
models	O
using	O
the	O
full	O
-	O
length	O
Rouge	Metric
F1	O
metric	O
that	O
we	O
employed	O
for	O
the	O
Gigaword	Material
corpus	O
,	O
but	O
with	O
one	O
notable	O
difference	O
:	O
in	O
both	O
system	O
and	O
gold	O
summaries	O
,	O
we	O
considered	O
each	O
highlight	O
to	O
be	O
a	O
separate	O
sentence	O
.	O

Results	O
:	O
Results	O
from	O
the	O
basic	O
attention	Method
encoder	Method
-	Method
decoder	Method
as	O
well	O
as	O
the	O
hierarchical	Method
attention	Method
model	Method
are	O
displayed	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Although	O
this	O
dataset	O
is	O
smaller	O
and	O
more	O
complex	O
than	O
the	O
Gigaword	Material
corpus	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
Rouge	Metric
numbers	O
are	O
in	O
the	O
same	O
range	O
.	O

However	O
,	O
the	O
hierarchical	Method
attention	Method
model	Method
described	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
outperforms	O
the	O
baseline	O
attentional	Method
decoder	Method
only	O
marginally	O
.	O

Upon	O
visual	O
inspection	O
of	O
the	O
system	O
output	O
,	O
we	O
noticed	O
that	O
on	O
this	O
dataset	O
,	O
both	O
these	O
models	O
produced	O
summaries	O
that	O
contain	O
repetitive	O
phrases	O
or	O
even	O
repetitive	O
sentences	O
at	O
times	O
.	O

Since	O
the	O
summaries	O
in	O
this	O
dataset	O
involve	O
multiple	O
sentences	O
,	O
it	O
is	O
likely	O
that	O
the	O
decoder	Method
‘	O
forgets	O
’	O
what	O
part	O
of	O
the	O
document	O
was	O
used	O
in	O
producing	O
earlier	O
highlights	O
.	O

To	O
overcome	O
this	O
problem	O
,	O
we	O
used	O
the	O
Temporal	Method
Attention	Method
model	Method
of	O
baskaran	Method
that	O
keeps	O
track	O
of	O
past	O
attentional	O
weights	O
of	O
the	O
decoder	O
and	O
expliticly	O
discourages	O
it	O
from	O
attending	O
to	O
the	O
same	O
parts	O
of	O
the	O
document	O
in	O
future	O
time	O
steps	O
.	O

The	O
model	O
works	O
as	O
shown	O
by	O
the	O
following	O
simple	O
equations	O
:	O
where	O
is	O
the	O
unnormalized	O
attention	O
-	O
weights	O
vector	O
at	O
the	O
time	O
-	O
step	O
of	O
the	O
decoder	Method
.	O

In	O
other	O
words	O
,	O
the	O
temporal	Method
attention	Method
model	Method
down	O
-	O
weights	O
the	O
attention	O
weights	O
at	O
the	O
current	O
time	O
step	O
if	O
the	O
past	O
attention	O
weights	O
are	O
high	O
on	O
the	O
same	O
part	O
of	O
the	O
document	O
.	O

Using	O
this	O
strategy	O
,	O
the	O
temporal	Method
attention	Method
model	Method
improves	O
performance	O
significantly	O
over	O
both	O
the	O
baseline	Method
model	Method
as	O
well	O
as	O
the	O
hierarchical	Method
attention	Method
model	Method
.	O

We	O
have	O
also	O
noticed	O
that	O
there	O
are	O
fewer	O
repetitions	O
of	O
summay	O
highlights	O
produced	O
by	O
this	O
model	O
as	O
shown	O
in	O
the	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O

These	O
results	O
,	O
although	O
preliminary	O
,	O
should	O
serve	O
as	O
a	O
good	O
baseline	O
for	O
future	O
researchers	O
to	O
compare	O
their	O
models	O
against	O
.	O

section	O
:	O
Qualitative	Task
Analysis	Task
Table	O
[	O
reference	O
]	O
presents	O
a	O
few	O
high	O
quality	O
and	O
poor	O
quality	O
output	O
on	O
the	O
validation	O
set	O
from	O
feats	Method
-	Method
lvt2k	Method
-	Method
2sent	Method
,	O
one	O
of	O
our	O
best	O
performing	O
models	O
.	O

Even	O
when	O
the	O
model	O
differs	O
from	O
the	O
target	O
summary	O
,	O
its	O
summaries	O
tend	O
to	O
be	O
very	O
meaningful	O
and	O
relevant	O
,	O
a	O
phenomenon	O
not	O
captured	O
by	O
word	Metric
/	Metric
phrase	Metric
matching	Metric
evaluation	Metric
metrics	Metric
such	O
as	O
Rouge	Metric
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
model	O
sometimes	O
‘	O
misinterprets	O
’	O
the	O
semantics	O
of	O
the	O
text	O
and	O
generates	O
a	O
summary	O
with	O
a	O
comical	O
interpretation	O
as	O
shown	O
in	O
the	O
poor	O
quality	O
examples	O
in	O
the	O
table	O
.	O

Clearly	O
,	O
capturing	O
the	O
‘	O
meaning	O
’	O
of	O
complex	O
sentences	O
remains	O
a	O
weakness	O
of	O
these	O
models	O
.	O

Our	O
next	O
example	O
output	O
,	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
displays	O
the	O
sample	O
output	O
from	O
the	O
switching	Method
generator	Method
/	O
pointer	Method
model	O
on	O
the	O
Gigaword	Material
corpus	O
.	O

It	O
is	O
apparent	O
from	O
the	O
examples	O
that	O
the	O
model	O
learns	O
to	O
use	O
pointers	O
very	O
accurately	O
not	O
only	O
for	O
named	O
entities	O
,	O
but	O
also	O
for	O
multi	Task
-	Task
word	Task
phrases	Task
.	O

Despite	O
its	O
accuracy	Metric
,	O
the	O
performance	O
improvement	O
of	O
the	O
overall	O
model	O
is	O
not	O
significant	O
.	O

We	O
believe	O
the	O
impact	O
of	O
this	O
model	O
may	O
be	O
more	O
pronounced	O
in	O
other	O
settings	O
with	O
a	O
heavier	O
tail	O
distribution	O
of	O
rare	O
words	O
.	O

We	O
intend	O
to	O
carry	O
out	O
more	O
experiments	O
with	O
this	O
model	O
in	O
the	O
future	O
.	O

On	O
CNN	Material
/	Material
Daily	Material
Mail	Material
data	Material
,	O
although	O
our	O
models	O
are	O
able	O
to	O
produce	O
good	O
quality	O
multi	O
-	O
sentence	O
summaries	O
,	O
we	O
notice	O
that	O
the	O
same	O
sentence	O
or	O
phrase	O
often	O
gets	O
repeated	O
in	O
the	O
summary	O
.	O

We	O
believe	O
models	O
that	O
incorporate	O
intra	Method
-	Method
attention	Method
such	O
as	O
lstmn	Method
can	O
fix	O
this	O
problem	O
by	O
encouraging	O
the	O
model	O
to	O
‘	O
remember	O
’	O
the	O
words	O
it	O
has	O
already	O
produced	O
in	O
the	O
past	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
apply	O
the	O
attentional	Method
encoder	Method
-	Method
decoder	Method
for	O
the	O
task	O
of	O
abstractive	Task
summarization	Task
with	O
very	O
promising	O
results	O
,	O
outperforming	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
significantly	O
on	O
two	O
different	O
datasets	O
.	O

Each	O
of	O
our	O
proposed	O
novel	O
models	O
addresses	O
a	O
specific	O
problem	O
in	O
abstractive	Task
summarization	Task
,	O
yielding	O
further	O
improvement	O
in	O
performance	O
.	O

We	O
also	O
propose	O
a	O
new	O
dataset	O
for	O
multi	Task
-	Task
sentence	Task
summarization	Task
and	O
establish	O
benchmark	O
numbers	O
on	O
it	O
.	O

As	O
part	O
of	O
our	O
future	O
work	O
,	O
we	O
plan	O
to	O
focus	O
our	O
efforts	O
on	O
this	O
data	O
and	O
build	O
more	O
robust	O
models	O
for	O
summaries	Task
consisting	O
of	O
multiple	O
sentences	O
.	O

bibliography	O
:	O
References	O
