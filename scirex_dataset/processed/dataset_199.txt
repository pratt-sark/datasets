document	O
:	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Networks	Method
Graph	O
-	O
structured	O
data	O
appears	O
frequently	O
in	O
domains	O
including	O
chemistry	O
,	O
natural	O
language	O
semantics	O
,	O
social	O
networks	O
,	O
and	O
knowledge	O
bases	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
feature	Method
learning	Method
techniques	Method
for	O
graph	O
-	O
structured	O
inputs	O
.	O

Our	O
starting	O
point	O
is	O
previous	O
work	O
on	O
Graph	Method
Neural	Method
Networks	Method
scarselli2009graph	O
,	O
which	O
we	O
modify	O
to	O
use	O
gated	Method
recurrent	Method
units	Method
and	O
modern	O
optimization	Method
techniques	Method
and	O
then	O
extend	O
to	O
output	O
sequences	O
.	O

The	O
result	O
is	O
a	O
flexible	O
and	O
broadly	O
useful	O
class	O
of	O
neural	Method
network	Method
models	Method
that	O
has	O
favorable	O
inductive	Metric
biases	Metric
relative	O
to	O
purely	O
sequence	Method
-	Method
based	Method
models	Method
(	O
e.g.	O
,	O
LSTMs	Method
)	O
when	O
the	O
problem	O
is	O
graph	O
-	O
structured	O
.	O

We	O
demonstrate	O
the	O
capabilities	O
on	O
some	O
simple	O
AI	Task
(	O
bAbI	O
)	O
and	O
graph	O
algorithm	O
learning	O
tasks	O
.	O

We	O
then	O
show	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
problem	O
from	O
program	Task
verification	Task
,	O
in	O
which	O
subgraphs	O
need	O
to	O
be	O
described	O
as	O
abstract	O
data	O
structures	O
.	O

Much	O
data	O
in	O
the	O
real	O
world	O
is	O
naturally	O
represented	O
as	O
a	O
graph	O
.	O

For	O
example	O
,	O
entities	O
can	O
be	O
represented	O
as	O
nodes	O
,	O
with	O
relations	O
between	O
them	O
represented	O
as	O
edges	O
.	O

Graph	O
-	O
structured	O
data	O
comes	O
up	O
often	O
in	O
chemistry	O
,	O
social	O
networks	O
,	O
knowledge	O
bases	O
,	O
and	O
web	O
-	O
based	O
data	O
,	O
and	O
in	O
other	O
domains	O
like	O
program	Task
verification	Task
which	O
requires	O
reasoning	O
about	O
abstract	O
data	O
structures	O
represented	O
as	O
graphs	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
feature	Method
learning	Method
techniques	Method
for	O
learning	Task
from	O
graph	O
-	O
structured	O
inputs	O
.	O

Our	O
starting	O
point	O
is	O
previous	O
work	O
on	O
Graph	Method
Neural	Method
Networks	Method
(	O
GNNs	Method
)	O
scarselli2009graph	O
.	O

We	O
begin	O
by	O
adapting	O
the	O
model	O
and	O
learning	Method
algorithm	Method
to	O
make	O
use	O
of	O
recent	O
advances	O
in	O
recurrent	Method
neural	Method
networks	Method
,	O
incorporating	O
gated	Method
recurrent	Method
units	Method
and	O
modern	O
optimization	Method
routines	Method
.	O

Our	O
main	O
contribution	O
is	O
then	O
to	O
extend	O
GNNs	Method
to	O
output	O
sequences	O
.	O

We	O
use	O
bAbI	Task
tasks	Task
to	O
illustrate	O
the	O
components	O
of	O
the	O
model	O
and	O
explain	O
what	O
is	O
being	O
learned	O
by	O
the	O
GNN	Method
models	Method
,	O
and	O
then	O
we	O
develop	O
a	O
full	O
model	O
that	O
learns	O
to	O
predict	O
loop	O
invariants	O
for	O
a	O
program	Task
verification	Task
task	Task
,	O
where	O
we	O
map	O
from	O
graphs	O
representing	O
the	O
state	O
of	O
memory	O
during	O
a	O
program	O
’s	O
execution	O
to	O
a	O
logical	O
description	O
of	O
the	O
data	O
structures	O
that	O
have	O
been	O
instantiated	O
.	O

We	O
show	O
that	O
we	O
are	O
able	O
to	O
match	O
the	O
performance	O
of	O
a	O
system	O
that	O
has	O
been	O
heavily	O
hand	O
-	O
engineered	O
,	O
indicating	O
the	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Networks	Method
are	O
promising	O
models	O
for	O
feature	Task
learning	Task
on	Task
graphs	Task
.	O

arrows	O
,	O
decorations.pathmorphing	O
,	O
backgrounds	O
,	O
fit	O
,	O
automata	O
,	O
shapes	O
,	O
calc	O
,	O
positioning	O
,	O
shadows	O
heapgraph	O
/	O
.style=	O
everynode	O
/	O
.style	O
=	O
rectangle	O
,	O
font=	O
,	O
tree	O
/	O
.appendstyle	O
=	O
draw	O
=	O
black	O
,	O
thick	O
,	O
minimumwidth=.4	O
cm	O
,	O
fill	O
=	O
blue!20	O
,	O
list	O
/	O
.appendstyle	O
=	O
draw	O
=	O
black	O
,	O
thick	O
,	O
minimumwidth=.4	O
cm	O
,	O
fill	O
=	O
green!20	O
,	O
prefixaftercommand=	O
everylabel	O
/	O
.style	O
=	O
font=	O
,	O
innersep=2pt	O
,	O
active	O
/	O
.appendstyle	O
=	O
double	O
,	O
explained	O
/	O
.appendstyle	O
=	O
fill	O
=	O
white	O
,	O
colLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
stepLabel	O
/	O
.appendstyle	O
=	O
textwidth=2	O
cm	O
,	O
align	O
=	O
left	O
,	O
font=	O
,	O
outLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
heapEdgeLabel	O
/	O
.appendstyle	O
=	O
font=	O
,	O
innersep=2pt	O
,	O
,	O
section	O
:	O
Introduction	O
Many	O
practical	O
applications	O
build	O
on	O
graph	O
-	O
structured	O
data	O
,	O
and	O
thus	O
we	O
often	O
want	O
to	O
perform	O
machine	Task
learning	Task
tasks	Task
that	O
take	O
graphs	O
as	O
inputs	O
.	O

Standard	O
approaches	O
to	O
the	O
problem	O
include	O
engineering	O
custom	O
features	O
of	O
an	O
input	O
graph	O
,	O
graph	Method
kernels	Method
kashima2003marginalized	O
,	O
shervashidze2011weisfeiler	O
,	O
and	O
methods	O
that	O
define	O
graph	O
features	O
in	O
terms	O
of	O
random	Method
walks	Method
on	Method
graphs	Method
perozzi2014deepwalk	O
.	O

More	O
closely	O
related	O
to	O
our	O
goal	O
in	O
this	O
work	O
are	O
methods	O
that	O
learn	O
features	O
on	O
graphs	O
,	O
including	O
Graph	Method
Neural	Method
Networks	Method
gori2005new	O
,	O
scarselli2009graph	O
,	O
spectral	Method
networks	Method
bruna2013spectral	O
and	O
recent	O
work	O
on	O
learning	O
graph	Task
fingerprints	Task
for	O
classification	Task
tasks	Task
on	O
graph	Method
representations	Method
of	O
chemical	O
molecules	O
duvenaud2015convolutional	O
.	O

Our	O
main	O
contribution	O
is	O
an	O
extension	O
of	O
Graph	Method
Neural	Method
Networks	Method
that	O
outputs	O
sequences	O
.	O

Previous	O
work	O
on	O
feature	Task
learning	Task
for	O
graph	Task
-	Task
structured	Task
inputs	Task
has	O
focused	O
on	O
models	O
that	O
produce	O
single	O
outputs	O
such	O
as	O
graph	O
-	O
level	O
classifications	O
,	O
but	O
many	O
problems	O
with	O
graph	O
inputs	O
require	O
outputting	O
sequences	O
.	O

Examples	O
include	O
paths	O
on	O
a	O
graph	O
,	O
enumerations	O
of	O
graph	O
nodes	O
with	O
desirable	O
properties	O
,	O
or	O
sequences	O
of	O
global	O
classifications	O
mixed	O
with	O
,	O
for	O
example	O
,	O
a	O
start	O
and	O
end	O
node	O
.	O

We	O
are	O
not	O
aware	O
of	O
existing	O
graph	Method
feature	Method
learning	Method
work	O
suitable	O
for	O
this	O
problem	O
.	O

Our	O
motivating	O
application	O
comes	O
from	O
program	Task
verification	Task
and	O
requires	O
outputting	O
logical	O
formulas	O
,	O
which	O
we	O
formulate	O
as	O
a	O
sequential	Task
output	Task
problem	Task
.	O

A	O
secondary	O
contribution	O
is	O
highlighting	O
that	O
Graph	Method
Neural	Method
Networks	Method
(	O
and	O
further	O
extensions	O
we	O
develop	O
here	O
)	O
are	O
a	O
broadly	O
useful	O
class	O
of	O
neural	Method
network	Method
model	Method
that	O
is	O
applicable	O
to	O
many	O
problems	O
currently	O
facing	O
the	O
field	O
.	O

There	O
are	O
two	O
settings	O
for	O
feature	Task
learning	Task
on	Task
graphs	Task
:	O
(	O
1	O
)	O
learning	O
a	O
representation	Method
of	Method
the	Method
input	Method
graph	Method
,	O
and	O
(	O
2	O
)	O
learning	O
representations	O
of	O
the	O
internal	O
state	O
during	O
the	O
process	O
of	O
producing	O
a	O
sequence	O
of	O
outputs	O
.	O

Here	O
,	O
(	O
1	O
)	O
is	O
mostly	O
achieved	O
by	O
previous	O
work	O
on	O
Graph	Method
Neural	Method
Networks	Method
scarselli2009graph	O
;	O
we	O
make	O
several	O
minor	O
adaptations	O
of	O
this	O
framework	O
,	O
including	O
changing	O
it	O
to	O
use	O
modern	O
practices	O
around	O
Recurrent	Method
Neural	Method
Networks	Method
.	O

(	O
2	O
)	O
is	O
important	O
because	O
we	O
desire	O
outputs	O
from	O
graph	Task
-	Task
structured	Task
problems	Task
that	O
are	O
not	O
solely	O
individual	O
classifications	O
.	O

In	O
these	O
cases	O
,	O
the	O
challenge	O
is	O
how	O
to	O
learn	O
features	O
on	O
the	O
graph	O
that	O
encode	O
the	O
partial	O
output	O
sequence	O
that	O
has	O
already	O
been	O
produced	O
(	O
e.g.	O
,	O
the	O
path	O
so	O
far	O
if	O
outputting	O
a	O
path	O
)	O
and	O
that	O
still	O
needs	O
to	O
be	O
produced	O
(	O
e.g.	O
,	O
the	O
remaining	O
path	O
)	O
.	O

We	O
will	O
show	O
how	O
the	O
GNN	Method
framework	Method
can	O
be	O
adapted	O
to	O
these	O
settings	O
,	O
leading	O
to	O
a	O
novel	O
graph	Method
-	Method
based	Method
neural	Method
network	Method
model	Method
that	O
we	O
call	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Networks	Method
(	O
GGS	Method
-	Method
NNs	Method
)	O
.	O

We	O
illustrate	O
aspects	O
of	O
this	O
general	O
model	O
in	O
experiments	O
on	O
bAbI	Task
tasks	Task
weston2015towards	O
and	O
graph	Task
algorithm	Task
learning	Task
tasks	Task
that	O
illustrate	O
the	O
capabilities	O
of	O
the	O
model	O
.	O

We	O
then	O
present	O
an	O
application	O
to	O
the	O
verification	Task
of	Task
computer	Task
programs	Task
.	O

When	O
attempting	O
to	O
prove	O
properties	O
such	O
as	O
memory	Task
safety	Task
(	O
i.e.	O
,	O
that	O
there	O
are	O
no	O
null	O
pointer	O
dereferences	O
in	O
a	O
program	O
)	O
,	O
a	O
core	O
problem	O
is	O
to	O
find	O
mathematical	O
descriptions	O
of	O
the	O
data	O
structures	O
used	O
in	O
a	O
program	O
.	O

Following	O
,	O
we	O
have	O
phrased	O
this	O
as	O
a	O
machine	Task
learning	Task
problem	Task
where	O
we	O
will	O
learn	O
to	O
map	O
from	O
a	O
set	O
of	O
input	O
graphs	O
,	O
representing	O
the	O
state	O
of	O
memory	O
,	O
to	O
a	O
logical	O
description	O
of	O
the	O
data	O
structures	O
that	O
have	O
been	O
instantiated	O
.	O

Whereas	O
relied	O
on	O
a	O
large	O
amount	O
of	O
hand	O
-	O
engineering	O
of	O
features	O
,	O
we	O
show	O
that	O
the	O
system	O
can	O
be	O
replaced	O
with	O
a	O
GGS	Method
-	Method
NN	Method
at	O
no	O
cost	O
in	O
accuracy	Metric
.	O

section	O
:	O
Graph	Method
Neural	Method
Networks	Method
In	O
this	O
section	O
,	O
we	O
review	O
Graph	Method
Neural	Method
Networks	Method
(	O
GNNs	Method
)	O
gori2005new	O
,	O
scarselli2009graph	O
and	O
introduce	O
notation	O
and	O
concepts	O
that	O
will	O
be	O
used	O
throughout	O
.	O

GNNs	Method
are	O
a	O
general	Method
neural	Method
network	Method
architecture	Method
defined	O
according	O
to	O
a	O
graph	O
structure	O
.	O

Nodes	O
take	O
unique	O
values	O
from	O
,	O
and	O
edges	O
are	O
pairs	O
.	O

We	O
will	O
focus	O
in	O
this	O
work	O
on	O
directed	O
graphs	O
,	O
so	O
represents	O
a	O
directed	O
edge	O
,	O
but	O
we	O
note	O
that	O
the	O
framework	O
can	O
easily	O
be	O
adapted	O
to	O
undirected	O
graphs	O
;	O
see	O
.	O

The	O
node	O
vector	O
(	O
or	O
node	Method
representation	Method
or	O
node	Method
embedding	Method
)	O
for	O
node	O
is	O
denoted	O
by	O
.	O

Graphs	O
may	O
also	O
contain	O
node	O
labels	O
for	O
each	O
node	O
and	O
edge	O
labels	O
or	O
edge	O
types	O
for	O
each	O
edge	O
.	O

We	O
will	O
overload	O
notation	O
and	O
let	O
when	O
is	O
a	O
set	O
of	O
nodes	O
,	O
and	O
when	O
is	O
a	O
set	O
of	O
edges	O
.	O

The	O
function	O
returns	O
the	O
set	O
of	O
predecessor	O
nodes	O
with	O
.	O

Analogously	O
,	O
is	O
the	O
set	O
of	O
successor	O
nodes	O
with	O
edges	O
.	O

The	O
set	O
of	O
all	O
nodes	O
neighboring	O
is	O
,	O
and	O
the	O
set	O
of	O
all	O
edges	O
incoming	O
to	O
or	O
outgoing	O
from	O
is	O
.	O

GNNs	Method
map	O
graphs	O
to	O
outputs	O
via	O
two	O
steps	O
.	O

First	O
,	O
there	O
is	O
a	O
propagation	Method
step	Method
that	O
computes	O
node	Method
representations	Method
for	O
each	O
node	O
;	O
second	O
,	O
an	O
output	Method
model	Method
maps	O
from	O
node	O
representations	O
and	O
corresponding	O
labels	O
to	O
an	O
output	O
for	O
each	O
.	O

In	O
the	O
notation	O
for	O
,	O
we	O
leave	O
the	O
dependence	O
on	O
parameters	O
implicit	O
,	O
and	O
we	O
will	O
continue	O
to	O
do	O
this	O
throughout	O
.	O

The	O
system	O
is	O
differentiable	O
from	O
end	O
-	O
to	O
-	O
end	O
,	O
so	O
all	O
parameters	O
are	O
learned	O
jointly	O
using	O
gradient	Method
-	Method
based	Method
optimization	Method
.	O

subsection	O
:	O
Propagation	Method
Model	Method
Here	O
,	O
an	O
iterative	Method
procedure	Method
propagates	O
node	Method
representations	Method
.	O

Initial	O
node	Method
representations	Method
are	O
set	O
to	O
arbitrary	O
values	O
,	O
then	O
each	O
node	Method
representation	Method
is	O
updated	O
following	O
the	O
recurrence	O
below	O
until	O
convergence	O
,	O
where	O
denotes	O
the	O
timestep	O
:	O
Several	O
variants	O
are	O
discussed	O
in	O
including	O
positional	O
graph	O
forms	O
,	O
node	O
-	O
specific	O
updates	O
,	O
and	O
alternative	O
representations	Method
of	Method
neighborhoods	Method
.	O

Concretely	O
,	O
suggest	O
decomposing	O
to	O
be	O
a	O
sum	Method
of	Method
per	Method
-	Method
edge	Method
terms	Method
:	O
where	O
is	O
either	O
a	O
linear	O
function	O
of	O
or	O
a	O
neural	Method
network	Method
.	O

The	O
parameters	O
of	O
depends	O
on	O
the	O
configuration	O
of	O
labels	O
,	O
e.g.	O
in	O
the	O
following	O
linear	O
case	O
,	O
and	O
are	O
learnable	O
parameters	O
,	O
subsection	O
:	O
Output	Method
Model	Method
and	O
Learning	Task
The	O
output	Method
model	Method
is	O
defined	O
per	O
node	O
and	O
is	O
a	O
differentiable	O
function	O
that	O
maps	O
to	O
an	O
output	O
.	O

This	O
is	O
generally	O
a	O
linear	Method
or	Method
neural	Method
network	Method
mapping	Method
.	O

focus	O
on	O
outputs	O
that	O
are	O
independent	O
per	O
node	O
,	O
which	O
are	O
implemented	O
by	O
mapping	O
the	O
final	O
node	O
representations	O
,	O
to	O
an	O
output	O
for	O
each	O
node	O
.	O

To	O
handle	O
graph	Task
-	Task
level	Task
classifications	Task
,	O
they	O
suggest	O
to	O
create	O
a	O
dummy	O
“	O
super	O
node	O
”	O
that	O
is	O
connected	O
to	O
all	O
other	O
nodes	O
by	O
a	O
special	O
type	O
of	O
edge	O
.	O

Thus	O
,	O
graph	Task
-	Task
level	Task
regression	Task
or	O
classification	Task
can	O
be	O
handled	O
in	O
the	O
same	O
manner	O
as	O
node	Method
-	Method
level	Method
regression	Method
or	O
classification	Task
.	O

Learning	Method
is	O
done	O
via	O
the	O
Almeida	Method
-	Method
Pineda	Method
algorithm	Method
almeida1990learning	Method
,	O
pineda1987generalization	Method
,	O
which	O
works	O
by	O
running	O
the	O
propagation	Method
to	O
convergence	O
,	O
and	O
then	O
computing	O
gradients	O
based	O
upon	O
the	O
converged	O
solution	O
.	O

This	O
has	O
the	O
advantage	O
of	O
not	O
needing	O
to	O
store	O
intermediate	O
states	O
in	O
order	O
to	O
compute	O
gradients	O
.	O

The	O
disadvantage	O
is	O
that	O
parameters	O
must	O
be	O
constrained	O
so	O
that	O
the	O
propagation	O
step	O
is	O
a	O
contraction	Method
map	Method
.	O

This	O
is	O
needed	O
to	O
ensure	O
convergence	O
,	O
but	O
it	O
may	O
limit	O
the	O
expressivity	O
of	O
the	O
model	O
.	O

When	O
is	O
a	O
neural	Method
network	Method
,	O
this	O
is	O
encouraged	O
using	O
a	O
penalty	O
term	O
on	O
the	O
1	Method
-	Method
norm	Method
of	Method
the	Method
network	Method
’s	O
Jacobian	O
.	O

See	O
Appendix	O
[	O
reference	O
]	O
for	O
an	O
example	O
that	O
gives	O
the	O
intuition	O
that	O
contraction	O
maps	O
have	O
trouble	O
propagating	O
information	O
across	O
a	O
long	O
range	O
in	O
a	O
graph	O
.	O

section	O
:	O
Gated	O
Graph	Method
Neural	Method
Networks	Method
We	O
now	O
describe	O
Gated	O
Graph	Method
Neural	Method
Networks	Method
(	O
GG	Method
-	Method
NNs	Method
)	O
,	O
our	O
adaptation	Method
of	Method
GNNs	Method
that	O
is	O
suitable	O
for	O
non	O
-	O
sequential	O
outputs	O
.	O

We	O
will	O
describe	O
sequential	O
outputs	O
in	O
the	O
next	O
section	O
.	O

The	O
biggest	O
modification	O
of	O
GNNs	Method
is	O
that	O
we	O
use	O
Gated	Method
Recurrent	Method
Units	Method
cho2014learning	O
and	O
unroll	O
the	O
recurrence	O
for	O
a	O
fixed	O
number	O
of	O
steps	O
and	O
use	O
backpropagation	O
through	O
time	O
in	O
order	O
to	O
compute	O
gradients	O
.	O

This	O
requires	O
more	O
memory	O
than	O
the	O
Almeida	Method
-	Method
Pineda	Method
algorithm	Method
,	O
but	O
it	O
removes	O
the	O
need	O
to	O
constrain	O
parameters	O
to	O
ensure	O
convergence	O
.	O

We	O
also	O
extend	O
the	O
underlying	O
representations	Method
and	O
output	Method
model	Method
.	O

subsection	O
:	O
Node	O
Annotations	O
In	O
GNNs	Method
,	O
there	O
is	O
no	O
point	O
in	O
initializing	O
node	O
representations	O
because	O
the	O
contraction	O
map	O
constraint	O
ensures	O
that	O
the	O
fixed	O
point	O
is	O
independent	O
of	O
the	O
initializations	O
.	O

This	O
is	O
no	O
longer	O
the	O
case	O
with	O
GG	Method
-	Method
NNs	Method
,	O
which	O
lets	O
us	O
incorporate	O
node	O
labels	O
as	O
additional	O
inputs	O
.	O

To	O
distinguish	O
these	O
node	O
labels	O
used	O
as	O
inputs	O
from	O
the	O
ones	O
introduced	O
before	O
,	O
we	O
call	O
them	O
node	O
annotations	O
,	O
and	O
use	O
vector	O
to	O
denote	O
these	O
annotations	O
.	O

To	O
illustrate	O
how	O
the	O
node	O
annotations	O
are	O
used	O
,	O
consider	O
an	O
example	O
task	O
of	O
training	O
a	O
graph	Method
neural	Method
network	Method
to	O
predict	O
whether	O
node	O
can	O
be	O
reached	O
from	O
node	O
on	O
a	O
given	O
graph	O
.	O

For	O
this	O
task	O
,	O
there	O
are	O
two	O
problem	O
-	O
related	O
special	O
nodes	O
,	O
and	O
.	O

To	O
mark	O
these	O
nodes	O
as	O
special	O
,	O
we	O
give	O
them	O
an	O
initial	O
annotation	O
.	O

The	O
first	O
node	O
gets	O
the	O
annotation	O
,	O
and	O
the	O
second	O
node	O
gets	O
the	O
annotation	O
.	O

All	O
other	O
nodes	O
have	O
their	O
initial	O
annotation	O
set	O
to	O
.	O

Intuitively	O
,	O
this	O
marks	O
as	O
the	O
first	O
input	O
argument	O
and	O
as	O
the	O
second	O
input	O
argument	O
.	O

We	O
then	O
initialize	O
the	O
node	O
state	O
vectors	O
using	O
these	O
label	O
vectors	O
by	O
copying	O
into	O
the	O
first	O
dimensions	O
and	O
padding	O
with	O
extra	O
0	O
’s	O
to	O
allow	O
hidden	O
states	O
that	O
are	O
larger	O
than	O
the	O
annotation	O
size	O
.	O

In	O
the	O
reachability	O
example	O
,	O
it	O
is	O
easy	O
for	O
the	O
propagation	Method
model	Method
to	O
learn	O
to	O
propagate	O
the	O
node	O
annotation	O
for	O
to	O
all	O
nodes	O
reachable	O
from	O
,	O
for	O
example	O
by	O
setting	O
the	O
propagation	O
matrix	O
associated	O
with	O
forward	O
edges	O
to	O
have	O
a	O
1	O
in	O
position	O
(	O
0	O
,	O
0	O
)	O
.	O

This	O
will	O
cause	O
the	O
first	O
dimension	O
of	O
node	Method
representation	Method
to	O
be	O
copied	O
along	O
forward	O
edges	O
.	O

With	O
this	O
setting	O
of	O
parameters	O
,	O
the	O
propagation	Method
step	Method
will	O
cause	O
all	O
nodes	O
reachable	O
from	O
to	O
have	O
their	O
first	O
bit	O
of	O
node	O
representation	O
set	O
to	O
1	O
.	O

The	O
output	O
step	O
classifier	Method
can	O
then	O
easily	O
tell	O
whether	O
node	O
is	O
reachable	O
from	O
by	O
looking	O
whether	O
some	O
node	O
has	O
nonzero	O
entries	O
in	O
the	O
first	O
two	O
dimensions	O
of	O
its	O
representation	O
vector	O
.	O

subsection	O
:	O
Propagation	Method
Model	Method
The	O
basic	O
recurrence	O
of	O
the	O
propagation	Method
model	Method
is	O
The	O
matrix	O
determines	O
how	O
nodes	O
in	O
the	O
graph	O
communicate	O
with	O
each	O
other	O
.	O

The	O
sparsity	O
structure	O
and	O
parameter	O
tying	O
in	O
is	O
illustrated	O
in	O
fig	O
:	O
graphs	O
-	O
and	O
-	O
sparsity	O
.	O

The	O
sparsity	O
structure	O
corresponds	O
to	O
the	O
edges	O
of	O
the	O
graph	O
,	O
and	O
the	O
parameters	O
in	O
each	O
submatrix	O
are	O
determined	O
by	O
the	O
edge	O
type	O
and	O
direction	O
.	O

are	O
the	O
two	O
columns	O
of	O
blocks	O
in	O
and	O
corresponding	O
to	O
node	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
the	O
initialization	Method
step	Method
,	O
which	O
copies	O
node	O
annotations	O
into	O
the	O
first	O
components	O
of	O
the	O
hidden	O
state	O
and	O
pads	O
the	O
rest	O
with	O
zeros	O
.	O

(	O
[	O
reference	O
]	O
)	O
is	O
the	O
step	O
that	O
passes	O
information	O
between	O
different	O
nodes	O
of	O
the	O
graph	O
via	O
incoming	O
and	O
outgoing	O
edges	O
with	O
parameters	O
dependent	O
on	O
the	O
edge	O
type	O
and	O
direction	O
.	O

contains	O
activations	O
from	O
edges	O
in	O
both	O
directions	O
.	O

The	O
remaining	O
are	O
GRU	Method
-	Method
like	Method
updates	Method
that	O
incorporate	O
information	O
from	O
the	O
other	O
nodes	O
and	O
from	O
the	O
previous	O
timestep	O
to	O
update	O
each	O
node	O
’s	O
hidden	O
state	O
.	O

and	O
are	O
the	O
update	O
and	O
reset	O
gates	O
,	O
is	O
the	O
logistic	Method
sigmoid	Method
function	Method
,	O
and	O
is	O
element	Method
-	Method
wise	Method
multiplication	Method
.	O

We	O
initially	O
experimented	O
with	O
a	O
vanilla	Method
recurrent	Method
neural	Method
network	Method
-	Method
style	Method
update	Method
,	O
but	O
in	O
preliminary	O
experiments	O
we	O
found	O
this	O
GRU	Method
-	Method
like	Method
propagation	Method
step	Method
to	O
be	O
more	O
effective	O
.	O

subsection	O
:	O
Output	Method
Models	Method
There	O
are	O
several	O
types	O
of	O
one	O
-	O
step	O
outputs	O
that	O
we	O
would	O
like	O
to	O
produce	O
in	O
different	O
situations	O
.	O

First	O
,	O
GG	Method
-	Method
NNs	Method
support	O
node	Task
selection	Task
tasks	Task
by	O
making	O
for	O
each	O
node	O
output	O
node	O
scores	O
and	O
applying	O
a	O
softmax	O
over	O
node	O
scores	O
.	O

Second	O
,	O
for	O
graph	O
-	O
level	O
outputs	O
,	O
we	O
define	O
a	O
graph	Method
level	Method
representation	Method
vector	Method
as	O
where	O
acts	O
as	O
a	O
soft	Method
attention	Method
mechanism	Method
that	O
decides	O
which	O
nodes	O
are	O
relevant	O
to	O
the	O
current	O
graph	Task
-	Task
level	Task
task	Task
.	O

and	O
are	O
neural	Method
networks	Method
that	O
take	O
the	O
concatenation	O
of	O
and	O
as	O
input	O
and	O
outputs	O
real	O
-	O
valued	O
vectors	O
.	O

The	O
functions	O
can	O
also	O
be	O
replaced	O
with	O
the	O
identity	O
.	O

section	O
:	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Networks	Method
Here	O
we	O
describe	O
Gated	Method
Graph	Method
Sequence	Method
Neural	Method
Networks	Method
(	O
GGS	Method
-	Method
NNs	Method
)	O
,	O
in	O
which	O
several	O
GG	Method
-	Method
NNs	Method
operate	O
in	O
sequence	O
to	O
produce	O
an	O
output	O
sequence	O
.	O

For	O
the	O
output	O
step	O
,	O
we	O
denote	O
the	O
matrix	O
of	O
node	O
annotations	O
as	O
.	O

We	O
use	O
two	O
GG	Method
-	Method
NNs	Method
k	O
and	O
k	O
:	O
k	O
for	O
predicting	Task
from	Task
,	O
and	O
k	O
for	O
predicting	O
from	O
.	O

can	O
be	O
seen	O
as	O
the	O
states	O
carried	O
over	O
from	O
step	O
to	O
.	O

Both	O
k	O
and	O
k	O
contain	O
a	O
propagation	Method
model	Method
and	O
an	O
output	Method
model	Method
.	O

In	O
the	O
propagation	Method
models	Method
,	O
we	O
denote	O
the	O
matrix	O
of	O
node	O
vectors	O
at	O
the	O
propagation	O
step	O
of	O
the	O
output	O
step	O
as	O
.	O

As	O
before	O
,	O
in	O
step	O
,	O
we	O
set	O
by	O
-	O
extending	O
per	O
node	O
.	O

An	O
overview	O
of	O
the	O
model	O
is	O
shown	O
in	O
fig	O
:	O
seq	Method
-	Method
architecture2	Method
.	O

Alternatively	O
,	O
and	O
can	O
share	O
a	O
single	O
propagation	Method
model	Method
,	O
and	O
just	O
have	O
separate	O
output	Method
models	Method
.	O

This	O
simpler	O
variant	O
is	O
faster	O
to	O
train	O
and	O
evaluate	O
,	O
and	O
in	O
many	O
cases	O
can	O
achieve	O
similar	O
performance	O
level	O
as	O
the	O
full	Method
model	Method
.	O

But	O
in	O
cases	O
where	O
the	O
desired	O
propagation	O
behavior	O
for	O
and	O
are	O
different	O
,	O
this	O
variant	O
may	O
not	O
work	O
as	O
well	O
.	O

[	O
hstate	O
/	O
.style	O
=	O
draw	O
,	O
rectangle	O
,	O
rounded	O
corners	O
,	O
inner	O
sep=2pt	O
,	O
font=	O
,	O
updateLabel	O
/	O
.style	O
=	O
sloped	O
,	O
font=	O
]	O
[	O
hstate	O
]	O
(	O
L1	O
)	O
at	O
(	O
0	O
,	O
0	O
)	O
;	O
[	O
hstate	O
]	O
(	O
H11	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
O1	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
L2	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
H21	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
O2	O
)	O
at	O
(	O
)	O
;	O
[	O
hstate	O
]	O
(	O
L3	O
)	O
at	O
(	O
)	O
;	O
[	O
]	O
(	O
cnt	O
)	O
at	O
(	O
)	O
;	O
above	O
,	O
updateLabel	O
]	O
Init	O
(	O
H11	O
)	O
(	O
H11	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
1	O
(	O
O1	O
)	O
(	O
H11	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
1	O
(	O
L2	O
)	O
(	O
L2	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
Init	O
(	O
H21	O
)	O
(	O
H21	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
2	O
(	O
O2	O
)	O
(	O
H21	O
)	O
edge	O
node	O
[	O
above	O
,	O
updateLabel	O
]	O
2	O
(	O
L3	O
)	O
(	O
L3	O
)	O
edge	O
(	O
cnt	O
)	O
;	O
We	O
introduce	O
a	O
node	Method
annotation	Method
output	Method
model	Method
for	O
predicting	Task
from	O
.	O

The	O
prediction	Task
is	O
done	O
for	O
each	O
node	O
independently	O
using	O
a	O
neural	Method
network	Method
that	O
takes	O
the	O
concatenation	O
of	O
and	O
as	O
input	O
and	O
outputs	O
a	O
vector	O
of	O
real	O
-	O
valued	O
scores	O
:	O
There	O
are	O
two	O
settings	O
for	O
training	O
GGS	Method
-	Method
NNs	Method
:	O
specifying	O
all	O
intermediate	O
annotations	O
,	O
or	O
training	O
the	O
full	Method
model	Method
end	O
-	O
to	O
-	O
end	O
given	O
only	O
,	O
graphs	O
and	O
target	O
sequences	O
.	O

The	O
former	O
can	O
improve	O
performance	O
when	O
we	O
have	O
domain	O
knowledge	O
about	O
specific	O
intermediate	O
information	O
that	O
should	O
be	O
represented	O
in	O
the	O
internal	O
state	O
of	O
nodes	O
,	O
while	O
the	O
latter	O
is	O
more	O
general	O
.	O

We	O
describe	O
both	O
.	O

paragraph	O
:	O
Sequence	O
outputs	O
with	O
observed	O
annotations	O
Consider	O
the	O
task	O
of	O
making	O
a	O
sequence	Task
of	Task
predictions	Task
for	O
a	O
graph	O
,	O
where	O
each	O
prediction	O
is	O
only	O
about	O
a	O
part	O
of	O
the	O
graph	O
.	O

In	O
order	O
to	O
ensure	O
we	O
predict	O
an	O
output	O
for	O
each	O
part	O
of	O
the	O
graph	O
exactly	O
once	O
,	O
it	O
suffices	O
to	O
have	O
one	O
bit	O
per	O
node	O
,	O
indicating	O
whether	O
the	O
node	O
has	O
been	O
“	O
explained	O
”	O
so	O
far	O
.	O

In	O
some	O
settings	O
,	O
a	O
small	O
number	O
of	O
annotations	O
are	O
sufficient	O
to	O
capture	O
the	O
state	O
of	O
the	O
output	O
procedure	O
.	O

When	O
this	O
is	O
the	O
case	O
,	O
we	O
may	O
want	O
to	O
directly	O
input	O
this	O
information	O
into	O
the	O
model	O
via	O
labels	O
indicating	O
target	O
intermediate	O
annotations	O
.	O

In	O
some	O
cases	O
,	O
these	O
annotations	O
may	O
be	O
sufficient	O
,	O
in	O
that	O
we	O
can	O
define	O
a	O
model	O
where	O
the	O
GG	Method
-	Method
NNs	Method
are	O
rendered	O
conditionally	O
independent	O
given	O
the	O
annotations	O
.	O

In	O
this	O
case	O
,	O
at	O
training	O
time	O
,	O
given	O
the	O
annotations	O
the	O
sequence	Task
prediction	Task
task	Task
decomposes	O
into	O
single	Task
step	Task
prediction	Task
tasks	Task
and	O
can	O
be	O
trained	O
as	O
separate	O
GG	Method
-	Method
NNs	Method
.	O

At	O
test	O
time	O
,	O
predicted	O
annotations	O
from	O
one	O
step	O
will	O
be	O
used	O
as	O
input	O
to	O
the	O
next	O
step	O
.	O

This	O
is	O
analogous	O
to	O
training	O
directed	Method
graphical	Method
models	Method
when	O
data	O
is	O
fully	O
observed	O
.	O

paragraph	O
:	O
Sequence	O
outputs	O
with	O
latent	O
annotations	O
More	O
generally	O
,	O
when	O
intermediate	O
node	O
annotations	O
are	O
not	O
available	O
during	O
training	O
,	O
we	O
treat	O
them	O
as	O
hidden	O
units	O
in	O
the	O
network	O
,	O
and	O
train	O
the	O
whole	O
model	O
jointly	O
by	O
backpropagating	O
through	O
the	O
whole	O
sequence	O
.	O

section	O
:	O
Explanatory	Task
Applications	Task
In	O
this	O
section	O
we	O
present	O
example	O
applications	O
that	O
concretely	O
illustrate	O
the	O
use	O
of	O
GGS	Method
-	Method
NNs	Method
.	O

We	O
focus	O
on	O
a	O
selection	O
of	O
bAbI	Task
artificial	Task
intelligence	Task
(	O
AI	Task
)	O
tasks	O
weston2015towards	O
and	O
two	O
graph	Method
algorithm	Method
learning	Method
tasks	Method
.	O

subsection	O
:	O
bAbI	Task
Tasks	Task
The	O
bAbI	Task
tasks	Task
are	O
meant	O
to	O
test	O
reasoning	Task
capabilities	Task
that	O
AI	Task
systems	O
should	O
be	O
capable	O
of	O
.	O

In	O
the	O
bAbI	O
suite	O
,	O
there	O
are	O
20	O
tasks	O
that	O
test	O
basic	O
forms	O
of	O
reasoning	Task
like	O
deduction	Task
,	O
induction	Task
,	O
counting	Task
,	O
and	O
path	Task
-	Task
finding	Task
.	O

We	O
have	O
defined	O
a	O
basic	O
transformation	Method
procedure	Method
that	O
maps	O
bAbI	Task
tasks	Task
to	O
GG	Method
-	Method
NNs	Method
or	O
GGS	Method
-	Method
NNs	Method
.	O

We	O
use	O
the	O
--	O
symbolic	O
option	O
from	O
the	O
released	O
bAbI	O
code	O
to	O
get	O
stories	O
that	O
just	O
involve	O
sequences	O
of	O
relations	O
between	O
entities	O
,	O
which	O
are	O
then	O
converted	O
into	O
a	O
graph	O
.	O

Each	O
entity	O
is	O
mapped	O
to	O
a	O
node	O
,	O
and	O
each	O
relation	O
is	O
mapped	O
to	O
an	O
edge	O
with	O
edge	O
label	O
given	O
by	O
the	O
relation	O
.	O

The	O
full	O
story	O
is	O
consumed	O
and	O
mapped	O
to	O
a	O
single	O
graph	O
.	O

Questions	O
are	O
marked	O
by	O
eval	O
in	O
the	O
data	O
and	O
are	O
comprised	O
of	O
a	O
question	O
type	O
(	O
e.g.	O
,	O
has_fear	O
)	O
,	O
and	O
some	O
argument	O
(	O
e.g.	O
,	O
one	O
or	O
more	O
nodes	O
)	O
.	O

The	O
arguments	O
are	O
converted	O
into	O
initial	O
node	O
annotations	O
,	O
with	O
the	O
-	O
th	O
bit	O
of	O
the	O
-	O
th	O
argument	O
node	O
’s	O
annotation	O
vector	O
set	O
to	O
1	O
.	O

For	O
example	O
,	O
if	O
the	O
eval	O
line	O
is	O
eval	O
E	O
>	O
A	O
true	O
,	O
then	O
E	O
gets	O
initial	O
annotation	O
,	O
A	O
gets	O
,	O
and	O
for	O
all	O
other	O
nodes	O
,	O
.	O

Question	O
type	O
is	O
1	O
(	O
for	O
‘	O
>	O
’	O
)	O
and	O
output	O
is	O
class	O
1	O
(	O
for	O
‘	O
true	O
’	O
)	O
.	O

Some	O
tasks	O
have	O
multiple	O
question	O
types	O
,	O
for	O
example	O
Task	O
4	O
which	O
has	O
4	O
question	O
types	O
:	O
e	O
,	O
s	O
,	O
w	O
,	O
n	O
.	O

For	O
such	O
tasks	O
we	O
simply	O
train	O
a	O
separate	O
GG	Method
-	Method
NN	Method
for	O
each	O
task	O
.	O

We	O
do	O
not	O
use	O
the	O
strong	O
supervision	O
labels	O
or	O
give	O
the	O
GGS	Method
-	Method
NNs	Method
any	O
intermediate	O
annotations	O
in	O
any	O
experiments	O
.	O

While	O
simple	O
,	O
this	O
transformation	O
does	O
not	O
preserve	O
all	O
information	O
about	O
the	O
story	O
(	O
e.g.	O
,	O
it	O
discards	O
temporal	O
order	O
of	O
the	O
inputs	O
)	O
,	O
and	O
it	O
does	O
not	O
easily	O
handle	O
ternary	O
and	O
higher	O
order	O
relations	O
(	O
e.g.	O
,	O
Yesterday	O
John	O
went	O
to	O
the	O
garden	O
is	O
not	O
easily	O
mapped	O
to	O
a	O
simple	O
edge	O
)	O
.	O

We	O
also	O
emphasize	O
that	O
it	O
is	O
a	O
non	O
-	O
trivial	O
task	O
to	O
map	O
general	O
natural	O
language	O
to	O
symbolic	O
form	O
,	O
so	O
we	O
could	O
not	O
directly	O
apply	O
this	O
approach	O
to	O
arbitrary	O
natural	O
language	O
.	O

Relaxing	O
these	O
restrictions	O
is	O
left	O
for	O
future	O
work	O
.	O

However	O
,	O
even	O
with	O
this	O
simple	O
transformation	O
,	O
there	O
are	O
a	O
variety	O
of	O
bAbI	Task
tasks	Task
that	O
can	O
be	O
formulated	O
,	O
including	O
Task	O
19	O
(	O
Path	Task
Finding	Task
)	O
,	O
which	O
is	O
arguably	O
the	O
hardest	O
task	O
.	O

We	O
provide	O
baselines	O
to	O
show	O
that	O
the	O
symbolic	Method
representation	Method
does	O
not	O
help	O
RNNs	Method
or	O
LSTMs	Method
significantly	O
,	O
and	O
show	O
that	O
GGS	Method
-	Method
NNs	Method
solve	O
the	O
problem	O
with	O
a	O
small	O
number	O
of	O
training	O
instances	O
.	O

We	O
also	O
develop	O
two	O
new	O
bAbI	Task
-	Task
like	Task
tasks	Task
that	O
involve	O
outputting	O
sequences	O
on	O
graphs	O
:	O
shortest	O
paths	O
,	O
and	O
a	O
simple	O
form	O
of	O
Eulerian	Method
circuits	Method
(	O
on	O
random	O
connected	O
2	O
-	O
regular	O
graphs	O
)	O
.	O

The	O
point	O
of	O
these	O
experiments	O
is	O
to	O
illustrate	O
the	O
capabilities	O
of	O
GGS	Method
-	Method
NNs	Method
across	O
a	O
variety	O
of	O
problems	O
.	O

paragraph	O
:	O
Example	O
1	O
.	O

As	O
an	O
example	O
,	O
below	O
is	O
an	O
instance	O
from	O
the	O
symbolic	O
dataset	O
for	O
bAbI	Task
task	Task
15	O
,	O
Basic	Task
Deduction	Task
.	O

Here	O
the	O
first	O
8	O
lines	O
describe	O
the	O
facts	O
,	O
the	O
GG	Method
-	Method
NN	Method
will	O
use	O
these	O
facts	O
to	O
build	O
a	O
graph	O
.	O

Capital	O
letters	O
are	O
nodes	O
,	O
is	O
and	O
has_fear	O
are	O
interpreted	O
as	O
edge	O
labels	O
or	O
edge	O
types	O
.	O

The	O
last	O
4	O
lines	O
are	O
4	O
questions	O
asked	O
for	O
this	O
input	O
data	O
.	O

has_fear	O
in	O
these	O
lines	O
are	O
interpreted	O
as	O
a	O
question	O
type	O
.	O

For	O
this	O
task	O
,	O
in	O
each	O
question	O
only	O
one	O
node	O
is	O
special	O
,	O
e.g.	O
the	O
B	O
in	O
eval	O
B	O
has_fear	O
,	O
and	O
we	O
assign	O
a	O
single	O
value	O
1	O
to	O
the	O
annotation	O
vector	O
for	O
this	O
special	O
node	O
and	O
0	O
to	O
all	O
the	O
other	O
nodes	O
.	O

For	O
RNN	Method
and	O
LSTM	Method
the	O
data	O
is	O
converted	O
into	O
token	O
sequences	O
like	O
below	O
:	O
n6	O
e1	O
n1	O
eol	O
n6	O
e1	O
n5	O
eol	O
n1	O
e1	O
n2	O
eol	O
n4	O
e1	O
n5	O
eol	O
n3	O
e1	O
n4	O
eol	O
n3	O
e1	O
n5	O
eol	O
n6	O
e1	O
n4	O
eol	O
q1	O
n6	O
n2	O
ans	O
1	O
where	O
n	O
<	O
i	O
d	O
>	O
are	O
nodes	O
,	O
e	O
<	O
i	O
d	O
>	O
are	O
edges	O
,	O
q	O
<	O
i	O
d	O
>	O
are	O
question	O
types	O
,	O
extra	O
tokens	O
eol	O
(	O
end	O
-	O
of	O
-	O
line	O
)	O
and	O
ans	O
(	O
answer	O
)	O
are	O
added	O
to	O
give	O
the	O
RNN	Method
&	O
LSTM	Method
access	O
to	O
the	O
complete	O
information	O
available	O
in	O
the	O
dataset	O
.	O

The	O
final	O
number	O
is	O
the	O
class	O
label	O
.	O

paragraph	O
:	O
Example	O
2	O
.	O

As	O
a	O
second	O
example	O
,	O
below	O
is	O
an	O
instance	O
from	O
the	O
symbolic	O
dataset	O
for	O
bAbI	Task
task	Task
19	O
,	O
Path	Task
Finding	Task
.	O

Here	O
the	O
first	O
4	O
lines	O
describe	O
edges	O
,	O
s	O
,	O
n	O
,	O
w	O
,	O
e	O
(	O
e	O
does	O
not	O
appear	O
in	O
this	O
example	O
)	O
are	O
all	O
different	O
edge	O
types	O
.	O

The	O
last	O
line	O
is	O
a	O
path	O
question	O
,	O
the	O
answer	O
is	O
a	O
sequence	O
of	O
directions	O
w	O
,	O
s	O
,	O
as	O
the	O
path	O
going	O
from	O
B	O
to	O
A	O
is	O
to	O
first	O
go	O
west	O
to	O
E	O
then	O
go	O
south	O
to	O
A	O
.	O

The	O
s	O
,	O
n	O
,	O
w	O
,	O
e	O
in	O
the	O
question	O
lines	O
are	O
treated	O
as	O
output	O
classes	O
.	O

paragraph	O
:	O
More	O
Training	O
Details	O
.	O

For	O
all	O
tasks	O
in	O
this	O
section	O
,	O
we	O
generate	O
1000	O
training	O
examples	O
and	O
1000	O
test	O
examples	O
,	O
50	O
of	O
the	O
training	O
examples	O
are	O
used	O
for	O
validation	Task
.	O

When	O
evaluating	O
model	Task
performance	O
,	O
for	O
all	O
bAbI	Task
tasks	Task
that	O
contain	O
more	O
than	O
one	O
questions	O
in	O
one	O
example	O
,	O
the	O
predictions	O
for	O
different	O
questions	O
were	O
evaluated	O
independently	O
.	O

As	O
there	O
is	O
randomness	O
in	O
the	O
dataset	Task
generation	Task
process	Task
,	O
we	O
generated	O
10	O
such	O
datasets	O
for	O
each	O
task	O
,	O
and	O
report	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
evaluation	Metric
performance	O
across	O
the	O
10	O
datasets	O
.	O

For	O
all	O
explanatory	Task
tasks	Task
,	O
we	O
start	O
by	O
training	O
different	O
models	O
on	O
only	O
50	O
training	O
examples	O
,	O
and	O
gradually	O
increase	O
the	O
number	O
of	O
training	O
examples	O
to	O
100	O
,	O
250	O
,	O
500	O
,	O
and	O
950	O
(	O
50	O
of	O
the	O
training	O
examples	O
are	O
reserved	O
for	O
validation	Task
)	O
until	O
the	O
model	O
’s	O
test	Metric
accuracy	Metric
reaches	O
95	O
%	O
or	O
above	O
,	O
a	O
success	O
by	O
bAbI	Metric
standard	Metric
.	O

For	O
each	O
method	O
,	O
we	O
report	O
the	O
minimum	O
number	O
of	O
training	O
examples	O
it	O
needs	O
to	O
reach	O
95	O
%	O
accuracy	Metric
along	O
with	O
the	O
accuracy	Metric
it	O
reaches	O
with	O
that	O
amount	O
of	O
training	O
examples	O
.	O

In	O
all	O
these	O
cases	O
,	O
we	O
unrolled	O
the	O
propagation	Method
process	Method
for	O
5	O
steps	O
.	O

For	O
bAbI	Task
task	Task
4	O
,	O
15	O
,	O
16	O
,	O
18	O
,	O
19	O
,	O
we	O
used	O
GG	Method
-	Method
NN	Method
with	O
the	O
size	O
of	O
node	O
vectors	O
set	O
to	O
,	O
,	O
,	O
and	O
respectively	O
.	O

For	O
all	O
the	O
GGS	Method
-	Method
NNs	Method
in	O
this	O
section	O
we	O
used	O
the	O
simpler	O
variant	O
in	O
which	O
and	O
share	O
a	O
single	O
propagation	Method
model	Method
.	O

For	O
shortest	Task
path	Task
and	O
Eulerian	Task
circuit	Task
tasks	Task
,	O
we	O
used	O
.	O

All	O
models	O
are	O
trained	O
long	O
enough	O
with	O
Adam	Method
kingma2014adam	O
,	O
and	O
the	O
validation	O
set	O
is	O
used	O
to	O
choose	O
the	O
best	O
model	O
to	O
evaluate	O
and	O
avoid	O
models	O
that	O
are	O
overfitting	O
.	O

subsubsection	Method
:	O
Single	O
Step	O
Outputs	O
We	O
choose	O
four	O
bAbI	Method
tasks	Method
that	O
are	O
suited	O
to	O
the	O
restrictions	O
described	O
above	O
and	O
require	O
single	O
step	O
outputs	O
:	O
4	O
(	O
Two	O
Argument	O
Relations	O
)	O
,	O
15	O
(	O
Basic	Task
Deduction	Task
)	O
,	O
16	O
(	O
Basic	Task
Induction	Task
)	O
,	O
and	O
18	O
(	O
Size	Task
Reasoning	Task
)	O
.	O

For	O
Task	O
4	O
,	O
15	O
and	O
16	O
,	O
a	O
node	O
selection	O
GG	Method
-	Method
NN	Method
is	O
used	O
.	O

For	O
Task	O
18	O
we	O
used	O
a	O
graph	Method
-	Method
level	Method
classification	Method
version	Method
.	O

All	O
the	O
GGNN	Method
networks	Method
contain	O
less	O
than	O
600	O
parameters	O
.	O

As	O
baselines	O
,	O
we	O
train	O
RNN	Method
and	O
LSTM	Method
models	Method
on	O
the	O
symbolic	O
data	O
in	O
raw	O
sequence	O
form	O
.	O

The	O
RNNs	Method
and	O
LSTMs	Method
use	O
50	O
dimensional	Method
embeddings	Method
and	O
50	O
dimensional	Method
hidden	Method
layers	Method
;	O
they	O
predict	O
a	O
single	O
output	O
at	O
the	O
end	O
of	O
the	O
sequences	O
and	O
the	O
output	O
is	O
treated	O
as	O
a	O
classification	Task
problem	Task
,	O
the	O
loss	Metric
is	O
cross	Metric
entropy	Metric
.	O

The	O
RNNs	Method
and	O
LSTMs	Method
contain	O
around	O
5k	O
and	O
30k	O
parameters	O
,	O
respectively	O
.	O

Test	O
results	O
appear	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
all	O
tasks	O
GG	Method
-	Method
NN	Method
achieves	O
perfect	O
test	Metric
accuracy	Metric
using	O
only	O
50	O
training	O
examples	O
,	O
while	O
the	O
RNN	Method
/	O
LSTM	Method
baselines	O
either	O
use	O
more	O
training	O
examples	O
(	O
Task	O
4	O
)	O
or	O
fail	O
to	O
solve	O
the	O
tasks	O
(	O
Task	O
15	O
,	O
16	O
and	O
18	O
)	O
.	O

In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
further	O
break	O
down	O
performance	O
of	O
the	O
baselines	O
for	O
task	O
4	O
as	O
the	O
amount	O
of	O
training	O
data	O
varies	O
.	O

While	O
both	O
the	O
RNN	Method
and	O
LSTM	Method
are	O
able	O
to	O
solve	O
the	O
task	O
almost	O
perfectly	O
,	O
the	O
GG	Method
-	Method
NN	Method
reaches	O
100	O
%	O
accuracy	Metric
with	O
much	O
less	O
data	O
.	O

subsubsection	Method
:	O
Sequential	O
Outputs	O
The	O
bAbI	Task
Task	Task
19	O
(	O
Path	Task
Finding	Task
)	O
is	O
arguably	O
the	O
hardest	O
task	O
among	O
all	O
bAbI	Task
tasks	Task
(	O
see	O
e.g.	O
,	O
sukhbaatar2015end	O
,	O
which	O
reports	O
an	O
accuracy	Metric
of	O
less	O
than	O
20	O
%	O
for	O
all	O
methods	O
that	O
do	O
not	O
use	O
the	O
strong	O
supervision	O
)	O
.	O

We	O
apply	O
a	O
GGS	Method
-	Method
NN	Method
to	O
this	O
problem	O
,	O
again	O
on	O
the	O
symbolic	O
form	O
of	O
the	O
data	O
(	O
so	O
results	O
are	O
not	O
comparable	O
to	O
those	O
in	O
sukhbaatar2015end	O
)	O
.	O

An	O
extra	O
‘	O
end	O
’	O
class	O
is	O
added	O
to	O
the	O
end	O
of	O
each	O
output	O
sequence	O
;	O
at	O
test	O
time	O
the	O
network	O
will	O
keep	O
making	O
predictions	O
until	O
it	O
predicts	O
the	O
‘	O
end	O
’	O
class	O
.	O

The	O
results	O
for	O
this	O
task	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Both	O
RNN	Method
and	O
LSTM	Method
fail	O
on	O
this	O
task	O
.	O

However	O
,	O
with	O
only	O
50	O
training	O
examples	O
,	O
our	O
GGS	Method
-	Method
NNs	Method
achieve	O
much	O
better	O
test	Metric
accuracy	Metric
than	O
RNN	Method
and	O
LSTM	Method
.	O

subsection	O
:	O
Learning	Method
Graph	Method
Algorithms	Method
We	O
further	O
developed	O
two	O
new	O
bAbI	Task
-	Task
like	Task
tasks	Task
based	O
on	O
algorithmic	Task
problems	Task
on	O
graphs	O
:	O
Shortest	O
Paths	O
,	O
and	O
Eulerian	O
Circuits	O
.	O

For	O
the	O
first	O
,	O
we	O
generate	O
random	O
graphs	O
and	O
produce	O
a	O
story	O
that	O
lists	O
all	O
edges	O
in	O
the	O
graphs	O
.	O

Questions	O
come	O
from	O
choosing	O
two	O
random	O
nodes	O
and	O
and	O
asking	O
for	O
the	O
shortest	Task
path	Task
(	O
expressed	O
as	O
a	O
sequence	O
of	O
nodes	O
)	O
that	O
connects	O
the	O
two	O
chosen	O
nodes	O
.	O

We	O
constrain	O
the	O
data	Task
generation	Task
to	O
only	O
produce	O
questions	O
where	O
there	O
is	O
a	O
unique	O
shortest	Task
path	Task
from	O
to	O
of	O
length	O
at	O
least	O
2	O
.	O

For	O
Eulerian	O
circuits	O
,	O
we	O
generate	O
a	O
random	Method
two	Method
-	Method
regular	Method
connected	Method
graph	Method
and	O
a	O
separate	O
random	Method
distractor	Method
graph	Method
.	O

The	O
question	O
gives	O
two	O
nodes	O
and	O
to	O
start	O
the	O
circuit	O
,	O
then	O
the	O
question	O
is	O
to	O
return	O
the	O
Eulerian	O
circuit	O
(	O
again	O
expressed	O
as	O
a	O
sequence	O
of	O
nodes	O
)	O
on	O
the	O
given	O
subgraph	O
that	O
starts	O
by	O
going	O
from	O
to	O
.	O

Results	O
are	O
shown	O
in	O
the	O
Table	O
[	O
reference	O
]	O
.	O

RNN	Method
and	O
LSTM	Method
fail	O
on	O
both	O
tasks	O
,	O
but	O
GGS	Method
-	Method
NNs	Method
learns	O
to	O
make	O
perfect	O
predictions	O
using	O
only	O
50	O
training	O
examples	O
.	O

section	O
:	O
Program	Task
Verification	Task
with	O
GGS	Method
-	Method
NNs	Method
Our	O
work	O
on	O
GGS	Method
-	Method
NNs	Method
is	O
motivated	O
by	O
a	O
practical	O
application	O
in	O
program	Task
verification	Task
.	O

A	O
crucial	O
step	O
in	O
automatic	Task
program	Task
verification	Task
is	O
the	O
inference	Task
of	Task
program	Task
invariants	Task
,	O
which	O
approximate	O
the	O
set	O
of	O
program	O
states	O
reachable	O
in	O
an	O
execution	O
.	O

Finding	Task
invariants	Task
about	Task
data	Task
structures	Task
is	O
an	O
open	O
problem	O
.	O

As	O
an	O
example	O
,	O
consider	O
the	O
simple	O
C	O
function	O
on	O
the	O
right	O
.	O

To	O
prove	O
that	O
this	O
program	O
indeed	O
concatenates	O
the	O
two	O
lists	O
a	O
and	O
b	O
and	O
that	O
all	O
pointer	O
dereferences	O
are	O
valid	O
,	O
we	O
need	O
to	O
(	O
mathematically	O
)	O
characterize	O
the	O
program	O
’s	O
heap	O
in	O
each	O
iteration	O
of	O
the	O
loop	O
.	O

For	O
this	O
,	O
we	O
use	O
separation	Method
logic	Method
OHearn01	O
,	O
Reynolds02	O
,	O
which	O
uses	O
inductive	O
predicates	O
to	O
describe	O
abstract	O
data	O
structures	O
.	O

For	O
example	O
,	O
a	O
l	Task
ist	Task
s	Task
egment	Task
is	O
defined	O
as	O
,	O
where	O
means	O
that	O
points	O
to	O
a	O
memory	O
region	O
that	O
contains	O
a	O
structure	O
with	O
val	O
and	O
next	O
fields	O
whose	O
values	O
are	O
in	O
turn	O
and	O
.	O

The	O
connective	O
is	O
a	O
conjunction	O
as	O
in	O
Boolean	O
logic	O
,	O
but	O
additionally	O
requires	O
that	O
its	O
operators	O
refer	O
to	O
“	O
separate	O
”	O
parts	O
of	O
the	O
heap	O
.	O

Thus	O
,	O
implies	O
that	O
cur	O
is	O
either	O
NULL	O
,	O
or	O
that	O
it	O
points	O
to	O
two	O
values	O
on	O
the	O
heap	O
,	O
where	O
is	O
described	O
by	O
again	O
.	O

The	O
formula	O
is	O
an	O
invariant	O
of	O
the	O
loop	O
(	O
i.e.	O
,	O
it	O
holds	O
when	O
entering	O
the	O
loop	O
,	O
and	O
after	O
every	O
iteration	O
)	O
.	O

Using	O
it	O
,	O
we	O
can	O
prove	O
that	O
no	O
program	O
run	O
will	O
fail	O
due	O
to	O
dereferencing	O
an	O
unallocated	O
memory	O
address	O
(	O
this	O
property	O
is	O
called	O
memory	O
safety	O
)	O
and	O
that	O
the	O
function	O
indeed	O
concatenates	O
two	O
lists	O
using	O
a	O
Hoare	Method
-	Method
style	Method
verification	Method
scheme	Method
Hoare69	O
.	O

The	O
hardest	O
part	O
of	O
this	O
process	O
is	O
coming	O
up	O
with	O
formulas	O
that	O
describe	O
data	O
structures	O
,	O
and	O
this	O
is	O
where	O
we	O
propose	O
to	O
use	O
machine	Method
learning	Method
.	O

Given	O
a	O
program	O
,	O
we	O
run	O
it	O
a	O
few	O
times	O
and	O
extract	O
the	O
state	O
of	O
memory	O
(	O
represented	O
as	O
a	O
graph	O
;	O
see	O
below	O
)	O
at	O
relevant	O
program	O
locations	O
,	O
and	O
then	O
predict	O
a	O
separation	Method
logic	Method
formula	O
.	O

Static	Method
program	Method
analysis	Method
tools	Method
(	O
e.g.	O
,	O
Piskac14	Method
)	O
can	O
check	O
whether	O
a	O
candidate	O
formula	O
is	O
sufficient	O
to	O
prove	O
the	O
desired	O
properties	O
(	O
e.g.	O
,	O
memory	O
safety	O
)	O
.	O

subsection	O
:	O
Formalization	O
paragraph	O
:	O
Representing	O
Heap	O
State	O
as	O
a	O
Graph	O
As	O
inputs	O
we	O
consider	O
directed	Method
,	Method
possibly	Method
cyclic	Method
graphs	Method
representing	O
the	O
heap	O
of	O
a	O
program	O
.	O

These	O
graphs	O
can	O
be	O
automatically	O
constructed	O
from	O
a	O
program	O
’s	O
memory	O
state	O
.	O

Each	O
graph	O
node	O
corresponds	O
to	O
an	O
address	O
in	O
memory	O
at	O
which	O
a	O
sequence	O
of	O
pointers	O
is	O
stored	O
(	O
we	O
ignore	O
non	O
-	O
pointer	O
values	O
in	O
this	O
work	O
)	O
.	O

Graph	O
edges	O
reflect	O
these	O
pointer	O
values	O
,	O
i.e.	O
,	O
has	O
edges	O
labeled	O
with	O
that	O
point	O
to	O
nodes	O
,	O
respectively	O
.	O

A	O
subset	O
of	O
nodes	O
are	O
labeled	O
as	O
corresponding	O
to	O
program	O
variables	O
.	O

An	O
example	O
input	O
graph	O
is	O
displayed	O
as	O
“	O
Input	O
”	O
in	O
fig	O
:	O
annotations2	O
.	O

In	O
it	O
,	O
the	O
node	O
i	O
d	O
(	O
i.e.	O
,	O
memory	O
address	O
)	O
is	O
displayed	O
in	O
the	O
node	O
.	O

Edge	O
labels	O
correspond	O
to	O
specific	O
fields	O
in	O
the	O
program	O
,	O
e.g.	O
,	O
in	O
our	O
example	O
corresponds	O
to	O
the	O
next	O
pointer	O
in	O
our	O
example	O
function	O
from	O
the	O
previous	O
section	O
.	O

For	O
binary	O
trees	O
there	O
are	O
two	O
more	O
types	O
of	O
pointers	O
left	O
and	O
right	O
pointing	O
to	O
the	O
left	O
and	O
right	O
children	O
of	O
a	O
tree	O
node	O
.	O

paragraph	O
:	O
Output	O
Representation	O
Our	O
aim	O
is	O
to	O
mathematically	O
describe	O
the	O
shape	O
of	O
the	O
heap	O
.	O

In	O
our	O
model	O
,	O
we	O
restrict	O
ourselves	O
to	O
a	O
syntactically	O
restricted	O
version	O
of	O
separation	Method
logic	Method
,	O
in	O
which	O
formulas	O
are	O
of	O
the	O
form	O
,	O
where	O
each	O
atomic	O
formula	O
is	O
either	O
(	O
a	O
list	O
from	O
to	O
)	O
,	O
(	O
a	O
binary	O
tree	O
starting	O
in	O
)	O
,	O
or	O
(	O
no	O
data	O
structure	O
at	O
)	O
.	O

Existential	O
quantifiers	O
are	O
used	O
to	O
give	O
names	O
to	O
heap	O
nodes	O
which	O
are	O
needed	O
to	O
describe	O
a	O
shape	O
,	O
but	O
not	O
labeled	O
by	O
a	O
program	O
variable	O
.	O

For	O
example	O
,	O
to	O
describe	O
a	O
“	O
panhandle	O
list	O
”	O
(	O
a	O
list	O
that	O
ends	O
in	O
a	O
cycle	O
)	O
,	O
the	O
first	O
list	O
element	O
on	O
the	O
cycle	O
needs	O
to	O
be	O
named	O
.	O

In	O
separation	Method
logic	Method
,	O
this	O
can	O
be	O
expressed	O
as	O
.	O

paragraph	O
:	O
Data	O
We	O
can	O
generate	O
synthetic	O
(	O
labeled	O
)	O
datasets	O
for	O
this	O
problem	O
.	O

For	O
this	O
,	O
we	O
fix	O
a	O
set	O
of	O
predicates	O
such	O
as	O
and	O
(	O
extensions	O
could	O
consider	O
doubly	O
-	O
linked	O
list	O
segments	O
,	O
multi	O
-	O
trees	O
,	O
)	O
together	O
with	O
their	O
inductive	Method
definitions	Method
.	O

Then	O
we	O
enumerate	O
separation	Method
logic	Method
formulas	O
instantiating	O
our	O
predicates	O
using	O
a	O
given	O
set	O
of	O
program	O
variables	O
.	O

Finally	O
,	O
for	O
each	O
formula	O
,	O
we	O
enumerate	O
heap	O
graphs	O
satisfying	O
that	O
formula	O
.	O

The	O
result	O
is	O
a	O
dataset	O
consisting	O
of	O
pairs	O
of	O
heap	O
graphs	O
and	O
associated	O
formulas	O
that	O
are	O
used	O
by	O
our	O
learning	Method
procedures	Method
.	O

subsection	O
:	O
Formulation	O
as	O
GGS	Method
-	Method
NNs	Method
It	O
is	O
easy	O
to	O
obtain	O
the	O
node	O
annotations	O
for	O
the	O
intermediate	O
prediction	Task
steps	Task
from	O
the	O
data	Task
generation	Task
process	Task
.	O

So	O
we	O
train	O
a	O
variant	O
of	O
GGS	Method
-	Method
NN	Method
with	O
observed	O
annotations	O
(	O
observed	O
at	O
training	O
time	O
;	O
not	O
test	O
time	O
)	O
to	O
infer	O
formulas	O
from	O
heap	O
graphs	O
.	O

Note	O
that	O
it	O
is	O
also	O
possible	O
to	O
use	O
an	O
unobserved	Method
GGS	Method
-	Method
NN	Method
variant	Method
and	O
do	O
end	Task
-	Task
to	Task
-	Task
end	Task
learning	Task
.	O

The	O
procedure	O
breaks	O
down	O
the	O
production	O
of	O
a	O
separation	Method
logic	Method
formula	O
into	O
a	O
sequence	O
of	O
steps	O
.	O

We	O
first	O
decide	O
whether	O
to	O
declare	O
existential	O
variables	O
,	O
and	O
if	O
so	O
,	O
choose	O
which	O
node	O
corresponds	O
to	O
the	O
variable	O
.	O

Once	O
we	O
have	O
declared	O
existentials	O
,	O
we	O
iterate	O
over	O
all	O
variable	O
names	O
and	O
produce	O
a	O
separation	Method
logic	Method
formula	O
describing	O
the	O
data	O
structure	O
rooted	O
at	O
the	O
node	O
corresponding	O
to	O
the	O
current	O
variable	O
.	O

The	O
full	O
algorithm	O
for	O
predicting	O
separation	Method
logic	Method
formula	O
appears	O
below	O
,	O
as	O
alg	O
:	O
seplogic	Method
-	Method
prediction	Method
.	O

We	O
use	O
three	O
explicit	O
node	O
annotations	O
,	O
namely	O
is	O
-	O
named	O
(	O
heap	O
node	O
labeled	O
by	O
program	O
variable	O
or	O
declared	O
existentially	O
quantified	O
variable	O
)	O
,	O
active	O
(	O
cf	O
.	O

algorithm	O
)	O
and	O
is	O
-	O
explained	O
(	O
heap	O
node	O
is	O
part	O
of	O
data	O
structure	O
already	O
predicted	O
)	O
.	O

Initial	O
node	O
labels	O
can	O
be	O
directly	O
computed	O
from	O
the	O
input	O
graph	O
:	O
“	O
is	O
-	O
named	O
”	O
is	O
on	O
for	O
nodes	O
labeled	O
by	O
program	O
variables	O
,	O
“	O
active	O
”	O
and	O
“	O
is	O
-	O
explained	O
”	O
are	O
always	O
off	O
(	O
done	O
in	O
line	O
2	O
)	O
.	O

The	O
commented	O
lines	O
in	O
the	O
algorithm	O
are	O
implemented	O
using	O
a	O
GG	Method
-	Method
NN	Method
,	O
i.e.	O
,	O
alg	O
:	O
seplogic	Method
-	Method
prediction	Method
is	O
an	O
instance	O
of	O
our	O
GGS	Method
-	Method
NN	Method
model	Method
.	O

An	O
illustration	O
of	O
the	O
beginning	O
of	O
a	O
run	O
of	O
the	O
algorithm	O
is	O
shown	O
in	O
fig	O
:	O
annotations2	O
,	O
where	O
each	O
step	O
is	O
related	O
to	O
one	O
line	O
of	O
the	O
algorithm	O
.	O

[	O
heapgraph	O
,	O
scale=.9	O
]	O
[	O
colLabel	O
]	O
(	O
labelStep	O
)	O
at	O
(	O
0	O
,	O
0	O
)	O
Step	O
;	O
[	O
colLabel	O
]	O
(	O
labelGraph	O
)	O
at	O
(	O
)	O
Labeled	O
Graph	O
;	O
[	O
colLabel	O
]	O
(	O
labelOut	O
)	O
at	O
(	O
)	O
Out	O
p	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
0labelStep	O
)	O
at	O
(	O
)	O
Input	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
0b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
0tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
0labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0bn	O
)	O
(	O
0bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0	O
t	O
)	O
(	O
0	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
0tn	O
)	O
(	O
0tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
0	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
1labelStep	O
)	O
at	O
(	O
)	O
Line	O
3	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
1b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
1bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
1	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
1tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
1labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1bn	O
)	O
(	O
1bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1	O
t	O
)	O
(	O
1	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
1tn	O
)	O
(	O
1tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
1	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
2labelStep	O
)	O
at	O
(	O
)	O
Line	O
4	O
-	O
7	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
label=90	O
:	O
b	O
]	O
(	O
2b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
2bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
2	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
2tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
2labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2bn	O
)	O
(	O
2bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2	O
t	O
)	O
(	O
2	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
2tn	O
)	O
(	O
2tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
2	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
north	O
]	O
(	O
3labelStep	O
)	O
at	O
(	O
)	O
Line	O
10	O
(	O
for	O
b	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
3b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
3bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
3	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
3tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
3labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3bn	O
)	O
(	O
3bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3	O
t	O
)	O
(	O
3	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
3tn	O
)	O
(	O
3tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
3	O
t	O
)	O
;	O
[	O
-	O
]	O
(	O
)	O
–	O
(	O
)	O
;	O
[	O
colLabel	O
]	O
(	O
labelStep	O
’	O
)	O
at	O
(	O
)	O
Step	O
;	O
[	O
colLabel	O
]	O
(	O
labelGraph	O
’	O
)	O
at	O
(	O
)	O
Labeled	O
Graph	O
;	O
[	O
colLabel	O
]	O
(	O
labelOut	O
’	O
)	O
at	O
(	O
)	O
Out	O
p	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
4labelStep	O
)	O
at	O
(	O
)	O
Line	O
11	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
4b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
4bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
4	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
4tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
outLabel	O
]	O
(	O
4labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4bn	O
)	O
(	O
4bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4	O
t	O
)	O
(	O
4	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
4tn	O
)	O
(	O
4tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
4	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
5labelStep	O
)	O
at	O
(	O
)	O
Line	O
13	O
,	O
14	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
active	O
,	O
label=90	O
:	O
b	O
]	O
(	O
5b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
5bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
5	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
5tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
5labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5bn	O
)	O
(	O
5bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5	O
t	O
)	O
(	O
5	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
5tn	O
)	O
(	O
5tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
5	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
6labelStep	O
)	O
at	O
(	O
)	O
Line	O
18	O
/	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
explained	O
,	O
label=90	O
:	O
b	O
]	O
(	O
6b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
explained	O
]	O
(	O
6bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
label=90	O
:	O
]	O
(	O
6	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
6tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
6labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6bn	O
)	O
(	O
6bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6	O
t	O
)	O
(	O
6	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
6tn	O
)	O
(	O
6tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
6	O
t	O
)	O
;	O
[	O
stepLabel	O
,	O
anchor	O
=	O
west	O
]	O
(	O
7labelStep	O
)	O
at	O
(	O
)	O
Line	O
10	O
(	O
for	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
list	O
,	O
explained	O
,	O
label=90	O
:	O
b	O
]	O
(	O
7b	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
explained	O
]	O
(	O
7bn	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
,	O
active	O
,	O
label=90	O
:	O
]	O
(	O
7	O
t	O
)	O
at	O
(	O
)	O
;	O
[	O
list	O
]	O
(	O
7tn	O
)	O
at	O
(	O
)	O
;	O
[	O
anchor	O
=	O
west	O
,	O
align	O
=	O
left	O
,	O
outLabel	O
]	O
(	O
7labelOut	O
)	O
at	O
(	O
)	O
;	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7bn	O
)	O
(	O
7bn	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7	O
t	O
)	O
(	O
7	O
t	O
)	O
edge	O
node	O
[	O
below	O
,	O
heapEdgeLabel	O
]	O
(	O
7tn	O
)	O
(	O
7tn	O
)	O
edge	O
[	O
bend	O
right	O
]	O
node	O
[	O
above	O
,	O
heapEdgeLabel	O
]	O
(	O
7	O
t	O
)	O
;	O
[	O
1	O
]	O
Heap	O
graph	O
with	O
named	O
program	O
variables	O
compute	O
initial	O
labels	O
from	O
initialize	O
node	O
vectors	O
by	O
-	O
extending	O
quantifier	O
needed	O
Graph	O
-	O
level	O
Classification	O
(	O
†	O
)	O
fresh	O
variable	O
name	O
pick	O
node	O
Node	Method
Selection	Method
(	O
‡	O
)	O
turn	O
on	O
“	O
is	O
-	O
named	O
”	O
for	O
in	O
print	O
“	O
”	O
node	O
with	O
label	O
“	O
is	O
-	O
named	O
”	O
in	O
initialize	O
node	O
vectors	O
,	O
turn	O
on	O
“	O
active	O
”	O
label	O
for	O
in	O
pick	O
data	O
structure	O
predicate	O
Graph	O
-	O
level	O
Classification	Task
(	O
⋆	O
)	O
pick	O
list	O
end	O
node	O
Node	Task
Selection	Task
(	O
♡	O
)	O
print	O
“	O
”	O
print	O
“	O
”	O
update	O
node	O
annotations	O
in	O
Node	O
Annotation	O
(	O
♠	O
)	O
Separation	Method
logic	Method
formula	Method
prediction	Method
procedure	Method
subsection	O
:	O
Model	O
Setup	O
Details	O
We	O
use	O
the	O
full	O
GGS	Method
-	Method
NN	Method
model	Method
where	O
and	O
have	O
separate	O
propagation	Method
models	Method
.	O

For	O
all	O
the	O
GG	Method
-	Method
NN	Method
components	O
in	O
the	O
GGS	Method
-	Method
NN	Method
pipeline	Method
,	O
we	O
unrolled	O
the	O
propagation	Method
process	Method
for	O
10	O
time	O
steps	O
.	O

The	O
GGS	Method
-	Method
NNs	Method
associated	O
with	O
step	O
(	O
)	O
(	O
deciding	O
wheter	O
more	O
existentially	O
quantified	O
variable	O
need	O
to	O
be	O
declared	O
)	O
and	O
(	O
)	O
(	O
identify	O
which	O
node	O
need	O
to	O
be	O
declared	O
as	O
existentially	O
quantified	O
)	O
uses	O
dimensional	Method
node	Method
representations	Method
.	O

For	O
all	O
other	O
GGS	Method
-	Method
NN	Method
components	Method
,	O
is	O
used	O
.	O

Adam	Method
kingma2014adam	O
is	O
used	O
for	O
optimization	Task
,	O
the	O
models	O
are	O
trained	O
on	O
minibatches	O
of	O
20	O
graphs	O
,	O
and	O
optimized	O
until	O
training	Metric
error	Metric
is	O
very	O
low	O
.	O

For	O
the	O
graph	Task
-	Task
level	Task
classification	Task
tasks	Task
,	O
we	O
also	O
artificially	O
balanced	O
classes	O
to	O
have	O
even	O
number	O
of	O
examples	O
from	O
each	O
class	O
in	O
each	O
minibatch	O
.	O

All	O
the	O
GGS	Method
-	Method
NN	Method
components	Method
contain	O
less	O
than	O
5k	O
parameters	O
and	O
no	O
overfitting	O
is	O
observed	O
during	O
training	O
.	O

subsection	O
:	O
Batch	Task
Prediction	Task
Details	O
In	O
practice	O
,	O
a	O
set	O
of	O
heap	O
graphs	O
will	O
be	O
given	O
as	O
input	O
and	O
a	O
single	O
output	O
formula	O
is	O
expected	O
to	O
describe	O
and	O
be	O
consistent	O
with	O
all	O
the	O
input	O
graphs	O
.	O

The	O
different	O
heap	O
graphs	O
can	O
be	O
snapshots	O
of	O
the	O
heap	O
state	O
at	O
different	O
points	O
in	O
the	O
program	O
execution	O
process	O
,	O
or	O
different	O
runs	O
of	O
the	O
same	O
program	O
with	O
different	O
inputs	O
.	O

We	O
call	O
this	O
the	O
“	O
batch	Task
prediction	Task
”	Task
setup	Task
contrasting	O
with	O
the	O
single	Method
graph	Method
prediction	Method
described	O
in	O
the	O
main	O
paper	O
.	O

To	O
make	O
batch	Task
predictions	Task
,	O
we	O
run	O
one	O
GGS	Method
-	Method
NN	Method
for	O
each	O
graph	O
simultaneously	O
.	O

For	O
each	O
prediction	O
step	O
,	O
the	O
outputs	O
of	O
all	O
the	O
GGS	Method
-	Method
NNs	Method
at	O
that	O
step	O
across	O
the	O
batch	O
of	O
graphs	O
are	O
aggregated	O
.	O

For	O
node	Task
selection	Task
outputs	Task
,	O
the	O
common	O
named	O
variables	O
link	O
nodes	O
on	O
different	O
graphs	O
togeter	O
,	O
which	O
is	O
the	O
key	O
for	O
aggregating	Task
predictions	Task
in	O
a	O
batch	O
.	O

We	O
compute	O
the	O
score	O
for	O
a	O
particular	O
named	O
variable	O
as	O
,	O
where	O
maps	O
variable	O
name	O
to	O
a	O
node	O
in	O
graph	O
,	O
and	O
is	O
the	O
output	O
score	O
for	O
named	O
variable	O
in	O
graph	O
.	O

When	O
applying	O
a	O
softmax	O
over	O
all	O
names	O
using	O
as	O
scores	O
,	O
this	O
is	O
equivalent	O
to	O
a	O
model	O
that	O
computes	O
.	O

For	O
graph	Task
-	Task
level	Task
classification	Task
outputs	Task
,	O
we	O
add	O
up	O
scores	O
of	O
a	O
particular	O
class	O
across	O
the	O
batch	O
of	O
graphs	O
,	O
or	O
equivalently	O
compute	O
.	O

Node	O
annotation	O
outputs	O
are	O
updated	O
for	O
each	O
graph	O
independently	O
as	O
different	O
graphs	O
have	O
completely	O
different	O
set	O
of	O
nodes	O
.	O

However	O
,	O
when	O
the	O
algorithm	O
tries	O
to	O
update	O
the	O
annotation	O
for	O
one	O
named	O
variable	O
,	O
the	O
nodes	O
associated	O
with	O
that	O
variable	O
in	O
all	O
graphs	O
are	O
updated	O
.	O

During	O
training	O
,	O
all	O
labels	O
for	O
intermediate	O
steps	O
are	O
available	O
to	O
us	O
from	O
the	O
data	Task
generation	Task
process	Task
,	O
so	O
the	O
training	Task
process	Task
again	O
can	O
be	O
decomposed	O
to	O
single	Task
output	Task
single	Task
graph	Task
training	Task
.	O

A	O
more	O
complex	O
scenario	O
allowing	O
for	O
nested	O
data	O
structures	O
(	O
e.g.	O
,	O
list	O
of	O
lists	O
)	O
was	O
discussed	O
in	O
.	O

We	O
have	O
also	O
successfully	O
extended	O
the	O
GGS	Method
-	Method
NN	Method
model	Method
to	O
this	O
case	O
.	O

More	O
details	O
on	O
this	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Experiments	O
.	O

For	O
this	O
paper	O
,	O
we	O
produced	O
a	O
dataset	O
of	O
327	O
formulas	O
that	O
involves	O
three	O
program	O
variables	O
,	O
with	O
498	O
graphs	O
per	O
formula	O
,	O
yielding	O
around	O
160	O
,	O
000	O
formula	O
/	O
heap	O
graph	O
combinations	O
.	O

To	O
evaluate	O
,	O
we	O
split	O
the	O
data	O
into	O
training	O
,	O
validation	O
and	O
test	O
sets	O
using	O
a	O
6:2:2	O
split	O
on	O
the	O
formulas	O
(	O
i.e.	O
,	O
the	O
formulas	O
in	O
the	O
test	O
set	O
were	O
not	O
in	O
the	O
training	O
set	O
)	O
.	O

We	O
measure	O
correctness	Metric
by	O
whether	O
the	O
formula	O
predicted	O
at	O
test	O
time	O
is	O
logically	O
equivalent	O
to	O
the	O
ground	O
truth	O
;	O
equivalence	O
is	O
approximated	O
by	O
canonicalizing	O
names	O
and	O
order	O
of	O
the	O
formulas	O
and	O
then	O
comparing	O
for	O
exact	O
equality	O
.	O

We	O
compared	O
our	O
GGS	Method
-	Method
NN	Method
-	Method
based	Method
model	Method
with	O
a	O
method	O
we	O
developed	O
earlier	O
brockschmidt2015learning	O
.	O

The	O
earlier	O
approach	O
treats	O
each	O
prediction	O
step	O
as	O
standard	O
classification	Task
,	O
and	O
requires	O
complex	O
,	O
manual	Task
,	Task
problem	Task
-	Task
specific	Task
feature	Task
engineering	Task
,	O
to	O
achieve	O
an	O
accuracy	Metric
of	O
89.11	O
%	O
.	O

In	O
contrast	O
,	O
our	O
new	O
model	O
was	O
trained	O
with	O
no	O
feature	Method
engineering	Method
and	O
very	O
little	O
domain	O
knowledge	O
and	O
achieved	O
an	O
accuracy	Metric
of	O
89.96	O
%	O
.	O

An	O
example	O
heap	O
graph	O
and	O
the	O
corresponding	O
separation	Method
logic	Method
formula	O
found	O
by	O
our	O
GGS	Method
-	Method
NN	Method
model	Method
is	O
shown	O
in	O
fig	O
:	O
appendix	O
-	O
heap	O
-	O
graph	O
-	O
example	O
.	O

This	O
example	O
also	O
involves	O
nested	O
data	O
structures	O
and	O
the	O
batching	Method
extension	Method
developed	O
in	O
the	O
previous	O
section	O
.	O

We	O
have	O
also	O
successfully	O
used	O
our	O
new	O
model	O
in	O
a	O
program	Method
verification	Method
framework	Method
,	O
supplying	O
needed	O
program	O
invariants	O
to	O
a	O
theorem	Method
prover	Method
to	O
prove	O
correctness	O
of	O
a	O
collection	O
of	O
list	Method
-	Method
manipulating	Method
algorithms	Method
such	O
as	O
insertion	Method
sort	Method
.	O

The	O
following	O
Table	O
[	O
reference	O
]	O
lists	O
a	O
set	O
of	O
benchmark	O
list	O
manipulation	O
programs	O
and	O
the	O
separation	Method
logic	Method
formula	O
invariants	O
found	O
by	O
the	O
GGS	Method
-	Method
NN	Method
model	Method
,	O
which	O
were	O
successfully	O
used	O
in	O
a	O
verification	Method
framework	Method
to	O
prove	O
the	O
correctness	O
of	O
corresponding	O
programs	O
.	O

A	O
further	O
extension	O
of	O
the	O
current	O
pipeline	O
has	O
been	O
shown	O
to	O
be	O
able	O
to	O
successfully	O
prove	O
more	O
sophisticated	O
programs	O
like	O
sorting	Method
programs	Method
and	O
various	O
other	O
list	Task
-	Task
manipulating	Task
programs	Task
.	O

section	O
:	O
Related	O
Work	O
The	O
most	O
closely	O
related	O
work	O
is	O
GNNs	Method
,	O
which	O
we	O
have	O
discussed	O
at	O
length	O
above	O
.	O

proposed	O
another	O
closely	O
related	O
model	O
that	O
differs	O
from	O
GNNs	Method
mainly	O
in	O
the	O
output	Method
model	Method
.	O

GNNs	Method
have	O
been	O
applied	O
in	O
several	O
domains	O
gori2005new	O
,	O
di2006comparison	O
,	O
scarselli2009graph	O
,	O
uwents2011neural	O
,	O
but	O
they	O
do	O
not	O
appear	O
to	O
be	O
in	O
widespread	O
use	O
in	O
the	O
ICLR	Task
community	Task
.	O

Part	O
of	O
our	O
aim	O
here	O
is	O
to	O
publicize	O
GNNs	Method
as	O
a	O
useful	O
and	O
interesting	O
neural	Method
network	Method
variant	Method
.	O

An	O
analogy	O
can	O
be	O
drawn	O
between	O
our	O
adaptation	O
from	O
GNNs	Method
to	O
GG	Method
-	Method
NNs	Method
,	O
to	O
the	O
work	O
of	O
and	O
in	O
the	O
structured	Task
prediction	Task
setting	Task
.	O

There	O
belief	Method
propagation	Method
(	O
which	O
must	O
be	O
run	O
to	O
near	O
convergence	O
to	O
get	O
good	O
gradients	O
)	O
is	O
replaced	O
with	O
truncated	Method
belief	Method
propagation	Method
updates	Method
,	O
and	O
then	O
the	O
model	O
is	O
trained	O
so	O
that	O
the	O
truncated	O
iteration	O
produce	O
good	O
results	O
after	O
a	O
fixed	O
number	O
of	O
iterations	O
.	O

Similarly	O
,	O
Recursive	Method
Neural	Method
Networks	Method
goller1996learning	O
,	O
socher2011parsing	O
being	O
extended	O
to	O
Tree	Method
LSTMs	Method
tai2015improved	O
is	O
analogous	O
to	O
our	O
using	O
of	O
GRU	Method
updates	Method
in	O
GG	Method
-	Method
NNs	Method
instead	O
of	O
the	O
standard	O
GNN	Method
recurrence	Method
with	O
the	O
aim	O
of	O
improving	O
the	O
long	Task
-	Task
term	Task
propagation	Task
of	Task
information	Task
across	O
a	O
graph	O
structure	O
.	O

The	O
general	O
idea	O
expressed	O
in	O
this	O
paper	O
of	O
assembling	O
problem	Method
-	Method
specific	Method
neural	Method
networks	Method
as	O
a	O
composition	O
of	O
learned	Method
components	Method
has	O
a	O
long	O
history	O
,	O
dating	O
back	O
at	O
least	O
to	O
the	O
work	O
of	O
on	O
assembling	O
neural	Method
networks	Method
according	O
to	O
a	O
family	O
tree	O
structure	O
in	O
order	O
to	O
predict	O
relations	O
between	O
people	O
.	O

Similar	O
ideas	O
appear	O
in	O
and	O
.	O

Graph	Method
kernels	Method
shervashidze2011weisfeiler	O
,	O
kashima2003marginalized	O
can	O
be	O
used	O
for	O
a	O
variety	O
of	O
kernel	Task
-	Task
based	Task
learning	Task
tasks	Task
with	O
graph	O
-	O
structured	O
inputs	O
,	O
but	O
we	O
are	O
not	O
aware	O
of	O
work	O
that	O
learns	O
the	O
kernels	O
and	O
outputs	O
sequences	O
.	O

convert	O
graphs	O
into	O
sequences	O
by	O
following	O
random	O
walks	O
on	O
the	O
graph	O
then	O
learns	O
node	Method
embeddings	Method
using	O
sequence	Method
-	Method
based	Method
methods	Method
.	O

map	O
graphs	O
to	O
graph	O
vectors	O
then	O
classify	O
using	O
an	O
output	Method
neural	Method
network	Method
.	O

There	O
are	O
several	O
models	O
that	O
make	O
use	O
of	O
similar	Method
propagation	Method
of	Method
node	Method
representations	Method
on	O
a	O
graph	O
structure	O
.	O

generalize	O
convolutions	Method
to	O
graph	O
structures	O
.	O

The	O
difference	O
between	O
their	O
work	O
and	O
GNNs	Method
is	O
analogous	O
to	O
the	O
difference	O
between	O
convolutional	Method
and	Method
recurrent	Method
networks	Method
.	O

also	O
consider	O
convolutional	Method
like	Method
operations	Method
on	O
graphs	O
,	O
building	O
a	O
learnable	O
,	O
differentiable	Method
variant	Method
of	O
a	O
successful	O
graph	O
feature	O
.	O

converts	O
an	O
arbitrary	O
undirected	O
graph	O
to	O
a	O
number	O
of	O
different	O
DAGs	O
with	O
different	O
orientations	O
and	O
then	O
propagates	O
node	O
representations	O
inwards	O
towards	O
each	O
root	O
,	O
training	O
an	O
ensemble	O
of	O
models	O
.	O

In	O
all	O
of	O
the	O
above	O
,	O
the	O
focus	O
is	O
on	O
one	Task
-	Task
step	Task
problems	Task
.	O

GNNs	Method
and	O
our	O
extensions	O
have	O
many	O
of	O
the	O
same	O
desirable	O
properties	O
of	O
pointer	Method
networks	Method
vinyals2015pointer	O
;	O
when	O
using	O
node	Method
selection	Method
output	Method
layers	Method
,	O
nodes	O
from	O
the	O
input	O
can	O
be	O
chosen	O
as	O
outputs	O
.	O

There	O
are	O
two	O
main	O
differences	O
:	O
first	O
,	O
in	O
GNNs	Method
the	O
graph	O
structure	O
is	O
explicit	O
,	O
which	O
makes	O
the	O
models	O
less	O
general	O
but	O
may	O
provide	O
stronger	O
generalization	Metric
ability	Metric
;	O
second	O
,	O
pointer	Method
networks	Method
require	O
that	O
each	O
node	O
has	O
properties	O
(	O
e.g.	O
,	O
a	O
location	O
in	O
space	O
)	O
,	O
while	O
GNNs	Method
can	O
represent	O
nodes	O
that	O
are	O
defined	O
only	O
by	O
their	O
position	O
in	O
the	O
graph	O
,	O
which	O
makes	O
them	O
more	O
general	O
along	O
a	O
different	O
dimension	O
.	O

GGS	Method
-	Method
NNs	Method
are	O
related	O
to	O
soft	Method
alignment	Method
and	Method
attentional	Method
models	Method
(	O
e.g.	O
,	O
)	O
in	O
two	O
respects	O
:	O
first	O
,	O
the	O
graph	Method
representation	Method
in	O
(	O
[	O
reference	O
]	O
)	O
uses	O
context	O
to	O
focus	O
attention	O
on	O
which	O
nodes	O
are	O
important	O
to	O
the	O
current	O
decision	O
;	O
second	O
,	O
node	O
annotations	O
in	O
the	O
program	O
verification	O
example	O
keep	O
track	O
of	O
which	O
nodes	O
have	O
been	O
explained	O
so	O
far	O
,	O
which	O
gives	O
an	O
explicit	O
mechanism	O
for	O
making	O
sure	O
that	O
each	O
node	O
in	O
the	O
input	O
has	O
been	O
used	O
over	O
the	O
sequence	O
of	O
producing	O
an	O
output	O
.	O

section	O
:	O
Discussion	O
paragraph	O
:	O
What	O
is	O
being	O
learned	O
?	O
It	O
is	O
instructive	O
to	O
consider	O
what	O
is	O
being	O
learned	O
by	O
the	O
GG	Method
-	Method
NNs	Method
.	O

To	O
do	O
so	O
,	O
we	O
can	O
draw	O
analogy	O
between	O
how	O
the	O
bAbI	Task
task	Task
15	O
would	O
be	O
solved	O
via	O
a	O
logical	Method
formulation	Method
.	O

As	O
an	O
example	O
,	O
consider	O
the	O
subset	O
of	O
lines	O
needed	O
to	O
answer	O
one	O
example	O
on	O
the	O
right	O
.	O

To	O
do	O
logical	Task
reasoning	Task
,	O
we	O
would	O
need	O
not	O
only	O
a	O
logical	Method
encoding	Method
of	O
the	O
facts	O
present	O
in	O
the	O
story	O
but	O
also	O
the	O
background	O
world	O
knowledge	O
encoded	O
as	O
inference	O
rules	O
such	O
as	O
Our	O
encoding	O
of	O
the	O
tasks	O
simplifies	O
the	O
parsing	O
of	O
the	O
story	O
into	O
graph	O
form	O
,	O
but	O
it	O
does	O
not	O
provide	O
any	O
of	O
the	O
background	O
knowledge	O
.	O

The	O
GG	Method
-	Method
NN	Method
model	O
can	O
be	O
seen	O
as	O
learning	O
this	O
,	O
with	O
results	O
stored	O
in	O
the	O
neural	O
network	O
weights	O
.	O

To	O
do	O
logical	Task
reasoning	Task
,	O
we	O
would	O
need	O
not	O
only	O
a	O
logical	Method
encoding	Method
of	O
the	O
facts	O
present	O
in	O
the	O
story	O
but	O
also	O
the	O
background	O
world	O
knowledge	O
encoded	O
as	O
inference	O
rules	O
such	O
as	O
Our	O
encoding	O
of	O
the	O
tasks	O
simplifies	O
the	O
parsing	Task
of	Task
the	Task
story	Task
into	O
graph	O
form	O
,	O
but	O
it	O
does	O
not	O
provide	O
any	O
of	O
the	O
background	O
world	O
knowledge	O
.	O

Thus	O
,	O
the	O
GG	Method
-	Method
NN	Method
model	O
can	O
be	O
seen	O
as	O
learning	O
the	O
background	O
world	O
knowledge	O
,	O
with	O
results	O
stored	O
in	O
the	O
neural	Method
network	Method
weights	Method
.	O

paragraph	O
:	O
Discussion	O
The	O
results	O
in	O
the	O
paper	O
show	O
that	O
GGS	Method
-	Method
NNs	Method
have	O
desirable	O
inductive	O
biases	O
across	O
a	O
range	O
of	O
problems	O
that	O
have	O
some	O
intrinsic	O
graph	O
structure	O
to	O
them	O
,	O
and	O
we	O
believe	O
there	O
to	O
be	O
many	O
more	O
cases	O
where	O
GGS	Method
-	Method
NNs	Method
will	O
be	O
useful	O
.	O

There	O
are	O
,	O
however	O
,	O
some	O
limitations	O
that	O
need	O
to	O
be	O
overcome	O
to	O
make	O
them	O
apply	O
even	O
more	O
broadly	O
.	O

Two	O
limitations	O
that	O
we	O
mentioned	O
previously	O
are	O
that	O
the	O
bAbI	Task
task	Task
translation	O
does	O
not	O
incorporate	O
temporal	O
order	O
of	O
inputs	O
or	O
ternary	O
and	O
higher	O
order	O
relations	O
.	O

We	O
can	O
imagine	O
several	O
possibilities	O
for	O
lifting	O
these	O
restrictions	O
,	O
such	O
as	O
concatenating	O
a	O
series	O
of	O
GG	Method
-	Method
NNs	Method
,	O
where	O
there	O
is	O
one	O
GG	Method
-	Method
NNs	Method
for	O
each	O
edge	O
,	O
and	O
representing	O
higher	O
order	O
relations	O
as	O
factor	O
graphs	O
.	O

A	O
more	O
significant	O
challenge	O
is	O
how	O
to	O
handle	O
less	O
structured	O
input	O
representations	O
.	O

For	O
example	O
,	O
in	O
the	O
bAbI	Task
tasks	Task
it	O
would	O
be	O
desirable	O
not	O
to	O
use	O
the	O
symbolic	O
form	O
of	O
the	O
inputs	O
.	O

One	O
possible	O
approach	O
is	O
to	O
incorporate	O
less	O
structured	O
inputs	O
,	O
and	O
latent	O
vectors	O
,	O
in	O
our	O
GGS	Method
-	Method
NNs	Method
.	O

However	O
,	O
experimentation	O
is	O
needed	O
to	O
find	O
the	O
best	O
way	O
of	O
addressing	O
these	O
issues	O
.	O

The	O
current	O
GGS	Method
-	Method
NNs	Method
formulation	O
specifies	O
a	O
question	O
only	O
after	O
all	O
the	O
facts	O
have	O
been	O
consumed	O
.	O

This	O
implies	O
that	O
the	O
network	O
must	O
try	O
to	O
derive	O
all	O
consequences	O
of	O
the	O
seen	O
facts	O
and	O
store	O
all	O
pertinent	O
information	O
to	O
a	O
node	O
within	O
its	O
node	Method
representation	Method
.	O

This	O
is	O
likely	O
not	O
ideal	O
;	O
it	O
would	O
be	O
preferable	O
to	O
develop	O
methods	O
that	O
take	O
the	O
question	O
as	O
an	O
initial	O
input	O
,	O
and	O
then	O
dynamically	O
derive	O
the	O
facts	O
needed	O
to	O
answer	O
the	O
question	O
.	O

We	O
are	O
optimistic	O
about	O
the	O
further	O
applications	O
of	O
GGS	Method
-	Method
NNs	Method
.	O

We	O
are	O
particularly	O
interested	O
in	O
continuing	O
to	O
develop	O
end	O
-	O
to	O
-	O
end	O
learnable	Method
systems	Method
that	O
can	O
learn	O
about	O
semantic	O
properties	O
of	O
programs	O
,	O
that	O
can	O
learn	O
more	O
complicated	O
graph	Method
algorithms	Method
,	O
and	O
in	O
applying	O
these	O
ideas	O
to	O
problems	O
that	O
require	O
reasoning	O
over	O
knowledge	O
bases	O
and	O
databases	O
.	O

More	O
generally	O
,	O
we	O
consider	O
these	O
graph	Method
neural	Method
networks	Method
as	O
representing	O
a	O
step	O
towards	O
a	O
model	O
that	O
can	O
combine	O
structured	Method
representations	Method
with	O
the	O
powerful	O
algorithms	O
of	O
deep	Method
learning	Method
,	O
with	O
the	O
aim	O
of	O
taking	O
advantage	O
of	O
known	O
structure	O
while	O
learning	O
and	O
inferring	O
how	O
to	O
reason	O
with	O
and	O
extend	O
these	O
representations	O
.	O

section	O
:	O
Acknowledgements	O
We	O
thank	O
Siddharth	O
Krishna	O
,	O
Alex	O
Gaunt	O
,	O
Emine	O
Yilmaz	O
,	O
Milad	O
Shokouhi	O
,	O
and	O
Pushmeet	O
Kohli	O
for	O
useful	O
conversations	O
and	O
Douwe	O
Kiela	O
for	O
comments	O
on	O
an	O
earlier	O
draft	O
of	O
the	O
paper	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Contraction	Method
Map	Method
Example	O
Consider	O
a	O
linear	Method
1	Method
-	Method
hidden	Method
unit	Method
cycle	Method
-	Method
structured	Method
GNN	Method
with	Method
nodes	Method
.	O

For	O
simplicity	O
we	O
ignored	O
all	O
edge	O
labels	O
and	O
node	O
labels	O
,	O
equivalently	O
this	O
is	O
a	O
simple	O
example	O
with	O
and	O
.	O

At	O
each	O
timestep	O
we	O
update	O
hidden	O
states	O
as	O
for	O
each	O
,	O
where	O
and	O
are	O
parameters	O
of	O
the	O
propagation	Method
model	Method
.	O

We	O
use	O
the	O
convention	O
that	O
cycles	O
around	O
and	O
refers	O
to	O
when	O
.	O

Let	O
,	O
and	O
.	O

We	O
can	O
write	O
the	O
joint	Method
update	Method
for	O
all	O
as	O
Restrict	O
the	O
update	O
to	O
define	O
a	O
contraction	O
mapping	O
in	O
the	O
Euclidean	O
metric	O
.	O

This	O
means	O
that	O
there	O
is	O
some	O
such	O
that	O
for	O
any	O
,	O
or	O
in	O
other	O
words	O
,	O
We	O
can	O
immediately	O
see	O
that	O
this	O
implies	O
that	O
for	O
each	O
by	O
letting	O
be	O
the	O
elementary	O
vector	O
that	O
is	O
all	O
zero	O
except	O
for	O
a	O
1	O
in	O
position	O
and	O
letting	O
be	O
the	O
all	O
zeros	O
vector	O
.	O

Expanding	O
Eq	O
.	O

[	O
reference	O
]	O
,	O
we	O
get	O
In	O
the	O
GNN	Method
model	Method
,	O
node	O
label	O
controls	O
which	O
values	O
of	O
and	O
are	O
used	O
during	O
the	O
propagation	Task
.	O

Looking	O
at	O
this	O
expansion	O
and	O
noting	O
that	O
for	O
all	O
,	O
we	O
see	O
that	O
information	O
about	O
labels	O
of	O
nodes	O
away	O
will	O
decay	O
at	O
a	O
rate	O
of	O
.	O

Thus	O
,	O
at	O
least	O
in	O
this	O
simple	O
case	O
,	O
the	O
restriction	O
that	O
be	O
a	O
contraction	O
means	O
that	O
it	O
is	O
not	O
able	O
to	O
maintain	O
long	O
-	O
range	O
dependencies	O
.	O

subsection	O
:	O
Nonlinear	Task
Case	Task
The	O
same	O
analysis	O
can	O
be	O
applied	O
to	O
a	O
nonlinear	Task
update	Task
,	O
i.e.	O
where	O
is	O
any	O
nonlinear	O
function	O
.	O

Then	O
.	O

Let	O
,	O
where	O
.	O

The	O
contraction	O
map	O
definition	O
Eq	O
.	O

[	O
reference	O
]	O
implies	O
that	O
each	O
entry	O
of	O
the	O
Jacobian	O
matrix	O
of	O
is	O
bounded	O
by	O
,	O
i.e.	O
To	O
see	O
this	O
,	O
consider	O
two	O
vectors	O
and	O
,	O
where	O
and	O
.	O

The	O
definition	O
in	O
(	O
[	O
reference	O
]	O
)	O
implies	O
that	O
for	O
all	O
,	O
Therefore	O
where	O
the	O
left	O
hand	O
side	O
is	O
by	O
definition	O
as	O
.	O

When	O
,	O
Also	O
,	O
because	O
of	O
the	O
special	O
cycle	O
graph	O
structure	O
,	O
for	O
all	O
other	O
s	O
we	O
have	O
.	O

Applying	O
this	O
to	O
the	O
update	O
at	O
timestep	O
,	O
we	O
get	O
Now	O
let	O
’s	O
see	O
how	O
a	O
change	O
in	O
could	O
affect	O
.	O

Using	O
the	O
chain	O
rule	O
and	O
the	O
special	O
graph	O
structure	O
,	O
we	O
have	O
As	O
,	O
this	O
derivative	O
will	O
approach	O
0	O
exponentially	O
fast	O
as	O
grows	O
.	O

Intuitively	O
,	O
this	O
means	O
that	O
the	O
impact	O
one	O
node	O
has	O
on	O
another	O
node	O
far	O
away	O
will	O
decay	O
exponetially	O
,	O
therefore	O
making	O
it	O
difficult	O
to	O
model	O
long	O
range	O
dependencies	O
.	O

appendix	O
:	O
Why	O
are	O
RNN	Method
and	O
LSTM	Method
so	O
Bad	O
on	O
the	O
Sequence	Task
Prediction	Task
Tasks	Task
?	O
RNN	Method
and	O
LSTM	Method
performance	O
on	O
the	O
sequence	Task
prediction	Task
tasks	Task
,	O
i.e.	O
bAbI	Task
task	Task
19	O
,	O
shortest	Task
path	Task
and	O
Eulerian	O
circuit	O
,	O
are	O
very	O
poor	O
compared	O
to	O
single	Task
output	Task
tasks	Task
.	O

The	O
Eulerian	Task
circuit	Task
task	Task
is	O
the	O
one	O
that	O
RNN	Method
and	O
LSTM	Method
fail	O
most	O
dramatically	O
.	O

A	O
typical	O
training	O
example	O
for	O
this	O
task	O
looks	O
like	O
the	O
following	O
,	O
This	O
describes	O
a	O
graph	O
with	O
two	O
cycles	O
3	O
-	O
7	O
-	O
5	O
-	O
8	O
-	O
6	O
and	O
1	O
-	O
2	O
-	O
4	O
-	O
0	O
,	O
where	O
3	O
-	O
7	O
-	O
5	O
-	O
8	O
-	O
6	O
is	O
the	O
target	O
cycle	O
and	O
1	O
-	O
2	O
-	O
4	O
-	O
0	O
is	O
a	O
smaller	O
distractor	O
graph	O
.	O

All	O
edges	O
are	O
presented	O
twice	O
in	O
both	O
directions	O
for	O
symmetry	O
.	O

The	O
task	O
is	O
to	O
find	O
the	O
cycle	O
that	O
starts	O
with	O
the	O
given	O
two	O
nodes	O
and	O
in	O
the	O
direction	O
from	O
the	O
first	O
to	O
the	O
second	O
.	O

The	O
distractor	O
graph	O
is	O
added	O
to	O
increase	O
the	O
difficulty	O
of	O
this	O
task	O
,	O
this	O
also	O
makes	O
the	O
output	O
cycle	O
not	O
strictly	O
“	O
Eulerian	O
”	O
.	O

For	O
RNN	Method
and	O
LSTM	Method
the	O
above	O
training	O
example	O
is	O
further	O
transformed	O
into	O
a	O
sequence	O
of	O
tokens	O
,	O
Note	O
the	O
node	O
IDs	O
here	O
are	O
different	O
from	O
the	O
ones	O
in	O
the	O
original	O
symbolic	O
data	O
.	O

The	O
RNN	Method
and	O
LSTM	Method
read	O
through	O
the	O
whole	O
sequence	O
,	O
and	O
start	O
to	O
predict	O
the	O
first	O
output	O
when	O
reading	O
the	O
ans	O
token	O
.	O

Then	O
for	O
each	O
prediction	O
step	O
,	O
the	O
ans	O
token	O
is	O
fed	O
as	O
the	O
input	O
and	O
the	O
target	O
node	O
ID	O
(	O
treated	O
as	O
a	O
class	O
label	O
)	O
is	O
expected	O
as	O
the	O
output	O
.	O

In	O
this	O
current	O
setup	O
,	O
the	O
output	O
of	O
each	O
prediction	O
step	O
is	O
not	O
fed	O
as	O
the	O
input	O
for	O
the	O
next	O
.	O

Our	O
GGS	Method
-	Method
NN	Method
model	Method
uses	O
the	O
same	O
setup	O
,	O
where	O
the	O
output	O
of	O
one	O
step	O
is	O
not	O
used	O
as	O
input	O
to	O
the	O
next	O
,	O
only	O
the	O
predicted	O
node	O
annotations	O
carry	O
over	O
from	O
one	O
step	O
to	O
the	O
next	O
,	O
so	O
the	O
comparison	O
is	O
still	O
fair	O
for	O
RNN	Method
and	O
LSTM	Method
.	O

Changing	O
both	O
our	O
method	O
and	O
the	O
baselines	O
to	O
make	O
use	O
of	O
previous	O
predictions	O
is	O
left	O
as	O
future	O
work	O
.	O

From	O
this	O
example	O
we	O
can	O
see	O
that	O
the	O
sequences	O
the	O
RNN	Method
and	O
LSTM	Method
have	O
to	O
handle	O
is	O
quite	O
long	O
,	O
close	O
to	O
80	O
tokens	O
before	O
the	O
predictions	O
are	O
made	O
.	O

Some	O
predictions	O
really	O
depend	O
on	O
long	O
range	O
memory	O
,	O
for	O
example	O
the	O
first	O
edge	O
(	O
3	O
-	O
7	O
)	O
and	O
first	O
a	O
few	O
tokens	O
(	O
n4	O
e1	O
n8	O
)	O
in	O
the	O
sequence	O
are	O
needed	O
to	O
make	O
prediction	O
in	O
the	O
third	O
prediction	O
step	O
(	O
3	O
in	O
the	O
original	O
symbolic	O
data	O
,	O
and	O
4	O
in	O
the	O
tokenized	O
RNN	Method
data	O
)	O
.	O

Keeping	O
long	O
range	O
memory	O
in	O
RNNs	Method
is	O
challenging	O
,	O
LSTMs	Method
do	O
better	O
than	O
RNNs	Method
but	O
still	O
ca	O
n’t	O
completely	O
solve	O
the	O
problem	O
.	O

Another	O
challenge	O
about	O
this	O
task	O
is	O
the	O
output	O
sequence	O
does	O
not	O
appear	O
in	O
the	O
same	O
order	O
as	O
in	O
the	O
input	O
sequence	O
.	O

In	O
fact	O
,	O
the	O
data	O
has	O
no	O
sequential	O
nature	O
at	O
all	O
,	O
even	O
when	O
the	O
edges	O
are	O
randomly	O
permutated	O
,	O
the	O
target	O
output	O
sequence	O
should	O
not	O
change	O
.	O

The	O
same	O
applies	O
for	O
bAbI	Task
task	Task
19	O
and	O
the	O
shortest	Task
path	Task
task	O
.	O

GGS	Method
-	Method
NNs	Method
are	O
good	O
at	O
handling	O
this	O
type	O
of	O
“	O
static	O
”	O
data	O
,	O
while	O
RNN	Method
and	O
LSTM	Method
are	O
not	O
.	O

However	O
future	O
work	O
is	O
needed	O
to	O
determine	O
how	O
best	O
to	O
apply	O
GGS	Method
-	Method
NNs	Method
to	O
temporal	O
sequential	O
data	O
which	O
RNN	Method
and	O
LSTM	Method
are	O
good	O
at	O
.	O

This	O
is	O
one	O
limitation	O
of	O
the	O
GGS	Method
-	Method
NNs	Method
model	O
which	O
we	O
discussed	O
in	O
sec	O
:	O
discussion	O
.	O

appendix	O
:	O
Nested	Task
Prediction	Task
Details	O
Data	O
structures	O
like	O
list	O
of	O
lists	O
are	O
nested	O
data	O
structures	O
,	O
in	O
which	O
the	O
val	O
pointer	O
of	O
each	O
node	O
in	O
a	O
data	O
structure	O
points	O
to	O
another	O
data	O
structure	O
.	O

Such	O
data	O
structures	O
can	O
be	O
represented	O
in	O
separation	Method
logic	Method
by	O
allowing	O
predicates	O
to	O
be	O
nested	O
.	O

For	O
example	O
,	O
a	O
list	O
of	O
lists	O
can	O
be	O
represented	O
as	O
,	O
where	O
is	O
a	O
lambda	O
expression	O
and	O
says	O
that	O
for	O
each	O
node	O
in	O
the	O
list	O
from	O
to	O
,	O
its	O
val	O
pointer	O
satisfies	O
.	O

So	O
there	O
is	O
a	O
list	O
from	O
to	O
,	O
where	O
each	O
node	O
in	O
that	O
list	O
points	O
to	O
another	O
list	O
.	O

A	O
simple	O
list	O
without	O
nested	O
structures	O
can	O
be	O
represented	O
as	O
where	O
represents	O
an	O
empty	O
predicate	O
.	O

Note	O
that	O
unlike	O
the	O
non	O
-	O
nested	O
case	O
where	O
the	O
val	O
pointer	O
always	O
points	O
to	O
NULL	O
,	O
we	O
have	O
to	O
consider	O
the	O
val	O
pointers	O
here	O
in	O
order	O
to	O
describe	O
and	O
handle	O
nested	O
data	O
structures	O
.	O

To	O
make	O
our	O
GGS	Method
-	Method
NNs	Method
able	O
to	O
predict	O
nested	O
formulas	O
,	O
we	O
adapt	O
alg	Method
:	O
seplogic	Method
-	Method
prediction	Method
to	O
alg	O
:	O
seplogic	O
-	O
nesting	O
.	O

Where	O
an	O
outer	O
loop	O
goes	O
through	O
each	O
named	O
variable	O
once	O
and	O
generate	O
a	O
nested	O
predicate	O
with	O
the	O
node	O
associated	O
with	O
that	O
variable	O
as	O
the	O
active	O
node	O
.	O

The	O
nested	Method
prediction	Method
procedure	Method
handles	O
prediction	Task
similarly	O
as	O
in	O
alg	O
:	O
seplogic	Method
-	Method
prediction	Method
.	O

Before	O
calling	O
the	O
nested	Method
prediction	Method
procedure	Method
recursively	O
,	O
the	O
node	Method
annotation	Method
update	Method
in	O
line	O
[	O
reference	O
]	O
not	O
only	O
annotates	O
nodes	O
in	O
the	O
current	O
structure	O
as	O
“	O
is	O
-	O
explained	O
”	O
,	O
but	O
also	O
annotates	O
nodes	O
linked	O
to	O
via	O
the	O
“	O
val	O
”	O
pointer	O
from	O
all	O
nodes	O
in	O
the	O
current	O
structure	O
as	O
“	O
active	O
”	O
.	O

For	O
the	O
list	O
of	O
lists	O
example	O
,	O
after	O
predicting	O
“	O
”	O
,	O
the	O
annotation	O
step	O
annotates	O
all	O
nodes	O
in	O
the	O
list	O
from	O
to	O
as	O
“	O
is	O
-	O
explained	O
”	O
and	O
all	O
nodes	O
the	O
val	O
pointer	O
points	O
to	O
from	O
the	O
list	O
as	O
“	O
active	O
”	O
.	O

This	O
knowledge	O
is	O
not	O
hard	O
coded	O
into	O
the	O
algorithm	O
,	O
the	O
annotation	Method
model	Method
can	O
learn	O
this	O
behavior	O
from	O
data	O
.	O

[	O
1	O
]	O
OuterLoop	O
Graph	O
with	O
named	O
program	O
variables	O
compute	O
initial	O
labels	O
from	O
each	O
variable	O
name	O
node	O
associated	O
with	O
turn	O
on	O
“	O
active	O
”	O
bit	O
for	O
in	O
PredictNestedFormula	O
,	O
,	O
PredictNestedFormula	O
,	O
,	O
initialize	O
node	O
vectors	O
by	O
-	O
extending	O
quantifier	O
needed	O
Graph	O
-	O
level	Method
Classification	Method
(	O
†	O
)	O
fresh	O
existentially	O
quantified	O
variable	O
name	O
pick	O
node	O
Node	Method
Selection	Method
(	O
‡	O
)	O
turn	O
on	O
“	O
is	O
-	O
named	O
”	O
for	O
in	O
print	O
“	O
”	O
is	O
a	O
lambda	O
variable	O
name	O
print	O
“	O
”	O
pick	O
data	O
structure	O
predicate	O
Graph	O
-	O
level	O
Classification	Task
(	O
⋆	O
)	O
pick	O
list	O
end	O
node	O
Node	O
Selection	O
(	O
♡	O
)	O
get	O
variable	O
name	O
associated	O
with	O
print	O
“	O
”	O
print	O
“	O
”	O
print	O
“	O
”	O
update	O
node	O
annotations	O
in	O
Node	O
Annotation	O
(	O
♠	O
)	O
fresh	O
lambda	O
variable	O
name	O
PredictNestedFormula	O
,	O
,	O
Recursively	O
predict	O
all	O
nested	O
formulas	O
.	O

print	O
“	O
”	O
Nested	O
separation	Method
logic	Method
formula	O
prediction	O
procedure	O
