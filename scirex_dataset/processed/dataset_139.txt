Deep	Method
Networks	Method
with	Method
Internal	Method
Selective	Method
Attention	Method
through	Method
Feedback	Method
Connections	Method
Traditional	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
are	O
stationary	O
and	O
feedforward	O
.	O

They	O
neither	O
change	O
their	O
parameters	O
during	O
evaluation	O
nor	O
use	O
feedback	Method
from	O
higher	O
to	O
lower	O
layers	O
.	O

Real	O
brains	O
,	O
however	O
,	O
do	O
.	O

So	O
does	O
our	O
Deep	Method
Attention	Method
Selective	Method
Network	Method
(	O
dasNet	Method
)	O
architecture	O
.	O

DasNet	O
’s	O
feedback	Method
structure	O
can	O
dynamically	O
alter	O
its	O
convolutional	Method
filter	Method
sensitivities	Method
during	O
classification	Task
.	O

It	O
harnesses	O
the	O
power	O
of	O
sequential	Method
processing	Method
to	O
improve	O
classification	Task
performance	O
,	O
by	O
allowing	O
the	O
network	O
to	O
iteratively	O
focus	O
its	O
internal	O
attention	O
on	O
some	O
of	O
its	O
convolutional	Method
filters	Method
.	O

Feedback	Method
is	O
trained	O
through	O
direct	Method
policy	Method
search	Method
in	O
a	O
huge	O
million	O
-	O
dimensional	O
parameter	O
space	O
,	O
through	O
scalable	Method
natural	Method
evolution	Method
strategies	Method
(	O
SNES	Method
)	O
.	O

On	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
,	O
dasNet	Method
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
unaugmented	O
datasets	O
.	O

1	O
Introduction	O
Deep	Method
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
[	O
1	O
]	O
with	O
max	Method
-	Method
pooling	Method
layers	Method
[	O
2	O
]	O
trained	O
by	O
backprop	Method
[	O
3	O
]	O
on	O
GPUs	Method
[	O
4	O
]	O
have	O
become	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
object	Task
recognition	Task
[	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
]	O
,	O
segmentation	Task
/	Task
detection	Task
[	O
9	O
,	O
10	O
]	O
,	O
and	O
scene	Task
parsing	Task
[	O
11	O
,	O
12	O
]	O
(	O
for	O
an	O
extensive	O
review	O
see	O
[	O
13	O
]	O
)	O
.	O

These	O
architectures	O
consist	O
of	O
many	O
stacked	Method
feedforward	Method
layers	Method
,	O
mimicking	O
the	O
bottom	O
-	O
up	O
path	O
of	O
the	O
human	O
visual	O
cortex	O
,	O
where	O
each	O
layer	O
learns	O
progressively	O
more	O
abstract	O
representations	O
of	O
the	O
input	O
data	O
.	O

Low	O
-	O
level	O
stages	O
tend	O
to	O
learn	O
biologically	O
plausible	O
feature	O
detectors	O
,	O
such	O
as	O
Gabor	Method
filters	Method
[	O
14	O
]	O
.	O

Detectors	O
in	O
higher	O
layers	O
learn	O
to	O
respond	O
to	O
concrete	O
visual	O
objects	O
or	O
their	O
parts	O
,	O
e.g.	O
,	O
[	O
15	O
]	O
.	O

Once	O
trained	O
,	O
the	O
CNN	Method
never	O
changes	O
its	O
weights	O
or	O
filters	O
during	O
evaluation	O
.	O

Evolution	Method
has	O
discovered	O
efficient	O
feedforward	Method
pathways	Method
for	O
recognizing	O
certain	O
objects	O
in	O
the	O
blink	O
of	O
an	O
eye	O
.	O

However	O
,	O
an	O
expert	O
ornithologist	O
,	O
asked	O
to	O
classify	O
a	O
bird	O
belonging	O
to	O
one	O
of	O
two	O
very	O
similar	O
species	O
,	O
may	O
have	O
to	O
think	O
for	O
more	O
than	O
a	O
few	O
milliseconds	O
before	O
answering	O
[	O
16	O
,	O
17	O
]	O
,	O
implying	O
that	O
several	O
feedforward	O
evaluations	O
are	O
performed	O
,	O
where	O
each	O
evaluation	O
tries	O
to	O
elicit	O
different	O
information	O
from	O
the	O
image	O
.	O

Since	O
humans	O
benefit	O
greatly	O
from	O
this	O
strategy	O
,	O
we	O
hypothesise	O
CNNs	Method
can	O
too	O
.	O

This	O
requires	O
:	O
(	O
1	O
)	O
the	O
formulation	O
of	O
a	O
non	O
-	O
stationary	O
CNN	Method
that	O
can	O
adapt	O
its	O
own	O
behaviour	O
post	O
-	O
training	O
,	O
and	O
(	O
2	O
)	O
a	O
process	O
that	O
decides	O
how	O
to	O
adapt	O
the	O
CNNs	Method
behaviour	O
.	O

This	O
paper	O
introduces	O
Deep	Method
Attention	Method
Selective	Method
Networks	Method
(	O
dasNet	Method
)	O
which	O
model	O
selective	Task
attention	Task
in	O
deep	Method
CNNs	Method
by	O
allowing	O
each	O
layer	O
to	O
influence	O
all	O
other	O
layers	O
on	O
successive	O
passes	O
over	O
an	O
image	O
through	O
special	O
connections	O
(	O
both	O
bottom	O
-	O
up	O
and	O
top	O
-	O
down	O
)	O
,	O
that	O
modulate	O
the	O
activity	O
of	O
the	O
convolutional	Method
filters	Method
.	O

The	O
weights	O
of	O
these	O
special	O
connections	O
implement	O
a	O
control	Method
policy	Method
that	O
is	O
learned	O
through	O
reinforcement	Method
learning	Method
after	O
the	O
CNN	Method
has	O
been	O
trained	O
in	O
the	O
usual	O
way	O
via	O
supervised	Method
learning	Method
.	O

Given	O
an	O
input	O
image	O
,	O
the	O
attentional	Method
policy	Method
can	O
enhance	O
or	O
suppress	O
features	O
over	O
multiple	O
passes	O
to	O
improve	O
the	O
classification	Task
of	Task
difficult	Task
cases	Task
not	O
captured	O
by	O
the	O
initially	O
supervised	O
∗Shared	O
first	O
author	O
.	O

training	O
.	O

Our	O
aim	O
is	O
to	O
let	O
the	O
system	O
check	O
the	O
usefulness	O
of	O
internal	O
CNN	Method
filters	O
automatically	O
,	O
omitting	O
manual	Task
inspection	Task
[	O
18	O
]	O
.	O

In	O
our	O
current	O
implementation	O
,	O
the	O
attentional	Method
policy	Method
is	O
evolved	O
using	O
Separable	Method
Natural	Method
Evolution	Method
Strategies	Method
(	O
SNES	Method
;	O
[	O
19	O
]	O
)	O
,	O
instead	O
of	O
a	O
conventional	O
,	O
single	Method
agent	Method
reinforcement	Method
learning	Method
method	Method
(	O
e.g.	O
value	Method
iteration	Method
,	O
temporal	O
difference	O
,	O
policy	O
gradients	O
,	O
etc	O
.	O

)	O
due	O
to	O
the	O
large	O
number	O
of	O
parameters	O
(	O
over	O
1	O
million	O
)	O
required	O
to	O
control	O
CNNs	Method
of	O
the	O
size	O
typically	O
used	O
in	O
image	Task
classification	Task
.	O

Experiments	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR100	Material
[	O
20	O
]	O
show	O
that	O
on	O
difficult	O
classification	Task
instances	Task
,	O
the	O
network	O
corrects	O
itself	O
by	O
emphasising	O
and	O
de	O
-	O
emphasising	O
certain	O
filters	O
,	O
outperforming	O
a	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CNN	Method
.	O

2	O
Maxout	Method
Networks	Method
In	O
this	O
work	O
we	O
use	O
the	O
Maxout	Method
networks	Method
[	O
7	O
]	O
,	O
combined	O
with	O
dropout	Method
[	O
21	O
]	O
,	O
as	O
the	O
underlying	O
model	O
for	O
dasNet	Method
.	O

Maxout	Method
networks	Method
represent	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
object	Task
recognition	Task
in	O
various	O
tasks	O
and	O
have	O
only	O
been	O
outperformed	O
(	O
by	O
a	O
small	O
margin	O
)	O
by	O
averaging	O
committees	O
of	O
several	O
convolutional	Method
neural	Method
networks	Method
.	O

A	O
similar	O
approach	O
,	O
which	O
does	O
not	O
reduce	O
dimensionality	O
in	O
favor	O
of	O
sparsity	O
in	O
the	O
representation	O
has	O
also	O
been	O
recently	O
presented	O
[	O
22	O
]	O
.	O

Maxout	O
CNNs	Method
consist	O
of	O
a	O
stack	Method
of	Method
alternating	Method
convolutional	Method
and	Method
maxout	Method
layers	Method
,	O
with	O
a	O
final	O
classification	Method
layer	Method
on	O
top	O
:	O
Convolutional	Method
Layer	Method
.	O

The	O
input	O
to	O
this	O
layer	O
can	O
be	O
an	O
image	O
or	O
the	O
output	O
of	O
a	O
previous	O
layer	O
,	O
consisting	O
of	O
c	O
input	O
maps	O
of	O
width	O
m	O
and	O
height	O
n	O
:	O
x	O
∈	O
Rc×m×n	O
.	O

The	O
output	O
consists	O
of	O
a	O
set	O
of	O
c′	O
output	O
maps	O
:	O
y	O
∈	O
Rc′×m′×n′	Method
.	O

The	O
convolutional	Method
layer	Method
is	O
parameterised	O
by	O
c	O
·	Method
c′	Method
filters	Method
of	O
size	O
k×	O
k.	O
We	O
denote	O
the	O
filters	O
by	O
F	O
ℓi	O
,	O
j	O
∈	O
Rk×k	O
,	O
where	O
i	O
and	O
j	O
are	O
indexes	O
of	O
the	O
input	O
and	O
output	O
maps	O
and	O
ℓ	O
denotes	O
the	O
layer	O
.	O

yℓj	O
=	O
i	O
=	O
c∑	O
i=0	O
ϕ	O
(	O
xi	O
∗	O
F	O
ℓi	O
,	O
j	O
)	O
(	O
1	O
)	O
where	O
i	O
and	O
j	O
index	O
the	O
input	O
and	O
output	O
map	O
respectively	O
,	O
∗	O
is	O
the	O
convolutional	O
operator	O
,	O
ϕ	O
is	O
an	O
element	O
-	O
wise	O
nonlinear	O
function	O
,	O
and	O
ℓ	O
is	O
used	O
to	O
index	O
the	O
layer	O
.	O

The	O
size	O
of	O
the	O
output	O
is	O
determined	O
by	O
the	O
kernel	O
size	O
and	O
the	O
stride	O
used	O
for	O
the	O
convolution	Method
(	O
see	O
[	O
7	O
]	O
)	O
.	O

Pooling	Method
Layer	Method
.	O

A	O
pooling	Method
layer	Method
is	O
used	O
to	O
reduced	O
the	O
dimensionality	O
of	O
the	O
output	O
from	O
a	O
convolutional	Method
layer	Method
.	O

The	O
usual	O
approach	O
is	O
to	O
take	O
the	O
maximum	O
value	O
among	O
non	O
-	O
or	O
partially	O
-	O
overlapping	O
patches	O
in	O
every	O
map	O
,	O
therefore	O
reducing	O
dimensionality	O
along	O
the	O
height	O
and	O
width	O
[	O
2	O
]	O
.	O

Instead	O
,	O
a	O
Maxout	Method
pooling	Method
layer	Method
reduces	O
every	O
b	O
consecutive	O
maps	O
to	O
one	O
map	O
,	O
by	O
keeping	O
only	O
the	O
maximum	O
value	O
for	O
every	O
pixel	O
-	O
position	O
,	O
where	O
b	O
is	O
called	O
the	O
block	O
size	O
.	O

Thus	O
the	O
map	O
reduces	O
c	O
input	O
maps	O
to	O
c′	O
=	O
c	O
/	O
b	O
output	O
maps	O
.	O

yℓj	O
,	O
x	O
,	O
y	O
=	O
b	O
max	O
i=0	O
yℓ−1j·b	O
+	O
i	O
,	O
x	O
,	O
y	O
(	O
2	O
)	O
where	O
yℓ	O
∈	O
Rc′×m′×n′	O
,	O
and	O
ℓ	O
again	O
is	O
used	O
to	O
index	O
the	O
layer	O
.	O

The	O
output	O
of	O
the	O
pooling	Method
layer	Method
can	O
either	O
be	O
used	O
as	O
input	O
to	O
another	O
pair	O
of	O
convolutional	Method
-	Method
and	Method
pooling	Method
layers	Method
,	O
or	O
form	O
input	O
to	O
a	O
final	O
classification	Method
layer	Method
.	O

Classification	Method
Layer	Method
.	O

Finally	O
,	O
a	O
classification	Method
step	Method
is	O
performed	O
.	O

First	O
the	O
output	O
of	O
the	O
last	O
pooling	Method
layer	Method
is	O
flattened	O
into	O
one	O
large	O
vector	O
x⃗	O
,	O
to	O
form	O
the	O
input	O
to	O
the	O
following	O
equations	O
:	O
ȳℓj	O
=	O
max	O
i=0	O
..	O
b	O
F	O
ℓj·b	O
+	O
ix⃗	O
(	O
3	O
)	O
v	O
=	O
σ	O
(	O
F	O
ℓ	O
+	O
1ȳℓ	O
)	O
(	O
4	O
)	O
where	O
F	O
ℓ	O
∈	O
RN×|x⃗|	O
(	O
N	O
is	O
chosen	O
)	O
,	O
and	O
σ	O
(	O
·	O
)	O
is	O
the	O
softmax	O
activation	O
function	O
which	O
produces	O
the	O
class	O
probabilities	O
v.	O
The	O
input	O
is	O
projected	O
by	O
F	O
and	O
then	O
reduced	O
using	O
a	O
maxout	Method
,	O
similar	O
to	O
the	O
pooling	Method
layer	Method
(	O
3	O
)	O
.	O

3	O
Reinforcement	Method
Learning	Method
Reinforcement	Method
learning	Method
(	O
RL	Method
)	O
is	O
a	O
general	O
framework	O
for	O
learning	Task
to	O
make	O
sequential	Task
decisions	Task
order	O
to	O
maximise	O
an	O
external	O
reward	O
signal	O
[	O
23	O
,	O
24	O
]	O
.	O

The	O
learning	Method
agent	Method
can	O
be	O
anything	O
that	O
has	O
the	O
ability	O
to	O
act	O
and	O
perceive	O
in	O
a	O
given	O
environment	O
.	O

At	O
time	O
t	O
,	O
the	O
agent	O
receives	O
an	O
observation	O
ot	O
∈	O
O	O
of	O
the	O
current	O
state	O
of	O
the	O
environment	O
st	O
∈	O
S	O
,	O
and	O
selects	O
an	O
action	O
,	O
at	O
∈	O
A	O
,	O
chosen	O
by	O
a	O
policy	Method
π	Method
:	O
O	O
→	O
A	O
,	O
where	O
S	O
,	O
O	O
and	O
A	O
the	O
spaces	O
of	O
all	O
possible	O
states	O
,	O
observations	O
,	O
and	O
action	O
,	O
respectively.1	O
The	O
agent	O
then	O
enters	O
state	O
st	O
+	O
1	O
and	O
receives	O
a	O
reward	O
rt	O
∈	O
R.	O
The	O
objective	O
is	O
to	O
find	O
the	O
policy	O
,	O
π	O
,	O
that	O
maximises	O
the	O
expected	O
future	O
discounted	O
reward	O
,	O
E	O
[	O
∑	O
t	O
γ	O
trt	O
]	O
,	O
where	O
γ	O
∈	O
[	O
0	O
,	O
1	O
]	O
discounts	O
the	O
future	O
,	O
modeling	O
the	O
“	O
farsightedness	O
”	O
of	O
the	O
agent	O
.	O

In	O
dasNet	Method
,	O
both	O
the	O
observation	O
and	O
action	O
spaces	O
are	O
real	O
valued	O
O	O
=	O
Rdim	O
(	O
O	O
)	O
,	O
A	O
=	O
Rdim	Method
(	Method
A	Method
)	O
.	O

Therefore	O
,	O
policy	Method
πθ	Method
must	O
be	O
represented	O
by	O
a	O
function	Method
approximator	Method
,	O
e.g.	O
a	O
neural	Method
network	Method
,	O
parameterised	O
by	O
θ	O
.	O

Because	O
the	O
policies	O
used	O
to	O
control	O
the	O
attention	O
of	O
the	O
dasNet	Method
have	O
state	O
and	O
actions	O
spaces	O
of	O
close	O
to	O
a	O
thousand	O
dimensions	O
,	O
the	O
policy	O
parameter	O
vector	O
,	O
θ	O
,	O
will	O
contain	O
close	O
to	O
a	O
million	O
weights	O
,	O
which	O
is	O
impractical	O
for	O
standard	O
RL	Method
methods	Method
.	O

Therefore	O
,	O
we	O
instead	O
evolve	O
the	O
policy	O
using	O
a	O
variant	O
for	O
Natural	Method
Evolution	Method
Strategies	Method
(	O
NES	Method
;	O
[	O
25	O
,	O
26	O
]	O
)	O
,	O
called	O
Separable	O
NES	Method
(	O
SNES	Method
;	O
[	O
19	O
]	O
)	O
.	O

The	O
NES	Method
family	O
of	O
black	O
-	O
box	O
optimization	O
algorithms	O
use	O
parameterised	O
probability	O
distributions	O
over	O
the	O
search	O
space	O
,	O
instead	O
of	O
an	O
explicit	O
population	O
(	O
i.e.	O
,	O
a	O
conventional	O
ES	Method
[	O
27	O
]	O
)	O
.	O

Typically	O
,	O
the	O
distribution	O
is	O
a	O
multivariate	Method
Gaussian	Method
parameterised	Method
by	O
mean	O
µ	O
and	O
covariance	Method
matrix	Method
Σ.	O
Each	O
epoch	O
a	O
generation	O
is	O
sampled	O
from	O
the	O
distribution	O
,	O
which	O
is	O
then	O
updated	O
the	O
direction	O
of	O
the	O
natural	O
gradient	O
of	O
the	O
expected	O
fitness	O
of	O
the	O
distribution	O
.	O

SNES	Method
differs	O
from	O
standard	O
NES	Method
in	O
that	O
instead	O
of	O
maintaining	O
the	O
full	O
covariance	O
matrix	O
of	O
the	O
search	O
distribution	O
,	O
uses	O
only	O
the	O
diagonal	O
entries	O
.	O

SNES	Method
is	O
theoretically	O
less	O
powerful	O
than	O
standard	O
NES	Method
,	O
but	O
is	O
substantially	O
more	O
efficient	O
.	O

4	O
Deep	Method
Attention	Method
Selective	Method
Networks	Method
(	O
dasNet	Method
)	O
The	O
idea	O
behind	O
dasNet	Method
is	O
to	O
harness	O
the	O
power	O
of	O
sequential	Method
processing	Method
to	O
improve	O
classification	Task
performance	O
by	O
allowing	O
the	O
network	O
to	O
iteratively	O
focus	O
the	O
attention	O
of	O
its	O
filters	O
.	O

First	O
,	O
the	O
standard	O
Maxout	Method
net	Method
(	O
see	O
Section	O
2	O
)	O
is	O
augmented	O
to	O
allow	O
the	O
filters	O
to	O
be	O
weighted	O
differently	O
on	O
different	O
passes	O
over	O
the	O
same	O
image	O
(	O
compare	O
to	O
equation	O
1	O
)	O
:	O
yℓj	O
=	O
a	O
ℓ	O
j	O
i	O
=	O
c∑	O
i=0	O
ϕ	O
(	O
xi	O
∗	O
F	O
ℓi	O
,	O
j	O
)	O
,	O
(	O
5	O
)	O
where	O
aℓj	O
is	O
the	O
weight	O
of	O
the	O
j	O
-	O
th	O
output	O
map	O
in	O
layer	O
ℓ	O
,	O
changing	O
the	O
strength	O
of	O
its	O
activation	O
,	O
before	O
applying	O
the	O
maxout	Method
pooling	Method
operator	Method
.	O

The	O
vector	O
a	O
=	O
[	O
a00	O
,	O
a	O
0	O
1	O
,	O
·	O
·	O
·	O
,	O
a0c′	O
,	O
a10	O
,	O
·	O
·	O
·	O
,	O
a1c′	O
,	O
·	O
·	O
·	O
]	O
represents	O
the	O
action	O
that	O
the	O
learned	O
policy	O
must	O
select	O
in	O
order	O
to	O
sequentially	O
focus	O
the	O
attention	O
of	O
the	O
Maxout	Method
net	Method
on	O
the	O
most	O
discriminative	O
features	O
in	O
the	O
image	O
being	O
processed	O
.	O

Changing	O
action	O
a	O
will	O
alter	O
the	O
behaviour	O
of	O
the	O
CNN	Method
,	O
resulting	O
in	O
different	O
outputs	O
,	O
even	O
when	O
the	O
image	O
x	O
does	O
not	O
change	O
.	O

We	O
indicate	O
this	O
with	O
the	O
following	O
notation	O
:	O
vt	O
=	O
Mt	O
(	O
θ	O
,	O
x	O
)	O
(	O
6	O
)	O
where	O
θ	O
is	O
the	O
parameter	O
vector	O
of	O
the	O
policy	O
,	O
πθ	O
,	O
and	O
vt	O
is	O
the	O
output	O
of	O
the	O
network	O
on	O
pass	O
t.	O
Algorithm	O
1	O
describes	O
the	O
dasNet	Method
training	O
algorithm	O
.	O

Given	O
a	O
Maxout	Method
net	Method
,	O
M	O
,	O
that	O
has	O
already	O
been	O
trained	O
to	O
classify	O
images	O
using	O
training	O
set	O
,	O
X	O
,	O
the	O
policy	O
,	O
π	O
,	O
is	O
evolved	O
using	O
SNES	Method
to	O
focus	O
the	O
attention	O
of	O
M.	O
Each	O
pass	O
through	O
the	O
while	Method
loop	Method
represents	O
one	O
generation	O
of	O
SNES	Method
.	O

Each	O
generation	O
starts	O
by	O
selecting	O
a	O
subset	O
of	O
n	O
images	O
from	O
X	O
at	O
random	O
.	O

Then	O
each	O
of	O
the	O
p	O
samples	O
drawn	O
from	O
the	O
SNES	Method
search	O
distribution	O
(	O
with	O
mean	O
µ	O
and	O
covariance	O
Σ	O
)	O
representing	O
the	O
parameters	O
,	O
θi	O
,	O
of	O
a	O
candidate	Method
policy	Method
,	O
πθi	Method
,	O
undergoes	O
n	O
trials	O
,	O
one	O
for	O
each	O
image	O
in	O
the	O
batch	O
.	O

During	O
a	O
trial	O
,	O
the	O
image	O
is	O
presented	O
to	O
the	O
Maxout	O
net	O
T	O
times	O
.	O

In	O
the	O
first	O
pass	O
,	O
t	O
=	O
0	O
,	O
the	O
action	O
,	O
a0	O
,	O
is	O
set	O
to	O
ai	O
=	O
1	O
,	O
∀i	O
,	O
so	O
that	O
the	O
Maxout	Method
network	Method
functions	O
as	O
it	O
would	O
normally	O
—	O
1In	O
this	O
work	O
π	O
:	O
O	O
→	O
A	O
is	O
a	O
deterministic	Method
policy	Method
;	O
given	O
an	O
observation	O
it	O
will	O
always	O
output	O
the	O
same	O
action	O
.	O

However	O
,	O
π	Method
could	O
be	O
extended	O
to	O
stochastic	Method
policies	Method
.	O

Algorithm	O
1	O
TRAIN	O
DASNET	Method
(	O
M	O
,	O
µ	O
,	O
Σ	O
,	O
p	O
,	O
n	O
)	O
1	O
:	O
while	O
True	O
do	O
2	O
:	O
images	O
⇐	O
NEXTBATCH	O
(	O
n	O
)	O
3	O
:	O
for	O
i	O
=	O
0	O
→	O
p	O
do	O
4	O
:	O
θi	O
∼	O
N	O
(	O
µ	O
,	O
Σ	O
)	O
5	O
:	O
for	O
j	O
=	O
0	O
→	O
n	O
do	O
6	O
:	O
a0	O
⇐	O
1	O
{	O
Initialise	O
gates	O
a	O
with	O
identity	O
activation	O
}	O
7	O
:	O
for	O
t	O
=	O
0	O
→	O
T	O
do	O
8	O
:	O
vt	O
=	O
Mt	O
(	O
θi	O
,	O
xi	O
)	O
9	O
:	O
ot	O
⇐	O
h	O
(	O
Mt	O
)	O
10	O
:	O
at	O
+	O
1	O
⇐	O
πθi	O
(	O
ot	O
)	O
11	O
:	O
end	O
for	O
12	O
:	O
Li	O
=	O
−λboostd	O
log	Method
(	Method
vT	Method
)	O
13	O
:	O
end	O
for	O
14	O
:	O
F	O
[	O
i	O
]	O
⇐	O
f	O
(	O
θi	O
)	O
15	O
:	O
Θ	O
[	O
i	O
]	O
⇐	O
θi	O
16	O
:	O
end	O
for	O
17	O
:	O
UPDATESNES	O
(	O
F	O
,	O
Θ	O
)	O
{	O
Details	O
in	O
supplementary	O
material	O
.	O

}	O
18	O
:	O
end	O
while	O
the	O
action	O
has	O
no	O
effect	O
.	O

Once	O
the	O
image	O
is	O
propagated	O
through	O
the	O
net	O
,	O
an	O
observation	O
vector	O
,	O
o0	O
,	O
is	O
constructed	O
by	O
concatenating	O
the	O
following	O
values	O
extracted	O
from	O
M	O
,	O
by	O
h	O
(	O
·	O
)	O
:	O
1	O
.	O

the	O
average	O
activation	O
of	O
every	O
output	O
map	O
Avg	O
(	O
yj	O
)	O
(	O
Equation	O
2	O
)	O
,	O
of	O
each	O
Maxout	Method
layer	Method
.	O

2	O
.	O

the	O
intermediate	O
activations	O
ȳj	O
of	O
the	O
classification	Method
layer	Method
.	O

3	O
.	O

the	O
class	O
probability	O
vector	O
,	O
vt	O
.	O

While	O
averaging	Method
map	Method
activations	Method
provides	O
only	O
partial	O
state	O
information	O
,	O
these	O
values	O
should	O
still	O
be	O
meaningful	O
enough	O
to	O
allow	O
for	O
the	O
selection	O
of	O
good	O
actions	O
.	O

The	O
candidate	Method
policy	Method
then	O
maps	O
the	O
observation	O
to	O
an	O
action	O
:	O
πθi	O
(	O
o	O
)	O
=	O
dim	O
(	O
A	O
)	O
σ	O
(	O
θiot	O
)	O
=	O
at	O
,	O
(	O
7	O
)	O
where	O
θ	O
∈	O
Rdim	O
(	O
A	O
)	O
×dim	O
(	O
O	O
)	O
is	O
the	O
weight	O
matrix	O
of	O
the	O
neural	Method
network	Method
,	O
and	O
σ	Method
is	O
the	O
softmax	O
.	O

Note	O
that	O
the	O
softmax	O
function	O
is	O
scaled	O
by	O
the	O
dimensionality	O
of	O
the	O
action	O
space	O
so	O
that	O
elements	O
in	O
the	O
action	O
vector	O
average	O
to	O
1	O
(	O
instead	O
of	O
regular	O
softmax	O
which	O
sums	O
to	O
1	O
)	O
,	O
ensuring	O
that	O
all	O
network	O
outputs	O
are	O
positive	O
,	O
thereby	O
keeping	O
the	O
filter	O
activations	O
stable	O
.	O

On	O
the	O
next	O
pass	O
,	O
the	O
same	O
image	O
is	O
processed	O
again	O
,	O
but	O
this	O
time	O
using	O
the	O
filter	Method
weighting	Method
,	O
a1	O
.	O

This	O
cycle	O
is	O
repeated	O
until	O
pass	O
T	O
(	O
see	O
figure	O
1	O
for	O
a	O
illustration	O
of	O
the	O
process	O
)	O
,	O
at	O
which	O
time	O
the	O
performance	O
of	O
the	O
network	O
is	O
scored	O
by	O
:	O
Li	O
=	O
−λboostd	O
log	Method
(	Method
vT	Method
)	O
(	O
8	O
)	O
vT	O
=	O
MT	O
(	O
θi	O
,	O
xi	O
)	O
(	O
9	O
)	O
λboost	Method
=	O
{	O
λcorrect	O
if	O
d	O
=	O
∥vT	O
∥∞	O
λmisclassified	O
otherwise	O
,	O
(	O
10	O
)	O
where	O
v	O
is	O
the	O
output	O
of	O
M	O
at	O
the	O
end	O
of	O
the	O
pass	O
T	O
,	O
d	O
is	O
the	O
correct	Metric
classification	Metric
,	O
and	O
λcorrect	O
and	O
λmisclassified	O
are	O
constants	O
.	O

Li	Method
measures	O
the	O
weighted	Metric
loss	Metric
,	O
where	O
misclassified	O
samples	O
are	O
weighted	O
higher	O
than	O
correctly	O
classified	O
samples	O
λmisclassified	O
>	O
λcorrect	O
.	O

This	O
simple	O
form	O
of	O
boosting	Method
is	O
used	O
to	O
focus	O
on	O
the	O
‘	O
difficult	O
’	O
misclassified	O
images	O
.	O

Once	O
all	O
of	O
the	O
input	O
images	O
have	O
been	O
processed	O
,	O
the	O
policy	O
is	O
assigned	O
the	O
fitness	O
:	O
f	O
(	O
θi	O
)	O
=	O
cumulative	O
score︷	O
︸︸	O
︷	O
n∑	O
i=1	O
Li	O
+	O
regularization︷	O
︸︸	O
︷	O
λL2∥θi∥2	O
(	O
11	O
)	O
where	O
λL2	Method
is	O
a	O
regularization	O
parameter	O
.	O

Once	O
all	O
of	O
the	O
candidate	O
policies	O
have	O
been	O
evaluated	O
,	O
SNES	Method
updates	O
its	O
distribution	O
parameters	O
(	O
µ	O
,	O
Σ	O
)	O
according	O
the	O
natural	O
gradient	O
calculated	O
from	O
the	O
sampled	O
fitness	O
values	O
,	O
F	O
.	O

As	O
SNES	Method
repeatedly	O
updates	O
the	O
distribution	O
over	O
the	O
course	O
of	O
many	O
generations	O
,	O
the	O
expected	O
fitness	O
of	O
the	O
distribution	O
improves	O
,	O
until	O
the	O
stopping	Metric
criterion	Metric
is	O
met	O
when	O
no	O
improvement	O
is	O
made	O
for	O
several	O
consecutive	O
epochs	O
.	O

5	O
Related	O
Work	O
Human	Task
vision	Task
is	O
still	O
the	O
most	O
advanced	O
and	O
flexible	O
perceptual	Method
system	Method
known	O
.	O

Architecturally	O
,	O
visual	O
cortex	O
areas	O
are	O
highly	O
connected	O
,	O
including	O
direct	O
connections	O
over	O
multiple	O
levels	O
and	O
topdown	O
connections	O
.	O

Felleman	O
and	O
Essen	O
[	O
28	O
]	O
constructed	O
a	O
(	O
now	O
famous	O
)	O
hierarchy	Method
diagram	Method
of	O
32	O
different	O
visual	O
cortical	O
areas	O
in	O
macaque	O
visual	O
cortex	O
.	O

About	O
40	O
%	O
of	O
all	O
pairs	O
of	O
areas	O
were	O
considered	O
connected	O
,	O
and	O
most	O
connected	O
areas	O
were	O
connected	O
bidirectionally	O
.	O

The	O
top	O
-	O
down	O
connections	O
are	O
more	O
numerous	O
than	O
bottom	O
-	O
up	O
connections	O
,	O
and	O
generally	O
more	O
diffuse	O
[	O
29	O
]	O
.	O

They	O
are	O
thought	O
to	O
play	O
primarily	O
a	O
modulatory	O
role	O
,	O
while	O
feedforward	O
connections	O
serve	O
as	O
directed	O
information	O
carriers	O
[	O
30	O
]	O
.	O

Analysis	O
of	O
response	O
latencies	O
to	O
a	O
newly	O
-	O
presented	O
image	O
lends	O
credence	O
to	O
the	O
theory	O
that	O
there	O
are	O
two	O
stages	O
of	O
visual	Task
processing	Task
:	O
a	O
fast	O
,	O
pre	Task
-	Task
attentive	Task
phase	Task
,	O
due	O
to	O
feedforward	Method
processing	Method
,	O
followed	O
by	O
an	O
attentional	O
phase	O
,	O
due	O
to	O
the	O
influence	O
of	O
recurrent	Method
processing	Method
[	O
31	O
]	O
.	O

After	O
the	O
feedforward	O
pass	O
,	O
we	O
can	O
recognise	O
and	O
localise	O
simple	O
salient	O
stimuli	O
,	O
which	O
can	O
“	O
pop	O
-	O
out	O
”	O
[	O
32	O
]	O
,	O
and	O
response	Metric
times	Metric
do	O
not	O
increase	O
regardless	O
of	O
the	O
number	O
of	O
distractors	O
.	O

However	O
,	O
this	O
effect	O
has	O
only	O
been	O
conclusively	O
shown	O
for	O
basic	O
features	O
such	O
as	O
colour	O
or	O
orientation	O
;	O
for	O
categorical	O
stimuli	O
or	O
faces	O
,	O
whether	O
there	O
is	O
a	O
pop	O
-	O
out	O
effect	O
remains	O
controversial	O
[	O
33	O
]	O
.	O

Regarding	O
the	O
attentional	O
phase	O
,	O
feedback	Method
connections	O
are	O
known	O
to	O
play	O
important	O
roles	O
,	O
such	O
as	O
in	O
feature	Task
grouping	Task
[	O
34	O
]	O
,	O
in	O
differentiating	O
a	O
foreground	O
from	O
its	O
background	O
,	O
(	O
especially	O
when	O
the	O
foreground	O
is	O
not	O
highly	O
salient	O
[	O
35	O
]	O
)	O
,	O
and	O
perceptual	O
filling	O
in	O
[	O
36	O
]	O
.	O

Work	O
by	O
Bar	O
et	O
al	O
.	O

[	O
37	O
]	O
supports	O
the	O
idea	O
that	O
top	O
-	O
down	O
projections	O
from	O
prefrontal	O
cortex	O
play	O
an	O
important	O
role	O
in	O
object	Task
recognition	Task
by	O
quickly	O
extracting	O
low	O
-	O
level	O
spatial	O
frequency	O
information	O
to	O
provide	O
an	O
initial	O
guess	O
about	O
potential	O
categories	O
,	O
forming	O
a	O
top	O
-	O
down	O
expectation	O
that	O
biases	O
recognition	Task
.	O

Recurrent	O
connections	O
seem	O
to	O
rely	O
heavily	O
on	O
competitive	O
inhibition	O
and	O
other	O
feedback	Method
to	O
make	O
object	Task
recognition	Task
more	O
robust	O
[	O
38	O
,	O
39	O
]	O
.	O

In	O
the	O
context	O
of	O
computer	Task
vision	Task
,	O
RL	Method
has	O
been	O
shown	O
to	O
be	O
able	O
to	O
learn	O
saccades	O
in	O
visual	O
scenes	O
to	O
learn	O
selective	O
attention	O
[	O
40	O
,	O
41	O
]	O
,	O
learn	O
feedback	Method
to	O
lower	O
levels	O
[	O
42	O
,	O
43	O
]	O
,	O
and	O
improve	O
face	Task
recognition	Task
[	O
44	O
,	O
45	O
]	O
.	O

It	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
object	Task
recognition	Task
[	O
46	O
]	O
,	O
and	O
has	O
also	O
been	O
combined	O
with	O
traditional	O
computer	Method
vision	Method
primitives	Method
[	O
47	O
]	O
.	O

Iterative	Method
processing	Method
of	O
images	O
using	O
recurrency	Method
has	O
been	O
successfully	O
used	O
for	O
image	Task
reconstruction	Task
[	O
48	O
]	O
,	O
face	Task
-	Task
localization	Task
[	O
49	O
]	O
and	O
compression	Task
[	O
50	O
]	O
.	O

All	O
these	O
approaches	O
show	O
that	O
recurrency	Task
in	Task
processing	Task
and	O
an	O
RL	Method
perspective	Method
can	O
lead	O
to	O
novel	O
algorithms	O
that	O
improve	O
performance	O
.	O

However	O
,	O
this	O
research	O
is	O
often	O
applied	O
to	O
simplified	O
datasets	O
for	O
demonstration	Task
purposes	Task
due	O
to	O
computation	O
constraints	O
,	O
and	O
are	O
not	O
aimed	O
at	O
improving	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O

In	O
contrast	O
,	O
we	O
apply	O
this	O
perspective	O
directly	O
to	O
the	O
known	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
networks	Method
to	O
show	O
that	O
this	O
approach	O
is	O
now	O
feasible	O
and	O
actually	O
increases	O
performance	O
.	O

6	O
Experiments	O
on	O
CIFAR	Material
-	Material
10	Material
/	O
100	Material
The	O
experimental	O
evaluation	O
of	O
dasNet	Method
focuses	O
on	O
ambiguous	Task
classification	Task
cases	Task
in	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
data	O
sets	O
where	O
,	O
due	O
to	O
a	O
high	O
number	O
of	O
common	O
features	O
,	O
two	O
classes	O
are	O
often	O
mistaken	O
for	O
each	O
other	O
.	O

These	O
are	O
the	O
most	O
interesting	O
cases	O
for	O
our	O
approach	O
.	O

By	O
learning	O
on	O
top	O
of	O
an	O
already	O
trained	O
model	O
,	O
dasNet	Method
must	O
aim	O
at	O
fixing	O
these	O
erroneous	O
predictions	O
without	O
disrupting	O
,	O
or	O
forgetting	O
,	O
what	O
has	O
been	O
learned	O
.	O

The	O
CIFAR	Material
-	Material
10	Material
dataset	Material
[	O
20	O
]	O
is	O
composed	O
of	O
32	O
×	O
32	O
colour	O
images	O
split	O
into	O
5×104	O
training	O
and	O
104	O
testing	O
samples	O
,	O
where	O
each	O
image	O
is	O
assigned	O
to	O
one	O
of	O
10	O
classes	O
.	O

The	O
CIFAR	Material
-	Material
100	Material
is	O
similarly	O
composed	O
,	O
but	O
contains	O
100	Material
classes	O
.	O

The	O
number	O
of	O
steps	O
was	O
experimentally	O
determined	O
and	O
fixed	O
at	O
T	O
=	O
5	O
;	O
small	O
enough	O
to	O
be	O
computationally	O
tractable	O
while	O
still	O
allowing	O
for	O
enough	O
interaction	O
.	O

In	O
all	O
experiments	O
we	O
set	O
λcorrect	O
=	O
0.005	O
,	O
λmisclassified	O
=	O
1	O
and	O
λL2	O
=	O
0.005	O
.	O

The	O
Maxout	Method
network	Method
,	O
M	Method
,	O
was	O
trained	O
with	O
data	Method
augmentation	Method
following	O
global	Method
contrast	Method
normalization	Method
and	O
ZCA	Method
normalization	Method
.	O

The	O
model	O
consists	O
of	O
three	O
convolutional	Method
maxout	Method
layers	Method
followed	O
by	O
a	O
fully	O
connected	O
maxout	O
and	O
softmax	O
outputs	O
.	O

Dropout	O
of	O
0.5	O
was	O
used	O
in	O
all	O
layers	O
except	O
the	O
input	O
layer	O
,	O
and	O
0.2	O
for	O
the	O
input	O
layer	O
.	O

The	O
population	O
size	O
for	O
SNES	Method
was	O
set	O
to	O
50	O
.	O

Training	O
took	O
of	O
dasNet	Method
took	O
around	O
4	O
days	O
on	O
a	O
GTX	O
560	O
Ti	O
GPU	O
,	O
excluding	O
the	O
original	O
time	O
used	O
to	O
train	O
M.	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
dasNet	Method
vs.	O
other	O
methods	O
,	O
where	O
it	O
achieves	O
a	O
relative	O
improvement	O
of	O
6	O
%	O
with	O
respect	O
to	O
the	O
vanilla	O
CNN	Method
.	O

This	O
establishes	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
for	O
this	O
challenging	O
dataset	O
,	O
for	O
unaugmented	O
data	O
.	O

Figure	O
3	O
shows	O
the	O
classification	Task
of	O
a	O
cat	O
-	O
image	O
from	O
the	O
test	O
-	O
set	O
.	O

All	O
output	O
map	O
activations	O
in	O
the	O
final	O
step	O
are	O
shown	O
at	O
the	O
top	O
.	O

The	O
difference	O
in	O
activations	O
compared	O
to	O
the	O
first	O
step	O
,	O
i.e.	O
,	O
the	O
(	O
de	O
-)	O
emphasis	O
of	O
each	O
map	O
,	O
is	O
shown	O
on	O
the	O
bottom	O
.	O

On	O
the	O
left	O
are	O
the	O
class	O
probabilities	O
for	O
each	O
time	O
-	O
step	O
.	O

At	O
the	O
first	O
step	O
,	O
the	O
classification	Task
is	O
‘	O
dog	O
’	O
,	O
and	O
the	O
cat	O
could	O
indeed	O
be	O
mistaken	O
for	O
a	O
puppy	O
.	O

Note	O
that	O
in	O
the	O
first	O
step	O
,	O
the	O
network	O
has	O
not	O
yet	O
received	O
any	O
feedback	Method
.	O

In	O
the	O
next	O
step	O
,	O
the	O
probability	O
for	O
‘	O
cat	O
’	O
goes	O
up	O
dramatically	O
,	O
and	O
subsequently	O
drops	O
a	O
bit	O
in	O
the	O
following	O
steps	O
.	O

The	O
network	O
has	O
successfully	O
disambiguated	O
a	O
cat	O
from	O
a	O
dog	O
.	O

If	O
we	O
investigate	O
the	O
filters	O
,	O
we	O
see	O
that	O
in	O
the	O
lower	O
layer	O
emphasis	O
changes	O
significantly	O
(	O
see	O
‘	O
change	O
of	O
layer	O
0	O
’	O
)	O
.	O

Some	O
filters	O
focus	O
more	O
on	O
surroundings	O
whilst	O
others	O
de	O
-	O
emphasise	O
the	O
eyes	O
.	O

In	O
the	O
second	O
layer	O
,	O
almost	O
all	O
output	O
maps	O
are	O
emphasised	O
.	O

In	O
the	O
third	O
and	O
highest	O
convolutional	Method
layer	Method
,	O
the	O
most	O
complex	O
changes	O
to	O
the	O
network	O
can	O
be	O
seen	O
.	O

At	O
this	O
level	O
the	O
positional	O
correspondence	O
is	O
largely	O
lost	O
,	O
and	O
the	O
filters	Method
are	O
known	O
to	O
code	O
for	O
‘	O
higher	O
level	O
’	O
features	O
.	O

It	O
is	O
in	O
this	O
layer	O
that	O
changes	O
are	O
the	O
most	O
influential	O
because	O
they	O
are	O
closest	O
to	O
the	O
final	O
output	O
layers	O
.	O

It	O
is	O
hard	O
to	O
qualitatively	O
analyze	O
the	O
effect	O
of	O
the	O
alterations	O
.	O

If	O
we	O
compare	O
each	O
final	O
activation	O
in	O
layer	O
2	O
to	O
its	O
corresponding	O
change	O
(	O
see	O
Figure	O
3	O
,	O
right	O
)	O
,	O
we	O
see	O
that	O
the	O
activations	O
are	O
not	O
simply	O
uniformly	O
enhanced	O
.	O

Instead	O
,	O
complex	O
suppression	O
and	O
enhancement	O
patterns	O
are	O
found	O
,	O
increasing	O
and	O
decreasing	O
activation	O
of	O
specific	O
pixels	O
.	O

Visualising	O
what	O
these	O
high	O
-	O
level	O
actually	O
do	O
is	O
an	O
open	O
problem	O
in	O
deep	Task
learning	Task
.	O

Dynamics	O
To	O
investigate	O
the	O
dynamics	O
,	O
a	O
small	O
2	O
-	O
layer	O
dasNet	Method
network	O
was	O
trained	O
for	O
different	O
values	O
of	O
T	O
.	O

Then	O
they	O
were	O
evaluated	O
by	O
allowing	O
them	O
to	O
run	O
for	O
[	O
0	O
..	O
9	O
]	O
steps	O
.	O

Figure	O
2	O
shows	O
results	O
of	O
training	O
dasNet	Method
on	O
CIFAR	Material
-	Material
100	Material
for	O
T	O
=	O
1	O
and	O
T	O
=	O
2	O
.	O

The	O
performance	O
goes	O
up	O
from	O
the	O
vanilla	O
CNN	Method
,	O
peaks	O
at	O
the	O
step	O
=	O
T	O
as	O
expected	O
,	O
and	O
reduces	O
but	O
stays	O
stable	O
after	O
that	O
.	O

So	O
even	O
though	O
the	O
dasNet	Method
was	O
trained	O
using	O
only	O
a	O
small	O
number	O
of	O
steps	O
,	O
the	O
dynamics	O
stay	O
stable	O
when	O
these	O
are	O
evaluated	O
for	O
as	O
many	O
as	O
10	O
steps	O
.	O

To	O
verify	O
whether	O
the	O
dasNet	Method
policy	O
is	O
actually	O
making	O
good	O
use	O
of	O
its	O
gates	O
,	O
we	O
estimate	O
their	O
information	O
content	O
in	O
the	O
following	O
way	O
:	O
The	O
gate	O
values	O
in	O
the	O
final	O
step	O
are	O
used	O
directly	O
for	O
classification	Task
.	O

The	O
hypothesis	O
is	O
that	O
if	O
the	O
gates	O
are	O
used	O
properly	O
,	O
then	O
their	O
activation	O
should	O
contain	O
information	O
that	O
is	O
relevant	O
for	O
classification	Task
.	O

For	O
this	O
purpose	O
,	O
a	O
dasNet	Method
that	O
was	O
trained	O
with	O
T	O
=	O
2	O
.	O

Then	O
using	O
only	O
the	O
final	O
gate	O
-	O
values	O
(	O
so	O
without	O
e.g.	O
the	O
output	O
of	O
the	O
classification	Method
layer	Method
)	O
,	O
a	O
classification	Method
using	O
15	Method
-	Method
nearest	Method
neighbour	Method
and	O
logistic	Method
regression	Method
was	O
performed	O
.	O

This	O
resulted	O
in	O
a	O
performance	O
of	O
40.70	O
%	O
and	O
45.74	O
%	O
correct	Metric
respectively	O
,	O
similar	O
to	O
the	O
performance	O
of	O
dasNet	Method
,	O
confirming	O
that	O
they	O
contain	O
significant	O
information	O
.	O

7	O
Conclusion	O
DasNet	Method
is	O
a	O
deep	Method
neural	Method
network	Method
with	O
feedback	Method
connections	O
that	O
are	O
learned	O
by	O
through	O
reinforcement	Method
learning	Method
to	O
direct	O
selective	O
internal	O
attention	O
to	O
certain	O
features	O
extracted	O
from	O
images	O
.	O

After	O
a	O
rapid	O
first	O
shot	O
image	Task
classification	Task
through	O
a	O
standard	O
stack	O
of	O
feedforward	Method
filters	Method
,	O
the	O
feedback	Method
can	O
actively	O
alter	O
the	O
importance	O
of	O
certain	O
filters	O
“	O
in	O
hindsight	O
”	O
,	O
correcting	O
the	O
initial	O
guess	O
via	O
additional	O
internal	O
“	O
thoughts	O
”	O
.	O

DasNet	Method
successfully	O
learned	O
to	O
correct	Metric
image	Task
misclassifications	Task
produced	O
by	O
a	O
fully	O
trained	O
feedforward	Method
Maxout	Method
network	Method
.	O

Its	O
active	O
,	O
selective	O
,	O
internal	O
spotlight	O
of	O
attention	O
enabled	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

Future	O
research	O
will	O
also	O
consider	O
more	O
complex	O
actions	O
that	O
spatially	O
focus	O
on	O
(	O
or	O
alter	O
)	O
parts	O
of	O
observed	O
images	O
.	O

Acknowledgments	O
We	O
acknowledge	O
Matthew	O
Luciw	O
for	O
his	O
discussions	O
and	O
for	O
providing	O
a	O
short	O
literature	O
review	O
,	O
included	O
in	O
the	O
Related	O
Work	O
section	O
.	O

