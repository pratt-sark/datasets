document	O
:	O
A	O
Fast	O
Unified	Method
Model	Method
for	O
Parsing	Task
and	Task
Sentence	Task
Understanding	Task
Tree	Method
-	Method
structured	Method
neural	Method
networks	Method
exploit	O
valuable	O
syntactic	O
parse	O
information	O
as	O
they	O
interpret	O
the	O
meanings	O
of	O
sentences	O
.	O

However	O
,	O
they	O
suffer	O
from	O
two	O
key	O
technical	O
problems	O
that	O
make	O
them	O
slow	O
and	O
unwieldy	O
for	O
large	Task
-	Task
scale	Task
NLP	Task
tasks	Task
:	O
they	O
usually	O
operate	O
on	O
parsed	O
sentences	O
and	O
they	O
do	O
not	O
directly	O
support	O
batched	Method
computation	Method
.	O

We	O
address	O
these	O
issues	O
by	O
introducing	O
the	O
Stack	Method
-	Method
augmented	Method
Parser	Method
-	Method
Interpreter	Method
Neural	Method
Network	Method
(	O
SPINN	Method
)	O
,	O
which	O
combines	O
parsing	Task
and	O
interpretation	Task
within	O
a	O
single	O
tree	Method
-	Method
sequence	Method
hybrid	Method
model	Method
by	O
integrating	O
tree	Method
-	Method
structured	Method
sentence	Method
interpretation	Method
into	O
the	O
linear	O
sequential	O
structure	O
of	O
a	O
shift	Method
-	Method
reduce	Method
parser	Method
.	O

Our	O
model	O
supports	O
batched	Task
computation	Task
for	O
a	O
speedup	O
of	O
up	O
to	O
25	O
over	O
other	O
tree	Method
-	Method
structured	Method
models	Method
,	O
and	O
its	O
integrated	Method
parser	Method
can	O
operate	O
on	O
unparsed	O
data	O
with	O
little	O
loss	O
in	O
accuracy	Metric
.	O

We	O
evaluate	O
it	O
on	O
the	O
Stanford	Task
NLI	Task
entailment	Task
task	Task
and	O
show	O
that	O
it	O
significantly	O
outperforms	O
other	O
sentence	Method
-	Method
encoding	Method
models	Method
.	O

section	O
:	O
Introduction	O
[	O
t	O
]	O
1	O
CompiledTikzPictures	O
/	O
paper	O
-	O
figure0.pdf	O
[	O
t	O
]	O
1	O
CompiledTikzPictures	O
/	O
paper	O
-	O
figure1.pdf	O
[	O
t	O
]	O
0.8	O
CompiledTikzPictures	O
/	O
paper	O
-	O
figure2.pdf	O
[	O
t	O
]	O
0.5	O
CompiledTikzPictures	O
/	O
paper	O
-	O
figure3.pdf	O
A	O
wide	O
range	O
of	O
current	O
models	O
in	O
NLP	Task
are	O
built	O
around	O
a	O
neural	Method
network	Method
component	Method
that	O
produces	O
vector	Method
representations	Method
of	Method
sentence	Method
meaning	Method
.	O

This	O
component	O
,	O
the	O
sentence	Method
encoder	Method
,	O
is	O
generally	O
formulated	O
as	O
a	O
learned	Method
parametric	Method
function	Method
from	O
a	O
sequence	O
of	O
word	O
vectors	O
to	O
a	O
sentence	O
vector	O
,	O
and	O
this	O
function	O
can	O
take	O
a	O
range	O
of	O
different	O
forms	O
.	O

Common	O
sentence	Method
encoders	Method
include	O
sequence	Method
-	Method
based	Method
recurrent	Method
neural	Method
network	Method
models	Method
(	O
RNNs	Method
,	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
with	O
Long	O
Short	O
-	O
Term	O
Memory	O
,	O
which	O
accumulate	O
information	O
over	O
the	O
sentence	O
sequentially	O
;	O
convolutional	Method
neural	Method
networks	Method
,	O
which	O
accumulate	O
information	O
using	O
filters	Method
over	O
short	O
local	O
sequences	O
of	O
words	O
or	O
characters	O
;	O
and	O
tree	Method
-	Method
structured	Method
recursive	Method
neural	Method
networks	Method
,	O
which	O
propagate	O
information	O
up	O
a	O
binary	O
parse	O
tree	O
.	O

Of	O
these	O
,	O
the	O
TreeRNN	Method
appears	O
to	O
be	O
the	O
principled	O
choice	O
,	O
since	O
meaning	O
in	O
natural	O
language	O
sentences	O
is	O
known	O
to	O
be	O
constructed	O
recursively	O
according	O
to	O
a	O
tree	O
structure	O
.	O

TreeRNNs	Method
have	O
shown	O
promise	O
,	O
but	O
have	O
largely	O
been	O
overlooked	O
in	O
favor	O
of	O
sequence	Method
-	Method
based	Method
RNNs	Method
because	O
of	O
their	O
incompatibility	O
with	O
batched	Method
computation	Method
and	O
their	O
reliance	O
on	O
external	Method
parsers	Method
.	O

Batched	Method
computation	Method
—	O
performing	O
synchronized	Task
computation	Task
across	O
many	O
examples	O
at	O
once	O
—	O
yields	O
order	O
-	O
of	O
-	O
magnitude	O
improvements	O
in	O
model	Metric
run	Metric
time	Metric
,	O
and	O
is	O
crucial	O
in	O
enabling	O
neural	Method
networks	Method
to	O
be	O
trained	O
efficiently	O
on	O
large	O
datasets	O
.	O

Because	O
TreeRNNs	Method
use	O
a	O
different	O
model	O
structure	O
for	O
each	O
sentence	O
,	O
as	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
efficient	O
batching	Task
is	O
impossible	O
in	O
standard	O
implementations	O
.	O

Partly	O
to	O
address	O
efficiency	Task
problems	Task
,	O
standard	O
TreeRNN	Method
models	Method
commonly	O
only	O
operate	O
on	O
sentences	O
that	O
have	O
already	O
been	O
processed	O
by	O
a	O
syntactic	Method
parser	Method
,	O
which	O
slows	O
and	O
complicates	O
the	O
use	O
of	O
these	O
models	O
at	O
test	O
time	O
for	O
most	O
applications	O
.	O

This	O
paper	O
introduces	O
a	O
new	O
model	O
to	O
address	O
both	O
these	O
issues	O
:	O
the	O
Stack	Method
-	Method
augmented	Method
Parser	Method
-	Method
Interpreter	Method
Neural	Method
Network	Method
,	O
or	O
SPINN	Method
,	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

SPINN	Method
executes	O
the	O
computations	O
of	O
a	O
tree	Method
-	Method
structured	Method
model	Method
in	O
a	O
linearized	O
sequence	O
,	O
and	O
can	O
incorporate	O
a	O
neural	Method
network	Method
parser	Method
that	O
produces	O
the	O
required	O
parse	O
structure	O
on	O
the	O
fly	O
.	O

This	O
design	O
improves	O
upon	O
the	O
TreeRNN	Method
architecture	Method
in	O
three	O
ways	O
:	O
At	O
test	O
time	O
,	O
it	O
can	O
simultaneously	O
parse	O
and	O
interpret	O
unparsed	O
sentences	O
,	O
removing	O
the	O
dependence	O
on	O
an	O
external	Method
parser	Method
at	O
nearly	O
no	O
additional	O
computational	Metric
cost	Metric
.	O

Secondly	O
,	O
it	O
supports	O
batched	Task
computation	Task
for	O
both	O
parsed	O
and	O
unparsed	O
sentences	O
,	O
yielding	O
dramatic	O
speedups	O
over	O
standard	O
TreeRNNs	Method
.	O

Finally	O
,	O
it	O
supports	O
a	O
novel	O
tree	Method
-	Method
sequence	Method
hybrid	Method
architecture	Method
for	O
handling	O
local	Task
linear	Task
context	Task
in	O
sentence	Task
interpretation	Task
.	O

This	O
model	O
is	O
a	O
basically	O
plausible	O
model	O
of	O
human	Task
sentence	Task
processing	Task
and	O
yields	O
substantial	O
accuracy	Metric
gains	O
over	O
pure	O
sequence	Method
-	Method
or	Method
tree	Method
-	Method
based	Method
models	Method
.	O

We	O
evaluate	O
SPINN	Method
on	O
the	O
Stanford	Task
Natural	Task
Language	Task
Inference	Task
entailment	Task
task	Task
,	O
and	O
find	O
that	O
it	O
significantly	O
outperforms	O
other	O
sentence	Method
-	Method
encoding	Method
-	Method
based	Method
models	Method
,	O
even	O
with	O
a	O
relatively	O
simple	O
and	O
underpowered	O
implementation	O
of	O
the	O
built	Method
-	Method
in	Method
parser	Method
.	O

We	O
also	O
find	O
that	O
SPINN	Method
yields	O
speed	O
increases	O
of	O
up	O
to	O
25	O
over	O
a	O
standard	O
TreeRNN	Method
implementation	Method
.	O

section	O
:	O
Related	O
work	O
There	O
is	O
a	O
fairly	O
long	O
history	O
of	O
work	O
on	O
building	O
neural	Method
network	Method
-	Method
based	Method
parsers	Method
that	O
use	O
the	O
core	O
operations	O
and	O
data	O
structures	O
from	O
transition	Method
-	Method
based	Method
parsing	Method
,	O
of	O
which	O
shift	Method
-	Method
reduce	Method
parsing	Method
is	O
a	O
variant	O
.	O

In	O
addition	O
,	O
there	O
has	O
been	O
recent	O
work	O
proposing	O
models	O
designed	O
primarily	O
for	O
generative	Task
language	Task
modeling	Task
tasks	Task
that	O
use	O
this	O
architecture	O
as	O
well	O
.	O

To	O
our	O
knowledge	O
,	O
SPINN	Method
is	O
the	O
first	O
model	O
to	O
use	O
this	O
architecture	O
for	O
the	O
purpose	O
of	O
sentence	Task
interpretation	Task
,	O
rather	O
than	O
parsing	Task
or	O
generation	Task
.	O

present	O
versions	O
of	O
the	O
TreeRNN	Method
model	Method
which	O
are	O
capable	O
of	O
operating	O
over	O
unparsed	O
inputs	O
.	O

However	O
,	O
these	O
methods	O
require	O
an	O
expensive	O
search	Method
process	Method
at	O
test	O
time	O
.	O

Our	O
model	O
presents	O
a	O
much	O
faster	O
alternative	O
approach	O
.	O

section	O
:	O
Our	O
model	O
:	O
SPINN	Method
subsection	O
:	O
Background	O
:	O
Shift	Method
-	Method
reduce	Method
parsing	Method
SPINN	Method
is	O
inspired	O
by	O
shift	Method
-	Method
reduce	Method
parsing	Method
,	O
which	O
builds	O
a	O
tree	O
structure	O
over	O
a	O
sequence	O
(	O
e.g.	O
,	O
a	O
natural	O
language	O
sentence	O
)	O
by	O
a	O
single	O
left	O
-	O
to	O
-	O
right	O
scan	O
over	O
its	O
tokens	O
.	O

The	O
formalism	O
is	O
widely	O
used	O
in	O
natural	Task
language	Task
parsing	Task
.	O

A	O
shift	Method
-	Method
reduce	Method
parser	Method
accepts	O
a	O
sequence	O
of	O
input	O
tokens	O
and	O
consumes	O
transitions	O
,	O
where	O
each	O
specifies	O
one	O
step	O
of	O
the	O
parsing	Method
process	Method
.	O

In	O
general	O
a	O
parser	Method
may	O
also	O
generate	O
these	O
transitions	O
on	O
the	O
fly	O
as	O
it	O
reads	O
the	O
tokens	O
.	O

It	O
proceeds	O
left	O
-	O
to	O
-	O
right	O
through	O
a	O
transition	O
sequence	O
,	O
combining	O
the	O
input	O
tokens	O
incrementally	O
into	O
a	O
tree	O
structure	O
.	O

For	O
any	O
binary	O
-	O
branching	O
tree	O
structure	O
over	O
words	O
,	O
this	O
requires	O
transitions	O
through	O
a	O
total	O
of	O
states	O
.	O

The	O
parser	Method
uses	O
two	O
auxiliary	O
data	O
structures	O
:	O
a	O
stack	O
of	O
partially	O
completed	O
subtrees	O
and	O
a	O
buffer	O
of	O
tokens	O
yet	O
to	O
be	O
parsed	O
.	O

The	O
parser	Method
is	O
initialized	O
with	O
the	O
stack	O
empty	O
and	O
the	O
buffer	O
containing	O
the	O
tokens	O
of	O
the	O
sentence	O
in	O
order	O
.	O

Let	O
denote	O
this	O
starting	O
state	O
.	O

It	O
next	O
proceeds	O
through	O
the	O
transition	O
sequence	O
,	O
where	O
each	O
transition	O
selects	O
one	O
of	O
the	O
two	O
following	O
operations	O
.	O

Below	O
,	O
the	O
symbol	O
denotes	O
the	O
cons	O
(	O
concatenation	O
)	O
operator	O
.	O

We	O
arbitrarily	O
choose	O
to	O
always	O
cons	O
on	O
the	O
left	O
in	O
the	O
notation	O
below	O
.	O

.	O

This	O
operation	O
pops	O
an	O
element	O
from	O
the	O
buffer	O
and	O
pushes	O
it	O
on	O
to	O
the	O
top	O
of	O
the	O
stack	O
.	O

.	O

This	O
operation	O
pops	O
the	O
top	O
two	O
elements	O
from	O
the	O
stack	O
,	O
merges	O
them	O
,	O
and	O
pushes	O
the	O
result	O
back	O
on	O
to	O
the	O
stack	O
.	O

subsection	O
:	O
Composition	O
and	O
representation	O
SPINN	Method
is	O
based	O
on	O
a	O
shift	Method
-	Method
reduce	Method
parser	Method
,	O
but	O
it	O
is	O
designed	O
to	O
produce	O
a	O
vector	Method
representation	Method
of	O
a	O
sentence	O
as	O
its	O
output	O
,	O
rather	O
than	O
a	O
tree	O
as	O
in	O
standard	O
shift	Method
-	Method
reduce	Method
parsing	Method
.	O

It	O
modifies	O
the	O
shift	Method
-	Method
reduce	Method
formalism	Method
by	O
using	O
fixed	O
length	O
vectors	O
to	O
represent	O
each	O
entry	O
in	O
the	O
stack	O
and	O
the	O
buffer	O
.	O

Correspondingly	O
,	O
its	O
reduce	Method
operation	Method
combines	O
two	O
vector	Method
representations	Method
from	O
the	O
stack	O
into	O
another	O
vector	O
using	O
a	O
neural	Method
network	Method
function	Method
.	O

paragraph	O
:	O
The	O
composition	Method
function	Method
When	O
a	O
reduce	Method
operation	Method
is	O
performed	O
,	O
the	O
vector	O
representations	O
of	O
two	O
tree	O
nodes	O
are	O
popped	O
off	O
of	O
the	O
stack	O
and	O
fed	O
into	O
a	O
composition	Method
function	Method
,	O
which	O
is	O
a	O
neural	Method
network	Method
function	Method
that	O
produces	O
a	O
representation	O
for	O
a	O
new	O
tree	O
node	O
that	O
is	O
the	O
parent	O
of	O
the	O
two	O
popped	O
nodes	O
.	O

This	O
new	O
node	O
is	O
pushed	O
on	O
to	O
the	O
stack	O
.	O

The	O
TreeLSTM	Method
composition	Method
function	Method
generalizes	O
the	O
LSTM	Method
neural	Method
network	Method
layer	Method
to	O
tree	O
-	O
rather	O
than	O
sequence	O
-	O
based	O
inputs	O
,	O
and	O
it	O
shares	O
with	O
the	O
LSTM	Method
the	O
idea	O
of	O
representing	O
intermediate	O
states	O
as	O
a	O
pair	O
of	O
an	O
active	Method
state	Method
representation	Method
and	O
a	O
memory	Method
representation	Method
.	O

Our	O
version	O
is	O
formulated	O
as	O
:	O
where	O
is	O
the	O
sigmoid	O
activation	O
function	O
,	O
is	O
the	O
elementwise	O
product	O
,	O
the	O
pairs	O
and	O
are	O
the	O
two	O
input	O
tree	O
nodes	O
popped	O
off	O
the	O
stack	O
,	O
and	O
is	O
an	O
optional	O
vector	O
-	O
valued	O
input	O
argument	O
which	O
is	O
either	O
empty	O
or	O
comes	O
from	O
an	O
external	O
source	O
like	O
the	O
tracking	Method
LSTM	Method
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
.	O

The	O
result	O
of	O
this	O
function	O
,	O
the	O
pair	O
,	O
is	O
placed	O
back	O
on	O
the	O
stack	O
.	O

Each	O
vector	O
-	O
valued	O
variable	O
listed	O
is	O
of	O
dimension	O
except	O
,	O
of	O
the	O
independent	O
dimension	O
.	O

paragraph	O
:	O
The	O
stack	O
and	O
buffer	O
The	O
stack	O
and	O
the	O
buffer	O
are	O
arrays	O
of	O
elements	O
each	O
(	O
for	O
sentences	O
of	O
up	O
to	O
words	O
)	O
,	O
with	O
the	O
two	O
-	O
dimensional	O
vectors	O
and	O
in	O
each	O
element	O
.	O

paragraph	O
:	O
Word	Method
representations	Method
We	O
use	O
word	Method
representations	Method
based	O
on	O
the	O
300D	O
vectors	O
provided	O
with	O
GloVe	Method
.	O

We	O
do	O
not	O
update	O
these	O
representations	O
during	O
training	O
.	O

Instead	O
,	O
we	O
use	O
a	O
learned	O
linear	Method
transformation	Method
to	O
map	O
each	O
input	O
word	O
vector	O
into	O
a	O
vector	O
pair	O
that	O
is	O
stored	O
in	O
the	O
buffer	O
:	O
subsection	O
:	O
The	O
tracking	Method
LSTM	Method
In	O
addition	O
to	O
the	O
stack	O
,	O
buffer	O
,	O
and	O
composition	O
function	O
,	O
our	O
full	Method
model	Method
includes	O
an	O
additional	O
component	O
:	O
the	O
tracking	Method
LSTM	Method
.	O

This	O
is	O
a	O
simple	O
sequence	Method
-	Method
based	Method
LSTM	Method
RNN	Method
that	O
operates	O
in	O
tandem	O
with	O
the	O
model	O
,	O
taking	O
inputs	O
from	O
the	O
buffer	O
and	O
stack	O
at	O
each	O
step	O
.	O

It	O
is	O
meant	O
to	O
maintain	O
a	O
low	O
-	O
resolution	O
summary	O
of	O
the	O
portion	O
of	O
the	O
sentence	O
that	O
has	O
been	O
processed	O
so	O
far	O
,	O
which	O
is	O
used	O
for	O
two	O
purposes	O
:	O
it	O
supplies	O
feature	Method
representations	Method
to	O
the	O
transition	Method
classifier	Method
,	O
which	O
allows	O
the	O
model	O
to	O
stand	O
alone	O
as	O
a	O
parser	Method
,	O
and	O
it	O
additionally	O
supplies	O
a	O
secondary	O
input	O
to	O
the	O
composition	Method
function	Method
—	O
see	O
(	O
[	O
reference	O
]	O
)	O
—allowing	O
context	O
information	O
to	O
enter	O
the	O
construction	Task
of	Task
sentence	Task
meaning	Task
and	O
forming	O
what	O
is	O
effectively	O
a	O
tree	Method
-	Method
sequence	Method
hybrid	Method
model	Method
.	O

The	O
tracking	Method
LSTM	Method
’s	Method
inputs	Method
(	O
yellow	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
are	O
the	O
top	O
element	O
of	O
the	O
buffer	O
(	O
which	O
would	O
be	O
moved	O
in	O
a	O
shift	O
operation	O
)	O
and	O
the	O
top	O
two	O
elements	O
of	O
the	O
stack	O
and	O
(	O
which	O
would	O
be	O
composed	O
in	O
a	O
reduce	O
operation	O
)	O
.	O

paragraph	O
:	O
Why	O
a	O
tree	Method
-	Method
sequence	Method
hybrid	Method
?	O
Lexical	Task
ambiguity	Task
is	O
ubiquitous	O
in	O
natural	O
language	O
.	O

Most	O
words	O
have	O
multiple	O
senses	O
or	O
meanings	O
,	O
and	O
it	O
is	O
generally	O
necessary	O
to	O
use	O
the	O
context	O
in	O
which	O
a	O
word	O
occurs	O
to	O
determine	O
which	O
of	O
its	O
senses	O
or	O
meanings	O
is	O
meant	O
in	O
a	O
given	O
sentence	O
.	O

Even	O
though	O
TreeRNNs	Method
are	O
more	O
effective	O
at	O
composing	O
meanings	O
in	O
principle	O
,	O
this	O
ambiguity	O
can	O
give	O
simpler	O
sequence	Method
-	Method
based	Method
sentence	Method
-	Method
encoding	Method
models	Method
an	O
advantage	O
:	O
when	O
a	O
sequence	Method
-	Method
based	Method
model	Method
first	O
processes	O
a	O
word	O
,	O
it	O
has	O
direct	O
access	O
to	O
a	O
state	O
vector	O
that	O
summarizes	O
the	O
left	O
context	O
of	O
that	O
word	O
,	O
which	O
acts	O
as	O
a	O
cue	O
for	O
disambiguation	Task
.	O

In	O
contrast	O
,	O
when	O
a	O
standard	O
tree	Method
-	Method
structured	Method
model	Method
first	O
processes	O
a	O
word	O
,	O
it	O
only	O
has	O
access	O
to	O
the	O
constituent	O
that	O
the	O
word	O
is	O
merging	O
with	O
,	O
which	O
is	O
often	O
just	O
a	O
single	O
additional	O
word	O
.	O

Feeding	O
a	O
context	Method
representation	Method
from	O
the	O
tracking	Method
LSTM	Method
into	O
the	O
composition	Method
function	Method
is	O
a	O
simple	O
and	O
efficient	O
way	O
to	O
mitigate	O
this	O
disadvantage	O
of	O
tree	Method
-	Method
structured	Method
models	Method
.	O

Using	O
left	O
linear	O
context	O
to	O
disambiguate	Task
is	O
also	O
a	O
plausible	O
model	O
of	O
human	Task
interpretation	Task
.	O

It	O
would	O
be	O
straightforward	O
to	O
augment	O
SPINN	Method
to	O
support	O
the	O
use	O
of	O
some	O
amount	O
of	O
right	O
-	O
side	O
context	O
as	O
well	O
,	O
but	O
this	O
would	O
add	O
complexity	O
to	O
the	O
model	O
that	O
we	O
think	O
is	O
largely	O
unnecessary	O
:	O
humans	O
are	O
very	O
effective	O
at	O
understanding	O
the	O
beginnings	O
of	O
sentences	O
before	O
having	O
seen	O
or	O
heard	O
the	O
ends	O
,	O
suggesting	O
that	O
it	O
is	O
possible	O
to	O
get	O
by	O
without	O
the	O
unavailable	O
right	O
-	O
side	O
context	O
.	O

subsection	O
:	O
Parsing	Task
:	O
Predicting	Task
transitions	Task
For	O
SPINN	Method
to	O
operate	O
on	O
unparsed	O
inputs	O
,	O
it	O
needs	O
to	O
produce	O
its	O
own	O
transition	O
sequence	O
rather	O
than	O
relying	O
on	O
an	O
external	Method
parser	Method
to	O
supply	O
it	O
as	O
part	O
of	O
the	O
input	O
.	O

To	O
do	O
this	O
,	O
the	O
model	O
predicts	O
at	O
each	O
step	O
using	O
a	O
simple	O
two	O
-	O
way	O
softmax	Method
classifier	Method
whose	O
input	O
is	O
the	O
state	O
of	O
the	O
tracking	Method
LSTM	Method
:	O
The	O
above	O
model	O
is	O
nearly	O
the	O
simplest	O
viable	O
implementation	O
of	O
a	O
transition	O
decision	O
function	O
.	O

In	O
contrast	O
,	O
the	O
decision	Method
functions	Method
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
transition	Method
-	Method
based	Method
parsers	Method
tend	O
to	O
use	O
significantly	O
richer	O
feature	O
sets	O
as	O
inputs	O
,	O
including	O
features	O
containing	O
information	O
about	O
several	O
upcoming	O
words	O
on	O
the	O
buffer	O
.	O

The	O
value	O
is	O
a	O
function	O
of	O
only	O
the	O
very	O
top	O
of	O
the	O
buffer	O
and	O
the	O
top	O
two	O
stack	O
elements	O
at	O
each	O
timestep	O
.	O

At	O
test	O
time	O
,	O
the	O
model	O
uses	O
whichever	O
transition	O
(	O
i.e.	O
,	O
shift	O
or	O
reduce	O
)	O
is	O
assigned	O
a	O
higher	O
(	O
unnormalized	O
)	O
probability	O
.	O

The	O
prediction	Method
function	Method
is	O
trained	O
to	O
mimic	O
the	O
decisions	O
of	O
an	O
external	Method
parser	Method
.	O

These	O
decisions	O
are	O
used	O
as	O
inputs	O
to	O
the	O
model	O
during	O
training	Task
.	O

For	O
SNLI	Material
,	O
we	O
use	O
the	O
binary	Method
Stanford	Method
PCFG	Method
Parser	Method
parses	Method
that	O
are	O
included	O
with	O
the	O
corpus	O
.	O

We	O
did	O
not	O
find	O
scheduled	Method
sampling	Method
—having	O
the	O
model	O
use	O
its	O
own	O
transition	O
decisions	O
sometimes	O
at	O
training	O
time	O
—	O
to	O
help	O
.	O

subsection	O
:	O
Implementation	O
issues	O
paragraph	O
:	O
Representing	O
the	O
stack	O
efficiently	O
A	O
naïve	O
implementation	O
of	O
SPINN	Method
needs	O
to	O
handle	O
a	O
size	O
stack	O
at	O
each	O
timestep	O
,	O
any	O
element	O
of	O
which	O
may	O
be	O
involved	O
in	O
later	O
computations	O
.	O

A	O
naïve	O
backpropagation	Method
implementation	Method
would	O
then	O
require	O
storing	O
each	O
of	O
the	O
stacks	O
for	O
a	O
backward	O
pass	O
,	O
leading	O
to	O
a	O
per	Metric
-	Metric
example	Metric
space	Metric
requirement	Metric
of	O
floats	O
.	O

This	O
requirement	O
is	O
prohibitively	O
large	O
for	O
significant	O
batch	O
sizes	O
or	O
sentence	O
lengths	O
.	O

Such	O
a	O
naïve	O
implementation	O
would	O
also	O
require	O
copying	O
a	O
largely	O
unchanged	O
stack	O
at	O
each	O
timestep	O
,	O
since	O
each	O
shift	O
or	O
reduce	O
operation	O
writes	O
only	O
one	O
new	O
representation	O
to	O
the	O
top	O
of	O
the	O
stack	O
.	O

We	O
propose	O
a	O
space	Method
-	Method
efficient	Method
stack	Method
representation	Method
inspired	O
by	O
the	O
zipper	Method
technique	Method
that	O
we	O
call	O
thin	Method
stack	Method
.	O

For	O
each	O
input	O
sentence	O
,	O
we	O
represent	O
the	O
stack	O
with	O
a	O
single	O
matrix	O
.	O

Each	O
row	O
(	O
for	O
)	O
represents	O
the	O
top	O
of	O
the	O
actual	O
stack	O
at	O
timestep	O
.	O

At	O
each	O
timestep	O
we	O
can	O
shift	O
a	O
new	O
element	O
onto	O
the	O
stack	O
,	O
or	O
reduce	O
the	O
top	O
two	O
elements	O
of	O
the	O
stack	O
into	O
a	O
single	O
element	O
.	O

To	O
shift	O
an	O
element	O
from	O
the	O
buffer	O
to	O
the	O
top	O
of	O
the	O
stack	O
at	O
timestep	O
,	O
we	O
simply	O
write	O
it	O
into	O
the	O
location	O
.	O

In	O
order	O
to	O
perform	O
the	O
reduce	Task
operation	Task
,	O
we	O
need	O
to	O
retrieve	O
the	O
top	O
two	O
elements	O
of	O
the	O
actual	O
stack	O
.	O

We	O
maintain	O
a	O
queue	O
of	O
pointers	O
into	O
which	O
contains	O
the	O
row	O
indices	O
of	O
which	O
are	O
still	O
present	O
in	O
the	O
actual	O
stack	O
.	O

The	O
top	O
two	O
elements	O
of	O
the	O
stack	O
can	O
be	O
found	O
by	O
using	O
the	O
final	O
two	O
pointers	O
in	O
the	O
queue	O
.	O

These	O
retrieved	O
elements	O
are	O
used	O
to	O
perform	O
the	O
reduce	Method
operation	Method
,	O
which	O
modifies	O
to	O
mark	O
that	O
some	O
rows	O
of	O
have	O
now	O
been	O
replaced	O
in	O
the	O
actual	O
stack	O
.	O

Algorithm	O
[	O
reference	O
]	O
describes	O
the	O
full	O
mechanics	O
of	O
a	O
stack	Task
feedforward	Task
in	O
this	O
compressed	Method
representation	Method
.	O

It	O
operates	O
on	O
the	O
single	O
matrix	O
and	O
a	O
backpointer	O
queue	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
an	O
example	O
run	O
.	O

[	O
t	O
]	O
The	O
thin	Method
stack	Method
algorithm	Method
[	O
1	O
]	O
StepbufferTop	O
,	O
,	O
,	O
,	O
=	O
shift	O
[	O
]	O
:	O
=	O
bufferTop	O
=	O
reduce	O
right	O
:	O
=	O
[	O
.pop	O
(	O
)	O
]	O
left	O
:	O
=	O
[	O
.pop	O
(	O
)	O
]	O
[	O
]	O
:	O
=	O
Composeleft	O
,	O
right	O
.push	O
(	O
)	O
This	O
stack	Method
representation	Method
requires	O
substantially	O
less	O
space	O
.	O

It	O
stores	O
each	O
element	O
involved	O
in	O
the	O
feedforward	Task
computation	Task
exactly	O
once	O
,	O
meaning	O
that	O
this	O
representation	O
can	O
still	O
support	O
efficient	O
backpropagation	Method
.	O

Furthermore	O
,	O
all	O
of	O
the	O
updates	O
to	O
and	O
can	O
be	O
performed	O
batched	O
and	O
in	O
-	O
place	O
on	O
a	O
GPU	O
,	O
yielding	O
substantial	O
speed	O
gains	O
over	O
both	O
a	O
more	O
naïve	O
SPINN	Method
implementation	O
and	O
a	O
standard	O
TreeRNN	Method
implementation	Method
.	O

We	O
describe	O
speed	O
results	O
in	O
Section	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Preparing	O
the	O
data	O
At	O
training	O
time	O
,	O
SPINN	Method
requires	O
both	O
a	O
transition	O
sequence	O
and	O
a	O
token	O
sequence	O
as	O
its	O
inputs	O
for	O
each	O
sentence	O
.	O

The	O
token	O
sequence	O
is	O
simply	O
the	O
words	O
in	O
the	O
sentence	O
in	O
order	O
.	O

can	O
be	O
obtained	O
from	O
any	O
constituency	O
parse	O
for	O
the	O
sentence	O
by	O
first	O
converting	O
that	O
parse	O
into	O
an	O
unlabeled	Method
binary	Method
parse	Method
,	O
then	O
linearizing	O
it	O
(	O
with	O
the	O
usual	O
in	Method
-	Method
order	Method
traversal	Method
)	O
,	O
then	O
taking	O
each	O
word	O
token	O
as	O
a	O
shift	O
transition	O
and	O
each	O
‘	O
)	O
’	O
as	O
a	O
reduce	O
transition	O
,	O
as	O
here	O
:	O
Unlabeled	O
binary	O
parse	O
:	O
(	O
(	O
the	O
cat	O
)	O
(	O
sat	O
down	O
)	O
)	O
x	O
:	O
the	O
,	O
cat	O
,	O
sat	O
,	O
downa	O
:	O
shift	O
,	O
shift	O
,	O
reduce	O
,	O
shift	O
,	O
shift	O
,	O
reduce	O
,	O
reduce	O
paragraph	O
:	O
Handling	O
variable	O
sentence	O
lengths	O
For	O
any	O
sentence	Method
model	Method
to	O
be	O
trained	O
with	O
batched	Task
computation	Task
,	O
it	O
is	O
necessary	O
to	O
pad	O
or	O
crop	O
sentences	O
to	O
a	O
fixed	O
length	O
.	O

We	O
fix	O
this	O
length	O
at	O
words	O
,	O
longer	O
than	O
about	O
98	O
%	O
of	O
sentences	O
in	O
SNLI	Material
.	O

Transition	O
sequences	O
are	O
cropped	O
at	O
the	O
left	O
or	O
padded	O
at	O
the	O
left	O
with	O
shift	O
s.	O
Token	O
sequences	O
are	O
then	O
cropped	O
or	O
padded	O
with	O
empty	O
tokens	O
at	O
the	O
left	O
to	O
match	O
the	O
number	O
of	O
shift	O
s	O
added	O
or	O
removed	O
from	O
,	O
and	O
can	O
then	O
be	O
padded	O
with	O
empty	O
tokens	O
at	O
the	O
right	O
to	O
meet	O
the	O
desired	O
length	O
.	O

subsection	O
:	O
TreeRNN	Method
-	Method
equivalence	Method
Without	O
the	O
addition	O
of	O
the	O
tracking	Method
LSTM	Method
,	O
SPINN	Method
(	O
in	O
particular	O
the	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
variant	Method
,	O
for	O
parsed	O
input	O
,	O
no	O
tracking	Task
)	O
is	O
precisely	O
equivalent	O
to	O
a	O
conventional	O
tree	Method
-	Method
structured	Method
neural	Method
network	Method
model	Method
in	O
the	O
function	O
that	O
it	O
computes	O
,	O
and	O
therefore	O
it	O
also	O
has	O
the	O
same	O
learning	O
dynamics	O
.	O

In	O
both	O
,	O
the	O
representation	O
of	O
each	O
sentence	O
consists	O
of	O
the	O
representations	O
of	O
the	O
words	O
combined	O
recursively	O
using	O
a	O
TreeRNN	Method
composition	Method
function	Method
(	O
in	O
our	O
case	O
,	O
the	O
TreeLSTM	Method
function	Method
)	O
.	O

SPINN	Method
,	O
however	O
,	O
is	O
dramatically	O
faster	O
,	O
and	O
supports	O
both	O
integrated	Task
parsing	Task
and	O
a	O
novel	O
approach	O
to	O
context	O
through	O
the	O
tracking	Method
LSTM	Method
.	O

subsection	O
:	O
Inference	Metric
speed	Metric
In	O
this	O
section	O
,	O
we	O
compare	O
the	O
test	Metric
-	Metric
time	Metric
speed	Metric
of	O
our	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
with	O
an	O
equivalent	O
TreeRNN	Method
implemented	O
in	O
the	O
conventional	O
fashion	O
and	O
with	O
a	O
standard	O
RNN	Method
sequence	Method
model	Method
.	O

While	O
the	O
full	O
models	O
evaluated	O
below	O
are	O
implemented	O
and	O
trained	O
using	O
Theano	O
,	O
which	O
is	O
reasonably	O
efficient	O
but	O
not	O
perfect	O
for	O
our	O
model	O
,	O
we	O
wish	O
to	O
compare	O
well	O
-	O
optimized	O
implementations	O
of	O
all	O
three	O
models	O
.	O

To	O
do	O
this	O
,	O
we	O
reimplement	O
the	O
feedforward	Method
of	Method
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
and	O
an	O
LSTM	Method
RNN	Method
baseline	Method
in	O
C	Method
++/	Method
CUDA	Method
,	O
and	O
compare	O
that	O
implementation	O
with	O
a	O
CPU	Method
-	Method
based	Method
C	Method
++/	Method
Eigen	Method
TreeRNN	Method
implementation	Method
from	O
,	O
which	O
we	O
modified	O
to	O
perform	O
exactly	O
the	O
same	O
computations	O
as	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
.	O

TreeRNNs	Method
like	O
this	O
can	O
only	O
operate	O
on	O
a	O
single	O
example	O
at	O
a	O
time	O
and	O
are	O
thus	O
poorly	O
suited	O
for	O
GPU	Task
computation	Task
.	O

1	O
!	O

CompiledTikzPictures	O
/	O
paper	O
-	O
figure4.pdf	O
Each	O
model	O
is	O
restricted	O
to	O
run	O
on	O
sentences	O
of	O
30	O
tokens	O
or	O
fewer	O
.	O

We	O
fix	O
the	O
model	O
dimension	O
and	O
the	O
word	O
embedding	O
dimension	O
at	O
300	O
.	O

We	O
run	O
the	O
CPU	Metric
performance	Metric
test	O
on	O
a	O
2.20	O
GHz	O
16	O
-	O
core	O
Intel	O
Xeon	O
E5	O
-	O
2660	O
processor	O
with	O
hyperthreading	O
enabled	O
.	O

We	O
test	O
our	O
thin	Method
-	Method
stack	Method
implementation	Method
and	O
the	O
RNN	Method
model	Method
on	O
an	O
NVIDIA	Method
Titan	Method
X	Method
GPU	Method
.	O

Figure	O
[	O
reference	O
]	O
compares	O
the	O
sentence	Metric
encoding	Metric
speed	Metric
of	O
the	O
three	O
models	O
on	O
random	O
input	O
data	O
.	O

We	O
observe	O
a	O
substantial	O
difference	O
in	O
runtime	Metric
between	O
the	O
CPU	Method
and	Method
thin	Method
-	Method
stack	Method
implementations	Method
that	O
increases	O
with	O
batch	O
size	O
.	O

With	O
a	O
large	O
but	O
practical	O
batch	O
size	O
of	O
512	O
,	O
the	O
largest	O
on	O
which	O
we	O
tested	O
the	O
TreeRNN	Method
,	O
our	O
model	O
is	O
about	O
25	O
faster	O
than	O
the	O
standard	O
CPU	Method
implementation	Method
,	O
and	O
about	O
4	O
slower	O
than	O
the	O
RNN	Method
baseline	Method
.	O

Though	O
this	O
experiment	O
only	O
covers	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
,	O
the	O
results	O
should	O
be	O
similar	O
for	O
the	O
full	O
SPINN	Method
model	Method
:	O
most	O
of	O
the	O
computation	O
involved	O
in	O
running	O
SPINN	Method
is	O
involved	O
in	O
populating	O
the	O
buffer	O
,	O
applying	O
the	O
composition	Method
function	Method
,	O
and	O
manipulating	O
the	O
buffer	O
and	O
the	O
stack	O
,	O
with	O
the	O
low	Method
-	Method
dimensional	Method
tracking	Method
and	Method
parsing	Method
components	Method
adding	O
only	O
a	O
small	O
additional	O
load	O
.	O

section	O
:	O
NLI	Task
Experiments	O
We	O
evaluate	O
SPINN	Method
on	O
the	O
task	O
of	O
natural	Task
language	Task
inference	Task
.	O

NLI	Task
is	O
a	O
sentence	Task
pair	Task
classification	Task
task	Task
,	O
in	O
which	O
a	O
model	O
reads	O
two	O
sentences	O
(	O
a	O
premise	O
and	O
a	O
hypothesis	O
)	O
,	O
and	O
outputs	O
a	O
judgment	O
of	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
,	O
reflecting	O
the	O
relationship	O
between	O
the	O
meanings	O
of	O
the	O
two	O
sentences	O
.	O

Below	O
is	O
an	O
example	O
sentence	O
pair	O
and	O
judgment	O
from	O
the	O
SNLI	Material
corpus	Material
which	O
we	O
use	O
in	O
our	O
experiments	O
:	O
Premise	O
:	O
Girl	O
in	O
a	O
red	O
coat	O
,	O
blue	O
head	O
wrap	O
and	O
jeans	O
is	O
making	O
a	O
snow	O
angel	O
.	O

Hypothesis	O
:	O
A	O
girl	O
outside	O
plays	O
in	O
the	O
snow	O
.	O

Label	O
:	O
entailment	Material
SNLI	Material
is	O
a	O
corpus	O
of	O
570k	O
human	O
-	O
labeled	O
pairs	O
of	O
scene	O
descriptions	O
like	O
this	O
one	O
.	O

We	O
use	O
the	O
standard	O
train	O
–	O
test	O
split	O
and	O
ignore	O
unlabeled	O
examples	O
,	O
which	O
leaves	O
about	O
549k	O
examples	O
for	O
training	O
,	O
9	O
,	O
842	O
for	O
development	Task
,	O
and	O
9	O
,	O
824	O
for	O
testing	O
.	O

SNLI	Material
labels	O
are	O
roughly	O
balanced	O
,	O
with	O
the	O
most	O
frequent	O
label	O
,	O
entailment	O
,	O
making	O
up	O
34.2	O
%	O
of	O
the	O
test	O
set	O
.	O

Although	O
NLI	Task
is	O
framed	O
as	O
a	O
simple	O
three	Task
-	Task
way	Task
classification	Task
task	Task
,	O
it	O
is	O
nonetheless	O
an	O
effective	O
way	O
of	O
evaluating	O
the	O
ability	O
of	O
a	O
model	O
to	O
extract	O
broadly	O
informative	Task
representations	Task
of	Task
sentence	Task
meaning	Task
.	O

In	O
order	O
for	O
a	O
model	O
to	O
perform	O
reliably	O
well	O
on	O
NLI	Task
,	O
it	O
must	O
be	O
able	O
to	O
represent	O
and	O
reason	O
with	O
the	O
core	O
phenomena	O
of	O
natural	O
language	O
semantics	O
,	O
including	O
quantification	O
,	O
coreference	O
,	O
scope	O
,	O
and	O
several	O
types	O
of	O
ambiguity	O
.	O

subsection	O
:	O
Applying	O
SPINN	Method
to	O
SNLI	Material
paragraph	O
:	O
Creating	O
a	O
sentence	Method
-	Method
pair	Method
classifier	Method
To	O
classify	O
an	O
SNLI	Material
sentence	O
pair	O
,	O
we	O
run	O
two	O
copies	O
of	O
SPINN	Method
with	O
shared	O
parameters	O
:	O
one	O
on	O
the	O
premise	O
sentence	O
and	O
another	O
on	O
the	O
hypothesis	O
sentence	O
.	O

We	O
then	O
use	O
their	O
outputs	O
(	O
the	O
states	O
at	O
the	O
top	O
of	O
each	O
stack	O
at	O
time	O
)	O
to	O
construct	O
a	O
feature	O
vector	O
for	O
the	O
pair	O
.	O

This	O
feature	O
vector	O
consists	O
of	O
the	O
concatenation	O
of	O
these	O
two	O
sentence	O
vectors	O
,	O
their	O
difference	O
,	O
and	O
their	O
elementwise	Method
product	Method
:	O
This	O
feature	O
vector	O
is	O
then	O
passed	O
to	O
a	O
series	O
of	O
1024D	Method
ReLU	Method
neural	Method
network	Method
layers	Method
(	O
i.e.	O
,	O
an	O
MLP	Method
;	O
the	O
number	O
of	O
layers	O
is	O
tuned	O
as	O
a	O
hyperparameter	O
)	O
,	O
then	O
passed	O
into	O
a	O
linear	Method
transformation	Method
,	O
and	O
then	O
finally	O
passed	O
to	O
a	O
softmax	Method
layer	Method
,	O
which	O
yields	O
a	O
distribution	O
over	O
the	O
three	O
labels	O
.	O

paragraph	O
:	O
The	O
objective	Metric
function	Metric
Our	O
objective	O
combines	O
a	O
cross	Metric
-	Metric
entropy	Metric
objective	Metric
for	O
the	O
SNLI	Material
classification	O
task	O
,	O
cross	Metric
-	Metric
entropy	Metric
objectives	Metric
and	O
for	O
the	O
parsing	Task
decision	Task
for	O
each	O
of	O
the	O
two	O
sentences	O
at	O
each	O
step	O
,	O
and	O
an	O
L2	Method
regularization	Method
term	Method
on	O
the	O
trained	O
parameters	O
.	O

The	O
terms	O
are	O
weighted	O
using	O
the	O
tuned	O
hyperparameters	Method
and	O
:	O
paragraph	O
:	O
Initialization	Task
,	O
optimization	Task
,	O
and	O
tuning	Task
We	O
initialize	O
the	O
model	O
parameters	O
using	O
the	O
nonparametric	Method
strategy	Method
of	O
,	O
with	O
the	O
exception	O
of	O
the	O
softmax	O
classifier	O
parameters	O
,	O
which	O
we	O
initialize	O
using	O
random	O
uniform	O
samples	O
from	O
.	O

We	O
use	O
minibatch	Method
SGD	Method
with	O
the	O
RMSProp	Method
optimizer	Method
and	O
a	O
tuned	O
starting	Metric
learning	Metric
rate	Metric
that	O
decays	O
by	O
a	O
factor	O
of	O
0.75	O
every	O
10k	O
steps	O
.	O

We	O
apply	O
both	O
dropout	Method
and	Method
batch	Method
normalization	Method
to	O
the	O
output	O
of	O
the	O
word	Method
embedding	Method
projection	Method
layer	Method
and	O
to	O
the	O
feature	O
vectors	O
that	O
serve	O
as	O
the	O
inputs	O
and	O
outputs	O
to	O
the	O
MLP	Method
that	O
precedes	O
the	O
final	O
entailment	Method
classifier	Method
.	O

We	O
train	O
each	O
model	O
for	O
250k	O
steps	O
in	O
each	O
run	O
,	O
using	O
a	O
batch	O
size	O
of	O
32	O
.	O

We	O
track	O
each	O
model	O
’s	O
performance	O
on	O
the	O
development	O
set	O
during	O
training	O
and	O
save	O
parameters	O
when	O
this	O
performance	O
reaches	O
a	O
new	O
peak	O
.	O

We	O
use	O
early	Method
stopping	Method
,	O
evaluating	O
on	O
the	O
test	O
set	O
using	O
the	O
parameters	O
that	O
perform	O
best	O
on	O
the	O
development	O
set	O
.	O

We	O
use	O
random	Method
search	Method
to	O
tune	O
the	O
hyperparameters	O
of	O
each	O
model	O
,	O
setting	O
the	O
ranges	O
for	O
search	O
for	O
each	O
hyperparameter	O
heuristically	O
(	O
and	O
validating	O
the	O
reasonableness	O
of	O
the	O
ranges	O
on	O
the	O
development	O
set	O
)	O
,	O
and	O
then	O
launching	O
eight	O
copies	O
of	O
each	O
experiment	O
each	O
with	O
newly	O
sampled	O
hyperparameters	O
from	O
those	O
ranges	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
hyperparameters	O
used	O
in	O
the	O
best	O
run	O
of	O
each	O
model	O
.	O

subsection	O
:	O
Models	O
evaluated	O
We	O
evaluate	O
four	O
models	O
.	O

The	O
four	O
all	O
use	O
the	O
sentence	Method
-	Method
pair	Method
classifier	Method
architecture	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
differ	O
only	O
in	O
the	O
function	O
computing	O
the	O
sentence	Method
encodings	Method
.	O

First	O
,	O
a	O
single	Method
-	Method
layer	Method
LSTM	Method
RNN	Method
serves	O
as	O
a	O
baseline	Method
encoder	Method
.	O

Next	O
,	O
the	O
minimal	O
SPINN	Method
-	O
PI	O
-	O
NT	O
model	O
(	O
equivalent	O
to	O
a	O
TreeLSTM	Method
)	O
introduces	O
the	O
SPINN	Method
model	O
design	O
.	O

SPINN	Method
-	O
PI	O
adds	O
the	O
tracking	Method
LSTM	Method
to	O
that	O
design	O
.	O

Finally	O
,	O
the	O
full	O
SPINN	Method
adds	O
the	O
integrated	Method
parser	Method
.	O

We	O
compare	O
our	O
models	O
against	O
several	O
baselines	O
,	O
including	O
the	O
strongest	O
published	O
non	Method
-	Method
neural	Method
network	Method
-	O
based	O
result	O
from	O
and	O
previous	O
neural	Method
network	Method
models	Method
built	O
around	O
several	O
types	O
of	O
sentence	Method
encoders	Method
.	O

subsection	O
:	O
Results	O
Table	O
[	O
reference	O
]	O
shows	O
our	O
results	O
on	O
SNLI	Material
.	O

For	O
the	O
full	O
SPINN	Method
,	O
we	O
also	O
report	O
a	O
measure	O
of	O
agreement	Metric
between	O
this	O
model	O
’s	O
parses	Method
and	O
the	O
parses	O
included	O
with	O
SNLI	Material
,	O
calculated	O
as	O
classification	O
accuracy	Metric
over	O
transitions	O
averaged	O
across	O
timesteps	O
.	O

We	O
find	O
that	O
the	O
bare	Method
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
model	Method
performs	O
little	O
better	O
than	O
the	O
RNN	Method
baseline	Method
,	O
but	O
that	O
SPINN	Method
-	O
PI	O
with	O
the	O
added	O
tracking	Method
LSTM	Method
performs	O
well	O
.	O

The	O
success	O
of	O
SPINN	Method
-	O
PI	O
,	O
which	O
is	O
the	O
hybrid	Method
tree	Method
-	Method
sequence	Method
model	Method
,	O
suggests	O
that	O
the	O
tree	Method
-	Method
and	Method
sequence	Method
-	Method
based	Method
encoding	Method
methods	Method
are	O
at	O
least	O
partially	O
complementary	O
,	O
with	O
the	O
sequence	Method
model	Method
presumably	O
providing	O
useful	O
local	Task
word	Task
disambiguation	Task
.	O

The	O
full	O
SPINN	Method
model	O
with	O
its	O
relatively	O
weak	O
internal	Method
parser	Method
performs	O
slightly	O
less	O
well	O
,	O
but	O
nonetheless	O
robustly	O
exceeds	O
the	O
performance	O
of	O
the	O
RNN	Method
baseline	Method
.	O

Both	O
SPINN	Method
-	O
PI	O
and	O
the	O
full	O
SPINN	Method
significantly	O
outperform	O
all	O
previous	O
sentence	Method
-	Method
encoding	Method
models	Method
.	O

Most	O
notably	O
,	O
these	O
models	O
outperform	O
the	O
tree	Method
-	Method
based	Method
CNN	Method
of	Method
,	O
which	O
also	O
uses	O
tree	Method
-	Method
structured	Method
composition	Method
for	O
local	Task
feature	Task
extraction	Task
,	O
but	O
uses	O
simpler	O
pooling	Method
techniques	Method
to	O
build	O
sentence	O
features	O
in	O
the	O
interest	O
of	O
efficiency	O
.	O

Our	O
results	O
show	O
that	O
a	O
model	O
that	O
uses	O
tree	Method
-	Method
structured	Method
composition	Method
fully	O
(	O
SPINN	Method
)	O
outperforms	O
one	O
which	O
uses	O
it	O
only	O
partially	O
(	O
tree	Method
-	Method
based	Method
CNN	Method
)	O
,	O
which	O
in	O
turn	O
outperforms	O
one	O
which	O
does	O
not	O
use	O
it	O
at	O
all	O
(	O
RNN	Method
)	O
.	O

The	O
full	O
SPINN	Method
performed	O
moderately	O
well	O
at	O
reproducing	O
the	O
Stanford	O
Parser	O
’s	O
parses	O
of	O
the	O
SNLI	Material
data	O
at	O
a	O
transition	O
-	O
by	O
-	O
transition	O
level	O
,	O
with	O
92.4	O
%	O
accuracy	Metric
at	O
test	O
time	O
.	O

However	O
,	O
its	O
transition	Metric
prediction	Metric
errors	Metric
are	O
fairly	O
evenly	O
distributed	O
across	O
sentences	O
,	O
and	O
most	O
sentences	O
were	O
assigned	O
partially	O
invalid	O
transition	O
sequences	O
that	O
either	O
left	O
a	O
few	O
words	O
out	O
of	O
the	O
final	O
representation	O
or	O
incorporated	O
a	O
few	O
padding	O
tokens	O
into	O
the	O
final	O
representation	O
.	O

subsection	O
:	O
Discussion	O
The	O
use	O
of	O
tree	O
structure	O
improves	O
the	O
performance	O
of	O
sentence	Method
-	Method
encoding	Method
models	Method
for	O
SNLI	Material
.	O

We	O
suspect	O
that	O
this	O
improvement	O
is	O
largely	O
due	O
to	O
the	O
more	O
efficient	O
learning	O
of	O
accurate	Task
generalizations	Task
overall	O
,	O
and	O
not	O
to	O
any	O
particular	O
few	O
phenomena	O
.	O

However	O
,	O
some	O
patterns	O
are	O
identifiable	O
in	O
the	O
results	O
.	O

While	O
all	O
four	O
models	O
under	O
study	O
have	O
trouble	O
with	O
negation	O
,	O
the	O
tree	O
-	O
structured	O
SPINN	Method
models	O
do	O
quite	O
substantially	O
better	O
on	O
these	O
pairs	O
.	O

This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
parse	O
trees	O
make	O
the	O
scope	O
of	O
any	O
instance	O
of	O
negation	O
(	O
the	O
portion	O
of	O
the	O
sentence	O
’s	O
content	O
that	O
is	O
negated	O
)	O
relatively	O
easy	O
to	O
identify	O
and	O
separate	O
from	O
the	O
rest	O
of	O
the	O
sentence	O
.	O

For	O
test	O
set	O
sentence	O
pairs	O
like	O
the	O
one	O
below	O
where	O
negation	O
(	O
not	O
or	O
n’t	O
)	O
does	O
not	O
appear	O
in	O
the	O
premise	O
but	O
does	O
appear	O
in	O
the	O
hypothesis	O
,	O
the	O
RNN	Method
shows	O
accuracy	Metric
,	O
while	O
all	O
three	O
tree	Method
-	Method
structured	Method
models	Method
exceed	O
.	O

Only	O
the	O
RNN	Method
got	O
the	O
below	O
example	O
wrong	O
:	O
Premise	O
:	O
The	O
rhythmic	O
gymnast	O
completes	O
her	O
floor	O
exercise	O
at	O
the	O
competition	O
.	O

Hypothesis	O
:	O
The	O
gymnast	O
can	O
not	O
finish	O
her	O
exercise	O
.	O

Label	O
:	O
contradiction	O
Note	O
that	O
the	O
presence	O
of	O
negation	O
in	O
the	O
hypothesis	O
is	O
correlated	O
with	O
a	O
label	O
of	O
contradiction	O
in	O
SNLI	Material
,	O
but	O
not	O
as	O
strongly	O
as	O
one	O
might	O
intuit	O
—	O
only	O
of	O
these	O
examples	O
in	O
the	O
test	O
set	O
are	O
labeled	O
as	O
contradictions	O
.	O

In	O
addition	O
,	O
it	O
seems	O
that	O
tree	Method
-	Method
structured	Method
models	Method
,	O
and	O
especially	O
the	O
tree	Method
-	Method
sequence	Method
hybrid	Method
models	Method
,	O
are	O
more	O
effective	O
than	O
RNNs	Method
at	O
extracting	O
informative	Task
representations	Task
of	Task
long	Task
sentences	Task
.	O

The	O
RNN	Method
model	Method
falls	O
off	O
in	O
test	O
accuracy	Metric
more	O
quickly	O
with	O
increasing	O
sentence	O
length	O
than	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
,	O
which	O
in	O
turn	O
falls	O
of	O
substantially	O
faster	O
than	O
the	O
two	O
hybrid	Method
models	Method
,	O
repeating	O
a	O
pattern	O
seen	O
more	O
dramatically	O
on	O
artificial	O
data	O
in	O
.	O

On	O
pairs	O
with	O
premises	O
of	O
20	O
or	O
more	O
words	O
,	O
the	O
RNN	Method
’s	O
76.7	O
%	O
accuracy	Metric
,	O
while	O
SPINN	Method
-	O
PI	O
reaches	O
80.2	O
%	O
.	O

All	O
three	O
SPINN	Method
models	O
labeled	O
the	O
following	O
example	O
correctly	O
,	O
while	O
the	O
RNN	Method
did	O
not	O
:	O
Premise	O
:	O
A	O
man	O
wearing	O
glasses	O
and	O
a	O
ragged	O
costume	O
is	O
playing	O
a	O
Jaguar	O
electric	O
guitar	O
and	O
singing	O
with	O
the	O
accompaniment	O
of	O
a	O
drummer	O
.	O

Hypothesis	O
:	O
A	O
man	O
with	O
glasses	O
and	O
a	O
disheveled	O
outfit	O
is	O
playing	O
a	O
guitar	O
and	O
singing	O
along	O
with	O
a	O
drummer	O
.	O

Label	O
:	O
entailment	Task
We	O
suspect	O
that	O
the	O
hybrid	O
nature	O
of	O
the	O
full	O
SPINN	Method
model	O
is	O
also	O
responsible	O
for	O
its	O
surprising	O
ability	O
to	O
perform	O
better	O
than	O
an	O
RNN	Method
baseline	Method
even	O
when	O
its	O
internal	Method
parser	Method
is	O
relatively	O
ineffective	O
at	O
producing	O
correct	O
full	Task
-	Task
sentence	Task
parses	Task
.	O

It	O
may	O
act	O
somewhat	O
like	O
the	O
tree	Method
-	Method
based	Method
CNN	Method
,	O
only	O
with	O
access	O
to	O
larger	O
trees	O
:	O
using	O
tree	O
structure	O
to	O
build	O
up	O
local	O
phrase	O
meanings	O
,	O
and	O
then	O
using	O
the	O
tracking	Method
LSTM	Method
,	O
at	O
least	O
in	O
part	O
,	O
to	O
combine	O
those	O
meanings	O
.	O

Finally	O
,	O
as	O
is	O
likely	O
inevitable	O
for	O
models	O
evaluated	O
on	O
SNLI	Material
,	O
all	O
four	O
models	O
under	O
study	O
did	O
several	O
percent	O
worse	O
on	O
test	O
examples	O
whose	O
ground	O
truth	O
label	O
is	O
neutral	O
than	O
on	O
examples	O
of	O
the	O
other	O
two	O
classes	O
.	O

Entailment	O
–	O
neutral	O
and	O
neutral	O
–	O
contradiction	O
confusions	O
appear	O
to	O
be	O
much	O
harder	O
to	O
avoid	O
than	O
entailment	O
–	O
contradiction	O
confusions	O
,	O
where	O
relatively	O
superficial	O
cues	O
might	O
be	O
more	O
readily	O
useful	O
.	O

section	O
:	O
Conclusions	O
and	O
future	O
work	O
We	O
introduce	O
a	O
model	Method
architecture	Method
(	O
SPINN	Method
-	Method
PI	Method
-	Method
NT	Method
)	O
that	O
is	O
equivalent	O
to	O
a	O
TreeLSTM	Method
,	O
but	O
an	O
order	O
of	O
magnitude	O
faster	O
at	O
test	O
time	O
.	O

We	O
expand	O
that	O
architecture	O
into	O
a	O
tree	Method
-	Method
sequence	Method
hybrid	Method
model	Method
(	O
SPINN	Method
-	O
PI	O
)	O
,	O
and	O
show	O
that	O
this	O
yields	O
significant	O
gains	O
on	O
the	O
SNLI	Material
entailment	O
task	O
.	O

Finally	O
,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
exploit	O
the	O
strengths	O
of	O
this	O
model	O
without	O
the	O
need	O
for	O
an	O
external	Method
parser	Method
by	O
integrating	O
a	O
fast	Method
parser	Method
into	O
the	O
model	O
(	O
as	O
in	O
the	O
full	O
SPINN	Method
)	O
,	O
and	O
that	O
the	O
lack	O
of	O
external	O
parse	O
information	O
yields	O
little	O
loss	O
in	O
accuracy	Metric
.	O

Because	O
this	O
paper	O
aims	O
to	O
introduce	O
a	O
general	Method
purpose	Method
model	Method
for	O
sentence	Task
encoding	Task
,	O
we	O
do	O
not	O
pursue	O
the	O
use	O
of	O
soft	O
attention	O
,	O
despite	O
its	O
demonstrated	O
effectiveness	O
on	O
the	O
SNLI	Material
task	O
.	O

However	O
,	O
we	O
expect	O
that	O
it	O
should	O
be	O
possible	O
to	O
productively	O
combine	O
our	O
model	O
with	O
soft	O
attention	O
to	O
reach	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

Our	O
tracking	Method
LSTM	Method
uses	O
only	O
simple	O
,	O
quick	O
-	O
to	O
-	O
compute	O
features	O
drawn	O
from	O
the	O
head	O
of	O
the	O
buffer	O
and	O
the	O
head	O
of	O
the	O
stack	O
.	O

It	O
is	O
plausible	O
that	O
giving	O
the	O
tracking	Method
LSTM	Method
access	Method
to	O
more	O
information	O
from	O
the	O
buffer	O
and	O
stack	O
at	O
each	O
step	O
would	O
allow	O
it	O
to	O
better	O
represent	O
the	O
context	O
at	O
each	O
tree	O
node	O
,	O
yielding	O
both	O
better	O
parsing	Task
and	O
better	O
sentence	Task
encoding	Task
.	O

One	O
promising	O
way	O
to	O
pursue	O
this	O
goal	O
would	O
be	O
to	O
encode	O
the	O
full	O
contents	O
of	O
the	O
stack	O
and	O
buffer	O
at	O
each	O
time	O
step	O
following	O
the	O
method	O
used	O
by	O
.	O

For	O
a	O
more	O
ambitious	O
goal	O
,	O
we	O
expect	O
that	O
it	O
should	O
be	O
possible	O
to	O
implement	O
a	O
variant	O
of	O
SPINN	Method
on	O
top	O
of	O
a	O
modified	O
stack	O
data	O
structure	O
with	O
differentiable	O
push	O
and	O
pop	O
operations	O
.	O

This	O
would	O
make	O
it	O
possible	O
for	O
the	O
model	O
to	O
learn	O
to	O
parse	O
using	O
guidance	O
from	O
the	O
semantic	Method
representation	Method
objective	Method
,	O
which	O
currently	O
is	O
blocked	O
from	O
influencing	O
the	O
key	O
parsing	O
parameters	O
by	O
our	O
use	O
of	O
hard	Method
shift	Method
/	Method
reduce	Method
decisions	Method
.	O

This	O
change	O
would	O
allow	O
the	O
model	O
to	O
learn	O
to	O
produce	O
parses	O
that	O
are	O
,	O
in	O
aggregate	O
,	O
better	O
suited	O
to	O
supporting	O
semantic	Task
interpretation	Task
than	O
those	O
supplied	O
in	O
the	O
training	O
data	O
.	O

subsubsection	O
:	O
Acknowledgments	O
We	O
acknowledge	O
financial	O
support	O
from	O
a	O
Google	O
Faculty	O
Research	O
Award	O
,	O
the	O
Stanford	O
Data	O
Science	O
Initiative	O
,	O
and	O
the	O
National	O
Science	O
Foundation	O
under	O
grant	O
nos	O
.	O

BCS	Method
1456077	O
and	O
IIS	O
1514268	O
.	O

Some	O
of	O
the	O
Tesla	Method
K40s	Method
used	O
for	O
this	O
research	O
were	O
donated	O
by	O
the	O
NVIDIA	O
Corporation	O
.	O

We	O
also	O
thank	O
Kelvin	O
Guu	O
,	O
Noah	O
Goodman	O
,	O
and	O
many	O
others	O
in	O
the	O
Stanford	O
NLP	O
group	O
for	O
helpful	O
comments	O
.	O

bibliography	O
:	O
References	O
