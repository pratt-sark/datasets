document	O
:	O
Faithful	O
to	O
the	O
Original	O
:	O
Fact	O
Aware	O
Neural	O
Abstractive	Task
Summarization	Task
Unlike	O
extractive	Task
summarization	Task
,	O
abstractive	O
summarization	Task
has	O
to	O
fuse	O
different	O
parts	O
of	O
the	O
source	O
text	O
,	O
which	O
inclines	O
to	O
create	O
fake	O
facts	O
.	O

Our	O
preliminary	O
study	O
reveals	O
nearly	O
30	O
%	O
of	O
the	O
outputs	O
from	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
summarization	Task
system	O
suffer	O
from	O
this	O
problem	O
.	O

While	O
previous	O
abstractive	O
summarization	Task
approaches	O
usually	O
focus	O
on	O
the	O
improvement	O
of	O
informativeness	Task
,	O
we	O
argue	O
that	O
faithfulness	O
is	O
also	O
a	O
vital	O
prerequisite	O
for	O
a	O
practical	O
abstractive	O
summarization	Task
system	O
.	O

To	O
avoid	O
generating	O
fake	O
facts	O
in	O
a	O
summary	O
,	O
we	O
leverage	O
open	Method
information	Method
extraction	Method
and	O
dependency	Method
parse	Method
technologies	Method
to	O
extract	O
actual	O
fact	O
descriptions	O
from	O
the	O
source	O
text	O
.	O

The	O
dual	O
-	O
attention	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
framework	O
is	O
then	O
proposed	O
to	O
force	O
the	O
generation	Task
conditioned	O
on	O
both	O
the	O
source	O
text	O
and	O
the	O
extracted	O
fact	O
descriptions	O
.	O

Experiments	O
on	O
the	O
Gigaword	Material
benchmark	Material
dataset	Material
demonstrate	O
that	O
our	O
model	O
can	O
greatly	O
reduce	O
fake	O
summaries	O
by	O
80	O
%	O
.	O

Notably	O
,	O
the	O
fact	O
descriptions	O
also	O
bring	O
significant	O
improvement	O
on	O
informativeness	O
since	O
they	O
often	O
condense	O
the	O
meaning	O
of	O
the	O
source	O
text	O
.	O

section	O
:	O
Introduction	O
The	O
exponentially	O
growing	O
online	O
information	O
has	O
necessitated	O
the	O
development	O
of	O
effective	O
automatic	O
summarization	Task
systems	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
an	O
increasingly	O
intriguing	O
task	O
,	O
i.e.	O
,	O
abstractive	O
sentence	O
summarization	Task
which	O
generates	O
a	O
shorter	O
version	O
of	O
a	O
given	O
sentence	O
while	O
attempting	O
to	O
preserve	O
its	O
original	O
meaning	O
.	O

This	O
task	O
is	O
different	O
from	O
document	O
-	O
level	O
summarization	Task
since	O
it	O
is	O
hard	O
to	O
apply	O
the	O
common	O
extractive	Method
techniques	Method
.	O

Selecting	O
existing	O
sentences	O
to	O
form	O
the	O
sentence	O
summary	O
is	O
impossible	O
.	O

Early	O
studies	O
on	O
sentence	O
summarization	Task
involve	O
handcrafted	O
rules	O
,	O
syntactic	Method
tree	Method
pruning	Method
and	O
statistical	Method
machine	Method
translation	Method
techniques	Method
.	O

Recently	O
,	O
the	O
application	O
of	O
the	O
attentional	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
(	O
s2s	Method
)	O
framework	O
has	O
attracted	O
growing	O
attention	O
in	O
this	O
area	O
.	O

As	O
we	O
know	O
,	O
sentence	O
summarization	Task
inevitably	O
needs	O
to	O
fuse	O
different	O
parts	O
in	O
the	O
source	O
sentence	O
and	O
is	O
abstractive	O
.	O

Consequently	O
,	O
the	O
generated	O
summaries	O
often	O
mismatch	O
with	O
the	O
original	O
relations	O
and	O
yield	O
fake	O
facts	O
.	O

Our	O
preliminary	O
study	O
reveals	O
that	O
nearly	O
30	O
%	O
of	O
the	O
outputs	O
from	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
s2s	Method
system	O
suffer	O
from	O
this	O
problem	O
.	O

Previous	O
researches	O
are	O
usually	O
devoted	O
to	O
increasing	O
summary	Task
informativeness	Task
.	O

However	O
,	O
one	O
of	O
the	O
most	O
essential	O
prerequisites	O
for	O
a	O
practical	O
abstractive	O
summarization	Task
system	O
is	O
that	O
the	O
generated	O
summaries	O
must	O
accord	O
with	O
the	O
facts	O
expressed	O
in	O
the	O
source	O
.	O

We	O
refer	O
to	O
this	O
aspect	O
as	O
summary	O
faithfulness	O
in	O
this	O
paper	O
.	O

A	O
fake	O
summary	O
may	O
greatly	O
misguide	O
the	O
comprehension	O
of	O
the	O
original	O
text	O
.	O

Look	O
at	O
an	O
illustrative	O
example	O
of	O
the	O
generation	Task
result	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
s2s	Method
model	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
actual	O
subject	O
of	O
the	O
verb	O
“	O
postponed	O
”	O
is	O
“	O
repatriation	O
”	O
.	O

Nevertheless	O
,	O
probably	O
because	O
the	O
entity	O
“	O
bosnian	O
moslems	O
”	O
is	O
closer	O
to	O
“	O
postponed	O
”	O
in	O
the	O
source	O
sentence	O
,	O
the	O
summarization	Task
system	O
wrongly	O
regards	O
“	O
bosnian	O
moslems	O
”	O
as	O
the	O
subject	O
and	O
counterfeits	O
a	O
fact	O
“	O
bosnian	O
moslems	O
postponed	O
”	O
.	O

Meanwhile	O
,	O
the	O
s2s	Method
system	O
generates	O
another	O
fake	O
fact	O
:	O
“	O
unhcr	O
pulled	O
out	O
of	O
bosnia	O
”	O
and	O
puts	O
it	O
into	O
the	O
summary	O
.	O

Consequently	O
,	O
although	O
the	O
informativeness	Metric
(	O
ROUGE	Metric
-	O
1	O
F1=0.57	O
)	O
and	O
readability	O
of	O
this	O
summary	O
are	O
high	O
,	O
its	O
meaning	O
departs	O
far	O
from	O
the	O
original	O
.	O

This	O
sort	O
of	O
summaries	O
is	O
nearly	O
useless	O
in	O
practice	O
.	O

Since	O
the	O
fact	Task
fabrication	Task
is	O
a	O
serious	O
problem	O
,	O
intuitively	O
,	O
encoding	O
existing	O
facts	O
into	O
the	O
summarization	Task
system	O
should	O
be	O
an	O
ideal	O
solution	O
to	O
avoid	O
fake	Task
generation	Task
.	O

To	O
achieve	O
this	O
goal	O
,	O
the	O
first	O
step	O
is	O
to	O
extract	O
the	O
facts	O
from	O
the	O
source	O
sentence	O
.	O

In	O
the	O
relatively	O
mature	O
task	O
of	O
Open	Task
Information	Task
Extraction	Task
(	O
OpenIE	Method
)	O
,	O
a	O
fact	O
is	O
usually	O
represented	O
by	O
a	O
relation	O
triple	O
consisting	O
of	O
(	O
subject	O
;	O
predicate	O
;	O
object	O
)	O
.	O

For	O
example	O
,	O
given	O
the	O
source	O
sentence	O
in	O
Table	O
[	O
reference	O
]	O
,	O
the	O
popular	O
OpenIE	Method
tool	O
generates	O
two	O
relation	O
triples	O
including	O
(	O
repatriation	O
;	O
was	O
postponed	O
;	O
friday	O
)	O
and	O
(	O
unhcr	O
;	O
pulled	O
out	O
of	O
;	O
first	O
joint	O
scheme	O
)	O
.	O

Obviously	O
,	O
these	O
triples	O
can	O
help	O
rectify	O
the	O
mistakes	O
made	O
by	O
the	O
s2s	Method
model	O
.	O

However	O
,	O
the	O
relation	O
triples	O
are	O
not	O
always	O
extractable	O
,	O
e.g.	O
,	O
from	O
the	O
imperative	O
sentences	O
.	O

Hence	O
,	O
we	O
further	O
adopt	O
a	O
dependency	Method
parser	Method
and	O
supplement	O
with	O
the	O
(	O
subject	O
;	O
predicate	O
)	O
and	O
(	O
predicate	O
;	O
object	O
)	O
tuples	O
identified	O
from	O
the	O
parse	O
tree	O
of	O
the	O
sentence	O
.	O

This	O
is	O
also	O
inspired	O
by	O
the	O
work	O
of	O
parse	Task
tree	Task
based	Task
sentence	Task
compression	Task
(	O
e.g.	O
,	O
)	O
.	O

We	O
represent	O
a	O
fact	O
through	O
merging	O
words	O
in	O
a	O
triple	O
or	O
tuples	O
to	O
form	O
a	O
short	O
sentence	O
,	O
defined	O
as	O
a	O
fact	O
description	O
.	O

Fact	O
descriptions	O
actually	O
form	O
the	O
skeletons	O
of	O
sentences	O
.	O

Thus	O
we	O
incorporate	O
them	O
as	O
an	O
additional	O
input	O
source	O
text	O
in	O
our	O
model	O
.	O

Our	O
experiments	O
reveal	O
that	O
the	O
words	O
in	O
the	O
extracted	O
fact	O
descriptions	O
are	O
40	O
%	O
more	O
likely	O
to	O
be	O
included	O
in	O
the	O
actual	O
summaries	O
than	O
the	O
entire	O
words	O
in	O
the	O
source	O
sentences	O
.	O

That	O
is	O
,	O
fact	O
descriptions	O
clearly	O
provide	O
the	O
right	O
guidance	O
for	O
summarization	Task
.	O

Next	O
,	O
using	O
both	O
source	O
sentence	O
and	O
fact	O
descriptions	O
as	O
input	O
,	O
we	O
extend	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
attentional	O
s2s	Method
model	O
to	O
fully	O
leverage	O
their	O
information	O
.	O

Specially	O
,	O
we	O
use	O
two	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
encoders	O
to	O
read	O
the	O
sentence	O
and	O
fact	O
descriptions	O
in	O
parallel	O
.	O

With	O
respective	O
attention	Method
mechanisms	Method
,	O
our	O
model	O
computes	O
the	O
sentence	O
and	O
fact	O
context	O
vectors	O
.	O

It	O
then	O
merges	O
the	O
two	O
vectors	O
according	O
to	O
their	O
relative	O
reliabilities	O
.	O

Finally	O
,	O
a	O
RNN	Method
decoder	O
makes	O
use	O
of	O
the	O
integrated	O
context	O
to	O
generate	O
the	O
summary	O
word	O
-	O
by	O
-	O
word	O
.	O

Since	O
our	O
summarization	Task
system	O
encodes	O
facts	O
to	O
enhance	O
faithfulness	O
,	O
we	O
call	O
it	O
FTSum	Method
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
FTSum	Method
,	O
we	O
conduct	O
extensive	O
experiments	O
on	O
the	O
Gigaword	Material
sentence	Material
summarization	Material
benchmark	Material
dataset	Material
.	O

The	O
results	O
show	O
that	O
our	O
model	O
greatly	O
reduces	O
the	O
fake	Metric
summaries	Metric
by	O
80	O
%	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
s2s	Method
framework	O
.	O

Due	O
to	O
the	O
compression	O
nature	O
of	O
fact	O
descriptions	O
,	O
the	O
use	O
of	O
them	O
also	O
brings	O
the	O
significant	O
improvement	O
in	O
terms	O
of	O
automatic	Metric
informativeness	Metric
evaluation	Metric
.	O

The	O
contributions	O
of	O
our	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
explore	O
the	O
faithfulness	O
problem	O
of	O
abstractive	O
summarization	Task
.	O

We	O
propose	O
a	O
dual	O
-	O
attention	O
s2s	Method
model	O
to	O
push	O
the	O
generation	Task
to	O
follow	O
the	O
original	O
facts	O
.	O

Since	O
the	O
fact	O
descriptions	O
often	O
condense	O
the	O
meaning	O
of	O
the	O
source	O
sentence	O
,	O
they	O
also	O
bring	O
the	O
significant	O
benefit	O
to	O
promote	O
informativeness	O
.	O

section	O
:	O
Fact	Method
Description	Method
Extraction	Method
Based	O
on	O
our	O
observation	O
,	O
30	O
%	O
of	O
summaries	Metric
generated	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
s2s	Method
models	O
suffer	O
from	O
fact	Task
fabrication	Task
,	O
such	O
as	O
the	O
mismatch	O
between	O
the	O
predicate	O
and	O
its	O
subject	O
or	O
object	O
.	O

Therefore	O
,	O
we	O
propose	O
to	O
explicitly	O
encode	O
existing	O
fact	O
descriptions	O
into	O
the	O
model	O
.	O

We	O
leverage	O
popular	O
tools	O
of	O
Open	Method
Information	Method
Extraction	Method
(	O
OpenIE	Method
)	O
and	O
dependency	Method
parser	Method
for	O
this	O
purpose	O
.	O

OpenIE	Method
refers	O
to	O
the	O
extraction	Task
of	Task
entity	Task
relations	Task
from	O
the	O
open	O
-	O
domain	O
text	O
.	O

In	O
OpenIE	Method
,	O
a	O
fact	O
is	O
typically	O
interpreted	O
as	O
a	O
relation	O
triple	O
consisting	O
of	O
(	O
subject	O
;	O
predicate	O
;	O
object	O
)	O
.	O

We	O
join	O
all	O
the	O
items	O
in	O
a	O
triple	O
(	O
i.e.	O
,	O
subject	O
+	O
predicate	O
+	O
object	O
)	O
since	O
it	O
usually	O
acts	O
as	O
a	O
concise	O
sentence	O
.	O

An	O
example	O
of	O
the	O
OpenIE	Method
outputs	O
is	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
we	O
can	O
see	O
,	O
OpenIE	Method
may	O
extract	O
multiple	O
triples	O
to	O
reflect	O
an	O
identical	O
fact	O
in	O
different	O
granularities	O
.	O

In	O
some	O
extreme	O
cases	O
,	O
one	O
relation	O
can	O
yield	O
over	O
50	O
triple	O
variants	O
,	O
which	O
brings	O
high	O
redundancy	O
and	O
burdens	O
the	O
computation	Metric
cost	Metric
of	O
the	O
model	O
.	O

To	O
balance	O
redundancy	O
and	O
fact	O
completeness	O
,	O
we	O
remove	O
a	O
relation	O
triple	O
if	O
all	O
its	O
words	O
are	O
covered	O
by	O
another	O
one	O
.	O

For	O
example	O
,	O
only	O
the	O
last	O
fact	O
description	O
(	O
i.e.	O
,	O
I	O
saw	O
cat	O
sitting	O
on	O
desk	O
)	O
in	O
Table	O
[	O
reference	O
]	O
is	O
reserved	O
.	O

When	O
different	O
fact	O
descriptions	O
are	O
extracted	O
at	O
the	O
end	O
,	O
we	O
use	O
a	O
special	O
separator	O
“	O
”	O
to	O
concatenate	O
them	O
to	O
accelerate	O
the	O
encoding	Task
process	Task
,	O
which	O
is	O
explained	O
by	O
Eq	O
.	O

[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

OpenIE	Method
is	O
able	O
to	O
give	O
a	O
complete	O
description	O
of	O
the	O
entity	O
relations	O
.	O

However	O
,	O
it	O
is	O
worth	O
noting	O
that	O
,	O
the	O
relation	O
triples	O
are	O
not	O
always	O
extractable	O
,	O
e.g.	O
,	O
from	O
the	O
imperative	O
sentences	O
.	O

In	O
fact	O
,	O
about	O
15	O
%	O
of	O
the	O
OpenIE	Method
outputs	O
are	O
empty	O
on	O
our	O
dataset	O
.	O

These	O
empty	O
instances	O
are	O
likely	O
to	O
damage	O
the	O
robustness	O
of	O
our	O
model	O
.	O

As	O
observed	O
,	O
although	O
the	O
complete	O
relation	O
triples	O
are	O
not	O
always	O
available	O
,	O
the	O
(	O
subject	O
;	O
predicate	O
)	O
or	O
(	O
predicate	O
;	O
object	O
)	O
tuples	O
are	O
almost	O
present	O
in	O
each	O
sentence	O
.	O

Therefore	O
,	O
we	O
leverage	O
the	O
dependency	Method
parser	Method
to	O
dig	O
out	O
the	O
appropriate	O
tuples	O
to	O
supplement	O
the	O
fact	O
descriptions	O
.	O

A	O
dependency	Method
parser	Method
converts	O
a	O
sentence	O
into	O
the	O
labeled	O
(	O
governor	O
;	O
dependent	O
)	O
tuples	O
.	O

We	O
extract	O
the	O
predicate	O
-	O
related	O
tuples	O
according	O
to	O
the	O
labels	O
:	O
nsubj	O
,	O
nsubjpass	O
,	O
csubj	O
,	O
csubjpass	O
and	O
dobj	O
.	O

To	O
acquire	O
more	O
complete	O
fact	O
descriptions	O
,	O
we	O
also	O
reserve	O
the	O
important	O
modifiers	O
including	O
the	O
adjectival	O
(	O
amod	O
)	O
,	O
numeric	O
(	O
nummod	O
)	O
and	O
noun	O
compound	O
(	O
compound	O
)	O
.	O

We	O
then	O
merge	O
the	O
tuples	O
containing	O
the	O
same	O
words	O
,	O
and	O
order	O
words	O
based	O
on	O
the	O
original	O
sentence	O
to	O
form	O
the	O
fact	O
descriptions	O
.	O

Take	O
the	O
dependency	O
tree	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
as	O
an	O
example	O
.	O

The	O
output	O
of	O
OpenIE	Method
is	O
empty	O
for	O
this	O
sentence	O
.	O

Based	O
on	O
the	O
dependency	Method
parser	Method
,	O
we	O
firstly	O
filter	O
the	O
following	O
predicate	O
-	O
related	O
tuples	O
:	O
(	O
prices	O
;	O
opened	O
)	O
(	O
opened	O
;	O
tuesday	O
)	O
(	O
dealers	O
;	O
said	O
)	O
and	O
the	O
modify	O
-	O
head	O
tuples	O
:	O
(	O
taiwan	O
;	O
price	O
)	O
(	O
share	O
;	O
price	O
)	O
(	O
lower	O
;	O
tuesday	O
)	O
.	O

These	O
tuples	O
are	O
then	O
merged	O
to	O
form	O
two	O
fact	O
descriptions	O
:	O
taiwan	O
share	O
prices	O
opened	O
lower	O
tuesday	O
dealers	O
said	O
.	O

In	O
the	O
experiments	O
,	O
we	O
employ	O
the	O
popular	O
NLP	Method
pipeline	Method
Stanford	Method
CoreNLP	Method
to	O
handle	O
OpenIE	Method
and	O
dependency	O
parse	O
at	O
the	O
same	O
time	O
.	O

We	O
combine	O
the	O
fact	O
descriptions	O
derived	O
from	O
both	O
parts	O
,	O
and	O
screen	O
out	O
the	O
fact	O
descriptions	O
with	O
the	O
pattern	O
“	O
somebody	O
said	O
/	O
declared	O
/	O
announced	O
”	O
,	O
which	O
are	O
usually	O
meaningless	O
and	O
insignificant	O
.	O

Referring	O
to	O
the	O
copy	O
ratios	O
in	O
Table	O
[	O
reference	O
]	O
,	O
words	O
in	O
fact	O
descriptions	O
are	O
40	O
%	O
more	O
likely	O
to	O
be	O
used	O
in	O
the	O
summary	O
than	O
the	O
words	O
in	O
the	O
original	O
sentence	O
.	O

It	O
indicates	O
that	O
fact	O
descriptions	O
truly	O
condense	O
the	O
meaning	O
of	O
sentences	O
to	O
a	O
large	O
extent	O
.	O

The	O
above	O
statistics	O
also	O
supports	O
the	O
practice	O
of	O
dependency	O
parse	O
based	O
compressive	O
summarization	Task
.	O

However	O
,	O
the	O
length	O
sum	O
of	O
extracted	O
fact	O
descriptions	O
is	O
shorter	O
than	O
the	O
actual	O
summary	O
in	O
20	O
%	O
of	O
the	O
sentences	O
,	O
and	O
4	O
%	O
of	O
the	O
sentences	O
even	O
hold	O
empty	O
fact	O
descriptions	O
.	O

In	O
addition	O
,	O
from	O
Table	O
[	O
reference	O
]	O
we	O
can	O
find	O
that	O
on	O
average	O
one	O
key	O
source	O
word	O
is	O
missing	O
in	O
the	O
fact	O
descriptions	O
.	O

Thus	O
,	O
without	O
the	O
source	O
sentence	O
,	O
we	O
can	O
not	O
reply	O
on	O
fact	O
descriptions	O
alone	O
to	O
generate	O
summaries	O
.	O

section	O
:	O
Fact	Task
Aware	Task
Neural	Task
Summarization	Task
subsection	O
:	O
Model	O
Framework	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
our	O
model	O
consists	O
of	O
three	O
modules	O
including	O
two	O
encoders	Method
and	O
a	O
dual	Method
-	Method
attention	Method
decoder	Method
equipped	O
with	O
a	O
context	Method
selection	Method
gate	Method
network	Method
.	O

The	O
sentence	Method
encoder	Method
reads	O
the	O
input	O
words	O
and	O
builds	O
its	O
corresponding	O
representation	O
.	O

Likewise	O
,	O
the	O
relation	Method
encoder	Method
converts	O
the	O
fact	O
descriptions	O
into	O
hidden	O
states	O
.	O

With	O
the	O
respective	O
attention	Method
mechanisms	Method
,	O
our	O
model	O
computes	O
the	O
sentence	O
and	O
relation	O
context	O
vectors	O
(	O
and	O
)	O
at	O
each	O
decoding	O
time	O
step	O
.	O

The	O
gate	Method
network	Method
is	O
followed	O
to	O
merge	O
the	O
context	O
vectors	O
according	O
to	O
their	O
relative	O
associations	O
with	O
the	O
current	O
generation	O
.	O

The	O
decoder	O
produces	O
summaries	O
word	O
-	O
by	O
-	O
word	O
conditioned	O
on	O
the	O
tailored	O
context	O
vector	O
which	O
embeds	O
the	O
semantics	O
of	O
both	O
source	O
sentence	O
and	O
fact	O
descriptions	O
.	O

subsection	O
:	O
Encoders	O
The	O
input	O
includes	O
the	O
source	O
sentence	O
and	O
the	O
fact	O
descriptions	O
.	O

For	O
each	O
sequence	O
,	O
we	O
employ	O
the	O
bidirectional	Method
Gated	Method
Recurrent	Method
Unit	Method
(	O
BiGRU	Method
)	O
encoder	O
,	O
to	O
construct	O
its	O
semantic	Method
representation	Method
.	O

Take	O
the	O
sentence	O
as	O
an	O
example	O
.	O

The	O
GRU	Method
at	O
the	O
time	O
step	O
is	O
defined	O
as	O
follows	O
:	O
The	O
BiGRU	Method
consists	O
of	O
a	O
forward	Method
GRU	Method
and	O
a	O
backward	Method
GRU	Method
.	O

Suppose	O
the	O
corresponding	O
outputs	O
are	O
and	O
,	O
respectively	O
.	O

Then	O
,	O
the	O
composite	O
hidden	O
state	O
of	O
a	O
word	O
is	O
the	O
concatenation	O
of	O
the	O
two	O
GRU	Method
representations	Method
,	O
i.e.	O
,	O
.	O

For	O
the	O
relation	O
sequence	O
,	O
since	O
it	O
contains	O
multiple	O
independent	O
fact	O
descriptions	O
,	O
we	O
introduce	O
boundary	O
indicators	O
to	O
separate	O
their	O
hidden	O
states	O
.	O

Specially	O
,	O
the	O
value	O
of	O
is	O
defined	O
as	O
follows	O
:	O
Then	O
,	O
is	O
used	O
to	O
reset	O
the	O
GRU	O
state	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
:	O
In	O
this	O
way	O
,	O
all	O
the	O
fact	O
descriptions	O
will	O
start	O
with	O
the	O
same	O
zero	O
vector	O
.	O

In	O
other	O
words	O
,	O
they	O
are	O
encoded	O
independently	O
.	O

Finally	O
,	O
both	O
sentence	O
hidden	O
states	O
and	O
relation	O
hidden	O
states	O
are	O
fed	O
to	O
the	O
decoder	Method
.	O

subsection	O
:	O
Dual	Method
-	Method
Attention	Method
Decoder	Method
Previous	O
s2s	Method
models	O
have	O
developed	O
some	O
task	O
-	O
specific	O
modifications	O
on	O
the	O
decoder	Method
,	O
such	O
as	O
to	O
incorporate	O
the	O
copying	Method
mechanism	Method
and	O
coverage	Method
mechanism	Method
.	O

As	O
this	O
paper	O
focuses	O
on	O
the	O
faithfulness	Task
problem	Task
,	O
we	O
use	O
the	O
most	O
popular	O
decoder	Method
,	O
i.e.	O
,	O
GRU	Method
with	O
attentions	Method
.	O

At	O
each	O
decoding	O
time	O
step	O
,	O
GRU	Method
reads	O
the	O
previous	O
output	O
and	O
context	O
vector	O
as	O
inputs	O
to	O
compute	O
new	O
hidden	O
state	O
:	O
Since	O
we	O
have	O
both	O
sentence	O
and	O
relation	O
representations	O
as	O
input	O
,	O
we	O
develop	O
two	O
attentional	Method
layers	Method
to	O
construct	O
the	O
overall	O
context	O
vector	O
.	O

For	O
instance	O
,	O
the	O
context	Method
representation	Method
of	O
the	O
sentence	O
at	O
time	O
step	O
is	O
computed	O
as	O
:	O
where	O
MLP	Method
stands	O
for	O
multi	Method
-	Method
layer	Method
perceptrons	Method
.	O

The	O
context	O
vector	O
of	O
the	O
relation	O
can	O
be	O
computed	O
similarly	O
.	O

We	O
combine	O
and	O
to	O
build	O
the	O
overall	O
context	O
vector	O
.	O

We	O
explore	O
two	O
alternative	O
combination	Method
approaches	Method
.	O

The	O
first	O
one	O
is	O
called	O
“	O
FTSum	Method
”	Method
,	O
which	O
simply	O
concatenates	O
two	O
context	O
vectors	O
:	O
The	O
other	O
approach	O
is	O
denoted	O
as	O
“	O
FTSum	Method
”	Method
,	O
where	O
we	O
also	O
use	O
MLP	Method
to	O
build	O
a	O
gate	Method
network	Method
and	O
combine	O
context	O
vectors	O
with	O
the	O
weighted	Method
sum	Method
:	O
where	O
“	O
”	O
means	O
the	O
element	O
-	O
wise	O
dot	O
.	O

Experiments	O
show	O
that	O
FTSum	Method
significantly	O
outperforms	O
FTSum	Method
,	O
and	O
the	O
gate	Metric
values	Metric
apparently	O
reflect	O
the	O
relative	Metric
reliability	Metric
of	O
sentence	O
and	O
fact	O
descriptions	O
.	O

Finally	O
,	O
the	O
softmax	Method
layer	Method
is	O
introduced	O
to	O
generate	O
the	O
next	O
word	O
based	O
on	O
previous	O
word	O
,	O
context	O
vector	O
and	O
current	O
decoder	O
state	O
.	O

where	O
stands	O
for	O
a	O
weight	O
matrix	O
.	O

subsection	O
:	O
Learning	Task
The	O
learning	Task
goal	Task
is	O
to	O
maximize	O
the	O
estimated	O
probability	O
of	O
the	O
actual	O
summary	O
.	O

We	O
adopt	O
the	O
common	O
negative	Method
log	Method
-	Method
likelihood	Method
(	O
NLL	Method
)	O
as	O
the	O
loss	Method
function	Method
.	O

where	O
denotes	O
the	O
training	O
dataset	O
and	O
stands	O
for	O
the	O
model	O
parameters	O
.	O

We	O
use	O
Adam	Method
with	O
mini	Method
-	Method
batches	Method
as	O
the	O
optimization	Method
algorithm	Method
.	O

We	O
set	O
the	O
learning	Metric
rate	Metric
and	O
the	O
mini	O
-	O
batch	O
size	O
to	O
32	O
.	O

Similar	O
to	O
,	O
we	O
evaluate	O
the	O
model	O
performance	O
on	O
the	O
development	O
set	O
for	O
every	O
2000	O
batches	O
and	O
halve	O
the	O
learning	Metric
rate	Metric
if	O
the	O
cost	O
increases	O
for	O
10	O
consecutive	O
validations	O
.	O

In	O
addition	O
,	O
we	O
apply	O
gradient	Method
clipping	Method
with	O
range	O
during	O
training	O
to	O
enhance	O
the	O
stability	O
of	O
the	O
model	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Datasets	O
We	O
conduct	O
experiments	O
on	O
the	O
Annotated	Material
English	Material
Gigaword	Material
corpus	Material
,	O
as	O
with	O
.	O

This	O
parallel	O
corpus	O
is	O
produced	O
by	O
pairing	O
the	O
first	O
sentence	O
in	O
the	O
news	O
article	O
and	O
its	O
headline	O
as	O
the	O
summary	O
with	O
heuristic	O
rules	O
.	O

The	O
training	O
and	O
development	O
datasets	O
are	O
built	O
through	O
the	O
script	O
released	O
by	O
.	O

The	O
script	O
also	O
performs	O
various	O
basic	O
text	Task
normalization	Task
,	O
including	O
tokenization	O
,	O
lower	Task
-	Task
casing	Task
,	O
replacing	O
all	O
digit	O
characters	O
with	O
#	O
,	O
and	O
mask	O
the	O
words	O
appearing	O
less	O
than	O
5	O
times	O
with	O
a	O
UNK	O
tag	O
.	O

It	O
comes	O
up	O
with	O
about	O
3.8	O
M	O
sentence	O
-	O
headline	O
pairs	O
as	O
the	O
training	O
set	O
and	O
189	O
K	O
pairs	O
as	O
the	O
development	O
set	O
.	O

We	O
use	O
the	O
same	O
Gigaword	Material
test	Material
set	Material
as	O
.	O

It	O
contains	O
2000	O
sentence	O
-	O
headline	O
pairs	O
.	O

Following	O
,	O
we	O
remove	O
pairs	O
with	O
empty	O
titles	O
,	O
leading	O
to	O
slightly	O
different	O
accuracy	Metric
compared	O
with	O
.	O

The	O
statistics	O
of	O
the	O
Gigaword	Material
corpus	Material
is	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Evaluation	Metric
Metric	Metric
We	O
adopt	O
ROUGE	Metric
for	O
automatic	Task
evaluation	Task
.	O

ROUGE	Metric
has	O
been	O
the	O
standard	O
evaluation	Metric
metric	Metric
for	O
DUC	Task
shared	Task
tasks	Task
since	O
2004	O
.	O

It	O
measures	O
the	O
quality	Metric
of	O
summary	O
by	O
computing	O
overlapping	O
lexical	O
units	O
between	O
the	O
candidate	O
summary	O
and	O
actual	O
summaries	O
,	O
such	O
as	O
unigram	O
,	O
bigram	O
and	O
longest	Method
common	Method
subsequence	Method
(	O
LCS	Metric
)	O
.	O

Following	O
the	O
common	O
practice	O
,	O
we	O
report	O
ROUGE	Metric
-	Metric
1	Metric
(	O
unigram	O
)	O
,	O
ROUGE	Metric
-	Metric
2	Metric
(	O
bi	O
-	O
gram	O
)	O
and	O
ROUGE	Metric
-	Metric
L	Metric
(	O
LCS	Metric
)	O
F1	Metric
scores	Metric
in	O
the	O
following	O
experiments	O
.	O

ROUGE	Metric
-	Metric
1	Metric
and	O
ROUGE	Metric
-	Metric
2	Metric
mainly	O
consider	O
informativeness	O
while	O
ROUGE	Metric
-	Metric
L	Metric
is	O
supposed	O
to	O
be	O
linked	O
to	O
readability	O
.	O

In	O
addition	O
,	O
we	O
manually	O
inspect	O
whether	O
the	O
generated	O
summaries	O
accord	O
with	O
the	O
facts	O
in	O
the	O
original	O
sentences	O
.	O

We	O
mark	O
summaries	O
into	O
three	O
categories	O
:	O
FAITHFUL	O
,	O
FAKE	O
and	O
UNCLEAR	O
.	O

The	O
last	O
one	O
refers	O
to	O
the	O
case	O
where	O
a	O
generated	O
summary	O
is	O
too	O
incomplete	O
to	O
judge	O
its	O
faithfulness	O
,	O
such	O
as	O
just	O
producing	O
a	O
UNK	O
tag	O
.	O

subsection	O
:	O
Implementation	O
Details	O
Since	O
the	O
dataset	O
has	O
already	O
masked	O
infrequent	O
words	O
with	O
the	O
UNK	O
tag	O
,	O
we	O
reserve	O
all	O
the	O
rest	O
words	O
in	O
the	O
training	O
set	O
.	O

As	O
a	O
result	O
,	O
the	O
sizes	O
of	O
source	O
and	O
target	O
vocabularies	O
are	O
120k	O
and	O
69k	O
,	O
respectively	O
.	O

With	O
reference	O
to	O
,	O
we	O
leverage	O
the	O
popular	O
s2s	Method
framework	O
dl4mt	O
as	O
the	O
starting	O
point	O
,	O
and	O
set	O
the	O
size	O
of	O
word	O
embeddings	O
to	O
200	O
.	O

We	O
initialize	O
word	O
embeddings	O
with	O
GloVe	Method
.	O

All	O
the	O
GRU	O
hidden	O
state	O
dimensions	O
are	O
fixed	O
to	O
400	O
.	O

We	O
use	O
dropout	O
with	O
probability	O
.	O

With	O
the	O
decoder	Method
,	O
we	O
use	O
the	O
beam	Method
search	Method
of	O
size	O
6	O
to	O
generate	O
the	O
summary	O
,	O
and	O
restrict	O
the	O
maximal	O
length	O
of	O
a	O
summary	O
to	O
20	O
words	O
.	O

We	O
find	O
that	O
the	O
average	O
system	Metric
summary	Metric
length	Metric
from	O
all	O
our	O
models	O
(	O
about	O
8.0	O
words	O
)	O
is	O
very	O
much	O
consistent	O
with	O
that	O
of	O
the	O
ground	O
truth	O
on	O
the	O
development	O
set	O
,	O
without	O
any	O
special	O
tuning	O
.	O

subsection	O
:	O
Baselines	O
We	O
compare	O
our	O
proposed	O
model	O
with	O
the	O
following	O
six	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
:	O
used	O
an	O
attentive	O
CNN	Method
encoder	O
and	O
NNLM	Method
decoder	Method
to	O
summarize	O
the	O
sentence	O
.	O

further	O
tuned	O
the	O
ABS	Method
model	Method
with	O
additional	O
features	O
to	O
balance	O
the	O
abstractive	O
and	O
extractive	O
tendency	O
.	O

As	O
the	O
extension	O
of	O
the	O
ABS	Method
model	Method
,	O
it	O
used	O
a	O
convolutional	Method
attention	Method
-	Method
based	Method
encoder	Method
and	O
an	O
RNN	Method
decoder	O
.	O

used	O
a	O
full	O
s2s	Method
RNN	Method
model	O
and	O
added	O
the	O
hand	O
-	O
crafted	O
features	O
such	O
as	O
POS	O
tag	O
and	O
NER	O
,	O
to	O
enhance	O
the	O
encoder	Method
representation	Method
.	O

applied	O
the	O
two	O
-	O
layer	Method
LSTMs	Method
Neural	Method
machine	Method
translation	Method
model	Method
with	O
500	O
hidden	O
units	O
in	O
each	O
layer	O
.	O

We	O
implement	O
the	O
standard	O
attentional	O
s2s	Method
with	O
dl4mt	Method
,	O
and	O
denote	O
this	O
baseline	O
as	O
“	O
att	O
-	O
s2s	Method
”	O
.	O

subsection	O
:	O
Informativeness	Metric
Evaluation	Metric
At	O
first	O
,	O
look	O
at	O
the	O
final	O
cost	Metric
values	Metric
during	O
training	O
in	O
Table	O
[	O
reference	O
]	O
.	O

We	O
can	O
see	O
that	O
our	O
model	O
achieves	O
the	O
lowest	O
perplexity	Metric
compared	O
against	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
.	O

It	O
is	O
also	O
noted	O
that	O
,	O
FTSum	Method
largely	O
outperforms	O
FTSum	Method
,	O
which	O
verifies	O
the	O
importance	O
of	O
context	Task
selection	Task
.	O

The	O
ROUGE	Metric
F1	Metric
scores	Metric
are	O
then	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Although	O
the	O
focus	O
of	O
our	O
model	O
focuses	O
is	O
to	O
improve	O
faithfulness	O
,	O
the	O
ROUGE	Metric
scores	O
it	O
receives	O
are	O
also	O
much	O
higher	O
than	O
the	O
other	O
methods	O
.	O

Note	O
that	O
,	O
ABS	O
+	O
and	O
Feats2s	Method
have	O
utilized	O
a	O
series	O
of	O
hand	O
-	O
crafted	O
features	O
,	O
but	O
our	O
model	O
is	O
totally	O
data	O
-	O
driven	O
.	O

Even	O
though	O
,	O
our	O
model	O
surpasses	O
Feats2s	Method
by	O
13	O
%	O
and	O
ABS	Method
+	Method
by	O
56	O
%	O
on	O
ROUGE	Metric
-	Metric
2	Metric
.	O

When	O
fact	O
descriptions	O
are	O
ignored	O
,	O
our	O
model	O
is	O
equivalent	O
to	O
the	O
standard	O
attentional	O
s2s	Method
model	O
s2s	Method
+	O
att	O
.	O

Therefore	O
,	O
it	O
is	O
safe	O
to	O
conclude	O
that	O
,	O
fact	O
descriptions	O
have	O
significant	O
contribute	O
to	O
the	O
increase	O
of	O
ROUGE	Metric
scores	O
.	O

One	O
probable	O
reason	O
is	O
that	O
fact	O
descriptions	O
are	O
much	O
more	O
informative	O
than	O
the	O
original	O
sentence	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

It	O
also	O
largely	O
explains	O
why	O
FTSum	Method
is	O
superior	O
to	O
FTSum	Method
.	O

FTSum	Method
treats	O
the	O
source	O
sentence	O
and	O
relations	O
equally	O
,	O
while	O
FTSum	Method
tells	O
the	O
fact	O
descriptions	O
are	O
often	O
more	O
reliable	O
,	O
as	O
discussed	O
in	O
more	O
detail	O
later	O
.	O

subsection	O
:	O
Faithfulness	Task
Evaluation	Task
Next	O
,	O
we	O
conduct	O
manual	Task
evaluation	Task
to	O
inspect	O
the	O
faithfulness	O
of	O
the	O
generated	O
summaries	O
.	O

Specially	O
,	O
we	O
randomly	O
select	O
100	O
sentences	O
from	O
the	O
test	O
set	O
.	O

Then	O
,	O
we	O
classify	O
the	O
generated	O
summaries	O
as	O
FAITHFUL	O
,	O
FAKE	O
or	O
UNCLEAR	O
.	O

For	O
the	O
sake	O
of	O
a	O
complete	O
comparison	O
,	O
we	O
present	O
the	O
results	O
of	O
our	O
system	O
FTSum	Method
together	O
with	O
the	O
the	O
attentional	O
s2s	Method
model	O
s2s	Method
+	O
att	O
.	O

As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
about	O
30	O
%	O
of	O
the	O
s2s	Method
-	O
att	O
outputs	O
gives	O
disinformation	O
.	O

This	O
number	O
greatly	O
reduces	O
to	O
6	O
%	O
by	O
our	O
model	O
.	O

Nearly	O
90	O
%	O
of	O
summaries	O
generated	O
by	O
our	O
model	O
is	O
faithful	O
,	O
which	O
makes	O
our	O
model	O
far	O
more	O
practical	O
.	O

We	O
find	O
that	O
s2s	Method
-	O
att	O
tends	O
to	O
copy	O
the	O
words	O
closer	O
to	O
the	O
predicate	O
and	O
regard	O
them	O
as	O
its	O
subject	O
and	O
object	O
.	O

However	O
,	O
this	O
is	O
not	O
always	O
reasonable	O
and	O
thus	O
it	O
is	O
actually	O
counterfeiting	O
messages	O
.	O

In	O
comparison	O
,	O
the	O
fact	O
descriptions	O
indeed	O
designate	O
the	O
relations	O
between	O
a	O
predicate	O
and	O
its	O
subject	O
and	O
object	O
.	O

As	O
a	O
result	O
,	O
generation	Task
in	O
line	O
with	O
the	O
fact	O
descriptions	O
is	O
usually	O
able	O
to	O
keep	O
the	O
faithfulness	O
.	O

We	O
illustrate	O
the	O
examples	O
of	O
defective	O
outputs	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
shown	O
,	O
att	O
-	O
s2s	Method
often	O
attempts	O
to	O
fuse	O
different	O
parts	O
in	O
the	O
source	O
sentence	O
to	O
form	O
the	O
summary	O
,	O
no	O
matter	O
whether	O
these	O
phrases	O
are	O
relevant	O
or	O
not	O
.	O

For	O
instance	O
,	O
att	O
-	O
s2s	Method
treats	O
“	O
bosnian	O
moslems	O
”	O
as	O
the	O
subject	O
of	O
“	O
postponed	O
”	O
and	O
“	O
bosnia	O
”	O
as	O
the	O
object	O
of	O
“	O
pulled	O
out	O
of	O
”	O
in	O
Example	O
1	O
.	O

By	O
contract	O
,	O
since	O
the	O
fact	O
description	O
point	O
out	O
the	O
actual	O
subject	O
and	O
object	O
,	O
the	O
output	O
of	O
our	O
model	O
is	O
faithful	O
.	O

In	O
fact	O
,	O
it	O
is	O
exactly	O
the	O
same	O
as	O
the	O
target	O
summary	O
.	O

In	O
Example	O
2	O
,	O
neither	O
att	O
-	O
s2s	Method
nor	O
our	O
model	O
achieves	O
satisfactory	O
performance	O
.	O

att	O
-	O
s2s	Method
again	O
mismatches	O
the	O
object	O
while	O
our	O
model	O
fails	O
to	O
produce	O
a	O
complete	O
sentence	O
.	O

To	O
take	O
a	O
closer	O
look	O
,	O
we	O
find	O
the	O
target	O
summary	O
of	O
this	O
sentence	O
is	O
somewhat	O
strange	O
–	O
it	O
merely	O
focuses	O
on	O
the	O
prepositional	O
phrase	O
(	O
after	O
taking	O
a	O
#	O
#	O
stoke	O
…	O
)	O
,	O
rather	O
than	O
the	O
main	O
clause	O
as	O
usual	O
.	O

Since	O
the	O
main	O
clause	O
is	O
hard	O
to	O
summarize	O
and	O
there	O
is	O
no	O
high	O
-	O
quality	O
fact	O
description	O
extracted	O
,	O
our	O
model	O
fails	O
to	O
give	O
a	O
complete	O
summary	O
.	O

It	O
is	O
also	O
noteworthy	O
that	O
,	O
given	O
multiple	O
long	O
fact	O
descriptions	O
,	O
the	O
generation	O
of	O
our	O
model	O
sometimes	O
traps	O
into	O
one	O
item	O
.	O

For	O
instance	O
,	O
there	O
are	O
two	O
long	O
fact	O
descriptions	O
in	O
Example	O
3	O
and	O
our	O
model	O
only	O
utilizes	O
the	O
first	O
one	O
for	O
generation	Task
.	O

As	O
a	O
result	O
,	O
despite	O
the	O
high	O
faithfulness	O
,	O
the	O
informativeness	O
is	O
somewhat	O
damaged	O
.	O

Therefore	O
,	O
it	O
seems	O
more	O
reliable	O
to	O
introduce	O
the	O
coverage	Method
mechanism	Method
to	O
handle	O
the	O
cases	O
like	O
this	O
one	O
.	O

We	O
leave	O
it	O
as	O
our	O
future	O
work	O
.	O

subsection	O
:	O
Gate	Method
Analysis	Method
As	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
,	O
FTSum	Method
achieves	O
much	O
higher	O
ROUGE	Metric
scores	O
than	O
FTSum	Method
.	O

Now	O
,	O
we	O
investigate	O
what	O
the	O
gate	Method
network	Method
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
actually	O
learns	O
.	O

The	O
changes	O
of	O
the	O
gate	O
values	O
on	O
the	O
development	O
set	O
during	O
training	O
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

At	O
the	O
beginning	O
,	O
the	O
average	O
gate	O
value	O
exceeds	O
0.5	O
,	O
which	O
means	O
the	O
generation	Task
is	O
biased	O
to	O
the	O
source	O
sentence	O
.	O

As	O
training	O
proceeds	O
,	O
the	O
model	O
realizes	O
that	O
the	O
fact	O
descriptions	O
are	O
more	O
reliable	O
,	O
resulting	O
in	O
a	O
consecutive	O
drop	O
of	O
the	O
gate	O
value	O
.	O

Finally	O
,	O
the	O
average	Metric
gate	Metric
value	Metric
is	O
gradually	O
stabilized	O
to	O
0.415	O
.	O

Interestingly	O
,	O
the	O
ratio	O
of	O
sentence	O
and	O
relation	O
gate	O
values	O
i.e.	O
,	O
,	O
is	O
extremely	O
close	O
to	O
the	O
ratio	O
of	O
copying	O
proportions	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
i.e.	O
,	O
.	O

It	O
seems	O
that	O
our	O
model	O
predicts	O
the	O
copy	O
proportion	O
and	O
normalizes	O
it	O
as	O
the	O
gate	O
value	O
.	O

Then	O
,	O
look	O
at	O
the	O
standard	Metric
deviation	Metric
of	Metric
gates	Metric
.	O

To	O
our	O
surprise	O
,	O
its	O
change	O
is	O
nearly	O
anti	O
-	O
symmetric	O
to	O
the	O
mean	O
value	O
.	O

The	O
final	O
standard	Metric
deviation	Metric
reaches	O
about	O
90	O
%	O
of	O
the	O
mean	Metric
gate	Metric
value	Metric
.	O

Thus	O
,	O
still	O
many	O
sentences	O
can	O
dominate	O
the	O
generation	Task
.	O

This	O
strange	O
observation	O
urges	O
us	O
to	O
carefully	O
check	O
the	O
summaries	O
with	O
top	O
/	O
bottom	O
-	O
100	O
gate	O
values	O
in	O
the	O
development	O
set	O
.	O

We	O
find	O
10	O
fact	O
descriptions	O
in	O
the	O
top	O
-	O
100	O
cases	O
are	O
empty	O
,	O
and	O
nearly	O
60	O
%	O
contains	O
the	O
UNK	O
tag	O
.	O

Our	O
model	O
believes	O
these	O
fact	O
descriptions	O
have	O
not	O
much	O
worth	O
to	O
guide	O
generation	Task
.	O

Instead	O
,	O
there	O
is	O
no	O
empty	O
fact	O
descriptions	O
and	O
only	O
1	O
UNK	O
tag	O
in	O
the	O
bottom	O
100	O
cases	O
.	O

Hence	O
these	O
fact	O
descriptions	O
are	O
usually	O
informative	O
enough	O
.	O

In	O
addition	O
,	O
we	O
find	O
the	O
instances	O
with	O
the	O
lowest	O
gate	O
values	O
often	O
hold	O
the	O
following	O
(	O
target	O
summary	O
;	O
fact	O
description	O
)	O
pair	O
:	O
COUNTRY	O
share	O
prices	O
close	O
/	O
open	O
#	O
.	O

#	O
percent	O
higher	O
/	O
lower	O
COUNTRY	O
share	O
prices	O
slumped	O
/	O
dropped	O
/	O
rose	O
#	O
.	O

#	O
percent	O
The	O
extracted	O
fact	O
description	O
itself	O
is	O
already	O
a	O
proper	O
summary	O
.	O

That	O
is	O
why	O
fact	O
descriptions	O
are	O
particularly	O
preferred	O
in	O
generation	Task
.	O

section	O
:	O
Related	O
Work	O
Abstractive	O
sentence	O
summarization	Task
aims	O
to	O
produce	O
a	O
shorter	O
version	O
of	O
a	O
given	O
sentence	O
while	O
preserving	O
its	O
meaning	O
.	O

Unlike	O
document	O
-	O
level	O
summarization	Task
,	O
it	O
is	O
impossible	O
for	O
this	O
task	O
to	O
apply	O
the	O
common	O
extractive	Method
techniques	Method
(	O
e.g.	O
,	O
)	O
.	O

Early	O
studies	O
for	O
sentence	O
summarization	Task
included	O
rule	Method
-	Method
based	Method
methods	Method
,	O
syntactic	Method
tree	Method
pruning	Method
and	O
statistical	Method
machine	Method
translation	Method
techniques	Method
.	O

Recently	O
,	O
the	O
application	O
of	O
encoder	Method
-	Method
decoder	Method
structures	Method
has	O
attracted	O
growing	O
attention	O
in	O
this	O
area	O
.	O

proposed	O
the	O
ABS	Method
model	Method
which	O
consisted	O
of	O
an	O
attentive	O
Convolutional	Method
Neural	Method
Network	Method
(	O
CNN	Method
)	O
encoder	O
and	O
an	O
neural	Method
network	Method
language	Method
model	Method
decoder	Method
.	O

extended	O
their	O
work	O
by	O
replacing	O
the	O
decoder	Method
with	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O

followed	O
this	O
line	O
and	O
developed	O
a	O
full	O
RNN	Method
based	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
(	O
s2s	Method
)	O
framework	O
.	O

Experiments	O
on	O
the	O
Gigaword	Material
test	Material
set	Material
show	O
that	O
the	O
above	O
models	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
addition	O
to	O
the	O
direct	O
application	O
of	O
the	O
general	O
s2s	Method
framework	O
,	O
researchers	O
attempted	O
to	O
import	O
various	O
properties	O
of	O
summarization	Task
.	O

For	O
example	O
,	O
enriched	O
the	O
encoder	Method
with	O
hand	O
-	O
crafted	O
features	O
such	O
as	O
named	O
entities	O
and	O
POS	O
tags	O
.	O

These	O
features	O
played	O
important	O
roles	O
in	O
traditional	O
feature	O
based	O
summarization	Task
systems	O
.	O

found	O
that	O
a	O
large	O
proportion	O
of	O
words	O
in	O
the	O
summary	O
were	O
copied	O
from	O
the	O
source	O
text	O
.	O

Therefore	O
,	O
they	O
proposed	O
CopyNet	Method
which	O
considered	O
the	O
copying	Method
mechanism	Method
during	O
generation	Task
.	O

Later	O
,	O
extended	O
this	O
work	O
by	O
directly	O
measuring	O
the	O
copying	Method
mechanism	Method
within	O
neural	Method
attentions	Method
.	O

Meanwhile	O
,	O
they	O
modified	O
the	O
decoder	Method
to	O
reflect	O
the	O
rewriting	O
behavior	O
in	O
summarization	Task
.	O

Recently	O
,	O
used	O
the	O
coverage	Method
mechanism	Method
to	O
discourage	O
repetition	O
.	O

There	O
were	O
also	O
studies	O
to	O
modify	O
the	O
loss	O
function	O
to	O
fit	O
the	O
evaluation	Metric
metrics	Metric
.	O

For	O
instance	O
,	O
applied	O
Minimum	Method
Risk	Method
Training	Method
strategy	Method
to	O
maximize	O
the	O
ROUGE	Metric
scores	O
of	O
generated	O
summaries	O
.	O

used	O
reinforcement	Method
learning	Method
algorithm	Method
to	O
optimize	O
a	O
mixed	Metric
objective	Metric
function	Metric
of	O
likelihood	O
and	O
ROUGE	Metric
scores	O
.	O

Notably	O
,	O
previous	O
researches	O
usually	O
focused	O
on	O
the	O
improvement	O
of	O
summary	Metric
informativeness	Metric
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
explore	O
the	O
faithfulness	O
problem	O
of	O
abstractive	O
summarization	Task
.	O

section	O
:	O
Conclusion	O
and	O
Future	O
Work	O
This	O
paper	O
investigates	O
the	O
faithfulness	Task
problem	Task
in	O
abstractive	O
summarization	Task
.	O

We	O
employ	O
popular	O
OpenIE	Method
and	O
dependency	O
parse	O
tools	O
to	O
extract	O
fact	O
descriptions	O
in	O
the	O
source	O
sentence	O
.	O

Then	O
,	O
we	O
propose	O
the	O
dual	O
-	O
attention	O
s2s	Method
framework	O
to	O
force	O
the	O
generation	Task
conditioned	O
on	O
both	O
source	O
sentence	O
and	O
the	O
fact	O
descriptions	O
.	O

Experiments	O
on	O
the	O
Gigaword	Material
benchmark	Material
demonstrate	O
that	O
our	O
model	O
greatly	O
reduce	O
fake	O
summaries	O
by	O
80	O
%	O
.	O

In	O
addition	O
,	O
since	O
the	O
fact	O
descriptions	O
often	O
condense	O
the	O
meaning	O
of	O
the	O
sentence	O
,	O
the	O
import	O
of	O
them	O
also	O
brings	O
significant	O
improvement	O
on	O
informativeness	Metric
.	O

We	O
believe	O
our	O
work	O
can	O
be	O
extended	O
in	O
various	O
aspects	O
.	O

On	O
the	O
one	O
hand	O
,	O
we	O
plan	O
to	O
improve	O
our	O
decoder	O
with	O
the	O
copying	Method
mechanism	Method
and	O
coverage	Method
mechanism	Method
,	O
which	O
is	O
further	O
adapted	O
to	O
summarization	Task
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
are	O
interested	O
in	O
the	O
automatic	Task
evaluation	Task
of	Task
summary	Task
faithfulness	Task
.	O

section	O
:	O
Acknowledgments	O
The	O
work	O
described	O
in	O
this	O
paper	O
was	O
supported	O
by	O
Research	O
Grants	O
Council	O
of	O
Hong	O
Kong	O
(	O
PolyU	O
152036	O
/	O
17E	O
)	O
,	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61672445	O
and	O
61572049	O
)	O
and	O
The	O
Hong	O
Kong	O
Polytechnic	O
University	O
(	O
G	O
-	O
YBP6	O
,	O
4	O
-	O
BCDV	O
)	O
.	O

bibliography	O
:	O
References	O
