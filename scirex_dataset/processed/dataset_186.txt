Photographic	Task
Image	Task
Synthesis	Task
with	O
Cascaded	Method
Refinement	Method
Networks	Method
section	O
:	O
The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	Task
photographic	Task
images	Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	Method
label	Method
map	Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	Method
engine	Method
that	O
takes	O
a	O
two	O
-	O
dimensional	Task
semantic	Task
specification	Task
of	Task
the	Task
scene	Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	Method
training	Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	Method
network	Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	Method
regression	Method
objective	Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
†	O
Intel	O
Labs	O
‡	O
Stanford	O
University	O
demonstrate	O
this	O
by	O
synthesizing	O
photographic	O
images	O
at	O
2	O
-	O
megapixel	O
resolution	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Extensive	O
perceptual	O
experiments	O
on	O
datasets	O
of	O
outdoor	O
and	O
indoor	O
scenes	O
demonstrate	O
that	O
images	O
synthesized	O
by	O
the	O
presented	O
approach	O
are	O
considerably	O
more	O
realistic	O
than	O
alternative	O
approaches	O
.	O

section	O
:	O
.	O

Given	O
a	O
pixelwise	Method
semantic	Method
layout	Method
,	O
the	O
presented	O
model	O
synthesizes	O
an	O
image	O
that	O
conforms	O
to	O
this	O
layout	O
.	O

(	O
a	O
)	O
Semantic	O
layouts	O
from	O
the	O
Cityscapes	Material
dataset	Material
of	Material
urban	Material
scenes	Material
;	O
semantic	O
classes	O
are	O
coded	O
by	O
color	O
.	O

(	O
b	O
)	O
Images	O
synthesized	O
by	O
our	O
model	O
for	O
these	O
layouts	O
.	O

The	O
layouts	O
shown	O
here	O
and	O
throughout	O
the	O
paper	O
are	O
from	O
the	O
validation	O
set	O
and	O
depict	O
scenes	O
from	O
new	O
cities	O
that	O
were	O
never	O
seen	O
during	O
training	O
.	O

Best	O
viewed	O
on	O
the	O
screen	O
.	O

section	O
:	O
Abstract	O
We	O
present	O
an	O
approach	O
to	O
synthesizing	Task
photographic	Task
images	Task
conditioned	O
on	O
semantic	O
layouts	O
.	O

Given	O
a	O
semantic	Method
label	Method
map	Method
,	O
our	O
approach	O
produces	O
an	O
image	O
with	O
photographic	O
appearance	O
that	O
conforms	O
to	O
the	O
input	O
layout	O
.	O

The	O
approach	O
thus	O
functions	O
as	O
a	O
rendering	Method
engine	Method
that	O
takes	O
a	O
two	O
-	O
dimensional	Task
semantic	Task
specification	Task
of	Task
the	Task
scene	Task
and	O
produces	O
a	O
corresponding	O
photographic	O
image	O
.	O

Unlike	O
recent	O
and	O
contemporaneous	O
work	O
,	O
our	O
approach	O
does	O
not	O
rely	O
on	O
adversarial	Method
training	Method
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
from	O
semantic	O
layouts	O
by	O
a	O
single	O
feedforward	Method
network	Method
with	O
appropriate	O
structure	O
,	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
direct	Method
regression	Method
objective	Method
.	O

The	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
;	O
we	O
section	O
:	O
Introduction	O
Consider	O
the	O
semantic	O
layouts	O
in	O
Figure	O
1	O
.	O

A	O
skilled	O
painter	O
could	O
draw	O
images	O
that	O
depict	O
urban	O
scenes	O
that	O
conform	O
to	O
these	O
layouts	O
.	O

Highly	O
trained	O
craftsmen	O
can	O
even	O
create	O
paintings	O
that	O
approach	O
photorealism	O
[	O
reference	O
]	O
.	O

Can	O
we	O
train	O
computational	Method
models	Method
that	O
have	O
this	O
ability	O
?	O
Given	O
a	O
semantic	O
layout	O
of	O
a	O
novel	O
scene	O
,	O
can	O
an	O
artificial	Method
system	Method
synthesize	O
an	O
image	O
that	O
depicts	O
this	O
scene	O
and	O
looks	O
like	O
a	O
photograph	O
?	O
This	O
question	O
is	O
connected	O
to	O
central	O
problems	O
in	O
computer	Task
graphics	Task
and	O
artificial	Task
intelligence	Task
.	O

First	O
,	O
consider	O
the	O
problem	O
of	O
photorealism	Task
in	Task
computer	Task
graphics	Task
.	O

A	O
system	O
that	O
synthesizes	O
photorealistic	O
images	O
from	O
semantic	O
layouts	O
would	O
in	O
effect	O
function	O
as	O
a	O
kind	O
of	O
rendering	Method
engine	Method
that	O
bypasses	O
the	O
laborious	O
specification	O
of	O
detailed	O
threedimensional	O
geometry	O
and	O
surface	O
reflectance	O
distributions	O
,	O
and	O
avoids	O
computationally	O
intensive	O
light	Method
transport	Method
simulation	Method
[	O
reference	O
]	O
.	O

A	O
direct	Method
synthesis	Method
approach	Method
could	O
not	O
immediately	O
replace	O
modern	O
rendering	Method
engines	Method
,	O
but	O
would	O
indicate	O
that	O
an	O
alternative	O
route	O
to	O
photorealism	Task
may	O
be	O
viable	O
and	O
could	O
some	O
day	O
complement	O
existing	O
computer	Method
graphics	Method
techniques	Method
.	O

Our	O
second	O
source	O
of	O
motivation	O
is	O
the	O
role	O
of	O
mental	O
imagery	O
and	O
simulation	Task
in	O
human	Task
cognition	Task
[	O
reference	O
]	O
.	O

Mental	Task
imagery	Task
is	O
believed	O
to	O
play	O
an	O
important	O
role	O
in	O
planning	Task
and	Task
decision	Task
making	Task
.	O

The	O
level	O
of	O
detail	O
and	O
completeness	O
of	O
mental	O
imagery	O
is	O
a	O
matter	O
of	O
debate	O
,	O
but	O
its	O
role	O
in	O
human	Task
intelligence	Task
suggests	O
that	O
the	O
ability	O
to	O
synthesize	O
photorealistic	O
images	O
may	O
support	O
the	O
development	O
of	O
artificial	Method
intelligent	Method
systems	Method
[	O
reference	O
]	O
.	O

In	O
this	O
work	O
,	O
we	O
develop	O
a	O
model	O
for	O
photographic	Task
image	Task
synthesis	Task
from	O
pixelwise	Task
semantic	Task
layouts	Task
.	O

Our	O
model	O
is	O
a	O
convolutional	Method
network	Method
,	O
trained	O
in	O
a	O
supervised	Method
fashion	Method
on	O
pairs	O
of	O
photographs	O
and	O
corresponding	O
semantic	O
layouts	O
.	O

Such	O
pairs	O
are	O
provided	O
with	O
semantic	O
segmentation	O
datasets	O
[	O
reference	O
]	O
.	O

We	O
use	O
them	O
not	O
to	O
infer	O
semantic	O
layouts	O
from	O
photographs	O
,	O
but	O
to	O
synthesize	O
photographs	O
from	O
semantic	O
layouts	O
.	O

In	O
this	O
sense	O
our	O
problem	O
is	O
the	O
inverse	Task
of	Task
semantic	Task
segmentation	Task
.	O

Images	O
synthesized	O
by	O
our	O
model	O
are	O
shown	O
in	O
Figure	O
1	O
.	O

We	O
show	O
that	O
photographic	O
images	O
can	O
be	O
synthesized	O
directly	O
by	O
a	O
single	O
feedforward	Method
convolutional	Method
network	Method
trained	O
to	O
minimize	O
a	O
regression	O
loss	O
.	O

This	O
departs	O
from	O
much	O
recent	O
and	O
contemporaneous	O
work	O
,	O
which	O
uses	O
adversarial	Method
training	Method
of	Method
generator	Method
-	Method
discriminator	Method
dyads	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

We	O
show	O
that	O
direct	O
supervised	Method
training	Method
of	O
a	O
single	Method
convolutional	Method
network	Method
can	O
yield	O
photographic	O
images	O
.	O

This	O
bypasses	O
adversarial	Method
training	Method
,	O
which	O
is	O
known	O
to	O
be	O
"	O
massively	O
unstable	O
"	O
[	O
reference	O
]	O
.	O

Furthermore	O
,	O
the	O
presented	O
approach	O
scales	O
seamlessly	O
to	O
high	O
image	O
resolutions	O
.	O

We	O
synthesize	O
images	O
with	O
resolution	O
up	O
to	O
2	O
megapixels	O
(	O
1024×2048	O
)	O
,	O
the	O
full	O
resolution	O
of	O
our	O
training	O
data	O
.	O

Doubling	O
the	O
output	O
resolution	O
and	O
generating	O
appropriate	O
details	O
at	O
that	O
resolution	O
amounts	O
to	O
adding	O
a	O
single	O
module	O
to	O
our	O
end	O
-	O
to	O
-	O
end	Method
model	Method
.	O

We	O
conduct	O
careful	O
perceptual	O
experiments	O
using	O
the	O
Amazon	Method
Mechanical	Method
Turk	Method
platform	O
,	O
comparing	O
the	O
presented	O
approach	O
to	O
a	O
range	O
of	O
baselines	O
.	O

These	O
experiments	O
clearly	O
indicate	O
that	O
images	O
synthesized	O
by	O
our	O
model	O
are	O
significantly	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
alternative	O
approaches	O
.	O

section	O
:	O
Related	O
Work	O
The	O
most	O
prominent	O
contemporary	O
approach	O
to	O
image	Task
synthesis	Task
is	O
based	O
on	O
generative	Method
adversarial	Method
networks	Method
(	O
GANs	Method
)	O
[	O
reference	O
]	O
.	O

In	O
the	O
original	O
work	O
of	O
Goodfellow	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
GANs	Method
were	O
used	O
to	O
synthesize	O
MNIST	Material
digits	Material
and	O
32	O
×	O
32	O
images	O
that	O
aimed	O
to	O
reproduce	O
the	O
appearance	O
of	O
different	O
classes	O
in	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
.	O

Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
proposed	O
training	O
multiple	O
separate	O
GANs	Method
,	O
one	O
for	O
each	O
level	O
in	O
a	O
Laplacian	O
pyramid	O
.	O

Each	O
model	O
is	O
trained	O
independently	O
to	O
synthesize	O
details	O
at	O
its	O
scale	O
.	O

Assembling	O
separatelytrained	Method
models	Method
in	O
this	O
fashion	O
enabled	O
the	O
authors	O
to	O
synthesize	O
smoother	O
images	O
and	O
to	O
push	O
resolution	O
up	O
to	O
96×96	O
.	O

This	O
work	O
is	O
an	O
important	O
precursor	O
to	O
ours	O
in	O
that	O
multiscale	Task
refinement	Task
is	O
a	O
central	O
characteristic	O
of	O
our	O
approach	O
.	O

Key	O
differences	O
are	O
that	O
we	O
train	O
a	O
single	O
model	O
end	O
-	O
to	O
-	O
end	O
to	O
directly	O
synthesize	O
the	O
output	O
image	O
,	O
and	O
that	O
no	O
adversarial	Method
training	Method
is	O
used	O
.	O

Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
remark	O
that	O
"	O
Historical	O
attempts	O
to	O
scale	O
up	O
GANs	Method
using	O
CNNs	Method
to	O
model	O
images	O
have	O
been	O
unsuccessful	O
"	O
and	O
describe	O
a	O
number	O
of	O
modifications	O
that	O
enable	O
scaling	O
up	O
adversarial	Method
training	Method
to	O
64×64	O
images	O
.	O

Salimans	O
et	O
al	O
.	O

[	O
reference	O
]	O
also	O
tackle	O
the	O
instability	O
of	O
GAN	Method
training	Method
and	O
describe	O
a	O
number	O
of	O
heuristics	Method
that	O
encourage	O
convergence	Metric
.	O

The	O
authors	O
synthesize	O
128	O
×	O
128	O
images	O
that	O
possess	O
plausible	O
low	O
-	O
level	O
statistics	O
.	O

Nevertheless	O
,	O
as	O
observed	O
in	O
recent	O
work	O
and	O
widely	O
known	O
in	O
the	O
folklore	O
,	O
GANs	Method
"	O
remain	O
remarkably	O
difficult	O
to	O
train	O
"	O
and	O
"	O
approaches	O
to	O
attacking	O
this	O
problem	O
still	O
rely	O
on	O
heuristics	Method
that	O
are	O
extremely	O
sensitive	O
to	O
modifications	O
"	O
[	O
reference	O
]	O
.	O

(	O
See	O
also	O
[	O
reference	O
]	O
.	O

)	O
Our	O
work	O
demonstrates	O
that	O
these	O
difficulties	O
can	O
be	O
avoided	O
in	O
the	O
setting	O
we	O
consider	O
.	O

Dosovitskiy	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
ConvNet	Method
to	O
generate	O
images	Task
of	Task
3D	Task
models	Task
,	O
given	O
a	O
model	O
ID	O
and	O
viewpoint	O
.	O

The	O
network	O
thus	O
acts	O
directly	O
as	O
a	O
rendering	Method
engine	Method
for	O
the	O
3D	Method
model	Method
.	O

This	O
is	O
also	O
an	O
important	O
precursor	O
to	O
our	O
work	O
as	O
it	O
uses	O
direct	O
feedforward	Method
synthesis	Method
through	O
a	O
network	O
trained	O
with	O
a	O
regression	Method
loss	Method
.	O

Our	O
model	O
,	O
loss	Method
,	O
and	O
problem	O
setting	O
are	O
different	O
,	O
enabling	O
synthesis	Task
of	Task
sharper	Task
higherresolution	Task
images	Task
of	Task
scenes	Task
without	Task
3D	Task
models	Task
.	O

Dosovitskiy	O
and	O
Brox	O
[	O
reference	O
]	O
introduced	O
a	O
family	O
of	O
composite	Method
loss	Method
functions	Method
for	O
image	Task
synthesis	Task
,	O
which	O
combine	O
regression	Method
over	O
the	O
activations	O
of	O
a	O
fixed	O
"	O
perceiver	Method
"	Method
network	Method
with	O
a	O
GAN	Method
loss	Method
.	O

Networks	Method
trained	O
using	O
these	O
composite	O
loss	O
functions	O
were	O
applied	O
to	O
synthesize	O
preimages	O
that	O
induce	O
desired	O
excitation	O
patterns	O
in	O
image	Method
classification	Method
models	Method
[	O
reference	O
]	O
and	O
images	O
that	O
excite	O
specific	O
elements	O
in	O
such	O
models	O
[	O
reference	O
]	O
.	O

In	O
recent	O
work	O
,	O
networks	O
trained	O
using	O
these	O
losses	O
were	O
applied	O
to	O
generate	O
diverse	O
sets	O
of	O
227×227	O
images	O
,	O
to	O
synthesize	O
images	O
for	O
given	O
captions	O
,	O
and	O
to	O
inpaint	O
missing	O
regions	O
[	O
reference	O
]	O
.	O

These	O
works	O
all	O
rely	O
on	O
the	O
aforementioned	O
composite	O
losses	O
,	O
which	O
require	O
balancing	O
the	O
adversarial	O
loss	O
with	O
a	O
regression	Method
loss	Method
.	O

Our	O
work	O
differs	O
in	O
that	O
GANs	Method
are	O
not	O
used	O
,	O
which	O
simplifies	O
the	O
train	O
-	O
[	O
reference	O
]	O
.	O

Zoom	O
in	O
for	O
details	O
.	O

ing	O
procedure	O
,	O
architecture	O
,	O
and	O
loss	O
.	O

Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
consider	O
a	O
family	O
of	O
problems	O
that	O
include	O
the	O
image	Task
synthesis	Task
problem	Task
we	O
focus	O
on	O
.	O

The	O
paper	O
of	O
Isola	O
et	O
al	O
.	O

appeared	O
on	O
arXiv	O
during	O
the	O
course	O
of	O
our	O
research	O
.	O

It	O
provides	O
an	O
opportunity	O
to	O
compare	O
our	O
approach	O
to	O
a	O
credible	O
alternative	O
that	O
was	O
independently	O
tested	O
on	O
the	O
same	O
data	O
.	O

Like	O
a	O
number	O
of	O
aforementioned	O
formulations	O
,	O
Isola	O
et	O
al	O
.	O

use	O
a	O
composite	Method
loss	Method
that	O
combines	O
a	O
GAN	Method
and	O
a	O
regression	Method
term	Method
.	O

The	O
authors	O
use	O
the	O
Cityscapes	Material
dataset	O
and	O
synthesize	O
256×256	O
images	O
for	O
given	O
semantic	O
layouts	O
.	O

In	O
comparison	O
,	O
our	O
simpler	O
direct	Method
formulation	Method
yields	O
much	O
more	O
realistic	O
images	O
and	O
scales	O
seamlessly	O
to	O
high	O
resolutions	O
.	O

A	O
qualitative	O
comparison	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
64×64	O
images	O
of	O
scenes	O
that	O
are	O
described	O
by	O
given	O
sentences	O
.	O

Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
describe	O
a	O
different	O
model	O
that	O
generates	O
32×32	O
images	O
that	O
aim	O
to	O
fit	O
sentences	O
.	O

Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
generate	O
64×64	O
images	O
of	O
faces	O
and	O
birds	O
with	O
given	O
attributes	O
.	O

Reed	O
et	O
al	O
.	O

[	O
reference	O
]	O
synthesize	O
128×128	O
images	O
of	O
birds	O
and	O
people	O
conditioned	O
on	O
text	O
descriptions	O
and	O
on	O
spatial	O
constraints	O
such	O
as	O
bounding	O
boxes	O
or	O
keypoints	O
.	O

Wang	O
and	O
Gupta	O
[	O
reference	O
]	O
synthesize	O
128×128	Task
images	Task
of	Task
indoor	Task
scenes	Task
by	O
factorizing	O
the	O
image	Method
generation	Method
process	Method
into	O
synthesis	O
of	O
a	O
normal	Method
map	Method
and	O
subsequent	O
synthesis	O
of	O
a	O
corresponding	O
color	O
image	O
.	O

Most	O
of	O
these	O
works	O
use	O
GANs	Method
,	O
with	O
the	O
exception	O
of	O
Yan	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
variational	Method
autoencoders	Method
and	O
Mansimov	O
et	O
al	O
.	O

[	O
reference	O
]	O
who	O
use	O
a	O
recurrent	Method
attention	Method
-	Method
based	Method
model	Method
[	O
reference	O
]	O
.	O

Our	O
problem	O
statement	O
is	O
different	O
in	O
that	O
our	O
input	O
is	O
a	O
pixelwise	O
semantic	O
layout	O
,	O
and	O
our	O
technical	O
approach	O
differs	O
substantially	O
in	O
that	O
a	O
single	O
feedforward	Method
convolutional	Method
network	Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
to	O
synthesize	O
a	O
high	O
-	O
resolution	O
image	O
.	O

A	O
line	O
of	O
work	O
considers	O
synthesis	Task
of	Task
future	Task
frames	Task
in	Task
video	Task
.	O

Srivastava	O
et	O
al	O
.	O

[	O
reference	O
]	O
train	O
a	O
recurrent	Method
network	Method
for	O
this	O
purpose	O
.	O

Mathieu	O
et	O
al	O
.	O

[	O
reference	O
]	O
build	O
on	O
the	O
work	O
of	O
Denton	O
et	O
al	O
.	O

[	O
reference	O
]	O
and	O
use	O
a	O
composite	Method
loss	Method
that	O
combines	O
an	O
adversarial	Method
term	Method
with	O
regression	O
penalties	O
on	O
colors	O
and	O
gradients	O
.	O

Oh	O
et	O
al	O
.	O

[	O
reference	O
]	O
predict	O
future	O
frames	O
in	O
Atari	Task
games	Task
conditioned	O
on	O
the	O
player	O
's	O
action	O
.	O

Finn	O
et	O
al	O
.	O

[	O
reference	O
]	O
explicitly	O
model	O
pixel	O
motion	O
and	O
also	O
condition	O
on	O
action	O
.	O

Vondrick	O
et	O
al	O
.	O

[	O
reference	O
]	O
learn	O
a	O
model	Method
of	Method
scene	Method
dynamics	Method
and	O
use	O
it	O
to	O
synthesize	O
video	O
sequences	O
from	O
single	O
images	O
.	O

Xue	O
et	O
al	O
.	O

[	O
reference	O
]	O
develop	O
a	O
probabilistic	Method
model	Method
that	O
enables	O
synthesizing	O
multiple	O
plausible	O
video	O
sequences	O
.	O

In	O
these	O
works	O
,	O
a	O
color	O
image	O
is	O
available	O
as	O
a	O
starting	O
point	O
for	O
synthesis	Task
.	O

Video	Task
synthesis	Task
can	O
be	O
accomplished	O
by	O
advecting	O
the	O
content	O
of	O
this	O
initial	O
image	O
.	O

In	O
our	O
setting	O
,	O
photographic	Task
scene	Task
appearance	Task
must	O
be	O
synthesized	O
without	O
such	O
initialization	O
.	O

Researchers	O
have	O
also	O
studied	O
image	Task
inpainting	Task
[	O
reference	O
]	O
,	O
superresolution	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
novel	O
view	Task
synthesis	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
interactive	Task
image	Task
manipulation	Task
[	O
reference	O
]	O
.	O

In	O
these	O
problems	O
,	O
photographic	O
content	O
is	O
given	O
as	O
input	O
,	O
whereas	O
we	O
are	O
concerned	O
with	O
synthesizing	O
photographic	O
images	O
from	O
semantic	O
layouts	O
alone	O
.	O

section	O
:	O
Method	O
section	O
:	O
Preliminaries	O
Consider	O
a	O
semantic	O
layout	O
L	O
∈	O
{	O
0	O
,	O
1	O
}	O
m×n×c	O
,	O
where	O
m×n	O
is	O
the	O
pixel	O
resolution	O
and	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

Each	O
pixel	O
in	O
L	O
is	O
represented	O
by	O
a	O
one	O
-	O
hot	O
vector	O
that	O
indicates	O
its	O
semantic	O
label	O
:	O
One	O
of	O
the	O
c	O
possible	O
labels	O
is	O
'	O
void	O
'	O
,	O
which	O
indicates	O
that	O
the	O
semantic	O
class	O
of	O
the	O
pixel	O
is	O
not	O
specified	O
.	O

Our	O
goal	O
is	O
to	O
train	O
a	O
parametric	Method
mapping	Method
g	Method
that	O
given	O
a	O
semantic	O
layout	O
L	O
produces	O
a	O
color	O
image	O
I	O
∈	O
R	O
m×n×3	O
that	O
conforms	O
to	O
L.	O
In	O
the	O
course	O
of	O
this	O
project	O
we	O
have	O
experimented	O
with	O
a	O
large	O
number	O
of	O
network	Method
architectures	Method
.	O

As	O
a	O
result	O
of	O
these	O
experiments	O
,	O
we	O
have	O
identified	O
three	O
characteristics	O
that	O
are	O
important	O
for	O
synthesizing	Task
photorealistic	Task
images	Task
.	O

We	O
review	O
these	O
characteristics	O
before	O
describing	O
our	O
solution	O
.	O

Global	Task
coordination	Task
.	O

Globally	O
consistent	O
structure	O
is	O
essential	O
for	O
photorealism	Task
.	O

Many	O
objects	O
exhibit	O
nonlocal	O
structural	O
relationships	O
,	O
such	O
as	O
symmetry	O
.	O

For	O
example	O
,	O
if	O
the	O
network	O
synthesizes	O
a	O
red	O
light	O
on	O
the	O
left	O
side	O
of	O
a	O
car	O
,	O
then	O
the	O
corresponding	O
light	O
on	O
the	O
right	O
should	O
also	O
be	O
red	O
.	O

This	O
distinguishes	O
photorealistic	Task
image	Task
synthesis	Task
from	O
texture	Task
synthesis	Task
,	O
which	O
can	O
leverage	O
statistical	Method
stationarity	Method
[	O
reference	O
]	O
.	O

Our	O
model	O
is	O
based	O
on	O
multi	Method
-	Method
resolution	Method
refinement	Method
.	O

The	O
synthesis	Task
begins	O
at	O
extremely	O
low	O
resolution	O
(	O
4	O
×	O
8	O
in	O
our	O
implementation	O
)	O
.	O

Feature	O
maps	O
are	O
then	O
progressively	O
refined	O
.	O

Thus	O
global	O
structure	O
can	O
be	O
coordinated	O
at	O
lower	O
octaves	O
,	O
where	O
even	O
distant	O
object	O
parts	O
are	O
represented	O
in	O
nearby	O
feature	O
columns	O
.	O

These	O
decisions	O
are	O
then	O
refined	O
at	O
higher	O
octaves	O
.	O

High	Task
resolution	Task
.	O

To	O
produce	O
truly	O
photorealistic	O
results	O
,	O
a	O
model	O
must	O
be	O
able	O
to	O
synthesize	O
high	O
-	O
resolution	O
images	O
.	O

Low	Task
resolution	Task
is	O
akin	O
to	O
myopic	Task
vision	Task
in	O
that	O
fine	O
visual	O
features	O
are	O
not	O
discernable	O
.	O

The	O
drive	O
to	O
high	O
image	Task
and	Task
video	Task
resolutions	Task
in	O
multiple	O
industries	O
is	O
a	O
testament	O
to	O
resolution	O
's	O
importance	O
.	O

Our	O
model	O
synthesizes	O
images	O
by	O
progressive	Method
refinement	Method
,	O
and	O
going	O
up	O
an	O
octave	O
in	O
resolution	O
(	O
e.g.	O
,	O
from	O
512p	O
to	O
1024p	O
)	O
amounts	O
to	O
adding	O
a	O
single	O
refinement	Method
module	Method
.	O

The	O
entire	O
cascade	Method
of	Method
refinement	Method
modules	Method
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Memory	O
.	O

We	O
conjecture	O
that	O
high	O
model	Metric
capacity	Metric
is	O
essential	O
for	O
synthesizing	Task
high	Task
-	Task
resolution	Task
photorealistic	Task
images	Task
.	O

Human	O
hyperrealistic	O
painters	O
use	O
photographic	O
references	O
as	O
external	O
memory	O
of	O
detailed	O
object	O
appearance	O
[	O
reference	O
]	O
.	O

The	O
best	O
existing	O
image	Method
compression	Method
techniques	Method
require	O
millions	O
of	O
bits	O
of	O
information	O
to	O
represent	O
the	O
content	O
of	O
a	O
single	O
high	O
-	O
resolution	O
image	O
:	O
there	O
exists	O
no	O
known	O
way	O
to	O
reconstruct	O
a	O
given	O
photograph	O
at	O
high	O
fidelity	O
from	O
a	O
lowercapacity	Method
representation	Method
[	O
reference	O
]	O
.	O

In	O
order	O
for	O
our	O
model	O
to	O
be	O
able	O
to	O
synthesize	O
diverse	O
scenes	O
from	O
a	O
given	O
domain	O
given	O
only	O
semantic	O
layouts	O
as	O
input	O
,	O
the	O
capacity	O
of	O
the	O
model	O
must	O
be	O
sufficiently	O
high	O
to	O
be	O
able	O
to	O
reproduce	O
the	O
detailed	O
photographic	O
appearance	O
of	O
many	O
objects	O
.	O

We	O
expect	O
a	O
successful	O
model	O
to	O
reproduce	O
images	O
in	O
the	O
training	O
set	O
extremely	O
well	O
(	O
memorization	O
)	O
and	O
also	O
to	O
apply	O
the	O
learned	O
representations	O
to	O
novel	O
layouts	O
(	O
generalization	Task
)	O
.	O

This	O
requires	O
high	O
model	O
capacity	O
.	O

Our	O
design	O
is	O
modular	O
and	O
the	O
capacity	O
of	O
the	O
model	O
can	O
be	O
expanded	O
as	O
allowed	O
by	O
hardware	O
.	O

The	O
network	O
used	O
in	O
most	O
of	O
our	O
experiments	O
has	O
105	O
M	O
parameters	O
and	O
maximizes	O
available	O
GPU	O
memory	O
.	O

We	O
have	O
consistently	O
found	O
that	O
increasing	O
model	Metric
capacity	Metric
increases	O
image	Metric
quality	Metric
.	O

section	O
:	O
Architecture	O
The	O
Cascaded	Method
Refinement	Method
Network	Method
(	O
CRN	Method
)	O
is	O
a	O
cascade	Method
of	Method
refinement	Method
modules	Method
.	O

Each	O
module	O
M	O
i	O
operates	O
at	O
a	O
given	O
resolution	O
.	O

In	O
our	O
implementation	O
,	O
the	O
resolution	O
of	O
the	O
first	O
module	O
(	O
M	O
0	O
)	O
is	O
4×8	O
.	O

Resolution	Method
is	O
doubled	O
between	O
consecutive	O
modules	O
(	O
from	O
M	O
i−1	O
to	O
M	O
i	O
)	O
.	O

Let	O
w	O
i	O
×	O
h	O
i	O
be	O
the	O
resolution	O
of	O
module	O
i.	O
The	O
first	O
module	O
,	O
M	O
0	O
,	O
receives	O
the	O
semantic	O
layout	O
L	O
as	O
input	O
(	O
downsampled	O
to	O
w	O
0	O
×h	O
0	O
)	O
and	O
produces	O
a	O
feature	Method
layer	Method
F	O
0	O
at	O
resolution	O
w	O
0	O
×	O
h	O
0	O
as	O
output	O
.	O

All	O
other	O
modules	O
M	O
i	O
(	O
for	O
i	O
=	O
0	O
)	O
are	O
structurally	O
identical	O
:	O
M	O
i	O
receives	O
a	O
concatenation	O
of	O
the	O
layout	O
L	O
(	O
downsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
and	O
the	O
feature	O
layer	O
F	O
i−1	O
(	O
upsampled	O
to	O
w	O
i	O
×h	O
i	O
)	O
as	O
input	O
,	O
and	O
produces	O
feature	O
layer	O
F	O
i	O
as	O
output	O
.	O

We	O
denote	O
the	O
number	O
of	O
feature	O
maps	O
in	O
F	O
i	O
by	O
d	O
i	O
.	O

Each	O
module	O
M	O
i	O
consists	O
of	O
three	O
feature	Method
layers	Method
:	O
the	O
input	Method
layer	Method
,	O
an	O
intermediate	Method
layer	Method
,	O
and	O
the	O
output	O
layer	O
.	O

This	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O

The	O
input	O
layer	O
has	O
dimensionality	O
w	O
i	O
×h	O
i	O
×	O
(	O
d	O
i−1	O
+	O
c	O
)	O
and	O
is	O
a	O
concatenation	O
of	O
the	O
downsampled	Method
semantic	Method
layout	Method
L	Method
(	O
c	O
channels	O
)	O
and	O
a	O
bilinearly	Method
upsampled	Method
feature	Method
layer	Method
F	O
i−1	O
(	O
d	O
i−1	O
channels	O
)	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
upconvolutions	Method
because	O
upconvolutions	Method
tend	O
to	O
introduce	O
characteristic	O
artifacts	O
[	O
reference	O
]	O
.	O

The	O
intermediate	O
layer	O
and	O
the	O
output	O
layer	O
both	O
have	O
dimensionality	O
w	O
i	O
×h	O
i	O
×d	O
i	O
.	O

Each	O
layer	O
is	O
followed	O
by	O
3×3	O
convolutions	O
,	O
layer	Method
normalization	Method
[	O
reference	O
]	O
,	O
and	O
LReLU	Method
nonlinearity	Method
[	O
reference	O
]	O
.	O

The	O
output	O
layer	O
Fī	O
of	O
the	O
final	O
module	O
Mī	Method
is	O
not	O
followed	O
by	O
normalization	O
or	O
nonlinearity	O
.	O

Instead	O
,	O
a	O
linear	Method
projection	Method
(	O
1×1	Method
convolution	Method
)	O
is	O
applied	O
to	O
map	O
Fī	O
(	O
dimensionality	O
wī	O
×hī	O
×dī	O
)	O
to	O
the	O
output	O
color	O
image	O
(	O
dimensionality	O
wī	O
×hī	O
×3	O
)	O
.	O

The	O
total	O
number	O
of	O
refinement	O
modules	O
in	O
a	O
cascade	O
depends	O
on	O
the	O
output	O
resolution	O
.	O

section	O
:	O
Training	O
The	O
CRN	Method
is	O
trained	O
in	O
a	O
supervised	Method
fashion	Method
on	O
a	O
semantic	O
segmentation	O
dataset	O
D	O
=	O
{	O
(	O
I	O
,	O
L	O
)	O
}.	O
A	O
semantic	O
layout	O
L	O
is	O
used	O
as	O
input	O
and	O
the	O
corresponding	O
color	O
image	O
I	O
as	O
output	O
.	O

This	O
can	O
be	O
thought	O
of	O
as	O
"	O
inverse	Task
semantic	Task
segmentation	Task
"	O
.	O

It	O
is	O
an	O
underconstrained	Task
one	Task
-	Task
to	Task
-	Task
many	Task
inverse	Task
problem	Task
.	O

We	O
will	O
generally	O
refer	O
to	O
I	O
as	O
a	O
"	O
reference	O
image	O
"	O
rather	O
than	O
"	O
ground	O
truth	O
"	O
,	O
since	O
many	O
valid	O
photographic	O
images	O
could	O
have	O
yielded	O
the	O
same	O
semantic	O
layout	O
.	O

Given	O
the	O
underconstrained	O
nature	O
of	O
the	O
problem	O
,	O
using	O
an	O
appropriate	O
loss	Method
function	Method
is	O
critical	O
,	O
as	O
observed	O
in	O
prior	O
work	O
on	O
image	Task
synthesis	Task
.	O

Simply	O
comparing	O
the	O
pixel	O
colors	O
of	O
the	O
synthesized	O
image	O
and	O
the	O
reference	O
image	O
could	O
severely	O
penalize	O
perfectly	O
realistic	O
outputs	O
.	O

For	O
example	O
,	O
synthesizing	O
a	O
white	O
car	O
instead	O
of	O
a	O
black	O
car	O
would	O
induce	O
a	O
very	O
high	O
loss	O
.	O

Instead	O
we	O
adopt	O
the	O
"	O
content	Method
representation	Method
"	O
of	O
Gatys	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
also	O
referred	O
to	O
as	O
a	O
perceptual	Task
loss	Task
or	O
feature	Task
matching	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

The	O
basic	O
idea	O
is	O
to	O
match	O
activations	O
in	O
a	O
visual	Method
perception	Method
network	Method
that	O
is	O
applied	O
to	O
the	O
synthesized	O
image	O
and	O
separately	O
to	O
the	O
reference	O
image	O
.	O

Let	O
Φ	O
be	O
a	O
trained	O
visual	Method
perception	Method
network	Method
(	O
we	O
use	O
VGG	Method
-	Method
19	Method
[	O
reference	O
]	O
)	O
.	O

Layers	O
in	O
the	O
network	O
represent	O
an	O
image	O
at	O
increasing	O
levels	O
of	O
abstraction	O
:	O
from	O
edges	O
and	O
colors	O
to	O
objects	O
and	O
categories	O
.	O

Matching	O
both	O
lower	O
-	O
layer	O
and	O
higher	O
-	O
layer	O
activations	O
in	O
the	O
perception	Method
network	Method
guides	O
the	O
synthesis	Method
network	Method
to	O
learn	O
both	O
fine	O
-	O
grained	O
details	O
and	O
more	O
global	O
part	O
arrangement	O
.	O

Let	O
{	O
Φ	O
l	O
}	O
be	O
a	O
collection	O
of	O
layers	O
in	O
the	O
network	Method
Φ	Method
,	O
such	O
that	O
Φ	O
0	O
denotes	O
the	O
input	O
image	O
.	O

Each	O
layer	O
is	O
a	O
threedimensional	O
tensor	O
.	O

For	O
a	O
training	O
pair	O
(	O
I	O
,	O
L	O
)	O
∈	O
D	O
,	O
our	O
loss	Metric
is	O
Here	O
g	O
is	O
the	O
image	Method
synthesis	Method
network	Method
being	O
trained	O
and	O
θ	O
is	O
the	O
set	O
of	O
parameters	O
of	O
this	O
network	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
balance	O
the	O
contribution	O
of	O
each	O
layer	O
l	O
to	O
the	O
loss	O
.	O

For	O
layers	O
Φ	O
l	O
(	O
l	O
≥	O
1	O
)	O
we	O
use	O
'	O
conv1	O
2	O
'	O
,	O
'	O
conv2	O
2	O
'	O
,	O
'	O
conv3	O
2	O
'	O
,	O
'	O
conv4	O
2	O
'	O
,	O
and	O
'	O
conv5	O
2	O
'	O
in	O
VGG	Method
-	Method
19	Method
[	O
reference	O
]	O
.	O

The	O
hyperparameters	O
{	O
λ	O
l	O
}	O
are	O
set	O
automatically	O
.	O

They	O
are	O
initialized	O
to	O
the	O
inverse	O
of	O
the	O
number	O
of	O
elements	O
in	O
each	O
layer	O
.	O

After	O
100	O
epochs	O
,	O
{	O
λ	O
l	O
}	O
are	O
rescaled	O
to	O
normalize	O
the	O
expected	O
contribution	O
of	O
each	O
term	O
Φ	O
l	O
(	O
I	O
)	O
−	O
Φ	O
l	O
(	O
g	O
(	O
L	O
;	O
θ	O
)	O
)	O
1	O
to	O
the	O
loss	O
.	O

section	O
:	O
Synthesizing	O
a	O
diverse	O
collection	O
The	O
architecture	O
and	O
training	Method
procedure	Method
described	O
so	O
far	O
synthesize	O
a	O
single	O
image	O
for	O
a	O
given	O
input	O
L.	O
In	O
our	O
experiments	O
this	O
already	O
yields	O
good	O
results	O
.	O

However	O
,	O
since	O
a	O
given	O
semantic	O
layout	O
can	O
correspond	O
to	O
many	O
images	O
,	O
it	O
also	O
makes	O
sense	O
to	O
generate	O
a	O
diverse	O
set	O
of	O
images	O
as	O
output	O
.	O

Conditional	Task
synthesis	Task
of	Task
diverse	Task
images	Task
can	O
be	O
approached	O
as	O
a	O
stochastic	Method
process	Method
[	O
reference	O
]	O
.	O

We	O
take	O
a	O
different	O
tack	O
and	O
modify	O
the	O
network	O
to	O
emit	O
a	O
collection	O
of	O
images	O
in	O
one	O
shot	O
,	O
with	O
a	O
modified	O
loss	O
that	O
encourages	O
diversity	O
within	O
the	O
collection	O
.	O

Specifically	O
,	O
we	O
change	O
the	O
number	O
of	O
output	O
channels	O
from	O
3	O
to	O
3k	O
,	O
where	O
k	O
is	O
the	O
desired	O
number	O
of	O
images	O
.	O

Each	O
consecutive	O
3	O
-	O
tuple	O
of	O
channels	O
forms	O
an	O
image	O
.	O

Now	O
consider	O
the	O
loss	O
.	O

If	O
loss	O
(	O
1	O
)	O
is	O
applied	O
independently	O
to	O
each	O
output	O
image	O
,	O
the	O
k	O
synthesized	O
images	O
will	O
be	O
identical	O
.	O

Our	O
first	O
modification	O
is	O
to	O
consider	O
the	O
set	O
of	O
k	O
outputs	O
together	O
and	O
define	O
the	O
loss	O
of	O
the	O
whole	O
collection	O
in	O
terms	O
of	O
the	O
best	O
synthesized	O
image	O
.	O

Let	O
g	O
u	O
(	O
L	O
;	O
θ	O
)	O
be	O
the	O
u	O
th	O
image	O
in	O
the	O
synthesized	O
collection	O
.	O

Our	O
first	O
version	O
of	O
the	O
modified	O
loss	O
is	O
based	O
on	O
the	O
hindsight	Method
loss	Method
developed	O
for	O
multiple	Task
choice	Task
learning	Task
[	O
reference	O
]	O
:	O
By	O
considering	O
only	O
the	O
best	O
synthesized	O
image	O
,	O
this	O
loss	O
encourages	O
the	O
network	O
to	O
spread	O
its	O
bets	O
and	O
cover	O
the	O
space	O
of	O
images	O
that	O
conform	O
to	O
the	O
input	O
semantic	O
layout	O
.	O

The	O
loss	O
is	O
structurally	O
akin	O
to	O
the	O
k	Method
-	Method
means	Method
clustering	Method
objective	Method
,	O
which	O
only	O
considers	O
the	O
closest	O
centroid	O
to	O
each	O
datapoint	O
and	O
thus	O
encourages	O
the	O
centroids	O
to	O
spread	O
and	O
cover	O
the	O
dataset	O
.	O

We	O
further	O
build	O
on	O
this	O
idea	O
and	O
formulate	O
a	O
loss	Method
that	O
considers	O
a	O
virtual	O
collection	O
of	O
up	O
to	O
k	O
c	O
images	O
.	O

(	O
Recall	O
that	O
c	O
is	O
the	O
number	O
of	O
semantic	O
classes	O
.	O

)	O
Specifically	O
,	O
for	O
each	O
semantic	O
class	O
p	O
,	O
let	O
L	O
p	O
denote	O
the	O
corresponding	O
channel	O
L	O
(	O
·	O
,	O
·	O
,	O
p	O
)	O
in	O
the	O
input	O
label	O
map	O
.	O

We	O
now	O
define	O
a	O
more	O
powerful	O
diversity	O
loss	O
as	O
where	O
Φ	O
j	O
l	O
is	O
the	O
j	O
th	O
feature	O
map	O
in	O
Φ	O
l	O
,	O
L	O
l	O
p	O
is	O
the	O
mask	O
L	O
p	O
downsampled	O
to	O
match	O
the	O
resolution	O
of	O
Φ	O
l	O
,	O
and	O
is	O
the	O
Hadamard	Method
product	Method
.	O

This	O
loss	O
in	O
effect	O
constructs	O
a	O
virtual	O
image	O
by	O
adaptively	O
taking	O
the	O
best	O
synthesized	O
content	O
for	O
each	O
semantic	O
class	O
from	O
the	O
whole	O
collection	O
,	O
and	O
scoring	O
the	O
collection	O
based	O
on	O
this	O
assembled	O
image	O
.	O

section	O
:	O
Baselines	O
The	O
approach	O
presented	O
in	O
Section	O
3	O
is	O
far	O
from	O
the	O
first	O
we	O
tried	O
.	O

In	O
this	O
section	O
we	O
describe	O
a	O
number	O
of	O
alternative	O
approaches	O
that	O
will	O
be	O
used	O
as	O
baselines	O
in	O
Section	O
5	O
.	O

GAN	Method
and	O
semantic	Task
segmentation	Task
.	O

Our	O
first	O
baseline	O
is	O
consistent	O
with	O
current	O
trends	O
in	O
the	O
research	O
community	O
.	O

It	O
combines	O
a	O
GAN	Method
with	O
a	O
semantic	Method
segmentation	Method
objective	Method
.	O

The	O
generator	O
is	O
trained	O
to	O
synthesize	O
an	O
image	O
that	O
fools	O
the	O
discriminator	Method
[	O
reference	O
]	O
.	O

An	O
additional	O
term	O
in	O
the	O
loss	O
specifies	O
that	O
when	O
the	O
synthesized	O
image	O
is	O
given	O
as	O
input	O
to	O
a	O
pretrained	Method
semantic	Method
segmentation	Method
network	Method
,	O
it	O
should	O
produce	O
a	O
label	O
map	O
that	O
is	O
as	O
close	O
to	O
the	O
input	O
layout	O
L	O
as	O
possible	O
.	O

The	O
GAN	O
setup	O
follows	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

The	O
input	O
to	O
the	O
generator	O
is	O
the	O
semantic	O
layout	O
L.	O
For	O
the	O
semantic	Task
segmentation	Task
network	Task
,	O
we	O
use	O
publicly	O
available	O
networks	O
that	O
were	O
pretrained	O
for	O
the	O
Cityscapes	Material
dataset	O
[	O
reference	O
]	O
and	O
the	O
NYU	Material
dataset	Material
[	O
reference	O
]	O
.	O

The	O
training	Metric
objective	Metric
combines	O
the	O
GAN	O
loss	O
and	O
the	O
semantic	Method
segmentation	Method
(	O
pixelwise	Method
cross	Method
-	Method
entropy	Method
)	O
loss	O
.	O

Full	Method
-	Method
resolution	Method
network	Method
.	O

Our	O
second	O
baseline	O
is	O
a	O
feedforward	Method
convolutional	Method
network	Method
that	O
operates	O
at	O
full	O
resolution	O
.	O

This	O
baseline	O
uses	O
the	O
same	O
loss	O
as	O
the	O
CRN	Method
described	O
in	O
Section	O
3	O
.	O

The	O
only	O
difference	O
is	O
the	O
network	Method
architecture	Method
.	O

In	O
particular	O
,	O
we	O
have	O
experimented	O
with	O
variants	O
of	O
the	O
multi	Method
-	Method
scale	Method
context	Method
aggregation	Method
network	Method
[	O
reference	O
]	O
.	O

An	O
appealing	O
property	O
of	O
this	O
network	O
is	O
that	O
it	O
retains	O
high	O
resolution	O
in	O
the	O
intermediate	O
layers	O
,	O
which	O
we	O
hypothesized	O
to	O
be	O
helpful	O
for	O
photorealistic	Task
image	Task
synthesis	Task
.	O

The	O
original	O
architecture	O
described	O
in	O
[	O
reference	O
]	O
did	O
not	O
yield	O
good	O
results	O
and	O
is	O
not	O
well	O
-	O
suited	O
to	O
our	O
problem	O
,	O
because	O
the	O
input	O
semantic	O
layouts	O
are	O
piecewise	O
constant	O
and	O
the	O
network	O
of	O
[	O
reference	O
]	O
begins	O
with	O
a	O
small	O
receptive	O
field	O
.	O

We	O
obtained	O
much	O
better	O
results	O
with	O
the	O
inverse	Method
architecture	Method
:	O
start	O
with	O
large	O
dilation	O
and	O
decrease	O
it	O
by	O
a	O
factor	O
of	O
2	O
in	O
each	O
layer	O
.	O

This	O
can	O
be	O
viewed	O
as	O
a	O
full	Task
-	Task
resolution	Task
counterpart	Task
to	O
the	O
CRN	Method
,	O
based	O
on	O
dilating	O
the	O
filters	Method
instead	O
of	O
scaling	O
the	O
feature	O
maps	O
.	O

One	O
of	O
the	O
drawbacks	O
of	O
this	O
approach	O
is	O
that	O
all	O
intermediate	Method
feature	Method
layers	Method
are	O
at	O
full	O
image	O
resolution	O
and	O
have	O
a	O
high	O
memory	O
footprint	O
.	O

Thus	O
the	O
ratio	O
of	O
capacity	O
(	O
number	O
of	O
parameters	O
)	O
to	O
memory	Metric
footprint	Metric
is	O
much	O
lower	O
than	O
in	O
the	O
CRN	Method
.	O

This	O
high	O
memory	O
footprint	O
of	O
intermediate	O
layers	O
also	O
constrains	O
the	O
resolution	O
to	O
which	O
this	O
approach	O
can	O
scale	O
:	O
with	O
10	O
layers	O
and	O
256	O
feature	O
maps	O
per	O
layer	O
,	O
the	O
maximal	O
resolution	O
that	O
could	O
be	O
trained	O
with	O
available	O
GPU	O
memory	O
is	O
256×512	O
.	O

Encoder	Method
-	Method
decoder	Method
.	O

Our	O
third	O
baseline	O
is	O
an	O
encoder	Method
-	Method
decoder	Method
network	Method
,	O
the	O
u	Method
-	Method
net	Method
[	O
reference	O
]	O
.	O

This	O
network	O
is	O
also	O
trained	O
with	O
the	O
same	O
loss	Metric
as	O
the	O
CRN	Method
.	O

It	O
is	O
thus	O
an	O
additional	O
baseline	O
that	O
evaluates	O
the	O
effect	O
of	O
using	O
the	O
CRN	Method
versus	O
a	O
different	O
architecture	O
,	O
when	O
everything	O
else	O
(	O
loss	O
,	O
training	O
procedure	O
)	O
is	O
held	O
fixed	O
.	O

Image	Task
-	Task
space	Task
loss	Task
.	O

Our	O
next	O
baseline	O
controls	O
for	O
the	O
feature	Task
matching	Task
loss	Task
used	O
to	O
train	O
the	O
CRN	Method
.	O

Here	O
we	O
use	O
exactly	O
the	O
same	O
architecture	O
as	O
in	O
Section	O
3	O
,	O
but	O
use	O
only	O
the	O
first	O
layer	O
Φ	O
0	O
(	O
image	O
color	O
)	O
in	O
the	O
loss	O
:	O
Image	Task
-	Task
to	Task
-	Task
image	Task
translation	Task
.	O

Our	O
last	O
baseline	O
is	O
the	O
contemporaneous	Method
approach	Method
of	O
Isola	O
et	O
al	O
.	O

,	O
the	O
implementation	O
and	O
results	O
of	O
which	O
are	O
publicly	O
available	O
[	O
reference	O
]	O
.	O

This	O
approach	O
uses	O
a	O
conditional	Method
GAN	Method
and	O
is	O
representative	O
of	O
the	O
dominant	O
stream	O
of	O
research	O
in	O
image	Task
synthesis	Task
.	O

The	O
generator	O
is	O
an	O
encoder	Method
-	Method
decoder	Method
[	O
reference	O
]	O
.	O

The	O
GAN	O
setup	O
is	O
derived	O
from	O
the	O
work	O
of	O
Radford	O
et	O
al	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Experiments	O
section	O
:	O
Experimental	O
procedure	O
Methodology	O
.	O

The	O
most	O
reliable	O
known	O
methodology	O
for	O
evaluating	O
the	O
realism	Task
of	O
synthesized	O
images	O
is	O
perceptual	O
experiments	O
with	O
human	O
observers	O
.	O

Such	O
experiments	O
yield	O
quantitative	O
results	O
and	O
have	O
been	O
used	O
in	O
related	O
work	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

There	O
have	O
also	O
been	O
attempts	O
to	O
design	O
automatic	Metric
measures	Metric
that	O
evaluate	O
realism	O
without	O
humans	O
in	O
the	O
loop	O
.	O

For	O
example	O
,	O
Salimans	O
et	O
al	O
.	O

ran	O
a	O
pretrained	Method
image	Method
classification	Method
network	Method
on	O
synthesized	O
images	O
and	O
analyzed	O
its	O
predictions	O
[	O
reference	O
]	O
.	O

We	O
experimented	O
with	O
such	O
automatic	Metric
measures	Metric
(	O
for	O
example	O
using	O
pretrained	Method
semantic	Method
segmentation	Method
networks	Method
)	O
and	O
found	O
that	O
they	O
can	O
all	O
be	O
fooled	O
by	O
augmenting	O
any	O
baseline	O
to	O
also	O
optimize	O
for	O
the	O
evaluated	O
measure	O
;	O
the	O
resulting	O
images	O
are	O
not	O
more	O
realistic	O
but	O
score	O
very	O
highly	O
[	O
reference	O
][	O
reference	O
]	O
.	O

Well	O
-	O
designed	O
perceptual	O
experiments	O
with	O
human	O
observers	O
are	O
more	O
reliable	O
.	O

We	O
therefore	O
use	O
carefully	O
designed	O
perceptual	O
experiments	O
for	O
quantitative	Task
evaluation	Task
.	O

We	O
will	O
release	O
our	O
complete	O
implementation	O
and	O
experimental	O
setup	O
so	O
that	O
our	O
experiments	O
can	O
be	O
replicated	O
by	O
others	O
.	O

All	O
experiments	O
use	O
pairwise	O
A	O
/	O
B	O
tests	O
deployed	O
on	O
the	O
Amazon	Method
Mechanical	Method
Turk	Method
(	O
MTurk	Method
)	O
platform	O
.	O

Similar	O
protocols	O
have	O
been	O
used	O
to	O
evaluate	O
the	O
realism	Task
of	Task
3D	Task
reconstructions	Task
[	O
reference	O
][	O
reference	O
]	O
.	O

Each	O
MTurk	Method
job	O
involves	O
a	O
batch	O
of	O
roughly	O
100	O
pairwise	O
comparisons	O
,	O
along	O
with	O
sentinel	O
pairs	O
that	O
test	O
whether	O
the	O
worker	O
is	O
attentive	O
and	O
diligent	O
.	O

Each	O
pair	O
contains	O
two	O
images	O
synthesized	O
for	O
the	O
same	O
label	O
map	O
by	O
two	O
different	O
approaches	O
(	O
or	O
a	O
corresponding	O
reference	O
image	O
from	O
the	O
dataset	O
)	O
.	O

The	O
workers	O
are	O
asked	O
to	O
select	O
the	O
more	O
realistic	O
image	O
in	O
each	O
pair	O
.	O

The	O
images	O
are	O
all	O
shown	O
at	O
the	O
same	O
resolution	O
(	O
200×400	O
)	O
.	O

The	O
comparisons	O
are	O
randomized	O
across	O
conditions	O
and	O
both	O
the	O
left	O
-	O
right	O
order	O
and	O
the	O
order	O
within	O
a	O
job	O
are	O
randomized	O
.	O

Two	O
types	O
of	O
experiments	O
are	O
conducted	O
.	O

In	O
the	O
first	O
,	O
images	O
are	O
shown	O
for	O
unlimited	O
time	O
and	O
the	O
worker	O
is	O
free	O
to	O
spend	O
as	O
much	O
time	O
as	O
desired	O
on	O
each	O
pair	O
.	O

In	O
the	O
second	O
,	O
each	O
pair	O
is	O
shown	O
for	O
a	O
randomly	O
chosen	O
duration	O
between	O
Table	O
1	O
.	O

Results	O
of	O
pairwise	Metric
comparisons	Metric
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	Material
and	O
NYU	Material
datasets	Material
.	O

Each	O
column	O
compares	O
our	O
approach	O
with	O
one	O
of	O
the	O
baselines	O
.	O

Each	O
cell	O
lists	O
the	O
fraction	O
of	O
pairwise	O
comparisons	O
in	O
which	O
images	O
synthesized	O
by	O
our	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
corresponding	O
baseline	O
.	O

Chance	O
is	O
at	O
50	O
%	O
.	O

Datasets	O
.	O

We	O
use	O
two	O
datasets	O
with	O
pixelwise	O
semantic	O
labels	O
,	O
one	O
depicting	O
outdoor	O
scenes	O
and	O
one	O
depicting	O
indoor	O
scenes	O
.	O

Our	O
primary	O
dataset	O
is	O
Cityscapes	Material
,	O
which	O
has	O
become	O
the	O
dominant	O
semantic	O
segmentation	O
dataset	O
due	O
to	O
the	O
quality	O
of	O
the	O
data	O
[	O
reference	O
]	O
.	O

We	O
train	O
on	O
the	O
training	O
set	O
(	O
3	O
K	O
images	O
)	O
and	O
evaluate	O
on	O
the	O
validation	O
set	O
(	O
500	O
images	O
)	O
.	O

(	O
Evaluating	O
"	O
inverse	Task
semantic	Task
segmentation	Task
"	O
on	O
the	O
test	O
set	O
is	O
impossible	O
because	O
the	O
label	O
maps	O
are	O
not	O
provided	O
.	O

)	O
Our	O
second	O
dataset	O
is	O
the	O
older	O
NYU	Material
dataset	Material
of	Material
indoor	Material
scenes	Material
[	O
reference	O
]	O
.	O

This	O
dataset	O
is	O
smaller	O
and	O
the	O
images	O
are	O
VGA	Material
resolution	Material
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
depth	O
data	O
in	O
the	O
NYU	Material
dataset	Material
,	O
only	O
the	O
semantic	O
layouts	O
and	O
the	O
color	O
images	O
.	O

We	O
use	O
the	O
first	O
1200	O
of	O
the	O
1449	O
labeled	O
images	O
for	O
training	O
and	O
the	O
remaining	O
249	O
for	O
testing	O
.	O

section	O
:	O
Results	O
Primary	O
experiments	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
by	O
models	O
trained	O
on	O
the	O
Cityscapes	Material
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
rated	O
more	O
realistic	O
than	O
images	O
synthesized	O
by	O
the	O
four	O
alternative	O
approaches	O
.	O

Note	O
that	O
the	O
'	O
image	Method
-	Method
space	Method
loss	Method
'	Method
baseline	Method
uses	O
the	O
same	O
architecture	O
as	O
the	O
CRN	Method
and	O
controls	O
for	O
the	O
loss	O
,	O
while	O
the	O
'	O
full	Method
-	Method
resolution	Method
network	Method
'	O
and	O
the	O
'	O
encoder	Method
-	Method
decoder	Method
'	O
use	O
the	O
same	O
loss	O
as	O
the	O
CRN	Method
and	O
control	O
for	O
the	O
architecture	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Compared	O
to	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	Method
were	O
rated	O
more	O
realistic	O
in	O
97	O
%	O
of	O
the	O
comparisons	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
5	O
.	O

Figure	O
4	O
reports	O
the	O
results	O
of	O
time	O
-	O
limited	O
pairwise	O
comparisons	O
of	O
real	O
Cityscapes	Material
images	O
,	O
images	O
synthesized	O
by	O
the	O
CRN	Method
,	O
and	O
images	O
synthesized	O
by	O
the	O
approach	O
of	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
(	O
referred	O
to	O
as	O
'	O
Pix2pix	Material
'	O
following	O
the	O
public	O
implementation	O
)	O
.	O

After	O
just	O
1	O
8	O
of	O
a	O
second	O
,	O
the	O
Pix2pix	Material
images	O
are	O
clearly	O
rated	O
less	O
realistic	O
than	O
the	O
real	O
Cityscapes	Material
images	O
or	O
the	O
CRN	Method
images	O
(	O
72.5	O
%	O
Real	Material
>	O
Pix2pix	Material
,	O
73.4	O
%	O
CRN	Method
>	O
Pix2pix	Material
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
CRN	Method
images	O
are	O
on	O
par	O
with	O
real	O
images	O
at	O
that	O
time	O
,	O
as	O
seen	O
both	O
in	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
(	O
52.6	O
%	O
)	O
and	O
in	O
the	O
nearly	O
identical	O
Real	Material
>	O
Pix2pix	Material
and	O
CRN	Method
>	O
Pix2pix	Material
rates	O
.	O

At	O
250	O
milliseconds	O
(	O
1	O
4	O
of	O
a	O
second	O
)	O
,	O
the	O
Real	Material
>	O
Pix2pix	Material
rate	O
rises	O
to	O
85.0	O
%	O
while	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
is	O
at	O
57.4	O
%	O
.	O

The	O
CRN	Method
>	O
Pix2pix	Material
rate	O
is	O
84.0	O
%	O
,	O
still	O
nearly	O
identical	O
to	O
Real	Material
>	O
Pix2pix	Material
.	O

At	O
500	O
milliseconds	O
,	O
the	O
Real	Material
>	O
Pix2pix	Material
and	O
CRN	Method
>	O
Pix2pix	Material
rates	O
finally	O
diverge	O
,	O
although	O
both	O
are	O
extremely	O
high	O
(	O
95.1	O
%	O
and	O
87.4	O
%	O
,	O
respectively	O
)	O
,	O
and	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
rises	O
to	O
64.2	O
%	O
.	O

Over	O
time	O
,	O
the	O
CRN	Method
>	O
Pix2pix	Material
rate	O
rises	O
above	O
90	O
%	O
and	O
the	O
Real	Material
>	O
Pix2pix	Material
rate	O
remains	O
consistently	O
higher	O
than	O
the	O
Real	Metric
>	Metric
CRN	Metric
rate	Metric
.	O

NYU	Material
dataset	Material
.	O

We	O
conduct	O
supporting	O
experiments	O
on	O
the	O
NYU	Material
dataset	Material
.	O

This	O
dataset	O
is	O
smaller	O
and	O
lower	O
-	O
resolution	O
,	O
so	O
the	O
quality	O
of	O
images	O
synthesized	O
by	O
all	O
approaches	O
is	O
lower	O
.	O

Nevertheless	O
,	O
the	O
differences	O
are	O
still	O
clear	O
.	O

Table	O
1	O
reports	O
the	O
results	O
of	O
randomized	O
pairwise	O
comparisons	O
of	O
images	O
synthesized	O
for	O
this	O
dataset	O
.	O

Images	O
synthesized	O
by	O
the	O
presented	O
approach	O
were	O
again	O
rated	O
consistently	O
more	O
realistic	O
than	O
the	O
baselines	O
.	O

All	O
results	O
are	O
statistically	O
significant	O
with	O
p	O
<	O
10	O
−3	O
.	O

Qualitative	O
results	O
are	O
shown	O
in	O
Figure	O
6	O
.	O

Diversity	Task
loss	Task
.	O

For	O
all	O
preceding	O
experiments	O
we	O
have	O
used	O
the	O
feature	O
matching	O
loss	O
specified	O
in	O
Equation	O
[	O
reference	O
]	O
.	O

The	O
models	O
produced	O
a	O
single	O
image	O
as	O
output	O
,	O
and	O
this	O
image	O
was	O
evaluated	O
against	O
baselines	O
.	O

We	O
now	O
qualitatively	O
demonstrate	O
the	O
effect	O
of	O
the	O
diversity	O
loss	O
described	O
in	O
Section	O
3.4	O
.	O

To	O
this	O
end	O
we	O
trained	O
models	O
that	O
produce	O
image	O
collections	O
as	O
output	O
(	O
9	O
images	O
at	O
a	O
time	O
)	O
.	O

Figure	O
7	O
shows	O
pairs	O
of	O
images	O
sampled	O
from	O
the	O
synthesized	O
collections	O
,	O
for	O
different	O
input	O
layouts	O
in	O
the	O
NYU	Material
validation	Material
set	Material
.	O

The	O
figure	O
illustrates	O
that	O
the	O
diversity	O
loss	O
does	O
lead	O
the	O
output	O
channels	O
to	O
spread	O
out	O
and	O
produce	O
different	O
appearances	O
.	O

section	O
:	O
Semantic	Method
layout	Method
section	O
:	O
GAN	Method
+	Method
semantic	Method
segmenation	Method
Full	Method
-	Method
resolution	Method
network	Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Encoder	Method
-	Method
decoder	Method
Semantic	Method
layout	Method
Our	O
result	O
Isola	O
et	O
al	O
.	O

[	O
reference	O
]	O
Full	Method
-	Method
resolution	Method
network	Method
Encoder	Method
-	Method
decoder	Method
Figure	O
6	O
.	O

Qualitative	O
comparison	O
on	O
the	O
NYU	Material
dataset	Material
.	O

Figure	O
7	O
.	O

Synthesizing	O
a	O
diverse	O
collection	O
,	O
illustrated	O
on	O
the	O
NYU	Material
dataset	Material
.	O

Each	O
pair	O
shows	O
two	O
images	O
from	O
a	O
collection	O
synthesized	O
for	O
a	O
given	O
semantic	O
layout	O
.	O

section	O
:	O
Conclusion	O
We	O
have	O
presented	O
a	O
direct	O
approach	O
to	O
photographic	Task
image	Task
synthesis	Task
conditioned	O
on	O
pixelwise	Method
semantic	Method
layouts	Method
.	O

Images	O
are	O
synthesized	O
by	O
a	O
convolutional	Method
network	Method
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
a	O
regression	Method
loss	Method
.	O

This	O
direct	O
approach	O
is	O
considerably	O
simpler	O
than	O
contemporaneous	O
work	O
,	O
and	O
produces	O
much	O
more	O
realistic	O
results	O
.	O

We	O
hope	O
that	O
the	O
simplicity	O
of	O
the	O
presented	O
approach	O
can	O
support	O
follow	O
-	O
up	O
work	O
that	O
will	O
further	O
advance	O
realism	O
and	O
explore	O
the	O
applications	O
of	O
photographic	Task
image	Task
synthesis	Task
.	O

Our	O
results	O
,	O
while	O
significantly	O
more	O
realistic	O
than	O
the	O
prior	O
state	O
of	O
the	O
art	O
,	O
are	O
clearly	O
not	O
indistinguishable	O
from	O
real	O
HD	O
images	O
.	O

Exciting	O
work	O
remains	O
to	O
be	O
done	O
to	O
achieve	O
perfect	O
photorealism	O
.	O

If	O
such	O
level	O
of	O
realism	O
is	O
ever	O
achieved	O
,	O
which	O
we	O
believe	O
to	O
be	O
possible	O
,	O
alternative	O
routes	O
for	O
image	Task
synthesis	Task
in	O
computer	Task
graphics	Task
will	O
open	O
up	O
.	O

section	O
:	O
