Sentence	Method
-	Method
State	Method
LSTM	Method
for	O
Text	Task
Representation	Task
section	O
:	O
Abstract	O
Bi	Method
-	Method
directional	Method
LSTMs	Method
are	O
a	O
powerful	O
tool	O
for	O
text	Task
representation	Task
.	O

On	O
the	O
other	O
hand	O
,	O
they	O
have	O
been	O
shown	O
to	O
suffer	O
various	O
limitations	O
due	O
to	O
their	O
sequential	O
nature	O
.	O

We	O
investigate	O
an	O
alternative	O
LSTM	Method
structure	O
for	O
encoding	Task
text	Task
,	O
which	O
consists	O
of	O
a	O
parallel	O
state	O
for	O
each	O
word	O
.	O

Recurrent	Method
steps	Method
are	O
used	O
to	O
perform	O
local	O
and	O
global	O
information	O
exchange	O
between	O
words	O
simultaneously	O
,	O
rather	O
than	O
incremental	O
reading	O
of	O
a	O
sequence	O
of	O
words	O
.	O

Results	O
on	O
various	O
classification	Task
and	Task
sequence	Task
labelling	Task
benchmarks	Task
show	O
that	O
the	O
proposed	O
model	O
has	O
strong	O
representation	O
power	O
,	O
giving	O
highly	O
competitive	O
performances	O
compared	O
to	O
stacked	Method
BiLSTM	Method
models	Method
with	O
similar	O
parameter	O
numbers	O
.	O

section	O
:	O
Introduction	O
Neural	Method
models	Method
have	O
become	O
the	O
dominant	O
approach	O
in	O
the	O
NLP	O
literature	O
.	O

Compared	O
to	O
handcrafted	O
indicator	O
features	O
,	O
neural	Method
sentence	Method
representations	Method
are	O
less	O
sparse	O
,	O
and	O
more	O
flexible	O
in	O
encoding	O
intricate	O
syntactic	O
and	O
semantic	O
information	O
.	O

Among	O
various	O
neural	Method
networks	Method
for	O
encoding	Task
sentences	Task
,	O
bi	Method
-	Method
directional	Method
LSTMs	Method
(	O
BiLSTM	Method
)	O
[	O
reference	O
]	O
have	O
been	O
a	O
dominant	O
method	O
,	O
giving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
language	Task
modelling	Task
[	O
reference	O
]	O
,	O
machine	Task
translation	Task
[	O
reference	O
]	O
,	O
syntactic	Task
parsing	Task
[	O
reference	O
]	O
and	O
question	Task
answering	Task
[	O
reference	O
]	O
.	O

Despite	O
their	O
success	O
,	O
BiLSTMs	Method
have	O
been	O
shown	O
to	O
suffer	O
several	O
limitations	O
.	O

For	O
example	O
,	O
their	O
inherently	O
sequential	O
nature	O
endows	O
computation	O
non	O
-	O
parallel	O
within	O
the	O
same	O
sentence	O
[	O
reference	O
]	O
,	O
which	O
can	O
lead	O
to	O
a	O
computational	Metric
bottleneck	Metric
,	O
hindering	O
their	O
use	O
in	O
the	O
in	Task
-	Task
dustry	Task
.	O

In	O
addition	O
,	O
local	Method
ngrams	Method
,	O
which	O
have	O
been	O
shown	O
a	O
highly	O
useful	O
source	O
of	O
contextual	O
information	O
for	O
NLP	Task
,	O
are	O
not	O
explicitly	O
modelled	O
[	O
reference	O
]	O
.	O

Finally	O
,	O
sequential	O
information	O
flow	O
leads	O
to	O
relatively	O
weaker	O
power	O
in	O
capturing	O
longrange	O
dependencies	O
,	O
which	O
results	O
in	O
lower	O
performance	O
in	O
encoding	O
longer	O
sentences	O
[	O
reference	O
]	O
.	O

We	O
investigate	O
an	O
alternative	O
recurrent	Method
neural	Method
network	Method
structure	Method
for	O
addressing	O
these	O
issues	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
main	O
idea	O
is	O
to	O
model	O
the	O
hidden	O
states	O
of	O
all	O
words	O
simultaneously	O
at	O
each	O
recurrent	O
step	O
,	O
rather	O
than	O
one	O
word	O
at	O
a	O
time	O
.	O

In	O
particular	O
,	O
we	O
view	O
the	O
whole	O
sentence	O
as	O
a	O
single	O
state	O
,	O
which	O
consists	O
of	O
sub	O
-	O
states	O
for	O
individual	O
words	O
and	O
an	O
overall	O
sentence	O
-	O
level	O
state	O
.	O

To	O
capture	O
local	O
and	O
non	O
-	O
local	O
contexts	O
,	O
states	O
are	O
updated	O
recurrently	O
by	O
exchanging	O
information	O
between	O
each	O
other	O
.	O

Consequently	O
,	O
we	O
refer	O
to	O
our	O
model	O
as	O
sentence	O
-	O
state	O
LSTM	Method
,	O
or	O
S	Method
-	Method
LSTM	Method
in	O
short	O
.	O

Empirically	O
,	O
S	Method
-	Method
LSTM	Method
can	O
give	O
effective	O
sentence	Task
encoding	Task
after	O
3	O
-	O
6	O
recurrent	O
steps	O
.	O

In	O
contrast	O
,	O
the	O
number	O
of	O
recurrent	O
steps	O
necessary	O
for	O
BiLSTM	Method
scales	O
with	O
the	O
size	O
of	O
the	O
sentence	O
.	O

At	O
each	O
recurrent	O
step	O
,	O
information	Task
exchange	Task
is	O
conducted	O
between	O
consecutive	O
words	O
in	O
the	O
sentence	O
,	O
and	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
each	O
word	O
.	O

In	O
particular	O
,	O
each	O
word	O
receives	O
information	O
from	O
its	O
predecessor	O
and	O
successor	O
simultaneously	O
.	O

From	O
an	O
initial	O
state	O
without	O
information	O
exchange	O
,	O
each	O
word	O
-	O
level	O
state	O
can	O
obtain	O
3	O
-	O
gram	O
,	O
5	O
-	O
gram	O
and	O
7	O
-	O
gram	O
information	O
after	O
1	O
,	O
2	O
and	O
3	O
recurrent	O
steps	O
,	O
respectively	O
.	O

Being	O
connected	O
with	O
every	O
word	O
,	O
the	O
sentence	O
-	O
level	O
state	O
vector	O
serves	O
to	O
exchange	O
non	O
-	O
local	O
information	O
with	O
each	O
word	O
.	O

In	O
addition	O
,	O
it	O
can	O
also	O
be	O
used	O
as	O
a	O
global	Method
sentence	Method
-	Method
level	Method
representation	Method
for	O
classification	Task
tasks	Task
.	O

Results	O
on	O
both	O
classification	Task
and	Task
sequence	Task
labelling	Task
show	O
that	O
S	Method
-	Method
LSTM	Method
gives	O
better	O
accuracies	Metric
compared	O
to	O
BiLSTM	Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
while	O
being	O
faster	O
.	O

We	O
release	O
our	O
code	O
and	O
models	O
at	O
https:	O
//	O
github.com	O
/	O
leuchine	O
/	O
S	O
-	O
LSTM	Method
,	O
which	O
include	O
all	O
baselines	O
and	O
the	O
final	O
model	O
.	O

section	O
:	O
Related	O
Work	O
LSTM	Method
[	O
reference	O
]	O
showed	O
its	O
early	O
potentials	O
in	O
NLP	Task
when	O
a	O
neural	Method
machine	Method
translation	Method
system	Method
that	O
leverages	O
LSTM	Method
source	O
encoding	O
gave	O
highly	O
competitive	O
results	O
compared	O
to	O
the	O
best	O
SMT	Method
models	Method
[	O
reference	O
]	O
.	O

LSTM	Method
encoders	O
have	O
since	O
been	O
explored	O
for	O
other	O
tasks	O
,	O
including	O
syntactic	Task
parsing	Task
[	O
reference	O
]	O
,	O
text	Task
classification	Task
[	O
reference	O
]	O
and	O
machine	Task
reading	Task
[	O
reference	O
]	O
.	O

Bidirectional	Method
extensions	Method
have	O
become	O
a	O
standard	O
configuration	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	Metric
among	O
various	O
tasks	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O

SLSTMs	Method
are	O
similar	O
to	O
BiLSTMs	Method
in	O
their	O
recurrent	O
bi	O
-	O
directional	O
message	O
flow	O
between	O
words	O
,	O
but	O
different	O
in	O
the	O
design	O
of	O
state	Task
transition	Task
.	O

CNNs	Method
[	O
reference	O
]	O
)	O
also	O
allow	O
better	O
parallelisation	O
compared	O
to	O
LSTMs	Method
for	O
sentence	Task
encoding	Task
[	O
reference	O
]	O
,	O
thanks	O
to	O
parallelism	O
among	O
convolution	Method
filters	Method
.	O

On	O
the	O
other	O
hand	O
,	O
convolution	O
features	O
embody	O
only	O
fix	O
-	O
sized	O
local	O
ngram	O
information	O
,	O
whereas	O
sentence	Method
-	Method
level	Method
feature	Method
aggregation	Method
via	O
pooling	Method
can	O
lead	O
to	O
loss	O
of	O
information	O
[	O
reference	O
]	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
uses	O
a	O
global	O
sentence	O
-	O
level	O
node	O
to	O
assemble	O
and	O
back	O
-	O
distribute	O
local	O
information	O
in	O
the	O
recurrent	Method
state	Method
transition	Method
process	Method
,	O
suffering	O
less	O
information	Metric
loss	Metric
compared	O
to	O
pooling	Task
.	O

Attention	Method
[	O
reference	O
]	O
has	O
recently	O
been	O
explored	O
as	O
a	O
standalone	O
method	O
for	O
sentence	Task
encoding	Task
,	O
giving	O
competitive	O
results	O
compared	O
to	O
Bi	O
-	O
LSTM	Method
encoders	O
for	O
neural	Task
machine	Task
translation	Task
[	O
reference	O
]	O
.	O

The	O
attention	Method
mechanism	Method
allows	O
parallelisation	Task
,	O
and	O
can	O
play	O
a	O
similar	O
role	O
to	O
the	O
sentence	O
-	O
level	O
state	O
in	O
S	Method
-	Method
LSTMs	Method
,	O
which	O
uses	O
neural	O
gates	O
to	O
integrate	O
word	O
-	O
level	O
information	O
compared	O
to	O
hierarchical	O
attention	O
.	O

S	Method
-	Method
LSTM	Method
further	O
allows	O
local	O
communication	O
between	O
neighbouring	O
words	O
.	O

Hierarchical	Method
stacking	Method
of	Method
CNN	Method
layers	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
allows	O
better	O
interaction	O
between	O
non	O
-	O
local	O
components	O
in	O
a	O
sentence	O
via	O
incremental	O
levels	O
of	O
abstraction	O
.	O

S	Method
-	Method
LSTM	Method
is	O
similar	O
to	O
hierarchical	Method
attention	Method
and	O
stacked	Method
CNN	Method
in	O
this	O
respect	O
,	O
incrementally	O
refining	O
sentence	Task
representations	Task
.	O

However	O
,	O
S	Method
-	Method
LSTM	Method
models	O
hierarchical	Method
encoding	Method
of	Method
sentence	Method
structure	Method
as	O
a	O
recurrent	Method
state	Method
transition	Method
process	Method
.	O

In	O
nature	O
,	O
our	O
work	O
belongs	O
to	O
the	O
family	O
of	O
LSTM	Method
sentence	O
representations	O
.	O

S	Method
-	Method
LSTM	Method
is	O
inspired	O
by	O
message	Method
passing	Method
over	Method
graphs	Method
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O

Graph	Method
-	Method
structure	Method
neural	Method
models	Method
have	O
been	O
used	O
for	O
computer	Task
program	Task
verification	Task
[	O
reference	O
]	O
and	O
image	Task
object	Task
detection	Task
[	O
reference	O
]	O
.	O

The	O
closest	O
previous	O
work	O
in	O
NLP	Task
includes	O
the	O
use	O
of	O
convolutional	Method
neural	Method
networks	Method
[	O
reference	O
]	O
and	O
DAG	Method
LSTMs	Method
[	O
reference	O
]	O
for	O
modelling	Task
syntactic	Task
structures	Task
.	O

Compared	O
to	O
our	O
work	O
,	O
their	O
motivations	O
and	O
network	O
structures	O
are	O
highly	O
different	O
.	O

In	O
particular	O
,	O
the	O
DAG	Method
LSTM	Method
of	O
[	O
reference	O
]	O
is	O
a	O
natural	O
extension	O
of	O
tree	Method
LSTM	Method
[	O
reference	O
]	O
,	O
and	O
is	O
sequential	O
rather	O
than	O
parallel	O
in	O
nature	O
.	O

To	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
investigate	O
a	O
graph	Method
RNN	Method
for	O
encoding	Task
sentences	Task
,	O
proposing	O
parallel	O
graph	O
states	O
for	O
integrating	O
word	O
-	O
level	O
and	O
sentence	O
-	O
level	O
information	O
.	O

In	O
this	O
perspective	O
,	O
our	O
contribution	O
is	O
similar	O
to	O
that	O
of	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
in	O
introducing	O
a	O
neural	Method
representation	Method
to	O
the	O
NLP	O
literature	O
.	O

section	O
:	O
Model	O
Given	O
a	O
sentence	O
s	O
=	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O

.	O

.	O

,	O
w	O
n	O
,	O
where	O
w	O
i	O
represents	O
the	O
ith	O
word	O
and	O
n	O
is	O
the	O
sentence	O
length	O
,	O
our	O
goal	O
is	O
to	O
find	O
a	O
neural	Method
representation	Method
of	Method
s	Method
,	O
which	O
consists	O
of	O
a	O
hidden	O
vector	O
h	O
i	O
for	O
each	O
input	O
word	O
w	O
i	O
,	O
and	O
a	O
global	O
sentence	O
-	O
level	O
hidden	O
vector	O
g.	O
Here	O
h	O
i	O
represents	O
syntactic	O
and	O
semantic	O
features	O
for	O
w	O
i	O
under	O
the	O
sentential	O
context	O
,	O
while	O
g	O
represents	O
features	O
for	O
the	O
whole	O
sentence	O
.	O

Following	O
previous	O
work	O
,	O
we	O
additionally	O
add	O
s	O
and	O
/	O
s	O
to	O
the	O
two	O
ends	O
of	O
the	O
sentence	O
as	O
w	O
0	O
and	O
w	O
n	O
+	O
1	O
,	O
respectively	O
.	O

section	O
:	O
Baseline	O
BiLSTM	Method
The	O
baseline	Method
BiLSTM	Method
model	Method
consists	O
of	O
two	O
LSTM	Method
components	O
,	O
which	O
process	O
the	O
input	O
in	O
the	O
forward	O
left	O
-	O
to	O
-	O
right	O
and	O
the	O
backward	O
rightto	O
-	O
left	O
directions	O
,	O
respectively	O
.	O

In	O
each	O
direction	O
,	O
the	O
reading	O
of	O
input	O
words	O
is	O
modelled	O
as	O
a	O
recurrent	Method
process	Method
with	O
a	O
single	O
hidden	O
state	O
.	O

Given	O
an	O
initial	O
value	O
,	O
the	O
state	O
changes	O
its	O
value	O
recurrently	O
,	O
each	O
time	O
consuming	O
an	O
incoming	O
word	O
.	O

Take	O
the	O
forward	O
LSTM	Method
component	O
for	O
example	O
.	O

Denoting	O
the	O
initial	O
state	O
as	O
−	O
→	O
h	O
0	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
the	O
recurrent	O
state	O
transition	O
step	O
for	O
calculating	O
−	O
→	O
h	O
1	O
,	O
.	O

.	O

.	O

,	O
−	O
→	O
h	O
n	O
+	O
1	O
is	O
defined	O
as	O
follows	O
[	O
reference	O
]	O
:	O
where	O
x	O
t	O
denotes	O
the	O
word	Method
representation	Method
of	Method
w	Method
t	Method
;	O
i	O
t	O
,	O
o	O
t	O
,	O
f	O
t	O
and	O
u	O
t	O
represent	O
the	O
values	O
of	O
an	O
input	O
gate	O
,	O
an	O
output	O
gate	O
,	O
a	O
forget	O
gate	O
and	O
an	O
actual	O
input	O
at	O
time	O
step	O
t	O
,	O
respectively	O
,	O
which	O
controls	O
the	O
information	O
flow	O
for	O
a	O
recurrent	Method
cell	Method
−	O
→	O
c	O
t	O
and	O
the	O
state	O
vector	O
−	O
→	O
h	O
t	O
;	O
W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
i	O
,	O
o	O
,	O
f	O
,	O
u	O
}	O
)	O
are	O
model	O
parameters	O
.	O

σ	Method
is	O
the	O
sigmoid	Method
function	Method
.	O

The	O
backward	O
LSTM	Method
component	O
follows	O
the	O
same	O
recurrent	Method
state	Method
transition	Method
process	Method
as	O
described	O
in	O
Eq	O
1	O
.	O

Starting	O
from	O
an	O
initial	O
state	O
h	O
n	O
+	O
1	O
,	O
which	O
is	O
a	O
model	O
parameter	O
,	O
it	O
reads	O
the	O
input	O
x	O
n	O
,	O
x	O
n−1	O
,	O
.	O

.	O

.	O

,	O
x	O
0	O
,	O
changing	O
its	O
value	O
to	O
The	O
BiLSTM	Method
model	O
uses	O
the	O
concatenated	O
value	O
of	O
−	O
→	O
h	O
t	O
and	O
←	O
−	O
h	O
t	O
as	O
the	O
hidden	O
vector	O
for	O
w	O
t	O
:	O
A	O
single	O
hidden	Method
vector	Method
representation	Method
g	Method
of	O
the	O
whole	O
input	O
sentence	O
can	O
be	O
obtained	O
using	O
the	O
final	O
state	O
values	O
of	O
the	O
two	O
LSTM	Method
components	O
:	O
Stacked	Method
BiLSTM	Method
Multiple	Method
layers	Method
of	Method
BiLTMs	Method
can	O
be	O
stacked	O
for	O
increased	O
representation	O
power	O
,	O
where	O
the	O
hidden	O
vectors	O
of	O
a	O
lower	O
layer	O
are	O
used	O
as	O
inputs	O
for	O
an	O
upper	O
layer	O
.	O

Different	O
model	O
parameters	O
are	O
used	O
in	O
each	O
stacked	Method
BiLSTM	Method
layer	Method
.	O

section	O
:	O
Sentence	Method
-	Method
State	Method
LSTM	Method
Formally	O
,	O
an	O
S	O
-	O
LSTM	Method
state	O
at	O
time	O
step	O
t	O
can	O
be	O
denoted	O
by	O
:	O
which	O
consists	O
of	O
a	O
sub	O
state	O
h	O
t	O
i	O
for	O
each	O
word	O
w	O
i	O
and	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
g	O
t	O
.	O

S	Method
-	Method
LSTM	Method
uses	O
a	O
recurrent	Method
state	Method
transition	Method
process	Method
to	O
model	O
information	O
exchange	O
between	O
sub	O
states	O
,	O
which	O
enriches	O
state	Method
representations	Method
incrementally	O
.	O

For	O
the	O
initial	O
state	O
H	O
0	O
,	O
we	O
set	O
h	O
0	O
i	O
=	O
g	O
0	O
=	O
h	O
0	O
,	O
where	O
h	O
0	O
is	O
a	O
parameter	O
.	O

The	O
state	O
transition	O
from	O
H	O
t−1	O
to	O
H	O
t	O
consists	O
of	O
sub	O
state	O
transitions	O
from	O
h	O
t−1	O
i	O
to	O
h	O
t	O
i	O
and	O
from	O
g	O
t−1	O
to	O
g	O
t	O
.	O

We	O
take	O
an	O
LSTM	Method
structure	O
similar	O
to	O
the	O
baseline	O
BiLSTM	Method
for	O
modelling	Task
state	Task
transition	Task
,	O
using	O
a	O
recurrent	O
cell	O
c	O
t	O
i	O
for	O
each	O
w	O
i	O
and	O
a	O
cell	O
c	O
t	O
g	O
for	O
g.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
value	O
of	O
each	O
h	O
t	O
i	O
is	O
computed	O
based	O
on	O
the	O
values	O
of	O
x	O
i	O
,	O
h	O
i	O
+	O
1	O
and	O
g	O
t−1	O
,	O
together	O
with	O
their	O
corresponding	O
cell	O
values	O
:	O
where	O
ξ	O
t	O
i	O
is	O
the	O
concatenation	O
of	O
hidden	O
vectors	O
of	O
a	O
context	O
window	O
,	O
and	O
where	O
f	O
t	O
0	O
,	O
.	O

.	O

.	O

,	O
f	O
t	O
n	O
+	O
1	O
and	O
f	O
t	O
g	O
are	O
gates	O
controlling	O
information	O
from	O
c	O
t−1	O
0	O
,	O
.	O

.	O

.	O

,	O
c	O
t−1	O
n	O
+	O
1	O
and	O
c	O
t−1	O
g	O
,	O
respectively	O
,	O
which	O
are	O
normalised	O
.	O

o	O
t	O
is	O
an	O
output	O
gate	O
from	O
the	O
recurrent	Method
cell	Method
c	O
t	O
g	O
to	O
g	O
t	O
.	O

W	O
x	O
,	O
U	O
x	O
and	O
b	O
x	O
(	O
x	O
∈	O
{	O
g	O
,	O
f	O
,	O
o	O
}	O
)	O
are	O
model	O
parameters	O
.	O

section	O
:	O
Contrast	O
with	O
BiLSTM	Method
The	O
difference	O
between	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
can	O
be	O
understood	O
with	O
respect	O
to	O
their	O
recurrent	O
states	O
.	O

While	O
BiL	Method
-	Method
STM	Method
uses	O
only	O
one	O
state	O
in	O
each	O
direction	O
to	O
represent	O
the	O
subsequence	O
from	O
the	O
beginning	O
to	O
a	O
certain	O
word	O
,	O
S	Method
-	Method
LSTM	Method
uses	O
a	O
structural	O
state	O
to	O
represent	O
the	O
full	O
sentence	O
,	O
which	O
consists	O
of	O
a	O
sentence	O
-	O
level	O
sub	O
state	O
and	O
n	O
+	O
2	O
word	O
-	O
level	O
sub	O
states	O
,	O
simultaneously	O
.	O

Different	O
from	O
BiLSTMs	Method
,	O
for	O
which	O
h	O
t	O
at	O
different	O
time	O
steps	O
are	O
used	O
to	O
represent	O
w	O
0	O
,	O
.	O

.	O

.	O

,	O
w	O
n	O
+	O
1	O
,	O
respectively	O
,	O
the	O
word	O
-	O
level	O
states	O
h	O
t	O
i	O
and	O
sentence	O
-	O
level	O
state	O
g	O
t	O
of	O
S	Method
-	Method
LSTMs	Method
directly	O
correspond	O
to	O
the	O
goal	O
outputs	O
h	O
i	O
and	O
g	O
,	O
as	O
introduced	O
in	O
the	O
beginning	O
of	O
this	O
section	O
.	O

As	O
t	O
increases	O
from	O
0	O
,	O
h	O
t	O
i	O
and	O
g	O
t	O
are	O
enriched	O
with	O
increasingly	O
deeper	O
context	O
information	O
.	O

From	O
the	O
perspective	O
of	O
information	Task
flow	Task
,	O
BiL	Method
-	Method
STM	Method
passes	O
information	O
from	O
one	O
end	O
of	O
the	O
sentence	O
to	O
the	O
other	O
.	O

As	O
a	O
result	O
,	O
the	O
number	O
of	O
time	O
steps	O
scales	O
with	O
the	O
size	O
of	O
the	O
input	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
allows	O
bi	O
-	O
directional	O
information	O
flow	O
at	O
each	O
word	O
simultaneously	O
,	O
and	O
additionally	O
between	O
the	O
sentence	O
-	O
level	O
state	O
and	O
every	O
wordlevel	O
state	O
.	O

At	O
each	O
step	O
,	O
each	O
h	O
i	O
captures	O
an	O
increasing	O
larger	O
ngram	O
context	O
,	O
while	O
additionally	O
communicating	O
globally	O
to	O
all	O
other	O
h	O
j	O
via	O
g.	O
The	O
optimal	O
number	O
of	O
recurrent	O
steps	O
is	O
decided	O
by	O
the	O
end	Metric
-	Metric
task	Metric
performance	Metric
,	O
and	O
does	O
not	O
necessarily	O
scale	O
with	O
the	O
sentence	O
size	O
.	O

As	O
a	O
result	O
,	O
S	Method
-	Method
LSTM	Method
can	O
potentially	O
be	O
both	O
more	O
efficient	O
and	O
more	O
accurate	O
compared	O
with	O
BiLSTMs	Method
.	O

Increasing	O
window	O
size	O
.	O

By	O
default	O
S	Method
-	Method
LSTM	Method
exchanges	O
information	O
only	O
between	O
neighbouring	O
words	O
,	O
which	O
can	O
be	O
seen	O
as	O
adopting	O
a	O
1	O
-	O
word	O
window	O
on	O
each	O
side	O
.	O

The	O
window	O
size	O
can	O
be	O
extended	O
to	O
2	O
,	O
3	O
or	O
more	O
words	O
in	O
order	O
to	O
allow	O
more	O
communication	O
in	O
a	O
state	O
transition	O
,	O
expediting	O
information	Task
exchange	Task
.	O

To	O
this	O
end	O
,	O
we	O
modify	O
Eq	O
2	O
,	O
integrating	O
additional	O
context	O
words	O
to	O
ξ	O
t	O
i	O
,	O
with	O
extended	O
gates	O
and	O
cells	O
.	O

For	O
example	O
,	O
with	O
a	O
window	O
size	O
of	O
2	O
,	O
We	O
study	O
the	O
effectiveness	O
of	O
window	O
size	O
in	O
our	O
experiments	O
.	O

Additional	O
sentence	O
-	O
level	O
nodes	O
.	O

By	O
default	O
S	Method
-	Method
LSTM	Method
uses	O
one	O
sentence	O
-	O
level	O
node	O
.	O

One	O
way	O
of	O
enriching	O
the	O
parameter	O
space	O
is	O
to	O
add	O
more	O
sentence	O
-	O
level	O
nodes	O
,	O
each	O
communicating	O
with	O
word	O
-	O
level	O
nodes	O
in	O
the	O
same	O
way	O
as	O
described	O
by	O
Eq	O
3	O
.	O

In	O
addition	O
,	O
different	O
sentence	O
-	O
level	O
nodes	O
can	O
communicate	O
with	O
each	O
other	O
during	O
state	O
transition	O
.	O

When	O
one	O
sentence	O
-	O
level	O
node	O
is	O
used	O
for	O
classification	O
outputs	O
,	O
the	O
other	O
sentencelevel	O
node	O
can	O
serve	O
as	O
hidden	O
memory	O
units	O
,	O
or	O
latent	O
features	O
.	O

We	O
study	O
the	O
effectiveness	O
of	O
multiple	O
sentence	O
-	O
level	O
nodes	O
empirically	O
.	O

section	O
:	O
Task	O
settings	O
We	O
consider	O
two	O
task	O
settings	O
,	O
namely	O
classification	Task
and	O
sequence	Task
labelling	Task
.	O

For	O
classification	Task
,	O
g	O
is	O
fed	O
to	O
a	O
softmax	Method
classification	Method
layer	Method
:	O
where	O
y	O
is	O
the	O
probability	O
distribution	O
of	O
output	O
class	O
labels	O
and	O
W	O
c	O
and	O
b	O
c	O
are	O
model	O
parameters	O
.	O

For	O
sequence	Task
labelling	Task
,	O
each	O
h	O
i	O
can	O
be	O
used	O
as	O
feature	Method
representation	Method
for	O
a	O
corresponding	O
word	O
w	O
i	O
.	O

External	O
attention	O
It	O
has	O
been	O
shown	O
that	O
summation	O
of	O
hidden	O
states	O
using	O
attention	O
[	O
reference	O
][	O
reference	O
]	O
give	O
better	O
accuracies	Metric
compared	O
to	O
using	O
the	O
end	O
states	O
of	O
BiLSTMs	Method
.	O

We	O
study	O
the	O
influence	O
of	O
attention	O
on	O
both	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
for	O
classification	Task
.	O

In	O
particular	O
,	O
additive	O
attention	O
(	O
Bahdanau	O
Here	O
W	O
α	O
,	O
u	O
and	O
b	O
α	O
are	O
model	O
parameters	O
.	O

External	O
CRF	O
For	O
sequential	Task
labelling	Task
,	O
we	O
use	O
a	O
CRF	Method
layer	Method
on	O
top	O
of	O
the	O
hidden	O
vectors	O
h	O
1	O
,	O
h	O
2	O
,	O
.	O

.	O

.	O

,	O
h	O
n	O
for	O
calculating	O
the	O
conditional	O
probabilities	O
of	O
label	O
sequences	O
[	O
reference	O
]	O
:	O
where	O
W	O
y	O
i−1	O
,	O
y	O
i	O
s	O
and	O
b	O
y	O
i−1	O
,	O
y	O
i	O
s	O
are	O
parameters	O
specific	O
to	O
two	O
consecutive	O
labels	O
y	O
i−1	O
and	O
y	O
i	O
.	O

For	O
training	Task
,	O
standard	O
log	Method
-	Method
likelihood	Method
loss	Method
is	O
used	O
with	O
L	Method
2	Method
regularization	Method
given	O
a	O
set	O
of	O
gold	O
-	O
standard	O
instances	O
.	O

section	O
:	O
Experiments	O
We	O
empirically	O
compare	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
on	O
different	O
classification	Task
and	Task
sequence	Task
labelling	Task
tasks	Task
.	O

All	O
experiments	O
are	O
conducted	O
using	O
a	O
GeForce	Method
GTX	Method
1080	Method
GPU	Method
with	O
8	O
GB	O
memory	O
.	O

[	O
reference	O
]	O
)	O
.	O

Statistics	O
of	O
the	O
four	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
.	O

Hyperparameters	Method
.	O

We	O
initialise	O
word	O
embeddings	O
using	O
GloVe	Method
[	O
reference	O
]	O
)	O
300	Method
dimensional	Method
embeddings	Method
.	O

1	O
Embeddings	Method
are	O
finetuned	O
during	O
model	Method
training	Method
for	O
all	O
tasks	O
.	O

Dropout	Method
[	O
reference	O
]	O
)	O
is	O
applied	O
to	O
embedding	O
hidden	O
states	O
,	O
with	O
a	O
rate	O
of	O
0.5	O
.	O

All	O
models	O
are	O
optimised	O
using	O
the	O
Adam	Method
optimizer	Method
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
,	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.001	O
and	O
a	O
decay	Metric
rate	Metric
of	Metric
0.97	Metric
.	O

Gradients	O
are	O
clipped	O
at	O
3	O
and	O
a	O
batch	O
size	O
of	O
10	O
is	O
adopted	O
.	O

Sentences	O
with	O
similar	O
lengths	O
are	O
batched	O
together	O
.	O

The	O
L2	O
regularization	O
parameter	O
is	O
set	O
to	O
0.001	O
.	O

section	O
:	O
Development	O
Experiments	O
We	O
use	O
the	O
movie	Material
review	Material
development	O
data	O
to	O
investigate	O
different	O
configurations	O
of	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
.	O

For	O
S	Method
-	Method
LSTMs	Method
,	O
the	O
default	O
configuration	O
uses	O
s	O
and	O
/	O
s	O
words	O
for	O
augmenting	O
words	O
Hyperparameters	O
:	O
Table	O
2	O
shows	O
the	O
development	O
results	O
of	O
various	O
S	O
-	O
LSTM	Method
settings	O
,	O
where	O
Time	O
refers	O
to	O
training	O
time	O
per	O
epoch	O
.	O

Without	O
the	O
sentence	O
-	O
level	O
node	O
,	O
the	O
accuracy	Metric
of	O
S	Method
-	Method
LSTM	Method
drops	O
to	O
81.76	O
%	O
,	O
demonstrating	O
the	O
necessity	O
of	O
global	Task
information	Task
exchange	Task
.	O

Adding	O
one	O
additional	O
sentence	O
-	O
level	O
node	O
as	O
described	O
in	O
Section	O
3.2	O
does	O
not	O
lead	O
to	O
accuracy	Metric
improvements	O
,	O
although	O
the	O
number	O
of	O
parameters	O
and	O
decoding	Metric
time	Metric
increase	O
accordingly	O
.	O

As	O
a	O
result	O
,	O
we	O
use	O
only	O
1	O
sentence	O
-	O
level	O
node	O
for	O
the	O
remaining	O
experiments	O
.	O

The	O
accuracies	Metric
of	O
S	Method
-	Method
LSTM	Method
increases	O
as	O
the	O
hidden	O
layer	O
size	O
for	O
each	O
node	O
increases	O
from	O
100	O
to	O
300	O
,	O
but	O
does	O
not	O
further	O
increase	O
when	O
the	O
size	O
increases	O
beyond	O
300	O
.	O

We	O
fix	O
the	O
hidden	O
size	O
to	O
300	O
accordingly	O
.	O

Without	O
using	O
s	O
and	O
/	Method
s	Method
,	O
the	O
performance	O
of	O
S	Method
-	Method
LSTM	Method
drops	O
from	O
82.64	O
%	O
to	O
82.36	O
%	O
,	O
showing	O
the	O
effectiveness	O
of	O
having	O
these	O
additional	O
nodes	O
.	O

Hyperparameters	Method
for	O
BiLSTM	Method
models	O
are	O
also	O
set	O
according	O
to	O
the	O
development	O
data	O
,	O
which	O
we	O
omit	O
here	O
.	O

State	O
transition	O
.	O

In	O
Table	O
2	O
,	O
the	O
number	O
of	O
recurrent	O
state	O
transition	O
steps	O
of	O
S	Method
-	Method
LSTM	Method
is	O
decided	O
according	O
to	O
the	O
best	O
development	O
performance	O
.	O

Figure	O
2	O
draws	O
the	O
development	O
accuracies	Metric
of	O
SLSTMs	Method
with	O
various	O
window	O
sizes	O
against	O
the	O
number	O
of	O
recurrent	O
steps	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
when	O
the	O
number	O
of	O
time	O
steps	O
increases	O
from	O
1	O
to	O
11	O
,	O
the	O
accuracies	Metric
generally	O
increase	O
,	O
before	O
reaching	O
a	O
maximum	O
value	O
.	O

This	O
shows	O
the	O
effectiveness	O
of	O
recurrent	Task
information	Task
exchange	Task
in	O
S	O
-	O
LSTM	Method
state	O
transition	O
.	O

On	O
the	O
other	O
hand	O
,	O
no	O
significant	O
differences	O
are	O
observed	O
on	O
the	O
peak	O
accuracies	Metric
given	O
by	O
different	O
window	O
sizes	O
,	O
although	O
a	O
larger	O
window	O
size	O
(	O
e.g.	O
4	O
)	O
generally	O
results	O
in	O
faster	O
plateauing	O
.	O

This	O
can	O
be	O
be	O
explained	O
by	O
the	O
intuition	O
that	O
information	O
exchange	O
between	O
distant	O
nodes	O
can	O
be	O
achieved	O
using	O
more	O
recurrent	O
steps	O
under	O
a	O
smaller	O
window	O
size	O
,	O
as	O
can	O
be	O
achieved	O
using	O
fewer	O
steps	O
under	O
a	O
larger	O
window	O
size	O
.	O

Considering	O
efficiency	O
,	O
we	O
choose	O
a	O
window	O
size	O
of	O
1	O
for	O
the	O
remaining	O
experiments	O
,	O
setting	O
the	O
number	O
of	O
recurrent	O
steps	O
to	O
9	O
according	O
to	O
Figure	O
2	O
.	O

S	Method
-	Method
LSTM	Method
vs	O
BiLSTM	Method
:	O
As	O
shown	O
in	O
Table	O
3	O
,	O
BiLSTM	Method
gives	O
significantly	O
better	O
accuracies	Metric
compared	O
to	O
uni	O
-	O
directional	O
LSTM	Method
2	O
,	O
with	O
the	O
training	Metric
time	Metric
per	O
epoch	O
growing	O
from	O
67	O
seconds	O
to	O
106	O
seconds	O
.	O

Stacking	O
2	O
layers	O
of	O
BiLSTM	Method
gives	O
further	O
improvements	O
to	O
development	O
results	O
,	O
with	O
a	O
larger	O
time	O
of	O
207	O
seconds	O
.	O

3	O
layers	O
of	O
stacked	O
BiLSTM	Method
does	O
not	O
further	O
improve	O
the	O
results	O
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
a	O
development	O
result	O
of	O
82.64	O
%	O
,	O
which	O
is	O
significantly	O
better	O
compared	O
to	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
,	O
with	O
a	O
smaller	O
number	O
of	O
model	O
parameters	O
and	O
a	O
shorter	O
time	O
of	O
65	O
seconds	O
.	O

We	O
additionally	O
make	O
comparisons	O
with	O
stacked	Method
CNNs	Method
and	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
shown	O
in	O
Table	O
3	O
(	O
the	O
CNN	O
and	O
Transformer	O
rows	O
)	O
,	O
where	O
N	O
indicates	O
the	O
number	O
of	O
attention	O
layers	O
.	O

CNN	Method
is	O
the	O
most	O
efficient	O
among	O
all	O
models	O
compared	O
,	O
with	O
the	O
smallest	O
model	O
size	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
3	Method
-	Method
layer	Method
stacked	Method
CNN	Method
gives	O
an	O
accuracy	Metric
of	O
81.46	O
%	O
,	O
which	O
is	O
also	O
the	O
lowest	O
compared	O
with	O
BiLSTM	Method
,	O
hierarchical	Method
attention	Method
and	O
S	Method
-	Method
LSTM	Method
.	O

The	O
best	O
performance	O
of	O
hierarchical	Task
attention	Task
is	O
between	O
single	Method
-	Method
layer	Method
and	O
two	Method
-	Method
layer	Method
BiLSTMs	Method
in	O
terms	O
of	O
both	O
accuracy	Metric
and	O
efficiency	Metric
.	O

S	Method
-	Method
LSTM	Method
gives	O
significantly	O
better	O
accuracies	Metric
compared	O
with	O
both	O
CNN	Method
and	O
hierarchical	Method
attention	Method
.	O

Influence	O
of	O
external	Method
attention	Method
mechanism	Method
.	O

Table	O
3	O
additionally	O
shows	O
the	O
results	O
of	O
BiLSTM	Method
and	O
S	Method
-	Method
LSTM	Method
when	O
external	O
attention	O
is	O
used	O
as	O
described	O
in	O
Section	O
3.3	O
.	O

Attention	Method
leads	O
to	O
improved	O
accuracies	Metric
for	O
both	O
BiLSTM	Method
and	O
S	Method
-	Method
LSTM	Method
in	O
classification	Task
,	O
with	O
S	Method
-	Method
LSTM	Method
still	O
outperforming	O
BiLSTM	Method
significantly	O
.	O

The	O
result	O
suggests	O
that	O
external	Method
techniques	Method
such	O
as	O
attention	O
can	O
play	O
orthogonal	O
roles	O
compared	O
with	O
internal	O
recurrent	O
structures	O
,	O
therefore	O
benefiting	O
both	O
BiLSTMs	Method
and	O
S	Method
-	Method
LSTMs	Method
.	O

Similar	O
observations	O
are	O
found	O
using	O
external	Method
CRF	Method
layers	Method
for	O
sequence	Task
labelling	Task
.	O

section	O
:	O
Final	O
Results	O
for	O
Classification	Task
The	O
final	O
results	O
on	O
the	O
movie	Material
review	Material
and	O
rich	O
text	O
classification	O
datasets	O
are	O
shown	O
in	O
Tables	O
4	O
and	O
5	O
,	O
respectively	O
.	O

In	O
addition	O
to	O
training	Metric
time	Metric
per	O
epoch	O
,	O
test	Metric
times	Metric
are	O
additionally	O
reported	O
.	O

We	O
use	O
the	O
best	O
settings	O
on	O
the	O
movie	Material
review	Material
development	O
dataset	O
for	O
both	O
S	Method
-	Method
LSTMs	Method
and	O
BiLSTMs	Method
.	O

The	O
step	O
number	O
for	O
S	Method
-	Method
LSTMs	Method
is	O
set	O
to	O
9	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
the	O
final	O
results	O
on	O
the	O
movie	Material
review	Material
dataset	Material
are	O
consistent	O
with	O
the	O
development	O
results	O
,	O
where	O
S	Method
-	Method
LSTM	Method
outperforms	O
BiL	Method
-	Method
STM	Method
significantly	O
,	O
with	O
a	O
faster	O
speed	O
.	O

Observations	O
on	O
CNN	Method
and	O
hierarchical	Task
attention	Task
are	O
consistent	O
with	O
the	O
development	O
results	O
.	O

S	Method
-	Method
LSTM	Method
also	O
gives	O
highly	O
competitive	O
results	O
when	O
compared	O
with	O
existing	O
methods	O
in	O
the	O
literature	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
among	O
the	O
16	O
datasets	O
of	O
[	O
reference	O
]	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
the	O
best	O
results	O
on	O
12	O
,	O
compared	O
with	O
BiLSTM	Method
and	O
2	O
layered	O
BiL	Method
-	Method
STM	Method
models	O
.	O

The	O
average	O
accuracy	Metric
of	O
S	Method
-	Method
LSTM	Method
is	O
85.6	O
%	O
,	O
significantly	O
higher	O
compared	O
with	O
84.9	O
%	O
by	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
.	O

3	Method
-	Method
layer	Method
stacked	Method
BiL	Method
-	Method
STM	Method
gives	O
an	O
average	O
accuracy	Metric
of	O
84.57	O
%	O
,	O
which	O
is	O
lower	O
compared	O
to	O
a	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
,	O
with	O
a	O
training	Metric
time	Metric
per	O
epoch	O
of	O
423.6	O
seconds	O
.	O

The	O
relative	Metric
speed	Metric
advantage	Metric
of	O
S	Method
-	Method
LSTM	Method
over	O
BiLSTM	Method
is	O
larger	O
on	O
the	O
16	O
datasets	O
as	O
compared	O
to	O
the	O
movie	Material
review	Material
test	O
test	O
.	O

This	O
is	O
because	O
the	O
average	O
length	O
of	O
inputs	O
is	O
larger	O
on	O
the	O
16	O
datasets	O
(	O
see	O
Section	O
4.5	O
)	O
.	O

section	O
:	O
Final	O
Results	O
for	O
Sequence	Task
Labelling	Task
Bi	Method
-	Method
directional	Method
RNN	Method
-	Method
CRF	Method
structures	Method
,	O
and	O
in	O
particular	O
BiLSTM	Method
-	O
CRFs	O
,	O
have	O
achieved	O
the	O
state	O
of	O
the	O
art	O
in	O
the	O
literature	O
for	O
sequence	Task
labelling	Task
tasks	Task
,	O
including	O
POS	Task
-	Task
tagging	Task
and	O
NER	Task
.	O

We	O
compare	O
S	Method
-	Method
LSTM	Method
-	Method
CRF	Method
with	O
BiLSTM	Method
-	Method
CRF	Method
for	O
sequence	Task
labelling	Task
,	O
using	O
the	O
same	O
settings	O
as	O
decided	O
on	O
the	O
movie	Material
review	Material
development	O
experiments	O
for	O
both	O
BiLSTMs	Method
and	O
S	Method
-	Method
LSTMs	Method
.	O

For	O
the	O
latter	O
,	O
we	O
decide	O
the	O
number	O
of	O
recurrent	O
steps	O
on	O
the	O
respective	O
development	O
sets	O
for	O
sequence	Task
labelling	Task
.	O

The	O
POS	Metric
accuracies	Metric
and	O
NER	Metric
F1	Metric
-	Metric
scores	Metric
against	O
the	O
number	O
of	O
recurrent	O
steps	O
are	O
shown	O
in	O
Figure	O
3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
respectively	O
.	O

For	O
POS	Task
tagging	Task
,	O
the	O
best	O
step	O
number	O
is	O
set	O
to	O
7	O
,	O
with	O
a	O
development	O
accuracy	Metric
of	O
97.58	O
%	O
.	O

For	O
NER	Task
,	O
the	O
step	O
number	O
is	O
set	O
to	O
9	O
,	O
with	O
a	O
development	O
F1	Metric
-	Metric
score	Metric
of	O
94.98	O
%	O
.	O

As	O
can	O
be	O
seen	O
in	O
(	O
Table	O
7	O
)	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
an	O
F1	Metric
-	Metric
score	Metric
of	O
91.57	O
%	O
on	O
the	O
CoNLL	Material
test	Material
set	Material
,	O
which	O
is	O
significantly	O
better	O
compared	O
with	O
BiLSTMs	Method
.	O

Stacking	O
more	O
layers	O
of	O
BiLSTMs	Method
leads	O
to	O
slightly	O
better	O
F1	Metric
-	Metric
scores	Metric
compared	O
with	O
a	O
single	Method
-	Method
layer	Method
BiL	Method
-	Method
STM	Method
.	O

Our	O
BiLSTM	Method
results	O
are	O
comparable	O
to	O
the	O
results	O
reported	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
who	O
also	O
use	O
bidirectional	Method
RNN	Method
-	Method
CRF	Method
structures	Method
.	O

In	O
contrast	O
,	O
S	Method
-	Method
LSTM	Method
gives	O
the	O
best	O
reported	O
results	O
under	O
the	O
same	O
settings	O
.	O

In	O
the	O
second	O
section	O
of	O
Table	O
7	O
learning	O
using	O
additional	O
language	O
model	O
objectives	O
,	O
obtaining	O
an	O
F	Metric
-	Metric
score	Metric
of	O
86.26	O
%	O
;	O
[	O
reference	O
]	O
leverage	O
character	Method
-	Method
level	Method
language	Method
models	Method
,	O
obtaining	O
an	O
F	Metric
-	Metric
score	Metric
of	O
91.93	O
%	O
,	O
which	O
is	O
the	O
current	O
best	O
result	O
on	O
the	O
dataset	O
.	O

All	O
the	O
three	O
models	O
are	O
based	O
on	O
BiLSTM	Method
-	Method
CRF	Method
.	O

On	O
the	O
other	O
hand	O
,	O
these	O
semi	Method
-	Method
supervised	Method
learning	Method
techniques	Method
are	O
orthogonal	O
to	O
our	O
work	O
,	O
and	O
can	O
potentially	O
be	O
used	O
for	O
S	Method
-	Method
LSTM	Method
also	O
.	O

section	O
:	O
Analysis	O
Figure	O
4	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
the	O
accuracies	Metric
against	O
the	O
sentence	O
length	O
on	O
the	O
movie	Material
review	Material
and	O
CoNLL	Material
datasets	Material
,	O
respectively	O
,	O
where	O
test	O
samples	O
are	O
binned	O
in	O
batches	O
of	O
80	O
.	O

We	O
find	O
that	O
the	O
performances	O
of	O
both	O
S	Method
-	Method
LSTM	Method
and	O
BiLSTM	Method
decrease	O
as	O
the	O
sentence	O
length	O
increases	O
.	O

On	O
the	O
other	O
hand	O
,	O
S	Method
-	Method
LSTM	Method
demonstrates	O
relatively	O
better	O
robustness	Metric
compared	O
to	O
BiLSTMs	Method
.	O

This	O
confirms	O
our	O
intuition	O
that	O
a	O
sentence	O
-	O
level	O
node	O
can	O
facilitate	O
better	O
non	Task
-	Task
local	Task
communication	Task
.	O

these	O
comparisons	O
,	O
we	O
mix	O
all	O
training	O
instances	O
,	O
order	O
them	O
by	O
the	O
size	O
,	O
and	O
put	O
them	O
into	O
10	O
equal	O
groups	O
,	O
the	O
medium	O
sentence	O
lengths	O
of	O
which	O
are	O
shown	O
.	O

As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
the	O
speed	Metric
advantage	Metric
of	O
S	Method
-	Method
LSTM	Method
is	O
larger	O
when	O
the	O
size	O
of	O
the	O
input	O
text	O
increases	O
,	O
thanks	O
to	O
a	O
fixed	O
number	O
of	O
recurrent	O
steps	O
.	O

Similar	O
to	O
hierarchical	O
attention	O
[	O
reference	O
]	O
,	O
there	O
is	O
a	O
relative	O
disadvantage	O
of	O
S	Method
-	Method
LSTM	Method
in	O
comparison	O
with	O
BiLSTM	Method
,	O
which	O
is	O
that	O
the	O
memory	Metric
consumption	Metric
is	O
relatively	O
larger	O
.	O

For	O
example	O
,	O
over	O
the	O
movie	Material
review	Material
development	O
set	O
,	O
the	O
actual	O
GPU	Metric
memory	Metric
consumption	Metric
by	O
S	Method
-	Method
LSTM	Method
,	O
BiLSTM	Method
,	O
2	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
and	O
4	Method
-	Method
layer	Method
stacked	Method
BiLSTM	Method
are	O
252	O
M	O
,	O
89	O
M	O
,	O
146	O
M	O
and	O
253	O
M	O
,	O
respectively	O
.	O

This	O
is	O
due	O
to	O
the	O
fact	O
that	O
computation	O
is	O
performed	O
in	O
parallel	O
by	O
S	Method
-	Method
LSTM	Method
and	O
hierarchical	Method
attention	Method
.	O

section	O
:	O
Conclusion	O
We	O
have	O
investigated	O
S	Method
-	Method
LSTM	Method
,	O
a	O
recurrent	Method
neural	Method
network	Method
for	O
encoding	Task
sentences	Task
,	O
which	O
offers	O
richer	O
contextual	O
information	O
exchange	O
with	O
more	O
parallelism	O
compared	O
to	O
BiLSTMs	Method
.	O

Results	O
on	O
a	O
range	O
of	O
classification	Task
and	Task
sequence	Task
labelling	Task
tasks	Task
show	O
that	O
S	Method
-	Method
LSTM	Method
outperforms	O
BiLSTMs	Method
using	O
the	O
same	O
number	O
of	O
parameters	O
,	O
demonstrating	O
that	O
S	Method
-	Method
LSTM	Method
can	O
be	O
a	O
useful	O
addition	O
to	O
the	O
neural	Method
toolbox	Method
for	O
encoding	Task
sentences	Task
.	O

The	O
structural	O
nature	O
in	O
S	O
-	O
LSTM	Method
states	O
allows	O
straightforward	O
extension	O
to	O
tree	O
structures	O
,	O
resulting	O
in	O
highly	O
parallelisable	Method
tree	Method
LSTMs	Method
.	O

We	O
leave	O
such	O
investigation	O
to	O
future	O
work	O
.	O

Next	O
directions	O
also	O
include	O
the	O
investigation	O
of	O
S	Method
-	Method
LSTM	Method
to	O
more	O
NLP	Task
tasks	Task
,	O
such	O
as	O
machine	Task
translation	Task
.	O

section	O
:	O
section	O
:	O
Acknowledge	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
constructive	O
and	O
thoughtful	O
comments	O
.	O

section	O
:	O
