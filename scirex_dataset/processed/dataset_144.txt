document	O
:	O
Ask	O
,	O
Attend	O
and	O
Answer	O
:	O
Exploring	O
Question	Task
-	Task
Guided	Task
Spatial	Task
Attention	Task
for	O
Visual	Task
Question	Task
Answering	Task
We	O
address	O
the	O
problem	O
of	O
Visual	Task
Question	Task
Answering	Task
(	O
VQA	Task
)	O
,	O
which	O
requires	O
joint	Task
image	Task
and	Task
language	Task
understanding	Task
to	O
answer	O
a	O
question	O
about	O
a	O
given	O
photograph	O
.	O

Recent	O
approaches	O
have	O
applied	O
deep	Method
image	Method
captioning	Method
methods	Method
based	O
on	O
convolutional	Method
-	Method
recurrent	Method
networks	Method
to	O
this	O
problem	O
,	O
but	O
have	O
failed	O
to	O
model	O
spatial	Task
inference	Task
.	O

To	O
remedy	O
this	O
,	O
we	O
propose	O
a	O
model	O
we	O
call	O
the	O
Spatial	Method
Memory	Method
Network	Method
and	O
apply	O
it	O
to	O
the	O
VQA	Task
task	Task
.	O

Memory	Method
networks	Method
are	O
recurrent	Method
neural	Method
networks	Method
with	O
an	O
explicit	Method
attention	Method
mechanism	Method
that	O
selects	O
certain	O
parts	O
of	O
the	O
information	O
stored	O
in	O
memory	O
.	O

Our	O
Spatial	Method
Memory	Method
Network	Method
stores	O
neuron	O
activations	O
from	O
different	O
spatial	O
regions	O
of	O
the	O
image	O
in	O
its	O
memory	O
,	O
and	O
uses	O
the	O
question	O
to	O
choose	O
relevant	O
regions	O
for	O
computing	O
the	O
answer	O
,	O
a	O
process	O
of	O
which	O
constitutes	O
a	O
single	O
“	O
hop	O
”	O
in	O
the	O
network	O
.	O

We	O
propose	O
a	O
novel	O
spatial	Method
attention	Method
architecture	Method
that	O
aligns	O
words	O
with	O
image	O
patches	O
in	O
the	O
first	O
hop	O
,	O
and	O
obtain	O
improved	O
results	O
by	O
adding	O
a	O
second	O
attention	Method
hop	Method
which	O
considers	O
the	O
whole	O
question	O
to	O
choose	O
visual	O
evidence	O
based	O
on	O
the	O
results	O
of	O
the	O
first	O
hop	O
.	O

To	O
better	O
understand	O
the	O
inference	Task
process	Task
learned	O
by	O
the	O
network	O
,	O
we	O
design	O
synthetic	O
questions	O
that	O
specifically	O
require	O
spatial	Task
inference	Task
and	O
visualize	O
the	O
attention	O
weights	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
published	O
visual	O
question	O
answering	O
datasets	O
,	O
DAQUAR	Material
and	O
VQA	Task
,	O
and	O
obtain	O
improved	O
results	O
compared	O
to	O
a	O
strong	O
deep	Method
baseline	Method
model	Method
(	O
iBOWIMG	Method
)	O
which	O
concatenates	O
image	O
and	O
question	O
features	O
to	O
predict	O
the	O
answer	O
.	O

section	O
:	O
Introduction	O
Visual	Task
Question	Task
Answering	Task
(	O
VQA	Task
)	O
is	O
an	O
emerging	O
interdisciplinary	Task
research	Task
problem	Task
at	O
the	O
intersection	O
of	O
computer	Task
vision	Task
,	O
natural	Task
language	Task
processing	Task
and	O
artificial	Task
intelligence	Task
.	O

It	O
has	O
many	O
real	Task
-	Task
life	Task
applications	Task
,	O
such	O
as	O
automatic	Task
querying	Task
of	Task
surveillance	Task
video	Task
or	O
assisting	O
the	O
visually	Task
impaired	Task
.	O

Compared	O
to	O
the	O
recently	O
popular	O
image	Task
captioning	Task
task	Task
,	O
VQA	Task
requires	O
a	O
deeper	O
understanding	O
of	O
the	O
image	O
,	O
but	O
is	O
considerably	O
easier	O
to	O
evaluate	O
.	O

It	O
also	O
puts	O
more	O
focus	O
on	O
artificial	Task
intelligence	Task
,	O
namely	O
the	O
inference	Method
process	Method
needed	O
to	O
produce	O
the	O
answer	O
to	O
the	O
visual	O
question	O
.	O

In	O
one	O
of	O
the	O
early	O
works	O
,	O
VQA	Task
is	O
seen	O
as	O
a	O
Turing	Method
test	Method
proxy	Method
.	O

The	O
authors	O
propose	O
an	O
approach	O
based	O
on	O
handcrafted	O
features	O
using	O
a	O
semantic	Method
parse	Method
of	Method
the	Method
question	Method
and	Method
scene	Method
analysis	Method
of	O
the	O
image	O
combined	O
in	O
a	O
latent	Method
-	Method
world	Method
Bayesian	Method
framework	Method
.	O

More	O
recently	O
,	O
several	O
end	O
-	O
to	O
-	O
end	Method
deep	Method
neural	Method
networks	Method
that	O
learn	O
features	O
directly	O
from	O
data	O
have	O
been	O
applied	O
to	O
this	O
problem	O
.	O

Most	O
of	O
these	O
are	O
directly	O
adapted	O
from	O
captioning	Method
models	Method
,	O
and	O
utilize	O
a	O
recurrent	Method
LSTM	Method
network	Method
,	O
which	O
takes	O
the	O
question	O
and	O
Convolutional	Method
Neural	Method
Net	Method
(	O
CNN	Method
)	O
image	O
features	O
as	O
input	O
,	O
and	O
outputs	O
the	O
answer	O
.	O

Though	O
the	O
deep	Method
learning	Method
methods	Method
in	O
have	O
shown	O
great	O
improvement	O
compared	O
to	O
the	O
handcrafted	Method
feature	Method
method	Method
,	O
they	O
have	O
their	O
own	O
drawbacks	O
.	O

These	O
models	O
based	O
on	O
the	O
LSTM	Method
reading	Method
in	O
both	O
the	O
question	O
and	O
the	O
image	O
features	O
do	O
not	O
show	O
a	O
clear	O
improvement	O
compared	O
to	O
an	O
LSTM	Method
reading	Method
in	O
the	O
question	O
only	O
.	O

Furthermore	O
,	O
the	O
rather	O
complicated	O
LSTM	Method
models	Method
obtain	O
similar	O
or	O
worse	O
accuracy	Metric
to	O
a	O
baseline	O
model	O
which	O
concatenates	O
CNN	Method
features	O
and	O
a	O
bag	Method
-	Method
of	Method
-	Method
words	Method
question	O
embedding	O
to	O
predict	O
the	O
answer	O
,	O
see	O
the	O
IMG	Method
+	O
BOW	Method
model	Method
in	O
and	O
the	O
iBOWIMG	Method
model	Method
in	O
.	O

A	O
major	O
drawback	O
of	O
existing	O
models	O
is	O
that	O
they	O
do	O
not	O
have	O
any	O
explicit	O
notion	O
of	O
object	O
position	O
,	O
and	O
do	O
not	O
support	O
the	O
computation	Task
of	Task
intermediate	Task
results	Task
based	O
on	O
spatial	O
attention	O
.	O

Our	O
intuition	O
is	O
that	O
answering	O
visual	Task
questions	Task
often	O
involves	O
looking	O
at	O
different	O
spatial	O
regions	O
and	O
comparing	O
their	O
contents	O
and	O
/	O
or	O
locations	O
.	O

For	O
example	O
,	O
to	O
answer	O
the	O
questions	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
need	O
to	O
look	O
at	O
a	O
portion	O
of	O
the	O
image	O
,	O
such	O
as	O
the	O
child	O
or	O
the	O
phone	O
booth	O
.	O

Similarly	O
,	O
to	O
answer	O
the	O
question	O
“	O
Is	O
there	O
a	O
cat	O
in	O
the	O
basket	O
?	O
”	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
we	O
can	O
first	O
find	O
the	O
basket	O
and	O
the	O
cat	O
objects	O
,	O
and	O
then	O
compare	O
their	O
locations	O
.	O

We	O
propose	O
a	O
new	O
deep	Method
learning	Method
approach	Method
to	O
VQA	Task
that	O
incorporates	O
explicit	Task
spatial	Task
attention	Task
,	O
which	O
we	O
call	O
the	O
Spatial	Method
Memory	Method
Network	Method
VQA	Method
(	O
SMem	Method
-	Method
VQA	Method
)	O
.	O

Our	O
approach	O
is	O
based	O
on	O
memory	Method
networks	Method
,	O
which	O
have	O
recently	O
been	O
proposed	O
for	O
text	O
Question	Task
Answering	Task
(	O
QA	Task
)	O
.	O

Memory	Method
networks	Method
combine	O
learned	O
text	Method
embeddings	Method
with	O
an	O
attention	Method
mechanism	Method
and	O
multi	Method
-	Method
step	Method
inference	Method
.	O

The	O
text	O
QA	Task
memory	O
network	O
stores	O
textual	O
knowledge	O
in	O
its	O
“	O
memory	O
”	O
in	O
the	O
form	O
of	O
sentences	O
,	O
and	O
selects	O
relevant	O
sentences	O
to	O
infer	O
the	O
answer	O
.	O

However	O
,	O
in	O
VQA	Task
,	O
the	O
knowledge	O
is	O
in	O
the	O
form	O
of	O
an	O
image	O
,	O
thus	O
the	O
memory	O
and	O
the	O
question	O
come	O
from	O
different	O
modalities	O
.	O

We	O
adapt	O
the	O
end	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
network	Method
to	O
solve	O
visual	Task
question	Task
answering	Task
by	O
storing	O
the	O
convolutional	Method
network	Method
outputs	O
obtained	O
from	O
different	O
receptive	O
fields	O
into	O
the	O
memory	O
,	O
which	O
explicitly	O
allows	O
spatial	O
attention	O
over	O
the	O
image	O
.	O

We	O
also	O
propose	O
to	O
repeat	O
the	O
process	O
of	O
gathering	O
evidence	O
from	O
attended	O
regions	O
,	O
enabling	O
the	O
model	O
to	O
update	O
the	O
answer	O
based	O
on	O
several	O
attention	O
steps	O
,	O
or	O
“	O
hops	O
”	O
.	O

The	O
entire	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
and	O
the	O
evidence	O
for	O
the	O
computed	O
answer	O
can	O
be	O
visualized	O
using	O
the	O
attention	O
weights	O
.	O

To	O
summarize	O
our	O
contributions	O
,	O
in	O
this	O
paper	O
we	O
propose	O
a	O
novel	O
multi	Method
-	Method
hop	Method
memory	Method
network	Method
with	O
spatial	Method
attention	Method
for	O
the	O
VQA	Task
task	Task
which	O
allows	O
one	O
to	O
visualize	O
the	O
spatial	Method
inference	Method
process	Method
used	O
by	O
the	O
deep	Method
network	Method
(	O
a	O
CAFFE	Method
implementation	Method
will	O
be	O
made	O
available	O
)	O
,	O
design	O
an	O
attention	Method
architecture	Method
in	O
the	O
first	O
hop	O
which	O
uses	O
each	O
word	O
embedding	O
to	O
capture	O
fine	O
-	O
grained	O
alignment	O
between	O
the	O
image	O
and	O
question	O
,	O
create	O
a	O
series	O
of	O
synthetic	Material
questions	Material
that	O
explicitly	O
require	O
spatial	Task
inference	Task
to	O
analyze	O
the	O
working	O
principles	O
of	O
the	O
network	O
,	O
and	O
show	O
that	O
it	O
learns	O
logical	O
inference	O
rules	O
by	O
visualizing	O
the	O
attention	O
weights	O
,	O
provide	O
an	O
extensive	O
evaluation	O
of	O
several	O
existing	O
models	O
and	O
our	O
own	O
model	O
on	O
the	O
same	O
publicly	O
available	O
datasets	O
.	O

Sec	O
.	O

2	O
introduces	O
relevant	O
work	O
on	O
memory	Method
networks	Method
and	O
attention	Method
models	Method
.	O

Sec	O
.	O

3	O
describes	O
our	O
design	O
of	O
the	O
multi	Method
-	Method
hop	Method
memory	Method
network	Method
architecture	Method
for	O
visual	Task
question	Task
answering	Task
(	O
SMem	Method
-	Method
VQA	Method
)	O
.	O

Sec	O
.	O

4	O
visualizes	O
the	O
inference	O
rules	O
learned	O
by	O
the	O
network	O
for	O
synthetic	Task
spatial	Task
questions	Task
and	O
shows	O
the	O
experimental	O
results	O
on	O
DAQUAR	Material
and	O
VQA	Material
datasets	Material
.	O

Sec	O
.	O

5	O
concludes	O
the	O
paper	O
.	O

section	O
:	O
Related	O
work	O
Before	O
the	O
popularity	O
of	O
visual	Task
question	Task
answering	Task
(	O
VQA	Task
)	O
,	O
text	Task
question	Task
answering	Task
(	O
QA	Task
)	O
had	O
already	O
been	O
established	O
as	O
a	O
mature	O
research	O
problem	O
in	O
the	O
area	O
of	O
natural	Task
language	Task
processing	Task
.	O

Previous	O
QA	Task
methods	O
include	O
searching	O
for	O
the	O
key	O
words	O
of	O
the	O
question	O
in	O
a	O
search	Method
engine	Method
;	O
parsing	O
the	O
question	O
as	O
a	O
knowledge	Method
base	Method
(	O
KB	Method
)	O
query	O
;	O
or	O
embedding	O
the	O
question	O
and	O
using	O
a	O
similarity	Method
measurement	Method
to	O
find	O
evidence	O
for	O
the	O
answer	O
.	O

Recently	O
,	O
memory	Method
networks	Method
were	O
proposed	O
for	O
solving	O
the	O
QA	Task
problem	O
.	O

first	O
introduces	O
the	O
memory	Method
network	Method
as	O
a	O
general	O
model	O
that	O
consists	O
of	O
a	O
memory	O
and	O
four	O
components	O
:	O
input	O
feature	O
map	O
,	O
generalization	Method
,	O
output	O
feature	O
map	O
and	O
response	O
.	O

The	O
model	O
is	O
investigated	O
in	O
the	O
context	O
of	O
question	Task
answering	Task
,	O
where	O
the	O
long	O
-	O
term	O
memory	O
acts	O
as	O
a	O
dynamic	O
knowledge	Method
base	Method
and	O
the	O
output	O
is	O
a	O
textual	O
response	O
.	O

proposes	O
a	O
competitive	Method
memory	Method
network	Method
model	Method
that	O
uses	O
less	O
supervision	O
,	O
called	O
end	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
network	Method
,	O
which	O
has	O
a	O
recurrent	Method
attention	Method
model	Method
over	O
a	O
large	O
external	O
memory	O
.	O

The	O
Neural	Method
Turing	Method
Machine	Method
(	O
NTM	Method
)	O
couples	O
a	O
neural	Method
network	Method
to	O
external	O
memory	O
and	O
interacts	O
with	O
it	O
by	O
attentional	Method
processes	Method
to	O
infer	O
simple	O
algorithms	O
such	O
as	O
copying	Task
,	O
sorting	Task
,	O
and	O
associative	Task
recall	Task
from	O
input	O
and	O
output	O
examples	O
.	O

In	O
this	O
paper	O
,	O
we	O
solve	O
the	O
VQA	Task
problem	Task
using	O
a	O
multimodal	Method
memory	Method
network	Method
architecture	Method
that	O
applies	O
a	O
spatial	Method
attention	Method
mechanism	Method
over	O
an	O
input	O
image	O
guided	O
by	O
an	O
input	O
text	O
question	O
.	O

The	O
neural	Method
attention	Method
mechanism	Method
has	O
been	O
widely	O
used	O
in	O
different	O
areas	O
of	O
computer	Task
vision	Task
and	O
natural	Task
language	Task
processing	Task
,	O
see	O
for	O
example	O
the	O
attention	Method
models	Method
in	O
image	Task
captioning	Task
,	O
video	Task
description	Task
generation	Task
,	O
machine	Task
translation	Task
and	O
machine	Task
reading	Task
systems	Task
.	O

Most	O
methods	O
use	O
the	O
soft	Method
attention	Method
mechanism	Method
first	O
proposed	O
in	O
,	O
which	O
adds	O
a	O
layer	O
to	O
the	O
network	O
that	O
predicts	O
soft	O
weights	O
and	O
uses	O
them	O
to	O
compute	O
a	O
weighted	O
combination	O
of	O
the	O
items	O
in	O
memory	O
.	O

The	O
two	O
main	O
types	O
of	O
soft	Method
attention	Method
mechanisms	Method
differ	O
in	O
the	O
function	O
that	O
aligns	O
the	O
input	O
feature	O
vector	O
and	O
the	O
candidate	O
feature	O
vectors	O
in	O
order	O
to	O
compute	O
the	O
soft	O
attention	O
weights	O
.	O

The	O
first	O
type	O
uses	O
an	O
alignment	O
function	O
based	O
on	O
“	O
concatenation	O
”	O
of	O
the	O
input	O
and	O
each	O
candidate	O
(	O
we	O
use	O
the	O
term	O
“	O
concatenation	O
”	O
as	O
described	O
)	O
,	O
and	O
the	O
second	O
type	O
uses	O
an	O
alignment	Method
function	Method
based	O
on	O
the	O
dot	O
product	O
of	O
the	O
input	O
and	O
each	O
candidate	O
.	O

The	O
“	O
concatenation	Method
”	Method
alignment	Method
function	Method
adds	O
one	O
input	O
vector	O
(	O
e.g.	O
hidden	O
state	O
vector	O
of	O
the	O
LSTM	Method
)	O
to	O
each	O
candidate	O
feature	O
vector	O
,	O
embeds	O
the	O
resulting	O
vectors	O
into	O
scalar	O
values	O
,	O
and	O
then	O
applies	O
the	O
softmax	Method
function	Method
to	O
generate	O
the	O
attention	O
weight	O
for	O
each	O
candidate	O
.	O

use	O
the	O
“	O
concatenation	Method
”	Method
alignment	Method
function	Method
in	O
their	O
soft	Method
attention	Method
models	Method
and	O
gives	O
a	O
literature	O
review	O
of	O
such	O
models	O
applied	O
to	O
different	O
tasks	O
.	O

On	O
the	O
other	O
hand	O
,	O
the	O
dot	Method
product	Method
alignment	Method
function	Method
first	O
projects	O
both	O
inputs	O
to	O
a	O
common	O
vector	O
embedding	O
space	O
,	O
then	O
takes	O
the	O
dot	O
product	O
of	O
the	O
two	O
input	O
vectors	O
,	O
and	O
applies	O
a	O
softmax	Method
function	Method
to	O
the	O
resulting	O
scalar	O
value	O
to	O
produce	O
the	O
attention	O
weight	O
for	O
each	O
candidate	O
.	O

The	O
end	O
-	O
to	O
-	O
end	Method
memory	Method
network	Method
uses	O
the	O
dot	Method
product	Method
alignment	Method
function	Method
.	O

In	O
,	O
the	O
authors	O
compare	O
these	O
two	O
alignment	Method
functions	Method
in	O
an	O
attention	Method
model	Method
for	O
the	O
neural	Task
machine	Task
translation	Task
task	Task
,	O
and	O
find	O
that	O
their	O
implementation	O
of	O
the	O
“	O
concatenation	Method
”	Method
alignment	Method
function	Method
does	O
not	O
yield	O
good	O
performance	O
on	O
their	O
task	O
.	O

Motivated	O
by	O
this	O
,	O
in	O
this	O
paper	O
we	O
use	O
the	O
dot	Method
product	Method
alignment	Method
function	Method
in	O
our	O
Spatial	Method
Memory	Method
Network	Method
.	O

VQA	Task
is	O
related	O
to	O
image	Task
captioning	Task
.	O

Several	O
early	O
papers	O
about	O
VQA	Task
directly	O
adapt	O
the	O
image	Method
captioning	Method
models	Method
to	O
solve	O
the	O
VQA	Task
problem	Task
by	O
generating	O
the	O
answer	O
using	O
a	O
recurrent	Method
LSTM	Method
network	Method
conditioned	O
on	O
the	O
CNN	Method
output	O
.	O

But	O
these	O
models	O
’	O
performance	O
is	O
still	O
limited	O
.	O

proposes	O
a	O
new	O
dataset	O
and	O
uses	O
a	O
similar	O
attention	Method
model	Method
to	O
that	O
in	O
image	Task
captioning	Task
,	O
but	O
does	O
not	O
give	O
results	O
on	O
the	O
more	O
common	O
VQA	Material
benchmark	Material
,	O
and	O
our	O
own	O
implementation	O
of	O
this	O
model	O
is	O
less	O
accurate	O
on	O
than	O
other	O
baseline	O
models	O
.	O

summarizes	O
several	O
recent	O
papers	O
reporting	O
results	O
on	O
the	O
VQA	Material
dataset	Material
on	O
arxiv.org	O
and	O
gives	O
a	O
simple	O
but	O
strong	O
baseline	O
model	O
(	O
iBOWIMG	Method
)	O
on	O
this	O
dataset	O
.	O

This	O
simple	O
baseline	O
concatenates	O
the	O
image	O
features	O
with	O
the	O
bag	Method
of	Method
word	Method
embedding	Method
question	Method
representation	Method
and	O
feeds	O
them	O
into	O
a	O
softmax	Method
classifier	Method
to	O
predict	O
the	O
answer	O
.	O

The	O
iBOWIMG	Method
model	Method
beats	O
most	O
VQA	Task
models	O
considered	O
in	O
the	O
paper	O
.	O

Here	O
,	O
we	O
compare	O
our	O
proposed	O
model	O
to	O
the	O
VQA	Task
models	O
(	O
namely	O
,	O
the	O
ACK	Method
model	Method
and	O
the	O
DPPnet	Method
model	Method
)	O
which	O
have	O
comparable	O
or	O
better	O
results	O
than	O
the	O
iBOWIMG	Method
model	Method
.	O

The	O
ACK	Method
model	Method
in	O
is	O
essentially	O
the	O
same	O
as	O
the	O
LSTM	Method
model	Method
in	O
,	O
except	O
that	O
it	O
uses	O
image	O
attribute	O
features	O
,	O
the	O
generated	O
image	O
caption	O
and	O
relevant	O
external	O
knowledge	O
from	O
a	O
knowledge	Method
base	Method
as	O
the	O
input	O
to	O
the	O
LSTM	Method
’s	O
first	O
time	O
step	O
.	O

The	O
DPPnet	Method
model	Method
in	O
tackles	O
VQA	Task
by	O
learning	O
a	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
with	O
some	O
parameters	O
predicted	O
from	O
a	O
separate	O
parameter	Method
prediction	Method
network	Method
.	O

Their	O
parameter	Method
prediction	Method
network	Method
uses	O
a	O
Gate	Method
Recurrent	Method
Unit	Method
(	O
GRU	Method
)	O
to	O
generate	O
a	O
question	Method
representation	Method
,	O
and	O
based	O
on	O
this	O
question	O
input	O
,	O
maps	O
the	O
predicted	O
weights	O
to	O
CNN	Method
via	O
hashing	Method
.	O

Neither	O
of	O
these	O
models	O
contain	O
a	O
spatial	Method
attention	Method
mechanism	Method
,	O
and	O
they	O
both	O
use	O
external	O
data	O
in	O
addition	O
to	O
the	O
VQA	Material
dataset	Material
,	O
e.g.	O
the	O
knowledge	Method
base	Method
in	O
and	O
the	O
large	O
-	O
scale	O
text	O
corpus	O
used	O
to	O
pre	O
-	O
train	O
the	O
GRU	Method
question	O
representation	O
.	O

In	O
this	O
paper	O
,	O
we	O
explore	O
a	O
complementary	O
approach	O
of	O
spatial	Task
attention	Task
to	O
both	O
improve	O
performance	O
and	O
visualize	O
the	O
network	Task
’s	Task
inference	Task
process	Task
,	O
and	O
obtain	O
improved	O
results	O
without	O
using	O
external	O
data	O
compared	O
to	O
the	O
iBOWIMG	Method
model	Method
as	O
well	O
as	O
the	O
ACK	Method
model	Method
and	O
the	O
DPPnet	Method
model	Method
which	O
use	O
external	O
data	O
.	O

section	O
:	O
Spatial	Method
Memory	Method
Network	Method
for	O
VQA	Task
We	O
first	O
give	O
an	O
overview	O
of	O
the	O
proposed	O
SMem	O
-	O
VQA	Task
network	O
,	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
.	O

Sec	O
.	O

[	O
reference	O
]	O
details	O
the	O
word	Task
-	Task
guided	Task
spatial	Task
attention	Task
process	Task
of	O
the	O
first	O
hop	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
,	O
and	O
Sec	O
.	O

[	O
reference	O
]	O
describes	O
adding	O
a	O
second	O
hop	O
into	O
SMem	O
-	O
VQA	Task
network	O
.	O

The	O
input	O
to	O
our	O
network	O
is	O
a	O
question	O
comprised	O
of	O
a	O
variable	O
-	O
length	O
sequence	O
of	O
words	O
,	O
and	O
an	O
image	O
of	O
fixed	O
size	O
.	O

Each	O
word	O
in	O
the	O
question	O
is	O
first	O
represented	O
as	O
a	O
one	O
-	O
hot	O
vector	O
in	O
the	O
size	O
of	O
the	O
vocabulary	O
,	O
with	O
a	O
value	O
of	O
one	O
only	O
in	O
the	O
corresponding	O
word	O
position	O
and	O
zeros	O
in	O
the	O
other	O
positions	O
.	O

Each	O
one	O
-	O
hot	O
vector	O
is	O
then	O
embedded	O
into	O
a	O
real	O
-	O
valued	O
word	O
vector	O
,	O
,	O
where	O
is	O
the	O
maximum	O
number	O
of	O
words	O
in	O
the	O
question	O
and	O
is	O
the	O
dimensionality	O
of	O
the	O
embedding	O
space	O
.	O

Sentences	O
with	O
length	O
less	O
than	O
are	O
padded	O
with	O
special	O
value	O
,	O
which	O
are	O
embedded	O
to	O
all	O
-	O
zero	O
word	O
vector	O
.	O

The	O
words	O
in	O
questions	O
are	O
used	O
to	O
compute	O
attention	O
over	O
the	O
visual	O
memory	O
,	O
which	O
contains	O
extracted	O
image	O
features	O
.	O

The	O
input	O
image	O
is	O
processed	O
by	O
a	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
to	O
extract	O
high	O
-	O
level	O
-	O
dimensional	O
visual	O
features	O
on	O
a	O
grid	O
of	O
spatial	O
locations	O
.	O

Specifically	O
,	O
we	O
use	O
to	O
represent	O
the	O
spatial	O
CNN	Method
features	O
at	O
each	O
of	O
the	O
grid	O
locations	O
.	O

In	O
this	O
paper	O
,	O
the	O
spatial	O
feature	O
outputs	O
of	O
the	O
last	O
convolutional	Method
layer	Method
of	O
GoogLeNet	Method
(	Method
)	Method
are	O
used	O
as	O
the	O
visual	O
features	O
for	O
the	O
image	O
.	O

The	O
convolutional	O
image	O
feature	O
vectors	O
at	O
each	O
location	O
are	O
embedded	O
into	O
a	O
common	O
semantic	O
space	O
with	O
the	O
word	O
vectors	O
.	O

Two	O
different	O
embeddings	O
are	O
used	O
:	O
the	O
“	O
attention	Method
”	Method
embedding	Method
and	O
the	O
“	O
evidence	Method
”	Method
embedding	Method
.	O

The	O
attention	Method
embedding	Method
projects	O
each	O
visual	O
feature	O
vector	O
such	O
that	O
its	O
combination	O
with	O
the	O
embedded	O
question	O
words	O
generates	O
the	O
attention	O
weight	O
at	O
that	O
location	O
.	O

The	O
evidence	Method
embedding	Method
detects	O
the	O
presence	O
of	O
semantic	O
concepts	O
or	O
objects	O
,	O
and	O
the	O
embedding	O
results	O
are	O
multiplied	O
with	O
attention	O
weights	O
and	O
summed	O
over	O
all	O
locations	O
to	O
generate	O
the	O
visual	O
evidence	O
vector	O
.	O

Finally	O
,	O
the	O
visual	O
evidence	O
vector	O
is	O
combined	O
with	O
the	O
question	Method
representation	Method
and	O
used	O
to	O
predict	O
the	O
answer	O
for	O
the	O
given	O
image	O
and	O
question	O
.	O

In	O
the	O
next	O
section	O
,	O
we	O
describe	O
the	O
one	Method
-	Method
hop	Method
Spatial	Method
Memory	Method
network	Method
model	Method
and	O
the	O
specific	O
attention	Method
mechanism	Method
it	O
uses	O
in	O
more	O
detail	O
.	O

subsection	O
:	O
Word	Task
Guided	Task
Spatial	Task
Attention	Task
in	O
One	Method
-	Method
Hop	Method
Model	Method
Rather	O
than	O
using	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
question	O
representation	O
to	O
guide	O
attention	Task
,	O
the	O
attention	Method
architecture	Method
in	O
the	O
first	O
hop	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
)	O
uses	O
each	O
word	O
vector	O
separately	O
to	O
extract	O
correlated	O
visual	O
features	O
in	O
memory	O
.	O

The	O
intuition	O
is	O
that	O
the	O
BOW	Method
representation	O
may	O
be	O
too	O
coarse	O
,	O
and	O
letting	O
each	O
word	O
select	O
a	O
related	O
region	O
may	O
provide	O
more	O
fine	O
-	O
grained	O
attention	O
.	O

The	O
correlation	O
matrix	O
between	O
word	O
vectors	O
and	O
visual	O
features	O
is	O
computed	O
as	O
where	O
contains	O
the	O
attention	O
embedding	O
weights	O
of	O
visual	O
features	O
,	O
and	O
is	O
the	O
bias	O
term	O
.	O

This	O
correlation	O
matrix	O
is	O
the	O
dot	O
product	O
result	O
of	O
each	O
word	O
embedding	O
and	O
each	O
spatial	O
location	O
’s	O
visual	O
feature	O
,	O
thus	O
each	O
value	O
in	O
correlation	Metric
matrix	Metric
measures	O
the	O
similarity	O
between	O
each	O
word	O
and	O
each	O
location	O
’s	O
visual	O
feature	O
.	O

The	O
spatial	O
attention	O
weights	O
are	O
calculated	O
by	O
taking	O
maximum	O
over	O
the	O
word	O
dimension	O
for	O
the	O
correlation	O
matrix	O
,	O
selecting	O
the	O
highest	O
correlation	O
value	O
for	O
each	O
spatial	O
location	O
,	O
and	O
then	O
applying	O
the	O
softmax	Method
function	Method
The	O
resulting	O
attention	O
weights	O
are	O
high	O
for	O
selected	O
locations	O
and	O
low	O
for	O
other	O
locations	O
,	O
with	O
the	O
sum	O
of	O
weights	O
equal	O
to	O
.	O

For	O
instance	O
,	O
in	O
the	O
example	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
the	O
question	O
“	O
Is	O
there	O
a	O
cat	O
in	O
the	O
basket	O
?	O
”	O
produces	O
high	O
attention	O
weights	O
for	O
the	O
location	O
of	O
the	O
basket	O
because	O
of	O
the	O
high	O
correlation	O
of	O
the	O
word	O
vector	O
for	O
basket	O
with	O
the	O
visual	O
features	O
at	O
that	O
location	O
.	O

The	O
evidence	Method
embedding	Method
projects	O
visual	O
features	O
to	O
produce	O
high	O
activations	O
for	O
certain	O
semantic	O
concepts	O
.	O

E.g.	O
,	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
it	O
has	O
high	O
activations	O
in	O
the	O
region	O
containing	O
the	O
cat	O
.	O

The	O
results	O
of	O
this	O
evidence	Method
embedding	Method
are	O
then	O
multiplied	O
by	O
the	O
generated	O
attention	O
weights	O
,	O
and	O
summed	O
to	O
produce	O
the	O
selected	O
visual	O
“	O
evidence	O
”	O
vector	O
,	O
where	O
are	O
the	O
evidence	O
embedding	O
weights	O
of	O
the	O
visual	O
features	O
,	O
and	O
is	O
the	O
bias	O
term	O
.	O

In	O
our	O
running	O
example	O
,	O
this	O
step	O
accumulates	O
cat	O
presence	O
features	O
at	O
the	O
basket	O
location	O
.	O

Finally	O
,	O
the	O
sum	O
of	O
this	O
evidence	O
vector	O
and	O
the	O
question	Method
embedding	Method
is	O
used	O
to	O
predict	O
the	O
answer	O
for	O
the	O
given	O
image	O
and	O
question	O
.	O

For	O
the	O
question	Task
representation	Task
,	O
we	O
choose	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
(	O
BOW	Method
)	O
.	O

Other	O
question	Method
representations	Method
,	O
such	O
as	O
an	O
LSTM	Method
,	O
can	O
also	O
be	O
used	O
,	O
however	O
,	O
BOW	Method
has	O
fewer	O
parameters	O
yet	O
has	O
shown	O
good	O
performance	O
.	O

As	O
noted	O
in	O
,	O
the	O
simple	O
BOW	Method
model	Method
performs	O
roughly	O
as	O
well	O
if	O
not	O
better	O
than	O
the	O
sequence	Method
-	Method
based	Method
LSTM	Method
for	O
the	O
VQA	Task
task	Task
.	O

Specifically	O
,	O
we	O
compute	O
where	O
represents	O
the	O
BOW	Method
weights	O
for	O
word	O
vectors	O
,	O
and	O
is	O
the	O
bias	O
term	O
.	O

The	O
final	O
prediction	Task
is	O
where	O
,	O
bias	O
term	O
,	O
and	O
represents	O
the	O
number	O
of	O
possible	O
prediction	O
answers	O
.	O

is	O
the	O
activation	O
function	O
,	O
and	O
we	O
use	O
ReLU	Method
here	O
.	O

In	O
our	O
running	O
example	O
,	O
this	O
step	O
adds	O
the	O
evidence	O
gathered	O
for	O
cat	O
near	O
the	O
basket	O
location	O
to	O
the	O
question	O
,	O
and	O
,	O
since	O
the	O
cat	O
was	O
not	O
found	O
,	O
predicts	O
the	O
answer	O
“	O
no	O
”	O
.	O

The	O
attention	Method
and	Method
evidence	Method
computation	Method
steps	Method
can	O
be	O
optionally	O
repeated	O
in	O
another	O
hop	O
,	O
before	O
predicting	O
the	O
final	O
answer	O
,	O
as	O
detailed	O
in	O
the	O
next	O
section	O
.	O

subsection	O
:	O
Spatial	Task
Attention	Task
in	O
Two	Method
-	Method
Hop	Method
Model	Method
We	O
can	O
repeat	O
hops	O
to	O
promote	O
deeper	Task
inference	Task
,	O
gathering	O
additional	O
evidence	O
at	O
each	O
hop	O
.	O

Recall	O
that	O
the	O
visual	O
evidence	O
vector	O
is	O
added	O
to	O
the	O
question	Method
representation	Method
in	O
the	O
first	O
hop	O
to	O
produce	O
an	O
updated	O
question	O
vector	O
,	O
On	O
the	O
next	O
hop	O
,	O
this	O
vector	O
is	O
used	O
in	O
place	O
of	O
the	O
individual	O
word	O
vectors	O
to	O
extract	O
additional	O
correlated	O
visual	O
features	O
to	O
the	O
whole	O
question	O
from	O
memory	O
and	O
update	O
the	O
visual	O
evidence	O
.	O

The	O
correlation	O
matrix	O
in	O
the	O
first	O
hop	O
provides	O
fine	O
-	O
grained	O
local	O
evidence	O
from	O
each	O
word	O
vectors	O
in	O
the	O
question	O
,	O
while	O
the	O
correlation	O
vector	O
in	O
next	O
hop	O
considers	O
the	O
global	O
evidence	O
from	O
the	O
whole	O
question	Method
representation	Method
.	O

The	O
correlation	O
vector	O
in	O
the	O
second	O
hop	O
is	O
calculated	O
by	O
where	O
should	O
be	O
the	O
attention	O
embedding	O
weights	O
of	O
visual	O
features	O
in	O
the	O
second	O
hop	O
and	O
should	O
be	O
the	O
bias	O
term	O
.	O

Since	O
the	O
attention	O
embedding	O
weights	O
in	O
the	O
second	O
hop	O
are	O
shared	O
with	O
the	O
evidence	O
embedding	O
in	O
the	O
first	O
hop	O
,	O
so	O
we	O
directly	O
use	O
and	O
from	O
first	O
hop	O
here	O
.	O

The	O
attention	O
weights	O
in	O
the	O
second	O
hop	O
are	O
obtained	O
by	O
applying	O
the	O
softmax	Method
function	Method
to	O
the	O
correlation	O
vector	O
.	O

Then	O
,	O
the	O
correlated	O
visual	O
information	O
in	O
the	O
second	O
hop	O
is	O
extracted	O
using	O
attention	O
weights	O
.	O

where	O
are	O
the	O
evidence	O
embedding	O
weights	O
of	O
visual	O
features	O
in	O
the	O
second	O
hop	O
,	O
and	O
is	O
the	O
bias	O
term	O
.	O

The	O
final	O
answer	O
is	O
predicted	O
by	O
combining	O
the	O
whole	Method
question	Method
representation	Method
,	O
the	O
local	O
visual	O
evidence	O
from	O
each	O
word	O
vector	O
in	O
the	O
first	O
hop	O
and	O
the	O
global	O
visual	O
evidence	O
from	O
the	O
whole	O
question	O
in	O
the	O
second	O
hop	O
,	O
where	O
,	O
bias	O
term	O
,	O
and	O
represents	O
the	O
number	O
of	O
possible	O
prediction	O
answers	O
.	O

is	O
activation	O
function	O
.	O

More	O
hops	O
can	O
be	O
added	O
in	O
this	O
manner	O
.	O

The	O
entire	O
network	O
is	O
differentiable	O
and	O
is	O
trained	O
using	O
stochastic	Method
gradient	Method
descent	Method
via	O
standard	O
backpropagation	Method
,	O
allowing	O
image	Task
feature	Task
extraction	Task
,	O
image	Task
embedding	Task
,	O
word	Task
embedding	Task
and	O
answer	Task
prediction	Task
to	O
be	O
jointly	O
optimized	O
on	O
the	O
training	O
image	O
/	O
question	O
/	O
answer	O
triples	O
.	O

section	O
:	O
Experiments	O
In	O
this	O
section	O
,	O
we	O
conduct	O
a	O
series	O
of	O
experiments	O
to	O
evaluate	O
our	O
model	O
.	O

To	O
explore	O
whether	O
the	O
model	O
learns	O
to	O
perform	O
the	O
spatial	Task
inference	Task
necessary	O
for	O
answering	Task
visual	Task
questions	Task
that	O
explicitly	O
require	O
spatial	O
reasoning	O
,	O
we	O
design	O
a	O
set	O
of	O
experiments	O
using	O
synthetic	O
visual	O
question	O
/	O
answer	O
data	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

The	O
experimental	O
results	O
of	O
our	O
model	O
in	O
standard	O
datasets	O
(	O
DAQUAR	Material
and	O
VQA	Material
datasets	Material
)	O
are	O
reported	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Exploring	O
Attention	Task
on	O
Synthetic	O
Data	O
The	O
questions	O
in	O
the	O
public	Material
VQA	Material
datasets	Material
are	O
quite	O
varied	O
and	O
difficult	O
and	O
often	O
require	O
common	O
sense	O
knowledge	O
to	O
answer	O
(	O
e.g.	O
,	O
“	O
Does	O
this	O
man	O
have	O
20	O
/	O
20	O
vision	O
?	O
”	O
about	O
a	O
person	O
wearing	O
glasses	O
)	O
.	O

Furthermore	O
,	O
past	O
work	O
showed	O
that	O
the	O
question	O
text	O
alone	O
(	O
no	O
image	O
)	O
is	O
a	O
very	O
strong	O
predictor	O
of	O
the	O
answer	O
.	O

Therefore	O
,	O
before	O
evaluating	O
on	O
standard	O
datasets	O
,	O
we	O
would	O
first	O
like	O
to	O
understand	O
how	O
the	O
proposed	O
model	O
uses	O
spatial	O
attention	O
to	O
answer	O
simple	O
visual	O
questions	O
where	O
the	O
answer	O
can	O
not	O
be	O
predicted	O
from	O
question	O
alone	O
.	O

Our	O
visualization	O
demonstrates	O
that	O
the	O
attention	Method
mechanism	Method
does	O
learn	O
to	O
attend	O
to	O
objects	O
and	O
gather	O
evidence	O
via	O
certain	O
inference	O
rules	O
.	O

subsubsection	O
:	O
Absolute	Task
Position	Task
Recognition	Task
We	O
investigate	O
whether	O
the	O
model	O
has	O
the	O
ability	O
to	O
recognize	O
the	O
absolute	O
location	O
of	O
the	O
object	O
in	O
the	O
image	O
.	O

We	O
explore	O
this	O
by	O
designing	O
a	O
simple	O
task	O
where	O
an	O
object	O
(	O
a	O
red	O
square	O
)	O
appears	O
in	O
some	O
region	O
of	O
a	O
white	O
-	O
background	O
image	O
,	O
and	O
the	O
question	O
is	O
“	O
Is	O
there	O
a	O
red	O
square	O
on	O
the	O
[	O
top	O
bottom	O
left	O
right	O
]	O
?	O
”	O
For	O
each	O
image	O
,	O
we	O
randomly	O
place	O
the	O
square	O
in	O
one	O
of	O
the	O
four	O
regions	O
,	O
and	O
generate	O
the	O
four	O
questions	O
above	O
,	O
together	O
with	O
three	O
“	O
no	O
”	O
answers	O
and	O
one	O
“	O
yes	O
”	O
answer	O
.	O

The	O
generated	O
data	O
is	O
split	O
into	O
training	O
and	O
testing	O
sets	O
.	O

Due	O
to	O
the	O
simplicity	O
of	O
this	O
synthetic	O
dataset	O
,	O
the	O
SMem	O
-	O
VQA	Task
one	O
-	O
hop	O
model	O
achieves	O
100	O
%	O
test	Metric
accuracy	Metric
.	O

However	O
,	O
the	O
baseline	Method
model	Method
(	O
iBOWIMG	Method
)	O
can	O
not	O
infer	O
the	O
answer	O
and	O
only	O
obtains	O
accuracy	Metric
of	O
around	O
75	O
%	O
,	O
which	O
is	O
the	O
prior	O
probability	O
of	O
the	O
answer	O
“	O
no	O
”	O
in	O
the	O
training	O
set	O
.	O

The	O
SMem	O
-	O
VQA	Task
one	O
-	O
hop	O
model	O
is	O
equivalent	O
to	O
the	O
iBOWIMG	Method
model	Method
if	O
the	O
attention	O
weights	O
in	O
our	O
one	Method
-	Method
hop	Method
model	Method
are	O
set	O
equally	O
for	O
each	O
location	O
,	O
since	O
the	O
iBOWIMG	Method
model	Method
uses	O
the	O
mean	O
pool	O
of	O
the	O
convolutional	O
feature	O
(	O
)	O
in	O
GoogLeNet	Method
that	O
we	O
use	O
in	O
SMem	O
-	O
VQA	Task
model	O
.	O

We	O
check	O
the	O
visualization	O
of	O
the	O
attention	O
weights	O
and	O
find	O
that	O
the	O
relationship	O
between	O
the	O
high	O
attention	O
position	O
and	O
the	O
answer	O
can	O
be	O
expressed	O
by	O
logical	O
expressions	O
.	O

We	O
show	O
the	O
attention	O
weights	O
of	O
several	O
typical	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
which	O
reflect	O
two	O
logic	O
rules	O
:	O
1	O
)	O
Look	O
at	O
the	O
position	O
specified	O
in	O
question	O
(	O
top	O
bottom	O
right	O
left	O
)	O
,	O
if	O
it	O
contains	O
a	O
square	O
,	O
then	O
answer	O
“	O
yes	O
”	O
;	O
if	O
it	O
does	O
not	O
contain	O
a	O
square	O
,	O
then	O
answer	O
“	O
no	O
”	O
.	O

2	O
)	O
Look	O
at	O
the	O
region	O
where	O
there	O
is	O
a	O
square	O
,	O
then	O
answer	O
“	O
yes	O
”	O
for	O
the	O
question	O
about	O
that	O
position	O
and	O
“	O
no	O
”	O
for	O
the	O
questions	O
about	O
the	O
other	O
three	O
positions	O
.	O

In	O
the	O
iBOWIMG	Method
model	Method
,	O
the	O
mean	Method
-	Method
pooled	Method
GoogLeNet	Method
visual	Method
features	Method
lose	O
spatial	O
information	O
and	O
thus	O
can	O
not	O
distinguish	O
images	O
with	O
a	O
square	O
in	O
different	O
positions	O
.	O

On	O
the	O
contrary	O
,	O
our	O
SMem	O
-	O
VQA	Task
model	O
can	O
pay	O
high	O
attention	O
to	O
different	O
regions	O
according	O
to	O
the	O
question	O
,	O
and	O
generate	O
an	O
answer	O
based	O
on	O
the	O
selected	O
region	O
,	O
using	O
some	O
learned	O
inference	Method
rules	Method
.	O

This	O
experiment	O
demonstrates	O
that	O
the	O
attention	Method
mechanism	Method
in	O
our	O
model	O
is	O
able	O
to	O
make	O
absolute	Task
spatial	Task
location	Task
inference	Task
based	O
on	O
the	O
spatial	O
attention	O
.	O

subsubsection	O
:	O
Relative	Method
Position	Method
Recognition	Method
In	O
order	O
to	O
check	O
whether	O
the	O
model	O
has	O
the	O
ability	O
to	O
infer	O
the	O
position	O
of	O
one	O
object	O
relative	O
to	O
another	O
object	O
,	O
we	O
collect	O
all	O
the	O
cat	O
images	O
from	O
the	O
MS	Material
COCO	Material
Detection	Material
dataset	Material
,	O
and	O
add	O
a	O
red	O
square	O
on	O
the	O
[	O
top	O
bottom	O
left	O
right	O
]	O
of	O
the	O
bounding	O
box	O
of	O
the	O
cat	O
in	O
the	O
images	O
.	O

For	O
each	O
generated	O
image	O
,	O
we	O
create	O
four	O
questions	O
,	O
“	O
Is	O
there	O
a	O
red	O
square	O
on	O
the	O
[	O
top	O
bottom	O
left	O
right	O
]	O
of	O
the	O
cat	O
?	O
”	O
together	O
with	O
three	O
“	O
no	O
”	O
answers	O
and	O
one	O
“	O
yes	O
”	O
answer	O
.	O

We	O
select	O
2639	O
training	O
cat	O
images	O
and	O
1395	O
testing	O
cat	O
images	O
from	O
MS	Material
COCO	Material
Detection	Material
dataset	Material
.	O

Our	O
SMem	O
-	O
VQA	Task
one	O
-	O
hop	O
model	O
achieves	O
96	O
%	O
test	Metric
accuracy	Metric
on	O
this	O
synthetic	O
task	O
,	O
while	O
the	O
baseline	O
model	O
(	O
iBOWIMG	Metric
)	Metric
accuracy	Metric
is	O
around	O
75	O
%	O
.	O

We	O
also	O
check	O
that	O
another	O
simple	O
baseline	O
that	O
predicts	O
the	O
answer	O
based	O
on	O
the	O
absolute	O
position	O
of	O
the	O
square	O
in	O
the	O
image	O
gets	O
around	O
70	O
%	O
accuracy	Metric
.	O

We	O
visualize	O
the	O
evidence	O
embedding	O
features	O
and	O
the	O
attention	O
weights	O
of	O
several	O
typical	O
examples	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

The	O
evidence	Method
embedding	Method
has	O
high	O
activations	O
on	O
the	O
cat	O
and	O
the	O
red	O
square	O
,	O
while	O
the	O
attention	O
weights	O
pay	O
high	O
attention	O
to	O
certain	O
locations	O
around	O
the	O
cat	O
.	O

We	O
can	O
analyze	O
the	O
attention	O
in	O
the	O
correctly	O
predicted	O
examples	O
using	O
the	O
same	O
rules	O
as	O
in	O
absolute	Task
position	Task
recognition	Task
experiment	O
.	O

These	O
rules	O
still	O
work	O
,	O
but	O
the	O
position	O
is	O
relative	O
to	O
the	O
cat	O
object	O
:	O
1	O
)	O
Check	O
the	O
specified	O
position	O
relative	O
to	O
the	O
cat	O
,	O
if	O
it	O
finds	O
the	O
square	O
,	O
then	O
answer	O
“	O
yes	O
”	O
,	O
otherwise	O
“	O
no	O
”	O
;	O
2	O
)	O
Find	O
the	O
square	O
,	O
then	O
answer	O
“	O
yes	O
”	O
for	O
the	O
specified	O
position	O
,	O
and	O
answer	O
“	O
no	O
”	O
for	O
the	O
other	O
positions	O
around	O
the	O
cat	O
.	O

We	O
also	O
check	O
the	O
images	O
where	O
our	O
model	O
makes	O
mistakes	O
,	O
and	O
find	O
that	O
the	O
mistakes	O
mainly	O
occur	O
in	O
images	O
with	O
more	O
than	O
one	O
cats	O
.	O

The	O
red	O
square	O
appears	O
near	O
only	O
one	O
of	O
the	O
cats	O
in	O
the	O
image	O
,	O
but	O
our	O
model	O
might	O
make	O
mistakes	O
by	O
focusing	O
on	O
the	O
other	O
cats	O
.	O

We	O
conclude	O
that	O
our	O
SMem	O
-	O
VQA	Task
model	O
can	O
infer	O
the	O
relative	O
spatial	O
position	O
based	O
on	O
the	O
spatial	O
attention	O
around	O
the	O
specified	O
object	O
,	O
which	O
can	O
also	O
be	O
represented	O
by	O
some	O
logical	Method
inference	Method
rules	Method
.	O

subsection	O
:	O
Experiments	O
on	O
Standard	O
Datasets	O
subsubsection	O
:	O
Results	O
on	O
DAQUAR	Material
The	O
DAQUAR	Material
dataset	O
is	O
a	O
relatively	O
small	O
dataset	O
which	O
builds	O
on	O
the	O
NYU	Material
Depth	Material
Dataset	Material
V2	Material
.	O

We	O
use	O
the	O
reduced	O
DAQUAR	Material
dataset	O
.	O

The	O
evaluation	Metric
metric	Metric
for	O
this	O
dataset	O
is	O
0	O
-	O
1	O
accuracy	Metric
.	O

The	O
embedding	O
dimension	O
is	O
512	O
for	O
our	O
models	O
running	O
on	O
the	O
DAQUAR	Material
dataset	O
.	O

We	O
use	O
several	O
reported	O
models	O
on	O
DAQUAR	Material
as	O
baselines	O
,	O
which	O
are	O
listed	O
below	O
:	O
Multi	Task
-	Task
World	Task
:	O
an	O
approach	O
based	O
on	O
handcrafted	O
features	O
using	O
a	O
semantic	Method
parse	Method
of	Method
the	Method
question	Method
and	Method
scene	Method
analysis	Method
of	O
the	O
image	O
combined	O
in	O
a	O
latent	Method
-	Method
world	Method
Bayesian	Method
framework	Method
.	O

Neural	O
-	O
Image	O
-	O
QA	Task
:	O
uses	O
an	O
LSTM	Method
to	O
encode	O
the	O
question	O
and	O
then	O
decode	O
the	O
hidden	O
information	O
into	O
the	O
answer	O
.	O

The	O
image	O
CNN	Method
feature	O
vector	O
is	O
shown	O
at	O
each	O
time	O
step	O
of	O
the	O
encoding	Method
phase	Method
.	O

Question	Method
LSTM	Method
:	O
only	O
shows	O
the	O
question	O
to	O
the	O
LSTM	Method
to	O
predict	O
the	O
answer	O
without	O
any	O
image	O
information	O
.	O

VIS	Method
+	Method
LSTM	Method
:	O
similar	O
to	O
Neural	O
-	O
Image	O
-	O
QA	Task
,	O
but	O
only	O
shows	O
the	O
image	O
features	O
to	O
the	O
LSTM	Method
at	O
the	O
first	O
time	O
step	O
,	O
and	O
the	O
question	O
in	O
the	O
remaining	O
time	O
steps	O
to	O
predict	O
the	O
answer	O
.	O

Question	O
BOW	Method
:	O
only	O
uses	O
the	O
BOW	Method
question	O
representation	O
and	O
a	O
single	O
hidden	Method
layer	Method
neural	Method
network	Method
to	O
predict	O
the	O
answer	O
,	O
without	O
any	O
image	O
features	O
.	O

IMG	Method
+	O
BOW	Method
:	O
concatenates	O
the	O
BOW	Method
question	O
representation	O
with	O
image	O
features	O
,	O
and	O
then	O
uses	O
a	O
single	O
hidden	Method
layer	Method
neural	Method
network	Method
to	O
predict	O
the	O
answer	O
.	O

This	O
model	O
is	O
similar	O
to	O
the	O
iBOWIMG	Method
baseline	Method
model	Method
in	O
.	O

Results	O
of	O
our	O
SMem	O
-	O
VQA	Task
model	O
on	O
the	O
DAQUAR	Material
dataset	O
and	O
the	O
baseline	O
model	O
results	O
reported	O
in	O
previous	O
work	O
are	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

From	O
the	O
DAQUAR	Material
result	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
see	O
that	O
models	O
based	O
on	O
deep	Method
features	Method
significantly	O
outperform	O
the	O
Multi	Method
-	Method
World	Method
approach	Method
based	O
on	O
hand	O
-	O
crafted	O
features	O
.	O

Modeling	O
the	O
question	O
only	O
with	O
either	O
the	O
LSTM	Method
model	Method
or	O
Question	O
BOW	Method
model	O
does	O
equally	O
well	O
in	O
comparison	O
,	O
indicating	O
the	O
the	O
question	O
text	O
contains	O
important	O
prior	O
information	O
for	O
predicting	O
the	O
answer	O
.	O

Also	O
,	O
on	O
this	O
dataset	O
,	O
the	O
VIS	Method
+	Method
LSTM	Method
model	Method
achieves	O
better	O
accuracy	Metric
than	O
Neural	O
-	O
Image	O
-	O
QA	Task
model	O
;	O
the	O
former	O
shows	O
the	O
image	O
only	O
at	O
the	O
first	O
timestep	O
of	O
the	O
LSTM	Method
,	O
while	O
the	O
latter	O
does	O
so	O
at	O
each	O
timestep	O
.	O

In	O
comparison	O
,	O
both	O
our	O
One	Method
-	Method
Hop	Method
model	Method
and	O
Two	Method
-	Method
Hop	Method
spatial	Method
attention	Method
models	Method
outperform	O
the	O
IMG	Method
+	O
BOW	Method
,	O
as	O
well	O
as	O
the	O
other	O
baseline	O
models	O
.	O

A	O
major	O
advantage	O
of	O
our	O
model	O
is	O
the	O
ability	O
to	O
visualize	O
the	O
inference	Task
process	Task
in	O
the	O
deep	Method
network	Method
.	O

To	O
illustrate	O
this	O
,	O
two	O
attention	O
weights	O
visualization	O
examples	O
in	O
SMem	Method
-	Method
VQA	Method
One	O
-	Method
Hop	Method
and	O
Two	Method
-	Method
Hop	Method
models	Method
on	O
DAQUAR	Material
dataset	O
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
bottom	O
row	O
)	O
.	O

subsubsection	O
:	O
Results	O
on	O
VQA	Task
The	O
VQA	Material
dataset	Material
is	O
a	O
recent	O
large	O
dataset	O
based	O
on	O
MS	Material
COCO	Material
.	O

We	O
use	O
the	O
full	O
release	O
(	O
V1.0	O
)	O
open	O
-	O
ended	O
dataset	O
,	O
which	O
contains	O
a	O
train	O
set	O
and	O
a	O
val	O
set	O
.	O

Following	O
standard	O
practice	O
,	O
we	O
choose	O
the	O
top	O
1000	O
answers	O
in	O
train	O
and	O
val	O
sets	O
as	O
possible	O
prediction	O
answers	O
,	O
and	O
only	O
keep	O
the	O
examples	O
whose	O
answers	O
belong	O
to	O
these	O
1000	O
answers	O
as	O
training	O
data	O
.	O

The	O
question	Metric
vocabulary	Metric
size	Metric
is	O
7477	O
with	O
the	O
word	O
frequency	O
of	O
at	O
least	O
three	O
.	O

Because	O
of	O
the	O
larger	O
training	O
size	O
,	O
the	O
embedding	O
dimension	O
is	O
1000	O
on	O
the	O
VQA	Material
dataset	Material
.	O

We	O
report	O
the	O
test	O
-	O
dev	O
and	O
test	O
-	O
standard	O
results	O
from	O
the	O
VQA	Task
evaluation	O
server	O
.	O

The	O
server	Task
evaluation	Task
uses	O
the	O
evaluation	Metric
metric	Metric
introduced	O
by	O
,	O
which	O
gives	O
partial	O
credit	O
to	O
certain	O
synonym	O
answers	O
:	O
.	O

For	O
the	O
attention	Method
models	Method
,	O
we	O
do	O
not	O
mirror	O
the	O
input	O
image	O
when	O
using	O
the	O
CNN	Method
to	O
extract	O
convolutional	O
features	O
,	O
since	O
this	O
might	O
cause	O
confusion	O
about	O
the	O
spatial	O
locations	O
of	O
objects	O
in	O
the	O
input	O
image	O
.	O

The	O
optimization	Method
algorithm	Method
used	O
is	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
a	O
minibatch	O
of	O
size	O
50	O
and	O
momentum	O
of	O
0.9	O
.	O

The	O
base	O
learning	Metric
rate	Metric
is	O
set	O
to	O
be	O
0.01	O
which	O
is	O
halved	O
every	O
six	O
epoches	O
.	O

Regularization	Method
,	O
dropout	Method
and	O
L2	Method
norm	Method
are	O
cross	O
-	O
validated	O
and	O
used	O
.	O

For	O
the	O
VQA	Material
dataset	Material
,	O
we	O
use	O
the	O
simple	O
iBOWIMG	Method
model	Method
in	O
as	O
one	O
baseline	O
model	O
,	O
which	O
beats	O
most	O
existing	O
VQA	Task
models	O
currently	O
on	O
arxiv.org	O
.	O

We	O
also	O
compare	O
to	O
two	O
models	O
in	O
which	O
have	O
comparable	O
or	O
better	O
results	O
to	O
the	O
iBOWIMG	Method
model	Method
.	O

These	O
three	O
baseline	O
models	O
as	O
well	O
the	O
best	O
model	O
in	O
VQA	Material
dataset	Material
paper	O
are	O
listed	O
in	O
the	O
following	O
:	O
LSTM	Method
Q	Method
+	Method
I	Method
:	O
uses	O
the	O
element	Method
-	Method
wise	Method
multiplication	Method
of	O
the	O
LSTM	Method
encoding	Method
of	O
the	O
question	O
and	O
the	O
image	O
feature	O
vector	O
to	O
predict	O
the	O
answer	O
.	O

This	O
is	O
the	O
best	O
model	O
in	O
the	O
VQA	Material
dataset	Material
paper	O
.	O

ACK	O
:	O
shows	O
the	O
image	O
attribute	O
features	O
,	O
the	O
generated	O
image	Material
caption	Material
and	O
relevant	O
external	O
knowledge	O
from	O
knowledge	Method
base	Method
to	O
the	O
LSTM	Method
at	O
the	O
first	O
time	O
step	O
,	O
and	O
the	O
question	O
in	O
the	O
remaining	O
time	O
steps	O
to	O
predict	O
the	O
answer	O
.	O

DPPnet	Method
:	O
uses	O
the	O
Gated	Method
Recurrent	Method
Unit	Method
(	O
GRU	Method
)	O
representation	O
of	O
question	O
to	O
predict	O
certain	O
parameters	O
for	O
a	O
CNN	Method
classification	O
network	O
.	O

They	O
pre	O
-	O
train	O
the	O
GRU	Method
for	O
question	Task
representation	Task
on	O
a	O
large	O
-	O
scale	O
text	O
corpus	O
to	O
improve	O
the	O
GRU	Method
generalization	O
performance	O
.	O

iBOWIMG	Method
:	O
concatenates	O
the	O
BOW	Method
question	O
representation	O
with	O
image	O
feature	O
(	O
GoogLeNet	Method
)	O
,	O
and	O
uses	O
a	O
softmax	Method
classification	Method
to	O
predict	O
the	O
answer	O
.	O

The	O
overall	O
accuracy	Metric
and	O
per	Metric
-	Metric
answer	Metric
category	Metric
accuracy	Metric
for	O
our	O
SMem	O
-	O
VQA	Task
models	O
and	O
the	O
four	O
baseline	O
models	O
on	O
VQA	Material
dataset	Material
are	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

From	O
the	O
table	O
,	O
we	O
can	O
see	O
that	O
the	O
SMem	O
-	O
VQA	Task
One	O
-	O
Hop	O
model	O
obtains	O
slightly	O
better	O
results	O
compared	O
to	O
the	O
iBOWIMG	Method
model	Method
.	O

However	O
,	O
the	O
SMem	O
-	O
VQA	Task
Two	O
-	O
Hop	O
model	O
achieves	O
an	O
improvement	O
of	O
2.27	O
%	O
on	O
test	Metric
-	Metric
dev	Metric
and	O
2.35	O
%	O
on	O
test	O
-	O
standard	O
compared	O
to	O
the	O
iBOWIMG	Method
model	Method
,	O
demonstrating	O
the	O
value	O
of	O
spatial	O
attention	O
.	O

The	O
SMem	O
-	O
VQA	Task
Two	O
-	O
Hop	O
model	O
also	O
shows	O
best	O
performance	O
in	O
the	O
per	Metric
-	Metric
answer	Metric
category	Metric
accuracy	Metric
.	O

The	O
SMem	O
-	O
VQA	Task
Two	O
-	O
Hop	O
model	O
has	O
slightly	O
better	O
result	O
than	O
the	O
DPPnet	Method
model	Method
.	O

The	O
DPPnet	Method
model	Method
uses	O
a	O
large	O
-	O
scale	O
text	O
corpus	O
to	O
pre	O
-	O
train	O
the	O
Gated	Method
Recurrent	Method
Unit	Method
(	O
GRU	Method
)	O
network	O
for	O
question	Task
representation	Task
.	O

Similar	O
pre	Method
-	Method
training	Method
work	O
on	O
extra	O
data	O
to	O
improve	O
model	Metric
accuracy	Metric
has	O
been	O
done	O
in	O
.	O

Considering	O
the	O
fact	O
that	O
our	O
model	O
does	O
not	O
use	O
extra	O
data	O
to	O
pre	O
-	O
train	O
the	O
word	O
embeddings	O
,	O
its	O
results	O
are	O
very	O
competitive	O
.	O

We	O
also	O
experiment	O
with	O
adding	O
a	O
third	O
hop	O
into	O
our	O
model	O
on	O
the	O
VQA	Material
dataset	Material
,	O
but	O
the	O
result	O
does	O
not	O
improve	O
further	O
.	O

The	O
attention	O
weights	O
visualization	O
examples	O
for	O
the	O
SMem	Method
-	Method
VQA	Method
One	Method
-	Method
Hop	Method
and	O
Two	Method
-	Method
Hop	Method
models	Method
on	O
the	O
VQA	Material
dataset	Material
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
top	O
row	O
)	O
.	O

From	O
the	O
visualization	O
,	O
we	O
can	O
see	O
that	O
the	O
two	Method
-	Method
hop	Method
model	Method
collects	O
supplementary	O
evidence	O
for	O
inferring	O
the	O
answer	O
,	O
which	O
may	O
be	O
necessary	O
to	O
achieve	O
an	O
improvement	O
on	O
these	O
complicated	O
real	O
-	O
world	O
datasets	O
.	O

We	O
also	O
visualize	O
the	O
fine	O
-	O
grained	O
alignment	O
in	O
the	O
first	O
hop	O
of	O
our	O
SMem	O
-	O
VQA	Task
Two	O
-	O
Hop	O
model	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

The	O
correlation	O
vector	O
values	O
(	O
blue	O
bars	O
)	O
measure	O
the	O
correlation	O
between	O
image	O
regions	O
and	O
each	O
word	O
vector	O
in	O
the	O
question	O
.	O

Higher	O
values	O
indicate	O
stronger	O
correlation	O
of	O
that	O
particular	O
word	O
with	O
the	O
specific	O
location	O
’s	O
image	O
features	O
.	O

We	O
observe	O
that	O
the	O
fine	O
-	O
grained	O
visual	O
evidence	O
collected	O
using	O
each	O
local	O
word	O
vector	O
,	O
together	O
with	O
the	O
global	O
visual	O
evidence	O
from	O
the	O
whole	O
question	O
,	O
complement	O
each	O
other	O
to	O
infer	O
the	O
correct	O
answer	O
for	O
the	O
given	O
image	O
and	O
question	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
proposed	O
the	O
Spatial	Method
Memory	Method
Network	Method
for	O
VQA	Task
,	O
a	O
memory	Method
network	Method
architecture	Method
with	O
a	O
spatial	Method
attention	Method
mechanism	Method
adapted	O
to	O
the	O
visual	Task
question	Task
answering	Task
task	Task
.	O

We	O
proposed	O
a	O
set	O
of	O
synthetic	O
spatial	O
questions	O
and	O
demonstrated	O
that	O
our	O
model	O
learns	O
inference	O
rules	O
based	O
on	O
spatial	O
attention	O
through	O
attention	Method
weight	Method
visualization	Method
.	O

Evaluation	O
on	O
the	O
challenging	O
DAQUAR	Material
and	O
VQA	Material
datasets	Material
showed	O
improved	O
results	O
over	O
previously	O
published	O
models	O
.	O

Our	O
model	O
can	O
be	O
used	O
to	O
visualize	O
the	O
inference	O
steps	O
learned	O
by	O
the	O
deep	Method
network	Method
,	O
giving	O
some	O
insight	O
into	O
its	O
processing	O
.	O

Future	O
work	O
may	O
include	O
further	O
exploring	O
the	O
inference	Task
ability	O
of	O
our	O
SMem	O
-	O
VQA	Task
model	O
and	O
exploring	O
other	O
VQA	Task
attention	O
models	O
.	O

bibliography	O
:	O
References	O
