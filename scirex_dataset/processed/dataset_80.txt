document	O
:	O
The	O
Arcade	Method
Learning	Method
Environment	Method
:	O
An	O
Evaluation	Method
Platform	Method
for	O
General	Task
Agents	Task
In	O
this	O
article	O
we	O
introduce	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
:	O
both	O
a	O
challenge	O
problem	O
and	O
a	O
platform	O
and	O
methodology	O
for	O
evaluating	O
the	O
development	O
of	O
general	Task
,	Task
domain	Task
-	Task
independent	Task
AI	Task
technology	Task
.	O

ALE	Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	Material
2600	Material
game	Material
environments	Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

ALE	Method
presents	O
significant	O
research	O
challenges	O
for	O
reinforcement	Task
learning	Task
,	O
model	Method
learning	Method
,	O
model	Method
-	Method
based	Method
planning	Method
,	O
imitation	Task
learning	Task
,	O
transfer	Task
learning	Task
,	O
and	O
intrinsic	Task
motivation	Task
.	O

Most	O
importantly	O
,	O
it	O
provides	O
a	O
rigorous	O
testbed	O
for	O
evaluating	O
and	O
comparing	O
approaches	O
to	O
these	O
problems	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	Method
by	O
developing	O
and	O
benchmarking	O
domain	Method
-	Method
independent	Method
agents	Method
designed	O
using	O
well	O
-	O
established	O
AI	Method
techniques	Method
for	O
both	O
reinforcement	Task
learning	Task
and	O
planning	Task
.	O

In	O
doing	O
so	O
,	O
we	O
also	O
propose	O
an	O
evaluation	O
methodology	O
made	O
possible	O
by	O
ALE	Method
,	O
reporting	O
empirical	O
results	O
on	O
over	O
55	O
different	O
games	O
.	O

All	O
of	O
the	O
software	O
,	O
including	O
the	O
benchmark	O
agents	O
,	O
is	O
publicly	O
available	O
.	O

472013253–27902	O
/	O
1306	O
/	O
13	O
TheArcadeLearningEnvironment	O
:	O
AnEvaluationPlatformforGeneralAgents	O
Bellemare	O
,	O
Naddaf	O
,	O
Veness	O
,&	O
Bowling	O
253	O
section	O
:	O
Introduction	O
A	O
longstanding	O
goal	O
of	O
artificial	Task
intelligence	Task
is	O
the	O
development	O
of	O
algorithms	O
capable	O
of	O
general	Task
competency	Task
in	O
a	O
variety	O
of	O
tasks	O
and	O
domains	O
without	O
the	O
need	O
for	O
domain	O
-	O
specific	O
tailoring	O
.	O

To	O
this	O
end	O
,	O
different	O
theoretical	O
frameworks	O
have	O
been	O
proposed	O
to	O
formalize	O
the	O
notion	O
of	O
“	O
big	O
”	O
artificial	O
intelligence	O
e.g.	O
,	O
¿Russell97rationalityand	O
,	O
Hutter:04uaibook	O
,	O
legg08machine	O
.	O

Similar	O
ideas	O
have	O
been	O
developed	O
around	O
the	O
theme	O
of	O
lifelong	Task
learning	Task
:	O
learning	O
a	O
reusable	Task
,	Task
high	Task
-	Task
level	Task
understanding	Task
of	Task
the	Task
world	Task
from	O
raw	Material
sensory	Material
data	Material
thrun95lifelong	O
,	O
pierce_kuipers_97	O
,	O
stober08pixels	O
,	O
sutton11horde	O
.	O

The	O
growing	O
interest	O
in	O
competitions	O
such	O
as	O
the	O
General	Task
Game	Task
Playing	Task
competition	Task
,	O
Reinforcement	Task
Learning	Task
competition	Task
,	O
and	O
the	O
International	Task
Planning	Task
competition	Task
coles_12	O
also	O
suggests	O
the	O
artificial	Task
intelligence	Task
community	O
’s	O
desire	O
for	O
the	O
emergence	O
of	O
algorithms	O
that	O
provide	O
general	O
competency	O
.	O

Designing	O
generally	O
competent	O
agents	O
raises	O
the	O
question	O
of	O
how	O
to	O
best	O
evaluate	O
them	O
.	O

Empirically	O
evaluating	O
general	Task
competency	Task
on	O
a	O
handful	O
of	O
parametrized	Material
benchmark	Material
problems	Material
is	O
,	O
by	O
definition	O
,	O
flawed	O
.	O

Such	O
an	O
evaluation	O
is	O
prone	O
to	O
method	O
overfitting	O
and	O
discounts	O
the	O
amount	O
of	O
expert	O
effort	O
necessary	O
to	O
transfer	O
the	O
algorithm	O
to	O
new	O
domains	O
.	O

Ideally	O
,	O
the	O
algorithm	O
should	O
be	O
compared	O
across	O
domains	O
that	O
are	O
(	O
i	O
)	O
varied	O
enough	O
to	O
claim	O
generality	O
,	O
(	O
ii	O
)	O
each	O
interesting	O
enough	O
to	O
be	O
representative	O
of	O
settings	O
that	O
might	O
be	O
faced	O
in	O
practice	O
,	O
and	O
(	O
iii	O
)	O
each	O
created	O
by	O
an	O
independent	O
party	O
to	O
be	O
free	O
of	O
experimenter	O
’s	O
bias	O
.	O

In	O
this	O
article	O
,	O
we	O
introduce	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
:	O
a	O
new	O
challenge	O
problem	O
,	O
platform	O
,	O
and	O
experimental	O
methodology	O
for	O
empirically	Task
assessing	Task
agents	Task
designed	O
for	O
general	Task
competency	Task
.	O

ALE	Method
is	O
a	O
software	Method
framework	Method
for	O
interfacing	O
with	O
emulated	O
Atari	Material
2600	Material
game	Material
environments	Material
.	O

The	O
Atari	Method
2600	Method
,	O
a	O
second	Method
generation	Method
game	Method
console	Method
,	O
was	O
originally	O
released	O
in	O
1977	O
and	O
remained	O
massively	O
popular	O
for	O
over	O
a	O
decade	O
.	O

Over	O
500	O
games	O
were	O
developed	O
for	O
the	O
Atari	Material
2600	Material
,	O
spanning	O
a	O
diverse	O
range	O
of	O
genres	O
such	O
as	O
shooters	Material
,	O
beat’em	Material
ups	Material
,	O
puzzle	Material
,	O
sports	Material
,	O
and	O
action	Task
-	Task
adventure	Task
games	Task
;	O
many	O
game	O
genres	O
were	O
pioneered	O
on	O
the	O
console	O
.	O

While	O
modern	O
game	O
consoles	O
involve	O
visuals	O
,	O
controls	O
,	O
and	O
a	O
general	O
complexity	O
that	O
rivals	O
the	O
real	O
world	O
,	O
Atari	Material
2600	Material
games	Material
are	O
far	O
simpler	O
.	O

In	O
spite	O
of	O
this	O
,	O
they	O
still	O
pose	O
a	O
variety	O
of	O
challenging	O
and	O
interesting	O
situations	O
for	O
human	O
players	O
.	O

ALE	Method
is	O
both	O
an	O
experimental	O
methodology	O
and	O
a	O
challenge	O
problem	O
for	O
general	Task
AI	Task
competency	Task
.	O

In	O
machine	Task
learning	Task
,	O
it	O
is	O
considered	O
poor	O
experimental	O
practice	O
to	O
both	O
train	O
and	O
evaluate	O
an	O
algorithm	O
on	O
the	O
same	O
data	O
set	O
,	O
as	O
it	O
can	O
grossly	O
over	O
-	O
estimate	O
the	O
algorithm	O
’s	O
performance	O
.	O

The	O
typical	O
practice	O
is	O
instead	O
to	O
train	O
on	O
a	O
training	O
set	O
then	O
evaluate	O
on	O
a	O
disjoint	O
test	O
set	O
.	O

With	O
the	O
large	O
number	O
of	O
available	O
games	O
in	O
ALE	Method
,	O
we	O
propose	O
that	O
a	O
similar	O
methodology	O
can	O
be	O
used	O
to	O
the	O
same	O
effect	O
:	O
an	O
approach	O
’s	O
domain	Method
representation	Method
and	O
parametrization	Method
should	O
be	O
first	O
tuned	O
on	O
a	O
small	O
number	O
of	O
training	O
games	O
,	O
before	O
testing	O
the	O
approach	O
on	O
unseen	Material
testing	Material
games	Material
.	O

Ideally	O
,	O
agents	O
designed	O
in	O
this	O
fashion	O
are	O
evaluated	O
on	O
the	O
testing	O
games	O
only	O
once	O
,	O
with	O
no	O
possibility	O
for	O
subsequent	O
modifications	O
to	O
the	O
algorithm	O
.	O

While	O
general	Method
competency	Method
remains	O
the	O
long	O
-	O
term	O
goal	O
for	O
artificial	Task
intelligence	Task
,	O
ALE	Method
proposes	O
an	O
achievable	O
stepping	O
stone	O
:	O
techniques	O
for	O
general	Task
competency	Task
across	O
the	O
gamut	O
of	O
Atari	Material
2600	Material
games	Material
.	O

We	O
believe	O
this	O
represents	O
a	O
goal	O
that	O
is	O
attainable	O
in	O
a	O
short	O
time	O
-	O
frame	O
yet	O
formidable	O
enough	O
to	O
require	O
new	O
technological	O
breakthroughs	O
.	O

section	O
:	O
Arcade	Method
Learning	Method
Environment	Method
We	O
begin	O
by	O
describing	O
our	O
main	O
contribution	O
,	O
the	O
Arcade	Method
Learning	Method
Environment	Method
(	O
ALE	Method
)	O
.	O

ALE	Method
is	O
a	O
software	Method
framework	Method
designed	O
to	O
make	O
it	O
easy	O
to	O
develop	O
agents	O
that	O
play	O
arbitrary	O
Atari	Material
2600	Material
games	Material
.	O

subsection	O
:	O
The	O
Atari	Material
2600	Material
The	O
Atari	Method
2600	Method
is	O
a	O
home	Method
video	Method
game	Method
console	Method
developed	O
in	O
1977	O
and	O
sold	O
for	O
over	O
a	O
decade	O
.	O

It	O
popularized	O
the	O
use	O
of	O
general	Method
purpose	Method
CPUs	Method
in	O
game	O
console	O
hardware	O
,	O
with	O
game	O
code	O
distributed	O
through	O
cartridges	O
.	O

Over	O
500	O
original	O
games	O
were	O
released	O
for	O
the	O
console	O
;	O
“	O
homebrew	Material
”	Material
games	Material
continue	O
to	O
be	O
developed	O
today	O
,	O
over	O
thirty	O
years	O
later	O
.	O

The	O
console	O
’s	O
joystick	O
,	O
as	O
well	O
as	O
some	O
of	O
the	O
original	O
games	O
such	O
as	O
Adventure	O
and	O
Pitfall	Material
!	Material

,	O
are	O
iconic	O
symbols	O
of	O
early	O
video	Material
games	Material
.	O

Nearly	O
all	O
arcade	Method
games	Method
of	O
the	O
time	O
–	O
Pac	Method
-	Method
Man	Method
and	O
Space	Task
Invaders	Task
are	O
two	O
well	O
-	O
known	O
examples	O
–	O
were	O
ported	O
to	O
the	O
console	O
.	O

Despite	O
the	O
number	O
and	O
variety	O
of	O
games	O
developed	O
for	O
the	O
Atari	Material
2600	Material
,	O
the	O
hardware	O
is	O
relatively	O
simple	O
.	O

It	O
has	O
a	O
1.19Mhz	Method
CPU	Method
and	O
can	O
be	O
emulated	O
much	O
faster	O
than	O
real	O
-	O
time	O
on	O
modern	O
hardware	O
.	O

The	O
cartridge	O
ROM	O
(	O
typically	O
2–4kB	O
)	O
holds	O
the	O
game	O
code	O
,	O
while	O
the	O
console	O
RAM	O
itself	O
only	O
holds	O
128	O
bytes	O
(	O
1024	O
bits	O
)	O
.	O

A	O
single	O
game	O
screen	O
is	O
160	O
pixels	O
wide	O
and	O
210	O
pixels	O
high	O
,	O
with	O
a	O
128	O
-	O
colour	O
palette	O
;	O
18	O
“	O
actions	O
”	O
can	O
be	O
input	O
to	O
the	O
game	O
via	O
a	O
digital	O
joystick	O
:	O
three	O
positions	O
of	O
the	O
joystick	O
for	O
each	O
axis	O
,	O
plus	O
a	O
single	O
button	O
.	O

The	O
Atari	Method
2600	Method
hardware	Method
limits	O
the	O
possible	O
complexity	O
of	O
games	O
,	O
which	O
we	O
believe	O
strikes	O
the	O
perfect	O
balance	O
:	O
a	O
challenging	O
platform	O
offering	O
conceivable	O
near	O
-	O
term	O
advancements	O
in	O
learning	Task
,	O
modelling	Task
,	O
and	O
planning	Task
.	O

subsection	O
:	O
Interface	O
ALE	Method
is	O
built	O
on	O
top	O
of	O
Stella	Method
,	O
an	O
open	Method
-	Method
source	Method
Atari	Method
2600	Method
emulator	Method
.	O

It	O
allows	O
the	O
user	O
to	O
interface	O
with	O
the	O
Atari	Material
2600	Material
by	O
receiving	O
joystick	O
motions	O
,	O
sending	O
screen	O
and	O
/	O
or	O
RAM	O
information	O
,	O
and	O
emulating	O
the	O
platform	O
.	O

ALE	Method
also	O
provides	O
a	O
game	Method
-	Method
handling	Method
layer	Method
which	O
transforms	O
each	O
game	O
into	O
a	O
standard	O
reinforcement	Task
learning	Task
problem	Task
by	O
identifying	O
the	O
accumulated	O
score	O
and	O
whether	O
the	O
game	O
has	O
ended	O
.	O

By	O
default	O
,	O
each	O
observation	O
consists	O
of	O
a	O
single	O
game	O
screen	O
(	O
frame	O
)	O
:	O
a	O
2D	O
array	O
of	O
7	O
-	O
bit	O
pixels	O
,	O
160	O
pixels	O
wide	O
by	O
210	O
pixels	O
high	O
.	O

The	O
action	O
space	O
consists	O
of	O
the	O
18	O
discrete	O
actions	O
defined	O
by	O
the	O
joystick	Method
controller	Method
.	O

The	O
game	Method
-	Method
handling	Method
layer	Method
also	O
specifies	O
the	O
minimal	O
set	O
of	O
actions	O
needed	O
to	O
play	O
a	O
particular	O
game	O
,	O
although	O
none	O
of	O
the	O
results	O
in	O
this	O
paper	O
make	O
use	O
of	O
this	O
information	O
.	O

When	O
running	O
in	O
real	O
-	O
time	O
,	O
the	O
simulator	O
generates	O
60	O
frames	O
per	O
second	O
,	O
and	O
at	O
full	O
speed	O
emulates	O
up	O
to	O
6000	O
frames	O
per	O
second	O
.	O

The	O
reward	O
at	O
each	O
time	O
-	O
step	O
is	O
defined	O
on	O
a	O
game	O
by	O
game	O
basis	O
,	O
typically	O
by	O
taking	O
the	O
difference	O
in	O
score	O
or	O
points	O
between	O
frames	O
.	O

An	O
episode	O
begins	O
on	O
the	O
first	O
frame	O
after	O
a	O
reset	O
command	O
is	O
issued	O
,	O
and	O
terminates	O
when	O
the	O
game	O
ends	O
.	O

The	O
game	Method
-	Method
handling	Method
layer	Method
also	O
offers	O
the	O
ability	O
to	O
end	O
the	O
episode	O
after	O
a	O
predefined	O
number	O
of	O
frames	O
.	O

The	O
user	O
therefore	O
has	O
access	O
to	O
several	O
dozen	O
games	O
through	O
a	O
single	O
common	O
interface	O
,	O
and	O
adding	O
support	O
for	O
new	O
games	O
is	O
relatively	O
straightforward	O
.	O

ALE	Method
further	O
provides	O
the	O
functionality	O
to	O
save	O
and	O
restore	O
the	O
state	O
of	O
the	O
emulator	O
.	O

When	O
issued	O
a	O
save	O
-	O
state	O
command	O
,	O
ALE	Method
saves	O
all	O
the	O
relevant	O
data	O
about	O
the	O
current	O
game	O
,	O
including	O
the	O
contents	O
of	O
the	O
RAM	O
,	O
registers	O
,	O
and	O
address	O
counters	O
.	O

The	O
restore	O
-	O
state	O
command	O
similarly	O
resets	O
the	O
game	O
to	O
a	O
previously	O
saved	O
state	O
.	O

This	O
allows	O
the	O
use	O
of	O
ALE	Method
as	O
a	O
generative	Method
model	Method
to	O
study	O
topics	O
such	O
as	O
planning	Task
and	O
model	Task
-	Task
based	Task
reinforcement	Task
learning	Task
.	O

subsection	O
:	O
Source	O
Code	O
ALE	Method
is	O
released	O
as	O
free	O
,	O
open	O
-	O
source	O
software	O
under	O
the	O
terms	O
of	O
the	O
GNU	O
General	O
Public	O
License	O
.	O

The	O
latest	O
version	O
of	O
the	O
source	O
code	O
is	O
publicly	O
available	O
at	O
:	O
The	O
source	O
code	O
for	O
the	O
agents	O
used	O
in	O
the	O
benchmark	O
experiments	O
below	O
is	O
also	O
available	O
on	O
the	O
publication	O
page	O
for	O
this	O
article	O
on	O
the	O
same	O
website	O
.	O

While	O
ALE	Method
itself	O
is	O
written	O
in	O
C	Method
++	Method
,	O
a	O
variety	O
of	O
interfaces	O
are	O
available	O
that	O
allow	O
users	O
to	O
interact	O
with	O
ALE	Method
in	O
the	O
programming	O
language	O
of	O
their	O
choice	O
.	O

Support	O
for	O
new	O
games	O
is	O
easily	O
added	O
by	O
implementing	O
a	O
derived	Method
class	Method
representing	O
the	O
game	O
’s	O
particular	O
reward	O
and	O
termination	O
functions	O
.	O

section	O
:	O
Benchmark	O
Results	O
Planning	Task
and	O
reinforcement	Method
learning	Method
are	O
two	O
different	O
AI	Task
problem	Task
formulations	Task
that	O
can	O
naturally	O
be	O
investigated	O
within	O
the	O
ALE	Method
framework	O
.	O

Our	O
purpose	O
in	O
presenting	O
benchmark	O
results	O
for	O
both	O
of	O
these	O
formulations	O
is	O
two	O
-	O
fold	O
.	O

First	O
,	O
these	O
results	O
provide	O
a	O
baseline	O
performance	O
for	O
traditional	O
techniques	O
,	O
establishing	O
a	O
point	O
of	O
comparison	O
with	O
future	O
,	O
more	O
advanced	O
,	O
approaches	O
.	O

Second	O
,	O
in	O
describing	O
these	O
results	O
we	O
illustrate	O
our	O
proposed	O
methodology	O
for	O
doing	O
empirical	Task
validation	Task
with	O
ALE	Method
.	O

subsection	O
:	O
Reinforcement	Method
Learning	Method
We	O
begin	O
by	O
providing	O
benchmark	O
results	O
using	O
SARSA	Method
,	O
a	O
traditional	O
technique	O
for	O
model	Method
-	Method
free	Method
reinforcement	Method
learning	Method
.	O

Note	O
that	O
in	O
the	O
reinforcement	Task
learning	Task
setting	Task
,	O
the	O
agent	O
does	O
not	O
have	O
access	O
to	O
a	O
model	O
of	O
the	O
game	O
dynamics	O
.	O

At	O
each	O
time	O
step	O
,	O
the	O
agent	O
selects	O
an	O
action	O
and	O
receives	O
a	O
reward	O
and	O
an	O
observation	O
,	O
and	O
the	O
agent	O
’s	O
aim	O
is	O
to	O
maximize	O
its	O
accumulated	O
reward	O
.	O

In	O
these	O
experiments	O
,	O
we	O
augmented	O
the	O
SARSA	Method
(	Method
)	Method
algorithm	Method
with	O
linear	Method
function	Method
approximation	Method
,	O
replacing	Method
traces	Method
,	O
and	O
-	Method
greedy	Method
exploration	Method
.	O

A	O
detailed	O
explanation	O
of	O
SARSA	Method
(	Method
)	O
and	O
its	O
extensions	O
can	O
be	O
found	O
in	O
the	O
work	O
of	O
sutton_barto_98	O
.	O

subsubsection	Method
:	O
Feature	Method
Construction	Method
In	O
our	O
approach	O
to	O
the	O
reinforcement	Task
learning	Task
setting	Task
,	O
the	O
most	O
important	O
design	O
issue	O
is	O
the	O
choice	O
of	O
features	O
to	O
use	O
with	O
linear	Method
function	Method
approximation	Method
.	O

We	O
ran	O
experiments	O
using	O
five	O
different	O
sets	O
of	O
features	O
,	O
which	O
we	O
now	O
briefly	O
explain	O
;	O
a	O
complete	O
description	O
of	O
these	O
feature	O
sets	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Of	O
these	O
sets	O
of	O
features	O
,	O
BASS	O
,	O
DISCO	Method
and	O
RAM	Method
were	O
originally	O
introduced	O
by	O
naddaf2010	O
,	O
while	O
the	O
rest	O
are	O
novel	O
.	O

paragraph	O
:	O
Basic	O
.	O

The	O
Basic	O
method	O
,	O
derived	O
from	O
naddaf2010	Method
’s	Method
BASS	Method
naddaf2010	Method
,	O
encodes	O
the	O
presence	O
of	O
colours	O
on	O
the	O
Atari	Material
2600	Material
screen	Material
.	O

The	O
Basic	O
method	O
first	O
removes	O
the	O
image	O
background	O
by	O
storing	O
the	O
frequency	O
of	O
colours	O
at	O
each	O
pixel	O
location	O
within	O
a	O
histogram	O
.	O

Each	O
game	Material
background	Material
is	O
precomputed	O
offline	O
,	O
using	O
18	O
,	O
000	O
observations	O
collected	O
from	O
sample	O
trajectories	O
.	O

The	O
sample	O
trajectories	O
are	O
generated	O
by	O
following	O
a	O
human	O
-	O
provided	O
trajectory	O
for	O
a	O
random	O
number	O
of	O
steps	O
and	O
subsequently	O
selecting	O
actions	O
uniformly	O
at	O
random	O
.	O

The	O
screen	O
is	O
then	O
divided	O
into	O
tiles	O
.	O

Basic	O
generates	O
one	O
binary	O
feature	O
for	O
each	O
of	O
the	O
colours	O
and	O
each	O
of	O
the	O
tiles	O
,	O
giving	O
a	O
total	O
of	O
28	O
,	O
672	O
features	O
.	O

paragraph	O
:	O
BASS	O
.	O

The	O
BASS	Method
method	Method
behaves	O
identically	O
to	O
the	O
Basic	O
method	O
save	O
in	O
two	O
respects	O
.	O

First	O
,	O
BASS	Method
augments	O
the	O
Basic	O
feature	O
set	O
with	O
pairwise	O
combinations	O
of	O
its	O
features	O
.	O

Second	O
,	O
BASS	Method
uses	O
a	O
smaller	O
,	O
8	Method
-	Method
colour	Method
encoding	Method
to	O
ensure	O
that	O
the	O
number	O
of	O
pairwise	O
combinations	O
remains	O
tractable	O
.	O

paragraph	O
:	O
DISCO	Method
.	O

The	O
DISCO	Method
method	Method
aims	O
to	O
detect	O
objects	O
within	O
the	O
Atari	Material
2600	Material
screen	Material
.	O

To	O
do	O
so	O
,	O
it	O
first	O
preprocesses	O
36	O
,	O
000	O
observations	O
from	O
sample	O
trajectories	O
generated	O
as	O
in	O
the	O
Basic	O
method	O
.	O

DISCO	Method
also	O
performs	O
the	O
background	Method
subtraction	Method
steps	Method
as	O
in	O
Basic	O
and	O
BASS	O
.	O

Extracted	O
objects	O
are	O
then	O
labelled	O
into	O
classes	O
.	O

During	O
the	O
actual	O
training	O
,	O
DISCO	Method
infers	O
the	O
class	O
label	O
of	O
detected	O
objects	O
and	O
encodes	O
their	O
position	O
and	O
velocity	O
using	O
tile	Method
coding	Method
.	O

paragraph	O
:	O
LSH	O
.	O

The	O
LSH	Method
method	Method
maps	O
raw	O
Atari	Material
2600	Material
screens	Material
into	O
a	O
small	O
set	O
of	O
binary	O
features	O
using	O
Locally	Method
Sensitive	Method
Hashing	Method
.	O

The	O
screens	O
are	O
mapped	O
using	O
random	O
projections	O
,	O
such	O
that	O
visually	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
generate	O
the	O
same	O
features	O
.	O

paragraph	O
:	O
RAM	O
.	O

The	O
RAM	Method
method	Method
works	O
on	O
an	O
entirely	O
different	O
observation	O
space	O
than	O
the	O
other	O
four	O
methods	O
.	O

Rather	O
than	O
receiving	O
in	O
Atari	Material
2600	Material
screen	Material
as	O
an	O
observation	O
,	O
it	O
directly	O
observes	O
the	O
Atari	O
2600	O
’s	O
1024	O
bits	O
of	O
memory	O
.	O

Each	O
bit	O
of	O
RAM	O
is	O
provided	O
as	O
a	O
binary	O
feature	O
together	O
with	O
the	O
pairwise	O
logical	O
-	O
AND	O
of	O
every	O
pair	O
of	O
bits	O
.	O

subsubsection	O
:	O
Evaluation	O
Methodology	O
We	O
first	O
constructed	O
two	O
sets	O
of	O
games	O
,	O
one	O
for	O
training	O
and	O
the	O
other	O
for	O
testing	O
.	O

We	O
used	O
the	O
training	O
games	O
for	O
parameter	Task
tuning	Task
as	O
well	O
as	O
design	Task
refinements	Task
,	O
and	O
the	O
testing	O
games	O
for	O
the	O
final	O
evaluation	O
of	O
our	O
methods	O
.	O

Our	O
training	O
set	O
consisted	O
of	O
five	O
games	O
:	O
Asterix	O
,	O
Beam	Task
Rider	Task
,	O
Freeway	Material
,	O
Seaquest	Material
and	O
Space	O
Invaders	O
.	O

The	O
parameter	Task
search	Task
involved	O
finding	O
suitable	O
values	O
for	O
the	O
parameters	O
to	O
the	O
SARSA	Method
(	Method
)	Method
algorithm	Method
,	O
i.e.	O
the	O
learning	Metric
rate	Metric
,	O
exploration	Metric
rate	Metric
,	O
discount	O
factor	O
,	O
and	O
the	O
decay	Metric
rate	Metric
.	O

We	O
also	O
searched	O
the	O
space	O
of	O
feature	O
generation	O
parameters	O
,	O
for	O
example	O
the	O
abstraction	O
level	O
for	O
the	O
BASS	Method
agent	Method
.	O

The	O
results	O
of	O
our	O
parameter	Method
search	Method
are	O
summarized	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Our	O
testing	O
set	O
was	O
constructed	O
by	O
choosing	O
semi	O
-	O
randomly	O
from	O
the	O
381	O
games	O
listed	O
on	O
Wikipedia	Material
at	O
the	O
time	O
of	O
writing	O
.	O

Of	O
these	O
games	O
,	O
123	O
games	O
have	O
their	O
own	O
Wikipedia	Material
page	Material
,	O
have	O
a	O
single	O
player	O
mode	O
,	O
are	O
not	O
adult	O
-	O
themed	O
or	O
prototypes	O
,	O
and	O
can	O
be	O
emulated	O
in	O
ALE	Method
.	O

From	O
this	O
list	O
,	O
50	O
games	O
were	O
chosen	O
at	O
random	O
to	O
form	O
the	O
test	O
set	O
.	O

Evaluation	O
of	O
each	O
method	O
on	O
each	O
game	O
was	O
performed	O
as	O
follows	O
.	O

An	O
episode	O
starts	O
on	O
the	O
frame	O
that	O
follows	O
the	O
reset	O
command	O
,	O
and	O
terminates	O
when	O
the	O
end	O
-	O
of	O
-	O
game	O
condition	O
is	O
detected	O
or	O
after	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
whichever	O
comes	O
first	O
.	O

During	O
an	O
episode	O
,	O
the	O
agent	O
acts	O
every	O
5	O
frames	O
,	O
or	O
equivalently	O
12	O
times	O
per	O
second	O
of	O
gameplay	O
.	O

A	O
reinforcement	Method
learning	Method
trial	Method
consists	O
of	O
5	O
,	O
000	O
training	O
episodes	O
,	O
followed	O
by	O
500	O
evaluation	O
episodes	O
during	O
which	O
no	O
learning	O
takes	O
place	O
.	O

The	O
agent	O
’s	O
performance	O
is	O
measured	O
as	O
the	O
average	Metric
score	Metric
achieved	O
during	O
the	O
evaluation	O
episodes	O
.	O

For	O
each	O
game	O
,	O
we	O
report	O
our	O
methods	O
’	O
average	O
performance	O
across	O
30	O
trials	O
.	O

For	O
purposes	O
of	O
comparison	O
,	O
we	O
also	O
provide	O
performance	O
measures	O
for	O
three	O
simple	O
baseline	O
agents	O
–	O
Random	O
,	O
Const	O
and	O
Perturb	Method
–	O
as	O
well	O
as	O
the	O
performance	O
of	O
a	O
non	O
-	O
expert	Method
human	Method
player	Method
.	O

The	O
Random	Method
agent	Method
picks	O
a	O
random	O
action	O
on	O
every	O
frame	O
.	O

The	O
Const	Method
agent	Method
selects	O
a	O
single	O
fixed	O
action	O
throughout	O
an	O
episode	O
;	O
our	O
results	O
reflect	O
the	O
highest	O
score	O
achieved	O
by	O
any	O
single	O
action	O
within	O
each	O
game	O
.	O

The	O
Perturb	Method
agent	Method
selects	O
a	O
fixed	O
action	O
with	O
probability	O
0.95	O
and	O
otherwise	O
acts	O
uniformly	O
randomly	O
;	O
for	O
each	O
game	O
,	O
we	O
report	O
the	O
performance	O
of	O
the	O
best	O
policy	O
of	O
this	O
type	O
.	O

Additionally	O
,	O
we	O
provide	O
human	O
player	O
results	O
that	O
report	O
the	O
five	O
-	O
episode	Metric
average	Metric
score	Metric
obtained	O
by	O
a	O
beginner	O
(	O
who	O
had	O
never	O
previously	O
played	O
Atari	O
2600	O
games	O
)	O
playing	O
selected	O
games	O
.	O

Our	O
aim	O
is	O
not	O
to	O
provide	O
exhaustive	O
or	O
accurate	O
human	Metric
-	Metric
level	Metric
benchmarks	Metric
,	O
which	O
would	O
be	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
,	O
but	O
rather	O
to	O
offer	O
insight	O
into	O
the	O
performance	O
level	O
achieved	O
by	O
our	O
agents	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
reinforcement	Method
learning	Method
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
a	O
small	O
subset	O
of	O
results	O
from	O
two	O
training	O
games	O
and	O
three	O
test	O
games	O
.	O

In	O
40	O
games	O
out	O
of	O
55	O
,	O
learning	Method
agents	Method
perform	O
better	O
than	O
the	O
baseline	O
agents	O
.	O

In	O
some	O
games	O
,	O
e.g.	O
,	O
Double	Task
Dunk	Task
,	O
Journey	Task
Escape	Task
and	O
Tennis	Task
,	O
the	O
no	Method
-	Method
action	Method
baseline	Method
policy	Method
performs	O
the	O
best	O
by	O
essentially	O
refusing	O
to	O
play	O
and	O
thus	O
incurring	O
no	O
negative	O
reward	O
.	O

Within	O
the	O
40	O
games	O
for	O
which	O
learning	Task
occurs	O
,	O
the	O
BASS	Method
method	Method
generally	O
performs	O
best	O
.	O

DISCO	Method
performed	O
particularly	O
poorly	O
compared	O
to	O
the	O
other	O
learning	Method
methods	Method
.	O

The	O
RAM	Method
-	Method
based	Method
agent	Method
,	O
surprisingly	O
,	O
did	O
not	O
outperform	O
image	Method
-	Method
based	Method
methods	Method
,	O
despite	O
building	O
its	O
representation	O
from	O
raw	O
game	O
state	O
.	O

It	O
appears	O
the	O
screen	Material
image	Material
carries	O
structural	O
information	O
that	O
is	O
not	O
easily	O
extracted	O
from	O
the	O
RAM	O
bits	O
.	O

Our	O
reinforcement	Method
learning	Method
results	O
show	O
that	O
while	O
some	O
learning	Task
progress	O
is	O
already	O
possible	O
in	O
Atari	Material
2600	Material
games	Material
,	O
much	O
more	O
work	O
remains	O
to	O
be	O
done	O
.	O

Different	O
methods	O
perform	O
well	O
on	O
different	O
games	O
,	O
and	O
no	O
single	O
method	O
performs	O
well	O
on	O
all	O
games	O
.	O

Some	O
games	O
are	O
particularly	O
challenging	O
.	O

For	O
example	O
,	O
platformers	O
such	O
as	O
Montezuma	Material
’s	Material
Revenge	Material
seem	O
to	O
require	O
high	O
-	O
level	O
planning	O
far	O
beyond	O
what	O
our	O
current	O
,	O
domain	Method
-	Method
independent	Method
methods	Method
provide	O
.	O

Tennis	Method
requires	O
fairly	O
elaborate	O
behaviour	O
before	O
observing	O
any	O
positive	O
reward	O
,	O
but	O
simple	O
behaviour	O
can	O
avoid	O
negative	O
rewards	O
.	O

Our	O
results	O
also	O
highlight	O
the	O
value	O
of	O
ALE	Method
as	O
an	O
experimental	O
methodology	O
.	O

For	O
example	O
,	O
the	O
DISCO	Method
approach	Method
performs	O
reasonably	O
well	O
on	O
the	O
training	Material
set	Material
,	O
but	O
suffers	O
a	O
dramatic	O
reduction	O
in	O
performance	O
when	O
applied	O
to	O
unseen	Material
games	Material
.	O

This	O
suggests	O
the	O
method	O
is	O
less	O
robust	O
than	O
the	O
other	O
methods	O
we	O
studied	O
.	O

After	O
a	O
quick	O
glance	O
at	O
the	O
full	O
table	O
of	O
results	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
it	O
is	O
clear	O
that	O
summarizing	O
results	O
across	O
such	O
varied	O
domains	O
needs	O
further	O
attention	O
;	O
we	O
explore	O
this	O
issue	O
further	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Planning	Task
The	O
Arcade	Method
Learning	Method
Environment	Method
can	O
naturally	O
be	O
used	O
to	O
study	O
planning	Method
techniques	Method
by	O
using	O
the	O
emulator	Method
itself	Method
as	O
a	O
generative	Method
model	Method
.	O

Initially	O
it	O
may	O
seem	O
that	O
allowing	O
the	O
agent	O
to	O
plan	O
into	O
the	O
future	O
with	O
a	O
perfect	O
model	O
trivializes	O
the	O
problem	O
.	O

However	O
,	O
this	O
is	O
not	O
the	O
case	O
:	O
the	O
size	O
of	O
state	O
space	O
in	O
Atari	Material
2600	Material
games	Material
prohibits	O
exhaustive	Method
search	Method
.	O

Eighteen	O
different	O
actions	O
are	O
available	O
at	O
every	O
frame	O
;	O
at	O
60	O
frames	O
per	O
second	O
,	O
looking	O
ahead	O
one	O
second	O
requires	O
simulation	O
steps	O
.	O

Furthermore	O
,	O
rewards	O
are	O
often	O
sparsely	O
distributed	O
,	O
which	O
causes	O
significant	O
horizon	O
effects	O
in	O
many	O
search	Method
algorithms	Method
.	O

subsubsection	Method
:	O
Search	Method
Methods	Method
We	O
now	O
provide	O
benchmark	O
ALE	Method
results	O
for	O
two	O
traditional	O
search	Method
methods	Method
.	O

Each	O
method	O
was	O
applied	O
online	O
to	O
select	O
an	O
action	O
at	O
every	O
time	O
step	O
(	O
every	O
five	O
frames	O
)	O
until	O
the	O
game	O
was	O
over	O
.	O

paragraph	O
:	O
Breadth	Method
-	Method
first	Method
Search	Method
.	O

Our	O
first	O
approach	O
builds	O
a	O
search	O
tree	O
in	O
a	O
breadth	O
-	O
first	O
fashion	O
until	O
a	O
node	O
limit	O
is	O
reached	O
.	O

Once	O
the	O
tree	O
is	O
expanded	O
,	O
node	O
values	O
are	O
updated	O
recursively	O
from	O
the	O
bottom	O
of	O
the	O
tree	O
to	O
the	O
root	O
.	O

The	O
agent	O
then	O
selects	O
the	O
action	O
corresponding	O
to	O
the	O
branch	O
with	O
the	O
highest	O
discounted	O
sum	O
of	O
rewards	O
.	O

Expanding	O
the	O
full	O
search	O
tree	O
requires	O
a	O
large	O
number	O
of	O
simulation	O
steps	O
.	O

For	O
instance	O
,	O
selecting	O
an	O
action	O
every	O
5	O
frames	O
and	O
allowing	O
a	O
maximum	O
of	O
100	O
,	O
000	O
simulation	O
steps	O
per	O
frame	O
,	O
the	O
agent	O
can	O
only	O
look	O
ahead	O
about	O
a	O
third	O
of	O
a	O
second	O
.	O

In	O
many	O
games	O
,	O
this	O
allows	O
the	O
agent	O
to	O
collect	O
immediate	O
rewards	O
and	O
avoid	O
death	O
but	O
little	O
else	O
.	O

For	O
example	O
,	O
in	O
Seaquest	O
the	O
agent	O
must	O
collect	O
a	O
swimmer	O
and	O
return	O
to	O
the	O
surface	O
before	O
running	O
out	O
of	O
air	O
,	O
which	O
involves	O
planning	O
far	O
beyond	O
one	O
second	O
.	O

paragraph	O
:	O
UCT	Method
:	O
Upper	Method
Confidence	Method
Bounds	Method
Applied	O
to	O
Trees	O
.	O

A	O
preferable	O
alternative	O
to	O
exhaustively	O
expanding	O
the	O
tree	O
is	O
to	O
simulate	O
deeper	O
into	O
the	O
more	O
promising	O
branches	O
.	O

To	O
do	O
this	O
,	O
we	O
need	O
to	O
find	O
a	O
balance	O
between	O
expanding	O
the	O
higher	O
-	O
valued	O
branches	O
and	O
spending	O
simulation	O
steps	O
on	O
the	O
lower	O
-	O
valued	O
branches	O
to	O
get	O
a	O
better	O
estimate	O
of	O
their	O
values	O
.	O

The	O
UCT	Method
algorithm	Method
,	O
developed	O
by	O
kocsis_06	Method
,	O
deals	O
with	O
the	O
exploration	Task
-	Task
exploitation	Task
dilemma	Task
by	O
treating	O
each	O
node	O
of	O
a	O
search	O
tree	O
as	O
a	O
multi	Task
-	Task
armed	Task
bandit	Task
problem	Task
.	O

UCT	Method
uses	O
a	O
variation	O
of	O
UCB1	Method
,	O
a	O
bandit	Method
algorithm	Method
,	O
to	O
choose	O
which	O
child	O
node	O
to	O
visit	O
next	O
.	O

A	O
common	O
practice	O
is	O
to	O
apply	O
a	O
-	O
step	O
random	Method
simulation	Method
at	O
the	O
end	O
of	O
each	O
leaf	O
node	O
to	O
obtain	O
an	O
estimate	O
from	O
a	O
longer	O
trajectory	O
.	O

By	O
expanding	O
the	O
more	O
valuable	O
branches	O
of	O
the	O
tree	O
and	O
carrying	O
out	O
a	O
random	O
simulation	O
at	O
the	O
leaf	O
nodes	O
,	O
UCT	Method
is	O
known	O
to	O
perform	O
well	O
in	O
many	O
different	O
settings	O
mcts_survery2012	Method
.	O

Our	O
UCT	Method
implementation	Method
was	O
entirely	O
standard	O
,	O
except	O
for	O
one	O
optimization	Task
.	O

Few	O
Atari	Method
games	Method
actually	O
distinguish	O
between	O
all	O
18	O
actions	O
at	O
every	O
time	O
step	O
.	O

In	O
Beam	Task
Rider	Task
,	O
for	O
example	O
,	O
the	O
down	O
action	O
does	O
nothing	O
,	O
and	O
pressing	O
the	O
button	O
when	O
a	O
bullet	O
has	O
already	O
been	O
shot	O
has	O
no	O
effect	O
.	O

We	O
exploit	O
this	O
fact	O
as	O
follows	O
:	O
after	O
expanding	O
the	O
children	O
of	O
a	O
node	O
in	O
the	O
search	O
tree	O
,	O
we	O
compare	O
the	O
resulting	O
emulator	O
states	O
.	O

Actions	O
that	O
result	O
in	O
the	O
same	O
state	O
are	O
treated	O
as	O
duplicates	O
and	O
only	O
one	O
of	O
the	O
actions	O
is	O
considered	O
in	O
the	O
search	O
tree	O
.	O

This	O
reduces	O
the	O
branching	O
factor	O
,	O
thus	O
allowing	O
deeper	Task
search	Task
.	O

At	O
every	O
step	O
,	O
we	O
also	O
reuse	O
the	O
part	O
of	O
our	O
search	O
tree	O
corresponding	O
to	O
the	O
selected	O
action	O
.	O

Pseudocode	O
for	O
our	O
implementation	O
of	O
the	O
UCT	Method
algorithm	Method
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Experimental	O
Setup	O
We	O
designed	O
and	O
tuned	O
our	O
algorithms	O
based	O
on	O
the	O
same	O
five	O
training	O
games	O
used	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
subsequently	O
evaluated	O
the	O
methods	O
on	O
the	O
fifty	O
games	O
of	O
the	O
testing	O
set	O
.	O

The	O
training	Material
games	Material
were	O
used	O
to	O
determine	O
the	O
length	O
of	O
the	O
search	O
horizon	O
as	O
well	O
as	O
the	O
constant	O
controlling	O
the	O
amount	O
of	O
exploration	O
at	O
internal	O
nodes	O
of	O
the	O
tree	O
.	O

Each	O
episode	O
was	O
set	O
to	O
last	O
up	O
to	O
5	O
minutes	O
of	O
real	O
-	O
time	O
play	O
(	O
18	O
,	O
000	O
frames	O
)	O
,	O
with	O
actions	O
selected	O
every	O
5	O
frames	O
,	O
matching	O
our	O
settings	O
in	O
Section	O
[	O
reference	O
]	O
.	O

On	O
average	O
,	O
each	O
action	Method
selection	Method
step	O
took	O
on	O
the	O
order	O
of	O
15	O
seconds	O
.	O

We	O
also	O
used	O
the	O
same	O
discount	O
factor	O
as	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
ran	O
our	O
algorithms	O
for	O
10	O
episodes	O
per	O
game	O
.	O

Details	O
of	O
the	O
algorithmic	O
parameters	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

subsubsection	O
:	O
Results	O
A	O
complete	O
report	O
of	O
our	O
search	O
results	O
is	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
results	O
on	O
a	O
selected	O
subset	O
of	O
games	O
.	O

For	O
reference	O
purposes	O
,	O
we	O
also	O
include	O
the	O
performance	O
of	O
the	O
best	O
learning	Method
agent	Method
and	O
the	O
best	O
baseline	O
policy	O
from	O
Table	O
[	O
reference	O
]	O
.	O

Together	O
,	O
our	O
two	O
search	Method
methods	Method
performed	O
better	O
than	O
both	O
learning	Method
agents	Method
and	O
the	O
baseline	Method
policies	Method
on	O
49	O
of	O
55	O
games	O
.	O

In	O
most	O
cases	O
,	O
UCT	Method
performs	O
significantly	O
better	O
than	O
breadth	Method
-	Method
first	Method
search	Method
.	O

Four	O
of	O
the	O
six	O
games	O
for	O
which	O
search	Method
methods	Method
do	O
not	O
perform	O
best	O
are	O
games	O
where	O
rewards	O
are	O
sparse	O
and	O
require	O
long	O
-	O
term	O
planning	O
.	O

These	O
are	O
Freeway	O
,	O
Private	O
Eye	O
,	O
Montezuma	O
’s	O
Revenge	O
and	O
Venture	O
.	O

section	O
:	O
Evaluation	Metric
Metrics	Metric
for	O
General	O
Atari	Task
2600	Task
Agents	Task
Applying	O
algorithms	O
to	O
a	O
large	O
set	O
of	O
games	O
as	O
we	O
did	O
in	O
Sections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
presents	O
difficulties	O
when	O
interpreting	O
the	O
results	O
.	O

While	O
the	O
agent	O
’s	O
goal	O
in	O
all	O
games	O
is	O
to	O
maximize	O
its	O
score	O
,	O
scores	O
for	O
two	O
different	O
games	O
can	O
not	O
be	O
easily	O
compared	O
.	O

Each	O
game	O
uses	O
its	O
own	O
scale	O
for	O
scores	O
,	O
and	O
different	O
game	O
mechanics	O
make	O
some	O
games	O
harder	O
to	O
learn	O
than	O
others	O
.	O

The	O
challenges	O
associated	O
with	O
comparing	O
general	Method
agents	Method
has	O
been	O
previously	O
highlighted	O
by	O
whiteson11	O
.	O

Although	O
we	O
can	O
always	O
report	O
full	O
performance	O
tables	O
,	O
as	O
we	O
did	O
in	O
Appendix	O
[	O
reference	O
]	O
,	O
some	O
more	O
compact	O
summary	Metric
statistics	Metric
are	O
also	O
desirable	O
.	O

We	O
now	O
introduce	O
some	O
simple	O
metrics	O
that	O
help	O
compare	O
agents	O
across	O
a	O
diverse	O
set	O
of	O
domains	O
,	O
such	O
as	O
our	O
test	O
set	O
of	O
Atari	Material
2600	Material
games	Material
.	O

subsection	O
:	O
Normalized	Metric
Scores	Metric
Consider	O
the	O
scores	O
and	O
achieved	O
by	O
two	O
algorithms	O
in	O
game	O
.	O

Our	O
goal	O
here	O
is	O
to	O
explore	O
methods	O
that	O
allow	O
us	O
to	O
compare	O
two	O
sets	O
of	O
scores	O
and	O
.	O

The	O
approach	O
we	O
take	O
is	O
to	O
transform	O
into	O
a	O
normalized	O
score	O
with	O
the	O
aim	O
of	O
comparing	O
normalized	O
scores	O
across	O
games	O
;	O
in	O
the	O
ideal	O
case	O
,	O
implies	O
that	O
algorithm	O
performs	O
as	O
well	O
on	O
game	O
as	O
on	O
game	O
.	O

In	O
order	O
to	O
compare	O
algorithms	O
over	O
a	O
set	O
of	O
games	O
,	O
we	O
aggregate	O
normalized	O
scores	O
for	O
each	O
game	O
and	O
each	O
algorithm	O
.	O

The	O
most	O
natural	O
way	O
to	O
compare	O
games	O
with	O
different	O
scoring	O
scales	O
is	O
to	O
normalize	O
scores	O
so	O
that	O
the	O
numerical	O
values	O
become	O
comparable	O
.	O

All	O
of	O
our	O
normalization	Method
methods	Method
are	O
defined	O
using	O
the	O
notion	O
of	O
a	O
score	O
range	O
computed	O
for	O
each	O
game	O
.	O

Given	O
such	O
a	O
score	O
range	O
,	O
score	O
is	O
normalized	O
by	O
computing	O
.	O

subsubsection	Method
:	O
Normalization	Method
to	O
a	O
Reference	O
Score	O
One	O
straightforward	O
method	O
is	O
to	O
normalize	O
to	O
a	O
score	O
range	O
defined	O
by	O
repeated	O
runs	O
of	O
a	O
random	O
agent	O
across	O
each	O
game	O
.	O

Here	O
,	O
is	O
the	O
absolute	O
value	O
of	O
the	O
average	Metric
score	Metric
achieved	O
by	O
the	O
random	Method
agent	Method
,	O
and	O
.	O

Figure	O
[	O
reference	O
]	O
a	O
depicts	O
the	O
random	Metric
-	Metric
normalized	Metric
scores	Metric
achieved	O
by	O
BASS	Method
and	O
RAM	Method
on	O
three	O
games	O
.	O

Two	O
issues	O
arise	O
with	O
this	O
approach	O
:	O
the	O
scale	O
of	O
normalized	O
scores	O
may	O
be	O
excessively	O
large	O
and	O
normalized	O
scores	O
are	O
generally	O
not	O
translation	O
invariant	O
.	O

The	O
issue	O
of	O
scale	O
is	O
best	O
seen	O
in	O
a	O
game	O
such	O
as	O
Freeway	Material
,	O
for	O
which	O
the	O
random	Method
agent	Method
achieves	O
a	O
score	O
close	O
to	O
0	O
:	O
scores	O
achieved	O
by	O
learning	Method
agents	Method
,	O
in	O
the	O
10	O
-	O
20	O
range	O
,	O
are	O
normalized	O
into	O
thousands	O
.	O

By	O
contrast	O
,	O
no	O
learning	Method
agent	Method
achieves	O
a	O
random	Metric
-	Metric
normalized	Metric
score	Metric
greater	O
than	O
1	O
in	O
Asteroids	O
.	O

subsubsection	Method
:	O
Normalizing	O
to	O
a	O
Baseline	O
Set	O
Rather	O
than	O
normalizing	O
to	O
a	O
single	O
reference	O
we	O
may	O
normalize	O
to	O
the	O
score	O
range	O
implied	O
by	O
a	O
set	O
of	O
references	O
.	O

Let	O
be	O
a	O
set	O
of	O
reference	O
scores	O
.	O

A	O
method	O
’s	O
baseline	Metric
score	Metric
is	O
computed	O
using	O
the	O
score	O
range	O
.	O

Given	O
a	O
sufficiently	O
rich	O
set	O
of	O
reference	O
scores	O
,	O
baseline	Method
normalization	Method
allows	O
us	O
to	O
reduce	O
the	O
scores	O
for	O
most	O
games	O
to	O
comparable	O
quantities	O
,	O
and	O
lets	O
us	O
know	O
whether	O
meaningful	O
performance	O
was	O
obtained	O
.	O

Figure	O
[	O
reference	O
]	O
b	O
shows	O
example	O
baseline	O
scores	O
.	O

The	O
score	O
range	O
for	O
these	O
scores	O
corresponds	O
to	O
the	O
scores	O
achieved	O
by	O
37	O
baseline	O
agents	O
(	O
Section	O
[	O
reference	O
]	O
)	O
:	O
Random	O
,	O
Const	O
(	O
one	O
policy	O
per	O
action	O
)	O
,	O
and	O
Perturb	O
(	O
one	O
policy	O
per	O
action	O
)	O
.	O

A	O
natural	O
idea	O
is	O
to	O
also	O
include	O
scores	O
achieved	O
by	O
human	O
players	O
into	O
the	O
baseline	O
set	O
.	O

For	O
example	O
,	O
one	O
may	O
include	O
the	O
score	O
achieved	O
by	O
an	O
expert	O
as	O
well	O
as	O
the	O
score	O
achieved	O
by	O
a	O
beginner	O
.	O

However	O
,	O
using	O
human	O
scores	O
raises	O
its	O
own	O
set	O
of	O
issues	O
.	O

For	O
example	O
,	O
humans	O
often	O
play	O
games	O
without	O
seeking	O
to	O
maximize	O
score	O
;	O
humans	O
also	O
benefit	O
from	O
prior	O
knowledge	O
that	O
is	O
difficult	O
to	O
incorporate	O
into	O
domain	Method
-	Method
independent	Method
agents	Method
.	O

subsubsection	Method
:	O
Inter	Method
-	Method
Algorithm	Method
Normalization	Method
A	O
third	O
alternative	O
is	O
to	O
normalize	O
using	O
the	O
scores	O
achieved	O
by	O
the	O
algorithms	O
themselves	O
.	O

Given	O
algorithms	O
,	O
each	O
achieving	O
score	O
on	O
game	O
,	O
we	O
define	O
the	O
inter	Metric
-	Metric
algorithm	Metric
score	Metric
using	O
the	O
score	O
range	O
.	O

By	O
definition	O
,	O
.	O

A	O
special	O
case	O
of	O
this	O
is	O
when	O
n=2	O
,	O
where	O
indicates	O
which	O
algorithm	O
is	O
better	O
than	O
the	O
other	O
.	O

Figure	O
[	O
reference	O
]	O
c	O
shows	O
example	O
inter	O
-	O
algorithm	O
scores	O
;	O
the	O
relevant	O
score	O
ranges	O
are	O
constructed	O
from	O
the	O
performance	O
of	O
all	O
five	O
learning	Method
agents	Method
.	O

Because	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
are	O
bounded	O
,	O
this	O
type	O
of	O
normalization	Method
is	O
an	O
appealing	O
solution	O
to	O
compare	O
the	O
relative	O
performance	O
of	O
different	O
methods	O
.	O

Its	O
main	O
drawback	O
is	O
that	O
it	O
gives	O
no	O
indication	O
of	O
the	O
objective	O
performance	O
of	O
the	O
best	O
algorithm	O
.	O

A	O
good	O
example	O
of	O
this	O
is	O
Venture	O
:	O
the	O
inter	O
-	O
algorithm	Metric
score	Metric
of	O
1.0	O
achieved	O
by	O
BASS	Method
does	O
not	O
reflect	O
the	O
fact	O
that	O
none	O
of	O
our	O
agents	O
achieved	O
a	O
score	O
remotely	O
comparable	O
to	O
a	O
human	O
’s	O
performance	O
.	O

The	O
lack	O
of	O
objective	O
reference	O
in	O
inter	O
-	O
algorithm	Method
normalization	Method
suggests	O
that	O
it	O
should	O
be	O
used	O
to	O
complement	O
other	O
scoring	Metric
metrics	Metric
.	O

subsection	O
:	O
Aggregating	O
Scores	O
Once	O
normalized	O
scores	O
are	O
obtained	O
for	O
each	O
game	O
,	O
the	O
next	O
step	O
is	O
to	O
produce	O
a	O
measure	O
that	O
reflects	O
how	O
well	O
each	O
agent	O
performs	O
across	O
the	O
set	O
of	O
games	O
.	O

As	O
illustrated	O
by	O
Table	O
[	O
reference	O
]	O
,	O
a	O
large	O
table	O
of	O
numbers	O
does	O
not	O
easily	O
permit	O
comparison	O
between	O
algorithms	O
.	O

We	O
now	O
describe	O
three	O
methods	O
to	O
aggregate	O
normalized	O
scores	O
.	O

subsubsection	Method
:	O
Average	Metric
Score	Metric
The	O
most	O
straightforward	O
method	O
of	O
aggregating	O
normalized	O
scores	O
is	O
to	O
compute	O
their	O
average	O
.	O

Without	O
perfect	Method
score	Method
normalization	Method
,	O
however	O
,	O
score	Metric
averages	Metric
tend	O
to	O
be	O
heavily	O
influenced	O
by	O
games	O
such	O
as	O
Zaxxon	O
for	O
which	O
baseline	O
scores	O
are	O
high	O
.	O

Averaging	Metric
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
obviates	O
this	O
issue	O
as	O
all	O
scores	O
are	O
bounded	O
between	O
0	O
and	O
1	O
.	O

Figure	O
[	O
reference	O
]	O
displays	O
average	O
baseline	O
and	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
for	O
our	O
learning	Method
agents	Method
.	O

subsubsection	O
:	O
Median	Method
Score	Method
Median	Metric
scores	Metric
are	O
generally	O
more	O
robust	O
to	O
outliers	O
than	O
average	O
scores	O
.	O

The	O
median	O
is	O
obtained	O
by	O
sorting	O
all	O
normalized	O
scores	O
and	O
selecting	O
the	O
middle	O
element	O
(	O
the	O
average	O
of	O
the	O
two	O
middle	O
elements	O
is	O
used	O
if	O
the	O
number	O
of	O
scores	O
is	O
even	O
)	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
median	O
baseline	O
and	O
inter	Metric
-	Metric
algorithm	Metric
scores	Metric
for	O
our	O
learning	Method
agents	Method
.	O

Comparing	O
medians	Metric
and	O
averages	O
in	O
the	O
baseline	Metric
score	Metric
(	O
upper	O
two	O
graphs	O
)	O
illustrates	O
exactly	O
the	O
outlier	Metric
sensitivity	Metric
of	O
the	O
average	Metric
score	Metric
,	O
where	O
the	O
LSH	Method
method	Method
appears	O
dramatically	O
superior	O
due	O
entirely	O
to	O
its	O
performance	O
in	O
Zaxxon	Task
.	O

subsubsection	O
:	O
Score	Method
Distribution	Method
The	O
score	Method
distribution	Method
aggregate	Method
is	O
a	O
natural	O
generalization	O
of	O
the	O
median	Metric
score	Metric
:	O
it	O
shows	O
the	O
fraction	O
of	O
games	O
on	O
which	O
an	O
algorithm	O
achieves	O
a	O
certain	O
normalized	O
score	O
or	O
better	O
.	O

It	O
is	O
essentially	O
a	O
quantile	Method
plot	Method
or	O
inverse	Method
empirical	Method
CDF	Method
.	O

Unlike	O
the	O
average	Metric
and	Metric
median	Metric
scores	Metric
,	O
the	O
score	Metric
distribution	Metric
accurately	O
represents	O
the	O
performance	O
of	O
an	O
agent	O
irrespective	O
of	O
how	O
individual	O
scores	O
are	O
distributed	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
baseline	O
and	O
inter	O
-	O
algorithm	Metric
score	Metric
distributions	Metric
.	O

Score	O
distributions	O
allow	O
us	O
to	O
compare	O
different	O
algorithms	O
at	O
a	O
glance	O
–	O
if	O
one	O
curve	O
is	O
above	O
another	O
,	O
the	O
corresponding	O
method	O
generally	O
obtains	O
higher	O
scores	O
.	O

Using	O
the	O
baseline	Metric
score	Metric
distribution	Metric
,	O
we	O
can	O
easily	O
determine	O
the	O
proportion	O
of	O
games	O
for	O
which	O
methods	O
perform	O
better	O
than	O
the	O
baseline	O
policies	O
(	O
scores	O
above	O
1	O
)	O
.	O

The	O
inter	Metric
-	Metric
algorithm	Metric
score	Metric
distribution	Metric
,	O
on	O
the	O
other	O
hand	O
,	O
effectively	O
conveys	O
the	O
relative	O
performance	O
of	O
each	O
method	O
.	O

In	O
particular	O
,	O
it	O
allows	O
us	O
to	O
conclude	O
that	O
BASS	Method
performs	O
slightly	O
better	O
than	O
Basic	Method
and	O
RAM	Method
,	O
and	O
that	O
DISCO	Method
performs	O
significantly	O
worse	O
than	O
the	O
other	O
methods	O
.	O

subsection	O
:	O
Paired	O
Tests	O
An	O
alternate	O
evaluation	Metric
metric	Metric
,	O
especially	O
useful	O
when	O
comparing	O
only	O
a	O
few	O
algorithms	O
,	O
is	O
to	O
perform	O
paired	O
tests	O
over	O
the	O
raw	O
scores	O
.	O

For	O
each	O
game	O
,	O
we	O
performed	O
a	O
two	O
-	O
tailed	O
Welsh	O
’s	O
-	O
test	O
with	O
99	O
%	O
confidence	O
intervals	O
to	O
determine	O
whether	O
one	O
algorithm	O
’s	O
score	O
was	O
statistically	O
different	O
than	O
the	O
other	O
’s	O
.	O

Table	O
[	O
reference	O
]	O
provides	O
,	O
for	O
each	O
pair	O
of	O
algorithms	O
,	O
the	O
number	O
of	O
games	O
for	O
which	O
one	O
algorithm	O
performs	O
statistically	O
better	O
or	O
worse	O
than	O
the	O
other	O
.	O

Because	O
of	O
their	O
ternary	O
nature	O
,	O
paired	Method
tests	Method
tend	O
to	O
magnify	O
small	O
but	O
significant	O
differences	O
in	O
scores	O
.	O

section	O
:	O
Related	O
Work	O
We	O
now	O
briefly	O
survey	O
recent	O
research	O
related	O
to	O
Atari	Material
2600	Material
games	Material
and	O
some	O
prior	O
work	O
on	O
the	O
construction	O
of	O
empirical	Metric
benchmarks	Metric
for	O
measuring	Task
general	Task
competency	Task
.	O

subsection	O
:	O
Atari	Task
Games	Task
There	O
has	O
been	O
some	O
attention	O
devoted	O
to	O
Atari	Task
2600	Task
game	Task
playing	Task
within	O
the	O
reinforcement	Task
learning	Task
community	Task
.	O

For	O
the	O
most	O
part	O
,	O
prior	O
work	O
has	O
focused	O
on	O
the	O
challenge	O
of	O
finding	O
good	O
state	O
features	O
for	O
this	O
domain	O
.	O

diuk2008	O
applied	O
their	O
DOORMAX	Method
algorithm	Method
to	O
a	O
restricted	O
version	O
of	O
the	O
game	Task
of	Task
Pitfall	Task
!	Task

.	O

Their	O
method	O
extracts	O
objects	O
from	O
the	O
displayed	Material
image	Material
with	O
game	Task
-	Task
specific	Task
object	Task
detection	Task
.	O

These	O
objects	O
are	O
then	O
converted	O
into	O
a	O
first	Method
-	Method
order	Method
logic	Method
representation	Method
of	Method
the	Method
world	Method
,	O
the	O
Object	Method
-	Method
Oriented	Method
Markov	Method
Decision	Method
Process	Method
(	O
OO	Method
-	Method
MDP	Method
)	O
.	O

Their	O
results	O
show	O
that	O
DOORMAX	Method
can	O
discover	O
the	O
optimal	O
behaviour	O
for	O
this	O
OO	Task
-	Task
MDP	Task
within	O
one	O
episode	O
.	O

wintermute2010	O
proposed	O
a	O
method	O
that	O
also	O
extracts	O
objects	O
from	O
the	O
displayed	Material
image	Material
and	O
embeds	O
them	O
into	O
a	O
logic	Method
-	Method
based	Method
architecture	Method
,	O
SOAR	Method
.	O

Their	O
method	O
uses	O
a	O
forward	Method
model	Method
of	O
the	O
scene	O
to	O
improve	O
the	O
performance	O
of	O
the	O
Q	Method
-	Method
Learning	Method
algorithm	Method
.	O

They	O
showed	O
that	O
by	O
using	O
such	O
a	O
model	O
,	O
a	O
reinforcement	Method
learning	Method
agent	Method
could	O
learn	O
to	O
play	O
a	O
restricted	O
version	O
of	O
the	O
game	O
of	O
Frogger	Material
.	O

cobo2011	Method
investigated	O
automatic	Task
feature	Task
discovery	Task
in	O
the	O
games	O
of	O
Pong	Task
and	O
Frogger	Task
,	O
using	O
their	O
own	O
simulator	O
.	O

Their	O
proposed	O
method	O
takes	O
advantage	O
of	O
human	O
trajectories	O
to	O
identify	O
state	O
features	O
that	O
are	O
important	O
for	O
playing	Task
console	Task
games	Task
.	O

Recently	O
,	O
hausknecht_12	O
proposed	O
HyperNEAT	Method
-	Method
GGP	Method
,	O
an	O
evolutionary	Method
approach	Method
for	O
finding	Task
policies	Task
to	O
play	O
Atari	Task
2600	Task
games	Task
.	O

Although	O
HyperNEAT	Method
-	Method
GGP	Method
is	O
presented	O
as	O
a	O
general	O
game	Method
playing	Method
approach	Method
,	O
it	O
is	O
currently	O
difficult	O
to	O
assess	O
its	O
general	O
performance	O
as	O
the	O
reported	O
results	O
were	O
limited	O
to	O
only	O
two	O
games	O
.	O

Finally	O
,	O
some	O
of	O
the	O
authors	O
of	O
this	O
paper	O
recently	O
presented	O
a	O
domain	Method
-	Method
independent	Method
feature	Method
generation	Method
technique	Method
that	O
attempts	O
to	O
focus	O
its	O
effort	O
around	O
the	O
location	O
of	O
the	O
player	O
avatar	O
.	O

This	O
work	O
used	O
the	O
evaluation	O
methodology	O
advocated	O
here	O
and	O
is	O
the	O
only	O
one	O
to	O
demonstrate	O
the	O
technique	O
across	O
a	O
large	O
set	O
of	O
testing	O
games	O
.	O

subsection	O
:	O
Evaluation	Method
Frameworks	Method
for	O
General	Task
Agents	Task
Although	O
the	O
idea	O
of	O
using	O
games	Method
to	O
evaluate	O
the	O
performance	Task
of	Task
agents	Task
has	O
a	O
long	O
history	O
in	O
artificial	Task
intelligence	Task
,	O
it	O
is	O
only	O
more	O
recently	O
that	O
an	O
emphasis	O
on	O
generality	O
has	O
assumed	O
a	O
more	O
prominent	O
role	O
.	O

pell93strategy	Method
advocated	O
the	O
design	O
of	O
agents	O
that	O
,	O
given	O
an	O
abstract	O
description	O
of	O
a	O
game	O
,	O
could	O
automatically	O
play	O
them	O
.	O

His	O
work	O
strongly	O
influenced	O
the	O
design	O
of	O
the	O
now	O
annual	O
General	Task
Game	Task
Playing	Task
competition	Task
.	O

Our	O
framework	O
differs	O
in	O
that	O
we	O
do	O
not	O
assume	O
to	O
have	O
access	O
to	O
a	O
compact	O
logical	O
description	O
of	O
the	O
game	O
semantics	O
.	O

schaul11	O
also	O
recently	O
presented	O
an	O
interesting	O
proposal	O
for	O
using	O
games	Method
to	O
measure	O
the	O
general	O
capabilities	O
of	O
an	O
agent	O
.	O

whiteson11	O
discuss	O
a	O
number	O
of	O
challenges	O
in	O
designing	O
empirical	Task
tests	Task
to	O
measure	O
general	Task
reinforcement	Task
learning	Task
performance	O
;	O
this	O
work	O
can	O
be	O
seen	O
as	O
attempting	O
to	O
address	O
their	O
important	O
concerns	O
.	O

Starting	O
in	O
2004	O
as	O
a	O
conference	O
workshop	O
,	O
the	O
Reinforcement	Task
Learning	Task
competition	Task
was	O
held	O
until	O
2009	O
(	O
a	O
new	O
iteration	O
of	O
the	O
competition	O
has	O
been	O
announced	O
for	O
2013	O
)	O
.	O

Each	O
year	O
new	O
domains	O
are	O
proposed	O
,	O
including	O
standard	O
RL	Method
benchmarks	Method
,	O
Tetris	Method
,	O
and	O
Infinite	Material
Mario	Material
.	O

In	O
a	O
typical	O
competition	Material
domain	Material
,	O
the	O
agent	O
’s	O
state	O
information	O
is	O
summarized	O
through	O
a	O
series	O
of	O
high	O
-	O
level	O
state	O
variables	O
rather	O
than	O
direct	O
sensory	O
information	O
.	O

Infinite	Task
Mario	Task
,	O
for	O
example	O
,	O
provides	O
the	O
agent	O
with	O
an	O
object	O
-	O
oriented	O
observation	O
space	O
.	O

In	O
the	O
past	O
,	O
organizers	O
have	O
provided	O
a	O
special	O
‘	O
Polyathlon	O
’	O
track	O
in	O
which	O
agents	O
must	O
behave	O
in	O
a	O
medley	O
of	O
continuous	O
-	O
observation	O
,	O
discrete	O
-	O
action	O
domains	O
.	O

Another	O
longstanding	O
competition	O
,	O
the	O
International	Task
Planning	Task
Competition	Task
(	O
IPC	Task
)	O
,	O
has	O
been	O
organized	O
since	O
1998	O
,	O
and	O
aims	O
to	O
“	O
produce	O
new	O
benchmarks	O
,	O
and	O
to	O
gather	O
and	O
disseminate	O
data	O
about	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
”	O
.	O

The	O
IPC	Method
is	O
composed	O
of	O
different	O
tracks	O
corresponding	O
to	O
different	O
types	O
of	O
planning	Task
problems	Task
,	O
including	O
factory	Task
optimization	Task
,	O
elevator	Task
control	Task
and	O
agent	Task
coordination	Task
.	O

For	O
example	O
,	O
one	O
of	O
the	O
problems	O
in	O
the	O
2011	O
competition	O
consists	O
in	O
coordinating	O
a	O
set	O
of	O
robots	O
around	O
a	O
two	O
-	O
dimensional	O
gridworld	O
so	O
that	O
every	O
tile	O
is	O
painted	O
with	O
a	O
specific	O
colour	O
.	O

Domains	Material
are	O
described	O
using	O
either	O
relational	Method
reinforcement	Method
learning	Method
,	O
yielding	O
parametrized	Method
Markov	Method
Decision	Method
Processes	Method
(	O
MDPs	Method
)	O
and	O
Partially	Method
Observable	Method
MDPs	Method
,	O
or	O
using	O
logic	O
predicates	O
,	O
e.g.	O
in	O
STRIPS	O
notation	O
.	O

One	O
indication	O
of	O
how	O
much	O
these	O
competitions	O
value	O
domain	O
variety	O
can	O
be	O
seen	O
in	O
the	O
time	O
spent	O
on	O
finding	O
a	O
good	O
specification	Method
language	Method
.	O

The	O
2008	O
-	O
2009	O
RL	Material
competitions	Material
,	O
for	O
example	O
,	O
used	O
RL	Method
-	Method
Glue	Method
specifically	O
for	O
this	O
purpose	O
;	O
the	O
2011	O
planning	O
under	O
uncertainty	O
track	O
of	O
the	O
IPC	O
similar	O
employed	O
the	O
Relation	Method
Dynamic	Method
Influence	Method
Diagram	Method
Language	Method
.	O

While	O
competitions	O
seek	O
to	O
spur	O
new	O
research	O
and	O
evaluate	O
existing	O
algorithms	O
through	O
a	O
standardized	O
set	O
of	O
benchmarks	O
,	O
they	O
are	O
not	O
independently	O
developed	O
,	O
in	O
the	O
sense	O
that	O
the	O
vast	O
majority	O
of	O
domains	O
are	O
provided	O
by	O
the	O
research	O
community	O
.	O

Thus	O
a	O
typical	O
competition	O
domain	O
reflects	O
existing	O
research	O
directions	O
:	O
Mountain	Material
Car	Material
and	O
Acrobot	Material
remain	O
staples	O
of	O
the	O
RL	Task
competition	Task
.	O

These	O
competitions	O
also	O
focus	O
their	O
research	O
effort	O
on	O
domains	O
that	O
provide	O
high	O
-	O
level	O
state	O
variables	O
,	O
for	O
example	O
the	O
location	Task
of	Task
robots	Task
in	O
the	O
floor	Task
-	Task
painting	Task
domain	Task
described	O
above	O
.	O

By	O
contrast	O
,	O
the	O
Arcade	Method
Learning	Method
Environment	Method
and	O
the	O
domain	O
-	O
independent	O
setting	O
force	O
us	O
to	O
consider	O
the	O
question	O
of	O
perceptual	Task
grounding	Task
:	O
how	O
to	O
extract	O
meaningful	O
state	O
information	O
from	O
raw	O
game	O
screens	O
(	O
or	O
RAM	O
information	O
)	O
.	O

In	O
turn	O
,	O
this	O
emphasizes	O
the	O
design	O
of	O
algorithms	O
that	O
can	O
be	O
applied	O
to	O
sensor	Material
-	Material
rich	Material
domains	Material
without	O
significant	O
expert	O
knowledge	O
.	O

There	O
have	O
also	O
been	O
a	O
number	O
of	O
attempts	O
to	O
define	O
formal	O
agent	Metric
performance	Metric
metrics	Metric
based	O
on	O
algorithmic	Method
information	Method
theory	Method
.	O

The	O
first	O
such	O
attempts	O
were	O
due	O
to	O
Hernandez	O
-	O
orallo98aformal	O
and	O
to	O
DoweHajek98	O
.	O

More	O
recently	O
,	O
the	O
approaches	O
of	O
Hern10	Method
and	O
of	O
legg11	Method
appear	O
to	O
have	O
some	O
potential	O
.	O

Although	O
these	O
frameworks	O
are	O
general	O
and	O
conceptually	O
clean	O
,	O
the	O
key	O
challenge	O
remains	O
how	O
to	O
specify	O
sufficiently	O
interesting	O
classes	O
of	O
environments	O
.	O

In	O
our	O
opinion	O
,	O
much	O
more	O
work	O
is	O
required	O
before	O
these	O
approaches	O
can	O
claim	O
to	O
rival	O
the	O
practicality	O
of	O
using	O
a	O
large	O
set	O
of	O
existing	O
human	Material
-	Material
designed	Material
environments	Material
for	O
agent	Task
evaluation	Task
.	O

section	O
:	O
Final	O
Remarks	O
The	O
Atari	Material
2600	Material
games	Material
were	O
developed	O
for	O
humans	O
and	O
as	O
such	O
exhibit	O
many	O
idiosyncrasies	O
that	O
make	O
them	O
both	O
challenging	O
and	O
exciting	O
.	O

Consider	O
,	O
for	O
example	O
,	O
the	O
game	O
Pong	O
.	O

Pong	Method
has	O
been	O
studied	O
in	O
a	O
variety	O
of	O
contexts	O
as	O
an	O
interesting	O
reinforcement	Task
learning	Task
domain	Task
.	O

The	O
Atari	Method
2600	Method
Pong	Method
,	O
however	O
,	O
is	O
significantly	O
more	O
complex	O
than	O
Pong	O
domains	O
developed	O
for	O
research	O
.	O

Games	Material
can	O
easily	O
last	O
10	O
,	O
000	O
time	O
steps	O
(	O
compared	O
to	O
200–1000	O
in	O
other	O
domains	O
)	O
;	O
observations	O
are	O
composed	O
of	O
7	Material
-	Material
bit	Material
images	Material
(	O
compared	O
to	O
black	Material
and	Material
white	Material
images	Material
in	O
the	O
work	O
of	O
stober08pixels	O
,	O
or	O
5	O
-	O
6	O
input	O
features	O
elsewhere	O
)	O
;	O
observations	O
are	O
also	O
more	O
complex	O
,	O
containing	O
the	O
two	O
players	O
’	O
score	O
and	O
side	O
walls	O
.	O

In	O
sheer	O
size	O
,	O
the	O
Atari	Material
2600	Material
Pong	Material
is	O
thus	O
a	O
larger	O
domain	O
.	O

Its	O
dynamics	O
are	O
also	O
more	O
complicated	O
.	O

In	O
research	O
implementations	O
of	O
Pong	Task
object	Task
motion	Task
is	O
implemented	O
using	O
first	Method
-	Method
order	Method
mechanics	Method
.	O

However	O
,	O
in	O
Atari	Task
2600	Task
Pong	Task
paddle	Task
control	Task
is	O
nonlinear	O
:	O
simple	O
experimentation	O
shows	O
that	O
fully	O
predicting	O
the	O
player	O
’s	O
paddle	O
requires	O
knowledge	O
of	O
the	O
last	O
18	O
actions	O
.	O

As	O
with	O
many	O
other	O
Atari	Task
games	Task
,	O
the	O
player	O
paddle	O
also	O
moves	O
every	O
other	O
frame	O
,	O
adding	O
a	O
degree	O
of	O
temporal	O
aliasing	O
to	O
the	O
domain	O
.	O

While	O
Atari	O
2600	O
Pong	O
may	O
appear	O
unnecessarily	O
contrived	O
,	O
it	O
in	O
fact	O
reflects	O
the	O
unexpected	O
complexity	O
of	O
the	O
problems	O
with	O
which	O
humans	O
are	O
faced	O
.	O

Most	O
,	O
if	O
not	O
all	O
Atari	Material
2600	Material
games	Material
are	O
subject	O
to	O
similar	O
programming	O
artifacts	O
:	O
in	O
Space	O
Invaders	O
,	O
for	O
example	O
,	O
the	O
invaders	O
’	O
velocity	O
increases	O
nonlinearly	O
with	O
the	O
number	O
of	O
remaining	O
invaders	O
.	O

In	O
this	O
way	O
the	O
Atari	Method
2600	Method
platform	Method
provides	O
AI	O
researchers	O
with	O
something	O
unique	O
:	O
clean	O
,	O
easily	O
-	O
emulated	O
domains	O
which	O
nevertheless	O
provide	O
many	O
of	O
the	O
challenges	O
typically	O
associated	O
with	O
real	Task
-	Task
world	Task
applications	Task
.	O

Should	O
technology	O
advance	O
so	O
as	O
to	O
render	O
general	O
Atari	Task
2600	Task
game	Task
playing	Task
achievable	O
,	O
our	O
challenge	O
problem	O
can	O
always	O
be	O
extended	O
to	O
use	O
more	O
recent	O
video	Method
game	Method
platforms	Method
.	O

A	O
natural	O
progression	O
,	O
for	O
example	O
,	O
would	O
be	O
to	O
move	O
on	O
to	O
the	O
Commodore	O
64	O
,	O
then	O
to	O
the	O
Nintendo	O
,	O
and	O
so	O
forth	O
towards	O
current	O
generation	O
consoles	O
.	O

All	O
of	O
these	O
consoles	O
have	O
hundreds	O
of	O
released	Material
games	Material
,	O
and	O
older	O
platforms	O
have	O
readily	O
available	O
emulators	Method
.	O

With	O
the	O
ultra	O
-	O
realism	O
of	O
current	O
generation	O
consoles	O
,	O
each	O
console	O
represents	O
a	O
natural	O
stepping	O
stone	O
toward	O
general	O
real	Task
-	Task
world	Task
competency	Task
.	O

Our	O
hope	O
is	O
that	O
by	O
using	O
the	O
methodology	O
advocated	O
in	O
this	O
paper	O
,	O
we	O
can	O
work	O
in	O
a	O
bottom	O
-	O
up	O
fashion	O
towards	O
developing	O
more	O
sophisticated	O
AI	Method
technology	Method
while	O
still	O
maintaining	O
empirical	O
rigor	O
.	O

section	O
:	O
Conclusion	O
This	O
article	O
has	O
introduced	O
the	O
Arcade	Method
Learning	Method
Environment	Method
,	O
a	O
platform	O
for	O
evaluating	O
the	O
development	Task
of	Task
general	Task
,	Task
domain	Task
-	Task
independent	Task
agents	Task
.	O

ALE	Method
provides	O
an	O
interface	O
to	O
hundreds	O
of	O
Atari	Material
2600	Material
game	Material
environments	Material
,	O
each	O
one	O
different	O
,	O
interesting	O
,	O
and	O
designed	O
to	O
be	O
a	O
challenge	O
for	O
human	O
players	O
.	O

We	O
illustrate	O
the	O
promise	O
of	O
ALE	Method
as	O
a	O
challenge	O
problem	O
by	O
benchmarking	O
several	O
domain	Method
-	Method
independent	Method
agents	Method
that	O
use	O
well	O
-	O
established	O
reinforcement	Method
learning	Method
and	Method
planning	Method
techniques	Method
.	O

Our	O
results	O
suggest	O
that	O
general	Task
Atari	Task
game	Task
playing	Task
is	O
a	O
challenging	O
but	O
not	O
intractable	O
problem	O
domain	O
with	O
the	O
potential	O
to	O
aid	O
the	O
development	O
and	O
evaluation	Task
of	Task
general	Task
agents	Task
.	O

We	O
would	O
like	O
to	O
thank	O
Marc	O
Lanctot	O
,	O
Erik	O
Talvitie	O
,	O
and	O
Matthew	O
Hausknecht	O
for	O
providing	O
suggestions	O
on	O
helping	O
debug	O
and	O
improving	O
the	O
Arcade	Method
Learning	Method
Environment	Method
source	O
code	O
.	O

We	O
would	O
also	O
like	O
to	O
thank	O
our	O
reviewers	O
for	O
their	O
helpful	O
feedback	O
and	O
enthusiasm	O
about	O
the	O
Atari	Material
2600	Material
as	O
a	O
research	O
platform	O
.	O

The	O
work	O
presented	O
here	O
was	O
supported	O
by	O
the	O
Alberta	Material
Innovates	Material
Technology	Material
Futures	Material
,	O
the	O
Alberta	O
Innovates	O
Centre	O
for	O
Machine	Task
Learning	Task
at	O
the	O
University	O
of	O
Alberta	O
,	O
and	O
the	O
Natural	Material
Science	Material
and	Material
Engineering	Material
Research	Material
Council	Material
of	O
Canada	O
.	O

Invaluable	Material
computational	Material
resources	Material
were	O
provided	O
by	O
Compute	O
/	O
Calcul	O
Canada	O
.	O

appendix	O
:	O
Feature	Method
Set	Method
Construction	Method
This	O
section	O
gives	O
a	O
detailed	O
description	O
of	O
the	O
five	O
feature	Method
generation	Method
techniques	Method
from	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Basic	O
Abstraction	O
of	O
the	O
ScreenShots	O
(	O
BASS	O
)	O
The	O
idea	O
behind	O
BASS	Method
is	O
to	O
directly	O
encode	O
colours	O
present	O
on	O
the	O
screen	O
.	O

This	O
method	O
is	O
motivated	O
by	O
three	O
observations	O
on	O
the	O
Atari	Material
2600	Material
hardware	Material
and	O
games	Material
:	O
While	O
the	O
Atari	Method
2600	Method
hardware	Method
supports	O
a	O
screen	O
resolution	O
of	O
,	O
game	O
objects	O
are	O
usually	O
larger	O
than	O
a	O
few	O
pixels	O
.	O

Overall	O
,	O
important	O
game	O
events	O
happen	O
at	O
a	O
much	O
lower	O
resolution	O
.	O

Many	O
Atari	Material
2600	Material
games	Material
have	O
a	O
static	O
background	O
,	O
with	O
a	O
few	O
important	O
objects	O
moving	O
on	O
the	O
screen	O
.	O

While	O
the	O
screen	O
matrix	O
is	O
densely	O
populated	O
,	O
the	O
actual	O
interesting	O
features	O
on	O
the	O
screen	O
are	O
often	O
sparse	O
.	O

While	O
the	O
hardware	O
can	O
show	O
up	O
to	O
128	O
colours	O
in	O
the	O
NTSC	O
mode	O
,	O
it	O
is	O
limited	O
to	O
only	O
8	O
colours	O
in	O
the	O
SECAM	Method
mode	Method
.	O

Consequently	O
,	O
most	O
games	O
use	O
a	O
few	O
number	O
of	O
colours	O
to	O
distinguish	O
important	O
objects	O
on	O
the	O
screen	O
.	O

The	O
game	Material
screen	Material
is	O
first	O
preprocessed	O
by	O
subtracting	O
its	O
background	O
,	O
detected	O
using	O
a	O
simple	O
histogram	Method
method	Method
.	O

BASS	Method
then	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
eight	O
SECAM	O
palette	O
colours	O
at	O
a	O
low	O
resolution	O
,	O
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Intuitively	O
,	O
BASS	Method
seeks	O
to	O
capture	O
the	O
presence	O
of	O
objects	O
of	O
certain	O
colours	O
at	O
different	O
screen	O
locations	O
.	O

BASS	Method
also	O
encodes	O
relations	O
between	O
objects	O
by	O
constructing	O
all	O
pairwise	O
combinations	O
of	O
its	O
encoded	O
colour	O
features	O
.	O

In	O
Asterix	Material
,	O
for	O
example	O
,	O
it	O
is	O
important	O
to	O
know	O
if	O
there	O
is	O
a	O
green	O
object	O
(	O
player	O
character	O
)	O
and	O
a	O
red	O
object	O
(	O
collectable	O
object	O
)	O
in	O
its	O
vicinity	O
.	O

Pairwise	O
features	O
allow	O
us	O
to	O
capture	O
such	O
object	O
relations	O
.	O

subsection	O
:	O
Basic	O
The	O
Basic	O
method	O
generates	O
the	O
same	O
set	O
of	O
features	O
as	O
BASS	O
,	O
but	O
omits	O
the	O
pairwise	O
combinations	O
.	O

This	O
allows	O
us	O
to	O
study	O
whether	O
the	O
additional	O
features	O
are	O
beneficial	O
or	O
harmful	O
to	O
learning	Task
.	O

Because	O
the	O
Basic	O
method	O
has	O
fewer	O
features	O
than	O
BASS	O
,	O
it	O
encodes	O
the	O
presence	O
of	O
each	O
of	O
the	O
128	O
colours	O
.	O

In	O
comparison	O
to	O
BASS	O
,	O
Basic	O
therefore	O
represents	O
colour	O
more	O
accurately	O
,	O
but	O
can	O
not	O
represent	O
object	O
interactions	O
.	O

subsection	O
:	O
Detecting	Task
Instances	Task
of	Task
Classes	Task
of	Task
Objects	Task
(	O
DISCO	Method
)	O
This	O
feature	Method
generation	Method
method	Method
is	O
based	O
on	O
detecting	O
a	O
set	O
of	O
classes	O
representing	O
game	O
entities	O
and	O
locating	O
instances	O
of	O
these	O
classes	O
on	O
the	O
screen	O
.	O

DISCO	Method
is	O
motivated	O
by	O
the	O
following	O
additional	O
observations	O
on	O
Atari	Task
2600	Task
games	Task
:	O
The	O
game	O
entities	O
are	O
often	O
instances	O
of	O
a	O
few	O
classes	O
of	O
objects	O
.	O

For	O
instance	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
shows	O
,	O
while	O
there	O
are	O
many	O
objects	O
in	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Freeway	O
,	O
all	O
of	O
these	O
objects	O
are	O
instances	O
of	O
only	O
two	O
classes	O
:	O
Chicken	O
and	O
Car	O
.	O

Similarly	O
,	O
all	O
the	O
objects	O
on	O
a	O
sample	O
screen	O
of	O
the	O
game	O
Seaquest	O
are	O
instances	O
of	O
one	O
of	O
these	O
six	O
classes	O
:	O
Fish	O
,	O
Swimmer	O
,	O
Player	O
Submarine	O
,	O
Enemy	O
Submarine	O
,	O
Player	O
Bullet	O
,	O
and	O
Enemy	O
Bullet	O
.	O

The	O
interaction	O
between	O
two	O
objects	O
can	O
often	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
their	O
respective	O
classes	O
.	O

As	O
an	O
example	O
,	O
consider	O
Car	Task
-	Task
Chicken	Task
object	Task
interactions	Task
in	O
Freeway	Material
:	O
learning	O
that	O
there	O
is	O
lower	O
value	O
associated	O
with	O
one	O
Chicken	O
instance	O
hitting	O
a	O
Car	O
instance	O
can	O
be	O
generalized	O
to	O
all	O
instances	O
of	O
those	O
two	O
classes	O
.	O

DISCO	Method
first	O
performs	O
a	O
series	O
of	O
preprocessing	Method
steps	Method
to	O
discover	O
classes	O
,	O
during	O
which	O
no	O
value	Method
function	Method
learning	Method
is	O
performed	O
.	O

When	O
the	O
agent	O
subsequently	O
learns	O
to	O
play	O
the	O
game	O
,	O
DISCO	O
generates	O
features	O
by	O
detecting	O
objects	O
on	O
the	O
screen	O
and	O
classifying	O
them	O
.	O

The	O
DISCO	Method
process	Method
is	O
summarized	O
by	O
the	O
following	O
steps	O
:	O
Locally	Method
Sensitive	Method
Hashing	Method
(	Method
LSH	Method
)	Method
Feature	Method
Generation	Method
(	O
hash	O
table	O
size	O
)	O
,	O
(	O
screen	O
bit	O
vector	O
size	O
)	O
(	O
number	O
of	O
random	O
bit	O
vectors	O
)	O
,	O
(	O
number	O
of	O
non	O
-	O
zero	O
entries	O
)	O
A	O
screen	O
matrix	O
with	O
elements	O
(	O
has	O
length	O
)	O
Initialize	O
hash	O
the	O
projection	O
of	O
onto	O
one	O
binary	O
feature	O
per	O
random	O
bit	O
vector	O
Initialize	O
,	O
(	O
)	O
Initialize	O
Select	O
distinct	O
coordinates	O
between	O
1	O
and	O
uniformly	O
at	O
random	O
;	O
;	O
…	O
;	O
Initialize	O
,	O
(	O
uniformly	O
random	O
coordinate	O
between	O
1	O
and	O
M	O
)	O
Preprocessing	Task
:	O
Background	Task
detection	Task
:	O
The	O
static	O
background	O
matrix	O
is	O
extracted	O
using	O
a	O
histogram	Method
method	Method
,	O
as	O
with	O
BASS	Method
.	O

Blob	Task
extraction	Task
:	O
A	O
list	O
of	O
moving	O
blob	O
(	O
foreground	O
)	O
objects	O
is	O
detected	O
in	O
each	O
game	O
screen	O
.	O

Class	Task
discovery	Task
:	O
A	O
set	O
of	O
classes	O
is	O
detected	O
from	O
the	O
extracted	O
blob	O
objects	O
.	O

Class	Method
filtering	Method
:	O
Classes	O
that	O
appear	O
infrequently	O
or	O
are	O
restricted	O
to	O
small	O
region	O
of	O
the	O
screen	O
are	O
removed	O
from	O
the	O
set	O
.	O

Class	Task
merging	Task
:	O
Classes	O
that	O
have	O
similar	O
shapes	O
are	O
merged	O
together	O
.	O

Feature	Task
generation	Task
:	O
Class	Method
instance	Method
detection	Method
:	O
At	O
each	O
time	O
step	O
,	O
class	O
instances	O
are	O
detected	O
from	O
the	O
current	O
screen	O
matrix	O
.	O

Feature	Method
vector	Method
generation	Method
:	O
A	O
feature	O
vector	O
is	O
generated	O
from	O
the	O
detected	O
instances	O
by	O
tile	O
-	O
coding	O
their	O
absolute	O
position	O
as	O
well	O
as	O
the	O
relative	O
position	O
and	O
velocity	O
of	O
every	O
pair	O
of	O
instances	O
from	O
different	O
classes	O
.	O

Multiple	O
instances	O
of	O
the	O
same	O
objects	O
are	O
combined	O
additively	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
discovered	O
objects	O
in	O
a	O
Seaquest	Material
frame	Material
.	O

This	O
image	O
illustrates	O
the	O
difficulties	O
in	O
detecting	Task
objects	Task
:	O
although	O
DISCO	Method
correctly	O
classifies	O
the	O
different	O
fish	O
as	O
part	O
of	O
the	O
same	O
class	O
,	O
it	O
also	O
detects	O
a	O
life	O
icon	O
and	O
the	O
oxygen	O
bar	O
as	O
part	O
of	O
that	O
class	O
.	O

subsection	O
:	O
Locality	Method
Sensitive	Method
Hashing	Method
(	O
LSH	Method
)	O
An	O
alternative	O
approach	O
to	O
BASS	Task
and	O
DISCO	Task
is	O
to	O
use	O
well	O
-	O
established	O
feature	Method
generation	Method
methods	Method
that	O
are	O
agnostic	O
about	O
the	O
type	O
of	O
input	O
they	O
receive	O
.	O

Such	O
methods	O
include	O
polynomial	Method
bases	Method
,	O
sparse	Method
distributed	Method
memories	Method
and	O
locality	Method
sensitive	Method
hashing	Method
(	Method
LSH	Method
)	Method
.	O

In	O
this	O
paper	O
we	O
consider	O
the	O
latter	O
as	O
a	O
simple	O
mean	O
of	O
reducing	O
the	O
large	Task
image	Task
space	Task
to	O
a	O
smaller	O
,	O
more	O
manageable	O
set	O
of	O
features	O
.	O

The	O
input	O
–	O
here	O
,	O
a	O
game	Material
screen	Material
–	O
is	O
first	O
mapped	O
to	O
a	O
bit	O
vector	O
of	O
size	O
.	O

The	O
resulting	O
vector	O
is	O
then	O
hashed	O
down	O
into	O
a	O
smaller	O
set	O
of	O
features	O
.	O

LSH	Method
performs	O
an	O
additional	O
random	Method
projection	Method
step	Method
to	O
ensure	O
that	O
similar	O
screens	O
are	O
more	O
likely	O
to	O
be	O
binned	O
together	O
.	O

The	O
LSH	Method
generation	Method
method	Method
is	O
detailed	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
RAM	Method
-	Method
based	Method
Feature	Method
Generation	Method
Unlike	O
the	O
previous	O
three	O
methods	O
,	O
which	O
generate	O
feature	O
vectors	O
based	O
on	O
the	O
game	Material
screen	Material
,	O
the	O
RAM	Method
-	Method
based	Method
feature	Method
generation	Method
method	Method
relies	O
on	O
the	O
contents	O
of	O
the	O
console	O
memory	O
.	O

The	O
Atari	Method
2600	Method
has	O
only	O
bits	O
of	O
random	O
access	O
memory	O
,	O
which	O
must	O
hold	O
the	O
complete	O
internal	O
state	O
of	O
a	O
game	O
:	O
location	O
of	O
game	O
entities	O
,	O
timers	O
,	O
health	O
indicators	O
,	O
etc	O
.	O

The	O
RAM	Method
is	O
therefore	O
a	O
relatively	O
compact	O
representation	O
of	O
the	O
game	O
state	O
,	O
and	O
in	O
contrast	O
to	O
the	O
game	O
screen	O
,	O
it	O
is	O
also	O
Markovian	Method
.	O

The	O
purpose	O
of	O
our	O
RAM	Method
-	Method
based	Method
agent	Method
is	O
to	O
investigate	O
whether	O
features	O
generated	O
from	O
the	O
RAM	Method
affect	O
performance	O
differently	O
from	O
features	O
generated	O
from	O
game	Material
screens	Material
.	O

The	O
first	O
part	O
of	O
the	O
generated	O
feature	O
vector	O
simply	O
includes	O
the	O
1024	O
bits	O
of	O
RAM	O
.	O

Atari	Material
2600	Material
game	Material
programmers	Material
often	O
used	O
these	O
bits	O
not	O
as	O
individual	O
values	O
,	O
but	O
as	O
part	O
of	O
4	O
-	O
bit	O
or	O
8	O
-	O
bit	O
words	O
.	O

Linear	Method
function	Method
approximation	Method
on	O
the	O
individual	O
bits	O
can	O
capture	O
the	O
value	O
of	O
these	O
multi	O
-	O
bit	O
words	O
.	O

We	O
are	O
also	O
interested	O
in	O
the	O
relation	O
between	O
pairs	O
of	O
values	O
in	O
memory	O
.	O

To	O
capture	O
these	O
relations	O
,	O
the	O
logical	O
-	O
AND	O
of	O
all	O
possible	O
bit	O
pairs	O
is	O
appended	O
to	O
the	O
feature	O
vector	O
.	O

Note	O
that	O
a	O
linear	O
function	O
on	O
the	O
pairwise	O
’s	Method
can	O
capture	O
products	O
of	O
both	O
4	O
-	O
bit	O
and	O
8	O
-	O
bit	O
words	O
.	O

This	O
is	O
because	O
the	O
product	O
of	O
two	O
-	O
bit	O
words	O
can	O
be	O
expressed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
pairwise	O
products	O
of	O
their	O
bits	O
.	O

appendix	O
:	O
UCT	Material
Pseudocode	Material
UCT	O
(	O
search	O
horizon	O
)	O
,	O
(	O
simulations	O
per	O
step	O
)	O
(	O
search	O
tree	O
)	O
(	O
current	O
state	O
)	O
is	O
empty	O
or	O
optional	O
is	O
not	O
a	O
leaf	O
,	O
some	O
action	O
was	O
never	O
taken	O
in	O
run	O
model	O
for	O
one	O
step	O
c	O
is	O
necessarily	O
a	O
leaf	O
update	O
-	O
value	O
(	O
n	O
,	O
R	O
)	O
propagate	O
values	O
back	O
up	O
action	O
most	O
frequently	O
taken	O
at	O
root	O
UCT	O
Routines	O
:	O
discount	O
factor	O
children	O
of	O
Initialize	O
Monte	O
-	O
Carlo	O
return	O
to	O
0	O
Select	O
according	O
to	O
some	O
rollout	Method
policy	Method
(	O
e.g.	O
uniformly	O
randomly	O
)	O
is	O
not	O
the	O
root	O
of	O
,	O
i.e.	O
appendix	O
:	O
Experimental	O
Parameters	O
appendix	O
:	O
Detailed	O
Results	O
subsection	O
:	O
Reinforcement	Method
Learning	Method
.	O

subsection	O
:	O
Planning	Task
.	O

bibliography	O
:	O
References	O
