document	O
:	O
Matching	Method
Networks	Method
for	O
One	Task
Shot	Task
Learning	Task
Learning	O
from	O
a	O
few	O
examples	O
remains	O
a	O
key	O
challenge	O
in	O
machine	Task
learning	Task
.	O

Despite	O
recent	O
advances	O
in	O
important	O
domains	O
such	O
as	O
vision	Task
and	O
language	Task
,	O
the	O
standard	O
supervised	Method
deep	Method
learning	Method
paradigm	Method
does	O
not	O
offer	O
a	O
satisfactory	O
solution	O
for	O
learning	O
new	O
concepts	O
rapidly	O
from	O
little	O
data	O
.	O

In	O
this	O
work	O
,	O
we	O
employ	O
ideas	O
from	O
metric	Method
learning	Method
based	O
on	O
deep	Method
neural	Method
features	Method
and	O
from	O
recent	O
advances	O
that	O
augment	O
neural	Method
networks	Method
with	O
external	O
memories	O
.	O

Our	O
framework	O
learns	O
a	O
network	O
that	O
maps	O
a	O
small	O
labelled	O
support	O
set	O
and	O
an	O
unlabelled	O
example	O
to	O
its	O
label	O
,	O
obviating	O
the	O
need	O
for	O
fine	O
-	O
tuning	O
to	O
adapt	O
to	O
new	O
class	O
types	O
.	O

We	O
then	O
define	O
one	Task
-	Task
shot	Task
learning	Task
problems	O
on	O
vision	Task
(	O
using	O
Omniglot	Material
,	O
ImageNet	Material
)	O
and	O
language	Task
tasks	O
.	O

Our	O
algorithm	O
improves	O
one	Metric
-	Metric
shot	Metric
accuracy	Metric
on	O
ImageNet	Material
from	O
87.6	O
%	O
to	O
93.2	O
%	O
and	O
from	O
88.0	O
%	O
to	O
93.8	O
%	O
on	O
Omniglot	Material
compared	O
to	O
competing	O
approaches	O
.	O

We	O
also	O
demonstrate	O
the	O
usefulness	O
of	O
the	O
same	O
model	O
on	O
language	Task
modeling	O
by	O
introducing	O
a	O
one	Task
-	Task
shot	Task
task	Task
on	O
the	O
Penn	O
Treebank	O
.	O

section	O
:	O
Introduction	O
Humans	O
learn	O
new	O
concepts	O
with	O
very	O
little	O
supervision	O
–	O
e.g.	O
a	O
child	O
can	O
generalize	O
the	O
concept	O
of	O
‘	O
‘	O
giraffe	O
’	O
’	O
from	O
a	O
single	O
picture	O
in	O
a	O
book	O
–	O
yet	O
our	O
best	O
deep	Method
learning	Method
systems	Method
need	O
hundreds	O
or	O
thousands	O
of	O
examples	O
.	O

This	O
motivates	O
the	O
setting	O
we	O
are	O
interested	O
in	O
:	O
‘	O
‘	O
one	Task
-	Task
shot	Task
’	Task
’	Task
learning	Task
,	O
which	O
consists	O
of	O
learning	O
a	O
class	O
from	O
a	O
single	O
labelled	O
example	O
.	O

Deep	Method
learning	Method
has	O
made	O
major	O
advances	O
in	O
areas	O
such	O
as	O
speech	Task
,	O
vision	Task
and	O
language	Task
,	O
but	O
is	O
notorious	O
for	O
requiring	O
large	O
datasets	O
.	O

Data	Method
augmentation	Method
and	O
regularization	Method
techniques	Method
alleviate	O
overfitting	O
in	O
low	O
data	O
regimes	O
,	O
but	O
do	O
not	O
solve	O
it	O
.	O

Furthermore	O
,	O
learning	Task
is	O
still	O
slow	O
and	O
based	O
on	O
large	O
datasets	O
,	O
requiring	O
many	O
weight	Method
updates	Method
using	O
stochastic	Method
gradient	Method
descent	Method
.	O

This	O
,	O
in	O
our	O
view	O
,	O
is	O
mostly	O
due	O
to	O
the	O
parametric	O
aspect	O
of	O
the	O
model	O
,	O
in	O
which	O
training	O
examples	O
need	O
to	O
be	O
slowly	O
learnt	O
by	O
the	O
model	O
into	O
its	O
parameters	O
.	O

In	O
contrast	O
,	O
many	O
non	Method
-	Method
parametric	Method
models	Method
allow	O
novel	O
examples	O
to	O
be	O
rapidly	O
assimilated	O
,	O
whilst	O
not	O
suffering	O
from	O
catastrophic	O
forgetting	O
.	O

Some	O
models	O
in	O
this	O
family	O
(	O
e.g.	O
,	O
nearest	O
neighbors	O
)	O
do	O
not	O
require	O
any	O
training	O
but	O
performance	O
depends	O
on	O
the	O
chosen	O
metric	O
.	O

Previous	O
work	O
on	O
metric	Task
learning	Task
in	O
non	Task
-	Task
parametric	Task
setups	Task
has	O
been	O
influential	O
on	O
our	O
model	O
,	O
and	O
we	O
aim	O
to	O
incorporate	O
the	O
best	O
characteristics	O
from	O
both	O
parametric	Method
and	Method
non	Method
-	Method
parametric	Method
models	Method
–	O
namely	O
,	O
rapid	O
acquisition	O
of	O
new	O
examples	O
while	O
providing	O
excellent	O
generalisation	O
from	O
common	O
examples	O
.	O

The	O
novelty	O
of	O
our	O
work	O
is	O
twofold	O
:	O
at	O
the	O
modeling	O
level	O
,	O
and	O
at	O
the	O
training	Task
procedure	Task
.	O

We	O
propose	O
Matching	Method
Nets	Method
(	O
MN	Method
)	Method
,	O
a	O
neural	Method
network	Method
which	O
uses	O
recent	O
advances	O
in	O
attention	Task
and	Task
memory	Task
that	O
enable	O
rapid	Task
learning	Task
.	O

Secondly	O
,	O
our	O
training	O
procedure	O
is	O
based	O
on	O
a	O
simple	O
machine	Method
learning	Method
principle	Method
:	O
test	O
and	O
train	O
conditions	O
must	O
match	O
.	O

Thus	O
to	O
train	O
our	O
network	O
to	O
do	O
rapid	Task
learning	Task
,	O
we	O
train	O
it	O
by	O
showing	O
only	O
a	O
few	O
examples	O
per	O
class	O
,	O
switching	O
the	O
task	O
from	O
minibatch	O
to	O
minibatch	O
,	O
much	O
like	O
how	O
it	O
will	O
be	O
tested	O
when	O
presented	O
with	O
a	O
few	O
examples	O
of	O
a	O
new	O
task	O
.	O

Besides	O
our	O
contributions	O
in	O
defining	O
a	O
model	O
and	O
training	Metric
criterion	Metric
amenable	O
for	O
one	Task
-	Task
shot	Task
learning	Task
,	O
we	O
contribute	O
by	O
the	O
definition	O
of	O
tasks	O
that	O
can	O
be	O
used	O
to	O
benchmark	O
other	O
approaches	O
on	O
both	O
ImageNet	Material
and	O
small	O
scale	O
language	Task
modeling	O
.	O

We	O
hope	O
that	O
our	O
results	O
will	O
encourage	O
others	O
to	O
work	O
on	O
this	O
challenging	O
problem	O
.	O

We	O
organized	O
the	O
paper	O
by	O
first	O
defining	O
and	O
explaining	O
our	O
model	O
whilst	O
linking	O
its	O
several	O
components	O
to	O
related	O
work	O
.	O

Then	O
in	O
the	O
following	O
section	O
we	O
briefly	O
elaborate	O
on	O
some	O
of	O
the	O
related	O
work	O
to	O
the	O
task	O
and	O
our	O
model	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
describe	O
both	O
our	O
general	O
setup	O
and	O
the	O
experiments	O
we	O
performed	O
,	O
demonstrating	O
strong	O
results	O
on	O
one	Task
-	Task
shot	Task
learning	Task
on	O
a	O
variety	O
of	O
tasks	O
and	O
setups	O
.	O

section	O
:	O
Model	O
Our	O
non	Method
-	Method
parametric	Method
approach	Method
to	O
solving	O
one	Task
-	Task
shot	Task
learning	Task
is	O
based	O
on	O
two	O
components	O
which	O
we	O
describe	O
in	O
the	O
following	O
subsections	O
.	O

First	O
,	O
our	O
model	O
architecture	O
follows	O
recent	O
advances	O
in	O
neural	Method
networks	Method
augmented	O
with	O
memory	O
(	O
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Given	O
a	O
(	O
small	O
)	O
support	O
set	O
,	O
our	O
model	O
defines	O
a	O
function	O
(	O
or	O
classifier	Method
)	O
for	O
each	O
,	O
i.e.	O
a	O
mapping	O
.	O

Second	O
,	O
we	O
employ	O
a	O
training	Method
strategy	Method
which	O
is	O
tailored	O
for	O
one	Task
-	Task
shot	Task
learning	Task
from	O
the	O
support	O
set	O
.	O

subsection	O
:	O
Model	O
Architecture	O
In	O
recent	O
years	O
,	O
many	O
groups	O
have	O
investigated	O
ways	O
to	O
augment	O
neural	Method
network	Method
architectures	Method
with	O
external	O
memories	O
and	O
other	O
components	O
that	O
make	O
them	O
more	O
‘	O
‘	O
computer	O
-	O
like	O
’	O
’	O
.	O

We	O
draw	O
inspiration	O
from	O
models	O
such	O
as	O
sequence	Task
to	Task
sequence	Task
(	O
seq2seq	Method
)	O
with	O
attention	Method
,	O
memory	Method
networks	Method
and	O
pointer	Method
networks	Method
.	O

In	O
all	O
these	O
models	O
,	O
a	O
neural	Method
attention	Method
mechanism	Method
,	O
often	O
fully	O
differentiable	O
,	O
is	O
defined	O
to	O
access	O
(	O
or	O
read	O
)	O
a	O
memory	O
matrix	O
which	O
stores	O
useful	O
information	O
to	O
solve	O
the	O
task	O
at	O
hand	O
.	O

Typical	O
uses	O
of	O
this	O
include	O
machine	Task
translation	Task
,	O
speech	Task
recognition	O
,	O
or	O
question	Task
answering	Task
.	O

More	O
generally	O
,	O
these	O
architectures	O
model	O
where	O
and	O
/	O
or	O
can	O
be	O
a	O
sequence	O
(	O
like	O
in	O
seq2seq	Method
models	Method
)	O
,	O
or	O
,	O
more	O
interestingly	O
for	O
us	O
,	O
a	O
set	O
.	O

Our	O
contribution	O
is	O
to	O
cast	O
the	O
problem	O
of	O
one	Task
-	Task
shot	Task
learning	Task
within	O
the	O
set	Method
-	Method
to	Method
-	Method
set	Method
framework	Method
.	O

The	O
key	O
point	O
is	O
that	O
when	O
trained	O
,	O
Matching	Method
Networks	Method
are	O
able	O
to	O
produce	O
sensible	O
test	O
labels	O
for	O
unobserved	O
classes	O
without	O
any	O
changes	O
to	O
the	O
network	O
.	O

More	O
precisely	O
,	O
we	O
wish	O
to	O
map	O
from	O
a	O
(	O
small	O
)	O
support	O
set	O
of	O
examples	O
of	O
image	O
-	O
label	O
pairs	O
to	O
a	O
classifier	Method
which	O
,	O
given	O
a	O
test	O
example	O
,	O
defines	O
a	O
probability	O
distribution	O
over	O
outputs	O
.	O

We	O
define	O
the	O
mapping	O
to	O
be	O
where	O
is	O
parameterised	O
by	O
a	O
neural	Method
network	Method
.	O

Thus	O
,	O
when	O
given	O
a	O
new	O
support	O
set	O
of	O
examples	O
from	O
which	O
to	O
one	O
-	O
shot	Task
learn	Task
,	O
we	O
simply	O
use	O
the	O
parametric	Method
neural	Method
network	Method
defined	O
by	O
to	O
make	O
predictions	O
about	O
the	O
appropriate	O
label	O
for	O
each	O
test	O
example	O
:	O
.	O

In	O
general	O
,	O
our	O
predicted	O
output	O
class	O
for	O
a	O
given	O
input	O
unseen	O
example	O
and	O
a	O
support	O
set	O
becomes	O
.	O

Our	O
model	O
in	O
its	O
simplest	O
form	O
computes	O
as	O
follows	O
:	O
where	O
are	O
the	O
samples	O
and	O
labels	O
from	O
the	O
support	O
set	O
,	O
and	O
is	O
an	O
attention	Method
mechanism	Method
which	O
we	O
discuss	O
below	O
.	O

Note	O
that	O
eq	O
.	O

[	O
reference	O
]	O
essentially	O
describes	O
the	O
output	O
for	O
a	O
new	O
class	O
as	O
a	O
linear	Method
combination	Method
of	O
the	O
labels	O
in	O
the	O
support	O
set	O
.	O

Where	O
the	O
attention	Method
mechanism	Method
is	O
a	O
kernel	Method
on	Method
,	O
then	O
(	O
[	O
reference	O
]	O
)	O
is	O
akin	O
to	O
a	O
kernel	Method
density	Method
estimator	Method
.	O

Where	O
the	O
attention	Method
mechanism	Method
is	O
zero	O
for	O
the	O
furthest	O
from	O
according	O
to	O
some	O
distance	O
metric	O
and	O
an	O
appropriate	O
constant	O
otherwise	O
,	O
then	O
(	O
[	O
reference	O
]	O
)	O
is	O
equivalent	O
to	O
‘	O
’	O
-	O
nearest	O
neighbours	O
(	O
although	O
this	O
requires	O
an	O
extension	O
to	O
the	O
attention	Method
mechanism	Method
that	O
we	O
describe	O
in	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Thus	O
(	O
[	O
reference	O
]	O
)	O
subsumes	O
both	O
KDE	Method
and	O
kNN	Method
methods	Method
.	O

Another	O
view	O
of	O
(	O
[	O
reference	O
]	O
)	O
is	O
where	O
acts	O
as	O
an	O
attention	Method
mechanism	Method
and	O
the	O
act	O
as	O
memories	O
bound	O
to	O
the	O
corresponding	O
.	O

In	O
this	O
case	O
we	O
can	O
understand	O
this	O
as	O
a	O
particular	O
kind	O
of	O
associative	Method
memory	Method
where	O
,	O
given	O
an	O
input	O
,	O
we	O
‘	O
‘	O
point	O
’	O
’	O
to	O
the	O
corresponding	O
example	O
in	O
the	O
support	O
set	O
,	O
retrieving	O
its	O
label	O
.	O

However	O
,	O
unlike	O
other	O
attentional	Method
memory	Method
mechanisms	Method
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
non	O
-	O
parametric	O
in	O
nature	O
:	O
as	O
the	O
support	O
set	O
size	O
grows	O
,	O
so	O
does	O
the	O
memory	O
used	O
.	O

Hence	O
the	O
functional	O
form	O
defined	O
by	O
the	O
classifier	Method
is	O
very	O
flexible	O
and	O
can	O
adapt	O
easily	O
to	O
any	O
new	O
support	O
set	O
.	O

subsubsection	O
:	O
The	O
Attention	Method
Kernel	Method
Equation	O
[	O
reference	O
]	O
relies	O
on	O
choosing	O
,	O
the	O
attention	Method
mechanism	Method
,	O
which	O
fully	O
specifies	O
the	O
classifier	Method
.	O

The	O
simplest	O
form	O
that	O
this	O
takes	O
(	O
and	O
which	O
has	O
very	O
tight	O
relationships	O
with	O
common	O
attention	Method
models	Method
and	O
kernel	Method
functions	Method
)	O
is	O
to	O
use	O
the	O
softmax	O
over	O
the	O
cosine	O
distance	O
,	O
i.e.	O
,	O
with	O
embedding	Method
functions	Method
and	O
being	O
appropriate	O
neural	Method
networks	Method
(	O
potentially	O
with	O
)	O
to	O
embed	O
and	O
.	O

In	O
our	O
experiments	O
we	O
shall	O
see	O
examples	O
where	O
and	O
are	O
parameterised	O
variously	O
as	O
deep	Method
convolutional	Method
networks	Method
for	O
image	Task
tasks	Task
(	O
as	O
in	O
VGG	Method
or	O
Inception	Method
)	O
or	O
a	O
simple	O
form	O
word	Method
embedding	Method
for	O
language	Task
tasks	O
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
.	O

We	O
note	O
that	O
,	O
though	O
related	O
to	O
metric	Method
learning	Method
,	O
the	O
classifier	Method
defined	O
by	O
Equation	O
[	O
reference	O
]	O
is	O
discriminative	O
.	O

For	O
a	O
given	O
support	O
set	O
and	O
sample	O
to	O
classify	O
,	O
it	O
is	O
enough	O
for	O
to	O
be	O
sufficiently	O
aligned	O
with	O
pairs	O
such	O
that	O
and	O
misaligned	O
with	O
the	O
rest	O
.	O

This	O
kind	O
of	O
loss	O
is	O
also	O
related	O
to	O
methods	O
such	O
as	O
Neighborhood	Method
Component	Method
Analysis	Method
(	O
NCA	Method
)	Method
,	O
triplet	Method
loss	Method
or	O
large	Method
margin	Method
nearest	Method
neighbor	Method
.	O

However	O
,	O
the	O
objective	O
that	O
we	O
are	O
trying	O
to	O
optimize	O
is	O
precisely	O
aligned	O
with	O
multi	Task
-	Task
way	Task
,	Task
one	Task
-	Task
shot	Task
classification	Task
,	O
and	O
thus	O
we	O
expect	O
it	O
to	O
perform	O
better	O
than	O
its	O
counterparts	O
.	O

Additionally	O
,	O
the	O
loss	Method
is	O
simple	O
and	O
differentiable	O
so	O
that	O
one	O
can	O
find	O
the	O
optimal	O
parameters	O
in	O
an	O
‘	O
‘	O
end	O
-	O
to	O
-	O
end	O
’	O
’	O
fashion	O
.	O

subsubsection	O
:	O
Full	Method
Context	Method
Embeddings	Method
The	O
main	O
novelty	O
of	O
our	O
model	O
lies	O
in	O
reinterpreting	O
a	O
well	O
studied	O
framework	O
(	O
neural	Method
networks	Method
with	O
external	O
memories	O
)	O
to	O
do	O
one	Task
-	Task
shot	Task
learning	Task
.	O

Closely	O
related	O
to	O
metric	Method
learning	Method
,	O
the	O
embedding	O
functions	O
and	O
act	O
as	O
a	O
lift	O
to	O
feature	O
space	O
to	O
achieve	O
maximum	Metric
accuracy	Metric
through	O
the	O
classification	O
function	O
described	O
in	O
eq	O
.	O

[	O
reference	O
]	O
.	O

Despite	O
the	O
fact	O
that	O
the	O
classification	Method
strategy	Method
is	O
fully	O
conditioned	O
on	O
the	O
whole	O
support	O
set	O
through	O
,	O
the	O
embeddings	O
on	O
which	O
we	O
apply	O
the	O
cosine	O
similarity	O
to	O
‘	O
‘	O
attend	O
’	O
’	O
,	O
‘	O
‘	O
point	O
’	O
’	O
or	O
simply	O
compute	O
the	O
nearest	O
neighbor	O
are	O
myopic	O
in	O
the	O
sense	O
that	O
each	O
element	O
gets	O
embedded	O
by	O
independently	O
of	O
other	O
elements	O
in	O
the	O
support	O
set	O
.	O

Furthermore	O
,	O
should	O
be	O
able	O
to	O
modify	O
how	O
we	O
embed	O
the	O
test	O
image	O
through	O
.	O

We	O
propose	O
embedding	O
the	O
elements	O
of	O
the	O
set	O
through	O
a	O
function	O
which	O
takes	O
as	O
input	O
the	O
full	O
set	O
in	O
addition	O
to	O
,	O
i.e.	O
becomes	O
.	O

Thus	O
,	O
as	O
a	O
function	O
of	O
the	O
whole	O
support	O
set	O
,	O
can	O
modify	O
how	O
to	O
embed	O
.	O

This	O
could	O
be	O
useful	O
when	O
some	O
element	O
is	O
very	O
close	O
to	O
,	O
in	O
which	O
case	O
it	O
may	O
be	O
beneficial	O
to	O
change	O
the	O
function	O
with	O
which	O
we	O
embed	O
–	O
some	O
evidence	O
of	O
this	O
is	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O

We	O
use	O
a	O
bidirectional	O
Long	Method
-	Method
Short	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
to	O
encode	O
in	O
the	O
context	O
of	O
the	O
support	O
set	O
,	O
considered	O
as	O
a	O
sequence	O
(	O
see	O
appendix	O
for	O
a	O
more	O
precise	O
definition	O
)	O
.	O

The	O
second	O
issue	O
can	O
be	O
fixed	O
via	O
an	O
LSTM	Method
with	O
read	O
-	O
attention	O
over	O
the	O
whole	O
set	O
,	O
whose	O
inputs	O
are	O
equal	O
to	O
:	O
where	O
are	O
the	O
features	O
(	O
e.g.	O
,	O
derived	O
from	O
a	O
CNN	Method
)	O
which	O
are	O
input	O
to	O
the	O
LSTM	Method
(	O
constant	O
at	O
each	O
time	O
step	O
)	O
.	O

is	O
the	O
fixed	O
number	O
of	O
unrolling	O
steps	O
of	O
the	O
LSTM	Method
,	O
and	O
is	O
the	O
set	O
over	O
which	O
we	O
attend	O
,	O
embedded	O
with	O
.	O

This	O
allows	O
for	O
the	O
model	O
to	O
potentially	O
ignore	O
some	O
elements	O
in	O
the	O
support	O
set	O
,	O
and	O
adds	O
‘	O
‘	O
depth	O
’	O
’	O
to	O
the	O
computation	Task
of	Task
attention	Task
(	O
see	O
appendix	O
for	O
more	O
details	O
)	O
.	O

subsection	O
:	O
Training	O
Strategy	O
In	O
the	O
previous	O
subsection	O
we	O
described	O
Matching	Method
Networks	Method
which	O
map	O
a	O
support	O
set	O
to	O
a	O
classification	O
function	O
,	O
.	O

We	O
achieve	O
this	O
via	O
a	O
modification	O
of	O
the	O
set	Method
-	Method
to	Method
-	Method
set	Method
paradigm	Method
augmented	O
with	O
attention	Method
,	O
with	O
the	O
resulting	O
mapping	O
being	O
of	O
the	O
form	O
,	O
noting	O
that	O
are	O
the	O
parameters	O
of	O
the	O
model	O
(	O
i.e.	O
of	O
the	O
embedding	O
functions	O
and	O
described	O
previously	O
)	O
.	O

The	O
training	O
procedure	O
has	O
to	O
be	O
chosen	O
carefully	O
so	O
as	O
to	O
match	O
inference	Task
at	O
test	O
time	O
.	O

Our	O
model	O
has	O
to	O
perform	O
well	O
with	O
support	O
sets	O
which	O
contain	O
classes	O
never	O
seen	O
during	O
training	O
.	O

More	O
specifically	O
,	O
let	O
us	O
define	O
a	O
task	O
as	O
distribution	O
over	O
possible	O
label	O
sets	O
.	O

Typically	O
we	O
consider	O
to	O
uniformly	O
weight	O
all	O
data	O
sets	O
of	O
up	O
to	O
a	O
few	O
unique	O
classes	O
(	O
e.g.	O
,	O
5	O
)	O
,	O
with	O
a	O
few	O
examples	O
per	O
class	O
(	O
e.g.	O
,	O
up	O
to	O
5	O
)	O
.	O

In	O
this	O
case	O
,	O
a	O
label	O
set	O
sampled	O
from	O
a	O
task	O
,	O
,	O
will	O
typically	O
have	O
5	O
to	O
25	O
examples	O
.	O

To	O
form	O
an	O
‘	O
‘	O
episode	O
’	O
’	O
to	O
compute	O
gradients	O
and	O
update	O
our	O
model	O
,	O
we	O
first	O
sample	O
from	O
(	O
e.g.	O
,	O
could	O
be	O
the	O
label	O
set	O
)	O
.	O

We	O
then	O
use	O
to	O
sample	O
the	O
support	O
set	O
and	O
a	O
batch	O
(	O
i.e.	O
,	O
both	O
and	O
are	O
labelled	O
examples	O
of	O
cats	O
and	O
dogs	O
)	O
.	O

The	O
Matching	Method
Net	Method
is	O
then	O
trained	O
to	O
minimise	O
the	O
error	O
predicting	O
the	O
labels	O
in	O
the	O
batch	O
conditioned	O
on	O
the	O
support	O
set	O
.	O

This	O
is	O
a	O
form	O
of	O
meta	Method
-	Method
learning	Method
since	O
the	O
training	Method
procedure	Method
explicitly	O
learns	O
to	O
learn	O
from	O
a	O
given	O
support	O
set	O
to	O
minimise	O
a	O
loss	O
over	O
a	O
batch	O
.	O

More	O
precisely	O
,	O
the	O
Matching	Task
Nets	Task
training	Task
objective	Task
is	O
as	O
follows	O
:	O
Training	O
with	O
eq	O
.	O

[	O
reference	O
]	O
yields	O
a	O
model	O
which	O
works	O
well	O
when	O
sampling	O
from	O
a	O
different	O
distribution	O
of	O
novel	O
labels	O
.	O

Crucially	O
,	O
our	O
model	O
does	O
not	O
need	O
any	O
fine	O
tuning	O
on	O
the	O
classes	O
it	O
has	O
never	O
seen	O
due	O
to	O
its	O
non	O
-	O
parametric	O
nature	O
.	O

Obviously	O
,	O
as	O
diverges	O
far	O
from	O
the	O
from	O
which	O
we	O
sampled	O
to	O
learn	O
,	O
the	O
model	O
will	O
not	O
work	O
–	O
we	O
belabor	O
this	O
point	O
further	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Related	O
Work	O
subsection	O
:	O
Memory	Method
Augmented	Method
Neural	Method
Networks	Method
A	O
recent	O
surge	O
of	O
models	O
which	O
go	O
beyond	O
‘	O
‘	O
static	O
’	O
’	O
classification	O
of	O
fixed	O
vectors	O
onto	O
their	O
classes	O
has	O
reshaped	O
current	O
research	O
and	O
industrial	O
applications	O
alike	O
.	O

This	O
is	O
most	O
notable	O
in	O
the	O
massive	O
adoption	O
of	O
LSTMs	Method
in	O
a	O
variety	O
of	O
tasks	O
such	O
as	O
speech	Task
,	O
translation	O
or	O
learning	Task
programs	Task
.	O

A	O
key	O
component	O
which	O
allowed	O
for	O
more	O
expressive	Method
models	Method
was	O
the	O
introduction	O
of	O
‘	O
‘	O
content	O
’	O
’	O
based	O
attention	Method
in	O
,	O
and	O
‘	O
‘	O
computer	O
-	O
like	O
’	O
’	Method
architectures	Method
such	O
as	O
the	O
Neural	Method
Turing	Method
Machine	Method
or	O
Memory	Method
Networks	Method
.	O

Our	O
work	O
takes	O
the	O
metalearning	Method
paradigm	Method
of	O
,	O
where	O
an	O
LSTM	Method
learnt	O
to	O
learn	O
quickly	O
from	O
data	O
presented	O
sequentially	O
,	O
but	O
we	O
treat	O
the	O
data	O
as	O
a	O
set	O
.	O

The	O
one	Task
-	Task
shot	Task
learning	Task
task	O
we	O
defined	O
on	O
the	O
Penn	O
Treebank	O
relates	O
to	O
evaluation	Method
techniques	Method
and	O
models	O
presented	O
in	O
,	O
and	O
we	O
discuss	O
this	O
in	O
Section	O
[	O
reference	O
]	O
.	O

subsection	O
:	O
Metric	Method
Learning	Method
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
there	O
are	O
many	O
links	O
between	O
content	Method
based	Method
attention	Method
,	O
kernel	Method
based	Method
nearest	Method
neighbor	Method
and	O
metric	Method
learning	Method
.	O

The	O
most	O
relevant	O
work	O
is	O
Neighborhood	Method
Component	Method
Analysis	Method
(	O
NCA	Method
)	O
,	O
and	O
the	O
follow	Method
up	Method
non	Method
-	Method
linear	Method
version	Method
.	O

The	O
loss	O
is	O
very	O
similar	O
to	O
ours	O
,	O
except	O
we	O
use	O
the	O
whole	O
support	O
set	O
instead	O
of	O
pair	O
-	O
wise	O
comparisons	O
which	O
is	O
more	O
amenable	O
to	O
one	Task
-	Task
shot	Task
learning	Task
.	O

Follow	O
-	O
up	O
work	O
in	O
the	O
form	O
of	O
deep	Method
convolutional	Method
siamese	Method
networks	Method
included	O
much	O
more	O
powerful	O
non	Method
-	Method
linear	Method
mappings	Method
.	O

Other	O
losses	O
which	O
include	O
the	O
notion	O
of	O
a	O
set	O
(	O
but	O
use	O
less	O
powerful	O
metrics	O
)	O
were	O
proposed	O
in	O
.	O

Lastly	O
,	O
the	O
work	O
in	O
one	Task
-	Task
shot	Task
learning	Task
in	O
was	O
inspirational	O
and	O
also	O
provided	O
us	O
with	O
the	O
invaluable	O
Omniglot	Material
dataset	O
–	O
referred	O
to	O
as	O
the	O
‘	O
‘	O
transpose	O
’	O
’	O
of	O
MNIST	O
.	O

Other	O
works	O
used	O
zero	Method
-	Method
shot	Method
learning	Method
on	O
ImageNet	Material
,	O
e.g.	O
.	O

However	O
,	O
there	O
is	O
not	O
much	O
one	O
-	O
shot	O
literature	O
on	O
ImageNet	Material
,	O
which	O
we	O
hope	O
to	O
amend	O
via	O
our	O
benchmark	O
and	O
task	O
definitions	O
in	O
the	O
following	O
section	O
.	O

section	O
:	O
Experiments	O
In	O
this	O
section	O
we	O
describe	O
the	O
results	O
of	O
many	O
experiments	O
,	O
comparing	O
our	O
Matching	Method
Networks	Method
model	O
against	O
strong	O
baselines	O
.	O

All	O
of	O
our	O
experiments	O
revolve	O
around	O
the	O
same	O
basic	O
task	O
:	O
an	O
-	Task
way	Task
-	Task
shot	Task
learning	Task
task	Task
.	O

Each	O
method	O
is	O
providing	O
with	O
a	O
set	O
of	O
labelled	O
examples	O
from	O
each	O
of	O
classes	O
that	O
have	O
not	O
previously	O
been	O
trained	O
upon	O
.	O

The	O
task	O
is	O
then	O
to	O
classify	O
a	O
disjoint	O
batch	O
of	O
unlabelled	O
examples	O
into	O
one	O
of	O
these	O
classes	O
.	O

Thus	O
random	O
performance	O
on	O
this	O
task	O
stands	O
at	O
.	O

We	O
compared	O
a	O
number	O
of	O
alternative	O
models	O
,	O
as	O
baselines	O
,	O
to	O
Matching	Method
Networks	Method
.	O

Let	O
denote	O
the	O
held	O
-	O
out	O
subset	O
of	O
labels	O
which	O
we	O
only	O
use	O
for	O
one	Task
-	Task
shot	Task
.	O

Unless	O
otherwise	O
specified	O
,	O
training	O
is	O
always	O
on	O
,	O
and	O
test	O
in	O
one	O
-	O
shot	O
mode	O
on	O
.	O

We	O
ran	O
one	O
-	O
shot	O
experiments	O
on	O
three	O
data	O
sets	O
:	O
two	O
image	O
classification	O
sets	O
(	O
Omniglot	Material
and	O
ImageNet	Material
)	O
and	O
one	O
language	Task
modeling	O
(	O
Penn	O
Treebank	O
)	O
.	O

The	O
experiments	O
on	O
the	O
three	O
data	O
sets	O
comprise	O
a	O
diverse	O
set	O
of	O
qualities	O
in	O
terms	O
of	O
complexity	Metric
,	O
sizes	O
,	O
and	O
modalities	O
.	O

subsection	O
:	O
Image	Task
Classification	Task
Results	O
For	O
vision	Task
problems	O
,	O
we	O
considered	O
four	O
kinds	O
of	O
baselines	O
:	O
matching	Task
on	O
raw	O
pixels	O
,	O
matching	O
on	O
discriminative	O
features	O
from	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
classifier	Method
(	O
Baseline	Method
Classifier	Method
)	O
,	O
MANN	Method
,	O
and	O
our	O
reimplementation	Method
of	O
the	O
Convolutional	Method
Siamese	Method
Net	Method
.	O

The	O
baseline	Method
classifier	Method
was	O
trained	O
to	O
classify	O
an	O
image	O
into	O
one	O
of	O
the	O
original	O
classes	O
present	O
in	O
the	O
training	O
data	O
set	O
,	O
but	O
excluding	O
the	O
classes	O
so	O
as	O
not	O
to	O
give	O
it	O
an	O
unfair	O
advantage	O
(	O
i.e.	O
,	O
trained	O
to	O
classify	O
classes	O
in	O
)	O
.	O

We	O
then	O
took	O
this	O
network	O
and	O
used	O
the	O
features	O
from	O
the	O
last	O
layer	O
(	O
before	O
the	O
softmax	Method
)	O
for	O
nearest	Task
neighbour	Task
matching	Task
,	O
a	O
strategy	O
commonly	O
used	O
in	O
computer	O
vision	Task
which	O
has	O
achieved	O
excellent	O
results	O
across	O
many	O
tasks	O
.	O

Following	O
,	O
the	O
convolutional	Method
siamese	Method
nets	Method
were	O
trained	O
on	O
a	O
same	O
-	O
or	O
-	O
different	O
task	O
of	O
the	O
original	O
training	O
data	O
set	O
and	O
then	O
the	O
last	O
layer	O
was	O
used	O
for	O
nearest	Task
neighbour	Task
matching	Task
.	O

We	O
also	O
tried	O
further	O
fine	O
tuning	O
the	O
features	O
using	O
only	O
the	O
support	O
set	O
sampled	O
from	O
.	O

This	O
yields	O
massive	O
overfitting	O
,	O
but	O
given	O
that	O
our	O
networks	O
are	O
highly	O
regularized	Method
,	O
can	O
yield	O
extra	O
gains	O
.	O

Note	O
that	O
,	O
even	O
when	O
fine	Task
tuning	Task
,	O
the	O
setup	O
is	O
still	O
one	O
-	O
shot	O
,	O
as	O
only	O
a	O
single	O
example	O
per	O
class	O
from	O
is	O
used	O
.	O

subsubsection	O
:	O
Omniglot	Material
Omniglot	Material
consists	O
of	O
1623	O
characters	O
from	O
50	O
different	O
alphabets	O
.	O

Each	O
of	O
these	O
was	O
hand	O
drawn	O
by	O
20	O
different	O
people	O
.	O

The	O
large	O
number	O
of	O
classes	O
(	O
characters	O
)	O
with	O
relatively	O
few	O
data	O
per	O
class	O
(	O
20	O
)	O
,	O
makes	O
this	O
an	O
ideal	O
data	O
set	O
for	O
testing	O
small	Task
-	Task
scale	Task
one	Task
-	Task
shot	Task
classification	Task
.	O

The	O
-	O
way	O
Omniglot	Material
task	O
setup	O
is	O
as	O
follows	O
:	O
pick	O
unseen	O
character	O
classes	O
,	O
independent	O
of	O
alphabet	O
,	O
as	O
.	O

Provide	O
the	O
model	O
with	O
one	O
drawing	O
of	O
each	O
of	O
the	O
characters	O
as	O
and	O
a	O
batch	O
.	O

Following	O
,	O
we	O
augmented	O
the	O
data	O
set	O
with	O
random	O
rotations	O
by	O
multiples	O
of	O
90	O
degrees	O
and	O
used	O
1200	O
characters	O
for	O
training	O
,	O
and	O
the	O
remaining	O
character	O
classes	O
for	O
evaluation	O
.	O

We	O
used	O
a	O
simple	O
yet	O
powerful	O
CNN	Method
as	O
the	O
embedding	Method
function	Method
–	O
consisting	O
of	O
a	O
stack	Method
of	Method
modules	Method
,	O
each	O
of	O
which	O
is	O
a	O
convolution	Method
with	Method
64	Method
filters	Method
followed	O
by	O
batch	Method
normalization	Method
,	O
a	O
Relu	Method
non	Method
-	Method
linearity	Method
and	O
max	Method
-	Method
pooling	Method
.	O

We	O
resized	O
all	O
the	O
images	O
to	O
so	O
that	O
,	O
when	O
we	O
stack	O
4	O
modules	O
,	O
the	O
resulting	O
feature	O
map	O
is	O
,	O
resulting	O
in	O
our	O
embedding	O
function	O
.	O

A	O
fully	Method
connected	Method
layer	Method
followed	O
by	O
a	O
softmax	Method
non	Method
-	Method
linearity	Method
is	O
used	O
to	O
define	O
the	O
Baseline	Method
Classifier	Method
.	O

Results	O
comparing	O
the	O
baselines	O
to	O
our	O
model	O
on	O
Omniglot	Material
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

For	O
both	O
-	O
shot	O
and	O
-	O
shot	O
,	O
-	O
way	O
and	O
-	O
way	O
,	O
our	O
model	O
outperforms	O
the	O
baselines	O
.	O

There	O
are	O
no	O
major	O
surprises	O
in	O
these	O
results	O
:	O
using	O
more	O
examples	O
for	O
k	Task
-	Task
shot	Task
classification	Task
helps	O
all	O
models	O
,	O
and	O
5	Method
-	Method
way	Method
is	O
easier	O
than	O
20	O
-	O
way	O
.	O

We	O
note	O
that	O
the	O
Baseline	O
Classifier	O
improves	O
a	O
bit	O
when	O
fine	O
tuning	O
on	O
,	O
and	O
using	O
cosine	O
distance	O
versus	O
training	O
a	O
small	O
softmax	O
from	O
the	O
small	O
training	O
set	O
(	O
thus	O
requiring	O
fine	Method
tuning	Method
)	O
also	O
performs	O
well	O
.	O

Siamese	Method
nets	Method
fare	O
well	O
versus	O
our	O
Matching	Method
Nets	Method
when	O
using	O
5	O
examples	O
per	O
class	O
,	O
but	O
their	O
performance	O
degrades	O
rapidly	O
in	O
one	O
-	O
shot	O
.	O

Fully	Method
Conditional	Method
Embeddings	Method
(	O
FCE	Method
)	O
did	O
not	O
seem	O
to	O
help	O
much	O
and	O
were	O
left	O
out	O
of	O
the	O
table	O
due	O
to	O
space	O
constraints	O
.	O

Like	O
the	O
authors	O
in	O
,	O
we	O
also	O
test	O
our	O
method	O
trained	O
on	O
Omniglot	Material
on	O
a	O
completely	O
disjoint	O
task	O
–	O
one	Task
-	Task
shot	Task
,	Task
10	Task
way	Task
MNIST	Task
classification	Task
.	O

The	O
Baseline	O
Classifier	O
does	O
about	O
63	O
%	O
accuracy	Metric
whereas	O
(	O
as	O
reported	O
in	O
their	O
paper	O
)	O
the	O
Siamese	Method
Nets	Method
do	O
70	O
%	O
.	O

Our	O
model	O
achieves	O
72	O
%	O
.	O

subsubsection	O
:	O
ImageNet	Material
Our	O
experiments	O
followed	O
the	O
same	O
setup	O
as	O
Omniglot	Material
for	O
testing	O
,	O
but	O
we	O
considered	O
a	O
rand	O
and	O
a	O
dogs	O
(	O
harder	O
)	O
setup	O
.	O

In	O
the	O
rand	O
setup	O
,	O
we	O
removed	O
118	O
labels	O
at	O
random	O
from	O
the	O
training	O
set	O
,	O
then	O
tested	O
only	O
on	O
these	O
118	O
classes	O
(	O
which	O
we	O
denote	O
as	O
)	O
.	O

For	O
the	O
dogs	Task
setup	Task
,	O
we	O
removed	O
all	O
classes	O
in	O
ImageNet	Material
descended	O
from	O
dogs	O
(	O
totalling	O
118	O
)	O
and	O
trained	O
on	O
all	O
non	O
-	O
dog	O
classes	O
,	O
then	O
tested	O
on	O
dog	O
classes	O
(	O
)	O
.	O

ImageNet	Material
is	O
a	O
notoriously	O
large	O
data	O
set	O
which	O
can	O
be	O
quite	O
a	O
feat	O
of	O
engineering	O
and	O
infrastructure	O
to	O
run	O
experiments	O
upon	O
it	O
,	O
requiring	O
many	O
resources	O
.	O

Thus	O
,	O
as	O
well	O
as	O
using	O
the	O
full	O
ImageNet	Material
data	O
set	O
,	O
we	O
devised	O
a	O
new	O
data	O
set	O
–	O
mini	O
ImageNet	Material
–	O
consisting	O
of	O
colour	Material
images	Material
of	O
size	O
with	O
classes	O
,	O
each	O
having	O
examples	O
.	O

This	O
dataset	O
is	O
more	O
complex	O
than	O
CIFAR10	O
,	O
but	O
fits	O
in	O
memory	O
on	O
modern	O
machines	O
,	O
making	O
it	O
very	O
convenient	O
for	O
rapid	O
prototyping	O
and	O
experimentation	O
.	O

This	O
dataset	O
is	O
fully	O
described	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O

We	O
used	O
classes	O
for	O
training	O
and	O
tested	O
on	O
the	O
remaining	O
classes	O
.	O

In	O
total	O
,	O
thus	O
,	O
we	O
have	O
rand	Method
ImageNet	Method
,	O
dogs	O
ImageNet	Material
,	O
and	O
mini	O
ImageNet	Material
.	O

The	O
results	O
of	O
the	O
mini	O
ImageNet	Material
experiments	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

As	O
with	O
Omniglot	Material
,	O
Matching	Method
Networks	Method
outperform	O
the	O
baselines	O
.	O

However	O
,	O
mini	O
ImageNet	Material
is	O
a	O
much	O
harder	O
task	O
than	O
Omniglot	Material
which	O
allowed	O
us	O
to	O
evaluate	O
Full	O
Contextual	O
Embeddings	O
(	O
FCE	Method
)	O
sensibly	O
(	O
on	O
Omniglot	Material
it	O
made	O
no	O
difference	O
)	O
.	O

As	O
we	O
an	O
see	O
,	O
FCE	Method
improves	O
the	O
performance	O
of	O
Matching	Method
Networks	Method
,	O
with	O
and	O
without	O
fine	Method
tuning	Method
,	O
typically	O
improving	O
performance	O
by	O
around	O
two	O
percentage	O
points	O
.	O

Next	O
we	O
turned	O
to	O
experiments	O
based	O
upon	O
full	O
size	O
,	O
full	O
scale	O
ImageNet	Material
.	O

Our	O
baseline	O
classifier	O
for	O
this	O
data	O
set	O
was	O
Inception	O
trained	O
to	O
classify	O
on	O
all	O
classes	O
except	O
those	O
in	O
the	O
test	O
set	O
of	O
classes	O
(	O
for	O
rand	Method
ImageNet	Method
)	O
or	O
those	O
concerning	O
dogs	O
(	O
for	O
dogs	O
ImageNet	Material
)	O
.	O

We	O
also	O
compared	O
to	O
features	O
from	O
an	O
Inception	Method
Oracle	Method
classifier	Method
trained	O
on	O
all	O
classes	O
in	O
ImageNet	Material
,	O
as	O
an	O
upper	O
bound	O
.	O

Our	O
Baseline	O
Classifier	O
is	O
one	O
of	O
the	O
strongest	O
published	O
ImageNet	Material
models	O
at	O
79	O
%	O
top	Metric
-	Metric
1	Metric
accuracy	Metric
on	O
the	O
standard	O
ImageNet	Material
validation	O
set	O
.	O

Instead	O
of	O
training	O
Matching	Method
Networks	Method
from	O
scratch	O
on	O
these	O
large	O
tasks	O
,	O
we	O
initialised	O
their	O
feature	Method
extractors	Method
and	O
with	O
the	O
parameters	O
from	O
the	O
Inception	Method
classifier	Method
(	O
pretrained	O
on	O
the	O
appropriate	O
subset	O
of	O
the	O
data	O
)	O
and	O
then	O
further	O
trained	O
the	O
resulting	O
network	O
on	O
random	Task
-	Task
way	Task
-	Task
shot	Task
tasks	Task
from	O
the	O
training	O
data	O
set	O
,	O
incorporating	O
Full	Method
Context	Method
Embeddings	Method
and	O
our	O
Matching	Method
Networks	Method
and	O
training	Method
strategy	Method
.	O

The	O
results	O
of	O
the	O
rand	Method
ImageNet	Method
and	O
dogs	Method
ImageNet	Method
experiments	Method
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
Inception	Method
Oracle	Method
(	O
trained	O
on	O
all	O
classes	O
)	O
performs	O
almost	O
perfectly	O
when	O
restricted	O
to	O
5	O
classes	O
only	O
,	O
which	O
is	O
not	O
too	O
surprising	O
given	O
its	O
impressive	O
top	O
-	O
1	O
accuracy	Metric
.	O

When	O
trained	O
solely	O
on	O
,	O
Matching	Method
Nets	Method
improve	O
upon	O
Inception	O
by	O
almost	O
when	O
tested	O
on	O
,	O
halving	O
the	O
errors	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
two	O
instances	O
of	O
5	Task
-	Task
way	Task
one	Task
-	Task
shot	Task
learning	Task
,	O
where	O
Inception	O
fails	O
.	O

Looking	O
at	O
all	O
the	O
errors	O
,	O
Inception	O
appears	O
to	O
sometimes	O
prefer	O
an	O
image	O
above	O
all	O
others	O
(	O
these	O
images	O
tend	O
to	O
be	O
cluttered	O
like	O
the	O
example	O
in	O
the	O
second	O
column	O
,	O
or	O
more	O
constant	O
in	O
color	O
)	O
.	O

Matching	Method
Nets	Method
,	O
on	O
the	O
other	O
hand	O
,	O
manage	O
to	O
recover	O
from	O
these	O
outliers	O
that	O
sometimes	O
appear	O
in	O
the	O
support	O
set	O
.	O

Matching	Method
Nets	Method
manage	O
to	O
improve	O
upon	O
Inception	O
on	O
the	O
complementary	O
subset	O
(	O
although	O
this	O
setup	O
is	O
not	O
one	O
-	O
shot	O
,	O
as	O
the	O
feature	Method
extraction	Method
has	O
been	O
trained	O
on	O
these	O
labels	O
)	O
.	O

However	O
,	O
on	O
the	O
much	O
more	O
challenging	O
subset	O
,	O
our	O
model	O
degrades	O
by	O
.	O

We	O
hypothesize	O
this	O
to	O
the	O
fact	O
that	O
the	O
sampled	O
set	O
during	O
training	O
,	O
,	O
comes	O
from	O
a	O
random	O
distribution	O
of	O
labels	O
(	O
from	O
)	O
,	O
whereas	O
the	O
testing	O
support	O
set	O
from	O
contains	O
similar	O
classes	O
,	O
more	O
akin	O
to	O
fine	Task
grained	Task
classification	Task
.	O

Thus	O
,	O
we	O
believe	O
that	O
if	O
we	O
adapted	O
our	O
training	Method
strategy	Method
to	O
samples	O
from	O
fine	O
grained	O
sets	O
of	O
labels	O
instead	O
of	O
sampling	O
uniformly	O
from	O
the	O
leafs	O
of	O
the	O
ImageNet	Material
class	O
tree	O
,	O
improvements	O
could	O
be	O
attained	O
.	O

We	O
leave	O
this	O
as	O
future	O
work	O
.	O

subsubsection	O
:	O
One	Method
-	Method
Shot	Method
Language	Method
Modeling	Method
We	O
also	O
introduce	O
a	O
new	O
one	Task
-	Task
shot	Task
language	Task
task	Task
which	O
is	O
analogous	O
to	O
those	O
examined	O
for	O
images	O
.	O

The	O
task	O
is	O
as	O
follows	O
:	O
given	O
a	O
query	O
sentence	O
with	O
a	O
missing	O
word	O
in	O
it	O
,	O
and	O
a	O
support	O
set	O
of	O
sentences	O
which	O
each	O
have	O
a	O
missing	O
word	O
and	O
a	O
corresponding	O
1	O
-	O
hot	O
label	O
,	O
choose	O
the	O
label	O
from	O
the	O
support	O
set	O
that	O
best	O
matches	O
the	O
query	O
sentence	O
.	O

Here	O
we	O
show	O
a	O
single	O
example	O
,	O
though	O
note	O
that	O
the	O
words	O
on	O
the	O
right	O
are	O
not	O
provided	O
and	O
the	O
labels	O
for	O
the	O
set	O
are	O
given	O
as	O
1	O
-	O
hot	O
-	O
of	O
-	O
5	O
vectors	O
.	O

Sentences	O
were	O
taken	O
from	O
the	O
Penn	O
Treebank	O
dataset	O
.	O

On	O
each	O
trial	O
,	O
we	O
make	O
sure	O
that	O
the	O
set	O
and	O
batch	O
are	O
populated	O
with	O
sentences	O
that	O
are	O
non	O
-	O
overlapping	O
.	O

This	O
means	O
that	O
we	O
do	O
not	O
use	O
words	O
with	O
very	O
low	O
frequency	O
counts	O
;	O
e.g.	O
if	O
there	O
is	O
only	O
a	O
single	O
sentence	O
for	O
a	O
given	O
word	O
we	O
do	O
not	O
use	O
this	O
data	O
since	O
the	O
sentence	O
would	O
need	O
to	O
be	O
in	O
both	O
the	O
set	O
and	O
the	O
batch	O
.	O

As	O
with	O
the	O
image	Task
tasks	Task
,	O
each	O
trial	O
consisted	O
of	O
a	O
5	O
way	O
choice	O
between	O
the	O
classes	O
available	O
in	O
the	O
set	O
.	O

We	O
used	O
a	O
batch	O
size	O
of	O
20	O
throughout	O
the	O
sentence	Task
matching	Task
task	Task
and	O
varied	O
the	O
set	O
size	O
across	O
k=1	O
,	O
2	O
,	O
3	O
.	O

We	O
ensured	O
that	O
the	O
same	O
number	O
of	O
sentences	O
were	O
available	O
for	O
each	O
class	O
in	O
the	O
set	O
.	O

We	O
split	O
the	O
words	O
into	O
a	O
randomly	O
sampled	O
9000	O
for	O
training	O
and	O
1000	O
for	O
testing	O
,	O
and	O
we	O
used	O
the	O
standard	O
test	O
set	O
to	O
report	O
results	O
.	O

Thus	O
,	O
neither	O
the	O
words	O
nor	O
the	O
sentences	O
used	O
during	O
test	O
time	O
had	O
been	O
seen	O
during	O
training	O
.	O

We	O
compared	O
our	O
one	Method
-	Method
shot	Method
matching	Method
model	Method
to	O
an	O
oracle	O
LSTM	Method
language	Task
model	O
(	O
LSTM	Method
-	O
LM	O
)	O
trained	O
on	O
all	O
the	O
words	O
.	O

In	O
this	O
setup	O
,	O
the	O
LSTM	Method
has	O
an	O
unfair	O
advantage	O
as	O
it	O
is	O
not	O
doing	O
one	Task
-	Task
shot	Task
learning	Task
but	O
seeing	O
all	O
the	O
data	O
–	O
thus	O
,	O
this	O
should	O
be	O
taken	O
as	O
an	O
upper	O
bound	O
.	O

To	O
do	O
so	O
,	O
we	O
examined	O
a	O
similar	O
setup	O
wherein	O
a	O
sentence	O
was	O
presented	O
to	O
the	O
model	O
with	O
a	O
single	O
word	O
filled	O
in	O
with	O
5	O
different	O
possible	O
words	O
(	O
including	O
the	O
correct	O
answer	O
)	O
.	O

For	O
each	O
of	O
these	O
5	O
sentences	O
the	O
model	O
gave	O
a	O
log	O
-	O
likelihood	O
and	O
the	O
max	O
of	O
these	O
was	O
taken	O
to	O
be	O
the	O
choice	O
of	O
the	O
model	O
.	O

As	O
with	O
the	O
other	O
5	O
way	Task
choice	Task
tasks	Task
,	O
chance	O
performance	O
on	O
this	O
task	O
was	O
20	O
%	O
.	O

The	O
LSTM	Method
language	Task
model	O
oracle	O
achieved	O
an	O
upper	O
bound	O
of	O
72.8	O
%	O
accuracy	Metric
on	O
the	O
test	O
set	O
.	O

Matching	Method
Networks	Method
with	O
a	O
simple	O
encoding	Method
model	Method
achieve	O
32.4	O
%	O
,	O
36.1	O
%	O
,	O
38.2	O
%	O
accuracy	Metric
on	O
the	O
task	O
with	O
examples	O
in	O
the	O
set	O
,	O
respectively	O
.	O

Future	O
work	O
should	O
explore	O
combining	O
parametric	Method
models	Method
such	O
as	O
an	O
LSTM	Method
-	O
LM	O
with	O
non	Method
-	Method
parametric	Method
components	Method
such	O
as	O
the	O
Matching	Method
Networks	Method
explored	O
here	O
.	O

Two	O
related	O
tasks	O
are	O
the	O
CNN	Method
QA	O
test	O
of	O
entity	Task
prediction	Task
from	O
news	O
articles	O
,	O
and	O
the	O
Children	O
’s	O
Book	O
Test	O
(	O
CBT	O
)	O
.	O

In	O
the	O
CBT	O
for	O
example	O
,	O
a	O
sequence	O
of	O
sentences	O
from	O
a	O
book	O
are	O
provided	O
as	O
context	O
.	O

In	O
the	O
final	O
sentence	O
one	O
of	O
the	O
words	O
,	O
which	O
has	O
appeared	O
in	O
a	O
previous	O
sentence	O
,	O
is	O
missing	O
.	O

The	O
task	O
is	O
to	O
choose	O
the	O
correct	O
word	O
to	O
fill	O
in	O
this	O
blank	O
from	O
a	O
small	O
set	O
of	O
words	O
given	O
as	O
possible	O
answers	O
,	O
all	O
of	O
which	O
occur	O
in	O
the	O
preceding	O
sentences	O
.	O

In	O
our	O
sentence	Task
matching	Task
task	Task
the	O
sentences	O
provided	O
in	O
the	O
set	O
are	O
randomly	O
drawn	O
from	O
the	O
PTB	O
corpus	O
and	O
are	O
related	O
to	O
the	O
sentences	O
in	O
the	O
query	O
batch	O
only	O
by	O
the	O
fact	O
that	O
they	O
share	O
a	O
word	O
.	O

In	O
contrast	O
to	O
CBT	O
and	O
CNN	Method
dataset	O
,	O
they	O
provide	O
only	O
a	O
generic	O
rather	O
than	O
specific	O
sequential	O
context	O
.	O

section	O
:	O
Conclusion	O
In	O
this	O
paper	O
we	O
introduced	O
Matching	Method
Networks	Method
,	O
a	O
new	O
neural	Method
architecture	Method
that	O
,	O
by	O
way	O
of	O
its	O
corresponding	O
training	O
regime	O
,	O
is	O
capable	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
variety	O
of	O
one	Task
-	Task
shot	Task
classification	Task
tasks	Task
.	O

There	O
are	O
a	O
few	O
key	O
insights	O
in	O
this	O
work	O
.	O

Firstly	O
,	O
one	Task
-	Task
shot	Task
learning	Task
is	O
much	O
easier	O
if	O
you	O
train	O
the	O
network	O
to	O
do	O
one	Task
-	Task
shot	Task
learning	Task
.	O

Secondly	O
,	O
non	O
-	O
parametric	O
structures	O
in	O
a	O
neural	Method
network	Method
make	O
it	O
easier	O
for	O
networks	O
to	O
remember	O
and	O
adapt	O
to	O
new	O
training	O
sets	O
in	O
the	O
same	O
tasks	O
.	O

Combining	O
these	O
observations	O
together	O
yields	O
Matching	Method
Networks	Method
.	O

Further	O
,	O
we	O
have	O
defined	O
new	O
one	Task
-	Task
shot	Task
tasks	Task
on	O
ImageNet	Material
,	O
a	O
reduced	O
version	O
of	O
ImageNet	Material
(	O
for	O
rapid	O
experimentation	O
)	O
,	O
and	O
a	O
language	Task
modeling	O
task	O
.	O

An	O
obvious	O
drawback	O
of	O
our	O
model	O
is	O
the	O
fact	O
that	O
,	O
as	O
the	O
support	O
set	O
grows	O
in	O
size	O
,	O
the	O
computation	O
for	O
each	O
gradient	Task
update	Task
becomes	O
more	O
expensive	O
.	O

Although	O
there	O
are	O
sparse	Method
and	Method
sampling	Method
-	Method
based	Method
methods	Method
to	O
alleviate	O
this	O
,	O
much	O
of	O
our	O
future	O
efforts	O
will	O
concentrate	O
around	O
this	O
limitation	O
.	O

Further	O
,	O
as	O
exemplified	O
in	O
the	O
ImageNet	Material
dogs	O
subtask	O
,	O
when	O
the	O
label	O
distribution	O
has	O
obvious	O
biases	O
(	O
such	O
as	O
being	O
fine	O
grained	O
)	O
,	O
our	O
model	O
suffers	O
.	O

We	O
feel	O
this	O
is	O
an	O
area	O
with	O
exciting	O
challenges	O
which	O
we	O
hope	O
to	O
keep	O
improving	O
in	O
future	O
work	O
.	O

section	O
:	O
Acknowledgements	O
We	O
would	O
like	O
to	O
thank	O
Nal	O
Kalchbrenner	O
for	O
brainstorming	O
around	O
the	O
design	O
of	O
the	O
function	O
,	O
and	O
Sander	O
Dieleman	O
and	O
Sergio	O
Guadarrama	O
for	O
their	O
help	O
setting	O
up	O
ImageNet	Material
.	O

We	O
would	O
also	O
like	O
thank	O
Simon	O
Osindero	O
for	O
useful	O
discussions	O
around	O
the	O
tasks	O
discussed	O
in	O
this	O
paper	O
,	O
and	O
Theophane	O
Weber	O
and	O
Remi	O
Munos	O
for	O
following	O
some	O
early	O
developments	O
.	O

Karen	O
Simonyan	O
and	O
David	O
Silver	O
helped	O
with	O
the	O
manuscript	O
,	O
as	O
well	O
as	O
many	O
at	O
Google	O
DeepMind	O
.	O

Thanks	O
also	O
to	O
Geoff	O
Hinton	O
and	O
Alex	O
Toshev	O
for	O
discussions	O
about	O
our	O
results	O
.	O

plus	O
1pt	O
bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
appendix	O
:	O
Model	O
Description	O
In	O
this	O
section	O
we	O
fully	O
specify	O
the	O
models	O
which	O
condition	O
the	O
embedding	O
functions	O
and	O
on	O
the	O
whole	O
support	O
set	O
.	O

Much	O
previous	O
work	O
has	O
fully	O
described	O
similar	O
mechanisms	O
,	O
which	O
is	O
why	O
we	O
left	O
the	O
precise	O
details	O
for	O
this	O
appendix	O
.	O

subsection	O
:	O
The	O
Fully	Method
Conditional	Method
Embedding	Method
As	O
described	O
in	O
section	O
[	O
reference	O
]	O
,	O
the	O
embedding	O
function	O
for	O
an	O
example	O
in	O
the	O
batch	O
is	O
as	O
follows	O
:	O
where	O
is	O
a	O
neural	Method
network	Method
(	O
e.g.	O
,	O
VGG	Method
or	O
Inception	O
,	O
as	O
described	O
in	O
the	O
main	O
text	O
)	O
.	O

We	O
define	O
to	O
be	O
the	O
number	O
of	O
‘	O
‘	O
processing	O
’	O
’	O
steps	O
following	O
work	O
from	O
from	O
their	O
‘	O
‘	O
Process	O
’	O
’	O
block	O
.	O

represents	O
the	O
embedding	O
function	O
applied	O
to	O
each	O
element	O
from	O
the	O
set	O
.	O

Thus	O
,	O
the	O
state	O
after	O
processing	O
steps	O
is	O
as	O
follows	O
:	O
noting	O
that	O
follows	O
the	O
same	O
LSTM	Method
implementation	O
defined	O
in	O
with	O
the	O
input	O
,	O
the	O
output	O
(	O
i.e.	O
,	O
cell	O
after	O
the	O
output	O
gate	O
)	O
,	O
and	O
the	O
cell	O
.	O

is	O
commonly	O
referred	O
to	O
as	O
‘	O
‘	O
content	O
’	O
’	O
based	O
attention	O
,	O
and	O
the	O
softmax	O
in	O
eq	O
.	O

[	O
reference	O
]	O
normalizes	O
w.r.t	O
.	O

.	O

The	O
read	O
-	O
out	O
from	O
is	O
concatenated	O
to	O
.	O

Since	O
we	O
do	O
steps	O
of	O
‘	O
‘	O
reads	O
’	O
’	O
,	O
where	O
is	O
as	O
described	O
in	O
eq	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
The	O
Fully	Method
Conditional	Method
Embedding	Method
In	O
section	O
[	O
reference	O
]	O
we	O
described	O
the	O
encoding	Method
function	Method
for	O
the	O
elements	O
in	O
the	O
support	O
set	O
,	O
,	O
as	O
a	O
bidirectional	O
LSTM	Method
.	O

More	O
precisely	O
,	O
let	O
be	O
a	O
neural	Method
network	Method
(	O
similar	O
to	O
above	O
,	O
e.g.	O
a	O
VGG	Method
or	O
Inception	Method
model	Method
)	O
.	O

Then	O
we	O
define	O
with	O
:	O
where	O
,	O
as	O
in	O
above	O
,	O
follows	O
the	O
same	O
LSTM	Method
implementation	O
defined	O
in	O
with	O
the	O
input	O
,	O
the	O
output	O
(	O
i.e.	O
,	O
cell	O
after	O
the	O
output	O
gate	O
)	O
,	O
and	O
the	O
cell	O
.	O

Note	O
that	O
the	O
recursion	O
for	O
starts	O
from	O
.	O

As	O
in	O
eq	O
.	O

[	O
reference	O
]	O
,	O
we	O
add	O
a	O
skip	O
connection	O
between	O
input	O
and	O
outputs	O
.	O

appendix	O
:	O
mini	O
ImageNet	Material
Description	O
To	O
construct	O
mini	O
ImageNet	Material
we	O
chose	O
100	O
random	O
classes	O
from	O
ImageNet	Material
,	O
and	O
used	O
the	O
first	O
80	O
for	O
training	O
,	O
and	O
the	O
last	O
20	O
for	O
testing	O
.	O

This	O
split	O
was	O
used	O
in	O
our	O
one	O
-	O
shot	O
experiments	O
described	O
in	O
section	O
[	O
reference	O
]	O
.	O

Note	O
that	O
the	O
last	O
20	O
class	O
objects	O
were	O
never	O
seen	O
during	O
training	O
.	O

For	O
the	O
exact	O
600	O
images	O
that	O
comprise	O
each	O
of	O
the	O
100	O
classes	O
in	O
mini	O
ImageNet	Material
,	O
please	O
see	O
the	O
following	O
text	O
file	O
:	O
n01614925	O
,	O
n01632777	O
,	O
n01641577	O
,	O
n01664065	O
,	O
n01687978	O
,	O
n01695060	O
,	O
n01729322	O
,	O
n01773157	O
,	O
n01833805	O
,	O
n01871265	O
,	O
n01877812	O
,	O
n01978455	O
,	O
n01986214	O
,	O
n02013706	O
,	O
n02066245	O
,	O
n02071294	O
,	O
n02088466	O
,	O
n02090379	O
,	O
n02091635	O
,	O
n02096437	O
,	O
n02097130	O
,	O
n02099429	O
,	O
n02108089	O
,	O
n02108915	O
,	O
n02109047	O
,	O
n02109525	O
,	O
n02111889	O
,	O
n02115641	O
,	O
n02123045	O
,	O
n02129165	O
,	O
n02167151	O
,	O
n02206856	O
,	O
n02264363	O
,	O
n02279972	O
,	O
n02342885	O
,	O
n02346627	O
,	O
n02364673	O
,	O
n02454379	O
,	O
n02481823	O
,	O
n02486261	O
,	O
n02494079	O
,	O
n02655020	O
,	O
n02793495	O
,	O
n02804414	O
,	O
n02808304	O
,	O
n02837789	O
,	O
n02895154	O
,	O
n02909870	O
,	O
n02917067	O
,	O
n02966687	O
,	O
n03000684	O
,	O
n03014705	O
,	O
n03041632	O
,	O
n03045698	O
,	O
n03065424	O
,	O
n03180011	O
,	O
n03216828	O
,	O
n03355925	O
,	O
n03384352	O
,	O
n03424325	O
,	O
n03452741	O
,	O
n03482405	O
,	O
n03494278	O
,	O
n03594734	O
,	O
n03599486	O
,	O
n03630383	O
,	O
n03649909	O
,	O
n03676483	O
,	O
n03690938	O
,	O
n03742115	O
,	O
n03868242	O
,	O
n03877472	O
,	O
n03976467	O
,	O
n03976657	O
,	O
n03998194	O
,	O
n04026417	O
,	O
n04069434	O
,	O
n04111531	O
,	O
n04118538	O
,	O
n04200800	O
n04201297	O
,	O
n04204347	O
,	O
n04239074	O
,	O
n04277352	O
,	O
n04370456	O
,	O
n04409515	O
,	O
n04456115	O
,	O
n04479046	O
,	O
n04487394	O
,	O
n04525038	O
,	O
n04591713	O
,	O
n04599235	O
,	O
n07565083	O
,	O
n07613480	O
,	O
n07695742	O
,	O
n07714571	O
,	O
n07717410	O
,	O
n07753275	O
,	O
n10148035	O
,	O
n12768682	O
appendix	O
:	O
ImageNet	Material
Class	O
Splits	O
Here	O
we	O
define	O
the	O
two	O
class	O
splits	O
used	O
in	O
our	O
full	O
ImageNet	Material
experiments	O
–	O
these	O
classes	O
were	O
excluded	O
for	O
training	O
during	O
our	O
one	O
-	O
shot	O
experiments	O
described	O
in	O
section	O
[	O
reference	O
]	O
.	O

n01498041	O
,	O
n01537544	O
,	O
n01580077	O
,	O
n01592084	O
,	O
n01632777	O
,	O
n01644373	O
,	O
n01665541	O
,	O
n01675722	O
,	O
n01688243	O
,	O
n01729977	O
,	O
n01775062	O
,	O
n01818515	O
,	O
n01843383	O
,	O
n01883070	O
,	O
n01950731	O
,	O
n02002724	O
,	O
n02013706	O
,	O
n02092339	O
,	O
n02093256	O
,	O
n02095314	O
,	O
n02097130	O
,	O
n02097298	O
,	O
n02098413	O
,	O
n02101388	O
,	O
n02106382	O
,	O
n02108089	O
,	O
n02110063	O
,	O
n02111129	O
,	O
n02111500	O
,	O
n02112350	O
,	O
n02115913	O
,	O
n02117135	O
,	O
n02120505	O
,	O
n02123045	O
,	O
n02125311	O
,	O
n02134084	O
,	O
n02167151	O
,	O
n02190166	O
,	O
n02206856	O
,	O
n02231487	O
,	O
n02256656	O
,	O
n02398521	O
,	O
n02480855	O
,	O
n02481823	O
,	O
n02490219	O
,	O
n02607072	O
,	O
n02666196	O
,	O
n02672831	O
,	O
n02704792	O
,	O
n02708093	O
,	O
n02814533	O
,	O
n02817516	O
,	O
n02840245	O
,	O
n02843684	O
,	O
n02870880	O
,	O
n02877765	O
,	O
n02966193	O
,	O
n03016953	O
,	O
n03017168	O
,	O
n03026506	O
,	O
n03047690	O
,	O
n03095699	O
,	O
n03134739	O
,	O
n03179701	O
,	O
n03255030	O
,	O
n03388183	O
,	O
n03394916	O
,	O
n03424325	O
,	O
n03467068	O
,	O
n03476684	O
,	O
n03483316	O
,	O
n03627232	O
,	O
n03658185	O
,	O
n03710193	O
,	O
n03721384	O
,	O
n03733131	O
,	O
n03785016	O
,	O
n03786901	O
,	O
n03792972	O
,	O
n03794056	O
,	O
n03832673	O
,	O
n03843555	O
,	O
n03877472	O
,	O
n03899768	O
,	O
n03930313	O
,	O
n03935335	O
,	O
n03954731	O
,	O
n03995372	O
,	O
n04004767	O
,	O
n04037443	O
,	O
n04065272	O
,	O
n04069434	O
,	O
n04090263	O
,	O
n04118538	O
,	O
n04120489	O
,	O
n04141975	O
,	O
n04152593	O
,	O
n04154565	O
,	O
n04204347	O
,	O
n04208210	O
,	O
n04209133	O
,	O
n04258138	O
,	O
n04311004	O
,	O
n04326547	O
,	O
n04367480	O
,	O
n04447861	O
,	O
n04483307	O
,	O
n04522168	O
,	O
n04548280	O
,	O
n04554684	O
,	O
n04597913	O
,	O
n04612504	O
,	O
n07695742	O
,	O
n07697313	O
,	O
n07697537	O
,	O
n07716906	O
,	O
n12998815	O
,	O
n13133613	O
n02085620	O
,	O
n02085782	O
,	O
n02085936	O
,	O
n02086079	O
,	O
n02086240	O
,	O
n02086646	O
,	O
n02086910	O
,	O
n02087046	O
,	O
n02087394	O
,	O
n02088094	O
,	O
n02088238	O
,	O
n02088364	O
,	O
n02088466	O
,	O
n02088632	O
,	O
n02089078	O
,	O
n02089867	O
,	O
n02089973	O
,	O
n02090379	O
,	O
n02090622	O
,	O
n02090721	O
,	O
n02091032	O
,	O
n02091134	O
,	O
n02091244	O
,	O
n02091467	O
,	O
n02091635	O
,	O
n02091831	O
,	O
n02092002	O
,	O
n02092339	O
,	O
n02093256	O
,	O
n02093428	O
,	O
n02093647	O
,	O
n02093754	O
,	O
n02093859	O
,	O
n02093991	O
,	O
n02094114	O
,	O
n02094258	O
,	O
n02094433	O
,	O
n02095314	O
,	O
n02095570	O
,	O
n02095889	O
,	O
n02096051	O
,	O
n02096177	O
,	O
n02096294	O
,	O
n02096437	O
,	O
n02096585	O
,	O
n02097047	O
,	O
n02097130	O
,	O
n02097209	O
,	O
n02097298	O
,	O
n02097474	O
,	O
n02097658	O
,	O
n02098105	O
,	O
n02098286	O
,	O
n02098413	O
,	O
n02099267	O
,	O
n02099429	O
,	O
n02099601	O
,	O
n02099712	O
,	O
n02099849	O
,	O
n02100236	O
,	O
n02100583	O
,	O
n02100735	O
,	O
n02100877	O
,	O
n02101006	O
,	O
n02101388	O
,	O
n02101556	O
,	O
n02102040	O
,	O
n02102177	O
,	O
n02102318	O
,	O
n02102480	O
,	O
n02102973	O
,	O
n02104029	O
,	O
n02104365	O
,	O
n02105056	O
,	O
n02105162	O
,	O
n02105251	O
,	O
n02105412	O
,	O
n02105505	O
,	O
n02105641	O
,	O
n02105855	O
,	O
n02106030	O
,	O
n02106166	O
,	O
n02106382	O
,	O
n02106550	O
,	O
n02106662	O
,	O
n02107142	O
,	O
n02107312	O
,	O
n02107574	O
,	O
n02107683	O
,	O
n02107908	O
,	O
n02108000	O
,	O
n02108089	O
,	O
n02108422	O
,	O
n02108551	O
,	O
n02108915	O
,	O
n02109047	O
,	O
n02109525	O
,	O
n02109961	O
,	O
n02110063	O
,	O
n02110185	O
,	O
n02110341	O
,	O
n02110627	O
,	O
n02110806	O
,	O
n02110958	O
,	O
n02111129	O
,	O
n02111277	O
,	O
n02111500	O
,	O
n02111889	O
,	O
n02112018	O
,	O
n02112137	O
,	O
n02112350	O
,	O
n02112706	O
,	O
n02113023	O
,	O
n02113186	O
,	O
n02113624	O
,	O
n02113712	O
,	O
n02113799	O
,	O
n02113978	O
appendix	O
:	O
PTB	O
Class	O
Splits	O
Here	O
we	O
define	O
the	O
two	O
class	O
splits	O
used	O
in	O
our	O
PTB	O
experiments	O
–	O
these	O
classes	O
were	O
excluded	O
for	O
training	O
during	O
our	O
one	O
-	O
shot	O
language	Task
experiments	O
described	O
in	O
section	O
[	O
reference	O
]	O
.	O

’s	O
,	O
12	O
-	O
year	O
,	O
190.58	O
-	O
point	O
,	O
1930s	O
,	O
26	O
-	O
week	O
,	O
a.c	O
.	O

,	O
abortion	O
,	O
absorbed	O
,	O
accelerating	O
,	O
acceptable	O
,	O
accords	O
,	O
accusations	O
,	O
achieve	O
,	O
acquires	O
,	O
actively	O
,	O
adapted	O
,	O
addition	O
,	O
adequate	O
,	O
admitting	O
,	O
adopt	O
,	O
adopted	O
,	O
adopting	O
,	O
advised	O
,	O
advisers	O
,	O
advises	O
,	O
advising	O
,	O
aer	O
,	O
affidavits	O
,	O
afternoon	O
,	O
ag	O
,	O
aged	O
,	O
ages	O
,	O
agreements	O
,	O
airport	O
,	O
akzo	O
,	O
alaska	O
,	O
alcohol	O
,	O
alert	O
,	O
alliance	O
,	O
allied	O
-	O
signal	O
,	O
ally	O
,	O
altman	O
,	O
ambrosiano	O
,	O
american	O
,	O
amgen	O
,	O
amount	O
,	O
amounts	O
,	O
an	O
,	O
andy	O
,	O
angry	O
,	O
animals	O
,	O
annuities	O
,	O
antitrust	O
,	O
anybody	O
,	O
anyway	O
,	O
appointed	O
,	O
approaching	O
,	O
approvals	O
,	O
arabs	O
,	O
arafat	O
,	O
arbitration	O
,	O
argentina	O
,	O
arranged	O
,	O
arrest	O
,	O
artists	O
,	O
assembled	O
,	O
associations	O
,	O
assume	O
,	O
assumptions	O
,	O
atoms	O
,	O
attitudes	O
,	O
audio	O
,	O
authorities	O
,	O
authority	O
,	O
away	O
,	O
balls	O
,	O
bally	O
,	O
banknote	O
,	O
banks	O
,	O
banning	O
,	O
barely	O
,	O
barred	O
,	O
barriers	O
,	O
bass	O
,	O
battery	O
,	O
baum	O
,	O
bears	O
,	O
bell	O
,	O
belt	O
,	O
best	O
,	O
best	O
-	O
known	O
,	O
billion	O
,	O
binge	O
,	O
blamed	O
,	O
blanket	O
,	O
bloc	O
,	O
block	O
,	O
blocking	O
,	O
boat	O
,	O
bodies	O
,	O
boesel	O
,	O
bolstered	O
,	O
bonuses	O
,	O
boston	O
,	O
bowed	O
,	O
boys	O
,	O
bozell	O
,	O
bradstreet	O
,	O
brains	O
,	O
breakers	O
,	O
breaks	O
,	O
briefly	O
,	O
brink	O
,	O
brisk	O
,	O
broad	O
-	O
based	O
,	O
broken	O
,	O
bronx	O
,	O
brother	O
,	O
bsn	O
,	O
built	O
,	O
buried	O
,	O
burmah	O
,	O
burned	O
,	O
bursts	O
,	O
bush	O
,	O
businessland	O
,	O
businessman	O
,	O
buys	O
,	O
calculate	O
,	O
calculated	O
,	O
caltrans	O
,	O
campbell	O
,	O
candlestick	O
,	O
capitalism	O
,	O
captured	O
,	O
careers	O
,	O
carpeting	O
,	O
carried	O
,	O
carry	O
-	O
forward	O
,	O
casting	O
,	O
castle	O
,	O
catholic	O
,	O
caught	O
,	O
ceiling	O
,	O
cells	O
,	O
centuries	O
,	O
chair	O
,	O
chairs	O
,	O
challenged	O
,	O
chances	O
,	O
chandler	O
,	O
characters	O
,	O
charts	O
,	O
cheating	O
,	O
checks	O
,	O
cherry	O
,	O
chiron	O
,	O
cie	O
,	O
cie	O
.	O

,	O
cincinnati	O
,	O
circuit	O
,	O
civic	O
,	O
clara	O
,	O
classroom	O
,	O
clean	O
-	O
air	O
,	O
climate	O
,	O
closer	O
,	O
cms	O
,	O
cnw	O
,	O
coast	O
,	O
coats	O
,	O
cocom	O
,	O
cold	O
,	O
collected	O
,	O
comes	O
,	O
commercial	O
,	O
commerzbank	O
,	O
commissioned	O
,	O
committed	O
,	O
commute	O
,	O
complains	O
,	O
completing	O
,	O
computer	O
,	O
confirm	O
,	O
confiscated	O
,	O
confronted	O
,	O
conn	O
,	O
conn	O
.	O

,	O
consisting	O
,	O
consortium	O
,	O
constitute	O
,	O
consultant	O
,	O
consumer	O
,	O
consumers	O
,	O
contemporary	O
,	O
contra	O
,	O
contraceptive	O
,	O
contributing	O
,	O
convinced	O
,	O
cost	O
-	O
cutting	O
,	O
count	O
,	O
counterparts	O
,	O
counties	O
,	O
courses	O
,	O
cover	O
,	O
cracks	O
,	O
craft	O
,	O
crane	O
,	O
create	O
,	O
creating	O
,	O
crossing	O
,	O
crumbling	O
,	O
crusade	O
,	O
crusaders	O
,	O
cubic	O
,	O
curtail	O
,	O
curve	O
,	O
cushion	O
,	O
cut	O
,	O
cynthia	O
,	O
dairy	O
,	O
dam	O
,	O
david	O
,	O
davis	O
,	O
day	O
,	O
deal	O
,	O
dealerships	O
,	O
debentures	O
,	O
debut	O
,	O
deceptive	O
,	O
decided	O
,	O
decision	O
,	O
decisions	O
,	O
deck	O
,	O
defended	O
,	O
defenders	O
,	O
defenses	O
,	O
definitely	O
,	O
delivering	O
,	O
della	O
,	O
demonstrated	O
,	O
department	O
,	O
departure	O
,	O
depress	O
,	O
designated	O
,	O
desk	O
,	O
desktop	O
,	O
detailing	O
,	O
devaluation	O
,	O
develops	O
,	O
devoe	O
,	O
di	O
,	O
dialogue	O
,	O
dictator	O
,	O
die	O
,	O
diesel	O
,	O
differ	O
,	O
digs	O
,	O
diluted	O
,	O
diminished	O
,	O
direct	O
-	O
mail	O
,	O
disappointing	O
,	O
discount	O
,	O
discrepancies	O
,	O
discuss	O
,	O
disease	O
,	O
disney	O
,	O
disruption	O
,	O
distributed	O
,	O
distributor	O
,	O
dive	O
,	O
diversified	O
,	O
divided	O
,	O
dividends	O
,	O
dodge	O
,	O
doing	O
,	O
domestic	O
,	O
dominant	O
,	O
domination	O
,	O
double	O
-	O
a	O
,	O
downgraded	O
,	O
downgrading	O
,	O
downtown	O
,	O
drives	O
,	O
drought	O
,	O
drunk	O
,	O
dunkin	O
,	O
earn	O
,	O
earthquakes	O
,	O
edisto	O
,	O
editions	O
,	O
educate	O
,	O
eggs	O
,	O
elaborate	O
,	O
elite	O
,	O
embarrassing	O
,	O
emerges	O
,	O
emerging	O
,	O
emigration	O
,	O
employers	O
,	O
empty	O
,	O
enactment	O
,	O
encourages	O
,	O
endorsement	O
,	O
enemies	O
,	O
engelken	O
,	O
enhanced	O
,	O
entertaining	O
,	O
enthusiastic	O
,	O
epicenter	O
,	O
equipped	O
,	O
era	O
,	O
erosion	O
,	O
esselte	O
,	O
est	O
,	O
ethical	O
,	O
ethiopia	O
,	O
eurodollar	O
,	O
events	O
,	O
everyone	O
,	O
exchanges	O
,	O
exciting	O
,	O
exclusively	O
,	O
executed	O
,	O
executing	O
,	O
executive	O
,	O
executives	O
,	O
exempt	O
,	O
expertise	O
,	O
explicit	O
,	O
explosion	O
,	O
expressed	O
,	O
expression	O
,	O
extending	O
,	O
extraordinary	O
,	O
faculty	O
,	O
failed	O
,	O
failure	O
,	O
fallout	O
,	O
faltered	O
,	O
fanfare	O
,	O
fare	O
,	O
farm	O
,	O
fast	O
-	O
growing	O
,	O
fasteners	O
,	O
fastest	O
,	O
fax	O
,	O
fazio	O
,	O
february	O
,	O
federated	O
,	O
fee	O
,	O
field	O
,	O
fifth	O
,	O
fighting	O
,	O
filipino	O
,	O
film	O
,	O
final	O
,	O
financiers	O
,	O
finished	O
,	O
finland	O
,	O
firmed	O
,	O
fiscal	O
,	O
fits	O
,	O
fitzwater	O
,	O
five	O
-	O
cent	O
,	O
fixed	O
-	O
income	O
,	O
fla	O
,	O
flamboyant	O
,	O
fleets	O
,	O
fleming	O
,	O
fletcher	O
,	O
flight	O
,	O
flights	O
,	O
flowers	O
,	O
focus	O
,	O
folk	O
,	O
following	O
,	O
foot	O
,	O
forecasting	O
,	O
found	O
,	O
fox	O
,	O
fray	O
,	O
freeway	O
,	O
freeways	O
,	O
freeze	O
,	O
frequency	O
,	O
freshman	O
,	O
fromstein	O
,	O
frustrating	O
,	O
fur	O
,	O
galileo	O
,	O
game	O
,	O
gandhi	O
,	O
garbage	O
,	O
gathered	O
,	O
gave	O
,	O
gear	O
,	O
gene	O
,	O
generale	O
,	O
genuine	O
,	O
gerard	O
,	O
giant	O
,	O
girl	O
,	O
gloomy	O
,	O
goes	O
,	O
golden	O
,	O
goodman	O
,	O
gov	O
.	O

,	O
governing	O
,	O
government	O
-	O
owned	O
,	O
governor	O
,	O
grave	O
,	O
greenhouse	O
,	O
gridlock	O
,	O
grim	O
,	O
guerrilla	O
,	O
guild	O
,	O
gun	O
,	O
h	O
&	O
r	O
,	O
half	O
-	O
hour	O
,	O
handicapped	O
,	O
handy	O
,	O
hanging	O
,	O
happening	O
,	O
happy	O
,	O
harold	O
,	O
haunts	O
,	O
headed	O
,	O
heating	O
,	O
heavier	O
,	O
heavily	O
,	O
hedges	O
,	O
heights	O
,	O
heller	O
,	O
helping	O
,	O
helps	O
,	O
hepatitis	O
,	O
hess	O
,	O
high	O
-	O
definition	O
,	O
high	O
-	O
technology	O
,	O
hiring	O
,	O
hoffman	O
,	O
hold	O
,	O
hole	O
,	O
homeless	O
,	O
honduras	O
,	O
hooker	O
,	O
horizon	O
,	O
hot	O
-	O
dipped	O
,	O
houses	O
,	O
how	O
,	O
hubbard	O
,	O
hurricane	O
,	O
hydro	O
-	O
quebec	O
,	O
hyman	O
,	O
idaho	O
,	O
ill	O
,	O
illness	O
,	O
illustrated	O
,	O
immune	O
,	O
impeachment	O
,	O
implicit	O
,	O
impose	O
,	O
impression	O
,	O
impressive	O
,	O
increase	O
,	O
incredible	O
,	O
incurred	O
,	O
indexing	O
,	O
indiana	O
,	O
indicates	O
,	O
indications	O
,	O
influences	O
,	O
influx	O
,	O
inherent	O
,	O
inquiry	O
,	O
intensive	O
,	O
intentions	O
,	O
internationally	O
,	O
involves	O
,	O
irish	O
,	O
ironically	O
,	O
isler	O
,	O
itel	O
,	O
itt	O
,	O
j.	O
,	O
jackson	O
,	O
jaguar	O
,	O
jazz	O
,	O
jefferson	O
,	O
jittery	O
,	O
jolted	O
,	O
july	O
,	O
jump	O
,	O
jury	O
,	O
justifies	O
,	O
karen	O
,	O
kean	O
,	O
keating	O
,	O
kent	O
,	O
kgb	O
,	O
khan	O
,	O
killing	O
,	O
knocked	O
,	O
knocking	O
,	O
koch	O
,	O
l.j	O
.	O

,	O
labs	O
,	O
lasts	O
,	O
lately	O
,	O
latest	O
,	O
lawrence	O
,	O
league	O
,	O
lean	O
,	O
least	O
,	O
leave	O
,	O
legitimacy	O
,	O
lehman	O
,	O
leisure	O
,	O
lend	O
,	O
leo	O
,	O
life	O
,	O
lighter	O
,	O
lights	O
,	O
linda	O
,	O
line	O
,	O
literature	O
,	O
live	O
,	O
living	O
,	O
longstanding	O
,	O
looking	O
,	O
looks	O
,	O
loral	O
,	O
lord	O
,	O
lose	O
,	O
lotus	O
,	O
louisville	O
,	O
lower	O
,	O
ltd	O
.	O

,	O
luis	O
,	O
lumpur	O
,	O
made	O
,	O
madrid	O
,	O
malcolm	O
,	O
male	O
,	O
manage	O
,	O
management	O
,	O
manic	O
,	O
manville	O
,	O
marcos	O
,	O
marked	O
,	O
market	O
-	O
makers	O
,	O
market	O
-	O
share	O
,	O
markets	O
,	O
mary	O
,	O
mass	O
-	O
market	O
,	O
mayor	O
,	O
mccormick	O
,	O
mcdonald	O
,	O
md	O
.	O

,	O
measured	O
,	O
member	O
,	O
members	O
,	O
memorandum	O
,	O
merabank	O
,	O
mercury	O
,	O
merely	O
,	O
merged	O
,	O
mergers	O
,	O
message	O
,	O
mich	O
.	O

,	O
mile	O
,	O
millions	O
,	O
mining	O
,	O
ministers	O
,	O
ministry	O
,	O
minorities	O
,	O
minutes	O
,	O
missile	O
,	O
mission	O
,	O
mitterrand	O
,	O
modified	O
,	O
monitor	O
,	O
months	O
,	O
moody	O
,	O
moore	O
,	O
mothers	O
,	O
motorola	O
,	O
movie	O
,	O
mr	O
.	O

,	O
much	O
,	O
multibillion	O
-	O
dollar	O
,	O
multiples	O
,	O
mundane	O
,	O
municipalities	O
,	O
muscle	O
,	O
mutual	O
,	O
mutual	O
-	O
fund	O
,	O
named	O
,	O
names	O
,	O
namibia	O
,	O
nashua	O
,	O
nathan	O
,	O
ncr	O
,	O
near	O
-	O
term	O
,	O
nec	O
,	O
necessary	O
,	O
necessity	O
,	O
negligence	O
,	O
negotiating	O
,	O
negotiation	O
,	O
nervousness	O
,	O
newcomers	O
,	O
newspaper	O
,	O
nice	O
,	O
noise	O
,	O
northrop	O
,	O
norton	O
,	O
nose	O
,	O
nothing	O
,	O
noticed	O
,	O
notification	O
,	O
notified	O
,	O
notwithstanding	O
,	O
november	O
,	O
nurses	O
,	O
nutritional	O
,	O
observed	O
,	O
oddly	O
,	O
off	O
,	O
offerings	O
,	O
official	O
,	O
oh	O
,	O
oklahoma	O
,	O
oldest	O
,	O
olympic	O
,	O
ones	O
,	O
opening	O
,	O
opera	O
,	O
optical	O
,	O
optimistic	O
,	O
or	O
,	O
original	O
,	O
orthodox	O
,	O
ortiz	O
,	O
ousted	O
,	O
outfit	O
,	O
outlook	O
,	O
outside	O
,	O
oversees	O
,	O
owen	O
,	O
oy	O
,	O
pachinko	O
,	O
packaged	O
,	O
painted	O
,	O
park	O
,	O
parker	O
,	O
part	O
,	O
particular	O
,	O
partly	O
,	O
patrick	O
,	O
patterson	O
,	O
payable	O
,	O
pc	O
,	O
peace	O
,	O
peaked	O
,	O
peddling	O
,	O
pegged	O
,	O
pepsico	O
,	O
perception	O
,	O
perfect	O
,	O
perfectly	O
,	O
pfizer	O
,	O
pharmaceutical	O
,	O
phelan	O
,	O
philippine	O
,	O
philippines	O
,	O
phony	O
,	O
photographic	O
,	O
physicians	O
,	O
picking	O
,	O
pigs	O
,	O
pittsburgh	O
,	O
place	O
,	O
plagued	O
,	O
plan	O
,	O
planes	O
,	O
planet	O
,	O
pleasure	O
,	O
poles	O
,	O
pool	O
,	O
portable	O
,	O
portfolio	O
,	O
ports	O
,	O
post	O
-	O
crash	O
,	O
pound	O
,	O
poured	O
,	O
poverty	O
,	O
precedent	O
,	O
preclude	O
,	O
pregnant	O
,	O
prescribed	O
,	O
presents	O
,	O
pretty	O
,	O
priced	O
,	O
privileges	O
,	O
procurement	O
,	O
products	O
,	O
profit	O
-	O
taking	O
,	O
projections	O
,	O
prominent	O
,	O
promise	O
,	O
promotional	O
,	O
prompted	O
,	O
proper	O
,	O
proponents	O
,	O
propose	O
,	O
prosecuted	O
,	O
protein	O
,	O
prototype	O
,	O
prove	O
,	O
proved	O
,	O
published	O
,	O
publisher	O
,	O
pull	O
,	O
pulled	O
,	O
pumped	O
,	O
pumping	O
,	O
pushing	O
,	O
quebec	O
,	O
quickview	O
,	O
quist	O
,	O
quite	O
,	O
radical	O
,	O
radio	O
,	O
rain	O
,	O
ranging	O
,	O
rank	O
,	O
rebates	O
,	O
rebel	O
,	O
rebound	O
,	O
rebuild	O
,	O
recent	O
,	O
recital	O
,	O
recognizes	O
,	O
recognizing	O
,	O
recorded	O
,	O
recorders	O
,	O
reduce	O
,	O
reduced	O
,	O
refinery	O
,	O
refrigerators	O
,	O
registered	O
,	O
regret	O
,	O
reinvest	O
,	O
rejected	O
,	O
rejecting	O
,	O
rejection	O
,	O
relations	O
,	O
relatively	O
,	O
relying	O
,	O
remark	O
,	O
remics	O
,	O
reorganization	O
,	O
repaired	O
,	O
repeatedly	O
,	O
reports	O
,	O
represent	O
,	O
repurchase	O
,	O
resembles	O
,	O
reserved	O
,	O
resisted	O
,	O
resolved	O
,	O
resort	O
,	O
rest	O
,	O
restraints	O
,	O
restrictions	O
,	O
restructured	O
,	O
restructuring	O
,	O
result	O
,	O
rican	O
,	O
right	O
,	O
ring	O
,	O
rise	O
,	O
robbed	O
,	O
robinson	O
,	O
robots	O
,	O
robust	O
,	O
roh	O
,	O
role	O
,	O
rolled	O
,	O
rose	O
,	O
rothschild	O
,	O
rough	O
,	O
royal	O
,	O
ruled	O
,	O
rushing	O
,	O
s.c	O
,	O
sale	O
,	O
salesmen	O
,	O
salespeople	O
,	O
salmonella	O
,	O
salvage	O
,	O
saul	O
,	O
says	O
,	O
scheduled	O
,	O
school	O
,	O
schwarz	O
,	O
seagram	O
,	O
second	O
,	O
sector	O
,	O
securities	O
,	O
seek	O
,	O
segment	O
,	O
seismic	O
,	O
seldom	O
,	O
selected	O
,	O
semel	O
,	O
sending	O
,	O
sentences	O
,	O
sentencing	O
,	O
session	O
,	O
settlement	O
,	O
seventh	O
,	O
shed	O
,	O
shell	O
,	O
sheraton	O
,	O
shifting	O
,	O
shocks	O
,	O
short	O
,	O
showed	O
,	O
shy	O
,	O
sigh	O
,	O
sights	O
,	O
signals	O
,	O
sir	O
,	O
site	O
,	O
sites	O
,	O
sitting	O
,	O
skinner	O
,	O
slashed	O
,	O
snapped	O
,	O
so	O
-	O
called	O
,	O
soldiers	O
,	O
solely	O
,	O
solo	O
,	O
somehow	O
,	O
sotheby	O
,	O
speak	O
,	O
specialist	O
,	O
specialize	O
,	O
specializing	O
,	O
specifically	O
,	O
specifications	O
,	O
speculate	O
,	O
speculated	O
,	O
spencer	O
,	O
sperry	O
,	O
spreading	O
,	O
spur	O
,	O
stake	O
,	O
standardized	O
,	O
standing	O
,	O
statistics	O
,	O
steady	O
,	O
stemmed	O
,	O
stern	O
,	O
stevens	O
,	O
stock	O
-	O
index	O
,	O
stockholm	O
,	O
straight	O
,	O
strategists	O
,	O
stream	O
,	O
strength	O
,	O
stress	O
-	O
related	O
,	O
strict	O
,	O
subscriber	O
,	O
suggestions	O
,	O
surplus	O
,	O
surprise	O
,	O
surprises	O
,	O
surrounding	O
,	O
syrian	O
,	O
taiwanese	O
,	O
tall	O
,	O
tap	O
,	O
tapped	O
,	O
task	O
,	O
taxation	O
,	O
taxed	O
,	O
tci	O
,	O
technicians	O
,	O
televised	O
,	O
temptation	O
,	O
testing	O
,	O
texans	O
,	O
theatre	O
,	O
third	O
,	O
this	O
,	O
thomas	O
,	O
those	O
,	O
thoughts	O
,	O
thriving	O
,	O
tickets	O
,	O
ties	O
,	O
tiger	O
,	O
tighter	O
,	O
tire	O
,	O
tisch	O
,	O
together	O
,	O
toronto	O
-	O
based	O
,	O
toshiba	O
,	O
towers	O
,	O
toxin	O
,	O
traditional	O
,	O
trains	O
,	O
transit	O
,	O
trap	O
,	O
treated	O
,	O
trecker	O
,	O
tribune	O
,	O
trigger	O
,	O
triggering	O
,	O
trillion	O
,	O
tube	O
,	O
tune	O
,	O
turn	O
,	O
turnaround	O
,	O
typically	O
,	O
u.k	O
.	O

,	O
u.n	O
.	O

,	O
uncertain	O
,	O
underlying	O
,	O
underwear	O
,	O
underwrite	O
,	O
underwriter	O
,	O
underwriting	O
,	O
undo	O
,	O
unfortunately	O
,	O
unidentified	O
,	O
unilab	O
,	O
unisys	O
,	O
unit	O
,	O
unknown	O
,	O
unlawful	O
,	O
unless	O
,	O
unused	O
,	O
upheld	O
,	O
upon	O
,	O
upside	O
,	O
urge	O
,	O
usia	O
,	O
uv	O
-	O
b	O
,	O
valid	O
,	O
van	O
,	O
vendors	O
,	O
very	O
,	O
victim	O
,	O
vienna	O
,	O
violations	O
,	O
virginia	O
,	O
vision	Task
,	O
visit	O
,	O
voluntary	O
,	O
w.	O
,	O
wade	O
,	O
wait	O
,	O
wanting	O
,	O
ward	O
,	O
warner	O
,	O
wars	O
,	O
wary	O
,	O
wash	O
.	O

,	O
wealthy	O
,	O
wednesday	O
,	O
when	O
-	O
issued	O
,	O
whether	O
,	O
white	O
-	O
collar	O
,	O
wholly	O
,	O
widening	O
,	O
will	O
,	O
willingness	O
,	O
wilmington	O
,	O
win	O
,	O
winnebago	O
,	O
winners	O
,	O
wish	O
,	O
wolf	O
,	O
words	O
,	O
work	O
,	O
working	O
,	O
worse	O
,	O
would	O
,	O
yard	O
,	O
yards	O
,	O
yearly	O
,	O
yielding	O
,	O
youth	O
,	O
z	O
,	O
zones	O
