document	O
:	O
Simple	O
Baseline	O
for	O
Visual	Task
Question	Task
Answering	Task
We	O
describe	O
a	O
very	O
simple	O
bag	Method
-	Method
of	Method
-	Method
words	Method
baseline	Method
for	O
visual	Task
question	Task
answering	Task
.	O

This	O
baseline	O
concatenates	O
the	O
word	O
features	O
from	O
the	O
question	O
and	O
CNN	O
features	O
from	O
the	O
image	O
to	O
predict	O
the	O
answer	O
.	O

When	O
evaluated	O
on	O
the	O
challenging	O
VQA	Material
dataset	Material
,	O
it	O
shows	O
comparable	O
performance	O
to	O
many	O
recent	O
approaches	O
using	O
recurrent	Method
neural	Method
networks	Method
.	O

To	O
explore	O
the	O
strength	O
and	O
weakness	O
of	O
the	O
trained	O
model	O
,	O
we	O
also	O
provide	O
an	O
interactive	O
web	O
demo	O
,	O
and	O
open	O
-	O
source	O
code	O
.	O

1	O
]	O
BoleiZhou2	O
]	O
YuandongTian2	O
]	O
SainbayarSukhbaatar2	O
]	O
ArthurSzlam2	O
]	O
RobFergus	O
[	O
1	O
]	O
MassachusettsInstituteofTechnology	O
[	O
2	O
]	O
FacebookAIResearch	O
section	O
:	O
Introduction	O
Combining	O
Natural	Task
Language	Task
Processing	Task
with	O
Computer	Task
Vision	Task
for	O
high	Task
-	Task
level	Task
scene	Task
interpretation	Task
is	O
a	O
recent	O
trend	O
,	O
e.g.	O
,	O
image	Task
captioning	Task
.	O

These	O
works	O
have	O
benefited	O
from	O
the	O
rapid	O
development	O
of	O
deep	Method
learning	Method
for	O
visual	Task
recognition	Task
(	O
object	Task
recognition	Task
and	Task
scene	Task
recognition	Task
)	O
,	O
and	O
have	O
been	O
made	O
possible	O
by	O
the	O
emergence	O
of	O
large	O
image	O
datasets	O
and	O
text	O
corpus	O
(	O
e.g.	O
,	O
)	O
.	O

Beyond	O
image	Task
captioning	Task
,	O
a	O
natural	O
next	O
step	O
is	O
visual	Task
question	Task
answering	Task
(	O
QA	Task
)	O
.	O

Compared	O
with	O
the	O
image	Task
captioning	Task
task	Task
,	O
in	O
which	O
an	O
algorithm	O
is	O
required	O
to	O
generate	O
free	O
-	O
form	O
text	O
description	O
for	O
a	O
given	O
image	O
,	O
visual	Task
QA	Task
can	O
involve	O
a	O
wider	O
range	O
of	O
knowledge	O
and	O
reasoning	O
skills	O
.	O

A	O
captioning	Method
algorithm	Method
has	O
the	O
liberty	O
to	O
pick	O
the	O
easiest	O
relevant	O
descriptions	O
of	O
the	O
image	O
,	O
whereas	O
as	O
responding	O
to	O
a	O
question	O
needs	O
to	O
find	O
the	O
correct	O
answer	O
for	O
*	O
that	O
*	O
question	O
.	O

Furthermore	O
,	O
the	O
algorithms	O
for	O
visual	Task
QA	Task
are	O
required	O
to	O
answer	O
all	O
kinds	O
of	O
questions	O
people	O
might	O
ask	O
about	O
the	O
image	O
,	O
some	O
of	O
which	O
might	O
be	O
relevant	O
to	O
the	O
image	O
contents	O
,	O
such	O
as	O
“	O
what	O
books	O
are	O
under	O
the	O
television	O
”	O
and	O
“	O
what	O
is	O
the	O
color	O
of	O
the	O
boat	O
”	O
,	O
while	O
others	O
might	O
require	O
knowledge	O
or	O
reasoning	O
beyond	O
the	O
image	O
content	O
,	O
such	O
as	O
“	O
why	O
is	O
the	O
baby	O
crying	O
?	O
”	O
and	O
“	O
which	O
chair	O
is	O
the	O
most	O
expensive	O
?	O
”	O
.	O

Building	O
robust	Method
algorithms	Method
for	O
visual	Task
QA	Task
that	O
perform	O
at	O
near	O
human	O
levels	O
would	O
be	O
an	O
important	O
step	O
towards	O
solving	O
AI	Task
.	O

Recently	O
,	O
several	O
papers	O
have	O
appeared	O
on	O
arXiv	O
(	O
after	O
CVPR’16	O
submission	O
deadline	O
)	O
proposing	O
neural	Method
network	Method
architectures	Method
for	O
visual	Task
question	Task
answering	Task
,	O
such	O
as	O
.	O

Some	O
of	O
them	O
are	O
derived	O
from	O
the	O
image	Method
captioning	Method
framework	Method
,	O
in	O
which	O
the	O
output	O
of	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
e.g.	O
,	O
LSTM	Method
)	O
applied	O
to	O
the	O
question	O
sentence	O
is	O
concatenated	O
with	O
visual	O
features	O
from	O
VGG	Method
or	O
other	O
CNNs	Method
to	O
feed	O
a	O
classifier	Method
to	O
predict	O
the	O
answer	O
.	O

Other	O
models	O
integrate	O
visual	Method
attention	Method
mechanisms	Method
and	O
visualize	O
how	O
the	O
network	O
learns	O
to	O
attend	O
the	O
local	O
image	O
regions	O
relevant	O
to	O
the	O
content	O
of	O
the	O
question	O
.	O

Interestingly	O
,	O
we	O
notice	O
that	O
in	O
one	O
of	O
the	O
earliest	O
VQA	O
papers	O
,	O
the	O
simple	O
baseline	Method
Bag	Method
-	Method
of	Method
-	Method
words	Method
+	Method
image	Method
feature	Method
(	O
referred	O
to	O
as	O
BOWIMG	Method
baseline	Method
)	O
outperforms	O
the	O
LSTM	Method
-	Method
based	Method
models	Method
on	O
a	O
synthesized	O
visual	O
QA	O
dataset	O
built	O
up	O
on	O
top	O
of	O
the	O
image	O
captions	O
of	O
COCO	Material
dataset	Material
.	O

For	O
the	O
recent	O
much	O
larger	O
COCO	Material
VQA	Material
dataset	Material
,	O
the	O
BOWIMG	Method
baseline	Method
performs	O
worse	O
than	O
the	O
LSTM	Method
-	Method
based	Method
models	Method
.	O

In	O
this	O
work	O
,	O
we	O
carefully	O
implement	O
the	O
BOWIMG	Method
baseline	Method
model	Method
.	O

We	O
call	O
it	O
iBOWIMG	Method
to	O
avoid	O
confusion	O
with	O
the	O
implementation	O
in	O
.	O

With	O
proper	O
setup	O
and	O
training	O
,	O
this	O
simple	O
baseline	O
model	O
shows	O
comparable	O
performance	O
to	O
many	O
recent	O
recurrent	Method
network	Method
-	Method
based	Method
approaches	Method
for	O
visual	Task
QA	Task
.	O

Further	O
analysis	O
shows	O
that	O
the	O
baseline	O
learns	O
to	O
correlate	O
the	O
informative	O
words	O
in	O
the	O
question	O
sentence	O
and	O
visual	O
concepts	O
in	O
the	O
image	O
with	O
the	O
answer	O
.	O

Furthermore	O
,	O
such	O
correlations	O
can	O
be	O
used	O
to	O
compute	O
reasonable	O
spatial	O
attention	O
map	O
with	O
the	O
help	O
of	O
the	O
CAM	Method
technique	Method
proposed	O
in	O
.	O

The	O
source	O
code	O
and	O
the	O
visual	O
QA	O
demo	O
based	O
on	O
the	O
trained	O
model	O
are	O
publicly	O
available	O
.	O

In	O
the	O
demo	O
,	O
iBOWIMG	Method
baseline	O
gives	O
answers	O
to	O
any	O
question	O
relevant	O
to	O
the	O
given	O
images	O
.	O

Playing	O
with	O
the	O
visual	Method
QA	Method
models	Method
interactively	O
could	O
reveal	O
the	O
strengths	O
and	O
weakness	O
of	O
the	O
trained	O
model	O
.	O

section	O
:	O
iBOWIMG	Method
for	O
Visual	Task
Question	Task
Answering	Task
In	O
most	O
of	O
the	O
recent	O
proposed	O
models	O
,	O
visual	Task
QA	Task
is	O
simplified	O
to	O
a	O
classification	Task
task	Task
:	O
the	O
number	O
of	O
the	O
different	O
answers	O
in	O
the	O
training	O
set	O
is	O
the	O
number	O
of	O
the	O
final	O
classes	O
the	O
models	O
need	O
to	O
learn	O
to	O
predict	O
.	O

The	O
general	O
pipeline	O
of	O
those	O
models	O
is	O
that	O
the	O
word	O
feature	O
extracted	O
from	O
the	O
question	O
sentence	O
is	O
concatenated	O
with	O
the	O
visual	O
feature	O
extracted	O
from	O
the	O
image	O
,	O
then	O
they	O
are	O
fed	O
into	O
a	O
softmax	Method
layer	Method
to	O
predict	O
the	O
answer	O
class	O
.	O

The	O
visual	O
feature	O
is	O
usually	O
taken	O
from	O
the	O
top	O
of	O
the	O
VGG	Method
network	Method
or	O
GoogLeNet	Method
,	O
while	O
the	O
word	O
features	O
of	O
the	O
question	O
sentence	O
are	O
usually	O
the	O
popular	O
LSTM	Method
-	Method
based	Method
features	Method
.	O

In	O
our	O
iBOWIMG	Method
model	O
,	O
we	O
simply	O
use	O
naive	O
bag	O
-	O
of	O
-	O
words	O
as	O
the	O
text	O
feature	O
,	O
and	O
use	O
the	O
deep	O
features	O
from	O
GoogLeNet	Method
as	O
the	O
visual	O
features	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
framework	O
of	O
the	O
iBOWIMG	Method
model	O
,	O
which	O
can	O
be	O
implemented	O
in	O
Torch	Method
with	O
no	O
more	O
than	O
10	O
lines	O
of	O
code	O
.	O

The	O
input	O
question	O
is	O
first	O
converted	O
to	O
a	O
one	O
-	O
hot	O
vector	O
,	O
which	O
is	O
transformed	O
to	O
word	O
feature	O
via	O
a	O
word	Method
embedding	Method
layer	Method
and	O
then	O
is	O
concatenated	O
with	O
the	O
image	O
feature	O
from	O
CNN	Method
.	O

The	O
combined	O
feature	O
is	O
sent	O
to	O
the	O
softmax	Method
layer	Method
to	O
predict	O
the	O
answer	O
class	O
,	O
which	O
essentially	O
is	O
a	O
multi	Method
-	Method
class	Method
logistic	Method
regression	Method
model	Method
.	O

section	O
:	O
Experiments	O
Here	O
we	O
train	O
and	O
evaluate	O
the	O
iBOWIMG	Method
model	O
on	O
the	O
Full	O
release	O
of	O
COCO	Material
VQA	Material
dataset	Material
,	O
the	O
largest	O
VQA	Material
dataset	Material
so	O
far	O
.	O

In	O
the	O
COCO	Material
VQA	Material
dataset	Material
,	O
there	O
are	O
3	O
questions	O
annotated	O
by	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
workers	O
for	O
each	O
image	O
in	O
the	O
COCO	Material
dataset	Material
.	O

For	O
each	O
question	O
,	O
10	O
answers	O
are	O
annotated	O
by	O
another	O
batch	O
of	O
AMT	Method
workers	Method
.	O

To	O
pre	O
-	O
process	O
the	O
annotation	O
for	O
training	Task
,	O
we	O
perform	O
majority	Method
voting	Method
on	O
the	O
10	O
ground	O
-	O
truth	O
answers	O
to	O
get	O
the	O
most	O
certain	O
answer	O
for	O
each	O
question	O
.	O

Here	O
the	O
answer	O
could	O
be	O
in	O
single	O
word	O
or	O
multiple	O
words	O
.	O

Then	O
we	O
have	O
the	O
3	O
question	O
-	O
answer	O
pairs	O
from	O
each	O
image	O
for	O
training	O
.	O

There	O
are	O
in	O
total	O
248	O
,	O
349	O
pairs	O
in	O
train2014	Method
and	O
121	O
,	O
512	O
pairs	O
in	O
val2014	O
,	O
for	O
123	O
,	O
287	O
images	O
overall	O
in	O
the	O
training	O
set	O
.	O

Here	O
train2014	Method
and	O
val2014	Method
are	O
the	O
standard	O
splits	O
of	O
the	O
image	O
set	O
in	O
the	O
COCO	Material
dataset	Material
.	O

To	O
generate	O
the	O
training	O
set	O
and	O
validation	O
set	O
for	O
our	O
model	O
,	O
we	O
first	O
randomly	O
split	O
the	O
images	O
of	O
COCO	Material
val2014	Material
into	O
70	O
%	O
subset	O
A	O
and	O
30	O
%	O
subset	O
B.	O
To	O
avoid	O
potential	O
overfitting	O
,	O
questions	O
sharing	O
the	O
same	O
image	O
will	O
be	O
placed	O
into	O
the	O
same	O
split	O
.	O

The	O
question	O
-	O
answer	O
pairs	O
from	O
the	O
images	O
of	O
COCO	Material
train2014	Material
+	O
val2014	O
subset	O
A	O
are	O
combined	O
and	O
used	O
for	O
training	O
,	O
while	O
the	O
val2014	O
subset	O
B	O
is	O
used	O
as	O
validation	O
set	O
for	O
parameter	Task
tuning	Task
.	O

After	O
we	O
find	O
the	O
best	O
model	O
parameters	O
,	O
we	O
combine	O
the	O
whole	O
train2014	Method
and	O
val2014	Method
to	O
train	O
the	O
final	O
model	O
.	O

We	O
submit	O
the	O
prediction	O
result	O
given	O
by	O
the	O
final	O
model	O
on	O
the	O
testing	O
set	O
(	O
COCO	Material
test2015	Material
)	O
to	O
the	O
evaluation	O
server	O
,	O
to	O
get	O
the	O
final	O
accuracy	Metric
on	O
the	O
test	O
-	O
dev	O
and	O
test	O
-	O
standard	O
set	O
.	O

For	O
Open	Material
-	Material
Ended	Material
Question	Material
track	Material
,	O
we	O
take	O
the	O
top	O
-	O
1	O
predicted	O
answer	O
from	O
the	O
softmax	O
output	O
.	O

For	O
the	O
Multiple	Material
-	Material
Choice	Material
Question	Material
track	Material
,	O
we	O
first	O
get	O
the	O
softmax	O
probability	O
for	O
each	O
of	O
the	O
given	O
choices	O
then	O
select	O
the	O
most	O
confident	O
one	O
.	O

The	O
code	O
is	O
implemented	O
in	O
Torch	Method
.	O

The	O
training	O
takes	O
about	O
10	O
hours	O
on	O
a	O
single	O
GPU	O
NVIDIA	O
Titan	O
Black	O
.	O

subsection	O
:	O
Benchmark	O
Performance	O
According	O
to	O
the	O
evaluation	O
standard	O
of	O
the	O
VQA	Material
dataset	Material
,	O
the	O
result	O
of	O
the	O
any	O
proposed	O
VQA	Method
models	Method
should	O
report	O
accuracy	Metric
on	O
the	O
test	O
-	O
standard	O
set	O
for	O
fair	O
comparison	O
.	O

We	O
report	O
our	O
baseline	O
on	O
the	O
test	O
-	O
dev	O
set	O
in	O
Table	O
[	O
reference	O
]	O
and	O
the	O
test	O
-	O
standard	O
set	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
test	O
-	O
dev	O
set	O
is	O
used	O
for	O
debugging	Task
and	Task
validation	Task
experiments	Task
and	O
allows	O
for	O
unlimited	O
submission	O
to	O
the	O
evaluation	O
server	O
,	O
while	O
test	O
-	O
standard	O
is	O
used	O
for	O
model	Task
comparison	Task
with	O
limited	Metric
submission	Metric
times	Metric
.	O

Since	O
this	O
VQA	Material
dataset	Material
is	O
rather	O
new	O
,	O
the	O
publicly	O
available	O
models	O
evaluated	O
on	O
the	O
dataset	O
are	O
all	O
from	O
non	O
-	O
peer	O
reviewed	O
arXiv	O
papers	O
.	O

We	O
include	O
the	O
performance	O
of	O
the	O
models	O
available	O
at	O
the	O
time	O
of	O
writing	O
(	O
Dec.5	O
,	O
2015	O
)	O
.	O

Note	O
that	O
some	O
models	O
are	O
evaluated	O
on	O
either	O
test	Metric
-	Metric
dev	Metric
or	O
test	Metric
-	Metric
standard	Metric
for	O
either	O
Open	Task
-	Task
Ended	Task
or	Task
Multiple	Task
-	Task
Choice	Task
track	Task
.	O

The	O
full	O
set	O
of	O
the	O
VQA	Material
dataset	Material
was	O
released	O
on	O
Oct.6	O
2015	O
;	O
previously	O
the	O
v0.1	O
version	O
and	O
v0.9	O
version	O
had	O
been	O
released	O
.	O

We	O
notice	O
that	O
some	O
models	O
are	O
evaluated	O
using	O
non	O
-	O
standard	O
setups	O
,	O
rendering	O
performance	O
comparisons	O
difficult	O
.	O

(	O
arXiv	O
dated	O
at	O
Nov.17	O
2015	O
)	O
used	O
v0.9	O
version	O
of	O
VQA	Method
with	O
their	O
own	O
split	O
of	O
training	O
and	O
testing	O
;	O
(	O
arXiv	O
dated	O
at	O
Nov.7	O
2015	O
)	O
used	O
their	O
own	O
split	O
of	O
training	O
and	O
testing	O
for	O
the	O
val2014	O
;	O
(	O
arXiv	O
dated	O
at	O
Nov.18	O
2015	O
)	O
used	O
v0.9	O
version	O
of	O
VQA	Material
dataset	Material
.	O

So	O
these	O
are	O
not	O
included	O
in	O
the	O
comparison	O
.	O

Except	O
for	O
these	O
IMG	Method
,	O
BOW	Method
,	O
BOWIMG	Method
baselines	Method
provided	O
in	O
the	O
,	O
all	O
the	O
compared	O
methods	O
use	O
either	O
deep	Method
or	Method
recursive	Method
neural	Method
networks	Method
.	O

However	O
,	O
our	O
iBOWIMG	Method
baseline	O
shows	O
comparable	O
performances	O
against	O
these	O
much	O
more	O
complex	O
models	O
,	O
except	O
for	O
DPPnet	Method
that	O
is	O
about	O
1.5	O
%	O
better	O
.	O

subsection	O
:	O
Training	O
Details	O
Learning	Metric
rate	Metric
and	O
weight	Metric
clip	Metric
.	O

We	O
find	O
that	O
setting	O
up	O
a	O
different	O
learning	Metric
rate	Metric
and	O
weight	O
clipping	O
for	O
the	O
word	Method
embedding	Method
layer	Method
and	O
softmax	Method
layer	Method
leads	O
to	O
better	O
performance	O
.	O

The	O
learning	Metric
rate	Metric
for	O
the	O
word	Method
embedding	Method
layer	Method
should	O
be	O
much	O
higher	O
than	O
the	O
learning	Metric
rate	Metric
of	O
softmax	Method
layer	Method
to	O
learn	O
a	O
good	O
word	Method
embedding	Method
.	O

From	O
the	O
performance	O
of	O
BOW	Method
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
a	O
good	O
word	Method
model	Method
is	O
crucial	O
to	O
the	O
accuracy	Metric
,	O
as	O
BOW	Method
model	O
alone	O
could	O
achieve	O
closely	O
to	O
48	O
%	O
,	O
even	O
without	O
looking	O
at	O
the	O
image	O
content	O
.	O

Model	O
parameters	O
to	O
tune	O
.	O

Though	O
our	O
model	O
could	O
be	O
considered	O
as	O
the	O
simplest	O
baseline	O
so	O
far	O
for	O
visual	Task
QA	Task
,	O
there	O
are	O
several	O
model	O
parameters	O
to	O
tune	O
:	O
1	O
)	O
the	O
number	O
of	O
epochs	O
to	O
train	O
.	O

2	O
)	O
the	O
learning	Metric
rate	Metric
and	O
weight	O
clip	O
.	O

3	O
)	O
the	O
threshold	O
for	O
removing	O
less	O
frequent	O
question	O
word	O
and	O
answer	O
classes	O
.	O

We	O
iterate	O
to	O
search	O
the	O
best	O
value	O
of	O
each	O
model	O
parameter	O
separately	O
on	O
the	O
val2014	O
subset	O
B.	O
In	O
our	O
best	O
model	O
,	O
there	O
are	O
5	O
,	O
746	O
words	O
in	O
the	O
dictionary	O
of	O
question	O
sentence	O
,	O
5	O
,	O
216	O
classes	O
of	O
answers	O
.	O

The	O
specific	O
model	O
parameters	O
can	O
be	O
found	O
in	O
the	O
source	O
code	O
.	O

subsection	O
:	O
Understanding	O
the	O
Visual	Task
QA	Task
model	O
From	O
the	O
comparisons	O
above	O
,	O
we	O
can	O
see	O
that	O
our	O
baseline	O
model	O
performs	O
as	O
well	O
as	O
the	O
recurrent	Method
neural	Method
network	Method
models	Method
on	O
the	O
VQA	Material
dataset	Material
.	O

Furthermore	O
,	O
due	O
to	O
its	O
simplicity	O
,	O
the	O
behavior	O
of	O
the	O
model	O
could	O
be	O
easily	O
interpreted	O
,	O
demonstrating	O
what	O
it	O
learned	O
for	O
visual	Task
QA	Task
.	O

Essentially	O
,	O
the	O
BOWIMG	Method
baseline	Method
model	Method
learns	O
to	O
memorize	O
the	O
correlation	O
between	O
the	O
answer	O
class	O
and	O
the	O
informative	O
words	O
in	O
the	O
question	O
sentence	O
along	O
with	O
the	O
visual	O
feature	O
.	O

We	O
split	O
the	O
learned	O
weights	O
of	O
softmax	O
into	O
two	O
parts	O
,	O
one	O
part	O
for	O
the	O
word	O
feature	O
and	O
the	O
other	O
part	O
for	O
the	O
visual	O
feature	O
.	O

Therefore	O
,	O
Here	O
the	O
softmax	O
matrix	O
M	O
is	O
decomposed	O
into	O
the	O
weights	O
for	O
word	O
feature	O
and	O
the	O
weights	O
for	O
the	O
visual	O
feature	O
whereas	O
.	O

is	O
the	O
response	O
of	O
the	O
answer	O
class	O
before	O
softmax	Method
normalization	Method
.	O

Denote	O
the	O
response	O
as	O
the	O
contribution	O
from	O
question	O
words	O
and	O
as	O
the	O
contribution	O
from	O
the	O
image	O
contents	O
.	O

Thus	O
for	O
each	O
predicted	O
answer	O
,	O
we	O
know	O
exactly	O
the	O
proportions	O
of	O
contribution	O
from	O
word	O
and	O
image	O
content	O
respectively	O
.	O

We	O
also	O
could	O
rank	O
and	O
to	O
know	O
what	O
the	O
predicted	O
answer	O
could	O
be	O
if	O
the	O
model	O
only	O
relies	O
on	O
one	O
side	O
of	O
information	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
some	O
examples	O
of	O
the	O
predictions	O
,	O
revealing	O
that	O
the	O
question	O
words	O
usually	O
have	O
dominant	O
influence	O
on	O
predicting	O
the	O
answer	O
.	O

For	O
example	O
,	O
the	O
correctly	O
predicted	O
answers	O
for	O
the	O
two	O
questions	O
given	O
for	O
the	O
first	O
image	O
‘	O
what	O
is	O
the	O
color	O
of	O
sofa	O
’	O
and	O
‘	O
which	O
brand	O
is	O
the	O
laptop	O
’	O
come	O
mostly	O
from	O
the	O
question	O
words	O
,	O
without	O
the	O
need	O
for	O
image	O
.	O

This	O
demonstrates	O
the	O
bias	O
in	O
the	O
frequency	O
of	O
object	O
and	O
actions	O
appearing	O
in	O
the	O
images	O
of	O
COCO	Material
dataset	Material
.	O

For	O
the	O
second	O
image	O
,	O
we	O
ask	O
‘	O
what	O
are	O
they	O
doing	O
’	O
:	O
the	O
words	Method
-	Method
only	Method
prediction	Method
gives	O
‘	O
playing	O
wii	O
(	O
10.62	O
)	O
,	O
eating	O
(	O
9.97	O
)	O
,	O
playing	O
frisbee	O
(	O
9.24	O
)	O
’	O
,	O
while	O
full	Method
prediction	Method
gives	O
the	O
correct	O
prediction	O
‘	O
playing	O
baseball	O
(	O
10.67	O
=	O
2.01	O
[	O
image	O
]	O
+	O
8.66	O
[	O
word	O
]	O
)	O
’	O
.	O

To	O
further	O
understand	O
the	O
answers	O
predicted	O
by	O
the	O
model	O
given	O
the	O
visual	O
feature	O
and	O
question	O
sentence	O
,	O
we	O
first	O
decompose	O
the	O
word	O
contribution	O
of	O
the	O
answer	O
into	O
single	O
words	O
of	O
the	O
question	O
sentence	O
,	O
then	O
we	O
visualize	O
the	O
informative	O
image	O
regions	O
relevant	O
to	O
the	O
answer	O
through	O
the	O
technique	O
proposed	O
in	O
.	O

Since	O
there	O
are	O
just	O
two	O
linear	Method
transformations	Method
(	O
one	O
is	O
word	Method
embedding	Method
and	O
the	O
other	O
is	O
softmax	Method
matrix	Method
multiplication	Method
)	O
from	O
the	O
one	O
hot	O
vector	O
to	O
the	O
answer	O
response	O
,	O
we	O
could	O
easily	O
know	O
the	O
importance	O
of	O
each	O
single	O
word	O
in	O
the	O
question	O
to	O
the	O
predicted	O
answer	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
plot	O
the	O
ranked	O
word	O
importance	O
for	O
each	O
word	O
in	O
the	O
question	O
sentence	O
.	O

In	O
the	O
first	O
image	O
question	O
word	O
‘	O
doing	O
’	O
is	O
informative	O
to	O
the	O
answer	O
‘	O
texting	O
’	O
while	O
in	O
the	O
second	O
image	O
question	O
word	O
‘	O
eating	O
’	O
is	O
informative	O
to	O
the	O
answer	O
‘	O
hot	O
dog	O
’	O
.	O

To	O
highlight	O
the	O
informative	O
image	O
regions	O
relevant	O
to	O
the	O
predicted	O
answer	O
we	O
apply	O
a	O
technique	O
called	O
Class	Method
Activation	Method
Mapping	Method
(	O
CAM	Method
)	O
proposed	O
in	O
.	O

The	O
CAM	Method
technique	Method
leverages	O
the	O
linear	O
relation	O
between	O
the	O
softmax	Method
prediction	Method
and	O
the	O
final	O
convolutional	O
feature	O
map	O
,	O
which	O
allows	O
us	O
to	O
identify	O
the	O
most	O
discriminative	O
image	O
regions	O
relevant	O
to	O
the	O
predicted	O
result	O
.	O

In	O
Figure	O
[	O
reference	O
]	O
we	O
plot	O
the	O
heatmaps	O
generated	O
by	O
the	O
CAM	Method
associated	O
with	O
the	O
predicted	O
answer	O
,	O
which	O
highlight	O
the	O
informative	O
image	O
regions	O
such	O
as	O
the	O
cellphone	O
in	O
the	O
first	O
image	O
to	O
the	O
answer	O
‘	O
texting	O
’	O
and	O
the	O
hot	O
dog	O
in	O
the	O
first	O
image	O
to	O
the	O
answer	O
‘	O
hot	O
dog	O
’	O
.	O

The	O
example	O
in	O
lower	O
part	O
of	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
heatmaps	O
generated	O
by	O
two	O
different	O
questions	O
and	O
answers	O
.	O

Visual	O
features	O
from	O
CNN	Method
already	O
have	O
implicit	O
attention	O
and	O
selectivity	O
over	O
the	O
image	O
region	O
,	O
thus	O
the	O
resulting	O
class	O
activation	O
maps	O
are	O
similar	O
to	O
the	O
maps	O
generated	O
by	O
the	O
attention	Method
mechanisms	Method
of	O
the	O
VQA	Method
models	Method
in	O
.	O

section	O
:	O
Interactive	O
Visual	Task
QA	Task
Demo	O
Question	Task
answering	Task
is	O
essentially	O
an	O
interactive	Task
activity	Task
,	O
thus	O
it	O
would	O
be	O
good	O
to	O
make	O
the	O
trained	O
models	O
able	O
to	O
interact	O
with	O
people	O
in	O
real	O
time	O
.	O

Aided	O
by	O
the	O
simplicity	O
of	O
the	O
baseline	O
model	O
,	O
we	O
built	O
a	O
web	O
demo	O
that	O
people	O
could	O
type	O
question	O
about	O
a	O
given	O
image	O
and	O
our	O
AI	Task
system	O
powered	O
by	O
iBOWIMG	Method
will	O
reply	O
the	O
most	O
possible	O
answers	O
.	O

Here	O
the	O
deep	O
feature	O
of	O
the	O
images	O
are	O
extracted	O
beforehand	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
a	O
snapshot	O
of	O
the	O
demo	O
.	O

People	O
could	O
play	O
with	O
the	O
demo	O
to	O
see	O
the	O
strength	O
and	O
weakness	O
of	O
VQA	Method
model	Method
.	O

section	O
:	O
Concluding	O
Remarks	O
For	O
visual	Task
question	Task
answering	Task
on	O
COCO	Material
dataset	Material
,	O
our	O
implementation	O
of	O
a	O
simple	O
baseline	O
achieves	O
comparable	O
performance	O
to	O
several	O
recently	O
proposed	O
recurrent	Method
neural	Method
network	Method
-	Method
based	Method
approaches	Method
.	O

To	O
reach	O
the	O
correct	O
prediction	O
,	O
the	O
baseline	O
captures	O
the	O
correlation	O
between	O
the	O
informative	O
words	O
in	O
the	O
question	O
and	O
the	O
answer	O
,	O
and	O
that	O
between	O
image	O
contents	O
and	O
the	O
answer	O
.	O

How	O
to	O
move	O
beyond	O
this	O
,	O
from	O
memorizing	O
the	O
correlations	O
to	O
actual	O
reasoning	O
and	O
understanding	O
of	O
the	O
question	O
and	O
image	O
,	O
is	O
a	O
goal	O
for	O
future	O
research	O
.	O

bibliography	O
:	O
References	O
