We	O
seek	O
to	O
improve	O
deep	Method
neural	Method
networks	Method
by	O
generalizing	O
the	O
pooling	Method
operations	Method
that	O
play	O
a	O
central	O
role	O
in	O
current	O
architectures	O
.	O

We	O
pursue	O
a	O
careful	O
exploration	O
of	O
approaches	O
to	O
allow	O
pooling	Method
to	O
learn	O
and	O
to	O
adapt	O
to	O
complex	O
and	O
variable	O
patterns	O
.	O

The	O
two	O
primary	O
directions	O
lie	O
in	O
(	O
1	O
)	O
learning	O
a	O
pooling	Method
function	O
via	O
(	O
two	O
strategies	O
of	O
)	O
combining	Method
of	Method
max	Method
and	Method
average	Method
pooling	Method
,	O
and	O
(	O
2	O
)	O
learning	O
a	O
pooling	Method
function	O
in	O
the	O
form	O
of	O
a	O
tree	Method
-	Method
structured	Method
fusion	Method
of	Method
pooling	Method
filters	Method
that	O
are	O
themselves	O
learned	O
.	O

In	O
our	O
experiments	O
every	O
generalized	O
pooling	Method
operation	O
we	O
explore	O
improves	O
performance	O
when	O
used	O
in	O
place	O
of	O
average	Method
or	O
max	Method
pooling	Method
.	O

We	O
experimentally	O
demonstrate	O
that	O
the	O
proposed	O
pooling	Method
operations	Method
provide	O
a	O
boost	O
in	O
invariance	Metric
properties	Metric
relative	O
to	O
conventional	O
pooling	Method
and	O
set	O
the	O
state	O
of	O
the	O
art	O
on	O
several	O
widely	O
adopted	O
benchmark	O
datasets	O
;	O
they	O
are	O
also	O
easy	O
to	O
implement	O
,	O
and	O
can	O
be	O
applied	O
within	O
various	O
deep	Method
neural	Method
network	Method
architectures	Method
.	O

These	O
benefits	O
come	O
with	O
only	O
a	O
light	O
increase	O
in	O
computational	Metric
overhead	Metric
during	O
training	Task
and	O
a	O
very	O
modest	O
increase	O
in	O
the	O
number	O
of	O
model	O
parameters	O
.	O

Generalizing	Method
Pooling	Method
Functions	Method
in	O
CNNs	Method
:	O
Mixed	O
,	O
Gated	O
,	O
and	O
Tree	O
section	O
:	O
Introduction	O
The	O
recent	O
resurgence	O
of	O
neurally	Method
-	Method
inspired	Method
systems	Method
such	O
as	O
deep	Method
belief	Method
nets	Method
(	O
DBN	Method
)	Method
,	O
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
,	O
and	O
the	O
sum	O
-	O
and	O
-	O
max	Method
infrastructure	O
has	O
derived	O
significant	O
benefit	O
from	O
building	O
more	O
sophisticated	O
network	Method
structures	Method
and	O
from	O
bringing	O
learning	Task
to	O
non	O
-	O
linear	O
activations	O
.	O

The	O
pooling	Method
operation	Method
has	O
also	O
played	O
a	O
central	O
role	O
,	O
contributing	O
to	O
invariance	O
to	O
data	O
variation	O
and	O
perturbation	O
.	O

However	O
,	O
pooling	Method
operations	Method
have	O
been	O
little	O
revised	O
beyond	O
the	O
current	O
primary	O
options	O
of	O
average	Method
,	O
max	Method
,	O
and	O
stochastic	Method
pooling	Method
;	O
this	O
despite	O
indications	O
that	O
e.g.	O
choosing	O
from	O
more	O
than	O
just	O
one	O
type	O
of	O
pooling	Method
operation	Method
can	O
benefit	O
performance	O
.	O

In	O
this	O
paper	O
,	O
we	O
desire	O
to	O
bring	O
learning	Task
and	O
“	O
responsiveness	O
”	O
(	O
i.e.	O
,	O
to	O
characteristics	O
of	O
the	O
region	O
being	O
pooled	O
)	O
into	O
the	O
pooling	Method
operation	Method
.	O

Various	O
approaches	O
are	O
possible	O
,	O
but	O
here	O
we	O
pursue	O
two	O
in	O
particular	O
.	O

In	O
the	O
first	O
approach	O
,	O
we	O
consider	O
combining	O
typical	O
pooling	Method
operations	Method
(	O
specifically	O
,	O
max	Method
pooling	Method
and	O
average	Method
pooling	Method
)	O
;	O
within	O
this	O
approach	O
we	O
further	O
investigate	O
two	O
strategies	O
by	O
which	O
to	O
combine	O
these	O
operations	O
.	O

One	O
of	O
the	O
strategies	O
is	O
“	O
unresponsive	O
”	O
;	O
for	O
reasons	O
discussed	O
later	O
,	O
we	O
call	O
this	O
strategy	O
mixed	O
max	Method
-	O
average	Method
pooling	Method
.	O

The	O
other	O
strategy	O
is	O
“	O
responsive	O
”	O
;	O
we	O
call	O
this	O
strategy	O
gated	O
max	Method
-	O
average	Method
pooling	Method
,	O
where	O
the	O
ability	O
to	O
be	O
responsive	O
is	O
provided	O
by	O
a	O
“	O
gate	O
”	O
in	O
analogy	O
to	O
the	O
usage	O
of	O
gates	O
elsewhere	O
in	O
deep	Method
learning	Method
.	O

Another	O
natural	O
generalization	O
of	O
pooling	Method
operations	O
is	O
to	O
allow	O
the	O
pooling	Method
operations	Method
that	O
are	O
being	O
combined	O
to	O
themselves	O
be	O
learned	O
.	O

Hence	O
in	O
the	O
second	O
approach	O
,	O
we	O
learn	O
to	O
combine	O
pooling	Method
filters	Method
that	O
are	O
themselves	O
learned	O
.	O

Specifically	O
,	O
the	O
learning	Task
is	O
performed	O
within	O
a	O
binary	O
tree	O
(	O
with	O
number	O
of	O
levels	O
that	O
is	O
pre	O
-	O
specified	O
rather	O
than	O
“	O
grown	O
”	O
as	O
in	O
traditional	O
decision	O
trees	O
)	O
in	O
which	O
each	O
leaf	O
is	O
associated	O
with	O
a	O
learned	O
pooling	Method
filter	Method
.	O

As	O
we	O
consider	O
internal	O
nodes	O
of	O
the	O
tree	O
,	O
each	O
parent	O
node	O
is	O
associated	O
with	O
an	O
output	O
value	O
that	O
is	O
the	O
mixture	O
of	O
the	O
child	O
node	O
output	O
values	O
,	O
until	O
we	O
finally	O
reach	O
the	O
root	O
node	O
.	O

The	O
root	O
node	O
corresponds	O
to	O
the	O
overall	O
output	O
produced	O
by	O
the	O
tree	O
.	O

We	O
refer	O
to	O
this	O
strategy	O
as	O
tree	Method
pooling	Method
.	O

Tree	Method
pooling	Method
is	O
intended	O
(	O
1	O
)	O
to	O
learn	O
pooling	Method
filters	Method
directly	O
from	O
the	O
data	O
;	O
(	O
2	O
)	O
to	O
learn	O
how	O
to	O
combine	O
leaf	O
node	O
pooling	Method
filters	O
in	O
a	O
differentiable	O
fashion	O
;	O
(	O
3	O
)	O
to	O
bring	O
together	O
these	O
other	O
characteristics	O
within	O
a	O
hierarchical	O
tree	O
structure	O
.	O

When	O
the	O
mixing	O
of	O
the	O
node	O
outputs	O
is	O
allowed	O
to	O
be	O
“	O
responsive	O
”	O
,	O
the	O
resulting	O
tree	Method
pooling	Method
operation	Method
becomes	O
an	O
integrated	O
method	O
for	O
learning	O
pooling	Method
filters	Method
and	O
combinations	O
of	O
those	O
filters	O
that	O
are	O
able	O
to	O
display	O
a	O
range	O
of	O
different	O
behaviors	O
depending	O
on	O
the	O
characteristics	O
of	O
the	O
region	O
being	O
pooled	O
.	O

We	O
pursue	O
experimental	O
validation	O
and	O
find	O
that	O
:	O
In	O
the	O
architectures	O
we	O
investigate	O
,	O
replacing	O
standard	O
pooling	Method
operations	Method
with	O
any	O
of	O
our	O
proposed	O
generalized	O
pooling	Method
methods	O
boosts	O
performance	O
on	O
each	O
of	O
the	O
standard	O
benchmark	O
datasets	O
,	O
as	O
well	O
as	O
on	O
the	O
larger	O
and	O
more	O
complex	O
ImageNet	Material
dataset	O
.	O

We	O
attain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
MNIST	Material
,	O
CIFAR10	Material
(	O
with	O
and	O
without	O
data	O
augmentation	O
)	O
,	O
and	O
SVHN	Material
.	O

Our	O
proposed	O
pooling	Method
operations	Method
can	O
be	O
used	O
as	O
drop	Method
-	Method
in	Method
replacements	Method
for	O
standard	O
pooling	Method
operations	Method
in	O
various	O
current	O
architectures	O
and	O
can	O
be	O
used	O
in	O
tandem	O
with	O
other	O
performance	Method
-	Method
boosting	Method
approaches	Method
such	O
as	O
learning	Method
activation	Method
functions	Method
,	O
training	O
with	O
data	Task
augmentation	Task
,	O
or	O
modifying	O
other	O
aspects	O
of	O
network	Method
architecture	Method
—	O
we	O
confirm	O
improvements	O
when	O
used	O
in	O
a	O
DSN	Method
-	Method
style	Method
architecture	Method
,	O
as	O
well	O
as	O
in	O
AlexNet	Method
and	O
GoogLeNet	Method
.	O

Our	O
proposed	O
pooling	Method
operations	Method
are	O
also	O
simple	O
to	O
implement	O
,	O
computationally	O
undemanding	O
(	O
ranging	O
from	O
to	O
additional	O
overhead	O
in	O
timing	O
experiments	O
)	O
,	O
differentiable	O
,	O
and	O
use	O
only	O
a	O
modest	O
number	O
of	O
additional	O
parameters	O
.	O

section	O
:	O
Related	O
Work	O
In	O
the	O
current	O
deep	Task
learning	Task
literature	Task
,	O
popular	O
pooling	Method
functions	Method
include	O
max	Method
,	O
average	Method
,	O
and	O
stochastic	Method
pooling	Method
.	O

A	O
recent	O
effort	O
using	O
more	O
complex	O
pooling	Method
operations	Method
,	O
spatial	O
pyramid	O
pooling	Method
,	O
is	O
mainly	O
designed	O
to	O
deal	O
with	O
images	O
of	O
varying	O
size	O
,	O
rather	O
than	O
delving	O
in	O
to	O
different	O
pooling	Method
functions	O
or	O
incorporating	O
learning	Method
.	O

Learning	O
pooling	Method
functions	O
is	O
analogous	O
to	O
receptive	Method
field	Method
learning	Method
.	O

However	O
methods	O
like	O
lead	O
to	O
a	O
more	O
difficult	O
learning	Method
procedure	Method
that	O
in	O
turn	O
leads	O
to	O
a	O
less	O
competitive	O
result	O
,	O
e.g.	O
an	O
error	Metric
rate	Metric
of	O
on	O
unaugmented	Material
CIFAR10	Material
.	O

Since	O
our	O
tree	Method
pooling	Method
approach	Method
involves	O
a	O
tree	O
structure	O
in	O
its	O
learning	Task
,	O
we	O
observe	O
an	O
analogy	O
to	O
“	O
logic	Method
-	Method
type	Method
”	Method
approaches	Method
such	O
as	O
decision	O
trees	O
or	O
“	O
logical	O
operators	O
”	O
.	O

Such	O
approaches	O
have	O
played	O
a	O
central	O
role	O
in	O
artificial	Task
intelligence	Task
for	O
applications	O
that	O
require	O
“	O
discrete	Task
”	Task
reasoning	Task
,	O
and	O
are	O
often	O
intuitively	O
appealing	O
.	O

Unfortunately	O
,	O
despite	O
the	O
appeal	O
of	O
such	O
logic	Method
-	Method
type	Method
approaches	Method
,	O
there	O
is	O
a	O
disconnect	O
between	O
the	O
functioning	O
of	O
decision	Method
trees	Method
and	O
the	O
functioning	O
of	O
CNNs	Method
—	O
the	O
output	O
of	O
a	O
standard	O
decision	Method
tree	Method
is	O
non	O
-	O
continuous	O
with	O
respect	O
to	O
its	O
input	O
(	O
and	O
thus	O
nondifferentiable	O
)	O
.	O

This	O
means	O
that	O
a	O
standard	O
decision	Method
tree	Method
is	O
not	O
able	O
to	O
be	O
used	O
in	O
CNNs	Method
,	O
whose	O
learning	Method
process	Method
is	O
performed	O
by	O
back	Method
propagation	Method
using	O
gradients	Method
of	Method
differentiable	Method
functions	Method
.	O

Part	O
of	O
what	O
allows	O
us	O
to	O
pursue	O
our	O
approaches	O
is	O
that	O
we	O
ensure	O
the	O
resulting	O
pooling	Method
operation	Method
is	O
differentiable	O
and	O
thus	O
usable	O
within	O
network	Method
backpropagation	Method
.	O

A	O
recent	O
work	O
,	O
referred	O
to	O
as	O
auto	Method
-	Method
encoder	Method
trees	Method
,	O
also	O
pays	O
attention	O
to	O
a	O
differentiable	O
use	O
of	O
tree	O
structures	O
in	O
deep	Task
learning	Task
,	O
but	O
is	O
distinct	O
from	O
our	O
method	O
as	O
it	O
focuses	O
on	O
learning	O
encoding	Method
and	Method
decoding	Method
methods	Method
(	O
rather	O
than	O
pooling	Method
methods	Method
)	O
using	O
a	O
“	O
soft	O
”	O
decision	O
tree	O
for	O
a	O
generative	Method
model	Method
.	O

In	O
the	O
supervised	Task
setting	Task
,	O
incorporates	O
multilayer	Method
perceptrons	Method
within	O
decision	Method
trees	Method
,	O
but	O
simply	O
uses	O
trained	O
perceptrons	Method
as	O
splitting	O
nodes	O
in	O
a	O
decision	Method
forest	Method
;	O
not	O
only	O
does	O
this	O
result	O
in	O
training	O
processes	O
that	O
are	O
separate	O
(	O
and	O
thus	O
more	O
difficult	O
to	O
train	O
than	O
an	O
integrated	O
training	Method
process	Method
)	O
,	O
this	O
training	Method
process	Method
does	O
not	O
involve	O
the	O
learning	O
of	O
any	O
pooling	Method
filters	Method
.	O

section	O
:	O
Generalizing	O
Pooling	Method
Operations	Method
A	O
typical	O
convolutional	Method
neural	Method
network	Method
is	O
structured	O
as	O
a	O
series	O
of	O
convolutional	Method
layers	Method
and	O
pooling	Method
layers	Method
.	O

Each	O
convolutional	Method
layer	Method
is	O
intended	O
to	O
produce	O
representations	O
(	O
in	O
the	O
form	O
of	O
activation	O
values	O
)	O
that	O
reflect	O
aspects	O
of	O
local	O
spatial	O
structures	O
,	O
and	O
to	O
consider	O
multiple	O
channels	O
when	O
doing	O
so	O
.	O

More	O
specifically	O
,	O
a	O
convolution	Method
layer	Method
computes	O
“	O
feature	O
response	O
maps	O
”	O
that	O
involve	O
multiple	O
channels	O
within	O
some	O
localized	O
spatial	O
region	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
pooling	Method
layer	Method
is	O
restricted	O
to	O
act	O
within	O
just	O
one	O
channel	O
at	O
a	O
time	O
,	O
“	O
condensing	O
”	O
the	O
activation	O
values	O
in	O
each	O
spatially	O
-	O
local	O
region	O
in	O
the	O
currently	O
considered	O
channel	O
.	O

An	O
early	O
reference	O
related	O
to	O
pooling	Method
operations	Method
(	O
although	O
not	O
explicitly	O
using	O
the	O
term	O
“	O
pooling	Method
”	O
)	O
can	O
be	O
found	O
in	O
.	O

In	O
modern	O
visual	Task
recognition	Task
systems	Task
,	O
pooling	Method
operations	Method
play	O
a	O
role	O
in	O
producing	O
“	O
downstream	Method
”	Method
representations	Method
that	O
are	O
more	O
robust	O
to	O
the	O
effects	O
of	O
variations	O
in	O
data	O
while	O
still	O
preserving	O
important	O
motifs	O
.	O

The	O
specific	O
choices	O
of	O
average	Method
pooling	Method
and	O
max	Method
pooling	Method
have	O
been	O
widely	O
used	O
in	O
many	O
CNN	Method
-	Method
like	Method
architectures	Method
;	O
includes	O
a	O
theoretical	O
analysis	O
(	O
albeit	O
one	O
based	O
on	O
assumptions	O
that	O
do	O
not	O
hold	O
here	O
)	O
.	O

Our	O
goal	O
is	O
to	O
bring	O
learning	O
and	O
“	O
responsiveness	O
”	O
into	O
the	O
pooling	Method
operation	Method
.	O

We	O
focus	O
on	O
two	O
approaches	O
in	O
particular	O
.	O

In	O
the	O
first	O
approach	O
,	O
we	O
begin	O
with	O
the	O
(	O
conventional	O
,	O
non	O
-	O
learned	O
)	O
pooling	Method
operations	Method
of	O
max	Method
pooling	Method
and	O
average	Method
pooling	Method
and	O
learn	O
to	O
combine	O
them	O
.	O

Within	O
this	O
approach	O
,	O
we	O
further	O
consider	O
two	O
strategies	O
by	O
which	O
to	O
combine	O
these	O
fixed	O
pooling	Method
operations	O
.	O

One	O
of	O
these	O
strategies	O
is	O
“	O
unresponsive	O
”	O
to	O
the	O
characteristics	O
of	O
the	O
region	O
being	O
pooled	O
;	O
the	O
learning	Method
process	Method
in	O
this	O
strategy	O
will	O
result	O
in	O
an	O
effective	O
pooling	Method
operation	Method
that	O
is	O
some	O
specific	O
,	O
unchanging	O
“	O
mixture	O
”	O
of	O
max	Method
and	O
average	Method
.	O

To	O
emphasize	O
this	O
unchanging	O
mixture	O
,	O
we	O
refer	O
to	O
this	O
strategy	O
as	O
mixed	O
max	Method
-	O
average	Method
pooling	Method
.	O

The	O
other	O
strategy	O
is	O
“	O
responsive	O
”	O
to	O
the	O
characteristics	O
of	O
the	O
region	O
being	O
pooled	O
;	O
the	O
learning	Method
process	Method
in	O
this	O
strategy	O
results	O
in	O
a	O
“	O
gating	O
mask	O
”	O
.	O

This	O
learned	O
gating	O
mask	O
is	O
then	O
used	O
to	O
determine	O
a	O
“	O
responsive	O
”	O
mix	O
of	O
max	Method
pooling	Method
and	O
average	Method
pooling	Method
;	O
specifically	O
,	O
the	O
value	O
of	O
the	O
inner	O
product	O
between	O
the	O
gating	O
mask	O
and	O
the	O
current	O
region	O
being	O
pooled	O
is	O
fed	O
through	O
a	O
sigmoid	Method
,	O
the	O
output	O
of	O
which	O
is	O
used	O
as	O
the	O
mixing	O
proportion	O
between	O
max	Method
and	O
average	Method
.	O

To	O
emphasize	O
the	O
role	O
of	O
the	O
gating	O
mask	O
in	O
determining	O
the	O
“	O
responsive	O
”	O
mixing	O
proportion	O
,	O
we	O
refer	O
to	O
this	O
strategy	O
as	O
gated	O
max	Method
-	O
average	Method
pooling	Method
.	O

Both	O
the	O
mixed	Method
strategy	Method
and	O
the	O
gated	Method
strategy	Method
involve	O
combinations	O
of	O
fixed	O
pooling	Method
operations	O
;	O
a	O
complementary	O
generalization	O
to	O
these	O
strategies	O
is	O
to	O
learn	O
the	O
pooling	Method
operations	O
themselves	O
.	O

From	O
this	O
,	O
we	O
are	O
in	O
turn	O
led	O
to	O
consider	O
learning	O
pooling	Method
operations	Method
and	O
also	O
learning	O
to	O
combine	O
those	O
pooling	Method
operations	Method
.	O

Since	O
these	O
combinations	O
can	O
be	O
considered	O
within	O
the	O
context	O
of	O
a	O
binary	O
tree	O
structure	O
,	O
we	O
refer	O
to	O
this	O
approach	O
as	O
tree	Method
pooling	Method
.	O

We	O
pursue	O
further	O
details	O
in	O
the	O
following	O
sections	O
.	O

subsection	O
:	O
Combining	O
max	Method
and	O
average	Method
pooling	Method
functions	O
subsubsection	O
:	O
“	O
Mixed	O
”	O
max	Method
-	O
average	Method
pooling	Method
The	O
conventional	O
pooling	Method
operation	Method
is	O
fixed	O
to	O
be	O
either	O
a	O
simple	O
average	Method
or	O
a	O
maximum	Method
operation	Method
,	O
where	O
the	O
vector	O
contains	O
the	O
activation	O
values	O
from	O
a	O
local	O
pooling	Method
region	O
of	O
pixels	O
(	O
typical	O
pooling	Method
region	O
dimensions	O
are	O
or	O
)	O
in	O
an	O
image	O
or	O
a	O
channel	O
.	O

At	O
present	O
,	O
max	Method
pooling	Method
is	O
often	O
used	O
as	O
the	O
default	O
in	O
CNNs	Task
.	O

We	O
touch	O
on	O
the	O
relative	O
performance	O
of	O
max	Method
pooling	Method
and	O
,	O
e.g.	O
,	O
average	Method
pooling	Method
as	O
part	O
of	O
a	O
collection	O
of	O
exploratory	O
experiments	O
to	O
test	O
the	O
invariance	O
properties	O
of	O
pooling	Method
functions	O
under	O
common	O
image	O
transformations	O
(	O
including	O
rotation	O
,	O
translation	O
,	O
and	O
scaling	O
)	O
;	O
see	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
results	O
indicate	O
that	O
,	O
on	O
the	O
evaluation	O
dataset	O
,	O
there	O
are	O
regimes	O
in	O
which	O
either	O
max	Method
pooling	Method
or	O
average	Method
pooling	Method
demonstrates	O
better	O
performance	O
than	O
the	O
other	O
(	O
although	O
we	O
observe	O
that	O
both	O
of	O
these	O
choices	O
are	O
outperformed	O
by	O
our	O
proposed	O
pooling	Method
operations	Method
)	O
.	O

In	O
the	O
light	O
of	O
observation	O
that	O
neither	O
max	Method
pooling	Method
nor	O
average	Method
pooling	Method
dominates	O
the	O
other	O
,	O
a	O
first	O
natural	O
generalization	O
is	O
the	O
strategy	O
we	O
call	O
“	O
mixed	O
”	O
max	Method
-	O
average	Method
pooling	Method
,	O
in	O
which	O
we	O
learn	O
specific	O
mixing	O
proportion	O
parameters	O
from	O
the	O
data	O
.	O

When	O
learning	O
such	O
mixing	Method
proportion	Method
parameters	Method
one	O
has	O
several	O
options	O
(	O
listed	O
in	O
order	O
of	O
increasing	O
number	O
of	O
parameters	O
)	O
:	O
learning	O
one	O
mixing	O
proportion	O
parameter	O
(	O
a	O
)	O
per	O
net	O
,	O
(	O
b	O
)	O
per	O
layer	O
,	O
(	O
c	O
)	O
per	O
layer	O
/	O
region	O
being	O
pooled	O
(	O
but	O
used	O
for	O
all	O
channels	O
across	O
that	O
region	O
)	O
,	O
(	O
d	O
)	O
per	O
layer	O
/	O
channel	O
(	O
but	O
used	O
for	O
all	O
regions	O
in	O
each	O
channel	O
)	O
(	O
e	O
)	O
per	O
layer	O
/	O
region	O
/	O
channel	O
combination	O
.	O

The	O
form	O
for	O
each	O
“	O
mixed	O
”	O
pooling	Method
operation	O
(	O
written	O
here	O
for	O
the	O
“	O
one	O
per	O
layer	O
”	O
option	O
;	O
the	O
expression	O
for	O
other	O
options	O
differs	O
only	O
in	O
the	O
subscript	O
of	O
the	O
mixing	O
proportion	O
)	O
is	O
:	O
where	O
is	O
a	O
scalar	O
mixing	O
proportion	O
specifying	O
the	O
specific	O
combination	O
of	O
max	Method
and	O
average	Method
;	O
the	O
subscript	O
is	O
used	O
to	O
indicate	O
that	O
this	O
equation	O
is	O
for	O
the	O
“	O
one	O
per	O
layer	O
”	O
option	O
.	O

Once	O
the	O
output	O
loss	O
function	O
is	O
defined	O
,	O
we	O
can	O
automatically	O
learn	O
each	O
mixing	O
proportion	O
(	O
where	O
we	O
now	O
suppress	O
any	O
subscript	O
specifying	O
which	O
of	O
the	O
options	O
we	O
choose	O
)	O
.	O

Vanilla	Method
backpropagation	Method
for	O
this	O
learning	Task
is	O
given	O
by	O
where	O
is	O
the	O
error	Metric
backpropagated	O
from	O
the	O
following	O
layer	O
.	O

Since	O
pooling	Method
operations	Method
are	O
typically	O
placed	O
in	O
the	O
midst	O
of	O
a	O
deep	Method
neural	Method
network	Method
,	O
we	O
also	O
need	O
to	O
compute	O
the	O
error	Metric
signal	O
to	O
be	O
propagated	O
back	O
to	O
the	O
previous	O
layer	O
:	O
where	O
denotes	O
the	O
indicator	O
function	O
.	O

In	O
the	O
experiment	O
section	O
,	O
we	O
report	O
results	O
for	O
the	O
“	O
one	O
parameter	O
per	O
pooling	Method
layer	O
”	O
option	O
;	O
the	O
network	O
for	O
this	O
experiment	O
has	O
2	O
pooling	Method
layers	O
and	O
so	O
has	O
2	O
more	O
parameters	O
than	O
a	O
network	O
using	O
standard	O
pooling	Method
operations	Method
.	O

We	O
found	O
that	O
even	O
this	O
simple	O
option	O
yielded	O
a	O
surprisingly	O
large	O
performance	O
boost	O
.	O

We	O
also	O
obtain	O
results	O
for	O
a	O
simple	O
50	O
/	O
50	O
mix	O
of	O
max	Method
and	O
average	Method
,	O
as	O
well	O
as	O
for	O
the	O
option	O
with	O
the	O
largest	O
number	O
of	O
parameters	O
:	O
one	O
parameter	O
for	O
each	O
combination	O
of	O
layer	O
/	O
channel	O
/	O
region	O
,	O
or	O
parameters	O
for	O
each	O
“	O
mixed	O
”	O
pooling	Method
layer	O
using	O
this	O
option	O
(	O
where	O
is	O
the	O
number	O
of	O
channels	O
being	O
pooled	O
by	O
the	O
pooling	Method
layer	Method
,	O
and	O
the	O
number	O
of	O
spatial	O
regions	O
being	O
pooled	O
in	O
each	O
channel	O
is	O
)	O
.	O

We	O
observe	O
that	O
the	O
increase	O
in	O
the	O
number	O
of	O
parameters	O
is	O
not	O
met	O
with	O
a	O
corresponding	O
boost	O
in	O
performance	O
,	O
and	O
so	O
we	O
pursue	O
the	O
“	O
one	Method
per	Method
layer	Method
”	O
option	O
.	O

subsubsection	O
:	O
“	O
Gated	O
”	O
max	Method
-	O
average	Method
pooling	Method
In	O
the	O
previous	O
section	O
we	O
considered	O
a	O
strategy	O
that	O
we	O
referred	O
to	O
as	O
“	O
mixed	O
”	O
max	Method
-	O
average	Method
pooling	Method
;	O
in	O
that	O
strategy	O
we	O
learned	O
a	O
mixing	O
proportion	O
to	O
be	O
used	O
in	O
combining	O
max	Method
pooling	Method
and	O
average	Method
pooling	Method
.	O

As	O
mentioned	O
earlier	O
,	O
once	O
learned	O
,	O
each	O
mixing	O
proportion	O
remains	O
fixed	O
—	O
it	O
is	O
“	O
nonresponsive	O
”	O
insofar	O
as	O
it	O
remains	O
the	O
same	O
no	O
matter	O
what	O
characteristics	O
are	O
present	O
in	O
the	O
region	O
being	O
pooled	O
.	O

We	O
now	O
consider	O
a	O
“	O
responsive	Method
”	Method
strategy	Method
that	O
we	O
call	O
“	O
gated	O
”	O
max	Method
-	O
average	Method
pooling	Method
.	O

In	O
this	O
strategy	O
,	O
rather	O
than	O
directly	O
learning	O
a	O
mixing	O
proportion	O
that	O
will	O
be	O
fixed	O
after	O
learning	O
,	O
we	O
instead	O
learn	O
a	O
“	O
gating	O
mask	O
”	O
(	O
with	O
spatial	O
dimensions	O
matching	O
that	O
of	O
the	O
regions	O
being	O
pooled	O
)	O
.	O

The	O
scalar	O
result	O
of	O
the	O
inner	O
product	O
between	O
the	O
gating	O
mask	O
and	O
the	O
region	O
being	O
pooled	O
is	O
fed	O
through	O
a	O
sigmoid	Method
to	O
produce	O
the	O
value	O
that	O
we	O
use	O
as	O
the	O
mixing	O
proportion	O
.	O

This	O
strategy	O
means	O
that	O
the	O
actual	O
mixing	O
proportion	O
can	O
vary	O
during	O
use	O
depending	O
on	O
characteristics	O
present	O
in	O
the	O
region	O
being	O
pooled	O
.	O

To	O
be	O
more	O
specific	O
,	O
suppose	O
we	O
use	O
to	O
denote	O
the	O
values	O
in	O
the	O
region	O
being	O
pooled	O
and	O
to	O
denote	O
the	O
values	O
in	O
a	O
“	O
gating	O
mask	O
”	O
.	O

The	O
“	O
responsive	O
”	O
mixing	O
proportion	O
is	O
then	O
given	O
by	O
,	O
where	O
is	O
a	O
sigmoid	Method
function	Method
.	O

Analogously	O
to	O
the	O
strategy	O
of	O
learning	O
mixing	O
proportion	O
parameter	O
,	O
when	O
learning	O
gating	O
masks	O
one	O
has	O
several	O
options	O
(	O
listed	O
in	O
order	O
of	O
increasing	O
number	O
of	O
parameters	O
)	O
:	O
learning	O
one	O
gating	O
mask	O
(	O
a	O
)	O
per	O
net	O
,	O
(	O
b	O
)	O
per	O
layer	O
,	O
(	O
c	O
)	O
per	O
layer	O
/	O
region	O
being	O
pooled	O
(	O
but	O
used	O
for	O
all	O
channels	O
across	O
that	O
region	O
)	O
,	O
(	O
d	O
)	O
per	O
layer	O
/	O
channel	O
(	O
but	O
used	O
for	O
all	O
regions	O
in	O
each	O
channel	O
)	O
(	O
e	O
)	O
per	O
layer	O
/	O
region	O
/	O
channel	O
combination	O
.	O

We	O
suppress	O
the	O
subscript	O
denoting	O
the	O
specific	O
option	O
,	O
since	O
the	O
equations	O
are	O
otherwise	O
identical	O
for	O
each	O
option	O
.	O

The	O
resulting	O
pooling	Method
operation	Method
for	O
this	O
“	O
gated	O
”	O
max	Method
-	O
average	Method
pooling	Method
is	O
:	O
We	O
can	O
compute	O
the	O
gradient	O
with	O
respect	O
to	O
the	O
internal	O
“	O
gating	O
mask	O
”	O
using	O
the	O
same	O
procedure	O
considered	O
previously	O
,	O
yielding	O
and	O
In	O
a	O
head	O
-	O
to	O
-	O
head	O
parameter	O
count	O
,	O
every	O
single	O
mixing	O
proportion	O
parameter	O
in	O
the	O
“	O
mixed	O
”	O
max	Method
-	O
average	Method
pooling	Method
strategy	O
corresponds	O
to	O
a	O
gating	O
mask	O
in	O
the	O
“	O
gated	Method
”	Method
strategy	Method
(	O
assuming	O
they	O
use	O
the	O
same	O
parameter	O
count	O
option	O
)	O
.	O

To	O
take	O
a	O
specific	O
example	O
,	O
suppose	O
that	O
we	O
consider	O
a	O
network	O
with	O
2	O
pooling	Method
layers	O
and	O
pooling	Method
regions	O
that	O
are	O
.	O

If	O
we	O
use	O
the	O
“	O
mixed	Method
”	Method
strategy	Method
and	O
the	O
per	O
-	O
layer	O
option	O
,	O
we	O
would	O
have	O
a	O
total	O
of	O
extra	O
parameters	O
relative	O
to	O
standard	O
pooling	Method
.	O

If	O
we	O
use	O
the	O
“	O
gated	Method
”	Method
strategy	Method
and	O
the	O
per	Method
-	Method
layer	Method
option	Method
,	O
we	O
would	O
have	O
a	O
total	O
of	O
extra	O
parameters	O
,	O
where	O
is	O
the	O
number	O
of	O
parameters	O
in	O
each	O
gating	O
mask	O
.	O

The	O
“	O
mixed	Method
”	Method
strategy	Method
detailed	O
immediately	O
above	O
uses	O
fewer	O
parameters	O
and	O
is	O
“	O
nonresponsive	O
”	O
;	O
the	O
“	O
gated	Method
”	Method
strategy	Method
involves	O
more	O
parameters	O
and	O
is	O
“	O
responsive	O
”	O
.	O

In	O
our	O
experiments	O
,	O
we	O
find	O
that	O
“	O
mixed	O
”	O
(	O
with	O
one	O
mix	O
per	O
pooling	Method
layer	O
)	O
is	O
outperformed	O
by	O
“	O
gated	Method
”	Method
with	O
one	O
gate	O
per	O
pooling	Method
layer	O
.	O

Interestingly	O
,	O
an	O
parameter	O
“	O
gated	Method
”	Method
network	Method
with	O
only	O
one	O
gate	O
per	O
pooling	Method
layer	O
also	O
outperforms	O
a	O
“	O
mixed	O
”	O
option	O
with	O
far	O
more	O
parameters	O
(	O
with	O
one	O
mix	O
per	O
layer	O
/	O
channel	O
/	O
region	O
)	O
—	O
except	O
on	O
the	O
relatively	O
large	O
SVHN	Material
dataset	Material
.	O

We	O
touch	O
on	O
this	O
below	O
;	O
Section	O
[	O
reference	O
]	O
contains	O
details	O
.	O

subsubsection	O
:	O
Quick	O
comparison	O
:	O
mixed	O
and	O
gated	O
pooling	Method
The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
indicate	O
the	O
benefit	O
of	O
learning	O
pooling	Method
operations	Method
over	O
not	O
learning	O
.	O

Within	O
learned	O
pooling	Method
operations	O
,	O
we	O
see	O
that	O
when	O
the	O
number	O
of	O
parameters	O
in	O
the	O
mixed	Method
strategy	Method
is	O
increased	O
,	O
performance	O
improves	O
;	O
however	O
,	O
parameter	O
count	O
is	O
not	O
the	O
entire	O
story	O
.	O

We	O
see	O
that	O
the	O
“	O
responsive	O
”	O
gated	O
max	Method
-	O
avg	O
strategy	O
consistently	O
yields	O
better	O
performance	O
(	O
using	O
18	O
extra	O
parameters	O
)	O
than	O
is	O
achieved	O
with	O
the	O
40k	O
extra	O
parameters	O
in	O
the	O
1	O
per	O
layer	O
/	O
rg	O
/	O
ch	O
“	O
non	O
-	O
responsive	O
”	O
mixed	O
max	Method
-	O
avg	O
strategy	O
.	O

The	O
relatively	O
larger	O
SVHN	Material
dataset	Material
provides	O
the	O
sole	O
exception	O
(	O
SVHN	Material
has	O
600k	O
training	O
images	O
versus	O
50k	O
for	O
MNIST	Material
,	O
CIFAR10	Material
,	O
and	O
CIFAR100	Material
)	O
—	O
we	O
found	O
baseline	O
1.91	O
%	O
,	O
50	O
/	O
50	O
mix	O
1.84	O
%	O
,	O
mixed	O
(	O
1	O
per	O
lyr	O
)	O
1.76	O
%	O
,	O
mixed	O
(	O
1	O
per	O
lyr	O
/	O
ch	O
/	O
rg	O
)	O
1.64	O
%	O
,	O
and	O
gated	O
(	O
1	O
per	O
lyr	O
)	O
1.74	O
%	O
.	O

Method	O
w	O
/	O
Stochastic	O
no	O
learning	O
0.38	O
0.04	O
8.50	O
0.05	O
7.30	O
0.07	O
33.48	O
0.27	O
w	O
/	O
50	O
/	O
50	O
mix	O
no	O
learning	O
0.34	O
0.012	O
8.11	O
0.10	O
6.78	O
0.17	O
33.53	O
0.16	O
w	O
/	O
Mixed	O
1	O
per	O
pool	O
layer	O
2	O
extra	O
params	O
0.33	O
0.018	O
8.09	O
0.19	O
6.62	O
0.21	O
33.51	O
0.11	O
w	O
/	O
Mixed	O
1	O
per	O
layer	O
/	O
ch	O
/	O
rg	O
40k	O
extra	O
params	O
0.30	O
0.012	O
8.05	O
0.16	O
6.58	O
0.30	O
33.35	O
0.19	O
w	O
/	O
Gated	O
1	O
per	O
pool	O
layer	O
18	O
extra	O
params	O
0.29	O
0.016	O
7.90	O
0.07	O
6.36	O
0.28	O
33.22	O
0.16	O
subsection	O
:	O
Tree	Method
pooling	Method
The	O
strategies	O
described	O
above	O
each	O
involve	O
combinations	O
of	O
fixed	O
pooling	Method
operations	O
;	O
another	O
natural	O
generalization	O
of	O
pooling	Method
operations	O
is	O
to	O
allow	O
the	O
pooling	Method
operations	O
that	O
are	O
being	O
combined	O
to	O
themselves	O
be	O
learned	O
.	O

These	O
pooling	Method
layers	Method
remain	O
distinct	O
from	O
convolution	Method
layers	Method
since	O
pooling	Method
is	O
performed	O
separately	O
within	O
each	O
channel	O
;	O
this	O
channel	O
isolation	O
also	O
means	O
that	O
even	O
the	O
option	O
that	O
introduces	O
the	O
largest	O
number	O
of	O
parameters	O
still	O
introduces	O
far	O
fewer	O
parameters	O
than	O
a	O
convolution	Method
layer	Method
would	O
introduce	O
.	O

The	O
most	O
basic	O
version	O
of	O
this	O
approach	O
would	O
not	O
involve	O
combining	O
learned	O
pooling	Method
operations	O
,	O
but	O
simply	O
learning	O
pooling	Method
operations	Method
in	O
the	O
form	O
of	O
the	O
values	O
in	O
âpooling	O
filtersâ.	O
One	O
step	O
further	O
brings	O
us	O
to	O
what	O
we	O
refer	O
to	O
as	O
tree	Method
pooling	Method
,	O
in	O
which	O
we	O
learn	O
pooling	Method
filters	Method
and	O
also	O
learn	O
to	O
responsively	O
combine	O
those	O
learned	O
filters	O
.	O

Both	O
aspects	O
of	O
this	O
learning	O
are	O
performed	O
within	O
a	O
binary	O
tree	O
(	O
with	O
number	O
of	O
levels	O
that	O
is	O
pre	O
-	O
specified	O
rather	O
than	O
“	O
grown	O
”	O
as	O
in	O
traditional	O
decision	O
trees	O
)	O
in	O
which	O
each	O
leaf	O
is	O
associated	O
with	O
a	O
pooling	Method
filter	Method
learned	O
during	O
training	O
.	O

As	O
we	O
consider	O
internal	O
nodes	O
of	O
the	O
tree	O
,	O
each	O
parent	O
node	O
is	O
associated	O
with	O
an	O
output	O
value	O
that	O
is	O
the	O
mixture	O
of	O
the	O
child	O
node	O
output	O
values	O
,	O
until	O
we	O
finally	O
reach	O
the	O
root	O
node	O
.	O

The	O
root	O
node	O
corresponds	O
to	O
the	O
overall	O
output	O
produced	O
by	O
the	O
tree	O
and	O
each	O
of	O
the	O
mixtures	O
(	O
by	O
which	O
child	O
outputs	O
are	O
“	O
fused	O
”	O
into	O
a	O
parent	O
output	O
)	O
is	O
responsively	O
learned	O
.	O

Tree	Method
pooling	Method
is	O
intended	O
(	O
1	O
)	O
to	O
learn	O
pooling	Method
filters	O
directly	O
from	O
the	O
data	O
;	O
(	O
2	O
)	O
to	O
learn	O
how	O
to	O
“	O
mix	O
”	O
leaf	O
node	O
pooling	Method
filters	O
in	O
a	O
differentiable	O
fashion	O
;	O
(	O
3	O
)	O
to	O
bring	O
together	O
these	O
other	O
characteristics	O
within	O
a	O
hierarchical	O
tree	O
structure	O
.	O

Each	O
leaf	O
node	O
in	O
our	O
tree	O
is	O
associated	O
with	O
a	O
“	O
pooling	Method
filter	Method
”	O
that	O
will	O
be	O
learned	O
;	O
for	O
a	O
node	O
with	O
index	O
,	O
we	O
denote	O
the	O
pooling	Method
filter	Method
by	O
.	O

If	O
we	O
had	O
a	O
“	O
degenerate	O
tree	O
”	O
consisting	O
of	O
only	O
a	O
single	O
(	O
leaf	O
)	O
node	O
,	O
pooling	Method
a	O
region	O
would	O
result	O
in	O
the	O
scalar	O
value	O
.	O

For	O
(	O
internal	O
)	O
nodes	O
(	O
at	O
which	O
two	O
child	O
values	O
are	O
combined	O
into	O
a	O
single	O
parent	O
value	O
)	O
,	O
we	O
proceed	O
in	O
a	O
fashion	O
analogous	O
to	O
the	O
case	O
of	O
gated	O
max	Method
-	O
average	Method
pooling	Method
,	O
with	O
learned	O
“	O
gating	O
masks	O
”	O
denoted	O
(	O
for	O
an	O
internal	O
node	O
)	O
by	O
.	O

The	O
“	O
pooling	Method
result	O
”	O
at	O
any	O
arbitrary	O
node	O
is	O
thus	O
The	O
overall	O
pooling	Method
operation	O
would	O
thus	O
be	O
the	O
result	O
of	O
evaluating	O
.	O

The	O
appeal	O
of	O
this	O
tree	Method
pooling	Method
approach	Method
would	O
be	O
limited	O
if	O
one	O
could	O
not	O
train	O
the	O
proposed	O
layer	O
in	O
a	O
fashion	O
that	O
was	O
integrated	O
within	O
the	O
network	O
as	O
a	O
whole	O
.	O

This	O
would	O
be	O
the	O
case	O
if	O
we	O
attempted	O
to	O
directly	O
use	O
a	O
traditional	O
decision	Method
tree	Method
,	O
since	O
its	O
output	O
presents	O
points	O
of	O
discontinuity	O
with	O
respect	O
to	O
its	O
inputs	O
.	O

The	O
reason	O
for	O
the	O
discontinuity	O
(	O
with	O
respect	O
to	O
input	O
)	O
of	O
traditional	O
decision	O
tree	O
output	O
is	O
that	O
a	O
decision	Method
tree	Method
makes	O
“	O
hard	O
”	O
decisions	O
;	O
in	O
the	O
terminology	O
we	O
have	O
used	O
above	O
,	O
a	O
“	O
hard	O
”	O
decision	O
node	O
corresponds	O
to	O
a	O
mixing	O
proportion	O
that	O
can	O
only	O
take	O
on	O
the	O
value	O
or	O
.	O

The	O
consequence	O
is	O
that	O
this	O
type	O
of	O
“	O
hard	O
”	O
function	O
is	O
not	O
differentiable	O
(	O
nor	O
even	O
continuous	O
with	O
respect	O
to	O
its	O
inputs	O
)	O
,	O
and	O
this	O
in	O
turn	O
interferes	O
with	O
any	O
ability	O
to	O
use	O
it	O
in	O
iterative	Method
parameter	Method
updates	Method
during	O
backpropagation	Method
.	O

This	O
motivates	O
us	O
to	O
instead	O
use	O
the	O
internal	O
node	O
sigmoid	O
“	O
gate	O
”	O
function	O
so	O
that	O
the	O
tree	O
pooling	Method
function	O
as	O
a	O
whole	O
will	O
be	O
differentiable	O
with	O
respect	O
to	O
its	O
parameters	O
and	O
its	O
inputs	O
.	O

For	O
the	O
specific	O
case	O
of	O
a	O
“	O
2	O
level	O
”	O
tree	O
(	O
with	O
leaf	O
nodes	O
“	O
1	O
”	O
and	O
“	O
2	O
”	O
and	O
internal	O
node	O
“	O
3	O
”	O
)	O
pooling	Method
function	O
,	O
we	O
can	O
use	O
the	O
chain	Method
rule	Method
to	O
compute	O
the	O
gradients	O
with	O
respect	O
to	O
the	O
leaf	O
node	O
pooling	Method
filters	O
and	O
the	O
internal	O
node	O
gating	O
mask	O
:	O
The	O
error	Metric
signal	O
to	O
be	O
propagated	O
back	O
to	O
the	O
previous	O
layer	O
is	O
subsubsection	O
:	O
Quick	O
comparison	O
:	O
tree	Method
pooling	Method
Table	O
[	O
reference	O
]	O
collects	O
results	O
related	O
to	O
tree	Method
pooling	Method
.	O

We	O
observe	O
that	O
on	O
all	O
datasets	O
but	O
the	O
comparatively	O
simple	O
MNIST	Material
,	O
adding	O
a	O
level	O
to	O
the	O
tree	Method
pooling	Method
operation	Method
improves	O
performance	O
.	O

However	O
,	O
even	O
further	O
benefit	O
is	O
obtained	O
from	O
the	O
use	O
of	O
tree	Method
pooling	Method
in	O
the	O
first	O
pooling	Method
layer	Method
and	O
gated	Method
max	Method
-	Method
avg	Method
in	O
the	O
second	O
.	O

Method	O
Tree	O
2	O
level	O
;	O
1	O
per	O
pool	O
layer	O
Tree	O
3	O
level	O
;	O
1	O
per	O
pool	O
layer	O
Tree	O
+	O
Max	O
-	O
Avg	O
1	O
per	O
pool	O
layer	O
Comparison	O
with	O
making	O
the	O
network	O
deeper	O
using	O
conv	O
layers	O
To	O
further	O
investigate	O
whether	O
simply	O
âadding	O
depthâ	O
to	O
our	O
baseline	O
network	O
gives	O
a	O
performance	O
boost	O
comparable	O
to	O
that	O
observed	O
for	O
our	O
proposed	O
pooling	Method
operations	Method
,	O
we	O
report	O
in	O
Table	O
[	O
reference	O
]	O
below	O
some	O
additional	O
experiments	O
on	O
CIFAR10	Material
(	O
error	Metric
rate	Metric
in	O
percent	O
;	O
no	O
data	Metric
augmentation	Metric
)	O
.	O

If	O
we	O
count	O
depth	O
by	O
counting	O
any	O
layer	O
with	O
learned	O
parameters	O
as	O
an	O
extra	O
layer	O
of	O
depth	O
(	O
even	O
if	O
there	O
is	O
only	O
1	O
parameter	O
)	O
,	O
the	O
number	O
of	O
parameter	O
layers	O
in	O
a	O
baseline	Method
network	Method
with	O
2	O
additional	O
standard	O
convolution	Method
layers	Method
matches	O
the	O
number	O
of	O
parameter	O
layers	O
in	O
our	O
best	O
performing	O
net	O
(	O
although	O
the	O
convolution	Method
layers	Method
contain	O
many	O
more	O
parameters	O
)	O
.	O

Our	O
method	O
requires	O
only	O
extra	O
parameters	O
and	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
error	Metric
.	O

On	O
the	O
other	O
hand	O
,	O
making	O
networks	O
deeper	O
with	O
conv	Method
layers	Method
adds	O
many	O
more	O
parameters	O
but	O
yields	O
test	O
error	Metric
that	O
does	O
not	O
drop	O
below	O
in	O
the	O
configuration	O
explored	O
.	O

Since	O
we	O
follow	O
each	O
additional	O
conv	Method
layer	Method
with	O
a	O
ReLU	Method
,	O
these	O
networks	O
correspond	O
to	O
increasing	O
nonlinearity	O
as	O
well	O
as	O
adding	O
depth	O
and	O
adding	O
(	O
many	O
)	O
parameters	O
.	O

These	O
experiments	O
indicate	O
that	O
the	O
performance	O
of	O
our	O
proposed	O
pooling	Method
is	O
not	O
accounted	O
for	O
as	O
a	O
simple	O
effect	O
of	O
the	O
addition	O
of	O
depth	O
/	O
parameters	O
/	O
nonlinearity	O
.	O

Method	O
Extra	O
parameters	O
Comparison	O
with	O
alternative	O
pooling	Method
layers	Method
To	O
see	O
whether	O
we	O
might	O
find	O
similar	O
performance	O
boosts	O
by	O
replacing	O
the	O
max	Method
pooling	Method
in	O
the	O
baseline	Method
network	Method
configuration	Method
with	O
alternative	O
pooling	Method
operations	Method
such	O
as	O
stochastic	Method
pooling	Method
,	O
“	O
pooling	Method
”	O
using	O
a	O
stride	Method
2	Method
convolution	Method
layer	Method
as	O
pooling	Method
(	O
cf	O
All	Method
-	Method
CNN	Method
)	O
,	O
or	O
a	O
simple	O
fixed	O
50	O
/	O
50	O
proportion	O
in	O
max	Method
-	O
avg	O
pooling	Method
,	O
we	O
performed	O
another	O
set	O
of	O
experiments	O
on	O
unaugmented	Material
CIFAR10	Material
.	O

From	O
the	O
baseline	O
error	Metric
rate	O
of	O
9.10	O
%	O
,	O
replacing	O
each	O
of	O
the	O
2	O
max	Method
pooling	Method
layers	O
with	O
stacked	O
stride	O
2	O
conv	O
:	O
ReLU	Method
(	O
as	O
in	O
)	O
lowers	O
the	O
error	Metric
to	O
8.77	O
%	O
,	O
but	O
adds	O
0.5	O
M	O
extra	O
parameters	O
.	O

Using	O
stochastic	Method
pooling	Method
adds	O
computational	Metric
overhead	Metric
but	O
no	O
parameters	O
and	O
results	O
in	O
8.50	O
%	O
error	Metric
.	O

A	O
simple	O
50	O
/	O
50	O
mix	O
of	O
max	Method
and	O
average	Method
is	O
computationally	O
light	O
and	O
yields	O
8.07	O
%	O
error	Metric
with	O
no	O
additional	O
parameters	O
.	O

Finally	O
,	O
our	O
tree	Method
+	Method
gated	Method
max	Method
-	Method
avg	Method
configuration	Method
adds	O
72	O
parameters	O
and	O
achieves	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
7.62	O
%	O
error	Metric
.	O

section	O
:	O
Quick	O
Performance	O
Overview	O
For	O
ease	O
of	O
discussion	O
,	O
we	O
collect	O
here	O
observations	O
from	O
subsequent	O
experiments	O
with	O
a	O
view	O
to	O
highlighting	O
aspects	O
that	O
shed	O
light	O
on	O
the	O
performance	O
characteristics	O
of	O
our	O
proposed	O
pooling	Method
functions	Method
.	O

First	O
,	O
as	O
seen	O
in	O
the	O
experiment	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
replacing	O
standard	O
pooling	Method
operations	Method
with	O
either	O
gated	Method
max	Method
-	Method
avg	Method
or	O
(	O
2	O
level	O
)	O
tree	Method
pooling	Method
(	O
each	O
using	O
the	O
“	O
one	O
per	O
layer	O
”	O
option	O
)	O
yielded	O
a	O
boost	O
(	O
relative	O
to	O
max	Method
or	O
avg	O
pooling	Method
)	O
in	O
CIFAR10	Material
test	O
accuracy	O
as	O
the	O
test	O
images	O
underwent	O
three	O
different	O
kinds	O
of	O
transformations	O
.	O

This	O
boost	O
was	O
observed	O
across	O
the	O
entire	O
range	O
of	O
transformation	O
amounts	O
for	O
each	O
of	O
the	O
transformations	O
(	O
with	O
the	O
exception	O
of	O
extreme	O
downscaling	O
)	O
.	O

We	O
already	O
observe	O
improved	O
robustness	Metric
in	O
this	O
initial	O
experiment	O
and	O
intend	O
to	O
investigate	O
more	O
instances	O
of	O
our	O
proposed	O
pooling	Method
operations	Method
as	O
time	O
permits	O
.	O

Second	O
,	O
the	O
performance	O
that	O
we	O
attain	O
in	O
the	O
experiments	O
reported	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
,	O
and	O
Table	O
[	O
reference	O
]	O
is	O
achieved	O
with	O
very	O
modest	O
additional	O
numbers	O
of	O
parameters	O
—	O
e.g.	O
on	O
CIFAR10	Material
,	O
our	O
best	O
performance	O
(	O
obtained	O
with	O
the	O
tree	Method
+	Method
gated	Method
max	Method
-	Method
avg	Method
configuration	Method
)	O
only	O
uses	O
an	O
additional	O
parameters	O
(	O
above	O
the	O
1.8	O
M	O
of	O
our	O
baseline	O
network	O
)	O
and	O
yet	O
reduces	O
test	O
error	Metric
from	O
to	O
;	O
see	O
the	O
CIFAR10	Material
Section	O
for	O
details	O
.	O

In	O
our	O
AlexNet	Task
experiment	O
,	O
replacing	O
the	O
maxpool	Method
layers	Method
with	O
our	O
proposed	O
pooling	Method
operations	Method
gave	O
a	O
relative	O
reduction	O
in	O
test	O
error	Metric
(	O
top	O
-	O
5	O
,	O
single	O
-	O
view	O
)	O
with	O
only	O
45	O
additional	O
parameters	O
(	O
above	O
the	O
50	O
M	O
of	O
standard	O
AlexNet	Method
)	O
;	O
see	O
the	O
ImageNet	Material
2012	O
Section	O
for	O
details	O
.	O

We	O
also	O
investigate	O
the	O
additional	O
time	O
incurred	O
when	O
using	O
our	O
proposed	O
pooling	Method
operations	Method
;	O
in	O
the	O
experiments	O
reported	O
in	O
the	O
Timing	O
section	O
,	O
this	O
overhead	O
ranges	O
from	O
to	O
.	O

Testing	O
invariance	O
properties	O
Before	O
going	O
to	O
the	O
overall	O
classification	Task
results	O
,	O
we	O
investigate	O
the	O
invariance	O
properties	O
of	O
networks	O
utilizing	O
either	O
standard	O
pooling	Method
operations	Method
(	O
max	Method
and	O
average	Method
)	O
or	O
two	O
instances	O
of	O
our	O
proposed	O
pooling	Method
operations	Method
(	O
gated	Method
max	Method
-	Method
avg	Method
and	O
2	O
level	O
tree	O
,	O
each	O
using	O
the	O
“	O
1	O
per	O
pool	O
layer	O
”	O
option	O
)	O
that	O
we	O
find	O
to	O
yield	O
best	O
performance	O
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
for	O
architecture	O
details	O
used	O
across	O
each	O
network	O
)	O
.	O

We	O
begin	O
by	O
training	O
four	O
different	O
networks	O
on	O
the	O
CIFAR10	Material
training	O
set	O
,	O
one	O
for	O
each	O
of	O
the	O
four	O
pooling	Method
operations	O
selected	O
for	O
consideration	O
;	O
training	O
details	O
are	O
found	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

We	O
seek	O
to	O
determine	O
the	O
respective	O
invariance	Metric
properties	Metric
of	O
these	O
networks	O
by	O
evaluating	O
their	O
accuracy	Metric
on	O
various	O
transformed	O
versions	O
of	O
the	O
CIFAR10	Material
test	O
set	O
.	O

Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
test	Metric
accuracy	Metric
attained	O
in	O
the	O
presence	O
of	O
image	O
rotation	O
,	O
(	O
vertical	O
)	O
translation	O
,	O
and	O
scaling	O
of	O
the	O
CIFAR10	Material
test	O
set	O
.	O

Timing	O
In	O
order	O
to	O
evaluate	O
how	O
much	O
additional	O
time	O
is	O
incurred	O
by	O
the	O
use	O
of	O
our	O
proposed	O
learned	O
pooling	Method
operations	O
,	O
we	O
measured	O
the	O
average	Method
forward	O
+	O
backward	O
time	O
per	O
CIFAR10	Material
image	O
.	O

In	O
each	O
case	O
,	O
the	O
one	O
per	Method
layer	Method
option	Method
is	O
used	O
.	O

We	O
find	O
that	O
the	O
additional	O
computation	Metric
time	Metric
incurred	O
ranges	O
from	O
to	O
.	O

More	O
specifically	O
,	O
the	O
baseline	O
network	O
took	O
3.90	O
ms	O
;	O
baseline	O
with	O
mixed	O
max	Method
-	O
avg	O
took	O
4.10	O
ms	O
;	O
baseline	O
with	O
gated	Method
max	Method
-	Method
avg	Method
took	O
4.16	O
ms	O
;	O
baseline	O
with	O
2	O
level	Method
tree	Method
pooling	Method
took	O
4.25	O
ms	O
;	O
finally	O
,	O
baseline	O
with	O
tree	O
+	O
gated	O
max	Method
-	O
avg	O
took	O
4.46	O
ms	O
.	O

section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
max	Method
-	O
average	Method
pooling	Method
and	O
tree	O
pooling	Method
approaches	O
on	O
five	O
standard	O
benchmark	O
datasets	O
:	O
MNIST	Material
,	O
CIFAR10	Material
,	O
CIFAR100	Material
,	O
SVHN	Material
and	O
ImageNet	Material
.	O

To	O
control	O
for	O
the	O
effect	O
of	O
differences	O
in	O
data	O
or	O
data	Task
preparation	Task
,	O
we	O
match	O
our	O
data	O
and	O
data	O
preparation	O
to	O
that	O
used	O
in	O
.	O

Please	O
refer	O
to	O
for	O
the	O
detailed	O
description	O
.	O

We	O
now	O
describe	O
the	O
basic	O
network	Method
architecture	Method
and	O
then	O
will	O
specify	O
the	O
various	O
hyperparameter	O
choices	O
.	O

The	O
basic	O
experiment	O
architecture	O
contains	O
six	O
standard	O
convolutional	Method
layers	Method
(	O
named	O
conv1	Method
to	O
conv6	Method
)	O
and	O
three	O
mlpconv	Method
layers	Method
(	O
named	O
mlpconv1	Method
to	O
mlpconv3	Method
)	O
,	O
placed	O
after	O
conv2	Method
,	O
conv4	Method
,	O
and	O
conv6	Method
,	O
respectively	O
.	O

We	O
chose	O
the	O
number	O
of	O
channels	O
at	O
each	O
layer	O
to	O
be	O
analogous	O
to	O
the	O
choices	O
in	O
;	O
the	O
specific	O
numbers	O
are	O
provided	O
in	O
the	O
sections	O
for	O
each	O
dataset	O
.	O

We	O
follow	O
every	O
one	O
of	O
these	O
conv	Method
-	Method
type	Method
layers	Method
with	O
ReLU	Method
activation	Method
functions	Method
.	O

One	O
final	O
mlpconv	Method
layer	Method
(	O
mlpconv4	Method
)	O
is	O
used	O
to	O
reduce	O
the	O
dimension	O
of	O
the	O
last	O
layer	O
to	O
match	O
the	O
total	O
number	O
of	O
classes	O
for	O
each	O
different	O
dataset	O
,	O
as	O
in	O
.	O

The	O
overall	O
model	O
has	O
parameter	O
count	O
analogous	O
to	O
.	O

The	O
proposed	O
max	Method
-	O
average	Method
pooling	Method
and	O
tree	Method
pooling	Method
layers	Method
with	O
pooling	Method
regions	O
are	O
used	O
after	O
mlpconv1	Method
and	Method
mlpconv2	Method
layers	Method
.	O

We	O
provide	O
a	O
detailed	O
listing	O
of	O
the	O
network	O
configurations	O
in	O
Table	O
[	O
reference	O
]	O
in	O
the	O
Supplementary	O
Materials	O
.	O

Moving	O
on	O
to	O
the	O
hyperparameter	O
settings	O
,	O
dropout	O
with	O
rate	O
is	O
used	O
after	O
each	O
pooling	Method
layer	Method
.	O

We	O
also	O
use	O
hidden	O
layer	O
supervision	O
to	O
ease	O
the	O
training	O
process	O
as	O
in	O
.	O

The	O
learning	Metric
rate	Metric
is	O
decreased	O
whenever	O
the	O
validation	O
error	Metric
stops	O
decreasing	O
;	O
we	O
use	O
the	O
schedule	O
for	O
all	O
experiments	O
.	O

The	O
momentum	O
of	O
and	O
weight	O
decay	O
of	O
are	O
fixed	O
for	O
all	O
datasets	O
as	O
another	O
regularizer	Method
besides	O
dropout	Method
.	O

All	O
the	O
initial	O
pooling	Method
filters	Method
and	O
pooling	Method
masks	O
have	O
values	O
sampled	O
from	O
a	O
Gaussian	Method
distribution	Method
with	O
zero	O
mean	O
and	O
standard	O
deviation	O
.	O

We	O
use	O
these	O
hyperparameter	O
settings	O
for	O
all	O
experiments	O
reported	O
in	O
Tables	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
.	O

No	O
model	Method
averaging	Method
is	O
done	O
at	O
test	O
time	O
.	O

subsection	O
:	O
Classification	Task
results	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
our	O
overall	O
experimental	O
results	O
.	O

Our	O
baseline	O
is	O
a	O
network	O
trained	O
with	O
conventional	O
max	Method
pooling	Method
.	O

Mixed	Method
refers	O
to	O
the	O
same	O
network	O
but	O
with	O
a	O
max	Method
-	O
avg	O
pooling	Method
strategy	O
in	O
both	O
the	O
first	O
and	O
second	O
pooling	Method
layers	O
(	O
both	O
using	O
the	O
mixed	Method
strategy	Method
)	O
;	O
Gated	O
has	O
a	O
corresponding	O
meaning	O
.	O

Tree	Method
(	O
with	O
specific	O
number	O
of	O
levels	O
noted	O
below	O
)	O
refers	O
to	O
the	O
same	O
again	O
,	O
but	O
with	O
our	O
tree	Method
pooling	Method
in	O
the	O
first	O
pooling	Method
layer	Method
only	O
;	O
we	O
do	O
not	O
see	O
further	O
improvement	O
when	O
tree	Method
pooling	Method
is	O
used	O
for	O
both	O
pooling	Method
layers	Method
.	O

This	O
observation	O
motivated	O
us	O
to	O
consider	O
following	O
a	O
tree	Method
pooling	Method
layer	Method
with	O
a	O
gated	O
max	Method
-	O
avg	O
pooling	Method
layer	O
:	O
Tree	Method
+	Method
Max	Method
-	O
Average	Method
refers	O
to	O
a	O
network	Method
configuration	Method
with	O
(	O
2	O
level	O
)	O
tree	Method
pooling	Method
for	O
the	O
first	O
pooling	Method
layer	Method
and	O
gated	O
max	Method
-	O
average	Method
pooling	Method
for	O
the	O
second	O
pooling	Method
layer	Method
.	O

All	O
results	O
are	O
produced	O
from	O
the	O
same	O
network	O
structure	O
and	O
hyperparameter	O
settings	O
—	O
the	O
only	O
difference	O
is	O
in	O
the	O
choice	O
of	O
pooling	Method
function	O
.	O

See	O
Table	O
[	O
reference	O
]	O
for	O
details	O
.	O

MNIST	Material
Our	O
MNIST	Material
model	O
has	O
channels	O
for	O
conv1	Method
to	O
conv6	Method
and	O
channels	O
for	O
mlpconv1	Method
to	O
mlpconv3	Method
,	O
respectively	O
.	O

Our	O
only	O
preprocessing	O
is	O
mean	Method
subtraction	Method
.	O

Tables	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
show	O
previous	O
best	O
results	O
and	O
those	O
for	O
our	O
proposed	O
pooling	Method
methods	Method
.	O

CIFAR10	Material
Our	O
CIFAR10	Material
model	O
has	O
channels	O
for	O
conv1	Method
to	O
conv6	Method
and	O
channels	O
for	O
mlpconv1	Method
to	O
mlpconv3	Method
,	O
respectively	O
.	O

We	O
also	O
performed	O
an	O
experiment	O
in	O
which	O
we	O
learned	O
a	O
single	O
pooling	Method
filter	Method
without	O
the	O
tree	O
structure	O
(	O
i.e.	O
,	O
a	O
singleton	O
leaf	O
node	O
containing	O
parameters	O
;	O
one	O
such	O
singleton	O
leaf	O
node	O
per	O
pooling	Method
layer	O
)	O
and	O
obtained	O
improvement	O
over	O
the	O
baseline	O
model	O
.	O

Our	O
results	O
indicate	O
that	O
performance	O
improves	O
when	O
the	O
pooling	Method
filter	Method
is	O
learned	O
,	O
and	O
further	O
improves	O
when	O
we	O
also	O
learn	O
how	O
to	O
combine	O
learned	O
pooling	Method
filters	Method
.	O

The	O
All	Method
-	Method
CNN	Method
method	Method
in	O
uses	O
convolutional	Method
layers	Method
in	O
place	O
of	O
pooling	Method
layers	Method
in	O
a	O
CNN	Method
-	Method
type	Method
network	Method
architecture	Method
.	O

However	O
,	O
a	O
standard	O
convolutional	Method
layer	Method
requires	O
many	O
more	O
parameters	O
than	O
a	O
gated	Method
max	Method
-	Method
average	Method
pooling	Method
layer	Method
(	O
only	O
parameters	O
for	O
a	O
pooling	Method
region	O
kernel	O
size	O
in	O
the	O
1	O
per	O
pooling	Method
layer	O
option	O
)	O
or	O
a	O
tree	Method
-	Method
pooling	Method
layer	Method
(	O
parameters	O
for	O
a	O
2	O
level	O
tree	O
and	O
pooling	Method
region	O
kernel	O
size	O
,	O
again	O
in	O
the	O
1	O
per	O
pooling	Method
layer	O
option	O
)	O
.	O

The	O
pooling	Method
operations	Method
in	O
our	O
tree	O
+	O
max	Method
-	O
avg	O
network	O
configuration	O
use	O
parameters	O
for	O
the	O
(	O
first	O
,	O
3	O
level	O
)	O
tree	Method
-	Method
pooling	Method
layer	Method
—	O
4	O
leaf	O
nodes	O
and	O
3	O
internal	O
nodes	O
—	O
and	O
parameters	O
in	O
the	O
gating	O
mask	O
used	O
for	O
the	O
(	O
second	O
)	O
gated	Method
max	Method
-	Method
average	Method
pooling	Method
layer	Method
,	O
while	O
the	O
best	O
result	O
in	O
contains	O
a	O
total	O
of	O
nearly	O
parameters	O
in	O
layers	O
performing	O
“	O
pooling	Method
like	O
”	O
operations	O
;	O
the	O
relative	O
CIFAR10	Material
accuracies	O
are	O
(	O
ours	O
)	O
and	O
(	O
All	Method
-	Method
CNN	Method
)	O
.	O

For	O
the	O
data	Task
augmentation	Task
experiment	O
,	O
we	O
followed	O
the	O
standard	O
data	Method
augmentation	Method
procedure	Method
.	O

When	O
training	O
with	O
augmented	O
data	O
,	O
we	O
observe	O
the	O
same	O
trends	O
seen	O
in	O
the	O
“	O
no	O
data	O
augmentation	O
”	O
experiments	O
.	O

We	O
note	O
that	O
reports	O
a	O
error	Metric
rate	Metric
with	O
extensive	O
data	Method
augmentation	Method
(	O
including	O
translations	O
,	O
rotations	O
,	O
reflections	O
,	O
stretching	O
,	O
and	O
shearing	O
operations	O
)	O
in	O
a	O
much	O
wider	O
and	O
deeper	O
million	O
parameter	O
network	O
—	O
times	O
more	O
than	O
are	O
in	O
our	O
networks	O
.	O

Our	O
Tree	Method
+	Method
Max	Method
-	Method
Avg	O
CIFAR100	Material
Our	O
CIFAR100	Material
model	O
has	O
channels	O
for	O
all	O
convolutional	O
layers	O
and	O
channels	O
for	O
mlpconv1	Method
to	O
mlpconv3	Method
,	O
respectively	O
.	O

Street	O
view	O
house	O
numbers	O
Our	O
SVHN	Material
model	O
has	O
channels	O
for	O
conv1	Method
to	O
conv6	Method
and	O
channels	O
for	O
mlpconv1	Method
to	O
mlpconv3	Method
,	O
respectively	O
.	O

In	O
terms	O
of	O
amount	O
of	O
data	O
,	O
SVHN	Material
has	O
a	O
larger	O
training	O
data	O
set	O
(	O
600k	O
versus	O
the	O
50k	O
of	O
most	O
of	O
the	O
other	O
benchmark	O
datasets	O
)	O
.	O

The	O
much	O
larger	O
amount	O
of	O
training	O
data	O
motivated	O
us	O
to	O
explore	O
what	O
performance	O
we	O
might	O
observe	O
if	O
we	O
pursued	O
the	O
one	O
per	O
layer	O
/	O
channel	O
/	O
region	O
option	O
,	O
which	O
even	O
for	O
the	O
simple	O
mixed	O
max	Method
-	O
avg	O
strategy	O
results	O
in	O
a	O
huge	O
increase	O
in	O
total	O
the	O
number	O
of	O
parameters	O
to	O
learn	O
in	O
our	O
proposed	O
pooling	Method
layers	Method
:	O
specifically	O
,	O
from	O
a	O
total	O
of	O
2	O
in	O
the	O
mixed	O
max	Method
-	O
avg	O
strategy	O
,	O
1	O
parameter	O
per	O
pooling	Method
layer	Method
option	O
,	O
we	O
increase	O
to	O
40	O
,	O
960	O
.	O

Using	O
this	O
one	O
per	Method
layer	Method
/	Method
channel	Method
/	Method
region	Method
option	Method
for	O
the	O
mixed	O
max	Method
-	O
avg	O
strategy	O
,	O
we	O
observe	O
test	O
error	Metric
(	O
in	O
)	O
of	O
0.30	O
on	O
MNIST	Material
,	O
8.02	O
on	O
CIFAR10	Material
,	O
6.61	O
on	O
CIFAR10	Material
,	O
33.27	O
on	O
CIFAR100	Material
,	O
and	O
1.64	O
on	O
SVHN	Material
.	O

Interestingly	O
,	O
for	O
MNIST	Material
,	O
CIFAR10	Material
,	O
and	O
CIFAR100	Material
this	O
mixed	O
max	Method
-	O
avg	O
(	O
1	O
per	Metric
layer	Metric
/	Metric
channel	Metric
/	Metric
region	Metric
)	O
performance	O
is	O
between	O
mixed	O
max	Method
-	O
avg	O
(	O
1	O
per	O
layer	O
)	O
and	O
gated	Method
max	Method
-	Method
avg	Method
(	O
1	O
per	O
layer	O
)	O
;	O
on	O
CIFAR10	Material
mixed	O
max	Method
-	O
avg	O
(	O
1	O
per	O
layer	O
/	O
channel	Metric
/	Metric
region	Metric
)	O
is	O
worse	O
than	O
either	O
of	O
the	O
1	O
per	Method
layer	Method
max	Method
-	Method
avg	Method
strategies	Method
.	O

The	O
SVHN	Material
result	O
using	O
mixed	O
max	Method
-	O
avg	O
(	O
1	O
per	O
layer	O
/	O
channel	O
/	O
region	O
)	O
sets	O
a	O
new	O
state	O
of	O
the	O
art	O
.	O

ImageNet	Material
2012	O
In	O
this	O
experiment	O
we	O
do	O
not	O
directly	O
compete	O
with	O
the	O
best	O
performing	O
result	O
in	O
the	O
challenge	O
(	O
since	O
the	O
winning	O
methods	O
involve	O
many	O
additional	O
aspects	O
beyond	O
pooling	Method
operations	O
)	O
,	O
but	O
rather	O
to	O
provide	O
an	O
illustrative	O
comparison	O
of	O
the	O
relative	O
benefit	O
of	O
the	O
proposed	O
pooling	Method
methods	Method
versus	O
conventional	O
max	Method
pooling	Method
on	O
this	O
dataset	O
.	O

We	O
use	O
the	O
same	O
network	O
structure	O
and	O
parameter	O
setup	O
as	O
in	O
(	O
no	O
hidden	O
layer	O
supervision	O
)	O
but	O
simply	O
replace	O
the	O
first	O
max	Method
pooling	Method
with	O
the	O
(	O
proposed	O
2	O
level	O
)	O
tree	Method
pooling	Method
(	O
2	O
leaf	O
nodes	O
and	O
1	O
internal	O
node	O
for	O
parameters	O
)	O
and	O
replace	O
the	O
second	O
and	O
third	O
max	Method
pooling	Method
with	O
gated	O
max	Method
-	O
average	Method
pooling	Method
(	O
2	O
gating	O
masks	O
for	O
parameters	O
)	O
.	O

Relative	O
to	O
the	O
original	O
AlexNet	Method
,	O
this	O
adds	O
more	O
parameters	O
(	O
over	O
the	O
50	O
M	O
in	O
the	O
original	O
)	O
and	O
achieves	O
relative	O
error	Metric
reduction	O
of	O
(	O
for	O
top	O
-	O
5	O
,	O
single	O
-	O
view	O
)	O
and	O
(	O
for	O
top	O
-	O
5	O
,	O
multi	O
-	O
view	O
)	O
.	O

Our	O
GoogLeNet	Method
configuration	Method
uses	O
4	O
gated	O
max	Method
-	O
avg	O
pooling	Method
layers	O
,	O
for	O
a	O
total	O
of	O
36	O
extra	O
parameters	O
over	O
the	O
6.8	O
million	O
in	O
standard	O
GoogLeNet	Material
.	O

Table	O
[	O
reference	O
]	O
shows	O
a	O
direct	O
comparison	O
(	O
in	O
each	O
case	O
we	O
use	O
single	O
net	O
predictions	O
rather	O
than	O
ensemble	Method
)	O
.	O

top	O
-	O
1	O
s	O
-	O
view	O
top	O
-	O
5	O
s	O
-	O
view	O
top	O
-	O
1	O
m	O
-	O
view	O
top	O
-	O
5	O
m	Method
-	Method
view	Method
section	O
:	O
Observations	O
from	O
Experiments	O
In	O
each	O
experiment	O
,	O
using	O
any	O
of	O
our	O
proposed	O
pooling	Method
operations	Method
boosted	O
performance	O
.	O

A	O
fixed	Method
network	Method
configuration	Method
using	O
the	O
proposed	O
tree	O
+	O
max	Method
-	O
avg	O
pooling	Method
(	O
1	O
per	O
pool	O
layer	O
option	O
)	O
yields	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
MNIST	Material
,	O
CIFAR10	Material
(	O
with	O
and	O
without	O
data	Task
augmentation	Task
)	O
,	O
and	O
SVHN	Material
.	O

We	O
observed	O
boosts	O
in	O
tandem	O
with	O
data	Task
augmentation	Task
,	O
multi	Task
-	Task
view	Task
predictions	Task
,	O
batch	Task
normalization	Task
,	O
and	O
several	O
different	O
architectures	O
—	O
NiN	Method
-	Method
style	Method
,	O
DSN	Method
-	Method
style	Method
,	O
the	O
50	O
M	Method
parameter	Method
AlexNet	Method
,	O
and	O
the	O
22	O
-	O
layer	O
GoogLeNet	Method
.	O

Acknowledgment	O
This	O
work	O
is	O
supported	O
by	O
NSF	O
awards	O
IIS	O
-	O
1216528	O
(	O
IIS	O
-	O
1360566	O
)	O
and	O
IIS	O
-	O
0844566	O
(	O
IIS	O
-	O
1360568	O
)	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Supplementary	O
Materials	O
Visualization	Task
of	Task
network	Task
internal	Task
representations	Task
To	O
gain	O
additional	O
qualitative	O
understanding	O
of	O
the	O
pooling	Method
methods	Method
we	O
are	O
considering	O
,	O
we	O
use	O
the	O
popular	O
t	Method
-	Method
SNE	Method
algorithm	Method
to	O
visualize	O
embeddings	O
of	O
some	O
internal	O
feature	O
responses	O
from	O
pooling	Method
operations	Method
.	O

Specifically	O
,	O
we	O
again	O
use	O
four	O
networks	O
(	O
one	O
utilizing	O
each	O
of	O
the	O
selected	O
types	O
of	O
pooling	Method
)	O
trained	O
on	O
the	O
CIFAR10	Material
training	O
set	O
(	O
see	O
Sec	O
.	O

[	O
reference	O
]	O
for	O
architecture	O
details	O
used	O
across	O
each	O
network	O
)	O
.	O

We	O
extract	O
feature	O
responses	O
for	O
a	O
randomly	O
chosen	O
-	O
image	O
subset	O
of	O
the	O
CIFAR10	Material
test	O
set	O
at	O
the	O
first	O
(	O
i.e.	O
,	O
earliest	O
)	O
and	O
second	O
pooling	Method
layers	O
of	O
each	O
network	O
.	O

These	O
feature	O
response	O
vectors	O
are	O
then	O
embedded	O
into	O
2	Method
-	Method
d	Method
using	O
t	Method
-	Method
SNE	Method
;	O
see	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
first	O
row	O
shows	O
the	O
embeddings	O
of	O
the	O
internal	O
activations	O
immediately	O
after	O
the	O
first	O
pooling	Method
operation	Method
;	O
the	O
second	O
row	O
shows	O
embeddings	O
of	O
activations	O
immediately	O
after	O
the	O
second	O
pooling	Method
operation	Method
.	O

From	O
left	O
to	O
right	O
we	O
plot	O
the	O
t	O
-	O
SNE	O
embeddings	O
of	O
the	O
pooling	Method
activations	O
within	O
networks	O
that	O
are	O
trained	O
with	O
average	Method
,	O
max	Method
,	O
gated	Method
max	Method
-	Method
avg	Method
,	O
and	O
(	O
2	O
level	O
)	O
tree	Method
pooling	Method
.	O

We	O
can	O
see	O
that	O
certain	O
classes	O
such	O
as	O
“	O
0	O
”	O
(	O
airplane	O
)	O
,	O
“	O
2	O
”	O
(	O
bird	O
)	O
,	O
and	O
“	O
9	O
”	O
(	O
truck	O
)	O
are	O
more	O
separated	O
with	O
the	O
proposed	O
methods	O
than	O
they	O
are	O
with	O
the	O
conventional	O
average	Method
and	O
max	Method
pooling	Method
functions	O
.	O

We	O
can	O
also	O
see	O
that	O
the	O
embeddings	O
of	O
the	O
second	O
-	O
pooling	Method
-	O
layer	O
activations	O
are	O
generally	O
more	O
separable	O
than	O
the	O
embeddings	O
of	O
first	O
-	O
pooling	Method
-	O
layer	O
activations	O
.	O

