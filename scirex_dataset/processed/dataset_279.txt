document	O
:	O
Frustum	Method
PointNets	Method
for	O
3D	Task
Object	Task
Detection	Task
from	O
RGB	Material
-	Material
D	Material
Data	Material
In	O
this	O
work	O
,	O
we	O
study	O
3D	O
object	O
detection	Task
from	O
RGB	Material
-	Material
D	Material
data	Material
in	O
both	O
indoor	O
and	O
outdoor	O
scenes	O
.	O

While	O
previous	O
methods	O
focus	O
on	O
images	O
or	O
3D	O
voxels	O
,	O
often	O
obscuring	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
3D	O
data	O
,	O
we	O
directly	O
operate	O
on	O
raw	O
point	O
clouds	O
by	O
popping	O
up	O
RGB	Material
-	Material
D	Material
scans	Material
.	O

However	O
,	O
a	O
key	O
challenge	O
of	O
this	O
approach	O
is	O
how	O
to	O
efficiently	O
localize	Task
objects	Task
in	O
point	Task
clouds	Task
of	Task
large	Task
-	Task
scale	Task
scenes	Task
(	O
region	Task
proposal	Task
)	O
.	O

Instead	O
of	O
solely	O
relying	O
on	O
3D	O
proposals	O
,	O
our	O
method	O
leverages	O
both	O
mature	O
2D	Method
object	Method
detectors	Method
and	O
advanced	O
3D	Method
deep	Method
learning	Method
for	O
object	Task
localization	Task
,	O
achieving	O
efficiency	O
as	O
well	O
as	O
high	O
recall	Metric
for	O
even	O
small	O
objects	O
.	O

Benefited	O
from	O
learning	O
directly	O
in	O
raw	Task
point	Task
clouds	Task
,	O
our	O
method	O
is	O
also	O
able	O
to	O
precisely	O
estimate	O
3D	Task
bounding	Task
boxes	Task
even	O
under	O
strong	O
occlusion	O
or	O
with	O
very	O
sparse	O
points	O
.	O

Evaluated	O
on	O
KITTI	Material
and	O
SUN	O
RGB	O
-	O
D	O
3D	Task
detection	Task
benchmarks	O
,	O
our	O
method	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
remarkable	O
margins	O
while	O
having	O
real	Metric
-	Metric
time	Metric
capability	Metric
.	O

section	O
:	O
Introduction	O
Recently	O
,	O
great	O
progress	O
has	O
been	O
made	O
on	O
2D	Task
image	Task
understanding	Task
tasks	Task
,	O
such	O
as	O
object	O
detection	Task
and	O
instance	Task
segmentation	Task
.	O

However	O
,	O
beyond	O
getting	O
2D	O
bounding	O
boxes	O
or	O
pixel	O
masks	O
,	O
3D	Task
understanding	Task
is	O
eagerly	O
in	O
demand	O
in	O
many	O
applications	O
such	O
as	O
autonomous	Task
driving	Task
and	O
augmented	Task
reality	Task
(	O
AR	Task
)	O
.	O

With	O
the	O
popularity	O
of	O
3D	Method
sensors	Method
deployed	O
on	O
mobile	O
devices	O
and	O
autonomous	O
vehicles	O
,	O
more	O
and	O
more	O
3D	O
data	O
is	O
captured	O
and	O
processed	O
.	O

In	O
this	O
work	O
,	O
we	O
study	O
one	O
of	O
the	O
most	O
important	O
3D	Task
perception	Task
tasks	Task
–	O
3D	O
object	O
detection	Task
,	O
which	O
classifies	O
the	O
object	Task
category	Task
and	O
estimates	O
oriented	Task
3D	Task
bounding	Task
boxes	Task
of	Task
physical	Task
objects	Task
from	O
3D	O
sensor	O
data	O
.	O

While	O
3D	O
sensor	O
data	O
is	O
often	O
in	O
the	O
form	O
of	O
point	O
clouds	O
,	O
how	O
to	O
represent	O
point	O
cloud	O
and	O
what	O
deep	Method
net	Method
architectures	Method
to	O
use	O
for	O
3D	O
object	O
detection	Task
remains	O
an	O
open	O
problem	O
.	O

Most	O
existing	O
works	O
convert	O
3D	O
point	O
clouds	O
to	O
images	O
by	O
projection	O
or	O
to	O
volumetric	O
grids	O
by	O
quantization	O
and	O
then	O
apply	O
convolutional	Method
networks	Method
.	O

This	O
data	Method
representation	Method
transformation	Method
,	O
however	O
,	O
may	O
obscure	O
natural	O
3D	O
patterns	O
and	O
invariances	O
of	O
the	O
data	O
.	O

Recently	O
,	O
a	O
number	O
of	O
papers	O
have	O
proposed	O
to	O
process	O
point	Task
clouds	Task
directly	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

For	O
example	O
,	O
proposed	O
new	O
types	O
of	O
deep	Method
net	Method
architectures	Method
,	O
called	O
PointNets	Method
,	O
which	O
have	O
shown	O
superior	O
performance	O
and	O
efficiency	O
in	O
several	O
3D	Task
understanding	Task
tasks	Task
such	O
as	O
object	Task
classification	Task
and	O
semantic	Task
segmentation	Task
.	O

While	O
PointNets	Method
are	O
capable	O
of	O
classifying	O
a	O
whole	Task
point	Task
cloud	Task
or	O
predicting	O
a	O
semantic	O
class	O
for	O
each	O
point	O
in	O
a	O
point	O
cloud	O
,	O
it	O
is	O
unclear	O
how	O
this	O
architecture	O
can	O
be	O
used	O
for	O
instance	O
-	O
level	O
3D	O
object	O
detection	Task
.	O

Towards	O
this	O
goal	O
,	O
we	O
have	O
to	O
address	O
one	O
key	O
challenge	O
:	O
how	O
to	O
efficiently	O
propose	O
possible	O
locations	O
of	O
3D	O
objects	O
in	O
a	O
3D	O
space	O
.	O

Imitating	O
the	O
practice	O
in	O
image	Task
detection	Task
,	O
it	O
is	O
straightforward	O
to	O
enumerate	O
candidate	O
3D	O
boxes	O
by	O
sliding	O
windows	O
or	O
by	O
3D	Method
region	Method
proposal	Method
networks	Method
such	O
as	O
.	O

However	O
,	O
the	O
computational	Metric
complexity	Metric
of	O
3D	Task
search	Task
typically	O
grows	O
cubically	O
with	O
respect	O
to	O
resolution	O
and	O
becomes	O
too	O
expensive	O
for	O
large	Task
scenes	Task
or	O
real	Task
-	Task
time	Task
applications	Task
such	O
as	O
autonomous	Task
driving	Task
.	O

Instead	O
,	O
in	O
this	O
work	O
,	O
we	O
reduce	O
the	O
search	O
space	O
following	O
the	O
dimension	Method
reduction	Method
principle	Method
:	O
we	O
take	O
the	O
advantage	O
of	O
mature	O
2D	Method
object	Method
detectors	Method
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

First	O
,	O
we	O
extract	O
the	O
3D	O
bounding	O
frustum	O
of	O
an	O
object	O
by	O
extruding	O
2D	O
bounding	O
boxes	O
from	O
image	Method
detectors	Method
.	O

Then	O
,	O
within	O
the	O
3D	O
space	O
trimmed	O
by	O
each	O
of	O
the	O
3D	O
frustums	O
,	O
we	O
consecutively	O
perform	O
3D	Method
object	Method
instance	Method
segmentation	Method
and	O
amodal	O
3D	Method
bounding	Method
box	Method
regression	Method
using	O
two	O
variants	O
of	O
PointNet	Method
.	O

The	O
segmentation	Method
network	Method
predicts	O
the	O
3D	O
mask	O
of	O
the	O
object	O
of	O
interest	O
(	O
i.e.	O
instance	Task
segmentation	Task
)	O
;	O
and	O
the	O
regression	Method
network	Method
estimates	O
the	O
amodal	O
3D	O
bounding	O
box	O
(	O
covering	O
the	O
entire	O
object	O
even	O
if	O
only	O
part	O
of	O
it	O
is	O
visible	O
)	O
.	O

In	O
contrast	O
to	O
previous	O
work	O
that	O
treats	O
RGB	Material
-	Material
D	Material
data	Material
as	O
2D	O
maps	O
for	O
CNNs	Method
,	O
our	O
method	O
is	O
more	O
3D	O
-	O
centric	O
as	O
we	O
lift	O
depth	O
maps	O
to	O
3D	O
point	O
clouds	O
and	O
process	O
them	O
using	O
3D	Method
tools	Method
.	O

This	O
3D	Method
-	Method
centric	Method
view	Method
enables	O
new	O
capabilities	O
for	O
exploring	O
3D	Task
data	Task
in	O
a	O
more	O
effective	O
manner	O
.	O

First	O
,	O
in	O
our	O
pipeline	O
,	O
a	O
few	O
transformations	O
are	O
applied	O
successively	O
on	O
3D	O
coordinates	O
,	O
which	O
align	O
point	O
clouds	O
into	O
a	O
sequence	O
of	O
more	O
constrained	O
and	O
canonical	O
frames	O
.	O

These	O
alignments	O
factor	O
out	O
pose	O
variations	O
in	O
data	O
,	O
and	O
thus	O
make	O
3D	O
geometry	O
pattern	O
more	O
evident	O
,	O
leading	O
to	O
an	O
easier	O
job	O
of	O
3D	Task
learners	Task
.	O

Second	O
,	O
learning	Task
in	Task
3D	Task
space	Task
can	O
better	O
exploits	O
the	O
geometric	O
and	O
topological	O
structure	O
of	O
3D	O
space	O
.	O

In	O
principle	O
,	O
all	O
objects	O
live	O
in	O
3D	O
space	O
;	O
therefore	O
,	O
we	O
believe	O
that	O
many	O
geometric	O
structures	O
,	O
such	O
as	O
repetition	O
,	O
planarity	O
,	O
and	O
symmetry	O
,	O
are	O
more	O
naturally	O
parameterized	O
and	O
captured	O
by	O
learners	Method
that	O
directly	O
operate	O
in	O
3D	O
space	O
.	O

The	O
usefulness	O
of	O
this	O
3D	Method
-	Method
centric	Method
network	Method
design	Method
philosophy	Method
has	O
been	O
supported	O
by	O
much	O
recent	O
experimental	O
evidence	O
.	O

Our	O
method	O
achieve	O
leading	O
positions	O
on	O
KITTI	Material
3D	O
object	O
detection	Task
and	O
bird	O
’s	O
eye	O
view	O
detection	Task
benchmarks	O
.	O

Compared	O
with	O
the	O
previous	O
state	O
of	O
the	O
art	O
,	O
our	O
method	O
is	O
8.04	O
%	O
better	O
on	O
3D	Metric
car	Metric
AP	Metric
with	O
high	O
efficiency	O
(	O
running	O
at	O
5	O
fps	Metric
)	O
.	O

Our	O
method	O
also	O
fits	O
well	O
to	O
indoor	Material
RGB	Material
-	Material
D	Material
data	Material
where	O
we	O
have	O
achieved	O
8.9	O
%	O
and	O
6.4	O
%	O
better	O
3D	Metric
mAP	Metric
than	O
and	O
on	O
SUN	Material
-	Material
RGBD	Material
while	O
running	O
one	O
to	O
three	O
orders	O
of	O
magnitude	O
faster	O
.	O

The	O
key	O
contributions	O
of	O
our	O
work	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
framework	O
for	O
RGB	O
-	O
D	O
data	O
based	O
3D	O
object	O
detection	Task
called	O
Frustum	Method
PointNets	Method
.	O

We	O
show	O
how	O
we	O
can	O
train	O
3D	Method
object	Method
detectors	Method
under	O
our	O
framework	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
standard	O
3D	O
object	O
detection	Task
benchmarks	O
.	O

We	O
provide	O
extensive	O
quantitative	O
evaluations	O
to	O
validate	O
our	O
design	O
choices	O
as	O
well	O
as	O
rich	O
qualitative	O
results	O
for	O
understanding	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
method	O
.	O

section	O
:	O
Related	O
Work	O
paragraph	O
:	O
3D	Task
Object	Task
Detection	Task
from	O
RGB	Material
-	Material
D	Material
Data	Material
Researchers	O
have	O
approached	O
the	O
3D	Task
detection	Task
problem	O
by	O
taking	O
various	O
ways	O
to	O
represent	O
RGB	Material
-	Material
D	Material
data	Material
.	O

Front	Method
view	Method
image	Method
based	Method
methods	Method
:	O
take	O
monocular	Material
RGB	Material
images	Material
and	O
shape	O
priors	O
or	O
occlusion	O
patterns	O
to	O
infer	O
3D	O
bounding	O
boxes	O
.	O

represent	O
depth	O
data	O
as	O
2D	O
maps	O
and	O
apply	O
CNNs	Method
to	O
localize	Task
objects	Task
in	O
2D	O
image	O
.	O

In	O
comparison	O
we	O
represent	O
depth	O
as	O
a	O
point	O
cloud	O
and	O
use	O
advanced	O
3D	Method
deep	Method
networks	Method
(	O
PointNets	Method
)	O
that	O
can	O
exploit	O
3D	O
geometry	O
more	O
effectively	O
.	O

Bird	Method
’s	Method
eye	Method
view	Method
based	Method
methods	Method
:	O
MV3D	Method
projects	O
LiDAR	Method
point	Method
cloud	Method
to	O
bird	O
’s	O
eye	O
view	O
and	O
trains	O
a	O
region	Method
proposal	Method
network	Method
(	O
RPN	Method
)	O
for	O
3D	Task
bounding	Task
box	Task
proposal	Task
.	O

However	O
,	O
the	O
method	O
lags	O
behind	O
in	O
detecting	Task
small	Task
objects	Task
,	O
such	O
as	O
pedestrians	O
and	O
cyclists	O
and	O
can	O
not	O
easily	O
adapt	O
to	O
scenes	O
with	O
multiple	O
objects	O
in	O
vertical	O
direction	O
.	O

3D	Method
based	Method
methods	Method
:	O
train	O
3D	Method
object	Method
classifiers	Method
by	O
SVMs	Method
on	O
hand	O
-	O
designed	O
geometry	O
features	O
extracted	O
from	O
point	O
cloud	O
and	O
then	O
localize	O
objects	O
using	O
sliding	Method
-	Method
window	Method
search	Method
.	O

extends	O
by	O
replacing	O
SVM	Method
with	O
3D	Method
CNN	Method
on	O
voxelized	O
3D	O
grids	O
.	O

designs	O
new	O
geometric	O
features	O
for	O
3D	O
object	O
detection	Task
in	O
a	O
point	O
cloud	O
.	O

convert	O
a	O
point	O
cloud	O
of	O
the	O
entire	O
scene	O
into	O
a	O
volumetric	O
grid	O
and	O
use	O
3D	Method
volumetric	Method
CNN	Method
for	O
object	Task
proposal	Task
and	Task
classification	Task
.	O

Computation	Metric
cost	Metric
for	O
those	O
method	O
is	O
usually	O
quite	O
high	O
due	O
to	O
the	O
expensive	O
cost	O
of	O
3D	O
convolutions	O
and	O
large	O
3D	O
search	O
space	O
.	O

Recently	O
,	O
proposes	O
a	O
2D	O
-	O
driven	O
3D	O
object	O
detection	Task
method	O
that	O
is	O
similar	O
to	O
ours	O
in	O
spirit	O
.	O

However	O
,	O
they	O
use	O
hand	O
-	O
crafted	O
features	O
(	O
based	O
on	O
histogram	Method
of	Method
point	Method
coordinates	Method
)	O
with	O
simple	O
fully	Method
connected	Method
networks	Method
to	O
regress	O
3D	Task
box	Task
location	Task
and	O
pose	O
,	O
which	O
is	O
sub	O
-	O
optimal	O
in	O
both	O
speed	Metric
and	O
performance	O
.	O

In	O
contrast	O
,	O
we	O
propose	O
a	O
more	O
flexible	O
and	O
effective	O
solution	O
with	O
deep	Method
3D	Method
feature	Method
learning	Method
(	O
PointNets	Method
)	O
.	O

paragraph	O
:	O
Deep	Task
Learning	Task
on	O
Point	Task
Clouds	Task
Most	O
existing	O
works	O
convert	O
point	O
clouds	O
to	O
images	O
or	O
volumetric	O
forms	O
before	O
feature	Method
learning	Method
.	O

voxelize	Task
point	Task
clouds	Task
into	O
volumetric	O
grids	O
and	O
generalize	O
image	Method
CNNs	Method
to	O
3D	Method
CNNs	Method
.	O

design	O
more	O
efficient	O
3D	Method
CNN	Method
or	Method
neural	Method
network	Method
architectures	Method
that	O
exploit	O
sparsity	O
in	O
point	O
cloud	O
.	O

However	O
,	O
these	O
CNN	Method
based	Method
methods	Method
still	O
require	O
quantitization	Task
of	Task
point	Task
clouds	Task
with	O
certain	O
voxel	O
resolution	O
.	O

Recently	O
,	O
a	O
few	O
works	O
propose	O
a	O
novel	O
type	O
of	O
network	Method
architectures	Method
(	O
PointNets	Method
)	O
that	O
directly	O
consumes	O
raw	O
point	O
clouds	O
without	O
converting	O
them	O
to	O
other	O
formats	O
.	O

While	O
PointNets	Method
have	O
been	O
applied	O
to	O
single	Task
object	Task
classification	Task
and	O
semantic	Task
segmentation	Task
,	O
our	O
work	O
explores	O
how	O
to	O
extend	O
the	O
architecture	O
for	O
the	O
purpose	O
of	O
3D	O
object	O
detection	Task
.	O

section	O
:	O
Problem	O
Definition	O
Given	O
RGB	Material
-	Material
D	Material
data	Material
as	O
input	O
,	O
our	O
goal	O
is	O
to	O
classify	O
and	O
localize	Task
objects	Task
in	Task
3D	Task
space	Task
.	O

The	O
depth	O
data	O
,	O
obtained	O
from	O
LiDAR	O
or	O
indoor	O
depth	O
sensors	O
,	O
is	O
represented	O
as	O
a	O
point	O
cloud	O
in	O
RGB	O
camera	O
coordinates	O
.	O

The	O
projection	O
matrix	O
is	O
also	O
known	O
so	O
that	O
we	O
can	O
get	O
a	O
3D	O
frustum	O
from	O
a	O
2D	O
image	O
region	O
.	O

Each	O
object	O
is	O
represented	O
by	O
a	O
class	O
(	O
one	O
among	O
predefined	O
classes	O
)	O
and	O
an	O
amodal	O
3D	O
bounding	O
box	O
.	O

The	O
amodal	O
box	O
bounds	O
the	O
complete	O
object	O
even	O
if	O
part	O
of	O
the	O
object	O
is	O
occluded	O
or	O
truncated	O
.	O

The	O
3D	O
box	O
is	O
parameterized	O
by	O
its	O
size	O
,	O
center	O
,	O
and	O
orientation	O
relative	O
to	O
a	O
predefined	O
canonical	O
pose	O
for	O
each	O
category	O
.	O

In	O
our	O
implementation	O
,	O
we	O
only	O
consider	O
the	O
heading	O
angle	O
around	O
the	O
up	O
-	O
axis	O
for	O
orientation	O
.	O

section	O
:	O
3D	Task
Detection	Task
with	O
Frustum	Method
PointNets	Method
As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
system	O
for	O
3D	O
object	O
detection	Task
consists	O
of	O
three	O
modules	O
:	O
frustum	Method
proposal	Method
,	O
3D	Task
instance	Task
segmentation	Task
,	O
and	O
3D	Method
amodal	Method
bounding	Method
box	Method
estimation	Method
.	O

We	O
will	O
introduce	O
each	O
module	O
in	O
the	O
following	O
subsections	O
.	O

We	O
will	O
focus	O
on	O
the	O
pipeline	O
and	O
functionality	O
of	O
each	O
module	O
,	O
and	O
refer	O
readers	O
to	O
supplementary	O
for	O
specific	O
architectures	O
of	O
the	O
deep	Method
networks	Method
involved	O
.	O

subsection	O
:	O
Frustum	Method
Proposal	Method
The	O
resolution	O
of	O
data	O
produced	O
by	O
most	O
3D	Method
sensors	Method
,	O
especially	O
real	Method
-	Method
time	Method
depth	Method
sensors	Method
,	O
is	O
still	O
lower	O
than	O
RGB	Material
images	Material
from	O
commodity	O
cameras	O
.	O

Therefore	O
,	O
we	O
leverage	O
mature	O
2D	Method
object	Method
detector	Method
to	O
propose	O
2D	O
object	O
regions	O
in	O
RGB	Material
images	Material
as	O
well	O
as	O
to	O
classify	O
objects	O
.	O

With	O
a	O
known	O
camera	O
projection	O
matrix	O
,	O
a	O
2D	O
bounding	O
box	O
can	O
be	O
lifted	O
to	O
a	O
frustum	O
(	O
with	O
near	O
and	O
far	O
planes	O
specified	O
by	O
depth	O
sensor	O
range	O
)	O
that	O
defines	O
a	O
3D	O
search	O
space	O
for	O
the	O
object	O
.	O

We	O
then	O
collect	O
all	O
points	O
within	O
the	O
frustum	O
to	O
form	O
a	O
frustum	O
point	O
cloud	O
.	O

As	O
shown	O
in	O
Fig	O
[	O
reference	O
]	O
(	O
a	O
)	O
,	O
frustums	O
may	O
orient	O
towards	O
many	O
different	O
directions	O
,	O
which	O
result	O
in	O
large	O
variation	O
in	O
the	O
placement	Task
of	Task
point	Task
clouds	Task
.	O

We	O
therefore	O
normalize	O
the	O
frustums	O
by	O
rotating	O
them	O
toward	O
a	O
center	O
view	O
such	O
that	O
the	O
center	O
axis	O
of	O
the	O
frustum	O
is	O
orthogonal	O
to	O
the	O
image	O
plane	O
.	O

This	O
normalization	O
helps	O
improve	O
the	O
rotation	O
-	O
invariance	O
of	O
the	O
algorithm	O
.	O

We	O
call	O
this	O
entire	O
procedure	O
for	O
extracting	Task
frustum	Task
point	Task
clouds	Task
from	O
RGB	Task
-	Task
D	Task
data	Task
frustum	Task
proposal	Task
generation	Task
.	O

While	O
our	O
3D	Task
detection	Task
framework	O
is	O
agnostic	O
to	O
the	O
exact	O
method	O
for	O
2D	Task
region	Task
proposal	Task
,	O
we	O
adopt	O
a	O
FPN	Method
based	Method
model	Method
.	O

We	O
pre	O
-	O
train	O
the	O
model	O
weights	O
on	O
ImageNet	Task
classification	Task
and	O
COCO	O
object	O
detection	Task
datasets	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
a	O
KITTI	Material
2D	O
object	O
detection	Task
dataset	O
to	O
classify	O
and	O
predict	O
amodal	Task
2D	Task
boxes	Task
.	O

More	O
details	O
of	O
the	O
2D	Method
detector	Method
training	Method
are	O
provided	O
in	O
the	O
supplementary	O
.	O

subsection	O
:	O
3D	Task
Instance	Task
Segmentation	Task
Given	O
a	O
2D	O
image	O
region	O
(	O
and	O
its	O
corresponding	O
3D	O
frustum	O
)	O
,	O
several	O
methods	O
might	O
be	O
used	O
to	O
obtain	O
3D	O
location	O
of	O
the	O
object	O
:	O
One	O
straightforward	O
solution	O
is	O
to	O
directly	O
regress	O
3D	O
object	O
locations	O
(	O
e.g.	O
,	O
by	O
3D	O
bounding	O
box	O
)	O
from	O
a	O
depth	Method
map	Method
using	O
2D	Method
CNNs	Method
.	O

However	O
,	O
this	O
problem	O
is	O
not	O
easy	O
as	O
occluding	O
objects	O
and	O
background	O
clutter	O
is	O
common	O
in	O
natural	O
scenes	O
(	O
as	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
,	O
which	O
may	O
severely	O
distract	O
the	O
3D	Task
localization	Task
task	Task
.	O

Because	O
objects	O
are	O
naturally	O
separated	O
in	O
physical	O
space	O
,	O
segmentation	Task
in	O
3D	Task
point	Task
cloud	Task
is	O
much	O
more	O
natural	O
and	O
easier	O
than	O
that	O
in	O
images	O
where	O
pixels	O
from	O
distant	O
objects	O
can	O
be	O
near	O
-	O
by	O
to	O
each	O
other	O
.	O

Having	O
observed	O
this	O
fact	O
,	O
we	O
propose	O
to	O
segment	O
instances	O
in	O
3D	Task
point	Task
cloud	Task
instead	O
of	O
in	O
2D	Task
image	Task
or	Task
depth	Task
map	Task
.	O

Similar	O
to	O
Mask	Method
-	Method
RCNN	Method
,	O
which	O
achieves	O
instance	Task
segmentation	Task
by	O
binary	Task
classification	Task
of	Task
pixels	Task
in	Task
image	Task
regions	Task
,	O
we	O
realize	O
3D	Method
instance	Method
segmentation	Method
using	O
a	O
PointNet	Method
-	Method
based	Method
network	Method
on	O
point	O
clouds	O
in	O
frustums	O
.	O

Based	O
on	O
3D	Task
instance	Task
segmentation	Task
,	O
we	O
are	O
able	O
to	O
achieve	O
residual	Task
based	Task
3D	Task
localization	Task
.	O

That	O
is	O
,	O
rather	O
than	O
regressing	O
the	O
absolute	O
3D	O
location	O
of	O
the	O
object	O
whose	O
offset	O
from	O
the	O
sensor	O
may	O
vary	O
in	O
large	O
ranges	O
(	O
e.g.	O
from	O
5	O
m	O
to	O
beyond	O
50	O
m	O
in	O
KITTI	Material
data	O
)	O
,	O
we	O
predict	O
the	O
3D	O
bounding	O
box	O
center	O
in	O
a	O
local	Method
coordinate	Method
system	Method
–	O
3D	O
mask	O
coordinates	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

paragraph	O
:	O
3D	Task
Instance	Task
Segmentation	Task
PointNet	Task
.	O

The	O
network	O
takes	O
a	O
point	O
cloud	O
in	O
frustum	Method
and	O
predicts	O
a	O
probability	O
score	O
for	O
each	O
point	O
that	O
indicates	O
how	O
likely	O
the	O
point	O
belongs	O
to	O
the	O
object	O
of	O
interest	O
.	O

Note	O
that	O
each	O
frustum	O
contains	O
exactly	O
one	O
object	O
of	O
interest	O
.	O

Here	O
those	O
“	O
other	O
”	O
points	O
could	O
be	O
points	O
of	O
non	O
-	O
relevant	O
areas	O
(	O
such	O
as	O
ground	O
,	O
vegetation	O
)	O
or	O
other	O
instances	O
that	O
occlude	O
or	O
are	O
behind	O
the	O
object	O
of	O
interest	O
.	O

Similar	O
to	O
the	O
case	O
in	O
2D	Task
instance	Task
segmentation	Task
,	O
depending	O
on	O
the	O
position	O
of	O
the	O
frustum	O
,	O
object	O
points	O
in	O
one	O
frustum	O
may	O
become	O
cluttered	O
or	O
occlude	O
points	O
in	O
another	O
.	O

Therefore	O
,	O
our	O
segmentation	Task
PointNet	Task
is	O
learning	O
the	O
occlusion	O
and	O
clutter	O
patterns	O
as	O
well	O
as	O
recognizing	O
the	O
geometry	O
for	O
the	O
object	O
of	O
a	O
certain	O
category	O
.	O

In	O
a	O
multi	O
-	O
class	O
detection	Task
case	O
,	O
we	O
also	O
leverage	O
the	O
semantics	O
from	O
a	O
2D	Method
detector	Method
for	O
better	O
instance	Task
segmentation	Task
.	O

For	O
example	O
,	O
if	O
we	O
know	O
the	O
object	O
of	O
interest	O
is	O
a	O
pedestrian	O
,	O
then	O
the	O
segmentation	Method
network	Method
can	O
use	O
this	O
prior	O
to	O
find	O
geometries	O
that	O
look	O
like	O
a	O
person	O
.	O

Specifically	O
,	O
in	O
our	O
architecture	O
we	O
encode	O
the	O
semantic	O
category	O
as	O
a	O
one	O
-	O
hot	O
class	O
vector	O
(	O
dimensional	O
for	O
the	O
pre	O
-	O
defined	O
categories	O
)	O
and	O
concatenate	O
the	O
one	O
-	O
hot	O
vector	O
to	O
the	O
intermediate	O
point	O
cloud	O
features	O
.	O

More	O
details	O
of	O
the	O
specific	O
architectures	O
are	O
described	O
in	O
the	O
supplementary	O
.	O

After	O
3D	Task
instance	Task
segmentation	Task
,	O
points	O
that	O
are	O
classified	O
as	O
the	O
object	O
of	O
interest	O
are	O
extracted	O
(	O
“	O
masking	O
”	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

Having	O
obtained	O
these	O
segmented	O
object	O
points	O
,	O
we	O
further	O
normalize	O
its	O
coordinates	O
to	O
boost	O
the	O
translational	O
invariance	O
of	O
the	O
algorithm	O
,	O
following	O
the	O
same	O
rationale	O
as	O
in	O
the	O
frustum	Method
proposal	Method
step	Method
.	O

In	O
our	O
implementation	O
,	O
we	O
transform	O
the	O
point	O
cloud	O
into	O
a	O
local	O
coordinate	O
by	O
subtracting	O
XYZ	O
values	O
by	O
its	O
centroid	O
.	O

This	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

Note	O
that	O
we	O
intentionally	O
do	O
not	O
scale	O
the	O
point	O
cloud	O
,	O
because	O
the	O
bounding	O
sphere	O
size	O
of	O
a	O
partial	O
point	O
cloud	O
can	O
be	O
greatly	O
affected	O
by	O
viewpoints	O
and	O
the	O
real	O
size	O
of	O
the	O
point	O
cloud	O
helps	O
the	O
box	Method
size	Method
estimation	Method
.	O

In	O
our	O
experiments	O
,	O
we	O
find	O
that	O
coordinate	O
transformations	O
such	O
as	O
the	O
one	O
above	O
and	O
the	O
previous	O
frustum	O
rotation	O
are	O
critical	O
for	O
3D	Task
detection	Task
result	O
as	O
shown	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

subsection	O
:	O
Amodal	Task
3D	Task
Box	Task
Estimation	Task
Given	O
the	O
segmented	O
object	O
points	O
(	O
in	O
3D	O
mask	O
coordinate	O
)	O
,	O
this	O
module	O
estimates	O
the	O
object	O
’s	O
amodal	O
oriented	O
3D	O
bounding	O
box	O
by	O
using	O
a	O
box	Method
regression	Method
PointNet	Method
together	O
with	O
a	O
preprocessing	Method
transformer	Method
network	Method
.	O

paragraph	O
:	O
Learning	Task
-	Task
based	Task
3D	Task
Alignment	Task
by	O
T	Method
-	Method
Net	Method
Even	O
though	O
we	O
have	O
aligned	O
segmented	O
object	O
points	O
according	O
to	O
their	O
centroid	O
position	O
,	O
we	O
find	O
that	O
the	O
origin	O
of	O
the	O
mask	O
coordinate	O
frame	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
)	O
may	O
still	O
be	O
quite	O
far	O
from	O
the	O
amodal	O
box	O
center	O
.	O

We	O
therefore	O
propose	O
to	O
use	O
a	O
light	Method
-	Method
weight	Method
regression	Method
PointNet	Method
(	O
T	Method
-	Method
Net	Method
)	O
to	O
estimate	O
the	O
true	O
center	O
of	O
the	O
complete	O
object	O
and	O
then	O
transform	O
the	O
coordinate	O
such	O
that	O
the	O
predicted	O
center	O
becomes	O
the	O
origin	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
architecture	O
and	O
training	O
of	O
our	O
T	Method
-	Method
Net	Method
is	O
similar	O
to	O
the	O
T	Method
-	Method
Net	Method
in	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
type	O
of	O
spatial	Method
transformer	Method
network	Method
(	O
STN	Method
)	O
.	O

However	O
,	O
different	O
from	O
the	O
original	O
STN	Method
that	O
has	O
no	O
direct	O
supervision	O
on	O
transformation	Task
,	O
we	O
explicitly	O
supervise	O
our	O
translation	Method
network	Method
to	O
predict	O
center	O
residuals	O
from	O
the	O
mask	O
coordinate	O
origin	O
to	O
real	O
object	O
center	O
.	O

paragraph	O
:	O
Amodal	O
3D	Task
Box	Task
Estimation	Task
PointNet	Task
The	O
box	Method
estimation	Method
network	Method
predicts	O
amodal	O
bounding	O
boxes	O
(	O
for	O
entire	O
object	O
even	O
if	O
part	O
of	O
it	O
is	O
unseen	O
)	O
for	O
objects	O
given	O
an	O
object	O
point	O
cloud	O
in	O
3D	O
object	O
coordinate	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O

The	O
network	Method
architecture	Method
is	O
similar	O
to	O
that	O
for	O
object	Task
classification	Task
,	O
however	O
the	O
output	O
is	O
no	O
longer	O
object	O
class	O
scores	O
but	O
parameters	O
for	O
a	O
3D	O
bounding	O
box	O
.	O

As	O
stated	O
in	O
Sec	O
.	O

[	O
reference	O
]	O
,	O
we	O
parameterize	O
a	O
3D	O
bounding	O
box	O
by	O
its	O
center	O
(	O
,	O
,	O
)	O
,	O
size	O
(	O
,	O
,	O
)	O
and	O
heading	O
angle	O
(	O
along	O
up	O
-	O
axis	O
)	O
.	O

We	O
take	O
a	O
“	O
residual	Method
”	Method
approach	Method
for	O
box	Task
center	Task
estimation	Task
.	O

The	O
center	O
residual	O
predicted	O
by	O
the	O
box	Method
estimation	Method
network	Method
is	O
combined	O
with	O
the	O
previous	O
center	O
residual	O
from	O
the	O
T	Method
-	Method
Net	Method
and	O
the	O
masked	O
points	O
’	O
centroid	O
to	O
recover	O
an	O
absolute	O
center	O
(	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

For	O
box	O
size	O
and	O
heading	O
angle	O
,	O
we	O
follow	O
previous	O
works	O
and	O
use	O
a	O
hybrid	O
of	O
classification	Method
and	Method
regression	Method
formulations	Method
.	O

Specifically	O
we	O
pre	O
-	O
define	O
size	O
templates	O
and	O
equally	O
split	O
angle	O
bins	O
.	O

Our	O
model	O
will	O
both	O
classify	O
size	O
/	O
heading	O
(	O
scores	O
for	O
size	O
,	O
scores	O
for	O
heading	Task
)	O
to	O
those	O
pre	O
-	O
defined	O
categories	O
as	O
well	O
as	O
predict	O
residual	O
numbers	O
for	O
each	O
category	O
(	O
residual	O
dimensions	O
for	O
height	O
,	O
width	O
,	O
length	O
,	O
residual	O
angles	O
for	O
heading	O
)	O
.	O

In	O
the	O
end	O
the	O
net	O
outputs	O
numbers	O
in	O
total	O
.	O

subsection	O
:	O
Training	O
with	O
Multi	Task
-	Task
task	Task
Losses	Task
We	O
simultaneously	O
optimize	O
the	O
three	O
nets	O
involved	O
(	O
3D	Task
instance	Task
segmentation	Task
PointNet	Task
,	O
T	Method
-	Method
Net	Method
and	O
amodal	Method
box	Method
estimation	Method
PointNet	Method
)	O
with	O
multi	O
-	O
task	O
losses	O
(	O
as	O
in	O
Eq	O
.	O

[	O
reference	O
]	O
)	O
.	O

is	O
for	O
T	Method
-	Method
Net	Method
and	O
is	O
for	O
center	Method
regression	Method
of	Method
box	Method
estimation	Method
net	Method
.	O

and	O
are	O
losses	O
for	O
heading	Task
angle	Task
prediction	Task
while	O
and	O
are	O
for	O
box	O
size	O
.	O

Softmax	Method
is	O
used	O
for	O
all	O
classification	Task
tasks	Task
and	O
smooth	Method
-	Method
(	Method
huber	Method
)	Method
loss	Method
is	O
used	O
for	O
all	O
regression	Task
cases	Task
.	O

paragraph	O
:	O
Corner	O
Loss	O
for	O
Joint	Task
Optimization	Task
of	Task
Box	Task
Parameters	Task
While	O
our	O
3D	Method
bounding	Method
box	Method
parameterization	Method
is	O
compact	O
and	O
complete	O
,	O
learning	Method
is	O
not	O
optimized	O
for	O
final	O
3D	Metric
box	Metric
accuracy	Metric
–	O
center	O
,	O
size	O
and	O
heading	O
have	O
separate	O
loss	O
terms	O
.	O

Imagine	O
cases	O
where	O
center	O
and	O
size	O
are	O
accurately	O
predicted	O
but	O
heading	O
angle	O
is	O
off	O
–	O
the	O
3D	O
IoU	O
with	O
ground	O
truth	O
box	O
will	O
then	O
be	O
dominated	O
by	O
the	O
angle	Metric
error	Metric
.	O

Ideally	O
all	O
three	O
terms	O
(	O
center	O
,	O
size	O
,	O
heading	O
)	O
should	O
be	O
jointly	O
optimized	O
for	O
best	O
3D	Task
box	Task
estimation	Task
(	O
under	O
IoU	Metric
metric	Metric
)	O
.	O

To	O
resolve	O
this	O
problem	O
we	O
propose	O
a	O
novel	O
regularization	Method
loss	Method
,	O
the	O
corner	Method
loss	Method
:	O
In	O
essence	O
,	O
the	O
corner	Metric
loss	Metric
is	O
the	O
sum	O
of	O
the	O
distances	O
between	O
the	O
eight	O
corners	O
of	O
a	O
predicted	O
box	O
and	O
a	O
ground	O
truth	O
box	O
.	O

Since	O
corner	O
positions	O
are	O
jointly	O
determined	O
by	O
center	O
,	O
size	O
and	O
heading	O
,	O
the	O
corner	Method
loss	Method
is	O
able	O
to	O
regularize	O
the	O
multi	Task
-	Task
task	Task
training	Task
for	O
those	O
parameters	O
.	O

To	O
compute	O
the	O
corner	O
loss	O
,	O
we	O
firstly	O
construct	O
“	O
anchor	O
”	O
boxes	O
from	O
all	O
size	O
templates	O
and	O
heading	O
angle	O
bins	O
.	O

The	O
anchor	O
boxes	O
are	O
then	O
translated	O
to	O
the	O
estimated	O
box	O
center	O
.	O

We	O
denote	O
the	O
anchor	O
box	O
corners	O
as	O
,	O
where	O
,	O
,	O
are	O
indices	O
for	O
the	O
size	O
class	O
,	O
heading	O
class	O
,	O
and	O
(	O
predefined	O
)	O
corner	O
order	O
,	O
respectively	O
.	O

To	O
avoid	O
large	O
penalty	O
from	O
flipped	Task
heading	Task
estimation	Task
,	O
we	O
further	O
compute	O
distances	O
to	O
corners	O
(	O
from	O
the	O
flipped	O
ground	O
truth	O
box	O
and	O
use	O
the	O
minimum	O
of	O
the	O
original	O
and	O
flipped	O
cases	O
.	O

,	O
which	O
is	O
one	O
for	O
the	O
ground	O
truth	O
size	O
/	O
heading	O
class	O
and	O
zero	O
else	O
wise	O
,	O
is	O
a	O
two	O
-	O
dimensional	O
mask	O
used	O
to	O
select	O
the	O
distance	O
term	O
we	O
care	O
about	O
.	O

section	O
:	O
Experiments	O
Experiments	O
are	O
divided	O
into	O
three	O
parts	O
.	O

First	O
we	O
compare	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
for	O
3D	O
object	O
detection	Task
on	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Second	O
,	O
we	O
provide	O
in	O
-	O
depth	O
analysis	O
to	O
validate	O
our	O
design	O
choices	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

Last	O
,	O
we	O
show	O
qualitative	O
results	O
and	O
discuss	O
the	O
strengths	O
and	O
limitations	O
of	O
our	O
methods	O
(	O
Sec	O
[	O
reference	O
]	O
)	O
.	O

subsection	O
:	O
Comparing	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Methods	O
We	O
evaluate	O
our	O
3D	Method
object	Method
detector	Method
on	O
KITTI	Material
and	O
SUN	Material
-	Material
RGBD	Material
benchmarks	O
for	O
3D	O
object	O
detection	Task
.	O

On	O
both	O
tasks	O
we	O
have	O
achieved	O
significantly	O
better	O
results	O
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

paragraph	O
:	O
KITTI	Material
Tab	O
.	O

[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
our	O
3D	Method
detector	Method
on	O
the	O
KITTI	Material
test	O
set	O
.	O

We	O
outperform	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
by	O
a	O
large	O
margin	O
.	O

While	O
MV3D	Method
uses	O
multi	Method
-	Method
view	Method
feature	Method
aggregation	Method
and	O
sophisticated	O
multi	Method
-	Method
sensor	Method
fusion	Method
strategy	Method
,	O
our	O
method	O
based	O
on	O
the	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	O
++	O
(	O
v2	O
)	O
backbone	O
is	O
much	O
cleaner	O
in	O
design	O
.	O

While	O
out	O
of	O
the	O
scope	O
for	O
this	O
work	O
,	O
we	O
expect	O
that	O
sensor	Task
fusion	Task
(	O
esp	O
.	O

aggregation	Task
of	Task
image	Task
feature	Task
for	O
3D	Task
detection	Task
)	O
could	O
further	O
improve	O
our	O
results	O
.	O

We	O
also	O
show	O
our	O
method	O
’s	O
performance	O
on	O
3D	O
object	Task
localization	Task
(	O
bird	O
’s	O
eye	O
view	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
.	O

In	O
the	O
3D	O
localization	Task
task	O
bounding	O
boxes	O
are	O
projected	O
to	O
bird	O
’s	O
eye	O
view	O
plane	O
and	O
IoU	O
is	O
evaluated	O
on	O
oriented	O
2D	O
boxes	O
.	O

Again	O
,	O
our	O
method	O
significantly	O
outperforms	O
previous	O
works	O
which	O
include	O
DoBEM	Method
and	O
MV3D	Method
that	O
use	O
CNNs	Method
on	O
projected	Material
LiDAR	Material
images	Material
,	O
as	O
well	O
as	O
3D	Method
FCN	Method
that	O
uses	O
3D	Method
CNNs	Method
on	O
voxelized	O
point	O
cloud	O
.	O

The	O
output	O
of	O
our	O
network	O
is	O
visualized	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
where	O
we	O
observe	O
accurate	O
3D	Task
instance	Task
segmentation	Task
and	O
box	Task
prediction	Task
even	O
under	O
very	O
challenging	O
cases	O
.	O

We	O
defer	O
more	O
discussions	O
on	O
success	O
and	O
failure	O
case	O
patterns	O
to	O
Sec	O
.	O

[	O
reference	O
]	O
.	O

We	O
also	O
report	O
performance	O
on	O
KITTI	Material
val	O
set	O
(	O
the	O
same	O
split	O
as	O
in	O
)	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
and	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
cars	O
)	O
to	O
support	O
comparison	O
with	O
more	O
published	O
works	O
,	O
and	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
for	O
pedestrians	O
and	O
cyclists	O
)	O
for	O
reference	O
.	O

paragraph	O
:	O
SUN	Material
-	Material
RGBD	Material
Most	O
previous	O
3D	Task
detection	Task
works	O
specialize	O
either	O
on	O
outdoor	Material
LiDAR	Material
scans	Material
where	O
objects	O
are	O
well	O
separated	O
in	O
space	O
and	O
the	O
point	O
cloud	O
is	O
sparse	O
(	O
so	O
that	O
it	O
’s	O
feasible	O
for	O
bird	Task
’s	Task
eye	Task
projection	Task
)	O
,	O
or	O
on	O
indoor	Task
depth	Task
maps	Task
that	O
are	O
regular	O
images	O
with	O
dense	O
pixel	O
values	O
such	O
that	O
image	Method
CNNs	Method
can	O
be	O
easily	O
applied	O
.	O

However	O
,	O
methods	O
designed	O
for	O
bird	Task
’s	Task
eye	Task
view	Task
may	O
be	O
incapable	O
for	O
indoor	O
rooms	O
where	O
multiple	O
objects	O
often	O
exist	O
together	O
in	O
vertical	O
space	O
.	O

On	O
the	O
other	O
hand	O
,	O
indoor	Method
focused	Method
methods	Method
could	O
find	O
it	O
hard	O
to	O
apply	O
to	O
sparse	Task
and	Task
large	Task
-	Task
scale	Task
point	Task
cloud	Task
from	O
LiDAR	Material
scans	Material
.	O

In	O
contrast	O
,	O
our	O
frustum	Method
-	Method
based	Method
PointNet	Method
is	O
a	O
generic	O
framework	O
for	O
both	O
outdoor	O
and	O
indoor	O
3D	O
object	O
detection	Task
.	O

By	O
applying	O
the	O
same	O
pipeline	O
we	O
used	O
for	O
KITTI	Material
data	O
set	O
,	O
we	O
’	O
ve	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
SUN	Material
-	Material
RGBD	Material
benchmark	O
(	O
Tab	O
.	O

[	O
reference	O
]	O
)	O
with	O
significantly	O
higher	O
mAP	Metric
as	O
well	O
as	O
much	O
faster	O
(	O
10x	O
-	O
1000x	O
)	O
inference	Metric
speed	Metric
.	O

subsection	O
:	O
Architecture	Task
Design	Task
Analysis	Task
In	O
this	O
section	O
we	O
provide	O
analysis	O
and	O
ablation	O
experiments	O
to	O
validate	O
our	O
design	O
choices	O
.	O

paragraph	O
:	O
Experiment	O
setup	O
.	O

Unless	O
otherwise	O
noted	O
,	O
all	O
experiments	O
in	O
this	O
section	O
are	O
based	O
on	O
our	O
v1	Method
model	Method
on	O
KITTI	Material
data	O
using	O
train	O
/	O
val	O
split	O
as	O
in	O
.	O

To	O
decouple	O
the	O
influence	O
of	O
2D	Method
detectors	Method
,	O
we	O
use	O
ground	O
truth	O
2D	O
boxes	O
for	O
region	Task
proposals	Task
and	O
use	O
3D	Metric
box	Metric
estimation	Metric
accuracy	Metric
(	O
IoU	Metric
threshold	Metric
0.7	Metric
)	O
as	O
the	O
evaluation	Metric
metric	Metric
.	O

We	O
will	O
only	O
focus	O
on	O
the	O
car	Task
category	Task
which	O
has	O
the	O
most	O
training	O
examples	O
.	O

paragraph	O
:	O
Comparing	O
with	O
alternative	O
approaches	O
for	O
3D	Task
detection	Task
.	O

In	O
this	O
part	O
we	O
evaluate	O
a	O
few	O
CNN	Method
-	Method
based	Method
baseline	Method
approaches	Method
as	O
well	O
as	O
ablated	Method
versions	Method
and	O
variants	O
of	O
our	O
pipelines	O
using	O
2D	O
masks	O
.	O

In	O
the	O
first	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
show	O
3D	Task
box	Task
estimation	Task
results	O
from	O
two	O
CNN	Method
-	Method
based	Method
networks	Method
.	O

The	O
baseline	O
methods	O
trained	O
VGG	Method
models	Method
on	O
ground	Material
truth	Material
boxes	Material
of	Material
RGB	Material
-	Material
D	Material
images	Material
and	O
adopt	O
the	O
same	O
box	O
parameter	O
and	O
loss	O
functions	O
as	O
our	O
main	O
method	O
.	O

While	O
the	O
model	O
in	O
the	O
first	O
row	O
directly	O
estimates	O
box	O
location	O
and	O
parameters	O
from	O
vanilla	Material
RGB	Material
-	Material
D	Material
image	Material
patch	Material
,	O
the	O
other	O
one	O
(	O
second	O
row	O
)	O
uses	O
a	O
FCN	Method
trained	O
from	O
the	O
COCO	Material
dataset	Material
for	O
2D	Task
mask	Task
estimation	Task
(	O
as	O
that	O
in	O
Mask	Method
-	Method
RCNN	Method
)	O
and	O
only	O
uses	O
features	O
from	O
the	O
masked	O
region	O
for	O
prediction	Task
.	O

The	O
depth	O
values	O
are	O
also	O
translated	O
by	O
subtracting	O
the	O
median	O
depth	O
within	O
the	O
2D	O
mask	O
.	O

However	O
,	O
both	O
CNN	Method
baselines	Method
get	O
far	O
worse	O
results	O
compared	O
to	O
our	O
main	O
method	O
.	O

To	O
understand	O
why	O
CNN	Method
baselines	Method
underperform	O
,	O
we	O
visualize	O
a	O
typical	O
2D	Task
mask	Task
prediction	Task
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

While	O
the	O
estimated	O
2D	O
mask	O
appears	O
in	O
high	O
quality	O
on	O
an	O
RGB	Material
image	Material
,	O
there	O
are	O
still	O
lots	O
of	O
clutter	O
and	O
foreground	O
points	O
in	O
the	O
2D	O
mask	O
.	O

In	O
comparison	O
,	O
our	O
3D	Method
instance	Method
segmentation	Method
gets	O
much	O
cleaner	O
result	O
,	O
which	O
greatly	O
eases	O
the	O
next	O
module	O
in	O
finer	Task
localization	Task
and	O
bounding	Task
box	Task
regression	Task
.	O

In	O
the	O
third	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
,	O
we	O
experiment	O
with	O
an	O
ablated	Method
version	Method
of	O
frustum	Method
PointNet	Method
that	O
has	O
no	O
3D	Method
instance	Method
segmentation	Method
module	Method
.	O

Not	O
surprisingly	O
,	O
the	O
model	O
gets	O
much	O
worse	O
results	O
than	O
our	O
main	O
method	O
,	O
which	O
indicates	O
the	O
critical	O
effect	O
of	O
our	O
3D	Method
instance	Method
segmentation	Method
module	Method
.	O

In	O
the	O
fourth	O
row	O
,	O
instead	O
of	O
3D	Task
segmentation	Task
we	O
use	O
point	O
clouds	O
from	O
2D	O
masked	O
depth	O
maps	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
for	O
3D	Task
box	Task
estimation	Task
.	O

However	O
,	O
since	O
a	O
2D	O
mask	O
is	O
not	O
able	O
to	O
cleanly	O
segment	O
the	O
3D	O
object	O
,	O
the	O
performance	O
is	O
more	O
than	O
12	O
%	O
worse	O
than	O
that	O
with	O
the	O
3D	Method
segmentation	Method
(	O
our	O
main	O
method	O
in	O
the	O
fifth	O
row	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
a	O
combined	O
usage	O
of	O
2D	Method
and	Method
3D	Method
masks	Method
–	O
applying	O
3D	Method
segmentation	Method
on	O
point	O
cloud	O
from	O
2D	O
masked	O
depth	O
map	O
–	O
also	O
shows	O
slightly	O
worse	O
results	O
than	O
our	O
main	O
method	O
probably	O
due	O
to	O
the	O
accumulated	O
error	O
from	O
inaccurate	O
2D	Method
mask	Method
predictions	Method
.	O

paragraph	O
:	O
Effects	O
of	O
point	Method
cloud	Method
normalization	Method
.	O

As	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
,	O
our	O
frustum	O
PointNet	Method
takes	O
a	O
few	O
key	O
coordinate	O
transformations	O
to	O
canonicalize	O
the	O
point	O
cloud	O
for	O
more	O
effective	O
learning	Task
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
how	O
each	O
normalization	Method
step	Method
helps	O
for	O
3D	Task
detection	Task
.	O

We	O
see	O
that	O
both	O
frustum	O
rotation	O
(	O
such	O
that	O
frustum	O
points	O
have	O
more	O
similar	O
XYZ	O
distributions	O
)	O
and	O
mask	Method
centroid	Method
subtraction	Method
(	O
such	O
that	O
object	O
points	O
have	O
smaller	O
and	O
more	O
canonical	O
XYZ	O
)	O
are	O
critical	O
.	O

In	O
addition	O
,	O
extra	O
alignment	O
of	O
object	O
point	O
cloud	O
to	O
object	O
center	O
by	O
T	Method
-	Method
Net	Method
also	O
contributes	O
significantly	O
to	O
the	O
performance	O
.	O

paragraph	O
:	O
Effects	O
of	O
regression	Method
loss	Method
formulation	Method
and	O
corner	Method
loss	Method
.	O

In	O
Tab	O
.	O

[	O
reference	O
]	O
we	O
compare	O
different	O
loss	O
options	O
and	O
show	O
that	O
a	O
combination	O
of	O
“	O
cls	Method
-	Method
reg	Method
”	Method
loss	Method
(	O
the	O
classification	Method
and	Method
residual	Method
regression	Method
approach	Method
for	O
heading	Task
and	Task
size	Task
regression	Task
)	O
and	O
a	O
regularizing	Method
corner	Method
loss	Method
achieves	O
the	O
best	O
result	O
.	O

The	O
naive	O
baseline	O
using	O
regression	Method
loss	Method
only	O
(	O
first	O
row	O
)	O
achieves	O
unsatisfactory	O
result	O
because	O
the	O
regression	O
target	O
is	O
large	O
in	O
range	O
(	O
object	O
size	O
from	O
0.2	O
m	O
to	O
5	O
m	O
)	O
.	O

In	O
comparison	O
,	O
the	O
cls	Method
-	Method
reg	Method
loss	Method
and	O
a	O
normalized	Method
version	Method
(	O
residual	O
normalized	O
by	O
heading	O
bin	O
size	O
or	O
template	O
shape	O
size	O
)	O
of	O
it	O
achieve	O
much	O
better	O
performance	O
.	O

At	O
last	O
row	O
we	O
show	O
that	O
a	O
regularizing	O
corner	O
loss	O
further	O
helps	O
optimization	Task
.	O

subsection	O
:	O
Qualitative	O
Results	O
and	O
Discussion	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
representative	O
outputs	O
of	O
our	O
frustum	Method
PointNet	Method
model	Method
.	O

We	O
see	O
that	O
for	O
simple	O
cases	O
of	O
non	O
-	O
occluded	O
objects	O
in	O
reasonable	O
distance	O
(	O
so	O
we	O
get	O
enough	O
number	O
of	O
points	O
)	O
,	O
our	O
model	O
outputs	O
remarkably	O
accurate	O
3D	O
instance	O
segmentation	O
mask	O
and	O
3D	O
bounding	O
boxes	O
.	O

Second	O
,	O
we	O
are	O
surprised	O
to	O
find	O
that	O
our	O
model	O
can	O
even	O
predict	O
correctly	O
posed	O
amodal	O
3D	O
box	O
from	O
partial	O
data	O
(	O
e.g.	O
parallel	O
parked	O
cars	O
)	O
with	O
few	O
points	O
.	O

Even	O
humans	O
find	O
it	O
very	O
difficult	O
to	O
annotate	O
such	O
results	O
with	O
point	O
cloud	O
data	O
only	O
.	O

Third	O
,	O
in	O
some	O
cases	O
that	O
seem	O
very	O
challenging	O
in	O
images	O
with	O
lots	O
of	O
nearby	O
or	O
even	O
overlapping	O
2D	O
boxes	O
,	O
when	O
converted	O
to	O
3D	O
space	O
,	O
the	O
localization	Task
becomes	O
much	O
easier	O
(	O
e.g.	O
P11	O
in	O
second	O
row	O
third	O
column	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
we	O
do	O
observe	O
several	O
failure	O
patterns	O
,	O
which	O
indicate	O
possible	O
directions	O
for	O
future	O
efforts	O
.	O

The	O
first	O
common	O
mistake	O
is	O
due	O
to	O
inaccurate	Task
pose	Task
and	Task
size	Task
estimation	Task
in	O
a	O
sparse	O
point	O
cloud	O
(	O
sometimes	O
less	O
than	O
5	O
points	O
)	O
.	O

We	O
think	O
image	O
features	O
could	O
greatly	O
help	O
esp	Task
.	O

since	O
we	O
have	O
access	O
to	O
high	O
resolution	O
image	O
patch	O
even	O
for	O
far	O
-	O
away	O
objects	O
.	O

The	O
second	O
type	O
of	O
challenge	O
is	O
when	O
there	O
are	O
multiple	O
instances	O
from	O
the	O
same	O
category	O
in	O
a	O
frustum	O
(	O
like	O
two	O
persons	O
standing	O
by	O
)	O
.	O

Since	O
our	O
current	O
pipeline	O
assumes	O
a	O
single	O
object	O
of	O
interest	O
in	O
each	O
frustum	O
,	O
it	O
may	O
get	O
confused	O
when	O
multiple	O
instances	O
appear	O
and	O
thus	O
outputs	O
mixed	O
segmentation	O
results	O
.	O

This	O
problem	O
could	O
potentially	O
be	O
mitigated	O
if	O
we	O
are	O
able	O
to	O
propose	O
multiple	O
3D	O
bounding	O
boxes	O
within	O
each	O
frustum	O
.	O

Thirdly	O
,	O
sometimes	O
our	O
2D	Method
detector	Method
misses	O
objects	O
due	O
to	O
dark	O
lighting	O
or	O
strong	O
occlusion	O
.	O

Since	O
our	O
frustum	Method
proposals	Method
are	O
based	O
on	O
region	Method
proposal	Method
,	O
no	O
3D	O
object	O
will	O
be	O
detected	O
given	O
no	O
2D	Task
detection	Task
.	O

However	O
,	O
our	O
3D	Task
instance	Task
segmentation	Task
and	O
amodal	Task
3D	Task
box	Task
estimation	Task
PointNets	Task
are	O
not	O
restricted	O
to	O
RGB	O
view	O
proposals	O
.	O

As	O
shown	O
in	O
the	O
supplementary	O
,	O
the	O
same	O
framework	O
can	O
also	O
be	O
extended	O
to	O
3D	O
regions	O
proposed	O
in	O
bird	O
’s	O
eye	O
view	O
.	O

paragraph	O
:	O
Acknowledgement	O
The	O
authors	O
wish	O
to	O
thank	O
the	O
support	O
of	O
Nuro	O
Inc.	O
,	O
ONR	O
MURI	O
grant	O
N00014	O
-	O
13	O
-	O
1	O
-	O
0341	O
,	O
NSF	O
grants	O
DMS	O
-	O
1546206	O
and	O
IIS	O
-	O
1528025	O
,	O
a	O
Samsung	O
GRO	O
award	O
,	O
and	O
gifts	O
from	O
Adobe	O
,	O
Amazon	O
,	O
and	O
Apple	O
.	O

bibliography	O
:	O
References	O
appendix	O
:	O
Overview	O
This	O
document	O
provides	O
additional	O
technical	O
details	O
,	O
extra	O
analysis	O
experiments	O
,	O
more	O
quantitative	O
results	O
and	O
qualitative	O
test	O
results	O
to	O
the	O
main	O
paper	O
.	O

In	O
Sec.ï½	O
[	O
reference	O
]	O
we	O
provide	O
more	O
details	O
on	O
network	Method
architectures	Method
of	O
PointNets	O
and	O
training	O
parameters	O
while	O
Sec	O
.	O

[	O
reference	O
]	O
explains	O
more	O
about	O
our	O
2D	Method
detector	Method
.	O

Sec	O
.	O

[	O
reference	O
]	O
shows	O
how	O
our	O
framework	O
can	O
be	O
extended	O
to	O
bird	O
’s	O
eye	O
view	O
(	O
BV	O
)	O
proposals	O
and	O
how	O
combining	O
BV	Method
and	Method
RGB	Method
proposals	Method
can	O
further	O
improve	O
detection	Task
performance	O
.	O

Then	O
Sec	O
.	O

[	O
reference	O
]	O
presents	O
results	O
from	O
more	O
analysis	O
experiments	O
.	O

At	O
last	O
,	O
Sec	O
.	O

[	O
reference	O
]	O
shows	O
more	O
visualization	O
results	O
for	O
3D	Task
detection	Task
on	O
SUN	Material
-	Material
RGBD	Material
dataset	O
.	O

appendix	O
:	O
Details	O
on	O
Frustum	Method
PointNets	Method
(	O
Sec	O
4.2	O
,	O
4.3	O
)	O
subsection	O
:	O
Network	Method
Architectures	Method
We	O
adopt	O
similar	O
network	Method
architectures	Method
as	O
in	O
the	O
original	O
works	O
of	O
PointNet	O
and	O
PointNet	O
++	O
for	O
our	O
v1	Method
and	Method
v2	Method
models	Method
respectively	O
.	O

What	O
is	O
different	O
is	O
that	O
we	O
add	O
an	O
extra	O
link	O
for	O
class	O
one	O
-	O
hot	O
vector	O
such	O
that	O
instance	Task
segmentation	Task
and	O
bounding	Task
box	Task
estimation	Task
can	O
leverage	O
semantics	O
predicted	O
from	O
RGB	Material
images	Material
.	O

The	O
detailed	O
network	Method
architectures	Method
are	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

For	O
v1	Method
model	Method
our	O
architecture	O
involves	O
point	Method
embedding	Method
layers	Method
(	O
as	O
shared	O
MLP	Method
on	O
each	O
point	O
independently	O
)	O
,	O
a	O
max	Method
pooling	Method
layer	Method
and	O
per	Method
-	Method
point	Method
classification	Method
multi	Method
-	Method
layer	Method
perceptron	Method
(	O
MLP	Method
)	O
based	O
on	O
aggregated	O
information	O
from	O
global	O
feature	O
and	O
each	O
point	O
as	O
well	O
as	O
an	O
one	O
-	O
hot	O
class	O
vector	O
.	O

Note	O
that	O
we	O
do	O
not	O
use	O
the	O
transformer	Method
networks	Method
as	O
in	O
because	O
frustum	O
points	O
are	O
viewpoint	O
based	O
(	O
not	O
complete	O
point	O
cloud	O
as	O
in	O
)	O
and	O
are	O
already	O
normalized	O
by	O
frustum	O
rotation	O
.	O

In	O
addition	O
to	O
XYZ	O
,	O
we	O
also	O
leverage	O
LiDAR	O
intensity	O
as	O
a	O
fourth	O
channel	O
.	O

For	O
v2	Method
model	Method
we	O
use	O
set	Method
abstraction	Method
layers	Method
for	O
hierarchical	Task
feature	Task
learning	Task
in	O
point	Task
clouds	Task
.	O

In	O
addition	O
,	O
because	O
LiDAR	Method
point	Method
cloud	Method
gets	O
increasingly	O
sparse	O
as	O
it	O
gets	O
farther	O
,	O
feature	Method
learning	Method
has	O
to	O
be	O
robust	O
to	O
those	O
density	O
variations	O
.	O

Therefore	O
we	O
used	O
a	O
robust	O
type	O
of	O
set	Method
abstraction	Method
layers	Method
–	O
multi	Method
-	Method
scale	Method
grouping	Method
(	Method
MSG	Method
)	Method
layers	Method
as	O
introduced	O
in	O
for	O
the	O
segmentation	Method
network	Method
.	O

With	O
hierarchical	O
features	O
and	O
learned	O
robustness	O
to	O
varying	O
densities	O
,	O
our	O
v2	Method
model	Method
shows	O
superior	O
performance	O
than	O
v1	Method
model	Method
in	O
both	O
segmentation	Task
and	O
box	Task
estimation	Task
.	O

subsection	O
:	O
Data	Task
Augmentation	Task
and	O
Training	Task
paragraph	O
:	O
Data	Task
augmentation	Task
Data	Task
augmentation	Task
plays	O
an	O
important	O
role	O
in	O
preventing	O
model	Task
overfitting	Task
.	O

Our	O
augmentation	O
involves	O
two	O
branches	O
:	O
one	O
is	O
2D	Method
box	Method
augmentation	Method
and	O
the	O
other	O
is	O
frustum	Method
point	Method
cloud	Method
augmentation	Method
.	O

We	O
use	O
ground	O
truth	O
2D	O
boxes	O
to	O
generate	O
frustum	O
point	O
clouds	O
for	O
Frustum	Method
PointNets	Method
training	O
and	O
augment	O
the	O
2D	O
boxes	O
by	O
random	O
translation	O
and	O
scaling	O
.	O

Specifically	O
,	O
we	O
firstly	O
compute	O
the	O
2D	O
box	O
height	O
(	O
)	O
and	O
width	O
(	O
)	O
and	O
translate	O
the	O
2D	O
box	O
center	O
by	O
random	O
distances	O
sampled	O
from	O
Uniform	O
[	O
]	O
and	O
Uniform	O
[	O
]	O
in	O
u	O
,	O
v	O
directions	O
respectively	O
.	O

The	O
height	O
and	O
width	O
are	O
also	O
augmented	O
by	O
two	O
random	O
scaling	O
factor	O
sampled	O
from	O
Uniform	O
[	O
]	O
.	O

We	O
augment	O
each	O
frustum	O
point	O
cloud	O
by	O
three	O
ways	O
.	O

First	O
,	O
we	O
randomly	O
sample	O
a	O
subset	O
of	O
points	O
from	O
the	O
frustum	O
point	O
cloud	O
on	O
the	O
fly	O
(	O
1	O
,	O
024	O
for	O
KITTI	Material
and	O
2	O
,	O
048	O
for	O
SUN	Material
-	Material
RGBD	Material
)	O
.	O

For	O
object	O
points	O
segmented	O
from	O
our	O
predicted	O
3D	O
mask	O
,	O
we	O
randomly	O
sample	O
512	O
points	O
from	O
it	O
(	O
if	O
there	O
are	O
less	O
than	O
512	O
points	O
we	O
will	O
randomly	O
resample	O
to	O
make	O
up	O
for	O
the	O
number	O
)	O
.	O

Second	O
,	O
we	O
randomly	O
flip	O
the	O
frustum	O
point	O
cloud	O
(	O
after	O
rotating	O
the	O
frustum	O
to	O
the	O
center	O
)	O
along	O
the	O
YZ	O
plane	O
in	O
camera	O
coordinate	O
(	O
Z	O
is	O
forward	O
,	O
Y	O
is	O
pointing	O
down	O
)	O
.	O

Thirdly	O
,	O
we	O
perturb	O
the	O
points	O
by	O
shifting	O
the	O
entire	O
frustum	O
point	O
cloud	O
in	O
Z	O
-	O
axis	O
direction	O
such	O
that	O
the	O
depth	O
of	O
points	O
is	O
augmented	O
.	O

Together	O
with	O
all	O
data	Task
augmentation	Task
,	O
we	O
modify	O
the	O
ground	O
truth	O
labels	O
for	O
3D	O
mask	O
and	O
headings	O
correspondingly	O
.	O

paragraph	O
:	O
KITTI	Material
Training	O
The	O
object	O
detection	Task
benchmark	O
in	O
KITTI	Material
provides	O
synchronized	Material
RGB	Material
images	Material
and	O
LiDAR	Material
point	Material
clouds	Material
with	O
ground	O
truth	O
amodal	O
2D	O
and	O
3D	O
box	O
annotations	O
for	O
vehicles	O
,	O
pedestrians	O
and	O
cyclists	O
.	O

The	O
training	O
set	O
contains	O
7	O
,	O
481	O
frames	O
and	O
an	O
undisclosed	O
test	O
set	O
contains	O
7	O
,	O
581	O
frames	O
.	O

In	O
our	O
own	O
experiments	O
(	O
except	O
those	O
for	O
test	O
sets	O
)	O
,	O
we	O
follow	O
to	O
split	O
the	O
official	O
training	O
set	O
to	O
a	O
train	O
set	O
of	O
3	O
,	O
717	O
frames	O
and	O
a	O
val	O
set	O
of	O
3769	O
frames	O
such	O
that	O
frames	O
in	O
train	O
/	O
val	O
sets	O
belong	O
to	O
different	O
video	O
clips	O
.	O

For	O
models	O
evaluated	O
on	O
the	O
test	O
set	O
we	O
train	O
our	O
model	O
on	O
our	O
own	O
train	O
/	O
val	O
split	O
where	O
around	O
80	O
%	O
of	O
the	O
training	O
data	O
is	O
used	O
such	O
that	O
the	O
model	O
can	O
achieve	O
better	O
generalization	Task
by	O
seeing	O
more	O
examples	O
.	O

To	O
get	O
ground	O
truth	O
for	O
3D	Task
instance	Task
segmentation	Task
we	O
simply	O
consider	O
all	O
points	O
that	O
fall	O
into	O
the	O
ground	O
truth	O
3D	O
bounding	O
box	O
as	O
object	O
points	O
.	O

Although	O
there	O
are	O
sometimes	O
false	O
labels	O
from	O
ground	O
points	O
or	O
points	O
from	O
other	O
closeby	O
objects	O
(	O
e.g.	O
a	O
person	O
standing	O
by	O
)	O
,	O
the	O
auto	O
-	O
labeled	O
segmentation	O
ground	O
truth	O
is	O
in	O
general	O
acceptable	O
.	O

For	O
both	O
of	O
our	O
v1	Method
and	Method
v2	Method
models	Method
,	O
we	O
use	O
Adam	Method
optimizer	Method
with	O
starting	Metric
learning	Metric
rate	Metric
0.001	O
,	O
with	O
step	O
-	O
wise	O
decay	O
(	O
by	O
half	O
)	O
in	O
every	O
60k	O
iterations	O
.	O

For	O
all	O
trainable	O
layers	O
except	O
the	O
last	O
classification	Task
or	Task
regression	Task
ones	Task
,	O
we	O
use	O
batch	Method
normalization	Method
with	O
a	O
start	O
decay	O
rate	O
of	O
0.5	O
and	O
gradually	O
decay	O
the	O
decay	Metric
rate	Metric
to	O
0.99	O
(	O
step	O
-	O
wise	O
decay	O
with	O
rate	O
0.5	O
in	O
every	O
20k	O
iterations	O
)	O
.	O

We	O
use	O
batch	O
size	O
32	O
for	O
v1	Method
models	Method
and	O
batch	O
size	O
24	O
for	O
v2	Method
models	Method
.	O

All	O
three	O
PointNets	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O

Trained	O
on	O
a	O
single	O
GTX	Method
1080	Method
GPU	Method
,	O
it	O
takes	O
around	O
one	O
day	O
to	O
train	O
a	O
v1	Method
model	Method
(	O
all	O
three	O
nets	O
)	O
for	O
200	O
epochs	O
while	O
it	O
takes	O
around	O
three	O
days	O
for	O
a	O
v2	Method
model	Method
.	O

We	O
picked	O
the	O
early	O
stopped	O
(	O
200	O
epochs	O
)	O
snapshot	Method
models	Method
for	O
evaluation	O
.	O

paragraph	O
:	O
SUN	Material
-	Material
RGBD	Material
Training	Material
The	O
data	O
set	O
consists	O
of	O
10	O
,	O
355	O
RGB	Material
-	Material
D	Material
images	Material
captured	O
from	O
various	O
depth	O
sensors	O
for	O
indoor	O
scenes	O
(	O
bedrooms	O
,	O
dining	O
rooms	O
etc	O
.	O

)	O
.	O

We	O
follow	O
the	O
same	O
train	O
/	O
val	O
splits	O
as	O
for	O
experiments	O
.	O

The	O
data	Method
augmentation	Method
and	O
optimization	O
parameters	O
are	O
the	O
same	O
as	O
that	O
in	O
KITTI	Material
.	O

As	O
to	O
auto	Task
-	Task
labeling	Task
of	Task
instance	Task
segmentation	Task
mask	Task
,	O
however	O
,	O
data	Metric
quality	Metric
is	O
much	O
lower	O
than	O
that	O
in	O
KITTI	Material
because	O
of	O
strong	O
occlusions	O
and	O
tight	O
arrangement	O
of	O
objects	O
in	O
indoor	O
scenes	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
some	O
examples	O
)	O
.	O

Nonetheless	O
we	O
still	O
consider	O
all	O
points	O
within	O
the	O
ground	O
truth	O
boxes	O
as	O
object	O
points	O
for	O
our	O
training	O
.	O

For	O
3D	Task
segmentation	Task
we	O
get	O
only	O
a	O
82.7	O
%	O
accuracy	Metric
compared	O
to	O
around	O
90	O
%	O
in	O
KITTI	Material
.	O

Due	O
to	O
the	O
heavy	O
noise	O
in	O
segmentation	O
mask	O
label	O
,	O
we	O
choose	O
to	O
only	O
train	O
and	O
evaluate	O
on	O
v1	Method
models	Method
that	O
has	O
more	O
strength	O
in	O
global	Method
feature	Method
learning	Method
than	O
v2	O
ones	O
.	O

For	O
future	O
works	O
,	O
we	O
think	O
higher	O
quality	O
in	O
3D	O
mask	O
labels	O
can	O
greatly	O
help	O
the	O
instance	Task
segmentation	Task
network	Task
training	Task
.	O

appendix	O
:	O
Details	O
on	O
RGB	Method
Detector	Method
(	O
Sec	O
4.1	O
)	O
For	O
2D	Task
RGB	Task
image	Task
detector	Task
,	O
we	O
use	O
the	O
encoder	Method
-	Method
decoder	Method
structure	Method
(	O
e.g.	O
DSSD	Method
,	O
FPN	Method
)	O
to	O
generate	O
region	O
proposals	O
from	O
multiple	O
feature	O
maps	O
using	O
focal	O
loss	O
and	O
use	O
Fast	Method
R	Method
-	Method
CNN	Method
to	O
predict	O
final	O
2D	Task
detection	Task
bounding	O
boxes	O
from	O
the	O
region	O
proposals	O
.	O

To	O
make	O
the	O
detector	Task
faster	O
,	O
we	O
take	O
the	O
reduced	Method
VGG	Method
base	Method
network	Method
architecture	Method
from	O
SSD	Method
,	O
sample	O
half	O
of	O
the	O
channels	O
per	O
layer	O
and	O
change	O
all	O
max	Method
pooling	Method
layers	Method
to	O
convolution	Method
layers	Method
with	O
kernel	O
size	O
and	O
stride	O
of	O
2	O
.	O

Then	O
we	O
fine	O
-	O
tune	O
it	O
on	O
ImageNet	Material
CLS	Material
-	Material
LOC	Material
dataset	Material
for	O
400k	O
iterations	O
with	O
batch	O
size	O
of	O
260	O
on	O
10	O
GPUs	O
.	O

The	O
resulting	O
base	O
network	Method
architecture	Method
has	O
about	O
66.7	O
%	O
top	Metric
-	Metric
1	Metric
classification	Metric
accuracy	Metric
on	O
the	O
CLS	Material
-	Material
LOC	Material
validation	Material
dataset	Material
and	O
only	O
needs	O
about	O
1.2ms	O
to	O
process	O
a	O
image	O
on	O
a	O
NVIDIA	Method
GTX	Method
1080	Method
.	O

We	O
then	O
add	O
the	O
feature	Method
pyramid	Method
layers	Method
from	O
conv3_3	Method
,	O
conv4_3	O
,	O
conv5_3	Method
,	O
and	O
fc7	Method
,	O
which	O
are	O
used	O
to	O
predict	O
region	O
proposals	O
with	O
scales	O
of	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
respectively	O
.	O

We	O
also	O
add	O
an	O
extra	O
convolutional	Method
layer	Method
(	O
conv8	Method
)	O
which	O
halves	O
the	O
fc7	O
feature	O
map	O
size	O
,	O
and	O
use	O
it	O
to	O
predict	O
proposals	O
with	O
scale	O
of	O
256	O
.	O

We	O
use	O
5	O
different	O
aspect	O
ratios	O
{	O
,	O
,	O
1	O
,	O
2	O
,	O
3	O
}	O
for	O
all	O
layers	O
except	O
that	O
we	O
ignore	O
{	O
,	O
3	O
}	O
for	O
conv3_3	O
.	O

Following	O
SSD	Method
,	O
we	O
also	O
use	O
normalization	Method
layer	Method
on	O
conv3_3	Method
,	O
conv4_3	O
,	O
and	O
conv5_3	O
and	O
initialize	O
the	O
norm	O
40	O
.	O

For	O
Fast	Task
R	Task
-	Task
CNN	Task
part	Task
,	O
we	O
extract	O
features	O
from	O
conv3_3	Method
,	O
conv5_3	Method
,	O
and	O
conv8	Method
for	O
each	O
region	O
proposal	O
and	O
concatenate	O
all	O
the	O
features	O
to	O
predict	O
class	O
scores	O
and	O
further	O
adjust	O
the	O
proposals	O
.	O

We	O
train	O
this	O
detector	O
from	O
COCO	Material
dataset	Material
with	O
input	O
image	O
and	O
have	O
achieved	O
35.5	O
mAP	Metric
on	O
the	O
COCO	Material
minival	Material
dataset	Material
,	O
with	O
only	O
10ms	O
processing	Metric
time	Metric
for	O
a	O
image	O
on	O
a	O
single	O
GPU	O
.	O

Finally	O
,	O
we	O
fine	O
-	O
tune	O
the	O
detector	O
on	O
car	O
,	O
people	O
,	O
and	O
bicycle	O
from	O
COCO	Material
dataset	Material
,	O
and	O
have	O
achieved	O
48.5	O
,	O
44.1	O
,	O
and	O
40.1	O
for	O
these	O
three	O
classes	O
on	O
COCO	Material
.	O

We	O
take	O
this	O
model	O
and	O
further	O
fine	O
-	O
tune	O
it	O
on	O
car	Task
,	Task
pedestrian	Task
,	O
and	O
cyclist	O
from	O
KITTI	Material
dataset	O
.	O

The	O
final	O
model	O
takes	O
about	O
30ms	O
to	O
process	O
a	O
image	O
.	O

To	O
increase	O
the	O
recall	Metric
of	O
the	O
detector	O
,	O
we	O
also	O
do	O
detection	Task
from	O
the	O
center	O
crop	O
of	O
the	O
image	O
besides	O
the	O
full	O
image	O
,	O
and	O
then	O
merge	O
the	O
detections	O
using	O
non	Method
-	Method
maximum	Method
suppression	Method
.	O

Tab	O
.	O

[	O
reference	O
]	O
shows	O
our	O
detector	O
’s	O
AP	Metric
(	O
2D	O
)	O
on	O
KITTI	Material
test	O
set	O
.	O

Our	O
detector	O
has	O
achieved	O
competitive	O
or	O
better	O
results	O
than	O
current	O
leading	O
players	O
on	O
KITTI	Material
leader	O
board	O
.	O

We	O
’	O
ve	O
also	O
reported	O
our	O
AP	Metric
(	O
2D	O
)	O
on	O
val	O
set	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

appendix	O
:	O
Bird	O
’s	O
Eye	O
View	O
PointNets	O
(	O
Sec	O
5.3	O
)	O
In	O
this	O
section	O
,	O
we	O
show	O
that	O
our	O
3D	Task
detection	Task
framework	O
can	O
also	O
be	O
extended	O
to	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
,	O
which	O
adds	O
another	O
orthogonal	Method
proposal	Method
source	Method
to	O
achieve	O
better	O
overall	O
3D	Task
detection	Task
performance	O
.	O

We	O
evaluate	O
the	O
results	O
of	O
car	Task
detection	Task
using	O
LiDAR	O
bird	O
’s	O
eye	O
view	O
only	O
proposals	O
+	O
point	O
net	O
(	O
Ours	O
(	O
BV	O
)	O
)	O
,	O
and	O
combine	O
frustum	Method
point	Method
net	Method
and	O
bird	O
’s	O
eye	O
view	O
point	O
net	O
using	O
3D	Method
non	Method
-	Method
maximum	Method
suppression	Method
(	Method
NMS	Method
)	Method
(	O
Ours	Method
(	Method
Frustum	Method
+	Method
BV	Method
)	Method
)	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
Proposal	O
Similar	O
to	O
MV3D	O
we	O
use	O
point	O
features	O
such	O
as	O
height	O
,	O
intensity	O
and	O
density	O
,	O
and	O
train	O
the	O
bird	O
’s	O
eye	O
view	O
2D	Method
proposal	Method
net	Method
using	O
the	O
standard	O
Faster	Method
-	Method
RCNN	Method
structure	Method
.	O

The	O
net	O
outputs	O
axis	O
-	O
aligned	O
2D	O
bounding	O
boxes	O
in	O
the	O
bird	O
’s	O
eye	O
view	O
.	O

In	O
detail	O
,	O
we	O
discretize	O
the	O
projected	O
point	O
clouds	O
into	O
2D	O
grids	O
with	O
resolution	O
of	O
meter	O
and	O
with	O
the	O
depth	O
and	O
width	O
range	O
meters	O
,	O
which	O
gives	O
us	O
the	O
input	O
size	O
.	O

For	O
each	O
cell	O
,	O
we	O
take	O
the	O
intensity	O
and	O
the	O
density	O
of	O
the	O
highest	O
point	O
and	O
divide	O
the	O
heights	O
into	O
bins	O
with	O
the	O
height	O
of	O
the	O
highest	O
point	O
in	O
each	O
bin	O
,	O
which	O
gives	O
us	O
channels	O
in	O
total	O
.	O

In	O
Faster	Task
R	Task
-	Task
CNN	Task
,	O
we	O
use	O
the	O
VGG	Method
-	Method
16	Method
with	O
anchor	O
scales	O
(	O
)	O
and	O
aspect	O
ratios	O
(	O
)	O
.	O

We	O
train	O
RPN	Method
and	O
Fast	Method
R	Method
-	Method
CNN	Method
together	O
using	O
the	O
approximate	Method
joint	Method
training	Method
.	O

To	O
combine	O
3D	Task
detection	Task
boxes	O
from	O
frustum	Method
PointNets	Method
and	O
the	O
bird	O
’s	O
eye	O
view	O
PointNets	O
,	O
we	O
use	O
3D	Method
NMS	Method
with	O
IoU	O
threshold	O
.	O

We	O
also	O
apply	O
a	O
weight	O
(	O
0.5	O
)	O
to	O
3D	O
boxes	O
from	O
BV	O
PointNets	O
since	O
it	O
is	O
a	O
weaker	O
detector	O
compared	O
with	O
our	O
frustum	Method
one	Method
.	O

paragraph	O
:	O
Bird	O
’s	O
Eye	O
View	O
(	O
BV	O
)	O
PointNets	O
Similar	O
to	O
Frustum	Method
PointNets	Method
that	O
take	O
point	O
cloud	O
in	O
frustum	O
,	O
segment	O
point	O
cloud	O
and	O
estimate	O
amodal	O
bounding	O
box	O
,	O
we	O
can	O
apply	O
PointNets	O
to	O
points	O
in	O
bird	O
’s	O
eye	O
view	O
regions	O
.	O

Since	O
bird	O
’s	O
eye	O
view	O
is	O
based	O
on	O
orthogonal	O
projection	O
,	O
the	O
3D	O
space	O
specified	O
by	O
a	O
BV	O
2D	O
box	O
is	O
a	O
3D	O
cuboid	O
(	O
cut	O
by	O
minimum	O
and	O
maximum	O
height	O
)	O
instead	O
of	O
a	O
frustum	O
.	O

paragraph	O
:	O
Results	O
Tab	O
.	O

[	O
reference	O
]	O
(	O
Ours	O
BV	O
)	O
shows	O
the	O
APs	O
we	O
get	O
by	O
using	O
bird	O
’s	O
eye	O
view	O
proposals	O
only	O
(	O
without	O
and	O
RGB	O
information	O
)	O
.	O

We	O
compare	O
with	O
two	O
previous	O
LiDAR	Method
only	Method
methods	Method
(	O
VeloFCN	Method
and	O
MV3D	Method
(	O
BV	Method
+	Method
FV	Method
)	Method
)	O
and	O
show	O
that	O
our	O
BV	Method
proposal	Method
based	Method
detector	Method
greatly	O
outperforms	O
VeloFCN	Method
on	O
all	O
cases	O
and	O
outperforms	O
MV3D	Method
(	O
BV	Method
+	Method
FV	Method
)	O
on	O
moderate	O
and	O
hard	O
cases	O
by	O
a	O
significant	O
margin	O
.	O

More	O
importantly	O
,	O
we	O
show	O
in	O
the	O
last	O
row	O
of	O
Tab	O
.	O

[	O
reference	O
]	O
that	O
bird	O
’s	O
eye	O
view	O
and	O
RGB	O
view	O
proposals	O
can	O
be	O
combined	O
to	O
achieve	O
an	O
even	O
better	O
performance	O
(	O
3.8	O
%	O
AP	Metric
improvement	O
on	O
hard	O
cases	O
)	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
gives	O
an	O
intuitive	O
explanation	O
of	O
why	O
bird	O
’s	O
eye	O
view	O
proposals	O
could	O
help	O
.	O

In	O
the	O
sample	O
frame	O
shown	O
:	O
while	O
our	O
2D	Method
detector	Method
misses	O
some	O
highly	O
occluded	O
cars	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
left	O
RGB	Material
image	Material
)	O
,	O
bird	O
’s	O
eye	O
view	O
based	O
RPN	Method
successfully	O
detects	O
them	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
:	O
blue	O
arrows	O
in	O
right	O
LiDAR	Material
image	Material
)	O
.	O

appendix	O
:	O
More	O
Experiments	O
(	O
Sec	O
5.2	O
)	O
subsection	O
:	O
Effects	O
of	O
PointNet	Method
Architectures	Method
Table	O
[	O
reference	O
]	O
compares	O
PointNet	O
(	O
v1	O
)	O
and	O
PointNet	Method
++	Method
(	Method
v2	Method
)	O
architectures	O
for	O
instance	Task
segmentation	Task
and	O
amodal	Task
box	Task
estimation	Task
.	O

The	O
v2	Method
model	Method
outperforms	O
v1	Method
model	Method
on	O
both	O
tasks	O
because	O
1	O
)	O
v2	Method
model	Method
learns	O
hierarchical	O
features	O
that	O
are	O
richer	O
and	O
more	O
generalizable	O
;	O
2	O
)	O
v2	Method
model	Method
uses	O
multi	Method
-	Method
scale	Method
feature	Method
learning	Method
that	O
adapts	O
to	O
varying	O
point	O
densities	O
.	O

Note	O
that	O
the	O
ours	O
(	O
v1	O
)	O
model	O
corresponds	O
to	O
first	O
row	O
of	O
Table	O
[	O
reference	O
]	O
while	O
the	O
ours	O
(	O
v2	O
)	O
links	O
to	O
the	O
last	O
row	O
.	O

subsection	O
:	O
Effects	O
of	O
Training	Metric
Data	Metric
Size	Metric
Recently	O
observed	O
linear	O
improvement	O
in	O
performance	O
of	O
deep	Method
learning	Method
models	Method
with	O
exponential	O
growth	O
of	O
data	O
set	O
size	O
.	O

In	O
our	O
Frustum	Method
PointNets	Method
we	O
observe	O
similar	O
trend	O
(	O
Fig	O
.	O

[	O
reference	O
]	O
)	O
.	O

This	O
trend	O
indicates	O
a	O
promising	O
performance	O
potential	O
of	O
our	O
methods	O
with	O
larger	O
datasets	O
.	O

We	O
train	O
three	O
separate	O
group	O
of	O
Frustum	Method
PointNets	Method
on	O
three	O
sets	O
of	O
training	O
data	O
and	O
then	O
evaluate	O
the	O
model	O
on	O
a	O
fixed	O
validation	O
set	O
(	O
1929	O
samples	O
)	O
.	O

The	O
three	O
data	O
points	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
represent	O
training	O
set	O
sizes	O
of	O
1388	O
,	O
2776	O
,	O
5552	O
samples	O
(	O
0.185x	O
,	O
0.371x	O
,	O
0.742x	O
of	O
the	O
entire	O
trainval	O
set	O
)	O
respectively	O
.	O

We	O
augment	O
the	O
training	O
data	O
such	O
that	O
the	O
total	O
amount	O
of	O
samples	O
are	O
the	O
same	O
for	O
each	O
of	O
the	O
three	O
cases	O
(	O
20x	O
,	O
10x	O
and	O
5x	O
augmentation	O
respectively	O
)	O
.	O

The	O
training	O
set	O
and	O
validation	O
set	O
are	O
chosen	O
such	O
that	O
they	O
do	O
n’t	O
share	O
frames	O
from	O
the	O
same	O
video	O
clips	O
.	O

subsection	O
:	O
Runtime	O
and	O
Model	Metric
Size	Metric
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
decomposed	O
runtime	Metric
cost	Metric
(	O
inference	Metric
time	Metric
)	O
for	O
our	O
frustum	Method
PointNets	Method
(	O
v1	O
and	O
v2	O
)	O
.	O

The	O
evaluation	O
is	O
based	O
on	O
TensorFlow	Method
with	O
a	O
NVIDIA	Method
GTX	Method
1080	Method
and	O
a	O
single	O
CPU	O
core	O
.	O

While	O
for	O
v1	Method
model	Method
frustum	Method
proposal	Method
(	O
with	O
CNN	Method
and	Method
backprojection	Method
)	O
takes	O
the	O
majority	O
time	O
,	O
for	O
v2	Method
model	Method
since	O
a	O
PointNet	Method
++	Method
model	Method
with	O
multi	Method
-	Method
scale	Method
grouping	Method
is	O
used	O
,	O
computation	O
bottleneck	O
shifts	O
to	O
instance	Task
segmentation	Task
.	O

Note	O
that	O
we	O
merge	O
batch	Method
normalization	Method
and	O
FC	Method
/	Method
convolution	Method
layers	Method
for	O
faster	O
inference	Task
(	O
since	O
they	O
are	O
both	O
linear	O
operation	O
with	O
multiply	O
and	O
sum	O
)	O
,	O
which	O
results	O
in	O
close	O
to	O
50	O
%	O
speedup	O
for	O
inference	Task
.	O

CNN	Method
model	Method
has	O
size	O
28	O
MB	O
.	O

v1	O
PointNets	Method
have	O
size	O
19	O
MB	O
.	O

v2	O
PointNets	Method
have	O
size	O
22	O
MB	O
.	O

The	O
total	O
size	O
is	O
therefore	O
47	O
MB	O
for	O
v1	Method
model	Method
and	O
50	O
MB	O
for	O
v2	Method
model	Method
.	O

appendix	O
:	O
Visualizations	Task
for	O
SUN	Material
-	Material
RGBD	Material
(	O
Sec	O
5.1	O
)	O
In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
visualize	O
some	O
representative	O
detection	Task
results	O
on	O
SUN	Material
-	Material
RGBD	Material
data	O
.	O

We	O
can	O
see	O
that	O
compared	O
with	O
KITTI	Material
LiDAR	O
data	O
,	O
depth	Material
images	Material
can	O
be	O
popped	O
up	O
to	O
much	O
more	O
dense	O
point	O
clouds	O
.	O

However	O
even	O
with	O
such	O
dense	O
point	O
cloud	O
,	O
strong	O
occlusions	O
of	O
indoor	O
objects	O
as	O
well	O
as	O
the	O
tight	O
arrangement	O
present	O
new	O
challenges	O
for	O
detection	Task
in	O
indoor	O
scenes	O
.	O

In	O
Fig	O
.	O

[	O
reference	O
]	O
we	O
report	O
the	O
3D	O
AP	Metric
curves	O
of	O
our	O
Frustum	Method
PointNets	Method
on	O
SUN	Material
-	Material
RGBD	Material
val	O
set	O
.	O

2D	Task
detection	Task
APs	O
of	O
our	O
RGB	Method
detector	Method
are	O
also	O
provided	O
in	O
Tab	O
.	O

[	O
reference	O
]	O
for	O
reference	O
.	O

