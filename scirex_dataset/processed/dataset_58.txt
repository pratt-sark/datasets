document	O
:	O
Stochastic	Method
Pooling	Method
for	O
Regularization	Task
of	O
Deep	Method
Convolutional	Method
Neural	Method
Networks	Method
We	O
introduce	O
a	O
simple	O
and	O
effective	O
method	O
for	O
regularizing	Method
large	Method
convolutional	Method
neural	Method
networks	Method
.	O

We	O
replace	O
the	O
conventional	O
deterministic	O
pooling	Method
operations	O
with	O
a	O
stochastic	Method
procedure	Method
,	O
randomly	O
picking	O
the	O
activation	O
within	O
each	O
pooling	Method
region	O
according	O
to	O
a	O
multinomial	O
distribution	O
,	O
given	O
by	O
the	O
activities	O
within	O
the	O
pooling	Method
region	O
.	O

The	O
approach	O
is	O
hyper	O
-	O
parameter	O
free	O
and	O
can	O
be	O
combined	O
with	O
other	O
regularization	Method
approaches	Method
,	O
such	O
as	O
dropout	Method
and	O
data	Method
augmentation	Method
.	O

We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
four	O
image	O
datasets	O
,	O
relative	O
to	O
other	O
approaches	O
that	O
do	O
not	O
utilize	O
data	Method
augmentation	Method
.	O

section	O
:	O
Introduction	O
Neural	Method
network	Method
models	Method
are	O
prone	O
to	O
over	O
-	O
fitting	O
due	O
to	O
their	O
high	O
capacity	O
.	O

A	O
range	O
of	O
regularization	Method
techniques	Method
are	O
used	O
to	O
prevent	O
this	O
,	O
such	O
as	O
weight	Method
decay	Method
,	O
weight	Method
tying	Method
and	O
the	O
augmentation	O
of	O
the	O
training	O
set	O
with	O
transformed	O
copies	O
.	O

These	O
allow	O
the	O
training	O
of	O
larger	O
capacity	Method
models	Method
than	O
would	O
otherwise	O
be	O
possible	O
,	O
which	O
yield	O
superior	O
test	O
performance	O
compared	O
to	O
smaller	O
un	Method
-	Method
regularized	Method
models	Method
.	O

Dropout	Method
,	O
recently	O
proposed	O
by	O
Hinton	O
et	O
al	O
.	O

[	O
]	O
,	O
is	O
another	O
regularization	Method
approach	Method
that	O
stochastically	O
sets	O
half	O
the	O
activations	O
within	O
a	O
layer	O
to	O
zero	O
for	O
each	O
training	O
sample	O
during	O
training	O
.	O

It	O
has	O
been	O
shown	O
to	O
deliver	O
significant	O
gains	O
in	O
performance	O
across	O
a	O
wide	O
range	O
of	O
problems	O
,	O
although	O
the	O
reasons	O
for	O
its	O
efficacy	O
are	O
not	O
yet	O
fully	O
understood	O
.	O

A	O
drawback	O
to	O
dropout	Method
is	O
that	O
it	O
does	O
not	O
seem	O
to	O
have	O
the	O
same	O
benefits	O
for	O
convolutional	Method
layers	Method
,	O
which	O
are	O
common	O
in	O
many	O
networks	O
designed	O
for	O
vision	Task
tasks	Task
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
type	O
of	O
regularization	Method
for	O
convolutional	Method
layers	Method
that	O
enables	O
the	O
training	O
of	O
larger	Method
models	Method
without	O
over	Task
-	Task
fitting	Task
,	O
and	O
produces	O
superior	O
performance	O
on	O
recognition	Task
tasks	Task
.	O

The	O
key	O
idea	O
is	O
to	O
make	O
the	O
pooling	Method
that	O
occurs	O
in	O
each	O
convolutional	Method
layer	Method
a	O
stochastic	Method
process	Method
.	O

Conventional	O
forms	O
of	O
pooling	Method
such	O
as	O
average	Method
and	O
max	Method
are	O
deterministic	O
,	O
the	O
latter	O
selecting	O
the	O
largest	O
activation	O
in	O
each	O
pooling	Method
region	O
.	O

In	O
our	O
stochastic	Method
pooling	Method
,	O
the	O
selected	O
activation	O
is	O
drawn	O
from	O
a	O
multinomial	O
distribution	O
formed	O
by	O
the	O
activations	O
within	O
the	O
pooling	Method
region	O
.	O

An	O
alternate	O
view	O
of	O
stochastic	Method
pooling	Method
is	O
that	O
it	O
is	O
equivalent	O
to	O
standard	O
max	Method
pooling	Method
but	O
with	O
many	O
copies	O
of	O
an	O
input	O
image	O
,	O
each	O
having	O
small	O
local	O
deformations	O
.	O

This	O
is	O
similar	O
to	O
explicit	O
elastic	O
deformations	O
of	O
the	O
input	O
images	O
,	O
which	O
delivers	O
excellent	O
MNIST	Metric
performance	Metric
.	O

Other	O
types	O
of	O
data	Task
augmentation	Task
,	O
such	O
as	O
flipping	O
and	O
cropping	Task
differ	O
in	O
that	O
they	O
are	O
global	Task
image	Task
transformations	Task
.	O

Furthermore	O
,	O
using	O
stochastic	Method
pooling	Method
in	O
a	O
multi	Method
-	Method
layer	Method
model	Method
gives	O
an	O
exponential	O
number	O
of	O
deformations	O
since	O
the	O
selections	O
in	O
higher	O
layers	O
are	O
independent	O
of	O
those	O
below	O
.	O

section	O
:	O
Review	O
of	O
Convolutional	Method
Networks	Method
Our	O
stochastic	O
pooling	Method
scheme	O
is	O
designed	O
for	O
use	O
in	O
a	O
standard	O
convolutional	Method
neural	Method
network	Method
architecture	Method
.	O

We	O
first	O
review	O
this	O
model	O
,	O
along	O
with	O
conventional	O
pooling	Method
schemes	O
,	O
before	O
introducing	O
our	O
novel	O
stochastic	O
pooling	Method
approach	O
.	O

A	O
classical	O
convolutional	Method
network	Method
is	O
composed	O
of	O
alternating	O
layers	O
of	O
convolution	O
and	O
pooling	Method
(	O
i.e.	O
subsampling	Method
)	O
.	O

The	O
aim	O
of	O
the	O
first	O
convolutional	Method
layer	Method
is	O
to	O
extract	O
patterns	O
found	O
within	O
local	O
regions	O
of	O
the	O
input	O
images	O
that	O
are	O
common	O
throughout	O
the	O
dataset	O
.	O

This	O
is	O
done	O
by	O
convolving	O
a	O
template	Method
or	Method
filter	Method
over	O
the	O
input	O
image	O
pixels	O
,	O
computing	O
the	O
inner	O
product	O
of	O
the	O
template	O
at	O
every	O
location	O
in	O
the	O
image	O
and	O
outputting	O
this	O
as	O
a	O
feature	O
map	O
,	O
for	O
each	O
filter	O
in	O
the	O
layer	O
.	O

This	O
output	O
is	O
a	O
measure	O
of	O
how	O
well	O
the	O
template	O
matches	O
each	O
portion	O
of	O
the	O
image	O
.	O

A	O
non	O
-	O
linear	O
function	O
is	O
then	O
applied	O
element	O
-	O
wise	O
to	O
each	O
feature	O
map	O
:	O
.	O

The	O
resulting	O
activations	O
are	O
then	O
passed	O
to	O
the	O
pooling	Method
layer	O
.	O

This	O
aggregates	O
the	O
information	O
within	O
a	O
set	O
of	O
small	O
local	O
regions	O
,	O
,	O
producing	O
a	O
pooled	Method
feature	Method
map	Method
(	O
of	O
smaller	O
size	O
)	O
as	O
output	O
.	O

Denoting	O
the	O
aggregation	O
function	O
as	O
,	O
for	O
each	O
feature	O
map	O
we	O
have	O
:	O
where	O
is	O
pooling	Method
region	O
in	O
feature	O
map	O
and	O
is	O
the	O
index	O
of	O
each	O
element	O
within	O
it	O
.	O

The	O
motivation	O
behind	O
pooling	Method
is	O
that	O
the	O
activations	O
in	O
the	O
pooled	Method
map	Method
are	O
less	O
sensitive	O
to	O
the	O
precise	O
locations	O
of	O
structures	O
within	O
the	O
image	O
than	O
the	O
original	O
feature	O
map	O
.	O

In	O
a	O
multi	Method
-	Method
layer	Method
model	Method
,	O
the	O
convolutional	Method
layers	Method
,	O
which	O
take	O
the	O
pooled	O
maps	O
as	O
input	O
,	O
can	O
thus	O
extract	O
features	O
that	O
are	O
increasingly	O
invariant	O
to	O
local	O
transformations	O
of	O
the	O
input	O
image	O
.	O

This	O
is	O
important	O
for	O
classification	Task
tasks	Task
,	O
since	O
these	O
transformations	O
obfuscate	O
the	O
object	O
identity	O
.	O

A	O
range	O
of	O
functions	O
can	O
be	O
used	O
for	O
,	O
with	O
and	O
logistic	Method
functions	Method
being	O
popular	O
choices	O
.	O

In	O
this	O
is	O
paper	O
we	O
use	O
a	O
linear	Method
rectification	Method
function	Method
as	O
the	O
non	O
-	O
linearity	O
.	O

In	O
general	O
,	O
this	O
has	O
been	O
shown	O
to	O
have	O
significant	O
benefits	O
over	O
or	O
logistic	O
functions	O
.	O

However	O
,	O
it	O
is	O
especially	O
suited	O
to	O
our	O
pooling	Method
mechanism	Method
since	O
:	O
(	O
i	O
)	O
our	O
formulation	O
involves	O
the	O
non	O
-	O
negativity	O
of	O
elements	O
in	O
the	O
pooling	Method
regions	O
and	O
(	O
ii	O
)	O
the	O
clipping	O
of	O
negative	O
responses	O
introduces	O
zeros	O
into	O
the	O
pooling	Method
regions	O
,	O
ensuring	O
that	O
the	O
stochastic	Method
sampling	Method
is	O
selecting	O
from	O
a	O
few	O
specific	O
locations	O
(	O
those	O
with	O
strong	O
responses	O
)	O
,	O
rather	O
than	O
all	O
possible	O
locations	O
in	O
the	O
region	O
.	O

There	O
are	O
two	O
conventional	O
choices	O
for	O
:	O
average	Method
and	O
max	Method
.	O

The	O
former	O
takes	O
the	O
arithmetic	O
mean	O
of	O
the	O
elements	O
in	O
each	O
pooling	Method
region	O
:	O
while	O
the	O
max	Method
operation	O
selects	O
the	O
largest	O
element	O
:	O
Both	O
types	O
of	O
pooling	Method
have	O
drawbacks	O
when	O
training	O
deep	Method
convolutional	Method
networks	Method
.	O

In	O
average	Method
pooling	Method
,	O
all	O
elements	O
in	O
a	O
pooling	Method
region	O
are	O
considered	O
,	O
even	O
if	O
many	O
have	O
low	O
magnitude	O
.	O

When	O
combined	O
with	O
linear	Method
rectification	Method
non	Method
-	Method
linearities	Method
,	O
this	O
has	O
the	O
effect	O
of	O
down	O
-	O
weighting	O
strong	O
activations	O
since	O
many	O
zero	O
elements	O
are	O
included	O
in	O
the	O
average	Method
.	O

Even	O
worse	O
,	O
with	O
non	O
-	O
linearities	O
,	O
strong	O
positive	O
and	O
negative	O
activations	O
can	O
cancel	O
each	O
other	O
out	O
,	O
leading	O
to	O
small	O
pooled	O
responses	O
.	O

While	O
max	Method
pooling	Method
does	O
not	O
suffer	O
from	O
these	O
drawbacks	O
,	O
we	O
find	O
it	O
easily	O
overfits	O
the	O
training	O
set	O
in	O
practice	O
,	O
making	O
it	O
hard	O
to	O
generalize	O
well	O
to	O
test	O
examples	O
.	O

Our	O
proposed	O
pooling	Method
scheme	O
has	O
the	O
advantages	O
of	O
max	Method
pooling	Method
but	O
its	O
stochastic	O
nature	O
helps	O
prevent	O
over	Task
-	Task
fitting	Task
.	O

section	O
:	O
Stochastic	Method
Pooling	Method
In	O
stochastic	Method
pooling	Method
,	O
we	O
select	O
the	O
pooled	O
map	O
response	O
by	O
sampling	O
from	O
a	O
multinomial	O
distribution	O
formed	O
from	O
the	O
activations	O
of	O
each	O
pooling	Method
region	O
.	O

More	O
precisely	O
,	O
we	O
first	O
compute	O
the	O
probabilities	O
for	O
each	O
region	O
by	O
normalizing	O
the	O
activations	O
within	O
the	O
region	O
:	O
We	O
then	O
sample	O
from	O
the	O
multinomial	Method
distribution	Method
based	O
on	O
to	O
pick	O
a	O
location	O
within	O
the	O
region	O
.	O

The	O
pooled	Method
activation	Method
is	O
then	O
simply	O
:	O
The	O
procedure	O
is	O
illustrated	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

The	O
samples	O
for	O
each	O
pooling	Method
region	O
in	O
each	O
layer	O
for	O
each	O
training	O
example	O
are	O
drawn	O
independently	O
to	O
one	O
another	O
.	O

When	O
back	O
-	O
propagating	O
through	O
the	O
network	O
this	O
same	O
selected	O
location	O
is	O
used	O
to	O
direct	O
the	O
gradient	O
back	O
through	O
the	O
pooling	Method
region	O
,	O
analogous	O
to	O
back	Method
-	Method
propagation	Method
with	O
max	Method
pooling	Method
.	O

Max	Method
pooling	Method
only	O
captures	O
the	O
strongest	O
activation	O
of	O
the	O
filter	O
template	O
with	O
the	O
input	O
for	O
each	O
region	O
.	O

However	O
,	O
there	O
may	O
be	O
additional	O
activations	O
in	O
the	O
same	O
pooling	Method
region	O
that	O
should	O
be	O
taken	O
into	O
account	O
when	O
passing	O
information	O
up	O
the	O
network	O
and	O
stochastic	Method
pooling	Method
ensures	O
that	O
these	O
non	O
-	O
maximal	O
activations	O
will	O
also	O
be	O
utilized	O
.	O

subsection	O
:	O
Probabilistic	Method
Weighting	Method
at	O
Test	O
Time	O
Using	O
stochastic	Method
pooling	Method
at	O
test	O
time	O
introduces	O
noise	O
into	O
the	O
network	O
’s	O
predictions	O
which	O
we	O
found	O
to	O
degrade	O
performance	O
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Instead	O
,	O
we	O
use	O
a	O
probabilistic	Method
form	Method
of	Method
averaging	Method
.	O

In	O
this	O
,	O
the	O
activations	O
in	O
each	O
region	O
are	O
weighted	O
by	O
the	O
probability	O
(	O
see	O
Eqn	O
.	O

[	O
reference	O
]	O
)	O
and	O
summed	O
:	O
This	O
differs	O
from	O
standard	O
average	Method
pooling	Method
because	O
each	O
element	O
has	O
a	O
potentially	O
different	O
weighting	O
and	O
the	O
denominator	O
is	O
the	O
sum	O
of	O
activations	O
,	O
rather	O
than	O
the	O
pooling	Method
region	O
size	O
.	O

In	O
practice	O
,	O
using	O
conventional	O
average	Method
(	O
or	O
sum	O
)	O
pooling	Method
results	O
in	O
a	O
huge	O
performance	O
drop	O
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
.	O

Our	O
probabilistic	Method
weighting	Method
can	O
be	O
viewed	O
as	O
a	O
form	O
of	O
model	Method
averaging	Method
in	O
which	O
each	O
setting	O
of	O
the	O
locations	O
in	O
the	O
pooling	Method
regions	O
defines	O
a	O
new	O
model	O
.	O

At	O
training	O
time	O
,	O
sampling	O
to	O
get	O
new	O
locations	O
produces	O
a	O
new	O
model	O
since	O
the	O
connection	O
structure	O
throughout	O
the	O
network	O
is	O
modified	O
.	O

At	O
test	O
time	O
,	O
using	O
the	O
probabilities	O
instead	O
of	O
sampling	Method
,	O
we	O
effectively	O
get	O
an	O
estimate	O
of	O
averaging	O
over	O
all	O
of	O
these	O
possible	O
models	O
without	O
having	O
to	O
instantiate	O
them	O
.	O

Given	O
a	O
network	Method
architecture	Method
with	O
different	O
pooling	Method
regions	O
,	O
each	O
of	O
size	O
,	O
the	O
number	O
of	O
possible	O
models	O
is	O
where	O
can	O
be	O
in	O
the	O
-	O
range	O
and	O
is	O
typically	O
4	O
,	O
9	O
,	O
or	O
16	O
for	O
example	O
(	O
corresponding	O
to	O
,	O
or	O
pooling	Method
regions	O
)	O
.	O

This	O
is	O
a	O
significantly	O
larger	O
number	O
than	O
the	O
model	Method
averaging	Method
that	O
occurs	O
in	O
dropout	Task
,	O
where	O
always	O
(	O
since	O
an	O
activation	O
is	O
either	O
present	O
or	O
not	O
)	O
.	O

In	O
Section	O
[	O
reference	O
]	O
we	O
confirm	O
that	O
using	O
this	O
probability	Method
weighting	Method
achieves	O
similar	O
performance	O
compared	O
to	O
using	O
a	O
large	O
number	O
of	O
model	O
instantiations	O
,	O
while	O
requiring	O
only	O
one	O
pass	O
through	O
the	O
network	O
.	O

Using	O
the	O
probabilities	O
for	O
sampling	O
at	O
training	O
time	O
and	O
for	O
weighting	O
the	O
activations	O
at	O
test	O
time	O
leads	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
many	O
common	O
benchmarks	O
,	O
as	O
we	O
now	O
demonstrate	O
.	O

section	O
:	O
Experiments	O
subsection	O
:	O
Overview	O
We	O
compare	O
our	O
method	O
to	O
average	Method
and	O
max	Method
pooling	Method
on	O
a	O
variety	O
of	O
image	Task
classification	Task
tasks	Task
.	O

In	O
all	O
experiments	O
we	O
use	O
mini	Method
-	Method
batch	Method
gradient	Method
descent	Method
with	O
momentum	Method
to	O
optimize	O
the	O
cross	O
entropy	O
between	O
our	O
network	O
’s	O
prediction	O
of	O
the	O
class	O
and	O
the	O
ground	O
truth	O
labels	O
.	O

For	O
a	O
given	O
parameter	O
at	O
time	O
the	O
weight	O
updates	O
added	O
to	O
the	O
parameters	O
,	O
are	O
where	O
is	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
with	O
respect	O
to	O
that	O
parameter	O
at	O
time	O
averaged	O
over	O
the	O
batch	O
and	O
is	O
a	O
learning	O
rate	O
set	O
by	O
hand	O
.	O

All	O
experiments	O
were	O
conducted	O
using	O
an	O
extremely	O
efficient	O
C	Method
++	Method
GPU	Method
convolution	Method
library	Method
wrapped	O
in	O
MATLAB	Method
using	O
GPUmat	Method
,	O
which	O
allowed	O
for	O
rapid	O
development	O
and	O
experimentation	O
.	O

We	O
begin	O
with	O
the	O
same	O
network	O
layout	O
as	O
in	O
Hinton	O
et	O
al	O
.	O

’s	O
dropout	O
work	O
,	O
which	O
has	O
convolutional	Method
layers	Method
with	O
5x5	O
filters	O
and	O
feature	Method
maps	Method
per	O
layer	O
with	O
rectified	Method
linear	Method
units	Method
as	O
their	O
outputs	O
.	O

We	O
use	O
this	O
same	O
model	O
and	O
train	O
for	O
280	O
epochs	O
in	O
all	O
experiments	O
aside	O
from	O
one	O
additional	O
model	O
in	O
Section	O
[	O
reference	O
]	O
that	O
has	O
128	O
feature	O
maps	O
in	O
layer	O
3	O
and	O
is	O
trained	O
for	O
500	O
epochs	O
.	O

Unless	O
otherwise	O
specified	O
we	O
use	O
pooling	Method
with	O
stride	O
(	O
i.e.	O
neighboring	O
pooling	Method
regions	O
overlap	O
by	O
element	O
along	O
the	O
borders	O
)	O
for	O
each	O
of	O
the	O
pooling	Method
layers	O
.	O

Additionally	O
,	O
after	O
each	O
pooling	Method
layer	O
there	O
is	O
a	O
response	Method
normalization	Method
layer	Method
(	O
as	O
in	O
)	O
,	O
which	O
normalizes	O
the	O
pooling	Method
outputs	O
at	O
each	O
location	O
over	O
a	O
subset	O
of	O
neighboring	O
feature	O
maps	O
.	O

This	O
typically	O
helps	O
training	O
by	O
suppressing	O
extremely	O
large	O
outputs	O
allowed	O
by	O
the	O
rectified	O
linear	O
units	O
as	O
well	O
as	O
helps	O
neighboring	O
features	O
communicate	O
.	O

Finally	O
,	O
we	O
use	O
a	O
single	O
fully	Method
-	Method
connected	Method
layer	Method
with	O
soft	O
-	O
max	Method
outputs	O
to	O
produce	O
the	O
network	O
’s	O
class	O
predictions	O
.	O

We	O
applied	O
this	O
model	O
to	O
four	O
different	O
datasets	O
:	O
MNIST	O
,	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
and	O
Street	Material
View	Material
House	Material
Numbers	Material
(	O
SVHN	Material
)	O
,	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
for	O
examples	O
images	O
.	O

subsection	O
:	O
CIFAR	Material
-	Material
10	Material
We	O
begin	O
our	O
experiments	O
with	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
where	O
convolutional	Method
networks	Method
and	O
methods	O
such	O
as	O
dropout	Method
are	O
known	O
to	O
work	O
well	O
.	O

This	O
dataset	O
is	O
composed	O
of	O
10	O
classes	O
of	O
natural	O
images	O
with	O
50	O
,	O
000	O
training	O
examples	O
in	O
total	O
,	O
5	O
,	O
000	O
per	O
class	O
.	O

Each	O
image	O
is	O
an	O
RGB	O
image	O
of	O
size	O
32x32	O
taken	O
from	O
the	O
tiny	O
images	O
dataset	O
and	O
labeled	O
by	O
hand	O
.	O

For	O
this	O
dataset	O
we	O
scale	O
to	O
[	O
0	O
,	O
1	O
]	O
and	O
follow	O
Hinton	O
et	O
al	O
.	O

’s	O
approach	O
of	O
subtracting	O
the	O
per	O
-	O
pixel	O
mean	O
computed	O
over	O
the	O
dataset	O
from	O
each	O
image	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
f	O
)	O
.	O

Cross	O
-	O
validating	O
with	O
a	O
set	O
of	O
5	O
,	O
000	O
CIFAR	Material
-	Material
10	Material
training	O
images	O
,	O
we	O
found	O
a	O
good	O
value	O
for	O
the	O
learning	Metric
rate	Metric
to	O
be	O
for	O
convolutional	Method
layers	Method
and	O
for	O
the	O
final	O
softmax	Method
output	Method
layer	Method
.	O

These	O
rates	O
were	O
annealed	O
linearly	O
throughout	O
training	O
to	O
of	O
their	O
original	O
values	O
.	O

Additionally	O
,	O
we	O
found	O
a	O
small	O
weight	O
decay	O
of	O
to	O
be	O
optimal	O
and	O
was	O
applied	O
to	O
all	O
layers	O
.	O

These	O
hyper	O
-	O
parameter	O
settings	O
found	O
through	O
cross	Metric
-	Metric
validation	Metric
were	O
used	O
for	O
all	O
other	O
datasets	O
in	O
our	O
experiments	O
.	O

Using	O
the	O
same	O
network	Method
architecture	Method
described	O
above	O
,	O
we	O
trained	O
three	O
models	O
using	O
average	Method
,	O
max	Method
and	O
stochastic	Method
pooling	Method
respectively	O
and	O
compare	O
their	O
performance	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
the	O
progression	O
of	O
train	O
and	O
test	O
errors	O
over	O
280	O
training	O
epochs	O
.	O

Stochastic	O
pooling	Method
avoids	O
over	Task
-	Task
fitting	Task
,	O
unlike	O
average	Method
and	O
max	Method
pooling	Method
,	O
and	O
produces	O
less	O
test	O
errors	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
test	O
performance	O
of	O
the	O
three	O
pooling	Method
approaches	O
to	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
CIFAR	Material
-	Material
10	Material
which	O
uses	O
no	O
data	Method
augmentation	Method
but	O
adds	O
dropout	Method
on	O
an	O
additional	O
locally	Method
connected	Method
layer	Method
.	O

Stochastic	O
pooling	Method
surpasses	O
this	O
result	O
by	O
0.47	O
%	O
using	O
the	O
same	O
architecture	O
but	O
without	O
requiring	O
the	O
locally	O
connected	O
layer	O
.	O

To	O
determine	O
the	O
effect	O
of	O
the	O
pooling	Method
region	O
size	O
on	O
the	O
behavior	O
of	O
the	O
system	O
with	O
stochastic	Method
pooling	Method
,	O
we	O
compare	O
the	O
CIFAR	Metric
-	Metric
10	Metric
train	Metric
and	O
test	Metric
set	Metric
performance	Metric
for	O
5x5	O
,	O
4x4	O
,	O
3x3	O
,	O
and	O
2x2	O
pooling	Method
sizes	O
throughout	O
the	O
network	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

The	O
optimal	O
size	O
appears	O
to	O
be	O
3x3	O
,	O
with	O
smaller	O
regions	O
over	O
-	O
fitting	O
and	O
larger	O
regions	O
possibly	O
being	O
too	O
noisy	O
during	O
training	O
.	O

At	O
all	O
sizes	O
the	O
stochastic	Method
pooling	Method
outperforms	O
both	O
max	Method
and	O
average	Method
pooling	Method
.	O

subsection	O
:	O
MNIST	O
The	O
MNIST	Task
digit	Task
classification	Task
task	Task
is	O
composed	O
of	O
28x28	O
images	O
of	O
the	O
10	O
handwritten	O
digits	O
.	O

There	O
are	O
60	O
,	O
000	O
training	O
images	O
with	O
10	O
,	O
000	O
test	O
images	O
in	O
this	O
benchmark	O
.	O

The	O
images	O
are	O
scaled	O
to	O
[	O
0	O
,	O
1	O
]	O
and	O
we	O
do	O
not	O
perform	O
any	O
other	O
pre	Method
-	Method
processing	Method
.	O

During	O
training	Task
,	O
the	O
error	Metric
using	O
both	O
stochastic	Method
pooling	Method
and	O
max	Method
pooling	Method
dropped	O
quickly	O
,	O
but	O
the	O
latter	O
completely	O
overfit	O
the	O
training	O
data	O
.	O

Weight	Method
decay	Method
prevented	O
average	Method
pooling	Method
from	O
over	Task
-	Task
fitting	Task
,	O
but	O
had	O
an	O
inferior	O
performance	O
to	O
the	O
other	O
two	O
methods	O
.	O

Table	O
[	O
reference	O
]	O
compares	O
the	O
three	O
pooling	Method
approaches	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
MNIST	O
,	O
which	O
also	O
utilize	O
convolutional	Method
networks	Method
.	O

Stochastic	O
pooling	Method
outperforms	O
all	O
other	O
methods	O
that	O
do	O
not	O
use	O
data	Method
augmentation	Method
methods	Method
such	O
as	O
jittering	O
or	O
elastic	O
distortions	O
.	O

The	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	Method
model	Method
approach	Method
by	O
CiresÌ§an	O
et	O
al	O
.	O

[	O
]	O
uses	O
elastic	O
distortions	O
to	O
augment	O
the	O
original	O
training	O
set	O
.	O

As	O
stochastic	Method
pooling	Method
is	O
a	O
different	O
type	O
of	O
regularization	Method
,	O
it	O
could	O
be	O
combined	O
with	O
data	Task
augmentation	Task
to	O
further	O
improve	O
performance	O
.	O

subsection	O
:	O
CIFAR	Material
-	Material
100	Material
The	O
CIFAR	Material
-	Material
100	Material
dataset	O
is	O
another	O
subset	O
of	O
the	O
tiny	O
images	O
dataset	O
,	O
but	O
with	O
100	O
classes	O
.	O

There	O
are	O
50	O
,	O
000	O
training	O
examples	O
in	O
total	O
(	O
500	O
per	O
class	O
)	O
and	O
10	O
,	O
000	O
test	O
examples	O
.	O

As	O
with	O
the	O
CIFAR	Material
-	Material
10	Material
,	O
we	O
scale	O
to	O
[	O
0	O
,	O
1	O
]	O
and	O
subtract	O
the	O
per	O
-	O
pixel	O
mean	O
from	O
each	O
image	O
as	O
shown	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
h	O
)	O
.	O

Due	O
to	O
the	O
limited	O
number	O
of	O
training	O
examples	O
per	O
class	O
,	O
typical	O
pooling	Method
methods	O
used	O
in	O
convolutional	Method
networks	Method
do	O
not	O
perform	O
well	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Stochastic	O
pooling	Method
outperforms	O
these	O
methods	O
by	O
preventing	O
over	Task
-	Task
fitting	Task
and	O
surpasses	O
what	O
we	O
believe	O
to	O
be	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
by	O
%	O
.	O

subsection	O
:	O
Street	Material
View	Material
House	Material
Numbers	Material
The	O
Street	Material
View	Material
House	Material
Numbers	Material
(	O
SVHN	Material
)	O
dataset	O
is	O
composed	O
of	O
604	O
,	O
388	O
images	O
(	O
using	O
both	O
the	O
difficult	O
training	O
set	O
and	O
simpler	O
extra	O
set	O
)	O
and	O
26	O
,	O
032	O
test	O
images	O
.	O

The	O
goal	O
of	O
this	O
task	O
is	O
to	O
classify	O
the	O
digit	O
in	O
the	O
center	O
of	O
each	O
cropped	O
32x32	O
color	O
image	O
.	O

This	O
is	O
a	O
difficult	O
real	Task
world	Task
problem	Task
since	O
multiple	O
digits	O
may	O
be	O
visible	O
within	O
each	O
image	O
.	O

The	O
practical	O
application	O
of	O
this	O
is	O
to	O
classify	Task
house	Task
numbers	Task
throughout	O
Google	O
’s	O
street	O
view	O
database	O
of	O
images	O
.	O

We	O
found	O
that	O
subtracting	O
the	O
per	O
-	O
pixel	O
mean	O
from	O
each	O
image	O
did	O
not	O
really	O
modify	O
the	O
statistics	O
of	O
the	O
images	O
(	O
see	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
)	O
)	O
and	O
left	O
large	O
variations	O
of	O
brightness	O
and	O
color	O
that	O
could	O
make	O
classification	Task
more	O
difficult	O
.	O

Instead	O
,	O
we	O
utilized	O
local	Method
contrast	Method
normalization	Method
(	O
as	O
in	O
)	O
on	O
each	O
of	O
the	O
three	O
RGB	O
channels	O
to	O
pre	O
-	O
process	O
the	O
images	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
c	O
)	O
.	O

This	O
normalized	O
the	O
brightness	O
and	O
color	O
variations	O
and	O
helped	O
training	O
proceed	O
quickly	O
on	O
this	O
relatively	O
large	O
dataset	O
.	O

Despite	O
having	O
significant	O
amounts	O
of	O
training	O
data	O
,	O
a	O
large	O
convolutional	Method
network	Method
can	O
still	O
overfit	O
.	O

For	O
this	O
dataset	O
,	O
we	O
train	O
an	O
additional	O
model	O
for	O
500	O
epochs	O
with	O
64	O
,	O
64	O
and	O
128	O
feature	O
maps	O
in	O
layers	O
1	O
,	O
2	O
and	O
3	O
respectively	O
.	O

Our	O
stochastic	Method
pooling	Method
helps	O
to	O
prevent	O
overfitting	O
even	O
in	O
this	O
large	O
model	O
(	O
denoted	O
64	O
-	O
64	O
-	O
128	O
in	O
Table	O
[	O
reference	O
]	O
)	O
,	O
despite	O
training	O
for	O
a	O
long	O
time	O
.	O

The	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
this	O
dataset	O
is	O
the	O
multi	Method
-	Method
stage	Method
convolutional	Method
network	Method
of	O
Sermanet	O
et	O
al	O
.	O

[	O
]	O
,	O
but	O
stochastic	Method
pooling	Method
beats	O
this	O
by	O
%	O
(	O
relative	O
gain	O
of	O
)	O
.	O

subsection	O
:	O
Reduced	O
Training	Metric
Set	Metric
Size	Metric
To	O
further	O
illustrate	O
the	O
ability	O
of	O
stochastic	Method
pooling	Method
to	O
prevent	O
over	Task
-	Task
fitting	Task
,	O
we	O
reduced	O
the	O
training	Metric
set	Metric
size	Metric
on	O
MINST	O
and	O
CIFAR	Material
-	Material
10	Material
datasets	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
shows	O
test	O
performance	O
when	O
training	O
on	O
a	O
random	O
selection	O
of	O
only	O
1000	O
,	O
2000	O
,	O
3000	O
,	O
5000	O
,	O
10000	O
,	O
half	O
,	O
or	O
the	O
full	O
training	O
set	O
.	O

In	O
most	O
cases	O
,	O
stochastic	Method
pooling	Method
overfits	O
less	O
than	O
the	O
other	O
pooling	Method
approaches	O
.	O

subsection	O
:	O
Importance	O
of	O
Model	Method
Averaging	Method
To	O
analyze	O
the	O
importance	O
of	O
stochastic	Method
sampling	Method
at	O
training	O
time	O
and	O
probability	Method
weighting	Method
at	O
test	O
time	O
,	O
we	O
use	O
different	O
methods	O
of	O
pooling	Method
when	O
training	O
and	O
testing	O
on	O
CIFAR	Material
-	Material
10	Material
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
.	O

Choosing	O
the	O
locations	O
stochastically	O
at	O
test	O
time	O
degrades	O
performance	O
slightly	O
as	O
could	O
be	O
expected	O
,	O
however	O
it	O
still	O
outperforms	O
models	O
where	O
max	Method
or	O
average	Method
pooling	Method
are	O
used	O
at	O
test	O
time	O
.	O

To	O
confirm	O
that	O
probability	Method
weighting	Method
is	O
a	O
valid	O
approximation	O
to	O
averaging	O
many	O
models	O
,	O
we	O
draw	O
samples	O
of	O
the	O
pooling	Method
locations	O
throughout	O
the	O
network	O
and	O
average	Method
the	O
output	O
probabilities	O
from	O
those	O
models	O
(	O
denoted	O
Stochastic	O
-	O
in	O
Table	O
[	O
reference	O
]	O
)	O
.	O

As	O
increases	O
,	O
the	O
results	O
approach	O
the	O
probability	Method
weighting	Method
method	Method
,	O
but	O
have	O
the	O
obvious	O
downside	O
of	O
an	O
-	O
fold	O
increase	O
in	O
computations	Metric
.	O

Using	O
a	O
model	O
trained	O
with	O
max	Method
or	O
average	Method
pooling	Method
and	O
using	O
stochastic	Method
pooling	Method
at	O
test	O
time	O
performs	O
poorly	O
.	O

This	O
suggests	O
that	O
training	O
with	O
stochastic	Method
pooling	Method
,	O
which	O
incorporates	O
non	O
-	O
maximal	O
elements	O
and	O
sampling	O
noise	O
,	O
makes	O
the	O
model	O
more	O
robust	O
at	O
test	O
time	O
.	O

Furthermore	O
,	O
if	O
these	O
non	O
-	O
maximal	O
elements	O
are	O
not	O
utilized	O
correctly	O
or	O
the	O
scale	O
produced	O
by	O
the	O
pooling	Method
function	O
is	O
not	O
correct	O
,	O
such	O
as	O
if	O
average	Method
pooling	Method
is	O
used	O
at	O
test	O
time	O
,	O
a	O
drastic	O
performance	O
hit	O
is	O
seen	O
.	O

When	O
using	O
probability	Method
weighting	Method
during	O
training	O
,	O
the	O
network	O
easily	O
over	O
-	O
fits	O
and	O
performs	O
sub	O
-	O
optimally	O
at	O
test	O
time	O
using	O
any	O
of	O
the	O
pooling	Method
methods	O
.	O

However	O
,	O
the	O
benefits	O
of	O
probability	Method
weighting	Method
at	O
test	O
time	O
are	O
seen	O
when	O
the	O
model	O
has	O
specifically	O
been	O
trained	O
to	O
utilize	O
it	O
through	O
either	O
probability	Method
weighting	Method
or	O
stochastic	Method
pooling	Method
at	O
training	O
time	O
.	O

subsection	O
:	O
Visualizations	Task
Some	O
insight	O
into	O
the	O
mechanism	O
of	O
stochastic	Method
pooling	Method
can	O
be	O
gained	O
by	O
using	O
a	O
deconvolutional	Method
network	Method
of	O
Zeiler	O
et	O
al	O
.	O

[	O
]	O
to	O
provide	O
a	O
novel	O
visualization	O
of	O
our	O
trained	Method
convolutional	Method
network	Method
.	O

The	O
deconvolutional	Method
network	Method
has	O
the	O
same	O
components	O
(	O
pooling	Method
,	O
filtering	Method
)	O
as	O
a	O
convolutional	Method
network	Method
but	O
are	O
inverted	O
to	O
act	O
as	O
a	O
top	Method
-	Method
down	Method
decoder	Method
that	O
maps	O
the	O
top	O
-	O
layer	O
feature	O
maps	O
back	O
to	O
the	O
input	O
pixels	O
.	O

The	O
unpooling	Method
operation	Method
uses	O
the	O
stochastically	O
chosen	O
locations	O
selected	O
during	O
the	O
forward	O
pass	O
.	O

The	O
deconvolution	Method
network	Method
filters	Method
(	O
now	O
applied	O
to	O
the	O
feature	O
maps	O
,	O
rather	O
than	O
the	O
input	O
)	O
are	O
the	O
transpose	O
of	O
the	O
feed	Method
-	Method
forward	Method
filters	Method
,	O
as	O
in	O
an	O
auto	Method
-	Method
encoder	Method
with	O
tied	Method
encoder	Method
/	O
decoder	Method
weights	Method
.	O

We	O
repeat	O
this	O
top	O
-	O
down	O
process	O
until	O
the	O
input	O
pixel	O
level	O
is	O
reached	O
,	O
producing	O
the	O
visualizations	O
in	O
Fig	O
.	O

[	O
reference	O
]	O
.	O

With	O
max	Method
pooling	Method
,	O
many	O
of	O
the	O
input	O
image	O
edges	O
are	O
present	O
,	O
but	O
average	Method
pooling	Method
produces	O
a	O
reconstruction	Method
with	O
no	O
discernible	O
structure	O
.	O

Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
16	O
examples	O
of	O
pixel	Method
-	Method
space	Method
reconstructions	Method
for	O
different	O
location	O
samples	O
throughout	O
the	O
network	O
.	O

The	O
reconstructions	O
are	O
similar	O
to	O
the	O
max	Method
pooling	Method
case	Method
,	O
but	O
as	O
the	O
pooling	Method
locations	O
change	O
they	O
result	O
in	O
small	O
local	O
deformations	O
of	O
the	O
visualized	O
image	O
.	O

Despite	O
the	O
stochastic	O
nature	O
of	O
the	O
model	O
,	O
the	O
multinomial	Method
distributions	Method
effectively	O
capture	O
the	O
regularities	O
of	O
the	O
data	O
.	O

To	O
demonstrate	O
this	O
,	O
we	O
compare	O
the	O
outputs	O
produced	O
by	O
a	O
deconvolutional	Method
network	Method
when	O
sampling	O
using	O
the	O
feedforward	Method
(	O
FF	Method
)	O
proabilities	O
versus	O
sampling	O
from	O
uniform	Method
(	O
UN	Method
)	O
distributions	O
.	O

In	O
contrast	O
to	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
a	O
)	O
which	O
uses	O
only	O
feedforward	Method
proabilities	O
,	O
Fig	O
.	O

[	O
reference	O
]	O
(	O
b	O
-	O
h	O
)	O
replace	O
one	O
or	O
more	O
of	O
the	O
pooling	Method
layers	O
’	O
distributions	O
with	O
uniform	Method
distributions	O
.	O

The	O
feed	O
forward	O
probabilities	O
encode	O
significant	O
structural	O
information	O
,	O
especially	O
in	O
the	O
lower	O
layers	O
of	O
the	O
model	O
.	O

Additional	O
visualizations	O
and	O
videos	O
of	O
the	O
sampling	Method
process	Method
are	O
provided	O
as	O
supplementary	O
material	O
at	O
.	O

section	O
:	O
Discussion	O
We	O
propose	O
a	O
simple	O
and	O
effective	O
stochastic	O
pooling	Method
strategy	O
that	O
can	O
be	O
combined	O
with	O
any	O
other	O
forms	O
of	O
regularization	Method
such	O
as	O
weight	O
decay	O
,	O
dropout	Method
,	O
data	Task
augmentation	Task
,	O
etc	O
.	O

to	O
prevent	O
over	Task
-	Task
fitting	Task
when	O
training	O
deep	Method
convolutional	Method
networks	Method
.	O

The	O
method	O
is	O
also	O
intuitive	O
,	O
selecting	O
from	O
information	O
the	O
network	O
is	O
already	O
providing	O
,	O
as	O
opposed	O
to	O
methods	O
such	O
as	O
dropout	Method
which	O
throw	O
information	O
away	O
.	O

We	O
show	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
numerous	O
datasets	O
,	O
when	O
comparing	O
to	O
other	O
approaches	O
that	O
do	O
not	O
employ	O
data	Method
augmentation	Method
.	O

Furthermore	O
,	O
our	O
method	O
has	O
negligible	O
computational	Metric
overhead	Metric
and	O
no	O
hyper	O
-	O
parameters	O
to	O
tune	O
,	O
thus	O
can	O
be	O
swapped	O
into	O
to	O
any	O
existing	O
convolutional	Method
network	Method
architecture	Method
.	O

bibliography	O
:	O
References	O
