document	O
:	O
FlowNet	Method
2.0	Method
:	O
Evolution	Method
of	Method
Optical	Method
Flow	Method
Estimation	Method
with	O
Deep	Method
Networks	Method
The	O
FlowNet	Method
demonstrated	O
that	O
optical	O
flow	Method
estimation	O
can	O
be	O
cast	O
as	O
a	O
learning	Task
problem	Task
.	O

However	O
,	O
the	O
state	O
of	O
the	O
art	O
with	O
regard	O
to	O
the	O
quality	O
of	O
the	O
flow	Method
has	O
still	O
been	O
defined	O
by	O
traditional	O
methods	O
.	O

Particularly	O
on	O
small	O
displacements	O
and	O
real	O
-	O
world	O
data	O
,	O
FlowNet	Method
can	O
not	O
compete	O
with	O
variational	Method
methods	Method
.	O

In	O
this	O
paper	O
,	O
we	O
advance	O
the	O
concept	O
of	O
end	O
-	O
to	O
-	O
end	O
learning	O
of	O
optical	O
flow	Method
and	O
make	O
it	O
work	O
really	O
well	O
.	O

The	O
large	O
improvements	O
in	O
quality	Metric
and	O
speed	Metric
are	O
caused	O
by	O
three	O
major	O
contributions	O
:	O
first	O
,	O
we	O
focus	O
on	O
the	O
training	O
data	O
and	O
show	O
that	O
the	O
schedule	O
of	O
presenting	O
data	O
during	O
training	Task
is	O
very	O
important	O
.	O

Second	O
,	O
we	O
develop	O
a	O
stacked	Method
architecture	Method
that	O
includes	O
warping	O
of	O
the	O
second	O
image	O
with	O
intermediate	O
optical	O
flow	Method
.	O

Third	O
,	O
we	O
elaborate	O
on	O
small	O
displacements	O
by	O
introducing	O
a	O
sub	Method
-	Method
network	Method
specializing	O
on	O
small	O
motions	O
.	O

FlowNet	Method
2.0	Method
is	O
only	O
marginally	O
slower	O
than	O
the	O
original	O
FlowNet	Method
but	O
decreases	O
the	O
estimation	Metric
error	Metric
by	O
more	O
than	O
50	O
%	O
.	O

It	O
performs	O
on	O
par	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
while	O
running	O
at	O
interactive	Metric
frame	Metric
rates	Metric
.	O

Moreover	O
,	O
we	O
present	O
faster	O
variants	O
that	O
allow	O
optical	O
flow	Method
computation	O
at	O
up	O
to	O
140fps	O
with	O
accuracy	Metric
matching	O
the	O
original	O
FlowNet	Method
.	O

section	O
:	O
Introduction	O
The	O
FlowNet	Method
by	O
Dosovitskiy	O
represented	O
a	O
paradigm	O
shift	O
in	O
optical	O
flow	Method
estimation	O
.	O

The	O
idea	O
of	O
using	O
a	O
simple	O
convolutional	Method
CNN	Method
architecture	Method
to	O
directly	O
learn	O
the	O
concept	O
of	O
optical	O
flow	Method
from	O
data	O
was	O
completely	O
disjoint	O
from	O
all	O
the	O
established	O
approaches	O
.	O

However	O
,	O
first	O
implementations	O
of	O
new	O
ideas	O
often	O
have	O
a	O
hard	O
time	O
competing	O
with	O
highly	O
fine	O
-	O
tuned	O
existing	O
methods	O
,	O
and	O
FlowNet	Method
was	O
no	O
exception	O
to	O
this	O
rule	O
.	O

It	O
is	O
the	O
successive	O
consolidation	O
that	O
resolves	O
the	O
negative	O
effects	O
and	O
helps	O
us	O
appreciate	O
the	O
benefits	O
of	O
new	O
ways	O
of	O
thinking	O
.	O

At	O
the	O
same	O
time	O
,	O
it	O
resolves	O
problems	O
with	O
small	O
displacements	O
and	O
noisy	O
artifacts	O
in	O
estimated	O
flow	Method
fields	O
.	O

This	O
leads	O
to	O
a	O
dramatic	O
performance	O
improvement	O
on	O
real	Task
-	Task
world	Task
applications	Task
such	O
as	O
action	Task
recognition	Task
and	O
motion	Task
segmentation	Task
,	O
bringing	O
FlowNet	Method
2.0	Method
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
level	O
.	O

The	O
way	O
towards	O
FlowNet	Method
2.0	Method
is	O
via	O
several	O
evolutionary	O
,	O
but	O
decisive	O
modifications	O
that	O
are	O
not	O
trivially	O
connected	O
to	O
the	O
observed	O
problems	O
.	O

First	O
,	O
we	O
evaluate	O
the	O
influence	O
of	O
dataset	O
schedules	O
.	O

Interestingly	O
,	O
the	O
more	O
sophisticated	O
training	O
data	O
provided	O
by	O
Mayer	O
leads	O
to	O
inferior	O
results	O
if	O
used	O
in	O
isolation	O
.	O

However	O
,	O
a	O
learning	Method
schedule	Method
consisting	O
of	O
multiple	O
datasets	O
improves	O
results	O
significantly	O
.	O

In	O
this	O
scope	O
,	O
we	O
also	O
found	O
that	O
the	O
FlowNet	Method
version	Method
with	O
an	O
explicit	Method
correlation	Method
layer	Method
outperforms	O
the	O
version	O
without	O
such	O
layer	O
.	O

This	O
is	O
in	O
contrast	O
to	O
the	O
results	O
reported	O
in	O
Dosovitskiy	O
.	O

As	O
a	O
second	O
contribution	O
,	O
we	O
introduce	O
a	O
warping	Method
operation	Method
and	O
show	O
how	O
stacking	O
multiple	Method
networks	Method
using	O
this	O
operation	O
can	O
significantly	O
improve	O
the	O
results	O
.	O

By	O
varying	O
the	O
depth	O
of	O
the	O
stack	O
and	O
the	O
size	O
of	O
individual	O
components	O
we	O
obtain	O
many	O
network	Method
variants	Method
with	O
different	O
size	O
and	O
runtime	O
.	O

This	O
allows	O
us	O
to	O
control	O
the	O
trade	O
-	O
off	O
between	O
accuracy	Metric
and	O
computational	Metric
resources	Metric
.	O

We	O
provide	O
networks	O
for	O
the	O
spectrum	O
between	O
fps	O
and	O
fps	O
.	O

Finally	O
,	O
we	O
focus	O
on	O
small	O
,	O
subpixel	O
motion	O
and	O
real	O
-	O
world	O
data	O
.	O

To	O
this	O
end	O
,	O
we	O
created	O
a	O
special	O
training	O
dataset	O
and	O
a	O
specialized	Method
network	Method
.	O

We	O
show	O
that	O
the	O
architecture	O
trained	O
with	O
this	O
dataset	O
performs	O
well	O
on	O
small	O
motions	O
typical	O
for	O
real	O
-	O
world	O
videos	O
.	O

To	O
reach	O
optimal	O
performance	O
on	O
arbitrary	O
displacements	O
,	O
we	O
add	O
a	O
network	O
that	O
learns	O
to	O
fuse	O
the	O
former	O
stacked	Method
network	Method
with	O
the	O
small	Method
displacement	Method
network	Method
in	O
an	O
optimal	O
manner	O
.	O

The	O
final	O
network	O
outperforms	O
the	O
previous	O
FlowNet	Method
by	O
a	O
large	O
margin	O
and	O
performs	O
on	O
par	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
the	O
Sintel	O
and	O
KITTI	O
benchmarks	O
.	O

It	O
can	O
estimate	O
small	O
and	O
large	O
displacements	O
with	O
very	O
high	O
level	O
of	O
detail	O
while	O
providing	O
interactive	Metric
frame	Metric
rates	Metric
.	O

section	O
:	O
Related	O
Work	O
End	O
-	O
to	O
-	O
end	O
optical	O
flow	Method
estimation	O
with	O
convolutional	Method
networks	Method
was	O
proposed	O
by	O
Dosovitskiy	O
in	O
.	O

Their	O
model	O
,	O
dubbed	O
FlowNet	Method
,	O
takes	O
a	O
pair	O
of	O
images	O
as	O
input	O
and	O
outputs	O
the	O
flow	Method
field	O
.	O

Following	O
FlowNet	Method
,	O
several	O
papers	O
have	O
studied	O
optical	O
flow	Method
estimation	O
with	O
CNNs	Method
:	O
featuring	O
a	O
3D	Method
convolutional	Method
network	Method
,	O
an	O
unsupervised	Method
learning	Method
objective	Method
,	O
carefully	O
designed	O
rotationally	Method
invariant	Method
architectures	Method
,	O
or	O
a	O
pyramidal	Method
approach	Method
based	O
on	O
the	O
coarse	Method
-	Method
to	Method
-	Method
fine	Method
idea	Method
of	Method
variational	Method
methods	Method
.	O

None	O
of	O
these	O
methods	O
significantly	O
outperforms	O
the	O
original	O
FlowNet	Method
.	O

An	O
alternative	O
approach	O
to	O
learning	O
-	O
based	O
optical	O
flow	Method
estimation	O
is	O
to	O
use	O
CNNs	Method
for	O
matching	Task
image	Task
patches	Task
.	O

Thewlis	O
formulate	O
Deep	Method
Matching	Method
as	O
a	O
convolutional	Method
network	Method
and	O
optimize	O
it	O
end	O
-	O
to	O
-	O
end	O
.	O

Gadot	O
&	O
Wolf	O
and	O
Bailer	O
learn	O
image	Method
patch	Method
descriptors	Method
using	O
Siamese	Method
network	Method
architectures	Method
.	O

These	O
methods	O
can	O
reach	O
good	O
accuracy	Metric
,	O
but	O
require	O
exhaustive	Method
matching	Method
of	Method
patches	Method
.	O

Thus	O
,	O
they	O
are	O
restrictively	O
slow	O
for	O
most	O
practical	O
applications	O
.	O

Moreover	O
,	O
patch	Method
based	Method
approaches	Method
lack	O
the	O
possibility	O
to	O
use	O
the	O
larger	O
context	O
of	O
the	O
whole	O
image	O
because	O
they	O
operate	O
on	O
small	O
image	O
patches	O
.	O

Convolutional	Method
networks	Method
trained	O
for	O
per	Task
-	Task
pixel	Task
prediction	Task
tasks	Task
often	O
produce	O
noisy	O
or	O
blurry	O
results	O
.	O

As	O
a	O
remedy	O
,	O
out	Task
-	Task
of	Task
-	Task
the	Task
-	Task
box	Task
optimization	Task
can	O
be	O
applied	O
to	O
the	O
network	Task
predictions	Task
as	O
a	O
postprocessing	Task
operation	Task
,	O
for	O
example	O
,	O
optical	O
flow	Method
estimates	O
can	O
be	O
refined	O
with	O
a	O
variational	Method
approach	Method
.	O

In	O
some	O
cases	O
,	O
this	O
refinement	O
can	O
be	O
approximated	O
by	O
neural	Method
networks	Method
:	O
Chen	O
&	O
Pock	O
formulate	O
reaction	Method
diffusion	Method
model	Method
as	O
a	O
CNN	Method
and	O
apply	O
it	O
to	O
image	Task
denoising	Task
,	O
deblocking	Task
and	Task
superresolution	Task
.	O

Recently	O
,	O
it	O
has	O
been	O
shown	O
that	O
similar	O
refinement	O
can	O
be	O
obtained	O
by	O
stacking	O
several	O
convolutional	Method
networks	Method
on	O
top	O
of	O
each	O
other	O
.	O

This	O
led	O
to	O
improved	O
results	O
in	O
human	Task
pose	Task
estimation	Task
and	O
semantic	Task
instance	Task
segmentation	Task
.	O

In	O
this	O
paper	O
we	O
adapt	O
the	O
idea	O
of	O
stacking	Task
multiple	Task
networks	Task
to	O
optical	O
flow	Method
estimation	O
.	O

Our	O
network	Method
architecture	Method
includes	O
warping	Method
layers	Method
that	O
compensate	O
for	O
some	O
already	O
estimated	O
preliminary	O
motion	O
in	O
the	O
second	O
image	O
.	O

The	O
concept	O
of	O
image	Task
warping	Task
is	O
common	O
to	O
all	O
contemporary	O
variational	O
optical	O
flow	Method
methods	O
and	O
goes	O
back	O
to	O
the	O
work	O
of	O
Lucas	O
&	O
Kanade	O
.	O

In	O
Brox	O
it	O
was	O
shown	O
to	O
correspond	O
to	O
a	O
numerical	Method
fixed	Method
point	Method
iteration	Method
scheme	Method
coupled	O
with	O
a	O
continuation	Method
method	Method
.	O

The	O
strategy	O
of	O
training	O
machine	Method
learning	Method
models	Method
on	O
a	O
series	O
of	O
gradually	Task
increasing	Task
tasks	Task
is	O
known	O
as	O
curriculum	Method
learning	Method
.	O

The	O
idea	O
dates	O
back	O
at	O
least	O
to	O
Elman	O
,	O
who	O
showed	O
that	O
both	O
the	O
evolution	O
of	O
tasks	O
and	O
the	O
network	Method
architectures	Method
can	O
be	O
beneficial	O
in	O
the	O
language	Task
processing	Task
scenario	Task
.	O

In	O
this	O
paper	O
we	O
revisit	O
this	O
idea	O
in	O
the	O
context	O
of	O
computer	Task
vision	Task
and	O
show	O
how	O
it	O
can	O
lead	O
to	O
dramatic	O
performance	O
improvement	O
on	O
a	O
complex	O
real	O
-	O
world	O
task	O
of	O
optical	O
flow	Method
estimation	O
.	O

section	O
:	O
Dataset	O
Schedules	O
High	O
quality	O
training	O
data	O
is	O
crucial	O
for	O
the	O
success	O
of	O
supervised	Task
training	Task
.	O

We	O
investigated	O
the	O
differences	O
in	O
the	O
quality	O
of	O
the	O
estimated	O
optical	O
flow	Method
depending	O
on	O
the	O
presented	O
training	O
data	O
.	O

Interestingly	O
,	O
it	O
turned	O
out	O
that	O
not	O
only	O
the	O
kind	O
of	O
data	O
is	O
important	O
but	O
also	O
the	O
order	O
in	O
which	O
it	O
is	O
presented	O
during	O
training	O
.	O

The	O
original	O
FlowNets	Method
were	O
trained	O
on	O
the	O
FlyingChairs	O
dataset	O
(	O
we	O
will	O
call	O
it	O
Chairs	O
)	O
.	O

This	O
rather	O
simplistic	O
dataset	O
contains	O
about	O
k	O
image	O
pairs	O
of	O
chairs	O
superimposed	O
on	O
random	O
background	O
images	O
from	O
Flickr	O
.	O

Random	Method
affine	Method
transformations	Method
are	O
applied	O
to	O
chairs	O
and	O
background	O
to	O
obtain	O
the	O
second	O
image	O
and	O
ground	O
truth	O
flow	Method
fields	O
.	O

The	O
dataset	O
contains	O
only	O
planar	O
motions	O
.	O

The	O
FlyingThings3D	O
(	O
Things3D	O
)	O
dataset	O
proposed	O
by	O
Mayer	O
can	O
be	O
seen	O
as	O
a	O
three	O
-	O
dimensional	O
version	O
of	O
the	O
FlyingChairs	O
.	O

The	O
dataset	O
consists	O
of	O
k	O
renderings	O
of	O
random	O
scenes	O
showing	O
3D	O
models	O
from	O
the	O
ShapeNet	O
dataset	O
moving	O
in	O
front	O
of	O
static	O
3D	O
backgrounds	O
.	O

In	O
contrast	O
to	O
Chairs	O
,	O
the	O
images	O
show	O
true	O
3D	O
motion	O
and	O
lighting	O
effects	O
and	O
there	O
is	O
more	O
variety	O
among	O
the	O
object	Method
models	Method
.	O

We	O
tested	O
the	O
two	O
network	Method
architectures	Method
introduced	O
by	O
Dosovitskiy	O
:	O
FlowNetS	Method
,	O
which	O
is	O
a	O
straightforward	O
encoder	Method
-	Method
decoder	Method
architecture	Method
,	O
and	O
FlowNetC	Method
,	O
which	O
includes	O
explicit	O
correlation	O
of	O
feature	O
maps	O
.	O

We	O
trained	O
FlowNetS	Method
and	O
FlowNetC	Method
on	O
Chairs	O
and	O
Things3D	O
and	O
an	O
equal	O
mixture	O
of	O
samples	O
from	O
both	O
datasets	O
using	O
the	O
different	O
learning	Metric
rate	Metric
schedules	Metric
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
basic	O
schedule	O
(	O
k	O
iterations	O
)	O
corresponds	O
to	O
Dosovitskiy	O
except	O
some	O
minor	O
changes	O
.	O

Apart	O
from	O
this	O
basic	O
schedule	O
,	O
we	O
investigated	O
a	O
longer	O
schedule	O
with	O
M	O
iterations	O
,	O
and	O
a	O
schedule	O
for	O
fine	Task
-	Task
tuning	Task
with	O
smaller	O
learning	Metric
rates	Metric
.	O

Results	O
of	O
networks	O
trained	O
on	O
Chairs	O
and	O
Things3D	O
with	O
the	O
different	O
schedules	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
results	O
lead	O
to	O
the	O
following	O
observations	O
:	O
The	O
order	O
of	O
presenting	O
training	O
data	O
with	O
different	O
properties	O
matters	O
.	O

Although	O
Things3D	O
is	O
more	O
realistic	O
,	O
training	O
on	O
Things3D	O
alone	O
leads	O
to	O
worse	O
results	O
than	O
training	O
on	O
Chairs	O
.	O

The	O
best	O
results	O
are	O
consistently	O
achieved	O
when	O
first	O
training	O
on	O
Chairs	O
and	O
only	O
then	O
fine	O
-	O
tuning	O
on	O
Things3D.	O
This	O
schedule	O
also	O
outperforms	O
training	O
on	O
a	O
mixture	O
of	O
Chairs	O
and	O
Things3D.	O
We	O
conjecture	O
that	O
the	O
simpler	O
Chairs	O
dataset	O
helps	O
the	O
network	O
learn	O
the	O
general	O
concept	O
of	O
color	Task
matching	Task
without	O
developing	O
possibly	O
confusing	O
priors	O
for	O
3D	O
motion	O
and	O
realistic	O
lighting	O
too	O
early	O
.	O

The	O
result	O
indicates	O
the	O
importance	O
of	O
training	O
data	O
schedules	O
for	O
avoiding	O
shortcuts	O
when	O
learning	O
generic	Task
concepts	Task
with	O
deep	Method
networks	Method
.	O

FlowNetC	Method
outperforms	O
FlowNetS.	Method
The	O
result	O
we	O
got	O
with	O
FlowNetS	Method
and	O
corresponds	O
to	O
the	O
one	O
reported	O
in	O
Dosovitskiy	O
.	O

However	O
,	O
we	O
obtained	O
much	O
better	O
results	O
on	O
FlowNetC.	Method
We	O
conclude	O
that	O
Dosovitskiy	O
did	O
not	O
train	O
FlowNetS	Method
and	O
FlowNetC	Method
under	O
the	O
exact	O
same	O
conditions	O
.	O

When	O
done	O
so	O
,	O
the	O
FlowNetC	Method
architecture	O
compares	O
favorably	O
to	O
the	O
FlowNetS	Method
architecture	O
.	O

Improved	O
results	O
.	O

Just	O
by	O
modifying	O
datasets	O
and	O
training	O
schedules	O
,	O
we	O
improved	O
the	O
FlowNetS	Method
result	O
reported	O
by	O
Dosovitskiy	O
by	O
and	O
the	O
FlowNetC	Method
result	O
by	O
.	O

In	O
this	O
section	O
,	O
we	O
did	O
not	O
yet	O
use	O
specialized	O
training	O
sets	O
for	O
specialized	O
scenarios	O
.	O

The	O
trained	O
network	O
is	O
rather	O
supposed	O
to	O
be	O
generic	O
and	O
to	O
work	O
well	O
in	O
various	O
scenarios	O
.	O

An	O
additional	O
optional	O
component	O
in	O
dataset	O
schedules	O
is	O
fine	O
-	O
tuning	O
of	O
a	O
generic	Method
network	Method
to	O
a	O
specific	O
scenario	O
,	O
such	O
as	O
the	O
driving	Task
scenario	Task
,	O
which	O
we	O
show	O
in	O
Section	O
[	O
reference	O
]	O
.	O

section	O
:	O
Stacking	Method
Networks	Method
subsection	O
:	O
Stacking	O
Two	O
Networks	O
for	O
Flow	Task
Refinement	Task
All	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
optical	O
flow	Method
approaches	O
rely	O
on	O
iterative	Method
methods	Method
.	O

Can	O
deep	Method
networks	Method
also	O
benefit	O
from	O
iterative	Method
refinement	Method
?	O
To	O
answer	O
this	O
,	O
we	O
experiment	O
with	O
stacking	O
multiple	O
FlowNetS	Method
and	O
FlowNetC	Method
architectures	O
.	O

The	O
first	O
network	O
in	O
the	O
stack	O
always	O
gets	O
the	O
images	O
and	O
as	O
input	O
.	O

Subsequent	O
networks	O
get	O
,	O
,	O
and	O
the	O
previous	O
flow	Method
estimate	O
,	O
where	O
denotes	O
the	O
index	O
of	O
the	O
network	O
in	O
the	O
stack	O
.	O

To	O
make	O
assessment	O
of	O
the	O
previous	O
error	O
and	O
computing	O
an	O
incremental	O
update	O
easier	O
for	O
the	O
network	O
,	O
we	O
also	O
optionally	O
warp	O
the	O
second	O
image	O
via	O
the	O
flow	Method
and	O
bilinear	O
interpolation	O
to	O
.	O

This	O
way	O
,	O
the	O
next	O
network	O
in	O
the	O
stack	O
can	O
focus	O
on	O
the	O
remaining	O
increment	O
between	O
and	O
.	O

When	O
using	O
warping	O
,	O
we	O
additionally	O
provide	O
and	O
the	O
error	O
as	O
input	O
to	O
the	O
next	O
network	O
;	O
see	O
Figure	O
[	O
reference	O
]	O
.	O

Thanks	O
to	O
bilinear	Method
interpolation	Method
,	O
the	O
derivatives	O
of	O
the	O
warping	O
operation	O
can	O
be	O
computed	O
(	O
see	O
supplemental	O
material	O
for	O
details	O
)	O
.	O

This	O
enables	O
training	O
of	O
stacked	Method
networks	Method
end	O
-	O
to	O
-	O
end	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
effect	O
of	O
stacking	O
two	O
networks	O
,	O
the	O
effect	O
of	O
warping	O
,	O
and	O
the	O
effect	O
of	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
.	O

We	O
take	O
the	O
best	O
FlowNetS	Method
from	O
Section	O
[	O
reference	O
]	O
and	O
add	O
another	O
FlowNetS	Method
on	O
top	O
.	O

The	O
second	O
network	O
is	O
initialized	O
randomly	O
and	O
then	O
the	O
stack	O
is	O
trained	O
on	O
Chairs	O
with	O
the	O
schedule	O
.	O

We	O
experimented	O
with	O
two	O
scenarios	O
:	O
keeping	O
the	O
weights	O
of	O
the	O
first	O
network	O
fixed	O
,	O
or	O
updating	O
them	O
together	O
with	O
the	O
weights	O
of	O
the	O
second	O
network	O
.	O

In	O
the	O
latter	O
case	O
,	O
the	O
weights	O
of	O
the	O
first	O
network	O
are	O
fixed	O
for	O
the	O
first	O
400k	O
iterations	O
to	O
first	O
provide	O
a	O
good	O
initialization	O
of	O
the	O
second	O
network	O
.	O

We	O
report	O
the	O
error	O
on	O
Sintel	O
train	O
clean	O
and	O
on	O
the	O
test	O
set	O
of	O
Chairs	O
.	O

Since	O
the	O
Chairs	O
test	O
set	O
is	O
much	O
more	O
similar	O
to	O
the	O
training	O
data	O
than	O
Sintel	O
,	O
comparing	O
results	O
on	O
both	O
datasets	O
allows	O
us	O
to	O
detect	O
tendencies	O
to	O
over	O
-	O
fitting	O
.	O

We	O
make	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
Just	O
stacking	Method
networks	Method
without	O
warping	Method
improves	O
results	O
on	O
Chairs	O
but	O
decreases	O
performance	O
on	O
Sintel	O
,	O
i.e.	O
the	O
stacked	Method
network	Method
is	O
over	O
-	O
fitting	O
.	O

(	O
2	O
)	O
With	O
warping	O
included	O
,	O
stacking	Task
always	O
improves	O
results	O
.	O

(	O
3	O
)	O
Adding	O
an	O
intermediate	O
loss	O
after	O
Net1	Method
is	O
advantageous	O
when	O
training	O
the	O
stacked	Method
network	Method
end	O
-	O
to	O
-	O
end	O
.	O

(	O
4	O
)	O
The	O
best	O
results	O
are	O
obtained	O
when	O
keeping	O
the	O
first	O
network	O
fixed	O
and	O
only	O
training	O
the	O
second	O
network	O
after	O
the	O
warping	Method
operation	Method
.	O

Clearly	O
,	O
since	O
the	O
stacked	Method
network	Method
is	O
twice	O
as	O
big	O
as	O
the	O
single	Method
network	Method
,	O
over	O
-	O
fitting	O
is	O
an	O
issue	O
.	O

The	O
positive	O
effect	O
of	O
flow	Method
refinement	O
after	O
warping	Method
can	O
counteract	O
this	O
problem	O
,	O
yet	O
the	O
best	O
of	O
both	O
is	O
obtained	O
when	O
the	O
stacked	Method
networks	Method
are	O
trained	O
one	O
after	O
the	O
other	O
,	O
since	O
this	O
avoids	O
over	Method
-	Method
fitting	Method
while	O
having	O
the	O
benefit	O
of	O
flow	Method
refinement	O
.	O

subsection	O
:	O
Stacking	O
Multiple	O
Diverse	Method
Networks	Method
Rather	O
than	O
stacking	O
identical	O
networks	O
,	O
it	O
is	O
possible	O
to	O
stack	O
networks	O
of	O
different	O
type	O
(	O
FlowNetC	Method
and	O
FlowNetS	Method
)	O
.	O

Reducing	O
the	O
size	O
of	O
the	O
individual	O
networks	O
is	O
another	O
valid	O
option	O
.	O

We	O
now	O
investigate	O
different	O
combinations	O
and	O
additionally	O
also	O
vary	O
the	O
network	O
size	O
.	O

We	O
call	O
the	O
first	O
network	O
the	O
bootstrap	Method
network	Method
as	O
it	O
differs	O
from	O
the	O
second	O
network	O
by	O
its	O
inputs	O
.	O

The	O
second	O
network	O
could	O
however	O
be	O
repeated	O
an	O
arbitray	O
number	O
of	O
times	O
in	O
a	O
recurrent	Method
fashion	Method
.	O

We	O
conducted	O
this	O
experiment	O
and	O
found	O
that	O
applying	O
a	O
network	O
with	O
the	O
same	O
weights	O
multiple	O
times	O
and	O
also	O
fine	O
-	O
tuning	O
this	O
recurrent	Method
part	Method
does	O
not	O
improve	O
results	O
(	O
see	O
supplemental	O
material	O
for	O
details	O
)	O
.	O

As	O
also	O
done	O
in	O
,	O
we	O
therefore	O
add	O
networks	O
with	O
different	O
weights	O
to	O
the	O
stack	O
.	O

Compared	O
to	O
identical	O
weights	O
,	O
stacking	Method
networks	Method
with	O
different	O
weights	O
increases	O
the	O
memory	Metric
footprint	Metric
,	O
but	O
does	O
not	O
increase	O
the	O
runtime	Metric
.	O

In	O
this	O
case	O
the	O
top	Method
networks	Method
are	O
not	O
constrained	O
to	O
a	O
general	O
improvement	O
of	O
their	O
input	O
,	O
but	O
can	O
perform	O
different	O
tasks	O
at	O
different	O
stages	O
and	O
the	O
stack	O
can	O
be	O
trained	O
in	O
smaller	O
pieces	O
by	O
fixing	O
existing	O
networks	O
and	O
adding	O
new	O
networks	O
one	O
-	O
by	O
-	O
one	O
.	O

We	O
do	O
so	O
by	O
using	O
the	O
Chairs	Method
Things3D	Method
schedule	Method
from	O
Section	O
[	O
reference	O
]	O
for	O
every	O
new	O
network	O
and	O
the	O
best	O
configuration	O
with	O
warping	O
from	O
Section	O
[	O
reference	O
]	O
.	O

Furthermore	O
,	O
we	O
experiment	O
with	O
different	O
network	O
sizes	O
and	O
alternatively	O
use	O
FlowNetS	Method
or	O
FlowNetC	Method
as	O
a	O
bootstrapping	Method
network	Method
.	O

We	O
use	O
FlowNetC	Method
only	O
in	O
case	O
of	O
the	O
bootstrap	Method
network	Method
,	O
as	O
the	O
input	O
to	O
the	O
next	O
network	O
is	O
too	O
diverse	O
to	O
be	O
properly	O
handeled	O
by	O
the	O
Siamese	O
structure	O
of	O
FlowNetC.	Method
Smaller	O
size	O
versions	O
of	O
the	O
networks	O
were	O
created	O
by	O
taking	O
only	O
a	O
fraction	O
of	O
the	O
number	O
of	O
channels	O
for	O
every	O
layer	O
in	O
the	O
network	O
.	O

Figure	O
[	O
reference	O
]	O
shows	O
the	O
network	Metric
accuracy	Metric
and	O
runtime	Metric
for	O
different	O
network	O
sizes	O
of	O
a	O
single	O
FlowNetS.	Method
Factor	Method
yields	O
a	O
good	O
trade	O
-	O
off	O
between	O
speed	Metric
and	O
accuracy	Metric
when	O
aiming	O
for	O
faster	Task
networks	Task
.	O

Notation	O
:	O
We	O
denote	O
networks	O
trained	O
by	O
the	O
Chairs	O
Things3D	O
schedule	O
from	O
Section	O
[	O
reference	O
]	O
starting	O
with	O
FlowNet2	Method
.	O

Networks	O
in	O
a	O
stack	O
are	O
trained	O
with	O
this	O
schedule	O
one	O
-	O
by	O
-	O
one	O
.	O

For	O
the	O
stack	Task
configuration	Task
we	O
append	O
upper	O
-	O
or	O
lower	O
-	O
case	O
letters	O
to	O
indicate	O
the	O
original	O
FlowNet	Method
or	O
the	O
thin	O
version	O
with	O
of	O
the	O
channels	O
.	O

E.g	O
:	O
FlowNet2	Method
-	Method
CSS	Method
stands	O
for	O
a	O
network	O
stack	O
consisting	O
of	O
one	O
FlowNetC	Method
and	O
two	O
FlowNetS.	Method
FlowNet2	O
-	O
css	O
is	O
the	O
same	O
but	O
with	O
fewer	O
channels	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
different	O
network	O
stacks	O
.	O

Most	O
notably	O
,	O
the	O
final	O
FlowNet2	Method
-	Method
CSS	Method
result	O
improves	O
by	O
over	O
the	O
single	O
network	O
FlowNet2	O
-	O
C	O
from	O
Section	O
[	O
reference	O
]	O
and	O
by	O
over	O
the	O
original	O
FlowNetC	Method
.	O

Furthermore	O
,	O
two	O
small	Method
networks	Method
in	O
the	O
beginning	O
always	O
outperform	O
one	O
large	O
network	O
,	O
despite	O
being	O
faster	O
and	O
having	O
fewer	O
weights	O
:	O
FlowNet2	Method
-	Method
ss	Method
(	O
M	O
weights	O
)	O
over	O
FlowNet2	Method
-	Method
S	Method
(	O
M	O
weights	O
)	O
,	O
and	O
FlowNet2	Method
-	Method
cs	Method
(	O
M	O
weights	O
)	O
over	O
FlowNet2	Method
-	Method
C	Method
(	O
M	O
weights	O
)	O
.	O

Training	O
smaller	O
units	O
step	O
by	O
step	O
proves	O
to	O
be	O
advantageous	O
and	O
enables	O
us	O
to	O
train	O
very	O
deep	Method
networks	Method
for	O
optical	O
flow	Method
.	O

At	O
last	O
,	O
FlowNet2	Method
-	Method
s	Method
provides	O
nearly	O
the	O
same	O
accuracy	Metric
as	O
the	O
original	O
FlowNet	Method
,	O
while	O
running	O
at	O
frames	O
per	O
second	O
.	O

section	O
:	O
Small	O
Displacements	O
subsection	O
:	O
Datasets	O
While	O
the	O
original	O
FlowNet	Method
performed	O
well	O
on	O
the	O
Sintel	O
benchmark	O
,	O
limitations	O
in	O
real	Task
-	Task
world	Task
applications	Task
have	O
become	O
apparent	O
.	O

In	O
particular	O
,	O
the	O
network	O
can	O
not	O
reliably	O
estimate	O
small	O
motions	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

This	O
is	O
counter	O
-	O
intuitive	O
,	O
since	O
small	O
motions	O
are	O
easier	O
for	O
traditional	O
methods	O
,	O
and	O
there	O
is	O
no	O
obvious	O
reason	O
why	O
networks	O
should	O
not	O
reach	O
the	O
same	O
performance	O
in	O
this	O
setting	O
.	O

Thus	O
,	O
we	O
examined	O
the	O
training	O
data	O
and	O
compared	O
it	O
to	O
the	O
UCF101	O
dataset	O
as	O
one	O
example	O
of	O
real	O
-	O
world	O
data	O
.	O

While	O
Chairs	O
are	O
similar	O
to	O
Sintel	O
,	O
UCF101	O
is	O
fundamentally	O
different	O
(	O
we	O
refer	O
to	O
our	O
supplemental	O
material	O
for	O
the	O
analysis	O
)	O
:	O
Sintel	O
is	O
an	O
action	O
movie	O
and	O
as	O
such	O
contains	O
many	O
fast	O
movements	O
that	O
are	O
difficult	O
for	O
traditional	O
methods	O
,	O
while	O
the	O
displacements	O
we	O
see	O
in	O
the	O
UCF101	O
dataset	O
are	O
much	O
smaller	O
,	O
mostly	O
smaller	O
than	O
pixel	O
.	O

Thus	O
,	O
we	O
created	O
a	O
dataset	O
in	O
the	O
visual	O
style	O
of	O
Chairs	O
but	O
with	O
very	O
small	O
displacements	O
and	O
a	O
displacement	O
histogram	O
much	O
more	O
like	O
UCF101	O
.	O

We	O
also	O
added	O
cases	O
with	O
a	O
background	O
that	O
is	O
homogeneous	O
or	O
just	O
consists	O
of	O
color	O
gradients	O
.	O

We	O
call	O
this	O
dataset	O
ChairsSDHom	O
and	O
will	O
release	O
it	O
upon	O
publication	O
.	O

subsection	O
:	O
Small	Method
Displacement	Method
Network	Method
and	O
Fusion	Task
We	O
fine	O
-	O
tuned	O
our	O
FlowNet2	Method
-	Method
CSS	Method
network	Method
for	O
smaller	O
displacements	O
by	O
further	O
training	O
the	O
whole	O
network	Method
stack	Method
on	O
a	O
mixture	O
of	O
Things3D	Method
and	O
ChairsSDHom	Method
and	O
by	O
applying	O
a	O
non	Method
-	Method
linearity	Method
to	O
the	O
error	O
to	O
downweight	O
large	O
displacements	O
.	O

We	O
denote	O
this	O
network	O
by	O
FlowNet2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
.	O

This	O
increases	O
performance	O
on	O
small	O
displacements	O
and	O
we	O
found	O
that	O
this	O
particular	O
mixture	O
does	O
not	O
sacrifice	O
performance	O
on	O
large	O
displacements	O
.	O

However	O
,	O
in	O
case	O
of	O
subpixel	Task
motion	Task
,	O
noise	O
still	O
remains	O
a	O
problem	O
and	O
we	O
conjecture	O
that	O
the	O
FlowNet	Method
architecture	Method
might	O
in	O
general	O
not	O
be	O
perfect	O
for	O
such	O
motion	O
.	O

Therefore	O
,	O
we	O
slightly	O
modified	O
the	O
original	O
FlowNetS	Method
architecture	O
and	O
removed	O
the	O
stride	O
in	O
the	O
first	O
layer	O
.	O

We	O
made	O
the	O
beginning	O
of	O
the	O
network	O
deeper	O
by	O
exchanging	O
the	O
and	Method
kernels	Method
in	O
the	O
beginning	O
with	O
multiple	O
kernels	O
.	O

Because	O
noise	O
tends	O
to	O
be	O
a	O
problem	O
with	O
small	O
displacements	O
,	O
we	O
add	O
convolutions	Method
between	O
the	O
upconvolutions	Method
to	O
obtain	O
smoother	Task
estimates	Task
as	O
in	O
.	O

We	O
denote	O
the	O
resulting	O
architecture	O
by	O
FlowNet2	Method
-	Method
SD	Method
;	O
see	O
Figure	O
[	O
reference	O
]	O
.	O

Finally	O
,	O
we	O
created	O
a	O
small	O
network	O
that	O
fuses	O
FlowNet2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
and	O
FlowNet2	Method
-	Method
SD	Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O

The	O
fusion	Method
network	Method
receives	O
the	O
flows	O
,	O
the	O
flow	Method
magnitudes	O
and	O
the	O
errors	O
in	O
brightness	O
after	O
warping	O
as	O
input	O
.	O

It	O
contracts	O
the	O
resolution	O
two	O
times	O
by	O
a	O
factor	O
of	O
and	O
expands	O
again	O
.	O

Contrary	O
to	O
the	O
original	O
FlowNet	Method
architecture	Method
it	O
expands	O
to	O
the	O
full	O
resolution	O
.	O

We	O
find	O
that	O
this	O
produces	O
crisp	O
motion	O
boundaries	O
and	O
performs	O
well	O
on	O
small	O
as	O
well	O
as	O
on	O
large	O
displacements	O
.	O

We	O
denote	O
the	O
final	O
network	O
as	O
FlowNet2	Method
.	O

section	O
:	O
Experiments	O
We	O
compare	O
the	O
best	O
variants	O
of	O
our	O
network	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
on	O
public	O
bechmarks	O
.	O

In	O
addition	O
,	O
we	O
provide	O
a	O
comparison	O
on	O
application	Task
tasks	Task
,	O
such	O
as	O
motion	Task
segmentation	Task
and	O
action	Task
recognition	Task
.	O

This	O
allows	O
benchmarking	O
the	O
method	O
on	O
real	O
data	O
.	O

subsection	O
:	O
Speed	O
and	O
Performance	O
on	O
Public	O
Benchmarks	O
MPI	O
Sintel	O
(	O
train	O
final	O
)	O
Average	O
EPE	Metric
Runtime	Metric
(	O
milliseconds	O
per	O
frame	O
)	O
CPU	O
GPU	Method
Ours	O
fps	O
fps	O
fps	O
EpicFlow	O
DeepFlow	O
FlowField	O
LDOF	O
LDOF	O
(	O
GPU	O
)	O
PCA	O
-	O
Flow	O
PCA	Method
-	Method
Layers	Method
DIS	Method
-	Method
Fast	Method
FlowNetS	Method
FlowNetC	Method
FN2	Method
-	Method
s	Method
FN2	Method
-	Method
ss	Method
FN2	Method
-	Method
css	Method
-	Method
ft	Method
-	Method
sd	Method
FN2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
FlowNet2	Method
Accurate	O
Fast	O
FlowNet	Method
2.0	O
We	O
evaluated	O
all	O
methods	O
on	O
a	O
system	O
with	O
an	O
Intel	O
Xeon	O
E5	O
with	O
2.40GHz	O
and	O
an	O
Nvidia	Method
GTX	Method
1080	Method
.	O

Where	O
applicable	O
,	O
dataset	O
-	O
specific	O
parameters	O
were	O
used	O
,	O
that	O
yield	O
best	O
performance	O
.	O

Endpoint	Metric
errors	Metric
and	O
runtimes	Metric
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

Sintel	O
:	O
On	O
Sintel	O
,	O
FlowNet2	Method
consistently	O
outperforms	O
DeepFlow	Method
and	O
EpicFlow	Method
and	O
is	O
on	O
par	O
with	O
FlowFields	Method
.	O

All	O
methods	O
with	O
comparable	O
runtimes	O
have	O
clearly	O
inferior	O
accuracy	Metric
.	O

We	O
fine	O
-	O
tuned	O
FlowNet2	Method
on	O
a	O
mixture	O
of	O
Sintel	O
clean	O
+	O
final	O
training	O
data	O
(	O
FlowNet2–ft	O
-	O
sintel	O
)	O
.	O

On	O
the	O
benchmark	O
,	O
in	O
case	O
of	O
clean	O
data	O
this	O
slightly	O
degraded	O
the	O
result	O
,	O
while	O
on	O
final	O
data	O
FlowNet2–ft	O
-	O
sintel	O
is	O
on	O
par	O
with	O
the	O
currently	O
published	O
state	O
-	O
of	O
-	O
the	O
art	O
method	O
DeepDiscreteFlow	Method
.	O

KITTI	O
:	O
On	O
KITTI	O
,	O
the	O
results	O
of	O
FlowNet2	Method
-	Method
CSS	Method
are	O
comparable	O
to	O
EpicFlow	Method
and	O
FlowFields	Method
.	O

Fine	Task
-	Task
tuning	Task
on	O
small	O
displacement	O
data	O
degrades	O
the	O
result	O
.	O

This	O
is	O
probably	O
due	O
to	O
KITTI	O
containing	O
very	O
large	O
displacements	O
in	O
general	O
.	O

Fine	O
-	O
tuning	O
on	O
a	O
combination	O
of	O
the	O
KITTI2012	O
and	O
KITTI2015	O
training	O
sets	O
reduces	O
the	O
error	Metric
roughly	O
by	O
a	O
factor	O
of	O
(	O
FlowNet2	Method
-	Method
ft	Method
-	Method
kitti	Method
)	O
.	O

Among	O
non	Method
-	Method
stereo	Method
methods	Method
we	O
obtain	O
the	O
best	O
EPE	Method
on	O
KITTI2012	O
and	O
the	O
first	O
rank	O
on	O
the	O
KITTI2015	O
benchmark	O
.	O

This	O
shows	O
how	O
well	O
and	O
elegantly	O
the	O
learning	Method
approach	Method
can	O
integrate	O
the	O
prior	O
of	O
the	O
driving	Task
scenario	Task
.	O

Middlebury	O
:	O
On	O
the	O
Middlebury	O
training	O
set	O
FlowNet2	Method
performs	O
comparable	O
to	O
traditional	O
methods	O
.	O

The	O
results	O
on	O
the	O
Middlebury	O
test	O
set	O
are	O
unexpectedly	O
a	O
lot	O
worse	O
.	O

Still	O
,	O
there	O
is	O
a	O
large	O
improvement	O
compared	O
to	O
FlowNetS	Method
.	O

Endpoint	Metric
error	Metric
vs.	O
runtime	Metric
evaluations	Metric
for	O
Sintel	O
are	O
provided	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

One	O
can	O
observe	O
that	O
the	O
FlowNet2	Method
family	Method
outperforms	O
the	O
best	O
and	O
fastest	O
existing	O
methods	O
by	O
large	O
margins	O
.	O

Depending	O
on	O
the	O
type	O
of	O
application	O
,	O
a	O
FlowNet2	Method
variant	Method
between	O
8	O
to	O
140	O
frames	O
per	O
second	O
can	O
be	O
used	O
.	O

subsection	O
:	O
Qualitative	O
Results	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
example	O
results	O
on	O
Sintel	O
and	O
on	O
real	O
-	O
world	O
data	O
.	O

While	O
the	O
performance	O
on	O
Sintel	O
is	O
similar	O
to	O
FlowFields	O
,	O
we	O
can	O
see	O
that	O
on	O
real	O
world	O
data	O
FlowNet	Method
2.0	Method
clearly	O
has	O
advantages	O
in	O
terms	O
of	O
being	O
robust	O
to	O
homogeneous	O
regions	O
(	O
rows	O
2	O
and	O
5	O
)	O
,	O
image	O
and	O
compression	O
artifacts	O
(	O
rows	O
3	O
and	O
4	O
)	O
and	O
it	O
yields	O
smooth	O
flow	Method
fields	O
with	O
sharp	O
motion	O
boundaries	O
.	O

subsection	O
:	O
Performance	O
on	O
Motion	Task
Segmentation	Task
and	O
Action	Task
Recognition	Task
To	O
assess	O
performance	O
of	O
FlowNet	Method
2.0	Method
in	O
real	Task
-	Task
world	Task
applications	Task
,	O
we	O
compare	O
the	O
performance	O
of	O
action	Task
recognition	Task
and	O
motion	Task
segmentation	Task
.	O

For	O
both	O
applications	O
,	O
good	O
optical	O
flow	Method
is	O
key	O
.	O

Thus	O
,	O
a	O
good	O
performance	O
on	O
these	O
tasks	O
also	O
serves	O
as	O
an	O
indicator	O
for	O
good	O
optical	O
flow	Method
.	O

For	O
motion	Task
segmentation	Task
,	O
we	O
rely	O
on	O
the	O
well	O
-	O
established	O
approach	O
of	O
Ochs	O
to	O
compute	O
long	Task
term	Task
point	Task
trajectories	Task
.	O

A	O
motion	Task
segmentation	Task
is	O
obtained	O
from	O
these	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
from	O
Keuper	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O

The	O
original	O
model	O
in	O
Ochs	O
was	O
built	O
on	O
Large	Method
Displacement	Method
Optical	Method
Flow	Method
.	O

We	O
included	O
also	O
other	O
popular	O
optical	O
flow	Method
methods	O
in	O
the	O
comparison	O
.	O

The	O
old	O
FlowNet	Method
was	O
not	O
useful	O
for	O
motion	Task
segmentation	Task
.	O

In	O
contrast	O
,	O
the	O
FlowNet2	Method
is	O
as	O
reliable	O
as	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
while	O
being	O
orders	O
of	O
magnitude	O
faster	O
.	O

Optical	O
flow	Method
is	O
also	O
a	O
crucial	O
feature	O
for	O
action	Task
recognition	Task
.	O

To	O
assess	O
the	O
performance	O
,	O
we	O
trained	O
the	O
temporal	O
stream	O
of	O
the	O
two	Method
-	Method
stream	Method
approach	Method
from	O
Simonyan	Method
with	O
different	O
optical	O
flow	Method
inputs	O
.	O

Table	O
[	O
reference	O
]	O
shows	O
that	O
FlowNetS	Method
did	O
not	O
provide	O
useful	O
results	O
,	O
while	O
the	O
flow	Method
from	O
FlowNet	Method
2.0	Method
yields	O
comparable	O
results	O
to	O
state	O
-	O
of	O
-	O
the	O
art	O
methods	O
.	O

section	O
:	O
Conclusions	O
We	O
have	O
presented	O
several	O
improvements	O
to	O
the	O
FlowNet	Method
idea	Method
that	O
have	O
led	O
to	O
accuracy	Metric
that	O
is	O
fully	O
on	O
par	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
while	O
FlowNet	Method
2.0	Method
runs	O
orders	O
of	O
magnitude	O
faster	O
.	O

We	O
have	O
quantified	O
the	O
effect	O
of	O
each	O
contribution	O
and	O
showed	O
that	O
all	O
play	O
an	O
important	O
role	O
.	O

The	O
experiments	O
on	O
motion	Task
segmentation	Task
and	O
action	Task
recognition	Task
show	O
that	O
the	O
estimated	O
optical	O
flow	Method
with	O
FlowNet	Method
2.0	Method
is	O
reliable	O
on	O
a	O
large	O
variety	O
of	O
scenes	O
and	O
applications	O
.	O

The	O
FlowNet	Method
2.0	O
family	O
provides	O
networks	O
running	O
at	O
speeds	O
from	O
8	O
to	O
140fps	O
.	O

This	O
further	O
extends	O
the	O
possible	O
range	O
of	O
applications	O
.	O

While	O
the	O
results	O
on	O
Middlebury	O
indicate	O
imperfect	O
performance	O
on	O
subpixel	O
motion	O
,	O
FlowNet	Method
2.0	Method
results	O
highlight	O
very	O
crisp	O
motion	O
boundaries	O
,	O
retrieval	O
of	O
fine	O
structures	O
,	O
and	O
robustness	Metric
to	O
compression	O
artifacts	O
.	O

Thus	O
,	O
we	O
expect	O
it	O
to	O
become	O
the	O
working	O
horse	O
for	O
all	O
applications	O
that	O
require	O
accurate	O
and	O
fast	O
optical	O
flow	Method
computation	O
.	O

section	O
:	O
Acknowledgements	O
We	O
acknowledge	O
funding	O
by	O
the	O
ERC	O
Starting	O
Grant	O
VideoLearn	O
,	O
the	O
DFG	O
Grant	O
BR	O
-	O
3815	O
/	O
7	O
-	O
1	O
,	O
and	O
the	O
EU	O
Horizon2020	O
project	O
TrimBot2020	O
.	O

bibliography	O
:	O
References	O
section	O
:	O
Video	O
Please	O
see	O
the	O
supplementary	O
video	O
for	O
FlowNet2	Method
results	O
on	O
a	O
number	O
of	O
diverse	O
video	O
sequences	O
,	O
a	O
comparison	O
between	O
FlowNet2	Method
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
,	O
and	O
an	O
illustration	O
of	O
the	O
speed	Metric
/	Metric
accuracy	Metric
trade	O
-	O
off	O
of	O
the	O
FlowNet	Method
2.0	O
family	O
of	O
models	O
.	O

paragraph	O
:	O
Optical	O
flow	Method
color	O
coding	O
.	O

For	O
optical	O
flow	Method
visualization	O
we	O
use	O
the	O
color	Method
coding	Method
of	Method
Butler	Method
.	O

The	O
color	Method
coding	Method
scheme	Method
is	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

Hue	O
represents	O
the	O
direction	O
of	O
the	O
displacement	O
vector	O
,	O
while	O
the	O
intensity	O
of	O
the	O
color	O
represents	O
its	O
magnitude	O
.	O

White	O
color	O
corresponds	O
to	O
no	O
motion	O
.	O

Because	O
the	O
range	O
of	O
motions	O
is	O
very	O
different	O
in	O
different	O
image	O
sequences	O
,	O
we	O
scale	O
the	O
flow	Method
fields	O
before	O
visualization	Task
:	O
independently	O
for	O
each	O
image	O
pair	O
shown	O
in	O
figures	O
,	O
and	O
independently	O
for	O
each	O
video	O
fragment	O
in	O
the	O
supplementary	O
video	O
.	O

Scaling	Method
is	O
always	O
the	O
same	O
for	O
all	O
methods	O
being	O
compared	O
.	O

section	O
:	O
Dataset	O
Schedules	O
:	O
KITTI2015	O
Results	O
In	O
Table	O
[	O
reference	O
]	O
we	O
show	O
more	O
results	O
of	O
training	Method
networks	Method
with	O
the	O
original	O
FlowNet	Method
schedule	Method
and	O
the	O
new	O
FlowNet2	Method
schedules	Method
and	O
.	O

We	O
provide	O
the	O
endpoint	Metric
error	Metric
when	O
testing	O
on	O
the	O
KITTI2015	O
train	O
dataset	O
.	O

Table	O
1	O
in	O
the	O
main	O
paper	O
shows	O
the	O
performance	O
of	O
the	O
same	O
networks	O
on	O
Sintel	O
.	O

One	O
can	O
observe	O
that	O
on	O
KITTI2015	O
,	O
as	O
well	O
as	O
on	O
Sintel	O
,	O
training	O
with	O
on	O
the	O
combination	O
of	O
Chairs	O
and	O
Things3D	O
works	O
best	O
(	O
in	O
the	O
paper	O
referred	O
to	O
as	O
Chairs	O
Things3D	O
schedule	O
)	O
.	O

section	O
:	O
Recurrently	Method
Stacking	Method
Networks	Method
with	O
the	O
Same	O
Weights	O
The	O
bootstrap	Method
network	Method
differs	O
from	O
the	O
succeeding	O
networks	O
by	O
its	O
task	O
(	O
it	O
needs	O
to	O
predict	O
a	O
flow	Method
field	O
from	O
scratch	O
)	O
and	O
inputs	O
(	O
it	O
does	O
not	O
get	O
a	O
previous	O
flow	Method
estimate	O
and	O
a	O
warped	O
image	O
)	O
.	O

The	O
network	O
after	O
the	O
bootstrap	Method
network	Method
only	O
refines	O
the	O
previous	O
flow	Method
estimate	O
,	O
so	O
it	O
can	O
be	O
applied	O
to	O
its	O
own	O
output	O
recursively	O
.	O

We	O
took	O
the	O
best	O
network	O
from	O
Table	O
2	O
of	O
the	O
main	O
paper	O
and	O
applied	O
Net2	O
recursively	O
multiple	O
times	O
.	O

We	O
then	O
continued	O
training	O
the	O
whole	O
stack	O
with	O
multiple	O
Net2	O
.	O

The	O
difference	O
from	O
our	O
final	O
FlowNet2	Method
architecture	Method
is	O
that	O
here	O
the	O
weights	O
are	O
shared	O
between	O
the	O
stacked	Method
networks	Method
,	O
similar	O
to	O
a	O
standard	O
recurrent	Method
network	Method
.	O

Results	O
are	O
given	O
in	O
Table	O
[	O
reference	O
]	O
.	O

In	O
all	O
cases	O
we	O
observe	O
no	O
or	O
negligible	O
improvements	O
compared	O
to	O
the	O
baseline	O
network	O
with	O
a	O
single	O
Net2	O
.	O

section	O
:	O
Small	O
Displacements	O
subsection	O
:	O
The	O
ChairsSDHom	O
Dataset	O
As	O
an	O
example	O
of	O
real	O
-	O
world	O
data	O
we	O
examine	O
the	O
UCF101	O
dataset	O
.	O

We	O
compute	O
optical	O
flow	Method
using	O
LDOF	Method
and	O
compare	O
the	O
flow	Method
magnitude	O
distribution	O
to	O
the	O
synthetic	O
datasets	O
we	O
use	O
for	O
training	O
and	O
benchmarking	O
,	O
this	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

While	O
Chairs	Method
are	O
similar	O
to	O
Sintel	O
,	O
UCF101	O
is	O
fundamentally	O
different	O
and	O
contains	O
much	O
more	O
small	O
displacments	O
.	O

To	O
create	O
a	O
training	O
dataset	O
similar	O
to	O
UCF101	O
,	O
following	O
,	O
we	O
generated	O
our	O
ChairsSDHom	Method
(	O
Chairs	O
Small	O
Displacement	O
Homogeneous	O
)	O
dataset	O
by	O
randomly	O
placing	O
and	O
moving	O
chairs	O
in	O
front	O
of	O
randomized	O
background	O
images	O
.	O

However	O
,	O
we	O
also	O
followed	O
Mayer	O
in	O
that	O
our	O
chairs	O
are	O
not	O
flat	O
2D	O
bitmaps	O
as	O
in	O
,	O
but	O
rendered	O
3D	O
objects	O
.	O

Similar	O
to	O
Mayer	O
,	O
we	O
rendered	O
our	O
data	O
first	O
in	O
a	O
‘	O
‘	O
raw	O
’	O
’	O
version	O
to	O
get	O
blend	O
-	O
free	O
flow	Method
boundaries	O
and	O
then	O
a	O
second	O
time	O
with	O
antialiasing	Method
to	O
obtain	O
the	O
color	O
images	O
.	O

To	O
match	O
the	O
characteristic	O
contents	O
of	O
the	O
UCF101	O
dataset	O
,	O
we	O
mostly	O
applied	O
small	O
motions	O
.	O

We	O
added	O
scenes	O
with	O
weakly	O
textured	O
background	O
to	O
the	O
dataset	O
,	O
being	O
monochrome	O
or	O
containing	O
a	O
very	O
subtle	O
color	O
gradient	O
.	O

Such	O
monotonous	O
backgrounds	O
are	O
not	O
unusual	O
in	O
natural	O
videos	O
,	O
but	O
almost	O
never	O
appear	O
in	O
Chairs	O
or	O
Things3D.	O
A	O
featureless	O
background	O
can	O
potentially	O
move	O
in	O
any	O
direction	O
(	O
an	O
extreme	O
case	O
of	O
the	O
aperture	Task
problem	Task
)	O
,	O
so	O
we	O
kept	O
these	O
background	O
images	O
fixed	O
to	O
introduce	O
a	O
meaningful	O
prior	O
into	O
the	O
dataset	O
.	O

Example	O
images	O
from	O
the	O
dataset	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

fraction	O
of	O
displacement	O
bin	O
ChairsSDHom	O
UCF101	O
FlyingThings3D	O
FlyingChairs	O
Sintel	O
subsection	O
:	O
Fine	Task
-	Task
Tuning	Task
FlowNet2	Task
-	Task
CSS	Task
-	Task
ft	Task
-	Task
sd	Task
With	O
the	O
new	O
ChairsSDHom	O
dataset	O
we	O
fine	O
-	O
tuned	O
our	O
FlowNet2	Method
-	Method
CSS	Method
network	Method
for	O
smaller	O
displacements	O
(	O
we	O
denote	O
this	O
by	O
FlowNet2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
)	O
.	O

We	O
experimented	O
with	O
different	O
configurations	O
to	O
avoid	O
sacrificing	O
performance	O
on	O
large	O
displacements	O
.	O

We	O
found	O
the	O
best	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
with	O
mini	O
-	O
batches	O
of	O
samples	O
:	O
from	O
Things3D	Method
and	O
from	O
ChairsSDHom	Method
.	O

Furthermore	O
,	O
we	O
applied	O
a	O
nonlinearity	O
of	O
to	O
the	O
endpoint	Metric
error	Metric
to	O
emphasize	O
the	O
small	O
-	O
magnitude	O
flows	O
.	O

subsection	O
:	O
Network	Method
Architectures	Method
The	O
architectures	O
of	O
the	O
small	Method
displacement	Method
network	Method
and	O
the	O
fusion	Method
network	Method
are	O
shown	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O

The	O
input	O
to	O
the	O
small	Method
displacement	Method
network	Method
is	O
formed	O
by	O
concatenating	O
both	O
RGB	O
images	O
,	O
resulting	O
in	O
input	O
channels	O
.	O

The	O
network	O
is	O
in	O
general	O
similar	O
to	O
FlowNetS.	Method
Differences	O
are	O
the	O
smaller	O
strides	O
and	O
smaller	O
kernel	O
sizes	O
in	O
the	O
beginning	O
and	O
the	O
convolutions	O
between	O
the	O
upconvolutions	Method
.	O

The	O
fusion	Method
network	Method
is	O
trained	O
to	O
merge	O
the	O
flow	Method
estimates	O
of	O
two	O
previously	O
trained	Method
networks	Method
,	O
and	O
this	O
task	O
dictates	O
the	O
input	O
structure	O
.	O

We	O
feed	O
the	O
following	O
data	O
into	O
the	O
network	O
:	O
the	O
first	O
image	O
from	O
the	O
image	O
pair	O
,	O
two	O
estimated	O
flow	Method
fields	O
,	O
their	O
magnitudes	O
,	O
and	O
finally	O
the	O
two	O
squared	Metric
Euclidean	Metric
photoconsistency	Metric
errors	Metric
,	O
that	O
is	O
,	O
per	O
-	O
pixel	O
squared	O
Euclidean	O
distance	O
between	O
the	O
first	O
image	O
and	O
the	O
second	O
image	O
warped	O
with	O
the	O
predicted	O
flow	Method
field	O
.	O

This	O
sums	O
up	O
to	O
channels	O
.	O

Note	O
that	O
we	O
do	O
not	O
input	O
the	O
second	O
image	O
directly	O
.	O

All	O
inputs	O
are	O
at	O
full	O
image	Method
resolution	Method
,	O
flow	Method
field	O
estimates	O
from	O
previous	O
networks	O
are	O
upsampled	O
with	O
nearest	Method
neighbor	Method
upsampling	Method
.	O

section	O
:	O
Evaluation	O
subsection	O
:	O
Intermediate	O
Results	O
in	O
Stacked	Method
Networks	Method
The	O
idea	O
of	O
the	O
stacked	Method
network	Method
architecture	Method
is	O
that	O
the	O
estimated	O
flow	Method
field	O
is	O
gradually	O
improved	O
by	O
every	O
network	O
in	O
the	O
stack	O
.	O

This	O
improvement	O
has	O
been	O
quantitatively	O
shown	O
in	O
the	O
paper	O
.	O

Here	O
,	O
we	O
additionally	O
show	O
qualitative	O
examples	O
which	O
clearly	O
highlight	O
this	O
effect	O
.	O

The	O
improvement	O
is	O
especially	O
dramatic	O
for	O
small	O
displacements	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O

The	O
initial	O
prediction	O
of	O
FlowNet2	Task
-	Task
C	Task
is	O
very	O
noisy	O
,	O
but	O
is	O
then	O
significantly	O
refined	O
by	O
the	O
two	O
succeeding	O
networks	O
.	O

The	O
FlowNet2	Method
-	Method
SD	Method
network	Method
,	O
specifically	O
trained	O
on	O
small	O
displacements	O
,	O
estimates	O
small	O
displacements	O
well	O
even	O
without	O
additional	O
refinement	O
.	O

Best	O
results	O
are	O
obtained	O
by	O
fusing	O
both	O
estimated	O
flow	Method
fields	O
.	O

Figure	O
[	O
reference	O
]	O
illustrates	O
this	O
for	O
a	O
large	Task
displacement	Task
case	Task
.	O

MPI	O
Sintel	O
(	O
train	O
final	O
)	O
Average	O
EPE	Metric
Runtime	Metric
(	O
milliseconds	O
per	O
frame	O
)	O
CPU	O
GPU	Method
Ours	O
fps	O
fps	O
fps	O
EpicFlow	O
DeepFlow	O
FlowFields	O
LDOF	O
LDOF	O
(	O
GPU	O
)	O
PCA	O
-	O
Flow	O
PCA	Method
-	Method
Layers	Method
DIS	Method
-	Method
Fast	Method
FlowNetS	Method
FlowNetC	Method
FN2	Method
-	Method
s	Method
FN2	Method
-	Method
ss	Method
FN2	Method
-	Method
css	Method
-	Method
ft	Method
-	Method
sd	Method
FN2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
FlowNet2	O
KITTI	O
2012	O
(	O
train	O
)	O
Average	O
EPE	Metric
Runtime	Metric
(	O
milliseconds	O
per	O
frame	O
)	O
CPU	O
GPU	Method
Ours	O
fps	O
fps	O
fps	O
EpicFlow	O
DeepFlow	O
FlowFields	O
LDOF	O
LDOF	O
(	O
GPU	O
)	O
PCA	O
-	O
Flow	O
PCA	Method
-	Method
Layers	Method
DIS	Method
-	Method
Fast	Method
FlowNetS	Method
FlowNetC	Method
FN2	Method
-	Method
s	Method
FN2	Method
-	Method
ss	Method
FN2	Method
-	Method
css	Method
-	Method
ft	Method
-	Method
sd	Method
FN2	Method
-	Method
CSS	Method
-	Method
ft	Method
-	Method
sd	Method
FlowNet2	Method
subsection	O
:	O
Speed	O
and	O
Performance	O
on	O
KITTI2012	O
Figure	O
[	O
reference	O
]	O
shows	O
runtime	O
vs.	O
endpoint	Metric
error	Metric
comparisons	Metric
of	O
various	O
optical	O
flow	Method
estimation	O
methods	O
on	O
two	O
datasets	O
:	O
Sintel	O
(	O
also	O
shown	O
in	O
the	O
main	O
paper	O
)	O
and	O
KITTI2012	O
.	O

In	O
both	O
cases	O
models	O
of	O
the	O
FlowNet	Method
2.0	O
family	O
offer	O
an	O
excellent	O
speed	Metric
/	Metric
accuracy	Metric
trade	O
-	O
off	O
.	O

Networks	Method
fine	O
-	O
tuned	O
on	O
KITTI	O
are	O
not	O
shown	O
.	O

The	O
corresponding	O
points	O
would	O
be	O
below	O
the	O
lower	O
border	O
of	O
the	O
KITTI2012	O
plot	O
.	O

subsection	O
:	O
Motion	Task
Segmentation	Task
Table	O
[	O
reference	O
]	O
shows	O
detailed	O
results	O
on	O
motion	Task
segmentation	Task
obtained	O
using	O
the	O
algorithms	O
from	O
with	O
flow	Method
fields	O
from	O
different	O
methods	O
as	O
input	O
.	O

For	O
FlowNetS	Method
the	O
algorithm	O
does	O
not	O
fully	O
converge	O
after	O
one	O
week	O
on	O
the	O
training	O
set	O
.	O

Due	O
to	O
the	O
bad	O
flow	Method
estimations	O
of	O
FlowNetS	Method
,	O
only	O
very	O
short	O
trajectories	O
can	O
be	O
computed	O
(	O
on	O
average	O
about	O
frames	O
)	O
,	O
yielding	O
an	O
excessive	O
number	O
of	O
trajectories	O
.	O

Therefore	O
we	O
do	O
not	O
evaluate	O
FlowNetS	Method
on	O
the	O
test	O
set	O
.	O

On	O
all	O
metrics	Metric
,	O
FlowNet2	Method
is	O
at	O
least	O
on	O
par	O
with	O
the	O
best	O
optical	O
flow	Method
estimation	O
methods	O
and	O
on	O
the	O
VI	Metric
(	Metric
variation	Metric
of	Metric
information	Metric
)	O
metric	O
it	O
is	O
even	O
significantly	O
better	O
.	O

subsection	O
:	O
Qualitative	O
results	O
on	O
KITTI2015	O
Figure	O
[	O
reference	O
]	O
shows	O
qualitative	O
results	O
on	O
the	O
KITTI2015	O
dataset	O
.	O

FlowNet2	Method
-	Method
kitti	Method
has	O
not	O
been	O
trained	O
on	O
these	O
images	O
during	O
fine	Task
-	Task
tuning	Task
.	O

KITTI	O
ground	O
truth	O
is	O
sparse	O
,	O
so	O
for	O
better	O
visualization	O
we	O
interpolated	O
the	O
ground	O
truth	O
with	O
bilinear	Method
interpolation	Method
.	O

FlowNet2	Method
-	Method
kitti	Method
significantly	O
outperforms	O
competing	O
approaches	O
both	O
quantitatively	O
and	O
qualitatively	O
.	O

section	O
:	O
Warping	Method
Layer	Method
The	O
following	O
two	O
sections	O
give	O
the	O
mathematical	O
details	O
of	O
forward	O
and	O
backward	O
passes	O
through	O
the	O
warping	Method
layer	Method
used	O
to	O
stack	Method
networks	Method
.	O

subsection	O
:	O
Definitions	O
and	O
Bilinear	Method
Interpolation	Method
Let	O
the	O
image	O
coordinates	O
be	O
and	O
the	O
set	O
of	O
valid	O
image	O
coordinates	O
.	O

Let	O
denote	O
the	O
image	O
and	O
the	O
flow	Method
field	O
.	O

The	O
image	O
can	O
also	O
be	O
a	O
feature	O
map	O
and	O
have	O
arbitrarily	O
many	O
channels	O
.	O

Let	O
channel	O
be	O
denoted	O
with	O
.	O

We	O
define	O
the	O
coefficients	O
:	O
and	O
compute	O
a	O
continuous	Method
version	Method
of	O
using	O
bilinear	Method
interpolation	Method
in	O
the	O
usual	O
way	O
:	O
subsection	O
:	O
Forward	O
Pass	O
During	O
the	O
forward	O
pass	O
,	O
we	O
compute	O
the	O
warped	O
image	O
by	O
following	O
the	O
flow	Method
vectors	O
.	O

We	O
define	O
all	O
pixels	O
to	O
be	O
zero	O
where	O
the	O
flow	Method
points	O
outside	O
of	O
the	O
image	O
:	O
subsection	O
:	O
Backward	Method
Pass	Method
During	O
the	O
backward	O
pass	O
,	O
we	O
need	O
to	O
compute	O
the	O
derivative	O
of	O
with	O
respect	O
to	O
its	O
inputs	O
and	O
,	O
where	O
and	O
are	O
different	O
integer	O
image	O
locations	O
.	O

Let	O
if	O
is	O
true	O
and	O
0	O
otherwise	O
,	O
and	O
let	O
.	O

For	O
brevity	O
,	O
we	O
omit	O
the	O
dependence	O
of	O
and	O
on	O
.	O

The	O
derivative	O
with	O
respect	O
to	O
is	O
then	O
computed	O
as	O
follows	O
:	O
The	O
derivative	O
with	O
respect	O
to	O
the	O
first	O
component	O
of	O
the	O
flow	Method
is	O
computed	O
as	O
follows	O
:	O
In	O
the	O
non	O
-	O
trivial	O
case	O
,	O
the	O
derivative	O
is	O
computed	O
as	O
follows	O
:	O
Note	O
that	O
the	O
ceiling	O
and	O
floor	O
functions	O
(	O
,	O
)	O
are	O
non	O
-	O
differentiable	O
at	O
points	O
with	O
integer	O
coordinates	O
and	O
we	O
use	O
directional	O
derivatives	O
in	O
these	O
cases	O
.	O

The	O
derivative	O
with	O
respect	O
to	O
is	O
analogous	O
.	O

