Table	Task
-	Task
to	Task
-	Task
text	Task
Generation	Task
by	O
Structure	Method
-	Method
aware	Method
Seq2seq	Task
Learning	Task
section	O
:	O
Abstract	O
Table	O
-	O
to	O
-	O
text	Task
generation	Task
aims	O
to	O
generate	O
a	O
description	O
for	O
a	O
factual	O
table	O
which	O
can	O
be	O
viewed	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
.	O
To	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
novel	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
which	O
consists	O
of	O
field	Method
-	Method
gating	Method
encoder	Method
and	O
description	Method
generator	Method
with	O
dual	O
attention	O
.	O
In	O
the	O
encoding	Task
phase	Task
,	O
we	O
update	O
the	O
cell	O
memory	O
of	O
the	O
LSTM	Method
unit	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
in	O
order	O
to	O
incorporate	O
field	O
information	O
into	O
table	Method
representation	Method
.	O
In	O
the	O
decoding	Task
phase	Task
,	O
dual	Method
attention	Method
mechanism	Method
which	O
contains	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
is	O
proposed	O
to	O
model	O
the	O
semantic	O
relevance	O
between	O
the	O
generated	O
description	O
and	O
the	O
table	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
WIKIBIO	Material
dataset	Material
which	O
contains	O
over	O
700k	O
biographies	O
and	O
corresponding	O
infoboxes	O
from	O
Wikipedia	O
.	O
The	O
attention	Task
visualizations	Task
and	O
case	O
studies	O
show	O
that	O
our	O
model	O
is	O
capable	O
of	O
generating	O
coherent	O
and	O
informative	O
descriptions	O
based	O
on	O
the	O
comprehensive	O
understanding	O
of	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O
Automatic	O
evaluations	O
also	O
show	O
our	O
model	O
outperforms	O
the	O
baselines	O
by	O
a	O
great	O
margin	O
.	O
Code	O
for	O
this	O
work	O
is	O
available	O
on	O
https:	O
//	O
github.com	O
/	O
tyliupku	O
/	O
wiki2bio	O
.	O
section	O
:	O
Introduction	O
Generating	Task
natural	Task
language	Task
description	Task
for	O
a	O
structured	O
table	O
is	O
an	O
important	O
task	O
for	O
text	Task
generation	Task
from	O
structured	O
data	O
.	O
Previous	O
researches	O
include	O
weather	Task
forecast	Task
based	O
on	O
a	O
set	O
of	O
weather	O
records	O
[	O
reference	O
]	O
and	O
sportscasting	O
based	O
on	O
temporally	O
ordered	O
events	O
[	O
reference	O
]	O
.	O
However	O
,	O
previous	O
work	O
models	O
the	O
structured	O
data	O
in	O
the	O
limited	O
pre	O
-	O
defined	O
schemas	O
.	O
For	O
example	O
,	O
a	O
weather	O
record	O
rainChance	O
(	O
time:06:00	O
-	O
21:00	O
,	O
mode	O
:	O
SSE	O
,	O
value:20	O
)	O
is	O
represented	O
by	O
a	O
fixed	O
-	O
length	O
onehot	O
vector	O
by	O
its	O
record	O
type	O
,	O
record	O
time	O
,	O
record	O
value	O
and	O
record	O
value	O
.	O
To	O
this	O
end	O
,	O
we	O
focus	O
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
which	O
involves	O
comprehensive	Method
representation	Method
for	O
the	O
complex	O
structure	O
of	O
a	O
table	O
rather	O
than	O
pre	O
-	O
defined	O
schemas	O
.	O
In	O
contrast	O
to	O
previous	O
work	O
experimented	O
on	O
small	O
datasets	O
which	O
contain	O
only	O
a	O
few	O
tens	O
of	O
thousands	O
of	O
records	O
such	O
as	O
WEATHERGOV	Material
[	O
reference	O
]	O
and	O
ROBOCUP	Material
[	O
reference	O
]	O
,	O
we	O
focus	O
on	O
a	O
more	O
challenging	O
task	O
to	O
generate	O
biographies	O
The	O
Wikipedia	O
infobox	O
of	O
Charles	O
Winstead	O
,	O
the	O
corresponding	O
introduction	O
on	O
his	O
wiki	O
page	O
reads	O
"	O
Charles	O
Winstead	O
[	O
reference	O
]	O
was	O
an	O
FBI	O
agent	O
in	O
the	O
1930s	O
-	O
40s	O
,	O
famous	O
for	O
being	O
one	O
of	O
the	O
agents	O
who	O
shot	O
and	O
killed	O
John	O
Dillinger	O
.	O
"	O
.	O
based	O
on	O
the	O
Wikipedia	O
infoboxes	O
.	O
As	O
shown	O
in	O
Fig	O
1	O
,	O
a	O
biographic	O
infobox	O
is	O
a	O
fixed	O
-	O
format	O
table	O
that	O
describes	O
a	O
person	O
with	O
many	O
field	O
-	O
value	O
records	O
like	O
(	O
Name	O
,	O
Charles	O
B.	O
Winstead	O
)	O
,	O
(	O
Nationality	O
,	O
American	O
)	O
,	O
(	O
Occupation	O
,	O
FBI	O
Agent	O
)	O
,	O
etc	O
.	O
We	O
utilize	O
WIKIBIO	Material
dataset	Material
proposed	O
by	O
[	O
reference	O
]	O
which	O
contains	O
700k	O
biographies	O
from	O
Wikipedia	O
,	O
with	O
400k	O
words	O
in	O
total	O
as	O
the	O
benchmark	O
dataset	O
.	O
Previous	O
work	O
has	O
made	O
significant	O
progress	O
on	O
this	O
task	O
.	O
[	O
reference	O
]	O
proposed	O
a	O
statistical	Method
n	Method
-	Method
gram	Method
model	Method
with	O
local	O
and	O
global	O
conditioning	O
on	O
a	O
Wikipedia	O
infobox	O
.	O
However	O
the	O
field	O
content	O
of	O
a	O
record	O
is	O
likely	O
to	O
be	O
a	O
sequence	O
of	O
words	O
,	O
the	O
statistical	Method
language	Method
model	Method
is	O
not	O
good	O
at	O
capturing	O
long	O
-	O
range	O
dependencies	O
between	O
words	O
.	O
[	O
reference	O
]	O
proposed	O
a	O
selective	O
generation	Task
method	O
based	O
on	O
an	O
encoder	Method
-	Method
alignerdecoder	Method
framework	Method
.	O
The	O
model	O
utilizes	O
a	O
sparse	Method
one	Method
-	Method
hot	Method
vector	Method
to	O
represent	O
a	O
weather	O
record	O
.	O
However	O
it	O
's	O
inefficient	O
to	O
represent	O
the	O
complex	O
structure	O
of	O
a	O
table	O
by	O
one	O
-	O
hot	O
vectors	O
.	O
We	O
propose	O
a	O
structure	Method
-	Method
aware	Method
sequence	Method
to	Method
sequence	Method
(	O
seq2seq	Method
)	O
generation	Task
framework	O
to	O
model	O
both	O
content	O
and	O
structure	O
of	O
the	O
table	O
by	O
local	O
and	O
global	O
addressing	O
.	O
When	O
a	O
human	O
writes	O
a	O
biography	O
for	O
a	O
person	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
,	O
he	O
will	O
firstly	O
determine	O
which	O
records	O
in	O
the	O
table	O
should	O
be	O
included	O
in	O
the	O
introduction	O
and	O
how	O
to	O
arrange	O
the	O
order	O
of	O
these	O
records	O
before	O
wording	O
.	O
After	O
that	O
,	O
the	O
writer	O
will	O
further	O
consider	O
which	O
words	O
or	O
phrases	O
in	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
to	O
paraphrase	O
.	O
We	O
summarize	O
the	O
two	O
phases	O
of	O
generation	Task
as	O
two	O
scopes	O
of	O
addressing	Task
:	O
local	Task
and	Task
global	Task
addressing	Task
.	O
Local	Task
addressing	Task
determines	O
which	O
particular	O
word	O
in	O
the	O
table	O
should	O
be	O
focused	O
on	O
while	O
generating	O
a	O
piece	O
of	O
description	O
at	O
certain	O
time	O
step	O
.	O
However	O
,	O
the	O
word	Method
level	Method
addressing	Method
can	O
not	O
fully	O
address	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
problem	O
as	O
the	O
factual	O
tables	O
usually	O
have	O
complex	O
structures	O
which	O
might	O
confuse	O
the	O
generator	O
.	O
Global	Task
addressing	Task
is	O
proposed	O
to	O
determine	O
which	O
records	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
while	O
generating	O
corresponding	O
description	O
.	O
Global	Task
addressing	Task
is	O
necessary	O
as	O
the	O
description	O
of	O
a	O
table	O
may	O
not	O
cover	O
all	O
the	O
records	O
.	O
For	O
example	O
,	O
the	O
'cause	O
of	O
death	O
'	O
field	O
in	O
Fig	O
1	O
is	O
not	O
mentioned	O
in	O
the	O
description	O
.	O
Furthermore	O
,	O
the	O
order	O
of	O
records	O
in	O
the	O
tables	O
may	O
not	O
always	O
be	O
homogeneous	O
.	O
For	O
example	O
,	O
we	O
can	O
introduce	O
a	O
person	O
as	O
an	O
order	O
of	O
his	O
(	O
BirthDeath	O
-	O
Nationality	O
-	O
Occupation	O
)	O
according	O
to	O
his	O
Wikipedia	O
infobox	O
.	O
However	O
the	O
other	O
infoboxes	O
may	O
be	O
arranged	O
as	O
(	O
Occupation	O
-	O
Nationality	O
-	O
Birth	O
-	O
Death	O
)	O
.	O
Local	Task
addressing	Task
is	O
realized	O
by	O
content	O
encoding	O
of	O
the	O
LSTM	Method
encoder	O
and	O
word	Method
level	Method
attention	Method
while	O
global	Method
addressing	Method
is	O
realized	O
by	O
field	Method
encoding	Method
of	O
the	O
field	O
-	O
gating	O
LSTM	Method
variation	O
and	O
field	Method
level	Method
attention	Method
in	O
our	O
model	O
.	O
The	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
we	O
proposed	O
exploits	O
encoder	Method
-	Method
decoder	Method
framework	Method
using	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
[	O
reference	O
]	O
units	O
with	O
local	O
and	O
global	O
addressing	O
on	O
the	O
structured	O
table	O
.	O
In	O
the	O
encoding	O
phase	O
,	O
our	O
model	O
first	O
encodes	O
the	O
sets	O
of	O
fieldvalue	O
records	O
in	O
the	O
table	O
by	O
integrating	O
field	O
information	O
and	O
content	Method
representation	Method
.	O
To	O
make	O
better	O
use	O
of	O
field	O
information	O
,	O
we	O
add	O
a	O
field	O
gate	O
to	O
the	O
cell	O
state	O
of	O
the	O
encoder	O
LSTM	Method
unit	O
to	O
incorporate	O
the	O
field	Method
embedding	Method
into	O
the	O
structural	O
representation	O
of	O
the	O
table	O
.	O
The	O
model	O
next	O
employs	O
a	O
LSTM	Method
decoder	O
to	O
generate	O
natural	Task
language	Task
description	Task
by	O
the	O
structural	Method
representation	Method
of	O
the	O
table	O
.	O
In	O
the	O
decoding	Task
phase	Task
,	O
we	O
also	O
propose	O
a	O
novel	O
dual	Method
attention	Method
mechanism	Method
which	O
consists	O
of	O
two	O
parts	O
:	O
word	Method
-	Method
level	Method
attention	Method
for	O
local	Task
addressing	Task
and	O
field	Task
-	Task
level	Task
attention	Task
for	O
global	Task
addressing	Task
.	O
Our	O
contributions	O
are	O
three	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
propose	O
an	O
endto	Method
-	Method
end	Method
structure	Method
-	Method
aware	Method
encoder	Method
-	Method
decoder	Method
architecture	Method
to	O
encode	O
field	O
information	O
into	O
the	O
representation	O
of	O
a	O
structured	O
table	O
.	O
(	O
2	O
)	O
Field	Method
-	Method
gating	Method
encoder	Method
and	O
dual	Method
attention	Method
mechanism	Method
are	O
proposed	O
to	O
operate	O
local	O
and	O
global	O
addressing	O
between	O
the	O
content	O
and	O
the	O
field	O
information	O
of	O
a	O
structured	O
table	O
.	O
(	O
3	O
)	O
Experiments	O
on	O
WIKIBIO	Material
dataset	Material
show	O
that	O
our	O
model	O
achieves	O
substantial	O
improvement	O
over	O
baselines	O
.	O
section	O
:	O
Related	O
Work	O
Most	O
generation	Task
systems	O
can	O
be	O
divided	O
into	O
two	O
independent	O
modules	O
:	O
(	O
1	O
)	O
content	O
selection	Task
involves	O
choosing	O
a	O
subset	O
of	O
relevant	O
records	O
in	O
a	O
table	O
to	O
talk	O
about	O
.	O
(	O
2	Task
)	Task
surface	Task
realization	Task
is	O
concerned	O
with	O
generating	Task
natural	Task
language	Task
descriptions	Task
for	O
this	O
subset	O
.	O
Many	O
approaches	O
have	O
been	O
proposed	O
to	O
learn	O
the	O
individual	O
modules	O
.	O
For	O
content	Task
selection	Task
module	Task
,	O
one	O
approach	O
builds	O
a	O
content	Method
selection	Method
model	Method
by	O
aligning	O
records	O
and	O
sentences	O
[	O
reference	O
][	O
reference	O
]	O
.	O
A	O
hierarchical	Method
semi	Method
-	Method
Markov	Method
method	Method
is	O
proposed	O
by	O
[	O
reference	O
]	O
which	O
first	O
associates	O
the	O
text	O
sequences	O
to	O
corresponding	O
records	O
and	O
then	O
generates	O
corresponding	O
descriptions	O
from	O
these	O
records	O
.	O
Surface	Task
realization	Task
is	O
often	O
treated	O
as	O
a	O
concept	Task
-	Task
to	Task
-	Task
text	Task
generation	Task
task	Task
from	O
a	O
given	O
representation	O
.	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
utilize	O
various	O
linguistic	O
features	O
to	O
train	O
sentence	Method
planners	Method
for	O
sentence	O
generation	Task
.	O
Context	Method
-	Method
free	Method
grammars	Method
are	O
also	O
used	O
to	O
generate	O
natural	O
language	O
sentences	O
from	O
formal	Method
meaning	Method
representations	Method
[	O
reference	O
][	O
reference	O
]	O
.	O
Other	O
effective	O
approaches	O
include	O
hybrid	O
alignment	O
tree	O
[	O
reference	O
]	O
,	O
tree	Method
conditional	Method
random	Method
fields	Method
[	O
reference	O
]	O
,	O
tree	Method
adjoining	Method
grammar	Method
[	O
reference	O
]	O
and	O
template	Method
extraction	Method
in	O
a	O
log	Method
-	Method
linear	Method
framework	Method
[	O
reference	O
]	O
.	O
Recent	O
work	O
combines	O
content	Task
selection	Task
and	O
surface	Task
realization	Task
in	O
a	O
unified	O
framework	O
(	O
Ratnaparkhi	O
2002	O
;	O
[	O
reference	O
][	O
reference	O
]	O
Our	O
model	O
borrowed	O
the	O
idea	O
of	O
representing	O
a	O
structured	O
table	O
by	O
its	O
field	O
and	O
content	O
information	O
from	O
(	O
Lebret	O
,	O
Grangier	O
,	O
and	O
Auli	O
2016	O
)	O
.	O
However	O
,	O
their	O
n	Method
-	Method
gram	Method
model	Method
is	O
inefficient	O
to	O
model	O
long	O
-	O
range	O
dependencies	O
while	O
generating	O
descriptions	O
.	O
[	O
reference	O
]	O
also	O
proposed	O
a	O
seq2seq	Method
model	O
with	O
an	O
aligner	O
between	O
weather	O
records	O
and	O
weather	O
broadcast	O
.	O
The	O
model	O
used	O
one	Method
-	Method
hot	Method
encoding	Method
to	O
represent	O
the	O
weather	O
records	O
as	O
they	O
are	O
relatively	O
simple	O
and	O
highly	O
structured	O
.	O
However	O
,	O
the	O
model	O
is	O
not	O
capable	O
to	O
represent	O
the	O
tables	O
with	O
complex	O
structure	O
like	O
Wikipedia	O
infoboxes	O
.	O
section	O
:	O
Task	O
Definition	O
We	O
model	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
in	O
an	O
end	O
-	O
to	O
-	O
end	O
structure	O
-	O
aware	O
seq2seq	Method
framework	O
.	O
The	O
given	O
table	O
T	O
can	O
be	O
viewed	O
as	O
a	O
combination	O
of	O
n	O
field	O
-	O
value	O
records	O
{	O
R	O
1	O
,	O
R	O
2	O
,	O
·	O
·	O
·	O
,	O
R	O
n	O
}	O
.	O
Each	O
record	O
R	O
i	O
consists	O
of	O
a	O
sequence	O
of	O
words	O
The	O
output	O
of	O
the	O
model	O
is	O
the	O
generated	O
description	O
S	O
for	O
table	O
T	O
which	O
contains	O
p	O
tokens	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
·	O
·	O
·	O
,	O
w	O
p	O
}	O
with	O
w	O
t	O
being	O
the	O
word	O
at	O
time	O
t.	O
We	O
formulate	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
as	O
the	O
inference	Task
over	O
a	O
probabilistic	Method
model	Method
.	O
The	O
goal	O
of	O
the	O
inference	Task
is	O
to	O
generate	O
a	O
sequence	O
w	O
*	O
1:p	O
which	O
maximizes	O
P	O
(	O
w	O
1:p	O
|R	O
1:n	O
)	O
.	O
bedding	O
for	O
a	O
segment	O
of	O
words	O
in	O
the	O
field	O
content	O
.	O
The	O
field	Method
embedding	Method
is	O
a	O
key	O
point	O
to	O
label	O
each	O
word	O
in	O
the	O
field	O
content	O
by	O
its	O
corresponding	O
field	O
name	O
and	O
occurrence	O
in	O
the	O
table	O
.	O
[	O
reference	O
]	O
represented	O
the	O
field	O
embeddding	O
Z	O
w	O
=	O
{	O
f	O
w	O
;	O
p	O
w	O
}	O
for	O
a	O
word	O
w	O
in	O
the	O
table	O
with	O
corresponding	O
field	O
name	O
f	O
w	O
and	O
position	O
information	O
p	O
w	O
.	O
The	O
position	O
information	O
can	O
be	O
further	O
represented	O
as	O
a	O
tuple	O
(	O
p	O
which	O
includes	O
the	O
positions	O
of	O
the	O
token	O
w	O
counted	O
from	O
the	O
begining	O
and	O
the	O
end	O
of	O
the	O
field	O
respectively	O
.	O
So	O
the	O
field	Method
embedding	Method
of	Method
token	Method
w	Method
is	O
extended	O
to	O
a	O
triple	O
:	O
As	O
shown	O
in	O
Fig	O
2	O
,	O
the	O
infobox	O
of	O
George	O
Mikell	O
contains	O
several	O
field	O
-	O
value	O
records	O
,	O
the	O
field	O
content	O
for	O
the	O
record	O
(	O
birthname	O
,	O
Jurgis	O
Mikelatitis	O
)	O
is	O
'	O
Jurgis	O
Mikelatitis	O
'	O
.	O
The	O
word	O
'	O
Jurgis	O
'	O
is	O
the	O
first	O
token	O
counted	O
from	O
the	O
beginning	O
of	O
the	O
field	O
'	O
birthname	O
'	O
and	O
also	O
the	O
second	O
token	O
counted	O
from	O
the	O
end	O
.	O
So	O
the	O
field	Method
embedding	Method
for	O
the	O
word	O
'	O
Jurgis	O
'	O
is	O
described	O
as	O
{	O
birthname	O
;	O
1	O
;	O
2}.	O
Each	O
token	O
in	O
the	O
table	O
has	O
an	O
unique	O
field	O
embedding	O
even	O
if	O
there	O
exists	O
two	O
same	O
words	O
in	O
the	O
same	O
field	O
due	O
to	O
the	O
unique	O
(	O
field	O
,	O
position	O
)	O
pair	O
.	O
section	O
:	O
Field	Method
-	Method
gating	Method
Table	Method
Encoder	Method
The	O
table	Method
encoder	Method
aims	O
to	O
encode	O
each	O
word	O
d	O
j	O
in	O
the	O
table	O
together	O
with	O
its	O
field	O
embedding	O
Z	O
dj	O
into	O
the	O
hidden	O
state	O
h	O
j	O
using	O
LSTM	Method
encoder	O
.	O
We	O
present	O
a	O
novel	O
field	O
-	O
gating	O
LSTM	Method
unit	O
to	O
incorporate	O
field	O
information	O
into	O
table	Task
encoding	Task
.	O
LSTM	Method
is	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
architecture	O
which	O
uses	O
a	O
vector	O
of	O
cell	O
state	O
c	O
t	O
and	O
a	O
set	O
of	O
element	O
-	O
wise	O
multiplication	O
gates	O
to	O
control	O
how	O
information	O
is	O
stored	O
,	O
forgotten	O
and	O
exploited	O
inside	O
the	O
network	O
.	O
Following	O
the	O
design	O
for	O
an	O
LSTM	Method
cell	O
in	O
[	O
reference	O
]	O
,	O
the	O
architecture	O
used	O
in	O
the	O
table	Method
encoder	Method
is	O
defined	O
by	O
following	O
equations	O
:	O
where	O
i	O
t	O
,	O
f	O
t	O
,	O
o	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
are	O
input	O
,	O
forget	O
and	O
output	O
gates	O
respectively	O
,	O
andĉ	O
t	O
and	O
c	O
t	O
are	O
proposed	O
cell	O
value	O
and	O
true	O
cell	O
value	O
in	O
time	O
t.	O
n	O
is	O
the	O
hidden	O
size	O
.	O
To	O
make	O
better	O
understanding	O
of	O
the	O
structure	O
of	O
a	O
table	O
,	O
the	O
field	O
information	O
should	O
also	O
be	O
encoded	O
into	O
the	O
encoder	Method
.	O
One	O
simple	O
way	O
is	O
to	O
take	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
and	O
corresponding	O
field	Method
embedding	Method
as	O
the	O
input	O
for	O
the	O
vanilla	O
LSTM	Method
unit	O
.	O
Actually	O
,	O
the	O
method	O
is	O
indeed	O
proved	O
to	O
be	O
useful	O
in	O
our	O
experiments	O
and	O
serves	O
as	O
a	O
baseline	O
for	O
comparison	O
.	O
However	O
,	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
and	O
field	Method
embedding	Method
only	O
treats	O
the	O
field	O
information	O
as	O
an	O
additional	O
label	O
of	O
certain	O
token	O
which	O
loses	O
the	O
structural	O
information	O
of	O
the	O
table	O
.	O
To	O
better	O
encode	O
the	O
structural	O
information	O
of	O
a	O
table	O
,	O
we	O
propose	O
a	O
field	Method
-	Method
gating	Method
variation	Method
on	O
the	O
vanilla	O
LSTM	Method
unit	O
to	O
update	O
the	O
cell	O
memory	O
by	O
a	O
field	O
gate	O
and	O
its	O
corresponding	O
field	O
value	O
.	O
The	O
field	O
-	O
gating	O
cell	O
state	O
is	O
described	O
as	O
follows	O
:	O
where	O
z	O
t	O
is	O
the	O
field	O
embedding	O
described	O
before	O
,	O
l	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
is	O
the	O
field	O
gate	O
to	O
determine	O
how	O
much	O
field	O
information	O
should	O
be	O
kept	O
in	O
the	O
cell	O
memory	O
,	O
ẑ	O
t	O
is	O
the	O
proposed	O
field	O
value	O
corresponding	O
to	O
field	O
gate	O
.	O
The	O
cell	O
state	O
c	O
t	O
is	O
updated	O
from	O
the	O
original	O
c	O
t	O
by	O
incorporating	O
field	O
information	O
of	O
the	O
table	O
.	O
section	O
:	O
Description	Method
Decoder	Method
with	O
Dual	Method
Attention	Method
To	O
conduct	O
local	Task
and	Task
global	Task
addressing	Task
towards	O
the	O
structured	O
table	O
,	O
we	O
use	O
LSTM	Method
architecture	O
with	O
dual	Method
attention	Method
mechanism	Method
as	O
our	O
description	Method
generator	Method
.	O
As	O
defined	O
in	O
the	O
equation	O
1	O
,	O
the	O
generated	O
token	O
w	O
t	O
at	O
time	O
t	O
in	O
the	O
decoder	O
is	O
predicated	O
based	O
on	O
all	O
the	O
previously	O
generated	O
tokens	O
w	O
<	O
t	O
before	O
w	O
t	O
,	O
the	O
hidden	O
states	O
H	O
=	O
{	O
h	O
t	O
}	O
L	O
t=1	O
of	O
the	O
table	Method
encoder	Method
and	O
the	O
field	Method
embeddings	Method
Z	O
=	O
{	O
z	O
t	O
}	O
L	O
t=1	O
.	O
To	O
be	O
more	O
specific	O
:	O
where	O
s	O
t	O
is	O
the	O
t	O
-	O
th	O
hidden	O
state	O
of	O
the	O
decoder	O
calculated	O
by	O
the	O
LSTM	Method
unit	O
.	O
The	O
computational	O
details	O
can	O
be	O
referred	O
in	O
Equation	O
3	O
,	O
4	O
and	O
5	O
.	O
a	O
t	O
is	O
the	O
attention	O
vector	O
which	O
is	O
widely	O
used	O
in	O
many	O
applications	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Vanilla	Method
attention	Method
mechanism	Method
is	O
proposed	O
to	O
encode	O
the	O
semantic	O
relevance	O
between	O
the	O
encoder	O
states	O
{	O
h	O
t	O
}	O
L	O
t=1	O
and	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
time	O
t.	O
The	O
attention	O
vector	O
is	O
usually	O
represented	O
by	O
the	O
weighted	O
sum	O
of	O
encoder	O
hidden	O
states	O
.	O
However	O
,	O
the	O
word	Method
level	Method
attention	Method
described	O
above	O
can	O
only	O
capture	O
the	O
semantic	O
relevance	O
between	O
generated	O
tokens	O
and	O
the	O
content	O
information	O
in	O
the	O
table	O
,	O
ignoring	O
the	O
structure	O
information	O
of	O
the	O
table	O
.	O
To	O
fully	O
utilize	O
the	O
structure	O
information	O
,	O
we	O
propose	O
a	O
higher	O
level	O
attention	O
over	O
generated	O
tokens	O
and	O
the	O
field	O
embedding	O
of	O
the	O
table	O
.	O
Field	O
level	O
attention	O
can	O
locate	O
the	O
particular	O
field	O
-	O
value	O
record	O
which	O
should	O
be	O
focused	O
on	O
while	O
generating	O
next	O
token	O
in	O
the	O
description	O
by	O
modeling	O
the	O
relevance	O
between	O
all	O
field	O
embeddings	O
{	O
z	O
t	O
}	O
L	O
t=1	O
and	O
the	O
decoder	O
state	O
s	O
t	O
at	O
t	O
-	O
th	O
time	O
.	O
Field	O
level	O
attention	O
weight	O
β	Method
ti	Method
is	O
presented	O
as	O
Equation	O
13	O
.	O
We	O
use	O
the	O
same	O
relevant	O
score	O
function	O
g	O
(	O
s	O
t	O
,	O
z	O
i	O
)	O
as	O
equation	O
12	O
.	O
Dual	O
attention	O
weight	O
γ	O
t	O
is	O
the	O
element	O
-	O
wise	O
production	O
between	O
field	O
level	O
attention	O
weight	O
β	O
t	O
and	O
word	Method
level	Method
attention	Method
weight	O
α	O
t	O
.	O
The	O
dual	O
attention	O
vector	O
a	O
t	O
is	O
updated	O
as	O
the	O
weighted	O
sum	O
of	O
encoder	O
states	O
{	O
h	O
t	O
}	O
t=1	O
by	O
γ	O
t	O
(	O
Equation	O
15	O
)	O
:	O
Furthermore	O
,	O
we	O
utilize	O
a	O
post	Method
-	Method
process	Method
operation	Method
for	O
the	O
generated	O
unknown	O
(	O
UNK	O
)	O
tokens	O
to	O
alleviate	O
the	O
out	Task
-	Task
ofvocabulary	Task
(	O
OOV	Task
)	O
problem	O
.	O
We	O
replace	O
a	O
specific	O
generated	O
UNK	O
token	O
with	O
the	O
most	O
relevant	O
token	O
in	O
the	O
corresponding	O
table	O
according	O
to	O
the	O
related	O
dual	O
attention	O
matrix	O
.	O
section	O
:	O
Local	Task
and	Task
Global	Task
Addressing	Task
Local	O
and	O
global	O
addressing	O
determine	O
which	O
part	O
of	O
the	O
table	O
should	O
be	O
more	O
focused	O
on	O
in	O
different	O
steps	O
of	O
description	O
generation	Task
.	O
The	O
two	O
scopes	O
of	O
addressings	O
play	O
a	O
very	O
important	O
role	O
in	O
understanding	O
and	O
representing	O
the	O
innerstructure	O
of	O
a	O
table	O
.	O
Next	O
we	O
will	O
introduce	O
how	O
our	O
model	O
conducts	O
local	Task
and	Task
global	Task
addressing	Task
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
with	O
the	O
help	O
of	O
Fig	O
3	O
.	O
Local	Task
addressing	Task
:	O
A	O
table	O
can	O
be	O
treated	O
as	O
a	O
set	O
of	O
fieldvalue	O
records	O
.	O
Local	Task
addressing	Task
tends	O
to	O
encode	O
the	O
table	O
content	O
inside	O
each	O
record	O
.	O
The	O
value	O
in	O
each	O
field	O
-	O
value	O
record	O
is	O
a	O
sequence	O
of	O
words	O
which	O
contains	O
2.7	O
tokens	O
on	O
average	O
.	O
Some	O
records	O
in	O
the	O
Wikipedia	O
infoboxes	O
even	O
contain	O
several	O
phrases	O
or	O
sentences	O
.	O
Previous	O
models	O
which	O
used	O
one	Method
-	Method
hot	Method
encoding	Method
or	O
statistical	Method
language	Method
model	Method
to	O
encode	O
field	O
content	O
are	O
inefficient	O
to	O
capture	O
the	O
semantic	O
relevance	O
between	O
words	O
inside	O
a	O
field	O
.	O
The	O
seq2seq	Method
structure	O
itself	O
has	O
a	O
strong	O
ability	O
to	O
model	O
the	O
context	O
of	O
a	O
piece	O
of	O
words	O
.	O
For	O
one	O
thing	O
,	O
the	O
LSTM	Method
encoder	O
can	O
capture	O
longrange	O
dependencies	O
between	O
words	O
in	O
the	O
table	O
.	O
For	O
another	O
,	O
the	O
word	Method
level	Method
attention	Method
of	O
the	O
proposed	O
dual	Method
attention	Method
mechanism	Method
can	O
also	O
build	O
a	O
connection	O
between	O
the	O
words	O
in	O
the	O
description	O
and	O
the	O
tokens	O
in	O
the	O
table	O
.	O
The	O
generated	O
word	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
refers	O
to	O
the	O
word	O
'	O
actor	O
'	O
in	O
the	O
'	O
Occupation	O
'	O
field	O
.	O
Global	Task
addressing	Task
:	O
The	O
goal	O
of	O
local	Task
addressing	Task
is	O
to	O
represent	O
inner	O
-	O
record	O
information	O
while	O
global	Task
addressing	Task
aims	O
to	O
model	O
inter	O
-	O
record	O
relevance	O
within	O
the	O
table	O
.	O
For	O
example	O
,	O
it	O
's	O
noteworthy	O
that	O
the	O
generated	O
token	O
'	O
actor	O
'	O
in	O
Fig	O
3	O
is	O
mapped	O
to	O
the	O
'	O
occupation	O
'	O
field	O
in	O
Table	O
2	O
.	O
Field	Method
-	Method
gating	Method
[	O
reference	O
]	O
.	O
We	O
should	O
make	O
it	O
clear	O
which	O
records	O
the	O
token	O
to	O
be	O
generated	O
is	O
focused	O
on	O
by	O
global	O
addressing	O
between	O
the	O
field	O
information	O
of	O
a	O
table	O
and	O
its	O
description	O
.	O
The	O
field	O
level	O
attention	O
of	O
dual	Method
attention	Method
mechanism	Method
is	O
introduced	O
to	O
determine	O
which	O
field	O
the	O
generator	O
focused	O
on	O
in	O
certain	O
time	O
step	O
.	O
Experiments	O
show	O
that	O
our	O
dual	Method
attention	Method
mechanism	Method
is	O
of	O
great	O
help	O
to	O
generate	O
description	O
from	O
certain	O
table	O
and	O
insensible	O
to	O
different	O
orders	O
of	O
table	O
records	O
.	O
section	O
:	O
Experiments	O
We	O
first	O
introduce	O
the	O
dataset	O
,	O
evaluation	Metric
metrics	Metric
and	O
experimental	O
setups	O
in	O
our	O
experiments	O
.	O
Then	O
we	O
compare	O
our	O
model	O
with	O
several	O
baselines	O
.	O
After	O
that	O
,	O
we	O
assess	O
the	O
performance	O
of	O
our	O
model	O
on	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O
Furthermore	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
the	O
disordered	O
tables	O
to	O
show	O
the	O
efficiency	O
of	O
global	Method
addressing	Method
mechanism	Method
.	O
section	O
:	O
Dataset	O
and	O
Evaluation	Metric
Metrics	Metric
We	O
use	O
WIKBIO	Material
dataset	Material
proposed	O
by	O
[	O
reference	O
]	O
as	O
the	O
benchmark	O
dataset	O
.	O
WIKBIO	Material
contains	O
728	O
,	O
321	O
articles	O
from	O
English	O
Wikipedia	O
[	O
reference	O
]	O
.	O
The	O
dataset	O
uses	O
the	O
first	O
sentence	O
of	O
each	O
article	O
as	O
the	O
description	O
of	O
the	O
corresponding	O
infobox	O
.	O
Table	O
1	O
summarizes	O
the	O
dataset	O
statistics	O
:	O
on	O
average	O
,	O
the	O
tokens	O
in	O
the	O
table	O
(	O
53.1	O
)	O
are	O
twice	O
as	O
long	O
as	O
those	O
in	O
the	O
first	O
sentence	O
(	O
26.1	O
)	O
.	O
9.5	O
tokens	O
in	O
the	O
description	O
text	O
also	O
occur	O
in	O
the	O
table	O
.	O
The	O
corpus	O
has	O
been	O
divided	O
in	O
to	O
training	O
(	O
80	O
%	O
)	O
,	O
testing	O
(	O
10	O
%	O
)	O
and	O
validation	Metric
(	O
10	O
%	O
)	O
sets	O
.	O
We	O
assess	O
the	O
generation	Task
quality	O
automatically	O
with	O
BLEU	Metric
-	Metric
4	Metric
and	O
ROUGE	Metric
-	Metric
4	Metric
(	O
F	Metric
measure	Metric
)	O
1	O
.	O
section	O
:	O
Baselines	O
We	O
compare	O
the	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
with	O
several	O
statistical	Method
language	Method
models	Method
and	O
the	O
vanilla	Method
encoder	Method
-	Method
decoder	Method
model	Method
.	O
The	O
baselines	O
are	O
listed	O
as	O
follows	O
:	O
•	O
KN	Method
:	O
The	O
Kneser	Method
-	Method
Ney	Method
(	O
KN	Method
)	O
model	O
is	O
a	O
widely	O
used	O
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
.	O
We	O
use	O
the	O
KenLM	Method
toolkit	Method
to	O
train	O
5	Method
-	Method
gram	Method
models	Method
without	O
pruning	Method
.	O
•	O
Template	O
KN	Method
:	O
Template	O
KN	Method
is	O
a	O
KN	Method
model	O
over	O
templates	O
which	O
also	O
serves	O
as	O
a	O
baseline	O
in	O
[	O
reference	O
]	O
.	O
The	O
model	O
replaces	O
the	O
words	O
occurring	O
in	O
both	O
the	O
table	O
and	O
the	O
training	O
sentences	O
with	O
a	O
special	O
token	O
reflecting	O
its	O
field	O
.	O
The	O
introduction	O
section	O
of	O
the	O
table	O
in	O
Fig	O
2	O
looks	O
as	O
follows	O
under	O
this	O
scheme	O
:	O
"	O
name	O
1	O
name	O
2	O
(	O
born	O
birthname	O
1	O
...	O
birthdate	O
3	O
)	O
is	O
a	O
LithuanianAustralian	O
occupation	O
1	O
and	O
occupation	O
3	O
best	O
known	O
for	O
his	O
performances	O
in	O
known	O
for	O
1	O
...	O
known	O
for	O
4	O
(	O
1961	O
)	O
and	O
known	O
for	O
5	O
...	O
known	O
for	O
7	O
(	O
1963	O
)	O
"	O
.	O
During	O
inference	Task
,	O
the	O
decoder	Method
is	O
constrained	O
to	O
emit	O
words	O
from	O
the	O
regular	O
vocabulary	O
or	O
special	O
tokens	O
occurring	O
in	O
the	O
input	O
table	O
.	O
•	O
NLM	Method
:	O
A	O
naive	Method
statistical	Method
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
for	O
comparison	O
.	O
The	O
model	O
uses	O
only	O
the	O
field	O
content	O
as	O
input	O
without	O
field	O
and	O
position	O
information	O
.	O
•	O
Table	O
NLM	Method
:	O
The	O
most	O
competitive	O
statistical	Method
language	Method
model	Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
which	O
includes	O
local	O
and	O
global	O
conditioning	O
over	O
the	O
table	O
by	O
integrating	O
related	O
field	O
and	O
position	Method
embedding	Method
into	O
the	O
table	Method
representation	Method
.	O
•	O
Vanilla	Method
Seq2seq	Method
:	O
The	O
vanilla	O
seq2seq	Method
neural	O
architecture	O
is	O
also	O
provided	O
as	O
a	O
strong	O
baseline	O
which	O
uses	O
the	O
concatenation	Method
of	Method
word	Method
embedding	Method
,	O
field	Method
embedding	Method
and	O
position	Method
embedding	Method
as	O
the	O
model	O
input	O
.	O
The	O
model	O
can	O
operate	O
local	Task
addressing	Task
over	O
the	O
table	O
by	O
the	O
natural	O
advantages	O
of	O
LSTM	Method
units	O
and	O
word	Method
level	Method
attention	Method
mechanism	O
.	O
section	O
:	O
Experiment	O
Setup	O
In	O
the	O
table	Task
encoding	Task
phase	Task
,	O
we	O
use	O
a	O
sequence	O
of	O
word	O
embeddings	O
and	O
their	O
corresponding	O
field	Method
and	Method
position	Method
embedding	Method
as	O
input	O
.	O
We	O
select	O
the	O
most	O
frequent	O
20	O
,	O
000	O
words	O
in	O
the	O
training	O
set	O
as	O
the	O
word	O
vocabulary	O
.	O
For	O
field	Task
embedding	Task
,	O
we	O
select	O
1480	O
fields	O
occurring	O
more	O
than	O
100	O
times	O
from	O
the	O
training	O
set	O
as	O
field	O
vocabulary	O
.	O
Additionally	O
,	O
we	O
filter	O
all	O
empty	O
fields	O
whose	O
values	O
are	O
none	O
while	O
feeding	O
field	O
information	O
to	O
the	O
network	O
.	O
We	O
also	O
limit	O
the	O
largest	O
position	O
number	O
as	O
30	O
.	O
Any	O
position	O
number	O
over	O
30	O
will	O
be	O
counted	O
as	O
30	O
.	O
While	O
generating	O
description	O
for	O
the	O
table	O
,	O
a	O
special	O
start	O
token	O
sos	O
is	O
feed	O
into	O
the	O
generator	O
in	O
the	O
beginning	O
of	O
the	O
Note	O
there	O
are	O
two	O
adjacent	O
'	O
belgium	O
's	O
in	O
'	O
birthplace	O
-	O
3	O
'	O
and	O
'	O
nationality	O
-	O
1	O
'	O
field	O
,	O
respectively	O
.	O
The	O
word	Method
level	Method
attention	Method
focuses	O
improperly	O
on	O
the	O
first	O
'	O
belgium	O
'	O
while	O
generating	O
'	O
a	O
belgian	O
film	O
director	O
'	O
.	O
In	O
contrast	O
,	O
the	O
field	O
level	O
attention	O
and	O
dual	O
attention	O
can	O
locate	O
the	O
second	O
'	O
belgium	O
'	O
properly	O
by	O
word	Method
-	Method
field	Method
modeling	Method
(	O
marked	O
in	O
the	O
black	O
boxes	O
)	O
.	O
Table	O
3	O
:	O
BLEU	Metric
-	Metric
4	Metric
and	O
ROUGE	Metric
-	Metric
4	Metric
for	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
(	O
last	O
three	O
rows	O
)	O
,	O
statistical	Method
language	Method
model	Method
(	O
first	O
four	O
rows	O
)	O
and	O
vanilla	O
seq2seq	Method
model	O
with	O
field	O
and	O
position	O
input	O
(	O
three	O
rows	O
in	O
the	O
middle	O
)	O
.	O
section	O
:	O
Model	O
decoding	Task
phase	Task
.	O
Then	O
we	O
use	O
the	O
last	O
generated	O
token	O
as	O
the	O
input	O
at	O
the	O
next	O
time	O
step	O
.	O
A	O
special	O
end	O
token	O
eos	O
is	O
used	O
to	O
mark	O
the	O
end	O
of	O
decoding	Task
.	O
We	O
also	O
restrict	O
the	O
generated	O
text	O
by	O
a	O
pre	O
-	O
defined	O
max	O
length	O
to	O
avoid	O
redundant	O
or	O
irrelevant	O
generation	Task
.	O
We	O
also	O
try	O
beam	Method
search	Method
with	O
beam	O
size	O
2	O
-	O
10	O
to	O
enhance	O
the	O
performance	O
.	O
We	O
use	O
grid	Method
search	Method
to	O
determine	O
the	O
parameters	O
of	O
our	O
model	O
.	O
The	O
detail	O
of	O
model	O
parameters	O
is	O
listed	O
in	O
Table	O
2	O
.	O
section	O
:	O
Generation	Task
Assessment	Task
The	O
assessment	O
for	O
description	O
generation	Task
is	O
listed	O
in	O
Table	O
3	O
.	O
We	O
have	O
following	O
observations	O
:	O
(	O
1	O
)	O
Neural	Method
network	Method
models	Method
perform	O
much	O
better	O
than	O
statistical	Method
language	Method
models	Method
.	O
Even	O
vanilla	O
seq2seq	Method
architecture	O
with	O
word	Method
level	Method
attention	Method
outperform	O
the	O
most	O
competitive	O
statistical	Method
model	Method
by	O
a	O
great	O
margin	O
.	O
(	O
2	O
)	O
The	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
can	O
further	O
improve	O
the	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
compared	O
with	O
the	O
competitive	O
vanilla	O
seq2seq	Method
.	O
Dual	Method
attention	Method
mechanism	Method
is	O
able	O
to	O
boost	O
the	O
model	O
performance	O
by	O
over	O
1	O
BLEU	Metric
compared	O
to	O
vanilla	Method
attention	Method
mechanism	Method
.	O
section	O
:	O
Research	O
on	O
Disordered	O
Tables	O
We	O
view	O
a	O
structured	O
table	O
as	O
a	O
set	O
of	O
field	O
-	O
value	O
records	O
and	O
then	O
feed	O
the	O
records	O
into	O
the	O
generator	O
sequentially	O
as	O
the	O
order	O
they	O
are	O
presented	O
in	O
the	O
table	O
.	O
The	O
order	O
of	O
records	O
can	O
guide	O
the	O
description	Method
generator	Method
to	O
produce	O
an	O
introduction	O
in	O
the	O
pre	O
-	O
defined	O
schemas	O
[	O
reference	O
]	O
.	O
However	O
,	O
not	O
all	O
the	O
tables	O
are	O
arranged	O
in	O
the	O
proper	O
order	O
.	O
So	O
global	O
addressing	O
between	O
the	O
generated	O
descriptions	O
and	O
the	O
records	O
of	O
the	O
table	O
is	O
necessary	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O
Furthermore	O
,	O
the	O
schemas	O
of	O
various	O
types	O
of	O
tables	O
differ	O
greatly	O
from	O
each	O
other	O
.	O
A	O
biography	O
about	O
a	O
politician	O
may	O
emphasize	O
his	O
or	O
her	O
social	O
activities	O
and	O
working	O
experience	O
while	O
a	O
biography	O
of	O
a	O
soccer	O
player	O
is	O
likely	O
to	O
highlight	O
which	O
team	O
he	O
or	O
she	O
used	O
to	O
serve	O
in	O
or	O
the	O
performance	O
in	O
his	O
or	O
her	O
career	O
.	O
To	O
cope	O
with	O
various	O
schemas	O
of	O
different	O
tables	O
,	O
it	O
's	O
essential	O
to	O
model	O
inter	O
-	O
record	O
information	O
within	O
the	O
tables	O
by	O
global	Method
addressing	Method
.	O
For	O
these	O
reasons	O
,	O
we	O
propose	O
a	O
pair	O
of	O
disordered	O
training	O
and	O
testing	O
set	O
based	O
on	O
WIKIBIO	Material
by	O
randomly	O
shuffling	O
the	O
records	O
of	O
a	O
infobox	O
.	O
For	O
example	O
,	O
the	O
order	O
of	O
several	O
records	O
in	O
a	O
specific	O
infobox	O
is	O
'	O
name	O
-	O
birthdateoccupation	O
-	O
spouse	O
'	O
,	O
we	O
randomly	O
shuffle	O
the	O
table	O
records	O
as	O
'	O
occupation	O
-	O
name	O
-	O
spouse	O
-	O
birthdate	O
'	O
,	O
without	O
changing	O
the	O
field	O
content	O
inside	O
the	O
'	O
occupation	O
'	O
,	O
'	O
name	O
'	O
,	O
'	O
spouse	O
'	O
and	O
'	O
birthdate	O
'	O
records	O
.	O
The	O
generated	O
descriptions	O
for	O
Binky	O
Jones	O
and	O
the	O
corresponding	O
reference	O
in	O
the	O
Wikipedia	O
.	O
Our	O
proposed	O
structaware	O
seq2seq	Method
model	O
can	O
generate	O
more	O
informative	O
and	O
accurate	O
description	O
compared	O
to	O
vanilla	O
seq2seq	Method
model	O
.	O
Table	O
4	O
shows	O
that	O
all	O
three	O
neural	Method
network	Method
models	Method
perform	O
not	O
as	O
good	O
as	O
before	O
,	O
which	O
means	O
the	O
order	O
of	O
table	O
records	O
is	O
an	O
essential	O
aspect	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O
However	O
,	O
the	O
BLEU	Metric
and	O
ROUGE	Metric
decreases	O
on	O
the	O
structureaware	O
seq2seq	Method
model	O
are	O
much	O
smaller	O
than	O
the	O
other	O
two	O
models	O
,	O
which	O
proves	O
the	O
efficiency	O
of	O
global	Method
addressing	Method
mechanism	Method
.	O
section	O
:	O
Model	O
section	O
:	O
Qualitative	Task
Analysis	Task
Analysis	Task
on	O
Dual	Task
Attention	Task
Dual	Method
attention	Method
mechanism	Method
models	O
the	O
relationship	O
between	O
the	O
generated	O
tokens	O
and	O
table	O
content	O
inside	O
each	O
record	O
by	O
word	Method
level	Method
attention	Method
while	O
encoding	O
the	O
relevance	O
of	O
generated	O
description	O
and	O
inter	O
-	O
record	O
information	O
within	O
the	O
table	O
by	O
field	O
level	O
attention	O
.	O
The	O
aggregation	O
of	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
can	O
model	O
more	O
precise	O
connection	O
between	O
the	O
table	O
and	O
its	O
generated	O
description	O
.	O
Fig	O
4	O
shows	O
an	O
example	O
of	O
the	O
three	O
attention	Method
mechanisms	Method
while	O
generating	O
a	O
piece	O
of	O
description	O
for	O
Frédéric	O
Fonteyne	O
based	O
on	O
his	O
Wikipedia	O
infobox	O
.	O
We	O
can	O
find	O
out	O
that	O
the	O
name	O
,	O
birthdate	O
,	O
nationality	O
and	O
occupation	O
information	O
contained	O
in	O
the	O
generated	O
sentence	O
can	O
properly	O
refer	O
to	O
the	O
related	O
table	O
content	O
by	O
the	O
aggregated	O
dual	O
attention	O
.	O
section	O
:	O
Case	O
Study	O
Fig	O
5	O
shows	O
the	O
generated	O
descriptions	O
for	O
different	O
variants	O
of	O
our	O
model	O
based	O
on	O
the	O
related	O
Wikipedia	O
infobox	O
.	O
All	O
three	O
neural	Method
network	Method
generators	Method
can	O
produce	O
coherent	O
and	O
understandable	O
sentences	O
with	O
the	O
help	O
of	O
local	Method
addressing	Method
mechanism	Method
.	O
All	O
of	O
them	O
contain	O
the	O
word	O
'	O
baseball	O
'	O
which	O
is	O
not	O
directly	O
mentioned	O
in	O
the	O
infobox	O
.	O
It	O
means	O
the	O
generators	O
deduce	O
from	O
table	O
content	O
that	O
Binky	O
Jones	O
is	O
a	O
baseball	O
player	O
.	O
However	O
,	O
the	O
two	O
vanilla	O
seq2seq	Method
models	O
also	O
generate	O
'	O
major	O
league	O
baseball	O
'	O
or	O
'	O
major	O
leagues	O
'	O
which	O
are	O
not	O
mentioned	O
in	O
the	O
table	O
and	O
probably	O
not	O
correct	O
.	O
Vanilla	O
seq2seq	Method
model	O
without	O
global	Method
addressing	Method
on	O
the	O
table	O
just	O
generates	O
the	O
most	O
possible	O
league	O
in	O
Wikipedia	O
for	O
a	O
baseball	O
player	O
to	O
play	O
in	O
.	O
Furthermore	O
,	O
the	O
two	O
biographies	O
generated	O
by	O
vanilla	O
seq2seq	Method
model	O
fail	O
to	O
contain	O
the	O
information	O
from	O
the	O
infobox	O
which	O
team	O
he	O
served	O
in	O
,	O
as	O
well	O
as	O
the	O
time	O
period	O
of	O
his	O
playing	O
in	O
that	O
team	O
.	O
The	O
biography	O
generated	O
by	O
our	O
proposed	O
structure	O
-	O
aware	O
seq2seq	Method
model	O
is	O
able	O
to	O
cover	O
nearly	O
all	O
the	O
information	O
mentioned	O
in	O
the	O
table	O
.	O
The	O
generated	O
segment	O
'	O
who	O
played	O
shortstop	O
from	O
april	O
15	O
to	O
april	O
27	O
for	O
the	O
brooklyn	O
robins	O
in	O
1924	O
'	O
(	O
15	O
words	O
)	O
includes	O
information	O
in	O
five	O
fields	O
of	O
the	O
table	O
:	O
'	O
position	O
'	O
,	O
'	O
debutdate	O
'	O
,	O
'	O
finaldate	O
'	O
,	O
'	O
debutteam	O
'	O
and	O
'	O
finalteam	O
'	O
,	O
which	O
is	O
achieved	O
by	O
the	O
global	O
addressing	O
between	O
the	O
fields	O
and	O
the	O
generated	O
tokens	O
.	O
section	O
:	O
Conclusions	O
We	O
propose	O
a	O
structure	O
-	O
aware	O
seq2seq	Method
architecture	O
to	O
encode	O
both	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
for	O
table	O
-	O
to	O
-	O
text	Task
generation	Task
.	O
The	O
model	O
consists	O
of	O
field	Method
-	Method
gating	Method
encoder	Method
and	O
description	Method
generator	Method
with	O
dual	Method
attention	Method
.	O
We	O
add	O
a	O
field	O
gate	O
to	O
the	O
encoder	O
LSTM	Method
unit	O
to	O
incorporate	O
the	O
field	O
information	O
.	O
Furthermore	O
,	O
dual	Method
attention	Method
mechanism	Method
which	O
contains	O
word	Method
level	Method
attention	Method
and	O
field	O
level	O
attention	O
can	O
operate	O
local	O
and	O
global	O
addressing	O
to	O
the	O
content	O
and	O
the	O
structure	O
of	O
a	O
table	O
.	O
A	O
series	O
of	O
visualizations	O
,	O
case	O
studies	O
and	O
generation	Task
assessments	O
show	O
that	O
our	O
model	O
outperforms	O
the	O
competitive	O
baselines	O
by	O
a	O
large	O
margin	O
.	O
section	O
:	O
section	O
:	O
Acknowledgments	O
Our	O
work	O
is	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
Grant	O
No.2017YFB1002101	O
and	O
project	O
61772040	O
supported	O
by	O
NSFC	O
.	O
The	O
corresponding	O
authors	O
of	O
this	O
paper	O
are	O
Baobao	O
Chang	O
and	O
Zhifang	O
Sui	O
.	O
section	O
:	O
