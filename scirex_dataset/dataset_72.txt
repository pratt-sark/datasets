document	O
:	O
FractalNet	Method
:	O
Ultra	Method
-	Method
Deep	Method
Neural	Method
Networks	Method
without	O
Residuals	O
We	O
introduce	O
a	O
design	Method
strategy	Method
for	O
neural	Method
network	Method
macro	Method
-	Method
architecture	Method
based	O
on	O
self	O
-	O
similarity	O
.	O
Repeated	O
application	O
of	O
a	O
simple	O
expansion	Method
rule	Method
generates	O
deep	Method
networks	Method
whose	O
structural	O
layouts	O
are	O
precisely	O
truncated	Method
fractals	Method
.	O
These	O
networks	O
contain	O
interacting	O
subpaths	O
of	O
different	O
lengths	O
,	O
but	O
do	O
not	O
include	O
any	O
pass	O
-	O
through	O
or	O
residual	O
connections	O
;	O
every	O
internal	O
signal	O
is	O
transformed	O
by	O
a	O
filter	O
and	O
nonlinearity	O
before	O
being	O
seen	O
by	O
subsequent	O
layers	O
.	O
In	O
experiments	O
,	O
fractal	Method
networks	Method
match	O
the	O
excellent	O
performance	O
of	O
standard	O
residual	Method
networks	Method
on	O
both	O
CIFAR	Task
and	O
ImageNet	Task
classification	Task
tasks	Task
,	O
thereby	O
demonstrating	O
that	O
residual	Method
representations	Method
may	O
not	O
be	O
fundamental	O
to	O
the	O
success	O
of	O
extremely	O
deep	Method
convolutional	Method
neural	Method
networks	Method
.	O
Rather	O
,	O
the	O
key	O
may	O
be	O
the	O
ability	O
to	O
transition	O
,	O
during	O
training	O
,	O
from	O
effectively	O
shallow	O
to	O
deep	O
.	O
We	O
note	O
similarities	O
with	O
student	O
-	O
teacher	O
behavior	O
and	O
develop	O
drop	Method
-	Method
path	Method
,	O
a	O
natural	O
extension	Method
of	Method
dropout	Method
,	O
to	O
regularize	O
co	Method
-	Method
adaptation	Method
of	Method
subpaths	Method
in	O
fractal	Method
architectures	Method
.	O
Such	O
regularization	Method
allows	O
extraction	O
of	O
high	O
-	O
performance	O
fixed	O
-	O
depth	Metric
subnetworks	O
.	O
Additionally	O
,	O
fractal	Method
networks	Method
exhibit	O
an	O
anytime	O
property	O
:	O
shallow	O
subnetworks	O
provide	O
a	O
quick	O
answer	O
,	O
while	O
deeper	O
subnetworks	O
,	O
with	O
higher	O
latency	O
,	O
provide	O
a	O
more	O
accurate	O
answer	O
.	O
darkbluergb0.0	O
,	O
0.0	O
,	O
0.55	O
section	O
:	O
Introduction	O
Residual	Method
networks	Method
he2015deep	O
,	O
or	O
ResNets	Method
,	O
lead	O
a	O
recent	O
and	O
dramatic	O
increase	O
in	O
both	O
depth	Metric
and	O
accuracy	Metric
of	O
convolutional	Method
neural	Method
networks	Method
,	O
facilitated	O
by	O
constraining	O
the	O
network	O
to	O
learn	O
residuals	O
.	O
ResNet	Method
variants	Method
he2015deep	O
,	O
he2016identity	O
,	O
huang2016stochasticdepth	O
and	O
related	O
architectures	O
srivastava2015highway	O
employ	O
the	O
common	O
technique	O
of	O
initializing	O
and	O
anchoring	O
,	O
via	O
a	O
pass	Method
-	Method
through	Method
channel	Method
,	O
a	O
network	O
to	O
the	O
identity	O
function	O
.	O
Training	O
now	O
differs	O
in	O
two	O
respects	O
.	O
First	O
,	O
the	O
objective	O
changes	O
to	O
learning	O
residual	O
outputs	O
,	O
rather	O
than	O
unreferenced	O
absolute	O
mappings	O
.	O
Second	O
,	O
these	O
networks	O
exhibit	O
a	O
type	O
of	O
deep	O
supervision	O
lee2014deeply	O
,	O
as	O
near	O
-	O
identity	O
layers	O
effectively	O
reduce	O
distance	O
to	O
the	O
loss	O
.	O
speculate	O
that	O
the	O
former	O
,	O
the	O
residual	Method
formulation	Method
itself	O
,	O
is	O
crucial	O
.	O
include	O
/	O
fig	O
-	O
overview	O
We	O
show	O
otherwise	O
,	O
by	O
constructing	O
a	O
competitive	Method
extremely	Method
deep	Method
architecture	Method
that	O
does	O
not	O
rely	O
on	O
residuals	O
.	O
Our	O
design	O
principle	O
is	O
pure	O
enough	O
to	O
communicate	O
in	O
a	O
single	O
word	O
,	O
fractal	O
,	O
and	O
a	O
simple	O
diagram	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
Yet	O
,	O
fractal	Method
networks	Method
implicitly	O
recapitulate	O
many	O
properties	O
hard	O
-	O
wired	O
into	O
previous	O
successful	O
architectures	O
.	O
Deep	Method
supervision	Method
not	O
only	O
arises	O
automatically	O
,	O
but	O
also	O
drives	O
a	O
type	O
of	O
student	Task
-	Task
teacher	Task
learning	Task
ba2014dodeep	O
,	O
urban2016dodeepsfollowup	O
internal	O
to	O
the	O
network	O
.	O
Modular	Method
building	Method
blocks	Method
of	O
other	O
designs	O
szegedy2015inception	O
,	O
liao2015competitive	O
resemble	O
special	O
cases	O
of	O
a	O
fractal	Method
network	Method
â€™s	Method
nested	Method
substructure	Method
.	O
For	O
fractal	Method
networks	Method
,	O
simplicity	O
of	O
training	O
mirrors	O
simplicity	O
of	O
design	O
.	O
A	O
single	O
loss	O
,	O
attached	O
to	O
the	O
final	O
layer	O
,	O
suffices	O
to	O
drive	O
internal	O
behavior	O
mimicking	O
deep	Task
supervision	Task
.	O
Parameters	O
are	O
randomly	O
initialized	O
.	O
As	O
they	O
contain	O
subnetworks	O
of	O
many	O
depths	O
,	O
fractal	Method
networks	Method
are	O
robust	O
to	O
choice	O
of	O
overall	O
depth	Metric
;	O
make	O
them	O
deep	O
enough	O
and	O
training	O
will	O
carve	O
out	O
a	O
useful	O
assembly	O
of	O
subnetworks	O
.	O
The	O
entirety	O
of	O
emergent	O
behavior	O
resulting	O
from	O
a	O
fractal	Method
design	Method
may	O
erode	O
the	O
need	O
for	O
recent	O
engineering	Method
tricks	Method
intended	O
to	O
achieve	O
similar	O
effects	O
.	O
These	O
tricks	O
include	O
residual	O
functional	O
forms	O
with	O
identity	Method
initialization	Method
,	O
manual	Method
deep	Method
supervision	Method
,	O
hand	Method
-	Method
crafted	Method
architectural	Method
modules	Method
,	O
and	O
student	Method
-	Method
teacher	Method
training	Method
regimes	Method
.	O
Section	O
[	O
reference	O
]	O
reviews	O
this	O
large	O
body	O
of	O
related	O
techniques	O
.	O
Hybrid	Method
designs	Method
could	O
certainly	O
integrate	O
any	O
of	O
them	O
with	O
a	O
fractal	Method
architecture	Method
;	O
we	O
leave	O
open	O
the	O
question	O
of	O
the	O
degree	O
to	O
which	O
such	O
hybrids	O
are	O
synergistic	O
.	O
Our	O
main	O
contribution	O
is	O
twofold	O
:	O
We	O
introduce	O
FractalNet	Method
,	O
the	O
first	O
simple	O
alternative	O
to	O
ResNet	Method
.	O
FractalNet	Method
shows	O
that	O
explicit	Method
residual	Method
learning	Method
is	O
not	O
a	O
requirement	O
for	O
building	O
ultra	Method
-	Method
deep	Method
neural	Method
networks	Method
.	O
Through	O
analysis	O
and	O
experiments	O
,	O
we	O
elucidate	O
connections	O
between	O
FractalNet	Method
and	O
an	O
array	O
of	O
phenomena	O
engineered	O
into	O
previous	O
deep	Method
network	Method
designs	Method
.	O
As	O
an	O
additional	O
contribution	O
,	O
we	O
develop	O
drop	Method
-	Method
path	Method
,	O
a	O
novel	O
regularization	Method
protocol	Method
for	O
ultra	Task
-	Task
deep	Task
fractal	Task
networks	Task
.	O
Without	O
data	Method
augmentation	Method
,	O
fractal	Method
networks	Method
,	O
trained	O
with	O
drop	Method
-	Method
path	Method
and	Method
dropout	Method
dropout	Method
,	O
exceed	O
the	O
performance	O
of	O
residual	Method
networks	Method
regularized	O
via	O
stochastic	O
depth	Metric
huang2016stochasticdepth	O
.	O
Though	O
,	O
like	O
stochastic	O
depth	Metric
,	O
it	O
randomly	O
removes	O
macro	O
-	O
scale	O
components	O
,	O
drop	Method
-	Method
path	Method
further	O
exploits	O
our	O
fractal	O
structure	O
in	O
choosing	O
which	O
components	O
to	O
disable	O
.	O
Drop	Method
-	Method
path	Method
constitutes	O
not	O
only	O
a	O
regularization	Method
strategy	Method
,	O
but	O
also	O
provides	O
means	O
of	O
optionally	O
imparting	O
fractal	Method
networks	Method
with	O
anytime	O
behavior	O
.	O
A	O
particular	O
schedule	O
of	O
dropped	O
paths	O
during	O
learning	O
prevents	O
subnetworks	O
of	O
different	O
depths	O
from	O
co	O
-	O
adapting	O
.	O
As	O
a	O
consequence	O
,	O
both	O
shallow	O
and	O
deep	O
subnetworks	O
must	O
individually	O
produce	O
correct	O
output	O
.	O
Querying	O
a	O
shallow	O
subnetwork	O
thus	O
yields	O
a	O
quick	O
and	O
moderately	O
accurate	O
result	O
in	O
advance	O
of	O
completion	O
of	O
the	O
full	Method
network	Method
.	O
Section	O
[	O
reference	O
]	O
elaborates	O
the	O
technical	O
details	O
of	O
fractal	Method
networks	Method
and	O
drop	Method
-	Method
path	Method
.	O
Section	O
[	O
reference	O
]	O
provides	O
experimental	O
comparisons	O
to	O
residual	Method
networks	Method
across	O
the	O
CIFAR	O
-	O
10	O
,	O
CIFAR	O
-	O
100	O
CIFAR	O
,	O
SVHN	Material
SVHN	Material
,	O
and	O
ImageNet	O
deng2009imagenet	O
datasets	O
.	O
We	O
also	O
evaluate	O
regularization	Method
and	Method
data	Method
augmentation	Method
strategies	Method
,	O
investigate	O
subnetwork	O
student	O
-	O
teacher	O
behavior	O
during	O
training	O
,	O
and	O
benchmark	O
anytime	Method
networks	Method
obtained	O
using	O
drop	Method
-	Method
path	Method
.	O
Section	O
[	O
reference	O
]	O
provides	O
synthesis	O
.	O
By	O
virtue	O
of	O
encapsulating	O
many	O
known	O
,	O
yet	O
seemingly	O
distinct	O
,	O
design	O
principles	O
,	O
self	O
-	O
similar	O
structure	O
may	O
materialize	O
as	O
a	O
fundamental	O
component	O
of	O
neural	Method
architectures	Method
.	O
section	O
:	O
Related	O
Work	O
Deepening	Method
feed	Method
-	Method
forward	Method
neural	Method
networks	Method
has	O
generally	O
returned	O
dividends	O
in	O
performance	O
.	O
A	O
striking	O
example	O
within	O
the	O
computer	Task
vision	Task
community	Task
is	O
the	O
improvement	O
on	O
the	O
ImageNet	O
deng2009imagenet	O
classification	Task
task	Task
when	O
transitioning	O
from	O
AlexNet	Method
alexNet12	Method
to	O
VGG	Method
vgg16	Method
to	O
GoogLeNet	Method
szegedy2015inception	O
to	O
ResNet	Method
he2015deep	O
.	O
Unfortunately	O
,	O
greater	O
depth	Metric
also	O
makes	O
training	Task
more	O
challenging	O
,	O
at	O
least	O
when	O
employing	O
a	O
first	Method
-	Method
order	Method
optimization	Method
method	Method
with	O
randomly	O
initialized	O
layers	O
.	O
As	O
the	O
network	O
grows	O
deeper	O
and	O
more	O
non	O
-	O
linear	O
,	O
the	O
linear	Method
approximation	Method
of	O
a	O
gradient	O
step	O
becomes	O
increasingly	O
inappropriate	O
.	O
Desire	O
to	O
overcome	O
these	O
difficulties	O
drives	O
research	O
on	O
both	O
optimization	Method
techniques	Method
and	O
network	Method
architectures	Method
.	O
On	O
the	O
optimization	Task
side	Task
,	O
much	O
recent	O
work	O
yields	O
improvements	O
.	O
To	O
prevent	O
vanishing	O
gradients	O
,	O
ReLU	O
activation	O
functions	O
now	O
widely	O
replace	O
sigmoid	Method
and	O
tanh	Method
units	Method
nair2010rectified	O
.	O
This	O
subject	O
remains	O
an	O
area	O
of	O
active	O
inquiry	O
,	O
with	O
various	O
tweaks	O
on	O
ReLUs	O
,	O
e.g.	O
PReLUs	O
he2015prelu	O
,	O
and	O
ELUs	O
elu	O
.	O
Even	O
with	O
ReLUs	Method
,	O
employing	O
batch	Method
normalization	Method
batchnorm	Method
speeds	O
training	Method
by	O
reducing	O
internal	O
covariate	O
shift	O
.	O
Good	O
initialization	Method
can	O
also	O
ameliorate	O
this	O
problem	O
glorot2010understanding	O
,	O
mishkin2015all	O
.	O
Path	Method
-	Method
SGD	Method
pathsgd	Method
offers	O
an	O
alternative	O
normalization	Method
scheme	Method
.	O
Progress	O
in	O
optimization	Task
is	O
somewhat	O
orthogonal	O
to	O
our	O
architectural	O
focus	O
,	O
with	O
the	O
expectation	O
that	O
advances	O
in	O
either	O
are	O
ripe	O
for	O
combination	O
.	O
Notable	O
ideas	O
in	O
architecture	O
reach	O
back	O
to	O
skip	O
connections	O
,	O
the	O
earliest	O
example	O
of	O
a	O
nontrivial	O
routing	O
pattern	O
within	O
a	O
neural	Method
network	Method
.	O
Recent	O
work	O
further	O
elaborates	O
upon	O
them	O
MYP	O
:	O
ACCV:2014	O
,	O
HAGM	O
:	O
CVPR:2015	O
.	O
Highway	Task
networks	Task
srivastava2015highway	O
and	O
ResNet	O
he2015deep	O
,	O
he2016identity	O
offer	O
additional	O
twists	O
in	O
the	O
form	O
of	O
parameterized	O
pass	O
-	O
through	O
and	O
gating	Method
.	O
In	O
work	O
subsequent	O
to	O
our	O
own	O
,	O
investigate	O
a	O
ResNet	Method
variant	Method
with	O
explicit	O
skip	O
connections	O
.	O
These	O
methods	O
share	O
distinction	O
as	O
the	O
only	O
other	O
designs	O
demonstrated	O
to	O
scale	O
to	O
hundreds	O
of	O
layers	O
and	O
beyond	O
.	O
ResNet	Method
â€™s	Method
building	Method
block	Method
uses	O
the	O
identity	O
map	O
as	O
an	O
anchor	O
point	O
and	O
explicitly	O
parameterizes	O
an	O
additive	O
correction	O
term	O
(	O
the	O
residual	O
)	O
.	O
Identity	Task
initialization	Task
also	O
appears	O
in	O
the	O
context	O
of	O
recurrent	Method
networks	Method
le2015simple	O
.	O
A	O
tendency	O
of	O
ResNet	Method
and	O
highway	Method
networks	Method
to	O
fall	O
-	O
back	O
to	O
the	O
identity	O
map	O
may	O
make	O
their	O
effective	O
depth	Metric
much	O
smaller	O
than	O
their	O
nominal	O
depth	Metric
.	O
Some	O
prior	O
results	O
hint	O
at	O
what	O
we	O
experimentally	O
demonstrate	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Namely	O
,	O
reduction	O
of	O
effective	O
depth	Metric
is	O
key	O
to	O
training	O
extremely	O
deep	Task
networks	Task
;	O
residuals	O
are	O
incidental	O
.	O
provide	O
one	O
clue	O
in	O
their	O
work	O
on	O
stochastic	O
depth	Metric
:	O
randomly	O
dropping	O
layers	O
from	O
ResNet	O
during	O
training	O
,	O
thereby	O
shrinking	O
network	O
depth	Metric
by	O
a	O
constant	O
factor	O
,	O
provides	O
additional	O
performance	O
benefit	O
.	O
We	O
build	O
upon	O
this	O
intuition	O
through	O
drop	Method
-	Method
path	Method
,	O
which	O
shrinks	O
depth	Metric
much	O
more	O
drastically	O
.	O
The	O
success	O
of	O
deep	Task
supervision	Task
lee2014deeply	O
provides	O
another	O
clue	O
that	O
effective	O
depth	Metric
is	O
crucial	O
.	O
Here	O
,	O
an	O
auxiliary	O
loss	O
,	O
forked	O
off	O
mid	O
-	O
level	O
layers	O
,	O
introduces	O
a	O
shorter	O
path	O
during	O
backpropagation	O
.	O
The	O
layer	O
at	O
the	O
fork	O
receives	O
two	O
gradients	O
,	O
originating	O
from	O
the	O
main	O
loss	O
and	O
the	O
auxiliary	O
loss	O
,	O
that	O
are	O
added	O
together	O
.	O
Deep	Task
supervision	Task
is	O
now	O
common	O
,	O
being	O
adopted	O
,	O
for	O
example	O
,	O
by	O
GoogLeNet	O
szegedy2015inception	O
.	O
However	O
,	O
irrelevance	O
of	O
the	O
auxiliary	O
loss	O
at	O
test	O
time	O
introduces	O
the	O
drawback	O
of	O
having	O
a	O
discrepancy	O
between	O
the	O
actual	O
objective	O
and	O
that	O
used	O
for	O
training	Task
.	O
Exploration	O
of	O
the	O
student	Method
-	Method
teacher	Method
paradigm	Method
ba2014dodeep	O
illuminates	O
the	O
potential	O
for	O
interplay	O
between	O
networks	O
of	O
different	O
depth	Metric
.	O
In	O
the	O
model	Task
compression	Task
scenario	Task
,	O
a	O
deeper	Method
network	Method
(	O
previously	O
trained	O
)	O
guides	O
and	O
improves	O
the	O
learning	Task
of	O
a	O
shallower	Method
and	Method
faster	Method
student	Method
network	Method
ba2014dodeep	O
,	O
urban2016dodeepsfollowup	O
.	O
This	O
is	O
accomplished	O
by	O
feeding	O
unlabeled	O
data	O
through	O
the	O
teacher	O
and	O
having	O
the	O
student	O
mimic	O
the	O
teacher	O
â€™s	O
soft	O
output	O
predictions	O
.	O
FitNets	O
romero2014fitnets	O
explicitly	O
couple	O
students	O
and	O
teachers	O
,	O
forcing	O
mimic	O
behavior	O
across	O
several	O
intermediate	O
points	O
in	O
the	O
network	O
.	O
Our	O
fractal	Method
networks	Method
capture	O
yet	O
another	O
alternative	O
,	O
in	O
the	O
form	O
of	O
implicit	O
coupling	O
,	O
with	O
the	O
potential	O
for	O
bidirectional	Task
information	Task
flow	Task
between	O
shallow	O
and	O
deep	O
subnetworks	O
.	O
Widening	Method
networks	Method
,	O
by	O
using	O
larger	O
modules	O
in	O
place	O
of	O
individual	O
layers	O
,	O
has	O
also	O
produced	O
performance	O
gains	O
.	O
For	O
example	O
,	O
an	O
Inception	Method
module	Method
szegedy2015inception	O
concatenates	O
results	O
of	O
convolutional	Method
layers	Method
of	O
different	O
receptive	O
field	O
size	O
.	O
Stacking	O
these	O
modules	O
forms	O
the	O
GoogLeNet	Method
architecture	Method
.	O
employ	O
a	O
variant	O
with	O
maxout	Method
in	O
place	O
of	O
concatenation	O
.	O
Figure	O
[	O
reference	O
]	O
makes	O
apparent	O
our	O
connection	O
with	O
such	O
work	O
.	O
As	O
a	O
fractal	Method
network	Method
deepens	O
,	O
it	O
also	O
widens	O
.	O
Moreover	O
,	O
note	O
that	O
stacking	O
two	O
2D	Method
convolutional	Method
layers	Method
with	O
the	O
same	O
spatial	O
receptive	O
field	O
(	O
e.g.	O
)	O
achieves	O
a	O
larger	O
(	O
)	O
receptive	O
field	O
.	O
A	O
horizontal	O
cross	O
-	O
section	O
of	O
a	O
fractal	Method
network	Method
is	O
reminiscent	O
of	O
an	O
Inception	Method
module	Method
,	O
except	O
with	O
additional	O
joins	O
due	O
to	O
recursive	O
structure	O
.	O
section	O
:	O
Fractal	Method
Networks	Method
We	O
begin	O
with	O
a	O
more	O
formal	O
presentation	O
of	O
the	O
ideas	O
sketched	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Convolutional	Method
neural	Method
networks	Method
serve	O
as	O
our	O
running	O
example	O
and	O
,	O
in	O
the	O
subsequent	O
section	O
,	O
our	O
experimental	O
platform	O
.	O
However	O
,	O
it	O
is	O
worth	O
emphasizing	O
that	O
our	O
framework	O
is	O
more	O
general	O
.	O
In	O
principle	O
,	O
convolutional	O
layers	O
in	O
Figure	O
[	O
reference	O
]	O
could	O
be	O
replaced	O
by	O
a	O
different	O
layer	O
type	O
,	O
or	O
even	O
a	O
custom	O
-	O
designed	O
module	O
or	O
subnetwork	O
,	O
in	O
order	O
to	O
generate	O
other	O
fractal	Method
architectures	Method
.	O
Let	O
denote	O
the	O
index	O
of	O
the	O
truncated	Method
fractal	Method
.	O
Our	O
network	O
â€™s	O
structure	O
,	O
connections	O
and	O
layer	O
types	O
,	O
is	O
defined	O
by	O
.	O
A	O
network	O
consisting	O
of	O
a	O
single	O
convolutional	Method
layer	Method
is	O
the	O
base	O
case	O
:	O
We	O
define	O
successive	O
fractals	Method
recursively	O
:	O
where	O
denotes	O
composition	Method
and	O
a	O
join	Method
operation	Method
.	O
When	O
drawn	O
in	O
the	O
style	O
of	O
Figure	O
[	O
reference	O
]	O
,	O
corresponds	O
to	O
the	O
number	O
of	O
columns	O
,	O
or	O
width	O
,	O
of	O
network	O
.	O
Depth	O
,	O
defined	O
to	O
be	O
the	O
number	O
of	O
layers	O
on	O
the	O
longest	O
path	O
between	O
input	O
and	O
output	O
,	O
scales	O
as	O
.	O
Convolutional	Method
networks	Method
for	O
classification	Task
typically	O
intersperse	O
pooling	O
layers	O
.	O
We	O
achieve	O
the	O
same	O
by	O
using	O
as	O
a	O
building	Method
block	Method
and	O
stacking	O
it	O
with	O
subsequent	O
pooling	O
layers	O
times	O
,	O
yielding	O
total	O
depth	Metric
.	O
The	O
join	Method
operation	Method
merges	O
two	O
feature	O
blobs	O
into	O
one	O
.	O
Here	O
,	O
a	O
blob	O
is	O
the	O
result	O
of	O
a	O
layer	O
:	O
a	O
tensor	O
holding	O
activations	O
for	O
a	O
fixed	O
number	O
of	O
channels	O
over	O
a	O
spatial	O
domain	O
.	O
The	O
channel	O
count	O
corresponds	O
to	O
the	O
size	O
of	O
the	O
filter	O
set	O
in	O
the	O
preceding	O
layer	O
.	O
As	O
the	O
fractal	Method
is	O
expanded	O
,	O
we	O
collapse	O
neighboring	O
joins	O
into	O
a	O
single	O
layer	O
which	O
spans	O
multiple	O
columns	O
,	O
as	O
shown	O
on	O
the	O
right	O
side	O
of	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
layer	O
merges	O
all	O
of	O
its	O
input	O
feature	O
blobs	O
into	O
a	O
single	O
output	O
blob	O
.	O
Several	O
choices	O
seem	O
reasonable	O
for	O
the	O
action	O
of	O
a	O
layer	O
,	O
including	O
concatenation	Method
and	O
addition	O
.	O
We	O
instantiate	O
each	O
to	O
compute	O
the	O
element	O
-	O
wise	O
mean	O
of	O
its	O
inputs	O
.	O
This	O
is	O
appropriate	O
for	O
convolutional	Method
networks	Method
in	O
which	O
channel	O
count	O
is	O
set	O
the	O
same	O
for	O
all	O
layers	O
within	O
a	O
fractal	O
block	O
.	O
Averaging	Method
might	O
appear	O
similar	O
to	O
ResNet	Method
â€™s	Method
addition	Method
operation	Method
,	O
but	O
there	O
are	O
critical	O
differences	O
:	O
ResNet	O
makes	O
clear	O
distinction	O
between	O
pass	O
-	O
through	O
and	O
residual	O
signals	O
.	O
In	O
FractalNet	Method
,	O
no	O
signal	O
is	O
privileged	O
.	O
Every	O
input	O
to	O
a	O
layer	O
is	O
the	O
output	O
of	O
an	O
immediately	O
preceding	O
layer	O
.	O
The	O
network	O
structure	O
alone	O
can	O
not	O
identify	O
any	O
as	O
being	O
primary	O
.	O
Drop	Method
-	Method
path	Method
regularization	Method
,	O
as	O
described	O
next	O
in	O
Section	O
[	O
reference	O
]	O
,	O
forces	O
each	O
input	O
to	O
a	O
to	O
be	O
individually	O
reliable	O
.	O
This	O
reduces	O
the	O
reward	O
for	O
even	O
implicitly	O
learning	O
to	O
allocate	O
part	O
of	O
one	O
signal	O
to	O
act	O
as	O
a	O
residual	O
for	O
another	O
.	O
Experiments	O
show	O
that	O
we	O
can	O
extract	O
high	O
-	O
performance	O
subnetworks	O
consisting	O
of	O
a	O
single	O
column	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Such	O
a	O
subnetwork	O
is	O
effectively	O
devoid	O
of	O
joins	O
,	O
as	O
only	O
a	O
single	O
path	O
is	O
active	O
throughout	O
.	O
They	O
produce	O
no	O
signal	O
to	O
which	O
a	O
residual	O
could	O
be	O
added	O
.	O
Together	O
,	O
these	O
properties	O
ensure	O
that	O
layers	Method
are	O
not	O
an	O
alternative	O
method	O
of	O
residual	Method
learning	Method
.	O
subsection	O
:	O
Regularization	Task
via	O
Drop	Method
-	Method
path	Method
include	O
/	O
fig	O
-	O
droppath	O
Dropout	O
dropout	O
and	O
drop	Method
-	Method
connect	Method
dropconnect	Method
modify	O
interactions	O
between	O
sequential	O
network	O
layers	O
in	O
order	O
to	O
discourage	O
co	Task
-	Task
adaptation	Task
.	O
Since	O
fractal	Method
networks	Method
contain	O
additional	O
macro	O
-	O
scale	O
structure	O
,	O
we	O
propose	O
to	O
complement	O
these	O
techniques	O
with	O
an	O
analogous	O
coarse	Method
-	Method
scale	Method
regularization	Method
scheme	Method
.	O
Figure	O
[	O
reference	O
]	O
illustrates	O
drop	O
-	O
path	O
.	O
Just	O
as	O
dropout	Method
prevents	O
co	Method
-	Method
adaptation	Method
of	Method
activations	Method
,	O
drop	Method
-	Method
path	Method
prevents	O
co	O
-	O
adaptation	O
of	O
parallel	O
paths	O
by	O
randomly	O
dropping	O
operands	O
of	O
the	O
layers	O
.	O
This	O
discourages	O
the	O
network	O
from	O
using	O
one	O
input	O
path	O
as	O
an	O
anchor	O
and	O
another	O
as	O
a	O
corrective	O
term	O
(	O
a	O
configuration	O
that	O
,	O
if	O
not	O
prevented	O
,	O
is	O
prone	O
to	O
overfitting	O
)	O
.	O
We	O
consider	O
two	O
sampling	Method
strategies	Method
:	O
Local	O
:	O
a	O
drops	O
each	O
input	O
with	O
fixed	O
probability	O
,	O
but	O
we	O
make	O
sure	O
at	O
least	O
one	O
survives	O
.	O
Global	O
:	O
a	O
single	O
path	O
is	O
selected	O
for	O
the	O
entire	O
network	O
.	O
We	O
restrict	O
this	O
path	O
to	O
be	O
a	O
single	O
column	O
,	O
thereby	O
promoting	O
individual	O
columns	O
as	O
independently	O
strong	O
predictors	O
.	O
As	O
with	O
dropout	Method
,	O
signals	O
may	O
need	O
appropriate	O
rescaling	Method
.	O
With	O
element	Method
-	Method
wise	Method
means	Method
,	O
this	O
is	O
trivial	O
;	O
each	O
computes	O
the	O
mean	O
of	O
only	O
its	O
active	O
inputs	O
.	O
In	O
experiments	O
,	O
we	O
train	O
with	O
dropout	Method
and	O
a	O
mixture	Method
model	Method
of	Method
local	Method
and	Method
global	Method
sampling	Method
for	O
drop	Task
-	Task
path	Task
.	O
We	O
sample	O
a	O
new	O
subnetwork	O
each	O
mini	O
-	O
batch	O
.	O
With	O
sufficient	O
memory	O
,	O
we	O
can	O
simultaneously	O
evaluate	O
one	O
local	O
sample	O
and	O
all	O
global	O
samples	O
for	O
each	O
mini	O
-	O
batch	O
by	O
keeping	O
separate	O
networks	O
and	O
tying	O
them	O
together	O
via	O
weight	Method
sharing	Method
.	O
While	O
fractal	O
connectivity	O
permits	O
the	O
use	O
of	O
paths	O
of	O
any	O
length	O
,	O
global	O
drop	O
-	O
path	O
forces	O
the	O
use	O
of	O
many	O
paths	O
whose	O
lengths	O
differ	O
by	O
orders	O
of	O
magnitude	O
(	O
powers	O
of	O
)	O
.	O
The	O
subnetworks	O
sampled	O
by	O
drop	Method
-	Method
path	Method
thus	O
exhibit	O
large	O
structural	O
diversity	O
.	O
This	O
property	O
stands	O
in	O
contrast	O
to	O
stochastic	O
depth	Metric
regularization	O
of	O
ResNet	Method
,	O
which	O
,	O
by	O
virtue	O
of	O
using	O
a	O
fixed	O
drop	O
probability	O
for	O
each	O
layer	O
in	O
a	O
chain	O
,	O
samples	O
subnetworks	O
with	O
a	O
concentrated	O
depth	Metric
distribution	O
huang2016stochasticdepth	O
.	O
Global	O
drop	O
-	O
path	O
serves	O
not	O
only	O
as	O
a	O
regularizer	Method
,	O
but	O
also	O
as	O
a	O
diagnostic	Method
tool	Method
.	O
Monitoring	O
performance	O
of	O
individual	O
columns	O
provides	O
insight	O
into	O
both	O
the	O
network	Method
and	Method
training	Method
mechanisms	Method
,	O
as	O
Section	O
[	O
reference	O
]	O
discusses	O
in	O
more	O
detail	O
.	O
Individually	O
strong	O
columns	O
of	O
various	O
depths	O
also	O
give	O
users	O
choices	O
in	O
the	O
trade	O
-	O
off	O
between	O
speed	Metric
(	O
shallow	O
)	O
and	O
accuracy	Metric
(	O
deep	O
)	O
.	O
subsection	O
:	O
Data	Task
Augmentation	Task
Data	Task
augmentation	Task
can	O
reduce	O
the	O
need	O
for	O
regularization	Task
.	O
ResNet	Method
demonstrates	O
this	O
,	O
achieving	O
27.22	O
%	O
error	Metric
rate	Metric
on	O
CIFAR	O
-	O
100	O
with	O
augmentation	Task
compared	O
to	O
44.76	O
%	O
without	O
huang2016stochasticdepth	O
.	O
While	O
augmentation	Method
benefits	O
fractal	Method
networks	Method
,	O
we	O
show	O
that	O
drop	Method
-	Method
path	Method
provides	O
highly	O
effective	O
regularization	Method
,	O
allowing	O
them	O
to	O
achieve	O
competitive	O
results	O
even	O
without	O
data	Method
augmentation	Method
.	O
subsection	O
:	O
Implementation	O
Details	O
We	O
implement	O
FractalNet	Method
using	O
Caffe	Method
caffe14	Method
.	O
Purely	O
for	O
convenience	O
,	O
we	O
flip	O
the	O
order	O
of	O
pool	O
and	O
join	O
layers	O
at	O
the	O
end	O
of	O
a	O
block	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
pool	O
individual	O
columns	O
immediately	O
before	O
the	O
joins	O
spanning	O
all	O
columns	O
,	O
rather	O
than	O
pooling	O
once	O
immediately	O
after	O
them	O
.	O
We	O
train	O
fractal	Method
networks	Method
using	O
stochastic	Method
gradient	Method
descent	Method
with	O
momentum	Method
.	O
As	O
now	O
standard	O
,	O
we	O
employ	O
batch	Method
normalization	Method
together	O
with	O
each	O
layer	O
(	O
convolution	Method
,	O
batch	Method
norm	Method
,	O
then	O
ReLU	Method
)	O
.	O
section	O
:	O
Experiments	O
table	O
CIFAR	O
-	O
100	O
/	O
CIFAR	O
-	O
10	O
/	O
SVHN	Material
.	O
We	O
compare	O
test	Metric
error	Metric
(	O
%	O
)	O
with	O
other	O
leading	Method
methods	Method
,	O
trained	O
with	O
either	O
no	O
data	Method
augmentation	Method
,	O
translation	O
/	O
mirroring	O
(	O
+	O
)	O
,	O
or	O
more	O
substantial	O
augmentation	O
(	O
+	O
+	O
)	O
.	O
Our	O
main	O
point	O
of	O
comparison	O
is	O
ResNet	Method
.	O
We	O
closely	O
match	O
its	O
benchmark	O
results	O
using	O
data	Method
augmentation	Method
,	O
and	O
outperform	O
it	O
by	O
large	O
margins	O
without	O
data	Method
augmentation	Method
.	O
Training	O
with	O
drop	O
-	O
path	O
,	O
we	O
can	O
extract	O
from	O
FractalNet	Method
single	O
-	O
column	O
(	O
plain	O
)	O
networks	O
that	O
are	O
highly	O
competitive	O
.	O
The	O
CIFAR	O
,	O
SVHN	Material
,	O
and	O
ImageNet	O
datasets	O
serve	O
as	O
testbeds	O
for	O
comparison	O
to	O
prior	O
work	O
and	O
analysis	O
of	O
FractalNet	Method
â€™s	O
internal	O
behavior	O
.	O
We	O
evaluate	O
performance	O
on	O
the	O
standard	O
classification	Task
task	Task
associated	O
with	O
each	O
dataset	O
.	O
For	O
CIFAR	O
and	O
SVHN	Material
,	O
which	O
consist	O
of	O
images	O
,	O
we	O
set	O
our	O
fractal	Method
network	Method
to	O
have	O
blocks	O
(	O
)	O
with	O
non	O
-	O
overlapping	O
max	Method
-	Method
pooling	Method
and	O
subsampling	Method
applied	O
after	O
each	O
.	O
This	O
reduces	O
the	O
input	O
spatial	O
resolution	O
to	O
over	O
the	O
course	O
of	O
the	O
entire	O
network	O
.	O
A	O
softmax	Method
prediction	Method
layer	Method
attaches	O
at	O
the	O
end	O
of	O
the	O
network	O
.	O
Unless	O
otherwise	O
noted	O
,	O
we	O
set	O
the	O
number	O
of	O
filter	O
channels	O
within	O
blocks	O
through	O
as	O
,	O
mostly	O
matching	O
the	O
convention	O
of	O
doubling	O
the	O
number	O
of	O
channels	O
after	O
halving	O
spatial	O
resolution	O
.	O
For	O
ImageNet	O
,	O
we	O
choose	O
a	O
fractal	Method
architecture	Method
to	O
facilitate	O
direct	O
comparison	O
with	O
the	O
-	O
layer	O
ResNet	O
of	O
.	O
We	O
use	O
the	O
same	O
first	O
and	O
last	O
layer	O
as	O
ResNet	O
-	O
34	O
,	O
but	O
change	O
the	O
middle	O
of	O
the	O
network	O
to	O
consist	O
of	O
blocks	O
(	O
)	O
,	O
each	O
of	O
layers	O
(	O
columns	O
)	O
.	O
We	O
use	O
a	O
filter	Method
channel	Method
progression	Method
of	O
in	O
blocks	O
through	O
.	O
subsection	O
:	O
Training	O
For	O
experiments	O
using	O
dropout	Method
,	O
we	O
fix	O
drop	Metric
rate	Metric
per	O
block	O
at	O
,	O
similar	O
to	O
.	O
Local	Method
drop	Method
-	Method
path	Method
uses	O
drop	O
rate	O
across	O
the	O
entire	O
network	O
.	O
We	O
run	O
for	O
epochs	O
on	O
CIFAR	O
,	O
epochs	O
on	O
SVHN	Material
,	O
and	O
epochs	O
on	O
ImageNet	O
.	O
Our	O
learning	Metric
rate	Metric
starts	O
at	O
(	O
for	O
ImageNet	O
,	O
)	O
and	O
we	O
train	O
using	O
stochastic	Method
gradient	Method
descent	Method
with	O
batch	O
size	O
(	O
for	O
ImageNet	O
,	O
)	O
and	O
momentum	O
.	O
For	O
CIFAR	O
/	O
SVHN	Material
,	O
we	O
drop	O
the	O
learning	Metric
rate	Metric
by	O
a	O
factor	O
of	O
whenever	O
the	O
number	O
of	O
remaining	O
epochs	O
halves	O
.	O
For	O
ImageNet	O
,	O
we	O
drop	O
by	O
a	O
factor	O
of	O
at	O
epochs	O
and	O
.	O
We	O
use	O
Xavier	Method
initialization	Method
glorot2010understanding	O
.	O
A	O
widely	O
employed	O
nin	Method
,	O
elu	Method
,	O
srivastava2015highway	O
,	O
he2015deep	O
,	O
he2016identity	O
,	O
huang2016stochasticdepth	O
,	O
rir	Method
scheme	Method
for	O
data	Task
augmentation	Task
on	O
CIFAR	O
consists	O
of	O
only	O
horizontal	O
mirroring	O
and	O
translation	O
(	O
uniform	O
offsets	O
in	O
)	O
,	O
with	O
images	O
zero	O
-	O
padded	O
where	O
needed	O
after	O
mean	Method
subtraction	Method
.	O
We	O
denote	O
results	O
achieved	O
using	O
no	O
more	O
than	O
this	O
degree	O
of	O
augmentation	O
by	O
appending	O
a	O
â€˜	O
â€˜	O
+	O
â€™	O
â€™	O
to	O
the	O
dataset	O
name	O
(	O
e.g.	O
CIFAR	O
-	O
100	O
+	O
)	O
.	O
A	O
â€˜	O
â€˜	O
+	O
+	O
â€™	O
â€™	O
marks	O
results	O
reliant	O
on	O
more	O
data	Task
augmentation	Task
;	O
here	O
exact	O
schemes	O
may	O
vary	O
.	O
Our	O
entry	O
in	O
this	O
category	O
is	O
modest	O
and	O
simply	O
changes	O
the	O
zero	O
-	O
padding	O
to	O
reflect	O
-	O
padding	O
.	O
table	O
ImageNet	O
(	O
validation	O
set	O
,	O
10	O
-	O
crop	O
)	O
.	O
table	O
Ultra	Method
-	Method
deep	Method
fractal	Method
networks	Method
(	O
CIFAR	O
-	O
100	O
++	O
)	O
.	O
Increasing	O
depth	Metric
greatly	O
improves	O
accuracy	Metric
until	O
eventual	O
diminishing	O
returns	O
.	O
Contrast	O
with	O
plain	Method
networks	Method
,	O
which	O
are	O
not	O
trainable	O
if	O
made	O
too	O
deep	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
table	O
Fractal	Method
structure	Method
as	O
a	O
training	O
apparatus	O
(	O
CIFAR	O
-	O
100	O
++	O
)	O
.	O
Plain	Method
networks	Method
perform	O
well	O
if	O
moderately	O
deep	O
,	O
but	O
exhibit	O
worse	O
convergence	Metric
during	O
training	Task
if	O
instantiated	O
with	O
great	O
depth	Metric
.	O
However	O
,	O
as	O
a	O
column	O
trained	O
within	O
,	O
and	O
then	O
extracted	O
from	O
,	O
a	O
fractal	Method
network	Method
with	O
mixed	O
drop	O
-	O
path	O
,	O
we	O
recover	O
a	O
plain	Method
network	Method
that	O
overcomes	O
such	O
depth	Metric
limitation	O
(	O
possibly	O
due	O
to	O
a	O
student	O
-	O
teacher	O
effect	O
)	O
.	O
subsection	O
:	O
Results	O
Table	O
[	O
reference	O
]	O
compares	O
performance	O
of	O
FractalNet	Method
on	O
CIFAR	O
and	O
SVHN	Material
with	O
competing	O
methods	O
.	O
FractalNet	Method
(	O
depth	Metric
)	O
outperforms	O
the	O
original	O
ResNet	Method
across	O
the	O
board	O
.	O
With	O
data	Task
augmentation	Task
,	O
our	O
CIFAR	O
-	O
100	O
accuracy	Metric
is	O
close	O
to	O
that	O
of	O
the	O
best	O
ResNet	Method
variants	Method
.	O
With	O
neither	O
augmentation	Method
nor	O
regularization	Method
,	O
FractalNet	Method
â€™s	O
performance	O
on	O
CIFAR	O
is	O
superior	O
to	O
both	O
ResNet	Method
and	O
ResNet	Method
with	O
stochastic	O
depth	Metric
,	O
suggesting	O
that	O
FractalNet	Method
may	O
be	O
less	O
prone	O
to	O
overfitting	O
.	O
Most	O
methods	O
perform	O
similarly	O
on	O
SVHN	Material
.	O
Increasing	O
depth	Metric
to	O
,	O
while	O
borrowing	O
some	O
parameter	Method
reduction	Method
tricks	Method
SqueezeNet	Method
,	O
reveals	O
FractalNet	Method
â€™s	O
performance	O
to	O
be	O
consistent	O
across	O
a	O
range	O
of	O
configuration	O
choices	O
.	O
Experiments	O
without	O
data	Method
augmentation	Method
highlight	O
the	O
power	O
of	O
drop	Method
-	Method
path	Method
regularization	Method
.	O
On	O
CIFAR	O
-	O
100	O
,	O
drop	Method
-	Method
path	Method
reduces	O
FractalNet	Method
â€™s	O
error	Metric
rate	Metric
from	O
to	O
.	O
Unregularized	Method
ResNet	Method
is	O
far	O
behind	O
(	O
)	O
and	O
ResNet	Method
with	O
stochastic	O
depth	Metric
(	O
)	O
does	O
not	O
catch	O
up	O
to	O
our	O
unregularized	O
starting	O
point	O
of	O
.	O
CIFAR	O
-	O
10	O
mirrors	O
this	O
story	O
.	O
With	O
data	Task
augmentation	Task
,	O
drop	Method
-	Method
path	Method
provides	O
a	O
boost	O
(	O
CIFAR	O
-	O
10	O
)	O
,	O
or	O
does	O
not	O
significantly	O
influence	O
FractalNet	Method
â€™s	O
performance	O
(	O
CIFAR	O
-	O
100	O
)	O
.	O
Note	O
that	O
the	O
performance	O
of	O
the	O
deepest	O
column	O
of	O
the	O
fractal	Method
network	Method
is	O
close	O
to	O
that	O
of	O
the	O
full	Method
network	Method
(	O
statistically	O
equivalent	O
on	O
CIFAR	O
-	O
10	O
)	O
.	O
This	O
suggests	O
that	O
the	O
fractal	O
structure	O
may	O
be	O
more	O
important	O
as	O
a	O
learning	Method
framework	Method
than	O
as	O
a	O
final	O
model	Method
architecture	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
FractalNet	Method
scales	O
to	O
ImageNet	O
,	O
matching	O
ResNet	O
he2015deep	O
at	O
equal	O
depth	Metric
.	O
Note	O
that	O
,	O
concurrent	O
with	O
our	O
work	O
,	O
refinements	O
to	O
the	O
residual	Method
network	Method
paradigm	Method
further	O
improve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
ImageNet	O
.	O
Wide	Method
residual	Method
networks	Method
wideresnet	O
of	O
-	O
layers	O
reduce	O
single	Method
-	Method
crop	Method
Top	Method
-	Method
1	Method
and	O
Top	Metric
-	Metric
5	Metric
validation	Metric
error	Metric
by	O
approximately	O
and	O
,	O
respectively	O
,	O
over	O
ResNet	O
-	O
by	O
doubling	O
feature	O
channels	O
in	O
each	O
layer	O
.	O
DenseNets	Method
densenet	Method
substantially	O
improve	O
performance	O
by	O
building	O
residual	O
blocks	O
that	O
concatenate	O
rather	O
than	O
add	O
feature	O
channels	O
.	O
Table	O
[	O
reference	O
]	O
demonstrates	O
that	O
FractalNet	Method
resists	O
performance	O
degradation	O
as	O
we	O
increase	O
to	O
obtain	O
extremely	O
deep	O
networks	O
(	O
layers	O
for	O
)	O
.	O
Scores	O
in	O
this	O
table	O
are	O
not	O
comparable	O
to	O
those	O
in	O
Table	O
[	O
reference	O
]	O
.	O
For	O
time	O
and	O
memory	Metric
efficiency	Metric
,	O
we	O
reduced	O
block	O
-	O
wise	O
feature	O
channels	O
to	O
and	O
the	O
batch	O
size	O
to	O
for	O
the	O
supporting	O
experiments	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
Table	O
[	O
reference	O
]	O
provides	O
a	O
baseline	O
showing	O
that	O
training	O
of	O
plain	Method
deep	Method
networks	Method
begins	O
to	O
degrade	O
by	O
the	O
time	O
their	O
depth	Metric
reaches	O
layers	O
.	O
In	O
our	O
experience	O
,	O
a	O
plain	O
-	O
layer	O
completely	O
fails	O
to	O
converge	O
.	O
This	O
table	O
also	O
highlights	O
the	O
ability	O
to	O
use	O
FractalNet	Method
and	O
drop	Method
-	Method
path	Method
as	O
an	O
engine	O
for	O
extracting	Task
trained	Task
networks	Task
(	O
columns	O
)	O
with	O
the	O
same	O
topology	O
as	O
plain	Method
networks	Method
,	O
but	O
much	O
higher	O
test	O
performance	O
.	O
subsection	O
:	O
Introspection	O
With	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
examine	O
the	O
evolution	O
of	O
a	O
-	O
layer	O
FractalNet	Method
during	O
training	O
.	O
Tracking	O
columns	O
individually	O
(	O
recording	O
their	O
losses	O
when	O
run	O
as	O
stand	O
-	O
alone	O
networks	O
)	O
,	O
we	O
observe	O
that	O
the	O
-	O
layer	O
column	O
initially	O
improves	O
slowly	O
,	O
but	O
picks	O
up	O
once	O
the	O
loss	O
of	O
the	O
rest	O
of	O
the	O
network	O
begins	O
to	O
stabilize	O
.	O
Contrast	O
with	O
a	O
plain	Method
-	Method
layer	Method
network	Method
trained	O
alone	O
(	O
dashed	O
blue	O
line	O
)	O
,	O
which	O
never	O
makes	O
fast	O
progress	O
.	O
The	O
column	O
has	O
the	O
same	O
initial	O
plateau	O
,	O
but	O
subsequently	O
improves	O
after	O
epochs	O
,	O
producing	O
a	O
loss	O
curve	O
uncharacteristic	O
of	O
plain	Method
networks	Method
.	O
We	O
hypothesize	O
that	O
the	O
fractal	O
structure	O
triggers	O
effects	O
akin	O
to	O
deep	O
supervision	O
and	O
lateral	O
student	O
-	O
teacher	O
information	O
flow	O
.	O
Column	O
#	O
4	O
joins	O
with	O
column	O
#	O
3	O
every	O
other	O
layer	O
,	O
and	O
in	O
every	O
fourth	O
layer	O
this	O
join	O
involves	O
no	O
other	O
columns	O
.	O
Once	O
the	O
fractal	Method
network	Method
partially	O
relies	O
on	O
the	O
signal	O
going	O
through	O
column	O
#	O
3	O
,	O
drop	O
-	O
path	O
puts	O
pressure	O
on	O
column	O
#	O
4	O
to	O
produce	O
a	O
replacement	O
signal	O
when	O
column	O
#	O
3	O
is	O
dropped	O
.	O
This	O
task	O
has	O
constrained	O
scope	O
.	O
A	O
particular	O
drop	O
only	O
requires	O
two	O
consecutive	O
layers	O
in	O
column	O
#	O
4	O
to	O
substitute	O
for	O
one	O
in	O
column	O
#	O
3	O
(	O
a	O
mini	Task
student	Task
-	Task
teacher	Task
problem	Task
)	O
.	O
This	O
explanation	O
of	O
FractalNet	Method
dynamics	O
parallels	O
what	O
,	O
in	O
concurrent	O
work	O
,	O
claim	O
for	O
ResNet	Task
.	O
Specifically	O
,	O
suggest	O
residual	Method
networks	Method
learn	O
unrolled	Method
iterative	Method
estimation	Method
,	O
with	O
each	O
layer	O
performing	O
a	O
gradual	O
refinement	O
on	O
its	O
input	O
representation	O
.	O
The	O
deepest	O
FractalNet	Method
column	O
could	O
behave	O
in	O
the	O
same	O
manner	O
,	O
with	O
the	O
remainder	O
of	O
the	O
network	O
acting	O
as	O
a	O
scaffold	O
for	O
building	O
smaller	O
refinement	O
steps	O
by	O
doubling	O
layers	O
from	O
one	O
column	O
to	O
the	O
next	O
.	O
These	O
interpretations	O
appear	O
not	O
to	O
mesh	O
with	O
the	O
conclusions	O
of	O
,	O
who	O
claim	O
that	O
ensemble	O
-	O
like	O
behavior	O
underlies	O
the	O
success	O
of	O
ResNet	Method
.	O
This	O
is	O
certainly	O
untrue	O
of	O
some	O
very	O
deep	Method
networks	Method
,	O
as	O
FractalNet	Method
provides	O
a	O
counterexample	O
:	O
we	O
can	O
extract	O
a	O
single	O
column	O
(	O
plain	O
network	O
topology	O
)	O
and	O
it	O
alone	O
(	O
no	O
ensembling	Method
)	O
performs	O
nearly	O
as	O
well	O
as	O
the	O
entire	O
network	O
.	O
Moreover	O
,	O
the	O
gradual	Method
refinement	Method
view	Method
may	O
offer	O
an	O
alternative	O
explanation	O
for	O
the	O
experiments	O
of	O
.	O
If	O
each	O
layer	O
makes	O
only	O
a	O
small	O
modification	O
,	O
removing	O
one	O
may	O
look	O
,	O
to	O
the	O
subsequent	O
portion	O
of	O
the	O
network	O
,	O
like	O
injecting	O
a	O
small	O
amount	O
of	O
input	O
noise	O
.	O
Perhaps	O
noise	O
tolerance	O
explains	O
the	O
gradual	O
performance	O
degradation	O
that	O
observe	O
when	O
removing	O
ResNet	O
layers	O
.	O
section	O
:	O
Conclusion	O
Our	O
experiments	O
with	O
fractal	Method
networks	Method
provide	O
strong	O
evidence	O
that	O
path	O
length	O
is	O
fundamental	O
for	O
training	O
ultra	Method
-	Method
deep	Method
neural	Method
networks	Method
;	O
residuals	O
are	O
incidental	O
.	O
Key	O
is	O
the	O
shared	O
characteristic	O
of	O
FractalNet	Method
and	O
ResNet	Method
:	O
large	O
nominal	O
network	O
depth	Metric
,	O
but	O
effectively	O
shorter	O
paths	O
for	O
gradient	Method
propagation	Method
during	O
training	O
.	O
Fractal	Method
architectures	Method
are	O
arguably	O
the	O
simplest	O
means	O
of	O
satisfying	O
this	O
requirement	O
,	O
and	O
match	O
residual	Method
networks	Method
in	O
experimental	O
performance	O
.	O
Fractal	Method
networks	Method
are	O
resistant	O
to	O
being	O
too	O
deep	O
;	O
extra	O
depth	Metric
may	O
slow	O
training	O
,	O
but	O
does	O
not	O
impair	O
accuracy	Metric
.	O
With	O
drop	Method
-	Method
path	Method
,	O
regularization	Method
of	O
extremely	O
deep	Method
fractal	Method
networks	Method
is	O
intuitive	O
and	O
effective	O
.	O
Drop	Method
-	Method
path	Method
doubles	O
as	O
a	O
method	O
of	O
enforcing	O
speed	Metric
(	O
latency	O
)	O
vs.	O
accuracy	Metric
tradeoffs	Metric
.	O
For	O
applications	O
where	O
fast	O
responses	O
have	O
utility	O
,	O
we	O
can	O
obtain	O
fractal	Method
networks	Method
whose	O
partial	O
evaluation	O
yields	O
good	O
answers	O
.	O
Our	O
analysis	O
connects	O
the	O
internal	O
behavior	O
of	O
fractal	Method
networks	Method
with	O
phenomena	O
engineered	O
into	O
other	O
networks	O
.	O
Their	O
substructure	O
resembles	O
hand	Method
-	Method
crafted	Method
modules	Method
used	O
as	O
components	O
in	O
prior	O
work	O
.	O
Their	O
training	O
evolution	O
may	O
emulate	O
deep	Task
supervision	Task
and	O
student	Task
-	Task
teacher	Task
learning	Task
.	O
subsection	O
:	O
Acknowledgments	O
We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
GPUs	O
used	O
for	O
this	O
research	O
.	O
This	O
work	O
was	O
partially	O
supported	O
by	O
the	O
NSF	O
award	O
RI:1409837	O
.	O
bibliography	O
:	O
References	O
