document O
: O
Deep Task
Learning Task
over O
Multi O
- O
field O
Categorical O
Data O
Predicting Task
user Task
responses Task
, O
such O
as O
click Task
- Task
through Task
rate O
and O
conversion Task
rate O
, O
are O
critical O
in O
many O
web Task
applications Task
including O
web Task
search Task
, O
personalised Task
recommendation Task
, O
and O
online Task
advertising Task
. O
Different O
from O
continuous O
raw O
features O
that O
we O
usually O
found O
in O
the O
image O
and O
audio O
domains O
, O
the O
input O
features O
in O
web O
space O
are O
always O
of O
multi O
- O
field O
and O
are O
mostly O
discrete O
and O
categorical O
while O
their O
dependencies O
are O
little O
known O
. O
Major O
user O
response O
prediction Task
models O
have O
to O
either O
limit O
themselves O
to O
linear Method
models Method
or O
require O
manually O
building O
up O
high O
- O
order O
combination O
features O
. O
The O
former O
loses O
the O
ability O
of O
exploring O
feature O
interactions O
, O
while O
the O
latter O
results O
in O
a O
heavy O
computation O
in O
the O
large O
feature O
space O
. O
To O
tackle O
the O
issue O
, O
we O
propose O
two O
novel O
models O
using O
deep Method
neural Method
networks Method
( O
DNNs Method
) O
to O
automatically O
learn O
effective O
patterns O
from O
categorical O
feature O
interactions O
and O
make O
predictions O
of O
users O
’ O
ad O
clicks O
. O
To O
get O
our O
DNNs Method
efficiently O
work O
, O
we O
propose O
to O
leverage O
three O
feature Method
transformation Method
methods Method
, O
i.e. O
, O
factorisation Method
machines Method
( O
FMs Method
) O
, O
restricted Method
Boltzmann Method
machines Method
( O
RBMs Method
) O
and O
denoising Method
auto Method
- Method
encoders Method
( O
DAEs Method
) O
. O
This O
paper O
presents O
the O
structure O
of O
our O
models O
and O
their O
efficient O
training Task
algorithms O
. O
The O
large O
- O
scale O
experiments O
with O
real O
- O
world O
data O
demonstrate O
that O
our O
methods O
work O
better O
than O
major O
state O
- O
of O
- O
the O
- O
art O
models O
. O
section O
: O
Introduction O
User Task
response Task
( O
e.g. O
, O
click Task
- Task
through Task
or O
conversion Task
) O
prediction Task
plays O
a O
critical O
part O
in O
many O
web Task
applications Task
including O
web Task
search Task
, O
recommender Task
systems Task
, O
sponsored Task
search Task
, O
and O
display Task
advertising Task
. O
In O
online Task
advertising Task
, O
for O
instance O
, O
the O
ability O
of O
targeting O
individual O
users O
is O
the O
key O
advantage O
compared O
to O
traditional O
offline Task
advertising Task
. O
All O
these O
targeting Method
techniques Method
, O
essentially O
, O
rely O
on O
the O
system Method
function Method
of O
predicting O
whether O
a O
specific O
user O
will O
think O
the O
potential O
ad O
is O
“ O
relevant O
” O
, O
i.e. O
, O
the O
probability O
that O
the O
user O
in O
a O
certain O
context O
will O
click O
a O
given O
ad O
. O
Sponsored Task
search Task
, O
contextual Task
advertising Task
, O
and O
the O
recently O
emerged O
real Task
- Task
time Task
bidding Task
( O
RTB Task
) O
display O
advertising O
all O
heavily O
rely O
on O
the O
ability O
of O
learned Method
models Method
to O
predict Task
ad Task
click Task
- Task
through Task
rates Task
( O
CTR Metric
) O
. O
The O
applied O
CTR Metric
estimation O
models O
today O
are O
mostly O
linear Method
, O
ranging O
from O
logistic Method
regression Method
and O
naive Method
Bayes Method
to O
FTRL Method
logistic Method
regression Method
and O
Bayesian Method
probit Method
regression Method
, O
all O
of O
which O
are O
based O
on O
a O
huge O
number O
of O
sparse O
features O
with O
one Method
- Method
hot Method
encoding Method
. O
Linear Method
models Method
have O
advantages O
of O
easy O
implementation O
, O
efficient O
learning Task
but O
relative O
low O
performance O
because O
of O
the O
failure O
of O
learning O
the O
non O
- O
trivial O
patterns O
to O
catch O
the O
interactions O
between O
the O
assumed O
( O
conditionally O
) O
independent O
raw O
features O
. O
Non Method
- Method
linear Method
models Method
, O
on O
the O
other O
hand O
, O
are O
able O
to O
utilise O
different O
feature O
combinations O
and O
thus O
could O
potentially O
improve O
estimation Task
performance O
. O
For O
example O
, O
factorisation Method
machines Method
( O
FMs Method
) O
map O
the O
user O
and O
item O
binary O
features O
into O
a O
low O
dimensional O
continuous O
space O
. O
And O
the O
feature O
interaction O
is O
automatically O
explored O
via O
vector Method
inner Method
product Method
. O
Gradient Method
boosting Method
trees Method
automatically O
learn O
feature O
combinations O
while O
growing O
each O
decision Method
/ O
regression Method
tree Method
. O
However O
, O
these O
models O
can O
not O
make O
use O
of O
all O
possible O
combinations O
of O
different O
features O
. O
In O
addition O
, O
many O
models O
require O
feature Method
engineering Method
that O
manually O
designs O
what O
the O
inputs O
should O
be O
. O
Another O
problem O
of O
the O
mainstream O
ad O
CTR Metric
estimation O
models O
is O
that O
most O
prediction Task
models O
have O
shallow O
structures O
and O
have O
limited O
expression O
to O
model O
the O
underlying O
patterns O
from O
complex O
and O
massive O
data O
. O
As O
a O
result O
, O
their O
data Method
modelling Method
and O
generalisation Task
ability Task
is O
still O
restricted O
. O
Deep Method
learning Method
has O
become O
successful O
in O
computer Task
vision Task
, O
speech Task
recognition Task
, O
and O
natural Task
language Task
processing Task
( O
NLP Task
) O
during O
recent O
five O
years O
. O
As O
visual O
, O
aural O
, O
and O
textual O
signals O
are O
known O
to O
be O
spatially O
and O
/ O
or O
temporally O
correlated O
, O
the O
newly O
introduced O
unsupervised O
training Task
on O
deep Method
structures Method
would O
be O
able O
to O
explore O
such O
local O
dependency O
and O
establish O
a O
dense Method
representation Method
of O
the O
feature O
space O
, O
making O
neural Method
network Method
models Method
effective O
in O
learning Task
high Task
- Task
order Task
features Task
directly O
from O
the O
raw O
feature O
input O
. O
With O
such O
learning Method
ability Method
, O
deep Method
learning Method
would O
be O
a O
good O
candidate O
to O
estimate O
online Metric
user Metric
response Metric
rate Metric
such O
as O
ad O
CTR Metric
. O
However O
, O
most O
input O
features O
in O
CTR Task
estimation Task
are O
of O
multi O
- O
field O
and O
are O
discrete O
categorical O
features O
, O
e.g. O
, O
the O
user O
location O
city O
( O
London O
, O
Paris O
) O
, O
device O
type O
( O
PC O
, O
Mobile O
) O
, O
ad O
category O
( O
Sports O
, O
Electronics O
) O
etc O
. O
, O
and O
their O
local O
dependencies O
( O
thus O
the O
sparsity O
in O
the O
feature O
space O
) O
are O
unknown O
. O
Therefore O
, O
it O
is O
of O
great O
interest O
to O
see O
how O
deep Method
learning Method
improves O
the O
CTR Task
estimation Task
via O
learning O
feature Method
representation Method
on O
such O
large O
- O
scale O
multi O
- O
field O
discrete O
categorical O
features O
. O
To O
our O
best O
knowledge O
, O
there O
is O
no O
previous O
literature O
of O
ad Task
CTR Task
estimation Task
using O
deep Method
learning Method
methods Method
thus O
far O
. O
In O
addition O
, O
training Task
deep Method
neural Method
networks Method
( O
DNNs Method
) O
on O
a O
large O
input O
feature O
space O
requires O
tuning O
a O
huge O
number O
of O
parameters O
, O
which O
is O
computationally O
expensive O
. O
For O
instance O
, O
unlike O
image O
and O
audio O
cases O
, O
we O
have O
about O
1 O
million O
binary O
input O
features O
and O
100 O
hidden O
units O
in O
the O
first O
layer O
; O
then O
it O
requires O
100 O
million O
links O
to O
build O
the O
first Method
layer Method
neural Method
network Method
. O
In O
this O
paper O
, O
we O
take O
ad Task
CTR Task
estimation Task
as O
a O
working O
example O
to O
study O
deep Task
learning Task
over O
a O
large O
multi O
- O
field O
categorical O
feature O
space O
by O
using O
embedding Method
methods Method
in O
both O
supervised Method
and Method
unsupervised Method
fashions Method
. O
We O
introduce O
two O
types O
of O
deep Method
learning Method
models Method
, O
called O
Factorisation Method
Machine Method
supported Method
Neural Method
Network Method
( O
FNN Method
) O
and O
Sampling Method
- Method
based Method
Neural Method
Network Method
( O
SNN Method
) O
. O
Specifically O
, O
FNN Method
with O
a O
supervised Method
- Method
learning Method
embedding Method
layer Method
using O
factorisation Method
machines Method
is O
proposed O
to O
efficiently O
reduce O
the O
dimension O
from O
sparse O
features O
to O
dense O
continuous O
features O
. O
The O
second O
model O
SNN Method
is O
a O
deep Method
neural Method
network Method
powered O
by O
a O
sampling Method
- Method
based Method
restricted Method
Boltzmann Method
machine Method
( O
SNN Method
- Method
RBM Method
) O
or O
a O
sampling Method
- Method
based Method
denoising Method
auto Method
- Method
encoder Method
( O
SNN Method
- Method
DAE Method
) O
with O
a O
proposed O
negative Method
sampling Method
method Method
. O
Based O
on O
the O
embedding Method
layer Method
, O
we O
build O
multiple Method
layers Method
neural Method
nets Method
with O
full O
connections O
to O
explore O
non O
- O
trivial O
data O
patterns O
. O
Our O
experiments O
on O
multiple O
real O
- O
world O
advertisers O
’ O
ad O
click O
data O
have O
demonstrated O
the O
consistent O
improvement O
of O
CTR Task
estimation Task
from O
our O
proposed O
models O
over O
the O
state O
- O
of O
- O
the O
- O
art O
ones O
. O
section O
: O
Related O
Work O
Click Metric
- Metric
through Metric
rate Metric
, O
defined O
as O
the O
probability O
of O
the O
ad O
click O
from O
a O
specific O
user O
on O
a O
displayed O
ad O
, O
is O
essential O
in O
online Task
advertising Task
. O
In O
order O
to O
maximise O
revenue O
and O
user Task
satisfaction Task
, O
online Task
advertising Task
platforms Task
must O
predict O
the O
expected O
user O
behaviour O
for O
each O
displayed O
ad O
and O
maximise O
the O
expectation O
that O
users O
will O
click O
. O
The O
majority O
of O
current O
models O
use O
logistic Method
regression Method
based O
on O
a O
set O
of O
sparse O
binary O
features O
converted O
from O
the O
original O
categorical O
features O
via O
one Method
- Method
hot Method
encoding Method
. O
Heavy O
engineering O
efforts O
are O
needed O
to O
design O
features O
such O
as O
locations O
, O
top O
unigrams O
, O
combination O
features O
, O
etc O
. O
. O
Embedding O
very O
large O
feature O
vector O
into O
low O
- O
dimensional O
vector O
spaces O
is O
useful O
for O
prediction Task
task Task
as O
it O
reduces O
the O
data O
and O
model Metric
complexity Metric
and O
improves O
both O
the O
effectiveness O
and O
the O
efficiency O
of O
the O
training Task
and O
prediction Task
. O
Various O
methods O
of O
embedding Method
architectures Method
have O
been O
proposed O
. O
Factorisation Method
machine Method
( O
FM Method
) O
, O
originally O
proposed O
for O
collaborative Task
filtering Task
recommendation Task
, O
is O
regarded O
as O
one O
of O
the O
most O
successful O
embedding Method
models Method
. O
FM Method
naturally O
has O
the O
capability O
of O
estimating O
interactions O
between O
any O
two O
features O
via O
mapping O
them O
into O
vectors O
in O
a O
low O
- O
rank O
latent O
space O
. O
Deep Method
Learning Method
is O
a O
branch O
of O
artificial Task
intelligence Task
research Task
that O
attempts O
to O
develop O
the O
techniques O
that O
will O
allow O
computers O
to O
handle O
complex O
tasks O
such O
as O
recognition Task
and O
prediction Task
at O
high O
performance O
. O
Deep Method
neural Method
networks Method
( O
DNNs Method
) O
are O
able O
to O
extract O
the O
hidden O
structures O
and O
intrinsic O
patterns O
at O
different O
levels O
of O
abstractions O
from O
training Task
data O
. O
DNNs Method
have O
been O
successfully O
applied O
in O
computer Task
vision Task
, O
speech Task
recognition Task
and O
natural Task
language Task
processing Task
( O
NLP Task
) O
. O
Furthermore O
, O
with O
the O
help O
of O
unsupervised O
pre O
- O
training Task
, O
we O
can O
get O
good O
feature Method
representation Method
which O
guides O
the O
learning O
towards O
basins O
of O
attraction O
of O
minima O
that O
support O
better O
generalisation O
from O
the O
training Task
data O
. O
Usually O
, O
these O
deep Method
models Method
have O
two O
stages O
in O
learning Task
: O
the O
first O
stage O
performs O
model Method
initialisation Method
via O
unsupervised Method
learning Method
( O
i.e. O
, O
the O
restricted Method
Boltzmann Method
machine Method
or O
stacked Method
denoising Method
auto Method
- Method
encoders Method
) O
to O
make O
the O
model O
catch O
the O
input O
data O
distribution O
; O
the O
second O
stage O
involves O
a O
fine O
tuning O
of O
the O
initialised Method
model Method
via O
supervised Method
learning Method
with O
back Method
- Method
propagation Method
. O
The O
novelty O
of O
our O
deep Method
learning Method
models Method
lies O
in O
the O
first Method
layer Method
initialisation Method
, O
where O
the O
input O
raw O
features O
are O
high O
dimensional O
and O
sparse O
binary O
features O
converted O
from O
the O
raw O
categorical O
features O
, O
which O
makes O
it O
hard O
to O
train O
traditional O
DNNs Method
in O
large O
scale O
. O
Compared O
with O
the O
word Method
- Method
embedding Method
techniques Method
used O
in O
NLP Task
, O
our O
models O
deal O
with O
more O
general O
multi O
- O
field O
categorical O
features O
without O
any O
assumed O
data O
structures O
such O
as O
word O
alignment O
and O
letter O
- O
n O
- O
gram O
etc O
. O
section O
: O
DNNs Method
for O
CTR Task
Estimation Task
given O
Categorical O
Features O
In O
this O
section O
, O
we O
discuss O
the O
two O
proposed O
DNN Method
architectures Method
in O
detail O
, O
namely O
Factorisation Method
- Method
machine Method
supported Method
Neural Method
Networks Method
( O
FNN Method
) O
and O
Sampling Method
- Method
based Method
Neural Method
Networks Method
( O
SNN Method
) O
. O
The O
input O
categorical O
features O
are O
field O
- O
wise O
one Method
- Method
hot Method
encoded Method
. O
For O
each O
field O
, O
e.g. O
, O
city O
, O
there O
are O
multiple O
units O
, O
each O
of O
which O
represents O
a O
specific O
value O
of O
this O
field O
, O
e.g. O
, O
city O
= O
London O
, O
and O
there O
is O
only O
one O
positive O
( O
1 O
) O
unit O
, O
while O
all O
others O
are O
negative O
( O
0 O
) O
. O
The O
encoded O
features O
, O
denoted O
as O
, O
are O
the O
input O
of O
many O
CTR Metric
estimation O
models O
as O
well O
as O
our O
DNN Method
models Method
, O
as O
depicted O
at O
the O
bottom O
layer O
of O
Figure O
[ O
reference O
] O
. O
subsection O
: O
Factorisation Method
- Method
machine Method
supported Method
Neural Method
Networks Method
( O
FNN Method
) O
Our O
first O
model O
FNN Method
is O
based O
on O
the O
factorisation Method
machine Method
as O
the O
bottom Method
layer Method
. O
The O
network O
structure O
is O
shown O
in O
Figure O
[ O
reference O
] O
. O
With O
a O
top Method
- Method
down Method
description Method
, O
the O
output O
unit O
is O
a O
real O
number O
as O
predicted Task
CTR Task
, O
i.e. O
, O
the O
probability O
of O
a O
specific O
user O
clicking O
a O
given O
ad O
in O
a O
certain O
context O
: O
where O
is O
the O
logistic O
activation O
function O
, O
, O
and O
as O
input O
for O
this O
layer O
. O
The O
calculation O
of O
is O
where O
, O
, O
and O
. O
We O
choose O
as O
it O
has O
optimal O
empirical Metric
learning Metric
performance Metric
than O
other O
activation Method
functions Method
, O
as O
will O
be O
discussed O
in O
Section O
[ O
reference O
] O
. O
Similarly O
, O
where O
, O
and O
. O
where O
is O
a O
global O
scalar O
parameter O
and O
is O
the O
number O
of O
fields O
in O
total O
. O
is O
a O
parameter O
vectors O
for O
the O
- O
th O
field O
in O
factorisation Method
machines Method
: O
where O
and O
are O
starting O
and O
ending O
feature O
indexes O
of O
the O
- O
th O
field O
, O
and O
is O
the O
input O
vector O
as O
described O
at O
beginning O
. O
All O
weights O
are O
initialised O
with O
the O
bias O
term O
and O
vector O
respectively O
( O
e.g. O
, O
is O
initialised O
by O
, O
is O
initialised O
by O
, O
is O
initialised O
by O
, O
etc O
. O
) O
. O
In O
this O
way O
, O
vector O
of O
the O
first Method
layer Method
is O
initialised O
as O
shown O
in O
Figure O
[ O
reference O
] O
via O
training Task
a O
factorisation Method
machine Method
( O
FM Method
) O
: O
where O
each O
feature O
is O
assigned O
with O
a O
bias O
weight O
and O
a O
- O
dimensional O
vector O
and O
the O
feature O
interaction O
is O
modelled O
as O
their O
vectors O
’ O
inner O
product O
. O
In O
this O
way O
, O
the O
above O
neural Method
nets Method
can O
learn O
more O
efficiently O
from O
factorisation Method
machine Method
representation Method
so O
that O
the O
computational Metric
complexity Metric
problem Metric
of O
the O
high O
- O
dimensional O
binary O
inputs O
has O
been O
naturally O
bypassed O
. O
Different O
hidden O
layers O
can O
be O
regarded O
as O
different O
internal O
functions O
capturing O
different O
forms O
of O
representations O
of O
the O
data O
instance O
. O
For O
this O
reason O
, O
this O
model O
has O
more O
abilities O
of O
catching O
intrinsic O
data O
patterns O
and O
leads O
to O
better O
performance O
. O
The O
idea O
using O
FM Method
in O
the O
bottom O
layer O
is O
ignited O
by O
Convolutional Method
Neural Method
Networks Method
( O
CNNs Method
) O
, O
which O
exploit O
spatially O
local O
correlation O
by O
enforcing O
a O
local O
connectivity O
pattern O
between O
neurons O
of O
adjacent O
layers O
. O
Similarly O
, O
the O
inputs O
of O
hidden O
layer O
1 O
are O
connected O
to O
the O
input O
units O
of O
a O
specific O
field O
. O
Also O
, O
the O
bottom Method
layer Method
is O
not O
fully O
connected O
as O
FM Method
performs O
a O
field Method
- Method
wise Method
training Method
for O
one O
- O
hot O
sparse O
encoded O
input O
, O
allowing O
local O
sparsity O
, O
illustrated O
as O
the O
dash O
lines O
in O
Figure O
[ O
reference O
] O
. O
FM Method
learns O
good O
structural Method
data Method
representation Method
in O
the O
latent O
space O
, O
helpful O
for O
any O
further O
model O
to O
build O
on O
. O
A O
subtle O
difference O
, O
though O
, O
appears O
between O
the O
product O
rule O
of O
FM Method
and O
the O
sum Method
rule Method
of Method
DNN Method
for O
combination Task
. O
However O
, O
according O
to O
, O
if O
the O
observational O
discriminatory O
information O
is O
highly O
ambiguous O
( O
which O
is O
true O
in O
our O
case O
for O
ad Task
click Task
behaviour Task
) O
, O
the O
posterior O
weights O
( O
from O
DNN Method
) O
will O
not O
deviate O
dramatically O
from O
the O
prior O
( O
FM Method
) O
. O
Furthermore O
, O
the O
weights O
in O
hidden O
layers O
( O
except O
the O
FM Method
layer O
) O
are O
initialised O
by O
layer O
- O
wise O
RBM O
pre O
- O
training Task
using O
contrastive Method
divergence Method
, O
which O
effectively O
preserves O
the O
information O
in O
input O
dataset O
as O
detailed O
in O
. O
The O
initial O
weights O
for O
FMs Method
are O
trained O
by O
stochastic Method
gradient Method
descent Method
( O
SGD Method
) Method
, O
as O
detailed O
in O
. O
Note O
that O
we O
only O
need O
to O
update O
weights O
which O
connect O
to O
the O
positive O
input O
units O
, O
which O
largely O
reduces O
the O
computational Metric
complexity Metric
. O
After O
pre O
- O
training Task
of O
the O
FM Method
and O
upper O
layers O
, O
supervised Method
fine Method
- Method
tuning Method
( O
back Method
propagation Method
) O
is O
applied O
to O
minimise O
loss Metric
function Metric
of Metric
cross Metric
entropy Metric
: O
where O
is O
the O
predicted Task
CTR Task
in O
Eq O
. O
( O
[ O
reference O
] O
) O
and O
is O
the O
binary O
click O
ground O
- O
truth O
label O
. O
Using O
the O
chain Method
rule Method
of Method
back Method
propagation Method
, O
the O
FNN Method
weights O
including O
FM Method
weights O
can O
be O
efficiently O
updated O
. O
For O
example O
, O
we O
update O
FM Method
layer O
weights O
via O
Due O
to O
the O
fact O
that O
the O
majority O
entries O
of O
are O
0 O
, O
we O
can O
accelerate O
fine Task
- Task
tuning Task
by O
updating O
weights O
linking O
to O
positive O
units O
only O
. O
subsection O
: O
Sampling Method
- Method
based Method
Neural Method
Networks Method
( O
SNN Method
) O
The O
structure O
of O
the O
second O
model O
SNN Method
is O
shown O
in O
Figure O
[ O
reference O
] O
( O
a O
) O
. O
The O
difference O
between O
SNN Method
and O
FNN Method
lies O
in O
the O
structure O
and O
training Task
method O
in O
the O
bottom O
layer O
. O
SNN Method
’s O
bottom O
layer O
is O
fully O
connected O
with O
sigmoid Method
activation Method
function Method
: O
To O
initialise O
the O
weights O
of O
the O
bottom O
layer O
, O
we O
tried O
both O
restricted Method
Boltzmann Method
machine Method
( O
RBM Method
) O
and O
denoising Method
auto Method
- Method
encoder Method
( O
DAE Method
) O
in O
the O
pre O
- O
training Task
stage O
. O
In O
order O
to O
deal O
with O
the O
computational Task
problem Task
of O
training Task
large O
sparse O
one O
- O
hot O
encoding O
data O
, O
we O
propose O
a O
sampling Method
- Method
based Method
RBM Method
( O
Figure O
[ O
reference O
] O
( O
b O
) O
, O
denoted O
as O
SNN Method
- Method
RBM Method
) O
and O
a O
sampling Method
- Method
based Method
DAE Method
in O
( O
Figure O
[ O
reference O
] O
( O
c O
) O
, O
denoted O
as O
SNN Method
- Method
DAE Method
) O
to O
efficiently O
calculate O
the O
initial O
weights O
of O
the O
bottom O
layer O
. O
Instead O
of O
modelling O
the O
whole O
feature O
set O
for O
each O
training Task
instance O
set O
, O
for O
each O
feature O
field O
, O
e.g. O
, O
city O
, O
there O
is O
only O
one O
positive O
value O
feature O
for O
each O
training Task
instance O
, O
e.g. O
, O
city O
= O
London O
, O
we O
sample O
negative O
units O
, O
e.g. O
, O
city O
= O
Paris O
when O
, O
randomly O
with O
value O
0 O
. O
Black O
units O
in O
Figure O
[ O
reference O
] O
( O
b O
) O
and O
[ O
reference O
] O
( O
c O
) O
are O
unsampled O
and O
thus O
ignored O
when O
pre O
- O
training Task
the O
data O
instance O
. O
With O
the O
sampled O
units O
, O
we O
can O
train O
an O
RBM Method
via O
contrastive Method
divergence Method
and O
a O
DAE Method
via O
SGD Method
with O
unsupervised Method
approaches Method
to O
largely O
reduce O
the O
data O
dimension O
with O
high O
recovery Metric
performance Metric
. O
The O
real O
- O
value O
dense O
vector O
is O
used O
as O
the O
input O
of O
the O
further O
layers O
in O
SNN Method
. O
In O
this O
way O
, O
computational Metric
complexity Metric
can O
be O
dramatically O
reduced O
and O
, O
in O
turn O
, O
initial O
weights O
can O
be O
calculated O
quickly O
and O
back Method
- Method
propagation Method
is O
then O
performed O
to O
fine O
- O
tune O
SNN Method
model O
. O
subsection O
: O
Regularisation Method
To O
prevent O
overfitting O
, O
the O
widely O
used O
L2 Method
regularisation Method
term Method
is O
added O
to O
the O
loss O
function O
. O
For O
example O
, O
the O
L2 Method
regularisation Method
for O
FNN Method
in O
Figure O
[ O
reference O
] O
is O
On O
the O
other O
hand O
, O
dropout Method
is O
a O
technique O
which O
becomes O
a O
popular O
and O
effective O
regularisation Method
technique Method
for O
deep Task
learning Task
during O
the O
recent O
years O
. O
We O
also O
implement O
this O
regularisation Method
and O
compare O
them O
in O
our O
experiment O
. O
section O
: O
Experiment O
subsection O
: O
Experiment O
Setup O
Data O
. O
We O
evaluate O
our O
models O
based O
on O
iPinYou Material
dataset Material
, O
a O
public O
real O
- O
world O
display O
ad O
dataset O
with O
each O
ad O
display O
information O
and O
corresponding O
user O
click O
feedback O
. O
The O
data O
logs O
are O
organised O
by O
different O
advertisers O
and O
in O
a O
row O
- O
per O
- O
record O
format O
. O
There O
are O
19.50 O
M O
data O
instances O
with O
14.79 O
K O
positive O
label O
( O
click O
) O
in O
total O
. O
The O
features O
for O
each O
data O
instance O
are O
all O
categorical O
. O
Feature O
examples O
in O
the O
ad O
log O
data O
are O
user O
agent O
, O
partially O
masked O
IP O
, O
region O
, O
city O
, O
ad O
exchange O
, O
domain O
, O
URL O
, O
ad O
slot O
ID O
, O
ad O
slot O
visibility O
, O
ad O
slot O
size O
, O
ad O
slot O
format O
, O
creative O
ID O
, O
user O
tags O
, O
etc O
. O
After O
one Method
- Method
hot Method
encoding Method
, O
the O
number O
of O
binary O
features O
is O
937.67 O
K O
in O
the O
whole O
dataset O
. O
We O
feed O
each O
compared O
model O
with O
these O
binary O
- O
feature O
data O
instances O
and O
the O
user O
click O
( O
1 O
) O
and O
non O
- O
click O
( O
0 O
) O
feedback O
as O
the O
ground O
- O
truth O
labels O
. O
In O
our O
experiments O
, O
we O
use O
training Task
data O
from O
advertiser O
1458 O
, O
2259 O
, O
2261 O
, O
2997 O
, O
3386 O
and O
the O
whole O
dataset O
, O
respectively O
. O
Models O
. O
We O
compare O
the O
performance O
of O
the O
following O
CTR Metric
estimation O
models O
: O
Logistic Method
Regression Method
is O
a O
linear Method
model Method
with O
simple O
implementation O
and O
fast O
training Task
speed O
, O
which O
is O
widely O
used O
in O
online Task
advertising Task
estimation Task
. O
Factorisation Method
Machine Method
is O
a O
non Method
- Method
linear Method
model Method
able O
to O
estimate O
feature O
interactions O
even O
in O
problems O
with O
huge O
sparsity O
. O
Factorisation Method
- Method
machine Method
supported Method
Neural Method
Network Method
is O
our O
proposed O
model O
as O
described O
in O
Section O
[ O
reference O
] O
. O
Sampling Method
- Method
based Method
Neural Method
Network Method
is O
also O
our O
proposed O
model O
with O
sampling Method
- Method
based Method
RBM Method
and O
DAE Method
pre Method
- Method
training Method
methods Method
for O
the O
first O
layer O
in O
Section O
[ O
reference O
] O
, O
denoted O
as O
SNN Method
- Method
RBM Method
and O
SNN Method
- Method
DAE Method
respectively O
. O
Our O
experiment O
code O
of O
both O
FNN Method
and O
SNN Method
is O
implemented O
with O
TheanoTheano O
: O
http: O
// O
deeplearning.net O
/ O
software O
/ O
theano O
/ O
. O
Metric O
. O
To O
measure O
the O
CTR Task
estimation Task
performance O
of O
each O
model O
, O
we O
employ O
the O
area Metric
under Metric
ROC Metric
curve Metric
( O
AUC Metric
) O
. O
The O
AUC Metric
metric Metric
is O
a O
widely O
used O
measure O
for O
evaluating O
the O
CTR Metric
performance O
. O
subsection O
: O
Performance O
Comparison O
Table O
[ O
reference O
] O
shows O
the O
results O
that O
compare O
LR Method
, O
FM Method
, O
FNN Method
and O
SNN Method
with O
RBM Method
and O
DAE Method
on O
5 O
different O
advertisers O
and O
the O
whole O
dataset O
. O
We O
observe O
that O
FM Method
is O
not O
significantly O
better O
than O
LR Method
, O
which O
means O
2 O
- O
order O
combination O
features O
might O
not O
be O
good O
enough O
to O
catch O
the O
underlying O
data O
patterns O
. O
The O
AUC Metric
performance O
of O
the O
proposed O
FNN Method
and O
SNN Method
is O
better O
than O
the O
performance O
of O
LR Method
and O
FM Method
on O
all O
tested O
datasets O
. O
Based O
on O
the O
latent O
structure O
learned O
by O
FM Method
, O
FNN Method
further O
learns O
effective O
patterns O
between O
these O
latent O
features O
and O
provides O
a O
consistent O
improvement O
over O
FM Method
. O
The O
performance O
of O
SNN Method
- Method
DAE Method
and O
SNN Method
- Method
RBM Method
is O
generally O
consistent O
, O
i.e. O
, O
the O
relative O
order O
of O
the O
results O
of O
the O
SNN Method
are O
almost O
the O
same O
. O
subsection O
: O
Hyperparameter Method
Tuning Method
Due O
to O
the O
fact O
that O
deep Method
neural Method
networks Method
involve O
many O
implementation O
details O
and O
need O
to O
tune O
a O
fairly O
large O
number O
of O
hyper O
- O
parameters O
, O
following O
details O
show O
how O
we O
implement O
our O
models O
and O
tune O
hyperparameters O
in O
the O
models O
. O
We O
use O
stochastic Method
gradient Method
descent Method
to O
learn O
most O
of O
our O
parameters O
for O
all O
proposed O
models O
. O
Regarding O
selecting O
the O
number O
of O
training Task
epochs O
, O
we O
use O
early O
stopping O
, O
i.e. O
, O
the O
training Task
stops O
when O
the O
validation Metric
error Metric
increases O
. O
We O
try O
different O
learning Metric
rate Metric
from O
1 O
, O
0.1 O
, O
0.01 O
, O
0.001 O
to O
0.0001 O
and O
choose O
the O
one O
with O
optimal O
performance O
on O
the O
validation O
dataset O
. O
For O
negative O
unit O
sampling O
of O
SNN Method
- Method
RBM Method
and O
SNN Method
- Method
DAE Method
, O
we O
try O
the O
negative O
sample O
number O
and O
per O
field O
as O
described O
in O
Section O
[ O
reference O
] O
, O
and O
find O
produces O
the O
best O
results O
in O
most O
situations O
. O
For O
the O
activation Method
functions Method
in O
both O
models O
on O
the O
hidden O
layers O
( O
as O
Eqs O
. O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
) O
, O
we O
try O
linear O
function O
, O
sigmoid O
function O
and O
tanh Method
function Method
, O
and O
find O
the O
result O
of O
tanh Method
function Method
is O
optimal O
. O
This O
might O
be O
because O
the O
hyperbolic O
tangent O
often O
converges O
faster O
than O
the O
sigmoid O
function O
. O
subsection O
: O
Architecture Task
Selection Task
In O
our O
models O
, O
we O
investigate O
architectures O
with O
3 O
, O
4 O
and O
5 O
hidden O
layers O
by O
fixing O
all O
layer O
sizes O
and O
find O
the O
architecture O
with O
3 O
hidden O
layers O
( O
i.e. O
, O
5 O
layers O
in O
total O
) O
is O
the O
best O
in O
terms O
of O
AUC Metric
performance Metric
. O
However O
, O
the O
range O
of O
choosing O
their O
layer O
sizes O
is O
exponential O
in O
the O
number O
of O
hidden O
layers O
. O
Suppose O
there O
is O
a O
deep Method
neural Method
network Method
with O
hidden Method
layers Method
and O
each O
of O
the O
hidden O
layers O
is O
trained O
with O
a O
range O
of O
hidden O
units O
from O
100 O
to O
500 O
with O
increments O
of O
100 O
, O
thus O
there O
are O
models O
in O
total O
to O
compare O
. O
Instead O
of O
trying O
all O
combinations O
of O
hidden O
units O
, O
in O
our O
experiment O
we O
use O
another O
strategy O
by O
starting O
tuning O
the O
different O
hidden O
layer O
sizes O
with O
the O
same O
number O
of O
hidden O
units O
in O
all O
three O
hidden O
layers O
since O
the O
architecture O
with O
equal O
- O
size O
hidden O
layers O
is O
empirically O
better O
than O
the O
architecture O
with O
increasing O
width O
or O
decreasing O
width O
in O
. O
For O
this O
reason O
, O
we O
start O
tuning O
layer O
sizes O
with O
equal O
hidden O
layer O
sizes O
. O
In O
fact O
, O
apart O
from O
increasing O
, O
constant O
, O
decreasing O
layer O
sizes O
, O
there O
is O
a O
more O
effective O
structure O
, O
which O
is O
the O
diamond Method
shape Method
of Method
neural Method
networks Method
, O
as O
shown O
in O
Figure O
[ O
reference O
] O
. O
We O
compare O
our O
diamond Method
shape Method
network Method
with O
other O
three O
shapes O
of O
networks O
and O
tune O
the O
total O
number O
of O
total O
hidden O
units O
on O
two O
different O
datasets O
shown O
in O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
The O
diamond Method
shape Method
architecture Method
outperforms O
others O
in O
almost O
all O
layer Metric
size Metric
settings Metric
. O
The O
reason O
why O
this O
diamond O
shape O
works O
might O
be O
because O
this O
special O
shape O
of O
neural Method
network Method
has O
certain O
constraint O
to O
the O
capacity O
of O
the O
neural Method
network Method
, O
which O
provides O
better O
generalisation O
on O
test O
sets O
. O
On O
the O
other O
hand O
, O
the O
performance O
of O
diamond Method
architecture Method
picks O
at O
the O
total O
hidden O
unit O
size O
of O
600 O
, O
i.e. O
, O
the O
combination O
of O
( O
200 O
, O
300 O
, O
100 O
) O
. O
This O
depends O
on O
the O
training Task
data O
observation O
numbers O
. O
Too O
many O
hidden O
units O
against O
a O
limited O
dataset O
could O
cause O
overfitting O
. O
subsection O
: O
Regularisation Method
Comparison Method
Neural O
network O
training Task
algorithms O
are O
very O
sensitive O
to O
the O
overfitting Task
problem Task
since O
deep Method
networks Method
have O
multiple O
non O
- O
linear O
layers O
, O
which O
makes O
them O
very O
expressive O
models O
that O
can O
learn O
very O
complicated O
functions O
. O
For O
DNN Method
models Method
, O
we O
compared O
L2 Method
regularisation Method
( O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
and O
dropout Method
for O
preventing O
complex O
co Task
- Task
adaptations Task
on O
the O
training Task
data O
. O
The O
dropout Metric
rate Metric
implemented O
in O
this O
experiment O
refers O
to O
the O
probability O
of O
each O
unit O
being O
active O
. O
Figure O
[ O
reference O
] O
shows O
the O
compared O
AUC Metric
performance O
of O
SNN Method
- O
RBM O
regularised O
by O
L2 Method
norm Method
and O
dropout Method
. O
It O
is O
obvious O
that O
dropout Method
outperforms O
L2 Method
in O
all O
compared O
settings O
. O
The O
reason O
why O
dropout Method
is O
more O
effective O
is O
that O
when O
feeding O
each O
training Task
case O
, O
each O
hidden O
unit O
is O
stochastically O
excluded O
from O
the O
network O
with O
a O
probability O
of O
dropout Metric
rate Metric
, O
i.e. O
, O
each O
training Task
case O
can O
be O
regarded O
as O
a O
new O
model O
and O
these O
models O
are O
averaged O
as O
a O
special O
case O
of O
bagging Method
, O
which O
effectively O
improves O
the O
generalisation Metric
ability Metric
of O
DNN Method
models Method
. O
subsection O
: O
Analysis O
of O
Parameters O
As O
a O
summary O
of O
Sections O
[ O
reference O
] O
and O
[ O
reference O
] O
, O
for O
both O
FNN Method
and O
SNN Method
, O
there O
are O
two O
important O
parameters O
which O
should O
be O
tuned O
to O
make O
the O
model O
more O
effective O
: O
( O
i O
) O
the O
parameters O
of O
layer O
size O
decide O
the O
architecture O
of O
the O
neural Method
network Method
and O
( O
ii O
) O
the O
parameter O
of O
dropout Metric
rate Metric
changes O
generalisation Metric
ability Metric
on O
all O
datasets O
compared O
to O
neural Method
networks Method
just O
with O
L2 Method
regularisation Method
. O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
show O
how O
the O
AUC Metric
performance Metric
changes O
with O
the O
increasing O
of O
dropout O
in O
both O
FNN Method
and O
SNN Method
. O
We O
can O
find O
that O
there O
is O
an O
upward O
trend O
of O
performance O
in O
both O
models O
at O
the O
beginning O
and O
then O
drop O
sharply O
with O
continuous O
decreasing O
of O
dropout Metric
rate Metric
. O
The O
distinction O
between O
two O
models O
is O
the O
different O
sensitivities O
of O
the O
dropout O
. O
From O
Figure O
[ O
reference O
] O
, O
we O
can O
see O
the O
model O
SNN Method
is O
sensitive O
to O
the O
dropout Metric
rate Metric
. O
This O
might O
be O
caused O
by O
the O
connectivities O
in O
the O
bottom O
layer O
. O
The O
bottom O
layer O
of O
the O
SNN Method
is O
fully O
connected O
with O
the O
input O
vector O
while O
the O
bottom Method
layer Method
for O
FNN Method
is O
partially O
connected O
and O
thus O
the O
FNN Method
is O
more O
robust O
when O
some O
hidden O
units O
are O
dropped O
out O
. O
Furthermore O
, O
the O
sigmoid Method
activation Method
function Method
tend O
to O
more O
effective O
than O
the O
linear O
activation O
function O
in O
terms O
of O
dropout Task
. O
Therefore O
, O
the O
dropout Metric
rates Metric
at O
the O
best O
performance O
of O
FNN Method
and O
SNN Method
are O
quite O
different O
. O
For O
FNN Method
the O
optimal O
dropout Metric
rate Metric
is O
around O
0.8 O
while O
for O
SNN Method
is O
about O
0.99 O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
investigated O
the O
potential O
of O
training Task
deep Method
neural Method
networks Method
( O
DNNs Method
) O
to O
predict O
users Task
’ Task
ad Task
click Task
response Task
based O
on O
multi O
- O
field O
categorical O
features O
. O
To O
deal O
with O
the O
computational Task
complexity Task
problem Task
of Task
high Task
- Task
dimensional Task
discrete Task
categorical Task
features Task
, O
we O
proposed O
two O
DNN Method
models Method
: O
field Method
- Method
wise Method
feature Method
embedding Method
with O
supervised Method
factorisation Method
machine Method
pre Method
- Method
training Method
, O
and O
fully Method
connected Method
DNN Method
with O
field O
- O
wise O
sampling Method
- Method
based Method
RBM Method
and O
DAE O
unsupervised O
pre O
- O
training Task
. O
These O
architectures O
and O
pre O
- O
training Task
algorithms O
make O
our O
DNNs Method
trained O
very O
efficiently O
. O
Comprehensive O
experiments O
on O
a O
public O
real O
- O
world O
dataset O
verifies O
that O
the O
proposed O
DNN Method
models Method
successfully O
learn O
the O
underlying O
data O
patterns O
and O
provide O
superior O
CTR Task
estimation Task
performance O
than O
other O
compared O
models O
. O
The O
proposed O
models O
are O
very O
general O
and O
could O
enable O
a O
wide O
range O
of O
future O
works O
. O
For O
example O
, O
the O
model O
performance O
can O
be O
improved O
by O
momentum Method
methods Method
in O
that O
it O
suffices O
for O
handling O
the O
curvature Task
problems Task
in O
DNN O
training Task
objectives O
without O
using O
complex O
second Method
- Method
order Method
methods Method
. O
In O
addition O
, O
the O
partial O
connection O
in O
the O
bottom O
layer O
could O
be O
extended O
to O
higher O
hidden O
layers O
as O
partial O
connectivities O
have O
many O
advantages O
such O
as O
lower O
complexity Metric
, O
higher O
generalisation Metric
ability Metric
and O
more O
similar O
to O
human O
brain O
. O
bibliography O
: O
References O
