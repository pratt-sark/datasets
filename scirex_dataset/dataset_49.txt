document	O
:	O
Dual	Method
Path	Method
Networks	Method
In	O
this	O
work	O
,	O
we	O
present	O
a	O
simple	O
,	O
highly	O
efficient	O
and	O
modularized	O
Dual	O
Path	O
Network	O
(	O
DPN	Method
)	O
for	O
image	Task
classification	Task
which	O
presents	O
a	O
new	O
topology	O
of	O
connection	O
paths	O
internally	O
.	O
By	O
revealing	O
the	O
equivalence	O
of	O
the	O
state	O
-	O
of	O
-	Method
the	Method
-	Method
art	Method
Residual	Method
Network	Method
(	O
ResNet	Method
)	O
and	O
Densely	Method
Convolutional	Method
Network	Method
(	O
DenseNet	Method
)	O
within	O
the	O
HORNN	Method
framework	Method
,	O
we	O
find	O
that	O
ResNet	Method
enables	O
feature	Task
re	Task
-	Task
usage	Task
while	O
DenseNet	Method
enables	O
new	O
features	Task
exploration	Task
which	O
are	O
both	O
important	O
for	O
learning	O
good	O
representations	Task
.	O
To	O
enjoy	O
the	O
benefits	O
from	O
both	O
path	O
topologies	O
,	O
our	O
proposed	O
Dual	Method
Path	Method
Network	Method
shares	O
common	O
features	O
while	O
maintaining	O
the	O
flexibility	O
to	O
explore	O
new	O
features	O
through	O
dual	Method
path	Method
architectures	Method
.	O
Extensive	O
experiments	O
on	O
three	O
benchmark	O
datasets	O
,	O
ImagNet	Material
-	Material
1k	Material
,	O
Places365	O
and	O
PASCAL	O
VOC	O
,	O
clearly	O
demonstrate	O
superior	O
performance	O
of	O
the	O
proposed	O
DPN	Method
over	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
.	O
In	O
particular	O
,	O
on	O
the	O
ImagNet	Material
-	Material
1k	Material
dataset	O
,	O
a	O
shallow	O
DPN	Method
surpasses	O
the	O
best	O
ResNeXt	Method
-	Method
101	Method
(	O
d	O
)	O
with	O
26	O
%	O
smaller	O
model	Metric
size	Metric
,	O
25	O
%	O
less	O
computational	Metric
cost	Metric
and	O
8	O
%	O
lower	O
memory	Metric
consumption	Metric
,	O
and	O
a	O
deeper	O
DPN	Method
(	O
DPN	Method
-	Method
131	Method
)	O
further	O
pushes	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	Method
model	Method
performance	O
with	O
about	O
2	O
times	O
faster	O
training	Metric
speed	Metric
.	O
Experiments	O
on	O
the	O
Places365	O
large	O
-	O
scale	O
scene	O
dataset	O
,	O
PASCAL	O
VOC	O
detection	O
dataset	O
,	O
and	O
PASCAL	O
VOC	O
segmentation	O
dataset	O
also	O
demonstrate	O
its	O
consistently	O
better	O
performance	O
than	O
DenseNet	Method
,	O
ResNet	Method
and	O
the	O
latest	O
ResNeXt	Method
model	Method
over	O
various	O
applications	O
.	O
figs	O
/	O
section	O
:	O
Introduction	O
‘	O
‘	O
Network	Method
engineering	Method
’	O
’	O
is	O
increasingly	O
more	O
important	O
for	O
visual	Task
recognition	Task
research	Task
.	O
In	O
this	O
paper	O
,	O
we	O
aim	O
to	O
develop	O
new	O
path	Method
topology	Method
of	Method
deep	Method
architectures	Method
to	O
further	O
push	O
the	O
frontier	O
of	O
representation	Task
learning	Task
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
analyzing	O
and	O
reforming	O
the	O
skip	O
connection	O
,	O
which	O
has	O
been	O
widely	O
used	O
in	O
designing	O
modern	O
deep	Method
neural	Method
networks	Method
and	O
offers	O
remarkable	O
success	O
in	O
many	O
applications	O
.	O
Skip	Method
connection	Method
creates	O
a	O
path	O
propagating	O
information	O
from	O
a	O
lower	O
layer	O
directly	O
to	O
a	O
higher	O
layer	O
.	O
During	O
the	O
forward	Method
propagation	Method
,	O
skip	O
connection	O
enables	O
a	O
very	O
top	O
layer	O
to	O
access	O
information	O
from	O
a	O
distant	O
bottom	O
layer	O
;	O
while	O
for	O
the	O
backward	Method
propagation	Method
,	O
it	O
facilitates	O
gradient	Method
back	Method
-	Method
propagation	Method
to	O
the	O
bottom	O
layer	O
without	O
diminishing	O
magnitude	O
,	O
which	O
effectively	O
alleviates	O
the	O
gradient	Task
vanishing	Task
problem	Task
and	O
eases	O
the	O
optimization	Task
.	O
Deep	Method
Residual	Method
Network	Method
(	O
ResNet	Method
)	O
is	O
one	O
of	O
the	O
first	O
works	O
that	O
successfully	O
adopt	O
skip	O
connections	O
,	O
where	O
each	O
mirco	O
-	O
block	O
,	O
a.k.a	O
.	O
residual	O
function	O
,	O
is	O
associated	O
with	O
a	O
skip	O
connection	O
,	O
called	O
residual	O
path	O
.	O
The	O
residual	O
path	O
element	O
-	O
wisely	O
adds	O
the	O
input	O
features	O
to	O
the	O
output	O
of	O
the	O
same	O
mirco	O
-	O
block	O
,	O
making	O
it	O
a	O
residual	Method
unit	Method
.	O
Depending	O
on	O
the	O
inner	Method
structure	Method
design	Method
of	O
the	O
mirco	Method
-	Method
block	Method
,	O
the	O
residual	Method
network	Method
has	O
developed	O
into	O
a	O
family	O
of	O
various	O
architectures	O
,	O
including	O
WRN	Method
,	O
Inception	Method
-	Method
resnet	Method
,	O
and	O
ResNeXt	Method
.	O
More	O
recently	O
,	O
proposed	O
a	O
different	O
network	Method
architecture	Method
that	O
achieves	O
comparable	O
accuracy	Metric
with	O
deep	Method
ResNet	Method
,	O
named	O
Dense	Method
Convolutional	Method
Network	Method
(	O
DenseNet	Method
)	O
.	O
Different	O
from	O
residual	Method
networks	Method
which	O
add	O
the	O
input	O
features	O
to	O
the	O
output	O
features	O
through	O
the	O
residual	O
path	O
,	O
the	O
DenseNet	Method
uses	O
a	O
densely	O
connected	O
path	O
to	O
concatenate	O
the	O
input	O
features	O
with	O
the	O
output	O
features	O
,	O
enabling	O
each	O
micro	O
-	O
block	O
to	O
receive	O
raw	O
information	O
from	O
all	O
previous	O
micro	O
-	O
blocks	O
.	O
Similar	O
with	O
residual	O
network	O
family	O
,	O
DenseNet	Method
can	O
be	O
categorized	O
to	O
the	O
densely	O
connected	O
network	O
family	O
.	O
Although	O
the	O
width	O
of	O
the	O
densely	O
connected	O
path	O
increases	O
linearly	O
as	O
it	O
goes	O
deeper	O
,	O
causing	O
the	O
number	O
of	O
parameters	O
to	O
grow	O
quadratically	O
,	O
DenseNet	Method
provides	O
higher	O
parameter	Metric
efficiency	Metric
compared	O
with	O
the	O
ResNet	Method
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
study	O
the	O
advantages	O
and	O
limitations	O
of	O
both	O
topologies	O
and	O
further	O
enrich	O
the	O
path	Method
design	Method
by	O
proposing	O
a	O
dual	Method
path	Method
architecture	Method
.	O
In	O
particular	O
,	O
we	O
first	O
provide	O
a	O
new	O
understanding	O
of	O
the	O
densely	Method
connected	Method
networks	Method
from	O
the	O
lens	O
of	O
a	O
higher	Method
order	Method
recurrent	Method
neural	Method
network	Method
(	O
HORNN	Method
)	O
,	O
and	O
explore	O
the	O
relations	O
between	O
densely	Method
connected	Method
networks	Method
and	O
residual	Method
networks	Method
.	O
More	O
specifically	O
,	O
we	O
bridge	O
the	O
densely	Method
connected	Method
networks	Method
with	O
the	O
HORNNs	Method
,	O
showing	O
that	O
the	O
densely	Method
connected	Method
networks	Method
are	O
HORNNs	O
when	O
the	O
weights	O
are	O
shared	O
across	O
steps	O
.	O
Inspired	O
by	O
which	O
demonstrates	O
the	O
relations	O
between	O
the	O
residual	Method
networks	Method
and	O
RNNs	Method
,	O
we	O
prove	O
that	O
the	O
residual	Method
networks	Method
are	O
densely	Method
connected	Method
networks	Method
when	O
connections	O
are	O
shared	O
across	O
layers	O
.	O
With	O
this	O
unified	O
view	O
on	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	Method
architecture	Method
,	O
we	O
find	O
that	O
the	O
deep	Method
residual	Method
networks	Method
implicitly	O
reuse	O
the	O
features	O
through	O
the	O
residual	O
path	O
,	O
while	O
densely	Method
connected	Method
networks	Method
keep	O
exploring	O
new	O
features	O
through	O
the	O
densely	O
connected	O
path	O
.	O
Based	O
on	O
this	O
new	O
view	O
,	O
we	O
propose	O
a	O
novel	O
dual	Method
path	Method
architecture	Method
,	O
called	O
the	O
Dual	Method
Path	Method
Network	Method
(	O
DPN	Method
)	O
.	O
This	O
new	O
architecture	O
inherits	O
both	O
advantages	O
of	O
residual	O
and	O
densely	O
connected	O
paths	O
,	O
enabling	O
effective	O
feature	Task
re	Task
-	Task
usage	Task
and	O
re	Task
-	Task
exploitation	Task
.	O
The	O
proposed	O
DPN	Method
also	O
enjoys	O
higher	O
parameter	Metric
efficiency	Metric
,	O
lower	O
computational	Metric
cost	Metric
and	O
lower	O
memory	Metric
consumption	Metric
,	O
and	O
being	O
friendly	O
for	O
optimization	Task
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
classification	Method
networks	Method
.	O
Experimental	O
results	O
validate	O
the	O
outstanding	O
high	O
accuracy	Metric
of	O
DPN	Method
compared	O
with	O
other	O
well	O
-	O
established	O
baselines	O
for	O
image	Task
classification	Task
on	O
both	O
ImageNet	Material
-	Material
1k	Material
dataset	Material
and	O
Places365	O
-	O
Standard	O
dataset	O
.	O
Additional	O
experiments	O
on	O
object	Task
detection	Task
task	Task
and	O
semantic	Task
segmentation	Task
task	Task
also	O
demonstrate	O
that	O
the	O
proposed	O
dual	Method
path	Method
architecture	Method
can	O
be	O
broadly	O
applied	O
for	O
various	O
tasks	O
and	O
consistently	O
achieve	O
the	O
best	O
performance	O
.	O
section	O
:	O
Related	O
work	O
Designing	O
an	O
advanced	O
neural	Method
network	Method
architecture	Method
is	O
one	O
of	O
the	O
most	O
challenging	O
but	O
effective	O
ways	O
for	O
improving	O
the	O
image	Task
classification	Task
performance	O
,	O
which	O
can	O
also	O
directly	O
benefit	O
a	O
variety	O
of	O
other	O
tasks	O
.	O
AlexNet	Method
and	O
VGG	Method
are	O
two	O
most	O
important	O
works	O
that	O
show	O
the	O
power	O
of	O
deep	Method
convolutional	Method
neural	Method
networks	Method
.	O
They	O
demonstrate	O
that	O
building	O
deeper	Method
networks	Method
with	O
tiny	Method
convolutional	Method
kernels	Method
is	O
a	O
promising	O
way	O
to	O
increase	O
the	O
learning	Metric
capacity	Metric
of	O
the	O
neural	Method
network	Method
.	O
Residual	Method
Network	Method
was	O
first	O
proposed	O
by	O
,	O
which	O
greatly	O
alleviates	O
the	O
optimization	Metric
difficulty	Metric
and	O
further	O
pushes	O
the	O
depth	O
of	O
deep	Method
neural	Method
networks	Method
to	O
hundreds	O
of	O
layers	O
by	O
using	O
skipping	O
connections	O
.	O
Since	O
then	O
,	O
different	O
kinds	O
of	O
residual	Method
networks	Method
arose	O
,	O
concentrating	O
on	O
either	O
building	O
a	O
more	O
efficient	O
micro	O
-	O
block	O
inner	O
structure	O
or	O
exploring	O
how	O
to	O
use	O
residual	O
connections	O
.	O
Recently	O
,	O
proposed	O
a	O
different	O
network	O
,	O
called	O
Dense	Method
Convolutional	Method
Networks	Method
,	O
where	O
skip	O
connections	O
are	O
used	O
to	O
concatenate	O
the	O
input	O
to	O
the	O
output	O
instead	O
of	O
adding	O
.	O
However	O
,	O
the	O
width	O
of	O
the	O
densely	O
connected	O
path	O
linearly	O
increases	O
as	O
the	O
depth	O
rises	O
,	O
causing	O
the	O
number	O
of	O
parameters	O
to	O
grow	O
quadratically	O
and	O
costing	O
a	O
large	O
amount	O
of	O
GPU	O
memory	O
compared	O
with	O
the	O
residual	Method
networks	Method
if	O
the	O
implementation	O
is	O
not	O
specifically	O
optimized	O
.	O
This	O
limits	O
the	O
building	O
of	O
a	O
deeper	O
and	O
wider	O
densenet	O
that	O
may	O
further	O
improve	O
the	O
accuracy	Metric
.	O
Besides	O
designing	O
new	O
architectures	O
,	O
researchers	O
also	O
try	O
to	O
re	O
-	O
explore	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
architectures	O
.	O
In	O
,	O
the	O
authors	O
showed	O
the	O
importance	O
of	O
the	O
residual	O
path	O
on	O
alleviating	O
the	O
optimization	Task
difficulty	Task
.	O
In	O
,	O
the	O
residual	Method
networks	Method
are	O
bridged	O
with	O
recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
,	O
which	O
helps	O
people	O
better	O
understand	O
the	O
deep	Method
residual	Method
network	Method
from	O
the	O
perspective	O
of	O
RNNs	Method
.	O
In	O
,	O
several	O
different	O
residual	Method
functions	Method
are	O
unified	O
,	O
trying	O
to	O
provide	O
a	O
better	O
understanding	O
of	O
designing	O
a	O
better	O
mirco	Method
structure	Method
with	O
higher	O
learning	Metric
capacity	Metric
.	O
But	O
still	O
,	O
for	O
the	O
densely	Task
connected	Task
networks	Task
,	O
in	O
addition	O
to	O
several	O
intuitive	O
explanations	O
on	O
better	O
feature	O
reusage	O
and	O
efficient	O
gradient	Method
flow	Method
introduced	O
,	O
there	O
have	O
been	O
few	O
works	O
that	O
are	O
able	O
to	O
provide	O
a	O
really	O
deeper	O
understanding	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
provide	O
a	O
deeper	O
understanding	O
of	O
the	O
densely	Task
connected	Task
network	Task
,	O
from	O
the	O
lens	Method
of	Method
Higher	Method
Order	Method
RNN	Method
,	O
and	O
explain	O
how	O
the	O
residual	Method
networks	Method
are	O
in	O
indeed	O
a	O
special	O
case	O
of	O
densely	Method
connected	Method
network	Method
.	O
Based	O
on	O
these	O
analysis	O
,	O
we	O
then	O
propose	O
a	O
novel	O
Dual	Method
Path	Method
Network	Method
architecture	Method
that	O
not	O
only	O
achieves	O
higher	O
accuracy	Metric
,	O
but	O
also	O
enjoys	O
high	O
parameter	O
and	O
computational	Metric
efficiency	Metric
.	O
section	O
:	O
Revisiting	O
ResNet	O
,	O
DenseNet	Method
and	O
Higher	Method
Order	Method
RNN	Method
In	O
this	O
section	O
,	O
we	O
first	O
bridge	O
the	O
densely	Method
connected	Method
network	Method
with	O
higher	Method
order	Method
recurrent	Method
neural	Method
networks	Method
to	O
provide	O
a	O
new	O
understanding	O
of	O
the	O
densely	Method
connected	Method
network	Method
.	O
We	O
prove	O
that	O
residual	O
networks	O
,	O
essentially	O
belong	O
to	O
the	O
family	O
of	O
densely	O
connected	O
networks	O
except	O
their	O
connections	O
are	O
shared	O
across	O
steps	O
.	O
Then	O
,	O
we	O
present	O
analysis	O
on	O
strengths	O
and	O
weaknesses	O
of	O
each	O
topology	Method
architecture	Method
,	O
which	O
motivates	O
us	O
to	O
develop	O
the	O
dual	Method
path	Method
network	Method
architecture	Method
.	O
For	O
exploring	O
the	O
above	O
relation	O
,	O
we	O
provide	O
a	O
new	O
view	O
on	O
the	O
densely	O
connected	O
networks	O
from	O
the	O
lens	O
of	O
Higher	Method
Order	Method
RNN	Method
,	O
explain	O
their	O
relations	O
and	O
then	O
specialize	O
the	O
analysis	O
to	O
residual	Task
networks	Task
.	O
Throughout	O
the	O
paper	O
,	O
we	O
formulate	O
the	O
HORNN	Method
in	O
a	O
more	O
generalized	O
form	O
.	O
We	O
use	O
to	O
denote	O
the	O
hidden	O
state	O
of	O
the	O
recurrent	Method
neural	Method
network	Method
at	O
the	O
-	O
th	O
step	O
and	O
use	O
as	O
the	O
index	O
of	O
the	O
current	O
step	O
.	O
Let	O
denotes	O
the	O
input	O
at	O
-	O
th	O
step	O
,	O
.	O
For	O
each	O
step	O
,	O
refers	O
to	O
the	O
feature	Method
extracting	Method
function	Method
which	O
takes	O
the	O
hidden	O
state	O
as	O
input	O
and	O
outputs	O
the	O
extracted	O
information	O
.	O
The	O
denotes	O
a	O
transformation	Method
function	Method
that	O
transforms	O
the	O
gathered	O
information	O
to	O
current	O
hidden	O
state	O
:	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
encapsulates	O
the	O
update	O
rule	O
of	O
various	O
network	Method
architectures	Method
in	O
a	O
generalized	O
way	O
.	O
For	O
HORNNs	O
,	O
weights	O
are	O
shared	O
across	O
steps	O
,	O
i.e.	O
and	O
.	O
For	O
the	O
densely	Task
connected	Task
networks	Task
,	O
each	O
step	O
(	O
micro	O
-	O
block	O
)	O
has	O
its	O
own	O
parameter	O
,	O
which	O
means	O
and	O
are	O
not	O
shared	O
.	O
Such	O
observation	O
shows	O
that	O
the	O
densely	O
connected	O
path	O
of	O
DenseNet	Method
is	O
essentially	O
a	O
higher	O
order	O
path	O
which	O
is	O
able	O
to	O
extract	O
new	O
information	O
from	O
previous	O
states	O
.	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)(	O
d	O
)	O
graphically	O
shows	O
the	O
relations	O
of	O
densely	Method
connected	Method
networks	Method
and	O
higher	Method
order	Method
recurrent	Method
networks	Method
.	O
We	O
then	O
explain	O
that	O
the	O
residual	Method
networks	Method
are	O
special	O
cases	O
of	O
densely	Method
connected	Method
networks	Method
if	O
taking	O
.	O
Here	O
,	O
for	O
succinctness	O
we	O
introduce	O
to	O
denote	O
the	O
intermediate	O
results	O
and	O
let	O
.	O
Then	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
rewritten	O
as	O
Thus	O
,	O
by	O
substituting	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
into	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
simplified	O
as	O
where	O
.	O
Obviously	O
,	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
has	O
the	O
same	O
form	O
as	O
the	O
residual	Method
network	Method
and	O
the	O
recurrent	Method
neural	Method
network	Method
.	O
Specifically	O
,	O
when	O
,	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
degenerates	O
to	O
an	O
RNN	Method
;	O
when	O
none	O
of	O
is	O
shared	O
and	O
,	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
produces	O
a	O
residual	Method
network	Method
.	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)(	O
b	O
)	O
graphically	O
shows	O
the	O
relation	O
.	O
Besides	O
,	O
recall	O
that	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
derived	O
under	O
the	O
condition	O
when	O
from	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
the	O
densely	O
connected	O
networks	O
are	O
in	O
forms	O
of	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
meaning	O
that	O
the	O
residual	O
network	O
family	O
essentially	O
belongs	O
to	O
the	O
densely	O
connected	O
network	O
family	O
.	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
–	O
c	O
)	O
give	O
an	O
example	O
and	O
demonstrate	O
such	O
equivalence	O
,	O
where	O
corresponds	O
to	O
the	O
first	O
convolutional	Method
layer	Method
and	O
the	O
corresponds	O
to	O
the	O
other	O
layers	O
within	O
a	O
micro	O
-	O
block	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
.	O
From	O
the	O
above	O
analysis	O
,	O
we	O
observe	O
:	O
1	O
)	O
both	O
residual	O
networks	O
and	O
densely	O
connected	O
networks	O
can	O
be	O
seen	O
as	O
a	O
HORNN	Method
when	O
and	O
are	O
shared	O
for	O
all	O
;	O
2	O
)	O
a	O
residual	Method
network	Method
is	O
a	O
densely	Method
connected	Method
network	Method
if	O
.	O
By	O
sharing	O
the	O
across	O
all	O
steps	O
,	O
receives	O
the	O
same	O
feature	O
from	O
a	O
given	O
output	O
state	O
,	O
which	O
encourages	O
the	O
feature	O
reusage	O
and	O
thus	O
reduces	O
the	O
feature	O
redundancy	O
.	O
However	O
,	O
such	O
an	O
information	Method
sharing	Method
strategy	Method
makes	O
it	O
difficult	O
for	O
residual	Method
networks	Method
to	O
explore	O
new	O
features	O
.	O
Comparatively	O
,	O
the	O
densely	Method
connected	Method
networks	Method
are	O
able	O
to	O
explore	O
new	O
information	O
from	O
previous	O
outputs	O
since	O
the	O
is	O
not	O
shared	O
across	O
steps	O
.	O
However	O
,	O
different	O
may	O
extract	O
the	O
same	O
type	O
of	O
features	O
multiple	O
times	O
,	O
leading	O
to	O
high	O
redundancy	O
.	O
In	O
the	O
following	O
section	O
,	O
we	O
present	O
the	O
dual	Method
path	Method
networks	Method
which	O
can	O
overcome	O
both	O
inherent	O
limitations	O
of	O
these	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
network	Method
architectures	Method
.	O
Their	O
relations	O
with	O
HORNN	Method
also	O
imply	O
that	O
our	O
proposed	O
architecture	O
can	O
be	O
used	O
for	O
improving	O
HORNN	Method
,	O
which	O
we	O
leave	O
for	O
future	O
works	O
.	O
section	O
:	O
Dual	Method
Path	Method
Networks	Method
Above	O
we	O
explain	O
the	O
relations	O
between	O
residual	Method
networks	Method
and	O
densely	Method
connected	Method
networks	Method
,	O
showing	O
that	O
the	O
residual	O
path	O
implicitly	O
reuses	O
features	O
,	O
but	O
it	O
is	O
not	O
good	O
at	O
exploring	O
new	O
features	O
.	O
In	O
contrast	O
the	O
densely	Method
connected	Method
network	Method
keeps	O
exploring	O
new	O
features	O
but	O
suffers	O
from	O
higher	O
redundancy	O
.	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
details	O
of	O
our	O
proposed	O
novel	O
dual	Method
path	Method
architecture	Method
,	O
i.e.	O
the	O
Dual	Method
Path	Method
Network	Method
(	O
DPN	Method
)	Method
.	O
In	O
the	O
following	O
,	O
we	O
first	O
introduce	O
and	O
formulate	O
the	O
dual	Method
path	Method
architecture	Method
,	O
and	O
then	O
present	O
the	O
network	O
structure	O
in	O
details	O
with	O
complexity	Task
analysis	Task
.	O
subsection	O
:	O
Dual	Method
Path	Method
Architecture	Method
Sec	O
.	O
[	O
reference	O
]	O
discusses	O
the	O
advantage	O
and	O
limitations	O
of	O
both	O
residual	Method
networks	Method
and	O
densely	Method
connected	Method
networks	Method
.	O
Based	O
on	O
the	O
analysis	O
,	O
we	O
propose	O
a	O
simple	O
dual	Method
path	Method
architecture	Method
which	O
shares	O
the	O
across	O
all	O
blocks	O
to	O
enjoy	O
the	O
benefits	O
of	O
reusing	O
common	O
features	O
with	O
low	O
redundancy	O
,	O
while	O
still	O
remaining	O
a	O
densely	O
connected	O
path	O
to	O
give	O
the	O
network	O
more	O
flexibility	O
in	O
learning	O
new	O
features	O
.	O
We	O
formulate	O
such	O
a	O
dual	Method
path	Method
architecture	Method
as	O
follows	O
:	O
where	O
and	O
denote	O
the	O
extracted	O
information	O
at	O
-	O
th	O
step	O
from	O
individual	O
path	O
,	O
is	O
a	O
feature	Method
learning	Method
function	Method
as	O
.	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
refers	O
to	O
the	O
densely	O
connected	O
path	O
that	O
enables	O
exploring	O
new	O
features	O
,	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
refers	O
to	O
the	O
residual	O
path	O
that	O
enables	O
common	O
features	O
re	O
-	O
usage	O
,	O
and	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
defines	O
the	O
dual	O
path	O
that	O
integrates	O
them	O
and	O
feeds	O
them	O
to	O
the	O
last	O
transformation	O
function	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
The	O
final	O
transformation	O
function	O
generates	O
current	O
state	O
,	O
which	O
is	O
used	O
for	O
making	O
next	Task
mapping	Task
or	Task
prediction	Task
.	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)(	O
e	O
)	O
show	O
an	O
example	O
of	O
the	O
dual	Method
path	Method
architecture	Method
that	O
is	O
being	O
used	O
in	O
our	O
experiments	O
.	O
More	O
generally	O
,	O
the	O
proposed	O
DPN	Method
is	O
a	O
family	O
of	O
convolutional	Method
neural	Method
networks	Method
which	O
contains	O
a	O
residual	O
alike	O
path	O
and	O
a	O
densely	O
connected	O
alike	O
path	O
,	O
as	O
explained	O
later	O
.	O
Similar	O
to	O
these	O
networks	O
,	O
one	O
can	O
customize	O
the	O
micro	O
-	O
block	O
function	O
of	O
DPN	Method
for	O
task	O
-	O
specific	O
usage	O
or	O
for	O
further	O
overall	O
performance	O
boosting	Task
.	O
subsection	O
:	O
Dual	Method
Path	Method
Networks	Method
The	O
proposed	O
network	O
is	O
built	O
by	O
stacking	O
multiple	O
modualized	O
mirco	O
-	O
blocks	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
In	O
this	O
work	O
,	O
the	O
structure	O
of	O
each	O
micro	O
-	O
block	O
is	O
designed	O
with	O
a	O
bottleneck	Method
style	Method
which	O
starts	O
with	O
a	O
convolutional	Method
layer	Method
followed	O
by	O
a	O
convolutional	Method
layer	Method
,	O
and	O
ends	O
with	O
a	O
convolutional	Method
layer	Method
.	O
The	O
output	O
of	O
the	O
last	O
convolutional	Method
layer	Method
is	O
split	O
into	O
two	O
parts	O
:	O
the	O
first	O
part	O
is	O
element	O
-	O
wisely	O
added	O
to	O
the	O
residual	O
path	O
,	O
and	O
the	O
second	O
part	O
is	O
concatenated	O
with	O
the	O
densly	O
connected	O
path	O
.	O
To	O
enhance	O
the	O
leaning	O
capacity	O
of	O
each	O
micro	O
-	O
block	O
,	O
we	O
use	O
the	O
grouped	Method
convolution	Method
layer	Method
in	O
the	O
second	O
layer	O
as	O
the	O
ResNeXt	O
.	O
Considering	O
that	O
the	O
residual	Method
networks	Method
are	O
more	O
wildly	O
used	O
than	O
the	O
densely	O
connected	O
networks	O
in	O
practice	O
,	O
we	O
choose	O
the	O
residual	Method
network	Method
as	O
the	O
backbone	O
and	O
add	O
a	O
thin	O
densely	O
connected	O
path	O
to	O
build	O
the	O
dual	Method
path	Method
network	Method
.	O
Such	O
design	O
also	O
helps	O
slow	O
the	O
width	O
increment	O
of	O
the	O
densely	O
connected	O
path	O
and	O
the	O
cost	O
of	O
GPU	O
memory	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
detailed	O
architecture	O
settings	O
.	O
In	O
the	O
table	O
,	O
refers	O
to	O
the	O
number	O
of	O
groups	O
,	O
and	O
refers	O
to	O
the	O
channels	O
increment	O
for	O
the	O
densely	O
connected	O
path	O
.	O
For	O
the	O
new	O
proposed	O
DPNs	Method
,	O
we	O
use	O
to	O
indicate	O
the	O
width	O
increment	O
of	O
the	O
densely	O
connected	O
path	O
.	O
The	O
overall	O
design	O
of	O
DPN	Method
inherits	O
backbone	Method
architecture	Method
of	O
the	O
vanilla	Method
ResNet	Method
/	Method
ResNeXt	Method
,	O
making	O
it	O
very	O
easy	O
to	O
implement	O
and	O
apply	O
to	O
other	O
tasks	O
.	O
One	O
can	O
simply	O
implement	O
a	O
DPN	Method
by	O
adding	O
one	O
more	O
‘	O
‘	O
slice	O
layer	O
’	O
’	O
and	O
‘	O
‘	O
concat	Method
layer	Method
’	O
’	O
upon	O
existing	O
residual	Method
networks	Method
.	O
Under	O
a	O
well	O
optimized	O
deep	Method
learning	Method
platform	Method
,	O
none	O
of	O
these	O
newly	O
added	O
operations	O
requires	O
extra	O
computational	Metric
cost	Metric
or	O
extra	O
memory	O
consumption	O
,	O
making	O
the	O
DPNs	Method
highly	O
efficient	O
.	O
In	O
order	O
to	O
demonstrate	O
the	O
appealing	O
effectiveness	O
of	O
the	O
dual	Method
path	Method
architecture	Method
,	O
we	O
intentionally	O
design	O
a	O
set	O
of	O
DPNs	Method
with	O
a	O
considerably	O
smaller	O
model	Metric
size	Metric
and	O
less	O
FLOPs	O
compared	O
with	O
the	O
sate	O
-	O
of	O
-	O
the	O
-	O
art	O
ResNeXts	Method
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Due	O
to	O
limited	O
computational	O
resources	O
,	O
we	O
set	O
these	O
hyper	O
-	O
parameters	O
based	O
on	O
our	O
previous	O
experience	O
instead	O
of	O
grid	Method
search	Method
experiments	O
.	O
paragraph	O
:	O
Model	Metric
complexity	Metric
We	O
measure	O
the	O
model	Metric
complexity	Metric
by	O
counting	O
the	O
total	O
number	O
of	O
learnable	O
parameters	O
within	O
each	O
neural	Method
network	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
for	O
different	O
models	O
.	O
The	O
DPN	Method
-	Method
92	Method
costs	O
about	O
fewer	O
parameters	O
than	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
,	O
while	O
the	O
DPN	Method
-	Method
98	Method
costs	O
about	O
fewer	O
parameters	O
than	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
.	O
paragraph	O
:	O
Computational	Metric
complexity	Metric
We	O
measure	O
the	O
computational	Metric
cost	Metric
of	O
each	O
deep	Method
neural	Method
network	Method
using	O
the	O
floating	O
-	O
point	O
operations	O
(	O
FLOPs	O
)	O
with	O
input	O
size	O
of	O
,	O
in	O
the	O
number	O
of	O
multiply	O
-	O
adds	O
following	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
theoretical	Metric
computational	Metric
cost	Metric
.	O
Though	O
the	O
actual	O
time	Metric
cost	Metric
might	O
be	O
influenced	O
by	O
other	O
factors	O
,	O
e.g.	O
GPU	O
bandwidth	O
and	O
coding	Metric
quality	Metric
,	O
the	O
computational	Metric
cost	Metric
shows	O
the	O
speed	Metric
upper	Metric
bound	Metric
.	O
As	O
can	O
be	O
see	O
from	O
the	O
results	O
,	O
DPN	Method
-	Method
92	Method
consumes	O
about	O
less	O
FLOPs	O
than	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
,	O
and	O
the	O
DPN	Method
-	Method
98	Method
consumes	O
about	O
less	O
FLOPs	O
than	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
.	O
section	O
:	O
Experiments	O
Extensive	O
experiments	O
are	O
conducted	O
for	O
evaluating	O
the	O
proposed	O
Dual	Method
Path	Method
Networks	Method
.	O
Specifically	O
,	O
we	O
evaluate	O
the	O
proposed	O
architecture	O
on	O
three	O
tasks	O
:	O
image	Task
classification	Task
,	O
object	Task
detection	Task
and	O
semantic	Task
segmentation	Task
,	O
using	O
three	O
standard	O
benchmark	O
datasets	O
:	O
the	O
ImageNet	Material
-	Material
1k	Material
dataset	Material
,	O
Places365	O
-	O
Standard	O
dataset	O
and	O
the	O
PASCAL	O
VOC	O
datasets	O
.	O
Key	O
properties	O
of	O
the	O
proposed	O
DPNs	Method
are	O
studied	O
on	O
the	O
ImageNet	Material
-	Material
1k	Material
object	Material
classification	Material
dataset	Material
and	O
further	O
verified	O
on	O
the	O
Places365	O
-	O
Standard	O
scene	O
understanding	O
dataset	O
.	O
To	O
verify	O
whether	O
the	O
proposed	O
DPNs	Method
can	O
benefit	O
other	O
tasks	O
besides	O
image	Task
classification	Task
,	O
we	O
further	O
conduct	O
experiments	O
on	O
the	O
PASCAL	O
VOC	O
dataset	O
to	O
evaluate	O
its	O
performance	O
in	O
object	Task
detection	Task
and	O
semantic	Task
segmentation	Task
.	O
subsection	O
:	O
Experiments	O
on	O
image	Task
classification	Task
task	O
We	O
implement	O
the	O
DPNs	Method
using	O
MXNet	Method
on	O
a	O
cluster	O
with	O
40	O
K80	O
graphic	O
cards	O
.	O
Following	O
,	O
we	O
adopt	O
standard	O
data	Method
augmentation	Method
methods	Method
and	O
train	O
the	O
networks	O
using	O
SGD	Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
32	O
for	O
each	O
GPU	O
.	O
For	O
the	O
deepest	Method
network	Method
,	O
i.e.	O
DPN	Method
-	Method
131	Method
,	O
the	O
mini	O
-	O
batch	O
size	O
is	O
limited	O
to	O
24	O
because	O
of	O
the	O
12	O
GB	O
GPU	O
memory	O
constraint	O
.	O
The	O
learning	Metric
rate	Metric
starts	O
from	O
for	O
DPN	Method
-	Method
92	Method
and	O
DPN	Method
-	Method
131	Method
,	O
and	O
from	O
for	O
DPN	Method
-	Method
98	Method
.	O
It	O
drops	O
in	O
a	O
‘	O
‘	O
steps	O
’	O
’	O
manner	O
by	O
a	O
factor	O
of	O
.	O
Following	O
,	O
batch	Method
normalization	Method
layers	Method
are	O
refined	O
after	O
training	O
.	O
subsubsection	O
:	O
ImageNet	Material
-	Material
1k	Material
dataset	Material
Firstly	O
,	O
we	O
compare	O
the	O
image	Task
classification	Task
performance	O
of	O
DPNs	Method
with	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
As	O
can	O
be	O
seen	O
from	O
the	O
first	O
block	O
in	O
Table	O
[	O
reference	O
]	O
,	O
a	O
shallow	O
DPN	Method
with	O
only	O
the	O
depth	O
of	O
92	O
reduces	O
the	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
by	O
an	O
absolute	O
value	O
of	O
compared	O
with	O
the	O
ResNeXt	Method
-	Method
101	Method
(	O
)	O
and	O
an	O
absolute	O
value	O
of	O
compared	O
with	O
the	O
DenseNet	Method
-	Method
161	Method
yet	O
provides	O
with	O
considerably	O
less	O
FLOPs	O
.	O
In	O
the	O
second	O
block	O
of	O
Table	O
[	O
reference	O
]	O
,	O
a	O
deeper	O
DPN	Method
(	O
DPN	Method
-	Method
98	Method
)	O
surpasses	O
the	O
best	O
residual	Method
network	Method
–	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
,	O
and	O
still	O
enjoys	O
less	O
FLOPs	O
and	O
a	O
much	O
smaller	O
model	Metric
size	Metric
(	O
236	O
MB	O
v.s	O
.	O
320	O
MB	O
)	O
.	O
In	O
order	O
to	O
further	O
push	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
,	O
we	O
slightly	O
increase	O
the	O
depth	O
of	O
the	O
DPN	Method
to	O
131	O
(	O
DPN	Method
-	Method
131	Method
)	O
.	O
The	O
results	O
are	O
shown	O
in	O
the	O
last	O
block	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Again	O
,	O
the	O
DPN	Method
shows	O
superior	O
accuracy	Metric
over	O
the	O
best	O
single	O
model	O
–	O
Very	O
Deep	Method
PolyNet	Method
,	O
with	O
a	O
much	O
smaller	O
model	O
size	O
(	O
304	O
MB	O
v.s	O
.	O
365	O
MB	O
)	O
.	O
Note	O
that	O
the	O
Very	O
Deep	Method
PolyNet	Method
adopts	O
numerous	O
tricks	O
,	O
e.g.	O
initialization	O
by	O
insertion	O
,	O
residual	O
scaling	O
,	O
stochastic	O
paths	O
,	O
to	O
assist	O
the	O
training	Task
process	Task
.	O
In	O
contrast	O
,	O
our	O
proposed	O
DPN	Method
-	Method
131	Method
is	O
simple	O
and	O
does	O
not	O
involve	O
these	O
tricks	O
,	O
DPN	Method
-	Method
131	Method
can	O
be	O
trained	O
using	O
a	O
standard	O
training	Method
strategy	Method
as	O
shallow	O
DPNs	Method
.	O
More	O
importantly	O
,	O
the	O
actual	O
training	Metric
speed	Metric
of	O
DPN	Method
-	Method
131	Method
is	O
about	O
2	O
times	O
faster	O
than	O
the	O
Very	O
Deep	Method
PolyNet	Method
,	O
as	O
discussed	O
in	O
the	O
following	O
paragraph	O
.	O
Secondly	O
,	O
we	O
compare	O
the	O
training	Metric
cost	Metric
between	O
the	O
best	O
performing	O
models	O
.	O
Here	O
,	O
we	O
focus	O
on	O
evaluating	O
two	O
key	O
properties	O
–	O
the	O
actual	O
GPU	Metric
memory	Metric
cost	Metric
and	O
the	O
actual	O
training	Metric
speed	Metric
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
results	O
.	O
As	O
can	O
be	O
seen	O
from	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)(	O
b	O
)	O
,	O
the	O
DPN	Method
-	Method
98	Method
is	O
faster	O
and	O
uses	O
less	O
memory	O
than	O
the	O
best	O
performing	O
ResNeXt	Method
with	O
a	O
considerably	O
lower	O
testing	Metric
error	Metric
rate	Metric
.	O
Note	O
that	O
theoretically	O
the	O
computational	Metric
cost	Metric
of	O
DPN	Method
-	Method
98	Method
shown	O
in	O
Table	O
[	O
reference	O
]	O
is	O
less	O
than	O
the	O
best	O
performing	O
ResNeXt	Method
,	O
indicating	O
there	O
is	O
still	O
room	O
for	O
code	Task
optimization	Task
.	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
presents	O
the	O
same	O
result	O
in	O
a	O
more	O
clear	O
way	O
.	O
The	O
deeper	O
DPN	Method
-	O
131	O
only	O
costs	O
about	O
more	O
training	Metric
time	Metric
compared	O
with	O
the	O
best	O
performing	O
ResNeXt	Method
,	O
but	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
model	O
performance	O
.	O
The	O
training	Metric
speed	Metric
of	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
model	O
,	O
i.e.	O
Very	O
Deep	Method
PolyNet	Method
(	O
537	O
layers	O
)	O
,	O
is	O
about	O
31	O
samples	O
per	O
second	O
based	O
on	O
our	O
implementation	O
using	O
MXNet	Method
,	O
showing	O
that	O
DPN	Method
-	Method
131	Method
runs	O
about	O
2	O
times	O
faster	O
than	O
the	O
Very	O
Deep	Method
PolyNet	Method
during	O
training	O
.	O
subsubsection	O
:	O
Place365	O
-	O
Standard	O
dataset	O
In	O
this	O
experiment	O
,	O
we	O
further	O
evaluate	O
the	O
accuracy	Metric
of	O
the	O
proposed	O
DPN	Method
on	O
the	O
scene	Task
classification	Task
task	Task
using	O
the	O
Places365	O
-	O
Standard	O
dataset	O
.	O
The	O
Places365	O
-	O
Standard	O
dataset	O
is	O
a	O
high	O
-	O
resolution	O
scene	O
understanding	O
dataset	O
with	O
more	O
than	O
1.8	O
million	O
images	O
of	O
365	O
scene	O
categories	O
.	O
Different	O
from	O
object	O
images	O
,	O
scene	O
images	O
do	O
not	O
have	O
very	O
clear	O
discriminative	O
patterns	O
and	O
require	O
a	O
higher	O
level	O
context	O
reasoning	O
ability	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
different	O
models	O
on	O
this	O
dataset	O
.	O
To	O
make	O
a	O
fair	O
comparison	O
,	O
we	O
perform	O
the	O
DPN	Method
-	Method
92	Method
on	O
this	O
dataset	O
instead	O
of	O
using	O
deeper	O
DPNs	Method
.	O
As	O
can	O
be	O
seen	O
from	O
the	O
results	O
,	O
DPN	Method
achieves	O
the	O
best	O
validation	O
accuracy	Metric
compared	O
with	O
other	O
methods	O
.	O
The	O
DPN	Method
-	Method
92	Method
requires	O
much	O
less	O
parameters	O
(	O
138	O
MB	O
v.s	O
.	O
163	O
MB	O
)	O
,	O
which	O
again	O
demonstrates	O
its	O
high	O
parameter	Metric
efficiency	Metric
and	O
high	O
generalization	Metric
ability	Metric
.	O
subsection	O
:	O
Experiments	O
on	O
the	O
object	Task
detection	Task
task	Task
We	O
further	O
evaluate	O
the	O
proposed	O
Dual	Method
Path	Method
Network	Method
on	O
the	O
object	Task
detection	Task
task	Task
.	O
Experiments	O
are	O
performed	O
on	O
the	O
PASCAL	O
VOC	O
2007	O
datasets	O
.	O
We	O
train	O
the	O
models	O
on	O
the	O
union	O
set	O
of	O
VOC	O
2007	O
trainval	O
and	O
VOC	O
2012	O
trainval	O
following	O
,	O
and	O
evaluate	O
them	O
on	O
VOC	O
2007	O
test	O
set	O
.	O
We	O
use	O
standard	O
evaluation	Metric
metrics	Metric
Average	Metric
Precision	Metric
(	O
AP	Metric
)	O
and	O
mean	Metric
of	Metric
AP	Metric
(	O
mAP	Metric
)	O
following	O
the	O
PASCAL	Method
challenge	Method
protocols	Method
for	O
evaluation	O
.	O
We	O
perform	O
all	O
experiments	O
based	O
on	O
the	O
ResNet	Method
-	Method
based	Method
Faster	Method
R	Method
-	Method
CNN	Method
framework	Method
,	O
following	O
and	O
make	O
comparisons	O
by	O
replacing	O
the	O
residual	Method
network	Method
,	O
while	O
keeping	O
other	O
parts	O
unchanged	O
.	O
Since	O
our	O
goal	O
is	O
to	O
evaluate	O
DPN	Method
,	O
rather	O
than	O
further	O
push	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
on	O
this	O
dataset	O
,	O
we	O
adopt	O
the	O
shallowest	O
DPN	Method
-	O
92	O
and	O
baseline	O
networks	O
at	O
roughly	O
the	O
same	O
complexity	O
level	O
.	O
Table	O
[	O
reference	O
]	O
provides	O
the	O
detection	Task
performance	O
comparisons	O
of	O
the	O
proposed	O
DPN	Method
with	O
several	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	Method
models	Method
.	O
It	O
can	O
be	O
observed	O
that	O
the	O
DPN	Method
obtains	O
the	O
mAP	Metric
of	O
,	O
which	O
makes	O
large	O
improvements	O
,	O
i.e.	O
compared	O
with	O
ResNet	Method
-	Method
101	Method
and	O
compared	O
with	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
.	O
The	O
better	O
results	O
shown	O
in	O
this	O
experiment	O
demonstrate	O
that	O
the	O
Dual	Method
Path	Method
Network	Method
is	O
also	O
capable	O
of	O
learning	O
better	O
feature	Method
representations	Method
for	O
detecting	Task
objects	Task
and	O
benefiting	O
the	O
object	Task
detection	Task
task	Task
.	O
subsection	O
:	O
Experiments	O
on	O
the	O
semantic	Task
segmentation	Task
task	Task
In	O
this	O
experiment	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
the	O
Dual	Method
Path	Method
Network	Method
for	O
dense	Task
prediction	Task
,	O
i.e.	O
semantic	Task
segmentation	Task
,	O
where	O
the	O
training	O
target	O
is	O
to	O
predict	O
the	O
semantic	O
label	O
for	O
each	O
pixel	O
in	O
the	O
input	O
image	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
PASCAL	O
VOC	O
2012	O
segmentation	O
benchmark	O
dataset	O
and	O
use	O
the	O
DeepLab	Method
-	Method
ASPP	Method
-	Method
L	Method
as	O
the	O
segmentation	Method
framework	Method
.	O
For	O
each	O
compared	O
method	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
replace	O
the	O
convolutional	Method
layers	Method
in	O
conv4	Method
and	O
conv5	Method
of	O
Table	O
[	O
reference	O
]	O
with	O
atrous	Method
convolution	Method
and	O
plug	O
in	O
a	O
head	O
of	O
Atrous	Method
Spatial	Method
Pyramid	Method
Pooling	Method
(	O
ASPP	Method
)	O
in	O
the	O
final	O
feature	O
maps	O
of	O
conv5	Method
.	O
We	O
adopt	O
the	O
same	O
training	Method
strategy	Method
for	O
all	O
networks	O
following	O
for	O
fair	O
comparison	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
different	O
convolutional	Method
neural	Method
networks	Method
.	O
It	O
can	O
be	O
observed	O
that	O
the	O
proposed	O
DPN	Method
-	Method
92	Method
has	O
the	O
highest	O
overall	O
mIoU	Metric
accuracy	Metric
.	O
Compared	O
with	O
the	O
ResNet	Method
-	Method
101	Method
which	O
has	O
a	O
larger	O
model	O
size	O
and	O
higher	O
computational	Metric
cost	Metric
,	O
the	O
proposed	O
DPN	Method
-	Method
92	Method
further	O
improves	O
the	O
IoU	Metric
for	O
most	O
categories	O
and	O
improves	O
the	O
overall	O
mIoU	Metric
by	O
an	O
absolute	O
value	O
.	O
Considering	O
the	O
ResNeXt	O
-	O
101	O
(	O
d	O
)	O
only	O
improves	O
the	O
overall	O
mIoU	Metric
by	O
an	O
absolute	O
value	O
compared	O
with	O
the	O
ResNet	Method
-	Method
101	Method
,	O
the	O
proposed	O
DPN	Method
-	Method
92	Method
gains	O
more	O
than	O
times	O
improvement	O
compared	O
with	O
the	O
ResNeXt	Method
-	Method
101	Method
(	O
d	O
)	O
.	O
The	O
better	O
results	O
once	O
again	O
demonstrate	O
the	O
proposed	O
Dual	Method
Path	Method
Network	Method
is	O
capable	O
of	O
learning	O
better	O
feature	Method
representation	Method
for	O
dense	Task
prediction	Task
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
revisited	O
the	O
densely	Task
connected	Task
networks	Task
,	O
bridged	O
the	O
densely	Method
connected	Method
networks	Method
with	O
Higher	Method
Order	Method
RNNs	Method
and	O
proved	O
the	O
residual	Method
networks	Method
are	O
essentially	O
densely	Method
connected	Method
networks	Method
with	O
shared	O
connections	O
.	O
Based	O
on	O
this	O
new	O
explanation	O
,	O
we	O
proposed	O
a	O
dual	Method
path	Method
architecture	Method
that	O
enjoys	O
benefits	O
from	O
both	O
sides	O
.	O
The	O
novel	O
network	O
,	O
DPN	Method
,	O
is	O
then	O
developed	O
based	O
on	O
this	O
dual	Method
path	Method
architecture	Method
.	O
Experiments	O
on	O
the	O
image	Task
classification	Task
task	O
demonstrate	O
that	O
the	O
DPN	Method
enjoys	O
high	O
accuracy	Metric
,	O
small	O
model	Metric
size	Metric
,	O
low	Metric
computational	Metric
cost	Metric
and	O
low	Metric
GPU	Metric
memory	Metric
consumption	Metric
,	O
thus	O
is	O
extremely	O
useful	O
for	O
not	O
only	O
research	O
but	O
also	O
real	Task
-	Task
word	Task
application	Task
.	O
Experiments	O
on	O
the	O
object	Task
detection	Task
task	Task
and	O
semantic	Task
segmentation	Task
tasks	Task
show	O
that	O
the	O
proposed	O
DPN	Method
can	O
also	O
benefit	O
other	O
tasks	O
by	O
simply	O
replacing	O
the	O
base	Method
network	Method
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Testing	O
with	O
Mean	Method
-	Method
Max	Method
Pooling	Method
Here	O
,	O
we	O
introduce	O
a	O
new	O
testing	Method
technique	Method
by	O
using	O
Mean	Method
-	Method
Max	Method
Pooling	Method
which	O
can	O
further	O
improve	O
the	O
performance	O
of	O
a	O
well	O
trained	Method
CNN	Method
in	O
the	O
testing	O
phase	O
without	O
any	O
noticeable	O
computational	Metric
overhead	Metric
.	O
This	O
testing	O
technique	O
is	O
very	O
effective	O
for	O
testing	O
images	O
with	O
size	O
larger	O
than	O
training	O
crops	O
.	O
The	O
idea	O
is	O
to	O
first	O
convert	O
a	O
trained	O
CNN	Method
model	Method
into	O
a	O
convolutional	Method
network	Method
and	O
then	O
insert	O
the	O
following	O
Mean	Method
-	Method
Max	Method
Pooling	Method
layer	Method
(	O
a.k.a	O
.	O
Max	Method
-	Method
Avg	Method
Pooling	Method
)	O
,	O
i.e.	O
0.5	O
*	O
(	O
global	Method
average	Method
pooling	Method
+	O
global	Method
max	Method
pooling	Method
)	O
,	O
just	O
before	O
the	O
final	O
softmax	Method
layer	Method
.	O
Comparisons	O
between	O
the	O
models	O
with	O
and	O
without	O
Mean	Method
-	Method
Max	Method
Pooling	Method
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
As	O
can	O
be	O
seen	O
from	O
the	O
results	O
,	O
the	O
simple	O
Mean	Method
-	Method
Max	Method
Pooling	Method
testing	Method
strategy	Method
successfully	O
improves	O
the	O
testing	O
accuracy	Metric
for	O
all	O
models	O
.	O
