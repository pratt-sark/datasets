Variational Method
Autoencoders Method
for O
Collaborative Task
Filtering Task
section O
: O
ABSTRACT O
We O
extend O
variational Method
autoencoders Method
( O
vaes Method
) O
to O
collaborative Task
filtering Task
for O
implicit Task
feedback Task
. O
This O
non Method
- Method
linear Method
probabilistic Method
model Method
enables O
us O
to O
go O
beyond O
the O
limited Method
modeling Method
capacity Method
of O
linear Method
factor Method
models Method
which O
still O
largely O
dominate O
collaborative Task
filtering Task
research Task
. O
We O
introduce O
a O
generative Method
model Method
with O
multinomial Method
likelihood Method
and O
use O
Bayesian Method
inference Method
for O
parameter Task
estimation Task
. O
Despite O
widespread O
use O
in O
language Task
modeling Task
and O
economics Task
, O
the O
multinomial Method
likelihood Method
receives O
less O
attention O
in O
the O
recommender Task
systems Task
literature Task
. O
We O
introduce O
a O
different O
regularization O
parameter O
for O
the O
learning Metric
objective Metric
, O
which O
proves O
to O
be O
crucial O
for O
achieving O
competitive O
performance O
. O
Remarkably O
, O
there O
is O
an O
efficient O
way O
to O
tune O
the O
parameter O
using O
annealing Method
. O
The O
resulting O
model O
and O
learning Method
algorithm Method
has O
information O
- O
theoretic O
connections O
to O
maximum Method
entropy Method
discrimination Method
and O
the O
information Method
bottleneck Method
principle Method
. O
Empirically O
, O
we O
show O
that O
the O
proposed O
approach O
significantly O
outperforms O
several O
state O
- O
of O
- O
the O
- O
art O
baselines O
, O
including O
two O
recently O
- O
proposed O
neural Method
network Method
approaches Method
, O
on O
several O
real O
- O
world O
datasets O
. O
We O
also O
provide O
extended O
experiments O
comparing O
the O
multinomial Method
likelihood Method
with O
other O
commonly O
used O
likelihood Method
functions Method
in O
the O
latent Task
factor Task
collaborative Task
filtering Task
literature O
and O
show O
favorable O
results O
. O
Finally O
, O
we O
identify O
the O
pros O
and O
cons O
of O
employing O
a O
principled Method
Bayesian Method
inference Method
approach Method
and O
characterize O
settings O
where O
it O
provides O
the O
most O
significant O
improvements O
. O
section O
: O
INTRODUCTION O
Recommender Method
systems Method
are O
an O
integral O
component O
of O
the O
web O
. O
In O
a O
typical O
recommendation Task
system Task
, O
we O
observe O
how O
a O
set O
of O
users O
interacts O
with O
a O
set O
of O
items O
. O
Using O
this O
data O
, O
we O
seek O
to O
show O
users O
a O
set O
of O
previously O
unseen O
items O
they O
will O
like O
. O
As O
the O
web O
grows O
in O
size O
, O
good O
recommendation Method
systems Method
will O
play O
an O
important O
part O
in O
helping O
users O
interact O
more O
effectively O
with O
larger O
amounts O
of O
content O
. O
Collaborative Task
filtering Task
is O
among O
the O
most O
widely O
applied O
approaches O
in O
recommender Task
systems Task
. O
Collaborative Task
filtering Task
predicts O
what O
items O
a O
user O
will O
prefer O
by O
discovering O
and O
exploiting O
the O
similarity O
patterns O
across O
users O
and O
items O
. O
Latent Method
factor Method
models Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
still O
largely O
dominate O
the O
collaborative Task
filtering Task
research O
literature O
due O
to O
their O
simplicity O
and O
effectiveness O
. O
However O
, O
these O
models O
are O
inherently O
linear O
, O
which O
limits O
their O
modeling O
capacity O
. O
Previous O
work O
[ O
reference O
] O
has O
demonstrated O
that O
adding O
carefully O
crafted O
non O
- O
linear O
features O
into O
the O
linear Method
latent Method
factor Method
models Method
can O
significantly O
boost O
recommendation Task
performance O
. O
Recently O
, O
a O
growing O
body O
of O
work O
involves O
applying O
neural Method
networks Method
to O
the O
collaborative Task
filtering Task
setting Task
with O
promising O
results O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Here O
, O
we O
extend O
variational Method
autoencoders Method
( O
vaes Method
) O
[ O
reference O
][ O
reference O
] O
to O
collaborative Task
filtering Task
for O
implicit Task
feedback Task
. O
Vaes Method
generalize O
linear Method
latent Method
- Method
factor Method
models Method
and O
enable O
us O
to O
explore O
non Method
- Method
linear Method
probabilistic Method
latent Method
- Method
variable Method
models Method
, O
powered O
by O
neural Method
networks Method
, O
on O
large Task
- Task
scale Task
recommendation Task
datasets Task
. O
We O
propose O
a O
neural Method
generative Method
model Method
with O
multinomial Method
conditional Method
likelihood Method
. O
Despite O
being O
widely O
used O
in O
language Task
modeling Task
and O
economics Task
[ O
reference O
][ O
reference O
] O
, O
multinomial O
likelihoods O
appear O
less O
studied O
in O
the O
collaborative Task
filtering Task
literature Task
, O
particularly O
within O
the O
context O
of O
latent Method
- Method
factor Method
models Method
. O
Recommender Task
systems Task
are O
often O
evaluated O
using O
rankingbased Metric
measures Metric
, O
such O
as O
mean Metric
average Metric
precision Metric
and O
normalized Metric
discounted Metric
cumulative Metric
gain Metric
[ O
reference O
] O
. O
Top Metric
- Metric
N Metric
ranking Metric
loss Metric
is O
difficult O
to O
optimize O
directly O
and O
previous O
work O
on O
direct Task
ranking Task
loss Task
minimization Task
resorts O
to O
relaxations O
and O
approximations O
[ O
reference O
][ O
reference O
] O
. O
Here O
, O
we O
show O
that O
the O
multinomial Method
likelihoods Method
are O
well O
- O
suited O
for O
modeling O
implicit O
feedback O
data O
, O
and O
are O
a O
closer O
proxy O
to O
the O
ranking Metric
loss Metric
relative O
to O
more O
popular O
likelihood Method
functions Method
such O
as O
Gaussian Method
and Method
logistic Method
. O
Though O
recommendation Task
is O
often O
considered O
a O
big Task
- Task
data Task
problem Task
( O
due O
to O
the O
huge O
numbers O
of O
users O
and O
items O
typically O
present O
in O
a O
recommender Method
system Method
) O
, O
we O
argue O
that O
, O
in O
contrast O
, O
it O
represents O
a O
uniquely O
challenging O
" O
small Task
- Task
data Task
" Task
problem Task
: O
most O
users O
only O
interact O
with O
a O
tiny O
proportion O
of O
the O
items O
and O
our O
goal O
is O
to O
collectively O
make O
informed O
inference O
about O
each O
user O
's O
preference O
. O
To O
make O
use O
of O
the O
sparse O
signals O
from O
users O
and O
avoid O
overfitting O
, O
we O
build O
a O
probabilistic Method
latent Method
- Method
variable Method
model Method
that O
shares O
statistical O
strength O
among O
users O
and O
items O
. O
Empirically O
, O
we O
show O
that O
employing O
a O
principled Method
Bayesian Method
approach Method
is O
more O
robust O
regardless O
of O
the O
scarcity O
of O
the O
data O
. O
Although O
vaes Method
have O
been O
extensively O
studied O
for O
image Task
modeling Task
and Task
generation Task
, O
there O
is O
surprisingly O
little O
work O
applying O
vaes Method
to O
recommender Task
systems Task
. O
We O
find O
that O
two O
adjustments O
are O
essential O
to O
getting O
state O
- O
of O
- O
the O
- O
art O
results O
with O
vaes Method
on O
this O
task O
: O
• O
First O
, O
we O
use O
a O
multinomial Method
likelihood Method
for O
the O
data Task
distribution Task
. O
We O
show O
that O
this O
simple O
choice O
realizes O
models O
that O
outperform O
the O
more O
commonly O
used O
Gaussian Method
and Method
logistic Method
likelihoods Method
. O
• O
Second O
, O
we O
reinterpret O
and O
adjust O
the O
standard O
vae Method
objective O
, O
which O
we O
argue O
is O
over O
- O
regularized O
. O
We O
draw O
connections O
between O
the O
learning Method
algorithm Method
resulting O
from O
our O
proposed O
regularization Method
and O
the O
information Method
- Method
bottleneck Method
principle Method
and O
maximum Method
- Method
entropy Method
discrimination Method
. O
The O
result O
is O
a O
recipe O
that O
makes O
vaes Method
practical O
solutions O
to O
this O
important O
problem O
. O
Empirically O
, O
our O
methods O
significantly O
outperform O
state O
- O
of O
- O
the O
- O
art O
baselines O
on O
several O
real O
- O
world O
datasets O
, O
including O
two O
recently O
proposed O
neural Method
- Method
network Method
approaches Method
. O
section O
: O
METHOD O
We O
use O
u O
∈ O
{ O
1 O
, O
. O
. O
. O
, O
U O
} O
to O
index O
users O
and O
i O
∈ O
{ O
1 O
, O
. O
. O
. O
, O
I O
} O
to O
index O
items O
. O
In O
this O
work O
, O
we O
consider O
learning Task
with O
implicit O
feedback O
[ O
reference O
][ O
reference O
] O
. O
The O
user O
- O
by O
- O
item O
interaction O
matrix O
is O
the O
click O
1 O
matrix O
X O
∈ O
N O
U O
×I O
. O
The O
lower O
case O
x O
u O
= O
[ O
x O
u1 O
, O
. O
. O
. O
, O
x O
uI O
] O
⊤ O
∈ O
N O
I O
is O
a O
bag O
- O
ofwords O
vector O
with O
the O
number O
of O
clicks O
for O
each O
item O
from O
user O
u. O
For O
simplicity O
, O
we O
binarize O
the O
click O
matrix O
. O
It O
is O
straightforward O
to O
extend O
it O
to O
general O
count O
data O
. O
section O
: O
Model O
The O
generative Method
process Method
we O
consider O
in O
this O
paper O
is O
similar O
to O
the O
deep Method
latent Method
Gaussian Method
model Method
[ O
reference O
] O
. O
For O
each O
user O
u O
, O
the O
model O
starts O
by O
sampling O
a O
K Method
- Method
dimensional Method
latent Method
representation Method
z Method
u Method
from O
a O
standard O
Gaussian Method
prior Method
. O
The O
latent Method
representation Method
z O
u O
is O
transformed O
via O
a O
non Method
- Method
linear Method
function Method
f O
θ O
( O
· O
) O
∈ O
R O
I O
to O
produce O
a O
probability O
distribution O
over O
I O
items O
π O
( O
z O
u O
) O
from O
which O
the O
click O
history O
x O
u O
is O
assumed O
to O
have O
been O
drawn O
: O
The O
non Method
- Method
linear Method
function Method
f O
θ O
( O
· O
) O
is O
a O
multilayer Method
perceptron Method
with O
parameters O
θ O
. O
The O
output O
of O
this O
transformation O
is O
normalized O
via O
a O
softmax Method
function Method
to O
produce O
a O
probability O
vector O
π O
( O
z O
u O
) O
∈ O
S O
I O
−1 O
( O
an O
( O
I O
− O
1 O
)- O
simplex O
) O
over O
the O
entire O
item O
set O
. O
Given O
the O
total O
number O
of O
clicks O
N O
u O
= O
i O
x O
ui O
from O
user O
u O
, O
the O
observed O
bag O
- O
of O
- O
words O
vector O
x O
u O
is O
assumed O
to O
be O
sampled O
from O
a O
multinomial Method
distribution Method
with O
probability O
π O
( O
z O
u O
) O
. O
This O
generative Method
model Method
generalizes O
the O
latentfactor Method
model Method
- O
we O
can O
recover O
classical O
matrix Method
factorization Method
[ O
reference O
] O
by O
setting O
f Method
θ Method
( O
· O
) O
to O
be O
linear O
and O
using O
a O
Gaussian Method
likelihood Method
. O
The O
log O
- O
likelihood O
for O
user O
u O
( O
conditioned O
on O
the O
latent Method
representation Method
) O
is O
: O
This O
multinomial O
likelihood O
is O
commonly O
used O
in O
language Method
models Method
, O
e.g. O
, O
latent Task
Dirichlet Task
allocation Task
[ O
reference O
] O
, O
and O
economics O
, O
e.g. O
, O
multinomial Method
logit Method
choice Method
model Method
[ O
reference O
] O
. O
It O
is O
also O
used O
in O
the O
cross Metric
- Metric
entropy Metric
loss Metric
2 O
for O
multi Task
- Task
class Task
classification Task
. O
For O
example O
, O
it O
has O
been O
used O
in O
recurrent Method
neural Method
networks Method
for O
session Task
- Task
based Task
sequential Task
recommendation Task
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
and O
in O
feedward Method
neural Method
networks Method
applied O
to O
Youtube Task
recommendation Task
[ O
reference O
] O
. O
The O
multinomial O
likelihood O
is O
less O
well O
studied O
in O
the O
context O
of O
latent Method
- Method
factor Method
models Method
such O
as O
matrix Method
factorization Method
and O
autoencoders Method
. O
A O
notable O
exception O
is O
the O
collaborative Method
competitive Method
filtering Method
( Method
CCF Method
) Method
model Method
[ O
reference O
] O
and O
its O
successors O
, O
which O
take O
advantage O
of O
more O
fine O
- O
grained O
information O
about O
what O
options O
were O
presented O
to O
which O
users O
. O
( O
If O
such O
information O
is O
available O
, O
it O
can O
also O
be O
incorporated O
into O
our O
vae Method
- O
based O
approach O
. O
) O
We O
believe O
the O
multinomial Method
distribution Method
is O
well O
suited O
to O
modeling O
click O
data O
. O
The O
likelihood O
of O
the O
click O
matrix O
( O
Eq O
. O
2 O
) O
rewards O
the O
model O
for O
putting O
probability O
mass O
on O
the O
non O
- O
zero O
entries O
in O
x O
u O
. O
But O
the O
model O
has O
a O
limited O
budget O
of O
probability O
mass O
, O
since O
π O
( O
z O
u O
) O
must O
sum O
to O
1 O
; O
the O
items O
must O
compete O
for O
this O
limited O
budget O
[ O
reference O
] O
. O
The O
model O
should O
therefore O
assign O
more O
probability O
mass O
to O
items O
that O
are O
more O
likely O
to O
be O
clicked O
. O
To O
the O
extent O
that O
it O
can O
, O
it O
will O
perform O
well O
under O
the O
top Metric
- Metric
N Metric
ranking Metric
loss Metric
that O
recommender Method
systems Method
are O
commonly O
evaluated O
on O
. O
By O
way O
of O
comparison O
, O
we O
present O
two O
popular O
choices O
of O
likelihood Method
functions Method
used O
in O
latent Method
- Method
factor Method
collaborative Method
filtering Method
: O
Gaussian Method
and Method
logistic Method
likelihoods Method
. O
Define O
f O
θ O
( O
z O
u O
) O
≡ O
[ O
f O
u1 O
, O
. O
. O
. O
, O
f O
uI O
] O
⊤ O
as O
the O
output O
of O
the O
generative Method
function Method
f Method
θ Method
( O
· O
) O
. O
The O
Gaussian Method
log Method
- Method
likelihood Method
for O
user O
u O
is O
We O
adopt O
the O
convention O
in O
Hu O
et O
al O
. O
[ O
reference O
] O
and O
introduce O
a O
" O
confidence O
" O
weight O
c O
x O
ui O
≡ O
c O
ui O
where O
c O
1 O
> O
c O
0 O
to O
balance O
the O
unobserved O
0 O
's O
which O
far O
outnumber O
the O
observed O
1 O
's O
in O
most O
click O
data O
. O
This O
is O
also O
equivalent O
to O
training O
the O
model O
with O
unweighted Method
Gaussian Method
likelihood Method
and O
negative Method
sampling Method
. O
The O
logistic O
log O
- O
likelihood O
3 O
for O
user O
u O
is O
where O
σ O
( O
x O
) O
= O
1 O
/( O
1 O
+ O
exp O
( O
−x O
) O
) O
is O
the O
logistic O
function O
. O
We O
compare O
multinomial Method
likelihood Method
with O
Gaussian Method
and O
logistic Method
in O
Section O
4 O
. O
section O
: O
Variational Task
inference Task
To O
learn O
the O
generative Method
model Method
in O
Eq O
. O
1 O
, O
we O
are O
interested O
in O
estimating O
θ O
( O
the O
parameters O
of O
f O
θ O
( O
· O
) O
) O
. O
To O
do O
so O
, O
for O
each O
data O
point O
we O
need O
to O
approximate O
the O
intractable Method
posterior Method
distribution Method
p Method
( Method
z Method
u O
| O
x O
u O
) O
. O
We O
resort O
to O
variational Task
inference Task
[ O
reference O
] O
. O
Variational Method
inference Method
approximates O
the O
true O
intractable O
posterior O
with O
a O
simpler O
variational Method
distribution Method
q O
( O
z O
u O
) O
. O
We O
set O
q O
( O
z O
u O
) O
to O
be O
a O
fully O
factorized Method
( Method
diagonal Method
) Method
Gaussian Method
distribution Method
: O
The O
objective O
of O
variational Task
inference Task
is O
to O
optimize O
the O
free O
variational O
parameters O
{ O
µ O
u O
, O
σ O
2 O
u O
} O
so O
that O
the O
Kullback O
- O
Leiber O
divergence O
KL O
( O
q O
( O
z O
u O
) O
∥p O
( O
z O
u O
|x O
u O
) O
) O
is O
minimized O
. O
section O
: O
Amortized Method
inference Method
and O
the O
variational Method
autoencoder Method
: O
With O
variational Method
inference Method
the O
number O
of O
parameters O
to O
optimize O
{ O
µ O
u O
, O
σ O
2 O
u O
} O
grows O
with O
the O
number O
of O
users O
and O
items O
in O
the O
dataset O
. O
This O
can O
become O
a O
bottleneck O
for O
commercial Task
recommender Task
systems Task
with O
millions O
of O
users O
and O
items O
. O
The O
variational O
autoencoder O
( O
vae Method
) O
[ O
reference O
][ O
reference O
] O
replaces O
individual O
variational O
parameters O
with O
a O
data O
- O
dependent O
function O
( O
commonly O
called O
an O
inference Method
model Method
) O
: O
parametrized O
by O
ϕ O
with O
both O
µ O
ϕ O
( O
x O
u O
) O
and O
σ O
ϕ O
( O
x O
u O
) O
being O
K O
- O
vectors O
and O
sets O
the O
variational O
distribution O
as O
follows O
: O
That O
is O
, O
using O
the O
observed O
data O
x O
u O
as O
input O
, O
the O
inference Method
model Method
outputs O
the O
corresponding O
variational O
parameters O
of O
variational Method
distribution Method
q O
ϕ O
( O
z O
u O
| O
x O
u O
) O
, O
which O
, O
when O
optimized O
, O
approximates O
the O
intractable O
posterior O
p O
( O
z O
u O
| O
x O
u O
) O
. O
[ O
reference O
] O
Putting O
q O
ϕ O
( O
z O
u O
| O
x O
u O
) O
and O
the O
generative Method
model Method
p O
θ O
( O
x O
u O
| O
z O
u O
) O
together O
in O
Figure O
2c O
, O
we O
end O
up O
with O
a O
neural Method
architecture Method
that O
resembles O
an O
autoencoder Method
- O
hence O
the O
name O
variational Method
autoencoder Method
. O
Vaes Method
make O
use O
of O
amortized Method
inference Method
[ O
reference O
] O
: O
they O
flexibly O
reuse O
inferences O
to O
answer O
related O
new O
problems O
. O
This O
is O
well O
aligned O
with O
the O
ethos O
of O
collaborative Task
filtering Task
: O
analyze O
user O
preferences O
by O
exploiting O
the O
similarity O
patterns O
inferred O
from O
past O
experiences O
. O
In O
Section O
2.4 O
, O
we O
discuss O
how O
this O
enables O
us O
to O
perform O
prediction Task
efficiently O
. O
Learning O
vaes Method
: O
As O
is O
standard O
when O
learning O
latent Method
- Method
variable Method
models Method
with O
variational Method
inference Method
[ O
reference O
] O
, O
we O
can O
lower O
- O
bound O
the O
log O
marginal O
likelihood O
of O
the O
data O
. O
This O
forms O
the O
objective O
we O
seek O
to O
maximize O
for O
user O
u O
( O
the O
objective O
function O
of O
the O
dataset O
is O
obtained O
by O
averaging O
the O
objective O
function O
over O
all O
the O
users O
) O
: O
This O
is O
commonly O
known O
as O
the O
evidence Metric
lower Metric
bound Metric
( O
elbo Method
) O
. O
Note O
that O
the O
elbo Method
is O
a O
function O
of O
both O
θ O
and O
ϕ. O
We O
can O
obtain O
an O
unbiased O
estimate O
of O
elbo Method
by O
sampling O
z Method
u Method
∼ Method
q Method
ϕ Method
and O
perform O
stochastic Method
gradient Method
ascent Method
to O
optimize O
it O
. O
However O
, O
the O
challenge O
is O
that O
we O
can O
not O
trivially O
take O
gradients O
with O
respect O
to O
ϕ O
through O
this O
sampling Method
process Method
. O
The O
reparametrization Method
trick Method
[ O
reference O
][ O
reference O
] O
sidesteps O
this O
issue O
: O
we O
sample O
ϵ O
∼ O
N O
( O
0 O
, O
I O
K O
) O
and O
reparametrize O
By O
doing O
so O
, O
the O
stochasticity O
in O
the O
sampling Method
process Method
is O
isolated O
and O
the O
gradient O
with O
respect O
to O
ϕ O
can O
be O
back O
- O
propagated O
through O
the O
sampled O
z O
u O
. O
The O
vae Method
training O
procedure O
is O
summarized O
in O
Algorithm O
1 O
. O
section O
: O
Alternative O
interpretation O
of O
elbo Method
. O
We O
can O
view O
elbo Method
defined O
in O
Eq O
. O
5 O
from O
a O
different O
perspective O
: O
the O
first O
term O
can O
be O
interpreted O
as O
( O
negative O
) O
reconstruction O
error O
, O
while O
the O
second O
KL O
term O
can O
be O
viewed O
as O
regularization O
. O
It O
is O
this O
perspective O
we O
work O
with O
because O
it O
allows O
us O
to O
make O
a O
trade O
- O
off O
that O
forms O
the O
crux O
of O
our O
method O
. O
From O
this O
perspective O
, O
it O
is O
natural O
to O
extend O
the O
elbo Method
by O
introducing O
a O
parameter O
β O
to O
control O
the O
strength O
of O
regularization O
: O
While O
the O
original O
vae Method
( O
trained O
with O
elbo Method
in O
Eq O
. O
5 O
) O
is O
a O
powerful O
generative Method
model Method
; O
we O
might O
ask O
whether O
we O
need O
all O
the O
statistical O
properties O
of O
a O
generative Method
model Method
for O
tackling O
problems O
in O
recommender Task
systems Task
. O
In O
particular O
, O
if O
we O
are O
willing O
to O
sacrifice O
the O
ability O
to O
perform O
ancestral Method
sampling Method
, O
can O
we O
improve O
our O
performance O
? O
The O
regularization Method
view Method
of O
the O
elbo Method
( O
Eq O
. O
6 O
) O
introduces O
a O
trade O
- O
off O
between O
how O
well O
we O
can O
fit O
the O
data O
and O
how O
close O
the O
approximate O
posterior O
stays O
to O
the O
prior O
during O
learning Task
. O
We O
propose O
using O
β O
1 O
. O
This O
means O
we O
are O
no O
longer O
optimizing O
a O
lower O
bound O
on O
the O
log O
marginal O
likelihood O
. O
If O
β O
< O
1 O
, O
then O
we O
are O
also O
weakening O
the O
influence O
of O
the O
prior O
constraint O
Figure O
1 O
illustrates O
the O
basic O
idea O
( O
we O
observe O
the O
same O
trend O
consistently O
across O
datasets O
) O
. O
Here O
we O
plot O
the O
validation Metric
ranking Metric
metric Metric
without O
KL Method
annealing Method
( O
blue O
solid O
) O
and O
with O
KL Method
annealing Method
all O
the O
way O
to O
β O
= O
1 O
( O
green O
dashed O
, O
β O
reaches O
1 O
at O
around O
80 O
epochs O
) O
. O
As O
we O
can O
see O
, O
the O
performance O
is O
poor O
without O
any O
KL Method
annealing Method
. O
With O
annealing Method
, O
the O
validation Metric
performance O
first O
increases O
as O
the O
training O
proceeds O
and O
then O
drops O
as O
β O
gets O
close O
to O
1 O
to O
a O
value O
that O
is O
only O
slightly O
better O
than O
doing O
no O
annealing Method
at O
all O
. O
Having O
identified O
the O
best O
β O
based O
on O
the O
peak Metric
validation Metric
metric Metric
, O
we O
can O
retrain O
the O
model O
with O
the O
same O
annealing O
schedule O
, O
but O
stop O
increasing O
β O
after O
reaching O
that O
value O
( O
shown O
as O
red O
dot O
- O
dashed O
in O
Figure O
1 O
) O
. O
[ O
reference O
] O
This O
might O
be O
sub O
- O
optimal O
compared O
to O
a O
thorough O
grid Method
search Method
. O
However O
, O
it O
is O
much O
more O
efficient O
, O
and O
gives O
us O
competitive O
empirical O
performance O
. O
If O
the O
computational Metric
budget Metric
is O
scarce O
, O
then O
within O
a O
single O
run O
, O
we O
can O
stop O
increasing O
β O
when O
we O
notice O
the O
validation Metric
metric Metric
dropping O
. O
Such O
a O
procedure O
incurs O
no O
additional O
runtime Metric
to O
learning O
a O
standard O
vae Method
. O
We O
denote O
this O
partially O
regularized O
vae Method
with O
multinomial Method
likelihood Method
as O
Mult Method
- Method
vae Method
pr Method
. O
section O
: O
Computational Metric
Burden Metric
. O
Previous O
collaborative Method
filtering Method
models Method
with O
neural Method
networks Method
[ O
reference O
][ O
reference O
] O
are O
trained O
with O
stochastic Method
gradient Method
descent Method
where O
in O
each O
step O
a O
single O
( O
user O
, O
item O
) O
entry O
from O
the O
click O
matrix O
is O
randomly O
sampled O
to O
perform O
a O
gradient Method
update Method
. O
In O
Algorithm O
1 O
we O
subsample O
users O
and O
take O
their O
entire O
click O
history O
( O
complete O
rows O
of O
the O
click O
matrix O
) O
to O
update O
model Method
parameters Method
. O
This O
eliminates O
the O
necessity O
of O
negative Method
sampling Method
( O
and O
consequently O
the O
hyperparameter Method
tuning Method
for O
picking O
the O
number O
of O
negative O
examples O
) O
, O
commonly O
used O
in O
the O
( O
user Method
, Method
item Method
) Method
entry Method
subsampling Method
scheme Method
. O
A O
computational Task
challenge Task
that O
comes O
with O
our O
approach O
, O
however O
, O
is O
that O
when O
the O
number O
of O
items O
is O
huge O
, O
computing O
the O
multinomial O
probability O
π O
( O
z O
u O
) O
could O
be O
computationally O
expensive O
, O
since O
it O
requires O
computing O
the O
predictions O
for O
all O
the O
items O
for O
normalization Task
. O
This O
is O
a O
common O
challenge O
for O
language Task
modeling Task
where O
the O
size O
of O
the O
vocabulary O
is O
in O
the O
order O
of O
millions O
or O
more O
[ O
reference O
] O
. O
In O
our O
experiments O
on O
some O
medium O
- O
to O
- O
large O
datasets O
with O
less O
than O
50 O
K O
items O
( O
Section O
4.1 O
) O
, O
this O
has O
not O
yet O
come O
up O
as O
a O
computational O
bottleneck O
. O
If O
this O
becomes O
a O
bottleneck O
when O
working O
with O
larger O
item O
sets O
, O
one O
can O
easily O
apply O
the O
simple O
and O
[ O
reference O
] O
We O
found O
this O
to O
give O
slightly O
better O
results O
than O
keeping O
β O
at O
the O
best O
value O
throughout O
the O
training O
. O
effective O
method O
proposed O
by O
Botev O
et O
al O
. O
[ O
reference O
] O
to O
approximate O
the O
normalization O
factor O
for O
π O
( O
z O
u O
) O
. O
section O
: O
A O
taxonomy O
of O
autoencoders Method
In O
Section O
2.2 O
, O
we O
introduced O
maximum O
marginal O
likelihood O
estimation O
of O
vaes Method
using O
approximate Method
Bayesian Method
inference Method
under O
a O
non Method
- Method
linear Method
generative Method
model Method
( O
Eq O
. O
1 O
) O
. O
We O
now O
describe O
our O
work O
from O
the O
perspective O
of O
learning Task
autoencoders Task
. O
Maximum Task
- Task
likelihood Task
estimation Task
in O
a O
regular Method
autoencoder Method
takes O
the O
following O
form O
: O
There O
are O
two O
key O
distinctions O
of O
note O
: O
( O
1 O
) O
The O
autoencoder Method
( O
and O
denoising Method
autoencoder Method
) O
effectively O
optimizes O
the O
first O
term O
in O
the O
vae Method
objective O
( O
Eq O
. O
5 O
and O
Eq O
. O
6 O
) O
using O
a O
delta Method
variational Method
distribution Method
is O
a O
δ Method
distribution Method
with O
mass O
only O
at O
the O
output O
of O
д O
ϕ O
( O
x O
u O
) O
. O
Contrast O
this O
to O
the O
vae Method
, O
where O
the O
learning Task
is O
done O
using O
a O
variational Method
distribution Method
, O
i.e. O
, O
д O
ϕ O
( O
x O
u O
) O
outputs O
the O
parameters O
( O
mean O
and O
variance O
) O
of O
a O
Gaussian Method
distribution Method
. O
This O
means O
that O
vae Method
has O
the O
ability O
to O
capture O
per O
- O
data O
- O
point O
variances O
in O
the O
latent O
state O
z O
u O
. O
In O
practice O
, O
we O
find O
that O
learning O
autoencoders Method
is O
extremely O
prone O
to O
overfitting O
as O
the O
network O
learns O
to O
put O
all O
the O
probability O
mass O
to O
the O
non O
- O
zero O
entries O
in O
x O
u O
. O
By O
introducing O
dropout Method
[ O
reference O
] O
at O
the O
input O
layer O
, O
the O
denoising Method
autoencoder Method
( O
dae O
) O
is O
less O
prone O
to O
overfitting O
and O
we O
find O
that O
it O
also O
gives O
competitive O
empirical O
results O
. O
In O
addition O
to O
the O
Mult Method
- Method
vae Method
pr Method
, O
we O
also O
study O
a O
denoising Method
autoencoder Method
with O
a O
multinomial O
likelihood O
. O
We O
denote O
this O
model O
Mult Method
- Method
dae Method
. O
In O
Section O
4 O
we O
characterize O
the O
tradeoffs O
in O
what O
is O
gained O
and O
lost O
by O
explicitly O
parameterizing O
the O
per O
- O
user O
variance O
with O
Mult Method
- Method
vae Method
pr Method
versus O
using O
a O
point Method
- Method
estimation Method
in O
Mult Method
- Method
dae Method
. O
To O
provide O
a O
unified O
view O
of O
different O
variants O
of O
autoencoders Method
and O
clarify O
where O
our O
work O
stands O
, O
we O
depict O
variants O
of O
autoencoders Method
commonly O
found O
in O
the O
literature O
in O
Figure O
2 O
. O
For O
each O
one O
, O
we O
specify O
the O
model O
( O
dotted O
arrows O
denote O
a O
sampling O
operation O
) O
and O
describe O
the O
training Metric
objective Metric
used O
in O
parameter Task
estimation Task
. O
In O
Figure O
2a O
we O
have O
autoencoder Method
. O
It O
is O
trained O
to O
reconstruct O
input O
with O
the O
same O
objective O
as O
in O
Eq O
. O
7 O
. O
Adding O
noise O
to O
the O
input O
( O
or O
the O
intermediate O
hidden O
representation O
) O
of O
an O
autoencoder Method
yields O
the O
denoising Method
autoencoder Method
in O
Figure O
2b O
. O
The O
training Metric
objective Metric
is O
the O
same O
as O
that O
of O
an O
autoencoder Method
. O
Mult Method
- Method
dae Method
belongs O
to O
this O
model O
class O
. O
Collaborative O
denoising Method
autoencoder Method
[ O
reference O
] O
is O
a O
variant O
of O
this O
model O
class O
. O
The O
vae Method
is O
depicted O
in O
Figure O
2c O
. O
Rather O
than O
using O
a O
delta Method
variational Method
distribution Method
, O
it O
uses O
an O
inference Method
model Method
parametrized O
by O
ϕ Method
to O
produce O
the O
mean Metric
and Metric
variance Metric
of O
the O
approximating Method
variational Method
distribution Method
. O
The O
training Metric
objective Metric
of O
the O
vae Method
is O
given O
in O
Eq O
. O
6 O
. O
Setting O
β O
to O
1 O
recovers O
the O
original O
vae Method
formulation O
[ O
reference O
][ O
reference O
] O
. O
Higgins O
et O
al O
. O
[ O
reference O
] O
study O
the O
case O
where O
β O
> O
1 O
. O
Our O
model O
, O
Mult Method
- Method
vae Method
pr Method
corresponds O
to O
learning O
vaes Method
with O
β O
∈ O
[ O
0 O
, O
1 O
] O
. O
section O
: O
Prediction Task
We O
now O
describe O
how O
we O
make O
predictions O
given O
a O
trained O
generative Method
model Method
of O
the O
form O
Eq O
. O
1 O
. O
For O
both O
, O
Mult Method
- Method
vae Method
pr Method
( O
Section O
2.2 O
) O
or O
Mult Method
- Method
dae Method
( O
Section O
2.3 O
) O
, O
we O
make O
predictions O
in O
the O
same O
way O
. O
Given O
a O
user O
's O
click O
history O
x O
, O
we O
rank O
all O
the O
items O
based O
on O
the O
un O
- O
normalized O
predicted O
multinomial O
probability O
f O
θ O
( O
z O
) O
. O
The O
latent Method
representation Method
z Method
for O
x O
is O
constructed O
as O
follows O
: O
For O
Mult Method
- Method
vae Method
pr Method
, O
we O
simply O
take O
the O
mean O
of O
the O
variational Method
distribution Method
z O
= O
µ O
ϕ O
( O
x O
) O
; O
for O
Mult Method
- Method
dae Method
, O
we O
take O
the O
output O
z O
= O
д O
ϕ O
( O
x O
) O
. O
It O
is O
easy O
to O
see O
the O
advantage O
of O
using O
autoencoders Method
. O
We O
can O
effectively O
make O
predictions O
for O
users O
by O
evaluating O
two O
functions O
- O
the O
inference Method
model Method
( O
encoder Method
) O
д O
ϕ O
( O
· O
) O
and O
the O
generative Method
model Method
( O
decoder Method
) O
f O
θ O
( O
· O
) O
. O
For O
most O
of O
the O
latent Method
factor Method
collaborative Method
filtering Method
model Method
, O
e.g. O
, O
matrix Method
factorization Method
[ O
reference O
][ O
reference O
] O
, O
when O
given O
the O
click O
history O
of O
a O
user O
that O
is O
not O
present O
in O
the O
training O
data O
, O
normally O
we O
need O
to O
perform O
some O
form O
of O
optimization Task
to O
obtain O
the O
latent O
factor O
for O
this O
user O
. O
This O
makes O
the O
use O
of O
autoencoders Method
particularly O
attractive O
in O
industrial Task
applications Task
, O
where O
it O
is O
important O
that O
predictions O
be O
made O
cheaply O
and O
with O
low O
latency O
. O
section O
: O
RELATED O
WORK O
Vaes Method
on O
sparse O
data O
. O
Variational Method
autoencoders Method
( O
vaes Method
) O
[ O
reference O
][ O
reference O
] O
have O
seen O
much O
application O
to O
images O
since O
their O
inception O
. O
Doersch O
[ O
reference O
] O
presents O
a O
review O
on O
different O
applications O
of O
vae Method
to O
image O
data O
. O
Miao O
et O
al O
. O
[ O
reference O
] O
study O
vaes Method
on O
text O
data O
. O
More O
recent O
results O
from O
Krishnan O
et O
al O
. O
[ O
reference O
] O
find O
that O
vaes Method
( O
trained O
with O
Eq O
. O
5 O
) O
suffer O
from O
underfitting O
when O
modeling O
large O
, O
sparse O
, O
high O
- O
dimensional O
data O
. O
We O
notice O
similar O
issues O
when O
fitting O
vae Method
without O
annealing Method
( O
Figure O
1 O
) O
or O
annealing O
to O
β O
= O
1 O
. O
By O
giving O
up O
the O
ability O
to O
perform O
ancestral Method
sampling Method
in O
the O
model O
, O
and O
setting O
β O
≤ O
1 O
, O
the O
resulting O
model O
is O
no O
longer O
a O
proper O
generative Method
model Method
though O
for O
collaborative Task
filtering Task
tasks Task
we O
always O
make O
predictions O
conditional O
on O
users O
' O
click O
history O
. O
Information O
- O
theoretic O
connection O
with O
vae Method
. O
The O
regularization Method
view Method
of O
the O
elbo Method
in O
Eq O
. O
6 O
resembles O
maximum Method
- Method
entropy Method
discrimination Method
[ O
reference O
] O
. O
Maximum Method
- Method
entropy Method
discrimination Method
attempts O
to O
combine O
discriminative Method
estimation Method
with O
Bayesian Method
inference Method
and O
generative Method
modeling Method
. O
In O
our O
case O
, O
in O
Eq O
. O
6 O
, O
β O
acts O
as O
a O
knob O
to O
balance O
discriminative O
and O
generative O
aspects O
of O
the O
model O
. O
The O
procedure O
in O
Eq O
. O
6 O
has O
information O
- O
theoretic O
connections O
described O
in O
Alemi O
et O
al O
. O
[ O
reference O
] O
. O
The O
authors O
propose O
the O
deep Method
variational Method
information Method
bottleneck Method
, O
which O
is O
a O
variational Method
approximation Method
to O
the O
information Method
bottleneck Method
principle Method
[ O
reference O
] O
. O
They O
show O
that O
as O
a O
special O
case O
they O
can O
recover O
the O
learning O
objective O
used O
by O
vaes Method
. O
They O
report O
more O
robust O
supervised Metric
classification Metric
performance O
with O
β O
< O
1 O
. O
This O
is O
consistent O
with O
our O
findings O
as O
well O
. O
Higgins O
et O
al O
. O
[ O
reference O
] O
proposed O
β O
- O
vae Method
, O
which O
leads O
to O
the O
same O
objective O
as O
Eq O
. O
6 O
. O
section O
: O
They O
motivate O
Neural Method
networks Method
for O
collaborative Task
filtering Task
. O
Early O
work O
on O
neural Method
- Method
network Method
- Method
based Method
collaborative Method
filtering Method
models Method
focus O
on O
explicit O
feedback O
data O
and O
evaluates O
on O
the O
task O
of O
rating Task
predictions Task
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
The O
importance O
of O
implicit O
feedback O
has O
been O
gradually O
recognized O
, O
and O
consequently O
most O
recent O
research O
, O
such O
as O
this O
work O
, O
has O
focused O
on O
it O
. O
The O
two O
papers O
that O
are O
most O
closely O
related O
to O
our O
approaches O
are O
collaborative O
denoising Method
autoencoder Method
[ O
reference O
] O
and O
neural Method
collaborative Method
filtering Method
[ O
reference O
] O
. O
Collaborative O
denoising Method
autoencoder Method
( O
cdae O
) O
[ O
reference O
] O
augments O
the O
standard O
denoising Method
autoencoder Method
, O
described O
in O
Section O
2.3 O
, O
by O
adding O
a O
per O
- O
user O
latent O
factor O
to O
the O
input O
. O
The O
number O
of O
parameters O
of O
the O
cdae Method
model Method
grows O
linearly O
with O
both O
the O
number O
of O
users O
as O
well O
as O
the O
number O
of O
items O
, O
making O
it O
more O
prone O
to O
overfitting O
. O
In O
contrast O
, O
the O
number O
of O
parameters O
in O
the O
vae Method
grows O
linearly O
with O
the O
number O
of O
items O
. O
The O
cdae Method
also O
requires O
additional O
optimization Task
to O
obtain O
the O
latent O
factor O
for O
unseen O
users O
to O
make O
predicion Task
. O
In O
the O
paper O
, O
the O
authors O
investigate O
the O
Gaussian Method
and Method
logistic Method
likelihood Method
loss Method
functions Method
- O
as O
we O
show O
, O
the O
multinomial Method
likelihood Method
is O
significantly O
more O
robust O
for O
use O
in O
recommender Task
systems Task
. O
Neural Method
collaborative Method
filtering Method
( O
ncf Method
) O
[ O
reference O
] O
explore O
a O
model O
with O
non O
- O
linear O
interactions O
between O
the O
user O
and O
item O
latent O
factors O
rather O
than O
the O
commonly O
used O
dot Method
product Method
. O
The O
authors O
demonstrate O
improvements O
of O
ncf Method
over O
standard O
baselines O
on O
two O
small O
datasets O
. O
Similar O
to O
cdae Method
, O
the O
number O
of O
parameters O
of O
ncf Method
also O
grows O
linearly O
with O
both O
the O
number O
of O
users O
as O
well O
as O
items O
. O
We O
find O
that O
this O
becomes O
problematic O
for O
much O
larger O
datasets O
. O
We O
compare O
with O
both O
cdae Method
and O
ncf Method
in O
Section O
4 O
. O
Asymmetric Method
matrix Method
factorization Method
[ O
reference O
] O
may O
also O
be O
interpreted O
as O
an O
autoencoder Method
, O
as O
elaborated O
in O
Steck O
[ O
reference O
] O
. O
We O
can O
recover O
this O
work O
by O
setting O
both O
f O
θ O
( O
· O
) O
and O
д O
ϕ O
( O
· O
) O
to O
be O
linear O
. O
Besides O
being O
applied O
in O
session Task
- Task
based Task
sequential Task
recommendation Task
( O
see O
Section O
2.1 O
) O
, O
various O
approaches O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
have O
applied O
neural Method
networks Method
to O
incorporate O
side O
information O
into O
collaborative Method
filtering Method
models Method
to O
better O
handle O
the O
cold Task
- Task
start Task
problem Task
. O
These O
approaches O
are O
complementary O
to O
ours O
. O
section O
: O
EMPIRICAL O
STUDY O
We O
evaluate O
the O
performance O
of O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
. O
We O
provide O
insights O
into O
their O
performance O
by O
exploring O
the O
resulting O
fits O
. O
We O
highlight O
the O
following O
results O
: O
• O
Mult Method
- Method
vae Method
pr Method
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
three O
realworld O
datasets O
when O
compared O
with O
various O
baselines O
, O
including O
recently O
proposed O
neural Method
- Method
network Method
- Method
based Method
collaborative Method
filtering Method
models Method
. O
• O
For O
the O
denoising Method
and Method
variational Method
autoencoder Method
, O
the O
multinomial Method
likelihood Method
compares O
favorably O
over O
the O
more O
common O
Gaussian Method
and Method
logistic Method
likelihoods Method
. O
• O
Both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
produce O
competitive O
empirical O
results O
. O
We O
identify O
when O
parameterizing O
the O
uncertainty O
explicitly O
as O
in O
Mult Method
- Method
vae Method
pr Method
does O
better O
/ O
worse O
than O
the O
point Method
estimate Method
used O
by O
Mult Method
- Method
dae Method
and O
list O
pros O
and O
cons O
for O
both O
approaches O
. O
The O
source O
code O
to O
reproduce O
the O
experimental O
results O
is O
available O
on O
GitHub O
6 O
. O
section O
: O
Datasets O
We O
study O
three O
medium O
- O
to O
large O
- O
scale O
user O
- O
item O
consumption O
datasets O
from O
various O
domains O
: O
MovieLens Material
- Material
20 Material
M Material
( O
ML Material
- Material
20 Material
M Material
) O
: O
These O
are O
user O
- O
movie O
ratings O
collected O
from O
a O
movie Task
recommendation Task
service Task
. O
We O
binarize O
the O
explicit O
data O
by O
keeping O
ratings O
of O
four O
or O
higher O
and O
interpret O
them O
as O
implicit O
feedback O
. O
We O
only O
keep O
users O
who O
have O
watched O
at O
least O
five O
movies O
. O
Netflix Material
Prize Material
( O
Netflix Material
) O
: O
This O
is O
the O
user O
- O
movie O
ratings O
data O
from O
the O
Netflix Material
Prize Material
7 O
. O
Similar O
to O
ML Material
- Material
20 Material
M Material
, O
we O
binarize O
explicit O
data O
by O
keeping O
ratings O
of O
four O
or O
higher O
. O
We O
only O
keep O
users O
who O
have O
watched O
at O
least O
five O
movies O
. O
Million Material
Song Material
Dataset Material
( O
MSD Material
) O
: O
This O
data O
contains O
the O
user O
- O
song O
play O
counts O
released O
as O
part O
of O
the O
Million Material
Song Material
Dataset Material
[ O
reference O
] O
. O
We O
binarize O
play O
counts O
and O
interpret O
them O
as O
implicit O
preference O
data O
. O
We O
only O
keep O
users O
with O
at O
least O
20 O
songs O
in O
their O
listening O
history O
and O
songs O
that O
are O
listened O
to O
by O
at O
least O
200 O
users O
. O
Table O
1 O
summarizes O
the O
dimensions O
of O
all O
the O
datasets O
after O
preprocessing O
. O
section O
: O
Metrics O
We O
use O
two O
ranking Metric
- Metric
based Metric
metrics Metric
: O
Recall@R Metric
and O
the O
truncated Method
normalized Method
discounted Method
cumulative Method
gain Method
( O
NDCG@R Method
) O
. O
For O
each O
user O
, O
both O
metrics O
compare O
the O
predicted O
rank O
of O
the O
held O
- O
out O
items O
with O
their O
true O
rank O
. O
For O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
, O
we O
get O
the O
predicted O
rank O
by O
sorting O
the O
un O
- O
normalized O
multinomial O
probability O
f O
θ O
( O
z O
) O
. O
While O
Recall@R Metric
considers O
all O
items O
ranked O
within O
the O
first O
R O
to O
be O
equally O
important O
, O
NDCG@R Method
uses O
a O
monotonically O
increasing O
discount O
to O
emphasize O
the O
importance O
of O
higher O
ranks O
versus O
lower O
ones O
. O
Formally O
, O
define O
ω O
( O
r O
) O
as O
the O
item O
at O
rank O
r O
, O
I O
[ O
· O
] O
is O
the O
indicator O
function O
, O
and O
I O
u O
is O
the O
set O
of O
held O
- O
out O
items O
that O
user O
u O
clicked O
on O
. O
Recall@R Metric
for O
user O
u O
is O
The O
expression O
in O
the O
denominator O
is O
the O
minimum O
of O
R O
and O
the O
number O
of O
items O
clicked O
on O
by O
user O
u. O
This O
normalizes O
Recall@R Metric
to O
have O
a O
maximum O
of O
1 O
, O
which O
corresponds O
to O
ranking O
all O
relevant O
items O
in O
the O
top O
R O
positions O
. O
Truncated Method
discounted Method
cumulative Method
gain Method
( O
DCG@R Method
) O
is O
NDCG@R Method
is O
the O
DCG@R Method
linearly O
normalized O
to O
[ O
0 O
, O
1 O
] O
after O
dividing O
by O
the O
best O
possible O
DCG@R Method
, O
where O
all O
the O
held O
- O
out O
items O
are O
ranked O
at O
the O
top O
. O
section O
: O
Experimental O
setup O
We O
study O
the O
performance O
of O
various O
models O
under O
strong O
generalization O
[ O
reference O
] O
: O
We O
split O
all O
users O
into O
training O
/ O
validation O
/ O
test O
sets O
. O
We O
train O
models O
using O
the O
entire O
click O
history O
of O
the O
training O
users O
. O
To O
evaluate O
, O
we O
take O
part O
of O
the O
click O
history O
from O
held O
- O
out O
( O
validation O
and O
test O
) O
users O
to O
learn O
the O
necessary O
user O
- O
level O
representations O
for O
the O
model O
and O
then O
compute O
metrics O
by O
looking O
at O
how O
well O
the O
model O
ranks O
the O
rest O
of O
the O
unseen O
click O
history O
from O
the O
held O
- O
out O
users O
. O
This O
is O
relatively O
more O
difficult O
than O
weak Method
generalization Method
where O
the O
user O
's O
click O
history O
can O
appear O
during O
both O
training O
and O
evaluation Task
. O
We O
consider O
it O
more O
realistic O
and O
robust O
as O
well O
. O
In O
the O
last O
row O
of O
Table O
1 O
, O
we O
list O
the O
number O
of O
held O
- O
out O
users O
( O
we O
use O
the O
same O
number O
of O
users O
for O
validation O
and O
test O
) O
. O
For O
each O
held O
- O
out O
user O
, O
we O
randomly O
choose O
80 O
% O
of O
the O
click O
history O
as O
the O
" O
fold O
- O
in O
" O
set O
to O
learn O
the O
necessary O
user Method
- Method
level Method
representation Method
and O
report O
metrics O
on O
the O
remaining O
20 O
% O
of O
the O
click O
history O
. O
We O
select O
model Method
hyperparameters Method
and O
architectures O
by O
evaluating O
NDCG@100 Method
on O
the O
validation O
users O
. O
For O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
, O
we O
keep O
the O
architecture O
for O
the O
generative Method
model Method
f O
θ O
( O
· O
) O
and O
the O
inference Method
model Method
д O
ϕ O
( O
· O
) O
symmetrical O
and O
explore O
multilayer Method
perceptron Method
( Method
mlp Method
) O
with O
0 O
, O
1 O
, O
and O
2 O
hidden O
layers O
. O
We O
set O
the O
dimension O
of O
the O
latent O
representation O
K O
to O
200 O
and O
any O
hidden O
layer O
to O
600 O
. O
As O
a O
concrete O
example O
, O
recall O
I O
is O
the O
total O
number O
of O
items O
, O
the O
overall O
architecture O
for O
a O
Mult O
- O
vae Method
pr O
/ O
Mult Method
- Method
dae Method
with O
1 Method
- Method
hidden Method
- Method
layer Method
mlp Method
generative Method
model Method
would O
be O
[ O
I O
→ O
600 O
→ O
200 O
→ O
600 O
→ O
I O
] O
. O
We O
find O
that O
going O
deeper O
does O
not O
improve O
performance O
. O
The O
best O
performing O
architectures O
are O
mlps Method
with O
either O
0 O
or O
1 O
hidden O
layers O
. O
We O
use O
a O
tanh Method
non Method
- Method
linearity Method
as O
the O
activation O
function O
between O
layers O
. O
Note O
that O
for O
Mult Method
- Method
vae Method
pr Method
, O
since O
the O
output O
of O
д O
ϕ O
( O
· O
) O
is O
used O
as O
the O
mean O
and O
variance O
of O
a O
Gaussian O
random O
variable O
, O
we O
do O
not O
apply O
an O
activation Method
function Method
to O
it O
. O
Thus O
, O
the O
Mult Method
- Method
vae Method
pr Method
with O
0 Method
- Method
hiddenlayer Method
mlp Method
is O
effectively O
a O
log Method
- Method
linear Method
model Method
. O
We O
tune O
the O
regularization O
parameter O
β O
for O
Mult Method
- Method
vae Method
pr Method
following O
the O
procedure O
described O
in O
Section O
2.2.2 O
. O
We O
anneal O
the O
Kullback O
- O
Leibler O
term O
linearly O
for O
200 O
, O
000 O
gradient Task
updates Task
. O
For O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
, O
we O
apply O
dropout Method
at O
the O
input O
layer O
with O
probability O
0.5 O
. O
We O
apply O
a O
weight Method
decay Method
of Method
0.01 Method
for O
Mult Method
- Method
dae Method
. O
We O
do O
not O
apply O
weight Method
decay Method
for O
any O
vae Method
models O
. O
We O
train O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
using O
Adam Method
[ O
reference O
] O
with O
batch O
size O
of O
500 O
users O
. O
For O
ML Material
- Material
20 Material
M Material
, O
we O
train O
for O
200 O
epochs O
. O
We O
train O
for O
100 O
epochs O
on O
the O
other O
two O
datasets O
. O
We O
keep O
the O
model O
with O
the O
best O
validation O
NDCG@100 Metric
and O
report O
test Metric
set Metric
metrics Metric
with O
it O
. O
section O
: O
Baselines O
We O
compare O
results O
with O
the O
following O
standard O
state O
- O
of O
- O
the O
- O
art O
collaborative Method
filtering Method
models Method
, O
both O
linear Method
and Method
non Method
- Method
linear Method
: O
Weighted Method
matrix Method
factorization Method
( O
wmf Method
) O
[ O
reference O
] O
: O
a O
linear Method
low Method
- Method
rank Method
factorization Method
model Method
. O
We O
train O
wmf Method
with O
alternating Method
least Method
squares Method
; O
this O
generally O
leads O
to O
better O
performance O
than O
with O
SGD Method
. O
We O
set O
the O
weights O
on O
all O
the O
0 O
's O
to O
1 O
and O
tune O
the O
weights O
on O
all O
the O
1 O
's O
in O
the O
click O
matrix O
among O
{ O
2 O
, O
5 O
, O
10 O
, O
30 O
, O
50 O
, O
100 O
} O
, O
as O
well O
as O
the O
latent O
representation O
dimension O
K O
∈ O
{ O
100 O
, O
200 O
} O
by O
evaluating O
NDCG@100 Method
on O
validation O
users O
. O
Slim Method
[ O
reference O
] O
: O
a O
linear Method
model Method
which O
learns O
a O
sparse O
item O
- O
to O
- O
item O
similarity O
matrix O
by O
solving O
a O
constrained Task
ℓ Task
1 Task
- Task
regularized Task
optimization Task
problem Task
. O
We O
grid O
- O
search O
both O
of O
the O
regularization O
parameters O
over O
{ O
0.1 O
, O
0.5 O
, O
1 O
, O
5 O
} O
and O
report O
the O
setting O
with O
the O
best O
NDCG@100 Metric
on O
validation O
users O
. O
We O
did O
not O
evaluate O
Slim Method
on O
MSD Method
because O
the O
dataset O
is O
too O
large O
for O
it O
to O
finish O
in O
a O
reasonable O
amount O
of O
time O
( O
for O
the O
Netflix Material
dataset Material
, O
the O
parallelized Task
grid Task
search Task
took O
about O
two O
weeks O
) O
. O
We O
also O
found O
that O
the O
faster O
approximation Method
of O
Slim Method
[ O
reference O
] O
did O
not O
yield O
competitive O
performance O
. O
Collaborative O
denoising Method
autoencoder Method
( O
cdae O
) O
[ O
reference O
] O
: O
augments O
the O
standard O
denoising Method
autoencoder Method
by O
adding O
a O
per O
- O
user O
latent O
factor O
to O
the O
input O
. O
We O
change O
the O
( O
user O
, O
item O
) O
entry Method
subsampling Method
strategy Method
in O
SGD Task
training Task
in O
the O
original O
paper O
to O
the O
user Method
- Method
level Method
subsampling Method
as O
we O
did O
with O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
. O
We O
generally O
find O
that O
this O
leads O
to O
more O
stable O
convergence Metric
and O
better O
performance O
. O
We O
set O
the O
dimension O
of O
the O
bottleneck O
layer O
to O
200 O
, O
and O
use O
a O
weighted Method
square Method
loss Method
, O
equivalent O
to O
what O
the O
square O
loss O
with O
negative O
sampling O
used O
in O
the O
original O
paper O
. O
We O
apply O
tanh Method
activation Method
at O
both O
the O
bottleneck Method
layer Method
as O
well O
as O
the O
output O
layer O
. O
[ O
reference O
] O
We O
use O
Adam Method
with O
a O
batch O
size O
of O
500 O
users O
. O
As O
mentioned O
in O
Section O
3 O
, O
the O
number O
of O
parameters O
for O
cdae Method
grows O
linearly O
with O
the O
number O
of O
users O
and O
items O
. O
Thus O
, O
it O
is O
crucial O
to O
control O
overfitting O
by O
applying O
weight Method
decay Method
. O
We O
select O
the O
weight O
decay O
parameter O
over O
{ O
0.01 O
, O
0.1 O
, O
· O
· O
· O
, O
100 O
} O
by O
examining O
the O
validation Metric
NDCG@100 Metric
. O
Neural Method
collaborative Method
filtering Method
( O
ncf Method
) O
[ O
reference O
] O
: O
explores O
non O
- O
linear O
interactions O
( O
via O
a O
neural Method
network Method
) O
between O
the O
user O
and O
item O
latent O
factors O
. O
Similar O
to O
cdae Method
, O
the O
number O
of O
parameters O
for O
ncf Method
grows O
linearly O
with O
the O
number O
of O
users O
and O
items O
. O
We O
use O
the O
publicly O
available O
source O
code O
provided O
by O
the O
authors O
, O
yet O
can O
not O
obtain O
competitive O
performance O
on O
the O
datasets O
used O
in O
this O
paper O
- O
the O
validation Metric
metrics Metric
drop O
within O
the O
first O
few O
epochs O
over O
a O
wide O
range O
of O
regularization O
parameters O
. O
The O
authors O
kindly O
provided O
the O
two O
datasets O
( O
ML Material
- Material
1 Material
M Material
and O
Pinterest Material
) O
used O
in O
the O
original O
paper O
, O
as O
well O
as O
the O
training O
/ O
test O
split O
, O
therefore O
we O
separately O
compare O
with O
ncf Method
on O
these O
two O
relatively O
smaller O
datasets O
in O
the O
empirical O
study O
. O
In O
particular O
, O
we O
compare O
with O
the O
hybrid Method
NeuCF Method
model Method
which O
gives O
the O
best O
performance O
in O
He O
et O
al O
. O
[ O
reference O
] O
, O
both O
with O
and O
without O
pre Method
- Method
training Method
. O
We O
also O
experiment O
with O
Bayesian Task
personalized Task
ranking Task
( O
bpr Method
) O
[ O
reference O
] O
. O
However O
, O
the O
performance O
is O
not O
on O
par O
with O
the O
other O
baselines O
above O
. O
This O
is O
consistent O
with O
some O
other O
studies O
with O
similar O
baselines O
[ O
reference O
] O
. O
Therefore O
, O
we O
do O
not O
include O
bpr O
in O
the O
following O
results O
and O
analysis O
. O
section O
: O
Experimental O
results O
and O
analysis O
In O
this O
section O
, O
we O
quantitatively O
compare O
our O
proposed O
methods O
with O
various O
baselines O
. O
In O
addition O
, O
we O
aim O
to O
answer O
the O
following O
two O
questions O
: O
1 O
. O
How O
does O
multinomial O
likelihood O
compare O
with O
other O
commonly O
used O
likelihood Method
models Method
for O
collaborative Task
filtering Task
? O
2 O
. O
When O
does O
Mult Method
- Method
vae Method
pr Method
perform O
better O
/ O
worse O
than O
Mult Method
- Method
dae Method
? O
Quantitative O
results O
. O
Table O
2 O
summarizes O
the O
results O
between O
our O
proposed O
methods O
and O
various O
baselines O
. O
Each O
metric O
is O
averaged O
across O
all O
test O
users O
. O
Both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
significantly O
outperform O
the O
baselines O
across O
datasets O
and O
metrics O
. O
Multvae Method
pr Method
significantly O
outperforms O
Mult Method
- Method
dae Method
on O
ML Material
- Material
20 Material
M Material
and O
Netflix Material
data Material
- Material
sets Material
. O
In O
most O
of O
the O
cases O
, O
non Method
- Method
linear Method
models Method
( O
Mult Method
- Method
vae Method
pr Method
, O
Multdae Method
, O
and O
cdae Method
) O
prove O
to O
be O
more O
powerful O
collaborative Method
filtering Method
models Method
than O
state O
- O
of O
- O
the O
- O
art O
linear Method
models Method
. O
The O
inferior O
results O
of O
cdae Method
on O
MSD Method
are O
possibly O
due O
to O
overfitting O
with O
the O
huge O
number O
of O
users O
and O
items O
, O
as O
validation Metric
metrics Metric
drop O
within O
the O
first O
few O
epochs O
even O
though O
the O
training Metric
objective Metric
continues O
improving O
. O
We O
compare O
with O
ncf Method
on O
the O
two O
relatively O
smaller O
datasets O
used O
in O
Hu O
et O
al O
. O
[ O
reference O
] O
: O
ML Material
- Material
1 Material
M Material
( O
6 O
, O
040 O
users O
, O
3 O
, O
704 O
items O
, O
4.47 O
% O
density O
) O
and O
Pinterest O
( O
55 O
, O
187 O
users O
, O
9 O
, O
916 O
items O
, O
0.27 O
% O
density O
) O
. O
Because O
of O
the O
size O
of O
these O
two O
datasets O
, O
we O
use O
Mult Method
- Method
dae Method
with O
a O
0 Method
- Method
hidden Method
- Method
layer Method
mlp Method
generative Method
model Method
- O
the O
overall O
architecture O
is O
[ O
I O
→ O
200 O
→ O
I O
] O
. O
( O
Recall O
Mult Method
- Method
vae Method
pr Method
with O
a O
0 Method
- Method
hidden Method
- Method
layer Method
mlp Method
generative Method
model Method
is O
effectively O
a O
log Method
- Method
linear Method
model Method
with O
limited Method
modeling Method
capacity Method
. O
) O
Table O
3 O
summarizes O
the O
results O
between O
Mult Method
- Method
dae Method
and O
ncf Method
. O
Multdae Method
significantly O
outperforms O
ncf Method
without O
pre Method
- Method
training Method
on O
both O
datasets O
. O
On O
the O
larger O
Pinterest Material
dataset Material
, O
Mult Method
- Method
dae Method
even O
improves O
over O
the O
pre Method
- Method
trained Method
ncf Method
model Method
by O
a O
big O
margin O
. O
How O
well O
does O
multinomial Method
likelihood Method
perform O
? O
Despite O
being O
commonly O
used O
in O
language Method
models Method
, O
multinomial Method
likelihoods Method
have O
typically O
received O
less O
attention O
in O
the O
collaborative Task
filtering Task
literature Task
, O
especially O
with O
latent Method
- Method
factor Method
models Method
. O
Most O
previous O
work O
builds O
on O
Gaussian O
likelihoods O
( O
square O
loss O
, O
Eq O
. O
3 O
) O
[ O
reference O
][ O
reference O
][ O
reference O
] O
or O
logistic O
likelihood O
( O
log O
loss O
, O
Eq O
. O
4 O
) O
[ O
reference O
][ O
reference O
] O
instead O
. O
We O
argue O
in O
Section O
2.1 O
that O
multinomial O
likelihood O
is O
in O
fact O
a O
good O
proxy O
for O
the O
top Metric
- Metric
N Metric
ranking Metric
loss Metric
and O
is O
well O
- O
suited O
for O
implicit O
feedback O
data O
. O
To O
demonstrate O
the O
effectiveness O
of O
multinomial Method
likelihood Method
, O
we O
take O
the O
best O
- O
performing O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
model O
on O
each O
dataset O
and O
swap O
the O
likelihood Method
distribution Method
model Method
for O
the O
data O
while O
keeping O
everything O
else O
exactly O
the O
same O
. O
Table O
4 O
summarizes O
the O
results O
of O
different O
likelihoods O
on O
ML Material
- Material
20 Material
M Material
( O
the O
results O
on O
the O
other O
two O
datasets O
are O
similar O
. O
) O
We O
tune O
the O
hyperparameters O
for O
each O
likelihood O
separately O
. O
[ O
reference O
] O
The O
multinomial Method
likelihood Method
performs O
better O
than O
the O
other O
likelihoods O
. O
The O
gap O
between O
logistic O
and O
multinomial O
likelihood O
is O
closer O
- O
this O
is O
understandable O
since O
multinomial O
likelihood O
can O
be O
approximated O
by O
individual O
binary Method
logistic Method
likelihood Method
, O
a O
strategy O
commonly O
adopted O
in O
language Method
modeling Method
[ O
reference O
][ O
reference O
] O
. O
We O
wish O
to O
emphasize O
that O
the O
choice O
of O
likelihood O
remains O
datadependent O
. O
For O
the O
task O
of O
collaborative Task
filtering Task
, O
the O
multinomial Method
likelihood Method
achieves O
excellent O
empirical O
results O
. O
The O
methodology O
behind O
the O
partial Method
regularization Method
in O
Mult Method
- Method
vae Method
pr Method
, O
however O
, O
is O
a O
technique O
we O
hypothesize O
will O
generalize O
to O
other O
domains O
. O
When O
does O
Mult Method
- Method
vae Method
pr Method
perform O
better O
/ O
worse O
than O
Multdae Method
? O
In O
Table O
2 O
we O
can O
see O
that O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
produce O
competitive O
empirical O
results O
with O
Mult Method
- Method
vae Method
pr Method
being O
comparably O
better O
. O
It O
is O
natural O
to O
wonder O
when O
a O
variational Method
Bayesian Method
inference Method
approach Method
( O
Mult Method
- Method
vae Method
pr Method
) O
will O
win O
over O
using O
a O
point Method
estimate Method
( O
Mult Method
- Method
dae Method
) O
and O
vice O
versa O
. O
Intuitively O
, O
Mult Method
- Method
vae Method
pr Method
imposes O
stronger O
modeling O
assumptions O
and O
therefore O
could O
be O
more O
robust O
when O
user O
- O
item O
interaction O
data O
is O
scarce O
. O
To O
study O
this O
, O
we O
considered O
two O
datasets O
: O
ML Material
- Material
20 Material
M Material
where O
Mult Method
- Method
vae Method
pr Method
has O
the O
largest O
margin O
over O
Mult Method
- Method
dae Method
and O
MSD Method
where O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
have O
roughly O
similar O
performance O
. O
The O
results O
on O
the O
Netflix Material
dataset Material
are O
similar O
to O
ML Material
- Material
20M. Material
We O
break O
down O
test O
users O
into O
quintiles O
based O
on O
their O
activity O
level O
in O
the O
fold O
- O
in O
set O
which O
is O
provided O
as O
input O
to O
the O
inference Method
model Method
д O
ϕ O
( O
· O
) O
to O
make O
prediction Task
. O
The O
activity O
level O
is O
simply O
the O
number O
of O
items O
each O
user O
has O
clicked O
on O
. O
We O
compute O
NDCG@100 Method
for O
each O
group O
of O
users O
using O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
and O
plot O
results O
in O
Figure O
3 O
. O
This O
summarizes O
how O
performance O
differs O
across O
users O
with O
various O
levels O
of O
activity O
. O
In O
Figure O
3 O
, O
we O
show O
performance O
across O
increasing O
user O
activity O
. O
Error O
bars O
represents O
one O
standard O
error O
. O
For O
each O
subplot O
, O
a O
paired O
t O
- O
test O
is O
performed O
and O
statistical O
significance O
is O
marked O
. O
Although O
there O
are O
slight O
variations O
across O
datasets O
, O
Mult Method
- Method
vae Method
pr Method
consistently O
improves O
recommendation Task
performance O
for O
users O
who O
have O
only O
clicked O
on O
a O
small O
number O
of O
items O
. O
This O
is O
particularly O
prominent O
for O
ML Material
- Material
20 Material
M Material
( O
Figure O
3a O
) O
. O
Interestingly O
, O
Mult Method
- Method
dae Method
actually O
[ O
reference O
] O
Surprisingly O
, O
partial Method
regularization Method
seems O
less O
effective O
for O
Gaussian Method
and Method
logistic Method
. O
Figure O
3 O
: O
NDCG@100 Task
breakdown Task
for O
users O
with O
increasing O
levels O
of O
activity O
( O
starting O
from O
0 O
% O
) O
, O
measured O
by O
how O
many O
items O
a O
user O
clicked O
on O
in O
the O
fold O
- O
in O
set O
. O
The O
error O
bars O
represents O
one O
standard O
error O
. O
For O
each O
subplot O
, O
a O
paired O
t O
- O
test O
is O
performed O
and O
* O
indicates O
statistical O
significance O
at O
α O
= O
0.05 O
level O
, O
* O
* O
at O
α O
= O
0.01 O
level O
, O
and O
* O
* O
* O
at O
α O
= O
0.001 O
level O
. O
Although O
details O
vary O
across O
datasets O
, O
Mult Method
- Method
vae Method
pr Method
consistently O
improves O
recommendation Task
performance O
for O
users O
who O
have O
only O
clicked O
on O
a O
small O
number O
of O
items O
. O
outperforms O
Mult Method
- Method
vae Method
pr Method
on O
the O
most O
active O
users O
. O
This O
indicates O
the O
stronger O
prior O
assumption O
could O
potentially O
hurt O
the O
performance O
when O
a O
lot O
of O
data O
is O
available O
for O
a O
user O
. O
For O
MSD Method
( O
Figure O
3b O
) O
, O
the O
least O
- O
active O
users O
have O
similar O
performance O
under O
both O
Multvae Method
pr Method
and O
Mult Method
- Method
dae Method
. O
However O
, O
as O
we O
described O
in O
Section O
4.1 O
, O
MSD Method
is O
pre O
- O
processed O
so O
that O
a O
user O
has O
at O
least O
listened O
to O
20 O
songs O
. O
Meanwhile O
for O
ML Material
- Material
20 Material
M Material
, O
each O
user O
has O
to O
watch O
at O
least O
5 O
movies O
. O
This O
means O
that O
the O
first O
bin O
of O
ML Material
- Material
20 Material
M Material
has O
much O
lower O
user O
activity O
than O
the O
first O
bin O
of O
MSD Method
. O
Overall O
, O
we O
find O
that O
Mult Method
- Method
vae Method
pr Method
, O
which O
may O
be O
viewed O
under O
the O
lens O
of O
a O
principled Method
Bayesian Method
inference Method
approach Method
, O
is O
more O
robust O
than O
the O
point Method
estimate Method
approach Method
of O
Mult Method
- Method
dae Method
, O
regardless O
of O
the O
scarcity O
of O
the O
data O
. O
More O
importantly O
, O
the O
Mult Method
- Method
vae Method
pr Method
is O
less O
sensitive O
to O
the O
choice O
of O
hyperparameters O
- O
weight O
decay O
is O
important O
for O
Mult Method
- Method
dae Method
to O
achieve O
competitive O
performance O
, O
yet O
it O
is O
not O
required O
for O
Mult Method
- Method
vae Method
pr Method
. O
On O
the O
other O
hand O
, O
Mult Method
- Method
dae Method
also O
has O
advantages O
: O
it O
requires O
fewer O
parameters O
in O
the O
bottleneck O
layer O
- O
Mult O
- O
vae Method
pr O
requires O
two O
sets O
of O
parameters O
to O
obtain O
the O
latent Method
representation Method
z O
: O
one O
set O
for O
the O
variational O
mean O
µ O
ϕ O
( O
· O
) O
and O
another O
for O
the O
variational Metric
variance Metric
σ Metric
ϕ O
( O
· O
) O
- O
and O
Mult Method
- Method
dae Method
is O
conceptually O
simpler O
for O
practitioners O
. O
section O
: O
CONCLUSION O
In O
this O
paper O
, O
we O
develop O
a O
variant O
of O
vae Method
for O
collaborative Task
filtering Task
on O
implicit O
feedback O
data O
. O
This O
enables O
us O
to O
go O
beyond O
linear Method
factor Method
models Method
with O
limited Method
modeling Method
capacity Method
. O
We O
introduce O
a O
generative Method
model Method
with O
a O
multinomial Method
likelihood Method
function Method
parameterized O
by O
neural Method
network Method
. O
We O
show O
that O
multinomial Method
likelihood Method
is O
particularly O
well O
suited O
to O
modeling O
user O
- O
item O
implicit O
feedback O
data O
. O
Based O
on O
an O
alternative O
interpretation O
of O
the O
vae Method
objective O
, O
we O
introduce O
an O
additional O
regularization O
parameter O
to O
partially O
regularize O
a O
vae Method
( O
Mult Method
- Method
vae Method
pr Method
) O
. O
We O
also O
provide O
a O
practical O
and O
efficient O
way O
to O
tune O
the O
additional O
parameter O
introduced O
using O
KL Method
annealing Method
. O
We O
compare O
the O
results O
obtained O
against O
a O
denoising Method
autoencoder Method
( O
Mult Method
- Method
dae Method
) O
. O
Empirically O
, O
we O
show O
that O
the O
both O
Mult Method
- Method
vae Method
pr Method
and O
Mult Method
- Method
dae Method
provide O
competitive O
performance O
with O
Mult Method
- Method
vae Method
pr Method
significantly O
outperforming O
the O
state O
- O
of O
- O
the O
- O
art O
baselines O
on O
several O
real O
- O
world O
datasets O
, O
including O
two O
recently O
proposed O
neural Method
- Method
network Method
- Method
based Method
approaches Method
. O
Finally O
, O
we O
identify O
the O
pros O
and O
cons O
of O
both O
Mult Method
- Method
vae Method
pr Method
and O
Multdae Method
and O
show O
that O
employing O
a O
principled Method
Bayesian Method
approach Method
is O
more O
robust O
. O
In O
future O
work O
, O
we O
would O
like O
to O
futher O
investigate O
the O
tradeoff O
introduced O
by O
the O
additional O
regularization O
parameter O
β O
and O
gain O
more O
theoretical O
insight O
into O
why O
it O
works O
so O
well O
. O
Extending O
Mult Method
- Method
vae Method
pr Method
by O
condition O
on O
side O
information O
might O
also O
be O
a O
way O
to O
improve O
performance O
. O
section O
: O
