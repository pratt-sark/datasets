Product	Method
-	Method
based	Method
Neural	Method
Networks	Method
for	O
User	Task
Response	Task
Prediction	Task
section	O
:	O
Abstract	O
-	O
Predicting	Task
user	Task
responses	Task
,	O
such	O
as	O
clicks	O
and	O
conversions	O
,	O
is	O
of	O
great	O
importance	O
and	O
has	O
found	O
its	O
usage	O
in	O
many	O
Web	Task
applications	Task
including	O
recommender	Task
systems	Task
,	O
web	Task
search	Task
and	O
online	Task
advertising	Task
.	O
The	O
data	O
in	O
those	O
applications	O
is	O
mostly	O
categorical	O
and	O
contains	O
multiple	O
fields	O
;	O
a	O
typical	O
representation	O
is	O
to	O
transform	O
it	O
into	O
a	O
high	Method
-	Method
dimensional	Method
sparse	Method
binary	Method
feature	Method
representation	Method
via	O
one	Method
-	Method
hot	Method
encoding	Method
.	O
Facing	O
with	O
the	O
extreme	O
sparsity	O
,	O
traditional	O
models	O
may	O
limit	O
their	O
capacity	O
of	O
mining	O
shallow	O
patterns	O
from	O
the	O
data	O
,	O
i.e.	O
low	O
-	O
order	O
feature	O
combinations	O
.	O
Deep	Method
models	Method
like	O
deep	Method
neural	Method
networks	Method
,	O
on	O
the	O
other	O
hand	O
,	O
can	O
not	O
be	O
directly	O
applied	O
for	O
the	O
high	O
-	O
dimensional	O
input	O
because	O
of	O
the	O
huge	O
feature	O
space	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
Product	Method
-	Method
based	Method
Neural	Method
Networks	Method
(	O
PNN	Method
)	O
with	O
an	O
embedding	Method
layer	Method
to	O
learn	O
a	O
distributed	Method
representation	Method
of	O
the	O
categorical	O
data	O
,	O
a	O
product	Method
layer	Method
to	O
capture	O
interactive	O
patterns	O
between	O
interfield	O
categories	O
,	O
and	O
further	O
fully	Method
connected	Method
layers	Method
to	O
explore	O
high	O
-	O
order	O
feature	O
interactions	O
.	O
Our	O
experimental	O
results	O
on	O
two	O
large	O
-	O
scale	O
real	O
-	O
world	O
ad	O
click	O
datasets	O
demonstrate	O
that	O
PNNs	Method
consistently	O
outperform	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
various	O
metrics	Metric
.	O
section	O
:	O
I.	O
INTRODUCTION	O
Learning	Task
and	Task
predicting	Task
user	Task
response	Task
now	O
plays	O
a	O
crucial	O
role	O
in	O
many	O
personalization	Task
tasks	Task
in	O
information	Task
retrieval	Task
(	O
IR	Task
)	Task
,	O
such	O
as	O
recommender	Task
systems	Task
,	O
web	Task
search	Task
and	O
online	Task
advertising	Task
.	O
The	O
goal	O
of	O
user	Task
response	Task
prediction	Task
is	O
to	O
estimate	O
the	O
probability	O
that	O
the	O
user	O
will	O
provide	O
a	O
predefined	O
positive	O
response	O
,	O
e.g.	O
clicks	O
,	O
purchases	O
etc	O
.	O
,	O
in	O
a	O
given	O
context	O
[	O
reference	O
]	O
.	O
This	O
predicted	O
probability	O
indicates	O
the	O
user	O
's	O
interest	O
on	O
the	O
specific	O
item	O
such	O
as	O
a	O
news	O
article	O
,	O
a	O
commercial	O
item	O
or	O
an	O
advertising	O
post	O
,	O
which	O
influences	O
the	O
subsequent	O
decision	Task
making	Task
such	O
as	O
document	Task
ranking	Task
[	O
reference	O
]	O
and	O
ad	Task
bidding	Task
[	O
reference	O
]	O
.	O
The	O
data	Task
collection	Task
in	O
these	O
IR	Task
tasks	Task
is	O
mostly	O
in	O
a	O
multifield	O
categorical	O
form	O
,	O
for	O
example	O
,	O
[	O
Weekday	O
=	O
Tuesday	O
,	O
Gender	O
=	O
Male	O
,	O
City	O
=	O
London	O
]	O
,	O
which	O
is	O
normally	O
transformed	O
into	O
high	O
-	O
dimensional	O
sparse	O
binary	O
features	O
via	O
onehot	Method
encoding	Method
[	O
reference	O
]	O
.	O
For	O
example	O
,	O
the	O
three	O
field	O
vectors	O
with	O
one	Method
-	Method
hot	Method
encoding	Method
are	O
concatenated	O
as	O
Many	O
machine	Method
learning	Method
models	Method
,	O
including	O
linear	Method
logistic	Method
regression	Method
[	O
reference	O
]	O
,	O
non	Method
-	Method
linear	Method
gradient	Method
boosting	Method
decision	Method
trees	Method
[	O
reference	O
]	O
and	O
factorization	Method
machines	Method
[	O
reference	O
]	O
,	O
have	O
been	O
proposed	O
to	O
work	O
on	O
such	O
high	O
-	O
dimensional	O
sparse	O
binary	O
features	O
and	O
produce	O
high	O
quality	O
user	Task
response	Task
predictions	Task
.	O
However	O
,	O
these	O
models	O
highly	O
depend	O
on	O
feature	Method
engineering	Method
in	O
order	O
to	O
capture	O
highorder	O
latent	O
patterns	O
[	O
reference	O
]	O
.	O
Recently	O
,	O
deep	Method
neural	Method
networks	Method
(	O
DNNs	Method
)	O
[	O
reference	O
]	O
have	O
shown	O
great	O
capability	O
in	O
classification	Task
and	Task
regression	Task
tasks	Task
,	O
including	O
computer	Task
vision	Task
[	O
reference	O
]	O
,	O
speech	Task
recognition	Task
[	O
reference	O
]	O
and	O
natural	Task
language	Task
processing	Task
[	O
reference	O
]	O
.	O
It	O
is	O
promising	O
to	O
adopt	O
DNNs	Method
in	O
user	Task
response	Task
prediction	Task
since	O
DNNs	Method
could	O
automatically	O
learn	O
more	O
expressive	Method
feature	Method
representations	Method
and	O
deliver	O
better	O
prediction	Task
performance	O
.	O
In	O
order	O
to	O
improve	O
the	O
multi	Task
-	Task
field	Task
categorical	Task
data	Task
interaction	Task
,	O
[	O
reference	O
]	O
presented	O
an	O
embedding	Method
methodology	Method
based	O
on	O
pre	Method
-	Method
training	Method
of	O
a	O
factorization	Method
machine	Method
.	O
Based	O
on	O
the	O
concatenated	O
embedding	O
vectors	O
,	O
multi	Method
-	Method
layer	Method
perceptrons	Method
(	O
MLPs	Method
)	O
were	O
built	O
to	O
explore	O
feature	O
interactions	O
.	O
However	O
,	O
the	O
quality	Metric
of	O
embedding	Task
initialization	Task
is	O
largely	O
limited	O
by	O
the	O
factorization	Method
machine	Method
.	O
More	O
importantly	O
,	O
the	O
"	O
add	Method
"	Method
operations	Method
of	O
the	O
perceptron	Method
layer	Method
might	O
not	O
be	O
useful	O
to	O
explore	O
the	O
interactions	O
of	O
categorical	O
data	O
in	O
multiple	O
fields	O
.	O
Previous	O
work	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
has	O
shown	O
that	O
local	O
dependencies	O
between	O
features	O
from	O
different	O
fields	O
can	O
be	O
effectively	O
explored	O
by	O
feature	Method
vector	Method
"	Method
product	Method
"	Method
operations	Method
instead	O
of	O
"	O
add	Method
"	Method
operations	Method
.	O
To	O
utilize	O
the	O
learning	O
ability	O
of	O
neural	Method
networks	Method
and	O
mine	O
the	O
latent	O
patterns	O
of	O
data	O
in	O
a	O
more	O
effective	O
way	O
than	O
MLPs	Method
,	O
in	O
this	O
paper	O
we	O
propose	O
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
(	O
PNN	Method
)	O
which	O
(	O
i	O
)	O
starts	O
from	O
an	O
embedding	Method
layer	Method
without	O
pretraining	Method
as	O
used	O
in	O
[	O
reference	O
]	O
,	O
and	O
(	O
ii	O
)	O
builds	O
a	O
product	Method
layer	Method
based	O
on	O
the	O
embedded	O
feature	O
vectors	O
to	O
model	O
the	O
inter	O
-	O
field	O
feature	O
interactions	O
,	O
and	O
(	O
iii	O
)	O
further	O
distills	O
the	O
high	O
-	O
order	O
feature	O
patterns	O
with	O
fully	Method
connected	Method
MLPs	Method
.	O
We	O
present	O
two	O
types	O
of	O
PNNs	Method
,	O
with	O
inner	Method
and	Method
outer	Method
product	Method
operations	Method
in	O
the	O
product	Method
layer	Method
,	O
to	O
efficiently	O
model	O
the	O
interactive	O
patterns	O
.	O
We	O
take	O
CTR	Task
estimation	Task
in	O
online	Task
advertising	Task
as	O
the	O
working	O
example	O
to	O
explore	O
the	O
learning	O
ability	O
of	O
our	O
PNN	Method
model	O
.	O
The	O
extensive	O
experimental	O
results	O
on	O
two	O
large	O
-	O
scale	O
realworld	O
datasets	O
demonstrate	O
the	O
consistent	O
superiority	O
of	O
our	O
model	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
user	Method
response	Method
prediction	Method
models	Method
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
on	O
various	O
metrics	Metric
.	O
section	O
:	O
II	O
.	O
RELATED	O
WORK	O
The	O
response	Task
prediction	Task
problem	Task
is	O
normally	O
formulated	O
as	O
a	O
binary	Task
classification	Task
problem	Task
with	O
prediction	O
likelihood	O
or	O
cross	Metric
entropy	Metric
as	O
the	O
training	Metric
objective	Metric
[	O
reference	O
]	O
.	O
Area	Metric
under	Metric
ROC	Metric
Curve	Metric
(	O
AUC	Metric
)	O
and	O
Relative	Metric
Information	Metric
Gain	Metric
(	O
RIG	Metric
)	O
are	O
common	O
evaluation	Metric
metrics	Metric
for	O
response	Metric
prediction	Metric
accuracy	Metric
[	O
reference	O
]	O
.	O
From	O
the	O
modeling	O
perspective	O
,	O
linear	Method
logistic	Method
regression	Method
(	Method
LR	Method
)	Method
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
non	Method
-	Method
linear	Method
gradient	Method
boosting	Method
decision	Method
trees	Method
(	O
GBDT	Method
)	O
[	O
reference	O
]	O
and	O
factorization	Method
machines	Method
(	O
FM	Method
)	Method
[	O
reference	O
]	O
are	O
widely	O
used	O
in	O
industrial	Task
applications	Task
.	O
However	O
,	O
these	O
models	O
are	O
limited	O
in	O
mining	Task
high	Task
-	Task
order	Task
latent	Task
patterns	Task
or	O
learning	Task
quality	Task
feature	Task
representations	Task
.	O
Deep	Method
learning	Method
is	O
able	O
to	O
explore	O
high	O
-	O
order	O
latent	O
patterns	O
as	O
well	O
as	O
generalizing	O
expressive	Method
data	Method
representations	Method
[	O
reference	O
]	O
.	O
The	O
input	O
data	O
of	O
DNNs	Method
are	O
usually	O
dense	O
real	O
vectors	O
,	O
while	O
the	O
solution	O
of	O
multi	O
-	O
field	O
categorical	O
data	O
has	O
not	O
been	O
well	O
studied	O
.	O
Factorization	Method
-	Method
machine	Method
supported	Method
neural	Method
networks	Method
(	O
FNN	Method
)	O
was	O
proposed	O
in	O
[	O
reference	O
]	O
to	O
learn	O
embedding	O
vectors	O
of	O
categorical	O
data	O
via	O
pre	O
-	O
trained	O
FM	Method
.	O
Convolutional	Method
Click	Method
Prediction	Method
Model	Method
(	O
CCPM	Method
)	O
was	O
proposed	O
in	O
[	O
reference	O
]	O
to	O
predict	O
ad	Task
click	Task
by	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
.	O
However	O
,	O
in	O
CCPM	Method
the	O
convolutions	Method
are	O
only	O
performed	O
on	O
the	O
neighbor	O
fields	O
in	O
a	O
certain	O
alignment	O
,	O
which	O
fails	O
to	O
model	O
the	O
full	O
interactions	O
among	O
non	O
-	O
neighbor	O
features	O
.	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
was	O
leveraged	O
to	O
model	O
the	O
user	O
queries	O
as	O
a	O
series	O
of	O
user	O
context	O
to	O
predict	O
the	O
ad	O
click	O
behavior	O
[	O
reference	O
]	O
.	O
Product	Method
unit	Method
neural	Method
network	Method
(	O
PUNN	Method
)	O
[	O
reference	O
]	O
was	O
proposed	O
to	O
build	O
high	O
-	O
order	O
combinations	O
of	O
the	O
inputs	O
.	O
However	O
,	O
neither	O
can	O
PUNN	O
learn	O
local	O
dependencies	O
,	O
nor	O
produce	O
bounded	O
outputs	O
to	O
fit	O
the	O
response	Metric
rate	Metric
.	O
In	O
this	O
paper	O
,	O
we	O
demonstrate	O
the	O
way	O
our	O
PNN	Method
models	O
learn	O
local	O
dependencies	O
and	O
high	O
-	O
order	O
feature	O
interactions	O
.	O
section	O
:	O
III	O
.	O
DEEP	Method
LEARNING	Method
FOR	O
CTR	Task
ESTIMATION	Task
We	O
take	O
CTR	Task
estimation	Task
in	O
online	Task
advertising	Task
[	O
reference	O
]	O
as	O
a	O
working	O
example	O
to	O
formulate	O
our	O
model	O
and	O
explore	O
the	O
performance	O
on	O
various	O
metrics	Metric
.	O
The	O
task	O
is	O
to	O
build	O
a	O
prediction	Method
model	Method
to	O
estimate	O
the	O
probability	O
of	O
a	O
user	O
clicking	O
a	O
specific	O
ad	O
in	O
a	O
given	O
context	O
.	O
Each	O
data	O
sample	O
consists	O
of	O
multiple	O
fields	O
of	O
categorical	O
data	O
such	O
as	O
user	O
information	O
(	O
City	O
,	O
Hour	O
,	O
etc	O
.	O
)	O
,	O
publisher	O
information	O
(	O
Domain	O
,	O
Ad	O
slot	O
,	O
etc	O
.	O
)	O
and	O
ad	O
information	O
(	O
Ad	O
creative	O
ID	O
,	O
Campaign	O
ID	O
,	O
etc	O
.	O
)	O
[	O
reference	O
]	O
.	O
All	O
the	O
information	O
is	O
represented	O
as	O
a	O
multi	O
-	O
field	O
categorical	O
feature	O
vector	O
,	O
where	O
each	O
field	O
(	O
e.g.	O
City	O
)	O
is	O
one	O
-	O
hot	O
encoded	O
as	O
discussed	O
in	O
Section	O
I.	O
Such	O
a	O
field	Method
-	Method
wise	Method
one	Method
-	Method
hot	Method
encoding	Method
representation	Method
results	O
in	O
curse	O
of	O
dimensionality	O
and	O
enormous	O
sparsity	O
[	O
reference	O
]	O
.	O
Besides	O
,	O
there	O
exist	O
local	O
dependencies	O
and	O
hierarchical	O
structures	O
among	O
fields	O
[	O
reference	O
]	O
.	O
Thus	O
we	O
are	O
seeking	O
a	O
DNN	Method
model	Method
to	O
capture	O
high	O
-	O
order	O
latent	O
patterns	O
in	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
And	O
we	O
come	O
up	O
with	O
the	O
idea	O
of	O
product	O
layers	O
to	O
explore	O
feature	O
interactions	O
automatically	O
.	O
In	O
FM	Task
,	O
feature	O
interaction	O
is	O
defined	O
as	O
the	O
inner	O
product	O
of	O
two	O
feature	O
vectors	O
[	O
reference	O
]	O
.	O
The	O
proposed	O
deep	Method
learning	Method
model	Method
is	O
named	O
as	O
Productbased	Method
Neural	Method
Network	Method
(	O
PNN	Method
)	O
.	O
In	O
this	O
section	O
,	O
we	O
present	O
PNN	Method
model	O
in	O
detail	O
and	O
discuss	O
two	O
variants	O
of	O
this	O
model	O
,	O
namely	O
Inner	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
(	O
IPNN	Method
)	Method
,	O
which	O
has	O
an	O
inner	Method
product	Method
layer	Method
,	O
and	O
Outer	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
(	O
OPNN	Method
)	O
which	O
uses	O
an	O
outer	Method
product	Method
expression	Method
.	O
section	O
:	O
A.	O
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
The	O
architecture	O
of	O
the	O
PNN	Method
model	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
From	O
a	O
top	O
-	O
down	O
perspective	O
,	O
the	O
output	O
of	O
PNN	Method
is	O
a	O
real	O
numberŷ	O
∈	O
(	O
0	O
,	O
1	O
)	O
as	O
the	O
predicted	O
CTR	O
:	O
where	O
W	O
3	O
∈	O
R	O
1×D2	O
and	O
b	O
3	O
∈	O
R	O
are	O
the	O
parameters	O
of	O
the	O
output	O
layer	O
,	O
l	O
2	O
∈	O
R	O
D2	O
is	O
the	O
output	O
of	O
the	O
second	O
hidden	O
layer	O
,	O
and	O
σ	O
(	O
x	O
)	O
is	O
the	O
sigmoid	O
activation	O
function	O
:	O
σ	O
(	O
x	O
)	O
=	O
1	O
/(	O
1	O
+	O
e	O
−x	O
)	O
.	O
And	O
we	O
use	O
D	O
i	O
to	O
represent	O
the	O
dimension	O
of	O
the	O
i	O
-	O
th	O
hidden	O
layer	O
.	O
The	O
output	O
l	O
2	O
of	O
the	O
second	O
hidden	O
layer	O
is	O
constructed	O
as	O
where	O
l	O
1	O
∈	O
R	O
D1	O
is	O
the	O
output	O
of	O
the	O
first	O
hidden	Method
layer	Method
.	O
The	O
rectified	O
linear	O
unit	O
(	O
relu	Method
)	O
,	O
defined	O
as	O
relu	Method
(	Method
x	Method
)	O
=	O
max	O
(	O
0	O
,	O
x	O
)	O
,	O
is	O
chosen	O
as	O
the	O
activation	O
function	O
for	O
hidden	O
layer	O
output	O
since	O
it	O
has	O
outstanding	O
performance	O
and	O
efficient	O
computation	Task
.	O
The	O
first	O
hidden	O
layer	O
is	O
fully	O
connected	O
with	O
the	O
product	Method
layer	Method
.	O
The	O
inputs	O
to	O
it	O
consist	O
of	O
linear	O
signals	O
l	O
z	O
and	O
quadratic	O
signals	O
l	O
p	O
.	O
With	O
respect	O
to	O
l	O
z	O
and	O
l	O
p	O
inputs	O
,	O
separately	O
,	O
the	O
formulation	O
of	O
l	Task
1	Task
is	O
:	O
where	O
all	O
l	O
z	O
,	O
l	O
p	O
and	O
the	O
bias	O
vector	O
b	O
1	O
∈	O
R	O
D1	O
.	O
Then	O
,	O
let	O
us	O
define	O
the	O
operation	O
of	O
tensor	Method
inner	Method
product	Method
:	O
where	O
firstly	O
element	Method
-	Method
wise	Method
multiplication	Method
is	O
applied	O
to	O
A	O
,	O
B	O
,	O
then	O
the	O
multiplication	O
result	O
is	O
summed	O
up	O
to	O
a	O
scalar	O
.	O
After	O
that	O
,	O
l	O
z	O
and	O
l	O
p	O
are	O
calculated	O
through	O
z	O
and	O
p	O
,	O
respectively	O
:	O
where	O
W	O
n	O
z	O
and	O
W	O
n	O
p	O
are	O
the	O
weights	O
in	O
the	O
product	O
layer	O
,	O
and	O
their	O
shapes	O
are	O
determined	O
by	O
z	O
and	O
p	O
respectively	O
.	O
By	O
introducing	O
a	O
"	O
1	O
"	O
constant	O
signal	O
,	O
the	O
product	Method
layer	Method
can	O
not	O
only	O
generate	O
the	O
quadratic	O
signals	O
p	O
,	O
but	O
also	O
maintaining	O
the	O
linear	O
signals	O
z	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
.	O
More	O
specifically	O
,	O
where	O
f	O
i	O
∈	O
R	O
M	O
is	O
the	O
embedding	O
vector	O
for	O
field	O
i.	O
p	O
i	O
,	O
j	O
=	O
g	O
(	O
f	O
i	O
,	O
f	O
j	O
)	O
defines	O
the	O
pairwise	O
feature	O
interaction	O
.	O
Our	O
PNN	Method
model	O
can	O
have	O
different	O
implementations	O
by	O
designing	O
different	O
operation	O
for	O
g.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
two	O
variants	O
of	O
PNN	Method
,	O
namely	O
IPNN	Method
and	O
OPNN	Method
,	O
as	O
will	O
be	O
discussed	O
later	O
.	O
The	O
embedding	O
vector	O
f	O
i	O
of	O
field	O
i	O
,	O
is	O
the	O
output	O
of	O
the	O
embedding	Method
layer	Method
:	O
where	O
x	O
is	O
the	O
input	O
feature	O
vector	O
containing	O
multiple	O
fields	O
,	O
and	O
x	O
[	O
start	O
i	O
:	O
end	O
i	O
]	O
represents	O
the	O
one	O
-	O
hot	O
encoded	O
vector	O
for	O
field	O
i.	O
W	O
0	O
represents	O
the	O
parameters	O
of	O
the	O
embedding	Method
layer	Method
,	O
and	O
)	O
is	O
fully	O
connected	O
with	O
field	O
i.	O
Finally	O
,	O
supervised	Method
training	Method
is	O
applied	O
to	O
minimize	O
the	O
log	Metric
loss	Metric
,	O
which	O
is	O
a	O
widely	O
used	O
objective	Metric
function	Metric
capturing	O
divergence	O
between	O
two	O
probability	O
distributions	O
:	O
where	O
y	O
is	O
the	O
ground	O
truth	O
(	O
1	O
for	O
click	O
,	O
0	O
for	O
non	O
-	O
click	O
)	O
,	O
and	O
y	O
is	O
the	O
predicted	O
CTR	O
of	O
our	O
model	O
as	O
in	O
Eq	O
.	O
(	O
1	O
)	O
.	O
section	O
:	O
B.	O
Inner	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
In	O
this	O
section	O
,	O
we	O
demonstrate	O
the	O
Inner	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
(	O
IPNN	Method
)	Method
.	O
In	O
IPNN	Task
,	O
we	O
firstly	O
define	O
the	O
pairwise	O
feature	O
interaction	O
as	O
vector	Method
inner	Method
product	Method
:	O
With	O
the	O
constant	O
signal	O
"	O
1	O
"	O
,	O
the	O
linear	O
information	O
z	O
is	O
preserved	O
as	O
:	O
As	O
for	O
the	O
quadratic	O
information	O
p	O
,	O
the	O
pairwise	O
inner	O
product	O
terms	O
of	O
g	O
(	O
and	O
the	O
commutative	O
law	O
in	O
vector	O
inner	O
product	O
,	O
p	O
and	O
W	O
n	O
p	O
should	O
be	O
symmetric	O
.	O
Such	O
pairwise	Method
connection	Method
expands	O
the	O
capacity	O
of	O
the	O
neural	Method
network	Method
,	O
but	O
also	O
enormously	O
increases	O
the	O
complexity	Metric
.	O
In	O
this	O
case	O
,	O
the	O
formulation	O
of	O
l	Task
1	Task
,	O
described	O
in	O
Eq	O
.	O
By	O
introducing	O
the	O
assumption	O
that	O
W	O
n	O
p	O
=	O
θ	O
n	O
θ	O
nT	O
,	O
where	O
θ	O
n	O
∈	O
R	O
N	O
,	O
we	O
can	O
simplify	O
l	Method
1	Method
's	Method
formulation	Method
as	O
:	O
where	O
With	O
the	O
first	Method
order	Method
decomposition	Method
on	O
n	O
-	O
th	O
single	O
node	O
,	O
we	O
give	O
the	O
l	O
p	O
complete	O
form	O
:	O
By	O
reduction	O
of	O
l	O
p	O
in	O
Eq	O
.	O
(	O
12	O
)	O
,	O
the	O
space	Metric
complexity	Metric
of	O
l	O
1	O
becomes	O
O	O
(	O
D	O
1	O
M	O
N	O
)	O
,	O
and	O
the	O
time	Metric
complexity	Metric
is	O
also	O
O	O
(	O
D	O
1	O
M	O
N	O
)	O
.	O
More	O
generally	O
,	O
we	O
discuss	O
K	Task
-	Task
order	Task
decomposition	Task
of	Task
W	Task
n	Task
p	Task
at	O
the	O
end	O
of	O
this	O
section	O
.	O
We	O
should	O
point	O
out	O
that	O
W	O
n	O
p	O
=	O
θ	O
n	O
θ	O
T	O
n	O
is	O
only	O
the	O
first	Method
order	Method
decomposition	Method
with	O
a	O
strong	O
assumption	O
.	O
The	O
general	O
matrix	Method
decomposition	Method
method	Method
can	O
be	O
derived	O
that	O
:	O
In	O
this	O
case	O
,	O
θ	O
i	O
n	O
∈	O
R	O
K	O
.	O
This	O
general	O
decomposition	O
is	O
more	O
expressive	O
with	O
weaker	O
assumptions	O
,	O
but	O
also	O
leading	O
to	O
K	O
times	O
model	Metric
complexity	Metric
.	O
section	O
:	O
C.	O
Outer	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
Vector	Method
inner	Method
product	Method
takes	O
a	O
pair	O
of	O
vectors	O
as	O
input	O
and	O
outputs	O
a	O
scalar	O
.	O
Different	O
from	O
that	O
,	O
vector	Method
outer	Method
product	Method
takes	O
a	O
pair	O
of	O
vectors	O
and	O
produces	O
a	O
matrix	O
.	O
IPNN	Method
defines	O
feature	O
interaction	O
by	O
vector	Method
inner	Method
product	Method
,	O
while	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
Outer	Method
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
(	O
OPNN	Method
)	O
.	O
The	O
only	O
difference	O
between	O
IPNN	Method
and	O
OPNN	Method
is	O
the	O
quadratic	O
term	O
p.	O
In	O
OPNN	Method
,	O
we	O
define	O
feature	O
interaction	O
as	O
is	O
a	O
square	O
matrix	O
.	O
For	O
calculating	Task
l	Task
1	Task
,	O
the	O
space	Metric
complexity	Metric
is	O
O	O
(	O
D	O
1	O
M	O
2	O
N	O
2	O
)	O
,	O
and	O
the	O
time	Metric
complexity	Metric
is	O
also	O
O	O
(	O
D	O
1	O
M	O
2	O
N	O
2	O
)	O
.	O
Recall	O
that	O
D	O
1	O
and	O
M	O
are	O
the	O
hyper	O
-	O
parameters	O
of	O
the	O
network	Method
architecture	Method
,	O
and	O
N	O
is	O
the	O
number	O
of	O
the	O
input	O
fields	O
,	O
this	O
implementation	O
is	O
expensive	O
in	O
practice	O
.	O
To	O
reduce	O
the	O
complexity	Metric
,	O
we	O
propose	O
the	O
idea	O
of	O
superposition	Method
.	O
By	O
element	Method
-	Method
wise	Method
superposition	Method
,	O
we	O
can	O
reduce	O
the	O
complexity	Metric
by	O
a	O
large	O
step	O
.	O
Specifically	O
,	O
we	O
re	O
-	O
define	O
p	Method
formulation	Method
as	O
where	O
p	O
∈	O
R	O
M	O
×M	O
becomes	O
symmetric	O
,	O
thus	O
W	O
n	O
p	O
should	O
also	O
be	O
symmetric	O
.	O
Recall	O
Eq	O
.	O
(	O
5	O
)	O
that	O
W	O
p	O
∈	O
R	O
D1×M	O
×M	O
.	O
In	O
this	O
case	O
,	O
the	O
space	Metric
complexity	Metric
of	O
l	O
1	O
becomes	O
O	O
(	O
D	O
1	O
M	O
(	O
M	O
+	O
N	O
)	O
)	O
,	O
and	O
the	O
time	Metric
complexity	Metric
is	O
also	O
O	O
(	O
D	O
1	O
M	O
(	O
M	O
+	O
N	O
)	O
)	O
.	O
section	O
:	O
D.	O
Discussions	O
Compared	O
with	O
FNN	Method
[	O
reference	O
]	O
,	O
PNN	Method
has	O
a	O
product	Method
layer	Method
.	O
If	O
removing	O
l	O
p	O
part	O
of	O
the	O
product	Method
layer	Method
,	O
PNN	Method
is	O
identical	O
to	O
FNN	Method
.	O
With	O
the	O
inner	Method
product	Method
operator	Method
,	O
PNN	Method
is	O
quite	O
similar	O
with	O
FM	Method
[	O
reference	O
]	O
:	O
if	O
there	O
is	O
no	O
hidden	O
layer	O
and	O
the	O
output	O
layer	O
is	O
simply	O
summing	O
up	O
with	O
uniform	O
weight	O
,	O
PNN	Method
is	O
identical	O
to	O
FM	Method
.	O
Inspired	O
by	O
Net2Net	Method
[	O
reference	O
]	O
,	O
we	O
can	O
firstly	O
train	O
a	O
part	O
of	O
PNN	Method
(	O
e.g.	O
,	O
the	O
FNN	Method
or	Method
FM	Method
part	Method
)	O
as	O
the	O
initialization	O
,	O
and	O
then	O
start	O
to	O
let	O
the	O
back	Method
propagation	Method
go	O
over	O
the	O
whole	O
net	O
.	O
The	O
resulted	O
PNN	Method
should	O
at	O
least	O
be	O
as	O
good	O
as	O
FNN	Method
or	O
FM	Method
.	O
In	O
general	O
,	O
PNN	Method
uses	O
product	Method
layers	Method
to	O
explore	O
feature	O
interactions	O
.	O
Vector	O
products	O
can	O
be	O
viewed	O
as	O
a	O
series	O
of	O
addition	Method
/	Method
multiplication	Method
operations	Method
.	O
Inner	Method
product	Method
and	O
outer	Method
product	Method
are	O
just	O
two	O
implementations	O
.	O
In	O
fact	O
,	O
we	O
can	O
define	O
more	O
general	O
or	O
complicated	O
product	O
layers	O
,	O
gaining	O
PNN	Method
better	O
capability	O
in	O
exploration	Task
of	Task
feature	Task
interactions	Task
.	O
Analogous	O
to	O
electronic	O
circuit	O
,	O
addition	O
acts	O
like	O
"	O
OR	O
"	O
gate	O
while	O
multiplication	O
acting	O
like	O
"	O
AND	O
"	O
gate	O
,	O
and	O
the	O
product	Method
layer	Method
seems	O
to	O
learn	O
rules	O
other	O
than	O
features	O
.	O
Reviewing	O
the	O
scenario	O
of	O
computer	Task
vision	Task
,	O
while	O
pixels	O
in	O
images	O
are	O
real	O
-	O
world	O
raw	O
features	O
,	O
categorical	O
data	O
in	O
web	Task
applications	Task
are	O
artificial	O
features	O
with	O
high	O
levels	O
and	O
rich	O
meanings	O
.	O
Logic	Method
is	O
a	O
powerful	O
tool	O
in	O
dealing	O
with	O
concepts	O
,	O
domains	O
and	O
relationships	O
.	O
Thus	O
we	O
believe	O
that	O
introducing	O
product	Method
operations	Method
in	O
neural	Method
networks	Method
will	O
improve	O
networks	O
'	O
ability	O
for	O
modeling	Task
multi	Task
-	Task
field	Task
categorical	Task
data	Task
.	O
section	O
:	O
IV	O
.	O
EXPERIMENTS	O
In	O
this	O
section	O
,	O
we	O
present	O
our	O
experiments	O
in	O
detail	O
,	O
including	O
datasets	O
,	O
data	Task
processing	Task
,	O
experimental	O
setup	O
,	O
model	Task
comparison	Task
,	O
and	O
the	O
corresponding	O
analysis	O
[	O
reference	O
]	O
.	O
In	O
our	O
experiments	O
,	O
PNN	Method
models	O
outperform	O
major	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
the	O
CTR	Task
estimation	Task
task	Task
on	O
two	O
real	O
-	O
world	O
datasets	O
.	O
A.	O
Datasets	O
1	O
)	O
Criteo	Material
:	O
Criteo	Material
1	O
TB	Material
click	Material
log	Material
2	Material
is	O
a	O
famous	O
ad	O
tech	O
industry	O
benchmarking	O
dataset	O
.	O
We	O
select	O
7	O
consecutive	O
days	O
of	O
samples	O
for	O
training	O
,	O
and	O
the	O
next	O
1	O
day	O
for	O
evaluation	O
.	O
Because	O
of	O
the	O
enormous	O
data	O
volume	O
and	O
high	O
bias	O
,	O
we	O
apply	O
negative	Method
down	Method
-	Method
sampling	Method
on	O
this	O
dataset	O
.	O
Define	O
the	O
down	Metric
-	Metric
sampling	Metric
ratio	Metric
as	O
w	O
,	O
the	O
predicted	O
CTR	O
as	O
p	O
,	O
the	O
recalibrated	Metric
CTR	Metric
q	Metric
should	O
be	O
q	O
=	O
p	O
/(	O
p	O
+	O
1−p	O
w	O
)	O
[	O
reference	O
]	O
.	O
After	O
down	Method
-	Method
sampling	Method
and	O
feature	Method
mapping	Method
,	O
we	O
get	O
a	O
dataset	O
,	O
which	O
comprises	O
79.38	O
M	O
instances	O
with	O
1.64	O
M	O
feature	O
dimensions	O
.	O
2	O
)	O
iPinYou	Material
:	O
The	O
iPinYou	Material
dataset	Material
3	O
is	O
another	O
real	O
-	O
world	O
dataset	O
for	O
ad	O
click	O
logs	O
over	O
10	O
days	O
.	O
After	O
one	Method
-	Method
hot	Method
encoding	Method
,	O
we	O
get	O
a	O
dataset	O
containing	O
19.50	O
M	O
instances	O
with	O
937.67	O
K	O
input	O
dimensions	O
.	O
We	O
keep	O
the	O
original	O
train	Method
/	Method
test	Method
splitting	Method
scheme	Method
,	O
where	O
for	O
each	O
advertiser	O
the	O
last	O
3	O
-	O
day	O
data	O
are	O
used	O
as	O
the	O
test	O
dataset	O
while	O
the	O
rest	O
as	O
the	O
training	O
dataset	O
.	O
section	O
:	O
B.	O
Model	O
Comparison	O
We	O
compare	O
7	O
models	O
in	O
our	O
experiments	O
,	O
which	O
are	O
implemented	O
with	O
TensorFlow	Method
4	Method
,	O
and	O
trained	O
with	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
.	O
LR	Method
:	O
LR	Method
is	O
the	O
most	O
widely	O
used	O
linear	Method
model	Method
in	O
industrial	Task
applications	Task
[	O
reference	O
]	O
.	O
It	O
is	O
easy	O
to	O
implement	O
and	O
fast	O
to	O
train	O
,	O
however	O
,	O
unable	O
to	O
capture	O
non	O
-	O
linear	O
information	O
.	O
FM	Method
:	O
FM	Method
has	O
many	O
successful	O
applications	O
in	O
recommender	Task
systems	Task
and	O
user	Task
response	Task
prediction	Task
tasks	Task
[	O
reference	Method
]	Method
.	Method
FM	Method
explores	O
feature	O
interactions	O
,	O
which	O
is	O
effective	O
on	O
sparse	O
data	O
.	O
FNN	Method
:	O
FNN	Method
is	O
proposed	O
in	O
[	O
reference	O
]	O
,	O
being	O
able	O
to	O
capture	O
highorder	O
latent	O
patterns	O
of	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
CCPM	Method
:	O
CCPM	Method
is	O
a	O
convolutional	Method
model	Method
for	O
click	Task
prediction	Task
[	O
reference	O
]	O
.	O
This	O
model	O
learns	O
local	O
-	O
global	O
features	O
efficiently	O
.	O
However	O
,	O
CCPM	Method
highly	O
relies	O
on	O
feature	Method
alignment	Method
,	O
and	O
is	O
lack	O
of	O
interpretation	O
.	O
IPNN	Method
:	O
PNN	Method
with	O
inner	Method
product	Method
layer	Method
III	O
-	O
B.	O
OPNN	Method
:	O
PNN	Method
with	O
outer	Method
product	Method
layer	Method
III	O
-	O
C.	O
PNN	Method
*	O
:	O
This	O
model	O
has	O
a	O
product	Method
layer	Method
,	O
which	O
is	O
a	O
concatenation	Method
of	Method
inner	Method
product	Method
and	O
outer	Method
product	Method
.	O
Additionally	O
,	O
in	O
order	O
to	O
prevent	O
over	Task
-	Task
fitting	Task
,	O
the	O
popular	O
L2	Method
regularization	Method
term	Method
is	O
added	O
to	O
the	O
loss	O
function	O
L	O
(	O
y	O
,	O
ŷ	O
)	O
when	O
training	O
LR	Task
and	O
FM	Task
.	O
And	O
we	O
also	O
employ	O
dropout	Method
as	O
a	O
regularization	Method
method	Method
to	O
prevent	O
over	Task
-	Task
fitting	Task
when	O
training	O
neural	Method
networks	Method
.	O
section	O
:	O
C.	O
Evaluation	Metric
Metrics	Metric
Four	O
evaluation	Metric
metrics	Metric
are	O
tested	O
in	O
our	O
experiments	O
.	O
The	O
two	O
major	O
metrics	O
are	O
:	O
AUC	Metric
:	O
Area	Metric
under	Metric
ROC	Metric
curve	Metric
is	O
a	O
widely	O
used	O
metric	O
in	O
evaluating	Task
classification	Task
problems	Task
.	O
Besides	O
,	O
some	O
work	O
validates	O
AUC	Metric
as	O
a	O
good	O
measurement	O
in	O
CTR	Task
estimation	Task
[	O
reference	O
]	O
.	O
RIG	Metric
:	O
Relative	Metric
Information	Metric
Gain	Metric
,	O
RIG	O
=	O
1	O
−	O
N	O
E	O
,	O
where	O
NE	O
is	O
the	O
Normalized	Metric
Cross	Metric
Entropy	Metric
[	O
reference	O
]	O
.	O
Besides	O
,	O
we	O
also	O
employ	O
Log	Metric
Loss	Metric
(	O
Eq	O
.	O
(	O
9	O
)	O
)	O
and	O
root	Metric
mean	Metric
square	Metric
error	Metric
(	Metric
RMSE	Metric
)	O
as	O
our	O
additional	O
evaluation	Metric
metrics	Metric
.	O
Table	O
I	O
and	O
II	O
show	O
the	O
overall	O
performance	O
on	O
Criteo	Material
and	O
iPinYou	Material
datasets	O
,	O
respectively	O
.	O
In	O
FM	Task
,	O
we	O
employ	O
10	Method
-	Method
order	Method
factorization	Method
and	O
correspondingly	O
,	O
we	O
employ	O
10	Method
-	Method
order	Method
embedding	Method
in	O
network	Method
models	Method
.	O
CCPM	Method
has	O
1	O
embedding	Method
layer	Method
,	O
2	O
convolution	Method
layers	Method
(	O
with	O
max	Method
pooling	Method
)	O
and	O
1	O
hidden	Method
layer	Method
(	O
5	O
layers	O
in	O
total	O
)	O
.	O
FNN	Method
has	O
1	O
embedding	Method
layer	Method
and	O
3	O
hidden	O
layers	O
(	O
4	O
layers	O
in	O
total	O
)	O
.	O
Every	O
PNN	Method
has	O
1	O
embedding	Method
layer	Method
,	O
1	O
product	Method
layer	Method
and	O
3	O
hidden	O
layers	O
(	O
5	O
layers	O
in	O
total	O
)	O
.	O
The	O
impact	O
of	O
network	O
depth	O
will	O
be	O
discussed	O
later	O
.	O
section	O
:	O
D.	O
Performance	O
Comparison	O
The	O
LR	Method
and	Method
FM	Method
models	Method
are	O
trained	O
with	O
L2	Method
norm	Method
regularization	Method
,	O
while	O
FNN	Method
,	O
CCPM	Method
and	O
PNNs	Method
are	O
trained	O
with	O
dropout	Method
.	O
By	O
default	O
,	O
we	O
set	O
dropout	O
rate	O
at	O
0.5	O
on	O
network	O
hidden	O
layers	O
,	O
which	O
is	O
proved	O
effective	O
in	O
Figure	O
2	O
.	O
Further	O
discussions	O
about	O
the	O
network	Method
architecture	Method
will	O
be	O
provided	O
in	O
Section	O
IV	O
-	O
E.	O
Criteo	Material
AUC	Metric
Firstly	O
,	O
we	O
focus	O
on	O
the	O
AUC	Metric
performance	O
.	O
The	O
overall	O
results	O
in	O
Table	O
I	O
and	O
II	O
illustrate	O
that	O
(	O
i	O
)	O
FM	Method
outperforms	O
LR	Method
,	O
demonstrating	O
the	O
effectiveness	O
of	O
feature	O
interactions	O
;	O
(	O
ii	O
)	O
Neural	Method
networks	Method
outperform	O
LR	Method
and	O
FM	Method
,	O
which	O
validates	O
the	O
importance	O
of	O
high	O
-	O
order	O
latent	O
patterns	O
;	O
(	O
iii	O
)	O
PNNs	Method
perform	O
the	O
best	O
on	O
both	O
Criteo	Material
and	O
iPinYou	Material
datasets	O
.	O
As	O
for	O
log	Metric
loss	Metric
,	O
RMSE	Metric
and	O
RIG	Task
,	O
the	O
results	O
are	O
similar	O
.	O
We	O
also	O
conduct	O
t	O
-	O
test	O
between	O
our	O
proposed	O
PNNs	Method
and	O
the	O
other	O
compared	O
models	O
.	O
Table	O
III	O
shows	O
the	O
calculated	O
p	Metric
-	Metric
values	Metric
under	O
log	Metric
loss	Metric
metric	O
on	O
both	O
datasets	O
.	O
The	O
results	O
verify	O
that	O
our	O
models	O
significantly	O
improve	O
the	O
performance	O
of	O
user	Task
response	Task
prediction	Task
against	O
the	O
baseline	O
models	O
.	O
We	O
also	O
find	O
that	O
PNN	Method
*	O
,	O
which	O
is	O
the	O
combination	O
of	O
IPNN	Method
and	Method
OPNN	Method
,	O
has	O
no	O
obvious	O
advantages	O
over	O
IPNN	Method
and	O
OPNN	Method
on	O
AUC	Metric
performance	O
.	O
We	O
consider	O
that	O
IPNN	Method
and	O
OPNN	Method
are	O
sufficient	O
to	O
capture	O
the	O
feature	O
interactions	O
in	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
Figure	O
3	O
shows	O
the	O
AUC	Metric
performance	O
with	O
respect	O
to	O
the	O
training	O
iterations	O
on	O
iPinYou	Material
dataset	Material
.	O
We	O
find	O
that	O
network	Method
models	Method
converge	O
more	O
quickly	O
than	O
LR	Method
and	O
FM	Method
.	O
We	O
also	O
observe	O
that	O
our	O
two	O
proposed	O
PNNs	Method
have	O
better	O
convergence	O
than	O
other	O
network	Method
models	Method
.	O
section	O
:	O
E.	O
Ablation	O
Study	O
on	O
Network	Task
Architecture	Task
In	O
this	O
section	O
,	O
we	O
discuss	O
the	O
impact	O
of	O
neural	Method
network	Method
architecture	Method
.	O
For	O
IPNN	Method
and	O
OPNN	Method
,	O
we	O
take	O
three	O
hyperparameters	O
(	O
or	O
settings	O
)	O
into	O
consideration	O
:	O
(	O
i	O
)	O
embedding	O
layer	O
size	O
,	O
(	O
ii	O
)	O
network	O
depth	O
and	O
(	O
iii	O
)	O
activation	O
function	O
.	O
Since	O
CCPM	Method
shares	O
few	O
similarities	O
with	O
other	O
neural	Method
networks	Method
and	O
PNN	Method
*	O
is	O
just	O
a	O
combination	O
of	O
IPNN	Method
and	O
OPNN	Method
,	O
we	O
only	O
compare	O
FNN	Method
,	O
IPNN	Method
and	O
OPNN	Method
in	O
this	O
section	O
.	O
section	O
:	O
1	O
)	O
Embedding	Method
Layer	Method
:	O
The	O
embedding	Method
layer	Method
is	O
to	O
convert	O
sparse	O
binary	O
inputs	O
to	O
dense	O
real	O
-	O
value	O
vectors	O
.	O
Take	O
word	Method
embedding	Method
as	O
an	O
example	O
[	O
reference	O
]	O
,	O
an	O
embedding	O
vector	O
contains	O
the	O
information	O
of	O
the	O
word	O
and	O
its	O
context	O
,	O
and	O
indicates	O
the	O
relationships	O
between	O
words	O
.	O
We	O
take	O
the	O
idea	O
of	O
embedding	Method
layer	Method
from	O
[	O
reference	O
]	O
.	O
In	O
this	O
paper	O
,	O
the	O
latent	O
vectors	O
learned	O
by	O
FM	Method
are	O
explained	O
as	O
node	Method
representations	Method
,	O
and	O
the	O
authors	O
use	O
a	O
pre	O
-	O
trained	O
FM	Method
to	O
initialize	O
the	O
embedding	Method
layers	Method
in	O
FNN	Method
.	O
Thus	O
the	O
factorization	O
order	O
of	O
FM	Method
keeps	O
consistent	O
with	O
the	O
embedding	O
order	O
.	O
The	O
input	O
units	O
are	O
fully	O
connected	O
with	O
the	O
embedding	Method
layer	Method
within	O
each	O
field	O
.	O
We	O
compare	O
different	O
orders	O
,	O
like	O
2	O
,	O
10	O
,	O
50	O
and	O
100	O
.	O
However	O
,	O
when	O
the	O
order	O
grows	O
larger	O
,	O
it	O
is	O
harder	O
to	O
fit	O
the	O
parameters	O
in	O
memory	O
,	O
and	O
the	O
models	O
are	O
much	O
easier	O
to	O
over	O
-	O
fit	O
.	O
In	O
our	O
experiments	O
,	O
we	O
take	O
10	Method
-	Method
order	Method
embedding	Method
in	O
neural	Method
networks	Method
.	O
2	O
)	O
Network	O
Depth	O
:	O
We	O
also	O
explore	O
the	O
impact	O
of	O
network	O
depth	O
by	O
adjusting	O
the	O
number	O
of	O
hidden	O
layers	O
in	O
FNN	Method
and	O
PNNs	Method
.	O
We	O
compare	O
different	O
number	O
of	O
hidden	O
layers	O
:	O
1	O
,	O
3	O
,	O
5	O
and	O
7	O
.	O
Figure	O
4	O
shows	O
the	O
performance	O
as	O
network	O
depth	O
grows	O
.	O
Generally	O
speaking	O
,	O
the	O
networks	O
with	O
3	O
hidden	O
layers	O
have	O
better	O
generalization	Task
on	O
the	O
test	O
set	O
.	O
For	O
convenience	O
,	O
we	O
call	O
convolution	Method
layers	Method
and	O
product	Method
layers	Method
as	O
representation	Method
layers	Method
.	O
These	O
layers	O
can	O
capture	O
complex	O
feature	O
patterns	O
using	O
fewer	O
parameters	O
,	O
thus	O
are	O
efficient	O
in	O
training	O
,	O
and	O
generalize	O
better	O
on	O
the	O
test	O
set	O
.	O
1	O
+	O
e	O
−2x	O
,	O
and	O
relu	O
(	O
x	O
)	O
=	O
max	O
(	O
0	O
,	O
x	O
)	O
.	O
Compared	O
with	O
the	O
sigmoidal	Method
family	Method
,	O
relu	Method
function	Method
has	O
the	O
advantages	O
of	O
sparsity	O
and	O
efficient	O
gradient	O
,	O
which	O
is	O
possible	O
to	O
gain	O
more	O
benefits	O
on	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
Figure	O
5	O
compares	O
these	O
activation	Method
functions	Method
on	O
FNN	Method
,	O
IPNN	Method
and	O
OPNN	Method
.	O
From	O
this	O
figure	O
,	O
we	O
find	O
that	O
tanh	Method
has	O
better	O
performance	O
than	O
sigmoid	Method
.	O
This	O
is	O
supported	O
by	O
[	O
reference	O
]	O
.	O
Besides	O
tanh	Method
,	O
we	O
find	O
relu	Method
function	Method
also	O
has	O
good	O
performance	O
.	O
Possible	O
reasons	O
include	O
:	O
(	O
i	O
)	O
Sparse	O
activation	O
,	O
nodes	O
with	O
negative	O
outputs	O
will	O
not	O
be	O
activated	O
;	O
(	O
ii	O
)	O
Efficient	O
gradient	Method
propagation	Method
,	O
no	O
vanishing	O
gradient	O
problem	O
or	O
exploding	O
effect	O
;	O
(	O
iii	O
)	O
Efficient	O
computation	Task
,	O
only	O
comparison	Task
,	O
addition	Method
and	O
multiplication	Task
.	O
section	O
:	O
V.	O
CONCLUSION	O
AND	O
FUTURE	O
WORK	O
In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
deep	Method
neural	Method
network	Method
model	Method
with	O
novel	O
architecture	O
,	O
namely	O
Product	Method
-	Method
based	Method
Neural	Method
Network	Method
,	O
to	O
improve	O
the	O
prediction	Task
performance	O
of	O
DNN	Method
working	O
on	O
categorical	O
data	O
.	O
And	O
we	O
chose	O
CTR	Task
estimation	Task
as	O
our	O
working	O
example	O
.	O
By	O
exploration	O
of	O
feature	O
interactions	O
,	O
PNN	Method
is	O
promising	O
to	O
learn	O
high	O
-	O
order	O
latent	O
patterns	O
on	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
We	O
designed	O
two	O
types	O
of	O
PNN	Method
:	O
IPNN	Method
based	O
on	O
inner	Method
product	Method
and	O
OPNN	Method
based	O
on	O
outer	Method
product	Method
.	O
We	O
also	O
discussed	O
solutions	O
to	O
reduce	O
complexity	Metric
,	O
making	O
PNN	Method
efficient	O
and	O
scalable	O
.	O
Our	O
experimental	O
results	O
demonstrated	O
that	O
PNN	Method
outperformed	O
the	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
4	O
metrics	O
on	O
2	O
datasets	O
.	O
To	O
sum	O
up	O
,	O
we	O
obtain	O
the	O
following	O
conclusions	O
:	O
(	O
i	O
)	O
By	O
investigating	O
feature	O
interactions	O
,	O
PNN	Method
gains	O
better	O
capacity	O
on	O
multi	O
-	O
field	O
categorical	O
data	O
.	O
(	O
ii	O
)	O
Being	O
both	O
efficient	O
and	O
effective	O
,	O
PNN	Method
outperforms	O
major	O
stateof	O
-	O
the	O
-	O
art	O
models	O
.	O
(	O
iii	O
)	O
Analogous	O
to	O
"	O
AND"	O
/	O
"OR	O
"	O
gates	O
,	O
the	O
product	Method
/	Method
add	Method
operations	Method
in	O
PNN	Method
provide	O
a	O
potential	O
strategy	O
for	O
data	Task
representation	Task
,	O
more	O
specifically	O
,	O
rule	Task
representation	Task
.	O
In	O
the	O
future	O
work	O
,	O
we	O
will	O
explore	O
PNN	Method
with	O
more	O
general	O
and	O
complicated	O
product	O
layers	O
.	O
Besides	O
,	O
we	O
are	O
interested	O
in	O
explaining	O
and	O
visualizing	O
the	O
feature	O
vectors	O
learned	O
by	O
our	O
models	O
.	O
We	O
will	O
investigate	O
their	O
properties	O
,	O
and	O
further	O
apply	O
these	O
node	Method
representations	Method
to	O
other	O
tasks	O
.	O
section	O
:	O
