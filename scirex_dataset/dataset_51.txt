document	O
:	O
M	Method
-	Method
Walk	Method
:	O
Learning	O
to	O
Walk	Task
over	Task
Graphs	Task
using	O
Monte	Method
Carlo	Method
Tree	Method
Search	Method
Learning	O
to	O
walk	O
over	O
a	O
graph	O
towards	O
a	O
target	O
node	O
for	O
a	O
given	O
query	O
and	O
a	O
source	O
node	O
is	O
an	O
important	O
problem	O
in	O
applications	O
such	O
as	O
knowledge	Task
base	Task
completion	Task
(	O
KBC	Task
)	Task
.	O
It	O
can	O
be	O
formulated	O
as	O
a	O
reinforcement	Method
learning	Method
(	Method
RL	Method
)	Method
problem	Method
with	O
a	O
known	Method
state	Method
transition	Method
model	Method
.	O
To	O
overcome	O
the	O
challenge	O
of	O
sparse	O
rewards	O
,	O
we	O
develop	O
a	O
graph	Method
-	Method
walking	Method
agent	Method
called	O
M	Method
-	Method
Walk	Method
,	O
which	O
consists	O
of	O
a	O
deep	Method
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
and	O
Monte	Method
Carlo	Method
Tree	Method
Search	Method
(	O
MCTS	Method
)	O
.	O
The	O
RNN	Method
encodes	O
the	O
state	O
(	O
i.e.	O
,	O
history	O
of	O
the	O
walked	O
path	O
)	O
and	O
maps	O
it	O
separately	O
to	O
a	O
policy	O
and	O
Q	O
-	O
values	O
.	O
In	O
order	O
to	O
effectively	O
train	O
the	O
agent	O
from	O
sparse	O
rewards	O
,	O
we	O
combine	O
MCTS	Method
with	O
the	O
neural	Method
policy	Method
to	O
generate	O
trajectories	O
yielding	O
more	O
positive	O
rewards	O
.	O
From	O
these	O
trajectories	O
,	O
the	O
network	O
is	O
improved	O
in	O
an	O
off	Method
-	Method
policy	Method
manner	Method
using	O
Q	Method
-	Method
learning	Method
,	O
which	O
modifies	O
the	O
RNN	Method
policy	Method
via	O
parameter	Method
sharing	Method
.	O
Our	O
proposed	O
RL	Method
algorithm	Method
repeatedly	O
applies	O
this	O
policy	Method
-	Method
improvement	Method
step	Method
to	O
learn	O
the	O
model	O
.	O
At	O
test	O
time	O
,	O
MCTS	Method
is	O
combined	O
with	O
the	O
neural	Method
policy	Method
to	O
predict	O
the	O
target	O
node	O
.	O
Experimental	O
results	O
on	O
several	O
graph	Method
-	Method
walking	Method
benchmarks	Method
show	O
that	O
M	Method
-	Method
Walk	Method
is	O
able	O
to	O
learn	O
better	O
policies	O
than	O
other	O
RL	Method
-	Method
based	Method
methods	Method
,	O
which	O
are	O
mainly	O
based	O
on	O
policy	O
gradients	O
.	O
M	Method
-	Method
Walk	Method
also	O
outperforms	O
traditional	O
KBC	Method
baselines	Method
.	O
section	O
:	O
Introduction	O
We	O
consider	O
the	O
problem	O
of	O
learning	Task
to	O
walk	O
over	O
a	O
graph	O
in	O
order	O
to	O
find	O
a	O
target	O
node	O
for	O
a	O
given	O
source	O
node	O
and	O
a	O
query	O
.	O
Such	O
problems	O
appear	O
in	O
,	O
for	O
example	O
,	O
knowledge	Task
base	Task
completion	Task
(	O
KBC	Task
)	O
.	O
A	O
knowledge	Method
graph	Method
is	O
a	O
structured	O
representation	O
of	O
world	O
knowledge	O
in	O
the	O
form	O
of	O
entities	O
and	O
their	O
relations	O
(	O
e.g.	O
,	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
and	O
has	O
a	O
wide	O
range	O
of	O
downstream	Task
applications	Task
such	O
as	O
question	Task
answering	Task
.	O
Although	O
a	O
typical	O
knowledge	O
graph	O
may	O
contain	O
millions	O
of	O
entities	O
and	O
billions	O
of	O
relations	O
,	O
it	O
is	O
usually	O
far	O
from	O
complete	O
.	O
KBC	Method
aims	O
to	O
predict	O
the	O
missing	O
relations	O
between	O
entities	O
using	O
information	O
from	O
the	O
existing	O
knowledge	O
graph	O
.	O
More	O
formally	O
,	O
let	O
denote	O
a	O
graph	O
,	O
which	O
consists	O
of	O
a	O
set	O
of	O
nodes	O
,	O
,	O
and	O
a	O
set	O
of	O
edges	O
,	O
,	O
that	O
connect	O
the	O
nodes	O
,	O
and	O
let	O
denote	O
an	O
input	O
query	O
.	O
The	O
problem	O
is	O
stated	O
as	O
using	O
the	O
graph	O
,	O
the	O
source	O
node	O
and	O
the	O
query	O
as	O
inputs	O
to	O
predict	O
the	O
target	O
node	O
.	O
In	O
KBC	Task
tasks	Task
,	O
is	O
a	O
given	O
knowledge	O
graph	O
,	O
is	O
a	O
collection	O
of	O
entities	O
(	O
nodes	O
)	O
,	O
and	O
is	O
a	O
set	O
of	O
relations	O
(	O
edges	O
)	O
that	O
connect	O
the	O
entities	O
.	O
In	O
the	O
example	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
objective	O
of	O
KBC	Method
is	O
to	O
identify	O
the	O
target	O
node	O
=	O
USA	O
for	O
the	O
given	O
head	O
entity	O
=	O
Obama	O
and	O
the	O
given	O
query	O
=	O
Citizenship	O
.	O
The	O
problem	O
can	O
also	O
be	O
understood	O
as	O
constructing	O
a	O
function	O
to	O
predict	O
,	O
where	O
the	O
functional	O
form	O
of	O
is	O
generally	O
unknown	O
and	O
has	O
to	O
be	O
learned	O
from	O
a	O
training	O
dataset	O
consisting	O
of	O
samples	O
like	O
.	O
In	O
this	O
work	O
,	O
we	O
model	O
by	O
means	O
of	O
a	O
graph	Method
-	Method
walking	Method
agent	Method
that	O
intelligently	O
navigates	O
through	O
a	O
subset	O
of	O
nodes	O
in	O
the	O
graph	O
from	O
towards	O
.	O
Since	O
is	O
unknown	O
,	O
the	O
problem	O
can	O
not	O
be	O
solved	O
by	O
conventional	O
search	Method
algorithms	Method
such	O
as	O
-	Method
search	Method
,	O
which	O
seeks	O
to	O
find	O
paths	O
between	O
the	O
given	O
source	O
and	O
target	O
nodes	O
.	O
Instead	O
,	O
the	O
agent	O
needs	O
to	O
learn	O
its	O
search	Method
policy	Method
from	O
the	O
training	O
dataset	O
so	O
that	O
,	O
after	O
training	O
is	O
complete	O
,	O
the	O
agent	O
knows	O
how	O
to	O
walk	O
over	O
the	O
graph	O
to	O
reach	O
the	O
correct	O
target	O
node	O
for	O
an	O
unseen	O
pair	O
of	O
.	O
Moreover	O
,	O
each	O
training	O
sample	O
is	O
in	O
the	O
form	O
of	O
‘	O
‘	O
(	O
source	O
node	O
,	O
query	O
,	O
target	O
node	O
)	O
’	O
’	O
,	O
and	O
there	O
is	O
no	O
intermediate	O
supervision	O
for	O
the	O
correct	O
search	O
path	O
.	O
Instead	O
,	O
the	O
agent	O
receives	O
only	O
delayed	O
evaluative	O
feedback	O
:	O
when	O
the	O
agent	O
correctly	O
(	O
or	O
incorrectly	O
)	O
predicts	O
the	O
target	O
node	O
in	O
the	O
training	O
set	O
,	O
the	O
agent	O
will	O
receive	O
a	O
positive	O
(	O
or	O
zero	O
)	O
reward	O
.	O
For	O
this	O
reason	O
,	O
we	O
formulate	O
the	O
problem	O
as	O
a	O
Markov	Method
decision	Method
process	Method
(	O
MDP	Method
)	O
and	O
train	O
the	O
agent	O
by	O
reinforcement	Method
learning	Method
(	O
RL	Method
)	O
.	O
The	O
problem	O
poses	O
two	O
major	O
challenges	O
.	O
Firstly	O
,	O
since	O
the	O
state	O
of	O
the	O
MDP	Method
is	O
the	O
entire	O
trajectory	O
,	O
reaching	O
a	O
correct	O
decision	O
usually	O
requires	O
not	O
just	O
the	O
query	O
,	O
but	O
also	O
the	O
entire	O
history	O
of	O
traversed	O
nodes	O
.	O
For	O
the	O
KBC	O
example	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
having	O
access	O
to	O
the	O
current	O
node	O
=	O
Hawaii	O
alone	O
is	O
not	O
sufficient	O
to	O
know	O
that	O
the	O
best	O
action	O
is	O
moving	O
to	O
=	O
USA	O
.	O
Instead	O
,	O
the	O
agent	O
must	O
track	O
the	O
entire	O
history	O
,	O
including	O
the	O
input	O
query	O
=	O
Citizenship	O
,	O
to	O
reach	O
this	O
decision	O
.	O
Secondly	O
,	O
the	O
reward	O
is	O
sparse	O
,	O
being	O
received	O
only	O
at	O
the	O
end	O
of	O
a	O
search	O
path	O
,	O
for	O
instance	O
,	O
after	O
correctly	O
predicting	O
=	O
USA	O
.	O
In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
neural	Method
graph	Method
-	Method
walking	Method
agent	Method
,	O
named	O
M	Method
-	Method
Walk	Method
,	O
that	O
effectively	O
addresses	O
these	O
two	O
challenges	O
.	O
First	O
,	O
M	Method
-	Method
Walk	Method
uses	O
a	O
novel	O
recurrent	Method
neural	Method
network	Method
(	Method
RNN	Method
)	Method
architecture	Method
to	O
encode	O
the	O
entire	O
history	O
of	O
the	O
trajectory	O
into	O
a	O
vector	Method
representation	Method
,	O
which	O
is	O
further	O
used	O
to	O
model	O
the	O
policy	Method
and	O
the	O
Q	Method
-	Method
function	Method
.	O
Second	O
,	O
to	O
address	O
the	O
challenge	O
of	O
sparse	Task
rewards	Task
,	O
M	Method
-	Method
Walk	Method
exploits	O
the	O
fact	O
that	O
the	O
MDP	Method
transition	Method
model	Method
is	O
known	O
and	O
deterministic	O
.	O
Specifically	O
,	O
it	O
combines	O
Monte	Method
Carlo	Method
Tree	Method
Search	Method
(	O
MCTS	Method
)	O
with	O
the	O
RNN	Method
to	O
generate	O
trajectories	O
that	O
obtain	O
significantly	O
more	O
positive	O
rewards	O
than	O
using	O
the	O
RNN	Method
policy	Method
alone	O
.	O
These	O
trajectories	O
can	O
be	O
viewed	O
as	O
being	O
generated	O
from	O
an	O
improved	O
version	O
of	O
the	O
RNN	Method
policy	Method
.	O
But	O
while	O
these	O
trajectories	O
can	O
improve	O
the	O
RNN	Method
policy	Method
,	O
their	O
off	Method
-	Method
policy	Method
nature	Method
prevents	O
them	O
from	O
being	O
leveraged	O
by	O
policy	Method
gradient	Method
RL	Method
methods	Method
.	O
To	O
solve	O
this	O
problem	O
,	O
we	O
design	O
a	O
structure	O
for	O
sharing	O
parameters	O
between	O
the	O
Q	Method
-	Method
value	Method
network	Method
and	O
the	O
RNN	Method
’s	Method
policy	Method
network	Method
.	O
This	O
allows	O
the	O
policy	Method
network	Method
to	O
be	O
indirectly	O
improved	O
through	O
Q	Method
-	Method
learning	Method
over	O
the	O
off	O
-	O
policy	O
trajectories	O
.	O
Our	O
method	O
is	O
in	O
sharp	O
contrast	O
to	O
existing	O
RL	Method
-	Method
based	Method
methods	Method
for	O
KBC	Method
,	O
which	O
use	O
a	O
policy	Method
gradients	Method
(	Method
REINFORCE	Method
)	Method
method	Method
and	O
usually	O
require	O
a	O
large	O
number	O
of	O
rollouts	O
to	O
obtain	O
a	O
trajectory	O
with	O
a	O
positive	O
reward	O
,	O
especially	O
in	O
the	O
early	O
stages	O
of	O
learning	Task
.	O
Experimental	O
results	O
on	O
several	O
benchmarks	O
,	O
including	O
a	O
synthetic	Task
task	Task
and	O
several	O
real	Task
-	Task
world	Task
KBC	Task
tasks	Task
,	O
show	O
that	O
our	O
approach	O
learns	O
better	O
policies	O
than	O
previous	O
RL	Method
-	Method
based	Method
methods	Method
and	O
traditional	O
KBC	Method
methods	Method
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
Section	O
[	O
reference	O
]	O
develops	O
the	O
M	Method
-	Method
Walk	Method
agent	O
,	O
including	O
the	O
model	Method
architecture	Method
,	O
the	O
training	Method
and	Method
testing	Method
algorithms	Method
.	O
Experimental	O
results	O
are	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Finally	O
,	O
we	O
discuss	O
related	O
work	O
in	O
Section	O
[	O
reference	O
]	O
and	O
conclude	O
the	O
paper	O
in	O
Section	O
[	O
reference	O
]	O
.	O
section	O
:	O
Graph	Task
Walking	Task
as	O
a	O
Markov	Method
Decision	Method
Process	Method
In	O
this	O
section	O
,	O
we	O
formulate	O
the	O
graph	Task
-	Task
walking	Task
problem	Task
as	O
a	O
Markov	Method
Decision	Method
Process	Method
(	O
MDP	Method
)	Method
,	O
which	O
is	O
defined	O
by	O
the	O
tuple	O
,	O
where	O
is	O
the	O
set	O
of	O
states	O
,	O
is	O
the	O
set	O
of	O
actions	O
,	O
is	O
the	O
reward	O
function	O
,	O
and	O
is	O
the	O
state	O
transition	O
probability	O
.	O
We	O
further	O
define	O
,	O
,	O
and	O
below	O
.	O
Figure	O
[	O
reference	O
]	O
illustrates	O
the	O
MDP	Method
corresponding	O
to	O
the	O
KBC	O
example	O
of	O
Figure	O
[	O
reference	O
]	O
.	O
Let	O
denote	O
the	O
state	O
at	O
time	O
.	O
Recalling	O
that	O
the	O
agent	O
needs	O
the	O
entire	O
history	O
of	O
traversed	O
nodes	O
and	O
the	O
query	O
to	O
make	O
a	O
correct	O
decision	O
,	O
we	O
define	O
by	O
the	O
following	O
recursion	O
:	O
where	O
denotes	O
the	O
action	O
selected	O
by	O
the	O
agent	O
at	O
time	O
,	O
denotes	O
the	O
currently	O
visited	O
node	O
at	O
time	O
,	O
is	O
the	O
set	O
of	O
all	O
edges	O
connected	O
to	O
,	O
and	O
is	O
the	O
set	O
of	O
all	O
nodes	O
connected	O
to	O
(	O
i.e.	O
,	O
the	O
neighborhood	O
)	O
.	O
Note	O
that	O
state	O
is	O
a	O
collection	O
of	O
(	O
i	O
)	O
all	O
the	O
traversed	O
nodes	O
(	O
along	O
with	O
their	O
edges	O
and	O
neighborhoods	O
)	O
up	O
to	O
time	O
,	O
(	O
ii	O
)	O
all	O
the	O
previously	O
selected	O
(	O
up	O
to	O
time	O
)	O
actions	O
,	O
and	O
(	O
iii	O
)	O
the	O
initial	O
query	O
.	O
The	O
set	O
consists	O
of	O
all	O
the	O
possible	O
values	O
of	O
.	O
Based	O
on	O
,	O
the	O
agent	O
takes	O
one	O
of	O
the	O
following	O
actions	O
at	O
each	O
time	O
:	O
(	O
i	O
)	O
choosing	O
an	O
edge	O
in	O
and	O
moving	O
to	O
the	O
next	O
node	O
,	O
or	O
(	O
ii	O
)	O
terminating	O
the	O
walk	O
(	O
denoted	O
as	O
the	O
‘	O
‘	O
STOP	O
’	O
’	O
action	O
)	O
.	O
Once	O
the	O
STOP	O
action	O
is	O
selected	O
,	O
the	O
MDP	Method
reaches	O
the	O
terminal	O
state	O
and	O
outputs	O
as	O
a	O
prediction	O
of	O
the	O
target	O
node	O
.	O
Therefore	O
,	O
we	O
define	O
the	O
set	O
of	O
feasible	O
actions	O
at	O
time	O
as	O
,	O
which	O
is	O
usually	O
time	O
-	O
varying	O
.	O
The	O
entire	O
action	O
space	O
is	O
the	O
union	O
of	O
all	O
,	O
i.e.	O
,	O
.	O
Recall	O
that	O
the	O
training	O
set	O
consists	O
of	O
samples	O
in	O
the	O
form	O
of	O
.	O
The	O
reward	O
is	O
defined	O
to	O
be	O
when	O
the	O
predicted	O
target	O
node	O
is	O
the	O
same	O
as	O
(	O
i.e.	O
,	O
)	O
,	O
and	O
zero	O
otherwise	O
.	O
In	O
the	O
example	O
of	O
Figure	O
[	O
reference	O
]	O
,	O
for	O
a	O
training	O
sample	O
(	O
Obama	O
,	O
Citizenship	O
,	O
USA	O
)	O
,	O
if	O
the	O
agent	O
successfully	O
navigates	O
from	O
Obama	O
to	O
USA	O
and	O
correctly	O
predicts	O
=	O
USA	O
,	O
the	O
reward	O
is	O
.	O
Otherwise	O
,	O
it	O
will	O
be	O
.	O
The	O
rewards	O
are	O
sparse	O
because	O
positive	O
reward	O
can	O
be	O
received	O
only	O
at	O
the	O
end	O
of	O
a	O
correct	O
path	O
.	O
Furthermore	O
,	O
since	O
the	O
graph	O
is	O
known	O
and	O
static	O
,	O
the	O
MDP	O
transition	O
probability	O
is	O
known	O
and	O
deterministic	O
,	O
and	O
is	O
defined	O
by	O
(	O
[	O
reference	O
]	O
)	O
.	O
To	O
see	O
this	O
,	O
we	O
observe	O
from	O
Figure	O
[	O
reference	O
]	O
that	O
once	O
an	O
action	O
(	O
i.e.	O
,	O
an	O
edge	O
in	O
or	O
‘	O
‘	O
STOP	O
’	O
’	O
)	O
is	O
selected	O
,	O
the	O
next	O
node	O
and	O
its	O
associated	O
and	O
are	O
known	O
.	O
By	O
(	O
[	O
reference	O
]	O
)	O
(	O
with	O
replaced	O
by	O
)	O
,	O
this	O
means	O
that	O
the	O
next	O
state	O
is	O
determined	O
.	O
This	O
important	O
(	O
model	O
-	O
based	O
)	O
knowledge	O
will	O
be	O
exploited	O
to	O
overcome	O
the	O
sparse	Task
-	Task
reward	Task
problem	Task
using	O
MCTS	Method
and	O
significantly	O
improve	O
the	O
performance	O
of	O
our	O
method	O
(	O
see	O
Sections	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
below	O
)	O
.	O
We	O
further	O
define	O
and	O
to	O
be	O
the	O
policy	O
and	O
the	O
Q	O
-	O
function	O
,	O
respectively	O
,	O
where	O
is	O
a	O
set	O
of	O
model	O
parameters	O
.	O
The	O
policy	Method
denotes	O
the	O
probability	O
of	O
taking	O
action	O
given	O
the	O
current	O
state	O
.	O
In	O
M	Method
-	Method
Walk	Method
,	O
it	O
is	O
used	O
as	O
a	O
prior	O
to	O
bias	O
the	O
MCTS	Method
search	Method
.	O
And	O
defines	O
the	O
long	O
-	O
term	O
reward	O
of	O
taking	O
action	O
at	O
state	O
and	O
then	O
following	O
the	O
optimal	Method
policy	Method
thereafter	O
.	O
The	O
objective	O
is	O
to	O
learn	O
a	O
policy	Method
that	O
maximizes	O
the	O
terminal	O
rewards	O
,	O
i.e.	O
,	O
correctly	O
identifies	O
the	O
target	O
node	O
with	O
high	O
probability	O
.	O
We	O
now	O
proceed	O
to	O
explain	O
how	O
to	O
model	O
and	O
jointly	O
learn	O
and	O
to	O
achieve	O
this	O
objective	O
.	O
section	O
:	O
The	O
M	Method
-	Method
Walk	Method
Agent	O
In	O
this	O
section	O
,	O
we	O
develop	O
a	O
neural	Method
graph	Method
-	Method
walking	Method
agent	Method
named	O
M	Method
-	Method
Walk	Method
(	O
i.e.	O
,	O
MCTS	Method
for	O
graph	Task
Walking	Task
)	O
,	O
which	O
consists	O
of	O
(	O
i	O
)	O
a	O
novel	O
neural	Method
architecture	Method
for	O
jointly	Task
modeling	Task
and	O
,	O
and	O
(	O
ii	O
)	O
Monte	Method
Carlo	Method
Tree	Method
Search	Method
(	O
MCTS	Method
)	O
.	O
We	O
first	O
introduce	O
the	O
overall	O
neural	Method
architecture	Method
and	O
then	O
explain	O
how	O
MCTS	Method
is	O
used	O
during	O
the	O
training	Task
and	Task
testing	Task
stages	Task
.	O
Finally	O
,	O
we	O
describe	O
some	O
further	O
details	O
of	O
the	O
neural	Method
architecture	Method
.	O
Our	O
discussion	O
focuses	O
on	O
addressing	O
the	O
two	O
challenges	O
described	O
earlier	O
:	O
history	O
-	O
dependent	O
state	O
and	O
sparse	O
rewards	O
.	O
subsection	O
:	O
The	O
neural	Method
architecture	Method
for	O
jointly	Task
modeling	Task
and	O
Recall	O
from	O
Section	O
[	O
reference	O
]	O
(	O
e.g.	O
,	O
(	O
[	O
reference	O
]	O
)	O
)	O
that	O
one	O
challenge	O
in	O
applying	O
RL	Method
to	O
the	O
graph	Task
-	Task
walking	Task
problem	Task
is	O
that	O
the	O
state	O
nominally	O
includes	O
the	O
entire	O
history	O
of	O
observations	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
special	O
RNN	Method
encoding	O
the	O
state	O
at	O
each	O
time	O
into	O
a	O
vector	Method
representation	Method
,	O
,	O
where	O
is	O
the	O
associated	O
model	O
parameter	O
.	O
We	O
defer	O
the	O
discussion	O
of	O
this	O
RNN	Method
state	Method
encoder	Method
to	O
Section	O
[	O
reference	O
]	O
,	O
and	O
focus	O
in	O
this	O
section	O
on	O
how	O
to	O
use	O
to	O
jointly	O
model	O
and	O
.	O
Specifically	O
,	O
the	O
vector	O
consists	O
of	O
several	O
sub	O
-	O
vectors	O
of	O
the	O
same	O
dimension	O
:	O
,	O
and	O
.	O
Each	O
sub	O
-	O
vector	O
encodes	O
part	O
of	O
the	O
state	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O
For	O
instance	O
,	O
the	O
vector	O
encodes	O
,	O
which	O
characterizes	O
the	O
history	O
in	O
the	O
state	O
.	O
The	O
vector	O
encodes	O
the	O
(	O
neighboring	O
)	O
node	O
and	O
the	O
edge	O
connected	O
to	O
,	O
which	O
can	O
be	O
viewed	O
as	O
a	O
vector	Method
representation	Method
of	O
the	O
-	O
th	O
candidate	O
action	O
(	O
excluding	O
the	O
STOP	O
action	O
)	O
.	O
And	O
the	O
vector	O
is	O
a	O
vector	Task
summarization	Task
of	O
and	O
,	O
which	O
is	O
used	O
to	O
model	O
the	O
STOP	O
action	O
probability	O
.	O
In	O
summary	O
,	O
we	O
use	O
the	O
sub	O
-	O
vectors	O
to	O
model	O
and	O
according	O
to	O
:	O
where	O
denotes	O
inner	O
product	O
,	O
is	O
a	O
fully	Method
-	Method
connected	Method
neural	Method
network	Method
with	O
model	O
parameter	O
,	O
denotes	O
the	O
element	Method
-	Method
wise	Method
sigmoid	Method
function	Method
,	O
and	O
is	O
the	O
softmax	O
function	O
with	O
temperature	O
parameter	O
.	O
Note	O
that	O
we	O
use	O
the	O
inner	O
product	O
between	O
the	O
vectors	O
and	O
to	O
compute	O
the	O
(	O
pre	O
-	O
softmax	O
)	O
score	O
for	O
choosing	O
the	O
-	O
th	O
candidate	O
action	O
,	O
where	O
.	O
The	O
inner	Method
product	Method
operation	Method
has	O
been	O
shown	O
to	O
be	O
useful	O
in	O
modeling	O
Q	Task
-	Task
functions	Task
when	O
the	O
candidate	O
actions	O
are	O
described	O
by	O
vector	Method
representations	Method
and	O
in	O
solving	O
other	O
problems	O
.	O
Moreover	O
,	O
the	O
value	O
of	O
is	O
computed	O
by	O
using	O
and	O
,	O
where	O
gives	O
the	O
(	O
pre	O
-	O
softmax	O
)	O
score	O
for	O
choosing	O
the	O
STOP	O
action	O
.	O
We	O
model	O
the	O
Q	O
-	O
function	O
by	O
applying	O
element	Method
-	Method
wise	Method
sigmoid	Method
to	O
,	O
and	O
we	O
model	O
the	O
policy	O
by	O
applying	O
the	O
softmax	Method
operation	Method
to	O
the	O
same	O
set	O
of	O
.	O
Note	O
that	O
the	O
policy	Method
network	Method
and	O
the	O
Q	Method
-	Method
network	Method
share	O
the	O
same	O
set	O
of	O
model	O
parameters	O
.	O
We	O
will	O
explain	O
in	O
Section	O
[	O
reference	O
]	O
how	O
such	O
parameter	Method
sharing	Method
enables	O
indirect	Task
updates	Task
to	O
the	O
policy	Method
via	O
Q	Method
-	Method
learning	Method
from	O
off	O
-	O
policy	O
data	O
.	O
subsection	O
:	O
The	O
training	Method
algorithm	Method
We	O
now	O
discuss	O
how	O
to	O
train	O
the	O
model	O
parameters	O
(	O
including	O
and	O
)	O
from	O
a	O
training	O
dataset	O
using	O
reinforcement	Method
learning	Method
.	O
One	O
approach	O
is	O
the	O
policy	Method
gradient	Method
method	Method
(	O
REINFORCE	Method
)	Method
,	O
which	O
uses	O
the	O
current	O
policy	O
to	O
roll	O
out	O
multiple	O
trajectories	O
to	O
estimate	O
a	O
stochastic	O
gradient	O
,	O
and	O
then	O
updates	O
the	O
policy	O
via	O
stochastic	Method
gradient	Method
ascent	Method
.	O
Previous	O
RL	Method
-	Method
based	Method
KBC	Method
methods	Method
typically	O
use	O
REINFORCE	Method
to	O
learn	O
the	O
policy	O
.	O
However	O
,	O
policy	Method
gradient	Method
methods	Method
generally	O
suffer	O
from	O
low	Metric
sample	Metric
efficiency	Metric
,	O
especially	O
when	O
the	O
reward	O
signal	O
is	O
sparse	O
,	O
because	O
large	O
numbers	O
of	O
Monte	Method
Carlo	Method
rollouts	Method
are	O
usually	O
needed	O
to	O
obtain	O
many	O
trajectories	O
with	O
positive	O
terminal	O
reward	O
,	O
particularly	O
in	O
the	O
early	O
stages	O
of	O
learning	Task
.	O
To	O
address	O
this	O
challenge	O
,	O
we	O
develop	O
a	O
novel	O
RL	Method
algorithm	Method
that	O
uses	O
MCTS	Method
to	O
exploit	O
the	O
deterministic	O
MDP	O
transition	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
.	O
Specifically	O
,	O
on	O
each	O
MCTS	Method
simulation	Method
,	O
a	O
trajectory	O
is	O
rolled	O
out	O
by	O
selecting	O
actions	O
according	O
to	O
a	O
variant	O
of	O
the	O
PUCT	Method
algorithm	Method
from	O
the	O
root	O
state	O
(	O
defined	O
in	O
(	O
[	O
reference	O
]	O
)	O
)	O
:	O
where	O
is	O
the	O
policy	O
defined	O
in	O
Section	O
[	O
reference	O
]	O
,	O
and	O
are	O
two	O
constants	O
that	O
control	O
the	O
level	O
of	O
exploration	O
,	O
and	O
and	O
are	O
the	O
visit	O
count	O
and	O
the	O
total	O
action	O
reward	O
accumulated	O
on	O
the	O
-	O
th	O
edge	O
on	O
the	O
MCTS	O
tree	O
.	O
Overall	O
,	O
PUCT	Method
treats	O
as	O
a	O
prior	O
probability	O
to	O
bias	O
the	O
MCTS	Method
search	Method
;	O
PUCT	Method
initially	O
prefers	O
actions	O
with	O
high	O
values	O
of	O
and	O
low	O
visit	O
count	O
(	O
because	O
the	O
first	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
is	O
large	O
)	O
,	O
but	O
then	O
asympotically	O
prefers	O
actions	O
with	O
high	O
value	O
(	O
because	O
the	O
first	O
term	O
in	O
(	O
[	O
reference	O
]	O
)	O
vanishes	O
and	O
the	O
second	O
term	O
dominates	O
)	O
.	O
When	O
PUCT	O
selects	O
the	O
STOP	O
action	O
or	O
the	O
maximum	O
search	O
horizon	O
has	O
been	O
reached	O
,	O
MCTS	Method
completes	O
one	O
simulation	O
and	O
updates	O
and	O
using	O
.	O
(	O
See	O
Figure	O
[	O
reference	O
]	O
for	O
an	O
example	O
and	O
Appendix	O
[	O
reference	O
]	O
for	O
more	O
details	O
.	O
)	O
The	O
key	O
idea	O
of	O
our	O
method	O
is	O
that	O
running	O
multiple	O
MCTS	Method
simulations	Method
generates	O
a	O
set	O
of	O
trajectories	O
with	O
more	O
positive	O
rewards	O
(	O
see	O
Section	O
[	O
reference	O
]	O
for	O
more	O
analysis	O
)	O
,	O
which	O
can	O
also	O
be	O
viewed	O
as	O
being	O
generated	O
by	O
an	O
improved	O
policy	O
.	O
Therefore	O
,	O
learning	O
from	O
these	O
trajectories	O
can	O
further	O
improve	O
.	O
Our	O
RL	Method
algorithm	Method
repeatedly	O
applies	O
this	O
policy	Method
-	Method
improvement	Method
step	Method
to	O
refine	O
the	O
policy	O
.	O
However	O
,	O
since	O
these	O
trajectories	O
are	O
generated	O
by	O
a	O
policy	Method
that	O
is	O
different	O
from	O
,	O
they	O
are	O
off	O
-	O
policy	O
data	O
,	O
breaking	O
the	O
assumptions	O
inherent	O
in	O
policy	Method
gradient	Method
methods	Method
.	O
For	O
this	O
reason	O
,	O
we	O
instead	O
update	O
the	O
Q	Method
-	Method
network	Method
from	O
these	O
trajectories	O
in	O
an	O
off	Method
-	Method
policy	Method
manner	Method
using	O
Q	Method
-	Method
learning	Method
:	O
.	O
Recall	O
from	O
Section	O
[	O
reference	O
]	O
that	O
and	O
share	O
the	O
same	O
set	O
of	O
model	O
parameters	O
;	O
once	O
the	O
Q	Method
-	Method
network	Method
is	O
updated	O
,	O
the	O
policy	Method
network	Method
will	O
also	O
be	O
automatically	O
improved	O
.	O
Finally	O
,	O
the	O
new	O
is	O
used	O
to	O
control	O
the	O
MCTS	O
in	O
the	O
next	O
iteration	O
.	O
The	O
main	O
idea	O
of	O
the	O
training	Method
algorithm	Method
is	O
summarized	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
The	O
prediction	Method
algorithm	Method
At	O
test	O
time	O
,	O
we	O
want	O
to	O
infer	O
the	O
target	O
node	O
for	O
an	O
unseen	O
pair	O
of	O
.	O
One	O
approach	O
is	O
to	O
use	O
the	O
learned	O
policy	Method
to	O
walk	O
through	O
the	O
graph	O
to	O
find	O
.	O
However	O
,	O
this	O
would	O
not	O
exploit	O
the	O
known	O
MDP	Method
transition	Method
model	Method
(	O
[	O
reference	O
]	O
)	O
.	O
Instead	O
,	O
we	O
combine	O
the	O
learned	O
and	O
with	O
MCTS	Method
to	O
generate	O
an	O
MCTS	O
search	O
tree	O
,	O
as	O
in	O
the	O
training	O
stage	O
.	O
Note	O
that	O
there	O
could	O
be	O
multiple	O
paths	O
that	O
reach	O
the	O
same	O
terminal	O
node	O
,	O
meaning	O
that	O
there	O
could	O
be	O
multiple	O
leaf	O
states	O
in	O
MCTS	O
corresponding	O
to	O
that	O
node	O
.	O
Therefore	O
,	O
the	O
prediction	O
results	O
from	O
these	O
MCTS	O
leaf	O
states	O
need	O
to	O
be	O
merged	O
into	O
one	O
score	O
to	O
rank	O
the	O
node	O
.	O
Specifically	O
,	O
we	O
use	O
,	O
where	O
is	O
the	O
total	O
number	O
of	O
MCTS	O
simulations	O
,	O
and	O
the	O
summation	O
is	O
over	O
all	O
the	O
leaf	O
states	O
that	O
correspond	O
to	O
the	O
same	O
node	O
.	O
is	O
a	O
weighted	O
average	O
of	O
the	O
terminal	O
state	O
values	O
associated	O
with	O
the	O
same	O
candidate	O
node	O
.	O
Among	O
all	O
the	O
candidates	O
nodes	O
,	O
we	O
select	O
the	O
predicted	O
target	O
node	O
to	O
be	O
the	O
one	O
with	O
the	O
highest	O
score	O
:	O
.	O
subsection	O
:	O
The	O
RNN	Method
state	Method
encoder	Method
We	O
now	O
discuss	O
the	O
details	O
of	O
the	O
RNN	Method
state	Method
encoder	Method
,	O
where	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Specifically	O
,	O
we	O
explain	O
how	O
the	O
sub	O
-	O
vectors	O
of	O
are	O
computed	O
.	O
We	O
introduce	O
as	O
an	O
auxiliary	O
variable	O
.	O
Then	O
,	O
the	O
state	O
in	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
written	O
as	O
.	O
Note	O
that	O
the	O
state	O
is	O
composed	O
of	O
two	O
parts	O
:	O
(	O
i	O
)	O
and	O
,	O
which	O
represent	O
the	O
candidate	O
actions	O
to	O
be	O
selected	O
(	O
excluding	O
the	O
STOP	O
action	O
)	O
,	O
and	O
(	O
ii	O
)	O
,	O
which	O
represents	O
the	O
history	O
.	O
We	O
use	O
two	O
different	O
neural	Method
networks	Method
to	O
encode	O
these	O
separately	O
.	O
For	O
the	O
-	O
th	O
candidate	O
action	O
(	O
)	O
,	O
we	O
concatenate	O
with	O
its	O
associated	O
and	O
input	O
them	O
into	O
a	O
fully	Method
connected	Method
network	Method
(	O
FCN	Method
)	O
to	O
compute	O
their	O
joint	Method
vector	Method
representation	Method
,	O
where	O
is	O
the	O
model	O
parameter	O
.	O
Recall	O
that	O
the	O
action	O
space	O
can	O
be	O
time	O
-	O
varying	O
when	O
the	O
size	O
of	O
changes	O
over	O
time	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
apply	O
the	O
same	O
FCN	Method
to	O
different	O
to	O
obtain	O
their	O
respective	O
representations	O
.	O
Then	O
,	O
we	O
use	O
a	O
coordinate	Method
-	Method
wise	Method
max	Method
-	Method
pooling	Method
operation	Method
over	O
to	O
obtain	O
a	O
(	O
fixed	O
-	O
length	O
)	O
overall	Method
vector	Method
representation	Method
of	Method
.	O
To	O
encode	O
,	O
we	O
call	O
upon	O
the	O
following	O
recursion	O
for	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
the	O
derivation	O
)	O
:	O
.	O
Inspired	O
by	O
this	O
recursion	O
,	O
we	O
propose	O
using	O
the	O
GRU	Method
-	Method
RNN	Method
to	O
encode	O
into	O
a	O
vector	Method
representation	Method
:	O
with	O
initialization	O
,	O
where	O
is	O
the	O
model	O
parameter	O
,	O
and	O
denotes	O
the	O
vector	O
at	O
.	O
We	O
use	O
and	O
computed	O
by	O
the	O
FCNs	Method
to	O
represent	O
and	O
,	O
respectively	O
.	O
Then	O
,	O
we	O
map	O
to	O
using	O
another	O
FCN	Method
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
and	O
analyze	O
the	O
effectiveness	O
of	O
M	Method
-	Method
Walk	Method
on	O
a	O
synthetic	Task
Three	Task
Glass	Task
Puzzle	Task
task	Task
and	O
two	O
real	Task
-	Task
world	Task
KBC	Task
tasks	Task
.	O
We	O
briefly	O
describe	O
the	O
tasks	O
here	O
,	O
and	O
give	O
the	O
experiment	O
details	O
and	O
hyperparameters	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Three	O
Glass	O
Puzzle	O
The	O
Three	Task
Glass	Task
Puzzle	Task
is	O
a	O
problem	O
studied	O
in	O
math	Task
puzzles	Task
and	O
graph	Task
theory	Task
.	O
It	O
involves	O
three	O
milk	O
containers	O
,	O
,	O
and	O
,	O
with	O
capacities	O
,	O
and	O
liters	O
,	O
respectively	O
.	O
The	O
containers	O
display	O
no	O
intermediate	O
markings	O
.	O
There	O
are	O
three	O
feasible	O
actions	O
at	O
each	O
time	O
step	O
:	O
(	O
i	O
)	O
fill	O
a	O
container	O
(	O
to	O
its	O
capacity	O
)	O
,	O
(	O
ii	O
)	O
empty	O
all	O
of	O
its	O
liquid	O
,	O
and	O
(	O
iii	O
)	O
pour	O
its	O
liquid	O
into	O
another	O
container	O
(	O
up	O
to	O
its	O
capacity	O
)	O
.	O
The	O
objective	O
of	O
the	O
problem	O
is	O
,	O
given	O
a	O
desired	O
volume	O
,	O
to	O
take	O
a	O
sequence	O
of	O
actions	O
on	O
the	O
three	O
containers	O
after	O
which	O
one	O
of	O
them	O
contains	O
liters	O
of	O
liquid	O
.	O
We	O
formulate	O
this	O
as	O
a	O
graph	Task
-	Task
walking	Task
problem	Task
;	O
in	O
the	O
graph	O
,	O
each	O
node	O
denotes	O
the	O
amounts	O
of	O
remaining	O
liquid	O
in	O
the	O
three	O
containers	O
,	O
each	O
edge	O
denotes	O
one	O
of	O
the	O
three	O
feasible	O
actions	O
,	O
and	O
the	O
input	O
query	O
is	O
the	O
desired	O
volume	O
.	O
The	O
reward	O
is	O
when	O
the	O
agent	O
successfully	O
fills	O
one	O
of	O
the	O
containers	O
to	O
and	O
otherwise	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
the	O
details	O
)	O
.	O
We	O
use	O
vanilla	Method
policy	Method
gradient	Method
(	O
REINFORCE	O
)	O
as	O
the	O
baseline	O
,	O
with	O
task	Metric
success	Metric
rate	Metric
as	O
the	O
evaluation	Metric
metric	Metric
.	O
paragraph	O
:	O
Knowledge	Task
Base	Task
Completion	Task
We	O
use	O
WN18RR	Material
and	O
NELL995	O
knowledge	O
graph	O
datasets	O
for	O
evaluation	O
.	O
WN18RR	Material
is	O
created	O
from	O
the	O
original	O
WN18	O
by	O
removing	O
various	O
sources	O
of	O
test	O
leakage	O
,	O
making	O
the	O
dataset	O
more	O
challenging	O
.	O
The	O
NELL995	O
dataset	O
was	O
released	O
by	O
and	O
has	O
separate	O
graphs	O
for	O
each	O
query	O
relation	O
.	O
We	O
use	O
the	O
same	O
data	O
split	O
and	O
preprocessing	Method
protocol	Method
as	O
in	O
for	O
WN18RR	Material
and	O
in	O
for	O
NELL995	O
.	O
As	O
in	O
,	O
we	O
study	O
the	O
10	O
relation	Task
tasks	Task
of	O
NELL995	O
separately	O
.	O
We	O
use	O
HITS@1	Metric
,	O
3	Metric
and	O
mean	Metric
reciprocal	Metric
rank	Metric
(	O
MRR	Metric
)	O
as	O
the	O
evaluation	Metric
metrics	Metric
for	O
WN18RR	Material
,	O
and	O
use	O
mean	Metric
average	Metric
precision	Metric
(	O
MAP	Metric
)	O
for	O
NELL995	O
,	O
where	O
HITS@	Metric
computes	O
the	O
percentage	O
of	O
the	O
desired	O
entities	O
being	O
ranked	O
among	O
the	O
top	O
-	O
list	O
,	O
and	O
MRR	Metric
computes	O
an	O
average	O
of	O
the	O
reciprocal	O
rank	O
of	O
the	O
desired	O
entities	O
.	O
We	O
compare	O
against	O
RL	Method
-	Method
based	Method
methods	Method
,	O
embedding	Method
-	Method
based	Method
models	Method
(	O
including	O
DistMult	Method
,	O
ComplEx	Method
and	Method
ConvE	Method
)	O
and	O
recent	O
work	O
in	O
logical	O
rules	O
(	O
NeuralLP	Method
)	O
.	O
For	O
all	O
the	O
baseline	O
methods	O
,	O
we	O
used	O
the	O
implementation	O
released	O
by	O
the	O
corresponding	O
authors	O
with	O
their	O
best	O
-	O
reported	O
hyperparameter	O
settings	O
.	O
The	O
details	O
of	O
the	O
hyperparameters	O
for	O
M	Method
-	Method
Walk	Method
are	O
described	O
in	O
Appendix	O
[	O
reference	O
]	O
of	O
the	O
supplementary	O
material	O
.	O
subsection	O
:	O
Performance	O
of	O
M	Method
-	Method
Walk	Method
We	O
first	O
report	O
the	O
overall	O
performance	O
of	O
the	O
M	Method
-	Method
Walk	Method
algorithm	O
on	O
the	O
three	O
tasks	O
and	O
compare	O
it	O
with	O
other	O
baseline	O
methods	O
.	O
We	O
ran	O
the	O
experiments	O
three	O
times	O
and	O
report	O
the	O
means	O
and	O
standard	O
deviations	O
(	O
except	O
for	O
PRA	O
,	O
TransE	O
,	O
and	O
TransR	O
on	O
NELL995	O
,	O
whose	O
results	O
are	O
directly	O
quoted	O
from	O
)	O
.	O
On	O
the	O
Three	Task
Glass	Task
Puzzle	Task
task	Task
,	O
M	Method
-	Method
Walk	Method
significantly	O
outperforms	O
the	O
baseline	O
:	O
the	O
best	O
model	O
of	O
M	Method
-	Method
Walk	Method
achieves	O
an	O
accuracy	Metric
of	O
while	O
the	O
best	O
REINFORCE	Method
method	Method
achieves	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
for	O
more	O
experiments	O
with	O
different	O
settings	O
on	O
this	O
task	O
)	O
.	O
For	O
the	O
two	O
KBC	Task
tasks	Task
,	O
we	O
report	O
their	O
results	O
in	O
Tables	O
[	O
reference	O
]	O
-	O
[	O
reference	O
]	O
,	O
where	O
PG	Method
-	Method
Walk	Method
and	O
Q	Method
-	Method
Walk	Method
are	O
two	O
methods	O
we	O
created	O
just	O
for	O
the	O
ablation	O
study	O
in	O
the	O
next	O
section	O
.	O
The	O
proposed	O
method	O
outperforms	O
previous	O
works	O
in	O
most	O
of	O
the	O
metrics	O
on	O
NELL995	O
and	O
WN18RR	Material
datasets	O
.	O
Additional	O
experiments	O
on	O
the	O
FB15k	O
-	O
237	O
dataset	O
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
of	O
the	O
supplementary	O
material	O
.	O
subsection	O
:	O
Analysis	O
of	O
M	Method
-	Method
Walk	Method
We	O
performed	O
extensive	O
experimental	O
analysis	O
to	O
understand	O
the	O
proposed	O
M	Method
-	Method
Walk	Method
algorithm	O
,	O
including	O
(	O
i	O
)	O
the	O
contributions	O
of	O
different	O
components	O
,	O
(	O
ii	O
)	O
its	O
ability	O
to	O
overcome	O
sparse	O
rewards	O
,	O
(	O
iii	O
)	O
hyperparameter	Method
analysis	Method
,	O
(	O
iv	O
)	O
its	O
strengths	O
and	O
weaknesses	O
compared	O
to	O
traditional	O
KBC	Method
methods	Method
,	O
and	O
(	O
v	O
)	O
its	O
running	Metric
time	Metric
.	O
First	O
,	O
we	O
used	O
ablation	Task
studies	Task
to	O
analyze	O
the	O
contributions	O
of	O
different	O
components	O
in	O
M	Method
-	Method
Walk	Method
.	O
To	O
understand	O
the	O
contribution	O
of	O
the	O
proposed	O
neural	Method
architecture	Method
in	O
M	Method
-	Method
Walk	Method
,	O
we	O
created	O
a	O
method	O
,	O
PG	Method
-	Method
Walk	Method
,	O
which	O
uses	O
the	O
same	O
neural	Method
architecture	Method
as	O
M	Method
-	Method
Walk	Method
but	O
with	O
the	O
same	O
training	O
(	O
PG	Method
)	O
and	O
testing	O
(	O
beam	Method
search	Method
)	Method
algorithms	Method
as	O
MINERVA	Method
.	O
We	O
observed	O
that	O
the	O
novel	O
neural	Method
architecture	Method
of	O
M	Method
-	Method
Walk	Method
contributes	O
an	O
overall	O
gain	O
relative	O
to	O
MINERVA	Method
on	O
NELL995	O
,	O
and	O
it	O
is	O
still	O
worse	O
than	O
M	Method
-	Method
Walk	Method
,	O
which	O
uses	O
MCTS	Method
for	O
training	O
and	O
testing	Task
.	O
To	O
further	O
understand	O
the	O
contribution	O
of	O
MCTS	Method
,	O
we	O
created	O
another	O
method	O
,	O
Q	Method
-	Method
Walk	Method
,	O
which	O
uses	O
the	O
same	O
model	Method
architecture	Method
as	O
M	Method
-	Method
Walk	Method
except	O
that	O
it	O
is	O
trained	O
by	O
Q	Method
-	Method
learning	Method
only	O
without	O
MCTS	Method
.	O
Note	O
that	O
this	O
lost	O
about	O
in	O
overall	O
performance	O
on	O
NELL995	O
.	O
We	O
observed	O
similar	O
trends	O
on	O
WN18RR	Material
.	O
In	O
addition	O
,	O
we	O
also	O
analyze	O
the	O
importance	O
of	O
MCTS	O
in	O
the	O
testing	Task
stage	Task
in	O
Appendix	O
[	O
reference	O
]	O
.	O
Second	O
,	O
we	O
analyze	O
the	O
ability	O
of	O
M	Method
-	Method
Walk	Method
to	O
overcome	O
the	O
sparse	Task
-	Task
reward	Task
problem	Task
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
positive	Metric
reward	Metric
rate	Metric
(	O
i.e.	O
,	O
the	O
percentage	O
of	O
trajectories	O
with	O
positive	O
reward	O
during	O
training	O
)	O
on	O
the	O
Three	O
Glass	Task
Puzzle	Task
task	Task
and	O
the	O
NELL995	O
tasks	O
.	O
Compared	O
to	O
the	O
policy	Method
gradient	Method
method	Method
(	O
PG	Method
-	Method
Walk	Method
)	O
,	O
and	O
Q	Method
-	Method
learning	Method
method	Method
(	O
Q	Method
-	Method
Walk	Method
)	Method
methods	Method
under	O
the	O
same	O
model	Method
architecture	Method
,	O
M	Method
-	Method
Walk	Method
with	O
MCTS	Method
is	O
able	O
to	O
generate	O
trajectories	O
with	O
more	O
positive	O
rewards	O
,	O
and	O
this	O
continues	O
to	O
improve	O
as	O
training	O
progresses	O
.	O
This	O
confirms	O
our	O
motivation	O
of	O
using	O
MCTS	Method
to	O
generate	O
higher	O
-	O
quality	O
trajectories	O
to	O
alleviate	O
the	O
sparse	Task
-	Task
reward	Task
problem	Task
in	O
graph	Task
walking	Task
.	O
Third	O
,	O
we	O
analyze	O
the	O
performance	O
of	O
M	Method
-	Method
Walk	Method
under	O
different	O
numbers	O
of	O
MCTS	Method
rollout	Method
simulations	Method
and	O
different	O
search	O
horizons	O
on	O
WN18RR	Material
dataset	O
,	O
with	O
results	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
observe	O
that	O
the	O
model	O
is	O
less	O
sensitive	O
to	O
search	O
horizon	O
and	O
more	O
sensitive	O
to	O
the	O
number	O
of	O
MCTS	O
rollouts	O
.	O
Finally	O
,	O
we	O
analyze	O
the	O
strengths	O
and	O
weaknesses	O
of	O
M	Method
-	Method
Walk	Method
relative	O
to	O
traditional	O
methods	O
on	O
the	O
WN18RR	Material
dataset	O
.	O
The	O
first	O
question	O
is	O
how	O
M	Method
-	Method
Walk	Method
performs	O
on	O
reasoning	O
paths	O
of	O
different	O
lengths	O
compared	O
to	O
baselines	O
.	O
To	O
answer	O
this	O
,	O
we	O
analyze	O
the	O
HITS@1	Metric
accuracy	Metric
against	O
ConvE	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
We	O
categorize	O
each	O
test	O
example	O
using	O
the	O
BFS	Method
(	Method
breadth	Method
-	Method
first	Method
search	Method
)	O
steps	O
from	O
the	O
query	O
entity	O
to	O
the	O
target	O
entity	O
(	O
-	O
1	O
means	O
not	O
reachable	O
)	O
.	O
We	O
observe	O
that	O
M	Method
-	Method
Walk	Method
outperforms	O
the	O
strong	O
baseline	O
ConvE	O
by	O
4.6–10.9	O
%	O
in	O
samples	O
that	O
require	O
2	O
or	O
3	Metric
steps	O
,	O
while	O
it	O
is	O
nearly	O
on	O
par	O
for	O
paths	O
of	O
length	O
one	O
.	O
Therefore	O
,	O
M	Method
-	Method
Walk	Method
does	O
better	O
at	O
reasoning	O
over	O
longer	O
paths	O
than	O
ConvE.	O
Another	O
question	O
is	O
what	O
are	O
the	O
major	O
types	O
of	O
errors	O
made	O
by	O
M	Method
-	Method
Walk	Method
.	O
Recall	O
that	O
M	Method
-	Method
Walk	Method
only	O
walks	O
through	O
a	O
subset	O
of	O
the	O
graph	O
and	O
ranks	O
a	O
subset	O
of	O
candidate	O
nodes	O
(	O
e.g.	O
,	O
MCTS	Method
produces	O
about	O
20–60	O
unique	O
candidates	O
on	O
WN18RR	Material
)	O
.	O
When	O
the	O
ground	O
truth	O
is	O
not	O
in	O
the	O
candidate	O
set	O
,	O
M	Method
-	Method
Walk	Method
always	O
makes	O
mistakes	O
and	O
we	O
define	O
this	O
type	O
of	O
error	O
as	O
out	Metric
-	Metric
of	Metric
-	Metric
candidate	Metric
-	Metric
set	Metric
error	Metric
.	O
To	O
examine	O
this	O
effect	O
,	O
we	O
show	O
in	O
Figure	O
[	O
reference	O
]	O
-	O
top	O
the	O
HITS@K	Metric
accuracies	Metric
when	O
the	O
ground	O
truth	O
is	O
in	O
the	O
candidate	O
set	O
.	O
It	O
shows	O
that	O
M	Method
-	Method
Walk	Method
has	O
very	O
high	O
accuracy	Metric
in	O
this	O
case	O
,	O
which	O
is	O
significantly	O
higher	O
than	O
ConvE	O
(	O
80	O
%	O
vs	O
39.6	O
%	O
in	O
HITS@1	Metric
)	O
.	O
We	O
further	O
examine	O
the	O
percentage	O
of	O
out	O
-	O
of	O
-	O
candidate	O
-	O
set	O
errors	O
among	O
all	O
errors	O
in	O
Figure	O
[	O
reference	O
]	O
-	O
bottom	O
.	O
It	O
shows	O
that	O
the	O
major	O
error	O
made	O
by	O
M	Method
-	Method
Walk	Method
is	O
the	O
out	O
-	O
of	O
-	O
candidate	Metric
-	Metric
set	Metric
error	Metric
.	O
These	O
observations	O
point	O
to	O
an	O
important	O
direction	O
for	O
improving	O
M	Method
-	Method
Walk	Method
in	O
future	O
work	O
:	O
increasing	O
the	O
chance	O
of	O
covering	O
the	O
target	O
by	O
the	O
candidate	O
set	O
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
running	Metric
time	Metric
of	O
M	Method
-	Method
Walk	Method
(	O
in	O
-	O
house	O
C	O
++	O
&	O
Cuda	Method
)	O
and	O
MINERVA	Method
(	O
TensorFlow	Method
-	Method
gpu	Method
)	O
for	O
both	O
training	Task
and	O
testing	Task
on	O
WN18RR	Material
with	O
different	O
values	O
of	O
search	O
horizon	O
and	O
number	O
of	O
rollouts	O
(	O
or	O
MCTS	O
simulation	O
number	O
)	O
.	O
Note	O
that	O
the	O
running	Metric
time	Metric
of	O
M	Method
-	Method
Walk	Method
is	O
comparable	O
to	O
that	O
of	O
MINERVA	Method
.	O
Additional	O
results	O
can	O
be	O
found	O
in	O
Figure	O
[	O
reference	O
]	O
of	O
the	O
supplementary	O
material	O
.	O
Finally	O
,	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
examples	O
of	O
reasoning	O
paths	O
found	O
by	O
M	Method
-	Method
Walk	Method
.	O
section	O
:	O
Related	O
Work	O
paragraph	O
:	O
Reinforcement	Method
Learning	Method
Recently	O
,	O
deep	Method
reinforcement	Method
learning	Method
has	O
achieved	O
great	O
success	O
in	O
many	O
artificial	Task
intelligence	Task
problems	Task
.	O
The	O
use	O
of	O
deep	Method
neural	Method
networks	Method
with	O
RL	Method
allows	O
policies	O
to	O
be	O
learned	O
from	O
raw	O
data	O
(	O
e.g.	O
,	O
images	O
)	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
Our	O
work	O
also	O
aligns	O
with	O
this	O
direction	O
.	O
Furthermore	O
,	O
the	O
idea	O
of	O
using	O
an	O
RNN	Method
to	O
encode	O
the	O
history	O
of	O
observations	O
also	O
appeared	O
in	O
.	O
The	O
combination	O
of	O
model	Method
-	Method
based	Method
and	Method
model	Method
-	Method
free	Method
information	Method
in	O
our	O
work	O
shares	O
the	O
same	O
spirit	O
as	O
.	O
Among	O
them	O
,	O
the	O
most	O
relevant	O
are	O
,	O
which	O
combine	O
MCTS	Method
with	O
neural	Method
policy	Method
and	O
value	Method
functions	Method
to	O
achieve	O
superhuman	O
performance	O
on	O
Go	Task
.	O
Different	O
from	O
our	O
work	O
,	O
the	O
policy	Method
and	O
the	O
value	Method
networks	Method
in	O
are	O
trained	O
separately	O
without	O
the	O
help	O
of	O
MCTS	Method
,	O
and	O
are	O
only	O
used	O
to	O
help	O
MCTS	Method
after	O
being	O
trained	O
.	O
The	O
work	O
uses	O
a	O
new	O
policy	Method
iteration	Method
method	Method
that	O
combines	O
the	O
neural	Method
policy	Method
and	O
value	Method
functions	Method
with	O
MCTS	Method
during	O
training	O
.	O
However	O
,	O
the	O
method	O
in	O
improves	O
the	O
policy	Method
network	Method
from	O
the	O
MCTS	O
probabilities	O
of	O
the	O
moves	O
,	O
while	O
our	O
method	O
improves	O
the	O
policy	O
from	O
the	O
trajectories	O
generated	O
by	O
MCTS	Method
.	O
Note	O
that	O
the	O
former	O
is	O
constructed	O
from	O
the	O
visit	O
counts	O
of	O
all	O
the	O
edges	O
connected	O
to	O
the	O
MCTS	O
root	O
node	O
;	O
it	O
only	O
uses	O
information	O
near	O
the	O
root	O
node	O
to	O
improve	O
the	O
policy	O
.	O
By	O
contrast	O
,	O
we	O
improve	O
the	O
policy	O
by	O
learning	O
from	O
the	O
trajectories	O
generated	O
by	O
MCTS	Method
,	O
using	O
information	O
over	O
the	O
entire	O
MCTS	O
search	O
tree	O
.	O
paragraph	O
:	O
Knowledge	Task
Base	Task
Completion	Task
In	O
KBC	Task
tasks	Task
,	O
early	O
work	O
focused	O
on	O
learning	O
vector	Task
representations	Task
of	Task
entities	Task
and	Task
relations	Task
.	O
Recent	O
approaches	O
have	O
demonstrated	O
limitations	O
of	O
these	O
prior	O
approaches	O
:	O
they	O
suffer	O
from	O
cascading	O
errors	O
when	O
dealing	O
with	O
compositional	O
(	O
multi	O
-	O
step	O
)	O
relationships	O
.	O
Hence	O
,	O
recent	O
works	O
have	O
proposed	O
approaches	O
for	O
injecting	O
multi	Task
-	Task
step	Task
paths	Task
such	O
as	O
random	O
walks	O
through	O
sequences	O
of	O
triples	O
during	O
training	O
,	O
further	O
improving	O
performance	O
on	O
KBC	Task
tasks	Task
.	O
IRN	Method
and	O
Neural	Method
LP	Method
explore	O
multi	O
-	O
step	O
relations	O
by	O
using	O
an	O
RNN	Method
controller	Method
with	O
attention	O
over	O
an	O
external	O
memory	O
.	O
Compared	O
to	O
RL	Method
-	Method
based	Method
approaches	Method
,	O
it	O
is	O
hard	O
to	O
interpret	O
the	O
traversal	O
paths	O
,	O
and	O
these	O
models	O
can	O
be	O
computationally	O
expensive	O
to	O
access	O
the	O
entire	O
graph	O
in	O
memory	O
.	O
Two	O
recent	O
works	O
,	O
DeepPath	Method
and	O
MINERVA	Method
,	O
use	O
RL	Method
-	Method
based	Method
approaches	Method
to	O
explore	O
paths	Task
in	Task
knowledge	Task
graphs	Task
.	O
DeepPath	Method
requires	O
target	O
entity	O
information	O
to	O
be	O
in	O
the	O
state	O
of	O
the	O
RL	Method
agent	Method
,	O
and	O
can	O
not	O
be	O
applied	O
to	O
tasks	O
where	O
the	O
target	O
entity	O
is	O
unknown	O
.	O
MINERVA	Method
uses	O
a	O
policy	Method
gradient	Method
method	Method
to	O
explore	O
paths	O
during	O
training	O
and	O
test	O
.	O
Our	O
proposed	O
model	O
further	O
exploits	O
state	O
transition	O
information	O
by	O
integrating	O
the	O
MCTS	Method
algorithm	Method
.	O
Empirically	O
,	O
our	O
proposed	O
algorithm	O
outperforms	O
both	O
DeepPath	Method
and	Method
MINERVA	Method
in	O
the	O
KBC	O
benchmarks	O
.	O
section	O
:	O
Conclusion	O
and	O
Discussion	O
We	O
developed	O
an	O
RL	Method
-	Method
agent	Method
(	O
M	Method
-	Method
Walk	Method
)	O
that	O
learns	O
to	O
walk	O
over	O
a	O
graph	O
towards	O
a	O
desired	O
target	O
node	O
for	O
given	O
input	O
query	O
and	O
source	O
nodes	O
.	O
Specifically	O
,	O
we	O
proposed	O
a	O
novel	O
neural	Method
architecture	Method
that	O
encodes	O
the	O
state	O
into	O
a	O
vector	Method
representation	Method
,	O
and	O
maps	O
it	O
to	O
Q	O
-	O
values	O
and	O
a	O
policy	Method
.	O
To	O
learn	O
from	O
sparse	O
rewards	O
,	O
we	O
propose	O
a	O
new	O
reinforcement	Method
learning	Method
algorithm	Method
,	O
which	O
alternates	O
between	O
an	O
MCTS	Method
trajectory	Method
-	Method
generation	Method
step	Method
and	O
a	O
policy	Method
-	Method
improvement	Method
step	Method
,	O
to	O
iteratively	O
refine	O
the	O
policy	O
.	O
At	O
test	O
time	O
,	O
the	O
learned	O
networks	O
are	O
combined	O
with	O
MCTS	Method
to	O
search	O
for	O
the	O
target	O
node	O
.	O
Experimental	O
results	O
on	O
several	O
benchmarks	O
demonstrate	O
that	O
our	O
method	O
learns	O
better	O
policies	O
than	O
other	O
baseline	O
methods	O
,	O
including	O
RL	Method
-	Method
based	Method
and	O
traditional	O
methods	O
on	O
KBC	Task
tasks	Task
.	O
Furthermore	O
,	O
we	O
also	O
performed	O
extensive	O
experimental	O
analysis	O
to	O
understand	O
M	Method
-	Method
Walk	Method
.	O
We	O
found	O
that	O
our	O
method	O
is	O
more	O
accurate	O
when	O
the	O
ground	O
truth	O
is	O
in	O
the	O
candidate	O
set	O
.	O
We	O
also	O
found	O
that	O
the	O
out	O
-	O
of	O
-	O
candidate	O
-	O
set	O
error	O
is	O
the	O
main	O
type	O
of	O
error	O
made	O
by	O
M	Method
-	Method
Walk	Method
.	O
Therefore	O
,	O
in	O
future	O
work	O
,	O
we	O
intend	O
to	O
improve	O
this	O
method	O
by	O
reducing	O
such	O
out	O
-	O
of	O
-	O
candidate	O
-	O
set	O
errors	O
.	O
subsection	O
:	O
Acknowledgments	O
We	O
thank	O
Ricky	O
Loynd	O
,	O
Adith	O
Swaminathan	O
,	O
and	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Derivation	O
of	O
the	O
recursion	O
for	O
Recalling	O
the	O
definition	O
and	O
using	O
the	O
recursion	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
where	O
step	O
(	O
a	O
)	O
uses	O
the	O
definition	O
of	O
,	O
step	O
(	O
b	O
)	O
substitutes	O
the	O
recursion	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
step	O
(	O
c	O
)	O
uses	O
the	O
definition	O
of	O
.	O
appendix	O
:	O
Algorithm	O
Implementation	O
Details	O
The	O
detailed	O
algorithm	O
of	O
M	Method
-	Method
Walk	Method
is	O
described	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
[	O
h	O
]	O
[	O
1	O
]	O
Input	O
:	O
Graph	O
;	O
Initial	O
node	O
;	O
Query	O
;	O
Target	O
node	O
;	O
Maximum	O
Path	O
Length	O
;	O
MCTS	O
Search	O
Number	O
;	O
episode	O
in	O
Set	O
current	O
node	O
;	O
Lookup	O
from	O
dictionary	O
to	O
obtain	O
and	O
Select	O
the	O
action	O
with	O
the	O
maximum	O
PUCT	O
value	O
:	O
Update	O
is	O
STOP	O
Compute	O
estimated	O
reward	O
value	O
Add	O
generated	O
path	O
into	O
a	O
path	O
list	O
Backup	O
along	O
the	O
path	O
to	O
update	O
the	O
visit	O
count	O
using	O
(	O
[	O
reference	O
]	O
)	O
and	O
the	O
total	O
action	O
reward	O
using	O
(	O
[	O
reference	O
]	O
)	O
on	O
the	O
-	O
th	O
edge	O
on	O
the	O
MCTS	Method
tree	Method
Break	O
each	O
path	O
in	O
the	O
path	O
list	O
Set	O
reward	O
if	O
the	O
end	O
of	O
the	O
path	O
otherwise	O
Repeatedly	O
update	O
the	O
model	Method
parameters	Method
with	O
Q	Method
-	Method
learning	Method
:	O
M	Method
-	Method
Walk	Method
Training	O
Algorithm	O
subsection	O
:	O
MCTS	Method
implementation	Method
In	O
the	O
MCTS	Method
implementation	Method
,	O
we	O
maintain	O
a	O
lookup	O
table	O
to	O
record	O
values	O
and	O
for	O
each	O
visited	O
state	O
-	O
action	O
pair	O
.	O
The	O
state	O
in	O
the	O
graph	Task
walk	Task
problem	Task
contains	O
all	O
the	O
information	O
along	O
the	O
traversal	O
path	O
,	O
and	O
is	O
the	O
node	O
at	O
the	O
current	O
step	O
.	O
We	O
assign	O
an	O
index	O
to	O
each	O
candidate	O
action	O
from	O
,	O
indicating	O
that	O
is	O
the	O
-	O
th	O
action	O
of	O
the	O
node	O
.	O
Thus	O
,	O
the	O
state	O
can	O
be	O
encoded	O
as	O
a	O
path	O
string	O
.	O
We	O
build	O
a	O
dictionary	O
using	O
the	O
path	O
string	O
as	O
a	O
key	O
,	O
and	O
we	O
record	O
and	O
as	O
values	O
in	O
.	O
In	O
the	O
backup	O
stage	O
,	O
the	O
and	O
values	O
are	O
updated	O
for	O
each	O
state	O
-	O
action	O
pair	O
along	O
with	O
the	O
traversal	O
path	O
in	O
MCTS	Method
:	O
where	O
is	O
the	O
length	O
of	O
the	O
traversal	O
path	O
,	O
is	O
the	O
discount	O
factor	O
of	O
the	O
MDP	Method
,	O
and	O
is	O
the	O
terminal	O
state	O
-	O
value	O
function	O
modeled	O
by	O
.	O
In	O
our	O
experiments	O
,	O
the	O
softmax	O
temperature	O
parameter	O
in	O
the	O
policy	Method
network	Method
(	O
see	O
(	O
[	O
reference	O
]	O
)	O
)	O
is	O
set	O
to	O
be	O
a	O
constant	O
.	O
An	O
alternative	O
choice	O
is	O
to	O
anneal	O
it	O
during	O
training	O
(	O
e.g.	O
,	O
)	O
.	O
However	O
,	O
we	O
did	O
not	O
observe	O
this	O
to	O
produce	O
any	O
significant	O
difference	O
in	O
performance	O
in	O
our	O
experiments	O
.	O
We	O
believe	O
the	O
main	O
reason	O
is	O
that	O
is	O
only	O
used	O
as	O
a	O
prior	O
to	O
bias	O
the	O
MCTS	Task
search	Task
,	O
while	O
the	O
exploration	O
of	O
MCTS	Method
is	O
controlled	O
by	O
the	O
parameters	O
and	O
of	O
(	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Experiment	O
details	O
subsubsection	O
:	O
Three	O
Glass	O
Puzzle	O
paragraph	O
:	O
An	O
example	O
Figure	O
[	O
reference	O
]	O
illustrates	O
one	O
step	O
in	O
solving	O
a	O
Three	Task
Glass	Task
Puzzle	Task
.	O
The	O
following	O
action	O
sequences	O
provide	O
one	O
solution	O
to	O
achieve	O
the	O
target	O
,	O
given	O
initially	O
empty	O
containers	O
with	O
capacities	O
,	O
where	O
denote	O
the	O
current	O
contents	O
of	O
the	O
containers	O
:	O
Initial	O
state	O
Fill	O
Pour	O
from	O
to	O
Empty	O
Pour	O
from	O
to	O
Fill	O
Pour	O
from	O
to	O
paragraph	O
:	O
Data	Task
generation	Task
In	O
the	O
Three	O
Glass	O
Puzzle	O
experiments	O
,	O
we	O
randomly	O
draw	O
four	O
integers	O
from	O
to	O
represent	O
the	O
capacities	O
,	O
and	O
the	O
desired	O
volume	O
.	O
We	O
further	O
restrict	O
the	O
values	O
so	O
that	O
and	O
,	O
to	O
avoid	O
data	O
duplication	O
.	O
We	O
discard	O
puzzles	O
for	O
which	O
there	O
is	O
no	O
solution	O
.	O
Finally	O
,	O
we	O
keep	O
600	O
unique	O
puzzles	O
as	O
the	O
experimental	O
dataset	O
,	O
where	O
500	O
puzzles	O
are	O
used	O
for	O
training	O
and	O
the	O
other	O
100	O
are	O
used	O
to	O
test	O
a	O
model	O
’s	O
generalization	Metric
capability	Metric
on	O
the	O
unseen	O
test	O
set	O
.	O
paragraph	O
:	O
Experiment	O
settings	O
and	O
hyperparameters	O
Let	O
be	O
the	O
current	O
status	O
of	O
each	O
container	O
,	O
and	O
define	O
the	O
puzzle	O
status	O
at	O
step	O
as	O
,	O
where	O
is	O
the	O
one	Method
-	Method
hot	Method
representation	Method
to	O
encode	O
the	O
value	O
of	O
.	O
Given	O
that	O
and	O
are	O
all	O
smaller	O
than	O
in	O
the	O
experiment	O
,	O
the	O
dimension	O
of	O
is	O
300	O
.	O
The	O
initial	O
query	O
is	O
obtained	O
by	O
,	O
where	O
is	O
a	O
query	O
embedding	O
lookup	O
table	O
and	O
indicates	O
the	O
-	O
th	O
column	O
.	O
The	O
query	O
embedding	O
dimension	O
is	O
set	O
to	O
.	O
In	O
the	O
Three	O
Glass	O
Puzzle	O
,	O
there	O
are	O
13	O
actions	O
in	O
total	O
:	O
fill	O
one	O
container	O
to	O
its	O
capacity	O
,	O
empty	O
one	O
container	O
,	O
pour	O
one	O
container	O
into	O
another	O
container	O
,	O
and	O
a	O
STOP	O
action	O
to	O
terminate	O
the	O
game	O
.	O
We	O
set	O
the	O
maximum	O
length	O
of	O
an	O
action	O
sequence	O
(	O
i.e.	O
,	O
the	O
search	O
horizon	O
)	O
to	O
be	O
,	O
where	O
only	O
the	O
STOP	O
action	O
can	O
be	O
taken	O
on	O
the	O
final	O
step	O
.	O
After	O
the	O
STOP	O
action	O
has	O
been	O
taken	O
,	O
the	O
system	O
evaluates	O
the	O
action	O
sequence	O
and	O
assigns	O
a	O
reward	O
if	O
the	O
final	O
status	O
is	O
a	O
success	O
,	O
otherwise	O
.	O
The	O
and	O
functions	O
are	O
modeled	O
by	O
two	O
different	O
DNNs	Method
with	O
the	O
same	O
architecture	O
:	O
two	O
fully	Method
-	Method
connected	Method
layers	Method
with	O
32	O
hidden	O
dimensions	O
and	O
ReLU	O
activation	O
function	O
.	O
is	O
two	O
fully	Method
-	Method
connected	Method
layers	Method
with	O
16	O
hidden	O
dimensions	O
,	O
where	O
the	O
first	O
hidden	Method
layer	Method
uses	O
a	O
ReLU	Method
activation	Method
function	Method
and	O
the	O
output	Method
layer	Method
uses	O
a	O
linear	O
activation	O
function	O
.	O
is	O
modeled	O
by	O
a	O
GRU	Method
with	O
hidden	O
size	O
64	O
.	O
The	O
hyperparameters	O
in	O
PUCT	Method
are	O
set	O
to	O
and	O
.	O
We	O
use	O
the	O
ADAM	Method
optimization	Method
algorithm	Method
with	O
learning	Method
rate	Method
during	O
training	O
,	O
and	O
we	O
set	O
the	O
mini	O
-	O
batch	O
size	O
to	O
.	O
subsubsection	O
:	O
Knowledge	Task
Base	Task
Completion	Task
paragraph	O
:	O
Statistics	O
of	O
the	O
three	O
datasets	O
The	O
NELL	O
-	O
995	O
knowledge	O
dataset	O
contains	O
unique	O
entities	O
and	O
relations	O
.	O
WN18RR	Material
contains	O
triples	O
with	O
entities	O
and	O
relations	O
.	O
And	O
FB15k	Method
-	Method
237	Method
,	O
a	O
subset	O
of	O
FB15k	O
where	O
inverse	O
relations	O
are	O
removed	O
,	O
contains	O
entities	O
and	O
relations	O
.	O
The	O
detailed	O
statstics	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Experiment	O
settings	O
and	O
hyperparameters	O
For	O
the	O
proposed	O
M	Method
-	Method
Walk	Method
,	O
we	O
set	O
the	O
entity	O
embedding	O
dimension	O
to	O
and	O
relation	O
embedding	O
dimension	O
to	O
.	O
The	O
maximum	O
length	O
of	O
the	O
graph	O
walking	O
path	O
(	O
i.e.	O
,	O
the	O
search	O
horizon	O
)	O
is	O
in	O
the	O
NELL	O
-	O
995	O
dataset	O
and	O
in	O
the	O
WN18RR	Material
dataset	O
.	O
After	O
the	O
STOP	O
action	O
has	O
been	O
taken	O
,	O
the	O
system	O
evaluates	O
the	O
action	O
sequence	O
and	O
assigns	O
a	O
reward	O
if	O
the	O
agent	O
reaches	O
the	O
target	O
node	O
,	O
otherwise	O
.	O
The	O
initial	O
query	O
is	O
the	O
concatenation	O
of	O
the	O
entity	O
embedding	O
vector	O
and	O
the	O
relation	O
embedding	O
vector	O
.	O
The	O
and	O
functions	O
are	O
modeled	O
by	O
two	O
different	O
DNNs	Method
with	O
the	O
same	O
architecture	O
:	O
two	O
fully	Method
-	Method
connected	Method
layers	Method
with	O
64	O
hidden	O
dimensions	O
and	O
the	O
ReLU	Method
activation	Method
function	Method
.	O
is	O
two	O
fully	Method
-	Method
connected	Method
layers	Method
with	O
16	O
hidden	O
dimensions	O
,	O
where	O
the	O
first	O
hidden	Method
layer	Method
uses	O
a	O
Tanh	Method
activation	Method
function	Method
and	O
the	O
output	Method
layer	Method
uses	O
a	O
linear	O
activation	O
function	O
.	O
is	O
modeled	O
by	O
a	O
GRU	Method
with	O
hidden	O
size	O
64	O
.	O
The	O
hyperparameters	O
in	O
PUCT	Method
are	O
set	O
to	O
and	O
.	O
We	O
roll	O
out	O
MCTS	O
paths	O
in	O
both	O
training	O
and	O
testing	O
in	O
the	O
NELL	O
-	O
995	O
dataset	O
and	O
MCTS	O
paths	O
in	O
the	O
WN18RR	Material
dataset	O
.	O
We	O
use	O
the	O
ADAM	Method
optimization	Method
algorithm	Method
for	O
model	Task
training	Task
with	O
learning	Metric
rate	Metric
,	O
and	O
we	O
set	O
the	O
mini	O
-	O
batch	O
size	O
to	O
.	O
appendix	O
:	O
Additional	O
Experiments	O
subsection	O
:	O
The	O
Three	O
Glass	Task
Puzzle	Task
task	Task
in	O
different	O
settings	O
We	O
now	O
present	O
more	O
experiments	O
on	O
the	O
Three	O
Glass	Task
Puzzle	Task
task	Task
under	O
different	O
settings	O
.	O
First	O
,	O
to	O
see	O
how	O
fast	O
M	Method
-	Method
Walk	Method
converges	O
,	O
we	O
show	O
in	O
Figure	O
[	O
reference	O
]	O
the	O
learning	O
curves	O
of	O
M	Method
-	Method
Walk	Method
and	O
PG	Method
.	O
It	O
shows	O
that	O
M	Method
-	Method
Walk	Method
converges	O
much	O
faster	O
than	O
PG	Method
and	O
achieves	O
better	O
results	O
on	O
this	O
task	O
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
report	O
the	O
test	O
accuracy	Metric
of	O
M	Method
-	Method
Walk	Method
and	O
vanilla	Method
policy	Method
gradient	Method
(	O
REINFORCE	Method
/	Method
PG	Method
)	O
with	O
different	O
beam	O
search	O
sizes	O
and	O
different	O
MCTS	O
rollouts	O
during	O
testing	O
.	O
The	O
number	O
of	O
MCTS	Method
simulations	Method
for	O
training	O
M	Method
-	Method
Walk	Method
is	O
fixed	O
to	O
be	O
.	O
We	O
observe	O
that	O
M	Method
-	Method
Walk	Method
with	O
MCTS	Method
achieves	O
the	O
best	O
test	Metric
accuracy	Metric
overall	O
.	O
In	O
addition	O
,	O
with	O
larger	O
beam	O
search	O
sizes	O
and	O
MCTS	O
rollouts	O
,	O
the	O
test	Metric
accuracy	Metric
improves	O
substantially	O
.	O
Furthermore	O
,	O
replacing	O
the	O
MCTS	Method
in	O
M	Method
-	Method
Walk	Method
by	O
beam	Method
search	Method
at	O
test	O
time	O
degrades	O
the	O
performance	O
greatly	O
,	O
which	O
shows	O
that	O
MCTS	Method
is	O
also	O
very	O
important	O
for	O
M	Method
-	Method
Walk	Method
at	O
test	O
time	O
.	O
As	O
mentioned	O
earlier	O
,	O
conventional	O
graph	Method
traversal	Method
algorithms	Method
such	O
as	O
Breadth	Method
-	Method
First	Method
Search	Method
(	Method
BFS	Method
)	Method
and	O
Depth	Method
-	Method
First	Method
Search	Method
(	O
DFS	Method
)	O
can	O
not	O
be	O
applied	O
to	O
the	O
graph	Task
walking	Task
problem	Task
,	O
because	O
the	O
ground	O
truth	O
target	O
node	O
is	O
not	O
known	O
at	O
test	O
time	O
.	O
However	O
,	O
to	O
understand	O
how	O
quickly	O
M	Method
-	Method
Walk	Method
with	O
MCTS	Method
can	O
find	O
the	O
correct	O
target	O
node	O
,	O
we	O
compare	O
it	O
with	O
BFS	Method
and	O
DFS	Method
in	O
the	O
following	O
cheating	O
setup	O
.	O
Specifically	O
,	O
we	O
apply	O
BFS	Method
and	O
DFS	Method
to	O
the	O
test	O
set	O
of	O
the	O
Three	O
Glass	Task
Puzzle	Task
task	Task
by	O
disclosing	O
the	O
target	O
node	O
to	O
them	O
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
report	O
the	O
average	O
traversal	O
steps	O
and	O
maximum	O
steps	O
to	O
reach	O
the	O
target	O
node	O
.	O
The	O
M	Method
-	Method
Walk	Method
with	O
MCTS	Method
algorithm	Method
is	O
able	O
to	O
find	O
the	O
target	O
node	O
more	O
efficiently	O
than	O
BFS	Method
or	O
DFS	Method
.	O
subsubsection	O
:	O
Knowledge	Task
Graph	Task
Link	Task
Prediction	Task
In	O
this	O
section	O
,	O
we	O
first	O
provide	O
additional	O
experimental	O
results	O
for	O
the	O
NELL995	Task
and	O
WN18RR	Material
tasks	O
to	O
support	O
our	O
analysis	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
positive	Metric
reward	Metric
rate	Metric
during	O
training	Task
on	O
the	O
NELL995	Task
task	Task
.	O
And	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
provide	O
more	O
hyperparameter	Method
analysis	Method
(	O
search	O
horizon	O
and	O
MCTS	O
simulation	O
number	O
)	O
and	O
training	Method
-	Method
time	Method
analysis	Method
.	O
Furthermore	O
,	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
HITS@K	Metric
and	O
MRR	Metric
results	O
on	O
NELL995	O
.	O
In	O
addition	O
,	O
we	O
conduct	O
further	O
experiments	O
on	O
the	O
FB15k	O
-	O
237	O
dataset	O
,	O
which	O
is	O
a	O
subset	O
of	O
FB15k	O
with	O
inverse	O
relations	O
being	O
removed	O
.	O
We	O
use	O
the	O
same	O
data	O
split	O
and	O
preprocessing	Method
protocol	Method
as	O
in	O
for	O
FB15k	Method
-	Method
237	Method
.	O
The	O
results	O
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
observe	O
that	O
M	Method
-	Method
Walk	Method
outperforms	O
the	O
other	O
RL	Method
-	Method
based	Method
method	Method
(	O
MINERVA	Method
)	O
.	O
However	O
,	O
it	O
is	O
still	O
worse	O
than	O
the	O
embedding	Method
-	Method
based	Method
methods	Method
.	O
In	O
future	O
work	O
,	O
we	O
intend	O
to	O
combine	O
the	O
strength	O
of	O
embedding	Method
-	Method
based	Method
methods	Method
and	O
our	O
method	O
to	O
further	O
improve	O
the	O
performance	O
of	O
M	Method
-	Method
Walk	Method
.	O
subsection	O
:	O
The	O
Reasoning	O
(	O
Traversal	O
)	O
Paths	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
reasoning	O
paths	O
of	O
M	Method
-	Method
Walk	Method
on	O
the	O
NELL995	O
dataset	O
.	O
Each	O
reasoning	O
path	O
is	O
generated	O
by	O
following	O
the	O
edges	O
on	O
the	O
MCTS	O
tree	O
with	O
the	O
highest	O
visiting	O
count	O
.	O
