document	O
:	O
Character	Method
-	Method
level	Method
Convolutional	Method
Networks	Method
for	O
Text	Task
Classification	Task
This	O
article	O
offers	O
an	O
empirical	O
exploration	O
on	O
the	O
use	O
of	O
character	Method
-	Method
level	Method
convolutional	Method
networks	Method
(	O
ConvNets	Method
)	O
for	O
text	Task
classification	Task
.	O
We	O
constructed	O
several	O
large	O
-	O
scale	O
datasets	O
to	O
show	O
that	O
character	Method
-	Method
level	Method
convolutional	Method
networks	Method
could	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
or	O
competitive	O
results	O
.	O
Comparisons	O
are	O
offered	O
against	O
traditional	O
models	O
such	O
as	O
bag	Method
of	Method
words	Method
,	O
n	Method
-	Method
grams	Method
and	O
their	O
TFIDF	Method
variants	Method
,	O
and	O
deep	Method
learning	Method
models	Method
such	O
as	O
word	O
-	O
based	O
ConvNets	Method
and	O
recurrent	Method
neural	Method
networks	Method
.	O
section	O
:	O
Introduction	O
Text	Task
classification	Task
is	O
a	O
classic	O
topic	O
for	O
natural	Task
language	Task
processing	Task
,	O
in	O
which	O
one	O
needs	O
to	O
assign	O
predefined	O
categories	O
to	O
free	O
-	O
text	O
documents	O
.	O
The	O
range	O
of	O
text	Task
classification	Task
research	O
goes	O
from	O
designing	O
the	O
best	O
features	O
to	O
choosing	O
the	O
best	O
possible	O
machine	Method
learning	Method
classifiers	Method
.	O
To	O
date	O
,	O
almost	O
all	O
techniques	O
of	O
text	Task
classification	Task
are	O
based	O
on	O
words	O
,	O
in	O
which	O
simple	O
statistics	O
of	O
some	O
ordered	O
word	O
combinations	O
(	O
such	O
as	O
n	O
-	O
grams	O
)	O
usually	O
perform	O
the	O
best	O
.	O
On	O
the	O
other	O
hand	O
,	O
many	O
researchers	O
have	O
found	O
convolutional	Method
networks	Method
(	O
ConvNets	Method
)	O
are	O
useful	O
in	O
extracting	Task
information	Task
from	O
raw	O
signals	O
,	O
ranging	O
from	O
computer	Task
vision	Task
applications	Task
to	O
speech	Task
recognition	Task
and	O
others	O
.	O
In	O
particular	O
,	O
time	Method
-	Method
delay	Method
networks	Method
used	O
in	O
the	O
early	O
days	O
of	O
deep	Task
learning	Task
research	Task
are	O
essentially	O
convolutional	Method
networks	Method
that	O
model	O
sequential	O
data	O
.	O
In	O
this	O
article	O
we	O
explore	O
treating	O
text	O
as	O
a	O
kind	O
of	O
raw	O
signal	O
at	O
character	O
level	O
,	O
and	O
applying	O
temporal	O
(	O
one	O
-	O
dimensional	O
)	O
ConvNets	Method
to	O
it	O
.	O
For	O
this	O
article	O
we	O
only	O
used	O
a	O
classification	Task
task	Task
as	O
a	O
way	O
to	O
exemplify	O
ConvNets	Method
’	O
ability	O
to	O
understand	O
texts	O
.	O
Historically	O
we	O
know	O
that	O
ConvNets	Method
usually	O
require	O
large	O
-	O
scale	O
datasets	O
to	O
work	O
,	O
therefore	O
we	O
also	O
build	O
several	O
of	O
them	O
.	O
An	O
extensive	O
set	O
of	O
comparisons	O
is	O
offered	O
with	O
traditional	O
models	O
and	O
other	O
deep	Method
learning	Method
models	Method
.	O
Applying	O
convolutional	Method
networks	Method
to	O
text	Task
classification	Task
or	O
natural	Task
language	Task
processing	Task
at	O
large	O
was	O
explored	O
in	O
literature	O
.	O
It	O
has	O
been	O
shown	O
that	O
ConvNets	Method
can	O
be	O
directly	O
applied	O
to	O
distributed	Task
or	Task
discrete	Task
embedding	Task
of	Task
words	Task
,	O
without	O
any	O
knowledge	O
on	O
the	O
syntactic	O
or	O
semantic	O
structures	O
of	O
a	O
language	O
.	O
These	O
approaches	O
have	O
been	O
proven	O
to	O
be	O
competitive	O
to	O
traditional	O
models	O
.	O
There	O
are	O
also	O
related	O
works	O
that	O
use	O
character	O
-	O
level	O
features	O
for	O
language	Task
processing	Task
.	O
These	O
include	O
using	O
character	Method
-	Method
level	Method
n	Method
-	Method
grams	Method
with	O
linear	Method
classifiers	Method
,	O
and	O
incorporating	O
character	O
-	O
level	O
features	O
to	O
ConvNets	Method
.	O
In	O
particular	O
,	O
these	O
ConvNet	Method
approaches	Method
use	O
words	O
as	O
a	O
basis	O
,	O
in	O
which	O
character	O
-	O
level	O
features	O
extracted	O
at	O
word	O
or	O
word	O
n	O
-	O
gram	O
level	O
form	O
a	O
distributed	Method
representation	Method
.	O
Improvements	O
for	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
and	O
information	Task
retrieval	Task
were	O
observed	O
.	O
This	O
article	O
is	O
the	O
first	O
to	O
apply	O
ConvNets	Method
only	O
on	O
characters	O
.	O
We	O
show	O
that	O
when	O
trained	O
on	O
large	O
-	O
scale	O
datasets	O
,	O
deep	O
ConvNets	Method
do	O
not	O
require	O
the	O
knowledge	O
of	O
words	O
,	O
in	O
addition	O
to	O
the	O
conclusion	O
from	O
previous	O
research	O
that	O
ConvNets	Method
do	O
not	O
require	O
the	O
knowledge	O
about	O
the	O
syntactic	O
or	O
semantic	O
structure	O
of	O
a	O
language	O
.	O
This	O
simplification	O
of	O
engineering	Task
could	O
be	O
crucial	O
for	O
a	O
single	O
system	O
that	O
can	O
work	O
for	O
different	O
languages	O
,	O
since	O
characters	O
always	O
constitute	O
a	O
necessary	O
construct	O
regardless	O
of	O
whether	O
segmentation	O
into	O
words	O
is	O
possible	O
.	O
Working	O
on	O
only	O
characters	O
also	O
has	O
the	O
advantage	O
that	O
abnormal	O
character	O
combinations	O
such	O
as	O
misspellings	O
and	O
emoticons	O
may	O
be	O
naturally	O
learnt	O
.	O
section	O
:	O
Character	Method
-	Method
level	Method
Convolutional	Method
Networks	Method
In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
design	O
of	O
character	O
-	O
level	O
ConvNets	Method
for	O
text	Task
classification	Task
.	O
The	O
design	O
is	O
modular	O
,	O
where	O
the	O
gradients	O
are	O
obtained	O
by	O
back	Method
-	Method
propagation	Method
to	O
perform	O
optimization	Task
.	O
subsection	O
:	O
Key	O
Modules	O
The	O
main	O
component	O
is	O
the	O
temporal	Method
convolutional	Method
module	Method
,	O
which	O
simply	O
computes	O
a	O
1	Method
-	Method
D	Method
convolution	Method
.	O
Suppose	O
we	O
have	O
a	O
discrete	O
input	O
function	O
and	O
a	O
discrete	Method
kernel	Method
function	Method
.	O
The	O
convolution	O
between	O
and	O
with	O
stride	O
is	O
defined	O
as	O
where	O
is	O
an	O
offset	O
constant	O
.	O
Just	O
as	O
in	O
traditional	O
convolutional	Method
networks	Method
in	O
vision	Task
,	O
the	O
module	O
is	O
parameterized	O
by	O
a	O
set	O
of	O
such	O
kernel	Method
functions	Method
which	O
we	O
call	O
weights	O
,	O
on	O
a	O
set	O
of	O
inputs	O
and	O
outputs	O
.	O
We	O
call	O
each	O
(	O
or	O
)	O
input	O
(	O
or	O
output	O
)	O
features	O
,	O
and	O
(	O
or	O
)	O
input	O
(	O
or	O
output	O
)	O
feature	O
size	O
.	O
The	O
outputs	O
is	O
obtained	O
by	O
a	O
sum	O
over	O
of	O
the	O
convolutions	O
between	O
and	O
.	O
One	O
key	O
module	O
that	O
helped	O
us	O
to	O
train	O
deeper	Method
models	Method
is	O
temporal	Method
max	Method
-	Method
pooling	Method
.	O
It	O
is	O
the	O
1	O
-	O
D	O
version	O
of	O
the	O
max	Method
-	Method
pooling	Method
module	Method
used	O
in	O
computer	Task
vision	Task
.	O
Given	O
a	O
discrete	O
input	O
function	O
,	O
the	O
max	Method
-	Method
pooling	Method
function	Method
of	O
is	O
defined	O
as	O
where	O
is	O
an	O
offset	O
constant	O
.	O
This	O
very	O
pooling	Method
module	Method
enabled	O
us	O
to	O
train	O
ConvNets	Method
deeper	O
than	O
6	O
layers	O
,	O
where	O
all	O
others	O
fail	O
.	O
The	O
analysis	O
by	O
might	O
shed	O
some	O
light	O
on	O
this	O
.	O
The	O
non	O
-	O
linearity	O
used	O
in	O
our	O
model	O
is	O
the	O
rectifier	Method
or	Method
thresholding	Method
function	Method
,	O
which	O
makes	O
our	O
convolutional	Method
layers	Method
similar	O
to	O
rectified	Method
linear	Method
units	Method
(	O
ReLUs	Method
)	O
.	O
The	O
algorithm	O
used	O
is	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
a	O
minibatch	O
of	O
size	O
128	O
,	O
using	O
momentum	O
and	O
initial	O
step	O
size	O
which	O
is	O
halved	O
every	O
3	O
epoches	O
for	O
10	O
times	O
.	O
Each	O
epoch	O
takes	O
a	O
fixed	O
number	O
of	O
random	O
training	O
samples	O
uniformly	O
sampled	O
across	O
classes	O
.	O
This	O
number	O
will	O
later	O
be	O
detailed	O
for	O
each	O
dataset	O
sparately	O
.	O
The	O
implementation	O
is	O
done	O
using	O
Torch	Method
7	Method
.	O
subsection	O
:	O
Character	Task
quantization	Task
Our	O
models	O
accept	O
a	O
sequence	O
of	O
encoded	O
characters	O
as	O
input	O
.	O
The	O
encoding	O
is	O
done	O
by	O
prescribing	O
an	O
alphabet	O
of	O
size	O
for	O
the	O
input	O
language	O
,	O
and	O
then	O
quantize	O
each	O
character	O
using	O
1	Method
-	Method
of	Method
-	Method
encoding	Method
(	O
or	O
“	O
one	Method
-	Method
hot	Method
”	Method
encoding	Method
)	O
.	O
Then	O
,	O
the	O
sequence	O
of	O
characters	O
is	O
transformed	O
to	O
a	O
sequence	O
of	O
such	O
sized	O
vectors	O
with	O
fixed	O
length	O
.	O
Any	O
character	O
exceeding	O
length	O
is	O
ignored	O
,	O
and	O
any	O
characters	O
that	O
are	O
not	O
in	O
the	O
alphabet	O
including	O
blank	O
characters	O
are	O
quantized	O
as	O
all	O
-	O
zero	O
vectors	O
.	O
The	O
character	O
quantization	O
order	O
is	O
backward	O
so	O
that	O
the	O
latest	O
reading	O
on	O
characters	O
is	O
always	O
placed	O
near	O
the	O
begin	O
of	O
the	O
output	O
,	O
making	O
it	O
easy	O
for	O
fully	O
connected	O
layers	O
to	O
associate	O
weights	O
with	O
the	O
latest	O
reading	O
.	O
The	O
alphabet	O
used	O
in	O
all	O
of	O
our	O
models	O
consists	O
of	O
70	O
characters	O
,	O
including	O
26	O
english	O
letters	O
,	O
10	O
digits	O
,	O
33	O
other	O
characters	O
and	O
the	O
new	O
line	O
character	O
.	O
The	O
non	O
-	O
space	O
characters	O
are	O
:	O
Later	O
we	O
also	O
compare	O
with	O
models	O
that	O
use	O
a	O
different	O
alphabet	O
in	O
which	O
we	O
distinguish	O
between	O
upper	O
-	O
case	O
and	O
lower	O
-	O
case	O
letters	O
.	O
subsection	O
:	O
Model	O
Design	O
We	O
designed	O
2	O
ConvNets	Method
–	O
one	O
large	O
and	O
one	O
small	O
.	O
They	O
are	O
both	O
9	O
layers	O
deep	O
with	O
6	O
convolutional	Method
layers	Method
and	O
3	O
fully	Method
-	Method
connected	Method
layers	Method
.	O
Figure	O
[	O
reference	O
]	O
gives	O
an	O
illustration	O
.	O
The	O
input	O
have	O
number	O
of	O
features	O
equal	O
to	O
70	O
due	O
to	O
our	O
character	Method
quantization	Method
method	Method
,	O
and	O
the	O
input	O
feature	O
length	O
is	O
1014	O
.	O
It	O
seems	O
that	O
1014	O
characters	O
could	O
already	O
capture	O
most	O
of	O
the	O
texts	O
of	O
interest	O
.	O
We	O
also	O
insert	O
2	O
dropout	Method
modules	Method
in	O
between	O
the	O
3	O
fully	Method
-	Method
connected	Method
layers	Method
to	O
regularize	O
.	O
They	O
have	O
dropout	Metric
probability	Metric
of	O
0.5	O
.	O
Table	O
[	O
reference	O
]	O
lists	O
the	O
configurations	O
for	O
convolutional	Method
layers	Method
,	O
and	O
table	O
[	O
reference	O
]	O
lists	O
the	O
configurations	O
for	O
fully	Method
-	Method
connected	Method
(	Method
linear	Method
)	Method
layers	Method
.	O
We	O
initialize	O
the	O
weights	O
using	O
a	O
Gaussian	Method
distribution	Method
.	O
The	O
mean	O
and	O
standard	O
deviation	O
used	O
for	O
initializing	O
the	O
large	Method
model	Method
is	O
and	O
small	Method
model	Method
.	O
For	O
different	O
problems	O
the	O
input	O
lengths	O
may	O
be	O
different	O
(	O
for	O
example	O
in	O
our	O
case	O
)	O
,	O
and	O
so	O
are	O
the	O
frame	O
lengths	O
.	O
From	O
our	O
model	O
design	O
,	O
it	O
is	O
easy	O
to	O
know	O
that	O
given	O
input	O
length	O
,	O
the	O
output	O
frame	O
length	O
after	O
the	O
last	O
convolutional	Method
layer	Method
(	O
but	O
before	O
any	O
of	O
the	O
fully	O
-	O
connected	O
layers	O
)	O
is	O
.	O
This	O
number	O
multiplied	O
with	O
the	O
frame	O
size	O
at	O
layer	O
6	O
will	O
give	O
the	O
input	O
dimension	O
the	O
first	O
fully	Method
-	Method
connected	Method
layer	Method
accepts	O
.	O
subsection	O
:	O
Data	Task
Augmentation	Task
using	O
Thesaurus	O
Many	O
researchers	O
have	O
found	O
that	O
appropriate	O
data	Method
augmentation	Method
techniques	Method
are	O
useful	O
for	O
controlling	O
generalization	Task
error	Task
for	O
deep	Method
learning	Method
models	Method
.	O
These	O
techniques	O
usually	O
work	O
well	O
when	O
we	O
could	O
find	O
appropriate	O
invariance	O
properties	O
that	O
the	O
model	O
should	O
possess	O
.	O
In	O
terms	O
of	O
texts	O
,	O
it	O
is	O
not	O
reasonable	O
to	O
augment	O
the	O
data	O
using	O
signal	Method
transformations	Method
as	O
done	O
in	O
image	Task
or	Task
speech	Task
recognition	Task
,	O
because	O
the	O
exact	O
order	O
of	O
characters	O
may	O
form	O
rigorous	O
syntactic	O
and	O
semantic	O
meaning	O
.	O
Therefore	O
,	O
the	O
best	O
way	O
to	O
do	O
data	Task
augmentation	Task
would	O
have	O
been	O
using	O
human	O
rephrases	O
of	O
sentences	O
,	O
but	O
this	O
is	O
unrealistic	O
and	O
expensive	O
due	O
the	O
large	O
volume	O
of	O
samples	O
in	O
our	O
datasets	O
.	O
As	O
a	O
result	O
,	O
the	O
most	O
natural	O
choice	O
in	O
data	Task
augmentation	Task
for	O
us	O
is	O
to	O
replace	O
words	O
or	O
phrases	O
with	O
their	O
synonyms	O
.	O
We	O
experimented	O
data	Task
augmentation	Task
by	O
using	O
an	O
English	O
thesaurus	O
,	O
which	O
is	O
obtained	O
from	O
the	O
mytheas	Method
component	Method
used	O
in	O
LibreOffice	O
project	O
.	O
That	O
thesaurus	O
in	O
turn	O
was	O
obtained	O
from	O
WordNet	Material
,	O
where	O
every	O
synonym	O
to	O
a	O
word	O
or	O
phrase	O
is	O
ranked	O
by	O
the	O
semantic	O
closeness	O
to	O
the	O
most	O
frequently	O
seen	O
meaning	O
.	O
To	O
decide	O
on	O
how	O
many	O
words	O
to	O
replace	O
,	O
we	O
extract	O
all	O
replaceable	O
words	O
from	O
the	O
given	O
text	O
and	O
randomly	O
choose	O
of	O
them	O
to	O
be	O
replaced	O
.	O
The	O
probability	O
of	O
number	O
is	O
determined	O
by	O
a	O
geometric	Method
distribution	Method
with	O
parameter	O
in	O
which	O
.	O
The	O
index	O
of	O
the	O
synonym	O
chosen	O
given	O
a	O
word	O
is	O
also	O
determined	O
by	O
a	O
another	O
geometric	Method
distribution	Method
in	O
which	O
.	O
This	O
way	O
,	O
the	O
probability	O
of	O
a	O
synonym	O
chosen	O
becomes	O
smaller	O
when	O
it	O
moves	O
distant	O
from	O
the	O
most	O
frequently	O
seen	O
meaning	O
.	O
We	O
will	O
report	O
the	O
results	O
using	O
this	O
new	O
data	Method
augmentation	Method
technique	Method
with	O
and	O
.	O
section	O
:	O
Comparison	O
Models	O
To	O
offer	O
fair	O
comparisons	O
to	O
competitive	O
models	O
,	O
we	O
conducted	O
a	O
series	O
of	O
experiments	O
with	O
both	O
traditional	Method
and	Method
deep	Method
learning	Method
methods	Method
.	O
We	O
tried	O
our	O
best	O
to	O
choose	O
models	O
that	O
can	O
provide	O
comparable	O
and	O
competitive	O
results	O
,	O
and	O
the	O
results	O
are	O
reported	O
faithfully	O
without	O
any	O
model	Method
selection	Method
.	O
subsection	O
:	O
Traditional	O
Methods	O
We	O
refer	O
to	O
traditional	O
methods	O
as	O
those	O
that	O
using	O
a	O
hand	Method
-	Method
crafted	Method
feature	Method
extractor	Method
and	O
a	O
linear	Method
classifier	Method
.	O
The	O
classifier	Method
used	O
is	O
a	O
multinomial	Method
logistic	Method
regression	Method
in	O
all	O
these	O
models	O
.	O
Bag	O
-	O
of	O
-	O
words	O
and	O
its	O
TFIDF	Method
.	O
For	O
each	O
dataset	O
,	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
model	Method
is	O
constructed	O
by	O
selecting	O
50	O
,	O
000	O
most	O
frequent	O
words	O
from	O
the	O
training	O
subset	O
.	O
For	O
the	O
normal	Task
bag	Task
-	Task
of	Task
-	Task
words	Task
,	O
we	O
use	O
the	O
counts	O
of	O
each	O
word	O
as	O
the	O
features	O
.	O
For	O
the	O
TFIDF	Method
(	Method
term	Method
-	Method
frequency	Method
inverse	Method
-	Method
document	Method
-	Method
frequency	Method
)	Method
version	Method
,	O
we	O
use	O
the	O
counts	O
as	O
the	O
term	O
-	O
frequency	O
.	O
The	O
inverse	O
document	O
frequency	O
is	O
the	O
logarithm	O
of	O
the	O
division	O
between	O
total	O
number	O
of	O
samples	O
and	O
number	O
of	O
samples	O
with	O
the	O
word	O
in	O
the	O
training	O
subset	O
.	O
The	O
features	O
are	O
normalized	O
by	O
dividing	O
the	O
largest	O
feature	O
value	O
.	O
Bag	Method
-	Method
of	Method
-	Method
ngrams	Method
and	O
its	O
TFIDF	Method
.	O
The	O
bag	Method
-	Method
of	Method
-	Method
ngrams	Method
models	Method
are	O
constructed	O
by	O
selecting	O
the	O
500	O
,	O
000	O
most	O
frequent	O
n	O
-	O
grams	O
(	O
up	O
to	O
5	O
-	O
grams	O
)	O
from	O
the	O
training	O
subset	O
for	O
each	O
dataset	O
.	O
The	O
feature	O
values	O
are	O
computed	O
the	O
same	O
way	O
as	O
in	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
model	Method
.	O
Bag	Method
-	Method
of	Method
-	Method
means	Method
on	O
word	Method
embedding	Method
.	O
We	O
also	O
have	O
an	O
experimental	O
model	O
that	O
uses	O
k	Method
-	Method
means	Method
on	O
word2vec	Method
learnt	O
from	O
the	O
training	O
subset	O
of	O
each	O
dataset	O
,	O
and	O
then	O
use	O
these	O
learnt	O
means	O
as	O
representatives	O
of	O
the	O
clustered	O
words	O
.	O
We	O
take	O
into	O
consideration	O
all	O
the	O
words	O
that	O
appeared	O
more	O
than	O
5	O
times	O
in	O
the	O
training	O
subset	O
.	O
The	O
dimension	O
of	O
the	O
embedding	O
is	O
300	O
.	O
The	O
bag	O
-	O
of	O
-	O
means	O
features	O
are	O
computed	O
the	O
same	O
way	O
as	O
in	O
the	O
bag	Method
-	Method
of	Method
-	Method
words	Method
model	Method
.	O
The	O
number	O
of	O
means	O
is	O
5000	O
.	O
subsection	O
:	O
Deep	Method
Learning	Method
Methods	Method
Recently	O
deep	Method
learning	Method
methods	Method
have	O
started	O
to	O
be	O
applied	O
to	O
text	Task
classification	Task
.	O
We	O
choose	O
two	O
simple	O
and	O
representative	O
models	O
for	O
comparison	O
,	O
in	O
which	O
one	O
is	O
word	Method
-	Method
based	Method
ConvNet	Method
and	O
the	O
other	O
a	O
simple	O
long	Method
-	Method
short	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
recurrent	O
neural	O
network	O
model	O
.	O
Word	O
-	O
based	O
ConvNets	Method
.	O
Among	O
the	O
large	O
number	O
of	O
recent	O
works	O
on	O
word	O
-	O
based	O
ConvNets	Method
for	O
text	Task
classification	Task
,	O
one	O
of	O
the	O
differences	O
is	O
the	O
choice	O
of	O
using	O
pretrained	O
or	O
end	Method
-	Method
to	Method
-	Method
end	Method
learned	Method
word	Method
representations	Method
.	O
We	O
offer	O
comparisons	O
with	O
both	O
using	O
the	O
pretrained	Method
word2vec	Method
embedding	Method
and	O
using	O
lookup	Method
tables	Method
.	O
The	O
embedding	Metric
size	Metric
is	O
300	O
in	O
both	O
cases	O
,	O
in	O
the	O
same	O
way	O
as	O
our	O
bag	Method
-	Method
of	Method
-	Method
means	Method
model	Method
.	O
To	O
ensure	O
fair	O
comparison	O
,	O
the	O
models	O
for	O
each	O
case	O
are	O
of	O
the	O
same	O
size	O
as	O
our	O
character	O
-	O
level	O
ConvNets	Method
,	O
in	O
terms	O
of	O
both	O
the	O
number	O
of	O
layers	O
and	O
each	O
layer	O
’s	O
output	O
size	O
.	O
Experiments	O
using	O
a	O
thesaurus	Method
for	O
data	Task
augmentation	Task
are	O
also	O
conducted	O
.	O
Long	Task
-	Task
short	Task
term	Task
memory	Task
.	O
We	O
also	O
offer	O
a	O
comparison	O
with	O
a	O
recurrent	Method
neural	Method
network	Method
model	Method
,	O
namely	O
long	Method
-	Method
short	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
.	O
The	O
LSTM	Method
model	O
used	O
in	O
our	O
case	O
is	O
word	Task
-	Task
based	Task
,	O
using	O
pretrained	Method
word2vec	Method
embedding	Method
of	O
size	O
300	O
as	O
in	O
previous	O
models	O
.	O
The	O
model	O
is	O
formed	O
by	O
taking	O
mean	O
of	O
the	O
outputs	O
of	O
all	O
LSTM	Method
cells	O
to	O
form	O
a	O
feature	O
vector	O
,	O
and	O
then	O
using	O
multinomial	Method
logistic	Method
regression	Method
on	O
this	O
feature	O
vector	O
.	O
The	O
output	O
dimension	O
is	O
512	O
.	O
The	O
variant	O
of	O
LSTM	Method
we	O
used	O
is	O
the	O
common	O
“	Method
vanilla	Method
”	Method
architecture	Method
.	O
We	O
also	O
used	O
gradient	Method
clipping	Method
in	O
which	O
the	O
gradient	O
norm	O
is	O
limited	O
to	O
5	O
.	O
Figure	O
[	O
reference	O
]	O
gives	O
an	O
illustration	O
.	O
subsection	O
:	O
Choice	O
of	O
Alphabet	O
For	O
the	O
alphabet	O
of	O
English	O
,	O
one	O
apparent	O
choice	O
is	O
whether	O
to	O
distinguish	O
between	O
upper	O
-	O
case	O
and	O
lower	O
-	O
case	O
letters	O
.	O
We	O
report	O
experiments	O
on	O
this	O
choice	O
and	O
observed	O
that	O
it	O
usually	O
(	O
but	O
not	O
always	O
)	O
gives	O
worse	O
results	O
when	O
such	O
distinction	O
is	O
made	O
.	O
One	O
possible	O
explanation	O
might	O
be	O
that	O
semantics	O
do	O
not	O
change	O
with	O
different	O
letter	O
cases	O
,	O
therefore	O
there	O
is	O
a	O
benefit	O
of	O
regularization	O
.	O
section	O
:	O
Large	O
-	O
scale	O
Datasets	O
and	O
Results	O
Previous	O
research	O
on	O
ConvNets	Method
in	O
different	O
areas	O
has	O
shown	O
that	O
they	O
usually	O
work	O
well	O
with	O
large	O
-	O
scale	O
datasets	O
,	O
especially	O
when	O
the	O
model	O
takes	O
in	O
low	O
-	O
level	O
raw	O
features	O
like	O
characters	O
in	O
our	O
case	O
.	O
However	O
,	O
most	O
open	O
datasets	O
for	O
text	Task
classification	Task
are	O
quite	O
small	O
,	O
and	O
large	O
-	O
scale	O
datasets	O
are	O
splitted	O
with	O
a	O
significantly	O
smaller	O
training	O
set	O
than	O
testing	O
.	O
Therefore	O
,	O
instead	O
of	O
confusing	O
our	O
community	O
more	O
by	O
using	O
them	O
,	O
we	O
built	O
several	O
large	O
-	O
scale	O
datasets	O
for	O
our	O
experiments	O
,	O
ranging	O
from	O
hundreds	O
of	O
thousands	O
to	O
several	O
millions	O
of	O
samples	O
.	O
Table	O
[	O
reference	O
]	O
is	O
a	O
summary	O
.	O
AG	Material
’s	Material
news	Material
corpus	Material
.	O
We	O
obtained	O
the	O
AG	Material
’s	Material
corpus	Material
of	Material
news	Material
article	Material
on	O
the	O
web	O
.	O
It	O
contains	O
496	O
,	O
835	O
categorized	O
news	Material
articles	Material
from	O
more	O
than	O
2000	O
news	O
sources	O
.	O
We	O
choose	O
the	O
4	O
largest	O
classes	O
from	O
this	O
corpus	O
to	O
construct	O
our	O
dataset	O
,	O
using	O
only	O
the	O
title	O
and	O
description	O
fields	O
.	O
The	O
number	O
of	O
training	O
samples	O
for	O
each	O
class	O
is	O
30	O
,	O
000	O
and	O
testing	O
1900	O
.	O
Sogou	Material
news	Material
corpus	Material
.	O
This	O
dataset	O
is	O
a	O
combination	O
of	O
the	O
SogouCA	Material
and	O
SogouCS	Material
news	Material
corpora	Material
,	O
containing	O
in	O
total	O
2	O
,	O
909	O
,	O
551	O
news	O
articles	O
in	O
various	O
topic	O
channels	O
.	O
We	O
then	O
labeled	O
each	O
piece	O
of	O
news	O
using	O
its	O
URL	O
,	O
by	O
manually	O
classifying	O
the	O
their	O
domain	O
names	O
.	O
This	O
gives	O
us	O
a	O
large	O
corpus	O
of	O
news	O
articles	O
labeled	O
with	O
their	O
categories	O
.	O
There	O
are	O
a	O
large	O
number	O
categories	O
but	O
most	O
of	O
them	O
contain	O
only	O
few	O
articles	O
.	O
We	O
choose	O
5	O
categories	O
–	O
“	O
sports	O
”	O
,	O
“	O
finance	Task
”	O
,	O
“	O
entertainment	O
”	O
,	O
“	O
automobile	O
”	O
and	O
“	O
technology	O
”	O
.	O
The	O
number	O
of	O
training	O
samples	O
selected	O
for	O
each	O
class	O
is	O
90	O
,	O
000	O
and	O
testing	O
12	O
,	O
000	O
.	O
Although	O
this	O
is	O
a	O
dataset	O
in	O
Chinese	O
,	O
we	O
used	O
pypinyin	Method
package	Method
combined	O
with	O
jieba	Method
Chinese	Method
segmentation	Method
system	Method
to	O
produce	O
Pinyin	Method
–	O
a	O
phonetic	O
romanization	O
of	O
Chinese	O
.	O
The	O
models	O
for	O
English	O
can	O
then	O
be	O
applied	O
to	O
this	O
dataset	O
without	O
change	O
.	O
The	O
fields	O
used	O
are	O
title	O
and	O
content	O
.	O
DBPedia	Material
ontology	Material
dataset	Material
.	O
DBpedia	Material
is	O
a	O
crowd	O
-	O
sourced	O
community	O
effort	O
to	O
extract	O
structured	O
information	O
from	O
Wikipedia	Material
.	O
The	O
DBpedia	Material
ontology	O
dataset	O
is	O
constructed	O
by	O
picking	O
14	O
non	O
-	O
overlapping	O
classes	O
from	O
DBpedia	Material
2014	O
.	O
From	O
each	O
of	O
these	O
14	O
ontology	O
classes	O
,	O
we	O
randomly	O
choose	O
40	O
,	O
000	O
training	O
samples	O
and	O
5	O
,	O
000	O
testing	O
samples	O
.	O
The	O
fields	O
we	O
used	O
for	O
this	O
dataset	O
contain	O
title	O
and	O
abstract	O
of	O
each	O
Wikipedia	O
article	O
.	O
Yelp	Material
reviews	Material
.	O
The	O
Yelp	Material
reviews	O
dataset	O
is	O
obtained	O
from	O
the	O
Yelp	Material
Dataset	O
Challenge	O
in	O
2015	O
.	O
This	O
dataset	O
contains	O
1	O
,	O
569	O
,	O
264	O
samples	O
that	O
have	O
review	O
texts	O
.	O
Two	O
classification	Task
tasks	Task
are	O
constructed	O
from	O
this	O
dataset	O
–	O
one	O
predicting	O
full	O
number	O
of	O
stars	O
the	O
user	O
has	O
given	O
,	O
and	O
the	O
other	O
predicting	O
a	O
polarity	O
label	O
by	O
considering	O
stars	O
1	O
and	O
2	O
negative	O
,	O
and	O
3	O
and	O
4	O
positive	O
.	O
The	O
full	O
dataset	O
has	O
130	O
,	O
000	O
training	O
samples	O
and	O
10	O
,	O
000	O
testing	O
samples	O
in	O
each	O
star	O
,	O
and	O
the	O
polarity	O
dataset	O
has	O
280	O
,	O
000	O
training	O
samples	O
and	O
19	O
,	O
000	O
test	O
samples	O
in	O
each	O
polarity	O
.	O
Yahoo	O
!	O
Answers	O
dataset	O
.	O
We	O
obtained	O
Yahoo	O
!	O
Answers	O
Comprehensive	O
Questions	O
and	O
Answers	O
version	O
1.0	O
dataset	O
through	O
the	O
Yahoo	O
!	O
Webscope	Method
program	Method
.	O
The	O
corpus	O
contains	O
4	O
,	O
483	O
,	O
032	O
questions	O
and	O
their	O
answers	O
.	O
We	O
constructed	O
a	O
topic	Method
classification	Method
dataset	Method
from	O
this	O
corpus	O
using	O
10	O
largest	O
main	O
categories	O
.	O
Each	O
class	O
contains	O
140	O
,	O
000	O
training	O
samples	O
and	O
5	O
,	O
000	O
testing	O
samples	O
.	O
The	O
fields	O
we	O
used	O
include	O
question	O
title	O
,	O
question	O
content	O
and	O
best	O
answer	O
.	O
Amazon	O
reviews	O
.	O
We	O
obtained	O
an	O
Amazon	Material
review	Material
dataset	Material
from	O
the	O
Stanford	Material
Network	Material
Analysis	Material
Project	Material
(	O
SNAP	Material
)	O
,	O
which	O
spans	O
18	O
years	O
with	O
34	O
,	O
686	O
,	O
770	O
reviews	O
from	O
6	O
,	O
643	O
,	O
669	O
users	O
on	O
2	O
,	O
441	O
,	O
053	O
products	O
.	O
Similarly	O
to	O
the	O
Yelp	Material
review	O
dataset	O
,	O
we	O
also	O
constructed	O
2	O
datasets	O
–	O
one	O
full	Method
score	Method
prediction	Method
and	O
another	O
polarity	Task
prediction	Task
.	O
The	O
full	O
dataset	O
contains	O
600	O
,	O
000	O
training	O
samples	O
and	O
130	O
,	O
000	O
testing	O
samples	O
in	O
each	O
class	O
,	O
whereas	O
the	O
polarity	O
dataset	O
contains	O
1	O
,	O
800	O
,	O
000	O
training	O
samples	O
and	O
200	O
,	O
000	O
testing	O
samples	O
in	O
each	O
polarity	O
sentiment	O
.	O
The	O
fields	O
used	O
are	O
review	O
title	O
and	O
review	O
content	O
.	O
Table	O
[	O
reference	O
]	O
lists	O
all	O
the	O
testing	O
errors	O
we	O
obtained	O
from	O
these	O
datasets	O
for	O
all	O
the	O
applicable	O
models	O
.	O
Note	O
that	O
since	O
we	O
do	O
not	O
have	O
a	O
Chinese	O
thesaurus	O
,	O
the	O
Sogou	Material
News	Material
dataset	Material
does	O
not	O
have	O
any	O
results	O
using	O
thesaurus	Method
augmentation	Method
.	O
We	O
labeled	O
the	O
best	O
result	O
in	O
blue	O
and	O
worse	O
result	O
in	O
red	O
.	O
section	O
:	O
Discussion	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.7	O
To	O
understand	O
the	O
results	O
in	O
table	O
[	O
reference	O
]	O
further	O
,	O
we	O
offer	O
some	O
empirical	O
analysis	O
in	O
this	O
section	O
.	O
To	O
facilitate	O
our	O
analysis	O
,	O
we	O
present	O
the	O
relative	Metric
errors	Metric
in	O
figure	O
[	O
reference	O
]	O
with	O
respect	O
to	O
comparison	O
models	O
.	O
Each	O
of	O
these	O
plots	O
is	O
computed	O
by	O
taking	O
the	O
difference	O
between	O
errors	O
on	O
comparison	Method
model	Method
and	O
our	O
character	Method
-	Method
level	Method
ConvNet	Method
model	Method
,	O
then	O
divided	O
by	O
the	O
comparison	Metric
model	Metric
error	Metric
.	O
All	O
ConvNets	Method
in	O
the	O
figure	O
are	O
the	O
large	Method
models	Method
with	O
thesaurus	Method
augmentation	Method
respectively	O
.	O
Character	Method
-	Method
level	Method
ConvNet	Method
is	O
an	O
effective	O
method	O
.	O
The	O
most	O
important	O
conclusion	O
from	O
our	O
experiments	O
is	O
that	O
character	O
-	O
level	O
ConvNets	Method
could	O
work	O
for	O
text	Task
classification	Task
without	O
the	O
need	O
for	O
words	O
.	O
This	O
is	O
a	O
strong	O
indication	O
that	O
language	O
could	O
also	O
be	O
thought	O
of	O
as	O
a	O
signal	O
no	O
different	O
from	O
any	O
other	O
kind	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
12	O
random	O
first	O
-	O
layer	O
patches	O
learnt	O
by	O
one	O
of	O
our	O
character	O
-	O
level	O
ConvNets	Method
for	O
DBPedia	Material
dataset	Material
.	O
Dataset	Metric
size	Metric
forms	O
a	O
dichotomy	O
between	O
traditional	O
and	O
ConvNets	Method
models	O
.	O
The	O
most	O
obvious	O
trend	O
coming	O
from	O
all	O
the	O
plots	O
in	O
figure	O
[	O
reference	O
]	O
is	O
that	O
the	O
larger	O
datasets	O
tend	O
to	O
perform	O
better	O
.	O
Traditional	O
methods	O
like	O
n	Method
-	Method
grams	Method
TFIDF	Method
remain	O
strong	O
candidates	O
for	O
dataset	O
of	O
size	O
up	O
to	O
several	O
hundreds	O
of	O
thousands	O
,	O
and	O
only	O
until	O
the	O
dataset	O
goes	O
to	O
the	O
scale	O
of	O
several	O
millions	O
do	O
we	O
observe	O
that	O
character	O
-	O
level	O
ConvNets	Method
start	O
to	O
do	O
better	O
.	O
ConvNets	Method
may	O
work	O
well	O
for	O
user	O
-	O
generated	O
data	O
.	O
User	O
-	O
generated	O
data	O
vary	O
in	O
the	O
degree	O
of	O
how	O
well	O
the	O
texts	O
are	O
curated	O
.	O
For	O
example	O
,	O
in	O
our	O
million	O
scale	O
datasets	O
,	O
Amazon	O
reviews	O
tend	O
to	O
be	O
raw	O
user	O
-	O
inputs	O
,	O
whereas	O
users	O
might	O
be	O
extra	O
careful	O
in	O
their	O
writings	O
on	O
Yahoo	O
!	O
Answers	O
.	O
Plots	O
comparing	O
word	Method
-	Method
based	Method
deep	Method
models	Method
(	O
figures	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
show	O
that	O
character	O
-	O
level	O
ConvNets	Method
work	O
better	O
for	O
less	O
curated	O
user	O
-	O
generated	O
texts	O
.	O
This	O
property	O
suggests	O
that	O
ConvNets	Method
may	O
have	O
better	O
applicability	O
to	O
real	Task
-	Task
world	Task
scenarios	Task
.	O
However	O
,	O
further	O
analysis	O
is	O
needed	O
to	O
validate	O
the	O
hypothesis	O
that	O
ConvNets	Method
are	O
truly	O
good	O
at	O
identifying	O
exotic	O
character	O
combinations	O
such	O
as	O
misspellings	O
and	O
emoticons	O
,	O
as	O
our	O
experiments	O
alone	O
do	O
not	O
show	O
any	O
explicit	O
evidence	O
.	O
Choice	O
of	O
alphabet	O
makes	O
a	O
difference	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
that	O
changing	O
the	O
alphabet	O
by	O
distinguishing	O
between	O
uppercase	O
and	O
lowercase	O
letters	O
could	O
make	O
a	O
difference	O
.	O
For	O
million	O
-	O
scale	O
datasets	O
,	O
it	O
seems	O
that	O
not	O
making	O
such	O
distinction	O
usually	O
works	O
better	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
there	O
is	O
a	O
regularization	O
effect	O
,	O
but	O
this	O
is	O
to	O
be	O
validated	O
.	O
Semantics	Task
of	Task
tasks	Task
may	O
not	O
matter	O
.	O
Our	O
datasets	O
consist	O
of	O
two	O
kinds	O
of	O
tasks	O
:	O
sentiment	Task
analysis	Task
(	O
Yelp	Material
and	O
Amazon	Material
reviews	Material
)	O
and	O
topic	Task
classification	Task
(	O
all	O
others	O
)	O
.	O
This	O
dichotomy	O
in	O
task	O
semantics	O
does	O
not	O
seem	O
to	O
play	O
a	O
role	O
in	O
deciding	O
which	O
method	O
is	O
better	O
.	O
Bag	Method
-	Method
of	Method
-	Method
means	Method
is	O
a	O
misuse	O
of	O
word2vec	Method
[	O
]	O
.	O
One	O
of	O
the	O
most	O
obvious	O
facts	O
one	O
could	O
observe	O
from	O
table	O
[	O
reference	O
]	O
and	O
figure	O
[	O
reference	O
]	O
is	O
that	O
the	O
bag	Method
-	Method
of	Method
-	Method
means	Method
model	Method
performs	O
worse	O
in	O
every	O
case	O
.	O
Comparing	O
with	O
traditional	O
models	O
,	O
this	O
suggests	O
such	O
a	O
simple	O
use	O
of	O
a	O
distributed	Method
word	Method
representation	Method
may	O
not	O
give	O
us	O
an	O
advantage	O
to	O
text	Task
classification	Task
.	O
However	O
,	O
our	O
experiments	O
does	O
not	O
speak	O
for	O
any	O
other	O
language	Task
processing	Task
tasks	Task
or	O
use	O
of	O
word2vec	O
in	O
any	O
other	O
way	O
.	O
There	O
is	O
no	O
free	O
lunch	O
.	O
Our	O
experiments	O
once	O
again	O
verifies	O
that	O
there	O
is	O
not	O
a	O
single	O
machine	Method
learning	Method
model	Method
that	O
can	O
work	O
for	O
all	O
kinds	O
of	O
datasets	O
.	O
The	O
factors	O
discussed	O
in	O
this	O
section	O
could	O
all	O
play	O
a	O
role	O
in	O
deciding	O
which	O
method	O
is	O
the	O
best	O
for	O
some	O
specific	O
application	O
.	O
section	O
:	O
Conclusion	O
and	O
Outlook	O
This	O
article	O
offers	O
an	O
empirical	O
study	O
on	O
character	Method
-	Method
level	Method
convolutional	Method
networks	Method
for	O
text	Task
classification	Task
.	O
We	O
compared	O
with	O
a	O
large	O
number	O
of	O
traditional	O
and	Method
deep	Method
learning	Method
models	Method
using	O
several	O
large	O
-	O
scale	O
datasets	O
.	O
On	O
one	O
hand	O
,	O
analysis	O
shows	O
that	O
character	Method
-	Method
level	Method
ConvNet	Method
is	O
an	O
effective	O
method	O
.	O
On	O
the	O
other	O
hand	O
,	O
how	O
well	O
our	O
model	O
performs	O
in	O
comparisons	O
depends	O
on	O
many	O
factors	O
,	O
such	O
as	O
dataset	O
size	O
,	O
whether	O
the	O
texts	O
are	O
curated	O
and	O
choice	O
of	O
alphabet	O
.	O
In	O
the	O
future	O
,	O
we	O
hope	O
to	O
apply	O
character	O
-	O
level	O
ConvNets	Method
for	O
a	O
broader	O
range	O
of	O
language	Task
processing	Task
tasks	Task
especially	O
when	O
structured	O
outputs	O
are	O
needed	O
.	O
section	O
:	O
Acknowledgement	O
We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
2	O
Tesla	O
K40	O
GPUs	O
used	O
for	O
this	O
research	O
.	O
We	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
Amazon.com	O
Inc	O
for	O
an	O
AWS	O
in	O
Education	O
Research	O
grant	O
used	O
for	O
this	O
research	O
.	O
bibliography	O
:	O
References	O
