document O
: O
MultiGrain Method
: O
a O
unified Method
image Method
embedding Method
for O
classes O
and O
instances O
MultiGrain Method
is O
a O
network Method
architecture Method
producing O
compact Method
vector Method
representations Method
that O
are O
suited O
both O
for O
image Task
classification Task
and O
particular O
object O
retrieval Task
. O
It O
builds O
on O
a O
standard O
classification Task
trunk O
. O
The O
top O
of O
the O
network O
produces O
an O
embedding O
containing O
coarse O
and O
fine O
- O
grained O
information O
, O
so O
that O
images O
can O
be O
recognized O
based O
on O
the O
object O
class O
, O
particular O
object O
, O
or O
if O
they O
are O
distorted O
copies O
. O
Our O
joint Method
training Method
is O
simple O
: O
we O
minimize O
a O
cross Metric
- Metric
entropy Metric
loss Metric
for O
classification Task
and O
a O
ranking Metric
loss Metric
that O
determines O
if O
two O
images O
are O
identical O
up O
to O
data Task
augmentation Task
, O
with O
no O
need O
for O
additional O
labels O
. O
A O
key O
component O
of O
MultiGrain Method
is O
a O
pooling Method
layer Method
that O
takes O
advantage O
of O
high O
- O
resolution O
images O
with O
a O
network O
trained O
at O
a O
lower O
resolution O
. O
When O
fed O
to O
a O
linear Method
classifier Method
, O
the O
learned O
embeddings Method
provide O
state O
- O
of O
- O
the O
- O
art O
classification Metric
accuracy Metric
. O
For O
instance O
, O
we O
obtain O
79.4 O
% O
top Metric
- Metric
1 Metric
accuracy Metric
with O
a O
ResNet Method
- Method
50 Method
learned O
on O
Imagenet Material
, O
which O
is O
a O
+ O
1.8 O
% O
absolute O
improvement O
over O
the O
AutoAugment Method
method Method
. O
When O
compared O
with O
the O
cosine O
similarity O
, O
the O
same O
embeddings O
perform O
on O
par O
with O
the O
state O
- O
of O
- O
the O
- O
art O
for O
image Task
retrieval Task
at O
moderate Task
resolutions Task
. O
fixltx2efixltx2eisnotrequired O
arrows.meta O
, O
positioning O
, O
fit O
, O
backgrounds O
, O
calc O
supplSupplementalreferences O
[ O
itemize O
] O
labelsep=5pt O
, O
labelindent=0.4itemindent=0pt O
, O
leftmargin=* O
, O
itemsep= O
- O
4pt O
, O
darkgreenRGB0 O
, O
140 O
, O
0 O
antiquefuchsiargb0.57 O
, O
0.36 O
, O
0.51 O
auburnrgb0.43 O
, O
0.21 O
, O
0.1 O
section O
: O
Introduction O
Image Task
recognition Task
is O
central O
to O
computer Task
vision Task
, O
with O
dozens O
of O
new O
approaches O
being O
proposed O
every O
year O
, O
each O
optimized O
for O
particular O
aspects O
of O
the O
problem O
. O
From O
coarse O
to O
fine O
, O
we O
may O
distinguish O
the O
recognition Task
of O
( O
a O
) O
classes O
, O
where O
one O
looks O
for O
a O
certain O
type O
of O
object O
regardless O
of O
intra O
- O
class O
variations O
, O
( O
b O
) O
instances O
, O
where O
one O
looks O
for O
a O
particular O
object O
despite O
changes O
in O
the O
viewing O
conditions O
, O
and O
( O
c O
) O
copies O
, O
where O
one O
looks O
for O
a O
copy O
of O
a O
specific O
image O
despite O
edits O
. O
While O
these O
problems O
are O
in O
many O
ways O
similar O
, O
the O
standard O
practice O
is O
to O
use O
specialized O
, O
and O
thus O
incompatible O
, O
image Method
representations Method
for O
each O
case O
. O
Specialized Method
representations Method
may O
be O
accurate O
, O
but O
constitute O
a O
significant O
bottleneck O
in O
some O
applications O
. O
Consider O
for O
example O
image Task
retrieval Task
, O
where O
the O
goal O
is O
to O
match O
a O
query O
image O
to O
a O
large O
database O
of O
other O
images O
. O
Very O
often O
one O
would O
like O
to O
search O
the O
same O
database O
with O
multiple O
granularities O
, O
by O
matching O
the O
query O
by O
class O
, O
instance O
, O
or O
copy O
. O
The O
performance O
of O
an O
image Task
retrieval Task
system O
depends O
primarily O
on O
the O
image Method
embeddings Method
it O
uses O
. O
These O
strike O
a O
trade O
- O
off O
between O
database Metric
size Metric
, O
matching Metric
and Metric
indexing Metric
speed Metric
, O
and O
retrieval Metric
accuracy Metric
. O
Adopting O
multiple O
embeddings O
, O
narrowly O
optimized O
for O
each O
type O
of O
query O
, O
means O
multiplying O
the O
resource O
usage O
. O
In O
this O
paper O
we O
present O
a O
new O
representation O
, O
MultiGrain Method
, O
that O
can O
achieve O
the O
three O
tasks O
together O
, O
regardless O
of O
differences O
in O
their O
semantic O
granularity O
, O
see O
fig O
: O
splash O
. O
We O
learn O
MultiGrain Method
by O
jointly O
training O
an O
image Method
embedding Method
for O
multiple O
tasks O
. O
The O
resulting O
representation O
is O
compact O
and O
outperforms O
narrowly Method
- Method
trained Method
embeddings Method
. O
Instance O
retrieval Task
has O
a O
wide O
range O
of O
industrial Task
applications Task
, O
including O
detection Task
of Task
copyrighted Task
images Task
and O
exemplar Task
- Task
based Task
recognition Task
of Task
unseen Task
objects Task
. O
In O
settings O
where O
billion O
of O
images O
have O
to O
be O
treated O
, O
it O
is O
of O
interest O
to O
obtain O
image Task
embeddings Task
suitable O
for O
more O
than O
one O
recognition Task
task Task
. O
For O
instance O
, O
an O
image Method
storage Method
platform Method
is O
likely O
to O
perform O
some O
classification Task
of O
the O
input O
images O
, O
aside O
from O
detecting O
copies O
or O
instances O
of O
the O
same O
object O
. O
An O
embedding O
relevant O
to O
all O
these O
tasks O
advantageously O
reduces O
both O
the O
computing Metric
time Metric
per Metric
image Metric
and O
storage O
space O
. O
In O
this O
perspective O
, O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
trained O
only O
for O
classification Task
already O
go O
a O
long O
way O
towards O
universal Method
features Method
extractors Method
. O
The O
fact O
that O
we O
can O
learn O
image Task
embeddings Task
that O
are O
simultaneously O
good O
for O
classification Task
and O
instance O
retrieval Task
is O
surprising O
but O
not O
contradictory O
. O
In O
fact O
, O
there O
is O
a O
logical O
dependence O
between O
the O
tasks O
: O
images O
that O
contain O
the O
same O
instance O
also O
contain O
, O
by O
definition O
, O
the O
same O
class O
; O
and O
copied O
images O
contain O
the O
same O
instance O
. O
This O
is O
in O
contrast O
to O
multi Task
- Task
task Task
settings Task
where O
tasks O
are O
in O
competition O
and O
are O
thus O
difficult O
to O
combine O
. O
Instead O
, O
both O
class O
, O
instance O
, O
and O
copy O
congruency O
lead O
to O
embeddings O
that O
should O
be O
close O
in O
feature O
space O
. O
Still O
, O
the O
degree O
of O
similarity O
is O
different O
in O
the O
different O
cases O
, O
with O
classification Task
requiring O
more O
invariance O
to O
appearance O
variations O
and O
copy O
detection O
sensitivity O
to O
small O
image O
details O
. O
In O
order O
to O
learn O
an O
image Method
representation Method
that O
satisfies O
the O
different O
trade O
- O
offs O
, O
we O
start O
from O
an O
existing O
image Task
classification Task
network O
. O
We O
use O
a O
generalized Method
mean Method
layer Method
that O
converts O
a O
spatial O
activation O
map O
to O
a O
fixed O
- O
size O
vector O
. O
Most O
importantly O
, O
we O
show O
that O
it O
is O
an O
effective O
way O
to O
learn O
an O
architecture O
that O
can O
adapt O
to O
different O
resolutions O
at O
test O
time O
, O
and O
offer O
higher O
accuracies Metric
. O
This O
circumvents O
the O
massive O
engineering O
and O
computational Metric
effort Metric
needed O
to O
learn O
networks Method
for O
larger O
input O
resolutions O
The O
joint O
training O
of O
classification Task
and O
instance O
recognition O
objectives O
is O
based O
on O
cross Metric
- Metric
entropy Metric
and O
contrastive O
losses O
, O
respectively O
. O
Remarkably O
, O
instance Task
recognition Task
is O
learned O
for O
free O
, O
without O
using O
labels O
specific O
to O
instance Task
recognition Task
or O
image Task
retrieval Task
: O
we O
simply O
use O
the O
identity O
of O
the O
images O
as O
labels O
, O
and O
data Method
augmentation Method
as O
a O
way O
to O
generate O
different O
versions O
of O
each O
image O
. O
In O
summary O
, O
our O
main O
contributions O
are O
as O
follows O
: O
We O
introduce O
the O
MultiGrain Method
architecture Method
, O
which O
outputs O
an O
image Task
embedding Task
incorporating O
different O
levels O
of O
granularity O
. O
Our O
dual O
classification Task
+ O
instance O
objective O
improves O
the O
classification Metric
accuracy Metric
on O
its O
own O
. O
We O
show O
that O
part O
of O
this O
gain O
is O
due O
to O
the O
batching Method
strategy Method
, O
where O
each O
batch O
contains O
repeated O
instances O
of O
its O
images O
with O
different O
data O
augmentations O
for O
the O
purpose O
of O
the O
retrieval Task
loss O
; O
We O
incorporate O
a O
pooling Method
layer Method
inspired O
by O
image Task
retrieval Task
. O
It O
provides O
a O
significant O
boost O
in O
classification Metric
accuracy Metric
when O
provided O
with O
high O
- O
resolution O
images O
. O
Overall O
, O
our O
architecture O
offers O
competing O
performance O
both O
for O
classification Task
and O
image Task
retrieval Task
. O
Noticeably O
, O
we O
report O
a O
significant O
boost O
in O
accuracy Metric
on O
Imagenet Material
with O
a O
ResNet Method
- Method
50 Method
network Method
over O
the O
state O
of O
the O
art O
. O
The O
paper O
is O
organized O
as O
follows O
. O
sec O
: O
related O
introduces O
related O
works O
. O
sec O
: O
arch O
introduces O
our O
architecture O
, O
the O
training O
procedure O
and O
explains O
how O
we O
adapt O
the O
resolution O
at O
test O
time O
. O
sec O
: O
experiments O
reports O
the O
main O
experiments O
. O
section O
: O
Related O
work O
paragraph O
: O
Image O
classification Task
. O
Most O
computer Method
vision Method
architectures Method
designed O
for O
a O
wide O
range O
of O
tasks O
leverage O
a O
trunk Method
architecture Method
initially O
designed O
for O
classification Task
, O
such O
as O
Residual Method
networks Method
. O
An O
improvement O
on O
the O
trunk Method
architecture Method
eventually O
translates O
to O
better O
accuracies Metric
in O
other O
tasks O
, O
as O
shown O
on O
the O
detection Task
task Task
of O
the O
LSVRC’15 Task
challenge Task
. O
While O
recent O
architectures O
have O
exhibited O
some O
additional O
gains O
, O
other O
lines O
of O
research O
have O
been O
investigated O
successfully O
. O
For O
instance O
, O
a O
recent O
trend O
is O
to O
train O
high Method
capacity Method
networks Method
by O
leveraging O
much O
larger O
training O
sets O
of O
of O
weakly O
annotated O
data O
. O
To O
our O
knowledge O
, O
the O
state O
of O
the O
art O
on O
Imagenet Material
ILSVRC O
2012 O
benchmark O
for O
a O
model O
learned O
from O
scratch O
on O
Imagenet Material
train O
data O
only O
is O
currently O
hold O
by O
the O
gigantic Method
AmoebaNet Method
- Method
B Method
architecture Method
( O
557 O
M O
parameters O
) O
, O
which O
takes O
480x480 O
images O
as O
input O
. O
In O
our O
paper O
, O
we O
choose O
ResNet O
- O
50 O
( O
25.6 O
M O
parameters O
) O
, O
as O
this O
architecture O
is O
adopted O
in O
the O
literature O
in O
many O
works O
both O
on O
image Task
classification Task
and O
instance O
retrieval Task
. O
paragraph O
: O
Image Task
search Task
: O
from O
local O
features O
to O
CNN Method
. O
“ O
Image Task
search Task
” Task
is O
a O
generic O
retrieval Task
task O
that O
is O
usually O
associated O
with O
and O
evaluated O
for O
more O
specific O
problems O
such O
as O
landmark Task
recognition Task
, O
particular Task
object Task
recognition Task
or O
copy Task
detection Task
, O
for O
which O
the O
objective O
is O
to O
find O
the O
images O
most O
similar O
to O
the O
query O
in O
a O
large O
image O
collection O
. O
In O
this O
paper O
“ O
image Task
retrieval Task
” O
will O
refer O
to O
instance O
- O
level O
retrieval Task
, O
where O
object O
instances O
are O
as O
broad O
as O
possible O
, O
not O
restricted O
to O
buildings O
, O
as O
in O
the O
Oxford Material
/ Material
Paris Material
benchmark Material
. O
Effective O
systems O
for O
image Task
retrieval Task
rely O
on O
accurate O
image Task
descriptors Task
. O
Typically O
, O
a O
query O
image O
is O
described O
by O
an O
embedding O
vector O
, O
and O
the O
task O
amounts O
to O
searching O
the O
nearest O
neighbors O
of O
this O
vector O
in O
the O
embedding O
space O
. O
Possible O
improvement O
include O
refinement Method
steps Method
such O
as O
geometric Task
verification Task
, O
query Task
expansion Task
, O
or O
database Task
- Task
side Task
pre Task
- Task
processing Task
or Task
augmentation Task
. O
Local Method
image Method
descriptors Method
are O
traditionally O
aggregated O
to O
global Method
image Method
descriptors Method
suited O
for O
matching Task
in O
an O
inverted O
database O
, O
as O
in O
the O
seminal O
bag Method
- Method
of Method
- Method
words Method
model Method
. O
After O
the O
emergence O
of O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
for O
large O
- O
scale O
classification Task
on O
ImageNet Material
, O
it O
has O
become O
apparent O
that O
CNNs Method
trained O
on O
classification Task
datasets O
are O
very O
competitive O
image Method
feature Method
extractors Method
for O
various O
vision Task
tasks Task
, O
including O
instance O
retrieval Task
. O
paragraph O
: O
Specific O
architectures O
for O
particular O
object O
retrieval Task
are O
built O
upon O
a O
regular O
classification Task
trunk O
, O
and O
modified O
so O
the O
pooling Method
stage Method
gives O
more O
spatial O
locality O
in O
order O
to O
cope O
with O
small O
objects O
and O
clutter O
. O
For O
instance O
, O
a O
competitive O
baseline O
for O
instance O
retrieval Task
on O
various O
datasets O
is O
the O
R Method
- Method
MAC Method
image Method
descriptor Method
. O
It O
aggregates O
regionally O
pooled O
features O
extracted O
from O
a O
CNN Method
. O
The O
authors O
show O
that O
this O
specialized Method
pooling Method
combined O
with O
PCA Method
whitening Method
leads O
to O
efficient O
many Task
- Task
to Task
- Task
many Task
comparisons Task
between Task
image Task
regions Task
, O
highly O
beneficial O
to O
image Task
retrieval Task
. O
Gordo O
et O
al O
. O
show O
that O
fine O
- O
tuning O
these O
regionally Method
- Method
aggregated Method
representations Method
end O
- O
to O
- O
end O
on O
an O
external O
image Task
retrieval Task
dataset O
using O
a O
ranking Method
loss Method
yields O
significant O
improvements O
for O
instance O
retrieval Task
. O
Radenović Method
show O
that O
R Method
- Method
MAC Method
pooling Method
is O
advantageously O
replaced O
by O
a O
generalized Method
mean Method
pooling Method
( O
see O
sec O
: O
p Method
- Method
pooling Method
) Method
, O
which O
is O
a O
spatial Method
pooling Method
of Method
the Method
features Method
exponentiated O
to O
an O
exponent O
over O
the O
whole O
image O
. O
The O
exponentiation O
localizes O
the O
features O
on O
the O
point O
of O
interests O
in O
the O
image O
, O
replacing O
regional Method
aggregation Method
in O
R Method
- Method
MAC Method
. O
paragraph O
: O
Multi Task
- Task
task Task
training Task
is O
an O
active O
area O
of O
research O
, O
motivated O
by O
the O
observation O
that O
deep Method
neural Method
networks Method
are O
transferable O
to O
a O
wide O
range O
of O
vision Task
tasks Task
. O
Moreover O
trained O
deep Method
neural Method
networks Method
exhibit O
a O
high O
level O
of O
compressibility Metric
. O
In O
some O
cases O
, O
sharing O
the O
capacity O
of O
neural Method
networks Method
between O
different O
tasks O
through O
shared O
parameters O
helps O
the O
learning Task
by O
allowing O
complementary O
training O
among O
datasets O
and O
low O
- O
level O
features O
. O
Despite O
some O
successes O
with O
multi Method
- Method
task Method
networks Method
for O
vision Task
such O
as O
UberNet Task
, O
the O
design O
and O
training O
of O
multi Method
- Method
task Method
networks Method
still O
involve O
numerous O
heuristics O
. O
Ongoing O
lines O
of O
work O
include O
finding O
the O
right O
architecture O
for O
an O
efficient O
sharing O
of O
parameters O
, O
and O
finding O
the O
right O
optimization O
parameters O
for O
such O
networks O
in O
order O
to O
depart O
from O
the O
traditional O
setting O
of O
single Task
- Task
task Task
single Task
- Task
dataset Task
end Task
- Task
to Task
- Task
end Task
gradient Task
descent Task
, O
and O
efficiently O
weight O
the O
gradients O
in O
order O
to O
obtain O
a O
well O
- O
performing O
network O
in O
all O
tasks O
. O
paragraph O
: O
Data Task
augmentation Task
is O
a O
cornerstone O
of O
the O
training Task
in O
large Task
- Task
scale Task
vision Task
applications Task
, O
which O
improves O
generalization Task
and O
reduces O
over Task
- Task
fitting Task
. O
In O
a O
stochastic Method
gradient Method
descent Method
( Method
SGD Method
) Method
optimization Method
setting Method
, O
we O
show O
that O
including O
multiple O
data O
- O
augmented O
instances O
of O
the O
same O
image O
in O
one O
optimization O
batch O
, O
rather O
than O
having O
only O
distinct O
images O
in O
the O
batch O
, O
significantly O
enhances O
the O
effect O
of O
data Task
- Task
augmentations Task
and O
improve O
the O
generalization Method
of Method
the Method
network Method
. O
A O
related O
batch Method
augmented Method
( Method
BA Method
) Method
sampling Method
strategy Method
was O
concurrently O
introduced O
by O
Hoffer O
et O
al O
. O
. O
When O
augmenting O
the O
size O
of O
the O
batches O
in O
a O
large O
- O
scale O
distributed Task
optimization Task
of O
a O
neural Method
network Method
, O
they O
show O
that O
filling O
these O
bigger O
batches O
with O
data O
- O
augmented O
copies O
of O
the O
image O
in O
the O
batch O
yields O
better O
generalization Task
performance O
, O
and O
uses O
computing O
resources O
more O
efficiently O
through O
reduced O
data Metric
processing Metric
time Metric
. O
As O
discussed O
in O
sec O
: O
data O
- O
augmented O
- O
batches O
and O
highlighted O
in O
our O
classification Task
results O
( O
sec O
: O
classif Task
- O
results O
) O
, O
we O
show O
that O
a O
gain O
in O
performance O
under O
this O
sampling Method
scheme Method
is O
obtained O
using O
the O
same O
batch O
size O
, O
, O
with O
a O
lower O
number O
of O
distinct O
images O
per O
batch O
. O
We O
consider O
this O
scheme O
of O
repeated Method
augmentations Method
( O
RA Method
) O
within O
the O
batch O
as O
a O
way O
to O
boost O
the O
effect O
of O
data Task
augmentation Task
over O
the O
course O
of O
the O
optimization Task
. O
Our O
results O
indicate O
that O
RA Method
is O
a O
technique O
of O
general O
interest O
, O
beyond O
large Task
- Task
scale Task
distributed Task
training Task
applications Task
, O
for O
improving O
the O
generalization Task
of Task
neural Task
networks Task
. O
section O
: O
Architecture O
design O
Our O
goal O
is O
to O
develop O
a O
convolutional Method
neural Method
network Method
that O
is O
suitable O
for O
both O
image Task
classification Task
and O
instance O
retrieval Task
. O
In O
the O
current O
best O
practices O
, O
the O
architectures Method
and O
training Method
procedures Method
used O
for O
class Task
and Task
instance Task
recognition Task
differ O
in O
a O
significant O
manner O
. O
This O
section O
describes O
such O
technical O
differences O
, O
summarized O
in O
tab O
: O
diff_classif_instance O
, O
together O
with O
our O
solutions O
to O
bridge O
them O
. O
This O
leads O
us O
to O
a O
unified O
architecture O
, O
shown O
in O
fig:3arch O
, O
that O
we O
jointly O
train O
for O
both O
tasks O
in O
an O
end O
- O
to O
- O
end O
manner O
. O
subsection O
: O
Spatial Method
pooling Method
operators Method
This O
section O
considers O
the O
final O
, O
global Method
spatial Method
pooling Method
layer Method
. O
Local Method
pooling Method
operators Method
, O
usually O
max Method
pooling Method
, O
are O
found O
throughout O
the O
layers O
of O
most O
convolutional Method
networks Method
to O
achieve O
local O
invariance O
to O
small O
translations O
. O
By O
contrast O
, O
global Method
spatial Method
pooling Method
converts O
a O
3D O
tensor O
of O
activations O
produced O
by O
a O
convolutional Method
trunk Method
to O
a O
vector O
. O
paragraph O
: O
Classification Task
. O
In O
early O
models O
such O
as O
LeNet Method
- Method
5 Method
or O
AlexNet Method
, O
the O
final O
spatial Method
pooling Method
is O
just O
a O
linearization Method
of Method
the Method
activation Method
map Method
. O
It O
is O
therefore O
sensitive O
to O
the O
absolute O
location O
. O
Recent O
architectures O
such O
as O
ResNet Method
and O
DenseNet Method
employ O
average Method
pooling Method
, O
which O
is O
permutation O
invariant O
and O
hence O
offers O
a O
more O
global O
translation O
invariance O
. O
paragraph O
: O
Image O
retrieval Task
requires O
more O
localized O
geometric O
information O
: O
particular O
objects O
or O
landmarks O
are O
visually O
more O
similar O
, O
but O
the O
task O
suffers O
more O
from O
clutter O
, O
and O
a O
given O
query O
image O
has O
no O
specific O
training O
data O
devoted O
to O
it O
. O
This O
is O
why O
the O
pooling Method
operator Method
tries O
to O
favor O
more O
locality O
. O
Next O
we O
discuss O
the O
generalized Method
mean Method
pooling Method
operator Method
. O
Let O
be O
the O
feature O
tensor O
computed O
by O
a O
convolutional Method
neural Method
network Method
for O
a O
given O
image O
, O
where O
is O
the O
number O
of O
feature O
channels O
and O
and O
are O
the O
height O
and O
width O
of O
the O
map O
, O
respectively O
. O
We O
denote O
by O
a O
“ O
pixel O
” O
in O
the O
map O
, O
by O
the O
channel O
, O
and O
by O
the O
corresponding O
tensor O
element O
: O
. O
The O
generalized Method
mean Method
pooling Method
( O
GeM Method
) O
layer O
computes O
the O
generalized O
mean O
of O
each O
channel O
in O
a O
tensor O
. O
Formally O
, O
the O
GeM Method
embedding O
is O
given O
by O
where O
is O
a O
parameter O
. O
Setting O
this O
exponent O
as O
increases O
the O
contrast O
of O
the O
pooled Method
feature Method
map Method
and O
focuses O
on O
the O
salient O
features O
of O
the O
image O
. O
GeM Method
is O
a O
generalization O
of O
the O
average Method
pooling Method
commonly O
used O
in O
classification Task
networks O
( O
) O
and O
of O
spatial Method
max Method
- Method
pooling Method
layer Method
( O
) O
. O
It O
is O
employed O
in O
the O
original O
R Method
- Method
MAC Method
as O
an O
approximation O
of O
max Method
pooling Method
, O
yet O
only O
recently O
it O
was O
shown O
to O
be O
competitive O
on O
its O
own O
with O
R Method
- Method
MAC Method
for O
image Task
retrieval Task
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
is O
the O
first O
to O
apply O
and O
evaluate O
GeM Method
pooling O
in O
an O
image Task
classification Task
setting O
. O
More O
importantly O
, O
we O
show O
later O
in O
this O
paper O
that O
adjusting O
the O
exponent O
is O
an O
effective O
way O
to O
change O
the O
input O
image O
resolution O
between O
train O
and O
test O
time O
for O
all O
tasks O
, O
which O
explains O
why O
image Task
retrieval Task
has O
benefited O
from O
it O
considering O
that O
this O
task O
employs O
higher O
- O
resolution O
images O
. O
subsection O
: O
Training Metric
objective Metric
In O
order O
to O
combine O
the O
classification Task
and O
retrieval Task
tasks Task
, O
we O
use O
a O
joint Metric
objective Metric
function Metric
composed O
of O
a O
classification Task
loss O
and O
an O
instance O
retrieval Task
loss O
. O
The O
two O
- O
branch Method
architecture Method
is O
illustrated O
in O
fig:3arch O
and O
detailed O
next O
. O
paragraph O
: O
Classification Metric
loss Metric
. O
For O
classification Task
, O
we O
adopt O
the O
standard O
cross Method
- Method
entropy Method
loss Method
. O
Formally O
, O
let O
be O
the O
embedding O
computed O
by O
the O
deep Method
network Method
for O
image O
, O
the O
parameters O
of O
a O
linear Method
classifier Method
for O
class O
, O
and O
be O
the O
ground O
- O
truth O
class O
for O
that O
image O
. O
Then O
where O
. O
We O
omit O
it O
for O
simplicity O
, O
but O
by O
adding O
a O
constant O
channel O
to O
the O
feature O
vector O
, O
the O
bias O
of O
the O
classification Task
layer O
is O
incorporated O
in O
its O
weight O
matrix O
. O
paragraph O
: O
Retrieval Metric
loss Metric
. O
For O
image Task
retrieval Task
, O
the O
embeddings O
of O
two O
matching O
images O
( O
a O
positive O
pair O
) O
should O
have O
distances O
smaller O
than O
embeddings O
of O
non O
- O
matching O
images O
( O
a O
negative O
pair O
) O
. O
This O
can O
be O
enforced O
in O
two O
ways O
. O
The O
contrastive O
loss O
requires O
distances O
between O
positive O
pairs O
to O
be O
smaller O
than O
a O
threshold O
, O
and O
distances O
between O
negative O
pairs O
to O
be O
greater O
. O
The O
triplet O
loss O
instead O
requires O
an O
image O
to O
be O
closer O
to O
a O
positive O
sibling O
than O
to O
a O
negative O
sibling O
, O
which O
is O
relative O
property O
of O
image O
triplets O
. O
These O
losses O
requires O
adjusting O
multiple O
parameters O
, O
including O
how O
pairs O
and O
triplets O
are O
sampled O
. O
These O
parameters O
are O
sometimes O
hard O
to O
tune O
, O
especially O
for O
the O
triplet Task
loss Task
. O
Wu O
et O
al O
. O
proposed O
an O
effective O
method O
that O
addresses O
these O
difficulties O
. O
Given O
a O
batch O
of O
images O
, O
they O
re O
- O
normalize O
their O
embeddings O
to O
the O
unit O
sphere O
, O
sample O
negative O
pairs O
as O
a O
function O
of O
the O
embedding O
similarity O
, O
and O
use O
those O
pairs O
in O
a O
margin Method
loss Method
, O
a O
variant O
of O
contrastive Method
loss Method
that O
shares O
some O
of O
the O
benefits O
of O
the O
triplet Method
loss Method
. O
In O
more O
detail O
, O
given O
images O
in O
a O
batch O
with O
embeddings O
, O
the O
margin O
loss O
is O
expressed O
as O
where O
is O
the O
Euclidean O
distance O
between O
the O
normalized O
embeddings O
, O
the O
label O
is O
equal O
to O
1 O
if O
the O
two O
images O
match O
and O
otherwise O
, O
the O
margin O
( O
a O
constant O
hyper O
- O
parameter O
) O
, O
and O
is O
a O
parameter O
( O
learned O
during O
training O
together O
with O
the O
model O
parameters O
) O
, O
controlling O
the O
volume O
of O
the O
embedding O
space O
occupied O
embedding O
vectors O
. O
Due O
to O
the O
normalization O
, O
is O
equivalent O
to O
a O
cosine O
similarity O
, O
which O
, O
up O
to O
whitening O
( O
sec O
: O
whiten O
) O
, O
is O
also O
used O
in O
retrieval Task
. O
Loss Method
( O
[ O
reference O
] O
) O
is O
computed O
on O
a O
subset O
of O
positive O
and O
negative O
pairs O
selected O
with O
the O
sampling O
where O
the O
conditional O
probability O
of O
choosing O
a O
negative O
for O
image O
is O
where O
is O
a O
parameter O
and O
is O
a O
PDF O
that O
depends O
on O
the O
embedding O
dimension O
. O
The O
use O
of O
distance Method
weighted Method
- Method
sampling Method
with O
margin Method
loss Method
is O
very O
suited O
to O
our O
joint Task
training Task
setting Task
: O
this O
framework O
tolerates O
relatively O
small O
batch O
sizes O
( O
to O
instances O
) O
while O
requiring O
only O
a O
small O
amount O
of O
positives O
images O
( O
3 O
to O
5 O
) O
of O
each O
instance O
in O
the O
batch O
, O
without O
the O
need O
for O
elaborate O
parameter Method
tuning Method
or O
offline Method
sampling Method
. O
paragraph O
: O
Joint O
loss O
and O
architecture O
. O
The O
joint Method
loss Method
is O
a O
combination O
of O
classification Task
and O
retrieval Task
loss O
weighted O
by O
a O
factor O
. O
For O
a O
batch O
of O
images O
, O
the O
joint O
loss O
writes O
as O
, O
losses O
are O
normalized O
by O
the O
number O
of O
items O
in O
the O
corresponding O
summations O
. O
subsection O
: O
Batching Method
with O
repeated Method
augmentation Method
( O
RA Method
) O
Here O
, O
we O
propose O
to O
use O
only O
a O
training O
dataset O
for O
image Task
classification Task
, O
and O
train O
instance Task
recognition Task
via O
data Task
augmentation Task
. O
The O
rationale O
is O
that O
data Task
augmentation Task
produces O
another O
image O
that O
contains O
the O
same O
object O
instance O
. O
This O
approach O
does O
not O
require O
more O
annotation O
beyond O
the O
standard O
classification Task
set O
. O
We O
introduce O
a O
new O
sampling Method
scheme Method
for O
training Task
with O
SGD Task
and Task
data Task
augmentation Task
, O
which O
we O
refer O
to O
as O
repeated Task
augmentations Task
. O
In O
RA Method
we O
form O
an O
image O
batch O
by O
sampling O
different O
images O
from O
the O
dataset O
, O
and O
transform O
them O
up O
to O
times O
by O
a O
set O
of O
data Method
augmentations Method
to O
fill O
the O
batch O
. O
Thus O
, O
the O
instance O
level O
ground O
- O
truth O
iff O
images O
and O
are O
two O
augmented O
versions O
of O
the O
same O
training O
image O
. O
The O
key O
difference O
with O
the O
standard O
sampling Method
scheme Method
in O
SGD Method
is O
that O
samples O
are O
not O
independent O
, O
as O
augmented O
versions O
of O
the O
same O
image O
are O
highly O
correlated O
. O
While O
this O
strategy O
reduces O
the O
performance O
if O
the O
batch O
size O
is O
small O
, O
for O
larger O
batch O
sizes O
RA Method
outperforms O
the O
standard O
i.i.d Method
. Method
scheme Method
– O
while O
using O
the O
same O
batch O
size O
and O
learning Metric
rate Metric
for O
both O
schemes O
. O
This O
is O
different O
from O
the O
observation O
of O
, O
who O
also O
consider O
repeated O
samples O
in O
a O
batch O
, O
but O
simultaneously O
increase O
the O
size O
of O
the O
latter O
. O
We O
conjecture O
that O
the O
benefit O
of O
correlated O
RA Method
samples O
is O
to O
facilitate O
learning O
features O
that O
are O
invariant O
to O
the O
only O
difference O
between O
the O
repeated O
images O
— O
the O
augmentations O
. O
By O
comparison O
, O
with O
standard O
SGD Method
sampling Method
, O
two O
versions O
of O
the O
same O
image O
are O
seen O
only O
in O
different O
epochs O
. O
A O
study O
of O
an O
idealized O
problem O
illustrates O
this O
phenomenon O
in O
the O
supplementary O
material O
[ O
reference O
] O
. O
subsection O
: O
PCA Method
whitening Method
In O
order O
to O
transfer O
features O
learned O
via O
data Task
augmentation Task
to O
standard O
retrieval Task
datasets O
, O
we O
apply O
a O
step O
of O
PCA Method
whitening Method
, O
in O
accordance O
with O
previous O
works O
in O
image Task
retrieval Task
. O
The O
Euclidean O
distance O
between O
transformed O
features O
is O
equivalent O
to O
the O
Mahalanobis O
distance O
between O
the O
input O
descriptors O
. O
This O
is O
done O
after O
training O
the O
network O
, O
using O
an O
external O
dataset O
of O
unlabelled O
images O
. O
The O
effect O
of O
PCA Method
whitening Method
can O
be O
undone O
in O
the O
parameters O
of O
the O
classification Task
layer O
, O
so O
that O
the O
whitened Method
embeddings Method
can O
be O
used O
for O
both O
classification Task
and O
instance O
retrieval Task
. O
In O
detail O
, O
let O
be O
an O
image O
embedding O
vector O
and O
the O
weight O
vector O
for O
class O
, O
such O
that O
are O
the O
outputs O
of O
the O
classifier Method
as O
in O
eq O
: O
crossent O
. O
The O
whitening Method
operation Method
can O
be O
written O
as O
given O
the O
whitening O
matrix O
and O
centering O
vector O
; O
hence O
where O
and O
are O
the O
modified O
weight O
and O
bias O
for O
class O
. O
We O
observed O
that O
inducing O
decorrelation O
via O
a O
loss Method
is O
insufficient O
to O
ensure O
that O
features O
generalize O
well O
, O
which O
concurs O
with O
prior O
works O
. O
subsection O
: O
Input O
sizes O
The O
standard O
practice O
in O
image Task
classification Task
is O
to O
resize O
and O
center O
- O
crop O
input O
images O
to O
a O
relatively O
low O
resolution O
, O
e.g. O
pixels O
. O
The O
benefits O
are O
a O
smaller O
memory Metric
footprint Metric
, O
faster O
inference Task
, O
and O
the O
possibility O
of O
batching O
the O
inputs O
if O
they O
are O
cropped O
to O
a O
common O
size O
. O
On O
the O
other O
hand O
, O
image Task
retrieval Task
is O
typically O
dependent O
on O
finer O
details O
in O
the O
images O
, O
as O
an O
instance O
can O
be O
seen O
under O
a O
variety O
of O
scales O
, O
and O
cover O
only O
a O
small O
amount O
of O
pixels O
. O
The O
currently O
best O
- O
performing O
feature Method
extractors Method
for O
image Task
retrieval Task
therefore O
commonly O
use O
input O
sizes O
of O
or O
pixels O
for O
the O
largest O
side O
, O
without O
cropping O
the O
image O
to O
a O
square O
. O
This O
is O
impractical O
for O
end Task
- Task
to Task
- Task
end Task
training Task
of O
a O
joint O
classification Task
and O
retrieval Task
network O
. O
Instead O
, O
we O
train O
our O
architecture O
at O
the O
standard O
resolution O
, O
and O
use O
larger O
input O
resolutions O
at O
test O
time O
only O
. O
This O
is O
possible O
due O
to O
a O
key O
advantage O
of O
our O
architecture O
: O
a O
network O
trained O
with O
a O
pooling O
exponent O
and O
resolution O
can O
be O
evaluated O
at O
a O
larger O
resolution O
using O
a O
larger O
pooling O
exponent O
, O
see O
our O
validation O
in O
sec O
: O
classif Metric
- O
results O
. O
paragraph O
: O
Proxy Task
task Task
for O
cross Metric
- Metric
validation Metric
of O
. O
In O
order O
to O
select O
the O
exponent O
, O
suitable O
for O
all O
tasks O
, O
we O
create O
a O
synthetic O
retrieval Task
task O
IN O
- O
aug O
in O
between O
classification Task
and O
retrieval Task
. O
We O
sample O
images O
from O
the O
training O
set O
of O
ImageNet Material
, O
per O
class O
, O
and O
create O
5 O
augmented O
copies O
of O
each O
of O
them O
, O
using O
the O
“ O
full O
” O
data Method
augmentation Method
described O
before O
. O
We O
evaluate O
the O
retrieval Metric
accuracy Metric
on O
IN Task
- Task
aug Task
in O
a O
fashion O
similar O
to O
UKBench Material
, O
with O
an O
accuracy Metric
ranging O
from O
0 O
to O
5 O
depending O
measuring O
how O
many O
of O
the O
first O
5 O
augmentations O
are O
ranked O
in O
top O
5 O
positions O
. O
We O
pick O
the O
best O
- O
performing O
on O
IN Task
- Task
aug Task
, O
which O
provides O
the O
following O
choices O
as O
a O
function O
of O
and O
: O
The O
optimal O
obtained O
on O
IN Task
- Task
aug Task
provides O
a O
trade O
- O
off O
between O
retrieval Task
and O
classification Task
. O
Experimentally O
, O
we O
observed O
that O
other O
choices O
are O
suitable O
for O
setting O
this O
parameter O
: O
fine O
- O
tuning O
the O
parameter O
alone O
using O
training O
inputs O
at O
a O
given O
resolution O
by O
back Method
- Method
propagation Method
of O
the O
cross Method
- Method
entropy Method
loss Method
provides O
similar O
results O
and O
values O
of O
. O
section O
: O
Experiments O
and O
Results O
After O
presenting O
the O
datasets O
, O
we O
provide O
a O
parametric O
study O
and O
our O
results O
in O
image Task
classification Task
and O
retrieval Task
. O
subsection O
: O
Experimental O
settings O
paragraph O
: O
Base O
architecture O
and O
training O
settings O
. O
The O
convolutional Method
trunk Method
is O
ResNet Method
- Method
50 Method
. O
SGD Method
starts O
with O
a O
learning Metric
rate Metric
of O
which O
is O
reduced O
tenfold O
at O
epochs O
for O
a O
total O
of O
epochs O
( O
a O
standard O
setting O
) O
. O
The O
batch O
size O
is O
set O
to O
and O
an O
epoch O
is O
defined O
as O
a O
fixed O
number O
of O
iterations O
. O
With O
uniform Method
batch Method
sampling Method
, O
one O
epoch O
corresponds O
to O
two O
passes O
over O
the O
training O
set O
; O
with O
RA Method
and O
, O
one O
epoch O
corresponds O
to O
of O
the O
images O
of O
the O
training O
set O
. O
All O
classification Task
baselines O
are O
trained O
using O
this O
longer O
schedule O
for O
a O
fair O
comparison O
. O
paragraph O
: O
Data Task
augmentation Task
. O
We O
use O
standard O
flips O
, O
random O
resized O
crops O
, O
random O
lighting O
noise O
and O
a O
color O
jittering O
of O
brightness O
, O
contrast O
and O
saturation O
. O
We O
refer O
to O
this O
set O
of O
augmentations O
as O
“ O
full O
” O
, O
see O
details O
in O
supplemental O
[ O
reference O
] O
. O
As O
indicated O
in O
tab O
: O
classifres O
our O
network O
reaches O
% O
top Metric
- Metric
1 Metric
validation Metric
error Metric
under O
our O
chosen O
schedule O
and O
data Method
augmentation Method
when O
trained O
with O
cross Method
- Method
entropy Method
alone Method
and O
uniform Method
batch Method
sampling Method
. O
This O
figure O
is O
on O
the O
high O
end O
of O
accuracies Metric
reported O
for O
the O
ResNet Method
- Method
50 Method
network Method
without O
specially O
- O
crafted O
regularization Method
terms Method
or O
data Method
augmentations Method
. O
Input O
image O
resolution O
full O
resolution O
paragraph O
: O
Pooling O
exponent O
. O
During O
the O
end Task
- Task
to Task
- Task
end Task
training Task
of O
our O
network O
, O
we O
consider O
two O
settings O
for O
the O
pooling O
exponent O
in O
the O
GeM Method
layer O
of O
sec Method
: O
p Method
- Method
pooling Method
: O
we O
set O
either O
or O
. O
corresponds O
to O
average Method
pooling Method
, O
as O
used O
in O
classification Task
architectures O
. O
The O
relevant O
literature O
and O
our O
preliminary O
experiments O
on O
off O
- O
the O
- O
shelf O
classification Task
networks O
suggest O
that O
the O
value O
improves O
the O
retrieval Task
performance O
on O
standard O
benchmarks O
. O
fig O
: O
heatmap O
illustrates O
this O
choice O
. O
By O
setting O
, O
the O
car O
is O
detected O
with O
high O
confidence Metric
and O
without O
spurious O
detections O
. O
Boureau O
analyse O
average Method
- Method
and Method
max Method
- Method
pooling Method
of Method
sparse Method
features Method
. O
They O
find O
that O
when O
the O
number O
of O
pooled O
features O
increases O
, O
it O
is O
beneficial O
to O
make O
them O
more O
sparse O
, O
which O
is O
consistent O
with O
the O
observation O
we O
make O
here O
. O
paragraph O
: O
Input O
size O
and O
cropping Task
. O
As O
described O
in O
sec O
: O
input O
- O
size O
, O
we O
train O
our O
network O
on O
crops O
of O
size O
pixels O
. O
For O
testing O
, O
we O
experiment O
with O
computing O
MultiGrain Method
embeddings Method
at O
resolutions O
. O
For O
resolution Task
, O
we O
follow O
the O
classical O
image Task
classification Task
protocol O
“ O
resolution O
224 O
” O
: O
the O
smallest O
side O
of O
an O
image O
is O
resized O
to O
and O
then O
a O
central O
crop O
is O
extracted O
. O
For O
resolution Task
, O
we O
instead O
follow O
the O
protocol O
common O
in O
image Task
retrieval Task
and O
resize O
the O
largest O
side O
of O
the O
image O
to O
the O
desired O
number O
of O
pixels O
and O
evaluate O
the O
network O
on O
the O
rectangular O
image O
, O
without O
cropping O
. O
paragraph O
: O
Margin Task
loss Task
and O
batch Task
sampling Task
. O
We O
use O
3 O
data O
- O
augmented O
repetitions O
per O
batch O
. O
We O
use O
the O
default O
margin O
loss O
hyperparameters O
of O
( O
details O
in O
supplementary O
[ O
reference O
] O
) O
. O
As O
in O
the O
distance Method
- Method
weighted Method
sampling Method
is O
performed O
independently O
on O
each O
of O
the O
4 O
GPUs Method
used O
for O
training Task
. O
paragraph O
: O
Datasets O
. O
We O
train O
our O
networks O
on O
the O
ImageNet Material
- Material
2012 Material
training Material
set Material
of O
1.2 O
million O
images O
labelled O
into O
object O
categories O
. O
Classification Metric
accuracies Metric
are O
reported O
on O
the O
validation O
images O
of O
this O
dataset O
. O
For O
image Task
retrieval Task
, O
we O
report O
the O
mean Metric
average Metric
precision Metric
on O
the O
Holidays Material
dataset Material
, O
with O
images O
rotated O
manually O
when O
necessary O
, O
as O
in O
prior O
evaluations O
on O
this O
dataset O
. O
We O
also O
report O
the O
accuracy Metric
on O
the O
UKB Material
object O
recognition O
benchmark O
, O
which O
contains O
instances O
of O
objects O
under O
varying O
viewpoints O
each O
; O
each O
image O
is O
used O
as O
a O
query O
to O
find O
its O
4 O
closest O
neighbors O
in O
embedding O
space O
; O
the O
number O
of O
correct O
neighbors O
is O
averaged O
across O
all O
images O
, O
yielding O
a O
maximum O
score O
of O
. O
We O
also O
report O
the O
performance O
of O
our O
network O
in O
a O
copy Task
detection Task
setting Task
, O
indicating O
the O
mean Metric
average Metric
precision Metric
on O
the O
“ O
strong O
” O
subset O
of O
the O
INRIA Material
Copydays Material
dataset Material
. O
We O
add O
10 O
K O
distractor O
images O
randomly O
sampled O
from O
the O
YFCC100 Material
M Material
large Material
- Material
scale Material
collection Material
of Material
unlabelled Material
images Material
. O
We O
call O
the O
combination O
C10k O
. O
The O
PCA Method
whitening Method
transformations Method
are O
computed O
from O
the O
features O
of O
20 O
K O
images O
from O
YFCC100 Material
M Material
, O
distinct O
from O
the O
C10k Material
distractors Material
. O
subsection O
: O
Expanding Task
resolution Task
with O
pooling O
exponent O
[ O
t O
] O
0.42 O
[ O
t O
] O
0.42 O
[ O
t O
] O
0.13 O
As O
our O
reference O
scheme O
, O
we O
train O
the O
network O
at O
resolution O
224x224 O
with O
RA Method
sampling O
and O
pooling O
exponent O
. O
When O
testing O
on O
images O
with O
the O
same O
224x224 O
resolution O
, O
this O
gives O
a O
top Metric
- Metric
1 Metric
validation O
accuracy O
on O
Imagenet Material
, O
% O
points O
above O
the O
non O
- O
RA Method
baseline O
, O
see O
tab O
: O
classifres Method
. O
We O
now O
feed O
larger O
images O
at O
test O
time O
, O
, O
we O
consider O
resolutions O
and O
vary O
the O
exponents O
at O
test O
time O
. O
fig O
: O
p_scale_classif O
, O
fig O
: O
p_scale_holidays Method
show O
the O
classification Metric
accuracy Metric
on O
ImageNet Metric
validation Metric
and O
the O
retrieval Metric
accuracy Metric
on O
Holidays O
at O
different O
resolutions O
, O
for O
different O
values O
of O
the O
test O
pooling O
exponent O
. O
As O
expected O
, O
at O
= O
, O
the O
pooling O
exponent O
yielding O
best O
accuracy Metric
in O
classification Task
is O
the O
exponent O
with O
which O
the O
network O
has O
been O
trained O
, O
= O
. O
Observe O
that O
testing O
at O
larger O
scale O
requires O
an O
exponent O
, O
both O
for O
classification Task
and O
for O
retrieval Task
. O
In O
the O
following O
, O
we O
adopt O
the O
values O
obtained O
by O
our O
cross Method
- Method
validation Method
on O
IN O
- O
aug O
, O
see O
sec O
: O
expanding O
- O
resolution O
. O
subsection O
: O
Analysis O
of O
the O
tradeoff O
parameter O
We O
now O
analyze O
the O
impact O
of O
the O
tradeoff O
parameter O
. O
Note O
, O
this O
parameter O
does O
not O
directly O
reflect O
the O
relative O
importance O
of O
the O
two O
loss O
terms O
during O
training Task
, O
since O
these O
are O
not O
homogeneous O
: O
= O
0.5 O
does O
not O
mean O
that O
they O
have O
equal O
importance O
. O
fig O
: O
gradients Method
analyzes O
the O
actual O
relative O
importance O
of O
the O
classification Task
and O
margin O
loss O
terms O
, O
by O
measuring O
the O
average O
norm O
of O
the O
gradient O
back O
- O
propagated O
through O
the O
network O
at O
epochs O
0 O
and O
120 O
. O
One O
can O
see O
that O
= O
means O
that O
the O
classification Task
has O
slightly O
more O
weight O
at O
the O
beginning O
of O
the O
training O
. O
The O
classification Task
term O
becomes O
dominant O
at O
the O
end O
of O
the O
training O
, O
meaning O
that O
the O
network O
has O
already O
learned O
to O
cancel O
data Task
augmentation Task
. O
In O
terms O
of O
performance O
, O
= O
leads O
to O
a O
poor O
classification Metric
accuracy Metric
. O
Interestingly O
, O
the O
classification Task
performance O
is O
higher O
for O
the O
intermediate O
= O
( O
at O
= O
) O
than O
for O
= O
, O
see O
tab O
: O
classifres Task
. O
Thus O
, O
the O
margin O
loss O
leads O
to O
a O
performance O
gain O
for O
the O
classification Task
task O
. O
We O
set O
in O
our O
following O
experiments O
, O
as O
it O
gives O
the O
best O
classification Metric
accuracy Metric
at O
the O
practical O
resolutions O
and O
pixels O
. O
As O
a O
reference O
, O
we O
also O
report O
a O
few O
results O
with O
. O
subsection O
: O
Classification Task
results O
From O
now O
on O
, O
our O
MultiGrain Method
nets Method
are O
trained O
at O
resolution O
= O
with O
exponent O
= O
( O
standard O
average Method
pooling Method
) O
or O
= O
in O
the O
GeM Method
pooling O
. O
For O
each O
evaluation O
resolutions O
= O
, O
the O
same O
exponent O
is O
selected O
according O
to O
sec O
: O
expanding O
- O
resolution O
, O
yielding O
a O
single O
embedding Method
for O
classification Task
and O
for O
the O
retrieval Task
. O
tab O
: O
classifres Method
presents O
the O
classification Task
results O
. O
There O
is O
a O
large O
improvement O
in O
classification Task
performance O
from O
our O
baseline O
Resnet Method
- Method
50 Method
with O
= O
, O
= O
, O
“ O
full O
” O
data O
augmentation O
( O
76.2 O
% O
top Metric
- Metric
1 Metric
accuracy Metric
) O
, O
to O
a O
MultiGrain Method
model Method
at O
= O
, O
= O
, O
= O
( O
78.6 O
% O
top Metric
- Metric
1 Metric
) O
. O
We O
identify O
four O
sources O
for O
this O
improvement O
: O
Repeated Method
augmentations Method
: O
adding O
RA Method
batch O
sampling O
( O
sec O
: O
data O
- O
augmented O
- O
batches O
) O
yields O
an O
improvement O
of O
. O
Margin Metric
loss Metric
: O
the O
retrieval Task
loss O
helps O
the O
generalizing O
effect O
of O
data Task
augmentation Task
: O
. O
pooling Method
: O
GeM Method
at O
training O
( O
sec O
: O
p Method
- Method
pooling Method
) O
allows O
the O
margin O
loss O
to O
have O
a O
much O
stronger O
effect O
thanks O
to O
increased O
localization O
of O
the O
features O
: O
. O
Expanding Task
resolution Task
: O
evaluating O
at O
resolution Task
adds O
to O
the O
MultiGrain Method
network Method
, O
reaching O
the O
top Metric
- Metric
1 Metric
accuracy Metric
. O
This O
is O
made O
possible O
by O
the O
training Method
– O
which O
yields O
sparser O
features O
, O
more O
generalizable O
over O
different O
resolutions O
, O
and O
by O
the O
pooling Method
adaptation Method
– O
without O
it O
the O
performance O
at O
this O
resolution O
is O
only O
. O
The O
selection O
for O
evaluation Task
at O
higher O
resolutions O
has O
its O
limits O
: O
at O
pixels O
, O
due O
to O
the O
large O
discrepancy O
between O
the O
training O
and O
testing O
scale O
for O
the O
feature Method
extractor Method
, O
the O
accuracy Metric
drops O
to O
( O
without O
the O
adaptation Method
) O
. O
paragraph O
: O
AutoAugment O
( O
AA Method
) O
is O
a O
method O
to O
learn O
data Task
- Task
augmentation Task
using O
reinforcement Method
learning Method
techniques Method
to O
improve O
the O
accuracy Metric
of O
classification Task
networks O
on O
ImageNet Material
. O
We O
directly O
integrate O
the O
data O
- O
augmentations O
found O
by O
the O
algorithm O
trained O
on O
their O
Resnet Method
- Method
50 Method
model Method
using O
a O
long O
schedule O
of O
270 O
passes O
over O
the O
dataset O
, O
with O
batch O
size O
. O
We O
have O
observed O
that O
this O
longer O
training O
gives O
more O
impact O
to O
the O
AA Method
- O
generated O
augmentations O
. O
We O
therefore O
use O
a O
longer O
schedule O
of O
7508 O
iterations O
per O
epoch O
, O
keeping O
the O
batch O
size O
to O
. O
Our O
method O
benefits O
from O
this O
data Method
- Method
augmentation Method
: O
MultiGrain Method
reaches O
top Metric
- Metric
1 Metric
accuracy Metric
at O
resolution Metric
with O
3 O
, O
0.5 O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
best O
top Metric
- Metric
1 Metric
accuracy Metric
reported O
for O
Resnet Method
- Method
50 Method
when O
training O
and O
evaluating O
at O
this O
resolution O
, O
significantly O
higher O
than O
the O
reported O
with O
AutoAugment Method
alone Method
or O
for O
mixup Method
. O
Using O
a O
higher O
resolution O
at O
test O
time O
improves O
the O
accuracy Metric
further O
: O
we O
obtain O
top Metric
- Metric
1 Metric
accuracy Metric
at O
resolution Metric
. O
Our O
strategy O
of O
adapting O
the O
pooling O
exponent O
to O
a O
larger O
resolution O
is O
still O
effective O
, O
and O
significantly O
outperforms O
the O
state O
of O
the O
art O
performance O
for O
a O
ResNet Method
- Method
50 Method
learned O
on O
ImageNet Material
at O
training O
resolution O
. O
subsection O
: O
Retrieval Task
results O
We O
present O
our O
retrieval Task
results O
in O
tab O
: O
instanceres O
, O
with O
an O
ablation O
study O
and O
copy Task
- Task
detection Task
results O
in O
the O
supplemental O
material O
( O
[ O
reference O
] O
) O
. O
Our O
MultiGrain Method
nets Method
improve O
accuracies Metric
on O
all O
datasets O
with O
respect O
to O
the O
Resnet Method
- Method
50 Method
baseline Method
for O
comparable O
resolutions O
. O
Repeated Method
augmentations Method
( O
RA Method
) O
is O
again O
a O
key O
ingredient O
in O
this O
context O
. O
We O
compare O
with O
baselines O
where O
no O
annotated O
retrieval Task
dataset O
is O
used O
. O
give O
off O
- O
the O
- O
shelf O
network Metric
accuracies Metric
with O
R Method
- Method
MAC Method
pooling Method
. O
MultiGrain Method
compares O
favorably O
with O
their O
results O
at O
a O
comparable O
resolution O
( O
800 O
) O
. O
They O
reach O
accuracies Metric
above O
mAP Metric
on O
Holidays O
but O
this O
requires O
a O
resolution O
1000 O
pixels O
. O
It O
is O
also O
worth O
noting O
that O
we O
reach O
reasonable O
retrieval Task
performance O
at O
resolution O
500 O
, O
which O
is O
a O
interesting O
operating O
point O
with O
respect O
to O
the O
traditional O
inference O
resolutions O
– O
for O
retrieval Task
. O
Indeed O
, O
a O
forward Method
pass Method
of O
Resnet Method
- Method
50 Method
on O
16 O
processor O
cores O
takes O
s O
at O
resolution O
, O
against O
s O
at O
resolution O
( O
slower O
) O
. O
Because O
of O
this O
quadratic O
increase O
in O
timing Metric
, O
and O
the O
single O
embedding Method
computed O
by O
MultiGrain Method
, O
our O
solution O
is O
particularly O
adapted O
to O
large Task
- Task
scale Task
or Task
low Task
- Task
resource Task
vision Task
applications Task
. O
For O
comparison O
, O
we O
also O
report O
some O
older O
related O
results O
on O
the O
UKB Material
and O
C10k Material
datasets Material
, O
that O
are O
not O
competitive O
with O
MultiGrain Method
. O
Neural Method
codes Method
is O
one O
of O
the O
first O
works O
on O
retrieval Task
with O
deep O
features O
. O
The O
Fisher Method
vector Method
is O
a O
pooling Method
method Method
that O
uses O
local Method
SIFT Method
descriptors Method
. O
At O
resolutions O
we O
see O
that O
the O
results O
with O
the O
margin O
loss O
( O
0.5 O
) O
are O
slightly O
lower O
than O
without O
( O
1 O
) O
. O
This O
is O
partly O
due O
to O
the O
limited O
transfer O
from O
the O
IN Task
- Task
aug Task
task Task
to O
the O
variations O
observed O
in O
retrieval Task
datasets O
. O
section O
: O
Conclusion O
In O
this O
work O
we O
have O
introduced O
MultiGrain Method
, O
a O
unified Method
embedding Method
for O
image Task
classification Task
and O
instance O
retrieval Task
. O
MultiGrain Method
relies O
on O
a O
classical O
convolutional Method
neural Method
network Method
trunk Method
, O
with O
a O
GeM Method
layer O
topped O
with O
two O
heads O
at O
training O
time O
. O
We O
have O
discovered O
that O
by O
adjusting O
this O
pooling Method
layer Method
we O
are O
able O
to O
increase O
the O
resolution O
of O
images O
used O
a O
inference O
time O
, O
while O
maintaining O
a O
small O
resolution O
at O
training O
time O
. O
We O
have O
shown O
that O
MultiGrain Method
embeddings Method
can O
perform O
well O
on O
classification Task
and O
retrieval Task
. O
Interestingly O
, O
MultiGrain Method
also O
sets O
a O
new O
state O
of O
the O
art O
on O
pure O
classification Task
compared O
to O
all O
results O
obtained O
with O
the O
same O
convolutional Method
trunk Method
. O
Overall O
, O
our O
results O
show O
that O
retrieval Task
and O
classification Task
tasks Task
can O
benefit O
from O
each O
other O
. O
An O
implementation O
of O
our O
method O
is O
open O
- O
sourced O
at O
. O
paragraph O
: O
Acknowledgments O
. O
We O
thank O
Kaiming O
He O
for O
useful O
feedback O
and O
references O
. O
Maxim O
Berman O
is O
supported O
by O
Research O
Foundation O
- O
Flanders O
( O
FWO O
) O
through O
project O
number O
G0A2716N. O
PSI O
– O
ESAT Method
acknowledges O
a O
GPU Method
server Method
donation O
from O
FAIR O
Partnership O
Program O
. O
bibliography O
: O
References O
figuresection O
tablesection O
equationsection O
We O
report O
a O
few O
additional O
experiments O
and O
results O
that O
did O
not O
fit O
in O
the O
main O
paper O
. O
Section O
[ O
reference O
] O
shows O
the O
effect O
of O
data O
- O
augmented O
batches O
when O
training O
a O
simple O
toy Method
model Method
. O
Sections O
[ O
reference O
] O
and O
[ O
reference O
] O
list O
the O
values O
of O
a O
few O
hyper O
- O
parameters O
used O
in O
our O
method O
. O
Section O
[ O
reference O
] O
gives O
a O
some O
more O
ablation O
results O
in O
the O
retrieval Task
setting O
. O
Finally O
, O
Section O
[ O
reference O
] O
shows O
how O
to O
use O
the O
ingredients O
of O
MultiGrain Method
to O
improve O
the O
accuracy Metric
of O
an O
off O
- O
the O
- O
shelf O
pre Method
- Method
trained Method
ConvNet Method
at O
almost O
no O
additional O
training Metric
cost Metric
. O
It O
obtains O
what O
appear O
to O
be O
the O
best O
reported O
classification Task
results O
on O
imagenet Material
- Material
2012 Material
for O
a O
convnet Method
with O
publicly O
available O
weights O
. O
appendix O
: O
Data O
- O
augmented O
batches O
: O
toy Method
model Method
We O
have O
observed O
in O
sec O
: O
data O
- O
augmented O
- O
batches O
, O
sec O
: O
tradeoff O
- O
parameter O
that O
training O
our O
architecture O
( O
ResNet Method
- Method
50 Method
trunk Method
) O
with O
data O
- O
augmented O
batches O
yields O
improvements O
with O
respect O
to O
the O
vanilla Method
uniform Method
sampling Method
scheme Method
, O
despite O
the O
decrease O
in O
image O
diversity O
. O
This O
observation O
holds O
even O
in O
the O
absence O
of O
ranking O
triplet O
loss O
, O
all O
things O
being O
equal O
otherwise O
: O
same O
number O
of O
iterations O
per O
epoch O
, O
number O
of O
epochs O
, O
learning Metric
rate Metric
schedule Metric
, O
and O
batch O
size O
. O
As O
an O
example O
, O
fig O
: O
suppl O
- O
compare Metric
- Metric
sampling Metric
shows O
the O
evolution O
of O
the O
validation Metric
accuracy Metric
of O
our O
network O
trained O
under O
cross Method
- Method
entropy Method
with O
our O
training O
schedule O
and O
a O
pooling Method
, O
batches O
of O
size O
512 O
, O
with O
the O
data Method
augmentation Method
introduced O
in O
sec O
: O
experimental O
- O
settings O
, O
with O
uniform O
batches O
vs. O
with O
batch Method
sampling Method
. O
While O
initial O
epochs O
suffer O
from O
the O
reduced O
diversity O
of O
the O
batches O
compared O
to O
the O
uniformly Method
- Method
sampled Method
variant Method
, O
the O
reinforced O
effect O
on O
data Task
augmentation Task
compensates O
for O
this O
in O
the O
long O
run O
, O
and O
makes O
the O
batch Method
- Method
augmented Method
variant Method
reach O
a O
higher O
final O
accuracy Metric
. O
Since O
we O
observe O
this O
better O
performance O
even O
for O
a O
pure O
image Task
classification Task
task O
, O
an O
interesting O
question O
is O
whether O
this O
benefit O
is O
specific O
to O
our O
architecture O
and O
training Method
method Method
( O
batch Method
- Method
norm Method
, O
etc O
) O
, O
or O
if O
it O
is O
more O
generally O
applicable O
? O
Hereafter O
we O
analyse O
a O
linear Method
model Method
and O
synthetic O
classification Task
task O
that O
seems O
to O
align O
with O
the O
second O
hypothesis O
. O
We O
consider O
an O
idealized O
model O
of O
the O
effect O
of O
including O
different O
data O
- O
augmented O
instances O
of O
the O
same O
image O
in O
one O
batch O
using O
standard O
stochastic Method
gradient Method
descent Method
. O
We O
create O
a O
synthetic O
training O
set O
of O
points O
pictured O
in O
fig O
: O
toy O
- O
train O
of O
= O
positive O
and O
= O
negative O
training O
points O
by O
sampling O
from O
two O
2D Method
Gaussian Method
distributions Method
: O
with O
being O
the O
ground O
truth O
label O
. O
We O
sample O
a O
test O
dataset O
in O
the O
same O
manner O
. O
We O
consider O
the O
SGD Method
training Method
of O
an O
SVM Method
using O
the O
Hinge Method
loss Method
We O
consider O
the O
symmetry O
across O
the O
x O
- O
axis O
as O
a O
label Task
- Task
preserving Task
data Task
- Task
augmentation Task
suited O
to O
our O
synthetic O
dataset O
. O
We O
train O
the O
SVM Method
( O
[ O
reference O
] O
) O
using O
one O
pass O
through O
the O
data O
- O
augmented O
dataset O
of O
size O
, O
using O
batches O
of O
size O
. O
The O
only O
difference O
between O
the O
two O
optimization Method
schedules Method
is O
the O
order O
in O
which O
the O
samples O
are O
batched O
and O
presented O
to O
the O
optimizer O
. O
We O
consider O
two O
batch Method
sampling Method
strategies Method
: O
Uniform Method
sampling Method
: O
we O
sample O
the O
elements O
of O
the O
batch O
randomly O
from O
, O
without O
replacement O
; O
Paired Method
sampling Method
: O
we O
generate O
a O
batch O
by O
pairing O
a O
random O
element O
from O
and O
its O
data O
- O
augmentation O
, O
removing O
these O
two O
elements O
from O
. O
fig O
: O
evo Method
- Method
toy Method
shows O
the O
evaluation O
of O
the O
accuracy Metric
with O
the O
iterations O
in O
both O
of O
these O
cases O
, O
averaged O
across O
100 O
runs O
. O
It O
is O
clear O
that O
pairing O
the O
data O
- O
augmented O
pairs O
in O
one O
batch O
accelerates O
the O
convergence O
of O
this O
model O
. O
This O
idealized O
experiment O
demonstrates O
that O
there O
are O
cases O
in O
which O
the O
repeated Method
augmentation Method
scheme Method
provides O
an O
optimization Task
and Task
generalization Task
boost Task
, O
and O
reinforces O
the O
effect O
of O
data Task
augmentation Task
. O
appendix O
: O
Margin O
loss O
hyper O
- O
parameters O
tab O
: O
training O
- O
hyperparam Method
gives O
the O
value O
of O
the O
hyper O
- O
parameters O
for O
the O
margin O
loss O
used O
during O
the O
training O
of O
our O
models O
. O
appendix O
: O
Data O
augmentation O
hyper O
- O
parameters O
tab O
: O
full Task
- Task
data Task
- Task
augment Task
gives O
the O
transformations O
in O
the O
full Task
data Task
augmentation Task
used O
in O
our O
experiments O
( O
sec O
: O
experimental O
- O
settings O
) O
, O
along O
with O
their O
parameters O
. O
appendix O
: O
Additional O
results O
and O
ablation O
study O
for O
Multigrain O
in O
retrieval Task
tab O
: O
instanceres O
- O
abl O
reports O
additional O
results O
of O
the O
MultiGrain Method
architecture Method
, O
with O
an O
ablation O
study O
analyzing O
the O
effect O
of O
each O
component O
. O
As O
already O
reported O
in O
the O
main O
paper O
, O
for O
some O
datasets O
the O
choice O
of O
not O
using O
the O
triplet O
loss O
( O
) O
is O
as O
good O
or O
better O
than O
our O
generic O
choice O
( O
) O
. O
Of O
course O
, O
then O
the O
embedding Method
is O
not O
multi O
- O
purpose O
anymore O
. O
Overall O
, O
the O
different O
elements O
employed O
in O
our O
architecture O
( O
RA Method
and O
the O
layers O
specific O
to O
Multigrain O
) O
still O
give O
a O
significant O
improvement O
over O
simply O
using O
the O
activations O
, O
and O
is O
competitive O
with O
the O
state O
of O
the O
art O
for O
the O
same O
resolution Metric
/ Metric
complexity Metric
. O
Note O
, O
the O
AutoAugment Method
data Method
augmentation Method
does O
not O
transfer O
well O
to O
the O
retrieval Task
tasks Task
. O
This O
can O
be O
explained O
by O
their O
specificity O
to O
Imagenet Material
classification Task
. O
This O
shows O
the O
limitation O
of O
a O
particular O
choice O
of O
data Task
- Task
augmentation Task
if O
a O
single O
embedding Method
for O
classification Task
and O
retrieval Task
datasets Task
is O
desired O
. O
Learning O
AutoAugment Task
specifically O
for O
the O
retrieval Task
task O
would O
certainly O
help O
, O
but O
would O
probably O
also O
result O
in O
less O
general O
embeddings O
. O
Hence O
, O
data Task
- Task
augmentation Task
is O
a O
limiting O
factor O
for O
multi Task
- Task
purpose Task
embeddings Task
: O
improving O
for O
one O
task O
like O
classification Task
hurts O
the O
performance O
for O
other O
tasks O
. O
appendix O
: O
Evaluation O
of O
off Method
- Method
the Method
- Method
shelf Method
classifiers Method
at O
higher O
resolutions O
In O
this O
section O
, O
we O
present O
some O
additional O
classification Task
results O
using O
off O
- O
the O
- O
shelf O
pretrained O
classification Task
networks O
trained O
with O
standard Method
average Method
pooling Method
( O
) O
. O
As O
outlined O
in O
sec O
: O
expand Method
- Method
pooling Method
, O
sec O
: O
expanding O
- O
resolution O
, O
one O
of O
our O
contributions O
is O
a O
strategy O
for O
evaluating O
classifier Method
networks Method
trained O
with O
GeM Method
pooling O
at O
scale O
and O
exponent O
at O
a O
higher O
resolution O
and O
adapted O
exponent O
. O
It O
can O
be O
used O
on O
pretrained Method
networks Method
as O
well O
. O
For O
an O
evaluation O
scale O
, O
we O
use O
the O
alternative O
strategy O
described O
in O
sec O
: O
expanding O
- O
resolution O
to O
choose O
: O
we O
finetune O
the O
parameter O
by O
stochastic Method
gradient Method
descent Method
, O
backpropagating O
the O
cross Method
- Method
entropy Method
loss Method
on O
training O
images O
from O
imagenet Material
, O
rescaled O
to O
the O
desired O
input O
resolution O
. O
Compared O
to O
a O
full O
finetuning O
at O
this O
input O
resolution O
, O
this O
strategy O
has O
a O
limited O
memory O
footprint O
, O
given O
that O
the O
backpropagation Method
only O
has O
to O
be O
done O
on O
the O
ultimate O
classification Task
layer O
before O
reaching O
the O
pooling Method
layer Method
, O
allowing O
for O
an O
efficient O
computation O
of O
the O
gradient O
of O
. O
Experimentally O
we O
also O
found O
that O
this O
process O
converges O
on O
a O
few O
thousands O
of O
training O
samples O
, O
while O
a O
finetuning O
of O
the O
classification Task
layer O
would O
require O
several O
data O
- O
augmented O
epochs O
on O
the O
full O
training O
set O
. O
The O
finetuning Task
is O
done O
using O
SGD Method
with O
batches O
of O
( O
non O
- O
cropped O
) O
images O
, O
with O
momentum O
and O
initial O
learning Metric
rate Metric
, O
decayed O
under O
a O
polynomial Metric
learning Metric
rate Metric
decay Metric
with O
the O
total O
number O
of O
iterations O
. O
We O
select O
images O
from O
the O
training O
set O
( O
per O
category O
) O
for O
the O
fine Task
- Task
tuning Task
and O
do O
one O
pass O
on O
this O
reduced O
dataset O
. O
We O
use O
off O
- O
the O
- O
shelf O
pretrained Method
convnets Method
from O
the O
Cadene Method
/ Method
pretrained Method
- Method
models.pytorch Method
GitHub Method
repository Method
. O
Table O
[ O
reference O
] O
outlines O
the O
resulting O
validation Metric
accuracies Metric
. O
We O
see O
that O
for O
each O
network O
there O
is O
a O
scale O
and O
choice O
of O
that O
performs O
better O
than O
the O
standard O
evaluation O
. O
These O
networks O
have O
not O
been O
trained O
using O
GeM Method
pooling O
with O
; O
as O
exhibited O
in O
our O
classification Task
results O
( O
tab O
: O
classifres Task
) O
we O
found O
this O
to O
be O
another O
key O
ingredient O
in O
ensuring O
a O
higher O
scale O
insensitivity O
and O
better O
performance O
at O
larger O
resolution O
. O
As O
in O
our O
main O
experiments O
with O
the O
MultiGrain Method
architecture Method
with O
a O
ResNet Method
- Method
50 Method
backbone Method
, O
it O
is O
likely O
that O
these O
networks O
would O
reach O
higher O
values O
when O
training O
from O
scratch O
with O
a O
pooling Method
, O
and O
adding O
repeated O
augmentations O
and O
margin O
loss O
. O
However O
, O
running O
training O
experiments O
on O
these O
large O
networks O
is O
significantly O
more O
expensive O
. O
Therefore O
, O
we O
leave O
this O
for O
future O
work O
. O
ieee O
