GENERATIVE Method
MULTI Method
- Method
ADVERSARIAL Method
NETWORKS Method
section O
: O
ABSTRACT O
Generative Method
adversarial Method
networks Method
( O
GANs Method
) O
are O
a O
framework O
for O
producing O
a O
generative Method
model Method
by O
way O
of O
a O
two Method
- Method
player Method
minimax Method
game Method
. O
In O
this O
paper O
, O
we O
propose O
the O
Generative Method
Multi Method
- Method
Adversarial Method
Network Method
( O
GMAN Method
) O
, O
a O
framework O
that O
extends O
GANs Method
to O
multiple O
discriminators O
. O
In O
previous O
work O
, O
the O
successful O
training Task
of O
GANs Method
requires O
modifying O
the O
minimax O
objective O
to O
accelerate O
training Task
early O
on O
. O
In O
contrast O
, O
GMAN Method
can O
be O
reliably O
trained O
with O
the O
original O
, O
untampered O
objective O
. O
We O
explore O
a O
number O
of O
design O
perspectives O
with O
the O
discriminator O
role O
ranging O
from O
formidable O
adversary O
to O
forgiving O
teacher O
. O
Image Task
generation Task
tasks Task
comparing O
the O
proposed O
framework O
to O
standard O
GANs Method
demonstrate O
GMAN Method
produces O
higher O
quality O
samples O
in O
a O
fraction O
of O
the O
iterations O
when O
measured O
by O
a O
pairwise Metric
GAM Metric
- Metric
type Metric
metric Metric
. O
section O
: O
INTRODUCTION O
Generative Method
adversarial Method
networks Method
[ O
reference O
] O
) O
( O
GANs Method
) O
are O
a O
framework O
for O
producing O
a O
generative Method
model Method
by O
way O
of O
a O
two Method
- Method
player Method
minimax Method
game Method
. O
One O
player O
, O
the O
generator Method
, O
attempts O
to O
generate O
realistic O
data O
samples O
by O
transforming O
noisy O
samples O
, O
z O
, O
drawn O
from O
a O
simple O
distribution O
( O
e.g. O
, O
z O
∼ O
N O
( O
0 O
, O
1 O
) O
) O
using O
a O
transformation Method
function Method
G O
θ O
( O
z O
) O
with O
learned O
weights O
, O
θ O
. O
The O
generator O
receives O
feedback O
as O
to O
how O
realistic O
its O
synthetic O
sample O
is O
from O
another O
player O
, O
the O
discriminator Method
, O
which O
attempts O
to O
discern O
between O
synthetic O
data O
samples O
produced O
by O
the O
generator O
and O
samples O
drawn O
from O
an O
actual O
dataset O
using O
a O
function O
D O
ω O
( O
x O
) O
with O
learned O
weights O
, O
ω O
. O
The O
GAN Method
framework O
is O
one O
of O
the O
more O
recent O
successes O
in O
a O
line O
of O
research O
on O
adversarial Task
training Task
in O
machine Task
learning Task
[ O
reference O
] O
; O
[ O
reference O
] O
; O
[ O
reference O
] O
) O
where O
games O
between O
learners O
are O
carefully O
crafted O
so O
that O
Nash O
equilibria O
coincide O
with O
some O
set O
of O
desired O
optimality Metric
criteria Metric
. O
Preliminary O
work O
on O
GANs Method
focused O
on O
generating Task
images Task
( O
e.g. O
, O
[ O
reference O
] O
) O
, O
CIFAR Material
[ O
reference O
] O
) O
) O
, O
however O
, O
GANs Method
have O
proven O
useful O
in O
a O
variety O
of O
application O
domains O
including O
learning Task
censored Task
representations Task
[ O
reference O
] O
) O
, O
imitating O
expert O
policies O
[ O
reference O
] O
) O
, O
and O
domain Task
transfer Task
[ O
reference O
] O
) O
. O
Work O
extending O
GANs Method
to O
semi Task
- Task
supervised Task
learning Task
; O
[ O
reference O
] O
; O
[ O
reference O
] O
; O
Springenberg O
( O
2015 O
) O
) O
, O
inference Task
[ O
reference O
] O
; O
[ O
reference O
] O
) O
, O
feature Method
learning Method
[ O
reference O
] O
) O
, O
and O
improved O
image Task
generation Task
[ O
reference O
] O
; O
[ O
reference O
] O
; O
[ O
reference O
] O
) O
have O
shown O
promise O
as O
well O
. O
Despite O
these O
successes O
, O
GANs Method
are O
reputably O
difficult O
to O
train O
. O
While O
research O
is O
still O
underway O
to O
improve O
training Method
techniques Method
and O
heuristics O
[ O
reference O
] O
) O
, O
most O
approaches O
have O
focused O
on O
understanding O
and O
generalizing O
GANs Method
theoretically O
with O
the O
aim O
of O
exploring O
more O
tractable O
formulations O
[ O
reference O
] O
; O
[ O
reference O
] O
; O
[ O
reference O
] O
; O
[ O
reference O
] O
) O
. O
In O
this O
paper O
, O
we O
theoretically O
and O
empirically O
justify O
generalizing O
the O
GAN Method
framework O
to O
multiple O
discriminators Method
. O
We O
review O
GANs Method
and O
summarize O
our O
extension O
in O
Section O
2 O
. O
In O
Sections O
3 O
and O
4 O
, O
we O
present O
our O
N Method
- Method
discriminator Method
extension Method
to O
the O
GAN Method
framework O
( O
Generative Method
Multi Method
- Method
Adversarial Method
Networks Method
) O
with O
several O
variants O
which O
range O
the O
role O
of O
the O
discriminator Method
from O
formidable O
adversary O
to O
forgiving O
teacher O
. O
Section O
4.2 O
explains O
how O
this O
extension O
makes O
training Task
with O
the O
untampered Task
minimax Task
objective Task
tractable O
. O
In O
Section O
5 O
, O
we O
define O
an O
intuitive Metric
metric Metric
( O
GMAM Method
) O
to O
quantify O
GMAN Method
performance O
and O
evaluate O
our O
framework O
on O
a O
variety O
of O
image Task
generation Task
tasks O
. O
Section O
6 O
concludes O
with O
a O
summary O
of O
our O
contributions O
and O
directions O
for O
future O
research O
. O
Contributions O
- O
To O
summarize O
, O
our O
main O
contributions O
are O
: O
i O
) O
a O
multi O
- O
discriminator O
GAN Method
framework O
, O
GMAN Method
, O
that O
allows O
training O
with O
the O
original O
, O
untampered O
minimax O
objective O
; O
ii O
) O
a O
generative Metric
multi Metric
- Metric
adversarial Metric
metric Metric
( O
GMAM Method
) O
to O
perform O
pairwise Task
evaluation Task
of O
separately Method
trained Method
frameworks Method
; O
iii O
) O
a O
particular O
instance O
of O
GMAN Method
, O
GMAN Method
* O
, O
that O
allows O
the O
generator Method
to O
automatically O
regulate O
training O
and O
reach O
higher O
performance O
( O
as O
measured O
by O
GMAM Method
) O
in O
a O
fraction O
of O
the O
training Metric
time Metric
required O
for O
the O
standard O
GAN Method
model O
. O
section O
: O
GENERATIVE Method
ADVERSARIAL Method
NETWORKS Method
TO O
GMAN Method
The O
original O
formulation O
of O
a O
GAN Method
is O
a O
minimax Method
game Method
between O
a O
generator Method
, O
G O
θ O
( O
z O
) O
: O
z O
→ O
x O
, O
and O
a O
discriminator Method
, O
where O
p O
data O
( O
x O
) O
is O
the O
true O
data O
distribution O
and O
p O
z O
( O
z O
) O
is O
a O
simple O
( O
usually O
fixed O
) O
distribution O
that O
is O
easy O
to O
draw O
samples O
from O
( O
e.g. O
, O
N O
( O
0 O
, O
1 O
) O
) O
. O
We O
differentiate O
between O
the O
function O
space O
of O
discriminators O
, O
D O
, O
and O
elements O
of O
this O
space O
, O
D. O
Let O
p O
G O
( O
x O
) O
be O
the O
distribution O
induced O
by O
the O
generator Method
, O
G O
θ O
( O
z O
) O
. O
We O
assume O
D O
, O
G O
to O
be O
deep Method
neural Method
networks Method
as O
is O
typically O
the O
case O
. O
In O
their O
original O
work O
, O
[ O
reference O
] O
proved O
that O
given O
sufficient O
network O
capacities O
and O
an O
oracle O
providing O
the O
optimal O
discriminator Method
, O
will O
recover O
the O
desired O
globally O
optimal O
solution O
, O
p O
G O
( O
x O
) O
= O
p O
data O
( O
x O
) O
, O
so O
that O
the O
generator Method
distribution Method
exactly O
matches O
the O
data O
distribution O
. O
In O
practice O
, O
they O
replaced O
the O
second O
term O
, O
log O
( O
1− O
D O
( O
G O
( O
z O
) O
) O
) O
, O
with O
− O
log O
( O
D O
( O
G O
( O
z O
) O
) O
) O
to O
enhance O
gradient O
signals O
at O
the O
start O
of O
the O
game O
; O
note O
this O
is O
no O
longer O
a O
zero Task
- Task
sum Task
game Task
. O
Part O
of O
their O
convergence O
and O
optimality O
proof O
involves O
using O
the O
oracle O
, O
D Method
* Method
, O
to O
reduce O
the O
minimax Method
game Method
to O
a O
minimization Task
over Task
G Task
only O
: O
where O
JSD Method
denotes O
Jensen Method
- Method
Shannon Method
divergence Method
. O
Minimizing Task
C Task
( Task
G Task
) O
necessarily O
minimizes O
JSD Method
, O
however O
, O
we O
rarely O
know O
D O
* O
and O
so O
we O
instead O
minimize O
V O
( O
D O
, O
G O
) O
, O
which O
is O
only O
a O
lower O
bound O
. O
This O
perspective O
of O
minimizing O
the O
distance O
between O
the O
distributions O
, O
p O
data O
and O
p O
G O
, O
motivated O
[ O
reference O
] O
to O
develop O
a O
generative Method
model Method
that O
matches O
all O
moments O
of O
p O
G O
( O
x O
) O
with O
p O
data O
( O
x O
) O
( O
at O
optimality O
) O
by O
minimizing O
maximum Method
mean Method
discrepancy Method
( O
MMD Method
) O
. O
Another O
approach O
, O
EBGAN Method
, O
[ O
reference O
] O
) O
explores O
a O
larger O
class O
of O
games O
( O
non Task
- Task
zero Task
- Task
sum Task
games Task
) O
which O
generalize O
the O
generator Method
and Method
discriminator Method
objectives Method
to O
take O
real O
- O
valued O
" O
energies O
" O
as O
input O
instead O
of O
probabilities O
. O
[ O
reference O
] O
and O
then O
[ O
reference O
] O
extended O
the O
JSD Method
perspective Method
on O
GANs Method
to O
more O
general O
divergences O
, O
specifically O
f Method
- Method
divergences Method
and O
then O
Bregman Method
- Method
divergences Method
respectively O
. O
In O
general O
, O
these O
approaches O
focus O
on O
exploring O
fundamental O
reformulations O
of O
V Task
( Task
D Task
, Task
G Task
) O
. O
Similarly O
, O
our O
work O
focuses O
on O
a O
fundamental O
reformulation O
, O
however O
, O
our O
aim O
is O
to O
provide O
a O
framework O
that O
accelerates O
training O
of O
the O
generator O
to O
a O
more O
robust O
state O
irrespective O
of O
the O
choice O
of O
V O
. O
section O
: O
GMAN Method
: O
A O
MULTI Method
- Method
ADVERSARIAL Method
EXTENSION Method
We O
propose O
introducing O
multiple Method
discriminators Method
, O
which O
brings O
with O
it O
a O
number O
of O
design O
possibilities O
. O
We O
explore O
approaches O
ranging O
between O
two O
extremes O
: O
1 O
) O
a O
more O
discriminating O
D O
( O
better O
approximating O
max O
D O
V O
( O
D O
, O
G O
) O
) O
and O
2 O
) O
a O
D O
better O
matched O
to O
the O
generator O
's O
capabilities O
. O
Mathematically O
, O
we O
reformulate O
G O
's O
objective O
as O
Figure O
1 O
) O
. O
Each O
D O
i O
is O
still O
expected O
to O
independently O
maximize O
its O
own O
V O
( O
D O
i O
, O
G O
) O
( O
i.e. O
no O
cooperation O
) O
. O
We O
sometimes O
abbreviate O
V O
( O
D O
i O
, O
G O
) O
with O
V O
i O
and O
F O
( O
V O
1 O
, O
. O
. O
. O
, O
V O
N O
) O
with O
F O
G O
( O
V O
i O
) O
. O
section O
: O
A O
FORMIDABLE Method
ADVERSARY Method
Here O
, O
we O
consider O
multi Method
- Method
discriminator Method
variants Method
that O
attempt O
to O
better O
approximate O
max O
D O
V O
( O
D O
, O
G O
) O
, O
providing O
a O
harsher O
critic O
to O
the O
generator O
. O
Figure O
1 O
: O
( O
GMAN Method
) O
The O
generator Method
trains O
using O
feedback O
aggregated O
over O
multiple O
discriminators Method
. O
If O
F O
: O
= O
max O
, O
G O
trains O
against O
the O
best O
discriminator Method
. O
If O
F O
: O
= O
mean O
, O
G O
trains O
against O
an O
ensemble O
. O
We O
explore O
other O
alternatives O
to O
F O
in O
Sections O
4.1 O
& O
4.4 O
that O
improve O
on O
both O
these O
options O
. O
section O
: O
MAXIMIZING O
V O
( O
D O
, O
G O
) O
For O
a O
fixed O
G O
, O
maximizing O
F O
G O
( O
V O
i O
) O
with O
F O
: O
= O
max O
and O
N O
randomly O
instantiated O
copies O
of O
our O
discriminator Method
is O
functionally O
equivalent O
to O
optimizing O
V O
( O
e.g. O
, O
stochastic Method
gradient Method
ascent Method
) O
with O
random O
restarts O
in O
parallel O
and O
then O
presenting O
max O
i∈{1 O
, O
... O
, O
N O
} O
V O
( O
D O
i O
, O
G O
) O
as O
the O
loss O
to O
the O
generator O
- O
a O
very O
pragmatic O
approach O
to O
the O
difficulties O
presented O
by O
the O
non O
- O
convexity O
of O
V O
caused O
by O
the O
deep O
net O
. O
Requiring O
the O
generator O
to O
minimize O
the O
max O
forces O
G O
to O
generate O
high O
fidelity O
samples O
that O
must O
hold O
up O
under O
the O
scrutiny O
of O
all O
N O
discriminators O
, O
each O
potentially O
representing O
a O
distinct O
max O
. O
In O
practice O
, O
max O
Di∈D O
V O
( O
D O
i O
, O
G O
) O
is O
not O
performed O
to O
convergence O
( O
or O
global O
optimality O
) O
, O
so O
the O
above O
problem O
is O
oversimplified O
. O
Furthermore O
, O
introducing O
N O
discriminators O
affects O
the O
dynamics O
of O
the O
game O
which O
affects O
the O
trajectories O
of O
the O
discriminators O
. O
This O
prevents O
us O
from O
claiming O
max{V O
1 O
( O
t O
) O
, O
. O
. O
. O
, O
V O
N O
( O
t O
) O
} O
> O
max{V O
1 O
( O
t O
) O
} O
∀t O
even O
if O
we O
initalize O
D O
1 O
( O
0 O
) O
= O
D O
1 O
( O
0 O
) O
as O
it O
is O
unlikely O
that O
D O
1 O
( O
t O
) O
= O
D O
1 O
( O
t O
) O
at O
some O
time O
t O
after O
the O
start O
of O
the O
game O
. O
section O
: O
BOOSTING Task
We O
can O
also O
consider O
taking O
the O
max Method
over Method
N Method
discriminators Method
as O
a O
form O
of O
boosting Method
for O
the O
discriminator Task
's Task
online Task
classification Task
problem Task
( O
online O
because O
G O
can O
produce O
an O
infinite O
data O
stream O
) O
. O
The O
boosted Method
discriminator Method
is O
given O
a O
sample O
x O
t O
and O
must O
predict O
whether O
it O
came O
from O
the O
generator O
or O
the O
dataset O
. O
The O
booster O
then O
makes O
its O
prediction O
using O
the O
predictions O
of O
the O
N O
weaker O
D O
i O
. O
There O
are O
a O
few O
differences O
between O
taking O
the O
max O
( O
case O
1 O
) O
and O
online Method
boosting Method
( O
case O
2 O
) O
. O
In O
case O
1 O
, O
our O
booster Method
is O
limited O
to O
selecting O
a O
single O
weak Method
discriminator Method
( O
i.e. O
a O
pure O
strategy O
) O
, O
while O
in O
case O
2 O
, O
many O
boosting Method
algorithms Method
more O
generally O
use O
linear Method
combinations Method
of O
the O
discriminators O
. O
Moreover O
, O
in O
case O
2 O
, O
a O
booster Method
must O
make O
a O
prediction O
before O
receiving O
a O
loss O
function O
. O
In O
case O
1 O
, O
we O
assume O
access O
to O
the O
loss O
function O
at O
prediction O
time O
, O
which O
allows O
us O
to O
compute O
the O
max O
. O
It O
is O
possible O
to O
train O
the O
weak Method
discriminators Method
using O
boosting Method
and O
then O
ignore O
the O
booster Method
's Method
prediction Method
by O
instead O
presenting O
max{V O
i O
} O
. O
We O
explore O
both O
variants O
in O
our O
experiments O
, O
using O
the O
adaptive Method
algorithm Method
proposed O
in O
[ O
reference O
] O
. O
Unfortunately O
, O
boosting Method
failed O
to O
produce O
promising O
results O
on O
the O
image Task
generation Task
tasks O
. O
It O
is O
possible O
that O
boosting Method
produces O
too O
strong O
an O
adversary O
for O
learning Task
which O
motivates O
the O
next O
section O
. O
Boosting Method
results O
appear O
in O
Appendix O
A.7 O
. O
section O
: O
A O
FORGIVING O
TEACHER O
The O
previous O
perspectives O
focus O
on O
improving O
the O
discriminator Method
with O
the O
goal O
of O
presenting O
a O
better O
approximation O
of O
max O
D O
V O
( O
D O
, O
G O
) O
to O
the O
generator Method
. O
Our O
next O
perspective O
asks O
the O
question O
, O
" O
Is O
max O
D O
V O
( O
D O
, O
G O
) O
too O
harsh O
a O
critic O
? O
" O
section O
: O
Soft Method
- Method
DISCRIMINATOR Method
In O
practice O
, O
training O
against O
a O
far O
superior O
discriminator Method
can O
impede O
the O
generator Method
's Method
learning Method
. O
This O
is O
because O
the O
generator O
is O
unlikely O
to O
generate O
any O
samples O
considered O
" O
realistic O
" O
by O
the O
discriminator O
's O
standards O
, O
and O
so O
the O
generator O
will O
receive O
uniformly O
negative O
feedback O
. O
This O
is O
problem O
- O
atic O
because O
the O
information O
contained O
in O
the O
gradient O
derived O
from O
negative O
feedback O
only O
dictates O
where O
to O
drive O
down O
p O
G O
( O
x O
) O
, O
not O
specifically O
where O
to O
increase O
p O
G O
( O
x O
) O
. O
Furthermore O
, O
driving O
down O
p O
G O
( O
x O
) O
necessarily O
increases O
p O
G O
( O
x O
) O
in O
other O
regions O
of O
X O
( O
to O
maintain O
X O
p O
G O
( O
x O
) O
= O
1 O
) O
which O
may O
or O
may O
not O
contain O
samples O
from O
the O
true O
dataset O
( O
whack O
- O
a O
- O
mole O
dilemma O
) O
. O
In O
contrast O
, O
a O
generator Method
is O
more O
likely O
to O
see O
positive O
feedback O
against O
a O
more O
lenient O
discriminator O
, O
which O
may O
better O
guide O
a O
generator O
towards O
amassing O
p O
G O
( O
x O
) O
in O
approximately O
correct O
regions O
of O
X O
. O
For O
this O
reason O
, O
we O
explore O
a O
variety O
of O
functions O
that O
allow O
us O
to O
soften O
the O
max O
operator O
. O
We O
choose O
to O
focus O
on O
soft O
versions O
of O
the O
three O
classical O
Pythagorean Method
means Method
parameterized O
by O
λ O
where O
λ O
= O
0 O
corresponds O
to O
the O
mean O
and O
the O
max O
is O
recovered O
as O
λ O
→ O
∞ O
: O
where O
Using O
a O
softmax Method
also O
has O
the O
well O
known O
advantage O
of O
being O
differentiable O
( O
as O
opposed O
to O
subdifferentiable O
for O
max O
) O
. O
Note O
that O
we O
only O
require O
continuity O
to O
guarantee O
that O
computing O
the O
softmax O
is O
actually O
equivalent O
to O
computing O
V O
( O
D O
, O
G O
) O
whereD O
is O
some O
convex O
combination O
of O
D O
i O
( O
see O
Appendix O
A.5 O
) O
. O
section O
: O
USING O
THE O
ORIGINAL O
MINIMAX Method
OBJECTIVE Method
To O
illustrate O
the O
effect O
the O
softmax Method
has O
on O
training Task
, O
observe O
that O
the O
component O
of O
AM O
sof O
t O
( O
V O
, O
0 O
) O
relevant O
to O
generator Task
training Task
can O
be O
rewritten O
as O
where O
. O
From O
this O
form O
, O
it O
is O
clear O
that O
z O
= O
1 O
if O
and O
only O
if O
D O
i O
= O
0 O
∀i O
, O
so O
G O
only O
receives O
a O
vanishing O
gradient O
if O
all O
D O
i O
agree O
that O
the O
sample O
is O
fake O
; O
this O
is O
especially O
unlikely O
for O
large O
N O
. O
In O
other O
words O
, O
G O
only O
needs O
to O
fool O
a O
single O
D O
i O
to O
receive O
constructive O
feedback O
. O
This O
result O
allows O
the O
generator O
to O
successfully O
minimize O
the O
original O
generator Metric
objective Metric
, O
log O
( O
1 O
− O
D O
) O
. O
This O
is O
in O
contrast O
to O
the O
more O
popular O
− Method
log Method
( Method
D Method
) Method
introduced O
to O
artificially O
enhance O
gradients O
at O
the O
start O
of O
training O
. O
At O
the O
beginning O
of O
training O
, O
when O
max O
Di O
V O
( O
D O
i O
, O
G O
) O
is O
likely O
too O
harsh O
a O
critic O
for O
the O
generator O
, O
we O
can O
set O
λ O
closer O
to O
zero O
to O
use O
the O
mean O
, O
increasing O
the O
odds O
of O
providing O
constructive O
feedback O
to O
the O
generator O
. O
In O
addition O
, O
the O
discriminators Method
have O
the O
added O
benefit O
of O
functioning O
as O
an O
ensemble Method
, O
reducing O
the O
variance O
of O
the O
feedback O
presented O
to O
the O
generator Method
, O
which O
is O
especially O
important O
when O
the O
discriminators O
are O
far O
from O
optimal O
and O
are O
still O
learning O
a O
reasonable O
decision O
boundary O
. O
As O
training O
progresses O
and O
the O
discriminators O
improve O
, O
we O
can O
increase O
λ O
to O
become O
more O
critical O
of O
the O
generator O
for O
more O
refined O
training Task
. O
section O
: O
MAINTAINING O
MULTIPLE O
HYPOTHESES O
We O
argue O
for O
this O
ensemble Method
approach Method
on O
a O
more O
fundamental O
level O
as O
well O
. O
Here O
, O
we O
draw O
on O
the O
density Method
ratio Method
estimation Method
perspective Method
of O
GANs Method
[ O
reference O
] O
) O
. O
The O
original O
GAN Method
proof O
assumes O
we O
have O
access O
to O
p O
data O
( O
x O
) O
, O
if O
only O
implicitly O
. O
In O
most O
cases O
of O
interest O
, O
the O
discriminator Method
only O
has O
access O
to O
a O
finite O
dataset O
sampled O
from O
p O
data O
( O
x O
) O
; O
therefore O
, O
when O
computing O
expectations O
of O
V O
( O
D O
, O
G O
) O
, O
we O
only O
draw O
samples O
from O
our O
finite O
dataset O
. O
This O
is O
equivalent O
to O
training O
a O
GAN Method
with O
p O
data O
( O
x O
) O
= O
p O
data O
which O
is O
a O
distribution O
consisting O
of O
point O
masses O
on O
all O
the O
data O
points O
in O
the O
dataset O
. O
For O
the O
sake O
of O
argument O
, O
let O
's O
assume O
we O
are O
training O
a O
discriminator Method
and Method
generator Method
, O
each O
with O
infinite O
capacity O
. O
In O
this O
case O
, O
the O
global O
optimum O
( O
p O
G O
( O
x O
) O
= O
p O
data O
( O
x O
) O
) O
fails O
to O
capture O
any O
of O
the O
interesting O
structure O
from O
p O
data O
( O
x O
) O
, O
the O
true O
distribution O
we O
are O
trying O
to O
learn O
. O
Therefore O
, O
it O
is O
actually O
critical O
that O
we O
avoid O
this O
global O
optimum O
. O
Figure O
2 O
: O
Consider O
a O
dataset O
consisting O
of O
the O
nine O
1 O
- O
dimensional O
samples O
in O
black O
. O
Their O
corresponding O
probability O
mass O
function O
is O
given O
in O
light O
gray O
. O
After O
training O
GMAN Method
, O
three O
discriminators Method
converge O
to O
distinct O
local O
optima O
which O
implicitly O
define O
distributions O
over O
the O
data O
( O
red O
, O
blue O
, O
yellow O
) O
. O
Each O
discriminator O
may O
specialize O
in O
discriminating O
a O
region O
of O
the O
data O
space O
( O
placing O
more O
diffuse O
mass O
in O
other O
regions O
) O
. O
Averaging O
over O
the O
three O
discriminators O
results O
in O
the O
distribution O
in O
black O
, O
which O
we O
expect O
has O
higher O
likelihood O
under O
reasonable O
assumptions O
on O
the O
structure O
of O
the O
true O
distribution O
. O
In O
practice O
, O
this O
degenerate O
result O
is O
avoided O
by O
employing O
learners Method
with Method
limited Method
capacity Method
and O
corrupting O
data O
samples O
with O
noise O
( O
i.e. O
, O
dropout Method
) O
, O
but O
we O
might O
better O
accomplish O
this O
by O
simultaneously O
training O
a O
variety O
of O
limited Method
capacity Method
discriminators Method
. O
With O
this O
approach O
, O
we O
might O
obtain O
a O
diverse O
set O
of O
seemingly O
tenable O
hypotheses O
for O
the O
true O
p O
data O
( O
x O
) O
. O
Averaging O
over O
these O
multiple O
locally Method
optimal Method
discriminators Method
increases O
the O
entropy O
ofp O
data O
( O
x O
) O
by O
diffusing O
the O
probability O
mass O
over O
the O
data O
space O
( O
see O
Figure O
2 O
for O
an O
example O
) O
. O
section O
: O
AUTOMATING Task
REGULATION Task
The O
problem O
of O
keeping O
the O
discriminator O
and O
generator O
in O
balance O
has O
been O
widely O
recognized O
in O
previous O
work O
with O
GANs Method
. O
Issues O
with O
unstable O
dynamics O
, O
oscillatory O
behavior O
, O
and O
generator Task
collapse Task
are O
not O
uncommon O
. O
In O
addition O
, O
the O
discriminator Method
is O
often O
times O
able O
to O
achieve O
a O
high O
degree O
of O
classification Metric
accuracy Metric
( O
producing O
a O
single O
scalar O
) O
before O
the O
generator O
has O
made O
sufficient O
progress O
on O
the O
arguably O
more O
difficult O
generative Task
task Task
( O
producing O
a O
high O
dimensional O
sample O
) O
. O
[ O
reference O
] O
suggested O
label Method
smoothing Method
to O
reduce O
the O
vulnerability O
of O
the O
generator O
to O
a O
relatively O
superior O
discriminator O
. O
Here O
, O
we O
explore O
an O
approach O
that O
enables O
the O
generator O
to O
automatically O
temper O
the O
performance O
of O
the O
discriminator O
when O
necessary O
, O
but O
still O
encourages O
the O
generator O
to O
challenge O
itself O
against O
more O
accurate O
adversaries O
. O
Specifically O
, O
we O
augment O
the O
generator Metric
objective Metric
: O
min O
G O
, O
λ>0 O
where O
f O
( O
λ O
) O
is O
monotonically O
increasing O
in O
λ O
which O
appears O
in O
the O
softmax O
equations O
, O
( O
3 O
)-( O
5 O
) O
. O
In O
experiments O
, O
we O
simply O
set O
f O
( O
λ O
) O
= O
cλ O
with O
c O
a O
constant O
( O
e.g. O
, O
0.001 O
) O
. O
The O
generator O
is O
incentivized O
to O
increase O
λ O
to O
reduce O
its O
objective O
at O
the O
expense O
of O
competing O
against O
the O
best O
available O
adversary O
D O
* O
( O
see O
Appendix O
A.6 O
) O
. O
section O
: O
EVALUATION O
Evaluating Task
GANs Task
is O
still O
an O
open O
problem O
. O
In O
their O
original O
work O
, O
[ O
reference O
] O
report O
log Method
likelihood Method
estimates Method
from O
Gaussian Method
Parzen Method
windows Method
, O
which O
they O
admit O
, O
has O
high O
variance O
and O
is O
known O
not O
to O
perform O
well O
in O
high O
dimensions O
. O
[ O
reference O
] O
recommend O
avoiding O
Parzen Method
windows Method
and O
argue O
that O
generative Method
models Method
should O
be O
evaluated O
with O
respect O
to O
their O
intended O
application O
. O
[ O
reference O
] O
suggest O
an O
Inception Metric
score Metric
, O
however O
, O
it O
assumes O
labels O
exist O
for O
the O
dataset O
. O
Recently O
, O
[ O
reference O
] O
introduced O
the O
Generative Method
Adversarial Method
Metric Method
( Method
GAM Method
) Method
for O
making O
pairwise Task
comparisons Task
between O
independently O
trained O
GAN Method
models O
. O
The O
core O
idea O
behind O
their O
approach O
is O
given O
two O
generator Method
, Method
discriminator Method
pairs Method
( O
G O
1 O
, O
D O
1 O
) O
and O
( O
G O
2 O
, O
D O
2 O
) O
, O
we O
should O
be O
able O
to O
learn O
their O
relative O
performance O
by O
judging O
each O
generator O
under O
the O
opponent O
's O
discriminator O
. O
section O
: O
METRIC O
In O
GMAN Method
, O
the O
opponent O
may O
have O
multiple O
discriminators O
, O
which O
makes O
it O
unclear O
how O
to O
perform O
the O
swaps O
needed O
for O
GAM O
. O
We O
introduce O
a O
variant O
of O
GAM Method
, O
the O
generative Metric
multi Metric
- Metric
adversarial Metric
metric Metric
( O
GMAM Method
) O
, O
that O
is O
amenable O
to O
training O
with O
multiple Method
discriminators Method
, O
where O
a O
and O
b O
refer O
to O
the O
two O
GMAN Method
variants O
( O
see O
Section O
3 O
for O
notation O
F O
G O
( O
V O
i O
) O
) O
. O
The O
idea O
here O
is O
similar O
. O
If O
G O
2 O
performs O
better O
than O
G O
1 O
with O
respect O
to O
both O
D O
1 O
and O
D O
2 O
, O
then O
GMAM>0 Method
( O
remember O
V O
≤0 O
always O
) O
. O
If O
G O
1 O
performs O
better O
in O
both O
cases O
, O
GMAM<0 Method
, O
otherwise O
, O
the O
result O
is O
indeterminate O
. O
section O
: O
EXPERIMENTS O
We O
evaluate O
the O
aforementioned O
variations O
of O
GMAN Method
on O
a O
variety O
of O
image Task
generation Task
tasks O
: O
MNIST Material
( O
LeCun O
et O
al O
. O
( O
1998 O
) O
) O
, O
CIFAR Material
- Material
10 Material
( O
Krizhevsky O
( O
2009 O
) O
) O
and O
CelebA Material
[ O
reference O
] O
) O
. O
We O
focus O
on O
rates O
of O
convergence Metric
to O
steady O
state O
along O
with O
quality O
of O
the O
steady Method
state Method
generator Method
according O
to O
the O
GMAM Method
metric O
. O
To O
summarize O
, O
loosely O
in O
order O
of O
increasing O
discriminator O
leniency O
, O
we O
compare O
• O
F Method
- Method
boost Method
: O
A O
single O
AdaBoost Method
. O
OL Method
- Method
boosted Method
discriminator Method
( O
see O
Appendix O
A.7 O
) O
. O
• O
P Method
- Method
boost Method
: O
D O
i O
is O
trained O
according O
to O
AdaBoost Method
. O
OL O
. O
A O
max Method
over O
the O
weak O
learner O
losses O
is O
presented O
to O
the O
generator O
instead O
of O
the O
boosted Method
prediction Method
( O
see O
Appendix O
A.7 O
) O
. O
• O
GMAN Method
- O
max O
: O
max{V O
i O
} O
is O
presented O
to O
the O
generator O
. O
• O
GAN Method
: O
Standard O
GAN Method
with O
a O
single O
discriminator Method
( O
see O
Appendix O
A.2 O
) O
. O
• O
mod O
- O
GAN Method
: O
GAN Method
with O
modified O
objective O
( O
generator O
minimizes O
− O
log O
( O
D O
( O
G O
( O
z O
) O
) O
) O
. O
• O
GMAN Method
- O
λ O
: O
GMAN Method
with O
F O
: O
= O
arithmetic Method
softmax Method
with O
parameter O
λ O
. O
• O
GMAN Method
* O
: O
The O
arithmetic O
softmax O
is O
controlled O
by O
the O
generator Method
through O
λ Method
. O
All O
generator Method
and Method
discriminator Method
models Method
are O
deep O
( O
de Method
) Method
convolutional Method
networks Method
[ O
reference O
] O
) O
, O
and O
aside O
from O
the O
boosted Method
variants Method
, O
all O
are O
trained O
with O
Adam Method
[ O
reference O
] O
) O
and O
batch Method
normalization Method
[ O
reference O
] O
) O
. O
Discriminators Method
convert O
the O
real O
- O
valued O
outputs O
of O
their O
networks O
to O
probabilities O
with O
squashed Method
- Method
sigmoids Method
to O
prevent O
saturating O
logarithms O
in O
the O
minimax O
objective O
( O
+ O
1−2 O
1 O
+ O
e O
−z O
) O
. O
See O
Appendix O
A.8 O
for O
further O
details O
. O
We O
test O
GMAN Method
systems O
with O
N O
= O
{ O
2 O
, O
5 O
} O
discriminators O
. O
We O
maintain O
discriminator O
diversity O
by O
varying O
dropout O
and O
network O
depth O
. O
Figure O
3 O
reveals O
that O
increasing O
the O
number O
of O
discriminators Method
reduces O
the O
number O
of O
iterations O
to O
steady O
- O
state O
by O
2x O
on O
MNIST Material
; O
increasing O
N O
( O
the O
size O
of O
the O
discriminator Method
ensemble Method
) O
also O
has O
the O
added O
benefit O
of O
reducing O
the O
variance Metric
the O
minimax Metric
objective Metric
over O
runs O
. O
Figure O
4 O
displays O
the O
variance O
of O
the O
same O
objective O
over O
a O
sliding O
time O
window O
, O
reaffirming O
GMAN Method
's O
acceleration O
to O
steadystate O
. O
Figure O
5 O
corroborates O
this O
conclusion O
with O
recognizable O
digits O
appearing O
approximately O
an O
epoch O
before O
the O
single O
discriminator O
run O
; O
digits O
at O
steady O
- O
state O
appear O
slightly O
sharper O
as O
well O
. O
section O
: O
MNIST Material
Our O
GMAM Method
metric O
( O
see O
Table O
1 O
) O
agrees O
with O
the O
relative O
quality O
of O
images O
in O
Figure O
5 O
with O
GMAN Method
* O
achieving O
the O
best O
overall O
performance O
. O
section O
: O
CELEBA Material
& O
CIFAR Material
- Material
10 Material
We O
see O
similar O
accelerated O
convergence O
behavior O
for O
the O
CelebA Material
dataset O
in O
Figure O
8 O
. O
We O
also O
found O
that O
GMAN Method
is O
robust O
to O
mode Task
collapse Task
. O
We O
believe O
this O
is O
because O
the O
generator O
must O
appease O
a O
diverse O
set O
of O
discriminators O
in O
each O
minibatch O
. O
Emitting O
a O
single O
sample O
will O
score O
well O
for O
one O
discriminator O
at O
the O
expense O
of O
the O
rest O
of O
the O
discriminators O
. O
Current O
solutions O
( O
e.g. O
, O
minibatch O
discrimination O
) O
are O
quadratic O
in O
batch O
size O
. O
GMAN Method
, O
however O
, O
is O
linear O
in O
batch O
size O
. O
section O
: O
CONCLUSION O
We O
introduced O
multiple O
discriminators Method
into O
the O
GAN Method
framework O
and O
explored O
discriminator O
roles O
ranging O
from O
a O
formidable Method
adversary Method
to O
a O
forgiving O
teacher O
. O
Allowing O
the O
generator O
to O
automatically O
tune O
its O
learning Method
schedule Method
( O
GMAN Method
* O
) O
outperformed O
GANs Method
with O
a O
single O
discriminator Method
on O
MNIST Material
. O
In O
general O
, O
GMAN Method
variants O
achieved O
faster O
convergence Metric
to O
a O
higher O
quality O
steady O
state O
on O
a O
variety O
of O
tasks O
as O
measured O
by O
a O
GAM Metric
- Metric
type Metric
metric Metric
( O
GMAM Method
) O
. O
In O
addition O
, O
GMAN Method
makes O
using O
the O
original O
GAN Method
objective O
possible O
by O
increasing O
the O
odds O
of O
the O
generator O
receiving O
constructive O
feedback O
. O
In O
future O
work O
, O
we O
will O
look O
at O
more O
sophisticated O
mechanisms O
for O
letting O
the O
generator O
control O
the O
game O
as O
well O
as O
other O
ways O
to O
ensure O
diversity O
among O
the O
discriminators O
. O
Introducing O
multiple O
generators O
is O
conceptually O
an O
obvious O
next O
step O
, O
however O
, O
we O
expect O
difficulties O
to O
arise O
from O
more O
complex O
game O
dynamics O
. O
For O
this O
reason O
, O
game Method
theory Method
and O
game Task
design Task
will O
likely O
be O
important O
. O
See O
Tables O
2 O
, O
3 O
[ O
reference O
] O
. O
Increasing O
the O
number O
of O
discriminators O
from O
2 O
to O
5 O
on O
CIFAR Material
- Material
10 Material
significantly O
improves O
scores O
over O
the O
standard O
GAN Method
both O
in O
terms O
of O
the O
GMAM Method
metric O
and O
Inception Metric
scores Metric
. O
Table O
6 O
: O
Inception Metric
score Metric
means O
with O
standard O
deviations O
for O
select O
models O
on O
CIFAR Material
- Material
10 Material
. O
Higher O
scores O
are O
better O
. O
GMAN Method
variants O
were O
trained O
with O
five O
discriminators Method
. O
section O
: O
A.3 O
GENERATED O
IMAGES O
section O
: O
A.4 O
SOMEWHAT O
RELATED O
WORK O
A O
GAN Method
framework O
with O
two O
discriminators Method
appeared O
in O
[ O
reference O
] O
, O
however O
, O
it O
is O
applicable O
only O
in O
a O
semi Task
- Task
supervised Task
case Task
where O
a O
label O
can O
be O
assigned O
to O
subsets O
of O
the O
dataset O
( O
e.g. O
, O
X O
= O
{ O
X O
1 O
= O
Domain O
1 O
, O
X O
2 O
= O
Domain O
2 O
, O
. O
. O
. O
} O
) O
. O
In O
contrast O
, O
our O
framework O
applies O
to O
an O
unsupervised Task
scenario Task
where O
an O
obvious O
partition O
of O
the O
dataset O
is O
unknown O
. O
Furthermore O
, O
extending O
GMAN Method
to O
the O
semi Task
- Task
supervised Task
domain Task
- Task
adaptation Task
scenario Task
would O
suggest O
multiple O
discriminators O
per O
domain O
, O
therefore O
our O
line O
of O
research O
is O
strictly O
orthogonal O
to O
that O
of O
their O
multi Method
- Method
domain Method
discriminator Method
approach Method
. O
Also O
, O
note O
that O
assigning O
a O
discriminator O
to O
each O
domain O
is O
akin O
to O
prescribing O
a O
new O
discriminator O
to O
each O
value O
of O
a O
conditional O
variable O
in O
conditional Method
GANs Method
[ O
reference O
] O
) O
. O
In O
this O
case O
, O
we O
interpret O
GMAN Method
as O
introducing O
multiple O
conditional Method
discriminators Method
and O
not O
a O
discriminator O
for O
each O
of O
the O
possibly O
exponentially O
many O
conditional O
labels O
. O
In O
Section O
4.4 O
, O
we O
describe O
an O
approach O
to O
customize O
adversarial Method
training Method
to O
better O
suit O
the O
development O
of O
the O
generator O
. O
An O
approach O
with O
similar O
conceptual O
underpinnings O
was O
described O
in O
[ O
reference O
] O
, O
however O
, O
similar O
to O
the O
above O
, O
it O
is O
only O
admissible O
in O
a O
semi Task
- Task
supervised Task
scenario Task
whereas O
our O
applies O
to O
the O
unsupervised Task
case Task
. O
section O
: O
A.6 O
UNCONSTRAINED Task
OPTIMIZATION Task
To O
convert O
GMAN Method
* O
minimax O
formulation O
to O
an O
unconstrained Task
minimax Task
formulation Task
, O
we O
introduce O
an O
auxiliary O
variable O
, O
Λ O
, O
define O
λ O
( O
Λ O
) O
= O
log O
( O
1 O
+ O
e O
Λ O
) O
, O
and O
let O
the O
generator O
minimize O
over O
Λ O
∈ O
R. O
A.7 O
BOOSTING Method
WITH O
AdaBoost Method
. O
OL O
AdaBoost Method
. O
OL O
[ O
reference O
] O
) O
does O
not O
require O
knowledge O
of O
the O
weak Method
learner Method
's O
slight O
edge O
over O
random Method
guessing Method
( O
P O
( O
correct O
label O
) O
= O
0.5 O
+ O
γ O
∈ O
( O
0 O
, O
0.5 O
] O
) O
, O
and O
in O
fact O
, O
allows O
γ O
< O
0 O
. O
This O
is O
crucial O
because O
our O
weak Method
learners Method
are O
deep Method
nets Method
with O
unknown O
, O
possibly O
negative O
, O
γ O
's O
. O
Figure O
16 O
: O
Sample O
of O
pictures O
generated O
across O
4 O
independent O
runs O
on O
MNIST Material
with O
F Method
- Method
boost Method
( O
similar O
results O
with O
P Method
- Method
boost Method
) O
. O
section O
: O
A.8 O
EXPERIMENTAL O
SETUP O
All O
experiments O
were O
conducted O
using O
an O
architecture O
similar O
to O
DCGAN Method
[ O
reference O
] O
) O
. O
We O
use O
convolutional Method
transpose Method
layers Method
[ O
reference O
] O
) O
for O
G O
and O
strided Method
convolutions Method
for O
D O
except O
for O
the O
input O
of O
G O
and O
the O
last O
layer O
of O
D. O
We O
use O
the O
single Method
step Method
gradient Method
method Method
as O
in O
[ O
reference O
] O
) O
, O
and O
batch Method
normalization Method
[ O
reference O
] O
) O
was O
used O
in O
each O
of O
the O
generator Method
layers Method
. O
The O
different O
discriminators O
were O
trained O
with O
varying O
dropout Metric
rates Metric
from O
[ O
reference O
] O
. O
Variations O
in O
the O
discriminators O
were O
effected O
in O
two O
ways O
. O
We O
varied O
the O
architecture O
by O
varying O
the O
number O
of O
filters O
in O
the O
discriminator O
layers O
( O
reduced O
by O
factors O
of O
2 O
, O
4 O
and O
so O
on O
) O
, O
as O
well O
as O
varying O
dropout Metric
rates Metric
. O
Secondly O
we O
also O
decorrelated O
the O
samples O
that O
the O
disriminators O
were O
training O
on O
by O
splitting O
the O
minibatch O
across O
the O
discriminators O
. O
The O
code O
was O
written O
in O
Tensorflow Method
[ O
reference O
] O
) O
and O
run O
on O
Nvidia Method
GTX Method
980 Method
GPUs Method
. O
Code O
to O
reproduce O
experiments O
and O
plots O
is O
at O
https: O
// O
github.com O
/ O
iDurugkar O
/ O
GMAN Method
. O
Specifics O
for O
the O
MNIST Material
architecture O
and O
training Task
are O
: O
• O
Generator Method
latent Method
variables Method
z O
∼ O
U O
( O
−1 O
, O
1 O
) O
section O
: O
100 O
• O
Generator Method
convolution Method
transpose Method
layers Method
: O
( O
4 O
, O
4 O
, O
128 O
) O
, O
[ O
reference O
] O
, O
( O
32 O
, O
32 O
, O
1 O
) O
• O
Base Method
Discriminator Method
architecture Method
: O
( O
32 O
, O
32 O
, O
1 O
) O
, O
[ O
reference O
] O
, O
( O
4 O
, O
4 O
, O
128 O
) O
. O
• O
Variants O
have O
either O
convolution O
3 O
( O
4 O
, O
4 O
, O
128 O
) O
removed O
or O
all O
the O
filter O
sizes O
are O
divided O
by O
2 O
or O
4 O
. O
That O
is O
, O
( O
32 O
, O
32 O
, O
1 O
) O
, O
[ O
reference O
] O
, O
( O
4 O
, O
4 O
, O
64 O
) O
or O
( O
32 O
, O
32 O
, O
1 O
) O
, O
[ O
reference O
] O
. O
• O
ReLu O
activations O
for O
all O
the O
hidden O
units O
. O
Tanh O
activation O
at O
the O
output O
units O
of O
the O
generator Method
. O
Sigmoid Method
at O
the O
output O
of O
the O
Discriminator Method
. O
• O
Training O
was O
performed O
with O
Adam Method
[ O
reference O
] O
) O
( O
lr O
= O
2 O
× O
10 O
−4 O
, O
β O
1 O
= O
0.5 O
) O
. O
• O
MNIST Material
was O
trained O
for O
20 O
epochs O
with O
a O
minibatch O
of O
size O
100 O
. O
• O
CelebA Material
and O
CIFAR Material
were O
trained O
over O
24000 O
iterations O
with O
a O
minibatch O
of O
size O
100 O
. O
section O
: O
section O
: O
ACKNOWLEDGMENTS O
We O
acknowledge O
helpful O
conversations O
with O
Stefan O
Dernbach O
, O
Archan O
Ray O
, O
Luke O
Vilnis O
, O
Ben O
Turtel O
, O
Stephen O
Giguere O
, O
Rajarshi O
Das O
, O
and O
Subhransu O
Maji O
. O
We O
also O
thank O
NVIDIA O
for O
donating O
a O
K40 O
GPU O
. O
This O
material O
is O
based O
upon O
work O
supported O
by O
the O
National O
Science O
Foundation O
under O
Grant O
Nos O
. O
IIS O
- O
1564032 O
. O
Any O
opinions O
, O
findings O
, O
and O
conclusions O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
authors O
and O
do O
not O
necessarily O
reflect O
the O
views O
of O
the O
NSF O
. O
section O
: O
