document O
: O
Enhanced Task
Image Task
Classification Task
With O
a O
Fast Method
- Method
Learning Method
Shallow Method
Convolutional Method
Neural Method
Network Method
We O
present O
a O
neural Method
network Method
architecture Method
and O
training Method
method Method
designed O
to O
enable O
very O
rapid O
training Task
and O
low Metric
implementation Metric
complexity Metric
. O
Due O
to O
its O
training Metric
speed Metric
and O
very O
few O
tunable O
parameters O
, O
the O
method O
has O
strong O
potential O
for O
applications O
requiring O
frequent O
retraining Task
or O
online Task
training Task
. O
The O
approach O
is O
characterized O
by O
( O
a O
) O
convolutional Method
filters Method
based O
on O
biologically Method
inspired Method
visual Method
processing Method
filters Method
, O
( O
b O
) O
randomly Method
- Method
valued Method
classifier Method
- Method
stage Method
input Method
weights Method
, O
( O
c O
) O
use O
of O
least Method
squares Method
regression Method
to O
train O
the O
classifier O
output O
weights O
in O
a O
single O
batch O
, O
and O
( O
d O
) O
linear Method
classifier Method
- Method
stage Method
output Method
units Method
. O
We O
demonstrate O
the O
efficacy O
of O
the O
method O
by O
applying O
it O
to O
image Task
classification Task
. O
Our O
results O
match O
existing O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
MNIST Material
( O
0.37 O
% O
error Metric
) O
and O
NORB O
- O
small O
( O
2.2 O
% O
error Metric
) O
image Task
classification Task
databases O
, O
but O
with O
very O
fast O
training Metric
times Metric
compared O
to O
standard O
deep Method
network Method
approaches Method
. O
The O
network O
’s O
performance O
on O
the O
Google Material
Street Material
View Material
House Material
Number Material
( O
SVHN Material
) O
( O
4 O
% O
error Metric
) O
database O
is O
also O
competitive O
with O
state O
- O
of O
- O
the O
art O
methods O
. O
section O
: O
Introduction O
State O
- O
of O
- O
the O
- O
art O
performance O
on O
many O
image Task
classification Task
databases O
has O
been O
achieved O
recently O
using O
multilayered Method
( Method
i.e. Method
, Method
deep Method
) Method
neural Method
networks Method
. O
Such O
performance O
generally O
relies O
on O
a O
convolutional Method
feature Method
extraction Method
stage Method
to O
obtain O
invariance O
to O
translations O
, O
rotations O
and O
scale O
. O
Training Task
of Task
deep Task
networks Task
, O
however O
, O
often O
requires O
significant O
resources O
, O
in O
terms O
of O
time O
, O
memory Metric
and O
computing Metric
power Metric
( O
e.g. O
in O
the O
order O
of O
hours O
on O
GPU O
clusters O
) O
. O
Tasks O
that O
require O
online Task
learning Task
, O
or O
periodic O
replacement O
of O
all O
network O
weights O
based O
on O
fresh O
data O
may O
thus O
not O
be O
able O
to O
benefit O
from O
deep Method
learning Method
techniques Method
. O
It O
is O
desirable O
, O
therefore O
, O
to O
seek O
very O
rapid O
training Method
methods Method
, O
even O
if O
this O
is O
potentially O
at O
the O
expense O
of O
a O
small O
performance O
decrease O
. O
Recent O
work O
has O
shown O
that O
good O
performance O
on O
image Task
classification Task
tasks O
can O
be O
achieved O
in O
‘ O
shallow Method
’ Method
convolutional Method
networks Method
— O
neural Method
architectures Method
containing O
a O
single O
training O
layer O
— O
provided O
sufficiently O
many O
features O
are O
extracted O
. O
Perhaps O
surprisingly O
, O
such O
performance O
arises O
even O
with O
the O
use O
of O
entirely O
random Method
convolutional Method
filters Method
or O
filters Method
based O
on O
randomly O
selected O
patches O
from O
training O
images O
. O
Although O
application O
of O
a O
relatively O
large O
numbers O
of O
filters O
is O
common O
( O
followed O
by O
spatial Method
image Method
smoothing Method
and O
downsampling Method
) O
, O
good O
classification Task
performance O
can O
also O
be O
obtained O
with O
a O
sparse Method
feature Method
representation Method
( O
i.e. O
relatively O
few O
filters O
and O
minimal O
downsampling O
) O
. O
Based O
on O
these O
insights O
and O
the O
goal O
of O
devising O
a O
fast O
training Method
method Method
, O
we O
introduce O
a O
method O
for O
combining O
several O
existing O
general Method
techniques Method
into O
what O
is O
equivalent O
to O
a O
five Method
layer Method
neural Method
network Method
( O
see O
Figure O
[ O
reference O
] O
) O
with O
only O
a O
single O
trained O
layer O
( O
the O
output O
layer O
) O
, O
and O
show O
that O
the O
method O
: O
produces O
state O
- O
of O
- O
the O
- O
art O
results O
on O
well O
known O
image Task
classification Task
databases O
; O
is O
trainable O
in O
times O
in O
the O
order O
of O
minutes O
( O
up O
to O
several O
hours O
for O
large O
training O
sets O
) O
on O
standard O
desktop O
/ O
laptop O
computers O
; O
is O
sufficiently O
versatile O
that O
the O
same O
hyper O
- O
parameter O
sets O
can O
be O
applied O
to O
different O
datasets O
and O
still O
produce O
results O
comparable O
to O
dataset O
- O
specific O
optimisation O
of O
hyper O
- O
parameters O
. O
The O
fast Method
training Method
method Method
we O
use O
has O
been O
developed O
independently O
several O
times O
and O
has O
gained O
increasing O
recognition O
in O
recent O
years O
— O
see O
for O
recent O
reviews O
of O
the O
different O
contexts O
and O
applications O
. O
The O
network Method
architecture Method
in O
the O
classification Task
stage Task
is O
that O
of O
a O
three Method
layer Method
neural Method
network Method
comprised O
from O
an O
input O
layer O
, O
a O
hidden Method
layer Method
of Method
nonlinear Method
units Method
, O
and O
a O
linear Method
output Method
layer Method
. O
The O
input O
weights O
are O
randomly O
chosen O
and O
untrained O
, O
and O
the O
output O
weights O
are O
trained O
in O
a O
single O
batch O
using O
least Method
squares Method
regression Method
. O
Due O
to O
the O
convexity O
of O
the O
objective O
function O
, O
this O
method O
ensures O
the O
output O
weights O
are O
optimally O
chosen O
for O
a O
given O
set O
of O
random O
input O
weights O
. O
The O
rapid O
speed O
of O
training Task
is O
due O
to O
the O
fact O
that O
the O
least Task
squares Task
optimisation Task
problem Task
an O
be O
solved O
using O
an O
O Method
( Method
) Method
algorithm Method
, O
where O
is O
the O
number O
of O
hidden O
units O
and O
the O
number O
of O
training O
points O
. O
When O
applied O
to O
pixel O
- O
level O
features O
, O
these O
networks O
can O
be O
trained O
as O
discriminative Method
classifiers Method
and O
produce O
excellent O
results O
on O
simple O
image O
databases O
but O
poor O
performance O
on O
more O
difficult O
ones O
. O
To O
our O
knowledge O
, O
however O
, O
the O
method O
has O
not O
yet O
been O
applied O
to O
convolutional O
features O
. O
Therefore O
, O
we O
have O
devised O
a O
network Method
architecture Method
( O
see O
Figure O
[ O
reference O
] O
) O
that O
consists O
of O
three O
key O
elements O
that O
work O
together O
to O
ensure O
fast Task
learning Task
and O
good O
classification Task
performance O
: O
namely O
, O
the O
use O
of O
( O
a O
) O
convolutional Method
feature Method
extraction Method
, O
( O
b O
) O
random O
- O
valued O
input O
weights O
for O
classification Task
, O
( O
c O
) O
least Method
squares Method
training Method
of O
output O
weights O
that O
feed O
in O
to O
( O
d O
) O
linear O
output O
units O
. O
We O
apply O
our O
network O
to O
several O
image Task
classification Task
databases O
, O
including O
MNIST Material
, O
CIFAR Material
- Material
10 Material
, O
Google Material
Street Material
View Material
House Material
Numbers Material
( O
SVHN Material
) O
and O
NORB O
. O
The O
network O
produces O
state O
- O
of O
- O
the O
- O
art O
classification Task
results O
on O
MNIST Material
and O
NORB O
- O
small O
databases O
and O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
SVHN Material
. O
These O
promising O
results O
are O
presented O
in O
this O
paper O
to O
demonstrate O
the O
potential O
benefits O
of O
the O
method O
; O
clearly O
further O
innovations O
within O
the O
method O
are O
required O
if O
it O
is O
to O
be O
competitive O
on O
harder O
datasets O
like O
CIFAR Material
- Material
10 Material
, O
or O
Imagenet O
. O
We O
expect O
that O
the O
most O
likely O
avenues O
for O
improving O
our O
presented O
results O
for O
CIFAR Material
- Material
10 Material
, O
whilst O
retaining O
the O
method O
’s O
core O
attributes O
, O
are O
( O
1 O
) O
to O
introduce O
limited O
training O
of O
the O
Stage Method
1 Method
filters Method
by O
generalizing O
the O
method O
of O
; O
( O
2 O
) O
introduction O
of O
training Task
data Task
augmentation Task
. O
We O
aim O
to O
pursuing O
these O
directions O
in O
our O
future O
work O
. O
The O
remainder O
of O
the O
paper O
is O
organized O
as O
follows O
. O
Section O
[ O
reference O
] O
contains O
a O
generic O
description O
of O
the O
network Method
architecture Method
and O
the O
algorithms O
we O
use O
for O
obtaining O
convolutional O
features O
and O
classifying O
inputs O
based O
on O
them O
. O
Section O
[ O
reference O
] O
describes O
how O
the O
generic Method
architecture Method
and O
training Method
algorithms Method
are O
specifically O
applied O
to O
four O
well O
- O
known O
benchmark O
image Task
classification Task
datasets O
. O
Next O
, O
Section O
[ O
reference O
] O
describes O
the O
results O
we O
obtained O
for O
these O
datasets O
, O
and O
finally O
the O
paper O
concludes O
with O
discussion O
and O
remarks O
in O
Section O
[ O
reference O
] O
. O
section O
: O
Network Method
architecture Method
and O
training Method
algorithms Method
The O
overall O
network O
is O
shown O
in O
Figure O
[ O
reference O
] O
. O
There O
are O
three O
hidden O
layers O
with O
nonlinear O
units O
, O
and O
four O
layers O
of O
weights O
. O
The O
first O
layer O
of O
weights O
is O
the O
convolutional Method
filter Method
layer Method
. O
The O
second O
layer O
is O
a O
pooling Method
( Method
low Method
pass Method
filtering Method
) O
and O
downsampling Method
layer Method
. O
The O
third O
layer O
is O
a O
random Method
projection Method
layer Method
. O
The O
fourth O
layer O
is O
the O
only O
trained O
layer O
. O
The O
output Method
layer Method
has O
linear O
units O
. O
The O
network O
can O
be O
conceptually O
divided O
into O
two O
stages O
and O
two O
algorithms O
, O
that O
to O
our O
knowledge O
have O
not O
previously O
been O
combined O
. O
The O
first O
stage O
is O
the O
convolutional Method
feature Method
extraction Method
stage Method
, O
and O
largely O
follows O
that O
of O
existing O
approaches O
to O
image Task
classification Task
. O
The O
second O
stage O
is O
the O
classifier Method
stage Method
, O
and O
largely O
follows O
the O
approach O
of O
. O
We O
now O
describe O
the O
two O
stages O
in O
detail O
. O
subsection O
: O
Stage O
1 O
Architecture O
: O
Convolutional Method
filtering Method
and O
pooling Method
The O
algorithm O
we O
apply O
to O
extract O
features O
from O
images O
( O
including O
those O
with O
multiple O
channels O
) O
is O
summarised O
in O
Algorithm O
. O
Note O
that O
the O
details O
of O
the O
filters O
and O
described O
in O
Algorithm O
are O
given O
in O
Section O
[ O
reference O
] O
, O
but O
here O
we O
introduce O
the O
size O
of O
these O
two O
- O
dimensional Method
filters Method
as O
and O
. O
The O
functions O
and O
are O
nonlinear Method
transformations Method
applied O
termwise O
to O
matrix O
inputs O
to O
produce O
matrix O
outputs O
of O
the O
same O
size O
. O
The O
symbol O
* O
represents O
two O
- O
dimensional Method
convolution Method
. O
InputInput O
OutputOutput O
Convolutional O
feature O
detection O
. O
Set O
of O
images O
, O
, O
each O
with O
channels O
Feature O
vectors O
, O
, O
split O
into O
channels O
, O
, O
filters O
Apply O
filter O
to O
each O
channel O
: O
Apply O
termwise Method
nonlinearity Method
: O
Apply O
lowpass Method
filter Method
: O
Apply O
termwise Method
nonlinearity Method
: O
Downsample Method
: O
Concatenate Method
channels Method
: O
Normalize O
: O
Concatenate Method
over Method
filters Method
: O
This O
sequence O
of O
steps O
in O
Algorithm O
suggest O
looping O
over O
all O
images O
and O
channels O
sequentially O
. O
However O
, O
the O
following O
mathematical O
formulation O
of O
the O
algorithm O
indicates O
a O
standard O
layered Method
neural Method
network Method
formulation Method
of O
this O
algorithm O
is O
applicable O
, O
as O
shown O
in O
Figure O
[ O
reference O
] O
, O
and O
therefore O
that O
computation O
of O
all O
features O
( O
) O
can O
be O
obtained O
in O
one O
shot O
from O
a O
- O
column O
matrix O
containing O
a O
batch O
of O
training O
points O
. O
The O
key O
to O
this O
formulation O
is O
to O
note O
that O
since O
convolution Method
is O
a O
linear Method
operator Method
, O
a O
matrix O
can O
be O
constructed O
that O
when O
multiplied O
by O
a O
data O
matrix O
produces O
the O
same O
result O
as O
convolution Method
applied O
to O
one O
instance O
of O
the O
data O
. O
Hence O
, O
for O
a O
total O
of O
features O
per O
image O
, O
we O
introduce O
the O
following O
matrices O
. O
Let O
be O
a O
feature O
matrix O
of O
size O
; O
be O
a O
data O
matrix O
with O
columns O
; O
be O
a O
concatenation O
of O
the O
convolution Method
matrices Method
corresponding O
to O
; O
be O
a O
convolution Method
matrix Method
corresponding O
to O
, O
that O
also O
down O
samples O
by O
a O
factor O
of O
; O
be O
a O
block O
diagonal O
matrix O
containing O
copies O
of O
on O
the O
diagonals O
. O
The O
entire O
flow O
described O
in O
Algorithm O
[ O
reference O
] O
can O
be O
written O
mathematically O
as O
where O
and O
are O
applied O
term O
by O
term O
to O
all O
elements O
of O
their O
arguments O
. O
The O
matrices O
and O
are O
sparse O
Toeplitz O
matrices O
. O
In O
practice O
we O
would O
not O
form O
them O
directly O
, O
but O
instead O
form O
one O
pooling O
matrix O
, O
and O
one O
filtering Method
matrix Method
for O
each O
filter O
, O
and O
sequential O
apply O
each O
filter O
to O
the O
entire O
data O
matrix O
, O
. O
We O
use O
a O
particular O
form O
for O
the O
nonlinear O
hidden O
- O
unit O
functions O
and O
inspired O
by O
LP Method
- Method
pooling Method
, O
which O
is O
of O
the O
form O
and O
. O
For O
example O
, O
with O
we O
have O
An O
intuitive O
explanation O
for O
the O
use O
of O
LP Method
- Method
pooling Method
is O
as O
follows O
. O
First O
, O
note O
that O
each O
hidden Method
unit Method
receives O
as O
input O
a O
linear O
combination O
of O
a O
patch O
of O
the O
input O
data O
, O
i.e. O
in O
has O
the O
form O
. O
Hence O
, O
squaring O
results O
in O
a O
sum O
that O
contains O
terms O
proportional O
to O
and O
terms O
proportional O
to O
products O
of O
each O
. O
Thus O
, O
squaring Method
is O
a O
simple O
way O
to O
produce O
hidden O
layer O
responses O
that O
depend O
on O
the O
product O
of O
pairs O
of O
input O
data O
elements O
, O
i.e. O
interaction O
terms O
, O
and O
this O
is O
important O
for O
discriminability Task
. O
Second O
, O
the O
square Method
root Method
transforms O
the O
distribution O
of O
the O
hidden O
- O
unit O
responses O
; O
we O
have O
observed O
that O
in O
practice O
, O
the O
result O
of O
the O
square Method
root Method
operation Method
is O
often O
a O
distribution O
that O
is O
closer O
to O
Gaussian O
than O
without O
it O
, O
which O
helps O
to O
regularise O
the O
least Method
squares Method
regression Method
method Method
of O
training O
the O
output O
weights O
. O
However O
, O
as O
will O
be O
described O
shortly O
, O
the O
classifier Method
of O
Stage O
2 O
also O
has O
a O
square O
nonlinearity O
. O
Using O
this O
nonlinearity O
, O
we O
have O
found O
that O
classification Task
performance O
is O
generally O
optimised O
by O
taking O
the O
square O
root O
of O
the O
input O
to O
the O
random Method
projection Method
layer Method
. O
Based O
on O
this O
observation O
, O
we O
do O
not O
strictly O
use O
LP Method
- Method
pooling Method
, O
and O
instead O
set O
and O
This O
effectively O
combines O
the O
implementation O
of O
L2 Method
- Method
pooling Method
, O
and O
the O
subsequent O
square Method
root Method
operation Method
. O
subsection O
: O
Stage O
2 O
Architecture O
: O
Classifier Method
The O
following O
descriptions O
are O
applicable O
whether O
or O
not O
raw O
pixels O
are O
treated O
as O
features O
or O
the O
input O
is O
the O
features O
extracted O
in O
stage O
1 O
. O
First O
, O
we O
introduce O
notation O
. O
Let O
: O
, O
of O
size O
, O
contain O
each O
length O
feature O
vector O
; O
be O
an O
indicator O
matrix O
of O
size O
, O
which O
numerically O
represents O
the O
labels O
of O
each O
training O
vector O
, O
where O
there O
are O
classes O
— O
we O
set O
each O
column O
to O
have O
a O
in O
a O
single O
row O
, O
corresponding O
to O
the O
label O
class O
for O
each O
training O
vector O
, O
and O
all O
other O
entries O
to O
be O
zero O
; O
, O
of O
size O
be O
the O
real O
- O
valued O
input O
weights O
matrix O
for O
the O
classifier Method
stage Method
; O
, O
of O
size O
be O
the O
real O
- O
valued O
output O
weights O
matrix O
for O
the O
classifier Method
stage Method
; O
the O
function O
be O
the O
activation O
function O
of O
each O
hidden O
- O
unit O
; O
for O
example O
, O
may O
be O
the O
logistic O
sigmoid O
, O
, O
or O
a O
squarer O
, O
; O
, O
of O
size O
, O
contain O
the O
hidden O
- O
unit O
activations O
that O
occur O
due O
to O
each O
feature O
vector O
; O
is O
applied O
termwise O
to O
each O
element O
in O
the O
matrix O
. O
subsection O
: O
Stage O
1 O
Training O
: O
Filters Method
and O
Pooling Task
In O
this O
paper O
we O
do O
not O
employ O
any O
form O
of O
training Method
for O
the O
filters O
and O
pooling O
matrices O
. O
The O
details O
of O
the O
filter O
weights O
and O
form O
of O
pooling Method
used O
for O
the O
example O
classification Task
problems Task
presented O
in O
this O
paper O
are O
given O
Section O
[ O
reference O
] O
. O
subsection O
: O
Stage O
2 O
Training O
: O
Classifier Method
Weights Method
The O
training O
approach O
for O
the O
classifier Method
is O
that O
described O
by O
e.g. O
. O
The O
default O
situation O
for O
these O
methods O
is O
that O
the O
input O
weights O
, O
, O
are O
generated O
randomly O
from O
a O
specific O
distribution O
, O
e.g. O
standard O
Gaussian O
, O
uniform O
, O
or O
bipolar O
. O
However O
, O
it O
is O
known O
that O
setting O
these O
weights O
non O
- O
randomly O
based O
on O
the O
training O
data O
leads O
to O
superior O
performance O
. O
In O
this O
paper O
, O
we O
use O
the O
method O
of O
. O
The O
input O
weights O
can O
also O
be O
trained O
iteratively O
, O
if O
desired O
, O
using O
single Method
- Method
batch Method
backpropagation Method
. O
Given O
a O
choice O
of O
, O
the O
output O
weights O
matrix O
is O
determined O
according O
to O
where O
is O
the O
size O
Moore O
- O
Penrose O
pseudo O
inverse O
corresponding O
to O
. O
This O
solution O
is O
equivalent O
to O
least Method
squares Method
regression Method
applied O
to O
an O
overcomplete O
set O
of O
linear O
equations O
, O
with O
an O
- O
dimensional O
target O
. O
It O
is O
known O
to O
often O
be O
useful O
to O
regularise O
such O
problems O
, O
and O
instead O
solve O
the O
following O
ridge Task
regression Task
problem Task
: O
where O
is O
a O
hyper O
- O
parameter O
and O
is O
the O
identity O
matrix O
. O
In O
practice O
, O
it O
is O
efficient O
to O
avoid O
explicit O
calculation O
of O
the O
inverse O
in O
Equation O
( O
[ O
reference O
] O
) O
and O
instead O
use O
QR Method
factorisation Method
to O
solve O
the O
following O
set O
of O
linear Method
equations Method
for O
the O
unknown O
variables O
in O
: O
Above O
we O
mentioned O
two O
algorithms O
, O
and O
Algorithm O
2 O
is O
simply O
to O
form O
and O
solve O
Eqn O
. O
( O
[ O
reference O
] O
) O
, O
followed O
by O
optimisation O
of O
using O
ridge Method
regression Method
. O
For O
large O
and O
( O
which O
is O
typically O
valid O
) O
the O
runtime Metric
bottleneck Metric
for O
this O
method O
is O
typically O
the O
O Method
matrix Method
multiplication Method
required O
to O
obtain O
the O
Gram O
matrix O
, O
. O
subsection O
: O
Application O
to O
Test O
Data O
For O
a O
total O
of O
test O
images O
contained O
in O
a O
matrix O
, O
we O
first O
obtain O
a O
matrix O
, O
of O
size O
, O
by O
following O
Algorithm O
. O
The O
output O
of O
the O
classifier Method
is O
then O
the O
matrix O
Note O
that O
we O
can O
write O
the O
response O
to O
all O
test O
images O
in O
terms O
of O
the O
training O
data O
: O
Thus O
, O
since O
the O
pseudo O
- O
inverse O
, O
, O
can O
be O
obtained O
from O
Equation O
( O
[ O
reference O
] O
) O
, O
Equations O
( O
[ O
reference O
] O
) O
, O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
constitute O
a O
closed O
- O
form Method
solution Method
for O
the O
entire O
test O
- O
data O
classification O
output O
, O
given O
specified O
matrices O
, O
, O
and O
, O
and O
hidden O
- O
unit O
activation O
functions O
, O
, O
and O
. O
The O
final O
classification Task
decision Task
for O
each O
image O
is O
obtained O
by O
taking O
the O
index O
of O
the O
maximum O
value O
of O
each O
column O
of O
. O
section O
: O
Image Task
Classification Task
Experiments O
: O
Specific O
Design O
We O
examined O
the O
method O
’s O
performance O
when O
used O
as O
a O
classifier Task
of Task
images Task
. O
Table O
[ O
reference O
] O
lists O
the O
attributes O
of O
four O
well O
known O
databases O
we O
used O
. O
For O
the O
two O
databases O
comprised O
from O
RGB O
images O
, O
we O
used O
channels O
, O
namely O
the O
raw O
RGB O
channels O
, O
and O
a O
conversion O
to O
greyscale O
. O
This O
approach O
was O
shown O
to O
be O
effective O
for O
SVHN Material
in O
. O
subsection O
: O
Preprocessing O
All O
raw O
image O
pixel O
values O
were O
scaled O
to O
the O
interval O
. O
Due O
to O
the O
use O
of O
quadratic Method
nonlinearities Method
and O
LP Method
- Method
pooling Method
, O
this O
scaling O
does O
not O
affect O
performance O
. O
The O
only O
other O
preprocessing O
done O
was O
as O
follows O
: O
MNIST Material
: O
None O
; O
NORB O
- O
small O
: O
downsample O
from O
96 O
96 O
to O
32 O
32 O
, O
for O
implementation O
efficiency O
reasons O
( O
this O
is O
consistent O
with O
some O
previous O
work O
on O
NORB O
- O
small O
, O
e.g. O
) O
; O
SVHN Material
: O
convert O
from O
3 O
channels O
to O
4 O
by O
adding O
a O
conversion O
to O
greyscale O
from O
the O
raw O
RGB O
. O
We O
found O
that O
local O
and O
/ O
or O
global O
contrast O
enhancement O
only O
diminished O
performance O
; O
CIFAR Material
- Material
10 Material
: O
convert O
from O
3 O
channels O
to O
4 O
by O
adding O
a O
conversion O
to O
greyscale O
from O
the O
raw O
RGB O
; O
apply O
ZCA Method
whitening Method
to O
each O
channel O
of O
each O
image O
, O
as O
in O
. O
subsection O
: O
Stage O
1 O
Design O
: O
Filters Method
and O
Pooling Task
Since O
our O
objective O
here O
was O
to O
train O
only O
a O
single O
layer O
of O
the O
network O
, O
we O
did O
not O
seek O
to O
train O
the O
network O
to O
find O
filters O
optimised O
for O
the O
training O
set O
. O
Instead O
, O
for O
the O
size Method
two Method
- Method
dimension Method
filters Method
, O
, O
we O
considered O
the O
following O
options O
: O
simple O
rotated Method
bar Method
and Method
corner Method
filters Method
, O
and O
square Method
uniform Method
centre Method
- Method
surround Method
filters Method
; O
filters O
trained O
on O
Imagenet O
and O
made O
available O
in O
Overfeat O
; O
we O
used O
only O
the O
96 O
stage O
- O
1 O
‘ O
accurate O
’ O
7 O
7 O
filters O
; O
patches O
obtained O
from O
the O
central O
region O
of O
randomly O
selected O
training O
images O
, O
with O
training O
images O
from O
each O
class O
. O
The O
filters O
from O
Overfeat Method
are O
RGB Method
filters Method
. O
Hence O
, O
for O
the O
databases O
with O
RGB O
images O
, O
we O
applied O
each O
channel O
of O
the O
filter O
to O
the O
corresponding O
channel O
of O
each O
image O
. O
When O
applied O
to O
greyscale O
channels O
, O
we O
converted O
the O
Overfeat Method
filter Method
to O
greyscale O
. O
For O
NORB Method
, O
we O
applied O
the O
same O
filter O
to O
both O
stereo O
channels O
. O
For O
all O
filters O
, O
we O
subtract O
the O
mean O
value O
over O
all O
dimensions O
in O
each O
channel O
, O
in O
order O
to O
ensure O
a O
mean O
of O
zero O
in O
each O
channel O
. O
In O
implementing O
the O
two Method
- Method
dimensional Method
convolution Method
operation Method
required O
for O
filtering O
the O
raw O
images O
using O
, O
we O
obtained O
only O
the O
central O
‘ O
valid O
’ O
region O
, O
i.e. O
for O
images O
of O
size O
, O
the O
total O
dimension O
of O
the O
valid O
region O
is O
. O
Consequently O
, O
the O
total O
number O
of O
features O
per O
image O
obtained O
prior O
to O
pooling O
, O
from O
filters Method
, O
and O
images O
with O
channels O
is O
. O
In O
previous O
work O
, O
e.g. O
, O
the O
form O
of O
the O
two Method
- Method
dimension Method
filter Method
, O
is O
a O
normalised Method
Gaussian Method
. O
Instead O
, O
we O
used O
a O
simple O
summing Method
filter Method
, O
equivalent O
to O
a O
kernel O
with O
all O
entries O
equal O
to O
the O
same O
value O
, O
i.e. O
In O
implementing O
the O
two O
- O
dimensional Method
convolution Method
operation Method
required O
for O
filtering Task
using O
, O
we O
obtained O
the O
‘ O
full O
’ O
convolutional O
region O
, O
which O
for O
images O
of O
size O
is O
, O
given O
the O
‘ O
valid O
’ O
convolution Method
first O
applied O
using O
, O
as O
described O
above O
. O
The O
remaining O
part O
of O
the O
pooling Method
step Method
is O
to O
downsample O
each O
image O
dimension O
by O
a O
factor O
of O
, O
resulting O
in O
a O
total O
of O
features O
per O
image O
. O
In O
choosing O
, O
we O
experimented O
with O
a O
variety O
of O
scales O
before O
settling O
on O
the O
value O
shown O
in O
Table O
[ O
reference O
] O
. O
We O
note O
there O
exists O
an O
interesting O
tradeoff O
between O
the O
number O
of O
filters O
, O
and O
the O
downsampling O
factor O
, O
. O
For O
example O
, O
in O
, O
, O
whereas O
in O
. O
We O
found O
that O
, O
up O
to O
a O
point O
, O
smaller O
enables O
a O
smaller O
number O
of O
filters O
, O
, O
for O
comparable O
performance O
. O
The O
hyper O
- O
parameters O
we O
used O
for O
each O
dataset O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
subsection O
: O
Stage O
2 O
Design O
: O
Classifier Method
projection Method
weights Method
To O
construct O
the O
matrix O
we O
use O
the O
method O
proposed O
by O
. O
In O
this O
method O
, O
each O
row O
of O
the O
matrix O
is O
chosen O
to O
be O
a O
normalized O
difference O
between O
the O
data O
vectors O
corresponding O
to O
randomly O
chosen O
examples O
from O
distinct O
classes O
of O
the O
training O
set O
. O
This O
method O
has O
previously O
been O
shown O
to O
be O
superior O
to O
setting O
the O
weights O
to O
values O
chosen O
from O
random O
distributions O
. O
For O
the O
nonlinearity O
in O
the O
classifier O
stage O
hidden O
units O
, O
, O
the O
typical O
choice O
in O
other O
work O
is O
a O
sigmoid O
. O
However O
, O
we O
found O
it O
sufficient O
( O
and O
much O
faster O
in O
an O
implementation O
) O
to O
use O
the O
quadratic O
nonlinearity O
. O
This O
suggests O
that O
good O
image Task
classification Task
is O
strongly O
dependent O
on O
the O
presence O
of O
interaction O
terms O
— O
see O
the O
discussion O
about O
this O
in O
Section O
[ O
reference O
] O
. O
subsection O
: O
Stage O
2 O
Design O
: O
Ridge Method
Regression Method
parameter Method
With O
these O
choices O
, O
there O
remains O
only O
two O
hyper O
- O
parameters O
for O
the O
Classifier Method
stage Method
: O
the O
regression O
parameter O
, O
, O
and O
the O
number O
of O
hidden O
- O
units O
, O
. O
In O
our O
experiments O
, O
we O
examined O
classification Metric
error Metric
rates Metric
as O
a O
function O
of O
varying O
. O
For O
each O
, O
we O
can O
optimize O
using O
cross Method
- Method
validation Method
. O
However O
, O
we O
also O
found O
that O
a O
good O
generic O
heuristic O
for O
setting O
was O
and O
this O
reduces O
the O
number O
of O
hyper O
- O
parameters O
for O
the O
classification Method
stage Method
to O
just O
one O
: O
the O
number O
of O
hidden O
- O
units O
, O
. O
subsection O
: O
Stage O
1 O
and O
2 O
Design O
: O
Nonlinearities O
For O
the O
hidden O
- O
layer O
nonlinearities O
, O
to O
reiterate O
, O
we O
use O
: O
section O
: O
Results O
We O
examined O
the O
performance O
of O
the O
network O
on O
classifying O
the O
test O
images O
in O
the O
four O
chosen O
databases O
, O
as O
a O
function O
of O
the O
number O
of O
filters O
, O
, O
the O
downsampling O
rate O
, O
and O
the O
number O
of O
hidden O
units O
in O
the O
classifier Method
stage Method
, O
. O
We O
use O
the O
maximum O
number O
of O
channels O
, O
, O
available O
in O
each O
dataset O
( O
recall O
from O
above O
that O
we O
convert O
RGB O
images O
to O
greyscale O
, O
as O
a O
fourth O
channel O
) O
. O
We O
considered O
the O
three O
kinds O
of O
untuned Method
filters Method
described O
in O
Section O
[ O
reference O
] O
, O
as O
well O
as O
combinations O
of O
them O
. O
We O
did O
not O
exhaustively O
consider O
all O
options O
, O
but O
settled O
on O
the O
Overfeat Method
filters Method
as O
being O
marginally O
superior O
for O
NORB O
, O
SVHN Material
and O
CIFAR Material
- Material
10 Material
( O
in O
the O
order O
of O
1 O
% O
in O
comparison O
with O
other O
options O
) O
, O
while O
hand Method
- Method
designed Method
filters Method
were O
superior O
for O
MNIST Material
, O
but O
only O
marginally O
compared O
to O
randomly O
selected O
patches O
from O
the O
training O
data O
. O
There O
is O
clearly O
more O
that O
can O
be O
investigated O
to O
determine O
whether O
hand Method
- Method
designed Method
filters Method
can O
match O
trained Method
filters Method
when O
using O
the O
method O
of O
this O
paper O
. O
subsection O
: O
Summary O
of O
best O
performance O
attained O
The O
best O
performance O
we O
achieved O
is O
summarised O
in O
Table O
[ O
reference O
] O
. O
subsection O
: O
Trend O
with O
increasing O
We O
now O
use O
MNIST Material
as O
an O
example O
to O
indicate O
how O
classification Metric
performance Metric
scales O
with O
the O
number O
of O
hidden O
units O
in O
the O
classifier Method
stage Method
, O
. O
The O
remain O
parameters O
were O
, O
and O
, O
which O
included O
hand Method
- Method
designed Method
filters Method
comprised O
from O
20 O
rotated O
bars O
( O
width O
of O
one O
pixel O
) O
, O
20 O
rotated O
corners O
( O
dimension O
4 O
pixels O
) O
and O
3 O
centred O
squares O
( O
dimensions O
3 O
, O
4 O
and O
5 O
pixels O
) O
, O
all O
with O
zero O
mean O
. O
The O
rotations O
were O
of O
binary Method
filters Method
and O
used O
standard O
pixel Method
value Method
interpolation Method
. O
Figure O
[ O
reference O
] O
shows O
a O
power O
law O
- O
like O
decrease O
in O
error Metric
rate O
as O
increases O
, O
with O
a O
linear O
trend O
on O
the O
log O
- O
log O
axes O
. O
The O
best O
error Metric
rate O
shown O
on O
this O
figure O
is O
0.40 O
% O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
we O
have O
attained O
a O
best O
repeatable O
rate O
of O
% O
using O
60 O
filters Method
and O
. O
When O
we O
combined O
Overfeat Method
filters Method
with O
hand Method
- Method
designed Method
filters Method
and O
randomly O
selected O
patches O
from O
the O
training O
data O
, O
we O
obtained O
up O
to O
% O
error Metric
on O
MNIST Material
, O
but O
this O
was O
an O
outlier O
since O
it O
was O
not O
repeatedly O
obtained O
by O
different O
samples O
of O
. O
subsection O
: O
Indicative Metric
training Metric
times Metric
For O
an O
implementation O
in O
Matlab Method
on O
a O
PC O
with O
4 O
cores O
and O
32 O
GB O
of O
RAM O
, O
for O
MNIST Material
( O
60000 O
training O
points O
) O
the O
total O
time O
required O
to O
generate O
all O
features O
for O
all O
60000 O
training O
images O
from O
one O
filter O
is O
approximately O
2 O
seconds O
. O
The O
largest O
number O
of O
filters O
we O
used O
to O
date O
was O
384 O
( O
96 O
RGB O
+ O
greyscale O
) O
, O
and O
when O
applied O
to O
SVHN Material
( O
600000 O
training O
points O
) O
, O
the O
total O
run Metric
time Metric
for O
feature Task
extraction Task
is O
then O
about O
two O
hours O
( O
in O
this O
case O
we O
used O
batches O
of O
size O
100000 O
images O
) O
. O
The O
runtime O
we O
achieve O
for O
feature Task
generation Task
benefits O
from O
carrying O
out O
convolutions Method
using O
matrix Method
multiplication Method
applied O
to O
large O
batches O
simultaneously O
; O
if O
instead O
we O
iterate O
over O
all O
training O
images O
individually O
, O
but O
still O
carry O
out O
convolutions Method
using O
matrix Method
multiplication Method
, O
the O
time O
for O
generating Task
features Task
approximately O
doubles O
. O
Note O
also O
that O
we O
employ O
Matlab Method
’s Method
sparse Method
matrix Method
data Method
structure Method
functionality Method
to O
represent O
and O
, O
which O
also O
provides O
a O
speed O
boost O
when O
multiplying O
these O
matrices O
to O
carry O
out O
the O
convolutions Method
. O
If O
we O
do O
not O
use O
the O
matrix Method
- Method
multiplication Method
method Method
for O
convolution Method
, O
and O
instead O
apply O
two O
- O
dimensional Method
convolutions Method
to O
each O
individual O
image O
, O
the O
feature Task
generation Task
is O
slowed O
even O
more O
. O
For O
the O
classifier Method
stage Method
, O
on O
MNIST Material
with O
, O
the O
runtime Metric
is O
approximately O
150 O
seconds O
for O
( O
there O
is O
a O
small O
time O
penalty O
for O
smaller O
, O
due O
to O
the O
larger O
dimension O
of O
the O
input O
to O
the O
classifier Method
stage Method
) O
. O
Hence O
, O
the O
total O
run Metric
time Metric
for O
MNIST Material
with O
40 O
filters Method
and O
is O
in O
the O
order O
of O
4 O
minutes O
to O
achieve O
a O
correct O
classification Metric
rate Metric
above O
99.5 O
% O
. O
With O
fewer O
filters O
and O
smaller O
, O
it O
is O
simple O
to O
achieve O
over O
99.2 O
% O
in O
a O
minute O
or O
less O
. O
For O
SVHN Material
and O
CIFAR Material
- Material
10 Material
where O
we O
scaled O
up O
to O
, O
the O
run Metric
time Metric
bottleneck Metric
is O
the O
classifier Method
, O
due O
to O
the O
runtime Metric
complexity Metric
. O
We O
found O
it O
necessary O
to O
use O
a O
PC O
with O
more O
RAM O
( O
peak O
usage O
was O
approximately O
70 O
GB O
) O
for O
. O
In O
the O
case O
of O
, O
the O
network O
was O
trained O
in O
under O
an O
hour O
on O
CIFAR Material
- Material
10 Material
, O
while O
SVHN Material
took O
about O
8 O
- O
9 O
hours O
. O
Results O
within O
a O
few O
percent O
of O
our O
best O
, O
however O
, O
can O
be O
obtained O
in O
far O
less O
time O
. O
section O
: O
Discussion O
and O
Conclusions O
As O
stated O
in O
the O
introduction O
, O
the O
purpose O
of O
this O
paper O
is O
to O
highlight O
the O
potential O
benefits O
of O
the O
method O
presented O
, O
namely O
that O
it O
can O
attain O
excellent O
results O
with O
a O
rapid O
training Metric
speed Metric
and O
low Metric
implementation Metric
complexity Metric
, O
whilst O
only O
suffering O
from O
reduced O
performance O
relative O
to O
state O
- O
of O
- O
the O
- O
art O
on O
particularly O
hard O
problems O
. O
In O
terms O
of O
efficacy O
on O
classification Task
tasks Task
, O
as O
shown O
in O
Table O
[ O
reference O
] O
, O
our O
best O
result O
( O
0.37 O
% O
error Metric
rate O
) O
surpasses O
the O
best O
ever O
reported O
performance O
for O
classification Task
of O
the O
MNIST Material
test O
set O
when O
no O
augmentation O
of O
the O
training O
set O
is O
done O
. O
We O
have O
also O
achieved O
, O
to O
our O
knowledge O
, O
the O
best O
performance O
reported O
in O
the O
literature O
for O
the O
NORB O
- O
small O
database O
, O
surpassing O
the O
previous O
best O
by O
about O
0.3 O
% O
. O
For O
SVHN Material
, O
our O
best O
result O
is O
within O
% O
of O
state O
- O
of O
- O
the O
- O
art O
. O
It O
is O
highly O
likely O
that O
using O
filters Method
trained O
on O
the O
SVHN Material
database O
rather O
than O
on O
Imagenet O
would O
reduce O
this O
gap O
, O
given O
the O
structured O
nature O
of O
digits O
, O
as O
opposed O
to O
the O
more O
complex O
nature O
of O
Imagenet O
images O
. O
Another O
avenue O
for O
closing O
the O
gap O
on O
state O
- O
of O
- O
the O
- O
art O
using O
the O
same O
filters O
would O
be O
to O
increase O
and O
decrease O
, O
thus O
resulting O
in O
more O
features O
and O
more O
classifier O
hidden O
units O
. O
Although O
we O
increased O
to O
40000 O
, O
we O
did O
not O
observe O
saturation O
in O
the O
error Metric
rate O
as O
we O
increased O
to O
this O
point O
. O
For O
CIFAR Material
- Material
10 Material
, O
it O
is O
less O
clear O
what O
is O
lacking O
in O
our O
method O
in O
comparison O
with O
the O
gap O
of O
about O
14 O
% O
to O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
We O
note O
that O
CIFAR Material
- Material
10 Material
has O
relatively O
few O
training O
points O
, O
and O
we O
observed O
that O
the O
gap O
between O
classification Metric
performance O
on O
the O
actual O
training O
set O
, O
in O
comparison O
with O
the O
test O
set O
, O
can O
be O
up O
to O
20 O
% O
. O
This O
suggests O
that O
designing O
enhanced Method
methods Method
of Method
regularisation Method
( O
e.g. O
methods O
similar O
to O
dropout Method
in O
the O
convolutional Method
stage Method
, O
or O
data Task
augmentation Task
) O
are O
necessary O
to O
ensure O
our O
method O
can O
achieve O
good O
performance O
on O
CIFAR Material
- Material
10 Material
. O
Another O
possibility O
is O
to O
use O
a O
nonlinearity O
in O
the O
classifier Method
stage Method
that O
ensures O
the O
hidden O
- O
layer O
responses O
reflect O
higher O
order O
correlations O
than O
possible O
from O
the O
squaring O
function O
we O
used O
. O
However O
, O
we O
expect O
that O
training O
the O
convolutional Method
filters Method
in O
Stage O
1 O
so O
that O
they O
extract O
features O
that O
are O
more O
discriminative O
for O
the O
specific O
dataset O
will O
be O
the O
most O
likely O
enhancement O
for O
improving O
results O
on O
CIFAR Material
- Material
10 Material
. O
Finally O
, O
we O
note O
that O
there O
exist O
iterative Method
approaches Method
for O
training O
the O
classifier Method
component Method
of O
Stage O
2 O
using O
least Method
squares Method
regression Method
, O
and O
without O
training O
the O
input O
weights O
— O
see O
, O
e.g. O
, O
. O
These O
methods O
can O
be O
easily O
adapted O
for O
use O
with O
the O
convolutional Method
front Method
- Method
end Method
, O
if O
, O
for O
example O
, O
additional O
batches O
of O
training O
data O
become O
available O
, O
or O
if O
the O
problem O
involves O
online Task
learning Task
. O
In O
closing O
, O
following O
acceptance O
of O
this O
paper O
, O
we O
became O
aware O
of O
a O
newly O
published O
paper O
that O
combines O
convolutional Method
feature Method
extraction Method
with O
least Method
squares Method
regression Method
training Method
of O
classifier Method
weights Method
to O
obtain O
good O
results O
for O
the O
NORB O
dataset O
. O
The O
three O
main O
differences O
between O
the O
method O
of O
the O
current O
paper O
and O
the O
method O
of O
are O
as O
follows O
. O
First O
, O
we O
used O
a O
hidden Method
layer Method
in O
our O
classifier Method
stage Method
, O
whereas O
solves O
for O
output O
weights O
using O
least Method
squares Method
regression Method
applied O
to O
the O
output O
of O
the O
pooling Method
stage Method
. O
Second O
, O
we O
used O
a O
variety O
of O
methods O
for O
the O
convolutional Method
filter Method
weights Method
, O
whereas O
uses O
orthogonalised O
random O
weights O
only O
. O
Third O
, O
we O
downsample O
following O
pooling Task
, O
whereas O
does O
not O
do O
so O
. O
section O
: O
Acknowledgment O
Mark O
D. O
McDonnell O
’s O
contribution O
was O
by O
supported O
by O
an O
Australian O
Research O
Fellowship O
from O
the O
Australian O
Research O
Council O
( O
project O
number O
DP1093425 O
) O
. O
We O
gratefully O
acknowledge O
Prof O
David O
Kearney O
and O
Dr O
Victor O
Stamatescu O
from O
University O
of O
South O
Australia O
and O
Dr O
Sebastien O
Wong O
of O
DSTO O
, O
Australia O
, O
for O
useful O
discussions O
and O
provision O
of O
computing O
resources O
. O
We O
also O
acknowledge O
discussions O
with O
Prof O
Philip O
De O
Chazal O
of O
University O
of O
Sydney O
. O
bibliography O
: O
References O
