document O
: O
Structural Method
Neural Method
Encoders Method
for O
AMR Task
- Task
to Task
- Task
text Task
Generation Task
AMR Task
- Task
to Task
- Task
text Task
generation Task
is O
a O
problem O
recently O
introduced O
to O
the O
NLP Task
community Task
, O
in O
which O
the O
goal O
is O
to O
generate O
sentences O
from O
Abstract Method
Meaning Method
Representation Method
( O
AMR Method
) Method
graphs Method
. O
Sequence Method
- Method
to Method
- Method
sequence Method
models Method
can O
be O
used O
to O
this O
end O
by O
converting O
the O
AMR Method
graphs Method
to O
strings O
. O
Approaching O
the O
problem O
while O
working O
directly O
with O
graphs O
requires O
the O
use O
of O
graph Method
- Method
to Method
- Method
sequence Method
models Method
that O
encode O
the O
AMR Method
graph Method
into O
a O
vector Method
representation Method
. O
Such O
encoding O
has O
been O
shown O
to O
be O
beneficial O
in O
the O
past O
, O
and O
unlike O
sequential Method
encoding Method
, O
it O
allows O
us O
to O
explicitly O
capture O
reentrant O
structures O
in O
the O
AMR O
graphs O
. O
We O
investigate O
the O
extent O
to O
which O
reentrancies O
( O
nodes O
with O
multiple O
parents O
) O
have O
an O
impact O
on O
AMR Task
- Task
to Task
- Task
text Task
generation Task
by O
comparing O
graph Method
encoders Method
to O
tree Method
encoders Method
, O
where O
reentrancies O
are O
not O
preserved O
. O
We O
show O
that O
improvements O
in O
the O
treatment O
of O
reentrancies O
and O
long O
- O
range O
dependencies O
contribute O
to O
higher O
overall O
scores O
for O
graph Method
encoders Method
. O
Our O
best O
model O
achieves O
24.40 O
BLEU Metric
on O
LDC2015E86 Material
, O
outperforming O
the O
state O
of O
the O
art O
by O
1.1 O
points O
and O
24.54 O
BLEU Metric
on O
LDC2017T10 Material
, O
outperforming O
the O
state O
of O
the O
art O
by O
1.24 O
points O
. O
section O
: O
Introduction O
( O
a O
) O
eat O
- O
01 O
he O
pizza O
finger O
: O
arg0 O
: O
arg1 O
: O
instrument O
part O
- O
of O
[ O
theme O
= O
simple O
] O
eat O
- O
01 O
& O
: O
arg0 O
& O
he O
& O
: O
arg1 O
& O
pizza O
& O
: O
instrument O
& O
finger O
& O
: O
part O
- O
of O
& O
he O
( O
tmp O
) O
[ O
below O
left O
of O
= O
11 O
, O
xshift O
= O
0.5 O
cm O
, O
yshift=1.2 O
cm O
] O
( O
b O
) O
; O
[ O
theme O
= O
simple O
] O
eat O
- O
01 O
& O
: O
arg0 O
& O
he O
& O
: O
arg1 O
& O
pizza O
& O
: O
instrument O
& O
finger O
& O
: O
part O
- O
of O
& O
he O
12 O
23 O
14 O
45 O
16 O
67 O
[ O
edge O
style O
= O
very O
thick O
] O
78 O
[ O
edge O
style O
= O
very O
thick O
] O
89 O
( O
tmp Method
) O
[ O
below O
left O
of O
= O
11 O
, O
xshift O
= O
0.5 O
cm O
, O
yshift=2 O
cm O
] O
( O
c O
) O
; O
[ O
theme O
= O
simple O
] O
eat O
- O
01 O
& O
: O
arg0 O
& O
he O
& O
: O
arg1 Method
& O
pizza O
& O
: O
instrument O
& O
finger O
& O
: O
part O
- O
of O
& O
he O
12 O
23 O
14 O
45 O
16 O
67 O
[ O
edge O
style O
= O
very O
thick O
] O
78 O
[ O
edge O
style O
= O
very O
thick O
] O
83 O
( O
tmp O
) O
[ O
below O
left O
of O
= O
11 O
, O
xshift O
= O
0.5 O
cm O
, O
yshift=2 O
cm O
] O
( O
d O
) O
; O
Abstract Method
Meaning Method
Representation Method
( O
AMR Method
; O
Banarescu13abstractmeaning O
) O
is O
a O
semantic Method
graph Method
representation Method
that O
abstracts O
away O
from O
the O
syntactic O
realization O
of O
a O
sentence O
, O
where O
nodes O
in O
the O
graph O
represent O
concepts O
and O
edges O
represent O
semantic O
relations O
between O
them O
. O
AMRs Method
are O
graphs O
, O
rather O
than O
trees O
, O
because O
co O
- O
references O
and O
control O
structures O
result O
in O
nodes O
with O
multiple O
parents O
, O
called O
reentrancies O
. O
For O
instance O
, O
the O
AMR O
of O
Figure O
[ O
reference O
] O
( O
a O
) O
contains O
a O
reentrancy O
between O
finger O
and O
he O
, O
caused O
by O
the O
possessive O
pronoun O
his O
. O
AMR Task
- Task
to Task
- Task
text Task
generation Task
is O
the O
task O
of O
automatically Task
generating Task
natural Task
language Task
from O
AMR Task
graphs Task
. O
Attentive Method
encoder Method
/ Method
decoder Method
architectures Method
, O
commonly O
used O
for O
Neural Task
Machine Task
Translation Task
( O
NMT Task
) O
, O
have O
been O
explored O
for O
this O
task O
. O
In O
order O
to O
use O
sequence Method
- Method
to Method
- Method
sequence Method
models Method
, O
konstas2017neural O
reduce O
the O
AMR O
graphs O
to O
sequences O
, O
while O
song O
and O
beck O
directly O
encode O
them O
as O
graphs O
. O
Graph Method
encoding Method
allows O
the O
model O
to O
explicitly O
encode O
reentrant O
structures O
present O
in O
the O
AMR O
graphs O
. O
While O
central O
to O
AMR Method
, O
reentrancies Method
are O
often O
hard O
to O
treat O
both O
in O
parsing Task
and O
in O
generation Task
. O
Previous O
work O
either O
removed O
them O
from O
the O
graphs O
, O
hence O
obtaining O
sequential Method
or O
tree O
- O
structured O
data O
, O
while O
other O
work O
maintained O
them O
but O
did O
not O
analyze O
their O
impact O
on O
performance O
] O
song O
, O
beck O
. O
damonte2016incremental O
showed O
that O
state O
- O
of O
- O
the O
- O
art O
parsers Method
do O
not O
perform O
well O
in O
predicting Task
reentrant Task
structures Task
, O
while O
van2017dealing O
compared O
different O
pre Method
- Method
and Method
post Method
- Method
processing Method
techniques Method
to O
improve O
the O
performance O
of O
sequence Method
- Method
to Method
- Method
sequence Method
parsers Method
with O
respect O
to O
reentrancies O
. O
It O
is O
not O
yet O
clear O
whether O
explicit O
encoding O
of O
reentrancies Method
is O
beneficial O
for O
generation Task
. O
In O
this O
paper O
, O
we O
compare O
three O
types O
of O
encoders Method
for O
AMR Method
: O
1 O
) O
sequential Method
encoders O
, O
which O
reduce O
AMR O
graphs O
to O
sequences O
; O
2 O
) O
tree Method
encoders Method
, O
which O
ignore O
reentrancies Method
; O
and O
3 O
) O
graph Method
encoders Method
. O
We O
pay O
particular O
attention O
to O
two O
phenomena O
: O
reentrancies O
, O
which O
mark O
co O
- O
reference O
and O
control O
structures O
, O
and O
long O
- O
range O
dependencies O
in O
the O
AMR O
graphs O
, O
which O
are O
expected O
to O
benefit O
from O
structural Method
encoding Method
. O
The O
contributions O
of O
the O
paper O
are O
two O
- O
fold O
: O
We O
present O
structural Method
encoders Method
for O
the O
encoder Method
/ Method
decoder Method
framework Method
and O
show O
the O
benefits O
of O
graph Method
encoders Method
not O
only O
compared O
to O
sequential Method
encoders O
but O
also O
compared O
to O
tree Method
encoders Method
, O
which O
have O
not O
been O
studied O
so O
far O
for O
AMR Task
- Task
to Task
- Task
text Task
generation Task
. O
We O
show O
that O
better O
treatment O
of O
reentrancies O
and O
long O
- O
range O
dependencies O
contributes O
to O
improvements O
in O
the O
graph Method
encoders Method
. O
Our O
best O
model O
, O
based O
on O
a O
graph Method
encoder Method
, O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
for O
both O
the O
LDC2015E86 Material
dataset Material
( O
24.40 O
on O
BLEU Metric
and O
23.79 O
on O
Meteor Metric
) O
and O
the O
LDC2017T10 Material
dataset Material
( O
24.54 O
on O
BLEU Metric
and O
24.07 O
on O
Meteor Metric
) O
. O
section O
: O
Input Method
Representations Method
paragraph O
: O
Graph Method
- Method
structured Method
AMRs Method
AMRs Method
are O
normally O
represented O
as O
rooted Method
and Method
directed Method
graphs Method
: O
where O
are O
the O
graph O
vertices O
( O
or O
nodes O
) O
and O
is O
a O
designated O
root O
node O
in O
. O
The O
edges O
in O
the O
AMR Method
are O
labeled O
: O
Each O
edge O
is O
a O
triple O
: O
, O
where O
is O
the O
parent O
node O
, O
is O
the O
edge O
label O
and O
is O
the O
child O
node O
. O
In O
order O
to O
obtain O
unlabeled O
edges O
, O
thus O
decreasing O
the O
total O
number O
of O
parameters O
required O
by O
the O
models O
, O
we O
replace O
each O
labeled O
edge O
with O
two O
unlabeled O
edges O
: O
: O
Each O
unlabeled O
edge O
is O
a O
pair O
: O
, O
where O
one O
of O
the O
following O
holds O
: O
and O
; O
and O
. O
For O
instance O
, O
the O
edge O
between O
eat O
- O
01 O
and O
he O
with O
label O
: O
arg0 O
of O
Figure O
[ O
reference O
] O
( O
a O
) O
is O
replaced O
by O
two O
edges O
in O
Figure O
[ O
reference O
] O
( O
d O
) O
: O
an O
edge O
between O
eat O
- O
01 O
and O
: O
arg0 O
and O
another O
one O
between O
: O
arg0 O
and O
he O
. O
The O
process O
, O
also O
used O
in O
beck O
, O
tranforms O
the O
input O
graph O
into O
its O
equivalent O
Levi Method
graph Method
. O
paragraph O
: O
Tree Method
- Method
structured Method
AMRs Method
In O
order O
to O
obtain O
tree O
structures O
, O
it O
is O
necessary O
to O
discard O
the O
reentrancies O
from O
the O
AMR Method
graphs Method
. O
Similarly O
to O
takase2016neural O
, O
we O
replace O
nodes O
with O
incoming O
edges O
with O
identically O
labeled O
nodes O
, O
each O
with O
a O
single O
incoming O
edge O
. O
paragraph O
: O
Sequential Method
AMRs Method
Following O
konstas2017neural O
, O
the O
input O
sequence O
is O
a O
linearized Method
and Method
anonymized Method
AMR Method
graph Method
. O
Linearization Method
is O
used O
to O
convert O
the O
graph O
into O
a O
sequence O
: O
The O
depth Method
- Method
first Method
traversal Method
of O
the O
graph O
defines O
the O
indexing O
between O
nodes O
and O
tokens O
in O
the O
sequence O
. O
For O
instance O
, O
the O
root O
node O
is O
, O
its O
leftmost O
child O
is O
and O
so O
on O
. O
Nodes O
with O
multiple O
parents O
are O
visited O
more O
than O
once O
. O
At O
each O
visit O
, O
their O
labels O
are O
repeated O
in O
the O
sequence O
, O
effectively O
losing O
reentrancy O
information O
, O
as O
shown O
in O
Figure O
[ O
reference O
] O
( O
b O
) O
. O
Anonymization Method
removes O
names O
and O
rare O
words O
with O
coarse O
categories O
to O
reduce O
data O
sparsity O
. O
An O
alternative O
to O
anonymization Task
is O
to O
employ O
a O
copy Method
mechanism Method
, O
where O
the O
models O
learn O
to O
copy O
rare O
words O
from O
the O
input O
itself O
. O
In O
this O
paper O
, O
we O
follow O
the O
anonymization Method
approach Method
. O
section O
: O
Encoders O
In O
this O
section O
, O
we O
review O
the O
encoders Method
adopted O
as O
building O
blocks O
for O
our O
tree Method
and Method
graph Method
encoders Method
. O
subsection O
: O
Recurrent Method
Neural Method
Network Method
Encoders Method
We O
reimplement O
the O
encoder Method
of O
konstas2017neural O
, O
where O
the O
sequential Method
linearization O
is O
the O
input O
to O
a O
bidirectional Method
LSTM Method
( O
BiLSTM Method
; O
graves2013speech O
) O
network O
. O
The O
hidden O
state O
of O
the O
BiLSTM Method
at O
step O
is O
used O
as O
a O
context Method
- Method
aware Method
word Method
representation Method
of O
the O
- O
th O
token O
in O
the O
sequence O
: O
where O
, O
is O
the O
size O
of O
the O
output O
embeddings O
. O
subsection O
: O
TreeLSTM Method
Encoders O
Tree Method
- Method
Structured Method
Long Method
Short Method
- Method
Term Method
Memory Method
Networks Method
( O
TreeLSTM Method
; O
tai2015improved O
) O
have O
been O
introduced O
primarily O
as O
a O
way O
to O
encode O
the O
hierarchical O
structure O
of O
syntactic O
trees O
, O
but O
they O
have O
also O
been O
applied O
to O
AMR Method
for O
the O
task O
of O
headline Task
generation Task
. O
TreeLSTMs Method
assume O
tree O
- O
structured O
input O
, O
so O
AMR Task
graphs Task
must O
be O
preprocessed O
to O
respect O
this O
constraint O
: O
reentrancies O
, O
which O
play O
an O
essential O
role O
in O
AMR Task
, O
must O
be O
removed O
, O
thereby O
transforming O
the O
graphs O
into O
trees O
. O
We O
use O
the O
Child Method
- Method
Sum Method
variant Method
introduced O
by O
tai2015improved Method
, O
which O
processes O
the O
tree O
in O
a O
bottom O
- O
up O
pass O
. O
When O
visiting O
a O
node O
, O
the O
hidden O
states O
of O
its O
children O
are O
summed O
up O
in O
a O
single O
vector O
which O
is O
then O
passed O
into O
recurrent Method
gates Method
. O
In O
order O
to O
use O
information O
from O
both O
incoming O
and O
outgoing O
edges O
( O
parents O
and O
children O
) O
, O
we O
employ O
bidirectional Method
TreeLSTMs Method
, O
where O
the O
bottom O
- O
up O
pass O
is O
followed O
by O
a O
top O
- O
down O
pass O
. O
The O
top O
- O
down O
state O
of O
the O
root O
node O
is O
obtained O
by O
feeding O
the O
bottom O
- O
up O
state O
of O
the O
root O
node O
through O
a O
feed Method
- Method
forward Method
layer Method
: O
where O
is O
the O
hidden O
state O
of O
node O
for O
the O
bottom O
- O
up O
pass O
and O
is O
the O
hidden O
state O
of O
node O
for O
the O
top O
- O
down O
pass O
. O
The O
bottom O
up O
states O
for O
all O
other O
nodes O
are O
computed O
with O
an O
LSTM Method
, O
with O
the O
cell O
state O
given O
by O
their O
parent O
nodes O
: O
where O
is O
the O
parent O
of O
node O
in O
the O
tree O
. O
The O
final O
hidden O
states O
are O
obtained O
by O
concatenating O
the O
states O
from O
the O
bottom O
- O
up O
pass O
and O
the O
top O
- O
down O
pass O
: O
The O
hidden O
state O
of O
the O
root O
node O
is O
usually O
used O
as O
a O
representation O
for O
the O
entire O
tree O
. O
In O
order O
to O
use O
attention O
over O
all O
nodes O
, O
as O
in O
traditional O
NMT Task
, O
we O
can O
however O
build O
node O
embeddings O
by O
extracting O
the O
hidden O
states O
of O
each O
node O
in O
the O
tree O
: O
where O
, O
is O
the O
size O
of O
the O
output O
embeddings O
. O
The O
encoder O
is O
related O
to O
the O
TreeLSTM Method
encoder O
of O
takase2016neural O
, O
which O
however O
encodes O
labeled O
trees O
and O
does O
not O
use O
a O
top O
- O
down O
pass O
. O
subsection O
: O
Graph Method
Convolutional Method
Network Method
Encoders Method
Graph Method
Convolutional Method
Network Method
( O
GCN Method
; O
duvenaud2015convolutional O
, O
kipf2016semi O
) O
is O
a O
neural Method
network Method
architecture Method
that O
learns O
embeddings O
of O
nodes O
in O
a O
graph O
by O
looking O
at O
its O
nearby O
nodes O
. O
In O
Natural Task
Language Task
Processing Task
, O
GCNs Method
have O
been O
used O
for O
Semantic Task
Role Task
Labeling Task
, O
NMT Task
, O
Named Task
Entity Task
Recognition Task
and O
text Task
generation Task
. O
A O
graph Method
- Method
to Method
- Method
sequence Method
neural Method
network Method
was O
first O
introduced O
by O
xu2018graph2seq O
. O
The O
authors O
review O
the O
similarities O
between O
their O
approach O
, O
GCN Method
and O
another O
approach O
, O
based O
on O
GRUs Method
. O
The O
latter O
recently O
inspired O
a O
graph Method
- Method
to Method
- Method
sequence Method
architecture Method
for O
AMR Task
- Task
to Task
- Task
text Task
generation Task
. O
Simultaneously O
, O
song O
proposed O
a O
graph Method
encoder Method
based O
on O
LSTMs Method
. O
The O
architectures O
of O
song Method
and O
beck O
are O
both O
based O
on O
the O
same O
core Method
computation Method
of O
a O
GCN Method
, O
which O
sums O
over O
the O
embeddings O
of O
the O
immediate O
neighborhood O
of O
each O
node O
: O
where O
is O
the O
embeddings O
of O
node O
at O
layer O
, O
is O
a O
non O
- O
linear O
activation O
function O
, O
is O
the O
set O
of O
the O
immediate O
neighbors O
of O
, O
and O
, O
with O
being O
the O
size O
of O
the O
embeddings O
. O
It O
is O
possible O
to O
use O
recurrent Method
networks Method
to O
model O
the O
update O
of O
the O
node O
embeddings O
. O
Specifically O
, O
beck O
uses O
a O
GRU Method
layer Method
where O
the O
gates O
are O
modeled O
as O
GCN Method
layers O
. O
song O
did O
not O
use O
the O
activation O
function O
and O
perform O
an O
LSTM Method
update Method
instead O
. O
The O
systems O
of O
song O
and O
beck O
further O
differ O
in O
design O
and O
implementation O
decisions O
such O
as O
in O
the O
use O
of O
edge O
label O
and O
edge O
directionality O
. O
Throughout O
the O
rest O
of O
the O
paper O
, O
we O
follow O
the O
traditional O
, O
non Method
- Method
recurrent Method
, Method
implementation Method
of O
GCN Method
also O
adopted O
in O
other O
NLP Task
tasks Task
. O
In O
our O
experiments O
, O
the O
node O
embeddings O
are O
computed O
as O
follows O
: O
where O
indicates O
the O
direction O
of O
the O
edge O
between O
and O
( O
i.e. O
, O
outgoing O
or O
incoming O
edge O
) O
. O
The O
hidden O
vectors O
from O
the O
last O
layer O
of O
the O
GCN Method
network O
are O
finally O
used O
to O
represent O
each O
node O
in O
the O
graph O
: O
where O
K O
is O
the O
number O
of O
GCN Method
layers O
used O
, O
, O
is O
the O
size O
of O
the O
output O
embeddings O
. O
To O
regularize O
the O
models O
we O
apply O
dropout Method
as O
well O
as O
edge Method
dropout Method
. O
We O
also O
include O
highway O
connections O
between O
GCN Method
layers O
. O
While O
GCN Method
can O
naturally O
be O
used O
to O
encode O
graphs O
, O
they O
can O
also O
be O
applied O
to O
trees O
by O
removing O
reentrancies O
from O
the O
input O
graphs O
. O
In O
the O
experiments O
of O
Section O
[ O
reference O
] O
, O
we O
explore O
GCN Method
- O
based O
models O
both O
as O
graph Method
encoders Method
( O
reentrancies O
are O
maintained O
) O
as O
well O
as O
tree Method
encoders Method
( O
reentrancies O
are O
ignored O
) O
. O
section O
: O
Stacking Method
Encoders Method
GCN Method
/ O
TreeLSTM Method
BiLSTM Method
BiLSTM Method
GCN Method
/ O
TreeLSTM Method
We O
aimed O
at O
stacking O
the O
explicit O
source O
of O
structural O
information O
provided O
by O
TreeLSTMs Method
and O
GCNs Method
with O
the O
sequential Method
information O
which O
BiLSTMs Method
extract O
well O
. O
This O
was O
shown O
to O
be O
effective O
for O
other O
tasks O
with O
both O
TreeLSTMs Method
and O
GCNs Method
. O
In O
previous O
work O
, O
the O
structural Method
encoders Method
( O
tree Method
or Method
graph Method
) O
were O
used O
on O
top O
of O
the O
BiLSTM Method
network O
: O
first O
, O
the O
input O
is O
passed O
through O
the O
sequential Method
encoder O
, O
the O
output O
of O
which O
is O
then O
fed O
into O
the O
structural Method
encoder Method
. O
While O
we O
experiment O
with O
this O
approach O
, O
we O
also O
propose O
an O
alternative O
solution O
where O
the O
BiLSTM Method
network O
is O
used O
on O
top O
of O
the O
structural Method
encoder Method
: O
the O
input O
embeddings O
are O
refined O
by O
exploiting O
the O
explicit O
structural O
information O
given O
by O
the O
graph O
. O
The O
refined O
embeddings O
are O
then O
fed O
into O
the O
BiLSTM Method
networks O
. O
See O
Figure O
[ O
reference O
] O
for O
a O
graphical O
representation O
of O
the O
two O
approaches O
. O
In O
our O
experiments O
, O
we O
found O
this O
approach O
to O
be O
more O
effective O
. O
Compared O
to O
models O
that O
interleave O
structural Method
and Method
recurrent Method
components Method
such O
as O
the O
systems O
of O
song Method
and O
beck O
, O
stacking O
the O
components O
allows O
us O
to O
test O
for O
their O
contributions O
more O
easily O
. O
subsection O
: O
Structure O
on O
Top O
of O
Sequence O
In O
this O
setup O
, O
BiLSTMs Method
are O
used O
as O
in O
Section O
[ O
reference O
] O
to O
encode O
the O
linearized O
and O
anonymized O
AMR O
. O
The O
context O
provided O
by O
the O
BiLSTM Method
is O
a O
sequential Method
one O
. O
We O
then O
apply O
either O
GCN Method
or O
TreeLSTM Method
on O
the O
output O
of O
the O
BiLSTM Method
, O
by O
initializing O
the O
GCN Method
or O
TreeLSTM Method
embeddings O
with O
the O
BiLSTM Method
hidden O
states O
. O
We O
call O
these O
models O
SeqGCN Method
and O
SeqTreeLSTM Method
. O
subsection O
: O
Sequence O
on O
Top O
of O
Structure O
We O
also O
propose O
a O
different O
approach O
for O
integrating O
graph O
information O
into O
the O
encoder O
, O
by O
swapping O
the O
order O
of O
the O
BiLSTM Method
and O
the O
structural Method
encoder Method
: O
we O
aim O
at O
using O
the O
structured O
information O
provided O
by O
the O
AMR Method
graph Method
as O
a O
way O
to O
refine O
the O
original O
word Method
representations Method
. O
We O
first O
apply O
the O
structural Method
encoder Method
to O
the O
input O
graphs O
. O
The O
GCN Method
or O
TreeLSTM Method
representations O
are O
then O
fed O
into O
the O
BiLSTM Method
. O
We O
call O
these O
models O
GCNSeq Method
and O
TreeLSTMSeq Method
. O
The O
motivation O
behind O
this O
approach O
is O
that O
we O
know O
that O
BiLSTMs Method
, O
given O
appropriate O
input O
embeddings O
, O
are O
very O
effective O
at O
encoding O
the O
input O
sequences O
. O
In O
order O
to O
exploit O
their O
strength O
, O
we O
do O
not O
amend O
their O
output O
but O
rather O
provide O
them O
with O
better O
input O
embeddings O
to O
start O
with O
, O
by O
explicitly O
taking O
the O
graph O
relations O
into O
account O
. O
section O
: O
Experiments O
We O
use O
both O
BLEU Metric
and O
Meteor Metric
as O
evaluation Metric
metrics Metric
. O
We O
report O
results O
on O
the O
AMR O
dataset O
LDC2015E86 Material
and O
LDC2017T10 Material
. O
All O
systems O
are O
implemented O
in O
PyTorch Method
using O
the O
framework O
OpenNMT Method
- Method
py Method
. O
Hyperparameters Method
of O
each O
model O
were O
tuned O
on O
the O
development O
set O
of O
LDC2015E86 Material
. O
For O
the O
GCN Method
components O
, O
we O
use O
two O
layers O
, O
activations O
, O
and O
highway Method
layers Method
. O
We O
use O
single Method
layer Method
LSTMs Method
. O
We O
train O
with O
SGD Method
with O
the O
initial O
learning Metric
rate Metric
set O
to O
1 O
and O
decay O
to O
0.8 O
. O
Batch O
size O
is O
set O
to O
100 O
. O
We O
first O
evaluate O
the O
overall O
performance O
of O
the O
models O
, O
after O
which O
we O
focus O
on O
two O
phenomena O
that O
we O
expect O
to O
benefit O
most O
from O
structural Method
encoders Method
: O
reentrancies O
and O
long O
- O
range O
dependencies O
. O
Table O
[ O
reference O
] O
shows O
the O
comparison O
on O
the O
development O
split O
of O
the O
LDC2015E86 Material
dataset Material
between O
sequential Method
, O
tree O
and O
graph O
encoders O
. O
The O
sequential Method
encoder O
( O
Seq Method
) O
is O
a O
re Method
- Method
implementation Method
of O
konstas2017neural O
. O
We O
test O
both O
approaches O
of O
stacking O
structural O
and O
sequential Method
components O
: O
structure O
on O
top O
of O
sequence O
( O
SeqTreeLSTM Method
and O
SeqGCN Method
) O
, O
and O
sequence O
on O
top O
of O
structure O
( O
TreeLSTMSeq Method
and O
GCNSeq Method
) O
. O
To O
inspect O
the O
effect O
of O
the O
sequential Method
component O
, O
we O
run O
ablation Method
tests Method
by O
removing O
the O
RNNs Method
altogether O
( O
TreeLSTM Method
and O
GCN Method
) O
. O
GCN Method
- O
based O
models O
are O
used O
both O
as O
tree Method
encoders Method
( O
reentrancies O
are O
removed O
) O
and O
graph Method
encoders Method
( O
reentrancies O
are O
maintained O
) O
. O
For O
both O
TreeLSTM Method
- O
based O
and O
GCN Method
- O
based O
models O
, O
our O
proposed O
approach O
of O
applying O
the O
structural Method
encoder Method
before O
the O
RNN Method
achieves O
better O
scores O
. O
This O
is O
especially O
true O
for O
GCN Method
- O
based O
models O
, O
for O
which O
we O
also O
note O
a O
drastic O
drop O
in O
performance O
when O
the O
RNN Method
is O
removed O
, O
highlighting O
the O
importance O
of O
a O
sequential Method
component O
. O
On O
the O
other O
hand O
, O
RNN Method
layers Method
seem O
to O
have O
less O
impact O
on O
TreeLSTM Method
- O
based O
models O
. O
This O
outcome O
is O
not O
unexpected O
, O
as O
TreeLSTMs Method
already O
use O
LSTM O
gates O
in O
their O
computation O
. O
The O
results O
show O
a O
clear O
advantage O
of O
tree Method
and Method
graph Method
encoders Method
over O
the O
sequential Method
encoder O
. O
The O
best O
performing O
model O
is O
GCNSeq Method
, O
both O
as O
a O
tree Method
and O
as O
a O
graph Method
encoder Method
, O
with O
the O
latter O
obtaining O
the O
highest O
results O
. O
Table O
[ O
reference O
] O
shows O
the O
comparison O
between O
our O
best O
sequential Method
( O
Seq Method
) O
, O
tree O
( O
GCNSeq Method
without O
reentrancies O
, O
henceforth O
called O
Tree Method
) O
and O
graph Method
encoders Method
( O
GCNSeq Method
with O
reentrancies Method
, O
henceforth O
called O
Graph Method
) O
on O
the O
test O
set O
of O
LDC2015E86 Material
and O
LDC2017T10 Material
. O
We O
also O
include O
state O
- O
of O
- O
the O
- O
art O
results O
reported O
on O
these O
datasets O
for O
sequential Method
encoding Method
konstas2017neural O
and O
graph O
encoding O
song O
, O
beck O
. O
In O
order O
to O
mitigate O
the O
effects O
of O
random O
seeds O
, O
we O
train O
five O
models O
with O
different O
random O
seeds O
and O
report O
the O
results O
of O
the O
median O
model O
, O
according O
to O
their O
BLEU Metric
score O
on O
the O
development O
set O
. O
We O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
with O
both O
tree Method
and Method
graph Method
encoders Method
, O
demonstrating O
the O
efficacy O
of O
our O
GCNSeq Method
approach O
. O
The O
graph Method
encoder Method
outperforms O
the O
other O
systems O
and O
previous O
work O
on O
both O
datasets O
. O
These O
results O
demonstrate O
the O
benefit O
of O
structural Method
encoders Method
over O
purely O
sequential Method
ones O
as O
well O
as O
the O
advantage O
of O
explicitly O
including O
reentrancies O
. O
The O
differences O
between O
our O
graph Method
encoder Method
and O
that O
of O
song O
and O
beck O
were O
discussed O
in O
Section O
[ O
reference O
] O
. O
subsection O
: O
Reentrancies O
Overall O
scores O
show O
an O
advantage O
of O
graph Method
encoder Method
over O
tree O
and O
sequential Method
encoders O
, O
but O
they O
do O
not O
shed O
light O
into O
how O
this O
is O
achieved O
. O
Because O
graph Method
encoders Method
are O
the O
only O
ones O
to O
model O
reentrancies O
explicitly O
, O
we O
expect O
them O
to O
deal O
better O
with O
these O
structures O
. O
It O
is O
, O
however O
, O
possible O
that O
the O
other O
models O
are O
capable O
of O
handling O
these O
structures O
implicitly O
. O
Moreover O
, O
the O
dataset O
contains O
a O
large O
number O
of O
examples O
that O
do O
not O
involve O
any O
reentrancies O
, O
as O
shown O
in O
Table O
[ O
reference O
] O
, O
so O
that O
the O
overall O
scores O
may O
not O
be O
representative O
of O
the O
ability O
of O
models O
to O
capture O
reentrancies O
. O
It O
is O
expected O
that O
the O
benefit O
of O
the O
graph Method
models Method
will O
be O
more O
evident O
for O
those O
examples O
containing O
more O
reentrancies O
. O
To O
test O
this O
hypothesis O
, O
we O
evaluate O
the O
various O
scenarios O
as O
a O
function O
of O
the O
number O
of O
reentrancies O
in O
each O
example O
, O
using O
the O
Meteor Metric
score O
as O
a O
metric O
. O
Table O
[ O
reference O
] O
shows O
that O
the O
gap O
between O
the O
graph Method
encoder Method
and O
the O
other O
encoders O
is O
widest O
for O
examples O
with O
more O
than O
six O
reentrancies O
. O
The O
Meteor Metric
score O
of O
the O
graph Method
encoder Method
for O
these O
cases O
is O
3.1 O
% O
higher O
than O
the O
one O
for O
the O
sequential Method
encoder O
and O
2.3 O
% O
higher O
than O
the O
score O
achieved O
by O
the O
tree Method
encoder Method
, O
demonstrating O
that O
explicitly O
encoding O
reentrancies O
is O
more O
beneficial O
than O
the O
overall O
scores O
suggest O
. O
Interestingly O
, O
it O
can O
also O
be O
observed O
that O
the O
graph Method
model Method
outperforms O
the O
tree Method
model Method
also O
for O
examples O
with O
no O
reentrancies O
, O
where O
tree O
and O
graph O
structures O
are O
identical O
. O
This O
suggests O
that O
preserving O
reentrancies O
in O
the O
training O
data O
has O
other O
beneficial O
effects O
. O
In O
Section O
[ O
reference O
] O
we O
explore O
one O
: O
better O
handling O
of O
long O
- O
range O
dependencies O
. O
subsubsection O
: O
Manual Task
Inspection Task
In O
order O
to O
further O
explore O
how O
the O
graph Method
model Method
handles O
reentrancies O
differently O
from O
the O
other O
models O
, O
we O
performed O
a O
manual O
inspection O
of O
the O
models O
’ O
output O
. O
We O
selected O
examples O
containing O
reentrancies O
, O
where O
the O
graph Method
model Method
performs O
better O
than O
the O
other O
models O
. O
These O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
In O
Example O
( O
1 O
) O
, O
we O
note O
that O
the O
graph Method
model Method
is O
the O
only O
one O
that O
correctly O
predicts O
the O
phrase O
he O
finds O
out O
. O
The O
wrong O
verb O
tense O
is O
due O
to O
the O
lack O
of O
tense O
information O
in O
AMR O
graphs O
. O
In O
the O
sequential Method
model O
, O
the O
pronoun O
is O
chosen O
correctly O
, O
but O
the O
wrong O
verb O
is O
predicted O
, O
while O
in O
the O
tree Method
model Method
the O
pronoun O
is O
missing O
. O
In O
Example O
( O
2 O
) O
, O
only O
the O
graph Method
model Method
correctly O
generates O
the O
phrase O
you O
tell O
them O
, O
while O
none O
of O
the O
models O
use O
people O
as O
the O
subject O
of O
the O
predicate O
can O
. O
In O
Example O
( O
3 O
) O
, O
both O
the O
graph Method
and O
the O
sequential Method
models O
deal O
well O
with O
the O
control O
structure O
caused O
by O
the O
recommend O
predicate O
. O
The O
sequential Method
model O
, O
however O
, O
overgenerates O
a O
wh O
- O
clause O
. O
Finally O
, O
in O
Example O
( O
4 O
) O
the O
tree Method
and Method
graph Method
models Method
deal O
correctly O
with O
the O
possessive O
pronoun O
to O
generate O
the O
phrase O
tell O
your O
ex O
, O
while O
the O
sequential Method
model O
does O
not O
. O
Overall O
, O
we O
note O
that O
the O
graph Method
model Method
produces O
a O
more O
accurate O
output O
than O
sequential Method
and O
tree O
models O
by O
generating O
the O
correct O
pronouns O
and O
mentions O
when O
control O
verbs O
and O
co O
- O
references O
are O
involved O
. O
subsubsection O
: O
Contrastive O
Pairs O
For O
a O
quantitative O
analysis O
of O
how O
the O
different O
models O
handle O
pronouns O
, O
we O
use O
a O
method O
to O
inspect O
NMT Task
output O
for O
specific O
linguistic Task
analysis Task
based O
on O
contrastive O
pairs O
. O
Given O
a O
reference O
output O
sentence O
, O
a O
contrastive O
sentence O
is O
generated O
by O
introducing O
a O
mistake O
related O
to O
the O
phenomenon O
we O
are O
interested O
in O
evaluating O
. O
The O
probability O
that O
the O
model O
assigns O
to O
the O
reference O
sentence O
is O
then O
compared O
to O
that O
of O
the O
contrastive O
sentence O
. O
The O
accuracy Metric
of O
a O
model O
is O
determined O
by O
the O
percentage O
of O
examples O
in O
which O
the O
reference O
sentence O
has O
a O
higher O
probability O
than O
the O
contrastive O
sentence O
. O
We O
produce O
contrastive O
examples O
by O
running O
CoreNLP Method
to O
identify O
co O
- O
references O
, O
which O
are O
the O
primary O
cause O
of O
reentrancies O
, O
and O
introducing O
a O
mistake O
. O
When O
an O
expression O
has O
multiple O
mentions O
, O
the O
antecedent O
is O
repeated O
in O
the O
linearized Method
AMR Method
. O
For O
instance O
, O
the O
linearization O
of O
Figure O
[ O
reference O
] O
( O
b O
) O
contains O
the O
token O
he O
twice O
, O
which O
instead O
appears O
only O
once O
in O
the O
sentence O
. O
This O
repetition O
may O
result O
in O
generating O
the O
token O
he O
twice O
, O
rather O
than O
using O
a O
pronoun O
to O
refer O
back O
to O
it O
. O
To O
investigate O
this O
possible O
mistake O
, O
we O
replace O
one O
of O
the O
mentions O
with O
the O
antecedent O
( O
e.g. O
, O
John O
ate O
the O
pizza O
with O
his O
fingers O
is O
replaced O
with O
John O
ate O
the O
pizza O
with O
John O
fingers O
, O
which O
is O
ungrammatical O
and O
as O
such O
should O
be O
less O
likely O
) O
. O
An O
alternative O
hypothesis O
is O
that O
even O
when O
the O
generation Method
system Method
correctly O
decides O
to O
predict O
a O
pronoun O
, O
it O
selects O
the O
wrong O
one O
. O
To O
test O
for O
this O
, O
we O
produce O
contrastive O
examples O
where O
a O
pronoun O
is O
replaced O
by O
either O
a O
different O
type O
of O
pronoun O
( O
e.g. O
, O
John O
ate O
the O
pizza O
with O
his O
fingers O
is O
replaced O
with O
John O
ate O
the O
pizza O
with O
him O
fingers O
) O
or O
by O
the O
same O
type O
of O
pronoun O
but O
for O
a O
different O
number O
( O
John O
ate O
the O
pizza O
with O
their O
fingers O
) O
or O
different O
gender O
( O
John O
ate O
the O
pizza O
with O
her O
fingers O
) O
. O
Note O
from O
Figure O
[ O
reference O
] O
that O
the O
graph Method
- Method
structured Method
AMR Method
is O
the O
one O
that O
more O
directly O
captures O
the O
relation O
between O
finger O
and O
he O
, O
and O
as O
such O
it O
is O
expected O
to O
deal O
better O
with O
this O
type O
of O
mistakes O
. O
From O
the O
test O
split O
of O
LDC2017T10 Material
, O
we O
generated O
251 O
contrastive O
examples O
due O
to O
antecedent O
replacements O
, O
912 O
due O
to O
pronoun O
type O
replacements O
, O
1840 O
due O
to O
number O
replacements O
and O
95 O
due O
to O
gender O
replacements O
. O
The O
results O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
The O
sequential Method
encoder O
performs O
surprisingly O
well O
at O
this O
task O
, O
with O
better O
or O
on O
par O
performance O
with O
respect O
to O
the O
tree Method
encoder Method
. O
The O
graph Method
encoder Method
outperforms O
the O
sequential Method
encoder O
only O
for O
pronoun Task
number Task
and Task
gender Task
replacements Task
. O
Future O
work O
is O
required O
to O
more O
precisely O
analyze O
if O
the O
different O
models O
cope O
with O
pronomial O
mentions O
in O
significantly O
different O
ways O
. O
Other O
approaches O
to O
inspect O
phenomena O
of O
co O
- O
reference O
and O
control O
verbs O
can O
also O
be O
explored O
, O
for O
instance O
by O
devising O
specific O
training O
objectives O
. O
subsection O
: O
Long O
- O
range O
Dependencies O
When O
we O
encode O
a O
long O
sequence O
, O
interactions O
between O
items O
that O
appear O
distant O
from O
each O
other O
in O
the O
sequence O
are O
difficult O
to O
capture O
. O
The O
problem O
of O
long Task
- Task
range Task
dependencies Task
in O
natural O
language O
is O
well O
known O
for O
RNN Method
architectures Method
. O
Indeed O
, O
the O
need O
to O
solve O
this O
problem O
motivated O
the O
introduction O
of O
LSTM Method
models Method
, O
which O
are O
known O
to O
model O
long O
- O
range O
dependencies O
better O
than O
traditional O
RNNs Method
. O
Because O
the O
nodes O
in O
the O
graphs O
are O
not O
aligned O
with O
words O
in O
the O
sentence O
, O
AMR Method
has O
no O
notion O
of O
distance O
between O
the O
nodes O
taking O
part O
in O
an O
edge O
. O
In O
order O
to O
define O
the O
length O
of O
an O
AMR O
edge O
, O
we O
resort O
to O
the O
AMR Method
linearization Method
discussed O
in O
Section O
[ O
reference O
] O
. O
Given O
the O
linearization O
of O
the O
AMR Method
, O
as O
discussed O
in O
Section O
[ O
reference O
] O
, O
and O
an O
edge O
between O
two O
nodes O
and O
, O
the O
length O
of O
the O
edge O
is O
defined O
as O
. O
We O
then O
compute O
the O
maximum O
dependency O
length O
for O
each O
AMR Method
graph Method
. O
In O
order O
to O
verify O
the O
hypothesis O
that O
long O
- O
range O
dependencies O
contribute O
to O
the O
improvements O
of O
graph Method
models Method
, O
we O
compare O
the O
models O
as O
a O
function O
of O
the O
maximum O
dependency O
length O
in O
each O
example O
. O
Longer O
dependencies O
are O
sometimes O
caused O
by O
reentrancies O
, O
as O
in O
the O
dependency O
between O
: O
part O
- O
of O
and O
he O
in O
Figure O
[ O
reference O
] O
. O
To O
verify O
that O
the O
contribution O
in O
terms O
of O
longer O
dependencies O
is O
complementary O
to O
that O
of O
reentrancies O
, O
we O
exclude O
sentences O
with O
reentrancies O
from O
this O
analysis O
. O
Table O
[ O
reference O
] O
shows O
the O
statistics O
for O
this O
measure O
. O
Results O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
The O
graph Method
encoder Method
always O
outperforms O
both O
the O
sequential Method
and O
the O
tree Method
encoder Method
. O
The O
gap O
with O
the O
sequential Method
encoder O
increases O
for O
longer O
dependencies O
. O
This O
indicates O
that O
longer O
dependencies O
are O
an O
important O
factor O
in O
improving O
results O
for O
both O
tree Method
and Method
graph Method
encoders Method
, O
especially O
for O
the O
latter O
. O
section O
: O
Conclusions O
We O
introduced O
models O
for O
AMR Task
- Task
to Task
- Task
text Task
generation Task
with O
the O
purpose O
of O
investigating O
the O
difference O
between O
sequential Method
, O
tree O
and O
graph O
encoders O
. O
We O
showed O
that O
encoding O
reentrancies O
improves O
overall O
performance O
. O
We O
observed O
bigger O
benefits O
when O
the O
input O
AMR O
graphs O
have O
a O
larger O
number O
of O
reentrant O
structures O
and O
longer O
dependencies O
. O
Our O
best O
graph Method
encoder Method
, O
which O
consists O
of O
a O
GCN Method
wired O
to O
a O
BiLSTM Method
network O
, O
improves O
over O
the O
state O
of O
the O
art O
on O
all O
tested O
datasets O
. O
We O
inspected O
the O
differences O
between O
the O
models O
, O
especially O
in O
terms O
of O
co O
- O
references O
and O
control O
structures O
. O
Further O
exploration O
of O
graph Method
encoders Method
is O
left O
to O
future O
work O
, O
which O
may O
result O
crucial O
to O
improve O
performance O
further O
. O
section O
: O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
the O
three O
anonymous O
reviewers O
and O
Adam O
Lopez O
, O
Ioannis O
Konstas O
, O
Diego O
Marcheggiani O
, O
Sorcha O
Gilroy O
, O
Sameer O
Bansal O
, O
Ida O
Szubert O
and O
Clara O
Vania O
for O
their O
help O
and O
comments O
. O
This O
research O
was O
supported O
by O
a O
grant O
from O
Bloomberg O
and O
by O
the O
H2020 O
project O
SUMMA O
, O
under O
grant O
agreement O
688139 O
. O
bibliography O
: O
References O
