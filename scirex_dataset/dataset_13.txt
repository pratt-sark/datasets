document	O
:	O
Bag	Method
of	Method
Tricks	Method
for	O
Efficient	O
Text	Task
Classification	Task
This	O
paper	O
explores	O
a	O
simple	O
and	O
efficient	O
baseline	O
for	O
text	Task
classification	Task
.	O
Our	O
experiments	O
show	O
that	O
our	O
fast	O
text	O
classifier	O
fastText	Method
is	O
often	O
on	O
par	O
with	O
deep	Method
learning	Method
classifiers	Method
in	O
terms	O
of	O
accuracy	Metric
,	O
and	O
many	O
orders	O
of	O
magnitude	O
faster	O
for	O
training	Task
and	O
evaluation	Task
.	O
We	O
can	O
train	O
fastText	Method
on	O
more	O
than	O
one	O
billion	O
words	O
in	O
less	O
than	O
ten	O
minutes	O
using	O
a	O
standard	O
multicore	Method
CPU	Method
,	O
and	O
classify	O
half	O
a	O
million	O
sentences	O
among	O
312	O
K	O
classes	O
in	O
less	O
than	O
a	O
minute	O
.	O
section	O
:	O
Introduction	O
Text	Task
classification	Task
is	O
an	O
important	O
task	O
in	O
Natural	Task
Language	Task
Processing	Task
with	O
many	O
applications	O
,	O
such	O
as	O
web	Task
search	Task
,	O
information	Task
retrieval	Task
,	O
ranking	Task
and	O
document	Task
classification	Task
.	O
Recently	O
,	O
models	O
based	O
on	O
neural	Method
networks	Method
have	O
become	O
increasingly	O
popular	O
.	O
While	O
these	O
models	O
achieve	O
very	O
good	O
performance	O
in	O
practice	O
,	O
they	O
tend	O
to	O
be	O
relatively	O
slow	O
both	O
at	O
train	Metric
and	Metric
test	Metric
time	Metric
,	O
limiting	O
their	O
use	O
on	O
very	O
large	O
datasets	O
.	O
Meanwhile	O
,	O
linear	Method
classifiers	Method
are	O
often	O
considered	O
as	O
strong	O
baselines	O
for	O
text	Task
classification	Task
problems	Task
.	O
Despite	O
their	O
simplicity	O
,	O
they	O
often	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
if	O
the	O
right	O
features	O
are	O
used	O
.	O
They	O
also	O
have	O
the	O
potential	O
to	O
scale	O
to	O
very	O
large	O
corpus	O
.	O
In	O
this	O
work	O
,	O
we	O
explore	O
ways	O
to	O
scale	O
these	O
baselines	O
to	O
very	O
large	O
corpus	O
with	O
a	O
large	O
output	O
space	O
,	O
in	O
the	O
context	O
of	O
text	Task
classification	Task
.	O
Inspired	O
by	O
the	O
recent	O
work	O
in	O
efficient	O
word	Task
representation	Task
learning	Task
,	O
we	O
show	O
that	O
linear	Method
models	Method
with	O
a	O
rank	O
constraint	O
and	O
a	O
fast	Method
loss	Method
approximation	Method
can	O
train	O
on	O
a	O
billion	O
words	O
within	O
ten	O
minutes	O
,	O
while	O
achieving	O
performance	O
on	O
par	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
We	O
evaluate	O
the	O
quality	O
of	O
our	O
approach	O
fastTexthttps:	O
//	O
github.com	O
/	O
facebookresearch	O
/	O
fastText	Method
on	O
two	O
different	O
tasks	O
,	O
namely	O
tag	Task
prediction	Task
and	O
sentiment	Task
analysis	Task
.	O
section	O
:	O
Model	O
architecture	O
A	O
simple	O
and	O
efficient	O
baseline	O
for	O
sentence	Task
classification	Task
is	O
to	O
represent	O
sentences	O
as	O
bag	O
of	O
words	O
(	O
BoW	Method
)	O
and	O
train	O
a	O
linear	Method
classifier	Method
,	O
e.g.	O
,	O
a	O
logistic	Method
regression	Method
or	O
an	O
SVM	Method
.	O
However	O
,	O
linear	Method
classifiers	Method
do	O
not	O
share	O
parameters	O
among	O
features	O
and	O
classes	O
.	O
This	O
possibly	O
limits	O
their	O
generalization	O
in	O
the	O
context	O
of	O
large	O
output	O
space	O
where	O
some	O
classes	O
have	O
very	O
few	O
examples	O
.	O
Common	O
solutions	O
to	O
this	O
problem	O
are	O
to	O
factorize	O
the	O
linear	Method
classifier	Method
into	O
low	O
rank	O
matrices	O
or	O
to	O
use	O
multilayer	Method
neural	Method
networks	Method
.	O
Figure	O
[	O
reference	O
]	O
shows	O
a	O
simple	O
linear	Method
model	Method
with	O
rank	O
constraint	O
.	O
The	O
first	O
weight	Method
matrix	Method
is	O
a	O
look	O
-	O
up	O
table	O
over	O
the	O
words	O
.	O
The	O
word	Method
representations	Method
are	O
then	O
averaged	O
into	O
a	O
text	Method
representation	Method
,	O
which	O
is	O
in	O
turn	O
fed	O
to	O
a	O
linear	Method
classifier	Method
.	O
The	O
text	Method
representation	Method
is	O
an	O
hidden	O
variable	O
which	O
can	O
be	O
potentially	O
be	O
reused	O
.	O
This	O
architecture	O
is	O
similar	O
to	O
the	O
cbow	Method
model	Method
of	O
mikolov2013efficient	O
,	O
where	O
the	O
middle	O
word	O
is	O
replaced	O
by	O
a	O
label	O
.	O
We	O
use	O
the	O
softmax	Method
function	Method
to	O
compute	O
the	O
probability	O
distribution	O
over	O
the	O
predefined	O
classes	O
.	O
For	O
a	O
set	O
of	O
documents	O
,	O
this	O
leads	O
to	O
minimizing	O
the	O
negative	O
log	O
-	O
likelihood	O
over	O
the	O
classes	O
:	O
where	O
is	O
the	O
normalized	O
bag	O
of	O
features	O
of	O
the	O
-	O
th	O
document	O
,	O
the	O
label	O
,	O
and	O
the	O
weight	O
matrices	O
.	O
This	O
model	O
is	O
trained	O
asynchronously	O
on	O
multiple	O
CPUs	Method
using	O
stochastic	Method
gradient	Method
descent	Method
and	O
a	O
linearly	Method
decaying	Method
learning	Method
rate	Method
.	O
subsection	O
:	O
Hierarchical	Method
softmax	Method
hidden	O
output	O
When	O
the	O
number	O
of	O
classes	O
is	O
large	O
,	O
computing	O
the	O
linear	Method
classifier	Method
is	O
computationally	O
expensive	O
.	O
More	O
precisely	O
,	O
the	O
computational	Metric
complexity	Metric
is	O
where	O
is	O
the	O
number	O
of	O
classes	O
and	O
the	O
dimension	O
of	O
the	O
text	Method
representation	Method
.	O
In	O
order	O
to	O
improve	O
our	O
running	O
time	O
,	O
we	O
use	O
a	O
hierarchical	Method
softmax	Method
based	O
on	O
the	O
Huffman	Method
coding	Method
tree	Method
.	O
During	O
training	Task
,	O
the	O
computational	Metric
complexity	Metric
drops	O
to	O
.	O
The	O
hierarchical	Method
softmax	Method
is	O
also	O
advantageous	O
at	O
test	O
time	O
when	O
searching	O
for	O
the	O
most	O
likely	O
class	O
.	O
Each	O
node	O
is	O
associated	O
with	O
a	O
probability	O
that	O
is	O
the	O
probability	O
of	O
the	O
path	O
from	O
the	O
root	O
to	O
that	O
node	O
.	O
If	O
the	O
node	O
is	O
at	O
depth	O
with	O
parents	O
,	O
its	O
probability	O
is	O
This	O
means	O
that	O
the	O
probability	O
of	O
a	O
node	O
is	O
always	O
lower	O
than	O
the	O
one	O
of	O
its	O
parent	O
.	O
Exploring	O
the	O
tree	O
with	O
a	O
depth	Method
first	Method
search	Method
and	O
tracking	O
the	O
maximum	O
probability	O
among	O
the	O
leaves	O
allows	O
us	O
to	O
discard	O
any	O
branch	O
associated	O
with	O
a	O
small	O
probability	O
.	O
In	O
practice	O
,	O
we	O
observe	O
a	O
reduction	O
of	O
the	O
complexity	Metric
to	O
at	O
test	O
time	O
.	O
This	O
approach	O
is	O
further	O
extended	O
to	O
compute	O
the	O
-	O
top	O
targets	O
at	O
the	O
cost	O
of	O
,	O
using	O
a	O
binary	O
heap	O
.	O
subsection	O
:	O
N	Method
-	Method
gram	Method
features	Method
Bag	O
of	O
words	O
is	O
invariant	O
to	O
word	O
order	O
but	O
taking	O
explicitly	O
this	O
order	O
into	O
account	O
is	O
often	O
computationally	O
very	O
expensive	O
.	O
Instead	O
,	O
we	O
use	O
a	O
bag	Method
of	Method
n	Method
-	Method
grams	Method
as	O
additional	O
features	O
to	O
capture	O
some	O
partial	O
information	O
about	O
the	O
local	O
word	O
order	O
.	O
This	O
is	O
very	O
efficient	O
in	O
practice	O
while	O
achieving	O
comparable	O
results	O
to	O
methods	O
that	O
explicitly	O
use	O
the	O
order	O
.	O
We	O
maintain	O
a	O
fast	O
and	O
memory	O
efficient	O
mapping	O
of	O
the	O
n	O
-	O
grams	O
by	O
using	O
the	O
hashing	Method
trick	Method
with	O
the	O
same	O
hashing	O
function	O
as	O
in	O
mikolov2011strategies	O
and	O
10	O
M	O
bins	O
if	O
we	O
only	O
used	O
bigrams	O
,	O
and	O
100	O
M	O
otherwise	O
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
fastText	Method
on	O
two	O
different	O
tasks	O
.	O
First	O
,	O
we	O
compare	O
it	O
to	O
existing	O
text	Method
classifers	Method
on	O
the	O
problem	O
of	O
sentiment	Task
analysis	Task
.	O
Then	O
,	O
we	O
evaluate	O
its	O
capacity	O
to	O
scale	O
to	O
large	O
output	O
space	O
on	O
a	O
tag	Task
prediction	Task
dataset	Task
.	O
Note	O
that	O
our	O
model	O
could	O
be	O
implemented	O
with	O
the	O
Vowpal	Method
Wabbit	Method
library	Method
,	O
but	O
we	O
observe	O
in	O
practice	O
,	O
that	O
our	O
tailored	O
implementation	O
is	O
at	O
least	O
2	O
-	O
5	O
faster	O
.	O
subsection	O
:	O
Sentiment	Task
analysis	Task
paragraph	O
:	O
Datasets	O
and	O
baselines	O
.	O
We	O
employ	O
the	O
same	O
8	O
datasets	O
and	O
evaluation	O
protocol	O
of	O
zhang2015character	O
.	O
We	O
report	O
the	O
n	O
-	O
grams	O
and	O
TFIDF	Method
baselines	Method
from	O
zhang2015character	O
,	O
as	O
well	O
as	O
the	O
character	Method
level	Method
convolutional	Method
model	Method
(	O
char	Method
-	Method
CNN	Method
)	O
of	O
zhang2015text	O
,	O
the	O
character	Method
based	Method
convolution	Method
recurrent	Method
network	Method
(	O
char	Method
-	Method
CRNN	Method
)	O
of	O
and	O
the	O
very	Method
deep	Method
convolutional	Method
network	Method
(	O
VDCNN	Method
)	O
of	O
conneau2016	O
.	O
We	O
also	O
compare	O
to	O
tang2015document	O
following	O
their	O
evaluation	O
protocol	O
.	O
We	O
report	O
their	O
main	O
baselines	O
as	O
well	O
as	O
their	O
two	O
approaches	O
based	O
on	O
recurrent	Method
networks	Method
(	O
Conv	Method
-	Method
GRNN	Method
and	Method
LSTM	Method
-	Method
GRNN	Method
)	O
.	O
paragraph	O
:	O
Results	O
.	O
We	O
present	O
the	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
use	O
10	O
hidden	O
units	O
and	O
run	O
fastText	Method
for	O
5	O
epochs	O
with	O
a	O
learning	Metric
rate	Metric
selected	O
on	O
a	O
validation	O
set	O
from	O
0.05	O
,	O
0.1	O
,	O
0.25	O
,	O
0.5	O
.	O
On	O
this	O
task	O
,	O
adding	O
bigram	O
information	O
improves	O
the	O
performance	O
by	O
1	O
-	O
4	O
.	O
Overall	O
our	O
accuracy	Metric
is	O
slightly	O
better	O
than	O
char	Method
-	Method
CNN	Method
and	O
char	Method
-	Method
CRNN	Method
and	O
,	O
a	O
bit	O
worse	O
than	O
VDCNN	Method
.	O
Note	O
that	O
we	O
can	O
increase	O
the	O
accuracy	Metric
slightly	O
by	O
using	O
more	O
n	O
-	O
grams	O
,	O
for	O
example	O
with	O
trigrams	O
,	O
the	O
performance	O
on	O
Sogou	Method
goes	O
up	O
to	O
97.1	O
.	O
Finally	O
,	O
Figure	O
[	O
reference	O
]	O
shows	O
that	O
our	O
method	O
is	O
competitive	O
with	O
the	O
methods	O
presented	O
in	O
tang2015document	Method
.	O
We	O
tune	O
the	O
hyper	O
-	O
parameters	O
on	O
the	O
validation	O
set	O
and	O
observe	O
that	O
using	O
n	O
-	O
grams	O
up	O
to	O
5	O
leads	O
to	O
the	O
best	O
performance	O
.	O
Unlike	O
tang2015document	O
,	O
fastText	Method
does	O
not	O
use	O
pre	O
-	O
trained	O
word	O
embeddings	O
,	O
which	O
can	O
be	O
explained	O
the	O
1	O
difference	O
in	O
accuracy	Metric
.	O
paragraph	O
:	O
Training	Metric
time	Metric
.	O
Both	O
char	Method
-	Method
CNN	Method
and	O
VDCNN	Method
are	O
trained	O
on	O
a	O
NVIDIA	Method
Tesla	Method
K40	Method
GPU	Method
,	O
while	O
our	O
models	O
are	O
trained	O
on	O
a	O
CPU	Method
using	O
20	O
threads	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
methods	O
using	O
convolutions	Method
are	O
several	O
orders	O
of	O
magnitude	O
slower	O
than	O
fastText	Method
.	O
While	O
it	O
is	O
possible	O
to	O
have	O
a	O
10	O
speed	Metric
up	Metric
for	O
char	Method
-	Method
CNN	Method
by	O
using	O
more	O
recent	O
CUDA	Method
implementations	Method
of	O
convolutions	Method
,	O
fastText	Method
takes	O
less	O
than	O
a	O
minute	O
to	O
train	O
on	O
these	O
datasets	O
.	O
The	O
GRNNs	Method
method	Method
of	O
tang2015document	Method
takes	O
around	O
12	O
hours	O
per	O
epoch	O
on	O
CPU	O
with	O
a	O
single	O
thread	O
.	O
Our	O
speed	O
-	O
up	O
compared	O
to	O
neural	Method
network	Method
based	Method
methods	Method
increases	O
with	O
the	O
size	O
of	O
the	O
dataset	O
,	O
going	O
up	O
to	O
at	O
least	O
a	O
15	O
,	O
000	O
speed	O
-	O
up	O
.	O
subsection	O
:	O
Tag	Task
prediction	Task
paragraph	O
:	O
Dataset	O
and	O
baselines	O
.	O
To	O
test	O
scalability	O
of	O
our	O
approach	O
,	O
further	O
evaluation	O
is	O
carried	O
on	O
the	O
YFCC100	O
M	O
dataset	O
which	O
consists	O
of	O
almost	O
100	O
M	O
images	O
with	O
captions	O
,	O
titles	O
and	O
tags	O
.	O
We	O
focus	O
on	O
predicting	O
the	O
tags	O
according	O
to	O
the	O
title	O
and	O
caption	O
(	O
we	O
do	O
not	O
use	O
the	O
images	O
)	O
.	O
We	O
remove	O
the	O
words	O
and	O
tags	O
occurring	O
less	O
than	O
100	O
times	O
and	O
split	O
the	O
data	O
into	O
a	O
train	O
,	O
validation	O
and	O
test	O
set	O
.	O
The	O
train	O
set	O
contains	O
91	O
,	O
188	O
,	O
648	O
examples	O
(	O
1.5B	O
tokens	O
)	O
.	O
The	O
validation	O
has	O
930	O
,	O
497	O
examples	O
and	O
the	O
test	O
set	O
543	O
,	O
424	O
.	O
The	O
vocabulary	Metric
size	Metric
is	O
297	O
,	O
141	O
and	O
there	O
are	O
312	O
,	O
116	O
unique	O
tags	O
.	O
We	O
will	O
release	O
a	O
script	O
that	O
recreates	O
this	O
dataset	O
so	O
that	O
our	O
numbers	O
could	O
be	O
reproduced	O
.	O
We	O
report	O
precision	Metric
at	O
1	O
.	O
We	O
consider	O
a	O
frequency	Method
-	Method
based	Method
baseline	Method
which	O
predicts	O
the	O
most	O
frequent	O
tag	O
.	O
We	O
also	O
compare	O
with	O
Tagspace	Method
,	O
which	O
is	O
a	O
tag	Method
prediction	Method
model	Method
similar	O
to	O
ours	O
,	O
but	O
based	O
on	O
the	O
Wsabie	Method
model	Method
of	O
weston2011wsabie	O
.	O
While	O
the	O
Tagspace	Method
model	Method
is	O
described	O
using	O
convolutions	Method
,	O
we	O
consider	O
the	O
linear	Method
version	Method
,	O
which	O
achieves	O
comparable	O
performance	O
but	O
is	O
much	O
faster	O
.	O
paragraph	O
:	O
Results	O
and	O
training	O
time	O
.	O
Table	O
[	O
reference	O
]	O
presents	O
a	O
comparison	O
of	O
fastText	Method
and	O
the	O
baselines	O
.	O
We	O
run	O
fastText	Method
for	O
5	O
epochs	O
and	O
compare	O
it	O
to	O
Tagspace	O
for	O
two	O
sizes	O
of	O
the	O
hidden	O
layer	O
,	O
i.e.	O
,	O
50	O
and	O
200	O
.	O
Both	O
models	O
achieve	O
a	O
similar	O
performance	O
with	O
a	O
small	O
hidden	O
layer	O
,	O
but	O
adding	O
bigrams	Method
gives	O
us	O
a	O
significant	O
boost	O
in	O
accuracy	Metric
.	O
At	O
test	O
time	O
,	O
Tagspace	O
needs	O
to	O
compute	O
the	O
scores	O
for	O
all	O
the	O
classes	O
which	O
makes	O
it	O
relatively	O
slow	O
,	O
while	O
our	O
fast	Method
inference	Method
gives	O
a	O
significant	O
speed	O
-	O
up	O
when	O
the	O
number	O
of	O
classes	O
is	O
large	O
(	O
more	O
than	O
300	O
K	O
here	O
)	O
.	O
Overall	O
,	O
we	O
are	O
more	O
than	O
an	O
order	O
of	O
magnitude	O
faster	O
to	O
obtain	O
model	O
with	O
a	O
better	O
quality	O
.	O
The	O
speedup	O
of	O
the	O
test	Method
phase	Method
is	O
even	O
more	O
significant	O
(	O
a	O
600	O
speedup	O
)	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
some	O
qualitative	O
examples	O
.	O
section	O
:	O
Discussion	O
and	O
conclusion	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
baseline	O
method	O
for	O
text	Task
classification	Task
.	O
Unlike	O
unsupervisedly	Method
trained	Method
word	Method
vectors	Method
from	O
word2vec	Method
,	O
our	O
word	O
features	O
can	O
be	O
averaged	O
together	O
to	O
form	O
good	O
sentence	Method
representations	Method
.	O
In	O
several	O
tasks	O
,	O
fastText	Method
obtains	O
performance	O
on	O
par	O
with	O
recently	O
proposed	O
methods	O
inspired	O
by	O
deep	Method
learning	Method
,	O
while	O
being	O
much	O
faster	O
.	O
Although	O
deep	Method
neural	Method
networks	Method
have	O
in	O
theory	O
much	O
higher	O
representational	Method
power	Method
than	O
shallow	Method
models	Method
,	O
it	O
is	O
not	O
clear	O
if	O
simple	O
text	Task
classification	Task
problems	Task
such	O
as	O
sentiment	Task
analysis	Task
are	O
the	O
right	O
ones	O
to	O
evaluate	O
them	O
.	O
We	O
will	O
publish	O
our	O
code	O
so	O
that	O
the	O
research	O
community	O
can	O
easily	O
build	O
on	O
top	O
of	O
our	O
work	O
.	O
paragraph	O
:	O
Acknowledgement	O
.	O
We	O
thank	O
Gabriel	O
Synnaeve	O
,	O
Hervé	O
Gégou	O
,	O
Jason	O
Weston	O
and	O
Léon	O
Bottou	O
for	O
their	O
help	O
and	O
comments	O
.	O
We	O
also	O
thank	O
Alexis	O
Conneau	O
,	O
Duyu	O
Tang	O
and	O
Zichao	O
Zhang	O
for	O
providing	O
us	O
with	O
information	O
about	O
their	O
methods	O
.	O
bibliography	O
:	O
References	O
