document O
: O
Learning O
Phrase Method
Representations Method
using O
RNN Method
Encoder O
– O
Decoder O
for O
Statistical Task
Machine Task
Translation Task
In O
this O
paper O
, O
we O
propose O
a O
novel O
neural Method
network Method
model Method
called O
RNN Method
Encoder O
– O
Decoder O
that O
consists O
of O
two O
recurrent Method
neural Method
networks Method
( O
RNN Method
) O
. O
One O
RNN Method
encodes O
a O
sequence O
of O
symbols O
into O
a O
fixed Method
- Method
length Method
vector Method
representation Method
, O
and O
the O
other O
decodes O
the O
representation O
into O
another O
sequence O
of O
symbols O
. O
The O
encoder Method
and O
decoder Method
of O
the O
proposed O
model O
are O
jointly O
trained O
to O
maximize O
the O
conditional O
probability O
of O
a O
target O
sequence O
given O
a O
source O
sequence O
. O
The O
performance O
of O
a O
statistical Task
machine Task
translation Task
system Task
is O
empirically O
found O
to O
improve O
by O
using O
the O
conditional O
probabilities O
of O
phrase O
pairs O
computed O
by O
the O
RNN Method
Encoder O
– O
Decoder O
as O
an O
additional O
feature O
in O
the O
existing O
log Method
- Method
linear Method
model Method
. O
Qualitatively O
, O
we O
show O
that O
the O
proposed O
model O
learns O
a O
semantically Method
and O
syntactically Method
meaningful Method
representation Method
of O
linguistic O
phrases O
. O
arxiv O
arxiv. O
/ O
figures O
/ O
section O
: O
Introduction O
Deep Method
neural Method
networks Method
have O
shown O
great O
success O
in O
various O
applications O
such O
as O
objection Task
recognition Task
( O
see O
, O
e.g. O
, O
) O
and O
speech Task
recognition Task
( O
see O
, O
e.g. O
, O
) O
. O
Furthermore O
, O
many O
recent O
works O
showed O
that O
neural Method
networks Method
can O
be O
successfully O
used O
in O
a O
number O
of O
tasks O
in O
natural Task
language Task
processing Task
( O
NLP Task
) Task
. O
These O
include O
, O
but O
are O
not O
limited O
to O
, O
language Task
modeling Task
, O
paraphrase Task
detection Task
and O
word Task
embedding Task
extraction Task
. O
In O
the O
field O
of O
statistical Task
machine Task
translation Task
( O
SMT Task
) Task
, O
deep Method
neural Method
networks Method
have O
begun O
to O
show O
promising O
results O
. O
summarizes O
a O
successful O
usage O
of O
feedforward Method
neural Method
networks Method
in O
the O
framework O
of O
phrase Method
- Method
based Method
SMT Method
system Method
. O
Along O
this O
line O
of O
research O
on O
using O
neural Method
networks Method
for O
SMT Task
, O
this O
paper O
focuses O
on O
a O
novel O
neural Method
network Method
architecture Method
that O
can O
be O
used O
as O
a O
part O
of O
the O
conventional O
phrase Method
- Method
based Method
SMT Method
system Method
. O
The O
proposed O
neural Method
network Method
architecture Method
, O
which O
we O
will O
refer O
to O
as O
an O
RNN Method
Encoder O
– O
Decoder O
, O
consists O
of O
two O
recurrent Method
neural Method
networks Method
( O
RNN Method
) O
that O
act O
as O
an O
encoder Method
and O
a O
decoder O
pair O
. O
The O
encoder Method
maps O
a O
variable O
- O
length O
source O
sequence O
to O
a O
fixed O
- O
length O
vector O
, O
and O
the O
decoder Method
maps O
the O
vector Method
representation Method
back O
to O
a O
variable O
- O
length O
target O
sequence O
. O
The O
two O
networks O
are O
trained O
jointly O
to O
maximize O
the O
conditional O
probability O
of O
the O
target O
sequence O
given O
a O
source O
sequence O
. O
Additionally O
, O
we O
propose O
to O
use O
a O
rather O
sophisticated O
hidden O
unit O
in O
order O
to O
improve O
both O
the O
memory Metric
capacity Metric
and O
the O
ease O
of O
training Task
. O
The O
proposed O
RNN Method
Encoder O
– O
Decoder O
with O
a O
novel O
hidden Method
unit Method
is O
empirically O
evaluated O
on O
the O
task O
of O
translating Task
from O
English Material
to Material
French Material
. O
We O
train O
the O
model O
to O
learn O
the O
translation O
probability O
of O
an O
English O
phrase O
to O
a O
corresponding O
French O
phrase O
. O
The O
model O
is O
then O
used O
as O
a O
part O
of O
a O
standard O
phrase Method
- Method
based Method
SMT Method
system Method
by O
scoring O
each O
phrase O
pair O
in O
the O
phrase O
table O
. O
The O
empirical O
evaluation O
reveals O
that O
this O
approach O
of O
scoring Task
phrase Task
pairs Task
with O
an O
RNN Method
Encoder O
– O
Decoder O
improves O
the O
translation Task
performance O
. O
We O
qualitatively O
analyze O
the O
trained O
RNN Method
Encoder O
– O
Decoder O
by O
comparing O
its O
phrase O
scores O
with O
those O
given O
by O
the O
existing O
translation Method
model Method
. O
The O
qualitative O
analysis O
shows O
that O
the O
RNN Method
Encoder O
– O
Decoder O
is O
better O
at O
capturing O
the O
linguistic O
regularities O
in O
the O
phrase O
table O
, O
indirectly O
explaining O
the O
quantitative O
improvements O
in O
the O
overall O
translation Metric
performance Metric
. O
The O
further O
analysis O
of O
the O
model O
reveals O
that O
the O
RNN Method
Encoder O
– O
Decoder O
learns O
a O
continuous Method
space Method
representation Method
of O
a O
phrase O
that O
preserves O
both O
the O
semantic O
and O
syntactic O
structure O
of O
the O
phrase O
. O
section O
: O
RNN Method
Encoder O
– O
Decoder O
subsection O
: O
Preliminary O
: O
Recurrent Method
Neural Method
Networks Method
A O
recurrent Method
neural Method
network Method
( O
RNN Method
) O
is O
a O
neural Method
network Method
that O
consists O
of O
a O
hidden O
state O
and O
an O
optional O
output O
which O
operates O
on O
a O
variable O
- O
length O
sequence O
. O
At O
each O
time O
step O
, O
the O
hidden O
state O
of O
the O
RNN Method
is O
updated O
by O
where O
is O
a O
non Method
- Method
linear Method
activation Method
function Method
. O
may O
be O
as O
simple O
as O
an O
element Method
- Method
wise Method
logistic Method
sigmoid Method
function Method
and O
as O
complex O
as O
a O
long Method
short Method
- Method
term Method
memory Method
( Method
LSTM Method
) Method
unit Method
. O
An O
RNN Method
can O
learn O
a O
probability O
distribution O
over O
a O
sequence O
by O
being O
trained O
to O
predict O
the O
next O
symbol O
in O
a O
sequence O
. O
In O
that O
case O
, O
the O
output O
at O
each O
timestep O
is O
the O
conditional O
distribution O
. O
For O
example O
, O
a O
multinomial Method
distribution Method
( Method
- Method
of Method
- Method
coding Method
) O
can O
be O
output O
using O
a O
softmax Method
activation Method
function Method
for O
all O
possible O
symbols O
, O
where O
are O
the O
rows O
of O
a O
weight O
matrix O
. O
By O
combining O
these O
probabilities O
, O
we O
can O
compute O
the O
probability O
of O
the O
sequence O
using O
From O
this O
learned Method
distribution Method
, O
it O
is O
straightforward O
to O
sample O
a O
new O
sequence O
by O
iteratively O
sampling O
a O
symbol O
at O
each O
time O
step O
. O
subsection O
: O
RNN Method
Encoder O
– O
Decoder O
In O
this O
paper O
, O
we O
propose O
a O
novel O
neural Method
network Method
architecture Method
that O
learns O
to O
encode O
a O
variable O
- O
length O
sequence O
into O
a O
fixed Method
- Method
length Method
vector Method
representation Method
and O
to O
decode O
a O
given O
fixed Method
- Method
length Method
vector Method
representation Method
back O
into O
a O
variable O
- O
length O
sequence O
. O
From O
a O
probabilistic O
perspective O
, O
this O
new O
model O
is O
a O
general O
method O
to O
learn O
the O
conditional O
distribution O
over O
a O
variable O
- O
length O
sequence O
conditioned O
on O
yet O
another O
variable O
- O
length O
sequence O
, O
e.g. O
, O
where O
one O
should O
note O
that O
the O
input O
and O
output O
sequence O
lengths O
and O
may O
differ O
. O
The O
encoder Method
is O
an O
RNN Method
that O
reads O
each O
symbol O
of O
an O
input O
sequence O
sequentially O
. O
As O
it O
reads O
each O
symbol O
, O
the O
hidden O
state O
of O
the O
RNN Method
changes O
according O
to O
Eq O
. O
( O
[ O
reference O
] O
) O
. O
After O
reading O
the O
end O
of O
the O
sequence O
( O
marked O
by O
an O
end O
- O
of O
- O
sequence O
symbol O
) O
, O
the O
hidden O
state O
of O
the O
RNN Method
is O
a O
summary O
of O
the O
whole O
input O
sequence O
. O
The O
decoder O
of O
the O
proposed O
model O
is O
another O
RNN Method
which O
is O
trained O
to O
generate O
the O
output O
sequence O
by O
predicting O
the O
next O
symbol O
given O
the O
hidden O
state O
. O
However O
, O
unlike O
the O
RNN Method
described O
in O
Sec O
. O
[ O
reference O
] O
, O
both O
and O
are O
also O
conditioned O
on O
and O
on O
the O
summary O
of O
the O
input O
sequence O
. O
Hence O
, O
the O
hidden O
state O
of O
the O
decoder O
at O
time O
is O
computed O
by O
, O
and O
similarly O
, O
the O
conditional O
distribution O
of O
the O
next O
symbol O
is O
for O
given O
activation O
functions O
and O
( O
the O
latter O
must O
produce O
valid O
probabilities O
, O
e.g. O
with O
a O
softmax Method
) O
. O
See O
Fig O
. O
[ O
reference O
] O
for O
a O
graphical O
depiction O
of O
the O
proposed O
model Method
architecture Method
. O
The O
two O
components O
of O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
are O
jointly O
trained O
to O
maximize O
the O
conditional O
log O
- O
likelihood O
where O
is O
the O
set O
of O
the O
model O
parameters O
and O
each O
is O
an O
( O
input O
sequence O
, O
output O
sequence O
) O
pair O
from O
the O
training O
set O
. O
In O
our O
case O
, O
as O
the O
output O
of O
the O
decoder Method
, O
starting O
from O
the O
input O
, O
is O
differentiable O
, O
we O
can O
use O
a O
gradient Method
- Method
based Method
algorithm Method
to O
estimate O
the O
model O
parameters O
. O
Once O
the O
RNN Method
Encoder O
– O
Decoder O
is O
trained O
, O
the O
model O
can O
be O
used O
in O
two O
ways O
. O
One O
way O
is O
to O
use O
the O
model O
to O
generate O
a O
target O
sequence O
given O
an O
input O
sequence O
. O
On O
the O
other O
hand O
, O
the O
model O
can O
be O
used O
to O
score O
a O
given O
pair O
of O
input O
and O
output O
sequences O
, O
where O
the O
score O
is O
simply O
a O
probability O
from O
Eqs O
. O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
. O
subsection O
: O
Hidden Method
Unit Method
that O
Adaptively O
Remembers O
and O
Forgets O
In O
addition O
to O
a O
novel O
model Method
architecture Method
, O
we O
also O
propose O
a O
new O
type O
of O
hidden O
unit O
( O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
that O
has O
been O
motivated O
by O
the O
LSTM Method
unit Method
but O
is O
much O
simpler O
to O
compute O
and O
implement O
. O
Fig O
. O
[ O
reference O
] O
shows O
the O
graphical O
depiction O
of O
the O
proposed O
hidden Method
unit Method
. O
Let O
us O
describe O
how O
the O
activation O
of O
the O
- O
th O
hidden Method
unit Method
is O
computed O
. O
First O
, O
the O
reset O
gate O
is O
computed O
by O
where O
is O
the O
logistic Method
sigmoid Method
function Method
, O
and O
denotes O
the O
- O
th O
element O
of O
a O
vector O
. O
and O
are O
the O
input O
and O
the O
previous O
hidden O
state O
, O
respectively O
. O
and O
are O
weight O
matrices O
which O
are O
learned O
. O
Similarly O
, O
the O
update O
gate O
is O
computed O
by O
The O
actual O
activation O
of O
the O
proposed O
unit O
is O
then O
computed O
by O
where O
In O
this O
formulation O
, O
when O
the O
reset O
gate O
is O
close O
to O
0 O
, O
the O
hidden O
state O
is O
forced O
to O
ignore O
the O
previous O
hidden O
state O
and O
reset O
with O
the O
current O
input O
only O
. O
This O
effectively O
allows O
the O
hidden O
state O
to O
drop O
any O
information O
that O
is O
found O
to O
be O
irrelevant O
later O
in O
the O
future O
, O
thus O
, O
allowing O
a O
more O
compact O
representation O
. O
On O
the O
other O
hand O
, O
the O
update O
gate O
controls O
how O
much O
information O
from O
the O
previous O
hidden O
state O
will O
carry O
over O
to O
the O
current O
hidden O
state O
. O
This O
acts O
similarly O
to O
the O
memory O
cell O
in O
the O
LSTM Method
network Method
and O
helps O
the O
RNN Method
to O
remember O
long O
- O
term O
information O
. O
Furthermore O
, O
this O
may O
be O
considered O
an O
adaptive Method
variant Method
of O
a O
leaky Method
- Method
integration Method
unit Method
. O
As O
each O
hidden Method
unit Method
has O
separate O
reset O
and O
update O
gates O
, O
each O
hidden Method
unit Method
will O
learn O
to O
capture O
dependencies O
over O
different O
time O
scales O
. O
Those O
units O
that O
learn O
to O
capture O
short O
- O
term O
dependencies O
will O
tend O
to O
have O
reset O
gates O
that O
are O
frequently O
active O
, O
but O
those O
that O
capture O
longer O
- O
term O
dependencies O
will O
have O
update O
gates O
that O
are O
mostly O
active O
. O
In O
our O
preliminary O
experiments O
, O
we O
found O
that O
it O
is O
crucial O
to O
use O
this O
new O
unit O
with O
gating Method
units Method
. O
We O
were O
not O
able O
to O
get O
meaningful O
result O
with O
an O
oft O
- O
used O
unit O
without O
any O
gating O
. O
section O
: O
Statistical Task
Machine Task
Translation Task
In O
a O
commonly O
used O
statistical Task
machine Task
translation Task
system Task
( O
SMT Task
) Task
, O
the O
goal O
of O
the O
system O
( O
decoder Method
, O
specifically O
) O
is O
to O
find O
a O
translation O
given O
a O
source O
sentence O
, O
which O
maximizes O
where O
the O
first O
term O
at O
the O
right O
hand O
side O
is O
called O
translation Method
model Method
and O
the O
latter O
language Method
model Method
( O
see O
, O
e.g. O
, O
) O
. O
In O
practice O
, O
however O
, O
most O
SMT Task
systems O
model O
as O
a O
log Method
- Method
linear Method
model Method
with O
additional O
features O
and O
corresponding O
weights O
: O
where O
and O
are O
the O
- O
th O
feature O
and O
weight O
, O
respectively O
. O
is O
a O
normalization O
constant O
that O
does O
not O
depend O
on O
the O
weights O
. O
The O
weights O
are O
often O
optimized O
to O
maximize O
the O
BLEU Metric
score Metric
on O
a O
development O
set O
. O
In O
the O
phrase O
- O
based O
SMT Task
framework O
introduced O
in O
and O
, O
the O
translation Method
model Method
is O
factorized O
into O
the O
translation O
probabilities O
of O
matching O
phrases O
in O
the O
source O
and O
target O
sentences O
. O
These O
probabilities O
are O
once O
again O
considered O
additional O
features O
in O
the O
log Method
- Method
linear Method
model Method
( O
see O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
and O
are O
weighted O
accordingly O
to O
maximize O
the O
BLEU Metric
score Metric
. O
Since O
the O
neural Method
net Method
language Method
model Method
was O
proposed O
in O
, O
neural Method
networks Method
have O
been O
used O
widely O
in O
SMT Method
systems Method
. O
In O
many O
cases O
, O
neural Method
networks Method
have O
been O
used O
to O
rescore Task
translation Task
hypotheses Task
( O
- O
best O
lists O
) O
( O
see O
, O
e.g. O
, O
) O
. O
Recently O
, O
however O
, O
there O
has O
been O
interest O
in O
training O
neural Method
networks Method
to O
score O
the O
translated O
sentence O
( O
or O
phrase O
pairs O
) O
using O
a O
representation O
of O
the O
source O
sentence O
as O
an O
additional O
input O
. O
See O
, O
e.g. O
, O
, O
and O
. O
subsection O
: O
Scoring O
Phrase O
Pairs O
with O
RNN Method
Encoder O
– O
Decoder O
Here O
we O
propose O
to O
train O
the O
RNN Method
Encoder O
– O
Decoder O
( O
see O
Sec O
. O
[ O
reference O
] O
) O
on O
a O
table O
of O
phrase O
pairs O
and O
use O
its O
scores O
as O
additional O
features O
in O
the O
log Method
- Method
linear Method
model Method
in O
Eq O
. O
( O
[ O
reference O
] O
) O
when O
tuning O
the O
SMT Task
decoder O
. O
When O
we O
train O
the O
RNN Method
Encoder O
– O
Decoder O
, O
we O
ignore O
the O
( O
normalized O
) O
frequencies O
of O
each O
phrase O
pair O
in O
the O
original O
corpora O
. O
This O
measure O
was O
taken O
in O
order O
( O
1 O
) O
to O
reduce O
the O
computational Metric
expense Metric
of O
randomly O
selecting O
phrase O
pairs O
from O
a O
large O
phrase O
table O
according O
to O
the O
normalized O
frequencies O
and O
( O
2 O
) O
to O
ensure O
that O
the O
RNN Method
Encoder O
– O
Decoder O
does O
not O
simply O
learn O
to O
rank O
the O
phrase O
pairs O
according O
to O
their O
numbers O
of O
occurrences O
. O
One O
underlying O
reason O
for O
this O
choice O
was O
that O
the O
existing O
translation O
probability O
in O
the O
phrase O
table O
already O
reflects O
the O
frequencies O
of O
the O
phrase O
pairs O
in O
the O
original O
corpus O
. O
With O
a O
fixed O
capacity O
of O
the O
RNN Method
Encoder O
– O
Decoder O
, O
we O
try O
to O
ensure O
that O
most O
of O
the O
capacity O
of O
the O
model O
is O
focused O
toward O
learning O
linguistic O
regularities O
, O
i.e. O
, O
distinguishing O
between O
plausible O
and O
implausible O
translations O
, O
or O
learning O
the O
“ O
manifold O
” O
( O
region O
of O
probability O
concentration O
) O
of O
plausible O
translations O
. O
Once O
the O
RNN Method
Encoder O
– O
Decoder O
is O
trained O
, O
we O
add O
a O
new O
score O
for O
each O
phrase O
pair O
to O
the O
existing O
phrase O
table O
. O
This O
allows O
the O
new O
scores O
to O
enter O
into O
the O
existing O
tuning Method
algorithm Method
with O
minimal O
additional O
overhead O
in O
computation Metric
. O
As O
Schwenk O
pointed O
out O
in O
, O
it O
is O
possible O
to O
completely O
replace O
the O
existing O
phrase O
table O
with O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
. O
In O
that O
case O
, O
for O
a O
given O
source O
phrase O
, O
the O
RNN Method
Encoder O
– O
Decoder O
will O
need O
to O
generate O
a O
list O
of O
( O
good O
) O
target O
phrases O
. O
This O
requires O
, O
however O
, O
an O
expensive O
sampling Method
procedure Method
to O
be O
performed O
repeatedly O
. O
In O
this O
paper O
, O
thus O
, O
we O
only O
consider O
rescoring O
the O
phrase O
pairs O
in O
the O
phrase O
table O
. O
subsection O
: O
Related O
Approaches O
: O
Neural Method
Networks Method
in O
Machine Task
Translation Task
Before O
presenting O
the O
empirical O
results O
, O
we O
discuss O
a O
number O
of O
recent O
works O
that O
have O
proposed O
to O
use O
neural Method
networks Method
in O
the O
context O
of O
SMT Task
. O
Schwenk O
in O
proposed O
a O
similar O
approach O
of O
scoring Task
phrase Task
pairs Task
. O
Instead O
of O
the O
RNN Method
- O
based O
neural O
network O
, O
he O
used O
a O
feedforward Method
neural Method
network Method
that O
has O
fixed O
- O
size O
inputs O
( O
7 O
words O
in O
his O
case O
, O
with O
zero O
- O
padding O
for O
shorter O
phrases O
) O
and O
fixed O
- O
size O
outputs O
( O
7 O
words O
in O
the O
target O
language O
) O
. O
When O
it O
is O
used O
specifically O
for O
scoring Task
phrases Task
for O
the O
SMT Method
system Method
, O
the O
maximum O
phrase O
length O
is O
often O
chosen O
to O
be O
small O
. O
However O
, O
as O
the O
length O
of O
phrases O
increases O
or O
as O
we O
apply O
neural Method
networks Method
to O
other O
variable O
- O
length O
sequence O
data O
, O
it O
is O
important O
that O
the O
neural Method
network Method
can O
handle O
variable O
- O
length O
input O
and O
output O
. O
The O
proposed O
RNN Method
Encoder O
– O
Decoder O
is O
well O
- O
suited O
for O
these O
applications O
. O
Similar O
to O
, O
Devlin O
et O
al O
. O
proposed O
to O
use O
a O
feedforward Method
neural Method
network Method
to O
model O
a O
translation Method
model Method
, O
however O
, O
by O
predicting O
one O
word O
in O
a O
target O
phrase O
at O
a O
time O
. O
They O
reported O
an O
impressive O
improvement O
, O
but O
their O
approach O
still O
requires O
the O
maximum O
length O
of O
the O
input O
phrase O
( O
or O
context O
words O
) O
to O
be O
fixed O
a O
priori O
. O
Although O
it O
is O
not O
exactly O
a O
neural Method
network Method
they O
train O
, O
the O
authors O
of O
proposed O
to O
learn O
a O
bilingual O
embedding O
of O
words O
/ O
phrases O
. O
They O
use O
the O
learned O
embedding Method
to O
compute O
the O
distance O
between O
a O
pair O
of O
phrases O
which O
is O
used O
as O
an O
additional O
score O
of O
the O
phrase O
pair O
in O
an O
SMT Method
system Method
. O
In O
, O
a O
feedforward Method
neural Method
network Method
was O
trained O
to O
learn O
a O
mapping O
from O
a O
bag Method
- Method
of Method
- Method
words Method
representation Method
of O
an O
input O
phrase O
to O
an O
output O
phrase O
. O
This O
is O
closely O
related O
to O
both O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
and O
the O
model O
proposed O
in O
, O
except O
that O
their O
input O
representation O
of O
a O
phrase O
is O
a O
bag O
- O
of O
- O
words O
. O
A O
similar O
approach O
of O
using O
bag Method
- Method
of Method
- Method
words Method
representations Method
was O
proposed O
in O
as O
well O
. O
Earlier O
, O
a O
similar O
encoder Method
– Method
decoder Method
model Method
using O
two O
recursive Method
neural Method
networks Method
was O
proposed O
in O
, O
but O
their O
model O
was O
restricted O
to O
a O
monolingual Task
setting Task
, O
i.e. O
the O
model O
reconstructs O
an O
input O
sentence O
. O
More O
recently O
, O
another O
encoder Method
– Method
decoder Method
model Method
using O
an O
RNN Method
was O
proposed O
in O
, O
where O
the O
decoder Method
is O
conditioned O
on O
a O
representation O
of O
either O
a O
source O
sentence O
or O
a O
source O
context O
. O
One O
important O
difference O
between O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
and O
the O
approaches O
in O
and O
is O
that O
the O
order O
of O
the O
words O
in O
source O
and O
target O
phrases O
is O
taken O
into O
account O
. O
The O
RNN Method
Encoder O
– O
Decoder O
naturally O
distinguishes O
between O
sequences O
that O
have O
the O
same O
words O
but O
in O
a O
different O
order O
, O
whereas O
the O
aforementioned O
approaches O
effectively O
ignore O
order O
information O
. O
The O
closest O
approach O
related O
to O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
is O
the O
Recurrent Method
Continuous Method
Translation Method
Model Method
( O
Model O
2 O
) O
proposed O
in O
. O
In O
their O
paper O
, O
they O
proposed O
a O
similar O
model O
that O
consists O
of O
an O
encoder Method
and Method
decoder Method
. O
The O
difference O
with O
our O
model O
is O
that O
they O
used O
a O
convolutional Method
- Method
gram Method
model Method
( O
CGM Method
) Method
for O
the O
encoder Method
and O
the O
hybrid O
of O
an O
inverse Method
CGM Method
and O
a O
recurrent Method
neural Method
network Method
for O
the O
decoder Method
. O
They O
, O
however O
, O
evaluated O
their O
model O
on O
rescoring Task
the Task
- Task
best Task
list Task
proposed O
by O
the O
conventional O
SMT Method
system Method
and O
computing O
the O
perplexity Metric
of O
the O
gold O
standard O
translations O
. O
section O
: O
Experiments O
We O
evaluate O
our O
approach O
on O
the O
English Task
/ Task
French Task
translation Task
task Task
of O
the O
WMT’14 Material
workshop Material
. O
subsection O
: O
Data O
and O
Baseline Method
System O
Large O
amounts O
of O
resources O
are O
available O
to O
build O
an O
English O
/ O
French O
SMT Task
system O
in O
the O
framework O
of O
the O
WMT’14 Task
translation Task
task Task
. O
The O
bilingual O
corpora O
include O
Europarl O
( O
61 O
M O
words O
) O
, O
news O
commentary O
( O
5.5 O
M O
) O
, O
UN O
( O
421 O
M O
) O
, O
and O
two O
crawled O
corpora O
of O
90 O
M O
and O
780 O
M O
words O
respectively O
. O
The O
last O
two O
corpora O
are O
quite O
noisy O
. O
To O
train O
the O
French Method
language Method
model Method
, O
about O
712 O
M O
words O
of O
crawled O
newspaper O
material O
is O
available O
in O
addition O
to O
the O
target O
side O
of O
the O
bitexts O
. O
All O
the O
word O
counts O
refer O
to O
French Material
words Material
after O
tokenization O
. O
It O
is O
commonly O
acknowledged O
that O
training O
statistical Method
models Method
on O
the O
concatenation O
of O
all O
this O
data O
does O
not O
necessarily O
lead O
to O
optimal O
performance O
, O
and O
results O
in O
extremely O
large O
models O
which O
are O
difficult O
to O
handle O
. O
Instead O
, O
one O
should O
focus O
on O
the O
most O
relevant O
subset O
of O
the O
data O
for O
a O
given O
task O
. O
We O
have O
done O
so O
by O
applying O
the O
data Method
selection Method
method Method
proposed O
in O
, O
and O
its O
extension O
to O
bitexts O
. O
By O
these O
means O
we O
selected O
a O
subset O
of O
418 O
M O
words O
out O
of O
more O
than O
2 O
G O
words O
for O
language Task
modeling Task
and O
a O
subset O
of O
348 O
M O
out O
of O
850 O
M O
words O
for O
training O
the O
RNN Method
Encoder O
– O
Decoder O
. O
We O
used O
the O
test O
set O
newstest2012 O
and O
2013 O
for O
data Task
selection Task
and O
weight Task
tuning Task
with O
MERT Method
, O
and O
newstest2014 O
as O
our O
test O
set O
. O
Each O
set O
has O
more O
than O
70 O
thousand O
words O
and O
a O
single O
reference O
translation O
. O
For O
training O
the O
neural Method
networks Method
, O
including O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
, O
we O
limited O
the O
source O
and O
target O
vocabulary O
to O
the O
most O
frequent O
15 O
, O
000 O
words O
for O
both O
English Material
and Material
French Material
. O
This O
covers O
approximately O
93 O
% O
of O
the O
dataset O
. O
All O
the O
out O
- O
of O
- O
vocabulary O
words O
were O
mapped O
to O
a O
special O
token O
( O
) O
. O
The O
baseline O
phrase Method
- Method
based Method
SMT Method
system Method
was O
built O
using O
Moses Method
with O
default O
settings O
. O
This O
system O
achieves O
a O
BLEU Metric
score Metric
of O
30.64 O
and O
33.3 O
on O
the O
development O
and O
test O
sets O
, O
respectively O
( O
see O
Table O
[ O
reference O
] O
) O
. O
subsubsection O
: O
RNN Method
Encoder O
– O
Decoder O
The O
RNN Method
Encoder O
– O
Decoder O
used O
in O
the O
experiment O
had O
1000 O
hidden O
units O
with O
the O
proposed O
gates O
at O
the O
encoder Method
and O
at O
the O
decoder Method
. O
The O
input O
matrix O
between O
each O
input O
symbol O
and O
the O
hidden O
unit O
is O
approximated O
with O
two O
lower O
- O
rank O
matrices O
, O
and O
the O
output O
matrix O
is O
approximated O
similarly O
. O
We O
used O
rank O
- O
100 O
matrices O
, O
equivalent O
to O
learning O
an O
embedding O
of O
dimension O
100 O
for O
each O
word O
. O
The O
activation O
function O
used O
for O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
is O
a O
hyperbolic O
tangent O
function O
. O
The O
computation O
from O
the O
hidden O
state O
in O
the O
decoder O
to O
the O
output O
is O
implemented O
as O
a O
deep Method
neural Method
network Method
with O
a O
single O
intermediate Method
layer Method
having O
500 O
maxout Method
units Method
each O
pooling O
2 O
inputs O
. O
All O
the O
weight O
parameters O
in O
the O
RNN Method
Encoder O
– O
Decoder O
were O
initialized O
by O
sampling O
from O
an O
isotropic Method
zero Method
- Method
mean Method
( Method
white Method
) Method
Gaussian Method
distribution Method
with O
its O
standard O
deviation O
fixed O
to O
, O
except O
for O
the O
recurrent O
weight O
parameters O
. O
For O
the O
recurrent O
weight O
matrices O
, O
we O
first O
sampled O
from O
a O
white Method
Gaussian Method
distribution Method
and O
used O
its O
left O
singular O
vectors O
matrix O
, O
following O
. O
We O
used O
Adadelta Method
and O
stochastic Method
gradient Method
descent Method
to O
train O
the O
RNN Method
Encoder O
– O
Decoder O
with O
hyperparameters Method
and O
. O
At O
each O
update O
, O
we O
used O
64 O
randomly O
selected O
phrase O
pairs O
from O
a O
phrase O
table O
( O
which O
was O
created O
from O
348 O
M O
words O
) O
. O
The O
model O
was O
trained O
for O
approximately O
three O
days O
. O
Details O
of O
the O
architecture O
used O
in O
the O
experiments O
are O
explained O
in O
more O
depth O
in O
the O
supplementary O
material O
. O
subsubsection O
: O
Neural Method
Language Method
Model Method
In O
order O
to O
assess O
the O
effectiveness O
of O
scoring O
phrase O
pairs O
with O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
, O
we O
also O
tried O
a O
more O
traditional O
approach O
of O
using O
a O
neural Method
network Method
for O
learning O
a O
target Method
language Method
model Method
( O
CSLM Method
) O
. O
Especially O
, O
the O
comparison O
between O
the O
SMT Method
system Method
using O
CSLM Method
and O
that O
using O
the O
proposed O
approach O
of O
phrase Method
scoring Method
by O
RNN Method
Encoder O
– O
Decoder O
will O
clarify O
whether O
the O
contributions O
from O
multiple O
neural Method
networks Method
in O
different O
parts O
of O
the O
SMT Method
system Method
add O
up O
or O
are O
redundant O
. O
We O
trained O
the O
CSLM Method
model O
on O
7 O
- O
grams O
from O
the O
target O
corpus O
. O
Each O
input O
word O
was O
projected O
into O
the O
embedding O
space O
, O
and O
they O
were O
concatenated O
to O
form O
a O
3072 O
- O
dimensional O
vector O
. O
The O
concatenated O
vector O
was O
fed O
through O
two O
rectified Method
layers Method
( O
of O
size O
1536 O
and O
1024 O
) O
. O
The O
output O
layer O
was O
a O
simple O
softmax Method
layer Method
( O
see O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
. O
All O
the O
weight O
parameters O
were O
initialized O
uniformly O
between O
and O
, O
and O
the O
model O
was O
trained O
until O
the O
validation Metric
perplexity Metric
did O
not O
improve O
for O
10 O
epochs O
. O
After O
training O
, O
the O
language Method
model Method
achieved O
a O
perplexity Metric
of O
45.80 O
. O
The O
validation O
set O
was O
a O
random O
selection O
of O
0.1 O
% O
of O
the O
corpus O
. O
The O
model O
was O
used O
to O
score O
partial O
translations O
during O
the O
decoding Task
process Task
, O
which O
generally O
leads O
to O
higher O
gains O
in O
BLEU Metric
score Metric
than O
n Method
- Method
best Method
list Method
rescoring Method
. O
To O
address O
the O
computational Metric
complexity Metric
of O
using O
a O
CSLM Method
in O
the O
decoder O
a O
buffer O
was O
used O
to O
aggregate O
n O
- O
grams O
during O
the O
stack Task
- Task
search Task
performed O
by O
the O
decoder O
. O
Only O
when O
the O
buffer O
is O
full O
, O
or O
a O
stack O
is O
about O
to O
be O
pruned O
, O
the O
n O
- O
grams O
are O
scored O
by O
the O
CSLM Method
. O
This O
allows O
us O
to O
perform O
fast O
matrix Task
- Task
matrix Task
multiplication Task
on O
GPU O
using O
Theano O
. O
subsection O
: O
Quantitative Task
Analysis Task
We O
tried O
the O
following O
combinations O
: O
Baseline Method
configuration O
Baseline Method
+ O
RNN Method
Baseline Method
+ O
CSLM Method
+ O
RNN Method
Baseline Method
+ O
CSLM Method
+ O
RNN Method
+ O
Word Method
penalty Method
The O
results O
are O
presented O
in O
Table O
[ O
reference O
] O
. O
As O
expected O
, O
adding O
features O
computed O
by O
neural Method
networks Method
consistently O
improves O
the O
performance O
over O
the O
baseline O
performance O
. O
The O
best O
performance O
was O
achieved O
when O
we O
used O
both O
CSLM Method
and O
the O
phrase O
scores O
from O
the O
RNN Method
Encoder O
– O
Decoder O
. O
This O
suggests O
that O
the O
contributions O
of O
the O
CSLM Method
and O
the O
RNN Method
Encoder O
– O
Decoder O
are O
not O
too O
correlated O
and O
that O
one O
can O
expect O
better O
results O
by O
improving O
each O
method O
independently O
. O
Furthermore O
, O
we O
tried O
penalizing O
the O
number O
of O
words O
that O
are O
unknown O
to O
the O
neural Method
networks Method
( O
i.e. O
words O
which O
are O
not O
in O
the O
shortlist O
) O
. O
We O
do O
so O
by O
simply O
adding O
the O
number O
of O
unknown O
words O
as O
an O
additional O
feature O
the O
log Method
- Method
linear Method
model Method
in O
Eq O
. O
( O
[ O
reference O
] O
) O
. O
However O
, O
in O
this O
case O
we O
were O
not O
able O
to O
achieve O
better O
performance O
on O
the O
test O
set O
, O
but O
only O
on O
the O
development O
set O
. O
subsection O
: O
Qualitative Method
Analysis Method
In O
order O
to O
understand O
where O
the O
performance O
improvement O
comes O
from O
, O
we O
analyze O
the O
phrase O
pair O
scores O
computed O
by O
the O
RNN Method
Encoder O
– O
Decoder O
against O
the O
corresponding O
from O
the O
translation Method
model Method
. O
Since O
the O
existing O
translation Method
model Method
relies O
solely O
on O
the O
statistics O
of O
the O
phrase O
pairs O
in O
the O
corpus O
, O
we O
expect O
its O
scores O
to O
be O
better O
estimated O
for O
the O
frequent O
phrases O
but O
badly O
estimated O
for O
rare O
phrases O
. O
Also O
, O
as O
we O
mentioned O
earlier O
in O
Sec O
. O
[ O
reference O
] O
, O
we O
further O
expect O
the O
RNN Method
Encoder O
– O
Decoder O
which O
was O
trained O
without O
any O
frequency O
information O
to O
score O
the O
phrase O
pairs O
based O
rather O
on O
the O
linguistic O
regularities O
than O
on O
the O
statistics O
of O
their O
occurrences O
in O
the O
corpus O
. O
We O
focus O
on O
those O
pairs O
whose O
source O
phrase O
is O
long O
( O
more O
than O
3 O
words O
per O
source O
phrase O
) O
and O
frequent O
. O
For O
each O
such O
source O
phrase O
, O
we O
look O
at O
the O
target O
phrases O
that O
have O
been O
scored O
high O
either O
by O
the O
translation O
probability O
or O
by O
the O
RNN Method
Encoder O
– O
Decoder O
. O
Similarly O
, O
we O
perform O
the O
same O
procedure O
with O
those O
pairs O
whose O
source O
phrase O
is O
long O
but O
rare O
in O
the O
corpus O
. O
Table O
[ O
reference O
] O
lists O
the O
top O
- O
target O
phrases O
per O
source O
phrase O
favored O
either O
by O
the O
translation Method
model Method
or O
by O
the O
RNN Method
Encoder O
– O
Decoder O
. O
The O
source O
phrases O
were O
randomly O
chosen O
among O
long O
ones O
having O
more O
than O
4 O
or O
5 O
words O
. O
In O
most O
cases O
, O
the O
choices O
of O
the O
target O
phrases O
by O
the O
RNN Method
Encoder O
– O
Decoder O
are O
closer O
to O
actual O
or O
literal O
translations O
. O
We O
can O
observe O
that O
the O
RNN Method
Encoder O
– O
Decoder O
prefers O
shorter O
phrases O
in O
general O
. O
Interestingly O
, O
many O
phrase O
pairs O
were O
scored O
similarly O
by O
both O
the O
translation Method
model Method
and O
the O
RNN Method
Encoder O
– O
Decoder O
, O
but O
there O
were O
as O
many O
other O
phrase O
pairs O
that O
were O
scored O
radically O
different O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
. O
This O
could O
arise O
from O
the O
proposed O
approach O
of O
training O
the O
RNN Method
Encoder O
– O
Decoder O
on O
a O
set O
of O
unique O
phrase O
pairs O
, O
discouraging O
the O
RNN Method
Encoder O
– O
Decoder O
from O
learning O
simply O
the O
frequencies O
of O
the O
phrase O
pairs O
from O
the O
corpus O
, O
as O
explained O
earlier O
. O
Furthermore O
, O
in O
Table O
[ O
reference O
] O
, O
we O
show O
for O
each O
of O
the O
source O
phrases O
in O
Table O
[ O
reference O
] O
, O
the O
generated O
samples O
from O
the O
RNN Method
Encoder O
– O
Decoder O
. O
For O
each O
source O
phrase O
, O
we O
generated O
50 O
samples O
and O
show O
the O
top O
- O
five O
phrases O
accordingly O
to O
their O
scores O
. O
We O
can O
see O
that O
the O
RNN Method
Encoder O
– O
Decoder O
is O
able O
to O
propose O
well O
- O
formed O
target O
phrases O
without O
looking O
at O
the O
actual O
phrase O
table O
. O
Importantly O
, O
the O
generated O
phrases O
do O
not O
overlap O
completely O
with O
the O
target O
phrases O
from O
the O
phrase O
table O
. O
This O
encourages O
us O
to O
further O
investigate O
the O
possibility O
of O
replacing O
the O
whole O
or O
a O
part O
of O
the O
phrase O
table O
with O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
in O
the O
future O
. O
subsection O
: O
Word Method
and Method
Phrase Method
Representations Method
Since O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
is O
not O
specifically O
designed O
only O
for O
the O
task O
of O
machine Task
translation Task
, O
here O
we O
briefly O
look O
at O
the O
properties O
of O
the O
trained O
model O
. O
It O
has O
been O
known O
for O
some O
time O
that O
continuous Method
space Method
language Method
models Method
using O
neural Method
networks Method
are O
able O
to O
learn O
semantically Method
meaningful O
embeddings O
( O
See O
, O
e.g. O
, O
) O
. O
Since O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
also O
projects O
to O
and O
maps O
back O
from O
a O
sequence O
of O
words O
into O
a O
continuous O
space O
vector O
, O
we O
expect O
to O
see O
a O
similar O
property O
with O
the O
proposed O
model O
as O
well O
. O
The O
left O
plot O
in O
Fig O
. O
[ O
reference O
] O
shows O
the O
2–D O
embedding O
of O
the O
words O
using O
the O
word Method
embedding Method
matrix Method
learned O
by O
the O
RNN Method
Encoder O
– O
Decoder O
. O
The O
projection O
was O
done O
by O
the O
recently O
proposed O
Barnes Method
- Method
Hut Method
- Method
SNE Method
. O
We O
can O
clearly O
see O
that O
semantically Method
similar O
words O
are O
clustered O
with O
each O
other O
( O
see O
the O
zoomed O
- O
in O
plots O
in O
Fig O
. O
[ O
reference O
] O
) O
. O
The O
proposed O
RNN Method
Encoder O
– O
Decoder O
naturally O
generates O
a O
continuous Method
- Method
space Method
representation Method
of Method
a Method
phrase Method
. O
The O
representation O
( O
in O
Fig O
. O
[ O
reference O
] O
) O
in O
this O
case O
is O
a O
1000 O
- O
dimensional O
vector O
. O
Similarly O
to O
the O
word Method
representations Method
, O
we O
visualize O
the O
representations O
of O
the O
phrases O
that O
consists O
of O
four O
or O
more O
words O
using O
the O
Barnes Method
- Method
Hut Method
- Method
SNE Method
in O
Fig O
. O
[ O
reference O
] O
. O
From O
the O
visualization O
, O
it O
is O
clear O
that O
the O
RNN Method
Encoder O
– O
Decoder O
captures O
both O
semantic O
and O
syntactic O
structures O
of O
the O
phrases O
. O
For O
instance O
, O
in O
the O
bottom O
- O
left O
plot O
, O
most O
of O
the O
phrases O
are O
about O
the O
duration O
of O
time O
, O
while O
those O
phrases O
that O
are O
syntactically O
similar O
are O
clustered O
together O
. O
The O
bottom O
- O
right O
plot O
shows O
the O
cluster O
of O
phrases O
that O
are O
semantically Method
similar O
( O
countries O
or O
regions O
) O
. O
On O
the O
other O
hand O
, O
the O
top O
- O
right O
plot O
shows O
the O
phrases O
that O
are O
syntactically O
similar O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
a O
new O
neural Method
network Method
architecture Method
, O
called O
an O
RNN Method
Encoder O
– O
Decoder O
that O
is O
able O
to O
learn O
the O
mapping O
from O
a O
sequence O
of O
an O
arbitrary O
length O
to O
another O
sequence O
, O
possibly O
from O
a O
different O
set O
, O
of O
an O
arbitrary O
length O
. O
The O
proposed O
RNN Method
Encoder O
– O
Decoder O
is O
able O
to O
either O
score O
a O
pair O
of O
sequences O
( O
in O
terms O
of O
a O
conditional O
probability O
) O
or O
generate O
a O
target O
sequence O
given O
a O
source O
sequence O
. O
Along O
with O
the O
new O
architecture O
, O
we O
proposed O
a O
novel O
hidden Method
unit Method
that O
includes O
a O
reset O
gate O
and O
an O
update O
gate O
that O
adaptively O
control O
how O
much O
each O
hidden Method
unit Method
remembers O
or O
forgets O
while O
reading O
/ O
generating O
a O
sequence O
. O
We O
evaluated O
the O
proposed O
model O
with O
the O
task O
of O
statistical Task
machine Task
translation Task
, O
where O
we O
used O
the O
RNN Method
Encoder O
– O
Decoder O
to O
score O
each O
phrase O
pair O
in O
the O
phrase O
table O
. O
Qualitatively O
, O
we O
were O
able O
to O
show O
that O
the O
new O
model O
is O
able O
to O
capture O
linguistic O
regularities O
in O
the O
phrase O
pairs O
well O
and O
also O
that O
the O
RNN Method
Encoder O
– O
Decoder O
is O
able O
to O
propose O
well O
- O
formed O
target O
phrases O
. O
The O
scores O
by O
the O
RNN Method
Encoder O
– O
Decoder O
were O
found O
to O
improve O
the O
overall O
translation Metric
performance Metric
in O
terms O
of O
BLEU Metric
scores Metric
. O
Also O
, O
we O
found O
that O
the O
contribution O
by O
the O
RNN Method
Encoder O
– O
Decoder O
is O
rather O
orthogonal O
to O
the O
existing O
approach O
of O
using O
neural Method
networks Method
in O
the O
SMT Method
system Method
, O
so O
that O
we O
can O
improve O
further O
the O
performance O
by O
using O
, O
for O
instance O
, O
the O
RNN Method
Encoder O
– O
Decoder O
and O
the O
neural Method
net Method
language Method
model Method
together O
. O
Our O
qualitative O
analysis O
of O
the O
trained O
model O
shows O
that O
it O
indeed O
captures O
the O
linguistic O
regularities O
in O
multiple O
levels O
i.e. O
at O
the O
word O
level O
as O
well O
as O
phrase O
level O
. O
This O
suggests O
that O
there O
may O
be O
more O
natural Task
language Task
related Task
applications Task
that O
may O
benefit O
from O
the O
proposed O
RNN Method
Encoder O
– O
Decoder O
. O
The O
proposed O
architecture O
has O
large O
potential O
for O
further O
improvement O
and O
analysis O
. O
One O
approach O
that O
was O
not O
investigated O
here O
is O
to O
replace O
the O
whole O
, O
or O
a O
part O
of O
the O
phrase O
table O
by O
letting O
the O
RNN Method
Encoder O
– O
Decoder O
propose O
target O
phrases O
. O
Also O
, O
noting O
that O
the O
proposed O
model O
is O
not O
limited O
to O
being O
used O
with O
written O
language O
, O
it O
will O
be O
an O
important O
future O
research O
to O
apply O
the O
proposed O
architecture O
to O
other O
applications O
such O
as O
speech Task
transcription Task
. O
section O
: O
Acknowledgments O
KC O
, O
BM O
, O
CG O
, O
DB O
and O
YB O
would O
like O
to O
thank O
NSERC O
, O
Calcul O
Québec O
, O
Compute O
Canada O
, O
the O
Canada O
Research O
Chairs O
and O
CIFAR O
. O
FB O
and O
HS O
were O
partially O
funded O
by O
the O
European O
Commission O
under O
the O
project O
MateCat O
, O
and O
by O
DARPA O
under O
the O
BOLT O
project O
. O
bibliography O
: O
References O
appendix O
: O
RNN Method
Encoder O
– O
Decoder O
In O
this O
document O
, O
we O
describe O
in O
detail O
the O
architecture O
of O
the O
RNN Method
Encoder O
– O
Decoder O
used O
in O
the O
experiments O
. O
Let O
us O
denote O
an O
source O
phrase O
by O
and O
a O
target O
phrase O
by O
. O
Each O
phrase O
is O
a O
sequence O
of O
- O
dimensional O
one O
- O
hot O
vectors O
, O
such O
that O
only O
one O
element O
of O
the O
vector O
is O
and O
all O
the O
others O
are O
. O
The O
index O
of O
the O
active O
( O
) O
element O
indicates O
the O
word O
represented O
by O
the O
vector O
. O
subsection O
: O
Encoder O
Each O
word O
of O
the O
source O
phrase O
is O
embedded O
in O
a O
- O
dimensional O
vector O
space O
: O
. O
is O
used O
in O
Sec O
. O
[ O
reference O
] O
to O
visualize O
the O
words O
. O
The O
hidden O
state O
of O
an O
encoder Method
consists O
of O
hidden O
units O
, O
and O
each O
one O
of O
them O
at O
time O
is O
computed O
by O
where O
and O
are O
a O
logistic Method
sigmoid Method
function Method
and O
an O
element Method
- Method
wise Method
multiplication Method
, O
respectively O
. O
To O
make O
the O
equations O
uncluttered O
, O
we O
omit O
biases O
. O
The O
initial O
hidden O
state O
is O
fixed O
to O
. O
Once O
the O
hidden O
state O
at O
the O
step O
( O
the O
end O
of O
the O
source O
phrase O
) O
is O
computed O
, O
the O
representation O
of O
the O
source O
phrase O
is O
subsubsection O
: O
Decoder Method
The O
decoder O
starts O
by O
initializing O
the O
hidden O
state O
with O
where O
we O
will O
use O
to O
distinguish O
parameters O
of O
the O
decoder O
from O
those O
of O
the O
encoder Method
. O
The O
hidden O
state O
at O
time O
of O
the O
decoder O
is O
computed O
by O
where O
and O
is O
an O
all O
- O
zero O
vector O
. O
Similarly O
to O
the O
case O
of O
the O
encoder Method
, O
is O
an O
embedding O
of O
a O
target O
word O
. O
Unlike O
the O
encoder Method
which O
simply O
encodes O
the O
source O
phrase O
, O
the O
decoder Method
is O
learned O
to O
generate O
a O
target O
phrase O
. O
At O
each O
time O
, O
the O
decoder Method
computes O
the O
probability O
of O
generating O
- O
th O
word O
by O
where O
the O
- O
element O
of O
is O
and O
In O
short O
, O
the O
is O
a O
so O
- O
called O
maxout O
unit O
. O
For O
the O
computational Metric
efficiency Metric
, O
instead O
of O
a O
single O
- O
matrix O
output O
weight O
, O
we O
use O
a O
product O
of O
two O
matrices O
such O
that O
where O
and O
. O
appendix O
: O
Word Method
and Method
Phrase Method
Representations Method
Here O
, O
we O
show O
enlarged O
plots O
of O
the O
word Method
and Method
phrase Method
representations Method
in O
Figs O
. O
[ O
reference O
] O
– O
[ O
reference O
] O
. O
arxiv O
arxiv O
