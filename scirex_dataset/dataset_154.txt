document	O
:	O
Identity	Task
Mappings	Task
in	O
Deep	Method
Residual	Method
Networks	Method
Deep	Method
residual	Method
networks	Method
have	O
emerged	O
as	O
a	O
family	O
of	O
extremely	O
deep	Method
architectures	Method
showing	O
compelling	O
accuracy	Metric
and	O
nice	O
convergence	Metric
behaviors	Metric
.	O
In	O
this	O
paper	O
,	O
we	O
analyze	O
the	O
propagation	Method
formulations	Method
behind	O
the	O
residual	O
building	O
blocks	O
,	O
which	O
suggest	O
that	O
the	O
forward	O
and	O
backward	O
signals	O
can	O
be	O
directly	O
propagated	O
from	O
one	O
block	O
to	O
any	O
other	O
block	O
,	O
when	O
using	O
identity	O
mappings	O
as	O
the	O
skip	O
connections	O
and	O
after	O
-	O
addition	O
activation	O
.	O
A	O
series	O
of	O
ablation	O
experiments	O
support	O
the	O
importance	O
of	O
these	O
identity	Method
mappings	Method
.	O
This	O
motivates	O
us	O
to	O
propose	O
a	O
new	O
residual	Method
unit	Method
,	O
which	O
makes	O
training	Task
easier	O
and	O
improves	O
generalization	Task
.	O
We	O
report	O
improved	O
results	O
using	O
a	O
1001	Method
-	Method
layer	Method
ResNet	Method
on	O
CIFAR	Material
-	Material
10	Material
(	O
4.62	O
%	O
error	Metric
)	O
and	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
a	O
200	Method
-	Method
layer	Method
ResNet	Method
on	O
ImageNet	Material
.	O
Code	O
is	O
available	O
at	O
:	O
.	O
section	O
:	O
Introduction	O
Deep	Method
residual	Method
networks	Method
(	O
ResNets	Method
)	O
consist	O
of	O
many	O
stacked	O
“	O
Residual	Method
Units	Method
”	O
.	O
Each	O
unit	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
a	O
)	O
)	O
can	O
be	O
expressed	O
in	O
a	O
general	O
form	O
:	O
where	O
and	O
are	O
input	O
and	O
output	O
of	O
the	O
-	O
th	O
unit	O
,	O
and	O
is	O
a	O
residual	O
function	O
.	O
In	O
,	O
is	O
an	O
identity	Method
mapping	Method
and	O
is	O
a	O
ReLU	Method
function	O
.	O
ResNets	Method
that	O
are	O
over	O
100	Material
-	O
layer	O
deep	O
have	O
shown	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
for	O
several	O
challenging	O
recognition	Task
tasks	Task
on	O
ImageNet	Material
and	O
MS	Material
COCO	Material
competitions	O
.	O
The	O
central	O
idea	O
of	O
ResNets	Method
is	O
to	O
learn	O
the	O
additive	O
residual	O
function	O
with	O
respect	O
to	O
,	O
with	O
a	O
key	O
choice	O
of	O
using	O
an	O
identity	Method
mapping	Method
.	O
This	O
is	O
realized	O
by	O
attaching	O
an	O
identity	O
skip	O
connection	O
(	O
“	O
shortcut	O
”	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
analyze	O
deep	Method
residual	Method
networks	Method
by	O
focusing	O
on	O
creating	O
a	O
“	O
direct	O
”	O
path	O
for	O
propagating	O
information	O
—	O
not	O
only	O
within	O
a	O
residual	O
unit	O
,	O
but	O
through	O
the	O
entire	O
network	O
.	O
Our	O
derivations	O
reveal	O
that	O
if	O
both	O
⁢h	O
(	O
xl	O
)	O
and	O
⁢f	O
(	O
yl	O
)	O
are	O
identity	O
mappings	O
,	O
the	O
signal	O
could	O
be	O
directly	O
propagated	O
from	O
one	O
unit	O
to	O
any	O
other	O
units	O
,	O
in	O
both	O
forward	O
and	O
backward	O
passes	O
.	O
Our	O
experiments	O
empirically	O
show	O
that	O
training	Task
in	O
general	O
becomes	O
easier	O
when	O
the	O
architecture	O
is	O
closer	O
to	O
the	O
above	O
two	O
conditions	O
.	O
To	O
understand	O
the	O
role	O
of	O
skip	O
connections	O
,	O
we	O
analyze	O
and	O
compare	O
various	O
types	O
of	O
.	O
We	O
find	O
that	O
the	O
identity	Method
mapping	Method
chosen	O
in	O
achieves	O
the	O
fastest	O
error	Metric
reduction	O
and	O
lowest	O
training	Metric
loss	Metric
among	O
all	O
variants	O
we	O
investigated	O
,	O
whereas	O
skip	O
connections	O
of	O
scaling	Method
,	O
gating	Method
,	O
and	O
1	O
1	Method
convolutions	Method
all	O
lead	O
to	O
higher	O
training	Metric
loss	Metric
and	O
error	Metric
.	O
These	O
experiments	O
suggest	O
that	O
keeping	O
a	O
“	O
clean	O
”	O
information	O
path	O
(	O
indicated	O
by	O
the	O
grey	O
arrows	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
)	O
is	O
helpful	O
for	O
easing	Task
optimization	Task
.	O
To	O
construct	O
an	O
identity	Task
mapping	Task
,	O
we	O
view	O
the	O
activation	O
functions	O
(	O
ReLU	Method
and	O
BN	Method
)	O
as	O
“	O
pre	O
-	O
activation	O
”	O
of	O
the	O
weight	O
layers	O
,	O
in	O
contrast	O
to	O
conventional	O
wisdom	O
of	O
“	O
post	O
-	O
activation	O
”	O
.	O
This	O
point	O
of	O
view	O
leads	O
to	O
a	O
new	O
residual	Method
unit	Method
design	Method
,	O
shown	O
in	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
)	O
)	O
.	O
Based	O
on	O
this	O
unit	O
,	O
we	O
present	O
competitive	O
results	O
on	O
CIFAR	Material
-	Material
10	Material
/	O
100	Material
with	O
a	O
1001	Method
-	Method
layer	Method
ResNet	Method
,	O
which	O
is	O
much	O
easier	O
to	O
train	O
and	O
generalizes	O
better	O
than	O
the	O
original	O
ResNet	Method
in	O
.	O
We	O
further	O
report	O
improved	O
results	O
on	O
ImageNet	Material
using	O
a	O
200	Method
-	Method
layer	Method
ResNet	Method
,	O
for	O
which	O
the	O
counterpart	O
of	O
starts	O
to	O
overfit	O
.	O
These	O
results	O
suggest	O
that	O
there	O
is	O
much	O
room	O
to	O
exploit	O
the	O
dimension	O
of	O
network	O
depth	O
,	O
a	O
key	O
to	O
the	O
success	O
of	O
modern	O
deep	Method
learning	Method
.	O
section	O
:	O
Analysis	O
of	O
Deep	Method
Residual	Method
Networks	Method
The	O
ResNets	Method
developed	O
in	O
are	O
modularized	Method
architectures	Method
that	O
stack	O
building	O
blocks	O
of	O
the	O
same	O
connecting	O
shape	O
.	O
In	O
this	O
paper	O
we	O
call	O
these	O
blocks	O
“	O
Residual	O
Units	O
”	O
.	O
The	O
original	O
Residual	Method
Unit	Method
in	O
performs	O
the	O
following	O
computation	O
:	O
Here	O
is	O
the	O
input	O
feature	O
to	O
the	O
-	Method
th	Method
Residual	Method
Unit	Method
.	O
is	O
a	O
set	O
of	O
weights	O
(	O
and	O
biases	O
)	O
associated	O
with	O
the	O
-	O
th	O
Residual	Method
Unit	Method
,	O
and	O
is	O
the	O
number	O
of	O
layers	O
in	O
a	O
Residual	O
Unit	O
(	O
is	O
2	O
or	O
3	O
in	O
)	O
.	O
denotes	O
the	O
residual	O
function	O
,	O
e.g.	O
,	O
a	O
stack	O
of	O
two	O
3	O
3	O
convolutional	Method
layers	Method
in	O
.	O
The	O
function	O
is	O
the	O
operation	O
after	O
element	O
-	O
wise	O
addition	O
,	O
and	O
in	O
is	O
ReLU	Method
.	O
The	O
function	O
is	O
set	O
as	O
an	O
identity	Method
mapping	Method
:	O
.	O
If	O
is	O
also	O
an	O
identity	O
mapping	O
:	O
,	O
we	O
can	O
put	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
into	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
obtain	O
:	O
Recursively	O
(	O
,	O
etc	O
.	O
)	O
we	O
will	O
have	O
:	O
for	O
any	O
deeper	O
unit	O
and	O
any	O
shallower	O
unit	O
.	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
exhibits	O
some	O
nice	O
properties	O
.	O
(	O
i	O
)	O
The	O
feature	O
of	O
any	O
deeper	O
unit	O
can	O
be	O
represented	O
as	O
the	O
feature	O
of	O
any	O
shallower	O
unit	O
plus	O
a	O
residual	O
function	O
in	O
a	O
form	O
of	O
,	O
indicating	O
that	O
the	O
model	O
is	O
in	O
a	O
residual	O
fashion	O
between	O
any	O
units	O
and	O
.	O
(	O
ii	O
)	O
The	O
feature	O
,	O
of	O
any	O
deep	Method
unit	Method
,	O
is	O
the	O
summation	O
of	O
the	O
outputs	O
of	O
all	O
preceding	O
residual	O
functions	O
(	O
plus	O
)	O
.	O
This	O
is	O
in	O
contrast	O
to	O
a	O
“	O
plain	Method
network	Method
”	O
where	O
a	O
feature	O
is	O
a	O
series	O
of	O
matrix	O
-	O
vector	O
products	O
,	O
say	O
,	O
(	O
ignoring	O
BN	Method
and	O
ReLU	Method
)	O
.	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
also	O
leads	O
to	O
nice	O
backward	O
propagation	O
properties	O
.	O
Denoting	O
the	O
loss	O
function	O
as	O
,	O
from	O
the	O
chain	Method
rule	Method
of	Method
backpropagation	Method
we	O
have	O
:	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
indicates	O
that	O
the	O
gradient	O
can	O
be	O
decomposed	O
into	O
two	O
additive	Method
terms	Method
:	O
a	O
term	O
of	O
that	O
propagates	O
information	O
directly	O
without	O
concerning	O
any	O
weight	O
layers	O
,	O
and	O
another	O
term	O
of	O
that	O
propagates	O
through	O
the	O
weight	O
layers	O
.	O
The	O
additive	O
term	O
of	O
ensures	O
that	O
information	O
is	O
directly	O
propagated	O
back	O
to	O
any	O
shallower	O
unit	O
.	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
also	O
suggests	O
that	O
it	O
is	O
unlikely	O
for	O
the	O
gradient	O
to	O
be	O
canceled	O
out	O
for	O
a	O
mini	O
-	O
batch	O
,	O
because	O
in	O
general	O
the	O
term	O
can	O
not	O
be	O
always	O
-	O
1	O
for	O
all	O
samples	O
in	O
a	O
mini	O
-	O
batch	O
.	O
This	O
implies	O
that	O
the	O
gradient	O
of	O
a	O
layer	O
does	O
not	O
vanish	O
even	O
when	O
the	O
weights	O
are	O
arbitrarily	O
small	O
.	O
subsection	O
:	O
Discussions	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
suggest	O
that	O
the	O
signal	O
can	O
be	O
directly	O
propagated	O
from	O
any	O
unit	O
to	O
another	O
,	O
both	O
forward	O
and	O
backward	O
.	O
The	O
foundation	O
of	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
two	O
identity	Method
mappings	Method
:	O
(	O
i	O
)	O
the	O
identity	O
skip	O
connection	O
,	O
and	O
(	O
ii	O
)	O
the	O
condition	O
that	O
is	O
an	O
identity	O
mapping	O
.	O
These	O
directly	O
propagated	O
information	O
flows	O
are	O
represented	O
by	O
the	O
grey	O
arrows	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
.	O
And	O
the	O
above	O
two	O
conditions	O
are	O
true	O
when	O
these	O
grey	O
arrows	O
cover	O
no	O
operations	O
(	O
expect	O
addition	O
)	O
and	O
thus	O
are	O
“	O
clean	O
”	O
.	O
In	O
the	O
following	O
two	O
sections	O
we	O
separately	O
investigate	O
the	O
impacts	O
of	O
the	O
two	O
conditions	O
.	O
section	O
:	O
On	O
the	O
Importance	O
of	O
Identity	O
Skip	O
Connections	O
Let	O
’s	O
consider	O
a	O
simple	O
modification	O
,	O
,	O
to	O
break	O
the	O
identity	O
shortcut	O
:	O
where	O
is	O
a	O
modulating	O
scalar	O
(	O
for	O
simplicity	O
we	O
still	O
assume	O
is	O
identity	O
)	O
.	O
Recursively	O
applying	O
this	O
formulation	O
we	O
obtain	O
an	O
equation	O
similar	O
to	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
:	O
,	O
or	O
simply	O
:	O
where	O
the	O
notation	O
absorbs	O
the	O
scalars	O
into	O
the	O
residual	O
functions	O
.	O
Similar	O
to	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
have	O
backpropagation	Method
of	O
the	O
following	O
form	O
:	O
Unlike	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
the	O
first	O
additive	O
term	O
is	O
modulated	O
by	O
a	O
factor	O
.	O
For	O
an	O
extremely	O
deep	Task
network	Task
(	O
is	O
large	O
)	O
,	O
if	O
for	O
all	O
,	O
this	O
factor	O
can	O
be	O
exponentially	O
large	O
;	O
if	O
for	O
all	O
,	O
this	O
factor	O
can	O
be	O
exponentially	O
small	O
and	O
vanish	O
,	O
which	O
blocks	O
the	O
backpropagated	O
signal	O
from	O
the	O
shortcut	O
and	O
forces	O
it	O
to	O
flow	O
through	O
the	O
weight	O
layers	O
.	O
This	O
results	O
in	O
optimization	Task
difficulties	O
as	O
we	O
show	O
by	O
experiments	O
.	O
In	O
the	O
above	O
analysis	O
,	O
the	O
original	O
identity	O
skip	O
connection	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
replaced	O
with	O
a	O
simple	O
scaling	Method
.	O
If	O
the	O
skip	O
connection	O
represents	O
more	O
complicated	O
transforms	O
(	O
such	O
as	O
gating	O
and	O
1	O
1	O
convolutions	O
)	O
,	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
the	O
first	O
term	O
becomes	O
where	O
is	O
the	O
derivative	O
of	O
.	O
This	O
product	O
may	O
also	O
impede	O
information	Task
propagation	Task
and	O
hamper	O
the	O
training	Method
procedure	Method
as	O
witnessed	O
in	O
the	O
following	O
experiments	O
.	O
subsection	O
:	O
Experiments	O
on	O
Skip	Task
Connections	Task
We	O
experiment	O
with	O
the	O
110	Method
-	Method
layer	Method
ResNet	Method
as	O
presented	O
in	O
on	O
CIFAR	Material
-	Material
10	Material
.	O
This	O
extremely	O
deep	O
ResNet	Method
-	Method
110	Method
has	O
54	O
two	O
-	O
layer	O
Residual	O
Units	O
(	O
consisting	O
of	O
3	O
3	O
convolutional	O
layers	O
)	O
and	O
is	O
challenging	O
for	O
optimization	Task
.	O
Our	O
implementation	O
details	O
(	O
see	O
appendix	O
)	O
are	O
the	O
same	O
as	O
.	O
Throughout	O
this	O
paper	O
we	O
report	O
the	O
median	Metric
accuracy	Metric
of	O
5	O
runs	O
for	O
each	O
architecture	O
on	O
CIFAR	Material
,	O
reducing	O
the	O
impacts	O
of	O
random	O
variations	O
.	O
Though	O
our	O
above	O
analysis	O
is	O
driven	O
by	O
identity	O
,	O
the	O
experiments	O
in	O
this	O
section	O
are	O
all	O
based	O
on	O
ReLU	Method
as	O
in	O
;	O
we	O
address	O
identity	O
in	O
the	O
next	O
section	O
.	O
Our	O
baseline	O
ResNet	Method
-	Method
110	Method
has	O
6.61	O
%	O
error	Metric
on	O
the	O
test	O
set	O
.	O
The	O
comparisons	O
of	O
other	O
variants	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
)	O
are	O
summarized	O
as	O
follows	O
:	O
Constant	Method
scaling	Method
.	O
We	O
set	O
for	O
all	O
shortcuts	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
)	O
)	O
.	O
We	O
further	O
study	O
two	O
cases	O
of	O
scaling	Task
:	O
(	O
i	O
)	O
is	O
not	O
scaled	O
;	O
or	O
(	O
ii	O
)	O
is	O
scaled	O
by	O
a	O
constant	O
scalar	O
of	O
,	O
which	O
is	O
similar	O
to	O
the	O
highway	Method
gating	Method
but	O
with	O
frozen	O
gates	O
.	O
The	O
former	O
case	O
does	O
not	O
converge	O
well	O
;	O
the	O
latter	O
is	O
able	O
to	O
converge	O
,	O
but	O
the	O
test	O
error	Metric
(	O
Table	O
[	O
reference	O
]	O
,	O
12.35	O
%	O
)	O
is	O
substantially	O
higher	O
than	O
the	O
original	O
ResNet	Method
-	Method
110	Method
.	O
Fig	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
that	O
the	O
training	O
error	Metric
is	O
higher	O
than	O
that	O
of	O
the	O
original	O
ResNet	Method
-	O
110	O
,	O
suggesting	O
that	O
the	O
optimization	Task
has	O
difficulties	O
when	O
the	O
shortcut	O
signal	O
is	O
scaled	O
down	O
.	O
Exclusive	Method
gating	Method
.	O
Following	O
the	O
Highway	Method
Networks	Method
that	O
adopt	O
a	O
gating	Method
mechanism	Method
,	O
we	O
consider	O
a	O
gating	Method
function	Method
where	O
a	O
transform	O
is	O
represented	O
by	O
weights	O
and	O
biases	O
followed	O
by	O
the	O
sigmoid	O
function	O
.	O
In	O
a	O
convolutional	Method
network	Method
is	O
realized	O
by	O
a	O
1	Method
1	Method
convolutional	Method
layer	Method
.	O
The	O
gating	Method
function	Method
modulates	O
the	O
signal	O
by	O
element	Method
-	Method
wise	Method
multiplication	Method
.	O
We	O
investigate	O
the	O
“	O
exclusive	O
”	O
gates	O
as	O
used	O
in	O
—	O
the	O
path	O
is	O
scaled	O
by	O
and	O
the	O
shortcut	O
path	O
is	O
scaled	O
by	O
.	O
See	O
Fig	O
[	O
reference	O
]	O
(	O
c	O
)	O
.	O
We	O
find	O
that	O
the	O
initialization	O
of	O
the	O
biases	O
is	O
critical	O
for	O
training	O
gated	Method
models	Method
,	O
and	O
following	O
the	O
guidelines	O
in	O
,	O
we	O
conduct	O
hyper	Method
-	Method
parameter	Method
search	Method
on	O
the	O
initial	O
value	O
of	O
in	O
the	O
range	O
of	O
0	O
to	O
-	O
10	O
with	O
a	O
decrement	O
step	O
of	O
-	O
1	O
on	O
the	O
training	O
set	O
by	O
cross	Method
-	Method
validation	Method
.	O
The	O
best	O
value	O
(	O
here	O
)	O
is	O
then	O
used	O
for	O
training	O
on	O
the	O
training	O
set	O
,	O
leading	O
to	O
a	O
test	O
result	O
of	O
8.70	O
%	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
which	O
still	O
lags	O
far	O
behind	O
the	O
ResNet	Method
-	O
110	O
baseline	O
.	O
Fig	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
the	O
training	O
curves	O
.	O
Table	O
[	O
reference	O
]	O
also	O
reports	O
the	O
results	O
of	O
using	O
other	O
initialized	O
values	O
,	O
noting	O
that	O
the	O
exclusive	Method
gating	Method
network	Method
does	O
not	O
converge	O
to	O
a	O
good	O
solution	O
when	O
is	O
not	O
appropriately	O
initialized	O
.	O
The	O
impact	O
of	O
the	O
exclusive	Method
gating	Method
mechanism	Method
is	O
two	O
-	O
fold	O
.	O
When	O
approaches	O
1	O
,	O
the	O
gated	O
shortcut	O
connections	O
are	O
closer	O
to	O
identity	O
which	O
helps	O
information	Task
propagation	Task
;	O
but	O
in	O
this	O
case	O
approaches	O
0	O
and	O
suppresses	O
the	O
function	O
.	O
To	O
isolate	O
the	O
effects	O
of	O
the	O
gating	O
functions	O
on	O
the	O
shortcut	O
path	O
alone	O
,	O
we	O
investigate	O
a	O
non	Method
-	Method
exclusive	Method
gating	Method
mechanism	Method
in	O
the	O
next	O
.	O
Shortcut	Method
-	Method
only	Method
gating	Method
.	O
In	O
this	O
case	O
the	O
function	O
is	O
not	O
scaled	O
;	O
only	O
the	O
shortcut	O
path	O
is	O
gated	O
by	O
.	O
See	O
Fig	O
[	O
reference	O
]	O
(	O
d	O
)	O
.	O
The	O
initialized	O
value	O
of	O
is	O
still	O
essential	O
in	O
this	O
case	O
.	O
When	O
the	O
initialized	O
is	O
0	O
(	O
so	O
initially	O
the	O
expectation	O
of	O
is	O
0.5	O
)	O
,	O
the	O
network	O
converges	O
to	O
a	O
poor	O
result	O
of	O
12.86	O
%	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
This	O
is	O
also	O
caused	O
by	O
higher	O
training	O
error	Metric
(	O
Fig	O
[	O
reference	O
]	O
(	O
c	O
)	O
)	O
.	O
When	O
the	O
initialized	O
is	O
very	O
negatively	O
biased	O
(	O
e.g.	O
,	O
)	O
,	O
the	O
value	O
of	O
is	O
closer	O
to	O
1	O
and	O
the	O
shortcut	O
connection	O
is	O
nearly	O
an	O
identity	Method
mapping	Method
.	O
Therefore	O
,	O
the	O
result	O
(	O
6.91	O
%	O
,	O
Table	O
[	O
reference	O
]	O
)	O
is	O
much	O
closer	O
to	O
the	O
ResNet	Method
-	O
110	O
baseline	O
.	O
1×1	O
convolutional	Method
shortcut	Method
.	O
Next	O
we	O
experiment	O
with	O
1	O
1	O
convolutional	Method
shortcut	Method
connections	Method
that	O
replace	O
the	O
identity	O
.	O
This	O
option	O
has	O
been	O
investigated	O
in	O
(	O
known	O
as	O
option	O
C	O
)	O
on	O
a	O
34	O
-	O
layer	O
ResNet	Method
(	O
16	O
Residual	O
Units	O
)	O
and	O
shows	O
good	O
results	O
,	O
suggesting	O
that	O
1	O
1	O
shortcut	O
connections	O
could	O
be	O
useful	O
.	O
But	O
we	O
find	O
that	O
this	O
is	O
not	O
the	O
case	O
when	O
there	O
are	O
many	O
Residual	O
Units	O
.	O
The	O
110	Method
-	Method
layer	Method
ResNet	Method
has	O
a	O
poorer	O
result	O
(	O
12.22	O
%	O
,	O
Table	O
[	O
reference	O
]	O
)	O
when	O
using	O
1	O
1	O
convolutional	Method
shortcuts	Method
.	O
Again	O
,	O
the	O
training	O
error	Metric
becomes	O
higher	O
(	O
Fig	O
[	O
reference	O
]	O
(	O
d	O
)	O
)	O
.	O
When	O
stacking	O
so	O
many	O
Residual	O
Units	O
(	O
54	O
for	O
ResNet	Method
-	Method
110	Method
)	O
,	O
even	O
the	O
shortest	O
path	O
may	O
still	O
impede	O
signal	Task
propagation	Task
.	O
We	O
witnessed	O
similar	O
phenomena	O
on	O
ImageNet	Material
with	O
ResNet	Method
-	Method
101	Method
when	O
using	O
1	O
1	O
convolutional	Method
shortcuts	Method
.	O
Dropout	Method
shortcut	Method
.	O
Last	O
we	O
experiment	O
with	O
dropout	Method
(	O
at	O
a	O
ratio	O
of	O
0.5	O
)	O
which	O
we	O
adopt	O
on	O
the	O
output	O
of	O
the	O
identity	O
shortcut	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
f	O
)	O
)	O
.	O
The	O
network	O
fails	O
to	O
converge	O
to	O
a	O
good	O
solution	O
.	O
Dropout	Method
statistically	O
imposes	O
a	O
scale	O
of	O
with	O
an	O
expectation	O
of	O
0.5	O
on	O
the	O
shortcut	O
,	O
and	O
similar	O
to	O
constant	O
scaling	O
by	O
0.5	O
,	O
it	O
impedes	O
signal	Task
propagation	Task
.	O
subsection	O
:	O
Discussions	O
As	O
indicated	O
by	O
the	O
grey	O
arrows	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
the	O
shortcut	O
connections	O
are	O
the	O
most	O
direct	O
paths	O
for	O
the	O
information	O
to	O
propagate	O
.	O
Multiplicative	O
manipulations	O
(	O
scaling	O
,	O
gating	O
,	O
1	O
1	O
convolutions	O
,	O
and	O
dropout	Method
)	O
on	O
the	O
shortcuts	O
can	O
hamper	O
information	Task
propagation	Task
and	O
lead	O
to	O
optimization	Task
problems	Task
.	O
It	O
is	O
noteworthy	O
that	O
the	O
gating	Method
and	O
1	O
1	O
convolutional	Method
shortcuts	Method
introduce	O
more	O
parameters	O
,	O
and	O
should	O
have	O
stronger	O
representational	O
abilities	O
than	O
identity	O
shortcuts	O
.	O
In	O
fact	O
,	O
the	O
shortcut	Method
-	Method
only	Method
gating	Method
and	O
1	O
1	O
convolution	Method
cover	O
the	O
solution	O
space	O
of	O
identity	O
shortcuts	O
(	O
i.e.	O
,	O
they	O
could	O
be	O
optimized	O
as	O
identity	O
shortcuts	O
)	O
.	O
However	O
,	O
their	O
training	O
error	Metric
is	O
higher	O
than	O
that	O
of	O
identity	O
shortcuts	O
,	O
indicating	O
that	O
the	O
degradation	O
of	O
these	O
models	O
is	O
caused	O
by	O
optimization	Task
issues	Task
,	O
instead	O
of	O
representational	O
abilities	O
.	O
section	O
:	O
On	O
the	O
Usage	O
of	O
Activation	O
Functions	O
Experiments	O
in	O
the	O
above	O
section	O
support	O
the	O
analysis	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
both	O
being	O
derived	O
under	O
the	O
assumption	O
that	O
the	O
after	Method
-	Method
addition	Method
activation	Method
is	O
the	O
identity	Method
mapping	Method
.	O
But	O
in	O
the	O
above	O
experiments	O
is	O
ReLU	Method
as	O
designed	O
in	O
,	O
so	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
are	O
approximate	O
in	O
the	O
above	O
experiments	O
.	O
Next	O
we	O
investigate	O
the	O
impact	O
of	O
.	O
We	O
want	O
to	O
make	O
an	O
identity	Task
mapping	Task
,	O
which	O
is	O
done	O
by	O
re	O
-	O
arranging	O
the	O
activation	O
functions	O
(	O
ReLU	Method
and	O
/	O
or	O
BN	Method
)	O
.	O
The	O
original	O
Residual	O
Unit	O
in	O
has	O
a	O
shape	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
a	O
)	O
—	O
BN	Method
is	O
used	O
after	O
each	O
weight	Method
layer	Method
,	O
and	O
ReLU	Method
is	O
adopted	O
after	O
BN	Method
except	O
that	O
the	O
last	O
ReLU	Method
in	O
a	O
Residual	O
Unit	O
is	O
after	O
element	O
-	O
wise	O
addition	O
(	O
ReLU	Method
)	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
-	O
e	O
)	O
show	O
the	O
alternatives	O
we	O
investigated	O
,	O
explained	O
as	O
following	O
.	O
subsection	O
:	O
Experiments	O
on	O
Activation	Task
In	O
this	O
section	O
we	O
experiment	O
with	O
ResNet	Method
-	Method
110	Method
and	O
a	O
164	Method
-	Method
layer	Method
Bottleneck	Method
architecture	Method
(	O
denoted	O
as	O
ResNet	Method
-	Method
164	Method
)	O
.	O
A	O
bottleneck	Method
Residual	Method
Unit	Method
consist	O
of	O
a	O
1	O
1	O
layer	O
for	O
reducing	O
dimension	O
,	O
a	O
3	O
3	O
layer	O
,	O
and	O
a	O
1	O
1	O
layer	O
for	O
restoring	O
dimension	O
.	O
As	O
designed	O
in	O
,	O
its	O
computational	Metric
complexity	Metric
is	O
similar	O
to	O
the	O
two	O
-	O
3	O
3	Method
Residual	Method
Unit	Method
.	O
More	O
details	O
are	O
in	O
the	O
appendix	O
.	O
The	O
baseline	O
ResNet	Method
-	Method
164	Method
has	O
a	O
competitive	O
result	O
of	O
5.93	O
%	O
on	O
CIFAR	Material
-	Material
10	Material
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
BN	Method
after	O
addition	O
.	O
Before	O
turning	O
into	O
an	O
identity	Task
mapping	Task
,	O
we	O
go	O
the	O
opposite	O
way	O
by	O
adopting	O
BN	Method
after	O
addition	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
)	O
)	O
.	O
In	O
this	O
case	O
involves	O
BN	Method
and	O
ReLU	Method
.	O
The	O
results	O
become	O
considerably	O
worse	O
than	O
the	O
baseline	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
Unlike	O
the	O
original	O
design	O
,	O
now	O
the	O
BN	Method
layer	O
alters	O
the	O
signal	O
that	O
passes	O
through	O
the	O
shortcut	O
and	O
impedes	O
information	Task
propagation	Task
,	O
as	O
reflected	O
by	O
the	O
difficulties	O
on	O
reducing	O
training	Metric
loss	Metric
at	O
the	O
beginning	O
of	O
training	O
(	O
Fib	O
.	O
[	O
reference	O
]	O
left	O
)	O
.	O
ReLU	Method
before	O
addition	O
.	O
A	O
naïve	O
choice	O
of	O
making	O
into	O
an	O
identity	Task
mapping	Task
is	O
to	O
move	O
the	O
ReLU	Method
before	O
addition	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
c	O
)	O
)	O
.	O
However	O
,	O
this	O
leads	O
to	O
a	O
non	O
-	O
negative	O
output	O
from	O
the	O
transform	O
,	O
while	O
intuitively	O
a	O
“	O
residual	O
”	O
function	O
should	O
take	O
values	O
in	O
.	O
As	O
a	O
result	O
,	O
the	O
forward	O
propagated	O
signal	O
is	O
monotonically	O
increasing	O
.	O
This	O
may	O
impact	O
the	O
representational	O
ability	O
,	O
and	O
the	O
result	O
is	O
worse	O
(	O
7.84	O
%	O
,	O
Table	O
[	O
reference	O
]	O
)	O
than	O
the	O
baseline	O
.	O
We	O
expect	O
to	O
have	O
a	O
residual	O
function	O
taking	O
values	O
in	O
.	O
This	O
condition	O
is	O
satisfied	O
by	O
other	O
Residual	Method
Units	Method
including	O
the	O
following	O
ones	O
.	O
Post	O
-	O
activation	O
or	O
pre	O
-	O
activation	O
?	O
In	O
the	O
original	O
design	O
(	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
and	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
)	O
,	O
the	O
activation	O
affects	O
both	O
paths	O
in	O
the	O
next	O
Residual	O
Unit	O
:	O
.	O
Next	O
we	O
develop	O
an	O
asymmetric	O
form	O
where	O
an	O
activation	O
only	O
affects	O
the	O
path	O
:	O
,	O
for	O
any	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
a	O
)	O
to	O
(	O
b	O
)	O
)	O
.	O
By	O
renaming	O
the	O
notations	O
,	O
we	O
have	O
the	O
following	O
form	O
:	O
It	O
is	O
easy	O
to	O
see	O
that	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
similar	O
to	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
can	O
enable	O
a	O
backward	Method
formulation	Method
similar	O
to	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
For	O
this	O
new	O
Residual	O
Unit	O
as	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
new	O
after	Method
-	Method
addition	Method
activation	Method
becomes	O
an	O
identity	Method
mapping	Method
.	O
This	O
design	O
means	O
that	O
if	O
a	O
new	O
after	Method
-	Method
addition	Method
activation	Method
is	O
asymmetrically	O
adopted	O
,	O
it	O
is	O
equivalent	O
to	O
recasting	O
as	O
the	O
pre	O
-	O
activation	O
of	O
the	O
next	O
Residual	O
Unit	O
.	O
This	O
is	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
The	O
distinction	O
between	O
post	O
-	O
activation	O
/	O
pre	O
-	O
activation	O
is	O
caused	O
by	O
the	O
presence	O
of	O
the	O
element	Method
-	Method
wise	Method
addition	Method
.	O
For	O
a	O
plain	Method
network	Method
that	O
has	O
layers	O
,	O
there	O
are	O
activations	O
(	O
BN	Method
/	O
ReLU	Method
)	O
,	O
and	O
it	O
does	O
not	O
matter	O
whether	O
we	O
think	O
of	O
them	O
as	O
post	O
-	O
or	O
pre	O
-	O
activations	O
.	O
But	O
for	O
branched	O
layers	O
merged	O
by	O
addition	O
,	O
the	O
position	O
of	O
activation	O
matters	O
.	O
We	O
experiment	O
with	O
two	O
such	O
designs	O
:	O
(	O
i	O
)	O
ReLU	Method
-	O
only	O
pre	O
-	O
activation	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
d	O
)	O
)	O
,	O
and	O
(	O
ii	O
)	O
full	O
pre	O
-	O
activation	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
e	O
)	O
)	O
where	O
BN	Method
and	O
ReLU	Method
are	O
both	O
adopted	O
before	O
weight	Method
layers	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
ReLU	Method
-	O
only	O
pre	O
-	O
activation	O
performs	O
very	O
similar	O
to	O
the	O
baseline	O
on	O
ResNet	Method
-	Method
110	Method
/	Method
164	Method
.	O
This	O
ReLU	Method
layer	O
is	O
not	O
used	O
in	O
conjunction	O
with	O
a	O
BN	Method
layer	O
,	O
and	O
may	O
not	O
enjoy	O
the	O
benefits	O
of	O
BN	Method
.	O
Somehow	O
surprisingly	O
,	O
when	O
BN	Method
and	O
ReLU	Method
are	O
both	O
used	O
as	O
pre	O
-	O
activation	O
,	O
the	O
results	O
are	O
improved	O
by	O
healthy	O
margins	O
(	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
)	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
report	O
results	O
using	O
various	O
architectures	O
:	O
(	O
i	O
)	O
ResNet	Method
-	Method
110	Method
,	O
(	O
ii	O
)	O
ResNet	Method
-	Method
164	Method
,	O
(	O
iii	O
)	O
a	O
110	O
-	O
layer	O
ResNet	Method
architecture	O
in	O
which	O
each	O
shortcut	O
skips	O
only	O
1	O
layer	O
(	O
i.e.	O
,	O
a	O
Residual	O
Unit	O
has	O
only	O
1	O
layer	O
)	O
,	O
denoted	O
as	O
“	O
ResNet	Method
-	O
110	O
(	O
1layer	O
)	O
”	O
,	O
and	O
(	O
iv	O
)	O
a	O
1001	Method
-	Method
layer	Method
bottleneck	Method
architecture	Method
that	O
has	O
333	O
Residual	O
Units	O
(	O
111	O
on	O
each	O
feature	O
map	O
size	O
)	O
,	O
denoted	O
as	O
“	O
ResNet	Method
-	Method
1001	Method
”	O
.	O
We	O
also	O
experiment	O
on	O
CIFAR	Material
-	Material
100	Material
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
our	O
“	O
pre	Method
-	Method
activation	Method
”	Method
models	Method
are	O
consistently	O
better	O
than	O
the	O
baseline	O
counterparts	O
.	O
We	O
analyze	O
these	O
results	O
in	O
the	O
following	O
.	O
subsection	O
:	O
Analysis	O
We	O
find	O
the	O
impact	O
of	O
pre	Task
-	Task
activation	Task
is	O
twofold	O
.	O
First	O
,	O
the	O
optimization	Task
is	O
further	O
eased	O
(	O
comparing	O
with	O
the	O
baseline	O
ResNet	Method
)	O
because	O
is	O
an	O
identity	Method
mapping	Method
.	O
Second	O
,	O
using	O
BN	Method
as	O
pre	Method
-	Method
activation	Method
improves	O
regularization	Task
of	O
the	O
models	O
.	O
Ease	Task
of	Task
optimization	Task
.	O
This	O
effect	O
is	O
particularly	O
obvious	O
when	O
training	O
the	O
1001	Method
-	Method
layer	Method
ResNet	Method
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
curves	O
.	O
Using	O
the	O
original	O
design	O
in	O
,	O
the	O
training	O
error	Metric
is	O
reduced	O
very	O
slowly	O
at	O
the	O
beginning	O
of	O
training	O
.	O
For	O
ReLU	Method
,	O
the	O
signal	O
is	O
impacted	O
if	O
it	O
is	O
negative	O
,	O
and	O
when	O
there	O
are	O
many	O
Residual	O
Units	O
,	O
this	O
effect	O
becomes	O
prominent	O
and	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
(	O
so	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
)	O
is	O
not	O
a	O
good	O
approximation	O
.	O
On	O
the	O
other	O
hand	O
,	O
when	O
is	O
an	O
identity	O
mapping	O
,	O
the	O
signal	O
can	O
be	O
propagated	O
directly	O
between	O
any	O
two	O
units	O
.	O
Our	O
1001	Method
-	Method
layer	Method
network	Method
reduces	O
the	O
training	Metric
loss	Metric
very	O
quickly	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
It	O
also	O
achieves	O
the	O
lowest	O
loss	Metric
among	O
all	O
models	O
we	O
investigated	O
,	O
suggesting	O
the	O
success	O
of	O
optimization	Task
.	O
We	O
also	O
find	O
that	O
the	O
impact	O
of	O
ReLU	Method
is	O
not	O
severe	O
when	O
the	O
ResNet	Method
has	O
fewer	O
layers	O
(	O
e.g.	O
,	O
164	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
right	O
)	O
)	O
.	O
The	O
training	O
curve	O
seems	O
to	O
suffer	O
a	O
little	O
bit	O
at	O
the	O
beginning	O
of	O
training	O
,	O
but	O
goes	O
into	O
a	O
healthy	O
status	O
soon	O
.	O
By	O
monitoring	O
the	O
responses	O
we	O
observe	O
that	O
this	O
is	O
because	O
after	O
some	O
training	O
,	O
the	O
weights	O
are	O
adjusted	O
into	O
a	O
status	O
such	O
that	O
in	O
Eqn	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
more	O
frequently	O
above	O
zero	O
and	O
does	O
not	O
truncate	O
it	O
(	O
is	O
always	O
non	O
-	O
negative	O
due	O
to	O
the	O
previous	O
ReLU	Method
,	O
so	O
is	O
below	O
zero	O
only	O
when	O
the	O
magnitude	O
of	O
is	O
very	O
negative	O
)	O
.	O
The	O
truncation	O
,	O
however	O
,	O
is	O
more	O
frequent	O
when	O
there	O
are	O
1000	O
layers	O
.	O
Reducing	Task
overfitting	Task
.	O
Another	O
impact	O
of	O
using	O
the	O
proposed	O
pre	Method
-	Method
activation	Method
unit	Method
is	O
on	O
regularization	Task
,	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
right	O
)	O
.	O
The	O
pre	Method
-	Method
activation	Method
version	Method
reaches	O
slightly	O
higher	O
training	Metric
loss	Metric
at	O
convergence	Metric
,	O
but	O
produces	O
lower	O
test	O
error	Metric
.	O
This	O
phenomenon	O
is	O
observed	O
on	O
ResNet	Method
-	Method
110	Method
,	O
ResNet	Method
-	Method
110	Method
(	Method
1	Method
-	Method
layer	Method
)	O
,	O
and	O
ResNet	Method
-	O
164	O
on	O
both	O
CIFAR	Material
-	Material
10	Material
and	O
100	Material
.	O
This	O
is	O
presumably	O
caused	O
by	O
BN	Method
’s	O
regularization	O
effect	O
.	O
In	O
the	O
original	O
Residual	O
Unit	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
a	O
)	O
)	O
,	O
although	O
the	O
BN	Method
normalizes	O
the	O
signal	O
,	O
this	O
is	O
soon	O
added	O
to	O
the	O
shortcut	O
and	O
thus	O
the	O
merged	O
signal	O
is	O
not	O
normalized	O
.	O
This	O
unnormalized	O
signal	O
is	O
then	O
used	O
as	O
the	O
input	O
of	O
the	O
next	O
weight	Method
layer	Method
.	O
On	O
the	O
contrary	O
,	O
in	O
our	O
pre	Method
-	Method
activation	Method
version	Method
,	O
the	O
inputs	O
to	O
all	O
weight	O
layers	O
have	O
been	O
normalized	O
.	O
section	O
:	O
Results	O
Comparisons	O
on	O
CIFAR	Material
-	Material
10	Material
/	O
100	Material
.	O
Table	O
[	O
reference	O
]	O
compares	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
CIFAR	Material
-	Material
10	Material
/	O
100	Material
,	O
where	O
we	O
achieve	O
competitive	O
results	O
.	O
We	O
note	O
that	O
we	O
do	O
not	O
specially	O
tailor	O
the	O
network	O
width	O
or	O
filter	O
sizes	O
,	O
nor	O
use	O
regularization	Method
techniques	Method
(	O
such	O
as	O
dropout	Method
)	O
which	O
are	O
very	O
effective	O
for	O
these	O
small	O
datasets	O
.	O
We	O
obtain	O
these	O
results	O
via	O
a	O
simple	O
but	O
essential	O
concept	O
—	O
going	O
deeper	O
.	O
These	O
results	O
demonstrate	O
the	O
potential	O
of	O
pushing	O
the	O
limits	O
of	O
depth	O
.	O
Comparisons	O
on	O
ImageNet	Material
.	O
Next	O
we	O
report	O
experimental	O
results	O
on	O
the	O
1000	O
-	O
class	O
ImageNet	Material
dataset	O
.	O
We	O
have	O
done	O
preliminary	O
experiments	O
using	O
the	O
skip	O
connections	O
studied	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
&	O
[	O
reference	O
]	O
on	O
ImageNet	Material
with	O
ResNet	Method
-	Method
101	Method
,	O
and	O
observed	O
similar	O
optimization	Metric
difficulties	Metric
.	O
The	O
training	O
error	Metric
of	O
these	O
non	Method
-	Method
identity	Method
shortcut	Method
networks	Method
is	O
obviously	O
higher	O
than	O
the	O
original	O
ResNet	Method
at	O
the	O
first	O
learning	Metric
rate	Metric
(	O
similar	O
to	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
,	O
and	O
we	O
decided	O
to	O
halt	O
training	O
due	O
to	O
limited	O
resources	O
.	O
But	O
we	O
did	O
finish	O
a	O
“	O
BN	Method
after	O
addition	O
”	O
version	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
)	O
)	O
of	O
ResNet	Method
-	Method
101	Method
on	O
ImageNet	Material
and	O
observed	O
higher	O
training	Metric
loss	Metric
and	O
validation	O
error	Metric
.	O
This	O
model	O
’s	O
single	O
-	O
crop	O
(	O
224	O
224	O
)	O
validation	O
error	Metric
is	O
24.6%	O
/	O
7.5	O
%	O
,	O
vs.	O
the	O
original	O
ResNet	Method
-	O
101	O
’s	O
23.6%	O
/	O
7.1	O
%	O
.	O
This	O
is	O
in	O
line	O
with	O
the	O
results	O
on	O
CIFAR	Material
in	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
left	O
)	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
ResNet	Method
-	Method
152	Method
and	O
ResNet	Method
-	Method
200	Method
,	O
all	O
trained	O
from	O
scratch	O
.	O
We	O
notice	O
that	O
the	O
original	O
ResNet	Method
paper	O
trained	O
the	O
models	O
using	O
scale	O
jittering	O
with	O
shorter	O
side	O
,	O
and	O
so	O
the	O
test	O
of	O
a	O
224	O
224	O
crop	O
on	O
(	O
as	O
did	O
in	O
)	O
is	O
negatively	O
biased	O
.	O
Instead	O
,	O
we	O
test	O
a	O
single	O
320	O
320	O
crop	O
from	O
,	O
for	O
all	O
original	O
and	O
our	O
ResNets	Method
.	O
Even	O
though	O
the	O
ResNets	Method
are	O
trained	O
on	O
smaller	O
crops	O
,	O
they	O
can	O
be	O
easily	O
tested	O
on	O
larger	O
crops	O
because	O
the	O
ResNets	Method
are	O
fully	O
convolutional	Method
by	O
design	O
.	O
This	O
size	O
is	O
also	O
close	O
to	O
299	O
299	O
used	O
by	O
Inception	Method
v3	Method
,	O
allowing	O
a	O
fairer	O
comparison	O
.	O
The	O
original	O
ResNet	Method
-	Method
152	Method
has	O
top	O
-	O
1	O
error	Metric
of	O
21.3	O
%	O
on	O
a	O
320	O
320	O
crop	O
,	O
and	O
our	O
pre	Method
-	Method
activation	Method
counterpart	Method
has	O
21.1	O
%	O
.	O
The	O
gain	O
is	O
not	O
big	O
on	O
ResNet	Method
-	Method
152	Method
because	O
this	O
model	O
has	O
not	O
shown	O
severe	O
generalization	O
difficulties	O
.	O
However	O
,	O
the	O
original	O
ResNet	Method
-	Method
200	Method
has	O
an	O
error	Metric
rate	O
of	O
21.8	O
%	O
,	O
higher	O
than	O
the	O
baseline	O
ResNet	Method
-	O
152	O
.	O
But	O
we	O
find	O
that	O
the	O
original	O
ResNet	Method
-	Method
200	Method
has	O
lower	O
training	O
error	Metric
than	O
ResNet	Method
-	O
152	O
,	O
suggesting	O
that	O
it	O
suffers	O
from	O
overfitting	O
.	O
Our	O
pre	O
-	O
activation	O
ResNet	Method
-	O
200	O
has	O
an	O
error	Metric
rate	O
of	O
20.7	O
%	O
,	O
which	O
is	O
1.1	O
%	O
lower	O
than	O
the	O
baseline	O
ResNet	Method
-	O
200	O
and	O
also	O
lower	O
than	O
the	O
two	O
versions	O
of	O
ResNet	Method
-	Method
152	Method
.	O
When	O
using	O
the	O
scale	O
and	O
aspect	O
ratio	O
augmentation	O
of	O
,	O
our	O
ResNet	Method
-	Method
200	Method
has	O
a	O
result	O
better	O
than	O
Inception	Method
v3	Method
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
Concurrent	O
with	O
our	O
work	O
,	O
an	O
Inception	O
-	O
ResNet	Method
-	O
v2	O
model	O
achieves	O
a	O
single	O
-	O
crop	O
result	O
of	O
19.9%	O
/	O
4.9	O
%	O
.	O
We	O
expect	O
our	O
observations	O
and	O
the	O
proposed	O
Residual	Method
Unit	Method
will	O
help	O
this	O
type	O
and	O
generally	O
other	O
types	O
of	O
ResNets	Method
.	O
Computational	Metric
Cost	Metric
.	O
Our	O
models	O
’	O
computational	Metric
complexity	Metric
is	O
linear	O
on	O
depth	O
(	O
so	O
a	O
1001	Method
-	Method
layer	Method
net	Method
is	O
10	O
complex	O
of	O
a	O
100	Material
-	O
layer	O
net	O
)	O
.	O
On	O
CIFAR	Material
,	O
ResNet	Method
-	Method
1001	Method
takes	O
about	O
27	O
hours	O
to	O
train	O
on	O
2	O
GPUs	Method
;	O
on	O
ImageNet	Material
,	O
ResNet	Method
-	Method
200	Method
takes	O
about	O
3	O
weeks	O
to	O
train	O
on	O
8	O
GPUs	O
(	O
on	O
par	O
with	O
VGG	Method
nets	Method
)	O
.	O
section	O
:	O
Conclusions	O
This	O
paper	O
investigates	O
the	O
propagation	Method
formulations	Method
behind	O
the	O
connection	Method
mechanisms	Method
of	Method
deep	Method
residual	Method
networks	Method
.	O
Our	O
derivations	O
imply	O
that	O
identity	O
shortcut	O
connections	O
and	O
identity	O
after	O
-	O
addition	O
activation	O
are	O
essential	O
for	O
making	O
information	Task
propagation	Task
smooth	O
.	O
Ablation	Task
experiments	O
demonstrate	O
phenomena	O
that	O
are	O
consistent	O
with	O
our	O
derivations	O
.	O
We	O
also	O
present	O
1000	Method
-	Method
layer	Method
deep	Method
networks	Method
that	O
can	O
be	O
easily	O
trained	O
and	O
achieve	O
improved	O
accuracy	Metric
.	O
subsubsection	O
:	O
Appendix	O
:	O
Implementation	O
Details	O
The	O
implementation	O
details	O
and	O
hyper	O
-	O
parameters	O
are	O
the	O
same	O
as	O
those	O
in	O
.	O
On	O
CIFAR	Material
we	O
use	O
only	O
the	O
translation	O
and	O
flipping	O
augmentation	O
in	O
for	O
training	Task
.	O
The	O
learning	Metric
rate	Metric
starts	O
from	O
0.1	O
,	O
and	O
is	O
divided	O
by	O
10	O
at	O
32k	O
and	O
48k	O
iterations	O
.	O
Following	O
,	O
for	O
all	O
CIFAR	Material
experiments	O
we	O
warm	O
up	O
the	O
training	O
by	O
using	O
a	O
smaller	O
learning	Metric
rate	Metric
of	O
0.01	O
at	O
the	O
beginning	O
400	O
iterations	O
and	O
go	O
back	O
to	O
0.1	O
after	O
that	O
,	O
although	O
we	O
remark	O
that	O
this	O
is	O
not	O
necessary	O
for	O
our	O
proposed	O
Residual	Method
Unit	Method
.	O
The	O
mini	O
-	O
batch	O
size	O
is	O
128	O
on	O
2	O
GPUs	O
(	O
64	O
each	O
)	O
,	O
the	O
weight	O
decay	O
is	O
0.0001	O
,	O
the	O
momentum	O
is	O
0.9	O
,	O
and	O
the	O
weights	O
are	O
initialized	O
as	O
in	O
.	O
On	O
ImageNet	Material
,	O
we	O
train	O
the	O
models	O
using	O
the	O
same	O
data	O
augmentation	O
as	O
in	O
.	O
The	O
learning	Metric
rate	Metric
starts	O
from	O
0.1	O
(	O
no	O
warming	O
up	O
)	O
,	O
and	O
is	O
divided	O
by	O
10	O
at	O
30	O
and	O
60	O
epochs	O
.	O
The	O
mini	O
-	O
batch	O
size	O
is	O
256	O
on	O
8	O
GPUs	O
(	O
32	O
each	O
)	O
.	O
The	O
weight	O
decay	O
,	O
momentum	O
,	O
and	O
weight	Method
initialization	Method
are	O
the	O
same	O
as	O
above	O
.	O
When	O
using	O
the	O
pre	O
-	O
activation	O
Residual	O
Units	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
d	O
)(	O
e	O
)	O
and	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
,	O
we	O
pay	O
special	O
attention	O
to	O
the	O
first	O
and	O
the	O
last	O
Residual	O
Units	O
of	O
the	O
entire	O
network	O
.	O
For	O
the	O
first	O
Residual	Method
Unit	Method
(	O
that	O
follows	O
a	O
stand	Method
-	Method
alone	Method
convolutional	Method
layer	Method
,	O
conv	Method
)	O
,	O
we	O
adopt	O
the	O
first	O
activation	O
right	O
after	O
conv	O
and	O
before	O
splitting	O
into	O
two	O
paths	O
;	O
for	O
the	O
last	O
Residual	Method
Unit	Method
(	O
followed	O
by	O
average	Method
pooling	Method
and	O
a	O
fully	Method
-	Method
connected	Method
classifier	Method
)	O
,	O
we	O
adopt	O
an	O
extra	O
activation	O
right	O
after	O
its	O
element	O
-	O
wise	O
addition	O
.	O
These	O
two	O
special	O
cases	O
are	O
the	O
natural	O
outcome	O
when	O
we	O
obtain	O
the	O
pre	Method
-	Method
activation	Method
network	Method
via	O
the	O
modification	Method
procedure	Method
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
The	O
bottleneck	O
Residual	O
Units	O
(	O
for	O
ResNet	Method
-	O
164	O
/	O
1001	O
on	O
CIFAR	Material
)	O
are	O
constructed	O
following	O
.	O
For	O
example	O
,	O
a	O
unit	O
in	O
ResNet	Method
-	O
110	O
is	O
replaced	O
with	O
a	O
unit	O
in	O
ResNet	Method
-	O
164	O
,	O
both	O
of	O
which	O
have	O
roughly	O
the	O
same	O
number	O
of	O
parameters	O
.	O
For	O
the	O
bottleneck	O
ResNets	Method
,	O
when	O
reducing	O
the	O
feature	O
map	O
size	O
we	O
use	O
projection	O
shortcuts	O
for	O
increasing	O
dimensions	O
,	O
and	O
when	O
pre	O
-	O
activation	O
is	O
used	O
,	O
these	O
projection	Method
shortcuts	Method
are	O
also	O
with	O
pre	O
-	O
activation	O
.	O
bibliography	O
:	O
References	O
