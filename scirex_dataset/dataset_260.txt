document	O
:	O
Neural	Method
Natural	Method
Language	Method
Inference	Method
Models	Method
Enhanced	O
with	O
External	O
Knowledge	O
Modeling	Task
natural	Task
language	Task
inference	Task
is	O
a	O
very	O
challenging	O
task	O
.	O
With	O
the	O
availability	O
of	O
large	O
annotated	O
data	O
,	O
it	O
has	O
recently	O
become	O
feasible	O
to	O
train	O
complex	Method
models	Method
such	O
as	O
neural	Method
-	Method
network	Method
-	Method
based	Method
inference	Method
models	Method
,	O
which	O
have	O
shown	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
Although	O
there	O
exist	O
relatively	O
large	O
annotated	O
data	O
,	O
can	O
machines	O
learn	O
all	O
knowledge	O
needed	O
to	O
perform	O
natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
from	O
these	O
data	O
?	O
If	O
not	O
,	O
how	O
can	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
benefit	O
from	O
external	O
knowledge	O
and	O
how	O
to	O
build	O
NLI	Task
models	O
to	O
leverage	O
it	O
?	O
In	O
this	O
paper	O
,	O
we	O
enrich	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
natural	Method
language	Method
inference	Method
models	Method
with	O
external	O
knowledge	O
.	O
We	O
demonstrate	O
that	O
the	O
proposed	O
models	O
improve	O
neural	O
NLI	Task
models	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
SNLI	Material
and	O
MultiNLI	Material
datasets	Material
.	O
[	O
enumerate	O
,	O
1	O
]	O
1	O
o	O
section	O
:	O
Introduction	O
Reasoning	Task
and	O
inference	Task
are	O
central	O
to	O
both	O
human	Task
and	Task
artificial	Task
intelligence	Task
.	O
Natural	Task
language	Task
inference	Task
(	O
NLI	Task
)	O
,	O
also	O
known	O
as	O
recognizing	Task
textual	Task
entailment	Task
(	O
RTE	Task
)	O
,	O
is	O
an	O
important	O
NLP	Task
problem	Task
concerned	O
with	O
determining	Task
inferential	Task
relationship	Task
(	O
e.g.	O
,	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
)	O
between	O
a	O
premise	O
p	O
and	O
a	O
hypothesis	O
h	O
.	O
In	O
general	O
,	O
modeling	O
informal	Task
inference	Task
in	Task
language	Task
is	O
a	O
very	O
challenging	O
and	O
basic	O
problem	O
towards	O
achieving	O
true	O
natural	Task
language	Task
understanding	Task
.	O
In	O
the	O
last	O
several	O
years	O
,	O
larger	O
annotated	O
datasets	O
were	O
made	O
available	O
,	O
e.g.	O
,	O
the	O
SNLI	Material
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
BowmanAPM15	O
and	O
MultiNLI	Material
datasets	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
WilliamsNB17	O
,	O
which	O
made	O
it	O
feasible	O
to	O
train	O
rather	O
complicated	O
neural	Method
-	Method
network	Method
-	Method
based	Method
models	Method
that	O
fit	O
a	O
large	O
set	O
of	O
parameters	O
to	O
better	O
model	O
NLI	Task
.	O
Such	O
models	O
have	O
shown	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
BowmanAPM15	O
,	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
BowmanGRGMP16	O
,	O
DBLP	Method
:	O
conf	O
/	O
eacl	O
/	O
YuM17	O
,	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
ParikhT0U16	O
,	O
DBLP	Method
:	O
conf	O
/	O
coling	O
/	O
ShaCSL16	O
,	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
,	O
DBLP	Method
:	O
conf	O
/	O
repeval	O
/	O
ChenZLWJI17	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1801	O
-	O
00102	O
.	O
While	O
neural	Method
networks	Method
have	O
been	O
shown	O
to	O
be	O
very	O
effective	O
in	O
modeling	O
NLI	Task
with	O
large	O
training	O
data	O
,	O
they	O
have	O
often	O
focused	O
on	O
end	Task
-	Task
to	Task
-	Task
end	Task
training	Task
by	O
assuming	O
that	O
all	O
inference	O
knowledge	O
is	O
learnable	O
from	O
the	O
provided	O
training	O
data	O
.	O
In	O
this	O
paper	O
,	O
we	O
relax	O
this	O
assumption	O
and	O
explore	O
whether	O
external	O
knowledge	O
can	O
further	O
help	O
NLI	Task
.	O
Consider	O
an	O
example	O
:	O
p	O
:	O
A	O
lady	O
standing	O
in	O
a	O
wheat	O
field	O
.	O
h	O
:	O
A	O
person	O
standing	O
in	O
a	O
corn	O
field	O
.	O
In	O
this	O
simplified	O
example	O
,	O
when	O
computers	O
are	O
asked	O
to	O
predict	O
the	O
relation	O
between	O
these	O
two	O
sentences	O
and	O
if	O
training	O
data	O
do	O
not	O
provide	O
the	O
knowledge	O
of	O
relationship	O
between	O
“	O
wheat	O
”	O
and	O
“	O
corn	O
”	O
(	O
e.g.	O
,	O
if	O
one	O
of	O
the	O
two	O
words	O
does	O
not	O
appear	O
in	O
the	O
training	O
data	O
or	O
they	O
are	O
not	O
paired	O
in	O
any	O
premise	O
-	O
hypothesis	O
pairs	O
)	O
,	O
it	O
will	O
be	O
hard	O
for	O
computers	O
to	O
correctly	O
recognize	O
that	O
the	O
premise	O
contradicts	O
the	O
hypothesis	O
.	O
In	O
general	O
,	O
although	O
in	O
many	O
tasks	O
learning	O
tabula	Method
rasa	Method
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
we	O
believe	O
complicated	O
NLP	Task
problems	Task
such	O
as	O
NLI	Task
could	O
benefit	O
from	O
leveraging	O
knowledge	O
accumulated	O
by	O
humans	O
,	O
particularly	O
in	O
a	O
foreseeable	O
future	O
when	O
machines	O
are	O
unable	O
to	O
learn	O
it	O
by	O
themselves	O
.	O
In	O
this	O
paper	O
we	O
enrich	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
with	O
external	O
knowledge	O
in	O
co	Task
-	Task
attention	Task
,	O
local	Task
inference	Task
collection	Task
,	O
and	O
inference	Method
composition	Method
components	Method
.	O
We	O
show	O
the	O
proposed	O
model	O
improves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NLI	Task
models	O
to	O
achieve	O
better	O
performances	O
on	O
the	O
SNLI	Material
and	O
MultiNLI	Material
datasets	Material
.	O
The	O
advantage	O
of	O
using	O
external	O
knowledge	O
is	O
more	O
significant	O
when	O
the	O
size	O
of	O
training	O
data	O
is	O
restricted	O
,	O
suggesting	O
that	O
if	O
more	O
knowledge	O
can	O
be	O
obtained	O
,	O
it	O
may	O
bring	O
more	O
benefit	O
.	O
In	O
addition	O
to	O
attaining	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
we	O
are	O
also	O
interested	O
in	O
understanding	O
how	O
external	O
knowledge	O
contributes	O
to	O
the	O
major	O
components	O
of	O
typical	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
.	O
section	O
:	O
Related	O
Work	O
Early	O
research	O
on	O
natural	Task
language	Task
inference	Task
and	O
recognizing	Task
textual	Task
entailment	Task
has	O
been	O
performed	O
on	O
relatively	O
small	O
datasets	O
(	O
refer	O
to	O
MacCartneyThesis	O
for	O
a	O
good	O
literature	O
survey	O
)	O
,	O
which	O
includes	O
a	O
large	O
bulk	O
of	O
contributions	O
made	O
under	O
the	O
name	O
of	O
RTE	Task
,	O
such	O
as	O
Dagan2005ThePR	O
,	O
Iftene	O
:	O
W07	O
-	O
1421	O
,	O
among	O
many	O
others	O
.	O
More	O
recently	O
the	O
availability	O
of	O
much	O
larger	O
annotated	O
data	O
,	O
e.g.	O
,	O
SNLI	Material
DBLP	Method
:	O
conf	Material
/	Material
emnlp	Material
/	Material
BowmanAPM15	Material
and	O
MultiNLI	Material
DBLP	Method
:	O
journals	Material
/	Material
corr	Material
/	O
WilliamsNB17	Material
,	O
has	O
made	O
it	O
possible	O
to	O
train	O
more	O
complex	O
models	O
.	O
These	O
models	O
mainly	O
fall	O
into	O
two	O
types	O
of	O
approaches	O
:	O
sentence	Method
-	Method
encoding	Method
-	Method
based	Method
models	Method
and	O
models	O
using	O
also	O
inter	O
-	O
sentence	O
attention	O
.	O
Sentence	Method
-	Method
encoding	Method
-	Method
based	Method
models	Method
use	O
Siamese	O
architecture	O
DBLP	Method
:	O
conf	O
/	O
nips	Material
/	O
BromleyGLSS93	Material
.	O
The	O
parameter	Method
-	Method
tied	Method
neural	Method
networks	Method
are	O
applied	O
to	O
encode	O
both	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
Then	O
a	O
neural	Method
network	Method
classifier	Method
is	O
applied	O
to	O
decide	O
relationship	O
between	O
the	O
two	O
sentences	O
.	O
Different	O
neural	Method
networks	Method
have	O
been	O
utilized	O
for	O
sentence	Task
encoding	Task
,	O
such	O
as	O
LSTM	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
BowmanAPM15	O
,	O
GRU	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
VendrovKFU15	O
,	O
CNN	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
MouMLX0YJ16	O
,	O
BiLSTM	Method
and	O
its	O
variants	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
LiuSLW16	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
LinFSYXZB17	O
,	O
DBLP	Method
:	O
conf	O
/	O
repeval	O
/	O
ChenZLWJI17	O
,	O
DBLP	Method
:	O
conf	O
/	O
repeval	O
/	O
NieB17	O
,	O
self	Method
-	Method
attention	Method
network	Method
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1709	O
-	O
04696	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
abs	O
-	O
1801	O
-	O
10296	O
,	O
and	O
more	O
complicated	O
neural	Method
networks	Method
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
BowmanGRGMP16	O
,	O
DBLP	Method
:	O
conf	O
/	O
eacl	O
/	O
YuM17a	O
,	O
DBLP	Method
:	O
conf	O
/	O
eacl	O
/	O
YuM17	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
ChoiYL17	O
.	O
Sentence	Method
-	Method
encoding	Method
-	Method
based	Method
models	Method
transform	O
sentences	O
into	O
fixed	Method
-	Method
length	Method
vector	Method
representations	Method
,	O
which	O
may	O
help	O
a	O
wide	O
range	O
of	O
tasks	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
ConneauKSBB17	O
.	O
The	O
second	O
set	O
of	O
models	O
use	O
inter	O
-	O
sentence	O
attention	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
RocktaschelGHKB15	O
,	O
DBLP	Method
:	O
conf	O
/	O
naacl	O
/	O
WangJ16	O
,	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
0001DL16	O
,	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
ParikhT0U16	O
,	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
.	O
Among	O
them	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
RocktaschelGHKB15	O
were	O
among	O
the	O
first	O
to	O
propose	O
neural	Method
attention	Method
-	Method
based	Method
models	Method
for	O
NLI	Task
.	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
proposed	O
an	O
enhanced	Method
sequential	Method
inference	Method
model	Method
(	O
ESIM	Method
)	Method
,	O
which	O
is	O
one	O
of	O
the	O
best	O
models	O
so	O
far	O
and	O
is	O
used	O
as	O
one	O
of	O
our	O
baselines	O
in	O
this	O
paper	O
.	O
In	O
this	O
paper	O
we	O
enrich	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
with	O
external	O
knowledge	O
.	O
Unlike	O
early	O
work	O
on	O
NLI	Task
Jijkoun2005RecognizingTE	O
,	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
MacCartneyGM08	O
,	O
MacCartneyThesis	O
that	O
explores	O
external	O
knowledge	O
in	O
conventional	O
NLI	Task
models	O
on	O
relatively	O
small	O
NLI	Task
datasets	O
,	O
we	O
aim	O
to	O
merge	O
the	O
advantage	O
of	O
powerful	O
modeling	O
ability	O
of	O
neural	Method
networks	Method
with	O
extra	O
external	O
inference	O
knowledge	O
.	O
We	O
show	O
that	O
the	O
proposed	O
model	O
improves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
NLI	Task
models	O
to	O
achieve	O
better	O
performances	O
on	O
the	O
SNLI	Material
and	O
MultiNLI	Material
datasets	Material
.	O
The	O
advantage	O
of	O
using	O
external	O
knowledge	O
is	O
more	O
significant	O
when	O
the	O
size	O
of	O
training	O
data	O
is	O
restricted	O
,	O
suggesting	O
that	O
if	O
more	O
knowledge	O
can	O
be	O
obtained	O
,	O
it	O
may	O
have	O
more	O
benefit	O
.	O
In	O
addition	O
to	O
attaining	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
we	O
are	O
also	O
interested	O
in	O
understanding	O
how	O
external	O
knowledge	O
affect	O
major	O
components	O
of	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
.	O
In	O
general	O
,	O
external	O
knowledge	O
has	O
shown	O
to	O
be	O
effective	O
in	O
neural	Method
networks	Method
for	O
other	O
NLP	Task
tasks	Task
,	O
including	O
word	O
embedding	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenLCCWJZ15	O
,	O
DBLP	Method
:	O
conf	O
/	O
naacl	O
/	O
FaruquiDJDHS15	O
,	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
Liu0WLH15	O
,	O
DBLP	Method
:	O
journals	O
/	O
tacl	O
/	O
WietingBGL15	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
MrksicVSLRGKY17	O
,	O
machine	O
translation	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ShiLRFLZSW16	O
,	O
DBLP	Method
:	O
conf	O
/	O
apsipa	O
/	O
ZhangMWH17	O
,	O
language	O
modeling	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
AhnCPB16	O
,	O
and	O
dialogue	O
systems	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
ChenHTCGD16	O
.	O
section	O
:	O
Neural	O
-	O
Network	O
-	O
Based	O
NLI	Task
Models	O
with	O
External	O
Knowledge	O
In	O
this	O
section	O
we	O
propose	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
models	O
to	O
incorporate	O
external	O
inference	O
knowledge	O
,	O
which	O
,	O
as	O
we	O
will	O
show	O
later	O
in	O
Section	O
[	O
reference	O
]	O
,	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
In	O
addition	O
to	O
attaining	O
the	O
leading	O
performance	O
we	O
are	O
also	O
interested	O
in	O
investigating	O
the	O
effects	O
of	O
external	O
knowledge	O
on	O
major	O
components	O
of	O
neural	O
-	O
network	O
-	O
based	O
NLI	Task
modeling	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
a	O
high	O
-	O
level	O
general	O
view	O
of	O
the	O
proposed	O
framework	O
.	O
While	O
specific	O
NLI	Task
systems	O
vary	O
in	O
their	O
implementation	O
,	O
typical	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NLI	Task
models	O
contain	O
the	O
main	O
components	O
(	O
or	O
equivalents	O
)	O
of	O
representing	O
premise	O
and	O
hypothesis	O
sentences	O
,	O
collecting	O
local	O
(	O
e.g.	O
,	O
lexical	O
)	O
inference	O
information	O
,	O
and	O
aggregating	O
and	O
composing	O
local	O
information	O
to	O
make	O
the	O
global	O
decision	O
at	O
the	O
sentence	O
level	O
.	O
We	O
incorporate	O
and	O
investigate	O
external	O
knowledge	O
accordingly	O
in	O
these	O
major	O
NLI	Task
components	O
:	O
computing	O
co	O
-	O
attention	O
,	O
collecting	O
local	O
inference	O
information	O
,	O
and	O
composing	O
inference	Task
to	O
make	O
final	O
decision	Task
.	O
subsection	O
:	O
External	O
Knowledge	O
As	O
discussed	O
above	O
,	O
although	O
there	O
exist	O
relatively	O
large	O
annotated	O
data	O
for	O
NLI	Task
,	O
can	O
machines	O
learn	O
all	O
inference	O
knowledge	O
needed	O
to	O
perform	O
NLI	Task
from	O
the	O
data	O
?	O
If	O
not	O
,	O
how	O
can	O
neural	O
network	O
-	O
based	O
NLI	Task
models	O
benefit	O
from	O
external	O
knowledge	O
and	O
how	O
to	O
build	O
NLI	Task
models	O
to	O
leverage	O
it	O
?	O
We	O
study	O
the	O
incorporation	O
of	O
external	Task
,	Task
inference	Task
-	Task
related	Task
knowledge	Task
in	O
major	O
components	O
of	O
neural	Method
networks	Method
for	O
natural	Task
language	Task
inference	Task
.	O
For	O
example	O
,	O
intuitively	O
knowledge	O
about	O
synonymy	O
,	O
antonymy	O
,	O
hypernymy	O
and	O
hyponymy	O
between	O
given	O
words	O
may	O
help	O
model	O
soft	O
-	O
alignment	O
between	O
premises	O
and	O
hypotheses	O
;	O
knowledge	O
about	O
hypernymy	O
and	O
hyponymy	O
may	O
help	O
capture	O
entailment	O
;	O
knowledge	O
about	O
antonymy	O
and	O
co	O
-	O
hyponyms	O
(	O
words	O
sharing	O
the	O
same	O
hypernym	O
)	O
may	O
benefit	O
the	O
modeling	O
of	O
contradiction	Task
.	O
In	O
this	O
section	O
,	O
we	O
discuss	O
the	O
incorporation	O
of	O
basic	O
,	O
lexical	O
-	O
level	O
semantic	O
knowledge	O
into	O
neural	O
NLI	Task
components	O
.	O
Specifically	O
,	O
we	O
consider	O
external	O
lexical	O
-	O
level	O
inference	O
knowledge	O
between	O
word	O
and	O
,	O
which	O
is	O
represented	O
as	O
a	O
vector	O
and	O
is	O
incorporated	O
into	O
three	O
specific	O
components	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
will	O
discuss	O
the	O
details	O
of	O
how	O
is	O
constructed	O
later	O
in	O
the	O
experiment	O
setup	O
section	O
(	O
Section	O
[	O
reference	O
]	O
)	O
but	O
instead	O
focus	O
on	O
the	O
proposed	O
model	O
in	O
this	O
section	O
.	O
Note	O
that	O
while	O
we	O
study	O
lexical	O
-	O
level	O
inference	O
knowledge	O
in	O
the	O
paper	O
,	O
if	O
inference	O
knowledge	O
about	O
larger	O
pieces	O
of	O
text	O
pairs	O
(	O
e.g.	O
,	O
inference	O
relations	O
between	O
phrases	O
)	O
are	O
available	O
,	O
the	O
proposed	O
model	O
can	O
be	O
easily	O
extended	O
to	O
handle	O
that	O
.	O
In	O
this	O
paper	O
,	O
we	O
instead	O
let	O
the	O
NLI	Task
models	O
to	O
compose	O
lexical	O
-	O
level	O
knowledge	O
to	O
obtain	O
inference	O
relations	O
between	O
larger	O
pieces	O
of	O
texts	O
.	O
subsection	O
:	O
Encoding	O
Premise	O
and	O
Hypothesis	O
Same	O
as	O
much	O
previous	O
work	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
,	O
DBLP	Method
:	O
conf	O
/	O
repeval	O
/	O
ChenZLWJI17	O
,	O
we	O
encode	O
the	O
premise	O
and	O
the	O
hypothesis	O
with	O
bidirectional	Method
LSTMs	Method
(	O
BiLSTMs	Method
)	O
.	O
The	O
premise	O
is	O
represented	O
as	O
and	O
the	O
hypothesis	O
is	O
,	O
where	O
and	O
are	O
the	O
lengths	O
of	O
the	O
sentences	O
.	O
Then	O
and	O
are	O
embedded	O
into	O
-	O
dimensional	O
vectors	O
and	O
using	O
the	O
embedding	O
matrix	O
,	O
where	O
is	O
the	O
vocabulary	O
size	O
and	O
can	O
be	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
word	Method
embedding	Method
.	O
To	O
represent	O
words	O
in	O
its	O
context	O
,	O
the	O
premise	O
and	O
the	O
hypothesis	O
are	O
fed	O
into	O
BiLSTM	O
encoders	O
DBLP	Method
:	O
journals	O
/	O
neco	O
/	O
HochreiterS97	O
to	O
obtain	O
context	O
-	O
dependent	O
hidden	O
states	O
and	O
:	O
where	O
and	O
indicate	O
the	O
-	O
th	O
word	O
in	O
the	O
premise	O
and	O
the	O
-	O
th	O
word	O
in	O
the	O
hypothesis	O
,	O
respectively	O
.	O
subsection	O
:	O
Knowledge	Task
-	Task
Enriched	Task
Co	Task
-	Task
Attention	Task
As	O
discussed	O
above	O
,	O
soft	O
-	O
alignment	O
of	O
word	O
pairs	O
between	O
the	O
premise	O
and	O
the	O
hypothesis	O
may	O
benefit	O
from	O
knowledge	Method
-	Method
enriched	Method
co	Method
-	Method
attention	Method
mechanism	Method
.	O
Given	O
the	O
relation	O
features	O
between	O
the	O
premise	O
’s	O
-	O
th	O
word	O
and	O
the	O
hypothesis	O
’s	O
-	O
th	O
word	O
derived	O
from	O
the	O
external	O
knowledge	O
,	O
the	O
co	O
-	O
attention	O
is	O
calculated	O
as	O
:	O
The	O
function	O
can	O
be	O
any	O
non	O
-	O
linear	O
or	O
linear	Method
functions	Method
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
,	O
where	O
is	O
a	O
hyper	O
-	O
parameter	O
tuned	O
on	O
the	O
development	O
set	O
and	O
is	O
the	O
indication	O
function	O
as	O
follows	O
:	O
Intuitively	O
,	O
word	O
pairs	O
with	O
semantic	O
relationship	O
,	O
e.g.	O
,	O
synonymy	O
,	O
antonymy	O
,	O
hypernymy	O
,	O
hyponymy	O
and	O
co	O
-	O
hyponyms	O
,	O
are	O
probably	O
aligned	O
together	O
.	O
We	O
will	O
discuss	O
how	O
we	O
construct	O
external	O
knowledge	O
later	O
in	O
Section	O
[	O
reference	O
]	O
.	O
We	O
have	O
also	O
tried	O
a	O
two	Method
-	Method
layer	Method
MLP	Method
as	O
a	O
universal	Method
function	Method
approximator	Method
in	O
function	O
to	O
learn	O
the	O
underlying	O
combination	O
function	O
but	O
did	O
not	O
observe	O
further	O
improvement	O
over	O
the	O
best	O
performance	O
we	O
obtained	O
on	O
the	O
development	O
datasets	O
.	O
Soft	Task
-	Task
alignment	Task
is	O
determined	O
by	O
the	O
co	O
-	O
attention	O
matrix	O
computed	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
,	O
which	O
is	O
used	O
to	O
obtain	O
the	O
local	O
relevance	O
between	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
For	O
the	O
hidden	O
state	O
of	O
the	O
-	O
th	O
word	O
in	O
the	O
premise	O
,	O
i.e.	O
,	O
(	O
already	O
encoding	O
the	O
word	O
itself	O
and	O
its	O
context	O
)	O
,	O
the	O
relevant	O
semantics	O
in	O
the	O
hypothesis	O
is	O
identified	O
into	O
a	O
context	O
vector	O
using	O
,	O
more	O
specifically	O
with	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
.	O
where	O
and	O
are	O
the	O
normalized	O
attention	O
weight	O
matrices	O
with	O
respect	O
to	O
the	O
-	O
axis	O
and	O
-	O
axis	O
.	O
The	O
same	O
calculation	O
is	O
performed	O
for	O
each	O
word	O
in	O
the	O
hypothesis	O
,	O
i.e.	O
,	O
,	O
with	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
to	O
obtain	O
the	O
context	O
vector	O
.	O
subsection	O
:	O
Local	Task
Inference	Task
Collection	Task
with	O
External	O
Knowledge	O
By	O
way	O
of	O
comparing	O
the	O
inference	O
-	O
related	O
semantic	O
relation	O
between	O
(	O
individual	O
word	Method
representation	Method
in	O
premise	O
)	O
and	O
(	O
context	O
representation	O
from	O
hypothesis	O
which	O
is	O
align	O
to	O
word	O
)	O
,	O
we	O
can	O
model	O
local	Method
inference	Method
(	O
i.e.	O
,	O
word	Method
-	Method
level	Method
inference	Method
)	O
between	O
aligned	O
word	O
pairs	O
.	O
Intuitively	O
,	O
for	O
example	O
,	O
knowledge	O
about	O
hypernymy	O
or	O
hyponymy	O
may	O
help	O
model	O
entailment	O
and	O
knowledge	O
about	O
antonymy	O
and	O
co	O
-	O
hyponyms	O
may	O
help	O
model	O
contradiction	O
.	O
Through	O
comparing	O
and	O
,	O
in	O
addition	O
to	O
their	O
relation	O
from	O
external	O
knowledge	O
,	O
we	O
can	O
obtain	O
word	O
-	O
level	O
inference	O
information	O
for	O
each	O
word	O
.	O
The	O
same	O
calculation	O
is	O
performed	O
for	O
and	O
.	O
Thus	O
,	O
we	O
collect	O
knowledge	O
-	O
enriched	O
local	O
inference	O
information	O
:	O
where	O
a	O
heuristic	Method
matching	Method
trick	Method
with	O
difference	O
and	O
element	Method
-	Method
wise	Method
product	Method
is	O
used	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
MouMLX0YJ16	O
,	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
.	O
The	O
last	O
terms	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
(	O
[	O
reference	O
]	O
)	O
are	O
used	O
to	O
obtain	O
word	O
-	O
level	O
inference	O
information	O
from	O
external	O
knowledge	O
.	O
Take	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
as	O
example	O
,	O
is	O
the	O
relation	O
feature	O
between	O
the	O
-	O
th	O
word	O
in	O
the	O
premise	O
and	O
the	O
-	O
th	O
word	O
in	O
the	O
hypothesis	O
,	O
but	O
we	O
care	O
more	O
about	O
semantic	O
relation	O
between	O
aligned	O
word	O
pairs	O
between	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
Thus	O
,	O
we	O
use	O
a	O
soft	Method
-	Method
aligned	Method
version	Method
through	O
the	O
soft	O
-	O
alignment	O
weight	O
.	O
For	O
the	O
-	O
th	O
word	O
in	O
the	O
premise	O
,	O
the	O
last	O
term	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
is	O
a	O
word	O
-	O
level	O
inference	O
information	O
based	O
on	O
external	O
knowledge	O
between	O
the	O
-	O
th	O
word	O
and	O
the	O
aligned	O
word	O
.	O
The	O
same	O
calculation	O
for	O
hypothesis	Task
is	O
performed	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
.	O
is	O
a	O
non	Method
-	Method
linear	Method
mapping	Method
function	Method
to	O
reduce	O
dimensionality	O
.	O
Specifically	O
,	O
we	O
use	O
a	O
1	Method
-	Method
layer	Method
feed	Method
-	Method
forward	Method
neural	Method
network	Method
with	O
the	O
ReLU	Method
activation	Method
function	Method
with	O
a	O
shortcut	O
connection	O
,	O
i.e.	O
,	O
concatenate	O
the	O
hidden	O
states	O
after	O
ReLU	O
with	O
the	O
input	O
(	O
or	O
)	O
as	O
the	O
output	O
(	O
or	O
)	O
.	O
subsection	O
:	O
Knowledge	Task
-	Task
Enhanced	Task
Inference	Task
Composition	Task
In	O
this	O
component	O
,	O
we	O
introduce	O
knowledge	Task
-	Task
enriched	Task
inference	Task
composition	Task
.	O
To	O
determine	O
the	O
overall	O
inference	O
relationship	O
between	O
the	O
premise	O
and	O
the	O
hypothesis	O
,	O
we	O
need	O
to	O
explore	O
a	O
composition	Method
layer	Method
to	O
compose	O
the	O
local	O
inference	O
vectors	O
(	O
and	O
)	O
collected	O
above	O
:	O
Here	O
,	O
we	O
also	O
use	O
BiLSTMs	Method
as	O
building	O
blocks	O
for	O
the	O
composition	Method
layer	Method
,	O
but	O
the	O
responsibility	O
of	O
BiLSTMs	O
in	O
the	O
inference	Method
composition	Method
layer	Method
is	O
completely	O
different	O
from	O
that	O
in	O
the	O
input	Method
encoding	Method
layer	Method
.	O
The	O
BiLSTMs	Method
here	O
read	O
local	O
inference	O
vectors	O
(	O
and	O
)	O
and	O
learn	O
to	O
judge	O
the	O
types	O
of	O
local	O
inference	O
relationship	O
and	O
distinguish	O
crucial	O
local	O
inference	O
vectors	O
for	O
overall	O
sentence	O
-	O
level	O
inference	O
relationship	O
.	O
Intuitively	O
,	O
the	O
final	O
prediction	Task
is	O
likely	O
to	O
depend	O
on	O
word	O
pairs	O
appearing	O
in	O
external	O
knowledge	O
that	O
have	O
some	O
semantic	O
relation	O
.	O
Our	O
inference	Method
model	Method
converts	O
the	O
output	O
hidden	O
vectors	O
of	O
BiLSTMs	O
to	O
the	O
fixed	O
-	O
length	O
vector	O
with	O
pooling	Method
operations	Method
and	O
puts	O
it	O
into	O
the	O
final	O
classifier	Method
to	O
determine	O
the	O
overall	O
inference	O
class	O
.	O
Particularly	O
,	O
in	O
addition	O
to	O
using	O
mean	Method
pooling	Method
and	O
max	Method
pooling	Method
similarly	O
to	O
ESIM	Method
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
,	O
we	O
propose	O
to	O
use	O
weighted	Method
pooling	Method
based	O
on	O
external	O
knowledge	O
to	O
obtain	O
a	O
fixed	O
-	O
length	O
vector	O
as	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
(	O
[	O
reference	O
]	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
regard	O
the	O
function	O
as	O
a	O
1	Method
-	Method
layer	Method
feed	Method
-	Method
forward	Method
neural	Method
network	Method
with	O
ReLU	Method
activation	Method
function	Method
.	O
We	O
concatenate	O
all	O
pooling	O
vectors	O
,	O
i.e.	O
,	O
mean	O
,	O
max	O
,	O
and	O
weighted	Method
pooling	Method
,	O
into	O
the	O
fixed	O
-	O
length	O
vector	O
and	O
then	O
put	O
the	O
vector	O
into	O
the	O
final	O
multilayer	Method
perceptron	Method
(	Method
MLP	Method
)	Method
classifier	Method
.	O
The	O
MLP	Method
has	O
one	O
hidden	O
layer	O
with	O
tanh	O
activation	O
and	O
softmax	O
output	O
layer	O
in	O
our	O
experiments	O
.	O
The	O
entire	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
through	O
minimizing	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
.	O
section	O
:	O
Experiment	O
Set	O
-	O
Up	O
subsection	O
:	O
Representation	Task
of	Task
External	Task
Knowledge	Task
paragraph	O
:	O
Lexical	O
Semantic	O
Relations	O
As	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
to	O
incorporate	O
external	O
knowledge	O
(	O
as	O
a	O
knowledge	O
vector	O
)	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
network	O
-	O
based	O
NLI	Task
models	O
,	O
we	O
first	O
explore	O
semantic	O
relations	O
in	O
WordNet	O
DBLP	Method
:	O
journals	O
/	O
cacm	O
/	O
Miller95	O
,	O
motivated	O
by	O
MacCartneyThesis	Method
.	O
Specifically	O
,	O
the	O
relations	O
of	O
lexical	O
pairs	O
are	O
derived	O
as	O
described	O
in	O
[	O
reference	O
]	O
-	O
[	O
reference	O
]	O
below	O
.	O
Instead	O
of	O
using	O
Jiang	O
-	O
Conrath	O
WordNet	O
distance	O
metric	O
DBLP	Method
:	O
conf	O
/	O
rocling	Method
/	O
JiangC97	Method
,	O
which	O
does	O
not	O
improve	O
the	O
performance	O
of	O
our	O
models	O
on	O
the	O
development	O
sets	O
,	O
we	O
add	O
a	O
new	O
feature	O
,	O
i.e.	O
,	O
co	O
-	O
hyponyms	O
,	O
which	O
consistently	O
benefit	O
our	O
models	O
.	O
Synonymy	Method
:	O
It	O
takes	O
the	O
value	O
if	O
the	O
words	O
in	O
the	O
pair	O
are	O
synonyms	O
in	O
WordNet	O
(	O
i.e.	O
,	O
belong	O
to	O
the	O
same	O
synset	O
)	O
,	O
and	O
otherwise	O
.	O
For	O
example	O
,	O
[	O
felicitous	O
,	O
good	O
]	O
=	O
,	O
[	O
dog	O
,	O
wolf	O
]	O
=	O
.	O
Antonymy	O
:	O
It	O
takes	O
the	O
value	O
if	O
the	O
words	O
in	O
the	O
pair	O
are	O
antonyms	O
in	O
WordNet	Material
,	O
and	O
otherwise	O
.	O
For	O
example	O
,	O
[	O
wet	O
,	O
dry	O
]	O
=	O
.	O
Hypernymy	Method
:	O
It	O
takes	O
the	O
value	O
if	O
one	O
word	O
is	O
a	O
(	O
direct	O
or	O
indirect	O
)	O
hypernym	O
of	O
the	O
other	O
word	O
in	O
WordNet	O
,	O
where	O
is	O
the	O
number	O
of	O
edges	O
between	O
the	O
two	O
words	O
in	O
hierarchies	O
,	O
and	O
otherwise	O
.	O
Note	O
that	O
we	O
ignore	O
pairs	O
in	O
the	O
hierarchy	O
which	O
have	O
more	O
than	O
8	O
edges	O
in	O
between	O
.	O
For	O
example	O
,	O
[	O
dog	O
,	O
canid	O
]	O
=	O
,	O
[	O
wolf	O
,	O
canid	O
]	O
=	O
,	O
[	O
dog	O
,	O
carnivore	O
]	O
=	O
,	O
[	O
canid	O
,	O
dog	O
]	O
=	O
Hyponymy	Method
:	O
It	O
is	O
simply	O
the	O
inverse	O
of	O
the	O
hypernymy	O
feature	O
.	O
For	O
example	O
,	O
[	O
canid	O
,	O
dog	O
]	O
=	O
,	O
[	O
dog	O
,	O
canid	O
]	O
=	O
.	O
Co	O
-	O
hyponyms	O
:	O
It	O
takes	O
the	O
value	O
if	O
the	O
two	O
words	O
have	O
the	O
same	O
hypernym	O
but	O
they	O
do	O
not	O
belong	O
to	O
the	O
same	O
synset	O
,	O
and	O
otherwise	O
.	O
For	O
example	O
,	O
[	O
dog	O
,	O
wolf	O
]	O
=	O
.	O
As	O
discussed	O
above	O
,	O
we	O
expect	O
features	O
like	O
synonymy	O
,	O
antonymy	O
,	O
hypernymy	O
,	O
hyponymy	O
and	O
co	O
-	O
hyponyms	O
would	O
help	O
model	O
co	O
-	O
attention	O
alignment	O
between	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
Knowledge	O
of	O
hypernymy	O
and	O
hyponymy	O
may	O
help	O
capture	O
entailment	O
;	O
knowledge	O
of	O
antonymy	O
and	O
co	O
-	O
hyponyms	O
may	O
help	O
model	O
contradiction	O
.	O
Their	O
final	O
contributions	O
will	O
be	O
learned	O
in	O
end	Task
-	Task
to	Task
-	Task
end	Task
model	Task
training	Task
.	O
We	O
regard	O
the	O
vector	O
as	O
the	O
relation	O
feature	O
derived	O
from	O
external	O
knowledge	O
,	O
where	O
is	O
here	O
.	O
In	O
addition	O
,	O
Table	O
[	O
reference	O
]	O
reports	O
some	O
key	O
statistics	O
of	O
these	O
features	O
.	O
In	O
addition	O
to	O
the	O
above	O
relations	O
,	O
we	O
also	O
use	O
more	O
relation	O
features	O
in	O
WordNet	O
,	O
including	O
instance	O
,	O
instance	O
of	O
,	O
same	O
instance	O
,	O
entailment	O
,	O
member	O
meronym	O
,	O
member	O
holonym	O
,	O
substance	O
meronym	O
,	O
substance	O
holonym	O
,	O
part	O
meronym	O
,	O
part	O
holonym	O
,	O
summing	O
up	O
to	O
15	O
features	O
,	O
but	O
these	O
additional	O
features	O
do	O
not	O
bring	O
further	O
improvement	O
on	O
the	O
development	O
dataset	O
,	O
as	O
also	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Relation	Method
Embeddings	Method
In	O
the	O
most	O
recent	O
years	O
graph	Task
embedding	Task
has	O
been	O
widely	O
employed	O
to	O
learn	O
representation	O
for	O
vertexes	O
and	O
their	O
relations	O
in	O
a	O
graph	O
.	O
In	O
our	O
work	O
here	O
,	O
we	O
also	O
capture	O
the	O
relation	O
between	O
any	O
two	O
words	O
in	O
WordNet	O
through	O
relation	Method
embedding	Method
.	O
Specifically	O
,	O
we	O
employed	O
TransE	O
DBLP	Method
:	O
conf	O
/	O
nips	O
/	O
BordesUGWY13	O
,	O
a	O
widely	O
used	O
graph	Method
embedding	Method
methods	Method
,	O
to	O
capture	O
relation	Task
embedding	Task
between	O
any	O
two	O
words	O
.	O
We	O
used	O
two	O
typical	O
approaches	O
to	O
obtaining	O
the	O
relation	Task
embedding	Task
.	O
The	O
first	O
directly	O
uses	O
18	O
relation	Method
embeddings	Method
pretrained	O
on	O
the	O
WN18	O
dataset	O
DBLP	Method
:	O
conf	O
/	O
nips	O
/	O
BordesUGWY13	O
.	O
Specifically	O
,	O
if	O
a	O
word	O
pair	O
has	O
a	O
certain	O
type	O
relation	O
,	O
we	O
take	O
the	O
corresponding	O
relation	Method
embedding	Method
.	O
Sometimes	O
,	O
if	O
a	O
word	O
pair	O
has	O
multiple	O
relations	O
among	O
the	O
18	O
types	O
;	O
we	O
take	O
an	O
average	O
of	O
the	O
relation	Method
embedding	Method
.	O
The	O
second	O
approach	O
uses	O
TransE	O
’s	O
word	Method
embedding	Method
(	O
trained	O
on	O
WordNet	Material
)	O
to	O
obtain	O
relation	O
embedding	O
,	O
through	O
the	O
objective	O
function	O
used	O
in	O
TransE	O
,	O
i.e.	O
,	O
,	O
where	O
indicates	O
relation	O
embedding	O
,	O
indicates	O
tail	O
entity	O
embedding	O
,	O
and	O
indicates	O
head	O
entity	O
embedding	O
.	O
Note	O
that	O
in	O
addition	O
to	O
relation	Method
embedding	Method
trained	O
on	O
WordNet	Material
,	O
other	O
relational	Method
embedding	Method
resources	Method
exist	O
;	O
e.g.	O
,	O
that	O
trained	O
on	O
Freebase	Material
(	Material
WikiData	Material
)	O
DBLP	Method
:	O
conf	O
/	O
aaai	O
/	O
BollackerCT07	O
,	O
but	O
such	O
knowledge	Method
resources	Method
are	O
mainly	O
about	O
facts	O
(	O
e.g.	O
,	O
relationship	O
between	O
Bill	O
Gates	O
and	O
Microsoft	O
)	O
and	O
are	O
less	O
for	O
commonsense	O
knowledge	O
used	O
in	O
general	O
natural	Task
language	Task
inference	Task
(	O
e.g.	O
,	O
the	O
color	O
yellow	O
potentially	O
contradicts	O
red	O
)	O
.	O
subsection	O
:	O
NLI	Task
Datasets	O
In	O
our	O
experiments	O
,	O
we	O
use	O
Stanford	Material
Natural	Material
Language	Material
Inference	Material
(	O
SNLI	Material
)	O
dataset	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
BowmanAPM15	O
and	O
Multi	Task
-	Task
Genre	Task
Natural	Task
Language	Task
Inference	Task
(	O
MultiNLI	Material
)	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
WilliamsNB17	O
dataset	O
,	O
which	O
focus	O
on	O
three	O
basic	O
relations	O
between	O
a	O
premise	O
and	O
a	O
potential	O
hypothesis	O
:	O
the	O
premise	O
entails	O
the	O
hypothesis	O
(	O
entailment	O
)	O
,	O
they	O
contradict	O
each	O
other	O
(	O
contradiction	O
)	O
,	O
or	O
they	O
are	O
not	O
related	O
(	O
neutral	O
)	O
.	O
We	O
use	O
the	O
same	O
data	O
split	O
as	O
in	O
previous	O
work	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
BowmanAPM15	O
,	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
WilliamsNB17	O
and	O
classification	Metric
accuracy	Metric
as	O
the	O
evaluation	Metric
metric	Metric
.	O
In	O
addition	O
,	O
we	O
test	O
our	O
models	O
(	O
trained	O
on	O
the	O
SNLI	Material
training	O
set	O
)	O
on	O
a	O
new	O
test	O
set	O
glockner_acl18	O
,	O
which	O
assesses	O
the	O
lexical	Task
inference	Task
abilities	Task
of	O
NLI	Task
systems	O
and	O
consists	O
of	O
8	O
,	O
193	O
samples	O
.	O
WordNet	O
3.0	O
DBLP	Method
:	O
journals	O
/	O
cacm	O
/	O
Miller95	O
is	O
used	O
to	O
extract	O
semantic	O
relation	O
features	O
between	O
words	O
.	O
The	O
words	O
are	O
lemmatized	O
using	O
Stanford	Method
CoreNLP	Method
3.7.0	O
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ManningSBFBM14	O
.	O
The	O
premise	O
and	O
the	O
hypothesis	O
sentences	O
fed	O
into	O
the	O
input	O
encoding	Method
layer	Method
are	O
tokenized	O
.	O
subsection	O
:	O
Training	O
Details	O
For	O
duplicability	O
,	O
we	O
release	O
our	O
code	O
.	O
All	O
our	O
models	O
were	O
strictly	O
selected	O
on	O
the	O
development	O
set	O
of	O
the	O
SNLI	Material
data	O
and	O
the	O
in	O
-	O
domain	O
development	O
set	O
of	O
MultiNLI	Material
and	O
were	O
then	O
tested	O
on	O
the	O
corresponding	O
test	O
set	O
.	O
The	O
main	O
training	O
details	O
are	O
as	O
follows	O
:	O
the	O
dimension	O
of	O
the	O
hidden	O
states	O
of	O
LSTMs	O
and	O
word	O
embeddings	O
are	O
.	O
The	O
word	O
embeddings	O
are	O
initialized	O
by	O
300D	O
GloVe	O
840B	O
DBLP	Method
:	O
conf	O
/	O
emnlp	O
/	O
PenningtonSM14	O
,	O
and	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
among	O
them	O
are	O
initialized	O
randomly	O
.	O
All	O
word	O
embeddings	O
are	O
updated	O
during	O
training	O
.	O
Adam	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
KingmaB14	O
is	O
used	O
for	O
optimization	Task
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
.	O
The	O
mini	O
-	O
batch	O
size	O
is	O
set	O
to	O
.	O
Note	O
that	O
the	O
above	O
hyperparameter	O
settings	O
are	O
same	O
as	O
those	O
used	O
in	O
the	O
baseline	O
ESIM	Method
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
model	O
.	O
ESIM	Method
is	O
a	O
strong	O
NLI	Task
baseline	O
framework	O
with	O
the	O
source	O
code	O
made	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
lukecq1231	O
/	O
nli	O
(	O
the	O
ESIM	Method
core	Method
code	Method
has	O
also	O
been	O
adapted	O
to	O
summarization	O
DBLP	Method
:	O
conf	O
/	O
ijcai	O
/	O
ChenZLWJ16	O
and	O
question	Task
-	Task
answering	Task
tasks	Task
Zhang	O
:	O
qa:2017	O
)	O
.	O
The	O
trade	O
-	O
off	O
for	O
calculating	O
co	O
-	O
attention	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
is	O
selected	O
in	O
based	O
on	O
the	O
development	O
set	O
.	O
When	O
training	O
TransE	Method
for	O
WordNet	Material
,	O
relations	O
are	O
represented	O
with	O
vectors	O
of	O
dimension	O
.	O
section	O
:	O
Experimental	O
Results	O
subsection	O
:	O
Overall	O
Performance	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
SNLI	Material
dataset	O
.	O
Among	O
them	O
,	O
ESIM	Method
DBLP	Method
:	O
conf	O
/	O
acl	O
/	O
ChenZLWJI17	O
is	O
one	O
of	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
with	O
an	O
88.0	O
%	O
test	Metric
-	Metric
set	Metric
accuracy	Metric
.	O
The	O
proposed	O
model	O
,	O
namely	O
Knowledge	Method
-	Method
based	Method
Inference	Method
Model	Method
(	O
KIM	Method
)	O
,	O
which	O
enriches	O
ESIM	Method
with	O
external	O
knowledge	O
,	O
obtains	O
an	O
accuracy	Metric
of	O
88.6	O
%	O
,	O
the	O
best	O
single	O
-	O
model	O
performance	O
reported	O
on	O
the	O
SNLI	Material
dataset	O
.	O
The	O
difference	O
between	O
ESIM	Method
and	O
KIM	Method
is	O
statistically	O
significant	O
under	O
the	O
one	O
-	O
tailed	O
paired	O
-	O
test	O
at	O
the	O
99	O
%	O
significance	O
level	O
.	O
Note	O
that	O
the	O
KIM	Method
model	O
reported	O
here	O
uses	O
five	O
semantic	O
relations	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
In	O
addition	O
to	O
that	O
,	O
we	O
also	O
use	O
15	O
semantic	O
relation	O
features	O
,	O
which	O
does	O
not	O
bring	O
additional	O
gains	O
in	O
performance	O
.	O
These	O
results	O
highlight	O
the	O
effectiveness	O
of	O
the	O
five	O
semantic	O
relations	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
To	O
further	O
investigate	O
external	O
knowledge	O
,	O
we	O
add	O
TransE	Method
relation	Method
embedding	Method
,	O
and	O
again	O
no	O
further	O
improvement	O
is	O
observed	O
on	O
both	O
the	O
development	O
and	O
test	O
sets	O
when	O
TransE	Method
relation	Method
embedding	Method
is	O
used	O
(	O
concatenated	O
)	O
with	O
the	O
semantic	O
relation	O
vectors	O
.	O
We	O
consider	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
TransE	Method
embedding	Method
is	O
not	O
specifically	O
sensitive	O
to	O
inference	O
information	O
;	O
e.g.	O
,	O
it	O
does	O
not	O
model	O
co	O
-	O
hyponyms	O
features	O
,	O
and	O
its	O
potential	O
benefit	O
has	O
already	O
been	O
covered	O
by	O
the	O
semantic	O
relation	O
features	O
used	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
performance	O
of	O
models	O
on	O
the	O
MultiNLI	Material
dataset	O
.	O
The	O
baseline	O
ESIM	Method
achieves	O
76.8	O
%	O
and	O
75.8	O
%	O
on	O
in	O
-	O
domain	O
and	O
cross	O
-	O
domain	O
test	O
set	O
,	O
respectively	O
.	O
If	O
we	O
extend	O
the	O
ESIM	Method
with	O
external	O
knowledge	O
,	O
we	O
achieve	O
significant	O
gains	O
to	O
77.2	O
%	O
and	O
76.4	O
%	O
respectively	O
.	O
Again	O
,	O
the	O
gains	O
are	O
consistent	O
on	O
SNLI	Material
and	O
MultiNLI	Material
,	O
and	O
we	O
expect	O
they	O
would	O
be	O
orthogonal	O
to	O
other	O
factors	O
when	O
external	O
knowledge	O
is	O
added	O
into	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
subsection	O
:	O
Ablation	Task
Results	O
Figure	O
[	O
reference	O
]	O
displays	O
the	O
ablation	Task
analysis	Task
of	O
different	O
components	O
when	O
using	O
the	O
external	O
knowledge	O
.	O
To	O
compare	O
the	O
effects	O
of	O
external	O
knowledge	O
under	O
different	O
training	O
data	O
scales	O
,	O
we	O
randomly	O
sample	O
different	O
ratios	O
of	O
the	O
entire	O
training	O
set	O
,	O
i.e.	O
,	O
0.8	O
%	O
,	O
4	O
%	O
,	O
20	O
%	O
and	O
100	O
%	O
.	O
“	O
A	O
”	O
indicates	O
adding	O
external	O
knowledge	O
in	O
calculating	O
the	O
co	O
-	O
attention	O
matrix	O
as	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
,	O
“	O
I	O
”	O
indicates	O
adding	O
external	O
knowledge	O
in	O
collecting	O
local	O
inference	O
information	O
as	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
(	O
[	O
reference	O
]	O
)	O
,	O
and	O
“	O
C	O
”	O
indicates	O
adding	O
external	O
knowledge	O
in	O
composing	Task
inference	Task
as	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
(	O
[	O
reference	O
]	O
)	O
.	O
When	O
we	O
only	O
have	O
restricted	O
training	O
data	O
,	O
i.e.	O
,	O
0.8	O
%	O
training	O
set	O
(	O
about	O
4	O
,	O
000	O
samples	O
)	O
,	O
the	O
baseline	O
ESIM	Method
has	O
a	O
poor	O
accuracy	Metric
of	O
62.4	O
%	O
.	O
When	O
we	O
only	O
add	O
external	O
knowledge	O
in	O
calculating	O
co	O
-	O
attention	O
(	O
“	O
A	O
”	O
)	O
,	O
the	O
accuracy	Metric
increases	O
to	O
66.6	O
%	O
(	O
+	O
absolute	O
4.2	O
%	O
)	O
.	O
When	O
we	O
only	O
utilize	O
external	O
knowledge	O
in	O
collecting	O
local	Task
inference	Task
information	Task
(	O
“	O
I	O
”	O
)	O
,	O
the	O
accuracy	Metric
has	O
a	O
significant	O
gain	O
,	O
to	O
70.3	O
%	O
(	O
+	O
absolute	O
7.9	O
%	O
)	O
.	O
When	O
we	O
only	O
add	O
external	O
knowledge	O
in	O
inference	Task
composition	Task
(	O
“	O
C	O
”	O
)	O
,	O
the	O
accuracy	Metric
gets	O
a	O
smaller	O
gain	O
to	O
63.4	O
%	O
(	O
+	O
absolute	O
1.0	O
%	O
)	O
.	O
The	O
comparison	O
indicates	O
that	O
“	O
I	O
”	O
plays	O
the	O
most	O
important	O
role	O
among	O
the	O
three	O
components	O
in	O
using	O
external	O
knowledge	O
.	O
Moreover	O
,	O
when	O
we	O
compose	O
the	O
three	O
components	O
(	O
“	O
A	O
,	O
I	O
,	O
C	O
”	O
)	O
,	O
we	O
obtain	O
the	O
best	O
result	O
of	O
72.6	O
%	O
(	O
+	O
absolute	O
10.2	O
%	O
)	O
.	O
When	O
we	O
use	O
more	O
training	O
data	O
,	O
i.e.	O
,	O
4	O
%	O
,	O
20	O
%	O
,	O
100	O
%	O
of	O
the	O
training	O
set	O
,	O
only	O
“	O
I	O
”	O
achieves	O
a	O
significant	O
gain	O
,	O
but	O
“	O
A	O
”	O
or	O
“	O
C	O
”	O
does	O
not	O
bring	O
any	O
significant	O
improvement	O
.	O
The	O
results	O
indicate	O
that	O
external	O
semantic	O
knowledge	O
only	O
helps	O
co	Task
-	Task
attention	Task
and	O
composition	Task
when	O
limited	O
training	O
data	O
is	O
limited	O
,	O
but	O
always	O
helps	O
in	O
collecting	O
local	O
inference	O
information	O
.	O
Meanwhile	O
,	O
for	O
less	O
training	O
data	O
,	O
is	O
usually	O
set	O
to	O
a	O
larger	O
value	O
.	O
For	O
example	O
,	O
the	O
optimal	O
on	O
the	O
development	O
set	O
is	O
for	O
0.8	O
%	O
training	O
set	O
,	O
for	O
the	O
4	O
%	O
training	O
set	O
,	O
for	O
the	O
20	O
%	O
training	O
set	O
and	O
for	O
the	O
100	O
%	O
training	O
set	O
.	O
Figure	O
[	O
reference	O
]	O
displays	O
the	O
results	O
of	O
using	O
different	O
ratios	O
of	O
external	O
knowledge	O
(	O
randomly	O
keep	O
different	O
percentages	O
of	O
whole	O
lexical	O
semantic	O
relations	O
)	O
under	O
different	O
sizes	O
of	O
training	O
data	O
.	O
Note	O
that	O
here	O
we	O
only	O
use	O
external	O
knowledge	O
in	O
collecting	O
local	O
inference	O
information	O
as	O
it	O
always	O
works	O
well	O
for	O
different	O
scale	O
of	O
the	O
training	O
set	O
.	O
Better	O
accuracies	Metric
are	O
achieved	O
when	O
using	O
more	O
external	O
knowledge	O
.	O
Especially	O
under	O
the	O
condition	O
of	O
restricted	O
training	O
data	O
(	O
0.8	O
%	O
)	O
,	O
the	O
model	O
obtains	O
a	O
large	O
gain	O
when	O
using	O
more	O
than	O
half	O
of	O
external	O
knowledge	O
.	O
subsection	O
:	O
Analysis	O
on	O
the	O
glockner_acl18	O
Test	O
Set	O
In	O
addition	O
,	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
on	O
a	O
newly	O
published	O
test	O
set	O
glockner_acl18	O
.	O
Compared	O
with	O
the	O
performance	O
on	O
the	O
SNLI	Material
test	O
set	O
,	O
the	O
performance	O
of	O
the	O
three	O
baseline	O
models	O
dropped	O
substantially	O
on	O
the	O
glockner_acl18	O
test	O
set	O
,	O
with	O
the	O
differences	O
ranging	O
from	O
22.3	O
%	O
to	O
32.8	O
%	O
in	O
accuracy	Metric
.	O
Instead	O
,	O
the	O
proposed	O
KIM	Method
achieves	O
83.5	O
%	O
on	O
this	O
test	O
set	O
(	O
with	O
only	O
a	O
5.1	O
%	O
drop	O
in	O
performance	O
)	O
,	O
which	O
demonstrates	O
its	O
better	O
ability	O
of	O
utilizing	O
lexical	Method
level	Method
inference	Method
and	O
hence	O
better	O
generalizability	Task
.	O
Figure	O
[	O
reference	O
]	O
displays	O
the	O
accuracy	Metric
of	O
ESIM	Method
and	O
KIM	Method
in	O
each	O
replacement	O
-	O
word	O
category	O
of	O
the	O
glockner_acl18	O
test	O
set	O
.	O
KIM	Method
outperforms	O
ESIM	Method
in	O
13	O
out	O
of	O
14	O
categories	O
,	O
and	O
only	O
performs	O
worse	O
on	O
synonyms	O
.	O
subsection	O
:	O
Analysis	O
by	O
Inference	O
Categories	O
We	O
perform	O
more	O
analysis	O
(	O
Table	O
[	O
reference	O
]	O
)	O
using	O
the	O
supplementary	O
annotations	O
provided	O
by	O
the	O
MultiNLI	Material
dataset	O
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
WilliamsNB17	O
,	O
which	O
have	O
495	O
samples	O
(	O
about	O
1	O
/	O
20	O
of	O
the	O
entire	O
development	O
set	O
)	O
for	O
both	O
in	O
-	O
domain	O
and	O
out	O
-	O
domain	O
set	O
.	O
We	O
compare	O
against	O
the	O
model	O
outputs	O
of	O
the	O
ESIM	Method
model	Method
across	O
13	O
categories	O
of	O
inference	Task
.	O
Table	O
[	O
reference	O
]	O
reports	O
the	O
results	O
.	O
We	O
can	O
see	O
that	O
KIM	Method
outperforms	O
ESIM	Method
on	O
overall	O
accuracies	Metric
on	O
both	O
in	O
-	O
domain	O
and	O
cross	O
-	O
domain	O
subset	O
of	O
development	O
set	O
.	O
KIM	Method
outperforms	O
or	O
equals	O
ESIM	Method
in	O
10	O
out	O
of	O
13	O
categories	O
on	O
the	O
cross	Task
-	Task
domain	Task
setting	Task
,	O
while	O
only	O
7	O
out	O
of	O
13	O
categories	O
on	O
in	O
-	O
domain	O
setting	O
.	O
It	O
indicates	O
that	O
external	O
knowledge	O
helps	O
more	O
in	O
cross	Task
-	Task
domain	Task
setting	Task
.	O
Especially	O
,	O
for	O
antonym	O
category	O
in	O
cross	O
-	O
domain	O
set	O
,	O
KIM	Method
outperform	O
ESIM	Method
significantly	O
(	O
+	O
absolute	O
5.0	O
%	O
)	O
as	O
expected	O
,	O
because	O
antonym	O
feature	O
captured	O
by	O
external	O
knowledge	O
would	O
help	O
unseen	O
cross	O
-	O
domain	O
samples	O
.	O
subsection	O
:	O
Case	O
Study	O
Table	O
[	O
reference	O
]	O
includes	O
some	O
examples	O
from	O
the	O
SNLI	Material
test	O
set	O
,	O
where	O
KIM	Method
successfully	O
predicts	O
the	O
inference	O
relation	O
and	O
ESIM	Method
fails	O
.	O
In	O
the	O
first	O
example	O
,	O
the	O
premise	O
is	O
“	O
An	O
African	O
person	O
standing	O
in	O
a	O
wheat	O
field	O
”	O
and	O
the	O
hypothesis	O
“	O
A	O
person	O
standing	O
in	O
a	O
corn	O
field	O
”	O
.	O
As	O
the	O
KIM	Method
model	O
knows	O
that	O
“	O
wheat	O
”	O
and	O
“	O
corn	O
”	O
are	O
both	O
a	O
kind	O
of	O
cereal	O
,	O
i.e	O
,	O
the	O
co	O
-	O
hyponyms	O
relationship	O
in	O
our	O
relation	O
features	O
,	O
KIM	Method
therefore	O
predicts	O
the	O
premise	O
contradicts	O
the	O
hypothesis	O
.	O
However	O
,	O
the	O
baseline	O
ESIM	Method
can	O
not	O
learn	O
the	O
relationship	O
between	O
“	O
wheat	O
”	O
and	O
“	O
corn	O
”	O
effectively	O
due	O
to	O
lack	O
of	O
enough	O
samples	O
in	O
the	O
training	O
sets	O
.	O
With	O
the	O
help	O
of	O
external	O
knowledge	O
,	O
i.e.	O
,	O
“	O
wheat	O
”	O
and	O
“	O
corn	O
”	O
having	O
the	O
same	O
hypernym	O
“	O
cereal	O
”	O
,	O
KIM	Method
predicts	O
contradiction	O
correctly	O
.	O
section	O
:	O
Conclusions	O
Our	O
neural	Method
-	Method
network	Method
-	Method
based	Method
model	Method
for	O
natural	Task
language	Task
inference	Task
with	O
external	O
knowledge	O
,	O
namely	O
KIM	Method
,	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	Metric
.	O
The	O
model	O
is	O
equipped	O
with	O
external	O
knowledge	O
in	O
its	O
main	O
components	O
,	O
specifically	O
,	O
in	O
calculating	Task
co	Task
-	Task
attention	Task
,	O
collecting	Task
local	Task
inference	Task
,	O
and	O
composing	Task
inference	Task
.	O
We	O
provide	O
detailed	O
analyses	O
on	O
our	O
model	O
and	O
results	O
.	O
The	O
proposed	O
model	O
of	O
infusing	Method
neural	Method
networks	Method
with	O
external	O
knowledge	O
may	O
also	O
help	O
shed	O
some	O
light	O
on	O
tasks	O
other	O
than	O
NLI	Task
.	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
Yibo	O
Sun	O
and	O
Bing	O
Qin	O
for	O
early	O
helpful	O
discussion	O
.	O
bibliography	O
:	O
References	O
