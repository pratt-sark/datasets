document	O
:	O
Deep	Method
Reinforcement	Method
Learning	Method
with	O
Double	Method
Q	Method
-	Method
learning	Method
The	O
popular	O
Q	Method
-	Method
learning	Method
algorithm	Method
is	O
known	O
to	O
overestimate	O
action	O
values	O
under	O
certain	O
conditions	O
.	O
It	O
was	O
not	O
previously	O
known	O
whether	O
,	O
in	O
practice	O
,	O
such	O
overestimations	O
are	O
common	O
,	O
whether	O
they	O
harm	O
performance	O
,	O
and	O
whether	O
they	O
can	O
generally	O
be	O
prevented	O
.	O
In	O
this	O
paper	O
,	O
we	O
answer	O
all	O
these	O
questions	O
affirmatively	O
.	O
In	O
particular	O
,	O
we	O
first	O
show	O
that	O
the	O
recent	O
DQN	Method
algorithm	Method
,	O
which	O
combines	O
Q	Method
-	Method
learning	Method
with	O
a	O
deep	Method
neural	Method
network	Method
,	O
suffers	O
from	O
substantial	O
overestimations	O
in	O
some	O
games	O
in	O
the	O
Atari	Material
2600	Material
domain	Material
.	O
We	O
then	O
show	O
that	O
the	O
idea	O
behind	O
the	O
Double	Method
Q	Method
-	Method
learning	Method
algorithm	Method
,	O
which	O
was	O
introduced	O
in	O
a	O
tabular	Task
setting	Task
,	O
can	O
be	O
generalized	O
to	O
work	O
with	O
large	Task
-	Task
scale	Task
function	Task
approximation	Task
.	O
We	O
propose	O
a	O
specific	O
adaptation	O
to	O
the	O
DQN	Method
algorithm	Method
and	O
show	O
that	O
the	O
resulting	O
algorithm	O
not	O
only	O
reduces	O
the	O
observed	O
overestimations	O
,	O
as	O
hypothesized	O
,	O
but	O
that	O
this	O
also	O
leads	O
to	O
much	O
better	O
performance	O
on	O
several	O
games	O
.	O
The	O
goal	O
of	O
reinforcement	Method
learning	Method
is	O
to	O
learn	O
good	O
policies	Task
for	O
sequential	Task
decision	Task
problems	Task
,	O
by	O
optimizing	O
a	O
cumulative	O
future	O
reward	O
signal	O
.	O
Q	Method
-	Method
learning	Method
is	O
one	O
of	O
the	O
most	O
popular	O
reinforcement	Method
learning	Method
algorithms	Method
,	O
but	O
it	O
is	O
known	O
to	O
sometimes	O
learn	O
unrealistically	O
high	O
action	O
values	O
because	O
it	O
includes	O
a	O
maximization	O
step	O
over	O
estimated	O
action	O
values	O
,	O
which	O
tends	O
to	O
prefer	O
overestimated	O
to	O
underestimated	O
values	O
.	O
In	O
previous	O
work	O
,	O
overestimations	O
have	O
been	O
attributed	O
to	O
insufficiently	O
flexible	O
function	Method
approximation	Method
and	O
noise	O
.	O
In	O
this	O
paper	O
,	O
we	O
unify	O
these	O
views	O
and	O
show	O
overestimations	O
can	O
occur	O
when	O
the	O
action	O
values	O
are	O
inaccurate	O
,	O
irrespective	O
of	O
the	O
source	O
of	O
approximation	O
error	O
.	O
Of	O
course	O
,	O
imprecise	O
value	Method
estimates	Method
are	O
the	O
norm	O
during	O
learning	Task
,	O
which	O
indicates	O
that	O
overestimations	O
may	O
be	O
much	O
more	O
common	O
than	O
previously	O
appreciated	O
.	O
It	O
is	O
an	O
open	O
question	O
whether	O
,	O
if	O
the	O
overestimations	O
do	O
occur	O
,	O
this	O
negatively	O
affects	O
performance	O
in	O
practice	O
.	O
Overoptimistic	Method
value	Method
estimates	Method
are	O
not	O
necessarily	O
a	O
problem	O
in	O
and	O
of	O
themselves	O
.	O
If	O
all	O
values	O
would	O
be	O
uniformly	O
higher	O
then	O
the	O
relative	O
action	O
preferences	O
are	O
preserved	O
and	O
we	O
would	O
not	O
expect	O
the	O
resulting	O
policy	O
to	O
be	O
any	O
worse	O
.	O
Furthermore	O
,	O
it	O
is	O
known	O
that	O
sometimes	O
it	O
is	O
good	O
to	O
be	O
optimistic	O
:	O
optimism	O
in	O
the	O
face	O
of	O
uncertainty	O
is	O
a	O
well	O
-	O
known	O
exploration	Method
technique	Method
.	O
If	O
,	O
however	O
,	O
the	O
overestimations	O
are	O
not	O
uniform	O
and	O
not	O
concentrated	O
at	O
states	O
about	O
which	O
we	O
wish	O
to	O
learn	O
more	O
,	O
then	O
they	O
might	O
negatively	O
affect	O
the	O
quality	O
of	O
the	O
resulting	O
policy	O
.	O
give	O
specific	O
examples	O
in	O
which	O
this	O
leads	O
to	O
suboptimal	Task
policies	Task
,	O
even	O
asymptotically	O
.	O
To	O
test	O
whether	O
overestimations	O
occur	O
in	O
practice	O
and	O
at	O
scale	O
,	O
we	O
investigate	O
the	O
performance	O
of	O
the	O
recent	Method
DQN	Method
algorithm	Method
.	O
DQN	Method
combines	O
Q	Method
-	Method
learning	Method
with	O
a	O
flexible	O
deep	Method
neural	Method
network	Method
and	O
was	O
tested	O
on	O
a	O
varied	O
and	O
large	O
set	O
of	O
deterministic	Material
Atari	Material
2600	Material
games	Material
,	O
reaching	O
human	O
-	O
level	O
performance	O
on	O
many	O
games	O
.	O
In	O
some	O
ways	O
,	O
this	O
setting	O
is	O
a	O
best	O
-	O
case	O
scenario	O
for	O
Q	Task
-	Task
learning	Task
,	O
because	O
the	O
deep	Method
neural	Method
network	Method
provides	O
flexible	O
function	Method
approximation	Method
with	O
the	O
potential	O
for	O
a	O
low	O
asymptotic	Metric
approximation	Metric
error	Metric
,	O
and	O
the	O
determinism	O
of	O
the	O
environments	O
prevents	O
the	O
harmful	O
effects	O
of	O
noise	O
.	O
Perhaps	O
surprisingly	O
,	O
we	O
show	O
that	O
even	O
in	O
this	O
comparatively	O
favorable	O
setting	O
DQN	Method
sometimes	O
substantially	O
overestimates	O
the	O
values	O
of	O
the	O
actions	O
.	O
We	O
show	O
that	O
the	O
idea	O
behind	O
the	O
Double	Method
Q	Method
-	Method
learning	Method
algorithm	Method
,	O
which	O
was	O
first	O
proposed	O
in	O
a	O
tabular	Task
setting	Task
,	O
can	O
be	O
generalized	O
to	O
work	O
with	O
arbitrary	O
function	Method
approximation	Method
,	O
including	O
deep	Method
neural	Method
networks	Method
.	O
We	O
use	O
this	O
to	O
construct	O
a	O
new	O
algorithm	O
we	O
call	O
Double	Method
DQN	Method
.	O
We	O
then	O
show	O
that	O
this	O
algorithm	O
not	O
only	O
yields	O
more	O
accurate	O
value	Metric
estimates	Metric
,	O
but	O
leads	O
to	O
much	O
higher	O
scores	O
on	O
several	O
games	O
.	O
This	O
demonstrates	O
that	O
the	O
overestimations	O
of	O
DQN	Method
were	O
indeed	O
leading	O
to	O
poorer	O
policies	O
and	O
that	O
it	O
is	O
beneficial	O
to	O
reduce	O
them	O
.	O
In	O
addition	O
,	O
by	O
improving	O
upon	O
DQN	Method
we	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
Atari	Material
domain	Material
.	O
section	O
:	O
Background	O
To	O
solve	O
sequential	Task
decision	Task
problems	Task
we	O
can	O
learn	O
estimates	O
for	O
the	O
optimal	O
value	O
of	O
each	O
action	O
,	O
defined	O
as	O
the	O
expected	O
sum	O
of	O
future	O
rewards	O
when	O
taking	O
that	O
action	O
and	O
following	O
the	O
optimal	O
policy	O
thereafter	O
.	O
Under	O
a	O
given	O
policy	O
,	O
the	O
true	O
value	O
of	O
an	O
action	O
in	O
a	O
state	O
is	O
where	O
is	O
a	O
discount	O
factor	O
that	O
trades	O
off	O
the	O
importance	O
of	O
immediate	O
and	O
later	O
rewards	O
.	O
The	O
optimal	O
value	O
is	O
then	O
.	O
An	O
optimal	Method
policy	Method
is	O
easily	O
derived	O
from	O
the	O
optimal	O
values	O
by	O
selecting	O
the	O
highest	O
-	O
valued	O
action	O
in	O
each	O
state	O
.	O
Estimates	O
for	O
the	O
optimal	O
action	O
values	O
can	O
be	O
learned	O
using	O
Q	Method
-	Method
learning	Method
,	O
a	O
form	O
of	O
temporal	Method
difference	Method
learning	Method
.	O
Most	O
interesting	O
problems	O
are	O
too	O
large	O
to	O
learn	O
all	O
action	O
values	O
in	O
all	O
states	O
separately	O
.	O
Instead	O
,	O
we	O
can	O
learn	O
a	O
parameterized	Method
value	Method
function	Method
.	O
The	O
standard	O
Q	Method
-	Method
learning	Method
update	Method
for	O
the	O
parameters	O
after	O
taking	O
action	O
in	O
state	O
and	O
observing	O
the	O
immediate	O
reward	O
and	O
resulting	O
state	O
is	O
then	O
where	O
is	O
a	O
scalar	O
step	O
size	O
and	O
the	O
target	O
is	O
defined	O
as	O
This	O
update	O
resembles	O
stochastic	Method
gradient	Method
descent	Method
,	O
updating	O
the	O
current	O
value	O
towards	O
a	O
target	O
value	O
.	O
subsection	O
:	O
Deep	Method
Q	Method
Networks	Method
A	O
deep	Method
Q	Method
network	Method
(	O
DQN	Method
)	Method
is	O
a	O
multi	Method
-	Method
layered	Method
neural	Method
network	Method
that	O
for	O
a	O
given	O
state	O
outputs	O
a	O
vector	O
of	O
action	O
values	O
,	O
where	O
are	O
the	O
parameters	O
of	O
the	O
network	O
.	O
For	O
an	O
-	O
dimensional	O
state	O
space	O
and	O
an	O
action	O
space	O
containing	O
actions	O
,	O
the	O
neural	Method
network	Method
is	O
a	O
function	O
from	O
to	O
.	O
Two	O
important	O
ingredients	O
of	O
the	O
DQN	Method
algorithm	Method
as	O
proposed	O
by	O
are	O
the	O
use	O
of	O
a	O
target	Method
network	Method
,	O
and	O
the	O
use	O
of	O
experience	Method
replay	Method
.	O
The	O
target	O
network	O
,	O
with	O
parameters	O
,	O
is	O
the	O
same	O
as	O
the	O
online	Method
network	Method
except	O
that	O
its	O
parameters	O
are	O
copied	O
every	O
steps	O
from	O
the	O
online	Method
network	Method
,	O
so	O
that	O
then	O
,	O
and	O
kept	O
fixed	O
on	O
all	O
other	O
steps	O
.	O
The	O
target	O
used	O
by	O
DQN	Method
is	O
then	O
For	O
the	O
experience	Task
replay	Task
,	O
observed	O
transitions	O
are	O
stored	O
for	O
some	O
time	O
and	O
sampled	O
uniformly	O
from	O
this	O
memory	O
bank	O
to	O
update	O
the	O
network	O
.	O
Both	O
the	O
target	O
network	O
and	O
the	O
experience	Task
replay	Task
dramatically	O
improve	O
the	O
performance	O
of	O
the	O
algorithm	O
.	O
subsection	O
:	O
Double	Method
Q	Method
-	Method
learning	Method
The	O
max	Method
operator	Method
in	O
standard	O
Q	Method
-	Method
learning	Method
and	O
DQN	Method
,	O
in	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
,	O
uses	O
the	O
same	O
values	O
both	O
to	O
select	O
and	O
to	O
evaluate	O
an	O
action	O
.	O
This	O
makes	O
it	O
more	O
likely	O
to	O
select	O
overestimated	O
values	O
,	O
resulting	O
in	O
overoptimistic	O
value	O
estimates	O
.	O
To	O
prevent	O
this	O
,	O
we	O
can	O
decouple	O
the	O
selection	Task
from	O
the	O
evaluation	Task
.	O
This	O
is	O
the	O
idea	O
behind	O
Double	Method
Q	Method
-	Method
learning	Method
.	O
In	O
the	O
original	O
Double	Method
Q	Method
-	Method
learning	Method
algorithm	Method
,	O
two	O
value	O
functions	O
are	O
learned	O
by	O
assigning	O
each	O
experience	O
randomly	O
to	O
update	O
one	O
of	O
the	O
two	O
value	O
functions	O
,	O
such	O
that	O
there	O
are	O
two	O
sets	O
of	O
weights	O
,	O
and	O
.	O
For	O
each	O
update	O
,	O
one	O
set	O
of	O
weights	O
is	O
used	O
to	O
determine	O
the	O
greedy	Method
policy	Method
and	O
the	O
other	O
to	O
determine	O
its	O
value	O
.	O
For	O
a	O
clear	O
comparison	O
,	O
we	O
can	O
first	O
untangle	O
the	O
selection	O
and	O
evaluation	Task
in	O
Q	Method
-	Method
learning	Method
and	O
rewrite	O
its	O
target	O
(	O
[	O
reference	O
]	O
)	O
as	O
The	O
Double	Metric
Q	Metric
-	Metric
learning	Metric
error	Metric
can	O
then	O
be	O
written	O
as	O
Notice	O
that	O
the	O
selection	O
of	O
the	O
action	O
,	O
in	O
the	O
,	O
is	O
still	O
due	O
to	O
the	O
online	O
weights	O
.	O
This	O
means	O
that	O
,	O
as	O
in	O
Q	Method
-	Method
learning	Method
,	O
we	O
are	O
still	O
estimating	O
the	O
value	O
of	O
the	O
greedy	Method
policy	Method
according	O
to	O
the	O
current	O
values	O
,	O
as	O
defined	O
by	O
.	O
However	O
,	O
we	O
use	O
the	O
second	O
set	O
of	O
weights	O
to	O
fairly	O
evaluate	O
the	O
value	O
of	O
this	O
policy	O
.	O
This	O
second	O
set	O
of	O
weights	O
can	O
be	O
updated	O
symmetrically	O
by	O
switching	O
the	O
roles	O
of	O
and	O
.	O
section	O
:	O
Overoptimism	O
due	O
to	O
estimation	O
errors	O
Q	Method
-	Method
learning	Method
’s	O
overestimations	O
were	O
first	O
investigated	O
by	O
,	O
who	O
showed	O
that	O
if	O
the	O
action	O
values	O
contain	O
random	O
errors	O
uniformly	O
distributed	O
in	O
an	O
interval	O
then	O
each	O
target	O
is	O
overestimated	O
up	O
to	O
,	O
where	O
is	O
the	O
number	O
of	O
actions	O
.	O
In	O
addition	O
,	O
give	O
a	O
concrete	O
example	O
in	O
which	O
these	O
overestimations	O
even	O
asymptotically	O
lead	O
to	O
sub	O
-	O
optimal	O
policies	O
,	O
and	O
show	O
the	O
overestimations	O
manifest	O
themselves	O
in	O
a	O
small	O
toy	Task
problem	Task
when	O
using	O
function	Method
approximation	Method
.	O
Later	O
argued	O
that	O
noise	O
in	O
the	O
environment	O
can	O
lead	O
to	O
overestimations	O
even	O
when	O
using	O
tabular	Method
representation	Method
,	O
and	O
proposed	O
Double	Method
Q	Method
-	Method
learning	Method
as	O
a	O
solution	O
.	O
In	O
this	O
section	O
we	O
demonstrate	O
more	O
generally	O
that	O
estimation	O
errors	O
of	O
any	O
kind	O
can	O
induce	O
an	O
upward	O
bias	O
,	O
regardless	O
of	O
whether	O
these	O
errors	O
are	O
due	O
to	O
environmental	O
noise	O
,	O
function	Method
approximation	Method
,	O
non	O
-	O
stationarity	O
,	O
or	O
any	O
other	O
source	O
.	O
This	O
is	O
important	O
,	O
because	O
in	O
practice	O
any	O
method	O
will	O
incur	O
some	O
inaccuracies	O
during	O
learning	Task
,	O
simply	O
due	O
to	O
the	O
fact	O
that	O
the	O
true	O
values	O
are	O
initially	O
unknown	O
.	O
The	O
result	O
by	O
cited	O
above	O
gives	O
an	O
upper	O
bound	O
to	O
the	O
overestimation	O
for	O
a	O
specific	O
setup	O
,	O
but	O
it	O
is	O
also	O
possible	O
,	O
and	O
potentially	O
more	O
interesting	O
,	O
to	O
derive	O
a	O
lower	O
bound	O
.	O
theorem	O
:	O
.	O
Consider	O
a	O
state	O
s	O
in	O
which	O
all	O
the	O
true	O
optimal	O
action	O
values	O
are	O
equal	O
at	O
=	O
⁢Q*	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
)	O
for	O
some	O
⁢V*	O
(	O
s	O
)	O
.	O
Let	O
Qt	O
be	O
arbitrary	O
value	Method
estimates	Method
that	O
are	O
on	O
the	O
whole	O
unbiased	O
in	O
the	O
sense	O
that	O
=	O
∑a	O
(-	O
⁢Qt	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
))	O
0	O
,	O
but	O
that	O
are	O
not	O
all	O
correct	O
,	O
such	O
that	O
=	O
⁢1m∑a	O
(-	O
⁢Qt	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
))	O
2C	O
for	O
some	O
>	O
C0	O
,	O
where	O
≥m2	O
is	O
the	O
number	O
of	O
actions	O
in	O
s.	O
Under	O
these	O
conditions	O
,	O
≥⁢maxaQt	O
(	O
s	O
,	O
a	O
)+	O
⁢V*	O
(	O
s	O
)	O
C	O
-	O
m1	O
.	O
This	O
lower	O
bound	O
is	O
tight	O
.	O
Under	O
the	O
same	O
conditions	O
,	O
the	O
lower	O
bound	O
on	O
the	O
absolute	Metric
error	Metric
of	O
the	O
Double	Method
Q	Method
-	Method
learning	Method
estimate	Method
is	O
zero	O
.	O
(	O
Proof	O
in	O
appendix	O
.	O
)	O
Note	O
that	O
we	O
did	O
not	O
need	O
to	O
assume	O
that	O
estimation	O
errors	O
for	O
different	O
actions	O
are	O
independent	O
.	O
This	O
theorem	O
shows	O
that	O
even	O
if	O
the	O
value	Method
estimates	Method
are	O
on	O
average	O
correct	O
,	O
estimation	O
errors	O
of	O
any	O
source	O
can	O
drive	O
the	O
estimates	O
up	O
and	O
away	O
from	O
the	O
true	O
optimal	O
values	O
.	O
The	O
lower	Metric
bound	Metric
in	O
Theorem	O
[	O
reference	O
]	O
decreases	O
with	O
the	O
number	O
of	O
actions	O
.	O
This	O
is	O
an	O
artifact	O
of	O
considering	O
the	O
lower	O
bound	O
,	O
which	O
requires	O
very	O
specific	O
values	O
to	O
be	O
attained	O
.	O
More	O
typically	O
,	O
the	O
overoptimism	O
increases	O
with	O
the	O
number	O
of	O
actions	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Q	Method
-	Method
learning	Method
’s	O
overestimations	O
there	O
indeed	O
increase	O
with	O
the	O
number	O
of	O
actions	O
,	O
while	O
Double	Method
Q	Method
-	Method
learning	Method
is	O
unbiased	O
.	O
As	O
another	O
example	O
,	O
if	O
for	O
all	O
actions	O
and	O
the	O
estimation	O
errors	O
are	O
uniformly	O
random	O
in	O
,	O
then	O
the	O
overoptimism	O
is	O
.	O
(	O
Proof	O
in	O
appendix	O
.	O
)	O
We	O
now	O
turn	O
to	O
function	Method
approximation	Method
and	O
consider	O
a	O
real	O
-	O
valued	O
continuous	O
state	O
space	O
with	O
10	O
discrete	O
actions	O
in	O
each	O
state	O
.	O
For	O
simplicity	O
,	O
the	O
true	O
optimal	O
action	O
values	O
in	O
this	O
example	O
depend	O
only	O
on	O
state	O
so	O
that	O
in	O
each	O
state	O
all	O
actions	O
have	O
the	O
same	O
true	O
value	O
.	O
These	O
true	O
values	O
are	O
shown	O
in	O
the	O
left	O
column	O
of	O
plots	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
purple	O
lines	O
)	O
and	O
are	O
defined	O
as	O
either	O
(	O
top	O
row	O
)	O
or	O
(	O
middle	O
and	O
bottom	O
rows	O
)	O
.	O
The	O
left	O
plots	O
also	O
show	O
an	O
approximation	O
for	O
a	O
single	O
action	O
(	O
green	O
lines	O
)	O
as	O
a	O
function	O
of	O
state	O
as	O
well	O
as	O
the	O
samples	O
the	O
estimate	O
is	O
based	O
on	O
(	O
green	O
dots	O
)	O
.	O
The	O
estimate	O
is	O
a	O
-	Method
degree	Method
polynomial	Method
that	O
is	O
fit	O
to	O
the	O
true	O
values	O
at	O
sampled	O
states	O
,	O
where	O
(	O
top	O
and	O
middle	O
rows	O
)	O
or	O
(	O
bottom	O
row	O
)	O
.	O
The	O
samples	O
match	O
the	O
true	O
function	O
exactly	O
:	O
there	O
is	O
no	O
noise	O
and	O
we	O
assume	O
we	O
have	O
ground	O
truth	O
for	O
the	O
action	O
value	O
on	O
these	O
sampled	O
states	O
.	O
The	O
approximation	O
is	O
inexact	O
even	O
on	O
the	O
sampled	O
states	O
for	O
the	O
top	O
two	O
rows	O
because	O
the	O
function	Method
approximation	Method
is	O
insufficiently	O
flexible	O
.	O
In	O
the	O
bottom	O
row	O
,	O
the	O
function	O
is	O
flexible	O
enough	O
to	O
fit	O
the	O
green	O
dots	O
,	O
but	O
this	O
reduces	O
the	O
accuracy	Metric
in	O
unsampled	O
states	O
.	O
Notice	O
that	O
the	O
sampled	O
states	O
are	O
spaced	O
further	O
apart	O
near	O
the	O
left	O
side	O
of	O
the	O
left	O
plots	O
,	O
resulting	O
in	O
larger	O
estimation	Metric
errors	Metric
.	O
In	O
many	O
ways	O
this	O
is	O
a	O
typical	O
learning	Task
setting	Task
,	O
where	O
at	O
each	O
point	O
in	O
time	O
we	O
only	O
have	O
limited	Material
data	Material
.	O
The	O
middle	O
column	O
of	O
plots	O
in	O
Figure	O
[	O
reference	O
]	O
shows	O
estimated	O
action	O
value	O
functions	O
for	O
all	O
10	O
actions	O
(	O
green	O
lines	O
)	O
,	O
as	O
functions	O
of	O
state	O
,	O
along	O
with	O
the	O
maximum	O
action	O
value	O
in	O
each	O
state	O
(	O
black	O
dashed	O
line	O
)	O
.	O
Although	O
the	O
true	O
value	O
function	O
is	O
the	O
same	O
for	O
all	O
actions	O
,	O
the	O
approximations	O
differ	O
because	O
we	O
have	O
supplied	O
different	O
sets	O
of	O
sampled	O
states	O
.	O
The	O
maximum	O
is	O
often	O
higher	O
than	O
the	O
ground	O
truth	O
shown	O
in	O
purple	O
on	O
the	O
left	O
.	O
This	O
is	O
confirmed	O
in	O
the	O
right	O
plots	O
,	O
which	O
shows	O
the	O
difference	O
between	O
the	O
black	O
and	O
purple	O
curves	O
in	O
orange	O
.	O
The	O
orange	O
line	O
is	O
almost	O
always	O
positive	O
,	O
indicating	O
an	O
upward	O
bias	O
.	O
The	O
right	O
plots	O
also	O
show	O
the	O
estimates	O
from	O
Double	Method
Q	Method
-	Method
learning	Method
in	O
blue	O
,	O
which	O
are	O
on	O
average	O
much	O
closer	O
to	O
zero	O
.	O
This	O
demonstrates	O
that	O
Double	Method
Q	Method
-	Method
learning	Method
indeed	O
can	O
successfully	O
reduce	O
the	O
overoptimism	O
of	O
Q	Method
-	Method
learning	Method
.	O
The	O
different	O
rows	O
in	O
Figure	O
[	O
reference	O
]	O
show	O
variations	O
of	O
the	O
same	O
experiment	O
.	O
The	O
difference	O
between	O
the	O
top	O
and	O
middle	O
rows	O
is	O
the	O
true	O
value	O
function	O
,	O
demonstrating	O
that	O
overestimations	O
are	O
not	O
an	O
artifact	O
of	O
a	O
specific	O
true	O
value	O
function	O
.	O
The	O
difference	O
between	O
the	O
middle	O
and	O
bottom	O
rows	O
is	O
the	O
flexibility	O
of	O
the	O
function	Method
approximation	Method
.	O
In	O
the	O
left	O
-	O
middle	O
plot	O
,	O
the	O
estimates	O
are	O
even	O
incorrect	O
for	O
some	O
of	O
the	O
sampled	O
states	O
because	O
the	O
function	O
is	O
insufficiently	O
flexible	O
.	O
The	O
function	O
in	O
the	O
bottom	O
-	O
left	O
plot	O
is	O
more	O
flexible	O
but	O
this	O
causes	O
higher	O
estimation	Metric
errors	Metric
for	O
unseen	O
states	O
,	O
resulting	O
in	O
higher	O
overestimations	O
.	O
This	O
is	O
important	O
because	O
flexible	O
parametric	Method
function	Method
approximators	Method
are	O
often	O
employed	O
in	O
reinforcement	Task
learning	Task
(	O
see	O
,	O
e.g.	O
,	O
)	O
.	O
In	O
contrast	O
to	O
we	O
did	O
not	O
use	O
a	O
statistical	Method
argument	Method
to	O
find	O
overestimations	O
,	O
the	O
process	O
to	O
obtain	O
Figure	O
[	O
reference	O
]	O
is	O
fully	O
deterministic	O
.	O
In	O
contrast	O
to	O
,	O
we	O
did	O
not	O
rely	O
on	O
inflexible	Method
function	Method
approximation	Method
with	O
irreducible	O
asymptotic	O
errors	O
;	O
the	O
bottom	O
row	O
shows	O
that	O
a	O
function	O
that	O
is	O
flexible	O
enough	O
to	O
cover	O
all	O
samples	O
leads	O
to	O
high	O
overestimations	O
.	O
This	O
indicates	O
that	O
the	O
overestimations	O
can	O
occur	O
quite	O
generally	O
.	O
In	O
the	O
examples	O
above	O
,	O
overestimations	O
occur	O
even	O
when	O
assuming	O
we	O
have	O
samples	O
of	O
the	O
true	O
action	O
value	O
at	O
certain	O
states	O
.	O
The	O
value	Method
estimates	Method
can	O
further	O
deteriorate	O
if	O
we	O
bootstrap	O
off	O
of	O
action	O
values	O
that	O
are	O
already	O
overoptimistic	O
,	O
since	O
this	O
causes	O
overestimations	O
to	O
propagate	O
throughout	O
our	O
estimates	O
.	O
Although	O
uniformly	O
overestimating	O
values	O
might	O
not	O
hurt	O
the	O
resulting	O
policy	O
,	O
in	O
practice	O
overestimation	Metric
errors	Metric
will	O
differ	O
for	O
different	O
states	O
and	O
actions	O
.	O
Overestimation	Method
combined	O
with	O
bootstrapping	Method
then	O
has	O
the	O
pernicious	O
effect	O
of	O
propagating	O
the	O
wrong	O
relative	O
information	O
about	O
which	O
states	O
are	O
more	O
valuable	O
than	O
others	O
,	O
directly	O
affecting	O
the	O
quality	O
of	O
the	O
learned	O
policies	O
.	O
The	O
overestimations	O
should	O
not	O
be	O
confused	O
with	O
optimism	O
in	O
the	O
face	O
of	O
uncertainty	O
,	O
where	O
an	O
exploration	O
bonus	O
is	O
given	O
to	O
states	O
or	O
actions	O
with	O
uncertain	O
values	O
.	O
Conversely	O
,	O
the	O
overestimations	O
discussed	O
here	O
occur	O
only	O
after	O
updating	O
,	O
resulting	O
in	O
overoptimism	O
in	O
the	O
face	O
of	O
apparent	O
certainty	O
.	O
This	O
was	O
already	O
observed	O
by	O
,	O
who	O
noted	O
that	O
,	O
in	O
contrast	O
to	O
optimism	O
in	O
the	O
face	O
of	O
uncertainty	O
,	O
these	O
overestimations	O
actually	O
can	O
impede	O
learning	O
an	O
optimal	Method
policy	Method
.	O
We	O
will	O
see	O
this	O
negative	O
effect	O
on	O
policy	Metric
quality	Metric
confirmed	O
later	O
in	O
the	O
experiments	O
as	O
well	O
:	O
when	O
we	O
reduce	O
the	O
overestimations	O
using	O
Double	Method
Q	Method
-	Method
learning	Method
,	O
the	O
policies	O
improve	O
.	O
section	O
:	O
Double	Method
DQN	Method
The	O
idea	O
of	O
Double	Method
Q	Method
-	Method
learning	Method
is	O
to	O
reduce	O
overestimations	O
by	O
decomposing	O
the	O
max	Method
operation	Method
in	O
the	O
target	O
into	O
action	Task
selection	Task
and	O
action	Task
evaluation	Task
.	O
Although	O
not	O
fully	O
decoupled	O
,	O
the	O
target	O
network	O
in	O
the	O
DQN	Method
architecture	Method
provides	O
a	O
natural	O
candidate	O
for	O
the	O
second	O
value	O
function	O
,	O
without	O
having	O
to	O
introduce	O
additional	O
networks	O
.	O
We	O
therefore	O
propose	O
to	O
evaluate	O
the	O
greedy	Method
policy	Method
according	O
to	O
the	O
online	Task
network	Task
,	O
but	O
using	O
the	O
target	O
network	O
to	O
estimate	O
its	O
value	O
.	O
In	O
reference	O
to	O
both	O
Double	Method
Q	Method
-	Method
learning	Method
and	O
DQN	Method
,	O
we	O
refer	O
to	O
the	O
resulting	O
algorithm	O
as	O
Double	Method
DQN	Method
.	O
Its	O
update	O
is	O
the	O
same	O
as	O
for	O
DQN	Method
,	O
but	O
replacing	O
the	O
target	O
with	O
In	O
comparison	O
to	O
Double	Method
Q	Method
-	Method
learning	Method
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
weights	O
of	O
the	O
second	O
network	O
are	O
replaced	O
with	O
the	O
weights	O
of	O
the	O
target	O
network	O
for	O
the	O
evaluation	O
of	O
the	O
current	O
greedy	Method
policy	Method
.	O
The	O
update	O
to	O
the	O
target	O
network	O
stays	O
unchanged	O
from	O
DQN	Method
,	O
and	O
remains	O
a	O
periodic	O
copy	O
of	O
the	O
online	Method
network	Method
.	O
This	O
version	O
of	O
Double	Method
DQN	Method
is	O
perhaps	O
the	O
minimal	O
possible	O
change	O
to	O
DQN	Method
towards	O
Double	Method
Q	Method
-	Method
learning	Method
.	O
The	O
goal	O
is	O
to	O
get	O
most	O
of	O
the	O
benefit	O
of	O
Double	Method
Q	Method
-	Method
learning	Method
,	O
while	O
keeping	O
the	O
rest	O
of	O
the	O
DQN	Method
algorithm	Method
intact	O
for	O
a	O
fair	O
comparison	O
,	O
and	O
with	O
minimal	O
computational	Metric
overhead	Metric
.	O
section	O
:	O
Empirical	O
results	O
In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
overestimations	O
of	O
DQN	Method
and	O
show	O
that	O
Double	Method
DQN	Method
improves	O
over	O
DQN	Method
both	O
in	O
terms	O
of	O
value	Metric
accuracy	Metric
and	O
in	O
terms	O
of	O
policy	Metric
quality	Metric
.	O
To	O
further	O
test	O
the	O
robustness	O
of	O
the	O
approach	O
we	O
additionally	O
evaluate	O
the	O
algorithms	O
with	O
random	O
starts	O
generated	O
from	O
expert	O
human	O
trajectories	O
,	O
as	O
proposed	O
by	O
.	O
Our	O
testbed	O
consists	O
of	O
Atari	Material
2600	Material
games	Material
,	O
using	O
the	O
Arcade	Method
Learning	Method
Environment	Method
.	O
The	O
goal	O
is	O
for	O
a	O
single	O
algorithm	O
,	O
with	O
a	O
fixed	O
set	O
of	O
hyperparameters	O
,	O
to	O
learn	O
to	O
play	O
each	O
of	O
the	O
games	O
separately	O
from	O
interaction	O
given	O
only	O
the	O
screen	O
pixels	O
as	O
input	O
.	O
This	O
is	O
a	O
demanding	O
testbed	O
:	O
not	O
only	O
are	O
the	O
inputs	O
high	O
-	O
dimensional	O
,	O
the	O
game	O
visuals	O
and	O
game	O
mechanics	O
vary	O
substantially	O
between	O
games	O
.	O
Good	O
solutions	O
must	O
therefore	O
rely	O
heavily	O
on	O
the	O
learning	Method
algorithm	Method
—	O
it	O
is	O
not	O
practically	O
feasible	O
to	O
overfit	O
the	O
domain	O
by	O
relying	O
only	O
on	O
tuning	Method
.	O
We	O
closely	O
follow	O
the	O
experimental	O
setting	O
and	O
network	Method
architecture	Method
outlined	O
by	O
.	O
Briefly	O
,	O
the	O
network	Method
architecture	Method
is	O
a	O
convolutional	Method
neural	Method
network	Method
with	O
3	O
convolution	Method
layers	Method
and	O
a	O
fully	Method
-	Method
connected	Method
hidden	Method
layer	Method
(	O
approximately	O
1.5	O
M	O
parameters	O
in	O
total	O
)	O
.	O
The	O
network	O
takes	O
the	O
last	O
four	O
frames	O
as	O
input	O
and	O
outputs	O
the	O
action	O
value	O
of	O
each	O
action	O
.	O
On	O
each	O
game	O
,	O
the	O
network	O
is	O
trained	O
on	O
a	O
single	O
GPU	Method
for	O
200	O
M	O
frames	O
,	O
or	O
approximately	O
1	O
week	O
.	O
subsection	O
:	O
Results	O
on	O
overoptimism	O
Figure	O
[	O
reference	O
]	O
shows	O
examples	O
of	O
DQN	Method
’s	Method
overestimations	Method
in	O
six	O
Atari	Task
games	Task
.	O
DQN	Method
and	O
Double	Method
DQN	Method
were	O
both	O
trained	O
under	O
the	O
exact	O
conditions	O
described	O
by	O
.	O
DQN	Method
is	O
consistently	O
and	O
sometimes	O
vastly	O
overoptimistic	O
about	O
the	O
value	O
of	O
the	O
current	O
greedy	Method
policy	Method
,	O
as	O
can	O
be	O
seen	O
by	O
comparing	O
the	O
orange	O
learning	O
curves	O
in	O
the	O
top	O
row	O
of	O
plots	O
to	O
the	O
straight	O
orange	O
lines	O
,	O
which	O
represent	O
the	O
actual	O
discounted	O
value	O
of	O
the	O
best	O
learned	Method
policy	Method
.	O
More	O
precisely	O
,	O
the	O
(	O
averaged	O
)	O
value	Method
estimates	Method
are	O
computed	O
regularly	O
during	O
training	O
with	O
full	O
evaluation	O
phases	O
of	O
length	O
steps	O
as	O
The	O
ground	O
truth	O
averaged	O
values	O
are	O
obtained	O
by	O
running	O
the	O
best	O
learned	O
policies	O
for	O
several	O
episodes	O
and	O
computing	O
the	O
actual	O
cumulative	O
rewards	O
.	O
Without	O
overestimations	O
we	O
would	O
expect	O
these	O
quantities	O
to	O
match	O
up	O
(	O
i.e.	O
,	O
the	O
curve	O
to	O
match	O
the	O
straight	O
line	O
at	O
the	O
right	O
of	O
each	O
plot	O
)	O
.	O
Instead	O
,	O
the	O
learning	O
curves	O
of	O
DQN	Method
consistently	O
end	O
up	O
much	O
higher	O
than	O
the	O
true	O
values	O
.	O
The	O
learning	O
curves	O
for	O
Double	Task
DQN	Task
,	O
shown	O
in	O
blue	O
,	O
are	O
much	O
closer	O
to	O
the	O
blue	O
straight	O
line	O
representing	O
the	O
true	O
value	O
of	O
the	O
final	O
policy	O
.	O
Note	O
that	O
the	O
blue	O
straight	O
line	O
is	O
often	O
higher	O
than	O
the	O
orange	O
straight	O
line	O
.	O
This	O
indicates	O
that	O
Double	Method
DQN	Method
does	O
not	O
just	O
produce	O
more	O
accurate	O
value	Method
estimates	Method
but	O
also	O
better	O
policies	O
.	O
More	O
extreme	O
overestimations	O
are	O
shown	O
in	O
the	O
middle	O
two	O
plots	O
,	O
where	O
DQN	Method
is	O
highly	O
unstable	O
on	O
the	O
games	O
Asterix	O
and	O
Wizard	O
of	O
Wor	O
.	O
Notice	O
the	O
log	O
scale	O
for	O
the	O
values	O
on	O
the	O
-	O
axis	O
.	O
The	O
bottom	O
two	O
plots	O
shows	O
the	O
corresponding	O
scores	O
for	O
these	O
two	O
games	O
.	O
Notice	O
that	O
the	O
increases	O
in	O
value	O
estimates	O
for	O
DQN	O
in	O
the	O
middle	O
plots	O
coincide	O
with	O
decreasing	O
scores	O
in	O
bottom	O
plots	O
.	O
Again	O
,	O
this	O
indicates	O
that	O
the	O
overestimations	O
are	O
harming	O
the	O
quality	O
of	O
the	O
resulting	O
policies	O
.	O
If	O
seen	O
in	O
isolation	O
,	O
one	O
might	O
perhaps	O
be	O
tempted	O
to	O
think	O
the	O
observed	O
instability	O
is	O
related	O
to	O
inherent	O
instability	Task
problems	Task
of	O
off	Method
-	Method
policy	Method
learning	Method
with	O
function	Method
approximation	Method
.	O
However	O
,	O
we	O
see	O
that	O
learning	Task
is	O
much	O
more	O
stable	O
with	O
Double	O
DQN	Method
,	O
suggesting	O
that	O
the	O
cause	O
for	O
these	O
instabilities	O
is	O
in	O
fact	O
Q	Method
-	Method
learning	Method
’s	O
overoptimism	O
.	O
Figure	O
[	O
reference	O
]	O
only	O
shows	O
a	O
few	O
examples	O
,	O
but	O
overestimations	O
were	O
observed	O
for	O
DQN	Method
in	O
all	O
49	O
tested	O
Atari	Material
games	Material
,	O
albeit	O
in	O
varying	O
amounts	O
.	O
subsection	O
:	O
Quality	O
of	O
the	O
learned	O
policies	O
Overoptimism	Method
does	O
not	O
always	O
adversely	O
affect	O
the	O
quality	O
of	O
the	O
learned	Method
policy	Method
.	O
For	O
example	O
,	O
DQN	Method
achieves	O
optimal	O
behavior	O
in	O
Pong	Task
despite	O
slightly	O
overestimating	O
the	O
policy	O
value	O
.	O
Nevertheless	O
,	O
reducing	O
overestimations	O
can	O
significantly	O
benefit	O
the	O
stability	O
of	O
learning	Task
;	O
we	O
see	O
clear	O
examples	O
of	O
this	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
now	O
assess	O
more	O
generally	O
how	O
much	O
Double	O
DQN	Method
helps	O
in	O
terms	O
of	O
policy	Metric
quality	Metric
by	O
evaluating	O
on	O
all	O
49	O
games	O
that	O
DQN	Method
was	O
tested	O
on	O
.	O
As	O
described	O
by	O
each	O
evaluation	O
episode	O
starts	O
by	O
executing	O
a	O
special	O
no	Method
-	Method
op	Method
action	Method
that	O
does	O
not	O
affect	O
the	O
environment	O
up	O
to	O
30	O
times	O
,	O
to	O
provide	O
different	O
starting	O
points	O
for	O
the	O
agent	O
.	O
Some	O
exploration	O
during	O
evaluation	Task
provides	O
additional	O
randomization	O
.	O
For	O
Double	Task
DQN	Task
we	O
used	O
the	O
exact	O
same	O
hyper	O
-	O
parameters	O
as	O
for	O
DQN	Method
,	O
to	O
allow	O
for	O
a	O
controlled	O
experiment	O
focused	O
just	O
on	O
reducing	O
overestimations	O
.	O
The	O
learned	O
policies	O
are	O
evaluated	O
for	O
5	O
mins	O
of	O
emulator	O
time	O
(	O
18	O
,	O
000	O
frames	O
)	O
with	O
an	O
-	Method
greedy	Method
policy	Method
where	O
.	O
The	O
scores	O
are	O
averaged	O
over	O
100	O
episodes	O
.	O
The	O
only	O
difference	O
between	O
Double	O
DQN	Method
and	O
DQN	Method
is	O
the	O
target	O
,	O
using	O
rather	O
than	O
.	O
This	O
evaluation	O
is	O
somewhat	O
adversarial	O
,	O
as	O
the	O
used	O
hyper	O
-	O
parameters	O
were	O
tuned	O
for	O
DQN	Method
but	O
not	O
for	O
Double	Task
DQN	Task
.	O
To	O
obtain	O
summary	O
statistics	O
across	O
games	O
,	O
we	O
normalize	O
the	O
score	O
for	O
each	O
game	O
as	O
follows	O
:	O
The	O
‘	O
random	O
’	O
and	O
‘	O
human	O
’	O
scores	O
are	O
the	O
same	O
as	O
used	O
by	O
,	O
and	O
are	O
given	O
in	O
the	O
appendix	O
.	O
Table	O
[	O
reference	O
]	O
,	O
under	O
no	O
ops	O
,	O
shows	O
that	O
on	O
the	O
whole	O
Double	O
DQN	Method
clearly	O
improves	O
over	O
DQN	Method
.	O
A	O
detailed	O
comparison	O
(	O
in	O
appendix	O
)	O
shows	O
that	O
there	O
are	O
several	O
games	O
in	O
which	O
Double	Method
DQN	Method
greatly	O
improves	O
upon	O
DQN	Method
.	O
Noteworthy	O
examples	O
include	O
Road	Task
Runner	Task
(	O
from	O
233	O
%	O
to	O
617	O
%	O
)	O
,	O
Asterix	Method
(	O
from	O
70	O
%	O
to	O
180	O
%	O
)	O
,	O
Zaxxon	O
(	O
from	O
54	O
%	O
to	O
111	O
%	O
)	O
,	O
and	O
Double	Method
Dunk	Method
(	O
from	O
17	O
%	O
to	O
397	O
%	O
)	O
.	O
The	O
Gorila	Method
algorithm	Method
,	O
which	O
is	O
a	O
massively	Method
distributed	Method
version	Method
of	Method
DQN	Method
,	O
is	O
not	O
included	O
in	O
the	O
table	O
because	O
the	O
architecture	O
and	O
infrastructure	O
is	O
sufficiently	O
different	O
to	O
make	O
a	O
direct	O
comparison	O
unclear	O
.	O
For	O
completeness	O
,	O
we	O
note	O
that	O
Gorila	O
obtained	O
median	O
and	O
mean	Metric
normalized	Metric
scores	Metric
of	O
96	O
%	O
and	O
495	O
%	O
,	O
respectively	O
.	O
subsection	O
:	O
Robustness	Metric
to	O
Human	O
starts	O
One	O
concern	O
with	O
the	O
previous	O
evaluation	O
is	O
that	O
in	O
deterministic	Task
games	Task
with	O
a	O
unique	O
starting	O
point	O
the	O
learner	O
could	O
potentially	O
learn	O
to	O
remember	O
sequences	O
of	O
actions	O
without	O
much	O
need	O
to	O
generalize	O
.	O
While	O
successful	O
,	O
the	O
solution	O
would	O
not	O
be	O
particularly	O
robust	O
.	O
By	O
testing	O
the	O
agents	O
from	O
various	O
starting	O
points	O
,	O
we	O
can	O
test	O
whether	O
the	O
found	O
solutions	O
generalize	O
well	O
,	O
and	O
as	O
such	O
provide	O
a	O
challenging	O
testbed	O
for	O
the	O
learned	O
polices	Task
.	O
We	O
obtained	O
100	O
starting	O
points	O
sampled	O
for	O
each	O
game	O
from	O
a	O
human	O
expert	O
’s	O
trajectory	O
,	O
as	O
proposed	O
by	O
.	O
We	O
start	O
an	O
evaluation	O
episode	O
from	O
each	O
of	O
these	O
starting	O
points	O
and	O
run	O
the	O
emulator	Method
for	O
up	O
to	O
108	O
,	O
000	O
frames	O
(	O
30	O
mins	O
at	O
60Hz	O
including	O
the	O
trajectory	O
before	O
the	O
starting	O
point	O
)	O
.	O
Each	O
agent	O
is	O
only	O
evaluated	O
on	O
the	O
rewards	O
accumulated	O
after	O
the	O
starting	O
point	O
.	O
For	O
this	O
evaluation	O
we	O
include	O
a	O
tuned	O
version	O
of	O
Double	Method
DQN	Method
.	O
Some	O
tuning	O
is	O
appropriate	O
because	O
the	O
hyperparameters	O
were	O
tuned	O
for	O
DQN	Method
,	O
which	O
is	O
a	O
different	O
algorithm	O
.	O
For	O
the	O
tuned	Method
version	Method
of	O
Double	Method
DQN	Method
,	O
we	O
increased	O
the	O
number	O
of	O
frames	O
between	O
each	O
two	O
copies	O
of	O
the	O
target	O
network	O
from	O
10	O
,	O
000	O
to	O
30	O
,	O
000	O
,	O
to	O
reduce	O
overestimations	O
further	O
because	O
immediately	O
after	O
each	O
switch	Method
DQN	Method
and	O
Double	Method
DQN	Method
both	O
revert	O
to	O
Q	Method
-	Method
learning	Method
.	O
In	O
addition	O
,	O
we	O
reduced	O
the	O
exploration	O
during	O
learning	Task
from	O
to	O
,	O
and	O
then	O
used	O
during	O
evaluation	Task
.	O
Finally	O
,	O
the	O
tuned	O
version	O
uses	O
a	O
single	O
shared	O
bias	O
for	O
all	O
action	O
values	O
in	O
the	O
top	O
layer	O
of	O
the	O
network	O
.	O
Each	O
of	O
these	O
changes	O
improved	O
performance	O
and	O
together	O
they	O
result	O
in	O
clearly	O
better	O
results	O
.	O
Table	O
[	O
reference	O
]	O
reports	O
summary	O
statistics	O
for	O
this	O
evaluation	O
on	O
the	O
49	O
games	O
from	O
.	O
Double	Method
DQN	Method
obtains	O
clearly	O
higher	O
median	O
and	O
mean	Metric
scores	Metric
.	O
Again	O
Gorila	Method
DQN	Method
is	O
not	O
included	O
in	O
the	O
table	O
,	O
but	O
for	O
completeness	O
note	O
it	O
obtained	O
a	O
median	O
of	O
78	O
%	O
and	O
a	O
mean	O
of	O
259	O
%	O
.	O
Detailed	O
results	O
,	O
plus	O
results	O
for	O
an	O
additional	O
8	O
games	O
,	O
are	O
available	O
in	O
Figure	O
[	O
reference	O
]	O
and	O
in	O
the	O
appendix	O
.	O
On	O
several	O
games	O
the	O
improvements	O
from	O
DQN	Method
to	O
Double	Method
DQN	Method
are	O
striking	O
,	O
in	O
some	O
cases	O
bringing	O
scores	O
much	O
closer	O
to	O
human	O
,	O
or	O
even	O
surpassing	O
these	O
.	O
Double	Method
DQN	Method
appears	O
more	O
robust	O
to	O
this	O
more	O
challenging	O
evaluation	O
,	O
suggesting	O
that	O
appropriate	O
generalizations	O
occur	O
and	O
that	O
the	O
found	O
solutions	O
do	O
not	O
exploit	O
the	O
determinism	O
of	O
the	O
environments	O
.	O
This	O
is	O
appealing	O
,	O
as	O
it	O
indicates	O
progress	O
towards	O
finding	O
general	O
solutions	O
rather	O
than	O
a	O
deterministic	O
sequence	O
of	O
steps	O
that	O
would	O
be	O
less	O
robust	O
.	O
section	O
:	O
Discussion	O
This	O
paper	O
has	O
five	O
contributions	O
.	O
First	O
,	O
we	O
have	O
shown	O
why	O
Q	Method
-	Method
learning	Method
can	O
be	O
overoptimistic	O
in	O
large	Task
-	Task
scale	Task
problems	Task
,	O
even	O
if	O
these	O
are	O
deterministic	O
,	O
due	O
to	O
the	O
inherent	O
estimation	Method
errors	Method
of	Method
learning	Method
.	O
Second	O
,	O
by	O
analyzing	O
the	O
value	Method
estimates	Method
on	O
Atari	Task
games	Task
we	O
have	O
shown	O
that	O
these	O
overestimations	O
are	O
more	O
common	O
and	O
severe	O
in	O
practice	O
than	O
previously	O
acknowledged	O
.	O
Third	O
,	O
we	O
have	O
shown	O
that	O
Double	Method
Q	Method
-	Method
learning	Method
can	O
be	O
used	O
at	O
scale	O
to	O
successfully	O
reduce	O
this	O
overoptimism	O
,	O
resulting	O
in	O
more	O
stable	O
and	O
reliable	O
learning	Task
.	O
Fourth	O
,	O
we	O
have	O
proposed	O
a	O
specific	O
implementation	O
called	O
Double	Method
DQN	Method
,	O
that	O
uses	O
the	O
existing	O
architecture	Method
and	O
deep	Method
neural	Method
network	Method
of	O
the	O
DQN	Method
algorithm	Method
without	O
requiring	O
additional	O
networks	O
or	O
parameters	O
.	O
Finally	O
,	O
we	O
have	O
shown	O
that	O
Double	Method
DQN	Method
finds	O
better	O
policies	O
,	O
obtaining	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
Atari	Material
2600	Material
domain	Material
.	O
section	O
:	O
Acknowledgments	O
We	O
would	O
like	O
to	O
thank	O
Tom	O
Schaul	O
,	O
Volodymyr	O
Mnih	Method
,	O
Marc	O
Bellemare	O
,	O
Thomas	O
Degris	O
,	O
Georg	O
Ostrovski	O
,	O
and	O
Richard	O
Sutton	O
for	O
helpful	O
comments	O
,	O
and	O
everyone	O
at	O
Google	O
DeepMind	O
for	O
a	O
constructive	O
research	O
environment	O
.	O
bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
theorem	O
:	O
.	O
Consider	O
a	O
state	O
s	O
in	O
which	O
all	O
the	O
true	O
optimal	O
action	O
values	O
are	O
equal	O
at	O
=	O
⁢Q*	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
)	O
for	O
some	O
⁢V*	O
(	O
s	O
)	O
.	O
Let	O
Qt	O
be	O
arbitrary	O
value	Method
estimates	Method
that	O
are	O
on	O
the	O
whole	O
unbiased	O
in	O
the	O
sense	O
that	O
=	O
∑a	O
(-	O
⁢Qt	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
))	O
0	O
,	O
but	O
that	O
are	O
not	O
all	O
zero	O
,	O
such	O
that	O
=	O
⁢1m∑a	O
(-	O
⁢Qt	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
))	O
2C	O
for	O
some	O
>	O
C0	O
,	O
where	O
≥m2	O
is	O
the	O
number	O
of	O
actions	O
in	O
s.	O
Under	O
these	O
conditions	O
,	O
≥⁢maxaQt	O
(	O
s	O
,	O
a	O
)+	O
⁢V*	O
(	O
s	O
)	O
C	O
-	O
m1	O
.	O
This	O
lower	O
bound	O
is	O
tight	O
.	O
Under	O
the	O
same	O
conditions	O
,	O
the	O
lower	O
bound	O
on	O
the	O
absolute	Metric
error	Metric
of	O
the	O
Double	Method
Q	Method
-	Method
learning	Method
estimate	Method
is	O
zero	O
.	O
proof	O
:	O
Proof	O
of	O
Theorem	O
1	O
.	O
Define	O
the	O
errors	O
for	O
each	O
action	O
as	O
.	O
Suppose	O
that	O
there	O
exists	O
a	O
setting	O
of	O
such	O
that	O
.	O
Let	O
be	O
the	O
set	O
of	O
positive	O
of	O
size	O
,	O
and	O
the	O
set	O
of	O
strictly	O
negative	O
of	O
size	O
(	O
such	O
that	O
)	O
.	O
If	O
,	O
then	O
,	O
which	O
contradicts	O
.	O
Hence	O
,	O
it	O
must	O
be	O
that	O
.	O
Then	O
,	O
,	O
and	O
therefore	O
(	O
using	O
the	O
constraint	O
)	O
we	O
also	O
have	O
that	O
.	O
This	O
implies	O
.	O
By	O
Hölder	O
’s	O
inequality	O
,	O
then	O
We	O
can	O
now	O
combine	O
these	O
relations	O
to	O
compute	O
an	O
upper	O
-	O
bound	O
on	O
the	O
sum	O
of	O
squares	O
for	O
all	O
:	O
This	O
contradicts	O
the	O
assumption	O
that	O
,	O
and	O
therefore	O
for	O
all	O
settings	O
of	O
that	O
satisfy	O
the	O
constraints	O
.	O
We	O
can	O
check	O
that	O
the	O
lower	O
-	O
bound	O
is	O
tight	O
by	O
setting	O
for	O
and	O
.	O
This	O
verifies	O
and	O
.	O
The	O
only	O
tight	O
lower	Metric
bound	Metric
on	O
the	O
absolute	Metric
error	Metric
for	O
Double	Method
Q	Method
-	Method
learning	Method
is	O
zero	O
.	O
This	O
can	O
be	O
seen	O
by	O
because	O
we	O
can	O
have	O
and	O
Then	O
the	O
conditions	O
of	O
the	O
theorem	O
hold	O
.	O
If	O
then	O
,	O
furthermore	O
,	O
we	O
have	O
then	O
the	O
error	O
is	O
zero	O
.	O
The	O
remaining	O
action	O
values	O
,	O
for	O
,	O
are	O
arbitrary	O
.	O
∎	O
theorem	O
:	O
.	O
Consider	O
a	O
state	O
s	O
in	O
which	O
all	O
the	O
true	O
optimal	O
action	O
values	O
are	O
equal	O
at	O
=	O
⁢Q*	O
(	O
s	O
,	O
a	O
)	O
⁢V*	O
(	O
s	O
)	O
.	O
Suppose	O
that	O
the	O
estimation	O
errors	O
-	O
⁢Qt	O
(	O
s	O
,	O
a	O
)	O
⁢Q*	O
(	O
s	O
,	O
a	O
)	O
are	O
independently	O
distributed	O
uniformly	O
randomly	O
in	O
[	O
-	O
1	O
,	O
1	O
]	O
.	O
Then	O
,	O
proof	O
:	O
Proof	O
.	O
Define	O
;	O
this	O
is	O
a	O
uniform	O
random	O
variable	O
in	O
.	O
The	O
probability	O
that	O
for	O
some	O
is	O
equal	O
to	O
the	O
probability	O
that	O
for	O
all	O
simultaneously	O
.	O
Because	O
the	O
estimation	O
errors	O
are	O
independent	O
,	O
we	O
can	O
derive	O
The	O
function	O
is	O
the	O
cumulative	Method
distribution	Method
function	Method
(	O
CDF	Method
)	O
of	O
,	O
which	O
here	O
is	O
simply	O
defined	O
as	O
This	O
implies	O
that	O
This	O
gives	O
us	O
the	O
CDF	O
of	O
the	O
random	O
variable	O
.	O
Its	O
expectation	O
can	O
be	O
written	O
as	O
an	O
integral	O
where	O
is	O
the	O
probability	O
density	O
function	O
of	O
this	O
variable	O
,	O
defined	O
as	O
the	O
derivative	O
of	O
the	O
CDF	O
:	O
,	O
so	O
that	O
for	O
we	O
have	O
.	O
Evaluating	O
the	O
integral	O
yields	O
section	O
:	O
Experimental	O
Details	O
for	O
the	O
Atari	Task
2600	Task
Domain	Task
We	O
selected	O
the	O
49	O
games	O
to	O
match	O
the	O
list	O
used	O
by	O
,	O
see	O
Tables	O
below	O
for	O
the	O
full	O
list	O
.	O
Each	O
agent	O
step	O
is	O
composed	O
of	O
four	O
frames	O
(	O
the	O
last	O
selected	O
action	O
is	O
repeated	O
during	O
these	O
frames	O
)	O
and	O
reward	O
values	O
(	O
obtained	O
from	O
the	O
Arcade	Method
Learning	Method
Environment	Method
)	O
are	O
clipped	O
between	O
-	O
1	O
and	O
1	O
.	O
subsection	O
:	O
Network	Method
Architecture	Method
The	O
convolution	Method
network	Method
used	O
in	O
the	O
experiment	O
is	O
exactly	O
the	O
one	O
proposed	O
by	O
proposed	O
by	O
,	O
we	O
only	O
provide	O
details	O
here	O
for	O
completeness	O
.	O
The	O
input	O
to	O
the	O
network	O
is	O
a	O
84x84x4	O
tensor	O
containing	O
a	O
rescaled	O
,	O
and	O
gray	O
-	O
scale	O
,	O
version	O
of	O
the	O
last	O
four	O
frames	O
.	O
The	O
first	O
convolution	Method
layer	Method
convolves	O
the	O
input	O
with	O
32	O
filters	O
of	O
size	O
8	O
(	O
stride	O
4	O
)	O
,	O
the	O
second	O
layer	O
has	O
64	O
layers	O
of	O
size	O
4	O
(	O
stride	O
2	O
)	O
,	O
the	O
final	O
convolution	Method
layer	Method
has	O
64	O
filters	O
of	O
size	O
3	O
(	O
stride	O
1	O
)	O
.	O
This	O
is	O
followed	O
by	O
a	O
fully	Method
-	Method
connected	Method
hidden	Method
layer	Method
of	Method
512	Method
units	Method
.	O
All	O
these	O
layers	O
are	O
separated	O
by	O
Rectifier	Method
Linear	Method
Units	Method
(	O
ReLu	Method
)	O
.	O
Finally	O
,	O
a	O
fully	Method
-	Method
connected	Method
linear	Method
layer	Method
projects	O
to	O
the	O
output	O
of	O
the	O
network	O
,	O
i.e.	O
,	O
the	O
Q	O
-	O
values	O
.	O
The	O
optimization	Task
employed	O
to	O
train	O
the	O
network	O
is	O
RMSProp	Method
(	O
with	O
momentum	O
parameter	O
)	O
.	O
subsection	O
:	O
Hyper	O
-	O
parameters	O
In	O
all	O
experiments	O
,	O
the	O
discount	O
was	O
set	O
to	O
,	O
and	O
the	O
learning	O
rate	O
to	O
.	O
The	O
number	O
of	O
steps	O
between	O
target	O
network	O
updates	O
was	O
.	O
Training	Task
is	O
done	O
over	O
50	O
M	O
steps	O
(	O
i.e.	O
,	O
200	O
M	O
frames	O
)	O
.	O
The	O
agent	O
is	O
evaluated	O
every	O
1	O
M	O
steps	O
,	O
and	O
the	O
best	O
policy	O
across	O
these	O
evaluations	O
is	O
kept	O
as	O
the	O
output	O
of	O
the	O
learning	Method
process	Method
.	O
The	O
size	O
of	O
the	O
experience	O
replay	O
memory	O
is	O
1	O
M	O
tuples	O
.	O
The	O
memory	O
gets	O
sampled	O
to	O
update	O
the	O
network	O
every	O
4	O
steps	O
with	O
minibatches	O
of	O
size	O
32	O
.	O
The	O
simple	O
exploration	Method
policy	Method
used	O
is	O
an	O
-	Method
greedy	Method
policy	Method
with	O
the	O
decreasing	O
linearly	O
from	O
1	O
to	O
over	O
1	O
M	O
steps	O
.	O
section	O
:	O
Supplementary	O
Results	O
in	O
the	O
Atari	Material
2600	Material
Domain	Material
The	O
Tables	O
below	O
provide	O
further	O
detailed	O
results	O
for	O
our	O
experiments	O
in	O
the	O
Atari	Material
domain	Material
.	O
