Pairwise Method
Confusion Method
for O
Fine Material
- Material
Grained Material
Visual Material
Classification Material
section O
: O
Abstract O
. O
Fine Material
- Material
Grained Material
Visual Material
Classification Material
( O
FGVC Material
) O
datasets Material
contain O
small O
sample O
sizes O
, O
along O
with O
significant O
intra O
- O
class O
variation O
and O
interclass Metric
similarity Metric
. O
While O
prior O
work O
has O
addressed O
intra Task
- Task
class Task
variation Task
using O
localization Method
and O
segmentation Method
techniques Method
, O
inter O
- O
class O
similarity O
may O
also O
affect O
feature Method
learning Method
and O
reduce O
classification Task
performance O
. O
In O
this O
work O
, O
we O
address O
this O
problem O
using O
a O
novel O
optimization Method
procedure Method
for O
the O
end Task
- Task
to Task
- Task
end Task
neural Task
network Task
training Task
on O
FGVC Task
tasks Task
. O
Our O
procedure O
, O
called O
Pairwise Method
Confusion Method
( O
PC Method
) O
reduces O
overfitting O
by O
intentionally O
introducing O
confusion O
in O
the O
activations O
. O
With O
PC Method
regularization Method
, O
we O
obtain O
state O
- O
ofthe O
- O
art O
performance O
on O
six O
of O
the O
most O
widely O
- O
used O
FGVC Material
datasets Material
and O
demonstrate O
improved O
localization Method
ability O
. O
PC Method
is O
easy O
to O
implement O
, O
does O
not O
need O
excessive O
hyperparameter Method
tuning Method
during O
training O
, O
and O
does O
not O
add O
significant O
overhead O
during O
test O
time O
. O
section O
: O
Introduction O
The O
Fine Material
- Material
Grained Material
Visual Material
Classification Material
( O
FGVC Material
) O
task O
focuses O
on O
differentiating O
between O
hard Task
- Task
to Task
- Task
distinguish Task
object Task
classes Task
, O
such O
as O
species O
of O
birds O
, O
flowers O
, O
or O
animals O
; O
and O
identifying O
the O
makes O
or O
models Task
of Task
vehicles Task
. O
FGVC Material
datasets Material
depart O
from O
conventional O
image Task
classification Task
in O
that O
they O
typically O
require O
expert O
knowledge O
, O
rather O
than O
crowdsourcing O
, O
for O
gathering Task
annotations Task
. O
FGVC Material
datasets Material
contain O
images O
with O
much O
higher O
visual O
similarity O
than O
those O
in O
large O
- O
scale O
visual O
classification O
( O
LSVC O
) O
. O
Moreover O
, O
FGVC Material
datasets Material
have O
minute O
inter O
- O
class O
visual O
differences O
in O
addition O
to O
the O
variations O
in O
pose O
, O
lighting O
and O
viewpoint O
found O
in O
LSVC O
[ O
reference O
] O
. O
Additionally O
, O
FGVC Material
datasets Material
often O
exhibit O
long O
tails O
in O
the O
data O
distribution O
, O
since O
the O
difficulty O
of O
obtaining O
examples O
of O
different O
classes O
may O
vary O
. O
This O
combination O
of O
small O
, O
non O
- O
uniform O
datasets Material
and O
subtle O
inter O
- O
class O
differences O
makes O
FGVC Material
challenging O
even O
for O
powerful O
deep Method
learning Method
algorithms Method
. O
Most O
of O
the O
prior O
work O
in O
FGVC Material
has O
focused O
on O
tackling O
the O
intra O
- O
class O
variation O
in O
pose O
, O
lighting O
, O
and O
viewpoint O
using O
localization Method
techniques O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
, O
and O
by O
augmenting O
training O
datasets Material
with O
additional O
data O
from O
the O
Web O
[ O
reference O
][ O
reference O
] O
. O
However O
, O
we O
observe O
that O
prior O
work O
in O
FGVC Material
does O
not O
pay O
much O
attention O
to O
the O
problems O
that O
may O
arise O
due O
to O
the O
inter O
- O
class O
visual O
similarity O
in O
the O
feature Method
extraction Method
pipeline Method
. O
Similar O
to O
LSVC Task
tasks Task
, O
neural Method
networks Method
for O
FGVC Task
tasks Task
are O
typically O
trained O
with O
cross Metric
- Metric
entropy Metric
loss Metric
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
In O
LSVC O
datasets Material
such O
as O
ImageNet Material
[ O
reference O
] O
, O
strongly O
discriminative Method
learning Method
using O
the O
cross Metric
- Metric
entropy Metric
loss Metric
is O
successful O
in O
part O
due O
to O
the O
significant O
inter O
- O
class O
variation O
( O
compared O
to O
intraclass O
variation O
) O
, O
which O
enables O
deep Method
networks Method
to O
learn O
generalized O
discriminatory O
features O
with O
large O
amounts O
of O
data O
. O
We O
posit O
that O
this O
formulation O
may O
not O
be O
ideal O
for O
FGVC Material
, O
which O
shows O
smaller O
visual O
differences O
between O
classes O
and O
larger O
differences O
within O
each O
class O
than O
LSVC Method
. O
For O
instance O
, O
if O
two O
samples O
in O
the O
training O
set O
have O
very O
similar O
visual O
content O
but O
different O
class O
labels O
, O
minimizing O
the O
cross Metric
- Metric
entropy Metric
loss Metric
will O
force O
the O
neural Method
network Method
to O
learn O
features O
that O
distinguish O
these O
two O
images O
with O
high O
confidence O
- O
potentially O
forcing O
the O
network O
to O
learn O
sample O
- O
specific O
artifacts O
for O
visually O
confusing O
classes O
in O
order O
to O
minimize O
training Metric
error Metric
. O
We O
suspect O
that O
this O
effect O
would O
be O
especially O
pronounced O
in O
FGVC Material
, O
since O
there O
are O
fewer O
samples O
from O
which O
the O
network O
can O
learn O
generalizable O
class O
- O
specific O
features O
. O
Based O
on O
this O
hypothesis O
, O
we O
propose O
that O
introducing O
confusion O
in O
output O
logit O
activations O
during O
training O
for O
an O
FGVC Material
task O
will O
force O
the O
network O
to O
learn O
slightly O
less O
discriminative O
features O
, O
thereby O
preventing O
it O
from O
overfitting O
to O
sample O
- O
specific O
artifacts O
. O
Specifically O
, O
we O
aim O
to O
confuse O
the O
network O
, O
by O
minimizing O
the O
distance O
between O
the O
predicted O
probability O
distributions O
for O
random O
pairs O
of O
samples O
from O
the O
training O
set O
. O
To O
do O
so O
, O
we O
propose O
Pairwise Method
Confusion Method
( O
PC Method
) O
[ O
reference O
] O
, O
a O
pairwise Method
algorithm Method
for O
training O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
end O
- O
to O
- O
end O
for O
fine Task
- Task
grained Task
visual Task
classification Task
. O
In O
Pairwise Method
Confusion Method
, O
we O
construct O
a O
Siamese Method
neural Method
network Method
trained O
with O
a O
novel O
loss Method
function Method
that O
attempts O
to O
bring O
class O
conditional O
probability O
distributions O
closer O
to O
each O
other O
. O
Using O
Pairwise Method
Confusion Method
with O
a O
standard O
network Method
architecture Method
like O
DenseNet Method
[ O
reference O
] O
or O
ResNet Method
[ O
reference O
] O
as O
a O
base O
network O
, O
we O
obtain O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
six O
of O
the O
most O
widely O
- O
used O
fine Task
- Task
grained Task
recognition Task
datasets Task
, O
improving O
over O
the O
previous O
- O
best O
published O
methods O
by O
1.86 O
% O
on O
average O
. O
In O
addition O
, O
PC Method
- O
trained O
networks O
show O
better O
localization Method
performance O
as O
compared O
to O
standard O
networks O
. O
Pairwise Method
Confusion Method
is O
simple O
to O
implement O
, O
has O
no O
added O
overhead O
in O
training O
or O
prediction Metric
time Metric
, O
and O
provides O
performance O
improvements O
both O
in O
FGVC Task
tasks Task
and O
other O
tasks O
that O
involve O
transfer Method
learning Method
with O
small O
amounts O
of O
training O
data O
. O
section O
: O
Related O
Work O
Fine Material
- Material
Grained Material
Visual Material
Classification Material
: O
Early O
FGVC Material
research O
focused O
on O
methods O
to O
train O
with O
limited O
labeled O
data O
and O
traditional O
image O
features O
. O
Yao O
et O
al O
. O
[ O
reference O
] O
combined O
strongly O
discriminative O
image O
patches O
with O
randomization Method
techniques Method
to O
prevent O
overfitting O
. O
Yao O
et O
al O
. O
[ O
reference O
] O
subsequently O
utilized O
template Method
matching Method
to O
avoid O
the O
need O
for O
a O
large O
number O
of O
annotations O
. O
Recently O
, O
improved O
localization Method
of O
the O
target O
object O
in O
training O
images O
has O
been O
shown O
to O
be O
useful O
for O
FGVC Material
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Zhang O
et O
al O
. O
[ O
reference O
] O
utilize O
part Method
- Method
based Method
Region Method
- Method
CNNs Method
[ O
reference O
] O
to O
perform O
finer O
localization Method
. O
Spatial Method
Transformer Method
Networks Method
[ O
reference O
] O
show O
that O
learning O
a O
content Method
- Method
based Method
affine Method
transformation Method
layer Method
improves O
FGVC Material
performance O
. O
Pose Method
- Method
normalized Method
CNNs Method
have O
also O
been O
shown O
to O
be O
effective O
at O
FGVC Material
[ O
reference O
][ O
reference O
] O
. O
Model Method
ensembling Method
and O
boosting Method
has O
also O
improved O
performance O
on O
FGVC Material
[ O
reference O
] O
. O
Lin O
et O
al O
. O
[ O
reference O
] O
introduced O
Bilinear Method
Pooling Method
, O
which O
combines O
pairwise O
local O
feature O
sets O
and O
improves O
classification Task
performance O
. O
Bilinear Method
Pooling Method
has O
been O
extended O
by O
Gao O
et O
al O
. O
[ O
reference O
] O
using O
a O
compact Method
bilinear Method
representation Method
and O
Cui O
et O
al O
. O
[ O
reference O
] O
using O
a O
general O
Kernel Method
- Method
based Method
pooling Method
framework Method
that O
captures O
higher O
- O
order O
interactions O
of O
features O
. O
Pairwise Task
Learning Task
: O
Chopra O
et O
al O
. O
[ O
reference O
] O
introduced O
a O
Siamese Method
neural Method
network Method
for O
handwriting Task
recognition Task
. O
Parikh O
and O
Grauman O
[ O
reference O
] O
developed O
a O
pairwise Method
ranking Method
scheme Method
for O
relative Task
attribute Task
learning Task
. O
Subsequently O
, O
pairwise Method
neural Method
network Method
models Method
have O
become O
common O
for O
attribute Task
modeling Task
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Learning O
from O
Label O
Confusion O
: O
Our O
method O
aims O
to O
improve O
classification Task
performance O
by O
introducing O
confusion O
within O
the O
output O
labels O
. O
Prior O
work O
in O
this O
area O
includes O
methods O
that O
utilize O
label O
noise O
( O
e.g. O
, O
[ O
reference O
] O
) O
and O
data O
noise O
( O
e.g. O
, O
[ O
reference O
] O
) O
in O
training O
. O
Krause O
et O
al O
. O
[ O
reference O
] O
utilized O
noisy O
training O
data O
for O
FGVC Material
. O
Neelakantan O
et O
al O
. O
[ O
reference O
] O
added O
noise O
to O
the O
gradient O
during O
training O
to O
improve O
generalization Task
performance O
in O
very Task
deep Task
networks Task
. O
Szegedy O
et O
al O
. O
[ O
reference O
] O
introduced O
label Method
- Method
smoothing Method
regularization Method
for O
training O
deep Method
Inception Method
models Method
. O
In O
this O
paper O
, O
we O
bring O
together O
concepts O
from O
pairwise Method
learning Method
and O
label Task
confusion Task
and O
take O
a O
step O
towards O
solving O
the O
problems O
of O
overfitting O
and O
samplespecific O
artifacts O
when O
training O
neural Method
networks Method
for O
FGVC Task
tasks Task
. O
section O
: O
Method O
FGVC Material
datasets Material
in O
computer Task
vision Task
are O
orders O
of O
magnitude O
smaller O
than O
LSVC O
datasets Material
and O
contain O
greater O
imbalance O
across O
classes O
( O
see O
Table O
1 O
) O
. O
Moreover O
, O
the O
samples O
of O
a O
class O
are O
not O
accurately O
representative O
of O
the O
complete O
variation O
in O
the O
visual O
class O
itself O
. O
The O
smaller O
dataset O
size O
can O
result O
in O
overfitting O
when O
training O
deep Method
neural Method
architectures Method
with O
large O
number O
of O
parameters O
- O
even O
with O
preliminary O
layers O
being O
frozen O
. O
In O
addition O
, O
the O
training O
data O
may O
not O
be O
completely O
representative O
of O
the O
real O
- O
world O
data O
, O
with O
issues O
such O
as O
more O
abundant O
sampling O
for O
certain O
classes O
. O
For O
example O
, O
in O
FGVC Material
of O
birds O
, O
certain O
species O
from O
geographically O
accessible O
areas O
may O
be O
overrepresented O
in O
the O
training O
dataset O
. O
As O
a O
result O
, O
the O
neural Method
network Method
may O
learn O
to O
latch O
on O
to O
sample O
- O
specific O
artifacts O
in O
the O
image O
, O
instead O
of O
learning O
a O
versatile Method
representation Method
for O
the O
target O
object O
. O
We O
aim O
to O
solve O
both O
of O
these O
issues O
in O
FGVC Material
( O
overfitting O
and O
sample Task
- Task
specific Task
artifacts Task
) O
by O
bringing O
the O
different O
class O
- O
conditional O
probability O
distributions O
closer O
together O
and O
confusing O
the O
deep Method
network Method
, O
subsequently O
reducing O
its O
prediction Metric
over Metric
- Metric
confidence Metric
, O
thus O
improving O
generalization Metric
performance Metric
. O
Let O
us O
formalize O
the O
idea O
of O
" O
confusing O
" O
the O
conditional O
probability O
distributions O
. O
Consider O
the O
conditional O
probability O
distributions O
for O
two O
input O
images O
x O
1 O
and O
x O
2 O
, O
which O
can O
be O
given O
by O
p O
θ O
( O
y|x O
1 O
) O
and O
p O
θ O
( O
y|x O
2 O
) O
respectively O
. O
For O
a O
classification Task
problem Task
with O
N O
output O
classes O
, O
each O
of O
these O
distributions O
is O
an O
N O
- O
dimensional O
vector O
, O
with O
each O
element O
i O
denoting O
the O
belief O
of O
the O
classifier O
in O
class O
y O
i O
given O
input O
x. O
If O
we O
wish O
to O
confuse O
the O
class O
outputs O
of O
the O
classifier Method
for O
the O
pair O
x O
1 O
and O
x O
2 O
, O
we O
should O
learn O
parameters O
θ O
that O
bring O
these O
conditional O
probability O
distributions O
" O
closer O
" O
under O
some O
distance Metric
metric Metric
, O
that O
is O
, O
make O
the O
predictions O
for O
x O
1 O
and O
x O
2 O
similar O
. O
While O
KL Method
- O
divergence O
might O
seem O
to O
be O
a O
reasonable O
choice O
to O
design O
a O
loss O
function O
for O
optimizing O
the O
distance O
between O
conditional O
probability O
distributions O
, O
in O
Section O
3.1 O
, O
we O
show O
that O
it O
is O
infeasible O
to O
train O
a O
neural Method
network Method
when O
using O
KL Method
- O
divergence O
as O
a O
regularizer Method
. O
Therefore O
, O
we O
introduce O
the O
Euclidean O
Distance O
between O
distributions O
as O
a O
metric O
for O
confusion Task
in O
Sections O
3.2 O
and O
3.3 O
and O
describe O
neural Method
network Method
training Method
with O
this O
metric O
in O
Section O
3.4 O
. O
section O
: O
Symmetric O
KL Method
- O
divergence O
or O
Jeffrey Method
's Method
Divergence Method
The O
most O
prevalent O
method O
to O
measure O
dissimilarity O
of O
one O
probability O
distribution O
from O
another O
is O
to O
use O
the O
Kullback Method
- Method
Liebler Method
( O
KL Method
) O
divergence O
. O
However O
, O
the O
standard O
KL Method
- O
divergence O
can O
not O
serve O
our O
purpose O
owing O
to O
its O
asymmetric O
nature O
. O
This O
could O
be O
remedied O
by O
using O
the O
symmetric O
KL Method
- O
divergence O
, O
defined O
for O
two O
probability O
distributions O
P O
, O
Q O
with O
mass O
functions O
p O
( O
· O
) O
, O
q O
( O
· O
) O
( O
for O
events O
u O
∈ O
U O
) O
: O
This O
symmetrized O
version O
of O
KL Method
- O
divergence O
, O
known O
as O
Jeffrey O
's O
divergence O
[ O
reference O
] O
, O
is O
a O
measure O
of O
the O
average Metric
relative Metric
entropy Metric
between O
two O
probability O
distributions O
[ O
reference O
] O
. O
For O
our O
model O
parameterized O
by O
θ O
, O
for O
samples O
x O
1 O
and O
x O
2 O
, O
the O
Jeffrey O
's O
divergence O
can O
be O
written O
as O
: O
Jeffrey Method
's Method
divergence Method
satisfies O
all O
of O
our O
basic O
requirements O
of O
a O
symmetric Metric
divergence Metric
metric Metric
between O
probability O
distributions O
, O
and O
therefore O
could O
be O
included O
as O
a O
regularizing Method
term Method
while O
training O
with O
cross O
- O
entropy O
, O
to O
achieve O
our O
desired O
confusion O
. O
However O
, O
when O
we O
learn O
model Method
parameters Method
using O
stochastic Method
gradient Method
descent Method
( O
SGD Method
) Method
, O
it O
can O
be O
difficult O
to O
train O
, O
especially O
if O
our O
distributions O
P O
, O
Q O
have O
mass O
concentrated O
on O
different O
events O
. O
This O
can O
be O
seen O
in O
Equation O
2 O
. O
Consider O
Jeffrey Method
's Method
divergence Method
with O
N O
= O
2 O
classes O
, O
and O
that O
x O
1 O
belongs O
to O
class O
1 O
, O
and O
x O
2 O
belongs O
to O
class O
2 O
. O
If O
the O
model O
parameters O
θ O
are O
such O
that O
it O
correctly O
identifies O
both O
x O
1 O
and O
x O
2 O
by O
training O
using O
cross Metric
- Metric
entropy Metric
loss Metric
, O
p O
θ O
( O
y O
1 O
|x O
1 O
) O
= O
1 O
− O
δ O
1 O
and O
p O
θ O
( O
y O
2 O
|x O
2 O
) O
= O
1 O
− O
δ O
2 O
, O
where O
0 O
< O
δ O
1 O
, O
δ O
2 O
< O
1 O
2 O
( O
since O
the O
classifier Method
outputs O
correct O
predictions O
for O
the O
input O
images O
) O
, O
we O
can O
show O
: O
Please O
see O
the O
supplementary O
material O
for O
an O
expanded O
proof O
. O
As O
training O
progresses O
with O
these O
labels O
, O
the O
cross Metric
- Metric
entropy Metric
loss Metric
will O
motivate O
the O
values O
of O
δ O
1 O
and O
δ O
2 O
to O
become O
closer O
to O
zero O
( O
but O
never O
equaling O
zero O
, O
since O
the O
probability O
outputs O
p O
θ O
( O
y|x O
1 O
) O
, O
p O
θ O
( O
y|x O
2 O
) O
are O
the O
outputs O
from O
a O
softmax Method
) Method
. O
As O
( O
δ O
1 O
, O
δ O
2 O
) O
→ O
( O
0 O
+ O
, O
0 O
+ O
) O
, O
the O
second O
term O
− O
log O
( O
δ O
1 O
δ O
2 O
) O
on O
the O
R.H.S. O
of O
inequality O
( O
4 O
) O
typically O
grows O
whereas O
( O
1 O
− O
δ O
1 O
− O
δ O
2 O
) O
approaches O
1 O
, O
which O
makes O
D O
J O
( O
p O
θ O
( O
y|x O
1 O
) O
, O
p O
θ O
( O
y|x O
2 O
) O
) O
larger O
as O
the O
predictions O
get O
closer O
to O
the O
true O
labels O
. O
In O
practice O
, O
we O
see O
that O
training O
with O
D O
J O
( O
p O
θ O
( O
y|x O
1 O
) O
, O
p O
θ O
( O
y|x O
2 O
) O
) O
as O
a O
regularizer Method
term Method
diverges O
, O
unless O
a O
very O
small O
regularizing O
parameter O
is O
chosen O
, O
which O
removes O
the O
effect O
of O
regularization O
altogether O
. O
A O
natural O
question O
that O
can O
arise O
from O
this O
analysis O
is O
that O
cross Method
- Method
entropy Method
training Method
itself O
involves O
optimizing O
KL Method
- O
divergence O
between O
the O
target O
label O
distribution O
and O
the O
model O
's O
predictions O
, O
however O
no O
such O
divergence O
occurs O
. O
This O
is O
because O
cross O
- O
entropy O
involves O
only O
one O
direction O
of O
the O
KL Method
- O
divergence O
, O
and O
the O
target O
distribution O
has O
all O
the O
mass O
concentrated O
at O
one O
event O
( O
the O
correct O
label O
) O
. O
Since O
( O
x O
log O
x O
) O
| O
x=0 O
= O
0 O
, O
for O
predicted O
label O
vector O
y O
with O
correct O
label O
class O
c O
, O
this O
simplifies O
the O
cross Metric
- Metric
entropy Metric
error Metric
L Metric
CE Metric
( O
p O
θ O
( O
y|x O
) O
, O
y O
) O
to O
be O
: O
This O
formulation O
does O
not O
diverge O
as O
the O
model O
trains O
, O
i.e. O
p O
θ O
( O
y O
c O
|x O
) O
→ O
1 O
. O
In O
some O
cases O
where O
label O
noise O
is O
added O
to O
the O
label O
vector O
( O
such O
as O
label Method
smoothing Method
[ O
reference O
][ O
reference O
] O
) O
, O
the O
label O
noise O
is O
a O
fixed O
constant O
and O
not O
approaching O
zero O
( O
as O
in O
the O
case O
of O
Jeffery O
's O
divergence O
between O
model O
predictions O
) O
and O
is O
hence O
feasible O
to O
train O
. O
Thus O
, O
Jeffrey O
's O
Divergence O
or O
symmetric O
KL Method
- O
divergence O
, O
while O
a O
seemingly O
natural O
choice O
, O
can O
not O
be O
used O
to O
train O
a O
neural Method
network Method
with O
SGD Method
. O
This O
motivates O
us O
to O
look O
for O
an O
alternative O
metric O
to O
measure O
" O
confusion O
" O
between O
conditional O
probability O
distributions O
. O
section O
: O
Euclidean O
Distance O
as O
Confusion O
Since O
the O
conditional O
probability O
distribution O
over O
N O
classes O
is O
an O
element O
within O
R O
N O
on O
the O
unit O
simplex O
, O
we O
can O
consider O
the O
Euclidean O
distance O
to O
be O
a O
metric O
of O
" O
confusion O
" O
between O
two O
conditional Method
probability Method
distributions Method
. O
Analogous O
to O
the O
previous O
setting O
, O
we O
define O
the O
Euclidean O
Confusion O
D O
EC O
( O
· O
, O
· O
) O
for O
a O
pair O
of O
inputs O
x O
1 O
, O
x O
2 O
with O
model O
parameters O
θ O
as O
: O
( O
5 O
) O
Unlike O
Jeffrey O
's O
Divergence O
, O
Euclidean O
Confusion O
does O
not O
diverge O
when O
used O
as O
a O
regularization Method
term Method
with O
cross O
- O
entropy O
. O
However O
, O
to O
verify O
this O
unconventional O
choice O
for O
a O
distance O
metric O
between O
probability O
distributions O
, O
we O
prove O
some O
properties O
that O
relate O
Euclidean Metric
Confusion Metric
to O
existing O
divergence Metric
measures Metric
. O
Lemma O
1 O
. O
On O
a O
finite O
probability O
space O
, O
the O
Euclidean O
Confusion O
D O
EC O
( O
P O
, O
Q O
) O
is O
a O
lower O
bound O
for O
the O
Jeffrey Metric
's Metric
Divergence Metric
D O
J O
( O
P O
, O
Q O
) O
for O
probability O
measures O
P O
, O
Q. O
Proof O
. O
This O
follows O
from O
Pinsker O
's O
Inequality O
and O
the O
relationship O
between O
1 O
and O
2 O
norms O
. O
Complete O
proof O
is O
provided O
in O
the O
supplementary O
material O
. O
By O
Lemma O
1 O
, O
we O
can O
see O
that O
the O
Euclidean O
Confusion O
is O
a O
conservative O
estimate O
for O
Jeffrey Metric
's Metric
divergence Metric
, O
the O
earlier O
proposed O
divergence Metric
measure Metric
. O
For O
finite O
probability O
spaces O
, O
the O
Total Metric
Variation Metric
Distance Metric
D O
TV O
( O
P O
, O
Q O
) O
2 O
= O
1 O
2 O
P O
− O
Q O
1 O
is O
also O
a O
measure O
of O
interest O
. O
However O
, O
due O
to O
its O
non O
- O
differentiable O
nature O
, O
it O
is O
unsuitable O
for O
our O
case O
. O
Nevertheless O
, O
we O
can O
relate O
the O
Euclidean Metric
Confusion Metric
and O
Total Metric
Variation Metric
Distance Metric
by O
the O
following O
result O
. O
Lemma O
2 O
. O
On O
a O
finite O
probability O
space O
, O
the O
Euclidean O
Confusion O
D O
EC O
( O
P O
, O
Q O
) O
is O
bounded O
by O
4D O
TV O
( O
P O
, O
Q O
) O
2 O
for O
probability O
measures O
P O
, O
Q. O
Proof O
. O
This O
follows O
directly O
from O
the O
relationship O
between O
1 O
and O
2 O
norms O
. O
Complete O
proof O
is O
provided O
in O
the O
supplementary O
material O
. O
section O
: O
Euclidean O
Confusion O
for O
Point O
Sets O
In O
a O
standard O
classification Task
setting Task
with O
N O
classes O
, O
we O
consider O
a O
training O
set O
with O
m O
= O
N O
i=1 O
m O
i O
training O
examples O
, O
where O
m O
i O
denotes O
the O
number O
of O
training O
samples O
for O
class O
i. O
For O
this O
setting O
, O
we O
can O
write O
the O
total O
Euclidean O
Confusion O
between O
points O
of O
classes O
i O
and O
j O
as O
the O
average O
of O
the O
Euclidean O
Confusion O
between O
all O
pairs O
of O
points O
belonging O
to O
those O
two O
classes O
. O
For O
simplicity O
of O
notation O
, O
let O
us O
denote O
the O
set O
of O
conditional O
probability O
distributions O
of O
all O
training O
points O
belonging O
to O
class O
i O
for O
a O
model O
parameterized O
by O
θ O
as O
Then O
, O
for O
a O
model O
parameterized O
by O
θ O
, O
the O
Euclidean Metric
Confusion Metric
is O
given O
by O
: O
We O
can O
simplify O
this O
equation O
by O
assuming O
an O
equal O
number O
of O
points O
n O
per O
class O
: O
This O
form O
of O
the O
Euclidean O
Confusion O
between O
the O
two O
sets O
of O
points O
gives O
us O
an O
interesting O
connection O
with O
another O
popular O
distance Metric
metric Metric
over O
probability O
distributions O
, O
known O
as O
the O
Energy O
Distance O
[ O
reference O
] O
. O
Introduced O
by O
Gabor O
Szekely O
[ O
reference O
] O
, O
the O
Energy O
Distance O
D O
EN O
( O
F O
, O
G O
) O
between O
two O
cumulative Method
probability Method
distribution Method
functions Method
F O
and O
G O
with O
random O
vectors O
X O
and O
Y O
in O
R O
N O
can O
be O
given O
by O
where O
( O
X O
, O
X O
, O
Y O
, O
Y O
) O
are O
independent O
, O
and O
X O
∼ O
F O
, O
X O
∼ O
F O
, O
Y O
∼ O
G O
, O
Y O
∼ O
G. O
If O
we O
consider O
the O
sets O
S O
i O
and O
S O
j O
, O
with O
a O
uniform O
probability O
of O
selecting O
any O
of O
the O
n O
points O
in O
each O
of O
these O
sets O
, O
then O
we O
obtain O
the O
following O
results O
. O
Lemma O
3 O
. O
For O
sets O
S O
i O
, O
S O
j O
and O
D O
EC O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
as O
defined O
in O
Equation O
( O
14 O
) O
: O
where O
D O
EN O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
is O
the O
Energy O
Distance O
under O
Euclidean O
norm O
between O
S O
i O
and O
S O
j O
( O
parameterized O
by O
θ O
) O
, O
and O
random O
vectors O
are O
selected O
with O
uniform O
probability O
in O
both O
S O
i O
and O
S O
j O
. O
Proof O
. O
This O
follows O
from O
the O
definition O
of O
Energy O
Distance O
with O
uniform O
probability O
of O
sampling O
. O
Complete O
proof O
is O
provided O
in O
the O
supplementary O
material O
. O
Corollary O
1 O
. O
For O
sets O
S O
i O
, O
S O
j O
and O
D O
EC O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
as O
defined O
in O
Equation O
( O
14 O
) O
, O
we O
have O
: O
with O
equality O
only O
when O
S O
i O
= O
S O
j O
. O
Proof O
. O
This O
follows O
from O
the O
fact O
that O
the O
Energy O
Distance O
D O
EN O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
is O
0 O
only O
when O
S O
i O
= O
S O
j O
. O
The O
complete O
version O
of O
the O
proof O
is O
included O
in O
the O
supplement O
. O
With O
these O
results O
, O
we O
restrict O
the O
behavior O
of O
Euclidean O
Confusion O
within O
two O
well O
- O
defined O
conventional O
probability Metric
distance Metric
measures Metric
, O
the O
Jeffrey Metric
's Metric
divergence Metric
and O
Energy Method
Distance Method
. O
One O
might O
consider O
optimizing O
the O
Energy O
Distance O
directly O
, O
due O
to O
its O
similar O
formulation O
and O
the O
fact O
that O
we O
uniformly O
sample O
points O
during O
training O
with O
SGD Method
. O
However O
, O
the O
Energy O
Distance O
additionally O
includes O
the O
two O
terms O
that O
account O
for O
the O
negative O
of O
the O
average O
all O
- O
pairs O
distances O
between O
points O
in O
S O
i O
and O
S O
j O
respectively O
, O
which O
we O
do O
not O
want O
to O
maximize O
, O
since O
we O
do O
not O
wish O
to O
push O
points O
within O
the O
same O
class O
further O
apart O
. O
Therefore O
, O
we O
proceed O
with O
our O
measure O
of O
Euclidean Metric
Confusion Metric
. O
section O
: O
Learning Task
with O
Gradient Method
Descent Method
We O
proceed O
to O
learn O
parameters O
θ O
* O
for O
a O
neural Method
network Method
, O
with O
the O
following O
learning Metric
objective Metric
function Metric
for O
a O
pair O
of O
input O
points O
, O
motivated O
by O
the O
formulation O
. O
We O
employ O
a O
Siamese Method
- Method
like Method
architecture Method
, O
with O
individual O
cross Method
entropy Method
calculations Method
for O
each O
branch O
, O
followed O
by O
a O
joint Method
energy Method
- Method
distance Method
minimization Method
loss Method
. O
We O
split O
each O
incoming O
batch O
of O
samples O
into O
two O
mini O
- O
batches O
, O
and O
feed O
the O
network O
pairwise O
samples O
. O
of O
Euclidean O
Confusion O
: O
This O
objective Metric
function Metric
can O
be O
explained O
as O
: O
for O
each O
point O
in O
the O
training O
set O
, O
we O
randomly O
select O
another O
point O
from O
a O
different O
class O
and O
calculate O
the O
individual O
cross Metric
- Metric
entropy Metric
losses Metric
and O
Euclidean Metric
Confusion Metric
until O
all O
pairs O
have O
been O
exhausted O
. O
For O
each O
point O
in O
the O
training O
dataset O
, O
there O
are O
n· O
( O
N O
− O
1 O
) O
valid O
choices O
for O
the O
other O
point O
, O
giving O
us O
a O
total O
of O
n O
2 O
· O
N O
· O
( O
N O
− O
1 O
) O
possible O
pairs O
. O
In O
practice O
, O
we O
find O
that O
we O
do O
not O
need O
to O
exhaust O
all O
combinations O
for O
effective Task
learning Task
using O
gradient Method
descent Method
, O
and O
in O
fact O
we O
observe O
that O
convergence Metric
is O
achieved O
far O
before O
all O
observations O
are O
observed O
. O
We O
simplify O
our O
formulation O
instead O
by O
using O
the O
following O
procedure O
described O
in O
Algorithm O
1 O
. O
Training O
Procedure O
: O
As O
described O
in O
Algorithm O
1 O
, O
our O
learning Method
procedure Method
is O
a O
slightly O
modified O
version O
of O
the O
standard O
SGD Method
. O
We O
randomly O
permute O
the O
training O
set O
twice O
, O
and O
then O
for O
each O
pair O
of O
points O
in O
the O
training O
set O
, O
add O
Euclidean O
Confusion O
only O
if O
the O
samples O
belong O
to O
different O
classes O
. O
This O
form O
of O
sampling O
approximates O
the O
exhaustive O
Euclidean O
Confusion O
, O
with O
some O
points O
with O
regular Method
gradient Method
descent Method
, O
which O
in O
practice O
does O
not O
alter O
the O
performance O
. O
Moreover O
, O
convergence Metric
is O
achieved O
after O
only O
a O
fraction O
of O
all O
the O
possible O
pairs O
are O
observed O
. O
Formally O
, O
we O
wish O
to O
model O
the O
conditional O
probability O
distribution O
p O
θ O
( O
y|x O
) O
over O
the O
p O
classes O
for O
function O
f O
( O
x O
; O
θ O
) O
= O
p O
θ O
( O
y|x O
) O
parameterized O
by O
model O
parameters O
θ O
. O
Given O
our O
optimization Method
procedure Method
, O
we O
can O
rewrite O
the O
total O
loss O
for O
a O
pair O
of O
section O
: O
Algorithm O
1 O
Training O
Using O
Euclidean Method
Confusion Method
where O
, O
γ O
( O
y O
1 O
, O
y O
2 O
) O
= O
1 O
when O
y O
i O
= O
y O
j O
, O
and O
0 O
otherwise O
. O
We O
denote O
training O
with O
this O
general O
architecture O
with O
the O
term O
Pairwise Method
Confusion Method
or O
PC Method
for O
short O
. O
Specifically O
, O
we O
train O
a O
Siamese Method
- Method
like Method
neural Method
network Method
[ O
reference O
] O
with O
shared O
weights O
, O
training O
each O
network O
individually O
using O
cross Method
- Method
entropy Method
, O
and O
add O
the O
Euclidean Metric
Confusion Metric
loss Metric
between O
the O
conditional O
probability O
distributions O
obtained O
from O
each O
network O
( O
Figure O
1 O
) O
. O
During O
training Task
, O
we O
split O
an O
incoming O
batch O
of O
training O
samples O
into O
two O
parts O
, O
and O
evaluating O
cross Metric
- Metric
entropy Metric
on O
each O
sub O
- O
batch O
identically O
, O
followed O
by O
a O
pairwise O
loss O
term O
calculated O
for O
corresponding O
pairs O
of O
samples O
across O
batches O
. O
During O
testing O
, O
only O
one O
branch O
of O
the O
network O
is O
active O
, O
and O
generates O
output O
predictions O
for O
the O
input O
image O
. O
As O
a O
result O
, O
implementing O
this O
method O
does O
not O
introduce O
any O
significant O
computational Metric
overhead Metric
during O
testing O
. O
section O
: O
CNN Method
Architectures Method
We O
experiment O
with O
VGGNet Method
[ O
reference O
] O
, O
GoogLeNet Method
[ O
reference O
] O
, O
ResNets Method
[ O
reference O
] O
, O
and O
DenseNets Method
[ O
reference O
] O
as O
base O
architectures O
for O
the O
Siamese Method
network Method
trained O
with O
PC Method
to O
demonstrate O
that O
our O
method O
is O
insensitive O
to O
the O
choice O
of O
source Method
architecture Method
. O
section O
: O
Experimental O
Details O
We O
perform O
all O
experiments O
using O
Caffe Method
[ O
reference O
] O
or O
PyTorch Method
[ O
reference O
] O
over O
a O
cluster O
of O
NVIDIA Method
Titan Method
X Method
, O
Tesla Method
K40c Method
and O
GTX Method
1080 Method
GPUs Method
. O
Our O
code O
and O
models O
are O
available O
at O
github.com O
/ O
abhimanyudubey O
/ O
confusion O
. O
Next O
, O
we O
provide O
brief O
descriptions O
of O
the O
various O
datasets Material
used O
in O
our O
paper O
. O
Table O
2 O
. O
Pairwise Method
Confusion Method
( O
PC Method
) O
obtains O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
six O
widelyused O
fine Task
- Task
grained Task
visual Task
classification Task
datasets Material
( O
A O
- O
F O
) O
. O
Improvement O
over O
the O
baseline O
model O
is O
reported O
as O
( O
∆ O
) O
. O
All O
results O
averaged O
over O
5 O
trials O
. O
( O
A O
) O
CUB Method
- Method
200 Method
- Method
2011 Method
Method Method
Top O
- O
1 O
∆ O
Gao O
et O
al O
. O
[ O
reference O
] O
84.00 O
- O
STN O
[ O
reference O
] O
84.10 O
- O
Zhang O
et O
al O
. O
[ O
reference O
] O
84.50 O
- O
Lin O
et O
al O
. O
[ O
reference O
] O
85.80 O
- O
Cui O
et O
al O
. O
[ O
reference O
] O
86 O
. O
car O
make O
, O
model O
, O
and O
year O
. O
The O
Aircraft Material
dataset Material
is O
a O
set O
of O
10 O
, O
000 O
images O
across O
100 O
classes O
denoting O
a O
fine O
- O
grained O
set O
of O
airplanes O
of O
different O
varieties O
[ O
reference O
] O
. O
These O
datasets Material
contain O
( O
i O
) O
large O
visual O
diversity O
in O
each O
class O
[ O
reference O
][ O
reference O
][ O
reference O
] O
, O
( O
ii O
) O
visually O
similar O
, O
often O
confusing O
samples O
belonging O
to O
different O
classes O
, O
and O
( O
iii O
) O
a O
large O
variation O
in O
the O
number O
of O
samples O
present O
per O
class O
, O
leading O
to O
greater O
class O
imbalance O
than O
LSVC O
datasets Material
like O
ImageNet Material
[ O
reference O
] O
. O
Additionally O
, O
some O
of O
these O
datasets Material
have O
densely O
annotated O
part O
information O
available O
, O
which O
we O
do O
not O
utilize O
in O
our O
experiments O
. O
section O
: O
Results O
section O
: O
Fine Material
- Material
Grained Material
Visual Material
Classification Material
We O
first O
describe O
our O
results O
on O
the O
six O
FGVC Material
datasets Material
from O
Table O
2 O
. O
In O
all O
experiments O
, O
we O
average O
results O
over O
5 O
trials O
per O
experiment O
- O
after O
choosing O
the O
best O
value O
of O
hyperparameter O
λ O
. O
Please O
see O
the O
supplementary O
material O
for O
mean O
and O
standard O
deviation O
values O
for O
all O
experiments O
. O
1 O
. O
Fine Method
- Method
tuning Method
from O
Baseline Method
Models Method
: O
We O
fine O
- O
tune O
from O
three O
baseline O
models O
using O
the O
PC Method
optimization O
procedure O
: O
ResNet Method
- Method
50 Method
[ O
reference O
] O
, O
Bilinear Method
CNN Method
[ O
reference O
] O
, O
and O
DenseNet Method
- Method
161 Method
[ O
reference O
] O
. O
As O
Tables O
2 O
-( O
A O
- O
F O
) O
show O
, O
PC Method
obtains O
substantial O
improvement O
across O
all O
datasets Material
and O
models O
. O
For O
instance O
, O
a O
baseline O
DenseNet Method
- Method
161 Method
architecture O
obtains O
an O
average O
accuracy Metric
of O
84.21 O
% O
, O
but O
PC Method
- O
DenseNet Method
- Method
161 Method
obtains O
an O
accuracy Metric
of O
86.87 O
% O
, O
an O
improvement O
of O
2.66 O
% O
. O
On O
NABirds Material
, O
we O
obtain O
improvements O
of O
4.60 O
% O
and O
3.42 O
% O
over O
baseline O
ResNet Method
- Method
50 Method
and O
DenseNet Method
- Method
161 Method
architectures O
. O
2 O
. O
Combining O
PC Method
with O
Specialized O
FGVC Material
models O
: O
Recent O
work O
in O
FGVC Material
has O
proposed O
several O
novel O
CNN Method
designs Method
that O
take O
part O
- O
localization Method
into O
account O
, O
such O
as O
bilinear Method
pooling Method
techniques Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
and O
spatial Method
transformer Method
networks Method
[ O
reference O
] O
. O
We O
train O
a O
Bilinear Method
CNN Method
[ O
reference O
] O
with O
PC Method
, O
and O
obtain O
an O
average O
improvement O
of O
1.7 O
% O
on O
the O
6 O
datasets Material
. O
We O
note O
two O
important O
aspects O
of O
our O
analysis O
: O
( O
1 O
) O
we O
do O
not O
compare O
with O
ensembling Method
and Method
data Method
augmentation Method
techniques Method
such O
as O
Boosted Method
CNNs Method
[ O
reference O
] O
and O
Krause O
, O
et O
al O
. O
[ O
reference O
] O
since O
prior O
evidence O
indicates O
that O
these O
techniques O
invariably O
improve O
performance O
, O
and O
( O
2 O
) O
we O
evaluate O
a O
single O
- O
crop O
, O
single Method
- Method
model Method
evaluation Method
without O
any O
part O
- O
or O
object O
- O
annotations O
, O
and O
perform O
competitively O
with O
methods O
that O
use O
both O
augmentations Method
. O
Choice O
of O
Hyperparameter O
λ O
: O
Since O
our O
formulation O
requires O
the O
selection O
of O
a O
hyperparameter O
λ O
, O
it O
is O
important O
to O
study O
the O
sensitivity O
of O
classification Metric
performance Metric
to O
the O
choice O
of O
λ O
. O
We O
conduct O
this O
experiment O
for O
four O
different O
models O
: O
GoogLeNet Method
[ O
reference O
] O
, O
ResNet Method
- Method
50 Method
[ O
reference O
] O
and O
VGGNet Method
- Method
16 Method
[ O
reference O
] O
and O
Bilinear Method
- Method
CNN Method
[ O
reference O
] O
on O
the O
CUB Material
- Material
200 Material
- Material
2011 Material
dataset Material
. O
PC Method
's O
performance O
is O
not O
very O
sensitive O
to O
the O
choice O
of O
λ O
( O
Figure O
2 O
and O
Supplementary O
Tables O
S1 O
- O
S5 O
) O
. O
For O
all O
six O
datasets Material
, O
the O
λ O
value O
is O
typically O
between O
the O
range O
[ O
reference O
][ O
reference O
] O
. O
On O
Bilinear Method
CNN Method
, O
setting O
λ O
= O
10 O
for O
all O
datasets Material
gives O
average O
performance O
within O
0.08 O
% O
compared O
to O
the O
reported O
values O
in O
Table O
2 O
. O
In O
general O
, O
PC Method
obtains O
optimum O
performance O
in O
the O
range O
of O
0.05N O
and O
0.15N O
, O
where O
N O
is O
the O
number O
of O
classes O
. O
section O
: O
Additional O
Experiments O
Since O
our O
method O
aims O
to O
improve O
classification Task
performance O
in O
FGVC Task
tasks Task
by O
introducing O
confusion O
in O
output O
logit O
activations O
, O
we O
would O
expect O
to O
see O
a O
larger O
improvement O
in O
datasets Material
with O
higher O
inter Metric
- Metric
class Metric
similarity Metric
and O
intra O
- O
class O
variation O
. O
To O
test O
this O
hypothesis O
, O
we O
conduct O
two O
additional O
experiments O
. O
In O
the O
first O
experiment O
, O
we O
construct O
two O
subsets O
of O
ImageNet Material
- Material
1 Material
K Material
[ O
reference O
] O
. O
The O
first O
dataset O
, O
ImageNet Material
- Material
Dogs Material
is O
a O
subset O
consisting O
only O
of O
species O
of O
dogs O
( O
117 O
classes O
and O
116 O
K O
images O
) O
. O
The O
second O
dataset O
, O
ImageNet Material
- O
Random O
contains O
randomly O
selected O
classes O
from O
ImageNet Material
- Material
1K. Material
Both O
datasets Material
contain O
equal O
number O
of O
classes O
( O
117 O
) O
and O
images O
( O
116 O
K O
) O
, O
but O
ImageNet Material
- Material
Dogs Material
has O
much O
higher O
interclass Metric
similarity Metric
and O
intra Metric
- Metric
class Metric
variation Metric
, O
as O
compared O
to O
ImageNet O
- O
Random O
. O
To O
test O
repeatability O
, O
we O
construct O
3 O
instances O
of O
Imagenet Material
- O
Random O
, O
by O
randomly O
choosing O
a O
different O
subset O
of O
ImageNet Material
with O
117 O
classes O
each O
time O
. O
For O
both O
experiments O
, O
we O
randomly O
construct O
a O
80 O
- O
20 O
train O
- O
val O
split O
from O
the O
training O
data O
to O
find O
optimal O
λ O
by O
cross Metric
- Metric
validation Metric
, O
and O
report O
the O
performance O
on O
the O
unseen Material
ImageNet Material
validation Material
set Material
of O
the O
subset O
of O
chosen O
classes O
. O
In O
Table O
3 O
, O
we O
compare O
the O
performance O
of O
training O
from O
scratch O
with O
- O
and O
without O
- O
PC Method
across O
three O
models O
: O
GoogLeNet Method
, O
ResNet Method
- Method
50 Method
, O
and O
DenseNet Method
- Method
161 Method
. O
As O
expected O
, O
PC Method
obtains O
a O
larger O
gain O
in O
classification O
accuracy Metric
( O
1.45 O
% O
) O
on O
ImageNet Material
- Material
Dogs Material
as O
compared O
to O
the O
ImageNet Material
- Material
Random Material
dataset Material
( O
0.54 O
% O
± O
0.28 O
) O
. O
In O
the O
second O
experiment O
, O
we O
utilize O
the O
CIFAR Material
- Material
10 Material
and O
CIFAR Material
- Material
100 Material
datasets Material
, O
which O
contain O
the O
same O
number O
of O
total O
images O
. O
CIFAR Material
- Material
100 Material
has O
10× O
the O
number O
of O
classes O
and O
10 O
% O
of O
images O
per O
class O
as O
CIFAR Material
- Material
10 Material
and O
contains O
larger O
inter O
- O
class O
similarity O
and O
intra O
- O
class O
variation O
. O
We O
train O
networks O
on O
both O
datasets Material
from O
scratch O
using O
default O
train O
- O
test O
splits O
( O
Table O
3 O
) O
. O
As O
expected O
, O
we O
obtain O
larger O
average O
gains O
of O
1.77 O
% O
on O
CIFAR Material
- Material
100 Material
, O
as O
compared O
to O
0.20 O
% O
on O
CIFAR Material
- Material
10 Material
. O
Additionally O
, O
when O
training O
with O
λ O
= O
10 O
on O
the O
entire O
ImageNet Material
dataset Material
, O
we O
obtain O
a O
top O
- O
1 O
accuracy Metric
of O
76.28 O
% O
( O
compared O
to O
a O
baseline O
of O
76.15 O
% O
) O
, O
which O
is O
a O
smaller O
improvement O
, O
which O
is O
in O
line O
with O
what O
we O
would O
expect O
for O
a O
large Task
- Task
scale Task
image Task
classification Task
problem Task
with O
large O
inter O
- O
class O
variation O
. O
Moreover O
, O
while O
training O
with O
PC Method
, O
we O
observe O
that O
the O
rate O
of O
convergence Metric
is O
always O
similar O
to O
or O
faster O
than O
training O
without O
PC Method
. O
For O
example O
, O
a O
GoogLeNet Method
trained O
on O
CUB Material
- Material
200 Material
- Material
2011 Material
( O
Figure O
2 O
( O
right O
) O
above O
) O
shows O
that O
PC Method
converges O
to O
higher O
validation O
accuracy Metric
faster O
than O
normal Method
training Method
using O
identical O
learning Metric
rate Metric
schedule Metric
and O
batch O
size O
. O
Note O
that O
the O
training O
accuracy Metric
is O
reduced O
when O
training O
with O
PC Method
, O
due O
to O
the O
regularization O
effect O
. O
In O
sum O
, O
classification Task
problems Task
that O
have O
large O
intra O
- O
class O
variation O
and O
high O
inter O
- O
class O
similarity O
benefit O
from O
optimization Task
with O
pairwise O
confusion O
. O
The O
improvement O
is O
even O
more O
prominent O
when O
training O
data O
is O
limited O
. O
section O
: O
Improvement O
in O
Localization Metric
Ability Metric
Recent O
techniques O
for O
improving O
classification Task
performance O
in O
fine Task
- Task
grained Task
recognition Task
are O
based O
on O
summarizing O
and O
extracting O
dense O
localization Method
information O
in O
images O
[ O
reference O
][ O
reference O
] O
. O
Since O
our O
technique O
increases O
classification O
accuracy Metric
, O
we O
wish O
to O
understand O
if O
the O
improvement O
is O
a O
result O
of O
enhanced O
CNN O
localization Method
abilities O
due O
to O
PC Method
. O
To O
measure O
the O
regions O
the O
CNN O
localizes O
on O
, O
we O
utilize O
Gradient Method
- Method
Weighted Method
Class Method
Activation Method
Mapping Method
( O
Grad Method
- Method
CAM Method
) O
[ O
reference O
] O
, O
a O
method O
that O
provides O
a O
heatmap O
of O
visual O
saliency O
as O
produced O
by O
the O
network O
. O
We O
perform O
both O
quantitative O
and O
qualitative O
studies O
of O
localization Method
ability O
of O
PC Method
- O
trained O
models O
. O
Overlap O
in O
Localized O
Regions O
: O
To O
quantify O
the O
improvement O
in O
localization Method
due O
to O
PC Method
, O
we O
construct O
bounding O
boxes O
around O
object O
regions O
obtained O
from O
Grad Method
- Method
CAM Method
, O
by O
thresholding O
the O
heatmap O
values O
at O
0.5 O
, O
and O
choosing O
the O
largest O
box O
returned O
. O
We O
then O
calculate O
the O
mean Metric
IoU Metric
( O
intersection Metric
- Metric
over Metric
- Metric
union Metric
) O
of O
the O
bounding O
box O
with O
the O
provided O
object O
bounding O
boxes O
for O
the O
CUB Material
- Material
200 Material
- Material
2011 Material
dataset Material
. O
We O
compare O
the O
mean Metric
IoU Metric
across O
several O
models O
, O
with O
and O
without O
PC Method
. O
As O
summarized O
in O
Table O
4 O
, O
we O
observe O
an O
average O
3.4 O
% O
improvement O
across O
five O
different O
networks O
, O
implying O
better O
localization Method
accuracy Metric
. O
Change O
in O
Class Method
- Method
Activation Method
Mapping Method
: O
To O
qualitatively O
study O
the O
improvement O
in O
localization Method
due O
to O
PC Method
, O
we O
obtain O
samples O
from O
the O
CUB Material
- Material
200 Material
- Material
2011 Material
dataset Material
and O
visualize O
the O
localization Method
regions O
returned O
from O
Grad Method
- Method
CAM Method
for O
both O
the O
baseline Method
and O
PC Method
- O
trained O
VGG O
- O
16 O
model O
. O
As O
shown O
in O
Figure O
3 O
, O
PC Method
models O
provide O
tighter O
, O
more O
accurate O
localization Method
around O
the O
target O
object O
, O
whereas O
sometimes O
the O
baseline Method
model Method
has O
localization Method
driven O
by O
image O
artifacts O
. O
Figure O
3 O
-( O
a O
) O
has O
an O
example O
of O
the O
types O
of O
distractions O
that O
are O
often O
present O
in O
FGVC Material
images O
( O
the O
cartoon O
bird O
on O
the O
right O
) O
. O
We O
see O
that O
the O
baseline Method
VGG Method
- Method
16 Method
network Method
pays O
For O
all O
cases O
, O
we O
consistently O
observe O
a O
tighter O
and O
more O
accurate O
localization Method
with O
PC Method
, O
whereas O
the O
baseline Method
VGG Method
- Method
16 Method
network Method
often O
latches O
on O
to O
artifacts O
, O
even O
while O
making O
correct O
predictions O
. O
significant O
attention O
to O
the O
distraction O
, O
despite O
making O
the O
correct O
prediction O
. O
With O
PC Method
, O
we O
find O
that O
the O
attention O
is O
limited O
almost O
exclusively O
to O
the O
correct O
object O
, O
as O
desired O
. O
Similarly O
for O
Figure O
3 O
-( O
b O
) O
, O
we O
see O
that O
the O
baseline O
method O
latches O
on O
to O
the O
incorrect O
bird O
category O
, O
which O
is O
corrected O
by O
the O
addition O
of O
PC Method
. O
In O
Figures O
3 O
-( O
c O
- O
d O
) O
, O
we O
see O
that O
the O
baseline Method
classifier Method
makes O
incorrect O
decisions O
due O
to O
poor O
localization Method
, O
mistakes O
that O
are O
resolved O
by O
PC Method
. O
section O
: O
Conclusion O
In O
this O
work O
, O
we O
introduce O
Pairwise Method
Confusion Method
( O
PC Method
) O
, O
an O
optimization Method
procedure Method
to O
improve O
generalizability Task
in O
fine Task
- Task
grained Task
visual Task
classification Task
( O
FGVC Material
) O
tasks O
by O
encouraging O
confusion O
in O
output O
activations O
. O
PC Method
improves O
FGVC Material
performance O
for O
a O
wide O
class O
of O
convolutional Method
architectures Method
while O
fine Task
- Task
tuning Task
. O
Our O
experiments O
indicate O
that O
PC Method
- O
trained O
networks O
show O
improved O
localization Method
performance O
which O
contributes O
to O
the O
gains O
in O
classification O
accuracy Metric
. O
PC Method
is O
easy O
to O
implement O
, O
does O
not O
need O
excessive O
tuning O
during O
training O
, O
and O
does O
not O
add O
significant O
overhead O
during O
test O
time O
, O
in O
contrast O
to O
methods O
that O
introduce O
complex O
localizationbased Method
pooling Method
steps Method
that O
are O
often O
difficult O
to O
implement O
and O
train O
. O
Therefore O
, O
our O
technique O
should O
be O
beneficial O
to O
a O
wide O
variety O
of O
specialized O
neural Method
network Method
models Method
for O
applications O
that O
demand O
for O
fine Task
- Task
grained Task
visual Task
classification Task
or O
learning Task
from O
limited O
labeled O
data O
. O
Consider O
Jeffrey Method
's Method
divergence Method
with O
N O
= O
2 O
classes O
, O
and O
that O
x O
1 O
belongs O
to O
class O
1 O
, O
and O
x O
2 O
belongs O
to O
class O
2 O
. O
For O
a O
model O
with O
parameters O
θ O
that O
correctly O
identifies O
both O
x O
1 O
and O
x O
2 O
by O
training O
using O
cross Metric
- Metric
entropy Metric
loss Metric
, O
p O
θ O
( O
y O
1 O
|x O
1 O
) O
= O
1 O
− O
δ O
1 O
and O
p O
θ O
( O
y O
2 O
|x O
2 O
) O
= O
1 O
− O
δ O
2 O
, O
where O
0 O
< O
δ O
1 O
, O
δ O
2 O
< O
1 O
2 O
( O
since O
the O
classifier Method
outputs O
correct O
predictions O
for O
the O
input O
images O
) O
, O
we O
get O
: O
section O
: O
S1.2 O
Lemmas O
1 O
and O
2 O
from O
Main O
Text O
( O
Euclidean Metric
Confusion Metric
Bounds Metric
) O
Lemma O
1 O
. O
On O
a O
finite O
probability O
space O
, O
for O
probability O
measures O
P O
, O
Q O
: O
where O
D O
J O
( O
P O
, O
Q O
) O
is O
the O
Jeffrey O
's O
Divergence O
between O
P O
and O
Q. O
Proof O
. O
By O
the O
definition O
of O
Euclidean O
Confusion O
, O
we O
have O
: O
For O
a O
finite O
- O
dimensional O
vector O
x O
, O
x O
2 O
≤ O
x O
1 O
, O
therefore O
: O
Since O
D O
TV O
( O
P O
, O
Q O
) O
= O
1 O
2 O
( O
u∈U O
|p O
( O
u O
) O
− O
q O
( O
u O
) O
| O
) O
for O
finite O
alphabet O
U O
, O
we O
have O
: O
Since O
Total O
Variation O
Distance O
is O
symmetric O
, O
we O
have O
: O
By O
Pinsker O
's O
Inequality O
, O
D O
TV O
( O
P O
, O
Q O
) O
≤ O
Lemma O
2 O
. O
On O
a O
finite O
probability O
space O
, O
for O
probability O
measures O
P O
, O
Q O
: O
where O
D O
TV O
denotes O
the O
total O
variation O
distance O
between O
P O
and O
Q. O
Proof O
. O
By O
the O
definition O
of O
Euclidean O
Confusion O
, O
we O
have O
: O
For O
a O
finite O
- O
dimensional O
vector O
x O
, O
x O
2 O
≤ O
x O
1 O
, O
therefore O
: O
Since O
D O
TV O
( O
P O
, O
Q O
) O
= O
Lemma O
3 O
. O
For O
sets O
S O
i O
, O
S O
j O
and O
D O
EC O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
as O
defined O
in O
Equation O
( O
14 O
) O
: O
where O
D O
EN O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
is O
the O
Energy O
Distance O
under O
Euclidean O
norm O
between O
S O
i O
and O
S O
j O
( O
parameterized O
by O
θ O
) O
, O
and O
random O
vectors O
are O
selected O
with O
uniform O
probability O
in O
both O
S O
i O
and O
S O
j O
. O
Proof O
. O
From O
the O
definition O
of O
Euclidean O
Confusion O
, O
we O
have O
: O
Considering O
X O
i O
∼ O
Uniform O
( O
S O
i O
) O
, O
then O
we O
get O
: O
Considering O
X O
j O
∼ O
Uniform O
( O
S O
j O
) O
, O
we O
obtain O
: O
Under O
the O
squared Metric
Euclidean Metric
norm Metric
distance Metric
, O
the O
Energy Metric
Distance Metric
can O
be O
given O
by O
: O
Where O
random O
variables O
X O
, O
X O
∼ O
P O
( O
S O
i O
) O
and O
Y O
, O
Y O
∼ O
P O
( O
S O
j O
) O
. O
If O
P O
( O
S O
i O
) O
= O
Uniform O
( O
S O
i O
) O
, O
and O
P O
( O
S O
j O
) O
= O
Uniform O
( O
S O
j O
) O
, O
we O
have O
by O
substitution O
of O
Equation O
( O
18 O
) O
: O
Corollary O
1 O
. O
For O
sets O
S O
i O
, O
S O
j O
and O
D O
EC O
( O
S O
i O
, O
S O
j O
; O
θ O
) O
as O
defined O
in O
Equation O
( O
14 O
) O
, O
we O
have O
: O
Proof O
. O
From O
Equation O
( O
20 O
) O
, O
we O
have O
: O
From O
Equation O
( O
18 O
) O
, O
we O
have O
: O
For O
S O
i O
= O
S O
j O
, O
we O
have O
with O
X O
i O
, O
X O
j O
∼ O
Uniform O
( O
S O
i O
) O
: O
Replacing O
this O
in O
Equation O
( O
20 O
) O
, O
we O
have O
with O
X O
, O
X O
∼ O
Uniform O
( O
S O
i O
) O
and O
Y O
, O
Y O
∼ O
Uniform O
( O
S O
j O
) O
: O
From O
Szekely O
et O
al O
. O
[ O
reference O
] O
, O
we O
know O
that O
the O
Energy O
Distance O
≥ O
0 O
with O
equality O
if O
and O
only O
if O
S O
i O
= O
S O
j O
. O
Thus O
, O
we O
have O
that O
: O
With O
equality O
only O
when O
S O
i O
= O
S O
j O
. O
section O
: O
S2 O
Training O
Details O
In O
this O
section O
, O
we O
describe O
the O
process O
for O
training O
with O
Pairwise Method
Confusion Method
for O
different O
base O
architectures O
, O
including O
the O
list O
of O
hyperparameters O
using O
for O
different O
datasets Material
. O
section O
: O
ResNet Method
- Method
50 Method
: O
In O
all O
experiments O
, O
we O
train O
for O
40000 O
iterations O
with O
batch O
- O
size O
8 O
, O
with O
a O
linear O
decay O
of O
the O
learning Metric
rate Metric
from O
an O
initial O
value O
of O
0.1 O
. O
The O
hyperparameter O
for O
the O
confusion O
term O
for O
each O
dataset O
is O
given O
in O
Table O
S1 O
. O
Bilinear O
and O
Compact O
Bilinear Method
CNN Method
: O
In O
all O
experiments O
, O
we O
use O
the O
training O
procedure O
described O
by O
the O
authors O
[ O
reference O
] O
. O
In O
addition O
, O
we O
repeat O
the O
described O
step O
2 O
without O
the O
loss O
on O
confusion O
from O
the O
obtained O
weights O
after O
performing O
Step O
2 O
with O
the O
loss O
, O
and O
obtain O
an O
additional O
0.5 O
percent O
gain O
in O
performance O
. O
The O
hyperparameter O
for O
the O
confusion O
term O
for O
each O
dataset O
is O
given O
in O
Table O
S2 O
. O
section O
: O
DenseNet Method
- Method
161 Method
: O
In O
all O
experiments O
, O
we O
train O
for O
40000 O
iterations O
with O
batchsize O
32 O
, O
with O
a O
linear O
decay O
of O
the O
learning Metric
rate Metric
from O
an O
initial O
value O
of O
0.1 O
. O
The O
hyperparameter O
for O
the O
confusion O
term O
for O
each O
dataset O
is O
given O
in O
Table O
S3 O
. O
GoogLeNet Method
: O
In O
all O
experiments O
, O
we O
train O
for O
300000 O
iterations O
with O
batch O
- O
size O
32 O
, O
with O
a O
step O
size O
of O
30000 O
, O
decreasing O
it O
by O
a O
ratio O
of O
0.96 O
. O
The O
hyperparameter O
for O
the O
confusion O
term O
is O
given O
in O
Table O
S4 O
. O
section O
: O
VGGNet Method
- Method
16 Method
: O
In O
all O
experiments O
, O
we O
train O
for O
40000 O
iterations O
with O
batchsize O
32 O
, O
with O
a O
linear O
decay O
of O
the O
learning Metric
rate Metric
from O
an O
initial O
value O
of O
0.1 O
. O
The O
hyperparameter O
for O
the O
confusion O
term O
is O
given O
in O
Table O
S5 O
. O
section O
: O
S3 O
Mean O
and O
Standard O
Deviation O
for O
FGVC Material
Results O
In O
Table O
S6 O
, O
we O
provide O
the O
mean O
and O
standard O
deviation O
values O
over O
five O
independent O
runs O
for O
training O
with O
Pairwise Method
Confusion Method
with O
different O
baseline Method
models Method
. O
These O
results O
correspond O
to O
Table O
2 O
in O
the O
main O
text O
. O
section O
: O
S4 O
Comparison O
with O
Regularization Method
We O
additionally O
compare O
the O
performance O
of O
our O
optimization Method
technique Method
with O
other O
regularization Method
methods Method
as O
well O
. O
We O
first O
compare O
Pairwise Method
Comparison Method
with O
with O
Label Method
- Method
Smoothing Method
Regularization Method
( O
LSR Method
) O
on O
all O
six O
FGVC Material
datasets Material
for O
VGG Material
- Material
Net16 Material
, O
ResNet Method
- Method
50 Method
and O
DenseNet Method
- Method
161 Method
. O
These O
results O
are O
summarized O
in O
Table O
S7 O
. O
Next O
, O
in O
Table O
S8 O
, O
we O
compare O
the O
performance O
of O
Pairwise Method
Confusion Method
( O
PC Method
) O
with O
several O
additional O
regularization Method
techniques Method
on O
the O
CIFAR Material
- Material
10 Material
and O
CIFAR Material
- Material
100 Material
datasets Material
using O
two O
small O
architectures O
: O
CIFAR Material
- Material
10 Material
Quick Material
( O
C10Quick Material
) O
and O
CIFAR Material
- Material
10 Material
Full Material
( O
C10Full Material
) O
, O
which O
are O
standard O
models O
available O
in O
the O
Caffe Method
framework Method
. O
section O
: O
S5 O
Changes O
to O
Class Metric
- Metric
wise Metric
Prediction Metric
Accuracy Metric
We O
find O
that O
while O
the O
average O
and O
lowest O
per O
- O
class O
accuracy Metric
increase O
when O
training O
with O
PC Method
, O
there O
is O
a O
small O
decline O
in O
top O
- O
performing O
class O
accuracy Metric
( O
See O
Table O
S9 O
) O
. O
Moreover O
, O
the O
standard Metric
deviation Metric
in O
per O
- O
class O
accuracy Metric
is O
reduced O
as O
well O
. O
We O
also O
found O
that O
using O
PC Method
slightly O
increased O
false Metric
positive Metric
errors Metric
while O
obtaining O
a O
larger O
reduction O
in O
false Metric
negative Metric
errors Metric
. O
For O
example O
, O
on O
CUB Material
- Material
200 Material
- Material
2011 Material
with O
ResNet Method
- Method
50 Method
, O
the O
average Metric
false Metric
positive Metric
error Metric
is O
increased O
by O
0.06 O
% O
, O
but O
Table O
S6 O
. O
Pairwise Method
Confusion Method
( O
PC Method
) O
obtains O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
six O
widelyused O
fine Task
- Task
grained Task
visual Task
classification Task
datasets Material
( O
A O
- O
F O
) O
. O
Improvement O
over O
the O
baseline O
model O
is O
reported O
as O
( O
∆ O
) O
. O
All O
results O
averaged O
over O
5 O
trials O
with O
standard O
deviations O
reported O
in O
parentheses O
. O
Table O
S8 O
. O
Image Task
classification Task
performance O
and O
train Metric
- Metric
val Metric
gap Metric
( O
∆ O
) O
) O
for O
Pairwise Method
Confusion Method
( O
PC Method
) O
and O
popular O
regularization Method
methods Method
. O
The O
standard O
deviation O
across O
trials O
is O
mentioned O
in O
parentheses O
. O
section O
: O
Method O
the O
average Metric
false Metric
negative Metric
error Metric
is O
reduced O
by O
0.13 O
% O
. O
So O
while O
some O
additional O
mistakes O
are O
made O
in O
terms O
of O
false Metric
positives Metric
, O
we O
curb O
/ O
reduce O
the O
problem O
of O
classifier Task
overconfidence Task
by O
a O
larger O
margin O
. O
section O
: O
section O
: O
Acknowledgements O
: O
We O
would O
like O
to O
thank O
Dr. O
Ashok O
Gupta O
for O
his O
guidance O
on O
bird Task
recognition Task
, O
and O
Dr. O
Sumeet O
Agarwal O
, O
Spandan O
Madan O
and O
Ishaan O
Grover O
for O
their O
feedback O
at O
various O
stages O
of O
this O
work O
. O
section O
: O
