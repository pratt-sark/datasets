Factoring Task
Variations Task
in O
Natural O
Images O
with O
Deep Method
Gaussian Method
Mixture Method
Models Method
Generative Method
models Method
can O
be O
seen O
as O
the O
swiss O
army O
knives O
of O
machine Task
learning Task
, O
as O
many O
problems O
can O
be O
written O
probabilistically O
in O
terms O
of O
the O
distribution O
of O
the O
data O
, O
including O
prediction Task
, O
reconstruction Task
, O
imputation Task
and O
simulation Task
. O
One O
of O
the O
most O
promising O
directions O
for O
unsupervised Task
learning Task
may O
lie O
in O
Deep Method
Learning Method
methods Method
, O
given O
their O
success O
in O
supervised Task
learning Task
. O
However O
, O
one O
of O
the O
current O
problems O
with O
deep Method
unsupervised Method
learning Method
methods Method
, O
is O
that O
they O
often O
are O
harder O
to O
scale O
. O
As O
a O
result O
there O
are O
some O
easier O
, O
more O
scalable O
shallow Method
methods Method
, O
such O
as O
the O
Gaussian Method
Mixture Method
Model Method
and O
the O
Student Method
- Method
t Method
Mixture Method
Model Method
, O
that O
remain O
surprisingly O
competitive O
. O
In O
this O
paper O
we O
propose O
a O
new O
scalable Method
deep Method
generative Method
model Method
for O
images O
, O
called O
the O
Deep Method
Gaussian Method
Mixture Method
Model Method
, O
that O
is O
a O
straightforward O
but O
powerful O
generalization Method
of Method
GMMs Method
to O
multiple O
layers O
. O
The O
parametrization Method
of O
a O
Deep Method
GMM Method
allows O
it O
to O
efficiently O
capture O
products O
of O
variations O
in O
natural O
images O
. O
We O
propose O
a O
new O
EM Method
- O
based O
algorithm O
that O
scales O
well O
to O
large O
datasets O
, O
and O
we O
show O
that O
both O
the O
Expectation O
and O
the O
Maximization Method
steps Method
can O
easily O
be O
distributed O
over O
multiple O
machines O
. O
In O
our O
density Task
estimation Task
experiments O
we O
show O
that O
deeper O
GMM Method
architectures O
generalize O
better O
than O
more O
shallow O
ones O
, O
with O
results O
in O
the O
same O
ballpark O
as O
the O
state O
of O
the O
art O
. O
1 O
Introduction O
There O
has O
been O
an O
increasing O
interest O
in O
generative Method
models Method
for O
unsupervised Task
learning Task
, O
with O
many O
applications O
in O
Image Task
processing Task
[ O
1 O
, O
2 O
] O
, O
natural Task
language Task
processing Task
[ O
3 O
, O
4 O
] O
, O
vision Task
[ O
5 O
] O
and O
audio Task
[ O
6 O
] O
. O
Generative Method
models Method
can O
be O
seen O
as O
the O
swiss O
army O
knives O
of O
machine Task
learning Task
, O
as O
many O
problems O
can O
be O
written O
probabilistically O
in O
terms O
of O
the O
distribution O
of O
the O
data O
, O
including O
prediction Task
, O
reconstruction Task
, O
imputation Task
and O
simulation Task
. O
One O
of O
the O
most O
promising O
directions O
for O
unsupervised Task
learning Task
may O
lie O
in O
Deep Method
Learning Method
methods Method
, O
given O
their O
recent O
results O
in O
supervised Task
learning Task
[ O
7 O
] O
. O
Although O
not O
a O
universal O
recipe O
for O
success O
, O
the O
merits O
of O
deep Method
learning Method
are O
well O
- O
established O
[ O
8 O
] O
. O
Because O
of O
their O
multilayered O
nature O
, O
these O
methods O
provide O
ways O
to O
efficiently O
represent O
increasingly O
complex O
relationships O
as O
the O
number O
of O
layers O
increases O
. O
“ O
Shallow Method
” Method
methods Method
will O
often O
require O
a O
very O
large O
number O
of O
units O
to O
represent O
the O
same O
functions O
, O
and O
may O
therefore O
overfit O
more O
. O
Looking O
at O
real O
- O
valued O
data O
, O
one O
of O
the O
current O
problems O
with O
deep Method
unsupervised Method
learning Method
methods Method
, O
is O
that O
they O
are O
often O
hard O
to O
scale O
to O
large O
datasets O
. O
This O
is O
especially O
a O
problem O
for O
unsupervised Task
learning Task
, O
because O
there O
is O
usually O
a O
lot O
of O
data O
available O
, O
as O
it O
does O
not O
have O
to O
be O
labeled O
( O
e.g. O
images O
, O
videos O
, O
text O
) O
. O
As O
a O
result O
there O
are O
some O
easier O
, O
more O
scalable O
shallow Method
methods Method
, O
such O
as O
the O
Gaussian O
Mixture O
Model O
( O
GMM Method
) O
and O
the O
Student Method
- Method
t Method
Mixture Method
Model Method
( O
STM Method
) Method
, O
that O
remain O
surprisingly O
competitive O
[ O
2 O
] O
. O
Of O
course O
, O
the O
disadvantage O
of O
these O
mixture Method
models Method
is O
that O
they O
have O
less O
representational Method
power Method
than O
deep Method
models Method
. O
In O
this O
paper O
we O
propose O
a O
new O
scalable Method
deep Method
generative Method
model Method
for O
images O
, O
called O
the O
Deep Method
Gaussian Method
Mixture Method
Model Method
( O
Deep Method
GMM Method
) O
. O
The O
Deep Method
GMM Method
is O
a O
straightforward O
but O
powerful O
generalization Method
of Method
Gaussian Method
Mixture Method
Models Method
to O
multiple O
layers O
. O
It O
is O
constructed O
by O
stacking O
multiple O
GMM Method
- O
layers O
on O
top O
of O
each O
other O
, O
which O
is O
similar O
to O
many O
other O
Deep Method
Learning Method
techniques Method
. O
Although O
for O
every O
deep O
GMM Method
, O
one O
could O
construct O
a O
shallow O
GMM Method
with O
the O
same O
density O
function O
, O
it O
would O
require O
an O
exponential O
number O
of O
mixture O
components O
to O
do O
so O
. O
The O
multilayer Method
architecture Method
of O
the O
Deep Method
GMM Method
gives O
rise O
to O
a O
specific O
kind O
of O
parameter Method
tying Method
. O
The O
parameterization O
is O
most O
interpretable O
in O
the O
case O
of O
images O
: O
the O
layers O
in O
the O
architecture O
are O
able O
to O
efficiently O
factorize O
the O
different O
variations O
that O
are O
present O
in O
natural O
images O
: O
changes O
in O
brightness O
, O
contrast O
, O
color O
and O
even O
translations O
or O
rotations O
of O
the O
objects O
in O
the O
image O
. O
Because O
each O
of O
these O
variations O
will O
affect O
the O
image O
separately O
, O
a O
traditional O
mixture Method
model Method
would O
need O
an O
exponential O
number O
of O
components O
to O
model O
each O
combination O
of O
variations O
, O
whereas O
a O
Deep Method
GMM Method
can O
factor O
these O
variations O
and O
model O
them O
individually O
. O
The O
proposed O
training Method
algorithm Method
for O
the O
Deep Method
GMM Method
is O
based O
on O
the O
most O
popular O
principle O
for O
training O
GMMs Method
: O
Expectation Method
Maximization Method
( O
EM Method
) O
. O
Although O
stochastic Method
gradient Method
( Method
SGD Method
) Method
is O
also O
a O
possible O
option O
, O
we O
suggest O
the O
use O
of O
EM Method
, O
as O
it O
is O
inherently O
more O
parallelizable O
. O
As O
we O
will O
show O
later O
, O
both O
the O
Expectation O
and O
the O
Maximization Method
steps Method
can O
easily O
be O
distributed O
on O
multiple O
computation O
units O
or O
machines O
, O
with O
only O
limited O
communication O
between O
compute O
nodes O
. O
Although O
there O
has O
been O
a O
lot O
of O
effort O
in O
scaling O
up O
SGD Method
for O
deep Task
networks Task
[ O
9 O
] O
, O
the O
Deep Method
GMM Method
is O
parallelizable O
by O
design O
. O
The O
remainder O
of O
this O
paper O
is O
organized O
as O
follows O
. O
We O
start O
by O
introducing O
the O
design O
of O
deep Method
GMMs Method
before O
explaining O
the O
EM Method
algorithm O
for O
training O
them O
. O
Next O
, O
we O
discuss O
the O
experiments O
where O
we O
examine O
the O
density Task
estimation Task
performance O
of O
the O
deep O
GMM Method
, O
as O
a O
function O
of O
the O
number O
of O
layers O
, O
and O
in O
comparison O
with O
other O
methods O
. O
We O
conclude O
in O
Section O
5 O
, O
where O
also O
discuss O
some O
unsolved O
problems O
for O
future O
work O
. O
2 O
Stacking Method
Gaussian Method
Mixture Method
layers Method
Deep Method
GMMs Method
are O
best O
introduced O
by O
looking O
at O
some O
special O
cases O
: O
the O
multivariate Method
normal Method
distribution Method
and O
the O
Gaussian Method
Mixture Method
model Method
. O
One O
way O
to O
define O
a O
multivariate O
normal O
variable O
x O
is O
as O
a O
standard O
normal O
variable O
z O
⇠ O
N O
( O
0 O
, O
In O
) O
that O
has O
been O
transformed O
with O
a O
certain O
linear Method
transformation Method
: O
x O
= O
Az O
+ O
b O
, O
so O
that O
p O
( O
x O
) O
= O
N O
x|b O
, O
AAT O
. O
This O
is O
visualized O
in O
Figure O
1 O
( O
a O
) O
. O
The O
same O
interpretation O
can O
be O
applied O
to O
Gaussian Method
Mixture Method
Models Method
, O
see O
Figure O
1 O
( O
b O
) O
. O
A O
transformation O
is O
chosen O
from O
set O
of O
( O
square O
) O
transformations O
Ai O
, O
i O
= O
1 O
. O
. O
. O
N O
( O
each O
having O
a O
bias O
term O
bi O
) O
with O
probabilities O
⇡ O
i O
, O
i O
= O
1 O
. O
. O
. O
N O
, O
such O
that O
the O
resulting O
distribution O
becomes O
: O
p O
( O
x O
) O
= O
NX O
i=1 O
⇡ O
iN O
x|bi O
, O
AiATi O
. O
With O
this O
in O
mind O
, O
it O
is O
easy O
to O
generalize O
GMMs Method
in O
a O
multi O
- O
layered O
fashion O
. O
Instead O
of O
sampling O
one O
transformation O
from O
a O
set O
, O
we O
can O
sample O
a O
path O
of O
transformations O
in O
a O
network O
of O
k O
layers O
, O
see O
Figure O
1 O
( O
c O
) O
. O
The O
standard O
normal O
variable O
z O
is O
now O
successively O
transformed O
with O
a O
transformation O
from O
each O
layer O
of O
the O
network O
. O
Let O
be O
the O
set O
of O
all O
possible O
paths O
through O
the O
network O
. O
Each O
path O
p O
= O
( O
p1 O
, O
p2 O
, O
. O
. O
. O
, O
pk O
) O
2 O
has O
a O
probability O
⇡ O
p O
of O
being O
sampled O
, O
with O
X O
p2 O
⇡ O
p O
= O
X O
p1 O
, O
p2 O
, O
... O
, O
pk O
⇡ O
( O
p1 O
, O
p2 O
, O
... O
, O
pk O
) O
= O
1 O
. O
Here O
Nj O
is O
the O
number O
of O
components O
in O
layer O
j. O
The O
density O
function O
of O
x O
is O
: O
p O
( O
x O
) O
= O
X O
p2 O
⇡ O
pN O
x| O
p O
, O
⌦ O
p O
⌦ O
Tp O
, O
( O
1 O
) O
with O
p O
= O
bk O
, O
pk O
+ O
Ak O
, O
ik O
( O
. O
. O
. O
( O
b2 O
, O
p2 O
+ O
A2 O
, O
p2b1 O
, O
p1 O
) O
) O
( O
2 O
) O
⌦ O
p O
= O
1Y O
j O
= O
k O
Aj O
, O
pj O
. O
( O
3 O
) O
Here O
Am O
, O
n O
and O
bm O
, O
n O
are O
the O
n’th O
transformation O
matrix O
and O
bias O
of O
the O
m’th O
layer O
. O
Notice O
that O
one O
can O
also O
factorize O
⇡ O
p O
as O
follows O
: O
⇡ O
( O
p1 O
, O
p2 O
, O
... O
, O
pk O
) O
= O
⇡ O
p1 O
⇡ O
p2 O
. O
. O
. O
⇡ O
pk O
, O
so O
that O
each O
layer O
has O
its O
own O
set O
of O
parameters O
associated O
with O
it O
. O
In O
our O
experiments O
, O
however O
, O
this O
had O
very O
little O
difference O
on O
the O
log O
likelihood O
. O
This O
would O
mainly O
be O
useful O
for O
very O
large O
networks O
. O
The O
GMM Method
is O
a O
special O
case O
of O
the O
deep O
GMM Method
having O
only O
one O
layer O
. O
Moreover O
, O
each O
deep O
GMM Method
can O
be O
constructed O
by O
a O
GMM Method
with O
Qk O
j O
Nj O
components O
, O
where O
every O
path O
in O
the O
network O
represents O
one O
component O
in O
the O
GMM Method
. O
The O
parameters O
of O
these O
components O
are O
tied O
to O
each O
other O
in O
the O
way O
the O
deep O
GMM Method
is O
defined O
. O
Because O
of O
this O
tying O
, O
the O
number O
of O
parameters O
to O
train O
is O
proportional O
toPk O
j O
Nj O
. O
Still O
, O
the O
density Method
estimator Method
is O
quite O
expressive O
as O
it O
can O
represent O
a O
large O
number O
of O
Gaussian Method
mixture Method
components Method
. O
This O
is O
often O
the O
case O
with O
deep Method
learning Method
methods Method
: O
Shallow Method
architectures Method
can O
often O
theoretically O
learn O
the O
same O
functions O
, O
but O
will O
require O
a O
much O
larger O
number O
of O
parameters O
[ O
8 O
] O
. O
When O
the O
kind O
of O
compound O
functions O
that O
a O
deep Method
learning Method
method Method
is O
able O
to O
model O
are O
appropriate O
for O
the O
type O
of O
data O
, O
their O
performance O
will O
often O
be O
better O
than O
their O
shallow O
equivalents O
, O
because O
of O
the O
smaller O
risk O
of O
overfitting O
. O
In O
the O
case O
of O
images O
, O
but O
also O
for O
other O
types O
of O
data O
, O
we O
can O
imagine O
why O
this O
network Method
structure Method
might O
be O
useful O
. O
A O
lot O
of O
images O
share O
the O
same O
variations O
such O
as O
rotations O
, O
translations O
, O
brightness O
changes O
, O
etc O
.. O
These O
deformations O
can O
be O
represented O
by O
a O
linear Method
transformation Method
in O
the O
pixel O
space O
. O
When O
learning O
a O
deep O
GMM Method
, O
the O
model O
may O
pick O
up O
on O
these O
variations O
in O
the O
data O
that O
are O
shared O
amongst O
images O
by O
factoring O
and O
describing O
them O
with O
the O
transformations O
in O
the O
network O
. O
The O
hypothesis O
of O
this O
paper O
is O
that O
Deep Method
GMMs Method
overfit O
less O
than O
normal Method
GMMs Method
as O
the O
complexity O
of O
their O
density O
functions O
increase O
because O
the O
parameter Method
tying Method
of O
the O
Deep Method
GMM Method
will O
force O
it O
to O
learn O
more O
useful O
functions O
. O
Note O
that O
this O
is O
one O
of O
the O
reasons O
why O
other O
deep Method
learning Method
methods Method
are O
so O
successful O
. O
The O
only O
difference O
is O
that O
the O
parameter Method
tying Method
in O
deep Method
GMMs Method
is O
more O
explicit O
and O
interpretable O
. O
A O
closely O
related O
method O
is O
the O
deep Method
mixture Method
of Method
factor Method
analyzers Method
( Method
DMFA Method
) Method
model Method
[ O
10 O
] O
, O
which O
is O
an O
extension O
of O
the O
Mixture Method
of Method
Factor Method
Analyzers Method
( O
MFA Method
) Method
model Method
[ O
11 O
] O
. O
The O
DMFA Method
model Method
has O
a O
tree O
structure O
in O
which O
every O
node O
is O
a O
factor Method
analyzer Method
that O
inherits O
the O
low O
- O
dimensional O
latent O
factors O
from O
its O
parent O
. O
Training Method
is O
performed O
layer O
by O
layer O
, O
where O
the O
dataset O
is O
hierarchically O
clustered O
and O
the O
children O
of O
each O
node O
are O
trained O
as O
a O
MFA Method
on O
a O
different O
subset O
of O
the O
data O
using O
the O
MFA O
EM Method
algorithm O
. O
The O
parents O
nodes O
are O
kept O
constant O
when O
training O
its O
children O
. O
The O
main O
difference O
with O
the O
proposed O
method O
is O
that O
in O
the O
Deep Method
GMM Method
the O
nodes O
of O
each O
layer O
are O
connected O
to O
all O
nodes O
of O
the O
layer O
above O
. O
The O
layers O
are O
trained O
jointly O
and O
the O
higher O
level O
nodes O
will O
adapt O
to O
the O
lower O
level O
nodes O
. O
3 O
Training O
deep Method
GMMs Method
with O
EM Method
The O
algorithm O
we O
propose O
for O
training O
Deep Method
GMMs Method
is O
based O
on O
Expectation O
Maximization O
( O
EM Method
) O
. O
The O
optimization Task
is O
similar O
to O
that O
of O
a O
GMM Method
: O
in O
the O
E O
- O
step O
we O
will O
compute O
the O
posterior O
probabilities O
np O
that O
a O
path O
p O
was O
responsible O
for O
generating O
xn O
, O
also O
called O
the O
responsibilities O
. O
In O
the O
maximization Task
step Task
, O
the O
parameters O
of O
the O
model O
will O
be O
optimized O
given O
those O
responsibilities O
. O
3.1 O
Expectation O
From O
Equation O
1 O
we O
get O
the O
the O
log O
- O
likelihood O
given O
the O
data O
: O
X O
n O
log O
p O
( O
xn O
) O
= O
X O
n O
log O
2 O
4 O
X O
p2 O
⇡ O
pN O
xn| O
p O
, O
⌦ O
p O
⌦ O
Tp O
3 O
5 O
. O
This O
is O
the O
global O
objective O
for O
the O
Deep Method
GMM Method
to O
optimize O
. O
When O
taking O
the O
derivative O
with O
respect O
to O
a O
parameter O
✓ O
we O
get O
: O
r O
✓ O
X O
n O
log O
p O
( O
xn O
) O
= O
X O
n O
, O
p O
⇡ O
pN O
xn| O
p O
, O
⌦ O
p O
⌦ O
Tp O
⇥ O
r O
✓ O
logN O
xn| O
p O
, O
⌦ O
p O
⌦ O
Tp O
⇤ O
P O
q O
⇡ O
qN O
xn| O
q O
, O
⌦ O
q O
⌦ O
Tq O
= O
X O
n O
, O
p O
npr O
✓ O
logN O
xn| O
p O
, O
⌦ O
p O
⌦ O
Tp O
, O
with O
np O
= O
⇡ O
pN O
xn| O
p O
, O
⌦ O
p O
⌦ O
Tp O
P O
q2 O
⇡ O
qN O
xn| O
q O
, O
⌦ O
q O
⌦ O
Tq O
, O
the O
equation O
for O
the O
responsibilities O
. O
Although O
np O
generally O
depend O
on O
the O
parameter O
✓ O
, O
in O
the O
EM Method
algorithm O
the O
responsibilities O
are O
assumed O
to O
remain O
constant O
when O
optimizing O
the O
model O
parameters O
in O
the O
M O
- O
step O
. O
The O
E O
- O
step O
is O
very O
similar O
to O
that O
of O
a O
standard O
GMM Method
, O
but O
instead O
of O
computing O
the O
responsibilities O
nk O
for O
every O
component O
k O
, O
one O
needs O
to O
compute O
them O
for O
every O
path O
p O
= O
( O
p1 O
, O
p2 O
, O
. O
. O
. O
, O
pk O
) O
2 O
. O
This O
is O
because O
every O
path O
represents O
a O
Gaussian Method
mixture Method
component Method
in O
the O
equivalent O
shallow O
GMM Method
. O
Because O
np O
needs O
to O
be O
computed O
for O
each O
datapoint O
independently O
, O
the O
E Method
- Method
step Method
is O
very O
easy O
to O
parallelize O
. O
Often O
a O
simple O
way O
to O
increase O
the O
speed O
of O
convergence Metric
and O
to O
reduce O
computation Metric
time Metric
is O
to O
use O
an O
EM Method
- O
variant O
with O
“ O
hard O
” O
assignments O
. O
Here O
only O
one O
of O
the O
responsibilities O
of O
each O
datapoint O
is O
set O
to O
1 O
: O
np O
= O
⇢ O
1 O
p O
= O
argmaxq O
⇡ O
qN O
xn| O
q O
, O
⌦ O
q O
⌦ O
Tq O
0 O
otherwise O
( O
4 O
) O
Heuristic O
Because O
the O
number O
of O
paths O
is O
the O
product O
of O
the O
number O
of O
components O
per O
layer O
( O
Qk O
j O
Nj O
) O
, O
computing O
the O
responsibilities O
can O
become O
intractable O
for O
big O
Deep O
GMM Method
networks O
. O
However O
, O
when O
using O
hard O
- O
EM Method
variant O
( O
eq O
. O
4 O
) O
, O
this O
problem O
reduces O
to O
finding O
the O
best O
path O
for O
each O
datapoint O
, O
for O
which O
we O
can O
use O
efficient O
heuristics O
. O
Here O
we O
introduce O
such O
a O
heuristic O
that O
does O
not O
hurt O
the O
performance O
significantly O
, O
while O
allowing O
us O
to O
train O
much O
larger O
networks O
. O
We O
optimize O
the O
path O
p O
= O
( O
p1 O
, O
p2 O
, O
. O
. O
. O
, O
pk O
) O
, O
which O
is O
a O
multivariate O
discrete O
variable O
, O
with O
a O
coordinate Method
ascent Method
algorithm Method
. O
This O
means O
we O
change O
the O
parameters O
pi O
layer O
per O
layer O
, O
while O
keeping O
the O
parameter O
values O
of O
the O
other O
layers O
constant O
. O
After O
we O
have O
changed O
all O
the O
variables O
one O
time O
( O
one O
pass O
) O
, O
we O
can O
repeat O
. O
The O
heuristic O
described O
above O
only O
requires O
Pk O
j O
Nj O
path O
evaluations O
per O
pass O
. O
In O
Figure O
2 O
we O
compare O
the O
heuristic O
with O
the O
full Method
search Method
. O
On O
the O
left O
we O
see O
that O
after O
3 O
passes O
the O
heuristic O
converges O
to O
a O
local O
optimum O
. O
In O
the O
middle O
we O
see O
that O
when O
repeating O
the O
heuristic Method
algorithm Method
a O
couple O
of O
times O
with O
different O
random O
initializations O
, O
and O
keeping O
the O
best O
path O
after O
each O
iteration O
, O
the O
loglikelihood Method
converges O
to O
the O
optimum O
. O
In O
our O
experiments O
we O
initialized O
the O
heuristic O
with O
the O
optimal O
path O
from O
the O
previous O
E O
- O
step O
( O
warm O
start O
) O
and O
performed O
the O
heuristic Method
algorithm Method
for O
1 O
pass O
. O
Subsequently O
we O
ran O
the O
algorithm O
for O
a O
second O
time O
with O
a O
random O
initialization O
for O
two O
passes O
for O
the O
possibility O
of O
finding O
a O
better O
optimum O
for O
each O
datapoint O
. O
Each O
E O
- O
step O
thus O
required O
3 O
⇣ O
Pk O
j O
Nj O
⌘ O
path O
evaluations O
. O
In O
Figure O
2 O
( O
c O
) O
we O
show O
an O
example O
of O
the O
percentage O
of O
data O
points O
( O
called O
the O
switch Metric
- Metric
rate Metric
) O
that O
had O
a O
better O
optimum O
with O
this O
second O
initialization O
for O
each O
EM Method
- O
iteration O
. O
We O
can O
see O
from O
this O
Figure O
that O
the O
switchrate O
quickly O
becomes O
very O
small O
, O
which O
means O
that O
using O
the O
responsibilities O
from O
the O
previous O
E O
- O
step O
is O
an O
efficient O
initialization O
for O
the O
current O
one O
. O
Although O
the O
number O
of O
path O
evaluations O
with O
the O
heuristic Method
is O
substantially O
smaller O
than O
with O
the O
full O
search O
, O
we O
saw O
in O
our O
experiments O
that O
the O
performance O
of O
the O
resulting O
trained O
Deep Method
GMMs Method
was O
ultimately O
similar O
. O
3.2 O
Maximization Task
In O
the O
maximization Task
step Task
, O
the O
parameters O
are O
updated O
to O
maximize O
the O
log O
likelihood O
of O
the O
data O
, O
given O
the O
responsibilities O
. O
Although O
standard O
optimization Method
techniques Method
for O
training O
deep Method
networks Method
can O
be O
used O
( O
such O
as O
SGD Method
) O
, O
Deep Method
GMMs Method
have O
some O
interesting O
properties O
that O
allow O
us O
to O
train O
them O
more O
efficiently O
. O
Because O
these O
properties O
are O
not O
obvious O
at O
first O
sight O
, O
we O
will O
derive O
the O
objective O
and O
gradient O
for O
the O
transformation O
matrices O
Ai O
, O
j O
in O
a O
Deep Method
GMM Method
. O
After O
that O
we O
will O
discuss O
various O
ways O
for O
optimizing O
them O
. O
For O
convenience O
, O
the O
derivations O
in O
this O
section O
are O
based O
on O
the O
hard O
- O
EM Method
variant O
and O
with O
omission O
of O
the O
bias O
- O
terms O
parameters O
. O
Equations O
without O
these O
simplifications O
can O
be O
obtained O
in O
a O
similar O
manner O
. O
In O
the O
hard O
- O
EM Method
variant O
, O
it O
is O
assumed O
that O
each O
datapoint O
in O
the O
dataset O
was O
generated O
by O
a O
path O
p O
, O
for O
which O
n O
, O
p O
= O
1 O
. O
The O
likelihood O
of O
x O
given O
the O
parameters O
of O
the O
transformations O
on O
this O
path O
is O
p O
( O
x O
) O
= O
A O
1 O
1 O
, O
p1 O
. O
. O
. O
A O
1k O
, O
pk O
N O
⇣ O
A O
1 O
1 O
, O
p1 O
. O
. O
. O
A O
1 O
k O
, O
pk O
x|0 O
, O
In O
⌘ O
, O
( O
5 O
) O
where O
we O
use O
|·| O
to O
denote O
the O
absolute O
value O
of O
the O
determinant O
. O
Now O
let O
’s O
rewrite O
: O
z O
= O
A O
1 O
i O
+ O
1 O
, O
pi O
+ O
1 O
. O
. O
. O
A O
1 O
k O
, O
pk O
x O
( O
6 O
) O
Q O
= O
A O
1 O
i O
, O
pi O
( O
7 O
) O
Rp O
= O
A O
1 O
1 O
, O
p1 O
. O
. O
. O
A O
1 O
i O
1 O
, O
pi O
1 O
, O
( O
8 O
) O
z O
so O
that O
we O
get O
( O
omitting O
the O
constant O
term O
w.r.t O
. O
Q O
) O
: O
log O
p O
( O
x O
) O
/ O
log O
|Q| O
+ O
logN O
( O
RpQz|0 O
, O
In O
) O
. O
( O
9 O
) O
Figure O
3 O
gives O
a O
visual O
overview O
. O
We O
have O
“ O
folded O
” O
the O
layers O
above O
the O
current O
layer O
into O
one O
. O
This O
means O
that O
each O
path O
p O
through O
the O
network O
above O
the O
current O
layer O
is O
equivalent O
to O
a O
transformation Method
Rp Method
in O
the O
folded Method
version Method
. O
The O
transformation O
matrix O
for O
which O
we O
will O
derive O
the O
objective O
and O
gradient O
is O
called O
Q. O
The O
average O
log O
- O
likelihood O
of O
all O
the O
data O
points O
that O
are O
generated O
by O
paths O
that O
pass O
through O
Q O
is O
: O
1 O
N O
X O
i O
log O
p O
( O
xi O
) O
/ O
log O
|Q| O
+ O
1 O
N O
X O
p O
X O
i2 O
p O
logN O
( O
RpQzi|0 O
, O
I O
) O
( O
10 O
) O
= O
log O
|Q| O
1 O
2 O
X O
p O
⇡ O
pTr O
⇥ O
pQ O
T O
⌦ O
pQ O
⇤ O
, O
( O
11 O
) O
where O
⇡ O
p O
= O
Np O
N O
, O
p O
= O
1 O
Np O
P O
i2 O
p O
ziz O
T O
i O
and O
⌦ O
p O
= O
RTp O
Rp O
. O
For O
the O
gradient O
we O
get O
: O
1 O
N O
rQ O
X O
i O
log O
p O
( O
xi O
) O
= O
Q O
T O
X O
p O
⇡ O
p O
pQ O
T O
⌦ O
p. O
( O
12 O
) O
Optimization Task
Notice O
how O
in O
Equation O
11 O
the O
summation O
over O
the O
data O
points O
has O
been O
converted O
to O
a O
summation Method
over Method
covariance Method
matrices Method
: O
one O
for O
each O
path1 O
. O
If O
the O
number O
of O
paths O
is O
small O
enough O
, O
this O
means O
we O
can O
use O
full Method
gradient Method
updates Method
instead O
of O
mini Method
- Method
batched Method
updates Method
( O
e.g. O
SGD Method
) O
. O
The O
computation O
of O
the O
covariance O
matrices O
is O
fairly O
efficient O
and O
can O
be O
done O
in O
parallel O
. O
This O
formulation O
also O
allows O
us O
to O
use O
more O
advanced O
optimization Method
methods Method
, O
such O
as O
LBFGS Method
- Method
B Method
[ O
12 O
] O
. O
In O
the O
setup O
described O
above O
, O
we O
need O
to O
keep O
the O
transformation O
Rp O
constant O
while O
optimizing O
Q. O
This O
is O
why O
in O
each O
M O
- O
step O
the O
Deep Method
GMM Method
is O
optimized O
layer O
- O
wise O
from O
top O
to O
bottom O
, O
updating O
one O
layer O
at O
a O
time O
. O
It O
is O
possible O
to O
go O
over O
this O
process O
multiple O
times O
for O
each O
M O
- O
step O
. O
Important O
to O
note O
is O
that O
this O
way O
the O
optimization Task
of Task
Q Task
does O
not O
depend O
on O
any O
other O
parameters O
in O
the O
same O
layer O
. O
So O
for O
each O
layer O
, O
the O
optimization Task
of O
the O
different O
nodes O
can O
be O
done O
in O
parallel O
on O
multiple O
cores O
or O
machines O
. O
Moreover O
, O
nodes O
in O
the O
same O
layer O
do O
not O
share O
data O
points O
when O
using O
the O
EMvariant Method
with O
hard O
- O
assignments O
. O
Another O
advantage O
is O
that O
this O
method O
is O
easy O
to O
control O
, O
as O
there O
are O
no O
learning O
rates O
or O
other O
optimization O
parameters O
to O
be O
tuned O
, O
when O
using O
L Method
- Method
BFGS Method
- Method
B Method
“ O
out O
of O
the O
box O
” O
. O
A O
disadvantage O
is O
that O
one O
needs O
to O
sum O
over O
all O
possible O
paths O
above O
the O
current O
node O
in O
the O
gradient Task
computation Task
. O
For O
deeper Task
networks Task
, O
this O
may O
become O
problematic O
when O
optimizing O
the O
lower O
- O
level O
nodes O
. O
Alternatively O
, O
one O
can O
also O
evaluate O
( O
11 O
) O
using O
Kronecker O
products O
as O
· O
· O
· O
= O
log O
|Q| O
+ O
vec O
( O
Q O
) O
T O
( O
X O
p O
⇡ O
p O
( O
⌦ O
p O
⌦ O
p O
) O
) O
vec O
( O
Q O
) O
( O
13 O
) O
1Actually O
we O
only O
need O
to O
sum O
over O
the O
number O
of O
possible O
transformations O
Rp O
above O
the O
node O
Q. O
and O
Equation O
12 O
as O
· O
· O
· O
= O
Q O
T O
+ O
2mat O
( O
X O
p O
⇡ O
p O
( O
⌦ O
p O
⌦ O
p O
) O
) O
vec O
( O
Q O
) O
! O
. O
( O
14 O
) O
Here O
vec Method
is O
the O
vectorization Method
operator Method
and O
mat O
its O
inverse O
. O
With O
these O
formulations O
we O
do O
n’t O
have O
to O
loop O
over O
the O
number O
of O
paths O
anymore O
during O
the O
optimization Task
. O
This O
makes O
the O
inner Method
optimization Method
with O
LBFGS Method
- Method
B Method
even O
faster O
. O
We O
only O
have O
to O
construct O
P O
p O
⇡ O
p O
( O
⌦ O
p O
⌦ O
p O
) O
once O
, O
which O
is O
also O
easy O
to O
parallelize O
. O
These O
equation O
thus O
allow O
us O
to O
train O
even O
bigger O
Deep O
GMM Method
architectures O
. O
A O
disadvantage O
, O
however O
, O
is O
that O
it O
requires O
the O
dimensionality O
of O
the O
data O
to O
be O
small O
enough O
to O
efficiently O
construct O
the O
Kronecker O
products O
. O
When O
the O
aforementioned O
formulations O
are O
intractable O
because O
there O
are O
too O
number O
layers O
in O
the O
Deep Method
GMM Method
and O
the O
data O
dimensionality O
is O
to O
high O
, O
we O
can O
also O
optimize O
the O
parameters O
using O
backpropagation Method
with O
a O
minibatch Method
algorithm Method
, O
such O
as O
Stochastic Method
Gradient Method
Descent Method
( O
SGD Method
) O
. O
This O
approach O
works O
for O
much O
deeper Task
networks Task
, O
because O
we O
do O
n’t O
need O
to O
sum O
over O
the O
number O
of O
paths O
. O
From O
Equation O
9 O
we O
see O
that O
this O
is O
basically O
the O
same O
as O
minimizing O
the O
L2 O
norm O
of O
RpQz O
, O
with O
log O
|Q| O
as O
regularization O
term O
. O
Disadvantages O
include O
the O
use O
of O
learning O
rates O
and O
other O
parameters O
such O
as O
momentum O
, O
which O
requires O
more O
engineering O
and O
fine O
- O
tuning O
. O
The O
most O
naive O
way O
is O
to O
optimize O
the O
deep O
GMM Method
with O
SGD Method
is O
by O
simultaneously O
optimizing O
all O
parameters O
, O
as O
is O
common O
in O
neural Method
networks Method
. O
When O
doing O
this O
it O
is O
important O
that O
the O
parameters O
of O
all O
nodes O
are O
converged O
enough O
in O
each O
M O
- O
step O
, O
otherwise O
nodes O
that O
are O
not O
optimized O
enough O
may O
have O
very O
low O
responsibilities O
in O
the O
following O
E O
- O
step O
( O
s O
) O
. O
This O
results O
in O
whole O
parts O
of O
the O
network O
becoming O
unused O
, O
which O
is O
the O
equivalent O
of O
empty O
clusters O
during O
GMM Method
or O
k O
- O
means O
training O
. O
An O
alternative O
way O
of O
using O
SGD Method
is O
again O
by O
optimizing O
the O
Deep O
GMM Method
layer O
by O
layer O
. O
This O
has O
the O
advantage O
that O
we O
have O
more O
control O
over O
the O
optimization Task
, O
which O
prevents O
the O
aforementioned O
problem O
of O
unused O
paths O
. O
But O
more O
importantly O
, O
we O
can O
now O
again O
parallelize O
over O
the O
number O
of O
nodes O
per O
layer O
. O
4 O
Experiments O
and O
Results O
For O
our O
experiments O
we O
used O
the O
Berkeley O
Segmentation O
Dataset O
( O
BSDS300 O
) O
[ O
13 O
] O
, O
which O
is O
a O
commonly O
used O
benchmark O
for O
density Task
modeling Task
of Task
image Task
patches Task
and O
the O
tiny O
images O
dataset O
[ O
14 O
] O
. O
For O
BSDS300 O
we O
follow O
the O
same O
setup O
of O
Uria O
et O
al O
. O
[ O
15 O
] O
, O
which O
is O
best O
practice O
for O
this O
dataset O
. O
8 O
by O
8 O
grayscale O
patches O
are O
drawn O
from O
images O
of O
the O
dataset O
. O
The O
train O
and O
test O
sets O
consists O
of O
200 O
and O
100 O
images O
respectively O
. O
Because O
each O
pixel O
is O
quantized O
, O
it O
can O
only O
contain O
integer O
values O
between O
0 O
and O
255 O
. O
To O
make O
the O
integer O
pixel O
values O
continuous O
, O
uniform O
noise O
( O
between O
0 O
and O
1 O
) O
is O
added O
. O
Afterwards O
, O
the O
images O
are O
divided O
by O
256 O
so O
that O
the O
pixel O
values O
lie O
in O
the O
range O
[ O
0 O
, O
1 O
] O
. O
Next O
, O
the O
patches O
are O
preprocessed O
by O
removing O
the O
mean O
pixel O
value O
of O
every O
image O
patch O
. O
Because O
this O
reduces O
the O
implicit O
dimensionality O
of O
the O
data O
, O
the O
last O
pixel O
value O
is O
removed O
. O
This O
results O
in O
the O
data O
points O
having O
63 O
dimensions O
. O
For O
the O
tiny O
images O
dataset O
we O
rescale O
the O
images O
to O
8 O
by O
8 O
and O
then O
follow O
the O
same O
setup O
. O
This O
way O
we O
also O
have O
low O
resolution O
image O
data O
to O
evaluate O
on O
. O
In O
all O
the O
experiments O
described O
in O
this O
section O
, O
we O
used O
the O
following O
setup O
for O
training O
Deep Method
GMMs Method
. O
We O
used O
the O
hard O
- O
EM Method
variant O
, O
with O
the O
aforementioned O
heuristic O
in O
the O
E O
- O
step O
. O
For O
each O
M O
- O
step O
we O
used O
LBFGS Method
- Method
B Method
for O
1000 O
iterations O
by O
using O
equations O
( O
13 O
) O
and O
( O
14 O
) O
for O
the O
objective O
and O
gradient O
. O
The O
total O
number O
of O
iterations O
we O
used O
for O
EM Method
was O
fixed O
to O
100 O
, O
although O
fewer O
iterations O
were O
usually O
sufficient O
. O
The O
only O
hyperparameters O
were O
the O
number O
of O
components O
for O
each O
layer O
, O
which O
were O
optimized O
on O
a O
validation O
set O
. O
Because O
GMMs Method
are O
in O
theory O
able O
to O
represent O
the O
same O
probability O
density O
functions O
as O
a O
Deep Method
GMM Method
, O
we O
first O
need O
to O
assess O
wether O
using O
multiple Method
layers Method
with O
a O
deep O
GMM Method
improves O
performance O
. O
The O
results O
of O
a O
GMM Method
( O
one O
layer O
) O
and O
Deep Method
GMMs Method
with O
two O
or O
three O
layers O
are O
given O
in O
4 O
( O
a O
) O
. O
As O
we O
increase O
the O
complexity O
and O
number O
of O
parameters O
of O
the O
model O
by O
changing O
the O
number O
of O
components O
in O
the O
top O
layer O
, O
a O
plateau O
is O
reached O
and O
the O
models O
ultimately O
start O
overfitting O
. O
For O
the O
deep Method
GMMs Method
, O
the O
number O
of O
components O
in O
the O
other O
layers O
was O
kept O
constant O
( O
5 O
components O
) O
. O
The O
Deep Method
GMMs Method
seem O
to O
generalize O
better O
. O
Although O
they O
have O
a O
similar O
number O
of O
parameters O
, O
they O
are O
able O
to O
model O
more O
complex O
relationships O
, O
without O
overfitting O
. O
We O
also O
tried O
this O
experiment O
on O
a O
more O
difficult O
dataset O
by O
using O
highly O
downscaled O
images O
from O
the O
tiny O
images O
dataset O
, O
see O
Figure O
4 O
( O
b O
) O
. O
Because O
there O
are O
less O
correlations O
between O
the O
pixels O
of O
a O
downscaled O
image O
than O
between O
those O
of O
an O
image O
patch O
, O
the O
average Metric
log Metric
likelihood Metric
values Metric
are O
lower O
. O
Overall O
we O
can O
see O
that O
the O
Deep Method
GMM Method
performs O
well O
on O
both O
low O
and O
high O
resolution O
natural O
images O
. O
Next O
we O
will O
compare O
the O
deep O
GMM Method
with O
other O
published O
methods O
on O
this O
task O
. O
Results O
are O
shown O
in O
Table O
1 O
. O
The O
first O
method O
is O
the O
RNADE Method
model Method
, O
a O
new O
deep Method
density Method
estimation Method
technique Method
which O
is O
an O
extension O
of O
the O
NADE Method
model Method
for O
real O
valued O
data O
[ O
16 O
, O
15 O
] O
. O
EoRNADE Method
, O
which O
stands O
for O
ensemble Method
of Method
RNADE Method
models Method
, O
is O
currently O
the O
state O
of O
the O
art O
. O
We O
also O
report O
the O
log Metric
- Metric
likelihood Metric
results O
of O
two O
mixture Method
models Method
: O
the O
GMM Method
and O
the O
Student Method
- Method
T Method
Mixture Method
model Method
, O
from O
[ O
2 O
] O
. O
Overall O
we O
see O
that O
the O
Deep Method
GMM Method
has O
a O
strong O
performance O
. O
It O
scores O
better O
than O
other O
single O
models O
( O
RNADE Method
, O
STM Method
) O
, O
but O
not O
as O
well O
as O
the O
ensemble Method
of Method
RNADE Method
models Method
. O
5 O
Conclusion O
In O
this O
work O
we O
introduced O
the O
deep Method
Gaussian Method
Mixture Method
Model Method
: O
a O
novel O
density Method
estimation Method
technique Method
for O
modeling Task
real Task
valued Task
data Task
. O
we O
show O
that O
the O
Deep Method
GMM Method
is O
on O
par O
with O
the O
current O
state O
of O
the O
art O
in O
image Method
patch Method
modeling Method
, O
and O
surpasses O
other O
mixture Method
models Method
. O
We O
conclude O
that O
the O
Deep Method
GMM Method
is O
a O
viable O
and O
scalable O
alternative O
for O
unsupervised Task
learning Task
. O
The O
deep O
GMM Method
tackles O
unsupervised Task
learning Task
from O
a O
different O
angle O
than O
other O
recent O
deep Method
unsupervised Method
learning Method
techniques Method
[ O
17 O
, O
18 O
, O
19 O
] O
, O
which O
makes O
it O
very O
interesting O
for O
future O
research O
. O
In O
follow O
- O
up O
work O
, O
we O
would O
like O
to O
make O
Deep Method
GMMs Method
suitable O
for O
larger O
images O
and O
other O
highdimensional O
data O
. O
Locally Method
connected Method
filters Method
, O
such O
as O
convolutions Method
would O
be O
useful O
for O
this O
. O
We O
would O
also O
like O
to O
extend O
our O
method O
to O
modeling Task
discrete Task
data Task
. O
Deep Method
GMMs Method
are O
currently O
only O
designed O
for O
continuous O
real O
- O
valued O
data O
, O
but O
our O
approach O
of O
reparametrizing O
the O
model O
into O
layers O
of O
successive Method
transformations Method
can O
also O
be O
applied O
to O
other O
types O
of O
mixture O
distributions O
. O
We O
would O
also O
like O
to O
compare O
this O
extension O
to O
other O
discrete Method
density Method
estimators Method
such O
as O
Restricted Method
Boltzmann Method
Machines Method
, O
Deep Method
Belief Method
Networks Method
and O
the O
NADE Method
model Method
[ O
15 O
] O
. O
