document O
: O
Weighted Method
Transformer Method
Network Method
for O
Machine Task
Translation Task
State O
- O
of O
- O
the O
- O
art O
results O
on O
neural Task
machine Task
translation Task
often O
use O
attentional Method
sequence Method
- Method
to Method
- Method
sequence Method
models Method
with O
some O
form O
of O
convolution Method
or Method
recursion Method
. O
vaswani2017attention O
propose O
a O
new O
architecture O
that O
avoids O
recurrence O
and O
convolution O
completely O
. O
Instead O
, O
it O
uses O
only O
self Method
- Method
attention Method
and Method
feed Method
- Method
forward Method
layers Method
. O
While O
the O
proposed O
architecture O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
several O
machine Task
translation Task
tasks Task
, O
it O
requires O
a O
large O
number O
of O
parameters O
and O
training O
iterations O
to O
converge O
. O
We O
propose O
Weighted Method
Transformer Method
, O
a O
Transformer Method
with O
modified Method
attention Method
layers Method
, O
that O
not O
only O
outperforms O
the O
baseline O
network O
in O
BLEU Metric
score Metric
but O
also O
converges O
faster O
. O
Specifically O
, O
we O
replace O
the O
multi O
- O
head O
attention O
by O
multiple O
self O
- O
attention O
branches O
that O
the O
model O
learns O
to O
combine O
during O
the O
training O
process O
. O
Our O
model O
improves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
by O
BLEU Metric
points Metric
on O
the O
WMT Task
2014 Task
English Task
- Task
to Task
- Task
German Task
translation O
task O
and O
by O
on O
the O
English Material
- Material
to Material
- Material
French Material
translation O
task O
. O
section O
: O
Introduction O
Recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
, O
such O
as O
long Method
short Method
- Method
term Method
memory Method
networks Method
( O
LSTMs Method
) O
hochreiter1997long O
, O
form O
an O
important O
building O
block O
for O
many O
tasks O
that O
require O
modeling Task
of Task
sequential Task
data Task
. O
RNNs Method
have O
been O
successfully O
employed O
for O
several O
such O
tasks O
including O
language Method
modeling Method
melis2017state O
, O
merity2017regularizing O
, O
speech Task
recognition Task
xiong2017microsoft O
, O
graves2013speech O
, O
and O
machine Task
translation Task
wu2016google O
, O
bahdanau2014neural O
. O
RNNs Method
make O
output O
predictions O
at O
each O
time O
step O
by O
computing O
a O
hidden O
state O
vector O
based O
on O
the O
current O
input O
token O
and O
the O
previous O
states O
. O
This O
sequential Method
computation Method
underlies O
their O
ability O
to O
map O
arbitrary O
input O
- O
output O
sequence O
pairs O
. O
However O
, O
because O
of O
their O
auto O
- O
regressive O
property O
of O
requiring O
previous O
hidden O
states O
to O
be O
computed O
before O
the O
current O
time O
step O
, O
they O
can O
not O
benefit O
from O
parallelization O
. O
Variants O
of O
recurrent Method
networks Method
that O
use O
strided Method
convolutions Method
eschew O
the O
traditional O
time Method
- Method
step Method
based Method
computation Method
kaiser2016can O
, O
lei2017training O
, O
bradbury2016quasi O
, O
gehring2016convenc O
, O
gehring2017convs2s O
, O
kalchbrenner2016neural O
. O
However O
, O
in O
these O
models O
, O
the O
operations O
needed O
to O
learn O
dependencies O
between O
distant O
positions O
can O
be O
difficult O
to O
learn O
hochreiter2001gradient O
, O
hochreiter1998vanishing O
. O
Attention Method
mechanisms Method
, O
often O
used O
in O
conjunction O
with O
recurrent Method
models Method
, O
have O
become O
an O
integral O
part O
of O
complex O
sequential Task
tasks Task
because O
they O
facilitate O
learning O
of O
such O
dependencies O
luong2015effective O
, O
bahdanau2014neural O
, O
parikh2016decomposable O
, O
paulus2017deep O
, O
kim2017structured O
. O
In O
vaswani2017attention O
, O
the O
authors O
introduce O
the O
Transformer Method
network Method
, O
a O
novel O
architecture O
that O
avoids O
the O
recurrence O
equation O
and O
maps O
the O
input O
sequences O
into O
hidden O
states O
solely O
using O
attention O
. O
Specifically O
, O
the O
authors O
use O
positional Method
encodings Method
in O
conjunction O
with O
a O
multi Method
- Method
head Method
attention Method
mechanism Method
. O
This O
allows O
for O
increased O
parallel Task
computation Task
and O
reduces O
time O
to O
convergence Metric
. O
The O
authors O
report O
results O
for O
neural Task
machine Task
translation Task
that O
show O
the O
Transformer Method
networks Method
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
WMT Task
2014 Task
English Task
- Task
to Task
- Task
German Task
and O
English Material
- Material
to Material
- Material
French Material
tasks O
while O
being O
orders O
- O
of O
- O
magnitude O
faster O
than O
prior O
approaches O
. O
Transformer Method
networks Method
still O
require O
a O
large O
number O
of O
parameters O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
In O
the O
case O
of O
the O
newstest2013 O
English Task
- Task
to Task
- Task
German Task
translation O
task O
, O
the O
base O
model O
required O
M O
parameters O
, O
and O
the O
large Method
model Method
required O
M O
parameters O
. O
We O
propose O
a O
variant O
of O
the O
Transformer Method
network Method
which O
we O
call O
Weighted Method
Transformer Method
that O
uses O
self O
- O
attention O
branches O
in O
lieu O
of O
the O
multi O
- O
head O
attention O
. O
The O
branches O
replace O
the O
multiple O
heads O
in O
the O
attention Method
mechanism Method
of O
the O
original O
Transformer Method
network Method
, O
and O
the O
model O
learns O
to O
combine O
these O
branches O
during O
training O
. O
This O
branched Method
architecture Method
enables O
the O
network O
to O
achieve O
comparable O
performance O
at O
a O
significantly O
lower O
computational Metric
cost Metric
. O
Indeed O
, O
through O
this O
modification O
, O
we O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
by O
and O
BLEU Metric
scores Metric
on O
the O
WMT Task
2014 Task
English Task
- Task
to Task
- Task
German Task
and O
English Task
- Task
to Task
- Task
French Task
tasks Task
, O
respectively O
. O
Finally O
, O
we O
present O
evidence O
that O
suggests O
a O
regularizing O
effect O
of O
the O
proposed O
architecture O
. O
section O
: O
Related O
Work O
Most O
architectures O
for O
neural Task
machine Task
translation Task
( O
NMT Task
) Task
use O
an O
encoder Method
and O
a O
decoder Method
that O
rely O
on O
deep Method
recurrent Method
neural Method
networks Method
like O
the O
LSTM Method
luong2015effective O
, O
sutskever2014sequence O
, O
bahdanau2014neural O
, O
wu2016google O
, O
barone2017deep O
, O
cho2014learning O
. O
Several O
architectures O
have O
been O
proposed O
to O
reduce O
the O
computational Metric
load Metric
associated O
with O
recurrence Method
- Method
based Method
computation Method
gehring2016convenc O
, O
gehring2017convs2s O
, O
kaiser2016can O
, O
kalchbrenner2016neural O
. O
Self O
- O
attention O
, O
which O
relies O
on O
dot O
- O
products O
between O
elements O
of O
the O
input O
sequence O
to O
compute O
a O
weighted O
sum O
lin2017structured O
, O
bahdanau2014neural O
, O
parikh2016decomposable O
, O
kim2017structured O
, O
has O
also O
been O
a O
critical O
ingredient O
in O
modern O
NMT Method
architectures Method
. O
The O
Transformer Method
network Method
vaswani2017attention O
avoids O
the O
recurrence O
completely O
and O
uses O
only O
self O
- O
attention O
. O
We O
propose O
a O
modified O
Transformer Method
network Method
wherein O
the O
multi Method
- Method
head Method
attention Method
layer Method
is O
replaced O
by O
a O
branched Method
self Method
- Method
attention Method
layer Method
. O
The O
contributions O
of O
the O
various O
branches O
is O
learned O
as O
part O
of O
the O
training Method
procedure Method
. O
The O
idea O
of O
multi Method
- Method
branch Method
networks Method
has O
been O
explored O
in O
several O
domains O
ahmed2017branchconnect O
, O
gastaldi2017shake O
, O
shazeer2017outrageously O
, O
xie2016aggregated O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
model O
using O
a O
branched O
structure O
in O
the O
Transformer Method
network Method
. O
In O
shazeer2017outrageously O
, O
the O
authors O
use O
a O
large O
network O
, O
with O
billions O
of O
weights O
, O
in O
conjunction O
with O
a O
sparse Method
expert Method
model Method
to O
achieve O
competitive O
performance O
. O
ahmed2017branchconnect O
analyze O
learned O
branching O
, O
through O
gates O
, O
in O
the O
context O
of O
computer Task
vision Task
while O
in O
gastaldi2017shake O
, O
the O
author O
analyzes O
a O
two O
- O
branch Method
model Method
with O
randomly O
sampled O
weights O
in O
the O
context O
of O
image Task
classification Task
. O
subsection O
: O
Transformer Method
Network Method
The O
original O
Transformer Method
network Method
uses O
an O
encoder Method
- Method
decoder Method
architecture Method
with O
each O
layer O
consisting O
of O
a O
novel O
attention Method
mechanism Method
, O
which O
the O
authors O
call O
multi O
- O
head O
attention O
, O
followed O
by O
a O
feed Method
- Method
forward Method
network Method
. O
We O
describe O
both O
these O
components O
below O
. O
From O
the O
source O
tokens O
, O
learned O
embeddings O
of O
dimension O
are O
generated O
which O
are O
then O
modified O
by O
an O
additive Method
positional Method
encoding Method
. O
The O
positional Method
encoding Method
is O
necessary O
since O
the O
network O
does O
not O
otherwise O
possess O
any O
means O
of O
leveraging O
the O
order O
of O
the O
sequence O
since O
it O
contains O
no O
recurrence O
or O
convolution O
. O
The O
authors O
use O
additive Method
encoding Method
which O
is O
defined O
as O
: O
where O
is O
the O
position O
of O
a O
word O
in O
the O
sentence O
and O
is O
the O
dimension O
of O
the O
vector O
. O
The O
authors O
also O
experiment O
with O
learned O
embeddings O
gehring2016convenc O
, O
gehring2017convs2s O
but O
found O
no O
benefit O
in O
doing O
so O
. O
The O
encoded O
word O
embeddings O
are O
then O
used O
as O
input O
to O
the O
encoder Method
which O
consists O
of O
layers O
each O
containing O
two O
sub O
- O
layers O
: O
( O
a O
) O
a O
multi Method
- Method
head Method
attention Method
mechanism Method
, O
and O
( O
b O
) O
a O
feed Method
- Method
forward Method
network Method
. O
A O
multi Method
- Method
head Method
attention Method
mechanism Method
builds O
upon O
scaled Method
dot Method
- Method
product Method
attention Method
, O
which O
operates O
on O
a O
query O
, O
key O
and O
a O
value O
: O
where O
is O
the O
dimension O
of O
the O
key O
. O
In O
the O
first O
layer O
, O
the O
inputs O
are O
concatenated O
such O
that O
each O
of O
is O
equal O
to O
the O
word O
vector O
matrix O
. O
This O
is O
identical O
to O
dot Method
- Method
product Method
attention Method
except O
for O
the O
scaling O
factor O
, O
which O
improves O
numerical Metric
stability Metric
. O
Multi Method
- Method
head Method
attention Method
mechanisms Method
obtain O
different O
representations O
of O
( O
, O
, O
) O
, O
compute O
scaled Method
dot Method
- Method
product Method
attention Method
for O
each O
representation O
, O
concatenate O
the O
results O
, O
and O
project O
the O
concatenation O
with O
a O
feed Method
- Method
forward Method
layer Method
. O
This O
can O
be O
expressed O
in O
the O
same O
notation O
as O
Equation O
( O
[ O
reference O
] O
) O
: O
where O
the O
and O
are O
parameter O
projection O
matrices O
that O
are O
learned O
. O
Note O
that O
, O
, O
and O
where O
denotes O
the O
number O
of O
heads O
in O
the O
multi O
- O
head O
attention O
. O
vaswani2017attention O
proportionally O
reduce O
so O
that O
the O
computational Metric
load Metric
of O
the O
multi Task
- Task
head Task
attention Task
is O
the O
same O
as O
simple O
self O
- O
attention O
. O
The O
second O
component O
of O
each O
layer O
of O
the O
Transformer Method
network Method
is O
a O
feed Method
- Method
forward Method
network Method
. O
The O
authors O
propose O
using O
a O
two Method
- Method
layered Method
network Method
with O
a O
ReLU Method
activation Method
. O
Given O
trainable O
weights O
, O
the O
sub Method
- Method
layer Method
is O
defined O
as O
: O
The O
dimension O
of O
the O
inner O
layer O
is O
which O
is O
set O
to O
in O
their O
experiments O
. O
For O
the O
sake O
of O
brevity O
, O
we O
refer O
the O
reader O
to O
for O
additional O
details O
regarding O
the O
architecture O
. O
For O
regularization Task
and O
ease O
of O
training Task
, O
the O
network O
uses O
layer Method
normalization Method
ba2016layer O
after O
each O
sub O
- O
layer O
and O
a O
residual O
connection O
around O
each O
full O
layer O
he2016deep O
. O
Analogously O
, O
each O
layer O
of O
the O
decoder O
contains O
the O
two O
sub O
- O
layers O
mentioned O
above O
as O
well O
as O
an O
additional O
multi Method
- Method
head Method
attention Method
sub Method
- Method
layer Method
that O
receives O
as O
inputs O
from O
the O
output O
of O
the O
corresponding O
encoding Method
layer Method
. O
In O
the O
case O
of O
the O
decoder Task
multi Task
- Task
head Task
attention Task
sub Task
- Task
layers Task
, O
the O
scaled O
dot O
- O
product O
attention O
is O
masked O
to O
prevent O
future O
positions O
from O
being O
attended O
to O
, O
or O
in O
other O
words O
, O
to O
prevent O
illegal O
leftward O
- O
ward O
information O
flow O
. O
One O
natural O
question O
regarding O
the O
Transformer Method
network Method
is O
why O
self Task
- Task
attention Task
should O
be O
preferred O
to O
recurrent Method
or Method
convolutional Method
models Method
. O
vaswani2017attention O
state O
three O
reasons O
for O
the O
preference O
: O
( O
a O
) O
computational Metric
complexity Metric
of O
each O
layer O
, O
( O
b O
) O
concurrency O
, O
and O
( O
c O
) O
path O
length O
between O
long O
- O
range O
dependencies O
. O
Assuming O
a O
sequence O
length O
of O
and O
vector O
dimension O
, O
the O
complexity Metric
of O
each O
layer O
is O
for O
self O
- O
attention O
layers O
while O
it O
is O
for O
recurrent Method
layers Method
. O
Given O
that O
typically O
, O
the O
complexity Metric
of O
self Method
- Method
attention Method
layers Method
is O
lower O
than O
that O
of O
recurrent Method
layers Method
. O
Further O
, O
the O
number O
of O
sequential O
computations O
is O
for O
self O
- O
attention O
layers O
and O
for O
recurrent Method
layers Method
. O
This O
helps O
improved O
utilization O
of O
parallel Method
computing Method
architectures Method
. O
Finally O
, O
the O
maximum O
path O
length O
between O
dependencies O
is O
for O
the O
self Method
- Method
attention Method
layer Method
while O
it O
is O
for O
the O
recurrent Method
layer Method
. O
This O
difference O
is O
instrumental O
in O
impeding O
recurrent Method
models Method
’ O
ability O
to O
learn O
long O
- O
range O
dependencies O
. O
section O
: O
Proposed O
Network Method
Architecture Method
We O
now O
describe O
the O
proposed O
architecture O
, O
the O
Weighted Method
Transformer Method
, O
which O
is O
more O
efficient O
to O
train O
and O
makes O
better O
use O
of O
representational O
power O
. O
In O
Equations O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
, O
we O
described O
the O
attention Method
layer Method
proposed O
in O
vaswani2017attention Method
comprising O
the O
multi Method
- Method
head Method
attention Method
sub Method
- Method
layer Method
and O
a O
FFN Method
sub Method
- Method
layer Method
. O
For O
the O
Weighted Method
Transformer Method
, O
we O
propose O
a O
branched Method
attention Method
that O
modifies O
the O
entire O
attention O
layer O
in O
the O
Transformer Method
network Method
( O
including O
both O
the O
multi Method
- Method
head Method
attention Method
and O
the O
feed Method
- Method
forward Method
network Method
) O
. O
The O
proposed O
attention Method
layer Method
can O
be O
described O
as O
: O
where O
denotes O
the O
total O
number O
of O
branches O
, O
are O
learned O
parameters O
and O
. O
The O
FFN Method
function Method
above O
is O
identical O
to O
Equation O
( O
[ O
reference O
] O
) O
. O
Further O
, O
we O
require O
that O
and O
so O
that O
Equation O
( O
[ O
reference O
] O
) O
is O
a O
weighted O
sum O
of O
the O
individual O
branch O
attention O
values O
. O
In O
the O
equations O
above O
, O
can O
be O
interpreted O
as O
a O
learned O
concatenation O
weight O
and O
as O
the O
learned O
addition O
weight O
. O
Indeed O
, O
scales O
the O
contribution O
of O
the O
various O
branches O
before O
is O
used O
to O
sum O
them O
in O
a O
weighted O
fashion O
. O
We O
ensure O
that O
all O
bounds O
are O
respected O
during O
each O
training O
step O
by O
projection O
. O
While O
it O
is O
possible O
that O
and O
could O
be O
merged O
into O
one O
variable O
and O
trained O
, O
we O
found O
better O
training O
outcomes O
by O
separating O
them O
. O
It O
also O
improves O
the O
interpretability O
of O
the O
models O
gives O
that O
can O
be O
thought O
of O
as O
probability O
masses O
on O
the O
various O
branches O
. O
It O
can O
be O
shown O
that O
if O
and O
for O
all O
, O
we O
recover O
the O
equation O
for O
the O
multi Task
- Task
head Task
attention Task
( O
[ O
reference O
] O
) O
. O
However O
, O
given O
the O
and O
bounds O
, O
these O
values O
are O
not O
permissible O
in O
the O
Weighted Method
Transformer Method
. O
One O
interpretation O
of O
our O
proposed O
architecture O
is O
that O
it O
replaces O
the O
multi Method
- Method
head Method
attention Method
by O
a O
multi Method
- Method
branch Method
attention Method
. O
Rather O
than O
concatenating O
the O
contributions O
of O
the O
different O
heads O
, O
they O
are O
instead O
treated O
as O
branches O
that O
a O
multi Method
- Method
branch Method
network Method
learns O
to O
combine O
. O
This O
mechanism O
adds O
trainable O
weights O
. O
This O
is O
an O
insignificant O
increase O
compared O
to O
the O
total O
number O
of O
weights O
. O
Indeed O
, O
in O
our O
experiments O
, O
the O
proposed O
mechanism O
added O
weights O
to O
a O
model O
containing O
weights O
already O
. O
Without O
these O
additional O
trainable O
weights O
, O
the O
proposed O
mechanism O
is O
identical O
to O
the O
multi Method
- Method
head Method
attention Method
mechanism Method
in O
the O
Transformer O
. O
The O
proposed O
attention Method
mechanism Method
is O
used O
in O
both O
the O
encoder O
and O
decoder O
layers O
and O
is O
masked O
in O
the O
decoder O
layers O
as O
in O
the O
Transformer Method
network Method
. O
Similarly O
, O
the O
positional Method
encoding Method
, O
layer Method
normalization Method
, O
and O
residual O
connections O
in O
the O
encoder O
- O
decoder O
layers O
are O
retained O
. O
We O
eliminate O
these O
details O
from O
Figure O
[ O
reference O
] O
for O
clarity O
. O
Instead O
of O
using O
learned O
weights O
, O
it O
is O
possible O
to O
also O
use O
a O
mixture Method
- Method
of Method
- Method
experts Method
normalization O
via O
a O
softmax Method
layer Method
shazeer2017outrageously O
. O
However O
, O
we O
found O
this O
to O
perform O
worse O
than O
our O
proposal O
. O
Unlike O
the O
Transformer Method
, O
which O
weighs O
all O
heads O
equally O
, O
the O
proposed O
mechanism O
allows O
for O
ascribing O
importance O
to O
different O
heads O
. O
This O
in O
turn O
prioritizes O
their O
gradients O
and O
eases O
the O
optimization Task
process Task
. O
Further O
, O
as O
is O
known O
from O
multi Method
- Method
branch Method
networks Method
in O
computer Task
vision Task
gastaldi2017shake O
, O
such O
mechanisms O
tend O
to O
cause O
the O
branches O
to O
learn O
decorrelated O
input O
- O
output O
mappings O
. O
This O
reduces O
co Method
- Method
adaptation Method
and O
improves O
generalization Task
. O
This O
observation O
also O
forms O
the O
basis O
for O
mixture Method
- Method
of Method
- Method
experts Method
models O
shazeer2017outrageously O
. O
section O
: O
Experiments O
subsection O
: O
Training O
Details O
The O
weights O
and O
are O
initialized O
randomly O
, O
as O
with O
the O
rest O
of O
the O
Transformer O
weights O
. O
In O
addition O
to O
the O
layer Method
normalization Method
and O
residual O
connections O
, O
we O
use O
label Method
smoothing Method
with O
, O
attention Method
dropout Method
, O
and O
residual Method
dropout Method
with O
probability O
. O
Attention Method
dropout Method
randomly O
drops O
out O
elements O
srivastava2014dropout O
from O
the O
softmax Method
in O
( O
[ O
reference O
] O
) O
. O
As O
in O
vaswani2017attention O
, O
we O
used O
the O
Adam Method
optimizer Method
kingma2014adam O
with O
and O
. O
We O
also O
use O
the O
learning Method
rate Method
warm Method
- Method
up Method
strategy Method
for O
Adam Method
wherein O
the O
learning Metric
rate Metric
takes O
on O
the O
form O
: O
for O
the O
all O
parameters O
except O
and O
for O
. O
This O
corresponds O
to O
the O
warm Method
- Method
up Method
strategy Method
used O
for O
the O
original O
Transformer Method
network Method
except O
that O
we O
use O
a O
larger O
peak O
learning Metric
rate Metric
for O
to O
compensate O
for O
their O
bounds O
. O
Further O
, O
we O
found O
that O
freezing O
the O
weights O
in O
the O
last O
iterations O
aids O
convergence O
. O
During O
this O
time O
, O
we O
continue O
training O
the O
rest O
of O
the O
network O
. O
We O
hypothesize O
that O
this O
freezing Method
process Method
helps O
stabilize O
the O
rest O
of O
the O
network O
weights O
given O
the O
weighting Method
scheme Method
. O
We O
note O
that O
the O
number O
of O
iterations O
required O
for O
convergence O
to O
the O
final O
score O
is O
substantially O
reduced O
for O
the O
Weighted Method
Transformer Method
. O
We O
found O
that O
Weighted Method
Transformer Method
converges O
– O
faster O
as O
measured O
by O
the O
total O
number O
of O
iterations O
to O
achieve O
optimal O
performance O
. O
We O
train O
the O
baseline Method
model Method
for O
K O
steps O
for O
the O
smaller O
variant O
and O
K O
for O
the O
larger O
. O
We O
train O
the O
Weighted Method
Transformer Method
for O
the O
respective O
variants O
for O
K O
and O
K O
iterations O
. O
We O
found O
that O
the O
objective O
did O
not O
significantly O
improve O
by O
running O
it O
for O
longer O
. O
Further O
, O
we O
do O
not O
use O
any O
averaging Method
strategies Method
employed O
in O
vaswani2017attention O
and O
simply O
return O
the O
final O
model O
for O
testing O
purposes O
. O
In O
order O
to O
reduce O
the O
computational Metric
load Metric
associated O
with O
padding O
, O
sentences O
were O
batched O
such O
that O
they O
were O
approximately O
of O
the O
same O
length O
. O
All O
sentences O
were O
encoded O
using O
byte Method
- Method
pair Method
encoding Method
sennrich2015neural O
and O
shared O
a O
common O
vocabulary O
. O
Weights O
for O
word O
embeddings O
were O
tied O
to O
corresponding O
entries O
in O
the O
final O
softmax Method
layer Method
inan2016tying O
, O
press2016using O
. O
We O
trained O
all O
our O
networks O
on O
NVIDIA Method
K80 Method
GPUs Method
with O
a O
batch O
containing O
roughly O
25 O
, O
000 O
source O
and O
target O
tokens O
. O
subsection O
: O
Results O
on O
Benchmark O
Data O
Sets O
We O
benchmark O
our O
proposed O
architecture O
on O
the O
WMT Task
2014 Task
English Task
- Task
to Task
- Task
German Task
and O
English Task
- Task
to Task
- Task
French Task
tasks Task
. O
The O
WMT Material
2014 Material
English Material
- Material
to Material
- Material
German Material
data Material
set Material
contains O
M O
sentence O
pairs O
. O
The O
English Material
- Material
to Material
- Material
French Material
contains O
M O
sentence O
pairs O
. O
Results O
of O
our O
experiments O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
The O
Weighted Method
Transformer Method
achieves O
a O
BLEU Metric
score Metric
improvement O
over O
the O
state O
- O
of O
- O
the O
- O
art O
on O
the O
English Task
- Task
to Task
- Task
German Task
task Task
for O
the O
smaller O
network O
and O
BLEU Metric
improvement Metric
for O
the O
larger O
network O
. O
In O
the O
case O
of O
the O
larger O
English Material
- Material
to Material
- Material
French Material
task O
, O
we O
note O
a O
BLEU Metric
improvement Metric
for O
the O
smaller O
model O
and O
a O
improvement O
for O
the O
larger O
model O
. O
Also O
, O
note O
that O
the O
performance O
of O
the O
smaller O
model O
for O
Weighted Task
Transformer Task
is O
close O
to O
that O
of O
the O
larger O
baseline O
model O
, O
especially O
for O
the O
English Task
- Task
to Task
- Task
German Task
task Task
. O
This O
suggests O
that O
the O
Weighted Method
Transformer Method
better O
utilizes O
available O
model O
capacity O
since O
it O
needs O
only O
of O
the O
parameters O
as O
the O
baseline Method
transformer Method
for O
matching O
its O
performance O
. O
Our O
relative O
improvements O
do O
not O
hinge O
on O
using O
the O
BLEU Metric
scores Metric
for O
comparison O
; O
experiments O
with O
the O
GLEU Metric
score Metric
proposed O
in O
wu2016google O
also O
yielded O
similar O
improvements O
. O
Finally O
, O
we O
comment O
on O
the O
regularizing O
effect O
of O
the O
Weighted Method
Transformer Method
. O
Given O
the O
improved O
results O
, O
a O
natural O
question O
is O
whether O
the O
results O
stem O
from O
improved O
regularization Method
of Method
the Method
model Method
. O
To O
investigate O
this O
, O
we O
report O
the O
testing Metric
loss Metric
of O
the O
Weighted Method
Transformer Method
and O
the O
baseline Method
Transformer Method
against O
the O
training Metric
loss Metric
in O
Figure O
[ O
reference O
] O
. O
Models O
which O
have O
a O
regularizing O
effect O
tend O
to O
have O
lower O
testing O
losses O
for O
the O
same O
training O
loss O
. O
We O
see O
this O
effect O
in O
our O
experiments O
suggesting O
that O
the O
proposed O
architecture O
may O
have O
better O
regularizing O
properties O
. O
This O
is O
not O
unexpected O
given O
similar O
outcomes O
for O
other O
branching Method
- Method
based Method
strategies Method
such O
as O
Shake Method
- Method
Shake Method
and O
mixture Method
- Method
of Method
- Method
experts Method
. O
subsection O
: O
Sensitivity O
Analysis O
In O
Table O
[ O
reference O
] O
, O
we O
report O
sensitivity O
results O
on O
the O
newstest2013 O
English Task
- Task
to Task
- Task
German Task
task Task
. O
Specifically O
, O
we O
vary O
the O
number O
of O
layers O
in O
the O
encoder Method
/ Method
decoder Method
and O
compare O
the O
performance O
of O
the O
Weighted Method
Transformer Method
and O
the O
Transformer Method
baseline Method
. O
The O
results O
clearly O
demonstrate O
the O
benefit O
of O
the O
branched Method
attention Method
; O
for O
every O
experiment O
, O
the O
Weighted Method
Transformer Method
outperforms O
the O
baseline Method
transformer Method
, O
in O
some O
cases O
by O
up O
to O
BLEU Metric
points Metric
. O
As O
in O
the O
case O
of O
the O
baseline Method
Transformer Method
, O
increasing O
the O
number O
of O
layers O
does O
not O
necessarily O
improve O
performance O
; O
a O
modest O
improvement O
is O
seen O
when O
the O
number O
of O
layers O
is O
increased O
from O
to O
and O
to O
but O
the O
performance O
degrades O
when O
is O
increased O
to O
. O
Increasing O
the O
number O
of O
heads O
from O
to O
in O
configuration O
( O
A O
) O
yielded O
an O
even O
better O
BLEU Metric
score Metric
. O
However O
, O
preliminary O
experiments O
with O
and O
, O
like O
in O
the O
case O
with O
, O
degrade O
the O
performance O
of O
the O
model O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
present O
the O
behavior O
of O
the O
weights O
for O
the O
second O
encoder Method
layer Method
of O
the O
configuration O
( O
C O
) O
for O
the O
English Task
- Task
to Task
- Task
German Task
newstest2013 O
task O
. O
The O
figure O
shows O
that O
, O
in O
terms O
of O
relative O
weights O
, O
the O
network O
does O
prioritize O
some O
branches O
more O
than O
others O
; O
circumstantially O
by O
as O
much O
as O
. O
Further O
, O
the O
relative O
ordering O
of O
the O
branches O
changes O
over O
time O
suggesting O
that O
the O
network O
is O
not O
purely O
exploitative O
. O
A O
purely O
exploitative Method
network Method
, O
which O
would O
learn O
to O
exploit O
a O
subset O
of O
the O
branches O
at O
the O
expense O
of O
the O
rest O
, O
would O
not O
be O
preferred O
since O
it O
would O
effectively O
reduce O
the O
number O
of O
available O
parameters O
and O
limit O
the O
representational O
power O
. O
Similar O
results O
are O
seen O
for O
other O
layers O
, O
including O
the O
decoder O
layers O
; O
we O
omit O
them O
for O
brevity O
. O
subsection O
: O
Randomization O
Baseline O
The O
proposed O
modification O
can O
also O
be O
interpreted O
as O
a O
form O
of O
Shake Method
- Method
Shake Method
regularization Method
proposed O
in O
. O
In O
this O
regularization Method
strategy Method
, O
random O
weights O
are O
sampled O
during O
forward O
and O
backward O
passes O
for O
weighing O
the O
various O
branches O
in O
a O
multi Method
- Method
branch Method
network Method
. O
During O
test O
time O
, O
they O
are O
weighed O
equally O
. O
In O
our O
strategy O
, O
the O
weights O
are O
learned O
instead O
of O
being O
sampled O
randomly O
. O
Consequently O
, O
no O
changes O
to O
the O
model O
are O
required O
during O
test O
time O
. O
In O
order O
to O
better O
understand O
whether O
the O
network O
benefits O
from O
the O
learned O
weights O
or O
if O
, O
at O
test O
time O
, O
random O
or O
uniform O
weights O
suffice O
, O
we O
propose O
the O
following O
experiment O
: O
the O
weights O
for O
the O
Weighted Method
Transformer Method
, O
including O
are O
trained O
as O
before O
, O
but O
, O
during O
test O
time O
, O
we O
replace O
them O
with O
( O
a O
) O
randomly O
sampled O
weights O
, O
and O
( O
b O
) O
where O
is O
the O
number O
of O
incoming O
branches O
. O
In O
Table O
[ O
reference O
] O
, O
we O
report O
experimental O
results O
on O
the O
configuration O
( O
C O
) O
of O
the O
Weighted Method
Transformer Method
on O
the O
English Task
- Task
to Task
- Task
German Task
newstest2013 O
data O
set O
( O
see O
Table O
[ O
reference O
] O
for O
details O
regarding O
the O
configuration O
) O
. O
It O
is O
evident O
that O
random O
or O
uniform O
weights O
can O
not O
replace O
the O
learned O
weights O
during O
test O
time O
. O
Preliminary O
experiments O
suggest O
that O
a O
Shake Method
- Method
Shake Method
- Method
like Method
strategy Method
where O
the O
weights O
are O
sampled O
randomly O
during O
training O
also O
leads O
to O
inferior O
performance O
. O
subsection O
: O
Gating Task
In O
order O
to O
analyze O
whether O
a O
hard O
( O
discrete O
) O
choice O
through O
gating O
will O
outperform O
our O
normalization Method
strategy Method
, O
we O
experimented O
with O
using O
gates Method
instead O
of O
the O
proposed O
concatenation Method
- Method
addition Method
strategy Method
. O
Specifically O
, O
we O
replaced O
the O
summation O
in O
Equation O
( O
[ O
reference O
] O
) O
by O
a O
gating Method
structure Method
that O
sums O
up O
the O
contributions O
of O
the O
top O
branches O
with O
the O
highest O
probabilities O
. O
This O
is O
similar O
to O
the O
sparsely Method
- Method
gated Method
mixture Method
of Method
experts Method
model Method
in O
shazeer2017outrageously O
. O
Despite O
significant O
hyper O
- O
parameter O
tuning O
of O
and O
, O
we O
found O
that O
this O
strategy O
performs O
worse O
than O
our O
proposed O
mechanism O
by O
a O
large O
margin O
. O
We O
hypothesize O
that O
this O
is O
due O
to O
the O
fact O
that O
the O
number O
of O
branches O
is O
low O
, O
typically O
less O
than O
16 O
. O
Hence O
, O
sparsely Method
- Method
gated Method
models Method
lose O
representational O
power O
due O
to O
reduced O
capacity O
in O
the O
model O
. O
We O
plan O
to O
investigate O
the O
setup O
with O
a O
large O
number O
of O
branches O
and O
sparse O
gates O
in O
future O
work O
. O
section O
: O
Conclusions O
We O
present O
the O
Weighted Method
Transformer Method
that O
trains O
faster O
and O
achieves O
better O
performance O
than O
the O
original O
Transformer Method
network Method
. O
The O
proposed O
architecture O
replaces O
the O
multi O
- O
head O
attention O
in O
the O
Transformer Method
network Method
by O
a O
multiple O
self Method
- Method
attention Method
branches Method
whose O
contributions O
are O
learned O
as O
a O
part O
of O
the O
training O
process O
. O
We O
report O
numerical O
results O
on O
the O
WMT Task
2014 Task
English Task
- Task
to Task
- Task
German Task
and O
English Task
- Task
to Task
- Task
French Task
tasks Task
and O
show O
that O
the O
Weighted Method
Transformer Method
improves O
the O
state O
- O
of O
- O
the O
- O
art O
BLEU Metric
scores Metric
by O
and O
points O
respectively O
. O
Further O
, O
our O
proposed O
architecture O
trains O
faster O
than O
the O
baseline Method
Transformer Method
. O
Finally O
, O
we O
present O
evidence O
suggesting O
the O
regularizing O
effect O
of O
the O
proposal O
and O
emphasize O
that O
the O
relative O
improvement O
in O
BLEU Metric
score Metric
is O
observed O
across O
various O
hyper O
- O
parameter O
settings O
for O
both O
small O
and O
large O
models O
. O
bibliography O
: O
References O
