document O
: O
Striving O
for O
Simplicity O
: O
The O
All Method
Convolutional Method
Net Method
Most O
modern O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
used O
for O
object Task
recognition Task
are O
built O
using O
the O
same O
principles O
: O
Alternating Method
convolution Method
and O
max Method
- Method
pooling Method
layers Method
followed O
by O
a O
small O
number O
of O
fully Method
connected Method
layers Method
. O
We O
re O
- O
evaluate O
the O
state O
of O
the O
art O
for O
object Task
recognition Task
from O
small O
images O
with O
convolutional Method
networks Method
, O
questioning O
the O
necessity O
of O
different O
components O
in O
the O
pipeline O
. O
We O
find O
that O
max Method
- Method
pooling Method
can O
simply O
be O
replaced O
by O
a O
convolutional Method
layer Method
with O
increased O
stride O
without O
loss O
in O
accuracy Metric
on O
several O
image Task
recognition Task
benchmarks Task
. O
Following O
this O
finding O
– O
and O
building O
on O
other O
recent O
work O
for O
finding O
simple O
network O
structures O
– O
we O
propose O
a O
new O
architecture O
that O
consists O
solely O
of O
convolutional Method
layers Method
and O
yields O
competitive O
or O
state O
of O
the O
art O
performance O
on O
several O
object O
recognition O
datasets O
( O
CIFAR Material
- Material
10 Material
, O
CIFAR Material
- Material
100 Material
, O
ImageNet O
) O
. O
To O
analyze O
the O
network O
we O
introduce O
a O
new O
variant O
of O
the O
“ O
deconvolution Method
approach Method
” O
for O
visualizing Task
features Task
learned O
by O
CNNs Method
, O
which O
can O
be O
applied O
to O
a O
broader O
range O
of O
network O
structures O
than O
existing O
approaches O
. O
⌈⌉ O
⌊⌋ O
section O
: O
Introduction O
and O
Related O
Work O
The O
vast O
majority O
of O
modern O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
used O
for O
object Task
recognition Task
are O
built O
using O
the O
same O
principles O
: O
They O
use O
alternating Method
convolution Method
and O
max Method
- Method
pooling Method
layers Method
followed O
by O
a O
small O
number O
of O
fully Method
connected Method
layers Method
( O
e.g. O
Jarrett_2009 O
, O
Krizhevsky_NIPS2012 O
, O
Ciresan_2011 O
) O
. O
Within O
each O
of O
these O
layers O
piecewise Method
- Method
linear Method
activation Method
functions Method
are O
used O
. O
The O
networks O
are O
typically O
parameterized O
to O
be O
large O
and O
regularized O
during O
training Task
using O
dropout Method
. O
A O
considerable O
amount O
of O
research O
has O
over O
the O
last O
years O
focused O
on O
improving O
the O
performance O
of O
this O
basic O
pipeline O
. O
Among O
these O
two O
major O
directions O
can O
be O
identified O
. O
First O
, O
a O
plethora O
of O
extensions O
were O
recently O
proposed O
to O
enhance O
networks O
which O
follow O
this O
basic O
scheme O
. O
Among O
these O
the O
most O
notable O
directions O
are O
work O
on O
using O
more O
complex O
activation O
functions O
Goodfellow2013 O
, O
Lin_2014 O
, O
SrivastavaSchmid_2013 O
techniques O
for O
improving O
class Task
inference Task
Stollenga_2014 O
, O
Nitish2013 O
as O
well O
as O
procedures O
for O
improved O
regularization Task
ZeilerStochastic2013 O
, O
SprRied2014a O
, O
WanLi2013 O
and O
layer Method
- Method
wise Method
pre Method
- Method
training Method
using O
label O
information O
Lee_2014 O
. O
Second O
, O
the O
success O
of O
CNNs Method
for O
large Task
scale Task
object Task
recognition Task
in O
the O
ImageNet O
challenge O
Krizhevsky_NIPS2012 O
has O
stimulated O
research O
towards O
experimenting O
with O
the O
different O
architectural O
choices O
in O
CNNs Method
. O
Most O
notably O
the O
top O
entries O
in O
the O
2014 O
ImageNet O
challenge O
deviated O
from O
the O
standard O
design O
principles O
by O
either O
introducing O
multiple O
convolutions Method
in O
between O
pooling Method
layers Method
VGG_2014 Method
or O
by O
building O
heterogeneous Method
modules Method
performing O
convolutions Method
and O
pooling Method
at O
multiple O
scales O
in O
each O
layer O
GoingDeep_2014 O
. O
Since O
all O
of O
these O
extensions O
and O
different O
architectures O
come O
with O
their O
own O
parameters O
and O
training O
procedures O
the O
question O
arises O
which O
components O
of O
CNNs Method
are O
actually O
necessary O
for O
achieving O
state O
of O
the O
art O
performance O
on O
current O
object O
recognition O
datasets O
. O
We O
take O
a O
first O
step O
towards O
answering O
this O
question O
by O
studying O
the O
most O
simple O
architecture O
we O
could O
conceive O
: O
a O
homogeneous Method
network Method
solely O
consisting O
of O
convolutional Method
layers Method
, O
with O
occasional O
dimensionality Method
reduction Method
by O
using O
a O
stride O
of O
2 O
. O
Surprisingly O
, O
we O
find O
that O
this O
basic O
architecture O
– O
trained O
using O
vanilla Method
stochastic Method
gradient Method
descent Method
with O
momentum Method
– O
reaches O
state O
of O
the O
art O
performance O
without O
the O
need O
for O
complicated O
activation O
functions O
, O
any O
response Method
normalization Method
or O
max Method
- Method
pooling Method
. O
We O
empirically O
study O
the O
effect O
of O
transitioning O
from O
a O
more O
standard O
architecture O
to O
our O
simplified Method
CNN Method
by O
performing O
an O
ablation O
study O
on O
CIFAR Material
- Material
10 Material
and O
compare O
our O
model O
to O
the O
state O
of O
the O
art O
on O
CIFAR Material
- Material
10 Material
, O
CIFAR Material
- Material
100 Material
and O
the O
ILSVRC O
- O
2012 O
ImageNet O
dataset O
. O
Our O
results O
both O
confirm O
the O
effectiveness O
of O
using O
small Method
convolutional Method
layers Method
as O
recently O
proposed O
by O
VGG_2014 Method
and O
give O
rise O
to O
interesting O
new O
questions O
about O
the O
necessity O
of O
pooling Task
in O
CNNs Method
. O
Since O
dimensionality Method
reduction Method
is O
performed O
via O
strided Method
convolution Method
rather O
than O
max Method
- Method
pooling Method
in O
our O
architecture O
it O
also O
naturally O
lends O
itself O
to O
studying O
questions O
about O
the O
invertibility Method
of Method
neural Method
networks Method
Estrach_2014 O
. O
For O
a O
first O
step O
in O
that O
direction O
we O
study O
properties O
of O
our O
network O
using O
a O
deconvolutional Method
approach Method
similar O
to O
Zeiler_ECCV2014 Method
. O
section O
: O
Model O
description O
- O
the O
all Method
convolutional Method
network Method
The O
models O
we O
use O
in O
our O
experiments O
differ O
from O
standard O
CNNs Method
in O
several O
key O
aspects O
. O
First O
– O
and O
most O
interestingly O
– O
we O
replace O
the O
pooling Method
layers Method
, O
which O
are O
present O
in O
practically O
all O
modern O
CNNs Method
used O
for O
object Task
recognition Task
, O
with O
standard O
convolutional Method
layers Method
with O
stride O
two O
. O
To O
understand O
why O
this O
procedure O
can O
work O
it O
helps O
to O
recall O
the O
standard O
formulation O
for O
defining O
convolution Task
and Task
pooling Task
operations Task
in O
CNNs Method
. O
Let O
denote O
a O
feature O
map O
produced O
by O
some O
layer O
of O
a O
CNN Method
. O
It O
can O
be O
described O
as O
a O
3 O
- O
dimensional O
array O
of O
size O
where O
and O
are O
the O
width O
and O
height O
and O
is O
the O
number O
of O
channels O
( O
in O
case O
is O
the O
output O
of O
a O
convolutional Method
layer Method
, O
is O
the O
number O
of O
filters O
in O
this O
layer O
) O
. O
Then O
p Method
- Method
norm Method
subsampling Method
( O
or O
pooling Method
) O
with O
pooling O
size O
( O
or O
half O
- O
length O
) O
and O
stride O
applied O
to O
the O
feature O
map O
is O
a O
3 O
- O
dimensional O
array O
with O
the O
following O
entries O
: O
where O
is O
the O
function O
mapping O
from O
positions O
in O
to O
positions O
in O
respecting O
the O
stride O
, O
is O
the O
order O
of O
the O
p Method
- Method
norm Method
( O
for O
, O
it O
becomes O
the O
commonly O
used O
max Method
pooling Method
) O
. O
If O
, O
pooling O
regions O
do O
not O
overlap O
; O
however O
, O
current O
CNN Method
architectures Method
typically O
include O
overlapping Method
pooling Method
with O
and O
. O
Let O
us O
now O
compare O
the O
pooling Method
operation Method
defined O
by O
Eq O
. O
[ O
reference O
] O
to O
the O
standard O
definition O
of O
a O
convolutional Method
layer Method
applied O
to O
feature O
map O
given O
as O
: O
where O
are O
the O
convolutional O
weights O
( O
or O
the O
kernel O
weights O
, O
or O
filters O
) O
, O
is O
the O
activation O
function O
, O
typically O
a O
rectified Method
linear Method
activation Method
ReLU Method
, O
and O
is O
the O
number O
of O
output O
feature O
( O
or O
channel O
) O
of O
the O
convolutional Method
layer Method
. O
When O
formalized O
like O
this O
it O
becomes O
clear O
that O
both O
operations O
depend O
on O
the O
same O
elements O
of O
the O
previous O
layer O
feature O
map O
. O
The O
pooling Method
layer Method
can O
be O
seen O
as O
performing O
a O
feature Method
- Method
wise Method
convolution Method
in O
which O
the O
activation O
function O
is O
replaced O
by O
the O
p O
- O
norm O
. O
One O
can O
therefore O
ask O
the O
question O
whether O
and O
why O
such O
special O
layers O
need O
to O
be O
introduced O
into O
the O
network O
. O
While O
a O
complete O
answer O
of O
this O
question O
is O
not O
easy O
to O
give O
( O
see O
the O
experiments O
and O
discussion O
for O
further O
details O
and O
remarks O
) O
we O
assume O
that O
in O
general O
there O
exist O
three O
possible O
explanations O
why O
pooling Method
can O
help O
in O
CNNs Method
: O
1 O
) O
the O
p Method
- Method
norm Method
makes O
the O
representation O
in O
a O
CNN Method
more O
invariant O
; O
2 O
) O
the O
spatial Method
dimensionality Method
reduction Method
performed O
by O
pooling Method
makes O
covering O
larger O
parts O
of O
the O
input O
in O
higher O
layers O
possible O
; O
3 O
) O
the O
feature O
- O
wise O
nature O
of O
the O
pooling Method
operation Method
( O
as O
opposed O
to O
a O
convolutional O
layer O
where O
features O
get O
mixed O
) O
could O
make O
optimization Task
easier O
. O
Assuming O
that O
only O
the O
second O
part O
– O
the O
dimensionality Method
reduction Method
performed O
by O
pooling Method
– O
is O
crucial O
for O
achieving O
good O
performance O
with O
CNNs Method
( O
a O
hypothesis O
that O
we O
later O
test O
in O
our O
experiments O
) O
one O
can O
now O
easily O
see O
that O
pooling Method
can O
be O
removed O
from O
a O
network O
without O
abandoning O
the O
spatial Method
dimensionality Method
reduction Method
by O
two O
means O
: O
We O
can O
remove O
each O
pooling Method
layer Method
and O
increase O
the O
stride O
of O
the O
convolutional Method
layer Method
that O
preceded O
it O
accordingly O
. O
We O
can O
replace O
the O
pooling Method
layer Method
by O
a O
normal Method
convolution Method
with O
stride O
larger O
than O
one O
( O
i.e. O
for O
a O
pooling Method
layer Method
with O
and O
we O
replace O
it O
with O
a O
convolution Method
layer Method
with O
corresponding O
stride O
and O
kernel O
size O
and O
number O
of O
output O
channels O
equal O
to O
the O
number O
of O
input O
channels O
) O
The O
first O
option O
has O
the O
downside O
that O
we O
significantly O
reduce O
the O
overlap O
of O
the O
convolutional O
layer O
that O
preceded O
the O
pooling Method
layer Method
. O
It O
is O
equivalent O
to O
a O
pooling Method
operation Method
in O
which O
only O
the O
top O
- O
left O
feature O
response O
is O
considered O
and O
can O
result O
in O
less O
accurate O
recognition Task
. O
The O
second O
option O
does O
not O
suffer O
from O
this O
problem O
, O
since O
all O
existing O
convolutional O
layers O
stay O
unchanged O
, O
but O
results O
in O
an O
increase O
of O
overall O
network O
parameters O
. O
It O
is O
worth O
noting O
that O
replacing O
pooling Method
by O
convolution Method
adds O
inter O
- O
feature O
dependencies O
unless O
the O
weight O
matrix O
is O
constrained O
. O
We O
emphasize O
that O
that O
this O
replacement O
can O
also O
be O
seen O
as O
learning O
the O
pooling O
operation O
rather O
than O
fixing O
it O
; O
which O
has O
previously O
been O
considered O
using O
different O
parameterizations O
in O
the O
literature O
LeCun_IEEE1998 O
, O
Gulcehre_2014 O
, O
Jia_2012 O
. O
We O
will O
evaluate O
both O
options O
in O
our O
experiments O
, O
ensuring O
a O
fair O
comparison O
w.r.t O
. O
the O
number O
of O
network O
parameters O
. O
Although O
we O
are O
not O
aware O
of O
existing O
studies O
containing O
such O
controlled O
experiments O
on O
replacing O
pooling Method
with O
convolution O
layers O
it O
should O
be O
noted O
that O
the O
idea O
of O
removing O
pooling Task
is O
not O
entirely O
unprecedented O
: O
First O
, O
the O
nomenclature O
in O
early O
work O
on O
CNNs Method
( O
referring O
to O
pooling O
layers O
as O
subsampling O
layers O
already O
) O
suggests O
the O
usage O
of O
different O
operations O
for O
subsampling Task
. O
Second O
, O
albeit O
only O
considering O
small Method
networks Method
, O
experiments O
on O
using O
only O
convolution O
layers O
( O
with O
occasional O
subsampling O
) O
in O
an O
architecture O
similar O
to O
traditional O
CNNs Method
already O
appeared O
in O
work O
on O
the O
“ O
neural Method
abstraction Method
pyramid Method
” O
. O
The O
second O
difference O
of O
the O
network Method
model Method
we O
consider O
to O
standard O
CNNs Method
is O
that O
– O
similar O
to O
models O
recently O
used O
for O
achieving O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
the O
ILSVRC Task
- Task
2012 Task
competition Task
VGG_2014 O
, O
GoingDeep_2014 O
– O
we O
make O
use O
of O
small Method
convolutional Method
layers Method
with O
which O
can O
greatly O
reduce O
the O
number O
of O
parameters O
in O
a O
network O
and O
thus O
serve O
as O
a O
form O
of O
regularization Method
. O
Additionally O
, O
to O
unify O
the O
architecture O
further O
, O
we O
make O
use O
of O
the O
fact O
that O
if O
the O
image O
area O
covered O
by O
units O
in O
the O
topmost Method
convolutional Method
layer Method
covers O
a O
portion O
of O
the O
image O
large O
enough O
to O
recognize O
its O
content O
( O
i.e. O
the O
object O
we O
want O
to O
recognize O
) O
then O
fully Method
connected Method
layers Method
can O
also O
be O
replaced O
by O
simple O
1 Method
- Method
by Method
- Method
1 Method
convolutions Method
. O
This O
leads O
to O
predictions O
of O
object O
classes O
at O
different O
positions O
which O
can O
then O
simply O
be O
averaged O
over O
the O
whole O
image O
. O
This O
scheme O
was O
first O
described O
by O
Lin_2014 Method
and O
further O
regularizes O
the O
network O
as O
the O
one Method
by Method
one Method
convolution Method
has O
much O
less O
parameters O
than O
a O
fully Method
connected Method
layer Method
. O
Overall O
our O
architecture O
is O
thus O
reduced O
to O
consist O
only O
of O
convolutional Method
layers Method
with O
rectified Method
linear Method
non Method
- Method
linearities Method
and O
an O
averaging Method
+ Method
softmax Method
layer Method
to O
produce O
predictions O
over O
the O
whole O
image O
. O
section O
: O
Experiments O
In O
order O
to O
quantify O
the O
effect O
of O
simplifying O
the O
model Method
architecture Method
we O
perform O
experiments O
on O
three O
datasets O
: O
CIFAR Material
- Material
10 Material
, O
CIFAR Material
- Material
100 Material
Krizhevsky2009 O
and O
ILSVRC O
- O
2012 O
ImageNet O
Imagenet_2009 O
. O
Specifically O
, O
we O
use O
CIFAR Material
- Material
10 Material
to O
perform O
an O
in O
- O
depth O
study O
of O
different O
models O
, O
since O
a O
large O
model O
on O
this O
dataset O
can O
be O
trained O
with O
moderate O
computing Metric
costs Metric
of O
hours O
on O
a O
modern O
GPU O
. O
We O
then O
test O
the O
best O
model O
found O
on O
CIFAR Material
- Material
10 Material
and O
CIFAR Material
- Material
100 Material
with O
and O
without O
augmentations O
and O
perform O
a O
first O
preliminary O
experiment O
on O
the O
ILSVRC O
- O
2012 O
ImageNet O
dataset O
. O
We O
performed O
all O
experiments O
using O
the O
Caffe Method
caffe Method
framework Method
. O
subsection O
: O
Experimental O
Setup O
In O
experiments O
on O
CIFAR Material
- Material
10 Material
and O
CIFAR Material
- Material
100 Material
we O
use O
three O
different O
base Method
network Method
models Method
which O
are O
intended O
to O
reflect O
current O
best O
practices O
for O
setting O
up O
CNNs Method
for O
object Task
recognition Task
. O
Architectures O
of O
these O
networks O
are O
described O
in O
Table O
[ O
reference O
] O
. O
Starting O
from O
model O
A O
( O
the O
simplest O
model O
) O
the O
depth O
and O
number O
of O
parameters O
in O
the O
network O
gradually O
increases O
to O
model O
C. O
Several O
things O
are O
to O
be O
noted O
here O
. O
First O
, O
as O
described O
in O
the O
table O
, O
all O
base O
networks O
we O
consider O
use O
a O
1 Method
- Method
by Method
- Method
1 Method
convolution Method
at O
the O
top O
to O
produce O
10 O
outputs O
of O
which O
we O
then O
compute O
an O
average O
over O
all O
positions O
and O
a O
softmax O
to O
produce O
class O
- O
probabilities O
( O
see O
Section O
[ O
reference O
] O
for O
the O
rationale O
behind O
this O
approach O
) O
. O
We O
performed O
additional O
experiments O
with O
fully Method
connected Method
layers Method
instead O
of O
1 Method
- Method
by Method
- Method
1 Method
convolutions Method
but O
found O
these O
models O
to O
consistently O
perform O
worse O
than O
their O
fully Method
convolutional Method
counterparts Method
. O
This O
is O
in O
line O
with O
similar O
findings O
from O
prior O
work O
Lin_2014 O
. O
We O
hence O
do O
not O
report O
these O
numbers O
here O
to O
avoid O
cluttering O
the O
experiments O
. O
Second O
, O
it O
can O
be O
observed O
that O
model O
B O
from O
the O
table O
is O
a O
variant O
of O
the O
Network Method
in Method
Network Method
architecture Method
proposed O
by O
Lin_2014 O
in O
which O
only O
one O
1 Method
- Method
by Method
- Method
1 Method
convolution Method
is O
performed O
after O
each O
“ O
normal Method
” Method
convolution Method
layer Method
. O
Third O
, O
model O
C O
replaces O
all O
convolutions Method
by O
simple O
convolutions Method
. O
This O
serves O
two O
purposes O
: O
1 O
) O
it O
unifies O
the O
architecture O
to O
consist O
only O
of O
layers O
operating O
on O
spatial O
neighborhoods O
of O
the O
previous O
layer O
feature O
map O
( O
with O
occasional O
subsampling O
) O
; O
2 O
) O
if O
max Method
- Method
pooling Method
is O
replaced O
by O
a O
convolutional Method
layer Method
, O
then O
is O
the O
minimum O
filter O
size O
to O
allow O
overlapping O
convolution O
with O
stride O
2 O
. O
We O
also O
highlight O
that O
model O
C O
resembles O
the O
very O
deep Method
models Method
used O
by O
VGG_2014 Method
in O
this O
years O
ImageNet O
competition O
. O
For O
each O
of O
the O
base O
models O
we O
then O
experiment O
with O
three O
additional O
variants O
. O
The O
additional O
( O
derived O
) O
models O
for O
base O
model O
C O
are O
described O
in O
in O
Table O
[ O
reference O
] O
. O
The O
derived O
models O
for O
base Method
models Method
A O
and O
B O
are O
built O
analogously O
but O
not O
shown O
in O
the O
table O
to O
avoid O
cluttering O
the O
paper O
. O
In O
general O
the O
additional O
models O
for O
each O
base O
model O
consist O
of O
: O
A O
model O
in O
which O
max Method
- Method
pooling Method
is O
removed O
and O
the O
stride O
of O
the O
convolution O
layers O
preceding O
the O
max Method
- Method
pool Method
layers Method
is O
increased O
by O
1 O
( O
to O
ensure O
that O
the O
next O
layer O
covers O
the O
same O
spatial O
region O
of O
the O
input O
image O
as O
before O
) O
. O
This O
is O
column O
“ O
Strided O
- O
CNN O
- O
C O
” O
in O
the O
table O
. O
A O
model O
in O
which O
max Method
- Method
pooling Method
is O
replaced O
by O
a O
convolution Method
layer Method
. O
This O
is O
column O
“ O
All O
- O
CNN O
- O
C O
” O
in O
the O
table O
. O
A O
model O
in O
which O
a O
dense Method
convolution Method
is O
placed O
before O
each O
max Method
- Method
pooling Method
layer Method
( O
the O
additional O
convolutions O
have O
the O
same O
kernel O
size O
as O
the O
respective O
pooling O
layer O
) O
. O
This O
is O
model O
“ O
ConvPool Method
- Method
CNN Method
- Method
C Method
” O
in O
the O
table O
. O
Experiments O
with O
this O
model O
are O
necessary O
to O
ensure O
that O
the O
effect O
we O
measure O
is O
not O
solely O
due O
to O
increasing O
model O
size O
when O
going O
from O
a O
“ O
normal O
” O
CNN Method
to O
its O
“ O
All Method
- Method
CNN Method
” Method
counterpart Method
. O
Finally O
, O
to O
test O
whether O
a O
network O
solely O
using O
convolutions Method
also O
performs O
well O
on O
a O
larger O
scale Task
recognition Task
problem Task
we O
trained O
an O
up O
- O
scaled O
version O
of O
ALL Method
- Method
CNN Method
- Method
B Method
on O
the O
ILSVRC O
2012 O
part O
of O
the O
ImageNet O
database O
. O
Although O
we O
expect O
that O
a O
larger O
network O
using O
only O
convolutions Method
and O
having O
stride O
in O
the O
first O
layer O
( O
and O
thus O
similar O
in O
style O
to O
VGG_2014 Method
) O
would O
perform O
even O
better O
on O
this O
dataset O
, O
training O
it O
would O
take O
several O
weeks O
and O
could O
thus O
not O
be O
completed O
in O
time O
for O
this O
manuscript O
. O
subsection O
: O
Classification Task
results O
subsubsection O
: O
CIFAR Material
- Material
10 Material
In O
our O
first O
experiment O
we O
compared O
all O
models O
from O
Section O
[ O
reference O
] O
on O
the O
CIFAR Material
- Material
10 Material
dataset O
without O
using O
any O
augmentations Method
. O
All O
networks O
were O
trained O
using O
stochastic Method
gradient Method
descent Method
with O
fixed O
momentum O
of O
. O
The O
learning Metric
rate Metric
was O
adapted O
using O
a O
schedule O
in O
which O
is O
multiplied O
by O
a O
fixed O
multiplier O
of O
after O
and O
epochs O
respectively O
. O
To O
keep O
the O
amount O
of O
computation O
necessary O
to O
perform O
our O
comparison O
bearable O
we O
only O
treat O
as O
a O
changeable O
hyperparameter O
for O
each O
method O
. O
The O
learning Metric
rate Metric
schedule Metric
and O
the O
total O
amount O
of O
training O
epochs O
were O
determined O
in O
a O
preliminary O
experiment O
using O
base O
model O
A O
and O
then O
fixed O
for O
all O
other O
experiments O
. O
We O
used O
and O
trained O
all O
networks O
for O
a O
total O
of O
350 O
epochs O
. O
It O
should O
be O
noted O
that O
this O
strategy O
is O
not O
guaranteed O
to O
result O
in O
the O
best O
performance O
for O
all O
methods O
and O
thus O
care O
must O
be O
taken O
when O
interpreting O
the O
the O
following O
results O
from O
our O
experiments O
. O
The O
learning Metric
rate Metric
was O
individually O
adapted O
for O
each O
model O
by O
searching O
over O
the O
fixed O
set O
. O
In O
the O
following O
we O
only O
report O
the O
results O
for O
the O
best O
for O
each O
method O
. O
Dropout Method
Hinton_arxiv2012 Method
was O
used O
to O
regularize O
all O
networks O
. O
We O
applied O
dropout Method
to O
the O
input O
image O
as O
well O
as O
after O
each O
pooling Method
layer Method
( O
or O
after O
the O
layer O
replacing O
the O
pooling O
layer O
respectively O
) O
. O
The O
dropout O
probabilities O
were O
for O
dropping O
out O
inputs O
and O
otherwise O
. O
We O
also O
experimented O
with O
additional O
dropout Method
( O
i.e. O
dropout O
on O
all O
layers O
or O
only O
on O
the O
convolution O
layer O
) O
which O
however O
did O
not O
result O
in O
increased O
accuracy Metric
. O
Additionally O
all O
models O
were O
regularized O
with O
weight Method
decay Method
. O
In O
experiments O
with O
data Task
augmentation Task
we O
perform O
only O
the O
augmentations Method
also O
used O
in O
previous O
work O
Goodfellow2013 O
, O
Lin_2014 O
in O
order O
to O
keep O
our O
results O
comparable O
. O
These O
include O
adding O
horizontally O
flipped O
examples O
of O
all O
images O
as O
well O
as O
randomly O
translated O
versions O
( O
with O
a O
maximum O
translation O
of O
5 O
pixels O
in O
each O
dimension O
) O
. O
In O
all O
experiments O
images O
were O
whitened O
and O
contrast O
normalized O
following O
Goodfellow2013 O
. O
The O
results O
for O
all O
models O
that O
we O
considered O
are O
given O
in O
Table O
[ O
reference O
] O
. O
Several O
trends O
can O
be O
observed O
from O
the O
table O
. O
First O
, O
confirming O
previous O
results O
from O
the O
literature O
Srivastava14a O
the O
simplest O
model O
( O
model O
A O
) O
already O
performs O
remarkably O
well O
, O
achieving O
error Metric
. O
Second O
, O
simply O
removing O
the O
max Method
- Method
pooling Method
layer Method
and O
just O
increasing O
the O
stride O
of O
the O
previous O
layer O
results O
in O
diminished O
performance O
in O
all O
settings O
. O
While O
this O
is O
to O
be O
expected O
we O
can O
already O
see O
that O
the O
drop O
in O
performance O
is O
not O
as O
dramatic O
as O
one O
might O
expect O
from O
such O
a O
drastic O
change O
to O
the O
network Method
architecture Method
. O
Third O
, O
surprisingly O
, O
when O
pooling Method
is O
replaced O
by O
an O
additional O
convolution Method
layer Method
with O
stride Method
performance O
stabilizes O
and O
even O
improves O
on O
the O
base O
model O
. O
To O
check O
that O
this O
is O
not O
only O
due O
to O
an O
increase O
in O
the O
number O
of O
trainable O
parameters O
we O
compare O
the O
results O
to O
the O
“ O
ConvPool Method
” Method
versions Method
of O
the O
respective O
base O
model O
. O
In O
all O
cases O
the O
performance O
of O
the O
model O
without O
any O
pooling Method
and O
the O
model O
with O
pooling Method
on O
top O
of O
the O
additional O
convolution Method
perform O
about O
on O
par O
. O
Surprisingly O
, O
this O
suggests O
that O
while O
pooling Method
can O
help O
to O
regularize Method
CNNs Method
, O
and O
generally O
does O
not O
hurt O
performance O
, O
it O
is O
not O
strictly O
necessary O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
( O
at O
least O
for O
current O
small Task
scale Task
object Task
recognition Task
datasets Task
) O
. O
In O
addition O
, O
our O
results O
confirm O
that O
small O
convolutions O
stacked O
after O
each O
other O
seem O
to O
be O
enough O
to O
achieve O
the O
best O
performance O
. O
Perhaps O
even O
more O
interesting O
is O
the O
comparison O
between O
the O
simple O
all Method
convolutional Method
network Method
derived O
from O
base Method
model Method
C Method
and O
the O
state O
of O
the O
art O
on O
CIFAR Material
- Material
10 Material
shown O
in O
Table O
[ O
reference O
] O
, O
both O
with O
and O
without O
data Method
augmentation Method
. O
In O
both O
cases O
the O
simple O
network O
performs O
better O
than O
the O
best O
previously O
reported O
result O
. O
This O
suggests O
that O
in O
order O
to O
perform O
well O
on O
current O
benchmarks O
“ O
almost O
all O
you O
need O
” O
is O
a O
stack O
of O
convolutional Method
layers Method
with O
occasional O
stride O
of O
to O
perform O
subsampling Method
. O
subsubsection O
: O
CIFAR Material
- Material
100 Material
We O
performed O
an O
additional O
experiment O
on O
the O
CIFAR Material
- Material
100 Material
dataset O
to O
confirm O
the O
efficacy O
of O
the O
best O
model O
( O
the O
All Method
- Method
CNN Method
- Method
C Method
) O
found O
for O
CIFAR Material
- Material
10 Material
. O
As O
is O
common O
practice O
we O
used O
the O
same O
model O
as O
on O
CIFAR Material
- Material
10 Material
and O
also O
kept O
all O
hyperparameters O
( O
the O
learning Metric
rate Metric
as O
well O
as O
its O
schedule O
) O
fixed O
. O
Again O
note O
that O
this O
does O
not O
necessarily O
give O
the O
best O
performance O
. O
The O
results O
of O
this O
experiment O
are O
given O
in O
Table O
[ O
reference O
] O
( O
right O
) O
. O
As O
can O
be O
seen O
, O
the O
simple O
model O
using O
only O
convolutions Method
again O
performs O
comparable O
to O
the O
state O
of O
the O
art O
for O
this O
dataset O
even O
though O
most O
of O
the O
other O
methods O
either O
use O
more O
complicated O
training Method
schemes Method
or O
network Method
architectures Method
. O
It O
is O
only O
outperformed O
by O
the O
fractional Method
max Method
- Method
pooling Method
approach Method
Graham2015 O
which O
uses O
a O
much O
larger O
network O
( O
on O
the O
order O
of O
parameters O
) O
. O
subsubsection O
: O
CIFAR Material
- Material
10 Material
with O
additional O
data Task
augmentation Task
After O
performing O
our O
experiments O
we O
became O
aware O
of O
recent O
results O
by O
Graham2015 O
who O
report O
a O
new O
state O
of O
the O
art O
on O
CIFAR Material
- Material
10 Material
/ O
100 Material
with O
data Task
augmentation Task
. O
These O
results O
were O
achieved O
using O
very O
deep O
CNNs Method
with O
convolution Method
layers Method
in O
combination O
with O
aggressive O
data Task
augmentation Task
in O
which O
the O
images O
are O
placed O
into O
large O
pixel O
images O
and O
can O
hence O
be O
heavily O
scaled O
, O
rotated O
and O
color O
augmented O
. O
We O
thus O
implemented O
the O
Large Method
- Method
All Method
- Method
CNN Method
, O
which O
is O
the O
all Method
convolutional Method
version Method
of O
this O
network O
( O
see O
Table O
[ O
reference O
] O
in O
the O
appendix O
for O
details O
) O
and O
report O
the O
results O
of O
this O
additional O
experiment O
in O
Table O
[ O
reference O
] O
( O
bottom O
right O
) O
. O
As O
can O
be O
seen O
, O
Large Method
- Method
All Method
- Method
CNN Method
achieves O
performance O
comparable O
to O
the O
network O
with O
max Method
- Method
pooling Method
. O
It O
is O
only O
outperformed O
by O
the O
fractional Method
max Method
- Method
pooling Method
approach Method
when O
performing O
multiple O
passes O
through O
the O
network O
. O
Note O
that O
these O
networks O
have O
vastly O
more O
parameters O
( O
M O
) O
than O
the O
networks O
from O
our O
previous O
experiments O
. O
We O
are O
currently O
re O
- O
training O
the O
Large Method
- Method
All Method
- Method
CNN Method
network Method
on O
CIFAR Material
- Material
100 Material
, O
and O
will O
include O
the O
results O
in O
Table O
[ O
reference O
] O
once O
training O
is O
finished O
. O
subsection O
: O
Classification Task
of Task
Imagenet Task
We O
performed O
additional O
experiments O
using O
the O
ILVRC O
- O
2012 O
subset O
of O
the O
ImageNet O
dataset O
. O
Since O
training O
a O
state O
of O
the O
art O
model O
on O
this O
dataset O
can O
take O
several O
weeks O
of O
computation O
on O
a O
modern O
GPU O
, O
we O
did O
not O
aim O
for O
best O
performance O
, O
but O
rather O
performed O
a O
simple O
’ O
proof O
of O
concept O
’ O
experiment O
. O
To O
test O
if O
the O
architectures O
performing O
best O
on O
CIFAR Material
- Material
10 Material
also O
apply O
to O
larger O
datasets O
, O
we O
trained O
an O
upscaled Method
version Method
of O
the O
All Method
- Method
CNN Method
- Method
B Method
network Method
( O
which O
is O
also O
similar O
to O
the O
architecture O
proposed O
by O
Lin_2014 O
) O
. O
It O
has O
12 O
convolutional Method
layers Method
( O
conv1 Method
- Method
conv12 Method
) O
and O
was O
trained O
for O
iterations O
with O
batches O
of O
samples O
each O
, O
starting O
with O
a O
learning Metric
rate Metric
of O
and O
dividing O
it O
by O
after O
every O
iterations O
. O
A O
weight O
decay O
of O
was O
used O
in O
all O
layers O
. O
The O
exact O
architecture O
used O
is O
given O
in O
Table O
[ O
reference O
] O
in O
the O
Appendix O
. O
This O
network O
achieves O
a O
Top Metric
- Metric
1 Metric
validation Metric
error Metric
of O
on O
ILSVRC O
- O
2012 O
, O
when O
only O
evaluating O
on O
the O
center O
patch O
, O
– O
which O
is O
comparable O
to O
the O
Top Metric
- Metric
1 Metric
error Metric
reported O
by O
Krizhevsky_NIPS2012 O
– O
while O
having O
less O
than O
million O
parameters O
( O
6 O
times O
less O
than O
the O
network O
of O
Krizhevsky_NIPS2012 O
) O
and O
taking O
roughly O
days O
to O
train O
on O
a O
Titan O
GPU O
. O
This O
supports O
our O
intuition O
that O
max Method
- Method
pooling Method
may O
not O
be O
necessary O
for O
training O
large Task
- Task
scale Task
convolutional Task
networks Task
. O
However O
, O
a O
more O
thorough O
analysis O
is O
needed O
to O
precisely O
evaluate O
the O
effect O
of O
max Method
- Method
pooling Method
on O
ImageNet Task
- Task
scale Task
networks Task
. O
Such O
a O
complete O
quantitative Task
analysis Task
using O
multiple Method
networks Method
on O
ImageNet Method
is O
extremely O
computation O
- O
time O
intensive O
and O
thus O
out O
of O
the O
scope O
of O
this O
paper O
. O
In O
order O
to O
still O
gain O
some O
insight O
into O
the O
effects O
of O
getting O
rid O
of O
max Method
- Method
pooling Method
layers Method
, O
we O
will O
try O
to O
analyze O
the O
representation O
learned O
by O
the O
all Method
convolutional Method
network Method
in O
the O
next O
section O
. O
subsection O
: O
Deconvolution Task
In O
order O
to O
analyze O
the O
network O
that O
we O
trained O
on O
ImageNet O
– O
and O
get O
a O
first O
impression O
of O
how O
well O
the O
model O
without O
pooling Method
lends O
itself O
to O
approximate Task
inversion Task
– O
we O
use O
a O
’ O
deconvolution Method
’ Method
approach Method
. O
We O
start O
from O
the O
idea O
of O
using O
a O
deconvolutional Method
network Method
for O
visualizing O
the O
parts O
of O
an O
image O
that O
are O
most O
discriminative O
for O
a O
given O
unit O
in O
a O
network O
, O
an O
approach O
recently O
proposed O
by O
Zeiler_ECCV2014 O
. O
Following O
this O
initial O
attempt O
– O
and O
observing O
that O
it O
does O
not O
always O
work O
well O
without O
max Method
- Method
pooling Method
layers Method
– O
we O
propose O
a O
new O
and O
efficient O
way O
of O
visualizing O
the O
concepts O
learned O
by O
higher Method
network Method
layers Method
. O
The O
deconvolutional Method
network Method
( O
’ O
deconvnet Method
’ Method
) O
approach O
to O
visualizing Task
concepts Task
learned O
by O
neurons O
in O
higher O
layers O
of O
a O
CNN Method
can O
be O
summarized O
as O
follows O
. O
Given O
a O
high O
- O
level O
feature O
map O
, O
the O
’ O
deconvnet Method
’ Method
inverts O
the O
data O
flow O
of O
a O
CNN Method
, O
going O
from O
neuron O
activations O
in O
the O
given O
layer O
down O
to O
an O
image O
. O
Typically O
, O
a O
single O
neuron O
is O
left O
non O
- O
zero O
in O
the O
high O
level O
feature O
map O
. O
Then O
the O
resulting O
reconstructed O
image O
shows O
the O
part O
of O
the O
input O
image O
that O
is O
most O
strongly O
activating O
this O
neuron O
( O
and O
hence O
the O
part O
that O
is O
most O
discriminative O
to O
it O
) O
. O
A O
schematic O
illustration O
of O
this O
procedure O
is O
shown O
in O
Figure O
[ O
reference O
] O
a O
) O
. O
In O
order O
to O
perform O
the O
reconstruction Task
through O
max Method
- Method
pooling Method
layers Method
, O
which O
are O
in O
general O
not O
invertible O
, O
the O
method O
of O
Zeiler O
and O
Fergus O
requires O
first O
to O
perform O
a O
forward O
pass O
of O
the O
network O
to O
compute O
’ O
switches O
’ O
– O
positions O
of O
maxima O
within O
each O
pooling O
region O
. O
These O
switches O
are O
then O
used O
in O
the O
’ O
deconvnet Method
’ Method
to O
obtain O
a O
discriminative Method
reconstruction Method
. O
By O
using O
the O
switches O
from O
a O
forward O
pass O
the O
’ O
deconvnet Method
’ Method
( O
and O
thereby O
its O
reconstruction O
) O
is O
hence O
conditioned O
on O
an O
image O
and O
does O
not O
directly O
visualize O
learned O
features O
. O
Our O
architecture O
does O
not O
include O
max Method
- Method
pooling Method
, O
meaning O
that O
in O
theory O
we O
can O
’ O
deconvolve O
’ O
without O
switches O
, O
i.e. O
not O
conditioning O
on O
an O
input O
image O
. O
This O
way O
we O
get O
insight O
into O
what O
lower O
layers O
of O
the O
network O
learn O
. O
Visualizations O
of O
features O
from O
the O
first O
three O
layers O
are O
shown O
in O
Figure O
[ O
reference O
] O
. O
Interestingly O
, O
the O
very O
first O
layer O
of O
the O
network O
does O
not O
learn O
the O
usual O
Gabor Method
filters Method
, O
but O
higher O
layers O
do O
. O
For O
higher O
layers O
of O
our O
network O
the O
method O
of O
Zeiler O
and O
Fergus Method
fails O
to O
produce O
sharp O
, O
recognizable O
, O
image O
structure O
. O
This O
is O
in O
agreement O
with O
the O
fact O
that O
lower O
layers O
learn O
general O
features O
with O
limited O
amount O
of O
invariance O
, O
which O
allows O
to O
reconstruct O
a O
single O
pattern O
that O
activates O
them O
. O
However O
, O
higher O
layers O
learn O
more O
invariant O
representations O
, O
and O
there O
is O
no O
single O
image O
maximally O
activating O
those O
neurons O
. O
Hence O
to O
get O
reasonable O
reconstructions O
it O
is O
necessary O
to O
condition O
on O
an O
input O
image O
. O
An O
alternative O
way O
of O
visualizing O
the O
part O
of O
an O
image O
that O
most O
activates O
a O
given O
neuron O
is O
to O
use O
a O
simple O
backward O
pass O
of O
the O
activation O
of O
a O
single O
neuron O
after O
a O
forward O
pass O
through O
the O
network O
; O
thus O
computing O
the O
gradient O
of O
the O
activation O
w.r.t O
. O
the O
image O
. O
The O
backward O
pass O
is O
, O
by O
design O
, O
partially O
conditioned O
on O
an O
image O
through O
both O
the O
activation O
functions O
of O
the O
network O
and O
the O
max Method
- Method
pooling Method
switches Method
( O
if O
present O
) O
. O
The O
connections O
between O
the O
deconvolution Method
and O
the O
backpropagation Method
approach Method
were O
recently O
discussed O
in O
Simonyan_arxiv2014 O
. O
In O
short O
the O
both O
methods O
differ O
mainly O
in O
the O
way O
they O
handle O
backpropagation Method
through O
the O
rectified Method
linear Method
( Method
ReLU Method
) Method
nonlinearity Method
. O
In O
order O
to O
obtain O
a O
reconstruction O
conditioned O
on O
an O
input O
image O
from O
our O
network O
without O
pooling Method
layers Method
we O
propose O
a O
modification O
of O
the O
’ O
deconvnet Method
’ Method
, O
which O
makes O
reconstructions Task
significantly O
more O
accurate O
, O
especially O
when O
reconstructing O
from O
higher O
layers O
of O
the O
network O
. O
The O
’ O
deconvolution Method
’ Method
is O
equivalent O
to O
a O
backward O
pass O
through O
the O
network O
, O
except O
that O
when O
propagating O
through O
a O
nonlinearity O
, O
its O
gradient O
is O
solely O
computed O
based O
on O
the O
top O
gradient O
signal O
, O
ignoring O
the O
bottom O
input O
. O
In O
case O
of O
the O
ReLU Method
nonlinearity Method
this O
amounts O
to O
setting O
to O
zero O
certain O
entries O
based O
on O
the O
top O
gradient O
. O
The O
two O
different O
approaches O
are O
depicted O
in O
Figure O
[ O
reference O
] O
b O
) O
, O
rows O
2 O
and O
3 O
. O
We O
propose O
to O
combine O
these O
two O
methods O
: O
rather O
than O
masking O
out O
values O
corresponding O
to O
negative O
entries O
of O
the O
top O
gradient O
( O
’ O
deconvnet O
’ O
) O
or O
bottom O
data O
( O
backpropagation O
) O
, O
we O
mask O
out O
the O
values O
for O
which O
at O
least O
one O
of O
these O
values O
is O
negative O
, O
see O
row O
4 O
of O
Figure O
[ O
reference O
] O
b O
) O
. O
We O
call O
this O
method O
guided Method
backpropagation Method
, O
because O
it O
adds O
an O
additional O
guidance O
signal O
from O
the O
higher O
layers O
to O
usual O
backpropagation Method
. O
This O
prevents O
backward O
flow O
of O
negative O
gradients O
, O
corresponding O
to O
the O
neurons O
which O
decrease O
the O
activation O
of O
the O
higher O
layer O
unit O
we O
aim O
to O
visualize O
. O
Interestingly O
, O
unlike O
the O
’ O
deconvnet Method
’ Method
, O
guided Method
backpropagation Method
works O
remarkably O
well O
without O
switches O
, O
and O
hence O
allows O
us O
to O
visualize O
intermediate O
layers O
( O
Figure O
[ O
reference O
] O
) O
as O
well O
as O
the O
last O
layers O
of O
our O
network O
( O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
in O
the O
Appendix O
) O
. O
In O
a O
sense O
, O
the O
bottom O
- O
up O
signal O
in O
form O
of O
the O
pattern O
of O
bottom O
ReLU O
activations O
substitutes O
the O
switches O
. O
To O
compare O
guided Method
backpropagation Method
and O
the O
’ O
deconvnet Method
’ Method
approach Method
, O
we O
replace O
the O
stride O
in O
our O
network O
by O
max Method
- Method
pooling Method
after O
training O
, O
which O
allows O
us O
to O
obtain O
the O
values O
of O
switches O
. O
We O
then O
visualize O
high O
level O
activations O
using O
three O
methods O
: O
backpropagation Method
, O
’ O
deconvnet Method
’ Method
and O
guided Method
backpropagation Method
. O
A O
striking O
difference O
in O
image Metric
quality Metric
is O
visible O
in O
the O
feature O
visualizations O
of O
the O
highest O
layers O
of O
the O
network O
, O
see O
Figures O
[ O
reference O
] O
and O
[ O
reference O
] O
in O
the O
Appendix O
. O
Guided Method
backpropagation Method
works O
equally O
well O
with O
and O
without O
switches O
, O
while O
the O
’ O
deconvnet Method
’ Method
approach Method
fails O
completely O
in O
the O
absence O
of O
switches O
. O
One O
potential O
reason O
why O
the O
’ O
deconvnet Method
’ O
underperforms O
in O
this O
experiment O
is O
that O
max Method
- Method
pooling Method
was O
only O
’ O
artificially O
’ O
introduced O
after O
training O
. O
As O
a O
control O
Figure O
[ O
reference O
] O
shows O
visualizations O
of O
units O
in O
the O
fully Method
connected Method
layer Method
of O
a O
network O
initially O
trained O
with O
max Method
- Method
pooling Method
. O
Again O
guided Method
backpropagation Method
produces O
cleaner O
visualizations O
than O
the O
’ O
deconvnet Method
’ Method
approach Method
. O
section O
: O
Discussion O
To O
conclude O
, O
we O
highlight O
a O
few O
key O
observations O
that O
we O
made O
in O
our O
experiments O
: O
With O
modern O
methods O
of O
training O
convolutional Method
neural Method
networks Method
very O
simple O
architectures O
may O
perform O
very O
well O
: O
a O
network O
using O
nothing O
but O
convolutions Method
and O
subsampling Method
matches O
or O
even O
slightly O
outperforms O
the O
state O
of O
the O
art O
on O
CIFAR Material
- Material
10 Material
and O
CIFAR Material
- Material
100 Material
. O
A O
similar O
architecture O
shows O
competitive O
results O
on O
ImageNet O
. O
In O
particular O
, O
as O
opposed O
to O
previous O
observations O
, O
including O
explicit Method
( Method
max Method
-) Method
pooling Method
operations Method
in O
a O
network O
does O
not O
always O
improve O
performance O
of O
CNNs Method
. O
This O
seems O
to O
be O
especially O
the O
case O
if O
the O
network O
is O
large O
enough O
for O
the O
dataset O
it O
is O
being O
trained O
on O
and O
can O
learn O
all O
necessary O
invariances O
just O
with O
convolutional Method
layers Method
. O
We O
propose O
a O
new O
method O
of O
visualizing O
the O
representations O
learned O
by O
higher Method
layers Method
of O
a O
convolutional Method
network Method
. O
While O
being O
very O
simple O
, O
it O
produces O
sharper Task
visualizations Task
of Task
descriptive Task
image Task
regions Task
than O
the O
previously O
known O
methods O
, O
and O
can O
be O
used O
even O
in O
the O
absence O
of O
’ O
switches O
’ O
– O
positions O
of O
maxima O
in O
max O
- O
pooling O
regions O
. O
We O
want O
to O
emphasize O
that O
this O
paper O
is O
not O
meant O
to O
discourage O
the O
use O
of O
pooling Method
or O
more O
sophisticated O
activation O
functions O
altogether O
. O
It O
should O
rather O
be O
understood O
as O
an O
attempt O
to O
both O
search O
for O
the O
minimum O
necessary O
ingredients O
for O
recognition Task
with O
CNNs Method
and O
establish O
a O
strong O
baseline O
on O
often O
used O
datasets O
. O
We O
also O
want O
to O
stress O
that O
the O
results O
of O
all O
models O
evaluated O
in O
this O
paper O
could O
potentially O
be O
improved O
by O
increasing O
the O
overall O
model Metric
size Metric
or O
a O
more O
thorough O
hyperparameter Method
search Method
. O
In O
a O
sense O
this O
fact O
makes O
it O
even O
more O
surprising O
that O
the O
simple O
model O
outperforms O
many O
existing O
approaches O
. O
section O
: O
Acknowledgments O
We O
acknowledge O
funding O
by O
the O
ERC O
Starting O
Grant O
VideoLearn O
( O
279401 O
) O
; O
the O
work O
was O
also O
partly O
supported O
by O
the O
BrainLinks O
- O
BrainTools O
Cluster O
of O
Excellence O
funded O
by O
the O
German O
Research O
Foundation O
( O
DFG O
, O
grant O
number O
EXC O
1086 O
) O
. O
bibliography O
: O
References O
section O
: O
Appendix O
appendix O
: O
Large Method
All Method
- Method
CNN Method
Model Method
for O
CIFAR Material
- Material
10 Material
The O
complete O
model O
architecture O
for O
the O
large Method
All Method
- Method
CNN Method
derived O
from O
the O
spatially Method
sparse Method
network Method
of O
Benjamin O
Graham O
( O
see O
Graham2015 O
for O
an O
explanation O
) O
is O
givenin O
Table O
[ O
reference O
] O
. O
Note O
that O
the O
network O
uses O
leaky O
ReLU O
units O
instead O
of O
ReLUs Method
as O
we O
found O
these O
to O
speed O
up O
training Task
. O
As O
can O
be O
seen O
it O
also O
requires O
a O
much O
larger O
input O
size O
in O
which O
the O
pixel O
image O
is O
centered O
( O
and O
then O
potentially O
augmented O
by O
applying O
multiple O
transformations O
such O
as O
scaling O
) O
. O
As O
a O
result O
the O
subsampling Method
performed O
by O
the O
convolutional Method
layers Method
with O
stride O
2 O
can O
hence O
be O
applied O
much O
more O
slowly O
. O
Also O
note O
that O
this O
network O
only O
consists O
of O
convolutions Method
with O
occasional O
subsampling Method
until O
the O
spatial O
dimensionality O
is O
reduced O
to O
. O
It O
does O
hence O
not O
employ O
global Method
average Method
pooling Method
at O
the O
end O
of O
the O
network O
. O
In O
a O
sense O
this O
architecture O
hence O
represents O
the O
most O
simple O
convolutional Method
network Method
usable O
for O
this O
task O
. O
appendix O
: O
Imagenet Method
Model Method
The O
complete O
model O
architecture O
for O
the O
network O
trained O
on O
the O
ILSVRC O
- O
2102 O
ImageNet O
dataset O
is O
given O
in O
Table O
[ O
reference O
] O
. O
appendix O
: O
Additional O
Visualizations O
Additional O
visualizations O
of O
the O
features O
learned O
by O
the O
last O
convolutional Method
layer Method
’ O
conv12 Method
’ O
as O
well O
as O
the O
pre Method
- Method
softmax Method
layer Method
’ O
global_pool O
’ O
are O
depicted O
in O
Figure O
[ O
reference O
] O
and O
Figure O
[ O
reference O
] O
respectively O
. O
To O
allow O
fair O
comparison O
of O
’ O
deconvnet Method
’ Method
and O
guided Method
backpropagation Method
, O
we O
additionally O
show O
in O
Figure O
[ O
reference O
] O
visualizations O
from O
a O
model O
with O
max Method
- Method
pooling Method
trained O
on O
ImageNet O
. O
