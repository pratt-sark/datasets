document	O
:	O
Reaching	O
Human	O
-	O
level	O
Performance	O
in	O
Automatic	Task
Grammatical	Task
Error	Task
Correction	Task
:	O
An	O
Empirical	O
Study	O
Neural	Method
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
(	O
seq2seq	Method
)	O
approaches	O
have	O
proven	O
to	O
be	O
successful	O
in	O
grammatical	Task
error	Task
correction	Task
(	O
GEC	Task
)	O
.	O
Based	O
on	O
the	O
seq2seq	Method
framework	O
,	O
we	O
propose	O
a	O
novel	O
fluency	Method
boost	Method
learning	Method
and	Method
inference	Method
mechanism	Method
.	O
Fluency	Method
boosting	Method
learning	Method
generates	O
diverse	O
error	O
-	O
corrected	O
sentence	O
pairs	O
during	O
training	O
,	O
enabling	O
the	O
error	Method
correction	Method
model	Method
to	O
learn	O
how	O
to	O
improve	O
a	O
sentence	O
’s	O
fluency	O
from	O
more	O
instances	O
,	O
while	O
fluency	Method
boosting	Method
inference	Method
allows	O
the	O
model	O
to	O
correct	O
a	O
sentence	O
incrementally	O
with	O
multiple	O
inference	Method
steps	Method
.	O
Combining	O
fluency	Method
boost	Method
learning	Method
and	O
inference	Method
with	O
convolutional	O
seq2seq	Method
models	O
,	O
our	O
approach	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
:	O
75.72	O
(	O
)	O
on	O
CoNLL	Material
-	O
2014	O
10	O
annotation	O
dataset	O
and	O
62.42	O
(	O
GLEU	Metric
)	O
on	O
JFLEG	Material
test	Material
set	Material
respectively	O
,	O
becoming	O
the	O
first	O
GEC	Method
system	Method
that	O
reaches	O
human	O
-	O
level	O
performance	O
(	O
72.58	O
for	O
CoNLL	Material
and	O
62.37	O
for	O
JFLEG	Material
)	O
on	O
both	O
of	O
the	O
benchmarks	O
.	O
section	O
:	O
Introduction	O
Sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	Method
)	O
models	O
cho	O
-	O
EtAl:2014:EMNLP2014	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
SutskeverVL14	O
for	O
grammatical	Task
error	Task
correction	Task
(	O
GEC	Task
)	O
have	O
drawn	O
growing	O
attention	O
yuan2016grammatical	O
,	O
xie2016neural	O
,	O
ji2017nested	O
,	O
schmaltz	O
-	O
EtAl:2017:EMNLP2017	O
,	O
sakaguchi2017grammatical	O
,	O
chollampatt2018	O
,	O
junczys2018approaching	O
in	O
recent	O
years	O
.	O
However	O
,	O
most	O
of	O
the	O
seq2seq	Method
models	Method
for	O
GEC	Task
have	O
two	O
flaws	O
.	O
First	O
,	O
the	O
seq2seq	Method
models	Method
are	O
trained	O
with	O
only	O
limited	O
error	O
-	O
corrected	O
sentence	O
pairs	O
like	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
.	O
Limited	O
by	O
the	O
size	O
of	O
training	O
data	O
,	O
the	O
models	O
with	O
millions	O
of	O
parameters	O
may	O
not	O
be	O
well	O
generalized	O
.	O
Thus	O
,	O
it	O
is	O
common	O
that	O
the	O
models	O
fail	O
to	O
correct	O
a	O
sentence	O
perfectly	O
even	O
if	O
the	O
sentence	O
is	O
slightly	O
different	O
from	O
the	O
training	O
instance	O
,	O
as	O
illustrated	O
by	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
.	O
Second	O
,	O
the	O
seq2seq	Method
models	Method
usually	O
can	O
not	O
perfectly	O
correct	O
a	O
sentence	O
with	O
many	O
grammatical	O
errors	O
through	O
single	O
-	O
round	O
seq2seq	Method
inference	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
and	O
[	O
reference	O
]	O
(	O
c	O
)	O
,	O
because	O
some	O
errors	O
in	O
a	O
sentence	O
may	O
make	O
the	O
context	O
strange	O
,	O
which	O
confuses	O
the	O
models	O
to	O
correct	O
other	O
errors	O
.	O
To	O
address	O
the	O
above	O
-	O
mentioned	O
limitations	O
in	O
model	Task
learning	Task
and	Task
inference	Task
,	O
we	O
propose	O
a	O
novel	O
fluency	Method
boost	Method
learning	Method
and	Method
inference	Method
mechanism	Method
,	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
For	O
fluency	Task
boosting	Task
learning	Task
,	O
not	O
only	O
is	O
a	O
seq2seq	Method
model	Method
trained	O
with	O
original	O
error	O
-	O
corrected	O
sentence	O
pairs	O
,	O
but	O
also	O
it	O
generates	O
less	O
fluent	O
sentences	O
(	O
e.g.	O
,	O
from	O
its	O
n	O
-	O
best	O
outputs	O
)	O
to	O
establish	O
new	O
error	O
-	O
corrected	O
sentence	O
pairs	O
by	O
pairing	O
them	O
with	O
their	O
correct	O
sentences	O
during	O
training	O
,	O
as	O
long	O
as	O
the	O
sentences	O
’	O
fluency	O
is	O
below	O
that	O
of	O
their	O
correct	O
sentences	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
.	O
Specifically	O
,	O
we	O
call	O
the	O
generated	O
error	O
-	O
corrected	O
sentence	O
pairs	O
fluency	O
boost	O
sentence	O
pairs	O
because	O
the	O
sentence	O
in	O
the	O
target	O
side	O
always	O
improves	O
fluency	O
over	O
that	O
in	O
the	O
source	O
side	O
.	O
The	O
generated	O
fluency	O
boost	O
sentence	O
pairs	O
during	O
training	O
will	O
be	O
used	O
as	O
additional	O
training	O
instances	O
during	O
subsequent	O
training	O
epochs	O
,	O
allowing	O
the	O
error	Method
correction	Method
model	Method
to	O
see	O
more	O
grammatically	O
incorrect	O
sentences	O
during	O
training	O
and	O
accordingly	O
improving	O
its	O
generalization	Metric
ability	Metric
.	O
For	O
model	Task
inference	Task
,	O
fluency	Method
boost	Method
inference	Method
mechanism	Method
allows	O
the	O
model	O
to	O
correct	O
a	O
sentence	O
incrementally	O
with	O
multi	Method
-	Method
round	Method
inference	Method
as	O
long	O
as	O
the	O
proposed	O
edits	O
can	O
boost	O
the	O
sentence	O
’s	O
fluency	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
.	O
For	O
a	O
sentence	O
with	O
multiple	O
grammatical	O
errors	O
,	O
some	O
of	O
the	O
errors	O
will	O
be	O
corrected	O
first	O
.	O
The	O
corrected	O
parts	O
will	O
make	O
the	O
context	O
clearer	O
,	O
which	O
may	O
benefit	O
the	O
model	O
to	O
correct	O
the	O
remaining	O
errors	O
.	O
Moreover	O
,	O
based	O
on	O
the	O
special	O
characteristics	O
of	O
this	O
task	O
that	O
the	O
output	Task
prediction	Task
can	O
be	O
repeatedly	O
edited	O
and	O
the	O
basic	O
fluency	Method
boost	Method
inference	Method
idea	Method
,	O
we	O
further	O
propose	O
a	O
round	Method
-	Method
way	Method
correction	Method
approach	Method
that	O
uses	O
two	O
seq2seq	Method
models	Method
whose	O
decoding	O
orders	O
are	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
respectively	O
.	O
For	O
round	Task
-	Task
way	Task
correction	Task
,	O
a	O
sentence	O
will	O
be	O
corrected	O
successively	O
by	O
the	O
right	O
-	O
to	O
-	O
left	O
and	O
left	O
-	O
to	O
-	O
right	O
seq2seq	Method
model	Method
.	O
Since	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	Method
decoder	Method
decode	O
a	O
sequence	O
with	O
different	O
contexts	O
,	O
they	O
have	O
their	O
unique	O
advantages	O
for	O
specific	O
error	O
types	O
.	O
Round	Method
-	Method
way	Method
correction	Method
can	O
fully	O
exploit	O
their	O
pros	O
and	O
make	O
them	O
complement	O
each	O
other	O
,	O
which	O
results	O
in	O
a	O
significant	O
improvement	O
of	O
recall	Metric
.	O
Experiments	O
show	O
that	O
combining	O
fluency	Method
boost	Method
learning	Method
and	O
inference	Method
with	O
convolutional	O
seq2seq	Method
models	O
,	O
our	O
best	O
GEC	Method
system	Method
achieves	O
75.72	O
on	O
CoNLL	Material
-	O
2014	O
10	O
annotation	O
dataset	O
and	O
62.42	O
on	O
JFLEG	Material
test	Material
set	Material
,	O
becoming	O
the	O
first	O
system	O
reaching	O
human	O
-	O
level	O
performance	O
on	O
both	O
of	O
the	O
GEC	Task
benchmarks	O
.	O
section	O
:	O
Background	O
:	O
Neural	Task
grammatical	Task
error	Task
correction	Task
As	O
neural	Task
machine	Task
translation	Task
(	O
NMT	Task
)	Task
,	O
a	O
typical	O
neural	O
GEC	Task
approach	O
uses	O
an	O
encoder	O
-	O
decoder	O
seq2seq	Method
model	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
SutskeverVL14	O
,	O
cho	O
-	O
EtAl:2014:EMNLP2014	O
with	O
attention	Method
mechanism	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
BahdanauCB14	O
to	O
edit	O
a	O
raw	O
sentence	O
into	O
the	O
grammatically	O
correct	O
sentence	O
it	O
should	O
be	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
.	O
Given	O
a	O
raw	O
sentence	O
and	O
its	O
corrected	O
sentence	O
in	O
which	O
and	O
are	O
the	O
-	O
th	O
and	O
-	O
th	O
words	O
of	O
sentence	O
and	O
respectively	O
,	O
the	O
error	O
correction	O
seq2seq	Method
model	O
learns	O
a	O
probabilistic	Method
mapping	Method
from	O
error	O
-	O
corrected	O
sentence	O
pairs	O
through	O
maximum	Method
likelihood	Method
estimation	Method
(	O
MLE	Method
)	O
,	O
which	O
learns	O
model	O
parameters	O
to	O
maximize	O
the	O
following	O
equation	O
:	O
where	O
denotes	O
the	O
set	O
of	O
error	O
-	O
corrected	O
sentence	O
pairs	O
.	O
For	O
model	Task
inference	Task
,	O
an	O
output	O
sequence	O
is	O
selected	O
through	O
beam	Method
search	Method
,	O
which	O
maximizes	O
the	O
following	O
equation	O
:	O
section	O
:	O
Fluency	Task
boost	Task
learning	Task
Conventional	O
seq2seq	Method
models	Method
for	O
GEC	Task
learn	O
model	O
parameters	O
only	O
from	O
original	O
error	O
-	O
corrected	O
sentence	O
pairs	O
.	O
However	O
,	O
such	O
error	O
-	O
corrected	O
sentence	O
pairs	O
are	O
not	O
sufficiently	O
available	O
.	O
As	O
a	O
result	O
,	O
many	O
neural	O
GEC	Task
models	O
are	O
not	O
very	O
well	O
generalized	O
.	O
Fortunately	O
,	O
neural	Method
GEC	Method
is	O
different	O
from	O
NMT	Method
.	O
For	O
neural	Method
GEC	Method
,	O
its	O
goal	O
is	O
improving	O
a	O
sentence	O
’s	O
fluency	O
without	O
changing	O
its	O
original	O
meaning	O
;	O
thus	O
,	O
any	O
sentence	O
pair	O
that	O
satisfies	O
this	O
condition	O
(	O
we	O
call	O
it	O
fluency	O
boost	O
condition	O
)	O
can	O
be	O
used	O
as	O
a	O
training	O
instance	O
.	O
In	O
this	O
work	O
,	O
we	O
define	O
as	O
the	O
fluency	Metric
score	Metric
of	O
a	O
sentence	O
:	O
where	O
is	O
the	O
probability	O
of	O
given	O
context	O
,	O
computed	O
by	O
a	O
language	Method
model	Method
,	O
and	O
is	O
the	O
length	O
of	O
sentence	O
.	O
is	O
actually	O
the	O
cross	O
entropy	O
of	O
the	O
sentence	O
,	O
whose	O
range	O
is	O
.	O
Accordingly	O
,	O
the	O
range	O
of	O
is	O
.	O
The	O
core	O
idea	O
of	O
fluency	Method
boost	Method
learning	Method
is	O
to	O
generate	O
fluency	O
boost	O
sentence	O
pairs	O
that	O
satisfy	O
the	O
fluency	O
boost	O
condition	O
during	O
training	O
,	O
as	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
illustrates	O
,	O
so	O
that	O
these	O
pairs	O
can	O
further	O
help	O
model	Task
learning	Task
.	O
In	O
this	O
section	O
,	O
we	O
present	O
three	O
fluency	Method
boost	Method
learning	Method
strategies	Method
:	O
back	Method
-	Method
boost	Method
,	O
self	Method
-	Method
boost	Method
,	O
and	O
dual	Method
-	Method
boost	Method
that	O
generate	O
fluency	O
boost	O
sentence	O
pairs	O
in	O
different	O
ways	O
,	O
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
Back	Method
-	Method
boost	Method
learning	Method
Back	Method
-	Method
boost	Method
learning	Method
borrows	O
the	O
idea	O
from	O
back	Task
translation	Task
sennrich2016improving	Task
in	O
NMT	Task
,	O
referring	O
to	O
training	O
a	O
backward	Method
model	Method
(	O
we	O
call	O
it	O
error	Method
generation	Method
model	Method
,	O
as	O
opposed	O
to	O
error	Method
correction	Method
model	Method
)	O
that	O
is	O
used	O
to	O
convert	O
a	O
fluent	O
sentence	O
to	O
a	O
less	O
fluent	O
sentence	O
with	O
errors	O
.	O
Since	O
the	O
less	O
fluent	O
sentences	O
are	O
generated	O
by	O
the	O
error	O
generation	O
seq2seq	Method
model	O
trained	O
with	O
error	O
-	O
corrected	O
data	O
,	O
they	O
usually	O
do	O
not	O
change	O
the	O
original	O
sentence	O
’s	O
meaning	O
;	O
thus	O
,	O
they	O
can	O
be	O
paired	O
with	O
their	O
correct	O
sentences	O
,	O
establishing	O
fluency	O
boost	O
sentence	O
pairs	O
that	O
can	O
be	O
used	O
as	O
training	O
instances	O
for	O
error	Method
correction	Method
models	Method
,	O
as	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
.	O
Specifically	O
,	O
we	O
first	O
train	O
a	O
seq2seq	Method
error	O
generation	O
model	O
with	O
which	O
is	O
identical	O
to	O
except	O
that	O
the	O
source	O
sentence	O
and	O
the	O
target	O
sentence	O
are	O
interchanged	O
.	O
Then	O
,	O
we	O
use	O
the	O
model	O
to	O
predict	O
-	O
best	O
outputs	O
given	O
a	O
correct	O
sentence	O
.	O
Given	O
the	O
fluency	O
boost	O
condition	O
,	O
we	O
compare	O
the	O
fluency	O
of	O
each	O
output	O
(	O
where	O
)	O
to	O
that	O
of	O
its	O
correct	O
sentence	O
.	O
If	O
an	O
output	O
sentence	O
’s	O
fluency	Metric
score	Metric
is	O
much	O
lower	O
than	O
its	O
correct	O
sentence	O
,	O
we	O
call	O
it	O
a	O
disfluency	O
candidate	O
of	O
.	O
To	O
formalize	O
this	O
process	O
,	O
we	O
first	O
define	O
to	O
denote	O
the	O
-	O
best	O
outputs	O
predicted	O
by	O
model	O
given	O
the	O
input	O
.	O
Then	O
,	O
disfluency	O
candidates	O
of	O
a	O
correct	O
sentence	O
can	O
be	O
derived	O
:	O
where	O
denotes	O
the	O
disfluency	O
candidate	O
set	O
for	O
in	O
back	Method
-	Method
boost	Method
learning	Method
.	O
is	O
a	O
threshold	O
to	O
determine	O
if	O
is	O
less	O
fluent	O
than	O
and	O
it	O
should	O
be	O
slightly	O
larger	O
than	O
,	O
which	O
helps	O
filter	O
out	O
sentence	O
pairs	O
with	O
unnecessary	O
edits	O
(	O
e.g.	O
,	O
I	O
like	O
this	O
book	O
.	O
I	O
like	O
the	O
book	O
.	O
)	O
.	O
In	O
the	O
subsequent	O
training	O
epochs	O
,	O
the	O
error	Method
correction	Method
model	Method
will	O
not	O
only	O
learn	O
from	O
the	O
original	O
error	O
-	O
corrected	O
sentence	O
pairs	O
(	O
,	O
)	O
,	O
but	O
also	O
learn	O
from	O
fluency	O
boost	O
sentence	O
pairs	O
(	O
,	O
)	O
where	O
is	O
a	O
sample	O
of	O
)	O
.	O
We	O
summarize	O
this	O
process	O
in	O
Algorithm	O
[	O
reference	O
]	O
where	O
is	O
the	O
set	O
of	O
original	O
error	O
-	O
corrected	O
sentence	O
pairs	O
,	O
and	O
can	O
be	O
tentatively	O
considered	O
identical	O
to	O
when	O
there	O
is	O
no	O
additional	O
native	O
data	O
to	O
help	O
model	Task
training	Task
(	O
see	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Note	O
that	O
we	O
constrain	O
the	O
size	O
of	O
not	O
to	O
exceed	O
(	O
the	O
7th	O
line	O
in	O
Algorithm	O
[	O
reference	O
]	O
)	O
to	O
avoid	O
that	O
too	O
many	O
fluency	O
boost	O
pairs	O
overwhelm	O
the	O
effects	O
of	O
the	O
original	O
error	O
-	O
corrected	O
pairs	O
on	O
model	Method
learning	Method
.	O
[	O
t	O
]	O
Back	Method
-	Method
boost	Method
learning	Method
[	O
1	O
]	O
Train	O
error	Method
generation	Method
model	Method
with	O
;	O
each	O
sentence	O
pair	O
Compute	O
according	O
to	O
Eq	O
(	O
[	O
reference	O
]	O
)	O
;	O
each	O
training	O
epoch	O
;	O
Derive	O
a	O
subset	O
by	O
randomly	O
sampling	O
elements	O
from	O
;	O
each	O
Establish	O
a	O
fluency	O
boost	O
pair	O
by	O
randomly	O
sampling	O
;	O
;	O
Update	O
error	Method
correction	Method
model	Method
with	O
;	O
subsection	O
:	O
Self	Method
-	Method
boost	Method
learning	Method
In	O
contrast	O
to	O
back	Method
-	Method
boost	Method
learning	Method
whose	O
core	O
idea	O
is	O
originally	O
from	O
NMT	Method
,	O
self	Method
-	Method
boost	Method
learning	Method
is	O
original	O
,	O
which	O
is	O
specially	O
devised	O
for	O
neural	Method
GEC	Method
.	O
The	O
idea	O
of	O
self	Method
-	Method
boost	Method
learning	Method
is	O
illustrated	O
by	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
and	O
was	O
already	O
briefly	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
and	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
.	O
Unlike	O
back	Method
-	Method
boost	Method
learning	Method
in	O
which	O
an	O
error	O
generation	O
seq2seq	Method
model	O
is	O
trained	O
to	O
generate	O
disfluency	O
candidates	O
,	O
self	Method
-	Method
boost	Method
learning	Method
allows	O
the	O
error	Method
correction	Method
model	Method
to	O
generate	O
the	O
candidates	O
by	O
itself	O
.	O
Since	O
the	O
disfluency	O
candidates	O
generated	O
by	O
the	O
error	O
correction	O
seq2seq	Method
model	O
trained	O
with	O
error	O
-	O
corrected	O
data	O
rarely	O
change	O
the	O
input	O
sentence	O
’s	O
meaning	O
;	O
thus	O
,	O
they	O
can	O
be	O
used	O
to	O
establish	O
fluency	O
boost	O
sentence	O
pairs	O
.	O
For	O
self	Method
-	Method
boost	Method
learning	Method
,	O
given	O
an	O
error	O
corrected	O
pair	O
,	O
an	O
error	Method
correction	Method
model	Method
first	O
predicts	O
-	O
best	O
outputs	O
for	O
the	O
raw	O
sentence	O
.	O
Among	O
the	O
-	O
best	O
outputs	O
,	O
any	O
output	O
that	O
is	O
not	O
identical	O
to	O
can	O
be	O
considered	O
as	O
an	O
error	Task
prediction	Task
.	O
Instead	O
of	O
treating	O
the	O
error	O
predictions	O
useless	O
,	O
self	Method
-	Method
boost	Method
learning	Method
fully	O
exploits	O
them	O
.	O
Specifically	O
,	O
if	O
an	O
error	Task
prediction	Task
is	O
much	O
less	O
fluent	O
than	O
that	O
of	O
its	O
correct	O
sentence	O
,	O
it	O
will	O
be	O
added	O
to	O
’s	O
disfluency	O
candidate	O
set	O
,	O
as	O
Eq	O
(	O
[	O
reference	O
]	O
)	O
shows	O
:	O
In	O
contrast	O
to	O
back	Method
-	Method
boost	Method
learning	Method
,	O
self	Method
-	Method
boost	Method
generates	O
disfluency	O
candidates	O
from	O
a	O
different	O
perspective	O
–	O
by	O
editing	O
the	O
raw	O
sentence	O
rather	O
than	O
the	O
correct	O
sentence	O
.	O
It	O
is	O
also	O
noteworthy	O
that	O
is	O
incrementally	O
expanded	O
because	O
the	O
error	Method
correction	Method
model	Method
is	O
dynamically	O
updated	O
,	O
as	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
[	O
h	O
]	O
Self	Method
-	Method
boost	Method
learning	Method
[	O
1	O
]	O
each	O
sentence	O
pair	O
;	O
each	O
training	O
epoch	O
Update	O
error	Method
correction	Method
model	Method
with	O
;	O
Derive	O
a	O
subset	O
by	O
randomly	O
sampling	O
elements	O
from	O
;	O
each	O
Update	O
according	O
to	O
Eq	O
(	O
[	O
reference	O
]	O
)	O
;	O
Establish	O
a	O
fluency	O
boost	O
pair	O
by	O
randomly	O
sampling	O
;	O
;	O
subsection	O
:	O
Dual	Method
-	Method
boost	Method
learning	Method
As	O
introduced	O
above	O
,	O
back	Method
-	Method
and	Method
self	Method
-	Method
boost	Method
learning	Method
generate	O
disfluency	O
candidates	O
from	O
different	O
perspectives	O
to	O
create	O
more	O
fluency	O
boost	O
sentence	O
pairs	O
to	O
benefit	O
training	O
the	O
error	Method
correction	Method
model	Method
.	O
Intuitively	O
,	O
the	O
more	O
diverse	O
disfluency	O
candidates	O
generated	O
,	O
the	O
more	O
helpful	O
for	O
training	O
an	O
error	Method
correction	Method
model	Method
.	O
Inspired	O
by	O
and	O
,	O
we	O
propose	O
a	O
dual	Method
-	Method
boost	Method
learning	Method
strategy	Method
,	O
combining	O
both	O
back	Method
-	Method
and	Method
self	Method
-	Method
boost	Method
’s	O
perspectives	O
to	O
generate	O
disfluency	O
candidates	O
.	O
As	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)	O
shows	O
,	O
disfluency	O
candidates	O
in	O
dual	Method
-	Method
boost	Method
learning	Method
are	O
from	O
both	O
the	O
error	Method
generation	Method
model	Method
and	O
the	O
error	Method
correction	Method
model	Method
:	O
Moreover	O
,	O
the	O
error	Method
correction	Method
model	Method
and	O
the	O
error	Method
generation	Method
model	Method
are	O
dual	O
and	O
both	O
of	O
them	O
are	O
dynamically	O
updated	O
,	O
which	O
improves	O
each	O
other	O
:	O
the	O
disfluency	O
candidates	O
produced	O
by	O
error	Method
generation	Method
model	Method
can	O
benefit	O
training	O
the	O
error	Method
correction	Method
model	Method
,	O
while	O
the	O
disfluency	O
candidates	O
created	O
by	O
error	Method
correction	Method
model	Method
can	O
be	O
used	O
as	O
training	O
data	O
for	O
the	O
error	Method
generation	Method
model	Method
.	O
We	O
summarize	O
this	O
learning	Method
approach	Method
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
[	O
t	O
]	O
Dual	Method
-	Method
boost	Method
learning	Method
[	O
1	O
]	O
each	O
;	O
;	O
;	O
each	O
training	O
epoch	O
Update	O
error	Method
correction	Method
model	Method
with	O
;	O
Update	Method
error	Method
generation	Method
model	Method
with	O
;	O
;	O
;	O
Derive	O
a	O
subset	O
by	O
randomly	O
sampling	O
elements	O
from	O
;	O
each	O
Update	O
according	O
to	O
Eq	O
(	O
[	O
reference	O
]	O
)	O
;	O
Establish	O
a	O
fluency	O
boost	O
pair	O
by	O
randomly	O
sampling	O
;	O
;	O
Establish	O
a	O
reversed	O
fluency	O
boost	O
pair	O
by	O
randomly	O
sampling	O
;	O
;	O
subsection	O
:	O
Fluency	Method
boost	Method
learning	Method
with	O
large	O
-	O
scale	O
native	O
data	O
Our	O
proposed	O
fluency	Method
boost	Method
learning	Method
strategies	Method
can	O
be	O
easily	O
extended	O
to	O
utilize	O
massive	O
native	O
text	O
data	O
which	O
proved	O
to	O
be	O
useful	O
for	O
GEC	Task
.	O
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
when	O
there	O
is	O
no	O
additional	O
native	O
data	O
,	O
in	O
Algorithm	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
is	O
identical	O
to	O
.	O
In	O
the	O
case	O
where	O
additional	O
native	O
data	O
is	O
available	O
to	O
help	O
model	Task
learning	Task
,	O
becomes	O
:	O
where	O
denotes	O
the	O
set	O
of	O
self	O
-	O
copied	O
sentence	O
pairs	O
from	O
native	O
data	O
.	O
section	O
:	O
Fluency	Task
boost	Task
inference	Task
subsection	O
:	O
Multi	Task
-	Task
round	Task
error	Task
correction	Task
As	O
we	O
discuss	O
in	O
Section	O
[	O
reference	O
]	O
,	O
some	O
sentences	O
with	O
multiple	O
grammatical	O
errors	O
usually	O
can	O
not	O
be	O
perfectly	O
corrected	O
through	O
normal	O
seq2seq	Method
inference	O
which	O
makes	O
only	O
single	Method
-	Method
round	Method
inference	Method
.	O
Fortunately	O
,	O
neural	Method
GEC	Method
is	O
different	O
from	O
NMT	Method
:	O
its	O
source	O
and	O
target	O
language	O
are	O
the	O
same	O
.	O
The	O
characteristic	O
allows	O
us	O
to	O
edit	O
a	O
sentence	O
more	O
than	O
once	O
through	O
multi	Method
-	Method
round	Method
model	Method
inference	Method
,	O
which	O
motivates	O
our	O
fluency	Method
boost	Method
inference	Method
.	O
As	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
,	O
fluency	Method
boost	Method
inference	Method
allows	O
a	O
sentence	O
to	O
be	O
incrementally	O
edited	O
through	O
multi	O
-	O
round	O
seq2seq	Method
inference	O
as	O
long	O
as	O
the	O
sentence	O
’s	O
fluency	O
can	O
be	O
improved	O
.	O
Specifically	O
,	O
an	O
error	O
correction	O
seq2seq	Method
model	O
first	O
takes	O
a	O
raw	O
sentence	O
as	O
an	O
input	O
and	O
outputs	O
a	O
hypothesis	O
.	O
Instead	O
of	O
regarding	O
as	O
the	O
final	O
prediction	O
,	O
fluency	Method
boost	Method
inference	Method
will	O
then	O
take	O
as	O
the	O
input	O
to	O
generate	O
the	O
next	O
output	O
.	O
The	O
process	O
will	O
not	O
terminate	O
unless	O
does	O
not	O
improve	O
in	O
terms	O
of	O
fluency	O
.	O
subsection	O
:	O
Round	Task
-	Task
way	Task
error	Task
correction	Task
Based	O
on	O
the	O
idea	O
of	O
multi	Method
-	Method
round	Method
correction	Method
,	O
we	O
further	O
propose	O
an	O
advanced	O
fluency	Method
boost	Method
inference	Method
approach	Method
:	O
round	Method
-	Method
way	Method
error	Method
correction	Method
.	O
Instead	O
of	O
progressively	O
correcting	O
a	O
sentence	O
with	O
the	O
same	O
seq2seq	Method
model	Method
as	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
,	O
round	O
-	O
way	O
correction	O
corrects	O
a	O
sentence	O
through	O
a	O
right	O
-	O
to	O
-	O
left	O
seq2seq	Method
model	Method
and	O
a	O
left	O
-	O
to	O
-	O
right	O
seq2seq	Method
model	Method
successively	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
motivation	O
of	O
round	Task
-	Task
way	Task
error	Task
correction	Task
is	O
straightforward	O
.	O
Decoders	Method
with	O
different	O
decoding	O
orders	O
decode	O
word	O
sequences	O
with	O
different	O
contexts	O
,	O
making	O
them	O
have	O
their	O
unique	O
advantages	O
for	O
specific	O
error	O
types	O
.	O
For	O
the	O
example	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
error	O
of	O
a	O
lack	O
of	O
an	O
article	O
(	O
i.e.	O
,	O
park	O
→	O
the	O
park	O
)	O
is	O
more	O
likely	O
to	O
be	O
corrected	O
by	O
the	O
right	O
-	O
to	O
-	O
left	O
seq2seq	Method
model	O
than	O
the	O
left	O
-	O
to	O
-	O
right	O
one	O
,	O
because	O
whether	O
to	O
add	O
an	O
article	O
depends	O
on	O
the	O
noun	O
park	O
that	O
was	O
already	O
seen	O
by	O
the	O
right	O
-	O
to	O
-	O
left	O
model	O
when	O
it	O
made	O
the	O
decision	O
.	O
In	O
contrast	O
,	O
the	O
left	O
-	O
to	O
-	O
right	Method
model	Method
might	O
be	O
better	O
at	O
dealing	O
with	O
subject	Task
-	Task
verb	Task
agreement	Task
errors	Task
(	O
e.g.	O
,	O
come	O
→	O
comes	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
because	O
the	O
keyword	O
that	O
decides	O
the	O
verb	O
form	O
is	O
its	O
subject	O
She	O
which	O
is	O
at	O
the	O
beginning	O
of	O
the	O
sentence	O
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Dataset	O
and	O
evaluation	O
As	O
previous	O
studies	O
ji2017nested	O
,	O
we	O
use	O
the	O
public	Material
Lang	Material
-	Material
8	Material
Corpus	Material
mizumoto2011mining	O
,	O
tajiri2012tense	O
,	O
Cambridge	Material
Learner	Material
Corpus	Material
(	O
CLC	Material
)	O
nicholls2003cambridge	O
and	O
NUS	Material
Corpus	Material
of	Material
Learner	Material
English	Material
(	O
NUCLE	Material
)	O
dahlmeier2013building	O
as	O
our	O
original	O
error	O
-	O
corrected	O
training	O
data	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
stats	O
of	O
the	O
datasets	O
.	O
In	O
addition	O
,	O
we	O
also	O
collect	O
2	O
,	O
865	O
,	O
639	O
non	O
-	O
public	O
error	O
-	O
corrected	O
sentence	O
pairs	O
from	O
Lang	Material
-	Material
8.com	Material
.	O
The	O
native	Material
data	Material
we	O
use	O
for	O
fluency	Task
boost	Task
learning	Task
is	O
English	Material
Wikipedia	Material
that	O
contains	O
61	O
,	O
677	O
,	O
453	O
sentences	O
.	O
We	O
use	O
CoNLL	Material
-	O
2014	O
shared	O
task	O
dataset	O
ng2014conll	O
and	O
JFLEG	Material
napoles2017jfleg	O
test	O
set	O
as	O
our	O
evaluation	O
datasets	O
.	O
CoNLL	Material
-	O
2014	O
test	O
set	O
contains	O
1	O
,	O
312	O
sentences	O
,	O
while	O
JFLEG	Material
test	Material
set	Material
has	O
747	O
sentences	O
.	O
Being	O
consistent	O
with	O
the	O
official	O
evaluation	Metric
metrics	Metric
,	O
we	O
use	O
MaxMatch	Method
(	O
M	Method
)	O
dahlmeier	Method
-	O
ng:2012:NAACL	O
-	O
HLT	O
for	O
CoNLL	Material
-	Material
2014	Material
and	O
use	O
GLEU	Metric
napoles2015ground	O
for	O
JFLEG	Material
evaluation	O
.	O
It	O
is	O
notable	O
that	O
the	O
original	O
annotations	O
for	O
CoNLL	Material
-	O
2014	O
dataset	O
are	O
from	O
2	O
human	Material
annotators	Material
,	O
which	O
are	O
later	O
enriched	O
by	O
that	O
contains	O
10	O
human	O
expert	O
annotations	O
for	O
each	O
test	O
sentence	O
.	O
We	O
evaluate	O
systems	O
’	O
performance	O
using	O
both	O
annotation	O
settings	O
for	O
the	O
CoNLL	Material
dataset	O
.	O
To	O
distinguish	O
between	O
these	O
two	O
annotation	O
settings	O
,	O
we	O
use	O
CoNLL	Material
-	O
2014	O
to	O
denote	O
the	O
original	O
annotations	O
,	O
and	O
CoNLL	Material
-	O
10	O
to	O
denote	O
the	O
10	Material
-	Material
human	Material
annotations	Material
.	O
As	O
previous	O
studies	O
,	O
we	O
use	O
CoNLL	Material
-	O
2013	O
test	O
set	O
and	O
JFLEG	Material
dev	O
set	O
as	O
our	O
development	O
sets	O
for	O
CoNLL	Material
-	Material
2014	Material
and	O
JFLEG	Material
test	Material
set	Material
respectively	O
.	O
subsection	O
:	O
Experimental	O
setting	O
We	O
use	O
7	Method
-	Method
layer	Method
convolutional	Method
seq2seq	Method
models	Method
gehring2017convolutional	O
as	O
our	O
error	Method
correction	Method
and	Method
error	Method
generation	Method
model	Method
,	O
which	O
have	O
proven	O
to	O
be	O
effective	O
for	O
GEC	Task
chollampatt2018	O
.	O
As	O
,	O
we	O
set	O
the	O
dimensionality	O
of	O
word	O
embeddings	O
in	O
both	O
encoders	O
and	O
decoders	O
to	O
500	O
,	O
the	O
hidden	O
size	O
of	O
encoders	O
and	O
decoders	Method
to	O
1	O
,	O
024	O
and	O
the	O
convolution	O
window	O
width	O
to	O
3	O
.	O
The	O
vocabularies	O
of	O
the	O
source	O
and	O
target	O
side	O
are	O
the	O
most	O
frequent	O
30	O
K	O
BPE	O
tokens	O
for	O
each	O
.	O
We	O
train	O
the	O
seq2seq	Method
models	Method
using	O
Nesterov	Method
Accelerated	Method
Gradient	Method
sutskever2013importance	O
optimizer	O
with	O
a	O
momentum	O
value	O
of	O
0.99	O
.	O
The	O
initial	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.25	O
and	O
it	O
will	O
be	O
reduced	O
by	O
an	O
order	O
of	O
magnitude	O
if	O
the	O
validation	Metric
perplexity	Metric
stops	O
improving	O
.	O
During	O
training	Task
,	O
we	O
allow	O
each	O
batch	O
to	O
have	O
at	O
most	O
3	O
,	O
000	O
tokens	O
per	O
GPU	O
and	O
set	O
dropout	Metric
rate	Metric
to	O
0.2	O
.	O
We	O
terminate	O
the	O
training	O
process	O
when	O
the	O
learning	Metric
rate	Metric
falls	O
below	O
.	O
As	O
and	O
,	O
we	O
train	O
4	O
models	O
with	O
different	O
random	Method
initializations	Method
for	O
ensemble	Task
decoding	Task
.	O
For	O
fluency	Task
boost	Task
learning	Task
,	O
we	O
adopt	O
dual	Method
-	Method
boost	Method
learning	Method
introduced	O
in	O
Section	O
[	O
reference	O
]	O
and	O
use	O
the	O
English	Material
Wikipedia	Material
data	Material
as	O
our	O
native	O
data	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Disfluency	O
candidates	O
are	O
generated	O
from	O
10	O
-	O
best	O
outputs	O
.	O
For	O
fluency	Task
boost	Task
inference	Task
,	O
we	O
use	O
round	Method
-	Method
way	Method
correction	Method
approach	Method
introduced	O
in	O
Section	O
[	O
reference	O
]	O
.	O
The	O
architecture	O
of	O
the	O
right	O
-	O
to	O
-	O
left	O
seq2seq	Method
model	O
in	O
round	Task
-	Task
way	Task
correction	Task
is	O
the	O
same	O
with	O
the	O
left	O
-	O
to	O
-	O
right	O
one	O
except	O
that	O
they	O
decode	O
sentences	O
in	O
the	O
opposite	O
directions	O
.	O
For	O
single	Task
-	Task
round	Task
inference	Task
,	O
we	O
follow	O
to	O
generate	O
12	O
-	O
best	O
predictions	O
and	O
choose	O
the	O
best	O
sentence	O
after	O
re	O
-	O
ranking	O
with	O
edit	O
operation	O
and	O
language	O
model	O
scores	O
.	O
The	O
language	Method
model	Method
is	O
the	O
5	Method
-	Method
gram	Method
language	Method
model	Method
trained	O
on	O
Common	Material
Crawl	Material
released	O
by	O
,	O
which	O
is	O
also	O
used	O
for	O
computing	O
fluency	Metric
score	Metric
in	O
Eq	O
(	O
[	O
reference	O
]	O
)	O
.	O
As	O
most	O
of	O
the	O
systems	O
sakaguchi2017grammatical	O
,	O
chollampatt2018	O
,	O
grundkiewicz2018near	O
evaluated	O
on	O
JFLEG	Material
that	O
use	O
an	O
additional	O
spell	Method
checker	Method
to	O
resolve	O
spelling	O
errors	O
,	O
we	O
use	O
a	O
public	Method
spell	Method
checker	Method
to	O
resolve	O
spelling	O
errors	O
in	O
JFLEG	Material
as	O
preprocessing	Task
.	O
subsection	O
:	O
Experimental	O
results	O
We	O
compare	O
our	O
systems	O
to	O
the	O
following	O
well	O
-	O
known	O
GEC	Task
systems	O
:	O
CAMB14	Method
,	O
CAMB16	Method
and	O
CAMB17	Method
:	O
GEC	Task
systems	O
felice2014grammatical	O
,	O
yuan2016grammatical	Method
,	O
yannakoudakis2017neural	O
developed	O
by	O
Cambridge	O
University	O
.	O
For	O
CAMB17	Method
,	O
we	O
report	O
its	O
best	O
result	O
.	O
CUUI	Method
and	O
VT16	Method
:	O
the	O
former	O
system	O
rozovskaya2014illinois	O
uses	O
a	O
classifier	Method
-	Method
based	Method
approach	O
,	O
which	O
is	O
improved	O
by	O
the	O
latter	O
system	O
rozovskaya2016grammatical	O
through	O
combining	O
it	O
with	O
an	O
SMT	Method
-	Method
based	Method
approach	Method
.	O
AMU14	Method
and	O
AMU16	Method
:	O
SMT	Method
-	Method
based	Method
GEC	Method
systems	Method
junczys2014amu	O
,	O
junczys2016phrase	O
developed	O
by	O
AMU	Method
.	O
NUS14	O
,	O
NUS16	O
,	O
NUS17	O
and	O
NUS18	Method
:	O
The	O
first	O
three	O
GEC	Task
systems	O
Susanto2014System	O
,	O
chollampatt2016adapting	O
,	O
chollampatt	O
-	O
ng:2017:BEA	O
are	O
SMT	Method
-	Method
based	Method
GEC	Method
systems	Method
that	O
are	O
combined	O
with	O
other	O
techniques	O
(	O
e.g.	O
,	O
classifiers	Method
)	O
.	O
The	O
last	O
one	O
chollampatt2018	O
uses	O
convolutional	O
seq2seq	Method
models	O
for	O
grammatical	Task
error	Task
correction	Task
.	O
Nested	O
-	O
RNN	Method
-	O
seq2seq	Method
:	O
a	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
seq2seq	Method
model	Method
with	O
nested	Method
attention	Method
ji2017nested	O
.	O
Back	O
-	O
CNN	O
-	O
seq2seq	Method
:	O
a	Method
convolutional	Method
seq2seq	Method
model	Method
xie2018noising	O
trained	O
with	O
synthesized	Material
data	Material
augmented	O
by	O
back	O
translation	O
.	O
Its	O
core	O
idea	O
is	O
somewhat	O
similar	O
to	O
the	O
idea	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
and	O
Section	O
[	O
reference	O
]	O
of	O
this	O
work	O
.	O
Adapted	O
-	O
transformer	Method
:	O
a	O
transformer	Method
vaswani2017attention	O
based	Method
GEC	Method
system	Method
junczys2018approaching	O
with	O
techniques	O
adapted	O
from	O
low	Task
-	Task
resource	Task
machine	Task
translation	Task
.	O
SMT	Method
-	Method
NMT	Method
hybrid	Method
:	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
GEC	Method
system	Method
grundkiewicz2018near	O
that	O
is	O
based	O
on	O
an	O
SMT	Method
-	Method
NMT	Method
hybrid	Method
approach	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
GEC	Task
systems	O
on	O
CoNLL	Material
and	O
JFLEG	Material
dataset	O
.	O
Our	O
base	O
convolutional	O
seq2seq	Method
model	O
outperforms	O
most	O
of	O
previous	O
GEC	Task
systems	O
owing	O
to	O
the	O
larger	O
size	O
of	O
training	Material
data	Material
we	O
use	O
.	O
Fluency	Method
boost	Method
learning	Method
further	O
improves	O
the	O
base	O
convolutional	O
seq2seq	Method
model	O
.	O
It	O
achieves	O
61.34	O
in	O
CoNLL	Material
-	Material
2014	Material
,	O
76.88	O
score	Metric
in	O
CoNLL	Material
-	O
10	O
benchmarks	O
,	O
and	O
61.41	O
GLEU	Metric
score	O
on	O
JFLEG	Material
test	Material
set	Material
.	O
When	O
we	O
further	O
add	O
fluency	Task
boost	Task
inference	Task
,	O
the	O
system	O
’s	O
performance	O
on	O
JFLEG	Material
test	Material
set	Material
is	O
improved	O
to	O
62.42	O
GLEU	Metric
score	O
,	O
while	O
its	O
scores	O
on	O
CoNLL	Material
benchmarks	O
drop	O
.	O
We	O
look	O
into	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Fluency	Method
boost	Method
learning	Method
improves	O
the	O
base	O
convolutional	O
seq2seq	Method
model	O
in	O
terms	O
of	O
all	O
aspects	O
(	O
i.e.	O
,	O
precision	Metric
,	O
recall	Metric
,	O
and	O
GLEU	Metric
)	O
,	O
demonstrating	O
fluency	Method
boost	Method
learning	Method
is	O
actually	O
helpful	O
for	O
training	O
a	O
seq2seq	Method
model	Method
for	O
GEC	Task
.	O
Adding	O
fluency	Method
boost	Method
inference	Method
improves	O
recall	Metric
(	O
from	O
36.30	O
to	O
40.18	O
on	O
CoNLL	Material
-	O
2014	O
and	O
from	O
50.31	O
to	O
53.15	O
on	O
CoNLL	Material
-	Material
10	Material
)	O
at	O
the	O
expense	O
of	O
a	O
drop	O
of	O
precision	Metric
(	O
from	O
74.12	O
to	O
68.45	O
on	O
CoNLL	Material
-	Material
2014	Material
and	O
from	O
88.56	O
to	O
84.71	O
on	O
CoNLL	Material
-	Material
10	Material
)	O
.	O
Since	O
weighs	O
precision	Metric
twice	O
as	O
recall	Metric
,	O
adding	O
fluency	Method
boost	Method
inference	Method
leads	O
to	O
a	O
drop	O
of	O
on	O
the	O
CoNLL	Material
dataset	O
.	O
In	O
contrast	O
,	O
for	O
JFLEG	Material
,	O
fluency	Method
boost	Method
inference	Method
improves	O
GLEU	Metric
score	O
from	O
61.41	O
to	O
62.42	O
,	O
demonstrating	O
its	O
effectiveness	O
for	O
improving	O
sentences	O
’	O
fluency	O
.	O
We	O
compare	O
our	O
systems	O
to	O
human	O
performance	O
on	O
CoNLL	Material
-	Material
10	Material
and	O
JFLEG	Material
benchmarks	Material
.	O
For	O
CoNLL	Material
-	Material
10	Material
,	O
we	O
follow	O
the	O
evaluation	O
setting	O
in	O
and	O
to	O
fairly	O
compare	O
systems	O
’	O
performance	O
to	O
human	O
’s	O
,	O
which	O
is	O
marked	O
with	O
(	O
SvH	O
)	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Among	O
our	O
systems	O
,	O
the	O
system	O
with	O
fluency	Method
boost	Method
learning	Method
and	O
inference	Method
outperforms	O
human	O
’s	O
performance	O
on	O
both	O
CoNLL	Material
and	O
JFLEG	Material
dataset	Material
,	O
while	O
the	O
system	O
with	O
only	O
fluency	Method
boost	Method
learning	Method
achieves	O
higher	O
scores	O
on	O
CoNLL	Material
dataset	O
.	O
We	O
further	O
study	O
the	O
effectiveness	O
of	O
fluency	Method
boost	Method
learning	Method
and	O
inference	Task
for	O
different	O
error	O
types	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
recall	Metric
of	O
base	O
convolutional	O
seq2seq	Method
model	O
and	O
the	O
model	O
trained	O
with	O
fluency	Method
boost	Method
learning	Method
for	O
each	O
error	O
type	O
in	O
CoNLL	Material
-	O
2014	O
dataset	O
(	O
original	O
annotation	O
setting	O
)	O
.	O
One	O
can	O
see	O
that	O
fluency	Method
boost	Method
learning	Method
improves	O
recall	Metric
for	O
most	O
error	O
types	O
,	O
demonstrating	O
that	O
fluency	Method
boost	Method
learning	Method
approach	Method
can	O
generate	O
sentences	O
with	O
diverse	O
errors	O
to	O
help	O
training	O
.	O
To	O
better	O
understand	O
the	O
effectiveness	O
of	O
fluency	Method
boost	Method
inference	Method
(	O
i.e.	O
,	O
round	Method
-	Method
way	Method
error	Method
correction	Method
)	O
,	O
we	O
show	O
in	O
Table	O
[	O
reference	O
]	O
the	O
recall	Metric
of	O
each	O
error	O
type	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
the	O
right	O
-	O
to	O
-	O
left	O
seq2seq	Method
in	O
CoNLL	Material
-	O
2014	O
dataset	O
(	O
original	O
annotation	O
setting	O
)	O
.	O
Note	O
that	O
to	O
clearly	O
see	O
pros	O
and	O
cons	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
model	O
,	O
here	O
we	O
do	O
not	O
re	O
-	O
rank	O
their	O
n	O
-	O
best	O
results	O
using	O
edit	Method
operations	Method
and	O
the	O
language	Method
model	Method
;	O
instead	O
,	O
we	O
directly	O
use	O
their	O
1	O
-	O
best	O
generated	O
sentence	O
as	O
their	O
prediction	O
.	O
According	O
to	O
Table	O
[	O
reference	O
]	O
,	O
the	O
right	O
-	O
to	O
-	O
left	Method
model	Method
does	O
better	O
in	O
the	O
error	Metric
types	Metric
like	O
ArtOrDet	Method
,	O
while	O
the	O
left	O
-	O
to	O
-	O
right	Method
model	Method
is	O
better	O
at	O
correcting	O
the	O
errors	O
like	O
SVA	Method
,	O
which	O
is	O
consistent	O
with	O
our	O
motivation	O
in	O
Section	O
[	O
reference	O
]	O
.	O
When	O
we	O
use	O
round	Method
-	Method
way	Method
correction	Method
,	O
the	O
errors	O
that	O
are	O
not	O
corrected	O
by	O
the	O
right	O
-	O
to	O
-	O
left	O
model	O
are	O
likely	O
to	O
be	O
corrected	O
by	O
the	O
left	O
-	O
to	O
-	O
right	O
one	O
,	O
which	O
is	O
reflected	O
by	O
the	O
recall	Metric
improvement	O
of	O
most	O
error	O
types	O
,	O
as	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
section	O
:	O
Related	O
work	O
Most	O
of	O
advanced	O
GEC	Task
systems	O
are	O
classifier	Method
-	Method
based	Method
chodorow2007detection	O
,	O
de2008classifier	O
,	O
han2010using	O
,	O
leacock2010automated	O
,	O
tetreault2010using	O
,	O
dale2011helping	O
or	O
MT	Method
-	O
based	O
brockett2006correcting	O
,	O
dahlmeier2011correcting	O
,	O
dahlmeier2012beam	O
,	O
yoshimoto2013naist	O
,	O
yuan2013constrained	O
,	O
behera2013automated	O
.	O
For	O
example	O
,	O
top	O
-	O
performing	O
systems	O
felice2014grammatical	O
,	O
rozovskaya2014illinois	O
,	O
junczys2014amu	O
in	O
CoNLL	Task
-	Task
2014	Task
shared	Task
task	Task
ng2014conll	O
use	O
either	O
of	O
the	O
methods	O
.	O
Recently	O
,	O
many	O
novel	O
approaches	O
Susanto2014System	O
,	O
chollampatt2016neural	O
,	O
chollampatt2016adapting	O
,	O
rozovskaya2016grammatical	O
,	O
junczys2016phrase	O
,	O
mizumoto2016discriminative	O
,	O
Yuan2016Candidate	O
,	O
Hoang2016Exploiting	O
,	O
yannakoudakis2017neural	O
have	O
been	O
proposed	O
for	O
GEC	Task
.	O
Among	O
them	O
,	O
seq2seq	Method
models	Method
yuan2016grammatical	O
,	O
xie2016neural	O
,	O
ji2017nested	O
,	O
sakaguchi2017grammatical	O
,	O
schmaltz	O
-	O
EtAl:2017:EMNLP2017	O
,	O
chollampatt2018	O
,	O
junczys2018approaching	O
have	O
caught	O
much	O
attention	O
.	O
Unlike	O
the	O
models	O
trained	O
only	O
with	O
original	O
error	Material
-	Material
corrected	Material
data	Material
,	O
we	O
propose	O
a	O
novel	O
fluency	Method
boost	Method
learning	Method
mechanism	Method
for	O
dynamic	Task
data	Task
augmentation	Task
along	O
with	O
training	Task
for	O
GEC	Task
,	O
despite	O
some	O
related	O
studies	O
that	O
explore	O
artificial	Task
error	Task
generation	Task
for	O
GEC	Task
brockett2006correcting	O
,	O
foster2009generrate	O
,	O
rozovskaya2010training	O
,	O
rozovskaya2011algorithm	O
,	O
Rozovskaya2012The	O
,	O
felice	O
-	O
yuan:2014:SRW	O
,	O
xie2016neural	O
,	O
rei2017artificial	O
,	O
xie2018noising	O
.	O
Moreover	O
,	O
we	O
propose	O
fluency	Method
boost	Method
inference	Method
which	O
allows	O
the	O
model	O
to	O
repeatedly	O
edit	O
a	O
sentence	O
as	O
long	O
as	O
the	O
sentence	O
’s	O
fluency	O
can	O
be	O
improved	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
it	O
is	O
the	O
first	O
to	O
conduct	O
multi	O
-	O
round	O
seq2seq	Method
inference	O
for	O
GEC	Task
,	O
while	O
similar	O
ideas	O
have	O
been	O
proposed	O
for	O
NMT	Method
DXiaTWLQYL17	Method
.	O
In	O
addition	O
to	O
the	O
studies	O
on	O
GEC	Task
,	O
there	O
is	O
also	O
much	O
research	O
on	O
grammatical	Task
error	Task
detection	Task
leacock2010automated	O
,	O
rei	O
-	O
yannakoudakis:2016:P16	O
-	O
1	O
,	O
kaneko2017grammatical	O
and	O
GEC	Task
evaluation	O
tetreault2010rethinking	O
,	O
madnani2011they	O
,	O
dahlmeier2012better	O
,	O
napoles2015ground	O
,	O
sakaguchi2016reassessing	O
,	O
napoles2016there	O
,	O
bryant2017automatic	O
,	O
asano2017reference	O
,	O
choshen2018inherent	O
.	O
We	O
do	O
not	O
introduce	O
them	O
in	O
detail	O
because	O
they	O
are	O
not	O
much	O
related	O
to	O
this	O
work	O
’s	O
contributions	O
.	O
section	O
:	O
Conclusion	O
We	O
present	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
convolutional	O
seq2seq	Method
model	O
based	O
GEC	Task
system	O
that	O
uses	O
a	O
novel	O
fluency	Method
boost	Method
learning	Method
and	Method
inference	Method
mechanism	Method
.	O
Fluency	Method
boost	Method
learning	Method
fully	O
exploits	O
both	O
error	Material
-	Material
corrected	Material
data	Material
and	O
native	Material
data	Material
by	O
generating	O
diverse	O
error	O
-	O
corrected	O
sentence	O
pairs	O
during	O
training	O
,	O
which	O
benefits	O
model	Method
learning	Method
and	O
improves	O
the	O
performance	O
over	O
the	O
base	O
seq2seq	Method
model	O
,	O
while	O
fluency	Method
boost	Method
inference	Method
utilizes	O
the	O
characteristic	O
of	O
GEC	Task
to	O
progressively	O
improve	O
a	O
sentence	O
’s	O
fluency	O
through	O
round	O
-	O
way	O
correction	O
.	O
The	O
powerful	O
learning	Method
and	Method
inference	Method
mechanism	Method
enables	O
our	O
system	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
and	O
reach	O
human	O
-	O
level	O
performance	O
on	O
both	O
CoNLL	Material
-	Material
2014	Material
and	O
JFLEG	Material
benchmark	Material
datasets	Material
.	O
bibliography	O
:	O
References	O
