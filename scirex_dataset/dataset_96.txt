document O
: O
End Method
- Method
To Method
- Method
End Method
Memory Method
Networks Method
We O
introduce O
a O
neural Method
network Method
with O
a O
recurrent Method
attention Method
model Method
over O
a O
possibly O
large O
external O
memory O
. O
The O
architecture O
is O
a O
form O
of O
Memory Method
Network Method
but O
unlike O
the O
model O
in O
that O
work O
, O
it O
is O
trained O
end O
- O
to O
- O
end O
, O
and O
hence O
requires O
significantly O
less O
supervision O
during O
training Task
, O
making O
it O
more O
generally O
applicable O
in O
realistic O
settings O
. O
It O
can O
also O
be O
seen O
as O
an O
extension O
of O
RNNsearch Method
to O
the O
case O
where O
multiple O
computational O
steps O
( O
hops O
) O
are O
performed O
per O
output O
symbol O
. O
The O
flexibility O
of O
the O
model O
allows O
us O
to O
apply O
it O
to O
tasks O
as O
diverse O
as O
( Task
synthetic Task
) Task
question Task
answering Task
and O
to O
language Task
modeling Task
. O
For O
the O
former O
our O
approach O
is O
competitive O
with O
Memory Method
Networks Method
, O
but O
with O
less O
supervision O
. O
For O
the O
latter O
, O
on O
the O
Penn O
TreeBank O
and O
Text8 O
datasets O
our O
approach O
demonstrates O
comparable O
performance O
to O
RNNs Method
and O
LSTMs Method
. O
In O
both O
cases O
we O
show O
that O
the O
key O
concept O
of O
multiple O
computational O
hops O
yields O
improved O
results O
. O
section O
: O
0pt0.5ex0.3ex O
subsection O
: O
0pt0.2ex0ex O
subsubsection O
: O
0pt0.1ex0ex O
[ O
itemize O
] O
leftmargin= O
* O
tabular O
table O
section O
: O
Introduction O
Two O
grand O
challenges O
in O
artificial Task
intelligence Task
research Task
have O
been O
to O
build O
models O
that O
can O
make O
multiple O
computational O
steps O
in O
the O
service O
of O
answering Task
a Task
question Task
or O
completing O
a O
task O
, O
and O
models O
that O
can O
describe O
long O
term O
dependencies O
in O
sequential O
data O
. O
Recently O
there O
has O
been O
a O
resurgence O
in O
models O
of O
computation Task
using O
explicit Task
storage Task
and O
a O
notion O
of O
attention O
; O
manipulating O
such O
a O
storage O
offers O
an O
approach O
to O
both O
of O
these O
challenges O
. O
In O
, O
the O
storage O
is O
endowed O
with O
a O
continuous Method
representation Method
; O
reads O
from O
and O
writes O
to O
the O
storage O
, O
as O
well O
as O
other O
processing O
steps O
, O
are O
modeled O
by O
the O
actions Method
of Method
neural Method
networks Method
. O
In O
this O
work O
, O
we O
present O
a O
novel O
recurrent Method
neural Method
network Method
( O
RNN Method
) O
architecture O
where O
the O
recurrence Method
reads O
from O
a O
possibly O
large O
external O
memory O
multiple O
times O
before O
outputting O
a O
symbol O
. O
Our O
model O
can O
be O
considered O
a O
continuous O
form O
of O
the O
Memory Method
Network Method
implemented O
in O
. O
The O
model O
in O
that O
work O
was O
not O
easy O
to O
train O
via O
backpropagation Method
, O
and O
required O
supervision O
at O
each O
layer O
of O
the O
network O
. O
The O
continuity O
of O
the O
model O
we O
present O
here O
means O
that O
it O
can O
be O
trained O
end O
- O
to O
- O
end O
from O
input O
- O
output O
pairs O
, O
and O
so O
is O
applicable O
to O
more O
tasks O
, O
i.e. O
tasks O
where O
such O
supervision O
is O
not O
available O
, O
such O
as O
in O
language Task
modeling Task
or O
realistically Task
supervised Task
question Task
answering Task
tasks Task
. O
Our O
model O
can O
also O
be O
seen O
as O
a O
version O
of O
RNNsearch Method
with O
multiple O
computational O
steps O
( O
which O
we O
term O
“ O
hops O
” O
) O
per O
output O
symbol O
. O
We O
will O
show O
experimentally O
that O
the O
multiple O
hops O
over O
the O
long O
- O
term O
memory O
are O
crucial O
to O
good O
performance O
of O
our O
model O
on O
these O
tasks O
, O
and O
that O
training O
the O
memory Method
representation Method
can O
be O
integrated O
in O
a O
scalable O
manner O
into O
our O
end Method
- Method
to Method
- Method
end Method
neural Method
network Method
model Method
. O
section O
: O
Approach O
Our O
model O
takes O
a O
discrete O
set O
of O
inputs O
that O
are O
to O
be O
stored O
in O
the O
memory O
, O
a O
query O
, O
and O
outputs O
an O
answer O
. O
Each O
of O
the O
, O
, O
and O
contains O
symbols O
coming O
from O
a O
dictionary O
with O
words O
. O
The O
model O
writes O
all O
to O
the O
memory O
up O
to O
a O
fixed O
buffer O
size O
, O
and O
then O
finds O
a O
continuous Method
representation Method
for O
the O
and O
. O
The O
continuous Method
representation Method
is O
then O
processed O
via O
multiple O
hops O
to O
output O
. O
This O
allows O
backpropagation O
of O
the O
error Metric
signal O
through O
multiple O
memory O
accesses O
back O
to O
the O
input O
during O
training O
. O
subsection O
: O
Single Method
Layer Method
We O
start O
by O
describing O
our O
model O
in O
the O
single Task
layer Task
case Task
, O
which O
implements O
a O
single Method
memory Method
hop Method
operation Method
. O
We O
then O
show O
it O
can O
be O
stacked O
to O
give O
multiple O
hops O
in O
memory O
. O
Input Method
memory Method
representation Method
: O
Suppose O
we O
are O
given O
an O
input O
set O
to O
be O
stored O
in O
memory O
. O
The O
entire O
set O
of O
are O
converted O
into O
memory O
vectors O
of O
dimension O
computed O
by O
embedding O
each O
in O
a O
continuous O
space O
, O
in O
the O
simplest O
case O
, O
using O
an O
embedding Method
matrix Method
( O
of O
size O
) O
. O
The O
query O
is O
also O
embedded O
( O
again O
, O
in O
the O
simplest O
case O
via O
another O
embedding Method
matrix Method
with O
the O
same O
dimensions O
as O
) O
to O
obtain O
an O
internal O
state O
. O
In O
the O
embedding O
space O
, O
we O
compute O
the O
match O
between O
and O
each O
memory O
by O
taking O
the O
inner Method
product Method
followed O
by O
a O
softmax Method
: O
where O
. O
Defined O
in O
this O
way O
is O
a O
probability O
vector O
over O
the O
inputs O
. O
Output Method
memory Method
representation Method
: O
Each O
has O
a O
corresponding O
output O
vector O
( O
given O
in O
the O
simplest O
case O
by O
another O
embedding Method
matrix Method
) O
. O
The O
response O
vector O
from O
the O
memory O
is O
then O
a O
sum O
over O
the O
transformed O
inputs O
, O
weighted O
by O
the O
probability O
vector O
from O
the O
input O
: O
Because O
the O
function O
from O
input O
to O
output O
is O
smooth O
, O
we O
can O
easily O
compute O
gradients O
and O
back O
- O
propagate O
through O
it O
. O
Other O
recently O
proposed O
forms O
of O
memory Task
or Task
attention Task
take O
this O
approach O
, O
notably O
Bahdanau O
et O
al O
. O
[ O
] O
and O
Graves O
et O
al O
. O
[ O
] O
, O
see O
also O
. O
Generating O
the O
final O
prediction Task
: O
In O
the O
single Task
layer Task
case Task
, O
the O
sum O
of O
the O
output O
vector O
and O
the O
input O
embedding O
is O
then O
passed O
through O
a O
final O
weight O
matrix O
( O
of O
size O
) O
and O
a O
softmax Method
to O
produce O
the O
predicted O
label O
: O
The O
overall O
model O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
( O
a O
) O
. O
During O
training O
, O
all O
three O
embedding O
matrices O
, O
and O
, O
as O
well O
as O
are O
jointly O
learned O
by O
minimizing O
a O
standard O
cross Metric
- Metric
entropy Metric
loss Metric
between O
and O
the O
true O
label O
. O
Training Method
is O
performed O
using O
stochastic Method
gradient Method
descent Method
( O
see O
Section O
[ O
reference O
] O
for O
more O
details O
) O
. O
subsection O
: O
Multiple O
Layers O
We O
now O
extend O
our O
model O
to O
handle O
hop Task
operations Task
. O
The O
memory Method
layers Method
are O
stacked O
in O
the O
following O
way O
: O
The O
input O
to O
layers O
above O
the O
first O
is O
the O
sum O
of O
the O
output O
and O
the O
input O
from O
layer O
( O
different O
ways O
to O
combine O
and O
are O
proposed O
later O
) O
: O
Each O
layer O
has O
its O
own O
embedding O
matrices O
, O
used O
to O
embed O
the O
inputs O
. O
However O
, O
as O
discussed O
below O
, O
they O
are O
constrained O
to O
ease O
training O
and O
reduce O
the O
number O
of O
parameters O
. O
At O
the O
top O
of O
the O
network O
, O
the O
input O
to O
also O
combines O
the O
input O
and O
the O
output O
of O
the O
top O
memory O
layer O
: O
. O
We O
explore O
two O
types O
of O
weight Method
tying Method
within O
the O
model O
: O
Adjacent O
: O
the O
output O
embedding O
for O
one O
layer O
is O
the O
input O
embedding O
for O
the O
one O
above O
, O
i.e. O
. O
We O
also O
constrain O
( O
a O
) O
the O
answer O
prediction O
matrix O
to O
be O
the O
same O
as O
the O
final O
output O
embedding O
, O
i.e O
, O
and O
( O
b O
) O
the O
question O
embedding O
to O
match O
the O
input O
embedding O
of O
the O
first O
layer O
, O
i.e. O
. O
Layer O
- O
wise O
( O
RNN Method
- O
like O
) O
: O
the O
input O
and O
output O
embeddings O
are O
the O
same O
across O
different O
layers O
, O
i.e. O
and O
. O
We O
have O
found O
it O
useful O
to O
add O
a O
linear Method
mapping Method
to O
the O
update O
of O
between O
hops O
; O
that O
is O
, O
. O
This O
mapping O
is O
learnt O
along O
with O
the O
rest O
of O
the O
parameters O
and O
used O
throughout O
our O
experiments O
for O
layer Method
- Method
wise Method
weight O
tying O
. O
A O
three O
- O
layer O
version O
of O
our O
memory Method
model Method
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
( O
b O
) O
. O
Overall O
, O
it O
is O
similar O
to O
the O
Memory Method
Network Method
model O
in O
, O
except O
that O
the O
hard O
max O
operations O
within O
each O
layer O
have O
been O
replaced O
with O
a O
continuous O
weighting O
from O
the O
softmax O
. O
Note O
that O
if O
we O
use O
the O
layer Method
- Method
wise Method
weight O
tying O
scheme O
, O
our O
model O
can O
be O
cast O
as O
a O
traditional O
RNN Method
where O
we O
divide O
the O
outputs O
of O
the O
RNN Method
into O
internal O
and O
external O
outputs O
. O
Emitting O
an O
internal O
output O
corresponds O
to O
considering O
a O
memory O
, O
and O
emitting O
an O
external O
output O
corresponds O
to O
predicting O
a O
label O
. O
From O
the O
RNN Method
point O
of O
view O
, O
in O
Fig O
. O
[ O
reference O
] O
( O
b O
) O
and O
Eqn O
. O
[ O
reference O
] O
is O
a O
hidden O
state O
, O
and O
the O
model O
generates O
an O
internal O
output O
( O
attention O
weights O
in O
Fig O
. O
[ O
reference O
] O
( O
a O
) O
) O
using O
. O
The O
model O
then O
ingests O
using O
, O
updates O
the O
hidden O
state O
, O
and O
so O
on O
. O
Here O
, O
unlike O
a O
standard O
RNN Method
, O
we O
explicitly O
condition O
on O
the O
outputs O
stored O
in O
memory O
during O
the O
hops O
, O
and O
we O
keep O
these O
outputs O
soft O
, O
rather O
than O
sampling O
them O
. O
Thus O
our O
model O
makes O
several O
computational O
steps O
before O
producing O
an O
output O
meant O
to O
be O
seen O
by O
the O
“ O
outside O
world O
” O
. O
section O
: O
Related O
Work O
A O
number O
of O
recent O
efforts O
have O
explored O
ways O
to O
capture O
long O
- O
term O
structure O
within O
sequences O
using O
RNNs Method
or O
LSTM Method
- Method
based Method
models Method
. O
The O
memory O
in O
these O
models O
is O
the O
state O
of O
the O
network O
, O
which O
is O
latent O
and O
inherently O
unstable O
over O
long O
timescales O
. O
The O
LSTM Method
- Method
based Method
models Method
address O
this O
through O
local O
memory O
cells O
which O
lock O
in O
the O
network O
state O
from O
the O
past O
. O
In O
practice O
, O
the O
performance O
gains O
over O
carefully O
trained O
RNNs Method
are O
modest O
( O
see O
Mikolov O
et O
al O
. O
[ O
] O
) O
. O
Our O
model O
differs O
from O
these O
in O
that O
it O
uses O
a O
global O
memory O
, O
with O
shared O
read O
and O
write O
functions O
. O
However O
, O
with O
layer Method
- Method
wise Method
weight O
tying O
our O
model O
can O
be O
viewed O
as O
a O
form O
of O
RNN Method
which O
only O
produces O
an O
output O
after O
a O
fixed O
number O
of O
time O
steps O
( O
corresponding O
to O
the O
number O
of O
hops O
) O
, O
with O
the O
intermediary O
steps O
involving O
memory O
input O
/ O
output O
operations O
that O
update O
the O
internal O
state O
. O
Some O
of O
the O
very O
early O
work O
on O
neural Method
networks Method
by O
Steinbuch O
and O
Piske O
and O
Taylor O
considered O
a O
memory Method
that O
performed O
nearest Method
- Method
neighbor Method
operations Method
on O
stored O
input O
vectors O
and O
then O
fit O
parametric Method
models Method
to O
the O
retrieved O
sets O
. O
This O
has O
similarities O
to O
a O
single O
layer O
version O
of O
our O
model O
. O
Subsequent O
work O
in O
the O
1990 O
’s O
explored O
other O
types O
of O
memory O
. O
For O
example O
, O
Das O
et O
al O
. O
[ O
] O
and O
Mozer O
et O
al O
. O
[ O
] O
introduced O
an O
explicit O
stack O
with O
push Method
and Method
pop Method
operations Method
which O
has O
been O
revisited O
recently O
by O
in O
the O
context O
of O
an O
RNN Method
model O
. O
Closely O
related O
to O
our O
model O
is O
the O
Neural Method
Turing Method
Machine Method
of O
Graves O
et O
al O
. O
[ O
] O
, O
which O
also O
uses O
a O
continuous Method
memory Method
representation Method
. O
The O
NTM Method
memory O
uses O
both O
content Method
and Method
address Method
- Method
based Method
access Method
, O
unlike O
ours O
which O
only O
explicitly O
allows O
the O
former O
, O
although O
the O
temporal O
features O
that O
we O
will O
introduce O
in O
Section O
[ O
reference O
] O
allow O
a O
kind O
of O
address Method
- Method
based Method
access Method
. O
However O
, O
in O
part O
because O
we O
always O
write O
each O
memory O
sequentially O
, O
our O
model O
is O
somewhat O
simpler O
, O
not O
requiring O
operations O
like O
sharpening O
. O
Furthermore O
, O
we O
apply O
our O
memory Method
model Method
to O
textual Task
reasoning Task
tasks Task
, O
which O
qualitatively O
differ O
from O
the O
more O
abstract O
operations O
of O
sorting Task
and O
recall Task
tackled O
by O
the O
NTM Method
. O
Our O
model O
is O
also O
related O
to O
Bahdanau O
et O
al O
. O
[ O
] O
. O
In O
that O
work O
, O
a O
bidirectional O
RNN Method
based O
encoder O
and O
gated O
RNN Method
based O
decoder O
were O
used O
for O
machine Task
translation Task
. O
The O
decoder Method
uses O
an O
attention Method
model Method
that O
finds O
which O
hidden O
states O
from O
the O
encoding O
are O
most O
useful O
for O
outputting O
the O
next O
translated O
word O
; O
the O
attention Method
model Method
uses O
a O
small O
neural Method
network Method
that O
takes O
as O
input O
a O
concatenation O
of O
the O
current O
hidden O
state O
of O
the O
decoder O
and O
each O
of O
the O
encoders O
hidden O
states O
. O
A O
similar O
attention Method
model Method
is O
also O
used O
in O
Xu O
et O
al O
. O
[ O
] O
for O
generating Task
image Task
captions Task
. O
Our O
“ O
memory O
” O
is O
analogous O
to O
their O
attention Method
mechanism Method
, O
although O
is O
only O
over O
a O
single O
sentence O
rather O
than O
many O
, O
as O
in O
our O
case O
. O
Furthermore O
, O
our O
model O
makes O
several O
hops O
on O
the O
memory O
before O
making O
an O
output O
; O
we O
will O
see O
below O
that O
this O
is O
important O
for O
good O
performance O
. O
There O
are O
also O
differences O
in O
the O
architecture O
of O
the O
small Method
network Method
used O
to O
score O
the O
memories O
compared O
to O
our O
scoring Method
approach Method
; O
we O
use O
a O
simple O
linear Method
layer Method
, O
whereas O
they O
use O
a O
more O
sophisticated O
gated Method
architecture Method
. O
We O
will O
apply O
our O
model O
to O
language Task
modeling Task
, O
an O
extensively O
studied O
task O
. O
Goodman O
showed O
simple O
but O
effective O
approaches O
which O
combine O
- O
grams O
with O
a O
cache O
. O
Bengio O
et O
al O
. O
[ O
] O
ignited O
interest O
in O
using O
neural Method
network Method
based O
models O
for O
the O
task O
, O
with O
RNNs Method
and O
LSTMs Method
showing O
clear O
performance O
gains O
over O
traditional O
methods O
. O
Indeed O
, O
the O
current O
state O
- O
of O
- O
the O
- O
art O
is O
held O
by O
variants O
of O
these O
models O
, O
for O
example O
very O
large O
LSTMs Method
with O
Dropout Method
or O
RNNs Method
with O
diagonal O
constraints O
on O
the O
weight O
matrix O
. O
With O
appropriate O
weight Method
tying Method
, O
our O
model O
can O
be O
regarded O
as O
a O
modified O
form O
of O
RNN Method
, O
where O
the O
recurrence O
is O
indexed O
by O
memory O
lookups O
to O
the O
word O
sequence O
rather O
than O
indexed O
by O
the O
sequence O
itself O
. O
section O
: O
Synthetic Task
Question Task
and Task
Answering Task
Experiments Task
We O
perform O
experiments O
on O
the O
synthetic Task
QA Task
tasks Task
defined O
in O
( O
using O
version O
1.1 O
of O
the O
dataset O
) O
. O
A O
given O
QA Task
task Task
consists O
of O
a O
set O
of O
statements O
, O
followed O
by O
a O
question O
whose O
answer O
is O
typically O
a O
single O
word O
( O
in O
a O
few O
tasks O
, O
answers O
are O
a O
set O
of O
words O
) O
. O
The O
answer O
is O
available O
to O
the O
model O
at O
training O
time O
, O
but O
must O
be O
predicted O
at O
test O
time O
. O
There O
are O
a O
total O
of O
20 O
different O
types O
of O
tasks O
that O
probe O
different O
forms O
of O
reasoning Task
and O
deduction Task
. O
Here O
are O
samples O
of O
three O
of O
the O
tasks O
: O
Note O
that O
for O
each O
question O
, O
only O
some O
subset O
of O
the O
statements O
contain O
information O
needed O
for O
the O
answer O
, O
and O
the O
others O
are O
essentially O
irrelevant O
distractors O
( O
e.g. O
the O
first O
sentence O
in O
the O
first O
example O
) O
. O
In O
the O
Memory Method
Networks Method
of O
Weston O
et O
al O
. O
[ O
] O
, O
this O
supporting O
subset O
was O
explicitly O
indicated O
to O
the O
model O
during O
training O
and O
the O
key O
difference O
between O
that O
work O
and O
this O
one O
is O
that O
this O
information O
is O
no O
longer O
provided O
. O
Hence O
, O
the O
model O
must O
deduce O
for O
itself O
at O
training O
and O
test O
time O
which O
sentences O
are O
relevant O
and O
which O
are O
not O
. O
Formally O
, O
for O
one O
of O
the O
20 O
QA Task
tasks Task
, O
we O
are O
given O
example O
problems O
, O
each O
having O
a O
set O
of O
sentences O
where O
; O
a O
question O
sentence O
and O
answer O
. O
Let O
the O
th O
word O
of O
sentence O
be O
, O
represented O
by O
a O
one O
- O
hot O
vector O
of O
length O
( O
where O
the O
vocabulary O
is O
of O
size O
, O
reflecting O
the O
simplistic O
nature O
of O
the O
QA O
language O
) O
. O
The O
same O
representation O
is O
used O
for O
the O
question O
and O
answer O
. O
Two O
versions O
of O
the O
data O
are O
used O
, O
one O
that O
has O
1000 O
training O
problems O
per O
task O
and O
a O
second O
larger O
one O
with O
10 O
, O
000 O
per O
task O
. O
subsection O
: O
Model O
Details O
Unless O
otherwise O
stated O
, O
all O
experiments O
used O
a O
hops Method
model Method
with O
the O
adjacent Method
weight Method
sharing Method
scheme Method
. O
For O
all O
tasks O
that O
output O
lists O
( O
i.e. O
the O
answers O
are O
multiple O
words O
) O
, O
we O
take O
each O
possible O
combination O
of O
possible O
outputs O
and O
record O
them O
as O
a O
separate O
answer O
vocabulary O
word O
. O
Sentence Method
Representation Method
: O
In O
our O
experiments O
we O
explore O
two O
different O
representations O
for O
the O
sentences O
. O
The O
first O
is O
the O
bag Method
- Method
of Method
- Method
words Method
( O
BoW Method
) O
representation O
that O
takes O
the O
sentence O
, O
embeds O
each O
word O
and O
sums O
the O
resulting O
vectors O
: O
e.g O
and O
. O
The O
input O
vector O
representing O
the O
question O
is O
also O
embedded O
as O
a O
bag O
of O
words O
: O
. O
This O
has O
the O
drawback O
that O
it O
can O
not O
capture O
the O
order O
of O
the O
words O
in O
the O
sentence O
, O
which O
is O
important O
for O
some O
tasks O
. O
We O
therefore O
propose O
a O
second O
representation O
that O
encodes O
the O
position O
of O
words O
within O
the O
sentence O
. O
This O
takes O
the O
form O
: O
, O
where O
is O
an O
element Method
- Method
wise Method
multiplication Method
. O
is O
a O
column O
vector O
with O
the O
structure O
( O
assuming O
1 Task
- Task
based Task
indexing Task
) O
, O
with O
being O
the O
number O
of O
words O
in O
the O
sentence O
, O
and O
is O
the O
dimension O
of O
the O
embedding O
. O
This O
sentence Method
representation Method
, O
which O
we O
call O
position Method
encoding Method
( O
PE Method
) O
, O
means O
that O
the O
order O
of O
the O
words O
now O
affects O
. O
The O
same O
representation O
is O
used O
for O
questions O
, O
memory O
inputs O
and O
memory O
outputs O
. O
Temporal Method
Encoding Method
: O
Many O
of O
the O
QA Task
tasks Task
require O
some O
notion O
of O
temporal O
context O
, O
i.e. O
in O
the O
first O
example O
of O
Section O
[ O
reference O
] O
, O
the O
model O
needs O
to O
understand O
that O
Sam O
is O
in O
the O
bedroom O
after O
he O
is O
in O
the O
kitchen O
. O
To O
enable O
our O
model O
to O
address O
them O
, O
we O
modify O
the O
memory O
vector O
so O
that O
, O
where O
is O
the O
th O
row O
of O
a O
special O
matrix O
that O
encodes O
temporal O
information O
. O
The O
output Method
embedding Method
is O
augmented O
in O
the O
same O
way O
with O
a O
matrix O
( O
e.g. O
) O
. O
Both O
and O
are O
learned O
during O
training O
. O
They O
are O
also O
subject O
to O
the O
same O
sharing O
constraints O
as O
and O
. O
Note O
that O
sentences O
are O
indexed O
in O
reverse O
order O
, O
reflecting O
their O
relative O
distance O
from O
the O
question O
so O
that O
is O
the O
last O
sentence O
of O
the O
story O
. O
Learning Task
time Task
invariance Task
by O
injecting O
random O
noise O
: O
we O
have O
found O
it O
helpful O
to O
add O
“ O
dummy O
” O
memories O
to O
regularize O
. O
That O
is O
, O
at O
training O
time O
we O
can O
randomly O
add O
10 O
% O
of O
empty O
memories O
to O
the O
stories O
. O
We O
refer O
to O
this O
approach O
as O
random O
noise O
( O
RN Method
) O
. O
subsection O
: O
Training O
Details O
10 O
% O
of O
the O
bAbI Material
training Material
set Material
was O
held O
- O
out O
to O
form O
a O
validation O
set O
, O
which O
was O
used O
to O
select O
the O
optimal O
model Method
architecture Method
and O
hyperparameters O
. O
Our O
models O
were O
trained O
using O
a O
learning Metric
rate Metric
of O
, O
with O
anneals O
every O
25 O
epochs O
by O
until O
100 O
epochs O
were O
reached O
. O
No O
momentum O
or O
weight O
decay O
was O
used O
. O
The O
weights O
were O
initialized O
randomly O
from O
a O
Gaussian Method
distribution Method
with O
zero Method
mean Method
and O
. O
When O
trained O
on O
all O
tasks O
simultaneously O
with O
1k O
training O
samples O
( O
10k O
training O
samples O
) O
, O
60 O
epochs O
( O
20 O
epochs O
) O
were O
used O
with O
learning Metric
rate Metric
anneals O
of O
every O
15 O
epochs O
( O
5 O
epochs O
) O
. O
All O
training O
uses O
a O
batch O
size O
of O
32 O
( O
but O
cost O
is O
not O
averaged O
over O
a O
batch O
) O
, O
and O
gradients O
with O
an O
norm O
larger O
than O
40 O
are O
divided O
by O
a O
scalar O
to O
have O
norm O
40 O
. O
In O
some O
of O
our O
experiments O
, O
we O
explored O
commencing O
training O
with O
the O
softmax O
in O
each O
memory O
layer O
removed O
, O
making O
the O
model O
entirely O
linear O
except O
for O
the O
final O
softmax O
for O
answer Task
prediction Task
. O
When O
the O
validation Metric
loss Metric
stopped O
decreasing O
, O
the O
softmax O
layers O
were O
re O
- O
inserted O
and O
training O
recommenced O
. O
We O
refer O
to O
this O
as O
linear Method
start Method
( O
LS Method
) O
training O
. O
In O
LS Method
training O
, O
the O
initial O
learning Metric
rate Metric
is O
set O
to O
. O
The O
capacity O
of O
memory O
is O
restricted O
to O
the O
most O
recent O
50 O
sentences O
. O
Since O
the O
number O
of O
sentences O
and O
the O
number O
of O
words O
per O
sentence O
varied O
between O
problems O
, O
a O
null O
symbol O
was O
used O
to O
pad O
them O
all O
to O
a O
fixed O
size O
. O
The O
embedding O
of O
the O
null O
symbol O
was O
constrained O
to O
be O
zero O
. O
On O
some O
tasks O
, O
we O
observed O
a O
large O
variance O
in O
the O
performance O
of O
our O
model O
( O
i.e. O
sometimes O
failing O
badly O
, O
other O
times O
not O
, O
depending O
on O
the O
initialization O
) O
. O
To O
remedy O
this O
, O
we O
repeated O
each O
training O
10 O
times O
with O
different O
random O
initializations O
, O
and O
picked O
the O
one O
with O
the O
lowest O
training Metric
error Metric
. O
subsection O
: O
Baselines O
We O
compare O
our O
approach O
( O
abbreviated O
to O
MemN2N Method
) O
to O
a O
range O
of O
alternate O
models O
: O
MemNN Method
: O
The O
strongly O
supervised O
AM O
+ O
NG O
+ O
NL O
Memory Method
Networks Method
approach O
, O
proposed O
in O
. O
This O
is O
the O
best O
reported O
approach O
in O
that O
paper O
. O
It O
uses O
a O
max Method
operation Method
( O
rather O
than O
softmax Method
) O
at O
each O
layer O
which O
is O
trained O
directly O
with O
supporting O
facts O
( O
strong O
supervision O
) O
. O
It O
employs O
- Method
gram Method
modeling Method
, O
nonlinear Method
layers Method
and O
an O
adaptive O
number O
of O
hops O
per O
query O
. O
MemNN Method
- Method
WSH Method
: O
A O
weakly Method
supervised Method
heuristic Method
version Method
of O
MemNN Method
where O
the O
supporting O
sentence O
labels O
are O
not O
used O
in O
training O
. O
Since O
we O
are O
unable O
to O
backpropagate O
through O
the O
max Method
operations Method
in O
each O
layer O
, O
we O
enforce O
that O
the O
first O
memory O
hop O
should O
share O
at O
least O
one O
word O
with O
the O
question O
, O
and O
that O
the O
second O
memory O
hop O
should O
share O
at O
least O
one O
word O
with O
the O
first O
hop O
and O
at O
least O
one O
word O
with O
the O
answer O
. O
All O
those O
memories O
that O
conform O
are O
called O
valid O
memories O
, O
and O
the O
goal O
during O
training O
is O
to O
rank O
them O
higher O
than O
invalid O
memories O
using O
the O
same O
ranking Metric
criteria Metric
as O
during O
strongly O
supervised Method
training Method
. O
LSTM Method
: O
A O
standard O
LSTM Method
model O
, O
trained O
using O
question O
/ O
answer O
pairs O
only O
( O
i.e. O
also O
weakly O
supervised O
) O
. O
For O
more O
detail O
, O
see O
. O
subsection O
: O
Results O
We O
report O
a O
variety O
of O
design O
choices O
: O
( O
i O
) O
BoW Method
vs O
Position Method
Encoding Method
( O
PE Method
) O
sentence Method
representation Method
; O
( O
ii O
) O
training O
on O
all O
20 O
tasks O
independently O
vs O
jointly O
training O
( O
joint Method
training Method
used O
an O
embedding O
dimension O
of O
, O
while O
independent Method
training Method
used O
) O
; O
( O
iii O
) O
two O
phase Method
training Method
: O
linear Method
start Method
( O
LS Method
) O
where O
softmaxes O
are O
removed O
initially O
vs O
training O
with O
softmaxes O
from O
the O
start O
; O
( O
iv O
) O
varying O
memory O
hops O
from O
1 O
to O
3 O
. O
The O
results O
across O
all O
20 O
tasks O
are O
given O
in O
Table O
[ O
reference O
] O
for O
the O
1k O
training O
set O
, O
along O
with O
the O
mean O
performance O
for O
10k O
training O
set O
. O
They O
show O
a O
number O
of O
interesting O
points O
: O
The O
best O
MemN2N Method
models Method
are O
reasonably O
close O
to O
the O
supervised Method
models Method
( O
e.g. O
1k O
: O
6.7 O
% O
for O
MemNN Method
vs O
12.6 O
% O
for O
MemN2N Method
with O
position Method
encoding Method
+ O
linear Method
start Method
+ O
random O
noise O
, O
jointly O
trained O
and O
10k O
: O
3.2 O
% O
for O
MemNN Method
vs O
4.2 O
% O
for O
MemN2N Method
with O
position Method
encoding Method
+ O
linear Method
start Method
+ O
random O
noise O
+ O
non O
- O
linearity O
, O
although O
the O
supervised Method
models Method
are O
still O
superior O
. O
All O
variants O
of O
our O
proposed O
model O
comfortably O
beat O
the O
weakly Method
supervised Method
baseline Method
methods Method
. O
The O
position Method
encoding Method
( O
PE Method
) O
representation O
improves O
over O
bag Method
- Method
of Method
- Method
words Method
( O
BoW Method
) O
, O
as O
demonstrated O
by O
clear O
improvements O
on O
tasks O
4 O
, O
5 O
, O
15 O
and O
18 O
, O
where O
word Task
ordering Task
is O
particularly O
important O
. O
The O
linear Method
start Method
( O
LS Method
) O
to O
training O
seems O
to O
help O
avoid O
local O
minima O
. O
See O
task O
16 O
in O
Table O
[ O
reference O
] O
, O
where O
PE Method
alone O
gets O
53.6 O
% O
error Metric
, O
while O
using O
LS Method
reduces O
it O
to O
1.6 O
% O
. O
Jittering O
the O
time O
index O
with O
random O
empty O
memories O
( O
RN Method
) O
as O
described O
in O
Section O
[ O
reference O
] O
gives O
a O
small O
but O
consistent O
boost O
in O
performance O
, O
especially O
for O
the O
smaller O
1k O
training O
set O
. O
Joint Task
training Task
on O
all O
tasks O
helps O
. O
Importantly O
, O
more O
computational O
hops O
give O
improved O
performance O
. O
We O
give O
examples O
of O
the O
hops O
performed O
( O
via O
the O
values O
of O
eq O
. O
( O
[ O
reference O
] O
) O
) O
over O
some O
illustrative O
examples O
in O
Fig O
. O
[ O
reference O
] O
and O
in O
Appendix O
[ O
reference O
] O
. O
section O
: O
Language Task
Modeling Task
Experiments O
The O
goal O
in O
language Task
modeling Task
is O
to O
predict O
the O
next O
word O
in O
a O
text O
sequence O
given O
the O
previous O
words O
. O
We O
now O
explain O
how O
our O
model O
can O
easily O
be O
applied O
to O
this O
task O
. O
We O
now O
operate O
on O
word O
level O
, O
as O
opposed O
to O
the O
sentence O
level O
. O
Thus O
the O
previous O
words O
in O
the O
sequence O
( O
including O
the O
current O
) O
are O
embedded O
into O
memory O
separately O
. O
Each O
memory O
cell O
holds O
only O
a O
single O
word O
, O
so O
there O
is O
no O
need O
for O
the O
BoW Method
or O
linear Method
mapping Method
representations O
used O
in O
the O
QA Task
tasks Task
. O
We O
employ O
the O
temporal Method
embedding Method
approach Method
of O
Section O
[ O
reference O
] O
. O
Since O
there O
is O
no O
longer O
any O
question O
, O
in O
Fig O
. O
[ O
reference O
] O
is O
fixed O
to O
a O
constant O
vector O
0.1 O
( O
without O
embedding O
) O
. O
The O
output O
softmax O
predicts O
which O
word O
in O
the O
vocabulary O
( O
of O
size O
) O
is O
next O
in O
the O
sequence O
. O
A O
cross Method
- Method
entropy Method
loss Method
is O
used O
to O
train O
model O
by O
backpropagating O
the O
error Metric
through O
multiple O
memory O
layers O
, O
in O
the O
same O
manner O
as O
the O
QA Task
tasks Task
. O
To O
aid O
training Task
, O
we O
apply O
ReLU Method
operations Method
to O
half O
of O
the O
units O
in O
each O
layer O
. O
We O
use O
layer Method
- Method
wise Method
( O
RNN Method
- Method
like Method
) O
weight Method
sharing Method
, O
i.e. O
the O
query O
weights O
of O
each O
layer O
are O
the O
same O
; O
the O
output O
weights O
of O
each O
layer O
are O
the O
same O
. O
As O
noted O
in O
Section O
[ O
reference O
] O
, O
this O
makes O
our O
architecture O
closely O
related O
to O
an O
RNN Method
which O
is O
traditionally O
used O
for O
language Task
modeling Task
tasks Task
; O
however O
here O
the O
“ O
sequence O
” O
over O
which O
the O
network O
is O
recurrent O
is O
not O
in O
the O
text O
, O
but O
in O
the O
memory O
hops O
. O
Furthermore O
, O
the O
weight Method
tying Method
restricts O
the O
number O
of O
parameters O
in O
the O
model O
, O
helping O
generalization Task
for O
the O
deeper Method
models Method
which O
we O
find O
to O
be O
effective O
for O
this O
task O
. O
We O
use O
two O
different O
datasets O
: O
Penn O
Tree O
Bank O
: O
This O
consists O
of O
929k O
/ O
73k O
/ O
82k O
train O
/ O
validation O
/ O
test O
words O
, O
distributed O
over O
a O
vocabulary O
of O
10k O
words O
. O
The O
same O
preprocessing O
as O
was O
used O
. O
Text8 O
: O
This O
is O
a O
a O
pre O
- O
processed O
version O
of O
the O
first O
100 O
M O
million O
characters O
, O
dumped O
from O
Wikipedia O
. O
This O
is O
split O
into O
93.3M O
/ O
5.7M O
/ O
1 O
M O
character O
train O
/ O
validation O
/ O
test O
sets O
. O
All O
word O
occurring O
less O
than O
5 O
times O
are O
replaced O
with O
the O
UNK O
token O
, O
resulting O
in O
a O
vocabulary Metric
size Metric
of O
44k O
. O
subsection O
: O
Training O
Details O
The O
training O
procedure O
we O
use O
is O
the O
same O
as O
the O
QA Task
tasks Task
, O
except O
for O
the O
following O
. O
For O
each O
mini Task
- Task
batch Task
update Task
, O
the O
norm O
of O
the O
whole O
gradient O
of O
all O
parameters O
is O
measured O
and O
if O
larger O
than O
, O
then O
it O
is O
scaled O
down O
to O
have O
norm O
. O
This O
was O
crucial O
for O
good O
performance O
. O
We O
use O
the O
learning Metric
rate Metric
annealing O
schedule O
from O
, O
namely O
, O
if O
the O
validation Metric
cost Metric
has O
not O
decreased O
after O
one O
epoch O
, O
then O
the O
learning Metric
rate Metric
is O
scaled O
down O
by O
a O
factor O
1.5 O
. O
Training Task
terminates O
when O
the O
learning Metric
rate Metric
drops O
below O
, O
i.e. O
after O
50 O
epochs O
or O
so O
. O
Weights O
are O
initialized O
using O
and O
batch O
size O
is O
set O
to O
128 O
. O
On O
the O
Penn O
tree O
dataset O
, O
we O
repeat O
each O
training O
10 O
times O
with O
different O
random O
initializations O
and O
pick O
the O
one O
with O
smallest Metric
validation Metric
cost Metric
. O
However O
, O
we O
have O
done O
only O
a O
single O
training O
run O
on O
Text8 O
dataset O
due O
to O
limited O
time O
constraints O
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
compares O
our O
model O
to O
RNN Method
, O
LSTM Method
and O
Structurally Method
Constrained Method
Recurrent Method
Nets Method
( O
SCRN Method
) Method
baselines Method
on O
the O
two O
benchmark O
datasets O
. O
Note O
that O
the O
baseline O
architectures O
were O
tuned O
in O
to O
give O
optimal O
perplexity O
. O
Our O
MemN2N Method
approach Method
achieves O
lower O
perplexity Metric
on O
both O
datasets O
( O
111 O
vs O
115 O
for O
RNN Method
/ O
SCRN Method
on O
Penn O
and O
147 O
vs O
154 O
for O
LSTM Method
on O
Text8 O
) O
. O
Note O
that O
MemN2N Method
has O
1.5x O
more O
parameters O
than O
RNNs Method
with O
the O
same O
number O
of O
hidden O
units O
, O
while O
LSTM Method
has O
4x O
more O
parameters O
. O
We O
also O
vary O
the O
number O
of O
hops O
and O
memory O
size O
of O
our O
MemN2N Method
, O
showing O
the O
contribution O
of O
both O
to O
performance O
; O
note O
in O
particular O
that O
increasing O
the O
number O
of O
hops O
helps O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
show O
how O
MemN2N Method
operates O
on O
memory O
with O
multiple O
hops O
. O
It O
shows O
the O
average O
weight O
of O
the O
activation O
of O
each O
memory O
position O
over O
the O
test O
set O
. O
We O
can O
see O
that O
some O
hops O
concentrate O
only O
on O
recent O
words O
, O
while O
other O
hops O
have O
more O
broad O
attention O
over O
all O
memory O
locations O
, O
which O
is O
consistent O
with O
the O
idea O
that O
succesful O
language Method
models Method
consist O
of O
a O
smoothed Method
- Method
gram Method
model Method
and O
a O
cache Method
. O
Interestingly O
, O
it O
seems O
that O
those O
two O
types O
of O
hops O
tend O
to O
alternate O
. O
Also O
note O
that O
unlike O
a O
traditional O
RNN Method
, O
the O
cache Method
does O
not O
decay O
exponentially O
: O
it O
has O
roughly O
the O
same O
average O
activation O
across O
the O
entire O
memory O
. O
This O
may O
be O
the O
source O
of O
the O
observed O
improvement O
in O
language Task
modeling Task
. O
section O
: O
Conclusions O
and O
Future O
Work O
In O
this O
work O
we O
showed O
that O
a O
neural Method
network Method
with O
an O
explicit O
memory O
and O
a O
recurrent Method
attention Method
mechanism Method
for O
reading O
the O
memory O
can O
be O
successfully O
trained O
via O
backpropagation Method
on O
diverse O
tasks O
from O
question Task
answering Task
to O
language Task
modeling Task
. O
Compared O
to O
the O
Memory Method
Network Method
implementation O
of O
there O
is O
no O
supervision O
of O
supporting O
facts O
and O
so O
our O
model O
can O
be O
used O
in O
a O
wider O
range O
of O
settings O
. O
Our O
model O
approaches O
the O
same O
performance O
of O
that O
model O
, O
and O
is O
significantly O
better O
than O
other O
baselines O
with O
the O
same O
level O
of O
supervision O
. O
On O
language Task
modeling Task
tasks Task
, O
it O
slightly O
outperforms O
tuned O
RNNs Method
and O
LSTMs Method
of Method
comparable Method
complexity Method
. O
On O
both O
tasks O
we O
can O
see O
that O
increasing O
the O
number O
of O
memory O
hops O
improves O
performance O
. O
However O
, O
there O
is O
still O
much O
to O
do O
. O
Our O
model O
is O
still O
unable O
to O
exactly O
match O
the O
performance O
of O
the O
memory Method
networks Method
trained O
with O
strong O
supervision O
, O
and O
both O
fail O
on O
several O
of O
the O
1k Task
QA Task
tasks Task
. O
Furthermore O
, O
smooth Method
lookups Method
may O
not O
scale O
well O
to O
the O
case O
where O
a O
larger O
memory O
is O
required O
. O
For O
these O
settings O
, O
we O
plan O
to O
explore O
multiscale O
notions O
of O
attention O
or O
hashing Method
, O
as O
proposed O
in O
. O
section O
: O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
Armand O
Joulin O
, O
Tomas O
Mikolov O
, O
Antoine O
Bordes O
and O
Sumit O
Chopra O
for O
useful O
comments O
and O
valuable O
discussions O
, O
and O
also O
the O
FAIR O
Infrastructure O
team O
for O
their O
help O
and O
support O
. O
bibliography O
: O
References O
section O
: O
Results O
on O
10k O
QA O
dataset O
section O
: O
Visualization Task
of Task
attention Task
weights Task
in O
QA Task
problems Task
