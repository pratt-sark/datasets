document O
: O
Learning O
Feature Method
Pyramids Method
for O
Human Task
Pose Task
Estimation Task
Articulated Task
human Task
pose Task
estimation Task
is O
a O
fundamental O
yet O
challenging O
task O
in O
computer Task
vision Task
. O
The O
difficulty O
is O
particularly O
pronounced O
in O
scale O
variations O
of O
human O
body O
parts O
when O
camera O
view O
changes O
or O
severe O
foreshortening O
happens O
. O
Although O
pyramid Method
methods Method
are O
widely O
used O
to O
handle O
scale O
changes O
at O
inference Task
time Task
, O
learning O
feature Method
pyramids Method
in O
deep Method
convolutional Method
neural Method
networks Method
( O
DCNNs Method
) O
is O
still O
not O
well O
explored O
. O
In O
this O
work O
, O
we O
design O
a O
Pyramid Method
Residual Method
Module Method
( O
PRMs Method
) O
to O
enhance O
the O
invariance O
in O
scales O
of O
DCNNs Method
. O
Given O
input O
features O
, O
the O
PRMs Method
learn O
convolutional Method
filters Method
on O
various O
scales O
of O
input O
features O
, O
which O
are O
obtained O
with O
different O
subsampling O
ratios O
in O
a O
multi Method
- Method
branch Method
network Method
. O
Moreover O
, O
we O
observe O
that O
it O
is O
inappropriate O
to O
adopt O
existing O
methods O
to O
initialize O
the O
weights O
of O
multi Method
- Method
branch Method
networks Method
, O
which O
achieve O
superior O
performance O
than O
plain Method
networks Method
in O
many O
tasks O
recently O
. O
Therefore O
, O
we O
provide O
theoretic O
derivation O
to O
extend O
the O
current O
weight Method
initialization Method
scheme Method
to O
multi Task
- Task
branch Task
network Task
structures Task
. O
We O
investigate O
our O
method O
on O
two O
standard O
benchmarks O
for O
human Task
pose Task
estimation Task
. O
Our O
approach O
obtains O
state O
- O
of O
- O
the O
- O
art O
results O
on O
both O
benchmarks O
. O
Code O
is O
available O
at O
. O
section O
: O
Introduction O
Localizing Task
body Task
parts Task
for O
human O
body O
is O
a O
fundamental O
yet O
challenging O
task O
in O
computer Task
vision Task
, O
and O
it O
serves O
as O
an O
important O
basis O
for O
high Task
- Task
level Task
vision Task
tasks Task
, O
, O
activity Task
recognition Task
, O
clothing Task
parsing Task
, O
human Task
re Task
- Task
identification Task
, O
and O
human Task
- Task
computer Task
interaction Task
. O
Achieving O
accurate Task
localization Task
, O
however O
, O
is O
difficult O
due O
to O
the O
highly O
articulated O
human O
body O
limbs O
, O
occlusion O
, O
change O
of O
viewpoint O
, O
and O
foreshortening O
. O
Significant O
progress O
on O
human Task
pose Task
estimation Task
has O
been O
achieved O
by O
deep Method
convolutional Method
neural Method
networks Method
( O
DCNNs Method
) O
. O
In O
these O
methods O
, O
the O
DCNNs Method
learn O
body Method
part Method
detectors Method
from O
images O
warped O
to O
the O
similar O
scale O
based O
on O
human O
body O
size O
. O
At O
inference O
time O
, O
testing O
images O
should O
also O
be O
warped O
to O
the O
same O
scale O
as O
that O
for O
training O
images O
. O
Although O
the O
right O
scale O
of O
the O
full O
human O
body O
is O
provided O
, O
scales O
for O
body O
parts O
may O
still O
be O
inconsistent O
due O
to O
inter O
- O
personal O
body O
shape O
variations O
and O
foreshortening O
caused O
by O
viewpoint O
change O
and O
body O
articulation O
. O
It O
results O
in O
difficulty O
for O
body Method
part Method
detectors Method
to O
localize O
body O
parts O
. O
For O
example O
, O
severe O
foreshortening O
is O
present O
in O
Figure O
[ O
reference O
] O
. O
When O
the O
images O
are O
warped O
to O
the O
same O
size O
according O
to O
human O
body O
scale O
, O
the O
hand O
in O
Figure O
[ O
reference O
] O
( O
a O
) O
has O
a O
larger O
scale O
than O
that O
in O
Figure O
[ O
reference O
] O
( O
b O
) O
. O
Therefore O
, O
the O
hand Method
detector Method
that O
can O
detect O
the O
hand O
in O
Figure O
[ O
reference O
] O
( O
a O
) O
might O
not O
be O
able O
to O
detect O
the O
hand O
in O
Figure O
[ O
reference O
] O
( O
b O
) O
reliably O
. O
In O
DCNNs Method
, O
this O
problem O
from O
scale O
change O
happens O
not O
only O
for O
high O
- O
level O
semantics O
in O
deeper O
layers O
, O
but O
also O
exists O
for O
low O
- O
level O
features O
in O
shallower O
layers O
. O
To O
enhance O
the O
robustness Metric
of O
DCNNs O
against O
scale O
variations O
of O
visual O
patterns O
, O
we O
design O
a O
Pyramid Method
Residual Method
Module Method
to O
explicitly O
learn O
convolutional Method
filters Method
for O
building O
feature Method
pyramids Method
. O
Given O
input O
features O
, O
the O
Pyramid Method
Residual Method
Module Method
obtains O
features O
of O
different O
scales O
via O
subsampling O
with O
different O
ratios O
. O
Then O
convolution Method
is O
used O
to O
learn O
filters O
for O
features O
in O
different O
scales O
. O
The O
filtered O
features O
are O
upsampled O
to O
the O
same O
resolution O
and O
are O
summed O
together O
for O
the O
following O
processing O
. O
This O
Pyramid Method
Residual Method
Module Method
can O
be O
used O
as O
building O
blocks O
in O
DCNNs Method
for O
learning O
feature Method
pyramids Method
at O
different O
levels O
of O
the O
network O
. O
There O
is O
a O
trend O
of O
designing O
networks Method
with O
branches Method
, O
, O
Inception Method
models Method
and O
ResNets Method
for O
classification Task
, O
ASPP Method
- Method
nets Method
for O
semantic Task
segmentation Task
, O
convolutional Method
pose Method
machines Method
and O
stacked Method
hourglass Method
networks Method
for O
human Task
pose Task
estimation Task
, O
in O
which O
the O
input O
of O
a O
layer O
is O
from O
multiple O
other O
layers O
or O
the O
output O
of O
a O
layer O
is O
used O
by O
many O
other O
layers O
. O
Our O
pyramid O
residual Method
module Method
also O
has O
branches O
. O
We O
observe O
that O
the O
existing O
weight Method
initialization Method
scheme Method
, O
, O
MSR O
and O
Xavier Method
methods O
, O
are O
not O
proper O
for O
layers O
with O
branches O
. O
Therefore O
, O
we O
extend O
the O
current O
weight Method
initialization Method
scheme Method
and O
provide O
theoretic O
derivation O
to O
show O
that O
the O
initialization O
of O
network O
parameters O
should O
take O
the O
number O
of O
branches O
into O
consideration O
. O
We O
also O
show O
another O
issue O
in O
the O
residual Method
unit Method
, O
where O
the O
variance O
of O
output O
of O
the O
residual Method
unit Method
accumulates O
as O
the O
depth O
increases O
. O
The O
problem O
is O
caused O
by O
the O
identity Task
mapping Task
. O
Since O
Hourglass Method
network Method
, O
also O
called O
conv Method
- Method
deconv Method
structure Method
, O
is O
an O
effective O
structure O
for O
pose Task
estimation Task
, O
object Task
detection Task
, O
and O
pixel Task
level Task
tasks Task
, O
we O
use O
it O
as O
the O
basic O
structure O
in O
experiments O
. O
We O
observe O
a O
problem O
of O
using O
residual Method
unit Method
for O
Hourglass Method
: O
when O
outputs O
of O
two O
residual Method
units Method
are O
summed O
up O
, O
the O
output O
variance O
is O
approximately O
doubled O
, O
which O
causes O
difficulty O
in O
optimization Task
. O
We O
propose O
a O
simple O
but O
efficient O
way O
with O
negligible O
additional O
parameters O
to O
solve O
this O
problem O
. O
The O
main O
contributions O
are O
three O
folds O
: O
We O
propose O
a O
Pyramid Method
Residual Method
Module Method
, O
which O
enhances O
the O
invariance O
in O
scales O
of O
deep Method
models Method
by O
learning O
feature Method
pyramids Method
in O
DCNNs Method
with O
only O
a O
small O
increase O
of O
complexity Metric
. O
We O
identify O
the O
problem O
for O
initializing O
DCNNs Method
including Method
layers Method
with O
multiple O
input O
or O
output O
branches O
. O
A O
weight Method
initialization Method
scheme Method
is O
then O
provided O
, O
which O
can O
be O
used O
for O
many O
network Method
structures Method
including O
inception Method
models Method
and O
ResNets Method
. O
We O
observe O
that O
the O
problem O
of O
activation Task
variance Task
accumulation Task
introduced O
by O
identity Method
mapping Method
may O
be O
harmful O
in O
some O
scenarios O
, O
, O
adding O
outputs O
of O
multiple O
residual Method
units Method
implemented O
by O
identity Method
mapping Method
together O
in O
the O
Hourglass Method
structure Method
. O
A O
simple O
yet O
effective O
solution O
is O
introduced O
for O
solving O
this O
issue O
. O
We O
evaluate O
the O
proposed O
method O
on O
two O
popular O
human Task
pose Task
estimation Task
benchmarks Task
, O
and O
report O
state O
- O
of O
- O
the O
- O
art O
results O
. O
We O
also O
demonstrate O
the O
generalization Metric
ability Metric
of O
our O
approach O
on O
standard O
image Task
classification Task
task Task
. O
Ablation O
study O
demonstrates O
the O
effectiveness O
of O
the O
pyramid O
residual Method
module Method
, O
the O
new O
initialization Method
scheme Method
, O
and O
the O
approach O
in O
handling O
drastic O
activation O
variance O
increase O
caused O
by O
adding O
residual Method
units Method
. O
section O
: O
Related O
Work O
Human Task
pose Task
estimation Task
. O
Graph O
structures O
, O
, O
Pictorial O
structures O
and O
loopy O
structures O
, O
have O
been O
broadly O
used O
to O
model O
the O
spatial O
relationships O
among O
body O
parts O
. O
All O
these O
methods O
were O
built O
on O
hand O
- O
crafted O
features O
such O
as O
HOG O
feature O
, O
and O
their O
performances O
relied O
heavily O
on O
image O
pyramid O
. O
Recently O
, O
deep Method
models Method
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
in O
human Task
pose Task
estimation Task
. O
Among O
them O
, O
DeepPose Method
is O
one O
of O
the O
first O
attempts O
on O
using O
DCNNs Method
for O
human Task
pose Task
estimation Task
. O
It O
regressed O
the O
coordinates O
of O
body O
parts O
directly O
, O
which O
suffered O
from O
the O
problem O
that O
image O
- O
to O
- O
locations O
is O
a O
difficult O
mapping O
to O
learn O
. O
Therefore O
, O
later O
methods O
modeled O
part O
locations O
as O
Gaussian O
peaks O
in O
score O
maps O
, O
and O
predicted O
the O
score Method
maps Method
with O
fully Method
convolutional Method
networks Method
. O
In O
order O
to O
achieve O
higher O
accuracy Metric
, O
multi Task
- Task
scale Task
testing Task
on O
image O
pyramids O
was O
often O
utilized O
, O
which O
produced O
a O
multi Method
- Method
scale Method
feature Method
representation Method
. O
Our O
method O
is O
a O
complementary O
to O
image Task
pyramids Task
. O
On O
the O
other O
hand O
, O
to O
learn O
a O
model O
with O
strong O
scale O
invariance O
, O
a O
multi Method
- Method
branch Method
network Method
trained O
on O
three O
scales O
of O
image O
pyramid O
was O
proposed O
in O
. O
However O
, O
when O
image O
pyramids O
are O
used O
for O
training Task
, O
computation Task
and O
memory O
linearly O
increases O
with O
the O
number O
of O
scales O
. O
In O
comparison O
, O
our O
pyramid O
residual Method
module Method
provides O
an O
efficient O
way O
of O
learning Task
multi Task
- Task
scale Task
features Task
, O
with O
relatively O
small O
cost O
in O
computation Metric
and O
memory Metric
. O
DCNNs Method
combining Method
multiple Method
layers Method
. O
In O
contrast O
to O
traditional O
plain Method
networks Method
( O
, O
AlexNet Method
and Method
VGG Method
- Method
nets Method
) O
, O
multi Method
- Method
branch Method
networks Method
exhibit O
better O
performance O
on O
various O
vision Task
tasks Task
. O
In O
classification Task
, O
the O
inception Method
models Method
are O
one O
of O
the O
most O
successful O
multi Method
- Method
branch Method
networks Method
. O
The O
input O
of O
each O
module O
is O
first O
mapped O
to O
low O
dimension O
by O
convolutions Method
, O
then O
transformed O
by O
a O
set O
of O
filters Method
with O
different O
sizes O
to O
capture O
various O
context O
information O
and O
combined O
by O
concatenation O
. O
ResNet Method
can O
be O
regarded O
as O
a O
two Method
- Method
branch Method
networks Method
with O
one O
identity O
mapping O
branch O
. O
ResNeXt Method
is O
an O
extension O
of O
ResNet Method
, O
in O
which O
all O
branches O
share O
the O
same O
topology O
. O
The O
implicitly O
learned O
transforms O
are O
aggregated O
by O
summation Method
. O
In O
our O
work O
, O
we O
use O
multi Method
- Method
branch Method
network Method
to O
explore O
another O
possibility O
: O
to O
learn O
multi O
- O
scale O
features O
. O
Recent O
methods O
in O
pose Task
estimation Task
, O
object Task
detection Task
and O
segmentation Task
used O
features O
from O
multiple O
layers O
for O
making O
predictions Task
. O
Our O
approach O
is O
complementary O
to O
these O
works O
. O
For O
example O
, O
we O
adopt O
Hourglass Method
as O
our O
basic O
structure O
, O
and O
replace O
its O
original O
residual Method
units Method
, O
which O
learn O
features O
from O
a O
single O
scale O
, O
with O
the O
proposed O
Pyramid Method
Residual Method
Module Method
. O
Weight Method
initialization Method
. O
Good O
initialization Task
is O
essential O
for O
training O
deep Method
models Method
. O
Hinton O
and O
Salakhutdinov O
adopted O
the O
layer Method
- Method
by Method
- Method
layer Method
pretraining Method
strategy Method
to O
train O
a O
deep Method
autoencoder Method
. O
Krizhevsky O
initialized O
the O
weight O
of O
each O
layer O
by O
drawing O
samples O
from O
a O
Gaussian Method
distribution Method
with O
zero O
mean O
and O
0.01 O
standard O
deviation O
. O
However O
, O
it O
has O
difficulty O
in O
training O
very O
deep Task
networks Task
due O
to O
the O
instability O
of O
gradients O
. O
Xavier Method
initialization O
has O
provided O
a O
theoretically O
sound O
estimation O
of O
the O
variance Metric
of Metric
weight Metric
. O
It O
assumes O
that O
the O
weights O
are O
initialized O
close O
to O
zero O
, O
hence O
the O
nonlinear O
activations O
like O
Sigmoid Method
and O
Tanh Method
can O
be O
regarded O
as O
linear O
functions O
. O
This O
assumption O
does O
not O
hold O
for O
rectifier O
activations O
. O
Thus O
He O
proposed O
an O
initialization Method
scheme Method
for O
rectifier Task
networks Task
based O
on O
. O
All O
the O
above O
initialization Method
methods Method
, O
however O
, O
are O
derived O
for O
plain Method
networks Method
with O
only O
one O
branch O
. O
We O
identify O
the O
problem O
of O
the O
initialization Method
methods Method
when O
applied O
for O
multi Task
- Task
branch Task
networks Task
. O
An O
initialization Method
scheme Method
for O
networks O
with O
multiple O
branches O
is O
provided O
to O
handle O
this O
problem O
. O
section O
: O
Framework O
An O
overview O
of O
the O
proposed O
framework O
is O
illustrated O
in O
Figure O
. O
[ O
reference O
] O
. O
We O
adopt O
the O
highly O
modularized Method
stacked Method
Hourglass Method
Network Method
as O
the O
basic O
network Method
structure Method
to O
investigate O
feature Method
pyramid Method
learning Method
for O
human Task
pose Task
estimation Task
. O
The O
building O
block O
of O
our O
network O
is O
the O
proposed O
Pyramid Method
Residual Method
Module Method
( O
PRM Method
) O
. O
We O
first O
briefly O
review O
the O
structure O
of O
hourglass Method
network Method
. O
Then O
a O
detailed O
discussion O
of O
our O
pyramid O
residual Method
module Method
is O
presented O
. O
subsection O
: O
Revisiting O
Stacked Method
Hourglass Method
Network Method
Hourglass Method
network Method
aims O
at O
capturing O
information O
at O
every O
scale O
in O
feed O
- O
forward O
fashion O
. O
It O
first O
performs O
bottom Method
- Method
up Method
processing Method
by O
subsampling O
the O
feature O
maps O
, O
and O
conducts O
top Method
- Method
down Method
processing Method
by O
upsampling O
the O
feature O
maps O
with O
the O
comination O
of O
higher O
resolution O
features O
from O
bottom O
layers O
, O
as O
demonstrated O
in O
Figure O
. O
[ O
reference O
] O
( O
b O
) O
. O
This O
bottom O
- O
up O
, O
top Method
- Method
down Method
processing Method
is O
repeated O
for O
several O
times O
to O
build O
a O
“ O
stacked Method
hourglass Method
” Method
network Method
, O
with O
intermediate O
supervision O
at O
the O
end O
of O
each O
stack O
. O
In O
, O
residual Method
unit Method
is O
used O
as O
the O
building O
block O
of O
the O
hourglass Method
network Method
. O
However O
, O
it O
can O
only O
capture O
visual O
patterns O
or O
semantics O
at O
one O
scale O
. O
In O
this O
work O
, O
we O
use O
the O
proposed O
pyramid O
residual Method
module Method
as O
the O
building Method
block Method
for O
capturing O
multi Task
- Task
scale Task
visual Task
patterns Task
or Task
semantics Task
. O
subsection O
: O
Pyramid Method
Residual Method
Modules Method
( O
PRMs Method
) O
The O
objective O
is O
to O
learn O
feature Method
pyramids Method
across O
different O
levels O
of O
DCNNs Method
. O
It O
allows O
the O
network O
to O
capture O
feature Method
pyramids Method
from O
primitive O
visual O
patterns O
to O
high O
- O
level O
semantics O
. O
Motivated O
by O
recent O
progress O
on O
residual Method
learning Method
, O
we O
propose O
a O
novel O
Pyramid Method
Residual Method
Module Method
( O
PRM Method
) Method
, O
which O
is O
able O
to O
learn O
multi O
- O
scale O
feature Method
pyramids Method
. O
The O
PRM Method
explicitly O
learns O
filters O
for O
input O
features O
with O
different O
resolutions O
. O
Let O
and O
be O
the O
input O
and O
the O
filter O
of O
the O
- Method
th Method
layer Method
, O
respectively O
. O
The O
PRM Method
can O
be O
formulated O
as O
, O
where O
is O
feature Method
pyramids Method
decomposed O
as O
: O
The O
in O
( O
[ O
reference O
] O
) O
denotes O
the O
number O
of O
pyramid O
levels O
, O
is O
the O
transformation O
for O
the O
- O
th O
pyramid O
level O
, O
and O
is O
the O
set O
of O
parameters O
. O
Outputs O
of O
transformations O
are O
summed O
up O
together O
, O
and O
further O
convolved O
by O
filters Method
. O
An O
illustration O
of O
the O
pyramid O
residual Method
module Method
is O
illustrated O
in O
Figure O
. O
[ O
reference O
] O
. O
To O
reduce O
the O
computational Metric
and Metric
space Metric
complexity Metric
, O
each O
is O
designed O
as O
a O
bottleneck Method
structure Method
. O
For O
example O
, O
in O
Figure O
. O
[ O
reference O
] O
, O
the O
feature O
dimension O
is O
reduced O
by O
a O
convolution Method
, O
then O
new O
features O
are O
computed O
on O
a O
set O
of O
subsampled O
input O
features O
by O
convolutions Method
. O
Finally O
, O
all O
the O
new O
features O
are O
upsampled O
to O
the O
same O
dimension O
and O
are O
summed O
together O
. O
Generation O
of O
input O
feature Method
pyramids Method
. O
Max Method
- Method
pooling Method
or O
average Method
- Method
pooling Method
are O
widely O
used O
in O
DCNNs Method
to O
reduce O
the O
resolution O
of O
feature O
maps O
, O
and O
to O
encode O
the O
translation O
invariance O
. O
But O
pooling Method
reduces O
the O
resolution O
too O
fast O
and O
coarse O
by O
a O
factor O
of O
an O
integer O
of O
at O
least O
two O
, O
which O
is O
unable O
to O
generate O
pyramids O
gently O
. O
In O
order O
to O
obtain O
input O
feature O
maps O
of O
different O
resolutions O
, O
we O
adopt O
the O
fractional Method
max Method
- Method
pooling Method
to O
approximate O
the O
smoothing Method
and Method
subsampling Method
process Method
used O
in O
generating O
traditional Task
image Task
pyramids Task
. O
The O
subsampling O
ratio O
of O
the O
th O
level O
pyramid O
is O
computed O
as O
: O
where O
denotes O
the O
relative O
resolution O
compared O
with O
the O
input O
features O
. O
For O
example O
, O
when O
, O
the O
output O
has O
the O
same O
resolution O
as O
its O
input O
. O
When O
, O
the O
map O
has O
half O
resolution O
of O
its O
input O
. O
In O
experiments O
, O
we O
set O
and O
, O
with O
which O
the O
lowest O
scale O
in O
pyramid O
is O
half O
the O
resolution O
of O
its O
input O
. O
subsection O
: O
Discussions O
PRM Method
for O
general Method
CNNs Method
. O
Our O
PRM Method
is O
a O
general O
module O
and O
can O
be O
used O
as O
the O
basic O
building O
block O
for O
various O
CNN Method
architectures Method
, O
, O
stacked Method
hourglass Method
networks Method
for O
pose Task
estimation Task
, O
and O
Wide Method
Residual Method
Nets Method
and O
ResNeXt Method
for O
image Task
classification Task
, O
as O
demonstrated O
in O
experiments O
. O
Variants O
in O
pyramid O
structure O
. O
Besides O
using O
fractional Method
max Method
- Method
pooling Method
, O
convolution Method
and O
upsampling Method
to O
learn O
feature Method
pyramids Method
, O
as O
illustrated O
in O
Figure O
. O
[ O
reference O
] O
( O
a O
- O
b O
) O
, O
one O
can O
also O
use O
dilated Method
convolution Method
to O
compute O
pyramids O
, O
as O
shown O
in O
Figure O
. O
[ O
reference O
] O
( O
c O
)( O
PRM O
- O
D Method
) O
. O
The O
summation O
of O
features O
in O
pyramid O
can O
also O
replaced O
by O
concatenation O
, O
as O
shown O
in O
Figure O
. O
[ O
reference O
] O
( O
b O
)( O
PRM O
- O
C Method
) O
. O
We O
discuss O
the O
performance O
of O
these O
variants O
in O
experiments O
, O
and O
show O
that O
the O
design O
in O
Figure O
. O
[ O
reference O
] O
( O
b O
)( O
PRM O
- O
B Method
) O
has O
comparable O
performance O
with O
others O
, O
while O
maintains O
relatively O
fewer O
parameters O
and O
smaller O
computational Metric
complexity Metric
. O
Weight Task
sharing Task
. O
To O
generate O
the O
feature Method
pyramids Method
, O
traditional O
methods O
usually O
apply O
a O
same O
handcrafted Method
filter Method
, O
, O
HOG Method
, O
on O
different O
levels O
of O
image O
pyramids O
. O
This O
process O
corresponds O
to O
sharing O
the O
weights O
across O
different O
levels O
of O
pyramid O
, O
which O
is O
able O
to O
greatly O
reduce O
the O
number O
of O
parameters O
. O
Complexity Metric
. O
The O
residual Method
unit Method
used O
in O
has O
256 O
- O
d O
input O
and O
output O
, O
which O
are O
reduced O
to O
128 O
- O
d O
within O
the O
residual Method
unit Method
. O
We O
adopt O
this O
structure O
for O
the O
branch O
with O
original O
scale O
( O
, O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
. O
Since O
features O
with O
smaller O
resolution O
contain O
relatively O
fewer O
information O
, O
we O
use O
fewer O
feature O
channels O
for O
branches O
with O
smaller O
scales O
. O
For O
example O
, O
given O
a O
PRM Method
with O
five O
branches O
and O
28 O
feature O
channels O
for O
branches O
with O
smaller O
scale O
( O
, O
to O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
, O
the O
increased O
complexity Metric
is O
about O
only O
compared O
with O
residual Method
unit Method
in O
terms O
of O
both O
parameters O
and O
GFLOPs Method
. O
section O
: O
Training O
and O
Inference Task
We O
use O
score O
maps O
to O
represent O
the O
body O
joint O
locations O
. O
Denote O
the O
ground O
- O
truth O
locations O
by O
, O
where O
denotes O
the O
location O
of O
the O
th O
body O
joint O
in O
the O
image O
. O
Then O
the O
ground O
- O
truth O
score O
map O
is O
generated O
from O
a O
Gaussian Method
with O
mean O
and O
variance O
as O
follows O
, O
where O
denotes O
the O
location O
, O
and O
is O
empirically O
set O
as O
an O
identity O
matrix O
I O
. O
Each O
stack O
of O
hourglass Method
network Method
predicts O
score Method
maps Method
, O
, O
for O
body O
joints O
. O
A O
loss O
is O
attached O
at O
the O
end O
of O
each O
stack O
defined O
by O
the O
squared Metric
error Metric
where O
is O
the O
number O
of O
samples O
. O
During O
inference Task
, O
we O
obtain O
the O
predicted O
body O
joint O
locations O
from O
the O
predicted O
score O
maps O
generated O
from O
the O
last O
stack O
of O
hourglass O
by O
taking O
the O
locations O
with O
the O
maximum O
score O
as O
follows O
: O
subsection O
: O
Initialization Task
Multi Task
- Task
Branch Task
Networks Task
Initialization Task
is O
essential O
to O
train O
very O
deep Method
networks Method
, O
especially O
for O
tasks O
of O
dense Task
prediction Task
, O
where O
Batch Task
Normalization Task
is O
less O
effective O
because O
of O
the O
small O
minibatch O
due O
to O
the O
large O
memory O
consumption O
of O
fully Method
convolutional Method
networks Method
. O
Existing O
weight Method
initialization Method
methods Method
are O
designed O
upon O
the O
assumption O
of O
a O
plain Method
networks Method
without O
branches O
. O
The O
proposed O
PRM Method
has O
multiple O
branches O
, O
and O
does O
not O
meet O
the O
assumption O
. O
Recent O
developed O
architectures O
with O
multiple O
branches O
, O
, O
Inception Method
models Method
and O
ResNets Method
, O
are O
not O
plain Method
network Method
either O
. O
Hence O
we O
discuss O
how O
to O
derive O
a O
proper O
initialization Method
for O
networks Task
adding O
multiple O
branches O
. O
Our O
derivation O
mainly O
follows O
. O
Forward Method
propagation Method
. O
Generally O
, O
multi Method
- Method
branch Method
networks Method
can O
be O
characterized O
by O
the O
number O
of O
input O
and O
output O
branches O
. O
Figure O
. O
[ O
reference O
] O
( O
a O
) O
shows O
an O
example O
where O
the O
th Method
layer Method
has O
input O
branches O
and O
one O
output O
branch O
. O
Figure O
. O
[ O
reference O
] O
( O
b O
) O
shows O
an O
example O
where O
the O
th Method
layer Method
has O
one O
input O
branch O
and O
output O
branches O
. O
During O
forward Method
propagation Method
, O
affects O
the O
variance O
for O
the O
output O
of O
the O
th O
layer O
while O
does O
not O
. O
At O
the O
th O
layer O
, O
assume O
there O
are O
input O
branches O
and O
output O
branches O
. O
There O
are O
input O
vectors O
. O
Take O
fully Method
- Method
connected Method
layer Method
for O
example O
, O
a O
response O
is O
computed O
as O
: O
where O
is O
the O
non O
- O
linear O
activation O
function O
. O
As O
in O
, O
we O
assume O
that O
and O
are O
both O
independent O
and O
identically O
distributed O
( O
i.i.d O
. O
) O
, O
and O
they O
are O
independent O
of O
each O
other O
. O
Therefore O
, O
we O
respectively O
denote O
and O
as O
the O
element O
in O
and O
. O
Then O
we O
have O
, O
where O
is O
the O
number O
of O
elements O
in O
for O
. O
Suppose O
has O
zero O
mean O
. O
The O
variance O
for O
the O
product O
of O
independent O
variables O
above O
is O
as O
follows O
: O
where O
depends O
on O
the O
activation O
function O
in O
( O
[ O
reference O
] O
) O
. O
for O
ReLU Method
and O
for O
Tanh Method
and Method
Sigmoid Method
. O
In O
order O
to O
make O
the O
variances O
of O
the O
output O
approximately O
the O
same O
for O
different O
layers O
, O
the O
following O
condition O
should O
be O
satisfied O
: O
Hence O
in O
initialization Task
, O
a O
proper O
variance O
for O
should O
be O
. O
Backward Method
propagation Method
. O
Denote O
and O
by O
and O
respectively O
. O
During O
backward Method
propagation Method
, O
the O
gradient O
is O
computed O
by O
chain Method
rule Method
, O
Suppose O
and O
are O
i.i.d O
. O
and O
independent O
of O
each O
other O
, O
then O
has O
zero O
mean O
when O
is O
initialized O
with O
zero O
mean O
and O
symmetric O
with O
small O
magnitude O
. O
Let O
denote O
the O
number O
of O
output O
neurons O
. O
Then O
we O
have O
, O
Denote O
. O
for O
ReLU Method
and O
for O
Tanh Method
and Method
Sigmoid Method
. O
We O
further O
assume O
that O
and O
are O
independent O
of O
each O
other O
, O
then O
from O
Eq O
. O
( O
[ O
reference O
] O
) O
, O
we O
have O
Then O
we O
can O
derive O
that O
. O
Therefore O
, O
from O
Eq O
. O
( O
[ O
reference O
] O
) O
we O
have O
, O
To O
ensure O
, O
we O
must O
have O
. O
In O
many O
cases O
, O
. O
As O
in O
, O
a O
compromise O
between O
the O
forward O
and O
backward O
constraints O
is O
to O
have O
, O
Special O
case O
. O
For O
plain Method
networks Method
with O
one O
input O
and O
one O
output O
branch O
, O
we O
have O
in O
( O
[ O
reference O
] O
) O
. O
In O
this O
case O
, O
the O
result O
in O
( O
[ O
reference O
] O
) O
degenerates O
to O
the O
conclusions O
obtained O
for O
Tanh O
and O
Sigmoid O
in O
and O
the O
conclusion O
in O
for O
ReLU Method
. O
General O
case O
. O
In O
general O
, O
a O
network O
with O
branches O
would O
have O
or O
for O
some O
s. O
Therefore O
, O
the O
number O
of O
input O
branches O
and O
output O
branches O
should O
be O
taken O
into O
consideration O
when O
initializing O
parameters O
. O
Specifically O
, O
if O
several O
multi O
- O
branch O
layers O
are O
stacked O
together O
without O
other O
operations O
( O
, O
batch O
normalization O
, O
convolution O
, O
ReLU Method
, O
. O
) O
, O
the O
output O
variance O
would O
be O
increased O
approximately O
times O
by O
using O
Xavier Method
or O
MSR O
initialization O
. O
subsection O
: O
Output Metric
Variance Metric
Accumulation Metric
Residual Method
learning Method
allows O
us O
to O
train O
extremely O
deep Method
neural Method
networks Method
due O
to O
identity O
mappings O
. O
But O
it O
is O
also O
the O
source O
of O
its O
drawbacks O
: O
identity Method
mapping Method
keeps O
increasing O
the O
variances O
of O
responses O
when O
the O
network O
goes O
deeper O
, O
which O
increases O
the O
difficulty O
of O
optimization Task
. O
The O
response O
of O
the O
residual Method
unit Method
is O
computed O
as O
follows O
: O
where O
denotes O
the O
residual O
function O
, O
, O
a O
bottleneck Method
structure Method
with O
three O
convolutions Method
( O
) O
. O
Assume O
and O
are O
uncorrelated O
, O
then O
the O
variance O
of O
the O
response O
of O
residual Method
unit Method
is O
as O
where O
is O
positive O
. O
In O
, O
the O
identity Method
mapping Method
will O
be O
replaced O
by O
convolution Method
layer Method
when O
the O
resolution O
of O
feature O
maps O
is O
reduced O
, O
or O
when O
the O
dimension O
of O
feature O
channels O
are O
increased O
. O
This O
allows O
the O
networks O
to O
reset O
the O
variance O
of O
response O
to O
a O
small O
value O
, O
and O
avoid O
responses O
with O
very O
large O
variance O
, O
as O
shown O
in O
Figure O
. O
[ O
reference O
] O
. O
The O
effect O
of O
increasing O
variance O
becomes O
more O
obvious O
in O
hourglass O
- O
like O
structures O
, O
where O
the O
responses O
of O
two O
residual Method
units Method
are O
summed O
together O
, O
as O
illustrated O
in O
Figure O
. O
[ O
reference O
] O
( O
a O
) O
. O
Assume O
branches O
are O
uncorrelated O
, O
then O
the O
variance O
will O
be O
increased O
as O
: O
Hence O
the O
output O
variance O
is O
almost O
doubled O
. O
When O
the O
network O
goes O
deeper O
, O
the O
variance O
will O
increase O
drastically O
. O
In O
this O
paper O
, O
we O
use O
a O
convolution Method
preceding Method
with O
batch Method
normalization Method
and O
ReLU Method
to O
replace O
the O
identity Method
mapping Method
when O
the O
output O
of O
two O
residual Method
units Method
are O
summed O
up O
, O
as O
illustrated O
in O
Figure O
. O
[ O
reference O
] O
( O
b O
) O
. O
This O
simple O
replacement O
stops O
the O
variance O
explosion O
, O
as O
demonstrated O
in O
Figure O
. O
[ O
reference O
] O
( O
c O
) O
. O
In O
experiments O
, O
we O
find O
that O
breaking O
the O
variance O
explosion O
also O
provide O
a O
better O
performance O
( O
Section O
[ O
reference O
] O
) O
. O
section O
: O
Experiments O
subsection O
: O
Experiments O
on O
Human Task
Pose Task
Estimation Task
We O
conduct O
experiments O
on O
two O
widely O
used O
human Task
pose Task
estimation Task
benchmarks Task
. O
( O
i O
) O
The O
MPII Material
human Material
pose Material
dataset Material
, O
which O
covers O
a O
wide O
range O
of O
human O
activities O
with O
25k O
images O
containing O
over O
40k O
people O
. O
( O
ii O
) O
The O
Leeds Material
Sports Material
Poses Material
( O
LSP Material
) O
and O
its O
extended O
training O
dataset O
, O
which O
contains O
12k O
images O
with O
challenging O
poses O
in O
sports O
. O
subsubsection O
: O
Implementation O
Details O
Our O
implementation O
follows O
. O
The O
input O
image O
is O
cropped O
from O
a O
resized O
image O
according O
to O
the O
annotated O
body O
position O
and O
scale O
. O
For O
the O
LSP Material
test Material
set Material
, O
we O
simply O
use O
the O
image O
center O
as O
the O
body O
position O
, O
and O
estimate O
the O
body O
scale O
by O
the O
image O
size O
. O
Training O
data O
are O
augmented O
by O
scaling O
, O
rotation O
, O
flipping O
, O
and O
adding O
color O
noise O
. O
All O
the O
models O
are O
trained O
using O
Torch Method
. O
We O
use O
RMSProp Method
to O
optimize O
the O
network O
on O
4 O
Titan O
X O
GPUs O
with O
a O
mini O
- O
batch O
size O
of O
16 O
( O
4 O
per O
GPU O
) O
for O
200 O
epochs O
. O
The O
learning Metric
rate Metric
is O
initialized O
as O
and O
is O
dropped O
by O
10 O
at O
the O
th O
and O
the O
th O
epoch O
. O
Testing O
is O
conducted O
on O
six O
- O
scale O
image O
pyramids O
with O
flipping O
. O
subsubsection O
: O
Experimental O
Results O
Evaluation Metric
measure Metric
. O
Following O
previous O
work O
, O
we O
use O
the O
Percentage Metric
Correct Metric
Keypoints Metric
( O
PCK Metric
) O
measure O
on O
the O
LSP Material
dataset Material
, O
and O
use O
the O
modified O
PCK Metric
measure Metric
that O
uses O
the O
matching O
threshold O
as O
of O
the O
head O
segment O
length O
( O
PCKh Metric
) O
on O
the O
MPII Material
dataset Material
. O
MPII Material
Human Material
Pose Material
. O
We O
report O
the O
performance O
on O
MPII Material
dataset Material
in O
Table O
[ O
reference O
] O
. O
Ours Method
- Method
A Method
is O
trained O
using O
the O
training O
and O
validation O
set O
used O
in O
. O
Ours O
- O
B Method
is O
trained O
with O
the O
same O
settings O
but O
using O
all O
the O
MPII Material
training Material
set Material
. O
Our O
approach O
achieves O
PCKh Metric
score O
at O
threshold O
of O
, O
which O
is O
the O
new O
state O
- O
of O
- O
the O
- O
art O
result O
. O
Specifically O
, O
our O
method O
achieves O
and O
improvements O
on O
wrist O
and O
ankle O
, O
which O
are O
considered O
as O
the O
most O
challenging O
parts O
to O
be O
detected O
. O
Qualitative O
results O
are O
demonstrated O
in O
Figure O
. O
[ O
reference O
] O
. O
Complexity Metric
. O
Our O
model O
increases O
the O
number O
of O
parameters O
by O
from O
M O
to O
M O
given O
an O
eight O
- O
stack Method
hourglass Method
network Method
. O
Our O
model O
needs O
GFLOPs Method
for O
a O
RGB Method
image Method
, O
which O
is O
a O
increase O
compared O
to O
hourglass Method
network Method
( O
GFLOPs Method
) O
. O
As O
reported O
in O
, O
deeper O
hourglass O
with O
more O
stacks O
hardly O
improves O
result O
. O
LSP Material
dataset Material
. O
Table O
[ O
reference O
] O
presents O
the O
PCK Metric
scores Metric
at O
the O
threshold O
of O
. O
We O
follow O
previous O
methods O
to O
train O
our O
model O
by O
adding O
MPII Material
training Material
set Material
to O
the O
LSP Material
and O
its O
extended O
training O
set O
. O
Our O
method O
improves O
the O
previous O
best O
result O
with O
a O
large O
margin O
by O
. O
For O
difficult O
body O
parts O
, O
, O
wrist O
and O
ankle O
, O
we O
have O
and O
improvements O
, O
respectively O
. O
Our O
method O
gains O
a O
lot O
due O
to O
the O
high O
occurrence O
of O
foreshortening O
and O
extreme O
poses O
presented O
in O
this O
dataset O
, O
as O
demonstrated O
in O
Figure O
. O
[ O
reference O
] O
. O
subsubsection O
: O
Ablation Task
Study Task
We O
conduct O
ablation O
study O
on O
the O
MPII Material
validation Material
set Material
used O
in O
with O
a O
2 Method
- Method
stack Method
hourglass Method
network Method
as O
the O
basic O
model O
. O
Architectures Method
of O
PRM Method
. O
We O
first O
evaluate O
different O
designs O
of O
PRM Method
, O
as O
discussed O
in O
Section O
[ O
reference O
] O
, O
with O
the O
same O
number O
of O
branches O
, O
and O
the O
same O
feature O
channels O
for O
each O
branch O
( O
, O
5 O
branches O
with O
28 O
feature O
channels O
for O
each O
pyramidal O
branch O
) O
. O
We O
use O
PRM Method
- O
A O
to O
PRM O
- O
D Method
, O
which O
corresponds O
to O
Figure O
. O
[ O
reference O
] O
, O
to O
denote O
the O
different O
architectures O
. O
Specifically O
, O
PRM Method
- O
A O
produces O
separate O
input O
feature O
maps O
for O
different O
levels O
of O
pyramids O
, O
while O
PRM Method
- O
B Method
uses O
shared O
feature O
maps O
for O
all O
levels O
of O
pyramids O
. O
PRM O
- O
C Method
uses O
concatenation Method
instead O
of O
addition O
to O
combine O
features O
generated O
from O
pyramid O
, O
which O
is O
similar O
to O
inception Method
models Method
. O
PRM O
- O
D Method
uses O
dilated Method
convolutions Method
, O
which O
are O
also O
used O
in O
ASPP Method
- Method
net Method
, O
instead O
of O
pooling Method
to O
build O
the O
pyramid O
. O
The O
validation Metric
accuracy Metric
is O
reported O
in O
Figure O
. O
[ O
reference O
] O
( O
a O
) O
. O
All O
the O
PRMs Method
have O
better O
accuracy Metric
compared O
with O
the O
baseline O
model O
. O
We O
observe O
that O
the O
difference O
in O
accuracy Metric
between O
PRM Method
- Method
A Method
to O
PRM O
- O
D Method
is O
subtle O
, O
while O
the O
parameters O
of O
PRM Method
- O
A O
/ O
C Method
are O
higher O
than O
PRM O
- O
B Method
/ O
B* O
/ O
D Method
( O
Figure O
. O
[ O
reference O
] O
( O
b O
) O
) O
, O
and O
the O
computational Metric
complexity Metric
( O
GFLOPs Metric
) O
of O
PRM Method
- Method
A Method
/ O
C Method
/ O
D Method
are O
higher O
than O
PRM Method
- O
B Method
/ O
B*. Method
Therefore O
, O
we O
use O
PRM O
- O
B Method
* O
in O
the O
rest O
of O
the O
experiments O
. O
Noted O
that O
increasing O
the O
number O
of O
channels O
to O
make O
the O
baseline O
model O
has O
the O
similar O
model O
size O
as O
ours O
( O
Wide O
BS O
) O
would O
slightly O
improve O
the O
performance O
. O
But O
it O
is O
still O
worse O
than O
ours O
. O
Scales O
of O
pyramids O
. O
To O
evaluate O
the O
trade O
- O
off O
between O
the O
scales O
of O
pyramids O
, O
we O
vary O
the O
scales O
from O
3 O
to O
5 O
, O
and O
fix O
the O
model O
size O
by O
tuning O
the O
feature O
channels O
in O
each O
scale O
. O
We O
observe O
that O
increasing O
scales O
generally O
improves O
the O
performance O
, O
as O
shown O
in O
Figure O
. O
[ O
reference O
] O
( O
a O
- O
b O
) O
. O
Weight Method
initialization Method
. O
We O
compare O
the O
performance O
of O
our O
initialization Method
scheme Method
with O
Xavier Method
and O
MSR Method
methods Method
. O
The O
training Metric
and Metric
validation Metric
curves Metric
of O
accuracy Metric
epoch Metric
are O
reported O
in O
Figure O
[ O
reference O
] O
( O
c O
- O
d O
) O
. O
It O
can O
be O
seen O
that O
the O
proposed O
initialization Method
scheme Method
achieves O
better O
performance O
than O
both O
methods O
. O
Controlling O
variance O
explosion O
. O
Controlling Task
variance Task
explosion Task
, O
as O
discussed O
in O
Section O
[ O
reference O
] O
, O
obtains O
higher O
validation Metric
score Metric
( O
88.0 O
) O
compared O
with O
the O
baseline O
model O
( O
87.6 O
) O
. O
With O
our O
pyramid O
residual Method
module Method
, O
the O
performance O
could O
be O
further O
improved O
to O
88.5 O
PCKh Metric
score O
. O
subsection O
: O
Experiments O
on O
CIFAR Material
- Material
10 Material
Image O
Classification O
The O
CIFAR Material
- Material
10 Material
dataset O
consists O
of O
50k O
training O
images O
and O
10k O
test O
images O
with O
size O
drawn O
from O
10 O
classes O
. O
We O
follow O
previous O
works O
for O
data Task
preparation Task
and O
augmentation Task
. O
We O
incorporate O
the O
proposed O
pyramid Method
branches Method
into O
two O
state O
- O
of O
- O
the O
- O
art O
network Method
architectures Method
, O
, O
Wide Method
residual Method
networks Method
and O
ResNeXt Method
. O
We O
add O
four O
pyramid O
branches O
with O
scales O
ranging O
from O
to O
into O
the O
building O
block O
of O
both O
Wide Method
ResNet Method
and O
ResNeXt Method
. O
For O
Wide O
ResNet Method
, O
the O
total O
width O
of O
all O
pyramid O
branches O
is O
equal O
to O
the O
width O
of O
the O
output O
of O
each O
residual Method
module Method
. O
For O
ResNeXt Method
, O
we O
simply O
use O
the O
same O
width O
as O
its O
original O
branches O
for O
our O
pyramid O
branches O
. O
Table O
[ O
reference O
] O
shows O
the O
top Metric
- Metric
1 Metric
test Metric
error Metric
, O
model Metric
sizes Metric
and O
GFLOPs Method
. O
Our O
method O
with O
similar O
or O
less O
model O
size O
( O
Ours O
- O
28 O
- O
9 O
and O
Ours O
- O
29 O
, O
×864d O
×1664d O
) O
achieve O
better O
results O
. O
A O
larger O
model O
with O
our O
pyramid Method
module Method
( O
Ours Method
- Method
29 Method
, O
×1664d Method
) O
achieves O
test Metric
error Metric
, O
which O
is O
the O
state O
- O
of O
- O
the O
- O
art O
result O
on O
CIFAR Material
- Material
10 Material
. O
section O
: O
Conclusion O
This O
paper O
has O
proposed O
a O
Pyramid Method
Residual Method
Module Method
to O
enhance O
the O
invariance O
in O
scales O
of O
the O
DCNNs Method
. O
We O
also O
provide O
a O
derivation O
of O
the O
initialization Method
scheme Method
for O
multi Task
- Task
branch Task
networks Task
, O
and O
demonstrate O
its O
theoretical O
soundness O
and O
efficiency O
through O
experimental O
analysis O
. O
Additionally O
, O
a O
simple O
yet O
effective O
method O
to O
prevent O
the O
variances O
of O
response O
from O
explosion O
when O
adding O
outputs O
of O
multiple O
identity O
mappings O
has O
been O
proposed O
. O
Our O
PRMs Method
and O
the O
initialization Method
scheme Method
for O
multi Task
- Task
branch Task
networks Task
are O
general O
, O
and O
would O
help O
other O
tasks O
. O
Acknowledgment O
: O
This O
work O
is O
supported O
by O
SenseTime O
Group O
Limited O
, O
the O
General O
Research O
Fund O
sponsored O
by O
the O
Research O
Grants O
Council O
of O
Hong O
Kong O
( O
Project O
Nos O
. O
CUHK14213616 O
, O
CUHK14206114 O
, O
CUHK14205615 O
, O
CUHK419412 O
, O
CUHK14203015 O
, O
CUHK14207814 O
, O
and O
CUHK14239816 O
) O
, O
the O
Hong O
Kong O
Innovation O
and O
Technology O
Support O
Programme O
( O
No O
. O
ITS O
/ O
121 O
/ O
15FX O
) O
, O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
61371192 O
) O
, O
and O
ONR O
N00014 O
- O
15 O
- O
1 O
- O
2356 O
. O
bibliography O
: O
References O
