HIERARCHICAL Method
MULTISCALE Method
RECURRENT Method
NEURAL Method
NETWORKS Method
section O
: O
ABSTRACT O
Learning O
both O
hierarchical Task
and Task
temporal Task
representation Task
has O
been O
among O
the O
longstanding O
challenges O
of O
recurrent Method
neural Method
networks Method
. O
Multiscale Method
recurrent Method
neural Method
networks Method
have O
been O
considered O
as O
a O
promising O
approach O
to O
resolve O
this O
issue O
, O
yet O
there O
has O
been O
a O
lack O
of O
empirical O
evidence O
showing O
that O
this O
type O
of O
models O
can O
actually O
capture O
the O
temporal O
dependencies O
by O
discovering O
the O
latent O
hierarchical O
structure O
of O
the O
sequence O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
multiscale Method
approach Method
, O
called O
the O
hierarchical Method
multiscale Method
recurrent Method
neural Method
network Method
, O
that O
can O
capture O
the O
latent O
hierarchical O
structure O
in O
the O
sequence O
by O
encoding O
the O
temporal O
dependencies O
with O
different O
timescales O
using O
a O
novel O
update Method
mechanism Method
. O
We O
show O
some O
evidence O
that O
the O
proposed O
model O
can O
discover O
underlying O
hierarchical O
structure O
in O
the O
sequences O
without O
using O
explicit O
boundary O
information O
. O
We O
evaluate O
our O
proposed O
model O
on O
character Task
- Task
level Task
language Task
modelling Task
and O
handwriting Task
sequence Task
generation Task
. O
section O
: O
INTRODUCTION O
One O
of O
the O
key O
principles O
of O
learning Task
in O
deep Method
neural Method
networks Method
as O
well O
as O
in O
the O
human O
brain O
is O
to O
obtain O
a O
hierarchical Method
representation Method
with O
increasing O
levels O
of O
abstraction O
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
A O
stack O
of O
representation O
layers O
, O
learned O
from O
the O
data O
in O
a O
way O
to O
optimize O
the O
target O
task O
, O
make O
deep Method
neural Method
networks Method
entertain O
advantages O
such O
as O
generalization O
to O
unseen O
examples O
[ O
reference O
] O
, O
sharing O
learned O
knowledge O
among O
multiple O
tasks O
, O
and O
discovering O
disentangling O
factors O
of O
variation O
[ O
reference O
] O
. O
The O
remarkable O
recent O
successes O
of O
the O
deep Method
convolutional Method
neural Method
networks Method
are O
particularly O
based O
on O
this O
ability O
to O
learn O
hierarchical Method
representation Method
for O
spatial O
data O
[ O
reference O
] O
. O
For O
modelling Task
temporal Task
data Task
, O
the O
recent O
resurgence O
of O
recurrent Method
neural Method
networks Method
( O
RNN Method
) O
has O
led O
to O
remarkable O
advances O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
However O
, O
unlike O
the O
spatial O
data O
, O
learning O
both O
hierarchical Method
and Method
temporal Method
representation Method
has O
been O
among O
the O
long O
- O
standing O
challenges O
of O
RNNs Method
in O
spite O
of O
the O
fact O
that O
hierarchical O
multiscale O
structures O
naturally O
exist O
in O
many O
temporal O
data O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
A O
promising O
approach O
to O
model O
such O
hierarchical Method
and Method
temporal Method
representation Method
is O
the O
multiscale Method
RNNs Method
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Based O
on O
the O
observation O
that O
high O
- O
level O
abstraction O
changes O
slowly O
with O
temporal O
coherency O
while O
low O
- O
level O
abstraction O
has O
quickly O
changing O
features O
sensitive O
to O
the O
precise O
local O
timing O
[ O
reference O
] O
, O
the O
multiscale Method
RNNs Method
group O
hidden O
units O
into O
multiple O
modules O
of O
different O
timescales O
. O
In O
addition O
to O
the O
fact O
that O
the O
architecture O
fits O
naturally O
to O
the O
latent O
hierarchical O
structures O
in O
many O
temporal O
data O
, O
the O
multiscale Method
approach Method
provides O
the O
following O
advantages O
that O
resolve O
some O
inherent O
problems O
of O
standard O
RNNs Method
: O
( O
a O
) O
computational Metric
efficiency O
obtained O
by O
updating O
the O
high O
- O
level O
layers O
less O
frequently O
, O
( O
b O
) O
efficiently O
delivering O
long O
- O
term O
dependencies O
with O
fewer O
updates O
at O
the O
high O
- O
level O
layers O
, O
which O
mitigates O
the O
vanishing Task
gradient Task
problem Task
, O
( O
c O
) O
flexible O
resource Task
allocation Task
( O
e.g. O
, O
more O
hidden O
units O
to O
the O
higher O
layers O
that O
focus O
on O
modelling O
long O
- O
term O
dependencies O
and O
less O
hidden O
units O
to O
the O
lower O
layers O
which O
are O
in O
charge O
of O
learning O
short O
- O
term O
dependencies O
) O
. O
In O
addition O
, O
the O
learned O
latent O
hierarchical O
structures O
can O
provide O
useful O
information O
to O
other O
downstream Task
tasks Task
such O
Published O
as O
a O
conference O
paper O
at O
ICLR O
2017 O
as O
module O
structures O
in O
computer Task
program Task
learning Task
, O
sub Task
- Task
task Task
structures Task
in O
hierarchical Task
reinforcement Task
learning Task
, O
and O
story Task
segments Task
in O
video Task
understanding Task
. O
There O
have O
been O
various O
approaches O
to O
implementing O
the O
multiscale Method
RNNs Method
. O
The O
most O
popular O
approach O
is O
to O
set O
the O
timescales O
as O
hyperparameters O
[ O
reference O
][ O
reference O
] O
instead O
of O
treating O
them O
as O
dynamic O
variables O
that O
can O
be O
learned O
from O
the O
data O
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
However O
, O
considering O
the O
fact O
that O
non O
- O
stationarity O
is O
prevalent O
in O
temporal O
data O
, O
and O
that O
many O
entities O
of O
abstraction O
such O
as O
words O
and O
sentences O
are O
in O
variable O
length O
, O
we O
claim O
that O
it O
is O
important O
for O
an O
RNN Method
to O
dynamically O
adapt O
its O
timescales O
to O
the O
particulars O
of O
the O
input O
entities O
of O
various O
length O
. O
While O
this O
is O
trivial O
if O
the O
hierarchical O
boundary O
structure O
is O
provided O
[ O
reference O
] O
, O
it O
has O
been O
a O
challenge O
for O
an O
RNN Method
to O
discover O
the O
latent O
hierarchical O
structure O
in O
temporal O
data O
without O
explicit O
boundary O
information O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
multiscale Method
RNN Method
model Method
, O
which O
can O
learn O
the O
hierarchical O
multiscale O
structure O
from O
temporal O
data O
without O
explicit O
boundary O
information O
. O
This O
model O
, O
called O
a O
hierarchical Method
multiscale Method
recurrent Method
neural Method
network Method
( Method
HM Method
- Method
RNN Method
) Method
, O
does O
not O
assign O
fixed O
update O
rates O
, O
but O
adaptively O
determines O
proper O
update O
times O
corresponding O
to O
different O
abstraction O
levels O
of O
the O
layers O
. O
We O
find O
that O
this O
model O
tends O
to O
learn O
fine O
timescales O
for O
low O
- O
level O
layers O
and O
coarse O
timescales O
for O
high O
- O
level O
layers O
. O
To O
do O
this O
, O
we O
introduce O
a O
binary Method
boundary Method
detector Method
at O
each O
layer O
. O
The O
boundary Method
detector Method
is O
turned O
on O
only O
at O
the O
time O
steps O
where O
a O
segment O
of O
the O
corresponding O
abstraction O
level O
is O
completely O
processed O
. O
Otherwise O
, O
i.e. O
, O
during O
the O
within Task
segment Task
processing Task
, O
it O
stays O
turned O
off O
. O
Using O
the O
hierarchical O
boundary O
states O
, O
we O
implement O
three O
operations O
, O
UPDATE O
, O
COPY O
and O
FLUSH O
, O
and O
choose O
one O
of O
them O
at O
each O
time O
step O
. O
The O
UPDATE Method
operation Method
is O
similar O
to O
the O
usual O
update Method
rule Method
of O
the O
long Method
short Method
- Method
term Method
memory Method
( O
LSTM Method
) O
[ O
reference O
] O
, O
except O
that O
it O
is O
executed O
sparsely O
according O
to O
the O
detected O
boundaries O
. O
The O
COPY Method
operation Method
simply O
copies O
the O
cell O
and O
hidden O
states O
of O
the O
previous O
time O
step O
. O
Unlike O
the O
leaky O
integration O
of O
the O
LSTM Method
or O
the O
Gated Method
Recurrent Method
Unit Method
( O
GRU Method
) O
[ O
reference O
] O
, O
the O
COPY Method
operation Method
retains O
the O
whole O
states O
without O
any O
loss O
of O
information O
. O
The O
FLUSH Method
operation Method
is O
executed O
when O
a O
boundary O
is O
detected O
, O
where O
it O
first O
ejects O
the O
summarized Method
representation Method
of O
the O
current O
segment O
to O
the O
upper O
layer O
and O
then O
reinitializes O
the O
states O
to O
start O
processing O
the O
next O
segment O
. O
Learning O
to O
select O
a O
proper O
operation O
at O
each O
time O
step O
and O
to O
detect O
the O
boundaries O
, O
the O
HM Method
- Method
RNN Method
discovers O
the O
latent O
hierarchical O
structure O
of O
the O
sequences O
. O
We O
find O
that O
the O
straight Method
- Method
through Method
estimator Method
[ O
reference Method
][ Method
reference Method
][ Method
reference Method
] O
is O
efficient O
for O
training O
this O
model O
containing O
discrete O
variables O
. O
We O
evaluate O
our O
model O
on O
two O
tasks O
: O
character Task
- Task
level Task
language Task
modelling Task
and O
handwriting Task
sequence Task
generation Task
. O
For O
the O
character Task
- Task
level Task
language Task
modelling Task
, O
the O
HM Method
- Method
RNN Method
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
Text8 Material
dataset Material
, O
and O
comparable O
results O
to O
the O
state O
- O
of O
- O
the O
- O
art O
on O
the O
Penn O
Treebank O
and O
Hutter Material
Prize Material
Wikipedia Material
datasets Material
. O
The O
HM Method
- Method
RNN Method
also O
outperforms O
the O
standard O
RNN Method
on O
the O
handwriting Task
sequence Task
generation Task
using O
the O
IAM O
- O
OnDB O
dataset O
. O
In O
addition O
, O
we O
demonstrate O
that O
the O
hierarchical O
structure O
found O
by O
the O
HM Method
- Method
RNN Method
is O
indeed O
very O
similar O
to O
the O
intrinsic O
structure O
observed O
in O
the O
data O
. O
The O
contributions O
of O
this O
paper O
are O
: O
• O
We O
propose O
for O
the O
first O
time O
an O
RNN Method
model Method
that O
can O
learn O
a O
latent O
hierarchical O
structure O
of O
a O
sequence O
without O
using O
explicit O
boundary O
information O
. O
• O
We O
show O
that O
it O
is O
beneficial O
to O
utilize O
the O
above O
structure O
through O
empirical O
evaluation O
. O
• O
We O
show O
that O
the O
straight Method
- Method
through Method
estimator Method
is O
an O
efficient O
way O
of O
training O
a O
model O
containing O
discrete O
variables O
. O
• O
We O
propose O
the O
slope Method
annealing Method
trick Method
to O
improve O
the O
training Method
procedure Method
based O
on O
the O
straight Method
- Method
through Method
estimator Method
. O
section O
: O
RELATED O
WORK O
Two O
notable O
early O
attempts O
inspiring O
our O
model O
are O
[ O
reference O
] O
and O
El Method
[ O
reference O
] O
. O
In O
these O
works O
, O
it O
is O
advocated O
to O
stack O
multiple O
layers O
of O
RNNs Method
in O
a O
decreasing O
order O
of O
update O
frequency O
for O
computational Metric
and O
learning Metric
efficiency Metric
. O
In O
[ O
reference O
] O
, O
the O
author O
shows O
a O
model O
that O
can O
self O
- O
organize O
a O
hierarchical O
multiscale O
structure O
. O
Particularly O
in O
El Method
[ O
reference O
] O
, O
the O
advantages O
of O
incorporating O
a O
priori O
knowledge O
, O
" O
temporal O
dependencies O
are O
structured O
hierarchically O
" O
, O
into O
the O
RNN Method
architecture Method
is O
studied O
. O
The O
authors O
propose O
an O
RNN Method
architecture Method
that O
updates O
each O
layer O
with O
a O
fixed O
but O
different O
rate O
, O
called O
a O
hierarchical Method
RNN Method
. O
LSTMs Method
[ O
reference O
] O
employ O
the O
multiscale Method
update Method
concept Method
, O
where O
the O
hidden O
units O
have O
different O
forget O
and O
update O
rates O
and O
thus O
can O
operate O
with O
different O
timescales O
. O
However O
, O
unlike O
our O
model O
, O
these O
timescales O
are O
not O
organized O
hierarchically O
. O
Although O
the O
LSTM Method
has O
a O
selfloop O
for O
the O
gradients O
that O
helps O
to O
capture O
the O
long O
- O
term O
dependencies O
by O
mitigating O
the O
vanishing Task
gradient Task
problem Task
, O
in O
practice O
, O
it O
is O
still O
limited O
to O
a O
few O
hundred O
time O
steps O
due O
to O
the O
leaky O
integration O
by O
which O
the O
contents O
to O
memorize O
for O
a O
long O
- O
term O
is O
gradually O
diluted O
at O
every O
time O
step O
. O
Also O
, O
the O
model O
remains O
computationally O
expensive O
because O
it O
has O
to O
perform O
the O
update O
at O
every O
time O
step O
for O
each O
unit O
. O
However O
, O
our O
model O
is O
less O
prone O
to O
these O
problems O
because O
it O
learns O
a O
hierarchical O
structure O
such O
that O
, O
by O
design O
, O
high O
- O
level O
layers O
learn O
to O
perform O
less O
frequent O
updates O
than O
low O
- O
level O
layers O
. O
We O
hypothesize O
that O
this O
property O
mitigates O
the O
vanishing Task
gradient Task
problem Task
more O
efficiently O
while O
also O
being O
computationally O
more O
efficient O
. O
A O
more O
recent O
model O
, O
the O
clockwork Method
RNN Method
( Method
CW Method
- Method
RNN Method
) O
[ O
reference O
] O
extends O
the O
hierarchical O
RNN O
( O
El Method
[ O
reference O
] O
and O
the O
NARX Method
RNN Method
[ O
reference O
] O
1 O
. O
The O
CW Method
- Method
RNN Method
tries O
to O
solve O
the O
issue O
of O
using O
soft O
timescales O
in O
the O
LSTM Method
, O
by O
explicitly O
assigning O
hard O
timescales O
. O
In O
the O
CW Method
- Method
RNN Method
, O
hidden O
units O
are O
partitioned O
into O
several O
modules O
, O
and O
different O
timescales O
are O
assigned O
to O
the O
modules O
such O
that O
a O
module O
i O
updates O
its O
hidden O
units O
at O
every O
2 O
( O
i−1 O
) O
- O
th O
time O
step O
. O
The O
CW Method
- Method
RNN Method
is O
computationally O
more O
efficient O
than O
the O
standard O
RNN Method
including O
the O
LSTM Method
since O
hidden Method
units Method
are O
updated O
only O
at O
the O
assigned O
clock O
rates O
. O
However O
, O
finding O
proper O
timescales O
in O
the O
CW Method
- Method
RNN Method
remains O
as O
a O
challenge O
whereas O
our O
model O
learns O
the O
intrinsic O
timescales O
from O
the O
data O
. O
In O
the O
biscale Method
RNNs Method
[ O
reference O
] O
, O
the O
authors O
proposed O
to O
model O
layer O
- O
wise O
timescales O
adaptively O
by O
having O
additional O
gating O
units O
, O
however O
this O
approach O
still O
relies O
on O
the O
soft Method
gating Method
mechanism Method
like O
LSTMs Method
. O
Other O
forms O
of O
Hierarchical Method
RNN Method
( Method
HRNN Method
) Method
architectures Method
have O
been O
proposed O
in O
the O
cases O
where O
the O
explicit O
hierarchical O
boundary O
structure O
is O
provided O
. O
In O
[ O
reference O
] O
, O
after O
obtaining O
the O
word O
boundary O
via O
tokenization Method
, O
the O
HRNN Method
architecture Method
is O
used O
for O
neural Task
machine Task
translation Task
by O
modelling O
the O
characters O
and O
words O
using O
the O
first Method
and Method
second Method
RNN Method
layers Method
, O
respectively O
. O
A O
similar O
HRNN Method
architecture Method
is O
also O
adopted O
in O
[ O
reference O
] O
to O
model O
dialogue O
utterances O
. O
However O
, O
in O
many O
cases O
, O
hierarchical O
boundary O
information O
is O
not O
explicitly O
observed O
or O
expensive O
to O
obtain O
. O
Also O
, O
it O
is O
unclear O
how O
to O
deploy O
more O
layers O
than O
the O
number O
of O
boundary O
levels O
that O
is O
explicitly O
observed O
in O
the O
data O
. O
While O
the O
above O
models O
focus O
on O
online Task
prediction Task
problems Task
, O
where O
a O
prediction Task
needs O
to O
be O
made O
by O
using O
only O
the O
past O
data O
, O
in O
some O
cases O
, O
predictions O
are O
made O
after O
observing O
the O
whole O
sequence O
. O
In O
this O
setting O
, O
the O
input O
sequence O
can O
be O
regarded O
as O
1 O
- O
D O
spatial O
data O
, O
convolutional Method
neural Method
networks Method
with O
1 Method
- Method
D Method
kernels Method
are O
proposed O
in O
[ O
reference O
] O
and O
[ O
reference O
] O
for O
language Task
modelling Task
and O
sentence Task
classification Task
. O
Also O
, O
in O
[ O
reference O
] O
and O
, O
the O
authors O
proposed O
to O
obtain O
high O
- O
level Method
representation Method
of O
the O
sequences O
of O
reduced O
length O
by O
repeatedly O
merging O
or O
pooling O
the O
lower Method
- Method
level Method
representation Method
of O
the O
sequences O
. O
Hierarchical Method
RNN Method
architectures Method
have O
also O
been O
used O
to O
discover O
the O
segmentation O
structure O
in O
sequences O
[ O
reference O
][ O
reference O
] O
. O
It O
is O
however O
different O
to O
our O
model O
in O
the O
sense O
that O
they O
optimize O
the O
objective O
with O
explicit O
labels O
on O
the O
hierarchical O
segments O
while O
our O
model O
discovers O
the O
intrinsic O
structure O
only O
from O
the O
sequences O
without O
segment O
label O
information O
. O
The O
COPY Method
operation Method
used O
in O
our O
model O
can O
be O
related O
to O
Zoneout Method
[ O
reference O
] O
which O
is O
a O
recurrent Method
generalization Method
of Method
stochastic Method
depth Method
[ O
reference O
] O
. O
In O
Zoneout Method
, O
an O
identity Method
transformation Method
is O
randomly O
applied O
to O
each O
hidden O
unit O
at O
each O
time O
step O
according O
to O
a O
Bernoulli Method
distribution Method
. O
This O
results O
in O
occasional O
copy O
operations O
of O
the O
previous O
hidden O
states O
. O
While O
the O
focus O
of O
Zoneout Task
is O
to O
propose O
a O
regularization Method
technique Method
similar O
to O
dropout Method
[ O
reference O
] O
( O
where O
the O
regularization O
strength O
is O
controlled O
by O
a O
hyperparameter O
) O
, O
our O
model O
learns O
( O
a O
) O
to O
dynamically O
determine O
when O
to O
copy O
from O
the O
context O
inputs O
and O
( O
b O
) O
to O
discover O
the O
hierarchical O
multiscale O
structure O
and O
representation O
. O
Although O
the O
main O
goal O
of O
our O
proposed O
model O
is O
not O
regularization O
, O
we O
found O
that O
our O
model O
also O
shows O
very O
good O
generalization Metric
performance O
. O
3 O
HIERARCHICAL Method
MULTISCALE Method
RECURRENT Method
NEURAL Method
NETWORKS Method
section O
: O
MOTIVATION O
To O
begin O
with O
, O
we O
provide O
an O
example O
of O
how O
a O
stacked Method
RNN Method
can O
model O
temporal O
data O
in O
an O
ideal O
setting O
, O
i.e. O
, O
when O
the O
hierarchy O
of O
segments O
is O
provided O
[ O
reference O
][ O
reference O
] O
. O
In O
Figure O
1 O
( O
a O
) O
, O
we O
depict O
a O
hierarchical Method
RNN Method
( Method
HRNN Method
) Method
for O
language Task
modelling Task
with O
two O
layers O
: O
the O
first O
layer O
receives O
characters O
as O
inputs O
and O
generates O
word Method
- Method
level Method
representations Method
( O
C2W Method
- Method
RNN Method
) O
, O
and O
the O
second O
layer O
takes O
the O
word Method
- Method
level Method
representations Method
as O
inputs O
and O
yields O
phrase Method
- Method
level Method
representations Method
( O
W2P Method
- Method
RNN Method
) O
. O
As O
shown O
, O
by O
means O
of O
the O
provided O
end O
- O
of O
- O
word O
labels O
, O
the O
C2W Method
- Method
RNN Method
obtains O
word Method
- Method
level Method
representation Method
after O
processing O
the O
last O
character O
of O
each O
word O
and O
passes O
the O
word Method
- Method
level Method
representation Method
to O
the O
W2P Method
- Method
RNN Method
. O
Then O
, O
the O
W2P Method
- Method
RNN Method
performs O
an O
update O
of O
the O
phrase Method
- Method
level Method
representation Method
. O
Note O
that O
the O
hidden O
states O
of O
the O
W2P Method
- Method
RNN Method
remains O
unchanged O
while O
all O
the O
characters O
of O
a O
word O
are O
processed O
by O
the O
C2W Method
- Method
RNN Method
. O
When O
the O
C2W Method
- Method
RNN Method
starts O
to O
process O
the O
next O
word O
, O
its O
hidden O
states O
are O
reinitialized O
using O
the O
latest O
hidden O
states O
of O
the O
W2P Method
- Method
RNN Method
, O
which O
contain O
summarized O
representation O
of O
all O
the O
words O
that O
have O
been O
processed O
by O
that O
time O
step O
, O
in O
that O
phrase O
. O
From O
this O
simple O
example O
, O
we O
can O
see O
the O
advantages O
of O
having O
a O
hierarchical O
multiscale O
structure O
: O
( O
1 O
) O
as O
the O
W2P Method
- Method
RNN Method
is O
updated O
at O
a O
much O
slower O
update Metric
rate Metric
than O
the O
C2W Method
- Method
RNN Method
, O
a O
considerable O
amount O
of O
computation O
can O
be O
saved O
, O
( O
2 O
) O
gradients O
are O
backpropagated O
through O
a O
much O
smaller O
number O
of O
time O
steps O
, O
and O
( O
3 O
) O
layer Method
- Method
wise Method
capacity Method
control Method
becomes O
possible O
( O
e.g. O
, O
use O
a O
smaller O
number O
of O
hidden O
units O
in O
the O
first O
layer O
which O
models O
short O
- O
term O
dependencies O
but O
whose O
updates O
are O
invoked O
much O
more O
often O
) O
. O
Can O
an O
RNN Method
discover O
such O
hierarchical O
multiscale O
structure O
without O
explicit O
hierarchical O
boundary O
information O
? O
Considering O
the O
fact O
that O
the O
boundary O
information O
is O
difficult O
to O
obtain O
( O
for O
example O
, O
consider O
languages O
where O
words O
are O
not O
always O
cleanly O
separated O
by O
spaces O
or O
punctuation O
symbols O
, O
and O
imperfect O
rules O
are O
used O
to O
separately O
perform O
segmentation Task
) O
or O
usually O
not O
provided O
at O
all O
, O
this O
is O
a O
legitimate O
problem O
. O
It O
gets O
worse O
when O
we O
consider O
higher O
- O
level O
concepts O
which O
we O
would O
like O
the O
RNN Method
to O
discover O
autonomously O
. O
In O
Section O
2 O
, O
we O
discussed O
the O
limitations O
of O
the O
existing O
RNN Method
models Method
under O
this O
setting O
, O
which O
either O
have O
to O
update O
all O
units O
at O
every O
time O
step O
or O
use O
fixed O
update O
frequencies O
[ O
reference O
][ O
reference O
] O
. O
Unfortunately O
, O
this O
kind O
of O
approach O
is O
not O
well O
suited O
to O
the O
case O
where O
different O
segments O
in O
the O
hierarchical Method
decomposition Method
have O
different O
lengths O
: O
for O
example O
, O
different O
words O
have O
different O
lengths O
, O
so O
a O
fixed O
hierarchy O
would O
not O
update O
its O
upper O
- O
level O
units O
in O
synchrony O
with O
the O
natural O
boundaries O
in O
the O
data O
. O
section O
: O
THE O
PROPOSED O
MODEL O
A O
key O
element O
of O
our O
model O
is O
the O
introduction O
of O
a O
parametrized Method
boundary Method
detector Method
, O
which O
outputs O
a O
binary O
value O
, O
in O
each O
layer O
of O
a O
stacked Method
RNN Method
, O
and O
learns O
when O
a O
segment O
should O
end O
in O
such O
a O
way O
to O
optimize O
the O
overall O
target O
objective Metric
. O
Whenever O
the O
boundary Method
detector Method
is O
turned O
on O
at O
a O
time O
step O
of O
layer O
( O
i.e. O
, O
when O
the O
boundary O
state O
is O
1 O
) O
, O
the O
model O
considers O
this O
to O
be O
the O
end O
of O
a O
segment O
corresponding O
to O
the O
latent O
abstraction O
level O
of O
that O
layer O
( O
e.g. O
, O
word O
or O
phrase O
) O
and O
feeds O
the O
summarized Method
representation Method
of O
the O
detected O
segment O
into O
the O
upper O
layer O
( O
+ O
1 O
) O
. O
Using O
the O
boundary O
states O
, O
at O
each O
time O
step O
, O
each O
layer O
selects O
one O
of O
the O
following O
operations O
: O
UPDATE O
, O
COPY O
or O
FLUSH O
. O
The O
selection O
is O
determined O
by O
( O
1 O
) O
the O
boundary O
state O
of O
the O
current O
time O
step O
in O
the O
layer O
below O
z O
−1 O
t O
and O
( O
2 O
) O
the O
boundary O
state O
of O
the O
previous O
time O
step O
in O
the O
same O
layer O
z O
t−1 O
. O
In O
the O
following O
, O
we O
describe O
an O
HM Method
- Method
RNN Method
based O
on O
the O
LSTM Method
update O
rule O
. O
We O
call O
this O
model O
a O
hierarchical Method
multiscale Method
LSTM Method
( O
HM Method
- Method
LSTM Method
) O
. O
Consider O
an O
HM Method
- Method
LSTM Method
model Method
of Method
L Method
layers Method
( O
= O
1 O
, O
. O
. O
. O
, O
L O
) O
which O
, O
at O
each O
layer O
, O
performs O
the O
following O
update O
at O
time O
step O
t O
: O
( O
Here O
, O
h O
and O
c O
denote O
the O
hidden O
and O
cell O
states O
, O
respectively O
. O
The O
function O
f O
HM O
- O
LSTM Method
is O
implemented O
as O
follows O
. O
First O
, O
using O
the O
two O
boundary O
states O
z O
t−1 O
and O
z O
−1 O
t O
, O
the O
cell O
state O
is O
updated O
by O
: O
and O
then O
the O
hidden O
state O
is O
obtained O
by O
: O
Here O
, O
( O
f O
, O
i O
, O
o O
) O
are O
forget O
, O
input O
, O
output O
gates O
, O
and O
g O
is O
a O
cell O
proposal O
vector O
. O
Note O
that O
unlike O
the O
LSTM Method
, O
it O
is O
not O
necessary O
to O
compute O
these O
gates O
and O
cell O
proposal O
values O
at O
every O
time O
step O
. O
For O
example O
, O
in O
the O
case O
of O
the O
COPY O
operation O
, O
we O
do O
not O
need O
to O
compute O
any O
of O
these O
values O
and O
thus O
can O
save O
computations O
. O
The O
COPY Method
operation Method
, O
which O
simply O
performs O
( O
c O
t O
, O
h O
t O
) O
← O
( O
c O
t−1 O
, O
h O
t−1 O
) O
, O
implements O
the O
observation O
that O
an O
upper O
layer O
should O
keep O
its O
state O
unchanged O
until O
it O
receives O
the O
summarized O
input O
from O
the O
lower O
layer O
. O
The O
UPDATE Method
operation Method
is O
performed O
to O
update O
the O
summary Task
representation Task
of Task
the Task
layer Task
if O
the O
boundary O
z O
−1 O
t O
is O
detected O
from O
the O
layer O
below O
but O
the O
boundary O
z O
t−1 O
was O
not O
found O
at O
the O
previous O
time O
step O
. O
Hence O
, O
the O
UPDATE Method
operation Method
is O
executed O
sparsely O
unlike O
the O
standard O
RNNs Method
where O
it O
is O
executed O
at O
every O
time O
step O
, O
making O
it O
computationally O
inefficient O
. O
If O
a O
boundary O
is O
detected O
, O
the O
FLUSH Method
operation Method
is O
executed O
. O
The O
FLUSH Method
operation Method
consists O
of O
two O
sub O
- O
operations O
: O
( O
a O
) O
EJECT O
to O
pass O
the O
current O
state O
to O
the O
upper O
layer O
and O
then O
( O
b O
) O
RESET O
to O
reinitialize O
the O
state O
before O
starting O
to O
read O
a O
new O
segment O
. O
This O
operation O
implicitly O
forces O
the O
upper O
layer O
to O
absorb O
the O
summary O
information O
of O
the O
lower O
layer O
segment O
, O
because O
otherwise O
it O
will O
be O
lost O
. O
Note O
that O
the O
FLUSH Method
operation Method
is O
a O
hard O
reset O
in O
the O
sense O
that O
it O
completely O
erases O
all O
the O
previous O
states O
of O
the O
same O
layer O
, O
which O
is O
different O
from O
the O
soft O
reset O
or O
soft O
forget O
operation O
in O
the O
GRU Method
or O
LSTM Method
. O
Whenever O
needed O
( O
depending O
on O
the O
chosen O
operation O
) O
, O
the O
gate O
values O
( O
f O
t O
, O
i O
t O
, O
o O
t O
) O
, O
the O
cell O
proposal O
g O
t O
, O
and O
the O
pre O
- O
activation O
of O
the O
boundary O
detectorz O
t O
2 O
are O
then O
obtained O
by O
: O
where O
Here O
, O
we O
use O
W O
) O
×dim O
( O
h O
) O
to O
denote O
state O
transition O
parameters O
from O
layer O
i O
to O
layer O
j O
, O
and O
b O
∈ O
R O
4dim O
( O
h O
) O
+ O
1 O
is O
a O
bias O
term O
. O
In O
the O
last O
layer O
L O
, O
the O
2z O
t O
can O
also O
be O
implemented O
as O
a O
function O
of O
h O
t O
, O
e.g. O
, O
z O
t O
= O
hard O
sigm O
( O
U O
h O
t O
) O
. O
top O
- O
down O
connection O
is O
ignored O
, O
and O
we O
use O
h O
0 O
t O
= O
x O
t O
. O
Since O
the O
input O
should O
not O
be O
omitted O
, O
we O
set O
z O
0 O
t O
= O
1 O
for O
all O
t. O
Also O
, O
we O
do O
not O
use O
the O
boundary Method
detector Method
for O
the O
last O
layer O
. O
The O
hard O
sigm O
is O
defined O
by O
hard Method
sigm Method
( Method
x Method
) O
= O
max O
0 O
, O
min O
1 O
, O
with O
a O
being O
the O
slope O
variable O
. O
Unlike O
the O
standard O
LSTM Method
, O
the O
HM Method
- Method
LSTM Method
has O
a O
top O
- O
down O
connection O
from O
( O
+ O
1 O
) O
to O
, O
which O
is O
allowed O
to O
be O
activated O
only O
if O
a O
boundary O
is O
detected O
at O
the O
previous O
time O
step O
of O
the O
layer O
( O
see O
Eq O
. O
6 O
) O
. O
This O
makes O
the O
layer O
to O
be O
initialized O
with O
more O
long O
- O
term O
information O
after O
the O
boundary O
is O
detected O
and O
execute O
the O
FLUSH O
operation O
. O
In O
addition O
, O
the O
input O
from O
the O
lower O
layer O
( O
− O
1 O
) O
becomes O
effective O
only O
when O
a O
boundary O
is O
detected O
at O
the O
current O
time O
step O
in O
the O
layer O
( O
− O
1 O
) O
due O
to O
the O
binary O
gate O
z O
−1 O
t O
. O
Figure O
2 O
( O
left O
) O
shows O
the O
gating Method
mechanism Method
of O
the O
HM Method
- Method
LSTM Method
at O
time O
step O
t. O
Finally O
, O
the O
binary O
boundary O
state O
z O
t O
is O
obtained O
by O
: O
For O
the O
binarization O
function O
f O
bound O
: O
R O
→ O
{ O
0 O
, O
1 O
} O
, O
we O
can O
either O
use O
a O
deterministic Method
step Method
function Method
: O
or O
sample O
from O
a O
Bernoulli Method
distribution Method
z O
t O
∼ O
Bernoulli Method
( Method
z Method
t Method
) O
. O
Although O
this O
binary O
decision O
is O
a O
key O
to O
our O
model O
, O
it O
is O
usually O
difficult O
to O
use O
stochastic Method
gradient Method
descent Method
to O
train O
such O
model O
with O
discrete O
decisions O
as O
it O
is O
not O
differentiable O
. O
section O
: O
COMPUTING Task
GRADIENT Task
OF Task
BOUNDARY Task
DETECTOR Task
Training O
neural Method
networks Method
with O
discrete O
variables O
requires O
more O
efforts O
since O
the O
standard O
backpropagation Method
is O
no O
longer O
applicable O
due O
to O
the O
non O
- O
differentiability O
. O
Among O
a O
few O
methods O
for O
training O
a O
neural Method
network Method
with O
discrete O
variables O
such O
as O
the O
REINFORCE Method
[ O
reference O
][ O
reference O
] O
and O
the O
straight Method
- Method
through Method
estimator Method
[ O
reference O
][ O
reference O
] O
, O
we O
use O
the O
straightthrough Method
estimator Method
to O
train O
our O
model O
. O
The O
straight Method
- Method
through Method
estimator Method
is O
a O
biased Method
estimator Method
because O
the O
non O
- O
differentiable O
function O
used O
in O
the O
forward O
pass O
( O
i.e. O
, O
the O
step O
function O
in O
our O
case O
) O
is O
replaced O
by O
a O
differentiable O
function O
during O
the O
backward O
pass O
( O
i.e. O
, O
the O
hard O
sigmoid O
function O
in O
our O
case O
) O
. O
The O
straight Method
- Method
through Method
estimator Method
, O
however O
, O
is O
much O
simpler O
and O
often O
works O
more O
efficiently O
in O
practice O
than O
other O
unbiased Method
but Method
high Method
- Method
variance Method
estimators Method
such O
as O
the O
REINFORCE Method
. O
The O
straight Method
- Method
through Method
estimator Method
has O
also O
been O
used O
in O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
The O
Slope Method
Annealing Method
Trick Method
. O
In O
our O
experiment O
, O
we O
use O
the O
slope Method
annealing Method
trick Method
to O
reduce O
the O
bias O
of O
the O
straight Method
- Method
through Method
estimator Method
. O
The O
idea O
is O
to O
reduce O
the O
discrepancy O
between O
the O
two O
functions O
used O
during O
the O
forward O
pass O
and O
the O
backward O
pass O
. O
That O
is O
, O
by O
gradually O
increasing O
the O
slope O
a O
of O
the O
hard O
sigmoid O
function O
, O
we O
make O
the O
hard O
sigmoid O
be O
close O
to O
the O
step O
function O
. O
Note O
that O
starting O
with O
a O
high O
slope O
value O
from O
the O
beginning O
can O
make O
the O
training O
difficult O
while O
it O
is O
more O
applicable O
later O
when O
the O
model O
parameters O
become O
more O
stable O
. O
In O
our O
experiments O
, O
starting O
from O
slope O
a O
= O
1 O
, O
we O
slowly O
increase O
the O
slope O
until O
it O
reaches O
a O
threshold O
with O
an O
appropriate O
scheduling O
. O
section O
: O
EXPERIMENTS O
We O
evaluate O
the O
proposed O
model O
on O
two O
tasks O
, O
character Task
- Task
level Task
language Task
modelling Task
and O
handwriting Task
sequence Task
generation Task
. O
Character Method
- Method
level Method
language Method
modelling Method
is O
a O
representative O
example O
of O
discrete O
[ O
reference O
] O
1.67 O
MRNN O
[ O
reference O
] O
1.60 O
GF Method
- Method
LSTM Method
[ O
reference O
] O
1.58 O
Grid Method
- Method
LSTM Method
[ O
reference O
] O
1.47 O
MI Method
- Method
LSTM Method
1.44 O
Recurrent Method
Memory Method
Array Method
Structures Method
( O
Rocki O
, O
2016a O
) O
1.40 O
SF Method
- Method
LSTM Method
( O
Rocki O
, O
2016b O
) O
‡ O
1.37 O
HyperNetworks Method
[ O
reference O
] O
1.35 O
LayerNorm Method
HyperNetworks Method
[ O
reference O
] O
1.34 O
Recurrent Method
Highway Method
Networks Method
[ O
reference O
] O
1 O
sequence Method
modelling Method
, O
where O
the O
discrete O
symbols O
form O
a O
distinct O
hierarchical O
multiscale O
structure O
. O
The O
performance O
on O
real O
- O
valued O
sequences O
is O
tested O
on O
the O
handwriting Task
sequence Task
generation Task
in O
which O
a O
relatively O
clear O
hierarchical O
multiscale O
structure O
exists O
compared O
to O
other O
data O
such O
as O
speech O
signals O
. O
section O
: O
CHARACTER Task
- Task
LEVEL Task
LANGUAGE Task
MODELLING Task
A O
sequence Task
modelling Task
task Task
aims O
at O
learning O
the O
probability O
distribution O
over O
sequences O
by O
minimizing O
the O
negative O
log O
- O
likelihood O
of O
the O
training O
sequences O
: O
where O
θ O
is O
the O
model O
parameter O
, O
N O
is O
the O
number O
of O
training O
sequences O
, O
and O
T O
n O
is O
the O
length O
of O
the O
n O
- O
th O
sequence O
. O
A O
symbol O
at O
time O
t O
of O
sequence O
n O
is O
denoted O
by O
x O
n O
t O
, O
and O
x O
n O
< O
t O
denotes O
all O
previous O
symbols O
at O
time O
t. O
We O
evaluate O
our O
model O
on O
three O
benchmark O
text O
corpora O
: O
( O
1 O
) O
Penn O
Treebank O
, O
( O
2 O
) O
Text8 Material
and O
( O
3 O
) O
Hutter Material
Prize Material
Wikipedia Material
. O
We O
use O
the O
bits Metric
- Metric
per Metric
- Metric
character Metric
( O
BPC Metric
) O
, O
E O
[ O
− O
log O
2 O
p O
( O
x O
t O
+ O
1 O
| O
x O
≤t O
) O
] O
, O
as O
the O
evaluation Metric
metric Metric
. O
Model O
We O
use O
a O
model O
consisting O
of O
an O
input Method
embedding Method
layer Method
, O
an O
RNN Method
module Method
and O
an O
output Method
module Method
. O
The O
input Method
embedding Method
layer Method
maps O
each O
input O
symbol O
into O
128 O
- O
dimensional O
continuous O
vector O
without O
using O
any O
non O
- O
linearity O
. O
The O
RNN Method
module Method
is O
the O
HM Method
- Method
LSTM Method
, O
described O
in O
Section O
3 O
, O
with O
three O
layers O
. O
The O
output Method
module Method
is O
a O
feedforward Method
neural Method
network Method
with O
two O
layers O
, O
an O
output Method
embedding Method
layer Method
and O
a O
softmax Method
layer Method
. O
Figure O
2 O
( O
right O
) O
shows O
a O
diagram O
of O
the O
output O
module O
. O
At O
each O
time O
step O
, O
the O
output Method
embedding Method
layer Method
receives O
the O
hidden O
states O
of O
the O
three O
RNN Method
layers Method
as O
input O
. O
In O
order O
to O
adaptively O
control O
the O
importance O
of O
each O
layer O
at O
each O
time O
step O
, O
we O
also O
introduce O
three O
scalar O
gating O
units O
g O
t O
∈ O
R O
to O
each O
of O
the O
layer O
outputs O
: O
where O
w O
∈ O
R O
L O
= O
1 O
dim O
( O
h O
) O
is O
the O
weight O
parameter O
. O
The O
output O
embedding O
h O
e O
t O
is O
computed O
by O
: O
where O
L O
= O
3 O
and O
ReLU O
( O
x O
) O
= O
max O
( O
0 O
, O
x O
) O
[ O
reference O
] O
. O
Finally O
, O
the O
probability O
distribution O
for O
the O
next O
target O
character O
is O
computed O
by O
the O
softmax O
function O
, O
softmax O
( O
xj O
) O
= O
, O
where O
each O
output O
class O
is O
a O
character O
. O
section O
: O
Text8 Material
Model O
BPC Metric
td O
- O
LSTM Method
1.63 O
HF O
- O
MRNN O
1.54 O
MI Method
- Method
RNN Method
1.52 Method
Skipping Method
- Method
RNN Method
[ O
reference O
] O
Table O
2 O
: O
BPC Metric
on O
the O
Text8 Material
test Material
set Material
. O
Penn O
Treebank O
We O
process O
the O
Penn O
Treebank O
dataset O
[ O
reference O
] O
by O
following O
the O
procedure O
introduced O
in O
. O
Each O
update O
is O
done O
by O
using O
a O
mini O
- O
batch O
of O
64 O
examples O
of O
length O
100 O
to O
prevent O
the O
memory O
overflow O
problem O
when O
unfolding O
the O
RNN Method
in O
time O
for O
backpropagation Method
. O
The O
last O
hidden O
state O
of O
a O
sequence O
is O
used O
to O
initialize O
the O
hidden O
state O
of O
the O
next O
sequence O
to O
approximate O
the O
full Method
backpropagation Method
. O
We O
train O
the O
model O
using O
Adam Method
[ O
reference O
] O
with O
an O
initial O
learning Metric
rate Metric
of O
0.002 O
. O
We O
divide O
the O
learning Metric
rate Metric
by O
a O
factor O
of O
50 O
when O
the O
validation O
negative O
log O
- O
likelihood O
stopped O
decreasing O
. O
The O
norm O
of O
the O
gradient O
is O
clipped O
with O
a O
threshold O
of O
1 O
[ O
reference O
][ O
reference O
] O
. O
We O
also O
apply O
layer Method
normalization Method
[ O
reference O
] O
) O
to O
our O
models O
. O
For O
all O
of O
the O
character Task
- Task
level Task
language Task
modelling Task
experiments O
, O
we O
apply O
the O
same O
procedure O
, O
but O
only O
change O
the O
number O
of O
hidden O
units O
, O
mini O
- O
batch O
size O
and O
the O
initial O
learning Metric
rate Metric
. O
For O
the O
Penn O
Treebank O
dataset O
, O
we O
use O
512 O
units O
in O
each O
layer O
of O
the O
HM Method
- Method
LSTM Method
and O
for O
the O
output Method
embedding Method
layer Method
. O
In O
Table O
1 O
( O
left O
) O
, O
we O
compare O
the O
test O
BPCs Metric
of O
four O
variants O
of O
our O
model O
to O
other O
baseline O
models O
. O
Note O
that O
the O
HM Method
- Method
LSTM Method
using O
the O
step O
function O
for O
the O
hard O
boundary O
decision O
outperforms O
the O
others O
using O
either O
sampling O
or O
soft O
boundary O
decision O
( O
i.e. O
, O
hard O
sigmoid O
) O
. O
The O
test O
BPC Metric
is O
further O
improved O
with O
the O
slope Method
annealing Method
trick Method
, O
which O
reduces O
the O
bias O
of O
the O
straight Method
- Method
through Method
estimator Method
. O
We O
increased O
the O
slope O
a O
with O
the O
following O
schedule O
a O
= O
min O
( O
5 O
, O
1 O
+ O
0.04 O
· O
N O
epoch O
) O
, O
where O
N O
epoch O
is O
the O
maximum O
number O
of O
epochs O
. O
The O
HM Method
- Method
LSTM Method
achieves O
test Metric
BPC Metric
score Metric
of O
1.24 O
. O
For O
the O
remaining O
tasks O
, O
we O
fixed O
the O
hard O
boundary O
decision O
using O
the O
step O
function O
without O
slope Method
annealing Method
due O
to O
the O
difficulty O
of O
finding O
a O
good O
annealing Method
schedule Method
on O
large O
- O
scale O
datasets O
. O
section O
: O
Text8 Material
The O
Text8 Material
dataset Material
( O
Mahoney O
, O
2009 O
) O
consists O
of O
100 O
M O
characters O
extracted O
from O
the O
Wikipedia O
corpus O
. O
Text8 Material
contains O
only O
alphabets O
and O
spaces O
, O
and O
thus O
we O
have O
total O
27 O
symbols O
. O
In O
order O
to O
compare O
with O
other O
previous O
works O
, O
we O
follow O
the O
data O
splits O
used O
in O
. O
We O
use O
1024 O
units O
for O
each O
HM Method
- Method
LSTM Method
layer Method
and O
2048 Method
units Method
for O
the O
output Method
embedding Method
layer Method
. O
The O
mini O
- O
batch O
size O
and O
the O
initial O
learning Metric
rate Metric
are O
set O
to O
128 O
and O
0.001 O
, O
respectively O
. O
The O
results O
are O
shown O
in O
Table O
2 O
. O
The O
HM Method
- Method
LSTM Method
obtains O
the O
state O
- O
of O
- O
the O
- O
art O
test O
BPC Metric
1.29 O
. O
Hutter Material
Prize Material
Wikipedia Material
The O
Hutter Material
Prize Material
Wikipedia Material
( O
enwik8 Material
) O
dataset O
[ O
reference O
] O
contains O
205 O
symbols O
including O
XML O
markups O
and O
special O
characters O
. O
We O
follow O
the O
data O
splits O
used O
in O
[ O
reference O
] O
where O
the O
first O
90 O
M O
characters O
are O
used O
to O
train O
the O
model O
, O
the O
next O
5 O
M O
characters O
for O
validation O
, O
and O
the O
remainders O
for O
the O
test O
set O
. O
We O
use O
the O
same O
model O
size O
, O
mini O
- O
batch O
size O
and O
the O
initial O
learning Metric
rate Metric
as O
in O
the O
Text8 Material
. O
In O
Table O
1 O
( O
right O
) O
, O
we O
show O
the O
HM Method
- Method
LSTM Method
achieving O
the O
test O
BPC Metric
1.32 O
, O
which O
is O
a O
tie O
with O
the O
state O
- O
of O
- O
the O
- O
art O
result O
among O
the O
neural Method
models Method
. O
Although O
the O
neural Method
models Method
, O
show O
remarkable O
performances O
, O
their O
compression Task
performance O
is O
still O
behind O
the O
best O
models O
such O
as O
PAQ8hp12 Method
[ O
reference O
] O
and O
decomp8 Method
[ O
reference O
] O
. O
Visualizing O
Learned O
Hierarchical O
Multiscale O
Structure O
In O
Figure O
3 O
and O
4 O
, O
we O
visualize O
the O
boundaries O
detected O
by O
the O
boundary Method
detectors Method
of O
the O
HM Method
- Method
LSTM Method
while O
reading O
a O
character O
sequence O
of O
total O
length O
270 O
taken O
from O
the O
validation O
set O
of O
either O
the O
Penn O
Treebank O
or O
Hutter Material
Prize Material
Wikipedia Material
dataset Material
. O
Due O
to O
the O
page O
width O
limit O
, O
the O
figure O
contains O
the O
sequence O
partitioned O
into O
three O
segments O
of O
length O
90 O
. O
The O
white O
blocks O
indicate O
boundaries O
z O
t O
= O
1 O
while O
the O
black O
blocks O
indicate O
the O
non O
- O
boundaries O
z O
t O
= O
0 O
. O
Interestingly O
in O
both O
figures O
, O
we O
can O
observe O
that O
the O
boundary O
detector O
of O
the O
first O
layer O
, O
z O
1 O
, O
tends O
to O
be O
turned O
on O
when O
it O
sees O
a O
space O
or O
after O
it O
sees O
a O
space O
, O
which O
is O
a O
reasonable O
breakpoint O
to O
separate O
between O
words O
. O
This O
is O
somewhat O
surprising O
because O
the O
model O
self O
- O
organizes O
this O
structure O
without O
any O
explicit O
boundary O
information O
. O
In O
Figure O
3 O
, O
we O
observe O
that O
the O
z O
1 O
tends O
to O
detect O
the O
boundaries O
of O
the O
words O
but O
also O
fires O
within O
the O
words O
, O
where O
the O
z O
2 O
tends O
to O
fire O
when O
it O
sees O
either O
an O
end O
of O
a O
word O
or O
2 O
, O
3 Method
- Method
grams Method
. O
In O
Figure O
4 O
, O
we O
also O
see O
flushing O
in O
the O
middle O
of O
a O
word O
, O
e.g. O
, O
" O
tele O
- O
FLUSH O
- O
phone O
" O
. O
Note O
that O
" O
tele O
" O
is O
a O
prefix O
after O
which O
a O
various O
number O
of O
postfixes O
can O
follow O
. O
From O
these O
, O
it O
seems O
that O
the O
model O
uses O
to O
some O
extent O
the O
concept O
of O
surprise O
to O
learn O
the O
boundary O
. O
Although O
interpretation O
of O
the O
second O
layer O
boundaries O
is O
not O
as O
apparent O
as O
the O
first O
layer O
boundaries O
, O
it O
seems O
to O
segment O
at O
reasonable O
semantic O
/ O
syntactic O
boundaries O
, O
e.g. O
, O
" O
consumers O
may O
" O
- O
"want O
to O
move O
their O
telephones O
a O
" O
- O
"little O
closer O
to O
the O
tv O
set O
< O
unk O
> O
" O
, O
and O
so O
on O
. O
Another O
remarkable O
point O
is O
the O
fact O
that O
we O
do O
not O
pose O
any O
constraint O
on O
the O
number O
of O
boundaries O
that O
the O
model O
can O
fire O
up O
. O
The O
model O
, O
however O
, O
learns O
that O
it O
is O
more O
beneficial O
to O
delay O
the O
information O
ejection O
to O
some O
extent O
. O
This O
is O
somewhat O
counterintuitive O
because O
it O
might O
look O
more O
beneficial O
to O
feed O
the O
fresh O
update O
to O
the O
upper O
layers O
at O
every O
time O
step O
without O
any O
delay O
. O
We O
conjecture O
the O
reason O
that O
the O
model O
works O
in O
this O
way O
is O
due O
to O
the O
FLUSH Method
operation Method
that O
poses O
an O
implicit O
constraint O
on O
the O
frequency Task
of Task
boundary Task
detection Task
, O
because O
it O
contains O
both O
a O
reward O
( O
feeding O
fresh O
information O
to O
upper O
layers O
) O
and O
a O
penalty O
( O
erasing O
accumulated O
information O
) O
. O
The O
model O
finds O
an O
optimal O
balance O
between O
the O
reward O
and O
the O
penalty O
. O
To O
understand O
the O
update Method
mechanism Method
more O
intuitively O
, O
in O
Figure O
4 O
, O
we O
also O
depict O
the O
heatmap O
of O
the O
2 O
- O
norm O
of O
the O
hidden O
states O
along O
with O
the O
states O
of O
the O
boundary Method
detectors Method
. O
As O
we O
expect O
, O
we O
can O
see O
that O
there O
is O
no O
change O
in O
the O
norm O
value O
within O
segments O
due O
to O
the O
COPY Method
operation Method
. O
Also O
, O
the O
color O
of O
h O
1 O
changes O
quickly O
( O
at O
every O
time O
step O
) O
because O
there O
is O
no O
COPY O
operation O
in O
the O
first O
layer O
. O
The O
color O
of O
h O
2 O
changes O
less O
frequently O
based O
on O
the O
states O
of O
z O
1 O
t O
and O
z O
2 O
t−1 O
. O
The O
color O
of O
h O
3 O
changes O
even O
slowly O
, O
i.e. O
, O
only O
when O
z O
2 O
t O
= O
1 O
. O
A O
notable O
advantage O
of O
the O
proposed O
architecture O
is O
that O
the O
internal O
process O
of O
the O
RNN Method
becomes O
more O
interpretable O
. O
For O
example O
, O
we O
can O
substitute O
the O
states O
of O
z O
1 O
t O
and O
z O
2 O
t−1 O
into O
Eq O
. O
2 O
and O
infer O
which O
operation O
among O
the O
UPDATE O
, O
COPY O
and O
FLUSH O
was O
applied O
to O
the O
second O
layer O
at O
time O
step O
t. O
We O
can O
also O
inspect O
the O
update O
frequencies O
of O
the O
layers O
simply O
by O
counting O
how O
many O
UPDATE O
and O
FLUSH O
operations O
were O
made O
in O
each O
layer O
. O
For O
example O
in O
Figure O
4 O
, O
we O
see O
that O
the O
first O
layer O
updates O
at O
every O
time O
step O
( O
which O
is O
270 O
UPDATE O
operations O
) O
, O
the O
second O
layer O
updates O
56 O
times O
, O
Figure O
5 O
: O
The O
visualization O
by O
segments O
based O
on O
either O
the O
given O
pen O
- O
tip O
location O
or O
states O
of O
the O
z O
2 O
. O
and O
only O
9 O
updates O
has O
made O
in O
the O
third O
layer O
. O
Note O
that O
, O
by O
design O
, O
the O
first O
layer O
performs O
UPDATE O
operation O
at O
every O
time O
step O
and O
then O
the O
number O
of O
UPDATE O
operations O
decreases O
as O
the O
layer O
level O
increases O
. O
In O
this O
example O
, O
the O
total O
number O
of O
updates O
is O
335 O
for O
the O
HM Method
- Method
LSTM Method
which O
is O
60 O
% O
of O
reduction O
from O
the O
810 O
updates O
of O
the O
standard O
RNN Method
architecture Method
. O
section O
: O
HANDWRITING Task
SEQUENCE Task
GENERATION Task
We O
extend O
the O
evaluation O
of O
the O
HM Method
- Method
LSTM Method
to O
a O
real Task
- Task
valued Task
sequence Task
modelling Task
task Task
using O
IAMOnDB O
[ O
reference O
] O
dataset O
. O
The O
IAM O
- O
OnDB O
dataset O
consists O
of O
12 O
, O
179 O
handwriting O
examples O
, O
each O
of O
which O
is O
a O
sequence O
of O
( O
x O
, O
y O
) O
coordinate O
and O
a O
binary O
indicator O
p O
for O
pen O
- O
tip O
location O
, O
giving O
us O
( O
x O
1:T O
n O
, O
y O
1:T O
n O
, O
p O
1:T O
n O
) O
, O
where O
n O
is O
an O
index O
of O
a O
sequence O
. O
At O
each O
time O
step O
, O
the O
model O
receives O
( O
x O
t O
, O
y O
t O
, O
p O
t O
) O
, O
and O
the O
goal O
is O
to O
predict O
( O
x O
t O
+ O
1 O
, O
y O
t O
+ O
1 O
, O
p O
t O
+ O
1 O
) O
. O
The O
pen O
- O
up O
( O
p O
t O
= O
1 O
) O
indicates O
an O
end O
of O
a O
stroke O
, O
and O
the O
pen O
- O
down O
( O
p O
t O
= O
0 O
) O
indicates O
that O
a O
stroke O
is O
in O
progress O
. O
There O
is O
usually O
a O
large O
shift O
in O
the O
( O
x O
, O
y O
) O
coordinate O
to O
start O
a O
new O
stroke O
after O
the O
pen O
- O
up O
happens O
. O
We O
remove O
all O
sequences O
whose O
length O
is O
shorter O
than O
300 O
. O
This O
leaves O
us O
10 O
, O
465 O
sequences O
for O
training O
, O
581 O
for O
validation Task
, O
582 O
for O
test O
. O
The O
average O
length O
of O
the O
sequences O
is O
648 O
. O
We O
normalize O
the O
range O
of O
the O
( O
x O
, O
y O
) O
coordinates O
separately O
with O
the O
mean O
and O
standard O
deviation O
obtained O
from O
the O
training O
set O
. O
We O
use O
the O
mini O
- O
batch O
size O
of O
32 O
, O
and O
the O
initial O
learning Metric
rate Metric
is O
set O
to O
0.0003 O
. O
We O
use O
the O
same O
model Method
architecture Method
as O
used O
in O
the O
character Method
- Method
level Method
language Method
model Method
, O
except O
that O
the O
output Method
layer Method
is O
modified O
to O
predict O
real O
- O
valued O
outputs O
. O
We O
use O
the O
mixture Method
density Method
network Method
as O
the O
output O
layer O
following O
[ O
reference O
] O
, O
and O
use O
400 O
units O
for O
each O
HM Method
- Method
LSTM Method
layer Method
and O
for O
the O
output Method
embedding Method
layer Method
. O
In O
Table O
3 O
, O
we O
compare O
the O
log Metric
- Metric
likelihood Metric
averaged O
over O
the O
test O
sequences O
of O
the O
IAM O
- O
OnDB O
dataset O
. O
We O
observe O
that O
the O
HM Method
- Method
LSTM Method
outperforms O
the O
standard O
LSTM Method
. O
The O
slope Method
annealing Method
trick Method
further O
improves O
the O
test Metric
log Metric
- Metric
likelihood Metric
of O
the O
HM Method
- Method
LSTM Method
into O
1167 O
in O
our O
setting O
. O
In O
this O
experiment O
, O
we O
increased O
the O
slope O
a O
with O
the O
following O
schedule O
a O
= O
min O
( O
3 O
, O
1 O
+ O
0.004 O
· O
N O
epoch O
) O
. O
In O
Figure O
5 O
, O
we O
let O
the O
HM Method
- Method
LSTM Method
to O
read O
a O
randomly O
picked O
validation O
sequence O
and O
present O
the O
visualization O
of O
handwriting O
examples O
by O
segments O
based O
on O
either O
the O
states O
of O
z O
2 O
or O
the O
states O
of O
pen O
- O
tip O
location O
3 O
. O
section O
: O
CONCLUSION O
In O
this O
paper O
, O
we O
proposed O
the O
HM Method
- Method
RNN Method
that O
can O
capture O
the O
latent O
hierarchical O
structure O
of O
the O
sequences O
. O
We O
introduced O
three O
types O
of O
operations O
to O
the O
RNN Method
, O
which O
are O
the O
COPY O
, O
UPDATE O
and O
FLUSH O
operations O
. O
In O
order O
to O
implement O
these O
operations O
, O
we O
introduced O
a O
set O
of O
binary O
variables O
and O
a O
novel O
update Method
rule Method
that O
is O
dependent O
on O
the O
states O
of O
these O
binary O
variables O
. O
Each O
binary O
variable O
is O
learned O
to O
find O
segments O
at O
its O
level O
, O
therefore O
, O
we O
call O
this O
binary O
variable O
, O
a O
boundary Method
detector Method
. O
On O
the O
character Task
- Task
level Task
language Task
modelling Task
, O
the O
HM Method
- Method
LSTM Method
achieved O
state O
- O
of O
- O
the O
- O
art O
result O
on O
the O
Text8 Material
dataset Material
and O
comparable O
results O
to O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
Penn O
Treebank O
and O
Hutter Material
Prize Material
Wikipedia Material
datasets Material
. O
Also O
, O
the O
HM Method
- Method
LSTM Method
outperformed O
the O
standard O
LSTM Method
on O
the O
handwriting Task
sequence Task
generation Task
. O
Our O
results O
and O
analysis O
suggest O
that O
the O
proposed O
HM Method
- Method
RNN Method
can O
discover O
the O
latent O
hierarchical O
structure O
of O
the O
sequences O
and O
can O
learn O
efficient O
hierarchical Method
multiscale Method
representation Method
that O
leads O
to O
better O
generalization Task
performance O
. O
section O
: O
section O
: O
ACKNOWLEDGMENTS O
The O
authors O
would O
like O
to O
thank O
Alex O
Graves O
, O
Tom O
Schaul O
and O
Hado O
van O
Hasselt O
for O
their O
fruitful O
comments O
and O
discussion O
. O
We O
acknowledge O
the O
support O
of O
the O
following O
agencies O
for O
research O
funding O
and O
computing O
support O
: O
Ubisoft O
, O
Samsung O
, O
IBM O
, O
Facebook O
, O
Google O
, O
Microsoft O
, O
NSERC O
, O
Calcul O
Québec O
, O
Compute O
Canada O
, O
the O
Canada O
Research O
Chairs O
and O
CIFAR O
. O
The O
authors O
thank O
the O
developers O
of O
Theano O
[ O
reference O
] O
. O
JC O
would O
like O
to O
thank O
Arnaud O
Bergenon O
and O
Frédéric O
Bastien O
for O
their O
technical O
support O
. O
JC O
would O
also O
like O
to O
thank O
Guillaume O
Alain O
, O
Kyle O
Kastner O
and O
David O
Ha O
for O
providing O
us O
useful O
pieces O
of O
code O
. O
section O
: O
