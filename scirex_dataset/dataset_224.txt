GRAPH	Method
ATTENTION	Method
NETWORKS	Method
section	O
:	O
ABSTRACT	O
We	O
present	O
graph	Method
attention	Method
networks	Method
(	O
GATs	Method
)	O
,	O
novel	O
neural	Method
network	Method
architectures	Method
that	O
operate	O
on	O
graph	O
-	O
structured	O
data	O
,	O
leveraging	O
masked	Method
self	Method
-	Method
attentional	Method
layers	Method
to	O
address	O
the	O
shortcomings	O
of	O
prior	O
methods	O
based	O
on	O
graph	Method
convolutions	Method
or	O
their	O
approximations	O
.	O
By	O
stacking	Method
layers	Method
in	O
which	O
nodes	O
are	O
able	O
to	O
attend	O
over	O
their	O
neighborhoods	O
'	O
features	O
,	O
we	O
enable	O
(	O
implicitly	O
)	O
specifying	O
different	O
weights	O
to	O
different	O
nodes	O
in	O
a	O
neighborhood	O
,	O
without	O
requiring	O
any	O
kind	O
of	O
costly	O
matrix	O
operation	O
(	O
such	O
as	O
inversion	Task
)	O
or	O
depending	O
on	O
knowing	O
the	O
graph	O
structure	O
upfront	O
.	O
In	O
this	O
way	O
,	O
we	O
address	O
several	O
key	O
challenges	O
of	O
spectral	Method
-	Method
based	Method
graph	Method
neural	Method
networks	Method
simultaneously	O
,	O
and	O
make	O
our	O
model	O
readily	O
applicable	O
to	O
inductive	Task
as	O
well	O
as	O
transductive	Task
problems	Task
.	O
Our	O
GAT	Method
models	O
have	O
achieved	O
or	O
matched	O
state	O
-	O
of	O
-	O
theart	O
results	O
across	O
four	O
established	O
transductive	Task
and	Task
inductive	Task
graph	Task
benchmarks	Task
:	O
the	O
Cora	Material
,	O
Citeseer	Material
and	O
Pubmed	Material
citation	O
network	O
datasets	O
,	O
as	O
well	O
as	O
a	O
proteinprotein	O
interaction	O
dataset	O
(	O
wherein	O
test	O
graphs	O
remain	O
unseen	O
during	O
training	O
)	O
.	O
section	O
:	O
INTRODUCTION	O
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNNs	Method
)	O
have	O
been	O
successfully	O
applied	O
to	O
tackle	O
problems	O
such	O
as	O
image	Task
classification	Task
[	O
reference	O
]	O
,	O
semantic	Task
segmentation	Task
[	O
reference	O
]	O
or	O
machine	Task
translation	Task
[	O
reference	O
]	O
,	O
where	O
the	O
underlying	O
data	Method
representation	Method
has	O
a	O
grid	O
-	O
like	O
structure	O
.	O
These	O
architectures	O
efficiently	O
reuse	O
their	O
local	Method
filters	Method
,	O
with	O
learnable	O
parameters	O
,	O
by	O
applying	O
them	O
to	O
all	O
the	O
input	O
positions	O
.	O
However	O
,	O
many	O
interesting	O
tasks	O
involve	O
data	O
that	O
can	O
not	O
be	O
represented	O
in	O
a	O
grid	O
-	O
like	O
structure	O
and	O
that	O
instead	O
lies	O
in	O
an	O
irregular	O
domain	O
.	O
This	O
is	O
the	O
case	O
of	O
3D	O
meshes	O
,	O
social	O
networks	O
,	O
telecommunication	O
networks	O
,	O
biological	O
networks	O
or	O
brain	O
connectomes	O
.	O
Such	O
data	O
can	O
usually	O
be	O
represented	O
in	O
the	O
form	O
of	O
graphs	O
.	O
There	O
have	O
been	O
several	O
attempts	O
in	O
the	O
literature	O
to	O
extend	O
neural	Method
networks	Method
to	O
deal	O
with	O
arbitrarily	O
structured	O
graphs	O
.	O
Early	O
work	O
used	O
recursive	Method
neural	Method
networks	Method
to	O
process	O
data	O
represented	O
in	O
graph	O
domains	O
as	O
directed	O
acyclic	O
graphs	O
[	O
reference	O
][	O
reference	O
]	O
.	O
Graph	Method
Neural	Method
Networks	Method
(	O
GNNs	Method
)	O
were	O
introduced	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
as	O
a	O
generalization	Method
of	Method
recursive	Method
neural	Method
networks	Method
that	O
can	O
directly	O
deal	O
with	O
a	O
more	O
general	O
class	O
of	O
graphs	O
,	O
e.g.	O
cyclic	O
,	O
directed	O
and	O
undirected	O
graphs	O
.	O
GNNs	Method
consist	O
of	O
an	O
iterative	Method
process	Method
,	O
which	O
propagates	O
the	O
node	O
states	O
until	O
equilibrium	O
;	O
followed	O
by	O
a	O
neural	Method
network	Method
,	O
which	O
produces	O
an	O
output	O
for	O
each	O
node	O
Nevertheless	O
,	O
there	O
is	O
an	O
increasing	O
interest	O
in	O
generalizing	O
convolutions	Method
to	O
the	O
graph	O
domain	O
.	O
Advances	O
in	O
this	O
direction	O
are	O
often	O
categorized	O
as	O
spectral	Method
approaches	Method
and	O
non	Method
-	Method
spectral	Method
approaches	Method
.	O
On	O
one	O
hand	O
,	O
spectral	Method
approaches	Method
work	O
with	O
a	O
spectral	Method
representation	Method
of	Method
the	Method
graphs	Method
and	O
have	O
been	O
successfully	O
applied	O
in	O
the	O
context	O
of	O
node	Task
classification	Task
.	O
In	O
[	O
reference	O
]	O
,	O
the	O
convolution	Method
operation	Method
is	O
defined	O
in	O
the	O
Fourier	O
domain	O
by	O
computing	O
the	O
eigendecomposition	Method
of	Method
the	Method
graph	Method
Laplacian	Method
,	O
resulting	O
in	O
potentially	O
intense	O
computations	O
and	O
non	Task
-	Task
spatially	Task
localized	Task
filters	Task
.	O
These	O
issues	O
were	O
addressed	O
by	O
subsequent	O
works	O
.	O
[	O
reference	O
]	O
introduced	O
a	O
parameterization	O
of	O
the	O
spectral	Method
filters	Method
with	O
smooth	O
coefficients	O
in	O
order	O
to	O
make	O
them	O
spatially	O
localized	O
.	O
Later	O
,	O
[	O
reference	O
]	O
proposed	O
to	O
approximate	O
the	O
filters	O
by	O
means	O
of	O
a	O
Chebyshev	Method
expansion	Method
of	Method
the	Method
graph	Method
Laplacian	Method
,	O
removing	O
the	O
need	O
to	O
compute	O
the	O
eigenvectors	O
of	O
the	O
Laplacian	O
and	O
yielding	O
spatially	Method
localized	Method
filters	Method
.	O
Finally	O
,	O
[	O
reference	O
]	O
simplified	O
the	O
previous	O
method	O
by	O
restricting	O
the	O
filters	O
to	O
operate	O
in	O
a	O
1	O
-	O
step	O
neighborhood	O
around	O
each	O
node	O
.	O
However	O
,	O
in	O
all	O
of	O
the	O
aforementioned	O
spectral	Method
approaches	Method
,	O
the	O
learned	Method
filters	Method
depend	O
on	O
the	O
Laplacian	O
eigenbasis	O
,	O
which	O
depends	O
on	O
the	O
graph	O
structure	O
.	O
Thus	O
,	O
a	O
model	O
trained	O
on	O
a	O
specific	O
structure	O
can	O
not	O
be	O
directly	O
applied	O
to	O
a	O
graph	O
with	O
a	O
different	O
structure	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
have	O
non	Method
-	Method
spectral	Method
approaches	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
which	O
define	O
convolutions	Method
directly	O
on	O
the	O
graph	O
,	O
operating	O
on	O
groups	O
of	O
spatially	O
close	O
neighbors	O
.	O
One	O
of	O
the	O
challenges	O
of	O
these	O
approaches	O
is	O
to	O
define	O
an	O
operator	O
which	O
works	O
with	O
different	O
sized	O
neighborhoods	O
and	O
maintains	O
the	O
weight	O
sharing	O
property	O
of	O
CNNs	Method
.	O
In	O
some	O
cases	O
,	O
this	O
requires	O
learning	O
a	O
specific	O
weight	O
matrix	O
for	O
each	O
node	O
degree	O
[	O
reference	O
]	O
,	O
using	O
the	O
powers	O
of	O
a	O
transition	O
matrix	O
to	O
define	O
the	O
neighborhood	O
while	O
learning	O
weights	O
for	O
each	O
input	O
channel	O
and	O
neighborhood	O
degree	O
[	O
reference	O
]	O
,	O
or	O
extracting	O
and	O
normalizing	O
neighborhoods	O
containing	O
a	O
fixed	O
number	O
of	O
nodes	O
[	O
reference	O
]	O
.	O
[	O
reference	O
]	O
presented	O
mixture	Method
model	Method
CNNs	Method
(	O
MoNet	Method
)	O
,	O
a	O
spatial	Method
approach	Method
which	O
provides	O
a	O
unified	O
generalization	Method
of	Method
CNN	Method
architectures	Method
to	O
graphs	O
.	O
More	O
recently	O
,	O
[	O
reference	O
]	O
introduced	O
GraphSAGE	Method
,	O
a	O
method	O
for	O
computing	Task
node	Task
representations	Task
in	O
an	O
inductive	O
manner	O
.	O
This	O
technique	O
operates	O
by	O
sampling	O
a	O
fixed	O
-	O
size	O
neighborhood	O
of	O
each	O
node	O
,	O
and	O
then	O
performing	O
a	O
specific	O
aggregator	O
over	O
it	O
(	O
such	O
as	O
the	O
mean	O
over	O
all	O
the	O
sampled	O
neighbors	O
'	O
feature	O
vectors	O
,	O
or	O
the	O
result	O
of	O
feeding	O
them	O
through	O
a	O
recurrent	Method
neural	Method
network	Method
)	O
.	O
This	O
approach	O
has	O
yielded	O
impressive	O
performance	O
across	O
several	O
large	O
-	O
scale	O
inductive	O
benchmarks	O
.	O
Attention	Method
mechanisms	Method
have	O
become	O
almost	O
a	O
de	O
facto	O
standard	O
in	O
many	O
sequence	Task
-	Task
based	Task
tasks	Task
[	O
reference	O
][	O
reference	O
]	O
.	O
One	O
of	O
the	O
benefits	O
of	O
attention	Method
mechanisms	Method
is	O
that	O
they	O
allow	O
for	O
dealing	O
with	O
variable	O
sized	O
inputs	O
,	O
focusing	O
on	O
the	O
most	O
relevant	O
parts	O
of	O
the	O
input	O
to	O
make	O
decisions	O
.	O
When	O
an	O
attention	Method
mechanism	Method
is	O
used	O
to	O
compute	O
a	O
representation	O
of	O
a	O
single	O
sequence	O
,	O
it	O
is	O
commonly	O
referred	O
to	O
as	O
self	O
-	O
attention	O
or	O
intra	Task
-	Task
attention	Task
.	O
Together	O
with	O
Recurrent	Method
Neural	Method
Networks	Method
(	O
RNNs	Method
)	O
or	O
convolutions	Method
,	O
self	Method
-	Method
attention	Method
has	O
proven	O
to	O
be	O
useful	O
for	O
tasks	O
such	O
as	O
machine	Task
reading	Task
[	O
reference	O
]	O
and	O
learning	O
sentence	Task
representations	Task
[	O
reference	O
]	O
.	O
However	O
,	O
[	O
reference	O
]	O
showed	O
that	O
not	O
only	O
self	Method
-	Method
attention	Method
can	O
improve	O
a	O
method	O
based	O
on	O
RNNs	Method
or	O
convolutions	Method
,	O
but	O
also	O
that	O
it	O
is	O
sufficient	O
for	O
constructing	O
a	O
powerful	O
model	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
machine	Task
translation	Task
task	Task
.	O
Inspired	O
by	O
this	O
recent	O
work	O
,	O
we	O
introduce	O
an	O
attention	Method
-	Method
based	Method
architecture	Method
to	O
perform	O
node	Task
classification	Task
of	O
graph	O
-	O
structured	O
data	O
.	O
The	O
idea	O
is	O
to	O
compute	O
the	O
hidden	O
representations	O
of	O
each	O
node	O
in	O
the	O
graph	O
,	O
by	O
attending	O
over	O
its	O
neighbors	O
,	O
following	O
a	O
self	Method
-	Method
attention	Method
strategy	Method
.	O
The	O
attention	Method
architecture	Method
has	O
several	O
interesting	O
properties	O
:	O
(	O
1	O
)	O
the	O
operation	O
is	O
efficient	O
,	O
since	O
it	O
is	O
parallelizable	O
across	O
nodeneighbor	O
pairs	O
;	O
(	O
2	O
)	O
it	O
can	O
be	O
applied	O
to	O
graph	O
nodes	O
having	O
different	O
degrees	O
by	O
specifying	O
arbitrary	O
weights	O
to	O
the	O
neighbors	O
;	O
and	O
(	O
3	O
)	O
the	O
model	O
is	O
directly	O
applicable	O
to	O
inductive	Task
learning	Task
problems	Task
,	O
including	O
tasks	O
where	O
the	O
model	O
has	O
to	O
generalize	O
to	O
completely	O
unseen	O
graphs	O
.	O
We	O
validate	O
the	O
proposed	O
approach	O
on	O
four	O
challenging	O
benchmarks	O
:	O
Cora	Material
,	O
Citeseer	Material
and	O
Pubmed	Material
citation	O
networks	O
as	O
well	O
as	O
an	O
inductive	O
protein	O
-	O
protein	O
interaction	O
dataset	O
,	O
achieving	O
or	O
matching	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
that	O
highlight	O
the	O
potential	O
of	O
attention	Method
-	Method
based	Method
models	Method
when	O
dealing	O
with	O
arbitrarily	O
structured	O
graphs	O
.	O
It	O
is	O
worth	O
noting	O
that	O
,	O
as	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
our	O
work	O
can	O
also	O
be	O
reformulated	O
as	O
a	O
particular	O
instance	O
of	O
MoNet	Method
[	O
reference	O
]	O
.	O
Moreover	O
,	O
our	O
approach	O
of	O
sharing	O
a	O
neural	Method
network	Method
computation	Method
across	O
edges	O
is	O
reminiscent	O
of	O
the	O
formulation	O
of	O
relational	Method
networks	Method
[	O
reference	O
]	O
and	O
VAIN	O
[	O
reference	O
]	O
,	O
wherein	O
relations	O
between	O
objects	O
or	O
agents	O
are	O
aggregated	O
pair	O
-	O
wise	O
,	O
by	O
employing	O
a	O
shared	Method
mechanism	Method
.	O
Similarly	O
,	O
our	O
proposed	O
attention	Method
model	Method
can	O
be	O
connected	O
to	O
the	O
works	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
which	O
use	O
a	O
neighborhood	O
attention	O
operation	O
to	O
compute	O
attention	O
coefficients	O
between	O
different	O
objects	O
in	O
an	O
environment	O
.	O
Other	O
related	O
approaches	O
include	O
locally	Method
linear	Method
embedding	Method
(	O
LLE	Method
)	O
[	O
reference	O
]	O
and	O
memory	Method
networks	Method
[	O
reference	O
]	O
.	O
LLE	O
selects	O
a	O
fixed	O
number	O
of	O
neighbors	O
around	O
each	O
data	O
point	O
,	O
and	O
learns	O
a	O
weight	O
coefficient	O
for	O
each	O
neighbor	O
to	O
reconstruct	O
each	O
point	O
as	O
a	O
weighted	O
sum	O
of	O
its	O
neighbors	O
.	O
A	O
second	O
optimization	Method
step	Method
extracts	O
the	O
point	Method
's	Method
feature	Method
embedding	Method
.	O
Memory	Method
networks	Method
also	O
share	O
some	O
connections	O
with	O
our	O
work	O
,	O
in	O
particular	O
,	O
if	O
we	O
interpret	O
the	O
neighborhood	O
of	O
a	O
node	O
as	O
the	O
memory	O
,	O
which	O
is	O
used	O
to	O
compute	O
the	O
node	O
features	O
by	O
attending	O
over	O
its	O
values	O
,	O
and	O
then	O
is	O
updated	O
by	O
storing	O
the	O
new	O
features	O
in	O
the	O
same	O
position	O
.	O
section	O
:	O
GAT	Method
ARCHITECTURE	O
In	O
this	O
section	O
,	O
we	O
will	O
present	O
the	O
building	Method
block	Method
layer	Method
used	O
to	O
construct	O
arbitrary	Method
graph	Method
attention	Method
networks	Method
(	O
through	O
stacking	O
this	O
layer	O
)	O
,	O
and	O
directly	O
outline	O
its	O
theoretical	O
and	O
practical	O
benefits	O
and	O
limitations	O
compared	O
to	O
prior	O
work	O
in	O
the	O
domain	O
of	O
neural	Task
graph	Task
processing	Task
.	O
section	O
:	O
GRAPH	Method
ATTENTIONAL	Method
LAYER	Method
We	O
will	O
start	O
by	O
describing	O
a	O
single	O
graph	Method
attentional	Method
layer	Method
,	O
as	O
the	O
sole	O
layer	O
utilized	O
throughout	O
all	O
of	O
the	O
GAT	Method
architectures	O
used	O
in	O
our	O
experiments	O
.	O
The	O
particular	O
attentional	O
setup	O
utilized	O
by	O
us	O
closely	O
follows	O
the	O
work	O
of	O
[	O
reference	O
]-	O
but	O
the	O
framework	O
is	O
agnostic	O
to	O
the	O
particular	O
choice	O
of	O
attention	Method
mechanism	Method
.	O
The	O
input	O
to	O
our	O
layer	O
is	O
a	O
set	O
of	O
node	O
features	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
nodes	O
,	O
and	O
F	O
is	O
the	O
number	O
of	O
features	O
in	O
each	O
node	O
.	O
The	O
layer	O
produces	O
a	O
new	O
set	O
of	O
node	O
features	O
(	O
of	O
potentially	O
different	O
cardinality	O
In	O
order	O
to	O
obtain	O
sufficient	O
expressive	O
power	O
to	O
transform	O
the	O
input	O
features	O
into	O
higher	O
-	O
level	O
features	O
,	O
at	O
least	O
one	O
learnable	Method
linear	Method
transformation	Method
is	O
required	O
.	O
To	O
that	O
end	O
,	O
as	O
an	O
initial	O
step	O
,	O
a	O
shared	Method
linear	Method
transformation	Method
,	O
parametrized	O
by	O
a	O
weight	O
matrix	O
,	O
W	O
∈	O
R	O
F	O
×F	O
,	O
is	O
applied	O
to	O
every	O
node	O
.	O
We	O
then	O
perform	O
self	O
-	O
attention	O
on	O
the	O
nodes	O
-	O
a	O
shared	Method
attentional	Method
mechanism	Method
a	O
:	O
that	O
indicate	O
the	O
importance	O
of	O
node	O
j	O
's	O
features	O
to	O
node	O
i.	O
In	O
its	O
most	O
general	O
formulation	O
,	O
the	O
model	O
allows	O
every	O
node	O
to	O
attend	O
on	O
every	O
other	O
node	O
,	O
dropping	O
all	O
structural	O
information	O
.	O
We	O
inject	O
the	O
graph	O
structure	O
into	O
the	O
mechanism	O
by	O
performing	O
masked	Method
attention	Method
-	O
we	O
only	O
compute	O
e	O
ij	O
for	O
nodes	O
j	O
∈	O
N	O
i	O
,	O
where	O
N	O
i	O
is	O
some	O
neighborhood	O
of	O
node	O
i	O
in	O
the	O
graph	O
.	O
In	O
all	O
our	O
experiments	O
,	O
these	O
will	O
be	O
exactly	O
the	O
first	O
-	O
order	O
neighbors	O
of	O
i	O
(	O
including	O
i	O
)	O
.	O
To	O
make	O
coefficients	O
easily	O
comparable	O
across	O
different	O
nodes	O
,	O
we	O
normalize	O
them	O
across	O
all	O
choices	O
of	O
j	O
using	O
the	O
softmax	O
function	O
:	O
In	O
our	O
experiments	O
,	O
the	O
attention	Method
mechanism	Method
a	Method
is	O
a	O
single	Method
-	Method
layer	Method
feedforward	Method
neural	Method
network	Method
,	O
parametrized	O
by	O
a	O
weight	O
vector	O
a	O
∈	O
R	O
2F	O
,	O
and	O
applying	O
the	O
LeakyReLU	Method
nonlinearity	Method
(	O
with	O
negative	O
input	O
slope	O
α	O
=	O
0.2	O
)	O
.	O
Fully	O
expanded	O
out	O
,	O
the	O
coefficients	O
computed	O
by	O
the	O
attention	Method
mechanism	Method
(	O
illustrated	O
by	O
Figure	O
1	O
(	O
left	O
)	O
)	O
may	O
then	O
be	O
expressed	O
as	O
:	O
where	O
·	O
T	O
represents	O
transposition	O
and	O
is	O
the	O
concatenation	O
operation	O
.	O
Once	O
obtained	O
,	O
the	O
normalized	O
attention	O
coefficients	O
are	O
used	O
to	O
compute	O
a	O
linear	Method
combination	Method
of	O
the	O
features	O
corresponding	O
to	O
them	O
,	O
to	O
serve	O
as	O
the	O
final	O
output	O
features	O
for	O
every	O
node	O
(	O
after	O
potentially	O
The	O
attention	Method
mechanism	Method
a	O
(	O
W	O
h	O
i	O
,	O
W	O
h	O
j	O
)	O
employed	O
by	O
our	O
model	O
,	O
parametrized	O
by	O
a	O
weight	O
vector	O
a	O
∈	O
R	O
2F	O
,	O
applying	O
a	O
LeakyReLU	Method
activation	Method
.	O
Right	O
:	O
An	O
illustration	O
of	O
multihead	Task
attention	Task
(	O
with	O
K	O
=	O
3	O
heads	O
)	O
by	O
node	O
1	O
on	O
its	O
neighborhood	O
.	O
Different	O
arrow	O
styles	O
and	O
colors	O
denote	O
independent	O
attention	O
computations	O
.	O
The	O
aggregated	O
features	O
from	O
each	O
head	O
are	O
concatenated	O
or	O
averaged	O
to	O
obtain	O
h	O
1	O
.	O
applying	O
a	O
nonlinearity	Method
,	O
σ	O
)	O
:	O
To	O
stabilize	O
the	O
learning	Task
process	Task
of	O
self	Task
-	Task
attention	Task
,	O
we	O
have	O
found	O
extending	O
our	O
mechanism	O
to	O
employ	O
multi	Task
-	Task
head	Task
attention	Task
to	O
be	O
beneficial	O
,	O
similarly	O
to	O
[	O
reference	O
]	O
.	O
Specifically	O
,	O
K	O
independent	Method
attention	Method
mechanisms	Method
execute	O
the	O
transformation	O
of	O
Equation	O
4	O
,	O
and	O
then	O
their	O
features	O
are	O
concatenated	O
,	O
resulting	O
in	O
the	O
following	O
output	Method
feature	Method
representation	Method
:	O
where	O
represents	O
concatenation	O
,	O
α	O
k	O
ij	O
are	O
normalized	O
attention	O
coefficients	O
computed	O
by	O
the	O
k	O
-	O
th	O
attention	Method
mechanism	Method
(	O
a	O
k	O
)	O
,	O
and	O
W	O
k	O
is	O
the	O
corresponding	O
input	O
linear	O
transformation	O
's	O
weight	O
matrix	O
.	O
Note	O
that	O
,	O
in	O
this	O
setting	O
,	O
the	O
final	O
returned	O
output	O
,	O
h	O
,	O
will	O
consist	O
of	O
KF	O
features	O
(	O
rather	O
than	O
F	O
)	O
for	O
each	O
node	O
.	O
Specially	O
,	O
if	O
we	O
perform	O
multi	Task
-	Task
head	Task
attention	Task
on	O
the	O
final	O
(	O
prediction	O
)	O
layer	O
of	O
the	O
network	O
,	O
concatenation	Task
is	O
no	O
longer	O
sensible	O
-	O
instead	O
,	O
we	O
employ	O
averaging	Method
,	O
and	O
delay	O
applying	O
the	O
final	O
nonlinearity	Method
(	O
usually	O
a	O
softmax	Method
or	Method
logistic	Method
sigmoid	Method
for	O
classification	Task
problems	Task
)	O
until	O
then	O
:	O
The	O
aggregation	Method
process	Method
of	O
a	O
multi	Method
-	Method
head	Method
graph	Method
attentional	Method
layer	Method
is	O
illustrated	O
by	O
Figure	O
1	O
(	O
right	O
)	O
.	O
section	O
:	O
COMPARISONS	O
TO	O
RELATED	O
WORK	O
The	O
graph	Method
attentional	Method
layer	Method
described	O
in	O
subsection	O
2.1	O
directly	O
addresses	O
several	O
issues	O
that	O
were	O
present	O
in	O
prior	O
approaches	O
to	O
modelling	O
graph	Task
-	Task
structured	Task
data	Task
with	O
neural	Method
networks	Method
:	O
•	O
Computationally	O
,	O
it	O
is	O
highly	O
efficient	O
:	O
the	O
operation	O
of	O
the	O
self	Method
-	Method
attentional	Method
layer	Method
can	O
be	O
parallelized	O
across	O
all	O
edges	O
,	O
and	O
the	O
computation	O
of	O
output	O
features	O
can	O
be	O
parallelized	O
across	O
all	O
nodes	O
.	O
No	O
eigendecompositions	Method
or	O
similar	O
costly	O
matrix	Method
operations	Method
are	O
required	O
.	O
The	O
time	Metric
complexity	Metric
of	O
a	O
single	O
GAT	Method
attention	Method
head	O
computing	O
F	O
features	O
may	O
be	O
expressed	O
as	O
O	O
(	O
|V	O
|F	O
F	O
+	O
|E|F	O
)	O
,	O
where	O
F	O
is	O
the	O
number	O
of	O
input	O
features	O
,	O
and	O
|V	O
|	O
and	O
|E|	O
are	O
the	O
numbers	O
of	O
nodes	O
and	O
edges	O
in	O
the	O
graph	O
,	O
respectively	O
.	O
This	O
complexity	Metric
is	O
on	O
par	O
with	O
the	O
baseline	O
methods	O
such	O
as	O
Graph	Method
Convolutional	Method
Networks	Method
(	O
GCNs	Method
)	O
[	O
reference	O
]	O
.	O
Applying	O
multi	Method
-	Method
head	Method
attention	Method
multiplies	O
the	O
storage	O
and	O
parameter	O
requirements	O
by	O
a	O
factor	O
of	O
K	O
,	O
while	O
the	O
individual	O
heads	O
'	O
computations	O
are	O
fully	O
independent	O
and	O
can	O
be	O
parallelized	O
.	O
•	O
As	O
opposed	O
to	O
GCNs	Method
,	O
our	O
model	O
allows	O
for	O
(	O
implicitly	O
)	O
assigning	O
different	O
importances	O
to	O
nodes	O
of	O
a	O
same	O
neighborhood	O
,	O
enabling	O
a	O
leap	O
in	O
model	O
capacity	O
.	O
Furthermore	O
,	O
analyzing	O
the	O
learned	O
attentional	O
weights	O
may	O
lead	O
to	O
benefits	O
in	O
interpretability	Task
,	O
as	O
was	O
the	O
case	O
in	O
the	O
machine	Task
translation	Task
domain	Task
(	O
e.g.	O
the	O
qualitative	Task
analysis	Task
of	O
[	O
reference	O
]	O
)	O
.	O
•	O
The	O
attention	Method
mechanism	Method
is	O
applied	O
in	O
a	O
shared	O
manner	O
to	O
all	O
edges	O
in	O
the	O
graph	O
,	O
and	O
therefore	O
it	O
does	O
not	O
depend	O
on	O
upfront	O
access	O
to	O
the	O
global	O
graph	O
structure	O
or	O
(	O
features	O
of	O
)	O
all	O
of	O
its	O
nodes	O
(	O
a	O
limitation	O
of	O
many	O
prior	O
techniques	O
)	O
.	O
This	O
has	O
several	O
desirable	O
implications	O
:	O
-	O
The	O
graph	O
is	O
not	O
required	O
to	O
be	O
undirected	O
(	O
we	O
may	O
simply	O
leave	O
out	O
computing	O
α	O
ij	O
if	O
edge	O
j	O
→	O
i	O
is	O
not	O
present	O
)	O
.	O
-	O
It	O
makes	O
our	O
technique	O
directly	O
applicable	O
to	O
inductive	Task
learning	Task
-	Task
including	Task
tasks	Task
where	O
the	O
model	O
is	O
evaluated	O
on	O
graphs	O
that	O
are	O
completely	O
unseen	O
during	O
training	O
.	O
•	O
The	O
recently	O
published	O
inductive	O
method	O
of	O
Hamilton	O
et	O
al	O
.	O
(	O
2017	O
)	O
samples	O
a	O
fixed	O
-	O
size	O
neighborhood	O
of	O
each	O
node	O
,	O
in	O
order	O
to	O
keep	O
its	O
computational	O
footprint	O
consistent	O
;	O
this	O
does	O
not	O
allow	O
it	O
access	O
to	O
the	O
entirety	O
of	O
the	O
neighborhood	O
while	O
performing	O
inference	Task
.	O
Moreover	O
,	O
this	O
technique	O
achieved	O
some	O
of	O
its	O
strongest	O
results	O
when	O
an	O
LSTM	Method
[	O
reference	O
])-	O
based	Method
neighborhood	Method
aggregator	Method
is	O
used	O
.	O
This	O
assumes	O
the	O
existence	O
of	O
a	O
consistent	O
sequential	O
node	O
ordering	O
across	O
neighborhoods	O
,	O
and	O
the	O
authors	O
have	O
rectified	O
it	O
by	O
consistently	O
feeding	O
randomly	O
-	O
ordered	O
sequences	O
to	O
the	O
LSTM	Method
.	O
Our	O
technique	O
does	O
not	O
suffer	O
from	O
either	O
of	O
these	O
issues	O
-	O
it	O
works	O
with	O
the	O
entirety	O
of	O
the	O
neighborhood	O
(	O
at	O
the	O
expense	O
of	O
a	O
variable	O
computational	Metric
footprint	Metric
,	O
which	O
is	O
still	O
on	O
-	O
par	O
with	O
methods	O
like	O
the	O
GCN	Method
)	O
,	O
and	O
does	O
not	O
assume	O
any	O
ordering	O
within	O
it	O
.	O
•	O
As	O
mentioned	O
in	O
Section	O
1	O
,	O
GAT	Method
can	O
be	O
reformulated	O
as	O
a	O
particular	O
instance	O
of	O
MoNet	Method
[	O
reference	O
]	O
.	O
More	O
specifically	O
,	O
setting	O
the	O
pseudo	O
-	O
coordinate	O
function	O
to	O
be	O
u	O
(	O
x	O
,	O
y	O
)	O
=	O
f	O
(	O
x	O
)	O
f	O
(	O
y	O
)	O
,	O
where	O
f	O
(	O
x	O
)	O
represent	O
(	O
potentially	O
MLP	Method
-	O
transformed	O
)	O
features	O
of	O
node	O
x	O
and	O
is	O
concatenation	O
;	O
and	O
the	O
weight	O
function	O
to	O
be	O
w	O
j	O
(	O
u	O
)	O
=	O
softmax	O
(	O
MLP	Method
(	O
u	O
)	O
)	O
(	O
with	O
the	O
softmax	O
performed	O
over	O
the	O
entire	O
neighborhood	O
of	O
a	O
node	O
)	O
would	O
make	O
MoNet	Method
's	Method
patch	Method
operator	Method
similar	O
to	O
ours	O
.	O
Nevertheless	O
,	O
one	O
should	O
note	O
that	O
,	O
in	O
comparison	O
to	O
previously	O
considered	O
MoNet	O
instances	O
,	O
our	O
model	O
uses	O
node	O
features	O
for	O
similarity	Task
computations	Task
,	O
rather	O
than	O
the	O
node	O
's	O
structural	O
properties	O
(	O
which	O
would	O
assume	O
knowing	O
the	O
graph	O
structure	O
upfront	O
)	O
.	O
We	O
were	O
able	O
to	O
produce	O
a	O
version	O
of	O
the	O
GAT	Method
layer	O
that	O
leverages	O
sparse	Method
matrix	Method
operations	Method
,	O
reducing	O
the	O
storage	Metric
complexity	Metric
to	O
linear	O
in	O
the	O
number	O
of	O
nodes	O
and	O
edges	O
and	O
enabling	O
the	O
execution	O
of	O
GAT	Method
models	O
on	O
larger	O
graph	O
datasets	O
.	O
However	O
,	O
the	O
tensor	Method
manipulation	Method
framework	Method
we	O
used	O
only	O
supports	O
sparse	Method
matrix	Method
multiplication	Method
for	O
rank	O
-	O
2	O
tensors	O
,	O
which	O
limits	O
the	O
batching	O
capabilities	O
of	O
the	O
layer	O
as	O
it	O
is	O
currently	O
implemented	O
(	O
especially	O
for	O
datasets	O
with	O
multiple	O
graphs	O
)	O
.	O
Appropriately	O
addressing	O
this	O
constraint	O
is	O
an	O
important	O
direction	O
for	O
future	O
work	O
.	O
Depending	O
on	O
the	O
regularity	O
of	O
the	O
graph	O
structure	O
in	O
place	O
,	O
GPUs	Method
may	O
not	O
be	O
able	O
to	O
offer	O
major	O
performance	O
benefits	O
compared	O
to	O
CPUs	Method
in	O
these	O
sparse	Task
scenarios	Task
.	O
It	O
should	O
also	O
be	O
noted	O
that	O
the	O
size	O
of	O
the	O
"	O
receptive	O
field	O
"	O
of	O
our	O
model	O
is	O
upper	O
-	O
bounded	O
by	O
the	O
depth	O
of	O
the	O
network	O
(	O
similarly	O
as	O
for	O
GCN	Method
and	Method
similar	Method
models	Method
)	O
.	O
Techniques	O
such	O
as	O
skip	O
connections	O
[	O
reference	O
]	O
could	O
be	O
readily	O
applied	O
for	O
appropriately	O
extending	O
the	O
depth	O
,	O
however	O
.	O
Lastly	O
,	O
parallelization	O
across	O
all	O
the	O
graph	O
edges	O
,	O
especially	O
in	O
a	O
distributed	O
manner	O
,	O
may	O
involve	O
a	O
lot	O
of	O
redundant	O
computation	O
,	O
as	O
the	O
neighborhoods	O
will	O
often	O
highly	O
overlap	O
in	O
graphs	O
of	O
interest	O
.	O
section	O
:	O
EVALUATION	O
We	O
have	O
performed	O
comparative	O
evaluation	O
of	O
GAT	Method
models	O
against	O
a	O
wide	O
variety	O
of	O
strong	O
baselines	O
and	O
previous	O
approaches	O
,	O
on	O
four	O
established	O
graph	O
-	O
based	O
benchmark	O
tasks	O
(	O
transductive	Task
as	O
well	O
as	O
inductive	Task
)	O
,	O
achieving	O
or	O
matching	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
all	O
of	O
them	O
.	O
This	O
section	O
summarizes	O
our	O
experimental	O
setup	O
,	O
results	O
,	O
and	O
a	O
brief	O
qualitative	O
analysis	O
of	O
a	O
GAT	Method
model	O
's	O
extracted	Method
feature	Method
representations	Method
.	O
section	O
:	O
DATASETS	O
Transductive	Method
learning	Method
We	O
utilize	O
three	O
standard	O
citation	O
network	O
benchmark	O
datasets	O
-	O
Cora	Material
,	O
Citeseer	Material
and	O
Pubmed	Material
[	O
reference	O
]-	O
and	O
closely	O
follow	O
the	O
transductive	O
experimental	O
setup	O
of	O
[	O
reference	O
]	O
.	O
In	O
all	O
of	O
these	O
datasets	O
,	O
nodes	O
correspond	O
to	O
documents	O
and	O
edges	O
to	O
(	O
undirected	O
)	O
citations	O
.	O
Node	O
features	O
correspond	O
to	O
elements	O
of	O
a	O
bag	Method
-	Method
of	Method
-	Method
words	Method
representation	Method
of	O
a	O
document	O
.	O
Each	O
node	O
has	O
a	O
class	O
label	O
.	O
We	O
allow	O
for	O
only	O
20	O
nodes	O
per	O
class	O
to	O
be	O
used	O
for	O
training	O
-	O
however	O
,	O
honoring	O
the	O
transductive	Method
setup	Method
,	O
the	O
training	Method
algorithm	Method
has	O
access	O
to	O
all	O
of	O
the	O
nodes	O
'	O
feature	O
vectors	O
.	O
The	O
predictive	O
power	O
of	O
the	O
trained	O
models	O
is	O
evaluated	O
on	O
1000	O
test	O
nodes	O
,	O
and	O
we	O
use	O
500	O
additional	O
nodes	O
for	O
validation	O
purposes	O
(	O
the	O
same	O
ones	O
as	O
used	O
by	O
[	O
reference	O
]	O
Inductive	Task
learning	Task
We	O
make	O
use	O
of	O
a	O
protein	O
-	O
protein	O
interaction	O
(	O
PPI	O
)	O
dataset	O
that	O
consists	O
of	O
graphs	O
corresponding	O
to	O
different	O
human	O
tissues	O
[	O
reference	O
]	O
.	O
The	O
dataset	O
contains	O
20	O
graphs	O
for	O
training	O
,	O
2	O
for	O
validation	Task
and	O
2	O
for	O
testing	O
.	O
Critically	O
,	O
testing	O
graphs	O
remain	O
completely	O
unobserved	O
during	O
training	O
.	O
To	O
construct	O
the	O
graphs	O
,	O
we	O
used	O
the	O
preprocessed	O
data	O
provided	O
by	O
[	O
reference	O
]	O
.	O
The	O
average	O
number	O
of	O
nodes	O
per	O
graph	O
is	O
2372	O
.	O
Each	O
node	O
has	O
50	O
features	O
that	O
are	O
composed	O
of	O
positional	O
gene	O
sets	O
,	O
motif	O
gene	O
sets	O
and	O
immunological	O
signatures	O
.	O
There	O
are	O
121	O
labels	O
for	O
each	O
node	O
set	O
from	O
gene	O
ontology	O
,	O
collected	O
from	O
the	O
Molecular	Material
Signatures	Material
Database	Material
[	O
reference	O
]	O
,	O
and	O
a	O
node	O
can	O
possess	O
several	O
labels	O
simultaneously	O
.	O
An	O
overview	O
of	O
the	O
interesting	O
characteristics	O
of	O
the	O
datasets	O
is	O
given	O
in	O
Table	O
1	O
.	O
section	O
:	O
STATE	O
-	O
OF	O
-	O
THE	O
-	O
ART	O
METHODS	O
Transductive	Task
learning	Task
For	O
transductive	Task
learning	Task
tasks	Task
,	O
we	O
compare	O
against	O
the	O
same	O
strong	O
baselines	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
as	O
specified	O
in	O
[	O
reference	O
]	O
.	O
This	O
includes	O
label	Method
propagation	Method
(	O
LP	Method
)	O
[	O
reference	O
]	O
,	O
semi	Method
-	Method
supervised	Method
embedding	Method
(	O
SemiEmb	Method
)	O
[	O
reference	O
]	O
,	O
manifold	Method
regularization	Method
(	O
ManiReg	Method
)	O
[	O
reference	O
]	O
,	O
skip	Method
-	Method
gram	Method
based	Method
graph	Method
embeddings	Method
(	O
DeepWalk	Method
)	O
[	O
reference	O
]	O
,	O
the	O
iterative	Method
classification	Method
algorithm	Method
(	O
ICA	Method
)	O
[	O
reference	O
]	O
and	O
Planetoid	Method
[	O
reference	O
]	O
.	O
We	O
also	O
directly	O
compare	O
our	O
model	O
against	O
GCNs	Method
[	O
reference	O
]	O
,	O
as	O
well	O
as	O
graph	Method
convolutional	Method
models	Method
utilising	O
higher	Method
-	Method
order	Method
Chebyshev	Method
filters	Method
[	O
reference	O
]	O
,	O
and	O
the	O
MoNet	Method
model	Method
presented	O
in	O
[	O
reference	O
]	O
.	O
Inductive	Task
learning	Task
For	O
the	O
inductive	Task
learning	Task
task	Task
,	O
we	O
compare	O
against	O
the	O
four	O
different	O
supervised	Method
GraphSAGE	Method
inductive	Method
methods	Method
presented	O
in	O
[	O
reference	O
]	O
.	O
These	O
provide	O
a	O
variety	O
of	O
approaches	O
to	O
aggregating	O
features	O
within	O
a	O
sampled	O
neighborhood	O
:	O
GraphSAGE	Method
-	Method
GCN	Method
(	O
which	O
extends	O
a	O
graph	Method
convolution	Method
-	Method
style	Method
operation	Method
to	O
the	O
inductive	Task
setting	Task
)	O
,	O
GraphSAGE	Method
-	Method
mean	Method
(	O
taking	O
the	O
elementwise	O
mean	O
value	O
of	O
feature	O
vectors	O
)	O
,	O
GraphSAGE	Method
-	Method
LSTM	Method
(	O
aggregating	O
by	O
feeding	O
the	O
neighborhood	O
features	O
into	O
an	O
LSTM	Method
)	O
and	O
GraphSAGE	Method
-	Method
pool	Method
(	O
taking	O
the	O
elementwise	Method
maximization	Method
operation	Method
of	O
feature	O
vectors	O
transformed	O
by	O
a	O
shared	O
nonlinear	O
multilayer	Method
perceptron	Method
)	O
.	O
The	O
other	O
transductive	Method
approaches	Method
are	O
either	O
completely	O
inappropriate	O
in	O
an	O
inductive	Task
setting	Task
or	O
assume	O
that	O
nodes	O
are	O
incrementally	O
added	O
to	O
a	O
single	O
graph	O
,	O
making	O
them	O
unusable	O
for	O
the	O
setup	O
where	O
test	O
graphs	O
are	O
completely	O
unseen	O
during	O
training	O
(	O
such	O
as	O
the	O
PPI	Material
dataset	Material
)	O
.	O
Additionally	O
,	O
for	O
both	O
tasks	O
we	O
provide	O
the	O
performance	O
of	O
a	O
per	O
-	O
node	O
shared	O
multilayer	Method
perceptron	Method
(	O
MLP	Method
)	O
classifier	O
(	O
that	O
does	O
not	O
incorporate	O
graph	O
structure	O
at	O
all	O
)	O
.	O
section	O
:	O
EXPERIMENTAL	O
SETUP	O
Transductive	Method
learning	Method
For	O
the	O
transductive	Task
learning	Task
tasks	Task
,	O
we	O
apply	O
a	O
two	O
-	O
layer	O
GAT	Method
model	O
.	O
Its	O
architectural	Method
hyperparameters	Method
have	O
been	O
optimized	O
on	O
the	O
Cora	Material
dataset	O
and	O
are	O
then	O
reused	O
for	O
Citeseer	Material
.	O
The	O
first	O
layer	O
consists	O
of	O
K	O
=	O
8	O
attention	Method
heads	Method
computing	O
F	O
=	O
8	O
features	O
each	O
(	O
for	O
a	O
total	O
of	O
64	O
features	O
)	O
,	O
followed	O
by	O
an	O
exponential	Method
linear	Method
unit	Method
(	O
ELU	Method
)	O
[	O
reference	O
]	O
nonlinearity	O
.	O
The	O
second	O
layer	O
is	O
used	O
for	O
classification	Task
:	O
a	O
single	O
attention	Method
head	Method
that	O
computes	O
C	O
features	O
(	O
where	O
C	O
is	O
the	O
number	O
of	O
classes	O
)	O
,	O
followed	O
by	O
a	O
softmax	Method
activation	Method
.	O
For	O
coping	O
with	O
the	O
small	O
training	O
set	O
sizes	O
,	O
regularization	Method
is	O
liberally	O
applied	O
within	O
the	O
model	O
.	O
During	O
training	Task
,	O
we	O
apply	O
L	Method
2	Method
regularization	Method
with	O
λ	Method
=	O
0.0005	O
.	O
Furthermore	O
,	O
dropout	Method
[	O
reference	O
]	O
with	O
p	O
=	O
0.6	O
is	O
applied	O
to	O
both	O
layers	O
'	O
inputs	O
,	O
as	O
well	O
as	O
to	O
the	O
normalized	O
attention	O
coefficients	O
(	O
critically	O
,	O
this	O
means	O
that	O
at	O
each	O
training	O
iteration	O
,	O
each	O
node	O
is	O
exposed	O
to	O
a	O
stochastically	O
sampled	O
neighborhood	O
)	O
.	O
Similarly	O
as	O
observed	O
by	O
[	O
reference	O
]	O
,	O
we	O
found	O
that	O
Pubmed	Material
's	O
training	O
set	O
size	O
(	O
60	O
examples	O
)	O
required	O
slight	O
changes	O
to	O
the	O
GAT	Method
architecture	O
:	O
we	O
have	O
applied	O
K	O
=	O
8	O
output	O
attention	O
heads	O
(	O
instead	O
of	O
one	O
)	O
,	O
and	O
strengthened	O
the	O
L	Method
2	Method
regularization	Method
to	O
λ	O
=	O
0.001	O
.	O
Otherwise	O
,	O
the	O
architecture	O
matches	O
the	O
one	O
used	O
for	O
Cora	Material
and	O
Citeseer	Material
.	O
Inductive	Method
learning	Method
For	O
the	O
inductive	Task
learning	Task
task	Task
,	O
we	O
apply	O
a	O
three	O
-	O
layer	O
GAT	Method
model	O
.	O
Both	O
of	O
the	O
first	O
two	O
layers	O
consist	O
of	O
K	O
=	O
4	O
attention	Method
heads	Method
computing	O
F	O
=	O
256	O
features	O
(	O
for	O
a	O
total	O
of	O
1024	O
features	O
)	O
,	O
followed	O
by	O
an	O
ELU	Method
nonlinearity	Method
.	O
The	O
final	O
layer	O
is	O
used	O
for	O
(	O
multi	Task
-	Task
label	Task
)	Task
classification	Task
:	O
K	O
=	O
6	O
attention	O
heads	O
computing	O
121	O
features	O
each	O
,	O
that	O
are	O
averaged	O
and	O
followed	O
by	O
a	O
logistic	Method
sigmoid	Method
activation	Method
.	O
The	O
training	O
sets	O
for	O
this	O
task	O
are	O
sufficiently	O
large	O
and	O
we	O
found	O
no	O
need	O
to	O
apply	O
L	Method
2	Method
regularization	Method
or	O
dropout	Method
-	O
we	O
have	O
,	O
however	O
,	O
successfully	O
employed	O
skip	O
connections	O
[	O
reference	O
]	O
across	O
the	O
intermediate	Method
attentional	Method
layer	Method
.	O
We	O
utilize	O
a	O
batch	O
size	O
of	O
2	O
graphs	O
during	O
training	O
.	O
To	O
strictly	O
evaluate	O
the	O
benefits	O
of	O
applying	O
an	O
attention	Method
mechanism	Method
in	O
this	O
setting	O
(	O
i.e.	O
comparing	O
with	O
a	O
near	Method
GCN	Method
-	Method
equivalent	Method
model	Method
)	O
,	O
we	O
also	O
provide	O
the	O
results	O
when	O
a	O
constant	Method
attention	Method
mechanism	Method
,	O
a	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
,	O
is	O
used	O
,	O
with	O
the	O
same	O
architecture	O
-	O
this	O
will	O
assign	O
the	O
same	O
weight	O
to	O
every	O
neighbor	O
.	O
Both	O
models	O
are	O
initialized	O
using	O
Glorot	Method
initialization	Method
[	O
reference	O
]	O
and	O
trained	O
to	O
minimize	O
cross	Metric
-	Metric
entropy	Metric
on	O
the	O
training	O
nodes	O
using	O
the	O
Adam	Method
SGD	Method
optimizer	Method
[	O
reference	O
]	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
for	O
Pubmed	Material
,	O
and	O
0.005	O
for	O
all	O
other	O
datasets	O
.	O
In	O
both	O
cases	O
we	O
use	O
an	O
early	Method
stopping	Method
strategy	Method
on	O
both	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
and	O
accuracy	Metric
(	O
transductive	Method
)	O
or	O
micro	Metric
-	Metric
F	Metric
1	Metric
(	O
inductive	Metric
)	Metric
score	Metric
on	O
the	O
validation	O
nodes	O
,	O
with	O
a	O
patience	O
of	O
100	O
epochs	O
1	O
.	O
section	O
:	O
RESULTS	O
The	O
results	O
of	O
our	O
comparative	O
evaluation	O
experiments	O
are	O
summarized	O
in	O
Tables	O
2	O
and	O
3	O
.	O
For	O
the	O
transductive	Task
tasks	Task
,	O
we	O
report	O
the	O
mean	O
classification	O
accuracy	Metric
(	O
with	O
standard	O
deviation	O
)	O
on	O
the	O
test	O
nodes	O
of	O
our	O
method	O
after	O
100	O
runs	O
,	O
and	O
reuse	O
the	O
metrics	O
already	O
reported	O
in	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
techniques	O
.	O
Specifically	O
,	O
for	O
the	O
Chebyshev	Method
filterbased	Method
approach	Method
[	O
reference	O
]	O
,	O
we	O
provide	O
the	O
maximum	O
reported	O
performance	O
for	O
filters	O
of	O
orders	O
K	O
=	O
2	O
and	O
K	O
=	O
3	O
.	O
In	O
order	O
to	O
fairly	O
assess	O
the	O
benefits	O
of	O
the	O
attention	Method
mechanism	Method
,	O
we	O
further	O
evaluate	O
a	O
GCN	Method
model	Method
that	O
computes	O
64	O
hidden	O
features	O
,	O
attempting	O
both	O
the	O
ReLU	O
and	O
ELU	O
activation	O
,	O
and	O
reporting	O
(	O
as	O
GCN	Method
-	Method
64	Method
*	Method
)	O
the	O
better	O
result	O
after	O
100	O
runs	O
(	O
which	O
was	O
the	O
ReLU	O
in	O
all	O
three	O
cases	O
)	O
.	O
For	O
the	O
inductive	Task
task	Task
,	O
we	O
report	O
the	O
micro	Metric
-	Metric
averaged	Metric
F	Metric
1	Metric
score	Metric
on	O
the	O
nodes	O
of	O
the	O
two	O
unseen	O
test	O
graphs	O
,	O
averaged	O
after	O
10	O
runs	O
,	O
and	O
reuse	O
the	O
metrics	O
already	O
reported	O
in	O
[	O
reference	O
]	O
for	O
[	O
reference	O
]	O
59.5	O
%	O
60.1	O
%	O
70.7	O
%	O
SemiEmb	Method
[	O
reference	O
]	O
59.0	O
%	O
59.6	O
%	O
71.7	O
%	O
LP	Method
[	O
reference	O
]	O
68.0	O
%	O
45.3	O
%	O
63.0	O
%	O
DeepWalk	Method
[	O
reference	O
]	O
67.2	O
%	O
43.2	O
%	O
65.3	O
%	O
ICA	Method
[	O
reference	O
]	O
75.1	O
%	O
69.1	O
%	O
73.9	O
%	O
Planetoid	Method
[	O
reference	O
]	O
75.7	O
%	O
64.7	O
%	O
77.2	O
%	O
Chebyshev	Method
[	O
reference	O
]	O
69.8	O
%	O
74.4	O
%	O
GCN	Method
[	O
reference	O
]	O
81.5	O
%	O
70.3	O
%	O
79.0	O
%	O
MoNet	Method
[	O
reference	O
]	O
81.7	O
±	O
0.5	O
%	O
-	O
78.8	O
±	O
0.3	O
%	O
GCN	Method
-	Method
64	Method
*	O
81.4	O
±	O
0.5	O
%	O
70.9	O
±	O
0.5	O
%	O
79.0	O
±	O
0.3	O
%	O
GAT	Method
(	O
ours	O
)	O
83.0	O
±	O
0.7	O
%	O
72.5	O
±	O
0.7	O
%	O
79.0	O
±	O
0.3	O
%	O
Table	O
3	O
:	O
Summary	O
of	O
results	O
in	O
terms	O
of	O
micro	Metric
-	Metric
averaged	Metric
F	Metric
1	Metric
scores	Metric
,	O
for	O
the	O
PPI	Material
dataset	Material
.	O
GraphSAGE	Method
*	Method
corresponds	O
to	O
the	O
best	O
GraphSAGE	O
result	O
we	O
were	O
able	O
to	O
obtain	O
by	O
just	O
modifying	O
its	O
architecture	O
.	O
Const	Method
-	Method
GAT	Method
corresponds	O
to	O
a	O
model	O
with	O
the	O
same	O
architecture	O
as	O
GAT	Method
,	O
but	O
with	O
a	O
constant	Method
attention	Method
mechanism	Method
(	O
assigning	O
same	O
importance	O
to	O
each	O
neighbor	O
;	O
GCN	O
-	O
like	O
inductive	O
operator	O
)	O
.	O
section	O
:	O
Inductive	O
section	O
:	O
Method	O
PPI	O
Random	O
0.396	O
MLP	Method
0.422	O
GraphSAGE	O
-	O
GCN	O
[	O
reference	O
]	O
0.500	O
GraphSAGE	O
-	O
mean	O
[	O
reference	O
]	O
0.598	O
GraphSAGE	Method
-	Method
LSTM	Method
(	O
Hamilton	O
et	O
al	O
.	O
,	O
2017	O
)	O
0.612	O
[	O
reference	O
]	O
0.600	O
0.973	O
±	O
0.002	O
the	O
other	O
techniques	O
.	O
Specifically	O
,	O
as	O
our	O
setup	O
is	O
supervised	O
,	O
we	O
compare	O
against	O
the	O
supervised	Method
GraphSAGE	Method
approaches	Method
.	O
To	O
evaluate	O
the	O
benefits	O
of	O
aggregating	O
across	O
the	O
entire	O
neighborhood	O
,	O
we	O
further	O
provide	O
(	O
as	O
GraphSAGE	O
*	O
)	O
the	O
best	O
result	O
we	O
were	O
able	O
to	O
achieve	O
with	O
GraphSAGE	Method
by	O
just	O
modifying	O
its	O
architecture	O
(	O
this	O
was	O
with	O
a	O
three	Method
-	Method
layer	Method
GraphSAGE	Method
-	Method
LSTM	Method
with	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
features	O
computed	O
in	O
each	O
layer	O
and	O
128	O
features	O
used	O
for	O
aggregating	O
neighborhoods	O
)	O
.	O
Finally	O
,	O
we	O
report	O
the	O
10	O
-	O
run	O
result	O
of	O
our	O
constant	Method
attention	Method
GAT	Method
model	Method
(	O
as	O
Const	Method
-	Method
GAT	Method
)	O
,	O
to	O
fairly	O
evaluate	O
the	O
benefits	O
of	O
the	O
attention	Method
mechanism	Method
against	O
a	O
GCN	Method
-	Method
like	Method
aggregation	Method
scheme	Method
(	O
with	O
the	O
same	O
architecture	O
)	O
.	O
Our	O
results	O
successfully	O
demonstrate	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
being	O
achieved	O
or	O
matched	O
across	O
all	O
four	O
datasets	O
-	O
in	O
concordance	O
with	O
our	O
expectations	O
,	O
as	O
per	O
the	O
discussion	O
in	O
Section	O
2.2	O
.	O
More	O
specifically	O
,	O
we	O
are	O
able	O
to	O
improve	O
upon	O
GCNs	Method
by	O
a	O
margin	O
of	O
1.5	O
%	O
and	O
1.6	O
%	O
on	O
Cora	Material
and	O
Citeseer	Material
,	O
respectively	O
,	O
suggesting	O
that	O
assigning	O
different	O
weights	O
to	O
nodes	O
of	O
a	O
same	O
neighborhood	O
may	O
be	O
beneficial	O
.	O
It	O
is	O
worth	O
noting	O
the	O
improvements	O
achieved	O
on	O
the	O
PPI	Material
dataset	Material
:	O
Our	O
GAT	Method
model	O
improves	O
by	O
20.5	O
%	O
w.r.t	O
.	O
the	O
best	O
GraphSAGE	O
result	O
we	O
were	O
able	O
to	O
obtain	O
,	O
demonstrating	O
that	O
our	O
model	O
has	O
the	O
potential	O
to	O
be	O
applied	O
in	O
inductive	Task
settings	Task
,	O
and	O
that	O
larger	O
predictive	Metric
power	Metric
can	O
be	O
leveraged	O
by	O
observing	O
the	O
entire	O
neighborhood	O
.	O
Furthermore	O
,	O
it	O
improves	O
by	O
3.9	O
%	O
w.r.t	O
.	O
Const	Method
-	Method
GAT	Method
(	O
the	O
identical	Method
architecture	Method
with	O
constant	Method
attention	Method
mechanism	Method
)	O
,	O
once	O
again	O
directly	O
demonstrating	O
the	O
significance	O
of	O
being	O
able	O
to	O
assign	O
different	O
weights	O
to	O
different	O
neighbors	O
.	O
The	O
effectiveness	O
of	O
the	O
learned	O
feature	Method
representations	Method
may	O
also	O
be	O
investigated	O
qualitatively	O
-	O
and	O
for	O
this	O
purpose	O
we	O
provide	O
a	O
visualization	O
of	O
the	O
t	Method
-	Method
SNE	Method
(	O
Maaten	O
&	O
Hinton	O
,	O
2008	O
)-	O
transformed	O
feature	Method
representations	Method
extracted	O
by	O
the	O
first	O
layer	O
of	O
a	O
GAT	Method
model	O
pre	O
-	O
trained	O
on	O
the	O
Cora	Material
dataset	O
(	O
Figure	O
2	O
)	O
.	O
The	O
representation	O
exhibits	O
discernible	O
clustering	O
in	O
the	O
projected	O
2D	O
space	O
.	O
Note	O
that	O
these	O
clusters	O
correspond	O
to	O
the	O
seven	O
labels	O
of	O
the	O
dataset	O
,	O
verifying	O
the	O
model	O
's	O
discriminative	O
power	O
across	O
the	O
seven	O
topic	O
classes	O
of	O
Cora	Material
.	O
Additionally	O
,	O
we	O
visualize	O
the	O
relative	O
strengths	O
of	O
the	O
normalized	O
attention	O
coefficients	O
(	O
averaged	O
across	O
all	O
eight	O
attention	O
heads	O
)	O
.	O
Properly	O
interpreting	O
these	O
coefficients	O
(	O
as	O
performed	O
by	O
e.g.	O
[	O
reference	O
]	O
)	O
will	O
require	O
further	O
domain	O
knowledge	O
about	O
the	O
dataset	O
under	O
study	O
,	O
and	O
is	O
left	O
for	O
future	O
work	O
.	O
section	O
:	O
CONCLUSIONS	O
We	O
have	O
presented	O
graph	Method
attention	Method
networks	Method
(	O
GATs	Method
)	O
,	O
novel	O
convolution	Method
-	Method
style	Method
neural	Method
networks	Method
that	O
operate	O
on	O
graph	O
-	O
structured	O
data	O
,	O
leveraging	O
masked	Method
self	Method
-	Method
attentional	Method
layers	Method
.	O
The	O
graph	Method
attentional	Method
layer	Method
utilized	O
throughout	O
these	O
networks	O
is	O
computationally	O
efficient	O
(	O
does	O
not	O
require	O
costly	O
matrix	O
operations	O
,	O
and	O
is	O
parallelizable	O
across	O
all	O
nodes	O
in	O
the	O
graph	O
)	O
,	O
allows	O
for	O
(	O
implicitly	O
)	O
assigning	O
different	O
importances	O
to	O
different	O
nodes	O
within	O
a	O
neighborhood	O
while	O
dealing	O
with	O
different	O
sized	O
neighborhoods	O
,	O
and	O
does	O
not	O
depend	O
on	O
knowing	O
the	O
entire	O
graph	O
structure	O
upfront	O
-	O
thus	O
addressing	O
many	O
of	O
the	O
theoretical	O
issues	O
with	O
previous	O
spectral	Method
-	Method
based	Method
approaches	Method
.	O
Our	O
models	O
leveraging	O
attention	Method
have	O
successfully	O
achieved	O
or	O
matched	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
four	O
well	O
-	O
established	O
node	Task
classification	Task
benchmarks	Task
,	O
both	O
transductive	Task
and	Task
inductive	Task
(	O
especially	O
,	O
with	O
completely	O
unseen	O
graphs	O
used	O
for	O
testing	O
)	O
.	O
There	O
are	O
several	O
potential	O
improvements	O
and	O
extensions	O
to	O
graph	Method
attention	Method
networks	Method
that	O
could	O
be	O
addressed	O
as	O
future	O
work	O
,	O
such	O
as	O
overcoming	O
the	O
practical	O
problems	O
described	O
in	O
subsection	O
2.2	O
to	O
be	O
able	O
to	O
handle	O
larger	O
batch	O
sizes	O
.	O
A	O
particularly	O
interesting	O
research	O
direction	O
would	O
be	O
taking	O
advantage	O
of	O
the	O
attention	Method
mechanism	Method
to	O
perform	O
a	O
thorough	O
analysis	O
on	O
the	O
model	Task
interpretability	Task
.	O
Moreover	O
,	O
extending	O
the	O
method	O
to	O
perform	O
graph	Task
classification	Task
instead	O
of	O
node	Task
classification	Task
would	O
also	O
be	O
relevant	O
from	O
the	O
application	O
perspective	O
.	O
Finally	O
,	O
extending	O
the	O
model	O
to	O
incorporate	O
edge	O
features	O
(	O
possibly	O
indicating	O
relationship	O
among	O
nodes	O
)	O
would	O
allow	O
us	O
to	O
tackle	O
a	O
larger	O
variety	O
of	O
problems	O
.	O
section	O
:	O
section	O
:	O
ACKNOWLEDGEMENTS	O
The	O
authors	O
would	O
like	O
to	O
thank	O
the	O
developers	O
of	O
TensorFlow	Method
[	O
reference	O
]	O
.	O
PV	O
and	O
PL	O
have	O
received	O
funding	O
from	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
PROPAG	O
-	O
AGEING	O
under	O
grant	O
agreement	O
No	O
634821	O
.	O
We	O
further	O
acknowledge	O
the	O
support	O
of	O
the	O
following	O
agencies	O
for	O
research	O
funding	O
and	O
computing	O
support	O
:	O
CIFAR	O
,	O
Canada	O
Research	O
Chairs	O
,	O
Compute	O
Canada	O
and	O
Calcul	O
Québec	O
,	O
as	O
well	O
as	O
NVIDIA	O
for	O
the	O
generous	O
GPU	O
support	O
.	O
Special	O
thanks	O
to	O
:	O
Benjamin	O
Day	O
and	O
Fabian	O
Jansen	O
for	O
kindly	O
pointing	O
out	O
issues	O
in	O
a	O
previous	O
iteration	O
of	O
the	O
paper	O
;	O
Michał	O
Drożdżal	O
for	O
useful	O
discussions	O
,	O
feedback	O
and	O
support	O
;	O
and	O
Gaétan	O
Marceau	O
for	O
reviewing	O
the	O
paper	O
prior	O
to	O
submission	O
.	O
section	O
:	O
