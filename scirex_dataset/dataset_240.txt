document O
: O
Convolutional Method
Kernel Method
Networks Method
An O
important O
goal O
in O
visual Task
recognition Task
is O
to O
devise O
image Method
representations Method
that O
are O
invariant O
to O
particular O
transformations O
. O
In O
this O
paper O
, O
we O
address O
this O
goal O
with O
a O
new O
type O
of O
convolutional Method
neural Method
network Method
( O
CNN Method
) O
whose O
invariance O
is O
encoded O
by O
a O
reproducing Method
kernel Method
. O
Unlike O
traditional O
approaches O
where O
neural Method
networks Method
are O
learned O
either O
to O
represent O
data O
or O
for O
solving O
a O
classification Task
task Task
, O
our O
network O
learns O
to O
approximate O
the O
kernel O
feature O
map O
on O
training O
data O
. O
Such O
an O
approach O
enjoys O
several O
benefits O
over O
classical O
ones O
. O
First O
, O
by O
teaching O
CNNs Method
to O
be O
invariant O
, O
we O
obtain O
simple O
network Method
architectures Method
that O
achieve O
a O
similar O
accuracy Metric
to O
more O
complex O
ones O
, O
while O
being O
easy O
to O
train O
and O
robust O
to O
overfitting O
. O
Second O
, O
we O
bridge O
a O
gap O
between O
the O
neural Method
network Method
literature Method
and O
kernels Method
, O
which O
are O
natural O
tools O
to O
model O
invariance Task
. O
We O
evaluate O
our O
methodology O
on O
visual Task
recognition Task
tasks Task
where O
CNNs Method
have O
proven O
to O
perform O
well O
, O
e.g. O
, O
digit Task
recognition Task
with O
the O
MNIST Material
dataset Material
, O
and O
the O
more O
challenging O
CIFAR Material
- Material
10 Material
and O
STL Material
- Material
10 Material
datasets Material
, O
where O
our O
accuracy Metric
is O
competitive O
with O
the O
state O
of O
the O
art O
. O
positioning O
, O
decorations.pathreplacing O
section O
: O
Introduction O
We O
have O
recently O
seen O
a O
revival O
of O
attention O
given O
to O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
due O
to O
their O
high O
performance O
for O
large Task
- Task
scale Task
visual Task
recognition Task
tasks Task
. O
The O
architecture O
of O
CNNs Method
is O
relatively O
simple O
and O
consists O
of O
successive O
layers O
organized O
in O
a O
hierarchical O
fashion O
; O
each O
layer O
involves O
convolutions Method
with O
learned Method
filters Method
followed O
by O
a O
pointwise Method
non Method
- Method
linearity Method
and O
a O
downsampling Method
operation Method
called O
“ O
feature Method
pooling Method
” O
. O
The O
resulting O
image Method
representation Method
has O
been O
empirically O
observed O
to O
be O
invariant O
to O
image O
perturbations O
and O
to O
encode O
complex O
visual O
patterns O
, O
which O
are O
useful O
properties O
for O
visual Task
recognition Task
. O
Training O
CNNs Method
remains O
however O
difficult O
since O
high Task
- Task
capacity Task
networks Task
may O
involve O
billions O
of O
parameters O
to O
learn O
, O
which O
requires O
both O
high O
computational Metric
power Metric
, O
e.g. O
, O
GPUs O
, O
and O
appropriate O
regularization Method
techniques Method
. O
The O
exact O
nature O
of O
invariance O
that O
CNNs Method
exhibit O
is O
also O
not O
precisely O
understood O
. O
Only O
recently O
, O
the O
invariance O
of O
related O
architectures O
has O
been O
characterized O
; O
this O
is O
the O
case O
for O
the O
wavelet Method
scattering Method
transform Method
or O
the O
hierarchical Method
models Method
of O
. O
Our O
work O
revisits O
convolutional Method
neural Method
networks Method
, O
but O
we O
adopt O
a O
significantly O
different O
approach O
than O
the O
traditional O
one O
. O
Indeed O
, O
we O
use O
kernels O
, O
which O
are O
natural O
tools O
to O
model O
invariance O
. O
Inspired O
by O
the O
hierarchical Method
kernel Method
descriptors Method
of O
, O
we O
propose O
a O
reproducing Method
kernel Method
that O
produces O
multi Method
- Method
layer Method
image Method
representations Method
. O
Our O
main O
contribution O
is O
an O
approximation Method
scheme Method
called O
convolutional Method
kernel Method
network Method
( O
CKN Method
) O
to O
make O
the O
kernel Method
approach Method
computationally O
feasible O
. O
Our O
approach O
is O
a O
new O
type O
of O
unsupervised Method
convolutional Method
neural Method
network Method
that O
is O
trained O
to O
approximate O
the O
kernel O
map O
. O
Interestingly O
, O
our O
network O
uses O
non O
- O
linear O
functions O
that O
resemble O
rectified O
linear O
units O
, O
even O
though O
they O
were O
not O
handcrafted O
and O
naturally O
emerge O
from O
an O
approximation Method
scheme Method
of O
the O
Gaussian Method
kernel Method
map Method
. O
By O
bridging O
a O
gap O
between O
kernel Method
methods Method
and O
neural Method
networks Method
, O
we O
believe O
that O
we O
are O
opening O
a O
fruitful O
research O
direction O
for O
the O
future O
. O
Our O
network O
is O
learned O
without O
supervision O
since O
the O
label O
information O
is O
only O
used O
subsequently O
in O
a O
support Method
vector Method
machine Method
( O
SVM Method
) O
. O
Yet O
, O
we O
achieve O
competitive O
results O
on O
several O
datasets O
such O
as O
MNIST Material
, O
CIFAR Material
- Material
10 Material
and O
STL Material
- Material
10 Material
with O
simple O
architectures O
, O
few O
parameters O
to O
learn O
, O
and O
no O
data Method
augmentation Method
. O
Open O
- O
source O
code O
for O
learning O
our O
convolutional Method
kernel Method
networks O
is O
available O
on O
the O
first O
author O
’s O
webpage O
. O
subsection O
: O
Related O
Work O
There O
have O
been O
several O
attempts O
to O
build O
kernel Method
- Method
based Method
methods Method
that O
mimic O
deep Method
neural Method
networks Method
; O
we O
only O
review O
here O
the O
ones O
that O
are O
most O
related O
to O
our O
approach O
. O
paragraph O
: O
Arc Method
- Method
cosine Method
kernels Method
. O
Kernels Method
for O
building O
deep Task
large Task
- Task
margin Task
classifiers Task
have O
been O
introduced O
in O
. O
The O
multilayer Method
arc Method
- Method
cosine Method
kernel Method
is O
built O
by O
successive O
kernel Method
compositions Method
, O
and O
each O
layer O
relies O
on O
an O
integral Method
representation Method
. O
Similarly O
, O
our O
kernels O
rely O
on O
an O
integral Method
representation Method
, O
and O
enjoy O
a O
multilayer Method
construction Method
. O
However O
, O
in O
contrast O
to O
arc Method
- Method
cosine Method
kernels Method
: O
( O
i O
) O
we O
build O
our O
sequence Method
of Method
kernels Method
by O
convolutions Method
, O
using O
local O
information O
over O
spatial O
neighborhoods O
( O
as O
opposed O
to O
compositions O
, O
using O
global O
information O
) O
; O
( O
ii O
) O
we O
propose O
a O
new O
training Method
procedure Method
for O
learning O
a O
compact Method
representation Method
of O
the O
kernel O
in O
a O
data O
- O
dependent O
manner O
. O
paragraph O
: O
Multilayer Method
derived Method
kernels Method
. O
Kernels Method
with O
invariance O
properties O
for O
visual Task
recognition Task
have O
been O
proposed O
in O
. O
Such O
kernels O
are O
built O
with O
a O
parameterized Method
“ Method
neural Method
response Method
” Method
function Method
, O
which O
consists O
in O
computing O
the O
maximal O
response O
of O
a O
base Method
kernel Method
over O
a O
local O
neighborhood O
. O
Multiple O
layers O
are O
then O
built O
by O
iteratively O
renormalizing O
the O
response O
kernels O
and O
pooling Method
using O
neural Method
response Method
functions Method
. O
Learning Method
is O
performed O
by O
plugging O
the O
obtained O
kernel Method
in O
an O
SVM Method
. O
In O
contrast O
to O
, O
we O
propagate O
information O
up O
, O
from O
lower O
to O
upper O
layers O
, O
by O
using O
sequences Method
of Method
convolutions Method
. O
Furthermore O
, O
we O
propose O
a O
simple O
and O
effective O
data O
- O
dependent O
way O
to O
learn O
a O
compact Method
representation Method
of O
our O
kernels O
and O
show O
that O
we O
obtain O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
several O
benchmarks O
. O
paragraph O
: O
Hierarchical Method
kernel Method
descriptors Method
. O
The O
kernels O
proposed O
in O
produce O
multilayer Method
image Method
representations Method
for O
visual Task
recognition Task
tasks Task
. O
We O
discuss O
in O
details O
these O
kernels O
in O
the O
next O
section O
: O
our O
paper O
generalizes O
them O
and O
establishes O
a O
strong O
link O
with O
convolutional Method
neural Method
networks Method
. O
section O
: O
Convolutional Method
Multilayer Method
Kernels Method
The O
convolutional Method
multilayer Method
kernel Method
is O
a O
generalization O
of O
the O
hierarchical Method
kernel Method
descriptors Method
introduced O
in O
computer Task
vision Task
. O
The O
kernel O
produces O
a O
sequence O
of O
image Method
representations Method
that O
are O
built O
on O
top O
of O
each O
other O
in O
a O
multilayer O
fashion O
. O
Each O
layer O
can O
be O
interpreted O
as O
a O
non Method
- Method
linear Method
transformation Method
of O
the O
previous O
one O
with O
additional O
spatial O
invariance O
. O
We O
call O
these O
layers O
image Method
feature Method
maps Method
, O
and O
formally O
define O
them O
as O
follows O
: O
theorem O
: O
. O
An O
image Method
feature Method
map Method
φ Method
is O
a O
function O
: O
φ→ΩH O
, O
where O
Ω O
is O
a O
( O
usually O
discrete O
) O
subset O
of O
[ O
0 O
, O
1 O
] O
d O
representing O
normalized O
“ O
coordinates O
” O
in O
the O
image O
and O
H O
is O
a O
Hilbert O
space O
. O
For O
all O
practical O
examples O
in O
this O
paper O
, O
is O
a O
two O
- O
dimensional O
grid O
and O
corresponds O
to O
different O
locations O
in O
a O
two O
- O
dimensional O
image O
. O
In O
other O
words O
, O
is O
a O
set O
of O
pixel O
coordinates O
. O
Given O
in O
, O
the O
point O
represents O
some O
characteristics O
of O
the O
image O
at O
location O
, O
or O
in O
a O
neighborhood O
of O
. O
For O
instance O
, O
a O
color O
image O
of O
size O
with O
three O
channels O
, O
red O
, O
green O
, O
and O
blue O
, O
may O
be O
represented O
by O
an O
initial O
feature Method
map Method
, O
where O
is O
an O
regular O
grid O
, O
is O
the O
Euclidean O
space O
, O
and O
provides O
the O
color O
pixel O
values O
. O
With O
the O
multilayer Method
scheme Method
, O
non O
- O
trivial O
feature O
maps O
will O
be O
obtained O
subsequently O
, O
which O
will O
encode O
more O
complex O
image O
characteristics O
. O
With O
this O
terminology O
in O
hand O
, O
we O
now O
introduce O
the O
convolutional Method
kernel Method
, O
first O
, O
for O
a O
single O
layer O
. O
theorem O
: O
( O
Convolutional Method
Kernel Method
with O
Single Method
Layer Method
) O
. O
Let O
us O
consider O
two O
images O
represented O
by O
two O
image Method
feature Method
maps Method
, O
respectively O
φ O
and O
: O
φ′→ΩH O
, O
where O
Ω O
is O
a O
set O
of O
pixel O
locations O
, O
and O
H O
is O
a O
Hilbert O
space O
. O
The O
one Method
- Method
layer Method
convolutional Method
kernel Method
between O
φ O
and O
φ′ O
is O
defined O
as O
where O
β O
and O
σ O
are O
smoothing O
parameters O
of O
Gaussian Method
kernels Method
, O
and O
: O
= O
⁢~φ O
( O
z O
) O
⁢ O
(/ O
1∥⁢φ O
( O
z O
) O
∥H O
) O
φ O
( O
z O
) O
if O
≠⁢φ O
( O
z O
) O
0 O
and O
= O
⁢~φ O
( O
z O
) O
0 O
otherwise O
. O
Similarly O
, O
⁢~φ′ O
( O
z′ O
) O
is O
a O
normalized Method
version Method
of Method
⁢φ′ Method
( Method
z′ Method
) Method
.When Method
Ω Method
is O
not O
discrete O
, O
the O
notation O
∑ O
in O
( O
) O
should O
be O
replaced O
by O
the O
Lebesgue O
integral O
∫ O
in O
the O
paper O
. O
It O
is O
easy O
to O
show O
that O
the O
kernel Method
is O
positive O
definite O
( O
see O
Appendix O
A O
) O
. O
It O
consists O
of O
a O
sum O
of O
pairwise O
comparisons O
between O
the O
image O
features O
and O
computed O
at O
all O
spatial O
locations O
and O
in O
. O
To O
be O
significant O
in O
the O
sum O
, O
a O
comparison O
needs O
the O
corresponding O
and O
to O
be O
close O
in O
, O
and O
the O
normalized O
features O
and O
to O
be O
close O
in O
the O
feature O
space O
. O
The O
parameters O
and O
respectively O
control O
these O
two O
definitions O
of O
“ O
closeness O
” O
. O
Indeed O
, O
when O
is O
large O
, O
the O
kernel Method
is O
invariant O
to O
the O
positions O
and O
but O
when O
is O
small O
, O
only O
features O
placed O
at O
the O
same O
location O
are O
compared O
to O
each O
other O
. O
Therefore O
, O
the O
role O
of O
is O
to O
control O
how O
much O
the O
kernel O
is O
locally O
shift O
- O
invariant O
. O
Next O
, O
we O
will O
show O
how O
to O
go O
beyond O
one O
single O
layer O
, O
but O
before O
that O
, O
we O
present O
concrete O
examples O
of O
simple O
input Method
feature Method
maps Method
. O
paragraph O
: O
Gradient Method
map Method
. O
Assume O
that O
and O
that O
provides O
the O
two O
- O
dimensional O
gradient O
of O
the O
image O
at O
pixel O
, O
which O
is O
often O
computed O
with O
first Method
- Method
order Method
differences Method
along O
each O
dimension O
. O
Then O
, O
the O
quantity O
is O
the O
gradient O
intensity O
, O
and O
is O
its O
orientation O
, O
which O
can O
be O
characterized O
by O
a O
particular O
angle O
— O
that O
is O
, O
there O
exists O
in O
such O
that O
. O
The O
resulting O
kernel O
is O
exactly O
the O
kernel Method
descriptor Method
introduced O
in O
for O
natural Task
image Task
patches Task
. O
paragraph O
: O
Patch Method
map Method
. O
In O
that O
setting O
, O
associates O
to O
a O
location O
an O
image O
patch O
of O
size O
centered O
at O
. O
Then O
, O
the O
space O
is O
simply O
, O
and O
is O
a O
contrast Method
- Method
normalized Method
version Method
of O
the O
patch O
, O
which O
is O
a O
useful O
transformation O
for O
visual Task
recognition Task
according O
to O
classical O
findings O
in O
computer Task
vision Task
. O
When O
the O
image O
is O
encoded O
with O
three O
color O
channels O
, O
patches O
are O
of O
size O
. O
We O
now O
define O
the O
multilayer Method
convolutional Method
kernel Method
, O
generalizing O
some O
ideas O
of O
. O
theorem O
: O
( O
Multilayer Method
Convolutional Method
Kernel Method
) O
. O
Let O
us O
consider O
a O
set O
⊆Ω⁢k–1 O
[ O
0 O
, O
1 O
] O
d O
and O
a O
Hilbert Method
space Method
H⁢k–1 Method
. O
We O
build O
a O
new O
set O
Ωk Method
and O
a O
new O
Hilbert Method
space Method
Hk Method
as O
follows O
: O
( O
i O
) O
choose O
a O
patch O
shape O
Pk O
defined O
as O
a O
bounded O
symmetric O
subset O
of O
[ O
- O
1 O
, O
1 O
] O
d O
, O
and O
a O
set O
of O
coordinates O
Ωk O
such O
that O
for O
all O
location O
zk O
in O
Ωk O
, O
the O
patch O
+ O
{ O
zk}Pk O
is O
a O
subset O
of O
Ω⁢k–1;For O
two O
sets O
A O
and O
B O
, O
the O
Minkowski O
sum O
+ O
AB O
is O
defined O
as O
{ O
+ O
ab:∈aA O
, O
∈bB}. O
In O
other O
words O
, O
each O
coordinate O
zk O
in O
Ωk O
corresponds O
to O
a O
valid O
patch O
in O
Ω⁢k–1 O
centered O
at O
zk O
. O
( O
ii O
) O
define O
the O
convolutional Method
kernel Method
Kk Method
on O
the O
“ O
patch O
” O
feature O
maps O
→PkH⁢k–1 O
, O
by O
replacing O
in O
( O
) O
: O
Ω O
by O
Pk O
, O
H O
by O
H⁢k–1 O
, O
and O
σ O
, O
β O
by O
appropriate O
smoothing Method
parameters Method
σk Method
, O
βk O
. O
We O
denote O
by O
Hk O
the O
Hilbert O
space O
for O
which O
the O
positive Method
definite Method
kernel Method
Kk Method
is O
reproducing O
. O
An O
image O
represented O
by O
a O
feature Method
map Method
: O
φ⁢k–1→Ω⁢k–1H⁢k–1 O
at O
layer O
⁢k–1 O
is O
now O
encoded O
in O
the O
k O
- O
th O
layer O
as O
: O
φk→ΩkHk O
, O
where O
for O
all O
zk O
in O
Ωk O
, O
⁢φk O
( O
zk O
) O
is O
the O
representation O
in O
Hk O
of O
the O
patch O
feature O
map O
↦z⁢φ⁢k–1 O
(+ O
zkz O
) O
for O
z O
in O
Pk O
. O
Concretely O
, O
the O
kernel O
between O
two O
patches O
of O
and O
at O
respective O
locations O
and O
is O
where O
is O
the O
Hilbertian O
norm O
of O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
illustrate O
the O
interactions O
between O
the O
sets O
of O
coordinates O
, O
patches O
, O
and O
feature O
spaces O
across O
layers O
. O
For O
two Task
- Task
dimensional Task
grids Task
, O
a O
typical O
patch O
shape O
is O
a O
square O
, O
for O
example O
for O
a O
patch O
in O
an O
image O
of O
size O
. O
Information O
encoded O
in O
the O
- O
th O
layer O
differs O
from O
the O
- O
th O
one O
in O
two O
aspects O
: O
first O
, O
each O
point O
in O
layer O
contains O
information O
about O
several O
points O
from O
the O
- O
th O
layer O
and O
can O
possibly O
represent O
larger O
patterns O
; O
second O
, O
the O
new O
feature Method
map Method
is O
more O
locally O
shift O
- O
invariant O
than O
the O
previous O
one O
due O
to O
the O
term O
involving O
the O
parameter O
in O
( O
[ O
reference O
] O
) O
. O
bottom O
middle O
top O
bottom O
, O
middle O
, O
top O
[ O
scale=1 Method
, Method
every Method
node Method
/ Method
.style Method
= O
minimum O
size=1 O
cm O
, O
on O
grid O
] O
bottom O
[ O
yshift=0 O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
[ O
white O
, O
fill O
opacity=0.9 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
3 O
, O
3 O
) O
; O
[ O
step=2 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
3 O
, O
3 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
3 O
, O
3 O
) O
; O
[ O
red!20 O
, O
fill O
] O
( O
0.4 O
, O
0.6 O
) O
rectangle O
( O
0.6 O
, O
0.8 O
) O
; O
[ O
red!80!black!100 O
] O
( O
0.4 O
, O
0.6 O
) O
rectangle O
( O
0.6 O
, O
0.8 O
) O
; O
[ O
blue!20 O
, O
fill O
] O
( O
1 O
, O
0.4 O
) O
rectangle O
( O
1.8 O
, O
1.2 O
) O
; O
[ O
blue!90 O
] O
( O
1 O
, O
0.4 O
) O
rectangle O
( O
1.8 O
, O
1.2 O
) O
; O
[ O
step=2 O
mm O
, O
blue!70 O
] O
( O
1 O
, O
0.4 O
) O
grid O
( O
1.8 O
, O
1.2 O
) O
; O
( O
a O
) O
at O
( O
0.5 O
, O
0.7 O
) O
; O
( O
B1 O
) O
at O
( O
1 O
, O
0.4 O
) O
; O
( O
B2 O
) O
at O
( O
1 O
, O
1.2 O
) O
; O
( O
B3 O
) O
at O
( O
1.8 O
, O
0.4 O
) O
; O
( O
B4 O
) O
at O
( O
1.8 O
, O
1.2 O
) O
; O
( O
B5 O
) O
at O
( O
1.4 O
, O
0.8 O
) O
; O
( O
aa O
) O
at O
( O
1 O
, O
0 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
1.8 O
, O
0.1 O
) O
node O
[ O
right O
] O
to O
[ O
out=180 O
, O
in= O
- O
50 O
] O
( O
aa Method
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
0.3 O
, O
0 O
) O
node O
[ O
left O
] O
red!80!black!80 O
to O
[ O
out=0 O
, O
in=270 O
] O
( O
a O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
0.5 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
to O
[ O
out=180 O
, O
in=50 O
] O
( O
B5 O
) O
; O
middle O
[ O
yshift=50 O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
[ O
white O
, O
fill O
opacity=.7 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
2.1 O
, O
2.1 O
) O
; O
[ O
green!20 O
, O
fill O
] O
( O
0 O
, O
0.9 O
) O
rectangle O
( O
0.9 O
, O
1.8 O
) O
; O
[ O
step=3 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
2.1 O
, O
2.1 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
2.1 O
, O
2.1 O
) O
; O
[ O
blue!20 O
, O
fill O
] O
( O
0.9 O
, O
0.3 O
) O
rectangle O
( O
1.2 O
, O
0.6 O
) O
; O
[ O
blue!90 O
] O
( O
0.9 O
, O
0.3 O
) O
rectangle O
( O
1.2 O
, O
0.6 O
) O
; O
[ O
green!70!black!100 O
] O
( O
0 O
, O
0.9 O
) O
rectangle O
( O
0.9 O
, O
1.8 O
) O
; O
[ O
step=3 O
mm O
, O
green!70 O
] O
( O
0 O
, O
0.9 O
) O
grid O
( O
0.9 O
, O
1.8 O
) O
; O
( O
b O
) O
at O
( O
1.05 O
, O
0.45 O
) O
; O
( O
c O
) O
at O
( O
0 O
, O
1.5 O
) O
; O
( O
A1 O
) O
at O
( O
0.9 O
, O
0.3 O
) O
; O
( O
A2 O
) O
at O
( O
0.9 O
, O
0.6 O
) O
; O
( O
A3 O
) O
at O
( O
1.2 O
, O
0.3 O
) O
; O
( O
A4 O
) O
at O
( O
1.2 O
, O
0.6 O
) O
; O
( O
D1 O
) O
at O
( O
0 O
, O
0.9 O
) O
; O
( O
D2 O
) O
at O
( O
0 O
, O
1.8 O
) O
; O
( O
D3 O
) O
at O
( O
0.9 O
, O
0.9 O
) O
; O
( O
D4 O
) O
at O
( O
0.9 O
, O
1.8 O
) O
; O
( O
D5 O
) O
at O
( O
0.45 O
, O
1.35 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
2.3 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
to O
[ O
out=180 O
, O
in=50 O
] O
( O
b O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
1.8 O
, O
2.4 O
) O
node O
[ O
left O
] O
to O
[ O
out=0 O
, O
in=180 O
] O
( O
c O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
3.5 O
) O
node O
[ O
right O
] O
green!60!black!100 O
to O
[ O
out=180 O
, O
in=50 O
] O
( O
D5 O
) O
; O
bottom O
[ O
thick O
, O
blue!70 O
] O
( O
A1 O
) O
– O
( O
B1 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
A2 O
) O
– O
( O
B2 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
A3 O
) O
– O
( O
B3 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
A4 O
) O
– O
( O
B4 O
) O
; O
top O
[ O
yshift=100 O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
[ O
white O
, O
fill O
opacity=.7 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.6 O
, O
1.6 O
) O
; O
[ O
step=4 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
1.6 O
, O
1.6 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.6 O
, O
1.6 O
) O
; O
[ O
green!20 O
, O
fill O
] O
( O
0 O
, O
0.8 O
) O
rectangle O
( O
0.4 O
, O
1.2 O
) O
; O
[ O
green!70!black!100 O
] O
( O
0 O
, O
0.8 O
) O
rectangle O
( O
0.4 O
, O
1.2 O
) O
; O
( O
CC1 O
) O
at O
( O
0 O
, O
1.6 O
) O
; O
( O
CC2 O
) O
at O
( O
0.2 O
, O
1.0 O
) O
; O
( O
C1 O
) O
at O
( O
0 O
, O
0.8 O
) O
; O
( O
C2 O
) O
at O
( O
0 O
, O
1.2 O
) O
; O
( O
C3 O
) O
at O
( O
0.4 O
, O
0.8 O
) O
; O
( O
C4 O
) O
at O
( O
0.4 O
, O
1.2 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
1.8 O
, O
4.2 O
) O
node O
[ O
left O
] O
to O
[ O
out=0 O
, O
in=180 O
] O
( O
CC1 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
4.8 O
) O
node O
[ O
right O
] O
green!60!black!100 O
to O
[ O
out=180 O
, O
in=50 O
] O
( O
CC2 O
) O
; O
middle O
[ O
thick O
, O
green!70!black!100 O
] O
( O
C1 O
) O
– O
( O
D1 O
) O
; O
[ O
thick O
, O
green!70!black!100 O
] O
( O
C2 O
) O
– O
( O
D2 O
) O
; O
[ O
thick O
, O
green!70!black!100 O
] O
( O
C3 O
) O
– O
( O
D3 O
) O
; O
[ O
thick O
, O
green!70!black!100 O
] O
( O
C4 O
) O
– O
( O
D4 O
) O
; O
[ O
decoration O
= O
brace Method
][ Method
scale=1 Method
, Method
every Method
node Method
/ Method
.style Method
= O
minimum O
size=1 O
cm O
, O
on O
grid O
] O
bottom O
ıin O
0 O
, O
1 O
, O
2 O
, O
3 O
, O
4 O
ı O
[ O
yshift= O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
( O
Xı O
) O
at O
( O
0.15 O
, O
0.75 O
) O
; O
( O
Gı O
) O
at O
( O
1.5 O
, O
0.45 O
) O
; O
( O
ZAı O
) O
at O
( O
1.05 O
, O
0 O
) O
; O
( O
ZBı O
) O
at O
( O
1.35 O
, O
0 O
) O
; O
( O
ZCı O
) O
at O
( O
0.75 O
, O
0 O
) O
; O
( O
AAAı O
) O
at O
( O
0 O
, O
0 O
) O
; O
( O
AABı Method
) O
at O
( O
0 O
, O
2.4 O
) O
; O
( O
AACı O
) O
at O
( O
2.4 O
, O
0 O
) O
; O
( O
AADı O
) O
at O
( O
2.4 O
, O
2.4 O
) O
; O
( O
AAı O
) O
at O
( O
0.6 O
, O
0 O
) O
; O
( O
ABı O
) O
at O
( O
0.6 O
, O
0.9 O
) O
; O
( O
ACı O
) O
at O
( O
1.5 O
, O
0 O
) O
; O
( O
ADı O
) O
at O
( O
1.5 O
, O
0.9 O
) O
; O
( O
EAı O
) O
at O
( O
0 O
, O
0.6 O
) O
; O
( O
EBı O
) O
at O
( O
0 O
, O
0.9 O
) O
; O
( O
ECı O
) O
at O
( O
0.3 O
, O
0.6 O
) O
; O
( O
EDı O
) O
at O
( O
0.3 O
, O
0.9 O
) O
; O
ı O
[ O
white O
, O
fill O
, O
opacity=.7 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
2.4 O
, O
2.4 O
) O
; O
[ O
step=3 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
2.4 O
, O
2.4 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
2.4 O
, O
2.4 O
) O
; O
[ O
blue!20 O
, O
fill O
] O
( O
0.6 O
, O
0 O
) O
rectangle O
( O
1.5 O
, O
0.9 O
) O
; O
[ O
blue!90 O
] O
( O
0.6 O
, O
0 O
) O
rectangle O
( O
1.5 O
, O
0.9 O
) O
; O
[ O
step=3 O
mm O
, O
blue!70 O
] O
( O
0.6 O
, O
0 O
) O
grid O
( O
1.5 O
, O
0.9 O
) O
; O
[ O
red!20 O
, O
fill O
] O
( O
0 O
, O
0.6 O
) O
rectangle O
( O
0.3 O
, O
0.9 O
) O
; O
[ O
red!80!black!100 O
] O
( O
0 O
, O
0.6 O
) O
rectangle O
( O
0.3 O
, O
0.9 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
1.9 O
, O
0.9 O
) O
node O
[ O
below O
, O
xshift=1 O
mm O
, O
yshift=2 O
mm O
] O
black O
to O
[ O
out=90 O
, O
in=270 O
] O
( O
AAB0 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
0.8 O
, O
0.2 O
) O
node O
[ O
left O
] O
red!80!black!80 O
to O
[ O
out=50 O
, O
in=150 O
] O
( O
X4 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
0.6 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
to O
[ O
out=200 O
, O
in= O
- O
90 O
] O
( O
ZA4 O
) O
; O
[ O
- O
latex O
, O
thick O
, O
white O
] O
( O
2 O
, O
0.25 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
( O
patch Method
extraction Method
) O
to O
[ O
out=200 O
, O
in= O
- O
90 O
] O
( O
2.2 O
, O
0.5 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
0.6 O
) O
node O
[ O
right O
] O
to O
[ O
out=200 O
, O
in= O
- O
90 O
] O
( O
ZB4 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
0.6 O
) O
node O
[ O
right O
] O
to O
[ O
out=200 O
, O
in= O
- O
90 O
] O
( O
ZC4 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.5 O
, O
1.7 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
to O
[ O
out=190 O
, O
in=0 O
] O
( O
G4 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
2.4 O
) O
node O
[ O
right O
] O
black!80 Method
convolution Method
to O
[ O
out=190 O
, O
in=0 O
] O
( O
1.0 O
, O
1.7 O
) O
; O
[ O
- O
latex O
, O
white O
] O
( O
2.2 O
, O
2.1 O
) O
node O
[ O
right O
] O
black!80 O
+ O
non O
- O
linearity O
to O
[ O
out=150 O
, O
in= O
- O
90 O
] O
( O
2.2 O
, O
2.1 O
) O
; O
middle O
ıin O
0 O
, O
1 O
, O
2 O
, O
3 O
, O
4 O
ı O
[ O
yshift= O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
( O
Wı O
) O
at O
( O
0.75 O
, O
0.15 O
) O
; O
( O
BAAı O
) O
at O
( O
0 O
, O
0 O
) O
; O
( O
BABı O
) O
at O
( O
0 O
, O
1.8 O
) O
; O
( O
BACı O
) O
at O
( O
1.8 O
, O
0 O
) O
; O
( O
BADı O
) O
at O
( O
1.8 O
, O
1.8 O
) O
; O
( O
BAı O
) O
at O
( O
0.6 O
, O
0 O
) O
; O
( O
BBı O
) O
at O
( O
0.6 O
, O
0.3 O
) O
; O
( O
BCı O
) O
at O
( O
0.9 O
, O
0 O
) O
; O
( O
BDı O
) O
at O
( O
0.9 O
, O
0.3 O
) O
; O
ı O
[ O
white O
, O
fill O
, O
opacity=.7 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.8 O
, O
1.8 O
) O
; O
[ O
step=3 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
1.8 O
, O
1.8 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.8 O
, O
1.8 O
) O
; O
[ O
blue!20 O
, O
fill O
] O
( O
0.6 O
, O
0 O
) O
rectangle O
( O
0.9 O
, O
0.3 O
) O
; O
[ O
blue!90 O
] O
( O
0.6 O
, O
0 O
) O
rectangle O
( O
0.9 O
, O
0.3 O
) O
; O
[ O
decorate O
, O
decoration O
= O
brace O
, O
mirror O
, O
raise=1pt O
, O
line O
width=1pt O
] O
( O
BAC0 O
) O
– O
( O
BAC4 O
) O
node O
[ O
right O
, O
yshift= O
- O
2 O
mm O
, O
xshift=1 O
mm O
] O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
3.3 O
) O
node O
[ O
right O
] O
blue!80!black!80 O
to O
[ O
out=180 O
, O
in=60 O
] O
( O
W4 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
1.7 O
, O
2.7 O
) O
node O
[ O
below O
, O
yshift=3 O
mm O
] O
black O
to O
[ O
out=90 O
, O
in=180 O
] O
( O
BAB0 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
4.3 O
) O
node O
[ O
right O
] O
black!80 Method
Gaussian Method
filtering Method
to O
[ O
out=190 O
, O
in=0 O
] O
( O
1.3 O
, O
4 O
) O
; O
[ O
- O
latex O
, O
thick O
, O
white O
] O
( O
2.2 O
, O
4 O
) O
node O
[ O
right O
] O
black!80 O
+ O
downsampling O
to O
[ O
out=190 O
, O
in=0 O
] O
( O
2.2 O
, O
4 O
) O
; O
[ O
- O
latex O
, O
thick O
, O
white O
] O
( O
2.2 O
, O
3.7 O
) O
node O
[ O
right O
] O
black!80 O
= O
pooling O
to O
[ O
out=190 O
, O
in=0 O
] O
( O
2.2 O
, O
3.7 O
) O
; O
bottom O
[ O
thick O
, O
blue!70 O
] O
( O
BA0 O
) O
– O
( O
AA4 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
BB0 O
) O
– O
( O
AB4 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
BC0 O
) O
– O
( O
AC4 O
) O
; O
[ O
thick O
, O
blue!70 O
] O
( O
BD0 O
) O
– O
( O
AD4 O
) O
; O
top O
ıin O
0 O
, O
1 O
, O
2 O
, O
3 O
, O
4 O
ı O
[ O
yshift= O
, O
every O
node O
/ O
.append O
style= O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
, O
yslant=0.5 O
, O
xslant= O
- O
1 O
, O
rotate= O
- O
10 O
] O
( O
Hı O
) O
at O
( O
1.25 O
, O
0.75 O
) O
; O
( O
CAAı O
) O
at O
( O
0 O
, O
0 O
) O
; O
( O
CABı O
) O
at O
( O
0 O
, O
1.5 O
) O
; O
( O
CACı O
) O
at O
( O
1.5 O
, O
0 O
) O
; O
( O
CADı O
) O
at O
( O
1.5 O
, O
1.5 O
) O
; O
( O
CAı O
) O
at O
( O
1.0 O
, O
0.5 O
) O
; O
( O
CBı O
) O
at O
( O
1.0 O
, O
1.0 O
) O
; O
( O
CCı O
) O
at O
( O
1.5 O
, O
0.5 O
) O
; O
( O
CDı O
) O
at O
( O
1.5 O
, O
1.0 O
) O
; O
ı O
[ O
white O
, O
fill O
, O
opacity=.7 O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.5 O
, O
1.5 O
) O
; O
[ O
step=5 O
mm O
, O
gray!70 O
] O
( O
0 O
, O
0 O
) O
grid O
( O
1.5 O
, O
1.5 O
) O
; O
[ O
black O
] O
( O
0 O
, O
0 O
) O
rectangle O
( O
1.5 O
, O
1.5 O
) O
; O
[ O
green!20 O
, O
fill O
] O
( O
1.0 O
, O
0.5 O
) O
rectangle O
( O
1.5 O
, O
1.0 O
) O
; O
[ O
green!70!black!100 O
] O
( O
1.0 O
, O
0.5 O
) O
rectangle O
( O
1.5 O
, O
1.0 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
- O
1.6 O
, O
4.2 O
) O
node O
[ O
below O
, O
yshift=3 O
mm O
] O
black O
to O
[ O
out=90 O
, O
in=180 O
] O
( O
CAB0 O
) O
; O
[ O
- O
latex O
, O
thick O
] O
( O
2.2 O
, O
5.2 O
) O
node O
[ O
right O
] O
green!60!black!100 O
to O
[ O
out=180 O
, O
in=60 O
] O
( O
H4 O
) O
; O
middle O
[ O
thick O
, O
black!50 O
] O
( O
CAA0 O
) O
– O
( O
BAA4 O
) O
; O
[ O
thick O
, O
black!50 O
] O
( O
CAB0 O
) O
– O
( O
BAB4 O
) O
; O
[ O
thick O
, O
black!50 O
] O
( O
CAC0 O
) O
– O
( O
BAC4 O
) O
; O
[ O
thick O
, O
black!50 O
] O
( O
CAD0 Method
) O
– O
( O
BAD4 Method
) O
; O
The O
multilayer Method
convolutional Method
kernel Method
slightly O
differs O
from O
the O
hierarchical Method
kernel Method
descriptors Method
of O
but O
exploits O
similar O
ideas O
. O
Bo O
et O
al O
. O
define O
indeed O
several O
ad Method
hoc Method
kernels Method
for O
representing O
local O
information O
in O
images O
, O
such O
as O
gradient O
, O
color O
, O
or O
shape O
. O
These O
kernels O
are O
close O
to O
the O
one O
defined O
in O
( O
[ O
reference O
] O
) O
but O
with O
a O
few O
variations O
. O
Some O
of O
them O
do O
not O
use O
normalized O
features O
, O
and O
these O
kernels O
use O
different O
weighting Method
strategies Method
for O
the O
summands O
of O
( O
[ O
reference O
] O
) O
that O
are O
specialized O
to O
the O
image O
modality O
, O
e.g. O
, O
color O
, O
or O
gradient O
, O
whereas O
we O
use O
the O
same O
weight O
for O
all O
kernels O
. O
The O
generic O
formulation O
( O
[ O
reference O
] O
) O
that O
we O
propose O
may O
be O
useful O
per O
se O
, O
but O
our O
main O
contribution O
comes O
in O
the O
next O
section O
, O
where O
we O
use O
the O
kernel Method
as O
a O
new O
tool O
for O
learning O
convolutional Method
neural Method
networks Method
. O
section O
: O
Training O
Invariant Method
Convolutional Method
Kernel Method
Networks Method
Generic O
schemes O
have O
been O
proposed O
for O
approximating O
a O
non Task
- Task
linear Task
kernel Task
with O
a O
linear Method
one Method
, O
such O
as O
the O
Nyström Method
method Method
and O
its O
variants O
, O
or O
random Method
sampling Method
techniques Method
in O
the O
Fourier O
domain O
for O
shift Method
- Method
invariant Method
kernels Method
. O
In O
the O
context O
of O
convolutional Method
multilayer Method
kernels Method
, O
such O
an O
approximation O
is O
critical O
because O
computing O
the O
full O
kernel O
matrix O
on O
a O
database O
of O
images O
is O
computationally O
infeasible O
, O
even O
for O
a O
moderate O
number O
of O
images O
( O
) O
and O
moderate O
number O
of O
layers O
. O
For O
this O
reason O
, O
Bo O
et O
al O
. O
use O
the O
Nyström Method
method Method
for O
their O
hierarchical Method
kernel Method
descriptors Method
. O
In O
this O
section O
, O
we O
show O
that O
when O
the O
coordinate O
sets O
are O
two O
- O
dimensional O
regular O
grids O
, O
a O
natural O
approximation O
for O
the O
multilayer Method
convolutional Method
kernel Method
consists O
of O
a O
sequence O
of O
spatial Method
convolutions Method
with O
learned Method
filters Method
, O
pointwise Method
non Method
- Method
linearities Method
, O
and O
pooling Method
operations Method
, O
as O
illustrated O
in O
Figure O
[ O
reference O
] O
. O
More O
precisely O
, O
our O
scheme O
approximates O
the O
kernel O
map O
of O
defined O
in O
( O
[ O
reference O
] O
) O
at O
layer O
by O
finite O
- O
dimensional Method
spatial Method
maps Method
, O
where O
is O
a O
set O
of O
coordinates O
related O
to O
, O
and O
is O
a O
positive O
integer O
controlling O
the O
quality O
of O
the O
approximation O
. O
Consider O
indeed O
two O
images O
represented O
at O
layer O
by O
image O
feature O
maps O
and O
, O
respectively O
. O
Then O
, O
the O
corresponding O
maps O
and O
are O
learned O
such O
that O
, O
where O
is O
the O
Euclidean O
inner O
- O
product O
acting O
as O
if O
and O
were O
vectors O
in O
; O
the O
set O
is O
linked O
to O
by O
the O
relation O
where O
is O
a O
patch O
shape O
, O
and O
the O
quantities O
in O
admit O
finite O
- O
dimensional O
approximations O
in O
; O
as O
illustrated O
in O
Figure O
[ O
reference O
] O
, O
is O
a O
patch O
from O
centered O
at O
location O
with O
shape O
; O
an O
activation Method
map Method
is O
computed O
from O
by O
convolution Method
with Method
filters Method
followed O
by O
a O
non Method
- Method
linearity Method
. O
The O
subsequent O
map O
is O
obtained O
from O
by O
a O
pooling Method
operation Method
. O
We O
call O
this O
approximation Method
scheme Method
a O
convolutional Method
kernel Method
network Method
( O
CKN Method
) O
. O
In O
comparison O
to O
CNNs Method
, O
our O
approach O
enjoys O
similar O
benefits O
such O
as O
efficient O
prediction Task
at O
test O
time O
, O
and O
involves O
the O
same O
set O
of O
hyper O
- O
parameters O
: O
number O
of O
layers O
, O
numbers O
of O
filters O
at O
layer O
, O
shape O
of O
the O
filters O
, O
sizes O
of O
the O
feature O
maps O
. O
The O
other O
parameters O
can O
be O
automatically O
chosen O
, O
as O
discussed O
later O
. O
Training O
a O
CKN Method
can O
be O
argued O
to O
be O
as O
simple O
as O
training O
a O
CNN Method
in O
an O
unsupervised O
manner O
since O
we O
will O
show O
that O
the O
main O
difference O
is O
in O
the O
cost O
function O
that O
is O
optimized O
. O
subsection O
: O
Fast Method
Approximation Method
of Method
the Method
Gaussian Method
Kernel Method
A O
key O
component O
of O
our O
formulation O
is O
the O
Gaussian Method
kernel Method
. O
We O
start O
by O
approximating O
it O
by O
a O
linear Method
operation Method
with O
learned Method
filters Method
followed O
by O
a O
pointwise Method
non Method
- Method
linearity Method
. O
Our O
starting O
point O
is O
the O
next O
lemma O
, O
which O
can O
be O
obtained O
after O
a O
simple O
calculation O
. O
theorem O
: O
( O
Linear Method
expansion Method
of Method
the Method
Gaussian Method
Kernel Method
) O
. O
For O
all O
x O
and O
x′ O
in O
Rm O
, O
and O
> O
σ0 O
, O
The O
lemma O
gives O
us O
a O
mapping O
of O
any O
in O
to O
the O
function O
in O
, O
where O
the O
kernel O
is O
linear O
, O
and O
is O
the O
constant O
in O
front O
of O
the O
integral O
. O
To O
obtain O
a O
finite Method
- Method
dimensional Method
representation Method
, O
we O
need O
to O
approximate O
the O
integral O
with O
a O
weighted Method
finite Method
sum Method
, O
which O
is O
a O
classical O
problem O
arising O
in O
statistics Task
( O
see O
and O
chapter O
8 O
of O
) O
. O
Then O
, O
we O
consider O
two O
different O
cases O
. O
paragraph O
: O
Small O
dimension O
, O
. O
When O
the O
data O
lives O
in O
a O
compact O
set O
of O
, O
the O
integral O
in O
( O
[ O
reference O
] O
) O
can O
be O
approximated O
by O
uniform Method
sampling Method
over O
a O
large O
enough O
set O
. O
We O
choose O
such O
a O
strategy O
for O
two O
types O
of O
kernels O
from O
Eq O
. O
( O
[ O
reference O
] O
) O
: O
( O
i O
) O
the O
spatial O
kernels O
; O
( O
ii O
) O
the O
terms O
when O
is O
the O
“ O
gradient O
map O
” O
presented O
in O
Section O
[ O
reference O
] O
. O
In O
the O
latter O
case O
, O
and O
is O
the O
gradient O
orientation O
. O
We O
typically O
sample O
a O
few O
orientations O
as O
explained O
in O
Section O
[ O
reference O
] O
. O
paragraph O
: O
Higher O
dimensions O
. O
To O
prevent O
the O
curse O
of O
dimensionality O
, O
we O
learn O
to O
approximate O
the O
kernel O
on O
training O
data O
, O
which O
is O
intrinsically O
low O
- O
dimensional O
. O
We O
optimize O
importance O
weights O
in O
and O
sampling O
points O
in O
on O
training O
pairs O
in O
: O
Interestingly O
, O
we O
may O
already O
draw O
some O
links O
with O
neural Method
networks Method
. O
When O
applied O
to O
unit O
- O
norm O
vectors O
and O
, O
problem O
( O
[ O
reference O
] O
) O
produces O
sampling O
points O
whose O
norm O
is O
close O
to O
one O
. O
After O
learning O
, O
a O
new O
unit O
- O
norm O
point O
in O
is O
mapped O
to O
the O
vector O
in O
, O
which O
may O
be O
written O
as O
, O
assuming O
that O
the O
norm O
of O
is O
always O
one O
, O
where O
is O
the O
function O
for O
in O
. O
Therefore O
, O
the O
finite Method
- Method
dimensional Method
representation Method
of Method
only O
involves O
a O
linear Method
operation Method
followed O
by O
a O
non O
- O
linearity O
, O
as O
in O
typical O
neural Method
networks Method
. O
In O
Figure O
[ O
reference O
] O
, O
we O
show O
that O
the O
shape O
of O
resembles O
the O
“ O
rectified Method
linear Method
unit Method
” Method
function Method
. O
main O
[ O
scale=2 O
] O
[ O
- O
¿ O
, O
very O
thick O
, O
black O
] O
( O
- O
1 O
, O
0 O
) O
– O
( O
1 O
, O
0 O
) O
node O
[ O
right O
] O
; O
[ O
- O
¿ O
, O
very O
thick O
, O
black O
] O
( O
0 O
,- O
0.1 O
) O
– O
( O
0 O
, O
1 O
) O
node O
[ O
right O
, O
yshift= O
- O
2 O
mm O
] O
; O
[ O
scale=1 O
, O
domain= O
- O
1:1 O
, O
smooth O
, O
variable= O
x O
, O
very O
thick O
, O
blue O
] O
plot O
( O
x O
, O
exp O
( O
2 O
* O
( O
x O
- O
1 O
) O
) O
) O
; O
[ O
scale=1 O
, O
domain= O
- O
1:1 O
, O
smooth O
, O
variable= O
x O
, O
very O
thick O
, O
blue O
] O
plot O
( O
x O
, O
exp O
( O
4 O
* O
( O
x O
- O
1 O
) O
) O
) O
node O
[ O
left O
, O
xshift= O
- O
3 O
cm O
, O
yshift= O
- O
0.5 O
cm O
] O
; O
[ O
scale=1 O
, O
domain= O
- O
1:1 O
, O
smooth O
, O
variable= O
x O
, O
very O
thick O
, O
blue O
] O
plot O
( O
x O
, O
exp O
( O
6 O
* O
( O
x O
- O
1 O
) O
) O
) O
; O
[ O
scale=1 O
, O
domain= O
- O
1:1 O
, O
smooth O
, O
variable= O
x O
, O
very O
thick O
, O
dashed O
, O
red O
] O
plot O
( O
x O
, O
max O
( O
x O
, O
0 O
) O
) O
node O
[ O
left O
, O
xshift= O
- O
3.29 O
cm O
, O
yshift= O
- O
1 O
cm O
] O
; O
( O
0 O
, O
0 O
) O
node O
[ O
anchor O
= O
north O
, O
xshift=2 O
mm O
] O
0 O
; O
( O
1 O
, O
0 O
) O
node O
[ O
anchor O
= O
north O
] O
1 O
; O
( O
- O
1 O
, O
0 O
) O
node O
[ O
anchor O
= O
north O
] O
- O
1 O
; O
[ O
very O
thick O
] O
( O
- O
1 O
, O
0.05 O
) O
– O
( O
- O
1 O
,- O
0.05 O
) O
; O
[ O
very O
thick O
] O
( O
1 O
, O
0.05 O
) O
– O
( O
1 O
,- O
0.05 O
) O
; O
subsection O
: O
Approximating O
the O
Multilayer Method
Convolutional Method
Kernel Method
We O
have O
now O
all O
the O
tools O
in O
hand O
to O
build O
our O
convolutional Method
kernel Method
network Method
. O
We O
start O
by O
making O
assumptions O
on O
the O
input O
data O
, O
and O
then O
present O
the O
learning Method
scheme Method
and O
its O
approximation Method
principles Method
. O
paragraph O
: O
The O
zeroth Method
layer Method
. O
We O
assume O
that O
the O
input O
data O
is O
a O
finite Method
- Method
dimensional Method
map Method
, O
and O
that O
“ O
extracts O
” O
patches O
from O
. O
Formally O
, O
there O
exists O
a O
patch O
shape O
such O
that O
, O
, O
and O
for O
all O
in O
, O
is O
a O
patch O
of O
centered O
at O
. O
Then O
, O
property O
( O
B O
) O
described O
at O
the O
beginning O
of O
Section O
[ O
reference O
] O
is O
satisfied O
for O
by O
choosing O
. O
The O
examples O
of O
input O
feature O
maps O
given O
earlier O
satisfy O
this O
finite Method
- Method
dimensional Method
assumption Method
: O
for O
the O
gradient Method
map Method
, O
is O
the O
gradient O
of O
the O
image O
along O
each O
direction O
, O
with O
, O
is O
a O
patch O
, O
, O
and O
; O
for O
the O
patch Method
map Method
, O
is O
the O
input O
image O
, O
say O
with O
for O
RGB Material
data Material
. O
paragraph O
: O
The O
convolutional Method
kernel Method
network Method
. O
The O
zeroth Method
layer Method
being O
characterized O
, O
we O
present O
in O
Algorithms O
[ O
reference O
] O
and O
[ O
reference O
] O
the O
subsequent O
layers O
and O
how O
to O
learn O
their O
parameters O
in O
a O
feedforward Method
manner Method
. O
It O
is O
interesting O
to O
note O
that O
the O
input O
parameters O
of O
the O
algorithm O
are O
exactly O
the O
same O
as O
a O
CNN Method
— O
that O
is O
, O
number O
of O
layers O
and O
filters O
, O
sizes O
of O
the O
patches O
and O
feature O
maps O
( O
obtained O
here O
via O
the O
subsampling O
factor O
) O
. O
Ultimately O
, O
CNNs Method
and O
CKNs Method
only O
differ O
in O
the O
cost Metric
function Metric
that O
is O
optimized O
for O
learning O
the O
filters O
and O
in O
the O
choice O
of O
non O
- O
linearities O
. O
As O
we O
show O
next O
, O
there O
exists O
a O
link O
between O
the O
parameters O
of O
a O
CKN Method
and O
those O
of O
a O
convolutional Method
multilayer Method
kernel Method
. O
Convolutional Method
kernel Method
network Method
- O
learning O
the O
parameters O
of O
the O
k O
- O
th O
layer O
. O
[ O
1 O
] O
( O
sequence O
of O
- O
th O
maps O
obtained O
from O
training O
images O
) O
; O
( O
patch O
shape O
) O
; O
( O
number O
of O
filters Method
) O
; O
( O
number O
of O
training O
pairs O
) O
; O
extract O
at O
random O
pairs O
of O
patches O
with O
shape O
from O
the O
maps O
; O
if O
not O
provided O
by O
the O
user O
, O
set O
to O
the O
quantile O
of O
the O
data O
( O
; O
unsupervised Method
learning Method
: O
optimize O
( O
[ O
reference O
] O
) O
to O
obtain O
the O
filters O
in O
and O
in O
; O
, O
, O
and O
( O
smoothing O
parameter O
) O
; O
Convolutional Method
kernel Method
network Method
- O
computing O
the O
k O
- O
th O
map O
form O
the O
( O
⁢k–1 O
)- O
th O
one O
. O
[ O
1 O
] O
( O
input O
map O
) O
; O
( O
patch O
shape O
) O
; O
( O
subsampling O
factor O
) O
; O
( O
number O
of O
filters O
) O
; O
( O
smoothing O
parameter O
) O
; O
and O
( O
layer O
parameters O
) O
; O
convolution Method
and O
non Method
- Method
linearity Method
: O
define O
the O
activation Method
map Method
as O
where O
is O
a O
vector O
representing O
a O
patch O
from O
centered O
at O
with O
shape O
, O
and O
the O
vector O
is O
an O
- O
normalized O
version O
of O
. O
This O
operation O
can O
be O
interpreted O
as O
a O
spatial Method
convolution Method
of Method
the Method
map Method
with O
the O
filters Method
followed O
by O
pointwise O
non O
- O
linearities O
; O
set O
to O
be O
times O
the O
spacing O
between O
two O
pixels O
in O
; O
feature Method
pooling Method
: O
is O
obtained O
by O
subsampling O
by O
a O
factor O
and O
we O
define O
a O
new O
map O
obtained O
from O
by O
linear Method
pooling Method
with O
Gaussian Method
weights Method
: O
( O
new O
map Method
) O
; O
paragraph O
: O
Approximation Method
principles Method
. O
We O
proceed O
recursively O
to O
show O
that O
the O
kernel O
approximation O
property O
( O
A O
) O
is O
satisfied O
; O
we O
assume O
that O
( O
B O
) O
holds O
at O
layer O
, O
and O
then O
, O
we O
show O
that O
( O
A O
) O
and O
( O
B O
) O
also O
hold O
at O
layer O
. O
This O
is O
sufficient O
for O
our O
purpose O
since O
we O
have O
previously O
assumed O
( O
B O
) O
for O
the O
zeroth O
layer O
. O
Given O
two O
images O
feature O
maps O
and O
, O
we O
start O
by O
approximating O
by O
replacing O
and O
by O
their O
finite Method
- Method
dimensional Method
approximations Method
provided O
by O
( O
B O
) O
: O
Then O
, O
we O
use O
the O
finite Method
- Method
dimensional Method
approximation Method
of O
the O
Gaussian Method
kernel Method
involving O
and O
where O
is O
defined O
in O
( O
[ O
reference O
] O
) O
and O
is O
defined O
similarly O
by O
replacing O
by O
. O
Finally O
, O
we O
approximate O
the O
remaining O
Gaussian Method
kernel Method
by O
uniform Method
sampling Method
on O
, O
following O
Section O
[ O
reference O
] O
. O
After O
exchanging O
sums O
and O
grouping O
appropriate O
terms O
together O
, O
we O
obtain O
the O
new O
approximation O
where O
the O
constant O
comes O
from O
the O
multiplication O
of O
the O
constant O
from O
( O
[ O
reference O
] O
) O
and O
the O
weight O
of O
uniform Method
sampling Method
orresponding O
to O
the O
square O
of O
the O
distance O
between O
two O
pixels O
of O
. O
As O
a O
result O
, O
the O
right O
- O
hand O
side O
is O
exactly O
, O
where O
is O
defined O
in O
( O
[ O
reference O
] O
) O
, O
giving O
us O
property O
( O
A O
) O
. O
It O
remains O
to O
show O
that O
property O
( O
B O
) O
also O
holds O
, O
specifically O
that O
the O
quantity O
( O
[ O
reference O
] O
) O
can O
be O
approximated O
by O
the O
Euclidean Method
inner Method
- Method
product Method
with O
the O
patches O
and O
of O
shape O
; O
we O
assume O
for O
that O
purpose O
that O
is O
a O
subsampled O
version O
of O
the O
patch O
shape O
by O
a O
factor O
. O
We O
remark O
that O
the O
kernel Method
( O
[ O
reference O
] O
) O
is O
the O
same O
as O
( O
[ O
reference O
] O
) O
applied O
to O
layer O
by O
replacing O
by O
. O
By O
doing O
the O
same O
substitution O
in O
( O
[ O
reference O
] O
) O
, O
we O
immediately O
obtain O
an O
approximation O
of O
( O
[ O
reference O
] O
) O
. O
Then O
, O
all O
Gaussian O
terms O
are O
negligible O
for O
all O
and O
that O
are O
far O
from O
each O
other O
— O
say O
when O
. O
Thus O
, O
we O
may O
replace O
the O
sums O
by O
, O
which O
has O
the O
same O
set O
of O
“ O
non O
- O
negligible O
” O
terms O
. O
This O
yields O
exactly O
the O
approximation O
. O
paragraph O
: O
Optimization Task
. O
Regarding O
problem O
( O
[ O
reference O
] O
) O
, O
stochastic Method
gradient Method
descent Method
( O
SGD Method
) Method
may O
be O
used O
since O
a O
potentially O
infinite O
amount O
of O
training O
data O
is O
available O
. O
However O
, O
we O
have O
preferred O
to O
use O
L Method
- Method
BFGS Method
- Method
B Method
on O
pairs O
of O
randomly O
selected O
training O
data O
points O
, O
and O
initialize O
with O
the O
K Method
- Method
means Method
algorithm Method
. O
L Method
- Method
BFGS Method
- Method
B Method
is O
a O
parameter O
- O
free O
state O
- O
of O
- O
the O
- O
art O
batch Method
method Method
, O
which O
is O
not O
as O
fast O
as O
SGD Method
but O
much O
easier O
to O
use O
. O
We O
always O
run O
the O
L Method
- Method
BFGS Method
- Method
B Method
algorithm Method
for O
iterations O
, O
which O
seems O
to O
ensure O
convergence O
to O
a O
stationary O
point O
. O
Our O
goal O
is O
to O
demonstrate O
the O
preliminary O
performance O
of O
a O
new O
type O
of O
convolutional Method
network Method
, O
and O
we O
leave O
as O
future O
work O
any O
speed O
improvement O
. O
section O
: O
Experiments O
We O
now O
present O
experiments O
that O
were O
performed O
using O
Matlab Method
and O
an O
L Method
- Method
BFGS Method
- Method
B Method
solver Method
interfaced O
by O
Stephen O
Becker O
. O
Each O
image O
is O
represented O
by O
the O
last O
map O
of O
the O
CKN Method
, O
which O
is O
used O
in O
a O
linear Method
SVM Method
implemented O
in O
the O
software Method
package Method
LibLinear Method
. O
These O
representations O
are O
centered O
, O
rescaled O
to O
have O
unit O
- O
norm O
on O
average O
, O
and O
the O
regularization O
parameter O
of O
the O
SVM Method
is O
always O
selected O
on O
a O
validation O
set O
or O
by O
- O
fold O
cross Metric
- Metric
validation Metric
in O
the O
range O
, O
. O
The O
patches O
are O
typically O
small O
; O
we O
tried O
the O
sizes O
with O
for O
the O
first O
layer O
, O
and O
for O
the O
upper O
ones O
. O
The O
number O
of O
filters O
in O
our O
experiments O
is O
in O
the O
set O
. O
The O
downsampling O
factor O
is O
always O
chosen O
to O
be O
between O
two O
consecutive O
layers O
, O
whereas O
the O
last O
layer O
is O
downsampled O
to O
produce O
final O
maps O
of O
a O
small O
size O
— O
say O
, O
or O
. O
For O
the O
gradient Task
map Task
, O
we O
approximate O
the O
Gaussian Method
kernel Method
by O
uniformly O
sampling O
orientations O
, O
setting O
. O
Finally O
, O
we O
also O
use O
a O
small O
offset O
to O
prevent O
numerical O
instabilities O
in O
the O
normalization O
steps O
. O
subsection O
: O
Discovering O
the O
Structure Task
of Task
Natural Task
Image Task
Patches Task
Unsupervised Method
learning Method
was O
first O
used O
for O
discovering O
the O
underlying Task
structure Task
of Task
natural Task
image Task
patches Task
by O
Olshausen O
and O
Field O
. O
Without O
making O
any O
a O
priori O
assumption O
about O
the O
data O
except O
a O
parsimony Method
principle Method
, O
the O
method O
is O
able O
to O
produce O
small O
prototypes O
that O
resemble O
Gabor Method
wavelets Method
— O
that O
is O
, O
spatially O
localized O
oriented O
basis O
functions O
. O
The O
results O
were O
found O
impressive O
by O
the O
scientific O
community O
and O
their O
work O
received O
substantial O
attention O
. O
It O
is O
also O
known O
that O
such O
results O
can O
also O
be O
achieved O
with O
CNNs Method
. O
We O
show O
in O
this O
section O
that O
this O
is O
also O
the O
case O
for O
convolutional Method
kernel Method
networks O
, O
even O
though O
they O
are O
not O
explicitly O
trained O
to O
reconstruct O
data O
. O
Following O
, O
we O
randomly O
select O
a O
database O
of O
whitened O
natural O
image O
patches O
of O
size O
and O
learn O
filters Method
using O
the O
formulation O
( O
[ O
reference O
] O
) O
. O
We O
initialize O
with O
Gaussian O
random O
noise O
without O
performing O
the O
K Method
- Method
means Method
step Method
, O
in O
order O
to O
ensure O
that O
the O
output O
we O
obtain O
is O
not O
an O
artifact O
of O
the O
initialization O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
display O
the O
filters O
associated O
to O
the O
top O
- O
largest O
weights O
. O
Among O
the O
filters O
, O
exhibit O
interpretable O
Gabor O
- O
like O
structures O
and O
the O
rest O
was O
less O
interpretable O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
time O
that O
the O
explicit O
kernel Method
map Method
of O
the O
Gaussian Method
kernel Method
for O
whitened O
natural O
image O
patches O
is O
shown O
to O
be O
related O
to O
Gabor Method
wavelets Method
. O
subsection O
: O
Digit Task
Classification Task
on O
MNIST Material
The O
MNIST Material
dataset Material
consists O
of O
images O
of O
handwritten O
digits O
for O
training O
and O
for O
testing O
. O
We O
use O
two O
types O
of O
initial O
maps O
in O
our O
networks O
: O
the O
“ O
patch Method
map Method
” Method
, O
denoted O
by O
CNK Method
- Method
PM Method
and O
the O
“ Method
gradient Method
map Method
” Method
, O
denoted O
by O
CNK Method
- Method
GM Method
. O
We O
follow O
the O
evaluation O
methodology O
of O
for O
comparison O
when O
varying O
the O
training Metric
set Metric
size Metric
. O
We O
select O
the O
regularization O
parameter O
of O
the O
SVM Method
by Method
- Method
fold Method
cross Method
validation Method
when O
the O
training O
size O
is O
smaller O
than O
, O
or O
otherwise O
, O
we O
keep O
examples O
from O
the O
training O
set O
for O
validation O
. O
We O
report O
in O
Table O
[ O
reference O
] O
the O
results O
obtained O
for O
four O
simple O
architectures O
. O
CKN Method
- Method
GM1 Method
is O
the O
simplest O
one O
: O
its O
second O
layer O
uses O
patches O
and O
only O
filters O
, O
resulting O
in O
a O
network O
with O
parameters O
. O
Yet O
, O
it O
achieves O
an O
outstanding O
performance O
of O
error Metric
on O
the O
full O
dataset O
. O
The O
best O
performing O
, O
CKN Method
- Method
GM2 Method
, O
is O
similar O
to O
CKN Method
- Method
GM1 Method
but O
uses O
filters Method
. O
When O
working O
with O
raw O
patches O
, O
two O
layers O
( O
CKN Method
- Method
PM2 Method
) O
gives O
better O
results O
than O
one O
layer O
. O
More O
details O
about O
the O
network Method
architectures Method
are O
provided O
in O
the O
supplementary O
material O
. O
In O
general O
, O
our O
method O
achieves O
a O
state O
- O
of O
- O
the O
- O
art O
accuracy Metric
for O
this O
task O
since O
lower O
error Metric
rates O
have O
only O
been O
reported O
by O
using O
data Method
augmentation Method
. O
subsection O
: O
Visual Task
Recognition Task
on O
CIFAR Material
- Material
10 Material
and O
STL Material
- Material
10 Material
We O
now O
move O
to O
the O
more O
challenging O
datasets O
CIFAR Material
- Material
10 Material
and O
STL Material
- Material
10 Material
. O
We O
select O
the O
best O
architectures O
on O
a O
validation O
set O
of O
examples O
from O
the O
training O
set O
for O
CIFAR Material
- Material
10 Material
, O
and O
by O
- O
fold O
cross Method
- Method
validation Method
on O
STL Material
- Material
10 Material
. O
We O
report O
in O
Table O
[ O
reference O
] O
results O
for O
CKN Method
- Method
GM Method
, O
defined O
in O
the O
previous O
section O
, O
without O
exploiting O
color O
information O
, O
and O
CKN Method
- Method
PM Method
when O
working O
on O
raw O
RGB O
patches O
whose O
mean O
color O
is O
subtracted O
. O
The O
best O
selected O
models O
have O
always O
two O
layers O
, O
with O
filters Method
for O
the O
top O
layer O
. O
Since O
CKN Method
- Method
PM Method
and O
CKN Method
- Method
GM Method
exploit O
a O
different O
information O
, O
we O
also O
report O
a O
combination O
of O
such O
two O
models O
, O
CKN Method
- Method
CO Method
, O
by O
concatenating O
normalized Method
image Method
representations Method
together O
. O
The O
standard O
deviations O
for O
STL Material
- Material
10 Material
was O
always O
below O
. O
Our O
approach O
appears O
to O
be O
competitive O
with O
the O
state O
of O
the O
art O
, O
especially O
on O
STL Material
- Material
10 Material
where O
only O
one O
method O
does O
better O
than O
ours O
, O
despite O
the O
fact O
that O
our O
models O
only O
use O
layers O
and O
require O
learning O
few O
parameters O
. O
Note O
that O
better O
results O
than O
those O
reported O
in O
Table O
[ O
reference O
] O
have O
been O
obtained O
in O
the O
literature O
by O
using O
either O
data Method
augmentation Method
( O
around O
on O
CIFAR Material
- Material
10 Material
for O
) O
, O
or O
external O
data O
( O
around O
on O
STL Material
- Material
10 Material
for O
) O
. O
We O
are O
planning O
to O
investigate O
similar O
data O
manipulations O
in O
the O
future O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
have O
proposed O
a O
new O
methodology O
for O
combining O
kernels Method
and Method
convolutional Method
neural Method
networks Method
. O
We O
show O
that O
mixing O
the O
ideas O
of O
these O
two O
concepts O
is O
fruitful O
, O
since O
we O
achieve O
near O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
several O
datasets O
such O
as O
MNIST Material
, O
CIFAR Material
- Material
10 Material
, O
and O
STL10 Material
, O
with O
simple O
architectures Method
and O
no O
data Method
augmentation Method
. O
Some O
challenges O
regarding O
our O
work O
are O
left O
open O
for O
the O
future O
. O
The O
first O
one O
is O
the O
use O
of O
supervision O
to O
better O
approximate O
the O
kernel Method
for O
the O
prediction Task
task Task
. O
The O
second O
consists O
in O
leveraging O
the O
kernel Method
interpretation Method
of O
our O
convolutional Method
neural Method
networks Method
to O
better O
understand O
the O
theoretical O
properties O
of O
the O
feature O
spaces O
that O
these O
networks O
produce O
. O
subsubsection O
: O
Acknowledgments O
This O
work O
was O
partially O
supported O
by O
grants O
from O
ANR O
( O
project O
MACARON O
ANR O
- O
14 O
- O
CE23 O
- O
0003 O
- O
01 O
) O
, O
MSR O
- O
Inria O
joint O
centre O
, O
European O
Research O
Council O
( O
project O
ALLEGRO O
) O
, O
CNRS O
- O
Mastodons O
program O
( O
project O
GARGANTUA Method
) O
, O
and O
the O
LabEx O
PERSYVAL O
- O
Lab O
( O
ANR O
- O
11 O
- O
LABX O
- O
0025 O
) O
. O
bibliography O
: O
References O
appendix O
: O
Positive O
Definiteness O
of O
To O
show O
that O
the O
kernel O
defined O
in O
( O
[ O
reference O
] O
) O
is O
positive O
definite O
( O
p.d O
. O
) O
, O
we O
simply O
use O
elementary O
rules O
from O
the O
kernel Method
literature Method
described O
in O
Sections O
2.3.2 O
and O
3.4.1 O
of O
. O
A O
linear Method
combination Method
of Method
p.d Method
. Method
kernels Method
with O
non O
- O
negative O
weights O
is O
also O
p.d O
. O
( O
see O
Proposition O
3.22 O
of O
) O
, O
and O
thus O
it O
is O
sufficient O
to O
show O
that O
for O
all O
in O
, O
the O
following O
kernel O
on O
is O
p.d O
. O
: O
Specifically O
, O
it O
is O
also O
sufficient O
to O
show O
that O
the O
following O
kernel O
on O
is O
p.d O
. O
: O
with O
the O
convention O
if O
. O
This O
is O
a O
pointwise Method
product Method
of Method
two Method
kernels Method
and O
is O
p.d O
. O
when O
each O
of O
the O
two O
kernels O
is O
p.d O
. O
The O
first O
one O
is O
obviously O
p.d O
. O
: O
. O
The O
second O
one O
is O
a O
composition O
of O
the O
Gaussian Method
kernel Method
— O
which O
is O
p.d.— O
, O
with O
feature O
maps O
of O
a O
normalized Method
linear Method
kernel Method
in O
. O
This O
composition O
is O
p.d O
. O
according O
to O
Proposition O
3.22 O
, O
item O
( O
v O
) O
of O
since O
the O
normalization Method
does O
not O
remove O
the O
positive O
- O
definiteness O
property O
. O
appendix O
: O
List O
of O
Architectures O
Reported O
in O
the O
Experiments O
We O
present O
in O
details O
the O
architectures O
used O
in O
the O
paper O
in O
Table O
[ O
reference O
] O
. O
