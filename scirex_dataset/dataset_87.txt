Under	O
review	O
as	O
a	O
conference	O
paper	O
at	O
ICLR	O
2017	O
LEARNING	O
TO	O
DRAW	O
SAMPLES	O
:	O
WITH	O
APPLICATION	O
TO	O
AMORTIZED	Method
MLE	Method
FOR	O
GENERATIVE	Task
ADVERSAR	Method
-	Method
IAL	Method
LEARNING	Method
section	O
:	O
ABSTRACT	O
We	O
propose	O
a	O
simple	O
algorithm	O
to	O
train	O
stochastic	Method
neural	Method
networks	Method
to	O
draw	O
samples	O
from	O
given	O
target	O
distributions	O
for	O
probabilistic	Task
inference	Task
.	O
Our	O
method	O
is	O
based	O
on	O
iteratively	O
adjusting	O
the	O
neural	O
network	O
parameters	O
so	O
that	O
the	O
output	O
changes	O
along	O
a	O
Stein	Method
variational	Method
gradient	Method
[	O
reference	O
]	O
)	O
that	O
maximumly	O
decreases	O
the	O
KL	O
divergence	O
with	O
the	O
target	O
distribution	O
.	O
Our	O
method	O
works	O
for	O
any	O
target	O
distribution	O
specified	O
by	O
their	O
unnormalized	O
density	O
function	O
,	O
and	O
can	O
train	O
any	O
black	Method
-	Method
box	Method
architectures	Method
that	O
are	O
differentiable	O
in	O
terms	O
of	O
the	O
parameters	O
we	O
want	O
to	O
adapt	O
.	O
As	O
an	O
application	O
of	O
our	O
method	O
,	O
we	O
propose	O
an	O
amortized	Method
MLE	Method
algorithm	Method
for	O
training	O
deep	Method
energy	Method
model	Method
,	O
where	O
a	O
neural	Method
sampler	Method
is	O
adaptively	O
trained	O
to	O
approximate	O
the	O
likelihood	O
function	O
.	O
Our	O
method	O
mimics	O
an	O
adversarial	Method
game	Method
between	O
the	O
deep	Method
energy	Method
model	Method
and	O
the	O
neural	Method
sampler	Method
,	O
and	O
obtains	O
realisticlooking	O
images	O
competitive	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
section	O
:	O
INTRODUCTION	O
Modern	O
machine	Task
learning	Task
increasingly	O
relies	O
on	O
highly	O
complex	O
probabilistic	Method
models	Method
to	O
reason	O
about	O
uncertainty	O
.	O
A	O
key	O
computational	Task
challenge	Task
is	O
to	O
develop	O
efficient	O
inference	Method
techniques	Method
to	O
approximate	O
,	O
or	O
draw	O
samples	O
from	O
complex	O
distributions	O
.	O
Currently	O
,	O
most	O
inference	Method
methods	Method
,	O
including	O
MCMC	Method
and	Method
variational	Method
inference	Method
,	O
are	O
hand	O
-	O
designed	O
by	O
researchers	O
or	O
domain	O
experts	O
.	O
This	O
makes	O
it	O
difficult	O
to	O
fully	O
optimize	O
the	O
choice	O
of	O
different	O
methods	O
and	O
their	O
parameters	O
,	O
and	O
exploit	O
the	O
structures	O
in	O
the	O
problems	O
of	O
interest	O
in	O
an	O
automatic	O
way	O
.	O
The	O
hand	Method
-	Method
designed	Method
algorithm	Method
can	O
also	O
be	O
inefficient	O
when	O
it	O
requires	O
to	O
make	O
fast	O
inference	Task
repeatedly	O
on	O
a	O
large	O
number	O
of	O
different	O
distributions	O
with	O
similar	O
structures	O
.	O
This	O
happens	O
,	O
for	O
example	O
,	O
when	O
we	O
need	O
to	O
reason	O
about	O
a	O
number	O
of	O
observed	O
datasets	O
in	O
settings	O
like	O
online	Task
learning	Task
,	O
or	O
need	O
fast	Task
inference	Task
as	O
inner	O
loops	O
for	O
other	O
algorithms	O
such	O
as	O
maximum	Method
likelihood	Method
training	Method
.	O
Therefore	O
,	O
it	O
is	O
highly	O
desirable	O
to	O
develop	O
more	O
intelligent	O
probabilistic	Method
inference	Method
systems	Method
that	O
can	O
adaptively	O
improve	O
its	O
own	O
performance	O
to	O
fully	O
the	O
optimize	Metric
computational	Metric
efficiency	Metric
,	O
and	O
generalize	O
to	O
new	O
tasks	O
with	O
similar	O
structures	O
.	O
Specifically	O
,	O
denote	O
by	O
p	O
(	O
x	O
)	O
a	O
probability	O
density	O
of	O
interest	O
specified	O
up	O
to	O
the	O
normalization	O
constant	O
,	O
which	O
we	O
want	O
to	O
draw	O
sample	O
from	O
,	O
or	O
marginalize	O
to	O
estimate	O
its	O
normalization	O
constant	O
.	O
We	O
want	O
to	O
study	O
the	O
following	O
problem	O
:	O
Problem	O
1	O
.	O
Given	O
a	O
distribution	O
with	O
density	Method
p	Method
(	Method
x	Method
)	O
and	O
a	O
function	O
f	O
(	O
η	O
;	O
ξ	O
)	O
with	O
parameter	O
η	O
and	O
random	O
input	O
ξ	O
,	O
for	O
which	O
we	O
only	O
have	O
assess	O
to	O
draws	O
of	O
the	O
random	O
input	O
ξ	O
(	O
without	O
knowing	O
its	O
true	O
distribution	O
q	O
0	O
)	O
,	O
and	O
the	O
output	O
values	O
of	O
f	O
(	O
η	O
;	O
ξ	O
)	O
and	O
its	O
derivative	O
∂	O
η	O
f	O
(	O
η	O
;	O
ξ	O
)	O
given	O
η	O
and	O
ξ	O
.	O
We	O
want	O
to	O
find	O
an	O
optimal	O
parameter	O
η	O
so	O
that	O
the	O
density	O
of	O
the	O
random	O
output	O
variable	O
x	O
=	O
f	O
(	O
η	O
;	O
ξ	O
)	O
with	O
ξ	O
∼	O
q	O
0	O
closely	O
matches	O
the	O
target	O
density	O
p	O
(	O
x	O
)	O
.	O
Because	O
we	O
have	O
no	O
assumption	O
on	O
the	O
structure	O
of	O
f	O
(	O
η	O
;	O
ξ	O
)	O
and	O
the	O
distribution	O
of	O
random	O
input	O
,	O
we	O
can	O
not	O
directly	O
calculate	O
the	O
actual	O
distribution	O
of	O
the	O
output	O
random	O
variable	O
x	O
=	O
f	O
(	O
η	O
;	O
ξ	O
)	O
;	O
this	O
1	O
arXiv:1611.01722v2	O
[	O
reference	O
]	O
section	O
:	O
Nov	O
2016	O
Under	O
review	O
as	O
a	O
conference	O
paper	O
at	O
ICLR	O
2017	O
makes	O
it	O
difficult	O
to	O
solve	O
Problem	O
1	O
using	O
the	O
traditional	O
variational	Method
inference	Method
(	Method
VI	Method
)	Method
methods	Method
.	O
Recall	O
that	O
traditional	O
VI	Method
approximates	O
p	O
(	O
x	O
)	O
using	O
simple	O
proposal	Method
distributions	Method
q	O
η	O
(	O
x	O
)	O
indexed	O
by	O
parameter	O
η	O
,	O
and	O
finds	O
the	O
optimal	O
η	O
by	O
minimizing	O
KL	O
divergence	O
KL	O
(	O
q	O
η	O
||	O
p	O
)	O
=	O
E	O
qη	O
[	O
log	O
(	O
q	O
η	O
/	O
p	O
)	O
]	O
,	O
which	O
requires	O
to	O
calculate	O
the	O
density	O
q	O
η	O
(	O
x	O
)	O
or	O
its	O
derivative	O
that	O
is	O
not	O
computable	O
by	O
our	O
assumption	O
(	O
even	O
when	O
the	O
Monte	Method
Carlo	Method
gradient	Method
estimation	Method
and	O
the	O
reparametrization	Method
trick	Method
[	O
reference	O
]	O
are	O
applied	O
)	O
.	O
In	O
fact	O
,	O
it	O
is	O
this	O
requirement	O
of	O
calculating	Task
q	Task
η	Task
(	Task
x	Task
)	O
that	O
has	O
been	O
the	O
major	O
constraint	O
for	O
the	O
designing	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
variational	Method
inference	Method
methods	Method
with	O
rich	O
approximation	Method
families	Method
;	O
the	O
recent	O
successful	O
algorithms	O
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
to	O
name	O
only	O
a	O
few	O
)	O
have	O
to	O
handcraft	O
special	O
variational	O
families	O
to	O
ensure	O
the	O
computational	Metric
tractability	Metric
of	Metric
q	Metric
η	Metric
(	O
x	O
)	O
and	O
simultaneously	O
obtain	O
high	O
approximation	Metric
accuracy	Metric
,	O
which	O
require	O
substantial	O
mathematical	O
insights	O
and	O
research	O
effects	O
.	O
Methods	O
that	O
do	O
not	O
require	O
to	O
explicitly	O
calculate	O
q	O
η	O
(	O
x	O
)	O
can	O
significantly	O
simplify	O
the	O
design	O
and	O
applications	O
of	O
VI	Method
methods	Method
,	O
allowing	O
practical	O
users	O
to	O
focus	O
more	O
on	O
choosing	O
proposals	O
that	O
work	O
best	O
with	O
their	O
specific	O
tasks	O
.	O
We	O
will	O
use	O
the	O
term	O
wild	Task
variational	Task
inference	Task
to	O
refer	O
to	O
new	O
variants	O
of	O
variational	Method
methods	Method
that	O
require	O
no	O
tractability	O
q	O
η	O
(	O
x	O
)	O
,	O
to	O
distinguish	O
with	O
the	O
black	Method
-	Method
box	Method
variational	Method
inference	Method
[	O
reference	O
]	O
which	O
refers	O
to	O
methods	O
that	O
work	O
for	O
generic	O
target	O
distributions	O
p	O
(	O
x	O
)	O
without	O
significant	O
model	O
-	O
by	O
-	O
model	O
consideration	O
(	O
but	O
still	O
require	O
to	O
calculate	O
the	O
proposal	O
density	O
q	O
η	O
(	O
x	O
)	O
)	O
.	O
A	O
similar	O
problem	O
also	O
appears	O
in	O
importance	Task
sampling	Task
(	O
IS	Task
)	Task
,	O
where	O
it	O
requires	O
to	O
calculate	O
the	O
IS	O
proposal	O
density	O
q	O
(	O
x	O
)	O
in	O
order	O
to	O
calculate	O
the	O
importance	O
weight	O
w	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)/	O
q	O
(	O
x	O
)	O
.	O
However	O
,	O
there	O
exist	O
methods	O
that	O
use	O
no	O
explicit	O
information	O
of	O
q	O
(	O
x	O
)	O
,	O
which	O
,	O
seemingly	O
counter	O
-	O
intuitively	O
,	O
give	O
better	O
asymptotic	Metric
variance	Metric
or	O
converge	Metric
rates	Metric
than	O
the	O
typical	O
IS	O
that	O
uses	O
the	O
proposal	O
information	O
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Discussions	O
on	O
this	O
phenomenon	O
dates	O
back	O
to	O
[	O
reference	O
]	O
,	O
who	O
argued	O
that	O
"	O
Monte	Method
Carlo	Method
(	O
that	O
uses	O
the	O
proposal	O
information	O
)	O
is	O
fundamentally	O
unsound	O
"	O
for	O
violating	O
the	O
Likelihood	O
Principle	O
,	O
and	O
developed	O
Bayesian	Method
Monte	Method
Carlo	Method
[	O
reference	O
]	O
as	O
an	O
example	O
that	O
uses	O
no	O
information	O
on	O
q	O
(	O
x	O
)	O
,	O
yet	O
gives	O
better	O
convergence	Metric
rate	Metric
than	O
the	O
typical	O
Monte	Method
Carlo	Method
O	Method
(	Method
n	Method
−1	Method
/	Method
2	Method
)	Method
rate	Method
[	O
reference	O
]	O
.	O
Despite	O
the	O
substantial	O
difference	O
between	O
IS	O
and	O
VI	O
,	O
these	O
results	O
intuitively	O
suggest	O
the	O
possibility	O
of	O
developing	O
efficient	O
variational	Method
inference	Method
without	O
calculating	O
q	Method
(	Method
x	Method
)	Method
explicitly	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
algorithm	O
for	O
Problem	O
1	O
by	O
iteratively	O
adjusting	O
the	O
network	O
parameter	O
η	O
to	O
make	O
its	O
output	O
random	O
variable	O
changes	O
along	O
a	O
Stein	Method
variational	Method
gradient	Method
direction	O
(	O
SVGD	Method
)	O
[	O
reference	O
]	O
)	O
that	O
optimally	O
decreases	O
its	O
KL	O
divergence	O
with	O
the	O
target	O
distribution	O
.	O
Critically	O
,	O
the	O
SVGD	Method
gradient	O
includes	O
a	O
repulsive	Method
term	Method
to	O
ensure	O
that	O
the	O
generated	O
samples	O
have	O
the	O
right	O
amount	O
of	O
variability	O
that	O
matches	O
p	O
(	O
x	O
)	O
.	O
In	O
this	O
way	O
,	O
we	O
"	O
amortize	O
SVGD	Method
"	O
using	O
a	O
neural	Method
network	Method
,	O
which	O
makes	O
it	O
possible	O
for	O
our	O
method	O
to	O
adaptively	O
improve	O
its	O
own	O
efficiency	O
by	O
leveraging	O
fast	O
experience	O
,	O
especially	O
in	O
cases	O
when	O
it	O
needs	O
to	O
perform	O
fast	Task
inference	Task
repeatedly	O
on	O
a	O
large	O
number	O
of	O
similar	O
tasks	O
.	O
As	O
an	O
application	O
,	O
we	O
use	O
our	O
method	O
to	O
amortize	O
the	O
MLE	Method
training	Method
of	Method
deep	Method
energy	Method
models	Method
,	O
where	O
a	O
neural	Method
sampler	Method
is	O
adaptively	O
trained	O
to	O
approximate	O
the	O
likelihood	O
function	O
.	O
Our	O
method	O
,	O
which	O
we	O
call	O
SteinGAN	Method
,	O
mimics	O
an	O
adversarial	Method
game	Method
between	O
the	O
energy	Method
model	Method
and	O
the	O
neural	Method
sampler	Method
,	O
and	O
obtains	O
realistic	O
-	O
looking	O
images	O
competitive	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
produced	O
by	O
generative	Method
adversarial	Method
networks	Method
(	O
GAN	Method
)	O
[	O
reference	O
][	O
reference	O
]	O
.	O
Related	O
Work	O
The	O
idea	O
of	O
amortized	Method
inference	Method
[	O
reference	O
]	O
has	O
been	O
recently	O
applied	O
in	O
various	O
domains	O
of	O
probabilistic	Task
reasoning	Task
,	O
including	O
both	O
amortized	Method
variational	Method
inference	Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
]	O
,	O
and	O
data	Method
-	Method
driven	Method
proposals	Method
for	O
(	O
sequential	O
)	O
Monte	Method
Carlo	Method
methods	Method
(	O
e.g.	O
,	O
[	O
reference	O
]	O
,	O
to	O
name	O
only	O
a	O
few	O
.	O
Most	O
of	O
these	O
methods	O
,	O
however	O
,	O
require	O
to	O
explicitly	O
calculate	O
q	O
(	O
x	O
)	O
(	O
or	O
its	O
gradient	O
)	O
.	O
One	O
exception	O
is	O
a	O
very	O
recent	O
paper	O
[	O
reference	O
]	O
)	O
that	O
avoids	O
calculating	O
q	O
(	O
x	O
)	O
using	O
an	O
idea	O
related	O
to	O
Stein	O
discrepancy	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
There	O
is	O
also	O
a	O
raising	O
interest	O
recently	O
on	O
a	O
similar	O
problem	O
of	O
"	O
learning	Task
to	O
optimize	Task
"	O
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
which	O
is	O
technically	O
easier	O
than	O
the	O
more	O
general	O
problem	O
of	O
"	O
learning	Task
to	O
sample	O
"	O
.	O
In	O
fact	O
,	O
we	O
show	O
that	O
our	O
algorithm	O
reduces	O
to	O
"	O
learning	Task
to	Task
optimize	Task
"	O
when	O
only	O
one	O
particle	O
is	O
used	O
in	O
SVGD	Method
.	O
Generative	O
adversarial	O
network	O
(	O
GAN	Method
)	O
and	O
its	O
variants	O
have	O
recently	O
gained	O
remarkable	O
success	O
on	O
generating	O
realistic	Task
-	Task
looking	Task
images	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
All	O
these	O
methods	O
are	O
set	O
up	O
to	O
train	O
latent	Method
variable	Method
models	Method
(	O
the	O
generator	Method
)	O
under	O
the	O
assistant	O
of	O
the	O
discriminator	Method
.	O
Our	O
SteinGAN	Method
instead	O
performs	O
traditional	O
MLE	Method
training	Method
for	O
a	O
deep	Method
energy	Method
model	Method
,	O
with	O
the	O
help	O
of	O
a	O
neural	Method
sampler	Method
that	O
learns	O
to	O
draw	O
samples	O
from	O
the	O
energy	Method
model	Method
to	O
approximate	O
the	O
likelihood	O
function	O
;	O
this	O
admits	O
an	O
adversarial	O
interpretation	O
:	O
we	O
can	O
view	O
the	O
neural	Method
sampler	Method
as	O
a	O
generator	Method
that	O
attends	O
to	O
fool	O
the	O
deep	Method
energy	Method
model	Method
,	O
which	O
in	O
turn	O
serves	O
as	O
a	O
discriminator	Method
that	O
distinguishes	O
the	O
real	O
samples	O
and	O
the	O
simulated	O
samples	O
given	O
by	O
the	O
neural	Method
sampler	Method
.	O
This	O
idea	O
of	O
training	O
MLE	Method
with	O
neural	Method
samplers	Method
was	O
first	O
discussed	O
by	O
[	O
reference	O
]	O
;	O
one	O
of	O
the	O
key	O
differences	O
is	O
that	O
the	O
neural	Method
sampler	Method
in	O
[	O
reference	O
]	O
is	O
trained	O
with	O
the	O
help	O
of	O
a	O
heuristic	Method
diversity	Method
regularizer	Method
based	O
on	O
batch	Method
normalization	Method
,	O
while	O
SVGD	Method
enforces	O
the	O
diversity	O
in	O
a	O
more	O
principled	O
way	O
.	O
Another	O
method	O
by	O
[	O
reference	O
]	O
also	O
trains	O
an	O
energy	O
score	O
to	O
distinguish	O
real	O
and	O
simulated	O
samples	O
,	O
but	O
within	O
a	O
non	O
-	O
probabilistic	Method
framework	Method
(	O
see	O
Section	O
5	O
for	O
more	O
discussion	O
)	O
.	O
Other	O
more	O
traditional	O
approaches	O
for	O
training	O
energy	Method
-	Method
based	Method
models	Method
(	O
e.g.	O
,	O
[	O
reference	O
][	O
reference	O
]	O
are	O
often	O
based	O
on	O
variants	O
of	O
MCMC	Method
-	Method
MLE	Method
or	O
contrastive	Method
divergence	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
have	O
difficulty	O
generating	O
realistic	Task
-	Task
looking	Task
images	Task
from	O
scratch	O
.	O
section	O
:	O
STEIN	Method
VARIATIONAL	Method
GRADIENT	Method
DESCENT	Method
(	O
SVGD	Method
)	O
Stein	Method
variational	Method
gradient	Method
descent	Method
(	O
SVGD	Method
)	O
[	O
reference	O
]	O
)	O
is	O
a	O
general	Method
purpose	Method
Bayesian	Method
inference	Method
algorithm	Method
motivated	O
by	O
Stein	Method
's	Method
method	Method
[	O
reference	O
][	O
reference	O
]	O
and	O
kernelized	Method
Stein	Method
discrepancy	Method
[	O
reference	O
][	O
reference	O
]	O
.	O
It	O
uses	O
an	O
efficient	O
deterministic	Method
gradient	Method
-	Method
based	Method
update	Method
to	O
iteratively	O
evolve	O
a	O
set	O
of	O
particles	O
{	O
x	O
i	O
}	O
n	O
i=1	O
to	O
minimize	O
the	O
KL	O
divergence	O
with	O
the	O
target	O
distribution	O
.	O
SVGD	Method
has	O
a	O
simple	O
form	O
that	O
reduces	O
to	O
the	O
typical	O
gradient	Method
descent	Method
for	O
maximizing	Task
log	Task
p	Task
when	O
using	O
only	O
one	O
particle	O
(	O
n	O
=	O
1	O
)	O
,	O
and	O
hence	O
can	O
be	O
easily	O
combined	O
with	O
the	O
successful	O
tricks	O
for	O
gradient	Task
optimization	Task
,	O
including	O
stochastic	Method
gradient	Method
,	O
adaptive	Method
learning	Method
rates	Method
(	O
such	O
as	O
adagrad	Method
)	O
,	O
and	O
momentum	Method
.	O
To	O
give	O
a	O
quick	O
overview	O
of	O
the	O
main	O
idea	O
of	O
SVGD	Method
,	O
let	O
p	O
(	O
x	O
)	O
be	O
a	O
positive	Method
density	Method
function	Method
on	O
R	O
d	O
which	O
we	O
want	O
to	O
approximate	O
with	O
a	O
set	O
of	O
particles	O
{	O
x	O
i	O
}	O
n	O
i=1	O
.	O
SVGD	Method
initializes	O
the	O
particles	O
by	O
sampling	O
from	O
some	O
simple	O
distribution	O
q	O
0	O
,	O
and	O
updates	O
the	O
particles	O
iteratively	O
by	O
where	O
is	O
a	O
step	O
size	O
,	O
and	O
φ	O
(	O
x	O
)	O
is	O
a	O
"	O
particle	O
gradient	O
direction	O
"	O
chosen	O
to	O
maximumly	O
decrease	O
the	O
KL	O
divergence	O
between	O
the	O
distribution	O
of	O
particles	O
and	O
the	O
target	O
distribution	O
,	O
in	O
the	O
sense	O
that	O
where	O
q	O
[	O
reference	O
]	O
denotes	O
the	O
density	O
of	O
the	O
updated	O
particle	O
x	O
=	O
x	O
+	O
φ	O
(	O
x	O
)	O
when	O
the	O
density	O
of	O
the	O
original	O
particle	O
x	O
is	O
q	O
,	O
and	O
F	O
is	O
the	O
set	O
of	O
perturbation	O
directions	O
that	O
we	O
optimize	O
over	O
.	O
We	O
choose	O
F	O
to	O
be	O
the	O
unit	O
ball	O
of	O
a	O
vector	Method
-	Method
valued	Method
reproducing	Method
kernel	Method
Hilbert	Method
space	Method
(	O
RKHS	Method
)	O
H	O
d	O
=	O
H	O
×	O
·	O
·	O
·	O
×	O
H	O
with	O
each	O
H	O
associating	O
with	O
a	O
positive	O
definite	O
kernel	O
k	O
(	O
x	O
,	O
x	O
)	O
;	O
note	O
that	O
H	O
is	O
dense	O
in	O
the	O
space	O
of	O
continuous	O
functions	O
with	O
universal	Method
kernels	Method
such	O
as	O
the	O
Gaussian	Method
RBF	Method
kernel	Method
.	O
Critically	O
,	O
the	O
gradient	O
of	O
KL	O
divergence	O
in	O
(	O
2	O
)	O
equals	O
a	O
simple	O
linear	O
functional	O
of	O
φ	O
,	O
allowing	O
us	O
to	O
obtain	O
a	O
closed	Method
form	Method
solution	Method
for	O
the	O
optimal	O
φ	O
.	O
[	O
reference	O
]	O
showed	O
that	O
with	O
Algorithm	O
1	O
Amortized	O
SVGD	Method
for	O
Problem	O
1	O
Set	O
batch	O
size	O
m	O
,	O
step	O
-	O
size	O
scheme	O
{	O
t	O
}	O
and	O
kernel	O
k	O
(	O
x	O
,	O
x	O
)	O
.	O
,	O
calculate	O
x	O
i	O
=	O
f	O
(	O
η	O
t	O
;	O
ξ	O
i	O
)	O
,	O
and	O
the	O
Stein	Method
variational	Method
gradient	Method
∆x	O
i	O
in	O
(	O
7	O
)	O
.	O
Update	O
parameter	O
η	O
using	O
(	O
8	O
)	O
,	O
(	O
9	O
)	O
or	O
(	O
10	O
)	O
.	O
end	O
for	O
where	O
T	O
p	O
is	O
considered	O
as	O
a	O
linear	O
operator	O
acting	O
on	O
function	O
φ	O
and	O
is	O
called	O
the	O
Stein	O
operator	O
in	O
connection	O
with	O
Stein	O
's	O
identity	O
which	O
shows	O
that	O
the	O
RHS	O
of	O
(	O
3	O
)	O
equals	O
zero	O
if	O
p	O
=	O
q	O
:	O
This	O
is	O
a	O
result	O
of	O
integration	O
by	O
parts	O
assuming	O
the	O
value	O
of	O
p	O
(	O
x	O
)	O
φ	O
(	O
x	O
)	O
vanishes	O
on	O
the	O
boundary	O
of	O
the	O
integration	O
domain	O
.	O
Therefore	O
,	O
the	O
optimization	Task
in	O
(	O
2	O
)	O
reduces	O
to	O
where	O
D	O
(	O
q	O
||	O
p	O
)	O
is	O
the	O
kernelized	O
Stein	O
discrepancy	O
defined	O
in	O
,	O
which	O
equals	O
zero	O
if	O
and	O
only	O
if	O
p	O
=	O
q	O
under	O
mild	O
regularity	O
conditions	O
.	O
Importantly	O
,	O
the	O
optimal	O
solution	O
of	O
(	O
6	O
)	O
yields	O
a	O
closed	O
form	O
By	O
approximating	O
the	O
expectation	O
under	O
q	O
with	O
the	O
empirical	O
average	O
of	O
the	O
current	O
particles	O
{	O
x	O
i	O
}	O
n	O
i=1	O
,	O
SVGD	Method
admits	O
a	O
simple	O
form	O
of	O
update	Task
:	O
where	O
The	O
two	O
terms	O
in	O
∆x	O
i	O
play	O
two	O
different	O
roles	O
:	O
the	O
term	O
with	O
the	O
gradient	O
∇	O
x	O
log	O
p	O
(	O
x	O
)	O
drives	O
the	O
particles	O
toward	O
the	O
high	O
probability	O
regions	O
of	O
p	O
(	O
x	O
)	O
,	O
while	O
the	O
term	O
with	O
∇	O
x	O
k	O
(	O
x	O
,	O
x	O
i	O
)	O
serves	O
as	O
a	O
repulsive	O
force	O
to	O
encourage	O
diversity	O
;	O
to	O
see	O
this	O
,	O
consider	O
a	O
stationary	Method
kernel	Method
k	Method
(	Method
x	Method
,	O
x	O
)	O
=	O
k	O
(	O
x	O
−	O
x	O
)	O
,	O
then	O
the	O
second	O
term	O
reduces	O
toÊ	O
x	O
∇	O
x	O
k	O
(	O
x	O
,	O
x	O
i	O
)	O
=	O
−Ê	O
x	O
∇	O
xi	O
k	O
(	O
x	O
,	O
x	O
i	O
)	O
,	O
which	O
can	O
be	O
treated	O
as	O
the	O
negative	O
gradient	O
for	O
minimizing	O
the	O
average	Metric
similarityÊ	Metric
x	Metric
k	Metric
(	Metric
x	Metric
,	O
x	O
i	O
)	O
in	O
terms	O
of	O
x	O
i	O
.	O
Overall	O
,	O
this	O
particle	Method
update	Method
produces	O
diverse	O
points	O
for	O
distributional	Task
approximation	Task
and	Task
uncertainty	Task
assessment	Task
,	O
and	O
also	O
has	O
an	O
interesting	O
"	O
momentum	O
"	O
effect	O
in	O
which	O
the	O
particles	O
move	O
collaboratively	O
to	O
escape	O
the	O
local	O
optima	O
.	O
It	O
is	O
easy	O
to	O
see	O
from	O
(	O
7	O
)	O
that	O
∆x	O
i	O
reduces	O
to	O
the	O
typical	O
gradient	O
∇	O
x	O
log	O
p	O
(	O
x	O
i	O
)	O
when	O
there	O
is	O
only	O
a	O
single	O
particle	O
(	O
n	O
=	O
1	O
)	O
and	O
∇	O
x	O
k	O
(	O
x	O
,	O
x	O
i	O
)	O
when	O
x	O
=	O
x	O
i	O
,	O
in	O
which	O
case	O
SVGD	Method
reduces	O
to	O
the	O
standard	O
gradient	Method
ascent	Method
for	O
maximizing	Task
log	Task
p	Task
(	Task
x	Task
)	O
(	O
i.e.	O
,	O
maximum	Task
a	Task
posteriori	Task
(	Task
MAP	Task
)	Task
)	O
.	O
3	O
AMORTIZED	O
SVGD	Method
:	O
TOWARDS	O
AN	O
AUTOMATIC	O
NEURAL	O
SAMPLER	O
SVGD	Method
and	O
other	O
particle	Method
-	Method
based	Method
methods	Method
become	O
inefficient	O
when	O
we	O
need	O
to	O
repeatedly	O
infer	O
a	O
large	O
number	O
different	O
target	O
distributions	O
for	O
multiple	O
tasks	O
,	O
including	O
online	Task
learning	Task
or	O
inner	O
loops	O
of	O
other	O
algorithms	O
,	O
because	O
they	O
can	O
not	O
improve	O
based	O
on	O
the	O
experience	O
from	O
the	O
past	O
tasks	O
,	O
and	O
may	O
require	O
a	O
large	O
memory	O
to	O
restore	O
a	O
large	O
number	O
of	O
particles	O
.	O
We	O
propose	O
to	O
"	O
amortize	O
SVGD	Method
"	O
by	O
training	O
a	O
neural	Method
network	Method
f	Method
(	O
η	O
;	O
ξ	O
)	O
to	O
mimic	O
the	O
SVGD	Method
dynamics	O
,	O
yielding	O
a	O
solution	O
for	O
Problem	O
1	O
.	O
One	O
straightforward	O
way	O
to	O
achieve	O
this	O
is	O
to	O
run	O
SVGD	Method
to	O
convergence	O
and	O
train	O
f	O
(	O
η	O
;	O
ξ	O
)	O
to	O
fit	O
the	O
SVGD	Method
results	O
.	O
This	O
,	O
however	O
,	O
requires	O
to	O
run	O
many	O
epochs	O
of	O
fully	O
converged	O
SVGD	Method
and	O
can	O
be	O
slow	O
in	O
practice	O
.	O
We	O
instead	O
propose	O
an	O
incremental	Method
approach	Method
in	O
which	O
η	O
is	O
iteratively	O
adjusted	O
so	O
that	O
the	O
network	O
outputs	O
x	O
=	O
f	O
(	O
η	O
;	O
ξ	O
)	O
changes	O
along	O
the	O
Stein	Method
variational	Method
gradient	Method
direction	O
in	O
(	O
7	O
)	O
in	O
order	O
to	O
decrease	O
the	O
KL	O
divergence	O
between	O
the	O
target	O
and	O
approximation	O
distribution	O
.	O
To	O
be	O
specific	O
,	O
denote	O
by	O
η	O
t	O
the	O
estimated	O
parameter	O
at	O
the	O
t	O
-	O
th	O
iteration	O
of	O
our	O
method	O
;	O
each	O
iteration	O
of	O
our	O
method	O
draws	O
a	O
batch	O
of	O
random	O
inputs	O
{	O
ξ	O
i	O
}	O
m	O
i=1	O
and	O
calculate	O
their	O
corresponding	O
output	O
x	O
i	O
=	O
f	O
(	O
η	O
;	O
ξ	O
i	O
)	O
based	O
on	O
η	O
t	O
;	O
here	O
m	O
is	O
a	O
mini	O
-	O
batch	O
size	O
(	O
e.g.	O
,	O
m	O
=	O
100	O
)	O
.	O
The	O
Stein	Method
variational	Method
gradient	Method
∆x	O
i	O
in	O
(	O
7	O
)	O
would	O
then	O
ensure	O
that	O
x	O
i	O
=	O
x	O
i	O
+	O
∆x	Method
i	Method
forms	O
a	O
better	O
approximation	O
of	O
the	O
target	Method
distribution	Method
p.	Method
Therefore	O
,	O
we	O
should	O
adjust	O
η	O
to	O
make	O
its	O
output	O
matches	O
{	O
x	O
i	O
}	O
,	O
that	O
is	O
,	O
we	O
want	O
to	O
update	O
η	O
by	O
,	O
where	O
See	O
Algorithm	O
1	O
for	O
the	O
summary	O
of	O
this	O
procedure	O
.	O
If	O
we	O
assume	O
is	O
very	O
small	O
,	O
then	O
(	O
8	O
)	O
reduces	O
to	O
a	O
least	Method
square	Method
optimization	Method
.	O
To	O
see	O
this	O
,	O
note	O
that	O
f	O
(	O
η	O
;	O
As	O
a	O
result	O
,	O
(	O
8	O
)	O
reduces	O
to	O
the	O
following	O
least	Task
square	Task
optimization	Task
:	O
Update	O
(	O
9	O
)	O
can	O
still	O
be	O
computationally	O
expensive	O
because	O
of	O
the	O
matrix	Method
inversion	Method
.	O
We	O
can	O
derive	O
a	O
further	O
approximation	O
by	O
performing	O
only	O
one	O
step	O
of	O
gradient	Method
descent	Method
of	O
(	O
8	O
)	O
(	O
or	O
(	O
9	O
)	O
)	O
,	O
which	O
gives	O
Although	O
update	O
(	O
10	O
)	O
is	O
derived	O
as	O
an	O
approximation	O
of	O
(	O
8	O
)-(	O
9	O
)	O
,	O
it	O
is	O
computationally	O
faster	O
and	O
we	O
find	O
it	O
works	O
very	O
effectively	O
in	O
practice	O
;	O
this	O
is	O
because	O
when	O
is	O
small	O
,	O
one	O
step	O
of	O
gradient	Method
update	Method
can	O
be	O
sufficiently	O
close	O
to	O
the	O
optimum	O
.	O
Update	O
(	O
10	O
)	O
also	O
has	O
a	O
simple	O
and	O
intuitive	O
form	O
:	O
(	O
10	O
)	O
can	O
be	O
thought	O
as	O
a	O
"	O
chain	Method
rule	Method
"	O
that	O
backpropagates	O
the	O
Stein	Method
variational	Method
gradient	Method
to	O
the	O
network	O
parameter	O
η	O
.	O
This	O
can	O
be	O
justified	O
by	O
considering	O
the	O
special	O
case	O
when	O
we	O
use	O
only	O
a	O
single	O
particle	O
(	O
n	O
=	O
1	O
)	O
in	O
which	O
case	O
∆x	O
i	O
in	O
(	O
7	O
)	O
reduces	O
to	O
the	O
typical	O
gradient	O
∇	O
x	O
log	O
p	O
(	O
x	O
i	O
)	O
of	O
log	O
p	O
(	O
x	O
)	O
,	O
and	O
update	O
(	O
10	O
)	O
reduces	O
to	O
the	O
typical	O
gradient	Method
ascent	Method
for	O
maximizing	Task
in	O
which	O
case	O
f	O
(	O
η	O
;	O
ξ	O
)	O
is	O
trained	O
to	O
maximize	O
log	O
p	O
(	O
x	O
)	O
(	O
that	O
is	O
,	O
learning	O
to	O
optimize	O
)	O
,	O
instead	O
of	O
learning	O
to	O
draw	O
samples	O
from	O
p	O
for	O
which	O
it	O
is	O
crucial	O
to	O
use	O
Stein	Method
variational	Method
gradient	Method
∆x	O
i	O
to	O
diversify	O
the	O
network	O
outputs	O
.	O
Update	O
(	O
10	O
)	O
also	O
has	O
a	O
close	O
connection	O
with	O
the	O
typical	O
variational	Method
inference	Method
with	O
the	O
reparameterization	Method
trick	Method
[	O
reference	O
]	O
.	O
Let	O
q	O
η	O
(	O
x	O
)	O
be	O
the	O
density	O
function	O
of	O
x	O
=	O
f	O
(	O
η	O
;	O
ξ	O
)	O
,	O
ξ	O
∼	O
q	O
0	O
.	O
Using	O
the	O
reparameterization	Method
trick	Method
,	O
the	O
gradient	O
of	O
KL	O
(	O
q	O
η	O
||	O
p	O
)	O
w.r.t	O
.	O
η	O
can	O
be	O
shown	O
to	O
be	O
With	O
{	O
ξ	O
i	O
}	O
i.i.d	O
.	O
drawn	O
from	O
q	O
0	O
and	O
x	O
i	O
=	O
f	O
(	O
η	O
;	O
ξ	O
i	O
)	O
,	O
∀i	O
,	O
the	O
standard	O
stochastic	Method
gradient	Method
descent	Method
for	O
minimizing	O
the	O
KL	O
divergence	O
is	O
This	O
is	O
similar	O
with	O
(	O
10	O
)	O
,	O
but	O
replaces	O
the	O
Stein	O
gradient	O
∆x	O
i	O
defined	O
in	O
(	O
7	O
)	O
with∆x	O
i	O
.	O
The	O
advantage	O
of	O
using	O
∆x	Method
i	Method
is	O
that	O
it	O
does	O
not	O
require	O
to	O
explicitly	O
calculate	O
q	O
η	O
,	O
and	O
hence	O
admits	O
a	O
solution	O
to	O
Problem	O
1	O
in	O
which	O
q	O
η	O
is	O
not	O
computable	O
for	O
complex	O
network	O
f	O
(	O
η	O
;	O
ξ	O
)	O
and	O
unknown	O
input	O
distribution	O
q	O
0	O
.	O
Further	O
insights	O
can	O
be	O
obtained	O
by	O
noting	O
that	O
where	O
(	O
12	O
)	O
is	O
obtained	O
by	O
using	O
Stein	O
's	O
identity	O
(	O
5	O
)	O
.	O
Therefore	O
,	O
∆x	O
i	O
can	O
be	O
treated	O
as	O
a	O
kernel	Method
smoothed	Method
version	Method
of∆x	Method
i	Method
.	O
section	O
:	O
AMORTIZED	Method
MLE	Method
FOR	O
GENERATIVE	Task
ADVERSARIAL	Task
TRAINING	Task
Our	O
method	O
allows	O
us	O
to	O
design	O
efficient	O
approximate	Method
sampling	Method
methods	Method
adaptively	O
and	O
automatically	O
,	O
and	O
enables	O
a	O
host	O
of	O
novel	O
applications	O
.	O
In	O
this	O
paper	O
,	O
we	O
apply	O
it	O
in	O
an	O
amortized	Method
MLE	Method
method	Method
for	O
training	O
deep	Method
generative	Method
models	Method
.	O
Maximum	Method
likelihood	Method
estimator	Method
(	O
MLE	Method
)	O
provides	O
a	O
fundamental	O
approach	O
for	O
learning	Task
probabilistic	Task
models	Task
from	O
data	O
,	O
but	O
can	O
be	O
computationally	O
prohibitive	O
on	O
distributions	O
for	O
which	O
drawing	O
samples	O
or	O
computing	O
likelihood	O
is	O
intractable	O
due	O
to	O
the	O
normalization	O
constant	O
.	O
Traditional	O
methods	O
such	O
as	O
MCMC	Method
-	Method
MLE	Method
use	O
hand	Method
-	Method
designed	Method
methods	Method
(	O
e.g.	O
,	O
MCMC	Method
)	O
to	O
approximate	O
the	O
intractable	O
likelihood	O
function	O
but	O
do	O
not	O
work	O
efficiently	O
in	O
practice	O
.	O
We	O
propose	O
to	O
adaptively	O
train	O
a	O
generative	Method
neural	Method
network	Method
to	O
draw	O
samples	O
from	O
the	O
distribution	O
during	O
MLE	Method
training	Method
,	O
which	O
not	O
only	O
provides	O
computational	O
advantage	O
,	O
and	O
also	O
allows	O
us	O
to	O
generate	O
realistic	O
-	O
looking	O
images	O
competitive	O
with	O
,	O
or	O
better	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
generative	Method
adversarial	Method
networks	Method
(	O
GAN	Method
)	O
[	O
reference	O
][	O
reference	O
]	O
)	O
(	O
see	O
Figure	O
1	O
-	O
5	O
)	O
.	O
To	O
be	O
specific	O
,	O
denote	O
by	O
{	O
x	O
i	O
,	O
obs	O
}	O
a	O
set	O
of	O
observed	O
data	O
.	O
We	O
consider	O
the	O
maximum	Method
likelihood	Method
training	Method
of	Method
energy	Method
-	Method
based	Method
models	Method
of	O
form	O
where	O
φ	O
(	O
x	O
;	O
θ	O
)	O
is	O
an	O
energy	O
function	O
for	O
x	O
indexed	O
by	O
parameter	O
θ	O
and	O
Φ	O
(	O
θ	O
)	O
is	O
the	O
log	O
-	O
normalization	O
constant	O
.	O
The	O
log	Method
-	Method
likelihood	Method
function	Method
of	Method
θ	Method
is	O
whose	O
gradient	O
is	O
whereÊ	O
obs	O
[	O
·	O
]	O
and	O
E	O
θ	O
[	O
·	O
]	O
denote	O
the	O
empirical	O
average	O
on	O
the	O
observed	O
data	O
{	O
x	O
i	O
,	O
obs	O
}	O
and	O
the	O
expectation	O
under	O
model	O
p	O
(	O
x|θ	O
)	O
,	O
respectively	O
.	O
The	O
key	O
computational	Metric
difficulty	Metric
is	O
to	O
approximate	O
the	O
model	O
expectation	O
E	O
θ	O
[	O
·	O
]	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
use	O
a	O
generative	Method
neural	Method
network	Method
x	Method
=	O
f	O
(	O
η	O
;	O
ξ	O
)	O
trained	O
by	O
Algorithm	O
1	O
to	O
approximately	O
sample	O
from	O
p	O
(	O
x|θ	O
)	O
,	O
yielding	O
a	O
gradient	Method
update	Method
for	O
θ	O
of	O
form	O
whereÊ	O
η	O
denotes	O
the	O
empirical	O
average	O
on	O
{	O
x	O
i	O
}	O
where	O
As	O
θ	O
is	O
updated	O
by	O
gradient	Method
ascent	Method
,	O
η	O
is	O
successively	O
updated	O
via	O
Algorithm	O
1	O
to	O
follow	O
p	O
(	O
x|θ	O
)	O
.	O
See	O
Algorithm	O
2	O
.	O
We	O
call	O
our	O
method	O
SteinGAN	Method
,	O
because	O
it	O
can	O
be	O
intuitively	O
interpreted	O
as	O
an	O
adversarial	Method
game	Method
between	O
the	O
generative	Method
network	Method
f	O
(	O
η	O
;	O
ξ	O
)	O
and	O
the	O
energy	Method
model	Method
p	Method
(	Method
x|θ	Method
)	O
which	O
serves	O
as	O
a	O
discriminator	Method
:	O
The	O
Algorithm	O
2	O
Amortized	Method
MLE	Method
as	O
Generative	Method
Adversarial	Method
Learning	Method
Goal	Method
:	O
MLE	Method
training	Method
for	O
energy	Method
model	Method
p	Method
(	Method
x|θ	Method
)	O
=	O
exp	O
(	O
−φ	O
(	O
x	O
,	O
θ	O
)	O
−	O
Φ	O
(	O
θ	O
)	O
)	O
.	O
Initialize	O
η	O
and	O
θ	O
.	O
for	O
iteration	O
t	O
do	O
Updating	O
η	O
:	O
Draw	O
ξ	O
i	O
∼	O
q	O
0	O
,	O
x	O
i	O
=	O
f	O
(	O
η	O
;	O
ξ	O
i	O
)	O
;	O
update	O
η	O
using	O
(	O
8	O
)	O
,	O
(	O
9	O
)	O
or	O
(	O
10	O
)	O
with	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x|θ	O
)	O
.	O
Repeat	O
several	O
times	O
when	O
needed	O
.	O
Updating	Task
θ	Task
:	O
Draw	O
a	O
mini	O
-	O
batch	O
of	O
observed	O
data	O
{	O
x	O
i	O
,	O
obs	O
}	O
,	O
and	O
simulated	O
data	O
x	O
i	O
=	O
f	O
(	O
η	O
;	O
ξ	O
i	O
)	O
,	O
update	O
θ	O
by	O
(	O
13	O
)	O
.	O
end	O
for	O
MLE	Method
gradient	Method
update	Method
of	Method
p	Method
(	Method
x|θ	Method
)	Method
effectively	O
decreases	O
the	O
energy	O
of	O
the	O
training	O
data	O
and	O
increases	O
the	O
energy	O
of	O
the	O
simulated	O
data	O
from	O
f	O
(	O
η	O
;	O
ξ	O
)	O
,	O
while	O
the	O
SVGD	Method
update	O
of	O
f	O
(	O
η	O
;	O
ξ	O
)	O
decreases	O
the	O
energy	O
of	O
the	O
simulated	O
data	O
to	O
fit	O
better	O
with	O
p	O
(	O
x|θ	O
)	O
.	O
Compared	O
with	O
the	O
traditional	O
methods	O
based	O
on	O
MCMC	Method
-	Method
MLE	Method
or	O
contrastive	Method
divergence	Method
,	O
we	O
amortize	O
the	O
sampler	O
as	O
we	O
train	O
,	O
which	O
gives	O
much	O
faster	O
speed	O
and	O
simultaneously	O
provides	O
a	O
high	O
quality	O
generative	Method
neural	Method
network	Method
that	O
can	O
generate	O
realistic	O
-	O
looking	O
images	O
;	O
see	O
[	O
reference	O
]	O
for	O
a	O
similar	O
idea	O
and	O
discussions	O
.	O
section	O
:	O
EMPIRICAL	O
RESULTS	O
We	O
evaluated	O
our	O
SteinGAN	Method
on	O
four	O
datasets	O
,	O
MNIST	O
,	O
CIFAR	Material
-	Material
10	Material
,	O
CelebA	O
[	O
reference	O
]	O
,	O
and	O
Large	Task
-	Task
scale	Task
Scene	Task
Understanding	Task
(	O
LSUN	Task
)	O
[	O
reference	O
]	O
,	O
on	O
which	O
we	O
find	O
our	O
method	O
tends	O
to	O
generate	O
realistic	Task
-	Task
looking	Task
images	Task
competitive	O
with	O
,	O
sometimes	O
better	O
than	O
DCGAN	Method
[	O
reference	O
]	O
)	O
(	O
see	O
Figure	O
2	O
-	O
Figure	O
3	O
)	O
.	O
Our	O
code	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
DartML	O
/	O
SteinGAN	Method
.	O
section	O
:	O
Model	O
Setup	O
In	O
order	O
to	O
generate	O
realistic	Task
-	Task
looking	Task
images	Task
,	O
we	O
define	O
our	O
energy	Method
model	Method
based	O
on	O
an	O
autoencoder	Method
:	O
where	O
x	O
denotes	O
the	O
image	O
.	O
This	O
choice	O
is	O
motivated	O
by	O
Energy	O
-	O
based	O
GAN	Method
[	O
reference	O
]	O
in	O
which	O
the	O
autoencoder	O
loss	O
is	O
used	O
as	O
a	O
discriminator	Method
but	O
without	O
a	O
probabilistic	O
interpretation	O
.	O
We	O
assume	O
f	O
(	O
η	O
;	O
ξ	O
)	O
to	O
be	O
a	O
neural	Method
network	Method
whose	O
input	O
ξ	O
is	O
a	O
100	O
-	O
dimensional	O
random	O
vector	O
drawn	O
by	O
Uniform	O
([	O
−1	O
,	O
1	O
]	O
)	O
.	O
The	O
positive	O
definite	O
kernel	O
in	O
SVGD	Method
is	O
defined	O
by	O
the	O
RBF	Method
kernel	Method
on	O
the	O
hidden	O
representation	O
obtained	O
by	O
the	O
autoencoder	Method
in	O
(	O
14	O
)	O
,	O
that	O
is	O
,	O
As	O
it	O
is	O
discussed	O
in	O
Section	O
3	O
,	O
the	O
kernel	Method
provides	O
a	O
repulsive	O
force	O
to	O
produce	O
an	O
amount	O
of	O
variability	O
required	O
for	O
generating	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
This	O
is	O
similar	O
to	O
the	O
heuristic	Method
repelling	Method
regularizer	Method
in	O
[	O
reference	O
]	O
and	O
the	O
batch	Method
normalization	Method
based	Method
regularizer	Method
in	O
[	O
reference	O
]	O
,	O
but	O
is	O
derived	O
in	O
a	O
more	O
principled	O
way	O
.	O
We	O
take	O
the	O
bandwidth	O
to	O
be	O
h	O
=	O
0.5	O
×	O
med	O
,	O
where	O
med	O
is	O
the	O
median	O
of	O
the	O
pairwise	O
distances	O
between	O
E	O
(	O
x	O
)	O
on	O
the	O
image	O
simulated	O
by	O
f	O
(	O
η	O
;	O
ξ	O
)	O
.	O
This	O
makes	O
the	O
kernel	O
change	O
adaptively	O
based	O
on	O
both	O
θ	O
(	O
through	O
E	O
(	O
x	O
;	O
θ	O
)	O
)	O
and	O
η	O
(	O
through	O
bandwidth	O
h	O
)	O
.	O
Some	O
datasets	O
include	O
both	O
images	O
x	O
and	O
their	O
associated	O
discrete	O
labels	O
y.	O
In	O
these	O
cases	O
,	O
we	O
train	O
a	O
joint	Method
energy	Method
model	Method
on	O
(	O
x	O
,	O
y	O
)	O
to	O
capture	O
both	O
the	O
inner	O
structure	O
of	O
the	O
images	O
and	O
its	O
predictive	O
relation	O
with	O
the	O
label	O
,	O
allowing	O
us	O
to	O
simulate	O
images	O
with	O
a	O
control	O
on	O
which	O
category	O
it	O
belongs	O
to	O
.	O
Our	O
joint	Method
energy	Method
model	Method
is	O
defined	O
to	O
be	O
where	O
σ	O
(	O
·	O
,	O
·	O
)	O
is	O
the	O
cross	O
entropy	O
loss	O
function	O
of	O
a	O
fully	Method
connected	Method
output	Method
layer	Method
.	O
In	O
this	O
case	O
,	O
our	O
neural	Method
sampler	Method
first	O
draws	O
a	O
label	O
y	O
randomly	O
according	O
to	O
the	O
empirical	O
counts	O
in	O
the	O
dataset	O
,	O
and	O
then	O
passes	O
y	O
into	O
a	O
neural	Method
network	Method
together	O
with	O
a	O
100	O
×	O
1	O
random	Method
vector	Method
ξ	Method
to	O
generate	O
image	O
x.	O
This	O
allows	O
us	O
to	O
generate	O
images	O
for	O
particular	O
categories	O
by	O
controlling	O
the	O
value	O
of	O
input	O
y.	O
Stabilization	Task
In	O
practice	O
,	O
we	O
find	O
it	O
is	O
useful	O
to	O
modify	O
(	O
13	O
)	O
to	O
be	O
where	O
γ	O
is	O
a	O
discount	O
factor	O
(	O
which	O
we	O
take	O
to	O
be	O
γ	O
=	O
0.7	O
)	O
.	O
This	O
is	O
equivalent	O
to	O
maximizing	O
a	O
regularized	O
likelihood	O
:	O
max	O
where	O
Φ	O
(	O
θ	O
)	O
is	O
the	O
log	O
-	O
partition	O
function	O
;	O
note	O
that	O
exp	O
(	O
γΦ	O
(	O
θ	O
)	O
)	O
is	O
a	O
conjugate	O
prior	O
of	O
p	O
(	O
x|θ	O
)	O
.	O
We	O
initialize	O
the	O
weights	O
of	O
both	O
the	O
generator	Method
and	O
discriminator	Method
from	O
Gaussian	Method
distribution	Method
N	O
(	O
0	O
,	O
0.02	O
)	O
,	O
and	O
train	O
them	O
using	O
Adam	Method
[	O
reference	O
]	O
)	O
with	O
a	O
learning	Metric
rate	Metric
of	O
0.001	O
for	O
the	O
generator	Method
and	O
0.0001	O
for	O
the	O
energy	Method
model	Method
(	O
the	O
discriminator	Method
)	O
.	O
In	O
order	O
to	O
keep	O
the	O
generator	O
and	O
discriminator	O
approximately	O
aligned	O
during	O
training	Task
,	O
we	O
speed	O
up	O
the	O
MLE	Method
update	Method
(	O
16	O
)	O
of	O
the	O
discriminator	O
(	O
by	O
increasing	O
its	O
learning	Metric
rate	Metric
to	O
0.0005	O
)	O
when	O
the	O
energy	O
of	O
the	O
real	O
data	O
batch	O
is	O
larger	O
than	O
the	O
energy	O
of	O
the	O
simulated	O
images	O
,	O
while	O
slow	O
down	O
it	O
(	O
by	O
freezing	O
the	O
MLE	Method
update	Method
of	O
θ	O
in	O
(	O
16	O
)	O
)	O
if	O
the	O
magnitude	O
of	O
the	O
energy	O
difference	O
between	O
the	O
real	O
images	O
and	O
the	O
simulated	O
images	O
goes	O
above	O
a	O
threshold	O
of	O
0.5	O
.	O
We	O
used	O
the	O
bag	Method
of	Method
architecture	Method
guidelines	Method
for	O
stable	Task
training	Task
suggested	O
in	O
DCGAN	Method
[	O
reference	O
]	O
.	O
section	O
:	O
Discussion	O
The	O
MNIST	O
dataset	O
has	O
a	O
training	O
set	O
of	O
60	O
,	O
000	O
examples	O
.	O
Both	O
DCGAN	Method
and	O
our	O
model	O
produce	O
high	O
quality	O
images	O
,	O
both	O
visually	O
indistinguishable	O
from	O
real	O
images	O
;	O
see	O
figure	O
1	O
.	O
CIFAR	Material
-	Material
10	Material
is	O
very	O
diverse	O
,	O
and	O
with	O
only	O
50	O
,	O
000	O
training	O
examples	O
.	O
Figure	O
2	O
shows	O
examples	O
of	O
simulated	O
images	O
by	O
DCGAN	Method
and	O
SteinGAN	Method
generated	O
conditional	O
on	O
each	O
category	O
,	O
which	O
look	O
equally	O
well	O
visually	O
.	O
We	O
also	O
provide	O
quantitively	O
evaluation	O
using	O
a	O
recently	O
proposed	O
inception	Metric
score	Metric
[	O
reference	O
]	O
,	O
as	O
well	O
as	O
the	O
classification	Metric
accuracy	Metric
when	O
training	O
ResNet	Method
using	O
50	O
,	O
000	O
simulated	O
images	O
as	O
train	O
sets	O
,	O
evaluated	O
on	O
a	O
separate	O
held	O
-	O
out	O
testing	O
set	O
never	O
seen	O
by	O
the	O
GAN	Method
models	O
.	O
Besides	O
DCGAN	Method
and	O
SteinGAN	Method
,	O
we	O
also	O
evaluate	O
another	O
simple	O
baseline	O
obtained	O
by	O
subsampling	O
500	O
real	O
images	O
from	O
the	O
training	O
set	O
and	O
duplicating	O
them	O
100	O
times	O
.	O
We	O
observe	O
that	O
these	O
scores	O
capture	O
rather	O
different	O
perspectives	O
of	O
image	Task
generation	Task
:	O
The	O
inception	Metric
score	Metric
favors	O
images	O
that	O
look	O
realistic	O
individually	O
and	O
have	O
uniformly	O
distributed	O
labels	O
;	O
as	O
a	O
result	O
,	O
the	O
inception	Metric
score	Metric
of	O
the	O
duplicated	O
500	O
images	O
is	O
almost	O
as	O
high	O
as	O
the	O
real	O
training	O
set	O
.	O
We	O
find	O
that	O
the	O
inception	Metric
score	Metric
of	O
SteinGAN	Method
is	O
comparable	O
,	O
or	O
slightly	O
lower	O
than	O
that	O
of	O
DCGAN	Method
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
classification	Metric
accuracy	Metric
measures	O
the	O
amount	O
information	O
captured	O
in	O
the	O
simulated	O
image	O
sets	O
;	O
we	O
find	O
that	O
SteinGAN	Method
achieves	O
the	O
highest	O
classification	Metric
accuracy	Metric
,	O
suggesting	O
that	O
it	O
captures	O
more	O
information	O
in	O
the	O
training	O
set	O
.	O
Figure	O
3	O
and	O
4	O
visualize	O
the	O
results	O
on	O
CelebA	O
(	O
with	O
more	O
than	O
200k	O
face	O
images	O
)	O
and	O
LSUN	Method
(	O
with	O
nearly	O
3	O
M	O
bedroom	O
images	O
)	O
,	O
respectively	O
.	O
We	O
cropped	O
and	O
resized	O
both	O
dataset	O
images	O
into	O
64	O
×	O
64	O
.	O
section	O
:	O
DCGAN	O
SteinGAN	Method
Figure	O
1	O
:	O
MNIST	O
images	O
generated	O
by	O
DCGAN	Method
and	O
our	O
SteinGAN	Method
.	O
We	O
use	O
the	O
joint	Method
model	Method
in	O
(	O
15	O
)	O
to	O
allow	O
us	O
to	O
generate	O
images	O
for	O
each	O
digit	O
.	O
We	O
set	O
m	O
=	O
0.2	O
.	O
Figure	O
2	O
:	O
Results	O
on	O
CIFAR	Material
-	Material
10	Material
.	O
"	O
500	O
Duplicate	O
"	O
denotes	O
500	O
images	O
randomly	O
subsampled	O
from	O
the	O
training	O
set	O
,	O
each	O
duplicated	O
100	O
times	O
.	O
Upper	O
:	O
images	O
simulated	O
by	O
DCGAN	Method
and	O
SteinGAN	Method
(	O
based	O
on	O
joint	Method
model	Method
(	O
15	O
)	O
)	O
conditional	O
on	O
each	O
category	O
.	O
Middle	O
:	O
inception	Metric
scores	Metric
for	O
samples	O
generated	O
by	O
various	O
methods	O
(	O
all	O
with	O
50	O
,	O
000	O
images	O
)	O
on	O
inception	Method
models	Method
trained	O
on	O
ImageNet	O
and	O
CIFAR	Material
-	Material
10	Material
,	O
respectively	O
.	O
Lower	O
:	O
testing	Metric
accuracy	Metric
on	O
real	O
testing	O
set	O
when	O
using	O
50	O
,	O
000	O
simulated	O
images	O
to	O
train	O
ResNets	Method
for	O
classification	Task
.	O
SteinGAN	Method
achieves	O
higher	O
testing	Metric
accuracy	Metric
than	O
DCGAN	Method
.	O
We	O
set	O
m	O
=	O
1	O
and	O
γ	O
=	O
0.8	O
.	O
section	O
:	O
CONCLUSION	O
We	O
propose	O
a	O
new	O
method	O
to	O
train	O
neural	Method
samplers	Method
for	O
given	O
distributions	O
,	O
together	O
with	O
a	O
new	O
SteinGAN	Method
method	O
for	O
generative	Task
adversarial	Task
training	Task
.	O
Future	O
directions	O
involve	O
more	O
applications	O
and	O
theoretical	O
understandings	O
for	O
training	O
neural	Method
samplers	Method
.	O
section	O
:	O
DCGAN	O
SteinGAN	Method
section	O
:	O
