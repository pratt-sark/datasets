document O
: O
Breaking O
the O
Softmax Method
Bottleneck Method
: O
A O
High O
- O
Rank O
RNN Method
Language O
Model O
We O
formulate O
language Method
modeling Method
as O
a O
matrix Task
factorization Task
problem Task
, O
and O
show O
that O
the O
expressiveness O
of O
Softmax Method
- Method
based Method
models Method
( O
including O
the O
majority O
of O
neural Method
language Method
models Method
) O
is O
limited O
by O
a O
Softmax Method
bottleneck Method
. O
Given O
that O
natural O
language O
is O
highly O
context O
- O
dependent O
, O
this O
further O
implies O
that O
in O
practice O
Softmax Method
with O
distributed Method
word Method
embeddings Method
does O
not O
have O
enough O
capacity O
to O
model O
natural O
language O
. O
We O
propose O
a O
simple O
and O
effective O
method O
to O
address O
this O
issue O
, O
and O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
perplexities Metric
on O
Penn Material
Treebank Material
and O
WikiText Material
- Material
2 Material
to O
47.69 O
and O
40.68 O
respectively O
. O
The O
proposed O
method O
also O
excels O
on O
the O
large O
- O
scale O
1B Material
Word O
dataset O
, O
outperforming O
the O
baseline O
by O
over O
5.6 O
points O
in O
perplexity Metric
. O
section O
: O
Introduction O
As O
a O
fundamental O
task O
in O
natural Task
language Task
processing Task
, O
statistical Task
language Task
modeling Task
has O
gone O
through O
significant O
development O
from O
traditional O
Ngram Method
language Method
models Method
to O
neural Method
language Method
models Method
in O
the O
last O
decade O
bengio2003neural O
, O
mnih2007three O
, O
mikolov2010recurrent O
. O
Despite O
the O
huge O
variety O
of O
models O
, O
as O
a O
density Task
estimation Task
problem Task
, O
language Method
modeling Method
mostly O
relies O
on O
a O
universal Method
auto Method
- Method
regressive Method
factorization Method
of O
the O
joint O
probability O
and O
then O
models O
each O
conditional Method
factor Method
using O
different O
approaches O
. O
Specifically O
, O
given O
a O
corpus O
of O
tokens O
, O
the O
joint O
probability O
factorizes O
as O
where O
is O
referred O
to O
as O
the O
context O
of O
the O
conditional O
probability O
hereafter O
. O
Based O
on O
the O
factorization Method
, O
recurrent Method
neural Method
networks Method
( O
RNN Method
) O
based O
language O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
various O
benchmarks O
merity2017regularizing O
, O
melis2017state O
, O
krause2017dynamic O
. O
A O
standard O
approach O
is O
to O
use O
a O
recurrent Method
network Method
to O
encode O
the O
context O
into O
a O
fixed O
size O
vector O
, O
which O
is O
then O
multiplied O
by O
the O
word O
embeddings O
inan2016tying O
, O
press2017using O
using O
dot Method
product Method
to O
obtain O
the O
logits O
. O
The O
logits O
are O
consumed O
by O
the O
Softmax Method
function Method
to O
give O
a O
categorical O
probability O
distribution O
over O
the O
next O
token O
. O
In O
spite O
of O
the O
expressiveness O
of O
RNNs Method
as O
universal Method
approximators Method
schafer2006recurrent O
, O
an O
unclear O
question O
is O
whether O
the O
combination O
of O
dot Method
product Method
and O
Softmax Method
is O
capable O
of O
modeling O
the O
conditional O
probability O
, O
which O
can O
vary O
dramatically O
with O
the O
change O
of O
the O
context O
. O
In O
this O
work O
, O
we O
study O
the O
expressiveness O
of O
the O
aforementioned O
Softmax Method
- Method
based Method
recurrent Method
language Method
models Method
from O
a O
perspective O
of O
matrix Method
factorization Method
. O
We O
show O
that O
learning O
a O
Softmax Method
- Method
based Method
recurrent Method
language Method
model Method
with O
the O
standard O
formulation O
is O
essentially O
equivalent O
to O
solving O
a O
matrix Task
factorization Task
problem Task
. O
More O
importantly O
, O
due O
to O
the O
fact O
that O
natural O
language O
is O
highly O
context O
- O
dependent O
, O
the O
matrix O
to O
be O
factorized O
can O
be O
high O
- O
rank O
. O
This O
further O
implies O
that O
standard O
Softmax Method
- Method
based Method
language Method
models Method
with O
distributed Method
( Method
output Method
) Method
word Method
embeddings Method
do O
not O
have O
enough O
capacity O
to O
model O
natural O
language O
. O
We O
call O
this O
the O
Softmax O
bottleneck O
. O
We O
propose O
a O
simple O
and O
effective O
method O
to O
address O
the O
Softmax Task
bottleneck Task
. O
Specifically O
, O
we O
introduce O
discrete O
latent O
variables O
into O
a O
recurrent Method
language Method
model Method
, O
and O
formulate O
the O
next O
- O
token O
probability O
distribution O
as O
a O
Mixture Method
of Method
Softmaxes Method
( O
MoS Method
) O
. O
Mixture Method
of Method
Softmaxes Method
is O
more O
expressive O
than O
Softmax Method
and O
other O
surrogates O
considered O
in O
prior O
work O
. O
Moreover O
, O
we O
show O
that O
MoS Method
learns O
matrices O
that O
have O
much O
larger O
normalized O
singular O
values O
and O
thus O
much O
higher O
rank Metric
than O
Softmax Method
and O
other O
baselines O
on O
real O
- O
world O
datasets O
. O
We O
evaluate O
our O
proposed O
approach O
on O
standard O
language Method
modeling Method
benchmarks O
. O
MoS Method
substantially O
improves O
over O
the O
current O
state O
- O
of O
- O
the O
- O
art O
results O
on O
benchmarks O
, O
by O
up O
to O
3.6 O
points O
in O
terms O
of O
perplexity Metric
, O
reaching O
perplexities Metric
47.69 O
on O
Penn Material
Treebank Material
and O
40.68 O
on O
WikiText Material
- Material
2 Material
. O
We O
further O
apply O
MoS Method
to O
a O
dialog O
dataset O
and O
show O
improved O
performance O
over O
Softmax Method
and O
other O
baselines O
. O
Our O
contribution O
is O
two O
- O
fold O
. O
First O
, O
we O
identify O
the O
Softmax Method
bottleneck Method
by O
formulating O
language Method
modeling Method
as O
a O
matrix Task
factorization Task
problem Task
. O
Second O
, O
we O
propose O
a O
simple O
and O
effective O
method O
that O
substantially O
improves O
over O
the O
current O
state O
- O
of O
- O
the O
- O
art O
results O
. O
section O
: O
Language Task
Modeling Task
as O
Matrix Task
Factorization Task
As O
discussed O
in O
Section O
[ O
reference O
] O
, O
with O
the O
autoregressive Method
factorization Method
, O
language Method
modeling Method
can O
be O
reduced O
to O
modeling O
the O
conditional O
distribution O
of O
the O
next O
token O
given O
the O
context O
. O
Though O
one O
might O
argue O
that O
a O
natural O
language O
allows O
an O
infinite O
number O
of O
contexts O
due O
to O
its O
compositionality O
pinker1994language O
, O
we O
proceed O
with O
our O
analysis O
by O
considering O
a O
finite O
set O
of O
possible O
contexts O
. O
The O
unboundedness O
of O
natural O
language O
does O
not O
affect O
our O
conclusions O
, O
which O
will O
be O
discussed O
later O
. O
We O
consider O
a O
natural O
language O
as O
a O
finite O
set O
of O
pairs O
of O
a O
context O
and O
its O
conditional Method
next Method
- Method
token Method
distribution Method
, O
where O
is O
the O
number O
of O
possible O
contexts O
. O
We O
assume O
everywhere O
to O
account O
for O
errors O
and O
flexibility O
in O
natural O
language O
. O
Let O
denote O
a O
set O
of O
possible O
tokens O
in O
the O
language O
. O
The O
objective O
of O
a O
language Method
model Method
is O
to O
learn O
a O
model O
distribution O
parameterized O
by O
to O
match O
the O
true O
data O
distribution O
. O
In O
this O
work O
, O
we O
study O
the O
expressiveness O
of O
the O
parametric Method
model Method
class Method
. O
In O
other O
words O
, O
we O
are O
asking O
the O
following O
question O
: O
given O
a O
natural O
language O
, O
does O
there O
exist O
a O
parameter O
such O
that O
for O
all O
in O
? O
We O
start O
by O
looking O
at O
a O
Softmax Method
- Method
based Method
model Method
class Method
since O
it O
is O
widely O
used O
. O
subsection O
: O
Softmax Method
The O
majority O
of O
parametric Method
language Method
models Method
use O
a O
Softmax Method
function Method
operating O
on O
a O
context O
vector O
( O
or O
hidden O
state O
) O
and O
a O
word Method
embedding Method
to O
define O
the O
conditional O
distribution O
. O
More O
specifically O
, O
the O
model Method
distribution Method
is O
usually O
written O
as O
where O
is O
a O
function O
of O
, O
and O
is O
a O
function O
of O
. O
Both O
functions O
are O
parameterized O
by O
. O
Both O
the O
context O
vector O
and O
the O
word Method
embedding Method
have O
the O
same O
dimension O
. O
The O
dot Method
product Method
is O
called O
a O
logit Method
. O
To O
help O
discuss O
the O
expressiveness O
of O
Softmax O
, O
we O
define O
three O
matrices O
: O
where O
, O
, O
, O
and O
the O
rows O
of O
, O
, O
and O
correspond O
to O
context O
vectors O
, O
word O
embeddings O
, O
and O
log O
probabilities O
of O
the O
true O
data O
distribution O
respectively O
. O
We O
use O
the O
subscript O
because O
is O
effectively O
a O
function O
indexed O
by O
the O
parameter O
, O
from O
the O
joint O
function O
family O
. O
Concretely O
, O
is O
implemented O
as O
deep Method
neural Method
networks Method
, O
such O
as O
a O
recurrent Method
network Method
, O
while O
is O
instantiated O
as O
an O
embedding Method
lookup Method
. O
We O
further O
specify O
a O
set O
of O
matrices O
formed O
by O
applying O
row Method
- Method
wise Method
shift Method
to O
where O
is O
an O
all O
- O
ones O
matrix O
with O
size O
. O
Essentially O
, O
the O
row Method
- Method
wise Method
shift Method
operation Method
adds O
an O
arbitrary O
real O
number O
to O
each O
row O
of O
. O
Thus O
, O
is O
an O
infinite O
set O
. O
Notably O
, O
the O
set O
has O
two O
important O
properties O
( O
see O
Appendix O
[ O
reference O
] O
for O
the O
proof O
) O
, O
which O
are O
key O
to O
our O
analysis O
. O
theorem O
: O
. O
For O
any O
matrix O
A′ O
, O
∈A′⁢F O
( O
A O
) O
if O
and O
only O
if O
= O
⁢Softmax O
( O
A′ O
) O
P*. O
In O
other O
words O
, O
⁢F O
( O
A O
) O
defines O
the O
set O
of O
all O
possible O
logits O
that O
correspond O
to O
the O
true O
data O
distribution O
. O
theorem O
: O
. O
For O
any O
A1≠A2∈⁢F O
( O
A O
) O
, O
≤| O
- O
⁢rank O
( O
A1 O
) O
⁢rank O
( O
A2 O
) O
|1 O
. O
In O
other O
words O
, O
all O
matrices O
in O
⁢F O
( O
A O
) O
have O
similar O
ranks O
, O
with O
the O
maximum O
rank O
difference O
being O
1 O
. O
Based O
on O
the O
Property O
[ O
reference O
] O
of O
, O
we O
immediately O
have O
the O
following O
Lemma O
. O
theorem O
: O
. O
Given O
a O
model O
parameter O
θ O
, O
∈⁢HθW⊤θ⁢F O
( O
A O
) O
if O
and O
only O
if O
Pθ O
( O
X|c O
) O
=P* O
( O
X|c O
) O
for O
all O
c O
in O
L. O
Now O
the O
expressiveness Task
question Task
becomes O
: O
does O
there O
exist O
a O
parameter O
and O
such O
that O
This O
is O
essentially O
a O
matrix Task
factorization Task
problem Task
. O
We O
want O
the O
model O
to O
learn O
matrices O
and O
that O
are O
able O
to O
factorize O
some O
matrix O
. O
First O
, O
note O
that O
for O
a O
valid O
factorization O
to O
exist O
, O
the O
rank O
of O
has O
to O
be O
at O
least O
as O
large O
as O
the O
rank O
of O
. O
Further O
, O
since O
and O
, O
the O
rank O
of O
is O
strictly O
upper O
bounded O
by O
the O
embedding Metric
size Metric
. O
As O
a O
result O
, O
if O
, O
a O
universal Method
approximator Method
can O
theoretically O
recover O
. O
However O
, O
if O
, O
no O
matter O
how O
expressive O
the O
function O
family O
is O
, O
no O
can O
even O
theoretically O
recover O
. O
We O
summarize O
the O
reasoning O
above O
as O
follows O
( O
see O
Appendix O
[ O
reference O
] O
for O
the O
proof O
) O
. O
theorem O
: O
. O
Given O
that O
the O
function Method
family Method
U Method
is O
a O
universal Method
approximator Method
, O
there O
exists O
a O
parameter O
θ O
such O
that O
Pθ O
( O
X|c O
) O
=P* O
( O
X|c O
) O
for O
all O
c O
in O
L O
if O
and O
only O
if O
≥d⁢min∈A′⁢F O
( O
A O
) O
rank O
( O
A′ O
) O
. O
Combining O
Proposition O
[ O
reference O
] O
with O
the O
Property O
[ O
reference O
] O
of O
, O
we O
are O
now O
able O
to O
state O
the O
Softmax Task
Bottleneck Task
problem Task
formally O
. O
theorem O
: O
. O
( O
Softmax Method
Bottleneck Method
) O
If O
< O
d O
- O
⁢rank O
( O
A O
) O
1 O
, O
for O
any O
function O
family O
U O
and O
any O
model O
parameter O
θ O
, O
there O
exists O
a O
context O
c O
in O
L O
such O
that O
Pθ O
( O
X|c O
) O
≠P* O
( O
X|c O
) O
. O
The O
above O
corollary O
indicates O
that O
when O
the O
dimension O
is O
too O
small O
, O
Softmax Method
does O
not O
have O
the O
capacity O
to O
express O
the O
true O
data O
distribution O
. O
Clearly O
, O
this O
conclusion O
is O
not O
restricted O
to O
a O
finite O
language O
. O
When O
is O
infinite O
, O
one O
can O
always O
take O
a O
finite O
subset O
and O
the O
Softmax O
bottleneck O
still O
exists O
. O
Next O
, O
we O
discuss O
why O
the O
Softmax Method
bottleneck Method
is O
an O
issue O
by O
presenting O
our O
hypothesis O
that O
is O
high O
- O
rank O
for O
natural O
language O
. O
subsection O
: O
Hypothesis O
: O
Natural O
Language O
is O
High O
- O
Rank O
We O
hypothesize O
that O
for O
a O
natural O
language O
, O
the O
log O
probability O
matrix O
is O
a O
high O
- O
rank O
matrix O
. O
It O
is O
difficult O
( O
if O
possible O
) O
to O
rigorously O
prove O
this O
hypothesis O
since O
we O
do O
not O
have O
access O
to O
the O
true O
data O
distribution O
of O
a O
natural O
language O
. O
However O
, O
it O
is O
suggested O
by O
the O
following O
intuitive O
reasoning O
and O
empirical O
observations O
: O
Natural O
language O
is O
highly O
context O
- O
dependent O
mikolov2012context O
. O
For O
example O
, O
the O
token O
‘ O
‘ O
north O
’ O
’ O
is O
likely O
to O
be O
followed O
by O
‘ O
‘ O
korea O
’ O
’ O
or O
‘ O
‘ O
korean O
’ O
’ O
in O
a O
news O
article O
on O
international O
politics O
, O
which O
however O
is O
unlikely O
in O
a O
textbook O
on O
U.S. O
domestic O
history O
. O
We O
hypothesize O
that O
such O
subtle O
context O
dependency O
should O
result O
in O
a O
high O
- O
rank O
matrix O
. O
If O
is O
low O
- O
rank O
, O
it O
means O
humans O
only O
need O
a O
limited O
number O
( O
e.g. O
a O
few O
hundred O
) O
of O
bases O
, O
and O
all O
semantic O
meanings O
can O
be O
created O
by O
( O
potentially O
) O
negating O
and O
( O
weighted O
) O
averaging O
these O
bases O
. O
However O
, O
it O
is O
hard O
to O
find O
a O
natural O
concept O
in O
linguistics Task
and O
cognitive Task
science Task
that O
corresponds O
to O
such O
bases O
, O
which O
questions O
the O
existence O
of O
such O
bases O
. O
For O
example O
, O
semantic O
meanings O
might O
not O
be O
those O
bases O
since O
a O
few O
hundred O
meanings O
may O
not O
be O
enough O
to O
cover O
everyday O
meanings O
, O
not O
to O
mention O
niche O
meanings O
in O
specialized O
domains O
. O
Empirically O
, O
our O
high Method
- Method
rank Method
language Method
model Method
outperforms O
conventional O
low Method
- Method
rank Method
language Method
models Method
on O
several O
benchmarks O
, O
as O
shown O
in O
Section O
[ O
reference O
] O
. O
We O
also O
provide O
evidences O
in O
Section O
[ O
reference O
] O
to O
support O
our O
hypothesis O
that O
learning O
a O
high Method
- Method
rank Method
language Method
model Method
is O
important O
. O
Given O
the O
hypothesis O
that O
natural O
language O
is O
high O
- O
rank O
, O
it O
is O
clear O
that O
the O
Softmax Method
bottleneck Method
limits O
the O
expressiveness O
of O
the O
models O
. O
In O
practice O
, O
the O
embedding O
dimension O
is O
usually O
set O
at O
the O
scale O
of O
, O
while O
the O
rank O
of O
can O
possibly O
be O
as O
high O
as O
( O
at O
the O
scale O
of O
) O
, O
which O
is O
orders O
of O
magnitude O
larger O
than O
. O
Softmax Method
is O
effectively O
learning O
a O
low Method
- Method
rank Method
approximation Method
to O
, O
and O
our O
experiments O
suggest O
that O
such O
approximation O
loses O
the O
ability O
to O
model O
context O
dependency O
, O
both O
qualitatively O
and O
quantitatively O
( O
Cf O
. O
Section O
[ O
reference O
] O
) O
. O
subsection O
: O
Easy O
Fixes O
? O
Identifying O
the O
Softmax O
bottleneck O
immediately O
suggests O
some O
possible O
‘ O
‘ O
easy O
fixes O
’ O
’ O
. O
First O
, O
as O
considered O
by O
a O
lot O
of O
prior O
work O
, O
one O
can O
employ O
a O
non Method
- Method
parametric Method
model Method
, O
namely O
an O
Ngram Method
model Method
kneser1995improved O
. O
Ngram Method
models Method
are O
not O
constrained O
by O
any O
parametric O
forms O
so O
it O
can O
universally O
approximate O
any O
natural O
language O
, O
given O
enough O
parameters O
. O
Second O
, O
it O
is O
possible O
to O
increase O
the O
dimension O
( O
e.g. O
, O
to O
match O
) O
so O
that O
the O
model O
can O
express O
a O
high O
- O
rank O
matrix O
. O
However O
, O
these O
two O
methods O
increase O
the O
number O
of O
parameters O
dramatically O
, O
compared O
to O
using O
a O
low Method
- Method
dimensional Method
Softmax Method
. O
More O
specifically O
, O
an O
Ngram O
needs O
parameters O
in O
order O
to O
express O
, O
where O
is O
potentially O
unbounded O
. O
Similarly O
, O
a O
high O
- O
dimensional O
Softmax O
requires O
parameters O
for O
the O
word O
embeddings O
. O
Increasing O
the O
number O
of O
model O
parameters O
easily O
leads O
to O
overfitting O
. O
In O
past O
work O
, O
used O
back O
- O
off O
to O
alleviate O
overfitting O
. O
Moreover O
, O
as O
deep Method
learning Method
models Method
were O
tuned O
by O
extensive O
hyper Method
- Method
parameter Method
search Method
, O
increasing O
the O
dimension O
beyond O
several O
hundred O
is O
not O
helpful O
merity2017regularizing O
, O
melis2017state O
, O
krause2017dynamic O
. O
Clearly O
there O
is O
a O
tradeoff O
between O
expressiveness O
and O
generalization Task
on O
language Method
modeling Method
. O
Naively O
increasing O
the O
expressiveness O
hurts O
generalization Task
. O
Below O
, O
we O
introduce O
an O
alternative O
approach O
that O
increases O
the O
expressiveness O
without O
exploding O
the O
parametric O
space O
. O
subsection O
: O
Mixture Method
of Method
Softmaxes Method
: O
A O
High Method
- Method
Rank Method
Language Method
Model Method
We O
propose O
a O
high Method
- Method
rank Method
language Method
model Method
called O
Mixture Method
of Method
Softmaxes Method
( O
MoS Method
) O
to O
alleviate O
the O
Softmax Task
bottleneck Task
issue Task
. O
MoS Method
formulates O
the O
conditional O
distribution O
as O
where O
is O
the O
prior O
or O
mixture O
weight O
of O
the O
- O
th O
component O
, O
and O
is O
the O
- O
th O
context O
vector O
associated O
with O
context O
. O
In O
other O
words O
, O
MoS Method
computes O
Softmax Method
distributions Method
and O
uses O
a O
weighted Method
average Method
of O
them O
as O
the O
next O
- O
token O
probability O
distribution O
. O
Similar O
to O
prior O
work O
on O
recurrent O
language Method
modeling Method
merity2017regularizing O
, O
melis2017state O
, O
krause2017dynamic O
, O
we O
first O
apply O
a O
stack Method
of Method
recurrent Method
layers Method
on O
top O
of O
to O
obtain O
a O
sequence O
of O
hidden O
states O
. O
The O
prior O
and O
the O
context O
vector O
for O
context O
are O
parameterized O
as O
and O
where O
and O
are O
model O
parameters O
. O
Our O
method O
is O
simple O
and O
easy O
to O
implement O
, O
and O
has O
the O
following O
advantages O
: O
Improved O
expressiveness Metric
( O
compared O
to O
Softmax Method
) O
. O
MoS Method
is O
theoretically O
more O
( O
or O
at O
least O
equally O
) O
expressive O
compared O
to O
Softmax Method
given O
the O
same O
dimension O
. O
This O
can O
be O
seen O
by O
the O
fact O
that O
MoS Method
with O
is O
reduced O
to O
Softmax O
. O
More O
importantly O
, O
MoS Method
effectively O
approximates O
by O
where O
is O
an O
diagonal O
matrix O
with O
elements O
being O
the O
prior O
. O
Because O
is O
a O
nonlinear O
function O
( O
log_sum_exp O
) O
of O
the O
context O
vectors O
and O
the O
word O
embeddings O
, O
can O
be O
arbitrarily O
high O
- O
rank O
. O
As O
a O
result O
, O
MoS Method
does O
not O
suffer O
from O
the O
rank O
limitation O
, O
compared O
to O
Softmax Method
. O
Improved O
generalization Task
( O
compared O
to O
Ngram Method
) O
. O
Ngram Method
models Method
and O
high O
- O
dimensional O
Softmax O
( O
Cf O
. O
Section O
[ O
reference O
] O
) O
improve O
the O
expressiveness O
but O
do O
not O
generalize O
well O
. O
In O
contrast O
, O
MoS Method
does O
not O
have O
a O
generalization Task
issue Task
due O
to O
the O
following O
reasons O
. O
First O
, O
MoS Method
defines O
the O
following O
generative Method
process Method
: O
a O
discrete O
latent O
variable O
is O
first O
sampled O
from O
, O
and O
then O
the O
next O
token O
is O
sampled O
based O
on O
the O
- Method
th Method
Softmax Method
component Method
. O
By O
doing O
so O
we O
introduce O
an O
inductive O
bias O
that O
the O
next O
token O
is O
generated O
based O
on O
a O
latent O
discrete O
decision O
( O
e.g. O
, O
a O
topic O
) O
, O
which O
is O
often O
safe O
in O
language Method
modeling Method
blei2003latent O
. O
Second O
, O
since O
is O
defined O
by O
a O
nonlinear O
function O
and O
not O
restricted O
by O
the O
rank Method
bottleneck Method
, O
in O
practice O
it O
is O
possible O
to O
reduce O
to O
compensate O
for O
the O
increase O
of O
model O
parameters O
brought O
by O
the O
mixture O
structure O
. O
As O
a O
result O
, O
MoS Method
has O
a O
similar O
model Metric
size Metric
compared O
to O
Softmax Method
and O
thus O
is O
not O
prone O
to O
overfitting O
. O
subsection O
: O
Mixture O
of O
Contexts O
: O
A O
Low Method
- Method
Rank Method
Baseline Method
Another O
possible O
approach O
is O
to O
directly O
mix O
the O
context O
vectors O
( O
or O
logits O
) O
before O
taking O
the O
Softmax O
, O
rather O
than O
mixing O
the O
probabilities O
afterwards O
as O
in O
MoS. Method
Specifically O
, O
the O
conditional O
distribution O
is O
parameterized O
as O
where O
and O
share O
the O
same O
parameterization O
as O
in O
MoS. Method
Despite O
its O
superficial O
similarity O
to O
MoS Method
, O
this O
model O
, O
which O
we O
refer O
to O
as O
mixture Method
of Method
contexts Method
( O
MoC Method
) O
, O
actually O
suffers O
from O
the O
same O
rank Task
limitation Task
problem Task
as O
Softmax Method
. O
This O
can O
be O
easily O
seen O
by O
defining O
, O
which O
turns O
the O
MoC Method
parameterization O
( O
[ O
reference O
] O
) O
into O
. O
Note O
that O
this O
is O
equivalent O
to O
the O
Softmax Method
parameterization Method
( O
[ O
reference O
] O
) O
. O
Thus O
, O
performing O
mixture O
in O
the O
feature O
space O
can O
only O
make O
the O
function O
family O
more O
expressive O
, O
but O
does O
not O
change O
the O
fact O
that O
the O
rank O
of O
is O
upper O
bounded O
by O
the O
embedding O
dimension O
. O
In O
our O
experiments O
, O
we O
implement O
MoC Method
as O
a O
baseline O
and O
compare O
it O
experimentally O
to O
MoS. Method
section O
: O
Experiments O
subsection O
: O
Main O
Results O
We O
conduct O
a O
series O
of O
experiments O
with O
the O
following O
settings O
: O
Following O
previous O
work O
krause2017dynamic O
, O
merity2017regularizing O
, O
melis2017state O
, O
we O
evaluate O
the O
proposed O
MoS Method
model O
on O
two O
widely O
used O
language Method
modeling Method
datasets O
, O
namely O
Penn Material
Treebank Material
( O
PTB Material
) O
mikolov2010recurrent O
and O
WikiText Material
- Material
2 Material
( O
WT2 Material
) O
merity2016pointer O
based O
on O
perplexity Metric
. O
For O
fair O
comparison O
, O
we O
closely O
follow O
the O
regularization Method
and Method
optimization Method
techniques Method
introduced O
by O
merity2017regularizing O
. O
We O
heuristically O
and O
manually O
search O
hyper O
- O
parameters O
for O
MoS Method
based O
on O
the O
validation Metric
performance Metric
while O
limiting O
the O
model Metric
size Metric
( O
see O
Appendix O
[ O
reference O
] O
for O
our O
hyper O
- O
parameters O
) O
. O
To O
investigate O
whether O
the O
effectiveness O
of O
MoS Method
can O
be O
extended O
to O
even O
larger O
datasets O
, O
we O
conduct O
an O
additional O
language Method
modeling Method
experiment O
on O
the O
1B Material
Word Material
dataset Material
chelba2013one O
. O
Specifically O
, O
we O
lower O
- O
case O
the O
text O
and O
choose O
the O
top O
100 O
K O
tokens O
as O
the O
vocabulary O
. O
A O
standard O
neural Method
language Method
model Method
with O
2 O
layers O
of O
LSTMs Method
followed O
by O
a O
Softmax O
output O
layer O
is O
used O
as O
the O
baseline O
. O
Again O
, O
the O
network Metric
size Metric
of O
MoS Method
is O
adjusted O
to O
ensure O
a O
comparable O
number O
of O
parameters O
. O
Notably O
, O
dropout Method
was O
not O
used O
, O
since O
we O
found O
it O
not O
helpful O
to O
either O
model O
( O
see O
Appendix O
[ O
reference O
] O
for O
more O
details O
) O
. O
To O
show O
that O
the O
MoS Method
is O
a O
generic O
structure O
that O
can O
be O
used O
to O
model O
other O
context O
- O
dependent O
distributions O
, O
we O
additionally O
conduct O
experiments O
in O
the O
dialog O
domain O
. O
We O
use O
the O
Switchboard Material
dataset Material
godfrey1997switchboard O
preprocessed O
by O
zhao2017learning O
to O
train O
a O
Seq2Seq Method
sutskever2014sequence O
model O
with O
MoS Method
added O
to O
the O
decoder O
RNN Method
. O
Then O
, O
a O
Seq2Seq Method
model O
using O
Softmax Method
and O
another O
one O
augmented O
by O
MoC Method
with O
comparable O
parameter O
sizes O
are O
used O
as O
baselines O
. O
For O
evaluation O
, O
we O
include O
both O
the O
perplexity Metric
and O
the O
precision Metric
/ Metric
recall Metric
of O
Smoothed Metric
Sentence Metric
- Metric
level Metric
BLEU Metric
, O
as O
suggested O
by O
zhao2017learning O
. O
When O
generating O
responses O
, O
we O
use O
beam Method
search Method
with O
beam O
size O
10 O
, O
restrict O
the O
maximum O
length O
to O
30 O
, O
and O
retain O
the O
top O
- O
5 O
responses O
. O
The O
language Method
modeling Method
results O
on O
PTB Material
and O
WT2 Material
are O
presented O
in O
Table O
[ O
reference O
] O
and O
Table O
[ O
reference O
] O
respectively O
. O
With O
a O
comparable O
number O
of O
parameters O
, O
MoS Method
outperforms O
all O
baselines O
with O
or O
without O
dynamic Metric
evaluation Metric
, O
and O
substantially O
improves O
over O
the O
current O
state O
of O
the O
art O
, O
by O
up O
to O
3.6 O
points O
in O
perplexity Metric
. O
The O
improvement O
on O
the O
large O
- O
scale O
dataset O
is O
even O
more O
significant O
. O
As O
shown O
in O
Table O
[ O
reference O
] O
, O
MoS Method
outperforms O
Softmax Method
by O
over O
5.6 O
points O
in O
perplexity Metric
. O
It O
suggests O
the O
effectiveness O
of O
MoS Method
is O
not O
limited O
to O
small O
datasets O
where O
many O
regularization Method
techniques Method
are O
used O
. O
Note O
that O
with O
limited O
computational O
resources O
, O
we O
did O
n’t O
tune O
the O
hyper O
- O
parameters O
for O
MoS. Method
Further O
, O
the O
experimental O
results O
on O
Switchboard O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
Clearly O
, O
on O
all O
metrics O
, O
MoS Method
outperforms O
MoC Method
and O
Softmax Method
, O
showing O
its O
general O
effectiveness O
. O
subsection O
: O
Ablation Task
Study Task
To O
further O
verify O
the O
improvement O
shown O
above O
does O
come O
from O
the O
MoS Method
structure O
rather O
than O
adding O
another O
hidden O
layer O
or O
finding O
a O
particular O
set O
of O
hyper O
- O
parameters O
, O
we O
conduct O
an O
ablation O
study O
on O
both O
PTB Material
and O
WT2 Material
. O
Firstly O
, O
we O
compare O
MoS Method
with O
an O
MoC Method
architecture O
with O
the O
same O
number O
of O
layers O
, O
hidden O
sizes O
, O
and O
embedding O
sizes O
, O
which O
thus O
has O
the O
same O
number O
of O
parameters O
. O
In O
addition O
, O
we O
adopt O
the O
hyper O
- O
parameters O
used O
to O
obtain O
the O
best O
MoS Method
model O
( O
denoted O
as O
MoS Method
hyper O
- O
parameters O
) O
, O
and O
train O
a O
baseline O
AWD Method
- Method
LSTM Method
. O
To O
avoid O
distractive O
factors O
and O
save O
computational O
resources O
, O
all O
ablative O
experiments O
excluded O
the O
use O
of O
finetuing O
and O
dynamic Task
evaluation Task
. O
The O
results O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
Compared O
to O
the O
vanilla Method
AWD Method
- Method
LSTM Method
, O
though O
being O
more O
expressive O
, O
MoC Method
performs O
only O
better O
on O
PTB Material
, O
but O
worse O
on O
WT2 Material
. O
It O
suggests O
that O
simply O
adding O
another O
hidden O
layer O
or O
employing O
a O
mixture O
structure O
in O
the O
feature O
space O
does O
not O
guarantee O
a O
better O
performance O
. O
On O
the O
other O
hand O
, O
training O
AWD Method
- Method
LSTM Method
using O
MoS Method
hyper O
- O
parameters O
severely O
hurts O
the O
performance O
, O
which O
rules O
out O
hyper O
- O
parameters O
as O
the O
main O
source O
of O
improvement O
. O
subsection O
: O
Verify O
the O
Role O
of O
Rank O
While O
the O
study O
above O
verifies O
that O
MoS Method
is O
the O
key O
to O
achieving O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
, O
it O
is O
still O
not O
clear O
whether O
the O
superiority O
of O
MoS Method
comes O
from O
its O
potential O
high O
rank O
, O
as O
suggested O
by O
our O
theoretical O
analysis O
in O
Section O
[ O
reference O
] O
. O
In O
the O
sequel O
, O
we O
take O
steps O
to O
verify O
this O
hypothesis O
. O
Firstly O
, O
we O
verify O
that O
MoS Method
does O
induce O
a O
high O
- O
rank O
log O
- O
probability O
matrix O
empirically O
, O
while O
MoC Method
and O
Softmax Method
fail O
. O
On O
the O
validation O
or O
test O
set O
of O
PTB Material
with Material
tokens Material
, O
we O
compute O
the O
log O
probabilities O
for O
each O
token O
using O
all O
three O
models O
. O
Then O
, O
for O
each O
model O
, O
we O
stack O
all O
log O
- O
probability O
vectors O
into O
a O
matrix O
, O
resulting O
in O
, O
and O
. O
Theoretically O
, O
the O
number O
of O
non O
- O
zero O
singular O
values O
of O
a O
matrix O
is O
equal O
to O
its O
rank O
. O
However O
, O
performing O
singular Method
value Method
decomposition Method
of Method
real Method
valued Method
matrices Method
using O
numerical Method
approaches Method
often O
encounter O
roundoff O
errors O
. O
Hence O
, O
we O
adopt O
the O
expected O
roundoff Metric
error Metric
suggested O
by O
press2007numerical O
when O
estimating O
the O
ranks O
of O
, O
and O
. O
The O
estimated O
ranks O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
As O
predicted O
by O
our O
theoretical O
analysis O
, O
the O
matrix O
ranks O
induced O
by O
Softmax Method
and O
MoC Method
are O
both O
limited O
by O
the O
corresponding O
embedding O
sizes O
. O
By O
contrast O
, O
the O
matrix O
rank O
obtained O
from O
MoS Method
does O
not O
suffer O
from O
this O
constraint O
, O
almost O
reaching O
full O
rank O
( O
) O
. O
In O
appendix O
[ O
reference O
] O
, O
we O
give O
additional O
evidences O
for O
the O
higher O
rank O
of O
MoS. Method
Secondly O
, O
we O
show O
that O
, O
before O
reaching O
full O
rank O
, O
increasing O
the O
number O
of O
mixture O
components O
in O
MoS Method
also O
increases O
the O
rank O
of O
the O
log O
- O
probability O
matrix O
, O
which O
in O
turn O
leads O
to O
improved O
performance O
( O
lower O
perplexity Metric
) O
. O
Specifically O
, O
on O
PTB Material
, O
with O
other O
hyper O
- O
parameters O
fixed O
as O
used O
in O
section O
[ O
reference O
] O
, O
we O
vary O
the O
number O
of O
mixtures O
used O
in O
MoS Method
and O
compare O
the O
corresponding O
empirical Metric
rank Metric
and O
test O
perplexity Metric
without O
finetuning Method
. O
Table O
[ O
reference O
] O
summarizes O
the O
results O
. O
This O
clear O
positive O
correlation O
between O
rank Metric
and O
performance O
strongly O
supports O
the O
our O
theoretical O
analysis O
in O
section O
[ O
reference O
] O
. O
Moreover O
, O
note O
that O
after O
reaching O
almost O
full O
rank O
( O
i.e. O
, O
using O
15 O
mixture O
components O
) O
, O
further O
increasing O
the O
number O
of O
components O
degrades O
the O
performance O
due O
to O
overfitting O
( O
as O
we O
inspected O
the O
training O
and O
test O
perplexities Metric
) O
. O
In O
addition O
, O
as O
performance O
improvement O
can O
often O
come O
from O
better O
regularization Method
, O
we O
investigate O
whether O
MoS Method
has O
a O
better O
, O
though O
unexpected O
, O
regularization Metric
effect Metric
compared O
to O
Softmax Method
. O
We O
consider O
the O
1B Material
word O
dataset O
where O
overfitting O
is O
unlikely O
and O
no O
explicit O
regularization Method
technique Method
( O
e.g. O
, O
dropout Method
) O
is O
employed O
. O
As O
we O
can O
see O
from O
the O
left O
part O
of O
Table O
[ O
reference O
] O
, O
MoS Method
and O
Softmax Method
achieve O
a O
similar O
generalization Metric
gap Metric
, O
i.e. O
, O
the O
performance O
gap O
between O
the O
test O
set O
and O
the O
training O
set O
. O
It O
suggests O
both O
models O
have O
similar O
regularization O
effects O
. O
Meanwhile O
, O
MoS Method
has O
a O
lower O
training O
perplexity Metric
compared O
to O
Softmax O
, O
indicating O
that O
the O
improvement O
of O
MoS Method
results O
from O
improved O
expressiveness O
. O
The O
last O
evidence O
we O
provide O
is O
based O
on O
an O
inverse O
experiment O
. O
Empirically O
, O
we O
find O
that O
when O
Softmax Method
does O
not O
suffer O
from O
a O
rank O
limitation O
, O
e.g. O
, O
in O
character O
- O
level O
language Method
modeling Method
, O
using O
MoS Method
will O
not O
improve O
the O
performance O
. O
Due O
to O
lack O
of O
space O
, O
we O
refer O
readers O
to O
Appendix O
[ O
reference O
] O
for O
details O
. O
subsection O
: O
Additional O
analysis O
paragraph O
: O
MoS Method
computational O
time O
The O
expressiveness O
of O
MoS Method
does O
come O
with O
a O
computational Metric
cost Metric
— O
computing O
a O
- O
times O
larger O
Softmax O
. O
To O
give O
readers O
a O
concrete O
idea O
of O
the O
influence O
on O
training Metric
time Metric
, O
we O
perform O
detailed O
analysis O
in O
Appendix O
[ O
reference O
] O
. O
As O
we O
will O
see O
, O
computational Metric
wall Metric
time Metric
of O
MoS Method
is O
actually O
sub O
- O
linear O
w.r.t O
. O
the O
number O
of O
Softmaxes O
. O
In O
most O
settings O
, O
we O
observe O
a O
two O
to O
three O
times O
slowdown O
when O
using O
MoS Method
with O
up O
to O
15 O
mixture Method
components Method
. O
paragraph O
: O
Qualitative Method
analysis Method
Finally O
, O
we O
conduct O
a O
case O
study O
on O
PTB Material
to O
see O
how O
MoS Method
improves O
the O
next Task
- Task
token Task
prediction Task
in O
detail O
. O
Due O
to O
lack O
of O
space O
, O
we O
refer O
readers O
to O
Appendix O
[ O
reference O
] O
for O
details O
. O
The O
key O
insight O
from O
the O
case O
study O
is O
that O
MoS Method
is O
better O
at O
making O
context Task
- Task
dependent Task
predictions Task
. O
Specifically O
, O
given O
the O
same O
immediate O
preceding O
word O
, O
MoS Method
will O
produce O
distinct O
next Task
- Task
step Task
prediction Task
based O
on O
long O
- O
term O
context O
in O
history O
. O
By O
contrast O
, O
the O
baseline O
often O
yields O
similar O
next Task
- Task
step Task
prediction Task
, O
independent O
of O
the O
long O
- O
term O
context O
. O
section O
: O
Related O
work O
In O
language Method
modeling Method
, O
hutchinson2011low O
, O
hutchinson2012sparse O
have O
previously O
considered O
the O
problem O
from O
a O
matrix O
rank O
perspective O
. O
However O
, O
their O
focus O
was O
to O
improve O
the O
generalization Method
of Method
Ngram Method
language Method
models Method
via O
a O
sparse Method
plus Method
low Method
- Method
rank Method
approximation Method
. O
By O
contrast O
, O
as O
neural Method
language Method
models Method
already O
generalize O
well O
, O
we O
focus O
on O
a O
high Method
- Method
rank Method
neural Method
language Method
model Method
that O
improves O
expressiveness O
without O
sacrificing O
generalization Task
. O
neubig2016generalizing O
proposed O
to O
mix O
Ngram Method
and Method
neural Method
language Method
models Method
to O
unify O
and O
benefit O
from O
both O
. O
However O
, O
this O
mixture O
might O
not O
generalize O
well O
since O
an O
Ngram Method
model Method
, O
which O
has O
poor O
generalization Task
, O
is O
included O
. O
Moreover O
, O
the O
fact O
that O
the O
two O
components O
are O
separately O
trained O
can O
limit O
its O
expressiveness O
. O
levy2014neural O
also O
considered O
the O
matrix Method
factorization Method
perspective Method
, O
but O
in O
the O
context O
of O
learning Task
word Task
embeddings Task
. O
In O
a O
general O
sense O
, O
Mixture Method
of Method
Softmaxes Method
proposed O
in O
this O
work O
can O
be O
seen O
as O
a O
particular O
instantiation O
of O
the O
long O
- O
existing O
idea O
called O
Mixture Method
of Method
Experts Method
( O
MoE Method
) O
jacobs1991adaptive O
. O
However O
, O
there O
are O
two O
core O
differences O
. O
Firstly O
, O
MoE Method
has O
usually O
been O
instantiated O
as O
mixture Method
of Method
Gaussians Method
to O
model O
data O
in O
continuous O
domains O
jacobs1991adaptive O
, O
graves2013generating O
, O
bazzani2016recurrent O
. O
More O
importantly O
, O
the O
motivation O
of O
using O
the O
mixture Method
structure Method
is O
distinct O
. O
For O
Gaussian Method
mixture Method
models Method
, O
the O
mixture O
structure O
is O
employed O
to O
allow O
for O
a O
parameterized O
multi O
- O
modal O
distribution O
. O
By O
contrast O
, O
Softmax Method
by O
itself O
can O
parameterize O
a O
multi O
- O
modal O
distribution O
, O
and O
MoS Method
is O
introduced O
to O
break O
the O
Softmax O
bottleneck O
as O
discussed O
in O
Section O
[ O
reference O
] O
. O
There O
has O
been O
previous O
work O
eigen2013learning O
, O
shazeer2017outrageously O
proposing O
architectures O
that O
can O
be O
categorized O
as O
instantiations O
of O
MoC Method
, O
since O
the O
mixture O
structure O
is O
employed O
in O
the O
feature O
space O
. O
The O
target O
of O
eigen2013learning O
is O
to O
create O
a O
more O
expressive O
feed Method
- Method
forward Method
layer Method
through O
the O
mixture O
structure O
. O
In O
comparison O
, O
shazeer2017outrageously O
focuses O
on O
a O
sparse Method
gating Method
mechanism Method
also O
on O
the O
feature O
level O
, O
which O
enables O
efficient O
conditional Task
computation Task
and O
allows O
the O
training O
of O
a O
very O
large O
neural Method
architecture Method
. O
In O
addition O
to O
having O
different O
motivations O
from O
our O
work O
, O
all O
these O
MoC Method
variants O
suffer O
from O
the O
same O
rank Task
limitation Task
problem Task
as O
discussed O
in O
Section O
[ O
reference O
] O
. O
Finally O
, O
several O
previous O
works O
have O
tried O
to O
introduce O
latent O
variables O
into O
sequence Method
modeling Method
bayer2014learning O
, O
gregor2015draw O
, O
chung2015recurrent O
, O
gan2015deep O
, O
fraccaro2016sequential O
, O
chung2016hierarchical O
. O
Except O
for O
chung2016hierarchical O
, O
these O
structures O
all O
define O
a O
continuous O
latent O
variable O
for O
each O
step O
of O
the O
RNN Method
computation O
, O
and O
rely O
on O
the O
SGVB Method
estimator Method
kingma2013auto O
to O
optimize O
a O
variational Metric
lower Metric
bound Metric
of O
the O
log O
- O
likelihood O
. O
Since O
exact Task
integration Task
is O
infeasible O
, O
these O
models O
can O
not O
estimate O
the O
likelihood O
( O
perplexity Metric
) O
exactly O
at O
test O
time O
. O
Moreover O
, O
for O
discrete O
data O
, O
the O
variational Method
lower Method
bound Method
is O
usually O
too O
loose O
to O
yield O
a O
competitive Method
approximation Method
compared O
to O
standard O
auto Method
- Method
regressive Method
models Method
. O
As O
an O
exception O
, O
chung2016hierarchical O
utilizes O
Bernoulli O
latent O
variables O
to O
model O
the O
hierarchical O
structure O
in O
language O
, O
where O
the O
Bernoulli Method
sampling Method
is O
replaced O
by O
a O
thresholding Method
operation Method
at O
test O
time O
to O
give O
perplexity Metric
estimation Metric
. O
section O
: O
Conclusions O
Under O
the O
matrix Method
factorization Method
framework Method
, O
the O
expressiveness O
of O
Softmax Method
- Method
based Method
language Method
models Method
is O
limited O
by O
the O
dimension O
of O
the O
word O
embeddings O
, O
which O
is O
termed O
as O
the O
Softmax O
bottleneck O
. O
Our O
proposed O
MoS Method
model O
improves O
the O
expressiveness Metric
over O
Softmax Method
, O
and O
at O
the O
same O
time O
avoids O
overfitting O
compared O
to O
non Method
- Method
parametric Method
models Method
and O
naively O
increasing O
the O
word O
embedding O
dimensions O
. O
Our O
method O
improves O
the O
current O
state O
- O
of O
- O
the O
- O
art O
results O
on O
standard O
benchmarks O
by O
a O
large O
margin O
, O
which O
in O
turn O
justifies O
our O
theoretical O
reasoning O
: O
it O
is O
important O
to O
have O
a O
high O
- O
rank Method
model Method
for O
natural O
language O
. O
subsubsection O
: O
Acknowledgments O
This O
work O
was O
supported O
by O
the O
DARPA O
award O
D17AP00001 O
, O
the O
Google O
focused O
award O
, O
and O
the O
Nvidia O
NVAIL O
award O
. O
bibliography O
: O
References O
appendix O
: O
Proofs O
Proof O
of O
Property O
proof O
: O
Proof O
. O
For O
any O
, O
let O
denote O
the O
distribution O
defined O
by O
applying O
Softmax Method
on O
the O
logits O
given O
by O
. O
Consider O
row O
column O
, O
by O
definition O
any O
entry O
in O
can O
be O
expressed O
as O
. O
It O
follows O
For O
any O
, O
for O
any O
and O
, O
we O
have O
It O
follows O
that O
for O
any O
, O
, O
and O
, O
As O
a O
result O
, O
This O
means O
each O
row O
in O
can O
be O
obtained O
by O
adding O
a O
real O
number O
to O
the O
corresponding O
row O
in O
. O
Therefore O
, O
there O
exists O
a O
diagonal O
matrix O
such O
that O
It O
follows O
that O
. O
∎ O
Proof O
of O
Property O
proof O
: O
Proof O
. O
For O
any O
and O
in O
, O
by O
definition O
we O
have O
, O
and O
where O
and O
are O
two O
diagonal O
matrices O
. O
It O
can O
be O
rewritten O
as O
Let O
be O
a O
maximum O
set O
of O
linearly O
independent O
rows O
in O
. O
Let O
be O
an O
all O
- O
ones O
vector O
with O
dimension O
. O
The O
- O
th O
row O
vector O
in O
can O
be O
written O
as O
Because O
is O
a O
linear O
combination O
of O
vectors O
in O
, O
is O
a O
linear O
combination O
of O
vectors O
in O
. O
It O
follows O
that O
Similarly O
, O
we O
can O
derive O
Therefore O
, O
∎ O
Proof O
of O
Proposition O
proof O
: O
Proof O
. O
If O
there O
exists O
a O
parameter O
such O
that O
for O
all O
in O
, O
by O
Lemma O
[ O
reference O
] O
, O
we O
have O
. O
As O
a O
result O
, O
there O
exists O
a O
matrix O
such O
that O
. O
Because O
and O
are O
of O
dimensions O
and O
respectively O
, O
we O
have O
If O
, O
there O
exist O
matrices O
, O
and O
, O
such O
that O
can O
be O
factorized O
as O
. O
Because O
is O
a O
universal Method
approximator Method
, O
there O
exists O
such O
that O
and O
. O
By O
Lemma O
[ O
reference O
] O
, O
for O
all O
in O
. O
∎ O
appendix O
: O
Experiment O
setting O
and O
Hyper O
- O
parameters O
subsection O
: O
PTB Material
and O
WT2 Material
The O
hyper O
- O
parameters O
used O
for O
MoS Method
in O
language Method
modeling Method
experiment O
is O
summarized O
below O
. O
The O
hyper O
- O
parameters O
used O
for O
dynamic O
evaluation O
of O
MoS Method
is O
summarized O
below O
. O
subsection O
: O
1B Material
Word Material
Dataset Material
For O
training O
, O
we O
use O
all O
of O
the O
100 O
training O
shards O
. O
For O
validation Task
, O
we O
use O
two O
shards Method
from O
the O
heldout Material
set Material
, O
namely O
[ O
heldout Material
- Material
00 Material
, O
heldout Material
- Material
10 Material
] O
. O
For O
test O
, O
we O
use O
another O
three O
shards O
from O
the O
heldout Material
set Material
, O
namely O
[ O
heldout Material
- Material
20 Material
, O
heldout Material
- Material
30 Material
, O
heldout Material
- Material
40 Material
] O
. O
The O
hyper O
- O
parameters O
are O
listed O
below O
. O
appendix O
: O
Additional O
experiments O
subsection O
: O
Higher O
empirical Metric
rank Metric
of O
MoS Method
compared O
to O
MoC Method
and O
Softmax Method
Cumulative O
percentage O
of O
normalized O
singulars O
given O
a O
value O
in O
[ O
0 O
, O
1 O
] O
. O
In O
section O
[ O
reference O
] O
, O
we O
compute O
the O
rank O
of O
different O
models O
based O
on O
the O
non O
- O
zero O
singular O
values O
of O
the O
empirical O
log O
- O
likelihood O
matrix O
. O
Since O
there O
can O
be O
roundoff O
mistakes O
, O
a O
less O
error O
- O
prone O
approach O
is O
to O
directly O
study O
the O
distribution O
of O
singular O
values O
. O
Specifically O
, O
if O
more O
singular O
values O
have O
relatively O
larger O
magnitude O
, O
the O
rank O
of O
the O
matrix O
tends O
to O
be O
higher O
. O
Motivated O
from O
this O
intuition O
, O
we O
visualize O
the O
distribution O
of O
the O
singular O
values O
. O
To O
account O
for O
the O
different O
magnitudes O
of O
singular O
values O
from O
different O
models O
, O
we O
first O
normalize O
all O
singular O
values O
to O
. O
Then O
, O
we O
plot O
the O
cumulative O
percentage O
of O
normalized O
singular O
values O
, O
i.e. O
, O
percentage O
of O
normalized O
singular O
values O
below O
a O
threshold O
, O
in O
Figure O
[ O
reference O
] O
. O
As O
we O
can O
see O
, O
most O
of O
the O
singular O
values O
of O
Softmax Method
and O
MoC Method
concentrate O
on O
an O
area O
with O
very O
low O
values O
. O
In O
comparison O
, O
the O
concentration O
area O
of O
the O
MoS Method
singular O
values O
is O
not O
only O
several O
orders O
larger O
, O
but O
also O
spans O
a O
much O
wider O
region O
. O
Intuitively O
, O
MoS Method
utilizes O
the O
corresponding O
singular O
vectors O
to O
capture O
a O
larger O
and O
more O
diverse O
set O
of O
contexts O
. O
What O
’s O
more O
, O
another O
indicator O
of O
high Metric
rank Metric
is O
that O
the O
model O
can O
precisely O
capture O
the O
nuance O
of O
difference O
contexts O
. O
If O
a O
model O
can O
better O
capture O
the O
distinctions O
among O
contexts O
, O
we O
expect O
the O
next O
- O
step O
conditional O
distributions O
to O
be O
less O
similar O
to O
each O
on O
average O
. O
Based O
on O
this O
intuition O
, O
we O
use O
the O
expected Metric
pairwise Metric
Kullback Metric
– Metric
Leibler Metric
divergence Metric
( O
KLD Method
) O
, O
i.e. O
, O
where O
denotes O
all O
possible O
contexts O
, O
as O
another O
metric O
to O
evaluate O
the O
ranks O
of O
the O
three O
models O
( O
MoS Method
, O
MoC Method
and O
Softmax Method
) O
. O
Practically O
, O
we O
sample O
from O
validation O
or O
test O
data O
of O
PTB Material
to O
get O
the O
empirical Method
estimations Method
for O
the O
three O
models O
, O
which O
are O
shown O
in O
the O
right O
half O
of O
Table O
[ O
reference O
] O
. O
As O
we O
expected O
, O
MoS Method
achieves O
higher O
expected O
pairwise Metric
KLD Metric
, O
indicating O
its O
superiority O
in O
covering O
more O
contexts O
of O
the O
next O
- O
token O
distribution O
. O
subsection O
: O
An O
inverse O
experiment O
on O
character O
- O
level O
language Method
modeling Method
Here O
, O
we O
detail O
the O
inverse O
experiment O
, O
which O
shows O
that O
when O
Softmax Method
does O
not O
suffer O
from O
a O
rank O
limitation O
, O
using O
MoS Method
will O
not O
improve O
the O
performance O
. O
Notice O
that O
character O
- O
level O
language Method
modeling Method
( O
CharLM Method
) O
is O
exactly O
such O
a O
problem O
, O
because O
the O
rank O
of O
the O
log O
- O
likelihood O
matrix O
is O
upper O
bounded O
by O
the O
vocabulary O
size O
, O
and O
CharLM Method
usually O
has O
a O
very O
limited O
vocabulary O
( O
tens O
of O
characters O
) O
. O
In O
this O
case O
, O
with O
the O
embedding Metric
size Metric
being O
hundreds O
in O
practice O
, O
Softmax Method
is O
no O
longer O
a O
bottleneck O
in O
this O
task O
. O
Hence O
, O
we O
expect O
MoS Method
to O
yield O
similar O
performance O
to O
Softmax Method
on O
CharLM Method
. O
We O
conduct O
experiments O
of O
CharLM Method
using O
the O
text8 Material
dataset Material
mahoney2011large O
, O
which O
consists O
of O
100 O
M O
characters O
including O
only O
alphabetical O
characters O
and O
spaces O
derived O
from O
Wikipedia Material
. O
We O
follow O
mikolov2012subword O
and O
use O
the O
first O
90 O
M O
characters O
for O
training O
, O
the O
next O
5 O
M O
for O
validation Task
and O
the O
final O
5 O
M O
for O
testing O
. O
The O
standard O
evaluation O
metric O
bit Metric
- Metric
per Metric
- Metric
character Metric
( O
BPC Metric
) O
is O
employed O
. O
We O
employ O
a O
1 Method
- Method
layer Method
1024 Method
- Method
unit Method
LSTM Method
followed O
by O
Softmax Method
as O
the O
baseline O
. O
For O
MoS Method
, O
we O
consider O
7 O
or O
10 O
mixtures O
and O
reduce O
the O
hidden O
and O
/ O
or O
embedding O
size O
to O
match O
the O
baseline Metric
capacity Metric
. O
When O
decreasing O
the O
hidden O
and O
/ O
or O
embedding O
size O
, O
we O
either O
keep O
both O
the O
same O
, O
or O
make O
the O
hidden O
size O
relatively O
larger O
. O
The O
results O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
Clearly O
, O
the O
Softmax O
and O
MoS Method
obtain O
the O
same O
BPC Metric
on O
the O
test O
set O
and O
comparable O
BPC Metric
on O
the O
validation O
set O
, O
which O
well O
match O
our O
hypothesis O
. O
Since O
the O
only O
difference O
in O
word O
- O
level O
language Method
modeling Method
is O
the O
existence O
of O
the O
Softmax Method
bottleneck Method
, O
the O
distinct O
behavior O
of O
MoS Method
again O
supports O
our O
hypothesis O
that O
it O
is O
solving O
the O
Softmax Task
bottleneck Task
problem Task
. O
subsection O
: O
MoS Method
Computational O
Time O
We O
evaluate O
the O
additional O
computational Metric
cost Metric
introduced O
by O
MoS. Method
We O
consider O
two O
sets O
of O
controlled O
experiments O
. O
In O
the O
first O
set O
, O
we O
compare O
the O
training Metric
time Metric
of O
MoS Method
and O
Softmax Method
using O
the O
same O
batch O
sizes O
. O
In O
the O
second O
set O
, O
we O
compare O
the O
training Metric
time Metric
of O
two O
methods O
using O
the O
hyper O
- O
parameter O
settings O
that O
achieve O
the O
best O
performance O
for O
each O
model O
( O
i.e. O
, O
the O
settings O
in O
Tables O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
) O
. O
In O
both O
sets O
, O
we O
control O
two O
models O
to O
have O
comparable O
model O
sizes O
. O
The O
results O
on O
the O
three O
datasets O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
Thanks O
to O
the O
efficiency O
of O
matrix Method
multiplication Method
on O
GPU Method
, O
the O
computational O
wall O
time O
of O
MoS Method
is O
actually O
sub O
- O
linear O
w.r.t O
. O
the O
number O
of O
Softmaxes O
. O
In O
most O
settings O
, O
we O
observe O
a O
two O
to O
three O
times O
slowdown O
when O
using O
MoS. Method
Specifically O
, O
the O
‘ O
‘ O
bs O
’ O
’ O
setting O
measures O
the O
computational Metric
cost Metric
introduced O
by O
MoS Method
given O
enough O
memory O
, O
which O
is O
1.9x O
, O
2.5x O
, O
and O
3.8x O
slowdown O
on O
PTB Material
, O
WT2 Material
, O
and O
1B Material
respectively O
. O
The O
‘ O
‘ O
best O
- O
1 O
’ O
’ O
setting O
is O
usually O
slower O
compared O
to O
‘ O
‘ O
bs O
’ O
’ O
, O
because O
a O
single O
batch O
does O
not O
fit O
into O
the O
memory O
of O
a O
single O
GPU O
using O
MoS Method
, O
in O
which O
case O
we O
have O
to O
split O
one O
batch O
into O
multiple O
small O
ones O
, O
resulting O
in O
further O
slowdown O
. O
In O
this O
sense O
, O
the O
gap O
between O
‘ O
‘ O
best O
- O
1 O
’ O
’ O
and O
‘ O
‘ O
bs O
’ O
’ O
measures O
the O
computational Metric
cost Metric
introduced O
due O
to O
the O
increase O
of O
memory O
consumed O
by O
MoS. Method
The O
‘ O
‘ O
best O
- O
3 O
’ O
’ O
alleviates O
this O
issue O
by O
using O
three O
GPUs Method
, O
which O
allows O
larger O
- O
batch Method
training Method
for O
MoS. Method
In O
this O
case O
, O
we O
reduce O
the O
computational Metric
cost Metric
to O
2.9x O
on O
WT2 Material
and O
2.1x O
on O
1B Material
with O
our O
best O
performing O
model O
. O
Note O
that O
the O
computational Metric
cost Metric
is O
closely O
related O
to O
the O
batch O
size O
, O
which O
is O
interleaved O
with O
optimization Task
. O
Though O
how O
batch O
sizes O
affect O
optimization Task
remains O
an O
open O
question O
and O
might O
be O
task O
dependent O
, O
we O
believe O
the O
‘ O
‘ O
best O
- O
1 O
’ O
’ O
and O
‘ O
‘ O
best O
- O
3 O
’ O
’ O
settings O
well O
reflect O
the O
actual O
computational Metric
cost Metric
brought O
by O
MoS Method
on O
language Method
modeling Method
tasks O
. O
subsection O
: O
Qualitative Method
Analysis Method
Since O
MoC Method
shows O
a O
stronger O
performance O
than O
Softmax Method
on O
PTB Material
, O
the O
qualitative O
study O
focuses O
on O
the O
comparison O
between O
MoC Method
and O
MoS. Method
Concretely O
, O
given O
the O
same O
context O
( O
previous O
tokens O
) O
, O
we O
search O
for O
prediction O
steps O
where O
MoS Method
achieves O
lower O
negative Metric
log Metric
loss Metric
than O
MoC Method
by O
a O
margin O
. O
We O
show O
some O
representative O
cases O
in O
Table O
[ O
reference O
] O
with O
the O
following O
observations O
: O
Comparing O
the O
first O
two O
cases O
, O
given O
the O
same O
preceding O
word O
‘ O
‘ O
N O
’ O
’ O
, O
MoS Method
flexibly O
adjusts O
its O
top O
predictions O
based O
on O
the O
different O
topic O
quantities O
being O
discussed O
in O
the O
context O
. O
In O
comparison O
, O
MoC Method
emits O
quite O
similar O
top O
choices O
regardless O
of O
the O
context O
, O
suggesting O
its O
inferiority O
in O
make O
context Task
- Task
dependent Task
predictions Task
. O
In O
the O
3rd O
case O
, O
the O
context O
is O
about O
international O
politics O
, O
where O
country O
/ O
region O
names O
are O
likely O
to O
appear O
. O
MoS Method
captures O
this O
nuance O
well O
, O
and O
yields O
top O
choices O
that O
can O
be O
used O
to O
complete O
a O
country O
name O
given O
the O
immediate O
preceding O
word O
‘ O
‘ O
south O
’ O
’ O
. O
Similarly O
, O
in O
the O
4th O
case O
, O
MoS Method
is O
able O
to O
include O
‘ O
‘ O
ual O
’ O
’ O
, O
a O
core O
entity O
of O
discussion O
in O
the O
context O
, O
in O
its O
top O
predictions O
. O
In O
contrast O
, O
MoC Method
gives O
rather O
generic O
predictions O
irrieselevant O
to O
the O
context O
in O
both O
cases O
. O
For O
the O
5th O
and O
the O
6th O
example O
, O
we O
see O
MoS Method
is O
able O
to O
exploit O
less O
common O
words O
accurately O
according O
to O
the O
context O
, O
while O
MoC Method
fails O
to O
yield O
such O
choices O
. O
This O
well O
matches O
our O
analysis O
that O
MoS Method
has O
the O
capacity O
of O
modeling O
context O
- O
dependent O
language O
. O
