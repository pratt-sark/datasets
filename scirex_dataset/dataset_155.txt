Neural Task
Text Task
Generation Task
from O
Structured O
Data O
with O
Application O
to O
the O
Biography O
Domain O
section O
: O
Abstract O
This O
paper O
introduces O
a O
neural Method
model Method
for O
concept Task
- Task
to Task
- Task
text Task
generation Task
that O
scales O
to O
large O
, O
rich O
domains O
. O
It O
generates O
biographical O
sentences O
from O
fact O
tables O
on O
a O
new O
dataset O
of O
biographies O
from O
Wikipedia Material
. O
This O
set O
is O
an O
order O
of O
magnitude O
larger O
than O
existing O
resources O
with O
over O
700k O
samples O
and O
a O
400k O
vocabulary O
. O
Our O
model O
builds O
on O
conditional Method
neural Method
language Method
models Method
for O
text Task
generation Task
. O
To O
deal O
with O
the O
large O
vocabulary O
, O
we O
extend O
these O
models O
to O
mix O
a O
fixed O
vocabulary O
with O
copy O
actions O
that O
transfer O
sample O
- O
specific O
words O
from O
the O
input O
database O
to O
the O
generated O
output O
sentence O
. O
To O
deal O
with O
structured O
data O
, O
we O
allow O
the O
model O
to O
embed O
words O
differently O
depending O
on O
the O
data O
fields O
in O
which O
they O
occur O
. O
Our O
neural Method
model Method
significantly O
outperforms O
a O
Templated Method
Kneser Method
- Method
Ney Method
language Method
model Method
by O
nearly O
15 O
BLEU Metric
. O
section O
: O
Introduction O
Concept O
- O
to O
- O
text Task
generation Task
renders O
structured O
records O
into O
natural O
language O
[ O
reference O
] O
. O
A O
typical O
application O
is O
to O
generate O
a O
weather Task
forecast Task
based O
on O
a O
set O
of O
structured O
meteorological O
measurements O
. O
In O
contrast O
to O
previous O
work O
, O
we O
scale O
to O
the O
large O
and O
very O
diverse O
problem O
of O
generating Task
biographies Task
based O
on O
Wikipedia Material
infoboxes Material
. O
An O
infobox O
is O
a O
fact O
table O
describing O
a O
person O
, O
similar O
to O
a O
person O
subgraph O
in O
a O
knowledge O
base O
[ O
reference O
][ O
reference O
] O
. O
Similar O
generation Task
applications Task
include O
the O
generation Task
of Task
product Task
descriptions Task
based O
on O
a O
catalog O
of O
millions O
of O
items O
with O
dozens O
of O
attributes O
each O
. O
Previous O
work O
experimented O
with O
datasets O
that O
contain O
only O
a O
few O
tens O
of O
thousands O
of O
records O
such O
as O
WEATHERGOV Task
or O
the O
ROBOCUP Material
dataset Material
, O
while O
our O
dataset O
contains O
over O
700k O
biographies O
from O
* O
Rémi O
performed O
this O
work O
while O
interning O
at O
Facebook Material
. O
Wikipedia Material
. O
Furthermore O
, O
these O
datasets O
have O
a O
limited O
vocabulary O
of O
only O
about O
350 O
words O
each O
, O
compared O
to O
over O
400k O
words O
in O
our O
dataset O
. O
To O
tackle O
this O
problem O
we O
introduce O
a O
statistical Method
generation Method
model Method
conditioned O
on O
a O
Wikipedia Material
infobox Material
. O
We O
focus O
on O
the O
generation O
of O
the O
first O
sentence O
of O
a O
biography O
which O
requires O
the O
model O
to O
select O
among O
a O
large O
number O
of O
possible O
fields O
to O
generate O
an O
adequate O
output O
. O
Such O
diversity O
makes O
it O
difficult O
for O
classical O
count Method
- Method
based Method
models Method
to O
estimate O
probabilities Task
of Task
rare Task
events Task
due O
to O
data O
sparsity O
. O
We O
address O
this O
issue O
by O
parameterizing O
words O
and O
fields O
as O
embeddings O
, O
along O
with O
a O
neural Method
language Method
model Method
operating O
on O
them O
[ O
reference O
] O
. O
This O
factorization O
allows O
us O
to O
scale O
to O
a O
larger O
number O
of O
words O
and O
fields O
than O
[ O
reference O
] O
, O
or O
[ O
reference O
] O
where O
the O
number O
of O
parameters O
grows O
as O
the O
product O
of O
the O
number O
of O
words O
and O
fields O
. O
Moreover O
, O
our O
approach O
does O
not O
restrict O
the O
relations O
between O
the O
field O
contents O
and O
the O
generated O
text O
. O
This O
contrasts O
with O
less O
flexible O
strategies O
that O
assume O
the O
generation O
to O
follow O
either O
a O
hybrid O
alignment O
tree O
[ O
reference O
] O
, O
a O
probabilistic Method
context Method
- Method
free Method
grammar Method
[ O
reference O
] O
, O
or O
a O
tree Method
adjoining Method
grammar Method
[ O
reference O
] O
. O
Our O
model O
exploits O
structured O
data O
both O
globally O
and O
locally O
. O
Global Method
conditioning Method
summarizes O
all O
information O
about O
a O
personality O
to O
understand O
highlevel O
themes O
such O
as O
that O
the O
biography O
is O
about O
a O
scientist O
or O
an O
artist O
, O
while O
as O
local Method
conditioning Method
describes O
the O
previously O
generated O
tokens O
in O
terms O
of O
the O
their O
relationship O
to O
the O
infobox O
. O
We O
analyze O
the O
effectiveness O
of O
each O
and O
demonstrate O
their O
complementarity O
. O
section O
: O
Related O
Work O
Traditionally O
, O
generation Method
systems Method
relied O
on O
rules O
and O
hand O
- O
crafted O
specifications O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Generation O
is O
divided O
into O
modular O
, O
yet O
highly O
interdependent O
, O
decisions O
: O
( O
1 O
) O
content Task
planning Task
defines O
which O
parts O
of O
the O
input O
fields O
or O
meaning O
representations O
should O
be O
selected O
; O
( O
2 O
) O
sentence Task
planning Task
determines O
which O
selected O
fields O
are O
to O
be O
dealt O
with O
in O
each O
output O
sentence O
; O
and O
( O
3 O
) O
surface Method
realization Method
generates O
those O
sentences O
. O
Data Method
- Method
driven Method
approaches Method
have O
been O
proposed O
to O
automatically O
learn O
the O
individual O
modules O
. O
One O
approach O
first O
aligns O
records O
and O
sentences O
and O
then O
learns O
a O
content Method
selection Method
model Method
[ O
reference O
][ O
reference O
] O
. O
Hierarchical Method
hidden Method
semi Method
- Method
Markov Method
generative Method
models Method
have O
also O
been O
used O
to O
first O
determine O
which O
facts O
to O
discuss O
and O
then O
to O
generate O
words O
from O
the O
predicates O
and O
arguments O
of O
the O
chosen O
facts O
[ O
reference O
] O
. O
Sentence Task
planning Task
has O
been O
formulated O
as O
a O
supervised Task
set Task
partitioning Task
problem Task
over O
facts O
where O
each O
partition O
corresponds O
to O
a O
sentence O
[ O
reference O
] O
. O
End O
- O
to O
- O
end Method
approaches Method
have O
combined O
sentence Task
planning Task
and O
surface Task
realization Task
by O
using O
explicitly O
aligned O
sentence O
/ O
meaning O
pairs O
as O
training O
data O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
More O
recently O
, O
content Task
selection Task
and O
surface Task
realization Task
have O
been O
combined O
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
At O
the O
intersection O
of O
rule Method
- Method
based Method
and Method
statistical Method
methods Method
, O
hybrid Method
systems Method
aim O
at O
leveraging O
human O
contributed O
rules O
and O
corpus O
statistics O
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Our O
approach O
is O
inspired O
by O
the O
recent O
success O
of O
neural Method
language Method
models Method
for O
image Task
captioning Task
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
, O
machine Task
translation Task
[ O
reference O
][ O
reference O
][ O
reference O
] O
, O
and O
modeling O
conversations Task
and Task
dialogues Task
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Our O
model O
is O
most O
similar O
to O
[ O
reference O
] O
who O
use O
an O
encoder Method
- Method
decoder Method
style Method
neural Method
network Method
model Method
to O
tackle O
the O
WEATHERGOV Task
and O
ROBOCUP Task
tasks Task
. O
Their O
architecture O
relies O
on O
LSTM Method
units Method
and O
an O
attention Method
mechanism Method
which O
reduces O
scalability O
compared O
to O
our O
simpler O
design O
. O
section O
: O
Language Method
Modeling Method
for O
Constrained Task
Sentence Task
generation Task
Conditional Method
language Method
models Method
are O
a O
popular O
choice O
to O
generate O
sentences O
. O
We O
introduce O
a O
tableconditioned Method
language Method
model Method
for O
constraining O
text Task
generation Task
to O
include O
elements O
from O
fact O
tables O
. O
section O
: O
Language Method
model Method
Given O
a O
sentence O
s O
= O
w O
1 O
, O
. O
. O
. O
, O
w O
T O
with O
T O
words O
from O
vocabulary O
W O
, O
a O
language Method
model Method
estimates O
: O
Let O
c O
t O
= O
w O
t− O
( O
n−1 O
) O
, O
. O
. O
. O
, O
w O
t−1 O
be O
the O
sequence O
of O
n O
− O
1 O
context O
words O
preceding O
w O
t O
. O
An O
n Method
- Method
gram Method
language Method
model Method
makes O
an O
order Method
n Method
Markov Method
assumption Method
, O
section O
: O
Language Method
model Method
conditioned O
on O
tables O
A O
table O
is O
a O
set O
of O
field O
/ O
value O
pairs O
, O
where O
values O
are O
sequences O
of O
words O
. O
We O
therefore O
propose O
language Method
models Method
that O
are O
conditioned O
on O
these O
pairs O
. O
Local Task
conditioning Task
refers O
to O
the O
information O
from O
the O
table O
that O
is O
applied O
to O
the O
description O
of O
the O
words O
which O
have O
already O
generated O
, O
i.e. O
the O
previous O
words O
that O
constitute O
the O
context O
of O
the O
language Method
model Method
. O
The O
table O
allows O
us O
to O
describe O
each O
word O
not O
only O
by O
its O
string O
( O
or O
index O
in O
the O
vocabulary O
) O
but O
also O
by O
a O
descriptor O
of O
its O
occurrence O
in O
the O
table O
. O
Let O
F O
define O
the O
set O
of O
all O
possible O
fields O
f O
. O
The O
occurrence O
of O
a O
word O
w O
in O
the O
table O
is O
described O
by O
a O
set O
of O
( O
field O
, O
position O
) O
pairs O
. O
where O
m O
is O
the O
number O
of O
occurrences O
of O
w. O
Each O
pair O
( O
f O
, O
p O
) O
indicates O
that O
w O
occurs O
in O
field O
f O
at O
position O
p. O
In O
this O
scheme O
, O
most O
words O
are O
described O
by O
the O
empty O
set O
as O
they O
do O
not O
occur O
in O
the O
table O
. O
For O
example O
, O
the O
word O
linguistics O
in O
the O
table O
of O
Figure O
1 O
is O
described O
as O
follows O
: O
assuming O
words O
are O
lower O
- O
cased O
and O
commas O
are O
treated O
as O
separate O
tokens O
. O
Conditioning O
both O
on O
the O
field O
type O
and O
the O
position O
within O
the O
field O
allows O
the O
model O
to O
encode O
field O
- O
specific O
regularities O
, O
e.g. O
, O
a O
number O
token O
in O
a O
date O
field O
is O
likely O
followed O
by O
a O
month O
token O
; O
knowing O
that O
the O
number O
is O
the O
first O
token O
in O
the O
date O
field O
makes O
this O
even O
more O
likely O
. O
The O
( O
field O
, O
position O
) O
description O
scheme O
of O
the O
table O
does O
not O
allow O
to O
express O
that O
a O
token O
terminates O
a O
field O
which O
can O
be O
useful O
to O
capture O
field O
transitions O
. O
For O
biographies O
, O
the O
last O
token O
of O
the O
name O
field O
is O
often O
followed O
by O
an O
introduction O
of O
the O
birth O
date O
like O
' O
( O
' O
or O
' O
was O
born O
' O
. O
We O
hence O
extend O
our O
descriptor O
to O
a O
triplet O
that O
includes O
the O
position O
of O
the O
token O
counted O
from O
the O
end O
of O
the O
field O
: O
where O
our O
example O
becomes O
: O
We O
extend O
Equation O
2 O
to O
use O
the O
above O
information O
as O
additional O
conditioning O
context O
when O
generating O
a O
sentence O
s O
: O
where O
z O
ct O
= O
z O
w O
t− O
( O
n−1 O
) O
, O
. O
. O
. O
, O
z O
w O
t−1 O
are O
referred O
to O
as O
the O
local O
conditioning O
variables O
since O
they O
describe O
the O
local O
context O
( O
previous O
word O
) O
relations O
with O
the O
table O
. O
Global Task
conditioning Task
refers O
to O
information O
from O
all O
tokens O
and O
fields O
of O
the O
table O
, O
regardless O
whether O
they O
appear O
in O
the O
previous O
generated O
words O
or O
not O
. O
The O
set O
of O
fields O
available O
in O
a O
table O
often O
impacts O
the O
structure O
of O
the O
generation Task
. O
For O
biographies O
, O
the O
fields O
used O
to O
describe O
a O
politician O
are O
different O
from O
the O
ones O
for O
an O
actor O
or O
an O
athlete O
. O
We O
introduce O
global O
conditioning O
on O
the O
available O
fields O
g O
f O
as O
Similarly O
, O
global O
conditioning O
g O
w O
on O
the O
available O
words O
occurring O
in O
the O
table O
is O
introduced O
: O
Tokens O
provide O
information O
complementary O
to O
fields O
. O
For O
example O
, O
it O
may O
be O
hard O
to O
distinguish O
a O
basketball O
player O
from O
a O
hockey O
player O
by O
looking O
only O
at O
the O
field O
names O
, O
e.g. O
teams O
, O
league O
, O
position O
, O
weight O
and O
height O
, O
etc O
. O
However O
the O
actual O
field O
tokens O
such O
as O
team O
names O
, O
league O
name O
, O
player O
's O
position O
can O
help O
the O
model O
to O
give O
a O
better O
prediction O
. O
Here O
, O
g O
f O
∈ O
{ O
0 O
, O
1 O
} O
F O
and O
g O
w O
∈ O
{ O
0 O
, O
1 O
} O
W O
are O
binary O
indicators O
over O
fixed O
field O
and O
word O
vocabularies O
. O
Figure O
2 O
illustrates O
the O
model O
with O
a O
schematic O
example O
. O
For O
predicting O
the O
next O
word O
w O
t O
after O
a O
given O
context O
c O
t O
, O
the O
language Method
model Method
is O
conditioned O
on O
sets O
of O
triplets O
for O
each O
word O
occurring O
in O
the O
table O
z O
ct O
, O
along O
with O
all O
fields O
and O
words O
from O
this O
table O
. O
section O
: O
Copy O
actions O
So O
far O
we O
extended O
the O
model O
conditioning O
with O
features O
derived O
from O
the O
fact O
table O
. O
We O
now O
turn O
to O
using O
table O
information O
when O
scoring O
output O
words O
. O
In O
particular O
, O
sentences O
which O
express O
facts O
from O
a O
given O
table O
often O
copy O
words O
from O
the O
table O
. O
We O
therefore O
extend O
our O
model O
to O
also O
score O
special O
field O
tokens O
such O
as O
name O
1 O
or O
name O
2 O
which O
are O
subsequently O
added O
to O
the O
score O
of O
the O
corresponding O
words O
from O
the O
field O
value O
. O
Our O
model O
reads O
a O
table O
and O
defines O
an O
output O
domain O
W O
∪Q. O
Q O
defines O
all O
tokens O
in O
the O
table O
, O
which O
might O
include O
out O
of O
vocabulary O
words O
( O
/ O
∈ O
W O
) O
. O
For O
instance O
Park O
- O
Rhodes O
in O
Figure O
1 O
is O
not O
in O
W. O
However O
, O
Park O
- O
Rhodes O
will O
be O
included O
in O
Q O
as O
name O
2 O
( O
since O
it O
is O
the O
second O
token O
of O
the O
name O
field O
) O
which O
allows O
our O
model O
to O
generate O
it O
. O
This O
mechanism O
is O
inspired O
by O
recent O
work O
on O
attention Method
based Method
word Method
copying Method
for O
neural Task
machine Task
translation Task
[ O
reference O
] O
as O
well O
as O
delexicalization Method
for O
neural Task
dialog Task
systems Task
[ O
reference O
] O
. O
It O
also O
builds O
upon O
older O
work O
such O
as O
class Method
- Method
based Method
language Method
models Method
for O
dialog Task
systems Task
[ O
reference O
] O
. O
section O
: O
A O
Neural Method
Language Method
Model Method
Approach Method
A O
feed Method
- Method
forward Method
neural Method
language Method
model Method
( O
NLM Method
) O
estimates O
P O
( O
w O
t O
|c O
t O
) O
with O
a O
parametric Method
function Method
φ Method
θ Method
( O
Equation O
1 O
) O
, O
where O
θ O
refers O
to O
all O
learnable O
parameters O
of O
the O
network O
. O
This O
function O
is O
a O
composition O
of O
simple O
differentiable Method
functions Method
or O
layers Method
. O
section O
: O
Mathematical O
notations O
and O
layers O
We O
denote O
matrices O
as O
bold O
upper O
case O
letters O
( O
X O
, O
Y O
, O
Z O
) O
, O
and O
vectors O
as O
bold O
lower O
- O
case O
letters O
( O
a O
, O
b O
, O
c O
) O
. O
A O
i O
represents O
the O
i O
th O
row O
of O
matrix O
A. O
When O
A O
is O
a O
3 O
- O
d O
matrix O
, O
then O
A O
i O
, O
j O
represents O
the O
vector O
of O
the O
i O
th O
first O
dimension O
and O
j O
th O
second O
dimension O
. O
Unless O
otherwise O
stated O
, O
vectors O
are O
assumed O
to O
be O
column O
vectors O
. O
We O
use O
[ O
v O
1 O
; O
v O
2 O
] O
to O
denote O
vector Task
concatenation Task
. O
Next O
, O
we O
introduce O
the O
notation O
for O
the O
different O
layers O
used O
in O
our O
approach O
. O
Embedding Method
layer Method
. O
Given O
a O
parameter O
matrix O
X O
∈ O
R O
N O
×d O
, O
the O
embedding Method
layer Method
is O
a O
lookup O
table O
that O
performs O
an O
array Method
indexing Method
operation Method
: O
where O
X O
i O
corresponds O
to O
the O
embedding O
of O
the O
element O
x O
i O
at O
row O
i. O
When O
X O
is O
a O
3 O
- O
d O
matrix O
, O
the O
lookup O
table O
takes O
two O
arguments O
: O
where O
X O
i O
, O
j O
corresponds O
to O
the O
embedding O
of O
the O
pair O
( O
x O
i O
, O
x O
j O
) O
at O
index O
( O
i O
, O
j O
) O
. O
The O
lookup O
table O
operation O
can O
be O
applied O
for O
a O
sequence O
of O
elements O
s O
= O
x O
1 O
, O
. O
. O
. O
, O
x O
T O
. O
A O
common O
approach O
is O
to O
concatenate O
all O
resulting O
embeddings O
: O
Linear Method
layer Method
. O
This O
layer O
applies O
a O
linear Method
transformation Method
to O
its O
inputs O
x O
∈ O
R O
n O
: O
where O
θ O
= O
{ O
W O
, O
b O
} O
are O
the O
trainable O
parameters O
with O
W O
∈ O
R O
m×n O
being O
the O
weight O
matrix O
, O
and O
b O
∈ O
R O
m O
is O
the O
bias O
term O
. O
Softmax Method
layer Method
. O
Given O
a O
context O
input O
c O
t O
, O
the O
final O
layer O
outputs O
a O
score O
for O
each O
word O
w O
t O
∈ O
W O
, O
φ O
θ O
( O
c O
t O
) O
∈ O
R O
|W| O
. O
The O
probability O
distribution O
is O
obtained O
by O
applying O
the O
softmax Method
activation Method
function Method
: O
section O
: O
Embeddings O
as O
inputs O
A O
key O
aspect O
of O
neural Method
language Method
models Method
is O
the O
use O
of O
word Method
embeddings Method
. O
Similar O
words O
tend O
to O
have O
similar O
embeddings O
and O
thus O
share O
latent O
features O
. O
The O
probability Method
estimates Method
of O
those O
models O
are O
smooth O
functions O
of O
these O
embeddings O
, O
and O
a O
small O
change O
in O
the O
features O
results O
in O
a O
small O
change O
in O
the O
probability O
estimates O
[ O
reference O
] O
. O
Therefore O
, O
neural Method
language Method
models Method
can O
achieve O
better O
generalization Task
for O
unseen O
n O
- O
grams O
. O
Next O
, O
we O
show O
how O
we O
map O
fact O
tables O
to O
continuous O
space O
in O
similar O
spirit O
. O
Word Task
embeddings Task
. O
Formally O
, O
the O
embedding Method
layer Method
maps O
each O
context O
word O
index O
to O
a O
continuous O
d O
- O
dimensional O
vector O
. O
It O
relies O
on O
a O
parameter O
matrix O
E O
∈ O
R O
|W|×d O
to O
convert O
the O
input O
c O
t O
into O
n O
− O
1 O
vectors O
of O
dimension O
d O
: O
E O
can O
be O
initialized O
randomly O
or O
with O
pre Method
- Method
trained Method
word Method
embeddings Method
. O
Table O
embeddings O
. O
As O
described O
in O
Section O
3.2 O
, O
the O
language Method
model Method
is O
conditioned O
on O
elements O
from O
the O
table O
. O
Embedding O
matrices O
are O
therefore O
defined O
to O
model O
both O
local O
and O
global O
conditioning O
information O
. O
For O
local Task
conditioning Task
, O
we O
denote O
the O
maximum O
length O
of O
a O
sequence O
of O
words O
as O
l. O
Each O
field O
f O
j O
∈ O
F O
is O
associated O
with O
2 O
× O
l O
vectors O
of O
d O
dimensions O
, O
the O
first O
l O
of O
those O
vectors O
embed O
all O
possible O
starting O
positions O
1 O
, O
. O
. O
. O
, O
l O
, O
and O
the O
remaining O
l O
vectors O
embed O
ending O
positions O
. O
This O
results O
in O
two O
parameter O
matrices O
Z O
= O
{ O
Z O
+ O
, O
Z O
− O
} O
∈ O
R O
|F O
|×l×d O
. O
For O
a O
given O
triplet O
( O
f O
j O
, O
p O
refer O
to O
the O
embedding O
vectors O
of O
the O
start O
and O
end O
position O
for O
field O
f O
j O
, O
respectively O
. O
Finally O
, O
global Method
conditioning Method
uses O
two O
parameter O
matrices O
G O
f O
∈ O
R O
|F O
|×g O
and O
G O
w O
∈ O
R O
|W|×g O
. O
ψ O
G O
f O
( O
f O
j O
) O
maps O
a O
table O
field O
f O
j O
into O
a O
vector O
of O
dimension O
g O
, O
while O
ψ O
G O
w O
( O
w O
t O
) O
maps O
a O
word O
w O
t O
into O
a O
vector O
of O
the O
same O
dimension O
. O
In O
general O
, O
G O
w O
shares O
its O
parameters O
with O
E O
, O
provided O
d O
= O
g. O
Aggregating Task
embeddings Task
. O
We O
represent O
each O
occurence O
of O
a O
word O
w O
as O
a O
triplet O
( O
field O
, O
start O
, O
end O
) O
where O
we O
have O
embeddings O
for O
the O
start O
and O
end O
position O
as O
described O
above O
. O
Often O
times O
a O
particular O
word O
w O
occurs O
multiple O
times O
in O
a O
table O
, O
e.g. O
, O
' O
linguistics O
' O
has O
two O
instances O
in O
Figure O
1 O
. O
In O
this O
case O
, O
we O
perform O
a O
component Method
- Method
wise Method
max Method
over O
the O
start O
embeddings O
of O
all O
instances O
of O
w O
to O
obtain O
the O
best O
features O
across O
all O
occurrences O
of O
w. O
We O
do O
the O
same O
for O
end O
position O
embeddings O
: O
A O
special O
no Method
- Method
field Method
embedding Method
is O
assigned O
to O
w O
t O
when O
the O
word O
is O
not O
associated O
to O
any O
fields O
. O
An O
embedding O
ψ O
Z O
( O
z O
ct O
) O
for O
encoding O
the O
local O
conditioning O
of O
the O
input O
c O
t O
is O
obtained O
by O
concatenation Method
. O
For O
global Task
conditioning Task
, O
we O
define O
F O
q O
⊂ O
F O
as O
the O
set O
of O
all O
the O
fields O
in O
a O
given O
table O
q O
, O
and O
Q O
as O
the O
set O
of O
all O
words O
in O
q. O
We O
also O
perform O
max Method
aggregation Method
. O
This O
yields O
the O
vectors O
and O
The O
final O
embedding O
which O
encodes O
the O
context O
input O
with O
conditioning O
is O
then O
the O
concatenation O
of O
these O
vectors O
: O
For O
simplification O
purpose O
, O
we O
define O
the O
context O
input O
x O
= O
{ O
c O
t O
, O
z O
ct O
, O
g O
f O
, O
g O
w O
} O
in O
the O
following O
equations O
. O
This O
context Method
embedding Method
is O
mapped O
to O
a O
latent Method
context Method
representation Method
using O
a O
linear Method
operation Method
followed O
by O
a O
hyperbolic O
tangent O
: O
where O
α O
2 O
= O
{ O
W O
2 O
, O
b O
2 O
} O
, O
with O
W O
2 O
∈ O
R O
nhu×d O
1 O
and O
b O
2 O
∈ O
R O
nhu O
. O
section O
: O
In O
- O
vocabulary O
outputs O
The O
hidden Method
representation Method
of O
the O
context O
then O
goes O
to O
another O
linear Method
layer Method
to O
produce O
a O
real O
value O
score O
for O
each O
word O
in O
the O
vocabulary O
: O
where O
α O
3 O
= O
{ O
W O
3 O
, O
b O
3 O
} O
, O
with O
W O
3 O
∈ O
R O
|W|×nhu O
and O
b O
3 O
∈ O
R O
|W| O
, O
and O
α O
= O
{ O
α O
1 O
, O
α O
2 O
, O
α O
3 O
} O
. O
section O
: O
Mixing O
outputs O
for O
better O
copying O
Section O
3.3 O
explains O
that O
each O
word O
w O
from O
the O
table O
is O
also O
associated O
with O
z O
w O
, O
the O
set O
of O
fields O
in O
which O
it O
occurs O
, O
along O
with O
the O
position O
in O
that O
field O
. O
Similar O
to O
local O
conditioning O
, O
we O
represent O
each O
field O
and O
position O
pair O
( O
f O
j O
, O
p O
i O
) O
with O
an O
embedding O
ψ O
F O
( O
f O
j O
, O
p O
i O
) O
, O
where O
F O
∈ O
R O
|F O
|×l×d O
. O
These O
embeddings O
are O
then O
projected O
into O
the O
same O
space O
as O
the O
latent Method
representation Method
of Method
context Method
input Method
h Method
( Method
x Method
) O
∈ O
R O
nhu O
. O
Using O
the O
max Method
operation Method
over O
the O
embedding O
dimension O
, O
each O
word O
is O
finally O
embedded O
into O
a O
unique O
vector O
: O
where O
β O
= O
{ O
W O
4 O
, O
b O
4 O
} O
with O
W O
4 O
∈ O
R O
nhu×d O
, O
and O
b O
4 O
∈ O
R O
nhu O
. O
A O
dot Method
product Method
with O
the O
context O
vector O
produces O
a O
score O
for O
each O
word O
w O
in O
the O
table O
, O
Each O
word O
w O
∈ O
W O
∪ O
Q O
receives O
a O
final O
score O
by O
summing O
the O
vocabulary Metric
score Metric
and O
the O
field Metric
score Metric
: O
with O
θ O
= O
{ O
α O
, O
β O
} O
, O
and O
where O
φ O
Q O
β O
( O
x O
, O
w O
) O
= O
0 O
when O
w O
/ O
∈ O
Q. O
The O
softmax Method
function Method
then O
maps O
the O
scores O
to O
a O
distribution O
over O
W O
∪ O
Q O
, O
log O
P O
( O
w|x O
) O
= O
φ O
θ O
( O
x O
, O
w O
) O
−log O
w O
∈W∪Q O
exp O
φ O
θ O
( O
x O
, O
w O
) O
. O
section O
: O
Training O
The O
neural Method
language Method
model Method
is O
trained O
to O
minimize O
the O
negative O
log O
- O
likelihood O
of O
a O
training O
sentence O
s O
with O
stochastic Method
gradient Method
descent Method
( O
SGD Method
; O
[ O
reference O
] O
: O
section O
: O
Experiments O
Our O
neural Method
network Method
model Method
( O
Section O
4 O
) O
is O
designed O
to O
generate O
sentences O
from O
tables O
for O
large Task
- Task
scale Task
problems Task
, O
where O
a O
diverse O
set O
of O
sentence O
types O
need O
to O
be O
generated O
. O
Biographies O
are O
therefore O
a O
good O
framework O
to O
evaluate O
our O
model O
, O
with O
Wikipedia Material
offering O
a O
large O
and O
diverse O
dataset O
. O
section O
: O
Biography O
dataset O
We O
introduce O
a O
new O
dataset O
for O
text Task
generation Task
, O
WIKIBIO Material
, O
a O
corpus O
of O
728 O
, O
321 O
articles O
from O
English Material
Wikipedia Material
[ O
reference O
] O
. O
It O
comprises O
all O
biography O
articles O
listed O
by O
WikiProject Material
Biography Material
1 O
which O
also O
have O
a O
table O
( O
infobox O
) O
. O
We O
extract O
and O
tokenize O
the O
first O
sentence O
of O
each O
article O
with O
Stanford Material
CoreNLP Material
[ O
reference O
] O
. O
All O
numbers O
are O
mapped O
to O
a O
special O
token O
, O
except O
for O
years O
which O
are O
mapped O
to O
different O
special O
token O
. O
Field O
values O
from O
tables O
are O
similarly O
tokenized O
. O
All O
tokens O
are O
lower O
- O
cased O
. O
Table O
2 O
summarizes O
the O
dataset O
statistics O
: O
on O
average O
, O
the O
first O
sentence O
is O
twice O
as O
short O
as O
the O
table O
( O
26.1 O
vs O
53.1 O
tokens O
) O
, O
about O
a O
third O
of O
the O
sentence O
tokens O
( O
9.5 O
) O
also O
occur O
in O
the O
table O
. O
The O
final O
corpus O
has O
been O
divided O
into O
three O
sub O
- O
parts O
to O
provide O
training O
( O
80 O
% O
) O
, O
validation Metric
( O
10 O
% O
) O
and O
test O
sets O
( O
10 O
% O
) O
. O
The O
dataset O
is O
available O
for O
download O
2 O
. O
section O
: O
Baseline O
Our O
baseline O
is O
an O
interpolated Method
Kneser Method
- Method
Ney Method
( O
KN Method
) O
language O
model O
and O
we O
use O
the O
KenLM Method
toolkit Method
to O
train O
5 Method
- Method
gram Method
models Method
without O
pruning O
[ O
reference O
] O
. O
We O
also O
learn O
a O
KN Method
language O
model O
over O
templates O
. O
For O
that O
purpose O
, O
we O
replace O
the O
words O
occurring O
in O
both O
the O
table O
and O
the O
training O
sentences O
with O
a O
special O
token O
reflecting O
its O
table O
descriptor O
z O
w O
( O
Equation O
3 O
) O
. O
The O
introduction O
section O
of O
the O
table O
in O
Figure O
1 O
looks O
as O
follows O
under O
this O
scheme O
: O
" O
name O
1 O
name O
2 O
( O
birthdate O
1 O
birthdate O
2 O
birthdate O
3 O
- O
deathdate O
1 O
deathdate O
2 O
deathdate O
3 O
) O
was O
an O
english O
linguist O
, O
fields O
3 O
pathologist O
, O
fields O
10 O
scientist O
, O
mathematician O
, O
mystic O
and O
mycologist O
. O
" O
During O
inference Task
, O
the O
decoder Method
is O
constrained O
to O
emit O
words O
from O
the O
regular O
vocabulary O
or O
special O
tokens O
occurring O
in O
the O
input O
table O
. O
When O
picking O
a O
special O
token O
we O
copy O
the O
corresponding O
word O
from O
the O
table O
. O
section O
: O
Training O
setup O
For O
our O
neural Method
models Method
, O
we O
train O
11 Method
- Method
gram Method
language Method
models Method
( O
n O
= O
11 O
) O
with O
a O
learning Metric
rate Metric
set O
to O
0.0025 O
. O
Table O
NLM Method
Table Method
1 O
: O
BLEU Metric
, O
ROUGE Metric
, O
NIST Metric
and O
perplexity Metric
without O
copy O
actions O
( O
first O
three O
rows O
) O
and O
with O
copy O
actions O
( O
last O
five O
rows O
) O
. O
For O
neural Method
models Method
we O
report O
" O
mean Metric
+ Metric
− Metric
standard Metric
deviation Metric
" O
for O
five O
training O
runs O
with O
different O
initialization O
. O
Decoding O
beam O
width O
is O
5 O
. O
Perplexities O
marked O
with O
and O
† O
are O
not O
directly O
comparable O
as O
the O
output O
vocabularies O
differ O
slightly O
. O
Table O
3 O
describes O
the O
other O
hyper O
- O
parameters O
. O
We O
include O
all O
fields O
occurring O
at O
least O
100 O
times O
in O
the O
training O
data O
in O
F O
, O
the O
set O
of O
fields O
. O
We O
include O
the O
20 O
, O
000 O
most O
frequent O
words O
in O
the O
vocabulary O
. O
The O
other O
hyperparameters O
are O
set O
through O
validation Task
, O
maximizing O
BLEU Metric
over O
a O
validation O
subset O
of O
1 O
, O
000 O
sentences O
. O
Similarly O
, O
early O
stopping O
is O
applied O
: O
training O
ends O
when O
BLEU Metric
stops O
improving O
on O
the O
same O
validation O
subset O
. O
One O
should O
note O
that O
the O
maximum O
number O
of O
tokens O
in O
a O
field O
l O
= O
10 O
means O
that O
we O
encode O
only O
10 O
positions O
: O
for O
longer O
field O
values O
the O
final O
tokens O
are O
not O
dropped O
but O
their O
position O
is O
capped O
to O
10 O
. O
We O
initialize O
the O
word O
embeddings O
W O
from O
Hellinger Method
PCA Method
computed O
over O
the O
set O
of O
training O
biographies O
. O
This O
representation O
has O
shown O
to O
be O
helpful O
for O
various O
applications O
[ O
reference O
] O
. O
section O
: O
Model O
section O
: O
Mean O
Percentile O
section O
: O
Evaluation Metric
metrics Metric
We O
use O
different O
metrics O
to O
evaluate O
our O
models O
. O
Performance O
is O
first O
evaluated O
in O
terms O
of O
perplexity Metric
which O
is O
the O
standard O
metric Metric
for O
language Task
modeling Task
. O
Generation Metric
quality Metric
is O
assessed O
automatically O
with O
BLEU Metric
- O
4 O
, O
ROUGE Metric
- O
4 O
( O
F O
- O
measure O
) O
and O
NIST Metric
- Metric
4 Metric
3 O
[ O
reference O
] O
. O
section O
: O
Results O
This O
section O
describes O
our O
results O
and O
discusses O
the O
impact O
of O
the O
different O
conditioning O
variables O
. O
section O
: O
The O
more O
, O
the O
better O
The O
results O
( O
Table O
1 O
) O
show O
that O
more O
conditioning O
information O
helps O
to O
improve O
the O
performance O
of O
our O
models O
. O
The O
generation Metric
metrics Metric
BLEU Metric
, O
ROUGE Metric
and O
NIST Metric
all O
gives O
the O
same O
performance O
ordering O
over O
models O
. O
We O
first O
discuss O
models O
without O
copy O
actions O
( O
the O
first O
three O
results O
) O
and O
then O
discuss O
models O
with O
copy O
actions O
( O
the O
remaining O
results O
) O
. O
Note O
that O
the O
factorization O
of O
our O
models O
results O
in O
three O
different O
output O
domains O
which O
makes O
perplexity Task
comparisons Task
less O
straightforward O
: O
models O
without O
copy O
actions O
operate O
over O
a O
fixed O
vocabulary O
. O
Template O
KN Method
adds O
a O
fixed O
set O
of O
field O
/ O
position O
pairs O
to O
this O
vocabulary O
while O
Table Method
NLM Method
models O
a O
variable O
set O
Q O
depending O
on O
the O
input O
table O
, O
see O
Section O
3.3 O
. O
Without O
copy O
actions O
. O
In O
terms O
of O
perplexity Task
the O
( O
i O
) O
neural Method
language Method
model Method
( O
NLM Method
) O
is O
slightly O
better O
than O
an O
interpolated O
KN Method
language O
model O
, O
and O
( O
ii O
) O
adding O
local O
conditioning O
on O
the O
field O
start O
and O
end O
position O
further O
improves O
accuracy Metric
. O
Generation Metric
metrics Metric
are O
generally O
very O
low O
but O
there O
is O
a O
clear O
improvement O
when O
using O
local O
conditioning O
since O
it O
allows O
to O
learn O
transitions O
between O
fields O
by O
linking O
previous O
predictions O
to O
the O
table O
unlike O
KN Method
or O
plain O
NLM Method
. O
With O
copy O
actions O
. O
For O
experiments O
with O
copy Task
actions Task
we O
use O
the O
full O
local O
conditioning O
( O
Equation O
4 O
) O
in O
the O
neural Method
language Method
models Method
. O
BLEU Metric
, O
ROUGE Metric
and O
NIST Metric
all O
improves O
when O
moving O
from O
Template O
KN Method
to O
Table Method
NLM Method
and O
more O
features O
successively O
improve O
accuracy Metric
. O
Global O
conditioning O
on O
the O
fields O
improves O
the O
model O
by O
over O
7 O
BLEU Metric
and O
adding O
words O
gives O
an O
additional O
1.3 O
BLEU Metric
. O
This O
is O
a O
total O
improvement O
of O
nearly O
15 O
BLEU Metric
over O
the O
Template Method
Kneser Method
- Method
Ney Method
baseline Method
. O
Similar O
observations O
are O
made O
for O
ROUGE Metric
+ Metric
15 Metric
and O
NIST Metric
+ Metric
2.8 Metric
. O
( O
Table Method
NLM Method
) O
and O
the O
baseline O
( O
Template O
KN Method
) O
for O
different O
beam O
sizes O
. O
The O
x O
- O
axis O
is O
the O
average O
timing O
( O
in O
milliseconds O
) O
for O
generating O
one O
sentence O
. O
The O
y O
- O
axis O
is O
the O
BLEU Metric
score O
. O
All O
results O
are O
measured O
on O
a O
subset O
of O
1 O
, O
000 O
samples O
of O
the O
validation O
set O
. O
section O
: O
Attention Method
mechanism Method
Our O
model O
implements O
attention O
over O
input O
table O
fields O
. O
For O
each O
word O
w O
in O
the O
table O
, O
Equation O
( O
23 O
) O
takes O
the O
language O
model O
score O
φ O
W O
ct O
and O
adds O
a O
bias O
φ O
Q O
ct O
. O
The O
bias O
is O
the O
dot O
- O
product O
between O
a O
representation O
of O
the O
table O
field O
in O
which O
w O
occurs O
and O
a O
representation O
of O
the O
context O
, O
Equation O
( O
22 O
) O
that O
summarizes O
the O
previously O
generated O
fields O
and O
words O
. O
Figure O
4 O
shows O
that O
this O
mechanism O
adds O
a O
large O
bias O
to O
continue O
a O
field O
if O
it O
has O
not O
generated O
all O
tokens O
from O
the O
table O
, O
e.g. O
, O
it O
emits O
the O
word O
occurring O
in O
name O
2 O
after O
generating O
name O
1 O
. O
It O
also O
nicely O
handles O
transitions O
between O
field O
types O
, O
e.g. O
, O
the O
model O
adds O
a O
large O
bias O
to O
the O
words O
occurring O
in O
the O
occupation O
field O
after O
emitting O
the O
birthdate O
. O
section O
: O
Sentence Task
decoding Task
We O
use O
a O
standard O
beam Method
search Method
to O
explore O
a O
larger O
set O
of O
sentences O
compared O
to O
simple O
greedy Method
search Method
. O
This O
allows O
us O
to O
explore O
K O
times O
more O
paths O
which O
comes O
at O
a O
linear O
increase O
in O
the O
number O
of O
forward O
computation O
steps O
for O
our O
language Method
model Method
. O
We O
compare O
various O
beam O
settings O
for O
the O
baseline O
Template O
KN Method
and O
our O
Table O
NLM Method
( O
Figure O
3 O
) O
. O
The O
best O
validation O
BLEU Metric
can O
be O
obtained O
with O
a O
beam O
size O
of O
K O
= O
5 O
. O
Our O
model O
is O
also O
several O
times O
faster O
than O
the O
baseline O
, O
requiring O
only O
about O
200 O
ms O
per O
sentence O
with O
K O
= O
5 O
. O
Beam Method
search Method
generates O
many O
ngram Method
lookups Method
for O
Kneser O
- O
Ney O
which O
requires O
many O
random O
memory O
accesses O
; O
while O
neural Method
models Method
perform O
scoring Task
through O
matrix Method
- Method
matrix Method
products Method
, O
an O
operation O
which O
is O
more O
local O
and O
can O
be O
performed O
in O
a O
block O
parallel O
manner O
where O
modern O
graphic O
processors O
shine O
[ O
reference O
] O
. O
Table O
4 O
shows O
generations O
for O
different O
variants O
of O
our O
model O
based O
on O
the O
Wikipedia Material
table Material
in O
Figure O
1 O
. O
First O
of O
all O
, O
comparing O
the O
reference O
to O
the O
fact O
table O
reveals O
that O
our O
training O
data O
is O
not O
perfect O
. O
The O
birth O
month O
mentioned O
in O
the O
fact O
table O
and O
the O
first O
sentence O
of O
the O
Wikipedia Material
article Material
are O
different O
; O
this O
may O
have O
been O
introduced O
by O
one O
contributor O
editing O
the O
article O
and O
not O
keeping O
the O
information O
consistent O
. O
section O
: O
Qualitative Method
analysis Method
All O
three O
versions O
of O
our O
model O
correctly O
generate O
the O
beginning O
of O
the O
sentence O
by O
copying O
the O
name O
, O
the O
birth O
date O
and O
the O
death O
date O
from O
the O
table O
. O
The O
model O
correctly O
uses O
the O
past O
tense O
since O
the O
death O
date O
in O
the O
table O
indicates O
that O
the O
person O
has O
passed O
away O
. O
Frederick O
Parker O
- O
Rhodes O
was O
a O
scientist O
, O
but O
this O
occupation O
is O
not O
directly O
mentioned O
in O
the O
table O
. O
The O
model O
without O
global O
conditioning O
can O
therefore O
not O
predict O
the O
right O
occupation O
, O
and O
it O
continues O
the O
generation O
with O
the O
most O
common O
occupation O
( O
in O
Wikipedia Material
) O
for O
a O
person O
who O
has O
died O
. O
In O
contrast O
, O
the O
global O
conditioning O
over O
the O
fields O
helps O
the O
model O
to O
understand O
that O
this O
person O
was O
indeed O
a O
scientist O
. O
However O
, O
it O
is O
only O
with O
the O
global O
conditioning O
on O
the O
words O
that O
the O
model O
can O
infer O
the O
correct O
occupation O
, O
i.e. O
, O
computer O
scientist O
. O
section O
: O
Conclusions O
We O
have O
shown O
that O
our O
model O
can O
generate O
fluent O
descriptions O
of O
arbitrary O
people O
based O
on O
structured O
data O
. O
Local Method
and Method
global Method
conditioning Method
improves O
our O
model O
by O
a O
large O
margin O
and O
we O
outperform O
a O
Kneser Method
- Method
Ney Method
language Method
model Method
by O
nearly O
15 O
BLEU Metric
. O
Our O
task O
uses O
an O
order O
of O
magnitude O
more O
data O
than O
previous O
work O
and O
has O
a O
vocabulary O
that O
is O
three O
orders O
of O
magnitude O
larger O
. O
In O
this O
paper O
, O
we O
have O
only O
focused O
on O
generating O
the O
first O
sentence O
and O
we O
will O
tackle O
the O
generation Task
of Task
longer Task
biographies Task
in O
future O
work O
. O
Also O
, O
the O
encoding O
of O
field O
values O
can O
be O
improved O
. O
Currently O
, O
we O
only O
attach O
the O
field O
type O
and O
token O
position O
to O
each O
word O
type O
and O
perform O
a O
max Method
- Method
pooling Method
for O
local Task
conditioning Task
. O
One O
could O
leverage O
a O
richer O
representation O
by O
learning O
an O
encoder Method
conditioned O
on O
the O
field O
type O
, O
e.g. O
a O
recurrent Method
encoder Method
or O
a O
convolutional Method
encoder Method
with O
different O
pooling Method
strategies Method
. O
Furthermore O
, O
the O
current O
training Method
loss Method
function Method
does O
not O
explicitly O
penalize O
the O
model O
for O
generating O
incorrect O
facts O
, O
e.g. O
predicting O
an O
incorrect O
nationality O
or O
occupation O
is O
currently O
not O
considered O
worse O
than O
choosing O
an O
incorrect O
determiner O
. O
A O
loss Method
function Method
that O
could O
assess O
factual Metric
accuracy Metric
would O
certainly O
improve O
sentence Task
generation Task
by O
avoiding O
such O
mistakes O
. O
Also O
it O
will O
be O
important O
to O
define O
a O
strategy O
for O
evaluating O
the O
factual Metric
accuracy Metric
of O
a O
generation Task
, O
beyond O
BLEU Metric
, O
ROUGE Metric
or O
NIST Metric
. O
section O
: O
