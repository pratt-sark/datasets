Artificial	Task
Error	Task
Generation	Task
with	O
Machine	Task
Translation	Task
and	O
Syntactic	O
Patterns	O
section	O
:	O
Abstract	O
Shortage	O
of	O
available	O
training	O
data	O
is	O
holding	O
back	O
progress	O
in	O
the	O
area	O
of	O
automated	Task
error	Task
detection	Task
.	O
This	O
paper	O
investigates	O
two	O
alternative	O
methods	O
for	O
artificially	Task
generating	Task
writing	Task
errors	Task
,	O
in	O
order	O
to	O
create	O
additional	O
resources	O
.	O
We	O
propose	O
treating	O
error	Task
generation	Task
as	O
a	O
machine	Method
translation	Method
task	O
,	O
where	O
grammatically	O
correct	O
text	O
is	O
translated	O
to	O
contain	O
errors	O
.	O
In	O
addition	O
,	O
we	O
explore	O
a	O
system	O
for	O
extracting	Task
textual	Task
patterns	Task
from	O
an	O
annotated	O
corpus	O
,	O
which	O
can	O
then	O
be	O
used	O
to	O
insert	O
errors	O
into	O
grammatically	O
correct	O
sentences	O
.	O
Our	O
experiments	O
show	O
that	O
the	O
inclusion	O
of	O
artificially	O
generated	O
errors	O
significantly	O
improves	O
error	Metric
detection	Metric
accuracy	Metric
on	O
both	O
FCE	Material
and	O
CoNLL	Material
2014	Material
datasets	Material
.	O
section	O
:	O
Introduction	O
Writing	Task
errors	Task
can	O
occur	O
in	O
many	O
different	O
formsfrom	O
relatively	O
simple	O
punctuation	O
and	O
determiner	O
errors	O
,	O
to	O
mistakes	O
including	O
word	O
tense	O
and	O
form	O
,	O
incorrect	O
collocations	O
and	O
erroneous	O
idioms	O
.	O
Automatically	O
identifying	O
all	O
of	O
these	O
errors	O
is	O
a	O
challenging	O
task	O
,	O
especially	O
as	O
the	O
amount	O
of	O
available	O
annotated	O
data	O
is	O
very	O
limited	O
.	O
[	O
reference	O
]	O
showed	O
that	O
while	O
some	O
error	Method
detection	Method
algorithms	Method
perform	O
better	O
than	O
others	O
,	O
it	O
is	O
additional	O
training	O
data	O
that	O
has	O
the	O
biggest	O
impact	O
on	O
improving	O
performance	O
.	O
Being	O
able	O
to	O
generate	O
realistic	O
artificial	O
data	O
would	O
allow	O
for	O
any	O
grammatically	O
correct	O
text	O
to	O
be	O
transformed	O
into	O
annotated	O
examples	O
containing	O
writing	O
errors	O
,	O
producing	O
large	O
amounts	O
of	O
additional	O
training	O
examples	O
.	O
Supervised	Method
error	Method
generation	Method
systems	Method
would	O
also	O
provide	O
an	O
efficient	O
method	O
for	O
anonymising	O
the	O
source	O
corpus	O
-	O
error	O
statistics	O
from	O
a	O
private	O
corpus	O
can	O
be	O
aggregated	O
and	O
applied	O
to	O
a	O
different	O
target	O
text	O
,	O
obscuring	O
sensitive	O
information	O
in	O
the	O
original	O
examination	O
scripts	O
.	O
However	O
,	O
the	O
task	O
of	O
creating	O
incorrect	O
data	O
is	O
somewhat	O
more	O
difficult	O
than	O
might	O
initially	O
appear	O
-	O
naive	O
methods	O
for	O
error	Task
generation	Task
can	O
create	O
data	O
that	O
does	O
not	O
resemble	O
natural	O
errors	O
,	O
thereby	O
making	O
downstream	Method
systems	Method
learn	O
misleading	O
or	O
uninformative	O
patterns	O
.	O
Previous	O
work	O
on	O
artificial	Task
error	Task
generation	Task
(	O
AEG	Task
)	O
has	O
focused	O
on	O
specific	O
error	O
types	O
,	O
such	O
as	O
prepositions	O
and	O
determiners	O
[	O
reference	O
]	O
,	O
or	O
noun	O
number	O
errors	O
[	O
reference	O
]	O
.	O
investigated	O
the	O
use	O
of	O
linguistic	O
information	O
when	O
generating	O
artificial	O
data	O
for	O
error	Task
correction	Task
,	O
but	O
also	O
restricting	O
the	O
approach	O
to	O
only	O
five	O
error	O
types	O
.	O
There	O
has	O
been	O
very	O
limited	O
research	O
on	O
generating	O
artificial	O
data	O
for	O
all	O
types	O
,	O
which	O
is	O
important	O
for	O
general	O
-	O
purpose	Task
error	Task
detection	Task
systems	Task
.	O
For	O
example	O
,	O
the	O
error	O
types	O
investigated	O
by	O
cover	O
only	O
35.74	O
%	O
of	O
all	O
errors	O
present	O
in	O
the	O
CoNLL	Material
2014	Material
training	Material
dataset	Material
,	O
providing	O
no	O
additional	O
information	O
for	O
the	O
majority	O
of	O
errors	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
two	O
supervised	Method
approaches	Method
for	O
generating	O
all	O
types	O
of	O
artificial	Task
errors	Task
.	O
We	O
propose	O
a	O
framework	O
for	O
generating	Task
errors	Task
based	O
on	O
statistical	Method
machine	Method
translation	Method
(	O
SMT	Method
)	Method
,	O
training	O
a	O
model	O
to	O
translate	O
from	O
correct	O
into	O
incorrect	O
sentences	O
.	O
In	O
addition	O
,	O
we	O
describe	O
a	O
method	O
for	O
learning	Task
error	Task
patterns	Task
from	O
an	O
annotated	O
corpus	O
and	O
transplanting	O
them	O
into	O
error	O
-	O
free	O
text	O
.	O
We	O
evaluate	O
the	O
effect	O
of	O
introducing	O
artificial	O
data	O
on	O
two	O
error	Task
detection	Task
benchmarks	Task
.	O
Our	O
results	O
show	O
that	O
each	O
method	O
provides	O
significant	O
improvements	O
over	O
using	O
only	O
the	O
available	O
training	O
set	O
,	O
and	O
a	O
combination	O
of	O
both	O
gives	O
an	O
absolute	O
improvement	O
of	O
4.3	O
%	O
in	O
F	Metric
0.5	Metric
,	O
without	O
requiring	O
any	O
additional	O
annotated	O
data	O
.	O
section	O
:	O
arXiv:1707.05236v1	O
[	O
cs	O
.	O
CL	O
]	O
17	O
Jul	O
2017	O
Original	O
We	O
are	O
a	O
well	O
-	O
mixed	O
class	O
with	O
equal	O
numbers	O
of	O
boys	O
and	O
girls	O
,	O
all	O
about	O
20	O
years	O
old	O
.	O
section	O
:	O
FY14	O
We	O
am	O
a	O
well	O
-	O
mixed	O
class	O
with	O
equal	O
numbers	O
of	O
boys	O
and	O
girls	O
,	O
all	O
about	O
20	O
years	O
old	O
.	O
section	O
:	O
PAT	Method
We	O
are	O
a	O
well	O
-	O
mixed	O
class	O
with	O
equal	O
numbers	O
of	O
boys	O
an	O
girls	O
,	O
all	O
about	O
20	O
year	O
old	O
.	O
section	O
:	O
MT	Method
We	O
are	O
a	O
well	O
-	O
mixed	O
class	O
with	O
equals	O
numbers	O
of	O
boys	O
and	O
girls	O
,	O
all	O
about	O
20	O
years	O
old	O
.	O
section	O
:	O
Error	Method
Generation	Method
Methods	Method
We	O
investigate	O
two	O
alternative	O
methods	O
for	O
AEG	Task
.	O
The	O
models	O
receive	O
grammatically	O
correct	O
text	O
as	O
input	O
and	O
modify	O
certain	O
tokens	O
to	O
produce	O
incorrect	O
sequences	O
.	O
The	O
alternative	O
versions	O
of	O
each	O
sentence	O
are	O
aligned	O
using	O
Levenshtein	O
distance	O
,	O
allowing	O
us	O
to	O
identify	O
specific	O
words	O
that	O
need	O
to	O
be	O
marked	O
as	O
errors	O
.	O
While	O
these	O
alignments	O
are	O
not	O
always	O
perfect	O
,	O
we	O
found	O
them	O
to	O
be	O
sufficient	O
for	O
practical	O
purposes	O
,	O
since	O
alternative	O
alignments	O
of	O
similar	O
sentences	O
often	O
result	O
in	O
the	O
same	O
binary	O
labeling	O
.	O
Future	O
work	O
could	O
explore	O
more	O
advanced	O
alignment	Method
methods	Method
,	O
such	O
as	O
proposed	O
by	O
.	O
In	O
Section	O
4	O
,	O
this	O
automatically	O
labeled	O
data	O
is	O
then	O
used	O
for	O
training	O
error	Method
detection	Method
models	Method
.	O
section	O
:	O
Machine	Task
Translation	Task
We	O
treat	O
AEG	Task
as	O
a	O
translation	Task
task	Task
-	O
given	O
a	O
correct	O
sentence	O
as	O
input	O
,	O
the	O
system	O
would	O
learn	O
to	O
translate	O
it	O
to	O
contain	O
likely	O
errors	O
,	O
based	O
on	O
a	O
training	O
corpus	O
of	O
parallel	O
data	O
.	O
Existing	O
SMT	Method
approaches	Method
are	O
already	O
optimised	O
for	O
identifying	O
context	O
patterns	O
that	O
correspond	O
to	O
specific	O
output	O
sequences	O
,	O
which	O
is	O
also	O
required	O
for	O
generating	O
human	O
-	O
like	O
errors	O
.	O
The	O
reverse	O
of	O
this	O
idea	O
,	O
translating	O
from	O
incorrect	O
to	O
correct	O
sentences	O
,	O
has	O
been	O
shown	O
to	O
work	O
well	O
for	O
error	Task
correction	Task
tasks	Task
[	O
reference	O
][	O
reference	O
]	O
,	O
and	O
roundtrip	Task
translation	Task
has	O
also	O
been	O
shown	O
to	O
be	O
promising	O
for	O
correcting	Task
grammatical	Task
errors	Task
[	O
reference	O
]	O
.	O
Following	O
previous	O
work	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
build	O
a	O
phrase	Method
-	Method
based	Method
SMT	Method
error	Method
generation	Method
system	Method
.	O
During	O
training	O
,	O
error	O
-	O
corrected	O
sentences	O
in	O
the	O
training	O
data	O
are	O
treated	O
as	O
the	O
source	O
,	O
and	O
the	O
original	O
sentences	O
written	O
by	O
language	O
learners	O
as	O
the	O
target	O
.	O
Pialign	Method
[	O
reference	O
]	O
)	O
is	O
used	O
to	O
create	O
a	O
phrase	O
translation	O
table	O
directly	O
from	O
model	O
probabilities	O
.	O
In	O
addition	O
to	O
default	O
features	O
,	O
we	O
add	O
character	O
-	O
level	O
Levenshtein	O
distance	O
to	O
each	O
mapping	O
in	O
the	O
phrase	O
table	O
,	O
as	O
proposed	O
by	O
.	O
Decoding	Task
is	O
performed	O
using	O
Moses	Method
[	O
reference	O
]	O
)	O
and	O
the	O
language	Method
model	Method
used	O
during	O
decoding	Task
is	O
built	O
from	O
the	O
original	O
erroneous	O
sentences	O
in	O
the	O
learner	O
corpus	O
.	O
The	O
IRSTLM	Method
Toolkit	Method
[	O
reference	O
]	O
is	O
used	O
for	O
building	O
a	O
5	Method
-	Method
gram	Method
language	Method
model	Method
with	O
modified	O
Kneser	Method
-	Method
Ney	Method
smoothing	Method
[	O
reference	O
]	O
.	O
section	O
:	O
Pattern	Task
Extraction	Task
We	O
also	O
describe	O
a	O
method	O
for	O
AEG	Task
using	O
patterns	O
over	O
words	O
and	O
part	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
tags	O
,	O
extracting	O
known	O
incorrect	O
sequences	O
from	O
a	O
corpus	O
of	O
annotated	O
corrections	O
.	O
This	O
approach	O
is	O
based	O
on	O
the	O
best	O
method	O
identified	O
by	O
,	O
using	O
error	Method
type	Method
distributions	Method
;	O
while	O
they	O
covered	O
only	O
5	O
error	O
types	O
,	O
we	O
relax	O
this	O
restriction	O
and	O
learn	O
patterns	O
for	O
generating	O
all	O
types	O
of	O
errors	O
.	O
The	O
original	O
and	O
corrected	O
sentences	O
in	O
the	O
corpus	O
are	O
aligned	O
and	O
used	O
to	O
identify	O
short	O
transformation	O
patterns	O
in	O
the	O
form	O
of	O
(	O
incorrect	O
phrase	O
,	O
correct	O
phrase	O
)	O
.	O
The	O
length	O
of	O
each	O
pattern	O
is	O
the	O
affected	O
phrase	O
,	O
plus	O
up	O
to	O
one	O
token	O
of	O
context	O
on	O
both	O
sides	O
.	O
If	O
a	O
word	O
form	O
changes	O
between	O
the	O
incorrect	O
and	O
correct	O
text	O
,	O
it	O
is	O
fully	O
saved	O
in	O
the	O
pattern	O
,	O
otherwise	O
the	O
POS	O
tags	O
are	O
used	O
for	O
matching	Task
.	O
For	O
example	O
,	O
the	O
original	O
sentence	O
'	O
We	O
went	O
shop	O
on	O
Saturday	O
'	O
and	O
the	O
corrected	O
version	O
'	O
We	O
went	O
shopping	O
on	O
Saturday	O
'	O
would	O
produce	O
the	O
following	O
pattern	O
:	O
(	O
VVD	O
shop	O
VV0	O
II	O
,	O
VVD	O
shopping	O
VVG	O
II	O
)	O
After	O
collecting	O
statistics	O
from	O
the	O
background	O
corpus	O
,	O
errors	O
can	O
be	O
inserted	O
into	O
error	O
-	O
free	O
text	O
.	O
The	O
learned	O
patterns	O
are	O
now	O
reversed	O
,	O
looking	O
for	O
the	O
correct	O
side	O
of	O
the	O
tuple	O
in	O
the	O
input	O
sentence	O
.	O
We	O
only	O
use	O
patterns	O
with	O
frequency	O
>	O
=	O
5	O
,	O
which	O
yields	O
a	O
total	O
of	O
35	O
,	O
625	O
patterns	O
from	O
our	O
training	O
data	O
.	O
For	O
each	O
input	O
sentence	O
,	O
we	O
first	O
decide	O
how	O
many	O
errors	O
will	O
be	O
generated	O
(	O
using	O
probabilities	O
from	O
the	O
background	O
corpus	O
)	O
and	O
attempt	O
to	O
create	O
them	O
by	O
sampling	O
from	O
the	O
collection	O
of	O
applicable	O
patterns	O
.	O
This	O
process	O
is	O
repeated	O
until	O
all	O
the	O
required	O
errors	O
have	O
been	O
generated	O
or	O
the	O
sentence	O
is	O
exhausted	O
.	O
During	O
generation	Task
,	O
we	O
try	O
to	O
balance	O
the	O
distribution	O
of	O
error	O
types	O
as	O
well	O
as	O
keeping	O
the	O
same	O
proportion	O
of	O
incorrect	O
and	O
correct	O
sentences	O
as	O
in	O
the	O
background	O
corpus	O
[	O
reference	O
]	O
.	O
The	O
required	O
POS	O
tags	O
were	O
generated	O
with	O
RASP	Method
[	O
reference	O
]	O
,	O
using	O
the	O
CLAWS2	Method
tagset	Method
.	O
section	O
:	O
Error	Method
Detection	Method
Model	Method
We	O
construct	O
a	O
neural	Method
sequence	Method
labeling	Method
model	Method
for	O
error	Task
detection	Task
,	O
following	O
the	O
previous	O
work	O
[	O
reference	O
][	O
reference	O
]	O
.	O
The	O
model	O
receives	O
a	O
sequence	O
of	O
tokens	O
as	O
input	O
and	O
outputs	O
a	O
prediction	O
for	O
each	O
position	O
,	O
indicating	O
whether	O
the	O
token	O
is	O
correct	O
or	O
incorrect	O
in	O
the	O
current	O
context	O
.	O
The	O
tokens	O
are	O
first	O
mapped	O
to	O
a	O
distributed	O
vector	O
space	O
,	O
resulting	O
in	O
a	O
sequence	O
of	O
word	Method
embeddings	Method
.	O
Next	O
,	O
the	O
embeddings	O
are	O
given	O
as	O
input	O
to	O
a	O
bidirectional	Method
LSTM	Method
[	O
reference	O
]	O
,	O
in	O
order	O
to	O
create	O
context	Method
-	Method
dependent	Method
representations	Method
for	O
every	O
token	O
.	O
The	O
hidden	O
states	O
from	O
forward	Method
-	Method
and	Method
backward	Method
-	Method
LSTMs	Method
are	O
concatenated	O
for	O
each	O
word	O
position	O
,	O
resulting	O
in	O
representations	O
that	O
are	O
conditioned	O
on	O
the	O
whole	O
sequence	O
.	O
This	O
concatenated	O
vector	O
is	O
then	O
passed	O
through	O
an	O
additional	O
feedforward	Method
layer	Method
,	O
and	O
a	O
softmax	O
over	O
the	O
two	O
possible	O
labels	O
(	O
correct	O
and	O
incorrect	O
)	O
is	O
used	O
to	O
output	O
a	O
probability	O
distribution	O
for	O
each	O
token	O
.	O
The	O
model	O
is	O
optimised	O
by	O
minimising	O
categorical	Metric
cross	Metric
-	Metric
entropy	Metric
with	O
respect	O
to	O
the	O
correct	O
labels	O
.	O
We	O
use	O
AdaDelta	Method
(	O
Zeiler	O
,	O
2012	O
)	O
for	O
calculating	O
an	O
adaptive	Metric
learning	Metric
rate	Metric
during	O
training	Task
,	O
which	O
accounts	O
for	O
a	O
higher	O
baseline	O
performance	O
compared	O
to	O
previous	O
results	O
.	O
section	O
:	O
Evaluation	O
We	O
trained	O
our	O
error	Method
generation	Method
models	Method
on	O
the	O
public	O
FCE	Material
training	Material
set	Material
[	O
reference	O
]	O
and	O
used	O
them	O
to	O
generate	O
additional	O
artificial	O
training	O
data	O
.	O
Grammatically	O
correct	O
text	O
is	O
needed	O
as	O
the	O
starting	O
point	O
for	O
inserting	O
artificial	O
errors	O
,	O
and	O
we	O
used	O
two	O
different	O
sources	O
:	O
1	O
)	O
the	O
corrected	O
version	O
of	O
the	O
same	O
FCE	Material
training	O
set	O
on	O
which	O
the	O
system	O
is	O
trained	O
(	O
450	O
K	O
tokens	O
)	O
,	O
and	O
2	O
)	O
example	O
sentences	O
extracted	O
from	O
the	O
English	O
Vocabulary	O
Profile	O
(	O
270	O
K	O
tokens	O
)	O
.	O
1	O
.	O
While	O
there	O
are	O
other	O
text	O
corpora	O
that	O
could	O
be	O
used	O
(	O
e.g.	O
,	O
Wikipedia	O
and	O
news	O
articles	O
)	O
,	O
our	O
development	O
experiments	O
showed	O
that	O
keeping	O
the	O
writing	O
style	O
and	O
vocabulary	O
close	O
to	O
the	O
target	O
domain	O
gives	O
better	O
results	O
compared	O
to	O
simply	O
including	O
more	O
data	O
.	O
We	O
evaluated	O
our	O
detection	Method
models	Method
on	O
three	O
benchmarks	O
:	O
the	O
FCE	Material
test	O
data	O
(	O
41	O
K	O
tokens	O
)	O
and	O
the	O
two	O
alternative	O
annotations	O
of	O
the	O
CoNLL	Material
2014	Material
Shared	Material
Task	Material
dataset	Material
(	O
30	O
K	O
tokens	O
)	O
[	O
reference	O
]	O
.	O
Each	O
artificial	Method
error	Method
generation	Method
system	Method
was	O
used	O
to	O
generate	O
3	O
different	O
versions	O
of	O
the	O
artificial	O
data	O
,	O
which	O
were	O
then	O
combined	O
with	O
the	O
original	O
annotated	O
dataset	O
and	O
used	O
for	O
training	O
an	O
error	Method
detection	Method
system	Method
.	O
Table	O
1	O
contains	O
example	O
sentences	O
from	O
the	O
error	Method
generation	Method
systems	Method
,	O
highlighting	O
each	O
of	O
the	O
edits	O
that	O
are	O
marked	O
as	O
errors	O
.	O
The	O
error	Metric
detection	Metric
results	O
can	O
be	O
seen	O
in	O
Table	O
2	O
.	O
We	O
use	O
F	Metric
0.5	Metric
as	O
the	O
main	O
evaluation	Metric
measure	Metric
,	O
which	O
was	O
established	O
as	O
the	O
preferred	O
measure	O
for	O
error	Task
correction	Task
and	O
detection	Task
by	O
the	O
CoNLL	Task
-	Task
14	Task
shared	Task
task	Task
[	O
reference	O
]	O
.	O
F	Metric
0.5	Metric
calculates	O
a	O
weighted	Metric
harmonic	Metric
mean	Metric
of	Metric
precision	Metric
and	Metric
recall	Metric
,	O
which	O
assigns	O
twice	O
as	O
much	O
importance	O
to	O
precision	Metric
-	O
this	O
is	O
motivated	O
by	O
practical	O
applications	O
,	O
where	O
accurate	O
predictions	O
from	O
an	O
error	Method
detection	Method
system	Method
are	O
more	O
important	O
compared	O
to	O
coverage	Metric
.	O
For	O
comparison	O
,	O
we	O
also	O
report	O
the	O
performance	O
of	O
the	O
error	Method
detection	Method
system	Method
by	O
[	O
reference	O
]	O
,	O
trained	O
using	O
the	O
same	O
FCE	Material
dataset	Material
.	O
The	O
results	O
show	O
that	O
error	Metric
detection	Metric
performance	O
is	O
substantially	O
improved	O
by	O
making	O
use	O
of	O
artificially	O
generated	O
data	O
,	O
created	O
by	O
any	O
of	O
the	O
described	O
methods	O
.	O
When	O
comparing	O
the	O
error	Method
generation	Method
system	Method
by	O
(	O
FY14	O
)	O
with	O
our	O
pattern	Method
-	Method
based	Method
(	O
PAT	Method
)	O
and	O
machine	Method
translation	Method
(	O
MT	Method
)	O
approaches	O
,	O
we	O
see	O
that	O
the	O
latter	O
methods	O
covering	O
all	O
error	O
types	O
consistently	O
improve	O
performance	O
.	O
While	O
the	O
added	O
error	O
types	O
tend	O
to	O
be	O
less	O
frequent	O
and	O
more	O
complicated	O
to	O
capture	O
,	O
the	O
added	O
coverage	O
is	O
indeed	O
beneficial	O
for	O
error	Task
detection	Task
.	O
Combining	O
the	O
patternbased	Method
approach	Method
with	O
the	O
machine	Method
translation	Method
system	Method
(	O
Ann	Method
+	O
PAT	Method
+	O
MT	Method
)	O
gave	O
the	O
best	O
overall	O
performance	O
on	O
all	O
datasets	O
.	O
The	O
two	O
frameworks	O
learn	O
to	O
generate	O
different	O
types	O
of	O
errors	O
,	O
and	O
taking	O
advantage	O
of	O
both	O
leads	O
to	O
substantial	O
improvements	O
in	O
error	Task
detection	Task
.	O
We	O
used	O
the	O
Approximate	Method
Randomisation	Method
Test	Method
[	O
reference	O
][	O
reference	O
]	O
Table	O
2	O
:	O
Error	Metric
detection	Metric
performance	O
when	O
combining	O
manually	O
annotated	O
and	O
artificial	O
training	O
data	O
.	O
for	O
each	O
of	O
the	O
systems	O
using	O
artificial	O
data	O
was	O
significant	O
over	O
using	O
only	O
manual	O
annotation	O
.	O
In	O
addition	O
,	O
the	O
final	O
combination	Method
system	Method
is	O
also	O
significantly	O
better	O
compared	O
to	O
the	O
system	O
,	O
on	O
all	O
three	O
datasets	O
.	O
While	O
[	O
reference	O
]	O
also	O
report	O
separate	O
experiments	O
that	O
achieve	O
even	O
higher	O
performance	O
,	O
these	O
models	O
were	O
trained	O
on	O
a	O
considerably	O
larger	O
proprietary	O
corpus	O
.	O
In	O
this	O
paper	O
we	O
compare	O
error	Method
detection	Method
frameworks	Method
trained	O
on	O
the	O
same	O
publicly	O
available	O
FCE	Material
dataset	Material
,	O
thereby	O
removing	O
the	O
confounding	O
factor	O
of	O
dataset	O
size	O
and	O
only	O
focusing	O
on	O
the	O
model	Method
architectures	Method
.	O
The	O
error	Method
generation	Method
methods	Method
can	O
generate	O
alternative	O
versions	O
of	O
the	O
same	O
input	O
text	O
-	O
the	O
pattern	Method
-	Method
based	Method
method	Method
randomly	O
samples	O
the	O
error	O
locations	O
,	O
and	O
the	O
SMT	Method
system	Method
can	O
provide	O
an	O
n	O
-	O
best	O
list	O
of	O
alternative	O
translations	O
.	O
Therefore	O
,	O
we	O
also	O
investigated	O
the	O
combination	O
of	O
multiple	O
error	O
-	O
generated	O
versions	O
of	O
the	O
input	O
files	O
when	O
training	O
error	Method
detection	Method
models	Method
.	O
Figure	O
1	O
shows	O
the	O
F	Metric
0.5	Metric
score	O
on	O
the	O
development	O
set	O
,	O
as	O
the	O
training	O
data	O
is	O
increased	O
by	O
using	O
more	O
translations	O
from	O
the	O
n	O
-	O
best	O
list	O
of	O
the	O
SMT	Method
system	Method
.	O
These	O
results	O
reveal	O
that	O
allowing	O
the	O
model	O
to	O
see	O
multiple	O
alternative	O
versions	O
of	O
the	O
same	O
file	O
gives	O
a	O
distinct	O
improvement	O
-	O
showing	O
the	O
model	O
both	O
correct	O
and	O
incorrect	O
variations	O
of	O
the	O
same	O
sentences	O
likely	O
assists	O
in	O
learning	O
a	O
discriminative	Method
model	Method
.	O
section	O
:	O
Related	O
Work	O
Our	O
work	O
builds	O
on	O
prior	O
research	O
into	O
AEG	Task
.	O
[	O
reference	O
]	O
constructed	O
regular	O
expressions	O
for	O
transforming	O
correct	O
sentences	O
to	O
contain	O
noun	O
number	O
errors	O
.	O
[	O
reference	O
]	O
learned	O
confusion	O
sets	O
from	O
an	O
annotated	O
corpus	O
in	O
order	O
to	O
generate	O
preposition	O
errors	O
.	O
Foster	O
and	O
Andersen	O
(	O
2009	O
)	O
devised	O
a	O
tool	O
for	O
generating	Task
errors	Task
for	O
different	O
types	O
using	O
patterns	O
provided	O
by	O
the	O
user	O
or	O
collected	O
automatically	O
from	O
an	O
annotated	O
corpus	O
.	O
However	O
,	O
their	O
method	O
uses	O
a	O
limited	O
number	O
of	O
edit	Method
operations	Method
and	O
is	O
thus	O
unable	O
to	O
generate	O
complex	O
errors	O
.	O
[	O
reference	O
]	O
compared	O
different	O
training	O
methodologies	O
and	O
showed	O
that	O
artificial	O
errors	O
helped	O
correct	O
prepositions	O
.	O
learned	O
error	Method
type	Method
distributions	Method
for	O
generating	O
five	O
types	O
of	O
errors	O
,	O
and	O
the	O
system	O
in	O
Section	O
2.2	O
is	O
an	O
extension	O
of	O
this	O
model	O
.	O
While	O
previous	O
work	O
focused	O
on	O
generating	O
a	O
specific	O
subset	O
of	O
error	O
types	O
,	O
we	O
explored	O
two	O
holistic	O
approaches	O
to	O
AEG	Task
and	O
showed	O
that	O
they	O
are	O
able	O
to	O
significantly	O
improve	O
error	Task
detection	Task
performance	O
.	O
section	O
:	O
Conclusion	O
This	O
paper	O
investigated	O
two	O
AEG	Method
methods	Method
,	O
in	O
order	O
to	O
create	O
additional	O
training	O
data	O
for	O
error	Task
detection	Task
.	O
First	O
,	O
we	O
explored	O
a	O
method	O
using	O
textual	O
patterns	O
learned	O
from	O
an	O
annotated	O
corpus	O
,	O
which	O
are	O
used	O
for	O
inserting	O
errors	O
into	O
correct	O
input	O
text	O
.	O
In	O
addition	O
,	O
we	O
proposed	O
formulating	O
error	Task
generation	Task
as	O
an	O
MT	Method
framework	O
,	O
learning	O
to	O
translate	O
from	O
grammatically	O
correct	O
to	O
incorrect	O
sentences	O
.	O
The	O
addition	O
of	O
artificial	O
data	O
to	O
the	O
training	O
process	O
was	O
evaluated	O
on	O
three	O
error	Metric
detection	Metric
annotations	Metric
,	O
using	O
the	O
FCE	Material
and	O
CoNLL	Material
2014	Material
datasets	Material
.	O
Making	O
use	O
of	O
artificial	O
data	O
provided	O
improvements	O
for	O
all	O
data	Method
generation	Method
methods	Method
.	O
By	O
relaxing	O
the	O
type	O
restrictions	O
and	O
generating	O
all	O
types	O
of	O
errors	O
,	O
our	O
pattern	Method
-	Method
based	Method
method	Method
consistently	O
outperformed	O
the	O
system	O
by	O
.	O
The	O
combination	O
of	O
the	O
pattern	Method
-	Method
based	Method
method	Method
with	O
the	O
machine	Method
translation	Method
approach	O
gave	O
further	O
substantial	O
improvements	O
and	O
the	O
best	O
performance	O
on	O
all	O
datasets	O
.	O
section	O
:	O
