document O
: O
Mastering O
Chess Task
and O
Shogi Task
by O
Self O
- O
Play O
with O
a O
General Method
Reinforcement Method
Learning Method
Algorithm Method
The O
game Task
of Task
chess Task
is O
the O
most O
widely O
- O
studied O
domain O
in O
the O
history O
of O
artificial Task
intelligence Task
. O
The O
strongest O
programs O
are O
based O
on O
a O
combination O
of O
sophisticated O
search Method
techniques Method
, O
domain Method
- Method
specific Method
adaptations Method
, O
and O
handcrafted Method
evaluation Method
functions Method
that O
have O
been O
refined O
by O
human O
experts O
over O
several O
decades O
. O
In O
contrast O
, O
the O
AlphaGo Method
Zero Method
program Method
recently O
achieved O
superhuman O
performance O
in O
the O
game O
of O
Go Task
, O
by O
tabula Method
rasa Method
reinforcement Method
learning Method
from O
games O
of O
self Task
- Task
play Task
. O
In O
this O
paper O
, O
we O
generalise O
this O
approach O
into O
a O
single O
AlphaZero Method
algorithm O
that O
can O
achieve O
, O
tabula Method
rasa Method
, O
superhuman O
performance O
in O
many O
challenging O
domains O
. O
Starting O
from O
random O
play O
, O
and O
given O
no O
domain O
knowledge O
except O
the O
game O
rules O
, O
AlphaZero Method
achieved O
within O
24 O
hours O
a O
superhuman O
level O
of O
play O
in O
the O
games O
of O
chess Task
and O
shogi Task
( O
Japanese Task
chess Task
) O
as O
well O
as O
Go Task
, O
and O
convincingly O
defeated O
a O
world Method
- Method
champion Method
program Method
in O
each O
case O
. O
The O
study O
of O
computer Task
chess Task
is O
as O
old O
as O
computer Task
science Task
itself O
. O
Babbage O
, O
Turing O
, O
Shannon Method
, O
and O
von Method
Neumann Method
devised O
hardware Method
, O
algorithms O
and O
theory O
to O
analyse O
and O
play O
the O
game Task
of Task
chess Task
. O
Chess Task
subsequently O
became O
the O
grand O
challenge O
task O
for O
a O
generation O
of O
artificial Task
intelligence Task
researchers Task
, O
culminating O
in O
high O
- O
performance O
computer Task
chess Task
programs Task
that O
perform O
at O
superhuman O
level O
. O
However O
, O
these O
systems O
are O
highly O
tuned O
to O
their O
domain O
, O
and O
can O
not O
be O
generalised O
to O
other O
problems O
without O
significant O
human O
effort O
. O
A O
long O
- O
standing O
ambition O
of O
artificial Task
intelligence Task
has O
been O
to O
create O
programs O
that O
can O
instead O
learn O
for O
themselves O
from O
first O
principles O
. O
Recently O
, O
the O
AlphaGo Method
Zero Method
algorithm O
achieved O
superhuman O
performance O
in O
the O
game O
of O
Go Task
, O
by O
representing O
Go Task
knowledge O
using O
deep Method
convolutional Method
neural Method
networks Method
, O
trained O
solely O
by O
reinforcement Method
learning Method
from O
games Method
of Method
self Method
- Method
play Method
. O
In O
this O
paper O
, O
we O
apply O
a O
similar O
but O
fully O
generic O
algorithm O
, O
which O
we O
call O
AlphaZero Method
, O
to O
the O
games O
of O
chess Task
and O
shogi Task
as O
well O
as O
Go Task
, O
without O
any O
additional O
domain O
knowledge O
except O
the O
rules O
of O
the O
game O
, O
demonstrating O
that O
a O
general Method
- Method
purpose Method
reinforcement Method
learning Method
algorithm Method
can O
achieve O
, O
tabula Method
rasa Method
, O
superhuman O
performance O
across O
many O
challenging O
domains O
. O
A O
landmark O
for O
artificial Task
intelligence Task
was O
achieved O
in O
1997 O
when O
Deep O
Blue O
defeated O
the O
human O
world O
champion O
. O
Computer Task
chess Task
programs Task
continued O
to O
progress O
steadily O
beyond O
human O
level O
in O
the O
following O
two O
decades O
. O
These O
programs O
evaluate O
positions O
using O
features O
handcrafted O
by O
human O
grandmasters O
and O
carefully O
tuned O
weights O
, O
combined O
with O
a O
high O
- O
performance O
alpha Method
- Method
beta Method
search Method
that O
expands O
a O
vast O
search O
tree O
using O
a O
large O
number O
of O
clever O
heuristics O
and O
domain O
- O
specific O
adaptations O
. O
In O
the O
Methods O
we O
describe O
these O
augmentations O
, O
focusing O
on O
the O
2016 O
Top O
Chess O
Engine O
Championship O
( O
TCEC O
) O
world O
- O
champion O
Stockfish O
; O
other O
strong O
chess Method
programs Method
, O
including O
Deep Method
Blue Method
, O
use O
very O
similar O
architectures O
. O
Shogi Task
is O
a O
significantly O
harder O
game O
, O
in O
terms O
of O
computational Metric
complexity Metric
, O
than O
chess Task
: O
it O
is O
played O
on O
a O
larger O
board O
, O
and O
any O
captured O
opponent O
piece O
changes O
sides O
and O
may O
subsequently O
be O
dropped O
anywhere O
on O
the O
board O
. O
The O
strongest O
shogi Task
programs O
, O
such O
as O
Computer O
Shogi Task
Association O
( O
CSA Method
) O
world O
- O
champion O
Elmo O
, O
have O
only O
recently O
defeated O
human O
champions O
. O
These O
programs O
use O
a O
similar O
algorithm O
to O
computer Method
chess Method
programs Method
, O
again O
based O
on O
a O
highly O
optimised O
alpha Method
- Method
beta Method
search Method
engine Method
with O
many O
domain O
- O
specific O
adaptations O
. O
Go Task
is O
well O
suited O
to O
the O
neural Method
network Method
architecture Method
used O
in O
AlphaGo Method
because O
the O
rules O
of O
the O
game O
are O
translationally O
invariant O
( O
matching O
the O
weight O
sharing O
structure O
of O
convolutional Method
networks Method
) O
, O
are O
defined O
in O
terms O
of O
liberties O
corresponding O
to O
the O
adjacencies O
between O
points O
on O
the O
board O
( O
matching O
the O
local Method
structure Method
of Method
convolutional Method
networks Method
) O
, O
and O
are O
rotationally O
and O
reflectionally O
symmetric O
( O
allowing O
for O
data Method
augmentation Method
and O
ensembling Method
) O
. O
Furthermore O
, O
the O
action O
space O
is O
simple O
( O
a O
stone O
may O
be O
placed O
at O
each O
possible O
location O
) O
, O
and O
the O
game O
outcomes O
are O
restricted O
to O
binary O
wins O
or O
losses O
, O
both O
of O
which O
may O
help O
neural Method
network Method
training Method
. O
Chess Task
and O
shogi Task
are O
, O
arguably O
, O
less O
innately O
suited O
to O
AlphaGo Method
’s Method
neural Method
network Method
architectures Method
. O
The O
rules O
are O
position O
- O
dependent O
( O
e.g. O
pawns O
may O
move O
two O
steps O
forward O
from O
the O
second O
rank O
and O
promote O
on O
the O
eighth O
rank O
) O
and O
asymmetric O
( O
e.g. O
pawns O
only O
move O
forward O
, O
and O
castling O
is O
different O
on O
kingside O
and O
queenside O
) O
. O
The O
rules O
include O
long O
- O
range O
interactions O
( O
e.g. O
the O
queen O
may O
traverse O
the O
board O
in O
one O
move O
, O
or O
checkmate O
the O
king O
from O
the O
far O
side O
of O
the O
board O
) O
. O
The O
action O
space O
for O
chess O
includes O
all O
legal O
destinations O
for O
all O
of O
the O
players O
’ O
pieces O
on O
the O
board O
; O
shogi Task
also O
allows O
captured O
pieces O
to O
be O
placed O
back O
on O
the O
board O
. O
Both O
chess O
and O
shogi Task
may O
result O
in O
draws O
in O
addition O
to O
wins O
and O
losses O
; O
indeed O
it O
is O
believed O
that O
the O
optimal O
solution O
to O
chess Task
is O
a O
draw O
. O
The O
AlphaZero Method
algorithm O
is O
a O
more O
generic O
version O
of O
the O
AlphaGo Method
Zero Method
algorithm O
that O
was O
first O
introduced O
in O
the O
context O
of O
Go Task
. O
It O
replaces O
the O
handcrafted O
knowledge O
and O
domain Method
- Method
specific Method
augmentations Method
used O
in O
traditional O
game Method
- Method
playing Method
programs Method
with O
deep Method
neural Method
networks Method
and O
a O
tabula Method
rasa Method
reinforcement Method
learning Method
algorithm Method
. O
Instead O
of O
a O
handcrafted O
evaluation O
function O
and O
move Method
ordering Method
heuristics Method
, O
AlphaZero Method
utilises O
a O
deep Method
neural Method
network Method
with O
parameters O
. O
This O
neural Method
network Method
takes O
the O
board O
position O
as O
an O
input O
and O
outputs O
a O
vector O
of O
move O
probabilities O
with O
components O
for O
each O
action O
, O
and O
a O
scalar O
value O
estimating O
the O
expected O
outcome O
from O
position O
, O
. O
AlphaZero Method
learns O
these O
move O
probabilities O
and O
value Method
estimates Method
entirely O
from O
self O
- O
play O
; O
these O
are O
then O
used O
to O
guide O
its O
search O
. O
Instead O
of O
an O
alpha Method
- Method
beta Method
search Method
with O
domain O
- O
specific O
enhancements O
, O
AlphaZero Method
uses O
a O
general Method
- Method
purpose Method
Monte Method
- Method
Carlo Method
tree Method
search Method
( O
MCTS Method
) Method
algorithm Method
. O
Each O
search O
consists O
of O
a O
series O
of O
simulated Method
games Method
of O
self O
- O
play O
that O
traverse O
a O
tree O
from O
root O
to O
leaf O
. O
Each O
simulation O
proceeds O
by O
selecting O
in O
each O
state O
a O
move O
with O
low O
visit O
count O
, O
high O
move O
probability O
and O
high O
value O
( O
averaged O
over O
the O
leaf O
states O
of O
simulations O
that O
selected O
from O
) O
according O
to O
the O
current O
neural Method
network Method
. O
The O
search O
returns O
a O
vector O
representing O
a O
probability O
distribution O
over O
moves O
, O
either O
proportionally O
or O
greedily O
with O
respect O
to O
the O
visit O
counts O
at O
the O
root O
state O
. O
The O
parameters O
of O
the O
deep Method
neural Method
network Method
in O
AlphaZero Method
are O
trained O
by O
self Method
- Method
play Method
reinforcement Method
learning Method
, O
starting O
from O
randomly O
initialised O
parameters O
. O
Games O
are O
played O
by O
selecting O
moves O
for O
both O
players O
by O
MCTS Method
, O
. O
At O
the O
end O
of O
the O
game O
, O
the O
terminal O
position O
is O
scored O
according O
to O
the O
rules O
of O
the O
game O
to O
compute O
the O
game O
outcome O
: O
for O
a O
loss O
, O
for O
a O
draw O
, O
and O
for O
a O
win O
. O
The O
neural Method
network Method
parameters Method
are O
updated O
so O
as O
to O
minimise O
the O
error O
between O
the O
predicted O
outcome O
and O
the O
game O
outcome O
, O
and O
to O
maximise O
the O
similarity O
of O
the O
policy O
vector O
to O
the O
search O
probabilities O
. O
Specifically O
, O
the O
parameters O
are O
adjusted O
by O
gradient Method
descent Method
on O
a O
loss Method
function Method
that O
sums O
over O
mean Metric
- Metric
squared Metric
error Metric
and O
cross Metric
- Metric
entropy Metric
losses Metric
respectively O
, O
where O
is O
a O
parameter O
controlling O
the O
level O
of O
weight Method
regularisation Method
. O
The O
updated O
parameters O
are O
used O
in O
subsequent O
games Task
of Task
self Task
- Task
play Task
. O
The O
AlphaZero Method
algorithm O
described O
in O
this O
paper O
differs O
from O
the O
original O
AlphaGo Method
Zero Method
algorithm O
in O
several O
respects O
. O
AlphaGo Method
Zero Method
estimates O
and O
optimises O
the O
probability O
of O
winning O
, O
assuming O
binary O
win O
/ O
loss O
outcomes O
. O
AlphaZero Method
instead O
estimates O
and O
optimises O
the O
expected O
outcome O
, O
taking O
account O
of O
draws O
or O
potentially O
other O
outcomes O
. O
The O
rules O
of O
Go Task
are O
invariant O
to O
rotation O
and O
reflection O
. O
This O
fact O
was O
exploited O
in O
AlphaGo Method
and O
AlphaGo Method
Zero Method
in O
two O
ways O
. O
First O
, O
training O
data O
was O
augmented O
by O
generating O
8 O
symmetries O
for O
each O
position O
. O
Second O
, O
during O
MCTS Method
, O
board O
positions O
were O
transformed O
using O
a O
randomly O
selected O
rotation O
or O
reflection O
before O
being O
evaluated O
by O
the O
neural Method
network Method
, O
so O
that O
the O
Monte Method
- Method
Carlo Method
evaluation Method
is O
averaged O
over O
different O
biases O
. O
The O
rules O
of O
chess Task
and O
shogi Task
are O
asymmetric O
, O
and O
in O
general O
symmetries O
can O
not O
be O
assumed O
. O
AlphaZero Method
does O
not O
augment O
the O
training O
data O
and O
does O
not O
transform O
the O
board O
position O
during O
MCTS Method
. O
In O
AlphaGo Method
Zero Method
, O
self O
- O
play O
games O
were O
generated O
by O
the O
best O
player O
from O
all O
previous O
iterations O
. O
After O
each O
iteration O
of O
training O
, O
the O
performance O
of O
the O
new O
player O
was O
measured O
against O
the O
best O
player O
; O
if O
it O
won O
by O
a O
margin O
of O
then O
it O
replaced O
the O
best O
player O
and O
self O
- O
play O
games O
were O
subsequently O
generated O
by O
this O
new O
player O
. O
In O
contrast O
, O
AlphaZero Method
simply O
maintains O
a O
single O
neural Method
network Method
that O
is O
updated O
continually O
, O
rather O
than O
waiting O
for O
an O
iteration O
to O
complete O
. O
Self Task
- Task
play Task
games Task
are O
generated O
by O
using O
the O
latest O
parameters O
for O
this O
neural Method
network Method
, O
omitting O
the O
evaluation O
step O
and O
the O
selection O
of O
best O
player O
. O
AlphaGo Method
Zero Method
tuned O
the O
hyper O
- O
parameter O
of O
its O
search O
by O
Bayesian Method
optimisation Method
. O
In O
AlphaZero Method
we O
reuse O
the O
same O
hyper O
- O
parameters O
for O
all O
games O
without O
game O
- O
specific O
tuning O
. O
The O
sole O
exception O
is O
the O
noise O
that O
is O
added O
to O
the O
prior O
policy O
to O
ensure O
exploration Task
; O
this O
is O
scaled O
in O
proportion O
to O
the O
typical O
number O
of O
legal O
moves O
for O
that O
game O
type O
. O
Like O
AlphaGo Method
Zero Method
, O
the O
board O
state O
is O
encoded O
by O
spatial O
planes O
based O
only O
on O
the O
basic O
rules O
for O
each O
game O
. O
The O
actions O
are O
encoded O
by O
either O
spatial O
planes O
or O
a O
flat O
vector O
, O
again O
based O
only O
on O
the O
basic O
rules O
for O
each O
game O
( O
see O
Methods O
) O
. O
We O
applied O
the O
AlphaZero Method
algorithm O
to O
chess Task
, O
shogi Task
, O
and O
also O
Go Task
. O
Unless O
otherwise O
specified O
, O
the O
same O
algorithm O
settings O
, O
network O
architecture O
, O
and O
hyper O
- O
parameters O
were O
used O
for O
all O
three O
games O
. O
We O
trained O
a O
separate O
instance O
of O
AlphaZero Method
for O
each O
game O
. O
Training O
proceeded O
for O
700 O
, O
000 O
steps O
( O
mini O
- O
batches O
of O
size O
4 O
, O
096 O
) O
starting O
from O
randomly O
initialised O
parameters O
, O
using O
5 O
, O
000 O
first O
- O
generation O
TPUs Method
to O
generate O
self Task
- Task
play Task
games Task
and O
64 O
second O
- O
generation O
TPUs Method
to O
train O
the O
neural Method
networks Method
. O
Further O
details O
of O
the O
training O
procedure O
are O
provided O
in O
the O
Methods O
. O
Figure O
[ O
reference O
] O
shows O
the O
performance O
of O
AlphaZero Method
during O
self Method
- Method
play Method
reinforcement Method
learning Method
, O
as O
a O
function O
of O
training O
steps O
, O
on O
an O
Elo O
scale O
. O
In O
chess Task
, O
AlphaZero Method
outperformed O
Stockfish Method
after O
just O
4 O
hours O
( O
300k O
steps O
) O
; O
in O
shogi Task
, O
AlphaZero Method
outperformed O
Elmo Method
after O
less O
than O
2 O
hours O
( O
110k O
steps O
) O
; O
and O
in O
Go Task
, O
AlphaZero Method
outperformed O
AlphaGo Method
Lee Method
after O
8 O
hours O
( O
165k O
steps O
) O
. O
We O
evaluated O
the O
fully O
trained O
instances O
of O
AlphaZero Method
against O
Stockfish Method
, O
Elmo Method
and O
the O
previous O
version O
of O
AlphaGo Method
Zero Method
( O
trained O
for O
3 O
days O
) O
in O
chess Task
, O
shogi Task
and O
Go Task
respectively O
, O
playing O
100 O
game O
matches O
at O
tournament O
time O
controls O
of O
one O
minute O
per O
move O
. O
AlphaZero Method
and O
the O
previous O
AlphaGo Method
Zero Method
used O
a O
single O
machine O
with O
4 O
TPUs O
. O
Stockfish Method
and O
Elmo Method
played O
at O
their O
strongest O
skill O
level O
using O
64 O
threads O
and O
a O
hash O
size O
of O
1 O
GB O
. O
AlphaZero Method
convincingly O
defeated O
all O
opponents O
, O
losing O
zero O
games O
to O
Stockfish O
and O
eight O
games O
to O
Elmo O
( O
see O
Supplementary O
Material O
for O
several O
example O
games O
) O
, O
as O
well O
as O
defeating O
the O
previous O
version O
of O
AlphaGo Method
Zero Method
( O
see O
Table O
[ O
reference O
] O
) O
. O
We O
also O
analysed O
the O
relative O
performance O
of O
AlphaZero Method
’s O
MCTS O
search O
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
alpha Method
- Method
beta Method
search Method
engines Method
used O
by O
Stockfish Method
and O
Elmo Method
. O
AlphaZero Method
searches O
just O
80 O
thousand O
positions O
per O
second O
in O
chess Task
and O
40 O
thousand O
in O
shogi Task
, O
compared O
to O
70 O
million O
for O
Stockfish Method
and O
35 O
million O
for O
Elmo Method
. O
AlphaZero Method
compensates O
for O
the O
lower O
number O
of O
evaluations O
by O
using O
its O
deep Method
neural Method
network Method
to O
focus O
much O
more O
selectively O
on O
the O
most O
promising O
variations O
– O
arguably O
a O
more O
“ O
human O
- O
like O
” O
approach O
to O
search Task
, O
as O
originally O
proposed O
by O
Shannon Method
. O
Figure O
[ O
reference O
] O
shows O
the O
scalability O
of O
each O
player O
with O
respect O
to O
thinking O
time O
, O
measured O
on O
an O
Elo O
scale O
, O
relative O
to O
Stockfish Method
or O
Elmo Method
with O
40ms O
thinking O
time O
. O
AlphaZero Method
’s O
MCTS O
scaled O
more O
effectively O
with O
thinking O
time O
than O
either O
Stockfish Method
or O
Elmo Method
, O
calling O
into O
question O
the O
widely O
held O
belief O
that O
alpha Method
- Method
beta Method
search Method
is O
inherently O
superior O
in O
these O
domains O
. O
Finally O
, O
we O
analysed O
the O
chess O
knowledge O
discovered O
by O
AlphaZero Method
. O
Table O
[ O
reference O
] O
analyses O
the O
most O
common O
human O
openings O
( O
those O
played O
more O
than O
100 O
, O
000 O
times O
in O
an O
online O
database O
of O
human O
chess O
games O
) O
. O
Each O
of O
these O
openings O
is O
independently O
discovered O
and O
played O
frequently O
by O
AlphaZero Method
during O
self Task
- Task
play Task
training Task
. O
When O
starting O
from O
each O
human O
opening O
, O
AlphaZero Method
convincingly O
defeated O
Stockfish Method
, O
suggesting O
that O
it O
has O
indeed O
mastered O
a O
wide O
spectrum O
of O
chess Task
play Task
. O
The O
game Task
of Task
chess Task
represented O
the O
pinnacle O
of O
AI Task
research Task
over O
several O
decades O
. O
State O
- O
of O
- O
the O
- O
art O
programs O
are O
based O
on O
powerful O
engines O
that O
search O
many O
millions O
of O
positions O
, O
leveraging O
handcrafted O
domain O
expertise O
and O
sophisticated O
domain Method
adaptations Method
. O
AlphaZero Method
is O
a O
generic O
reinforcement Method
learning Method
algorithm Method
– O
originally O
devised O
for O
the O
game O
of O
Go Task
– O
that O
achieved O
superior O
results O
within O
a O
few O
hours O
, O
searching O
a O
thousand O
times O
fewer O
positions O
, O
given O
no O
domain O
knowledge O
except O
the O
rules O
of O
chess O
. O
Furthermore O
, O
the O
same O
algorithm O
was O
applied O
without O
modification O
to O
the O
more O
challenging O
game O
of O
shogi Task
, O
again O
outperforming O
the O
state O
of O
the O
art O
within O
a O
few O
hours O
. O
bibliography O
: O
References O
section O
: O
Methods O
subsection O
: O
Anatomy O
of O
a O
Computer Task
Chess Task
Program Task
In O
this O
section O
we O
describe O
the O
components O
of O
a O
typical O
computer Method
chess Method
program Method
, O
focusing O
specifically O
on O
Stockfish Method
, O
an O
open O
source O
program O
that O
won O
the O
2016 O
TCEC O
computer O
chess O
championship O
. O
For O
an O
overview O
of O
standard O
methods O
, O
see O
. O
Each O
position O
is O
described O
by O
a O
sparse O
vector O
of O
handcrafted O
features O
, O
including O
midgame O
/ O
endgame O
- O
specific O
material O
point O
values O
, O
material O
imbalance O
tables O
, O
piece O
- O
square O
tables O
, O
mobility O
and O
trapped O
pieces O
, O
pawn O
structure O
, O
king O
safety O
, O
outposts O
, O
bishop O
pair O
, O
and O
other O
miscellaneous O
evaluation O
patterns O
. O
Each O
feature O
is O
assigned O
, O
by O
a O
combination O
of O
manual Method
and Method
automatic Method
tuning Method
, O
a O
corresponding O
weight O
and O
the O
position O
is O
evaluated O
by O
a O
linear Method
combination Method
. O
However O
, O
this O
raw O
evaluation O
is O
only O
considered O
accurate O
for O
positions O
that O
are O
“ O
quiet O
” O
, O
with O
no O
unresolved O
captures O
or O
checks O
. O
A O
domain Method
- Method
specialised Method
quiescence Method
search Method
is O
used O
to O
resolve O
ongoing O
tactical O
situations O
before O
the O
evaluation O
function O
is O
applied O
. O
The O
final O
evaluation O
of O
a O
position O
is O
computed O
by O
a O
minimax Method
search Method
that O
evaluates O
each O
leaf O
using O
a O
quiescence Method
search Method
. O
Alpha Method
- Method
beta Method
pruning Method
is O
used O
to O
safely O
cut O
any O
branch O
that O
is O
provably O
dominated O
by O
another O
variation O
. O
Additional O
cuts O
are O
achieved O
using O
aspiration O
windows O
and O
principal Method
variation Method
search Method
. O
Other O
pruning Method
strategies Method
include O
null Method
move Method
pruning Method
( O
which O
assumes O
a O
pass O
move O
should O
be O
worse O
than O
any O
variation O
, O
in O
positions O
that O
are O
unlikely O
to O
be O
in O
zugzwang O
, O
as O
determined O
by O
simple O
heuristics Method
) O
, O
futility Method
pruning Method
( O
which O
assumes O
knowledge O
of O
the O
maximum O
possible O
change O
in O
evaluation O
) O
, O
and O
other O
domain Method
- Method
dependent Method
pruning Method
rules Method
( O
which O
assume O
knowledge O
of O
the O
value O
of O
captured O
pieces O
) O
. O
The O
search O
is O
focused O
on O
promising O
variations O
both O
by O
extending O
the O
search O
depth O
of O
promising O
variations O
, O
and O
by O
reducing O
the O
search O
depth O
of O
unpromising O
variations O
based O
on O
heuristics Method
like O
history O
, O
static Method
- Method
exchange Method
evaluation Method
( O
SEE O
) O
, O
and O
moving Method
piece Method
type Method
. O
Extensions O
are O
based O
on O
domain Method
- Method
independent Method
rules Method
that O
identify O
singular O
moves O
with O
no O
sensible O
alternative O
, O
and O
domain O
- O
dependent O
rules O
, O
such O
as O
extending O
check O
moves O
. O
Reductions Method
, O
such O
as O
late Method
move Method
reductions Method
, O
are O
based O
heavily O
on O
domain O
knowledge O
. O
The O
efficiency O
of O
alpha Method
- Method
beta Method
search Method
depends O
critically O
upon O
the O
order O
in O
which O
moves O
are O
considered O
. O
Moves O
are O
therefore O
ordered O
by O
iterative Method
deepening Method
( O
using O
a O
shallower O
search O
to O
order O
moves O
for O
a O
deeper O
search O
) O
. O
In O
addition O
, O
a O
combination O
of O
domain Method
- Method
independent Method
move Method
ordering Method
heuristics Method
, O
such O
as O
killer Method
heuristic Method
, O
history Method
heuristic Method
, O
counter Method
- Method
move Method
heuristic Method
, O
and O
also O
domain O
- O
dependent O
knowledge O
based O
on O
captures O
( O
SEE O
) O
and O
potential Method
captures Method
( O
MVV Method
/ Method
LVA Method
) O
. O
A O
transposition O
table O
facilitates O
the O
reuse O
of O
values O
and O
move O
orders O
when O
the O
same O
position O
is O
reached O
by O
multiple O
paths O
. O
A O
carefully O
tuned O
opening O
book O
is O
used O
to O
select O
moves O
at O
the O
start O
of O
the O
game O
. O
An O
endgame O
tablebase O
, O
precalculated O
by O
exhaustive Method
retrograde Method
analysis Method
of O
endgame O
positions O
, O
provides O
the O
optimal O
move O
in O
all O
positions O
with O
six O
and O
sometimes O
seven O
pieces O
or O
less O
. O
Other O
strong O
chess Method
programs Method
, O
and O
also O
earlier O
programs O
such O
as O
Deep Method
Blue Method
, O
have O
used O
very O
similar O
architectures O
including O
the O
majority O
of O
the O
components O
described O
above O
, O
although O
important O
details O
vary O
considerably O
. O
None O
of O
the O
techniques O
described O
in O
this O
section O
are O
used O
by O
AlphaZero Method
. O
It O
is O
likely O
that O
some O
of O
these O
techniques O
could O
further O
improve O
the O
performance O
of O
AlphaZero Method
; O
however O
, O
we O
have O
focused O
on O
a O
pure O
self Method
- Method
play Method
reinforcement Method
learning Method
approach Method
and O
leave O
these O
extensions O
for O
future O
research O
. O
subsection O
: O
Prior O
Work O
on O
Computer Task
Chess Task
and O
Shogi Task
In O
this O
section O
we O
discuss O
some O
notable O
prior O
work O
on O
reinforcement Task
learning Task
in O
computer Task
chess Task
. O
NeuroChess Method
evaluated O
positions O
by O
a O
neural Method
network Method
that O
used O
175 O
handcrafted O
input O
features O
. O
It O
was O
trained O
by O
temporal Method
- Method
difference Method
learning Method
to O
predict O
the O
final O
game O
outcome O
, O
and O
also O
the O
expected O
features O
after O
two O
moves O
. O
NeuroChess Method
won O
13 O
% O
of O
games O
against O
GnuChess Method
using O
a O
fixed O
depth Method
2 Method
search Method
. O
Beal O
and O
Smith O
applied O
temporal Method
- Method
difference Method
learning Method
to O
estimate O
the O
piece O
values O
in O
chess Task
and O
shogi Task
, O
starting O
from O
random O
values O
and O
learning O
solely O
by O
self O
- O
play O
. O
KnightCap O
evaluated O
positions O
by O
a O
neural Method
network Method
that O
used O
an O
attack O
- O
table O
based O
on O
knowledge O
of O
which O
squares O
are O
attacked O
or O
defended O
by O
which O
pieces O
. O
It O
was O
trained O
by O
a O
variant O
of O
temporal Method
- Method
difference Method
learning Method
, O
known O
as O
TD Method
( Method
leaf Method
) Method
, O
that O
updates O
the O
leaf O
value O
of O
the O
principal O
variation O
of O
an O
alpha Method
- Method
beta Method
search Method
. O
KnightCap O
achieved O
human O
master O
level O
after O
training O
against O
a O
strong O
computer O
opponent O
with O
hand O
- O
initialised O
piece O
- O
value O
weights O
. O
Meep O
evaluated O
positions O
by O
a O
linear Method
evaluation Method
function Method
based O
on O
handcrafted O
features O
. O
It O
was O
trained O
by O
another O
variant O
of O
temporal Method
- Method
difference Method
learning Method
, O
known O
as O
TreeStrap Method
, O
that O
updated O
all O
nodes O
of O
an O
alpha Method
- Method
beta Method
search Method
. O
Meep O
defeated O
human O
international O
master O
players O
in O
13 O
out O
of O
15 O
games O
, O
after O
training O
by O
self O
- O
play O
with O
randomly O
initialised O
weights O
. O
Kaneko O
and O
Hoki O
trained O
the O
weights O
of O
a O
shogi Task
evaluation O
function O
comprising O
a O
million O
features O
, O
by O
learning O
to O
select O
expert O
human O
moves O
during O
alpha O
- O
beta O
serach O
. O
They O
also O
performed O
a O
large Task
- Task
scale Task
optimization Task
based O
on O
minimax Method
search Method
regulated O
by O
expert O
game O
logs O
; O
this O
formed O
part O
of O
the O
Bonanza Method
engine Method
that O
won O
the O
2013 O
World O
Computer O
Shogi Task
Championship O
. O
Giraffe O
evaluated O
positions O
by O
a O
neural Method
network Method
that O
included O
mobility O
maps O
and O
attack Method
and Method
defend Method
maps Method
describing O
the O
lowest O
valued O
attacker O
and O
defender O
of O
each O
square O
. O
It O
was O
trained O
by O
self O
- O
play O
using O
TD Method
( Method
leaf Method
) Method
, O
also O
reaching O
a O
standard O
of O
play O
comparable O
to O
international O
masters O
. O
DeepChess Method
trained O
a O
neural Method
network Method
to O
performed O
pair O
- O
wise O
evaluations O
of O
positions O
. O
It O
was O
trained O
by O
supervised Method
learning Method
from O
a O
database O
of O
human O
expert O
games O
that O
was O
pre O
- O
filtered O
to O
avoid O
capture O
moves O
and O
drawn O
games O
. O
DeepChess O
reached O
a O
strong O
grandmaster O
level O
of O
play O
. O
All O
of O
these O
programs O
combined O
their O
learned O
evaluation Method
functions Method
with O
an O
alpha Method
- Method
beta Method
search Method
enhanced O
by O
a O
variety O
of O
extensions O
. O
An O
approach O
based O
on O
training O
dual Method
policy Method
and Method
value Method
networks Method
using O
AlphaZero Method
- O
like O
policy O
iteration O
was O
successfully O
applied O
to O
improve O
on O
the O
state O
- O
of O
- O
the O
- O
art O
in O
Hex Task
. O
subsection O
: O
MCTS Method
and O
Alpha Method
- Method
Beta Method
Search Method
For O
at O
least O
four O
decades O
the O
strongest O
computer Method
chess Method
programs Method
have O
used O
alpha Method
- Method
beta Method
search Method
. O
AlphaZero Method
uses O
a O
markedly O
different O
approach O
that O
averages O
over O
the O
position O
evaluations O
within O
a O
subtree O
, O
rather O
than O
computing O
the O
minimax O
evaluation O
of O
that O
subtree O
. O
However O
, O
chess Method
programs Method
using O
traditional O
MCTS Method
were O
much O
weaker O
than O
alpha Method
- Method
beta Method
search Method
programs Method
, O
; O
while O
alpha Method
- Method
beta Method
programs Method
based O
on O
neural Method
networks Method
have O
previously O
been O
unable O
to O
compete O
with O
faster O
, O
handcrafted O
evaluation O
functions O
. O
AlphaZero Method
evaluates O
positions O
using O
non Method
- Method
linear Method
function Method
approximation Method
based O
on O
a O
deep Method
neural Method
network Method
, O
rather O
than O
the O
linear Method
function Method
approximation Method
used O
in O
typical O
chess Method
programs Method
. O
This O
provides O
a O
much O
more O
powerful O
representation O
, O
but O
may O
also O
introduce O
spurious O
approximation O
errors O
. O
MCTS Method
averages O
over O
these O
approximation O
errors O
, O
which O
therefore O
tend O
to O
cancel O
out O
when O
evaluating O
a O
large O
subtree O
. O
In O
contrast O
, O
alpha Method
- Method
beta Method
search Method
computes O
an O
explicit O
minimax Method
, O
which O
propagates O
the O
biggest O
approximation O
errors O
to O
the O
root O
of O
the O
subtree O
. O
Using O
MCTS Method
may O
allow O
AlphaZero Method
to O
effectively O
combine O
its O
neural Method
network Method
representations Method
with O
a O
powerful O
, O
domain Method
- Method
independent Method
search Method
. O
subsection O
: O
Domain O
Knowledge O
The O
input O
features O
describing O
the O
position O
, O
and O
the O
output O
features O
describing O
the O
move O
, O
are O
structured O
as O
a O
set O
of O
planes O
; O
i.e. O
the O
neural Method
network Method
architecture Method
is O
matched O
to O
the O
grid O
- O
structure O
of O
the O
board O
. O
AlphaZero Method
is O
provided O
with O
perfect O
knowledge O
of O
the O
game O
rules O
. O
These O
are O
used O
during O
MCTS Method
, O
to O
simulate O
the O
positions O
resulting O
from O
a O
sequence O
of O
moves O
, O
to O
determine O
game O
termination O
, O
and O
to O
score O
any O
simulations O
that O
reach O
a O
terminal O
state O
. O
Knowledge O
of O
the O
rules O
is O
also O
used O
to O
encode O
the O
input O
planes O
( O
i.e. O
castling O
, O
repetition O
, O
no O
- O
progress O
) O
and O
output O
planes O
( O
how O
pieces O
move O
, O
promotions O
, O
and O
piece O
drops O
in O
shogi Task
) O
. O
The O
typical O
number O
of O
legal O
moves O
is O
used O
to O
scale O
the O
exploration O
noise O
( O
see O
below O
) O
. O
Chess Task
and O
shogi Task
games O
exceeding O
a O
maximum O
number O
of O
steps O
( O
determined O
by O
typical O
game O
length O
) O
were O
terminated O
and O
assigned O
a O
drawn O
outcome O
; O
Go Task
games O
were O
terminated O
and O
scored O
with O
Tromp Method
- Method
Taylor Method
rules Method
, O
similarly O
to O
previous O
work O
. O
AlphaZero Method
did O
not O
use O
any O
form O
of O
domain O
knowledge O
beyond O
the O
points O
listed O
above O
. O
subsection O
: O
Representation O
In O
this O
section O
we O
describe O
the O
representation O
of O
the O
board O
inputs O
, O
and O
the O
representation O
of O
the O
action O
outputs O
, O
used O
by O
the O
neural Method
network Method
in O
AlphaZero Method
. O
Other O
representations O
could O
have O
been O
used O
; O
in O
our O
experiments O
the O
training Method
algorithm Method
worked O
robustly O
for O
many O
reasonable O
choices O
. O
The O
input O
to O
the O
neural Method
network Method
is O
an O
image O
stack O
that O
represents O
state O
using O
a O
concatenation O
of O
sets O
of O
planes O
of O
size O
. O
Each O
set O
of O
planes O
represents O
the O
board O
position O
at O
a O
time O
- O
step O
, O
and O
is O
set O
to O
zero O
for O
time O
- O
steps O
less O
than O
1 O
. O
The O
board O
is O
oriented O
to O
the O
perspective O
of O
the O
current O
player O
. O
The O
feature O
planes O
are O
composed O
of O
binary O
feature O
planes O
indicating O
the O
presence O
of O
the O
player O
’s O
pieces O
, O
with O
one O
plane O
for O
each O
piece O
type O
, O
and O
a O
second O
set O
of O
planes O
indicating O
the O
presence O
of O
the O
opponent O
’s O
pieces O
. O
For O
shogi Task
there O
are O
additional O
planes O
indicating O
the O
number O
of O
captured O
prisoners O
of O
each O
type O
. O
There O
are O
an O
additional O
constant O
- O
valued O
input O
planes O
denoting O
the O
player O
’s O
colour O
, O
the O
total O
move O
count O
, O
and O
the O
state O
of O
special O
rules O
: O
the O
legality O
of O
castling O
in O
chess O
( O
kingside O
or O
queenside O
) O
; O
the O
repetition O
count O
for O
that O
position O
( O
3 O
repetitions O
is O
an O
automatic O
draw O
in O
chess O
; O
4 O
in O
shogi Task
) O
; O
and O
the O
number O
of O
moves O
without O
progress O
in O
chess O
( O
50 O
moves O
without O
progress O
is O
an O
automatic O
draw O
) O
. O
Input O
features O
are O
summarised O
in O
Table O
[ O
reference O
] O
. O
A O
move O
in O
chess Task
may O
be O
described O
in O
two O
parts O
: O
selecting O
the O
piece O
to O
move O
, O
and O
then O
selecting O
among O
the O
legal O
moves O
for O
that O
piece O
. O
We O
represent O
the O
policy O
by O
a O
stack Method
of Method
planes Method
encoding O
a O
probability O
distribution O
over O
4 O
, O
672 O
possible O
moves O
. O
Each O
of O
the O
positions O
identifies O
the O
square O
from O
which O
to O
“ O
pick O
up O
” O
a O
piece O
. O
The O
first O
56 O
planes O
encode O
possible O
‘ O
queen O
moves O
’ O
for O
any O
piece O
: O
a O
number O
of O
squares O
in O
which O
the O
piece O
will O
be O
moved O
, O
along O
one O
of O
eight O
relative O
compass O
directions O
. O
The O
next O
8 O
planes O
encode O
possible O
knight O
moves O
for O
that O
piece O
. O
The O
final O
9 O
planes O
encode O
possible O
underpromotions O
for O
pawn O
moves O
or O
captures O
in O
two O
possible O
diagonals O
, O
to O
knight O
, O
bishop O
or O
rook O
respectively O
. O
Other O
pawn O
moves O
or O
captures O
from O
the O
seventh O
rank O
are O
promoted O
to O
a O
queen O
. O
The O
policy O
in O
shogi Task
is O
represented O
by O
a O
stack Method
of Method
planes Method
similarly O
encoding O
a O
probability O
distribution O
over O
11 O
, O
259 O
possible O
moves O
. O
The O
first O
64 O
planes O
encode O
‘ O
queen O
moves O
’ O
and O
the O
next O
2 O
moves O
encode O
knight O
moves O
. O
An O
additional O
planes O
encode O
promoting O
queen O
moves O
and O
promoting O
knight O
moves O
respectively O
. O
The O
last O
7 O
planes O
encode O
a O
captured O
piece O
dropped O
back O
into O
the O
board O
at O
that O
location O
. O
The O
policy O
in O
Go Task
is O
represented O
identically O
to O
AlphaGo Method
Zero Method
, O
using O
a O
flat O
distribution O
over O
moves O
representing O
possible O
stone O
placements O
and O
the O
pass O
move O
. O
We O
also O
tried O
using O
a O
flat O
distribution O
over O
moves O
for O
chess Task
and O
shogi Task
; O
the O
final O
result O
was O
almost O
identical O
although O
training O
was O
slightly O
slower O
. O
The O
action Method
representations Method
are O
summarised O
in O
Table O
[ O
reference O
] O
. O
Illegal O
moves O
are O
masked O
out O
by O
setting O
their O
probabilities O
to O
zero O
, O
and O
re O
- O
normalising O
the O
probabilities O
for O
remaining O
moves O
. O
subsection O
: O
Configuration O
During O
training O
, O
each O
MCTS Method
used O
800 O
simulations O
. O
The O
number O
of O
games O
, O
positions O
, O
and O
thinking O
time O
varied O
per O
game O
due O
largely O
to O
different O
board O
sizes O
and O
game O
lengths O
, O
and O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
The O
learning Metric
rate Metric
was O
set O
to O
0.2 O
for O
each O
game O
, O
and O
was O
dropped O
three O
times O
( O
to O
0.02 O
, O
0.002 O
and O
0.0002 O
respectively O
) O
during O
the O
course O
of O
training O
. O
Moves O
are O
selected O
in O
proportion O
to O
the O
root O
visit O
count O
. O
Dirichlet O
noise O
was O
added O
to O
the O
prior O
probabilities O
in O
the O
root O
node O
; O
this O
was O
scaled O
in O
inverse O
proportion O
to O
the O
approximate O
number O
of O
legal O
moves O
in O
a O
typical O
position O
, O
to O
a O
value O
of O
for O
chess Task
, O
shogi Task
and O
Go Task
respectively O
. O
Unless O
otherwise O
specified O
, O
the O
training Method
and Method
search Method
algorithm Method
and O
parameters O
are O
identical O
to O
AlphaGo Method
Zero Method
. O
During O
evaluation Task
, O
AlphaZero Method
selects O
moves O
greedily O
with O
respect O
to O
the O
root O
visit O
count O
. O
Each O
MCTS Method
was O
executed O
on O
a O
single O
machine O
with O
4 O
TPUs O
. O
subsection O
: O
Evaluation O
To O
evaluate O
performance O
in O
chess Task
, O
we O
used O
Stockfish Method
version Method
8 O
( O
official O
Linux O
release O
) O
as O
a O
baseline O
program O
, O
using O
64 O
CPU O
threads O
and O
a O
hash O
size O
of O
1 O
GB O
. O
To O
evaluate O
performance O
in O
shogi Task
, O
we O
used O
Elmo Method
version Method
WCSC27 Method
in O
combination O
with O
YaneuraOu O
2017 O
Early O
KPPT Method
4.73 O
64AVX2 O
with O
64 O
CPU O
threads O
and O
a O
hash O
size O
of O
1 O
GB O
with O
the O
usi O
option O
of O
EnteringKingRule O
set O
to O
NoEnteringKing O
. O
We O
evaluated O
the O
relative O
strength O
of O
AlphaZero Method
( O
Figure O
[ O
reference O
] O
) O
by O
measuring O
the O
Elo Metric
rating Metric
of O
each O
player O
. O
We O
estimate O
the O
probability O
that O
player O
will O
defeat O
player O
by O
a O
logistic Method
function Method
, O
and O
estimate O
the O
ratings O
by O
Bayesian Method
logistic Method
regression Method
, O
computed O
by O
the O
BayesElo Method
program Method
using O
the O
standard O
constant O
. O
Elo Metric
ratings Metric
were O
computed O
from O
the O
results O
of O
a O
1 O
second O
per O
move O
tournament O
between O
iterations O
of O
AlphaZero Method
during O
training O
, O
and O
also O
a O
baseline O
player O
: O
either O
Stockfish Method
, O
Elmo Method
or O
AlphaGo Method
Lee Method
respectively O
. O
The O
Elo Metric
rating Metric
of O
the O
baseline O
players O
was O
anchored O
to O
publicly O
available O
values O
. O
We O
also O
measured O
the O
head O
- O
to O
- O
head O
performance O
of O
AlphaZero Method
against O
each O
baseline O
player O
. O
Settings O
were O
chosen O
to O
correspond O
with O
computer O
chess O
tournament O
conditions O
: O
each O
player O
was O
allowed O
1 O
minute O
per O
move O
, O
resignation O
was O
enabled O
for O
all O
players O
( O
- O
900 O
centipawns O
for O
10 O
consecutive O
moves O
for O
Stockfish O
and O
Elmo O
, O
5 O
% O
winrate O
for O
AlphaZero Method
) O
. O
Pondering O
was O
disabled O
for O
all O
players O
. O
subsection O
: O
Example O
games O
In O
this O
section O
we O
include O
10 O
example O
games O
played O
by O
AlphaZero Method
against O
Stockfish O
during O
the O
100 O
game O
match O
using O
1 O
minute O
per O
move O
. O
