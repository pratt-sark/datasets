In	O
this	O
paper	O
we	O
introduce	O
a	O
simple	O
approach	O
for	O
exploration	Task
in	Task
reinforcement	Task
learning	Task
(	Task
RL	Task
)	O
that	O
allows	O
us	O
to	O
develop	O
theoretically	O
justified	Method
algorithms	Method
in	O
the	O
tabular	Task
case	Task
but	O
that	O
is	O
also	O
extendable	O
to	O
settings	O
where	O
function	Method
approximation	Method
is	O
required	O
.	O
Our	O
approach	O
is	O
based	O
on	O
the	O
successor	Method
representation	Method
(	O
SR	Method
)	O
,	O
which	O
was	O
originally	O
introduced	O
as	O
a	O
representation	O
defining	O
state	Task
generalization	Task
by	O
the	O
similarity	O
of	O
successor	O
states	O
.	O
Here	O
we	O
show	O
that	O
the	O
norm	O
of	O
the	O
SR	Method
,	O
while	O
it	O
is	O
being	O
learned	O
,	O
can	O
be	O
used	O
as	O
a	O
reward	O
bonus	O
to	O
incentivize	O
exploration	Task
.	O
In	O
order	O
to	O
better	O
understand	O
this	O
transient	O
behavior	O
of	O
the	O
norm	O
of	O
the	O
SR	Method
we	O
introduce	O
the	O
substochastic	Method
successor	Method
representation	Method
(	O
SSR	Method
)	O
and	O
we	O
show	O
that	O
it	O
implicitly	O
counts	O
the	O
number	O
of	O
times	O
each	O
state	O
(	O
or	O
feature	O
)	O
has	O
been	O
observed	O
.	O
We	O
use	O
this	O
result	O
to	O
introduce	O
an	O
algorithm	O
that	O
performs	O
as	O
well	O
as	O
some	O
theoretically	Method
sample	Method
-	Method
efficient	Method
approaches	Method
.	O
Finally	O
,	O
we	O
extend	O
these	O
ideas	O
to	O
a	O
deep	Method
RL	Method
algorithm	Method
and	O
show	O
that	O
it	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
Atari	Material
2600	Material
games	Material
.	O
Count	O
-	O
BasedExplorationwiththeSuccessorRepresentation	O
section	O
:	O
Introduction	O
Reinforcement	Method
learning	Method
(	O
RL	Method
)	O
tackles	O
sequential	Task
decision	Task
making	Task
problems	Task
by	O
formulating	O
them	O
as	O
tasks	O
where	O
an	O
agent	O
must	O
learn	O
how	O
to	O
act	O
optimally	O
through	O
trial	O
and	O
error	O
interactions	O
with	O
the	O
environment	O
.	O
The	O
goal	O
in	O
these	O
problems	O
is	O
to	O
maximize	O
the	O
discounted	O
sum	O
of	O
the	O
numerical	O
reward	O
signal	O
observed	O
at	O
each	O
time	O
step	O
.	O
Because	O
the	O
actions	O
taken	O
by	O
the	O
agent	O
influence	O
not	O
just	O
the	O
immediate	O
reward	O
but	O
also	O
the	O
states	O
and	O
associated	O
rewards	O
in	O
the	O
future	O
,	O
sequential	Task
decision	Task
making	Task
problems	Task
require	O
agents	O
to	O
deal	O
with	O
the	O
trade	O
-	O
off	O
between	O
immediate	O
and	O
delayed	O
rewards	O
.	O
Here	O
we	O
focus	O
on	O
the	O
problem	O
of	O
exploration	Task
in	Task
RL	Task
,	O
which	O
aims	O
to	O
reduce	O
the	O
number	O
of	O
samples	O
(	O
i.e.	O
,	O
interactions	O
)	O
an	O
agent	O
needs	O
in	O
order	O
to	O
learn	O
to	O
perform	O
well	O
in	O
these	O
tasks	O
when	O
the	O
environment	O
is	O
initially	O
unknown	O
.	O
Surprisingly	O
,	O
the	O
most	O
common	O
approach	O
in	O
the	O
field	O
is	O
to	O
select	O
exploratory	O
actions	O
uniformly	O
at	O
random	O
,	O
with	O
even	O
high	O
-	O
profile	O
success	O
stories	O
being	O
obtained	O
with	O
this	O
strategy	O
[	O
e.g.	O
,][]	O
Tesauro95	O
,	O
Mnih15	O
.	O
However	O
,	O
random	Method
exploration	Method
often	O
fails	O
in	O
environments	O
with	O
sparse	O
rewards	O
,	O
that	O
is	O
,	O
environments	O
where	O
the	O
agent	O
observes	O
a	O
reward	O
signal	O
of	O
value	O
zero	O
for	O
the	O
majority	O
of	O
states	O
.	O
In	O
this	O
paper	O
we	O
introduce	O
a	O
novel	O
approach	O
for	O
exploration	Task
in	Task
RL	Task
based	O
on	O
the	O
successor	Method
representation	Method
[	O
SR;	O
][]	O
Dayan93	O
.	O
The	O
SR	Method
is	O
a	O
representation	O
that	O
generalizes	O
between	O
states	O
using	O
the	O
similarity	O
between	O
their	O
successors	O
,	O
that	O
is	O
,	O
the	O
states	O
that	O
follow	O
the	O
current	O
state	O
given	O
the	O
agent	O
’s	O
policy	O
.	O
The	O
SR	Method
is	O
defined	O
for	O
any	O
problem	O
,	O
it	O
can	O
be	O
learned	O
with	O
temporal	Method
-	Method
difference	Method
(	O
TD	Method
)	O
learning	O
Sutton88	O
and	O
,	O
as	O
we	O
discuss	O
below	O
,	O
it	O
can	O
be	O
seen	O
as	O
implicitly	O
estimating	O
the	O
transition	O
dynamics	O
of	O
the	O
environment	O
.	O
The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
to	O
show	O
that	O
the	O
norm	O
of	O
the	O
SR	Method
can	O
be	O
used	O
as	O
an	O
exploration	O
bonus	O
.	O
We	O
perform	O
an	O
extensive	O
empirical	O
evaluation	O
to	O
demonstrate	O
this	O
and	O
we	O
introduce	O
the	O
substochastic	Method
successor	Method
representation	Method
(	O
SSR	Method
)	O
to	O
also	O
understand	O
,	O
theoretically	O
,	O
the	O
behavior	O
of	O
such	O
bonus	O
.	O
The	O
SSR	Method
behaves	O
similarly	O
to	O
the	O
SR	Method
but	O
it	O
is	O
more	O
amenable	O
to	O
theoretical	O
analyses	O
.	O
We	O
show	O
that	O
the	O
SSR	Method
implicitly	O
counts	O
state	O
visitation	O
,	O
suggesting	O
that	O
the	O
exploration	O
bonus	O
obtained	O
from	O
the	O
SR	Method
,	O
while	O
it	O
is	O
being	O
learned	O
,	O
might	O
also	O
be	O
incorporating	O
some	O
notion	O
of	O
state	O
visitation	O
counts	O
.	O
Finally	O
,	O
we	O
extend	O
the	O
idea	O
of	O
using	O
the	O
norm	O
of	O
the	O
SR	Method
as	O
an	O
exploration	O
bonus	O
to	O
the	O
function	Task
approximation	Task
case	Task
,	O
designing	O
a	O
deep	Method
RL	Method
algorithm	Method
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
hard	Task
exploration	Task
Atari	Task
2600	Task
games	Task
when	O
in	O
a	O
low	O
sample	Metric
-	Metric
complexity	Metric
regime	Metric
.	O
Importantly	O
,	O
the	O
proposed	O
algorithm	O
is	O
also	O
simpler	O
than	O
traditional	O
baselines	O
such	O
as	O
pseudo	Method
-	Method
count	Method
-	Method
based	Method
methods	Method
as	O
it	O
does	O
not	O
require	O
domain	Method
-	Method
specific	Method
density	Method
models	Method
Bellemare16	O
,	O
Ostrovski17	O
.	O
section	O
:	O
Preliminaries	O
We	O
consider	O
an	O
agent	O
interacting	O
with	O
its	O
environment	O
in	O
a	O
sequential	O
manner	O
.	O
Starting	O
from	O
a	O
state	O
,	O
at	O
each	O
step	O
the	O
agent	O
takes	O
an	O
action	O
,	O
to	O
which	O
the	O
environment	O
responds	O
with	O
a	O
state	O
according	O
to	O
a	O
transition	O
probability	O
function	O
,	O
and	O
with	O
a	O
reward	O
signal	O
,	O
where	O
indicates	O
the	O
expected	O
reward	O
for	O
a	O
transition	O
from	O
state	O
under	O
action	O
,	O
i.e.	O
,	O
.	O
The	O
value	O
of	O
a	O
state	O
when	O
following	O
a	O
policy	O
,	O
,	O
is	O
defined	O
to	O
be	O
the	O
expected	O
sum	O
of	O
discounted	O
rewards	O
from	O
that	O
state	O
:	O
,	O
with	O
being	O
the	O
discount	O
factor	O
.	O
When	O
the	O
transition	O
probability	O
function	O
and	O
the	O
reward	O
function	O
are	O
known	O
,	O
we	O
can	O
compute	O
recursively	O
by	O
solving	O
the	O
system	O
of	O
equations	O
below	O
Bellman57	O
:	O
These	O
equations	O
can	O
also	O
be	O
written	O
in	O
matrix	O
form	O
with	O
,	O
and	O
:	O
where	O
is	O
the	O
state	O
to	O
state	O
transition	O
probability	O
function	O
induced	O
by	O
,	O
that	O
is	O
,	O
.	O
Traditional	O
model	Method
-	Method
based	Method
algorithms	Method
work	O
by	O
learning	O
estimates	O
of	O
the	O
matrix	O
and	O
of	O
the	O
vector	O
and	O
using	O
them	O
to	O
estimate	O
,	O
for	O
example	O
by	O
solving	O
Equation	O
[	O
reference	O
]	O
.	O
We	O
use	O
and	O
to	O
denote	O
empirical	O
estimates	O
of	O
and	O
.	O
Formally	O
,	O
where	O
denotes	O
the	O
-	O
th	O
entry	O
in	O
the	O
vector	O
,	O
is	O
the	O
number	O
of	O
times	O
the	O
transition	O
was	O
observed	O
,	O
,	O
and	O
is	O
the	O
sum	O
of	O
the	O
rewards	O
associated	O
with	O
the	O
transitions	O
(	O
we	O
drop	O
the	O
action	O
to	O
simplify	O
notation	O
)	O
.	O
However	O
,	O
model	Method
-	Method
based	Method
approaches	Method
are	O
rarely	O
successful	O
in	O
problems	O
with	O
large	O
state	O
spaces	O
due	O
to	O
the	O
difficulty	O
in	O
learning	Task
accurate	Task
models	Task
.	O
Because	O
of	O
the	O
challenges	O
in	O
model	Task
learning	Task
,	O
model	Method
-	Method
free	Method
solutions	Method
largely	O
dominate	O
the	O
literature	O
.	O
In	O
model	Task
-	Task
free	Task
RL	Task
,	O
instead	O
of	O
estimating	O
and	O
we	O
estimate	O
directly	O
from	O
samples	O
.	O
We	O
often	O
use	O
TD	Method
learning	Method
Sutton88	O
to	O
update	O
our	O
estimates	O
of	O
,	O
,	O
online	O
:	O
where	O
is	O
the	O
step	O
-	O
size	O
parameter	O
.	O
Generalization	Task
is	O
required	O
in	O
problems	O
with	O
large	O
state	O
spaces	O
,	O
where	O
it	O
is	O
unfeasible	O
to	O
learn	O
an	O
individual	O
value	O
for	O
each	O
state	O
.	O
We	O
do	O
so	O
by	O
parametrizing	O
with	O
a	O
set	O
of	O
weights	O
.	O
We	O
write	O
,	O
given	O
the	O
weights	O
,	O
and	O
,	O
where	O
.	O
Model	Method
-	Method
free	Method
methods	Method
have	O
performed	O
well	O
in	O
problems	O
with	O
large	O
state	O
spaces	O
,	O
mainly	O
due	O
to	O
the	O
use	O
of	O
neural	Method
networks	Method
as	O
function	Method
approximators	Method
[	O
e.g.	O
,][]	O
Mnih15	O
.	O
The	O
ideas	O
presented	O
here	O
are	O
based	O
on	O
the	O
successor	Method
representation	Method
[	O
SR;	O
][]	O
Dayan93	O
.	O
The	O
successor	Method
representation	Method
with	O
respect	O
to	O
a	O
policy	O
,	O
,	O
is	O
defined	O
as	O
where	O
we	O
assume	O
the	O
sum	O
is	O
convergent	O
with	O
denoting	O
the	O
indicator	O
function	O
.	O
This	O
expectation	O
can	O
actually	O
be	O
estimated	O
from	O
samples	O
with	O
TD	Method
learning	Method
:	O
for	O
all	O
and	O
denoting	O
the	O
step	O
-	O
size	O
.	O
The	O
SR	Method
also	O
corresponds	O
to	O
the	O
Neumann	O
series	O
of	O
:	O
Notice	O
that	O
the	O
SR	Method
is	O
part	O
of	O
the	O
solution	O
when	O
computing	O
a	O
value	O
function	O
:	O
(	O
Eq	O
.	O
[	O
reference	O
]	O
)	O
.	O
We	O
use	O
to	O
denote	O
the	O
SR	Method
computed	O
through	O
,	O
the	O
approximation	Method
of	Method
.	O
The	O
definition	O
of	O
the	O
SR	Method
can	O
also	O
be	O
extended	O
to	O
features	O
.	O
Successor	Method
features	Method
Barreto17	O
generalize	O
the	O
successor	Method
representation	Method
to	O
the	O
function	Task
approximation	Task
setting	Task
.	O
We	O
use	O
the	O
definition	O
for	O
the	O
uncontrolled	Task
case	Task
in	O
this	O
paper	O
.	O
Successor	Method
features	Method
can	O
also	O
be	O
learned	O
with	O
TD	Method
learning	Method
.	O
theorem	O
:	O
(	O
Successor	O
Features	O
)	O
.	O
For	O
a	O
given	O
0≤γ<1	O
,	O
policy	O
π	O
,	O
and	O
for	O
a	O
feature	Method
representation	Method
∈⁢ϕ	O
(	O
s	O
)	O
Rd	O
,	O
the	O
successor	O
features	O
for	O
a	O
state	O
s	O
are	O
:	O
Alternatively	O
,	O
in	O
matrix	O
form	O
,	O
we	O
can	O
write	O
the	O
successor	O
features	O
as	O
,	O
where	O
is	O
a	O
matrix	O
encoding	O
the	O
feature	Method
representation	Method
of	O
each	O
state	O
such	O
that	O
.	O
This	O
definition	O
reduces	O
to	O
the	O
SR	Method
in	O
the	O
tabular	O
case	O
,	O
where	O
.	O
section	O
:	O
as	O
an	O
Exploration	O
Bonus	O
It	O
is	O
now	O
well	O
-	O
known	O
that	O
the	O
SR	Method
incorporates	O
diffusion	O
properties	O
of	O
the	O
environment	O
[	O
e.g.	O
,][]	O
Machado18b	O
,	O
Wu19	O
.	O
These	O
properties	O
can	O
be	O
used	O
to	O
accelerate	O
learning	Task
,	O
for	O
example	O
,	O
with	O
options	O
that	O
promote	O
exploration	Task
Machado18b	O
.	O
Inspired	O
by	O
these	O
results	O
,	O
in	O
this	O
section	O
we	O
argue	O
that	O
the	O
SR	Method
can	O
be	O
used	O
in	O
a	O
more	O
direct	O
way	O
to	O
promote	O
exploration	Task
.	O
We	O
show	O
that	O
the	O
norm	O
of	O
the	O
SR	Method
,	O
while	O
it	O
is	O
being	O
learned	O
,	O
behaves	O
as	O
an	O
exploration	O
bonus	O
that	O
rewards	O
agents	O
for	O
visiting	O
states	O
it	O
has	O
visited	O
less	O
often	O
.	O
We	O
first	O
demonstrate	O
this	O
behavior	O
empirically	O
,	O
in	O
the	O
tabular	O
case	O
,	O
to	O
clearly	O
present	O
the	O
idea	O
behind	O
the	O
introduced	O
algorithm	O
.	O
We	O
then	O
introduce	O
the	O
substochastic	Method
successor	Method
representation	Method
in	O
order	O
to	O
provide	O
some	O
theoretical	O
intuition	O
that	O
justifies	O
this	O
idea	O
.	O
Later	O
we	O
show	O
how	O
these	O
ideas	O
carry	O
over	O
to	O
the	O
function	Task
approximation	Task
setting	Task
.	O
subsection	O
:	O
Empirical	O
Demonstration	O
To	O
demonstrate	O
the	O
usefulness	O
of	O
the	O
norm	O
of	O
the	O
SR	Method
as	O
an	O
exploration	O
bonus	O
we	O
compare	O
the	O
performance	O
of	O
traditional	O
Sarsa	Method
Rummery94	Method
to	O
Sarsa	Method
+	O
SR	Method
,	O
which	O
incorporates	O
the	O
norm	O
of	O
the	O
SR	Method
as	O
an	O
exploration	O
bonus	O
in	O
the	O
Sarsa	Method
update	Method
.	O
The	O
update	Method
equation	Method
for	O
Sarsa	Method
+	O
SR	Method
is	O
where	O
is	O
a	O
scaling	O
factor	O
and	O
,	O
at	O
each	O
time	O
step	O
,	O
is	O
updated	O
before	O
as	O
per	O
Equation	O
4	O
.	O
We	O
evaluted	O
this	O
algorithm	O
in	O
RiverSwim	O
and	O
SixArms	O
Strehl08	O
,	O
traditional	O
domains	O
in	O
the	O
PAC	O
-	O
MDP	O
literature	O
that	O
are	O
used	O
to	O
evaluate	O
provably	Method
sample	Method
-	Method
efficient	Method
algorithms	Method
.	O
In	O
these	O
domains	O
it	O
is	O
very	O
likely	O
that	O
an	O
agent	O
will	O
first	O
observe	O
a	O
small	O
reward	O
generated	O
in	O
a	O
state	O
that	O
is	O
easy	O
to	O
get	O
to	O
.	O
If	O
the	O
agent	O
does	O
not	O
have	O
a	O
good	O
exploration	Method
policy	Method
it	O
is	O
likely	O
to	O
converge	O
to	O
a	O
suboptimal	O
behavior	O
,	O
never	O
observing	O
larger	O
rewards	O
available	O
in	O
states	O
that	O
are	O
difficult	O
to	O
get	O
to	O
.	O
Our	O
results	O
suggest	O
that	O
the	O
proposed	O
exploration	Method
bonus	Method
has	O
a	O
profound	O
impact	O
in	O
the	O
algorithm	O
’s	O
performance	O
.	O
When	O
evaluating	O
the	O
agent	O
for	O
time	O
steps	O
,	O
Sarsa	Method
obtains	O
an	O
average	O
return	O
of	O
approximately	O
,	O
while	O
Sarsa	Method
+	O
SR	Method
obtains	O
an	O
approximate	O
average	Metric
return	Metric
of	O
million	O
!	O
Notice	O
that	O
,	O
in	O
RiverSwim	O
,	O
the	O
reward	O
that	O
is	O
“	O
easy	O
to	O
get	O
”	O
has	O
value	O
,	O
implying	O
that	O
,	O
different	O
from	O
Sarsa	Method
+	O
SR	Method
,	O
Sarsa	Method
almost	O
never	O
explores	O
the	O
state	O
space	O
well	O
enough	O
.	O
In	O
SixArms	O
the	O
trend	O
is	O
the	O
same	O
.	O
Sarsa	Method
obtains	O
an	O
approximate	O
average	Metric
return	Metric
of	O
while	O
Sarsa	Method
+	O
SR	Method
achieves	O
approximately	O
millon	O
.	O
The	O
actual	O
numbers	O
,	O
which	O
were	O
averaged	O
over	O
runs	O
,	O
are	O
available	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Details	O
about	O
parameters	O
as	O
well	O
as	O
the	O
empirical	O
methodology	O
are	O
available	O
in	O
the	O
Appendix	O
.	O
subsection	O
:	O
Theoretical	O
Justification	O
It	O
is	O
difficult	O
to	O
characterize	O
the	O
behavior	O
of	O
our	O
proposed	O
exploration	Method
bonus	Method
because	O
it	O
is	O
updated	O
at	O
each	O
time	O
step	O
with	O
TD	Method
learning	Method
.	O
It	O
is	O
hard	O
to	O
analyze	O
the	O
behavior	O
of	O
estimates	O
obtained	O
with	O
TD	Method
learning	Method
in	O
the	O
interim	O
.	O
Also	O
,	O
at	O
its	O
fixed	O
point	O
,	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
is	O
for	O
all	O
states	O
,	O
making	O
it	O
hard	O
for	O
us	O
to	O
use	O
the	O
fixed	O
point	O
of	O
the	O
SR	Method
to	O
theoretically	O
analyze	O
the	O
behavior	O
of	O
this	O
exploration	O
bonus	O
.	O
In	O
this	O
section	O
we	O
introduce	O
the	O
substochastic	Method
successor	Method
representation	Method
(	O
SSR	Method
)	O
to	O
provide	O
some	O
theoretical	O
intuition	O
of	O
why	O
the	O
norm	O
of	O
the	O
SR	Method
is	O
a	O
good	O
exploration	O
bonus	O
.	O
The	O
SSR	Method
behaves	O
similarly	O
to	O
the	O
SR	Method
but	O
it	O
is	O
simpler	O
to	O
analyze	O
.	O
theorem	O
:	O
(	O
Substochastic	Method
Successor	Method
Representation	Method
)	O
.	O
Let	O
~Pπ	O
denote	O
the	O
substochastic	O
matrix	O
induced	O
by	O
the	O
environment	O
’s	O
dynamics	O
and	O
by	O
the	O
policy	Method
π	Method
such	O
that	O
~Pπ	O
(	O
s′|s	O
)	O
=⁢n	O
(	O
s	O
,	O
s′	O
)+	O
⁢n	O
(	O
s	O
)	O
1	O
.	O
For	O
a	O
given	O
0≤γ<1	O
,	O
the	O
substochastic	Method
successor	Method
representation	Method
,	O
~Ψπ	O
,	O
is	O
defined	O
as	O
:	O
The	O
SSR	Method
only	O
differs	O
from	O
the	O
empirical	O
SR	Method
in	O
its	O
incorporation	O
of	O
an	O
additional	O
“	O
phantom	O
”	O
transition	O
from	O
each	O
state	O
,	O
making	O
it	O
underestimate	O
the	O
real	O
SR	Method
.	O
Through	O
algebraic	O
manipulation	O
we	O
show	O
that	O
the	O
SSR	Method
allows	O
us	O
to	O
recover	O
an	O
estimate	O
of	O
the	O
visit	O
counts	O
,	O
.	O
This	O
result	O
provides	O
some	O
intuition	O
of	O
why	O
the	O
exploration	O
bonus	O
we	O
propose	O
in	O
this	O
paper	O
performs	O
so	O
well	O
,	O
as	O
exploration	O
bonuses	O
based	O
on	O
state	O
visitation	O
counts	O
are	O
known	O
to	O
generate	O
proper	O
exploration	Task
.	O
As	O
aforementioned	O
,	O
the	O
SSR	Method
behaves	O
similarly	O
to	O
the	O
SR	Method
.	O
When	O
computing	O
the	O
norm	O
of	O
the	O
SR	Method
,	O
while	O
it	O
is	O
being	O
learned	O
with	O
TD	Method
learning	Method
,	O
it	O
is	O
as	O
if	O
a	O
reward	O
of	O
1	O
was	O
observed	O
at	O
each	O
time	O
step	O
.	O
Thus	O
,	O
there	O
is	O
little	O
variance	O
in	O
the	O
target	O
,	O
with	O
the	O
predictions	O
slowly	O
approaching	O
the	O
true	O
value	O
of	O
the	O
SR	Method
.	O
If	O
pessimistically	O
initialized	O
,	O
as	O
traditionally	O
done	O
,	O
the	O
estimates	O
of	O
the	O
SR	Method
approach	O
the	O
target	O
from	O
below	O
.	O
In	O
this	O
sense	O
,	O
the	O
number	O
of	O
times	O
a	O
prediction	O
has	O
been	O
updated	O
in	O
a	O
given	O
state	O
is	O
a	O
good	O
proxy	O
to	O
estimate	O
how	O
far	O
this	O
prediction	O
is	O
from	O
its	O
final	O
target	O
.	O
From	O
the	O
definition	O
above	O
we	O
can	O
see	O
that	O
the	O
SSR	Method
have	O
similar	O
properties	O
.	O
It	O
underestimates	O
the	O
true	O
target	O
but	O
slowly	O
approaches	O
it	O
,	O
converging	O
to	O
the	O
true	O
SR	Method
in	O
the	O
limit	O
.	O
The	O
SSR	Method
simplifies	O
the	O
analysis	O
by	O
not	O
taking	O
bootstrapping	O
into	O
consideration	O
.	O
The	O
theorem	O
below	O
formalizes	O
the	O
idea	O
that	O
the	O
norm	O
of	O
the	O
SSR	Method
implicitly	O
counts	O
state	O
visitation	O
,	O
shedding	O
some	O
light	O
on	O
why	O
the	O
exploration	O
bonus	O
we	O
propose	O
works	O
so	O
well	O
.	O
theorem	O
:	O
.	O
Let	O
⁢n	O
(	O
s	O
)	O
denote	O
the	O
number	O
of	O
times	O
state	O
s	O
has	O
been	O
visited	O
and	O
let	O
~Ψπ	O
denote	O
the	O
substochastic	Method
successor	Method
representation	Method
as	O
in	O
Definition	O
.	O
For	O
a	O
given	O
0≤γ<1	O
,	O
proof	O
:	O
Proof	O
of	O
Theorem	O
[	O
reference	O
]	O
.	O
Let	O
be	O
the	O
empirical	O
transition	O
matrix	O
.	O
We	O
first	O
rewrite	O
in	O
terms	O
of	O
:	O
This	O
expression	O
can	O
also	O
be	O
written	O
in	O
matrix	O
form	O
:	O
,	O
where	O
denotes	O
the	O
diagonal	O
matrix	O
of	O
augmented	O
inverse	O
counts	O
.	O
Expanding	O
we	O
have	O
:	O
The	O
top	O
eigenvector	O
of	O
a	O
stochastic	O
matrix	O
is	O
the	O
all	O
-	O
ones	O
vector	O
,	O
meyn12markov	O
,	O
and	O
it	O
corresponds	O
to	O
the	O
eigenvalue	O
1	O
.	O
Using	O
this	O
fact	O
and	O
the	O
definition	O
of	O
with	O
respect	O
to	O
we	O
have	O
:	O
We	O
can	O
now	O
bound	O
the	O
term	O
using	O
the	O
fact	O
that	O
is	O
also	O
the	O
top	O
eigenvector	O
of	O
the	O
successor	Method
representation	Method
and	O
has	O
eigenvalue	O
Machado18b	O
:	O
Plugging	O
(	O
[	O
reference	O
]	O
)	O
into	O
the	O
definition	O
of	O
the	O
SR	Method
we	O
have	O
(	O
notice	O
that	O
)	O
:	O
When	O
we	O
also	O
use	O
the	O
other	O
bound	O
on	O
the	O
quadratic	O
term	O
we	O
conclude	O
that	O
,	O
for	O
any	O
state	O
,	O
∎	O
In	O
other	O
words	O
,	O
the	O
SSR	Method
,	O
obtained	O
after	O
a	O
slight	O
change	O
to	O
the	O
SR	Method
,	O
can	O
be	O
used	O
to	O
recover	O
state	O
visitation	O
counts	O
.	O
The	O
intuition	O
behind	O
this	O
result	O
is	O
that	O
the	O
phantom	O
transition	O
,	O
represented	O
by	O
the	O
in	O
the	O
denominator	O
of	O
the	O
SSR	Method
,	O
serves	O
as	O
a	O
proxy	O
for	O
the	O
uncertainty	O
about	O
that	O
state	O
by	O
underestimating	O
the	O
SR	Method
.	O
This	O
is	O
due	O
to	O
the	O
fact	O
that	O
gets	O
closer	O
to	O
each	O
time	O
state	O
is	O
visited	O
.	O
As	O
a	O
sanity	O
check	O
,	O
we	O
used	O
this	O
result	O
to	O
implement	O
a	O
simple	O
model	Method
-	Method
based	Method
algorithm	Method
that	O
penalizes	O
the	O
agent	O
for	O
visiting	O
commonly	O
visited	O
states	O
with	O
the	O
exploration	O
bonus	O
.	O
Thus	O
,	O
our	O
agent	O
actually	O
maximizes	O
,	O
with	O
being	O
a	O
scaling	O
parameter	O
.	O
The	O
shift	O
in	O
the	O
theorem	O
has	O
no	O
effect	O
in	O
the	O
agent	O
’s	O
policy	O
because	O
it	O
is	O
the	O
same	O
across	O
all	O
states	O
.	O
In	O
this	O
algorithm	O
the	O
agent	O
updates	O
its	O
transition	Method
probability	Method
model	Method
and	O
reward	Method
model	Method
through	O
Equation	O
[	O
reference	O
]	O
and	O
its	O
SSR	Method
estimate	O
as	O
in	O
Definition	O
[	O
reference	O
]	O
.	O
Table	O
[	O
reference	O
]	O
depicts	O
the	O
performance	O
of	O
this	O
algorithm	O
,	O
dubbed	O
ESSR	Method
,	O
as	O
well	O
as	O
the	O
performance	O
of	O
some	O
algorithms	O
with	O
polynomial	Metric
sample	Metric
-	Metric
complexity	Metric
bounds	Metric
.	O
The	O
goal	O
with	O
this	O
evaluation	O
is	O
not	O
to	O
outperform	O
these	O
algorithms	O
,	O
but	O
to	O
evaluate	O
how	O
well	O
ESSR	Method
performs	O
when	O
compared	O
to	O
algorithms	O
that	O
explicitly	O
keep	O
visitation	O
counts	O
to	O
promote	O
exploration	Task
.	O
ESSR	Method
performs	O
as	O
well	O
as	O
R	Method
-	Method
Max	Method
Brafman02	O
and	O
E	O
Kearns02	O
on	O
RiverSwim	O
and	O
it	O
outperforms	O
these	O
algorithms	O
on	O
SixArms	O
;	O
while	O
MBIE	Method
Strehl08	O
,	O
which	O
explicitly	O
estimates	O
confidence	O
intervals	O
over	O
the	O
expected	O
return	O
in	O
each	O
state	O
,	O
outperforms	O
ESSR	Method
in	O
these	O
domains	O
.	O
These	O
results	O
clearly	O
show	O
that	O
ESSR	Method
performs	O
,	O
on	O
average	O
,	O
similarly	O
to	O
other	O
algorithms	O
with	O
PAC	Method
-	Method
MDP	Method
guarantees	Method
,	O
suggesting	O
that	O
the	O
norm	O
of	O
the	O
SSR	Method
is	O
a	O
promising	O
exploration	O
bonus	O
.	O
Additional	O
details	O
about	O
the	O
algorithm	O
and	O
the	O
experimental	O
methodology	O
are	O
available	O
in	O
the	O
Appendix	O
.	O
section	O
:	O
Counting	O
Feature	O
Activations	O
with	O
the	O
SR	Method
In	O
large	O
environments	O
,	O
where	O
enumerating	O
all	O
states	O
is	O
not	O
an	O
option	O
,	O
directly	O
using	O
Sarsa	Method
+	O
SR	Method
as	O
described	O
in	O
the	O
previous	O
section	O
is	O
not	O
viable	O
.	O
Learning	O
the	O
SR	Method
becomes	O
even	O
more	O
challenging	O
when	O
the	O
representation	Task
,	O
,	O
is	O
also	O
being	O
learned	O
.	O
Using	O
neural	Method
networks	Method
to	O
learn	O
a	O
representation	O
while	O
learning	O
to	O
estimate	O
state	Method
-	Method
action	Method
value	Method
function	Method
is	O
the	O
approach	O
that	O
currently	O
often	O
leads	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
the	O
field	O
.	O
In	O
this	O
section	O
we	O
describe	O
an	O
algorithm	O
that	O
uses	O
the	O
same	O
ideas	O
described	O
so	O
far	O
but	O
in	O
the	O
function	Task
approximation	Task
setting	Task
.	O
Our	O
algorithm	O
was	O
inspired	O
by	O
recent	O
work	O
that	O
have	O
shown	O
that	O
successor	O
features	O
can	O
be	O
learned	O
jointly	O
with	O
the	O
feature	Method
representation	Method
itself	O
Kulkarni16	O
,	O
Machado18b	O
.	O
An	O
overview	O
of	O
the	O
neural	Method
network	Method
we	O
used	O
to	O
learn	O
the	O
agent	O
’s	O
value	O
function	O
while	O
also	O
learning	O
the	O
feature	Method
representation	Method
and	O
the	O
SR	Method
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
layers	O
used	O
to	O
compute	O
the	O
state	O
-	O
action	O
value	O
function	O
,	O
,	O
are	O
structured	O
as	O
in	O
DQN	Method
Mnih15	O
,	O
but	O
with	O
different	O
numbers	O
of	O
parameters	O
(	O
i	O
..	O
e	O
,	O
filter	O
sizes	O
,	O
stride	O
,	O
and	O
number	O
of	O
nodes	O
)	O
.	O
This	O
was	O
done	O
to	O
match	O
Oh15	Method
’s	Method
(	O
Oh15	Method
)	O
architecture	O
,	O
which	O
is	O
known	O
to	O
succeed	O
in	O
the	O
auxiliary	Task
task	Task
of	O
predicting	O
the	O
agent	O
’s	O
next	O
observation	O
,	O
which	O
we	O
detail	O
below	O
.	O
From	O
here	O
on	O
,	O
we	O
call	O
the	O
part	O
of	O
our	O
architecture	O
that	O
predicts	O
DQN	Method
to	O
distinguish	O
between	O
the	O
parameters	O
of	O
this	O
network	O
and	O
DQN	Method
.	O
It	O
is	O
trained	O
to	O
minimize	O
with	O
and	O
being	O
defined	O
as	O
This	O
loss	O
is	O
known	O
as	O
the	O
mixed	Method
Monte	Method
-	Method
Carlo	Method
return	Method
(	O
MMC	Method
)	O
and	O
it	O
has	O
been	O
used	O
in	O
the	O
past	O
by	O
the	O
algorithms	O
that	O
achieved	O
succesful	O
exploration	O
in	O
deep	Task
reinforcement	Task
learning	Task
Bellemare16	O
,	O
Ostrovski17	O
.	O
The	O
distinction	O
between	O
and	O
is	O
standard	O
in	O
the	O
field	O
,	O
with	O
denoting	O
the	O
parameters	O
of	O
the	O
target	O
network	O
,	O
which	O
is	O
updated	O
less	O
often	O
for	O
stability	Task
purposes	Task
Mnih15	Task
.	O
As	O
before	O
,	O
we	O
use	O
to	O
denote	O
the	O
exploration	O
bonus	O
obtained	O
from	O
the	O
successor	O
features	O
of	O
the	O
internal	Method
representation	Method
,	O
,	O
which	O
will	O
be	O
defined	O
below	O
.	O
Moreover	O
,	O
to	O
ensure	O
all	O
features	O
are	O
in	O
the	O
same	O
range	O
,	O
we	O
normalize	O
the	O
feature	O
vector	O
so	O
that	O
.	O
In	O
Fig	O
.	O
[	O
reference	O
]	O
we	O
highlight	O
with	O
the	O
layer	O
in	O
which	O
we	O
normalize	O
its	O
output	O
.	O
Notice	O
that	O
the	O
features	O
are	O
always	O
non	O
-	O
negative	O
due	O
to	O
the	O
use	O
of	O
ReLU	O
gates	O
.	O
The	O
successor	O
features	O
,	O
,	O
at	O
the	O
bottom	O
of	O
the	O
diagram	O
,	O
are	O
obtained	O
by	O
minimizing	O
the	O
loss	O
Zero	O
is	O
a	O
fixed	O
point	O
for	O
the	O
SR	Method
,	O
which	O
is	O
particularly	O
concerning	O
in	O
settings	O
with	O
sparse	O
rewards	O
.	O
The	O
agent	O
might	O
end	O
up	O
learning	O
to	O
set	O
to	O
achieve	O
zero	O
loss	O
.	O
We	O
address	O
this	O
problem	O
by	O
not	O
propagating	O
to	O
(	O
this	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
as	O
an	O
open	O
circle	O
stopping	O
the	O
gradient	O
)	O
;	O
and	O
by	O
creating	O
an	O
auxiliary	Task
task	Task
Jaderberg17	O
to	O
encourage	O
a	O
representation	O
to	O
be	O
learned	O
before	O
a	O
non	O
-	O
zero	O
reward	O
is	O
observed	O
.	O
As	O
Machado18b	O
,	O
we	O
use	O
the	O
auxiliary	Task
task	Task
of	O
predicting	O
the	O
next	O
observation	O
,	O
learned	O
through	O
the	O
architecture	O
proposed	O
by	O
Oh15	Method
,	O
which	O
is	O
depicted	O
as	O
the	O
top	O
layers	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
loss	O
we	O
minimize	O
for	O
this	O
last	O
part	O
of	O
the	O
network	O
is	O
The	O
overall	O
loss	O
minimized	O
by	O
the	O
network	O
is	O
The	O
last	O
step	O
in	O
describing	O
our	O
algorithm	O
is	O
to	O
define	O
,	O
the	O
intrinsic	O
reward	O
we	O
use	O
to	O
encourage	O
exploration	Task
.	O
We	O
choose	O
the	O
exploration	O
bonus	O
to	O
be	O
the	O
inverse	O
of	O
the	O
-	O
norm	O
of	O
the	O
vector	O
of	O
successor	O
features	O
of	O
the	O
current	O
state	O
,	O
as	O
in	O
Sarsa	Method
+	O
SR	Method
.	O
That	O
is	O
,	O
where	O
denotes	O
the	O
successor	O
features	O
of	O
state	O
parametrized	O
by	O
.	O
The	O
exploration	O
bonus	O
comes	O
from	O
the	O
same	O
intuition	O
presented	O
in	O
the	O
previous	O
section	O
(	O
we	O
observed	O
in	O
preliminary	O
experiments	O
not	O
discussed	O
here	O
that	O
DQN	Method
performs	O
better	O
when	O
dealing	O
with	O
positive	O
rewards	O
)	O
.	O
Moreover	O
,	O
we	O
use	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
instead	O
of	O
the	O
-	O
norm	O
,	O
which	O
was	O
used	O
so	O
far	O
.	O
This	O
mismatch	O
is	O
further	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O
It	O
was	O
driven	O
by	O
the	O
-	O
norm	O
leading	O
to	O
slightly	O
better	O
performance	O
.	O
A	O
complete	O
description	O
of	O
the	O
network	Method
architecture	Method
is	O
available	O
in	O
the	O
Appendix	O
.	O
section	O
:	O
Evaluation	O
of	O
Exploration	Task
in	O
Deep	Task
RL	Task
We	O
evaluated	O
our	O
algorithm	O
on	O
the	O
Arcade	O
Learning	O
Environment	O
Bellemare13	O
.	O
Following	O
Bellemare16	O
’s	O
(	O
Bellemare16	O
)	O
taxonomy	O
,	O
we	O
focused	O
on	O
the	O
Atari	Material
2600	Material
games	Material
with	O
sparse	O
rewards	O
that	O
pose	O
hard	Task
exploration	Task
problems	Task
.	O
They	O
are	O
:	O
Freeway	O
,	O
Gravitar	O
,	O
Montezuma	Material
’s	Material
Revenge	Material
,	O
Private	O
Eye	O
,	O
Solaris	O
,	O
and	O
Venture	O
.	O
We	O
used	O
the	O
evaluation	O
protocol	O
proposed	O
by	O
Machado18a	O
.	O
We	O
used	O
Montezuma	Material
’s	Material
Revenge	Material
to	O
tune	O
our	O
parameters	O
.	O
The	O
reported	O
results	O
are	O
the	O
average	O
over	O
10	O
seeds	O
after	O
100	O
million	O
frames	O
.	O
We	O
evaluated	O
our	O
agents	O
in	O
the	O
stochastic	Task
setting	Task
(	O
sticky	O
actions	O
,	O
)	O
using	O
a	O
frame	O
skip	O
of	O
with	O
the	O
full	O
action	O
set	O
(	O
)	O
.	O
The	O
agent	O
learns	O
from	O
raw	O
pixels	O
i.e.	O
,	O
it	O
uses	O
the	O
game	O
screen	O
as	O
input	O
.	O
Our	O
results	O
were	O
obtained	O
with	O
the	O
algorithm	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
We	O
set	O
after	O
a	O
rough	O
sweep	O
over	O
values	O
in	O
the	O
game	O
Montezuma	Material
’s	Material
Revenge	Material
.	O
We	O
annealed	O
in	O
DQN	Method
’s	O
-	O
greedy	O
exploration	O
over	O
the	O
first	O
million	O
steps	O
,	O
starting	O
at	O
and	O
stopping	O
at	O
as	O
done	O
by	O
Bellemare16	O
.	O
We	O
trained	O
the	O
network	O
with	O
RMSprop	Method
with	O
a	O
step	O
-	O
size	O
of	O
,	O
an	O
value	O
of	O
,	O
and	O
a	O
decay	O
of	O
,	O
which	O
are	O
the	O
standard	O
parameters	O
for	O
training	O
DQN	Method
Mnih15	O
.	O
The	O
discount	O
factor	O
,	O
,	O
is	O
set	O
to	O
and	O
,	O
,	O
.	O
The	O
weights	O
,	O
,	O
and	O
were	O
set	O
so	O
that	O
the	O
loss	O
functions	O
would	O
be	O
roughly	O
the	O
same	O
scale	O
.	O
All	O
other	O
parameters	O
are	O
the	O
same	O
as	O
those	O
used	O
by	O
Mnih15	O
.	O
subsection	O
:	O
Overall	O
Performance	O
and	O
Baselines	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
results	O
after	O
100	O
million	O
frames	O
.	O
The	O
performance	O
of	O
other	O
algorithms	O
is	O
also	O
provided	O
for	O
reference	O
.	O
Notice	O
we	O
are	O
reporting	O
learning	Task
performance	O
for	O
all	O
algorithms	O
instead	O
of	O
the	O
maximum	O
scores	O
achieved	O
by	O
the	O
algorithm	O
.	O
We	O
use	O
the	O
superscript	O
to	O
distinguish	O
between	O
the	O
algorithms	O
that	O
use	O
MMC	Method
from	O
those	O
that	O
do	O
not	O
.	O
When	O
comparing	O
our	O
algorithm	O
,	O
DQN	Method
+	O
SR	Method
,	O
to	O
DQN	Method
we	O
can	O
see	O
how	O
much	O
our	O
approach	O
improves	O
over	O
the	O
most	O
traditional	O
baseline	O
.	O
By	O
comparing	O
our	O
algorithm	O
’s	O
performance	O
to	O
DQN	Method
+	O
CTS	Method
Bellemare16	O
and	O
DQN	Method
+	O
PixelCNN	Method
Ostrovski17	O
we	O
compare	O
our	O
algorithm	O
to	O
established	O
baselines	O
for	O
exploration	Task
that	O
are	O
closer	O
to	O
our	O
method	O
.	O
By	O
comparing	O
our	O
algorithm	O
’s	O
performance	O
to	O
Random	Method
Network	Method
Distillation	Method
[	O
RND;	O
][]	O
Burda19	O
we	O
compare	O
our	O
algorithm	O
to	O
the	O
most	O
recent	O
paper	O
in	O
the	O
field	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
As	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
parameters	O
of	O
the	O
network	O
we	O
used	O
are	O
different	O
from	O
those	O
used	O
in	O
the	O
traditional	O
DQN	Method
network	O
,	O
so	O
we	O
also	O
compared	O
the	O
performance	O
of	O
our	O
algorithm	O
to	O
the	O
performance	O
of	O
the	O
same	O
network	O
our	O
algorithm	O
uses	O
but	O
without	O
the	O
additional	O
modules	O
(	O
next	Method
state	Method
prediction	Method
and	O
successor	Method
representation	Method
)	O
by	O
setting	O
and	O
without	O
the	O
intrinsic	O
reward	O
bonus	O
by	O
setting	O
.	O
The	O
column	O
labeled	O
DQN	Method
contains	O
the	O
results	O
for	O
this	O
baseline	O
.	O
This	O
comparison	O
allows	O
us	O
to	O
explicitly	O
quantify	O
the	O
improvement	O
provided	O
by	O
the	O
proposed	O
exploration	Method
bonus	Method
.	O
The	O
learning	O
curves	O
of	O
these	O
algorithms	O
and	O
their	O
performance	O
after	O
different	O
amounts	O
of	O
experience	O
are	O
available	O
in	O
the	O
Appendix	O
.	O
We	O
can	O
clearly	O
see	O
that	O
our	O
algorithm	O
achieves	O
scores	O
much	O
higher	O
than	O
those	O
achieved	O
by	O
DQN	Method
,	O
which	O
struggles	O
in	O
games	Task
that	O
pose	O
hard	Task
exploration	Task
problems	Task
.	O
Moreover	O
,	O
by	O
comparing	O
DQN	Method
+	O
SR	Method
to	O
DQN	Method
we	O
can	O
see	O
that	O
the	O
provided	O
exploration	O
bonus	O
has	O
a	O
big	O
impact	O
in	O
the	O
game	O
Montezuma	Material
’s	Material
Revenge	Material
,	O
which	O
is	O
probably	O
known	O
as	O
the	O
hardest	O
game	O
among	O
those	O
we	O
used	O
in	O
our	O
evaluation	O
,	O
and	O
the	O
only	O
game	O
where	O
agents	O
do	O
not	O
learn	O
how	O
to	O
achieve	O
scores	O
greater	O
than	O
zero	O
with	O
random	Method
exploration	Method
.	O
Interestingly	O
,	O
the	O
change	O
in	O
architecture	O
and	O
the	O
use	O
of	O
MMC	Method
leads	O
to	O
a	O
big	O
improvement	O
in	O
games	Task
such	O
as	O
Gravitar	Task
and	O
Venture	Task
,	O
which	O
we	O
can	O
not	O
fully	O
explain	O
.	O
However	O
,	O
notice	O
that	O
the	O
change	O
in	O
architecture	O
does	O
not	O
have	O
any	O
effect	O
in	O
Montezuma	Material
’s	Material
Revenge	Material
.	O
The	O
proposed	O
exploration	O
bonus	O
seems	O
to	O
be	O
essential	O
in	O
games	O
with	O
very	O
sparse	O
rewards	O
.	O
We	O
also	O
compared	O
our	O
algorithm	O
to	O
DQN	Method
+	O
CTS	Method
and	O
DQN	Method
+	O
PixelCNN	Method
.	O
We	O
can	O
observe	O
that	O
,	O
on	O
average	O
,	O
DQN	Method
+	O
SR	Method
outperforms	O
these	O
algorithms	O
while	O
being	O
simpler	O
since	O
it	O
does	O
not	O
require	O
a	O
density	Method
model	Method
.	O
Instead	O
,	O
our	O
algorithm	O
requires	O
the	O
SR	Method
,	O
which	O
is	O
domain	O
-	O
independent	O
as	O
it	O
is	O
already	O
defined	O
for	O
every	O
problem	O
since	O
it	O
is	O
a	O
component	O
of	O
the	O
value	Method
function	Method
estimates	Method
,	O
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Finally	O
,	O
DQN	Method
+	O
SR	Method
also	O
outperforms	O
RND	Method
Burda19	O
when	O
it	O
is	O
trained	O
for	O
100	O
million	O
frames	O
.	O
Importantly	O
,	O
RND	Method
is	O
currently	O
considered	O
to	O
be	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approach	O
for	O
exploration	Task
in	O
Atari	Material
2600	Material
games	Material
.	O
Burda19	O
did	O
not	O
evaluate	O
RND	Method
in	O
Freeway	O
.	O
Details	O
about	O
how	O
the	O
RND	Method
performance	O
was	O
obtained	O
are	O
avaiable	O
in	O
the	O
Appendix	O
.	O
subsection	O
:	O
Evaluating	O
the	O
Impact	O
of	O
the	O
Auxiliary	Task
Task	Task
While	O
the	O
results	O
depicted	O
in	O
Table	O
3	O
allow	O
us	O
to	O
clearly	O
see	O
the	O
benefit	O
of	O
using	O
an	O
exploration	O
bonus	O
derived	O
from	O
the	O
SR	Method
,	O
they	O
do	O
not	O
inform	O
us	O
about	O
the	O
impact	O
of	O
the	O
auxiliary	Task
task	Task
in	O
the	O
results	O
.	O
The	O
experiments	O
in	O
this	O
section	O
aim	O
at	O
addressing	O
this	O
issue	O
.	O
We	O
focus	O
on	O
Montezuma	Material
’s	Material
Revenge	Material
because	O
it	O
is	O
the	O
game	O
where	O
the	O
problem	O
of	O
exploration	Task
is	O
maximized	O
,	O
with	O
most	O
algorithms	O
not	O
being	O
able	O
to	O
do	O
anything	O
without	O
an	O
exploration	O
bonus	O
.	O
The	O
first	O
question	O
we	O
asked	O
was	O
whether	O
the	O
auxiliary	Task
task	Task
was	O
necessary	O
in	O
our	O
algorithm	O
.	O
We	O
evaluated	O
this	O
by	O
dropping	O
the	O
reconstruction	Method
module	Method
from	O
the	O
network	O
to	O
test	O
whether	O
the	O
initial	O
random	O
noise	O
generated	O
by	O
the	O
SR	Method
is	O
enough	O
to	O
drive	O
representation	Task
learning	Task
.	O
It	O
is	O
not	O
.	O
When	O
dropping	O
the	O
auxiliary	Task
task	Task
,	O
the	O
average	O
performance	O
of	O
this	O
baseline	O
over	O
4	O
seeds	O
in	O
Montezuma	Material
’s	Material
Revenge	Material
after	O
million	O
frames	O
was	O
points	O
(	O
=	O
;	O
min	O
:	O
,	O
max	O
:	O
)	O
.	O
As	O
comparison	O
,	O
our	O
algorithm	O
obtains	O
points	O
(	O
=	O
,	O
min	O
:	O
,	O
max	O
:	O
)	O
.	O
These	O
results	O
suggest	O
that	O
auxiliary	Task
tasks	Task
seem	O
to	O
be	O
necessary	O
for	O
our	O
method	O
to	O
perform	O
well	O
.	O
We	O
also	O
evaluated	O
whether	O
the	O
auxiliary	Task
task	Task
was	O
sufficient	O
to	O
generate	O
the	O
results	O
we	O
observed	O
.	O
To	O
do	O
so	O
we	O
dropped	O
the	O
SR	Method
module	O
and	O
set	O
to	O
evaluate	O
whether	O
our	O
exploration	O
bonus	O
was	O
actually	O
improving	O
the	O
agent	O
’s	O
performance	O
or	O
whether	O
the	O
auxiliary	O
task	O
was	O
doing	O
it	O
.	O
The	O
exploration	O
bonus	O
seems	O
to	O
be	O
essential	O
.	O
When	O
dropping	O
the	O
exploration	O
bonus	O
and	O
the	O
SR	Method
module	O
,	O
the	O
average	O
performance	O
of	O
this	O
baseline	O
over	O
4	O
seeds	O
in	O
Montezuma	Material
’s	Material
Revenge	Material
after	O
million	O
frames	O
was	O
points	O
(	O
=	O
;	O
min	O
:	O
,	O
max	O
:	O
)	O
.	O
Again	O
,	O
clearly	O
,	O
the	O
auxiliary	Task
task	Task
is	O
not	O
a	O
sufficient	O
condition	O
for	O
the	O
performance	O
we	O
report	O
.	O
The	O
reported	O
results	O
use	O
the	O
same	O
parameters	O
as	O
before	O
.	O
Learning	O
curves	O
for	O
each	O
run	O
are	O
available	O
in	O
the	O
Appendix	O
.	O
subsection	O
:	O
On	O
the	O
Mismatch	O
between	O
the	O
Used	O
Norms	O
As	O
aforementioned	O
,	O
there	O
is	O
a	O
mismatch	O
between	O
theory	O
and	O
practice	O
when	O
looking	O
at	O
the	O
deep	Method
RL	Method
algorithm	Method
we	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
.	O
While	O
DQN	Method
+	O
SR	Method
uses	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
to	O
generate	O
the	O
exploration	O
bonus	O
,	O
our	O
theoretical	O
result	O
is	O
stated	O
with	O
respect	O
to	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
.	O
This	O
mismatch	O
was	O
driven	O
by	O
the	O
fact	O
that	O
,	O
empirically	O
,	O
DQN	Method
+	O
SR	Method
exhibits	O
a	O
slightly	O
better	O
performance	O
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
instead	O
of	O
the	O
-	O
norm	O
.	O
We	O
conjecture	O
this	O
might	O
be	O
due	O
to	O
the	O
fact	O
that	O
the	O
-	O
norm	O
is	O
smoother	O
than	O
the	O
-	O
norm	O
,	O
a	O
property	O
that	O
is	O
particularly	O
important	O
when	O
training	O
neural	Method
networks	Method
.	O
Table	O
[	O
reference	O
]	O
depicts	O
a	O
comparison	O
between	O
the	O
performance	O
of	O
DQN	Method
+	O
SR	Method
when	O
using	O
the	O
-	O
norm	O
and	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
to	O
generate	O
its	O
exploration	O
bonus	O
(	O
is	O
normalized	O
with	O
the	O
respective	O
norm	O
)	O
.	O
We	O
followed	O
the	O
same	O
evaluation	O
protocol	O
described	O
before	O
,	O
averaging	O
the	O
performance	O
of	O
DQN	Method
+	O
SR	Method
with	O
the	O
-	O
norm	O
over	O
10	O
runs	O
.	O
The	O
parameter	O
is	O
the	O
only	O
parameter	O
not	O
shared	O
by	O
both	O
algorithms	O
.	O
While	O
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
,	O
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
.	O
These	O
results	O
also	O
support	O
the	O
claim	O
that	O
the	O
norm	O
of	O
the	O
SR	Method
can	O
be	O
used	O
to	O
generate	O
exploration	O
bonuses	O
.	O
DQN	Method
+	O
SR	Method
,	O
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
,	O
exhibits	O
performance	O
comparable	O
to	O
pseudo	Method
-	Method
count	Method
based	Method
methods	Method
,	O
despite	O
not	O
being	O
the	O
best	O
results	O
we	O
obtained	O
.	O
We	O
also	O
revisited	O
the	O
results	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
to	O
evaluate	O
the	O
impact	O
of	O
the	O
different	O
norms	O
in	O
Sarsa	Method
+	O
SR	Method
.	O
We	O
swept	O
over	O
all	O
the	O
parameters	O
,	O
as	O
previously	O
described	O
.	O
The	O
results	O
reported	O
for	O
Sarsa	Method
+	O
SR	Method
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
are	O
the	O
average	O
over	O
100	O
runs	O
.	O
The	O
actual	O
numbers	O
are	O
available	O
in	O
Table	O
[	O
reference	O
]	O
.	O
The	O
used	O
parameters	O
are	O
discussed	O
in	O
the	O
Appendix	O
.	O
Interestingly	O
,	O
we	O
observe	O
the	O
same	O
trend	O
we	O
observed	O
in	O
the	O
deep	Task
RL	Task
case	Task
.	O
The	O
-	O
norm	O
of	O
the	O
SR	Method
leads	O
to	O
even	O
better	O
results	O
.	O
The	O
fact	O
that	O
the	O
algorithms	O
proposed	O
in	O
this	O
paper	O
perform	O
better	O
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
instead	O
of	O
the	O
-	O
norm	O
deserves	O
further	O
investigation	O
,	O
either	O
empirically	O
or	O
theoretically	O
.	O
We	O
conjecture	O
it	O
might	O
be	O
possible	O
to	O
derive	O
theoretical	O
guarantees	O
for	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
that	O
are	O
similar	O
to	O
those	O
derived	O
here	O
.	O
Nevertheless	O
,	O
these	O
results	O
suggest	O
that	O
the	O
idea	O
of	O
using	O
the	O
norm	O
of	O
the	O
SR	Method
for	O
exploration	Task
is	O
quite	O
general	O
,	O
with	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
being	O
effective	O
for	O
more	O
than	O
one	O
value	O
of	O
.	O
section	O
:	O
Related	O
Work	O
There	O
are	O
multiple	O
algorithms	O
in	O
the	O
tabular	Task
,	Task
model	Task
-	Task
based	Task
case	Task
with	O
guarantees	O
about	O
their	O
performance	O
in	O
terms	O
of	O
regret	Metric
bounds	Metric
[	O
e.g.	O
,][]	O
Osband16b	O
or	O
sample	Metric
-	Metric
complexity	Metric
[	O
e.g.	O
,][]	O
Brafman02	O
,	O
Kearns02	O
,	O
Strehl08	O
.	O
RiverSwim	O
and	O
SixArms	O
are	O
domains	O
traditionally	O
used	O
when	O
evaluating	O
these	O
algorithms	O
.	O
In	O
this	O
paper	O
we	O
have	O
introduced	O
a	O
model	Method
-	Method
free	Method
algorithm	Method
that	O
performs	O
particularly	O
well	O
in	O
these	O
domains	O
.	O
We	O
have	O
also	O
introduced	O
a	O
model	Method
-	Method
based	Method
algorithm	Method
that	O
performs	O
as	O
well	O
as	O
some	O
of	O
these	O
algorithms	O
with	O
theoretical	O
guarantees	O
.	O
Among	O
these	O
algorithms	O
,	O
R	Method
-	Method
Max	Method
seems	O
the	O
closest	O
approach	O
to	O
ours	O
.	O
As	O
with	O
R	Method
-	Method
Max	Method
,	O
the	O
algorithm	O
we	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
augments	O
the	O
state	O
-	O
space	O
with	O
an	O
imaginary	O
state	O
and	O
encourages	O
the	O
agent	O
to	O
visit	O
that	O
state	O
,	O
implicitly	O
reducing	O
the	O
algorithm	O
’s	O
uncertainty	O
in	O
the	O
state	O
-	O
space	O
.	O
However	O
,	O
R	Method
-	Method
Max	Method
deletes	O
the	O
transition	O
to	O
this	O
imaginary	O
state	O
once	O
a	O
state	O
has	O
been	O
visited	O
a	O
given	O
number	O
of	O
times	O
.	O
Ours	O
,	O
on	O
the	O
other	O
hand	O
,	O
lets	O
the	O
probability	O
of	O
visiting	O
this	O
imaginary	O
state	O
vanish	O
with	O
additional	O
visitations	O
.	O
Importantly	O
,	O
notice	O
that	O
it	O
is	O
not	O
clear	O
how	O
to	O
apply	O
these	O
traditional	O
algorithms	O
such	O
as	O
R	Method
-	Method
Max	Method
and	O
E	Method
to	O
large	O
domains	O
where	O
function	Task
approximation	Task
is	O
required	O
.	O
Conversely	O
,	O
there	O
are	O
not	O
many	O
model	Method
-	Method
free	Method
approaches	Method
with	O
proven	O
sample	Metric
-	Metric
complexity	Metric
bounds	Metric
[	O
e.g.	O
,][]	O
Strehl06	O
,	O
but	O
there	O
are	O
multiple	O
model	Method
-	Method
free	Method
algorithms	Method
for	O
exploration	Task
that	O
actually	O
work	O
in	O
large	O
domains	O
[	O
e.g.	O
,][]	O
Stadie15	O
,	O
Bellemare16	O
,	O
Ostrovski17	O
,	O
Plappert18	O
,	O
Burda19	O
.	O
Among	O
these	O
algorithms	O
,	O
the	O
use	O
of	O
pseudo	Method
-	Method
counts	Method
through	O
density	Method
models	Method
is	O
the	O
closest	O
to	O
ours	O
Bellemare16	O
,	O
Ostrovski17	O
.	O
Inspired	O
by	O
those	O
papers	O
we	O
used	O
the	O
mixed	O
Monte	O
-	O
Carlo	O
return	O
as	O
a	O
target	O
in	O
the	O
update	Method
rule	Method
.	O
In	O
Section	O
[	O
reference	O
]	O
we	O
have	O
shown	O
that	O
our	O
algorithm	O
outperforms	O
these	O
approaches	O
while	O
being	O
simpler	O
by	O
not	O
requiring	O
a	O
density	Method
model	Method
.	O
Importantly	O
,	O
Martin17	O
had	O
already	O
shown	O
that	O
counting	O
activations	O
of	O
fixed	O
,	O
handcrafted	O
features	O
in	O
Atari	Material
2600	Material
games	Material
leads	O
to	O
good	O
exploration	O
behavior	O
.	O
Nevertheless	O
,	O
by	O
using	O
the	O
SR	Method
we	O
are	O
not	O
only	O
counting	O
learned	O
features	O
but	O
we	O
are	O
also	O
implicitly	O
capturing	O
the	O
induced	O
transition	O
dynamics	O
.	O
Finally	O
,	O
the	O
SR	Method
has	O
already	O
been	O
used	O
in	O
the	O
context	O
of	O
exploration	Task
.	O
However	O
,	O
it	O
was	O
used	O
to	O
help	O
the	O
agent	O
learn	O
how	O
to	O
act	O
in	O
a	O
higher	O
level	O
of	O
abstraction	O
in	O
order	O
to	O
navigate	O
through	O
the	O
state	O
space	O
faster	O
Machado18b	O
.	O
Such	O
an	O
approach	O
has	O
led	O
to	O
promising	O
results	O
in	O
the	O
tabular	O
case	O
but	O
only	O
anecdotal	O
evidence	O
about	O
its	O
scalability	O
has	O
been	O
provided	O
when	O
the	O
idea	O
was	O
applied	O
to	O
large	O
domains	O
such	O
as	O
Atari	Material
2600	Material
games	Material
.	O
Importantly	O
,	O
the	O
work	O
developed	O
by	O
Machado18b	O
,	O
Kulkarni16	O
and	O
Oh15	Method
are	O
the	O
main	O
motivation	O
for	O
the	O
neural	Method
network	Method
architecture	Method
presented	O
here	O
.	O
Oh15	Method
have	O
shown	O
how	O
one	O
can	O
predict	O
the	O
next	O
screen	O
given	O
the	O
current	O
observation	O
and	O
action	O
(	O
our	O
auxiliary	O
task	O
)	O
,	O
while	O
Machado18b	O
and	O
Kulkarni16	O
have	O
proposed	O
different	O
architectures	O
for	O
learning	O
the	O
SR	Method
from	O
raw	O
pixels	O
.	O
section	O
:	O
Conclusion	O
RL	Method
algorithms	Method
tend	O
to	O
have	O
high	O
sample	Metric
complexity	Metric
,	O
which	O
often	O
prevents	O
them	O
from	O
being	O
used	O
in	O
the	O
real	O
-	O
world	O
.	O
Poor	O
exploration	Method
strategies	Method
is	O
one	O
of	O
the	O
main	O
reasons	O
for	O
this	O
high	O
sample	Metric
-	Metric
complexity	Metric
.	O
Despite	O
all	O
of	O
its	O
shortcomings	O
,	O
uniform	Method
random	Method
exploration	Method
is	O
,	O
to	O
date	O
,	O
the	O
most	O
commonly	O
used	O
approach	O
for	O
exploration	Task
.	O
This	O
is	O
mainly	O
due	O
to	O
the	O
fact	O
that	O
most	O
approaches	O
for	O
tackling	O
the	O
exploration	Task
problem	Task
still	O
rely	O
on	O
domain	O
-	O
specific	O
knowledge	O
(	O
e.g.	O
,	O
density	Method
models	Method
,	O
handcrafted	O
features	O
)	O
,	O
or	O
on	O
having	O
an	O
agent	O
learn	O
a	O
perfect	O
model	O
of	O
the	O
environment	O
.	O
In	O
this	O
paper	O
we	O
introduced	O
a	O
general	O
method	O
for	O
exploration	Task
in	O
RL	Task
that	O
implicitly	O
counts	O
state	O
(	O
or	O
feature	O
)	O
visitation	O
in	O
order	O
to	O
guide	O
the	O
exploration	Task
process	Task
.	O
It	O
is	O
compatible	O
to	O
representation	Task
learning	Task
and	O
the	O
idea	O
can	O
also	O
be	O
adapted	O
to	O
be	O
applied	O
to	O
large	O
domains	O
.	O
This	O
result	O
opens	O
up	O
multiple	O
possibilities	O
for	O
future	O
work	O
.	O
Based	O
on	O
the	O
results	O
presented	O
in	O
Section	O
[	O
reference	O
]	O
,	O
for	O
example	O
,	O
we	O
conjecture	O
that	O
the	O
substochastic	Method
successor	Method
representation	Method
can	O
be	O
actually	O
used	O
to	O
generate	O
algorithms	O
with	O
PAC	Method
-	Method
MDP	Method
bounds	Method
.	O
Investigating	O
to	O
what	O
extent	O
different	O
auxiliary	O
tasks	O
impact	O
the	O
algorithm	O
’s	O
performance	O
,	O
and	O
whether	O
simpler	O
tasks	O
such	O
as	O
predicting	O
feature	O
activations	O
or	O
parts	O
of	O
the	O
input	O
Jaderberg17	O
are	O
effective	O
is	O
also	O
worth	O
studying	O
.	O
Finally	O
,	O
it	O
might	O
be	O
interesting	O
to	O
further	O
investigate	O
the	O
connection	O
between	O
representation	Task
learning	Task
and	O
exploration	Task
,	O
since	O
it	O
is	O
also	O
known	O
that	O
better	O
representations	O
can	O
lead	O
to	O
faster	O
exploration	Task
Jiang17	O
.	O
section	O
:	O
Acknowledgements	O
The	O
authors	O
would	O
like	O
to	O
thank	O
Jesse	O
Farebrother	O
for	O
the	O
initial	O
implementation	O
of	O
DQN	Method
used	O
in	O
this	O
paper	O
,	O
Georg	O
Ostrovski	O
for	O
the	O
discussions	O
and	O
for	O
kindly	O
providing	O
us	O
the	O
exact	O
results	O
we	O
report	O
for	O
DQNMMC	Method
+	O
CTS	Method
and	O
DQNMMC	Method
+	O
PixelCNN	Method
,	O
and	O
Yuri	O
Burda	O
for	O
kindly	O
providing	O
us	O
the	O
data	O
we	O
used	O
to	O
compute	O
the	O
performance	O
we	O
report	O
for	O
RND	Method
in	O
Atari	Material
2600	Material
games	Material
.	O
We	O
would	O
also	O
like	O
to	O
thank	O
Carles	O
Gelada	O
,	O
George	O
Tucker	O
and	O
Or	O
Sheffet	O
for	O
useful	O
discussions	O
,	O
as	O
well	O
as	O
the	O
anonymous	O
reviewers	O
for	O
their	O
feedback	O
.	O
This	O
work	O
was	O
supported	O
by	O
grants	O
from	O
Alberta	O
Innovates	O
Technology	O
Futures	O
and	O
the	O
Alberta	O
Machine	O
Intelligence	O
Institute	O
(	O
Amii	O
)	O
.	O
Computing	O
resources	O
were	O
provided	O
by	O
Compute	O
Canada	O
through	O
CalculQuébec	O
.	O
bibliography	O
:	O
References	O
Supplemental	O
Material	O
:	O
Count	Method
-	Method
Based	Method
Exploration	Method
with	O
the	O
Successor	Method
Representation	Method
This	O
document	O
contains	O
details	O
omitted	O
from	O
the	O
main	O
text	O
due	O
to	O
space	O
constraints	O
.	O
The	O
list	O
of	O
contents	O
is	O
below	O
:	O
Description	O
of	O
RiverSwim	O
and	O
SixArms	O
,	O
the	O
tabular	O
domains	O
we	O
used	O
in	O
our	O
evaluation	O
;	O
Details	O
about	O
the	O
methodology	O
we	O
used	O
to	O
evaluate	O
Sarsa	Method
and	O
Sarsa	Method
+	O
SR	Method
;	O
Pseudo	O
-	O
code	O
for	O
the	O
model	Method
-	Method
based	Method
algorithm	Method
discussed	O
in	O
Section	O
3.2	O
;	O
Detailed	O
description	O
of	O
the	O
neural	Method
network	Method
implemented	O
with	O
DQN	Method
+	O
SR	Method
;	O
Learning	Method
curves	Method
for	O
the	O
experiments	O
with	O
DQN	Method
and	O
DQN	Method
+	O
SR	Method
and	O
the	O
performance	O
of	O
these	O
algorithms	O
after	O
different	O
amounts	O
of	O
experience	O
in	O
the	O
Atari	Material
2600	Material
games	Material
used	O
in	O
our	O
evaluation	O
;	O
Learning	O
curves	O
for	O
the	O
experiments	O
designed	O
to	O
evaluate	O
the	O
role	O
of	O
the	O
auxiliary	Task
task	Task
in	O
DQN	Method
+	O
SR	Method
;	O
Details	O
about	O
how	O
we	O
computed	O
the	O
performance	O
of	O
Random	Method
Network	Method
Distillation	Method
(	O
RND	Method
)	O
;	O
Learning	Method
curves	Method
and	O
the	O
performance	O
after	O
different	O
amounts	O
of	O
experience	O
of	O
DQN	Method
+	O
SR	Method
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
to	O
generate	O
the	O
exploration	O
bonus	O
.	O
section	O
:	O
Description	O
of	O
RiverSwim	O
and	O
SixArms	O
The	O
two	O
domains	O
we	O
used	O
as	O
testbed	O
to	O
evaluate	O
our	O
ideas	O
in	O
the	O
tabular	O
case	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
These	O
domains	O
are	O
the	O
same	O
used	O
by	O
Strehl08	O
.	O
For	O
SixArms	O
,	O
the	O
agent	O
starts	O
in	O
state	O
.	O
For	O
RiverSwim	Task
,	O
the	O
agent	O
starts	O
in	O
either	O
state	O
or	O
with	O
equal	O
probability	O
.	O
[	O
b	O
]	O
0.55	O
[	O
b	O
]	O
0.42	O
section	O
:	O
Methodology	O
used	O
to	O
Evaluate	O
Sarsa	Method
and	O
Sarsa	Method
+	O
SR	Method
Both	O
Sarsa	Method
and	O
Sarsa	Method
+	O
SR	Method
acted	O
-	O
greedily	O
,	O
maximizing	O
the	O
discounted	O
return	O
using	O
.	O
For	O
Sarsa	Method
+	O
SR	Method
,	O
we	O
swept	O
over	O
different	O
values	O
of	O
,	O
,	O
,	O
and	O
,	O
with	O
,	O
,	O
,	O
and	O
.	O
For	O
Sarsa	Method
,	O
we	O
swept	O
over	O
the	O
parameters	O
and	O
.	O
For	O
fairness	O
,	O
we	O
looked	O
at	O
a	O
finer	O
granularity	O
for	O
these	O
parameters	O
,	O
with	O
for	O
ranging	O
from	O
to	O
,	O
and	O
with	O
for	O
ranging	O
from	O
to	O
.	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
parameter	O
settings	O
that	O
led	O
to	O
the	O
best	O
results	O
for	O
each	O
algorithm	O
in	O
RiverSwim	O
and	O
SixArms	O
.	O
section	O
:	O
Exploration	O
through	O
the	O
Substochastic	Method
Successor	Method
Representation	Method
In	O
the	O
main	O
paper	O
we	O
described	O
ESSR	Method
as	O
a	O
standard	O
model	Method
-	Method
based	Method
algorithm	Method
where	O
the	O
agent	O
updates	O
its	O
transition	Method
probability	Method
model	Method
and	O
reward	Method
model	Method
through	O
Equation	O
2	O
and	O
its	O
substochastic	Method
successor	Method
representation	Method
estimate	Method
as	O
in	O
Definition	O
3.1	O
.	O
The	O
pseudo	O
-	O
code	O
with	O
details	O
about	O
the	O
implementation	O
is	O
presented	O
below	O
.	O
[	O
h	O
]	O
Exploration	O
through	O
the	O
Substochastic	Method
Successor	Method
Representation	Method
(	O
ESSR	Method
)	O
episode	O
is	O
not	O
over	O
Observe	O
,	O
take	O
action	O
selected	O
according	O
to	O
,	O
and	O
observe	O
a	O
reward	O
and	O
a	O
next	O
state	O
each	O
state	O
PolicyIteration	O
(	O
^P	O
,+	O
^r⁢βrint	O
)	O
section	O
:	O
Detailed	O
Neural	Method
Network	Method
Architecture	Method
of	O
DQN	Method
+	O
SR	Method
In	O
order	O
to	O
allow	O
the	O
reader	O
to	O
focus	O
on	O
the	O
general	O
idea	O
proposed	O
in	O
the	O
paper	O
,	O
we	O
decided	O
to	O
not	O
present	O
,	O
in	O
the	O
main	O
text	O
,	O
the	O
detailed	O
description	O
of	O
the	O
architecture	O
we	O
used	O
in	O
DQN	Method
+	O
SR	Method
.	O
The	O
detailed	O
architecture	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
we	O
explicitly	O
represent	O
each	O
layer	O
and	O
their	O
parameters	O
.	O
The	O
loss	Method
functions	Method
and	O
the	O
optimizer	Method
used	O
were	O
described	O
in	O
the	O
main	O
paper	O
.	O
The	O
only	O
other	O
important	O
information	O
is	O
regarding	O
the	O
network	O
’s	O
initialization	O
.	O
We	O
initialize	O
our	O
network	O
the	O
same	O
way	O
Oh15	Method
does	O
.	O
We	O
use	O
Xavier	Method
initialization	Method
Glorot10	O
in	O
all	O
layers	O
except	O
the	O
fully	O
connected	O
layers	O
around	O
the	O
element	O
-	O
wise	O
multiplication	O
denoted	O
by	O
,	O
which	O
are	O
initialized	O
uniformly	O
with	O
values	O
between	O
and	O
.	O
section	O
:	O
Additional	O
Results	O
for	O
DQN	Method
+	O
SR	Method
and	O
DQN	Method
in	O
the	O
Atari	Material
2600	Material
Games	Material
As	O
recommended	O
by	O
Machado18a	O
,	O
we	O
report	O
the	O
performance	O
of	O
DQN	Method
+	O
SR	Method
and	O
DQN	Method
after	O
different	O
amounts	O
of	O
experience	O
(	O
,	O
,	O
and	O
million	O
frames	O
)	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
Finally	O
,	O
Figure	O
[	O
reference	O
]	O
depicts	O
the	O
learning	O
curves	O
obtained	O
with	O
the	O
evaluated	O
algorithms	O
in	O
each	O
game	O
.	O
Lighter	O
lines	O
represent	O
individual	O
runs	O
while	O
the	O
solid	O
lines	O
encode	O
the	O
average	O
over	O
the	O
multiple	O
runs	O
.	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
section	O
:	O
Evaluation	O
of	O
the	O
Impact	O
of	O
the	O
Auxiliary	Task
Task	Task
in	O
DQN	Method
+	O
SR	Method
In	O
the	O
main	O
paper	O
we	O
also	O
evaluated	O
whether	O
the	O
introduced	O
auxiliary	Task
task	Task
was	O
necessary	O
and	O
whether	O
this	O
auxiliary	O
task	O
was	O
sufficient	O
to	O
obtain	O
the	O
performance	O
reported	O
for	O
DQN	Method
+	O
SR	Method
.	O
The	O
results	O
suggest	O
that	O
while	O
an	O
auxiliary	Task
task	Task
is	O
necessary	O
for	O
DQN	Method
+	O
SR	Method
,	O
it	O
is	O
definitely	O
not	O
sufficient	O
to	O
explain	O
the	O
obtained	O
performance	O
.	O
This	O
discussion	O
is	O
available	O
in	O
the	O
main	O
paper	O
.	O
Figure	O
[	O
reference	O
]	O
depicts	O
the	O
learning	O
curves	O
for	O
each	O
individual	O
run	O
.	O
[	O
b	O
]	O
0.31	O
[	O
b	O
]	O
0.31	O
section	O
:	O
On	O
the	O
Reported	O
Performance	O
of	O
Random	Method
Network	Method
Distillation	Method
(	O
RND	Method
)	O
in	O
the	O
ALE	Task
We	O
calculated	O
the	O
performance	O
of	O
RND	Method
from	O
the	O
data	O
used	O
by	O
Burda19	O
to	O
plot	O
Figure	O
7	O
of	O
their	O
paper	O
.	O
The	O
authors	O
shared	O
this	O
data	O
with	O
us	O
.	O
The	O
performance	O
we	O
report	O
is	O
the	O
average	O
performance	O
after	O
rollouts	O
.	O
Each	O
rollout	O
consists	O
of	O
128	O
time	O
steps	O
with	O
4	O
frames	O
per	O
time	O
step	O
(	O
128	O
environments	O
were	O
executed	O
in	O
parallel	O
)	O
,	O
leading	O
to	O
frames	O
.	O
We	O
averaged	O
the	O
performance	O
over	O
seeds	O
in	O
the	O
games	O
Gravitar	O
,	O
Private	O
Eye	O
,	O
Solaris	O
,	O
and	O
Venture	O
.	O
The	O
performance	O
reported	O
for	O
Montezuma	Material
’s	Material
Revenge	Material
is	O
the	O
average	O
over	O
seeds	O
.	O
section	O
:	O
The	O
Impact	O
of	O
Using	O
the	O
and	O
-	O
norm	O
of	O
the	O
SR	Method
in	O
DQN	Method
+	O
SR	Method
In	O
the	O
main	O
paper	O
we	O
evaluated	O
DQN	Method
+	O
SR	Method
using	O
both	O
the	O
-	O
and	O
-	O
norm	O
of	O
the	O
SR	Method
.	O
In	O
this	O
section	O
we	O
provide	O
additional	O
data	O
related	O
to	O
DQN	Method
+	O
SR	Method
when	O
using	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
.	O
More	O
specifically	O
,	O
Table	O
[	O
reference	O
]	O
depicts	O
the	O
agent	O
’s	O
performance	O
after	O
different	O
amounts	O
of	O
experience	O
while	O
Figure	O
[	O
reference	O
]	O
depicts	O
the	O
full	O
learning	O
curves	O
.	O
Finally	O
,	O
for	O
completeness	O
,	O
Table	O
[	O
reference	O
]	O
depicts	O
the	O
agent	O
’s	O
performance	O
after	O
different	O
amounts	O
of	O
experience	O
for	O
DQN	Method
when	O
the	O
feature	Method
representation	Method
is	O
normalized	O
with	O
the	O
-	O
norm	O
instead	O
of	O
the	O
-	O
norm	O
of	O
the	O
SR	Method
.	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
[	O
b	O
]	O
0.32	O
