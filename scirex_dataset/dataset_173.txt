document O
: O
Towards O
Faster O
Training O
of O
Global Method
Covariance Method
Pooling Method
Networks Method
by O
Iterative Method
Matrix Method
Square Method
Root Method
Normalization Method
Global Method
covariance Method
pooling Method
in O
convolutional Method
neural Method
networks Method
has O
achieved O
impressive O
improvement O
over O
the O
classical O
first Method
- Method
order Method
pooling Method
. O
Recent O
works O
have O
shown O
matrix O
square Method
root Method
normalization Method
plays O
a O
central O
role O
in O
achieving O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
However O
, O
existing O
methods O
depend O
heavily O
on O
eigendecomposition Method
( O
EIG Method
) O
or O
singular Method
value Method
decomposition Method
( O
SVD Method
) O
, O
suffering O
from O
inefficient O
training Task
due O
to O
limited O
support O
of O
EIG Method
and O
SVD Method
on O
GPU O
. O
Towards O
addressing O
this O
problem O
, O
we O
propose O
an O
iterative Method
matrix Method
square Method
root Method
normalization Method
method Method
for O
fast O
end Task
- Task
to Task
- Task
end Task
training Task
of Task
global Task
covariance Task
pooling Task
networks Task
. O
At O
the O
core O
of O
our O
method O
is O
a O
meta Method
- Method
layer Method
designed O
with O
loop O
- O
embedded O
directed O
graph O
structure O
. O
The O
meta Method
- Method
layer Method
consists O
of O
three O
consecutive O
nonlinear Method
structured Method
layers Method
, O
which O
perform O
pre Method
- Method
normalization Method
, O
coupled Method
matrix Method
iteration Method
and O
post Task
- Task
compensation Task
, O
respectively O
. O
Our O
method O
is O
much O
faster O
than O
EIG Method
or O
SVD Method
based Method
ones Method
, O
since O
it O
involves O
only O
matrix Method
multiplications Method
, O
suitable O
for O
parallel Task
implementation Task
on O
GPU O
. O
Moreover O
, O
the O
proposed O
network O
with O
ResNet Method
architecture Method
can O
converge O
in O
much O
less O
epochs O
, O
further O
accelerating O
network Task
training Task
. O
On O
large O
- O
scale O
ImageNet O
, O
we O
achieve O
competitive O
performance O
superior O
to O
existing O
counterparts O
. O
By O
finetuning O
our O
models O
pre O
- O
trained O
on O
ImageNet Material
, O
we O
establish O
state O
- O
of O
- O
the O
- O
art O
results O
on O
three O
challenging O
fine Task
- Task
grained Task
benchmarks Task
. O
The O
source O
code O
and O
network Method
models Method
will O
be O
available O
at O
. O
footnote O
[ O
page O
] O
section O
: O
Introduction O
Deep Method
convolutional Method
neural Method
networks Method
( O
ConvNets Method
) O
have O
made O
significant O
progress O
in O
the O
past O
years O
, O
achieving O
recognition Metric
accuracy Metric
surpassing O
human O
beings O
in O
large Task
- Task
scale Task
object Task
recognition Task
. O
The O
ConvNet Method
models Method
pre O
- O
trained O
on O
ImageNet Material
have O
been O
proven O
to O
benefit O
a O
multitude O
of O
other O
computer Task
vision Task
tasks Task
, O
ranging O
from O
fine Task
- Task
grained Task
visual Task
categorization Task
( O
FGVC Task
) O
, O
object Task
detection Task
, O
semantic Task
segmentation Task
to O
scene Task
parsing Task
, O
where O
labeled O
data O
are O
insufficient O
for O
training O
from O
scratch O
. O
The O
common Method
layers Method
such O
as O
convolution Method
, O
non Method
- Method
linear Method
rectification Method
, O
pooling Method
and O
batch Method
normalization Method
have O
become O
off O
- O
the O
- O
shelf O
commodities O
, O
widely O
supported O
on O
devices O
including O
workstations O
, O
PCs Method
and O
embedded Task
systems Task
. O
Although O
the O
architecture O
of O
ConvNet Method
has O
greatly O
evolved O
in O
the O
past O
years O
, O
its O
basic O
layers O
largely O
keep O
unchanged O
. O
Recently O
, O
researchers O
have O
shown O
increasing O
interests O
in O
exploring O
structured O
layers O
to O
enhance O
representation Task
capability Task
of Task
networks Task
. O
One O
particular O
kind O
of O
structured Method
layer Method
is O
concerned O
with O
global Method
covariance Method
pooling Method
after O
the O
last O
convolution Method
layer Method
, O
which O
has O
shown O
impressive O
improvement O
over O
the O
classical O
first Method
- Method
order Method
pooling Method
, O
successfully O
used O
in O
FGVC Task
, O
visual Task
question Task
answering Task
and O
video Task
action Task
recognition Task
. O
Very O
recent O
works O
have O
demonstrated O
that O
matrix Method
square Method
root Method
normalization Method
of Method
global Method
covariance Method
pooling Method
plays O
a O
key O
role O
in O
achieving O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
both O
large Task
- Task
scale Task
visual Task
recognition Task
and O
challenging Task
FGVC Task
. O
CUDA Method
support O
Scalability O
to O
multi Method
- Method
GPUs Method
Large Method
- Method
scale Method
( O
LS Method
) O
or O
Small Method
- Method
scale Method
( O
SS Method
) O
EIG Method
algorithm Method
BP Method
of O
EIG Method
limited O
G O
DeNet O
SVD Method
algorithm O
BP O
of O
SVD Method
limited O
Improved O
B Method
- Method
CNN Method
Newton Method
- Method
Schulz Method
Iter Method
. O
BP Method
by O
Lyapunov Method
equation Method
( O
SCHUR O
or O
EIG Method
required O
) O
iSQRT Method
- Method
COV Method
( O
ours O
) O
Newton Method
- Method
Schulz Method
Iter Method
. O
BP Method
of Method
Newton Method
- Method
Schulz Method
Iter Method
. O
For O
computing O
matrix Task
square Task
root Task
, O
existing O
methods O
depend O
heavily O
on O
eigendecomposition Method
( O
EIG Method
) O
or O
singular Method
value Method
decomposition Method
( O
SVD Method
) O
. O
However O
, O
fast O
implementation O
of O
EIG Method
or O
SVD Method
on O
GPU Method
is O
an O
open O
problem O
, O
which O
is O
limitedly O
supported O
on O
NVIDIA Method
CUDA Method
platform Method
, O
significantly O
slower O
than O
their O
CPU Method
counterparts Method
. O
As O
such O
, O
existing O
methods O
opt O
for O
EIG Method
or O
SVD Method
on O
CPU Method
for O
computing O
matrix Task
square Task
root Task
. O
Nevertheless O
, O
current O
implementations O
of O
meta Method
- Method
layers Method
depending O
on O
CPU Method
are O
far O
from O
ideal O
, O
particularly O
for O
multi Task
- Task
GPU Task
configuration Task
. O
Since O
GPUs Method
with O
powerful O
parallel O
computing O
ability O
have O
to O
be O
interrupted O
and O
await O
CPUs O
with O
limited O
parallel O
ability O
, O
their O
concurrency O
and O
throughput O
are O
greatly O
restricted O
. O
In O
, O
for O
the O
purpose O
of O
fast Task
forward Task
propagation Task
( Task
FP Task
) Task
, O
Lin O
and O
Maji O
use O
Newton Method
- Method
Schulz Method
iteration Method
( O
called O
modified Method
Denman Method
- Method
Beavers Method
iteration Method
therein O
) O
algorithm O
, O
which O
is O
proposed O
in O
, O
to O
compute O
matrix O
square O
- O
root O
. O
Unfortunately O
, O
for O
backward Task
propagation Task
( O
BP Method
) O
, O
they O
compute O
the O
gradient O
through O
Lyapunov Method
equation Method
solution Method
which O
depends O
on O
the O
GPU Method
unfriendly Method
Schur Method
- Method
decomposition Method
( O
SCHUR Method
) O
or O
EIG Method
. O
Hence O
, O
the O
training O
in O
is O
expensive O
though O
FP Method
which O
involves O
only O
matrix Method
multiplication Method
runs O
very O
fast O
. O
Inspired O
by O
that O
work O
, O
we O
propose O
a O
fast O
end Method
- Method
to Method
- Method
end Method
training Method
method Method
, O
called O
iterative Method
matrix Method
square Method
root Method
normalization Method
of Method
covariance Method
pooling Method
( O
iSQRT Method
- Method
COV Method
) O
, O
depending O
on O
Newton Method
- Method
Schulz Method
iteration Method
in O
both O
forward Method
and Method
backward Method
propagations Method
. O
At O
the O
core O
of O
iSQRT Method
- Method
COV Method
is O
a O
meta Method
- Method
layer Method
with O
loop Method
- Method
embedded Method
directed Method
graph Method
structure Method
, O
specifically O
designed O
for O
ensuring O
both O
convergence O
of O
Newton Method
- Method
Schulz Method
iteration Method
and O
performance O
of O
global Method
covariance Method
pooling Method
networks Method
. O
The O
meta Method
- Method
layer Method
consists O
of O
three O
consecutive O
structured Method
layers Method
, O
performing O
pre Method
- Method
normalization Method
, O
coupled Method
matrix Method
iteration Method
and O
post Task
- Task
compensation Task
, O
respectively O
. O
We O
derive O
the O
gradients O
associated O
with O
the O
involved O
non Method
- Method
linear Method
layers Method
based O
on O
matrix Method
backpropagation Method
theory Method
. O
The O
design O
of O
sandwiching Method
Newton Method
- Method
Schulz Method
iteration Method
using O
pre Method
- Method
normalization Method
by O
Frobenius O
norm O
or O
trace O
and O
post Method
- Method
compensation Method
is O
essential O
, O
which O
, O
as O
far O
as O
we O
know O
, O
did O
not O
appear O
in O
previous O
literature O
( O
e.g. O
in O
or O
) O
. O
The O
pre Method
- Method
normalization Method
guarantees O
convergence O
of O
Newton Method
- Method
Schulz Method
( Method
NS Method
) Method
iteration Method
, O
while O
post Method
- Method
compensation Method
plays O
a O
key O
role O
in O
achieving O
state O
- O
of O
- O
the O
- O
art O
performance O
with O
prevalent O
deep Method
ConvNet Method
architectures Method
, O
e.g. O
ResNet Method
. O
The O
main O
differences O
between O
our O
method O
and O
other O
related O
works O
are O
summarized O
in O
Tab O
. O
[ O
reference O
] O
. O
section O
: O
Related O
Work O
B Method
- Method
CNN Method
is O
one O
of O
the O
first O
end Task
- Task
to Task
- Task
end Task
covariance Task
pooling Task
ConvNets Task
. O
It O
performs O
element Method
- Method
wise Method
square Method
root Method
normalization Method
followed O
by O
normalization Method
for O
covariance O
matrix O
, O
achieving O
impressive O
performance O
in O
FGVC Task
task Task
. O
Improved O
B Method
- Method
CNN Method
shows O
that O
additional O
matrix Method
square Method
root Method
normalization Method
before O
element Method
- Method
wise Method
square Method
root Method
and O
normalization Method
can O
further O
attain O
large O
improvement O
. O
In O
training Task
process Task
, O
they O
perform O
FP Method
using O
Newton Method
- Method
Schulz Method
iteration Method
or O
using O
SVD Method
, O
and O
perform O
BP Method
by O
solving O
Lyapunov Method
equation Method
or O
compute O
gradients O
associated O
with O
SVD Method
. O
In O
any O
case O
, O
improved O
B Method
- Method
CNN Method
suffers O
from O
GPU O
unfriendly O
SVD Method
, O
SCHUR O
or O
EIG Method
and O
so O
network Method
training Method
is O
expensive O
. O
Our O
iSQRT Method
- Method
COV Method
differs O
from O
in O
three O
aspects O
. O
First O
, O
both O
FP Method
and O
BP Method
of O
our O
method O
are O
based O
on O
Newton Method
- Method
Schulz Method
iteration Method
, O
making O
network Method
training Method
very O
efficient O
as O
only O
GPU Method
friendly Method
matrix Method
multiplications Method
are O
involved O
. O
Second O
, O
we O
propose O
sandwiching O
Newton Method
- Method
Schulz Method
iteration Method
using O
pre Method
- Method
normalization Method
and O
post Method
- Method
compensation Method
which O
is O
essential O
and O
plays O
a O
key O
role O
in O
training O
extremely O
deep Task
ConvNets Task
. O
Finally O
, O
we O
evaluate O
extensively O
on O
both O
large Material
- Material
scale Material
ImageNet Material
and O
on O
three O
popular O
fine O
- O
grained O
benchmarks O
. O
In O
, O
matrix Method
power Method
normalized Method
covariance Method
pooling Method
method Method
( O
MPN Method
- Method
COV Method
) O
is O
proposed O
for O
large Task
- Task
scale Task
visual Task
recognition Task
. O
It O
achieves O
impressive O
improvements O
over O
first Method
- Method
order Method
pooling Method
with O
AlexNet Method
, O
VGG Method
- Method
Net Method
and O
ResNet Method
architectures Method
. O
MPN Method
- Method
COV Method
has O
shown O
that O
, O
given O
a O
small O
number O
of O
high O
- O
dimensional O
features O
, O
matrix O
power O
is O
consistent O
with O
shrinkage Method
principle Method
of Method
robust Method
covariance Method
estimation Method
, O
and O
matrix Method
square Method
root Method
can O
be O
derived O
as O
a O
robust Method
covariance Method
estimator Method
via O
a O
von Method
Neumann Method
regularized Method
maximum Method
likelihood Method
estimation Method
. O
It O
is O
also O
shown O
that O
matrix Method
power Method
normalization Method
approximately O
yet O
effectively O
exploits O
geometry O
of O
the O
manifold O
of O
covariance O
matrices O
, O
superior O
to O
matrix Method
logarithm Method
normalization Method
for O
high Task
- Task
dimensional Task
features Task
. O
All O
computations O
of O
MPN Method
- Method
COV Method
meta Method
- Method
layer Method
are O
implemented O
with O
NVIDIA Method
cuBLAS Method
library Method
running O
on O
GPU O
, O
except O
EIG Method
which O
runs O
on O
CPU O
. O
G Method
DeNet Method
is O
concerned O
with O
inserting O
global Method
Gaussian Method
distributions Method
into O
ConvNets Method
for O
end Task
- Task
to Task
- Task
end Task
learning Task
. O
In O
G O
DeNet O
, O
each O
Gaussian O
is O
identified O
as O
square O
root O
of O
a O
symmetric O
positive O
definite O
matrix O
based O
on O
Lie O
group O
structure O
of O
Gaussian O
manifold O
. O
The O
matrix Method
square Method
root Method
plays O
a O
central O
role O
in O
obtaining O
the O
competitive O
performance O
. O
Compact Method
bilinear Method
pooling Method
( O
CBP Method
) O
clarifies O
that O
bilinear Method
pooling Method
is O
closely O
related O
to O
the O
second Method
- Method
order Method
polynomial Method
kernel Method
, O
and O
presents O
two O
compact Method
representations Method
via O
low Method
- Method
dimensional Method
feature Method
maps Method
for O
kernel Method
approximation Method
. O
Kernel Method
pooling Method
approximates O
Gaussian Method
RBF Method
kernel Method
to O
a O
given O
order O
through O
compact O
explicit O
feature O
maps O
, O
aiming O
to O
characterize O
higher O
order O
feature O
interactions O
. O
Cai O
et O
al O
. O
introduce O
a O
polynomial Method
kernel Method
based Method
predictor Method
to O
model O
higher O
- O
order O
statistics O
of O
convolutional O
features O
across O
multiple O
layers O
. O
section O
: O
Proposed O
iSQRT Method
- Method
COV Method
Network O
In O
this O
section O
, O
we O
first O
give O
an O
overview O
of O
the O
proposed O
iSQRT Method
- Method
COV Method
network O
. O
Then O
we O
describe O
matrix Method
square Method
root Method
computation Method
and O
its O
forward Method
propagation Method
. O
We O
finally O
derive O
the O
corresponding O
backward O
gradients O
. O
subsection O
: O
Overview O
of O
Method O
The O
flowchart O
of O
the O
proposed O
network O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
Let O
output O
of O
the O
last O
convolutional Method
layer Method
( O
with O
ReLU Method
) O
be O
a O
tensor O
with O
spatial O
height O
, O
width O
and O
channel O
. O
We O
reshape O
the O
tensor O
to O
a O
feature O
matrix O
consisting O
of O
features O
of O
dimension O
. O
Then O
we O
perform O
second Method
- Method
order Method
pooling Method
by O
computing O
the O
covariance O
matrix O
, O
where O
, O
and O
are O
the O
identity O
matrix O
and O
matrix O
of O
all O
ones O
, O
respectively O
. O
Our O
meta Method
- Method
layer Method
is O
designed O
to O
have O
loop O
- O
embedded O
directed O
graph O
structure O
, O
consisting O
of O
three O
consecutive O
nonlinear Method
structured Method
layers Method
. O
The O
purpose O
of O
the O
first O
layer O
( O
i.e. O
, O
pre Method
- Method
normalization Method
) O
is O
to O
guarantee O
the O
convergence O
of O
the O
following O
Newton Method
- Method
Schulz Method
iteration Method
, O
achieved O
by O
dividing O
the O
covariance O
matrix O
by O
its O
trace O
( O
or O
Frobenius O
norm O
) O
. O
The O
second O
layer O
is O
of O
loop O
structure O
, O
repeating O
the O
coupled Method
matrix Method
equations Method
involved O
in O
Newton Method
- Method
Schulz Method
iteration Method
a O
fixed O
number O
of O
times O
, O
for O
computing O
approximate Task
matrix Task
square Task
root Task
. O
The O
pre O
- O
normalization Task
nontrivially O
changes O
data O
magnitudes O
, O
so O
we O
design O
the O
third O
layer O
( O
i.e. O
, O
post O
- O
compensation O
) O
to O
counteract O
the O
adverse O
effect O
by O
multiplying O
trace O
( O
or O
Frobenius O
norm O
) O
of O
the O
square O
root O
of O
the O
covariance O
matrix O
. O
As O
the O
output O
of O
our O
meta Method
- Method
layer Method
is O
a O
symmetric O
matrix O
, O
we O
concatenate O
its O
upper O
triangular O
entries O
forming O
an O
- O
dimensional O
vector O
, O
submitted O
to O
the O
subsequent O
layer O
of O
the O
ConvNet Method
. O
subsection O
: O
Matrix Method
Square Method
Root Method
and O
Forward Method
Propagation Method
Square O
roots O
of O
matrices O
, O
particularly O
covariance Method
matrices Method
which O
are O
symmetric Method
positive Method
( Method
semi Method
) Method
definite Method
( Method
SPD Method
) Method
, O
find O
applications O
in O
a O
variety O
of O
fields O
including O
computer Task
vision Task
, O
medical Task
imaging Task
and O
chemical Task
physics Task
. O
It O
is O
well O
- O
known O
any O
SPD Method
matrix Method
has O
a O
unique O
square O
root O
which O
can O
be O
computed O
accurately O
by O
EIG Method
or O
SVD Method
. O
Briefly O
, O
let O
be O
an O
SPD Method
matrix Method
and O
it O
has O
EIG Method
, O
where O
is O
orthogonal O
and O
is O
a O
diagonal O
matrix O
of O
eigenvalues O
of O
. O
Then O
has O
a O
square O
root O
, O
i.e. O
, O
. O
Unfortunately O
, O
both O
EIG Method
and O
SVD Method
are O
not O
well O
supported O
on O
GPU O
. O
paragraph O
: O
Newton Method
- Method
Schulz Method
Iteration Method
Higham O
studied O
a O
class O
of O
methods O
for O
iteratively Task
computing Task
matrix Task
square Task
root Task
. O
These O
methods O
, O
termed O
as O
Newton Method
- Method
Padé Method
iterations Method
, O
are O
developed O
based O
on O
the O
connection O
between O
matrix O
sign O
function O
and O
matrix O
square O
root O
, O
together O
with O
rational Method
Padé Method
approximation Method
. O
Specifically O
, O
for O
computing O
the O
square O
root O
of O
, O
given O
and O
, O
for O
, O
the O
coupled Method
iteration Method
takes O
the O
following O
form O
: O
where O
and O
are O
polynomials O
, O
and O
and O
are O
non O
- O
negative O
integers O
. O
Eqn O
. O
( O
[ O
reference O
] O
) O
converges O
only O
locally O
: O
if O
where O
denotes O
any O
induced O
( O
or O
consistent O
) O
matrix O
norm O
, O
and O
quadratically O
converge O
to O
and O
, O
respectively O
. O
The O
family O
of O
coupled Method
iteration Method
is O
stable O
in O
that O
small O
errors O
in O
the O
previous O
iteration O
will O
not O
be O
amplified O
. O
The O
case O
of O
called O
Newton Method
- Method
Schulz Method
iteration Method
fits O
for O
our O
purpose O
as O
no O
GPU Method
unfriendly Method
matrix Method
inverse Method
is O
involved O
: O
Clearly O
Eqn O
. O
( O
[ O
reference O
] O
) O
involves O
only O
matrix Method
product Method
, O
suitable O
for O
parallel Task
implementation Task
on O
GPU O
. O
Compared O
to O
accurate O
square O
root O
computed O
by O
EIG Method
, O
one O
can O
only O
obtain O
approximate Method
solution Method
with O
a O
small O
number O
of O
iterations O
. O
We O
determine O
the O
number O
of O
iterations O
by O
cross Method
- Method
validation Method
. O
Interestingly O
, O
compared O
to O
EIG Method
or O
SVD Method
based O
methods O
, O
experiments O
on O
large Material
- Material
scale Material
ImageNet Material
show O
that O
we O
can O
obtain O
matching O
or O
marginally O
better O
performance O
under O
AlexNet Method
architecture Method
( O
Sec O
. O
[ O
reference O
] O
) O
and O
better O
performance O
under O
ResNet Method
architecture Method
( O
Sec O
. O
[ O
reference O
] O
) O
, O
using O
no O
more O
than O
5 O
iterations O
. O
paragraph O
: O
Pre O
- O
normalization Task
and O
Post Task
- Task
compensation Task
As O
Newton Method
- Method
Schulz Method
iteration Method
only O
converges O
locally O
, O
we O
pre O
- O
normalize O
by O
trace O
or O
Frobenius O
norm O
, O
i.e. O
, O
Let O
be O
eigenvalues O
of O
, O
arranged O
in O
nondecreasing O
order O
. O
As O
and O
, O
it O
is O
easy O
to O
see O
that O
, O
which O
equals O
to O
the O
largest O
singular O
value O
of O
, O
is O
and O
for O
the O
case O
of O
trace Metric
and Metric
Frobenius Metric
norm Metric
, O
respectively O
, O
both O
less O
than O
1 O
. O
Hence O
, O
the O
convergence O
condition O
is O
satisfied O
. O
The O
above O
pre Method
- Method
normalization Method
of Method
covariance Method
matrix Method
nontrivially O
changes O
the O
data O
magnitudes O
such O
that O
it O
produces O
adverse O
effect O
on O
network O
. O
Hence O
, O
to O
counteract O
this O
change O
, O
after O
the O
Newton Method
- Method
Schulz Method
iteration Method
, O
we O
accordingly O
perform O
post Method
- Method
compensation Method
, O
i.e. O
, O
An O
alternative O
scheme O
to O
counterbalance O
the O
influence O
incurred O
by O
pre Method
- Method
normalization Method
is O
Batch Method
Normalization Method
( O
BN Method
) O
. O
One O
may O
even O
consider O
without O
using O
any O
post Method
- Method
compensation Method
. O
However O
, O
our O
experiment O
on O
ImageNet Material
has O
shown O
that O
, O
without O
post Method
- Method
normalization Method
, O
prevalent O
ResNet Method
fails O
to O
converge O
, O
while O
our O
scheme O
outperforms O
BN Method
by O
about O
1 O
% O
( O
see O
[ O
reference O
] O
for O
details O
) O
. O
subsection O
: O
Backward Method
Propagation Method
( O
BP Method
) O
The O
gradients O
associated O
with O
the O
structured Method
layers Method
are O
derived O
using O
matrix Method
backpropagation Method
methodology Method
, O
which O
establishes O
the O
chain O
rule O
of O
a O
general Method
matrix Method
function Method
by O
first Method
- Method
order Method
Taylor Method
approximation Method
. O
Below O
we O
take O
pre O
- O
normalization O
by O
trace O
as O
an O
example O
, O
deriving O
the O
corresponding O
gradients O
. O
paragraph O
: O
BP Task
of Task
Post Task
- Task
compensation Task
Given O
where O
is O
the O
loss O
function O
, O
the O
chain Method
rule Method
is O
of O
the O
form O
where O
denotes O
variation O
of O
. O
After O
some O
manipulations O
, O
we O
have O
paragraph O
: O
BP Method
of Method
Newton Method
- Method
Schulz Method
Iteration Method
Then O
we O
are O
to O
compute O
the O
partial O
derivatives O
of O
the O
loss O
function O
with O
respect O
to O
and O
, O
, O
given O
computed O
by O
Eqn O
. O
( O
[ O
reference O
] O
) O
and O
. O
As O
the O
covariance O
matrix O
is O
symmetric O
, O
it O
is O
easy O
to O
see O
from O
Eqn O
. O
( O
[ O
reference O
] O
) O
that O
and O
are O
both O
symmetric O
. O
According O
to O
the O
chain O
rules O
( O
omitted O
hereafter O
for O
simplicity O
) O
of O
matrix Method
backpropagation Method
and O
after O
some O
manipulations O
, O
, O
we O
can O
derive O
The O
final O
step O
of O
this O
layer O
is O
concerned O
with O
the O
partial O
derivative O
with O
respect O
to O
, O
which O
is O
given O
by O
paragraph O
: O
BP O
of O
Pre Method
- Method
normalization Method
Note O
that O
here O
we O
need O
to O
combine O
the O
gradient O
of O
the O
loss O
function O
with O
respect O
to O
, O
backpropagated O
from O
the O
post Method
- Method
compensation Method
layer Method
. O
As O
such O
, O
by O
referring O
to O
Eqn O
. O
( O
[ O
reference O
] O
) O
, O
we O
make O
similar O
derivations O
as O
before O
and O
obtain O
If O
we O
adopt O
pre O
- O
normalization O
by O
Frobenius O
norm O
, O
the O
gradients O
associated O
with O
post Method
- Method
compensation Method
become O
and O
that O
with O
respect O
to O
pre O
- O
normalization O
is O
while O
the O
backward O
gradients O
of O
Newton Method
- Method
Schulz Method
iteration Method
( O
[ O
reference O
] O
) O
keep O
unchanged O
. O
Finally O
, O
given O
, O
one O
can O
derive O
the O
gradient O
of O
the O
loss O
function O
with O
respect O
to O
input O
matrix O
, O
which O
takes O
the O
following O
form O
: O
section O
: O
Experiments O
We O
evaluate O
the O
proposed O
method O
on O
both O
large Task
- Task
scale Task
image Task
classification Task
and O
challenging Task
fine Task
- Task
grained Task
visual Task
categorization Task
tasks Task
. O
We O
make O
experiments O
using O
two O
PCs Method
each O
of O
which O
is O
equipped O
with O
a O
4 O
- O
core O
Intel O
i7 O
- O
4790k@4.0GHz O
CPU O
, O
32 O
G O
RAM O
, O
512 O
GB O
Samsung Method
PRO Method
SSD Method
and O
two O
Titan Method
Xp Method
GPUs Method
. O
We O
implement O
our O
networks O
using O
MatConvNet Method
and O
Matlab2015b Method
, O
under O
Ubuntu Method
14.04.5 Method
LTS Method
. O
subsection O
: O
Datasets O
and O
Our O
Meta Method
- Method
layer Method
Implementation Method
Datasets O
For O
large Task
- Task
scale Task
image Task
classification Task
, O
we O
adopt O
ImageNet Material
LSVRC2012 Material
dataset Material
with O
1 O
, O
000 O
object O
categories O
. O
The O
dataset O
contains O
1.28 O
M O
images O
for O
training O
, O
50 O
K O
images O
for O
validation Task
and O
100 O
K O
images O
for O
testing O
( O
without O
published O
labels O
) O
. O
As O
in O
, O
we O
report O
the O
results O
on O
the O
validation O
set O
. O
For O
fine Task
- Task
grained Task
categorization Task
, O
we O
use O
three O
popular O
fine O
- O
grained O
benchmarks O
, O
i.e. O
, O
CUB Material
- Material
200 Material
- Material
2011 Material
( O
Birds Material
) O
, O
FGVC Material
- Material
aircraft Material
( O
Aircrafts Material
) O
and O
Stanford Material
cars Material
( O
Cars Material
) O
. O
The O
Birds Material
dataset Material
contains O
11 O
, O
788 O
images O
from O
200 O
species O
, O
with O
large O
intra O
- O
class O
variation O
but O
small O
inter O
- O
class O
variation O
. O
The O
Aircrafts Material
dataset O
includes O
100 O
aircraft Material
classes Material
and O
a O
total O
of O
10 O
, O
000 O
images O
with O
small O
background O
noise O
but O
higher O
inter O
- O
class O
similarity O
. O
The O
Cars Material
dataset O
consists O
of O
16 O
, O
185 O
images O
from O
196 O
classes O
. O
For O
all O
datasets O
, O
we O
adopt O
the O
provided O
training O
/ O
test O
split O
, O
using O
neither O
bounding O
boxes O
nor O
part O
annotations O
. O
Implementation O
of O
iSQRT Method
- Method
COV Method
Meta O
- O
layer O
We O
encapsulate O
our O
code O
in O
three O
computational O
blocks O
, O
which O
implement O
forward Method
& Method
backward Method
computation Method
of Method
pre Method
- Method
normalization Method
layer Method
, O
Newton Method
- Method
Schulz Method
iteration Method
layer Method
and O
post Method
- Method
compensation Method
layer Method
, O
respectively O
. O
The O
code O
is O
written O
in O
C Method
++ Method
based O
on O
NVIDIA Method
on O
top O
of O
CUDA Method
toolkit Method
8.0 Method
. O
In O
addition O
, O
we O
write O
code O
in O
C Method
++ Method
based O
on O
cuBLAS Method
for O
computing Task
covariance Task
matrices Task
. O
We O
create O
MEX Method
files Method
so O
that O
the O
above O
subroutines O
can O
be O
called O
in O
Matlab O
environment O
. O
For O
AlexNet Task
, O
we O
insert O
our O
meta Method
- Method
layer Method
after O
the O
last O
convolution Method
layer Method
( O
with O
ReLU Method
) O
, O
which O
outputs O
an O
tensor O
. O
For O
ResNet Method
architecture Method
, O
as O
suggested O
, O
we O
do O
not O
perform O
downsampling O
for O
the O
last O
set O
of O
convolutional O
blocks O
, O
and O
add O
one O
convolution Method
with O
channels O
after O
the O
last O
sum Method
layer Method
( O
with O
ReLU Method
) O
. O
The O
added O
convolution Method
layer Method
outputs O
an O
tensor O
. O
Hence O
, O
with O
both O
architectures O
, O
the O
covariance O
matrix O
is O
of O
size O
and O
our O
meta Method
- Method
layer Method
outputs O
an O
- O
dimensional O
vector O
as O
the O
image Method
representation Method
. O
subsection O
: O
Evaluation O
with O
AlexNet Method
on O
ImageNet Task
In O
the O
first O
part O
of O
experiments O
, O
we O
analyze O
, O
with O
AlexNet Method
architecture Method
, O
the O
design O
choices O
of O
our O
iSQRT Method
- Method
COV Method
method O
, O
including O
the O
number O
of O
Newton Method
- Method
Schulz Method
iterations Method
, O
time Metric
and Metric
memory Metric
usage Metric
, O
and O
behaviors O
of O
different O
pre Method
- Method
normalization Method
methods Method
. O
We O
select O
AlexNet Method
because O
it O
runs O
faster O
with O
shallower O
depth O
, O
and O
the O
results O
can O
extrapolate O
to O
deeper Method
networks Method
which O
mostly O
follow O
its O
architecture O
design O
. O
We O
follow O
for O
color Task
augmentation Task
and O
weight Task
initialization Task
, O
adopting O
BN Method
and Method
no Method
dropout Method
. O
We O
use O
SGD Method
with O
a O
mini O
- O
batch O
of O
128 O
, O
unless O
otherwise O
stated O
. O
The O
momentum O
is O
0.9 O
and O
weight O
decay O
is O
0.0005 O
. O
We O
train O
iSQRT Method
- Method
COV Method
networks O
from O
scratch O
in O
20 O
epochs O
where O
learning Metric
rate Metric
follows O
exponential Method
decay Method
. O
All O
training O
and O
test O
images O
are O
uniformly O
resized O
with O
shorter O
sides O
of O
256 O
. O
During O
training O
we O
randomly O
crop O
a O
patch O
from O
each O
image O
or O
its O
horizontal O
flip O
. O
We O
make O
inference Task
on O
one O
single O
center O
crop O
from O
a O
test O
image O
. O
Impact O
of O
Number O
N O
of O
Newton Method
- Method
Schulz Method
Iterations Method
Fig O
. O
[ O
reference O
] O
shows O
top Metric
- Metric
1 Metric
error Metric
rate Metric
as O
a O
function O
of O
number O
of O
Newton O
- O
Schulz O
iterations O
in O
Eqn O
. O
( O
[ O
reference O
] O
) O
. O
Plain Method
- Method
COV Method
indicates O
simple O
covariance Method
pooling Method
without O
any O
normalization Method
. O
With O
one O
single O
iteration O
, O
our O
method O
outperforms O
Plain Method
- Method
COV Method
by O
. O
As O
iteration O
number O
grows O
, O
the O
error Metric
rate Metric
of O
iSQRT Method
- Method
COV Method
gradually O
declines O
. O
With O
3 O
iterations O
, O
iSQRT Method
- Method
COV Method
is O
comparable O
to O
MPN Method
- Method
COV Method
, O
having O
only O
0.3 O
% O
higher O
error Metric
rate Metric
, O
while O
performing O
marginally O
better O
than O
MPN Method
- Method
COV Method
between O
5 O
and O
7 O
iterations O
. O
After O
, O
the O
error Metric
rate Metric
consistently O
increases O
, O
indicating O
growth O
of O
iteration O
number O
is O
not O
helpful O
for O
improving O
accuracy Metric
. O
As O
larger O
incurs O
higher O
computational Metric
cost Metric
, O
to O
balance O
efficiency O
and O
accuracy Metric
, O
we O
set O
to O
5 O
in O
the O
remaining O
experiments O
. O
Notably O
, O
the O
approximate Method
square Method
root Method
normalization Method
improves O
a O
little O
over O
the O
accurate O
one O
obtained O
via O
EIG Method
. O
This O
interesting O
problem O
will O
be O
discussed O
in O
Sec O
. O
[ O
reference O
] O
, O
where O
iSQRT Method
- Method
COV Method
is O
further O
evaluated O
on O
substantially O
deeper O
ResNets O
. O
C Method
++ Method
N O
/ O
A O
Impro O
. O
B O
- O
CNN O
SVD Method
or O
EIG Method
CUDA O
cuSOLVER O
Matlab O
( O
CPU O
function O
) O
Matlab O
( O
GPU O
function O
) O
Time Method
and Method
Memory Method
Analysis Method
We O
compare O
time Metric
and O
memory Metric
consumed O
by O
single O
meta Method
- Method
layer Method
of O
different O
methods O
. O
We O
use O
public O
code O
for O
, O
and O
released O
by O
the O
respective O
authors O
. O
As O
shown O
in O
Tab O
. O
[ O
reference O
] O
table O
: O
single O
- O
layer O
, O
iSQRT Method
- Method
COV Method
( O
) O
and O
iSQRT Method
- Method
COV Method
( O
) O
are O
3.1x O
faster O
and O
1.8x O
faster O
than O
MPN Method
- Method
COV Method
, O
respectively O
. O
Furthermore O
, O
iSQRT Method
- Method
COV Method
( O
) O
is O
five O
times O
more O
efficient O
than O
improved O
B Method
- Method
CNN Method
and O
G Method
DeNet Method
. O
For O
improved O
B Task
- Task
CNN Task
, O
the O
forward Task
computation Task
of O
Newton Method
- Method
Schulz Method
( Method
NS Method
) Method
iteration Method
is O
much O
faster O
than O
that O
of O
SVD Method
, O
but O
the O
total O
time O
of O
two O
methods O
is O
comparable O
. O
The O
authors O
of O
improved O
B Method
- Method
CNN Method
also O
proposed O
two O
other O
implementations O
, O
i.e. O
, O
FP Method
by O
NS Method
iteration Method
plus O
BP Method
by O
SVD Method
and O
FP Method
by O
SVD Method
plus O
BP Method
by O
Lyapunov Method
( O
Lyap Method
. O
) O
, O
which O
take O
15.31 O
( O
2.09 O
) O
and O
12.21 O
( O
11.19 O
) O
, O
respectively O
. O
We O
observe O
that O
, O
in O
any O
case O
, O
the O
forward O
backward O
time O
taken O
by O
single O
meta Method
- Method
layer Method
of O
improved Method
B Method
- Method
CNN Method
is O
significant O
as O
GPU O
unfriendly O
SVD Method
or O
EIG Method
can O
not O
be O
avoided O
, O
even O
though O
the O
forward Method
computation Method
is O
very O
efficient O
when O
NS Method
iteration Method
is O
used O
. O
Tab O
. O
[ O
reference O
] O
table O
: O
time O
- O
matrix Method
- Method
decomposition Method
presents O
running Metric
time Metric
of O
EIG Method
and O
SVD Method
of O
an O
covariance Method
matrix Method
. O
Matlab Method
( O
M O
) O
built O
- O
in O
CPU O
functions O
and O
GPU Method
functions Method
deliver O
over O
10x O
and O
2.1x O
speedups O
over O
their O
CUDA Method
counterparts Method
, O
respectively O
. O
Our O
method O
needs O
to O
store O
and O
in O
Eqn O
. O
( O
[ O
reference O
] O
) O
which O
will O
be O
used O
in O
backpropagation Method
, O
taking O
up O
more O
memory O
than O
EIG Method
or O
SVD Method
based O
ones O
. O
Among O
all O
, O
our O
iSQRT Method
- Method
COV Method
( O
) O
takes O
up O
the O
largest O
memory O
of O
1.129 O
MB O
, O
which O
is O
insignificant O
compared O
to O
12 O
GB O
memory O
on O
a O
Titan O
Xp O
. O
Note O
that O
for O
network Task
inference Task
only O
, O
our O
method O
takes O
0.125 O
MB O
memory O
as O
it O
is O
unnecessary O
to O
store O
and O
. O
Next O
, O
we O
compare O
in O
Fig O
. O
[ O
reference O
] O
speed O
of O
network Method
training Method
between O
MPN Method
- Method
COV Method
and O
iSQRT Method
- Method
COV Method
with O
both O
one O
- O
GPU O
and O
two O
- O
GPU Method
configurations Method
. O
For O
one O
- O
GPU O
configuration O
, O
the O
speed Metric
gap Metric
vs. O
batch Metric
size Metric
between O
the O
two O
methods O
keeps O
nearly O
constant O
. O
For O
two O
- O
GPU O
configuration O
, O
their O
speed Metric
gap Metric
becomes O
more O
significant O
when O
batch O
size O
gets O
larger O
. O
As O
can O
be O
seen O
, O
the O
speed O
of O
iSQRT Method
- Method
COV Method
network O
continuously O
grows O
with O
increase O
of O
batch O
size O
while O
that O
of O
MPN Method
- Method
COV Method
tends O
to O
saturate O
when O
batch O
size O
is O
larger O
than O
512 O
. O
Clearly O
our O
iSQRT Method
- Method
COV Method
network O
can O
make O
better O
use O
of O
computing O
power O
of O
multiple O
GPUs Method
than O
MPN Method
- Method
COV Method
. O
Pre O
- O
normalization O
by O
Trace O
vs. O
by O
Frobenius O
Norm O
Sec O
. O
[ O
reference O
] O
describes O
two O
pre Method
- Method
normalization Method
methods Method
. O
Here O
we O
compare O
them O
in O
Tab O
. O
[ O
reference O
] O
( O
bottom O
rows O
) O
, O
where O
iSQRT Method
- Method
COV Method
( O
trace Method
) O
indicates O
pre Method
- Method
normalization Method
by O
trace O
. O
We O
can O
see O
that O
pre O
- O
normalization Method
by O
trace Method
produces O
0.3 O
% O
lower O
error Metric
rate Metric
than O
that O
by O
Frobenius Method
norm Method
, O
while O
taking O
similar O
time O
with O
the O
latter O
. O
Hence O
, O
in O
all O
the O
remaining O
experiments O
, O
we O
adopt O
trace Method
based Method
pre Method
- Method
normalization Method
method Method
. O
Comparison O
with O
Other O
Covariance Method
Pooling Method
Methods Method
We O
compare O
iSQRT Method
- Method
COV Method
with O
other O
covariance Method
pooling Method
methods Method
, O
as O
shown O
in O
Tab O
. O
[ O
reference O
] O
. O
The O
results O
of O
MPN Method
- Method
COV Method
, O
B Method
- Method
CNN Method
and O
DeepO Method
P Method
are O
duplicated O
from O
. O
We O
train O
from O
scratch O
G Method
DeNet Method
and O
improved O
B Method
- Method
CNN Method
on O
ImageNet Material
. O
We O
use O
the O
most O
efficient O
implementation O
of O
improved O
B Method
- Method
CNN Method
, O
i.e. O
, O
FP Method
by O
SVD Method
and O
BP Method
by O
Lyap Method
. O
, O
and O
we O
mention O
all O
implementations O
of O
improved O
B Method
- Method
CNN Method
produce O
similar O
results O
. O
Our O
iSQRT Method
- Method
COV Method
using O
pre Method
- Method
normalization Method
by O
trace Method
is O
marginally O
better O
than O
MPN Method
- Method
COV Method
. O
All O
matrix Method
square Method
root Method
normalization Method
methods Method
except O
improved O
B Method
- Method
CNN Method
outperform O
B Method
- Method
CNN Method
and O
DeepO Method
P. Method
Since O
improved O
B Method
- Method
CNN Method
is O
identical O
to O
MPN Method
- Method
COV Method
if O
element Method
- Method
wise Method
square Method
root Method
normalization Method
and O
normalization Method
are O
neglected O
, O
its O
unsatisfactory O
performance O
suggests O
that O
, O
after O
matrix Method
square Method
root Method
normalization Method
, O
further O
element Method
- Method
wise Method
square Method
root Method
normalization Method
and O
normalization Method
hurt O
large Task
- Task
scale Task
ImageNet Task
classification Task
. O
This O
is O
consistent O
with O
the O
observation O
in O
, O
where O
after O
matrix Method
power Method
normalization Method
, O
additional O
normalization Method
by O
Frobenius Method
norm Method
or O
matrix Method
norm Method
makes O
performance O
decline O
. O
subsection O
: O
Results O
on O
ImageNet Method
with Method
ResNet Method
Architecture Method
This O
section O
evaluates O
iSQRT Method
- Method
COV Method
with O
ResNet Method
architecture Method
. O
We O
follow O
for O
color Task
augmentation Task
and O
weight Task
initialization Task
. O
We O
rescale O
each O
training O
image O
with O
its O
shorter O
side O
randomly O
sampled O
on O
. O
The O
fixed O
- O
size O
patch O
is O
randomly O
cropped O
from O
the O
rescaled O
image O
or O
its O
horizontal O
flip O
. O
We O
rescale O
each O
test O
image O
with O
a O
shorter O
side O
of O
256 O
and O
evaluate O
a O
single O
center O
crop O
for O
inference Task
. O
We O
use O
SGD Method
with O
a O
mini O
- O
batch O
size O
of O
256 O
, O
a O
weight O
decay O
of O
0.0001 O
and O
a O
momentum O
of O
0.9 O
. O
We O
train O
iSQRT Method
- Method
COV Method
networks O
from O
scratch O
in O
60 O
epochs O
, O
initializing O
the O
learning Metric
rate Metric
to O
which O
is O
divided O
by O
10 O
at O
epoch O
30 O
and O
45 O
, O
respectively O
. O
Significance O
of O
Post Method
- Method
compensation Method
Rather O
than O
our O
post Method
- Method
compensation Method
scheme Method
, O
one O
may O
choose O
Batch Method
Normalization Method
( O
BN Method
) O
or O
simply O
do O
nothing O
( O
i.e. O
, O
without O
post Method
- Method
compensation Method
) O
. O
He O
et O
al O
. O
FBN Method
SORT Method
MPN O
- O
COV O
iSQRT Method
- Method
COV Method
He O
et O
al O
. O
iSQRT Method
- Method
COV Method
He O
et O
al O
. O
Tab O
. O
[ O
reference O
] O
summarizes O
impact O
of O
different O
schemes O
on O
iSQRT Method
- Method
COV Method
network O
with O
ResNet Method
- Method
50 Method
architecture Method
. O
Without O
post Method
- Method
compensation Method
, O
iSQRT Method
- Method
COV Method
network O
fails O
to O
converge O
. O
Careful O
observations O
show O
that O
in O
this O
case O
the O
gradients O
are O
very O
small O
( O
on O
the O
order O
of O
) O
, O
and O
largely O
tuning O
of O
learning Metric
rate Metric
helps O
little O
. O
Option O
of O
BN Method
helps O
the O
network O
converge O
, O
but O
producing O
about O
1 O
% O
higher O
top Metric
- Metric
1 Metric
error Metric
rate Metric
than O
our O
post Method
- Method
compensation Method
scheme Method
. O
The O
comparison O
above O
suggests O
that O
our O
post Method
- Method
compensation Method
scheme Method
is O
essential O
for O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
. O
paragraph O
: O
Fast O
Convergence O
of O
iSQRT Method
- Method
COV Method
Network O
We O
compare O
convergence O
of O
iSQRT Method
- Method
COV Method
and O
MPN O
- O
COV O
with O
ResNet Method
- Method
50 Method
architecture Method
, O
as O
well O
as O
the O
original O
ResNet Method
- Method
50 Method
in O
which O
global Method
average Method
pooling Method
is O
performed O
after O
the O
last O
convolution O
layer O
. O
Fig O
. O
[ O
reference O
] O
presents O
the O
convergence O
curves O
. O
Compared O
to O
the O
original O
ResNet Method
- Method
50 Method
, O
the O
convergence Metric
of O
both O
iSQRT Method
- Method
COV Method
and O
MPN Method
- Method
COV Method
is O
significantly O
faster O
. O
We O
observe O
that O
iSQRT Method
- Method
COV Method
can O
converge O
well O
within O
60 O
epochs O
, O
achieving O
top Metric
- Metric
1 Metric
error Metric
rate Metric
of O
22.14 O
% O
, O
0.6 O
% O
lower O
than O
MPN Method
- Method
COV Method
. O
We O
also O
trained O
iSQRT Method
- Method
COV Method
with O
90 O
epochs O
using O
same O
setting O
with O
MPN Method
- Method
COV Method
, O
obtaining O
top Metric
- Metric
5 Metric
error Metric
of O
6.12 O
% O
, O
slightly O
lower O
than O
that O
with O
60 O
epochs O
( O
6.22 O
% O
) O
. O
This O
indicates O
iSQRT Method
- Method
COV Method
can O
converge O
in O
less O
epochs O
, O
so O
further O
accelerating Task
training Task
, O
as O
opposed O
to O
MPN Method
- Method
COV Method
. O
The O
fast Metric
convergence Metric
property Metric
of O
iSQRT Method
- Method
COV Method
is O
appealing O
. O
As O
far O
as O
we O
know O
, O
previous O
networks O
with O
ResNet Method
- Method
50 Method
architecture Method
require O
at O
least O
90 O
epochs O
to O
converge O
to O
competitive O
results O
. O
Comparison O
with O
State O
- O
of O
- O
the O
- O
arts O
In O
Tab O
. O
[ O
reference O
] O
, O
we O
compare O
our O
method O
with O
other O
second Method
- Method
order Method
networks Method
, O
as O
well O
as O
the O
original O
ResNets Method
. O
With O
ResNet Method
- Method
50 Method
architecture Method
, O
all O
the O
second Method
- Method
order Method
networks Method
improve O
over O
the O
first O
- O
order O
one O
while O
our O
method O
performing O
best O
. O
MPN Method
- Method
COV Method
and O
iSQRT Method
- Method
COV Method
, O
both O
of O
which O
involve O
square Method
root Method
normalization Method
, O
are O
superior O
to O
FBN Method
which O
uses O
no O
normalization O
and O
SORT Method
which O
introduces O
dot Method
product Method
transform Method
in O
the O
linear Method
sum Method
of Method
two Method
- Method
branch Method
module Method
followed O
by O
element Method
- Method
wise Method
normalization Method
. O
Moreover O
, O
our O
iSQRT Method
- Method
COV Method
outperforms O
MPN Method
- Method
COV Method
by O
0.6 O
% O
in O
top Metric
- Metric
1 Metric
error Metric
. O
Note O
that O
our O
50 O
- O
layer O
iSQRT Method
- Method
COV Method
network O
achieves O
lower O
error Metric
rate Metric
than O
much O
deeper O
ResNet Method
- Method
101 Method
and O
ResNet Method
- Method
152 Method
, O
while O
our O
101 O
- O
layer O
iSQRT Method
- Method
COV Method
network O
outperforming O
the O
original O
ResNet Method
- Method
101 Method
by O
2.4 O
% O
and O
ResNet Method
- Method
152 Method
by O
1.8 O
% O
, O
respectively O
. O
Why O
Approximate Method
Square Method
Root Method
Performs O
Better O
Fig O
. O
[ O
reference O
] O
shows O
that O
more O
iterations O
which O
lead O
to O
more O
accurate O
square O
root O
is O
not O
helpful O
for O
iSQRT Method
- Method
COV Method
with O
AlexNet Method
. O
From O
Tab O
. O
[ O
reference O
] O
, O
we O
observe O
that O
iSQRT Method
- Method
COV Method
with O
ResNet Method
computing Method
approximate Method
square Method
root Method
performs O
better O
than O
MPN Method
- Method
COV Method
which O
can O
obtain O
exact O
square O
root O
by O
EIG Method
. O
Recall O
that O
, O
for O
covariance Task
pooling Task
ConvNets Task
, O
we O
face O
the O
problem O
of O
small O
sample O
of O
large O
dimensionality O
, O
and O
matrix Method
square Method
root Method
is O
consistent O
with O
general O
shrinkage Method
principle Method
of Method
robust Method
covariance Method
estimation Method
. O
Hence O
, O
we O
conjuncture O
that O
approximate Method
matrix Method
square Method
root Method
may O
be O
a O
better O
robust Method
covariance Method
estimator Method
than O
the O
exact O
square Method
root Method
. O
Despite O
this O
analysis O
, O
we O
think O
this O
problem O
is O
worth O
future O
research O
. O
Compactness O
of O
iSQRT Method
- Method
COV Method
Our O
iSQRT Method
- Method
COV Method
outputs O
32k Method
- Method
dimensional Method
representation Method
which O
is O
high O
. O
Here O
we O
consider O
to O
compress O
this O
representation O
. O
Compactness Method
by O
PCA Method
is O
not O
viable O
since O
obtaining O
the O
principal Method
components Method
on O
ImageNet Material
is O
too O
expensive O
. O
CBP Method
is O
not O
applicable O
to O
our O
iSQRT Method
- Method
COV Method
as O
well O
, O
as O
it O
does O
not O
explicitly O
estimate O
the O
covariance O
matrix O
. O
We O
propose O
a O
simple O
scheme O
, O
which O
decreases O
the O
dimension Metric
( O
dim O
. O
) O
of O
covariance Method
representation Method
by O
lowering O
the O
number O
of O
channels O
of O
convolutional Method
layer Method
before O
our O
covariance Method
pooling Method
. O
Tab O
. O
[ O
reference O
] O
summarizes O
results O
of O
compact O
iSQRT Method
- Method
COV Method
. O
The O
recognition Metric
error Metric
increases O
slightly O
( O
) O
when O
decreases O
from O
256 O
to O
128 O
( O
correspondingly O
, O
dim O
. O
of O
image Method
representation Method
) O
. O
The O
error Metric
rate Metric
is O
23.73 O
if O
the O
dimension O
is O
compressed O
to O
2 O
K O
, O
still O
outperforming O
the O
original O
ResNet Method
- Method
50 Method
which O
performs O
global Method
average Method
pooling Method
. O
subsection O
: O
Fine Task
- Task
grained Task
Visual Task
Categorization Task
( O
FGVC Method
) O
Finally O
, O
we O
apply O
iSQRT Method
- Method
COV Method
models O
pre O
- O
trained O
on O
ImageNet Method
to O
FGVC Method
. O
For O
fair O
comparison O
, O
we O
follow O
for O
experimental O
setting O
and O
evaluation O
protocol O
. O
On O
all O
datasets O
, O
we O
crop O
patches O
as O
input O
images O
. O
We O
replace O
1000 Method
- Method
way Method
softmax Method
layer Method
of O
a O
pre O
- O
trained O
iSQRT Method
- Method
COV Method
model O
by O
a O
k Method
- Method
way Method
softmax Method
layer Method
, O
where O
is O
number O
of O
classes O
in O
the O
fine O
- O
grained O
dataset O
, O
and O
finetune O
the O
network O
using O
SGD Method
with O
momentum O
of O
0.9 O
for O
50 O
100 O
epochs O
with O
a O
small O
learning Metric
rate Metric
( O
) O
for O
all O
layers O
except O
the O
fully O
- O
connected O
layer O
, O
which O
is O
set O
to O
. O
We O
use O
horizontal O
flipping O
as O
data Task
augmentation Task
. O
After O
finetuning O
, O
the O
outputs O
of O
iSQRT Method
- Method
COV Method
layer O
are O
normalized O
before O
inputted O
to O
train O
one Method
- Method
vs Method
- Method
all Method
linear Method
SVMs Method
with O
hyperparameter Method
. O
We O
predict O
the O
label O
of O
a O
test O
image O
by O
averaging O
SVM O
scores O
of O
the O
image O
and O
its O
horizontal O
flip O
. O
ResNet O
- O
50 O
iSQRT Method
- Method
COV Method
VGG O
- O
D O
Improved O
B O
- O
CNN O
Tab O
. O
[ O
reference O
] O
presents O
classification Task
results O
of O
different O
methods O
, O
where O
column O
3 O
lists O
the O
dimension O
of O
the O
corresponding O
representation O
. O
With O
ResNet Method
- Method
50 Method
architecture Method
, O
KP Method
performs O
much O
better O
than O
CBP Method
, O
while O
iSQRT Method
- Method
COV Method
( O
8 O
K O
) O
respectively O
outperforms O
KP Method
( O
14 O
K O
) O
by O
about O
2.6 O
% O
, O
3.8 O
% O
and O
0.6 O
% O
on O
Birds Material
, O
Aircrafts Material
and O
Cars Material
, O
and O
iSQRT Method
- Method
COV Method
( O
32 O
K O
) O
further O
improves O
accuracy Metric
. O
Note O
that O
KP Method
combines O
first O
- O
order O
up O
to O
fourth O
- O
order O
statistics O
while O
iSQRT Method
- Method
COV Method
only O
exploits O
second O
- O
order O
one O
. O
With O
VGG Method
- Method
D Method
, O
iSQRT Method
- Method
COV Method
( O
32k O
) O
matches O
or O
outperforms O
state O
- O
of O
- O
the O
- O
art O
competitors O
, O
but O
inferior O
to O
iSQRT Method
- Method
COV Method
( O
32k O
) O
with O
ResNet Method
- Method
50 Method
. O
On O
all O
fine O
- O
grained O
datasets O
, O
KP Method
and O
CBP Method
with O
16 Method
- Method
layer Method
VGG Method
- Method
D Method
perform O
better O
than O
their O
counterparts O
with O
50 Method
- Method
layer Method
ResNet Method
, O
despite O
the O
fact O
that O
ResNet Method
- Method
50 Method
significantly O
outperforms O
VGG Method
- Method
D Method
on O
ImageNet Material
. O
The O
reason O
may O
be O
that O
the O
last O
convolution Method
layer Method
of O
pre O
- O
trained O
ResNet Method
- Method
50 Method
outputs O
2048 O
- O
dimensional O
features O
, O
much O
higher O
than O
512 Method
- Method
dimensional Method
one Method
of Method
VGG Method
- Method
D Method
, O
which O
are O
not O
suitable O
for O
existing O
second Method
- Method
or Method
higher Method
- Method
order Method
pooling Method
methods Method
. O
Different O
from O
all O
existing O
methods O
which O
use O
models O
pre O
- O
trained O
on O
ImageNet Material
with O
first O
- O
order O
information O
, O
our O
pre O
- O
trained Method
models Method
are O
of O
second O
- O
order O
. O
Using O
pre O
- O
trained O
iSQRT Method
- Method
COV Method
models O
with O
ResNet Method
- Method
50 Method
, O
we O
achieve O
recognition Task
results O
superior O
to O
all O
the O
compared O
methods O
, O
and O
furthermore O
, O
establish O
state O
- O
of O
- O
the O
- O
art O
results O
on O
three O
fine O
- O
grained O
benchmarks O
using O
iSQRT Method
- Method
COV Method
model O
with O
ResNet Method
- Method
101 Method
. O
section O
: O
Conclusion O
We O
presented O
an O
iterative Method
matrix Method
square Method
root Method
normalization Method
of Method
covariance Method
pooling Method
( O
iSQRT Method
- Method
COV Method
) O
network O
which O
can O
be O
trained O
end O
- O
to O
- O
end O
. O
Compared O
to O
existing O
works O
depending O
heavily O
on O
GPU O
unfriendly O
EIG Method
or O
SVD Method
, O
our O
method O
, O
based O
on O
coupled Method
Newton Method
- Method
Schulz Method
iteration Method
, O
runs O
much O
faster O
as O
it O
involves O
only O
matrix Method
multiplications Method
, O
suitable O
for O
parallel Task
implementation Task
on O
GPU O
. O
We O
validated O
our O
method O
on O
both O
large Material
- Material
scale Material
ImageNet Material
dataset Material
and O
challenging O
fine O
- O
grained O
benchmarks O
. O
Given O
efficiency O
and O
promising O
performance O
of O
our O
iSQRT Method
- Method
COV Method
, O
we O
hope O
global Method
covariance Method
pooling Method
will O
be O
a O
promising O
alternative O
to O
global Method
average Method
pooling Method
in O
other O
deep Method
network Method
architectures Method
, O
e.g. O
, O
ResNeXt Method
, O
Inception Method
and O
DenseNet Method
. O
bibliography O
: O
References O
