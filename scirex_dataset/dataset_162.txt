Top	O
-	O
performing	O
deep	Method
architectures	Method
are	O
trained	O
on	O
massive	O
amounts	O
of	O
labeled	O
data	O
.	O
In	O
the	O
absence	O
of	O
labeled	O
data	O
for	O
a	O
certain	O
task	O
,	O
domain	Method
adaptation	Method
often	O
provides	O
an	O
attractive	O
option	O
given	O
that	O
labeled	O
data	O
of	O
similar	O
nature	O
but	O
from	O
a	O
different	O
domain	O
(	O
e.g.	O
synthetic	O
images	O
)	O
are	O
available	O
.	O
Here	O
,	O
we	O
propose	O
a	O
new	O
approach	O
to	O
domain	Method
adaptation	Method
in	O
deep	Method
architectures	Method
that	O
can	O
be	O
trained	O
on	O
large	O
amount	O
of	O
labeled	O
data	O
from	O
the	O
source	O
domain	O
and	O
large	O
amount	O
of	O
unlabeled	O
data	O
from	O
the	O
target	O
domain	O
(	O
no	O
labeled	O
target	O
-	O
domain	O
data	O
is	O
necessary	O
)	O
.	O
As	O
the	O
training	O
progresses	O
,	O
the	O
approach	O
promotes	O
the	O
emergence	O
of	O
“	O
deep	O
”	O
features	O
that	O
are	O
(	O
i	O
)	O
discriminative	O
for	O
the	O
main	O
learning	Task
task	Task
on	O
the	O
source	O
domain	O
and	O
(	O
ii	O
)	O
invariant	O
with	O
respect	O
to	O
the	O
shift	O
between	O
the	O
domains	O
.	O
We	O
show	O
that	O
this	O
adaptation	O
behaviour	O
can	O
be	O
achieved	O
in	O
almost	O
any	O
feed	Method
-	Method
forward	Method
model	Method
by	O
augmenting	O
it	O
with	O
few	O
standard	Method
layers	Method
and	O
a	O
simple	O
new	O
gradient	Method
reversal	Method
layer	Method
.	O
The	O
resulting	O
augmented	Method
architecture	Method
can	O
be	O
trained	O
using	O
standard	O
backpropagation	Method
.	O
Overall	O
,	O
the	O
approach	O
can	O
be	O
implemented	O
with	O
little	O
effort	O
using	O
any	O
of	O
the	O
deep	Method
-	Method
learning	Method
packages	Method
.	O
The	O
method	O
performs	O
very	O
well	O
in	O
a	O
series	O
of	O
image	Task
classification	Task
experiments	Task
,	O
achieving	O
adaptation	O
effect	O
in	O
the	O
presence	O
of	O
big	O
domain	O
shifts	O
and	O
outperforming	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
Office	Material
datasets	O
.	O
separate	O
-	O
uncertainty	O
=	O
true	O
positioning	O
calc	O
compat	O
=	O
newest	O
plotcoordinates	O
/	O
mathparser	O
=	O
false	O
UnsupervisedDomainAdaptationbyBackpropagation	O
section	O
:	O
Introduction	O
Deep	Method
feed	Method
-	Method
forward	Method
architectures	Method
have	O
brought	O
impressive	O
advances	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
across	O
a	O
wide	O
variety	O
of	O
machine	Task
-	Task
learning	Task
tasks	Task
and	O
applications	O
.	O
At	O
the	O
moment	O
,	O
however	O
,	O
these	O
leaps	O
in	O
performance	O
come	O
only	O
when	O
a	O
large	O
amount	O
of	O
labeled	O
training	O
data	O
is	O
available	O
.	O
At	O
the	O
same	O
time	O
,	O
for	O
problems	O
lacking	O
labeled	O
data	O
,	O
it	O
may	O
be	O
still	O
possible	O
to	O
obtain	O
training	O
sets	O
that	O
are	O
big	O
enough	O
for	O
training	O
large	Method
-	Method
scale	Method
deep	Method
models	Method
,	O
but	O
that	O
suffer	O
from	O
the	O
shift	O
in	O
data	O
distribution	O
from	O
the	O
actual	O
data	O
encountered	O
at	O
“	O
test	O
time	O
”	O
.	O
One	O
particularly	O
important	O
example	O
is	O
synthetic	O
or	O
semi	O
-	O
synthetic	O
training	O
data	O
,	O
which	O
may	O
come	O
in	O
abundance	O
and	O
be	O
fully	O
labeled	O
,	O
but	O
which	O
inevitably	O
have	O
a	O
distribution	O
that	O
is	O
different	O
from	O
real	O
data	O
.	O
Learning	O
a	O
discriminative	Method
classifier	Method
or	O
other	O
predictor	O
in	O
the	O
presence	O
of	O
a	O
shift	O
between	O
training	O
and	O
test	O
distributions	O
is	O
known	O
as	O
domain	Method
adaptation	Method
(	O
DA	Method
)	O
.	O
A	O
number	O
of	O
approaches	O
to	O
domain	Method
adaptation	Method
has	O
been	O
suggested	O
in	O
the	O
context	O
of	O
shallow	Task
learning	Task
,	O
e.g.	O
in	O
the	O
situation	O
when	O
data	O
representation	O
/	O
features	O
are	O
given	O
and	O
fixed	O
.	O
The	O
proposed	O
approaches	O
then	O
build	O
the	O
mappings	O
between	O
the	O
source	O
(	O
training	O
-	O
time	O
)	O
and	O
the	O
target	O
(	O
test	O
-	O
time	O
)	O
domains	O
,	O
so	O
that	O
the	O
classifier	Method
learned	O
for	O
the	O
source	O
domain	O
can	O
also	O
be	O
applied	O
to	O
the	O
target	O
domain	O
,	O
when	O
composed	O
with	O
the	O
learned	O
mapping	O
between	O
domains	O
.	O
The	O
appeal	O
of	O
the	O
domain	Method
adaptation	Method
approaches	Method
is	O
the	O
ability	O
to	O
learn	O
a	O
mapping	O
between	O
domains	O
in	O
the	O
situation	O
when	O
the	O
target	O
domain	O
data	O
are	O
either	O
fully	O
unlabeled	O
(	O
unsupervised	Task
domain	Task
annotation	Task
)	O
or	O
have	O
few	O
labeled	O
samples	O
(	O
semi	O
-	O
supervised	O
domain	Method
adaptation	Method
)	O
.	O
Below	O
,	O
we	O
focus	O
on	O
the	O
harder	O
unsupervised	Task
case	Task
,	O
although	O
the	O
proposed	O
approach	O
can	O
be	O
generalized	O
to	O
the	O
semi	Task
-	Task
supervised	Task
case	Task
rather	O
straightforwardly	O
.	O
Unlike	O
most	O
previous	O
papers	O
on	O
domain	Method
adaptation	Method
that	O
worked	O
with	O
fixed	Method
feature	Method
representations	Method
,	O
we	O
focus	O
on	O
combining	O
domain	Method
adaptation	Method
and	O
deep	Method
feature	Method
learning	Method
within	O
one	O
training	Method
process	Method
(	O
deep	Method
domain	Method
adaptation	Method
)	O
.	O
Our	O
goal	O
is	O
to	O
embed	O
domain	Method
adaptation	Method
into	O
the	O
process	O
of	O
learning	Task
representation	Task
,	O
so	O
that	O
the	O
final	O
classification	Task
decisions	Task
are	O
made	O
based	O
on	O
features	O
that	O
are	O
both	O
discriminative	O
and	O
invariant	O
to	O
the	O
change	O
of	O
domains	O
,	O
i.e.	O
have	O
the	O
same	O
or	O
very	O
similar	O
distributions	O
in	O
the	O
source	O
and	O
the	O
target	O
domains	O
.	O
In	O
this	O
way	O
,	O
the	O
obtained	O
feed	Method
-	Method
forward	Method
network	Method
can	O
be	O
applicable	O
to	O
the	O
target	O
domain	O
without	O
being	O
hindered	O
by	O
the	O
shift	O
between	O
the	O
two	O
domains	O
.	O
We	O
thus	O
focus	O
on	O
learning	O
features	O
that	O
combine	O
(	O
i	O
)	O
discriminativeness	O
and	O
(	O
ii	O
)	O
domain	O
-	O
invariance	O
.	O
This	O
is	O
achieved	O
by	O
jointly	O
optimizing	O
the	O
underlying	O
features	O
as	O
well	O
as	O
two	O
discriminative	Method
classifiers	Method
operating	O
on	O
these	O
features	O
:	O
(	O
i	O
)	O
the	O
label	Method
predictor	Method
that	O
predicts	O
class	O
labels	O
and	O
is	O
used	O
both	O
during	O
training	O
and	O
at	O
test	O
time	O
and	O
(	O
ii	O
)	O
the	O
domain	Method
classifier	Method
that	O
discriminates	O
between	O
the	O
source	O
and	O
the	O
target	O
domains	O
during	O
training	O
.	O
While	O
the	O
parameters	O
of	O
the	O
classifiers	Method
are	O
optimized	O
in	O
order	O
to	O
minimize	O
their	O
error	O
on	O
the	O
training	O
set	O
,	O
the	O
parameters	O
of	O
the	O
underlying	O
deep	Method
feature	Method
mapping	Method
are	O
optimized	O
in	O
order	O
to	O
minimize	O
the	O
loss	O
of	O
the	O
label	Method
classifier	Method
and	O
to	O
maximize	O
the	O
loss	O
of	O
the	O
domain	Method
classifier	Method
.	O
The	O
latter	O
encourages	O
domain	O
-	O
invariant	O
features	O
to	O
emerge	O
in	O
the	O
course	O
of	O
the	O
optimization	Task
.	O
Crucially	O
,	O
we	O
show	O
that	O
all	O
three	O
training	Method
processes	Method
can	O
be	O
embedded	O
into	O
an	O
appropriately	O
composed	O
deep	Method
feed	Method
-	Method
forward	Method
network	Method
(	O
Figure	O
[	O
reference	O
]	O
)	O
that	O
uses	O
standard	O
layers	O
and	O
loss	O
functions	O
,	O
and	O
can	O
be	O
trained	O
using	O
standard	O
backpropagation	Method
algorithms	Method
based	O
on	O
stochastic	Method
gradient	Method
descent	Method
or	O
its	O
modifications	O
(	O
e.g.	O
SGD	Method
with	O
momentum	O
)	O
.	O
Our	O
approach	O
is	O
generic	O
as	O
it	O
can	O
be	O
used	O
to	O
add	O
domain	Method
adaptation	Method
to	O
any	O
existing	O
feed	Method
-	Method
forward	Method
architecture	Method
that	O
is	O
trainable	O
by	O
backpropagation	Method
.	O
In	O
practice	O
,	O
the	O
only	O
non	O
-	O
standard	O
component	O
of	O
the	O
proposed	O
architecture	O
is	O
a	O
rather	O
trivial	O
gradient	Method
reversal	Method
layer	Method
that	O
leaves	O
the	O
input	O
unchanged	O
during	O
forward	Method
propagation	Method
and	O
reverses	O
the	O
gradient	O
by	O
multiplying	O
it	O
by	O
a	O
negative	O
scalar	O
during	O
the	O
backpropagation	O
.	O
Below	O
,	O
we	O
detail	O
the	O
proposed	O
approach	O
to	O
domain	Method
adaptation	Method
in	O
deep	Task
architectures	Task
,	O
and	O
present	O
results	O
on	O
traditional	O
deep	Method
learning	Method
image	O
datasets	O
(	O
such	O
as	O
MNIST	Material
and	O
SVHN	Material
)	O
as	O
well	O
as	O
on	O
Office	Material
benchmarks	O
,	O
where	O
the	O
proposed	O
method	O
considerably	O
improves	O
over	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
.	O
section	O
:	O
Related	O
work	O
A	O
large	O
number	O
of	O
domain	Method
adaptation	Method
methods	Method
have	O
been	O
proposed	O
over	O
the	O
recent	O
years	O
,	O
and	O
here	O
we	O
focus	O
on	O
the	O
most	O
related	O
ones	O
.	O
Multiple	O
methods	O
perform	O
unsupervised	Task
domain	Task
adaptation	Task
by	O
matching	O
the	O
feature	O
distributions	O
in	O
the	O
source	O
and	O
the	O
target	O
domains	O
.	O
Some	O
approaches	O
perform	O
this	O
by	O
reweighing	O
or	O
selecting	O
samples	O
from	O
the	O
source	O
domain	O
,	O
while	O
others	O
seek	O
an	O
explicit	O
feature	Method
space	Method
transformation	Method
that	O
would	O
map	O
source	O
distribution	O
into	O
the	O
target	O
ones	O
.	O
An	O
important	O
aspect	O
of	O
the	O
distribution	Method
matching	Method
approach	Method
is	O
the	O
way	O
the	O
(	O
dis	O
)	O
similarity	O
between	O
distributions	O
is	O
measured	O
.	O
Here	O
,	O
one	O
popular	O
choice	O
is	O
matching	O
the	O
distribution	O
means	O
in	O
the	O
kernel	Method
-	Method
reproducing	Method
Hilbert	Method
space	Method
,	O
whereas	O
map	O
the	O
principal	O
axes	O
associated	O
with	O
each	O
of	O
the	O
distributions	O
.	O
Our	O
approach	O
also	O
attempts	O
to	O
match	O
feature	O
space	O
distributions	O
,	O
however	O
this	O
is	O
accomplished	O
by	O
modifying	O
the	O
feature	Method
representation	Method
itself	O
rather	O
than	O
by	O
reweighing	O
or	O
geometric	Method
transformation	Method
.	O
Also	O
,	O
our	O
method	O
uses	O
(	O
implicitly	O
)	O
a	O
rather	O
different	O
way	O
to	O
measure	O
the	O
disparity	O
between	O
distributions	O
based	O
on	O
their	O
separability	O
by	O
a	O
deep	Method
discriminatively	Method
-	Method
trained	Method
classifier	Method
.	O
Several	O
approaches	O
perform	O
gradual	O
transition	O
from	O
the	O
source	O
to	O
the	O
target	O
domain	O
by	O
a	O
gradual	O
change	O
of	O
the	O
training	O
distribution	O
.	O
Among	O
these	O
methods	O
,	O
does	O
this	O
in	O
a	O
“	O
deep	O
”	O
way	O
by	O
the	O
layerwise	Method
training	Method
of	O
a	O
sequence	O
of	O
deep	Method
autoencoders	Method
,	O
while	O
gradually	O
replacing	O
source	O
-	O
domain	O
samples	O
with	O
target	O
-	O
domain	O
samples	O
.	O
This	O
improves	O
over	O
a	O
similar	O
approach	O
of	O
that	O
simply	O
trains	O
a	O
single	O
deep	Method
autoencoder	Method
for	O
both	O
domains	O
.	O
In	O
both	O
approaches	O
,	O
the	O
actual	O
classifier	Method
/	Method
predictor	Method
is	O
learned	O
in	O
a	O
separate	O
step	O
using	O
the	O
feature	Method
representation	Method
learned	O
by	O
autoencoder	Method
(	Method
s	Method
)	O
.	O
In	O
contrast	O
to	O
,	O
our	O
approach	O
performs	O
feature	Method
learning	Method
,	O
domain	Method
adaptation	Method
and	O
classifier	Method
learning	Method
jointly	O
,	O
in	O
a	O
unified	Method
architecture	Method
,	O
and	O
using	O
a	O
single	O
learning	Method
algorithm	Method
(	O
backpropagation	Method
)	O
.	O
We	O
therefore	O
argue	O
that	O
our	O
approach	O
is	O
simpler	O
(	O
both	O
conceptually	O
and	O
in	O
terms	O
of	O
its	O
implementation	O
)	O
.	O
Our	O
method	O
also	O
achieves	O
considerably	O
better	O
results	O
on	O
the	O
popular	O
Office	Material
benchmark	O
.	O
While	O
the	O
above	O
approaches	O
perform	O
unsupervised	Task
domain	Task
adaptation	Task
,	O
there	O
are	O
approaches	O
that	O
perform	O
supervised	O
domain	Method
adaptation	Method
by	O
exploiting	O
labeled	O
data	O
from	O
the	O
target	O
domain	O
.	O
In	O
the	O
context	O
of	O
deep	Method
feed	Method
-	Method
forward	Method
architectures	Method
,	O
such	O
data	O
can	O
be	O
used	O
to	O
“	O
fine	O
-	O
tune	O
”	O
the	O
network	O
trained	O
on	O
the	O
source	O
domain	O
.	O
Our	O
approach	O
does	O
not	O
require	O
labeled	O
target	O
-	O
domain	O
data	O
.	O
At	O
the	O
same	O
time	O
,	O
it	O
can	O
easily	O
incorporate	O
such	O
data	O
when	O
it	O
is	O
available	O
.	O
An	O
idea	O
related	O
to	O
ours	O
is	O
described	O
in	O
.	O
While	O
their	O
goal	O
is	O
quite	O
different	O
(	O
building	O
generative	Method
deep	Method
networks	Method
that	O
can	O
synthesize	O
samples	O
)	O
,	O
the	O
way	O
they	O
measure	O
and	O
minimize	O
the	O
discrepancy	O
between	O
the	O
distribution	O
of	O
the	O
training	O
data	O
and	O
the	O
distribution	O
of	O
the	O
synthesized	O
data	O
is	O
very	O
similar	O
to	O
the	O
way	O
our	O
architecture	O
measures	O
and	O
minimizes	O
the	O
discrepancy	O
between	O
feature	O
distributions	O
for	O
the	O
two	O
domains	O
.	O
Finally	O
,	O
a	O
recent	O
and	O
concurrent	O
report	O
by	O
also	O
focuses	O
on	O
domain	Method
adaptation	Method
in	O
feed	Method
-	Method
forward	Method
networks	Method
.	O
Their	O
set	O
of	O
techniques	O
measures	O
and	O
minimizes	O
the	O
distance	O
of	O
the	O
data	O
means	O
across	O
domains	O
.	O
This	O
approach	O
may	O
be	O
regarded	O
as	O
a	O
“	O
first	O
-	O
order	O
”	O
approximation	O
to	O
our	O
approach	O
,	O
which	O
seeks	O
a	O
tighter	O
alignment	O
between	O
distributions	O
.	O
section	O
:	O
Deep	Task
Domain	Task
Adaptation	Task
subsection	O
:	O
The	O
model	O
We	O
now	O
detail	O
the	O
proposed	O
model	O
for	O
the	O
domain	Method
adaptation	Method
.	O
We	O
assume	O
that	O
the	O
model	O
works	O
with	O
input	O
samples	O
,	O
where	O
is	O
some	O
input	O
space	O
and	O
certain	O
labels	O
(	O
output	O
)	O
from	O
the	O
label	O
space	O
.	O
Below	O
,	O
we	O
assume	O
classification	Task
problems	Task
where	O
is	O
a	O
finite	O
set	O
(	O
)	O
,	O
however	O
our	O
approach	O
is	O
generic	O
and	O
can	O
handle	O
any	O
output	O
label	O
space	O
that	O
other	O
deep	Method
feed	Method
-	Method
forward	Method
models	Method
can	O
handle	O
.	O
We	O
further	O
assume	O
that	O
there	O
exist	O
two	O
distributions	O
and	O
on	O
,	O
which	O
will	O
be	O
referred	O
to	O
as	O
the	O
source	O
distribution	O
and	O
the	O
target	O
distribution	O
(	O
or	O
the	O
source	O
domain	O
and	O
the	O
target	O
domain	O
)	O
.	O
Both	O
distributions	O
are	O
assumed	O
complex	O
and	O
unknown	O
,	O
and	O
furthermore	O
similar	O
but	O
different	O
(	O
in	O
other	O
words	O
,	O
is	O
“	O
shifted	O
”	O
from	O
by	O
some	O
domain	O
shift	O
)	O
.	O
Our	O
ultimate	O
goal	O
is	O
to	O
be	O
able	O
to	O
predict	O
labels	O
given	O
the	O
input	O
for	O
the	O
target	O
distribution	O
.	O
At	O
training	O
time	O
,	O
we	O
have	O
an	O
access	O
to	O
a	O
large	O
set	O
of	O
training	O
samples	O
from	O
both	O
the	O
source	O
and	O
the	O
target	O
domains	O
distributed	O
according	O
to	O
the	O
marginal	O
distributions	O
and	O
.	O
We	O
denote	O
with	O
the	O
binary	O
variable	O
(	O
domain	O
label	O
)	O
for	O
the	O
-	O
th	O
example	O
,	O
which	O
indicates	O
whether	O
come	O
from	O
the	O
source	O
distribution	O
(	O
if	O
)	O
or	O
from	O
the	O
target	O
distribution	O
(	O
if	O
)	O
.	O
For	O
the	O
examples	O
from	O
the	O
source	O
distribution	O
(	O
)	O
the	O
corresponding	O
labels	O
are	O
known	O
at	O
training	O
time	O
.	O
For	O
the	O
examples	O
from	O
the	O
target	O
domains	O
,	O
we	O
do	O
not	O
know	O
the	O
labels	O
at	O
training	O
time	O
,	O
and	O
we	O
want	O
to	O
predict	O
such	O
labels	O
at	O
test	O
time	O
.	O
We	O
now	O
define	O
a	O
deep	Method
feed	Method
-	Method
forward	Method
architecture	Method
that	O
for	O
each	O
input	O
predicts	O
its	O
label	O
and	O
its	O
domain	O
label	O
.	O
We	O
decompose	O
such	O
mapping	O
into	O
three	O
parts	O
.	O
We	O
assume	O
that	O
the	O
input	O
is	O
first	O
mapped	O
by	O
a	O
mapping	Method
(	O
a	O
feature	Method
extractor	Method
)	O
to	O
a	O
-	O
dimensional	O
feature	O
vector	O
.	O
The	O
feature	Method
mapping	Method
may	O
also	O
include	O
several	O
feed	Method
-	Method
forward	Method
layers	Method
and	O
we	O
denote	O
the	O
vector	O
of	O
parameters	O
of	O
all	O
layers	O
in	O
this	O
mapping	O
as	O
,	O
i.e.	O
.	O
Then	O
,	O
the	O
feature	O
vector	O
is	O
mapped	O
by	O
a	O
mapping	Method
(	O
label	Method
predictor	Method
)	O
to	O
the	O
label	O
,	O
and	O
we	O
denote	O
the	O
parameters	O
of	O
this	O
mapping	O
with	O
.	O
Finally	O
,	O
the	O
same	O
feature	O
vector	O
is	O
mapped	O
to	O
the	O
domain	O
label	O
by	O
a	O
mapping	Method
(	O
domain	Method
classifier	Method
)	O
with	O
the	O
parameters	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
During	O
the	O
learning	Task
stage	Task
,	O
we	O
aim	O
to	O
minimize	O
the	O
label	Task
prediction	Task
loss	Task
on	O
the	O
annotated	O
part	O
(	O
i.e.	O
the	O
source	O
part	O
)	O
of	O
the	O
training	O
set	O
,	O
and	O
the	O
parameters	O
of	O
both	O
the	O
feature	Method
extractor	Method
and	O
the	O
label	Method
predictor	Method
are	O
thus	O
optimized	O
in	O
order	O
to	O
minimize	O
the	O
empirical	Metric
loss	Metric
for	O
the	O
source	O
domain	O
samples	O
.	O
This	O
ensures	O
the	O
discriminativeness	O
of	O
the	O
features	O
and	O
the	O
overall	O
good	O
prediction	Metric
performance	O
of	O
the	O
combination	O
of	O
the	O
feature	Method
extractor	Method
and	O
the	O
label	Method
predictor	Method
on	O
the	O
source	O
domain	O
.	O
At	O
the	O
same	O
time	O
,	O
we	O
want	O
to	O
make	O
the	O
features	O
domain	O
-	O
invariant	O
.	O
That	O
is	O
,	O
we	O
want	O
to	O
make	O
the	O
distributions	O
and	O
to	O
be	O
similar	O
.	O
Under	O
the	O
covariate	Method
shift	Method
assumption	Method
,	O
this	O
would	O
make	O
the	O
label	O
prediction	O
accuracy	Metric
on	O
the	O
target	O
domain	O
to	O
be	O
the	O
same	O
as	O
on	O
the	O
source	O
domain	O
.	O
Measuring	O
the	O
dissimilarity	O
of	O
the	O
distributions	O
and	O
is	O
however	O
non	O
-	O
trivial	O
,	O
given	O
that	O
is	O
high	O
-	O
dimensional	O
,	O
and	O
that	O
the	O
distributions	O
themselves	O
are	O
constantly	O
changing	O
as	O
learning	O
progresses	O
.	O
One	O
way	O
to	O
estimate	O
the	O
dissimilarity	O
is	O
to	O
look	O
at	O
the	O
loss	O
of	O
the	O
domain	Method
classifier	Method
,	O
provided	O
that	O
the	O
parameters	O
of	O
the	O
domain	Method
classifier	Method
have	O
been	O
trained	O
to	O
discriminate	O
between	O
the	O
two	O
feature	O
distributions	O
in	O
an	O
optimal	O
way	O
.	O
This	O
observation	O
leads	O
to	O
our	O
idea	O
.	O
At	O
training	O
time	O
,	O
in	O
order	O
to	O
obtain	O
domain	O
-	O
invariant	O
features	O
,	O
we	O
seek	O
the	O
parameters	O
of	O
the	O
feature	Method
mapping	Method
that	O
maximize	O
the	O
loss	Metric
of	O
the	O
domain	Method
classifier	Method
(	O
by	O
making	O
the	O
two	O
feature	O
distributions	O
as	O
similar	O
as	O
possible	O
)	O
,	O
while	O
simultaneously	O
seeking	O
the	O
parameters	O
of	O
the	O
domain	Method
classifier	Method
that	O
minimize	O
the	O
loss	Metric
of	O
the	O
domain	Method
classifier	Method
.	O
In	O
addition	O
,	O
we	O
seek	O
to	O
minimize	O
the	O
loss	O
of	O
the	O
label	Method
predictor	Method
.	O
More	O
formally	O
,	O
we	O
consider	O
the	O
functional	O
:	O
Here	O
,	O
is	O
the	O
loss	Metric
for	O
label	Task
prediction	Task
(	O
e.g.	O
multinomial	Task
)	O
,	O
is	O
the	O
loss	O
for	O
the	O
domain	Task
classification	Task
(	O
e.g.	O
logistic	O
)	O
,	O
while	O
and	O
denote	O
the	O
corresponding	O
loss	O
functions	O
evaluated	O
at	O
the	O
-	O
th	O
training	O
example	O
.	O
Based	O
on	O
our	O
idea	O
,	O
we	O
are	O
seeking	O
the	O
parameters	O
that	O
deliver	O
a	O
saddle	O
point	O
of	O
the	O
functional	O
(	O
[	O
reference	O
]	O
)	O
:	O
At	O
the	O
saddle	O
point	O
,	O
the	O
parameters	O
of	O
the	O
domain	Method
classifier	Method
minimize	O
the	O
domain	Task
classification	Task
loss	Task
(	O
since	O
it	O
enters	O
into	O
(	O
[	O
reference	O
]	O
)	O
with	O
the	O
minus	O
sign	O
)	O
while	O
the	O
parameters	O
of	O
the	O
label	Method
predictor	Method
minimize	O
the	O
label	Metric
prediction	Metric
loss	Metric
.	O
The	O
feature	O
mapping	O
parameters	O
minimize	O
the	O
label	Metric
prediction	Metric
loss	Metric
(	O
i.e.	O
the	O
features	O
are	O
discriminative	O
)	O
,	O
while	O
maximizing	O
the	O
domain	Metric
classification	Metric
loss	Metric
(	O
i.e.	O
the	O
features	O
are	O
domain	O
-	O
invariant	O
)	O
.	O
The	O
parameter	O
controls	O
the	O
trade	O
-	O
off	O
between	O
the	O
two	O
objectives	O
that	O
shape	O
the	O
features	O
during	O
learning	Task
.	O
Below	O
,	O
we	O
demonstrate	O
that	O
standard	O
stochastic	Method
gradient	Method
solvers	Method
(	O
SGD	Method
)	O
can	O
be	O
adapted	O
for	O
the	O
search	O
of	O
the	O
saddle	O
point	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Optimization	Task
with	O
backpropagation	Method
A	O
saddle	O
point	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
found	O
as	O
a	O
stationary	O
point	O
of	O
the	O
following	O
stochastic	Method
updates	Method
:	O
where	O
is	O
the	O
learning	Metric
rate	Metric
(	O
which	O
can	O
vary	O
over	O
time	O
)	O
.	O
The	O
updates	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
are	O
very	O
similar	O
to	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
updates	O
for	O
a	O
feed	Method
-	Method
forward	Method
deep	Method
model	Method
that	O
comprises	O
feature	Method
extractor	Method
fed	O
into	O
the	O
label	Method
predictor	Method
and	O
into	O
the	O
domain	Method
classifier	Method
.	O
The	O
difference	O
is	O
the	O
factor	O
in	O
(	O
[	O
reference	O
]	O
)	O
(	O
the	O
difference	O
is	O
important	O
,	O
as	O
without	O
such	O
factor	O
,	O
stochastic	Method
gradient	Method
descent	Method
would	O
try	O
to	O
make	O
features	O
dissimilar	O
across	O
domains	O
in	O
order	O
to	O
minimize	O
the	O
domain	O
classification	O
loss	O
)	O
.	O
Although	O
direct	O
implementation	O
of	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
as	O
SGD	Method
is	O
not	O
possible	O
,	O
it	O
is	O
highly	O
desirable	O
to	O
reduce	O
the	O
updates	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
to	O
some	O
form	O
of	O
SGD	Method
,	O
since	O
SGD	Method
(	O
and	O
its	O
variants	O
)	O
is	O
the	O
main	O
learning	Method
algorithm	Method
implemented	O
in	O
most	O
packages	O
for	O
deep	Method
learning	Method
.	O
Fortunately	O
,	O
such	O
reduction	O
can	O
be	O
accomplished	O
by	O
introducing	O
a	O
special	O
gradient	Method
reversal	Method
layer	Method
(	O
GRL	Method
)	O
defined	O
as	O
follows	O
.	O
The	O
gradient	Method
reversal	Method
layer	Method
has	O
no	O
parameters	O
associated	O
with	O
it	O
(	O
apart	O
from	O
the	O
meta	O
-	O
parameter	O
,	O
which	O
is	O
not	O
updated	O
by	O
backpropagation	Method
)	O
.	O
During	O
the	O
forward	Method
propagation	Method
,	O
GRL	Method
acts	O
as	O
an	O
identity	Method
transform	Method
.	O
During	O
the	O
backpropagation	O
though	O
,	O
GRL	Method
takes	O
the	O
gradient	O
from	O
the	O
subsequent	O
level	O
,	O
multiplies	O
it	O
by	O
and	O
passes	O
it	O
to	O
the	O
preceding	O
layer	O
.	O
Implementing	O
such	O
layer	O
using	O
existing	O
object	Method
-	Method
oriented	Method
packages	Method
for	O
deep	Method
learning	Method
is	O
simple	O
,	O
as	O
defining	O
procedures	O
for	O
forwardprop	Method
(	O
identity	Method
transform	Method
)	O
,	O
backprop	O
(	O
multiplying	O
by	O
a	O
constant	O
)	O
,	O
and	O
parameter	O
update	O
(	O
nothing	O
)	O
is	O
trivial	O
.	O
The	O
GRL	Method
as	O
defined	O
above	O
is	O
inserted	O
between	O
the	O
feature	Method
extractor	Method
and	O
the	O
domain	Method
classifier	Method
,	O
resulting	O
in	O
the	O
architecture	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
As	O
the	O
backpropagation	Method
process	Method
passes	O
through	O
the	O
GRL	Method
,	O
the	O
partial	O
derivatives	O
of	O
the	O
loss	O
that	O
is	O
downstream	O
the	O
GRL	O
(	O
i.e.	O
)	O
w.r.t	O
.	O
the	O
layer	O
parameters	O
that	O
are	O
upstream	O
the	O
GRL	O
(	O
i.e.	O
)	O
get	O
multiplied	O
by	O
,	O
i.e.	O
is	O
effectively	O
replaced	O
with	O
.	O
Therefore	O
,	O
running	O
SGD	Method
in	O
the	O
resulting	O
model	O
implements	O
the	O
updates	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
and	O
converges	O
to	O
a	O
saddle	O
point	O
of	O
(	O
[	O
reference	O
]	O
)	O
.	O
Mathematically	O
,	O
we	O
can	O
formally	O
treat	O
the	O
gradient	Method
reversal	Method
layer	Method
as	O
a	O
“	O
pseudo	O
-	O
function	O
”	O
defined	O
by	O
two	O
(	O
incompatible	O
)	O
equations	O
describing	O
its	O
forward	O
-	O
and	O
backpropagation	O
behaviour	O
:	O
where	O
is	O
an	O
identity	O
matrix	O
.	O
We	O
can	O
then	O
define	O
the	O
objective	O
“	O
pseudo	O
-	O
function	O
”	O
of	O
that	O
is	O
being	O
optimized	O
by	O
the	O
stochastic	Method
gradient	Method
descent	Method
within	O
our	O
method	O
:	O
Running	O
updates	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
can	O
then	O
be	O
implemented	O
as	O
doing	O
SGD	Method
for	O
(	O
[	O
reference	O
]	O
)	O
and	O
leads	O
to	O
the	O
emergence	O
of	O
features	O
that	O
are	O
domain	O
-	O
invariant	O
and	O
discriminative	O
at	O
the	O
same	O
time	O
.	O
After	O
the	O
learning	Task
,	O
the	O
label	Method
predictor	Method
can	O
be	O
used	O
to	O
predict	O
labels	O
for	O
samples	O
from	O
the	O
target	O
domain	O
(	O
as	O
well	O
as	O
from	O
the	O
source	O
domain	O
)	O
.	O
The	O
simple	O
learning	Method
procedure	Method
outlined	O
above	O
can	O
be	O
re	O
-	O
derived	O
/	O
generalized	O
along	O
the	O
lines	O
suggested	O
in	O
(	O
see	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Relation	O
to	O
-	O
distance	O
In	O
this	O
section	O
we	O
give	O
a	O
brief	O
analysis	O
of	O
our	O
method	O
in	O
terms	O
of	O
-	O
distance	O
which	O
is	O
widely	O
used	O
in	O
the	O
theory	O
of	O
non	O
-	O
conservative	O
domain	Method
adaptation	Method
.	O
Formally	O
,	O
defines	O
a	O
discrepancy	O
distance	O
between	O
two	O
distributions	O
and	O
w.r.t	O
.	O
a	O
hypothesis	O
set	O
.	O
Using	O
this	O
notion	O
one	O
can	O
obtain	O
a	O
probabilistic	O
bound	O
on	O
the	O
performance	O
of	O
some	O
classifier	Method
from	O
evaluated	O
on	O
the	O
target	O
domain	O
given	O
its	O
performance	O
on	O
the	O
source	O
domain	O
:	O
where	O
and	O
are	O
source	O
and	O
target	O
distributions	O
respectively	O
,	O
and	O
does	O
not	O
depend	O
on	O
particular	O
.	O
Consider	O
fixed	O
and	O
over	O
the	O
representation	O
space	O
produced	O
by	O
the	O
feature	Method
extractor	Method
and	O
a	O
family	O
of	O
label	Method
predictors	Method
.	O
We	O
assume	O
that	O
the	O
family	O
of	O
domain	Method
classifiers	Method
is	O
rich	O
enough	O
to	O
contain	O
the	O
symmetric	O
difference	O
hypothesis	O
set	O
of	O
:	O
It	O
is	O
not	O
an	O
unrealistic	O
assumption	O
as	O
we	O
have	O
a	O
freedom	O
to	O
pick	O
whichever	O
we	O
want	O
.	O
For	O
example	O
,	O
we	O
can	O
set	O
the	O
architecture	O
of	O
the	O
domain	Method
discriminator	Method
to	O
be	O
the	O
layer	Method
-	Method
by	Method
-	Method
layer	Method
concatenation	Method
of	O
two	O
replicas	O
of	O
the	O
label	Method
predictor	Method
followed	O
by	O
a	O
two	O
layer	Method
non	Method
-	Method
linear	Method
perceptron	Method
aimed	O
to	O
learn	O
the	O
XOR	O
-	O
function	O
.	O
Given	O
the	O
assumption	O
holds	O
,	O
one	O
can	O
easily	O
show	O
that	O
training	O
the	O
is	O
closely	O
related	O
to	O
the	O
estimation	Task
of	Task
.	O
Indeed	O
,	O
where	O
is	O
maximized	O
by	O
the	O
optimal	O
.	O
Thus	O
,	O
optimal	Method
discriminator	Method
gives	O
the	O
upper	O
bound	O
for	O
.	O
At	O
the	O
same	O
time	O
,	O
backpropagation	O
of	O
the	O
reversed	O
gradient	O
changes	O
the	O
representation	O
space	O
so	O
that	O
becomes	O
smaller	O
effectively	O
reducing	O
and	O
leading	O
to	O
the	O
better	O
approximation	O
of	O
by	O
.	O
section	O
:	O
Experiments	O
We	O
perform	O
extensive	O
evaluation	O
of	O
the	O
proposed	O
approach	O
on	O
a	O
number	O
of	O
popular	O
image	O
datasets	O
and	O
their	O
modifications	O
.	O
These	O
include	O
large	O
-	O
scale	O
datasets	O
of	O
small	O
images	O
popular	O
with	O
deep	Method
learning	Method
methods	Method
,	O
and	O
the	O
Office	Material
datasets	O
,	O
which	O
are	O
a	O
de	O
facto	O
standard	O
for	O
domain	Method
adaptation	Method
in	O
computer	Task
vision	Task
,	O
but	O
have	O
much	O
fewer	O
images	O
.	O
Baselines	O
.	O
For	O
the	O
bulk	O
of	O
experiments	O
the	O
following	O
baselines	O
are	O
evaluated	O
.	O
The	O
source	Method
-	Method
only	Method
model	Method
is	O
trained	O
without	O
consideration	O
for	O
target	O
-	O
domain	O
data	O
(	O
no	O
domain	Method
classifier	Method
branch	O
included	O
into	O
the	O
network	O
)	O
.	O
The	O
train	Method
-	Method
on	Method
-	Method
target	Method
model	Method
is	O
trained	O
on	O
the	O
target	O
domain	O
with	O
class	O
labels	O
revealed	O
.	O
This	O
model	O
serves	O
as	O
an	O
upper	O
bound	O
on	O
DA	Method
methods	O
,	O
assuming	O
that	O
target	O
data	O
are	O
abundant	O
and	O
the	O
shift	O
between	O
the	O
domains	O
is	O
considerable	O
.	O
In	O
addition	O
,	O
we	O
compare	O
our	O
approach	O
against	O
the	O
recently	O
proposed	O
unsupervised	O
DA	Method
method	O
based	O
on	O
subspace	Method
alignment	Method
(	O
SA	Method
)	O
,	O
which	O
is	O
simple	O
to	O
setup	O
and	O
test	O
on	O
new	O
datasets	O
,	O
but	O
has	O
also	O
been	O
shown	O
to	O
perform	O
very	O
well	O
in	O
experimental	O
comparisons	O
with	O
other	O
“	O
shallow	O
”	O
DA	Method
methods	O
.	O
To	O
boost	O
the	O
performance	O
of	O
this	O
baseline	O
,	O
we	O
pick	O
its	O
most	O
important	O
free	O
parameter	O
(	O
the	O
number	O
of	O
principal	O
components	O
)	O
from	O
the	O
range	O
,	O
so	O
that	O
the	O
test	O
performance	O
on	O
the	O
target	O
domain	O
is	O
maximized	O
.	O
To	O
apply	O
SA	Method
in	O
our	O
setting	O
,	O
we	O
train	O
a	O
source	Method
-	Method
only	Method
model	Method
and	O
then	O
consider	O
the	O
activations	O
of	O
the	O
last	O
hidden	O
layer	O
in	O
the	O
label	Method
predictor	Method
(	O
before	O
the	O
final	O
linear	Method
classifier	Method
)	O
as	O
descriptors	O
/	O
features	O
,	O
and	O
learn	O
the	O
mapping	O
between	O
the	O
source	O
and	O
the	O
target	O
domains	O
.	O
Since	O
the	O
SA	Method
baseline	O
requires	O
to	O
train	O
a	O
new	O
classifier	Method
after	O
adapting	O
the	O
features	O
,	O
and	O
in	O
order	O
to	O
put	O
all	O
the	O
compared	O
settings	O
on	O
an	O
equal	O
footing	O
,	O
we	O
retrain	O
the	O
last	O
layer	O
of	O
the	O
label	Method
predictor	Method
using	O
a	O
standard	O
linear	Method
SVM	Method
for	O
all	O
four	O
considered	O
methods	O
(	O
including	O
ours	O
;	O
the	O
performance	O
on	O
the	O
target	O
domain	O
remains	O
approximately	O
the	O
same	O
after	O
the	O
retraining	Task
)	O
.	O
For	O
the	O
Office	Material
dataset	O
,	O
we	O
directly	O
compare	O
the	O
performance	O
of	O
our	O
full	Method
network	Method
(	O
feature	Method
extractor	Method
and	Method
label	Method
predictor	Method
)	O
against	O
recent	O
DA	Method
approaches	O
using	O
previously	O
published	O
results	O
.	O
CNN	Method
architectures	Method
.	O
In	O
general	O
,	O
we	O
compose	O
feature	Method
extractor	Method
from	O
two	O
or	O
three	O
convolutional	Method
layers	Method
,	O
picking	O
their	O
exact	O
configurations	O
from	O
previous	O
works	O
.	O
We	O
give	O
the	O
exact	O
architectures	O
in	O
[	O
reference	O
]	O
.	O
For	O
the	O
domain	Method
adaptator	Method
we	O
stick	O
to	O
the	O
three	O
fully	O
connected	O
layers	O
(	O
)	O
,	O
except	O
for	O
MNIST	Material
where	O
we	O
used	O
a	O
simpler	O
(	O
)	O
architecture	O
to	O
speed	O
up	O
the	O
experiments	O
.	O
For	O
loss	O
functions	O
,	O
we	O
set	O
and	O
to	O
be	O
the	O
logistic	O
regression	O
loss	O
and	O
the	O
binomial	Metric
cross	Metric
-	Metric
entropy	Metric
respectively	O
.	O
CNN	Method
training	Method
procedure	Method
.	O
The	O
model	O
is	O
trained	O
on	O
-	O
sized	O
batches	O
.	O
Images	O
are	O
preprocessed	O
by	O
the	O
mean	Method
subtraction	Method
.	O
A	O
half	O
of	O
each	O
batch	O
is	O
populated	O
by	O
the	O
samples	O
from	O
the	O
source	O
domain	O
(	O
with	O
known	O
labels	O
)	O
,	O
the	O
rest	O
is	O
comprised	O
of	O
the	O
target	O
domain	O
(	O
with	O
unknown	O
labels	O
)	O
.	O
In	O
order	O
to	O
suppress	O
noisy	O
signal	O
from	O
the	O
domain	Method
classifier	Method
at	O
the	O
early	O
stages	O
of	O
the	O
training	O
procedure	O
instead	O
of	O
fixing	O
the	O
adaptation	O
factor	O
,	O
we	O
gradually	O
change	O
it	O
from	O
to	O
using	O
the	O
following	O
schedule	O
:	O
where	O
was	O
set	O
to	O
in	O
all	O
experiments	O
(	O
the	O
schedule	O
was	O
not	O
optimized	O
/	O
tweaked	O
)	O
.	O
Further	O
details	O
on	O
the	O
CNN	Task
training	Task
can	O
be	O
found	O
in	O
[	O
reference	O
]	O
.	O
Visualizations	Task
.	O
We	O
use	O
t	Method
-	Method
SNE	Method
projection	Method
to	O
visualize	O
feature	O
distributions	O
at	O
different	O
points	O
of	O
the	O
network	O
,	O
while	O
color	O
-	O
coding	O
the	O
domains	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
We	O
observe	O
strong	O
correspondence	O
between	O
the	O
success	O
of	O
the	O
adaptation	O
in	O
terms	O
of	O
the	O
classification	O
accuracy	Metric
for	O
the	O
target	O
domain	O
,	O
and	O
the	O
overlap	O
between	O
the	O
domain	O
distributions	O
in	O
such	O
visualizations	Method
.	O
Choosing	O
meta	O
-	O
parameters	O
.	O
In	O
general	O
,	O
good	O
unsupervised	O
DA	Method
methods	O
should	O
provide	O
ways	O
to	O
set	O
meta	O
-	O
parameters	O
(	O
such	O
as	O
,	O
the	O
learning	Metric
rate	Metric
,	O
the	O
momentum	O
rate	O
,	O
the	O
network	Method
architecture	Method
for	O
our	O
method	O
)	O
in	O
an	O
unsupervised	O
way	O
,	O
i.e.	O
without	O
referring	O
to	O
labeled	O
data	O
in	O
the	O
target	O
domain	O
.	O
In	O
our	O
method	O
,	O
one	O
can	O
assess	O
the	O
performance	O
of	O
the	O
whole	O
system	O
(	O
and	O
the	O
effect	O
of	O
changing	O
hyper	O
-	O
parameters	O
)	O
by	O
observing	O
the	O
test	Metric
error	Metric
on	O
the	O
source	O
domain	O
and	O
the	O
domain	Metric
classifier	Metric
error	Metric
.	O
In	O
general	O
,	O
we	O
observed	O
a	O
good	O
correspondence	O
between	O
the	O
success	O
of	O
adaptation	Task
and	O
these	O
errors	O
(	O
adaptation	Task
is	O
more	O
successful	O
when	O
the	O
source	Metric
domain	Metric
test	Metric
error	Metric
is	O
low	O
,	O
while	O
the	O
domain	Metric
classifier	Metric
error	Metric
is	O
high	O
)	O
.	O
In	O
addition	O
,	O
the	O
layer	O
,	O
where	O
the	O
the	O
domain	Method
adaptator	Method
is	O
attached	O
can	O
be	O
picked	O
by	O
computing	O
difference	O
between	O
means	O
as	O
suggested	O
in	O
.	O
MNIST	Material
→	O
MNIST	Material
-	Material
M	Material
:	O
top	O
feature	Method
extractor	Method
layer	Method
Syn	Method
Numbers	Method
→	O
SVHN	Material
:	O
last	O
hidden	Method
layer	Method
of	O
the	O
label	Method
predictor	Method
subsection	O
:	O
Results	O
We	O
now	O
discuss	O
the	O
experimental	O
settings	O
and	O
the	O
results	O
.	O
In	O
each	O
case	O
,	O
we	O
train	O
on	O
the	O
source	O
dataset	O
and	O
test	O
on	O
a	O
different	O
target	O
domain	O
dataset	O
,	O
with	O
considerable	O
shifts	O
between	O
domains	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O
MNIST	Material
→	O
MNIST	Material
-	O
M.	O
Our	O
first	O
experiment	O
deals	O
with	O
the	O
MNIST	Material
dataset	Material
(	O
source	O
)	O
.	O
In	O
order	O
to	O
obtain	O
the	O
target	O
domain	O
(	O
MNIST	Material
-	Material
M	Material
)	O
we	O
blend	O
digits	O
from	O
the	O
original	O
set	O
over	O
patches	O
randomly	O
extracted	O
from	O
color	O
photos	O
from	O
BSDS500	Material
.	O
This	O
operation	O
is	O
formally	O
defined	O
for	O
two	O
images	O
as	O
,	O
where	O
are	O
the	O
coordinates	O
of	O
a	O
pixel	O
and	O
is	O
a	O
channel	O
index	O
.	O
In	O
other	O
words	O
,	O
an	O
output	O
sample	O
is	O
produced	O
by	O
taking	O
a	O
patch	O
from	O
a	O
photo	O
and	O
inverting	O
its	O
pixels	O
at	O
positions	O
corresponding	O
to	O
the	O
pixels	O
of	O
a	O
digit	O
.	O
For	O
a	O
human	O
the	O
classification	Task
task	Task
becomes	O
only	O
slightly	O
harder	O
compared	O
to	O
the	O
original	O
dataset	O
(	O
the	O
digits	O
are	O
still	O
clearly	O
distinguishable	O
)	O
whereas	O
for	O
a	O
CNN	Method
trained	O
on	O
MNIST	Material
this	O
domain	O
is	O
quite	O
distinct	O
,	O
as	O
the	O
background	O
and	O
the	O
strokes	O
are	O
no	O
longer	O
constant	O
.	O
Consequently	O
,	O
the	O
source	Method
-	Method
only	Method
model	Method
performs	O
poorly	O
.	O
Our	O
approach	O
succeeded	O
at	O
aligning	O
feature	O
distributions	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
which	O
led	O
to	O
successful	O
adaptation	Task
results	O
(	O
considering	O
that	O
the	O
adaptation	Task
is	O
unsupervised	O
)	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
improvement	O
over	O
source	Method
-	Method
only	Method
model	Method
achieved	O
by	O
subspace	Method
alignment	Method
(	O
SA	Method
)	O
is	O
quite	O
modest	O
,	O
thus	O
highlighting	O
the	O
difficulty	O
of	O
the	O
adaptation	Task
task	Task
.	O
Synthetic	Material
numbers	Material
→	O
SVHN	Material
.	O
To	O
address	O
a	O
common	O
scenario	O
of	O
training	O
on	O
synthetic	O
data	O
and	O
testing	O
on	O
real	O
data	O
,	O
we	O
use	O
Street	Material
-	Material
View	Material
House	Material
Number	Material
dataset	Material
SVHN	Material
as	O
the	O
target	O
domain	O
and	O
synthetic	O
digits	O
as	O
the	O
source	O
.	O
The	O
latter	O
(	O
Syn	Material
Numbers	Material
)	O
consists	O
of	O
500	O
,	O
000	O
images	O
generated	O
by	O
ourselves	O
from	O
Windows	O
fonts	O
by	O
varying	O
the	O
text	O
(	O
that	O
includes	O
different	O
one	O
-	O
,	O
two	O
-	O
,	O
and	O
three	O
-	O
digit	O
numbers	O
)	O
,	O
positioning	O
,	O
orientation	O
,	O
background	O
and	O
stroke	O
colors	O
,	O
and	O
the	O
amount	O
of	O
blur	O
.	O
The	O
degrees	O
of	O
variation	O
were	O
chosen	O
manually	O
to	O
simulate	O
SVHN	Material
,	O
however	O
the	O
two	O
datasets	O
are	O
still	O
rather	O
distinct	O
,	O
the	O
biggest	O
difference	O
being	O
the	O
structured	O
clutter	O
in	O
the	O
background	O
of	O
SVHN	Material
images	O
.	O
The	O
proposed	O
backpropagation	Method
-	Method
based	Method
technique	Method
works	O
well	O
covering	O
two	O
thirds	O
of	O
the	O
gap	O
between	O
training	O
with	O
source	O
data	O
only	O
and	O
training	O
on	O
target	O
domain	O
data	O
with	O
known	O
target	O
labels	O
.	O
In	O
contrast	O
,	O
SA	Method
does	O
not	O
result	O
in	O
any	O
significant	O
improvement	O
in	O
the	O
classification	O
accuracy	Metric
,	O
thus	O
highlighting	O
that	O
the	O
adaptation	Task
task	Task
is	O
even	O
more	O
challenging	O
than	O
in	O
the	O
case	O
of	O
the	O
MNIST	Material
experiment	O
.	O
MNIST	Material
↔	Material
SVHN	Material
.	O
In	O
this	O
experiment	O
,	O
we	O
further	O
increase	O
the	O
gap	O
between	O
distributions	O
,	O
and	O
test	O
on	O
MNIST	Material
and	O
SVHN	Material
,	O
which	O
are	O
significantly	O
different	O
in	O
appearance	O
.	O
Training	O
on	O
SVHN	Material
even	O
without	O
adaptation	Method
is	O
challenging	O
—	O
classification	Metric
error	Metric
stays	O
high	O
during	O
the	O
first	O
150	O
epochs	O
.	O
In	O
order	O
to	O
avoid	O
ending	O
up	O
in	O
a	O
poor	O
local	O
minimum	O
we	O
,	O
therefore	O
,	O
do	O
not	O
use	O
learning	Method
rate	Method
annealing	Method
here	O
.	O
Obviously	O
,	O
the	O
two	O
directions	O
(	O
MNIST	Material
SVHN	Material
and	O
SVHN	Material
MNIST	Material
)	O
are	O
not	O
equally	O
difficult	O
.	O
As	O
SVHN	Material
is	O
more	O
diverse	O
,	O
a	O
model	O
trained	O
on	O
SVHN	Material
is	O
expected	O
to	O
be	O
more	O
generic	O
and	O
to	O
perform	O
reasonably	O
on	O
the	O
MNIST	Material
dataset	Material
.	O
This	O
,	O
indeed	O
,	O
turns	O
out	O
to	O
be	O
the	O
case	O
and	O
is	O
supported	O
by	O
the	O
appearance	O
of	O
the	O
feature	O
distributions	O
.	O
We	O
observe	O
a	O
quite	O
strong	O
separation	O
between	O
the	O
domains	O
when	O
we	O
feed	O
them	O
into	O
the	O
CNN	Method
trained	O
solely	O
on	O
MNIST	Material
,	O
whereas	O
for	O
the	O
SVHN	Material
-	O
trained	O
network	O
the	O
features	O
are	O
much	O
more	O
intermixed	O
.	O
This	O
difference	O
probably	O
explains	O
why	O
our	O
method	O
succeeded	O
in	O
improving	O
the	O
performance	O
by	O
adaptation	Task
in	O
the	O
SVHN	Material
MNIST	Material
scenario	O
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
but	O
not	O
in	O
the	O
opposite	O
direction	O
(	O
SA	Method
is	O
not	O
able	O
to	O
perform	O
adaptation	Task
in	O
this	O
case	O
either	O
)	O
.	O
Unsupervised	Method
adaptation	Method
from	O
MNIST	Material
to	Material
SVHN	Material
gives	O
a	O
failure	O
example	O
for	O
our	O
approach	O
(	O
we	O
are	O
unaware	O
of	O
any	O
unsupervised	O
DA	Method
methods	O
capable	O
of	O
performing	O
such	O
adaptation	O
)	O
.	O
Synthetic	Method
Signs	Method
→	O
GTSRB	Method
.	O
Overall	O
,	O
this	O
setting	O
is	O
similar	O
to	O
the	O
Syn	Task
Numbers	Task
SVHN	Task
experiment	Task
,	O
except	O
the	O
distribution	O
of	O
the	O
features	O
is	O
more	O
complex	O
due	O
to	O
the	O
significantly	O
larger	O
number	O
of	O
classes	O
(	O
43	O
instead	O
of	O
10	O
)	O
.	O
For	O
the	O
source	O
domain	O
we	O
obtained	O
100	O
,	O
000	O
synthetic	O
images	O
(	O
which	O
we	O
call	O
Syn	O
Signs	O
)	O
simulating	O
various	O
photoshooting	O
conditions	O
.	O
Once	O
again	O
,	O
our	O
method	O
achieves	O
a	O
sensible	O
increase	O
in	O
performance	O
once	O
again	O
proving	O
its	O
suitability	O
for	O
the	O
synthetic	Task
-	Task
to	Task
-	Task
real	Task
data	Task
adaptation	Task
.	O
[	O
font=	O
]	O
[	O
width=0.95092height	O
=	O
at=	O
(	O
0	O
)	O
,	O
scale	O
only	O
axis	O
,	O
xmin=10000	O
,	O
xmax=50000	O
,	O
xlabel	O
=	O
Batches	O
seen	O
,	O
ymin=0	O
,	O
ymax=1	O
,	O
ylabel	O
=	O
Validation	Metric
error	Metric
,	O
axis	O
x	O
line*=bottom	O
,	O
axis	O
y	O
line*=left	O
,	O
legend	O
style	O
=	O
at=	O
(+(	O
1	O
,	O
1	O
)(-	O
⁢0.1cm	O
,-	O
⁢0.1cm	O
)),	O
anchor	O
=	O
north	O
east	O
,	O
align	O
=	O
left	O
,	O
legend	O
cell	O
align	O
=	O
left	O
,	O
draw	O
=	O
black	O
,	O
xmajorgrids	O
,	O
ymajorgrids	O
,	O
grid	O
style	O
=	O
dashed	O
]	O
width=1.0pt	O
]	O
table	O
[	O
row	O
sep	O
=	O
crcr	O
]	O
10500	O
0.19975799663299711000	O
0.1916298400673411500	O
0.19078808922558912000	O
0.19291877104377112500	O
0.19639099326599313000	O
0.18552714646464613500	O
0.19047243265993314000	O
0.18560606060606114500	O
0.18342276936026915000	O
0.18905197811447815500	O
0.19152462121212116000	O
0.18607954545454516500	O
0.17942445286195317000	O
0.18768413299663317500	O
0.18786826599326618000	O
0.18092382154882218500	O
0.18731586700336719000	O
0.17866161616161619500	O
0.1810290404040420000	O
0.18055555555555620500	O
0.17666245791245821000	O
0.18379103535353521500	O
0.17921401515151522000	O
0.17889835858585922500	O
0.17889835858585923000	O
0.17447916666666723500	O
0.17474221380471424000	O
0.17105955387205424500	O
0.17795138888888925000	O
0.17479482323232325500	O
0.17408459595959626000	O
0.17463699494949526500	O
0.16903409090909127000	O
0.17119107744107727500	O
0.17087542087542128000	O
0.17150673400673428500	O
0.17021780303030329000	O
0.16924452861952929500	O
0.16987584175084230000	O
0.16874473905723930500	O
0.1704808501683531000	O
0.16945496632996631500	O
0.16777146464646532000	O
0.16884995791245832500	O
0.16832386363636433000	O
0.16871843434343433500	O
0.16566708754208834000	O
0.16737689393939434500	O
0.16900778619528635000	O
0.16714015151515235500	O
0.16566708754208836000	O
0.16785037878787936500	O
0.16982323232323237000	O
0.17069128787878837500	O
0.1664036195286238000	O
0.16798190235690238500	O
0.16987584175084239000	O
0.16677188552188639500	O
0.16937605218855240000	O
0.16808712121212140500	O
0.16550925925925941000	O
0.16771885521885541500	O
0.16806081649831742000	O
0.16603535353535442500	O
0.16669297138047143000	O
0.16642992424242443500	O
0.16703493265993344000	O
0.17034932659932744500	O
0.16974431818181845000	O
0.16821864478114545500	O
0.16642992424242446000	O
0.16632470538720546500	O
0.16877104377104447000	O
0.16803451178451247500	O
0.16871843434343448000	O
0.17105955387205448500	O
0.17063867845117849000	O
0.1681923400673449500	O
0.16898148148148150000	O
0.167902988215488	O
;	O
data	O
only	O
;	O
[	O
color	O
=	O
cyan	O
,	O
solid	O
,	O
line	O
width=1.0pt	O
]	O
table	O
[	O
row	O
sep	O
=	O
crcr	O
]	O
10500	O
0.962511000	O
0.7976562511500	O
0.71562512000	O
0.614062512500	O
0.5210937513000	O
0.45937513500	O
0.448437514000	O
0.42187514500	O
0.3945312515000	O
0.410937515500	O
0.3429687516000	O
0.3687516500	O
0.335937517000	O
0.3617187517500	O
0.317187518000	O
0.348437518500	O
0.3242187519000	O
0.31562519500	O
0.34687520000	O
0.3187520500	O
0.3539062521000	O
0.326562521500	O
0.3335937522000	O
0.317187522500	O
0.2851562523000	O
0.3054687523500	O
0.30937524000	O
0.279687524500	O
0.3085937525000	O
0.3070312525500	O
0.307812526000	O
0.2867187526500	O
0.287527000	O
0.3148437527500	O
0.285937528000	O
0.2937528500	O
0.3132812529000	O
0.307812529500	O
0.285937530000	O
0.289062530500	O
0.28437531000	O
0.295312531500	O
0.2695312532000	O
0.2992187532500	O
0.3007812533000	O
0.264062533500	O
0.30937534000	O
0.273437534500	O
0.29062535000	O
0.2679687535500	O
0.301562536000	O
0.2679687536500	O
0.292187537000	O
0.26562537500	O
0.276562538000	O
0.285937538500	O
0.3210937539000	O
0.2804687539500	O
0.27540000	O
0.2492187540500	O
0.2914062541000	O
0.2664062541500	O
0.26562542000	O
0.25937542500	O
0.276562543000	O
0.2679687543500	O
0.276562544000	O
0.2726562544500	O
0.2554687545000	O
0.2648437545500	O
0.27187546000	O
0.270312546500	O
0.2617187547000	O
0.24687547500	O
0.2507812548000	O
0.2960937548500	O
0.264062549000	O
0.2687549500	O
0.2601562550000	O
0.2578125	O
;	O
data	O
only	O
;	O
[	O
color	O
=	O
red	O
,	O
solid	O
,	O
line	O
width=1.0pt	O
]	O
table	O
[	O
row	O
sep	O
=	O
crcr	O
]	O
10500	O
0.94389204545454511000	O
0.94389204545454511500	O
0.94389204545454512000	O
0.94389204545454512500	O
0.84830071548821613000	O
0.65872264309764313500	O
0.59059343434343414000	O
0.47548400673400714500	O
0.31394675925925915000	O
0.23569023569023615500	O
0.1787931397306416000	O
0.15238320707070716500	O
0.1291298400673417000	O
0.11447811447811417500	O
0.11621422558922618000	O
0.101562518500	O
0.1006681397306419000	O
0.10198337542087519500	O
0.091435185185185220000	O
0.089567550505050520500	O
0.089436026936026921000	O
0.082728324915824921500	O
0.079861111111111122000	O
0.085963804713804722500	O
0.079913720538720523000	O
0.077861952861952923500	O
0.073758417508417524000	O
0.074258207070707124500	O
0.077677819865319925000	O
0.077151725589225625500	O
0.072574705387205426000	O
0.073942550505050526500	O
0.073495370370370427000	O
0.073074494949494927500	O
0.068892045454545528000	O
0.070207281144781128500	O
0.07233796296296329000	O
0.067024410774410829500	O
0.073363846801346830000	O
0.066761363636363630500	O
0.069234006734006731000	O
0.065209385521885531500	O
0.066472011784511832000	O
0.065577651515151532500	O
0.067129629629629633000	O
0.065603956228956233500	O
0.064604377104377134000	O
0.066866582491582534500	O
0.063867845117845135000	O
0.06507786195286235500	O
0.064998947811447836000	O
0.067234848484848536500	O
0.066866582491582537000	O
0.062605218855218937500	O
0.065209385521885538000	O
0.062631523569023638500	O
0.062789351851851839000	O
0.061316287878787939500	O
0.06323653198653240000	O
0.062920875420875440500	O
0.063946759259259341000	O
0.061289983164983241500	O
0.065340909090909142000	O
0.060869107744107742500	O
0.061342592592592643000	O
0.063026094276094343500	O
0.06010627104377144000	O
0.063867845117845144500	O
0.060237794612794645000	O
0.057738846801346845500	O
0.06268413299663346000	O
0.060816498316498346500	O
0.060316708754208847000	O
0.057765151515151547500	O
0.058317550505050548000	O
0.059132996632996648500	O
0.060711279461279549000	O
0.058580597643097649500	O
0.058317550505050550000	O
0.0590540824915825	O
;	O
As	O
an	O
additional	O
experiment	O
,	O
we	O
also	O
evaluate	O
the	O
proposed	O
algorithm	O
for	O
semi	O
-	O
supervised	O
domain	Method
adaptation	Method
,	O
i.e.	O
when	O
one	O
is	O
additionally	O
provided	O
with	O
a	O
small	O
amount	O
of	O
labeled	O
target	O
data	O
.	O
For	O
that	O
purpose	O
we	O
split	O
GTSRB	Method
into	O
the	O
train	O
set	O
(	O
1280	O
random	O
samples	O
with	O
labels	O
)	O
and	O
the	O
validation	O
set	O
(	O
the	O
rest	O
of	O
the	O
dataset	O
)	O
.	O
The	O
validation	O
part	O
is	O
used	O
solely	O
for	O
the	O
evaluation	O
and	O
does	O
not	O
participate	O
in	O
the	O
adaptation	Task
.	O
The	O
training	O
procedure	O
changes	O
slightly	O
as	O
the	O
label	Method
predictor	Method
is	O
now	O
exposed	O
to	O
the	O
target	O
data	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
change	O
of	O
the	O
validation	Metric
error	Metric
throughout	O
the	O
training	O
.	O
While	O
the	O
graph	O
clearly	O
suggests	O
that	O
our	O
method	O
can	O
be	O
used	O
in	O
the	O
semi	Task
-	Task
supervised	Task
setting	Task
,	O
thorough	O
verification	O
of	O
semi	Task
-	Task
supervised	Task
setting	Task
is	O
left	O
for	O
future	O
work	O
.	O
Office	Material
dataset	O
.	O
We	O
finally	O
evaluate	O
our	O
method	O
on	O
Office	Material
dataset	O
,	O
which	O
is	O
a	O
collection	O
of	O
three	O
distinct	O
domains	O
:	O
Amazon	Material
,	O
DSLR	Material
,	O
and	O
Webcam	Material
.	O
Unlike	O
previously	O
discussed	O
datasets	O
,	O
Office	Material
is	O
rather	O
small	O
-	O
scale	O
with	O
only	O
2817	O
labeled	O
images	O
spread	O
across	O
31	O
different	O
categories	O
in	O
the	O
largest	O
domain	O
.	O
The	O
amount	O
of	O
available	O
data	O
is	O
crucial	O
for	O
a	O
successful	O
training	O
of	O
a	O
deep	Method
model	Method
,	O
hence	O
we	O
opted	O
for	O
the	O
fine	O
-	O
tuning	O
of	O
the	O
CNN	Method
pre	O
-	O
trained	O
on	O
the	O
ImageNet	Material
as	O
it	O
is	O
done	O
in	O
some	O
recent	O
DA	Method
works	O
.	O
We	O
make	O
our	O
approach	O
more	O
comparable	O
with	O
by	O
using	O
exactly	O
the	O
same	O
network	Method
architecture	Method
replacing	O
domain	Method
mean	Method
-	Method
based	Method
regularization	Method
with	O
the	O
domain	Method
classifier	Method
.	O
Following	O
most	O
previous	O
works	O
,	O
we	O
evaluate	O
our	O
method	O
using	O
5	O
random	O
splits	O
for	O
each	O
of	O
the	O
3	O
transfer	Task
tasks	Task
commonly	O
used	O
for	O
evaluation	O
.	O
Our	O
training	O
protocol	O
is	O
close	O
to	O
as	O
we	O
use	O
the	O
same	O
number	O
of	O
labeled	O
source	O
-	O
domain	O
images	O
per	O
category	O
.	O
Unlike	O
those	O
works	O
and	O
similarly	O
to	O
e.g.	O
DLID	Method
we	O
use	O
the	O
whole	O
unlabeled	O
target	O
domain	O
(	O
as	O
the	O
premise	O
of	O
our	O
method	O
is	O
the	O
abundance	O
of	O
unlabeled	O
data	O
in	O
the	O
target	O
domain	O
)	O
.	O
Under	O
this	O
transductive	Method
setting	Method
,	O
our	O
method	O
is	O
able	O
to	O
improve	O
previously	O
-	O
reported	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
for	O
unsupervised	Task
adaptation	Task
very	O
considerably	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
especially	O
in	O
the	O
most	O
challenging	O
Amazon	Material
Webcam	Material
scenario	Material
(	O
the	O
two	O
domains	O
with	O
the	O
largest	O
domain	O
shift	O
)	O
.	O
section	O
:	O
Discussion	O
We	O
have	O
proposed	O
a	O
new	O
approach	O
to	O
unsupervised	Task
domain	Task
adaptation	Task
of	O
deep	Method
feed	Method
-	Method
forward	Method
architectures	Method
,	O
which	O
allows	O
large	Task
-	Task
scale	Task
training	Task
based	O
on	O
large	O
amount	O
of	O
annotated	O
data	O
in	O
the	O
source	O
domain	O
and	O
large	O
amount	O
of	O
unannotated	O
data	O
in	O
the	O
target	O
domain	O
.	O
Similarly	O
to	O
many	O
previous	O
shallow	O
and	O
deep	O
DA	Method
techniques	O
,	O
the	O
adaptation	O
is	O
achieved	O
through	O
aligning	O
the	O
distributions	O
of	O
features	O
across	O
the	O
two	O
domains	O
.	O
However	O
,	O
unlike	O
previous	O
approaches	O
,	O
the	O
alignment	Task
is	O
accomplished	O
through	O
standard	O
backpropagation	Method
training	Method
.	O
The	O
approach	O
is	O
therefore	O
rather	O
scalable	O
,	O
and	O
can	O
be	O
implemented	O
using	O
any	O
deep	Method
learning	Method
package	Method
.	O
To	O
this	O
end	O
we	O
plan	O
to	O
release	O
the	O
source	O
code	O
for	O
the	O
Gradient	Method
Reversal	Method
layer	Method
along	O
with	O
the	O
usage	O
examples	O
as	O
an	O
extension	O
to	O
Caffe	Task
.	O
Further	O
evaluation	O
on	O
larger	Task
-	Task
scale	Task
tasks	Task
and	O
in	O
semi	Task
-	Task
supervised	Task
settings	Task
constitutes	O
future	O
work	O
.	O
It	O
is	O
also	O
interesting	O
whether	O
the	O
approach	O
can	O
benefit	O
from	O
a	O
good	O
initialization	O
of	O
the	O
feature	Method
extractor	Method
.	O
For	O
this	O
,	O
a	O
natural	O
choice	O
would	O
be	O
to	O
use	O
deep	Method
autoencoder	Method
/	O
deconvolution	Method
network	Method
trained	O
on	O
both	O
domains	O
(	O
or	O
on	O
the	O
target	O
domain	O
)	O
in	O
the	O
same	O
vein	O
as	O
,	O
effectively	O
using	O
as	O
an	O
initialization	O
to	O
our	O
method	O
.	O
appendix	O
:	O
An	O
alternative	O
optimization	Method
approach	Method
There	O
exists	O
an	O
alternative	O
construction	O
(	O
inspired	O
by	O
)	O
that	O
leads	O
to	O
the	O
same	O
updates	O
(	O
[	O
reference	O
]	O
)	O
-	O
(	O
[	O
reference	O
]	O
)	O
.	O
Rather	O
than	O
using	O
the	O
gradient	Method
reversal	Method
layer	Method
,	O
the	O
construction	O
introduces	O
two	O
different	O
loss	O
functions	O
for	O
the	O
domain	Method
classifier	Method
.	O
Minimization	Task
of	O
the	O
first	O
domain	O
loss	O
(	O
)	O
should	O
lead	O
to	O
a	O
better	O
domain	Task
discrimination	Task
,	O
while	O
the	O
second	O
domain	O
loss	O
(	O
)	O
is	O
minimized	O
when	O
the	O
domains	O
are	O
distinct	O
.	O
Stochastic	O
updates	O
for	O
and	O
are	O
then	O
defined	O
as	O
:	O
Thus	O
,	O
different	O
parameters	O
participate	O
in	O
the	O
optimization	Task
of	O
different	O
losses	O
In	O
this	O
framework	O
,	O
the	O
gradient	Method
reversal	Method
layer	Method
constitutes	O
a	O
special	O
case	O
,	O
corresponding	O
to	O
the	O
pair	O
of	O
domain	O
losses	O
.	O
However	O
,	O
other	O
pairs	O
of	O
loss	O
functions	O
can	O
be	O
used	O
.	O
One	O
example	O
would	O
be	O
the	O
binomial	O
cross	O
-	O
entropy	O
:	O
where	O
indicates	O
domain	O
indices	O
and	O
is	O
an	O
output	O
of	O
the	O
predictor	Method
.	O
In	O
that	O
case	O
“	O
adversarial	O
”	O
loss	O
is	O
easily	O
obtained	O
by	O
swapping	O
domain	O
labels	O
,	O
i.e.	O
.	O
This	O
particular	O
pair	O
has	O
a	O
potential	O
advantage	O
of	O
producing	O
stronger	O
gradients	O
at	O
early	O
learning	O
stages	O
if	O
the	O
domains	O
are	O
quite	O
dissimilar	O
.	O
In	O
our	O
experiments	O
,	O
however	O
,	O
we	O
did	O
not	O
observe	O
any	O
significant	O
improvement	O
resulting	O
from	O
this	O
choice	O
of	O
losses	O
.	O
appendix	O
:	O
CNN	Method
architectures	Method
fnodebottomRGB132	O
,	O
170	O
,	O
81	O
fnodetopRGB172	O
,	O
222	O
,	O
106	O
cnodebottomRGB120	O
,	O
128	O
,	O
164	O
cnodetopRGB158	O
,	O
167	O
,	O
218	O
dnodebottomRGB174	O
,	O
109	O
,	O
146	O
dnodetopRGB230	O
,	O
141	O
,	O
192	O
[	O
black!50	O
,	O
text	O
=	O
black	O
,	O
node	O
distance=4	O
mm	O
,	O
grlnode	O
/	O
.style=	O
align	O
=	O
center	O
,	O
circle	O
,	O
minimum	O
size=6	O
mm	O
,	O
inner	O
sep=5pt	O
,	O
very	O
thick	O
,	O
draw	O
=	O
black!50	O
,	O
font=	O
,	O
fnode	O
/	O
.style=	O
align	O
=	O
center	O
,	O
rectangle	O
,	O
minimum	O
size=6	O
mm	O
,	O
rounded	O
corners	O
,	O
inner	O
sep=5pt	O
,	O
very	O
thick	O
,	O
draw	O
=	O
black!50	O
,	O
top	O
color	O
=	O
fnodetop	O
,	O
bottom	O
color	O
=	O
fnodebottom	O
,	O
font=	O
,	O
cnode	O
/	O
.style=	O
fnode	O
,	O
top	O
color	O
=	O
cnodetop	O
,	O
bottom	O
color	O
=	O
cnodebottom	O
,	O
dnode	O
/	O
.style=	O
fnode	O
,	O
top	O
color	O
=	O
dnodetop	O
,	O
bottom	O
color	O
=	O
dnodebottom	O
,	O
vhedge	O
/	O
.style=	O
rounded	O
corners	O
,	O
to	O
path=—	O
-	O
(	O
)	O
]	O
