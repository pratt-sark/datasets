Modeling Task
the Task
distribution Task
of Task
natural Task
images Task
is O
a O
landmark Task
problem Task
in O
unsupervised Task
learning Task
. O
This O
task O
requires O
an O
image Method
model Method
that O
is O
at O
once O
expressive O
, O
tractable O
and O
scalable O
. O
We O
present O
a O
deep Method
neural Method
network Method
that O
sequentially O
predicts O
the O
pixels O
in O
an O
image O
along O
the O
two O
spatial O
dimensions O
. O
Our O
method O
models O
the O
discrete O
probability O
of O
the O
raw O
pixel O
values O
and O
encodes O
the O
complete O
set O
of O
dependencies O
in O
the O
image O
. O
Architectural O
novelties O
include O
fast O
two Method
- Method
dimensional Method
recurrent Method
layers Method
and O
an O
effective O
use O
of O
residual O
connections O
in O
deep Method
recurrent Method
networks Method
. O
We O
achieve O
log Metric
- Metric
likelihood Metric
scores Metric
on O
natural O
images O
that O
are O
considerably O
better O
than O
the O
previous O
state O
of O
the O
art O
. O
Our O
main O
results O
also O
provide O
benchmarks O
on O
the O
diverse O
ImageNet Material
dataset Material
. O
Samples O
generated O
from O
the O
model O
appear O
crisp O
, O
varied O
and O
globally O
coherent O
. O
PixelRecurrentNeuralNetworks O
section O
: O
Introduction O
Generative Method
image Method
modeling Method
is O
a O
central O
problem O
in O
unsupervised Task
learning Task
. O
Probabilistic Method
density Method
models Method
can O
be O
used O
for O
a O
wide O
variety O
of O
tasks O
that O
range O
from O
image Task
compression Task
and O
forms Task
of Task
reconstruction Task
such O
as O
image Task
inpainting Task
( O
e.g. O
, O
see O
Figure O
[ O
reference O
] O
) O
and O
deblurring Task
, O
to O
generation Task
of Task
new Task
images Task
. O
When O
the O
model O
is O
conditioned O
on O
external O
information O
, O
possible O
applications O
also O
include O
creating O
images O
based O
on O
text O
descriptions O
or O
simulating O
future O
frames O
in O
a O
planning Task
task Task
. O
One O
of O
the O
great O
advantages O
in O
generative Method
modeling Method
is O
that O
there O
are O
practically O
endless O
amounts O
of O
image O
data O
available O
to O
learn O
from O
. O
However O
, O
because O
images O
are O
high O
dimensional O
and O
highly O
structured O
, O
estimating O
the O
distribution Task
of Task
natural Task
images Task
is O
extremely O
challenging O
. O
occluded Task
completions Task
original O
One O
of O
the O
most O
important O
obstacles O
in O
generative Task
modeling Task
is O
building O
complex O
and O
expressive Method
models Method
that O
are O
also O
tractable O
and O
scalable O
. O
This O
trade O
- O
off O
has O
resulted O
in O
a O
large O
variety O
of O
generative Method
models Method
, O
each O
having O
their O
advantages O
. O
Most O
work O
focuses O
on O
stochastic Method
latent Method
variable Method
models Method
such O
as O
VAE Method
’s Method
that O
aim O
to O
extract O
meaningful O
representations O
, O
but O
often O
come O
with O
an O
intractable Task
inference Task
step Task
that O
can O
hinder O
their O
performance O
. O
One O
effective O
approach O
to O
tractably O
model O
a O
joint Task
distribution Task
of Task
the Task
pixels Task
in O
the O
image O
is O
to O
cast O
it O
as O
a O
product Method
of Method
conditional Method
distributions Method
; O
this O
approach O
has O
been O
adopted O
in O
autoregressive Method
models Method
such O
as O
NADE Method
and O
fully Method
visible Method
neural Method
networks Method
. O
The O
factorization Method
turns O
the O
joint Task
modeling Task
problem Task
into O
a O
sequence Task
problem Task
, O
where O
one O
learns O
to O
predict O
the O
next O
pixel O
given O
all O
the O
previously O
generated O
pixels O
. O
But O
to O
model O
the O
highly O
nonlinear O
and O
long O
- O
range O
correlations O
between O
pixels O
and O
the O
complex O
conditional O
distributions O
that O
result O
, O
a O
highly O
expressive O
sequence Method
model Method
is O
necessary O
. O
Recurrent Method
Neural Method
Networks Method
( O
RNN Method
) O
are O
powerful O
models O
that O
offer O
a O
compact O
, O
shared O
parametrization O
of O
a O
series O
of O
conditional O
distributions O
. O
RNNs Method
have O
been O
shown O
to O
excel O
at O
hard Task
sequence Task
problems Task
ranging O
from O
handwriting Task
generation Task
, O
to O
character Task
prediction Task
and O
to O
machine Task
translation Task
. O
A O
two O
- O
dimensional O
RNN Method
has O
produced O
very O
promising O
results O
in O
modeling O
grayscale O
images O
and O
textures O
. O
.14 O
.14 O
.16 O
In O
this O
paper O
we O
advance O
two O
- O
dimensional Method
RNNs Method
and O
apply O
them O
to O
large Task
- Task
scale Task
modeling Task
of Task
natural Task
images Task
. O
The O
resulting O
PixelRNNs Method
are O
composed O
of O
up O
to O
twelve O
, O
fast O
two O
- O
dimensional O
Long Method
Short Method
- Method
Term Method
Memory Method
( O
LSTM Method
) O
layers O
. O
These O
layers O
use O
LSTM Method
units O
in O
their O
state O
and O
adopt O
a O
convolution Method
to O
compute O
at O
once O
all O
the O
states O
along O
one O
of O
the O
spatial O
dimensions O
of O
the O
data O
. O
We O
design O
two O
types O
of O
these O
layers O
. O
The O
first O
type O
is O
the O
Row O
LSTM Method
layer O
where O
the O
convolution Method
is O
applied O
along O
each O
row O
; O
a O
similar O
technique O
is O
described O
in O
. O
The O
second O
type O
is O
the O
Diagonal Method
BiLSTM Method
layer Method
where O
the O
convolution Method
is O
applied O
in O
a O
novel O
fashion O
along O
the O
diagonals O
of O
the O
image O
. O
The O
networks O
also O
incorporate O
residual O
connections O
around O
LSTM Method
layers O
; O
we O
observe O
that O
this O
helps O
with O
training O
of O
the O
PixelRNN Method
for O
up O
to O
twelve O
layers O
of O
depth O
. O
We O
also O
consider O
a O
second O
, O
simplified O
architecture O
which O
shares O
the O
same O
core O
components O
as O
the O
PixelRNN Method
. O
We O
observe O
that O
Convolutional Method
Neural Method
Networks Method
( O
CNN Method
) Method
can O
also O
be O
used O
as O
sequence Method
model Method
with O
a O
fixed O
dependency O
range O
, O
by O
using O
Masked Method
convolutions Method
. O
The O
PixelCNN Method
architecture O
is O
a O
fully Method
convolutional Method
network Method
of Method
fifteen Method
layers Method
that O
preserves O
the O
spatial O
resolution O
of O
its O
input O
throughout O
the O
layers O
and O
outputs O
a O
conditional O
distribution O
at O
each O
location O
. O
Both O
PixelRNN Method
and O
PixelCNN Method
capture O
the O
full O
generality O
of O
pixel O
inter O
- O
dependencies O
without O
introducing O
independence O
assumptions O
as O
in O
e.g. O
, O
latent Method
variable Method
models Method
. O
The O
dependencies O
are O
also O
maintained O
between O
the O
RGB O
color O
values O
within O
each O
individual O
pixel O
. O
Furthermore O
, O
in O
contrast O
to O
previous O
approaches O
that O
model O
the O
pixels O
as O
continuous Method
values O
( O
e.g. O
, O
) O
, O
we O
model O
the O
pixels O
as O
discrete O
values O
using O
a O
multinomial Method
distribution Method
implemented O
with O
a O
simple O
softmax Method
layer Method
. O
We O
observe O
that O
this O
approach O
gives O
both O
representational O
and O
training O
advantages O
for O
our O
models O
. O
The O
contributions O
of O
the O
paper O
are O
as O
follows O
. O
In O
Section O
[ O
reference O
] O
we O
design O
two O
types O
of O
PixelRNNs Method
corresponding O
to O
the O
two O
types O
of O
LSTM Method
layers O
; O
we O
describe O
the O
purely O
convolutional O
PixelCNN Method
that O
is O
our O
fastest O
architecture O
; O
and O
we O
design O
a O
Multi Method
- Method
Scale Method
version Method
of O
the O
PixelRNN Method
. O
In O
Section O
[ O
reference O
] O
we O
show O
the O
relative O
benefits O
of O
using O
the O
discrete O
softmax O
distribution O
in O
our O
models O
and O
of O
adopting O
residual O
connections O
for O
the O
LSTM Method
layers O
. O
Next O
we O
test O
the O
models O
on O
MNIST Material
and O
on O
CIFAR Material
- Material
10 Material
and O
show O
that O
they O
obtain O
log Metric
- Metric
likelihood Metric
scores Metric
that O
are O
considerably O
better O
than O
previous O
results O
. O
We O
also O
provide O
results O
for O
the O
large O
- O
scale O
ImageNet Material
dataset Material
resized O
to O
both O
and O
pixels O
; O
to O
our O
knowledge O
likelihood O
values O
from O
generative Method
models Method
have O
not O
previously O
been O
reported O
on O
this O
dataset O
. O
Finally O
, O
we O
give O
a O
qualitative O
evaluation O
of O
the O
samples O
generated O
from O
the O
PixelRNNs Method
. O
section O
: O
Model O
Our O
aim O
is O
to O
estimate O
a O
distribution O
over O
natural O
images O
that O
can O
be O
used O
to O
tractably O
compute O
the O
likelihood O
of O
images O
and O
to O
generate O
new O
ones O
. O
The O
network O
scans O
the O
image O
one O
row O
at O
a O
time O
and O
one O
pixel O
at O
a O
time O
within O
each O
row O
. O
For O
each O
pixel O
it O
predicts O
the O
conditional O
distribution O
over O
the O
possible O
pixel O
values O
given O
the O
scanned O
context O
. O
Figure O
[ O
reference O
] O
illustrates O
this O
process O
. O
The O
joint O
distribution O
over O
the O
image O
pixels O
is O
factorized O
into O
a O
product Method
of Method
conditional Method
distributions Method
. O
The O
parameters O
used O
in O
the O
predictions O
are O
shared O
across O
all O
pixel O
positions O
in O
the O
image O
. O
To O
capture O
the O
generation Task
process Task
, O
propose O
to O
use O
a O
two O
- O
dimensional O
LSTM Method
network O
that O
starts O
at O
the O
top O
left O
pixel O
and O
proceeds O
towards O
the O
bottom O
right O
pixel O
. O
The O
advantage O
of O
the O
LSTM Method
network O
is O
that O
it O
effectively O
handles O
long O
- O
range O
dependencies O
that O
are O
central O
to O
object Task
and Task
scene Task
understanding Task
. O
The O
two O
- O
dimensional O
structure O
ensures O
that O
the O
signals O
are O
well O
propagated O
both O
in O
the O
left O
- O
to O
- O
right O
and O
top O
- O
to O
- O
bottom O
directions O
. O
In O
this O
section O
we O
first O
focus O
on O
the O
form O
of O
the O
distribution O
, O
whereas O
the O
next O
section O
will O
be O
devoted O
to O
describing O
the O
architectural Method
innovations Method
inside O
PixelRNN Method
. O
subsection O
: O
Generating O
an O
Image O
Pixel O
by O
Pixel O
The O
goal O
is O
to O
assign O
a O
probability O
to O
each O
image O
formed O
of O
pixels O
. O
We O
can O
write O
the O
image O
as O
a O
one O
- O
dimensional O
sequence O
where O
pixels O
are O
taken O
from O
the O
image O
row O
by O
row O
. O
To O
estimate O
the O
joint O
distribution O
we O
write O
it O
as O
the O
product O
of O
the O
conditional O
distributions O
over O
the O
pixels O
: O
The O
value O
is O
the O
probability O
of O
the O
- O
th O
pixel O
given O
all O
the O
previous O
pixels O
. O
The O
generation Task
proceeds O
row O
by O
row O
and O
pixel O
by O
pixel O
. O
Figure O
[ O
reference O
] O
( O
Left O
) O
illustrates O
the O
conditioning Method
scheme Method
. O
Each O
pixel O
is O
in O
turn O
jointly O
determined O
by O
three O
values O
, O
one O
for O
each O
of O
the O
color O
channels O
Red O
, O
Green O
and O
Blue O
( O
RGB O
) O
. O
We O
rewrite O
the O
distribution O
as O
the O
following O
product O
: O
Each O
of O
the O
colors O
is O
thus O
conditioned O
on O
the O
other O
channels O
as O
well O
as O
on O
all O
the O
previously O
generated O
pixels O
. O
Note O
that O
during O
training O
and O
evaluation Task
the O
distributions O
over O
the O
pixel O
values O
are O
computed O
in O
parallel O
, O
while O
the O
generation Task
of O
an O
image O
is O
sequential O
. O
subsection O
: O
Pixels O
as O
Discrete O
Variables O
Previous O
approaches O
use O
a O
continuous Method
distribution O
for O
the O
values O
of O
the O
pixels O
in O
the O
image O
( O
e.g. O
) O
. O
By O
contrast O
we O
model O
as O
a O
discrete O
distribution O
, O
with O
every O
conditional O
distribution O
in O
Equation O
[ O
reference O
] O
being O
a O
multinomial O
that O
is O
modeled O
with O
a O
softmax Method
layer Method
. O
Each O
channel O
variable O
simply O
takes O
one O
of O
256 O
distinct O
values O
. O
The O
discrete Method
distribution Method
is O
representationally O
simple O
and O
has O
the O
advantage O
of O
being O
arbitrarily O
multimodal O
without O
prior O
on O
the O
shape O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
. O
Experimentally O
we O
also O
find O
the O
discrete O
distribution O
to O
be O
easy O
to O
learn O
and O
to O
produce O
better O
performance O
compared O
to O
a O
continuous Method
distribution O
( O
Section O
[ O
reference O
] O
) O
. O
section O
: O
Pixel Method
Recurrent Method
Neural Method
Networks Method
In O
this O
section O
we O
describe O
the O
architectural Method
components Method
that O
compose O
the O
PixelRNN Method
. O
In O
Sections O
[ O
reference O
] O
and O
[ O
reference O
] O
, O
we O
describe O
the O
two O
types O
of O
LSTM Method
layers O
that O
use O
convolutions Method
to O
compute O
at O
once O
the O
states O
along O
one O
of O
the O
spatial O
dimensions O
. O
In O
Section O
[ O
reference O
] O
we O
describe O
how O
to O
incorporate O
residual O
connections O
to O
improve O
the O
training O
of O
a O
PixelRNN Method
with O
many O
LSTM Method
layers O
. O
In O
Section O
[ O
reference O
] O
we O
describe O
the O
softmax Method
layer Method
that O
computes O
the O
discrete O
joint O
distribution O
of O
the O
colors O
and O
the O
masking Method
technique Method
that O
ensures O
the O
proper O
conditioning Method
scheme Method
. O
In O
Section O
[ O
reference O
] O
we O
describe O
the O
PixelCNN Method
architecture O
. O
Finally O
in O
Section O
[ O
reference O
] O
we O
describe O
the O
multi Method
- Method
scale Method
architecture Method
. O
subsection O
: O
Row O
LSTM Method
The O
Row O
LSTM Method
is O
a O
unidirectional Method
layer Method
that O
processes O
the O
image O
row O
by O
row O
from O
top O
to O
bottom O
computing O
features O
for O
a O
whole O
row O
at O
once O
; O
the O
computation O
is O
performed O
with O
a O
one Method
- Method
dimensional Method
convolution Method
. O
For O
a O
pixel O
the O
layer O
captures O
a O
roughly O
triangular O
context O
above O
the O
pixel O
as O
shown O
in O
Figure O
[ O
reference O
] O
( O
center O
) O
. O
The O
kernel Method
of O
the O
one Method
- Method
dimensional Method
convolution Method
has O
size O
where O
; O
the O
larger O
the O
value O
of O
the O
broader O
the O
context O
that O
is O
captured O
. O
The O
weight O
sharing O
in O
the O
convolution Method
ensures O
translation O
invariance O
of O
the O
computed O
features O
along O
each O
row O
. O
The O
computation O
proceeds O
as O
follows O
. O
An O
LSTM Method
layer O
has O
an O
input Method
- Method
to Method
- Method
state Method
component Method
and O
a O
recurrent Method
state Method
- Method
to Method
- Method
state Method
component Method
that O
together O
determine O
the O
four O
gates O
inside O
the O
LSTM Method
core O
. O
To O
enhance O
parallelization O
in O
the O
Row O
LSTM Method
the O
input Task
- Task
to Task
- Task
state Task
component Task
is O
first O
computed O
for O
the O
entire O
two O
- O
dimensional O
input O
map O
; O
for O
this O
a O
convolution Method
is O
used O
to O
follow O
the O
row O
- O
wise O
orientation O
of O
the O
LSTM Method
itself O
. O
The O
convolution Method
is O
masked O
to O
include O
only O
the O
valid O
context O
( O
see O
Section O
[ O
reference O
] O
) O
and O
produces O
a O
tensor O
of O
size O
, O
representing O
the O
four O
gate O
vectors O
for O
each O
position O
in O
the O
input O
map O
, O
where O
is O
the O
number O
of O
output O
feature O
maps O
. O
To O
compute O
one O
step O
of O
the O
state Method
- Method
to Method
- Method
state Method
component Method
of O
the O
LSTM Method
layer O
, O
one O
is O
given O
the O
previous O
hidden O
and O
cell O
states O
and O
, O
each O
of O
size O
. O
The O
new O
hidden O
and O
cell O
states O
, O
are O
obtained O
as O
follows O
: O
where O
of O
size O
is O
row O
of O
the O
input O
map O
, O
and O
represents O
the O
convolution Method
operation Method
and O
the O
element Method
- Method
wise Method
multiplication Method
. O
The O
weights O
and O
are O
the O
kernel O
weights O
for O
the O
state O
- O
to O
- O
state O
and O
the O
input O
- O
to O
- O
state O
components O
, O
where O
the O
latter O
is O
precomputed O
as O
described O
above O
. O
In O
the O
case O
of O
the O
output O
, O
forget O
and O
input O
gates O
, O
and O
, O
the O
activation O
is O
the O
logistic Method
sigmoid Method
function Method
, O
whereas O
for O
the O
content O
gate O
, O
is O
the O
function O
. O
Each O
step O
computes O
at O
once O
the O
new O
state O
for O
an O
entire O
row O
of O
the O
input O
map O
. O
Because O
the O
Row O
LSTM Method
has O
a O
triangular O
receptive O
field O
( O
Figure O
[ O
reference O
] O
) O
, O
it O
is O
unable O
to O
capture O
the O
entire O
available O
context O
. O
subsection O
: O
Diagonal Method
BiLSTM Method
The O
Diagonal Method
BiLSTM Method
is O
designed O
to O
both O
parallelize O
the O
computation O
and O
to O
capture O
the O
entire O
available O
context O
for O
any O
image O
size O
. O
Each O
of O
the O
two O
directions O
of O
the O
layer O
scans O
the O
image O
in O
a O
diagonal O
fashion O
starting O
from O
a O
corner O
at O
the O
top O
and O
reaching O
the O
opposite O
corner O
at O
the O
bottom O
. O
Each O
step O
in O
the O
computation O
computes O
at O
once O
the O
LSTM Method
state O
along O
a O
diagonal O
in O
the O
image O
. O
Figure O
[ O
reference O
] O
( O
right O
) O
illustrates O
the O
computation O
and O
the O
resulting O
receptive O
field O
. O
The O
diagonal Task
computation Task
proceeds O
as O
follows O
. O
We O
first O
skew O
the O
input O
map O
into O
a O
space O
that O
makes O
it O
easy O
to O
apply O
convolutions Method
along O
diagonals O
. O
The O
skewing Method
operation Method
offsets O
each O
row O
of O
the O
input O
map O
by O
one O
position O
with O
respect O
to O
the O
previous O
row O
, O
as O
illustrated O
in O
Figure O
[ O
reference O
] O
; O
this O
results O
in O
a O
map O
of O
size O
. O
At O
this O
point O
we O
can O
compute O
the O
input O
- O
to O
- O
state O
and O
state O
- O
to O
- O
state O
components O
of O
the O
Diagonal Method
BiLSTM Method
. O
For O
each O
of O
the O
two O
directions O
, O
the O
input O
- O
to O
- O
state O
component O
is O
simply O
a O
convolution Method
that O
contributes O
to O
the O
four O
gates O
in O
the O
LSTM Method
core O
; O
the O
operation O
generates O
a O
tensor O
. O
The O
state Method
- Method
to Method
- Method
state Method
recurrent Method
component Method
is O
then O
computed O
with O
a O
column Method
- Method
wise Method
convolution Method
that O
has O
a O
kernel O
of O
size O
. O
The O
step O
takes O
the O
previous O
hidden O
and O
cell O
states O
, O
combines O
the O
contribution O
of O
the O
input O
- O
to O
- O
state O
component O
and O
produces O
the O
next O
hidden O
and O
cell O
states O
, O
as O
defined O
in O
Equation O
[ O
reference O
] O
. O
The O
output O
feature O
map O
is O
then O
skewed O
back O
into O
an O
map O
by O
removing O
the O
offset O
positions O
. O
This O
computation O
is O
repeated O
for O
each O
of O
the O
two O
directions O
. O
Given O
the O
two O
output O
maps O
, O
to O
prevent O
the O
layer O
from O
seeing O
future O
pixels O
, O
the O
right O
output O
map O
is O
then O
shifted O
down O
by O
one O
row O
and O
added O
to O
the O
left O
output O
map O
. O
Besides O
reaching O
the O
full O
dependency O
field O
, O
the O
Diagonal Method
BiLSTM Method
has O
the O
additional O
advantage O
that O
it O
uses O
a O
convolutional Method
kernel Method
of O
size O
that O
processes O
a O
minimal O
amount O
of O
information O
at O
each O
step O
yielding O
a O
highly O
non O
- O
linear Task
computation Task
. O
Kernel Method
sizes Method
larger O
than O
are O
not O
particularly O
useful O
as O
they O
do O
not O
broaden O
the O
already O
global O
receptive O
field O
of O
the O
Diagonal Method
BiLSTM Method
. O
subsection O
: O
Residual O
Connections O
We O
train O
PixelRNNs Method
of O
up O
to O
twelve O
layers O
of O
depth O
. O
As O
a O
means O
to O
both O
increase O
convergence Metric
speed Metric
and O
propagate O
signals O
more O
directly O
through O
the O
network O
, O
we O
deploy O
residual O
connections O
from O
one O
LSTM Method
layer O
to O
the O
next O
. O
Figure O
[ O
reference O
] O
shows O
a O
diagram O
of O
the O
residual O
blocks O
. O
The O
input O
map O
to O
the O
PixelRNN Method
LSTM Method
layer O
has O
features O
. O
The O
input Method
- Method
to Method
- Method
state Method
component Method
reduces O
the O
number O
of O
features O
by O
producing O
features O
per O
gate O
. O
After O
applying O
the O
recurrent Method
layer Method
, O
the O
output O
map O
is O
upsampled O
back O
to O
features O
per O
position O
via O
a O
convolution Method
and O
the O
input O
map O
is O
added O
to O
the O
output O
map O
. O
This O
method O
is O
related O
to O
previous O
approaches O
that O
use O
gating O
along O
the O
depth O
of O
the O
recurrent Method
network Method
, O
but O
has O
the O
advantage O
of O
not O
requiring O
additional O
gates O
. O
Apart O
from O
residual O
connections O
, O
one O
can O
also O
use O
learnable O
skip O
connections O
from O
each O
layer O
to O
the O
output O
. O
In O
the O
experiments O
we O
evaluate O
the O
relative O
effectiveness O
of O
residual O
and O
layer O
- O
to O
- O
output O
skip O
connections O
. O
.4 O
subsection O
: O
Masked Method
Convolution Method
The O
features O
for O
each O
input O
position O
at O
every O
layer O
in O
the O
network O
are O
split O
into O
three O
parts O
, O
each O
corresponding O
to O
one O
of O
the O
RGB O
channels O
. O
When O
predicting O
the O
R O
channel O
for O
the O
current O
pixel O
, O
only O
the O
generated O
pixels O
left O
and O
above O
of O
can O
be O
used O
as O
context O
. O
When O
predicting O
the O
G O
channel O
, O
the O
value O
of O
the O
R O
channel O
can O
also O
be O
used O
as O
context O
in O
addition O
to O
the O
previously O
generated O
pixels O
. O
Likewise O
, O
for O
the O
B O
channel O
, O
the O
values O
of O
both O
the O
R O
and O
G O
channels O
can O
be O
used O
. O
To O
restrict O
connections O
in O
the O
network O
to O
these O
dependencies O
, O
we O
apply O
a O
mask O
to O
the O
input O
- O
to O
- O
state O
convolutions O
and O
to O
other O
purely Method
convolutional Method
layers Method
in O
a O
PixelRNN Method
. O
We O
use O
two O
types O
of O
masks O
that O
we O
indicate O
with O
mask O
A O
and O
mask O
B O
, O
as O
shown O
in O
Figure O
[ O
reference O
] O
( O
Right O
) O
. O
Mask Method
A Method
is O
applied O
only O
to O
the O
first O
convolutional Method
layer Method
in O
a O
PixelRNN Method
and O
restricts O
the O
connections O
to O
those O
neighboring O
pixels O
and O
to O
those O
colors O
in O
the O
current O
pixels O
that O
have O
already O
been O
predicted O
. O
On O
the O
other O
hand O
, O
mask O
B O
is O
applied O
to O
all O
the O
subsequent O
input O
- O
to O
- O
state O
convolutional O
transitions O
and O
relaxes O
the O
restrictions O
of O
mask O
A O
by O
also O
allowing O
the O
connection O
from O
a O
color O
to O
itself O
. O
The O
masks O
can O
be O
easily O
implemented O
by O
zeroing O
out O
the O
corresponding O
weights O
in O
the O
input O
- O
to O
- O
state O
convolutions O
after O
each O
update O
. O
Similar O
masks O
have O
also O
been O
used O
in O
variational Method
autoencoders Method
. O
subsection O
: O
PixelCNN Method
The O
Row O
and O
Diagonal O
LSTM Method
layers O
have O
a O
potentially O
unbounded O
dependency O
range O
within O
their O
receptive O
field O
. O
This O
comes O
with O
a O
computational Metric
cost Metric
as O
each O
state O
needs O
to O
be O
computed O
sequentially O
. O
One O
simple O
workaround O
is O
to O
make O
the O
receptive O
field O
large O
, O
but O
not O
unbounded O
. O
We O
can O
use O
standard O
convolutional Method
layers Method
to O
capture O
a O
bounded O
receptive O
field O
and O
compute O
features O
for O
all O
pixel O
positions O
at O
once O
. O
The O
PixelCNN Method
uses O
multiple O
convolutional Method
layers Method
that O
preserve O
the O
spatial O
resolution O
; O
pooling O
layers O
are O
not O
used O
. O
Masks O
are O
adopted O
in O
the O
convolutions Method
to O
avoid O
seeing O
the O
future O
context O
; O
masks O
have O
previously O
also O
been O
used O
in O
non Method
- Method
convolutional Method
models Method
such O
as O
MADE Task
. O
Note O
that O
the O
advantage O
of O
parallelization O
of O
the O
PixelCNN Method
over O
the O
PixelRNN Method
is O
only O
available O
during O
training O
or O
during O
evaluating Task
of Task
test Task
images Task
. O
The O
image Task
generation Task
process Task
is O
sequential O
for O
both O
kinds O
of O
networks O
, O
as O
each O
sampled O
pixel O
needs O
to O
be O
given O
as O
input O
back O
into O
the O
network O
. O
subsection O
: O
Multi O
- O
Scale O
PixelRNN Method
The O
Multi O
- O
Scale O
PixelRNN Method
is O
composed O
of O
an O
unconditional O
PixelRNN Method
and O
one O
or O
more O
conditional O
PixelRNNs O
. O
The O
unconditional Method
network Method
first O
generates O
in O
the O
standard O
way O
a O
smaller O
image O
that O
is O
subsampled O
from O
the O
original O
image O
. O
The O
conditional Method
network Method
then O
takes O
the O
image O
as O
an O
additional O
input O
and O
generates O
a O
larger O
image O
, O
as O
shown O
in O
Figure O
[ O
reference O
] O
( O
Middle O
) O
. O
The O
conditional Method
network Method
is O
similar O
to O
a O
standard O
PixelRNN Method
, O
but O
each O
of O
its O
layers O
is O
biased O
with O
an O
upsampled Method
version Method
of O
the O
small O
image O
. O
The O
upsampling Method
and Method
biasing Method
processes Method
are O
defined O
as O
follows O
. O
In O
the O
upsampling Task
process Task
, O
one O
uses O
a O
convolutional Method
network Method
with O
deconvolutional Method
layers Method
to O
construct O
an O
enlarged O
feature O
map O
of O
size O
, O
where O
is O
the O
number O
of O
features O
in O
the O
output O
map O
of O
the O
upsampling Method
network Method
. O
Then O
, O
in O
the O
biasing Method
process Method
, O
for O
each O
layer O
in O
the O
conditional O
PixelRNN Method
, O
one O
simply O
maps O
the O
conditioning O
map O
into O
a O
map O
that O
is O
added O
to O
the O
input O
- O
to O
- O
state O
map O
of O
the O
corresponding O
layer O
; O
this O
is O
performed O
using O
a O
unmasked Method
convolution Method
. O
The O
larger O
image O
is O
then O
generated O
as O
usual O
. O
section O
: O
Specifications O
of O
Models O
In O
this O
section O
we O
give O
the O
specifications O
of O
the O
PixelRNNs Method
used O
in O
the O
experiments O
. O
We O
have O
four O
types O
of O
networks O
: O
the O
PixelRNN Method
based O
on O
Row O
LSTM Method
, O
the O
one O
based O
on O
Diagonal Method
BiLSTM Method
, O
the O
fully Method
convolutional Method
one Method
and O
the O
Multi Method
- Method
Scale Method
one Method
. O
Table O
[ O
reference O
] O
specifies O
each O
layer O
in O
the O
single Task
- Task
scale Task
networks Task
. O
The O
first O
layer O
is O
a O
convolution Method
that O
uses O
the O
mask O
of O
type O
A. O
The O
two O
types O
of O
LSTM Method
networks O
then O
use O
a O
variable O
number O
of O
recurrent Method
layers Method
. O
The O
input Task
- Task
to Task
- Task
state Task
convolution Task
in O
this O
layer O
uses O
a O
mask O
of O
type O
B O
, O
whereas O
the O
state Method
- Method
to Method
- Method
state Method
convolution Method
is O
not O
masked O
. O
The O
PixelCNN Method
uses O
convolutions Method
of Method
size Method
with O
a O
mask O
of O
type O
B. O
The O
top O
feature O
map O
is O
then O
passed O
through O
a O
couple O
of O
layers O
consisting O
of O
a O
Rectified Method
Linear Method
Unit Method
( O
ReLU Method
) O
and O
a O
convolution Method
. O
For O
the O
CIFAR Material
- Material
10 Material
and O
ImageNet Material
experiments Material
, O
these O
layers O
have O
1024 O
feature O
maps O
; O
for O
the O
MNIST Material
experiment O
, O
the O
layers O
have O
32 O
feature O
maps O
. O
Residual O
and O
layer O
- O
to O
- O
output O
connections O
are O
used O
across O
the O
layers O
of O
all O
three O
networks O
. O
The O
networks O
used O
in O
the O
experiments O
have O
the O
following O
hyperparameters O
. O
For O
MNIST Material
we O
use O
a O
Diagonal Method
BiLSTM Method
with O
7 O
layers O
and O
a O
value O
of O
( O
Section O
[ O
reference O
] O
and O
Figure O
[ O
reference O
] O
right O
) O
. O
For O
CIFAR Material
- Material
10 Material
the O
Row Method
and Method
Diagonal Method
BiLSTMs Method
have O
12 O
layers O
and O
a O
number O
of O
units O
. O
The O
PixelCNN Method
has O
15 O
layers O
and O
. O
For O
ImageNet Material
we O
adopt O
a O
12 O
layer O
Row O
LSTM Method
with O
units O
and O
for O
ImageNet Material
we O
use O
a O
4 O
layer O
Row O
LSTM Method
with O
units O
; O
the O
latter O
model O
does O
not O
use O
residual O
connections O
. O
section O
: O
Experiments O
In O
this O
section O
we O
describe O
our O
experiments O
and O
results O
. O
We O
begin O
by O
describing O
the O
way O
we O
evaluate O
and O
compare O
our O
results O
. O
In O
Section O
[ O
reference O
] O
we O
give O
details O
about O
the O
training Task
. O
Then O
we O
give O
results O
on O
the O
relative O
effectiveness O
of O
architectural Method
components Method
and O
our O
best O
results O
on O
the O
MNIST Material
, O
CIFAR Material
- Material
10 Material
and O
ImageNet Material
datasets Material
. O
subsection O
: O
Evaluation O
All O
our O
models O
are O
trained O
and O
evaluated O
on O
the O
log O
- O
likelihood O
loss O
function O
coming O
from O
a O
discrete O
distribution O
. O
Although O
natural O
image O
data O
is O
usually O
modeled O
with O
continuous Method
distributions O
using O
density Method
functions Method
, O
we O
can O
compare O
our O
results O
with O
previous O
art O
in O
the O
following O
way O
. O
In O
the O
literature O
it O
is O
currently O
best O
practice O
to O
add O
real O
- O
valued O
noise O
to O
the O
pixel O
values O
to O
dequantize O
the O
data O
when O
using O
density Method
functions Method
. O
When O
uniform O
noise O
is O
added O
( O
with O
values O
in O
the O
interval O
[ O
0 O
, O
1 O
] O
) O
, O
then O
the O
log O
- O
likelihoods O
of O
continuous Method
and O
discrete Method
models Method
are O
directly O
comparable O
. O
In O
our O
case O
, O
we O
can O
use O
the O
values O
from O
the O
discrete Method
distribution Method
as O
a O
piecewise O
- O
uniform O
continuous Method
function O
that O
has O
a O
constant O
value O
for O
every O
interval O
. O
This O
corresponding O
distribution O
will O
have O
the O
same O
log O
- O
likelihood O
( O
on O
data O
with O
added O
noise O
) O
as O
the O
original O
discrete Method
distribution Method
( O
on O
discrete O
data O
) O
. O
For O
MNIST Material
we O
report O
the O
negative Metric
log Metric
- Metric
likelihood Metric
in O
nats Method
as O
it O
is O
common O
practice O
in O
literature O
. O
For O
CIFAR Material
- Material
10 Material
and O
ImageNet Material
we O
report O
negative Metric
log Metric
- Metric
likelihoods Metric
in O
bits O
per O
dimension O
. O
The O
total O
discrete Metric
log Metric
- Metric
likelihood Metric
is O
normalized O
by O
the O
dimensionality O
of O
the O
images O
( O
e.g. O
, O
for O
CIFAR Material
- Material
10 Material
) O
. O
These O
numbers O
are O
interpretable O
as O
the O
number O
of O
bits O
that O
a O
compression Method
scheme Method
based O
on O
this O
model O
would O
need O
to O
compress O
every O
RGB O
color O
value O
; O
in O
practice O
there O
is O
also O
a O
small O
overhead O
due O
to O
arithmetic Method
coding Method
. O
subsection O
: O
Training O
Details O
Our O
models O
are O
trained O
on O
GPUs Method
using O
the O
Torch Method
toolbox Method
. O
From O
the O
different O
parameter Method
update Method
rules Method
tried O
, O
RMSProp Method
gives O
best O
convergence Metric
performance O
and O
is O
used O
for O
all O
experiments O
. O
The O
learning Metric
rate Metric
schedules Metric
were O
manually O
set O
for O
every O
dataset O
to O
the O
highest O
values O
that O
allowed O
fast O
convergence O
. O
The O
batch O
sizes O
also O
vary O
for O
different O
datasets O
. O
For O
smaller O
datasets O
such O
as O
MNIST Material
and O
CIFAR Material
- Material
10 Material
we O
use O
smaller O
batch O
sizes O
of O
16 O
images O
as O
this O
seems O
to O
regularize O
the O
models O
. O
For O
ImageNet Material
we O
use O
as O
large O
a O
batch O
size O
as O
allowed O
by O
the O
GPU O
memory O
; O
this O
corresponds O
to O
64 O
images O
/ O
batch O
for O
ImageNet Material
, O
and O
32 O
images O
/ O
batch O
for O
ImageNet Material
. O
Apart O
from O
scaling O
and O
centering O
the O
images O
at O
the O
input O
of O
the O
network O
, O
we O
do O
n’t O
use O
any O
other O
preprocessing Method
or O
augmentation Task
. O
For O
the O
multinomial O
loss O
function O
we O
use O
the O
raw O
pixel O
color O
values O
as O
categories O
. O
For O
all O
the O
PixelRNN Method
models O
, O
we O
learn O
the O
initial O
recurrent O
state O
of O
the O
network O
. O
subsection O
: O
Discrete Method
Softmax Method
Distribution Method
Apart O
from O
being O
intuitive O
and O
easy O
to O
implement O
, O
we O
find O
that O
using O
a O
softmax Method
on O
discrete O
pixel O
values O
instead O
of O
a O
mixture Method
density Method
approach Method
on O
continuous Method
pixel O
values O
gives O
better O
results O
. O
For O
the O
Row O
LSTM Method
model O
with O
a O
softmax O
output O
distribution O
we O
obtain O
3.06 O
bits O
/ O
dim O
on O
the O
CIFAR Material
- Material
10 Material
validation O
set O
. O
For O
the O
same O
model O
with O
a O
Mixture Method
of Method
Conditional Method
Gaussian Method
Scale Method
Mixtures Method
( O
MCGSM Method
) O
we O
obtain O
3.22 O
bits O
/ O
dim O
. O
In O
Figure O
[ O
reference O
] O
we O
show O
a O
few O
softmax O
activations O
from O
the O
model O
. O
Although O
we O
do O
n’t O
embed O
prior O
information O
about O
the O
meaning O
or O
relations O
of O
the O
256 O
color O
categories O
, O
e.g. O
that O
pixel O
values O
51 O
and O
52 O
are O
neighbors O
, O
the O
distributions O
predicted O
by O
the O
model O
are O
meaningful O
and O
can O
be O
multimodal O
, O
skewed O
, O
peaked O
or O
long O
tailed O
. O
Also O
note O
that O
values O
0 O
and O
255 O
often O
get O
a O
much O
higher O
probability O
as O
they O
are O
more O
frequent O
. O
Another O
advantage O
of O
the O
discrete Method
distribution Method
is O
that O
we O
do O
not O
worry O
about O
parts O
of O
the O
distribution O
mass O
lying O
outside O
the O
interval O
[ O
0 O
, O
255 O
] O
, O
which O
is O
something O
that O
typically O
happens O
with O
continuous Method
distributions O
. O
.5 O
.5 O
subsection O
: O
Residual O
Connections O
Another O
core O
component O
of O
the O
networks O
is O
residual O
connections O
. O
In O
Table O
[ O
reference O
] O
we O
show O
the O
results O
of O
having O
residual O
connections O
, O
having O
standard O
skip O
connections O
or O
having O
both O
, O
in O
the O
12 O
- O
layer O
CIFAR Material
- Material
10 Material
Row O
LSTM Method
model O
. O
We O
see O
that O
using O
residual O
connections O
is O
as O
effective O
as O
using O
skip O
connections O
; O
using O
both O
is O
also O
effective O
and O
preserves O
the O
advantage O
. O
When O
using O
both O
the O
residual O
and O
skip O
connections O
, O
we O
see O
in O
Table O
[ O
reference O
] O
that O
performance O
of O
the O
Row O
LSTM Method
improves O
with O
increased O
depth O
. O
This O
holds O
for O
up O
to O
the O
12 O
LSTM Method
layers O
that O
we O
tried O
. O
subsection O
: O
MNIST Material
Although O
the O
goal O
of O
our O
work O
was O
to O
model O
natural O
images O
on O
a O
large O
scale O
, O
we O
also O
tried O
our O
model O
on O
the O
binary Method
version Method
of O
MNIST Material
as O
it O
is O
a O
good O
sanity O
check O
and O
there O
is O
a O
lot O
of O
previous O
art O
on O
this O
dataset O
to O
compare O
with O
. O
In O
Table O
[ O
reference O
] O
we O
report O
the O
performance O
of O
the O
Diagonal Method
BiLSTM Method
model Method
and O
that O
of O
previous O
published O
results O
. O
To O
our O
knowledge O
this O
is O
the O
best O
reported O
result O
on O
MNIST Material
so O
far O
. O
.5 O
.5 O
subsection O
: O
CIFAR Material
- Material
10 Material
Next O
we O
test O
our O
models O
on O
the O
CIFAR Material
- Material
10 Material
dataset O
. O
Table O
[ O
reference O
] O
lists O
the O
results O
of O
our O
models O
and O
that O
of O
previously O
published O
approaches O
. O
All O
our O
results O
were O
obtained O
without O
data Method
augmentation Method
. O
For O
the O
proposed O
networks O
, O
the O
Diagonal Method
BiLSTM Method
has O
the O
best O
performance O
, O
followed O
by O
the O
Row O
LSTM Method
and O
the O
PixelCNN Method
. O
This O
coincides O
with O
the O
size O
of O
the O
respective O
receptive O
fields O
: O
the O
Diagonal Method
BiLSTM Method
has O
a O
global O
view O
, O
the O
Row O
LSTM Method
has O
a O
partially O
occluded O
view O
and O
the O
PixelCNN Method
sees O
the O
fewest O
pixels O
in O
the O
context O
. O
This O
suggests O
that O
effectively O
capturing O
a O
large O
receptive O
field O
is O
important O
. O
Figure O
[ O
reference O
] O
( O
left O
) O
shows O
CIFAR Material
- Material
10 Material
samples O
generated O
from O
the O
Diagonal Method
BiLSTM Method
. O
occluded O
completions O
original O
subsection O
: O
ImageNet Material
Although O
to O
our O
knowledge O
the O
are O
no O
published O
results O
on O
the O
ILSVRC O
ImageNet Material
dataset Material
that O
we O
can O
compare O
our O
models O
with O
, O
we O
give O
our O
ImageNet Metric
log Metric
- Metric
likelihood Metric
performance O
in O
Table O
[ O
reference O
] O
( O
without O
data Method
augmentation Method
) O
. O
On O
ImageNet Material
the O
current O
PixelRNNs Method
do O
not O
appear O
to O
overfit O
, O
as O
we O
saw O
that O
their O
validation O
performance O
improved O
with O
size O
and O
depth O
. O
The O
main O
constraint O
on O
model Metric
size Metric
are O
currently O
computation Metric
time Metric
and O
GPU O
memory O
. O
Note O
that O
the O
ImageNet Method
models Method
are O
in O
general O
less O
compressible O
than O
the O
CIFAR Material
- Material
10 Material
images O
. O
ImageNet Material
has O
greater O
variety O
of O
images O
, O
and O
the O
CIFAR Material
- Material
10 Material
images O
were O
most O
likely O
resized O
with O
a O
different O
algorithm O
than O
the O
one O
we O
used O
for O
ImageNet Material
images Material
. O
The O
ImageNet Material
images Material
are O
less O
blurry O
, O
which O
means O
neighboring O
pixels O
are O
less O
correlated O
to O
each O
other O
and O
thus O
less O
predictable O
. O
Because O
the O
downsampling Method
method Method
can O
influence O
the O
compression Metric
performance Metric
, O
we O
have O
made O
the O
used O
downsampled O
images O
available O
. O
Figure O
[ O
reference O
] O
( O
right O
) O
shows O
samples O
drawn O
from O
our O
model O
trained O
on O
ImageNet Material
. O
Figure O
[ O
reference O
] O
shows O
samples O
from O
the O
same O
model O
with O
and O
without O
multi O
- O
scale O
conditioning O
. O
Finally O
, O
we O
also O
show O
image O
completions O
sampled O
from O
the O
model O
in O
Figure O
[ O
reference O
] O
. O
section O
: O
Conclusion O
In O
this O
paper O
we O
significantly O
improve O
and O
build O
upon O
deep Method
recurrent Method
neural Method
networks Method
as O
generative Method
models Method
for O
natural Material
images Material
. O
We O
have O
described O
novel O
two O
- O
dimensional O
LSTM Method
layers O
: O
the O
Row O
LSTM Method
and O
the O
Diagonal Method
BiLSTM Method
, O
that O
scale O
more O
easily O
to O
larger O
datasets O
. O
The O
models O
were O
trained O
to O
model O
the O
raw O
RGB O
pixel O
values O
. O
We O
treated O
the O
pixel O
values O
as O
discrete O
random O
variables O
by O
using O
a O
softmax Method
layer Method
in O
the O
conditional O
distributions O
. O
We O
employed O
masked Method
convolutions Method
to O
allow O
PixelRNNs O
to O
model O
full O
dependencies O
between O
the O
color O
channels O
. O
We O
proposed O
and O
evaluated O
architectural O
improvements O
in O
these O
models O
resulting O
in O
PixelRNNs Method
with O
up O
to O
12 O
LSTM Method
layers O
. O
We O
have O
shown O
that O
the O
PixelRNNs Method
significantly O
improve O
the O
state O
of O
the O
art O
on O
the O
MNIST Material
and O
CIFAR Material
- Material
10 Material
datasets Material
. O
We O
also O
provide O
new O
benchmarks O
for O
generative Task
image Task
modeling Task
on O
the O
ImageNet Material
dataset Material
. O
Based O
on O
the O
samples O
and O
completions O
drawn O
from O
the O
models O
we O
can O
conclude O
that O
the O
PixelRNNs Method
are O
able O
to O
model O
both O
spatially O
local O
and O
long O
- O
range O
correlations O
and O
are O
able O
to O
produce O
images O
that O
are O
sharp O
and O
coherent O
. O
Given O
that O
these O
models O
improve O
as O
we O
make O
them O
larger O
and O
that O
there O
is O
practically O
unlimited O
data O
available O
to O
train O
on O
, O
more O
computation O
and O
larger O
models O
are O
likely O
to O
further O
improve O
the O
results O
. O
section O
: O
Acknowledgements O
The O
authors O
would O
like O
to O
thank O
Shakir O
Mohamed O
and O
Guillaume O
Desjardins O
for O
helpful O
input O
on O
this O
paper O
and O
Lucas O
Theis O
, O
Alex O
Graves O
, O
Karen O
Simonyan O
, O
Lasse O
Espeholt O
, O
Danilo O
Rezende O
, O
Karol O
Gregor O
and O
Ivo O
Danihelka O
for O
insightful O
discussions O
. O
bibliography O
: O
References O
