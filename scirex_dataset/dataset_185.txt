document	O
:	O
Explicit	Method
Interaction	Method
Model	Method
towards	O
Text	Task
Classification	Task
Text	Task
classification	Task
is	O
one	O
of	O
the	O
fundamental	O
tasks	O
in	O
natural	Task
language	Task
processing	Task
.	O
Recently	O
,	O
deep	Method
neural	Method
networks	Method
have	O
achieved	O
promising	O
performance	O
in	O
the	O
text	Task
classification	Task
task	Task
compared	O
to	O
shallow	Method
models	Method
.	O
Despite	O
of	O
the	O
significance	O
of	O
deep	Method
models	Method
,	O
they	O
ignore	O
the	O
fine	O
-	O
grained	O
(	O
matching	O
signals	O
between	O
words	O
and	O
classes	O
)	O
classification	O
clues	O
since	O
their	O
classifications	O
mainly	O
rely	O
on	O
the	O
text	Method
-	Method
level	Method
representations	Method
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
introduce	O
the	O
interaction	Method
mechanism	Method
to	O
incorporate	O
word	O
-	O
level	O
matching	O
signals	O
into	O
the	O
text	Task
classification	Task
task	Task
.	O
In	O
particular	O
,	O
we	O
design	O
a	O
novel	O
framework	O
,	O
EXplicit	Method
interAction	Method
Model	Method
(	O
dubbed	O
as	O
EXAM	Method
)	O
,	O
equipped	O
with	O
the	O
interaction	Method
mechanism	Method
.	O
We	O
justified	O
the	O
proposed	O
approach	O
on	O
several	O
benchmark	O
datasets	O
including	O
both	O
multi	Task
-	Task
label	Task
and	O
multi	Task
-	Task
class	Task
text	Task
classification	Task
tasks	Task
.	O
Extensive	O
experimental	O
results	O
demonstrate	O
the	O
superiority	O
of	O
the	O
proposed	O
method	O
.	O
As	O
a	O
byproduct	O
,	O
we	O
have	O
released	O
the	O
codes	O
and	O
parameter	O
settings	O
to	O
facilitate	O
other	O
researches	O
.	O
section	O
:	O
Introduction	O
Text	Task
classification	Task
is	O
one	O
of	O
the	O
fundamental	O
tasks	O
in	O
natural	Task
language	Task
processing	Task
,	O
targeting	O
at	O
classifying	O
a	O
piece	O
of	O
text	O
content	O
into	O
one	O
or	O
multiple	O
categories	O
.	O
According	O
to	O
the	O
number	O
of	O
desired	O
categories	O
,	O
text	Task
classification	Task
can	O
be	O
divided	O
into	O
two	O
groups	O
,	O
namely	O
,	O
multi	Task
-	Task
label	Task
(	O
multiple	O
categories	O
)	O
and	O
multi	Task
-	Task
class	Task
(	O
unique	O
category	O
)	O
.	O
For	O
instance	O
,	O
classifying	O
an	O
article	O
into	O
different	O
topics	O
(	O
e.g.	O
,	O
machine	Task
learning	Task
or	O
data	Task
mining	Task
)	O
falls	O
into	O
the	O
former	O
one	O
since	O
an	O
article	O
could	O
be	O
under	O
several	O
topics	O
simultaneously	O
.	O
By	O
contrast	O
,	O
classifying	O
a	O
comment	O
of	O
a	O
movie	O
into	O
its	O
corresponding	O
rating	O
level	O
lies	O
into	O
the	O
multi	Task
-	Task
class	Task
group	O
.	O
Both	O
multi	Task
-	Task
label	Task
and	O
multi	Task
-	Task
class	Task
text	Task
classifications	Task
have	O
been	O
widely	O
applied	O
in	O
many	O
fields	O
like	O
sentimental	Task
analysis	Task
,	O
topic	Task
tagging	Task
,	O
and	O
document	Task
classification	Task
.	O
Feature	Method
engineering	Method
dominates	O
the	O
performance	O
of	O
traditional	O
shallow	Method
text	Method
classification	Method
methods	Method
for	O
a	O
very	O
long	O
time	O
.	O
Various	O
rule	Method
-	Method
based	Method
and	Method
statistical	Method
features	Method
like	O
bag	O
-	O
of	O
-	O
words	O
and	O
N	Method
-	Method
grams	Method
are	O
designed	O
to	O
describe	O
the	O
text	O
,	O
and	O
fed	O
into	O
the	O
shallow	Method
machine	Method
learning	Method
models	Method
such	O
as	O
Linear	Method
Regression	Method
and	O
Support	Method
Vector	Method
Machine	Method
to	O
make	O
the	O
judgment	O
.	O
Traditional	O
solutions	O
suffer	O
from	O
two	O
defects	O
:	O
1	O
)	O
High	O
labor	O
intensity	O
for	O
the	O
manually	O
crafted	O
features	O
,	O
and	O
2	O
)	O
data	O
sparsity	O
(	O
a	O
N	O
-	O
grams	O
could	O
occur	O
only	O
several	O
times	O
in	O
a	O
given	O
dataset	O
)	O
.	O
Recently	O
,	O
owing	O
to	O
the	O
ability	O
of	O
tackling	O
the	O
aforementioned	O
problems	O
,	O
deep	Method
neural	Method
networks	Method
have	O
become	O
the	O
promising	O
solutions	O
for	O
the	O
text	Task
classification	Task
.	O
Deep	Method
neural	Method
networks	Method
typically	O
learn	O
a	O
word	Method
-	Method
level	Method
representation	Method
for	O
the	O
input	O
text	O
,	O
which	O
is	O
usually	O
a	O
matrix	O
with	O
each	O
row	O
/	O
column	O
as	O
an	O
embedding	O
of	O
a	O
word	O
in	O
the	O
text	O
.	O
They	O
then	O
compress	O
the	O
word	Method
-	Method
level	Method
representation	Method
into	O
a	O
text	Method
-	Method
level	Method
representation	Method
(	O
vector	Method
)	O
with	O
aggregation	Method
operations	Method
(	O
e.g.	O
,	O
pooling	Method
)	O
.	O
Thereafter	O
,	O
a	O
fully	Method
-	Method
connected	Method
(	O
FC	Method
)	O
layer	O
at	O
the	O
topmost	O
of	O
the	O
network	O
is	O
appended	O
to	O
make	O
the	O
final	O
decision	O
.	O
Note	O
that	O
these	O
solutions	O
are	O
also	O
called	O
encoding	Method
-	Method
based	Method
methods	Method
,	O
since	O
they	O
encode	O
the	O
textual	O
content	O
into	O
a	O
latent	Method
vector	Method
representation	Method
.	O
Although	O
great	O
success	O
has	O
been	O
achieved	O
,	O
these	O
deep	Method
neural	Method
network	Method
based	Method
solutions	Method
naturally	O
ignore	O
the	O
fine	O
-	O
grained	O
classification	O
clues	O
(	O
i.e.	O
,	O
matching	O
signals	O
between	O
words	O
and	O
classes	O
)	O
,	O
since	O
their	O
classifications	O
are	O
based	O
on	O
text	Method
-	Method
level	Method
representations	Method
.	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
classification	Method
(	O
i.e.	O
,	O
FC	Method
)	O
layer	O
of	O
these	O
solutions	O
matches	O
the	O
text	Method
-	Method
level	Method
representation	Method
with	O
class	Method
representations	Method
via	O
a	O
dot	Method
-	Method
product	Method
operation	Method
.	O
Mathematically	O
,	O
it	O
interprets	O
the	O
parameter	O
matrix	O
of	O
the	O
FC	Method
layer	O
as	O
a	O
set	O
of	O
class	Method
representations	Method
(	O
each	O
column	O
is	O
associated	O
with	O
a	O
class	O
)	O
.	O
As	O
such	O
,	O
the	O
probability	O
of	O
the	O
text	O
belonging	O
to	O
a	O
class	O
is	O
largely	O
determined	O
by	O
their	O
overall	O
matching	Metric
score	Metric
regardless	O
of	O
word	O
-	O
level	O
matching	O
signals	O
,	O
which	O
would	O
provide	O
explicit	O
signals	O
for	O
classification	Task
(	O
e.g.	O
,	O
missile	O
strongly	O
indicates	O
the	O
topic	O
of	O
military	O
)	O
.	O
To	O
address	O
the	O
aforementioned	O
problems	O
,	O
we	O
introduce	O
the	O
interaction	Method
mechanism	Method
,	O
which	O
is	O
capable	O
of	O
incorporating	O
the	O
word	O
-	O
level	O
matching	O
signals	O
for	O
text	Task
classification	Task
.	O
The	O
key	O
idea	O
behind	O
the	O
interaction	Method
mechanism	Method
is	O
to	O
explicitly	O
calculate	O
the	O
matching	O
scores	O
between	O
the	O
words	O
and	O
classes	O
.	O
From	O
the	O
word	Method
-	Method
level	Method
representation	Method
,	O
it	O
computes	O
an	O
interaction	O
matrix	O
,	O
in	O
which	O
each	O
entry	O
is	O
the	O
matching	O
score	O
between	O
a	O
word	O
and	O
a	O
class	O
(	O
dot	O
-	O
product	O
between	O
their	O
representations	O
)	O
,	O
illustrating	O
the	O
word	O
-	O
level	O
matching	O
signals	O
.	O
By	O
taking	O
the	O
interaction	O
matrix	O
as	O
a	O
text	Method
representation	Method
,	O
the	O
later	O
classification	Method
layer	Method
could	O
incorporate	O
fine	O
-	O
grained	O
word	O
level	O
signals	O
for	O
the	O
finer	O
classification	Task
rather	O
than	O
simply	O
making	O
the	O
text	Method
-	Method
level	Method
matching	Method
.	O
Based	O
upon	O
the	O
interaction	Method
mechanism	Method
,	O
we	O
devise	O
an	O
EXplicit	Method
interAction	Method
Model	Method
(	O
dubbed	O
as	O
EXAM	Method
)	O
.	O
Specifically	O
,	O
the	O
proposed	O
framework	O
consists	O
of	O
three	O
main	O
components	O
:	O
word	Method
-	Method
level	Method
encoder	Method
,	O
interaction	Method
layer	Method
,	O
and	O
aggregation	Method
layer	Method
.	O
The	O
word	Method
-	Method
level	Method
encoder	Method
projects	O
the	O
textual	O
contents	O
into	O
the	O
word	Method
-	Method
level	Method
representations	Method
.	O
Hereafter	O
,	O
the	O
interaction	Method
layer	Method
calculates	O
the	O
matching	O
scores	O
between	O
the	O
words	O
and	O
classes	O
(	O
i.e.	O
,	O
constructs	O
the	O
interaction	O
matrix	O
)	O
.	O
Then	O
,	O
the	O
last	O
layer	O
aggregates	O
those	O
matching	O
scores	O
into	O
predictions	O
over	O
each	O
class	O
,	O
respectively	O
.	O
We	O
justify	O
our	O
proposed	O
EXAM	Method
model	O
over	O
both	O
the	O
multi	Task
-	Task
label	Task
and	O
multi	Task
-	Task
class	Task
text	O
classifications	O
.	O
Extensive	O
experiments	O
on	O
several	O
benchmarks	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
,	O
surpassing	O
the	O
corresponding	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
remarkably	O
.	O
In	O
summary	O
,	O
the	O
contributions	O
of	O
this	O
work	O
are	O
threefold	O
:	O
We	O
present	O
a	O
novel	O
framework	O
,	O
EXAM	Method
,	O
which	O
leverages	O
the	O
interaction	Method
mechanism	Method
to	O
explicitly	O
compute	O
the	O
word	O
-	O
level	O
interaction	O
signals	O
for	O
the	O
text	Task
classification	Task
.	O
We	O
justify	O
the	O
proposed	O
EXAM	Method
model	O
over	O
both	O
multi	Task
-	Task
label	Task
and	O
multi	Task
-	Task
class	Task
text	O
classifications	O
.	O
Extensive	O
experimental	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
.	O
We	O
release	O
the	O
implementation	O
of	O
our	O
method	O
(	O
including	O
some	O
baselines	O
)	O
and	O
the	O
involved	O
parameter	O
settings	O
to	O
facilitate	O
later	O
researchers	O
.	O
section	O
:	O
Preliminaries	O
In	O
this	O
section	O
,	O
we	O
introduce	O
two	O
widely	O
-	O
used	O
word	Method
-	Method
level	Method
encoders	Method
:	O
Gated	Method
Recurrent	Method
Units	Method
and	O
Region	Method
Embedding	Method
.	O
These	O
encoders	O
project	O
a	O
piece	O
of	O
input	O
text	O
into	O
a	O
word	Method
-	Method
level	Method
representation	Method
,	O
serving	O
as	O
the	O
building	O
blocks	O
of	O
the	O
proposed	O
method	O
.	O
For	O
the	O
notations	O
in	O
this	O
paper	O
,	O
we	O
use	O
bold	O
capital	O
letters	O
(	O
e.g.	O
,	O
X	O
)	O
and	O
bold	O
lowercase	O
letters	O
(	O
e.g.	O
,	O
x	O
)	O
to	O
denote	O
matrices	O
and	O
vectors	O
,	O
respectively	O
.	O
We	O
employ	O
non	O
-	O
bold	O
letters	O
(	O
e.g.	O
,	O
x	O
)	O
to	O
represent	O
scalars	O
,	O
and	O
Greek	O
letters	O
(	O
e.g.	O
,	O
)	O
as	O
parameters	O
.	O
is	O
used	O
to	O
refer	O
the	O
i	O
-	O
th	O
row	O
of	O
the	O
matrix	O
,	O
to	O
represent	O
the	O
j	O
-	O
th	O
column	O
vector	O
and	O
to	O
denote	O
the	O
element	O
in	O
the	O
i	O
-	O
th	O
row	O
and	O
j	O
-	O
th	O
column	O
.	O
subsection	O
:	O
Gated	Method
Recurrent	Method
Units	Method
Owing	O
to	O
the	O
ability	O
of	O
capturing	O
the	O
sequential	O
dependencies	O
and	O
being	O
easily	O
optimized	O
(	O
i.e.	O
,	O
avoid	O
the	O
gradient	Task
vanishing	Task
and	Task
explosion	Task
problems	Task
)	O
,	O
Gated	Method
Recurrent	Method
Units	Method
(	O
GRU	Method
)	O
becomes	O
a	O
widely	O
used	O
word	Method
-	Method
level	Method
encoder	Method
.	O
Typically	O
,	O
a	O
GRU	Method
generates	O
word	Method
-	Method
level	Method
representations	Method
in	O
two	O
phases	O
:	O
1	O
)	O
mapping	O
each	O
word	O
in	O
the	O
text	O
into	O
an	O
embedding	O
(	O
a	O
real	O
-	O
valued	O
vector	O
)	O
,	O
and	O
2	O
)	O
projecting	O
the	O
sequence	Method
of	Method
word	Method
embeddings	Method
into	O
a	O
sequence	O
of	O
hidden	Method
representations	Method
,	O
which	O
encodes	O
the	O
sequential	O
dependencies	O
.	O
Word	Method
embedding	Method
.	O
Word	Method
embedding	Method
is	O
a	O
general	O
method	O
to	O
map	O
a	O
word	O
from	O
one	O
hot	O
vector	O
to	O
a	O
low	O
dimensional	O
and	O
real	O
-	O
valued	O
vector	O
.	O
With	O
enough	O
data	O
,	O
word	Method
embedding	Method
can	O
capture	O
high	O
-	O
level	O
representations	O
of	O
words	O
.	O
Hidden	Method
representation	Method
.	O
Given	O
an	O
embedding	O
feature	O
sequence	O
,	O
GRU	Method
will	O
compute	O
a	O
vector	O
at	O
the	O
i	O
-	O
th	O
time	O
-	O
step	O
for	O
each	O
,	O
and	O
is	O
defined	O
as	O
:	O
where	O
and	O
are	O
trainable	O
parameters	O
in	O
the	O
GRU	Method
,	O
and	O
and	O
are	O
sigmoid	O
and	O
tanh	O
activation	O
functions	O
,	O
respectively	O
.	O
The	O
sequence	O
of	O
hidden	Method
representations	Method
is	O
denoted	O
as	O
the	O
word	Method
-	Method
level	Method
representation	Method
of	O
the	O
input	O
text	O
.	O
subsection	O
:	O
Region	Method
Embedding	Method
Although	O
word	Method
embedding	Method
is	O
a	O
good	O
representation	O
for	O
the	O
word	O
,	O
it	O
can	O
only	O
compute	O
the	O
feature	O
vector	O
for	O
the	O
single	O
word	O
.	O
Qiao	O
et	O
al	O
.	O
regionemb	Method
proposed	O
region	Method
embedding	Method
to	O
learn	O
and	O
utilize	O
task	Method
-	Method
specific	Method
distributed	Method
representations	Method
of	Method
N	Method
-	Method
grams	Method
.	O
In	O
the	O
region	Method
embedding	Method
layer	Method
,	O
the	O
representation	O
of	O
a	O
word	O
has	O
two	O
parts	O
,	O
the	O
embedding	O
of	O
the	O
word	O
itself	O
and	O
a	O
weighting	O
matrix	O
to	O
interact	O
with	O
the	O
local	O
context	O
.	O
For	O
the	O
word	O
,	O
the	O
first	O
part	O
is	O
learned	O
by	O
an	O
embedding	Method
matrix	Method
and	O
the	O
second	O
part	O
is	O
looked	O
up	O
in	O
the	O
tensor	O
by	O
’s	O
index	O
in	O
the	O
vocabulary	O
,	O
where	O
is	O
the	O
size	O
of	O
the	O
vocabulary	O
,	O
the	O
region	O
size	O
and	O
the	O
embedding	O
size	O
.	O
And	O
then	O
,	O
each	O
column	O
in	O
is	O
used	O
to	O
interact	O
with	O
the	O
context	O
word	O
in	O
the	O
corresponding	O
relative	O
position	O
of	O
to	O
get	O
the	O
context	O
-	O
aware	O
for	O
each	O
word	O
in	O
the	O
region	O
.	O
Formally	O
it	O
is	O
computed	O
by	O
the	O
following	O
function	O
:	O
where	O
denotes	O
element	Method
-	Method
wise	Method
multiply	Method
.	O
And	O
the	O
final	O
representation	O
of	O
the	O
middle	O
word	O
is	O
computed	O
as	O
follows	O
:	O
section	O
:	O
Model	O
subsection	O
:	O
Problem	O
Formulation	O
Multi	Task
-	Task
Class	Task
Classification	Task
.	O
In	O
this	O
task	O
,	O
we	O
should	O
categorize	O
each	O
text	O
instance	O
to	O
precisely	O
one	O
of	O
classes	O
.	O
Suppose	O
that	O
we	O
have	O
a	O
data	O
set	O
=	O
,	O
where	O
denotes	O
the	O
text	O
and	O
the	O
one	O
-	O
hot	O
vector	O
represents	O
the	O
label	O
for	O
,	O
our	O
goal	O
is	O
to	O
learn	O
a	O
neural	Method
network	Method
to	O
classify	O
the	O
text	O
.	O
Multi	Task
-	Task
Label	Task
Classification	Task
.	O
In	O
this	O
task	O
,	O
each	O
text	O
instance	O
belongs	O
to	O
a	O
set	O
of	O
target	O
labels	O
.	O
Formally	O
,	O
suppose	O
that	O
we	O
have	O
a	O
dataset	O
=	O
,	O
where	O
denotes	O
the	O
text	O
and	O
the	O
multi	O
-	O
hot	O
vector	O
represents	O
the	O
label	O
for	O
the	O
text	O
.	O
Our	O
goal	O
is	O
to	O
learn	O
a	O
neural	Method
network	Method
to	O
classify	O
the	O
text	O
.	O
subsection	O
:	O
Model	O
Overview	O
Motivated	O
by	O
the	O
limitation	O
of	O
encoding	Method
-	Method
based	Method
models	Method
for	O
text	Task
classification	Task
,	O
which	O
is	O
lacking	O
the	O
fine	Task
-	Task
grained	Task
classification	Task
clue	Task
,	O
we	O
propose	O
a	O
novel	O
framework	O
,	O
named	O
EXplicit	Method
interAction	Method
Model	Method
(	O
EXAM	Method
)	O
,	O
leveraging	O
the	O
interaction	Method
mechanism	Method
to	O
incorporate	O
word	O
-	O
level	O
matching	O
signals	O
.	O
As	O
can	O
be	O
seen	O
from	O
Figure	O
[	O
reference	O
]	O
,	O
EXAM	Method
mainly	O
contains	O
three	O
components	O
:	O
A	O
word	Method
-	Method
level	Method
encoder	Method
to	O
project	O
the	O
input	O
text	O
into	O
a	O
word	Method
-	Method
level	Method
representation	Method
.	O
An	O
interaction	Method
layer	Method
to	O
compute	O
the	O
interaction	O
signals	O
between	O
the	O
words	O
and	O
classes	O
.	O
An	O
aggregation	Method
layer	Method
to	O
aggregate	O
the	O
interaction	O
signals	O
for	O
each	O
class	O
and	O
make	O
the	O
final	O
predictions	O
.	O
Considering	O
that	O
word	Method
-	Method
level	Method
encoders	Method
are	O
well	O
investigated	O
in	O
previous	O
studies	O
(	O
as	O
mentioned	O
in	O
the	O
Section	O
2	O
)	O
,	O
and	O
the	O
target	O
of	O
this	O
work	O
is	O
to	O
learn	O
the	O
fine	Task
-	Task
grained	Task
classification	Task
signals	Task
,	O
we	O
only	O
elaborate	O
the	O
interaction	Method
layer	Method
and	O
aggregation	Method
layer	Method
in	O
the	O
following	O
subsections	O
.	O
subsection	O
:	O
Interaction	Method
Layer	Method
Interaction	Method
mechanism	Method
is	O
widely	O
used	O
in	O
tasks	O
of	O
matching	Task
source	Task
and	Task
target	Task
textual	Task
contents	Task
,	O
such	O
as	O
natural	Task
language	Task
inference	Task
and	O
retrieve	Task
-	Task
based	Task
chatbot	Task
.	O
The	O
key	O
idea	O
of	O
interaction	Method
mechanism	Method
is	O
to	O
use	O
the	O
interaction	O
features	O
between	O
the	O
small	O
units	O
(	O
e.g.	O
,	O
words	O
in	O
the	O
textual	O
contents	O
)	O
to	O
infer	O
fine	O
-	O
grained	O
clues	O
whether	O
two	O
contents	O
are	O
matching	O
.	O
Inspired	O
by	O
the	O
success	O
of	O
methods	O
equipped	O
with	O
interaction	Method
mechanism	Method
over	O
encode	Method
-	Method
based	Method
methods	Method
in	O
matching	O
the	O
textual	O
contents	O
,	O
we	O
introduce	O
the	O
interaction	Method
mechanism	Method
into	O
the	O
task	O
of	O
matching	Task
textual	Task
contents	Task
with	O
their	O
classes	O
(	O
i.e.	O
,	O
text	Task
classification	Task
)	O
.	O
Specifically	O
,	O
we	O
devise	O
an	O
interaction	Method
layer	Method
which	O
aims	O
to	O
compute	O
the	O
matching	Metric
score	Metric
between	O
the	O
word	O
and	O
class	O
.	O
Different	O
from	O
conventional	O
interaction	Method
layer	Method
,	O
where	O
the	O
word	O
-	O
level	O
representations	O
of	O
both	O
source	O
and	O
target	O
are	O
extracted	O
with	O
encoders	Method
like	O
GRU	Method
,	O
here	O
we	O
first	O
project	O
classes	O
into	O
real	Method
-	Method
valued	Method
latent	Method
representations	Method
.	O
In	O
other	O
words	O
,	O
we	O
employ	O
a	O
trainable	Method
representation	Method
matrix	Method
to	O
encode	O
classes	O
(	O
each	O
row	O
represents	O
a	O
class	O
)	O
,	O
where	O
denotes	O
the	O
amount	O
of	O
classes	O
and	O
is	O
the	O
embedding	O
size	O
equals	O
to	O
that	O
of	O
words	O
.	O
We	O
then	O
adopt	O
dot	Method
product	Method
as	O
the	O
interaction	O
function	O
to	O
estimate	O
the	O
matching	Metric
score	Metric
between	O
the	O
target	O
word	O
and	O
class	O
,	O
of	O
which	O
the	O
formulation	O
is	O
,	O
where	O
denotes	O
word	Method
-	Method
level	Method
representation	Method
of	O
the	O
text	O
,	O
extracted	O
by	O
the	O
encoder	Method
with	O
denoting	O
the	O
length	O
of	O
the	O
text	O
.	O
In	O
this	O
way	O
,	O
we	O
can	O
compute	O
the	O
interaction	O
matrix	O
by	O
following	O
:	O
Note	O
that	O
we	O
reject	O
more	O
complex	O
interaction	O
functions	O
like	O
element	O
-	O
wise	O
multiply	O
and	O
cosine	O
similarity	O
for	O
the	O
consideration	O
of	O
efficiency	Metric
.	O
subsection	O
:	O
Aggregation	Method
Layer	Method
This	O
layer	O
is	O
devised	O
to	O
aggregate	O
the	O
interaction	O
features	O
for	O
each	O
class	O
into	O
a	O
logits	O
,	O
which	O
denotes	O
the	O
matching	O
score	O
between	O
class	O
and	O
the	O
input	O
text	O
.	O
The	O
aggregation	Method
layer	Method
can	O
be	O
implemented	O
in	O
different	O
ways	O
such	O
as	O
CNN	Method
and	Method
LSTM	Method
.	O
However	O
,	O
to	O
keep	O
the	O
simplicity	O
and	O
efficiency	O
of	O
EXAM	Method
,	O
here	O
we	O
only	O
use	O
a	O
MLP	Method
with	O
two	O
FC	Method
layers	O
,	O
where	O
ReLU	Method
is	O
employed	O
as	O
the	O
activation	O
function	O
of	O
the	O
first	Method
layer	Method
.	O
Formally	O
,	O
the	O
MLP	Method
aggregates	O
the	O
interaction	O
features	O
for	O
class	O
,	O
and	O
compute	O
its	O
associated	O
logits	O
as	O
following	O
:	O
where	O
and	O
are	O
trainable	O
parameters	O
and	O
is	O
the	O
bias	O
in	O
the	O
first	O
layer	O
.	O
We	O
then	O
normalize	O
the	O
logits	O
into	O
probabilities	O
.	O
Note	O
that	O
we	O
follow	O
previous	O
work	O
and	O
employ	O
softmax	Method
and	O
sigmoid	Method
for	O
multi	Task
-	Task
class	Task
and	O
multi	Task
-	Task
label	Task
classifications	Task
,	O
respectively	O
.	O
subsection	O
:	O
Loss	Method
Function	Method
Similar	O
to	O
previous	O
studies	O
,	O
in	O
the	O
multi	Task
-	Task
class	Task
text	O
classification	O
,	O
we	O
use	O
cross	O
entorpy	O
loss	O
as	O
our	O
loss	Method
function	Method
:	O
Following	O
previous	O
researchers	O
,	O
we	O
choose	O
binary	Method
classification	Method
loss	Method
as	O
our	O
loss	Method
function	Method
for	O
the	O
multi	Task
-	Task
label	Task
one	O
:	O
section	O
:	O
Generalized	Method
Encoding	Method
-	Method
Based	Method
Model	Method
In	O
this	O
section	O
,	O
we	O
elaborate	O
how	O
the	O
encoding	Method
-	Method
based	Method
model	Method
can	O
be	O
interpreted	O
as	O
a	O
special	O
case	O
of	O
our	O
EXAM	Method
framework	O
.	O
As	O
FastText	Method
is	O
the	O
most	O
popular	O
model	O
for	O
text	Task
classification	Task
and	O
has	O
been	O
investigated	O
extensively	O
in	O
the	O
literature	O
,	O
being	O
able	O
to	O
recover	O
it	O
allows	O
EXAM	Method
to	O
mimic	O
a	O
large	O
family	O
of	O
text	Method
classification	Method
models	Method
.	O
FastText	Method
contains	O
three	O
layers	O
:	O
1	O
)	O
an	O
embedding	Method
layer	Method
to	O
get	O
the	O
word	Method
-	Method
level	Method
representation	Method
for	O
the	O
word	O
,	O
2	O
)	O
an	O
average	Method
pooling	Method
layer	Method
to	O
get	O
the	O
text	Method
-	Method
level	Method
representation	Method
,	O
and	O
3	O
)	O
a	O
FC	Method
layer	O
to	O
get	O
the	O
final	O
logits	O
,	O
where	O
denotes	O
the	O
embedding	O
size	O
and	O
means	O
the	O
number	O
of	O
classes	O
.	O
Note	O
that	O
we	O
omit	O
the	O
subscript	O
of	O
the	O
document	O
ID	O
for	O
conciseness	O
.	O
Formally	O
,	O
it	O
computes	O
the	O
logits	O
of	O
-	O
th	O
class	O
as	O
follows	O
:	O
where	O
and	O
are	O
the	O
trainable	O
parameters	O
in	O
the	O
last	O
FC	Method
layer	O
,	O
and	O
denotes	O
the	O
length	O
of	O
the	O
text	O
.	O
The	O
Eqn.	O
(	O
9	O
)	O
has	O
an	O
equivalent	O
form	O
as	O
following	O
:	O
It	O
is	O
worth	O
noting	O
that	O
is	O
exactly	O
the	O
interaction	O
feature	O
between	O
word	O
and	O
class	O
.	O
Therefore	O
,	O
the	O
FastText	Method
is	O
a	O
special	O
case	O
of	O
EXAM	Method
with	O
an	O
average	Method
pooling	Method
as	O
the	O
aggregation	Method
layer	Method
.	O
In	O
EXAM	Method
,	O
we	O
use	O
a	O
non	Method
-	Method
linear	Method
MLP	Method
to	O
be	O
the	O
aggregation	Method
layer	Method
,	O
and	O
it	O
will	O
generalize	O
FastText	Method
to	O
a	O
non	Task
-	Task
linear	Task
setting	Task
which	O
might	O
be	O
more	O
expressive	O
than	O
the	O
original	O
one	O
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Multi	Task
-	Task
Class	Task
Classification	Task
subsubsection	O
:	O
Datasets	O
We	O
used	O
publicly	O
available	O
benchmark	O
datasets	O
from	O
to	O
evaluate	O
EXAM	Method
.	O
There	O
are	O
in	O
total	O
6	O
text	O
classification	O
datasets	O
,	O
corresponding	O
to	O
sentiment	Task
analysis	Task
,	O
news	Task
classification	Task
,	O
question	Task
-	Task
answer	Task
and	O
ontology	Task
extraction	Task
tasks	Task
,	O
respectively	O
.	O
Table	O
1	O
shows	O
the	O
descriptive	O
statistics	O
of	O
datasets	O
used	O
in	O
our	O
experiments	O
.	O
Stanford	Task
tokenizer	Task
is	O
used	O
to	O
tokenize	O
the	O
text	O
and	O
all	O
words	O
are	O
converted	O
to	O
lower	O
case	O
.	O
We	O
used	O
padding	O
to	O
handle	O
the	O
various	O
lengths	O
of	O
the	O
text	O
,	O
and	O
different	O
maximum	O
lengths	O
are	O
set	O
for	O
each	O
dataset	O
,	O
respectively	O
.	O
If	O
the	O
length	O
of	O
the	O
text	O
is	O
less	O
than	O
the	O
corresponding	O
predefined	O
value	O
,	O
we	O
padded	O
it	O
with	O
zero	O
;	O
otherwise	O
we	O
truncated	O
the	O
original	O
text	O
.	O
To	O
guarantee	O
a	O
fair	O
comparison	O
,	O
the	O
same	O
evaluation	Metric
protocol	Metric
of	O
is	O
employed	O
.	O
We	O
split	O
10	O
%	O
samples	O
from	O
the	O
training	O
set	O
as	O
the	O
validation	O
set	O
to	O
perform	O
early	O
stop	O
for	O
our	O
models	O
.	O
subsubsection	O
:	O
Hyperparameters	O
For	O
the	O
multi	Task
-	Task
class	Task
task	O
,	O
we	O
chose	O
region	Method
embedding	Method
as	O
the	O
Encoder	O
in	O
EXAM	Method
.	O
The	O
region	O
size	O
is	O
7	O
and	O
embedding	O
size	O
is	O
128	O
.	O
We	O
used	O
adam	Method
as	O
the	O
optimizer	Method
with	O
the	O
initial	O
learning	Metric
rate	Metric
0.0001	O
and	O
the	O
batch	O
size	O
is	O
set	O
to	O
16	O
.	O
As	O
for	O
the	O
aggregation	Method
MLP	Method
,	O
we	O
set	O
the	O
size	O
of	O
the	O
hidden	O
layer	O
as	O
2	O
times	O
interaction	O
feature	O
length	O
.	O
Our	O
models	O
are	O
implemented	O
and	O
trained	O
by	O
MXNet	Method
with	O
a	O
single	O
NVIDIA	Method
TITAN	Method
Xp	Method
.	O
subsubsection	O
:	O
Baselines	O
To	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
EXAM	Method
,	O
we	O
compared	O
it	O
with	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O
The	O
baselines	O
are	O
mainly	O
in	O
three	O
variants	O
:	O
1	O
)	O
models	O
based	O
on	O
feature	Method
engineering	Method
;	O
2	O
)	O
Char	Method
-	Method
based	Method
deep	Method
models	Method
,	O
and	O
3	O
)	O
Word	Method
-	Method
based	Method
deep	Method
models	Method
.	O
The	O
first	O
category	O
uses	O
the	O
feature	O
from	O
the	O
text	O
to	O
conduct	O
the	O
classification	Task
,	O
and	O
we	O
reported	O
the	O
results	O
from	O
BoW	Method
,	O
N	Method
-	Method
grams	Method
and	O
N	Method
-	Method
grams	Method
TFIDF	Method
as	O
baselines	O
.	O
The	O
second	O
one	O
means	O
the	O
input	O
of	O
the	O
model	O
is	O
the	O
character	O
in	O
the	O
original	O
text	O
,	O
and	O
we	O
chose	O
the	O
Char	Method
-	Method
CNN	Method
,	O
Char	Method
-	Method
CRNN	Method
and	O
VDCNN	Method
as	O
baselines	O
.	O
As	O
for	O
the	O
word	Method
-	Method
based	Method
deep	Method
models	Method
,	O
the	O
text	O
is	O
pre	O
-	O
segmented	O
into	O
words	O
as	O
the	O
input	O
,	O
and	O
we	O
applied	O
Small	Method
word	Method
CNN	Method
,	O
Large	Method
word	Method
CNN	Method
,	O
LSTM	Method
,	O
FastText	Method
and	O
W.C	Method
RegionEmb	Method
as	O
the	O
baselines	O
.	O
It	O
is	O
worth	O
emphasizing	O
that	O
all	O
the	O
baselines	O
and	O
our	O
EXAM	Method
do	O
not	O
use	O
pre	O
-	O
trained	O
word	Method
embedding	Method
over	O
other	O
corpus	O
like	O
glove	Method
.	O
subsubsection	O
:	O
Overall	O
Performance	O
We	O
compared	O
our	O
EXAM	Method
to	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
with	O
respect	O
to	O
accuracy	Metric
.	O
All	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Four	O
points	O
are	O
observed	O
as	O
following	O
:	O
Models	O
based	O
on	O
feature	Method
engineering	Method
get	O
the	O
worst	O
results	O
on	O
all	O
the	O
five	O
datasets	O
compared	O
to	O
the	O
other	O
methods	O
.	O
The	O
main	O
reason	O
is	O
that	O
the	O
feature	Method
engineering	Method
can	O
not	O
take	O
full	O
advantage	O
of	O
the	O
supervision	O
from	O
the	O
training	O
set	O
and	O
it	O
also	O
suffers	O
from	O
the	O
data	O
sparsity	O
.	O
Char	Method
-	Method
based	Method
models	Method
get	O
the	O
highest	O
overall	O
scores	O
on	O
the	O
two	O
Amazon	Material
datasets	Material
.	O
There	O
are	O
possibly	O
two	O
reasons	O
,	O
1	O
)	O
compared	O
to	O
the	O
word	Method
-	Method
based	Method
models	Method
,	O
char	Method
-	Method
based	Method
models	Method
enrich	O
the	O
supervision	O
from	O
characters	O
and	O
the	O
characters	O
are	O
combined	O
to	O
form	O
N	O
-	O
grams	O
,	O
stems	O
,	O
words	O
and	O
phrase	O
which	O
are	O
helpful	O
in	O
the	O
sentimental	Task
classification	Task
.	O
2	O
)	O
The	O
two	O
Amazon	Material
datasets	Material
contain	O
millions	O
of	O
training	O
samples	O
,	O
perfectly	O
fitting	O
the	O
deep	Method
residual	Method
architecture	Method
for	O
the	O
VDCNN	Method
.	O
For	O
the	O
three	O
char	Method
-	Method
based	Method
baselines	Method
,	O
VDCNN	Method
gets	O
the	O
best	O
performance	O
on	O
almost	O
all	O
the	O
datasets	O
because	O
it	O
has	O
29	O
convolutional	Method
layers	Method
allowing	O
the	O
model	O
to	O
learn	O
more	O
combinations	O
of	O
characters	O
.	O
Word	Method
-	Method
based	Method
baselines	Method
exceed	O
the	O
other	O
variants	O
on	O
three	O
datasets	O
and	O
lose	O
on	O
the	O
two	O
Amazon	Material
datasets	Material
.	O
The	O
main	O
reason	O
is	O
that	O
the	O
three	O
tasks	O
like	O
news	Task
classification	Task
conduct	O
categorization	Task
mainly	O
via	O
key	O
words	O
,	O
and	O
the	O
word	Method
-	Method
based	Method
models	Method
are	O
able	O
to	O
directly	O
use	O
the	O
word	Method
embedding	Method
without	O
combining	O
the	O
characters	O
.	O
For	O
the	O
five	O
baselines	O
,	O
W.C	Method
RegionEmb	Method
performs	O
the	O
best	O
,	O
because	O
it	O
learns	O
the	O
region	Method
embedding	Method
to	O
utilize	O
the	O
N	O
-	O
grams	O
feature	O
from	O
the	O
text	O
.	O
It	O
is	O
clear	O
to	O
see	O
that	O
EXAM	Method
achieves	O
the	O
best	O
performance	O
over	O
the	O
three	O
datasets	O
:	O
AG	O
,	O
Yah	O
.	O
A.	O
and	O
DBP	O
.	O
For	O
the	O
Yah	O
.	O
A.	O
,	O
EXAM	Method
improves	O
the	O
best	O
performance	O
by	O
1.1	O
%	O
.	O
Additionally	O
,	O
as	O
a	O
word	Method
-	Method
based	Method
model	Method
,	O
EXAM	Method
beats	O
all	O
the	O
word	Method
-	Method
based	Method
baselines	Method
on	O
the	O
other	O
two	O
Amazon	Material
datasets	Material
with	O
a	O
performance	O
gain	O
of	O
1.0	O
%	O
on	O
the	O
Amazon	Material
Full	Material
,	O
because	O
our	O
EXAM	Method
considers	O
more	O
fine	O
-	O
grained	O
interaction	O
features	O
between	O
classes	O
and	O
words	O
,	O
which	O
is	O
quite	O
helpful	O
in	O
this	O
task	O
.	O
subsubsection	O
:	O
Component	Method
-	Method
wise	Method
Evaluation	Method
We	O
studied	O
the	O
variant	O
of	O
our	O
model	O
to	O
further	O
investigate	O
the	O
effectiveness	O
of	O
the	O
interaction	O
layer	O
and	O
aggregation	Method
layer	Method
.	O
We	O
built	O
a	O
model	O
called	O
EXAM	Method
to	O
preserve	O
only	O
the	O
Encoder	Method
component	Method
with	O
a	O
max	Method
pooling	Method
layer	Method
and	O
FC	Method
layer	O
to	O
derive	O
the	O
final	O
probabilities	O
.	O
EXAM	Method
does	O
not	O
consider	O
the	O
interaction	O
features	O
between	O
the	O
classes	O
and	O
words	O
,	O
so	O
it	O
will	O
automatically	O
be	O
degenerated	O
into	O
the	O
Encoding	Method
-	Method
Based	Method
model	Method
.	O
We	O
reported	O
the	O
results	O
of	O
the	O
two	O
models	O
on	O
all	O
the	O
datasets	O
at	O
Table	O
[	O
reference	O
]	O
,	O
and	O
it	O
is	O
clear	O
to	O
see	O
that	O
EXAM	Method
is	O
not	O
a	O
patch	O
on	O
the	O
original	O
EXAM	Method
,	O
verifying	O
the	O
effectiveness	O
of	O
interaction	Method
mechanism	Method
.	O
We	O
also	O
drew	O
the	O
convergence	O
lines	O
for	O
EXAM	Method
and	O
the	O
EXAM	Method
for	O
the	O
datasets	O
.	O
From	O
the	O
Figure	O
[	O
reference	O
]	O
,	O
where	O
the	O
red	O
lines	O
represent	O
EXAM	Method
and	O
the	O
blue	O
is	O
EXAM	Method
,	O
we	O
observed	O
that	O
EXAM	Method
converges	O
faster	O
than	O
EXAM	Method
with	O
respect	O
to	O
all	O
the	O
datasets	O
.	O
Therefore	O
,	O
the	O
interaction	O
brings	O
not	O
only	O
performance	O
improvement	O
but	O
also	O
faster	O
convergence	Metric
.	O
The	O
possible	O
reason	O
is	O
that	O
a	O
non	Method
-	Method
linear	Method
aggregation	Method
layer	Method
introduces	O
more	O
parameters	O
to	O
fit	O
the	O
interaction	O
features	O
compared	O
to	O
the	O
average	Method
pooling	Method
layer	Method
as	O
mentioned	O
in	O
Section	O
4	O
.	O
subsection	O
:	O
Multi	Task
-	Task
Label	Task
Classification	Task
subsubsection	O
:	O
Datasets	O
We	O
conducted	O
experiments	O
on	O
two	O
different	O
multi	Task
-	Task
label	Task
text	O
classification	O
datasets	O
,	O
named	O
KanShan	Material
-	Material
Cup	Material
dataset	Material
(	O
a	O
benchmark	O
)	O
and	O
Zhihu	Material
dataset	Material
,	O
respectively	O
.	O
KanShan	Material
-	Material
Cup	Material
dataset	Material
.	O
This	O
dataset	O
is	O
released	O
by	O
a	O
competition	O
of	O
tagging	O
topics	O
for	O
questions	O
(	O
multi	Task
-	Task
label	Task
classification	O
)	O
posted	O
in	O
the	O
largest	O
Chinese	Material
community	Material
question	Material
answering	Material
platform	Material
,	O
Zhihu	O
.	O
The	O
dataset	O
contains	O
3	O
,	O
000	O
,	O
000	O
questions	O
and	O
1	O
,	O
999	O
topics	O
(	O
classes	O
)	O
,	O
where	O
one	O
question	O
may	O
belong	O
to	O
one	O
to	O
five	O
topics	O
.	O
For	O
questions	O
with	O
more	O
than	O
30	O
words	O
,	O
we	O
kept	O
the	O
last	O
30	O
words	O
,	O
otherwise	O
,	O
we	O
padded	O
zeros	O
.	O
We	O
separated	O
the	O
dataset	O
into	O
training	O
,	O
validation	Task
,	O
and	O
testing	O
with	O
2	O
,	O
800	O
,	O
000	O
,	O
20	O
,	O
000	O
,	O
and	O
180	O
,	O
000	O
questions	O
,	O
respectively	O
.	O
Zhihu	Material
dataset	Material
.	O
Considering	O
the	O
user	O
privacy	O
and	O
data	Task
security	Task
,	O
KanShan	Material
-	Material
Cup	Material
does	O
not	O
provide	O
the	O
original	O
texts	O
of	O
the	O
questions	O
and	O
topics	O
,	O
but	O
uses	O
numbered	O
codes	O
and	O
numbered	O
segmented	O
words	O
to	O
represent	O
text	O
messages	O
.	O
Therefore	O
,	O
it	O
is	O
inconvenient	O
for	O
researchers	O
to	O
perform	O
analyses	O
like	O
visualization	Task
and	O
case	Task
study	Task
.	O
To	O
solve	O
this	O
problem	O
,	O
we	O
constructed	O
a	O
dataset	O
named	O
Zhihu	Material
dataset	Material
.	O
We	O
chose	O
the	O
top	O
1	O
,	O
999	O
frequent	O
topics	O
from	O
Zhihu	O
and	O
crawled	O
all	O
the	O
questions	O
relevant	O
to	O
these	O
topics	O
.	O
Finally	O
,	O
we	O
acquired	O
3	O
,	O
300	O
,	O
000	O
questions	O
,	O
with	O
less	O
than	O
5	O
topics	O
for	O
each	O
question	O
.	O
We	O
adopted	O
3	O
,	O
000	O
,	O
000	O
samples	O
as	O
the	O
training	O
set	O
,	O
30	O
,	O
000	O
samples	O
as	O
validation	O
and	O
300	O
,	O
000	O
samples	O
as	O
testing	O
.	O
subsubsection	O
:	O
Baselines	O
We	O
applied	O
the	O
following	O
models	O
as	O
baselines	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
EXAM	Method
.	O
Char	Method
-	Method
based	Method
Model	Method
.	O
We	O
chose	O
Char	Method
-	Method
CNN	Method
and	O
Char	Method
-	Method
RNN	Method
as	O
the	O
baselines	O
to	O
represent	O
this	O
kind	O
of	O
methods	O
.	O
Word	Method
-	Method
based	Method
Model	Method
.	O
For	O
the	O
word	Method
-	Method
based	Method
models	Method
,	O
we	O
reported	O
the	O
results	O
from	O
TextCNN	Material
,	O
TextRNN	Material
and	O
FastText	Material
.	O
The	O
three	O
models	O
got	O
the	O
best	O
performance	O
in	O
the	O
KanShan	Task
-	Task
Cup	Task
competition	Task
,	O
so	O
we	O
applied	O
them	O
as	O
the	O
word	Method
-	Method
based	Method
baselines	Method
.	O
subsubsection	O
:	O
Hyperparameters	O
We	O
implemented	O
the	O
baseline	O
models	O
and	O
EXAM	Method
by	O
MXNet	Method
.	O
We	O
used	O
the	O
matrix	O
trained	O
by	O
word2vec	Method
to	O
initialize	O
the	O
embedding	Method
layer	Method
,	O
and	O
the	O
embedding	Metric
size	Metric
is	O
256	O
.	O
We	O
adopted	O
GRU	Method
as	O
the	O
Encoder	Method
,	O
and	O
each	O
GRU	Method
Cell	Method
has	O
1	O
,	O
024	O
hidden	O
states	O
.	O
The	O
accumulated	O
MLP	Method
has	O
60	O
hidden	O
units	O
.	O
We	O
applied	O
Adam	Method
to	O
optimize	O
models	O
on	O
one	O
NVIDIA	Method
TITAN	Method
Xp	Method
with	O
the	O
batch	O
size	O
of	O
1000	O
and	O
the	O
initial	O
learning	Metric
rate	Metric
is	O
0.001	O
.	O
The	O
validation	O
set	O
is	O
applied	O
for	O
early	Task
-	Task
stopping	Task
to	O
avoid	O
overfitting	O
.	O
All	O
hyperparameters	O
are	O
chosen	O
empirically	O
.	O
subsubsection	O
:	O
Metrics	O
We	O
used	O
the	O
following	O
metrics	O
to	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
and	O
baseline	O
models	O
.	O
Precision	Metric
:	O
Different	O
from	O
the	O
traditional	O
precision	Metric
metric	Metric
(	O
Precision@5	Metric
)	O
which	O
is	O
set	O
as	O
the	O
fraction	O
of	O
the	O
relevant	O
topic	O
tags	O
among	O
the	O
five	O
returned	O
tags	O
,	O
we	O
utilized	O
weighted	O
precision	O
to	O
encourage	O
the	O
relevant	O
topic	O
tags	O
to	O
be	O
ranked	O
higher	O
in	O
the	O
returned	O
list	O
.	O
Formally	O
,	O
the	O
Precision	Metric
is	O
computed	O
as	O
following	O
,	O
Recall@5	Metric
:	O
Recall	Metric
is	O
the	O
fraction	O
of	O
relevant	O
topic	O
tags	O
that	O
have	O
been	O
retrieved	O
over	O
the	O
total	O
amount	O
of	O
five	O
relevant	O
topic	O
tags	O
,	O
high	O
recall	O
means	O
that	O
the	O
model	O
returns	O
most	O
of	O
the	O
relevant	O
topic	O
tags	O
.	O
:	O
is	O
the	O
harmonic	Metric
average	Metric
of	O
the	O
precision	Metric
and	O
recall	Metric
,	O
we	O
computed	O
it	O
as	O
following	O
,	O
subsubsection	O
:	O
Performance	O
Comparison	O
Table	O
[	O
reference	O
]	O
gives	O
the	O
performance	O
of	O
our	O
model	O
and	O
baselines	O
over	O
two	O
different	O
datasets	O
with	O
respect	O
to	O
Precision	Metric
,	O
Recall@5	Metric
and	O
.	O
We	O
observed	O
the	O
following	O
from	O
the	O
Table	O
[	O
reference	O
]	O
:	O
Word	Method
-	Method
based	Method
models	Method
are	O
better	O
than	O
char	Method
-	Method
based	Method
models	Method
in	O
Kanshan	Material
-	Material
Cup	Material
dataset	Material
.	O
That	O
may	O
be	O
because	O
in	O
Chinese	O
the	O
words	O
can	O
offer	O
more	O
supervisions	O
than	O
characters	O
and	O
the	O
question	Task
tagging	Task
task	Task
needs	O
more	O
word	O
supervision	O
.	O
For	O
word	Method
-	Method
based	Method
baseline	Method
models	Method
,	O
all	O
the	O
baselines	O
have	O
similar	O
performance	O
which	O
corroborates	O
the	O
conclusion	O
in	O
FastText	Task
that	O
simple	Method
network	Method
is	O
on	O
par	O
with	O
deep	Method
learning	Method
classifiers	Method
in	O
text	Task
classification	Task
.	O
Our	O
models	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
over	O
two	O
different	O
datasets	O
though	O
we	O
only	O
slightly	O
modified	O
TextRNN	Material
to	O
build	O
EXAM	Method
.	O
Different	O
from	O
the	O
traditional	O
models	O
which	O
encode	O
the	O
whole	O
text	O
into	O
a	O
vector	O
,	O
in	O
EXAM	Method
,	O
the	O
representations	O
of	O
classes	O
firstly	O
interact	O
with	O
words	O
to	O
get	O
more	O
fine	O
-	O
grained	O
features	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
results	O
suggest	O
that	O
word	O
-	O
level	O
interaction	O
features	O
are	O
relatively	O
more	O
important	O
than	O
global	Method
text	Method
-	Method
level	Method
representations	Method
in	O
this	O
task	O
.	O
subsubsection	O
:	O
Interaction	Task
Visualization	Task
To	O
illustrate	O
the	O
effectiveness	O
of	O
explicit	Task
interaction	Task
,	O
we	O
visualized	O
an	O
interaction	O
feature	O
I	O
of	O
the	O
question	O
“	O
Second	O
-	O
hand	O
TIDDA	O
1.6	O
T	O
Mannual	O
gear	O
has	O
gotten	O
some	O
problems	O
,	O
please	O
everybody	O
help	O
me	O
to	O
solve	O
it	O
?	O
”	O
.	O
This	O
question	O
has	O
5	O
topics	O
:	O
Car	O
,	O
Second	Task
-	Task
hand	Task
Car	Task
,	O
Motor	Task
Dom	Task
,	O
Autocar	Task
Conversation	Task
and	O
Autocar	Task
Service	Task
.	O
EXAM	Method
only	O
misclassified	O
the	O
last	O
topic	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
observed	O
that	O
when	O
classifying	O
different	O
topics	O
,	O
the	O
interaction	O
features	O
are	O
different	O
.	O
The	O
topics	O
“	O
Car	O
”	O
and	O
“	O
Second	O
-	O
hand	O
Car	O
”	O
pay	O
much	O
attention	O
to	O
the	O
words	O
like	O
“	O
Second	O
-	O
hand	O
TIIDA	O
”	O
and	O
the	O
other	O
topic	O
like	O
“	O
Autocar	Material
Conversation	Material
”	O
focuses	O
more	O
on	O
“	O
got	O
some	O
problems	O
”	O
.	O
The	O
results	O
clearly	O
signify	O
that	O
the	O
interaction	O
feature	O
between	O
the	O
word	O
and	O
class	O
is	O
well	O
-	O
learned	O
and	O
highly	O
meaningful	O
.	O
section	O
:	O
Related	O
Work	O
subsubsection	O
:	O
Text	Task
Classification	Task
Existing	O
researches	O
on	O
text	Task
classification	Task
can	O
be	O
categorized	O
into	O
two	O
groups	O
:	O
feature	Method
-	Method
based	Method
and	O
deep	Method
neural	Method
models	Method
.	O
The	O
former	O
focuses	O
on	O
hand	O
-	O
craft	O
features	O
and	O
uses	O
machine	Method
learning	Method
algorithms	Method
as	O
the	O
classifier	Method
.	O
Bag	Method
-	Method
of	Method
-	Method
words	Method
is	O
a	O
very	O
efficient	O
way	O
to	O
conduct	O
the	O
feature	Task
engineering	Task
.	O
SVM	Method
and	O
Naive	Method
Bayes	Method
are	O
constantly	O
the	O
classifier	Method
.	O
The	O
latter	O
,	O
deep	Method
neural	Method
models	Method
,	O
taking	O
advantage	O
of	O
neural	Method
networks	Method
to	O
accomplish	O
the	O
model	Method
learning	Method
from	O
data	O
,	O
have	O
become	O
the	O
promising	O
solution	O
for	O
the	O
text	Task
classification	Task
.	O
For	O
instance	O
,	O
Iyyer	O
et	O
al	O
.	O
dan	Method
proposed	O
Deep	Method
Averaging	Method
Networks	Method
(	O
DAN	Method
)	O
and	O
Grave	O
et	O
al	O
.	O
fasttext	Method
proposed	O
the	O
FastText	Method
,	O
and	O
both	O
are	O
simple	O
but	O
efficient	O
.	O
To	O
get	O
the	O
temporal	O
features	O
between	O
the	O
words	O
in	O
the	O
text	O
,	O
some	O
models	O
like	O
TextCNN	Method
and	Method
Char	Method
-	Method
CNN	Method
exploit	O
the	O
convolutional	Method
neural	Method
network	Method
,	O
and	O
there	O
are	O
also	O
some	O
models	O
based	O
on	O
Recurrent	Method
Neural	Method
Network	Method
(	O
RNN	Method
)	O
.	O
Recently	O
,	O
Johnson	O
et	O
al	O
.	O
vdcnn	Method
investigated	O
the	O
residual	Method
architecture	Method
and	O
built	O
a	O
model	O
called	O
VD	Method
-	Method
CNN	Method
and	O
Qiao	O
et	O
al	O
.	O
regionemb	Method
proposed	O
a	O
new	O
method	O
of	O
region	Method
embedding	Method
for	O
the	O
text	Task
classification	Task
.	O
However	O
,	O
as	O
mentioned	O
in	O
the	O
Introduction	O
,	O
all	O
these	O
methods	O
are	O
text	Method
-	Method
level	Method
models	Method
while	O
EXAM	Method
conducts	O
the	O
matching	Task
at	O
the	O
word	O
level	O
.	O
subsubsection	O
:	O
Interaction	Method
Mechanism	Method
Interaction	Method
Mechanism	Method
is	O
widely	O
used	O
in	O
Natural	Task
Language	Task
Sentence	Task
Matching	Task
(	O
NLSM	Task
)	O
.	O
The	O
key	O
idea	O
of	O
interaction	Method
mechanism	Method
is	O
to	O
use	O
the	O
interaction	O
features	O
between	O
the	O
small	O
units	O
(	O
like	O
words	O
in	O
sentence	O
)	O
to	O
make	O
the	O
matching	Task
.	O
Wang	O
et	O
al	O
.	O
llstm	Method
proposed	O
a	O
“	O
matching	Method
-	Method
aggregation	Method
”	Method
framework	Method
to	O
perform	O
the	O
interaction	Task
in	O
Natural	Task
Language	Task
Inference	Task
.	O
Following	O
this	O
work	O
,	O
Parikh	O
et	O
al	O
.	O
DATTENTION	O
integrated	O
the	O
attention	Method
mechanism	Method
into	O
this	O
framework	O
,	O
called	O
Decomposable	Method
Attention	Method
Model	Method
.	O
Then	O
Wang	O
et	O
al	O
.	O
Wang2016ACM	O
discussed	O
different	O
interaction	O
functions	O
in	O
Text	Task
Matching	Task
.	O
Yu	O
et	O
al	O
.	O
TREE	Method
adopted	Method
tree	Method
-	Method
LSTM	Method
to	O
get	O
different	O
level	O
units	O
to	O
perform	O
the	O
interaction	O
.	O
Gong	O
et	O
al	O
.	O
diin	O
proposed	O
a	O
densely	Method
interactive	Method
inference	Method
network	Method
to	O
use	O
DenseNet	O
to	O
aggregate	O
dense	O
interaction	O
features	O
.	O
Our	O
work	O
is	O
different	O
from	O
them	O
since	O
they	O
mainly	O
apply	O
this	O
mechanism	O
in	O
text	Task
matching	Task
instead	O
of	O
the	O
classification	Task
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
novel	O
framework	O
named	O
EXAM	Method
which	O
employs	O
the	O
interaction	Method
mechanism	Method
to	O
explicitly	O
compute	O
the	O
word	O
-	O
level	O
interaction	O
signals	O
for	O
the	O
text	Task
classification	Task
.	O
We	O
apply	O
the	O
proposed	O
EXAM	Method
on	O
multi	Task
-	Task
class	Task
and	O
multi	Task
-	Task
label	Task
text	Task
classifications	Task
.	O
Experiments	O
over	O
several	O
benchmark	O
datasets	O
verify	O
the	O
effectiveness	O
of	O
our	O
proposed	O
mechanism	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
investigate	O
the	O
effect	O
of	O
different	O
interaction	O
functions	O
in	O
the	O
interaction	O
mechanism	O
.	O
Besides	O
,	O
we	O
are	O
interested	O
in	O
extend	O
EXAM	Method
by	O
introducing	O
more	O
complex	O
aggregation	Method
layers	Method
like	O
ResNet	Method
or	O
DenseNet	Method
.	O
section	O
:	O
Acknowledgments	O
This	O
work	O
is	O
supported	O
by	O
the	O
National	O
Basic	O
Research	O
Program	O
of	O
China	O
(	O
973	O
Program	O
)	O
,	O
No	O
.	O
:	O
2015CB352502	O
;	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
,	O
No	O
.	O
:	O
61772310	O
,	O
No.:61702300	O
,	O
and	O
No.:61702302	O
;	O
the	O
Project	O
of	O
Thousand	O
Youth	O
Talents	O
2016	O
;	O
and	O
the	O
Tencent	O
AI	O
Lab	O
Rhino	O
-	O
Bird	O
Joint	O
Research	O
Program	O
(	O
No	O
.	O
JR201805	O
)	O
;	O
Fundamental	O
Research	O
Funds	O
of	O
Shandong	O
University	O
(	O
No	O
.	O
2017HW001	O
)	O
.	O
bibliography	O
:	O
References	O
