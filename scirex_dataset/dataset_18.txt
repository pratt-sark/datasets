document	O
:	O
Deep	Method
Residual	Method
Networks	Method
with	O
Exponential	Method
Linear	Method
Unit	Method
Very	O
deep	Method
convolutional	Method
neural	Method
networks	Method
introduced	O
new	O
problems	O
like	O
vanishing	O
gradient	O
and	O
degradation	O
.	O
The	O
recent	O
successful	O
contributions	O
towards	O
solving	O
these	O
problems	O
are	O
Residual	Method
and	O
Highway	Method
Networks	Method
.	O
These	O
networks	O
introduce	O
skip	O
connections	O
that	O
allow	O
the	O
information	O
(	O
from	O
the	O
input	O
or	O
those	O
learned	O
in	O
earlier	O
layers	O
)	O
to	O
flow	O
more	O
into	O
the	O
deeper	O
layers	O
.	O
These	O
very	O
deep	Method
models	Method
have	O
lead	O
to	O
a	O
considerable	O
decrease	O
in	O
test	Metric
errors	Metric
,	O
on	O
benchmarks	O
like	O
ImageNet	O
and	O
COCO	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
use	O
of	O
exponential	Method
linear	Method
unit	Method
instead	O
of	O
the	O
combination	O
of	O
ReLU	Method
and	O
Batch	Method
Normalization	Method
in	O
Residual	Method
Networks	Method
.	O
We	O
show	O
that	O
this	O
not	O
only	O
speeds	O
up	O
learning	Task
in	O
Residual	Method
Networks	Method
but	O
also	O
improves	O
the	O
accuracy	Metric
as	O
the	O
depth	O
increases	O
.	O
It	O
improves	O
the	O
test	Metric
error	Metric
on	O
almost	O
all	O
data	O
sets	O
,	O
like	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
.	O
section	O
:	O
Introduction	O
The	O
Vision	Task
Community	Task
has	O
been	O
mesmerized	O
by	O
the	O
effectiveness	O
of	O
deep	Method
convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
that	O
have	O
led	O
to	O
a	O
breakthrough	O
in	O
computer	Task
vision	Task
-	Task
related	Task
problems	Task
.	O
Hence	O
,	O
there	O
has	O
been	O
a	O
notable	O
shift	O
towards	O
CNNs	Method
in	O
many	O
areas	O
of	O
computer	Task
vision	Task
.	O
Convolutional	Method
neural	Method
networks	Method
were	O
popularized	O
through	O
AlexNet	Method
in	O
2009	O
and	O
their	O
much	O
celebrated	O
victory	O
at	O
the	O
2012	O
ImageNet	O
competiton	O
.	O
After	O
that	O
,	O
there	O
have	O
been	O
several	O
attempts	O
at	O
building	O
deeper	Method
and	Method
deeper	Method
CNNs	Method
like	O
the	O
VGG	Method
network	Method
and	O
GoogLeNet	Method
in	O
2014	O
which	O
have	O
19	O
and	O
22	O
layers	O
respectively	O
.	O
But	O
,	O
very	O
deep	Method
models	Method
introduce	O
problems	O
like	O
vanishing	O
and	O
exploding	O
gradients	O
,	O
which	O
hamper	O
their	O
convergence	O
.	O
The	O
vanishing	Task
gradient	Task
problem	Task
is	O
trivial	O
in	O
very	O
deep	Task
networks	Task
.	O
During	O
the	O
backpropagation	O
phase	O
,	O
the	O
gradients	O
are	O
computed	O
by	O
the	O
chain	Method
rule	Method
.	O
Multiplication	O
of	O
small	O
numbers	O
in	O
the	O
chain	Method
rule	Method
leads	O
to	O
an	O
exponential	O
decrease	O
in	O
the	O
gradient	O
.	O
Due	O
to	O
this	O
,	O
very	O
deep	Method
networks	Method
learn	O
very	O
slowly	O
.	O
Sometimes	O
,	O
the	O
gradient	O
in	O
the	O
earlier	O
layer	O
gets	O
larger	O
because	O
derivatives	O
of	O
some	O
activation	O
functions	O
can	O
take	O
larger	O
values	O
.	O
This	O
leads	O
to	O
the	O
problem	O
of	O
exploding	O
gradient	O
.	O
These	O
problems	O
have	O
been	O
reduced	O
in	O
practice	O
through	O
normalized	Method
initialization	Method
and	O
most	O
recently	O
,	O
Batch	Method
Normalization	Method
.	O
Exponential	Method
linear	Method
unit	Method
(	O
ELU	Method
)	O
also	O
reduces	O
the	O
vanishing	Task
gradient	Task
problem	Task
.	O
ELUs	Method
introduce	O
negative	O
values	O
which	O
push	O
the	O
mean	O
activation	O
towards	O
zero	O
.	O
This	O
reduces	O
the	O
bias	O
shift	O
and	O
speeds	O
up	O
learning	Task
.	O
ELUs	Method
give	O
better	O
accuracy	Metric
and	O
learning	Metric
speed	Metric
-	Metric
up	Metric
compared	O
to	O
the	O
combination	O
of	O
ReLU	Method
and	O
Batch	Method
Normalization	Method
.	O
After	O
reducing	O
the	O
vanishing	Task
/	Task
exploding	Task
gradient	Task
problem	Task
,	O
the	O
networks	O
start	O
converging	O
.	O
However	O
,	O
the	O
accuracy	Metric
degrades	O
in	O
such	O
very	O
deep	Method
models	Method
.	O
The	O
most	O
recent	O
contributions	O
towards	O
solving	O
this	O
problem	O
are	O
Highway	Method
Networks	Method
and	O
Residual	Method
Networks	Method
.	O
These	O
networks	O
introduce	O
skip	O
connections	O
,	O
which	O
allow	O
information	O
flow	O
into	O
the	O
deeper	O
layers	O
and	O
enable	O
us	O
to	O
have	O
deeper	O
networks	O
with	O
better	O
accuracy	Metric
.	O
The	O
152	O
-	O
layer	O
ResNet	Method
outperforms	O
all	O
other	O
models	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
use	O
exponential	Method
linear	Method
unit	Method
instead	O
of	O
the	O
combination	O
of	O
ReLU	Method
and	O
Batch	Method
Normalization	Method
.	O
Since	O
exponential	Method
linear	Method
units	Method
reduce	O
the	O
vanishing	Task
gradient	Task
problem	Task
and	O
give	O
better	O
accuracy	Metric
compared	O
to	O
the	O
combination	O
of	O
ReLU	Method
and	O
Batch	Method
Normalization	Method
,	O
we	O
use	O
it	O
in	O
our	O
model	O
to	O
further	O
increase	O
the	O
accuracy	Metric
of	O
Residual	Method
Networks	Method
.	O
We	O
also	O
notice	O
that	O
ELU	Method
speeds	O
up	O
learning	Task
in	O
very	O
deep	Task
networks	Task
as	O
well	O
.	O
We	O
show	O
that	O
our	O
model	O
increases	O
the	O
accuracy	Metric
on	O
datasets	O
like	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
,	O
compared	O
to	O
the	O
original	O
model	O
.	O
It	O
is	O
seen	O
that	O
as	O
the	O
depth	O
increases	O
,	O
the	O
difference	O
in	O
accuracy	Metric
between	O
our	O
model	O
and	O
the	O
original	O
model	O
increases	O
.	O
section	O
:	O
Background	O
Deeper	Method
neural	Method
networks	Method
are	O
very	O
difficult	O
to	O
train	O
.	O
The	O
vanishing	Task
/	Task
exploding	Task
gradients	Task
problem	Task
impedes	O
the	O
convergence	Task
of	Task
deeper	Task
networks	Task
.	O
This	O
problem	O
has	O
been	O
solved	O
by	O
normalized	Method
initialization	Method
.	O
A	O
notable	O
recent	O
contribution	O
towards	O
reducing	O
the	O
vanishing	Task
gradients	Task
problem	Task
is	O
Batch	Method
Normalization	Method
.	O
Instead	O
of	O
normalized	Method
initialization	Method
and	O
keeping	O
a	O
lower	O
learning	Metric
rate	Metric
,	O
Batch	Method
Normalization	Method
makes	O
normalization	Method
a	O
part	O
of	O
the	O
model	O
and	O
performs	O
it	O
for	O
each	O
mini	O
-	O
batch	O
.	O
Once	O
the	O
deeper	O
networks	O
start	O
converging	O
,	O
a	O
degradation	Task
problem	Task
occurs	O
.	O
Due	O
to	O
this	O
,	O
the	O
accuracy	Metric
degrades	O
rapidly	O
after	O
it	O
is	O
saturated	O
.	O
The	O
training	Metric
error	Metric
increases	O
as	O
we	O
add	O
more	O
layers	O
to	O
a	O
deep	Method
model	Method
,	O
as	O
mentioned	O
in	O
.	O
To	O
solve	O
this	O
problem	O
,	O
several	O
authors	O
introduced	O
skip	O
connections	O
to	O
improve	O
the	O
information	O
flow	O
across	O
several	O
layers	O
.	O
Highway	Method
Networks	Method
have	O
parameterized	O
skip	O
connections	O
,	O
known	O
as	O
information	O
highways	O
,	O
which	O
allow	O
information	O
to	O
flow	O
unimpeded	O
into	O
deeper	O
layers	O
.	O
During	O
the	O
training	O
phase	O
,	O
the	O
skip	O
connection	O
parameters	O
are	O
adjusted	O
to	O
control	O
the	O
amount	O
of	O
information	O
allowed	O
on	O
these	O
highways	O
.	O
Residual	Method
Networks	Method
(	O
ResNets	Method
)	O
utilize	O
shortcut	O
connections	O
with	O
the	O
help	O
of	O
identity	Method
transformation	Method
.	O
Unlike	O
Highway	Method
Networks	Method
,	O
these	O
neither	O
introduce	O
extra	O
parameter	O
nor	O
computation	Metric
complexity	Metric
.	O
This	O
improves	O
the	O
accuracy	Metric
of	O
deeper	Method
networks	Method
.	O
With	O
increasing	O
depth	O
,	O
ResNets	Method
give	O
better	O
function	Method
approximation	Method
capabilities	O
as	O
they	O
gain	O
more	O
parameters	O
.	O
The	O
authors	O
â€™	O
hypothesis	O
is	O
that	O
the	O
plain	O
deeper	Method
networks	Method
give	O
worse	O
function	Method
approximation	Method
because	O
the	O
gradients	O
vanish	O
when	O
they	O
are	O
propagated	O
through	O
many	O
layers	O
.	O
To	O
fix	O
this	O
problem	O
,	O
they	O
introduce	O
skip	O
connections	O
to	O
the	O
network	O
.	O
Formally	O
,	O
If	O
the	O
output	O
of	O
layer	O
is	O
and	O
represents	O
multiple	O
convolutional	Method
transformation	O
from	O
layer	O
to	O
,	O
we	O
obtain	O
where	O
represents	O
the	O
identity	O
function	O
and	O
is	O
the	O
default	O
activation	O
function	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
illustrates	O
the	O
basic	O
building	O
block	O
of	O
a	O
Residual	Method
Network	O
which	O
consists	O
of	O
multiple	O
convolutional	Method
and	O
Batch	Method
Normalization	Method
layers	Method
.	O
The	O
identity	Method
transformation	Method
,	O
is	O
used	O
to	O
reduce	O
the	O
dimensions	O
of	O
to	O
match	O
those	O
of	O
.	O
In	O
Residual	Method
Networks	Method
,	O
the	O
gradients	O
and	O
features	O
learned	O
in	O
earlier	O
layers	O
are	O
passed	O
back	O
and	O
forth	O
between	O
the	O
layers	O
via	O
the	O
identity	Method
transformations	Method
.	O
Exponential	Method
Linear	Method
Unit	Method
(	O
ELU	Method
)	O
alleviates	O
the	O
vanishing	Task
gradient	Task
problem	Task
and	O
also	O
speeds	O
up	O
learning	Task
in	O
deep	Method
neural	Method
networks	Method
which	O
leads	O
to	O
higher	O
classification	Metric
accuracies	Metric
.	O
The	O
exponential	O
linear	O
unit	O
(	O
ELU	Method
)	O
is	O
The	O
ReLUs	Method
are	O
non	O
-	O
negative	O
and	O
thus	O
have	O
mean	O
activations	O
larger	O
than	O
zero	O
,	O
whereas	O
ELUs	Method
have	O
negative	O
values	O
,	O
which	O
push	O
the	O
mean	O
activations	O
towards	O
zero	O
.	O
ELUs	Method
saturate	O
to	O
a	O
negative	O
value	O
when	O
the	O
input	O
gets	O
smaller	O
.	O
This	O
decreases	O
the	O
forward	O
propagated	O
variation	O
and	O
information	O
,	O
which	O
draws	O
the	O
mean	O
activations	O
to	O
zero	O
.	O
Units	O
with	O
non	O
-	O
zero	O
mean	O
activations	O
act	O
as	O
a	O
bias	O
for	O
the	O
next	O
layer	O
.	O
If	O
these	O
units	O
do	O
not	O
cancel	O
each	O
other	O
out	O
,	O
then	O
the	O
learning	Method
causes	O
a	O
bias	O
shift	O
for	O
units	O
in	O
the	O
next	O
layer	O
.	O
Therefore	O
,	O
ELUs	Method
decrease	O
the	O
bias	O
shift	O
as	O
the	O
mean	O
activations	O
are	O
closer	O
to	O
zero	O
.	O
Less	O
bias	O
shift	O
also	O
speeds	O
up	O
learning	Task
by	O
bringing	O
standard	O
gradient	O
closer	O
towards	O
the	O
unit	O
natural	O
gradient	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
of	O
ReLU	Method
and	O
ELU	Method
(	O
)	O
.	O
.24	O
.24	O
.24	O
.24	O
section	O
:	O
Residual	Method
Networks	Method
with	O
Exponential	Method
Linear	Method
Unit	Method
subsection	O
:	O
ResNet	Method
Architecture	O
The	O
Residual	Method
Network	O
in	O
is	O
a	O
functional	Method
composition	Method
of	Method
residual	Method
blocks	Method
(	O
ResBlocks	Method
)	O
,	O
each	O
encoding	O
the	O
update	Method
rule	Method
(	O
[	O
reference	O
]	O
)	O
.	O
Fig	O
[	O
reference	O
]	O
shows	O
the	O
schematic	O
illustration	O
of	O
the	O
ResBlock	O
.	O
In	O
this	O
example	O
,	O
consists	O
of	O
a	O
sequence	O
of	O
layers	O
:	O
Conv	Method
-	O
BN	Method
-	O
ReLU	Method
-	O
Conv	Method
-	O
BN	Method
,	O
where	O
Conv	Method
and	O
BN	Method
stands	O
for	O
Convolution	Method
and	O
Batch	Method
Normalization	Method
respectively	O
.	O
This	O
construction	Method
scheme	Method
is	O
adopted	O
in	O
all	O
our	O
experiments	O
while	O
reproducing	O
the	O
results	O
of	O
.	O
The	O
function	O
is	O
parameterized	O
by	O
some	O
set	O
of	O
parameters	O
,	O
which	O
we	O
omit	O
for	O
notational	O
simplicity	O
.	O
Normally	O
,	O
we	O
use	O
64	O
,	O
32	O
or	O
16	O
filters	O
in	O
the	O
convolutional	Method
layers	O
.	O
The	O
size	O
of	O
receptive	O
field	O
is	O
.	O
Although	O
it	O
does	O
not	O
seem	O
attractive	O
but	O
,	O
in	O
practice	O
it	O
gives	O
better	O
accuracy	Metric
without	O
adding	O
any	O
overhead	Metric
costs	Metric
,	O
as	O
compared	O
to	O
plain	Method
networks	Method
.	O
subsection	O
:	O
ResNet	Method
with	O
ELU	Method
In	O
comparison	O
with	O
the	O
ResNet	Method
model	O
,	O
we	O
use	O
Exponential	Method
Linear	Method
Unit	Method
(	O
ELU	Method
)	O
in	O
place	O
of	O
a	O
combination	O
of	O
ReLU	Method
with	O
Batch	Method
Normalization	Method
.	O
Fig	O
.	O
[	O
reference	O
]	O
illustrates	O
our	O
different	O
experiments	O
with	O
ELUs	Method
in	O
ResBlock	Method
.	O
subsubsection	O
:	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
ELU	Method
In	O
this	O
model	O
,	O
consists	O
of	O
a	O
sequence	O
of	O
layers	O
:	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
ELU	Method
.	O
Fig	O
.	O
[	O
reference	O
]	O
represents	O
the	O
basic	O
building	O
block	O
of	O
this	O
experiment	O
.	O
We	O
trained	O
our	O
model	O
using	O
the	O
specification	O
mentioned	O
in	O
[	O
reference	O
]	O
.	O
But	O
we	O
found	O
that	O
after	O
few	O
iterations	O
,	O
the	O
gradients	O
blew	O
up	O
.	O
When	O
the	O
learning	Metric
rate	Metric
is	O
decreased	O
,	O
the	O
20	Method
-	Method
layer	Method
model	O
starts	O
converging	O
but	O
to	O
very	O
less	O
accuracy	Metric
.	O
The	O
deeper	O
models	O
like	O
56	O
and	O
110	O
-	O
layer	O
still	O
do	O
not	O
converge	O
after	O
decreasing	O
the	O
learning	Metric
rate	Metric
.	O
This	O
model	O
clearly	O
fails	O
as	O
the	O
trivial	O
problem	O
of	O
exploding	O
gradient	O
can	O
not	O
be	O
reduced	O
in	O
very	O
deep	Method
models	Method
.	O
subsubsection	O
:	O
ELU	Method
-	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
This	O
is	O
a	O
full	O
pre	Method
-	Method
activation	Method
unit	Method
ResBlock	O
with	O
ELU	Method
.	O
The	O
sequence	O
of	O
layers	O
is	O
ELU	Method
-	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
.	O
Fig	O
.	O
[	O
reference	O
]	O
highlights	O
the	O
basic	O
ResBlock	O
of	O
this	O
experiment	O
.	O
During	O
the	O
training	O
of	O
this	O
model	O
too	O
,	O
the	O
gradients	O
exploded	O
after	O
few	O
iterations	O
.	O
Due	O
to	O
the	O
exponential	O
function	O
,	O
the	O
gradients	O
get	O
larger	O
and	O
lead	O
to	O
exploding	Task
gradient	Task
problem	Task
.	O
Even	O
decreasing	O
the	O
learning	Metric
rate	Metric
also	O
does	O
not	O
reduce	O
this	O
problem	O
.	O
We	O
decided	O
to	O
add	O
a	O
Batch	Method
Normalization	Method
layer	O
before	O
Addition	O
to	O
control	O
this	O
problem	O
.	O
subsubsection	O
:	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
BN	Method
and	O
ELU	Method
after	O
Addition	O
.23	O
.23	O
To	O
control	O
the	O
exploding	O
gradient	O
,	O
we	O
added	O
a	O
Batch	Method
Normalization	Method
before	O
addition	O
.	O
So	O
,	O
the	O
sequence	O
of	O
layers	O
in	O
this	O
ResBlock	O
is	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
BN	Method
and	O
ELU	Method
after	O
addition	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
represents	O
the	O
ResBlock	O
used	O
in	O
this	O
experiment	O
.	O
Thus	O
in	O
this	O
ResBlock	O
,	O
the	O
update	O
rule	O
(	O
[	O
reference	O
]	O
)	O
for	O
the	O
layer	O
is	O
The	O
Batch	Method
Normalization	Method
layer	O
reduces	O
the	O
exploding	Task
gradient	Task
problem	Task
found	O
in	O
the	O
previous	O
two	O
models	O
.	O
We	O
found	O
that	O
this	O
model	O
gives	O
better	O
accuracy	Metric
for	O
20	Method
-	Method
layer	Method
model	O
.	O
However	O
,	O
as	O
we	O
increased	O
the	O
depth	O
of	O
the	O
network	O
,	O
the	O
accuracy	Metric
degrades	O
for	O
the	O
deeper	Method
models	Method
.	O
If	O
the	O
ELU	Method
activation	O
function	O
is	O
placed	O
after	O
addtion	O
,	O
then	O
the	O
mean	O
activation	O
of	O
the	O
output	O
pushes	O
towards	O
zero	O
.	O
This	O
could	O
be	O
beneficial	O
.	O
However	O
,	O
this	O
forces	O
each	O
skip	O
connection	O
to	O
perturb	O
the	O
output	O
.	O
This	O
has	O
a	O
harmful	O
effect	O
and	O
we	O
found	O
that	O
this	O
leads	O
to	O
degradation	O
of	O
accuracy	Metric
in	O
very	O
deep	O
ResNets	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
depicts	O
the	O
effects	O
of	O
including	O
ELU	Method
after	O
addition	O
in	O
this	O
ResBlock	O
.	O
subsubsection	O
:	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
BN	Method
and	O
No	O
ELU	Method
after	O
Addition	O
.33	O
.33	O
.33	O
.4	O
.4	O
Fig	O
.	O
[	O
reference	O
]	O
gives	O
an	O
illustration	O
of	O
the	O
basic	O
building	O
block	O
of	O
our	O
model	O
.	O
Thus	O
in	O
our	O
model	O
,	O
represents	O
the	O
following	O
sequence	O
of	O
layers	O
:	O
Conv	Method
-	O
ELU	Method
-	O
Conv	Method
-	O
BN	Method
.	O
The	O
update	O
rule	O
(	O
[	O
reference	O
]	O
)	O
for	O
the	O
layer	O
is	O
This	O
is	O
the	O
basic	O
building	O
block	O
for	O
all	O
our	O
experiments	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
.	O
We	O
show	O
that	O
not	O
including	O
ELU	Method
after	O
addition	O
does	O
not	O
degrade	O
the	O
accuracy	Metric
,	O
unlike	O
the	O
previous	O
model	O
.	O
This	O
ResBlock	O
improves	O
the	O
learning	O
behavior	O
and	O
the	O
classification	Metric
performance	Metric
of	O
the	O
Residual	Method
Network	O
.	O
section	O
:	O
Results	O
We	O
empirically	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
model	O
on	O
a	O
series	O
of	O
benchmark	O
data	O
sets	O
:	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
.	O
In	O
our	O
experiments	O
,	O
we	O
compare	O
the	O
learning	Metric
behavior	Metric
and	O
the	O
classification	Metric
performance	Metric
of	O
both	O
the	O
models	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
.	O
The	O
experiments	O
prove	O
that	O
our	O
model	O
outperforms	O
the	O
original	O
ResNet	Method
model	O
in	O
terms	O
of	O
learning	Metric
behavior	Metric
and	O
classification	Metric
performance	Metric
on	O
both	O
the	O
datasets	O
.	O
Finally	O
,	O
we	O
compare	O
the	O
classification	Metric
performance	Metric
of	O
our	O
model	O
with	O
other	O
previously	O
published	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
subsection	O
:	O
CIFAR	Material
-	Material
10	Material
Analysis	O
The	O
first	O
experiment	O
was	O
performed	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	O
,	O
which	O
consists	O
of	O
50k	O
training	O
images	O
and	O
10k	O
test	O
images	O
in	O
10	O
classes	O
.	O
In	O
our	O
experiments	O
,	O
we	O
performed	O
training	O
on	O
the	O
training	O
set	O
and	O
evaluation	O
on	O
the	O
test	O
set	O
.	O
The	O
inputs	O
to	O
the	O
network	O
are	O
images	O
which	O
are	O
color	O
-	O
normalized	O
.	O
We	O
use	O
a	O
receptive	O
field	O
in	O
the	O
convolution	Method
layer	Method
.	O
We	O
use	O
a	O
stack	Method
of	Method
layers	Method
with	O
convolution	Method
on	O
the	O
feature	O
maps	O
of	O
sizes	O
respectively	O
,	O
with	O
on	O
each	O
feature	O
map	O
.	O
The	O
number	O
of	O
filters	O
are	O
respectively	O
.	O
The	O
original	O
ResNet	Method
model	O
ends	O
with	O
a	O
global	Method
average	Method
pooling	Method
,	O
a	O
10	Method
-	Method
way	Method
fully	Method
-	Method
connected	Method
layer	Method
and	O
a	O
softmax	Method
layer	Method
.	O
In	O
our	O
model	O
,	O
we	O
add	O
an	O
ELU	Method
activation	O
function	O
just	O
before	O
the	O
global	Method
average	Method
pooling	Method
layer	Method
.	O
These	O
two	O
models	O
are	O
trained	O
on	O
a	O
AWS	Method
g2.2xlarge	Method
instance	Method
(	O
which	O
has	O
a	O
single	O
GPU	O
)	O
with	O
a	O
mini	O
batch	O
-	O
size	O
of	O
128	O
.	O
We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
,	O
and	O
adopt	O
the	O
weight	Method
initialization	Method
in	O
and	O
BN	Method
but	O
with	O
no	O
dropout	Method
.	O
We	O
start	O
with	O
a	O
learning	Metric
rate	Metric
of	O
0.1	O
and	O
divide	O
by	O
10	O
after	O
81	O
epochs	O
,	O
and	O
again	O
divide	O
by	O
10	O
after	O
122	O
epochs	O
.	O
We	O
use	O
the	O
data	Method
augmentation	Method
mentioned	O
in	O
during	O
the	O
training	O
phase	O
:	O
Add	O
4	O
pixels	O
on	O
each	O
side	O
and	O
do	O
a	O
random	O
crop	O
from	O
the	O
padded	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O
During	O
the	O
testing	O
phase	O
,	O
we	O
only	O
use	O
a	O
color	O
-	O
normalized	O
image	O
.	O
Our	O
experiments	O
are	O
executed	O
on	O
20	O
,	O
32	O
,	O
44	O
,	O
56	O
and	O
110	O
-	O
layer	O
networks	O
.	O
subsubsection	O
:	O
Learning	Task
Behavior	Task
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
of	O
learning	Metric
behaviours	Metric
between	O
our	O
model	O
and	O
the	O
original	O
ResNet	Method
model	O
on	O
CIFAR	Material
-	Material
10	Material
dataset	O
for	O
20	O
,	O
32	O
,	O
44	O
,	O
56	O
and	O
110	O
-	O
layers	O
.	O
The	O
graphs	O
prove	O
that	O
for	O
all	O
the	O
different	O
number	O
of	O
layers	O
,	O
our	O
model	O
possesses	O
a	O
superior	O
learning	Metric
behavior	Metric
and	O
converges	O
many	O
epochs	O
before	O
the	O
original	O
model	O
.	O
As	O
the	O
depth	O
of	O
the	O
model	O
increases	O
,	O
our	O
model	O
also	O
learns	O
faster	O
than	O
the	O
original	O
model	O
.	O
The	O
difference	O
between	O
the	O
learning	Metric
rate	Metric
of	O
these	O
two	O
models	O
increases	O
as	O
the	O
depth	O
increases	O
.	O
Comparing	O
Fig	O
.	O
[	O
reference	O
]	O
and	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
we	O
can	O
easily	O
notice	O
the	O
huge	O
difference	O
in	O
learning	Metric
rates	Metric
for	O
20	Method
-	Method
layer	Method
and	O
110	Method
-	Method
layer	Method
models	Method
.	O
After	O
125	O
epochs	O
,	O
both	O
the	O
models	O
converge	O
to	O
almost	O
the	O
same	O
value	O
.	O
But	O
,	O
our	O
model	O
has	O
a	O
slightly	O
lower	O
training	Metric
loss	Metric
compared	O
to	O
the	O
original	O
model	O
.	O
subsubsection	O
:	O
Classification	Metric
Performance	Metric
Fig	O
.	O
[	O
reference	O
]	O
illustrates	O
the	O
comparison	O
of	O
classification	Metric
performance	Metric
between	O
our	O
model	O
and	O
the	O
original	O
one	O
on	O
CIFAR	Material
-	Material
10	Material
dataset	O
for	O
20	O
,	O
32	O
,	O
44	O
,	O
56	O
and	O
110	O
layers	O
.	O
We	O
observe	O
that	O
for	O
the	O
20	Method
-	Method
layer	Method
model	O
,	O
the	O
test	Metric
error	Metric
is	O
nearly	O
the	O
same	O
for	O
both	O
the	O
models	O
.	O
But	O
,	O
as	O
the	O
depth	O
increases	O
,	O
our	O
model	O
significantly	O
outperforms	O
the	O
original	O
model	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
test	O
error	O
for	O
both	O
the	O
models	O
from	O
the	O
epoch	O
with	O
the	O
lowest	O
validation	Metric
error	Metric
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
that	O
the	O
gap	O
between	O
the	O
test	O
error	O
of	O
the	O
two	O
models	O
increases	O
as	O
the	O
depth	O
is	O
also	O
increased	O
.	O
.33	O
.33	O
.33	O
.4	O
.4	O
subsection	O
:	O
CIFAR	Material
-	Material
100	Material
Analysis	O
Similar	O
to	O
CIFAR	Material
-	Material
10	Material
,	O
the	O
CIFAR	Material
-	Material
100	Material
dataset	Material
also	O
contains	O
images	O
with	O
the	O
same	O
train	O
-	O
test	O
split	O
,	O
but	O
from	O
100	O
classes	O
.	O
For	O
both	O
the	O
original	O
model	O
and	O
our	O
model	O
,	O
the	O
experimental	O
settings	O
are	O
exactly	O
the	O
same	O
as	O
those	O
of	O
CIFAR	Material
-	Material
10	Material
.	O
We	O
trained	O
only	O
for	O
the	O
110	Method
-	Method
layer	Method
models	Method
as	O
it	O
gives	O
us	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
that	O
for	O
CIFAR	Material
-	Material
100	Material
dataset	Material
as	O
well	O
,	O
our	O
model	O
learns	O
faster	O
than	O
the	O
original	O
ResNet	Method
model	O
.	O
The	O
original	O
model	O
yields	O
a	O
test	Metric
error	Metric
of	O
27.23	O
%	O
,	O
which	O
is	O
already	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
CIFAR	Material
-	Material
100	Material
with	O
standard	O
data	Method
augmentation	Method
.	O
Our	O
model	O
reduces	O
the	O
test	Metric
error	Metric
to	O
26.55	O
%	O
and	O
is	O
again	O
one	O
of	O
the	O
best	O
published	O
single	O
model	O
performances	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
that	O
the	O
test	O
error	O
of	O
our	O
model	O
is	O
much	O
lower	O
from	O
the	O
starting	O
epoch	O
itself	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
of	O
our	O
result	O
with	O
other	O
previously	O
published	O
results	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
.	O
.23	O
.23	O
section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
Residual	Method
Networks	Method
with	O
exponential	Method
linear	Method
units	Method
which	O
learn	O
faster	O
than	O
the	O
current	O
Residual	Method
Networks	Method
.	O
They	O
also	O
give	O
better	O
accuracy	Metric
than	O
the	O
original	O
ones	O
when	O
the	O
depth	O
is	O
increased	O
.	O
On	O
datasets	O
like	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
,	O
we	O
improve	O
beyond	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
terms	O
of	O
test	Metric
error	Metric
,	O
while	O
also	O
learning	O
faster	O
than	O
these	O
models	O
using	O
ELUs	Method
.	O
ELUs	Method
push	O
the	O
mean	O
activations	O
towards	O
zero	O
as	O
they	O
introduce	O
small	O
negative	O
values	O
.	O
This	O
reduces	O
the	O
bias	O
shift	O
and	O
increases	O
the	O
learning	Metric
speed	Metric
.	O
Our	O
experiments	O
show	O
that	O
not	O
only	O
does	O
our	O
model	O
have	O
superior	O
learning	O
behavior	O
,	O
but	O
it	O
also	O
provides	O
better	O
accuracy	Metric
as	O
compared	O
to	O
the	O
current	O
model	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	Material
.	O
This	O
enables	O
the	O
researchers	O
to	O
use	O
very	O
deep	Method
models	Method
and	O
also	O
increase	O
their	O
learning	Metric
behavior	Metric
and	O
classification	Metric
performance	Metric
at	O
the	O
same	O
time	O
.	O
bibliography	O
:	O
References	O
