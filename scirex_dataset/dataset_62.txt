document	O
:	O
Multimodal	Task
Sentiment	Task
Analysis	Task
using	O
Hierarchical	Method
Fusion	Method
with	O
Context	Method
Modeling	Method
Multimodal	Task
sentiment	Task
analysis	Task
is	O
a	O
very	O
actively	O
growing	O
field	O
of	O
research	O
.	O
A	O
promising	O
area	O
of	O
opportunity	O
in	O
this	O
field	O
is	O
to	O
improve	O
the	O
multimodal	Method
fusion	Method
mechanism	Method
.	O
We	O
present	O
a	O
novel	O
feature	O
fusion	Method
strategy	Method
that	O
proceeds	O
in	O
a	O
hierarchical	O
fashion	O
,	O
first	O
fusing	O
the	O
modalities	O
two	O
in	O
two	O
and	O
only	O
then	O
fusing	O
all	O
three	O
modalities	O
.	O
On	O
multimodal	Task
sentiment	Task
analysis	Task
of	O
individual	O
utterances	O
,	O
our	O
strategy	O
outperforms	O
conventional	O
concatenation	Method
of	Method
features	Method
by	O
1	O
%	O
,	O
which	O
amounts	O
to	O
5	O
%	O
reduction	O
in	O
error	Metric
rate	Metric
.	O
On	O
utterance	Task
-	Task
level	Task
multimodal	Task
sentiment	Task
analysis	Task
of	O
multi	O
-	O
utterance	O
video	O
clips	O
,	O
for	O
which	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
techniques	O
incorporate	O
contextual	O
information	O
from	O
other	O
utterances	O
of	O
the	O
same	O
clip	O
,	O
our	O
hierarchical	Method
fusion	Method
gives	O
up	O
to	O
2.4	O
%	O
(	O
almost	O
10	O
%	O
error	Metric
rate	Metric
reduction	Metric
)	O
over	O
currently	O
used	O
concatenation	Method
.	O
The	O
implementation	O
of	O
our	O
method	O
is	O
publicly	O
available	O
in	O
the	O
form	O
of	O
open	O
-	O
source	O
code	O
.	O
section	O
:	O
Introduction	O
On	O
numerous	O
social	O
media	O
platforms	O
,	O
such	O
as	O
YouTube	O
,	O
Facebook	O
,	O
or	O
Instagram	O
,	O
people	O
share	O
their	O
opinions	O
on	O
all	O
kinds	O
of	O
topics	O
in	O
the	O
form	O
of	O
posts	O
,	O
images	O
,	O
and	O
video	O
clips	O
.	O
With	O
the	O
proliferation	O
of	O
smartphones	O
and	O
tablets	O
,	O
which	O
has	O
greatly	O
boosted	O
content	Task
sharing	Task
,	O
people	O
increasingly	O
share	O
their	O
opinions	O
on	O
newly	O
released	O
products	O
or	O
on	O
other	O
topics	O
in	O
form	O
of	O
video	O
reviews	O
or	O
comments	O
.	O
This	O
is	O
an	O
excellent	O
opportunity	O
for	O
large	O
companies	O
to	O
capitalize	O
on	O
,	O
by	O
extracting	O
user	O
sentiment	O
,	O
suggestions	O
,	O
and	O
complaints	O
on	O
their	O
products	O
from	O
these	O
video	O
reviews	O
.	O
This	O
information	O
also	O
opens	O
new	O
horizons	O
to	O
improving	O
our	O
quality	O
of	O
life	O
by	O
making	O
informed	O
decisions	O
on	O
the	O
choice	O
of	O
products	O
we	O
buy	O
,	O
services	O
we	O
use	O
,	O
places	O
we	O
visit	O
,	O
or	O
movies	O
we	O
watch	O
basing	O
on	O
the	O
experience	O
and	O
opinions	O
of	O
other	O
users	O
.	O
Videos	O
convey	O
information	O
through	O
three	O
channels	O
:	O
audio	O
,	O
video	O
,	O
and	O
text	O
(	O
in	O
the	O
form	O
of	O
speech	O
)	O
.	O
Mining	Task
opinions	Task
from	O
this	O
plethora	O
of	O
multimodal	O
data	O
calls	O
for	O
a	O
solid	O
multimodal	Task
sentiment	Task
analysis	Task
technology	Task
.	O
One	O
of	O
the	O
major	O
problems	O
faced	O
in	O
multimodal	Task
sentiment	Task
analysis	Task
is	O
the	O
fusion	Task
of	Task
features	Task
pertaining	O
to	O
different	O
modalities	O
.	O
For	O
this	O
,	O
the	O
majority	O
of	O
the	O
recent	O
works	O
in	O
multimodal	Task
sentiment	Task
analysis	Task
have	O
simply	O
concatenated	O
the	O
feature	O
vectors	O
of	O
different	O
modalities	O
.	O
However	O
,	O
this	O
does	O
not	O
take	O
into	O
account	O
that	O
different	O
modalities	O
may	O
carry	O
conflicting	O
information	O
.	O
We	O
hypothesize	O
that	O
the	O
fusion	Method
method	Method
we	O
present	O
in	O
this	O
paper	O
deals	O
with	O
this	O
issue	O
better	O
,	O
and	O
present	O
experimental	O
evidence	O
showing	O
improvement	O
over	O
simple	O
concatenation	Method
of	Method
feature	Method
vectors	Method
.	O
Also	O
,	O
following	O
the	O
state	O
of	O
the	O
art	O
,	O
we	O
employ	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
to	O
propagate	O
contextual	O
information	O
between	O
utterances	O
in	O
a	O
video	O
clip	O
,	O
which	O
significantly	O
improves	O
the	O
classification	Task
results	O
and	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
a	O
significant	O
margin	O
of	O
1–2	O
%	O
for	O
all	O
the	O
modality	O
combinations	O
.	O
In	O
our	O
method	O
,	O
we	O
first	O
obtain	O
unimodal	O
features	O
for	O
each	O
utterance	O
for	O
all	O
three	O
modalities	O
.	O
Then	O
,	O
using	O
RNN	Method
we	O
extract	O
context	O
-	O
aware	O
utterance	O
features	O
.	O
Thus	O
,	O
we	O
transform	O
the	O
context	O
-	O
aware	O
utterance	O
vectors	O
to	O
the	O
vectors	O
of	O
the	O
same	O
dimensionality	O
.	O
We	O
assume	O
that	O
these	O
transformed	O
vectors	O
contain	O
abstract	O
features	O
representing	O
the	O
attributes	O
relevant	O
to	O
sentiment	Task
classification	Task
.	O
Next	O
,	O
we	O
compare	O
and	O
combine	O
each	O
bimodal	Method
combination	Method
of	O
these	O
abstract	O
features	O
using	O
fully	Method
-	Method
connected	Method
layers	Method
.	O
This	O
yields	O
fused	O
bimodal	O
feature	O
vectors	O
.	O
Similarly	O
to	O
the	O
unimodal	Task
case	Task
,	O
we	O
use	O
RNN	Method
to	O
generate	O
context	O
-	O
aware	O
features	O
.	O
Finally	O
,	O
we	O
combine	O
these	O
bimodal	O
vectors	O
into	O
a	O
trimodal	Method
vector	Method
using	O
,	O
again	O
,	O
fully	Method
-	Method
connected	Method
layers	Method
and	O
use	O
a	O
RNN	Method
to	O
pass	O
contextual	O
information	O
between	O
them	O
.	O
We	O
empirically	O
show	O
that	O
the	O
feature	O
vectors	O
obtained	O
in	O
this	O
manner	O
are	O
more	O
useful	O
for	O
the	O
sentiment	Task
classification	Task
task	Task
.	O
The	O
implementation	O
of	O
our	O
method	O
is	O
publicly	O
available	O
in	O
the	O
form	O
of	O
open	O
-	O
source	O
code	O
.	O
This	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
sec	O
:	O
related	O
-	O
work	O
-	O
1	O
briefly	O
discusses	O
important	O
previous	O
work	O
in	O
multimodal	Task
feature	Task
fusion	Task
;	O
sec	O
:	O
model	O
describes	O
our	O
method	O
in	O
details	O
;	O
sec	O
:	O
experiments	O
reports	O
the	O
results	O
of	O
our	O
experiments	O
and	O
discuss	O
their	O
implications	O
;	O
finally	O
,	O
sec	O
:	O
conclusions	O
concludes	O
the	O
paper	O
and	O
discusses	O
future	O
work	O
.	O
section	O
:	O
Related	O
Work	O
In	O
recent	O
years	O
,	O
sentiment	Task
analysis	Task
has	O
become	O
increasingly	O
popular	O
for	O
processing	O
social	O
media	O
data	O
on	O
online	O
communities	O
,	O
blogs	O
,	O
wikis	O
,	O
microblogging	O
platforms	O
,	O
and	O
other	O
online	O
collaborative	O
media	O
.	O
Sentiment	Task
analysis	Task
is	O
a	O
branch	O
of	O
affective	Task
computing	Task
research	Task
that	O
aims	O
to	O
classify	O
text	O
–	O
but	O
sometimes	O
also	O
audio	O
and	O
video	O
–	O
into	O
either	O
positive	O
or	O
negative	O
–	O
but	O
sometimes	O
also	O
neutral	O
.	O
Most	O
of	O
the	O
literature	O
is	O
on	O
English	O
language	O
but	O
recently	O
an	O
increasing	O
number	O
of	O
works	O
are	O
tackling	O
the	O
multilinguality	Task
issue	Task
,	O
especially	O
in	O
booming	O
online	O
languages	O
such	O
as	O
Chinese	O
.	O
Sentiment	Method
analysis	Method
techniques	Method
can	O
be	O
broadly	O
categorized	O
into	O
symbolic	Method
and	Method
sub	Method
-	Method
symbolic	Method
approaches	Method
:	O
the	O
former	O
include	O
the	O
use	O
of	O
lexicons	O
,	O
ontologies	O
,	O
and	O
semantic	Method
networks	Method
to	O
encode	O
the	O
polarity	O
associated	O
with	O
words	O
and	O
multiword	O
expressions	O
;	O
the	O
latter	O
consist	O
of	O
supervised	Method
,	Method
semi	Method
-	Method
supervised	Method
and	Method
unsupervised	Method
machine	Method
learning	Method
techniques	Method
that	O
perform	O
sentiment	Task
classification	Task
based	O
on	O
word	O
co	O
-	O
occurrence	O
frequencies	O
.	O
Among	O
these	O
,	O
the	O
most	O
popular	O
recently	O
are	O
algorithms	O
based	O
on	O
deep	Method
neural	Method
networks	Method
and	O
generative	Method
adversarial	Method
networks	Method
.	O
While	O
most	O
works	O
approach	O
it	O
as	O
a	O
simple	O
categorization	Task
problem	Task
,	O
sentiment	Task
analysis	Task
is	O
actually	O
a	O
suitcase	O
research	O
problem	O
that	O
requires	O
tackling	O
many	O
NLP	Task
tasks	Task
,	O
including	O
word	Task
polarity	Task
disambiguation	Task
,	O
subjectivity	Task
detection	Task
,	O
personality	Task
recognition	Task
,	O
microtext	Task
normalization	Task
,	O
concept	Task
extraction	Task
,	O
time	Task
tagging	Task
,	O
and	O
aspect	Task
extraction	Task
.	O
Sentiment	Task
analysis	Task
has	O
raised	O
growing	O
interest	O
both	O
within	O
the	O
scientific	O
community	O
,	O
leading	O
to	O
many	O
exciting	O
open	O
challenges	O
,	O
as	O
well	O
as	O
in	O
the	O
business	Task
world	Task
,	O
due	O
to	O
the	O
remarkable	O
benefits	O
to	O
be	O
had	O
from	O
financial	Task
and	Task
political	Task
forecasting	Task
,	O
e	Task
-	Task
health	Task
and	O
e	Task
-	Task
tourism	Task
,	O
user	Task
profiling	Task
and	O
community	Task
detection	Task
,	O
manufacturing	Task
and	Task
supply	Task
chain	Task
applications	Task
,	O
human	Task
communication	Task
comprehension	Task
and	O
dialogue	Task
systems	Task
,	O
etc	O
.	O
In	O
the	O
field	O
of	O
emotion	Task
recognition	Task
,	O
early	O
works	O
by	O
and	O
showed	O
that	O
fusion	Method
of	Method
audio	Method
and	Method
visual	Method
systems	Method
,	O
creating	O
a	O
bimodal	O
signal	O
,	O
yielded	O
a	O
higher	O
accuracy	Metric
than	O
any	O
unimodal	Method
system	Method
.	O
Such	O
fusion	O
has	O
been	O
analyzed	O
at	O
both	O
feature	Metric
level	Metric
and	O
decision	Metric
level	Metric
.	O
Although	O
there	O
is	O
much	O
work	O
done	O
on	O
audio	Task
-	Task
visual	Task
fusion	Task
for	O
emotion	Task
recognition	Task
,	O
exploring	O
contribution	O
of	O
text	O
along	O
with	O
audio	O
and	O
visual	O
modalities	O
in	O
multimodal	Task
emotion	Task
detection	Task
has	O
been	O
little	O
explored	O
.	O
and	O
fused	O
information	O
from	O
audio	O
,	O
visual	O
and	O
textual	O
modalities	O
to	O
extract	O
emotion	O
and	O
sentiment	O
.	O
and	O
fused	O
audio	O
and	O
textual	O
modalities	O
for	O
emotion	Task
recognition	Task
.	O
Both	O
approaches	O
relied	O
on	O
a	O
feature	Method
-	Method
level	Method
fusion	Method
.	O
fused	O
audio	O
and	O
textual	O
clues	O
at	O
decision	O
level	O
.	O
uses	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
to	O
extract	O
features	O
from	O
the	O
modalities	O
and	O
then	O
employs	O
multiple	Method
-	Method
kernel	Method
learning	Method
(	O
MKL	Method
)	Method
for	O
sentiment	Task
analysis	Task
.	O
The	O
current	O
state	O
of	O
the	O
art	O
,	O
set	O
forth	O
by	O
,	O
extracts	O
contextual	O
information	O
from	O
the	O
surrounding	O
utterances	O
using	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
.	O
fuses	O
different	O
modalities	O
with	O
deep	Method
learning	Method
-	Method
based	Method
tools	Method
.	O
uses	O
tensor	Method
fusion	Method
.	O
further	O
extends	O
upon	O
the	O
ensemble	Method
of	Method
CNN	Method
and	O
MKL	Method
.	O
Unlike	O
existing	O
approaches	O
,	O
which	O
use	O
simple	O
concatenation	O
based	O
early	Method
fusion	Method
and	O
non	Method
-	Method
trainable	Method
tensors	Method
based	Method
fusion	Method
,	O
this	O
work	O
proposes	O
a	O
hierarchical	Method
fusion	Method
capable	O
of	O
learning	O
the	O
bimodal	O
and	O
trimodal	O
correlations	O
for	O
data	Task
fusion	Task
using	O
deep	Method
neural	Method
networks	Method
.	O
The	O
method	O
is	O
end	O
-	O
to	O
-	O
end	O
and	O
,	O
in	O
order	O
to	O
accomplish	O
the	O
fusion	Task
,	O
it	O
can	O
be	O
plugged	O
into	O
any	O
deep	Method
neural	Method
network	Method
based	Method
multimodal	Method
sentiment	Method
analysis	Method
framework	Method
.	O
section	O
:	O
Our	O
Method	O
In	O
this	O
section	O
,	O
we	O
discuss	O
our	O
novel	O
methodology	O
behind	O
solving	O
the	O
sentiment	Task
classification	Task
problem	Task
.	O
First	O
we	O
discuss	O
the	O
overview	O
of	O
our	O
method	O
and	O
then	O
we	O
discuss	O
the	O
whole	O
method	O
in	O
details	O
,	O
step	O
by	O
step	O
.	O
subsection	O
:	O
Overview	O
subsubsection	O
:	O
Unimodal	Method
Feature	Method
Extraction	Method
We	O
extract	O
utterance	O
-	O
level	O
features	O
for	O
three	O
modalities	O
.	O
This	O
step	O
is	O
discussed	O
in	O
UFE	O
.	O
subsubsection	O
:	O
Multimodal	Task
Fusion	Task
paragraph	O
:	O
Problems	O
of	O
early	Method
fusion	Method
The	O
majority	O
of	O
the	O
work	O
on	O
multimodal	O
data	O
use	O
concatenation	Method
,	O
or	O
early	Method
fusion	Method
(	O
fig	O
:	O
early_fusion	Method
)	O
,	O
as	O
their	O
fusion	Method
strategy	Method
.	O
The	O
problem	O
with	O
this	O
simplistic	O
approach	O
is	O
that	O
it	O
can	O
not	O
filter	O
out	O
and	O
conflicting	O
or	O
redundant	O
information	O
obtained	O
from	O
different	O
modalities	O
.	O
To	O
address	O
this	O
major	O
issue	O
,	O
we	O
devise	O
an	O
hierarchical	Method
approach	Method
which	O
proceeds	O
from	O
unimodal	O
to	O
bimodal	O
vectors	O
and	O
then	O
bimodal	O
to	O
trimodal	O
vectors	O
.	O
paragraph	O
:	O
Bimodal	Method
fusion	Method
We	O
fuse	O
the	O
utterance	O
feature	O
vectors	O
for	O
each	O
bimodal	O
combination	O
,	O
i.e.	O
,	O
T	O
+	O
V	O
,	O
T	O
+	O
A	O
,	O
and	O
A	O
+	O
V.	O
This	O
step	O
is	O
depicted	O
in	O
fig	O
:	O
hfusion	Method
-	O
bimodal	O
and	O
discussed	O
in	O
details	O
in	O
sec	O
:	O
bimodal	O
.	O
We	O
use	O
the	O
penultimate	O
layer	O
for	O
fig	O
:	O
hfusion	Method
-	O
bimodal	O
as	O
bimodal	O
features	O
.	O
paragraph	O
:	O
Trimodal	Method
fusion	Method
We	O
fuse	O
the	O
three	O
bimodal	O
features	O
to	O
obtain	O
trimodal	O
feature	O
as	O
depicted	O
in	O
fig	O
:	O
hfusion	Method
-	O
trimodal	O
.	O
This	O
step	O
is	O
discussed	O
in	O
details	O
in	O
sec	O
:	O
trimodal	Method
.	O
paragraph	O
:	O
Addition	O
of	O
context	O
We	O
also	O
improve	O
the	O
quality	O
of	O
feature	O
vectors	O
(	O
both	O
unimodal	O
and	O
multimodal	O
)	O
by	O
incorporating	O
information	O
from	O
surrounding	O
utterances	O
using	O
RNN	Method
.	O
We	O
model	O
the	O
context	O
using	O
gated	Method
recurrent	Method
unit	Method
(	O
GRU	Method
)	O
as	O
depicted	O
in	O
fig	O
:	O
architecture	O
.	O
The	O
details	O
of	O
context	Method
modeling	Method
is	O
discussed	O
in	O
sec	O
:	O
context	O
and	O
the	O
following	O
subsections	O
.	O
paragraph	O
:	O
Classification	Task
We	O
classify	O
the	O
feature	O
vectors	O
using	O
a	O
softmax	Method
layer	Method
.	O
subsection	O
:	O
Unimodal	Task
Feature	Task
Extraction	Task
In	O
this	O
section	O
,	O
we	O
discuss	O
the	O
method	O
of	O
feature	Task
extraction	Task
for	O
three	O
different	O
modalities	O
:	O
audio	O
,	O
video	O
,	O
and	O
text	O
.	O
subsubsection	O
:	O
Textual	Method
Feature	Method
Extraction	Method
The	O
textual	O
data	O
is	O
obtained	O
from	O
the	O
transcripts	O
of	O
the	O
videos	O
.	O
We	O
apply	O
a	O
deep	Method
Convolutional	Method
Neural	Method
Networks	Method
(	O
CNN	Method
)	O
on	O
each	O
utterance	O
to	O
extract	O
textual	O
features	O
.	O
Each	O
utterance	O
in	O
the	O
text	O
is	O
represented	O
as	O
an	O
array	O
of	O
pre	O
-	O
trained	O
300	O
-	O
dimensional	O
word2vec	O
vectors	O
.	O
Further	O
,	O
the	O
utterances	O
are	O
truncated	O
or	O
padded	O
with	O
null	O
vectors	O
to	O
have	O
exactly	O
50	O
words	O
.	O
Next	O
,	O
these	O
utterances	O
as	O
array	O
of	O
vectors	O
are	O
passed	O
through	O
two	O
different	O
convolutional	Method
layers	Method
;	O
first	O
layer	O
having	O
two	O
filters	O
of	O
size	O
3	O
and	O
4	O
respectively	O
with	O
50	O
feature	O
maps	O
each	O
and	O
the	O
second	O
layer	O
has	O
a	O
filter	Method
of	O
size	O
2	O
with	O
100	O
feature	O
maps	O
.	O
Each	O
convolutional	Method
layer	Method
is	O
followed	O
by	O
a	O
max	Method
-	Method
pooling	Method
layer	Method
with	O
window	O
.	O
The	O
output	O
of	O
the	O
second	O
max	Method
-	Method
pooling	Method
layer	Method
is	O
fed	O
to	O
a	O
fully	Method
-	Method
connected	Method
layer	Method
with	O
500	O
neurons	O
with	O
a	O
rectified	O
linear	O
unit	O
(	O
ReLU	Method
)	Method
activation	Method
,	O
followed	O
by	O
softmax	O
output	O
.	O
The	O
output	O
of	O
the	O
penultimate	Method
fully	Method
-	Method
connected	Method
layer	Method
is	O
used	O
as	O
the	O
textual	O
feature	O
.	O
The	O
translation	Method
of	Method
convolution	Method
filter	Method
over	O
makes	O
the	O
CNN	Method
learn	O
abstract	O
features	O
and	O
with	O
each	O
subsequent	O
layer	O
the	O
context	O
of	O
the	O
features	O
expands	O
further	O
.	O
subsubsection	Method
:	O
Audio	Method
Feature	Method
Extraction	Method
The	O
audio	Method
feature	Method
extraction	Method
process	Method
is	O
performed	O
at	O
30	O
Hz	O
frame	O
rate	O
with	O
100	O
ms	O
sliding	O
window	O
.	O
We	O
use	O
openSMILE	Method
,	O
which	O
is	O
capable	O
of	O
automatic	Task
pitch	Task
and	Task
voice	Task
intensity	Task
extraction	Task
,	O
for	O
audio	Task
feature	Task
extraction	Task
.	O
Prior	O
to	O
feature	Task
extraction	Task
audio	Task
signals	Task
are	O
processed	O
with	O
voice	Method
intensity	Method
thresholding	Method
and	O
voice	Method
normalization	Method
.	O
Specifically	O
,	O
we	O
use	O
Z	Method
-	Method
standardization	Method
for	O
voice	Task
normalization	Task
.	O
In	O
order	O
to	O
filter	O
out	O
audio	O
segments	O
without	O
voice	O
,	O
we	O
threshold	O
voice	O
intensity	O
.	O
OpenSMILE	Method
is	O
used	O
to	O
perform	O
both	O
these	O
steps	O
.	O
Using	O
openSMILE	Method
we	O
extract	O
several	O
Low	Method
Level	Method
Descriptors	Method
(	O
LLD	Method
)	O
(	O
e.g.	O
,	O
pitch	O
,	O
voice	O
intensity	O
)	O
and	O
various	O
statistical	Method
functionals	Method
of	O
them	O
(	O
e.g.	O
,	O
amplitude	Metric
mean	Metric
,	O
arithmetic	Metric
mean	Metric
,	O
root	Metric
quadratic	Metric
mean	Metric
,	O
standard	Metric
deviation	Metric
,	O
flatness	O
,	O
skewness	O
,	O
kurtosis	O
,	O
quartiles	O
,	O
inter	O
-	O
quartile	O
ranges	O
,	O
and	O
linear	O
regression	O
slope	O
)	O
.	O
“	O
IS13	O
-	O
ComParE	O
”	O
configuration	O
file	O
of	O
openSMILE	O
is	O
used	O
to	O
for	O
our	O
purposes	O
.	O
Finally	O
,	O
we	O
extracted	O
total	O
6392	O
features	O
from	O
each	O
input	O
audio	O
segment	O
.	O
subsubsection	O
:	O
Visual	Task
Feature	Task
Extraction	Task
To	O
extract	O
visual	O
features	O
,	O
we	O
focus	O
not	O
only	O
on	O
feature	Task
extraction	Task
from	O
each	O
video	O
frame	O
but	O
also	O
try	O
to	O
model	O
temporal	O
features	O
across	O
frames	O
.	O
To	O
achieve	O
this	O
,	O
we	O
use	O
3D	Method
-	Method
CNN	Method
on	O
the	O
video	O
.	O
3D	Method
-	Method
CNNs	Method
have	O
been	O
successful	O
in	O
the	O
past	O
,	O
specially	O
in	O
the	O
field	O
of	O
object	Task
classification	Task
on	O
3D	O
data	O
.	O
Its	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
such	O
tasks	O
motivates	O
its	O
use	O
in	O
this	O
paper	O
.	O
Let	O
the	O
video	O
be	O
called	O
,	O
where	O
represents	O
the	O
three	O
RGB	O
channels	O
of	O
an	O
image	O
and	O
denote	O
the	O
cardinality	O
,	O
height	O
,	O
and	O
width	O
of	O
the	O
frames	O
,	O
respectively	O
.	O
A	O
3D	Method
convolutional	Method
filter	Method
,	O
named	O
,	O
is	O
applied	O
to	O
this	O
video	O
,	O
where	O
,	O
similar	O
to	O
a	O
2D	Method
-	Method
CNN	Method
,	O
the	O
filter	O
translates	O
across	O
the	O
video	O
and	O
generates	O
the	O
convolution	O
output	O
.	O
Here	O
,	O
denote	O
number	O
of	O
feature	O
maps	O
,	O
depth	O
of	O
filter	O
,	O
height	O
of	O
filter	O
,	O
and	O
width	O
of	O
filter	O
,	O
respectively	O
.	O
Finally	O
,	O
we	O
apply	O
max	Method
-	Method
pooling	Method
operation	Method
to	O
the	O
,	O
which	O
selects	O
the	O
most	O
relevant	O
features	O
.	O
This	O
operation	O
is	O
applied	O
only	O
to	O
the	O
last	O
three	O
dimensions	O
of	O
.	O
This	O
is	O
followed	O
by	O
a	O
dense	Method
layer	Method
and	O
softmax	Method
computation	Method
.	O
The	O
activations	O
of	O
this	O
layer	O
is	O
used	O
as	O
the	O
overall	O
video	O
features	O
for	O
each	O
utterance	O
video	O
.	O
In	O
our	O
experiments	O
,	O
we	O
receive	O
the	O
best	O
results	O
with	O
filter	O
dimensions	O
and	O
.	O
Also	O
,	O
for	O
the	O
max	Task
-	Task
pooling	Task
,	O
we	O
set	O
the	O
window	O
size	O
as	O
and	O
the	O
succeeding	O
dense	O
layer	O
with	O
neurons	O
.	O
subsection	O
:	O
Context	Method
Modeling	Method
Utterances	O
in	O
the	O
videos	O
are	O
semantically	O
dependent	O
on	O
each	O
other	O
.	O
In	O
other	O
words	O
,	O
complete	O
meaning	O
of	O
an	O
utterance	O
may	O
be	O
determined	O
by	O
taking	O
preceding	O
utterances	O
into	O
consideration	O
.	O
We	O
call	O
this	O
the	O
context	O
of	O
an	O
utterance	O
.	O
Following	O
,	O
we	O
use	O
RNN	Method
,	O
specifically	O
GRU	Method
to	O
model	O
semantic	O
dependency	O
among	O
the	O
utterances	O
in	O
a	O
video	O
.	O
Let	O
the	O
following	O
items	O
represent	O
unimodal	O
features	O
:	O
where	O
maximum	O
number	O
of	O
utterances	O
in	O
a	O
video	O
.	O
We	O
pad	O
the	O
shorter	O
videos	O
with	O
dummy	O
utterances	O
represented	O
by	O
null	O
vectors	O
of	O
corresponding	O
length	O
.	O
For	O
each	O
modality	O
,	O
we	O
feed	O
the	O
unimodal	O
utterance	O
features	O
(	O
where	O
)	O
(	O
discussed	O
in	O
UFE	Method
)	O
of	O
a	O
video	O
to	O
with	O
output	O
size	O
,	O
which	O
is	O
defined	O
as	O
where	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
and	O
.	O
This	O
yields	O
hidden	O
outputs	O
as	O
context	O
-	O
aware	O
unimodal	O
features	O
for	O
each	O
modality	O
.	O
Hence	O
,	O
we	O
define	O
,	O
where	O
.	O
Thus	O
,	O
the	O
context	O
-	O
aware	O
multimodal	O
features	O
can	O
be	O
defined	O
as	O
subsection	O
:	O
Multimodal	Task
Fusion	Task
In	O
this	O
section	O
,	O
we	O
use	O
context	O
-	O
aware	O
unimodal	O
features	O
and	O
to	O
a	O
unified	O
feature	O
space	O
.	O
The	O
unimodal	O
features	O
may	O
have	O
different	O
dimensions	O
,	O
i.e.	O
,	O
.	O
Thus	O
,	O
we	O
map	O
them	O
to	O
the	O
same	O
dimension	O
,	O
say	O
(	O
we	O
obtained	O
best	O
results	O
with	O
)	O
,	O
using	O
fully	Method
-	Method
connected	Method
layer	Method
as	O
follows	O
:	O
where	O
,	O
,	O
,	O
,	O
,	O
and	O
.	O
We	O
can	O
represent	O
the	O
mapping	O
for	O
each	O
dimension	O
as	O
where	O
and	O
are	O
scalars	O
for	O
all	O
and	O
.	O
Also	O
,	O
in	O
the	O
rows	O
represent	O
the	O
utterances	O
and	O
the	O
columns	O
the	O
feature	O
values	O
.	O
We	O
can	O
see	O
these	O
values	O
as	O
more	O
abstract	O
feature	O
values	O
derived	O
from	O
fundamental	O
feature	O
values	O
(	O
which	O
are	O
the	O
components	O
of	O
,	O
,	O
and	O
)	O
.	O
For	O
example	O
,	O
an	O
abstract	O
feature	O
can	O
be	O
the	O
angriness	O
of	O
a	O
speaker	O
in	O
a	O
video	O
.	O
We	O
can	O
infer	O
the	O
degree	O
of	O
angriness	O
from	O
visual	O
features	O
(	O
;	O
facial	O
muscle	O
movements	O
)	O
,	O
acoustic	O
features	O
(	O
,	O
such	O
as	O
pitch	O
and	O
raised	O
voice	O
)	O
,	O
or	O
textual	O
features	O
(	O
,	O
such	O
as	O
the	O
language	O
and	O
choice	O
of	O
words	O
)	O
.	O
Therefore	O
,	O
the	O
degree	O
of	O
angriness	O
can	O
be	O
represented	O
by	O
,	O
where	O
is	O
,	O
,	O
or	O
,	O
is	O
some	O
fixed	O
integer	O
between	O
and	O
,	O
and	O
is	O
some	O
fixed	O
integer	O
between	O
and	O
.	O
Now	O
,	O
the	O
evaluation	O
of	O
abstract	O
feature	O
values	O
from	O
all	O
the	O
modalities	O
may	O
not	O
have	O
the	O
same	O
merit	O
or	O
may	O
even	O
contradict	O
each	O
other	O
.	O
Hence	O
,	O
we	O
need	O
the	O
network	O
to	O
make	O
comparison	O
among	O
the	O
feature	O
values	O
derived	O
from	O
different	O
modalities	O
to	O
make	O
a	O
more	O
refined	O
evaluation	O
of	O
the	O
degree	Metric
of	Metric
anger	Metric
.	O
To	O
this	O
end	O
,	O
we	O
take	O
each	O
bimodal	Method
combination	Method
(	O
which	O
are	O
audio	O
–	O
video	O
,	O
audio	O
–	O
text	O
,	O
and	O
video	O
–	O
text	O
)	O
at	O
a	O
time	O
and	O
compare	O
and	O
combine	O
each	O
of	O
their	O
respective	O
abstract	O
feature	O
values	O
(	O
i.e.	O
with	O
,	O
with	O
,	O
and	O
with	O
)	O
using	O
fully	Method
-	Method
connected	Method
layers	Method
as	O
follows	O
:	O
where	O
,	O
is	O
scalar	O
,	O
,	O
is	O
scalar	O
,	O
,	O
and	O
is	O
scalar	O
,	O
for	O
all	O
and	O
.	O
We	O
hypothesize	O
that	O
it	O
will	O
enable	O
the	O
network	O
to	O
compare	O
the	O
decisions	O
from	O
each	O
modality	O
against	O
the	O
others	O
and	O
help	O
achieve	O
a	O
better	O
fusion	O
of	O
modalities	O
.	O
paragraph	O
:	O
Bimodal	Method
fusion	Method
bimodal:1bimodal:3	Method
are	O
used	O
for	O
bimodal	Method
fusion	Method
.	O
The	O
bimodal	O
fused	O
features	O
for	O
video	O
–	O
audio	O
,	O
audio	O
–	O
text	O
,	O
video	O
–	O
text	O
are	O
defined	O
as	O
We	O
further	O
employ	O
(	O
sec	O
:	O
context	O
)	O
(	O
)	O
,	O
to	O
incorporate	O
contextual	O
information	O
among	O
the	O
utterances	O
in	O
a	O
video	O
with	O
where	O
,	O
,	O
and	O
are	O
context	O
-	O
aware	O
bimodal	O
features	O
represented	O
as	O
vectors	O
and	O
is	O
scalar	O
for	O
,	O
,	O
,	O
and	O
.	O
paragraph	O
:	O
Trimodal	Method
fusion	Method
We	O
combine	O
all	O
three	O
modalities	O
using	O
fully	Method
-	Method
connected	Method
layers	Method
as	O
follows	O
:	O
where	O
and	O
is	O
a	O
scalar	O
for	O
all	O
and	O
.	O
So	O
,	O
we	O
define	O
the	O
fused	O
features	O
as	O
where	O
,	O
is	O
scalar	O
for	O
and	O
.	O
Similarly	O
to	O
bimodal	Method
fusion	Method
(	O
sec	O
:	O
bimodal	Method
)	O
,	O
after	O
trimodal	Method
fusion	Method
we	O
pass	O
the	O
fused	O
features	O
through	O
to	O
incorporate	O
contextual	O
information	O
in	O
them	O
,	O
which	O
yields	O
where	O
,	O
is	O
scalar	O
for	O
,	O
,	O
,	O
and	O
is	O
the	O
context	O
-	O
aware	O
trimodal	O
feature	O
vector	O
.	O
subsection	O
:	O
Classification	Task
In	O
order	O
to	O
perform	O
classification	Task
,	O
we	O
feed	O
the	O
fused	O
features	O
(	O
where	O
and	O
)	O
to	O
a	O
softmax	Method
layer	Method
with	O
outputs	O
.	O
The	O
classifier	Method
can	O
be	O
described	O
as	O
follows	O
:	O
where	O
,	O
,	O
,	O
class	O
value	O
(	O
or	O
)	O
,	O
and	O
estimated	O
class	O
value	O
.	O
subsection	O
:	O
Training	O
We	O
employ	O
categorical	Metric
cross	Metric
-	Metric
entropy	Metric
as	O
loss	O
function	O
(	O
)	O
for	O
training	O
,	O
where	O
number	O
of	O
samples	O
,	O
index	O
of	O
a	O
sample	O
,	O
class	O
value	O
,	O
and	O
Adam	Method
is	O
used	O
as	O
optimizer	O
due	O
to	O
its	O
ability	O
to	O
adapt	O
learning	O
rate	O
for	O
each	O
parameter	O
individually	O
.	O
We	O
train	O
the	O
network	O
for	O
200	O
epochs	O
with	O
early	O
stopping	O
,	O
where	O
we	O
optimize	O
the	O
parameter	O
set	O
where	O
,	O
,	O
and	O
.	O
algorithm	O
summarizes	O
our	O
method	O
.	O
[	O
!	O
ht	O
]	O
{	O
algorithmic}	O
[	O
1	O
]	O
TrainAndTestModelU	O
,	O
V	O
=	O
train	O
set	O
,	O
V	O
=	O
test	O
set	O
feature	Method
extraction	Method
:	O
baseline	O
features	O
∈{A	O
,	O
V	O
,	O
T	O
}	O
=	O
⁢GRUm	O
(	O
fm	O
)	O
equalization	O
bimodal	Method
fusion	Method
∈{⁢VA	O
,	O
⁢AT	O
,	O
⁢VT	O
}	O
=	O
⁢GRUm	O
(	O
fm	O
)	O
fusion	O
=	O
⁢GRU⁢AVT	O
(	O
f⁢AVT	O
)	O
i:	O
[	O
1	O
,	O
N	O
]	O
softmax	Method
classification	Method
MapToSpacexz	Method
modality	Method
z	O
gz	O
BimodalFusiongz1	O
,	O
gz2	O
modality	O
z1	O
and	O
z2	O
,	O
where	O
≠z1z2	O
f⁢z1z2	O
TrimodalFusionfz1	O
,	O
fz2	O
,	O
fz3	O
modality	O
combination	O
z1	O
,	O
z2	O
,	O
and	O
z3	O
,	O
where	O
z1≠z2≠z3	O
f⁢z1z2z3	O
TestModelV	O
to	O
training	O
phase	O
,	O
V	O
is	O
passed	O
through	O
the	O
learnt	O
models	O
to	O
get	O
the	O
features	O
and	O
classification	O
outputs	O
.	O
mentions	O
the	O
trainable	O
parameters	O
(	O
θ	O
)	O
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Dataset	O
Details	O
Most	O
research	O
works	O
in	O
multimodal	Task
sentiment	Task
analysis	Task
are	O
performed	O
on	O
datasets	O
where	O
train	O
and	O
test	O
splits	O
may	O
share	O
certain	O
speakers	O
.	O
Since	O
,	O
each	O
individual	O
has	O
an	O
unique	O
way	O
of	O
expressing	O
emotions	O
and	O
sentiments	O
,	O
finding	O
generic	O
and	O
person	O
-	O
independent	O
features	O
for	O
sentiment	Task
analysis	Task
is	O
crucial	O
.	O
tab	O
:	O
dataset	O
shows	O
the	O
train	O
and	O
test	O
split	O
for	O
the	O
datasets	O
used	O
.	O
subsubsection	O
:	O
CMU	Material
-	Material
MOSI	Material
CMU	Material
-	Material
MOSI	Material
dataset	Material
is	O
rich	O
in	O
sentimental	O
expressions	O
,	O
where	O
89	O
people	O
review	O
various	O
topics	O
in	O
English	O
.	O
The	O
videos	O
are	O
segmented	O
into	O
utterances	O
where	O
each	O
utterance	O
is	O
annotated	O
with	O
scores	O
between	O
(	O
strongly	O
negative	O
)	O
and	O
(	O
strongly	O
positive	O
)	O
by	O
five	O
annotators	O
.	O
We	O
took	O
the	O
average	O
of	O
these	O
five	O
annotations	O
as	O
the	O
sentiment	O
polarity	O
and	O
considered	O
only	O
two	O
classes	O
(	O
positive	O
and	O
negative	O
)	O
.	O
Given	O
every	O
individual	O
’s	O
unique	O
way	O
of	O
expressing	O
sentiments	O
,	O
real	O
world	O
applications	O
should	O
be	O
able	O
to	O
model	O
generic	O
person	O
independent	O
features	O
and	O
be	O
robust	O
to	O
person	O
variance	O
.	O
To	O
this	O
end	O
,	O
we	O
perform	O
person	O
-	O
independent	O
experiments	O
to	O
emulate	O
unseen	O
conditions	O
.	O
Our	O
train	O
/	O
test	O
splits	O
of	O
the	O
dataset	O
are	O
completely	O
disjoint	O
with	O
respect	O
to	O
speakers	O
.	O
The	O
train	O
/	O
validation	O
set	O
consists	O
of	O
the	O
first	O
62	O
individuals	O
in	O
the	O
dataset	O
.	O
The	O
test	O
set	O
contains	O
opinionated	O
videos	O
by	O
rest	O
of	O
the	O
31	O
speakers	O
.	O
In	O
particular	O
,	O
1447	O
and	O
752	O
utterances	O
are	O
used	O
for	O
training	O
and	O
test	O
respectively	O
.	O
subsubsection	O
:	O
IEMOCAP	Material
IEMOCAP	Material
contains	O
two	O
way	O
conversations	O
among	O
ten	O
speakers	O
,	O
segmented	O
into	O
utterances	O
.	O
The	O
utterances	O
are	O
tagged	O
with	O
the	O
labels	O
anger	O
,	O
happiness	O
,	O
sadness	O
,	O
neutral	O
,	O
excitement	O
,	O
frustration	O
,	O
fear	O
,	O
surprise	O
,	O
and	O
other	O
.	O
We	O
consider	O
the	O
first	O
four	O
ones	O
to	O
compare	O
with	O
the	O
state	O
of	O
the	O
art	O
and	O
other	O
works	O
.	O
It	O
contains	O
1083	O
angry	O
,	O
1630	O
happy	O
,	O
1083	O
sad	O
,	O
and	O
1683	O
neutral	O
videos	O
.	O
Only	O
the	O
videos	O
by	O
the	O
first	O
eight	O
speakers	O
are	O
considered	O
for	O
training	O
.	O
subsection	O
:	O
Baselines	O
We	O
compare	O
our	O
method	O
with	O
the	O
following	O
strong	O
baselines	O
.	O
paragraph	O
:	O
Early	Task
fusion	Task
We	O
extract	O
unimodal	O
features	O
(	O
UFE	O
)	O
and	O
simply	O
concatenate	O
them	O
to	O
produce	O
multimodal	O
features	O
.	O
Followed	O
by	O
support	Method
vector	Method
machine	Method
(	O
SVM	Method
)	O
being	O
applied	O
on	O
this	O
feature	O
vector	O
for	O
the	O
final	O
sentiment	Task
classification	Task
.	O
paragraph	O
:	O
Method	O
from	O
We	O
have	O
implemented	O
and	O
compared	O
our	O
method	O
with	O
the	O
approach	O
proposed	O
by	O
.	O
In	O
their	O
approach	O
,	O
they	O
extracted	O
visual	O
features	O
using	O
CLM	Method
-	Method
Z	Method
,	O
audio	O
features	O
using	O
openSMILE	Method
,	O
and	O
textual	O
features	O
using	O
CNN	Method
.	O
MKL	Method
was	O
then	O
applied	O
to	O
the	O
features	O
obtained	O
from	O
concatenation	O
of	O
the	O
unimodal	O
features	O
.	O
However	O
,	O
they	O
did	O
not	O
conduct	O
speaker	O
independent	O
experiments	O
.	O
In	O
order	O
to	O
perform	O
a	O
fair	O
comparison	O
with	O
,	O
we	O
employ	O
our	O
fusion	Method
method	Method
on	O
the	O
features	O
extracted	O
by	O
.	O
paragraph	O
:	O
Method	O
from	O
We	O
have	O
compared	O
our	O
method	O
with	O
,	O
which	O
takes	O
advantage	O
of	O
contextual	O
information	O
obtained	O
from	O
the	O
surrounding	O
utterances	O
.	O
This	O
context	Method
modeling	Method
is	O
achieved	O
using	O
LSTM	Method
.	O
We	O
reran	O
the	O
experiments	O
of	O
without	O
using	O
SVM	Method
for	O
classification	Task
since	O
using	O
SVM	Method
with	Method
neural	Method
networks	Method
is	O
usually	O
discouraged	O
.	O
This	O
provides	O
a	O
fair	O
comparison	O
with	O
our	O
model	O
which	O
does	O
not	O
use	O
SVM	Method
.	O
paragraph	O
:	O
Method	O
from	O
In	O
,	O
they	O
proposed	O
a	O
trimodal	Method
fusion	Method
method	O
based	O
on	O
the	O
tensors	O
.	O
We	O
have	O
also	O
compared	O
our	O
method	O
with	O
their	O
.	O
In	O
particular	O
,	O
their	O
dataset	O
configuration	O
was	O
different	O
than	O
us	O
so	O
we	O
have	O
adapted	O
their	O
publicly	O
available	O
code	O
and	O
employed	O
that	O
on	O
our	O
dataset	O
.	O
subsection	O
:	O
Experimental	O
Setting	O
We	O
considered	O
two	O
variants	O
of	O
experimental	O
setup	O
while	O
evaluating	O
our	O
model	O
.	O
paragraph	O
:	O
HFusion	Method
In	O
this	O
setup	O
,	O
we	O
evaluated	O
hierarchical	Method
fusion	Method
without	O
context	O
-	O
aware	O
features	O
with	O
CMU	Material
-	Material
MOSI	Material
dataset	Material
.	O
We	O
removed	O
all	O
the	O
GRUs	O
from	O
the	O
model	O
described	O
in	O
sec	O
:	O
mul_fusion	Method
,	O
sec	O
:	O
context	O
forwarded	O
utterance	O
specific	O
features	O
directly	O
to	O
the	O
next	O
layer	O
.	O
This	O
setup	O
is	O
depicted	O
in	O
fig	O
:	O
hfusion	Method
-	O
trimodal	O
.	O
paragraph	O
:	O
CHFusion	Method
This	O
setup	O
is	O
exactly	O
as	O
the	O
model	O
described	O
in	O
sec	O
:	O
model	O
.	O
subsection	O
:	O
Results	O
and	O
Discussion	O
We	O
discuss	O
the	O
results	O
for	O
the	O
different	O
experimental	O
settings	O
discussed	O
in	O
sec	O
:	O
exp_set	O
.	O
subsubsection	O
:	O
Hierarchical	Method
Fusion	Method
(	O
HFusion	Method
)	O
The	O
results	O
of	O
our	O
experiments	O
are	O
presented	O
in	O
table	O
:	O
hfusion	Method
.	O
We	O
evaluated	O
this	O
setup	O
with	O
CMU	Material
-	Material
MOSI	Material
dataset	Material
(	O
sec	O
:	O
mosi	Material
)	O
and	O
two	O
feature	O
sets	O
:	O
the	O
feature	O
set	O
used	O
in	O
and	O
the	O
set	O
of	O
unimodal	O
features	O
discussed	O
in	O
UFE	Method
.	O
Our	O
model	O
outperformed	O
,	O
which	O
employed	O
MKL	Method
,	O
for	O
all	O
bimodal	O
and	O
trimodal	O
scenarios	O
by	O
a	O
margin	O
of	O
1–1.8	O
%	O
.	O
This	O
leads	O
us	O
to	O
present	O
two	O
observations	O
.	O
Firstly	O
,	O
the	O
features	O
used	O
in	O
are	O
inferior	O
to	O
the	O
features	O
extracted	O
in	O
our	O
approach	O
.	O
Second	O
,	O
our	O
hierarchical	Method
fusion	Method
method	Method
is	O
better	O
than	O
their	O
fusion	Method
method	Method
.	O
It	O
is	O
already	O
established	O
in	O
the	O
literature	O
that	O
multimodal	Method
analysis	Method
outperforms	O
unimodal	Method
analysis	Method
.	O
We	O
also	O
observe	O
the	O
same	O
trend	O
in	O
our	O
experiments	O
where	O
trimodal	Method
and	Method
bimodal	Method
classifiers	Method
outperform	O
unimodal	Method
classifiers	Method
.	O
The	O
textual	Method
modality	Method
performed	O
best	O
among	O
others	O
with	O
a	O
higher	O
unimodal	O
classification	O
accuracy	Metric
of	O
75	O
%	O
.	O
Although	O
other	O
modalities	O
contribute	O
to	O
improve	O
the	O
performance	O
of	O
multimodal	Method
classifiers	Method
,	O
that	O
contribution	O
is	O
little	O
in	O
compare	O
to	O
the	O
textual	O
modality	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
compared	O
our	O
model	O
with	O
early	Method
fusion	Method
(	O
early	Method
-	Method
fusion	Method
)	O
for	O
aforementioned	O
feature	O
sets	O
(	O
UFE	O
)	O
.	O
Our	O
fusion	Method
mechanism	Method
consistently	O
outperforms	O
early	Method
fusion	Method
for	O
all	O
combination	O
of	O
modalities	O
.	O
This	O
supports	O
our	O
hypothesis	O
that	O
our	O
hierarchical	Method
fusion	Method
method	Method
captures	O
the	O
inter	O
-	O
relation	O
among	O
the	O
modalities	O
and	O
produce	O
better	O
performance	O
vector	O
than	O
early	Method
fusion	Method
.	O
Text	O
is	O
the	O
strongest	O
individual	O
modality	O
,	O
and	O
we	O
observe	O
that	O
the	O
text	O
modality	O
paired	O
with	O
remaining	O
two	O
modalities	O
results	O
in	O
consistent	O
performance	O
improvement	O
.	O
Overall	O
,	O
the	O
results	O
give	O
a	O
strong	O
indication	O
that	O
the	O
comparison	O
among	O
the	O
abstract	O
feature	O
values	O
dampens	O
the	O
effect	O
of	O
less	O
important	O
modalities	O
,	O
which	O
was	O
our	O
hypothesis	O
.	O
For	O
example	O
,	O
we	O
can	O
notice	O
that	O
for	O
early	Method
fusion	Method
T	O
+	O
V	O
and	O
T	Method
+	Method
A	Method
both	O
yield	O
the	O
same	O
performance	O
.	O
However	O
,	O
with	O
our	O
method	O
text	O
with	O
video	O
performs	O
better	O
than	O
text	O
with	O
audio	O
,	O
which	O
is	O
more	O
aligned	O
with	O
our	O
expectations	O
,	O
since	O
facial	O
muscle	O
movements	O
usually	O
carry	O
more	O
emotional	O
nuances	O
than	O
voice	O
.	O
In	O
particular	O
,	O
we	O
observe	O
that	O
our	O
model	O
outperformed	O
all	O
the	O
strong	O
baselines	O
mentioned	O
above	O
.	O
The	O
method	O
by	O
is	O
only	O
able	O
to	O
fuse	O
using	O
concatenation	Method
.	O
Our	O
proposed	O
method	O
outperformed	O
their	O
approach	O
by	O
a	O
significant	O
margin	O
;	O
thanks	O
to	O
the	O
power	O
of	O
hierarchical	Method
fusion	Method
which	O
proves	O
the	O
capability	O
of	O
our	O
method	O
in	O
modeling	O
bimodal	Task
and	Task
trimodal	Task
correlations	Task
.	O
However	O
on	O
the	O
other	O
hand	O
,	O
the	O
method	O
by	O
is	O
capable	O
of	O
fusing	O
the	O
modalities	O
using	O
a	O
tensor	O
.	O
Interestingly	O
our	O
method	O
also	O
outperformed	O
them	O
and	O
we	O
think	O
the	O
reason	O
is	O
the	O
capability	O
of	O
bimodal	Method
fusion	Method
and	O
use	O
that	O
for	O
trimodal	Method
fusion	Method
.	O
Tensor	Method
fusion	Method
network	Method
is	O
incapable	O
to	O
learn	O
the	O
weights	O
of	O
the	O
bimodal	O
and	O
trimodal	O
correlations	O
in	O
the	O
fusion	Task
.	O
Tensor	Task
Fusion	Task
is	O
mathematically	O
formed	O
by	O
an	O
outer	Method
product	Method
,	O
it	O
has	O
no	O
learn	O
-	O
able	O
parameters	O
.	O
Wherein	O
our	O
method	O
learns	O
the	O
weights	O
automatically	O
using	O
a	O
neural	Method
network	Method
(	O
Equation	O
1	O
,	O
2	O
and	O
3	O
)	O
.	O
subsubsection	O
:	O
Context	Method
-	Method
Aware	Method
Hierarchical	Method
Fusion	Method
(	O
CHFusion	Method
)	O
The	O
results	O
of	O
this	O
experiment	O
are	O
shown	O
in	O
table	O
:	O
chfusion	Method
.	O
This	O
setting	O
fully	O
utilizes	O
the	O
model	O
described	O
in	O
sec	O
:	O
model	O
.	O
We	O
applied	O
this	O
experimental	O
setting	O
for	O
two	O
datasets	O
,	O
namely	O
CMU	Material
-	Material
MOSI	Material
(	O
sec	O
:	O
mosi	Material
)	O
and	O
IEMOCAP	Material
(	O
sec	O
:	O
iemocap	Material
)	O
.	O
We	O
used	O
the	O
feature	O
set	O
discussed	O
in	O
UFE	Method
,	O
which	O
was	O
also	O
used	O
by	O
.	O
As	O
expected	O
our	O
method	O
outperformed	O
the	O
simple	O
early	Method
fusion	Method
based	Method
fusion	Method
by	O
,	O
tensor	Method
fusion	Method
by	O
.	O
The	O
method	O
by	O
used	O
a	O
scheme	O
to	O
learn	O
contextual	O
features	O
from	O
the	O
surrounding	O
features	O
.	O
However	O
,	O
as	O
a	O
method	O
of	O
fusion	Task
they	O
adapted	O
simple	O
concatenation	O
based	O
fusion	Method
method	Method
by	O
.	O
As	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
employed	O
their	O
contextual	Method
feature	Method
extraction	Method
framework	Method
and	O
integrated	O
our	O
proposed	O
fusion	Method
method	Method
to	O
that	O
.	O
This	O
has	O
helped	O
us	O
to	O
outperform	O
by	O
significant	O
margin	O
thanks	O
to	O
the	O
hierarchical	Method
fusion	Method
(	O
HFusion	Method
)	O
.	O
paragraph	O
:	O
CMU	Material
-	Material
MOSI	Material
We	O
achieve	O
1–2	O
%	O
performance	O
improvement	O
over	O
the	O
state	O
of	O
the	O
art	O
for	O
all	O
the	O
modality	O
combinations	O
having	O
textual	Method
component	Method
.	O
For	O
A	O
+	O
V	O
modality	Task
combination	Task
we	O
achieve	O
better	O
but	O
similar	O
performance	O
to	O
the	O
state	O
of	O
the	O
art	O
.	O
We	O
suspect	O
that	O
it	O
is	O
due	O
to	O
both	O
audio	O
and	O
video	O
modality	O
being	O
significantly	O
less	O
informative	O
than	O
textual	O
modality	O
.	O
It	O
is	O
evident	O
from	O
the	O
unimodal	O
performance	O
where	O
we	O
observe	O
that	O
textual	O
modality	O
on	O
its	O
own	O
performs	O
around	O
21	O
%	O
better	O
than	O
both	O
audio	O
and	O
video	O
modality	O
.	O
Also	O
,	O
audio	O
and	O
video	O
modality	O
performs	O
close	O
to	O
majority	O
baseline	O
.	O
On	O
the	O
other	O
hand	O
,	O
it	O
is	O
important	O
to	O
notice	O
that	O
with	O
all	O
modalities	O
combined	O
we	O
achieve	O
about	O
3.5	O
%	O
higher	O
accuracy	Metric
than	O
text	O
alone	O
.	O
For	O
example	O
,	O
consider	O
the	O
following	O
utterance	O
:	O
so	O
overall	O
new	O
moon	O
even	O
with	O
the	O
bigger	O
better	O
budgets	O
huh	O
it	O
was	O
still	O
too	O
long	O
.	O
The	O
speaker	O
discusses	O
her	O
opinion	O
on	O
the	O
movie	O
Twilight	O
New	O
Moon	O
.	O
Textually	O
the	O
utterance	O
is	O
abundant	O
with	O
positive	O
words	O
however	O
audio	O
and	O
video	O
comprises	O
of	O
a	O
frown	O
which	O
is	O
observed	O
by	O
the	O
hierarchical	Method
fusion	Method
based	O
model	O
.	O
paragraph	O
:	O
IEMOCAP	Material
As	O
the	O
IEMOCAP	Material
dataset	Material
contains	O
four	O
distinct	O
emotion	O
categories	O
,	O
in	O
the	O
last	O
layer	O
of	O
the	O
network	O
we	O
used	O
a	O
softmax	Method
classifier	Method
whose	O
output	O
dimension	O
is	O
set	O
to	O
4	O
.	O
In	O
order	O
to	O
perform	O
classification	Task
on	O
IEMOCAP	Material
dataset	Material
we	O
feed	O
the	O
fused	O
features	O
(	O
where	O
and	O
)	O
to	O
a	O
softmax	Method
layer	Method
with	O
outputs	O
.	O
The	O
classifier	Method
can	O
be	O
described	O
as	O
follows	O
:	O
where	O
,	O
,	O
,	O
class	O
value	O
(	O
or	O
or	O
or	O
)	O
,	O
and	O
estimated	O
class	O
value	O
.	O
Here	O
as	O
well	O
,	O
we	O
achieve	O
performance	O
improvement	O
consistent	O
with	O
CMU	Material
-	Material
MOSI	Material
.	O
This	O
method	O
performs	O
1–2.4	O
%	O
better	O
than	O
the	O
state	O
of	O
the	O
art	O
for	O
all	O
the	O
modality	O
combinations	O
.	O
Also	O
,	O
trimodal	O
accuracy	Metric
is	O
3	O
%	O
higher	O
than	O
the	O
same	O
for	O
textual	O
modality	O
.	O
Since	O
,	O
IEMOCAP	Material
dataset	O
imbalanced	O
,	O
we	O
also	O
present	O
the	O
f	Metric
-	Metric
score	Metric
for	O
each	O
modality	O
combination	O
for	O
a	O
better	O
evaluation	O
.	O
One	O
key	O
observation	O
for	O
IEMOCAP	Material
dataset	Material
is	O
that	O
its	O
A	Method
+	Method
V	Method
modality	Method
combination	Method
performs	O
significantly	O
better	O
than	O
the	O
same	O
of	O
CMU	Material
-	Material
MOSI	Material
dataset	Material
.	O
We	O
think	O
that	O
this	O
is	O
due	O
to	O
the	O
audio	O
and	O
video	O
modality	O
of	O
IEMOCAP	Material
being	O
richer	O
than	O
the	O
same	O
of	O
CMU	Material
-	Material
MOSI	Material
.	O
The	O
performance	O
difference	O
with	O
another	O
strong	O
baseline	O
is	O
even	O
more	O
ranging	O
from	O
2.1	O
%	O
to	O
3	O
%	O
on	O
CMU	Material
-	Material
MOSI	Material
dataset	Material
and	O
2.2	O
%	O
to	O
5	O
%	O
on	O
IEMOCAP	Material
dataset	Material
.	O
This	O
again	O
confirms	O
the	O
superiority	O
of	O
the	O
hierarchical	Method
fusion	Method
in	O
compare	O
to	O
.	O
We	O
think	O
this	O
is	O
mainly	O
because	O
of	O
learning	O
the	O
weights	O
of	O
bimodal	O
and	O
trimodal	O
correlation	O
(	O
representing	O
the	O
degree	O
of	O
correlations	O
)	O
calculations	O
at	O
the	O
time	O
of	O
fusion	Task
while	O
Tensor	Method
Fusion	Method
Network	Method
(	O
TFN	Method
)	O
just	O
relies	O
on	O
the	O
non	O
-	O
trainable	O
outer	Method
product	Method
of	Method
tensors	Method
to	O
model	O
such	O
correlations	O
for	O
fusion	Task
.	O
Additionally	O
,	O
we	O
present	O
class	O
-	O
wise	O
accuracy	Metric
and	O
f	Metric
-	Metric
score	Metric
for	O
IEMOCAP	Material
for	O
trimodal	Task
(	O
A	O
+	O
V	O
+	O
T	O
)	O
scenario	O
in	O
table	O
:	O
iemocap	O
-	O
classwise	O
.	O
subsubsection	Method
:	O
HFusion	Method
vs.	O
CHFusion	Method
We	O
compare	O
HFusion	Method
and	O
CHFusion	Method
models	Method
over	O
CMU	Material
-	Material
MOSI	Material
dataset	Material
.	O
We	O
observe	O
that	O
CHFusion	Method
performs	O
1–2	O
%	O
better	O
than	O
HFusion	Method
model	Method
for	O
all	O
the	O
modality	O
combinations	O
.	O
This	O
performance	O
boost	O
is	O
achieved	O
by	O
the	O
inclusion	O
of	O
utterance	O
-	O
level	O
contextual	O
information	O
in	O
HFusion	Method
model	Method
by	O
adding	O
GRUs	Method
in	O
different	O
levels	O
of	O
fusion	O
hierarchy	O
.	O
section	O
:	O
Conclusion	O
Multimodal	O
fusion	Method
strategy	Method
is	O
an	O
important	O
issue	O
in	O
multimodal	Task
sentiment	Task
analysis	Task
.	O
However	O
,	O
little	O
work	O
has	O
been	O
done	O
so	O
far	O
in	O
this	O
direction	O
.	O
In	O
this	O
paper	O
,	O
we	O
have	O
presented	O
a	O
novel	O
and	O
comprehensive	O
fusion	Method
strategy	Method
.	O
Our	O
method	O
outperforms	O
the	O
widely	O
used	O
early	Method
fusion	Method
on	O
both	O
datasets	O
typically	O
used	O
to	O
test	O
multimodal	Method
sentiment	Method
analysis	Method
methods	Method
.	O
Moreover	O
,	O
with	O
the	O
addition	O
of	O
context	Method
modeling	Method
with	O
GRU	Method
,	O
our	O
method	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
in	O
multimodal	Task
sentiment	Task
analysis	Task
and	O
emotion	Task
detection	Task
by	O
significant	O
margin	O
.	O
In	O
our	O
future	O
work	O
,	O
we	O
plan	O
to	O
improve	O
the	O
quality	O
of	O
unimodal	O
features	O
,	O
especially	O
textual	O
features	O
,	O
which	O
will	O
further	O
improve	O
the	O
accuracy	Metric
of	O
classification	Task
.	O
We	O
will	O
also	O
experiment	O
with	O
more	O
sophisticated	O
network	Method
architectures	Method
.	O
section	O
:	O
Acknowledgement	O
The	O
work	O
was	O
partially	O
supported	O
by	O
the	O
Instituto	O
Politécnico	O
Nacional	O
via	O
grant	O
SIP	O
20172008	O
to	O
A.	O
Gelbukh	O
.	O
bibliography	O
:	O
References	O
