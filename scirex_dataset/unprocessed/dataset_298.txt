document	O
:	O
Deep	O
Pyramidal	O
Residual	Method
Networks	Method
Deep	Method
convolutional	Method
neural	Method
networks	Method
(	O
DCNNs	Method
)	O
have	O
shown	O
remarkable	O
performance	O
in	O
image	Task
classification	Task
tasks	Task
in	O
recent	O
years	O
.	O
Generally	O
,	O
deep	Method
neural	Method
network	Method
architectures	Method
are	O
stacks	O
consisting	O
of	O
a	O
large	O
number	O
of	O
convolutional	Method
layers	Method
,	O
and	O
they	O
perform	O
downsampling	O
along	O
the	O
spatial	O
dimension	O
via	O
pooling	Method
to	O
reduce	O
memory	O
usage	O
.	O
Concurrently	O
,	O
the	O
feature	O
map	O
dimension	O
(	O
i.e.	O
,	O
the	O
number	O
of	O
channels	O
)	O
is	O
sharply	O
increased	O
at	O
downsampling	O
locations	O
,	O
which	O
is	O
essential	O
to	O
ensure	O
effective	O
performance	O
because	O
it	O
increases	O
the	O
diversity	O
of	O
high	O
-	O
level	O
attributes	O
.	O
This	O
also	O
applies	O
to	O
residual	Method
networks	Method
and	O
is	O
very	O
closely	O
related	O
to	O
their	O
performance	O
.	O
In	O
this	O
research	O
,	O
instead	O
of	O
sharply	O
increasing	O
the	O
feature	O
map	O
dimension	O
at	O
units	O
that	O
perform	O
downsampling	O
,	O
we	O
gradually	O
increase	O
the	O
feature	O
map	O
dimension	O
at	O
all	O
units	O
to	O
involve	O
as	O
many	O
locations	O
as	O
possible	O
.	O
This	O
design	O
,	O
which	O
is	O
discussed	O
in	O
depth	O
together	O
with	O
our	O
new	O
insights	O
,	O
has	O
proven	O
to	O
be	O
an	O
effective	O
means	O
of	O
improving	O
generalization	Metric
ability	Metric
.	O
Furthermore	O
,	O
we	O
propose	O
a	O
novel	O
residual	Method
unit	Method
capable	O
of	O
further	O
improving	O
the	O
classification	Metric
accuracy	Metric
with	O
our	O
new	O
network	Method
architecture	Method
.	O
Experiments	O
on	O
benchmark	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
ImageNet	Material
datasets	O
have	O
shown	O
that	O
our	O
network	Method
architecture	Method
has	O
superior	O
generalization	Metric
ability	Metric
compared	O
to	O
the	O
original	O
residual	Method
networks	Method
.	O
Code	O
is	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
jhkim89	O
/	O
PyramidNet	Method
section	O
:	O
Introduction	O
The	O
emergence	O
of	O
deep	Method
convolutional	Method
neural	Method
networks	Method
(	O
DCNNs	Method
)	O
has	O
greatly	O
contributed	O
to	O
advancements	O
in	O
solving	O
complex	Task
tasks	Task
in	O
computer	Task
vision	Task
with	O
significantly	O
improved	O
performance	O
.	O
Since	O
the	O
proposal	O
of	O
LeNet	Method
,	O
which	O
introduced	O
the	O
use	O
of	O
deep	Method
neural	Method
network	Method
architectures	Method
for	O
computer	Task
vision	Task
tasks	Task
,	O
the	O
advanced	O
architecture	O
AlexNet	Method
was	O
selected	O
as	O
the	O
winner	O
of	O
the	O
2012	O
ImageNet	Material
competition	O
by	O
a	O
large	O
margin	O
over	O
traditional	O
methods	O
.	O
Subsequently	O
,	O
ZF	Method
-	Method
net	Method
,	O
VGG	Method
,	O
GoogleNet	Method
,	O
Residual	Method
Networks	Method
,	O
and	O
Inception	Method
Residual	Method
Networks	Method
were	O
successively	O
proposed	O
to	O
demonstrate	O
advances	O
in	O
network	Method
architectures	Method
.	O
In	O
particular	O
,	O
Residual	Method
Networks	Method
(	O
ResNets	Method
)	O
leverage	O
the	O
concept	O
of	O
shortcut	O
connections	O
inside	O
a	O
proposed	O
residual	Method
unit	Method
for	O
residual	Task
learning	Task
,	O
to	O
make	O
it	O
possible	O
to	O
train	O
much	O
deeper	O
network	Method
architectures	Method
.	O
Deeper	Method
network	Method
architectures	Method
are	O
known	O
for	O
their	O
superior	O
performance	O
,	O
and	O
these	O
network	Method
architectures	Method
commonly	O
have	O
deeply	Method
stacked	Method
convolutional	Method
filters	Method
with	O
nonlinearity	O
.	O
With	O
respect	O
to	O
feature	O
map	O
dimension	O
,	O
the	O
conventional	O
method	O
of	O
stacking	O
several	O
convolutional	Method
filters	Method
is	O
to	O
increase	O
the	O
dimension	O
while	O
decreasing	O
the	O
size	O
of	O
feature	O
maps	O
by	O
increasing	O
the	O
strides	O
of	O
the	O
filters	O
or	O
poolings	Method
.	O
This	O
is	O
the	O
widely	O
adopted	O
method	O
of	O
controlling	O
the	O
size	O
of	O
feature	O
maps	O
,	O
because	O
extracting	O
the	O
diversified	O
high	O
-	O
level	O
attributes	O
with	O
the	O
increased	O
feature	O
map	O
dimension	O
is	O
very	O
effective	O
for	O
classification	Task
tasks	Task
.	O
Architectures	O
such	O
as	O
those	O
of	O
AlexNet	Method
and	O
VGG	Method
utilize	O
this	O
method	O
of	O
increasing	O
the	O
feature	O
map	O
dimension	O
to	O
construct	O
their	O
network	Method
architectures	Method
.	O
The	O
most	O
successful	O
deep	Method
neural	Method
network	Method
,	O
ResNets	Method
,	O
which	O
was	O
introduced	O
by	O
He	O
et	O
al	O
.	O
,	O
also	O
follows	O
this	O
approach	O
for	O
filter	Task
stacking	Task
.	O
According	O
to	O
the	O
research	O
of	O
Veit	O
et	O
al	O
.	O
,	O
ResNets	Method
are	O
considered	O
to	O
behave	O
as	O
ensembles	Method
of	Method
relatively	Method
shallow	Method
networks	Method
.	O
These	O
researchers	O
showed	O
that	O
the	O
deletion	O
of	O
an	O
individual	O
residual	O
unit	O
from	O
ResNets	O
,	O
i.e.	O
,	O
such	O
that	O
only	O
a	O
shortcut	O
connection	O
remains	O
,	O
does	O
not	O
significantly	O
affect	O
the	O
overall	O
performance	O
,	O
proving	O
that	O
deleting	O
a	O
residual	O
unit	O
is	O
equivalent	O
to	O
deleting	O
some	O
shallow	Method
networks	Method
in	O
the	O
ensemble	Method
networks	Method
.	O
Contrary	O
to	O
this	O
,	O
deleting	O
a	O
single	O
layer	O
in	O
plain	Method
network	O
architectures	O
such	O
as	O
a	O
VGG	Method
-	Method
network	Method
damages	O
the	O
network	O
by	O
causing	O
additional	O
severe	O
errors	O
.	O
However	O
,	O
in	O
the	O
case	O
of	O
ResNets	Task
,	O
it	O
was	O
found	O
that	O
deleting	O
the	O
building	O
blocks	O
in	O
a	O
residual	O
unit	O
with	O
downsampling	O
,	O
where	O
the	O
feature	O
map	O
dimension	O
is	O
doubled	O
,	O
still	O
increases	O
the	O
classification	Metric
error	Metric
by	O
a	O
significant	O
margin	O
.	O
Interestingly	O
,	O
when	O
the	O
residual	Method
net	Method
is	O
trained	O
using	O
a	O
stochastic	O
depth	O
,	O
it	O
was	O
found	O
that	O
deleting	O
the	O
blocks	O
with	O
downsampling	O
does	O
not	O
degrade	O
the	O
classification	Metric
performance	Metric
,	O
as	O
shown	O
in	O
Figure	O
8	O
in	O
.	O
One	O
may	O
think	O
that	O
this	O
phenomenon	O
is	O
related	O
to	O
the	O
overall	O
improvement	O
in	O
the	O
classification	Metric
performance	Metric
enabled	O
by	O
stochastic	O
depth	O
.	O
Motivated	O
by	O
the	O
ensemble	O
interpretation	O
of	O
residual	Method
networks	Method
in	O
Veit	O
et	O
al	O
.	O
and	O
the	O
results	O
with	O
stochastic	O
depth	O
,	O
we	O
devised	O
another	O
method	O
to	O
handle	O
the	O
phenomenon	O
associated	O
with	O
deleting	O
the	O
downsampling	O
unit	O
.	O
In	O
the	O
proposed	O
method	O
,	O
the	O
feature	O
map	O
dimensions	O
are	O
increased	O
at	O
all	O
layers	O
to	O
distribute	O
the	O
burden	O
concentrated	O
at	O
locations	O
of	O
residual	O
units	O
affected	O
by	O
downsampling	O
,	O
such	O
that	O
it	O
is	O
equally	O
distributed	O
across	O
all	O
units	O
.	O
It	O
was	O
found	O
that	O
using	O
the	O
proposed	O
new	O
network	Method
architecture	Method
,	O
deleting	O
the	O
units	O
with	O
downsampling	O
does	O
not	O
degrade	O
the	O
performance	O
significantly	O
.	O
In	O
our	O
paper	O
,	O
we	O
refer	O
to	O
this	O
network	Method
architecture	Method
as	O
a	O
deep	Method
“	Method
pyramidal	Method
”	Method
network	Method
and	O
a	O
“	O
pyramidal	Method
”	Method
residual	Method
network	Method
with	O
a	O
residual	Method
-	Method
type	Method
network	Method
architecture	Method
.	O
This	O
reflects	O
the	O
fact	O
that	O
the	O
shape	O
of	O
the	O
network	Method
architecture	Method
can	O
be	O
compared	O
to	O
that	O
of	O
a	O
pyramid	O
.	O
That	O
is	O
,	O
the	O
number	O
of	O
channels	O
gradually	O
increases	O
as	O
a	O
function	O
of	O
the	O
depth	O
at	O
which	O
the	O
layer	O
occurs	O
,	O
which	O
is	O
similar	O
to	O
a	O
pyramid	O
structure	O
of	O
which	O
the	O
shape	O
gradually	O
widens	O
from	O
the	O
top	O
downwards	O
.	O
This	O
structure	O
is	O
illustrated	O
in	O
comparison	O
to	O
other	O
network	Method
architectures	Method
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
key	O
contributions	O
are	O
summarized	O
as	O
follows	O
:	O
A	O
deep	O
pyramidal	O
residual	Method
network	Method
(	O
PyramidNet	Method
)	O
is	O
introduced	O
.	O
The	O
key	O
idea	O
is	O
to	O
concentrate	O
on	O
the	O
feature	O
map	O
dimension	O
by	O
increasing	O
it	O
gradually	O
instead	O
of	O
by	O
increasing	O
it	O
sharply	O
at	O
each	O
residual	O
unit	O
with	O
downsampling	O
.	O
In	O
addition	O
,	O
our	O
network	Method
architecture	Method
works	O
as	O
a	O
mixture	O
of	O
both	O
plain	Method
and	O
residual	Method
networks	Method
by	O
using	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcut	Method
connections	Method
when	O
increasing	O
the	O
feature	O
map	O
dimension	O
.	O
A	O
novel	O
residual	Method
unit	Method
is	O
also	O
proposed	O
,	O
which	O
can	O
further	O
improve	O
the	O
performance	O
of	O
ResNet	Method
-	O
based	O
architectures	O
(	O
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
network	Method
architectures	Method
)	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
Section	O
[	O
reference	O
]	O
presents	O
our	O
PyramidNets	Method
and	O
introduces	O
a	O
novel	O
residual	Method
unit	Method
that	O
can	O
further	O
improve	O
ResNet	Method
.	O
Section	O
[	O
reference	O
]	O
closely	O
analyzes	O
our	O
PyramidNets	Method
via	O
several	O
discussions	O
.	O
Section	O
[	O
reference	O
]	O
presents	O
experimental	O
results	O
and	O
comparisons	O
with	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	Method
network	Method
architectures	Method
.	O
Section	O
[	O
reference	O
]	O
concludes	O
our	O
paper	O
with	O
suggestions	O
for	O
future	O
works	O
.	O
section	O
:	O
Network	Method
Architecture	Method
In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
network	Method
architectures	Method
of	O
our	O
PyramidNets	Method
.	O
The	O
major	O
difference	O
between	O
PyramidNets	Method
and	O
other	O
network	Method
architectures	Method
is	O
that	O
the	O
dimension	O
of	O
channels	O
gradually	O
increases	O
,	O
instead	O
of	O
maintaining	O
the	O
dimension	O
until	O
a	O
residual	O
unit	O
with	O
downsampling	O
appears	O
.	O
A	O
schematic	O
illustration	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)	O
to	O
facilitate	O
understanding	O
of	O
our	O
network	Method
architecture	Method
.	O
subsection	O
:	O
Feature	Method
Map	Method
Dimension	Method
Configuration	Method
Most	O
deep	Method
CNN	Method
architectures	Method
utilize	O
an	O
approach	O
whereby	O
feature	O
map	O
dimensions	O
are	O
increased	O
by	O
a	O
large	O
margin	O
when	O
the	O
size	O
of	O
the	O
feature	O
map	O
decreases	O
,	O
and	O
feature	O
map	O
dimensions	O
are	O
not	O
increased	O
until	O
they	O
encounter	O
a	O
layer	O
with	O
downsampling	O
.	O
In	O
the	O
case	O
of	O
the	O
original	O
ResNet	Method
for	O
CIFAR	Material
datasets	Material
,	O
the	O
number	O
of	O
feature	O
map	O
dimensions	O
of	O
the	O
-	O
th	O
residual	O
unit	O
that	O
belongs	O
to	O
the	O
-	O
th	O
group	O
can	O
be	O
described	O
as	O
follows	O
:	O
in	O
which	O
denotes	O
the	O
index	O
of	O
the	O
group	O
to	O
which	O
the	O
k	O
-	O
th	O
residual	O
unit	O
belongs	O
.	O
The	O
residual	O
units	O
that	O
belong	O
to	O
the	O
same	O
group	O
have	O
an	O
equal	O
feature	O
map	O
size	O
,	O
and	O
the	O
-	O
th	O
group	O
contains	O
residual	O
units	O
.	O
In	O
the	O
first	O
group	O
,	O
there	O
is	O
only	O
one	O
convolutional	Method
layer	Method
that	O
converts	O
an	O
RGB	Method
image	Method
into	O
multiple	O
feature	O
maps	O
.	O
For	O
the	O
-	O
th	O
group	O
,	O
after	O
residual	O
units	O
have	O
passed	O
,	O
the	O
feature	O
size	O
is	O
downsampled	O
by	O
half	O
and	O
the	O
number	O
of	O
dimensions	O
is	O
doubled	O
.	O
We	O
propose	O
a	O
method	O
of	O
increasing	O
the	O
feature	O
map	O
dimension	O
as	O
follows	O
:	O
in	O
which	O
denotes	O
the	O
total	O
number	O
of	O
residual	O
units	O
,	O
defined	O
as	O
.	O
The	O
dimension	O
is	O
increased	O
by	O
a	O
step	O
factor	O
of	O
,	O
and	O
the	O
output	O
dimension	O
of	O
the	O
final	O
unit	O
of	O
each	O
group	O
becomes	O
with	O
same	O
number	O
of	O
residual	O
units	O
in	O
each	O
group	O
.	O
The	O
details	O
of	O
our	O
network	Method
architecture	Method
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O
The	O
above	O
equations	O
are	O
based	O
on	O
an	O
addition	Method
-	Method
based	Method
widening	Method
step	Method
factor	Method
for	O
increasing	O
dimensions	O
.	O
However	O
,	O
of	O
course	O
,	O
multiplication	Method
-	Method
based	Method
widening	Method
(	O
i.e.	O
,	O
the	O
process	O
of	O
multiplying	O
by	O
a	O
factor	O
to	O
increase	O
the	O
channel	O
dimension	O
geometrically	O
)	O
presents	O
another	O
possibility	O
for	O
creating	O
a	O
pyramid	O
-	O
like	O
structure	O
.	O
Then	O
,	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
can	O
be	O
transformed	O
as	O
follows	O
:	O
The	O
main	O
difference	O
between	O
additive	O
and	O
multiplicative	O
PyramidNets	Method
is	O
that	O
the	O
feature	O
map	O
dimension	O
of	O
an	O
additive	Method
network	Method
gradually	O
increases	O
linearly	O
,	O
whereas	O
the	O
dimension	O
of	O
a	O
multiplicative	Method
network	Method
increases	O
geometrically	O
.	O
That	O
is	O
,	O
the	O
dimension	O
slowly	O
increases	O
in	O
input	O
-	O
side	O
layers	O
and	O
sharply	O
increases	O
in	O
output	O
-	O
side	O
layers	O
.	O
This	O
process	O
is	O
similar	O
to	O
that	O
of	O
the	O
original	O
deep	Method
network	Method
architectures	Method
such	O
as	O
VGG	Method
and	O
ResNet	Method
.	O
The	O
visual	O
illustrations	O
of	O
additive	O
and	O
multiplicative	O
PyramidNets	Method
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
In	O
this	O
paper	O
,	O
we	O
compare	O
the	O
performance	O
of	O
both	O
of	O
these	O
dimension	Method
-	Method
increasing	Method
approaches	Method
by	O
comparing	O
an	O
additive	O
PyramidNet	Method
(	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
)	O
and	O
a	O
multiplicative	O
PyramidNet	Method
(	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
)	O
in	O
section	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
Building	O
Block	O
The	O
building	Method
block	Method
(	O
i.e.	O
,	O
the	O
convolutional	Method
filter	Method
stacks	Method
with	O
ReLUs	Method
and	Method
BN	Method
layers	Method
)	O
in	O
a	O
residual	Method
unit	Method
is	O
the	O
core	O
of	O
ResNet	Method
-	O
based	O
architectures	O
.	O
It	O
is	O
obvious	O
that	O
in	O
order	O
to	O
maximize	O
the	O
capability	O
of	O
the	O
network	Method
architecture	Method
,	O
designing	O
a	O
good	O
building	O
block	O
is	O
essential	O
.	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
the	O
layers	O
can	O
be	O
stacked	O
in	O
various	O
manners	O
to	O
construct	O
a	O
single	O
building	O
block	O
.	O
We	O
found	O
the	O
building	O
block	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)	O
to	O
be	O
the	O
most	O
promising	O
,	O
and	O
therefore	O
we	O
included	O
this	O
structure	O
as	O
building	O
block	O
in	O
our	O
PyramidNets	Method
.	O
The	O
discussion	O
of	O
this	O
matter	O
is	O
continued	O
in	O
the	O
following	O
section	O
.	O
In	O
terms	O
of	O
shortcut	O
connections	O
,	O
many	O
researchers	O
either	O
use	O
those	O
based	O
on	O
identity	Method
mapping	Method
,	O
or	O
those	O
employing	O
convolution	Method
-	Method
based	Method
projection	Method
.	O
However	O
,	O
as	O
the	O
feature	O
map	O
dimension	O
of	O
PyramidNet	Method
is	O
increased	O
at	O
every	O
unit	O
,	O
we	O
can	O
only	O
consider	O
two	O
options	O
:	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcuts	Method
,	O
and	O
projection	Method
shortcuts	Method
conducted	O
by	O
1	Method
1	Method
convolutions	Method
.	O
However	O
,	O
as	O
mentioned	O
in	O
the	O
work	O
of	O
He	O
et	O
al	O
.	O
,	O
the	O
1	O
1	Method
convolutional	Method
shortcut	Method
produces	O
a	O
poor	O
result	O
when	O
there	O
are	O
too	O
many	O
residual	O
units	O
,	O
i.e.	O
,	O
this	O
shortcut	Method
is	O
unsuitable	O
for	O
very	O
deep	Method
network	Method
architectures	Method
.	O
Therefore	O
,	O
we	O
select	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcuts	Method
for	O
all	O
residual	O
units	O
.	O
Further	O
discussions	O
about	O
the	O
zero	Task
-	Task
padded	Task
shortcut	Task
are	O
provided	O
in	O
the	O
following	O
section	O
.	O
section	O
:	O
Discussions	O
In	O
this	O
section	O
,	O
we	O
present	O
an	O
in	O
-	O
depth	O
study	O
of	O
the	O
architecture	O
of	O
our	O
PyramidNet	Method
,	O
together	O
with	O
the	O
proposed	O
novel	O
residual	Method
units	Method
.	O
The	O
experiments	O
we	O
include	O
here	O
support	O
the	O
study	O
and	O
confirm	O
that	O
insights	O
obtained	O
from	O
our	O
network	Method
architecture	Method
can	O
further	O
improve	O
the	O
performance	O
of	O
existing	O
ResNet	Method
-	O
based	O
architectures	O
.	O
subsection	O
:	O
Effect	O
of	O
PyramidNet	Method
According	O
to	O
the	O
work	O
of	O
Veit	O
et	O
al	O
.	O
,	O
ResNets	Method
can	O
be	O
viewed	O
as	O
ensembles	Method
of	Method
relatively	Method
shallow	Method
networks	Method
,	O
supported	O
by	O
the	O
observation	O
that	O
deleting	O
an	O
individual	O
building	O
block	O
in	O
a	O
residual	O
unit	O
of	O
ResNets	Method
incurs	O
minor	O
classification	Metric
loss	Metric
,	O
whereas	O
removing	O
layers	O
from	O
plain	Method
networks	O
such	O
as	O
VGG	Method
severely	O
reduces	O
the	O
classification	Metric
rate	Metric
.	O
However	O
,	O
in	O
both	O
original	O
and	O
pre	Task
-	Task
activation	Task
ResNets	Task
,	O
another	O
noteworthy	O
aspect	O
is	O
that	O
deleting	O
the	O
units	O
with	O
downsampling	O
(	O
and	O
doubling	O
the	O
feature	O
dimension	O
)	O
still	O
degrades	O
performance	O
by	O
a	O
large	O
margin	O
.	O
Meanwhile	O
,	O
when	O
a	O
stochastic	O
depth	O
is	O
applied	O
,	O
this	O
phenomenon	O
is	O
not	O
observed	O
,	O
and	O
the	O
performance	O
is	O
also	O
improved	O
,	O
according	O
to	O
the	O
experiment	O
of	O
Veit	O
et	O
al	O
.	O
.	O
The	O
objective	O
of	O
our	O
PyramidNet	Method
is	O
to	O
resolve	O
this	O
phenomenon	O
differently	O
,	O
by	O
attempting	O
to	O
gradually	O
increase	O
the	O
feature	O
map	O
dimension	O
instead	O
of	O
doubling	O
it	O
at	O
one	O
of	O
the	O
residual	O
units	O
and	O
to	O
evenly	O
distribute	O
the	O
burden	O
of	O
increasing	O
the	O
feature	O
maps	O
.	O
We	O
observed	O
that	O
our	O
PyramidNet	Method
indeed	O
resolves	O
this	O
phenomenon	O
and	O
at	O
the	O
same	O
time	O
improves	O
overall	O
performance	O
.	O
We	O
further	O
analyze	O
the	O
effect	O
of	O
our	O
PyramidNet	Method
by	O
comparing	O
it	O
against	O
the	O
pre	O
-	O
activation	O
ResNet	Method
,	O
with	O
the	O
following	O
experimental	O
results	O
.	O
First	O
,	O
we	O
compare	O
the	O
training	Metric
and	O
test	Metric
error	Metric
curves	Metric
of	O
our	O
PyramidNet	Method
with	O
those	O
of	O
the	O
pre	O
-	O
activation	O
ResNet	Method
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
standard	O
pre	O
-	O
activation	O
ResNet	Method
with	O
110	O
layers	O
is	O
used	O
for	O
comparison	O
.	O
For	O
our	O
PyramidNet	Method
,	O
we	O
used	O
a	O
depth	O
of	O
110	O
layers	O
with	O
a	O
widening	O
factor	O
of	O
;	O
it	O
had	O
the	O
same	O
number	O
of	O
parameters	O
(	O
1.7	O
M	O
)	O
as	O
the	O
pre	O
-	O
activation	O
ResNet	Method
to	O
allow	O
for	O
a	O
fair	O
comparison	O
.	O
The	O
results	O
indicate	O
that	O
our	O
PyramidNet	Method
has	O
superior	O
test	Metric
accuracy	Metric
,	O
thereby	O
confirming	O
its	O
greater	O
ability	O
to	O
generalize	O
compared	O
to	O
existing	O
deep	Method
networks	Method
.	O
Second	O
,	O
we	O
verify	O
the	O
ensemble	O
effect	O
of	O
our	O
PyramidNets	Method
by	O
evaluating	O
the	O
performance	O
after	O
deleting	O
individual	O
units	O
,	O
similar	O
to	O
the	O
experiment	O
of	O
Veit	O
et	O
al	O
.	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
As	O
mentioned	O
by	O
Veit	O
et	O
al	O
.	O
,	O
removing	O
individual	O
units	O
only	O
causes	O
a	O
slight	O
performance	O
loss	O
,	O
compared	O
with	O
a	O
plain	Method
network	O
such	O
as	O
the	O
VGG	Method
.	O
However	O
,	O
in	O
the	O
case	O
of	O
the	O
pre	O
-	O
activation	O
ResNet	Method
,	O
removing	O
the	O
blocks	O
subjected	O
to	O
downsampling	O
tends	O
to	O
affect	O
the	O
classification	Metric
accuracy	Metric
by	O
a	O
relatively	O
large	O
margin	O
,	O
whereas	O
this	O
does	O
not	O
occur	O
with	O
our	O
PyramidNets	Method
.	O
Furthermore	O
,	O
the	O
mean	O
average	Metric
error	Metric
differences	Metric
between	O
the	O
baseline	O
result	O
and	O
the	O
result	O
obtained	O
when	O
individual	O
units	O
were	O
deleted	O
from	O
both	O
the	O
pre	O
-	O
activation	O
ResNet	Method
and	O
our	O
PyramidNet	Method
were	O
0.72	O
%	O
and	O
0.54	O
%	O
,	O
respectively	O
.	O
This	O
result	O
shows	O
that	O
the	O
ensemble	O
effect	O
of	O
our	O
PyramidNet	Method
becomes	O
stronger	O
than	O
the	O
original	O
ResNet	Method
,	O
such	O
that	O
generalization	Metric
ability	Metric
is	O
improved	O
.	O
subsection	O
:	O
Zero	Method
-	Method
padded	Method
Shortcut	Method
Connection	Method
ResNets	Method
and	O
pre	Method
-	Method
activation	Method
ResNets	Method
were	O
studied	O
several	O
types	O
of	O
shortcuts	O
,	O
such	O
as	O
an	O
identity	Method
-	Method
mapping	Method
shortcut	Method
or	O
projection	Method
shortcut	Method
.	O
The	O
experimental	O
results	O
in	O
showed	O
that	O
the	O
identity	Method
-	Method
mapping	Method
shortcut	Method
is	O
a	O
much	O
more	O
appropriate	O
choice	O
than	O
other	O
shortcuts	O
.	O
Because	O
an	O
identity	Method
-	Method
mapping	Method
shortcut	Method
does	O
not	O
have	O
parameters	O
,	O
it	O
has	O
a	O
lower	O
possibility	O
of	O
overfitting	O
compared	O
to	O
the	O
other	O
types	O
of	O
shortcuts	Method
;	O
this	O
ensures	O
improved	O
generalization	Metric
ability	Metric
.	O
Moreover	O
,	O
it	O
can	O
purely	O
pass	O
through	O
the	O
gradient	O
according	O
to	O
the	O
identity	Method
mapping	Method
,	O
and	O
therefore	O
it	O
provides	O
more	O
stability	O
in	O
the	O
training	Metric
stage	O
.	O
In	O
the	O
case	O
of	O
our	O
PyramidNet	Method
,	O
identity	Method
mapping	Method
alone	O
can	O
not	O
be	O
used	O
for	O
a	O
shortcut	O
because	O
the	O
feature	O
map	O
dimension	O
differs	O
among	O
individual	O
residual	O
units	O
.	O
Therefore	O
,	O
only	O
a	O
zero	Method
-	Method
padded	Method
shortcut	Method
or	O
projection	Method
shortcut	Method
can	O
be	O
used	O
for	O
all	O
the	O
residual	O
units	O
.	O
However	O
,	O
as	O
discussed	O
in	O
,	O
a	O
projection	Method
shortcut	Method
can	O
hamper	O
information	Task
propagation	Task
and	O
lead	O
to	O
optimization	Task
problems	Task
,	O
especially	O
for	O
very	O
deep	Method
networks	Method
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
found	O
that	O
the	O
zero	Method
-	Method
padded	Method
shortcut	Method
does	O
not	O
lead	O
to	O
the	O
overfitting	Task
problem	Task
because	O
no	O
additional	O
parameters	O
exist	O
,	O
and	O
surprisingly	O
,	O
it	O
shows	O
significant	O
generalization	Metric
ability	Metric
compared	O
to	O
other	O
shortcuts	Method
.	O
We	O
now	O
examine	O
the	O
effect	O
of	O
the	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcut	Method
on	O
the	O
-	O
th	O
residual	O
unit	O
that	O
belongs	O
to	O
the	O
-	O
th	O
group	O
with	O
the	O
reshaped	O
vector	O
of	O
the	O
-	O
th	O
feature	O
map	O
:	O
where	O
denotes	O
the	O
-	O
th	O
residual	O
function	O
of	O
the	O
-	O
th	O
residual	O
unit	O
and	O
represents	O
the	O
pre	O
-	O
defined	O
channel	O
dimensions	O
of	O
the	O
-	O
th	O
residual	O
unit	O
.	O
From	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
zero	O
-	O
padded	O
elements	O
of	O
the	O
identity	Method
-	Method
mapping	Method
shortcut	Method
for	O
increasing	O
dimension	O
let	O
contain	O
the	O
outputs	O
of	O
both	O
residual	Method
networks	Method
and	O
plain	Method
networks	O
.	O
Therefore	O
,	O
we	O
could	O
conjecture	O
that	O
each	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcut	Method
can	O
provide	O
a	O
mixture	O
of	O
the	O
residual	Method
network	Method
and	O
plain	Method
network	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Furthermore	O
,	O
our	O
PyramidNet	Method
increases	O
the	O
channel	O
dimension	O
at	O
every	O
residual	O
unit	O
,	O
and	O
the	O
mixture	O
effect	O
of	O
the	O
residual	Method
network	Method
and	O
plain	Method
network	O
increases	O
markedly	O
.	O
Figure	O
[	O
reference	O
]	O
supports	O
the	O
conclusion	O
that	O
the	O
test	Metric
error	Metric
of	O
PyramidNet	Method
does	O
not	O
oscillate	O
as	O
much	O
as	O
that	O
of	O
the	O
pre	O
-	O
activation	O
ResNet	Method
.	O
Finally	O
,	O
we	O
investigate	O
several	O
types	O
of	O
shortcuts	O
including	O
proposed	O
zero	Method
-	Method
padded	Method
identity	Method
-	Method
mapping	Method
shortcut	Method
in	O
Table	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
A	O
New	O
Building	O
Block	O
To	O
maximize	O
the	O
capability	O
of	O
the	O
network	O
,	O
it	O
is	O
natural	O
to	O
ask	O
the	O
following	O
question	O
:	O
“	O
Can	O
we	O
design	O
a	O
better	O
building	O
block	O
by	O
altering	O
the	O
stacked	O
elements	O
inside	O
the	O
building	O
block	O
in	O
more	O
principled	O
way	O
?	O
”	O
.	O
The	O
first	O
building	O
block	O
types	O
were	O
proposed	O
in	O
the	O
original	O
paper	O
on	O
ResNets	Task
,	O
and	O
another	O
type	O
of	O
building	O
block	O
was	O
subsequently	O
proposed	O
in	O
the	O
paper	O
on	O
pre	Task
-	Task
activation	Task
ResNets	Task
,	O
to	O
answer	O
the	O
question	O
.	O
Moreover	O
,	O
pre	O
-	O
activation	Method
ResNets	Method
attempted	O
to	O
solve	O
the	O
backward	Task
gradient	Task
flowing	Task
problem	Task
by	O
redesigning	O
residual	Method
modules	Method
;	O
this	O
proved	O
to	O
be	O
successful	O
in	O
trials	O
.	O
However	O
,	O
although	O
the	O
pre	O
-	O
activation	O
residual	O
unit	O
was	O
discovered	O
with	O
empirically	O
improved	O
performance	O
,	O
further	O
investigation	O
over	O
the	O
possible	O
combinations	O
is	O
not	O
yet	O
performed	O
,	O
leaving	O
a	O
potential	O
room	O
for	O
improvement	O
.	O
We	O
next	O
attempt	O
to	O
answer	O
the	O
question	O
from	O
two	O
points	O
of	O
view	O
by	O
considering	O
Rectified	Method
Linear	Method
Units	Method
(	O
ReLUs	Method
)	O
and	O
Batch	Method
Normalization	Method
(	O
BN	Method
)	Method
layers	Method
.	O
subsubsection	O
:	O
ReLUs	O
in	O
a	O
Building	O
Block	O
Including	O
ReLUs	O
in	O
the	O
building	O
blocks	O
of	O
residual	Method
units	Method
is	O
essential	O
for	O
nonlinearity	Task
;	O
however	O
,	O
we	O
found	O
empirically	O
that	O
the	O
performance	O
can	O
vary	O
depending	O
on	O
the	O
locations	O
and	O
the	O
number	O
of	O
ReLUs	O
.	O
This	O
could	O
be	O
discussed	O
with	O
original	O
ResNets	Method
,	O
for	O
which	O
it	O
was	O
shown	O
that	O
the	O
performance	O
increases	O
as	O
the	O
network	O
becomes	O
deeper	O
;	O
however	O
,	O
if	O
the	O
depth	O
exceeds	O
1	O
,	O
000	O
layers	O
,	O
overfitting	O
still	O
occurs	O
and	O
the	O
result	O
is	O
less	O
accurate	O
than	O
that	O
generated	O
by	O
shallower	Method
ResNets	Method
.	O
First	O
,	O
we	O
note	O
that	O
using	O
ReLUs	Method
after	O
the	O
addition	O
of	O
residual	O
units	O
adversely	O
affects	O
performance	O
:	O
where	O
the	O
ReLUs	Method
seem	O
to	O
have	O
the	O
function	O
of	O
filtering	O
non	O
-	O
negative	O
elements	O
.	O
Gross	O
and	O
Wilber	O
found	O
that	O
simply	O
removing	O
ReLUs	O
from	O
the	O
original	O
ResNet	Method
after	O
each	O
addition	O
with	O
the	O
shortcut	O
connection	O
leads	O
to	O
small	O
performance	O
improvements	O
.	O
This	O
could	O
be	O
understood	O
by	O
considering	O
that	O
,	O
after	O
addition	O
,	O
ReLUs	O
provide	O
non	O
-	O
negative	O
input	O
to	O
the	O
subsequent	O
residual	O
units	O
,	O
and	O
therefore	O
the	O
shortcut	O
connection	O
is	O
always	O
non	O
-	O
negative	O
and	O
the	O
convolutional	Method
layers	Method
would	O
take	O
responsibility	O
for	O
producing	O
negative	O
output	O
before	O
addition	O
;	O
this	O
may	O
decrease	O
the	O
overall	O
capability	O
of	O
the	O
network	Method
architecture	Method
as	O
analyzed	O
in	O
.	O
The	O
pre	Method
-	Method
activation	Method
ResNets	Method
proposed	O
by	O
He	O
et	O
al	O
.	O
also	O
overcame	O
this	O
issue	O
with	O
pre	O
-	O
activated	O
residual	O
units	O
that	O
place	O
BN	O
layers	O
and	O
ReLUs	O
before	O
(	O
instead	O
of	O
after	O
)	O
the	O
convolutional	Method
layers	Method
:	O
where	O
ReLUs	O
are	O
removed	O
after	O
addition	O
to	O
create	O
an	O
identity	O
path	O
.	O
Consequently	O
,	O
the	O
overall	O
performance	O
has	O
increased	O
by	O
a	O
large	O
margin	O
without	O
overfitting	O
,	O
even	O
at	O
depths	O
exceeding	O
1	O
,	O
000	O
layers	O
.	O
Furthermore	O
,	O
Shen	O
et	O
al	O
.	O
proposed	O
a	O
weighted	O
residual	Method
network	Method
architecture	O
,	O
which	O
locates	O
a	O
ReLU	O
inside	O
a	O
residual	O
unit	O
(	O
instead	O
of	O
locating	O
ReLU	O
after	O
addition	O
)	O
to	O
create	O
an	O
identity	O
path	O
,	O
and	O
showed	O
that	O
this	O
structure	O
also	O
does	O
not	O
overfit	O
even	O
at	O
depths	O
of	O
more	O
than	O
1	O
,	O
000	O
layers	O
.	O
Second	O
,	O
we	O
found	O
that	O
the	O
use	O
of	O
a	O
large	O
number	O
of	O
ReLUs	O
in	O
the	O
blocks	O
of	O
each	O
residual	O
unit	O
may	O
negatively	O
affect	O
performance	O
.	O
Removing	O
the	O
first	O
ReLU	O
in	O
the	O
blocks	O
of	O
each	O
residual	O
unit	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
and	O
(	O
d	O
)	O
,	O
was	O
found	O
to	O
enhance	O
performance	O
compared	O
with	O
the	O
blocks	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
and	O
(	O
c	O
)	O
.	O
Experimentally	O
,	O
we	O
found	O
that	O
removal	O
of	O
the	O
first	O
ReLU	O
in	O
the	O
stack	O
is	O
preferable	O
and	O
that	O
the	O
other	O
ReLU	O
should	O
remain	O
to	O
ensure	O
nonlinearity	O
.	O
Removing	O
the	O
second	O
ReLU	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
changes	O
the	O
blocks	O
to	O
BN	Method
-	Method
ReLU	Method
-	Method
conv	Method
-	Method
BN	Method
-	Method
conv	Method
,	O
and	O
it	O
is	O
clear	O
that	O
,	O
in	O
these	O
blocks	O
,	O
the	O
convolutional	Method
layers	Method
are	O
successively	O
located	O
without	O
ReLUs	O
to	O
weaken	O
their	O
representation	O
powers	O
of	O
each	O
other	O
.	O
However	O
,	O
when	O
we	O
remove	O
the	O
first	O
ReLU	Method
,	O
the	O
blocks	O
are	O
changed	O
to	O
BN	Method
-	Method
conv	Method
-	Method
BN	Method
-	Method
ReLU	Method
-	Method
conv	Method
,	O
in	O
which	O
case	O
the	O
two	O
convolutional	Method
layers	Method
are	O
separated	O
by	O
the	O
second	O
ReLU	Method
,	O
thereby	O
guaranteeing	O
nonlinearity	O
.	O
The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
confirm	O
that	O
removing	O
the	O
first	O
ReLU	O
as	O
in	O
(	O
b	O
)	O
and	O
(	O
d	O
)	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
enhances	O
the	O
performance	O
.	O
Consequently	O
,	O
provided	O
that	O
an	O
appropriate	O
number	O
of	O
ReLUs	O
are	O
used	O
to	O
guarantee	O
the	O
nonlinearity	O
of	O
the	O
feature	O
space	O
manifold	O
,	O
the	O
remaining	O
ReLUs	O
could	O
be	O
removed	O
to	O
improve	O
network	Task
performance	O
.	O
subsubsection	O
:	O
BN	Method
Layers	Method
in	O
a	O
Building	Method
Block	Method
The	O
main	O
role	O
of	O
a	O
BN	Method
layer	Method
is	O
to	O
normalize	O
the	O
activations	O
for	O
fast	O
convergence	O
and	O
to	O
improve	O
performance	O
.	O
The	O
experimental	O
results	O
of	O
the	O
four	O
structures	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
show	O
that	O
the	O
BN	O
layer	O
can	O
be	O
used	O
to	O
maximize	O
the	O
capability	O
of	O
a	O
single	O
residual	O
unit	O
.	O
A	O
BN	Method
layer	Method
conducts	O
an	O
affine	Method
transformation	Method
with	O
the	O
following	O
equation	O
:	O
where	O
and	O
are	O
learned	O
for	O
every	O
activation	O
in	O
feature	O
maps	O
.	O
We	O
experimentally	O
found	O
that	O
the	O
learned	O
and	O
could	O
closely	O
approximate	O
.	O
This	O
implies	O
that	O
if	O
the	O
learned	O
and	O
are	O
both	O
close	O
to	O
,	O
then	O
the	O
corresponding	O
activation	O
is	O
considered	O
not	O
to	O
be	O
useful	O
.	O
Weighted	Task
ResNets	Task
,	O
in	O
which	O
the	O
learnable	O
weights	O
occur	O
at	O
the	O
end	O
of	O
their	O
building	O
blocks	O
,	O
are	O
also	O
similarly	O
learned	O
to	O
determine	O
whether	O
the	O
corresponding	O
residual	O
unit	O
is	O
useful	O
.	O
Thus	O
,	O
the	O
BN	Method
layers	Method
at	O
the	O
end	O
of	O
each	O
residual	O
unit	O
are	O
a	O
generalized	O
version	O
including	O
to	O
enable	O
decisions	O
to	O
be	O
made	O
as	O
to	O
whether	O
each	O
residual	O
unit	O
is	O
helpful	O
.	O
Therefore	O
,	O
the	O
degrees	O
of	O
freedom	O
obtained	O
by	O
involving	O
and	O
from	O
the	O
BN	O
layers	O
could	O
improve	O
the	O
capability	O
of	O
the	O
network	Method
architecture	Method
.	O
The	O
results	O
in	O
Table	O
[	O
reference	O
]	O
support	O
the	O
conclusion	O
that	O
adding	O
a	O
BN	O
layer	O
at	O
the	O
end	O
of	O
each	O
building	O
block	O
,	O
as	O
in	O
type	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
improves	O
the	O
performance	O
.	O
Note	O
that	O
the	O
aforementioned	O
network	O
removing	O
the	O
first	O
ReLU	Method
is	O
also	O
improved	O
by	O
adding	O
a	O
BN	Method
layer	Method
after	O
the	O
final	O
convolutional	Method
layer	Method
.	O
Furthermore	O
,	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
show	O
that	O
both	O
PyramidNet	Method
and	O
a	O
new	O
building	Method
block	Method
improve	O
the	O
performance	O
significantly	O
.	O
section	O
:	O
Experimental	O
Results	O
We	O
evaluate	O
and	O
compare	O
the	O
performance	O
of	O
our	O
algorithm	O
with	O
that	O
of	O
existing	O
algorithms	O
using	O
representative	O
benchmark	O
datasets	O
:	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
.	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
each	O
contain	O
32	O
32	O
-	O
pixel	O
color	O
images	O
,	O
consists	O
of	O
50	O
,	O
000	O
training	Metric
images	O
and	O
10	O
,	O
000	O
testing	O
images	O
.	O
But	O
in	O
case	O
of	O
CIFAR	Material
-	Material
10	Material
,	O
it	O
includes	O
10	O
classes	O
,	O
and	O
CIFAR	Material
-	Material
100	Material
includes	O
100	O
classes	O
.	O
The	O
standard	O
data	Method
augmentation	Method
,	O
horizontal	O
flipping	O
,	O
and	O
translation	O
by	O
4	O
pixels	O
are	O
adopted	O
in	O
our	O
experiments	O
,	O
following	O
the	O
common	O
practice	O
.	O
The	O
results	O
achieved	O
by	O
PyramidNets	Method
are	O
based	O
on	O
the	O
proposed	O
residual	Method
unit	Method
:	O
placing	O
a	O
BN	Method
layer	Method
after	O
the	O
final	O
convolutional	Method
layer	Method
,	O
and	O
removing	O
the	O
first	O
ReLU	O
as	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)	O
.	O
Our	O
code	O
is	O
built	O
on	O
Torch	Method
open	Method
source	Method
deep	Method
learning	Method
framework	Method
.	O
subsection	O
:	O
Training	O
Settings	O
Our	O
PyramidNets	Method
are	O
trained	O
using	O
backpropagation	O
by	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
with	O
Nesterov	Method
momentum	Method
for	O
300	O
epochs	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
datasets	O
.	O
The	O
initial	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.1	O
for	O
CIFAR	Material
-	Material
10	Material
and	O
0.5	O
for	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
is	O
decayed	O
by	O
a	O
factor	O
of	O
0.1	O
at	O
150	O
and	O
225	O
epochs	O
,	O
respectively	O
.	O
The	O
filter	Method
parameters	Method
are	O
initialized	O
by	O
“	O
msra	Method
”	Method
.	O
We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
,	O
a	O
dampening	O
of	O
0	O
,	O
a	O
momentum	O
of	O
0.9	O
,	O
and	O
a	O
batch	O
size	O
of	O
128	O
.	O
subsection	O
:	O
Performance	O
Evaluation	O
In	O
our	O
work	O
,	O
we	O
mainly	O
use	O
the	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
for	O
evaluating	O
our	O
network	Method
architecture	Method
.	O
Additive	O
PyramidNets	Method
with	O
both	O
basic	O
and	O
pyramidal	Method
bottleneck	Method
residual	Method
units	Method
are	O
used	O
.	O
The	O
error	Metric
rates	Metric
are	O
provided	O
in	O
Table	O
[	O
reference	O
]	O
for	O
ours	O
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
The	O
experimental	O
results	O
show	O
that	O
our	O
network	O
has	O
superior	O
generalization	Metric
ability	Metric
,	O
in	O
terms	O
of	O
the	O
number	O
of	O
parameters	O
,	O
showing	O
the	O
best	O
results	O
compared	O
with	O
other	O
models	O
.	O
Figure	O
[	O
reference	O
]	O
compares	O
additive	O
and	O
multiplicative	O
PyramidNets	Method
using	O
CIFAR	Material
datasets	Material
.	O
When	O
the	O
number	O
of	O
parameters	O
is	O
low	O
,	O
both	O
additive	O
and	O
multiplicative	O
PyramidNets	Method
show	O
similar	O
performance	O
,	O
because	O
these	O
two	O
network	Method
architectures	Method
do	O
not	O
have	O
significant	O
structural	O
differences	O
.	O
As	O
the	O
number	O
of	O
parameters	O
increases	O
,	O
they	O
start	O
to	O
show	O
a	O
more	O
marked	O
difference	O
in	O
terms	O
of	O
the	O
feature	O
map	O
dimension	O
configuration	O
.	O
Because	O
the	O
feature	O
map	O
dimension	O
increases	O
linearly	O
in	O
the	O
case	O
of	O
additive	O
PyramidNets	Method
,	O
the	O
feature	O
map	O
dimensions	O
of	O
the	O
input	O
-	O
side	O
layers	O
tend	O
to	O
be	O
larger	O
,	O
and	O
those	O
of	O
the	O
output	O
-	O
side	O
layers	O
tend	O
to	O
be	O
smaller	O
,	O
compared	O
with	O
multiplicative	O
PyramidNets	Method
as	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Previous	O
works	O
typically	O
set	O
multiplicative	Method
scaling	Method
of	Method
feature	Method
map	Method
dimension	Method
for	O
downsampling	Method
modules	Method
,	O
which	O
is	O
implemented	O
to	O
give	O
a	O
larger	O
degree	O
of	O
freedom	O
to	O
the	O
classification	Task
part	Task
by	O
increasing	O
the	O
feature	O
map	O
dimension	O
of	O
the	O
output	O
-	O
side	O
layers	O
.	O
However	O
,	O
for	O
our	O
PyramidNet	Method
,	O
the	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
implies	O
that	O
increasing	O
the	O
model	O
capacity	O
of	O
the	O
input	O
-	O
side	O
layers	O
would	O
lead	O
to	O
a	O
better	O
performance	O
improvement	O
than	O
using	O
a	O
conventional	O
way	O
of	O
multiplicative	Method
scaling	Method
of	Method
feature	Method
map	Method
dimension	Method
.	O
We	O
also	O
note	O
that	O
,	O
although	O
the	O
use	O
of	O
regularization	Method
methods	Method
such	O
as	O
dropout	Method
or	O
stochastic	Method
depth	Method
could	O
further	O
improve	O
the	O
performance	O
of	O
our	O
model	O
,	O
we	O
did	O
not	O
involve	O
those	O
methods	O
to	O
ensure	O
a	O
fair	O
comparison	O
with	O
other	O
models	O
.	O
subsection	O
:	O
ImageNet	Material
1	O
,	O
000	O
-	O
class	O
ImageNet	Material
dataset	O
used	O
for	O
ILSVRC	Task
contains	O
more	O
than	O
one	O
million	O
training	Metric
images	O
and	O
50	O
,	O
000	O
validation	O
images	O
.	O
We	O
use	O
additive	O
PyramidNets	Method
with	O
the	O
pyramidal	Method
bottleneck	Method
residual	Method
units	Method
,	O
deleting	O
the	O
first	O
ReLU	Method
and	O
adding	O
a	O
BN	O
layer	O
at	O
the	O
last	O
layer	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
and	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
d	O
)	O
for	O
further	O
performance	O
improvement	O
.	O
We	O
train	O
our	O
models	O
for	O
120	O
epochs	O
with	O
a	O
batch	O
size	O
of	O
128	O
,	O
and	O
the	O
initial	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.05	O
,	O
divided	O
by	O
10	O
at	O
60	O
,	O
90	O
and	O
105	O
epochs	O
.	O
We	O
use	O
the	O
same	O
weight	O
decay	O
,	O
momentum	O
,	O
and	O
initialization	O
settings	O
as	O
those	O
of	O
CIFAR	Material
datasets	Material
.	O
We	O
train	O
our	O
model	O
by	O
using	O
a	O
standard	O
data	Method
augmentation	Method
with	O
scale	O
jittering	O
and	O
aspect	O
ratio	O
as	O
suggested	O
in	O
Szegedy	O
et	O
al	O
.	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
our	O
PyramidNets	Method
in	O
ImageNet	Material
dataset	O
compared	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
The	O
experimental	O
results	O
show	O
that	O
our	O
PyramidNet	Method
with	O
has	O
a	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
of	O
20.5	O
,	O
which	O
is	O
1.2	O
lower	O
than	O
the	O
pre	O
-	O
activation	O
ResNet	Method
-	O
200	O
which	O
has	O
a	O
similar	O
number	O
of	O
parameters	O
but	O
higher	O
output	O
feature	O
dimension	O
than	O
our	O
model	O
.	O
We	O
also	O
notice	O
that	O
increasing	O
with	O
an	O
appropriate	O
regularization	Method
method	Method
can	O
further	O
improve	O
the	O
performance	O
.	O
For	O
comparison	O
with	O
the	O
Inception	O
-	O
ResNet	Method
that	O
uses	O
a	O
testing	O
crop	O
with	O
size	O
,	O
we	O
test	O
our	O
model	O
on	O
a	O
crop	O
,	O
by	O
the	O
same	O
reason	O
with	O
the	O
work	O
of	O
He	O
et	O
al	O
.	O
.	O
Our	O
PyramidNet	Method
with	O
shows	O
a	O
top	Metric
-	Metric
1	Metric
error	Metric
rate	Metric
of	O
19.6	O
,	O
which	O
outperforms	O
both	O
the	O
pre	O
-	O
activation	O
ResNet	Method
and	O
the	O
Inception	O
-	O
ResNet	Method
-	O
v2	O
models	O
.	O
section	O
:	O
Conclusion	O
The	O
main	O
idea	O
of	O
the	O
novel	O
deep	Method
network	Method
architecture	Method
described	O
in	O
this	O
paper	O
involves	O
increasing	O
the	O
feature	O
map	O
dimension	O
gradually	O
,	O
in	O
order	O
to	O
construct	O
so	O
-	O
called	O
PyramidNets	Method
along	O
with	O
the	O
concept	O
of	O
ResNets	O
.	O
We	O
also	O
developed	O
a	O
novel	O
residual	Method
unit	Method
,	O
which	O
includes	O
a	O
new	O
building	Method
block	Method
for	O
a	O
residual	Method
unit	Method
with	O
a	O
zero	Method
-	Method
padded	Method
shortcut	Method
;	O
this	O
design	O
leads	O
to	O
significantly	O
improved	O
generalization	Metric
ability	Metric
.	O
In	O
tests	O
using	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
and	O
ImageNet	Material
-	Material
1k	Material
datasets	Material
,	O
our	O
PyramidNets	Method
outperform	O
all	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	Method
network	Method
architectures	Method
.	O
Furthermore	O
,	O
the	O
insights	O
in	O
this	O
paper	O
could	O
be	O
utilized	O
by	O
any	O
network	Method
architecture	Method
,	O
to	O
improve	O
their	O
capacity	O
for	O
better	O
performance	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
develop	O
methods	O
of	O
optimizing	O
parameters	O
such	O
as	O
feature	O
map	O
dimensions	O
in	O
more	O
principled	O
ways	O
with	O
proper	O
cost	O
functions	O
that	O
give	O
insight	O
into	O
the	O
nature	O
of	O
residual	Method
networks	Method
.	O
Acknowledgements	O
:	O
This	O
work	O
was	O
supported	O
by	O
the	O
ICT	O
R	O
&	O
D	O
program	O
of	O
MSIP	O
/	O
IITP	O
,	O
2016	O
-	O
0	O
-	O
00563	O
,	O
Research	O
on	O
Adaptive	Method
Machine	Method
Learning	Method
Technology	Method
Development	O
for	O
Intelligent	Task
Autonomous	Task
Digital	Task
Companion	Task
.	O
bibliography	O
:	O
References	O
