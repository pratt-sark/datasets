Knowledge	Method
graphs	Method
are	O
structured	O
representations	O
of	O
real	O
world	O
facts	O
.	O
However	O
,	O
they	O
typically	O
contain	O
only	O
a	O
small	O
subset	O
of	O
all	O
possible	O
facts	O
.	O
Link	Task
prediction	Task
is	O
a	O
task	O
of	O
inferring	Task
missing	Task
facts	Task
based	O
on	O
existing	O
ones	O
.	O
We	O
propose	O
TuckER	Method
,	O
a	O
relatively	O
simple	O
but	O
powerful	O
linear	Method
model	Method
based	O
on	O
Tucker	Method
decomposition	Method
of	O
the	O
binary	Method
tensor	Method
representation	Method
of	Method
knowledge	Method
graph	Method
triples	Method
.	O
TuckER	Method
outperforms	O
all	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
across	O
standard	O
link	O
prediction	O
datasets	O
.	O
We	O
prove	O
that	O
TuckER	Method
is	O
a	O
fully	Method
expressive	Method
model	Method
,	O
deriving	O
the	O
bound	O
on	O
its	O
entity	Metric
and	Metric
relation	Metric
embedding	Metric
dimensionality	Metric
for	O
full	Task
expressiveness	Task
which	O
is	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE.	O
We	O
further	O
show	O
that	O
several	O
previously	O
introduced	O
linear	Method
models	Method
can	O
be	O
viewed	O
as	O
special	O
cases	O
of	O
TuckER	Method
.	O
TuckER	Method
:	O
TensorFactorizationforKnowledgeGraphCompletion	O
section	O
:	O
Introduction	O
Vast	O
amounts	O
of	O
information	O
available	O
in	O
the	O
world	O
can	O
be	O
represented	O
succinctly	O
as	O
entities	O
and	O
relations	O
between	O
them	O
.	O
Knowledge	Method
graphs	Method
are	O
large	O
,	O
graph	O
-	O
structured	O
databases	O
which	O
store	O
facts	O
in	O
triple	O
form	O
,	O
with	O
and	O
representing	O
subject	O
and	O
object	O
entities	O
and	O
a	O
relation	O
between	O
them	O
.	O
Knowledge	Method
graphs	Method
are	O
used	O
for	O
a	O
wide	O
range	O
of	O
natural	Task
language	Task
processing	Task
and	O
information	Task
extraction	Task
tasks	Task
.	O
However	O
,	O
far	O
from	O
all	O
available	O
information	O
is	O
stored	O
in	O
existing	O
knowledge	O
graphs	O
and	O
manually	O
adding	O
new	O
information	O
is	O
costly	O
,	O
which	O
creates	O
the	O
need	O
for	O
algorithms	O
that	O
are	O
able	O
to	O
automatically	O
infer	O
missing	O
facts	O
based	O
on	O
existing	O
ones	O
.	O
Knowledge	Task
graphs	Task
can	O
be	O
represented	O
by	O
a	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
,	O
where	O
each	O
element	O
corresponds	O
to	O
a	O
triple	O
,	O
1	O
indicating	O
a	O
true	O
fact	O
and	O
0	O
indicating	O
the	O
unknown	O
(	O
either	O
a	O
false	O
or	O
a	O
missing	O
fact	O
)	O
.	O
One	O
of	O
the	O
most	O
important	O
tasks	O
in	O
relational	Task
machine	Task
learning	Task
is	O
link	Task
prediction	Task
:	O
predicting	O
whether	O
two	O
entities	O
are	O
related	O
,	O
based	O
on	O
known	O
relations	O
already	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O
The	O
task	O
of	O
link	Task
prediction	Task
is	O
thus	O
to	O
infer	O
which	O
of	O
the	O
0	O
entries	O
in	O
the	O
tensor	O
are	O
indeed	O
false	O
,	O
and	O
which	O
are	O
missing	O
but	O
actually	O
true	O
.	O
A	O
large	O
number	O
of	O
approaches	O
to	O
link	Task
prediction	Task
so	O
far	O
have	O
been	O
linear	O
,	O
based	O
on	O
various	O
methods	O
of	O
factorizing	Method
the	Method
third	Method
-	Method
order	Method
binary	Method
tensor	Method
.	O
Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
using	O
non	Method
-	Method
linear	Method
convolutional	Method
models	Method
.	O
Despite	O
achieving	O
very	O
good	O
performance	O
,	O
the	O
fundamental	O
problem	O
with	O
deep	Method
,	Method
non	Method
-	Method
linear	Method
models	Method
is	O
that	O
they	O
are	O
non	O
-	O
transparent	O
and	O
poorly	O
understood	O
,	O
as	O
opposed	O
to	O
more	O
mathematically	O
principled	O
and	O
widely	O
studied	O
tensor	Method
decomposition	Method
models	Method
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
TuckER	Method
(	O
E	O
stands	O
for	O
entities	O
,	O
R	O
for	O
relations	O
)	O
,	O
a	O
simple	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	O
knowledge	Task
graphs	Task
,	O
based	O
on	O
Tucker	Method
decomposition	Method
of	O
the	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
of	Method
triples	Method
.	O
Tucker	Method
decomposition	Method
factorizes	O
a	O
tensor	O
into	O
a	O
core	O
tensor	O
multiplied	O
by	O
a	O
matrix	O
along	O
each	O
mode	O
.	O
It	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
higher	Method
-	Method
order	Method
singular	Method
value	Method
decomposition	Method
(	O
HOSVD	Method
)	Method
in	O
the	O
special	O
case	O
where	O
matrices	O
are	O
orthogonal	O
and	O
the	O
core	O
tensor	O
is	O
“	O
all	O
-	O
orthogonal	O
”	O
.	O
In	O
our	O
case	O
,	O
rows	O
of	O
the	O
three	O
matrices	O
contain	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
while	O
entries	O
of	O
the	O
core	O
tensor	O
determine	O
the	O
level	O
of	O
interaction	O
between	O
them	O
.	O
Further	O
,	O
subject	O
and	O
object	O
entity	O
embedding	O
matrices	O
are	O
assumed	O
equivalent	O
,	O
i.e.	O
we	O
make	O
no	O
distinction	O
between	O
the	O
embeddings	O
of	O
an	O
entity	O
depending	O
on	O
whether	O
it	O
appears	O
as	O
a	O
subject	O
or	O
as	O
an	O
object	O
in	O
a	O
particular	O
triple	O
.	O
Given	O
that	O
knowledge	O
graphs	O
contain	O
several	O
relation	O
types	O
(	O
symmetric	O
,	O
asymmetric	O
,	O
transitive	O
,	O
etc	O
.	O
)	O
,	O
it	O
is	O
important	O
for	O
a	O
link	Method
prediction	Method
model	Method
to	O
have	O
enough	O
expressive	O
power	O
to	O
accurately	O
represent	O
all	O
of	O
them	O
.	O
We	O
thus	O
show	O
that	O
TuckER	Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
any	O
ground	O
truth	O
over	O
the	O
triples	O
,	O
there	O
exists	O
an	O
assignment	O
of	O
values	O
to	O
the	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separates	O
the	O
true	O
triples	O
from	O
false	O
ones	O
.	O
We	O
also	O
derive	O
a	O
bound	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
that	O
guarantees	O
full	O
expressiveness	O
,	O
finding	O
it	O
to	O
be	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
.	O
This	O
enables	O
TuckER	Method
to	O
achieve	O
better	O
results	O
with	O
much	O
smaller	O
embedding	Metric
sizes	Metric
than	O
needed	O
by	O
those	O
models	O
,	O
important	O
for	O
efficiency	O
in	O
downstream	Task
tasks	Task
.	O
Finally	O
,	O
we	O
show	O
that	O
several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
linear	Method
models	Method
,	O
RESCAL	Method
,	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
are	O
special	O
cases	O
of	O
TuckER	Method
.	O
In	O
summary	O
,	O
the	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
proposing	O
TuckER	Method
,	O
a	O
new	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	Task
knowledge	Task
graphs	Task
,	O
that	O
is	O
simple	O
,	O
expressive	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
all	O
standard	O
datasets	O
;	O
proving	O
that	O
TuckER	Method
is	O
fully	O
expressive	O
and	O
deriving	O
a	O
bound	O
on	O
the	O
entity	Metric
and	Metric
relation	Metric
embedding	Metric
dimensionality	Metric
for	O
full	O
expressiveness	O
which	O
is	O
several	O
orders	O
of	O
magnitude	O
lower	O
than	O
the	O
bound	O
of	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
ComplEx	O
and	O
SimplE	O
;	O
and	O
showing	O
that	O
TuckER	Method
subsumes	O
several	O
previously	O
proposed	O
tensor	Method
factorization	Method
approaches	Method
to	O
link	Task
prediction	Task
,	O
i.e.	O
that	O
RESCAL	O
,	O
DistMult	O
,	O
ComplEx	Method
and	O
SimplE	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O
section	O
:	O
Related	O
Work	O
Several	O
linear	Method
models	Method
for	O
link	Task
prediction	Task
have	O
previously	O
been	O
proposed	O
:	O
RESCAL	O
An	O
early	O
linear	Method
model	Method
,	O
RESCAL	Method
,	O
optimizes	O
a	O
scoring	Method
function	Method
containing	O
a	O
bilinear	O
product	O
between	O
vector	Method
embeddings	Method
for	O
each	O
subject	O
and	O
object	O
entity	O
and	O
a	O
full	O
rank	O
matrix	O
for	O
each	O
relation	O
.	O
Although	O
a	O
very	O
expressive	O
and	O
powerful	O
model	O
,	O
RESCAL	Method
is	O
prone	O
to	O
overfitting	O
due	O
to	O
its	O
large	O
number	O
of	O
parameters	O
,	O
which	O
increases	O
quadratically	O
in	O
the	O
embedding	O
dimension	O
with	O
the	O
number	O
of	O
relations	O
in	O
a	O
knowledge	O
graph	O
.	O
DistMult	Method
DistMult	Method
is	O
a	O
special	O
case	O
of	O
RESCAL	O
with	O
a	O
diagonal	O
matrix	O
per	O
relation	O
,	O
so	O
the	O
number	O
of	O
parameters	O
of	O
DistMult	Method
grows	O
linearly	O
with	O
respect	O
to	O
the	O
embedding	O
dimension	O
,	O
reducing	O
overfitting	O
.	O
However	O
,	O
the	O
linear	Method
transformation	Method
performed	O
on	O
subject	O
entity	O
embedding	O
vectors	O
in	O
DistMult	Method
is	O
limited	O
to	O
a	O
stretch	O
.	O
Given	O
the	O
equivalence	O
of	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
,	O
third	O
-	O
order	O
binary	O
tensor	O
learned	O
by	O
DistMult	Method
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
and	O
thus	O
DistMult	Method
can	O
not	O
model	O
asymmetric	O
relations	O
.	O
ComplEx	Method
ComplEx	Method
extends	O
DistMult	Method
to	O
the	O
complex	O
domain	O
.	O
Even	O
though	O
each	O
relation	O
matrix	O
of	O
ComplEx	O
is	O
still	O
diagonal	O
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
no	O
longer	O
equivalent	O
,	O
but	O
complex	O
conjugates	O
,	O
which	O
introduces	O
asymmetry	O
into	O
the	O
tensor	Method
decomposition	Method
and	O
thus	O
enables	O
ComplEx	Method
to	O
model	O
asymmetric	O
relations	O
.	O
SimplE	Method
SimplE	Method
is	O
a	O
linear	Method
model	Method
based	O
on	O
Canonical	Method
Polyadic	Method
(	Method
CP	Method
)	Method
decomposition	Method
.	O
In	O
CP	Task
decomposition	Task
,	O
subject	O
and	O
object	O
entity	O
embeddings	O
for	O
the	O
same	O
entity	O
are	O
independent	O
(	O
note	O
that	O
DistMult	Method
is	O
a	O
special	O
case	O
of	O
CP	O
,	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
equivalent	O
)	O
.	O
SimplE	Method
’s	Method
scoring	Method
function	Method
alters	O
CP	O
to	O
make	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
dependent	O
on	O
each	O
other	O
,	O
i.e.	O
it	O
computes	O
the	O
average	O
of	O
two	O
terms	O
,	O
first	O
of	O
which	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	Method
embedding	Method
of	O
the	O
subject	O
entity	O
,	O
relation	Method
embedding	Method
and	O
tail	O
embedding	O
of	O
the	O
object	O
entity	O
and	O
the	O
second	O
is	O
a	O
bilinear	O
product	O
of	O
the	O
head	O
embedding	O
of	O
the	O
object	O
entity	O
,	O
inverse	Method
relation	Method
embedding	Method
and	O
tail	O
embedding	O
of	O
the	O
subject	O
entity	O
.	O
Recently	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
have	O
been	O
achieved	O
with	O
non	Method
-	Method
linear	Method
models	Method
:	O
ConvE	Method
ConvE	Method
is	O
the	O
first	O
non	Method
-	Method
linear	Method
model	Method
that	O
significantly	O
outperformed	O
the	O
preceding	O
linear	Method
models	Method
.	O
In	O
ConvE	O
,	O
a	O
global	Method
2D	Method
convolution	Method
operation	Method
is	O
performed	O
on	O
the	O
subject	O
entity	O
and	O
relation	O
embedding	O
vectors	O
,	O
after	O
they	O
are	O
reshaped	O
to	O
matrices	O
and	O
concatenated	O
.	O
The	O
obtained	O
feature	O
maps	O
are	O
flattened	O
,	O
transformed	O
through	O
a	O
fully	Method
connected	Method
layer	Method
,	O
and	O
the	O
inner	O
product	O
is	O
taken	O
with	O
all	O
object	O
entity	O
vectors	O
to	O
generate	O
a	O
score	O
for	O
each	O
triple	O
.	O
Whilst	O
results	O
achieved	O
by	O
ConvE	Method
are	O
impressive	O
,	O
its	O
reshaping	O
and	O
concatenating	Method
of	Method
vectors	Method
as	O
well	O
as	O
using	O
2D	Method
convolution	Method
on	O
word	Method
embeddings	Method
is	O
unintuitive	O
.	O
HypER	O
HypER	Method
is	O
a	O
simplified	Method
convolutional	Method
model	Method
,	O
that	O
uses	O
a	O
hypernetwork	Method
to	O
generate	O
1D	Method
convolutional	Method
filters	Method
for	O
each	O
relation	O
,	O
extracting	O
relation	O
-	O
specific	O
features	O
from	O
subject	O
entity	O
embeddings	O
.	O
The	O
authors	O
show	O
that	O
convolution	Method
is	O
a	O
way	O
of	O
introducing	O
sparsity	Task
and	Task
parameter	Task
tying	Task
and	O
that	O
HypER	Method
can	O
be	O
understood	O
in	O
terms	O
of	O
tensor	Method
factorization	Method
up	O
to	O
a	O
non	O
-	O
linearity	O
,	O
thus	O
placing	O
HypER	Method
closer	O
to	O
the	O
well	O
established	O
family	O
of	O
factorization	Method
models	Method
.	O
The	O
drawback	O
of	O
HypER	Method
is	O
that	O
it	O
sets	O
most	O
elements	O
of	O
the	O
core	O
weight	O
tensor	O
to	O
0	O
,	O
which	O
amounts	O
to	O
hard	O
regularization	O
,	O
rather	O
than	O
letting	O
the	O
model	O
learn	O
which	O
parameters	O
to	O
use	O
via	O
a	O
soft	Method
regularization	Method
approach	Method
.	O
Scoring	Metric
functions	Metric
of	O
all	O
models	O
described	O
above	O
and	O
TuckER	Method
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
section	O
:	O
Background	O
Let	O
denote	O
the	O
set	O
of	O
all	O
entities	O
and	O
the	O
set	O
of	O
all	O
relations	O
present	O
in	O
a	O
knowledge	O
graph	O
.	O
A	O
triple	O
is	O
represented	O
as	O
,	O
with	O
denoting	O
subject	O
and	O
object	O
entities	O
respectively	O
and	O
the	O
relation	O
between	O
them	O
.	O
subsection	O
:	O
Link	Task
Prediction	Task
In	O
link	Task
prediction	Task
,	O
we	O
are	O
given	O
a	O
subset	O
of	O
all	O
true	O
triples	O
and	O
the	O
aim	O
is	O
to	O
learn	O
a	O
scoring	Method
function	Method
that	O
assigns	O
a	O
score	O
to	O
each	O
triple	O
,	O
indicating	O
whether	O
that	O
triple	O
is	O
true	O
or	O
false	O
,	O
with	O
the	O
ultimate	O
goal	O
of	O
being	O
able	O
to	O
correctly	O
score	O
all	O
missing	O
triples	O
.	O
The	O
scoring	Method
function	Method
is	O
either	O
a	O
specific	O
form	O
of	O
tensor	Method
factorization	Method
in	O
the	O
case	O
of	O
linear	Method
models	Method
or	O
a	O
more	O
complex	O
(	O
deep	Method
)	Method
neural	Method
network	Method
architecture	Method
in	O
the	O
case	O
of	O
non	Method
-	Method
linear	Method
models	Method
.	O
Typically	O
,	O
a	O
positive	O
score	O
for	O
a	O
particular	O
triple	O
indicates	O
a	O
true	O
fact	O
predicted	O
by	O
the	O
model	O
,	O
while	O
a	O
negative	O
score	O
indicates	O
a	O
false	O
one	O
.	O
With	O
most	O
recent	O
models	O
,	O
a	O
non	Method
-	Method
linearity	Method
such	O
as	O
the	O
logistic	O
sigmoid	O
function	O
is	O
typically	O
applied	O
to	O
the	O
score	O
to	O
give	O
a	O
corresponding	O
probability	O
prediction	O
as	O
to	O
whether	O
a	O
certain	O
fact	O
is	O
true	O
.	O
subsection	O
:	O
Tucker	Method
Decomposition	Method
Tucker	Method
decomposition	Method
,	O
named	O
after	O
Ledyard	O
R.	O
Tucker	O
and	O
refined	O
in	O
his	O
subsequent	O
work	O
,	O
decomposes	O
a	O
tensor	O
into	O
a	O
set	O
of	O
matrices	O
and	O
a	O
smaller	O
core	O
tensor	O
.	O
In	O
a	O
three	Task
-	Task
mode	Task
case	Task
,	O
given	O
the	O
original	O
tensor	O
,	O
Tucker	Method
decomposition	Method
outputs	O
a	O
tensor	O
and	O
three	O
matrices	O
,	O
,	O
:	O
with	O
indicating	O
the	O
tensor	O
product	O
along	O
the	O
n	O
-	O
th	O
mode	O
and	O
the	O
vector	O
inner	O
product	O
.	O
Factor	O
matrices	O
,	O
and	O
,	O
when	O
orthogonal	O
,	O
can	O
be	O
thought	O
of	O
as	O
the	O
principal	O
components	O
in	O
each	O
mode	O
.	O
Elements	O
of	O
the	O
core	O
tensor	O
show	O
the	O
level	O
of	O
interaction	O
between	O
the	O
different	O
components	O
.	O
Typically	O
,	O
,	O
,	O
are	O
smaller	O
than	O
,	O
,	O
respectively	O
,	O
so	O
can	O
be	O
thought	O
of	O
as	O
a	O
compressed	O
version	O
of	O
.	O
Tucker	Method
decomposition	Method
is	O
not	O
unique	O
,	O
i.e.	O
we	O
can	O
transform	O
without	O
affecting	O
the	O
fit	O
if	O
we	O
apply	O
the	O
inverse	O
of	O
that	O
transformation	O
to	O
the	O
factor	O
matrices	O
.	O
Imposing	O
additional	O
constraints	O
on	O
the	O
structure	O
of	O
,	O
such	O
as	O
sparsity	O
,	O
making	O
its	O
elements	O
small	O
or	O
making	O
the	O
core	O
“	O
all	O
-	O
orthogonal	O
”	O
,	O
can	O
lead	O
to	O
improved	O
uniqueness	O
.	O
section	O
:	O
Tucker	Method
Decomposition	Method
for	O
Link	Task
Prediction	Task
We	O
propose	O
a	O
model	O
that	O
uses	O
Tucker	Method
decomposition	Method
for	O
link	Task
prediction	Task
on	O
the	O
third	Method
-	Method
order	Method
binary	Method
tensor	Method
representation	Method
of	O
a	O
knowledge	O
graph	O
,	O
with	O
entity	O
embedding	O
matrix	O
that	O
is	O
equivalent	O
for	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
relation	O
embedding	O
matrix	O
,	O
where	O
and	O
represent	O
the	O
number	O
of	O
entities	O
and	O
relations	O
and	O
and	O
the	O
dimensionality	O
of	O
entity	O
and	O
relation	O
embedding	O
vectors	O
respectively	O
.	O
We	O
define	O
the	O
scoring	Metric
function	Metric
for	O
TuckER	Method
as	O
:	O
where	O
are	O
the	O
rows	O
of	O
representing	O
the	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
,	O
the	O
rows	O
of	O
representing	O
the	O
relation	O
embedding	O
vector	O
,	O
is	O
the	O
core	Method
tensor	Method
of	Method
Tucker	Method
decomposition	Method
and	O
is	O
the	O
tensor	O
product	O
along	O
the	O
-	O
th	O
mode	O
.	O
We	O
apply	O
logistic	Method
sigmoid	Method
to	O
each	O
score	O
to	O
obtain	O
the	O
predicted	O
probability	O
of	O
a	O
triple	O
being	O
true	O
.	O
Visualization	O
of	O
the	O
TuckER	Method
model	O
architecture	O
can	O
be	O
seen	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
As	O
proven	O
in	O
Section	O
[	O
reference	O
]	O
,	O
TuckER	Method
is	O
fully	O
expressive	O
,	O
i.e.	O
given	O
sufficient	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
,	O
it	O
is	O
able	O
to	O
assign	O
values	O
to	O
the	O
embeddings	O
that	O
correctly	O
separate	O
any	O
combination	O
of	O
ground	O
truth	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O
The	O
number	O
of	O
parameters	O
of	O
TuckER	Method
increases	O
linearly	O
with	O
respect	O
to	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
,	O
as	O
the	O
number	O
of	O
entities	O
and	O
relations	O
(	O
and	O
respectively	O
)	O
increases	O
,	O
since	O
the	O
number	O
of	O
parameters	O
of	O
core	Method
tensor	Method
depends	O
only	O
on	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
and	O
not	O
on	O
the	O
number	O
of	O
entities	O
or	O
relations	O
.	O
By	O
having	O
the	O
core	O
tensor	O
,	O
unlike	O
simpler	O
models	O
such	O
as	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
TuckER	Method
does	O
not	O
encode	O
all	O
the	O
learned	O
knowledge	O
into	O
the	O
embeddings	O
;	O
some	O
is	O
stored	O
in	O
the	O
core	O
tensor	O
and	O
shared	O
between	O
all	O
entities	O
and	O
relations	O
.	O
subsection	O
:	O
Training	O
Given	O
we	O
can	O
not	O
use	O
analytical	Method
methods	Method
for	O
computing	O
the	O
tensor	Task
factorization	Task
,	O
since	O
the	O
tensor	O
being	O
factorized	O
is	O
comprised	O
of	O
and	O
(	O
after	O
applying	O
the	O
inverse	O
of	O
logistic	O
sigmoid	O
)	O
,	O
we	O
use	O
numerical	Method
methods	Method
to	O
train	O
TuckER	Method
.	O
Following	O
the	O
training	O
procedure	O
introduced	O
by	O
dettmers2018convolutional	Method
with	O
the	O
goal	O
of	O
speeding	O
up	O
training	Task
and	O
increasing	O
accuracy	Metric
,	O
we	O
use	O
1	Method
-	Method
N	Method
scoring	Method
,	O
i.e.	O
we	O
simultaneously	O
score	O
a	O
pair	O
and	O
with	O
all	O
entities	O
,	O
in	O
contrast	O
to	O
1	O
-	O
1	O
scoring	O
,	O
where	O
individual	O
triples	O
are	O
trained	O
one	O
at	O
a	O
time	O
.	O
This	O
way	O
we	O
make	O
use	O
of	O
the	O
local	O
-	O
closed	O
world	O
assumption	O
,	O
where	O
we	O
assume	O
that	O
a	O
knowledge	O
graph	O
is	O
only	O
locally	O
complete	O
,	O
i.e.	O
we	O
include	O
only	O
the	O
non	O
-	O
existing	O
triples	O
and	O
of	O
the	O
observed	O
pairs	O
and	O
respectively	O
as	O
negative	O
samples	O
and	O
all	O
observed	O
triples	O
as	O
positive	O
samples	O
.	O
We	O
train	O
our	O
model	O
to	O
minimize	O
the	O
Bernoulli	O
negative	O
log	O
-	O
likelihood	O
loss	O
function	O
:	O
where	O
is	O
the	O
vector	O
of	O
probabilities	O
predicted	O
by	O
the	O
model	O
and	O
is	O
the	O
label	O
vector	O
of	O
ones	O
for	O
true	O
and	O
zeros	O
for	O
false	O
triples	O
.	O
section	O
:	O
Theoretical	O
Analysis	O
subsection	O
:	O
Bound	O
on	O
Embedding	Metric
Dimensionality	Metric
for	O
Full	Task
Expressiveness	Task
As	O
previously	O
stated	O
in	O
Section	O
[	O
reference	O
]	O
,	O
a	O
tensor	Method
factorization	Method
model	Method
is	O
said	O
to	O
be	O
fully	O
expressive	O
if	O
for	O
any	O
ground	O
truth	O
over	O
all	O
entities	O
and	O
relations	O
,	O
there	O
exist	O
entity	O
and	O
relation	O
embeddings	O
that	O
accurately	O
separate	O
the	O
true	O
triples	O
from	O
the	O
false	O
ones	O
.	O
As	O
shown	O
in	O
,	O
ComplEx	Method
is	O
fully	O
expressive	O
with	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
of	O
for	O
achieving	O
full	O
expressiveness	O
.	O
Similarly	O
to	O
ComplEx	Method
,	O
kazemi2018simple	Method
show	O
that	O
SimplE	Method
is	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embeddings	O
of	O
size	O
,	O
with	O
representing	O
the	O
number	O
of	O
true	O
facts	O
.	O
The	O
authors	O
further	O
prove	O
other	O
models	O
are	O
not	O
fully	O
expressive	O
:	O
DistMult	Method
,	O
because	O
it	O
can	O
not	O
model	O
asymmetric	O
relations	O
;	O
and	O
transitive	Method
models	Method
such	O
as	O
TransE	Method
and	O
its	O
variants	O
FTransE	Method
and	O
STransE	Method
,	O
because	O
of	O
certain	O
contradictions	O
that	O
they	O
impose	O
between	O
different	O
relation	O
types	O
.	O
By	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
establish	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
(	O
i.e.	O
rank	O
of	O
the	O
decomposition	O
)	O
that	O
guarantees	O
full	O
expressiveness	O
of	O
TuckER	Method
.	O
theorem	O
:	O
.	O
Given	O
any	O
ground	O
truth	O
over	O
a	O
set	O
of	O
entities	O
E	O
and	O
relations	O
R	O
,	O
there	O
exists	O
a	O
TuckER	Method
model	O
with	O
subject	Method
and	Method
object	Method
entity	Method
embeddings	Method
of	O
dimensionality	O
=	O
dene	O
and	O
relation	O
embeddings	O
of	O
dimensionality	O
=	O
drnr	O
,	O
where	O
=	O
ne|E|	O
is	O
the	O
number	O
of	O
entities	O
and	O
=	O
nr|R|	O
the	O
number	O
of	O
relations	O
,	O
that	O
accurately	O
represents	O
that	O
ground	O
truth	O
.	O
proof	O
:	O
Proof	O
.	O
Let	O
and	O
be	O
the	O
-	O
dimensional	O
one	O
-	O
hot	Method
binary	Method
vector	Method
representations	Method
of	Method
subject	Method
and	Method
object	Method
entities	Method
and	O
respectively	O
and	O
the	O
-	O
dimensional	O
one	Method
-	Method
hot	Method
binary	Method
vector	Method
representation	Method
of	O
a	O
relation	O
.	O
For	O
each	O
subject	O
entity	O
,	O
relation	O
and	O
object	O
entity	O
,	O
we	O
let	O
the	O
-	O
th	O
,	O
-	O
th	O
and	O
-	O
th	O
element	O
respectively	O
of	O
the	O
corresponding	O
vectors	O
,	O
and	O
be	O
1	O
and	O
all	O
other	O
elements	O
0	O
.	O
Further	O
,	O
we	O
set	O
the	O
element	O
of	O
the	O
tensor	O
to	O
1	O
if	O
the	O
fact	O
holds	O
and	O
-	O
1	O
otherwise	O
.	O
Thus	O
the	O
tensor	O
product	O
of	O
these	O
entity	Method
embeddings	Method
and	O
the	O
relation	Method
embedding	Method
with	O
the	O
core	O
tensor	O
,	O
after	O
applying	O
the	O
logistic	Method
sigmoid	Method
,	O
accurately	O
represents	O
the	O
original	O
tensor	O
.	O
∎	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
Theorem	O
[	O
reference	O
]	O
shows	O
that	O
it	O
is	O
straightforward	O
to	O
prove	O
that	O
the	O
required	O
dimensionality	Metric
of	O
TuckER	Method
embeddings	O
to	O
ensure	O
full	O
expressiveness	O
is	O
lower	O
than	O
the	O
required	O
dimensionality	Metric
for	O
SimplE	O
and	O
ComplEx	O
by	O
a	O
factor	O
of	O
for	O
entity	Method
embeddings	Method
and	O
by	O
a	O
factor	O
of	O
for	O
relation	Method
embeddings	Method
.	O
Existing	O
knowledge	O
graphs	O
usually	O
contain	O
tens	O
of	O
thousands	O
of	O
entities	O
and	O
hundreds	O
or	O
even	O
thousands	O
of	O
relations	O
.	O
This	O
allows	O
TuckER	Method
to	O
be	O
fully	O
expressive	O
with	O
entity	O
and	O
relation	O
embedding	O
dimensionalities	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
those	O
of	O
ComplEx	Method
and	O
SimplE.	O
In	O
practice	O
,	O
we	O
expect	O
the	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
needed	O
for	O
full	Task
reconstruction	Task
of	O
the	O
underlying	O
binary	O
tensor	O
to	O
be	O
much	O
smaller	O
than	O
the	O
bound	O
stated	O
above	O
,	O
since	O
the	O
assignment	O
of	O
values	O
to	O
the	O
tensor	O
is	O
not	O
random	O
but	O
follows	O
a	O
certain	O
structure	O
,	O
otherwise	O
nothing	O
unknown	O
could	O
be	O
predicted	O
.	O
Even	O
more	O
so	O
,	O
low	O
decomposition	O
rank	O
is	O
actually	O
a	O
desired	O
property	O
,	O
forcing	O
the	O
model	O
to	O
learn	O
that	O
structure	O
and	O
generalize	O
to	O
new	O
data	O
,	O
rather	O
than	O
simply	O
memorizing	O
the	O
input	O
.	O
We	O
expect	O
TuckER	Method
to	O
perform	O
better	O
than	O
ComplEx	Method
and	O
SimplE	O
with	O
embeddings	O
of	O
lower	O
dimensionality	O
due	O
to	O
parameter	O
sharing	O
in	O
the	O
core	O
tensor	O
(	O
shown	O
empirically	O
in	O
Section	O
[	O
reference	O
]	O
)	O
,	O
which	O
could	O
be	O
of	O
importance	O
for	O
efficiency	O
in	O
downstream	Task
tasks	Task
.	O
subsection	O
:	O
Relation	O
of	O
TuckER	Method
to	O
Previous	O
Tensor	Method
Factorization	Method
Approaches	Method
Several	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
TuckER	Method
:	O
RESCAL	O
[	O
]	O
Following	O
the	O
notation	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
RESCAL	Metric
scoring	Metric
function	Metric
has	O
the	O
form	O
:	O
This	O
corresponds	O
to	O
Equation	O
[	O
reference	O
]	O
with	O
,	O
,	O
and	O
the	O
identity	O
matrix	O
,	O
i.e	O
the	O
second	O
dimension	O
of	O
original	O
tensor	O
is	O
not	O
reduced	O
by	O
.	O
This	O
is	O
also	O
known	O
as	O
Tucker2	Method
decomposition	Method
.	O
As	O
is	O
the	O
case	O
with	O
TuckER	Method
,	O
the	O
entity	O
embedding	O
matrix	O
of	O
RESCAL	O
is	O
shared	O
between	O
subject	O
and	O
object	O
entities	O
,	O
i.e.	O
and	O
the	O
relation	O
matrices	O
are	O
the	O
slices	O
of	O
the	O
core	O
tensor	O
.	O
As	O
mentioned	O
in	O
Section	O
[	O
reference	O
]	O
,	O
the	O
drawback	O
of	O
RESCAL	Method
compared	O
to	O
TuckER	Method
is	O
that	O
its	O
number	O
of	O
parameters	O
grows	O
quadratically	O
in	O
the	O
entity	Metric
embedding	Metric
dimension	Metric
as	O
the	O
number	O
of	O
relations	O
increases	O
.	O
Therefore	O
,	O
RESCAL	Method
tends	O
to	O
overfit	O
for	O
those	O
relations	O
for	O
which	O
only	O
a	O
small	O
number	O
of	O
training	O
triples	O
is	O
available	O
.	O
DistMult	Method
[	O
]	O
The	O
scoring	Metric
function	Metric
of	O
DistMult	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
can	O
be	O
viewed	O
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
that	O
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
superdiagonal	O
with	O
1s	O
on	O
that	O
superdiagonal	O
,	O
i.e.	O
all	O
elements	O
with	O
are	O
1	O
and	O
all	O
the	O
other	O
elements	O
are	O
0	O
(	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
with	O
a	O
core	O
tensor	O
,	O
,	O
which	O
is	O
diagonal	O
for	O
every	O
slice	O
,	O
i.e.	O
all	O
elements	O
apart	O
from	O
with	O
are	O
0	O
.	O
If	O
we	O
adopt	O
the	O
TuckER	Method
view	O
of	O
the	O
DistMult	Method
scoring	Method
function	Method
,	O
rows	O
of	O
contain	O
subject	O
and	O
object	O
entity	O
embedding	O
vectors	O
and	O
rows	O
of	O
contain	O
relation	O
embedding	O
vectors	O
.	O
By	O
adopting	O
the	O
RESCAL	O
view	O
,	O
entity	O
embedding	O
embedding	O
vectors	O
remain	O
the	O
same	O
,	O
but	O
relation	O
embedding	O
vectors	O
are	O
now	O
the	O
diagonals	O
of	O
slices	O
of	O
.	O
It	O
is	O
interesting	O
to	O
note	O
that	O
the	O
TuckER	Method
interpretation	O
of	O
the	O
DistMult	Method
scoring	Method
function	Method
,	O
given	O
that	O
matrices	O
and	O
are	O
identical	O
,	O
can	O
alternatively	O
be	O
interpreted	O
as	O
a	O
special	O
case	O
of	O
CP	Method
decomposition	Method
,	O
since	O
Tucker	Method
decomposition	Method
with	O
a	O
superdiagonal	O
core	O
tensor	O
becomes	O
equivalent	O
to	O
CP	Method
decomposition	Method
.	O
Because	O
of	O
its	O
simplicity	O
,	O
DistMult	Method
learns	O
a	O
binary	O
tensor	O
that	O
is	O
symmetric	O
in	O
the	O
subject	O
and	O
object	O
entity	O
mode	O
,	O
so	O
it	O
can	O
not	O
learn	O
to	O
represent	O
asymmetric	O
relations	O
.	O
ComplEx	O
[	O
]	O
Bilinear	Method
models	Method
are	O
a	O
family	O
of	O
models	O
where	O
subject	O
and	O
object	O
entity	O
embeddings	O
are	O
represented	O
by	O
vectors	O
,	O
a	O
relation	O
is	O
represented	O
by	O
a	O
matrix	O
and	O
the	O
scoring	O
function	O
takes	O
the	O
form	O
of	O
a	O
bilinear	O
product	O
between	O
the	O
two	O
embedding	O
vectors	O
and	O
the	O
relation	O
matrix	O
,	O
i.e.	O
.	O
It	O
is	O
trivial	O
to	O
show	O
that	O
both	O
RESCAL	Method
and	O
DistMult	Method
belong	O
to	O
the	O
family	Method
of	Method
bilinear	Method
models	Method
.	O
As	O
explained	O
by	O
kazemi2018simple	O
,	O
ComplEx	Method
can	O
be	O
considered	O
a	O
bilinear	Method
model	Method
with	O
the	O
real	O
and	O
imaginary	O
part	O
of	O
an	O
embedding	O
for	O
each	O
entity	O
concatenated	O
in	O
a	O
single	O
vector	O
,	O
for	O
subject	O
,	O
for	O
object	O
,	O
and	O
a	O
relation	O
matrix	O
,	O
constrained	O
in	O
a	O
way	O
that	O
its	O
leading	O
diagonal	O
contains	O
duplicated	O
elements	O
of	O
,	O
its	O
-	O
diagonal	O
contains	O
elements	O
of	O
and	O
its	O
-	O
-	O
diagonal	O
has	O
elements	O
of	O
-	O
,	O
with	O
all	O
other	O
elements	O
set	O
to	O
0	O
,	O
where	O
and	O
-	O
represent	O
offsets	O
from	O
the	O
leading	O
diagonal	O
.	O
This	O
makes	O
the	O
scoring	Metric
function	Metric
of	O
ComplEx	Method
(	O
see	O
Table	O
[	O
reference	O
]	O
)	O
equivalent	O
to	O
that	O
of	O
RESCAL	O
with	O
relation	O
matrix	O
constrained	O
as	O
described	O
.	O
Therefore	O
,	O
similarly	O
to	O
DistMult	Method
,	O
we	O
can	O
regard	O
the	O
scoring	O
function	O
of	O
ComplEx	Method
in	O
two	O
ways	O
:	O
as	O
equivalent	O
to	O
the	O
scoring	Method
function	Method
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
different	O
tensor	O
diagonals	O
are	O
set	O
to	O
1	O
,	O
elements	O
on	O
one	O
tensor	O
diagonal	O
are	O
set	O
to	O
-	O
1	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
as	O
equivalent	O
to	O
the	O
scoring	Metric
function	Metric
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
of	O
,	O
all	O
elements	O
on	O
the	O
leading	O
diagonal	O
are	O
set	O
to	O
,	O
the	O
-	O
diagonal	O
is	O
set	O
to	O
,	O
the	O
-	O
-	O
diagonal	O
is	O
set	O
to	O
-	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
.	O
This	O
shows	O
that	O
the	O
scoring	O
function	O
of	O
ComplEx	Method
,	O
which	O
computes	O
a	O
bilinear	O
product	O
with	O
complex	O
entity	O
and	O
relation	O
embeddings	O
and	O
disregards	O
the	O
imaginary	O
part	O
of	O
the	O
obtained	O
result	O
,	O
is	O
equivalent	O
to	O
a	O
hard	O
regularization	O
of	O
the	O
core	O
tensor	O
of	O
TuckER	Method
in	O
the	O
real	O
domain	O
.	O
SimplE	O
[	O
]	O
The	O
authors	O
show	O
that	O
SimplE	O
belongs	O
to	O
the	O
family	O
of	O
bilinear	Method
models	Method
by	O
concatenating	Method
embeddings	Method
for	O
head	O
and	O
tail	O
entities	O
for	O
both	O
subject	O
and	O
object	O
into	O
vectors	O
and	O
and	O
constraining	O
the	O
relation	O
matrix	O
so	O
that	O
it	O
contains	O
the	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
diagonal	O
and	O
the	O
inverse	O
relation	O
embedding	O
vector	O
on	O
its	O
-	O
-	O
diagonal	O
and	O
0s	O
elsewhere	O
.	O
The	O
SimplE	O
scoring	Method
function	Method
is	O
therefore	O
equivalent	O
to	O
:	O
that	O
of	O
TuckER	Method
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
,	O
where	O
elements	O
on	O
two	O
tensor	O
diagonals	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
set	O
to	O
0	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
;	O
and	O
that	O
of	O
RESCAL	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
,	O
with	O
core	O
tensor	O
,	O
where	O
for	O
each	O
slice	O
,	O
elements	O
on	O
the	O
-	O
diagonal	O
are	O
set	O
to	O
,	O
elements	O
on	O
the	O
-	O
-	O
diagonal	O
are	O
set	O
to	O
and	O
all	O
other	O
elements	O
are	O
0	O
.	O
subsection	O
:	O
Representing	O
Asymmetric	O
Relations	O
Each	O
relation	O
in	O
a	O
knowledge	O
graph	O
can	O
be	O
characterized	O
by	O
a	O
certain	O
set	O
of	O
properties	O
,	O
such	O
as	O
symmetry	O
,	O
reflexivity	O
,	O
transitivity	O
,	O
etc	O
.	O
A	O
relation	O
is	O
asymmetric	O
if	O
,	O
for	O
all	O
subject	O
entities	O
that	O
are	O
related	O
to	O
their	O
corresponding	O
object	O
entities	O
through	O
,	O
the	O
reciprocal	O
necessarily	O
does	O
not	O
hold	O
,	O
i.e.	O
none	O
of	O
the	O
object	O
entities	O
are	O
related	O
to	O
the	O
subject	O
entities	O
through	O
.	O
So	O
far	O
,	O
there	O
have	O
been	O
two	O
possible	O
ways	O
in	O
which	O
linear	Method
link	Method
prediction	Method
models	Method
introduce	O
asymmetry	O
into	O
factorization	Task
of	O
the	O
binary	O
tensor	O
of	O
triples	O
.	O
One	O
is	O
to	O
have	O
distinct	O
(	O
although	O
possibly	O
related	O
)	O
embeddings	O
for	O
subject	O
and	O
object	O
entities	O
and	O
a	O
diagonal	O
matrix	O
(	O
or	O
equivalently	O
a	O
vector	O
)	O
for	O
each	O
relation	O
,	O
as	O
is	O
the	O
case	O
with	O
models	O
such	O
as	O
ComplEx	O
and	O
SimplE.	O
This	O
puts	O
a	O
strict	O
constraint	O
on	O
the	O
relation	O
matrix	O
and	O
imposes	O
a	O
hard	O
limit	O
on	O
the	O
type	O
of	O
transformation	O
applied	O
on	O
entity	O
embeddings	O
.	O
The	O
other	O
way	O
of	O
modeling	O
asymmetry	O
is	O
for	O
subject	O
and	O
object	O
entity	O
embeddings	O
to	O
be	O
equivalent	O
,	O
but	O
representing	O
a	O
relation	O
as	O
a	O
full	O
rank	O
matrix	O
,	O
which	O
is	O
the	O
case	O
with	O
RESCAL	O
.	O
The	O
drawback	O
of	O
the	O
latter	O
approach	O
is	O
quadratic	O
growth	O
of	O
parameter	O
number	O
with	O
the	O
number	O
of	O
relations	O
,	O
which	O
often	O
leads	O
to	O
overfitting	O
,	O
especially	O
for	O
relations	O
with	O
a	O
small	O
number	O
of	O
training	O
triples	O
.	O
TuckER	Method
introduces	O
a	O
novel	O
approach	O
to	O
dealing	O
with	O
asymmetry	O
:	O
by	O
representing	O
relations	O
as	O
vectors	O
,	O
which	O
makes	O
the	O
parameter	O
number	O
grow	O
linearly	O
with	O
the	O
number	O
of	O
relations	O
;	O
and	O
by	O
having	O
an	O
asymmetric	O
relation	O
-	O
agnostic	O
core	O
tensor	O
,	O
which	O
enables	O
knowledge	Task
sharing	Task
between	O
relations	O
.	O
Multiplying	O
with	O
along	O
the	O
second	O
mode	O
,	O
we	O
obtain	O
a	O
full	O
rank	O
relation	O
-	O
specific	O
matrix	O
,	O
which	O
is	O
capable	O
of	O
performing	O
all	O
possible	O
linear	O
transformations	O
on	O
the	O
entity	O
embeddings	O
,	O
i.e.	O
rotation	O
,	O
reflection	O
or	O
stretch	O
,	O
and	O
thus	O
capable	O
of	O
modeling	O
asymmetry	O
.	O
Regardless	O
of	O
what	O
kind	O
of	O
transformation	O
is	O
needed	O
for	O
modeling	O
a	O
particular	O
relation	O
,	O
TuckER	Method
is	O
capable	O
of	O
learning	O
it	O
from	O
the	O
data	O
,	O
rather	O
than	O
through	O
explicitly	O
limiting	O
the	O
relation	O
matrix	O
.	O
section	O
:	O
Experiments	O
and	O
Results	O
subsection	O
:	O
Datasets	O
We	O
evaluate	O
TuckER	Method
using	O
four	O
standard	O
link	O
prediction	O
datasets	O
:	O
FB15k	Material
is	O
a	O
subset	O
of	O
Freebase	O
,	O
a	O
large	O
database	O
of	O
real	O
world	O
facts	O
containing	O
information	O
about	O
films	O
,	O
actors	O
,	O
sports	O
,	O
etc	O
.	O
FB15k	Material
-	Material
237	Material
was	O
created	O
from	O
FB15k	Material
by	O
removing	O
the	O
inverse	O
of	O
many	O
relations	O
that	O
are	O
present	O
in	O
the	O
training	O
set	O
from	O
validation	O
and	O
test	O
sets	O
,	O
making	O
it	O
more	O
difficult	O
for	O
simple	O
models	O
to	O
do	O
well	O
.	O
WN18	Material
is	O
a	O
subset	O
of	O
WordNet	O
,	O
a	O
database	O
containing	O
lexical	O
relations	O
between	O
words	O
.	O
WN18	Material
follows	O
a	O
hierarchical	O
structure	O
.	O
WN18RR	Material
is	O
a	O
subset	O
of	O
WN18	Material
,	O
created	O
by	O
removing	O
the	O
inverse	O
relations	O
from	O
validation	O
and	O
test	O
sets	O
.	O
Number	O
of	O
entities	O
and	O
relations	O
for	O
each	O
dataset	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
Implementation	O
and	O
Experiments	O
We	O
implement	O
TuckER	Method
in	O
PyTorch	Method
and	O
make	O
our	O
code	O
available	O
on	O
Github	O
.	O
We	O
choose	O
all	O
hyper	O
-	O
parameters	O
by	O
random	Method
search	Method
based	O
on	O
the	O
validation	Metric
set	Metric
performance	O
.	O
For	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
,	O
we	O
set	O
both	O
entity	O
and	O
relation	O
embedding	O
dimensions	O
to	O
.	O
For	O
WN18	Material
and	O
WN18RR	Material
,	O
which	O
both	O
contain	O
a	O
significantly	O
smaller	O
number	O
of	O
relations	O
relative	O
to	O
the	O
number	O
of	O
entities	O
as	O
well	O
as	O
a	O
small	O
number	O
of	O
relations	O
compared	O
to	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
,	O
we	O
set	O
and	O
.	O
We	O
use	O
both	O
batch	Method
normalization	Method
and	O
dropout	Method
to	O
control	O
overfitting	O
and	O
improve	O
predictions	Task
.	O
We	O
choose	O
the	O
learning	Metric
rate	Metric
from	O
and	O
learning	Metric
rate	Metric
decay	Metric
from	O
.	O
We	O
find	O
the	O
following	O
combinations	O
of	O
learning	Metric
rate	Metric
and	O
learning	Metric
rate	Metric
decay	Metric
to	O
give	O
the	O
best	O
results	O
:	O
for	O
FB15k	Material
,	O
for	O
FB15k	Material
-	Material
237	Material
,	O
for	O
WN18	Material
and	O
for	O
WN18RR	Material
.	O
We	O
train	O
the	O
model	O
using	O
Adam	Method
and	O
set	O
the	O
batch	O
size	O
to	O
128	O
.	O
We	O
evaluate	O
each	O
triple	O
from	O
the	O
test	O
set	O
as	O
in	O
:	O
for	O
a	O
given	O
triple	O
,	O
we	O
generate	O
test	O
triples	O
by	O
keeping	O
the	O
subject	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
object	O
entity	O
with	O
all	O
possible	O
entities	O
and	O
by	O
keeping	O
the	O
object	O
entity	O
and	O
relation	O
fixed	O
and	O
replacing	O
the	O
subject	O
entity	O
with	O
all	O
entities	O
.	O
We	O
then	O
rank	O
the	O
scores	O
obtained	O
.	O
We	O
use	O
the	O
filtered	O
setting	O
only	O
,	O
i.e.	O
we	O
remove	O
all	O
other	O
true	O
triples	O
apart	O
from	O
the	O
currently	O
observed	O
test	O
triple	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
two	O
evaluation	Metric
metrics	Metric
used	O
across	O
the	O
link	Task
prediction	Task
literature	Task
:	O
mean	Metric
reciprocal	Metric
rank	Metric
(	O
MRR	Metric
)	O
and	O
hits@	Metric
,	O
.	O
Mean	Metric
reciprocal	Metric
rank	Metric
is	O
the	O
average	O
of	O
the	O
inverse	O
of	O
a	O
mean	O
rank	O
assigned	O
to	O
the	O
true	O
triple	O
over	O
all	O
generated	O
triples	O
.	O
Hits@	Metric
measures	O
the	O
percentage	O
of	O
times	O
the	O
true	O
triple	O
is	O
ranked	O
in	O
the	O
top	O
of	O
the	O
generated	O
triples	O
.	O
The	O
aim	O
is	O
for	O
a	O
model	O
to	O
achieve	O
high	O
MRR	Metric
and	O
hits@	Metric
.	O
subsection	O
:	O
Link	Task
Prediction	Task
Results	O
Link	Task
prediction	Task
results	O
on	O
all	O
four	O
datasets	O
are	O
shown	O
in	O
Tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
Overall	O
,	O
TuckER	Method
outperforms	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
all	O
metrics	O
across	O
all	O
datasets	O
(	O
apart	O
from	O
hits@10	Metric
on	O
WN18	Material
where	O
a	O
non	Method
-	Method
linear	Method
model	Method
,	O
R	Method
-	Method
GCN	Method
,	O
does	O
better	O
)	O
,	O
which	O
shows	O
that	O
this	O
relatively	O
simple	O
yet	O
fully	O
flexible	O
linear	Method
model	Method
leads	O
to	O
very	O
good	O
performance	O
.	O
Results	O
achieved	O
by	O
TuckER	Method
are	O
not	O
only	O
better	O
than	O
those	O
of	O
other	O
linear	Method
models	Method
,	O
such	O
as	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
,	O
but	O
also	O
better	O
than	O
the	O
results	O
of	O
many	O
more	O
complex	O
deep	Method
neural	Method
network	Method
and	Method
reinforcement	Method
learning	Method
architectures	Method
,	O
e.g.	O
R	Method
-	Method
GCN	Method
,	O
MINERVA	Method
,	O
ConvE	Method
and	O
HypER	Method
,	O
demonstrating	O
the	O
expressive	O
power	O
of	O
linear	Method
models	Method
.	O
Even	O
though	O
TuckER	Method
has	O
more	O
parameters	O
than	O
some	O
more	O
simpler	O
linear	Method
models	Method
(	O
DistMult	Method
,	O
ComplEx	O
and	O
SimplE	O
)	O
due	O
to	O
the	O
presence	O
of	O
core	O
tensor	O
(	O
containing	O
million	O
parameters	O
for	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
and	O
million	O
parameters	O
for	O
WN18	Material
and	O
WN18RR	Material
)	O
,	O
it	O
consistently	O
obtains	O
better	O
results	O
than	O
any	O
of	O
those	O
models	O
.	O
We	O
believe	O
this	O
is	O
achieved	O
by	O
exploiting	O
knowledge	O
sharing	O
between	O
relations	O
through	O
the	O
core	O
tensor	O
and	O
implicit	Method
regularization	Method
from	O
dropout	Method
,	O
which	O
allows	O
the	O
model	O
to	O
learn	O
which	O
parameters	O
to	O
ignore	O
rather	O
than	O
explicitly	O
setting	O
them	O
to	O
0	O
.	O
We	O
find	O
the	O
value	O
of	O
the	O
dropout	O
parameter	O
to	O
have	O
a	O
significant	O
influence	O
on	O
results	O
,	O
with	O
lower	O
dropout	O
values	O
required	O
for	O
datasets	O
with	O
a	O
higher	O
number	O
of	O
training	O
triples	O
per	O
relation	O
and	O
thus	O
less	O
risk	O
of	O
overfitting	O
(	O
WN18	Material
and	O
WN18RR	Material
)	O
and	O
higher	O
dropout	Metric
values	Metric
required	O
for	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
FB15k	Material
and	O
FB15k	Material
-	Material
237	Material
)	O
.	O
We	O
further	O
note	O
that	O
TuckER	Method
improves	O
the	O
results	O
of	O
all	O
previous	O
linear	Method
models	Method
by	O
a	O
larger	O
margin	O
on	O
datasets	O
with	O
a	O
large	O
number	O
of	O
relations	O
(	O
e.g.	O
improvement	O
on	O
FB15k	Material
results	O
over	O
ComplEx	Method
,	O
improvement	O
over	O
SimplE	O
on	O
the	O
toughest	Metric
hits@1	Metric
metric	Metric
)	O
,	O
which	O
supports	O
our	O
belief	O
that	O
TuckER	Method
makes	O
use	O
of	O
the	O
parameters	O
shared	O
between	O
similar	O
relations	O
to	O
improve	O
predictions	Task
by	O
multi	Task
-	Task
task	Task
learning	Task
.	O
subsection	O
:	O
Influence	O
of	O
Embedding	Metric
Dimensionality	Metric
In	O
Section	O
[	O
reference	O
]	O
,	O
we	O
derive	O
the	O
bound	O
on	O
entity	O
and	O
relation	O
embedding	O
dimensionality	O
for	O
full	Task
expressiveness	Task
that	O
is	O
much	O
lower	O
for	O
TuckER	Method
than	O
for	O
simpler	O
linear	Method
models	Method
ComplEx	O
and	O
SimplE.	O
This	O
suggests	O
TuckER	Method
should	O
need	O
a	O
lower	O
embedding	O
dimensionality	O
(	O
i.e.	O
lower	O
rank	O
of	O
the	O
decomposition	O
)	O
for	O
obtaining	O
good	O
results	O
than	O
ComplEx	Method
or	O
SimplE.	O
To	O
test	O
this	O
,	O
we	O
train	O
ComplEx	Method
,	O
SimplE	O
and	O
TuckER	Method
on	O
FB15k	Material
-	Material
237	Material
with	O
embedding	O
sizes	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
obtained	O
MRR	Metric
on	O
the	O
test	O
set	O
for	O
each	O
of	O
the	O
models	O
.	O
We	O
can	O
see	O
from	O
Figure	O
[	O
reference	O
]	O
that	O
the	O
difference	O
between	O
the	O
MRRs	Metric
of	O
ComplEx	Method
,	O
SimplE	O
and	O
TuckER	Method
is	O
approximately	O
constant	O
for	O
embedding	O
sizes	O
100	O
and	O
200	O
.	O
However	O
,	O
for	O
lower	O
embedding	O
sizes	O
,	O
the	O
difference	O
between	O
MRRs	Metric
increases	O
by	O
for	O
embedding	Metric
size	Metric
50	O
and	O
by	O
for	O
embedding	Metric
size	Metric
20	O
for	O
ComplEx	O
and	O
by	O
for	O
embedding	Metric
size	Metric
50	O
and	O
by	O
for	O
embedding	O
size	O
20	O
for	O
SimplE.	O
At	O
embedding	Metric
size	Metric
20	O
,	O
the	O
performance	O
of	O
TuckER	Method
is	O
almost	O
as	O
good	O
as	O
the	O
performance	O
of	O
ComplEx	Method
and	O
SimplE	O
at	O
embedding	O
size	O
200	O
,	O
which	O
supports	O
our	O
initial	O
assumption	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
introduce	O
TuckER	Method
,	O
a	O
relatively	O
simple	O
yet	O
highly	O
flexible	O
linear	Method
model	Method
for	O
link	Task
prediction	Task
in	O
knowledge	Task
graphs	Task
based	O
on	O
the	O
Tucker	Method
decomposition	Method
of	O
a	O
third	O
-	O
order	O
binary	O
tensor	O
of	O
training	O
set	O
triples	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
standard	O
link	Task
prediction	Task
datasets	Task
.	O
As	O
well	O
as	O
being	O
fully	O
expressive	O
,	O
TuckER	Method
’s	O
number	O
of	O
parameters	O
grows	O
linearly	O
with	O
respect	O
to	O
embedding	O
dimension	O
as	O
the	O
number	O
of	O
entities	O
or	O
relations	O
in	O
a	O
knowledge	O
graph	O
increases	O
.	O
We	O
further	O
show	O
that	O
previous	O
linear	Method
state	Method
-	Method
of	Method
-	Method
the	Method
-	Method
art	Method
models	Method
,	O
RESCAL	Method
,	O
DistMult	Method
,	O
ComplEx	Method
and	O
SimplE	O
,	O
are	O
all	O
special	O
cases	O
of	O
our	O
model	O
.	O
Future	O
work	O
might	O
include	O
exploring	O
various	O
means	O
of	O
softly	Method
regularizing	Method
the	O
model	O
other	O
than	O
dropout	Method
and	O
finding	O
a	O
way	O
to	O
incorporate	O
background	O
knowledge	O
on	O
individual	O
relation	O
properties	O
into	O
the	O
existing	O
model	O
.	O
bibliography	O
:	O
References	O
